{"sha": "2c35c7be14cfc9b9c515e5f4767c2f56e9217c44", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MmMzNWM3YmUxNGNmYzliOWM1MTVlNWY0NzY3YzJmNTZlOTIxN2M0NA==", "commit": {"author": {"name": "Phil Edwards", "email": "pme@gcc.gnu.org", "date": "2002-06-24T08:07:05Z"}, "committer": {"name": "Phil Edwards", "email": "pme@gcc.gnu.org", "date": "2002-06-24T08:07:05Z"}, "message": "stl_alloc.h: Reformat as per C++STYLE.\n\n2002-06-24  Phil Edwards  <pme@gcc.gnu.org>\n\n\t* include/bits/stl_alloc.h:  Reformat as per C++STYLE.\n\nFrom-SVN: r54949", "tree": {"sha": "4a2e1aa39d01b0fa30e713cbd9643c950d454273", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/4a2e1aa39d01b0fa30e713cbd9643c950d454273"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/2c35c7be14cfc9b9c515e5f4767c2f56e9217c44", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2c35c7be14cfc9b9c515e5f4767c2f56e9217c44", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2c35c7be14cfc9b9c515e5f4767c2f56e9217c44", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2c35c7be14cfc9b9c515e5f4767c2f56e9217c44/comments", "author": null, "committer": null, "parents": [{"sha": "6f68de5b29afc7d7941be58bc4a4bbf848c64bf3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6f68de5b29afc7d7941be58bc4a4bbf848c64bf3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6f68de5b29afc7d7941be58bc4a4bbf848c64bf3"}], "stats": {"total": 964, "additions": 488, "deletions": 476}, "files": [{"sha": "2af89a6a43da8c0440f67feaeea08ca97c0b8b4b", "filename": "libstdc++-v3/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2c35c7be14cfc9b9c515e5f4767c2f56e9217c44/libstdc%2B%2B-v3%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2c35c7be14cfc9b9c515e5f4767c2f56e9217c44/libstdc%2B%2B-v3%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2FChangeLog?ref=2c35c7be14cfc9b9c515e5f4767c2f56e9217c44", "patch": "@@ -1,3 +1,7 @@\n+2002-06-24  Phil Edwards  <pme@gcc.gnu.org>\n+\n+\t* include/bits/stl_alloc.h:  Reformat as per C++STYLE.\n+\n 2002-06-24  Phil Edwards  <pme@gcc.gnu.org>\n \n \t* config/cpu/*/bits/*:  Move header files up a level.  Remove bits."}, {"sha": "d4de9d3844de968ea674e690680e5f2088e84fd9", "filename": "libstdc++-v3/include/bits/stl_alloc.h", "status": "modified", "additions": 484, "deletions": 476, "changes": 960, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2c35c7be14cfc9b9c515e5f4767c2f56e9217c44/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fstl_alloc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2c35c7be14cfc9b9c515e5f4767c2f56e9217c44/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fstl_alloc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fstl_alloc.h?ref=2c35c7be14cfc9b9c515e5f4767c2f56e9217c44", "patch": "@@ -87,115 +87,118 @@\n \n namespace std\n {\n-  /**\n-   *  @if maint\n-   *  A new-based allocator, as required by the standard.  Allocation and\n-   *  deallocation forward to global new and delete.  \"SGI\" style, minus\n-   *  reallocate().\n-   *  @endif\n-   *  (See @link Allocators allocators info @endlink for more.)\n-  */\n-  class __new_alloc \n-  {\n-  public:\n-    static void* \n-    allocate(size_t __n)\n+\n+/**\n+ *  @if maint\n+ *  A new-based allocator, as required by the standard.  Allocation and\n+ *  deallocation forward to global new and delete.  \"SGI\" style, minus\n+ *  reallocate().\n+ *  @endif\n+ *  (See @link Allocators allocators info @endlink for more.)\n+*/\n+class __new_alloc\n+{\n+public:\n+  static void*\n+  allocate(size_t __n)\n     { return ::operator new(__n); }\n-    \n-    static void \n-    deallocate(void* __p, size_t)\n+\n+  static void\n+  deallocate(void* __p, size_t)\n     { ::operator delete(__p); }\n-  };\n-  \n-\n-  /**\n-   *  @if maint\n-   *  A malloc-based allocator.  Typically slower than the\n-   *  __default_alloc_template (below).  Typically thread-safe and more\n-   *  storage efficient.  The template argument is unused and is only present\n-   *  to permit multiple instantiations (but see __default_alloc_template\n-   *  for caveats).  \"SGI\" style, plus __set_malloc_handler for OOM conditions.\n-   *  @endif\n-   *  (See @link Allocators allocators info @endlink for more.)\n-  */\n-  template <int __inst>\n-    class __malloc_alloc_template \n-    {\n-    private:\n-      static void* _S_oom_malloc(size_t);\n-      static void* _S_oom_realloc(void*, size_t);\n-      static void (* __malloc_alloc_oom_handler)();\n-      \n-    public:\n-      static void* \n-      allocate(size_t __n)\n-      {\n-\tvoid* __result = malloc(__n);\n-\tif (0 == __result) __result = _S_oom_malloc(__n);\n-\treturn __result;\n-      }\n+};\n \n-      static void \n-      deallocate(void* __p, size_t /* __n */)\n-      { free(__p); }\n \n-      static void* \n-      reallocate(void* __p, size_t /* old_sz */, size_t __new_sz)\n-      {\n-\tvoid* __result = realloc(__p, __new_sz);\n-\tif (0 == __result) __result = _S_oom_realloc(__p, __new_sz);\n-\treturn __result;\n-      }\n-      \n-      static void (* __set_malloc_handler(void (*__f)()))()\n+/**\n+ *  @if maint\n+ *  A malloc-based allocator.  Typically slower than the\n+ *  __default_alloc_template (below).  Typically thread-safe and more\n+ *  storage efficient.  The template argument is unused and is only present\n+ *  to permit multiple instantiations (but see __default_alloc_template\n+ *  for caveats).  \"SGI\" style, plus __set_malloc_handler for OOM conditions.\n+ *  @endif\n+ *  (See @link Allocators allocators info @endlink for more.)\n+*/\n+template <int __inst>\n+  class __malloc_alloc_template\n+{\n+private:\n+  static void* _S_oom_malloc(size_t);\n+  static void* _S_oom_realloc(void*, size_t);\n+  static void (* __malloc_alloc_oom_handler)();\n+\n+public:\n+  static void*\n+  allocate(size_t __n)\n+  {\n+    void* __result = malloc(__n);\n+    if (0 == __result) __result = _S_oom_malloc(__n);\n+    return __result;\n+  }\n+\n+  static void\n+  deallocate(void* __p, size_t /* __n */)\n+  { free(__p); }\n+\n+  static void*\n+  reallocate(void* __p, size_t /* old_sz */, size_t __new_sz)\n+  {\n+    void* __result = realloc(__p, __new_sz);\n+    if (0 == __result) __result = _S_oom_realloc(__p, __new_sz);\n+    return __result;\n+  }\n+\n+  static void (* __set_malloc_handler(void (*__f)()))()\n+  {\n+    void (* __old)() = __malloc_alloc_oom_handler;\n+    __malloc_alloc_oom_handler = __f;\n+    return(__old);\n+  }\n+};\n+\n+// malloc_alloc out-of-memory handling\n+template <int __inst>\n+  void (* __malloc_alloc_template<__inst>::__malloc_alloc_oom_handler)() = 0;\n+\n+template <int __inst>\n+  void*\n+  __malloc_alloc_template<__inst>::\n+  _S_oom_malloc(size_t __n)\n+  {\n+    void (* __my_malloc_handler)();\n+    void* __result;\n+\n+    for (;;)\n       {\n-\tvoid (* __old)() = __malloc_alloc_oom_handler;\n-\t__malloc_alloc_oom_handler = __f;\n-\treturn(__old);\n+        __my_malloc_handler = __malloc_alloc_oom_handler;\n+        if (0 == __my_malloc_handler)\n+          std::__throw_bad_alloc();\n+        (*__my_malloc_handler)();\n+        __result = malloc(__n);\n+        if (__result)\n+          return(__result);\n       }\n-    };\n+  }\n \n-  // malloc_alloc out-of-memory handling\n-  template <int __inst>\n-    void (* __malloc_alloc_template<__inst>::__malloc_alloc_oom_handler)() = 0;\n+template <int __inst>\n+  void*\n+  __malloc_alloc_template<__inst>::\n+  _S_oom_realloc(void* __p, size_t __n)\n+  {\n+    void (* __my_malloc_handler)();\n+    void* __result;\n \n-  template <int __inst>\n-    void*\n-    __malloc_alloc_template<__inst>::_S_oom_malloc(size_t __n)\n-    {\n-      void (* __my_malloc_handler)();\n-      void* __result;\n-      \n-      for (;;) \n-\t{\n-\t  __my_malloc_handler = __malloc_alloc_oom_handler;\n-\t  if (0 == __my_malloc_handler) \n-\t    std::__throw_bad_alloc();\n-\t  (*__my_malloc_handler)();\n-\t  __result = malloc(__n);\n-\t  if (__result) \n-\t    return(__result);\n-\t}\n-    }\n-  \n-  template <int __inst>\n-    void* \n-    __malloc_alloc_template<__inst>::_S_oom_realloc(void* __p, size_t __n)\n-    { \n-      void (* __my_malloc_handler)();\n-      void* __result;\n-      \n-      for (;;) \n-\t{\n-\t  __my_malloc_handler = __malloc_alloc_oom_handler;\n-\t  if (0 == __my_malloc_handler) \n-\t    std::__throw_bad_alloc();\n-\t  (*__my_malloc_handler)();\n-\t  __result = realloc(__p, __n);\n-\t  if (__result) \n-\t    return(__result);\n-\t}\n-    }\n+    for (;;)\n+      {\n+        __my_malloc_handler = __malloc_alloc_oom_handler;\n+        if (0 == __my_malloc_handler)\n+          std::__throw_bad_alloc();\n+        (*__my_malloc_handler)();\n+        __result = realloc(__p, __n);\n+        if (__result)\n+          return(__result);\n+      }\n+  }\n \n \n // Determines the underlying allocator choice for the node allocator.\n@@ -206,82 +209,88 @@ namespace std\n #endif\n \n \n-  /**\n-   *  @if maint\n-   *  This is used primarily (only?) in _Alloc_traits and other places to\n-   *  help provide the _Alloc_type typedef.\n-   *\n-   *  This is neither \"standard\"-conforming nor \"SGI\".  The _Alloc parameter\n-   *  must be \"SGI\" style.\n-   *  @endif\n-   *  (See @link Allocators allocators info @endlink for more.)\n-  */\n-  template<class _Tp, class _Alloc>\n+/**\n+ *  @if maint\n+ *  This is used primarily (only?) in _Alloc_traits and other places to\n+ *  help provide the _Alloc_type typedef.\n+ *\n+ *  This is neither \"standard\"-conforming nor \"SGI\".  The _Alloc parameter\n+ *  must be \"SGI\" style.\n+ *  @endif\n+ *  (See @link Allocators allocators info @endlink for more.)\n+*/\n+template<class _Tp, class _Alloc>\n   class __simple_alloc\n-  {\n-  public:\n-    static _Tp* allocate(size_t __n)\n+{\n+public:\n+  static _Tp*\n+  allocate(size_t __n)\n     { return 0 == __n ? 0 : (_Tp*) _Alloc::allocate(__n * sizeof (_Tp)); }\n \n-    static _Tp* allocate()\n+  static _Tp*\n+  allocate()\n     { return (_Tp*) _Alloc::allocate(sizeof (_Tp)); }\n \n-    static void deallocate(_Tp* __p, size_t __n)\n+  static void\n+  deallocate(_Tp* __p, size_t __n)\n     { if (0 != __n) _Alloc::deallocate(__p, __n * sizeof (_Tp)); }\n \n-    static void deallocate(_Tp* __p)\n+  static void\n+  deallocate(_Tp* __p)\n     { _Alloc::deallocate(__p, sizeof (_Tp)); }\n-  };\n+};\n \n \n-  /**\n-   *  @if maint\n-   *  An adaptor for an underlying allocator (_Alloc) to check the size\n-   *  arguments for debugging.  Errors are reported using assert; these\n-   *  checks can be disabled via NDEBUG, but the space penalty is still\n-   *  paid, therefore it is far better to just use the underlying allocator\n-   *  by itelf when no checking is desired.\n-   *\n-   *  \"There is some evidence that this can confuse Purify.\" - SGI comment\n-   *\n-   *  This adaptor is \"SGI\" style.  The _Alloc parameter must also be \"SGI\".\n-   *  @endif\n-   *  (See @link Allocators allocators info @endlink for more.)\n-  */\n-  template <class _Alloc>\n+/**\n+ *  @if maint\n+ *  An adaptor for an underlying allocator (_Alloc) to check the size\n+ *  arguments for debugging.  Errors are reported using assert; these\n+ *  checks can be disabled via NDEBUG, but the space penalty is still\n+ *  paid, therefore it is far better to just use the underlying allocator\n+ *  by itelf when no checking is desired.\n+ *\n+ *  \"There is some evidence that this can confuse Purify.\" - SGI comment\n+ *\n+ *  This adaptor is \"SGI\" style.  The _Alloc parameter must also be \"SGI\".\n+ *  @endif\n+ *  (See @link Allocators allocators info @endlink for more.)\n+*/\n+template <class _Alloc>\n   class __debug_alloc\n+{\n+private:\n+  enum {_S_extra = 8};  // Size of space used to store size.  Note that this\n+                        // must be large enough to preserve alignment.\n+\n+public:\n+  static void*\n+  allocate(size_t __n)\n   {\n-  private:\n-    enum {_S_extra = 8};  // Size of space used to store size.  Note that this\n-                          // must be large enough to preserve alignment.\n-  \n-  public:\n-  \n-    static void* allocate(size_t __n)\n-    {\n-      char* __result = (char*)_Alloc::allocate(__n + (int) _S_extra);\n-      *(size_t*)__result = __n;\n-      return __result + (int) _S_extra;\n-    }\n-  \n-    static void deallocate(void* __p, size_t __n)\n-    {\n-      char* __real_p = (char*)__p - (int) _S_extra;\n-      assert(*(size_t*)__real_p == __n);\n-      _Alloc::deallocate(__real_p, __n + (int) _S_extra);\n-    }\n-  \n-    static void* reallocate(void* __p, size_t __old_sz, size_t __new_sz)\n-    {\n-      char* __real_p = (char*)__p - (int) _S_extra;\n-      assert(*(size_t*)__real_p == __old_sz);\n-      char* __result = (char*)\n-        _Alloc::reallocate(__real_p, __old_sz + (int) _S_extra,\n-                                     __new_sz + (int) _S_extra);\n-      *(size_t*)__result = __new_sz;\n-      return __result + (int) _S_extra;\n-    }\n-  };\n+    char* __result = (char*)_Alloc::allocate(__n + (int) _S_extra);\n+    *(size_t*)__result = __n;\n+    return __result + (int) _S_extra;\n+  }\n+\n+  static void\n+  deallocate(void* __p, size_t __n)\n+  {\n+    char* __real_p = (char*)__p - (int) _S_extra;\n+    assert(*(size_t*)__real_p == __n);\n+    _Alloc::deallocate(__real_p, __n + (int) _S_extra);\n+  }\n+\n+  static void*\n+  reallocate(void* __p, size_t __old_sz, size_t __new_sz)\n+  {\n+    char* __real_p = (char*)__p - (int) _S_extra;\n+    assert(*(size_t*)__real_p == __old_sz);\n+    char* __result = (char*)\n+      _Alloc::reallocate(__real_p, __old_sz + (int) _S_extra,\n+                                   __new_sz + (int) _S_extra);\n+    *(size_t*)__result = __new_sz;\n+    return __result + (int) _S_extra;\n+  }\n+};\n \n \n #ifdef __USE_MALLOC\n@@ -297,15 +306,15 @@ typedef __mem_interface __single_client_alloc;\n  *  Default node allocator.  \"SGI\" style.  Uses __mem_interface for its\n  *  underlying requests (and makes as few requests as possible).\n  *  **** Currently __mem_interface is always __new_alloc, never __malloc*.\n- * \n+ *\n  *  Important implementation properties:\n  *  1. If the clients request an object of size > _MAX_BYTES, the resulting\n  *     object will be obtained directly from the underlying __mem_interface.\n  *  2. In all other cases, we allocate an object of size exactly\n  *     _S_round_up(requested_size).  Thus the client has enough size\n  *     information that we can return the object to the proper free list\n  *     without permanently losing part of the object.\n- * \n+ *\n  *  The first template parameter specifies whether more than one thread may\n  *  use this allocator.  It is safe to allocate an object from one instance\n  *  of a default_alloc and deallocate it with another one.  This effectively\n@@ -323,272 +332,273 @@ typedef __mem_interface __single_client_alloc;\n */\n template<bool __threads, int __inst>\n   class __default_alloc_template\n+{\n+private:\n+  enum {_ALIGN = 8};\n+  enum {_MAX_BYTES = 128};\n+  enum {_NFREELISTS = _MAX_BYTES / _ALIGN};\n+\n+  union _Obj\n   {\n-  private:\n-    enum {_ALIGN = 8};\n-    enum {_MAX_BYTES = 128};\n-    enum {_NFREELISTS = _MAX_BYTES / _ALIGN};\n-    \n-    union _Obj \n-    {\n-      union _Obj* _M_free_list_link;\n-      char        _M_client_data[1];    // The client sees this.\n-    };\n+    union _Obj* _M_free_list_link;\n+    char        _M_client_data[1];    // The client sees this.\n+  };\n+\n+  static _Obj* volatile         _S_free_list[_NFREELISTS];\n \n-    static _Obj* volatile \t_S_free_list[_NFREELISTS]; \n+  // Chunk allocation state.\n+  static char*                  _S_start_free;\n+  static char*                  _S_end_free;\n+  static size_t                 _S_heap_size;\n \n-    // Chunk allocation state.\n-    static char* \t\t_S_start_free;\n-    static char* \t\t_S_end_free;\n-    static size_t \t\t_S_heap_size;\n-    \n-    static _STL_mutex_lock \t_S_node_allocator_lock;\n+  static _STL_mutex_lock        _S_node_allocator_lock;\n \n-    static size_t\n-    _S_round_up(size_t __bytes) \n+  static size_t\n+  _S_round_up(size_t __bytes)\n     { return (((__bytes) + (size_t) _ALIGN-1) & ~((size_t) _ALIGN - 1)); }\n \n-    static size_t \n-    _S_freelist_index(size_t __bytes)\n+  static size_t\n+  _S_freelist_index(size_t __bytes)\n     { return (((__bytes) + (size_t)_ALIGN-1)/(size_t)_ALIGN - 1); }\n \n-    // Returns an object of size __n, and optionally adds to size __n\n-    // free list.\n-    static void* \n-    _S_refill(size_t __n);\n-\n-    // Allocates a chunk for nobjs of size size.  nobjs may be reduced\n-    // if it is inconvenient to allocate the requested number.\n-    static char* \n-    _S_chunk_alloc(size_t __size, int& __nobjs);\n-    \n-    // It would be nice to use _STL_auto_lock here.  But we need a\n-    // test whether threads are in use.\n-    class _Lock \n-    {\n-    public:\n-      _Lock() { if (__threads) _S_node_allocator_lock._M_acquire_lock(); }\n-      ~_Lock() { if (__threads) _S_node_allocator_lock._M_release_lock(); }\n-    } __attribute__ ((__unused__));\n-    friend class _Lock;\n-    \n-  public:\n-    // __n must be > 0\n-    static void* \n-    allocate(size_t __n)\n-    {\n-      void* __ret = 0;\n-      \n-      if (__n > (size_t) _MAX_BYTES) \n-\t__ret = __mem_interface::allocate(__n);\n-      else \n-\t{\n-\t  _Obj* volatile* __my_free_list = _S_free_list \n-\t    + _S_freelist_index(__n);\n-\t  // Acquire the lock here with a constructor call.  This\n-\t  // ensures that it is released in exit or during stack\n-\t  // unwinding.\n-\t  _Lock __lock_instance;\n-\t  _Obj* __restrict__ __result = *__my_free_list;\n-\t  if (__result == 0)\n-\t    __ret = _S_refill(_S_round_up(__n));\n-\t  else \n-\t    {\n-\t      *__my_free_list = __result -> _M_free_list_link;\n-\t      __ret = __result;\n-\t    }\n-\t}\n-      return __ret;\n-    };\n-\n-    // __p may not be 0\n-    static void \n-    deallocate(void* __p, size_t __n)\n+  // Returns an object of size __n, and optionally adds to size __n\n+  // free list.\n+  static void*\n+  _S_refill(size_t __n);\n+\n+  // Allocates a chunk for nobjs of size size.  nobjs may be reduced\n+  // if it is inconvenient to allocate the requested number.\n+  static char*\n+  _S_chunk_alloc(size_t __size, int& __nobjs);\n+\n+  // It would be nice to use _STL_auto_lock here.  But we need a\n+  // test whether threads are in use.\n+  struct _Lock\n+  {\n+    _Lock() { if (__threads) _S_node_allocator_lock._M_acquire_lock(); }\n+    ~_Lock() { if (__threads) _S_node_allocator_lock._M_release_lock(); }\n+  } __attribute__ ((__unused__));\n+  friend struct _Lock;\n+\n+public:\n+  // __n must be > 0\n+  static void*\n+  allocate(size_t __n)\n+  {\n+    void* __ret = 0;\n+\n+    if (__n > (size_t) _MAX_BYTES)\n+      __ret = __mem_interface::allocate(__n);\n+    else\n     {\n-      if (__n > (size_t) _MAX_BYTES)\n-\t__mem_interface::deallocate(__p, __n);\n-      else \n-\t{\n-\t  _Obj* volatile*  __my_free_list\n-\t    = _S_free_list + _S_freelist_index(__n);\n-\t  _Obj* __q = (_Obj*)__p;\n-\t  \n-\t  // Acquire the lock here with a constructor call.  This ensures that\n-\t  // it is released in exit or during stack unwinding.\n-\t  _Lock __lock_instance;\n-\t  __q -> _M_free_list_link = *__my_free_list;\n-\t  *__my_free_list = __q;\n-\t}\n+      _Obj* volatile* __my_free_list = _S_free_list + _S_freelist_index(__n);\n+      // Acquire the lock here with a constructor call.  This ensures that\n+      // it is released in exit or during stack unwinding.\n+      _Lock __lock_instance;\n+      _Obj* __restrict__ __result = *__my_free_list;\n+      if (__result == 0)\n+        __ret = _S_refill(_S_round_up(__n));\n+      else\n+        {\n+          *__my_free_list = __result -> _M_free_list_link;\n+          __ret = __result;\n+        }\n     }\n-    \n-    static void* \n-    reallocate(void* __p, size_t __old_sz, size_t __new_sz);\n+    return __ret;\n   };\n \n+  // __p may not be 0\n+  static void\n+  deallocate(void* __p, size_t __n)\n+  {\n+    if (__n > (size_t) _MAX_BYTES)\n+      __mem_interface::deallocate(__p, __n);\n+    else\n+    {\n+      _Obj* volatile*  __my_free_list = _S_free_list + _S_freelist_index(__n);\n+      _Obj* __q = (_Obj*)__p;\n+\n+      // Acquire the lock here with a constructor call.  This ensures that\n+      // it is released in exit or during stack unwinding.\n+      _Lock __lock_instance;\n+      __q -> _M_free_list_link = *__my_free_list;\n+      *__my_free_list = __q;\n+    }\n+  }\n \n-  template<bool __threads, int __inst>\n-    inline bool \n-    operator==(const __default_alloc_template<__threads, __inst>&,\n-\t       const __default_alloc_template<__threads, __inst>&)\n-    { return true; }\n+  static void*\n+  reallocate(void* __p, size_t __old_sz, size_t __new_sz);\n+};\n \n-  template<bool __threads, int __inst>\n-    inline bool \n-    operator!=(const __default_alloc_template<__threads, __inst>&,\n-\t       const __default_alloc_template<__threads, __inst>&)\n-    { return false; }\n \n+template<bool __threads, int __inst>\n+  inline bool\n+  operator==(const __default_alloc_template<__threads,__inst>&,\n+             const __default_alloc_template<__threads,__inst>&)\n+  { return true; }\n \n-  // We allocate memory in large chunks in order to avoid fragmenting the\n-  // malloc heap (or whatever __mem_interface is using) too much.  We assume\n-  // that __size is properly aligned.  We hold the allocation lock.\n-  template<bool __threads, int __inst>\n-    char*\n-    __default_alloc_template<__threads, __inst>::_S_chunk_alloc(size_t __size, \n-\t\t\t\t\t\t\t\tint& __nobjs)\n-    {\n-      char* __result;\n-      size_t __total_bytes = __size * __nobjs;\n-      size_t __bytes_left = _S_end_free - _S_start_free;\n-      \n-      if (__bytes_left >= __total_bytes) \n+template<bool __threads, int __inst>\n+  inline bool\n+  operator!=(const __default_alloc_template<__threads,__inst>&,\n+             const __default_alloc_template<__threads,__inst>&)\n+  { return false; }\n+\n+\n+// We allocate memory in large chunks in order to avoid fragmenting the\n+// malloc heap (or whatever __mem_interface is using) too much.  We assume\n+// that __size is properly aligned.  We hold the allocation lock.\n+template<bool __threads, int __inst>\n+  char*\n+  __default_alloc_template<__threads, __inst>::\n+  _S_chunk_alloc(size_t __size, int& __nobjs)\n+  {\n+    char* __result;\n+    size_t __total_bytes = __size * __nobjs;\n+    size_t __bytes_left = _S_end_free - _S_start_free;\n+\n+    if (__bytes_left >= __total_bytes)\n       {\n         __result = _S_start_free;\n         _S_start_free += __total_bytes;\n         return(__result);\n-      } \n-      else if (__bytes_left >= __size) \n-\t{\n-\t  __nobjs = (int)(__bytes_left/__size);\n-\t  __total_bytes = __size * __nobjs;\n-\t  __result = _S_start_free;\n-\t  _S_start_free += __total_bytes;\n-\t  return(__result);\n-\t} \n-      else \n-\t{\n-\t  size_t __bytes_to_get = \n-\t    2 * __total_bytes + _S_round_up(_S_heap_size >> 4);\n-\t  // Try to make use of the left-over piece.\n-\t  if (__bytes_left > 0) \n-\t    {\n-\t      _Obj* volatile* __my_free_list =\n-\t\t_S_free_list + _S_freelist_index(__bytes_left);\n-\t      \n-\t      ((_Obj*)_S_start_free) -> _M_free_list_link = *__my_free_list;\n-\t      *__my_free_list = (_Obj*)_S_start_free;\n-\t    }\n-\t  _S_start_free = (char*) __mem_interface::allocate(__bytes_to_get);\n-\t  if (0 == _S_start_free) \n-\t    {\n-\t      size_t __i;\n-\t      _Obj* volatile* __my_free_list;\n-\t      _Obj* __p;\n-\t      // Try to make do with what we have.  That can't hurt.  We\n-\t      // do not try smaller requests, since that tends to result\n-\t      // in disaster on multi-process machines.\n-\t      __i = __size;\n-\t      for (; __i <= (size_t) _MAX_BYTES; __i += (size_t) _ALIGN) \n-\t\t{\n-\t\t  __my_free_list = _S_free_list + _S_freelist_index(__i);\n-\t\t  __p = *__my_free_list;\n-\t\t  if (0 != __p) \n-\t\t    {\n-\t\t      *__my_free_list = __p -> _M_free_list_link;\n-\t\t      _S_start_free = (char*)__p;\n-\t\t      _S_end_free = _S_start_free + __i;\n-\t\t      return(_S_chunk_alloc(__size, __nobjs));\n-\t\t      // Any leftover piece will eventually make it to the\n-\t\t      // right free list.\n-\t\t    }\n-\t\t}\n-\t      _S_end_free = 0;\t// In case of exception.\n-\t      _S_start_free = (char*)__mem_interface::allocate(__bytes_to_get);\n-\t      // This should either throw an exception or remedy the situation.\n-\t      // Thus we assume it succeeded.\n-\t    }\n-\t  _S_heap_size += __bytes_to_get;\n-\t  _S_end_free = _S_start_free + __bytes_to_get;\n-\t  return(_S_chunk_alloc(__size, __nobjs));\n-\t}\n-    }\n-  \n-  \n-  // Returns an object of size __n, and optionally adds to \"size\n-  // __n\"'s free list.  We assume that __n is properly aligned.  We\n-  // hold the allocation lock.\n-  template<bool __threads, int __inst>\n-    void*\n-    __default_alloc_template<__threads, __inst>::_S_refill(size_t __n)\n-    {\n-      int __nobjs = 20;\n-      char* __chunk = _S_chunk_alloc(__n, __nobjs);\n-      _Obj* volatile* __my_free_list;\n-      _Obj* __result;\n-      _Obj* __current_obj;\n-      _Obj* __next_obj;\n-      int __i;\n-      \n-      if (1 == __nobjs) return(__chunk);\n-      __my_free_list = _S_free_list + _S_freelist_index(__n);\n-      \n-      /* Build free list in chunk */\n-      __result = (_Obj*)__chunk;\n-      *__my_free_list = __next_obj = (_Obj*)(__chunk + __n);\n-      for (__i = 1; ; __i++) {\n-        __current_obj = __next_obj;\n-        __next_obj = (_Obj*)((char*)__next_obj + __n);\n-        if (__nobjs - 1 == __i) {\n-\t  __current_obj -> _M_free_list_link = 0;\n-\t  break;\n-        } else {\n-\t  __current_obj -> _M_free_list_link = __next_obj;\n-        }\n       }\n-      return(__result);\n-    }\n+    else if (__bytes_left >= __size)\n+      {\n+        __nobjs = (int)(__bytes_left/__size);\n+        __total_bytes = __size * __nobjs;\n+        __result = _S_start_free;\n+        _S_start_free += __total_bytes;\n+        return(__result);\n+      }\n+    else\n+      {\n+        size_t __bytes_to_get =\n+          2 * __total_bytes + _S_round_up(_S_heap_size >> 4);\n+        // Try to make use of the left-over piece.\n+        if (__bytes_left > 0)\n+          {\n+            _Obj* volatile* __my_free_list =\n+              _S_free_list + _S_freelist_index(__bytes_left);\n+\n+            ((_Obj*)_S_start_free) -> _M_free_list_link = *__my_free_list;\n+            *__my_free_list = (_Obj*)_S_start_free;\n+          }\n+        _S_start_free = (char*) __mem_interface::allocate(__bytes_to_get);\n+        if (0 == _S_start_free)\n+          {\n+            size_t __i;\n+            _Obj* volatile* __my_free_list;\n+            _Obj* __p;\n+            // Try to make do with what we have.  That can't hurt.  We\n+            // do not try smaller requests, since that tends to result\n+            // in disaster on multi-process machines.\n+            __i = __size;\n+            for (; __i <= (size_t) _MAX_BYTES; __i += (size_t) _ALIGN)\n+              {\n+                __my_free_list = _S_free_list + _S_freelist_index(__i);\n+                __p = *__my_free_list;\n+                if (0 != __p)\n+                  {\n+                    *__my_free_list = __p -> _M_free_list_link;\n+                    _S_start_free = (char*)__p;\n+                    _S_end_free = _S_start_free + __i;\n+                    return(_S_chunk_alloc(__size, __nobjs));\n+                    // Any leftover piece will eventually make it to the\n+                    // right free list.\n+                  }\n+              }\n+            _S_end_free = 0;        // In case of exception.\n+            _S_start_free = (char*)__mem_interface::allocate(__bytes_to_get);\n+            // This should either throw an exception or remedy the situation.\n+            // Thus we assume it succeeded.\n+          }\n+        _S_heap_size += __bytes_to_get;\n+        _S_end_free = _S_start_free + __bytes_to_get;\n+        return(_S_chunk_alloc(__size, __nobjs));\n+      }\n+  }\n \n \n-  template<bool threads, int inst>\n-    void*\n-    __default_alloc_template<threads, inst>::reallocate(void* __p, \n-\t\t\t\t\t\t\tsize_t __old_sz,\n-\t\t\t\t\t\t\tsize_t __new_sz)\n+// Returns an object of size __n, and optionally adds to \"size\n+// __n\"'s free list.  We assume that __n is properly aligned.  We\n+// hold the allocation lock.\n+template<bool __threads, int __inst>\n+  void*\n+  __default_alloc_template<__threads, __inst>::\n+  _S_refill(size_t __n)\n+  {\n+    int __nobjs = 20;\n+    char* __chunk = _S_chunk_alloc(__n, __nobjs);\n+    _Obj* volatile* __my_free_list;\n+    _Obj* __result;\n+    _Obj* __current_obj;\n+    _Obj* __next_obj;\n+    int __i;\n+\n+    if (1 == __nobjs)\n+      return(__chunk);\n+    __my_free_list = _S_free_list + _S_freelist_index(__n);\n+\n+    /* Build free list in chunk */\n+    __result = (_Obj*)__chunk;\n+    *__my_free_list = __next_obj = (_Obj*)(__chunk + __n);\n+    for (__i = 1; ; __i++)\n     {\n-      void* __result;\n-      size_t __copy_sz;\n-      \n-      if (__old_sz > (size_t) _MAX_BYTES && __new_sz > (size_t) _MAX_BYTES) {\n-        return(realloc(__p, __new_sz));\n+      __current_obj = __next_obj;\n+      __next_obj = (_Obj*)((char*)__next_obj + __n);\n+      if (__nobjs - 1 == __i)\n+      {\n+        __current_obj -> _M_free_list_link = 0;\n+        break;\n+      }\n+      else\n+      {\n+        __current_obj -> _M_free_list_link = __next_obj;\n       }\n-      if (_S_round_up(__old_sz) == _S_round_up(__new_sz)) return(__p);\n-      __result = allocate(__new_sz);\n-      __copy_sz = __new_sz > __old_sz? __old_sz : __new_sz;\n-      memcpy(__result, __p, __copy_sz);\n-      deallocate(__p, __old_sz);\n-      return(__result);\n     }\n-  \n-  template<bool __threads, int __inst>\n-  _STL_mutex_lock\n-  __default_alloc_template<__threads, __inst>::_S_node_allocator_lock\n-  __STL_MUTEX_INITIALIZER;\n-  \n-  template<bool __threads, int __inst>\n-  char* __default_alloc_template<__threads, __inst>::_S_start_free = 0;\n-  \n-  template<bool __threads, int __inst>\n-  char* __default_alloc_template<__threads, __inst>::_S_end_free = 0;\n-  \n-  template<bool __threads, int __inst>\n-  size_t __default_alloc_template<__threads, __inst>::_S_heap_size = 0;\n-  \n-  template<bool __threads, int __inst>\n-  typename __default_alloc_template<__threads, __inst>::_Obj* volatile\n-  __default_alloc_template<__threads, __inst>::_S_free_list[_NFREELISTS];\n-  \n-  typedef __default_alloc_template<true, 0>    __alloc;\n-  typedef __default_alloc_template<false, 0>   __single_client_alloc;\n+    return(__result);\n+  }\n+\n+\n+template<bool threads, int inst>\n+  void*\n+  __default_alloc_template<threads, inst>::\n+  reallocate(void* __p, size_t __old_sz, size_t __new_sz)\n+  {\n+    void* __result;\n+    size_t __copy_sz;\n+\n+    if (__old_sz > (size_t) _MAX_BYTES && __new_sz > (size_t) _MAX_BYTES)\n+      return(realloc(__p, __new_sz));\n+    if (_S_round_up(__old_sz) == _S_round_up(__new_sz))\n+      return(__p);\n+    __result = allocate(__new_sz);\n+    __copy_sz = __new_sz > __old_sz? __old_sz : __new_sz;\n+    memcpy(__result, __p, __copy_sz);\n+    deallocate(__p, __old_sz);\n+    return(__result);\n+  }\n+\n+template<bool __threads, int __inst>\n+_STL_mutex_lock\n+__default_alloc_template<__threads,__inst>::_S_node_allocator_lock\n+__STL_MUTEX_INITIALIZER;\n+\n+template<bool __threads, int __inst>\n+char* __default_alloc_template<__threads,__inst>::_S_start_free = 0;\n+\n+template<bool __threads, int __inst>\n+char* __default_alloc_template<__threads,__inst>::_S_end_free = 0;\n+\n+template<bool __threads, int __inst>\n+size_t __default_alloc_template<__threads,__inst>::_S_heap_size = 0;\n+\n+template<bool __threads, int __inst>\n+typename __default_alloc_template<__threads,__inst>::_Obj* volatile\n+__default_alloc_template<__threads,__inst>::_S_free_list[_NFREELISTS];\n+\n+typedef __default_alloc_template<true,0>    __alloc;\n+typedef __default_alloc_template<false,0>   __single_client_alloc;\n \n \n #endif /* ! __USE_MALLOC */\n@@ -637,24 +647,28 @@ class allocator\n \n   // __n is permitted to be 0.  The C++ standard says nothing about what\n   // the return value is when __n == 0.\n-  _Tp* allocate(size_type __n, const void* = 0) {\n-    return __n != 0 ? static_cast<_Tp*>(_Alloc::allocate(__n * sizeof(_Tp))) \n+  _Tp*\n+  allocate(size_type __n, const void* = 0)\n+  {\n+    return __n != 0 ? static_cast<_Tp*>(_Alloc::allocate(__n * sizeof(_Tp)))\n                     : 0;\n   }\n \n   // __p is not permitted to be a null pointer.\n-  void deallocate(pointer __p, size_type __n)\n+  void\n+  deallocate(pointer __p, size_type __n)\n     { _Alloc::deallocate(__p, __n * sizeof(_Tp)); }\n \n-  size_type max_size() const throw() \n-    { return size_t(-1) / sizeof(_Tp); }\n+  size_type\n+  max_size() const throw() { return size_t(-1) / sizeof(_Tp); }\n \n   void construct(pointer __p, const _Tp& __val) { new(__p) _Tp(__val); }\n   void destroy(pointer __p) { __p->~_Tp(); }\n };\n \n template<>\n-class allocator<void> {\n+class allocator<void>\n+{\n public:\n   typedef size_t      size_type;\n   typedef ptrdiff_t   difference_type;\n@@ -669,16 +683,14 @@ class allocator<void> {\n \n \n template <class _T1, class _T2>\n-inline bool operator==(const allocator<_T1>&, const allocator<_T2>&) \n-{\n-  return true;\n-}\n+  inline bool\n+  operator==(const allocator<_T1>&, const allocator<_T2>&)\n+  { return true; }\n \n template <class _T1, class _T2>\n-inline bool operator!=(const allocator<_T1>&, const allocator<_T2>&)\n-{\n-  return false;\n-}\n+  inline bool\n+  operator!=(const allocator<_T1>&, const allocator<_T2>&)\n+  { return false; }\n \n \n /**\n@@ -693,7 +705,7 @@ inline bool operator!=(const allocator<_T1>&, const allocator<_T2>&)\n  *  (See @link Allocators allocators info @endlink for more.)\n */\n template <class _Tp, class _Alloc>\n-struct __allocator\n+  struct __allocator\n {\n   _Alloc __underlying_alloc;\n \n@@ -712,7 +724,7 @@ struct __allocator\n   __allocator() throw() {}\n   __allocator(const __allocator& __a) throw()\n     : __underlying_alloc(__a.__underlying_alloc) {}\n-  template <class _Tp1> \n+  template <class _Tp1>\n   __allocator(const __allocator<_Tp1, _Alloc>& __a) throw()\n     : __underlying_alloc(__a.__underlying_alloc) {}\n   ~__allocator() throw() {}\n@@ -721,25 +733,29 @@ struct __allocator\n   const_pointer address(const_reference __x) const { return &__x; }\n \n   // __n is permitted to be 0.\n-  _Tp* allocate(size_type __n, const void* = 0) {\n-    return __n != 0 \n-        ? static_cast<_Tp*>(__underlying_alloc.allocate(__n * sizeof(_Tp))) \n+  _Tp*\n+  allocate(size_type __n, const void* = 0)\n+  {\n+    return __n != 0\n+        ? static_cast<_Tp*>(__underlying_alloc.allocate(__n * sizeof(_Tp)))\n         : 0;\n   }\n \n   // __p is not permitted to be a null pointer.\n-  void deallocate(pointer __p, size_type __n)\n+  void\n+  deallocate(pointer __p, size_type __n)\n     { __underlying_alloc.deallocate(__p, __n * sizeof(_Tp)); }\n \n-  size_type max_size() const throw() \n-    { return size_t(-1) / sizeof(_Tp); }\n+  size_type\n+  max_size() const throw() { return size_t(-1) / sizeof(_Tp); }\n \n   void construct(pointer __p, const _Tp& __val) { new(__p) _Tp(__val); }\n   void destroy(pointer __p) { __p->~_Tp(); }\n };\n \n template <class _Alloc>\n-class __allocator<void, _Alloc> {\n+class __allocator<void, _Alloc>\n+{\n   typedef size_t      size_type;\n   typedef ptrdiff_t   difference_type;\n   typedef void*       pointer;\n@@ -752,18 +768,16 @@ class __allocator<void, _Alloc> {\n };\n \n template <class _Tp, class _Alloc>\n-inline bool operator==(const __allocator<_Tp, _Alloc>& __a1,\n-                       const __allocator<_Tp, _Alloc>& __a2)\n-{\n-  return __a1.__underlying_alloc == __a2.__underlying_alloc;\n-}\n+  inline bool\n+  operator==(const __allocator<_Tp,_Alloc>& __a1,\n+             const __allocator<_Tp,_Alloc>& __a2)\n+  { return __a1.__underlying_alloc == __a2.__underlying_alloc; }\n \n template <class _Tp, class _Alloc>\n-inline bool operator!=(const __allocator<_Tp, _Alloc>& __a1,\n-                       const __allocator<_Tp, _Alloc>& __a2)\n-{\n-  return __a1.__underlying_alloc != __a2.__underlying_alloc;\n-}\n+  inline bool\n+  operator!=(const __allocator<_Tp, _Alloc>& __a1,\n+                         const __allocator<_Tp, _Alloc>& __a2)\n+  { return __a1.__underlying_alloc != __a2.__underlying_alloc; }\n \n \n //@{\n@@ -772,30 +786,28 @@ inline bool operator!=(const __allocator<_Tp, _Alloc>& __a1,\n  *  correctly.  As required, all allocators compare equal.\n */\n template <int inst>\n-inline bool operator==(const __malloc_alloc_template<inst>&,\n-                       const __malloc_alloc_template<inst>&)\n-{\n-  return true;\n-}\n+  inline bool\n+  operator==(const __malloc_alloc_template<inst>&,\n+             const __malloc_alloc_template<inst>&)\n+  { return true; }\n \n template <int __inst>\n-inline bool operator!=(const __malloc_alloc_template<__inst>&,\n-                       const __malloc_alloc_template<__inst>&)\n-{\n-  return false;\n-}\n+  inline bool\n+  operator!=(const __malloc_alloc_template<__inst>&,\n+             const __malloc_alloc_template<__inst>&)\n+  { return false; }\n \n template <class _Alloc>\n-inline bool operator==(const __debug_alloc<_Alloc>&,\n-                       const __debug_alloc<_Alloc>&) {\n-  return true;\n-}\n+  inline bool\n+  operator==(const __debug_alloc<_Alloc>&,\n+             const __debug_alloc<_Alloc>&)\n+  { return true; }\n \n template <class _Alloc>\n-inline bool operator!=(const __debug_alloc<_Alloc>&,\n-                       const __debug_alloc<_Alloc>&) {\n-  return false;\n-}\n+  inline bool\n+  operator!=(const __debug_alloc<_Alloc>&,\n+             const __debug_alloc<_Alloc>&)\n+  { return false; }\n //@}\n \n \n@@ -872,9 +884,9 @@ template <class _Tp, bool __threads, int __inst>\n struct _Alloc_traits<_Tp, __default_alloc_template<__threads, __inst> >\n {\n   static const bool _S_instanceless = true;\n-  typedef __simple_alloc<_Tp, __default_alloc_template<__threads, __inst> > \n+  typedef __simple_alloc<_Tp, __default_alloc_template<__threads, __inst> >\n           _Alloc_type;\n-  typedef __allocator<_Tp, __default_alloc_template<__threads, __inst> > \n+  typedef __allocator<_Tp, __default_alloc_template<__threads, __inst> >\n           allocator_type;\n };\n #endif\n@@ -891,7 +903,7 @@ struct _Alloc_traits<_Tp, __debug_alloc<_Alloc> >\n //@{\n /// Versions for the __allocator adaptor used with the predefined \"SGI\" style allocators.\n template <class _Tp, class _Tp1, int __inst>\n-struct _Alloc_traits<_Tp, \n+struct _Alloc_traits<_Tp,\n                      __allocator<_Tp1, __malloc_alloc_template<__inst> > >\n {\n   static const bool _S_instanceless = true;\n@@ -901,14 +913,14 @@ struct _Alloc_traits<_Tp,\n \n #ifndef __USE_MALLOC\n template <class _Tp, class _Tp1, bool __thr, int __inst>\n-struct _Alloc_traits<_Tp, \n-                      __allocator<_Tp1, \n+struct _Alloc_traits<_Tp,\n+                      __allocator<_Tp1,\n                                   __default_alloc_template<__thr, __inst> > >\n {\n   static const bool _S_instanceless = true;\n-  typedef __simple_alloc<_Tp, __default_alloc_template<__thr,__inst> > \n+  typedef __simple_alloc<_Tp, __default_alloc_template<__thr,__inst> >\n           _Alloc_type;\n-  typedef __allocator<_Tp, __default_alloc_template<__thr,__inst> > \n+  typedef __allocator<_Tp, __default_alloc_template<__thr,__inst> >\n           allocator_type;\n };\n #endif\n@@ -922,20 +934,16 @@ struct _Alloc_traits<_Tp, __allocator<_Tp1, __debug_alloc<_Alloc> > >\n };\n //@}\n \n-  // Inhibit implicit instantiations for required instantiations,\n-  // which are defined via explicit instantiations elsewhere.  \n-  // NB: This syntax is a GNU extension.\n-  extern template class allocator<char>;\n-  extern template class allocator<wchar_t>;\n+// Inhibit implicit instantiations for required instantiations,\n+// which are defined via explicit instantiations elsewhere.\n+// NB: This syntax is a GNU extension.\n+extern template class allocator<char>;\n+extern template class allocator<wchar_t>;\n #ifdef __USE_MALLOC\n-  extern template class __malloc_alloc_template<0>;\n+extern template class __malloc_alloc_template<0>;\n #else\n-  extern template class __default_alloc_template<true, 0>;\n+extern template class __default_alloc_template<true,0>;\n #endif\n } // namespace std\n \n #endif /* __GLIBCPP_INTERNAL_ALLOC_H */\n-\n-// Local Variables:\n-// mode:C++\n-// End:"}]}