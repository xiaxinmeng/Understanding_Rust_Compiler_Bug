{"sha": "47a3dae13e864748d4184738555900f44643593d", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NDdhM2RhZTEzZTg2NDc0OGQ0MTg0NzM4NTU1OTAwZjQ0NjQzNTkzZA==", "commit": {"author": {"name": "Zdenek Dvorak", "email": "rakdver@atrey.karlin.mff.cuni.cz", "date": "2003-04-01T20:23:54Z"}, "committer": {"name": "Zdenek Dvorak", "email": "rakdver@gcc.gnu.org", "date": "2003-04-01T20:23:54Z"}, "message": "gcse.c (struct ls_expr): Added pattern_regs field.\n\n\t* gcse.c (struct ls_expr): Added pattern_regs field.\n\t(ldst_entry): Initialize it.\n\t(extract_mentioned_regs, extract_mentioned_regs_helper): New.\n\t(store_ops_ok): Use regs precomputed by them.\n\t(find_loads, store_killed_in_insn, load_kills_store): Change return\n\ttype to bool.\n\t(store_killed_before, store_killed_after): Take position of register\n\tset in account.\n\t(reg_set_info): Store position of the setter.\n\t(gcse_main): Enable store motion.\n\t(mems_conflict_for_gcse_p): Enable load motion of non-symbol mems.\n\t(pre_insert_copy_insn, update_ld_motion_stores, insert_store): Prevent rtl\n\tsharing.\n\t(simple_mem): Enable store motion of non-symbol mems.\n\t(regvec): Type changed.\n\t(LAST_AVAIL_CHECK_FAILURE): New.\n\t(compute_store_table_current_insn): New.\n\t(build_store_vectors): Computation of availability and anticipatability\n\tmoved ...\n\t(compute_store_table, find_moveable_store): ... here.\n\t(delete_store): Remove senseless comment.\n\t(store_motion): Reorganize.\n\nFrom-SVN: r65141", "tree": {"sha": "0db4fb2ef18fbbab125fcd45b4f2b01a576ea6c4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/0db4fb2ef18fbbab125fcd45b4f2b01a576ea6c4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/47a3dae13e864748d4184738555900f44643593d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/47a3dae13e864748d4184738555900f44643593d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/47a3dae13e864748d4184738555900f44643593d", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/47a3dae13e864748d4184738555900f44643593d/comments", "author": null, "committer": null, "parents": [{"sha": "63855aa6edce91dc134af8ad24ba4898bcf60f0e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/63855aa6edce91dc134af8ad24ba4898bcf60f0e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/63855aa6edce91dc134af8ad24ba4898bcf60f0e"}], "stats": {"total": 612, "additions": 408, "deletions": 204}, "files": [{"sha": "0de4ad0cad252191247a0e81c8e7e3439e424118", "filename": "gcc/ChangeLog", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/47a3dae13e864748d4184738555900f44643593d/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/47a3dae13e864748d4184738555900f44643593d/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=47a3dae13e864748d4184738555900f44643593d", "patch": "@@ -1,3 +1,28 @@\n+2003-04-01  Zdenek Dvorak  <rakdver@atrey.karlin.mff.cuni.cz>\n+\n+\t* gcse.c (struct ls_expr): Added pattern_regs field.\n+\t(ldst_entry): Initialize it.\n+\t(extract_mentioned_regs, extract_mentioned_regs_helper): New.\n+\t(store_ops_ok): Use regs precomputed by them.\n+\t(find_loads, store_killed_in_insn, load_kills_store): Change return\n+\ttype to bool.\n+\t(store_killed_before, store_killed_after): Take position of register\n+\tset in account.\n+\t(reg_set_info): Store position of the setter.\n+\t(gcse_main): Enable store motion.\n+\t(mems_conflict_for_gcse_p): Enable load motion of non-symbol mems.\n+\t(pre_insert_copy_insn, update_ld_motion_stores, insert_store): Prevent rtl\n+\tsharing.\n+\t(simple_mem): Enable store motion of non-symbol mems.\n+\t(regvec): Type changed.\n+\t(LAST_AVAIL_CHECK_FAILURE): New.\n+\t(compute_store_table_current_insn): New.\n+\t(build_store_vectors): Computation of availability and anticipatability\n+\tmoved ...\n+\t(compute_store_table, find_moveable_store): ... here.\n+\t(delete_store): Remove senseless comment.\n+\t(store_motion): Reorganize.\n+\n 2003-04-01  Kevin Buettner  <kevinb@redhat.com>\n \n \t* config/mips/mips.c (override_options): Provide mappings for"}, {"sha": "93f656f8804f4e6a5ba34c8c69962986735ea33d", "filename": "gcc/gcse.c", "status": "modified", "additions": 383, "deletions": 204, "changes": 587, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/47a3dae13e864748d4184738555900f44643593d/gcc%2Fgcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/47a3dae13e864748d4184738555900f44643593d/gcc%2Fgcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgcse.c?ref=47a3dae13e864748d4184738555900f44643593d", "patch": "@@ -470,6 +470,7 @@ struct ls_expr\n {\n   struct expr * expr;\t\t/* Gcse expression reference for LM.  */\n   rtx pattern;\t\t\t/* Pattern of this mem.  */\n+  rtx pattern_regs;\t\t/* List of registers mentioned by the mem.  */\n   rtx loads;\t\t\t/* INSN list of loads seen.  */\n   rtx stores;\t\t\t/* INSN list of stores seen.  */\n   struct ls_expr * next;\t/* Next in the list.  */\n@@ -687,14 +688,18 @@ static void compute_ld_motion_mems\tPARAMS ((void));\n static void trim_ld_motion_mems\t\tPARAMS ((void));\n static void update_ld_motion_stores\tPARAMS ((struct expr *));\n static void reg_set_info\t\tPARAMS ((rtx, rtx, void *));\n-static int store_ops_ok\t\t\tPARAMS ((rtx, basic_block));\n-static void find_moveable_store\t\tPARAMS ((rtx));\n+static bool store_ops_ok\t\tPARAMS ((rtx, int *));\n+static rtx extract_mentioned_regs\tPARAMS ((rtx));\n+static rtx extract_mentioned_regs_helper PARAMS ((rtx, rtx));\n+static void find_moveable_store\t\tPARAMS ((rtx, int *, int *));\n static int compute_store_table\t\tPARAMS ((void));\n-static int load_kills_store\t\tPARAMS ((rtx, rtx));\n-static int find_loads\t\t\tPARAMS ((rtx, rtx));\n-static int store_killed_in_insn\t\tPARAMS ((rtx, rtx));\n-static int store_killed_after\t\tPARAMS ((rtx, rtx, basic_block));\n-static int store_killed_before\t\tPARAMS ((rtx, rtx, basic_block));\n+static bool load_kills_store\t\tPARAMS ((rtx, rtx));\n+static bool find_loads\t\t\tPARAMS ((rtx, rtx));\n+static bool store_killed_in_insn\tPARAMS ((rtx, rtx, rtx));\n+static bool store_killed_after\t\tPARAMS ((rtx, rtx, rtx, basic_block,\n+\t\t\t\t\t\t int *, rtx *));\n+static bool store_killed_before\t\tPARAMS ((rtx, rtx, rtx, basic_block,\n+\t\t\t\t\t\t int *));\n static void build_store_vectors\t\tPARAMS ((void));\n static void insert_insn_start_bb\tPARAMS ((rtx, basic_block));\n static int insert_store\t\t\tPARAMS ((struct ls_expr *, edge));\n@@ -910,9 +915,9 @@ gcse_main (f, file)\n   end_alias_analysis ();\n   allocate_reg_info (max_reg_num (), FALSE, FALSE);\n \n-  /* Store motion disabled until it is fixed.  */\n-  if (0 && !optimize_size && flag_gcse_sm)\n+  if (!optimize_size && flag_gcse_sm)\n     store_motion ();\n+\n   /* Record where pseudo-registers are set.  */\n   return run_jump_opt_after_gcse;\n }\n@@ -1464,7 +1469,7 @@ mems_conflict_for_gcse_p (dest, setter, data)\n   /* If we are setting a MEM in our list of specially recognized MEMs,\n      don't mark as killed this time.  */\n \n-  if (dest == gcse_mem_operand && pre_ldst_mems != NULL)\n+  if (expr_equiv_p (dest, gcse_mem_operand) && pre_ldst_mems != NULL)\n     {\n       if (!find_rtx_in_ldst (dest))\n \tgcse_mems_conflict_p = 1;\n@@ -5438,7 +5443,7 @@ pre_insert_copy_insn (expr, insn)\n   if (!set)\n     abort ();\n \n-  new_insn = emit_insn_after (gen_move_insn (reg, SET_DEST (set)), insn);\n+  new_insn = emit_insn_after (gen_move_insn (reg, copy_rtx (SET_DEST (set))), insn);\n \n   /* Keep register set table up to date.  */\n   record_one_set (regno, new_insn);\n@@ -6515,6 +6520,7 @@ ldst_entry (x)\n       ptr->next         = pre_ldst_mems;\n       ptr->expr         = NULL;\n       ptr->pattern      = x;\n+      ptr->pattern_regs\t= NULL_RTX;\n       ptr->loads        = NULL_RTX;\n       ptr->stores       = NULL_RTX;\n       ptr->reaching_reg = NULL_RTX;\n@@ -6657,13 +6663,23 @@ simple_mem (x)\n   if (GET_MODE (x) == BLKmode)\n     return 0;\n \n-  if (flag_float_store && FLOAT_MODE_P (GET_MODE (x)))\n+  /* If we are handling exceptions, we must be careful with memory references\n+     that may trap. If we are not, the behavior is undefined, so we may just\n+     continue.  */\n+  if (flag_non_call_exceptions && may_trap_p (x))\n     return 0;\n \n-  if (!rtx_varies_p (XEXP (x, 0), 0))\n-    return 1;\n+  if (side_effects_p (x))\n+    return 0;\n \n-  return 0;\n+  /* Do not consider function arguments passed on stack.  */\n+  if (reg_mentioned_p (stack_pointer_rtx, x))\n+    return 0;\n+\n+  if (flag_float_store && FLOAT_MODE_P (GET_MODE (x)))\n+    return 0;\n+\n+  return 1;\n }\n \n /* Make sure there isn't a buried reference in this pattern anywhere.\n@@ -6876,7 +6892,7 @@ update_ld_motion_stores (expr)\n \t      fprintf (gcse_file, \"\\n\");\n \t    }\n \n-\t  copy = gen_move_insn ( reg, SET_SRC (pat));\n+\t  copy = gen_move_insn ( reg, copy_rtx (SET_SRC (pat)));\n \t  new = emit_insn_before (copy, insn);\n \t  record_one_set (REGNO (reg), new);\n \t  SET_SRC (pat) = reg;\n@@ -6890,9 +6906,16 @@ update_ld_motion_stores (expr)\n \f\n /* Store motion code.  */\n \n+#define ANTIC_STORE_LIST(x)\t\t((x)->loads)\n+#define AVAIL_STORE_LIST(x)\t\t((x)->stores)\n+#define LAST_AVAIL_CHECK_FAILURE(x)\t((x)->reaching_reg)\n+\n /* This is used to communicate the target bitvector we want to use in the\n    reg_set_info routine when called via the note_stores mechanism.  */\n-static sbitmap * regvec;\n+static int * regvec;\n+\n+/* And current insn, for the same routine.  */\n+static rtx compute_store_table_current_insn;\n \n /* Used in computing the reverse edge graph bit vectors.  */\n static sbitmap * st_antloc;\n@@ -6911,16 +6934,43 @@ reg_set_info (dest, setter, data)\n     dest = SUBREG_REG (dest);\n \n   if (GET_CODE (dest) == REG)\n-    SET_BIT (*regvec, REGNO (dest));\n+    regvec[REGNO (dest)] = INSN_UID (compute_store_table_current_insn);\n }\n \n-/* Return nonzero if the register operands of expression X are killed\n-   anywhere in basic block BB.  */\n+/* Return zero if some of the registers in list X are killed\n+   due to set of registers in bitmap REGS_SET.  */\n+  \n+static bool\n+store_ops_ok (x, regs_set)\n+     rtx x;\n+     int *regs_set;\n+{\n+  rtx reg;\n+\n+  for (; x; x = XEXP (x, 1))\n+    {\n+      reg = XEXP (x, 0);\n+      if (regs_set[REGNO(reg)])\n+\treturn false; \n+    }\n \n-static int\n-store_ops_ok (x, bb)\n+  return true;\n+}\n+\n+/* Returns a list of registers mentioned in X.  */\n+static rtx\n+extract_mentioned_regs (x)\n      rtx x;\n-     basic_block bb;\n+{\n+  return extract_mentioned_regs_helper (x, NULL_RTX);\n+}\n+\n+/* Helper for extract_mentioned_regs; ACCUM is used to accumulate used\n+   registers.  */\n+static rtx\n+extract_mentioned_regs_helper (x, accum)\n+     rtx x;\n+     rtx accum;\n {\n   int i;\n   enum rtx_code code;\n@@ -6930,15 +6980,13 @@ store_ops_ok (x, bb)\n  repeat:\n \n   if (x == 0)\n-    return 1;\n+    return accum;\n \n   code = GET_CODE (x);\n   switch (code)\n     {\n     case REG:\n-\t/* If a reg has changed after us in this\n-\t   block, the operand has been killed.  */\n-\treturn TEST_BIT (reg_set_in_block[bb->index], REGNO (x));\n+      return alloc_EXPR_LIST (0, x, accum);\n \n     case MEM:\n       x = XEXP (x, 0);\n@@ -6948,7 +6996,8 @@ store_ops_ok (x, bb)\n     case PRE_INC:\n     case POST_DEC:\n     case POST_INC:\n-      return 0;\n+      /* We do not run this function with arguments having side effects.  */\n+      abort ();\n \n     case PC:\n     case CC0: /*FIXME*/\n@@ -6960,7 +7009,7 @@ store_ops_ok (x, bb)\n     case LABEL_REF:\n     case ADDR_VEC:\n     case ADDR_DIFF_VEC:\n-      return 1;\n+      return accum;\n \n     default:\n       break;\n@@ -6976,88 +7025,216 @@ store_ops_ok (x, bb)\n \t  rtx tem = XEXP (x, i);\n \n \t  /* If we are about to do the last recursive call\n-\t     needed at this level, change it into iteration.\n-\t     This function is called enough to be worth it.  */\n+\t     needed at this level, change it into iteration.  */\n \t  if (i == 0)\n \t    {\n \t      x = tem;\n \t      goto repeat;\n \t    }\n \n-\t  if (! store_ops_ok (tem, bb))\n-\t    return 0;\n+\t  accum = extract_mentioned_regs_helper (tem, accum);\n \t}\n       else if (fmt[i] == 'E')\n \t{\n \t  int j;\n \n \t  for (j = 0; j < XVECLEN (x, i); j++)\n-\t    {\n-\t      if (! store_ops_ok (XVECEXP (x, i, j), bb))\n-\t\treturn 0;\n-\t    }\n+\t    accum = extract_mentioned_regs_helper (XVECEXP (x, i, j), accum);\n \t}\n     }\n \n-  return 1;\n+  return accum;\n }\n \n-/* Determine whether insn is MEM store pattern that we will consider moving.  */\n+/* Determine whether INSN is MEM store pattern that we will consider moving.\n+   REGS_SET_BEFORE is bitmap of registers set before (and including) the\n+   current insn, REGS_SET_AFTER is bitmap of registers set after (and\n+   including) the insn in this basic block.  We must be passing through BB from\n+   head to end, as we are using this fact to speed things up.\n+   \n+   The results are stored this way:\n+\n+   -- the first anticipatable expression is added into ANTIC_STORE_LIST\n+   -- if the processed expression is not anticipatable, NULL_RTX is added\n+      there instead, so that we can use it as indicator that no further\n+      expression of this type may be anticipatable\n+   -- if the expression is available, it is added as head of AVAIL_STORE_LIST;\n+      consequently, all of them but this head are dead and may be deleted.\n+   -- if the expression is not available, the insn due to that it fails to be\n+      available is stored in reaching_reg.\n+\n+   The things are complicated a bit by fact that there already may be stores\n+   to the same MEM from other blocks; also caller must take care of the\n+   neccessary cleanup of the temporary markers after end of the basic block.\n+   */\n \n static void\n-find_moveable_store (insn)\n+find_moveable_store (insn, regs_set_before, regs_set_after)\n      rtx insn;\n+     int *regs_set_before;\n+     int *regs_set_after;\n {\n   struct ls_expr * ptr;\n-  rtx dest = PATTERN (insn);\n+  rtx dest, set, tmp;\n+  int check_anticipatable, check_available;\n+  basic_block bb = BLOCK_FOR_INSN (insn);\n \n-  if (GET_CODE (dest) != SET\n-      || GET_CODE (SET_SRC (dest)) == ASM_OPERANDS)\n+  set = single_set (insn);\n+  if (!set)\n     return;\n \n-  dest = SET_DEST (dest);\n+  dest = SET_DEST (set);\n \n   if (GET_CODE (dest) != MEM || MEM_VOLATILE_P (dest)\n       || GET_MODE (dest) == BLKmode)\n     return;\n \n-  if (GET_CODE (XEXP (dest, 0)) != SYMBOL_REF)\n-      return;\n+  if (side_effects_p (dest))\n+    return;\n \n-  if (rtx_varies_p (XEXP (dest, 0), 0))\n+  /* If we are handling exceptions, we must be careful with memory references\n+     that may trap. If we are not, the behavior is undefined, so we may just\n+     continue.  */\n+  if (flag_exceptions && may_trap_p (dest))\n+    return;\n+    \n+  /* Do not consider MEMs that mention stack pointer; in the following\n+     we rely on that constant functions do not read memory, which of course\n+     does not include their arguments if passed on stack.\n+     \n+     Note that this is not quite correct -- we may use other registers\n+     to address stack.  See store_killed_in_insn for handling of this\n+     case.  */\n+  if (reg_mentioned_p (stack_pointer_rtx, dest))\n     return;\n \n   ptr = ldst_entry (dest);\n-  ptr->stores = alloc_INSN_LIST (insn, ptr->stores);\n-}\n+  if (!ptr->pattern_regs)\n+    ptr->pattern_regs = extract_mentioned_regs (dest);\n+\n+  /* Do not check for anticipatability if we either found one anticipatable\n+     store already, or tested for one and found out that it was killed.  */\n+  check_anticipatable = 0;\n+  if (!ANTIC_STORE_LIST (ptr))\n+    check_anticipatable = 1;\n+  else\n+    {\n+      tmp = XEXP (ANTIC_STORE_LIST (ptr), 0);\n+      if (tmp != NULL_RTX\n+\t  && BLOCK_FOR_INSN (tmp) != bb)\n+\tcheck_anticipatable = 1;\n+    }\n+  if (check_anticipatable)\n+    {\n+      if (store_killed_before (dest, ptr->pattern_regs, insn, bb, regs_set_before))\n+\ttmp = NULL_RTX;\n+      else\n+\ttmp = insn;\n+      ANTIC_STORE_LIST (ptr) = alloc_INSN_LIST (tmp,\n+\t\t\t\t\t\tANTIC_STORE_LIST (ptr));\n+    }\n \n-/* Perform store motion. Much like gcse, except we move expressions the\n-   other way by looking at the flowgraph in reverse.  */\n+  /* It is not neccessary to check whether store is available if we did\n+     it successfully before; if we failed before, do not bother to check\n+     until we reach the insn that caused us to fail.  */\n+  check_available = 0;\n+  if (!AVAIL_STORE_LIST (ptr))\n+    check_available = 1;\n+  else\n+    {\n+      tmp = XEXP (AVAIL_STORE_LIST (ptr), 0);\n+      if (BLOCK_FOR_INSN (tmp) != bb)\n+\tcheck_available = 1;\n+    }\n+  if (check_available)\n+    {\n+      /* Check that we have already reached the insn at that the check\n+\t failed last time.  */\n+      if (LAST_AVAIL_CHECK_FAILURE (ptr))\n+\t{\n+\t  for (tmp = bb->end;\n+\t       tmp != insn && tmp != LAST_AVAIL_CHECK_FAILURE (ptr);\n+\t       tmp = PREV_INSN (tmp))\n+\t    continue;\n+\t  if (tmp == insn)\n+\t    check_available = 0;\n+\t}\n+      else\n+\tcheck_available = store_killed_after (dest, ptr->pattern_regs, insn,\n+\t\t\t\t\t      bb, regs_set_after,\n+\t\t\t\t\t      &LAST_AVAIL_CHECK_FAILURE (ptr));\n+    }\n+  if (!check_available)\n+    AVAIL_STORE_LIST (ptr) = alloc_INSN_LIST (insn, AVAIL_STORE_LIST (ptr));\n+}\n+  \n+/* Find available and anticipatable stores.  */\n \n static int\n compute_store_table ()\n {\n   int ret;\n   basic_block bb;\n   unsigned regno;\n-  rtx insn, pat;\n+  rtx insn, pat, tmp;\n+  int *last_set_in, *already_set;\n+  struct ls_expr * ptr, **prev_next_ptr_ptr;\n \n   max_gcse_regno = max_reg_num ();\n \n   reg_set_in_block = (sbitmap *) sbitmap_vector_alloc (last_basic_block,\n \t\t\t\t\t\t       max_gcse_regno);\n   sbitmap_vector_zero (reg_set_in_block, last_basic_block);\n   pre_ldst_mems = 0;\n+  last_set_in = xmalloc (sizeof (int) * max_gcse_regno);\n+  already_set = xmalloc (sizeof (int) * max_gcse_regno);\n \n   /* Find all the stores we care about.  */\n   FOR_EACH_BB (bb)\n     {\n-      regvec = & (reg_set_in_block[bb->index]);\n-      for (insn = bb->end;\n-\t   insn && insn != PREV_INSN (bb->end);\n-\t   insn = PREV_INSN (insn))\n+      /* First compute the registers set in this block.  */\n+      memset (last_set_in, 0, sizeof (int) * max_gcse_regno);\n+      regvec = last_set_in;\n+\n+      for (insn = bb->head;\n+\t   insn != NEXT_INSN (bb->end);\n+\t   insn = NEXT_INSN (insn))\n+\t{\n+\t  if (! INSN_P (insn))\n+\t    continue;\n+\n+\t  if (GET_CODE (insn) == CALL_INSN)\n+\t    {\n+\t      bool clobbers_all = false;\n+#ifdef NON_SAVING_SETJMP\n+\t      if (NON_SAVING_SETJMP\n+\t\t  && find_reg_note (insn, REG_SETJMP, NULL_RTX))\n+\t\tclobbers_all = true;\n+#endif\n+\n+\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n+\t\tif (clobbers_all\n+\t\t    || TEST_HARD_REG_BIT (regs_invalidated_by_call, regno))\n+\t\t  last_set_in[regno] = INSN_UID (insn);\n+\t    }\n+\n+\t  pat = PATTERN (insn);\n+\t  compute_store_table_current_insn = insn;\n+\t  note_stores (pat, reg_set_info, NULL);\n+\t}\n+\n+      /* Record the set registers.  */\n+      for (regno = 0; regno < max_gcse_regno; regno++)\n+\tif (last_set_in[regno])\n+\t  SET_BIT (reg_set_in_block[bb->index], regno);\n+\n+      /* Now find the stores.  */\n+      memset (already_set, 0, sizeof (int) * max_gcse_regno);\n+      regvec = already_set;\n+      for (insn = bb->head;\n+\t   insn != NEXT_INSN (bb->end);\n+\t   insn = NEXT_INSN (insn))\n \t{\n-\t  /* Ignore anything that is not a normal insn.  */\n \t  if (! INSN_P (insn))\n \t    continue;\n \n@@ -7073,61 +7250,91 @@ compute_store_table ()\n \t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n \t\tif (clobbers_all\n \t\t    || TEST_HARD_REG_BIT (regs_invalidated_by_call, regno))\n-\t\t  SET_BIT (reg_set_in_block[bb->index], regno);\n+\t\t  already_set[regno] = 1;\n \t    }\n \n \t  pat = PATTERN (insn);\n \t  note_stores (pat, reg_set_info, NULL);\n \n \t  /* Now that we've marked regs, look for stores.  */\n-\t  if (GET_CODE (pat) == SET)\n-\t    find_moveable_store (insn);\n+\t  find_moveable_store (insn, already_set, last_set_in);\n+\n+\t  /* Unmark regs that are no longer set.  */\n+\t  for (regno = 0; regno < max_gcse_regno; regno++)\n+\t    if (last_set_in[regno] == INSN_UID (insn))\n+\t      last_set_in[regno] = 0;\n+\t}\n+\n+      /* Clear temporary marks.  */\n+      for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n+\t{\n+\t  LAST_AVAIL_CHECK_FAILURE(ptr) = NULL_RTX;\n+\t  if (ANTIC_STORE_LIST (ptr)\n+\t      && (tmp = XEXP (ANTIC_STORE_LIST (ptr), 0)) == NULL_RTX)\n+\t    ANTIC_STORE_LIST (ptr) = XEXP (ANTIC_STORE_LIST (ptr), 1);\n+\t}\n+    }\n+\n+  /* Remove the stores that are not available anywhere, as there will\n+     be no opportunity to optimize them.  */\n+  for (ptr = pre_ldst_mems, prev_next_ptr_ptr = &pre_ldst_mems;\n+       ptr != NULL;\n+       ptr = *prev_next_ptr_ptr)\n+    {\n+      if (!AVAIL_STORE_LIST (ptr))\n+\t{\n+\t  *prev_next_ptr_ptr = ptr->next;\n+\t  free_ldst_entry (ptr);\n \t}\n+      else\n+\tprev_next_ptr_ptr = &ptr->next;\n     }\n \n   ret = enumerate_ldsts ();\n \n   if (gcse_file)\n     {\n-      fprintf (gcse_file, \"Store Motion Expressions.\\n\");\n+      fprintf (gcse_file, \"ST_avail and ST_antic (shown under loads..)\\n\");\n       print_ldst_list (gcse_file);\n     }\n \n+  free (last_set_in);\n+  free (already_set);\n   return ret;\n }\n \n /* Check to see if the load X is aliased with STORE_PATTERN.  */\n \n-static int\n+static bool\n load_kills_store (x, store_pattern)\n      rtx x, store_pattern;\n {\n   if (true_dependence (x, GET_MODE (x), store_pattern, rtx_addr_varies_p))\n-    return 1;\n-  return 0;\n+    return true;\n+  return false;\n }\n \n /* Go through the entire insn X, looking for any loads which might alias\n-   STORE_PATTERN.  Return 1 if found.  */\n+   STORE_PATTERN.  Return true if found.  */\n \n-static int\n+static bool\n find_loads (x, store_pattern)\n      rtx x, store_pattern;\n {\n   const char * fmt;\n   int i, j;\n-  int ret = 0;\n+  int ret = false;\n \n   if (!x)\n-    return 0;\n+    return false;\n \n   if (GET_CODE (x) == SET)\n     x = SET_SRC (x);\n \n   if (GET_CODE (x) == MEM)\n     {\n       if (load_kills_store (x, store_pattern))\n-\treturn 1;\n+\treturn true;\n     }\n \n   /* Recursively process the insn.  */\n@@ -7145,20 +7352,29 @@ find_loads (x, store_pattern)\n }\n \n /* Check if INSN kills the store pattern X (is aliased with it).\n-   Return 1 if it it does.  */\n+   Return true if it it does.  */\n \n-static int\n-store_killed_in_insn (x, insn)\n-     rtx x, insn;\n+static bool\n+store_killed_in_insn (x, x_regs, insn)\n+     rtx x, x_regs, insn;\n {\n   if (GET_RTX_CLASS (GET_CODE (insn)) != 'i')\n-    return 0;\n+    return false;\n \n   if (GET_CODE (insn) == CALL_INSN)\n     {\n       /* A normal or pure call might read from pattern,\n \t but a const call will not.  */\n-      return ! CONST_OR_PURE_CALL_P (insn) || pure_call_p (insn);\n+      if (! CONST_OR_PURE_CALL_P (insn) || pure_call_p (insn))\n+\treturn true;\n+\n+      /* But even a const call reads its parameters.  It is not trivial\n+\t check that base of the mem is not related to stack pointer,\n+\t so unless it contains no registers, just assume it may.  */\n+      if (x_regs)\n+\treturn true;\n+\n+      return false;\n     }\n \n   if (GET_CODE (PATTERN (insn)) == SET)\n@@ -7168,80 +7384,78 @@ store_killed_in_insn (x, insn)\n       if (GET_CODE (SET_DEST (pat)) == MEM && !expr_equiv_p (SET_DEST (pat), x))\n \t/* pretend its a load and check for aliasing.  */\n \tif (find_loads (SET_DEST (pat), x))\n-\t  return 1;\n+\t  return true;\n       return find_loads (SET_SRC (pat), x);\n     }\n   else\n     return find_loads (PATTERN (insn), x);\n }\n \n-/* Returns 1 if the expression X is loaded or clobbered on or after INSN\n-   within basic block BB.  */\n+/* Returns true if the expression X is loaded or clobbered on or after INSN\n+   within basic block BB.  REGS_SET_AFTER is bitmap of registers set in\n+   or after the insn.  X_REGS is list of registers mentioned in X. If the store\n+   is killed, return the last insn in that it occurs in FAIL_INSN.  */\n \n-static int\n-store_killed_after (x, insn, bb)\n-     rtx x, insn;\n+static bool\n+store_killed_after (x, x_regs, insn, bb, regs_set_after, fail_insn)\n+     rtx x, x_regs, insn;\n      basic_block bb;\n+     int *regs_set_after;\n+     rtx *fail_insn;\n {\n-  rtx last = bb->end;\n-\n-  if (insn == last)\n-    return 0;\n+  rtx last = bb->end, act;\n \n-  /* Check if the register operands of the store are OK in this block.\n-     Note that if registers are changed ANYWHERE in the block, we'll\n-     decide we can't move it, regardless of whether it changed above\n-     or below the store. This could be improved by checking the register\n-     operands while looking for aliasing in each insn.  */\n-  if (!store_ops_ok (XEXP (x, 0), bb))\n-    return 1;\n+  if (!store_ops_ok (x_regs, regs_set_after))\n+    { \n+      /* We do not know where it will happen.  */\n+      if (fail_insn)\n+\t*fail_insn = NULL_RTX;\n+      return true;\n+    }\n \n-  for ( ; insn && insn != NEXT_INSN (last); insn = NEXT_INSN (insn))\n-    if (store_killed_in_insn (x, insn))\n-      return 1;\n+  /* Scan from the end, so that fail_insn is determined correctly.  */\n+  for (act = last; act != PREV_INSN (insn); act = PREV_INSN (act))\n+    if (store_killed_in_insn (x, x_regs, act))\n+      {\n+\tif (fail_insn)\n+\t  *fail_insn = act;\n+\treturn true;\n+      }\n \n-  return 0;\n+  return false;\n }\n-\n-/* Returns 1 if the expression X is loaded or clobbered on or before INSN\n-   within basic block BB.  */\n-static int\n-store_killed_before (x, insn, bb)\n-     rtx x, insn;\n+  \n+/* Returns true if the expression X is loaded or clobbered on or before INSN\n+   within basic block BB. X_REGS is list of registers mentioned in X.\n+   REGS_SET_BEFORE is bitmap of registers set before or in this insn.  */\n+static bool\n+store_killed_before (x, x_regs, insn, bb, regs_set_before)\n+     rtx x, x_regs, insn;\n      basic_block bb;\n+     int *regs_set_before;\n {\n   rtx first = bb->head;\n \n-  if (insn == first)\n-    return store_killed_in_insn (x, insn);\n-\n-  /* Check if the register operands of the store are OK in this block.\n-     Note that if registers are changed ANYWHERE in the block, we'll\n-     decide we can't move it, regardless of whether it changed above\n-     or below the store. This could be improved by checking the register\n-     operands while looking for aliasing in each insn.  */\n-  if (!store_ops_ok (XEXP (x, 0), bb))\n-    return 1;\n+  if (!store_ops_ok (x_regs, regs_set_before))\n+    return true;\n \n-  for ( ; insn && insn != PREV_INSN (first); insn = PREV_INSN (insn))\n-    if (store_killed_in_insn (x, insn))\n-      return 1;\n+  for ( ; insn != PREV_INSN (first); insn = PREV_INSN (insn))\n+    if (store_killed_in_insn (x, x_regs, insn))\n+      return true;\n \n-  return 0;\n+  return false;\n }\n-\n-#define ANTIC_STORE_LIST(x)\t((x)->loads)\n-#define AVAIL_STORE_LIST(x)\t((x)->stores)\n-\n-/* Given the table of available store insns at the end of blocks,\n-   determine which ones are not killed by aliasing, and generate\n-   the appropriate vectors for gen and killed.  */\n+  \n+/* Fill in available, anticipatable, transparent and kill vectors in\n+   STORE_DATA, based on lists of available and anticipatable stores.  */\n static void\n build_store_vectors ()\n {\n-  basic_block bb, b;\n+  basic_block bb;\n+  int *regs_set_in_block;\n   rtx insn, st;\n   struct ls_expr * ptr;\n+  unsigned regno;\n \n   /* Build the gen_vector. This is any store in the table which is not killed\n      by aliasing later in its block.  */\n@@ -7253,98 +7467,66 @@ build_store_vectors ()\n \n   for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n     {\n-      /* Put all the stores into either the antic list, or the avail list,\n-\t or both.  */\n-      rtx store_list = ptr->stores;\n-      ptr->stores = NULL_RTX;\n-\n-      for (st = store_list; st != NULL; st = XEXP (st, 1))\n+      for (st = AVAIL_STORE_LIST (ptr); st != NULL; st = XEXP (st, 1))\n \t{\n \t  insn = XEXP (st, 0);\n \t  bb = BLOCK_FOR_INSN (insn);\n \n-\t  if (!store_killed_after (ptr->pattern, insn, bb))\n-\t    {\n-\t      /* If we've already seen an available expression in this block,\n-\t\t we can delete the one we saw already (It occurs earlier in\n-\t\t the block), and replace it with this one). We'll copy the\n-\t\t old SRC expression to an unused register in case there\n-\t\t are any side effects.  */\n-\t      if (TEST_BIT (ae_gen[bb->index], ptr->index))\n-\t\t{\n-\t\t  /* Find previous store.  */\n-\t\t  rtx st;\n-\t\t  for (st = AVAIL_STORE_LIST (ptr); st ; st = XEXP (st, 1))\n-\t\t    if (BLOCK_FOR_INSN (XEXP (st, 0)) == bb)\n-\t\t      break;\n-\t\t  if (st)\n-\t\t    {\n-\t\t      rtx r = gen_reg_rtx (GET_MODE (ptr->pattern));\n-\t\t      if (gcse_file)\n-\t\t\tfprintf (gcse_file, \"Removing redundant store:\\n\");\n-\t\t      replace_store_insn (r, XEXP (st, 0), bb);\n-\t\t      XEXP (st, 0) = insn;\n-\t\t      continue;\n-\t\t    }\n-\t\t}\n-\t      SET_BIT (ae_gen[bb->index], ptr->index);\n-\t      AVAIL_STORE_LIST (ptr) = alloc_INSN_LIST (insn,\n-\t\t\t\t\t\t\tAVAIL_STORE_LIST (ptr));\n-\t    }\n-\n-\t  if (!store_killed_before (ptr->pattern, insn, bb))\n+\t  /* If we've already seen an available expression in this block,\n+\t     we can delete this one (It occurs earlier in the block). We'll\n+\t     copy the SRC expression to an unused register in case there\n+\t     are any side effects.  */\n+\t  if (TEST_BIT (ae_gen[bb->index], ptr->index))\n \t    {\n-\t      SET_BIT (st_antloc[BLOCK_NUM (insn)], ptr->index);\n-\t      ANTIC_STORE_LIST (ptr) = alloc_INSN_LIST (insn,\n-\t\t\t\t\t\t\tANTIC_STORE_LIST (ptr));\n+\t      rtx r = gen_reg_rtx (GET_MODE (ptr->pattern));\n+\t      if (gcse_file)\n+\t\tfprintf (gcse_file, \"Removing redundant store:\\n\");\n+\t      replace_store_insn (r, XEXP (st, 0), bb);\n+\t      continue;\n \t    }\n+\t  SET_BIT (ae_gen[bb->index], ptr->index);\n \t}\n \n-      /* Free the original list of store insns.  */\n-      free_INSN_LIST_list (&store_list);\n+      for (st = ANTIC_STORE_LIST (ptr); st != NULL; st = XEXP (st, 1))\n+\t{\n+\t  insn = XEXP (st, 0);\n+\t  bb = BLOCK_FOR_INSN (insn);\n+\t  SET_BIT (st_antloc[bb->index], ptr->index);\n+\t}\n     }\n \n   ae_kill = (sbitmap *) sbitmap_vector_alloc (last_basic_block, num_stores);\n   sbitmap_vector_zero (ae_kill, last_basic_block);\n \n   transp = (sbitmap *) sbitmap_vector_alloc (last_basic_block, num_stores);\n   sbitmap_vector_zero (transp, last_basic_block);\n+  regs_set_in_block = xmalloc (sizeof (int) * max_gcse_regno);\n \n-  for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n-    FOR_EACH_BB (b)\n-      {\n-\tif (store_killed_after (ptr->pattern, b->head, b))\n-\t  {\n-\t    /* The anticipatable expression is not killed if it's gen'd.  */\n-\t    /*\n-\t      We leave this check out for now. If we have a code sequence\n-\t      in a block which looks like:\n-\t\t\tST MEMa = x\n-\t\t\tL     y = MEMa\n-\t\t\tST MEMa = z\n-\t      We should flag this as having an ANTIC expression, NOT\n-\t      transparent, NOT killed, and AVAIL.\n-\t      Unfortunately, since we haven't re-written all loads to\n-\t      use the reaching reg, we'll end up doing an incorrect\n-\t      Load in the middle here if we push the store down. It happens in\n-\t\t    gcc.c-torture/execute/960311-1.c with -O3\n-\t      If we always kill it in this case, we'll sometimes do\n-\t      unnecessary work, but it shouldn't actually hurt anything.\n-\t    if (!TEST_BIT (ae_gen[b], ptr->index)).  */\n-\t    SET_BIT (ae_kill[b->index], ptr->index);\n-\t  }\n-\telse\n-\t  SET_BIT (transp[b->index], ptr->index);\n-      }\n+  FOR_EACH_BB (bb)\n+    {\n+      for (regno = 0; regno < max_gcse_regno; regno++)\n+\tregs_set_in_block[regno] = TEST_BIT (reg_set_in_block[bb->index], regno);\n+\n+      for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n+\t{\n+\t  if (store_killed_after (ptr->pattern, ptr->pattern_regs, bb->head,\n+\t\t\t\t  bb, regs_set_in_block, NULL))\n+\t    {\n+\t      /* It should not be neccessary to consider the expression\n+\t\t killed if it is both anticipatable and available.  */\n+\t      if (!TEST_BIT (st_antloc[bb->index], ptr->index)\n+\t\t  || !TEST_BIT (ae_gen[bb->index], ptr->index))\n+\t\tSET_BIT (ae_kill[bb->index], ptr->index);\n+  \t    }\n+  \t  else\n+  \t    SET_BIT (transp[bb->index], ptr->index);\n+       \t}\n+    }\n+\n+  free (regs_set_in_block);\n \n-  /* Any block with no exits calls some non-returning function, so\n-     we better mark the store killed here, or we might not store to\n-     it at all.  If we knew it was abort, we wouldn't have to store,\n-     but we don't know that for sure.  */\n   if (gcse_file)\n     {\n-      fprintf (gcse_file, \"ST_avail and ST_antic (shown under loads..)\\n\");\n-      print_ldst_list (gcse_file);\n       dump_sbitmap_vector (gcse_file, \"st_antloc\", \"\", st_antloc, last_basic_block);\n       dump_sbitmap_vector (gcse_file, \"st_kill\", \"\", ae_kill, last_basic_block);\n       dump_sbitmap_vector (gcse_file, \"Transpt\", \"\", transp, last_basic_block);\n@@ -7405,7 +7587,7 @@ insert_store (expr, e)\n     return 0;\n \n   reg = expr->reaching_reg;\n-  insn = gen_move_insn (expr->pattern, reg);\n+  insn = gen_move_insn (copy_rtx (expr->pattern), reg);\n \n   /* If we are inserting this expression on ALL predecessor edges of a BB,\n      insert it at the start of the BB, and reset the insert bits on the other\n@@ -7493,9 +7675,6 @@ delete_store (expr, bb)\n   if (expr->reaching_reg == NULL_RTX)\n     expr->reaching_reg = gen_reg_rtx (GET_MODE (expr->pattern));\n \n-\n-  /* If there is more than 1 store, the earlier ones will be dead,\n-     but it doesn't hurt to replace them here.  */\n   reg = expr->reaching_reg;\n \n   for (i = AVAIL_STORE_LIST (expr); i; i = XEXP (i, 1))\n@@ -7557,7 +7736,7 @@ store_motion ()\n \n   init_alias_analysis ();\n \n-  /* Find all the stores that are live to the end of their block.  */\n+  /* Find all the available and anticipatable stores.  */\n   num_stores = compute_store_table ();\n   if (num_stores == 0)\n     {\n@@ -7566,9 +7745,9 @@ store_motion ()\n       return;\n     }\n \n-  /* Now compute whats actually available to move.  */\n-  add_noreturn_fake_exit_edges ();\n+  /* Now compute kill & transp vectors.  */\n   build_store_vectors ();\n+  add_noreturn_fake_exit_edges ();\n \n   edge_list = pre_edge_rev_lcm (gcse_file, num_stores, transp, ae_gen,\n \t\t\t\tst_antloc, ae_kill, &pre_insert_map,"}]}