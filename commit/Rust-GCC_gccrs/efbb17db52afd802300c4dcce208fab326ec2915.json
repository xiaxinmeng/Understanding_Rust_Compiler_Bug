{"sha": "efbb17db52afd802300c4dcce208fab326ec2915", "node_id": "C_kwDOANBUbNoAKGVmYmIxN2RiNTJhZmQ4MDIzMDBjNGRjY2UyMDhmYWIzMjZlYzI5MTU", "commit": {"author": {"name": "Paul A. Clarke", "email": "pc@us.ibm.com", "date": "2022-02-17T02:01:41Z"}, "committer": {"name": "Paul A. Clarke", "email": "pc@us.ibm.com", "date": "2022-02-17T19:13:05Z"}, "message": "rs6000: __Uglify non-uglified local variables in headers\n\nProperly prefix (with \"__\")  all local variables in shipped headers for x86\ncompatibility intrinsics implementations.  This avoids possible problems with\nusages like:\n```\n```\n\n2022-02-16  Paul A. Clarke  <pc@us.ibm.com>\n\ngcc\n\tPR target/104257\n\t* config/rs6000/bmi2intrin.h: Uglify local variables.\n\t* config/rs6000/emmintrin.h: Likewise.\n\t* config/rs6000/mm_malloc.h: Likewise.\n\t* config/rs6000/mmintrin.h: Likewise.\n\t* config/rs6000/pmmintrin.h: Likewise.\n\t* config/rs6000/smmintrin.h: Likewise.\n\t* config/rs6000/tmmintrin.h: Likewise.\n\t* config/rs6000/xmmintrin.h: Likewise.", "tree": {"sha": "6e4a585ab34ee6c3344f72f35f64e486c574c4ac", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6e4a585ab34ee6c3344f72f35f64e486c574c4ac"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/efbb17db52afd802300c4dcce208fab326ec2915", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/efbb17db52afd802300c4dcce208fab326ec2915", "html_url": "https://github.com/Rust-GCC/gccrs/commit/efbb17db52afd802300c4dcce208fab326ec2915", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/efbb17db52afd802300c4dcce208fab326ec2915/comments", "author": {"login": "ThinkOpenly", "id": 12301795, "node_id": "MDQ6VXNlcjEyMzAxNzk1", "avatar_url": "https://avatars.githubusercontent.com/u/12301795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ThinkOpenly", "html_url": "https://github.com/ThinkOpenly", "followers_url": "https://api.github.com/users/ThinkOpenly/followers", "following_url": "https://api.github.com/users/ThinkOpenly/following{/other_user}", "gists_url": "https://api.github.com/users/ThinkOpenly/gists{/gist_id}", "starred_url": "https://api.github.com/users/ThinkOpenly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ThinkOpenly/subscriptions", "organizations_url": "https://api.github.com/users/ThinkOpenly/orgs", "repos_url": "https://api.github.com/users/ThinkOpenly/repos", "events_url": "https://api.github.com/users/ThinkOpenly/events{/privacy}", "received_events_url": "https://api.github.com/users/ThinkOpenly/received_events", "type": "User", "site_admin": false}, "committer": {"login": "ThinkOpenly", "id": 12301795, "node_id": "MDQ6VXNlcjEyMzAxNzk1", "avatar_url": "https://avatars.githubusercontent.com/u/12301795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ThinkOpenly", "html_url": "https://github.com/ThinkOpenly", "followers_url": "https://api.github.com/users/ThinkOpenly/followers", "following_url": "https://api.github.com/users/ThinkOpenly/following{/other_user}", "gists_url": "https://api.github.com/users/ThinkOpenly/gists{/gist_id}", "starred_url": "https://api.github.com/users/ThinkOpenly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ThinkOpenly/subscriptions", "organizations_url": "https://api.github.com/users/ThinkOpenly/orgs", "repos_url": "https://api.github.com/users/ThinkOpenly/repos", "events_url": "https://api.github.com/users/ThinkOpenly/events{/privacy}", "received_events_url": "https://api.github.com/users/ThinkOpenly/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fac15bf84807a58f83c741b1034c1bc96348319d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fac15bf84807a58f83c741b1034c1bc96348319d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fac15bf84807a58f83c741b1034c1bc96348319d"}], "stats": {"total": 2681, "additions": 1340, "deletions": 1341}, "files": [{"sha": "b7a7dedad18b02a22594a87151f79e0b0ee568b6", "filename": "gcc/config/rs6000/bmi2intrin.h", "status": "modified", "additions": 34, "deletions": 34, "changes": 68, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fbmi2intrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fbmi2intrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fbmi2intrin.h?ref=efbb17db52afd802300c4dcce208fab326ec2915", "patch": "@@ -77,39 +77,39 @@ extern __inline unsigned long long\n __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _pdep_u64 (unsigned long long __X, unsigned long long __M)\n {\n-  unsigned long result = 0x0UL;\n-  const unsigned long mask = 0x8000000000000000UL;\n-  unsigned long m = __M;\n-  unsigned long c, t;\n-  unsigned long p;\n+  unsigned long __result = 0x0UL;\n+  const unsigned long __mask = 0x8000000000000000UL;\n+  unsigned long __m = __M;\n+  unsigned long __c, __t;\n+  unsigned long __p;\n \n   /* The pop-count of the mask gives the number of the bits from\n    source to process.  This is also needed to shift bits from the\n    source into the correct position for the result.  */\n-  p = 64 - __builtin_popcountl (__M);\n+  __p = 64 - __builtin_popcountl (__M);\n \n   /* The loop is for the number of '1' bits in the mask and clearing\n    each mask bit as it is processed.  */\n-  while (m != 0)\n+  while (__m != 0)\n     {\n-      c = __builtin_clzl (m);\n-      t = __X << (p - c);\n-      m ^= (mask >> c);\n-      result |= (t & (mask >> c));\n-      p++;\n+      __c = __builtin_clzl (__m);\n+      __t = __X << (__p - __c);\n+      __m ^= (__mask >> __c);\n+      __result |= (__t & (__mask >> __c));\n+      __p++;\n     }\n-  return (result);\n+  return __result;\n }\n \n extern __inline unsigned long long\n __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _pext_u64 (unsigned long long __X, unsigned long long __M)\n {\n-  unsigned long p = 0x4040404040404040UL; // initial bit permute control\n-  const unsigned long mask = 0x8000000000000000UL;\n-  unsigned long m = __M;\n-  unsigned long c;\n-  unsigned long result;\n+  unsigned long __p = 0x4040404040404040UL; // initial bit permute control\n+  const unsigned long __mask = 0x8000000000000000UL;\n+  unsigned long __m = __M;\n+  unsigned long __c;\n+  unsigned long __result;\n \n   /* if the mask is constant and selects 8 bits or less we can use\n    the Power8 Bit permute instruction.  */\n@@ -118,35 +118,35 @@ _pext_u64 (unsigned long long __X, unsigned long long __M)\n       /* Also if the pext mask is constant, then the popcount is\n        constant, we can evaluate the following loop at compile\n        time and use a constant bit permute vector.  */\n-      long i;\n-      for (i = 0; i < __builtin_popcountl (__M); i++)\n+      long __i;\n+      for (__i = 0; __i < __builtin_popcountl (__M); __i++)\n \t{\n-\t  c = __builtin_clzl (m);\n-\t  p = (p << 8) | c;\n-\t  m ^= (mask >> c);\n+\t  __c = __builtin_clzl (__m);\n+\t  __p = (__p << 8) | __c;\n+\t  __m ^= (__mask >> __c);\n \t}\n-      result = __builtin_bpermd (p, __X);\n+      __result = __builtin_bpermd (__p, __X);\n     }\n   else\n     {\n-      p = 64 - __builtin_popcountl (__M);\n-      result = 0;\n+      __p = 64 - __builtin_popcountl (__M);\n+      __result = 0;\n       /* We could a use a for loop here, but that combined with\n        -funroll-loops can expand to a lot of code.  The while\n        loop avoids unrolling and the compiler commons the xor\n        from clearing the mask bit with the (m != 0) test.  The\n        result is a more compact loop setup and body.  */\n-      while (m != 0)\n+      while (__m != 0)\n \t{\n-\t  unsigned long t;\n-\t  c = __builtin_clzl (m);\n-\t  t = (__X & (mask >> c)) >> (p - c);\n-\t  m ^= (mask >> c);\n-\t  result |= (t);\n-\t  p++;\n+\t  unsigned long __t;\n+\t  __c = __builtin_clzl (__m);\n+\t  __t = (__X & (__mask >> __c)) >> (__p - __c);\n+\t  __m ^= (__mask >> __c);\n+\t  __result |= (__t);\n+\t  __p++;\n \t}\n     }\n-  return (result);\n+  return __result;\n }\n \n /* these 32-bit implementations depend on 64-bit pdep/pext"}, {"sha": "832967917ac127ed0e91488c02284ee3a4238a28", "filename": "gcc/config/rs6000/emmintrin.h", "status": "modified", "additions": 454, "deletions": 454, "changes": 908, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Femmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Femmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Femmintrin.h?ref=efbb17db52afd802300c4dcce208fab326ec2915", "patch": "@@ -141,9 +141,9 @@ _mm_setzero_pd (void)\n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_move_sd (__m128d __A, __m128d __B)\n {\n-  __v2df result = (__v2df) __A;\n-  result [0] = ((__v2df) __B)[0];\n-  return (__m128d) result;\n+  __v2df __result = (__v2df) __A;\n+  __result [0] = ((__v2df) __B)[0];\n+  return (__m128d) __result;\n }\n \n /* Load two DPFP values from P.  The address must be 16-byte aligned.  */\n@@ -329,9 +329,9 @@ _mm_sqrt_pd (__m128d __A)\n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sqrt_sd (__m128d __A, __m128d __B)\n {\n-  __v2df c;\n-  c = vec_sqrt ((__v2df) _mm_set1_pd (__B[0]));\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __v2df __c;\n+  __c = vec_sqrt ((__v2df) _mm_set1_pd (__B[0]));\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -343,11 +343,11 @@ _mm_min_pd (__m128d __A, __m128d __B)\n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n-  c = vec_min (a, b);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n+  __c = vec_min (__a, __b);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -359,11 +359,11 @@ _mm_max_pd (__m128d __A, __m128d __B)\n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n-  c = vec_max (a, b);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n+  __c = vec_max (__a, __b);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -399,8 +399,8 @@ _mm_cmpge_pd (__m128d __A, __m128d __B)\n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpneq_pd (__m128d __A, __m128d __B)\n {\n-  __v2df temp = (__v2df) vec_cmpeq ((__v2df) __A, (__v2df)__B);\n-  return ((__m128d)vec_nor (temp, temp));\n+  __v2df __temp = (__v2df) vec_cmpeq ((__v2df) __A, (__v2df)__B);\n+  return ((__m128d)vec_nor (__temp, __temp));\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -430,163 +430,163 @@ _mm_cmpnge_pd (__m128d __A, __m128d __B)\n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpord_pd (__m128d __A, __m128d __B)\n {\n-  __v2du c, d;\n+  __v2du __c, __d;\n   /* Compare against self will return false (0's) if NAN.  */\n-  c = (__v2du)vec_cmpeq (__A, __A);\n-  d = (__v2du)vec_cmpeq (__B, __B);\n+  __c = (__v2du)vec_cmpeq (__A, __A);\n+  __d = (__v2du)vec_cmpeq (__B, __B);\n   /* A != NAN and B != NAN.  */\n-  return ((__m128d)vec_and(c, d));\n+  return ((__m128d)vec_and(__c, __d));\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpunord_pd (__m128d __A, __m128d __B)\n {\n #if _ARCH_PWR8\n-  __v2du c, d;\n+  __v2du __c, __d;\n   /* Compare against self will return false (0's) if NAN.  */\n-  c = (__v2du)vec_cmpeq ((__v2df)__A, (__v2df)__A);\n-  d = (__v2du)vec_cmpeq ((__v2df)__B, (__v2df)__B);\n+  __c = (__v2du)vec_cmpeq ((__v2df)__A, (__v2df)__A);\n+  __d = (__v2du)vec_cmpeq ((__v2df)__B, (__v2df)__B);\n   /* A == NAN OR B == NAN converts too:\n      NOT(A != NAN) OR NOT(B != NAN).  */\n-  c = vec_nor (c, c);\n-  return ((__m128d)vec_orc(c, d));\n+  __c = vec_nor (__c, __c);\n+  return ((__m128d)vec_orc(__c, __d));\n #else\n-  __v2du c, d;\n+  __v2du __c, __d;\n   /* Compare against self will return false (0's) if NAN.  */\n-  c = (__v2du)vec_cmpeq ((__v2df)__A, (__v2df)__A);\n-  d = (__v2du)vec_cmpeq ((__v2df)__B, (__v2df)__B);\n+  __c = (__v2du)vec_cmpeq ((__v2df)__A, (__v2df)__A);\n+  __d = (__v2du)vec_cmpeq ((__v2df)__B, (__v2df)__B);\n   /* Convert the true ('1's) is NAN.  */\n-  c = vec_nor (c, c);\n-  d = vec_nor (d, d);\n-  return ((__m128d)vec_or(c, d));\n+  __c = vec_nor (__c, __c);\n+  __d = vec_nor (__d, __d);\n+  return ((__m128d)vec_or(__c, __d));\n #endif\n }\n \n extern __inline  __m128d  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_sd(__m128d  __A, __m128d  __B)\n {\n-  __v2df a, b, c;\n+  __v2df __a, __b, __c;\n   /* PowerISA VSX does not allow partial (for just lower double)\n      results. So to insure we don't generate spurious exceptions\n      (from the upper double values) we splat the lower double\n      before we do the operation. */\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n-  c = (__v2df) vec_cmpeq(a, b);\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n+  __c = (__v2df) vec_cmpeq(__a, __b);\n   /* Then we merge the lower double result with the original upper\n      double from __A.  */\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmplt_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n-  c = (__v2df) vec_cmplt(a, b);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n+  __c = (__v2df) vec_cmplt(__a, __b);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmple_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n-  c = (__v2df) vec_cmple(a, b);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n+  __c = (__v2df) vec_cmple(__a, __b);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n-  c = (__v2df) vec_cmpgt(a, b);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n+  __c = (__v2df) vec_cmpgt(__a, __b);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpge_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n-  c = (__v2df) vec_cmpge(a, b);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n+  __c = (__v2df) vec_cmpge(__a, __b);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpneq_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n-  c = (__v2df) vec_cmpeq(a, b);\n-  c = vec_nor (c, c);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n+  __c = (__v2df) vec_cmpeq(__a, __b);\n+  __c = vec_nor (__c, __c);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnlt_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n   /* Not less than is just greater than or equal.  */\n-  c = (__v2df) vec_cmpge(a, b);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __c = (__v2df) vec_cmpge(__a, __b);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnle_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n   /* Not less than or equal is just greater than.  */\n-  c = (__v2df) vec_cmpge(a, b);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __c = (__v2df) vec_cmpge(__a, __b);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpngt_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n   /* Not greater than is just less than or equal.  */\n-  c = (__v2df) vec_cmple(a, b);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __c = (__v2df) vec_cmple(__a, __b);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnge_sd (__m128d __A, __m128d __B)\n {\n-  __v2df a, b, c;\n-  a = vec_splats (__A[0]);\n-  b = vec_splats (__B[0]);\n+  __v2df __a, __b, __c;\n+  __a = vec_splats (__A[0]);\n+  __b = vec_splats (__B[0]);\n   /* Not greater than or equal is just less than.  */\n-  c = (__v2df) vec_cmplt(a, b);\n-  return (__m128d) _mm_setr_pd (c[0], __A[1]);\n+  __c = (__v2df) vec_cmplt(__a, __b);\n+  return (__m128d) _mm_setr_pd (__c[0], __A[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpord_sd (__m128d __A, __m128d __B)\n {\n-  __v2df r;\n-  r = (__v2df)_mm_cmpord_pd (vec_splats (__A[0]), vec_splats (__B[0]));\n-  return (__m128d) _mm_setr_pd (r[0], ((__v2df)__A)[1]);\n+  __v2df __r;\n+  __r = (__v2df)_mm_cmpord_pd (vec_splats (__A[0]), vec_splats (__B[0]));\n+  return (__m128d) _mm_setr_pd (__r[0], ((__v2df)__A)[1]);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpunord_sd (__m128d __A, __m128d __B)\n {\n-  __v2df r;\n-  r = _mm_cmpunord_pd (vec_splats (__A[0]), vec_splats (__B[0]));\n-  return (__m128d) _mm_setr_pd (r[0], __A[1]);\n+  __v2df __r;\n+  __r = _mm_cmpunord_pd (vec_splats (__A[0]), vec_splats (__B[0]));\n+  return (__m128d) _mm_setr_pd (__r[0], __A[1]);\n }\n \n /* FIXME\n@@ -845,12 +845,12 @@ _mm_setzero_si128 (void)\n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi32_pd (__m128i __A)\n {\n-  __v2di val;\n+  __v2di __val;\n   /* For LE need to generate Vector Unpack Low Signed Word.\n      Which is generated from unpackh.  */\n-  val = (__v2di)vec_unpackh ((__v4si)__A);\n+  __val = (__v2di)vec_unpackh ((__v4si)__A);\n \n-  return (__m128d)vec_ctf (val, 0);\n+  return (__m128d)vec_ctf (__val, 0);\n }\n #endif\n \n@@ -863,116 +863,116 @@ _mm_cvtepi32_ps (__m128i __A)\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpd_epi32 (__m128d __A)\n {\n-  __v2df rounded = vec_rint (__A);\n-  __v4si result, temp;\n-  const __v4si vzero =\n+  __v2df __rounded = vec_rint (__A);\n+  __v4si __result, __temp;\n+  const __v4si __vzero =\n     { 0, 0, 0, 0 };\n \n   /* VSX Vector truncate Double-Precision to integer and Convert to\n    Signed Integer Word format with Saturate.  */\n   __asm__(\n       \"xvcvdpsxws %x0,%x1\"\n-      : \"=wa\" (temp)\n-      : \"wa\" (rounded)\n+      : \"=wa\" (__temp)\n+      : \"wa\" (__rounded)\n       : );\n \n #ifdef _ARCH_PWR8\n #ifdef __LITTLE_ENDIAN__\n-  temp = vec_mergeo (temp, temp);\n+  __temp = vec_mergeo (__temp, __temp);\n #else\n-  temp = vec_mergee (temp, temp);\n+  __temp = vec_mergee (__temp, __temp);\n #endif\n-  result = (__v4si) vec_vpkudum ((__vector long long) temp,\n-\t\t\t\t (__vector long long) vzero);\n+  __result = (__v4si) vec_vpkudum ((__vector long long) __temp,\n+\t\t\t\t (__vector long long) __vzero);\n #else\n   {\n-    const __v16qu pkperm = {0x00, 0x01, 0x02, 0x03, 0x08, 0x09, 0x0a, 0x0b,\n+    const __v16qu __pkperm = {0x00, 0x01, 0x02, 0x03, 0x08, 0x09, 0x0a, 0x0b,\n \t0x14, 0x15, 0x16, 0x17, 0x1c, 0x1d, 0x1e, 0x1f };\n-    result = (__v4si) vec_perm ((__v16qu) temp, (__v16qu) vzero, pkperm);\n+    __result = (__v4si) vec_perm ((__v16qu) __temp, (__v16qu) __vzero, __pkperm);\n   }\n #endif\n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpd_pi32 (__m128d __A)\n {\n-  __m128i result = _mm_cvtpd_epi32(__A);\n+  __m128i __result = _mm_cvtpd_epi32(__A);\n \n-  return (__m64) result[0];\n+  return (__m64) __result[0];\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpd_ps (__m128d __A)\n {\n-  __v4sf result;\n-  __v4si temp;\n-  const __v4si vzero = { 0, 0, 0, 0 };\n+  __v4sf __result;\n+  __v4si __temp;\n+  const __v4si __vzero = { 0, 0, 0, 0 };\n \n   __asm__(\n       \"xvcvdpsp %x0,%x1\"\n-      : \"=wa\" (temp)\n+      : \"=wa\" (__temp)\n       : \"wa\" (__A)\n       : );\n \n #ifdef _ARCH_PWR8\n #ifdef __LITTLE_ENDIAN__\n-  temp = vec_mergeo (temp, temp);\n+  __temp = vec_mergeo (__temp, __temp);\n #else\n-  temp = vec_mergee (temp, temp);\n+  __temp = vec_mergee (__temp, __temp);\n #endif\n-  result = (__v4sf) vec_vpkudum ((__vector long long) temp,\n-\t\t\t\t (__vector long long) vzero);\n+  __result = (__v4sf) vec_vpkudum ((__vector long long) __temp,\n+\t\t\t\t (__vector long long) __vzero);\n #else\n   {\n-    const __v16qu pkperm = {0x00, 0x01, 0x02, 0x03, 0x08, 0x09, 0x0a, 0x0b,\n+    const __v16qu __pkperm = {0x00, 0x01, 0x02, 0x03, 0x08, 0x09, 0x0a, 0x0b,\n \t0x14, 0x15, 0x16, 0x17, 0x1c, 0x1d, 0x1e, 0x1f };\n-    result = (__v4sf) vec_perm ((__v16qu) temp, (__v16qu) vzero, pkperm);\n+    __result = (__v4sf) vec_perm ((__v16qu) __temp, (__v16qu) __vzero, __pkperm);\n   }\n #endif\n-  return ((__m128)result);\n+  return ((__m128)__result);\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttpd_epi32 (__m128d __A)\n {\n-  __v4si result;\n-  __v4si temp;\n-  const __v4si vzero = { 0, 0, 0, 0 };\n+  __v4si __result;\n+  __v4si __temp;\n+  const __v4si __vzero = { 0, 0, 0, 0 };\n \n   /* VSX Vector truncate Double-Precision to integer and Convert to\n    Signed Integer Word format with Saturate.  */\n   __asm__(\n       \"xvcvdpsxws %x0,%x1\"\n-      : \"=wa\" (temp)\n+      : \"=wa\" (__temp)\n       : \"wa\" (__A)\n       : );\n \n #ifdef _ARCH_PWR8\n #ifdef __LITTLE_ENDIAN__\n-  temp = vec_mergeo (temp, temp);\n+  __temp = vec_mergeo (__temp, __temp);\n #else\n-  temp = vec_mergee (temp, temp);\n+  __temp = vec_mergee (__temp, __temp);\n #endif\n-  result = (__v4si) vec_vpkudum ((__vector long long) temp,\n-\t\t\t\t (__vector long long) vzero);\n+  __result = (__v4si) vec_vpkudum ((__vector long long) __temp,\n+\t\t\t\t (__vector long long) __vzero);\n #else\n   {\n-    const __v16qu pkperm = {0x00, 0x01, 0x02, 0x03, 0x08, 0x09, 0x0a, 0x0b,\n+    const __v16qu __pkperm = {0x00, 0x01, 0x02, 0x03, 0x08, 0x09, 0x0a, 0x0b,\n \t0x14, 0x15, 0x16, 0x17, 0x1c, 0x1d, 0x1e, 0x1f };\n-    result = (__v4si) vec_perm ((__v16qu) temp, (__v16qu) vzero, pkperm);\n+    __result = (__v4si) vec_perm ((__v16qu) __temp, (__v16qu) __vzero, __pkperm);\n   }\n #endif\n \n-  return ((__m128i) result);\n+  return ((__m128i) __result);\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttpd_pi32 (__m128d __A)\n {\n-  __m128i result = _mm_cvttpd_epi32 (__A);\n+  __m128i __result = _mm_cvttpd_epi32 (__A);\n \n-  return (__m64) result[0];\n+  return (__m64) __result[0];\n }\n \n extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -985,35 +985,35 @@ _mm_cvtsi128_si32 (__m128i __A)\n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpi32_pd (__m64 __A)\n {\n-  __v4si temp;\n-  __v2di tmp2;\n-  __v2df result;\n+  __v4si __temp;\n+  __v2di __tmp2;\n+  __v2df __result;\n \n-  temp = (__v4si)vec_splats (__A);\n-  tmp2 = (__v2di)vec_unpackl (temp);\n-  result = vec_ctf ((__vector signed long long) tmp2, 0);\n-  return (__m128d)result;\n+  __temp = (__v4si)vec_splats (__A);\n+  __tmp2 = (__v2di)vec_unpackl (__temp);\n+  __result = vec_ctf ((__vector signed long long) __tmp2, 0);\n+  return (__m128d)__result;\n }\n #endif\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtps_epi32 (__m128 __A)\n {\n-  __v4sf rounded;\n-  __v4si result;\n+  __v4sf __rounded;\n+  __v4si __result;\n \n-  rounded = vec_rint((__v4sf) __A);\n-  result = vec_cts (rounded, 0);\n-  return (__m128i) result;\n+  __rounded = vec_rint((__v4sf) __A);\n+  __result = vec_cts (__rounded, 0);\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttps_epi32 (__m128 __A)\n {\n-  __v4si result;\n+  __v4si __result;\n \n-  result = vec_cts ((__v4sf) __A, 0);\n-  return (__m128i) result;\n+  __result = vec_cts ((__v4sf) __A, 0);\n+  return (__m128i) __result;\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1025,48 +1025,48 @@ _mm_cvtps_pd (__m128 __A)\n #else\n   /* Otherwise the compiler is not current and so need to generate the\n      equivalent code.  */\n-  __v4sf a = (__v4sf)__A;\n-  __v4sf temp;\n-  __v2df result;\n+  __v4sf __a = (__v4sf)__A;\n+  __v4sf __temp;\n+  __v2df __result;\n #ifdef __LITTLE_ENDIAN__\n   /* The input float values are in elements {[0], [1]} but the convert\n      instruction needs them in elements {[1], [3]}, So we use two\n      shift left double vector word immediates to get the elements\n      lined up.  */\n-  temp = __builtin_vsx_xxsldwi (a, a, 3);\n-  temp = __builtin_vsx_xxsldwi (a, temp, 2);\n+  __temp = __builtin_vsx_xxsldwi (__a, __a, 3);\n+  __temp = __builtin_vsx_xxsldwi (__a, __temp, 2);\n #else\n   /* The input float values are in elements {[0], [1]} but the convert\n      instruction needs them in elements {[0], [2]}, So we use two\n      shift left double vector word immediates to get the elements\n      lined up.  */\n-  temp = vec_vmrghw (a, a);\n+  __temp = vec_vmrghw (__a, __a);\n #endif\n   __asm__(\n       \" xvcvspdp %x0,%x1\"\n-      : \"=wa\" (result)\n-      : \"wa\" (temp)\n+      : \"=wa\" (__result)\n+      : \"wa\" (__temp)\n       : );\n-  return (__m128d) result;\n+  return (__m128d) __result;\n #endif\n }\n \n extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsd_si32 (__m128d __A)\n {\n-  __v2df rounded = vec_rint((__v2df) __A);\n-  int result = ((__v2df)rounded)[0];\n+  __v2df __rounded = vec_rint((__v2df) __A);\n+  int __result = ((__v2df)__rounded)[0];\n \n-  return result;\n+  return __result;\n }\n /* Intel intrinsic.  */\n extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsd_si64 (__m128d __A)\n {\n-  __v2df rounded = vec_rint ((__v2df) __A );\n-  long long result = ((__v2df) rounded)[0];\n+  __v2df __rounded = vec_rint ((__v2df) __A );\n+  long long __result = ((__v2df) __rounded)[0];\n \n-  return result;\n+  return __result;\n }\n \n /* Microsoft intrinsic.  */\n@@ -1079,18 +1079,18 @@ _mm_cvtsd_si64x (__m128d __A)\n extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttsd_si32 (__m128d __A)\n {\n-  int result = ((__v2df)__A)[0];\n+  int __result = ((__v2df)__A)[0];\n \n-  return result;\n+  return __result;\n }\n \n /* Intel intrinsic.  */\n extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttsd_si64 (__m128d __A)\n {\n-  long long result = ((__v2df)__A)[0];\n+  long long __result = ((__v2df)__A)[0];\n \n-  return result;\n+  return __result;\n }\n \n /* Microsoft intrinsic.  */\n@@ -1103,46 +1103,46 @@ _mm_cvttsd_si64x (__m128d __A)\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsd_ss (__m128 __A, __m128d __B)\n {\n-  __v4sf result = (__v4sf)__A;\n+  __v4sf __result = (__v4sf)__A;\n \n #ifdef __LITTLE_ENDIAN__\n-  __v4sf temp_s;\n+  __v4sf __temp_s;\n   /* Copy double element[0] to element [1] for conversion.  */\n-  __v2df temp_b = vec_splat((__v2df)__B, 0);\n+  __v2df __temp_b = vec_splat((__v2df)__B, 0);\n \n   /* Pre-rotate __A left 3 (logically right 1) elements.  */\n-  result = __builtin_vsx_xxsldwi (result, result, 3);\n+  __result = __builtin_vsx_xxsldwi (__result, __result, 3);\n   /* Convert double to single float scalar in a vector.  */\n   __asm__(\n       \"xscvdpsp %x0,%x1\"\n-      : \"=wa\" (temp_s)\n-      : \"wa\" (temp_b)\n+      : \"=wa\" (__temp_s)\n+      : \"wa\" (__temp_b)\n       : );\n   /* Shift the resulting scalar into vector element [0].  */\n-  result = __builtin_vsx_xxsldwi (result, temp_s, 1);\n+  __result = __builtin_vsx_xxsldwi (__result, __temp_s, 1);\n #else\n-  result [0] = ((__v2df)__B)[0];\n+  __result [0] = ((__v2df)__B)[0];\n #endif\n-  return (__m128) result;\n+  return (__m128) __result;\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi32_sd (__m128d __A, int __B)\n {\n-  __v2df result = (__v2df)__A;\n-  double db = __B;\n-  result [0] = db;\n-  return (__m128d)result;\n+  __v2df __result = (__v2df)__A;\n+  double __db = __B;\n+  __result [0] = __db;\n+  return (__m128d)__result;\n }\n \n /* Intel intrinsic.  */\n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64_sd (__m128d __A, long long __B)\n {\n-  __v2df result = (__v2df)__A;\n-  double db = __B;\n-  result [0] = db;\n-  return (__m128d)result;\n+  __v2df __result = (__v2df)__A;\n+  double __db = __B;\n+  __result [0] = __db;\n+  return (__m128d)__result;\n }\n \n /* Microsoft intrinsic.  */\n@@ -1157,45 +1157,45 @@ _mm_cvtss_sd (__m128d __A, __m128 __B)\n {\n #ifdef __LITTLE_ENDIAN__\n   /* Use splat to move element [0] into position for the convert. */\n-  __v4sf temp = vec_splat ((__v4sf)__B, 0);\n-  __v2df res;\n+  __v4sf __temp = vec_splat ((__v4sf)__B, 0);\n+  __v2df __res;\n   /* Convert single float scalar to double in a vector.  */\n   __asm__(\n       \"xscvspdp %x0,%x1\"\n-      : \"=wa\" (res)\n-      : \"wa\" (temp)\n+      : \"=wa\" (__res)\n+      : \"wa\" (__temp)\n       : );\n-  return (__m128d) vec_mergel (res, (__v2df)__A);\n+  return (__m128d) vec_mergel (__res, (__v2df)__A);\n #else\n-  __v2df res = (__v2df)__A;\n-  res [0] = ((__v4sf)__B) [0];\n-  return (__m128d) res;\n+  __v2df __res = (__v2df)__A;\n+  __res [0] = ((__v4sf)__B) [0];\n+  return (__m128d) __res;\n #endif\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shuffle_pd(__m128d __A, __m128d __B, const int __mask)\n {\n-  __vector double result;\n-  const int litmsk = __mask & 0x3;\n+  __vector double __result;\n+  const int __litmsk = __mask & 0x3;\n \n-  if (litmsk == 0)\n-    result = vec_mergeh (__A, __B);\n+  if (__litmsk == 0)\n+    __result = vec_mergeh (__A, __B);\n #if __GNUC__ < 6\n-  else if (litmsk == 1)\n-    result = vec_xxpermdi (__B, __A, 2);\n-  else if (litmsk == 2)\n-    result = vec_xxpermdi (__B, __A, 1);\n+  else if (__litmsk == 1)\n+    __result = vec_xxpermdi (__B, __A, 2);\n+  else if (__litmsk == 2)\n+    __result = vec_xxpermdi (__B, __A, 1);\n #else\n-  else if (litmsk == 1)\n-    result = vec_xxpermdi (__A, __B, 2);\n-  else if (litmsk == 2)\n-    result = vec_xxpermdi (__A, __B, 1);\n+  else if (__litmsk == 1)\n+    __result = vec_xxpermdi (__A, __B, 2);\n+  else if (__litmsk == 2)\n+    __result = vec_xxpermdi (__A, __B, 1);\n #endif\n   else\n-    result = vec_mergel (__A, __B);\n+    __result = vec_mergel (__A, __B);\n \n-  return result;\n+  return __result;\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1213,17 +1213,17 @@ _mm_unpacklo_pd (__m128d __A, __m128d __B)\n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadh_pd (__m128d __A, double const *__B)\n {\n-  __v2df result = (__v2df)__A;\n-  result [1] = *__B;\n-  return (__m128d)result;\n+  __v2df __result = (__v2df)__A;\n+  __result [1] = *__B;\n+  return (__m128d)__result;\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadl_pd (__m128d __A, double const *__B)\n {\n-  __v2df result = (__v2df)__A;\n-  result [0] = *__B;\n-  return (__m128d)result;\n+  __v2df __result = (__v2df)__A;\n+  __result [0] = *__B;\n+  return (__m128d)__result;\n }\n \n #ifdef _ARCH_PWR8\n@@ -1236,8 +1236,8 @@ _mm_movemask_pd (__m128d  __A)\n #ifdef _ARCH_PWR10\n   return vec_extractm ((__v2du) __A);\n #else\n-  __vector unsigned long long result;\n-  static const __vector unsigned int perm_mask =\n+  __vector unsigned long long __result;\n+  static const __vector unsigned int __perm_mask =\n     {\n #ifdef __LITTLE_ENDIAN__\n \t0x80800040, 0x80808080, 0x80808080, 0x80808080\n@@ -1246,14 +1246,14 @@ _mm_movemask_pd (__m128d  __A)\n #endif\n     };\n \n-  result = ((__vector unsigned long long)\n+  __result = ((__vector unsigned long long)\n \t    vec_vbpermq ((__vector unsigned char) __A,\n-\t\t\t (__vector unsigned char) perm_mask));\n+\t\t\t (__vector unsigned char) __perm_mask));\n \n #ifdef __LITTLE_ENDIAN__\n-  return result[1];\n+  return __result[1];\n #else\n-  return result[0];\n+  return __result[0];\n #endif\n #endif /* !_ARCH_PWR10 */\n }\n@@ -1426,17 +1426,17 @@ _mm_subs_epu16 (__m128i __A, __m128i __B)\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_madd_epi16 (__m128i __A, __m128i __B)\n {\n-  __vector signed int zero = {0, 0, 0, 0};\n+  __vector signed int __zero = {0, 0, 0, 0};\n \n-  return (__m128i) vec_vmsumshm ((__v8hi)__A, (__v8hi)__B, zero);\n+  return (__m128i) vec_vmsumshm ((__v8hi)__A, (__v8hi)__B, __zero);\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mulhi_epi16 (__m128i __A, __m128i __B)\n {\n-  __vector signed int w0, w1;\n+  __vector signed int __w0, __w1;\n \n-  __vector unsigned char xform1 = {\n+  __vector unsigned char __xform1 = {\n #ifdef __LITTLE_ENDIAN__\n       0x02, 0x03, 0x12, 0x13,  0x06, 0x07, 0x16, 0x17,\n       0x0A, 0x0B, 0x1A, 0x1B,  0x0E, 0x0F, 0x1E, 0x1F\n@@ -1446,9 +1446,9 @@ _mm_mulhi_epi16 (__m128i __A, __m128i __B)\n #endif\n     };\n \n-  w0 = vec_vmulesh ((__v8hi)__A, (__v8hi)__B);\n-  w1 = vec_vmulosh ((__v8hi)__A, (__v8hi)__B);\n-  return (__m128i) vec_perm (w0, w1, xform1);\n+  __w0 = vec_vmulesh ((__v8hi)__A, (__v8hi)__B);\n+  __w1 = vec_vmulosh ((__v8hi)__A, (__v8hi)__B);\n+  return (__m128i) vec_perm (__w0, __w1, __xform1);\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1460,35 +1460,35 @@ _mm_mullo_epi16 (__m128i __A, __m128i __B)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mul_su32 (__m64 __A, __m64 __B)\n {\n-  unsigned int a = __A;\n-  unsigned int b = __B;\n+  unsigned int __a = __A;\n+  unsigned int __b = __B;\n \n-  return ((__m64)a * (__m64)b);\n+  return ((__m64)__a * (__m64)__b);\n }\n \n #ifdef _ARCH_PWR8\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mul_epu32 (__m128i __A, __m128i __B)\n {\n #if __GNUC__ < 8\n-  __v2du result;\n+  __v2du __result;\n \n #ifdef __LITTLE_ENDIAN__\n   /* VMX Vector Multiply Odd Unsigned Word.  */\n   __asm__(\n       \"vmulouw %0,%1,%2\"\n-      : \"=v\" (result)\n+      : \"=v\" (__result)\n       : \"v\" (__A), \"v\" (__B)\n       : );\n #else\n   /* VMX Vector Multiply Even Unsigned Word.  */\n   __asm__(\n       \"vmuleuw %0,%1,%2\"\n-      : \"=v\" (result)\n+      : \"=v\" (__result)\n       : \"v\" (__A), \"v\" (__B)\n       : );\n #endif\n-  return (__m128i) result;\n+  return (__m128i) __result;\n #else\n   return (__m128i) vec_mule ((__v4su)__A, (__v4su)__B);\n #endif\n@@ -1498,144 +1498,144 @@ _mm_mul_epu32 (__m128i __A, __m128i __B)\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_epi16 (__m128i __A, int __B)\n {\n-  __v8hu lshift;\n-  __v8hi result = { 0, 0, 0, 0, 0, 0, 0, 0 };\n+  __v8hu __lshift;\n+  __v8hi __result = { 0, 0, 0, 0, 0, 0, 0, 0 };\n \n   if (__B >= 0 && __B < 16)\n     {\n       if (__builtin_constant_p(__B))\n-\tlshift = (__v8hu) vec_splat_s16(__B);\n+\t__lshift = (__v8hu) vec_splat_s16(__B);\n       else\n-\tlshift = vec_splats ((unsigned short) __B);\n+\t__lshift = vec_splats ((unsigned short) __B);\n \n-      result = vec_sl ((__v8hi) __A, lshift);\n+      __result = vec_sl ((__v8hi) __A, __lshift);\n     }\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_epi32 (__m128i __A, int __B)\n {\n-  __v4su lshift;\n-  __v4si result = { 0, 0, 0, 0 };\n+  __v4su __lshift;\n+  __v4si __result = { 0, 0, 0, 0 };\n \n   if (__B >= 0 && __B < 32)\n     {\n       if (__builtin_constant_p(__B) && __B < 16)\n-\tlshift = (__v4su) vec_splat_s32(__B);\n+\t__lshift = (__v4su) vec_splat_s32(__B);\n       else\n-\tlshift = vec_splats ((unsigned int) __B);\n+\t__lshift = vec_splats ((unsigned int) __B);\n \n-      result = vec_sl ((__v4si) __A, lshift);\n+      __result = vec_sl ((__v4si) __A, __lshift);\n     }\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n #ifdef _ARCH_PWR8\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_epi64 (__m128i __A, int __B)\n {\n-  __v2du lshift;\n-  __v2di result = { 0, 0 };\n+  __v2du __lshift;\n+  __v2di __result = { 0, 0 };\n \n   if (__B >= 0 && __B < 64)\n     {\n       if (__builtin_constant_p(__B) && __B < 16)\n-\tlshift = (__v2du) vec_splat_s32(__B);\n+\t__lshift = (__v2du) vec_splat_s32(__B);\n       else\n-\tlshift = (__v2du) vec_splats ((unsigned int) __B);\n+\t__lshift = (__v2du) vec_splats ((unsigned int) __B);\n \n-      result = vec_sl ((__v2di) __A, lshift);\n+      __result = vec_sl ((__v2di) __A, __lshift);\n     }\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n #endif\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srai_epi16 (__m128i __A, int __B)\n {\n-  __v8hu rshift = { 15, 15, 15, 15, 15, 15, 15, 15 };\n-  __v8hi result;\n+  __v8hu __rshift = { 15, 15, 15, 15, 15, 15, 15, 15 };\n+  __v8hi __result;\n \n   if (__B < 16)\n     {\n       if (__builtin_constant_p(__B))\n-\trshift = (__v8hu) vec_splat_s16(__B);\n+\t__rshift = (__v8hu) vec_splat_s16(__B);\n       else\n-\trshift = vec_splats ((unsigned short) __B);\n+\t__rshift = vec_splats ((unsigned short) __B);\n     }\n-  result = vec_sra ((__v8hi) __A, rshift);\n+  __result = vec_sra ((__v8hi) __A, __rshift);\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srai_epi32 (__m128i __A, int __B)\n {\n-  __v4su rshift = { 31, 31, 31, 31 };\n-  __v4si result;\n+  __v4su __rshift = { 31, 31, 31, 31 };\n+  __v4si __result;\n \n   if (__B < 32)\n     {\n       if (__builtin_constant_p(__B))\n \t{\n \t  if (__B < 16)\n-\t      rshift = (__v4su) vec_splat_s32(__B);\n+\t      __rshift = (__v4su) vec_splat_s32(__B);\n \t    else\n-\t      rshift = (__v4su) vec_splats((unsigned int)__B);\n+\t      __rshift = (__v4su) vec_splats((unsigned int)__B);\n \t}\n       else\n-\trshift = vec_splats ((unsigned int) __B);\n+\t__rshift = vec_splats ((unsigned int) __B);\n     }\n-  result = vec_sra ((__v4si) __A, rshift);\n+  __result = vec_sra ((__v4si) __A, __rshift);\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_bslli_si128 (__m128i __A, const int __N)\n {\n-  __v16qu result;\n-  const __v16qu zeros = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n+  __v16qu __result;\n+  const __v16qu __zeros = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n \n   if (__N < 16)\n-    result = vec_sld ((__v16qu) __A, zeros, __N);\n+    __result = vec_sld ((__v16qu) __A, __zeros, __N);\n   else\n-    result = zeros;\n+    __result = __zeros;\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_bsrli_si128 (__m128i __A, const int __N)\n {\n-  __v16qu result;\n-  const __v16qu zeros = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n+  __v16qu __result;\n+  const __v16qu __zeros = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n \n   if (__N < 16)\n #ifdef __LITTLE_ENDIAN__\n     if (__builtin_constant_p(__N))\n       /* Would like to use Vector Shift Left Double by Octet\n \t Immediate here to use the immediate form and avoid\n \t load of __N * 8 value into a separate VR.  */\n-      result = vec_sld (zeros, (__v16qu) __A, (16 - __N));\n+      __result = vec_sld (__zeros, (__v16qu) __A, (16 - __N));\n     else\n #endif\n       {\n-\t__v16qu shift = vec_splats((unsigned char)(__N*8));\n+\t__v16qu __shift = vec_splats((unsigned char)(__N*8));\n #ifdef __LITTLE_ENDIAN__\n-\tresult = vec_sro ((__v16qu)__A, shift);\n+\t__result = vec_sro ((__v16qu)__A, __shift);\n #else\n-\tresult = vec_slo ((__v16qu)__A, shift);\n+\t__result = vec_slo ((__v16qu)__A, __shift);\n #endif\n       }\n   else\n-    result = zeros;\n+    __result = __zeros;\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1647,239 +1647,239 @@ _mm_srli_si128 (__m128i __A, const int __N)\n extern __inline  __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_si128 (__m128i __A, const int _imm5)\n {\n-  __v16qu result;\n-  const __v16qu zeros = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n+  __v16qu __result;\n+  const __v16qu __zeros = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n \n   if (_imm5 < 16)\n #ifdef __LITTLE_ENDIAN__\n-    result = vec_sld ((__v16qu) __A, zeros, _imm5);\n+    __result = vec_sld ((__v16qu) __A, __zeros, _imm5);\n #else\n-    result = vec_sld (zeros, (__v16qu) __A, (16 - _imm5));\n+    __result = vec_sld (__zeros, (__v16qu) __A, (16 - _imm5));\n #endif\n   else\n-    result = zeros;\n+    __result = __zeros;\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n \n _mm_srli_epi16 (__m128i  __A, int __B)\n {\n-  __v8hu rshift;\n-  __v8hi result = { 0, 0, 0, 0, 0, 0, 0, 0 };\n+  __v8hu __rshift;\n+  __v8hi __result = { 0, 0, 0, 0, 0, 0, 0, 0 };\n \n   if (__B < 16)\n     {\n       if (__builtin_constant_p(__B))\n-\trshift = (__v8hu) vec_splat_s16(__B);\n+\t__rshift = (__v8hu) vec_splat_s16(__B);\n       else\n-\trshift = vec_splats ((unsigned short) __B);\n+\t__rshift = vec_splats ((unsigned short) __B);\n \n-      result = vec_sr ((__v8hi) __A, rshift);\n+      __result = vec_sr ((__v8hi) __A, __rshift);\n     }\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srli_epi32 (__m128i __A, int __B)\n {\n-  __v4su rshift;\n-  __v4si result = { 0, 0, 0, 0 };\n+  __v4su __rshift;\n+  __v4si __result = { 0, 0, 0, 0 };\n \n   if (__B < 32)\n     {\n       if (__builtin_constant_p(__B))\n \t{\n \t  if (__B < 16)\n-\t      rshift = (__v4su) vec_splat_s32(__B);\n+\t      __rshift = (__v4su) vec_splat_s32(__B);\n \t    else\n-\t      rshift = (__v4su) vec_splats((unsigned int)__B);\n+\t      __rshift = (__v4su) vec_splats((unsigned int)__B);\n \t}\n       else\n-\trshift = vec_splats ((unsigned int) __B);\n+\t__rshift = vec_splats ((unsigned int) __B);\n \n-      result = vec_sr ((__v4si) __A, rshift);\n+      __result = vec_sr ((__v4si) __A, __rshift);\n     }\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n #ifdef _ARCH_PWR8\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srli_epi64 (__m128i __A, int __B)\n {\n-  __v2du rshift;\n-  __v2di result = { 0, 0 };\n+  __v2du __rshift;\n+  __v2di __result = { 0, 0 };\n \n   if (__B < 64)\n     {\n       if (__builtin_constant_p(__B))\n \t{\n \t  if (__B < 16)\n-\t      rshift = (__v2du) vec_splat_s32(__B);\n+\t      __rshift = (__v2du) vec_splat_s32(__B);\n \t    else\n-\t      rshift = (__v2du) vec_splats((unsigned long long)__B);\n+\t      __rshift = (__v2du) vec_splats((unsigned long long)__B);\n \t}\n       else\n-\trshift = (__v2du) vec_splats ((unsigned int) __B);\n+\t__rshift = (__v2du) vec_splats ((unsigned int) __B);\n \n-      result = vec_sr ((__v2di) __A, rshift);\n+      __result = vec_sr ((__v2di) __A, __rshift);\n     }\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n #endif\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_epi16 (__m128i __A, __m128i __B)\n {\n-  __v8hu lshift;\n-  __vector __bool short shmask;\n-  const __v8hu shmax = { 15, 15, 15, 15, 15, 15, 15, 15 };\n-  __v8hu result;\n+  __v8hu __lshift;\n+  __vector __bool short __shmask;\n+  const __v8hu __shmax = { 15, 15, 15, 15, 15, 15, 15, 15 };\n+  __v8hu __result;\n \n #ifdef __LITTLE_ENDIAN__\n-  lshift = vec_splat ((__v8hu) __B, 0);\n+  __lshift = vec_splat ((__v8hu) __B, 0);\n #else\n-  lshift = vec_splat ((__v8hu) __B, 3);\n+  __lshift = vec_splat ((__v8hu) __B, 3);\n #endif\n-  shmask = vec_cmple (lshift, shmax);\n-  result = vec_sl ((__v8hu) __A, lshift);\n-  result = vec_sel ((__v8hu) shmask, result, shmask);\n+  __shmask = vec_cmple (__lshift, __shmax);\n+  __result = vec_sl ((__v8hu) __A, __lshift);\n+  __result = vec_sel ((__v8hu) __shmask, __result, __shmask);\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_epi32 (__m128i __A, __m128i __B)\n {\n-  __v4su lshift;\n-  __vector __bool int shmask;\n-  const __v4su shmax = { 32, 32, 32, 32 };\n-  __v4su result;\n+  __v4su __lshift;\n+  __vector __bool int __shmask;\n+  const __v4su __shmax = { 32, 32, 32, 32 };\n+  __v4su __result;\n #ifdef __LITTLE_ENDIAN__\n-  lshift = vec_splat ((__v4su) __B, 0);\n+  __lshift = vec_splat ((__v4su) __B, 0);\n #else\n-  lshift = vec_splat ((__v4su) __B, 1);\n+  __lshift = vec_splat ((__v4su) __B, 1);\n #endif\n-  shmask = vec_cmplt (lshift, shmax);\n-  result = vec_sl ((__v4su) __A, lshift);\n-  result = vec_sel ((__v4su) shmask, result, shmask);\n+  __shmask = vec_cmplt (__lshift, __shmax);\n+  __result = vec_sl ((__v4su) __A, __lshift);\n+  __result = vec_sel ((__v4su) __shmask, __result, __shmask);\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n #ifdef _ARCH_PWR8\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_epi64 (__m128i __A, __m128i __B)\n {\n-  __v2du lshift;\n-  __vector __bool long long shmask;\n-  const __v2du shmax = { 64, 64 };\n-  __v2du result;\n+  __v2du __lshift;\n+  __vector __bool long long __shmask;\n+  const __v2du __shmax = { 64, 64 };\n+  __v2du __result;\n \n-  lshift = vec_splat ((__v2du) __B, 0);\n-  shmask = vec_cmplt (lshift, shmax);\n-  result = vec_sl ((__v2du) __A, lshift);\n-  result = vec_sel ((__v2du) shmask, result, shmask);\n+  __lshift = vec_splat ((__v2du) __B, 0);\n+  __shmask = vec_cmplt (__lshift, __shmax);\n+  __result = vec_sl ((__v2du) __A, __lshift);\n+  __result = vec_sel ((__v2du) __shmask, __result, __shmask);\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n #endif\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sra_epi16 (__m128i __A, __m128i __B)\n {\n-  const __v8hu rshmax = { 15, 15, 15, 15, 15, 15, 15, 15 };\n-  __v8hu rshift;\n-  __v8hi result;\n+  const __v8hu __rshmax = { 15, 15, 15, 15, 15, 15, 15, 15 };\n+  __v8hu __rshift;\n+  __v8hi __result;\n \n #ifdef __LITTLE_ENDIAN__\n-  rshift = vec_splat ((__v8hu)__B, 0);\n+  __rshift = vec_splat ((__v8hu)__B, 0);\n #else\n-  rshift = vec_splat ((__v8hu)__B, 3);\n+  __rshift = vec_splat ((__v8hu)__B, 3);\n #endif\n-  rshift = vec_min (rshift, rshmax);\n-  result = vec_sra ((__v8hi) __A, rshift);\n+  __rshift = vec_min (__rshift, __rshmax);\n+  __result = vec_sra ((__v8hi) __A, __rshift);\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sra_epi32 (__m128i __A, __m128i __B)\n {\n-  const __v4su rshmax = { 31, 31, 31, 31 };\n-  __v4su rshift;\n-  __v4si result;\n+  const __v4su __rshmax = { 31, 31, 31, 31 };\n+  __v4su __rshift;\n+  __v4si __result;\n \n #ifdef __LITTLE_ENDIAN__\n-  rshift = vec_splat ((__v4su)__B, 0);\n+  __rshift = vec_splat ((__v4su)__B, 0);\n #else\n-  rshift = vec_splat ((__v4su)__B, 1);\n+  __rshift = vec_splat ((__v4su)__B, 1);\n #endif\n-  rshift = vec_min (rshift, rshmax);\n-  result = vec_sra ((__v4si) __A, rshift);\n+  __rshift = vec_min (__rshift, __rshmax);\n+  __result = vec_sra ((__v4si) __A, __rshift);\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_epi16 (__m128i __A, __m128i __B)\n {\n-  __v8hu rshift;\n-  __vector __bool short shmask;\n-  const __v8hu shmax = { 15, 15, 15, 15, 15, 15, 15, 15 };\n-  __v8hu result;\n+  __v8hu __rshift;\n+  __vector __bool short __shmask;\n+  const __v8hu __shmax = { 15, 15, 15, 15, 15, 15, 15, 15 };\n+  __v8hu __result;\n \n #ifdef __LITTLE_ENDIAN__\n-  rshift = vec_splat ((__v8hu) __B, 0);\n+  __rshift = vec_splat ((__v8hu) __B, 0);\n #else\n-  rshift = vec_splat ((__v8hu) __B, 3);\n+  __rshift = vec_splat ((__v8hu) __B, 3);\n #endif\n-  shmask = vec_cmple (rshift, shmax);\n-  result = vec_sr ((__v8hu) __A, rshift);\n-  result = vec_sel ((__v8hu) shmask, result, shmask);\n+  __shmask = vec_cmple (__rshift, __shmax);\n+  __result = vec_sr ((__v8hu) __A, __rshift);\n+  __result = vec_sel ((__v8hu) __shmask, __result, __shmask);\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_epi32 (__m128i __A, __m128i __B)\n {\n-  __v4su rshift;\n-  __vector __bool int shmask;\n-  const __v4su shmax = { 32, 32, 32, 32 };\n-  __v4su result;\n+  __v4su __rshift;\n+  __vector __bool int __shmask;\n+  const __v4su __shmax = { 32, 32, 32, 32 };\n+  __v4su __result;\n \n #ifdef __LITTLE_ENDIAN__\n-  rshift = vec_splat ((__v4su) __B, 0);\n+  __rshift = vec_splat ((__v4su) __B, 0);\n #else\n-  rshift = vec_splat ((__v4su) __B, 1);\n+  __rshift = vec_splat ((__v4su) __B, 1);\n #endif\n-  shmask = vec_cmplt (rshift, shmax);\n-  result = vec_sr ((__v4su) __A, rshift);\n-  result = vec_sel ((__v4su) shmask, result, shmask);\n+  __shmask = vec_cmplt (__rshift, __shmax);\n+  __result = vec_sr ((__v4su) __A, __rshift);\n+  __result = vec_sel ((__v4su) __shmask, __result, __shmask);\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n #ifdef _ARCH_PWR8\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_epi64 (__m128i __A, __m128i __B)\n {\n-  __v2du rshift;\n-  __vector __bool long long shmask;\n-  const __v2du shmax = { 64, 64 };\n-  __v2du result;\n+  __v2du __rshift;\n+  __vector __bool long long __shmask;\n+  const __v2du __shmax = { 64, 64 };\n+  __v2du __result;\n \n-  rshift = vec_splat ((__v2du) __B, 0);\n-  shmask = vec_cmplt (rshift, shmax);\n-  result = vec_sr ((__v2du) __A, rshift);\n-  result = vec_sel ((__v2du) shmask, result, shmask);\n+  __rshift = vec_splat ((__v2du) __B, 0);\n+  __shmask = vec_cmplt (__rshift, __shmax);\n+  __result = vec_sr ((__v2du) __A, __rshift);\n+  __result = vec_sel ((__v2du) __shmask, __result, __shmask);\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n #endif\n \n@@ -1994,11 +1994,11 @@ _mm_extract_epi16 (__m128i const __A, int const __N)\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_epi16 (__m128i const __A, int const __D, int const __N)\n {\n-  __v8hi result = (__v8hi)__A;\n+  __v8hi __result = (__v8hi)__A;\n \n-  result [(__N & 7)] = __D;\n+  __result [(__N & 7)] = __D;\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -2037,21 +2037,21 @@ _mm_movemask_epi8 (__m128i __A)\n #ifdef _ARCH_PWR10\n   return vec_extractm ((__v16qu) __A);\n #else\n-  __vector unsigned long long result;\n-  static const __vector unsigned char perm_mask =\n+  __vector unsigned long long __result;\n+  static const __vector unsigned char __perm_mask =\n     {\n \t0x78, 0x70, 0x68, 0x60, 0x58, 0x50, 0x48, 0x40,\n \t0x38, 0x30, 0x28, 0x20, 0x18, 0x10, 0x08, 0x00\n     };\n \n-  result = ((__vector unsigned long long)\n+  __result = ((__vector unsigned long long)\n \t    vec_vbpermq ((__vector unsigned char) __A,\n-\t\t\t (__vector unsigned char) perm_mask));\n+\t\t\t (__vector unsigned char) __perm_mask));\n \n #ifdef __LITTLE_ENDIAN__\n-  return result[1];\n+  return __result[1];\n #else\n-  return result[0];\n+  return __result[0];\n #endif\n #endif /* !_ARCH_PWR10 */\n }\n@@ -2060,8 +2060,8 @@ _mm_movemask_epi8 (__m128i __A)\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mulhi_epu16 (__m128i __A, __m128i __B)\n {\n-  __v4su w0, w1;\n-  __v16qu xform1 = {\n+  __v4su __w0, __w1;\n+  __v16qu __xform1 = {\n #ifdef __LITTLE_ENDIAN__\n       0x02, 0x03, 0x12, 0x13,  0x06, 0x07, 0x16, 0x17,\n       0x0A, 0x0B, 0x1A, 0x1B,  0x0E, 0x0F, 0x1E, 0x1F\n@@ -2071,113 +2071,113 @@ _mm_mulhi_epu16 (__m128i __A, __m128i __B)\n #endif\n     };\n \n-  w0 = vec_vmuleuh ((__v8hu)__A, (__v8hu)__B);\n-  w1 = vec_vmulouh ((__v8hu)__A, (__v8hu)__B);\n-  return (__m128i) vec_perm (w0, w1, xform1);\n+  __w0 = vec_vmuleuh ((__v8hu)__A, (__v8hu)__B);\n+  __w1 = vec_vmulouh ((__v8hu)__A, (__v8hu)__B);\n+  return (__m128i) vec_perm (__w0, __w1, __xform1);\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shufflehi_epi16 (__m128i __A, const int __mask)\n {\n-  unsigned long element_selector_98 = __mask & 0x03;\n-  unsigned long element_selector_BA = (__mask >> 2) & 0x03;\n-  unsigned long element_selector_DC = (__mask >> 4) & 0x03;\n-  unsigned long element_selector_FE = (__mask >> 6) & 0x03;\n-  static const unsigned short permute_selectors[4] =\n+  unsigned long __element_selector_98 = __mask & 0x03;\n+  unsigned long __element_selector_BA = (__mask >> 2) & 0x03;\n+  unsigned long __element_selector_DC = (__mask >> 4) & 0x03;\n+  unsigned long __element_selector_FE = (__mask >> 6) & 0x03;\n+  static const unsigned short __permute_selectors[4] =\n     {\n #ifdef __LITTLE_ENDIAN__\n \t      0x0908, 0x0B0A, 0x0D0C, 0x0F0E\n #else\n \t      0x0809, 0x0A0B, 0x0C0D, 0x0E0F\n #endif\n     };\n-  __v2du pmask =\n+  __v2du __pmask =\n #ifdef __LITTLE_ENDIAN__\n       { 0x1716151413121110UL,  0UL};\n #else\n       { 0x1011121314151617UL,  0UL};\n #endif\n-  __m64_union t;\n-  __v2du a, r;\n+  __m64_union __t;\n+  __v2du __a, __r;\n \n-  t.as_short[0] = permute_selectors[element_selector_98];\n-  t.as_short[1] = permute_selectors[element_selector_BA];\n-  t.as_short[2] = permute_selectors[element_selector_DC];\n-  t.as_short[3] = permute_selectors[element_selector_FE];\n-  pmask[1] = t.as_m64;\n-  a = (__v2du)__A;\n-  r = vec_perm (a, a, (__vector unsigned char)pmask);\n-  return (__m128i) r;\n+  __t.as_short[0] = __permute_selectors[__element_selector_98];\n+  __t.as_short[1] = __permute_selectors[__element_selector_BA];\n+  __t.as_short[2] = __permute_selectors[__element_selector_DC];\n+  __t.as_short[3] = __permute_selectors[__element_selector_FE];\n+  __pmask[1] = __t.as_m64;\n+  __a = (__v2du)__A;\n+  __r = vec_perm (__a, __a, (__vector unsigned char)__pmask);\n+  return (__m128i) __r;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shufflelo_epi16 (__m128i __A, const int __mask)\n {\n-  unsigned long element_selector_10 = __mask & 0x03;\n-  unsigned long element_selector_32 = (__mask >> 2) & 0x03;\n-  unsigned long element_selector_54 = (__mask >> 4) & 0x03;\n-  unsigned long element_selector_76 = (__mask >> 6) & 0x03;\n-  static const unsigned short permute_selectors[4] =\n+  unsigned long __element_selector_10 = __mask & 0x03;\n+  unsigned long __element_selector_32 = (__mask >> 2) & 0x03;\n+  unsigned long __element_selector_54 = (__mask >> 4) & 0x03;\n+  unsigned long __element_selector_76 = (__mask >> 6) & 0x03;\n+  static const unsigned short __permute_selectors[4] =\n     {\n #ifdef __LITTLE_ENDIAN__\n \t      0x0100, 0x0302, 0x0504, 0x0706\n #else\n \t      0x0001, 0x0203, 0x0405, 0x0607\n #endif\n     };\n-  __v2du pmask =\n+  __v2du __pmask =\n #ifdef __LITTLE_ENDIAN__\n                  { 0UL,  0x1f1e1d1c1b1a1918UL};\n #else\n                  { 0UL,  0x18191a1b1c1d1e1fUL};\n #endif\n-  __m64_union t;\n-  __v2du a, r;\n-  t.as_short[0] = permute_selectors[element_selector_10];\n-  t.as_short[1] = permute_selectors[element_selector_32];\n-  t.as_short[2] = permute_selectors[element_selector_54];\n-  t.as_short[3] = permute_selectors[element_selector_76];\n-  pmask[0] = t.as_m64;\n-  a = (__v2du)__A;\n-  r = vec_perm (a, a, (__vector unsigned char)pmask);\n-  return (__m128i) r;\n+  __m64_union __t;\n+  __v2du __a, __r;\n+  __t.as_short[0] = __permute_selectors[__element_selector_10];\n+  __t.as_short[1] = __permute_selectors[__element_selector_32];\n+  __t.as_short[2] = __permute_selectors[__element_selector_54];\n+  __t.as_short[3] = __permute_selectors[__element_selector_76];\n+  __pmask[0] = __t.as_m64;\n+  __a = (__v2du)__A;\n+  __r = vec_perm (__a, __a, (__vector unsigned char)__pmask);\n+  return (__m128i) __r;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shuffle_epi32 (__m128i __A, const int __mask)\n {\n-  unsigned long element_selector_10 = __mask & 0x03;\n-  unsigned long element_selector_32 = (__mask >> 2) & 0x03;\n-  unsigned long element_selector_54 = (__mask >> 4) & 0x03;\n-  unsigned long element_selector_76 = (__mask >> 6) & 0x03;\n-  static const unsigned int permute_selectors[4] =\n+  unsigned long __element_selector_10 = __mask & 0x03;\n+  unsigned long __element_selector_32 = (__mask >> 2) & 0x03;\n+  unsigned long __element_selector_54 = (__mask >> 4) & 0x03;\n+  unsigned long __element_selector_76 = (__mask >> 6) & 0x03;\n+  static const unsigned int __permute_selectors[4] =\n     {\n #ifdef __LITTLE_ENDIAN__\n \t0x03020100, 0x07060504, 0x0B0A0908, 0x0F0E0D0C\n #else\n       0x00010203, 0x04050607, 0x08090A0B, 0x0C0D0E0F\n #endif\n     };\n-  __v4su t;\n+  __v4su __t;\n \n-  t[0] = permute_selectors[element_selector_10];\n-  t[1] = permute_selectors[element_selector_32];\n-  t[2] = permute_selectors[element_selector_54] + 0x10101010;\n-  t[3] = permute_selectors[element_selector_76] + 0x10101010;\n-  return (__m128i)vec_perm ((__v4si) __A, (__v4si)__A, (__vector unsigned char)t);\n+  __t[0] = __permute_selectors[__element_selector_10];\n+  __t[1] = __permute_selectors[__element_selector_32];\n+  __t[2] = __permute_selectors[__element_selector_54] + 0x10101010;\n+  __t[3] = __permute_selectors[__element_selector_76] + 0x10101010;\n+  return (__m128i)vec_perm ((__v4si) __A, (__v4si)__A, (__vector unsigned char)__t);\n }\n \n extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskmoveu_si128 (__m128i __A, __m128i __B, char *__C)\n {\n-  __v2du hibit = { 0x7f7f7f7f7f7f7f7fUL, 0x7f7f7f7f7f7f7f7fUL};\n-  __v16qu mask, tmp;\n-  __m128i_u *p = (__m128i_u*)__C;\n+  __v2du __hibit = { 0x7f7f7f7f7f7f7f7fUL, 0x7f7f7f7f7f7f7f7fUL};\n+  __v16qu __mask, __tmp;\n+  __m128i_u *__p = (__m128i_u*)__C;\n \n-  tmp = (__v16qu)_mm_loadu_si128(p);\n-  mask = (__v16qu)vec_cmpgt ((__v16qu)__B, (__v16qu)hibit);\n-  tmp = vec_sel (tmp, (__v16qu)__A, mask);\n-  _mm_storeu_si128 (p, (__m128i)tmp);\n+  __tmp = (__v16qu)_mm_loadu_si128(__p);\n+  __mask = (__v16qu)vec_cmpgt ((__v16qu)__B, (__v16qu)__hibit);\n+  __tmp = vec_sel (__tmp, (__v16qu)__A, __mask);\n+  _mm_storeu_si128 (__p, (__m128i)__tmp);\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -2196,26 +2196,26 @@ _mm_avg_epu16 (__m128i __A, __m128i __B)\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sad_epu8 (__m128i __A, __m128i __B)\n {\n-  __v16qu a, b;\n-  __v16qu vabsdiff;\n-  __v4si vsum;\n-  const __v4su zero = { 0, 0, 0, 0 };\n-  __v4si result;\n+  __v16qu __a, __b;\n+  __v16qu __vabsdiff;\n+  __v4si __vsum;\n+  const __v4su __zero = { 0, 0, 0, 0 };\n+  __v4si __result;\n \n-  a = (__v16qu) __A;\n-  b = (__v16qu) __B;\n+  __a = (__v16qu) __A;\n+  __b = (__v16qu) __B;\n #ifndef _ARCH_PWR9\n-  __v16qu vmin = vec_min (a, b);\n-  __v16qu vmax = vec_max (a, b);\n-  vabsdiff = vec_sub (vmax, vmin);\n+  __v16qu __vmin = vec_min (__a, __b);\n+  __v16qu __vmax = vec_max (__a, __b);\n+  __vabsdiff = vec_sub (__vmax, __vmin);\n #else\n-  vabsdiff = vec_absd (a, b);\n+  __vabsdiff = vec_absd (__a, __b);\n #endif\n   /* Sum four groups of bytes into integers.  */\n-  vsum = (__vector signed int) vec_sum4s (vabsdiff, zero);\n+  __vsum = (__vector signed int) vec_sum4s (__vabsdiff, __zero);\n #ifdef __LITTLE_ENDIAN__\n   /* Sum across four integers with two integer results.  */\n-  __asm__ (\"vsum2sws %0,%1,%2\" : \"=v\" (result) : \"v\" (vsum), \"v\" (zero));\n+  __asm__ (\"vsum2sws %0,%1,%2\" : \"=v\" (__result) : \"v\" (__vsum), \"v\" (__zero));\n   /* Note: vec_sum2s could be used here, but on little-endian, vector\n      shifts are added that are not needed for this use-case.\n      A vector shift to correctly position the 32-bit integer results\n@@ -2224,11 +2224,11 @@ _mm_sad_epu8 (__m128i __A, __m128i __B)\n      integers ([1]|[0] and [3]|[2]).  Thus, no shift is performed.  */\n #else\n   /* Sum across four integers with two integer results.  */\n-  result = vec_sum2s (vsum, (__vector signed int) zero);\n+  __result = vec_sum2s (__vsum, (__vector signed int) __zero);\n   /* Rotate the sums into the correct position.  */\n-  result = vec_sld (result, result, 6);\n+  __result = vec_sld (__result, __result, 6);\n #endif\n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))"}, {"sha": "721f7563b1c7c6840d0ab83e6489a2efb8548046", "filename": "gcc/config/rs6000/mm_malloc.h", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fmm_malloc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fmm_malloc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fmm_malloc.h?ref=efbb17db52afd802300c4dcce208fab326ec2915", "patch": "@@ -35,28 +35,28 @@ extern \"C\" int posix_memalign (void **, size_t, size_t) throw ();\n #endif\n \n static __inline void *\n-_mm_malloc (size_t size, size_t alignment)\n+_mm_malloc (size_t __size, size_t __alignment)\n {\n   /* PowerPC64 ELF V2 ABI requires quadword alignment.  */\n-  size_t vec_align = sizeof (__vector float);\n+  size_t __vec_align = sizeof (__vector float);\n   /* Linux GLIBC malloc alignment is at least 2 X ptr size.  */\n-  size_t malloc_align = (sizeof (void *) + sizeof (void *));\n-  void *ptr;\n-\n-  if (alignment == malloc_align && alignment == vec_align)\n-    return malloc (size);\n-  if (alignment < vec_align)\n-    alignment = vec_align;\n-  if (posix_memalign (&ptr, alignment, size) == 0)\n-    return ptr;\n+  size_t __malloc_align = (sizeof (void *) + sizeof (void *));\n+  void *__ptr;\n+\n+  if (__alignment == __malloc_align && __alignment == __vec_align)\n+    return malloc (__size);\n+  if (__alignment < __vec_align)\n+    __alignment = __vec_align;\n+  if (__posix_memalign (&__ptr, __alignment, __size) == 0)\n+    return __ptr;\n   else\n     return NULL;\n }\n \n static __inline void\n-_mm_free (void * ptr)\n+_mm_free (void * __ptr)\n {\n-  free (ptr);\n+  free (__ptr);\n }\n \n #endif /* _MM_MALLOC_H_INCLUDED */"}, {"sha": "bf7f3b1c7018dc500a4f54f8c035630a429c3e94", "filename": "gcc/config/rs6000/mmintrin.h", "status": "modified", "additions": 384, "deletions": 384, "changes": 768, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fmmintrin.h?ref=efbb17db52afd802300c4dcce208fab326ec2915", "patch": "@@ -170,17 +170,17 @@ _mm_cvtsi64_si64x (__m64 __i)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_packs_pi16 (__m64 __m1, __m64 __m2)\n {\n-  __vector signed short vm1;\n-  __vector signed char vresult;\n+  __vector signed short __vm1;\n+  __vector signed char __vresult;\n \n-  vm1 = (__vector signed short) (__vector unsigned long long)\n+  __vm1 = (__vector signed short) (__vector unsigned long long)\n #ifdef __LITTLE_ENDIAN__\n         { __m1, __m2 };\n #else\n         { __m2, __m1 };\n #endif\n-  vresult = vec_packs (vm1, vm1);\n-  return (__m64) ((__vector long long) vresult)[0];\n+  __vresult = vec_packs (__vm1, __vm1);\n+  return (__m64) ((__vector long long) __vresult)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -195,17 +195,17 @@ _m_packsswb (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_packs_pi32 (__m64 __m1, __m64 __m2)\n {\n-  __vector signed int vm1;\n-  __vector signed short vresult;\n+  __vector signed int __vm1;\n+  __vector signed short __vresult;\n \n-  vm1 = (__vector signed int) (__vector unsigned long long)\n+  __vm1 = (__vector signed int) (__vector unsigned long long)\n #ifdef __LITTLE_ENDIAN__\n         { __m1, __m2 };\n #else\n         { __m2, __m1 };\n #endif\n-  vresult = vec_packs (vm1, vm1);\n-  return (__m64) ((__vector long long) vresult)[0];\n+  __vresult = vec_packs (__vm1, __vm1);\n+  return (__m64) ((__vector long long) __vresult)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -220,19 +220,19 @@ _m_packssdw (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_packs_pu16 (__m64 __m1, __m64 __m2)\n {\n-  __vector unsigned char r;\n-  __vector signed short vm1 = (__vector signed short) (__vector long long)\n+  __vector unsigned char __r;\n+  __vector signed short __vm1 = (__vector signed short) (__vector long long)\n #ifdef __LITTLE_ENDIAN__\n         { __m1, __m2 };\n #else\n         { __m2, __m1 };\n #endif\n   const __vector signed short __zero = { 0 };\n-  __vector __bool short __select = vec_cmplt (vm1, __zero);\n-  r = vec_packs ((__vector unsigned short) vm1, (__vector unsigned short) vm1);\n-  __vector __bool char packsel = vec_pack (__select, __select);\n-  r = vec_sel (r, (const __vector unsigned char) __zero, packsel);\n-  return (__m64) ((__vector long long) r)[0];\n+  __vector __bool short __select = vec_cmplt (__vm1, __zero);\n+  __r = vec_packs ((__vector unsigned short) __vm1, (__vector unsigned short) __vm1);\n+  __vector __bool char __packsel = vec_pack (__select, __select);\n+  __r = vec_sel (__r, (const __vector unsigned char) __zero, __packsel);\n+  return (__m64) ((__vector long long) __r)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -248,28 +248,28 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_unpackhi_pi8 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR8\n-  __vector unsigned char a, b, c;\n+  __vector unsigned char __a, __b, __c;\n \n-  a = (__vector unsigned char)vec_splats (__m1);\n-  b = (__vector unsigned char)vec_splats (__m2);\n-  c = vec_mergel (a, b);\n-  return (__m64) ((__vector long long) c)[1];\n+  __a = (__vector unsigned char)vec_splats (__m1);\n+  __b = (__vector unsigned char)vec_splats (__m2);\n+  __c = vec_mergel (__a, __b);\n+  return (__m64) ((__vector long long) __c)[1];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_char[0] = m1.as_char[4];\n-  res.as_char[1] = m2.as_char[4];\n-  res.as_char[2] = m1.as_char[5];\n-  res.as_char[3] = m2.as_char[5];\n-  res.as_char[4] = m1.as_char[6];\n-  res.as_char[5] = m2.as_char[6];\n-  res.as_char[6] = m1.as_char[7];\n-  res.as_char[7] = m2.as_char[7];\n+  __res.as_char[0] = __mu1.as_char[4];\n+  __res.as_char[1] = __mu2.as_char[4];\n+  __res.as_char[2] = __mu1.as_char[5];\n+  __res.as_char[3] = __mu2.as_char[5];\n+  __res.as_char[4] = __mu1.as_char[6];\n+  __res.as_char[5] = __mu2.as_char[6];\n+  __res.as_char[6] = __mu1.as_char[7];\n+  __res.as_char[7] = __mu2.as_char[7];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -284,17 +284,17 @@ _m_punpckhbw (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_pi16 (__m64 __m1, __m64 __m2)\n {\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_short[0] = m1.as_short[2];\n-  res.as_short[1] = m2.as_short[2];\n-  res.as_short[2] = m1.as_short[3];\n-  res.as_short[3] = m2.as_short[3];\n+  __res.as_short[0] = __mu1.as_short[2];\n+  __res.as_short[1] = __mu2.as_short[2];\n+  __res.as_short[2] = __mu1.as_short[3];\n+  __res.as_short[3] = __mu2.as_short[3];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -307,15 +307,15 @@ _m_punpckhwd (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_pi32 (__m64 __m1, __m64 __m2)\n {\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_int[0] = m1.as_int[1];\n-  res.as_int[1] = m2.as_int[1];\n+  __res.as_int[0] = __mu1.as_int[1];\n+  __res.as_int[1] = __mu2.as_int[1];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -329,28 +329,28 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_unpacklo_pi8 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR8\n-  __vector unsigned char a, b, c;\n+  __vector unsigned char __a, __b, __c;\n \n-  a = (__vector unsigned char)vec_splats (__m1);\n-  b = (__vector unsigned char)vec_splats (__m2);\n-  c = vec_mergel (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector unsigned char)vec_splats (__m1);\n+  __b = (__vector unsigned char)vec_splats (__m2);\n+  __c = vec_mergel (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_char[0] = m1.as_char[0];\n-  res.as_char[1] = m2.as_char[0];\n-  res.as_char[2] = m1.as_char[1];\n-  res.as_char[3] = m2.as_char[1];\n-  res.as_char[4] = m1.as_char[2];\n-  res.as_char[5] = m2.as_char[2];\n-  res.as_char[6] = m1.as_char[3];\n-  res.as_char[7] = m2.as_char[3];\n+  __res.as_char[0] = __mu1.as_char[0];\n+  __res.as_char[1] = __mu2.as_char[0];\n+  __res.as_char[2] = __mu1.as_char[1];\n+  __res.as_char[3] = __mu2.as_char[1];\n+  __res.as_char[4] = __mu1.as_char[2];\n+  __res.as_char[5] = __mu2.as_char[2];\n+  __res.as_char[6] = __mu1.as_char[3];\n+  __res.as_char[7] = __mu2.as_char[3];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -364,17 +364,17 @@ _m_punpcklbw (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_pi16 (__m64 __m1, __m64 __m2)\n {\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_short[0] = m1.as_short[0];\n-  res.as_short[1] = m2.as_short[0];\n-  res.as_short[2] = m1.as_short[1];\n-  res.as_short[3] = m2.as_short[1];\n+  __res.as_short[0] = __mu1.as_short[0];\n+  __res.as_short[1] = __mu2.as_short[0];\n+  __res.as_short[2] = __mu1.as_short[1];\n+  __res.as_short[3] = __mu2.as_short[1];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -388,15 +388,15 @@ _m_punpcklwd (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_pi32 (__m64 __m1, __m64 __m2)\n {\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_int[0] = m1.as_int[0];\n-  res.as_int[1] = m2.as_int[0];\n+  __res.as_int[0] = __mu1.as_int[0];\n+  __res.as_int[1] = __mu2.as_int[0];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -410,28 +410,28 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_add_pi8 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR8\n-  __vector signed char a, b, c;\n+  __vector signed char __a, __b, __c;\n \n-  a = (__vector signed char)vec_splats (__m1);\n-  b = (__vector signed char)vec_splats (__m2);\n-  c = vec_add (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed char)vec_splats (__m1);\n+  __b = (__vector signed char)vec_splats (__m2);\n+  __c = vec_add (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_char[0] = m1.as_char[0] + m2.as_char[0];\n-  res.as_char[1] = m1.as_char[1] + m2.as_char[1];\n-  res.as_char[2] = m1.as_char[2] + m2.as_char[2];\n-  res.as_char[3] = m1.as_char[3] + m2.as_char[3];\n-  res.as_char[4] = m1.as_char[4] + m2.as_char[4];\n-  res.as_char[5] = m1.as_char[5] + m2.as_char[5];\n-  res.as_char[6] = m1.as_char[6] + m2.as_char[6];\n-  res.as_char[7] = m1.as_char[7] + m2.as_char[7];\n+  __res.as_char[0] = __mu1.as_char[0] + __mu2.as_char[0];\n+  __res.as_char[1] = __mu1.as_char[1] + __mu2.as_char[1];\n+  __res.as_char[2] = __mu1.as_char[2] + __mu2.as_char[2];\n+  __res.as_char[3] = __mu1.as_char[3] + __mu2.as_char[3];\n+  __res.as_char[4] = __mu1.as_char[4] + __mu2.as_char[4];\n+  __res.as_char[5] = __mu1.as_char[5] + __mu2.as_char[5];\n+  __res.as_char[6] = __mu1.as_char[6] + __mu2.as_char[6];\n+  __res.as_char[7] = __mu1.as_char[7] + __mu2.as_char[7];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -446,24 +446,24 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_add_pi16 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR8\n-  __vector signed short a, b, c;\n+  __vector signed short __a, __b, __c;\n \n-  a = (__vector signed short)vec_splats (__m1);\n-  b = (__vector signed short)vec_splats (__m2);\n-  c = vec_add (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed short)vec_splats (__m1);\n+  __b = (__vector signed short)vec_splats (__m2);\n+  __c = vec_add (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_short[0] = m1.as_short[0] + m2.as_short[0];\n-  res.as_short[1] = m1.as_short[1] + m2.as_short[1];\n-  res.as_short[2] = m1.as_short[2] + m2.as_short[2];\n-  res.as_short[3] = m1.as_short[3] + m2.as_short[3];\n+  __res.as_short[0] = __mu1.as_short[0] + __mu2.as_short[0];\n+  __res.as_short[1] = __mu1.as_short[1] + __mu2.as_short[1];\n+  __res.as_short[2] = __mu1.as_short[2] + __mu2.as_short[2];\n+  __res.as_short[3] = __mu1.as_short[3] + __mu2.as_short[3];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -478,22 +478,22 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_add_pi32 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR9\n-  __vector signed int a, b, c;\n+  __vector signed int __a, __b, __c;\n \n-  a = (__vector signed int)vec_splats (__m1);\n-  b = (__vector signed int)vec_splats (__m2);\n-  c = vec_add (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed int)vec_splats (__m1);\n+  __b = (__vector signed int)vec_splats (__m2);\n+  __c = vec_add (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_int[0] = m1.as_int[0] + m2.as_int[0];\n-  res.as_int[1] = m1.as_int[1] + m2.as_int[1];\n+  __res.as_int[0] = __mu1.as_int[0] + __mu2.as_int[0];\n+  __res.as_int[1] = __mu1.as_int[1] + __mu2.as_int[1];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -508,28 +508,28 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_sub_pi8 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR8\n-  __vector signed char a, b, c;\n+  __vector signed char __a, __b, __c;\n \n-  a = (__vector signed char)vec_splats (__m1);\n-  b = (__vector signed char)vec_splats (__m2);\n-  c = vec_sub (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed char)vec_splats (__m1);\n+  __b = (__vector signed char)vec_splats (__m2);\n+  __c = vec_sub (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_char[0] = m1.as_char[0] - m2.as_char[0];\n-  res.as_char[1] = m1.as_char[1] - m2.as_char[1];\n-  res.as_char[2] = m1.as_char[2] - m2.as_char[2];\n-  res.as_char[3] = m1.as_char[3] - m2.as_char[3];\n-  res.as_char[4] = m1.as_char[4] - m2.as_char[4];\n-  res.as_char[5] = m1.as_char[5] - m2.as_char[5];\n-  res.as_char[6] = m1.as_char[6] - m2.as_char[6];\n-  res.as_char[7] = m1.as_char[7] - m2.as_char[7];\n+  __res.as_char[0] = __mu1.as_char[0] - __mu2.as_char[0];\n+  __res.as_char[1] = __mu1.as_char[1] - __mu2.as_char[1];\n+  __res.as_char[2] = __mu1.as_char[2] - __mu2.as_char[2];\n+  __res.as_char[3] = __mu1.as_char[3] - __mu2.as_char[3];\n+  __res.as_char[4] = __mu1.as_char[4] - __mu2.as_char[4];\n+  __res.as_char[5] = __mu1.as_char[5] - __mu2.as_char[5];\n+  __res.as_char[6] = __mu1.as_char[6] - __mu2.as_char[6];\n+  __res.as_char[7] = __mu1.as_char[7] - __mu2.as_char[7];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -544,24 +544,24 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_sub_pi16 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR8\n-  __vector signed short a, b, c;\n+  __vector signed short __a, __b, __c;\n \n-  a = (__vector signed short)vec_splats (__m1);\n-  b = (__vector signed short)vec_splats (__m2);\n-  c = vec_sub (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed short)vec_splats (__m1);\n+  __b = (__vector signed short)vec_splats (__m2);\n+  __c = vec_sub (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_short[0] = m1.as_short[0] - m2.as_short[0];\n-  res.as_short[1] = m1.as_short[1] - m2.as_short[1];\n-  res.as_short[2] = m1.as_short[2] - m2.as_short[2];\n-  res.as_short[3] = m1.as_short[3] - m2.as_short[3];\n+  __res.as_short[0] = __mu1.as_short[0] - __mu2.as_short[0];\n+  __res.as_short[1] = __mu1.as_short[1] - __mu2.as_short[1];\n+  __res.as_short[2] = __mu1.as_short[2] - __mu2.as_short[2];\n+  __res.as_short[3] = __mu1.as_short[3] - __mu2.as_short[3];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -576,22 +576,22 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_sub_pi32 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR9\n-  __vector signed int a, b, c;\n+  __vector signed int __a, __b, __c;\n \n-  a = (__vector signed int)vec_splats (__m1);\n-  b = (__vector signed int)vec_splats (__m2);\n-  c = vec_sub (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed int)vec_splats (__m1);\n+  __b = (__vector signed int)vec_splats (__m2);\n+  __c = vec_sub (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_int[0] = m1.as_int[0] - m2.as_int[0];\n-  res.as_int[1] = m1.as_int[1] - m2.as_int[1];\n+  __res.as_int[0] = __mu1.as_int[0] - __mu2.as_int[0];\n+  __res.as_int[1] = __mu1.as_int[1] - __mu2.as_int[1];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -729,30 +729,30 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_cmpeq_pi8 (__m64 __m1, __m64 __m2)\n {\n #if defined(_ARCH_PWR6) && defined(__powerpc64__)\n-  __m64 res;\n+  __m64 __res;\n   __asm__(\n       \"cmpb %0,%1,%2;\\n\"\n-      : \"=r\" (res)\n+      : \"=r\" (__res)\n       : \"r\" (__m1),\n \t\"r\" (__m2)\n       : );\n-  return (res);\n+  return (__res);\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_char[0] = (m1.as_char[0] == m2.as_char[0])? -1: 0;\n-  res.as_char[1] = (m1.as_char[1] == m2.as_char[1])? -1: 0;\n-  res.as_char[2] = (m1.as_char[2] == m2.as_char[2])? -1: 0;\n-  res.as_char[3] = (m1.as_char[3] == m2.as_char[3])? -1: 0;\n-  res.as_char[4] = (m1.as_char[4] == m2.as_char[4])? -1: 0;\n-  res.as_char[5] = (m1.as_char[5] == m2.as_char[5])? -1: 0;\n-  res.as_char[6] = (m1.as_char[6] == m2.as_char[6])? -1: 0;\n-  res.as_char[7] = (m1.as_char[7] == m2.as_char[7])? -1: 0;\n+  __res.as_char[0] = (__mu1.as_char[0] == __mu2.as_char[0])? -1: 0;\n+  __res.as_char[1] = (__mu1.as_char[1] == __mu2.as_char[1])? -1: 0;\n+  __res.as_char[2] = (__mu1.as_char[2] == __mu2.as_char[2])? -1: 0;\n+  __res.as_char[3] = (__mu1.as_char[3] == __mu2.as_char[3])? -1: 0;\n+  __res.as_char[4] = (__mu1.as_char[4] == __mu2.as_char[4])? -1: 0;\n+  __res.as_char[5] = (__mu1.as_char[5] == __mu2.as_char[5])? -1: 0;\n+  __res.as_char[6] = (__mu1.as_char[6] == __mu2.as_char[6])? -1: 0;\n+  __res.as_char[7] = (__mu1.as_char[7] == __mu2.as_char[7])? -1: 0;\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -766,28 +766,28 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_cmpgt_pi8 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR8\n-  __vector signed char a, b, c;\n+  __vector signed char __a, __b, __c;\n \n-  a = (__vector signed char)vec_splats (__m1);\n-  b = (__vector signed char)vec_splats (__m2);\n-  c = (__vector signed char)vec_cmpgt (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed char)vec_splats (__m1);\n+  __b = (__vector signed char)vec_splats (__m2);\n+  __c = (__vector signed char)vec_cmpgt (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_char[0] = (m1.as_char[0] > m2.as_char[0])? -1: 0;\n-  res.as_char[1] = (m1.as_char[1] > m2.as_char[1])? -1: 0;\n-  res.as_char[2] = (m1.as_char[2] > m2.as_char[2])? -1: 0;\n-  res.as_char[3] = (m1.as_char[3] > m2.as_char[3])? -1: 0;\n-  res.as_char[4] = (m1.as_char[4] > m2.as_char[4])? -1: 0;\n-  res.as_char[5] = (m1.as_char[5] > m2.as_char[5])? -1: 0;\n-  res.as_char[6] = (m1.as_char[6] > m2.as_char[6])? -1: 0;\n-  res.as_char[7] = (m1.as_char[7] > m2.as_char[7])? -1: 0;\n+  __res.as_char[0] = (__mu1.as_char[0] > __mu2.as_char[0])? -1: 0;\n+  __res.as_char[1] = (__mu1.as_char[1] > __mu2.as_char[1])? -1: 0;\n+  __res.as_char[2] = (__mu1.as_char[2] > __mu2.as_char[2])? -1: 0;\n+  __res.as_char[3] = (__mu1.as_char[3] > __mu2.as_char[3])? -1: 0;\n+  __res.as_char[4] = (__mu1.as_char[4] > __mu2.as_char[4])? -1: 0;\n+  __res.as_char[5] = (__mu1.as_char[5] > __mu2.as_char[5])? -1: 0;\n+  __res.as_char[6] = (__mu1.as_char[6] > __mu2.as_char[6])? -1: 0;\n+  __res.as_char[7] = (__mu1.as_char[7] > __mu2.as_char[7])? -1: 0;\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -803,24 +803,24 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_cmpeq_pi16 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR8\n-  __vector signed short a, b, c;\n+  __vector signed short __a, __b, __c;\n \n-  a = (__vector signed short)vec_splats (__m1);\n-  b = (__vector signed short)vec_splats (__m2);\n-  c = (__vector signed short)vec_cmpeq (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed short)vec_splats (__m1);\n+  __b = (__vector signed short)vec_splats (__m2);\n+  __c = (__vector signed short)vec_cmpeq (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_short[0] = (m1.as_short[0] == m2.as_short[0])? -1: 0;\n-  res.as_short[1] = (m1.as_short[1] == m2.as_short[1])? -1: 0;\n-  res.as_short[2] = (m1.as_short[2] == m2.as_short[2])? -1: 0;\n-  res.as_short[3] = (m1.as_short[3] == m2.as_short[3])? -1: 0;\n+  __res.as_short[0] = (__mu1.as_short[0] == __mu2.as_short[0])? -1: 0;\n+  __res.as_short[1] = (__mu1.as_short[1] == __mu2.as_short[1])? -1: 0;\n+  __res.as_short[2] = (__mu1.as_short[2] == __mu2.as_short[2])? -1: 0;\n+  __res.as_short[3] = (__mu1.as_short[3] == __mu2.as_short[3])? -1: 0;\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -834,24 +834,24 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_cmpgt_pi16 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR8\n-  __vector signed short a, b, c;\n+  __vector signed short __a, __b, __c;\n \n-  a = (__vector signed short)vec_splats (__m1);\n-  b = (__vector signed short)vec_splats (__m2);\n-  c = (__vector signed short)vec_cmpgt (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed short)vec_splats (__m1);\n+  __b = (__vector signed short)vec_splats (__m2);\n+  __c = (__vector signed short)vec_cmpgt (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_short[0] = (m1.as_short[0] > m2.as_short[0])? -1: 0;\n-  res.as_short[1] = (m1.as_short[1] > m2.as_short[1])? -1: 0;\n-  res.as_short[2] = (m1.as_short[2] > m2.as_short[2])? -1: 0;\n-  res.as_short[3] = (m1.as_short[3] > m2.as_short[3])? -1: 0;\n+  __res.as_short[0] = (__mu1.as_short[0] > __mu2.as_short[0])? -1: 0;\n+  __res.as_short[1] = (__mu1.as_short[1] > __mu2.as_short[1])? -1: 0;\n+  __res.as_short[2] = (__mu1.as_short[2] > __mu2.as_short[2])? -1: 0;\n+  __res.as_short[3] = (__mu1.as_short[3] > __mu2.as_short[3])? -1: 0;\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -867,22 +867,22 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_cmpeq_pi32 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR9\n-  __vector signed int a, b, c;\n+  __vector signed int __a, __b, __c;\n \n-  a = (__vector signed int)vec_splats (__m1);\n-  b = (__vector signed int)vec_splats (__m2);\n-  c = (__vector signed int)vec_cmpeq (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed int)vec_splats (__m1);\n+  __b = (__vector signed int)vec_splats (__m2);\n+  __c = (__vector signed int)vec_cmpeq (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_int[0] = (m1.as_int[0] == m2.as_int[0])? -1: 0;\n-  res.as_int[1] = (m1.as_int[1] == m2.as_int[1])? -1: 0;\n+  __res.as_int[0] = (__mu1.as_int[0] == __mu2.as_int[0])? -1: 0;\n+  __res.as_int[1] = (__mu1.as_int[1] == __mu2.as_int[1])? -1: 0;\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -896,22 +896,22 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_cmpgt_pi32 (__m64 __m1, __m64 __m2)\n {\n #if _ARCH_PWR9\n-  __vector signed int a, b, c;\n+  __vector signed int __a, __b, __c;\n \n-  a = (__vector signed int)vec_splats (__m1);\n-  b = (__vector signed int)vec_splats (__m2);\n-  c = (__vector signed int)vec_cmpgt (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed int)vec_splats (__m1);\n+  __b = (__vector signed int)vec_splats (__m2);\n+  __c = (__vector signed int)vec_cmpgt (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __mu1, __mu2, __res;\n \n-  m1.as_m64 = __m1;\n-  m2.as_m64 = __m2;\n+  __mu1.as_m64 = __m1;\n+  __mu2.as_m64 = __m2;\n \n-  res.as_int[0] = (m1.as_int[0] > m2.as_int[0])? -1: 0;\n-  res.as_int[1] = (m1.as_int[1] > m2.as_int[1])? -1: 0;\n+  __res.as_int[0] = (__mu1.as_int[0] > __mu2.as_int[0])? -1: 0;\n+  __res.as_int[1] = (__mu1.as_int[1] > __mu2.as_int[1])? -1: 0;\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -927,12 +927,12 @@ _m_pcmpgtd (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_pi8 (__m64 __m1, __m64 __m2)\n {\n-  __vector signed char a, b, c;\n+  __vector signed char __a, __b, __c;\n \n-  a = (__vector signed char)vec_splats (__m1);\n-  b = (__vector signed char)vec_splats (__m2);\n-  c = vec_adds (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed char)vec_splats (__m1);\n+  __b = (__vector signed char)vec_splats (__m2);\n+  __c = vec_adds (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -945,12 +945,12 @@ _m_paddsb (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_pi16 (__m64 __m1, __m64 __m2)\n {\n-  __vector signed short a, b, c;\n+  __vector signed short __a, __b, __c;\n \n-  a = (__vector signed short)vec_splats (__m1);\n-  b = (__vector signed short)vec_splats (__m2);\n-  c = vec_adds (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed short)vec_splats (__m1);\n+  __b = (__vector signed short)vec_splats (__m2);\n+  __c = vec_adds (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -963,12 +963,12 @@ _m_paddsw (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_pu8 (__m64 __m1, __m64 __m2)\n {\n-  __vector unsigned char a, b, c;\n+  __vector unsigned char __a, __b, __c;\n \n-  a = (__vector unsigned char)vec_splats (__m1);\n-  b = (__vector unsigned char)vec_splats (__m2);\n-  c = vec_adds (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector unsigned char)vec_splats (__m1);\n+  __b = (__vector unsigned char)vec_splats (__m2);\n+  __c = vec_adds (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -982,12 +982,12 @@ _m_paddusb (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_pu16 (__m64 __m1, __m64 __m2)\n {\n-  __vector unsigned short a, b, c;\n+  __vector unsigned short __a, __b, __c;\n \n-  a = (__vector unsigned short)vec_splats (__m1);\n-  b = (__vector unsigned short)vec_splats (__m2);\n-  c = vec_adds (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector unsigned short)vec_splats (__m1);\n+  __b = (__vector unsigned short)vec_splats (__m2);\n+  __c = vec_adds (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1001,12 +1001,12 @@ _m_paddusw (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_pi8 (__m64 __m1, __m64 __m2)\n {\n-  __vector signed char a, b, c;\n+  __vector signed char __a, __b, __c;\n \n-  a = (__vector signed char)vec_splats (__m1);\n-  b = (__vector signed char)vec_splats (__m2);\n-  c = vec_subs (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed char)vec_splats (__m1);\n+  __b = (__vector signed char)vec_splats (__m2);\n+  __c = vec_subs (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1020,12 +1020,12 @@ _m_psubsb (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_pi16 (__m64 __m1, __m64 __m2)\n {\n-  __vector signed short a, b, c;\n+  __vector signed short __a, __b, __c;\n \n-  a = (__vector signed short)vec_splats (__m1);\n-  b = (__vector signed short)vec_splats (__m2);\n-  c = vec_subs (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed short)vec_splats (__m1);\n+  __b = (__vector signed short)vec_splats (__m2);\n+  __c = vec_subs (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1039,12 +1039,12 @@ _m_psubsw (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_pu8 (__m64 __m1, __m64 __m2)\n {\n-  __vector unsigned char a, b, c;\n+  __vector unsigned char __a, __b, __c;\n \n-  a = (__vector unsigned char)vec_splats (__m1);\n-  b = (__vector unsigned char)vec_splats (__m2);\n-  c = vec_subs (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector unsigned char)vec_splats (__m1);\n+  __b = (__vector unsigned char)vec_splats (__m2);\n+  __c = vec_subs (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1058,12 +1058,12 @@ _m_psubusb (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_pu16 (__m64 __m1, __m64 __m2)\n {\n-  __vector unsigned short a, b, c;\n+  __vector unsigned short __a, __b, __c;\n \n-  a = (__vector unsigned short)vec_splats (__m1);\n-  b = (__vector unsigned short)vec_splats (__m2);\n-  c = vec_subs (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector unsigned short)vec_splats (__m1);\n+  __b = (__vector unsigned short)vec_splats (__m2);\n+  __c = vec_subs (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1078,14 +1078,14 @@ _m_psubusw (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_madd_pi16 (__m64 __m1, __m64 __m2)\n {\n-  __vector signed short a, b;\n-  __vector signed int c;\n-  __vector signed int zero = {0, 0, 0, 0};\n+  __vector signed short __a, __b;\n+  __vector signed int __c;\n+  __vector signed int __zero = {0, 0, 0, 0};\n \n-  a = (__vector signed short)vec_splats (__m1);\n-  b = (__vector signed short)vec_splats (__m2);\n-  c = vec_vmsumshm (a, b, zero);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed short)vec_splats (__m1);\n+  __b = (__vector signed short)vec_splats (__m2);\n+  __c = vec_vmsumshm (__a, __b, __zero);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1098,10 +1098,10 @@ _m_pmaddwd (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mulhi_pi16 (__m64 __m1, __m64 __m2)\n {\n-  __vector signed short a, b;\n-  __vector signed short c;\n-  __vector signed int w0, w1;\n-  __vector unsigned char xform1 = {\n+  __vector signed short __a, __b;\n+  __vector signed short __c;\n+  __vector signed int __w0, __w1;\n+  __vector unsigned char __xform1 = {\n #ifdef __LITTLE_ENDIAN__\n       0x02, 0x03, 0x12, 0x13,  0x06, 0x07, 0x16, 0x17,\n       0x0A, 0x0B, 0x1A, 0x1B,  0x0E, 0x0F, 0x1E, 0x1F\n@@ -1111,14 +1111,14 @@ _mm_mulhi_pi16 (__m64 __m1, __m64 __m2)\n #endif\n     };\n \n-  a = (__vector signed short)vec_splats (__m1);\n-  b = (__vector signed short)vec_splats (__m2);\n+  __a = (__vector signed short)vec_splats (__m1);\n+  __b = (__vector signed short)vec_splats (__m2);\n \n-  w0 = vec_vmulesh (a, b);\n-  w1 = vec_vmulosh (a, b);\n-  c = (__vector signed short)vec_perm (w0, w1, xform1);\n+  __w0 = vec_vmulesh (__a, __b);\n+  __w1 = vec_vmulosh (__a, __b);\n+  __c = (__vector signed short)vec_perm (__w0, __w1, __xform1);\n \n-  return (__m64) ((__vector long long) c)[0];\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1132,12 +1132,12 @@ _m_pmulhw (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mullo_pi16 (__m64 __m1, __m64 __m2)\n {\n-  __vector signed short a, b, c;\n+  __vector signed short __a, __b, __c;\n \n-  a = (__vector signed short)vec_splats (__m1);\n-  b = (__vector signed short)vec_splats (__m2);\n-  c = a * b;\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector signed short)vec_splats (__m1);\n+  __b = (__vector signed short)vec_splats (__m2);\n+  __c = __a * __b;\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1150,15 +1150,15 @@ _m_pmullw (__m64 __m1, __m64 __m2)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_pi16 (__m64 __m, __m64 __count)\n {\n-  __vector signed short m, r;\n-  __vector unsigned short c;\n+  __vector signed short __r;\n+  __vector unsigned short __c;\n \n   if (__count <= 15)\n     {\n-      m = (__vector signed short)vec_splats (__m);\n-      c = (__vector unsigned short)vec_splats ((unsigned short)__count);\n-      r = vec_sl (m, (__vector unsigned short)c);\n-      return (__m64) ((__vector long long) r)[0];\n+      __r = (__vector signed short)vec_splats (__m);\n+      __c = (__vector unsigned short)vec_splats ((unsigned short)__count);\n+      __r = vec_sl (__r, (__vector unsigned short)__c);\n+      return (__m64) ((__vector long long) __r)[0];\n     }\n   else\n   return (0);\n@@ -1187,13 +1187,13 @@ _m_psllwi (__m64 __m, int __count)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_pi32 (__m64 __m, __m64 __count)\n {\n-  __m64_union m, res;\n+  __m64_union __res;\n \n-  m.as_m64 = __m;\n+  __res.as_m64 = __m;\n \n-  res.as_int[0] = m.as_int[0] << __count;\n-  res.as_int[1] = m.as_int[1] << __count;\n-  return (res.as_m64);\n+  __res.as_int[0] = __res.as_int[0] << __count;\n+  __res.as_int[1] = __res.as_int[1] << __count;\n+  return (__res.as_m64);\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1219,15 +1219,15 @@ _m_pslldi (__m64 __m, int __count)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sra_pi16 (__m64 __m, __m64 __count)\n {\n-  __vector signed short m, r;\n-  __vector unsigned short c;\n+  __vector signed short __r;\n+  __vector unsigned short __c;\n \n   if (__count <= 15)\n     {\n-\tm = (__vector signed short)vec_splats (__m);\n-\tc = (__vector unsigned short)vec_splats ((unsigned short)__count);\n-\tr = vec_sra (m, (__vector unsigned short)c);\n-        return (__m64) ((__vector long long) r)[0];\n+\t__r = (__vector signed short)vec_splats (__m);\n+\t__c = (__vector unsigned short)vec_splats ((unsigned short)__count);\n+\t__r = vec_sra (__r, (__vector unsigned short)__c);\n+        return (__m64) ((__vector long long) __r)[0];\n     }\n   else\n   return (0);\n@@ -1256,13 +1256,13 @@ _m_psrawi (__m64 __m, int __count)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sra_pi32 (__m64 __m, __m64 __count)\n {\n-  __m64_union m, res;\n+  __m64_union __res;\n \n-  m.as_m64 = __m;\n+  __res.as_m64 = __m;\n \n-  res.as_int[0] = m.as_int[0] >> __count;\n-  res.as_int[1] = m.as_int[1] >> __count;\n-  return (res.as_m64);\n+  __res.as_int[0] = __res.as_int[0] >> __count;\n+  __res.as_int[1] = __res.as_int[1] >> __count;\n+  return (__res.as_m64);\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1288,15 +1288,15 @@ _m_psradi (__m64 __m, int __count)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_pi16 (__m64 __m, __m64 __count)\n {\n-  __vector unsigned short m, r;\n-  __vector unsigned short c;\n+  __vector unsigned short __r;\n+  __vector unsigned short __c;\n \n   if (__count <= 15)\n     {\n-\tm = (__vector unsigned short)vec_splats (__m);\n-\tc = (__vector unsigned short)vec_splats ((unsigned short)__count);\n-\tr = vec_sr (m, (__vector unsigned short)c);\n-        return (__m64) ((__vector long long) r)[0];\n+\t__r = (__vector unsigned short)vec_splats (__m);\n+\t__c = (__vector unsigned short)vec_splats ((unsigned short)__count);\n+\t__r = vec_sr (__r, (__vector unsigned short)__c);\n+        return (__m64) ((__vector long long) __r)[0];\n     }\n   else\n     return (0);\n@@ -1325,13 +1325,13 @@ _m_psrlwi (__m64 __m, int __count)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_pi32 (__m64 __m, __m64 __count)\n {\n-  __m64_union m, res;\n+  __m64_union __res;\n \n-  m.as_m64 = __m;\n+  __res.as_m64 = __m;\n \n-  res.as_int[0] = (unsigned int)m.as_int[0] >> __count;\n-  res.as_int[1] = (unsigned int)m.as_int[1] >> __count;\n-  return (res.as_m64);\n+  __res.as_int[0] = (unsigned int)__res.as_int[0] >> __count;\n+  __res.as_int[1] = (unsigned int)__res.as_int[1] >> __count;\n+  return (__res.as_m64);\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1358,53 +1358,53 @@ _m_psrldi (__m64 __m, int __count)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_pi32 (int __i1, int __i0)\n {\n-  __m64_union res;\n+  __m64_union __res;\n \n-  res.as_int[0] = __i0;\n-  res.as_int[1] = __i1;\n-  return (res.as_m64);\n+  __res.as_int[0] = __i0;\n+  __res.as_int[1] = __i1;\n+  return (__res.as_m64);\n }\n \n /* Creates a vector of four 16-bit values; W0 is least significant.  */\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_pi16 (short __w3, short __w2, short __w1, short __w0)\n {\n-  __m64_union res;\n+  __m64_union __res;\n \n-  res.as_short[0] = __w0;\n-  res.as_short[1] = __w1;\n-  res.as_short[2] = __w2;\n-  res.as_short[3] = __w3;\n-  return (res.as_m64);\n+  __res.as_short[0] = __w0;\n+  __res.as_short[1] = __w1;\n+  __res.as_short[2] = __w2;\n+  __res.as_short[3] = __w3;\n+  return (__res.as_m64);\n }\n \n /* Creates a vector of eight 8-bit values; B0 is least significant.  */\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_pi8 (char __b7, char __b6, char __b5, char __b4,\n \t     char __b3, char __b2, char __b1, char __b0)\n {\n-  __m64_union res;\n+  __m64_union __res;\n \n-  res.as_char[0] = __b0;\n-  res.as_char[1] = __b1;\n-  res.as_char[2] = __b2;\n-  res.as_char[3] = __b3;\n-  res.as_char[4] = __b4;\n-  res.as_char[5] = __b5;\n-  res.as_char[6] = __b6;\n-  res.as_char[7] = __b7;\n-  return (res.as_m64);\n+  __res.as_char[0] = __b0;\n+  __res.as_char[1] = __b1;\n+  __res.as_char[2] = __b2;\n+  __res.as_char[3] = __b3;\n+  __res.as_char[4] = __b4;\n+  __res.as_char[5] = __b5;\n+  __res.as_char[6] = __b6;\n+  __res.as_char[7] = __b7;\n+  return (__res.as_m64);\n }\n \n /* Similar, but with the arguments in reverse order.  */\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setr_pi32 (int __i0, int __i1)\n {\n-  __m64_union res;\n+  __m64_union __res;\n \n-  res.as_int[0] = __i0;\n-  res.as_int[1] = __i1;\n-  return (res.as_m64);\n+  __res.as_int[0] = __i0;\n+  __res.as_int[1] = __i1;\n+  return (__res.as_m64);\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1424,11 +1424,11 @@ _mm_setr_pi8 (char __b0, char __b1, char __b2, char __b3,\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_pi32 (int __i)\n {\n-  __m64_union res;\n+  __m64_union __res;\n \n-  res.as_int[0] = __i;\n-  res.as_int[1] = __i;\n-  return (res.as_m64);\n+  __res.as_int[0] = __i;\n+  __res.as_int[1] = __i;\n+  return (__res.as_m64);\n }\n \n /* Creates a vector of four 16-bit values, all elements containing W.  */\n@@ -1441,13 +1441,13 @@ _mm_set1_pi16 (short __w)\n   w = (__vector signed short)vec_splats (__w);\n   return (__m64) ((__vector long long) w)[0];\n #else\n-  __m64_union res;\n+  __m64_union __res;\n \n-  res.as_short[0] = __w;\n-  res.as_short[1] = __w;\n-  res.as_short[2] = __w;\n-  res.as_short[3] = __w;\n-  return (res.as_m64);\n+  __res.as_short[0] = __w;\n+  __res.as_short[1] = __w;\n+  __res.as_short[2] = __w;\n+  __res.as_short[3] = __w;\n+  return (__res.as_m64);\n #endif\n }\n \n@@ -1456,22 +1456,22 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_set1_pi8 (signed char __b)\n {\n #if _ARCH_PWR8\n-  __vector signed char b;\n+  __vector signed char __res;\n \n-  b = (__vector signed char)vec_splats (__b);\n-  return (__m64) ((__vector long long) b)[0];\n+  __res = (__vector signed char)vec_splats (__b);\n+  return (__m64) ((__vector long long) __res)[0];\n #else\n-  __m64_union res;\n-\n-  res.as_char[0] = __b;\n-  res.as_char[1] = __b;\n-  res.as_char[2] = __b;\n-  res.as_char[3] = __b;\n-  res.as_char[4] = __b;\n-  res.as_char[5] = __b;\n-  res.as_char[6] = __b;\n-  res.as_char[7] = __b;\n-  return (res.as_m64);\n+  __m64_union __res;\n+\n+  __res.as_char[0] = __b;\n+  __res.as_char[1] = __b;\n+  __res.as_char[2] = __b;\n+  __res.as_char[3] = __b;\n+  __res.as_char[4] = __b;\n+  __res.as_char[5] = __b;\n+  __res.as_char[6] = __b;\n+  __res.as_char[7] = __b;\n+  return (__res.as_m64);\n #endif\n }\n #endif /* _MMINTRIN_H_INCLUDED */"}, {"sha": "e1b5426a918dce53f248d4c405c6b38fd4ac8cf1", "filename": "gcc/config/rs6000/pmmintrin.h", "status": "modified", "additions": 14, "deletions": 14, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fpmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fpmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fpmmintrin.h?ref=efbb17db52afd802300c4dcce208fab326ec2915", "patch": "@@ -58,55 +58,55 @@\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_addsub_ps (__m128 __X, __m128 __Y)\n {\n-  const __v4sf even_n0 = {-0.0, 0.0, -0.0, 0.0};\n-  __v4sf even_neg_Y = vec_xor(__Y, even_n0);\n-  return (__m128) vec_add (__X, even_neg_Y);\n+  const __v4sf __even_n0 = {-0.0, 0.0, -0.0, 0.0};\n+  __v4sf __even_neg_Y = vec_xor(__Y, __even_n0);\n+  return (__m128) vec_add (__X, __even_neg_Y);\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_addsub_pd (__m128d __X, __m128d __Y)\n {\n-  const __v2df even_n0 = {-0.0, 0.0};\n-  __v2df even_neg_Y = vec_xor(__Y, even_n0);\n-  return (__m128d) vec_add (__X, even_neg_Y);\n+  const __v2df __even_n0 = {-0.0, 0.0};\n+  __v2df __even_neg_Y = vec_xor(__Y, __even_n0);\n+  return (__m128d) vec_add (__X, __even_neg_Y);\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hadd_ps (__m128 __X, __m128 __Y)\n {\n-  __vector unsigned char xform2 = {\n+  __vector unsigned char __xform2 = {\n       0x00, 0x01, 0x02, 0x03,\n       0x08, 0x09, 0x0A, 0x0B,\n       0x10, 0x11, 0x12, 0x13,\n       0x18, 0x19, 0x1A, 0x1B\n     };\n-  __vector unsigned char xform1 = {\n+  __vector unsigned char __xform1 = {\n       0x04, 0x05, 0x06, 0x07,\n       0x0C, 0x0D, 0x0E, 0x0F,\n       0x14, 0x15, 0x16, 0x17,\n       0x1C, 0x1D, 0x1E, 0x1F\n     };\n-  return (__m128) vec_add (vec_perm ((__v4sf) __X, (__v4sf) __Y, xform2),\n-\t\t\t   vec_perm ((__v4sf) __X, (__v4sf) __Y, xform1));\n+  return (__m128) vec_add (vec_perm ((__v4sf) __X, (__v4sf) __Y, __xform2),\n+\t\t\t   vec_perm ((__v4sf) __X, (__v4sf) __Y, __xform1));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsub_ps (__m128 __X, __m128 __Y)\n {\n-  __vector unsigned char xform2 = {\n+  __vector unsigned char __xform2 = {\n       0x00, 0x01, 0x02, 0x03,\n       0x08, 0x09, 0x0A, 0x0B,\n       0x10, 0x11, 0x12, 0x13,\n       0x18, 0x19, 0x1A, 0x1B\n     };\n-  __vector unsigned char xform1 = {\n+  __vector unsigned char __xform1 = {\n       0x04, 0x05, 0x06, 0x07,\n       0x0C, 0x0D, 0x0E, 0x0F,\n       0x14, 0x15, 0x16, 0x17,\n       0x1C, 0x1D, 0x1E, 0x1F\n     };\n-  return (__m128) vec_sub (vec_perm ((__v4sf) __X, (__v4sf) __Y, xform2),\n-\t\t\t   vec_perm ((__v4sf) __X, (__v4sf) __Y, xform1));\n+  return (__m128) vec_sub (vec_perm ((__v4sf) __X, (__v4sf) __Y, __xform2),\n+\t\t\t   vec_perm ((__v4sf) __X, (__v4sf) __Y, __xform1));\n }\n \n extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))"}, {"sha": "3628c886a6b5d648d0abd45573217354ef491f5c", "filename": "gcc/config/rs6000/smmintrin.h", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fsmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fsmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fsmmintrin.h?ref=efbb17db52afd802300c4dcce208fab326ec2915", "patch": "@@ -273,31 +273,31 @@ _mm_round_ss (__m128 __A, __m128 __B, int __rounding)\n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_epi8 (__m128i const __A, int const __D, int const __N)\n {\n-  __v16qi result = (__v16qi)__A;\n+  __v16qi __result = (__v16qi)__A;\n \n-  result [__N & 0xf] = __D;\n+  __result [__N & 0xf] = __D;\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_epi32 (__m128i const __A, int const __D, int const __N)\n {\n-  __v4si result = (__v4si)__A;\n+  __v4si __result = (__v4si)__A;\n \n-  result [__N & 3] = __D;\n+  __result [__N & 3] = __D;\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_epi64 (__m128i const __A, long long const __D, int const __N)\n {\n-  __v2di result = (__v2di)__A;\n+  __v2di __result = (__v2di)__A;\n \n-  result [__N & 1] = __D;\n+  __result [__N & 1] = __D;\n \n-  return (__m128i) result;\n+  return (__m128i) __result;\n }\n \n extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))"}, {"sha": "05b985bac89a81bef953567682ee354d06fcef49", "filename": "gcc/config/rs6000/tmmintrin.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Ftmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Ftmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Ftmmintrin.h?ref=efbb17db52afd802300c4dcce208fab326ec2915", "patch": "@@ -112,8 +112,8 @@ _mm_alignr_epi8 (__m128i __A, __m128i __B, const unsigned int __count)\n     {\n       if (__count >= 32)\n \t{\n-\t  const __v16qu zero = { 0 };\n-\t  return (__m128i) zero;\n+\t  const __v16qu __zero = { 0 };\n+\t  return (__m128i) __zero;\n \t}\n       else\n \t{"}, {"sha": "c60201136ccfd9cee9f94cfb76d67e6b04e4280a", "filename": "gcc/config/rs6000/xmmintrin.h", "status": "modified", "additions": 430, "deletions": 431, "changes": 861, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fxmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/efbb17db52afd802300c4dcce208fab326ec2915/gcc%2Fconfig%2Frs6000%2Fxmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fxmmintrin.h?ref=efbb17db52afd802300c4dcce208fab326ec2915", "patch": "@@ -127,14 +127,14 @@ extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artif\n _mm_loadr_ps (float const *__P)\n {\n   __v4sf   __tmp;\n-  __m128 result;\n-  static const __vector unsigned char permute_vector =\n+  __m128 __result;\n+  static const __vector unsigned char __permute_vector =\n     { 0x1C, 0x1D, 0x1E, 0x1F, 0x18, 0x19, 0x1A, 0x1B, 0x14, 0x15, 0x16,\n \t0x17, 0x10, 0x11, 0x12, 0x13 };\n \n   __tmp = vec_ld (0, (__v4sf *) __P);\n-  result = (__m128) vec_perm (__tmp, __tmp, permute_vector);\n-  return result;\n+  __result = (__m128) vec_perm (__tmp, __tmp, __permute_vector);\n+  return __result;\n }\n \n /* Create a vector with all four elements equal to F.  */\n@@ -184,11 +184,11 @@ extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artific\n _mm_storer_ps (float *__P, __m128 __A)\n {\n   __v4sf   __tmp;\n-  static const __vector unsigned char permute_vector =\n+  static const __vector unsigned char __permute_vector =\n     { 0x1C, 0x1D, 0x1E, 0x1F, 0x18, 0x19, 0x1A, 0x1B, 0x14, 0x15, 0x16,\n \t0x17, 0x10, 0x11, 0x12, 0x13 };\n \n-  __tmp = (__m128) vec_perm (__A, __A, permute_vector);\n+  __tmp = (__m128) vec_perm (__A, __A, __permute_vector);\n \n   _mm_store_ps (__P, __tmp);\n }\n@@ -218,9 +218,9 @@ _mm_set_ss (float __F)\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_move_ss (__m128 __A, __m128 __B)\n {\n-  static const __vector unsigned int mask = {0xffffffff, 0, 0, 0};\n+  static const __vector unsigned int __mask = {0xffffffff, 0, 0, 0};\n \n-  return (vec_sel ((__v4sf)__A, (__v4sf)__B, mask));\n+  return (vec_sel ((__v4sf)__A, (__v4sf)__B, __mask));\n }\n \n /* Create a vector with element 0 as *P and the rest zero.  */\n@@ -245,18 +245,18 @@ extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artif\n _mm_add_ss (__m128 __A, __m128 __B)\n {\n #ifdef _ARCH_PWR7\n-  __m128 a, b, c;\n-  static const __vector unsigned int mask = {0xffffffff, 0, 0, 0};\n+  __m128 __a, __b, __c;\n+  static const __vector unsigned int __mask = {0xffffffff, 0, 0, 0};\n   /* PowerISA VSX does not allow partial (for just lower double)\n      results. So to insure we don't generate spurious exceptions\n      (from the upper double values) we splat the lower double\n      before we to the operation.  */\n-  a = vec_splat (__A, 0);\n-  b = vec_splat (__B, 0);\n-  c = a + b;\n+  __a = vec_splat (__A, 0);\n+  __b = vec_splat (__B, 0);\n+  __c = __a + __b;\n   /* Then we merge the lower float result with the original upper\n      float elements from __A.  */\n-  return (vec_sel (__A, c, mask));\n+  return (vec_sel (__A, __c, __mask));\n #else\n   __A[0] = __A[0] + __B[0];\n   return (__A);\n@@ -267,18 +267,18 @@ extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artif\n _mm_sub_ss (__m128 __A, __m128 __B)\n {\n #ifdef _ARCH_PWR7\n-  __m128 a, b, c;\n-  static const __vector unsigned int mask = {0xffffffff, 0, 0, 0};\n+  __m128 __a, __b, __c;\n+  static const __vector unsigned int __mask = {0xffffffff, 0, 0, 0};\n   /* PowerISA VSX does not allow partial (for just lower double)\n      results. So to insure we don't generate spurious exceptions\n      (from the upper double values) we splat the lower double\n      before we to the operation.  */\n-  a = vec_splat (__A, 0);\n-  b = vec_splat (__B, 0);\n-  c = a - b;\n+  __a = vec_splat (__A, 0);\n+  __b = vec_splat (__B, 0);\n+  __c = __a - __b;\n   /* Then we merge the lower float result with the original upper\n      float elements from __A.  */\n-  return (vec_sel (__A, c, mask));\n+  return (vec_sel (__A, __c, __mask));\n #else\n   __A[0] = __A[0] - __B[0];\n   return (__A);\n@@ -289,18 +289,18 @@ extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artif\n _mm_mul_ss (__m128 __A, __m128 __B)\n {\n #ifdef _ARCH_PWR7\n-  __m128 a, b, c;\n-  static const __vector unsigned int mask = {0xffffffff, 0, 0, 0};\n+  __m128 __a, __b, __c;\n+  static const __vector unsigned int __mask = {0xffffffff, 0, 0, 0};\n   /* PowerISA VSX does not allow partial (for just lower double)\n      results. So to insure we don't generate spurious exceptions\n      (from the upper double values) we splat the lower double\n      before we to the operation.  */\n-  a = vec_splat (__A, 0);\n-  b = vec_splat (__B, 0);\n-  c = a * b;\n+  __a = vec_splat (__A, 0);\n+  __b = vec_splat (__B, 0);\n+  __c = __a * __b;\n   /* Then we merge the lower float result with the original upper\n      float elements from __A.  */\n-  return (vec_sel (__A, c, mask));\n+  return (vec_sel (__A, __c, __mask));\n #else\n   __A[0] = __A[0] * __B[0];\n   return (__A);\n@@ -311,18 +311,18 @@ extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artif\n _mm_div_ss (__m128 __A, __m128 __B)\n {\n #ifdef _ARCH_PWR7\n-  __m128 a, b, c;\n-  static const __vector unsigned int mask = {0xffffffff, 0, 0, 0};\n+  __m128 __a, __b, __c;\n+  static const __vector unsigned int __mask = {0xffffffff, 0, 0, 0};\n   /* PowerISA VSX does not allow partial (for just lower double)\n      results. So to insure we don't generate spurious exceptions\n      (from the upper double values) we splat the lower double\n      before we to the operation.  */\n-  a = vec_splat (__A, 0);\n-  b = vec_splat (__B, 0);\n-  c = a / b;\n+  __a = vec_splat (__A, 0);\n+  __b = vec_splat (__B, 0);\n+  __c = __a / __b;\n   /* Then we merge the lower float result with the original upper\n      float elements from __A.  */\n-  return (vec_sel (__A, c, mask));\n+  return (vec_sel (__A, __c, __mask));\n #else\n   __A[0] = __A[0] / __B[0];\n   return (__A);\n@@ -332,17 +332,17 @@ _mm_div_ss (__m128 __A, __m128 __B)\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sqrt_ss (__m128 __A)\n {\n-  __m128 a, c;\n-  static const __vector unsigned int mask = {0xffffffff, 0, 0, 0};\n+  __m128 __a, __c;\n+  static const __vector unsigned int __mask = {0xffffffff, 0, 0, 0};\n   /* PowerISA VSX does not allow partial (for just lower double)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper double values) we splat the lower double\n    * before we to the operation. */\n-  a = vec_splat (__A, 0);\n-  c = vec_sqrt (a);\n+  __a = vec_splat (__A, 0);\n+  __c = vec_sqrt (__a);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return (vec_sel (__A, c, mask));\n+  return (vec_sel (__A, __c, __mask));\n }\n \n /* Perform the respective operation on the four SPFP values in A and B.  */\n@@ -391,81 +391,81 @@ _mm_rsqrt_ps (__m128 __A)\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rcp_ss (__m128 __A)\n {\n-  __m128 a, c;\n-  static const __vector unsigned int mask = {0xffffffff, 0, 0, 0};\n+  __m128 __a, __c;\n+  static const __vector unsigned int __mask = {0xffffffff, 0, 0, 0};\n   /* PowerISA VSX does not allow partial (for just lower double)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper double values) we splat the lower double\n    * before we to the operation. */\n-  a = vec_splat (__A, 0);\n-  c = _mm_rcp_ps (a);\n+  __a = vec_splat (__A, 0);\n+  __c = _mm_rcp_ps (__a);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return (vec_sel (__A, c, mask));\n+  return (vec_sel (__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rsqrt_ss (__m128 __A)\n {\n-  __m128 a, c;\n-  static const __vector unsigned int mask = {0xffffffff, 0, 0, 0};\n+  __m128 __a, __c;\n+  static const __vector unsigned int __mask = {0xffffffff, 0, 0, 0};\n   /* PowerISA VSX does not allow partial (for just lower double)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper double values) we splat the lower double\n    * before we to the operation. */\n-  a = vec_splat (__A, 0);\n-  c = vec_rsqrte (a);\n+  __a = vec_splat (__A, 0);\n+  __c = vec_rsqrte (__a);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return (vec_sel (__A, c, mask));\n+  return (vec_sel (__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_ss (__m128 __A, __m128 __B)\n {\n-  __v4sf a, b, c;\n-  static const __vector unsigned int mask = {0xffffffff, 0, 0, 0};\n+  __v4sf __a, __b, __c;\n+  static const __vector unsigned int __mask = {0xffffffff, 0, 0, 0};\n   /* PowerISA VSX does not allow partial (for just lower float)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper float values) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat ((__v4sf)__A, 0);\n-  b = vec_splat ((__v4sf)__B, 0);\n-  c = vec_min (a, b);\n+  __a = vec_splat ((__v4sf)__A, 0);\n+  __b = vec_splat ((__v4sf)__B, 0);\n+  __c = vec_min (__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return (vec_sel ((__v4sf)__A, c, mask));\n+  return (vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_ss (__m128 __A, __m128 __B)\n {\n-  __v4sf a, b, c;\n-  static const __vector unsigned int mask = {0xffffffff, 0, 0, 0};\n+  __v4sf __a, __b, __c;\n+  static const __vector unsigned int __mask = {0xffffffff, 0, 0, 0};\n   /* PowerISA VSX does not allow partial (for just lower float)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper float values) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat (__A, 0);\n-  b = vec_splat (__B, 0);\n-  c = vec_max (a, b);\n+  __a = vec_splat (__A, 0);\n+  __b = vec_splat (__B, 0);\n+  __c = vec_max (__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return (vec_sel ((__v4sf)__A, c, mask));\n+  return (vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_ps (__m128 __A, __m128 __B)\n {\n-  __vector __bool int m = vec_cmpgt ((__v4sf) __B, (__v4sf) __A);\n-  return vec_sel (__B, __A, m);\n+  __vector __bool int __m = vec_cmpgt ((__v4sf) __B, (__v4sf) __A);\n+  return vec_sel (__B, __A, __m);\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_ps (__m128 __A, __m128 __B)\n {\n-  __vector __bool int m = vec_cmpgt ((__v4sf) __A, (__v4sf) __B);\n-  return vec_sel (__B, __A, m);\n+  __vector __bool int __m = vec_cmpgt ((__v4sf) __A, (__v4sf) __B);\n+  return vec_sel (__B, __A, __m);\n }\n \n /* Perform logical bit-wise operations on 128-bit values.  */\n@@ -530,8 +530,8 @@ _mm_cmpge_ps (__m128 __A, __m128 __B)\n extern __inline  __m128  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpneq_ps (__m128  __A, __m128  __B)\n {\n-  __v4sf temp = (__v4sf ) vec_cmpeq ((__v4sf) __A, (__v4sf)__B);\n-  return ((__m128)vec_nor (temp, temp));\n+  __v4sf __temp = (__v4sf ) vec_cmpeq ((__v4sf) __A, (__v4sf)__B);\n+  return ((__m128)vec_nor (__temp, __temp));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -561,31 +561,31 @@ _mm_cmpnge_ps (__m128 __A, __m128 __B)\n extern __inline  __m128  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpord_ps (__m128  __A, __m128  __B)\n {\n-  __vector unsigned int a, b;\n-  __vector unsigned int c, d;\n-  static const __vector unsigned int float_exp_mask =\n+  __vector unsigned int __a, __b;\n+  __vector unsigned int __c, __d;\n+  static const __vector unsigned int __float_exp_mask =\n     { 0x7f800000, 0x7f800000, 0x7f800000, 0x7f800000 };\n \n-  a = (__vector unsigned int) vec_abs ((__v4sf)__A);\n-  b = (__vector unsigned int) vec_abs ((__v4sf)__B);\n-  c = (__vector unsigned int) vec_cmpgt (float_exp_mask, a);\n-  d = (__vector unsigned int) vec_cmpgt (float_exp_mask, b);\n-  return ((__m128 ) vec_and (c, d));\n+  __a = (__vector unsigned int) vec_abs ((__v4sf)__A);\n+  __b = (__vector unsigned int) vec_abs ((__v4sf)__B);\n+  __c = (__vector unsigned int) vec_cmpgt (__float_exp_mask, __a);\n+  __d = (__vector unsigned int) vec_cmpgt (__float_exp_mask, __b);\n+  return ((__m128 ) vec_and (__c, __d));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpunord_ps (__m128 __A, __m128 __B)\n {\n-  __vector unsigned int a, b;\n-  __vector unsigned int c, d;\n-  static const __vector unsigned int float_exp_mask =\n+  __vector unsigned int __a, __b;\n+  __vector unsigned int __c, __d;\n+  static const __vector unsigned int __float_exp_mask =\n     { 0x7f800000, 0x7f800000, 0x7f800000, 0x7f800000 };\n \n-  a = (__vector unsigned int) vec_abs ((__v4sf)__A);\n-  b = (__vector unsigned int) vec_abs ((__v4sf)__B);\n-  c = (__vector unsigned int) vec_cmpgt (a, float_exp_mask);\n-  d = (__vector unsigned int) vec_cmpgt (b, float_exp_mask);\n-  return ((__m128 ) vec_or (c, d));\n+  __a = (__vector unsigned int) vec_abs ((__v4sf)__A);\n+  __b = (__vector unsigned int) vec_abs ((__v4sf)__B);\n+  __c = (__vector unsigned int) vec_cmpgt (__a, __float_exp_mask);\n+  __d = (__vector unsigned int) vec_cmpgt (__b, __float_exp_mask);\n+  return ((__m128 ) vec_or (__c, __d));\n }\n \n /* Perform a comparison on the lower SPFP values of A and B.  If the\n@@ -594,222 +594,222 @@ _mm_cmpunord_ps (__m128 __A, __m128 __B)\n extern __inline  __m128  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_ss (__m128  __A, __m128  __B)\n {\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n-  __v4sf a, b, c;\n+  __v4sf __a, __b, __c;\n   /* PowerISA VMX does not allow partial (for just element 0)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper elements) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat ((__v4sf) __A, 0);\n-  b = vec_splat ((__v4sf) __B, 0);\n-  c = (__v4sf) vec_cmpeq(a, b);\n+  __a = vec_splat ((__v4sf) __A, 0);\n+  __b = vec_splat ((__v4sf) __B, 0);\n+  __c = (__v4sf) vec_cmpeq (__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmplt_ss (__m128 __A, __m128 __B)\n {\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n-  __v4sf a, b, c;\n+  __v4sf __a, __b, __c;\n   /* PowerISA VMX does not allow partial (for just element 0)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper elements) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat ((__v4sf) __A, 0);\n-  b = vec_splat ((__v4sf) __B, 0);\n-  c = (__v4sf) vec_cmplt(a, b);\n+  __a = vec_splat ((__v4sf) __A, 0);\n+  __b = vec_splat ((__v4sf) __B, 0);\n+  __c = (__v4sf) vec_cmplt(__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmple_ss (__m128 __A, __m128 __B)\n {\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n-  __v4sf a, b, c;\n+  __v4sf __a, __b, __c;\n   /* PowerISA VMX does not allow partial (for just element 0)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper elements) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat ((__v4sf) __A, 0);\n-  b = vec_splat ((__v4sf) __B, 0);\n-  c = (__v4sf) vec_cmple(a, b);\n+  __a = vec_splat ((__v4sf) __A, 0);\n+  __b = vec_splat ((__v4sf) __B, 0);\n+  __c = (__v4sf) vec_cmple(__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_ss (__m128 __A, __m128 __B)\n {\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n-  __v4sf a, b, c;\n+  __v4sf __a, __b, __c;\n   /* PowerISA VMX does not allow partial (for just element 0)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper elements) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat ((__v4sf) __A, 0);\n-  b = vec_splat ((__v4sf) __B, 0);\n-  c = (__v4sf) vec_cmpgt(a, b);\n+  __a = vec_splat ((__v4sf) __A, 0);\n+  __b = vec_splat ((__v4sf) __B, 0);\n+  __c = (__v4sf) vec_cmpgt(__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpge_ss (__m128 __A, __m128 __B)\n {\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n-  __v4sf a, b, c;\n+  __v4sf __a, __b, __c;\n   /* PowerISA VMX does not allow partial (for just element 0)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper elements) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat ((__v4sf) __A, 0);\n-  b = vec_splat ((__v4sf) __B, 0);\n-  c = (__v4sf) vec_cmpge(a, b);\n+  __a = vec_splat ((__v4sf) __A, 0);\n+  __b = vec_splat ((__v4sf) __B, 0);\n+  __c = (__v4sf) vec_cmpge(__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpneq_ss (__m128 __A, __m128 __B)\n {\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n-  __v4sf a, b, c;\n+  __v4sf __a, __b, __c;\n   /* PowerISA VMX does not allow partial (for just element 0)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper elements) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat ((__v4sf) __A, 0);\n-  b = vec_splat ((__v4sf) __B, 0);\n-  c = (__v4sf) vec_cmpeq(a, b);\n-  c = vec_nor (c, c);\n+  __a = vec_splat ((__v4sf) __A, 0);\n+  __b = vec_splat ((__v4sf) __B, 0);\n+  __c = (__v4sf) vec_cmpeq(__a, __b);\n+  __c = vec_nor (__c, __c);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnlt_ss (__m128 __A, __m128 __B)\n {\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n-  __v4sf a, b, c;\n+  __v4sf __a, __b, __c;\n   /* PowerISA VMX does not allow partial (for just element 0)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper elements) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat ((__v4sf) __A, 0);\n-  b = vec_splat ((__v4sf) __B, 0);\n-  c = (__v4sf) vec_cmpge(a, b);\n+  __a = vec_splat ((__v4sf) __A, 0);\n+  __b = vec_splat ((__v4sf) __B, 0);\n+  __c = (__v4sf) vec_cmpge(__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnle_ss (__m128 __A, __m128 __B)\n {\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n-  __v4sf a, b, c;\n+  __v4sf __a, __b, __c;\n   /* PowerISA VMX does not allow partial (for just element 0)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper elements) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat ((__v4sf) __A, 0);\n-  b = vec_splat ((__v4sf) __B, 0);\n-  c = (__v4sf) vec_cmpgt(a, b);\n+  __a = vec_splat ((__v4sf) __A, 0);\n+  __b = vec_splat ((__v4sf) __B, 0);\n+  __c = (__v4sf) vec_cmpgt(__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpngt_ss (__m128 __A, __m128 __B)\n {\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n-  __v4sf a, b, c;\n+  __v4sf __a, __b, __c;\n   /* PowerISA VMX does not allow partial (for just element 0)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper elements) we splat the lower float\n    * before we to the operation. */\n-  a = vec_splat ((__v4sf) __A, 0);\n-  b = vec_splat ((__v4sf) __B, 0);\n-  c = (__v4sf) vec_cmple(a, b);\n+  __a = vec_splat ((__v4sf) __A, 0);\n+  __b = vec_splat ((__v4sf) __B, 0);\n+  __c = (__v4sf) vec_cmple(__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnge_ss (__m128 __A, __m128 __B)\n {\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n-  __v4sf a, b, c;\n+  __v4sf __a, __b, __c;\n   /* PowerISA VMX does not allow partial (for just element 0)\n    * results. So to insure we don't generate spurious exceptions\n    * (from the upper elements) we splat the lower float\n    * before we do the operation. */\n-  a = vec_splat ((__v4sf) __A, 0);\n-  b = vec_splat ((__v4sf) __B, 0);\n-  c = (__v4sf) vec_cmplt(a, b);\n+  __a = vec_splat ((__v4sf) __A, 0);\n+  __b = vec_splat ((__v4sf) __B, 0);\n+  __c = (__v4sf) vec_cmplt(__a, __b);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, __c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpord_ss (__m128 __A, __m128 __B)\n {\n-  __vector unsigned int a, b;\n-  __vector unsigned int c, d;\n-  static const __vector unsigned int float_exp_mask =\n+  __vector unsigned int __a, __b;\n+  __vector unsigned int __c, __d;\n+  static const __vector unsigned int __float_exp_mask =\n     { 0x7f800000, 0x7f800000, 0x7f800000, 0x7f800000 };\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n \n-  a = (__vector unsigned int) vec_abs ((__v4sf)__A);\n-  b = (__vector unsigned int) vec_abs ((__v4sf)__B);\n-  c = (__vector unsigned int) vec_cmpgt (float_exp_mask, a);\n-  d = (__vector unsigned int) vec_cmpgt (float_exp_mask, b);\n-  c = vec_and (c, d);\n+  __a = (__vector unsigned int) vec_abs ((__v4sf)__A);\n+  __b = (__vector unsigned int) vec_abs ((__v4sf)__B);\n+  __c = (__vector unsigned int) vec_cmpgt (__float_exp_mask, __a);\n+  __d = (__vector unsigned int) vec_cmpgt (__float_exp_mask, __b);\n+  __c = vec_and (__c, __d);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, (__v4sf)c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, (__v4sf)__c, __mask));\n }\n \n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpunord_ss (__m128 __A, __m128 __B)\n {\n-  __vector unsigned int a, b;\n-  __vector unsigned int c, d;\n-  static const __vector unsigned int float_exp_mask =\n+  __vector unsigned int __a, __b;\n+  __vector unsigned int __c, __d;\n+  static const __vector unsigned int __float_exp_mask =\n     { 0x7f800000, 0x7f800000, 0x7f800000, 0x7f800000 };\n-  static const __vector unsigned int mask =\n+  static const __vector unsigned int __mask =\n     { 0xffffffff, 0, 0, 0 };\n \n-  a = (__vector unsigned int) vec_abs ((__v4sf)__A);\n-  b = (__vector unsigned int) vec_abs ((__v4sf)__B);\n-  c = (__vector unsigned int) vec_cmpgt (a, float_exp_mask);\n-  d = (__vector unsigned int) vec_cmpgt (b, float_exp_mask);\n-  c = vec_or (c, d);\n+  __a = (__vector unsigned int) vec_abs ((__v4sf)__A);\n+  __b = (__vector unsigned int) vec_abs ((__v4sf)__B);\n+  __c = (__vector unsigned int) vec_cmpgt (__a, __float_exp_mask);\n+  __d = (__vector unsigned int) vec_cmpgt (__b, __float_exp_mask);\n+  __c = vec_or (__c, __d);\n   /* Then we merge the lower float result with the original upper\n    * float elements from __A.  */\n-  return ((__m128)vec_sel ((__v4sf)__A, (__v4sf)c, mask));\n+  return ((__m128)vec_sel ((__v4sf)__A, (__v4sf)__c, __mask));\n }\n \n /* Compare the lower SPFP values of A and B and return 1 if true\n@@ -905,9 +905,9 @@ _mm_cvtss_f32 (__m128 __A)\n extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtss_si32 (__m128 __A)\n {\n-  int res;\n+  int __res;\n #ifdef _ARCH_PWR8\n-  double dtmp;\n+  double __dtmp;\n   __asm__(\n #ifdef __LITTLE_ENDIAN__\n       \"xxsldwi %x0,%x0,%x0,3;\\n\"\n@@ -916,13 +916,13 @@ _mm_cvtss_si32 (__m128 __A)\n       \"fctiw  %2,%2;\\n\"\n       \"mfvsrd  %1,%x2;\\n\"\n       : \"+wa\" (__A),\n-        \"=r\" (res),\n-        \"=f\" (dtmp)\n+        \"=r\" (__res),\n+        \"=f\" (__dtmp)\n       : );\n #else\n-  res = __builtin_rint(__A[0]);\n+  __res = __builtin_rint(__A[0]);\n #endif\n-  return (res);\n+  return __res;\n }\n \n extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -938,9 +938,9 @@ _mm_cvt_ss2si (__m128 __A)\n extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtss_si64 (__m128 __A)\n {\n-  long long res;\n+  long long __res;\n #if defined (_ARCH_PWR8) && defined (__powerpc64__)\n-  double dtmp;\n+  double __dtmp;\n   __asm__(\n #ifdef __LITTLE_ENDIAN__\n       \"xxsldwi %x0,%x0,%x0,3;\\n\"\n@@ -949,13 +949,13 @@ _mm_cvtss_si64 (__m128 __A)\n       \"fctid  %2,%2;\\n\"\n       \"mfvsrd  %1,%x2;\\n\"\n       : \"+wa\" (__A),\n-        \"=r\" (res),\n-        \"=f\" (dtmp)\n+        \"=r\" (__res),\n+        \"=f\" (__dtmp)\n       : );\n #else\n-  res = __builtin_llrint(__A[0]);\n+  __res = __builtin_llrint(__A[0]);\n #endif\n-  return (res);\n+  return __res;\n }\n \n /* Microsoft intrinsic.  */\n@@ -992,15 +992,15 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_cvtps_pi32 (__m128 __A)\n {\n   /* Splat two lower SPFP values to both halves.  */\n-  __v4sf temp, rounded;\n-  __vector unsigned long long result;\n+  __v4sf __temp, __rounded;\n+  __vector unsigned long long __result;\n \n   /* Splat two lower SPFP values to both halves.  */\n-  temp = (__v4sf) vec_splat ((__vector long long)__A, 0);\n-  rounded = vec_rint(temp);\n-  result = (__vector unsigned long long) vec_cts (rounded, 0);\n+  __temp = (__v4sf) vec_splat ((__vector long long)__A, 0);\n+  __rounded = vec_rint (__temp);\n+  __result = (__vector unsigned long long) vec_cts (__rounded, 0);\n \n-  return (__m64) ((__vector long long) result)[0];\n+  return (__m64) ((__vector long long) __result)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1014,9 +1014,9 @@ extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artifici\n _mm_cvttss_si32 (__m128 __A)\n {\n   /* Extract the lower float element.  */\n-  float temp = __A[0];\n+  float __temp = __A[0];\n   /* truncate to 32-bit integer and return.  */\n-  return temp;\n+  return __temp;\n }\n \n extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1030,34 +1030,34 @@ extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __ar\n _mm_cvttss_si64 (__m128 __A)\n {\n   /* Extract the lower float element.  */\n-  float temp = __A[0];\n+  float __temp = __A[0];\n   /* truncate to 32-bit integer and return.  */\n-  return temp;\n+  return __temp;\n }\n \n /* Microsoft intrinsic.  */\n extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttss_si64x (__m128 __A)\n {\n   /* Extract the lower float element.  */\n-  float temp = __A[0];\n+  float __temp = __A[0];\n   /* truncate to 32-bit integer and return.  */\n-  return temp;\n+  return __temp;\n }\n \n /* Truncate the two lower SPFP values to 32-bit integers.  Return the\n    integers in packed form.  */\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttps_pi32 (__m128 __A)\n {\n-  __v4sf temp;\n-  __vector unsigned long long result;\n+  __v4sf __temp;\n+  __vector unsigned long long __result;\n \n   /* Splat two lower SPFP values to both halves.  */\n-  temp = (__v4sf) vec_splat ((__vector long long)__A, 0);\n-  result = (__vector unsigned long long) vec_cts (temp, 0);\n+  __temp = (__v4sf) vec_splat ((__vector long long)__A, 0);\n+  __result = (__vector unsigned long long) vec_cts (__temp, 0);\n \n-  return (__m64) ((__vector long long) result)[0];\n+  return (__m64) ((__vector long long) __result)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1070,8 +1070,8 @@ _mm_cvtt_ps2pi (__m128 __A)\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi32_ss (__m128 __A, int __B)\n {\n-  float temp = __B;\n-  __A[0] = temp;\n+  float __temp = __B;\n+  __A[0] = __temp;\n \n   return __A;\n }\n@@ -1087,8 +1087,8 @@ _mm_cvt_si2ss (__m128 __A, int __B)\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64_ss (__m128 __A, long long __B)\n {\n-  float temp = __B;\n-  __A[0] = temp;\n+  float __temp = __B;\n+  __A[0] = __temp;\n \n   return __A;\n }\n@@ -1105,14 +1105,14 @@ _mm_cvtsi64x_ss (__m128 __A, long long __B)\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpi32_ps (__m128        __A, __m64        __B)\n {\n-  __vector signed int vm1;\n-  __vector float vf1;\n+  __vector signed int __vm1;\n+  __vector float __vf1;\n \n-  vm1 = (__vector signed int) (__vector unsigned long long) {__B, __B};\n-  vf1 = (__vector float) vec_ctf (vm1, 0);\n+  __vm1 = (__vector signed int) (__vector unsigned long long) {__B, __B};\n+  __vf1 = (__vector float) vec_ctf (__vm1, 0);\n \n   return ((__m128) (__vector unsigned long long)\n-    { ((__vector unsigned long long)vf1) [0],\n+    { ((__vector unsigned long long)__vf1) [0],\n \t((__vector unsigned long long)__A) [1]});\n }\n \n@@ -1126,151 +1126,151 @@ _mm_cvt_pi2ps (__m128 __A, __m64 __B)\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpi16_ps (__m64 __A)\n {\n-  __vector signed short vs8;\n-  __vector signed int vi4;\n-  __vector float vf1;\n+  __vector signed short __vs8;\n+  __vector signed int __vi4;\n+  __vector float __vf1;\n \n-  vs8 = (__vector signed short) (__vector unsigned long long) { __A, __A };\n-  vi4 = vec_vupklsh (vs8);\n-  vf1 = (__vector float) vec_ctf (vi4, 0);\n+  __vs8 = (__vector signed short) (__vector unsigned long long) { __A, __A };\n+  __vi4 = vec_vupklsh (__vs8);\n+  __vf1 = (__vector float) vec_ctf (__vi4, 0);\n \n-  return (__m128) vf1;\n+  return (__m128) __vf1;\n }\n \n /* Convert the four unsigned 16-bit values in A to SPFP form.  */\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpu16_ps (__m64 __A)\n {\n-  const __vector unsigned short zero =\n+  const __vector unsigned short __zero =\n     { 0, 0, 0, 0, 0, 0, 0, 0 };\n-  __vector unsigned short vs8;\n-  __vector unsigned int vi4;\n-  __vector float vf1;\n+  __vector unsigned short __vs8;\n+  __vector unsigned int __vi4;\n+  __vector float __vf1;\n \n-  vs8 = (__vector unsigned short) (__vector unsigned long long) { __A, __A };\n-  vi4 = (__vector unsigned int) vec_mergel\n+  __vs8 = (__vector unsigned short) (__vector unsigned long long) { __A, __A };\n+  __vi4 = (__vector unsigned int) vec_mergel\n #ifdef __LITTLE_ENDIAN__\n-                                           (vs8, zero);\n+                                           (__vs8, __zero);\n #else\n-                                           (zero, vs8);\n+                                           (__zero, __vs8);\n #endif\n-  vf1 = (__vector float) vec_ctf (vi4, 0);\n+  __vf1 = (__vector float) vec_ctf (__vi4, 0);\n \n-  return (__m128) vf1;\n+  return (__m128) __vf1;\n }\n \n /* Convert the low four signed 8-bit values in A to SPFP form.  */\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpi8_ps (__m64 __A)\n {\n-  __vector signed char vc16;\n-  __vector signed short vs8;\n-  __vector signed int vi4;\n-  __vector float vf1;\n+  __vector signed char __vc16;\n+  __vector signed short __vs8;\n+  __vector signed int __vi4;\n+  __vector float __vf1;\n \n-  vc16 = (__vector signed char) (__vector unsigned long long) { __A, __A };\n-  vs8 = vec_vupkhsb (vc16);\n-  vi4 = vec_vupkhsh (vs8);\n-  vf1 = (__vector float) vec_ctf (vi4, 0);\n+  __vc16 = (__vector signed char) (__vector unsigned long long) { __A, __A };\n+  __vs8 = vec_vupkhsb (__vc16);\n+  __vi4 = vec_vupkhsh (__vs8);\n+  __vf1 = (__vector float) vec_ctf (__vi4, 0);\n \n-  return (__m128) vf1;\n+  return (__m128) __vf1;\n }\n \n /* Convert the low four unsigned 8-bit values in A to SPFP form.  */\n extern __inline  __m128  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n \n _mm_cvtpu8_ps (__m64  __A)\n {\n-  const __vector unsigned char zero =\n+  const __vector unsigned char __zero =\n     { 0, 0, 0, 0, 0, 0, 0, 0 };\n-  __vector unsigned char vc16;\n-  __vector unsigned short vs8;\n-  __vector unsigned int vi4;\n-  __vector float vf1;\n+  __vector unsigned char __vc16;\n+  __vector unsigned short __vs8;\n+  __vector unsigned int __vi4;\n+  __vector float __vf1;\n \n-  vc16 = (__vector unsigned char) (__vector unsigned long long) { __A, __A };\n+  __vc16 = (__vector unsigned char) (__vector unsigned long long) { __A, __A };\n #ifdef __LITTLE_ENDIAN__\n-  vs8 = (__vector unsigned short) vec_mergel (vc16, zero);\n-  vi4 = (__vector unsigned int) vec_mergeh (vs8,\n-\t\t\t\t\t    (__vector unsigned short) zero);\n+  __vs8 = (__vector unsigned short) vec_mergel (__vc16, __zero);\n+  __vi4 = (__vector unsigned int) vec_mergeh (__vs8,\n+\t\t\t\t\t    (__vector unsigned short) __zero);\n #else\n-  vs8 = (__vector unsigned short) vec_mergel (zero, vc16);\n-  vi4 = (__vector unsigned int) vec_mergeh ((__vector unsigned short) zero,\n-                                            vs8);\n+  __vs8 = (__vector unsigned short) vec_mergel (__zero, __vc16);\n+  __vi4 = (__vector unsigned int) vec_mergeh ((__vector unsigned short) __zero,\n+                                            __vs8);\n #endif\n-  vf1 = (__vector float) vec_ctf (vi4, 0);\n+  __vf1 = (__vector float) vec_ctf (__vi4, 0);\n \n-  return (__m128) vf1;\n+  return (__m128) __vf1;\n }\n \n /* Convert the four signed 32-bit values in A and B to SPFP form.  */\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpi32x2_ps (__m64 __A, __m64 __B)\n {\n-  __vector signed int vi4;\n-  __vector float vf4;\n+  __vector signed int __vi4;\n+  __vector float __vf4;\n \n-  vi4 = (__vector signed int) (__vector unsigned long long) { __A, __B };\n-  vf4 = (__vector float) vec_ctf (vi4, 0);\n-  return (__m128) vf4;\n+  __vi4 = (__vector signed int) (__vector unsigned long long) { __A, __B };\n+  __vf4 = (__vector float) vec_ctf (__vi4, 0);\n+  return (__m128) __vf4;\n }\n \n /* Convert the four SPFP values in A to four signed 16-bit integers.  */\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtps_pi16 (__m128 __A)\n {\n-  __v4sf rounded;\n-  __vector signed int temp;\n-  __vector unsigned long long result;\n+  __v4sf __rounded;\n+  __vector signed int __temp;\n+  __vector unsigned long long __result;\n \n-  rounded = vec_rint(__A);\n-  temp = vec_cts (rounded, 0);\n-  result = (__vector unsigned long long) vec_pack (temp, temp);\n+  __rounded = vec_rint(__A);\n+  __temp = vec_cts (__rounded, 0);\n+  __result = (__vector unsigned long long) vec_pack (__temp, __temp);\n \n-  return (__m64) ((__vector long long) result)[0];\n+  return (__m64) ((__vector long long) __result)[0];\n }\n \n /* Convert the four SPFP values in A to four signed 8-bit integers.  */\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtps_pi8 (__m128 __A)\n {\n-  __v4sf rounded;\n-  __vector signed int tmp_i;\n-  static const __vector signed int zero = {0, 0, 0, 0};\n-  __vector signed short tmp_s;\n-  __vector signed char res_v;\n+  __v4sf __rounded;\n+  __vector signed int __tmp_i;\n+  static const __vector signed int __zero = {0, 0, 0, 0};\n+  __vector signed short __tmp_s;\n+  __vector signed char __res_v;\n \n-  rounded = vec_rint(__A);\n-  tmp_i = vec_cts (rounded, 0);\n-  tmp_s = vec_pack (tmp_i, zero);\n-  res_v = vec_pack (tmp_s, tmp_s);\n-  return (__m64) ((__vector long long) res_v)[0];\n+  __rounded = vec_rint(__A);\n+  __tmp_i = vec_cts (__rounded, 0);\n+  __tmp_s = vec_pack (__tmp_i, __zero);\n+  __res_v = vec_pack (__tmp_s, __tmp_s);\n+  return (__m64) ((__vector long long) __res_v)[0];\n }\n \n /* Selects four specific SPFP values from A and B based on MASK.  */\n extern __inline  __m128  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n \n _mm_shuffle_ps (__m128  __A, __m128  __B, int const __mask)\n {\n-  unsigned long element_selector_10 = __mask & 0x03;\n-  unsigned long element_selector_32 = (__mask >> 2) & 0x03;\n-  unsigned long element_selector_54 = (__mask >> 4) & 0x03;\n-  unsigned long element_selector_76 = (__mask >> 6) & 0x03;\n-  static const unsigned int permute_selectors[4] =\n+  unsigned long __element_selector_10 = __mask & 0x03;\n+  unsigned long __element_selector_32 = (__mask >> 2) & 0x03;\n+  unsigned long __element_selector_54 = (__mask >> 4) & 0x03;\n+  unsigned long __element_selector_76 = (__mask >> 6) & 0x03;\n+  static const unsigned int __permute_selectors[4] =\n     {\n #ifdef __LITTLE_ENDIAN__\n       0x03020100, 0x07060504, 0x0B0A0908, 0x0F0E0D0C\n #else\n       0x00010203, 0x04050607, 0x08090A0B, 0x0C0D0E0F\n #endif\n     };\n-  __vector unsigned int t;\n+  __vector unsigned int __t;\n \n-  t[0] = permute_selectors[element_selector_10];\n-  t[1] = permute_selectors[element_selector_32];\n-  t[2] = permute_selectors[element_selector_54] + 0x10101010;\n-  t[3] = permute_selectors[element_selector_76] + 0x10101010;\n-  return vec_perm ((__v4sf) __A, (__v4sf)__B, (__vector unsigned char)t);\n+  __t[0] = __permute_selectors[__element_selector_10];\n+  __t[1] = __permute_selectors[__element_selector_32];\n+  __t[2] = __permute_selectors[__element_selector_54] + 0x10101010;\n+  __t[3] = __permute_selectors[__element_selector_76] + 0x10101010;\n+  return vec_perm ((__v4sf) __A, (__v4sf)__B, (__vector unsigned char)__t);\n }\n \n /* Selects and interleaves the upper two SPFP values from A and B.  */\n@@ -1355,8 +1355,8 @@ _mm_movemask_ps (__m128  __A)\n #ifdef _ARCH_PWR10\n   return vec_extractm ((__vector unsigned int) __A);\n #else\n-  __vector unsigned long long result;\n-  static const __vector unsigned int perm_mask =\n+  __vector unsigned long long __result;\n+  static const __vector unsigned int __perm_mask =\n     {\n #ifdef __LITTLE_ENDIAN__\n \t0x00204060, 0x80808080, 0x80808080, 0x80808080\n@@ -1365,14 +1365,14 @@ _mm_movemask_ps (__m128  __A)\n #endif\n     };\n \n-  result = ((__vector unsigned long long)\n+  __result = ((__vector unsigned long long)\n \t    vec_vbpermq ((__vector unsigned char) __A,\n-\t\t\t (__vector unsigned char) perm_mask));\n+\t\t\t (__vector unsigned char) __perm_mask));\n \n #ifdef __LITTLE_ENDIAN__\n-  return result[1];\n+  return __result[1];\n #else\n-  return result[0];\n+  return __result[0];\n #endif\n #endif /* !_ARCH_PWR10 */\n }\n@@ -1395,12 +1395,12 @@ _mm_load_ps1 (float const *__P)\n extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_extract_pi16 (__m64 const __A, int const __N)\n {\n-  unsigned int shiftr = __N & 3;\n+  unsigned int __shiftr = __N & 3;\n #ifdef __BIG_ENDIAN__\n-  shiftr = 3 - shiftr;\n+  __shiftr = 3 - __shiftr;\n #endif\n \n-  return ((__A >> (shiftr * 16)) & 0xffff);\n+  return ((__A >> (__shiftr * 16)) & 0xffff);\n }\n \n extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1414,12 +1414,12 @@ _m_pextrw (__m64 const __A, int const __N)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_pi16 (__m64 const __A, int const __D, int const __N)\n {\n-  const int shiftl = (__N & 3) * 16;\n-  const __m64 shiftD = (const __m64) __D << shiftl;\n-  const __m64 mask = 0xffffUL << shiftl;\n-  __m64 result = (__A & (~mask)) | (shiftD & mask);\n+  const int __shiftl = (__N & 3) * 16;\n+  const __m64 __shiftD = (const __m64) __D << __shiftl;\n+  const __m64 __mask = 0xffffUL << __shiftl;\n+  __m64 __result = (__A & (~__mask)) | (__shiftD & __mask);\n \n-  return (result);\n+  return __result;\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1434,30 +1434,30 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_max_pi16 (__m64 __A, __m64 __B)\n {\n #if _ARCH_PWR8\n-  __vector signed short a, b, r;\n-  __vector __bool short c;\n-\n-  a = (__vector signed short)vec_splats (__A);\n-  b = (__vector signed short)vec_splats (__B);\n-  c = (__vector __bool short)vec_cmpgt (a, b);\n-  r = vec_sel (b, a, c);\n-  return (__m64) ((__vector long long) r)[0];\n+  __vector signed short __a, __b, __r;\n+  __vector __bool short __c;\n+\n+  __a = (__vector signed short)vec_splats (__A);\n+  __b = (__vector signed short)vec_splats (__B);\n+  __c = (__vector __bool short)vec_cmpgt (__a, __b);\n+  __r = vec_sel (__b, __a, __c);\n+  return (__m64) ((__vector long long) __r)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __m1, __m2, __res;\n \n-  m1.as_m64 = __A;\n-  m2.as_m64 = __B;\n+  __m1.as_m64 = __A;\n+  __m2.as_m64 = __B;\n \n-  res.as_short[0] =\n-      (m1.as_short[0] > m2.as_short[0]) ? m1.as_short[0] : m2.as_short[0];\n-  res.as_short[1] =\n-      (m1.as_short[1] > m2.as_short[1]) ? m1.as_short[1] : m2.as_short[1];\n-  res.as_short[2] =\n-      (m1.as_short[2] > m2.as_short[2]) ? m1.as_short[2] : m2.as_short[2];\n-  res.as_short[3] =\n-      (m1.as_short[3] > m2.as_short[3]) ? m1.as_short[3] : m2.as_short[3];\n+  __res.as_short[0] =\n+      (__m1.as_short[0] > __m2.as_short[0]) ? __m1.as_short[0] : __m2.as_short[0];\n+  __res.as_short[1] =\n+      (__m1.as_short[1] > __m2.as_short[1]) ? __m1.as_short[1] : __m2.as_short[1];\n+  __res.as_short[2] =\n+      (__m1.as_short[2] > __m2.as_short[2]) ? __m1.as_short[2] : __m2.as_short[2];\n+  __res.as_short[3] =\n+      (__m1.as_short[3] > __m2.as_short[3]) ? __m1.as_short[3] : __m2.as_short[3];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -1472,28 +1472,27 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_max_pu8 (__m64 __A, __m64 __B)\n {\n #if _ARCH_PWR8\n-  __vector unsigned char a, b, r;\n-  __vector __bool char c;\n-\n-  a = (__vector unsigned char)vec_splats (__A);\n-  b = (__vector unsigned char)vec_splats (__B);\n-  c = (__vector __bool char)vec_cmpgt (a, b);\n-  r = vec_sel (b, a, c);\n-  return (__m64) ((__vector long long) r)[0];\n+  __vector unsigned char __a, __b, __r;\n+  __vector __bool char __c;\n+\n+  __a = (__vector unsigned char)vec_splats (__A);\n+  __b = (__vector unsigned char)vec_splats (__B);\n+  __c = (__vector __bool char)vec_cmpgt (__a, __b);\n+  __r = vec_sel (__b, __a, __c);\n+  return (__m64) ((__vector long long) __r)[0];\n #else\n-  __m64_union m1, m2, res;\n-  long i;\n+  __m64_union __m1, __m2, __res;\n+  long __i;\n \n-  m1.as_m64 = __A;\n-  m2.as_m64 = __B;\n+  __m1.as_m64 = __A;\n+  __m2.as_m64 = __B;\n \n+  for (__i = 0; __i < 8; __i++)\n+    __res.as_char[__i] =\n+      ((unsigned char) __m1.as_char[__i] > (unsigned char) __m2.as_char[__i]) ?\n+\t  __m1.as_char[__i] : __m2.as_char[__i];\n \n-  for (i = 0; i < 8; i++)\n-  res.as_char[i] =\n-      ((unsigned char) m1.as_char[i] > (unsigned char) m2.as_char[i]) ?\n-\t  m1.as_char[i] : m2.as_char[i];\n-\n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -1508,30 +1507,30 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_min_pi16 (__m64 __A, __m64 __B)\n {\n #if _ARCH_PWR8\n-  __vector signed short a, b, r;\n-  __vector __bool short c;\n-\n-  a = (__vector signed short)vec_splats (__A);\n-  b = (__vector signed short)vec_splats (__B);\n-  c = (__vector __bool short)vec_cmplt (a, b);\n-  r = vec_sel (b, a, c);\n-  return (__m64) ((__vector long long) r)[0];\n+  __vector signed short __a, __b, __r;\n+  __vector __bool short __c;\n+\n+  __a = (__vector signed short)vec_splats (__A);\n+  __b = (__vector signed short)vec_splats (__B);\n+  __c = (__vector __bool short)vec_cmplt (__a, __b);\n+  __r = vec_sel (__b, __a, __c);\n+  return (__m64) ((__vector long long) __r)[0];\n #else\n-  __m64_union m1, m2, res;\n+  __m64_union __m1, __m2, __res;\n \n-  m1.as_m64 = __A;\n-  m2.as_m64 = __B;\n+  __m1.as_m64 = __A;\n+  __m2.as_m64 = __B;\n \n-  res.as_short[0] =\n-      (m1.as_short[0] < m2.as_short[0]) ? m1.as_short[0] : m2.as_short[0];\n-  res.as_short[1] =\n-      (m1.as_short[1] < m2.as_short[1]) ? m1.as_short[1] : m2.as_short[1];\n-  res.as_short[2] =\n-      (m1.as_short[2] < m2.as_short[2]) ? m1.as_short[2] : m2.as_short[2];\n-  res.as_short[3] =\n-      (m1.as_short[3] < m2.as_short[3]) ? m1.as_short[3] : m2.as_short[3];\n+  __res.as_short[0] =\n+      (__m1.as_short[0] < __m2.as_short[0]) ? __m1.as_short[0] : __m2.as_short[0];\n+  __res.as_short[1] =\n+      (__m1.as_short[1] < __m2.as_short[1]) ? __m1.as_short[1] : __m2.as_short[1];\n+  __res.as_short[2] =\n+      (__m1.as_short[2] < __m2.as_short[2]) ? __m1.as_short[2] : __m2.as_short[2];\n+  __res.as_short[3] =\n+      (__m1.as_short[3] < __m2.as_short[3]) ? __m1.as_short[3] : __m2.as_short[3];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -1546,28 +1545,28 @@ extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artifi\n _mm_min_pu8 (__m64 __A, __m64 __B)\n {\n #if _ARCH_PWR8\n-  __vector unsigned char a, b, r;\n-  __vector __bool char c;\n-\n-  a = (__vector unsigned char)vec_splats (__A);\n-  b = (__vector unsigned char)vec_splats (__B);\n-  c = (__vector __bool char)vec_cmplt (a, b);\n-  r = vec_sel (b, a, c);\n-  return (__m64) ((__vector long long) r)[0];\n+  __vector unsigned char __a, __b, __r;\n+  __vector __bool char __c;\n+\n+  __a = (__vector unsigned char)vec_splats (__A);\n+  __b = (__vector unsigned char)vec_splats (__B);\n+  __c = (__vector __bool char)vec_cmplt (__a, __b);\n+  __r = vec_sel (__b, __a, __c);\n+  return (__m64) ((__vector long long) __r)[0];\n #else\n-  __m64_union m1, m2, res;\n-  long i;\n+  __m64_union __m1, __m2, __res;\n+  long __i;\n \n-  m1.as_m64 = __A;\n-  m2.as_m64 = __B;\n+  __m1.as_m64 = __A;\n+  __m2.as_m64 = __B;\n \n \n-  for (i = 0; i < 8; i++)\n-  res.as_char[i] =\n-      ((unsigned char) m1.as_char[i] < (unsigned char) m2.as_char[i]) ?\n-\t  m1.as_char[i] : m2.as_char[i];\n+  for (__i = 0; __i < 8; __i++)\n+    __res.as_char[__i] =\n+      ((unsigned char) __m1.as_char[__i] < (unsigned char) __m2.as_char[__i]) ?\n+\t  __m1.as_char[__i] : __m2.as_char[__i];\n \n-  return (__m64) res.as_m64;\n+  return (__m64) __res.as_m64;\n #endif\n }\n \n@@ -1582,24 +1581,24 @@ extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artifici\n _mm_movemask_pi8 (__m64 __A)\n {\n #ifdef __powerpc64__\n-  unsigned long long p =\n+  unsigned long long __p =\n #ifdef __LITTLE_ENDIAN__\n                          0x0008101820283038UL; // permute control for sign bits\n #else\n                          0x3830282018100800UL; // permute control for sign bits\n #endif\n-  return __builtin_bpermd (p, __A);\n+  return __builtin_bpermd (__p, __A);\n #else\n #ifdef __LITTLE_ENDIAN__\n-  unsigned int mask = 0x20283038UL;\n-  unsigned int r1 = __builtin_bpermd (mask, __A) & 0xf;\n-  unsigned int r2 = __builtin_bpermd (mask, __A >> 32) & 0xf;\n+  unsigned int __mask = 0x20283038UL;\n+  unsigned int __r1 = __builtin_bpermd (__mask, __A) & 0xf;\n+  unsigned int __r2 = __builtin_bpermd (__mask, __A >> 32) & 0xf;\n #else\n-  unsigned int mask = 0x38302820UL;\n-  unsigned int r1 = __builtin_bpermd (mask, __A >> 32) & 0xf;\n-  unsigned int r2 = __builtin_bpermd (mask, __A) & 0xf;\n+  unsigned int __mask = 0x38302820UL;\n+  unsigned int __r1 = __builtin_bpermd (__mask, __A >> 32) & 0xf;\n+  unsigned int __r2 = __builtin_bpermd (__mask, __A) & 0xf;\n #endif\n-  return (r2 << 4) | r1;\n+  return (__r2 << 4) | __r1;\n #endif\n }\n \n@@ -1614,10 +1613,10 @@ _m_pmovmskb (__m64 __A)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mulhi_pu16 (__m64 __A, __m64 __B)\n {\n-  __vector unsigned short a, b;\n-  __vector unsigned short c;\n-  __vector unsigned int w0, w1;\n-  __vector unsigned char xform1 = {\n+  __vector unsigned short __a, __b;\n+  __vector unsigned short __c;\n+  __vector unsigned int __w0, __w1;\n+  __vector unsigned char __xform1 = {\n #ifdef __LITTLE_ENDIAN__\n       0x02, 0x03, 0x12, 0x13,  0x06, 0x07, 0x16, 0x17,\n       0x0A, 0x0B, 0x1A, 0x1B,  0x0E, 0x0F, 0x1E, 0x1F\n@@ -1627,14 +1626,14 @@ _mm_mulhi_pu16 (__m64 __A, __m64 __B)\n #endif\n     };\n \n-  a = (__vector unsigned short)vec_splats (__A);\n-  b = (__vector unsigned short)vec_splats (__B);\n+  __a = (__vector unsigned short)vec_splats (__A);\n+  __b = (__vector unsigned short)vec_splats (__B);\n \n-  w0 = vec_vmuleuh (a, b);\n-  w1 = vec_vmulouh (a, b);\n-  c = (__vector unsigned short)vec_perm (w0, w1, xform1);\n+  __w0 = vec_vmuleuh (__a, __b);\n+  __w1 = vec_vmulouh (__a, __b);\n+  __c = (__vector unsigned short)vec_perm (__w0, __w1, __xform1);\n \n-  return (__m64) ((__vector long long) c)[0];\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1648,36 +1647,36 @@ _m_pmulhuw (__m64 __A, __m64 __B)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shuffle_pi16 (__m64 __A, int const __N)\n {\n-  unsigned long element_selector_10 = __N & 0x03;\n-  unsigned long element_selector_32 = (__N >> 2) & 0x03;\n-  unsigned long element_selector_54 = (__N >> 4) & 0x03;\n-  unsigned long element_selector_76 = (__N >> 6) & 0x03;\n-  static const unsigned short permute_selectors[4] =\n+  unsigned long __element_selector_10 = __N & 0x03;\n+  unsigned long __element_selector_32 = (__N >> 2) & 0x03;\n+  unsigned long __element_selector_54 = (__N >> 4) & 0x03;\n+  unsigned long __element_selector_76 = (__N >> 6) & 0x03;\n+  static const unsigned short __permute_selectors[4] =\n     {\n #ifdef __LITTLE_ENDIAN__\n \t      0x0908, 0x0B0A, 0x0D0C, 0x0F0E\n #else\n \t      0x0607, 0x0405, 0x0203, 0x0001\n #endif\n     };\n-  __m64_union t;\n-  __vector unsigned long long a, p, r;\n+  __m64_union __t;\n+  __vector unsigned long long __a, __p, __r;\n \n #ifdef __LITTLE_ENDIAN__\n-  t.as_short[0] = permute_selectors[element_selector_10];\n-  t.as_short[1] = permute_selectors[element_selector_32];\n-  t.as_short[2] = permute_selectors[element_selector_54];\n-  t.as_short[3] = permute_selectors[element_selector_76];\n+  __t.as_short[0] = __permute_selectors[__element_selector_10];\n+  __t.as_short[1] = __permute_selectors[__element_selector_32];\n+  __t.as_short[2] = __permute_selectors[__element_selector_54];\n+  __t.as_short[3] = __permute_selectors[__element_selector_76];\n #else\n-  t.as_short[3] = permute_selectors[element_selector_10];\n-  t.as_short[2] = permute_selectors[element_selector_32];\n-  t.as_short[1] = permute_selectors[element_selector_54];\n-  t.as_short[0] = permute_selectors[element_selector_76];\n+  __t.as_short[3] = __permute_selectors[__element_selector_10];\n+  __t.as_short[2] = __permute_selectors[__element_selector_32];\n+  __t.as_short[1] = __permute_selectors[__element_selector_54];\n+  __t.as_short[0] = __permute_selectors[__element_selector_76];\n #endif\n-  p = vec_splats (t.as_m64);\n-  a = vec_splats (__A);\n-  r = vec_perm (a, a, (__vector unsigned char)p);\n-  return (__m64) ((__vector long long) r)[0];\n+  __p = vec_splats (__t.as_m64);\n+  __a = vec_splats (__A);\n+  __r = vec_perm (__a, __a, (__vector unsigned char)__p);\n+  return (__m64) ((__vector long long) __r)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1692,14 +1691,14 @@ _m_pshufw (__m64 __A, int const __N)\n extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskmove_si64 (__m64 __A, __m64 __N, char *__P)\n {\n-  __m64 hibit = 0x8080808080808080UL;\n-  __m64 mask, tmp;\n-  __m64 *p = (__m64*)__P;\n+  __m64 __hibit = 0x8080808080808080UL;\n+  __m64 __mask, __tmp;\n+  __m64 *__p = (__m64*)__P;\n \n-  tmp = *p;\n-  mask = _mm_cmpeq_pi8 ((__N & hibit), hibit);\n-  tmp = (tmp & (~mask)) | (__A & mask);\n-  *p = tmp;\n+  __tmp = *__p;\n+  __mask = _mm_cmpeq_pi8 ((__N & __hibit), __hibit);\n+  __tmp = (__tmp & (~__mask)) | (__A & __mask);\n+  *__p = __tmp;\n }\n \n extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1712,12 +1711,12 @@ _m_maskmovq (__m64 __A, __m64 __N, char *__P)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_avg_pu8 (__m64 __A, __m64 __B)\n {\n-  __vector unsigned char a, b, c;\n+  __vector unsigned char __a, __b, __c;\n \n-  a = (__vector unsigned char)vec_splats (__A);\n-  b = (__vector unsigned char)vec_splats (__B);\n-  c = vec_avg (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector unsigned char)vec_splats (__A);\n+  __b = (__vector unsigned char)vec_splats (__B);\n+  __c = vec_avg (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1730,12 +1729,12 @@ _m_pavgb (__m64 __A, __m64 __B)\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_avg_pu16 (__m64 __A, __m64 __B)\n {\n-  __vector unsigned short a, b, c;\n+  __vector unsigned short __a, __b, __c;\n \n-  a = (__vector unsigned short)vec_splats (__A);\n-  b = (__vector unsigned short)vec_splats (__B);\n-  c = vec_avg (a, b);\n-  return (__m64) ((__vector long long) c)[0];\n+  __a = (__vector unsigned short)vec_splats (__A);\n+  __b = (__vector unsigned short)vec_splats (__B);\n+  __c = vec_avg (__a, __b);\n+  return (__m64) ((__vector long long) __c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1750,26 +1749,26 @@ _m_pavgw (__m64 __A, __m64 __B)\n extern __inline    __m64    __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sad_pu8 (__m64  __A, __m64  __B)\n {\n-  __vector unsigned char a, b;\n-  __vector unsigned char vmin, vmax, vabsdiff;\n-  __vector signed int vsum;\n-  const __vector unsigned int zero =\n+  __vector unsigned char __a, __b;\n+  __vector unsigned char __vmin, __vmax, __vabsdiff;\n+  __vector signed int __vsum;\n+  const __vector unsigned int __zero =\n     { 0, 0, 0, 0 };\n-  __m64_union result = {0};\n+  __m64_union __result = {0};\n \n-  a = (__vector unsigned char) (__vector unsigned long long) { 0UL, __A };\n-  b = (__vector unsigned char) (__vector unsigned long long) { 0UL, __B };\n-  vmin = vec_min (a, b);\n-  vmax = vec_max (a, b);\n-  vabsdiff = vec_sub (vmax, vmin);\n+  __a = (__vector unsigned char) (__vector unsigned long long) { 0UL, __A };\n+  __b = (__vector unsigned char) (__vector unsigned long long) { 0UL, __B };\n+  __vmin = vec_min (__a, __b);\n+  __vmax = vec_max (__a, __b);\n+  __vabsdiff = vec_sub (__vmax, __vmin);\n   /* Sum four groups of bytes into integers.  */\n-  vsum = (__vector signed int) vec_sum4s (vabsdiff, zero);\n+  __vsum = (__vector signed int) vec_sum4s (__vabsdiff, __zero);\n   /* Sum across four integers with integer result.  */\n-  vsum = vec_sums (vsum, (__vector signed int) zero);\n+  __vsum = vec_sums (__vsum, (__vector signed int) __zero);\n   /* The sum is in the right most 32-bits of the vector result.\n      Transfer to a GPR and truncate to 16 bits.  */\n-  result.as_short[0] = vsum[3];\n-  return result.as_m64;\n+  __result.as_short[0] = __vsum[3];\n+  return __result.as_m64;\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))"}]}