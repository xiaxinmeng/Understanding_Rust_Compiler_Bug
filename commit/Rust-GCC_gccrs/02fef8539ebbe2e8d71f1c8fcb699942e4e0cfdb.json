{"sha": "02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDJmZWY4NTM5ZWJiZTJlOGQ3MWYxYzhmY2I2OTk5NDJlNGUwY2ZkYg==", "commit": {"author": {"name": "Daniel Berlin", "email": "dberlin@dberlin.org", "date": "2004-01-05T19:23:50Z"}, "committer": {"name": "Daniel Berlin", "email": "dberlin@gcc.gnu.org", "date": "2004-01-05T19:23:50Z"}, "message": "ggc-zone.c: Remove everything in #ifdef USING_MALLOC_PAGE_GROUPS\n\n2004-01-05  Daniel Berlin  <dberlin@dberlin.org>\n\n\t* ggc-zone.c: Remove everything in #ifdef USING_MALLOC_PAGE_GROUPS\n\t(USING_MMAP): We don't support non-mmap.\n\t(struct alloc_chunk): Steal 1 bit from typecode, use it to mark\n\tlarge objects.\n\t(struct page_entry): Remove bytes_free.\n\t(struct page_table_chain): Remove.\n\t(struct globals): Remove page_table member.\n\t(loookup_page_table_entry): Function deleted.\n\t(set_page_table_entry): Ditto.\n\t(ggc_allocated_p): No longer need page table lookups.\n\t(ggc_marked_p): Ditto.\n\t(alloc_small_page): Don't care about bytes_free anymore.\n\t(alloc_large_page): Round up size.\n\t(ggc_alloc_zone_1): Mark large objects as such, and calculate\n\ttheir size the new way.\n\tRemove page table lookups and setting.\n\t(ggc_get_size): Calculate large object size the new way.\n\t(sweep_pages): Redo to account for fact that we no longer have\n\tbytes_free.\n\t(ggc_collect): No longer need to reincrement bytes_free.\n\t(ggc_pch_alloc_object): Handle new large objects properly.\n\t(ggc_pch_read): Put PCH stuff into it's own uncollected zone.\n\nFrom-SVN: r75438", "tree": {"sha": "5aaaedfdf436558d04931e3b3bbb509a847af114", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/5aaaedfdf436558d04931e3b3bbb509a847af114"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb/comments", "author": {"login": "dberlin", "id": 324715, "node_id": "MDQ6VXNlcjMyNDcxNQ==", "avatar_url": "https://avatars.githubusercontent.com/u/324715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dberlin", "html_url": "https://github.com/dberlin", "followers_url": "https://api.github.com/users/dberlin/followers", "following_url": "https://api.github.com/users/dberlin/following{/other_user}", "gists_url": "https://api.github.com/users/dberlin/gists{/gist_id}", "starred_url": "https://api.github.com/users/dberlin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dberlin/subscriptions", "organizations_url": "https://api.github.com/users/dberlin/orgs", "repos_url": "https://api.github.com/users/dberlin/repos", "events_url": "https://api.github.com/users/dberlin/events{/privacy}", "received_events_url": "https://api.github.com/users/dberlin/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "ab22bc9148e058a649bc50cb0bdc160a9ead763b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ab22bc9148e058a649bc50cb0bdc160a9ead763b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ab22bc9148e058a649bc50cb0bdc160a9ead763b"}], "stats": {"total": 510, "additions": 87, "deletions": 423}, "files": [{"sha": "09d6b9b921bc90ae66fc80962dd8a8f1140bc7c2", "filename": "gcc/ChangeLog", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb", "patch": "@@ -1,3 +1,28 @@\n+2004-01-05  Daniel Berlin  <dberlin@dberlin.org>\n+\t\n+\t* ggc-zone.c: Remove everything in #ifdef USING_MALLOC_PAGE_GROUPS\n+\t(USING_MMAP): We don't support non-mmap.\n+\t(struct alloc_chunk): Steal 1 bit from typecode, use it to mark\n+\tlarge objects.\n+\t(struct page_entry): Remove bytes_free.\n+\t(struct page_table_chain): Remove.\n+\t(struct globals): Remove page_table member.\n+\t(loookup_page_table_entry): Function deleted.\n+\t(set_page_table_entry): Ditto.\n+\t(ggc_allocated_p): No longer need page table lookups.\n+\t(ggc_marked_p): Ditto.\n+\t(alloc_small_page): Don't care about bytes_free anymore.\n+\t(alloc_large_page): Round up size.\n+\t(ggc_alloc_zone_1): Mark large objects as such, and calculate\n+\ttheir size the new way. \n+\tRemove page table lookups and setting.\n+\t(ggc_get_size): Calculate large object size the new way.\n+\t(sweep_pages): Redo to account for fact that we no longer have\n+\tbytes_free.\n+\t(ggc_collect): No longer need to reincrement bytes_free.\n+\t(ggc_pch_alloc_object): Handle new large objects properly.\n+\t(ggc_pch_read): Put PCH stuff into it's own uncollected zone.\n+\n 2004-01-05  Kazu Hirata  <kazu@cs.umass.edu>\n \n \t* doc/invoke.texi: Remove a page break."}, {"sha": "5a7d6d447f552e8f11bfe77a595211d4cf01029b", "filename": "gcc/ggc-zone.c", "status": "modified", "additions": 62, "deletions": 423, "changes": 485, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb/gcc%2Fggc-zone.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb/gcc%2Fggc-zone.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fggc-zone.c?ref=02fef8539ebbe2e8d71f1c8fcb699942e4e0cfdb", "patch": "@@ -1,6 +1,7 @@\n /* \"Bag-of-pages\" zone garbage collector for the GNU compiler.\n    Copyright (C) 1999, 2000, 2001, 2002, 2003 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson (rth@redhat.com) and Daniel Berlin (dberlin@dberlin.org)\n+   Contributed by Richard Henderson (rth@redhat.com) and Daniel Berlin\n+   (dberlin@dberlin.org) \n \n \n This file is part of GCC.\n@@ -76,7 +77,7 @@ Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n #endif\n \n #ifndef USING_MMAP\n-#define USING_MALLOC_PAGE_GROUPS\n+#error \"Zone collector requires mmap\"\n #endif\n \n #if (GCC_VERSION < 3001)\n@@ -143,7 +144,8 @@ struct alloc_chunk {\n   unsigned int magic;\n #endif\n   unsigned int type:1;\n-  unsigned int typecode:15;\n+  unsigned int typecode:14;\n+  unsigned int large:1;\n   unsigned int size:15;\n   unsigned int mark:1;\n   union {\n@@ -211,38 +213,6 @@ struct max_alignment {\n \n #define ROUND_UP(x, f) (CEIL (x, f) * (f))\n \n-/* A two-level tree is used to look up the page-entry for a given\n-   pointer.  Two chunks of the pointer's bits are extracted to index\n-   the first and second levels of the tree, as follows:\n-\n-\t\t\t\t   HOST_PAGE_SIZE_BITS\n-\t\t\t   32\t\t|      |\n-       msb +----------------+----+------+------+ lsb\n-\t\t\t    |    |      |\n-\t\t\t PAGE_L1_BITS   |\n-\t\t\t\t |      |\n-\t\t\t       PAGE_L2_BITS\n-\n-   The bottommost HOST_PAGE_SIZE_BITS are ignored, since page-entry\n-   pages are aligned on system page boundaries.  The next most\n-   significant PAGE_L2_BITS and PAGE_L1_BITS are the second and first\n-   index values in the lookup table, respectively.\n-\n-   For 32-bit architectures and the settings below, there are no\n-   leftover bits.  For architectures with wider pointers, the lookup\n-   tree points to a list of pages, which must be scanned to find the\n-   correct one.  */\n-\n-#define PAGE_L1_BITS\t(8)\n-#define PAGE_L2_BITS\t(32 - PAGE_L1_BITS - G.lg_pagesize)\n-#define PAGE_L1_SIZE\t((size_t) 1 << PAGE_L1_BITS)\n-#define PAGE_L2_SIZE\t((size_t) 1 << PAGE_L2_BITS)\n-\n-#define LOOKUP_L1(p) \\\n-  (((size_t) (p) >> (32 - PAGE_L1_BITS)) & ((1 << PAGE_L1_BITS) - 1))\n-\n-#define LOOKUP_L2(p) \\\n-  (((size_t) (p) >> G.lg_pagesize) & ((1 << PAGE_L2_BITS) - 1))\n \n /* A page_entry records the status of an allocation page.  */\n typedef struct page_entry\n@@ -261,15 +231,6 @@ typedef struct page_entry\n   /* The address at which the memory is allocated.  */\n   char *page;\n \n-#ifdef USING_MALLOC_PAGE_GROUPS\n-  /* Back pointer to the page group this page came from.  */\n-  struct page_group *group;\n-#endif\n-\n-  /* Number of bytes on the page unallocated.  Only used during\n-     collection, and even then large pages merely set this nonzero.  */\n-  size_t bytes_free;\n-\n   /* Context depth of this page.  */\n   unsigned short context_depth;\n \n@@ -280,50 +241,10 @@ typedef struct page_entry\n   struct alloc_zone *zone;\n } page_entry;\n \n-#ifdef USING_MALLOC_PAGE_GROUPS\n-/* A page_group describes a large allocation from malloc, from which\n-   we parcel out aligned pages.  */\n-typedef struct page_group\n-{\n-  /* A linked list of all extant page groups.  */\n-  struct page_group *next;\n-\n-  /* The address we received from malloc.  */\n-  char *allocation;\n-\n-  /* The size of the block.  */\n-  size_t alloc_size;\n-\n-  /* A bitmask of pages in use.  */\n-  unsigned int in_use;\n-} page_group;\n-#endif\n-\n-#if HOST_BITS_PER_PTR <= 32\n-\n-/* On 32-bit hosts, we use a two level page table, as pictured above.  */\n-typedef page_entry **page_table[PAGE_L1_SIZE];\n-\n-#else\n-\n-/* On 64-bit hosts, we use the same two level page tables plus a linked\n-   list that disambiguates the top 32-bits.  There will almost always be\n-   exactly one entry in the list.  */\n-typedef struct page_table_chain\n-{\n-  struct page_table_chain *next;\n-  size_t high_bits;\n-  page_entry **table[PAGE_L1_SIZE];\n-} *page_table;\n-\n-#endif\n \n /* The global variables.  */\n static struct globals\n {\n-  /* The page lookup table.  A single page can only belong to one\n-     zone.  This means free pages are zone-specific ATM.  */\n-  page_table lookup;\n   /* The linked list of zones.  */\n   struct alloc_zone *zones;\n \n@@ -374,10 +295,6 @@ struct alloc_zone\n   /* A cache of free system pages.  */\n   page_entry *free_pages;\n \n-#ifdef USING_MALLOC_PAGE_GROUPS\n-  page_group *page_groups;\n-#endif\n-\n   /* Next zone in the linked list of zones.  */\n   struct alloc_zone *next_zone;\n \n@@ -399,16 +316,9 @@ struct alloc_zone *tree_zone;\n #define GGC_QUIRE_SIZE 16\n \n static int ggc_allocated_p (const void *);\n-static page_entry *lookup_page_table_entry (const void *);\n-static void set_page_table_entry (void *, page_entry *);\n #ifdef USING_MMAP\n static char *alloc_anon (char *, size_t, struct alloc_zone *);\n #endif\n-#ifdef USING_MALLOC_PAGE_GROUPS\n-static size_t page_group_index (char *, char *);\n-static void set_page_group_in_use (page_group *, char *);\n-static void clear_page_group_in_use (page_group *, char *);\n-#endif\n static struct page_entry * alloc_small_page ( struct alloc_zone *);\n static struct page_entry * alloc_large_page (size_t, struct alloc_zone *);\n static void free_chunk (struct alloc_chunk *, size_t, struct alloc_zone *);\n@@ -425,94 +335,17 @@ static void check_cookies (void);\n static inline int\n ggc_allocated_p (const void *p)\n {\n-  page_entry ***base;\n-  size_t L1, L2;\n-\n-#if HOST_BITS_PER_PTR <= 32\n-  base = &G.lookup[0];\n-#else\n-  page_table table = G.lookup;\n-  size_t high_bits = (size_t) p & ~ (size_t) 0xffffffff;\n-  while (1)\n-    {\n-      if (table == NULL)\n-\treturn 0;\n-      if (table->high_bits == high_bits)\n-\tbreak;\n-      table = table->next;\n-    }\n-  base = &table->table[0];\n-#endif\n-\n-  /* Extract the level 1 and 2 indices.  */\n-  L1 = LOOKUP_L1 (p);\n-  L2 = LOOKUP_L2 (p);\n-\n-  return base[L1] && base[L1][L2];\n-}\n-\n-/* Traverse the page table and find the entry for a page.\n-   Die (probably) if the object wasn't allocated via GC.  */\n-\n-static inline page_entry *\n-lookup_page_table_entry(const void *p)\n-{\n-  page_entry ***base;\n-  size_t L1, L2;\n-\n-#if HOST_BITS_PER_PTR <= 32\n-  base = &G.lookup[0];\n-#else\n-  page_table table = G.lookup;\n-  size_t high_bits = (size_t) p & ~ (size_t) 0xffffffff;\n-  while (table->high_bits != high_bits)\n-    table = table->next;\n-  base = &table->table[0];\n+  struct alloc_chunk *chunk;\n+  chunk = (struct alloc_chunk *) ((char *)p - CHUNK_OVERHEAD);\n+#ifdef COOKIE_CHECKING\n+  if (chunk->magic != CHUNK_MAGIC)\n+    abort ();\n #endif\n-\n-  /* Extract the level 1 and 2 indices.  */\n-  L1 = LOOKUP_L1 (p);\n-  L2 = LOOKUP_L2 (p);\n-\n-  return base[L1][L2];\n-\n+  if (chunk->type == 1)\n+    return true;  \n+  return false;\n }\n \n-/* Set the page table entry for a page.  */\n-\n-static void\n-set_page_table_entry(void *p, page_entry *entry)\n-{\n-  page_entry ***base;\n-  size_t L1, L2;\n-\n-#if HOST_BITS_PER_PTR <= 32\n-  base = &G.lookup[0];\n-#else\n-  page_table table;\n-  size_t high_bits = (size_t) p & ~ (size_t) 0xffffffff;\n-  for (table = G.lookup; table; table = table->next)\n-    if (table->high_bits == high_bits)\n-      goto found;\n-\n-  /* Not found -- allocate a new table.  */\n-  table = (page_table) xcalloc (1, sizeof(*table));\n-  table->next = G.lookup;\n-  table->high_bits = high_bits;\n-  G.lookup = table;\n-found:\n-  base = &table->table[0];\n-#endif\n-\n-  /* Extract the level 1 and 2 indices.  */\n-  L1 = LOOKUP_L1 (p);\n-  L2 = LOOKUP_L2 (p);\n-\n-  if (base[L1] == NULL)\n-    base[L1] = (page_entry **) xcalloc (PAGE_L2_SIZE, sizeof (page_entry *));\n-\n-  base[L1][L2] = entry;\n-}\n \n #ifdef USING_MMAP\n /* Allocate SIZE bytes of anonymous memory, preferably near PREF,\n@@ -547,42 +380,15 @@ alloc_anon (char *pref ATTRIBUTE_UNUSED, size_t size, struct alloc_zone *zone)\n   return page;\n }\n #endif\n-#ifdef USING_MALLOC_PAGE_GROUPS\n-/* Compute the index for this page into the page group.  */\n-\n-static inline size_t\n-page_group_index (char *allocation, char *page)\n-{\n-  return (size_t) (page - allocation) >> G.lg_pagesize;\n-}\n-\n-/* Set and clear the in_use bit for this page in the page group.  */\n-\n-static inline void\n-set_page_group_in_use (page_group *group, char *page)\n-{\n-  group->in_use |= 1 << page_group_index (group->allocation, page);\n-}\n-\n-static inline void\n-clear_page_group_in_use (page_group *group, char *page)\n-{\n-  group->in_use &= ~(1 << page_group_index (group->allocation, page));\n-}\n-#endif\n \n /* Allocate a new page for allocating objects of size 2^ORDER,\n-   and return an entry for it.  The entry is not added to the\n-   appropriate page_table list.  */\n+   and return an entry for it.  */\n \n static inline struct page_entry *\n alloc_small_page (struct alloc_zone *zone)\n {\n   struct page_entry *entry;\n   char *page;\n-#ifdef USING_MALLOC_PAGE_GROUPS\n-  page_group *group;\n-#endif\n \n   page = NULL;\n \n@@ -594,9 +400,7 @@ alloc_small_page (struct alloc_zone *zone)\n       zone->free_pages = entry->next;\n       page = entry->page;\n \n-#ifdef USING_MALLOC_PAGE_GROUPS\n-      group = entry->group;\n-#endif\n+\n     }\n #ifdef USING_MMAP\n   else\n@@ -623,104 +427,27 @@ alloc_small_page (struct alloc_zone *zone)\n       zone->free_pages = f;\n     }\n #endif\n-#ifdef USING_MALLOC_PAGE_GROUPS\n-  else\n-    {\n-      /* Allocate a large block of memory and serve out the aligned\n-\t pages therein.  This results in much less memory wastage\n-\t than the traditional implementation of valloc.  */\n-\n-      char *allocation, *a, *enda;\n-      size_t alloc_size, head_slop, tail_slop;\n-      int multiple_pages = (entry_size == G.pagesize);\n-\n-      if (multiple_pages)\n-\talloc_size = GGC_QUIRE_SIZE * G.pagesize;\n-      else\n-\talloc_size = entry_size + G.pagesize - 1;\n-      allocation = xmalloc (alloc_size);\n-      VALGRIND_MALLOCLIKE_BLOCK(addr, alloc_size, 0, 0);\n-\n-      page = (char *) (((size_t) allocation + G.pagesize - 1) & -G.pagesize);\n-      head_slop = page - allocation;\n-      if (multiple_pages)\n-\ttail_slop = ((size_t) allocation + alloc_size) & (G.pagesize - 1);\n-      else\n-\ttail_slop = alloc_size - entry_size - head_slop;\n-      enda = allocation + alloc_size - tail_slop;\n-\n-      /* We allocated N pages, which are likely not aligned, leaving\n-\t us with N-1 usable pages.  We plan to place the page_group\n-\t structure somewhere in the slop.  */\n-      if (head_slop >= sizeof (page_group))\n-\tgroup = (page_group *)page - 1;\n-      else\n-\t{\n-\t  /* We magically got an aligned allocation.  Too bad, we have\n-\t     to waste a page anyway.  */\n-\t  if (tail_slop == 0)\n-\t    {\n-\t      enda -= G.pagesize;\n-\t      tail_slop += G.pagesize;\n-\t    }\n-\t  if (tail_slop < sizeof (page_group))\n-\t    abort ();\n-\t  group = (page_group *)enda;\n-\t  tail_slop -= sizeof (page_group);\n-\t}\n-\n-      /* Remember that we allocated this memory.  */\n-      group->next = G.page_groups;\n-      group->allocation = allocation;\n-      group->alloc_size = alloc_size;\n-      group->in_use = 0;\n-      zone->page_groups = group;\n-      G.bytes_mapped += alloc_size;\n-\n-      /* If we allocated multiple pages, put the rest on the free list.  */\n-      if (multiple_pages)\n-\t{\n-\t  struct page_entry *e, *f = G.free_pages;\n-\t  for (a = enda - G.pagesize; a != page; a -= G.pagesize)\n-\t    {\n-\t      e = (struct page_entry *) xmalloc (sizeof (struct page_entry));\n-\t      e->bytes = G.pagesize;\n-\t      e->page = a;\n-\t      e->group = group;\n-\t      e->next = f;\n-\t      f = e;\n-\t    }\n-\t  zone->free_pages = f;\n-\t}\n-    }\n-#endif\n-\n   if (entry == NULL)\n     entry = (struct page_entry *) xmalloc (sizeof (struct page_entry));\n \n   entry->next = 0;\n   entry->bytes = G.pagesize;\n-  entry->bytes_free = G.pagesize;\n   entry->page = page;\n   entry->context_depth = zone->context_depth;\n   entry->large_p = false;\n   entry->zone = zone;\n   zone->context_depth_allocations |= (unsigned long)1 << zone->context_depth;\n \n-#ifdef USING_MALLOC_PAGE_GROUPS\n-  entry->group = group;\n-  set_page_group_in_use (group, page);\n-#endif\n-\n-  set_page_table_entry (page, entry);\n-\n   if (GGC_DEBUG_LEVEL >= 2)\n     fprintf (G.debug_file,\n \t     \"Allocating %s page at %p, data %p-%p\\n\", entry->zone->name,\n \t     (PTR) entry, page, page + G.pagesize - 1);\n \n   return entry;\n }\n+/* Compute the smallest multiple of F that is >= X.  */\n+\n+#define ROUND_UP(x, f) (CEIL (x, f) * (f))\n \n /* Allocate a large page of size SIZE in ZONE.  */\n \n@@ -729,24 +456,18 @@ alloc_large_page (size_t size, struct alloc_zone *zone)\n {\n   struct page_entry *entry;\n   char *page;\n-\n+  size =  ROUND_UP (size, 1024);\n   page = (char *) xmalloc (size + CHUNK_OVERHEAD + sizeof (struct page_entry));\n   entry = (struct page_entry *) (page + size + CHUNK_OVERHEAD);\n \n   entry->next = 0;\n   entry->bytes = size;\n-  entry->bytes_free = LARGE_OBJECT_SIZE + CHUNK_OVERHEAD;\n   entry->page = page;\n   entry->context_depth = zone->context_depth;\n   entry->large_p = true;\n   entry->zone = zone;\n   zone->context_depth_allocations |= (unsigned long)1 << zone->context_depth;\n \n-#ifdef USING_MALLOC_PAGE_GROUPS\n-  entry->group = NULL;\n-#endif\n-  set_page_table_entry (page, entry);\n-\n   if (GGC_DEBUG_LEVEL >= 2)\n     fprintf (G.debug_file,\n \t     \"Allocating %s large page at %p, data %p-%p\\n\", entry->zone->name,\n@@ -766,8 +487,6 @@ free_page (page_entry *entry)\n \t     \"Deallocating %s page at %p, data %p-%p\\n\", entry->zone->name, (PTR) entry,\n \t     entry->page, entry->page + entry->bytes - 1);\n \n-  set_page_table_entry (entry->page, NULL);\n-\n   if (entry->large_p)\n     {\n       free (entry->page);\n@@ -779,10 +498,6 @@ free_page (page_entry *entry)\n \t avoid handle leak.  */\n       VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (entry->page, entry->bytes));\n \n-#ifdef USING_MALLOC_PAGE_GROUPS\n-      clear_page_group_in_use (entry->group, entry->page);\n-#endif\n-\n       entry->next = entry->zone->free_pages;\n       entry->zone->free_pages = entry;\n     }\n@@ -823,34 +538,6 @@ release_pages (struct alloc_zone *zone)\n \n   zone->free_pages = NULL;\n #endif\n-#ifdef USING_MALLOC_PAGE_GROUPS\n-  page_entry **pp, *p;\n-  page_group **gp, *g;\n-\n-  /* Remove all pages from free page groups from the list.  */\n-  pp = &(zone->free_pages);\n-  while ((p = *pp) != NULL)\n-    if (p->group->in_use == 0)\n-      {\n-\t*pp = p->next;\n-\tfree (p);\n-      }\n-    else\n-      pp = &p->next;\n-\n-  /* Remove all free page groups, and release the storage.  */\n-  gp = &(zone->page_groups);\n-  while ((g = *gp) != NULL)\n-    if (g->in_use == 0)\n-      {\n-\t*gp = g->next;\n-\tzone->bytes_mapped -= g->alloc_size;\n-\tfree (g->allocation);\n-\tVALGRIND_FREELIKE_BLOCK(g->allocation, 0);\n-      }\n-    else\n-      gp = &g->next;\n-#endif\n }\n \n /* Place CHUNK of size SIZE on the free list for ZONE.  */\n@@ -896,15 +583,16 @@ ggc_alloc_zone_1 (size_t size, struct alloc_zone *zone, short type)\n   /* Large objects are handled specially.  */\n   if (size >= G.pagesize - 2*CHUNK_OVERHEAD - FREE_BIN_DELTA)\n     {\n+      size = ROUND_UP (size, 1024);\n       entry = alloc_large_page (size, zone);\n       entry->survived = 0;\n       entry->next = entry->zone->pages;\n       entry->zone->pages = entry;\n \n-\n       chunk = (struct alloc_chunk *) entry->page;\n       VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (chunk, sizeof (struct alloc_chunk)));\n-      chunk->size = LARGE_OBJECT_SIZE;\n+      chunk->large = 1;\n+      chunk->size = CEIL (size, 1024);\n \n       goto found;\n     }\n@@ -943,11 +631,13 @@ ggc_alloc_zone_1 (size_t size, struct alloc_zone *zone, short type)\n       chunk = (struct alloc_chunk *) entry->page;\n       VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (chunk, sizeof (struct alloc_chunk)));\n       chunk->size = G.pagesize - CHUNK_OVERHEAD;\n+      chunk->large = 0;\n     }\n   else\n     {\n       *pp = chunk->u.next_free;\n       VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (chunk, sizeof (struct alloc_chunk)));\n+      chunk->large = 0;\n     }\n   /* Release extra memory from a chunk that's too big.  */\n   lsize = chunk->size - size;\n@@ -965,6 +655,7 @@ ggc_alloc_zone_1 (size_t size, struct alloc_zone *zone, short type)\n       lchunk->type = 0;\n       lchunk->mark = 0;\n       lchunk->size = lsize;\n+      lchunk->large = 0;\n       free_chunk (lchunk, lsize, zone);\n     }\n   /* Calculate the object's address.  */\n@@ -1049,16 +740,8 @@ ggc_alloc_zone (size_t size, struct alloc_zone *zone)\n int\n ggc_set_mark (const void *p)\n {\n-  page_entry *entry;\n   struct alloc_chunk *chunk;\n \n-#ifdef ENABLE_CHECKING\n-  /* Look up the page on which the object is alloced.  If the object\n-     wasn't allocated by the collector, we'll probably die.  */\n-  entry = lookup_page_table_entry (p);\n-  if (entry == NULL)\n-    abort ();\n-#endif\n   chunk = (struct alloc_chunk *) ((char *)p - CHUNK_OVERHEAD);\n #ifdef COOKIE_CHECKING\n   if (chunk->magic != CHUNK_MAGIC)\n@@ -1068,17 +751,6 @@ ggc_set_mark (const void *p)\n     return 1;\n   chunk->mark = 1;\n \n-#ifndef ENABLE_CHECKING\n-  entry = lookup_page_table_entry (p);\n-#endif\n-\n-  /* Large pages are either completely full or completely empty. So if\n-     they are marked, they are completely full.  */\n-  if (entry->large_p)\n-    entry->bytes_free = 0;\n-  else\n-    entry->bytes_free -= chunk->size + CHUNK_OVERHEAD;\n-\n   if (GGC_DEBUG_LEVEL >= 4)\n     fprintf (G.debug_file, \"Marking %p\\n\", p);\n \n@@ -1094,14 +766,6 @@ ggc_marked_p (const void *p)\n {\n   struct alloc_chunk *chunk;\n \n-#ifdef ENABLE_CHECKING\n-  {\n-    page_entry *entry = lookup_page_table_entry (p);\n-    if (entry == NULL)\n-      abort ();\n-  }\n-#endif\n-\n   chunk = (struct alloc_chunk *) ((char *)p - CHUNK_OVERHEAD);\n #ifdef COOKIE_CHECKING\n   if (chunk->magic != CHUNK_MAGIC)\n@@ -1116,26 +780,14 @@ size_t\n ggc_get_size (const void *p)\n {\n   struct alloc_chunk *chunk;\n-  struct page_entry *entry;\n-\n-#ifdef ENABLE_CHECKING\n-  entry = lookup_page_table_entry (p);\n-  if (entry == NULL)\n-    abort ();\n-#endif\n \n   chunk = (struct alloc_chunk *) ((char *)p - CHUNK_OVERHEAD);\n #ifdef COOKIE_CHECKING\n   if (chunk->magic != CHUNK_MAGIC)\n     abort ();\n #endif\n-  if (chunk->size == LARGE_OBJECT_SIZE)\n-    {\n-#ifndef ENABLE_CHECKING\n-      entry = lookup_page_table_entry (p);\n-#endif\n-      return entry->bytes;\n-    }\n+  if (chunk->large)\n+    return chunk->size * 1024;\n \n   return chunk->size;\n }\n@@ -1278,8 +930,6 @@ ggc_pop_context (void)\n   for (zone = G.zones; zone; zone = zone->next_zone)\n     ggc_pop_context_1 (zone);\n }\n-\n-\n /* Poison the chunk.  */\n #ifdef ENABLE_GC_CHECKING\n #define poison_chunk(CHUNK, SIZE) \\\n@@ -1296,40 +946,35 @@ sweep_pages (struct alloc_zone *zone)\n   page_entry **pp, *p, *next;\n   struct alloc_chunk *chunk, *last_free, *end;\n   size_t last_free_size, allocated = 0;\n-\n+  bool nomarksinpage;\n   /* First, reset the free_chunks lists, since we are going to\n      re-free free chunks in hopes of coalescing them into large chunks.  */\n   memset (zone->free_chunks, 0, sizeof (zone->free_chunks));\n   pp = &zone->pages;\n   for (p = zone->pages; p ; p = next)\n     {\n       next = p->next;\n-\n-      /* For empty pages, just free the page.  */\n-      if (p->bytes_free == G.pagesize && p->context_depth == zone->context_depth)\n+      /* Large pages are all or none affairs. Either they are\n+\t completely empty, or they are completely full.\n+\t \n+\t XXX: Should we bother to increment allocated.  */\n+      if (p->large_p)\n \t{\n-\t  *pp = next;\n+\t  if (((struct alloc_chunk *)p->page)->mark == 1)\n+\t    {\n+\t      ((struct alloc_chunk *)p->page)->mark = 0;\n+\t    }\n+\t  else\n+\t    {\n+\t      *pp = next;\n #ifdef ENABLE_GC_CHECKING\n \t  /* Poison the page.  */\n \t  memset (p->page, 0xb5, p->bytes);\n #endif\n-\t  free_page (p);\n-\t  continue;\n-\t}\n-\n-      /* Large pages are all or none affairs. Either they are\n-\t completely empty, or they are completely full.\n-\t Thus, if the above didn't catch it, we need not do anything\n-\t except remove the mark and reset the bytes_free.\n-\n-\t XXX: Should we bother to increment allocated.  */\n-      else if (p->large_p)\n-\t{\n-\t  p->bytes_free = p->bytes;\n-\t  ((struct alloc_chunk *)p->page)->mark = 0;\n+\t      free_page (p);\n+\t    }\n \t  continue;\n \t}\n-      pp = &p->next;\n \n       /* This page has now survived another collection.  */\n       p->survived++;\n@@ -1343,12 +988,13 @@ sweep_pages (struct alloc_zone *zone)\n       end = (struct alloc_chunk *)(p->page + G.pagesize);\n       last_free = NULL;\n       last_free_size = 0;\n-\n+      nomarksinpage = true;\n       do\n \t{\n \t  prefetch ((struct alloc_chunk *)(chunk->u.data + chunk->size));\n \t  if (chunk->mark || p->context_depth < zone->context_depth)\n \t    {\n+\t      nomarksinpage = false;\n \t      if (last_free)\n \t\t{\n \t\t  last_free->type = 0;\n@@ -1361,13 +1007,8 @@ sweep_pages (struct alloc_zone *zone)\n \t      if (chunk->mark)\n \t        {\n \t          allocated += chunk->size + CHUNK_OVERHEAD;\n- \t          p->bytes_free += chunk->size + CHUNK_OVERHEAD;\n \t\t}\n \t      chunk->mark = 0;\n-#ifdef ENABLE_CHECKING\n-\t      if (p->bytes_free > p->bytes)\n-\t\tabort ();\n-#endif\n \t    }\n \t  else\n \t    {\n@@ -1386,14 +1027,25 @@ sweep_pages (struct alloc_zone *zone)\n \t}\n       while (chunk < end);\n \n-      if (last_free)\n+      if (nomarksinpage)\n+\t{\n+\t  *pp = next;\n+#ifdef ENABLE_GC_CHECKING\n+\t  /* Poison the page.  */\n+\t  memset (p->page, 0xb5, p->bytes);\n+#endif\n+\t  free_page (p);\n+\t  continue;\n+\t}\n+      else if (last_free)\n \t{\n \t  last_free->type = 0;\n \t  last_free->size = last_free_size;\n \t  last_free->mark = 0;\n \t  poison_chunk (last_free, last_free_size);\n \t  free_chunk (last_free, last_free_size, zone);\n \t}\n+      pp = &p->next;\n     }\n \n   zone->allocated = allocated;\n@@ -1496,8 +1148,6 @@ check_cookies (void)\n     }\n #endif\n }\n-\n-\n /* Top level collection routine.  */\n \n void\n@@ -1569,12 +1219,6 @@ ggc_collect (void)\n \t\t    prefetch ((struct alloc_chunk *)(chunk->u.data + chunk->size));\n \t\t    if (chunk->mark || p->context_depth < zone->context_depth)\n \t\t      {\n-\t\t        if (chunk->mark)\n-\t\t \t  p->bytes_free += chunk->size + CHUNK_OVERHEAD;\n-#ifdef ENABLE_CHECKING\n-\t\t\tif (p->bytes_free > p->bytes)\n-\t\t\t  abort ();\n-#endif\n \t\t\tchunk->mark = 0;\n \t\t      }\n \t\t    chunk = (struct alloc_chunk *)(chunk->u.data + chunk->size);\n@@ -1583,7 +1227,6 @@ ggc_collect (void)\n \t      }\n \t    else\n \t      {\n-\t\tp->bytes_free = p->bytes;\n \t\t((struct alloc_chunk *)p->page)->mark = 0;\n \t      }\n \t  }\n@@ -1679,7 +1322,7 @@ ggc_pch_alloc_object (struct ggc_pch_data *d, void *x,\n   if (!is_string)\n     {\n       struct alloc_chunk *chunk = (struct alloc_chunk *) ((char *)x - CHUNK_OVERHEAD);\n-      if (chunk->size == LARGE_OBJECT_SIZE)\n+      if (chunk->large)\n \td->base += ggc_get_size (x) + CHUNK_OVERHEAD;\n       else\n \td->base += chunk->size + CHUNK_OVERHEAD;\n@@ -1735,24 +1378,20 @@ ggc_pch_finish (struct ggc_pch_data *d, FILE *f)\n     fatal_error (\"can't write PCH file: %m\");\n   free (d);\n }\n-\n-\n void\n ggc_pch_read (FILE *f, void *addr)\n {\n   struct ggc_pch_ondisk d;\n   struct page_entry *entry;\n-  char *pte;\n+  struct alloc_zone *pch_zone;\n   if (fread (&d, sizeof (d), 1, f) != 1)\n     fatal_error (\"can't read PCH file: %m\");\n   entry = xcalloc (1, sizeof (struct page_entry));\n   entry->bytes = d.total;\n   entry->page = addr;\n   entry->context_depth = 0;\n-  entry->zone = &main_zone;\n-  for (pte = entry->page;\n-       pte < entry->page + entry->bytes;\n-       pte += G.pagesize)\n-    set_page_table_entry (pte, entry);\n-\n+  pch_zone = new_ggc_zone (\"PCH zone\");\n+  entry->zone = pch_zone;\n+  entry->next = entry->zone->pages;\n+  entry->zone->pages = entry;\n }"}]}