{"sha": "ae0533da54015456538aa8ebeddf385230c36c98", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YWUwNTMzZGE1NDAxNTQ1NjUzOGFhOGViZWRkZjM4NTIzMGMzNmM5OA==", "commit": {"author": {"name": "Alan Lawrence", "email": "alan.lawrence@arm.com", "date": "2014-05-29T16:57:42Z"}, "committer": {"name": "Alan Lawrence", "email": "alalaw01@gcc.gnu.org", "date": "2014-05-29T16:57:42Z"}, "message": "Detect EXT patterns to vec_perm_const, use for EXT intrinsics\n\n        * config/aarch64/aarch64-builtins.c (aarch64_types_binopv_qualifiers,\n        TYPES_BINOPV): New static data.\n        * config/aarch64/aarch64-simd-builtins.def (im_lane_bound): New builtin.\n        * config/aarch64/aarch64-simd.md (aarch64_ext, aarch64_im_lane_boundsi):\n        New patterns.\n        * config/aarch64/aarch64.c (aarch64_expand_vec_perm_const_1): Match\n        patterns for EXT.\n        (aarch64_evpc_ext): New function.\n\n        * config/aarch64/iterators.md (UNSPEC_EXT): New enum element.\n\n        * config/aarch64/arm_neon.h (vext_f32, vext_f64, vext_p8, vext_p16,\n        vext_s8, vext_s16, vext_s32, vext_s64, vext_u8, vext_u16, vext_u32,\n        vext_u64, vextq_f32, vextq_f64, vextq_p8, vextq_p16, vextq_s8,\n        vextq_s16, vextq_s32, vextq_s64, vextq_u8, vextq_u16, vextq_u32,\n        vextq_u64): Replace __asm with __builtin_shuffle and im_lane_boundsi.\n\nFrom-SVN: r211058", "tree": {"sha": "541f7a9824180da9820f0042cf6d33cf07317218", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/541f7a9824180da9820f0042cf6d33cf07317218"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/ae0533da54015456538aa8ebeddf385230c36c98", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ae0533da54015456538aa8ebeddf385230c36c98", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ae0533da54015456538aa8ebeddf385230c36c98", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ae0533da54015456538aa8ebeddf385230c36c98/comments", "author": null, "committer": null, "parents": [{"sha": "ed00b1fb97eede0e1f72d69180259ce74079376f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ed00b1fb97eede0e1f72d69180259ce74079376f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ed00b1fb97eede0e1f72d69180259ce74079376f"}], "stats": {"total": 722, "additions": 409, "deletions": 313}, "files": [{"sha": "273bf64570e7b832ad03a494cae3250ac944ddcd", "filename": "gcc/ChangeLog", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=ae0533da54015456538aa8ebeddf385230c36c98", "patch": "@@ -1,3 +1,22 @@\n+2014-05-29  Alan Lawrence  <alan.lawrence@arm.com>\n+\n+\t* config/aarch64/aarch64-builtins.c (aarch64_types_binopv_qualifiers,\n+\tTYPES_BINOPV): New static data.\n+\t* config/aarch64/aarch64-simd-builtins.def (im_lane_bound): New builtin.\n+\t* config/aarch64/aarch64-simd.md (aarch64_ext, aarch64_im_lane_boundsi):\n+\tNew patterns.\n+\t* config/aarch64/aarch64.c (aarch64_expand_vec_perm_const_1): Match\n+\tpatterns for EXT.\n+\t(aarch64_evpc_ext): New function.\n+\n+\t* config/aarch64/iterators.md (UNSPEC_EXT): New enum element.\n+\n+\t* config/aarch64/arm_neon.h (vext_f32, vext_f64, vext_p8, vext_p16,\n+\tvext_s8, vext_s16, vext_s32, vext_s64, vext_u8, vext_u16, vext_u32,\n+\tvext_u64, vextq_f32, vextq_f64, vextq_p8, vextq_p16, vextq_s8,\n+\tvextq_s16, vextq_s32, vextq_s64, vextq_u8, vextq_u16, vextq_u32,\n+\tvextq_u64): Replace __asm with __builtin_shuffle and im_lane_boundsi.\n+\n 2014-05-29  Tom de Vries  <tom@codesourcery.com>\n \n \t* rtl.h (BLOCK_SYMBOL_CHECK): Use SYMBOL_REF_FLAGS."}, {"sha": "ca14d51715d30f5695fe81edb02c89f382c8603d", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=ae0533da54015456538aa8ebeddf385230c36c98", "patch": "@@ -169,6 +169,10 @@ aarch64_types_binop_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_none, qualifier_maybe_immediate };\n #define TYPES_BINOP (aarch64_types_binop_qualifiers)\n static enum aarch64_type_qualifiers\n+aarch64_types_binopv_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_void, qualifier_none, qualifier_none };\n+#define TYPES_BINOPV (aarch64_types_binopv_qualifiers)\n+static enum aarch64_type_qualifiers\n aarch64_types_binopu_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_unsigned, qualifier_unsigned, qualifier_unsigned };\n #define TYPES_BINOPU (aarch64_types_binopu_qualifiers)"}, {"sha": "b5d9965cbcbb2e074f40bebdc83e40049fe313a1", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=ae0533da54015456538aa8ebeddf385230c36c98", "patch": "@@ -410,3 +410,6 @@\n   /* Implemented by aarch64_crypto_pmull<mode>.  */\n   VAR1 (BINOPP, crypto_pmull, 0, di)\n   VAR1 (BINOPP, crypto_pmull, 0, v2di)\n+\n+  /* Meta-op to check lane bounds of immediate in aarch64_expand_builtin.  */\n+  VAR1 (BINOPV, im_lane_bound, 0, si)"}, {"sha": "c239677a58116d97b788254d23bb144718263313", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 29, "deletions": 0, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=ae0533da54015456538aa8ebeddf385230c36c98", "patch": "@@ -4167,6 +4167,35 @@\n   [(set_attr \"type\" \"neon_permute<q>\")]\n )\n \n+;; Note immediate (third) operand is lane index not byte index.\n+(define_insn \"aarch64_ext<mode>\"\n+  [(set (match_operand:VALL 0 \"register_operand\" \"=w\")\n+        (unspec:VALL [(match_operand:VALL 1 \"register_operand\" \"w\")\n+                      (match_operand:VALL 2 \"register_operand\" \"w\")\n+                      (match_operand:SI 3 \"immediate_operand\" \"i\")]\n+                     UNSPEC_EXT))]\n+  \"TARGET_SIMD\"\n+{\n+  operands[3] = GEN_INT (INTVAL (operands[3])\n+      * GET_MODE_SIZE (GET_MODE_INNER (<MODE>mode)));\n+  return \"ext\\\\t%0.<Vbtype>, %1.<Vbtype>, %2.<Vbtype>, #%3\";\n+}\n+  [(set_attr \"type\" \"neon_ext<q>\")]\n+)\n+\n+;; This exists solely to check the arguments to the corresponding __builtin.\n+;; Used where we want an error for out-of-range indices which would otherwise\n+;; be silently wrapped (e.g. the mask to a __builtin_shuffle).\n+(define_expand \"aarch64_im_lane_boundsi\"\n+  [(match_operand:SI 0 \"immediate_operand\" \"i\")\n+   (match_operand:SI 1 \"immediate_operand\" \"i\")]\n+  \"TARGET_SIMD\"\n+{\n+  aarch64_simd_lane_bounds (operands[0], 0, INTVAL (operands[1]));\n+  DONE;\n+}\n+)\n+\n (define_insn \"aarch64_st2<mode>_dreg\"\n   [(set (match_operand:TI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n \t(unspec:TI [(match_operand:OI 1 \"register_operand\" \"w\")"}, {"sha": "f69457a5068b04659282f2a5fd518e4c1765d77c", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 67, "deletions": 1, "changes": 68, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=ae0533da54015456538aa8ebeddf385230c36c98", "patch": "@@ -8990,6 +8990,70 @@ aarch64_evpc_zip (struct expand_vec_perm_d *d)\n   return true;\n }\n \n+/* Recognize patterns for the EXT insn.  */\n+\n+static bool\n+aarch64_evpc_ext (struct expand_vec_perm_d *d)\n+{\n+  unsigned int i, nelt = d->nelt;\n+  rtx (*gen) (rtx, rtx, rtx, rtx);\n+  rtx offset;\n+\n+  unsigned int location = d->perm[0]; /* Always < nelt.  */\n+\n+  /* Check if the extracted indices are increasing by one.  */\n+  for (i = 1; i < nelt; i++)\n+    {\n+      unsigned int required = location + i;\n+      if (d->one_vector_p)\n+        {\n+          /* We'll pass the same vector in twice, so allow indices to wrap.  */\n+\t  required &= (nelt - 1);\n+\t}\n+      if (d->perm[i] != required)\n+        return false;\n+    }\n+\n+  /* The mid-end handles masks that just return one of the input vectors.  */\n+  gcc_assert (location != 0);\n+\n+  switch (d->vmode)\n+    {\n+    case V16QImode: gen = gen_aarch64_extv16qi; break;\n+    case V8QImode: gen = gen_aarch64_extv8qi; break;\n+    case V4HImode: gen = gen_aarch64_extv4hi; break;\n+    case V8HImode: gen = gen_aarch64_extv8hi; break;\n+    case V2SImode: gen = gen_aarch64_extv2si; break;\n+    case V4SImode: gen = gen_aarch64_extv4si; break;\n+    case V2SFmode: gen = gen_aarch64_extv2sf; break;\n+    case V4SFmode: gen = gen_aarch64_extv4sf; break;\n+    case V2DImode: gen = gen_aarch64_extv2di; break;\n+    case V2DFmode: gen = gen_aarch64_extv2df; break;\n+    default:\n+      return false;\n+    }\n+\n+  /* Success! */\n+  if (d->testing_p)\n+    return true;\n+\n+  if (BYTES_BIG_ENDIAN)\n+    {\n+      /* After setup, we want the high elements of the first vector (stored\n+         at the LSB end of the register), and the low elements of the second\n+         vector (stored at the MSB end of the register). So swap.  */\n+      rtx temp = d->op0;\n+      d->op0 = d->op1;\n+      d->op1 = temp;\n+      /* location != 0 (above), so safe to assume (nelt - location) < nelt.  */\n+      location = nelt - location;\n+    }\n+\n+  offset = GEN_INT (location);\n+  emit_insn (gen (d->target, d->op0, d->op1, offset));\n+  return true;\n+}\n+\n static bool\n aarch64_evpc_dup (struct expand_vec_perm_d *d)\n {\n@@ -9094,7 +9158,9 @@ aarch64_expand_vec_perm_const_1 (struct expand_vec_perm_d *d)\n \n   if (TARGET_SIMD)\n     {\n-      if (aarch64_evpc_zip (d))\n+      if (aarch64_evpc_ext (d))\n+\treturn true;\n+      else if (aarch64_evpc_zip (d))\n \treturn true;\n       else if (aarch64_evpc_uzp (d))\n \treturn true;"}, {"sha": "c4b5731996c3400d3776f132c0aca12d90f1a671", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 286, "deletions": 312, "changes": 598, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=ae0533da54015456538aa8ebeddf385230c36c98", "patch": "@@ -5661,318 +5661,6 @@ vcvtxd_f32_f64 (float64_t a)\n   return result;\n }\n \n-#define vext_f32(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float32x2_t b_ = (b);                                            \\\n-       float32x2_t a_ = (a);                                            \\\n-       float32x2_t result;                                              \\\n-       __asm__ (\"ext %0.8b, %1.8b, %2.8b, #%3*4\"                        \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_f64(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float64x1_t b_ = (b);                                            \\\n-       float64x1_t a_ = (a);                                            \\\n-       float64x1_t result;                                              \\\n-       __asm__ (\"ext %0.8b, %1.8b, %2.8b, #%3*8\"                        \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_p8(a, b, c)                                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       poly8x8_t b_ = (b);                                              \\\n-       poly8x8_t a_ = (a);                                              \\\n-       poly8x8_t result;                                                \\\n-       __asm__ (\"ext %0.8b,%1.8b,%2.8b,%3\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_p16(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       poly16x4_t b_ = (b);                                             \\\n-       poly16x4_t a_ = (a);                                             \\\n-       poly16x4_t result;                                               \\\n-       __asm__ (\"ext %0.8b, %1.8b, %2.8b, #%3*2\"                        \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_s8(a, b, c)                                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int8x8_t b_ = (b);                                               \\\n-       int8x8_t a_ = (a);                                               \\\n-       int8x8_t result;                                                 \\\n-       __asm__ (\"ext %0.8b,%1.8b,%2.8b,%3\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_s16(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x4_t b_ = (b);                                              \\\n-       int16x4_t a_ = (a);                                              \\\n-       int16x4_t result;                                                \\\n-       __asm__ (\"ext %0.8b, %1.8b, %2.8b, #%3*2\"                        \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_s32(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x2_t b_ = (b);                                              \\\n-       int32x2_t a_ = (a);                                              \\\n-       int32x2_t result;                                                \\\n-       __asm__ (\"ext %0.8b, %1.8b, %2.8b, #%3*4\"                        \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_s64(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int64x1_t b_ = (b);                                              \\\n-       int64x1_t a_ = (a);                                              \\\n-       int64x1_t result;                                                \\\n-       __asm__ (\"ext %0.8b, %1.8b, %2.8b, #%3*8\"                        \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_u8(a, b, c)                                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint8x8_t b_ = (b);                                              \\\n-       uint8x8_t a_ = (a);                                              \\\n-       uint8x8_t result;                                                \\\n-       __asm__ (\"ext %0.8b,%1.8b,%2.8b,%3\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_u16(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x4_t b_ = (b);                                             \\\n-       uint16x4_t a_ = (a);                                             \\\n-       uint16x4_t result;                                               \\\n-       __asm__ (\"ext %0.8b, %1.8b, %2.8b, #%3*2\"                        \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_u32(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x2_t b_ = (b);                                             \\\n-       uint32x2_t a_ = (a);                                             \\\n-       uint32x2_t result;                                               \\\n-       __asm__ (\"ext %0.8b, %1.8b, %2.8b, #%3*4\"                        \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vext_u64(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint64x1_t b_ = (b);                                             \\\n-       uint64x1_t a_ = (a);                                             \\\n-       uint64x1_t result;                                               \\\n-       __asm__ (\"ext %0.8b, %1.8b, %2.8b, #%3*8\"                        \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_f32(a, b, c)                                              \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float32x4_t b_ = (b);                                            \\\n-       float32x4_t a_ = (a);                                            \\\n-       float32x4_t result;                                              \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3*4\"                     \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_f64(a, b, c)                                              \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float64x2_t b_ = (b);                                            \\\n-       float64x2_t a_ = (a);                                            \\\n-       float64x2_t result;                                              \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3*8\"                     \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_p8(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       poly8x16_t b_ = (b);                                             \\\n-       poly8x16_t a_ = (a);                                             \\\n-       poly8x16_t result;                                               \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3\"                       \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_p16(a, b, c)                                              \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       poly16x8_t b_ = (b);                                             \\\n-       poly16x8_t a_ = (a);                                             \\\n-       poly16x8_t result;                                               \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3*2\"                     \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_s8(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int8x16_t b_ = (b);                                              \\\n-       int8x16_t a_ = (a);                                              \\\n-       int8x16_t result;                                                \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3\"                       \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_s16(a, b, c)                                              \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x8_t b_ = (b);                                              \\\n-       int16x8_t a_ = (a);                                              \\\n-       int16x8_t result;                                                \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3*2\"                     \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_s32(a, b, c)                                              \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x4_t b_ = (b);                                              \\\n-       int32x4_t a_ = (a);                                              \\\n-       int32x4_t result;                                                \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3*4\"                     \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_s64(a, b, c)                                              \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int64x2_t b_ = (b);                                              \\\n-       int64x2_t a_ = (a);                                              \\\n-       int64x2_t result;                                                \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3*8\"                     \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_u8(a, b, c)                                               \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint8x16_t b_ = (b);                                             \\\n-       uint8x16_t a_ = (a);                                             \\\n-       uint8x16_t result;                                               \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3\"                       \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_u16(a, b, c)                                              \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x8_t b_ = (b);                                             \\\n-       uint16x8_t a_ = (a);                                             \\\n-       uint16x8_t result;                                               \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3*2\"                     \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_u32(a, b, c)                                              \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x4_t b_ = (b);                                             \\\n-       uint32x4_t a_ = (a);                                             \\\n-       uint32x4_t result;                                               \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3*4\"                     \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vextq_u64(a, b, c)                                              \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint64x2_t b_ = (b);                                             \\\n-       uint64x2_t a_ = (a);                                             \\\n-       uint64x2_t result;                                               \\\n-       __asm__ (\"ext %0.16b, %1.16b, %2.16b, #%3*8\"                     \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vfma_f32 (float32x2_t a, float32x2_t b, float32x2_t c)\n {\n@@ -17444,6 +17132,292 @@ vdupd_laneq_u64 (uint64x2_t __a, const int __b)\n   return __aarch64_vgetq_lane_u64 (__a, __b);\n }\n \n+/* vext  */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vext_f32 (float32x2_t __a, float32x2_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint32x2_t) {2-__c, 3-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {__c, __c+1});\n+#endif\n+}\n+\n+__extension__ static __inline float64x1_t __attribute__ ((__always_inline__))\n+vext_f64 (float64x1_t __a, float64x1_t __b, __const int __c)\n+{\n+  /* The only possible index to the assembler instruction returns element 0.  */\n+  __builtin_aarch64_im_lane_boundsi (__c, 1);\n+  return __a;\n+}\n+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n+vext_p8 (poly8x8_t __a, poly8x8_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint8x8_t)\n+      {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x8_t) {__c, __c+1, __c+2, __c+3, __c+4, __c+5, __c+6, __c+7});\n+#endif\n+}\n+\n+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n+vext_p16 (poly16x4_t __a, poly16x4_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a,\n+      (uint16x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {__c, __c+1, __c+2, __c+3});\n+#endif\n+}\n+\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vext_s8 (int8x8_t __a, int8x8_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint8x8_t)\n+      {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x8_t) {__c, __c+1, __c+2, __c+3, __c+4, __c+5, __c+6, __c+7});\n+#endif\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vext_s16 (int16x4_t __a, int16x4_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a,\n+      (uint16x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {__c, __c+1, __c+2, __c+3});\n+#endif\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vext_s32 (int32x2_t __a, int32x2_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint32x2_t) {2-__c, 3-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {__c, __c+1});\n+#endif\n+}\n+\n+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n+vext_s64 (int64x1_t __a, int64x1_t __b, __const int __c)\n+{\n+  /* The only possible index to the assembler instruction returns element 0.  */\n+  __builtin_aarch64_im_lane_boundsi (__c, 1);\n+  return __a;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vext_u8 (uint8x8_t __a, uint8x8_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint8x8_t)\n+      {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x8_t) {__c, __c+1, __c+2, __c+3, __c+4, __c+5, __c+6, __c+7});\n+#endif\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vext_u16 (uint16x4_t __a, uint16x4_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a,\n+      (uint16x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {__c, __c+1, __c+2, __c+3});\n+#endif\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vext_u32 (uint32x2_t __a, uint32x2_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint32x2_t) {2-__c, 3-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {__c, __c+1});\n+#endif\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vext_u64 (uint64x1_t __a, uint64x1_t __b, __const int __c)\n+{\n+  /* The only possible index to the assembler instruction returns element 0.  */\n+  __builtin_aarch64_im_lane_boundsi (__c, 1);\n+  return __a;\n+}\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vextq_f32 (float32x4_t __a, float32x4_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a,\n+      (uint32x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {__c, __c+1, __c+2, __c+3});\n+#endif\n+}\n+\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vextq_f64 (float64x2_t __a, float64x2_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint64x2_t) {2-__c, 3-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {__c, __c+1});\n+#endif\n+}\n+\n+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n+vextq_p8 (poly8x16_t __a, poly8x16_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 16);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint8x16_t)\n+      {16-__c, 17-__c, 18-__c, 19-__c, 20-__c, 21-__c, 22-__c, 23-__c,\n+       24-__c, 25-__c, 26-__c, 27-__c, 28-__c, 29-__c, 30-__c, 31-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {__c, __c+1, __c+2, __c+3, __c+4, __c+5, __c+6, __c+7,\n+       __c+8, __c+9, __c+10, __c+11, __c+12, __c+13, __c+14, __c+15});\n+#endif\n+}\n+\n+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n+vextq_p16 (poly16x8_t __a, poly16x8_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint16x8_t)\n+      {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+      (uint16x8_t) {__c, __c+1, __c+2, __c+3, __c+4, __c+5, __c+6, __c+7});\n+#endif\n+}\n+\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vextq_s8 (int8x16_t __a, int8x16_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 16);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint8x16_t)\n+      {16-__c, 17-__c, 18-__c, 19-__c, 20-__c, 21-__c, 22-__c, 23-__c,\n+       24-__c, 25-__c, 26-__c, 27-__c, 28-__c, 29-__c, 30-__c, 31-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {__c, __c+1, __c+2, __c+3, __c+4, __c+5, __c+6, __c+7,\n+       __c+8, __c+9, __c+10, __c+11, __c+12, __c+13, __c+14, __c+15});\n+#endif\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vextq_s16 (int16x8_t __a, int16x8_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint16x8_t)\n+      {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+      (uint16x8_t) {__c, __c+1, __c+2, __c+3, __c+4, __c+5, __c+6, __c+7});\n+#endif\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vextq_s32 (int32x4_t __a, int32x4_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a,\n+      (uint32x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {__c, __c+1, __c+2, __c+3});\n+#endif\n+}\n+\n+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n+vextq_s64 (int64x2_t __a, int64x2_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint64x2_t) {2-__c, 3-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {__c, __c+1});\n+#endif\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vextq_u8 (uint8x16_t __a, uint8x16_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 16);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint8x16_t)\n+      {16-__c, 17-__c, 18-__c, 19-__c, 20-__c, 21-__c, 22-__c, 23-__c,\n+       24-__c, 25-__c, 26-__c, 27-__c, 28-__c, 29-__c, 30-__c, 31-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {__c, __c+1, __c+2, __c+3, __c+4, __c+5, __c+6, __c+7,\n+       __c+8, __c+9, __c+10, __c+11, __c+12, __c+13, __c+14, __c+15});\n+#endif\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vextq_u16 (uint16x8_t __a, uint16x8_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint16x8_t)\n+      {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+      (uint16x8_t) {__c, __c+1, __c+2, __c+3, __c+4, __c+5, __c+6, __c+7});\n+#endif\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vextq_u32 (uint32x4_t __a, uint32x4_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a,\n+      (uint32x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {__c, __c+1, __c+2, __c+3});\n+#endif\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vextq_u64 (uint64x2_t __a, uint64x2_t __b, __const int __c)\n+{\n+  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a, (uint64x2_t) {2-__c, 3-__c});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {__c, __c+1});\n+#endif\n+}\n+\n /* vfma_lane  */\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))"}, {"sha": "05611f4cd61bc1f1e01766ced61abd5476623554", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ae0533da54015456538aa8ebeddf385230c36c98/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=ae0533da54015456538aa8ebeddf385230c36c98", "patch": "@@ -270,6 +270,7 @@\n     UNSPEC_UZP2\t\t; Used in vector permute patterns.\n     UNSPEC_TRN1\t\t; Used in vector permute patterns.\n     UNSPEC_TRN2\t\t; Used in vector permute patterns.\n+    UNSPEC_EXT\t\t; Used in aarch64-simd.md.\n     UNSPEC_AESE\t\t; Used in aarch64-simd.md.\n     UNSPEC_AESD         ; Used in aarch64-simd.md.\n     UNSPEC_AESMC        ; Used in aarch64-simd.md."}]}