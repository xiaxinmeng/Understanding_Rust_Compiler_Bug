{"sha": "7f679e470b6255bcf495b8200d66acfed1e975a3", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6N2Y2NzllNDcwYjYyNTViY2Y0OTViODIwMGQ2NmFjZmVkMWU5NzVhMw==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2017-12-20T12:54:10Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2017-12-20T12:54:10Z"}, "message": "poly_int: store_field & co\n\nThis patch makes store_field and related routines use poly_ints\nfor bit positions and sizes.  It keeps the existing choices\nbetween signed and unsigned types (there are a mixture of both).\n\n2017-12-20  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood  <david.sherwood@arm.com>\n\ngcc/\n\t* expr.c (store_constructor_field): Change bitsize from a\n\tunsigned HOST_WIDE_INT to a poly_uint64 and bitpos from a\n\tHOST_WIDE_INT to a poly_int64.\n\t(store_constructor): Change size from a HOST_WIDE_INT to\n\ta poly_int64.\n\t(store_field): Likewise bitsize and bitpos.\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r255880", "tree": {"sha": "bfbce249f1cad3e19c5517dfe7a57d0826459ae5", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/bfbce249f1cad3e19c5517dfe7a57d0826459ae5"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/7f679e470b6255bcf495b8200d66acfed1e975a3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7f679e470b6255bcf495b8200d66acfed1e975a3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7f679e470b6255bcf495b8200d66acfed1e975a3", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7f679e470b6255bcf495b8200d66acfed1e975a3/comments", "author": null, "committer": null, "parents": [{"sha": "8c59e5e735fcc19f170b4b7d72b55400da99e7b4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8c59e5e735fcc19f170b4b7d72b55400da99e7b4", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8c59e5e735fcc19f170b4b7d72b55400da99e7b4"}], "stats": {"total": 153, "additions": 90, "deletions": 63}, "files": [{"sha": "9f2d0657004a4fab2c3bb1ea13ef76ab55d9fc07", "filename": "gcc/ChangeLog", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7f679e470b6255bcf495b8200d66acfed1e975a3/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7f679e470b6255bcf495b8200d66acfed1e975a3/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=7f679e470b6255bcf495b8200d66acfed1e975a3", "patch": "@@ -1,3 +1,14 @@\n+2017-12-20  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* expr.c (store_constructor_field): Change bitsize from a\n+\tunsigned HOST_WIDE_INT to a poly_uint64 and bitpos from a\n+\tHOST_WIDE_INT to a poly_int64.\n+\t(store_constructor): Change size from a HOST_WIDE_INT to\n+\ta poly_int64.\n+\t(store_field): Likewise bitsize and bitpos.\n+\n 2017-12-20  Richard Sandiford  <richard.sandiford@linaro.org>\n \t    Alan Hayward  <alan.hayward@arm.com>\n \t    David Sherwood  <david.sherwood@arm.com>"}, {"sha": "14348e31fb252822c7304a9cf4caec0c1dd22838", "filename": "gcc/expr.c", "status": "modified", "additions": 79, "deletions": 63, "changes": 142, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7f679e470b6255bcf495b8200d66acfed1e975a3/gcc%2Fexpr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7f679e470b6255bcf495b8200d66acfed1e975a3/gcc%2Fexpr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.c?ref=7f679e470b6255bcf495b8200d66acfed1e975a3", "patch": "@@ -79,9 +79,8 @@ static void emit_block_move_via_loop (rtx, rtx, rtx, unsigned);\n static void clear_by_pieces (rtx, unsigned HOST_WIDE_INT, unsigned int);\n static rtx_insn *compress_float_constant (rtx, rtx);\n static rtx get_subtarget (rtx);\n-static void store_constructor (tree, rtx, int, HOST_WIDE_INT, bool);\n-static rtx store_field (rtx, HOST_WIDE_INT, HOST_WIDE_INT,\n-\t\t\tpoly_uint64, poly_uint64,\n+static void store_constructor (tree, rtx, int, poly_int64, bool);\n+static rtx store_field (rtx, poly_int64, poly_int64, poly_uint64, poly_uint64,\n \t\t\tmachine_mode, tree, alias_set_type, bool, bool);\n \n static unsigned HOST_WIDE_INT highest_pow2_factor_for_target (const_tree, const_tree);\n@@ -6103,31 +6102,34 @@ all_zeros_p (const_tree exp)\n    clear a substructure if the outer structure has already been cleared.  */\n \n static void\n-store_constructor_field (rtx target, unsigned HOST_WIDE_INT bitsize,\n-\t\t\t HOST_WIDE_INT bitpos,\n+store_constructor_field (rtx target, poly_uint64 bitsize, poly_int64 bitpos,\n \t\t\t poly_uint64 bitregion_start,\n \t\t\t poly_uint64 bitregion_end,\n \t\t\t machine_mode mode,\n \t\t\t tree exp, int cleared,\n \t\t\t alias_set_type alias_set, bool reverse)\n {\n+  poly_int64 bytepos;\n+  poly_uint64 bytesize;\n   if (TREE_CODE (exp) == CONSTRUCTOR\n       /* We can only call store_constructor recursively if the size and\n \t bit position are on a byte boundary.  */\n-      && bitpos % BITS_PER_UNIT == 0\n-      && (bitsize > 0 && bitsize % BITS_PER_UNIT == 0)\n+      && multiple_p (bitpos, BITS_PER_UNIT, &bytepos)\n+      && maybe_ne (bitsize, 0U)\n+      && multiple_p (bitsize, BITS_PER_UNIT, &bytesize)\n       /* If we have a nonzero bitpos for a register target, then we just\n \t let store_field do the bitfield handling.  This is unlikely to\n \t generate unnecessary clear instructions anyways.  */\n-      && (bitpos == 0 || MEM_P (target)))\n+      && (known_eq (bitpos, 0) || MEM_P (target)))\n     {\n       if (MEM_P (target))\n-\ttarget\n-\t  = adjust_address (target,\n-\t\t\t    GET_MODE (target) == BLKmode\n-\t\t\t    || (bitpos\n-\t\t\t\t% GET_MODE_ALIGNMENT (GET_MODE (target))) != 0\n-\t\t\t    ? BLKmode : VOIDmode, bitpos / BITS_PER_UNIT);\n+\t{\n+\t  machine_mode target_mode = GET_MODE (target);\n+\t  if (target_mode != BLKmode\n+\t      && !multiple_p (bitpos, GET_MODE_ALIGNMENT (target_mode)))\n+\t    target_mode = BLKmode;\n+\t  target = adjust_address (target, target_mode, bytepos);\n+\t}\n \n \n       /* Update the alias set, if required.  */\n@@ -6138,8 +6140,7 @@ store_constructor_field (rtx target, unsigned HOST_WIDE_INT bitsize,\n \t  set_mem_alias_set (target, alias_set);\n \t}\n \n-      store_constructor (exp, target, cleared, bitsize / BITS_PER_UNIT,\n-\t\t\t reverse);\n+      store_constructor (exp, target, cleared, bytesize, reverse);\n     }\n   else\n     store_field (target, bitsize, bitpos, bitregion_start, bitregion_end, mode,\n@@ -6173,12 +6174,12 @@ fields_length (const_tree type)\n    If REVERSE is true, the store is to be done in reverse order.  */\n \n static void\n-store_constructor (tree exp, rtx target, int cleared, HOST_WIDE_INT size,\n+store_constructor (tree exp, rtx target, int cleared, poly_int64 size,\n \t\t   bool reverse)\n {\n   tree type = TREE_TYPE (exp);\n   HOST_WIDE_INT exp_size = int_size_in_bytes (type);\n-  HOST_WIDE_INT bitregion_end = size > 0 ? size * BITS_PER_UNIT - 1 : 0;\n+  poly_int64 bitregion_end = known_gt (size, 0) ? size * BITS_PER_UNIT - 1 : 0;\n \n   switch (TREE_CODE (type))\n     {\n@@ -6193,7 +6194,7 @@ store_constructor (tree exp, rtx target, int cleared, HOST_WIDE_INT size,\n \treverse = TYPE_REVERSE_STORAGE_ORDER (type);\n \n \t/* If size is zero or the target is already cleared, do nothing.  */\n-\tif (size == 0 || cleared)\n+\tif (known_eq (size, 0) || cleared)\n \t  cleared = 1;\n \t/* We either clear the aggregate or indicate the value is dead.  */\n \telse if ((TREE_CODE (type) == UNION_TYPE\n@@ -6222,14 +6223,14 @@ store_constructor (tree exp, rtx target, int cleared, HOST_WIDE_INT size,\n \t   the whole structure first.  Don't do this if TARGET is a\n \t   register whose mode size isn't equal to SIZE since\n \t   clear_storage can't handle this case.  */\n-\telse if (size > 0\n+\telse if (known_size_p (size)\n \t\t && (((int) CONSTRUCTOR_NELTS (exp) != fields_length (type))\n \t\t     || mostly_zeros_p (exp))\n \t\t && (!REG_P (target)\n-\t\t     || ((HOST_WIDE_INT) GET_MODE_SIZE (GET_MODE (target))\n-\t\t\t == size)))\n+\t\t     || known_eq (GET_MODE_SIZE (GET_MODE (target)), size)))\n \t  {\n-\t    clear_storage (target, GEN_INT (size), BLOCK_OP_NORMAL);\n+\t    clear_storage (target, gen_int_mode (size, Pmode),\n+\t\t\t   BLOCK_OP_NORMAL);\n \t    cleared = 1;\n \t  }\n \n@@ -6410,12 +6411,13 @@ store_constructor (tree exp, rtx target, int cleared, HOST_WIDE_INT size,\n \t      need_to_clear = 1;\n \t  }\n \n-\tif (need_to_clear && size > 0)\n+\tif (need_to_clear && maybe_gt (size, 0))\n \t  {\n \t    if (REG_P (target))\n-\t      emit_move_insn (target,  CONST0_RTX (GET_MODE (target)));\n+\t      emit_move_insn (target, CONST0_RTX (GET_MODE (target)));\n \t    else\n-\t      clear_storage (target, GEN_INT (size), BLOCK_OP_NORMAL);\n+\t      clear_storage (target, gen_int_mode (size, Pmode),\n+\t\t\t     BLOCK_OP_NORMAL);\n \t    cleared = 1;\n \t  }\n \n@@ -6429,7 +6431,7 @@ store_constructor (tree exp, rtx target, int cleared, HOST_WIDE_INT size,\n \tFOR_EACH_CONSTRUCTOR_ELT (CONSTRUCTOR_ELTS (exp), i, index, value)\n \t  {\n \t    machine_mode mode;\n-\t    HOST_WIDE_INT bitsize;\n+\t    poly_int64 bitsize;\n \t    HOST_WIDE_INT bitpos;\n \t    rtx xtarget = target;\n \n@@ -6522,7 +6524,8 @@ store_constructor (tree exp, rtx target, int cleared, HOST_WIDE_INT size,\n \t\t    xtarget = adjust_address (xtarget, mode, 0);\n \t\t    if (TREE_CODE (value) == CONSTRUCTOR)\n \t\t      store_constructor (value, xtarget, cleared,\n-\t\t\t\t\t bitsize / BITS_PER_UNIT, reverse);\n+\t\t\t\t\t exact_div (bitsize, BITS_PER_UNIT),\n+\t\t\t\t\t reverse);\n \t\t    else\n \t\t      store_expr (value, xtarget, 0, false, reverse);\n \n@@ -6691,12 +6694,13 @@ store_constructor (tree exp, rtx target, int cleared, HOST_WIDE_INT size,\n \t    need_to_clear = (count < n_elts || 4 * zero_count >= 3 * count);\n \t  }\n \n-\tif (need_to_clear && size > 0 && !vector)\n+\tif (need_to_clear && maybe_gt (size, 0) && !vector)\n \t  {\n \t    if (REG_P (target))\n \t      emit_move_insn (target, CONST0_RTX (mode));\n \t    else\n-\t      clear_storage (target, GEN_INT (size), BLOCK_OP_NORMAL);\n+\t      clear_storage (target, gen_int_mode (size, Pmode),\n+\t\t\t     BLOCK_OP_NORMAL);\n \t    cleared = 1;\n \t  }\n \n@@ -6784,7 +6788,7 @@ store_constructor (tree exp, rtx target, int cleared, HOST_WIDE_INT size,\n    If REVERSE is true, the store is to be done in reverse order.  */\n \n static rtx\n-store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n+store_field (rtx target, poly_int64 bitsize, poly_int64 bitpos,\n \t     poly_uint64 bitregion_start, poly_uint64 bitregion_end,\n \t     machine_mode mode, tree exp,\n \t     alias_set_type alias_set, bool nontemporal,  bool reverse)\n@@ -6795,7 +6799,7 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n   /* If we have nothing to store, do nothing unless the expression has\n      side-effects.  Don't do that for zero sized addressable lhs of\n      calls.  */\n-  if (bitsize == 0\n+  if (known_eq (bitsize, 0)\n       && (!TREE_ADDRESSABLE (TREE_TYPE (exp))\n \t  || TREE_CODE (exp) != CALL_EXPR))\n     return expand_expr (exp, const0_rtx, VOIDmode, EXPAND_NORMAL);\n@@ -6804,14 +6808,15 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n     {\n       /* We're storing into a struct containing a single __complex.  */\n \n-      gcc_assert (!bitpos);\n+      gcc_assert (known_eq (bitpos, 0));\n       return store_expr (exp, target, 0, nontemporal, reverse);\n     }\n \n   /* If the structure is in a register or if the component\n      is a bit field, we cannot use addressing to access it.\n      Use bit-field techniques or SUBREG to store in it.  */\n \n+  poly_int64 decl_bitsize;\n   if (mode == VOIDmode\n       || (mode != BLKmode && ! direct_store[(int) mode]\n \t  && GET_MODE_CLASS (mode) != MODE_COMPLEX_INT\n@@ -6822,21 +6827,23 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n \t store it as a bit field.  */\n       || (mode != BLKmode\n \t  && ((((MEM_ALIGN (target) < GET_MODE_ALIGNMENT (mode))\n-\t\t|| bitpos % GET_MODE_ALIGNMENT (mode))\n+\t\t|| !multiple_p (bitpos, GET_MODE_ALIGNMENT (mode)))\n \t       && targetm.slow_unaligned_access (mode, MEM_ALIGN (target)))\n-\t      || (bitpos % BITS_PER_UNIT != 0)))\n-      || (bitsize >= 0 && mode != BLKmode\n-\t  && GET_MODE_BITSIZE (mode) > bitsize)\n+\t      || !multiple_p (bitpos, BITS_PER_UNIT)))\n+      || (known_size_p (bitsize)\n+\t  && mode != BLKmode\n+\t  && maybe_gt (GET_MODE_BITSIZE (mode), bitsize))\n       /* If the RHS and field are a constant size and the size of the\n \t RHS isn't the same size as the bitfield, we must use bitfield\n \t operations.  */\n-      || (bitsize >= 0\n-\t  && TREE_CODE (TYPE_SIZE (TREE_TYPE (exp))) == INTEGER_CST\n-\t  && compare_tree_int (TYPE_SIZE (TREE_TYPE (exp)), bitsize) != 0\n+      || (known_size_p (bitsize)\n+\t  && poly_int_tree_p (TYPE_SIZE (TREE_TYPE (exp)))\n+\t  && maybe_ne (wi::to_poly_offset (TYPE_SIZE (TREE_TYPE (exp))),\n+\t\t       bitsize)\n \t  /* Except for initialization of full bytes from a CONSTRUCTOR, which\n \t     we will handle specially below.  */\n \t  && !(TREE_CODE (exp) == CONSTRUCTOR\n-\t       && bitsize % BITS_PER_UNIT == 0)\n+\t       && multiple_p (bitsize, BITS_PER_UNIT))\n \t  /* And except for bitwise copying of TREE_ADDRESSABLE types,\n \t     where the FIELD_DECL has the right bitsize, but TREE_TYPE (exp)\n \t     includes some extra padding.  store_expr / expand_expr will in\n@@ -6847,14 +6854,14 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n \t     get_base_address needs to live in memory.  */\n \t  && (!TREE_ADDRESSABLE (TREE_TYPE (exp))\n \t      || TREE_CODE (exp) != COMPONENT_REF\n-\t      || TREE_CODE (DECL_SIZE (TREE_OPERAND (exp, 1))) != INTEGER_CST\n-\t      || (bitsize % BITS_PER_UNIT != 0)\n-\t      || (bitpos % BITS_PER_UNIT != 0)\n-\t      || (compare_tree_int (DECL_SIZE (TREE_OPERAND (exp, 1)), bitsize)\n-\t\t  != 0)))\n+\t      || !multiple_p (bitsize, BITS_PER_UNIT)\n+\t      || !multiple_p (bitpos, BITS_PER_UNIT)\n+\t      || !poly_int_tree_p (DECL_SIZE (TREE_OPERAND (exp, 1)),\n+\t\t\t\t   &decl_bitsize)\n+\t      || maybe_ne (decl_bitsize, bitsize)))\n       /* If we are expanding a MEM_REF of a non-BLKmode non-addressable\n          decl we must use bitfield operations.  */\n-      || (bitsize >= 0\n+      || (known_size_p (bitsize)\n \t  && TREE_CODE (exp) == MEM_REF\n \t  && TREE_CODE (TREE_OPERAND (exp, 0)) == ADDR_EXPR\n \t  && DECL_P (TREE_OPERAND (TREE_OPERAND (exp, 0), 0))\n@@ -6875,17 +6882,23 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n \t  tree type = TREE_TYPE (exp);\n \t  if (INTEGRAL_TYPE_P (type)\n \t      && TYPE_PRECISION (type) < GET_MODE_BITSIZE (TYPE_MODE (type))\n-\t      && bitsize == TYPE_PRECISION (type))\n+\t      && known_eq (bitsize, TYPE_PRECISION (type)))\n \t    {\n \t      tree op = gimple_assign_rhs1 (nop_def);\n \t      type = TREE_TYPE (op);\n-\t      if (INTEGRAL_TYPE_P (type) && TYPE_PRECISION (type) >= bitsize)\n+\t      if (INTEGRAL_TYPE_P (type)\n+\t\t  && known_ge (TYPE_PRECISION (type), bitsize))\n \t\texp = op;\n \t    }\n \t}\n \n       temp = expand_normal (exp);\n \n+      /* We don't support variable-sized BLKmode bitfields, since our\n+\t handling of BLKmode is bound up with the ability to break\n+\t things into words.  */\n+      gcc_assert (mode != BLKmode || bitsize.is_constant ());\n+\n       /* Handle calls that return values in multiple non-contiguous locations.\n \t The Irix 6 ABI has examples of this.  */\n       if (GET_CODE (temp) == PARALLEL)\n@@ -6926,9 +6939,11 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n \t  if (reverse)\n \t    temp = flip_storage_order (temp_mode, temp);\n \n-\t  if (bitsize < size\n+\t  gcc_checking_assert (known_le (bitsize, size));\n+\t  if (maybe_lt (bitsize, size)\n \t      && reverse ? !BYTES_BIG_ENDIAN : BYTES_BIG_ENDIAN\n-\t      && !(mode == BLKmode && bitsize > BITS_PER_WORD))\n+\t      /* Use of to_constant for BLKmode was checked above.  */\n+\t      && !(mode == BLKmode && bitsize.to_constant () > BITS_PER_WORD))\n \t    temp = expand_shift (RSHIFT_EXPR, temp_mode, temp,\n \t\t\t\t size - bitsize, NULL_RTX, 1);\n \t}\n@@ -6945,24 +6960,24 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n \t  && (GET_MODE (target) == BLKmode\n \t      || (MEM_P (target)\n \t\t  && GET_MODE_CLASS (GET_MODE (target)) == MODE_INT\n-\t\t  && (bitpos % BITS_PER_UNIT) == 0\n-\t\t  && (bitsize % BITS_PER_UNIT) == 0)))\n+\t\t  && multiple_p (bitpos, BITS_PER_UNIT)\n+\t\t  && multiple_p (bitsize, BITS_PER_UNIT))))\n \t{\n-\t  gcc_assert (MEM_P (target) && MEM_P (temp)\n-\t\t      && (bitpos % BITS_PER_UNIT) == 0);\n+\t  gcc_assert (MEM_P (target) && MEM_P (temp));\n+\t  poly_int64 bytepos = exact_div (bitpos, BITS_PER_UNIT);\n+\t  poly_int64 bytesize = bits_to_bytes_round_up (bitsize);\n \n-\t  target = adjust_address (target, VOIDmode, bitpos / BITS_PER_UNIT);\n+\t  target = adjust_address (target, VOIDmode, bytepos);\n \t  emit_block_move (target, temp,\n-\t\t\t   GEN_INT ((bitsize + BITS_PER_UNIT - 1)\n-\t\t\t\t    / BITS_PER_UNIT),\n+\t\t\t   gen_int_mode (bytesize, Pmode),\n \t\t\t   BLOCK_OP_NORMAL);\n \n \t  return const0_rtx;\n \t}\n \n       /* If the mode of TEMP is still BLKmode and BITSIZE not larger than the\n \t word size, we need to load the value (see again store_bit_field).  */\n-      if (GET_MODE (temp) == BLKmode && bitsize <= BITS_PER_WORD)\n+      if (GET_MODE (temp) == BLKmode && known_le (bitsize, BITS_PER_WORD))\n \t{\n \t  scalar_int_mode temp_mode = smallest_int_mode_for_size (bitsize);\n \t  temp = extract_bit_field (temp, bitsize, 0, 1, NULL_RTX, temp_mode,\n@@ -6979,7 +6994,8 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n   else\n     {\n       /* Now build a reference to just the desired component.  */\n-      rtx to_rtx = adjust_address (target, mode, bitpos / BITS_PER_UNIT);\n+      rtx to_rtx = adjust_address (target, mode,\n+\t\t\t\t   exact_div (bitpos, BITS_PER_UNIT));\n \n       if (to_rtx == target)\n \tto_rtx = copy_rtx (to_rtx);\n@@ -6989,10 +7005,10 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n \n       /* Above we avoided using bitfield operations for storing a CONSTRUCTOR\n \t into a target smaller than its type; handle that case now.  */\n-      if (TREE_CODE (exp) == CONSTRUCTOR && bitsize >= 0)\n+      if (TREE_CODE (exp) == CONSTRUCTOR && known_size_p (bitsize))\n \t{\n-\t  gcc_assert (bitsize % BITS_PER_UNIT == 0);\n-\t  store_constructor (exp, to_rtx, 0, bitsize / BITS_PER_UNIT, reverse);\n+\t  poly_int64 bytesize = exact_div (bitsize, BITS_PER_UNIT);\n+\t  store_constructor (exp, to_rtx, 0, bytesize, reverse);\n \t  return to_rtx;\n \t}\n "}]}