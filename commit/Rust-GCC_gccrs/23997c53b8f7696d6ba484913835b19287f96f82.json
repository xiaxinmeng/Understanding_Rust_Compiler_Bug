{"sha": "23997c53b8f7696d6ba484913835b19287f96f82", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjM5OTdjNTNiOGY3Njk2ZDZiYTQ4NDkxMzgzNWIxOTI4N2Y5NmY4Mg==", "commit": {"author": {"name": "Segher Boessenkool", "email": "segher@kernel.crashing.org", "date": "2015-09-15T00:38:21Z"}, "committer": {"name": "Segher Boessenkool", "email": "segher@gcc.gnu.org", "date": "2015-09-15T00:38:21Z"}, "message": "shrink-wrap: Rewrite\n\nThis patch rewrites the shrink-wrapping algorithm, allowing non-linear\npieces of CFG to be duplicated for use without prologue instead of just\nlinear pieces.\n\n\t* shrink-wrap.c (requires_stack_frame_p): Fix formatting.\n\t(dup_block_and_redirect): Delete function.\n\t(can_dup_for_shrink_wrapping): New function.\n\t(fix_fake_fallthrough_edge): New function.\n\t(try_shrink_wrapping): Rewrite function.\n\t(convert_to_simple_return): Call fix_fake_fallthrough_edge.\n\nFrom-SVN: r227775", "tree": {"sha": "d9a541717d4670d10789ddcd410d796e10b6f89c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d9a541717d4670d10789ddcd410d796e10b6f89c"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/23997c53b8f7696d6ba484913835b19287f96f82", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/23997c53b8f7696d6ba484913835b19287f96f82", "html_url": "https://github.com/Rust-GCC/gccrs/commit/23997c53b8f7696d6ba484913835b19287f96f82", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/23997c53b8f7696d6ba484913835b19287f96f82/comments", "author": {"login": "segher", "id": 417629, "node_id": "MDQ6VXNlcjQxNzYyOQ==", "avatar_url": "https://avatars.githubusercontent.com/u/417629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/segher", "html_url": "https://github.com/segher", "followers_url": "https://api.github.com/users/segher/followers", "following_url": "https://api.github.com/users/segher/following{/other_user}", "gists_url": "https://api.github.com/users/segher/gists{/gist_id}", "starred_url": "https://api.github.com/users/segher/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/segher/subscriptions", "organizations_url": "https://api.github.com/users/segher/orgs", "repos_url": "https://api.github.com/users/segher/repos", "events_url": "https://api.github.com/users/segher/events{/privacy}", "received_events_url": "https://api.github.com/users/segher/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "311adabec523e8ad99930cd2e24716098ed77fc7", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/311adabec523e8ad99930cd2e24716098ed77fc7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/311adabec523e8ad99930cd2e24716098ed77fc7"}], "stats": {"total": 797, "additions": 431, "deletions": 366}, "files": [{"sha": "328675a87de2414f9b9eb958cd798aaf42cb30c9", "filename": "gcc/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/23997c53b8f7696d6ba484913835b19287f96f82/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/23997c53b8f7696d6ba484913835b19287f96f82/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=23997c53b8f7696d6ba484913835b19287f96f82", "patch": "@@ -1,3 +1,12 @@\n+2015-09-14  Segher Boessenkool  <segher@kernel.crashing.org>\n+\n+\t* shrink-wrap.c (requires_stack_frame_p): Fix formatting.\n+\t(dup_block_and_redirect): Delete function.\n+\t(can_dup_for_shrink_wrapping): New function.\n+\t(fix_fake_fallthrough_edge): New function.\n+\t(try_shrink_wrapping): Rewrite function.\n+\t(convert_to_simple_return): Call fix_fake_fallthrough_edge.\n+\n 2015-09-14  Rich Felker  <dalias@libc.org>\n \n \t* configure.ac: Change target pattern for sh TLS support"}, {"sha": "138759432eef0c355147475f43ed2c135e16349e", "filename": "gcc/shrink-wrap.c", "status": "modified", "additions": 422, "deletions": 366, "changes": 788, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/23997c53b8f7696d6ba484913835b19287f96f82/gcc%2Fshrink-wrap.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/23997c53b8f7696d6ba484913835b19287f96f82/gcc%2Fshrink-wrap.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fshrink-wrap.c?ref=23997c53b8f7696d6ba484913835b19287f96f82", "patch": "@@ -91,8 +91,7 @@ requires_stack_frame_p (rtx_insn *insn, HARD_REG_SET prologue_used,\n       if (!REG_P (dreg))\n \tcontinue;\n \n-      add_to_hard_reg_set (&hardregs, GET_MODE (dreg),\n-\t\t\t   REGNO (dreg));\n+      add_to_hard_reg_set (&hardregs, GET_MODE (dreg), REGNO (dreg));\n     }\n   if (hard_reg_set_intersect_p (hardregs, prologue_used))\n     return true;\n@@ -463,414 +462,469 @@ prepare_shrink_wrap (basic_block entry_block)\n       }\n }\n \n-/* Create a copy of BB instructions and insert at BEFORE.  Redirect\n-   preds of BB to COPY_BB if they don't appear in NEED_PROLOGUE.  */\n-static void\n-dup_block_and_redirect (basic_block bb, basic_block copy_bb, rtx_insn *before,\n-\t\t\tbitmap_head *need_prologue)\n+/* Return whether we can duplicate basic block BB for shrink wrapping.  We\n+   cannot if the block cannot be duplicated at all, or if any of its incoming\n+   edges are complex and come from a block that does not require a prologue\n+   (we cannot redirect such edges), or if the block is too big to copy.\n+   PRO is the basic block before which we would put the prologue, MAX_SIZE is\n+   the maximum size block we allow to be copied.  */\n+\n+static bool\n+can_dup_for_shrink_wrapping (basic_block bb, basic_block pro, unsigned max_size)\n {\n-  edge_iterator ei;\n-  edge e;\n-  rtx_insn *insn = BB_END (bb);\n+  if (!can_duplicate_block_p (bb))\n+    return false;\n \n-  /* We know BB has a single successor, so there is no need to copy a\n-     simple jump at the end of BB.  */\n-  if (simplejump_p (insn))\n-    insn = PREV_INSN (insn);\n+  edge e;\n+  edge_iterator ei;\n+  FOR_EACH_EDGE (e, ei, bb->preds)\n+    if (e->flags & EDGE_COMPLEX\n+\t&& !dominated_by_p (CDI_DOMINATORS, e->src, pro))\n+      return false;\n \n-  start_sequence ();\n-  duplicate_insn_chain (BB_HEAD (bb), insn);\n-  if (dump_file)\n-    {\n-      unsigned count = 0;\n-      for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n-\tif (active_insn_p (insn))\n-\t  ++count;\n-      fprintf (dump_file, \"Duplicating bb %d to bb %d, %u active insns.\\n\",\n-\t       bb->index, copy_bb->index, count);\n-    }\n-  insn = get_insns ();\n-  end_sequence ();\n-  emit_insn_before (insn, before);\n+  unsigned size = 0;\n \n-  /* Redirect all the paths that need no prologue into copy_bb.  */\n-  for (ei = ei_start (bb->preds); (e = ei_safe_edge (ei));)\n-    if (!bitmap_bit_p (need_prologue, e->src->index))\n+  rtx_insn *insn;\n+  FOR_BB_INSNS (bb, insn)\n+    if (NONDEBUG_INSN_P (insn))\n       {\n-\tint freq = EDGE_FREQUENCY (e);\n-\tcopy_bb->count += e->count;\n-\tcopy_bb->frequency += EDGE_FREQUENCY (e);\n-\te->dest->count -= e->count;\n-\tif (e->dest->count < 0)\n-\t  e->dest->count = 0;\n-\te->dest->frequency -= freq;\n-\tif (e->dest->frequency < 0)\n-\t  e->dest->frequency = 0;\n-\tredirect_edge_and_branch_force (e, copy_bb);\n-\tcontinue;\n+\tsize += get_attr_min_length (insn);\n+\tif (size > max_size)\n+\t  return false;\n       }\n-    else\n-      ei_next (&ei);\n+\n+  return true;\n }\n \n+/* If the source of edge E has more than one successor, the verifier for\n+   branch probabilities gets confused by the fake edges we make where\n+   simple_return statements will be inserted later (because those are not\n+   marked as fallthrough edges).  Fix this by creating an extra block just\n+   for that fallthrough.  */\n+\n+static edge\n+fix_fake_fallthrough_edge (edge e)\n+{\n+  if (EDGE_COUNT (e->src->succs) <= 1)\n+    return e;\n+\n+  basic_block old_bb = e->src;\n+  rtx_insn *end = BB_END (old_bb);\n+  rtx_note *note = emit_note_after (NOTE_INSN_DELETED, end);\n+  basic_block new_bb = create_basic_block (note, note, old_bb);\n+  BB_COPY_PARTITION (new_bb, old_bb);\n+  BB_END (old_bb) = end;\n+\n+  redirect_edge_succ (e, new_bb);\n+  e->flags |= EDGE_FALLTHRU;\n+  e->flags &= ~EDGE_FAKE;\n+\n+  return make_edge (new_bb, EXIT_BLOCK_PTR_FOR_FN (cfun), EDGE_FAKE);\n+}\n \n /* Try to perform a kind of shrink-wrapping, making sure the\n    prologue/epilogue is emitted only around those parts of the\n-   function that require it.  */\n+   function that require it.\n+\n+   There will be exactly one prologue, and it will be executed either\n+   zero or one time, on any path.  Depending on where the prologue is\n+   placed, some of the basic blocks can be reached via both paths with\n+   and without a prologue.  Such blocks will be duplicated here, and the\n+   edges changed to match.\n+\n+   Paths that go to the exit without going through the prologue will use\n+   a simple_return instead of the epilogue.  We maximize the number of\n+   those, making sure to only duplicate blocks that can be duplicated.\n+   If the prologue can then still be placed in multiple locations, we\n+   place it as early as possible.\n+\n+   An example, where we duplicate blocks with control flow (legend:\n+   _B_egin, _R_eturn and _S_imple_return; edges without arrowhead should\n+   be taken to point down or to the right, to simplify the diagram; here,\n+   block 3 needs a prologue, the rest does not):\n+\n+\n+       B                 B\n+       |                 |\n+       2                 2\n+       |\\                |\\\n+       | 3    becomes    | 3\n+       |/                |  \\\n+       4                 7   4\n+       |\\                |\\  |\\\n+       | 5               | 8 | 5\n+       |/                |/  |/\n+       6                 9   6\n+       |                 |   |\n+       R                 S   R\n+\n+\n+   (bb 4 is duplicated to 7, and so on; the prologue is inserted on the\n+   edge 2->3).\n+\n+   Another example, where part of a loop is duplicated (again, bb 3 is\n+   the only block that needs a prologue):\n+\n+\n+       B   3<--              B       ->3<--\n+       |   |   |             |      |  |   |\n+       |   v   |   becomes   |      |  v   |\n+       2---4---              2---5--   4---\n+           |                     |     |\n+           R                     S     R\n+\n+\n+   (bb 4 is duplicated to 5; the prologue is inserted on the edge 5->3).\n+\n+   ENTRY_EDGE is the edge where the prologue will be placed, possibly\n+   changed by this function.  ORIG_ENTRY_EDGE is the edge where it\n+   would be placed without shrink-wrapping.  BB_WITH is a bitmap that,\n+   if we do shrink-wrap, will on return contain the interesting blocks\n+   that run with prologue.  PROLOGUE_SEQ is the prologue we will insert.  */\n \n void\n try_shrink_wrapping (edge *entry_edge, edge orig_entry_edge,\n-\t\t     bitmap_head *bb_flags, rtx_insn *prologue_seq)\n+\t\t     bitmap_head *bb_with, rtx_insn *prologue_seq)\n {\n-  edge e;\n-  edge_iterator ei;\n-  bool nonempty_prologue = false;\n-  unsigned max_grow_size;\n-  rtx_insn *seq;\n+  /* If we cannot shrink-wrap, are told not to shrink-wrap, or it makes\n+     no sense to shrink-wrap: then do not shrink-wrap!  */\n+\n+  if (!SHRINK_WRAPPING_ENABLED)\n+    return;\n+\n+  if (crtl->profile && !targetm.profile_before_prologue ())\n+    return;\n \n-  for (seq = prologue_seq; seq; seq = NEXT_INSN (seq))\n-    if (!NOTE_P (seq) || NOTE_KIND (seq) != NOTE_INSN_PROLOGUE_END)\n+  if (crtl->calls_eh_return)\n+    return;\n+\n+  bool empty_prologue = true;\n+  for (rtx_insn *insn = prologue_seq; insn; insn = NEXT_INSN (insn))\n+    if (!(NOTE_P (insn) && NOTE_KIND (insn) == NOTE_INSN_PROLOGUE_END))\n       {\n-\tnonempty_prologue = true;\n+\tempty_prologue = false;\n \tbreak;\n       }\n+  if (empty_prologue)\n+    return;\n+\n+  /* Move some code down to expose more shrink-wrapping opportunities.  */\n+\n+  basic_block entry = (*entry_edge)->dest;\n+  prepare_shrink_wrap (entry);\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Attempting shrink-wrapping optimization.\\n\");\n+\n+  /* Compute the registers set and used in the prologue.  */\n+\n+  HARD_REG_SET prologue_clobbered, prologue_used;\n+  CLEAR_HARD_REG_SET (prologue_clobbered);\n+  CLEAR_HARD_REG_SET (prologue_used);\n+  for (rtx_insn *insn = prologue_seq; insn; insn = NEXT_INSN (insn))\n+    if (NONDEBUG_INSN_P (insn))\n+      {\n+\tHARD_REG_SET this_used;\n+\tCLEAR_HARD_REG_SET (this_used);\n+\tnote_uses (&PATTERN (insn), record_hard_reg_uses, &this_used);\n+\tAND_COMPL_HARD_REG_SET (this_used, prologue_clobbered);\n+\tIOR_HARD_REG_SET (prologue_used, this_used);\n+\tnote_stores (PATTERN (insn), record_hard_reg_sets, &prologue_clobbered);\n+      }\n \n-  if (SHRINK_WRAPPING_ENABLED\n-      && (targetm.profile_before_prologue () || !crtl->profile)\n-      && nonempty_prologue && !crtl->calls_eh_return)\n+  /* Find out what registers are set up by the prologue; any use of these\n+     cannot happen before the prologue.  */\n+\n+  struct hard_reg_set_container set_up_by_prologue;\n+  CLEAR_HARD_REG_SET (set_up_by_prologue.set);\n+  add_to_hard_reg_set (&set_up_by_prologue.set, Pmode, STACK_POINTER_REGNUM);\n+  add_to_hard_reg_set (&set_up_by_prologue.set, Pmode, ARG_POINTER_REGNUM);\n+  if (frame_pointer_needed)\n+    add_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n+\t\t\t HARD_FRAME_POINTER_REGNUM);\n+  if (pic_offset_table_rtx \n+      && (unsigned) PIC_OFFSET_TABLE_REGNUM != INVALID_REGNUM)\n+    add_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n+\t\t\t PIC_OFFSET_TABLE_REGNUM);\n+  if (crtl->drap_reg)\n+    add_to_hard_reg_set (&set_up_by_prologue.set,\n+\t\t\t GET_MODE (crtl->drap_reg),\n+\t\t\t REGNO (crtl->drap_reg));\n+  if (targetm.set_up_by_prologue)\n+    targetm.set_up_by_prologue (&set_up_by_prologue);\n+\n+  /* We will insert the prologue before the basic block PRO.  PRO should\n+     dominate all basic blocks that need the prologue to be executed\n+     before them.  First, make PRO the \"tightest wrap\" possible.  */\n+\n+  calculate_dominance_info (CDI_DOMINATORS);\n+\n+  basic_block pro = 0;\n+\n+  basic_block bb;\n+  edge e;\n+  edge_iterator ei;\n+  FOR_EACH_BB_FN (bb, cfun)\n     {\n-      HARD_REG_SET prologue_clobbered, prologue_used, live_on_edge;\n-      struct hard_reg_set_container set_up_by_prologue;\n-      rtx_insn *p_insn;\n-      vec<basic_block> vec;\n-      basic_block bb;\n-      bitmap_head bb_antic_flags;\n-      bitmap_head bb_on_list;\n-      bitmap_head bb_tail;\n+      rtx_insn *insn;\n+      FOR_BB_INSNS (bb, insn)\n+\tif (NONDEBUG_INSN_P (insn)\n+\t    && requires_stack_frame_p (insn, prologue_used,\n+\t\t\t\t       set_up_by_prologue.set))\n+\t  {\n+\t    if (dump_file)\n+\t      fprintf (dump_file, \"Block %d needs the prologue.\\n\", bb->index);\n+\t    pro = nearest_common_dominator (CDI_DOMINATORS, pro, bb);\n+\t    break;\n+\t  }\n+    }\n \n+  /* If nothing needs a prologue, just put it at the start.  This really\n+     shouldn't happen, but we cannot fix it here.  */\n+\n+  if (pro == 0)\n+    {\n       if (dump_file)\n-\tfprintf (dump_file, \"Attempting shrink-wrapping optimization.\\n\");\n+\tfprintf(dump_file, \"Nothing needs a prologue, but it isn't empty; \"\n+\t\t\t   \"putting it at the start.\\n\");\n+      pro = entry;\n+    }\n \n-      /* Compute the registers set and used in the prologue.  */\n-      CLEAR_HARD_REG_SET (prologue_clobbered);\n-      CLEAR_HARD_REG_SET (prologue_used);\n-      for (p_insn = prologue_seq; p_insn; p_insn = NEXT_INSN (p_insn))\n-\t{\n-\t  HARD_REG_SET this_used;\n-\t  if (!NONDEBUG_INSN_P (p_insn))\n-\t    continue;\n-\n-\t  CLEAR_HARD_REG_SET (this_used);\n-\t  note_uses (&PATTERN (p_insn), record_hard_reg_uses,\n-\t\t     &this_used);\n-\t  AND_COMPL_HARD_REG_SET (this_used, prologue_clobbered);\n-\t  IOR_HARD_REG_SET (prologue_used, this_used);\n-\t  note_stores (PATTERN (p_insn), record_hard_reg_sets,\n-\t\t       &prologue_clobbered);\n-\t}\n+  if (dump_file)\n+    fprintf (dump_file, \"After wrapping required blocks, PRO is now %d\\n\",\n+\t     pro->index);\n \n-      prepare_shrink_wrap ((*entry_edge)->dest);\n-\n-      bitmap_initialize (&bb_antic_flags, &bitmap_default_obstack);\n-      bitmap_initialize (&bb_on_list, &bitmap_default_obstack);\n-      bitmap_initialize (&bb_tail, &bitmap_default_obstack);\n-\n-      /* Find the set of basic blocks that require a stack frame,\n-\t and blocks that are too big to be duplicated.  */\n-\n-      vec.create (n_basic_blocks_for_fn (cfun));\n-\n-      CLEAR_HARD_REG_SET (set_up_by_prologue.set);\n-      add_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n-\t\t\t   STACK_POINTER_REGNUM);\n-      add_to_hard_reg_set (&set_up_by_prologue.set, Pmode, ARG_POINTER_REGNUM);\n-      if (frame_pointer_needed)\n-\tadd_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n-\t\t\t     HARD_FRAME_POINTER_REGNUM);\n-      if (pic_offset_table_rtx \n-\t  && (unsigned) PIC_OFFSET_TABLE_REGNUM != INVALID_REGNUM)\n-\tadd_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n-\t\t\t     PIC_OFFSET_TABLE_REGNUM);\n-      if (crtl->drap_reg)\n-\tadd_to_hard_reg_set (&set_up_by_prologue.set,\n-\t\t\t     GET_MODE (crtl->drap_reg),\n-\t\t\t     REGNO (crtl->drap_reg));\n-      if (targetm.set_up_by_prologue)\n-\ttargetm.set_up_by_prologue (&set_up_by_prologue);\n-\n-      /* We don't use a different max size depending on\n-\t optimize_bb_for_speed_p because increasing shrink-wrapping\n-\t opportunities by duplicating tail blocks can actually result\n-\t in an overall decrease in code size.  */\n-      max_grow_size = get_uncond_jump_length ();\n-      max_grow_size *= PARAM_VALUE (PARAM_MAX_GROW_COPY_BB_INSNS);\n-\n-      FOR_EACH_BB_FN (bb, cfun)\n-\t{\n-\t  rtx_insn *insn;\n-\t  unsigned size = 0;\n+  /* Now see if we can put the prologue at the start of PRO.  Putting it\n+     there might require duplicating a block that cannot be duplicated;\n+     if so, try again with the immediate dominator of PRO, and so on.\n \n-\t  FOR_BB_INSNS (bb, insn)\n-\t    if (NONDEBUG_INSN_P (insn))\n-\t      {\n-\t\tif (requires_stack_frame_p (insn, prologue_used,\n-\t\t\t\t\t    set_up_by_prologue.set))\n-\t\t  {\n-\t\t    if (bb == (*entry_edge)->dest)\n-\t\t      goto fail_shrinkwrap;\n-\t\t    bitmap_set_bit (bb_flags, bb->index);\n-\t\t    vec.quick_push (bb);\n-\t\t    break;\n-\t\t  }\n-\t\telse if (size <= max_grow_size)\n-\t\t  {\n-\t\t    size += get_attr_min_length (insn);\n-\t\t    if (size > max_grow_size)\n-\t\t      bitmap_set_bit (&bb_on_list, bb->index);\n-\t\t  }\n-\t      }\n-\t}\n+     The blocks that need duplicating are those reachable from PRO but\n+     not dominated by it.  We keep in BB_WITH a bitmap of the blocks\n+     reachable from PRO that we already found, and in VEC a stack of\n+     those we still need to consider (to find successors).  */\n \n-      /* Blocks that really need a prologue, or are too big for tails.  */\n-      bitmap_ior_into (&bb_on_list, bb_flags);\n+  bitmap_set_bit (bb_with, pro->index);\n \n-      /* For every basic block that needs a prologue, mark all blocks\n-\t reachable from it, so as to ensure they are also seen as\n-\t requiring a prologue.  */\n-      while (!vec.is_empty ())\n-\t{\n-\t  basic_block tmp_bb = vec.pop ();\n+  vec<basic_block> vec;\n+  vec.create (n_basic_blocks_for_fn (cfun));\n+  vec.quick_push (pro);\n \n-\t  FOR_EACH_EDGE (e, ei, tmp_bb->succs)\n-\t    if (e->dest != EXIT_BLOCK_PTR_FOR_FN (cfun)\n-\t\t&& bitmap_set_bit (bb_flags, e->dest->index))\n-\t      vec.quick_push (e->dest);\n-\t}\n+  unsigned max_grow_size = get_uncond_jump_length ();\n+  max_grow_size *= PARAM_VALUE (PARAM_MAX_GROW_COPY_BB_INSNS);\n \n-      /* Find the set of basic blocks that need no prologue, have a\n-\t single successor, can be duplicated, meet a max size\n-\t requirement, and go to the exit via like blocks.  */\n-      vec.quick_push (EXIT_BLOCK_PTR_FOR_FN (cfun));\n-      while (!vec.is_empty ())\n-\t{\n-\t  basic_block tmp_bb = vec.pop ();\n+  while (!vec.is_empty () && pro != entry)\n+    {\n+      basic_block bb = vec.pop ();\n+      if (!can_dup_for_shrink_wrapping (bb, pro, max_grow_size))\n+\twhile (!dominated_by_p (CDI_DOMINATORS, bb, pro))\n+\t  {\n+\t    gcc_assert (pro != entry);\n \n-\t  FOR_EACH_EDGE (e, ei, tmp_bb->preds)\n-\t    if (single_succ_p (e->src)\n-\t\t&& !bitmap_bit_p (&bb_on_list, e->src->index)\n-\t\t&& can_duplicate_block_p (e->src))\n-\t      {\n-\t\tedge pe;\n-\t\tedge_iterator pei;\n-\n-\t\t/* If there is predecessor of e->src which doesn't\n-\t\t   need prologue and the edge is complex,\n-\t\t   we might not be able to redirect the branch\n-\t\t   to a copy of e->src.  */\n-\t\tFOR_EACH_EDGE (pe, pei, e->src->preds)\n-\t\t  if ((pe->flags & EDGE_COMPLEX) != 0\n-\t\t      && !bitmap_bit_p (bb_flags, pe->src->index))\n-\t\t    break;\n-\t\tif (pe == NULL && bitmap_set_bit (&bb_tail, e->src->index))\n-\t\t  vec.quick_push (e->src);\n-\t      }\n-\t}\n+\t    pro = get_immediate_dominator (CDI_DOMINATORS, pro);\n+\n+\t    bitmap_set_bit (bb_with, pro->index);\n+\t    vec.quick_push (pro);\n+\t  }\n+\n+      FOR_EACH_EDGE (e, ei, bb->succs)\n+\tif (e->dest != EXIT_BLOCK_PTR_FOR_FN (cfun)\n+\t    && bitmap_set_bit (bb_with, e->dest->index))\n+\t  vec.quick_push (e->dest);\n+    }\n+\n+  vec.release ();\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Avoiding non-duplicatable blocks, PRO is now %d\\n\",\n+\t     pro->index);\n+\n+  /* If we can move PRO back without having to duplicate more blocks, do so.\n+     We can move back to a block PRE if every path from PRE will eventually\n+     need a prologue, that is, PRO is a post-dominator of PRE.  */\n+\n+  if (pro != entry)\n+    {\n+      calculate_dominance_info (CDI_POST_DOMINATORS);\n \n-      /* Now walk backwards from every block that is marked as needing\n-\t a prologue to compute the bb_antic_flags bitmap.  Exclude\n-\t tail blocks; They can be duplicated to be used on paths not\n-\t needing a prologue.  */\n-      bitmap_clear (&bb_on_list);\n-      bitmap_and_compl (&bb_antic_flags, bb_flags, &bb_tail);\n-      FOR_EACH_BB_FN (bb, cfun)\n+      while (pro != entry)\n \t{\n-\t  if (!bitmap_bit_p (&bb_antic_flags, bb->index))\n-\t    continue;\n-\t  FOR_EACH_EDGE (e, ei, bb->preds)\n-\t    if (!bitmap_bit_p (&bb_antic_flags, e->src->index)\n-\t\t&& bitmap_set_bit (&bb_on_list, e->src->index))\n-\t      vec.quick_push (e->src);\n+\t  basic_block pre = get_immediate_dominator (CDI_DOMINATORS, pro);\n+\t  if (dominated_by_p (CDI_POST_DOMINATORS, pre, pro))\n+\t    pro = pre;\n+\t  else\n+\t    break;\n \t}\n-      while (!vec.is_empty ())\n-\t{\n-\t  basic_block tmp_bb = vec.pop ();\n-\t  bool all_set = true;\n \n-\t  bitmap_clear_bit (&bb_on_list, tmp_bb->index);\n-\t  FOR_EACH_EDGE (e, ei, tmp_bb->succs)\n-\t    if (!bitmap_bit_p (&bb_antic_flags, e->dest->index))\n-\t      {\n-\t\tall_set = false;\n-\t\tbreak;\n-\t      }\n+      free_dominance_info (CDI_POST_DOMINATORS);\n+    }\n \n-\t  if (all_set)\n-\t    {\n-\t      bitmap_set_bit (&bb_antic_flags, tmp_bb->index);\n-\t      FOR_EACH_EDGE (e, ei, tmp_bb->preds)\n-\t\tif (!bitmap_bit_p (&bb_antic_flags, e->src->index)\n-\t\t    && bitmap_set_bit (&bb_on_list, e->src->index))\n-\t\t  vec.quick_push (e->src);\n-\t    }\n-\t}\n-      /* Find exactly one edge that leads to a block in ANTIC from\n-\t a block that isn't.  */\n-      if (!bitmap_bit_p (&bb_antic_flags, (*entry_edge)->dest->index))\n-\tFOR_EACH_BB_FN (bb, cfun)\n+  if (dump_file)\n+    fprintf (dump_file, \"Bumping back to anticipatable blocks, PRO is now %d\\n\",\n+\t     pro->index);\n+\n+  /* If there is more than one predecessor of PRO not dominated by PRO, fail.\n+     Also find that single edge that leads to PRO.  */\n+\n+  bool multi = false;\n+  edge the_edge = 0;\n+  FOR_EACH_EDGE (e, ei, pro->preds)\n+    if (!dominated_by_p (CDI_DOMINATORS, e->src, pro))\n+      {\n+\tif (the_edge)\n+\t  multi = true;\n+\telse\n+\t  the_edge = e;\n+      }\n+\n+  if (multi)\n+    {\n+      the_edge = orig_entry_edge;\n+\n+      if (dump_file)\n+\tfprintf (dump_file, \"More than one candidate edge.\\n\");\n+    }\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Found candidate edge for shrink-wrapping, %d->%d.\\n\",\n+\t     the_edge->src->index, the_edge->dest->index);\n+\n+  *entry_edge = the_edge;\n+\n+  /* Compute what fraction of the frequency and count of the blocks that run\n+     both with and without prologue are for running with prologue.  This gives\n+     the correct answer for reducible flow graphs; for irreducible flow graphs\n+     our profile is messed up beyond repair anyway.  */\n+\n+  int num = (*entry_edge)->probability;\n+  int den = REG_BR_PROB_BASE;\n+\n+  if (*entry_edge == orig_entry_edge)\n+    goto out;\n+\n+  /* Test whether the prologue is known to clobber any register\n+     (other than FP or SP) which are live on the edge.  */\n+\n+  HARD_REG_SET live_on_edge;\n+  CLEAR_HARD_REG_BIT (prologue_clobbered, STACK_POINTER_REGNUM);\n+  if (frame_pointer_needed)\n+    CLEAR_HARD_REG_BIT (prologue_clobbered, HARD_FRAME_POINTER_REGNUM);\n+  REG_SET_TO_HARD_REG_SET (live_on_edge,\n+\t\t\t   df_get_live_in ((*entry_edge)->dest));\n+  if (hard_reg_set_intersect_p (live_on_edge, prologue_clobbered))\n+    {\n+      *entry_edge = orig_entry_edge;\n+      if (dump_file)\n+\tfprintf (dump_file,\n+\t\t \"Shrink-wrapping aborted due to clobber.\\n\");\n+      goto out;\n+    }\n+\n+  /* All is okay, so do it.  */\n+\n+  crtl->shrink_wrapped = true;\n+  if (dump_file)\n+    fprintf (dump_file, \"Performing shrink-wrapping.\\n\");\n+\n+  /* Copy the blocks that can run both with and without prologue.  The\n+     originals run with prologue, the copies without.  Store a pointer to\n+     the copy in the ->aux field of the original.  */\n+\n+  FOR_EACH_BB_FN (bb, cfun)\n+    if (bitmap_bit_p (bb_with, bb->index)\n+\t&& !dominated_by_p (CDI_DOMINATORS, bb, pro))\n+      {\n+\tbasic_block dup = duplicate_block (bb, 0, 0);\n+\n+\tbb->aux = dup;\n+\n+\tif (JUMP_P (BB_END (dup)) && !any_condjump_p (BB_END (dup)))\n+\t  emit_barrier_after_bb (dup);\n+\n+\tif (EDGE_COUNT (dup->succs) == 0)\n+\t  emit_barrier_after_bb (dup);\n+\n+\tif (dump_file)\n+\t  fprintf (dump_file, \"Duplicated %d to %d\\n\", bb->index, dup->index);\n+\n+\tbb->frequency = RDIV (num * bb->frequency, den);\n+\tdup->frequency -= bb->frequency;\n+\tbb->count = RDIV (num * bb->count, den);\n+\tdup->count -= bb->count;\n+      }\n+\n+  /* Change ENTRY_EDGE, if its src is duplicated.  Do this first, before\n+     the redirects have had a chance to create new blocks on the edge we\n+     want to use for the prologue, which makes us not find it.  */\n+\n+  gcc_assert (!dominated_by_p (CDI_DOMINATORS, (*entry_edge)->src, pro));\n+\n+  if (bitmap_bit_p (bb_with, (*entry_edge)->src->index))\n+    {\n+      basic_block src = (basic_block) (*entry_edge)->src->aux;\n+      FOR_EACH_EDGE (e, ei, src->succs)\n+\tif (e->dest == pro)\n+\t  *entry_edge = e;\n+    }\n+\n+  /* Now change the edges to point to the copies, where appropriate.  */\n+\n+  FOR_EACH_BB_FN (bb, cfun)\n+    if (!dominated_by_p (CDI_DOMINATORS, bb, pro))\n+      {\n+\tbasic_block src = bb;\n+\tif (bitmap_bit_p (bb_with, bb->index))\n+\t  src = (basic_block) bb->aux;\n+\n+\tFOR_EACH_EDGE (e, ei, src->succs)\n \t  {\n-\t    if (!bitmap_bit_p (&bb_antic_flags, bb->index))\n+\t    if (e->dest == EXIT_BLOCK_PTR_FOR_FN (cfun))\n \t      continue;\n-\t    FOR_EACH_EDGE (e, ei, bb->preds)\n-\t      if (!bitmap_bit_p (&bb_antic_flags, e->src->index))\n-\t\t{\n-\t\t  if (*entry_edge != orig_entry_edge)\n-\t\t    {\n-\t\t      *entry_edge = orig_entry_edge;\n-\t\t      if (dump_file)\n-\t\t\tfprintf (dump_file, \"More than one candidate edge.\\n\");\n-\t\t      goto fail_shrinkwrap;\n-\t\t    }\n-\t\t  if (dump_file)\n-\t\t    fprintf (dump_file, \"Found candidate edge for \"\n-\t\t\t     \"shrink-wrapping, %d->%d.\\n\", e->src->index,\n-\t\t\t     e->dest->index);\n-\t\t  *entry_edge = e;\n-\t\t}\n-\t  }\n \n-      if (*entry_edge != orig_entry_edge)\n-\t{\n-\t  /* Test whether the prologue is known to clobber any register\n-\t     (other than FP or SP) which are live on the edge.  */\n-\t  CLEAR_HARD_REG_BIT (prologue_clobbered, STACK_POINTER_REGNUM);\n-\t  if (frame_pointer_needed)\n-\t    CLEAR_HARD_REG_BIT (prologue_clobbered, HARD_FRAME_POINTER_REGNUM);\n-\t  REG_SET_TO_HARD_REG_SET (live_on_edge,\n-\t\t\t\t   df_get_live_in ((*entry_edge)->dest));\n-\t  if (hard_reg_set_intersect_p (live_on_edge, prologue_clobbered))\n-\t    {\n-\t      *entry_edge = orig_entry_edge;\n-\t      if (dump_file)\n-\t\tfprintf (dump_file,\n-\t\t\t \"Shrink-wrapping aborted due to clobber.\\n\");\n-\t    }\n-\t}\n-      if (*entry_edge != orig_entry_edge)\n-\t{\n-\t  crtl->shrink_wrapped = true;\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \"Performing shrink-wrapping.\\n\");\n-\n-\t  /* Find tail blocks reachable from both blocks needing a\n-\t     prologue and blocks not needing a prologue.  */\n-\t  if (!bitmap_empty_p (&bb_tail))\n-\t    FOR_EACH_BB_FN (bb, cfun)\n+\t    if (bitmap_bit_p (bb_with, e->dest->index)\n+\t\t&& !dominated_by_p (CDI_DOMINATORS, e->dest, pro))\n \t      {\n-\t\tbool some_pro, some_no_pro;\n-\t\tif (!bitmap_bit_p (&bb_tail, bb->index))\n-\t\t  continue;\n-\t\tsome_pro = some_no_pro = false;\n-\t\tFOR_EACH_EDGE (e, ei, bb->preds)\n-\t\t  {\n-\t\t    if (bitmap_bit_p (bb_flags, e->src->index))\n-\t\t      some_pro = true;\n-\t\t    else\n-\t\t      some_no_pro = true;\n-\t\t  }\n-\t\tif (some_pro && some_no_pro)\n-\t\t  vec.quick_push (bb);\n-\t\telse\n-\t\t  bitmap_clear_bit (&bb_tail, bb->index);\n+\t\tif (dump_file)\n+\t\t  fprintf (dump_file, \"Redirecting edge %d->%d to %d\\n\",\n+\t\t\t   e->src->index, e->dest->index,\n+\t\t\t   ((basic_block) e->dest->aux)->index);\n+\t\tredirect_edge_and_branch_force (e, (basic_block) e->dest->aux);\n \t      }\n-\t  /* Find the head of each tail.  */\n-\t  while (!vec.is_empty ())\n-\t    {\n-\t      basic_block tbb = vec.pop ();\n+\t    else if (e->flags & EDGE_FALLTHRU\n+\t\t     && bitmap_bit_p (bb_with, bb->index))\n+\t      force_nonfallthru (e);\n+\t  }\n+      }\n \n-\t      if (!bitmap_bit_p (&bb_tail, tbb->index))\n-\t\tcontinue;\n+  /* Also redirect the function entry edge if necessary.  */\n \n-\t      while (single_succ_p (tbb))\n-\t\t{\n-\t\t  tbb = single_succ (tbb);\n-\t\t  bitmap_clear_bit (&bb_tail, tbb->index);\n-\t\t}\n-\t    }\n-\t  /* Now duplicate the tails.  */\n-\t  if (!bitmap_empty_p (&bb_tail))\n-\t    FOR_EACH_BB_REVERSE_FN (bb, cfun)\n-\t      {\n-\t\tbasic_block copy_bb, tbb;\n-\t\tint eflags;\n-\n-\t\tif (!bitmap_clear_bit (&bb_tail, bb->index))\n-\t\t  continue;\n-\n-\t\t/* Create a copy of BB, instructions and all, for\n-\t\t   use on paths that don't need a prologue.\n-\t\t   Ideal placement of the copy is on a fall-thru edge\n-\t\t   or after a block that would jump to the copy.  */\n-\t\tFOR_EACH_EDGE (e, ei, bb->preds)\n-\t\t  if (!bitmap_bit_p (bb_flags, e->src->index)\n-\t\t      && single_succ_p (e->src))\n-\t\t    break;\n-\t\tif (e)\n-\t\t  {\n-                    /* Make sure we insert after any barriers.  */\n-                    rtx_insn *end = get_last_bb_insn (e->src);\n-                    copy_bb = create_basic_block (NEXT_INSN (end),\n-                                                  NULL_RTX, e->src);\n-\t\t    BB_COPY_PARTITION (copy_bb, e->src);\n-\t\t  }\n-\t\telse\n-\t\t  {\n-\t\t    /* Otherwise put the copy at the end of the function.  */\n-\t\t    copy_bb = create_basic_block (NULL_RTX, NULL_RTX,\n-\t\t\t\t\t\t  EXIT_BLOCK_PTR_FOR_FN (cfun)->prev_bb);\n-\t\t    BB_COPY_PARTITION (copy_bb, bb);\n-\t\t  }\n-\n-\t\trtx_note *insert_point = emit_note_after (NOTE_INSN_DELETED,\n-\t\t\t\t\t\t\t  BB_END (copy_bb));\n-\t\temit_barrier_after (BB_END (copy_bb));\n-\n-\t\ttbb = bb;\n-\t\twhile (1)\n-\t\t  {\n-\t\t    dup_block_and_redirect (tbb, copy_bb, insert_point,\n-\t\t\t\t\t    bb_flags);\n-\t\t    tbb = single_succ (tbb);\n-\t\t    if (tbb == EXIT_BLOCK_PTR_FOR_FN (cfun))\n-\t\t      break;\n-\t\t    e = split_block (copy_bb, PREV_INSN (insert_point));\n-\t\t    copy_bb = e->dest;\n-\t\t  }\n-\n-\t\t/* Quiet verify_flow_info by (ab)using EDGE_FAKE.\n-\t\t   We have yet to add a simple_return to the tails,\n-\t\t   as we'd like to first convert_jumps_to_returns in\n-\t\t   case the block is no longer used after that.  */\n-\t\teflags = EDGE_FAKE;\n-\t\tif (CALL_P (PREV_INSN (insert_point))\n-\t\t    && SIBLING_CALL_P (PREV_INSN (insert_point)))\n-\t\t  eflags = EDGE_SIBCALL | EDGE_ABNORMAL;\n-\t\tmake_single_succ_edge (copy_bb, EXIT_BLOCK_PTR_FOR_FN (cfun),\n-\t\t\t\t       eflags);\n-\n-\t\t/* verify_flow_info doesn't like a note after a\n-\t\t   sibling call.  */\n-\t\tdelete_insn (insert_point);\n-\t\tif (bitmap_empty_p (&bb_tail))\n-\t\t  break;\n-\t      }\n-\t}\n+  FOR_EACH_EDGE (e, ei, ENTRY_BLOCK_PTR_FOR_FN (cfun)->succs)\n+    if (bitmap_bit_p (bb_with, e->dest->index)\n+\t&& !dominated_by_p (CDI_DOMINATORS, e->dest, pro))\n+      {\n+\tbasic_block split_bb = split_edge (e);\n+\te = single_succ_edge (split_bb);\n+\tredirect_edge_and_branch_force (e, (basic_block) e->dest->aux);\n+      }\n \n-    fail_shrinkwrap:\n-      bitmap_clear (&bb_tail);\n-      bitmap_clear (&bb_antic_flags);\n-      bitmap_clear (&bb_on_list);\n-      vec.release ();\n-    }\n+  /* Change all the exits that should get a simple_return to FAKE.\n+     They will be converted later.  */\n+\n+  FOR_EACH_BB_FN (bb, cfun)\n+    if (!bitmap_bit_p (bb_with, bb->index))\n+      FOR_EACH_EDGE (e, ei, bb->succs)\n+\tif (e->dest == EXIT_BLOCK_PTR_FOR_FN (cfun))\n+\t  {\n+\t    e = fix_fake_fallthrough_edge (e);\n+\n+\t    e->flags &= ~EDGE_FALLTHRU;\n+\t    if (!(e->flags & EDGE_SIBCALL))\n+\t      e->flags |= EDGE_FAKE;\n+\n+\t    emit_barrier_after_bb (e->src);\n+\t  }\n+\n+out:\n+  free_dominance_info (CDI_DOMINATORS);\n }\n \n /* If we're allowed to generate a simple return instruction, then by\n@@ -1018,6 +1072,8 @@ convert_to_simple_return (edge entry_edge, edge orig_entry_edge,\n \t    && (e->flags & EDGE_FAKE) != 0\n \t    && !bitmap_bit_p (&bb_flags, e->src->index))\n \t  {\n+\t    e = fix_fake_fallthrough_edge (e);\n+\n \t    emit_return_into_block (true, e->src);\n \t    e->flags &= ~(EDGE_FALLTHRU | EDGE_FAKE);\n \t  }"}]}