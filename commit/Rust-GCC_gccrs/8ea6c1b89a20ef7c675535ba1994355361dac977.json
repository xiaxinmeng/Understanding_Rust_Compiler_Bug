{"sha": "8ea6c1b89a20ef7c675535ba1994355361dac977", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OGVhNmMxYjg5YTIwZWY3YzY3NTUzNWJhMTk5NDM1NTM2MWRhYzk3Nw==", "commit": {"author": {"name": "Mihail Ionescu", "email": "mihail.ionescu@arm.com", "date": "2020-02-18T14:23:09Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2020-02-25T18:36:52Z"}, "message": "aarch64: Add bfloat16 vdup and vreinterpret ACLE intrinsics\n\nThis patch adds support for the bf16 duplicate and reinterpret intrinsics.\nACLE documents are at https://developer.arm.com/docs/101028/latest\nISA documents are at https://developer.arm.com/docs/ddi0596/latest\n\n2020-02-25  Mihail Ionescu  <mihail.ionescu@arm.com>\n\ngcc/\n\t* config/aarch64/iterators.md (VDQF_F16) Add V4BF and V8BF.\n\t(VALL_F16): Likewise.\n\t(VALLDI_F16): Likewise.\n\t(Vtype): Likewise.\n\t(Vetype): Likewise.\n\t(vswap_width_name): Likewise.\n\t(VSWAP_WIDTH): Likewise.\n\t(Vel): Likewise.\n\t(VEL): Likewise.\n\t(q): Likewise.\n\t* config/aarch64/arm_neon.h (vset_lane_bf16, vsetq_lane_bf16): New.\n\t(vget_lane_bf16, vgetq_lane_bf16): New.\n\t(vcreate_bf16): New.\n\t(vdup_n_bf16, vdupq_n_bf16): New.\n\t(vdup_lane_bf16, vdup_laneq_bf16): New.\n\t(vdupq_lane_bf16, vdupq_laneq_bf16): New.\n\t(vduph_lane_bf16, vduph_laneq_bf16): New.\n\t(vreinterpret_bf16_u8, vreinterpretq_bf16_u8): New.\n\t(vreinterpret_bf16_u16, vreinterpretq_bf16_u16): New.\n\t(vreinterpret_bf16_u32, vreinterpretq_bf16_u32): New.\n\t(vreinterpret_bf16_u64, vreinterpretq_bf16_u64): New.\n\t(vreinterpret_bf16_s8, vreinterpretq_bf16_s8): New.\n\t(vreinterpret_bf16_s16, vreinterpretq_bf16_s16): New.\n\t(vreinterpret_bf16_s32, vreinterpretq_bf16_s32): New.\n\t(vreinterpret_bf16_s64, vreinterpretq_bf16_s64): New.\n\t(vreinterpret_bf16_p8, vreinterpretq_bf16_p8): New.\n\t(vreinterpret_bf16_p16, vreinterpretq_bf16_p16): New.\n\t(vreinterpret_bf16_p64, vreinterpretq_bf16_p64): New\n\t(vreinterpret_bf16_f16, vreinterpretq_bf16_f16): New\n\t(vreinterpret_bf16_f32, vreinterpretq_bf16_f32): New.\n\t(vreinterpret_bf16_f64, vreinterpretq_bf16_f64): New.\n\t(vreinterpretq_bf16_p128): New.\n\t(vreinterpret_s8_bf16, vreinterpretq_s8_bf16): New.\n\t(vreinterpret_s16_bf16, vreinterpretq_s16_bf16): New.\n\t(vreinterpret_s32_bf16, vreinterpretq_s32_bf16): New.\n\t(vreinterpret_s64_bf16, vreinterpretq_s64_bf16): New.\n\t(vreinterpret_u8_bf16, vreinterpretq_u8_bf16): New.\n\t(vreinterpret_u16_bf16, vreinterpretq_u16_bf16): New.\n\t(vreinterpret_u32_bf16, vreinterpretq_u32_bf16): New.\n\t(vreinterpret_u64_bf16, vreinterpretq_u64_bf16): New.\n\t(vreinterpret_p8_bf16, vreinterpretq_p8_bf16): New.\n\t(vreinterpret_p16_bf16, vreinterpretq_p16_bf16): New.\n\t(vreinterpret_p64_bf16, vreinterpretq_p64_bf16): New.\n\t(vreinterpret_f32_bf16, vreinterpretq_f32_bf16): New.\n\t(vreinterpret_f64_bf16,vreinterpretq_f64_bf16): New.\n\t(vreinterpret_f16_bf16,vreinterpretq_f16_bf16): New.\n\t(vreinterpretq_p128_bf16): New.\n\ngcc/testsuite/\n\t* gcc.target/aarch64/advsimd-intrinsics/bf16_dup.c: New test.\n\t* gcc.target/aarch64/advsimd-intrinsics/bf16_reinterpret.c: New test.", "tree": {"sha": "57f831837d1d2fbebc7eddec0d5c9fef20af6e47", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/57f831837d1d2fbebc7eddec0d5c9fef20af6e47"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/8ea6c1b89a20ef7c675535ba1994355361dac977", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8ea6c1b89a20ef7c675535ba1994355361dac977", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8ea6c1b89a20ef7c675535ba1994355361dac977", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8ea6c1b89a20ef7c675535ba1994355361dac977/comments", "author": null, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "76a8c0f65e2b676bf78e44dbb5a3048f0d6e3170", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/76a8c0f65e2b676bf78e44dbb5a3048f0d6e3170", "html_url": "https://github.com/Rust-GCC/gccrs/commit/76a8c0f65e2b676bf78e44dbb5a3048f0d6e3170"}], "stats": {"total": 1121, "additions": 1118, "deletions": 3}, "files": [{"sha": "334a16e44e59fbcbcefb0cff39c035b96114e979", "filename": "gcc/ChangeLog", "status": "modified", "additions": 50, "deletions": 0, "changes": 50, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=8ea6c1b89a20ef7c675535ba1994355361dac977", "patch": "@@ -1,3 +1,53 @@\n+2020-02-25  Mihail Ionescu  <mihail.ionescu@arm.com>\n+\n+\t* config/aarch64/iterators.md (VDQF_F16) Add V4BF and V8BF.\n+\t(VALL_F16): Likewise.\n+\t(VALLDI_F16): Likewise.\n+\t(Vtype): Likewise.\n+\t(Vetype): Likewise.\n+\t(vswap_width_name): Likewise.\n+\t(VSWAP_WIDTH): Likewise.\n+\t(Vel): Likewise.\n+\t(VEL): Likewise.\n+\t(q): Likewise.\n+\t* config/aarch64/arm_neon.h (vset_lane_bf16, vsetq_lane_bf16): New.\n+\t(vget_lane_bf16, vgetq_lane_bf16): New.\n+\t(vcreate_bf16): New.\n+\t(vdup_n_bf16, vdupq_n_bf16): New.\n+\t(vdup_lane_bf16, vdup_laneq_bf16): New.\n+\t(vdupq_lane_bf16, vdupq_laneq_bf16): New.\n+\t(vduph_lane_bf16, vduph_laneq_bf16): New.\n+\t(vreinterpret_bf16_u8, vreinterpretq_bf16_u8): New.\n+\t(vreinterpret_bf16_u16, vreinterpretq_bf16_u16): New.\n+\t(vreinterpret_bf16_u32, vreinterpretq_bf16_u32): New.\n+\t(vreinterpret_bf16_u64, vreinterpretq_bf16_u64): New.\n+\t(vreinterpret_bf16_s8, vreinterpretq_bf16_s8): New.\n+\t(vreinterpret_bf16_s16, vreinterpretq_bf16_s16): New.\n+\t(vreinterpret_bf16_s32, vreinterpretq_bf16_s32): New.\n+\t(vreinterpret_bf16_s64, vreinterpretq_bf16_s64): New.\n+\t(vreinterpret_bf16_p8, vreinterpretq_bf16_p8): New.\n+\t(vreinterpret_bf16_p16, vreinterpretq_bf16_p16): New.\n+\t(vreinterpret_bf16_p64, vreinterpretq_bf16_p64): New\n+\t(vreinterpret_bf16_f16, vreinterpretq_bf16_f16): New\n+\t(vreinterpret_bf16_f32, vreinterpretq_bf16_f32): New.\n+\t(vreinterpret_bf16_f64, vreinterpretq_bf16_f64): New.\n+\t(vreinterpretq_bf16_p128): New.\n+\t(vreinterpret_s8_bf16, vreinterpretq_s8_bf16): New.\n+\t(vreinterpret_s16_bf16, vreinterpretq_s16_bf16): New.\n+\t(vreinterpret_s32_bf16, vreinterpretq_s32_bf16): New.\n+\t(vreinterpret_s64_bf16, vreinterpretq_s64_bf16): New.\n+\t(vreinterpret_u8_bf16, vreinterpretq_u8_bf16): New.\n+\t(vreinterpret_u16_bf16, vreinterpretq_u16_bf16): New.\n+\t(vreinterpret_u32_bf16, vreinterpretq_u32_bf16): New.\n+\t(vreinterpret_u64_bf16, vreinterpretq_u64_bf16): New.\n+\t(vreinterpret_p8_bf16, vreinterpretq_p8_bf16): New.\n+\t(vreinterpret_p16_bf16, vreinterpretq_p16_bf16): New.\n+\t(vreinterpret_p64_bf16, vreinterpretq_p64_bf16): New.\n+\t(vreinterpret_f32_bf16, vreinterpretq_f32_bf16): New.\n+\t(vreinterpret_f64_bf16,vreinterpretq_f64_bf16): New.\n+\t(vreinterpret_f16_bf16,vreinterpretq_f16_bf16): New.\n+\t(vreinterpretq_p128_bf16): New.\n+\n 2020-02-25  Dennis Zhang  <dennis.zhang@arm.com>\n \n \t* config/arm/arm_neon.h (vbfdot_f32, vbfdotq_f32): New"}, {"sha": "a4f2dd276f738e1a06924aa8747355adbbd5b374", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 501, "deletions": 0, "changes": 501, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=8ea6c1b89a20ef7c675535ba1994355361dac977", "patch": "@@ -34554,6 +34554,507 @@ vrnd64xq_f64 (float64x2_t __a)\n #pragma GCC push_options\n #pragma GCC target (\"arch=armv8.2-a+bf16\")\n \n+__extension__ extern __inline bfloat16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vset_lane_bf16 (bfloat16_t __elem, bfloat16x4_t __vec, const int __index)\n+{\n+  return __aarch64_vset_lane_any (__elem, __vec, __index);\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vsetq_lane_bf16 (bfloat16_t __elem, bfloat16x8_t __vec, const int __index)\n+{\n+  return __aarch64_vset_lane_any (__elem, __vec, __index);\n+}\n+\n+__extension__ extern __inline bfloat16_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vget_lane_bf16 (bfloat16x4_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_any (__a, __b);\n+}\n+\n+__extension__ extern __inline bfloat16_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vgetq_lane_bf16 (bfloat16x8_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_any (__a, __b);\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcreate_bf16 (uint64_t __a)\n+{\n+  return (bfloat16x4_t) __a;\n+}\n+\n+/* vdup */\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdup_n_bf16 (bfloat16_t __a)\n+{\n+  return (bfloat16x4_t) {__a, __a, __a, __a};\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdupq_n_bf16 (bfloat16_t __a)\n+{\n+  return (bfloat16x8_t) {__a, __a, __a, __a, __a, __a, __a, __a};\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdup_lane_bf16 (bfloat16x4_t __a, const int __b)\n+{\n+  return vdup_n_bf16 (__aarch64_vget_lane_any (__a, __b));\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdup_laneq_bf16 (bfloat16x8_t __a, const int __b)\n+{\n+  return vdup_n_bf16 (__aarch64_vget_lane_any (__a, __b));\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdupq_lane_bf16 (bfloat16x4_t __a, const int __b)\n+{\n+  return vdupq_n_bf16 (__aarch64_vget_lane_any (__a, __b));\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdupq_laneq_bf16 (bfloat16x8_t __a, const int __b)\n+{\n+  return vdupq_n_bf16 (__aarch64_vget_lane_any (__a, __b));\n+}\n+\n+__extension__ extern __inline bfloat16_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vduph_lane_bf16 (bfloat16x4_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_any (__a, __b);\n+}\n+\n+__extension__ extern __inline bfloat16_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vduph_laneq_bf16 (bfloat16x8_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_any (__a, __b);\n+}\n+\n+/* vreinterpret */\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_u8 (uint8x8_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_u16 (uint16x4_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_u32 (uint32x2_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_u64 (uint64x1_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_s8 (int8x8_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_s16 (int16x4_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_s32 (int32x2_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_s64 (int64x1_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_p8 (poly8x8_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_p16 (poly16x4_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_p64 (poly64x1_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_f16 (float16x4_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_f32 (float32x2_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_bf16_f64 (float64x1_t __a)\n+{\n+  return (bfloat16x4_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_u8 (uint8x16_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_u16 (uint16x8_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_u32 (uint32x4_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_u64 (uint64x2_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_s8 (int8x16_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_s16 (int16x8_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_s32 (int32x4_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_s64 (int64x2_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_p8 (poly8x16_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_p16 (poly16x8_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_p64 (poly64x2_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_p128 (poly128_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_f16 (float16x8_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_f32 (float32x4_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_bf16_f64 (float64x2_t __a)\n+{\n+  return (bfloat16x8_t)__a;\n+}\n+\n+__extension__ extern __inline int8x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_s8_bf16 (bfloat16x4_t __a)\n+{\n+  return (int8x8_t)__a;\n+}\n+\n+__extension__ extern __inline int16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_s16_bf16 (bfloat16x4_t __a)\n+{\n+  return (int16x4_t)__a;\n+}\n+\n+__extension__ extern __inline int32x2_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_s32_bf16 (bfloat16x4_t __a)\n+{\n+  return (int32x2_t)__a;\n+}\n+\n+__extension__ extern __inline int64x1_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_s64_bf16 (bfloat16x4_t __a)\n+{\n+  return (int64x1_t)__a;\n+}\n+\n+__extension__ extern __inline uint8x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_u8_bf16 (bfloat16x4_t __a)\n+{\n+  return (uint8x8_t)__a;\n+}\n+\n+__extension__ extern __inline uint16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_u16_bf16 (bfloat16x4_t __a)\n+{\n+  return (uint16x4_t)__a;\n+}\n+\n+__extension__ extern __inline uint32x2_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_u32_bf16 (bfloat16x4_t __a)\n+{\n+  return (uint32x2_t)__a;\n+}\n+\n+__extension__ extern __inline uint64x1_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_u64_bf16 (bfloat16x4_t __a)\n+{\n+  return (uint64x1_t)__a;\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_f16_bf16 (bfloat16x4_t __a)\n+{\n+  return (float16x4_t)__a;\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_f32_bf16 (bfloat16x4_t __a)\n+{\n+  return (float32x2_t)__a;\n+}\n+\n+__extension__ extern __inline float64x1_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_f64_bf16 (bfloat16x4_t __a)\n+{\n+  return (float64x1_t)__a;\n+}\n+\n+__extension__ extern __inline poly8x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p8_bf16 (bfloat16x4_t __a)\n+{\n+  return (poly8x8_t)__a;\n+}\n+\n+__extension__ extern __inline poly16x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p16_bf16 (bfloat16x4_t __a)\n+{\n+  return (poly16x4_t)__a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_bf16 (bfloat16x4_t __a)\n+{\n+  return (poly64x1_t)__a;\n+}\n+\n+__extension__ extern __inline int8x16_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_s8_bf16 (bfloat16x8_t __a)\n+{\n+  return (int8x16_t)__a;\n+}\n+\n+__extension__ extern __inline int16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_s16_bf16 (bfloat16x8_t __a)\n+{\n+  return (int16x8_t)__a;\n+}\n+\n+__extension__ extern __inline int32x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_s32_bf16 (bfloat16x8_t __a)\n+{\n+  return (int32x4_t)__a;\n+}\n+\n+__extension__ extern __inline int64x2_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_s64_bf16 (bfloat16x8_t __a)\n+{\n+  return (int64x2_t)__a;\n+}\n+\n+__extension__ extern __inline uint8x16_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_u8_bf16 (bfloat16x8_t __a)\n+{\n+  return (uint8x16_t)__a;\n+}\n+\n+__extension__ extern __inline uint16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_u16_bf16 (bfloat16x8_t __a)\n+{\n+  return (uint16x8_t)__a;\n+}\n+\n+__extension__ extern __inline uint32x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_u32_bf16 (bfloat16x8_t __a)\n+{\n+  return (uint32x4_t)__a;\n+}\n+\n+__extension__ extern __inline uint64x2_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_u64_bf16 (bfloat16x8_t __a)\n+{\n+  return (uint64x2_t)__a;\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_f16_bf16 (bfloat16x8_t __a)\n+{\n+  return (float16x8_t)__a;\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_f32_bf16 (bfloat16x8_t __a)\n+{\n+  return (float32x4_t)__a;\n+}\n+\n+__extension__ extern __inline float64x2_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_f64_bf16 (bfloat16x8_t __a)\n+{\n+  return (float64x2_t)__a;\n+}\n+\n+__extension__ extern __inline poly8x16_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p8_bf16 (bfloat16x8_t __a)\n+{\n+  return (poly8x16_t)__a;\n+}\n+\n+__extension__ extern __inline poly16x8_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p16_bf16 (bfloat16x8_t __a)\n+{\n+  return (poly16x8_t)__a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_bf16 (bfloat16x8_t __a)\n+{\n+  return (poly64x2_t)__a;\n+}\n+\n+__extension__ extern __inline poly128_t\n+__attribute__  ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p128_bf16 (bfloat16x8_t __a)\n+{\n+  return (poly128_t)__a;\n+}\n+\n __extension__ extern __inline float32x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vbfdot_f32 (float32x2_t __r, bfloat16x4_t __a, bfloat16x4_t __b)"}, {"sha": "571a5fae03a470f7029115ac28f06b6e8ae70ba5", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=8ea6c1b89a20ef7c675535ba1994355361dac977", "patch": "@@ -139,7 +139,8 @@\n (define_mode_iterator PTR [(SI \"ptr_mode == SImode\") (DI \"ptr_mode == DImode\")])\n \n ;; Advanced SIMD Float modes suitable for moving, loading and storing.\n-(define_mode_iterator VDQF_F16 [V4HF V8HF V2SF V4SF V2DF])\n+(define_mode_iterator VDQF_F16 [V4HF V8HF V2SF V4SF V2DF\n+\t\t\t\tV4BF V8BF])\n \n ;; Advanced SIMD Float modes.\n (define_mode_iterator VDQF [V2SF V4SF V2DF])\n@@ -180,7 +181,7 @@\n \n ;; All Advanced SIMD modes suitable for moving, loading, and storing.\n (define_mode_iterator VALL_F16 [V8QI V16QI V4HI V8HI V2SI V4SI V2DI\n-\t\t\t\tV4HF V8HF V2SF V4SF V2DF])\n+\t\t\t\tV4HF V8HF V4BF V8BF V2SF V4SF V2DF])\n \n ;; All Advanced SIMD modes suitable for moving, loading, and storing,\n ;; including special Bfloat vector types.\n@@ -196,7 +197,7 @@\n \n ;; All Advanced SIMD modes and DI.\n (define_mode_iterator VALLDI_F16 [V8QI V16QI V4HI V8HI V2SI V4SI V2DI\n-\t\t\t\t  V4HF V8HF V2SF V4SF V2DF DI])\n+\t\t\t\t  V4HF V8HF V4BF V8BF V2SF V4SF V2DF DI])\n \n ;; All Advanced SIMD modes, plus DI and DF.\n (define_mode_iterator VALLDIF [V8QI V16QI V4HI V8HI V2SI V4SI\n@@ -972,6 +973,7 @@\n \n (define_mode_attr Vtype [(V8QI \"8b\") (V16QI \"16b\")\n \t\t\t (V4HI \"4h\") (V8HI  \"8h\")\n+\t\t\t (V4BF \"4h\") (V8BF  \"8h\")\n                          (V2SI \"2s\") (V4SI  \"4s\")\n                          (DI   \"1d\") (DF    \"1d\")\n                          (V2DI \"2d\") (V2SF \"2s\")\n@@ -1015,6 +1017,7 @@\n \t\t\t  (VNx4SF \"s\") (VNx2SF \"s\")\n \t\t\t  (VNx2DI \"d\")\n \t\t\t  (VNx2DF \"d\")\n+\t\t\t  (BF \"h\") (V4BF \"h\") (V8BF \"h\")\n \t\t\t  (HF \"h\")\n \t\t\t  (SF \"s\") (DF \"d\")\n \t\t\t  (QI \"b\") (HI \"h\")\n@@ -1083,6 +1086,7 @@\n \t\t       (DF   \"DF\") (V2DF  \"DF\")\n \t\t       (SI   \"SI\") (HI    \"HI\")\n \t\t       (QI   \"QI\")\n+\t\t       (V4BF \"BF\") (V8BF \"BF\")\n \t\t       (VNx16QI \"QI\") (VNx8QI \"QI\") (VNx4QI \"QI\") (VNx2QI \"QI\")\n \t\t       (VNx8HI \"HI\") (VNx4HI \"HI\") (VNx2HI \"HI\")\n \t\t       (VNx8HF \"HF\") (VNx4HF \"HF\") (VNx2HF \"HF\")\n@@ -1102,6 +1106,7 @@\n \t\t       (V2DF \"df\") (DF   \"df\")\n \t\t       (SI   \"si\") (HI   \"hi\")\n \t\t       (QI   \"qi\")\n+\t\t       (V4BF \"bf\") (V8BF \"bf\")\n \t\t       (VNx16QI \"qi\") (VNx8QI \"qi\") (VNx4QI \"qi\") (VNx2QI \"qi\")\n \t\t       (VNx8HI \"hi\") (VNx4HI \"hi\") (VNx2HI \"hi\")\n \t\t       (VNx8HF \"hf\") (VNx4HF \"hf\") (VNx2HF \"hf\")\n@@ -1422,6 +1427,7 @@\n \n (define_mode_attr VSWAP_WIDTH [(V8QI \"V16QI\") (V16QI \"V8QI\")\n \t\t\t\t(V4HI \"V8HI\") (V8HI  \"V4HI\")\n+\t\t\t\t(V8BF \"V4BF\") (V4BF  \"V8BF\")\n \t\t\t\t(V2SI \"V4SI\") (V4SI  \"V2SI\")\n \t\t\t\t(DI   \"V2DI\") (V2DI  \"DI\")\n \t\t\t\t(V2SF \"V4SF\") (V4SF  \"V2SF\")\n@@ -1434,6 +1440,7 @@\n \t\t\t\t    (DI   \"to_128\") (V2DI  \"to_64\")\n \t\t\t\t    (V4HF \"to_128\") (V8HF  \"to_64\")\n \t\t\t\t    (V2SF \"to_128\") (V4SF  \"to_64\")\n+\t\t\t\t    (V4BF \"to_128\") (V8BF  \"to_64\")\n \t\t\t\t    (DF   \"to_128\") (V2DF  \"to_64\")])\n \n ;; For certain vector-by-element multiplication instructions we must\n@@ -1467,6 +1474,7 @@\n ;; Defined to '_q' for 128-bit types.\n (define_mode_attr q [(V8QI \"\") (V16QI \"_q\")\n \t\t     (V4HI \"\") (V8HI  \"_q\")\n+\t\t     (V4BF \"\") (V8BF  \"_q\")\n \t\t     (V2SI \"\") (V4SI  \"_q\")\n \t\t     (DI   \"\") (V2DI  \"_q\")\n \t\t     (V4HF \"\") (V8HF \"_q\")"}, {"sha": "c942486bafb556004e74f20eff118fea680c506b", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=8ea6c1b89a20ef7c675535ba1994355361dac977", "patch": "@@ -1,3 +1,8 @@\n+2020-02-25  Mihail Ionescu  <mihail.ionescu@arm.com>\n+\n+\t* gcc.target/aarch64/advsimd-intrinsics/bf16_dup.c: New test.\n+\t* gcc.target/aarch64/advsimd-intrinsics/bf16_reinterpret.c: New test.\n+\n 2020-02-25  Dennis Zhang  <dennis.zhang@arm.com>\n \n \t* gcc.target/arm/simd/bf16_dot_1.c: New test."}, {"sha": "c42c7acbbe92d348e9cd18d3a03a38895af83632", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/bf16_dup.c", "status": "added", "additions": 85, "deletions": 0, "changes": 85, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_dup.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_dup.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_dup.c?ref=8ea6c1b89a20ef7c675535ba1994355361dac977", "patch": "@@ -0,0 +1,85 @@\n+/* { dg-do assemble { target { aarch64*-*-* } } } */\n+/* { dg-require-effective-target arm_v8_2a_bf16_neon_ok } */\n+/* { dg-options \"-O2\" } */\n+/* { dg-add-options arm_v8_2a_bf16_neon }  */\n+/* { dg-additional-options \"-save-temps\" } */\n+\n+#include <arm_neon.h>\n+\n+float32x2_t test_vcreate (float32x2_t r, uint64_t a, uint64_t b)\n+{\n+  bfloat16x4_t _a = vcreate_bf16(a);\n+  bfloat16x4_t _b = vcreate_bf16(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+/* { dg-final { scan-assembler {bfdot\\tv[0-9]+.2s, v[0-9]+.4h, v[0-9]+.4h} } } */\n+\n+bfloat16x4_t test_vset_lane_bf16 (bfloat16_t a, bfloat16x4_t b)\n+{\n+  return vset_lane_bf16 (a, b, 3);\n+}\n+\n+bfloat16x8_t test_vsetq_lane_bf16 (bfloat16_t a, bfloat16x8_t b)\n+{\n+  return vsetq_lane_bf16 (a, b, 7);\n+}\n+/* { dg-final { scan-assembler-times \"ins\\\\t\" 2 } } */\n+\n+bfloat16x4_t vdup_test (bfloat16_t a)\n+{\n+  return vdup_n_bf16 (a);\n+}\n+/* { dg-final { scan-assembler \"dup\\\\tv\\[0-9\\]+\\.4h, v\\[0-9\\]+.h\\\\\\[0\\\\\\]\" } } */\n+\n+bfloat16x8_t vdupq_test (bfloat16_t a)\n+{\n+  return vdupq_n_bf16 (a);\n+}\n+\n+bfloat16x8_t test_vdupq_lane_bf16 (bfloat16x4_t a)\n+{\n+  return vdupq_lane_bf16 (a, 1);\n+}\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.8h, v\\[0-9\\]+.h\\\\\\[0\\\\\\]\" 2 } } */\n+\n+bfloat16_t test_vget_lane_bf16 (bfloat16x4_t a)\n+{\n+  return vget_lane_bf16 (a, 1);\n+}\n+/* { dg-final { scan-assembler-times \"dup\\\\th\\[0-9\\]+, v\\[0-9\\]+\\.h\\\\\\[1\\\\\\]\" 2 } } */\n+\n+bfloat16x4_t test_vdup_lane_bf16 (bfloat16x4_t a)\n+{\n+  return vdup_lane_bf16 (a, 1);\n+}\n+/* { dg-final { scan-assembler \"dup\\\\tv\\[0-9\\]+\\.4h, v\\[0-9\\]+\\.h\\\\\\[1\\\\\\]\" } } */\n+\n+bfloat16x4_t test_vdup_laneq_bf16 (bfloat16x8_t a)\n+{\n+  return vdup_laneq_bf16 (a, 7);\n+}\n+/* { dg-final { scan-assembler \"dup\\\\tv\\[0-9\\]+\\.8h, v\\[0-9\\]+\\.h\\\\\\[7\\\\\\]\" } } */\n+\n+bfloat16x8_t test_vdupq_laneq_bf16 (bfloat16x8_t a)\n+{\n+  return vdupq_laneq_bf16 (a, 5);\n+}\n+/* { dg-final { scan-assembler \"dup\\\\tv\\[0-9\\]+\\.8h, v\\[0-9\\]+\\.h\\\\\\[5\\\\\\]\" } } */\n+\n+bfloat16_t test_vduph_lane_bf16 (bfloat16x4_t a)\n+{\n+  return vduph_lane_bf16 (a, 3);\n+}\n+/* { dg-final { scan-assembler \"dup\\\\th\\[0-9\\]+, v\\[0-9\\]+\\.h\\\\\\[3\\\\\\]\" } } */\n+\n+bfloat16_t test_vgetq_lane_bf16 (bfloat16x8_t a)\n+{\n+  return vgetq_lane_bf16 (a, 7);\n+}\n+\n+bfloat16_t test_vduph_laneq_bf16 (bfloat16x8_t a)\n+{\n+  return vduph_laneq_bf16 (a, 7);\n+}\n+/* { dg-final { scan-assembler-times \"dup\\\\th\\[0-9\\]+, v\\[0-9\\]+\\.h\\\\\\[7\\\\\\]\" 2 } } */"}, {"sha": "f5adf40c648e16c649ef5d68accb291b822f2936", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/bf16_reinterpret.c", "status": "added", "additions": 466, "deletions": 0, "changes": 466, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_reinterpret.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8ea6c1b89a20ef7c675535ba1994355361dac977/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_reinterpret.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_reinterpret.c?ref=8ea6c1b89a20ef7c675535ba1994355361dac977", "patch": "@@ -0,0 +1,466 @@\n+/* { dg-do assemble { target { aarch64*-*-* } } } */\n+/* { dg-require-effective-target arm_v8_2a_bf16_neon_ok } */\n+/* { dg-add-options arm_v8_2a_bf16_neon }  */\n+/* { dg-additional-options \"-save-temps\" } */\n+\n+#include <arm_neon.h>\n+\n+float32x2_t\n+test_vbfdot_f32_s8 (float32x2_t r, int8x8_t a, int8x8_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_s8(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_s8(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_s16 (float32x2_t r, int16x4_t a, int16x4_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_s16(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_s16(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_s32 (float32x2_t r, int32x2_t a, int32x2_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_s32(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_s32(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_s64 (float32x2_t r, int64x1_t a, int64x1_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_s64(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_s64(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_u8 (float32x2_t r, uint8x8_t a, uint8x8_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_u8(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_u8(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_u16 (float32x2_t r, uint16x4_t a, uint16x4_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_u16(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_u16(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_u32 (float32x2_t r, uint32x2_t a, uint32x2_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_u32(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_u32(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_u64 (float32x2_t r, uint64x1_t a, uint64x1_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_u64(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_u64(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_p8 (float32x2_t r, poly8x8_t a, poly8x8_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_p8(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_p8(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_p16 (float32x2_t r, poly16x4_t a, poly16x4_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_p16(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_p16(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_p64 (float32x2_t r, poly64x1_t a, poly64x1_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_p64(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_p64(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_f16 (float32x2_t r, float16x4_t a, float16x4_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_f16(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_f16(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_f32 (float32x2_t r, float32x2_t a, float32x2_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_f32(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_f32(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x2_t\n+test_vbfdot_f32_f64 (float32x2_t r, float64x1_t a, float64x1_t b)\n+{\n+  bfloat16x4_t _a = vreinterpret_bf16_f64(a);\n+  bfloat16x4_t _b = vreinterpret_bf16_f64(b);\n+\n+  return vbfdot_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_s8 (float32x4_t r, int8x16_t a, int8x16_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_s8(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_s8(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_s16 (float32x4_t r, int16x8_t a, int16x8_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_s16(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_s16(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_s32 (float32x4_t r, int32x4_t a, int32x4_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_s32(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_s32(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_s64 (float32x4_t r, int64x2_t a, int64x2_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_s64(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_s64(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_u8 (float32x4_t r, uint8x16_t a, uint8x16_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_u8(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_u8(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_u16 (float32x4_t r, uint16x8_t a, uint16x8_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_u16(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_u16(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_u32 (float32x4_t r, uint32x4_t a, uint32x4_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_u32(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_u32(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_u64 (float32x4_t r, uint64x2_t a, uint64x2_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_u64(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_u64(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_p8 (float32x4_t r, poly8x16_t a, poly8x16_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_p8(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_p8(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_p16 (float32x4_t r, poly16x8_t a, poly16x8_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_p16(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_p16(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_p64 (float32x4_t r, poly64x2_t a, poly64x2_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_p64(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_p64(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_p128 (float32x4_t r, poly128_t a, poly128_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_p128(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_p128(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_f16 (float32x4_t r, float16x8_t a, float16x8_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_f16(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_f16(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_f32 (float32x4_t r, float32x4_t a, float32x4_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_f32(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_f32(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+float32x4_t\n+test_vbfdotq_f32_f64 (float32x4_t r, float64x2_t a, float64x2_t b)\n+{\n+  bfloat16x8_t _a = vreinterpretq_bf16_f64(a);\n+  bfloat16x8_t _b = vreinterpretq_bf16_f64(b);\n+\n+  return vbfdotq_f32 (r, _a, _b);\n+}\n+\n+/* { dg-final { scan-assembler-times {bfdot\\tv[0-9]+.2s, v[0-9]+.4h, v[0-9]+.4h} 14 } } */\n+/* { dg-final { scan-assembler-times {bfdot\\tv[0-9]+.4s, v[0-9]+.8h, v[0-9]+.8h} 15 } } */\n+\n+int8x8_t test_vreinterpret_s8_bf16 (bfloat16x4_t a, int8x8_t b)\n+{\n+  int8x8_t _a = vreinterpret_s8_bf16 (a);\n+  return vadd_s8 (_a, b);\n+}\n+\n+int16x4_t test_vreinterpret_s16_bf16 (bfloat16x4_t a, int16x4_t b)\n+{\n+  int16x4_t _a = vreinterpret_s16_bf16 (a);\n+  return vadd_s16 (_a, b);\n+}\n+\n+int32x2_t test_vreinterpret_s32_bf16 (bfloat16x4_t a, int32x2_t b)\n+{\n+  int32x2_t _a = vreinterpret_s32_bf16 (a);\n+  return vadd_s32 (_a, b);\n+}\n+\n+int64x1_t test_vreinterpret_s64_bf16 (bfloat16x4_t a, int64x1_t b)\n+{\n+  int64x1_t _a = vreinterpret_s64_bf16 (a);\n+  return vrshl_s64 (_a, b);\n+}\n+\n+uint8x8_t test_vreinterpret_u8_bf16 (bfloat16x4_t a, uint8x8_t b)\n+{\n+  uint8x8_t _a = vreinterpret_u8_bf16 (a);\n+  return vadd_u8 (_a, b);\n+}\n+\n+uint16x4_t test_vreinterpret_u16_bf16 (bfloat16x4_t a, uint16x4_t b)\n+{\n+  uint16x4_t _a = vreinterpret_u16_bf16 (a);\n+  return vadd_u16 (_a, b);\n+}\n+\n+uint32x2_t test_vreinterpret_u32_bf16 (bfloat16x4_t a, uint32x2_t b)\n+{\n+  uint32x2_t _a = vreinterpret_u32_bf16 (a);\n+  return vadd_u32 (_a, b);\n+}\n+\n+uint64x1_t test_vreinterpret_u64_bf16 (bfloat16x4_t a, int64x1_t b)\n+{\n+  uint64x1_t _a = vreinterpret_u64_bf16 (a);\n+  return vrshl_u64 (_a, b);\n+}\n+\n+poly8x8_t test_vreinterpret_p8_bf16 (bfloat16x4_t a, poly8x8_t b)\n+{\n+  poly8x8_t _a = vreinterpret_p8_bf16 (a);\n+  return vzip1_p8 (_a, b);\n+}\n+\n+poly16x4_t test_vreinterpret_p16_bf16 (bfloat16x4_t a, poly16x4_t b)\n+{\n+  poly16x4_t _a = vreinterpret_p16_bf16 (a);\n+  return vzip1_p16 (_a, b);\n+}\n+\n+poly64x1_t test_vreinterpret_p64_bf16 (bfloat16x4_t a, poly64x1_t b)\n+{\n+  poly64x1_t _a = vreinterpret_p64_bf16 (a);\n+  return vsli_n_p64 (_a, b, 3);\n+}\n+\n+float32x2_t test_vreinterpret_f32_bf16 (bfloat16x4_t a, float32x2_t b)\n+{\n+  float32x2_t _a = vreinterpret_f32_bf16 (a);\n+  return vsub_f32 (_a, b);\n+}\n+\n+float64x1_t test_vreinterpret_f64_bf16 (bfloat16x4_t a, float64x1_t b)\n+{\n+  float64x1_t _a = vreinterpret_f64_bf16 (a);\n+  return vsub_f64 (_a, b);\n+}\n+\n+int8x16_t test_vreinterpretq_s8_bf16 (bfloat16x8_t a, int8x16_t b)\n+{\n+  int8x16_t _a = vreinterpretq_s8_bf16 (a);\n+  return vaddq_s8 (_a, b);\n+}\n+\n+int16x8_t test_vreinterpretq_s16_bf16 (bfloat16x8_t a, int16x8_t b)\n+{\n+  int16x8_t _a = vreinterpretq_s16_bf16 (a);\n+  return vaddq_s16 (_a, b);\n+}\n+\n+int32x4_t test_vreinterpretq_s32_bf16 (bfloat16x8_t a, int32x4_t b)\n+{\n+  int32x4_t _a = vreinterpretq_s32_bf16 (a);\n+  return vaddq_s32 (_a, b);\n+}\n+\n+int64x2_t test_vreinterpretq_s64_bf16 (bfloat16x8_t a, int64x2_t b)\n+{\n+  int64x2_t _a = vreinterpretq_s64_bf16 (a);\n+  return vaddq_s64 (_a, b);\n+}\n+\n+uint8x16_t test_vreinterpretq_u8_bf16 (bfloat16x8_t a, uint8x16_t b)\n+{\n+  uint8x16_t _a = vreinterpretq_u8_bf16 (a);\n+  return vaddq_u8 (_a, b);\n+}\n+\n+uint16x8_t test_vreinterpretq_u16_bf16 (bfloat16x8_t a, uint16x8_t b)\n+{\n+  uint16x8_t _a = vreinterpretq_u16_bf16 (a);\n+  return vaddq_u16 (_a, b);\n+}\n+\n+uint32x4_t test_vreinterpretq_u32_bf16 (bfloat16x8_t a, uint32x4_t b)\n+{\n+  uint32x4_t _a = vreinterpretq_u32_bf16 (a);\n+  return vaddq_u32 (_a, b);\n+}\n+\n+uint64x2_t test_vreinterpretq_u64_bf16 (bfloat16x8_t a, uint64x2_t b)\n+{\n+  uint64x2_t _a = vreinterpretq_u64_bf16 (a);\n+  return vaddq_u64 (_a, b);\n+}\n+\n+poly8x16_t test_vreinterpretq_p8_bf16 (bfloat16x8_t a, poly8x16_t b)\n+{\n+  poly8x16_t _a = vreinterpretq_p8_bf16 (a);\n+  return vzip1q_p8 (_a, b);\n+}\n+\n+poly16x8_t test_vreinterpretq_p16_bf16 (bfloat16x8_t a, poly16x8_t b)\n+{\n+  poly16x8_t _a = vreinterpretq_p16_bf16 (a);\n+  return vzip1q_p16 (_a, b);\n+}\n+\n+poly64x2_t test_vreinterpretq_p64_bf16 (bfloat16x8_t a, poly64x2_t b)\n+{\n+  poly64x2_t _a = vreinterpretq_p64_bf16 (a);\n+  return vsliq_n_p64 (_a, b, 3);\n+}\n+\n+poly128_t test_vreinterpretq_p128_bf16 (bfloat16x8_t a, poly16x8_t b)\n+{\n+  poly128_t _a = vreinterpretq_p128_bf16 (a);\n+  return _a;\n+}\n+\n+float32x4_t test_vreinterpretq_f32_bf16 (bfloat16x8_t a, float32x4_t b)\n+{\n+  float32x4_t _a = vreinterpretq_f32_bf16 (a);\n+  return vsubq_f32 (_a, b);\n+}\n+\n+float64x2_t test_vreinterpretq_f64_bf16 (bfloat16x8_t a, float64x2_t b)\n+{\n+  float64x2_t _a = vreinterpretq_f64_bf16 (a);\n+  return vsubq_f64 (_a, b);\n+}\n+\n+float16x4_t test_vreinterpret_f16_bf16 (bfloat16x4_t a)\n+{\n+  return vreinterpret_f16_bf16 (a);\n+}\n+\n+float16x8_t test_vreinterpretq_f16_bf16 (bfloat16x8_t a)\n+{\n+  return vreinterpretq_f16_bf16 (a);\n+}\n+\n+/* { dg-final { scan-assembler-times {add\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.2s} 2 } } */\n+/* { dg-final { scan-assembler-times {add\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.4h} 2 } } */\n+/* { dg-final { scan-assembler-times {add\\tv[0-9]+.8b, v[0-9]+.8b, v[0-9]+.8b} 2 } } */\n+\n+/* { dg-final { scan-assembler-times {add\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4s} 2 } } */\n+/* { dg-final { scan-assembler-times {add\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8h} 2 } } */\n+/* { dg-final { scan-assembler-times {add\\tv[0-9]+.16b, v[0-9]+.16b, v[0-9]+.16b} 2 } } */\n+\n+/* { dg-final { scan-assembler {fsub\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.2s} } } */\n+/* { dg-final { scan-assembler {fsub\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4s} } } */\n+/* { dg-final { scan-assembler {fsub\\tv[0-9]+.2d, v[0-9]+.2d, v[0-9]+.2d} } } */\n+/* { dg-final { scan-assembler {fsub\\td[0-9]+, d[0-9]+, d[0-9]+} } } */\n+\n+/* { dg-final { scan-assembler {zip1\\tv[0-9]+.8b, v[0-9]+.8b, v[0-9]+.8b} } } */\n+/* { dg-final { scan-assembler {zip1\\tv[0-9]+.16b, v[0-9]+.16b, v[0-9]+.16b} } } */\n+/* { dg-final { scan-assembler {zip1\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.4h} } } */\n+/* { dg-final { scan-assembler {zip1\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8h} } } */\n+\n+/* { dg-final { scan-assembler {sli\\tv[0-9]+.2d, v[0-9]+.2d, 3} } } */\n+/* { dg-final { scan-assembler {sli\\td[0-9]+, d[0-9]+, 3} } } */\n+\n+/* { dg-final { scan-assembler {urshl\\td[0-9]+, d[0-9]+, d[0-9]+} } } */\n+/* { dg-final { scan-assembler {srshl\\td[0-9]+, d[0-9]+, d[0-9]+} } } */"}]}