{"sha": "1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MWYwYmU5ZWU4NmY2M2JhYzljNDU0MWE5Y2ZhZjUyY2I1YWU1ZTg5YQ==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-10-14T22:51:46Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-10-14T22:51:46Z"}, "message": "runtime: copy mprof code from Go 1.7 runtime\n    \n    Also create a gccgo version of some of the traceback code in\n    traceback_gccgo.go, replacing some code currently in C.\n    \n    This required modifying the compiler so that when compiling the runtime\n    package a slice expression does not cause a local array variable to\n    escape to the heap.\n    \n    Reviewed-on: https://go-review.googlesource.com/31230\n\nFrom-SVN: r241189", "tree": {"sha": "584ab0cd64a2743fa7198ca34c7b13282c1c0ad7", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/584ab0cd64a2743fa7198ca34c7b13282c1c0ad7"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/comments", "author": null, "committer": null, "parents": [{"sha": "2045acd902fd8028514a72c58c98dba11749b8ad", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2045acd902fd8028514a72c58c98dba11749b8ad", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2045acd902fd8028514a72c58c98dba11749b8ad"}], "stats": {"total": 1855, "additions": 965, "deletions": 890}, "files": [{"sha": "2e09ec1db85a37443e12f0813e54587d86bbd8d9", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -1,4 +1,4 @@\n-993840643e27e52cda7e86e6a775f54443ea5d07\n+ec3dc927da71d15cac48a13c0fb0c1f94572d0d2\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "261129faa0c99f41f597b9696c4732fb005d51d0", "filename": "gcc/go/gofrontend/expressions.cc", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -10308,7 +10308,7 @@ Array_index_expression::do_determine_type(const Type_context*)\n // Check types of an array index.\n \n void\n-Array_index_expression::do_check_types(Gogo*)\n+Array_index_expression::do_check_types(Gogo* gogo)\n {\n   Numeric_constant nc;\n   unsigned long v;\n@@ -10427,7 +10427,18 @@ Array_index_expression::do_check_types(Gogo*)\n       if (!this->array_->is_addressable())\n \tthis->report_error(_(\"slice of unaddressable value\"));\n       else\n-\tthis->array_->address_taken(true);\n+\t{\n+\t  bool escapes = true;\n+\n+\t  // When compiling the runtime, a slice operation does not\n+\t  // cause local variables to escape.  When escape analysis\n+\t  // becomes the default, this should be changed to make it an\n+\t  // error if we have a slice operation that escapes.\n+\t  if (gogo->compiling_runtime() && gogo->package_name() == \"runtime\")\n+\t    escapes = false;\n+\n+\t  this->array_->address_taken(escapes);\n+\t}\n     }\n }\n "}, {"sha": "f90331fc5292b887fe1cab351003f0cf5b644650", "filename": "libgo/Makefile.am", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2FMakefile.am", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2FMakefile.am", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2FMakefile.am?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -478,7 +478,6 @@ runtime_files = \\\n \truntime/go-signal.c \\\n \truntime/go-strcmp.c \\\n \truntime/go-strslice.c \\\n-\truntime/go-traceback.c \\\n \truntime/go-type-complex.c \\\n \truntime/go-type-eface.c \\\n \truntime/go-type-float.c \\\n@@ -515,7 +514,6 @@ runtime_files = \\\n \tgo-iface.c \\\n \tlfstack.c \\\n \tmalloc.c \\\n-\tmprof.c \\\n \tnetpoll.c \\\n \trdebug.c \\\n \treflect.c \\"}, {"sha": "1955ede5b5dd200226389222d9bba2db0c836fa6", "filename": "libgo/Makefile.in", "status": "modified", "additions": 10, "deletions": 21, "changes": 31, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2FMakefile.in?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -253,17 +253,17 @@ am__objects_6 = go-append.lo go-assert.lo go-assert-interface.lo \\\n \tgo-nanotime.lo go-now.lo go-new.lo go-nosys.lo go-panic.lo \\\n \tgo-recover.lo go-reflect-call.lo go-runtime-error.lo \\\n \tgo-setenv.lo go-signal.lo go-strcmp.lo go-strslice.lo \\\n-\tgo-traceback.lo go-type-complex.lo go-type-eface.lo \\\n-\tgo-type-float.lo go-type-identity.lo go-type-interface.lo \\\n-\tgo-type-string.lo go-typedesc-equal.lo go-unsafe-new.lo \\\n-\tgo-unsafe-newarray.lo go-unsafe-pointer.lo go-unsetenv.lo \\\n-\tgo-unwind.lo go-varargs.lo env_posix.lo heapdump.lo mcache.lo \\\n-\tmcentral.lo $(am__objects_1) mfixalloc.lo mgc0.lo mheap.lo \\\n-\tmsize.lo $(am__objects_2) panic.lo parfor.lo print.lo proc.lo \\\n+\tgo-type-complex.lo go-type-eface.lo go-type-float.lo \\\n+\tgo-type-identity.lo go-type-interface.lo go-type-string.lo \\\n+\tgo-typedesc-equal.lo go-unsafe-new.lo go-unsafe-newarray.lo \\\n+\tgo-unsafe-pointer.lo go-unsetenv.lo go-unwind.lo go-varargs.lo \\\n+\tenv_posix.lo heapdump.lo mcache.lo mcentral.lo \\\n+\t$(am__objects_1) mfixalloc.lo mgc0.lo mheap.lo msize.lo \\\n+\t$(am__objects_2) panic.lo parfor.lo print.lo proc.lo \\\n \truntime.lo signal_unix.lo thread.lo $(am__objects_3) yield.lo \\\n-\t$(am__objects_4) go-iface.lo lfstack.lo malloc.lo mprof.lo \\\n-\tnetpoll.lo rdebug.lo reflect.lo runtime1.lo sigqueue.lo \\\n-\ttime.lo $(am__objects_5)\n+\t$(am__objects_4) go-iface.lo lfstack.lo malloc.lo netpoll.lo \\\n+\trdebug.lo reflect.lo runtime1.lo sigqueue.lo time.lo \\\n+\t$(am__objects_5)\n am_libgo_llgo_la_OBJECTS = $(am__objects_6)\n libgo_llgo_la_OBJECTS = $(am_libgo_llgo_la_OBJECTS)\n libgo_llgo_la_LINK = $(LIBTOOL) --tag=CC $(AM_LIBTOOLFLAGS) \\\n@@ -877,7 +877,6 @@ runtime_files = \\\n \truntime/go-signal.c \\\n \truntime/go-strcmp.c \\\n \truntime/go-strslice.c \\\n-\truntime/go-traceback.c \\\n \truntime/go-type-complex.c \\\n \truntime/go-type-eface.c \\\n \truntime/go-type-float.c \\\n@@ -914,7 +913,6 @@ runtime_files = \\\n \tgo-iface.c \\\n \tlfstack.c \\\n \tmalloc.c \\\n-\tmprof.c \\\n \tnetpoll.c \\\n \trdebug.c \\\n \treflect.c \\\n@@ -1593,7 +1591,6 @@ distclean-compile:\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-signal.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-strcmp.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-strslice.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-traceback.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-type-complex.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-type-eface.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-type-float.Plo@am__quote@\n@@ -1620,7 +1617,6 @@ distclean-compile:\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mfixalloc.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mgc0.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mheap.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mprof.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/msize.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/netpoll.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/netpoll_epoll.Plo@am__quote@\n@@ -1986,13 +1982,6 @@ go-strslice.lo: runtime/go-strslice.c\n @AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n @am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o go-strslice.lo `test -f 'runtime/go-strslice.c' || echo '$(srcdir)/'`runtime/go-strslice.c\n \n-go-traceback.lo: runtime/go-traceback.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT go-traceback.lo -MD -MP -MF $(DEPDIR)/go-traceback.Tpo -c -o go-traceback.lo `test -f 'runtime/go-traceback.c' || echo '$(srcdir)/'`runtime/go-traceback.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/go-traceback.Tpo $(DEPDIR)/go-traceback.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/go-traceback.c' object='go-traceback.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o go-traceback.lo `test -f 'runtime/go-traceback.c' || echo '$(srcdir)/'`runtime/go-traceback.c\n-\n go-type-complex.lo: runtime/go-type-complex.c\n @am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT go-type-complex.lo -MD -MP -MF $(DEPDIR)/go-type-complex.Tpo -c -o go-type-complex.lo `test -f 'runtime/go-type-complex.c' || echo '$(srcdir)/'`runtime/go-type-complex.c\n @am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/go-type-complex.Tpo $(DEPDIR)/go-type-complex.Plo"}, {"sha": "56e307fb3f553886783ec9aa9bf23dff9c9e4bde", "filename": "libgo/go/runtime/debug.go", "status": "modified", "additions": 0, "deletions": 151, "changes": 151, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fgo%2Fruntime%2Fdebug.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fgo%2Fruntime%2Fdebug.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fdebug.go?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -4,18 +4,6 @@\n \n package runtime\n \n-// Breakpoint executes a breakpoint trap.\n-func Breakpoint()\n-\n-// LockOSThread wires the calling goroutine to its current operating system thread.\n-// Until the calling goroutine exits or calls UnlockOSThread, it will always\n-// execute in that thread, and no other goroutine can.\n-func LockOSThread()\n-\n-// UnlockOSThread unwires the calling goroutine from its fixed operating system thread.\n-// If the calling goroutine has not called LockOSThread, UnlockOSThread is a no-op.\n-func UnlockOSThread()\n-\n // GOMAXPROCS sets the maximum number of CPUs that can be executing\n // simultaneously and returns the previous setting. If n < 1, it does not\n // change the current setting.\n@@ -36,145 +24,6 @@ func NumCgoCall() int64\n // NumGoroutine returns the number of goroutines that currently exist.\n func NumGoroutine() int\n \n-// MemProfileRate controls the fraction of memory allocations\n-// that are recorded and reported in the memory profile.\n-// The profiler aims to sample an average of\n-// one allocation per MemProfileRate bytes allocated.\n-//\n-// To include every allocated block in the profile, set MemProfileRate to 1.\n-// To turn off profiling entirely, set MemProfileRate to 0.\n-//\n-// The tools that process the memory profiles assume that the\n-// profile rate is constant across the lifetime of the program\n-// and equal to the current value.  Programs that change the\n-// memory profiling rate should do so just once, as early as\n-// possible in the execution of the program (for example,\n-// at the beginning of main).\n-var MemProfileRate int = 512 * 1024\n-\n-// A MemProfileRecord describes the live objects allocated\n-// by a particular call sequence (stack trace).\n-type MemProfileRecord struct {\n-\tAllocBytes, FreeBytes     int64       // number of bytes allocated, freed\n-\tAllocObjects, FreeObjects int64       // number of objects allocated, freed\n-\tStack0                    [32]uintptr // stack trace for this record; ends at first 0 entry\n-}\n-\n-// InUseBytes returns the number of bytes in use (AllocBytes - FreeBytes).\n-func (r *MemProfileRecord) InUseBytes() int64 { return r.AllocBytes - r.FreeBytes }\n-\n-// InUseObjects returns the number of objects in use (AllocObjects - FreeObjects).\n-func (r *MemProfileRecord) InUseObjects() int64 {\n-\treturn r.AllocObjects - r.FreeObjects\n-}\n-\n-// Stack returns the stack trace associated with the record,\n-// a prefix of r.Stack0.\n-func (r *MemProfileRecord) Stack() []uintptr {\n-\tfor i, v := range r.Stack0 {\n-\t\tif v == 0 {\n-\t\t\treturn r.Stack0[0:i]\n-\t\t}\n-\t}\n-\treturn r.Stack0[0:]\n-}\n-\n-// MemProfile returns n, the number of records in the current memory profile.\n-// If len(p) >= n, MemProfile copies the profile into p and returns n, true.\n-// If len(p) < n, MemProfile does not change p and returns n, false.\n-//\n-// If inuseZero is true, the profile includes allocation records\n-// where r.AllocBytes > 0 but r.AllocBytes == r.FreeBytes.\n-// These are sites where memory was allocated, but it has all\n-// been released back to the runtime.\n-//\n-// Most clients should use the runtime/pprof package or\n-// the testing package's -test.memprofile flag instead\n-// of calling MemProfile directly.\n-func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool)\n-\n-// A StackRecord describes a single execution stack.\n-type StackRecord struct {\n-\tStack0 [32]uintptr // stack trace for this record; ends at first 0 entry\n-}\n-\n-// Stack returns the stack trace associated with the record,\n-// a prefix of r.Stack0.\n-func (r *StackRecord) Stack() []uintptr {\n-\tfor i, v := range r.Stack0 {\n-\t\tif v == 0 {\n-\t\t\treturn r.Stack0[0:i]\n-\t\t}\n-\t}\n-\treturn r.Stack0[0:]\n-}\n-\n-// ThreadCreateProfile returns n, the number of records in the thread creation profile.\n-// If len(p) >= n, ThreadCreateProfile copies the profile into p and returns n, true.\n-// If len(p) < n, ThreadCreateProfile does not change p and returns n, false.\n-//\n-// Most clients should use the runtime/pprof package instead\n-// of calling ThreadCreateProfile directly.\n-func ThreadCreateProfile(p []StackRecord) (n int, ok bool)\n-\n-// GoroutineProfile returns n, the number of records in the active goroutine stack profile.\n-// If len(p) >= n, GoroutineProfile copies the profile into p and returns n, true.\n-// If len(p) < n, GoroutineProfile does not change p and returns n, false.\n-//\n-// Most clients should use the runtime/pprof package instead\n-// of calling GoroutineProfile directly.\n-func GoroutineProfile(p []StackRecord) (n int, ok bool)\n-\n-// CPUProfile returns the next chunk of binary CPU profiling stack trace data,\n-// blocking until data is available.  If profiling is turned off and all the profile\n-// data accumulated while it was on has been returned, CPUProfile returns nil.\n-// The caller must save the returned data before calling CPUProfile again.\n-//\n-// Most clients should use the runtime/pprof package or\n-// the testing package's -test.cpuprofile flag instead of calling\n-// CPUProfile directly.\n-func CPUProfile() []byte\n-\n-// SetCPUProfileRate sets the CPU profiling rate to hz samples per second.\n-// If hz <= 0, SetCPUProfileRate turns off profiling.\n-// If the profiler is on, the rate cannot be changed without first turning it off.\n-//\n-// Most clients should use the runtime/pprof package or\n-// the testing package's -test.cpuprofile flag instead of calling\n-// SetCPUProfileRate directly.\n-func SetCPUProfileRate(hz int)\n-\n-// SetBlockProfileRate controls the fraction of goroutine blocking events\n-// that are reported in the blocking profile.  The profiler aims to sample\n-// an average of one blocking event per rate nanoseconds spent blocked.\n-//\n-// To include every blocking event in the profile, pass rate = 1.\n-// To turn off profiling entirely, pass rate <= 0.\n-func SetBlockProfileRate(rate int)\n-\n-// BlockProfileRecord describes blocking events originated\n-// at a particular call sequence (stack trace).\n-type BlockProfileRecord struct {\n-\tCount  int64\n-\tCycles int64\n-\tStackRecord\n-}\n-\n-// BlockProfile returns n, the number of records in the current blocking profile.\n-// If len(p) >= n, BlockProfile copies the profile into p and returns n, true.\n-// If len(p) < n, BlockProfile does not change p and returns n, false.\n-//\n-// Most clients should use the runtime/pprof package or\n-// the testing package's -test.blockprofile flag instead\n-// of calling BlockProfile directly.\n-func BlockProfile(p []BlockProfileRecord) (n int, ok bool)\n-\n-// Stack formats a stack trace of the calling goroutine into buf\n-// and returns the number of bytes written to buf.\n-// If all is true, Stack formats stack traces of all other goroutines\n-// into buf after the trace for the current goroutine.\n-func Stack(buf []byte, all bool) int\n-\n // Get field tracking information.  Only fields with a tag go:\"track\"\n // are tracked.  This function will add every such field that is\n // referenced to the map.  The keys in the map will be"}, {"sha": "8d110317003f65174db1faec1ecc7076915b3f6f", "filename": "libgo/go/runtime/mprof.go", "status": "added", "additions": 689, "deletions": 0, "changes": 689, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fgo%2Fruntime%2Fmprof.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fgo%2Fruntime%2Fmprof.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmprof.go?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -0,0 +1,689 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Malloc profiling.\n+// Patterned after tcmalloc's algorithms; shorter code.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"unsafe\"\n+)\n+\n+// Export temporarily for gccgo's C code to call:\n+//go:linkname mProf_Malloc runtime.mProf_Malloc\n+//go:linkname mProf_Free runtime.mProf_Free\n+//go:linkname mProf_GC runtime.mProf_GC\n+//go:linkname tracealloc runtime.tracealloc\n+//go:linkname tracefree runtime.tracefree\n+//go:linkname tracegc runtime.tracegc\n+//go:linkname iterate_memprof runtime.iterate_memprof\n+\n+// NOTE(rsc): Everything here could use cas if contention became an issue.\n+var proflock mutex\n+\n+// All memory allocations are local and do not escape outside of the profiler.\n+// The profiler is forbidden from referring to garbage-collected memory.\n+\n+const (\n+\t// profile types\n+\tmemProfile bucketType = 1 + iota\n+\tblockProfile\n+\n+\t// size of bucket hash table\n+\tbuckHashSize = 179999\n+\n+\t// max depth of stack to record in bucket\n+\tmaxStack = 32\n+)\n+\n+type bucketType int\n+\n+// A bucket holds per-call-stack profiling information.\n+// The representation is a bit sleazy, inherited from C.\n+// This struct defines the bucket header. It is followed in\n+// memory by the stack words and then the actual record\n+// data, either a memRecord or a blockRecord.\n+//\n+// Per-call-stack profiling information.\n+// Lookup by hashing call stack into a linked-list hash table.\n+type bucket struct {\n+\tnext    *bucket\n+\tallnext *bucket\n+\ttyp     bucketType // memBucket or blockBucket\n+\thash    uintptr\n+\tsize    uintptr\n+\tnstk    uintptr\n+}\n+\n+// A memRecord is the bucket data for a bucket of type memProfile,\n+// part of the memory profile.\n+type memRecord struct {\n+\t// The following complex 3-stage scheme of stats accumulation\n+\t// is required to obtain a consistent picture of mallocs and frees\n+\t// for some point in time.\n+\t// The problem is that mallocs come in real time, while frees\n+\t// come only after a GC during concurrent sweeping. So if we would\n+\t// naively count them, we would get a skew toward mallocs.\n+\t//\n+\t// Mallocs are accounted in recent stats.\n+\t// Explicit frees are accounted in recent stats.\n+\t// GC frees are accounted in prev stats.\n+\t// After GC prev stats are added to final stats and\n+\t// recent stats are moved into prev stats.\n+\tallocs      uintptr\n+\tfrees       uintptr\n+\talloc_bytes uintptr\n+\tfree_bytes  uintptr\n+\n+\t// changes between next-to-last GC and last GC\n+\tprev_allocs      uintptr\n+\tprev_frees       uintptr\n+\tprev_alloc_bytes uintptr\n+\tprev_free_bytes  uintptr\n+\n+\t// changes since last GC\n+\trecent_allocs      uintptr\n+\trecent_frees       uintptr\n+\trecent_alloc_bytes uintptr\n+\trecent_free_bytes  uintptr\n+}\n+\n+// A blockRecord is the bucket data for a bucket of type blockProfile,\n+// part of the blocking profile.\n+type blockRecord struct {\n+\tcount  int64\n+\tcycles int64\n+}\n+\n+var (\n+\tmbuckets  *bucket // memory profile buckets\n+\tbbuckets  *bucket // blocking profile buckets\n+\tbuckhash  *[179999]*bucket\n+\tbucketmem uintptr\n+)\n+\n+// newBucket allocates a bucket with the given type and number of stack entries.\n+func newBucket(typ bucketType, nstk int) *bucket {\n+\tsize := unsafe.Sizeof(bucket{}) + uintptr(nstk)*unsafe.Sizeof(location{})\n+\tswitch typ {\n+\tdefault:\n+\t\tthrow(\"invalid profile bucket type\")\n+\tcase memProfile:\n+\t\tsize += unsafe.Sizeof(memRecord{})\n+\tcase blockProfile:\n+\t\tsize += unsafe.Sizeof(blockRecord{})\n+\t}\n+\n+\tb := (*bucket)(persistentalloc(size, 0, &memstats.buckhash_sys))\n+\tbucketmem += size\n+\tb.typ = typ\n+\tb.nstk = uintptr(nstk)\n+\treturn b\n+}\n+\n+// stk returns the slice in b holding the stack.\n+func (b *bucket) stk() []location {\n+\tstk := (*[maxStack]location)(add(unsafe.Pointer(b), unsafe.Sizeof(*b)))\n+\treturn stk[:b.nstk:b.nstk]\n+}\n+\n+// mp returns the memRecord associated with the memProfile bucket b.\n+func (b *bucket) mp() *memRecord {\n+\tif b.typ != memProfile {\n+\t\tthrow(\"bad use of bucket.mp\")\n+\t}\n+\tdata := add(unsafe.Pointer(b), unsafe.Sizeof(*b)+b.nstk*unsafe.Sizeof(location{}))\n+\treturn (*memRecord)(data)\n+}\n+\n+// bp returns the blockRecord associated with the blockProfile bucket b.\n+func (b *bucket) bp() *blockRecord {\n+\tif b.typ != blockProfile {\n+\t\tthrow(\"bad use of bucket.bp\")\n+\t}\n+\tdata := add(unsafe.Pointer(b), unsafe.Sizeof(*b)+b.nstk*unsafe.Sizeof(location{}))\n+\treturn (*blockRecord)(data)\n+}\n+\n+// Return the bucket for stk[0:nstk], allocating new bucket if needed.\n+func stkbucket(typ bucketType, size uintptr, stk []location, alloc bool) *bucket {\n+\tif buckhash == nil {\n+\t\tbuckhash = (*[buckHashSize]*bucket)(sysAlloc(unsafe.Sizeof(*buckhash), &memstats.buckhash_sys))\n+\t\tif buckhash == nil {\n+\t\t\tthrow(\"runtime: cannot allocate memory\")\n+\t\t}\n+\t}\n+\n+\t// Hash stack.\n+\tvar h uintptr\n+\tfor _, loc := range stk {\n+\t\th += loc.pc\n+\t\th += h << 10\n+\t\th ^= h >> 6\n+\t}\n+\t// hash in size\n+\th += size\n+\th += h << 10\n+\th ^= h >> 6\n+\t// finalize\n+\th += h << 3\n+\th ^= h >> 11\n+\n+\ti := int(h % buckHashSize)\n+\tfor b := buckhash[i]; b != nil; b = b.next {\n+\t\tif b.typ == typ && b.hash == h && b.size == size && eqslice(b.stk(), stk) {\n+\t\t\treturn b\n+\t\t}\n+\t}\n+\n+\tif !alloc {\n+\t\treturn nil\n+\t}\n+\n+\t// Create new bucket.\n+\tb := newBucket(typ, len(stk))\n+\tcopy(b.stk(), stk)\n+\tb.hash = h\n+\tb.size = size\n+\tb.next = buckhash[i]\n+\tbuckhash[i] = b\n+\tif typ == memProfile {\n+\t\tb.allnext = mbuckets\n+\t\tmbuckets = b\n+\t} else {\n+\t\tb.allnext = bbuckets\n+\t\tbbuckets = b\n+\t}\n+\treturn b\n+}\n+\n+func eqslice(x, y []location) bool {\n+\tif len(x) != len(y) {\n+\t\treturn false\n+\t}\n+\tfor i, xi := range x {\n+\t\tif xi != y[i] {\n+\t\t\treturn false\n+\t\t}\n+\t}\n+\treturn true\n+}\n+\n+func mprof_GC() {\n+\tfor b := mbuckets; b != nil; b = b.allnext {\n+\t\tmp := b.mp()\n+\t\tmp.allocs += mp.prev_allocs\n+\t\tmp.frees += mp.prev_frees\n+\t\tmp.alloc_bytes += mp.prev_alloc_bytes\n+\t\tmp.free_bytes += mp.prev_free_bytes\n+\n+\t\tmp.prev_allocs = mp.recent_allocs\n+\t\tmp.prev_frees = mp.recent_frees\n+\t\tmp.prev_alloc_bytes = mp.recent_alloc_bytes\n+\t\tmp.prev_free_bytes = mp.recent_free_bytes\n+\n+\t\tmp.recent_allocs = 0\n+\t\tmp.recent_frees = 0\n+\t\tmp.recent_alloc_bytes = 0\n+\t\tmp.recent_free_bytes = 0\n+\t}\n+}\n+\n+// Record that a gc just happened: all the 'recent' statistics are now real.\n+func mProf_GC() {\n+\tlock(&proflock)\n+\tmprof_GC()\n+\tunlock(&proflock)\n+}\n+\n+// Called by malloc to record a profiled block.\n+func mProf_Malloc(p unsafe.Pointer, size uintptr) {\n+\tvar stk [maxStack]location\n+\tnstk := callers(4, stk[:])\n+\tlock(&proflock)\n+\tb := stkbucket(memProfile, size, stk[:nstk], true)\n+\tmp := b.mp()\n+\tmp.recent_allocs++\n+\tmp.recent_alloc_bytes += size\n+\tunlock(&proflock)\n+\n+\t// Setprofilebucket locks a bunch of other mutexes, so we call it outside of proflock.\n+\t// This reduces potential contention and chances of deadlocks.\n+\t// Since the object must be alive during call to mProf_Malloc,\n+\t// it's fine to do this non-atomically.\n+\tsystemstack(func() {\n+\t\tsetprofilebucket(p, b)\n+\t})\n+}\n+\n+// Called when freeing a profiled block.\n+func mProf_Free(b *bucket, size uintptr) {\n+\tlock(&proflock)\n+\tmp := b.mp()\n+\tmp.prev_frees++\n+\tmp.prev_free_bytes += size\n+\tunlock(&proflock)\n+}\n+\n+var blockprofilerate uint64 // in CPU ticks\n+\n+// SetBlockProfileRate controls the fraction of goroutine blocking events\n+// that are reported in the blocking profile. The profiler aims to sample\n+// an average of one blocking event per rate nanoseconds spent blocked.\n+//\n+// To include every blocking event in the profile, pass rate = 1.\n+// To turn off profiling entirely, pass rate <= 0.\n+func SetBlockProfileRate(rate int) {\n+\tvar r int64\n+\tif rate <= 0 {\n+\t\tr = 0 // disable profiling\n+\t} else if rate == 1 {\n+\t\tr = 1 // profile everything\n+\t} else {\n+\t\t// convert ns to cycles, use float64 to prevent overflow during multiplication\n+\t\tr = int64(float64(rate) * float64(tickspersecond()) / (1000 * 1000 * 1000))\n+\t\tif r == 0 {\n+\t\t\tr = 1\n+\t\t}\n+\t}\n+\n+\tatomic.Store64(&blockprofilerate, uint64(r))\n+}\n+\n+func blockevent(cycles int64, skip int) {\n+\tif cycles <= 0 {\n+\t\tcycles = 1\n+\t}\n+\trate := int64(atomic.Load64(&blockprofilerate))\n+\tif rate <= 0 || (rate > cycles && int64(fastrand1())%rate > cycles) {\n+\t\treturn\n+\t}\n+\tgp := getg()\n+\tvar nstk int\n+\tvar stk [maxStack]location\n+\tif gp.m.curg == nil || gp.m.curg == gp {\n+\t\tnstk = callers(skip, stk[:])\n+\t} else {\n+\t\t// FIXME: This should get a traceback of gp.m.curg.\n+\t\t// nstk = gcallers(gp.m.curg, skip, stk[:])\n+\t\tnstk = callers(skip, stk[:])\n+\t}\n+\tlock(&proflock)\n+\tb := stkbucket(blockProfile, 0, stk[:nstk], true)\n+\tb.bp().count++\n+\tb.bp().cycles += cycles\n+\tunlock(&proflock)\n+}\n+\n+// Go interface to profile data.\n+\n+// A StackRecord describes a single execution stack.\n+type StackRecord struct {\n+\tStack0 [32]uintptr // stack trace for this record; ends at first 0 entry\n+}\n+\n+// Stack returns the stack trace associated with the record,\n+// a prefix of r.Stack0.\n+func (r *StackRecord) Stack() []uintptr {\n+\tfor i, v := range r.Stack0 {\n+\t\tif v == 0 {\n+\t\t\treturn r.Stack0[0:i]\n+\t\t}\n+\t}\n+\treturn r.Stack0[0:]\n+}\n+\n+// MemProfileRate controls the fraction of memory allocations\n+// that are recorded and reported in the memory profile.\n+// The profiler aims to sample an average of\n+// one allocation per MemProfileRate bytes allocated.\n+//\n+// To include every allocated block in the profile, set MemProfileRate to 1.\n+// To turn off profiling entirely, set MemProfileRate to 0.\n+//\n+// The tools that process the memory profiles assume that the\n+// profile rate is constant across the lifetime of the program\n+// and equal to the current value. Programs that change the\n+// memory profiling rate should do so just once, as early as\n+// possible in the execution of the program (for example,\n+// at the beginning of main).\n+var MemProfileRate int = 512 * 1024\n+\n+// A MemProfileRecord describes the live objects allocated\n+// by a particular call sequence (stack trace).\n+type MemProfileRecord struct {\n+\tAllocBytes, FreeBytes     int64       // number of bytes allocated, freed\n+\tAllocObjects, FreeObjects int64       // number of objects allocated, freed\n+\tStack0                    [32]uintptr // stack trace for this record; ends at first 0 entry\n+}\n+\n+// InUseBytes returns the number of bytes in use (AllocBytes - FreeBytes).\n+func (r *MemProfileRecord) InUseBytes() int64 { return r.AllocBytes - r.FreeBytes }\n+\n+// InUseObjects returns the number of objects in use (AllocObjects - FreeObjects).\n+func (r *MemProfileRecord) InUseObjects() int64 {\n+\treturn r.AllocObjects - r.FreeObjects\n+}\n+\n+// Stack returns the stack trace associated with the record,\n+// a prefix of r.Stack0.\n+func (r *MemProfileRecord) Stack() []uintptr {\n+\tfor i, v := range r.Stack0 {\n+\t\tif v == 0 {\n+\t\t\treturn r.Stack0[0:i]\n+\t\t}\n+\t}\n+\treturn r.Stack0[0:]\n+}\n+\n+// MemProfile returns a profile of memory allocated and freed per allocation\n+// site.\n+//\n+// MemProfile returns n, the number of records in the current memory profile.\n+// If len(p) >= n, MemProfile copies the profile into p and returns n, true.\n+// If len(p) < n, MemProfile does not change p and returns n, false.\n+//\n+// If inuseZero is true, the profile includes allocation records\n+// where r.AllocBytes > 0 but r.AllocBytes == r.FreeBytes.\n+// These are sites where memory was allocated, but it has all\n+// been released back to the runtime.\n+//\n+// The returned profile may be up to two garbage collection cycles old.\n+// This is to avoid skewing the profile toward allocations; because\n+// allocations happen in real time but frees are delayed until the garbage\n+// collector performs sweeping, the profile only accounts for allocations\n+// that have had a chance to be freed by the garbage collector.\n+//\n+// Most clients should use the runtime/pprof package or\n+// the testing package's -test.memprofile flag instead\n+// of calling MemProfile directly.\n+func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool) {\n+\tlock(&proflock)\n+\tclear := true\n+\tfor b := mbuckets; b != nil; b = b.allnext {\n+\t\tmp := b.mp()\n+\t\tif inuseZero || mp.alloc_bytes != mp.free_bytes {\n+\t\t\tn++\n+\t\t}\n+\t\tif mp.allocs != 0 || mp.frees != 0 {\n+\t\t\tclear = false\n+\t\t}\n+\t}\n+\tif clear {\n+\t\t// Absolutely no data, suggesting that a garbage collection\n+\t\t// has not yet happened. In order to allow profiling when\n+\t\t// garbage collection is disabled from the beginning of execution,\n+\t\t// accumulate stats as if a GC just happened, and recount buckets.\n+\t\tmprof_GC()\n+\t\tmprof_GC()\n+\t\tn = 0\n+\t\tfor b := mbuckets; b != nil; b = b.allnext {\n+\t\t\tmp := b.mp()\n+\t\t\tif inuseZero || mp.alloc_bytes != mp.free_bytes {\n+\t\t\t\tn++\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif n <= len(p) {\n+\t\tok = true\n+\t\tidx := 0\n+\t\tfor b := mbuckets; b != nil; b = b.allnext {\n+\t\t\tmp := b.mp()\n+\t\t\tif inuseZero || mp.alloc_bytes != mp.free_bytes {\n+\t\t\t\trecord(&p[idx], b)\n+\t\t\t\tidx++\n+\t\t\t}\n+\t\t}\n+\t}\n+\tunlock(&proflock)\n+\treturn\n+}\n+\n+// Write b's data to r.\n+func record(r *MemProfileRecord, b *bucket) {\n+\tmp := b.mp()\n+\tr.AllocBytes = int64(mp.alloc_bytes)\n+\tr.FreeBytes = int64(mp.free_bytes)\n+\tr.AllocObjects = int64(mp.allocs)\n+\tr.FreeObjects = int64(mp.frees)\n+\tfor i, loc := range b.stk() {\n+\t\tif i >= len(r.Stack0) {\n+\t\t\tbreak\n+\t\t}\n+\t\tr.Stack0[i] = loc.pc\n+\t}\n+\tfor i := int(b.nstk); i < len(r.Stack0); i++ {\n+\t\tr.Stack0[i] = 0\n+\t}\n+}\n+\n+func iterate_memprof(fn func(*bucket, uintptr, *location, uintptr, uintptr, uintptr)) {\n+\tlock(&proflock)\n+\tfor b := mbuckets; b != nil; b = b.allnext {\n+\t\tmp := b.mp()\n+\t\tfn(b, b.nstk, &b.stk()[0], b.size, mp.allocs, mp.frees)\n+\t}\n+\tunlock(&proflock)\n+}\n+\n+// BlockProfileRecord describes blocking events originated\n+// at a particular call sequence (stack trace).\n+type BlockProfileRecord struct {\n+\tCount  int64\n+\tCycles int64\n+\tStackRecord\n+}\n+\n+// BlockProfile returns n, the number of records in the current blocking profile.\n+// If len(p) >= n, BlockProfile copies the profile into p and returns n, true.\n+// If len(p) < n, BlockProfile does not change p and returns n, false.\n+//\n+// Most clients should use the runtime/pprof package or\n+// the testing package's -test.blockprofile flag instead\n+// of calling BlockProfile directly.\n+func BlockProfile(p []BlockProfileRecord) (n int, ok bool) {\n+\tlock(&proflock)\n+\tfor b := bbuckets; b != nil; b = b.allnext {\n+\t\tn++\n+\t}\n+\tif n <= len(p) {\n+\t\tok = true\n+\t\tfor b := bbuckets; b != nil; b = b.allnext {\n+\t\t\tbp := b.bp()\n+\t\t\tr := &p[0]\n+\t\t\tr.Count = bp.count\n+\t\t\tr.Cycles = bp.cycles\n+\t\t\ti := 0\n+\t\t\tvar loc location\n+\t\t\tfor i, loc = range b.stk() {\n+\t\t\t\tif i >= len(r.Stack0) {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t\tr.Stack0[i] = loc.pc\n+\t\t\t}\n+\t\t\tfor ; i < len(r.Stack0); i++ {\n+\t\t\t\tr.Stack0[i] = 0\n+\t\t\t}\n+\t\t\tp = p[1:]\n+\t\t}\n+\t}\n+\tunlock(&proflock)\n+\treturn\n+}\n+\n+// ThreadCreateProfile returns n, the number of records in the thread creation profile.\n+// If len(p) >= n, ThreadCreateProfile copies the profile into p and returns n, true.\n+// If len(p) < n, ThreadCreateProfile does not change p and returns n, false.\n+//\n+// Most clients should use the runtime/pprof package instead\n+// of calling ThreadCreateProfile directly.\n+func ThreadCreateProfile(p []StackRecord) (n int, ok bool) {\n+\tfirst := (*m)(atomic.Loadp(unsafe.Pointer(allm())))\n+\tfor mp := first; mp != nil; mp = mp.alllink {\n+\t\tn++\n+\t}\n+\tif n <= len(p) {\n+\t\tok = true\n+\t\ti := 0\n+\t\tfor mp := first; mp != nil; mp = mp.alllink {\n+\t\t\tfor j := range mp.createstack {\n+\t\t\t\tp[i].Stack0[j] = mp.createstack[j].pc\n+\t\t\t}\n+\t\t\ti++\n+\t\t}\n+\t}\n+\treturn\n+}\n+\n+// GoroutineProfile returns n, the number of records in the active goroutine stack profile.\n+// If len(p) >= n, GoroutineProfile copies the profile into p and returns n, true.\n+// If len(p) < n, GoroutineProfile does not change p and returns n, false.\n+//\n+// Most clients should use the runtime/pprof package instead\n+// of calling GoroutineProfile directly.\n+func GoroutineProfile(p []StackRecord) (n int, ok bool) {\n+\tgp := getg()\n+\n+\tisOK := func(gp1 *g) bool {\n+\t\t// Checking isSystemGoroutine here makes GoroutineProfile\n+\t\t// consistent with both NumGoroutine and Stack.\n+\t\treturn gp1 != gp && readgstatus(gp1) != _Gdead && !isSystemGoroutine(gp1)\n+\t}\n+\n+\tstopTheWorld(\"profile\")\n+\n+\tn = 1\n+\tfor _, gp1 := range allgs() {\n+\t\tif isOK(gp1) {\n+\t\t\tn++\n+\t\t}\n+\t}\n+\n+\tif n <= len(p) {\n+\t\tok = true\n+\t\tr := p\n+\n+\t\t// Save current goroutine.\n+\t\tsaveg(gp, &r[0])\n+\t\tr = r[1:]\n+\n+\t\t// Save other goroutines.\n+\t\tfor _, gp1 := range allgs() {\n+\t\t\tif isOK(gp1) {\n+\t\t\t\tif len(r) == 0 {\n+\t\t\t\t\t// Should be impossible, but better to return a\n+\t\t\t\t\t// truncated profile than to crash the entire process.\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t\tsaveg(gp1, &r[0])\n+\t\t\t\tr = r[1:]\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tstartTheWorld()\n+\n+\treturn n, ok\n+}\n+\n+func saveg(gp *g, r *StackRecord) {\n+\tif gp == getg() {\n+\t\tvar locbuf [32]location\n+\t\tn := callers(1, locbuf[:])\n+\t\tfor i := 0; i < n; i++ {\n+\t\t\tr.Stack0[i] = locbuf[i].pc\n+\t\t}\n+\t\tif n < len(r.Stack0) {\n+\t\t\tr.Stack0[n] = 0\n+\t\t}\n+\t} else {\n+\t\t// FIXME: Not implemented.\n+\t\tr.Stack0[0] = 0\n+\t}\n+}\n+\n+// Stack formats a stack trace of the calling goroutine into buf\n+// and returns the number of bytes written to buf.\n+// If all is true, Stack formats stack traces of all other goroutines\n+// into buf after the trace for the current goroutine.\n+func Stack(buf []byte, all bool) int {\n+\tif all {\n+\t\tstopTheWorld(\"stack trace\")\n+\t}\n+\n+\tn := 0\n+\tif len(buf) > 0 {\n+\t\tgp := getg()\n+\t\t// Force traceback=1 to override GOTRACEBACK setting,\n+\t\t// so that Stack's results are consistent.\n+\t\t// GOTRACEBACK is only about crash dumps.\n+\t\tgp.m.traceback = 1\n+\t\tgp.writebuf = buf[0:0:len(buf)]\n+\t\tgoroutineheader(gp)\n+\t\ttraceback()\n+\t\tif all {\n+\t\t\ttracebackothers(gp)\n+\t\t}\n+\t\tgp.m.traceback = 0\n+\t\tn = len(gp.writebuf)\n+\t\tgp.writebuf = nil\n+\t}\n+\n+\tif all {\n+\t\tstartTheWorld()\n+\t}\n+\treturn n\n+}\n+\n+// Tracing of alloc/free/gc.\n+\n+var tracelock mutex\n+\n+func tracealloc(p unsafe.Pointer, size uintptr, typ *_type) {\n+\tlock(&tracelock)\n+\tgp := getg()\n+\tgp.m.traceback = 2\n+\tif typ == nil {\n+\t\tprint(\"tracealloc(\", p, \", \", hex(size), \")\\n\")\n+\t} else {\n+\t\tprint(\"tracealloc(\", p, \", \", hex(size), \", \", *typ.string, \")\\n\")\n+\t}\n+\tif gp.m.curg == nil || gp == gp.m.curg {\n+\t\tgoroutineheader(gp)\n+\t\ttraceback()\n+\t} else {\n+\t\tgoroutineheader(gp.m.curg)\n+\t\t// FIXME: Can't do traceback of other g.\n+\t}\n+\tprint(\"\\n\")\n+\tgp.m.traceback = 0\n+\tunlock(&tracelock)\n+}\n+\n+func tracefree(p unsafe.Pointer, size uintptr) {\n+\tlock(&tracelock)\n+\tgp := getg()\n+\tgp.m.traceback = 2\n+\tprint(\"tracefree(\", p, \", \", hex(size), \")\\n\")\n+\tgoroutineheader(gp)\n+\ttraceback()\n+\tprint(\"\\n\")\n+\tgp.m.traceback = 0\n+\tunlock(&tracelock)\n+}\n+\n+func tracegc() {\n+\tlock(&tracelock)\n+\tgp := getg()\n+\tgp.m.traceback = 2\n+\tprint(\"tracegc()\\n\")\n+\t// running on m->g0 stack; show all non-g0 goroutines\n+\ttracebackothers(gp)\n+\tprint(\"end tracegc\\n\")\n+\tprint(\"\\n\")\n+\tgp.m.traceback = 0\n+\tunlock(&tracelock)\n+}"}, {"sha": "660d81e47f344b0d0300ebb84c0efa9c9287d4a9", "filename": "libgo/go/runtime/runtime2.go", "status": "modified", "additions": 1, "deletions": 16, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fgo%2Fruntime%2Fruntime2.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fgo%2Fruntime%2Fruntime2.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime2.go?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -394,7 +394,7 @@ type g struct {\n \tissystem     bool // do not output in stack dump\n \tisbackground bool // ignore in deadlock detector\n \n-\ttraceback *traceback // stack traceback buffer\n+\ttraceback *tracebackg // stack traceback buffer\n \n \tcontext      g_ucontext_t       // saved context for setcontext\n \tstackcontext [10]unsafe.Pointer // split-stack context\n@@ -801,21 +801,6 @@ var (\n // array.\n type g_ucontext_t [(_sizeof_ucontext_t + 15) / unsafe.Sizeof(unsafe.Pointer(nil))]unsafe.Pointer\n \n-// traceback is used to collect stack traces from other goroutines.\n-type traceback struct {\n-\tgp     *g\n-\tlocbuf [_TracebackMaxFrames]location\n-\tc      int\n-}\n-\n-// location is a location in the program, used for backtraces.\n-type location struct {\n-\tpc       uintptr\n-\tfilename string\n-\tfunction string\n-\tlineno   int\n-}\n-\n // cgoMal tracks allocations made by _cgo_allocate\n // FIXME: _cgo_allocate has been removed from gc and can probably be\n // removed from gccgo too."}, {"sha": "e1c28fac5432d69f76fb47e21db9488a71e33185", "filename": "libgo/go/runtime/stubs.go", "status": "modified", "additions": 21, "deletions": 5, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fgo%2Fruntime%2Fstubs.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fgo%2Fruntime%2Fstubs.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstubs.go?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -307,11 +307,6 @@ func gopark(func(*g, unsafe.Pointer) bool, unsafe.Pointer, string, byte, int)\n func goparkunlock(*mutex, string, byte, int)\n func goready(*g, int)\n \n-// Temporary for gccgo until we port mprof.go.\n-var blockprofilerate uint64\n-\n-func blockevent(cycles int64, skip int) {}\n-\n // Temporary hack for gccgo until we port proc.go.\n //go:nosplit\n func acquireSudog() *sudog {\n@@ -428,3 +423,24 @@ func sysAlloc(n uintptr, sysStat *uint64) unsafe.Pointer\n func cpuprofAdd(stk []uintptr) {\n \tcpuprof.add(stk)\n }\n+\n+// For gccgo until we port proc.go.\n+func Breakpoint()\n+func LockOSThread()\n+func UnlockOSThread()\n+func allm() *m\n+func allgs() []*g\n+\n+//go:nosplit\n+func readgstatus(gp *g) uint32 {\n+\treturn atomic.Load(&gp.atomicstatus)\n+}\n+\n+// Temporary for gccgo until we port malloc.go\n+func persistentalloc(size, align uintptr, sysStat *uint64) unsafe.Pointer\n+\n+// Temporary for gccgo until we port mheap.go\n+func setprofilebucket(p unsafe.Pointer, b *bucket)\n+\n+// Currently in proc.c.\n+func tracebackothers(*g)"}, {"sha": "4f3d7c039bec2e8656bd21b68612eeda20a15a66", "filename": "libgo/go/runtime/traceback_gccgo.go", "status": "added", "additions": 164, "deletions": 0, "changes": 164, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -0,0 +1,164 @@\n+// Copyright 2016 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Traceback support for gccgo.\n+// The actual traceback code is written in C.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/sys\"\n+\t_ \"unsafe\" // for go:linkname\n+)\n+\n+// For gccgo, use go:linkname to rename compiler-called functions to\n+// themselves, so that the compiler will export them.\n+// These are temporary for C runtime code to call.\n+//go:linkname traceback runtime.traceback\n+//go:linkname printtrace runtime.printtrace\n+//go:linkname goroutineheader runtime.goroutineheader\n+//go:linkname printcreatedby runtime.printcreatedby\n+\n+func printcreatedby(gp *g) {\n+\t// Show what created goroutine, except main goroutine (goid 1).\n+\tpc := gp.gopc\n+\ttracepc := pc // back up to CALL instruction for funcfileline.\n+\tentry := funcentry(tracepc)\n+\tif entry != 0 && tracepc > entry {\n+\t\ttracepc -= sys.PCQuantum\n+\t}\n+\tfunction, file, line := funcfileline(tracepc, -1)\n+\tif function != \"\" && showframe(function, gp) && gp.goid != 1 {\n+\t\tprint(\"created by \", function, \"\\n\")\n+\t\tprint(\"\\t\", file, \":\", line)\n+\t\tif entry != 0 && pc > entry {\n+\t\t\tprint(\" +\", hex(pc-entry))\n+\t\t}\n+\t\tprint(\"\\n\")\n+\t}\n+}\n+\n+// tracebackg is used to collect stack traces from other goroutines.\n+type tracebackg struct {\n+\tgp     *g\n+\tlocbuf [_TracebackMaxFrames]location\n+\tc      int\n+}\n+\n+// location is a location in the program, used for backtraces.\n+type location struct {\n+\tpc       uintptr\n+\tfilename string\n+\tfunction string\n+\tlineno   int\n+}\n+\n+//extern runtime_callers\n+func c_callers(skip int32, locbuf *location, max int32, keepThunks bool) int32\n+\n+// callers returns a stack trace of the current goroutine.\n+// The gc version of callers takes []uintptr, but we take []location.\n+func callers(skip int, locbuf []location) int {\n+\tn := c_callers(int32(skip), &locbuf[0], int32(len(locbuf)), false)\n+\treturn int(n)\n+}\n+\n+// traceback prints a traceback of the current goroutine.\n+// This differs from the gc version, which is given pc, sp, lr and g and\n+// can print a traceback of any goroutine.\n+func traceback() {\n+\tvar locbuf [100]location\n+\tc := c_callers(1, &locbuf[0], int32(len(locbuf)), false)\n+\tprinttrace(locbuf[:c], getg())\n+}\n+\n+// printtrace prints a traceback from locbuf.\n+func printtrace(locbuf []location, gp *g) {\n+\tfor i := range locbuf {\n+\t\tif showframe(locbuf[i].function, gp) {\n+\t\t\tprint(locbuf[i].function, \"\\n\\t\", locbuf[i].filename, \":\", locbuf[i].lineno)\n+\t\t}\n+\t}\n+}\n+\n+// showframe returns whether to print a frame in a traceback.\n+// name is the function name.\n+func showframe(name string, gp *g) bool {\n+\tg := getg()\n+\tif g.m.throwing > 0 && gp != nil && (gp == g.m.curg || gp == g.m.caughtsig.ptr()) {\n+\t\treturn true\n+\t}\n+\tlevel, _, _ := gotraceback()\n+\n+\t// Special case: always show runtime.gopanic frame, so that we can\n+\t// see where a panic started in the middle of a stack trace.\n+\t// See golang.org/issue/5832.\n+\t// __go_panic is the current gccgo name.\n+\tif name == \"runtime.gopanic\" || name == \"__go_panic\" {\n+\t\treturn true\n+\t}\n+\n+\treturn level > 1 || contains(name, \".\") && (!hasprefix(name, \"runtime.\") || isExportedRuntime(name))\n+}\n+\n+// isExportedRuntime reports whether name is an exported runtime function.\n+// It is only for runtime functions, so ASCII A-Z is fine.\n+func isExportedRuntime(name string) bool {\n+\tconst n = len(\"runtime.\")\n+\treturn len(name) > n && name[:n] == \"runtime.\" && 'A' <= name[n] && name[n] <= 'Z'\n+}\n+\n+var gStatusStrings = [...]string{\n+\t_Gidle:      \"idle\",\n+\t_Grunnable:  \"runnable\",\n+\t_Grunning:   \"running\",\n+\t_Gsyscall:   \"syscall\",\n+\t_Gwaiting:   \"waiting\",\n+\t_Gdead:      \"dead\",\n+\t_Gcopystack: \"copystack\",\n+}\n+\n+func goroutineheader(gp *g) {\n+\tgpstatus := readgstatus(gp)\n+\n+\tisScan := gpstatus&_Gscan != 0\n+\tgpstatus &^= _Gscan // drop the scan bit\n+\n+\t// Basic string status\n+\tvar status string\n+\tif 0 <= gpstatus && gpstatus < uint32(len(gStatusStrings)) {\n+\t\tstatus = gStatusStrings[gpstatus]\n+\t} else {\n+\t\tstatus = \"???\"\n+\t}\n+\n+\t// Override.\n+\tif gpstatus == _Gwaiting && gp.waitreason != \"\" {\n+\t\tstatus = gp.waitreason\n+\t}\n+\n+\t// approx time the G is blocked, in minutes\n+\tvar waitfor int64\n+\tif (gpstatus == _Gwaiting || gpstatus == _Gsyscall) && gp.waitsince != 0 {\n+\t\twaitfor = (nanotime() - gp.waitsince) / 60e9\n+\t}\n+\tprint(\"goroutine \", gp.goid, \" [\", status)\n+\tif isScan {\n+\t\tprint(\" (scan)\")\n+\t}\n+\tif waitfor >= 1 {\n+\t\tprint(\", \", waitfor, \" minutes\")\n+\t}\n+\tif gp.lockedm != nil {\n+\t\tprint(\", locked to thread\")\n+\t}\n+\tprint(\"]:\\n\")\n+}\n+\n+// isSystemGoroutine reports whether the goroutine g must be omitted in\n+// stack dumps and deadlock detector.\n+func isSystemGoroutine(gp *g) bool {\n+\t// FIXME.\n+\treturn false\n+}"}, {"sha": "7b33cca86815e39a2e8b568b8ecfb3d4bebd5d43", "filename": "libgo/runtime/go-traceback.c", "status": "removed", "additions": 0, "deletions": 37, "changes": 37, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2045acd902fd8028514a72c58c98dba11749b8ad/libgo%2Fruntime%2Fgo-traceback.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2045acd902fd8028514a72c58c98dba11749b8ad/libgo%2Fruntime%2Fgo-traceback.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-traceback.c?ref=2045acd902fd8028514a72c58c98dba11749b8ad", "patch": "@@ -1,37 +0,0 @@\n-/* go-traceback.c -- stack backtrace for Go.\n-\n-   Copyright 2012 The Go Authors. All rights reserved.\n-   Use of this source code is governed by a BSD-style\n-   license that can be found in the LICENSE file.  */\n-\n-#include \"config.h\"\n-\n-#include \"runtime.h\"\n-\n-/* Print a stack trace for the current goroutine.  */\n-\n-void\n-runtime_traceback ()\n-{\n-  Location locbuf[100];\n-  int32 c;\n-\n-  c = runtime_callers (1, locbuf, nelem (locbuf), false);\n-  runtime_printtrace (locbuf, c, true);\n-}\n-\n-void\n-runtime_printtrace (Location *locbuf, int32 c, bool current)\n-{\n-  int32 i;\n-\n-  for (i = 0; i < c; ++i)\n-    {\n-      if (runtime_showframe (locbuf[i].function, current))\n-\t{\n-\t  runtime_printf (\"%S\\n\", locbuf[i].function);\n-\t  runtime_printf (\"\\t%S:%D\\n\", locbuf[i].filename,\n-\t\t\t  (int64) locbuf[i].lineno);\n-\t}\n-    }\n-}"}, {"sha": "011eaa91d0705a99748987d9f6595cdcab9a9d01", "filename": "libgo/runtime/malloc.h", "status": "modified", "additions": 21, "deletions": 13, "changes": 34, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fruntime%2Fmalloc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fruntime%2Fmalloc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmalloc.h?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -303,7 +303,7 @@ struct SpecialFinalizer\n };\n \n // The described object is being heap profiled.\n-typedef struct Bucket Bucket; // from mprof.goc\n+typedef struct bucket Bucket; // from mprof.go\n typedef struct SpecialProfile SpecialProfile;\n struct SpecialProfile\n {\n@@ -414,7 +414,8 @@ void\truntime_MHeap_Scavenger(void*);\n void\truntime_MHeap_SplitSpan(MHeap *h, MSpan *s);\n \n void*\truntime_mallocgc(uintptr size, uintptr typ, uint32 flag);\n-void*\truntime_persistentalloc(uintptr size, uintptr align, uint64 *stat);\n+void*\truntime_persistentalloc(uintptr size, uintptr align, uint64 *stat)\n+  __asm__(GOSYM_PREFIX \"runtime.persistentalloc\");\n int32\truntime_mlookup(void *v, byte **base, uintptr *size, MSpan **s);\n void\truntime_gc(int32 force);\n uintptr\truntime_sweepone(void);\n@@ -428,12 +429,15 @@ void\truntime_markspan(void *v, uintptr size, uintptr n, bool leftover);\n void\truntime_unmarkspan(void *v, uintptr size);\n void\truntime_purgecachedstats(MCache*);\n void*\truntime_cnew(const Type*)\n-\t  __asm__(GOSYM_PREFIX \"runtime.newobject\");\n+  __asm__(GOSYM_PREFIX \"runtime.newobject\");\n void*\truntime_cnewarray(const Type*, intgo)\n-\t  __asm__(GOSYM_PREFIX \"runtime.newarray\");\n-void\truntime_tracealloc(void*, uintptr, uintptr);\n-void\truntime_tracefree(void*, uintptr);\n-void\truntime_tracegc(void);\n+  __asm__(GOSYM_PREFIX \"runtime.newarray\");\n+void\truntime_tracealloc(void*, uintptr, uintptr)\n+  __asm__ (GOSYM_PREFIX \"runtime.tracealloc\");\n+void\truntime_tracefree(void*, uintptr)\n+  __asm__ (GOSYM_PREFIX \"runtime.tracefree\");\n+void\truntime_tracegc(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.tracegc\");\n \n uintptr\truntime_gettype(void*);\n \n@@ -455,10 +459,14 @@ struct Obj\n \tuintptr\tti;\t// type info\n };\n \n-void\truntime_MProf_Malloc(void*, uintptr);\n-void\truntime_MProf_Free(Bucket*, uintptr, bool);\n-void\truntime_MProf_GC(void);\n-void\truntime_iterate_memprof(void (*callback)(Bucket*, uintptr, Location*, uintptr, uintptr, uintptr));\n+void\truntime_MProf_Malloc(void*, uintptr)\n+  __asm__ (GOSYM_PREFIX \"runtime.mProf_Malloc\");\n+void\truntime_MProf_Free(Bucket*, uintptr, bool)\n+  __asm__ (GOSYM_PREFIX \"runtime.mProf_Free\");\n+void\truntime_MProf_GC(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.mProf_GC\");\n+void\truntime_iterate_memprof(void (*callback)(Bucket*, uintptr, Location*, uintptr, uintptr, uintptr))\n+  __asm__ (GOSYM_PREFIX \"runtime.iterate_memprof\");\n int32\truntime_gcprocs(void);\n void\truntime_helpgc(int32 nproc);\n void\truntime_gchelper(void);\n@@ -467,7 +475,8 @@ G*\truntime_wakefing(void);\n extern bool\truntime_fingwait;\n extern bool\truntime_fingwake;\n \n-void\truntime_setprofilebucket(void *p, Bucket *b);\n+void\truntime_setprofilebucket(void *p, Bucket *b)\n+  __asm__ (GOSYM_PREFIX \"runtime.setprofilebucket\");\n \n struct __go_func_type;\n struct __go_ptr_type;\n@@ -533,7 +542,6 @@ int32\truntime_setgcpercent(int32);\n #define PoisonStack ((uintptr)0x6868686868686868ULL)\n \n struct Workbuf;\n-void\truntime_MProf_Mark(struct Workbuf**, void (*)(struct Workbuf**, Obj));\n void\truntime_proc_scan(struct Workbuf**, void (*)(struct Workbuf**, Obj));\n void\truntime_time_scan(struct Workbuf**, void (*)(struct Workbuf**, Obj));\n void\truntime_netpoll_scan(struct Workbuf**, void (*)(struct Workbuf**, Obj));"}, {"sha": "2c8e5a8e643c97bbdb6b7a290329781f3542d325", "filename": "libgo/runtime/mgc0.c", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fruntime%2Fmgc0.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fruntime%2Fmgc0.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmgc0.c?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -1277,7 +1277,6 @@ markroot(ParFor *desc, uint32 i)\n \t\tenqueue1(&wbuf, (Obj){(byte*)&runtime_allp, sizeof runtime_allp, 0});\n \t\tenqueue1(&wbuf, (Obj){(byte*)&work, sizeof work, 0});\n \t\truntime_proc_scan(&wbuf, enqueue1);\n-\t\truntime_MProf_Mark(&wbuf, enqueue1);\n \t\truntime_time_scan(&wbuf, enqueue1);\n \t\truntime_netpoll_scan(&wbuf, enqueue1);\n \t\tbreak;"}, {"sha": "c4966a4ba81ee481b0162e4ca6223382972ff094", "filename": "libgo/runtime/mprof.goc", "status": "removed", "additions": 0, "deletions": 564, "changes": 564, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2045acd902fd8028514a72c58c98dba11749b8ad/libgo%2Fruntime%2Fmprof.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2045acd902fd8028514a72c58c98dba11749b8ad/libgo%2Fruntime%2Fmprof.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmprof.goc?ref=2045acd902fd8028514a72c58c98dba11749b8ad", "patch": "@@ -1,564 +0,0 @@\n-// Copyright 2009 The Go Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style\n-// license that can be found in the LICENSE file.\n-\n-// Malloc profiling.\n-// Patterned after tcmalloc's algorithms; shorter code.\n-\n-package runtime\n-#include \"runtime.h\"\n-#include \"arch.h\"\n-#include \"malloc.h\"\n-#include \"defs.h\"\n-#include \"go-type.h\"\n-#include \"go-string.h\"\n-\n-// NOTE(rsc): Everything here could use cas if contention became an issue.\n-static Lock proflock;\n-\n-// All memory allocations are local and do not escape outside of the profiler.\n-// The profiler is forbidden from referring to garbage-collected memory.\n-\n-enum { MProf, BProf };  // profile types\n-\n-// Per-call-stack profiling information.\n-// Lookup by hashing call stack into a linked-list hash table.\n-struct Bucket\n-{\n-\tBucket\t*next;\t// next in hash list\n-\tBucket\t*allnext;\t// next in list of all mbuckets/bbuckets\n-\tint32\ttyp;\n-\t// Generally unions can break precise GC,\n-\t// this one is fine because it does not contain pointers.\n-\tunion\n-\t{\n-\t\tstruct  // typ == MProf\n-\t\t{\n-\t\t\t// The following complex 3-stage scheme of stats accumulation\n-\t\t\t// is required to obtain a consistent picture of mallocs and frees\n-\t\t\t// for some point in time.\n-\t\t\t// The problem is that mallocs come in real time, while frees\n-\t\t\t// come only after a GC during concurrent sweeping. So if we would\n-\t\t\t// naively count them, we would get a skew toward mallocs.\n-\t\t\t//\n-\t\t\t// Mallocs are accounted in recent stats.\n-\t\t\t// Explicit frees are accounted in recent stats.\n-\t\t\t// GC frees are accounted in prev stats.\n-\t\t\t// After GC prev stats are added to final stats and\n-\t\t\t// recent stats are moved into prev stats.\n-\t\t\tuintptr\tallocs;\n-\t\t\tuintptr\tfrees;\n-\t\t\tuintptr\talloc_bytes;\n-\t\t\tuintptr\tfree_bytes;\n-\n-\t\t\tuintptr\tprev_allocs;  // since last but one till last gc\n-\t\t\tuintptr\tprev_frees;\n-\t\t\tuintptr\tprev_alloc_bytes;\n-\t\t\tuintptr\tprev_free_bytes;\n-\n-\t\t\tuintptr\trecent_allocs;  // since last gc till now\n-\t\t\tuintptr\trecent_frees;\n-\t\t\tuintptr\trecent_alloc_bytes;\n-\t\t\tuintptr\trecent_free_bytes;\n-\n-\t\t};\n-\t\tstruct  // typ == BProf\n-\t\t{\n-\t\t\tint64\tcount;\n-\t\t\tint64\tcycles;\n-\t\t};\n-\t};\n-\tuintptr\thash;\t// hash of size + stk\n-\tuintptr\tsize;\n-\tuintptr\tnstk;\n-\tLocation stk[1];\n-};\n-enum {\n-\tBuckHashSize = 179999,\n-};\n-static Bucket **buckhash;\n-static Bucket *mbuckets;  // memory profile buckets\n-static Bucket *bbuckets;  // blocking profile buckets\n-static uintptr bucketmem;\n-\n-// Return the bucket for stk[0:nstk], allocating new bucket if needed.\n-static Bucket*\n-stkbucket(int32 typ, uintptr size, Location *stk, int32 nstk, bool alloc)\n-{\n-\tint32 i, j;\n-\tuintptr h;\n-\tBucket *b;\n-\n-\tif(buckhash == nil) {\n-\t\tbuckhash = runtime_SysAlloc(BuckHashSize*sizeof buckhash[0], &mstats()->buckhash_sys);\n-\t\tif(buckhash == nil)\n-\t\t\truntime_throw(\"runtime: cannot allocate memory\");\n-\t}\n-\n-\t// Hash stack.\n-\th = 0;\n-\tfor(i=0; i<nstk; i++) {\n-\t\th += stk[i].pc;\n-\t\th += h<<10;\n-\t\th ^= h>>6;\n-\t}\n-\t// hash in size\n-\th += size;\n-\th += h<<10;\n-\th ^= h>>6;\n-\t// finalize\n-\th += h<<3;\n-\th ^= h>>11;\n-\n-\ti = h%BuckHashSize;\n-\tfor(b = buckhash[i]; b; b=b->next) {\n-\t\tif(b->typ == typ && b->hash == h && b->size == size && b->nstk == (uintptr)nstk) {\n-\t\t\tfor(j = 0; j < nstk; j++) {\n-\t\t\t\tif(b->stk[j].pc != stk[j].pc ||\n-\t\t\t\t   b->stk[j].lineno != stk[j].lineno ||\n-\t\t\t\t   !__go_strings_equal(b->stk[j].filename, stk[j].filename))\n-\t\t\t\t\tbreak;\n-\t\t\t}\n-\t\t\tif (j == nstk)\n-\t\t\t\treturn b;\n-\t\t}\n-\t}\n-\n-\tif(!alloc)\n-\t\treturn nil;\n-\n-\tb = runtime_persistentalloc(sizeof *b + nstk*sizeof stk[0], 0, &mstats()->buckhash_sys);\n-\tbucketmem += sizeof *b + nstk*sizeof stk[0];\n-\truntime_memmove(b->stk, stk, nstk*sizeof stk[0]);\n-\tb->typ = typ;\n-\tb->hash = h;\n-\tb->size = size;\n-\tb->nstk = nstk;\n-\tb->next = buckhash[i];\n-\tbuckhash[i] = b;\n-\tif(typ == MProf) {\n-\t\tb->allnext = mbuckets;\n-\t\tmbuckets = b;\n-\t} else {\n-\t\tb->allnext = bbuckets;\n-\t\tbbuckets = b;\n-\t}\n-\treturn b;\n-}\n-\n-static void\n-MProf_GC(void)\n-{\n-\tBucket *b;\n-\n-\tfor(b=mbuckets; b; b=b->allnext) {\n-\t\tb->allocs += b->prev_allocs;\n-\t\tb->frees += b->prev_frees;\n-\t\tb->alloc_bytes += b->prev_alloc_bytes;\n-\t\tb->free_bytes += b->prev_free_bytes;\n-\n-\t\tb->prev_allocs = b->recent_allocs;\n-\t\tb->prev_frees = b->recent_frees;\n-\t\tb->prev_alloc_bytes = b->recent_alloc_bytes;\n-\t\tb->prev_free_bytes = b->recent_free_bytes;\n-\n-\t\tb->recent_allocs = 0;\n-\t\tb->recent_frees = 0;\n-\t\tb->recent_alloc_bytes = 0;\n-\t\tb->recent_free_bytes = 0;\n-\t}\n-}\n-\n-// Record that a gc just happened: all the 'recent' statistics are now real.\n-void\n-runtime_MProf_GC(void)\n-{\n-\truntime_lock(&proflock);\n-\tMProf_GC();\n-\truntime_unlock(&proflock);\n-}\n-\n-// Called by malloc to record a profiled block.\n-void\n-runtime_MProf_Malloc(void *p, uintptr size)\n-{\n-\tLocation stk[32];\n-\tBucket *b;\n-\tint32 nstk;\n-\n-\tnstk = runtime_callers(1, stk, nelem(stk), false);\n-\truntime_lock(&proflock);\n-\tb = stkbucket(MProf, size, stk, nstk, true);\n-\tb->recent_allocs++;\n-\tb->recent_alloc_bytes += size;\n-\truntime_unlock(&proflock);\n-\n-\t// Setprofilebucket locks a bunch of other mutexes, so we call it outside of proflock.\n-\t// This reduces potential contention and chances of deadlocks.\n-\t// Since the object must be alive during call to MProf_Malloc,\n-\t// it's fine to do this non-atomically.\n-\truntime_setprofilebucket(p, b);\n-}\n-\n-// Called when freeing a profiled block.\n-void\n-runtime_MProf_Free(Bucket *b, uintptr size, bool freed)\n-{\n-\truntime_lock(&proflock);\n-\tif(freed) {\n-\t\tb->recent_frees++;\n-\t\tb->recent_free_bytes += size;\n-\t} else {\n-\t\tb->prev_frees++;\n-\t\tb->prev_free_bytes += size;\n-\t}\n-\truntime_unlock(&proflock);\n-}\n-\n-int64 runtime_blockprofilerate;  // in CPU ticks\n-\n-void runtime_SetBlockProfileRate(intgo) __asm__ (GOSYM_PREFIX \"runtime.SetBlockProfileRate\");\n-\n-void\n-runtime_SetBlockProfileRate(intgo rate)\n-{\n-\tint64 r;\n-\n-\tif(rate <= 0)\n-\t\tr = 0;  // disable profiling\n-\telse {\n-\t\t// convert ns to cycles, use float64 to prevent overflow during multiplication\n-\t\tr = (float64)rate*runtime_tickspersecond()/(1000*1000*1000);\n-\t\tif(r == 0)\n-\t\t\tr = 1;\n-\t}\n-\truntime_atomicstore64((uint64*)&runtime_blockprofilerate, r);\n-}\n-\n-void\n-runtime_blockevent(int64 cycles, int32 skip)\n-{\n-\tint32 nstk;\n-\tint64 rate;\n-\tLocation stk[32];\n-\tBucket *b;\n-\n-\tif(cycles <= 0)\n-\t\treturn;\n-\trate = runtime_atomicload64((uint64*)&runtime_blockprofilerate);\n-\tif(rate <= 0 || (rate > cycles && runtime_fastrand1()%rate > cycles))\n-\t\treturn;\n-\n-\tnstk = runtime_callers(skip, stk, nelem(stk), false);\n-\truntime_lock(&proflock);\n-\tb = stkbucket(BProf, 0, stk, nstk, true);\n-\tb->count++;\n-\tb->cycles += cycles;\n-\truntime_unlock(&proflock);\n-}\n-\n-// Go interface to profile data.  (Declared in debug.go)\n-\n-// Must match MemProfileRecord in debug.go.\n-typedef struct Record Record;\n-struct Record {\n-\tint64 alloc_bytes, free_bytes;\n-\tint64 alloc_objects, free_objects;\n-\tuintptr stk[32];\n-};\n-\n-// Write b's data to r.\n-static void\n-record(Record *r, Bucket *b)\n-{\n-\tuint32 i;\n-\n-\tr->alloc_bytes = b->alloc_bytes;\n-\tr->free_bytes = b->free_bytes;\n-\tr->alloc_objects = b->allocs;\n-\tr->free_objects = b->frees;\n-\tfor(i=0; i<b->nstk && i<nelem(r->stk); i++)\n-\t\tr->stk[i] = b->stk[i].pc;\n-\tfor(; i<nelem(r->stk); i++)\n-\t\tr->stk[i] = 0;\n-}\n-\n-func MemProfile(p Slice, include_inuse_zero bool) (n int, ok bool) {\n-\tBucket *b;\n-\tRecord *r;\n-\tbool clear;\n-\n-\truntime_lock(&proflock);\n-\tn = 0;\n-\tclear = true;\n-\tfor(b=mbuckets; b; b=b->allnext) {\n-\t\tif(include_inuse_zero || b->alloc_bytes != b->free_bytes)\n-\t\t\tn++;\n-\t\tif(b->allocs != 0 || b->frees != 0)\n-\t\t\tclear = false;\n-\t}\n-\tif(clear) {\n-\t\t// Absolutely no data, suggesting that a garbage collection\n-\t\t// has not yet happened. In order to allow profiling when\n-\t\t// garbage collection is disabled from the beginning of execution,\n-\t\t// accumulate stats as if a GC just happened, and recount buckets.\n-\t\tMProf_GC();\n-\t\tMProf_GC();\n-\t\tn = 0;\n-\t\tfor(b=mbuckets; b; b=b->allnext)\n-\t\t\tif(include_inuse_zero || b->alloc_bytes != b->free_bytes)\n-\t\t\t\tn++;\n-\t}\n-\tok = false;\n-\tif(n <= p.__count) {\n-\t\tok = true;\n-\t\tr = (Record*)p.__values;\n-\t\tfor(b=mbuckets; b; b=b->allnext)\n-\t\t\tif(include_inuse_zero || b->alloc_bytes != b->free_bytes)\n-\t\t\t\trecord(r++, b);\n-\t}\n-\truntime_unlock(&proflock);\n-}\n-\n-void\n-runtime_MProf_Mark(struct Workbuf **wbufp, void (*enqueue1)(struct Workbuf**, Obj))\n-{\n-\t// buckhash is not allocated via mallocgc.\n-\tenqueue1(wbufp, (Obj){(byte*)&mbuckets, sizeof mbuckets, 0});\n-\tenqueue1(wbufp, (Obj){(byte*)&bbuckets, sizeof bbuckets, 0});\n-}\n-\n-void\n-runtime_iterate_memprof(void (*callback)(Bucket*, uintptr, Location*, uintptr, uintptr, uintptr))\n-{\n-\tBucket *b;\n-\n-\truntime_lock(&proflock);\n-\tfor(b=mbuckets; b; b=b->allnext) {\n-\t\tcallback(b, b->nstk, b->stk, b->size, b->allocs, b->frees);\n-\t}\n-\truntime_unlock(&proflock);\n-}\n-\n-// Must match BlockProfileRecord in debug.go.\n-typedef struct BRecord BRecord;\n-struct BRecord {\n-\tint64 count;\n-\tint64 cycles;\n-\tuintptr stk[32];\n-};\n-\n-func BlockProfile(p Slice) (n int, ok bool) {\n-\tBucket *b;\n-\tBRecord *r;\n-\tint32 i;\n-\n-\truntime_lock(&proflock);\n-\tn = 0;\n-\tfor(b=bbuckets; b; b=b->allnext)\n-\t\tn++;\n-\tok = false;\n-\tif(n <= p.__count) {\n-\t\tok = true;\n-\t\tr = (BRecord*)p.__values;\n-\t\tfor(b=bbuckets; b; b=b->allnext, r++) {\n-\t\t\tr->count = b->count;\n-\t\t\tr->cycles = b->cycles;\n-\t\t\tfor(i=0; (uintptr)i<b->nstk && (uintptr)i<nelem(r->stk); i++)\n-\t\t\t\tr->stk[i] = b->stk[i].pc;\n-\t\t\tfor(; (uintptr)i<nelem(r->stk); i++)\n-\t\t\t\tr->stk[i] = 0;\t\t\t\n-\t\t}\n-\t}\n-\truntime_unlock(&proflock);\n-}\n-\n-// Must match StackRecord in debug.go.\n-typedef struct TRecord TRecord;\n-struct TRecord {\n-\tuintptr stk[32];\n-};\n-\n-func ThreadCreateProfile(p Slice) (n int, ok bool) {\n-\tTRecord *r;\n-\tM *first, *mp;\n-\tint32 i;\n-\t\n-\tfirst = runtime_atomicloadp(&runtime_allm);\n-\tn = 0;\n-\tfor(mp=first; mp; mp=mp->alllink)\n-\t\tn++;\n-\tok = false;\n-\tif(n <= p.__count) {\n-\t\tok = true;\n-\t\tr = (TRecord*)p.__values;\n-\t\tfor(mp=first; mp; mp=mp->alllink) {\n-\t\t\tfor(i = 0; (uintptr)i < nelem(r->stk); i++) {\n-\t\t\t\tr->stk[i] = mp->createstack[i].pc;\n-\t\t\t}\n-\t\t\tr++;\n-\t\t}\n-\t}\n-}\n-\n-func Stack(b Slice, all bool) (n int) {\n-\tbyte *pc;\n-\tbool enablegc = false;\n-\t\n-\tpc = (byte*)(uintptr)runtime_getcallerpc(&b);\n-\n-\tif(all) {\n-\t\truntime_acquireWorldsema();\n-\t\truntime_m()->gcing = 1;\n-\t\truntime_stopTheWorldWithSema();\n-\t\tenablegc = mstats()->enablegc;\n-\t\tmstats()->enablegc = false;\n-\t}\n-\n-\tif(b.__count == 0)\n-\t\tn = 0;\n-\telse{\n-\t\tG* g = runtime_g();\n-\t\tg->writebuf.__values = b.__values;\n-\t\tg->writebuf.__count = 0;\n-\t\tg->writebuf.__capacity = b.__count;\n-\t\tUSED(pc);\n-\t\truntime_goroutineheader(g);\n-\t\truntime_traceback();\n-\t\truntime_printcreatedby(g);\n-\t\tif(all)\n-\t\t\truntime_tracebackothers(g);\n-\t\tn = g->writebuf.__count;\n-\t\tg->writebuf.__values = nil;\n-\t\tg->writebuf.__count = 0;\n-\t\tg->writebuf.__capacity = 0;\n-\t}\n-\t\n-\tif(all) {\n-\t\truntime_m()->gcing = 0;\n-\t\tmstats()->enablegc = enablegc;\n-\t\truntime_releaseWorldsema();\n-\t\truntime_startTheWorldWithSema();\n-\t}\n-}\n-\n-static void\n-saveg(G *gp, TRecord *r)\n-{\n-\tint32 n, i;\n-\tLocation locstk[nelem(r->stk)];\n-\n-\tif(gp == runtime_g()) {\n-\t\tn = runtime_callers(0, locstk, nelem(r->stk), false);\n-\t\tfor(i = 0; i < n; i++)\n-\t\t\tr->stk[i] = locstk[i].pc;\n-\t}\n-\telse {\n-\t\t// FIXME: Not implemented.\n-\t\tn = 0;\n-\t}\n-\tif((size_t)n < nelem(r->stk))\n-\t\tr->stk[n] = 0;\n-}\n-\n-func GoroutineProfile(b Slice) (n int, ok bool) {\n-\tuintptr i;\n-\tTRecord *r;\n-\tG *gp;\n-\t\n-\tok = false;\n-\tn = runtime_gcount();\n-\tif(n <= b.__count) {\n-\t\truntime_acquireWorldsema();\n-\t\truntime_m()->gcing = 1;\n-\t\truntime_stopTheWorldWithSema();\n-\n-\t\tn = runtime_gcount();\n-\t\tif(n <= b.__count) {\n-\t\t\tG* g = runtime_g();\n-\t\t\tok = true;\n-\t\t\tr = (TRecord*)b.__values;\n-\t\t\tsaveg(g, r++);\n-\t\t\tfor(i = 0; i < runtime_allglen; i++) {\n-\t\t\t\tgp = runtime_allg[i];\n-\t\t\t\tif(gp == g || gp->atomicstatus == _Gdead)\n-\t\t\t\t\tcontinue;\n-\t\t\t\tsaveg(gp, r++);\n-\t\t\t}\n-\t\t}\n-\t\n-\t\truntime_m()->gcing = 0;\n-\t\truntime_releaseWorldsema();\n-\t\truntime_startTheWorldWithSema();\n-\t}\n-}\n-\n-// Tracing of alloc/free/gc.\n-\n-static Lock tracelock;\n-\n-static const char*\n-typeinfoname(int32 typeinfo)\n-{\n-\tif(typeinfo == TypeInfo_SingleObject)\n-\t\treturn \"single object\";\n-\telse if(typeinfo == TypeInfo_Array)\n-\t\treturn \"array\";\n-\telse if(typeinfo == TypeInfo_Chan)\n-\t\treturn \"channel\";\n-\truntime_throw(\"typinfoname: unknown type info\");\n-\treturn nil;\n-}\n-\n-void\n-runtime_tracealloc(void *p, uintptr size, uintptr typ)\n-{\n-\tconst char *name;\n-\tType *type;\n-\n-\truntime_lock(&tracelock);\n-\truntime_m()->traceback = 2;\n-\ttype = (Type*)(typ & ~3);\n-\tname = typeinfoname(typ & 3);\n-\tif(type == nil)\n-\t\truntime_printf(\"tracealloc(%p, %p, %s)\\n\", p, size, name);\n-\telse\t\n-\t\truntime_printf(\"tracealloc(%p, %p, %s of %S)\\n\", p, size, name, *type->__reflection);\n-\tif(runtime_m()->curg == nil || runtime_g() == runtime_m()->curg) {\n-\t\truntime_goroutineheader(runtime_g());\n-\t\truntime_traceback();\n-\t} else {\n-\t\truntime_goroutineheader(runtime_m()->curg);\n-\t\truntime_traceback();\n-\t}\n-\truntime_printf(\"\\n\");\n-\truntime_m()->traceback = 0;\n-\truntime_unlock(&tracelock);\n-}\n-\n-void\n-runtime_tracefree(void *p, uintptr size)\n-{\n-\truntime_lock(&tracelock);\n-\truntime_m()->traceback = 2;\n-\truntime_printf(\"tracefree(%p, %p)\\n\", p, size);\n-\truntime_goroutineheader(runtime_g());\n-\truntime_traceback();\n-\truntime_printf(\"\\n\");\n-\truntime_m()->traceback = 0;\n-\truntime_unlock(&tracelock);\n-}\n-\n-void\n-runtime_tracegc(void)\n-{\n-\truntime_lock(&tracelock);\n-\truntime_m()->traceback = 2;\n-\truntime_printf(\"tracegc()\\n\");\n-\t// running on m->g0 stack; show all non-g0 goroutines\n-\truntime_tracebackothers(runtime_g());\n-\truntime_printf(\"end tracegc\\n\");\n-\truntime_printf(\"\\n\");\n-\truntime_m()->traceback = 0;\n-\truntime_unlock(&tracelock);\n-}"}, {"sha": "7d65c4b3b725fae71f56e56ec64fb6e8ade0cdd4", "filename": "libgo/runtime/proc.c", "status": "modified", "additions": 34, "deletions": 58, "changes": 92, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fruntime%2Fproc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fruntime%2Fproc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fproc.c?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -657,68 +657,13 @@ runtime_main(void* dummy __attribute__((unused)))\n \t\t*(int32*)0 = 0;\n }\n \n-void\n-runtime_goroutineheader(G *gp)\n-{\n-\tString status;\n-\tint64 waitfor;\n-\n-\tswitch(gp->atomicstatus) {\n-\tcase _Gidle:\n-\t\tstatus = runtime_gostringnocopy((const byte*)\"idle\");\n-\t\tbreak;\n-\tcase _Grunnable:\n-\t\tstatus = runtime_gostringnocopy((const byte*)\"runnable\");\n-\t\tbreak;\n-\tcase _Grunning:\n-\t\tstatus = runtime_gostringnocopy((const byte*)\"running\");\n-\t\tbreak;\n-\tcase _Gsyscall:\n-\t\tstatus = runtime_gostringnocopy((const byte*)\"syscall\");\n-\t\tbreak;\n-\tcase _Gwaiting:\n-\t\tif(gp->waitreason.len > 0)\n-\t\t\tstatus = gp->waitreason;\n-\t\telse\n-\t\t\tstatus = runtime_gostringnocopy((const byte*)\"waiting\");\n-\t\tbreak;\n-\tdefault:\n-\t\tstatus = runtime_gostringnocopy((const byte*)\"???\");\n-\t\tbreak;\n-\t}\n-\n-\t// approx time the G is blocked, in minutes\n-\twaitfor = 0;\n-\tif((gp->atomicstatus == _Gwaiting || gp->atomicstatus == _Gsyscall) && gp->waitsince != 0)\n-\t\twaitfor = (runtime_nanotime() - gp->waitsince) / (60LL*1000*1000*1000);\n-\n-\tif(waitfor < 1)\n-\t\truntime_printf(\"goroutine %D [%S]:\\n\", gp->goid, status);\n-\telse\n-\t\truntime_printf(\"goroutine %D [%S, %D minutes]:\\n\", gp->goid, status, waitfor);\n-}\n-\n-void\n-runtime_printcreatedby(G *g)\n-{\n-\tif(g != nil && g->gopc != 0 && g->goid != 1) {\n-\t\tString fn;\n-\t\tString file;\n-\t\tintgo line;\n-\n-\t\tif(__go_file_line(g->gopc - 1, -1, &fn, &file, &line)) {\n-\t\t\truntime_printf(\"created by %S\\n\", fn);\n-\t\t\truntime_printf(\"\\t%S:%D\\n\", file, (int64) line);\n-\t\t}\n-\t}\n-}\n-\n void\n runtime_tracebackothers(G * volatile me)\n {\n \tG * volatile gp;\n \tTraceback tb;\n \tint32 traceback;\n+\tSlice slice;\n \tvolatile uintptr i;\n \n \ttb.gp = me;\n@@ -739,7 +684,10 @@ runtime_tracebackothers(G * volatile me)\n \t\t  runtime_gogo(gp);\n \t\t}\n \n-\t\truntime_printtrace(tb.locbuf, tb.c, false);\n+\t\tslice.__values = &tb.locbuf[0];\n+\t\tslice.__count = tb.c;\n+\t\tslice.__capacity = tb.c;\n+\t\truntime_printtrace(slice, nil);\n \t\truntime_printcreatedby(gp);\n \t}\n \n@@ -780,7 +728,10 @@ runtime_tracebackothers(G * volatile me)\n \t\t\t\truntime_gogo(gp);\n \t\t\t}\n \n-\t\t\truntime_printtrace(tb.locbuf, tb.c, false);\n+\t\t\tslice.__values = &tb.locbuf[0];\n+\t\t\tslice.__count = tb.c;\n+\t\t\tslice.__capacity = tb.c;\n+\t\t\truntime_printtrace(slice, nil);\n \t\t\truntime_printcreatedby(gp);\n \t\t}\n \t}\n@@ -3597,3 +3548,28 @@ sync_runtime_doSpin()\n {\n \truntime_procyield(ACTIVE_SPIN_CNT);\n }\n+\n+// For Go code to look at variables, until we port proc.go.\n+\n+extern M** runtime_go_allm(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.allm\");\n+\n+M**\n+runtime_go_allm()\n+{\n+\treturn &runtime_allm;\n+}\n+\n+extern Slice runtime_go_allgs(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.allgs\");\n+\n+Slice\n+runtime_go_allgs()\n+{\n+\tSlice s;\n+\n+\ts.__values = runtime_allg;\n+\ts.__count = runtime_allglen;\n+\ts.__capacity = allgcap;\n+\treturn s;\n+}"}, {"sha": "70331f4767e7fd7c18768c08021223c3ffd42753", "filename": "libgo/runtime/runtime.c", "status": "modified", "additions": 0, "deletions": 12, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fruntime%2Fruntime.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fruntime%2Fruntime.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.c?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -90,18 +90,6 @@ runtime_cputicks(void)\n #endif\n }\n \n-bool\n-runtime_showframe(String s, bool current)\n-{\n-\tstatic int32 traceback = -1;\n-\n-\tif(current && runtime_m()->throwing > 0)\n-\t\treturn 1;\n-\tif(traceback < 0)\n-\t\ttraceback = runtime_gotraceback(nil);\n-\treturn traceback > 1 || (__builtin_memchr(s.str, '.', s.len) != nil && __builtin_memcmp(s.str, \"runtime.\", 7) != 0);\n-}\n-\n // Called to initialize a new m (including the bootstrap m).\n // Called on the parent thread (main thread in case of bootstrap), can allocate memory.\n void"}, {"sha": "be19e9527ccb2f571c5d2fb2547f7953fa3152e7", "filename": "libgo/runtime/runtime.h", "status": "modified", "additions": 11, "deletions": 7, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fruntime%2Fruntime.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a/libgo%2Fruntime%2Fruntime.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.h?ref=1f0be9ee86f63bac9c4541a9cfaf52cb5ae5e89a", "patch": "@@ -89,7 +89,7 @@ typedef struct\t__go_interface_type\tInterfaceType;\n typedef struct\t__go_map_type\t\tMapType;\n typedef struct\t__go_channel_type\tChanType;\n \n-typedef struct  traceback\tTraceback;\n+typedef struct  tracebackg\tTraceback;\n \n typedef struct\tlocation\tLocation;\n \n@@ -261,8 +261,10 @@ enum {\n };\n void\truntime_hashinit(void);\n \n-void\truntime_traceback(void);\n-void\truntime_tracebackothers(G*);\n+void\truntime_traceback(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.traceback\");\n+void\truntime_tracebackothers(G*)\n+  __asm__ (GOSYM_PREFIX \"runtime.tracebackothers\");\n enum\n {\n \t// The maximum number of frames we print for a traceback\n@@ -325,8 +327,10 @@ void\truntime_sigenable(uint32 sig);\n void\truntime_sigdisable(uint32 sig);\n void\truntime_sigignore(uint32 sig);\n int32\truntime_gotraceback(bool *crash);\n-void\truntime_goroutineheader(G*);\n-void\truntime_printtrace(Location*, int32, bool);\n+void\truntime_goroutineheader(G*)\n+  __asm__ (GOSYM_PREFIX \"runtime.goroutineheader\");\n+void\truntime_printtrace(Slice, G*)\n+  __asm__ (GOSYM_PREFIX \"runtime.printtrace\");\n #define runtime_open(p, f, m) open((p), (f), (m))\n #define runtime_read(d, v, n) read((d), (v), (n))\n #define runtime_write(d, v, n) write((d), (v), (n))\n@@ -561,8 +565,8 @@ void\truntime_lockOSThread(void);\n void\truntime_unlockOSThread(void);\n bool\truntime_lockedOSThread(void);\n \n-bool\truntime_showframe(String, bool);\n-void\truntime_printcreatedby(G*);\n+void\truntime_printcreatedby(G*)\n+  __asm__(GOSYM_PREFIX \"runtime.printcreatedby\");\n \n uintptr\truntime_memlimit(void);\n "}]}