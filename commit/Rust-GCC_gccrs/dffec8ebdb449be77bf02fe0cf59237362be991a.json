{"sha": "dffec8ebdb449be77bf02fe0cf59237362be991a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZGZmZWM4ZWJkYjQ0OWJlNzdiZjAyZmUwY2Y1OTIzNzM2MmJlOTkxYQ==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2017-11-20T10:08:48Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2017-11-20T10:08:48Z"}, "message": "tree-ssa-math-opts.c (nop_stats, [...]): Moved to ...\n\n\t* tree-ssa-math-opts.c (nop_stats, bswap_stats, struct symbolic_number,\n\tBITS_PER_MARKER, MARKER_MASK, MARKER_BYTE_UNKNOWN, HEAD_MARKER, CMPNOP,\n\tCMPXCHG, do_shift_rotate, verify_symbolic_number_p,\n\tinit_symbolic_number, find_bswap_or_nop_load, perform_symbolic_merge,\n\tfind_bswap_or_nop_1, find_bswap_or_nop, pass_data_optimize_bswap,\n\tclass pass_optimize_bswap, bswap_replace,\n\tpass_optimize_bswap::execute): Moved to ...\n\t* gimple-ssa-store-merging.c: ... this file.\n\tInclude optabs-tree.h.\n\t(nop_stats, bswap_stats, do_shift_rotate, verify_symbolic_number_p,\n\tinit_symbolic_number, find_bswap_or_nop_load, perform_symbolic_merge,\n\tfind_bswap_or_nop_1, find_bswap_or_nop, bswap_replace): Put into\n\tanonymous namespace, remove static keywords.\n\t(pass_optimize_bswap::gate): Test BITS_PER_UNIT == 8 here...\n\t(pass_optimize_bswap::execute): ... rather than here.  Formatting fix.\n\nFrom-SVN: r254947", "tree": {"sha": "39db051e345ba306c9d06d607abcfdcb79212369", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/39db051e345ba306c9d06d607abcfdcb79212369"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/dffec8ebdb449be77bf02fe0cf59237362be991a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/dffec8ebdb449be77bf02fe0cf59237362be991a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/dffec8ebdb449be77bf02fe0cf59237362be991a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/dffec8ebdb449be77bf02fe0cf59237362be991a/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "12b8cb2e5b236faeb012fc544b27d32ce6cedde7", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/12b8cb2e5b236faeb012fc544b27d32ce6cedde7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/12b8cb2e5b236faeb012fc544b27d32ce6cedde7"}], "stats": {"total": 2176, "additions": 1096, "deletions": 1080}, "files": [{"sha": "e6ffa2640f64b49d5154657fdd92a0bca18e227e", "filename": "gcc/ChangeLog", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/dffec8ebdb449be77bf02fe0cf59237362be991a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/dffec8ebdb449be77bf02fe0cf59237362be991a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=dffec8ebdb449be77bf02fe0cf59237362be991a", "patch": "@@ -1,3 +1,21 @@\n+2017-11-20  Jakub Jelinek  <jakub@redhat.com>\n+\n+\t* tree-ssa-math-opts.c (nop_stats, bswap_stats, struct symbolic_number,\n+\tBITS_PER_MARKER, MARKER_MASK, MARKER_BYTE_UNKNOWN, HEAD_MARKER, CMPNOP,\n+\tCMPXCHG, do_shift_rotate, verify_symbolic_number_p,\n+\tinit_symbolic_number, find_bswap_or_nop_load, perform_symbolic_merge,\n+\tfind_bswap_or_nop_1, find_bswap_or_nop, pass_data_optimize_bswap,\n+\tclass pass_optimize_bswap, bswap_replace,\n+\tpass_optimize_bswap::execute): Moved to ...\n+\t* gimple-ssa-store-merging.c: ... this file.\n+\tInclude optabs-tree.h.\n+\t(nop_stats, bswap_stats, do_shift_rotate, verify_symbolic_number_p,\n+\tinit_symbolic_number, find_bswap_or_nop_load, perform_symbolic_merge,\n+\tfind_bswap_or_nop_1, find_bswap_or_nop, bswap_replace): Put into\n+\tanonymous namespace, remove static keywords.\n+\t(pass_optimize_bswap::gate): Test BITS_PER_UNIT == 8 here...\n+\t(pass_optimize_bswap::execute): ... rather than here.  Formatting fix.\n+\n 2017-11-20  Jan Hubicka  <hubicka@ucw.cz>\n \n \tPR bootstrap/83062"}, {"sha": "b84f892e88174385dd6bd121ae169de1affa7a08", "filename": "gcc/gimple-ssa-store-merging.c", "status": "modified", "additions": 1078, "deletions": 4, "changes": 1082, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/dffec8ebdb449be77bf02fe0cf59237362be991a/gcc%2Fgimple-ssa-store-merging.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/dffec8ebdb449be77bf02fe0cf59237362be991a/gcc%2Fgimple-ssa-store-merging.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-ssa-store-merging.c?ref=dffec8ebdb449be77bf02fe0cf59237362be991a", "patch": "@@ -1,5 +1,5 @@\n-/* GIMPLE store merging pass.\n-   Copyright (C) 2016-2017 Free Software Foundation, Inc.\n+/* GIMPLE store merging and byte swapping passes.\n+   Copyright (C) 2009-2017 Free Software Foundation, Inc.\n    Contributed by ARM Ltd.\n \n    This file is part of GCC.\n@@ -18,8 +18,8 @@\n    along with GCC; see the file COPYING3.  If not see\n    <http://www.gnu.org/licenses/>.  */\n \n-/* The purpose of this pass is to combine multiple memory stores of\n-   constant values, values loaded from memory or bitwise operations\n+/* The purpose of the store merging pass is to combine multiple memory\n+   stores of constant values, values loaded from memory or bitwise operations\n    on those to consecutive memory locations into fewer wider stores.\n    For example, if we have a sequence peforming four byte stores to\n    consecutive memory locations:\n@@ -157,6 +157,7 @@\n #include \"gimplify-me.h\"\n #include \"rtl.h\"\n #include \"expr.h\"\t/* For get_bit_range.  */\n+#include \"optabs-tree.h\"\n #include \"selftest.h\"\n \n /* The maximum size (in bits) of the stores this pass should generate.  */\n@@ -169,6 +170,1079 @@\n \n namespace {\n \n+struct\n+{\n+  /* Number of hand-written 16-bit nop / bswaps found.  */\n+  int found_16bit;\n+\n+  /* Number of hand-written 32-bit nop / bswaps found.  */\n+  int found_32bit;\n+\n+  /* Number of hand-written 64-bit nop / bswaps found.  */\n+  int found_64bit;\n+} nop_stats, bswap_stats;\n+\n+/* A symbolic number structure is used to detect byte permutation and selection\n+   patterns of a source.  To achieve that, its field N contains an artificial\n+   number consisting of BITS_PER_MARKER sized markers tracking where does each\n+   byte come from in the source:\n+\n+   0\t   - target byte has the value 0\n+   FF\t   - target byte has an unknown value (eg. due to sign extension)\n+   1..size - marker value is the byte index in the source (0 for lsb).\n+\n+   To detect permutations on memory sources (arrays and structures), a symbolic\n+   number is also associated:\n+   - a base address BASE_ADDR and an OFFSET giving the address of the source;\n+   - a range which gives the difference between the highest and lowest accessed\n+     memory location to make such a symbolic number;\n+   - the address SRC of the source element of lowest address as a convenience\n+     to easily get BASE_ADDR + offset + lowest bytepos;\n+   - number of expressions N_OPS bitwise ored together to represent\n+     approximate cost of the computation.\n+\n+   Note 1: the range is different from size as size reflects the size of the\n+   type of the current expression.  For instance, for an array char a[],\n+   (short) a[0] | (short) a[3] would have a size of 2 but a range of 4 while\n+   (short) a[0] | ((short) a[0] << 1) would still have a size of 2 but this\n+   time a range of 1.\n+\n+   Note 2: for non-memory sources, range holds the same value as size.\n+\n+   Note 3: SRC points to the SSA_NAME in case of non-memory source.  */\n+\n+struct symbolic_number {\n+  uint64_t n;\n+  tree type;\n+  tree base_addr;\n+  tree offset;\n+  HOST_WIDE_INT bytepos;\n+  tree src;\n+  tree alias_set;\n+  tree vuse;\n+  unsigned HOST_WIDE_INT range;\n+  int n_ops;\n+};\n+\n+#define BITS_PER_MARKER 8\n+#define MARKER_MASK ((1 << BITS_PER_MARKER) - 1)\n+#define MARKER_BYTE_UNKNOWN MARKER_MASK\n+#define HEAD_MARKER(n, size) \\\n+  ((n) & ((uint64_t) MARKER_MASK << (((size) - 1) * BITS_PER_MARKER)))\n+\n+/* The number which the find_bswap_or_nop_1 result should match in\n+   order to have a nop.  The number is masked according to the size of\n+   the symbolic number before using it.  */\n+#define CMPNOP (sizeof (int64_t) < 8 ? 0 : \\\n+  (uint64_t)0x08070605 << 32 | 0x04030201)\n+\n+/* The number which the find_bswap_or_nop_1 result should match in\n+   order to have a byte swap.  The number is masked according to the\n+   size of the symbolic number before using it.  */\n+#define CMPXCHG (sizeof (int64_t) < 8 ? 0 : \\\n+  (uint64_t)0x01020304 << 32 | 0x05060708)\n+\n+/* Perform a SHIFT or ROTATE operation by COUNT bits on symbolic\n+   number N.  Return false if the requested operation is not permitted\n+   on a symbolic number.  */\n+\n+inline bool\n+do_shift_rotate (enum tree_code code,\n+\t\t struct symbolic_number *n,\n+\t\t int count)\n+{\n+  int i, size = TYPE_PRECISION (n->type) / BITS_PER_UNIT;\n+  unsigned head_marker;\n+\n+  if (count % BITS_PER_UNIT != 0)\n+    return false;\n+  count = (count / BITS_PER_UNIT) * BITS_PER_MARKER;\n+\n+  /* Zero out the extra bits of N in order to avoid them being shifted\n+     into the significant bits.  */\n+  if (size < 64 / BITS_PER_MARKER)\n+    n->n &= ((uint64_t) 1 << (size * BITS_PER_MARKER)) - 1;\n+\n+  switch (code)\n+    {\n+    case LSHIFT_EXPR:\n+      n->n <<= count;\n+      break;\n+    case RSHIFT_EXPR:\n+      head_marker = HEAD_MARKER (n->n, size);\n+      n->n >>= count;\n+      /* Arithmetic shift of signed type: result is dependent on the value.  */\n+      if (!TYPE_UNSIGNED (n->type) && head_marker)\n+\tfor (i = 0; i < count / BITS_PER_MARKER; i++)\n+\t  n->n |= (uint64_t) MARKER_BYTE_UNKNOWN\n+\t\t  << ((size - 1 - i) * BITS_PER_MARKER);\n+      break;\n+    case LROTATE_EXPR:\n+      n->n = (n->n << count) | (n->n >> ((size * BITS_PER_MARKER) - count));\n+      break;\n+    case RROTATE_EXPR:\n+      n->n = (n->n >> count) | (n->n << ((size * BITS_PER_MARKER) - count));\n+      break;\n+    default:\n+      return false;\n+    }\n+  /* Zero unused bits for size.  */\n+  if (size < 64 / BITS_PER_MARKER)\n+    n->n &= ((uint64_t) 1 << (size * BITS_PER_MARKER)) - 1;\n+  return true;\n+}\n+\n+/* Perform sanity checking for the symbolic number N and the gimple\n+   statement STMT.  */\n+\n+inline bool\n+verify_symbolic_number_p (struct symbolic_number *n, gimple *stmt)\n+{\n+  tree lhs_type;\n+\n+  lhs_type = gimple_expr_type (stmt);\n+\n+  if (TREE_CODE (lhs_type) != INTEGER_TYPE)\n+    return false;\n+\n+  if (TYPE_PRECISION (lhs_type) != TYPE_PRECISION (n->type))\n+    return false;\n+\n+  return true;\n+}\n+\n+/* Initialize the symbolic number N for the bswap pass from the base element\n+   SRC manipulated by the bitwise OR expression.  */\n+\n+bool\n+init_symbolic_number (struct symbolic_number *n, tree src)\n+{\n+  int size;\n+\n+  if (! INTEGRAL_TYPE_P (TREE_TYPE (src)))\n+    return false;\n+\n+  n->base_addr = n->offset = n->alias_set = n->vuse = NULL_TREE;\n+  n->src = src;\n+\n+  /* Set up the symbolic number N by setting each byte to a value between 1 and\n+     the byte size of rhs1.  The highest order byte is set to n->size and the\n+     lowest order byte to 1.  */\n+  n->type = TREE_TYPE (src);\n+  size = TYPE_PRECISION (n->type);\n+  if (size % BITS_PER_UNIT != 0)\n+    return false;\n+  size /= BITS_PER_UNIT;\n+  if (size > 64 / BITS_PER_MARKER)\n+    return false;\n+  n->range = size;\n+  n->n = CMPNOP;\n+  n->n_ops = 1;\n+\n+  if (size < 64 / BITS_PER_MARKER)\n+    n->n &= ((uint64_t) 1 << (size * BITS_PER_MARKER)) - 1;\n+\n+  return true;\n+}\n+\n+/* Check if STMT might be a byte swap or a nop from a memory source and returns\n+   the answer. If so, REF is that memory source and the base of the memory area\n+   accessed and the offset of the access from that base are recorded in N.  */\n+\n+bool\n+find_bswap_or_nop_load (gimple *stmt, tree ref, struct symbolic_number *n)\n+{\n+  /* Leaf node is an array or component ref. Memorize its base and\n+     offset from base to compare to other such leaf node.  */\n+  HOST_WIDE_INT bitsize, bitpos;\n+  machine_mode mode;\n+  int unsignedp, reversep, volatilep;\n+  tree offset, base_addr;\n+\n+  /* Not prepared to handle PDP endian.  */\n+  if (BYTES_BIG_ENDIAN != WORDS_BIG_ENDIAN)\n+    return false;\n+\n+  if (!gimple_assign_load_p (stmt) || gimple_has_volatile_ops (stmt))\n+    return false;\n+\n+  base_addr = get_inner_reference (ref, &bitsize, &bitpos, &offset, &mode,\n+\t\t\t\t   &unsignedp, &reversep, &volatilep);\n+\n+  if (TREE_CODE (base_addr) == MEM_REF)\n+    {\n+      offset_int bit_offset = 0;\n+      tree off = TREE_OPERAND (base_addr, 1);\n+\n+      if (!integer_zerop (off))\n+\t{\n+\t  offset_int boff, coff = mem_ref_offset (base_addr);\n+\t  boff = coff << LOG2_BITS_PER_UNIT;\n+\t  bit_offset += boff;\n+\t}\n+\n+      base_addr = TREE_OPERAND (base_addr, 0);\n+\n+      /* Avoid returning a negative bitpos as this may wreak havoc later.  */\n+      if (wi::neg_p (bit_offset))\n+\t{\n+\t  offset_int mask = wi::mask <offset_int> (LOG2_BITS_PER_UNIT, false);\n+\t  offset_int tem = wi::bit_and_not (bit_offset, mask);\n+\t  /* TEM is the bitpos rounded to BITS_PER_UNIT towards -Inf.\n+\t     Subtract it to BIT_OFFSET and add it (scaled) to OFFSET.  */\n+\t  bit_offset -= tem;\n+\t  tem >>= LOG2_BITS_PER_UNIT;\n+\t  if (offset)\n+\t    offset = size_binop (PLUS_EXPR, offset,\n+\t\t\t\t    wide_int_to_tree (sizetype, tem));\n+\t  else\n+\t    offset = wide_int_to_tree (sizetype, tem);\n+\t}\n+\n+      bitpos += bit_offset.to_shwi ();\n+    }\n+\n+  if (bitpos % BITS_PER_UNIT)\n+    return false;\n+  if (bitsize % BITS_PER_UNIT)\n+    return false;\n+  if (reversep)\n+    return false;\n+\n+  if (!init_symbolic_number (n, ref))\n+    return false;\n+  n->base_addr = base_addr;\n+  n->offset = offset;\n+  n->bytepos = bitpos / BITS_PER_UNIT;\n+  n->alias_set = reference_alias_ptr_type (ref);\n+  n->vuse = gimple_vuse (stmt);\n+  return true;\n+}\n+\n+/* Compute the symbolic number N representing the result of a bitwise OR on 2\n+   symbolic number N1 and N2 whose source statements are respectively\n+   SOURCE_STMT1 and SOURCE_STMT2.  */\n+\n+gimple *\n+perform_symbolic_merge (gimple *source_stmt1, struct symbolic_number *n1,\n+\t\t\tgimple *source_stmt2, struct symbolic_number *n2,\n+\t\t\tstruct symbolic_number *n)\n+{\n+  int i, size;\n+  uint64_t mask;\n+  gimple *source_stmt;\n+  struct symbolic_number *n_start;\n+\n+  tree rhs1 = gimple_assign_rhs1 (source_stmt1);\n+  if (TREE_CODE (rhs1) == BIT_FIELD_REF\n+      && TREE_CODE (TREE_OPERAND (rhs1, 0)) == SSA_NAME)\n+    rhs1 = TREE_OPERAND (rhs1, 0);\n+  tree rhs2 = gimple_assign_rhs1 (source_stmt2);\n+  if (TREE_CODE (rhs2) == BIT_FIELD_REF\n+      && TREE_CODE (TREE_OPERAND (rhs2, 0)) == SSA_NAME)\n+    rhs2 = TREE_OPERAND (rhs2, 0);\n+\n+  /* Sources are different, cancel bswap if they are not memory location with\n+     the same base (array, structure, ...).  */\n+  if (rhs1 != rhs2)\n+    {\n+      uint64_t inc;\n+      HOST_WIDE_INT start_sub, end_sub, end1, end2, end;\n+      struct symbolic_number *toinc_n_ptr, *n_end;\n+      basic_block bb1, bb2;\n+\n+      if (!n1->base_addr || !n2->base_addr\n+\t  || !operand_equal_p (n1->base_addr, n2->base_addr, 0))\n+\treturn NULL;\n+\n+      if (!n1->offset != !n2->offset\n+\t  || (n1->offset && !operand_equal_p (n1->offset, n2->offset, 0)))\n+\treturn NULL;\n+\n+      if (n1->bytepos < n2->bytepos)\n+\t{\n+\t  n_start = n1;\n+\t  start_sub = n2->bytepos - n1->bytepos;\n+\t}\n+      else\n+\t{\n+\t  n_start = n2;\n+\t  start_sub = n1->bytepos - n2->bytepos;\n+\t}\n+\n+      bb1 = gimple_bb (source_stmt1);\n+      bb2 = gimple_bb (source_stmt2);\n+      if (dominated_by_p (CDI_DOMINATORS, bb1, bb2))\n+\tsource_stmt = source_stmt1;\n+      else\n+\tsource_stmt = source_stmt2;\n+\n+      /* Find the highest address at which a load is performed and\n+\t compute related info.  */\n+      end1 = n1->bytepos + (n1->range - 1);\n+      end2 = n2->bytepos + (n2->range - 1);\n+      if (end1 < end2)\n+\t{\n+\t  end = end2;\n+\t  end_sub = end2 - end1;\n+\t}\n+      else\n+\t{\n+\t  end = end1;\n+\t  end_sub = end1 - end2;\n+\t}\n+      n_end = (end2 > end1) ? n2 : n1;\n+\n+      /* Find symbolic number whose lsb is the most significant.  */\n+      if (BYTES_BIG_ENDIAN)\n+\ttoinc_n_ptr = (n_end == n1) ? n2 : n1;\n+      else\n+\ttoinc_n_ptr = (n_start == n1) ? n2 : n1;\n+\n+      n->range = end - n_start->bytepos + 1;\n+\n+      /* Check that the range of memory covered can be represented by\n+\t a symbolic number.  */\n+      if (n->range > 64 / BITS_PER_MARKER)\n+\treturn NULL;\n+\n+      /* Reinterpret byte marks in symbolic number holding the value of\n+\t bigger weight according to target endianness.  */\n+      inc = BYTES_BIG_ENDIAN ? end_sub : start_sub;\n+      size = TYPE_PRECISION (n1->type) / BITS_PER_UNIT;\n+      for (i = 0; i < size; i++, inc <<= BITS_PER_MARKER)\n+\t{\n+\t  unsigned marker\n+\t    = (toinc_n_ptr->n >> (i * BITS_PER_MARKER)) & MARKER_MASK;\n+\t  if (marker && marker != MARKER_BYTE_UNKNOWN)\n+\t    toinc_n_ptr->n += inc;\n+\t}\n+    }\n+  else\n+    {\n+      n->range = n1->range;\n+      n_start = n1;\n+      source_stmt = source_stmt1;\n+    }\n+\n+  if (!n1->alias_set\n+      || alias_ptr_types_compatible_p (n1->alias_set, n2->alias_set))\n+    n->alias_set = n1->alias_set;\n+  else\n+    n->alias_set = ptr_type_node;\n+  n->vuse = n_start->vuse;\n+  n->base_addr = n_start->base_addr;\n+  n->offset = n_start->offset;\n+  n->src = n_start->src;\n+  n->bytepos = n_start->bytepos;\n+  n->type = n_start->type;\n+  size = TYPE_PRECISION (n->type) / BITS_PER_UNIT;\n+\n+  for (i = 0, mask = MARKER_MASK; i < size; i++, mask <<= BITS_PER_MARKER)\n+    {\n+      uint64_t masked1, masked2;\n+\n+      masked1 = n1->n & mask;\n+      masked2 = n2->n & mask;\n+      if (masked1 && masked2 && masked1 != masked2)\n+\treturn NULL;\n+    }\n+  n->n = n1->n | n2->n;\n+  n->n_ops = n1->n_ops + n2->n_ops;\n+\n+  return source_stmt;\n+}\n+\n+/* find_bswap_or_nop_1 invokes itself recursively with N and tries to perform\n+   the operation given by the rhs of STMT on the result.  If the operation\n+   could successfully be executed the function returns a gimple stmt whose\n+   rhs's first tree is the expression of the source operand and NULL\n+   otherwise.  */\n+\n+gimple *\n+find_bswap_or_nop_1 (gimple *stmt, struct symbolic_number *n, int limit)\n+{\n+  enum tree_code code;\n+  tree rhs1, rhs2 = NULL;\n+  gimple *rhs1_stmt, *rhs2_stmt, *source_stmt1;\n+  enum gimple_rhs_class rhs_class;\n+\n+  if (!limit || !is_gimple_assign (stmt))\n+    return NULL;\n+\n+  rhs1 = gimple_assign_rhs1 (stmt);\n+\n+  if (find_bswap_or_nop_load (stmt, rhs1, n))\n+    return stmt;\n+\n+  /* Handle BIT_FIELD_REF.  */\n+  if (TREE_CODE (rhs1) == BIT_FIELD_REF\n+      && TREE_CODE (TREE_OPERAND (rhs1, 0)) == SSA_NAME)\n+    {\n+      unsigned HOST_WIDE_INT bitsize = tree_to_uhwi (TREE_OPERAND (rhs1, 1));\n+      unsigned HOST_WIDE_INT bitpos = tree_to_uhwi (TREE_OPERAND (rhs1, 2));\n+      if (bitpos % BITS_PER_UNIT == 0\n+\t  && bitsize % BITS_PER_UNIT == 0\n+\t  && init_symbolic_number (n, TREE_OPERAND (rhs1, 0)))\n+\t{\n+\t  /* Handle big-endian bit numbering in BIT_FIELD_REF.  */\n+\t  if (BYTES_BIG_ENDIAN)\n+\t    bitpos = TYPE_PRECISION (n->type) - bitpos - bitsize;\n+\n+\t  /* Shift.  */\n+\t  if (!do_shift_rotate (RSHIFT_EXPR, n, bitpos))\n+\t    return NULL;\n+\n+\t  /* Mask.  */\n+\t  uint64_t mask = 0;\n+\t  uint64_t tmp = (1 << BITS_PER_UNIT) - 1;\n+\t  for (unsigned i = 0; i < bitsize / BITS_PER_UNIT;\n+\t       i++, tmp <<= BITS_PER_UNIT)\n+\t    mask |= (uint64_t) MARKER_MASK << (i * BITS_PER_MARKER);\n+\t  n->n &= mask;\n+\n+\t  /* Convert.  */\n+\t  n->type = TREE_TYPE (rhs1);\n+\t  if (!n->base_addr)\n+\t    n->range = TYPE_PRECISION (n->type) / BITS_PER_UNIT;\n+\n+\t  return verify_symbolic_number_p (n, stmt) ? stmt : NULL;\n+\t}\n+\n+      return NULL;\n+    }\n+\n+  if (TREE_CODE (rhs1) != SSA_NAME)\n+    return NULL;\n+\n+  code = gimple_assign_rhs_code (stmt);\n+  rhs_class = gimple_assign_rhs_class (stmt);\n+  rhs1_stmt = SSA_NAME_DEF_STMT (rhs1);\n+\n+  if (rhs_class == GIMPLE_BINARY_RHS)\n+    rhs2 = gimple_assign_rhs2 (stmt);\n+\n+  /* Handle unary rhs and binary rhs with integer constants as second\n+     operand.  */\n+\n+  if (rhs_class == GIMPLE_UNARY_RHS\n+      || (rhs_class == GIMPLE_BINARY_RHS\n+\t  && TREE_CODE (rhs2) == INTEGER_CST))\n+    {\n+      if (code != BIT_AND_EXPR\n+\t  && code != LSHIFT_EXPR\n+\t  && code != RSHIFT_EXPR\n+\t  && code != LROTATE_EXPR\n+\t  && code != RROTATE_EXPR\n+\t  && !CONVERT_EXPR_CODE_P (code))\n+\treturn NULL;\n+\n+      source_stmt1 = find_bswap_or_nop_1 (rhs1_stmt, n, limit - 1);\n+\n+      /* If find_bswap_or_nop_1 returned NULL, STMT is a leaf node and\n+\t we have to initialize the symbolic number.  */\n+      if (!source_stmt1)\n+\t{\n+\t  if (gimple_assign_load_p (stmt)\n+\t      || !init_symbolic_number (n, rhs1))\n+\t    return NULL;\n+\t  source_stmt1 = stmt;\n+\t}\n+\n+      switch (code)\n+\t{\n+\tcase BIT_AND_EXPR:\n+\t  {\n+\t    int i, size = TYPE_PRECISION (n->type) / BITS_PER_UNIT;\n+\t    uint64_t val = int_cst_value (rhs2), mask = 0;\n+\t    uint64_t tmp = (1 << BITS_PER_UNIT) - 1;\n+\n+\t    /* Only constants masking full bytes are allowed.  */\n+\t    for (i = 0; i < size; i++, tmp <<= BITS_PER_UNIT)\n+\t      if ((val & tmp) != 0 && (val & tmp) != tmp)\n+\t\treturn NULL;\n+\t      else if (val & tmp)\n+\t\tmask |= (uint64_t) MARKER_MASK << (i * BITS_PER_MARKER);\n+\n+\t    n->n &= mask;\n+\t  }\n+\t  break;\n+\tcase LSHIFT_EXPR:\n+\tcase RSHIFT_EXPR:\n+\tcase LROTATE_EXPR:\n+\tcase RROTATE_EXPR:\n+\t  if (!do_shift_rotate (code, n, (int) TREE_INT_CST_LOW (rhs2)))\n+\t    return NULL;\n+\t  break;\n+\tCASE_CONVERT:\n+\t  {\n+\t    int i, type_size, old_type_size;\n+\t    tree type;\n+\n+\t    type = gimple_expr_type (stmt);\n+\t    type_size = TYPE_PRECISION (type);\n+\t    if (type_size % BITS_PER_UNIT != 0)\n+\t      return NULL;\n+\t    type_size /= BITS_PER_UNIT;\n+\t    if (type_size > 64 / BITS_PER_MARKER)\n+\t      return NULL;\n+\n+\t    /* Sign extension: result is dependent on the value.  */\n+\t    old_type_size = TYPE_PRECISION (n->type) / BITS_PER_UNIT;\n+\t    if (!TYPE_UNSIGNED (n->type) && type_size > old_type_size\n+\t\t&& HEAD_MARKER (n->n, old_type_size))\n+\t      for (i = 0; i < type_size - old_type_size; i++)\n+\t\tn->n |= (uint64_t) MARKER_BYTE_UNKNOWN\n+\t\t\t<< ((type_size - 1 - i) * BITS_PER_MARKER);\n+\n+\t    if (type_size < 64 / BITS_PER_MARKER)\n+\t      {\n+\t\t/* If STMT casts to a smaller type mask out the bits not\n+\t\t   belonging to the target type.  */\n+\t\tn->n &= ((uint64_t) 1 << (type_size * BITS_PER_MARKER)) - 1;\n+\t      }\n+\t    n->type = type;\n+\t    if (!n->base_addr)\n+\t      n->range = type_size;\n+\t  }\n+\t  break;\n+\tdefault:\n+\t  return NULL;\n+\t};\n+      return verify_symbolic_number_p (n, stmt) ? source_stmt1 : NULL;\n+    }\n+\n+  /* Handle binary rhs.  */\n+\n+  if (rhs_class == GIMPLE_BINARY_RHS)\n+    {\n+      struct symbolic_number n1, n2;\n+      gimple *source_stmt, *source_stmt2;\n+\n+      if (code != BIT_IOR_EXPR)\n+\treturn NULL;\n+\n+      if (TREE_CODE (rhs2) != SSA_NAME)\n+\treturn NULL;\n+\n+      rhs2_stmt = SSA_NAME_DEF_STMT (rhs2);\n+\n+      switch (code)\n+\t{\n+\tcase BIT_IOR_EXPR:\n+\t  source_stmt1 = find_bswap_or_nop_1 (rhs1_stmt, &n1, limit - 1);\n+\n+\t  if (!source_stmt1)\n+\t    return NULL;\n+\n+\t  source_stmt2 = find_bswap_or_nop_1 (rhs2_stmt, &n2, limit - 1);\n+\n+\t  if (!source_stmt2)\n+\t    return NULL;\n+\n+\t  if (TYPE_PRECISION (n1.type) != TYPE_PRECISION (n2.type))\n+\t    return NULL;\n+\n+\t  if (!n1.vuse != !n2.vuse\n+\t      || (n1.vuse && !operand_equal_p (n1.vuse, n2.vuse, 0)))\n+\t    return NULL;\n+\n+\t  source_stmt\n+\t    = perform_symbolic_merge (source_stmt1, &n1, source_stmt2, &n2, n);\n+\n+\t  if (!source_stmt)\n+\t    return NULL;\n+\n+\t  if (!verify_symbolic_number_p (n, stmt))\n+\t    return NULL;\n+\n+\t  break;\n+\tdefault:\n+\t  return NULL;\n+\t}\n+      return source_stmt;\n+    }\n+  return NULL;\n+}\n+\n+/* Check if STMT completes a bswap implementation or a read in a given\n+   endianness consisting of ORs, SHIFTs and ANDs and sets *BSWAP\n+   accordingly.  It also sets N to represent the kind of operations\n+   performed: size of the resulting expression and whether it works on\n+   a memory source, and if so alias-set and vuse.  At last, the\n+   function returns a stmt whose rhs's first tree is the source\n+   expression.  */\n+\n+gimple *\n+find_bswap_or_nop (gimple *stmt, struct symbolic_number *n, bool *bswap)\n+{\n+  unsigned rsize;\n+  uint64_t tmpn, mask;\n+/* The number which the find_bswap_or_nop_1 result should match in order\n+   to have a full byte swap.  The number is shifted to the right\n+   according to the size of the symbolic number before using it.  */\n+  uint64_t cmpxchg = CMPXCHG;\n+  uint64_t cmpnop = CMPNOP;\n+\n+  gimple *ins_stmt;\n+  int limit;\n+\n+  /* The last parameter determines the depth search limit.  It usually\n+     correlates directly to the number n of bytes to be touched.  We\n+     increase that number by log2(n) + 1 here in order to also\n+     cover signed -> unsigned conversions of the src operand as can be seen\n+     in libgcc, and for initial shift/and operation of the src operand.  */\n+  limit = TREE_INT_CST_LOW (TYPE_SIZE_UNIT (gimple_expr_type (stmt)));\n+  limit += 1 + (int) ceil_log2 ((unsigned HOST_WIDE_INT) limit);\n+  ins_stmt = find_bswap_or_nop_1 (stmt, n, limit);\n+\n+  if (!ins_stmt)\n+    return NULL;\n+\n+  /* Find real size of result (highest non-zero byte).  */\n+  if (n->base_addr)\n+    for (tmpn = n->n, rsize = 0; tmpn; tmpn >>= BITS_PER_MARKER, rsize++);\n+  else\n+    rsize = n->range;\n+\n+  /* Zero out the bits corresponding to untouched bytes in original gimple\n+     expression.  */\n+  if (n->range < (int) sizeof (int64_t))\n+    {\n+      mask = ((uint64_t) 1 << (n->range * BITS_PER_MARKER)) - 1;\n+      cmpxchg >>= (64 / BITS_PER_MARKER - n->range) * BITS_PER_MARKER;\n+      cmpnop &= mask;\n+    }\n+\n+  /* Zero out the bits corresponding to unused bytes in the result of the\n+     gimple expression.  */\n+  if (rsize < n->range)\n+    {\n+      if (BYTES_BIG_ENDIAN)\n+\t{\n+\t  mask = ((uint64_t) 1 << (rsize * BITS_PER_MARKER)) - 1;\n+\t  cmpxchg &= mask;\n+\t  cmpnop >>= (n->range - rsize) * BITS_PER_MARKER;\n+\t}\n+      else\n+\t{\n+\t  mask = ((uint64_t) 1 << (rsize * BITS_PER_MARKER)) - 1;\n+\t  cmpxchg >>= (n->range - rsize) * BITS_PER_MARKER;\n+\t  cmpnop &= mask;\n+\t}\n+      n->range = rsize;\n+    }\n+\n+  /* A complete byte swap should make the symbolic number to start with\n+     the largest digit in the highest order byte. Unchanged symbolic\n+     number indicates a read with same endianness as target architecture.  */\n+  if (n->n == cmpnop)\n+    *bswap = false;\n+  else if (n->n == cmpxchg)\n+    *bswap = true;\n+  else\n+    return NULL;\n+\n+  /* Useless bit manipulation performed by code.  */\n+  if (!n->base_addr && n->n == cmpnop && n->n_ops == 1)\n+    return NULL;\n+\n+  n->range *= BITS_PER_UNIT;\n+  return ins_stmt;\n+}\n+\n+const pass_data pass_data_optimize_bswap =\n+{\n+  GIMPLE_PASS, /* type */\n+  \"bswap\", /* name */\n+  OPTGROUP_NONE, /* optinfo_flags */\n+  TV_NONE, /* tv_id */\n+  PROP_ssa, /* properties_required */\n+  0, /* properties_provided */\n+  0, /* properties_destroyed */\n+  0, /* todo_flags_start */\n+  0, /* todo_flags_finish */\n+};\n+\n+class pass_optimize_bswap : public gimple_opt_pass\n+{\n+public:\n+  pass_optimize_bswap (gcc::context *ctxt)\n+    : gimple_opt_pass (pass_data_optimize_bswap, ctxt)\n+  {}\n+\n+  /* opt_pass methods: */\n+  virtual bool gate (function *)\n+    {\n+      return flag_expensive_optimizations && optimize && BITS_PER_UNIT == 8;\n+    }\n+\n+  virtual unsigned int execute (function *);\n+\n+}; // class pass_optimize_bswap\n+\n+/* Perform the bswap optimization: replace the expression computed in the rhs\n+   of CUR_STMT by an equivalent bswap, load or load + bswap expression.\n+   Which of these alternatives replace the rhs is given by N->base_addr (non\n+   null if a load is needed) and BSWAP.  The type, VUSE and set-alias of the\n+   load to perform are also given in N while the builtin bswap invoke is given\n+   in FNDEL.  Finally, if a load is involved, SRC_STMT refers to one of the\n+   load statements involved to construct the rhs in CUR_STMT and N->range gives\n+   the size of the rhs expression for maintaining some statistics.\n+\n+   Note that if the replacement involve a load, CUR_STMT is moved just after\n+   SRC_STMT to do the load with the same VUSE which can lead to CUR_STMT\n+   changing of basic block.  */\n+\n+bool\n+bswap_replace (gimple *cur_stmt, gimple *ins_stmt, tree fndecl,\n+\t       tree bswap_type, tree load_type, struct symbolic_number *n,\n+\t       bool bswap)\n+{\n+  gimple_stmt_iterator gsi;\n+  tree src, tmp, tgt;\n+  gimple *bswap_stmt;\n+\n+  gsi = gsi_for_stmt (cur_stmt);\n+  src = n->src;\n+  tgt = gimple_assign_lhs (cur_stmt);\n+\n+  /* Need to load the value from memory first.  */\n+  if (n->base_addr)\n+    {\n+      gimple_stmt_iterator gsi_ins = gsi_for_stmt (ins_stmt);\n+      tree addr_expr, addr_tmp, val_expr, val_tmp;\n+      tree load_offset_ptr, aligned_load_type;\n+      gimple *addr_stmt, *load_stmt;\n+      unsigned align;\n+      HOST_WIDE_INT load_offset = 0;\n+      basic_block ins_bb, cur_bb;\n+\n+      ins_bb = gimple_bb (ins_stmt);\n+      cur_bb = gimple_bb (cur_stmt);\n+      if (!dominated_by_p (CDI_DOMINATORS, cur_bb, ins_bb))\n+\treturn false;\n+\n+      align = get_object_alignment (src);\n+\n+      /* Move cur_stmt just before  one of the load of the original\n+\t to ensure it has the same VUSE.  See PR61517 for what could\n+\t go wrong.  */\n+      if (gimple_bb (cur_stmt) != gimple_bb (ins_stmt))\n+\treset_flow_sensitive_info (gimple_assign_lhs (cur_stmt));\n+      gsi_move_before (&gsi, &gsi_ins);\n+      gsi = gsi_for_stmt (cur_stmt);\n+\n+      /* Compute address to load from and cast according to the size\n+\t of the load.  */\n+      addr_expr = build_fold_addr_expr (unshare_expr (src));\n+      if (is_gimple_mem_ref_addr (addr_expr))\n+\taddr_tmp = addr_expr;\n+      else\n+\t{\n+\t  addr_tmp = make_temp_ssa_name (TREE_TYPE (addr_expr), NULL,\n+\t\t\t\t\t \"load_src\");\n+\t  addr_stmt = gimple_build_assign (addr_tmp, addr_expr);\n+\t  gsi_insert_before (&gsi, addr_stmt, GSI_SAME_STMT);\n+\t}\n+\n+      /* Perform the load.  */\n+      aligned_load_type = load_type;\n+      if (align < TYPE_ALIGN (load_type))\n+\taligned_load_type = build_aligned_type (load_type, align);\n+      load_offset_ptr = build_int_cst (n->alias_set, load_offset);\n+      val_expr = fold_build2 (MEM_REF, aligned_load_type, addr_tmp,\n+\t\t\t      load_offset_ptr);\n+\n+      if (!bswap)\n+\t{\n+\t  if (n->range == 16)\n+\t    nop_stats.found_16bit++;\n+\t  else if (n->range == 32)\n+\t    nop_stats.found_32bit++;\n+\t  else\n+\t    {\n+\t      gcc_assert (n->range == 64);\n+\t      nop_stats.found_64bit++;\n+\t    }\n+\n+\t  /* Convert the result of load if necessary.  */\n+\t  if (!useless_type_conversion_p (TREE_TYPE (tgt), load_type))\n+\t    {\n+\t      val_tmp = make_temp_ssa_name (aligned_load_type, NULL,\n+\t\t\t\t\t    \"load_dst\");\n+\t      load_stmt = gimple_build_assign (val_tmp, val_expr);\n+\t      gimple_set_vuse (load_stmt, n->vuse);\n+\t      gsi_insert_before (&gsi, load_stmt, GSI_SAME_STMT);\n+\t      gimple_assign_set_rhs_with_ops (&gsi, NOP_EXPR, val_tmp);\n+\t    }\n+\t  else\n+\t    {\n+\t      gimple_assign_set_rhs_with_ops (&gsi, MEM_REF, val_expr);\n+\t      gimple_set_vuse (cur_stmt, n->vuse);\n+\t    }\n+\t  update_stmt (cur_stmt);\n+\n+\t  if (dump_file)\n+\t    {\n+\t      fprintf (dump_file,\n+\t\t       \"%d bit load in target endianness found at: \",\n+\t\t       (int) n->range);\n+\t      print_gimple_stmt (dump_file, cur_stmt, 0);\n+\t    }\n+\t  return true;\n+\t}\n+      else\n+\t{\n+\t  val_tmp = make_temp_ssa_name (aligned_load_type, NULL, \"load_dst\");\n+\t  load_stmt = gimple_build_assign (val_tmp, val_expr);\n+\t  gimple_set_vuse (load_stmt, n->vuse);\n+\t  gsi_insert_before (&gsi, load_stmt, GSI_SAME_STMT);\n+\t}\n+      src = val_tmp;\n+    }\n+  else if (!bswap)\n+    {\n+      gimple *g;\n+      if (!useless_type_conversion_p (TREE_TYPE (tgt), TREE_TYPE (src)))\n+\t{\n+\t  if (!is_gimple_val (src))\n+\t    return false;\n+\t  g = gimple_build_assign (tgt, NOP_EXPR, src);\n+\t}\n+      else\n+\tg = gimple_build_assign (tgt, src);\n+      if (n->range == 16)\n+\tnop_stats.found_16bit++;\n+      else if (n->range == 32)\n+\tnop_stats.found_32bit++;\n+      else\n+\t{\n+\t  gcc_assert (n->range == 64);\n+\t  nop_stats.found_64bit++;\n+\t}\n+      if (dump_file)\n+\t{\n+\t  fprintf (dump_file,\n+\t\t   \"%d bit reshuffle in target endianness found at: \",\n+\t\t   (int) n->range);\n+\t  print_gimple_stmt (dump_file, cur_stmt, 0);\n+\t}\n+      gsi_replace (&gsi, g, true);\n+      return true;\n+    }\n+  else if (TREE_CODE (src) == BIT_FIELD_REF)\n+    src = TREE_OPERAND (src, 0);\n+\n+  if (n->range == 16)\n+    bswap_stats.found_16bit++;\n+  else if (n->range == 32)\n+    bswap_stats.found_32bit++;\n+  else\n+    {\n+      gcc_assert (n->range == 64);\n+      bswap_stats.found_64bit++;\n+    }\n+\n+  tmp = src;\n+\n+  /* Convert the src expression if necessary.  */\n+  if (!useless_type_conversion_p (TREE_TYPE (tmp), bswap_type))\n+    {\n+      gimple *convert_stmt;\n+\n+      tmp = make_temp_ssa_name (bswap_type, NULL, \"bswapsrc\");\n+      convert_stmt = gimple_build_assign (tmp, NOP_EXPR, src);\n+      gsi_insert_before (&gsi, convert_stmt, GSI_SAME_STMT);\n+    }\n+\n+  /* Canonical form for 16 bit bswap is a rotate expression.  Only 16bit values\n+     are considered as rotation of 2N bit values by N bits is generally not\n+     equivalent to a bswap.  Consider for instance 0x01020304 r>> 16 which\n+     gives 0x03040102 while a bswap for that value is 0x04030201.  */\n+  if (bswap && n->range == 16)\n+    {\n+      tree count = build_int_cst (NULL, BITS_PER_UNIT);\n+      src = fold_build2 (LROTATE_EXPR, bswap_type, tmp, count);\n+      bswap_stmt = gimple_build_assign (NULL, src);\n+    }\n+  else\n+    bswap_stmt = gimple_build_call (fndecl, 1, tmp);\n+\n+  tmp = tgt;\n+\n+  /* Convert the result if necessary.  */\n+  if (!useless_type_conversion_p (TREE_TYPE (tgt), bswap_type))\n+    {\n+      gimple *convert_stmt;\n+\n+      tmp = make_temp_ssa_name (bswap_type, NULL, \"bswapdst\");\n+      convert_stmt = gimple_build_assign (tgt, NOP_EXPR, tmp);\n+      gsi_insert_after (&gsi, convert_stmt, GSI_SAME_STMT);\n+    }\n+\n+  gimple_set_lhs (bswap_stmt, tmp);\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"%d bit bswap implementation found at: \",\n+\t       (int) n->range);\n+      print_gimple_stmt (dump_file, cur_stmt, 0);\n+    }\n+\n+  gsi_insert_after (&gsi, bswap_stmt, GSI_SAME_STMT);\n+  gsi_remove (&gsi, true);\n+  return true;\n+}\n+\n+/* Find manual byte swap implementations as well as load in a given\n+   endianness. Byte swaps are turned into a bswap builtin invokation\n+   while endian loads are converted to bswap builtin invokation or\n+   simple load according to the target endianness.  */\n+\n+unsigned int\n+pass_optimize_bswap::execute (function *fun)\n+{\n+  basic_block bb;\n+  bool bswap32_p, bswap64_p;\n+  bool changed = false;\n+  tree bswap32_type = NULL_TREE, bswap64_type = NULL_TREE;\n+\n+  bswap32_p = (builtin_decl_explicit_p (BUILT_IN_BSWAP32)\n+\t       && optab_handler (bswap_optab, SImode) != CODE_FOR_nothing);\n+  bswap64_p = (builtin_decl_explicit_p (BUILT_IN_BSWAP64)\n+\t       && (optab_handler (bswap_optab, DImode) != CODE_FOR_nothing\n+\t\t   || (bswap32_p && word_mode == SImode)));\n+\n+  /* Determine the argument type of the builtins.  The code later on\n+     assumes that the return and argument type are the same.  */\n+  if (bswap32_p)\n+    {\n+      tree fndecl = builtin_decl_explicit (BUILT_IN_BSWAP32);\n+      bswap32_type = TREE_VALUE (TYPE_ARG_TYPES (TREE_TYPE (fndecl)));\n+    }\n+\n+  if (bswap64_p)\n+    {\n+      tree fndecl = builtin_decl_explicit (BUILT_IN_BSWAP64);\n+      bswap64_type = TREE_VALUE (TYPE_ARG_TYPES (TREE_TYPE (fndecl)));\n+    }\n+\n+  memset (&nop_stats, 0, sizeof (nop_stats));\n+  memset (&bswap_stats, 0, sizeof (bswap_stats));\n+  calculate_dominance_info (CDI_DOMINATORS);\n+\n+  FOR_EACH_BB_FN (bb, fun)\n+    {\n+      gimple_stmt_iterator gsi;\n+\n+      /* We do a reverse scan for bswap patterns to make sure we get the\n+\t widest match. As bswap pattern matching doesn't handle previously\n+\t inserted smaller bswap replacements as sub-patterns, the wider\n+\t variant wouldn't be detected.  */\n+      for (gsi = gsi_last_bb (bb); !gsi_end_p (gsi);)\n+\t{\n+\t  gimple *ins_stmt, *cur_stmt = gsi_stmt (gsi);\n+\t  tree fndecl = NULL_TREE, bswap_type = NULL_TREE, load_type;\n+\t  enum tree_code code;\n+\t  struct symbolic_number n;\n+\t  bool bswap;\n+\n+\t  /* This gsi_prev (&gsi) is not part of the for loop because cur_stmt\n+\t     might be moved to a different basic block by bswap_replace and gsi\n+\t     must not points to it if that's the case.  Moving the gsi_prev\n+\t     there make sure that gsi points to the statement previous to\n+\t     cur_stmt while still making sure that all statements are\n+\t     considered in this basic block.  */\n+\t  gsi_prev (&gsi);\n+\n+\t  if (!is_gimple_assign (cur_stmt))\n+\t    continue;\n+\n+\t  code = gimple_assign_rhs_code (cur_stmt);\n+\t  switch (code)\n+\t    {\n+\t    case LROTATE_EXPR:\n+\t    case RROTATE_EXPR:\n+\t      if (!tree_fits_uhwi_p (gimple_assign_rhs2 (cur_stmt))\n+\t\t  || tree_to_uhwi (gimple_assign_rhs2 (cur_stmt))\n+\t\t     % BITS_PER_UNIT)\n+\t\tcontinue;\n+\t      /* Fall through.  */\n+\t    case BIT_IOR_EXPR:\n+\t      break;\n+\t    default:\n+\t      continue;\n+\t    }\n+\n+\t  ins_stmt = find_bswap_or_nop (cur_stmt, &n, &bswap);\n+\n+\t  if (!ins_stmt)\n+\t    continue;\n+\n+\t  switch (n.range)\n+\t    {\n+\t    case 16:\n+\t      /* Already in canonical form, nothing to do.  */\n+\t      if (code == LROTATE_EXPR || code == RROTATE_EXPR)\n+\t\tcontinue;\n+\t      load_type = bswap_type = uint16_type_node;\n+\t      break;\n+\t    case 32:\n+\t      load_type = uint32_type_node;\n+\t      if (bswap32_p)\n+\t\t{\n+\t\t  fndecl = builtin_decl_explicit (BUILT_IN_BSWAP32);\n+\t\t  bswap_type = bswap32_type;\n+\t\t}\n+\t      break;\n+\t    case 64:\n+\t      load_type = uint64_type_node;\n+\t      if (bswap64_p)\n+\t\t{\n+\t\t  fndecl = builtin_decl_explicit (BUILT_IN_BSWAP64);\n+\t\t  bswap_type = bswap64_type;\n+\t\t}\n+\t      break;\n+\t    default:\n+\t      continue;\n+\t    }\n+\n+\t  if (bswap && !fndecl && n.range != 16)\n+\t    continue;\n+\n+\t  if (bswap_replace (cur_stmt, ins_stmt, fndecl, bswap_type, load_type,\n+\t\t\t     &n, bswap))\n+\t    changed = true;\n+\t}\n+    }\n+\n+  statistics_counter_event (fun, \"16-bit nop implementations found\",\n+\t\t\t    nop_stats.found_16bit);\n+  statistics_counter_event (fun, \"32-bit nop implementations found\",\n+\t\t\t    nop_stats.found_32bit);\n+  statistics_counter_event (fun, \"64-bit nop implementations found\",\n+\t\t\t    nop_stats.found_64bit);\n+  statistics_counter_event (fun, \"16-bit bswap implementations found\",\n+\t\t\t    bswap_stats.found_16bit);\n+  statistics_counter_event (fun, \"32-bit bswap implementations found\",\n+\t\t\t    bswap_stats.found_32bit);\n+  statistics_counter_event (fun, \"64-bit bswap implementations found\",\n+\t\t\t    bswap_stats.found_64bit);\n+\n+  return (changed ? TODO_update_ssa : 0);\n+}\n+\n+} // anon namespace\n+\n+gimple_opt_pass *\n+make_pass_optimize_bswap (gcc::context *ctxt)\n+{\n+  return new pass_optimize_bswap (ctxt);\n+}\n+\n+namespace {\n+\n /* Struct recording one operand for the store, which is either a constant,\n    then VAL represents the constant and all the other fields are zero,\n    or a memory load, then VAL represents the reference, BASE_ADDR is non-NULL"}, {"sha": "07030439c7add3d26272571201ce6f32ed516aa1", "filename": "gcc/tree-ssa-math-opts.c", "status": "modified", "additions": 0, "deletions": 1076, "changes": 1076, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/dffec8ebdb449be77bf02fe0cf59237362be991a/gcc%2Ftree-ssa-math-opts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/dffec8ebdb449be77bf02fe0cf59237362be991a/gcc%2Ftree-ssa-math-opts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-math-opts.c?ref=dffec8ebdb449be77bf02fe0cf59237362be991a", "patch": "@@ -165,18 +165,6 @@ static struct\n   int inserted;\n } sincos_stats;\n \n-static struct\n-{\n-  /* Number of hand-written 16-bit nop / bswaps found.  */\n-  int found_16bit;\n-\n-  /* Number of hand-written 32-bit nop / bswaps found.  */\n-  int found_32bit;\n-\n-  /* Number of hand-written 64-bit nop / bswaps found.  */\n-  int found_64bit;\n-} nop_stats, bswap_stats;\n-\n static struct\n {\n   /* Number of widening multiplication ops inserted.  */\n@@ -1934,1070 +1922,6 @@ make_pass_cse_sincos (gcc::context *ctxt)\n   return new pass_cse_sincos (ctxt);\n }\n \n-/* A symbolic number structure is used to detect byte permutation and selection\n-   patterns of a source.  To achieve that, its field N contains an artificial\n-   number consisting of BITS_PER_MARKER sized markers tracking where does each\n-   byte come from in the source:\n-\n-   0\t   - target byte has the value 0\n-   FF\t   - target byte has an unknown value (eg. due to sign extension)\n-   1..size - marker value is the byte index in the source (0 for lsb).\n-\n-   To detect permutations on memory sources (arrays and structures), a symbolic\n-   number is also associated:\n-   - a base address BASE_ADDR and an OFFSET giving the address of the source;\n-   - a range which gives the difference between the highest and lowest accessed\n-     memory location to make such a symbolic number;\n-   - the address SRC of the source element of lowest address as a convenience\n-     to easily get BASE_ADDR + offset + lowest bytepos;\n-   - number of expressions N_OPS bitwise ored together to represent\n-     approximate cost of the computation.\n-\n-   Note 1: the range is different from size as size reflects the size of the\n-   type of the current expression.  For instance, for an array char a[],\n-   (short) a[0] | (short) a[3] would have a size of 2 but a range of 4 while\n-   (short) a[0] | ((short) a[0] << 1) would still have a size of 2 but this\n-   time a range of 1.\n-\n-   Note 2: for non-memory sources, range holds the same value as size.\n-\n-   Note 3: SRC points to the SSA_NAME in case of non-memory source.  */\n-\n-struct symbolic_number {\n-  uint64_t n;\n-  tree type;\n-  tree base_addr;\n-  tree offset;\n-  HOST_WIDE_INT bytepos;\n-  tree src;\n-  tree alias_set;\n-  tree vuse;\n-  unsigned HOST_WIDE_INT range;\n-  int n_ops;\n-};\n-\n-#define BITS_PER_MARKER 8\n-#define MARKER_MASK ((1 << BITS_PER_MARKER) - 1)\n-#define MARKER_BYTE_UNKNOWN MARKER_MASK\n-#define HEAD_MARKER(n, size) \\\n-  ((n) & ((uint64_t) MARKER_MASK << (((size) - 1) * BITS_PER_MARKER)))\n-\n-/* The number which the find_bswap_or_nop_1 result should match in\n-   order to have a nop.  The number is masked according to the size of\n-   the symbolic number before using it.  */\n-#define CMPNOP (sizeof (int64_t) < 8 ? 0 : \\\n-  (uint64_t)0x08070605 << 32 | 0x04030201)\n-\n-/* The number which the find_bswap_or_nop_1 result should match in\n-   order to have a byte swap.  The number is masked according to the\n-   size of the symbolic number before using it.  */\n-#define CMPXCHG (sizeof (int64_t) < 8 ? 0 : \\\n-  (uint64_t)0x01020304 << 32 | 0x05060708)\n-\n-/* Perform a SHIFT or ROTATE operation by COUNT bits on symbolic\n-   number N.  Return false if the requested operation is not permitted\n-   on a symbolic number.  */\n-\n-static inline bool\n-do_shift_rotate (enum tree_code code,\n-\t\t struct symbolic_number *n,\n-\t\t int count)\n-{\n-  int i, size = TYPE_PRECISION (n->type) / BITS_PER_UNIT;\n-  unsigned head_marker;\n-\n-  if (count % BITS_PER_UNIT != 0)\n-    return false;\n-  count = (count / BITS_PER_UNIT) * BITS_PER_MARKER;\n-\n-  /* Zero out the extra bits of N in order to avoid them being shifted\n-     into the significant bits.  */\n-  if (size < 64 / BITS_PER_MARKER)\n-    n->n &= ((uint64_t) 1 << (size * BITS_PER_MARKER)) - 1;\n-\n-  switch (code)\n-    {\n-    case LSHIFT_EXPR:\n-      n->n <<= count;\n-      break;\n-    case RSHIFT_EXPR:\n-      head_marker = HEAD_MARKER (n->n, size);\n-      n->n >>= count;\n-      /* Arithmetic shift of signed type: result is dependent on the value.  */\n-      if (!TYPE_UNSIGNED (n->type) && head_marker)\n-\tfor (i = 0; i < count / BITS_PER_MARKER; i++)\n-\t  n->n |= (uint64_t) MARKER_BYTE_UNKNOWN\n-\t\t  << ((size - 1 - i) * BITS_PER_MARKER);\n-      break;\n-    case LROTATE_EXPR:\n-      n->n = (n->n << count) | (n->n >> ((size * BITS_PER_MARKER) - count));\n-      break;\n-    case RROTATE_EXPR:\n-      n->n = (n->n >> count) | (n->n << ((size * BITS_PER_MARKER) - count));\n-      break;\n-    default:\n-      return false;\n-    }\n-  /* Zero unused bits for size.  */\n-  if (size < 64 / BITS_PER_MARKER)\n-    n->n &= ((uint64_t) 1 << (size * BITS_PER_MARKER)) - 1;\n-  return true;\n-}\n-\n-/* Perform sanity checking for the symbolic number N and the gimple\n-   statement STMT.  */\n-\n-static inline bool\n-verify_symbolic_number_p (struct symbolic_number *n, gimple *stmt)\n-{\n-  tree lhs_type;\n-\n-  lhs_type = gimple_expr_type (stmt);\n-\n-  if (TREE_CODE (lhs_type) != INTEGER_TYPE)\n-    return false;\n-\n-  if (TYPE_PRECISION (lhs_type) != TYPE_PRECISION (n->type))\n-    return false;\n-\n-  return true;\n-}\n-\n-/* Initialize the symbolic number N for the bswap pass from the base element\n-   SRC manipulated by the bitwise OR expression.  */\n-\n-static bool\n-init_symbolic_number (struct symbolic_number *n, tree src)\n-{\n-  int size;\n-\n-  if (! INTEGRAL_TYPE_P (TREE_TYPE (src)))\n-    return false;\n-\n-  n->base_addr = n->offset = n->alias_set = n->vuse = NULL_TREE;\n-  n->src = src;\n-\n-  /* Set up the symbolic number N by setting each byte to a value between 1 and\n-     the byte size of rhs1.  The highest order byte is set to n->size and the\n-     lowest order byte to 1.  */\n-  n->type = TREE_TYPE (src);\n-  size = TYPE_PRECISION (n->type);\n-  if (size % BITS_PER_UNIT != 0)\n-    return false;\n-  size /= BITS_PER_UNIT;\n-  if (size > 64 / BITS_PER_MARKER)\n-    return false;\n-  n->range = size;\n-  n->n = CMPNOP;\n-  n->n_ops = 1;\n-\n-  if (size < 64 / BITS_PER_MARKER)\n-    n->n &= ((uint64_t) 1 << (size * BITS_PER_MARKER)) - 1;\n-\n-  return true;\n-}\n-\n-/* Check if STMT might be a byte swap or a nop from a memory source and returns\n-   the answer. If so, REF is that memory source and the base of the memory area\n-   accessed and the offset of the access from that base are recorded in N.  */\n-\n-bool\n-find_bswap_or_nop_load (gimple *stmt, tree ref, struct symbolic_number *n)\n-{\n-  /* Leaf node is an array or component ref. Memorize its base and\n-     offset from base to compare to other such leaf node.  */\n-  HOST_WIDE_INT bitsize, bitpos;\n-  machine_mode mode;\n-  int unsignedp, reversep, volatilep;\n-  tree offset, base_addr;\n-\n-  /* Not prepared to handle PDP endian.  */\n-  if (BYTES_BIG_ENDIAN != WORDS_BIG_ENDIAN)\n-    return false;\n-\n-  if (!gimple_assign_load_p (stmt) || gimple_has_volatile_ops (stmt))\n-    return false;\n-\n-  base_addr = get_inner_reference (ref, &bitsize, &bitpos, &offset, &mode,\n-\t\t\t\t   &unsignedp, &reversep, &volatilep);\n-\n-  if (TREE_CODE (base_addr) == MEM_REF)\n-    {\n-      offset_int bit_offset = 0;\n-      tree off = TREE_OPERAND (base_addr, 1);\n-\n-      if (!integer_zerop (off))\n-\t{\n-\t  offset_int boff, coff = mem_ref_offset (base_addr);\n-\t  boff = coff << LOG2_BITS_PER_UNIT;\n-\t  bit_offset += boff;\n-\t}\n-\n-      base_addr = TREE_OPERAND (base_addr, 0);\n-\n-      /* Avoid returning a negative bitpos as this may wreak havoc later.  */\n-      if (wi::neg_p (bit_offset))\n-\t{\n-\t  offset_int mask = wi::mask <offset_int> (LOG2_BITS_PER_UNIT, false);\n-\t  offset_int tem = wi::bit_and_not (bit_offset, mask);\n-\t  /* TEM is the bitpos rounded to BITS_PER_UNIT towards -Inf.\n-\t     Subtract it to BIT_OFFSET and add it (scaled) to OFFSET.  */\n-\t  bit_offset -= tem;\n-\t  tem >>= LOG2_BITS_PER_UNIT;\n-\t  if (offset)\n-\t    offset = size_binop (PLUS_EXPR, offset,\n-\t\t\t\t    wide_int_to_tree (sizetype, tem));\n-\t  else\n-\t    offset = wide_int_to_tree (sizetype, tem);\n-\t}\n-\n-      bitpos += bit_offset.to_shwi ();\n-    }\n-\n-  if (bitpos % BITS_PER_UNIT)\n-    return false;\n-  if (bitsize % BITS_PER_UNIT)\n-    return false;\n-  if (reversep)\n-    return false;\n-\n-  if (!init_symbolic_number (n, ref))\n-    return false;\n-  n->base_addr = base_addr;\n-  n->offset = offset;\n-  n->bytepos = bitpos / BITS_PER_UNIT;\n-  n->alias_set = reference_alias_ptr_type (ref);\n-  n->vuse = gimple_vuse (stmt);\n-  return true;\n-}\n-\n-/* Compute the symbolic number N representing the result of a bitwise OR on 2\n-   symbolic number N1 and N2 whose source statements are respectively\n-   SOURCE_STMT1 and SOURCE_STMT2.  */\n-\n-static gimple *\n-perform_symbolic_merge (gimple *source_stmt1, struct symbolic_number *n1,\n-\t\t\tgimple *source_stmt2, struct symbolic_number *n2,\n-\t\t\tstruct symbolic_number *n)\n-{\n-  int i, size;\n-  uint64_t mask;\n-  gimple *source_stmt;\n-  struct symbolic_number *n_start;\n-\n-  tree rhs1 = gimple_assign_rhs1 (source_stmt1);\n-  if (TREE_CODE (rhs1) == BIT_FIELD_REF\n-      && TREE_CODE (TREE_OPERAND (rhs1, 0)) == SSA_NAME)\n-    rhs1 = TREE_OPERAND (rhs1, 0);\n-  tree rhs2 = gimple_assign_rhs1 (source_stmt2);\n-  if (TREE_CODE (rhs2) == BIT_FIELD_REF\n-      && TREE_CODE (TREE_OPERAND (rhs2, 0)) == SSA_NAME)\n-    rhs2 = TREE_OPERAND (rhs2, 0);\n-\n-  /* Sources are different, cancel bswap if they are not memory location with\n-     the same base (array, structure, ...).  */\n-  if (rhs1 != rhs2)\n-    {\n-      uint64_t inc;\n-      HOST_WIDE_INT start_sub, end_sub, end1, end2, end;\n-      struct symbolic_number *toinc_n_ptr, *n_end;\n-      basic_block bb1, bb2;\n-\n-      if (!n1->base_addr || !n2->base_addr\n-\t  || !operand_equal_p (n1->base_addr, n2->base_addr, 0))\n-\treturn NULL;\n-\n-      if (!n1->offset != !n2->offset\n-\t  || (n1->offset && !operand_equal_p (n1->offset, n2->offset, 0)))\n-\treturn NULL;\n-\n-      if (n1->bytepos < n2->bytepos)\n-\t{\n-\t  n_start = n1;\n-\t  start_sub = n2->bytepos - n1->bytepos;\n-\t}\n-      else\n-\t{\n-\t  n_start = n2;\n-\t  start_sub = n1->bytepos - n2->bytepos;\n-\t}\n-\n-      bb1 = gimple_bb (source_stmt1);\n-      bb2 = gimple_bb (source_stmt2);\n-      if (dominated_by_p (CDI_DOMINATORS, bb1, bb2))\n-\tsource_stmt = source_stmt1;\n-      else\n-\tsource_stmt = source_stmt2;\n-\n-      /* Find the highest address at which a load is performed and\n-\t compute related info.  */\n-      end1 = n1->bytepos + (n1->range - 1);\n-      end2 = n2->bytepos + (n2->range - 1);\n-      if (end1 < end2)\n-\t{\n-\t  end = end2;\n-\t  end_sub = end2 - end1;\n-\t}\n-      else\n-\t{\n-\t  end = end1;\n-\t  end_sub = end1 - end2;\n-\t}\n-      n_end = (end2 > end1) ? n2 : n1;\n-\n-      /* Find symbolic number whose lsb is the most significant.  */\n-      if (BYTES_BIG_ENDIAN)\n-\ttoinc_n_ptr = (n_end == n1) ? n2 : n1;\n-      else\n-\ttoinc_n_ptr = (n_start == n1) ? n2 : n1;\n-\n-      n->range = end - n_start->bytepos + 1;\n-\n-      /* Check that the range of memory covered can be represented by\n-\t a symbolic number.  */\n-      if (n->range > 64 / BITS_PER_MARKER)\n-\treturn NULL;\n-\n-      /* Reinterpret byte marks in symbolic number holding the value of\n-\t bigger weight according to target endianness.  */\n-      inc = BYTES_BIG_ENDIAN ? end_sub : start_sub;\n-      size = TYPE_PRECISION (n1->type) / BITS_PER_UNIT;\n-      for (i = 0; i < size; i++, inc <<= BITS_PER_MARKER)\n-\t{\n-\t  unsigned marker\n-\t    = (toinc_n_ptr->n >> (i * BITS_PER_MARKER)) & MARKER_MASK;\n-\t  if (marker && marker != MARKER_BYTE_UNKNOWN)\n-\t    toinc_n_ptr->n += inc;\n-\t}\n-    }\n-  else\n-    {\n-      n->range = n1->range;\n-      n_start = n1;\n-      source_stmt = source_stmt1;\n-    }\n-\n-  if (!n1->alias_set\n-      || alias_ptr_types_compatible_p (n1->alias_set, n2->alias_set))\n-    n->alias_set = n1->alias_set;\n-  else\n-    n->alias_set = ptr_type_node;\n-  n->vuse = n_start->vuse;\n-  n->base_addr = n_start->base_addr;\n-  n->offset = n_start->offset;\n-  n->src = n_start->src;\n-  n->bytepos = n_start->bytepos;\n-  n->type = n_start->type;\n-  size = TYPE_PRECISION (n->type) / BITS_PER_UNIT;\n-\n-  for (i = 0, mask = MARKER_MASK; i < size; i++, mask <<= BITS_PER_MARKER)\n-    {\n-      uint64_t masked1, masked2;\n-\n-      masked1 = n1->n & mask;\n-      masked2 = n2->n & mask;\n-      if (masked1 && masked2 && masked1 != masked2)\n-\treturn NULL;\n-    }\n-  n->n = n1->n | n2->n;\n-  n->n_ops = n1->n_ops + n2->n_ops;\n-\n-  return source_stmt;\n-}\n-\n-/* find_bswap_or_nop_1 invokes itself recursively with N and tries to perform\n-   the operation given by the rhs of STMT on the result.  If the operation\n-   could successfully be executed the function returns a gimple stmt whose\n-   rhs's first tree is the expression of the source operand and NULL\n-   otherwise.  */\n-\n-static gimple *\n-find_bswap_or_nop_1 (gimple *stmt, struct symbolic_number *n, int limit)\n-{\n-  enum tree_code code;\n-  tree rhs1, rhs2 = NULL;\n-  gimple *rhs1_stmt, *rhs2_stmt, *source_stmt1;\n-  enum gimple_rhs_class rhs_class;\n-\n-  if (!limit || !is_gimple_assign (stmt))\n-    return NULL;\n-\n-  rhs1 = gimple_assign_rhs1 (stmt);\n-\n-  if (find_bswap_or_nop_load (stmt, rhs1, n))\n-    return stmt;\n-\n-  /* Handle BIT_FIELD_REF.  */\n-  if (TREE_CODE (rhs1) == BIT_FIELD_REF\n-      && TREE_CODE (TREE_OPERAND (rhs1, 0)) == SSA_NAME)\n-    {\n-      unsigned HOST_WIDE_INT bitsize = tree_to_uhwi (TREE_OPERAND (rhs1, 1));\n-      unsigned HOST_WIDE_INT bitpos = tree_to_uhwi (TREE_OPERAND (rhs1, 2));\n-      if (bitpos % BITS_PER_UNIT == 0\n-\t  && bitsize % BITS_PER_UNIT == 0\n-\t  && init_symbolic_number (n, TREE_OPERAND (rhs1, 0)))\n-\t{\n-\t  /* Handle big-endian bit numbering in BIT_FIELD_REF.  */\n-\t  if (BYTES_BIG_ENDIAN)\n-\t    bitpos = TYPE_PRECISION (n->type) - bitpos - bitsize;\n-\n-\t  /* Shift.  */\n-\t  if (!do_shift_rotate (RSHIFT_EXPR, n, bitpos))\n-\t    return NULL;\n-\n-\t  /* Mask.  */\n-\t  uint64_t mask = 0;\n-\t  uint64_t tmp = (1 << BITS_PER_UNIT) - 1;\n-\t  for (unsigned i = 0; i < bitsize / BITS_PER_UNIT;\n-\t       i++, tmp <<= BITS_PER_UNIT)\n-\t    mask |= (uint64_t) MARKER_MASK << (i * BITS_PER_MARKER);\n-\t  n->n &= mask;\n-\n-\t  /* Convert.  */\n-\t  n->type = TREE_TYPE (rhs1);\n-\t  if (!n->base_addr)\n-\t    n->range = TYPE_PRECISION (n->type) / BITS_PER_UNIT;\n-\n-\t  return verify_symbolic_number_p (n, stmt) ? stmt : NULL;\n-\t}\n-\n-      return NULL;\n-    }\n-\n-  if (TREE_CODE (rhs1) != SSA_NAME)\n-    return NULL;\n-\n-  code = gimple_assign_rhs_code (stmt);\n-  rhs_class = gimple_assign_rhs_class (stmt);\n-  rhs1_stmt = SSA_NAME_DEF_STMT (rhs1);\n-\n-  if (rhs_class == GIMPLE_BINARY_RHS)\n-    rhs2 = gimple_assign_rhs2 (stmt);\n-\n-  /* Handle unary rhs and binary rhs with integer constants as second\n-     operand.  */\n-\n-  if (rhs_class == GIMPLE_UNARY_RHS\n-      || (rhs_class == GIMPLE_BINARY_RHS\n-\t  && TREE_CODE (rhs2) == INTEGER_CST))\n-    {\n-      if (code != BIT_AND_EXPR\n-\t  && code != LSHIFT_EXPR\n-\t  && code != RSHIFT_EXPR\n-\t  && code != LROTATE_EXPR\n-\t  && code != RROTATE_EXPR\n-\t  && !CONVERT_EXPR_CODE_P (code))\n-\treturn NULL;\n-\n-      source_stmt1 = find_bswap_or_nop_1 (rhs1_stmt, n, limit - 1);\n-\n-      /* If find_bswap_or_nop_1 returned NULL, STMT is a leaf node and\n-\t we have to initialize the symbolic number.  */\n-      if (!source_stmt1)\n-\t{\n-\t  if (gimple_assign_load_p (stmt)\n-\t      || !init_symbolic_number (n, rhs1))\n-\t    return NULL;\n-\t  source_stmt1 = stmt;\n-\t}\n-\n-      switch (code)\n-\t{\n-\tcase BIT_AND_EXPR:\n-\t  {\n-\t    int i, size = TYPE_PRECISION (n->type) / BITS_PER_UNIT;\n-\t    uint64_t val = int_cst_value (rhs2), mask = 0;\n-\t    uint64_t tmp = (1 << BITS_PER_UNIT) - 1;\n-\n-\t    /* Only constants masking full bytes are allowed.  */\n-\t    for (i = 0; i < size; i++, tmp <<= BITS_PER_UNIT)\n-\t      if ((val & tmp) != 0 && (val & tmp) != tmp)\n-\t\treturn NULL;\n-\t      else if (val & tmp)\n-\t\tmask |= (uint64_t) MARKER_MASK << (i * BITS_PER_MARKER);\n-\n-\t    n->n &= mask;\n-\t  }\n-\t  break;\n-\tcase LSHIFT_EXPR:\n-\tcase RSHIFT_EXPR:\n-\tcase LROTATE_EXPR:\n-\tcase RROTATE_EXPR:\n-\t  if (!do_shift_rotate (code, n, (int) TREE_INT_CST_LOW (rhs2)))\n-\t    return NULL;\n-\t  break;\n-\tCASE_CONVERT:\n-\t  {\n-\t    int i, type_size, old_type_size;\n-\t    tree type;\n-\n-\t    type = gimple_expr_type (stmt);\n-\t    type_size = TYPE_PRECISION (type);\n-\t    if (type_size % BITS_PER_UNIT != 0)\n-\t      return NULL;\n-\t    type_size /= BITS_PER_UNIT;\n-\t    if (type_size > 64 / BITS_PER_MARKER)\n-\t      return NULL;\n-\n-\t    /* Sign extension: result is dependent on the value.  */\n-\t    old_type_size = TYPE_PRECISION (n->type) / BITS_PER_UNIT;\n-\t    if (!TYPE_UNSIGNED (n->type) && type_size > old_type_size\n-\t\t&& HEAD_MARKER (n->n, old_type_size))\n-\t      for (i = 0; i < type_size - old_type_size; i++)\n-\t\tn->n |= (uint64_t) MARKER_BYTE_UNKNOWN\n-\t\t\t<< ((type_size - 1 - i) * BITS_PER_MARKER);\n-\n-\t    if (type_size < 64 / BITS_PER_MARKER)\n-\t      {\n-\t\t/* If STMT casts to a smaller type mask out the bits not\n-\t\t   belonging to the target type.  */\n-\t\tn->n &= ((uint64_t) 1 << (type_size * BITS_PER_MARKER)) - 1;\n-\t      }\n-\t    n->type = type;\n-\t    if (!n->base_addr)\n-\t      n->range = type_size;\n-\t  }\n-\t  break;\n-\tdefault:\n-\t  return NULL;\n-\t};\n-      return verify_symbolic_number_p (n, stmt) ? source_stmt1 : NULL;\n-    }\n-\n-  /* Handle binary rhs.  */\n-\n-  if (rhs_class == GIMPLE_BINARY_RHS)\n-    {\n-      struct symbolic_number n1, n2;\n-      gimple *source_stmt, *source_stmt2;\n-\n-      if (code != BIT_IOR_EXPR)\n-\treturn NULL;\n-\n-      if (TREE_CODE (rhs2) != SSA_NAME)\n-\treturn NULL;\n-\n-      rhs2_stmt = SSA_NAME_DEF_STMT (rhs2);\n-\n-      switch (code)\n-\t{\n-\tcase BIT_IOR_EXPR:\n-\t  source_stmt1 = find_bswap_or_nop_1 (rhs1_stmt, &n1, limit - 1);\n-\n-\t  if (!source_stmt1)\n-\t    return NULL;\n-\n-\t  source_stmt2 = find_bswap_or_nop_1 (rhs2_stmt, &n2, limit - 1);\n-\n-\t  if (!source_stmt2)\n-\t    return NULL;\n-\n-\t  if (TYPE_PRECISION (n1.type) != TYPE_PRECISION (n2.type))\n-\t    return NULL;\n-\n-\t  if (!n1.vuse != !n2.vuse\n-\t      || (n1.vuse && !operand_equal_p (n1.vuse, n2.vuse, 0)))\n-\t    return NULL;\n-\n-\t  source_stmt\n-\t    = perform_symbolic_merge (source_stmt1, &n1, source_stmt2, &n2, n);\n-\n-\t  if (!source_stmt)\n-\t    return NULL;\n-\n-\t  if (!verify_symbolic_number_p (n, stmt))\n-\t    return NULL;\n-\n-\t  break;\n-\tdefault:\n-\t  return NULL;\n-\t}\n-      return source_stmt;\n-    }\n-  return NULL;\n-}\n-\n-/* Check if STMT completes a bswap implementation or a read in a given\n-   endianness consisting of ORs, SHIFTs and ANDs and sets *BSWAP\n-   accordingly.  It also sets N to represent the kind of operations\n-   performed: size of the resulting expression and whether it works on\n-   a memory source, and if so alias-set and vuse.  At last, the\n-   function returns a stmt whose rhs's first tree is the source\n-   expression.  */\n-\n-static gimple *\n-find_bswap_or_nop (gimple *stmt, struct symbolic_number *n, bool *bswap)\n-{\n-  unsigned rsize;\n-  uint64_t tmpn, mask;\n-/* The number which the find_bswap_or_nop_1 result should match in order\n-   to have a full byte swap.  The number is shifted to the right\n-   according to the size of the symbolic number before using it.  */\n-  uint64_t cmpxchg = CMPXCHG;\n-  uint64_t cmpnop = CMPNOP;\n-\n-  gimple *ins_stmt;\n-  int limit;\n-\n-  /* The last parameter determines the depth search limit.  It usually\n-     correlates directly to the number n of bytes to be touched.  We\n-     increase that number by log2(n) + 1 here in order to also\n-     cover signed -> unsigned conversions of the src operand as can be seen\n-     in libgcc, and for initial shift/and operation of the src operand.  */\n-  limit = TREE_INT_CST_LOW (TYPE_SIZE_UNIT (gimple_expr_type (stmt)));\n-  limit += 1 + (int) ceil_log2 ((unsigned HOST_WIDE_INT) limit);\n-  ins_stmt = find_bswap_or_nop_1 (stmt, n, limit);\n-\n-  if (!ins_stmt)\n-    return NULL;\n-\n-  /* Find real size of result (highest non-zero byte).  */\n-  if (n->base_addr)\n-    for (tmpn = n->n, rsize = 0; tmpn; tmpn >>= BITS_PER_MARKER, rsize++);\n-  else\n-    rsize = n->range;\n-\n-  /* Zero out the bits corresponding to untouched bytes in original gimple\n-     expression.  */\n-  if (n->range < (int) sizeof (int64_t))\n-    {\n-      mask = ((uint64_t) 1 << (n->range * BITS_PER_MARKER)) - 1;\n-      cmpxchg >>= (64 / BITS_PER_MARKER - n->range) * BITS_PER_MARKER;\n-      cmpnop &= mask;\n-    }\n-\n-  /* Zero out the bits corresponding to unused bytes in the result of the\n-     gimple expression.  */\n-  if (rsize < n->range)\n-    {\n-      if (BYTES_BIG_ENDIAN)\n-\t{\n-\t  mask = ((uint64_t) 1 << (rsize * BITS_PER_MARKER)) - 1;\n-\t  cmpxchg &= mask;\n-\t  cmpnop >>= (n->range - rsize) * BITS_PER_MARKER;\n-\t}\n-      else\n-\t{\n-\t  mask = ((uint64_t) 1 << (rsize * BITS_PER_MARKER)) - 1;\n-\t  cmpxchg >>= (n->range - rsize) * BITS_PER_MARKER;\n-\t  cmpnop &= mask;\n-\t}\n-      n->range = rsize;\n-    }\n-\n-  /* A complete byte swap should make the symbolic number to start with\n-     the largest digit in the highest order byte. Unchanged symbolic\n-     number indicates a read with same endianness as target architecture.  */\n-  if (n->n == cmpnop)\n-    *bswap = false;\n-  else if (n->n == cmpxchg)\n-    *bswap = true;\n-  else\n-    return NULL;\n-\n-  /* Useless bit manipulation performed by code.  */\n-  if (!n->base_addr && n->n == cmpnop && n->n_ops == 1)\n-    return NULL;\n-\n-  n->range *= BITS_PER_UNIT;\n-  return ins_stmt;\n-}\n-\n-namespace {\n-\n-const pass_data pass_data_optimize_bswap =\n-{\n-  GIMPLE_PASS, /* type */\n-  \"bswap\", /* name */\n-  OPTGROUP_NONE, /* optinfo_flags */\n-  TV_NONE, /* tv_id */\n-  PROP_ssa, /* properties_required */\n-  0, /* properties_provided */\n-  0, /* properties_destroyed */\n-  0, /* todo_flags_start */\n-  0, /* todo_flags_finish */\n-};\n-\n-class pass_optimize_bswap : public gimple_opt_pass\n-{\n-public:\n-  pass_optimize_bswap (gcc::context *ctxt)\n-    : gimple_opt_pass (pass_data_optimize_bswap, ctxt)\n-  {}\n-\n-  /* opt_pass methods: */\n-  virtual bool gate (function *)\n-    {\n-      return flag_expensive_optimizations && optimize;\n-    }\n-\n-  virtual unsigned int execute (function *);\n-\n-}; // class pass_optimize_bswap\n-\n-/* Perform the bswap optimization: replace the expression computed in the rhs\n-   of CUR_STMT by an equivalent bswap, load or load + bswap expression.\n-   Which of these alternatives replace the rhs is given by N->base_addr (non\n-   null if a load is needed) and BSWAP.  The type, VUSE and set-alias of the\n-   load to perform are also given in N while the builtin bswap invoke is given\n-   in FNDEL.  Finally, if a load is involved, SRC_STMT refers to one of the\n-   load statements involved to construct the rhs in CUR_STMT and N->range gives\n-   the size of the rhs expression for maintaining some statistics.\n-\n-   Note that if the replacement involve a load, CUR_STMT is moved just after\n-   SRC_STMT to do the load with the same VUSE which can lead to CUR_STMT\n-   changing of basic block.  */\n-\n-static bool\n-bswap_replace (gimple *cur_stmt, gimple *ins_stmt, tree fndecl,\n-\t       tree bswap_type, tree load_type, struct symbolic_number *n,\n-\t       bool bswap)\n-{\n-  gimple_stmt_iterator gsi;\n-  tree src, tmp, tgt;\n-  gimple *bswap_stmt;\n-\n-  gsi = gsi_for_stmt (cur_stmt);\n-  src = n->src;\n-  tgt = gimple_assign_lhs (cur_stmt);\n-\n-  /* Need to load the value from memory first.  */\n-  if (n->base_addr)\n-    {\n-      gimple_stmt_iterator gsi_ins = gsi_for_stmt (ins_stmt);\n-      tree addr_expr, addr_tmp, val_expr, val_tmp;\n-      tree load_offset_ptr, aligned_load_type;\n-      gimple *addr_stmt, *load_stmt;\n-      unsigned align;\n-      HOST_WIDE_INT load_offset = 0;\n-      basic_block ins_bb, cur_bb;\n-\n-      ins_bb = gimple_bb (ins_stmt);\n-      cur_bb = gimple_bb (cur_stmt);\n-      if (!dominated_by_p (CDI_DOMINATORS, cur_bb, ins_bb))\n-\treturn false;\n-\n-      align = get_object_alignment (src);\n-\n-      /* Move cur_stmt just before  one of the load of the original\n-\t to ensure it has the same VUSE.  See PR61517 for what could\n-\t go wrong.  */\n-      if (gimple_bb (cur_stmt) != gimple_bb (ins_stmt))\n-\treset_flow_sensitive_info (gimple_assign_lhs (cur_stmt));\n-      gsi_move_before (&gsi, &gsi_ins);\n-      gsi = gsi_for_stmt (cur_stmt);\n-\n-      /* Compute address to load from and cast according to the size\n-\t of the load.  */\n-      addr_expr = build_fold_addr_expr (unshare_expr (src));\n-      if (is_gimple_mem_ref_addr (addr_expr))\n-\taddr_tmp = addr_expr;\n-      else\n-\t{\n-\t  addr_tmp = make_temp_ssa_name (TREE_TYPE (addr_expr), NULL,\n-\t\t\t\t\t \"load_src\");\n-\t  addr_stmt = gimple_build_assign (addr_tmp, addr_expr);\n-\t  gsi_insert_before (&gsi, addr_stmt, GSI_SAME_STMT);\n-\t}\n-\n-      /* Perform the load.  */\n-      aligned_load_type = load_type;\n-      if (align < TYPE_ALIGN (load_type))\n-\taligned_load_type = build_aligned_type (load_type, align);\n-      load_offset_ptr = build_int_cst (n->alias_set, load_offset);\n-      val_expr = fold_build2 (MEM_REF, aligned_load_type, addr_tmp,\n-\t\t\t      load_offset_ptr);\n-\n-      if (!bswap)\n-\t{\n-\t  if (n->range == 16)\n-\t    nop_stats.found_16bit++;\n-\t  else if (n->range == 32)\n-\t    nop_stats.found_32bit++;\n-\t  else\n-\t    {\n-\t      gcc_assert (n->range == 64);\n-\t      nop_stats.found_64bit++;\n-\t    }\n-\n-\t  /* Convert the result of load if necessary.  */\n-\t  if (!useless_type_conversion_p (TREE_TYPE (tgt), load_type))\n-\t    {\n-\t      val_tmp = make_temp_ssa_name (aligned_load_type, NULL,\n-\t\t\t\t\t    \"load_dst\");\n-\t      load_stmt = gimple_build_assign (val_tmp, val_expr);\n-\t      gimple_set_vuse (load_stmt, n->vuse);\n-\t      gsi_insert_before (&gsi, load_stmt, GSI_SAME_STMT);\n-\t      gimple_assign_set_rhs_with_ops (&gsi, NOP_EXPR, val_tmp);\n-\t    }\n-\t  else\n-\t    {\n-\t      gimple_assign_set_rhs_with_ops (&gsi, MEM_REF, val_expr);\n-\t      gimple_set_vuse (cur_stmt, n->vuse);\n-\t    }\n-\t  update_stmt (cur_stmt);\n-\n-\t  if (dump_file)\n-\t    {\n-\t      fprintf (dump_file,\n-\t\t       \"%d bit load in target endianness found at: \",\n-\t\t       (int) n->range);\n-\t      print_gimple_stmt (dump_file, cur_stmt, 0);\n-\t    }\n-\t  return true;\n-\t}\n-      else\n-\t{\n-\t  val_tmp = make_temp_ssa_name (aligned_load_type, NULL, \"load_dst\");\n-\t  load_stmt = gimple_build_assign (val_tmp, val_expr);\n-\t  gimple_set_vuse (load_stmt, n->vuse);\n-\t  gsi_insert_before (&gsi, load_stmt, GSI_SAME_STMT);\n-\t}\n-      src = val_tmp;\n-    }\n-  else if (!bswap)\n-    {\n-      gimple *g;\n-      if (!useless_type_conversion_p (TREE_TYPE (tgt), TREE_TYPE (src)))\n-\t{\n-\t  if (!is_gimple_val (src))\n-\t    return false;\n-\t  g = gimple_build_assign (tgt, NOP_EXPR, src);\n-\t}\n-      else\n-\tg = gimple_build_assign (tgt, src);\n-      if (n->range == 16)\n-\tnop_stats.found_16bit++;\n-      else if (n->range == 32)\n-\tnop_stats.found_32bit++;\n-      else\n-\t{\n-\t  gcc_assert (n->range == 64);\n-\t  nop_stats.found_64bit++;\n-\t}\n-      if (dump_file)\n-\t{\n-\t  fprintf (dump_file,\n-\t\t   \"%d bit reshuffle in target endianness found at: \",\n-\t\t   (int) n->range);\n-\t  print_gimple_stmt (dump_file, cur_stmt, 0);\n-\t}\n-      gsi_replace (&gsi, g, true);\n-      return true;\n-    }\n-  else if (TREE_CODE (src) == BIT_FIELD_REF)\n-    src = TREE_OPERAND (src, 0);\n-\n-  if (n->range == 16)\n-    bswap_stats.found_16bit++;\n-  else if (n->range == 32)\n-    bswap_stats.found_32bit++;\n-  else\n-    {\n-      gcc_assert (n->range == 64);\n-      bswap_stats.found_64bit++;\n-    }\n-\n-  tmp = src;\n-\n-  /* Convert the src expression if necessary.  */\n-  if (!useless_type_conversion_p (TREE_TYPE (tmp), bswap_type))\n-    {\n-      gimple *convert_stmt;\n-\n-      tmp = make_temp_ssa_name (bswap_type, NULL, \"bswapsrc\");\n-      convert_stmt = gimple_build_assign (tmp, NOP_EXPR, src);\n-      gsi_insert_before (&gsi, convert_stmt, GSI_SAME_STMT);\n-    }\n-\n-  /* Canonical form for 16 bit bswap is a rotate expression.  Only 16bit values\n-     are considered as rotation of 2N bit values by N bits is generally not\n-     equivalent to a bswap.  Consider for instance 0x01020304 r>> 16 which\n-     gives 0x03040102 while a bswap for that value is 0x04030201.  */\n-  if (bswap && n->range == 16)\n-    {\n-      tree count = build_int_cst (NULL, BITS_PER_UNIT);\n-      src = fold_build2 (LROTATE_EXPR, bswap_type, tmp, count);\n-      bswap_stmt = gimple_build_assign (NULL, src);\n-    }\n-  else\n-    bswap_stmt = gimple_build_call (fndecl, 1, tmp);\n-\n-  tmp = tgt;\n-\n-  /* Convert the result if necessary.  */\n-  if (!useless_type_conversion_p (TREE_TYPE (tgt), bswap_type))\n-    {\n-      gimple *convert_stmt;\n-\n-      tmp = make_temp_ssa_name (bswap_type, NULL, \"bswapdst\");\n-      convert_stmt = gimple_build_assign (tgt, NOP_EXPR, tmp);\n-      gsi_insert_after (&gsi, convert_stmt, GSI_SAME_STMT);\n-    }\n-\n-  gimple_set_lhs (bswap_stmt, tmp);\n-\n-  if (dump_file)\n-    {\n-      fprintf (dump_file, \"%d bit bswap implementation found at: \",\n-\t       (int) n->range);\n-      print_gimple_stmt (dump_file, cur_stmt, 0);\n-    }\n-\n-  gsi_insert_after (&gsi, bswap_stmt, GSI_SAME_STMT);\n-  gsi_remove (&gsi, true);\n-  return true;\n-}\n-\n-/* Find manual byte swap implementations as well as load in a given\n-   endianness. Byte swaps are turned into a bswap builtin invokation\n-   while endian loads are converted to bswap builtin invokation or\n-   simple load according to the target endianness.  */\n-\n-unsigned int\n-pass_optimize_bswap::execute (function *fun)\n-{\n-  basic_block bb;\n-  bool bswap32_p, bswap64_p;\n-  bool changed = false;\n-  tree bswap32_type = NULL_TREE, bswap64_type = NULL_TREE;\n-\n-  if (BITS_PER_UNIT != 8)\n-    return 0;\n-\n-  bswap32_p = (builtin_decl_explicit_p (BUILT_IN_BSWAP32)\n-\t       && optab_handler (bswap_optab, SImode) != CODE_FOR_nothing);\n-  bswap64_p = (builtin_decl_explicit_p (BUILT_IN_BSWAP64)\n-\t       && (optab_handler (bswap_optab, DImode) != CODE_FOR_nothing\n-\t\t   || (bswap32_p && word_mode == SImode)));\n-\n-  /* Determine the argument type of the builtins.  The code later on\n-     assumes that the return and argument type are the same.  */\n-  if (bswap32_p)\n-    {\n-      tree fndecl = builtin_decl_explicit (BUILT_IN_BSWAP32);\n-      bswap32_type = TREE_VALUE (TYPE_ARG_TYPES (TREE_TYPE (fndecl)));\n-    }\n-\n-  if (bswap64_p)\n-    {\n-      tree fndecl = builtin_decl_explicit (BUILT_IN_BSWAP64);\n-      bswap64_type = TREE_VALUE (TYPE_ARG_TYPES (TREE_TYPE (fndecl)));\n-    }\n-\n-  memset (&nop_stats, 0, sizeof (nop_stats));\n-  memset (&bswap_stats, 0, sizeof (bswap_stats));\n-  calculate_dominance_info (CDI_DOMINATORS);\n-\n-  FOR_EACH_BB_FN (bb, fun)\n-    {\n-      gimple_stmt_iterator gsi;\n-\n-      /* We do a reverse scan for bswap patterns to make sure we get the\n-\t widest match. As bswap pattern matching doesn't handle previously\n-\t inserted smaller bswap replacements as sub-patterns, the wider\n-\t variant wouldn't be detected.  */\n-      for (gsi = gsi_last_bb (bb); !gsi_end_p (gsi);)\n-        {\n-\t  gimple *ins_stmt, *cur_stmt = gsi_stmt (gsi);\n-\t  tree fndecl = NULL_TREE, bswap_type = NULL_TREE, load_type;\n-\t  enum tree_code code;\n-\t  struct symbolic_number n;\n-\t  bool bswap;\n-\n-\t  /* This gsi_prev (&gsi) is not part of the for loop because cur_stmt\n-\t     might be moved to a different basic block by bswap_replace and gsi\n-\t     must not points to it if that's the case.  Moving the gsi_prev\n-\t     there make sure that gsi points to the statement previous to\n-\t     cur_stmt while still making sure that all statements are\n-\t     considered in this basic block.  */\n-\t  gsi_prev (&gsi);\n-\n-\t  if (!is_gimple_assign (cur_stmt))\n-\t    continue;\n-\n-\t  code = gimple_assign_rhs_code (cur_stmt);\n-\t  switch (code)\n-\t    {\n-\t    case LROTATE_EXPR:\n-\t    case RROTATE_EXPR:\n-\t      if (!tree_fits_uhwi_p (gimple_assign_rhs2 (cur_stmt))\n-\t\t  || tree_to_uhwi (gimple_assign_rhs2 (cur_stmt))\n-\t\t     % BITS_PER_UNIT)\n-\t\tcontinue;\n-\t      /* Fall through.  */\n-\t    case BIT_IOR_EXPR:\n-\t      break;\n-\t    default:\n-\t      continue;\n-\t    }\n-\n-\t  ins_stmt = find_bswap_or_nop (cur_stmt, &n, &bswap);\n-\n-\t  if (!ins_stmt)\n-\t    continue;\n-\n-\t  switch (n.range)\n-\t    {\n-\t    case 16:\n-\t      /* Already in canonical form, nothing to do.  */\n-\t      if (code == LROTATE_EXPR || code == RROTATE_EXPR)\n-\t\tcontinue;\n-\t      load_type = bswap_type = uint16_type_node;\n-\t      break;\n-\t    case 32:\n-\t      load_type = uint32_type_node;\n-\t      if (bswap32_p)\n-\t\t{\n-\t\t  fndecl = builtin_decl_explicit (BUILT_IN_BSWAP32);\n-\t\t  bswap_type = bswap32_type;\n-\t\t}\n-\t      break;\n-\t    case 64:\n-\t      load_type = uint64_type_node;\n-\t      if (bswap64_p)\n-\t\t{\n-\t\t  fndecl = builtin_decl_explicit (BUILT_IN_BSWAP64);\n-\t\t  bswap_type = bswap64_type;\n-\t\t}\n-\t      break;\n-\t    default:\n-\t      continue;\n-\t    }\n-\n-\t  if (bswap && !fndecl && n.range != 16)\n-\t    continue;\n-\n-\t  if (bswap_replace (cur_stmt, ins_stmt, fndecl, bswap_type, load_type,\n-\t\t\t     &n, bswap))\n-\t    changed = true;\n-\t}\n-    }\n-\n-  statistics_counter_event (fun, \"16-bit nop implementations found\",\n-\t\t\t    nop_stats.found_16bit);\n-  statistics_counter_event (fun, \"32-bit nop implementations found\",\n-\t\t\t    nop_stats.found_32bit);\n-  statistics_counter_event (fun, \"64-bit nop implementations found\",\n-\t\t\t    nop_stats.found_64bit);\n-  statistics_counter_event (fun, \"16-bit bswap implementations found\",\n-\t\t\t    bswap_stats.found_16bit);\n-  statistics_counter_event (fun, \"32-bit bswap implementations found\",\n-\t\t\t    bswap_stats.found_32bit);\n-  statistics_counter_event (fun, \"64-bit bswap implementations found\",\n-\t\t\t    bswap_stats.found_64bit);\n-\n-  return (changed ? TODO_update_ssa : 0);\n-}\n-\n-} // anon namespace\n-\n-gimple_opt_pass *\n-make_pass_optimize_bswap (gcc::context *ctxt)\n-{\n-  return new pass_optimize_bswap (ctxt);\n-}\n-\n /* Return true if stmt is a type conversion operation that can be stripped\n    when used in a widening multiply operation.  */\n static bool"}]}