{"sha": "2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjE5M2FkN2ZiZjNlOTE3YTBlZjVhMmI0OGUxM2Y4NGRhMWJlNDRmMQ==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2017-01-09T19:37:19Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2017-01-09T19:37:19Z"}, "message": "runtime: copy more of scheduler from Go 1.7 runtime\n    \n    This started by moving procresize from C to Go so that we can pass the\n    right type to the memory allocator when allocating a p, which forced\n    the gomaxprocs variable to move from C to Go, and everything else\n    followed from that.\n    \n    Reviewed-on: https://go-review.googlesource.com/34916\n\nFrom-SVN: r244236", "tree": {"sha": "979f6f9c20889cbbef8a131ed63b6e7f4b7216f6", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/979f6f9c20889cbbef8a131ed63b6e7f4b7216f6"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/comments", "author": null, "committer": null, "parents": [{"sha": "d1261ac6eb62f0c589ef314c61884ab445e5552b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d1261ac6eb62f0c589ef314c61884ab445e5552b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d1261ac6eb62f0c589ef314c61884ab445e5552b"}], "stats": {"total": 3539, "additions": 1997, "deletions": 1542}, "files": [{"sha": "2bbbc0f3292fafd695be8748ab9ecafc98c29ed6", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -1,4 +1,4 @@\n-eef0fb3b092dc22d9830cac15a536760da5d033a\n+189ea81cc758e000325fd6cca7882c252d33f8f0\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "55937ff8f05f05983fc45553fb7e07c1d7f3cf7b", "filename": "libgo/go/runtime/debug.go", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fdebug.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fdebug.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fdebug.go?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -14,7 +14,25 @@ import (\n // change the current setting.\n // The number of logical CPUs on the local machine can be queried with NumCPU.\n // This call will go away when the scheduler improves.\n-func GOMAXPROCS(n int) int\n+func GOMAXPROCS(n int) int {\n+\tif n > _MaxGomaxprocs {\n+\t\tn = _MaxGomaxprocs\n+\t}\n+\tlock(&sched.lock)\n+\tret := int(gomaxprocs)\n+\tunlock(&sched.lock)\n+\tif n <= 0 || n == ret {\n+\t\treturn ret\n+\t}\n+\n+\tstopTheWorld(\"GOMAXPROCS\")\n+\n+\t// newprocs will be processed by startTheWorld\n+\tnewprocs = int32(n)\n+\n+\tstartTheWorld()\n+\treturn ret\n+}\n \n // NumCPU returns the number of logical CPUs usable by the current process.\n //"}, {"sha": "36e6256992e3902f2db3d82d335710de022c7ae3", "filename": "libgo/go/runtime/export_test.go", "status": "modified", "additions": 21, "deletions": 41, "changes": 62, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fexport_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fexport_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fexport_test.go?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -7,6 +7,7 @@\n package runtime\n \n import (\n+\t\"runtime/internal/atomic\"\n \t\"unsafe\"\n )\n \n@@ -47,39 +48,6 @@ func GCMask(x interface{}) (ret []byte) {\n \treturn nil\n }\n \n-//func testSchedLocalQueue()\n-//func testSchedLocalQueueSteal()\n-//\n-//func RunSchedLocalQueueTest() {\n-//\ttestSchedLocalQueue()\n-//}\n-//\n-//func RunSchedLocalQueueStealTest() {\n-//\ttestSchedLocalQueueSteal()\n-//}\n-\n-//var StringHash = stringHash\n-//var BytesHash = bytesHash\n-//var Int32Hash = int32Hash\n-//var Int64Hash = int64Hash\n-//var EfaceHash = efaceHash\n-//var IfaceHash = ifaceHash\n-//var MemclrBytes = memclrBytes\n-\n-var HashLoad = &hashLoad\n-\n-// entry point for testing\n-//func GostringW(w []uint16) (s string) {\n-//\ts = gostringw(&w[0])\n-//\treturn\n-//}\n-\n-//var Gostringnocopy = gostringnocopy\n-//var Maxstring = &maxstring\n-\n-//type Uintreg uintreg\n-\n-/*\n func RunSchedLocalQueueTest() {\n \t_p_ := new(p)\n \tgs := make([]g, len(_p_.runq))\n@@ -177,14 +145,26 @@ func RunSchedLocalQueueEmptyTest(iters int) {\n \t}\n }\n \n-var StringHash = stringHash\n-var BytesHash = bytesHash\n-var Int32Hash = int32Hash\n-var Int64Hash = int64Hash\n-var EfaceHash = efaceHash\n-var IfaceHash = ifaceHash\n-var MemclrBytes = memclrBytes\n-*/\n+//var StringHash = stringHash\n+//var BytesHash = bytesHash\n+//var Int32Hash = int32Hash\n+//var Int64Hash = int64Hash\n+//var EfaceHash = efaceHash\n+//var IfaceHash = ifaceHash\n+//var MemclrBytes = memclrBytes\n+\n+var HashLoad = &hashLoad\n+\n+// entry point for testing\n+//func GostringW(w []uint16) (s string) {\n+//\ts = gostringw(&w[0])\n+//\treturn\n+//}\n+\n+//var Gostringnocopy = gostringnocopy\n+//var Maxstring = &maxstring\n+\n+//type Uintreg uintreg\n \n var Open = open\n var Close = closefd"}, {"sha": "4d914b25fadc989ee132bf42a75d8c9d42475e36", "filename": "libgo/go/runtime/lock_futex.go", "status": "modified", "additions": 10, "deletions": 11, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Flock_futex.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Flock_futex.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Flock_futex.go?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -149,13 +149,9 @@ func notewakeup(n *note) {\n \n func notesleep(n *note) {\n \tgp := getg()\n-\n-\t// Currently OK to sleep in non-g0 for gccgo.  It happens in\n-\t// stoptheworld because we have not implemented preemption.\n-\t// if gp != gp.m.g0 {\n-\t// \tthrow(\"notesleep not on g0\")\n-\t// }\n-\n+\tif gp != gp.m.g0 {\n+\t\tthrow(\"notesleep not on g0\")\n+\t}\n \tfor atomic.Load(key32(&n.key)) == 0 {\n \t\tgp.m.blocked = true\n \t\tfutexsleep(key32(&n.key), 0, -1)\n@@ -202,10 +198,13 @@ func notetsleep_internal(n *note, ns int64) bool {\n }\n \n func notetsleep(n *note, ns int64) bool {\n-\tgp := getg()\n-\tif gp != gp.m.g0 && gp.m.preemptoff != \"\" {\n-\t\tthrow(\"notetsleep not on g0\")\n-\t}\n+\t// Currently OK to sleep in non-g0 for gccgo.  It happens in\n+\t// stoptheworld because our version of systemstack does not\n+\t// change to g0.\n+\t// gp := getg()\n+\t// if gp != gp.m.g0 && gp.m.preemptoff != \"\" {\n+\t//\tthrow(\"notetsleep not on g0\")\n+\t// }\n \n \treturn notetsleep_internal(n, ns)\n }"}, {"sha": "5c70a747dabe3be0ee95fa94b79e6227a31aa020", "filename": "libgo/go/runtime/lock_sema.go", "status": "modified", "additions": 5, "deletions": 8, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Flock_sema.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Flock_sema.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Flock_sema.go?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -162,13 +162,9 @@ func notewakeup(n *note) {\n \n func notesleep(n *note) {\n \tgp := getg()\n-\n-\t// Currently OK to sleep in non-g0 for gccgo.  It happens in\n-\t// stoptheworld because we have not implemented preemption.\n-\t// if gp != gp.m.g0 {\n-\t//\tthrow(\"notesleep not on g0\")\n-\t// }\n-\n+\tif gp != gp.m.g0 {\n+\t\tthrow(\"notesleep not on g0\")\n+\t}\n \tsemacreate(gp.m)\n \tif !atomic.Casuintptr(&n.key, 0, uintptr(unsafe.Pointer(gp.m))) {\n \t\t// Must be locked (got wakeup).\n@@ -257,7 +253,8 @@ func notetsleep(n *note, ns int64) bool {\n \tgp := getg()\n \n \t// Currently OK to sleep in non-g0 for gccgo.  It happens in\n-\t// stoptheworld because we have not implemented preemption.\n+\t// stoptheworld because our version of systemstack does not\n+\t// change to g0.\n \t// if gp != gp.m.g0 && gp.m.preemptoff != \"\" {\n \t//\tthrow(\"notetsleep not on g0\")\n \t// }"}, {"sha": "659b17d19072b94747616c9d1a3085da816114b8", "filename": "libgo/go/runtime/proc.go", "status": "modified", "additions": 1739, "deletions": 5, "changes": 1744, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fproc.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fproc.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fproc.go?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -11,15 +11,45 @@ import (\n \n // Functions temporarily called by C code.\n //go:linkname newextram runtime.newextram\n+//go:linkname acquirep runtime.acquirep\n+//go:linkname releasep runtime.releasep\n+//go:linkname incidlelocked runtime.incidlelocked\n //go:linkname checkdead runtime.checkdead\n+//go:linkname sysmon runtime.sysmon\n //go:linkname schedtrace runtime.schedtrace\n //go:linkname allgadd runtime.allgadd\n+//go:linkname ready runtime.ready\n+//go:linkname gcprocs runtime.gcprocs\n+//go:linkname needaddgcproc runtime.needaddgcproc\n+//go:linkname stopm runtime.stopm\n+//go:linkname handoffp runtime.handoffp\n+//go:linkname wakep runtime.wakep\n+//go:linkname stoplockedm runtime.stoplockedm\n+//go:linkname schedule runtime.schedule\n+//go:linkname execute runtime.execute\n+//go:linkname procresize runtime.procresize\n+//go:linkname helpgc runtime.helpgc\n+//go:linkname stopTheWorldWithSema runtime.stopTheWorldWithSema\n+//go:linkname startTheWorldWithSema runtime.startTheWorldWithSema\n+//go:linkname mput runtime.mput\n+//go:linkname mget runtime.mget\n+//go:linkname globrunqput runtime.globrunqput\n+//go:linkname pidleget runtime.pidleget\n+//go:linkname runqempty runtime.runqempty\n+//go:linkname runqput runtime.runqput\n \n // Functions temporarily in C that have not yet been ported.\n func allocm(*p, bool, *unsafe.Pointer, *uintptr) *m\n func malg(bool, bool, *unsafe.Pointer, *uintptr) *g\n+func startm(*p, bool)\n+func newm(unsafe.Pointer, *p)\n+func gchelper()\n+func getfingwait() bool\n+func getfingwake() bool\n+func wakefing() *g\n \n // C functions for ucontext management.\n+func gogo(*g)\n func setGContext()\n func makeGContext(*g, unsafe.Pointer, uintptr)\n func getTraceback(me, gp *g)\n@@ -30,6 +60,12 @@ func getTraceback(me, gp *g)\n // it is closed, meaning cgocallbackg can reliably receive from it.\n var main_init_done chan bool\n \n+func goready(gp *g, traceskip int) {\n+\tsystemstack(func() {\n+\t\tready(gp, traceskip, true)\n+\t})\n+}\n+\n var (\n \tallgs    []*g\n \tallglock mutex\n@@ -56,6 +92,117 @@ func allgadd(gp *g) {\n \tunlock(&allglock)\n }\n \n+func dumpgstatus(gp *g) {\n+\t_g_ := getg()\n+\tprint(\"runtime: gp: gp=\", gp, \", goid=\", gp.goid, \", gp->atomicstatus=\", readgstatus(gp), \"\\n\")\n+\tprint(\"runtime:  g:  g=\", _g_, \", goid=\", _g_.goid, \",  g->atomicstatus=\", readgstatus(_g_), \"\\n\")\n+}\n+\n+// Mark gp ready to run.\n+func ready(gp *g, traceskip int, next bool) {\n+\tif trace.enabled {\n+\t\ttraceGoUnpark(gp, traceskip)\n+\t}\n+\n+\tstatus := readgstatus(gp)\n+\n+\t// Mark runnable.\n+\t_g_ := getg()\n+\t_g_.m.locks++ // disable preemption because it can be holding p in a local var\n+\tif status&^_Gscan != _Gwaiting {\n+\t\tdumpgstatus(gp)\n+\t\tthrow(\"bad g->status in ready\")\n+\t}\n+\n+\t// status is Gwaiting or Gscanwaiting, make Grunnable and put on runq\n+\tcasgstatus(gp, _Gwaiting, _Grunnable)\n+\trunqput(_g_.m.p.ptr(), gp, next)\n+\tif atomic.Load(&sched.npidle) != 0 && atomic.Load(&sched.nmspinning) == 0 { // TODO: fast atomic\n+\t\twakep()\n+\t}\n+\t_g_.m.locks--\n+}\n+\n+func gcprocs() int32 {\n+\t// Figure out how many CPUs to use during GC.\n+\t// Limited by gomaxprocs, number of actual CPUs, and MaxGcproc.\n+\tlock(&sched.lock)\n+\tn := gomaxprocs\n+\tif n > ncpu {\n+\t\tn = ncpu\n+\t}\n+\tif n > _MaxGcproc {\n+\t\tn = _MaxGcproc\n+\t}\n+\tif n > sched.nmidle+1 { // one M is currently running\n+\t\tn = sched.nmidle + 1\n+\t}\n+\tunlock(&sched.lock)\n+\treturn n\n+}\n+\n+func needaddgcproc() bool {\n+\tlock(&sched.lock)\n+\tn := gomaxprocs\n+\tif n > ncpu {\n+\t\tn = ncpu\n+\t}\n+\tif n > _MaxGcproc {\n+\t\tn = _MaxGcproc\n+\t}\n+\tn -= sched.nmidle + 1 // one M is currently running\n+\tunlock(&sched.lock)\n+\treturn n > 0\n+}\n+\n+func helpgc(nproc int32) {\n+\t_g_ := getg()\n+\tlock(&sched.lock)\n+\tpos := 0\n+\tfor n := int32(1); n < nproc; n++ { // one M is currently running\n+\t\tif allp[pos].mcache == _g_.m.mcache {\n+\t\t\tpos++\n+\t\t}\n+\t\tmp := mget()\n+\t\tif mp == nil {\n+\t\t\tthrow(\"gcprocs inconsistency\")\n+\t\t}\n+\t\tmp.helpgc = n\n+\t\tmp.p.set(allp[pos])\n+\t\tmp.mcache = allp[pos].mcache\n+\t\tpos++\n+\t\tnotewakeup(&mp.park)\n+\t}\n+\tunlock(&sched.lock)\n+}\n+\n+// freezeStopWait is a large value that freezetheworld sets\n+// sched.stopwait to in order to request that all Gs permanently stop.\n+const freezeStopWait = 0x7fffffff\n+\n+// Similar to stopTheWorld but best-effort and can be called several times.\n+// There is no reverse operation, used during crashing.\n+// This function must not lock any mutexes.\n+func freezetheworld() {\n+\t// stopwait and preemption requests can be lost\n+\t// due to races with concurrently executing threads,\n+\t// so try several times\n+\tfor i := 0; i < 5; i++ {\n+\t\t// this should tell the scheduler to not start any new goroutines\n+\t\tsched.stopwait = freezeStopWait\n+\t\tatomic.Store(&sched.gcwaiting, 1)\n+\t\t// this should stop running goroutines\n+\t\tif !preemptall() {\n+\t\t\tbreak // no running goroutines\n+\t\t}\n+\t\tusleep(1000)\n+\t}\n+\t// to be sure\n+\tusleep(1000)\n+\tpreemptall()\n+\tusleep(1000)\n+}\n+\n // All reads and writes of g's status go through readgstatus, casgstatus\n // castogscanstatus, casfrom_Gscanstatus.\n //go:nosplit\n@@ -123,6 +270,217 @@ func casgstatus(gp *g, oldval, newval uint32) {\n \t}\n }\n \n+// stopTheWorld stops all P's from executing goroutines, interrupting\n+// all goroutines at GC safe points and records reason as the reason\n+// for the stop. On return, only the current goroutine's P is running.\n+// stopTheWorld must not be called from a system stack and the caller\n+// must not hold worldsema. The caller must call startTheWorld when\n+// other P's should resume execution.\n+//\n+// stopTheWorld is safe for multiple goroutines to call at the\n+// same time. Each will execute its own stop, and the stops will\n+// be serialized.\n+//\n+// This is also used by routines that do stack dumps. If the system is\n+// in panic or being exited, this may not reliably stop all\n+// goroutines.\n+func stopTheWorld(reason string) {\n+\tsemacquire(&worldsema, false)\n+\tgetg().m.preemptoff = reason\n+\tsystemstack(stopTheWorldWithSema)\n+}\n+\n+// startTheWorld undoes the effects of stopTheWorld.\n+func startTheWorld() {\n+\tsystemstack(startTheWorldWithSema)\n+\t// worldsema must be held over startTheWorldWithSema to ensure\n+\t// gomaxprocs cannot change while worldsema is held.\n+\tsemrelease(&worldsema)\n+\tgetg().m.preemptoff = \"\"\n+}\n+\n+// Holding worldsema grants an M the right to try to stop the world\n+// and prevents gomaxprocs from changing concurrently.\n+var worldsema uint32 = 1\n+\n+// stopTheWorldWithSema is the core implementation of stopTheWorld.\n+// The caller is responsible for acquiring worldsema and disabling\n+// preemption first and then should stopTheWorldWithSema on the system\n+// stack:\n+//\n+//\tsemacquire(&worldsema, false)\n+//\tm.preemptoff = \"reason\"\n+//\tsystemstack(stopTheWorldWithSema)\n+//\n+// When finished, the caller must either call startTheWorld or undo\n+// these three operations separately:\n+//\n+//\tm.preemptoff = \"\"\n+//\tsystemstack(startTheWorldWithSema)\n+//\tsemrelease(&worldsema)\n+//\n+// It is allowed to acquire worldsema once and then execute multiple\n+// startTheWorldWithSema/stopTheWorldWithSema pairs.\n+// Other P's are able to execute between successive calls to\n+// startTheWorldWithSema and stopTheWorldWithSema.\n+// Holding worldsema causes any other goroutines invoking\n+// stopTheWorld to block.\n+func stopTheWorldWithSema() {\n+\t_g_ := getg()\n+\n+\t// If we hold a lock, then we won't be able to stop another M\n+\t// that is blocked trying to acquire the lock.\n+\tif _g_.m.locks > 0 {\n+\t\tthrow(\"stopTheWorld: holding locks\")\n+\t}\n+\n+\tlock(&sched.lock)\n+\tsched.stopwait = gomaxprocs\n+\tatomic.Store(&sched.gcwaiting, 1)\n+\tpreemptall()\n+\t// stop current P\n+\t_g_.m.p.ptr().status = _Pgcstop // Pgcstop is only diagnostic.\n+\tsched.stopwait--\n+\t// try to retake all P's in Psyscall status\n+\tfor i := 0; i < int(gomaxprocs); i++ {\n+\t\tp := allp[i]\n+\t\ts := p.status\n+\t\tif s == _Psyscall && atomic.Cas(&p.status, s, _Pgcstop) {\n+\t\t\tif trace.enabled {\n+\t\t\t\ttraceGoSysBlock(p)\n+\t\t\t\ttraceProcStop(p)\n+\t\t\t}\n+\t\t\tp.syscalltick++\n+\t\t\tsched.stopwait--\n+\t\t}\n+\t}\n+\t// stop idle P's\n+\tfor {\n+\t\tp := pidleget()\n+\t\tif p == nil {\n+\t\t\tbreak\n+\t\t}\n+\t\tp.status = _Pgcstop\n+\t\tsched.stopwait--\n+\t}\n+\twait := sched.stopwait > 0\n+\tunlock(&sched.lock)\n+\n+\t// wait for remaining P's to stop voluntarily\n+\tif wait {\n+\t\tfor {\n+\t\t\t// wait for 100us, then try to re-preempt in case of any races\n+\t\t\tif notetsleep(&sched.stopnote, 100*1000) {\n+\t\t\t\tnoteclear(&sched.stopnote)\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tpreemptall()\n+\t\t}\n+\t}\n+\tif sched.stopwait != 0 {\n+\t\tthrow(\"stopTheWorld: not stopped\")\n+\t}\n+\tfor i := 0; i < int(gomaxprocs); i++ {\n+\t\tp := allp[i]\n+\t\tif p.status != _Pgcstop {\n+\t\t\tthrow(\"stopTheWorld: not stopped\")\n+\t\t}\n+\t}\n+}\n+\n+func mhelpgc() {\n+\t_g_ := getg()\n+\t_g_.m.helpgc = -1\n+}\n+\n+func startTheWorldWithSema() {\n+\t_g_ := getg()\n+\n+\t_g_.m.locks++        // disable preemption because it can be holding p in a local var\n+\tgp := netpoll(false) // non-blocking\n+\tinjectglist(gp)\n+\tadd := needaddgcproc()\n+\tlock(&sched.lock)\n+\n+\tprocs := gomaxprocs\n+\tif newprocs != 0 {\n+\t\tprocs = newprocs\n+\t\tnewprocs = 0\n+\t}\n+\tp1 := procresize(procs)\n+\tsched.gcwaiting = 0\n+\tif sched.sysmonwait != 0 {\n+\t\tsched.sysmonwait = 0\n+\t\tnotewakeup(&sched.sysmonnote)\n+\t}\n+\tunlock(&sched.lock)\n+\n+\tfor p1 != nil {\n+\t\tp := p1\n+\t\tp1 = p1.link.ptr()\n+\t\tif p.m != 0 {\n+\t\t\tmp := p.m.ptr()\n+\t\t\tp.m = 0\n+\t\t\tif mp.nextp != 0 {\n+\t\t\t\tthrow(\"startTheWorld: inconsistent mp->nextp\")\n+\t\t\t}\n+\t\t\tmp.nextp.set(p)\n+\t\t\tnotewakeup(&mp.park)\n+\t\t} else {\n+\t\t\t// Start M to run P.  Do not start another M below.\n+\t\t\tnewm(nil, p)\n+\t\t\tadd = false\n+\t\t}\n+\t}\n+\n+\t// Wakeup an additional proc in case we have excessive runnable goroutines\n+\t// in local queues or in the global queue. If we don't, the proc will park itself.\n+\t// If we have lots of excessive work, resetspinning will unpark additional procs as necessary.\n+\tif atomic.Load(&sched.npidle) != 0 && atomic.Load(&sched.nmspinning) == 0 {\n+\t\twakep()\n+\t}\n+\n+\tif add {\n+\t\t// If GC could have used another helper proc, start one now,\n+\t\t// in the hope that it will be available next time.\n+\t\t// It would have been even better to start it before the collection,\n+\t\t// but doing so requires allocating memory, so it's tricky to\n+\t\t// coordinate. This lazy approach works out in practice:\n+\t\t// we don't mind if the first couple gc rounds don't have quite\n+\t\t// the maximum number of procs.\n+\t\tnewm(unsafe.Pointer(funcPC(mhelpgc)), nil)\n+\t}\n+\t_g_.m.locks--\n+}\n+\n+// runSafePointFn runs the safe point function, if any, for this P.\n+// This should be called like\n+//\n+//     if getg().m.p.runSafePointFn != 0 {\n+//         runSafePointFn()\n+//     }\n+//\n+// runSafePointFn must be checked on any transition in to _Pidle or\n+// _Psyscall to avoid a race where forEachP sees that the P is running\n+// just before the P goes into _Pidle/_Psyscall and neither forEachP\n+// nor the P run the safe-point function.\n+func runSafePointFn() {\n+\tp := getg().m.p.ptr()\n+\t// Resolve the race between forEachP running the safe-point\n+\t// function on this P's behalf and this P running the\n+\t// safe-point function directly.\n+\tif !atomic.Cas(&p.runSafePointFn, 1, 0) {\n+\t\treturn\n+\t}\n+\tsched.safePointFn(p)\n+\tlock(&sched.lock)\n+\tsched.safePointWait--\n+\tif sched.safePointWait == 0 {\n+\t\tnotewakeup(&sched.safePointNote)\n+\t}\n+\tunlock(&sched.lock)\n+}\n+\n // needm is called when a cgo callback happens on a\n // thread without an m (a thread not created by Go).\n // In this case, needm is expected to find an m to use\n@@ -245,9 +603,6 @@ func oneNewExtraM() {\n \tmp.lockedg = gp\n \tgp.lockedm = mp\n \tgp.goid = int64(atomic.Xadd64(&sched.goidgen, 1))\n-\tif raceenabled {\n-\t\tgp.racectx = racegostart(funcPC(newextram))\n-\t}\n \t// put on allg for garbage collector\n \tallgadd(gp)\n \n@@ -365,6 +720,744 @@ func unlockextra(mp *m) {\n \tatomic.Storeuintptr(&extram, uintptr(unsafe.Pointer(mp)))\n }\n \n+// Stops execution of the current m until new work is available.\n+// Returns with acquired P.\n+func stopm() {\n+\t_g_ := getg()\n+\n+\tif _g_.m.locks != 0 {\n+\t\tthrow(\"stopm holding locks\")\n+\t}\n+\tif _g_.m.p != 0 {\n+\t\tthrow(\"stopm holding p\")\n+\t}\n+\tif _g_.m.spinning {\n+\t\tthrow(\"stopm spinning\")\n+\t}\n+\n+retry:\n+\tlock(&sched.lock)\n+\tmput(_g_.m)\n+\tunlock(&sched.lock)\n+\tnotesleep(&_g_.m.park)\n+\tnoteclear(&_g_.m.park)\n+\tif _g_.m.helpgc != 0 {\n+\t\tgchelper()\n+\t\t_g_.m.helpgc = 0\n+\t\t_g_.m.mcache = nil\n+\t\t_g_.m.p = 0\n+\t\tgoto retry\n+\t}\n+\tacquirep(_g_.m.nextp.ptr())\n+\t_g_.m.nextp = 0\n+}\n+\n+// Hands off P from syscall or locked M.\n+// Always runs without a P, so write barriers are not allowed.\n+//go:nowritebarrier\n+func handoffp(_p_ *p) {\n+\t// handoffp must start an M in any situation where\n+\t// findrunnable would return a G to run on _p_.\n+\n+\t// if it has local work, start it straight away\n+\tif !runqempty(_p_) || sched.runqsize != 0 {\n+\t\tstartm(_p_, false)\n+\t\treturn\n+\t}\n+\t// if it has GC work, start it straight away\n+\tif gcBlackenEnabled != 0 && gcMarkWorkAvailable(_p_) {\n+\t\tstartm(_p_, false)\n+\t\treturn\n+\t}\n+\t// no local work, check that there are no spinning/idle M's,\n+\t// otherwise our help is not required\n+\tif atomic.Load(&sched.nmspinning)+atomic.Load(&sched.npidle) == 0 && atomic.Cas(&sched.nmspinning, 0, 1) { // TODO: fast atomic\n+\t\tstartm(_p_, true)\n+\t\treturn\n+\t}\n+\tlock(&sched.lock)\n+\tif sched.gcwaiting != 0 {\n+\t\t_p_.status = _Pgcstop\n+\t\tsched.stopwait--\n+\t\tif sched.stopwait == 0 {\n+\t\t\tnotewakeup(&sched.stopnote)\n+\t\t}\n+\t\tunlock(&sched.lock)\n+\t\treturn\n+\t}\n+\tif _p_.runSafePointFn != 0 && atomic.Cas(&_p_.runSafePointFn, 1, 0) {\n+\t\tsched.safePointFn(_p_)\n+\t\tsched.safePointWait--\n+\t\tif sched.safePointWait == 0 {\n+\t\t\tnotewakeup(&sched.safePointNote)\n+\t\t}\n+\t}\n+\tif sched.runqsize != 0 {\n+\t\tunlock(&sched.lock)\n+\t\tstartm(_p_, false)\n+\t\treturn\n+\t}\n+\t// If this is the last running P and nobody is polling network,\n+\t// need to wakeup another M to poll network.\n+\tif sched.npidle == uint32(gomaxprocs-1) && atomic.Load64(&sched.lastpoll) != 0 {\n+\t\tunlock(&sched.lock)\n+\t\tstartm(_p_, false)\n+\t\treturn\n+\t}\n+\tpidleput(_p_)\n+\tunlock(&sched.lock)\n+}\n+\n+// Tries to add one more P to execute G's.\n+// Called when a G is made runnable (newproc, ready).\n+func wakep() {\n+\t// be conservative about spinning threads\n+\tif !atomic.Cas(&sched.nmspinning, 0, 1) {\n+\t\treturn\n+\t}\n+\tstartm(nil, true)\n+}\n+\n+// Stops execution of the current m that is locked to a g until the g is runnable again.\n+// Returns with acquired P.\n+func stoplockedm() {\n+\t_g_ := getg()\n+\n+\tif _g_.m.lockedg == nil || _g_.m.lockedg.lockedm != _g_.m {\n+\t\tthrow(\"stoplockedm: inconsistent locking\")\n+\t}\n+\tif _g_.m.p != 0 {\n+\t\t// Schedule another M to run this p.\n+\t\t_p_ := releasep()\n+\t\thandoffp(_p_)\n+\t}\n+\tincidlelocked(1)\n+\t// Wait until another thread schedules lockedg again.\n+\tnotesleep(&_g_.m.park)\n+\tnoteclear(&_g_.m.park)\n+\tstatus := readgstatus(_g_.m.lockedg)\n+\tif status&^_Gscan != _Grunnable {\n+\t\tprint(\"runtime:stoplockedm: g is not Grunnable or Gscanrunnable\\n\")\n+\t\tdumpgstatus(_g_)\n+\t\tthrow(\"stoplockedm: not runnable\")\n+\t}\n+\tacquirep(_g_.m.nextp.ptr())\n+\t_g_.m.nextp = 0\n+}\n+\n+// Schedules the locked m to run the locked gp.\n+// May run during STW, so write barriers are not allowed.\n+//go:nowritebarrier\n+func startlockedm(gp *g) {\n+\t_g_ := getg()\n+\n+\tmp := gp.lockedm\n+\tif mp == _g_.m {\n+\t\tthrow(\"startlockedm: locked to me\")\n+\t}\n+\tif mp.nextp != 0 {\n+\t\tthrow(\"startlockedm: m has p\")\n+\t}\n+\t// directly handoff current P to the locked m\n+\tincidlelocked(-1)\n+\t_p_ := releasep()\n+\tmp.nextp.set(_p_)\n+\tnotewakeup(&mp.park)\n+\tstopm()\n+}\n+\n+// Stops the current m for stopTheWorld.\n+// Returns when the world is restarted.\n+func gcstopm() {\n+\t_g_ := getg()\n+\n+\tif sched.gcwaiting == 0 {\n+\t\tthrow(\"gcstopm: not waiting for gc\")\n+\t}\n+\tif _g_.m.spinning {\n+\t\t_g_.m.spinning = false\n+\t\t// OK to just drop nmspinning here,\n+\t\t// startTheWorld will unpark threads as necessary.\n+\t\tif int32(atomic.Xadd(&sched.nmspinning, -1)) < 0 {\n+\t\t\tthrow(\"gcstopm: negative nmspinning\")\n+\t\t}\n+\t}\n+\t_p_ := releasep()\n+\tlock(&sched.lock)\n+\t_p_.status = _Pgcstop\n+\tsched.stopwait--\n+\tif sched.stopwait == 0 {\n+\t\tnotewakeup(&sched.stopnote)\n+\t}\n+\tunlock(&sched.lock)\n+\tstopm()\n+}\n+\n+// Schedules gp to run on the current M.\n+// If inheritTime is true, gp inherits the remaining time in the\n+// current time slice. Otherwise, it starts a new time slice.\n+// Never returns.\n+func execute(gp *g, inheritTime bool) {\n+\t_g_ := getg()\n+\n+\tcasgstatus(gp, _Grunnable, _Grunning)\n+\tgp.waitsince = 0\n+\tgp.preempt = false\n+\tif !inheritTime {\n+\t\t_g_.m.p.ptr().schedtick++\n+\t}\n+\t_g_.m.curg = gp\n+\tgp.m = _g_.m\n+\n+\t// Check whether the profiler needs to be turned on or off.\n+\thz := sched.profilehz\n+\tif _g_.m.profilehz != hz {\n+\t\tresetcpuprofiler(hz)\n+\t}\n+\n+\tif trace.enabled {\n+\t\t// GoSysExit has to happen when we have a P, but before GoStart.\n+\t\t// So we emit it here.\n+\t\tif gp.syscallsp != 0 && gp.sysblocktraced {\n+\t\t\ttraceGoSysExit(gp.sysexitticks)\n+\t\t}\n+\t\ttraceGoStart()\n+\t}\n+\n+\tgogo(gp)\n+}\n+\n+// Finds a runnable goroutine to execute.\n+// Tries to steal from other P's, get g from global queue, poll network.\n+func findrunnable() (gp *g, inheritTime bool) {\n+\t_g_ := getg()\n+\n+\t// The conditions here and in handoffp must agree: if\n+\t// findrunnable would return a G to run, handoffp must start\n+\t// an M.\n+\n+top:\n+\t_p_ := _g_.m.p.ptr()\n+\tif sched.gcwaiting != 0 {\n+\t\tgcstopm()\n+\t\tgoto top\n+\t}\n+\tif _p_.runSafePointFn != 0 {\n+\t\trunSafePointFn()\n+\t}\n+\tif getfingwait() && getfingwake() {\n+\t\tif gp := wakefing(); gp != nil {\n+\t\t\tready(gp, 0, true)\n+\t\t}\n+\t}\n+\n+\t// local runq\n+\tif gp, inheritTime := runqget(_p_); gp != nil {\n+\t\treturn gp, inheritTime\n+\t}\n+\n+\t// global runq\n+\tif sched.runqsize != 0 {\n+\t\tlock(&sched.lock)\n+\t\tgp := globrunqget(_p_, 0)\n+\t\tunlock(&sched.lock)\n+\t\tif gp != nil {\n+\t\t\treturn gp, false\n+\t\t}\n+\t}\n+\n+\t// Poll network.\n+\t// This netpoll is only an optimization before we resort to stealing.\n+\t// We can safely skip it if there a thread blocked in netpoll already.\n+\t// If there is any kind of logical race with that blocked thread\n+\t// (e.g. it has already returned from netpoll, but does not set lastpoll yet),\n+\t// this thread will do blocking netpoll below anyway.\n+\tif netpollinited() && sched.lastpoll != 0 {\n+\t\tif gp := netpoll(false); gp != nil { // non-blocking\n+\t\t\t// netpoll returns list of goroutines linked by schedlink.\n+\t\t\tinjectglist(gp.schedlink.ptr())\n+\t\t\tcasgstatus(gp, _Gwaiting, _Grunnable)\n+\t\t\tif trace.enabled {\n+\t\t\t\ttraceGoUnpark(gp, 0)\n+\t\t\t}\n+\t\t\treturn gp, false\n+\t\t}\n+\t}\n+\n+\t// Steal work from other P's.\n+\tprocs := uint32(gomaxprocs)\n+\tif atomic.Load(&sched.npidle) == procs-1 {\n+\t\t// Either GOMAXPROCS=1 or everybody, except for us, is idle already.\n+\t\t// New work can appear from returning syscall/cgocall, network or timers.\n+\t\t// Neither of that submits to local run queues, so no point in stealing.\n+\t\tgoto stop\n+\t}\n+\t// If number of spinning M's >= number of busy P's, block.\n+\t// This is necessary to prevent excessive CPU consumption\n+\t// when GOMAXPROCS>>1 but the program parallelism is low.\n+\tif !_g_.m.spinning && 2*atomic.Load(&sched.nmspinning) >= procs-atomic.Load(&sched.npidle) { // TODO: fast atomic\n+\t\tgoto stop\n+\t}\n+\tif !_g_.m.spinning {\n+\t\t_g_.m.spinning = true\n+\t\tatomic.Xadd(&sched.nmspinning, 1)\n+\t}\n+\tfor i := 0; i < 4; i++ {\n+\t\tfor enum := stealOrder.start(fastrand1()); !enum.done(); enum.next() {\n+\t\t\tif sched.gcwaiting != 0 {\n+\t\t\t\tgoto top\n+\t\t\t}\n+\t\t\tstealRunNextG := i > 2 // first look for ready queues with more than 1 g\n+\t\t\tif gp := runqsteal(_p_, allp[enum.position()], stealRunNextG); gp != nil {\n+\t\t\t\treturn gp, false\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+stop:\n+\n+\t// We have nothing to do. If we're in the GC mark phase, can\n+\t// safely scan and blacken objects, and have work to do, run\n+\t// idle-time marking rather than give up the P.\n+\tif gcBlackenEnabled != 0 && _p_.gcBgMarkWorker != 0 && gcMarkWorkAvailable(_p_) {\n+\t\t_p_.gcMarkWorkerMode = gcMarkWorkerIdleMode\n+\t\tgp := _p_.gcBgMarkWorker.ptr()\n+\t\tcasgstatus(gp, _Gwaiting, _Grunnable)\n+\t\tif trace.enabled {\n+\t\t\ttraceGoUnpark(gp, 0)\n+\t\t}\n+\t\treturn gp, false\n+\t}\n+\n+\t// return P and block\n+\tlock(&sched.lock)\n+\tif sched.gcwaiting != 0 || _p_.runSafePointFn != 0 {\n+\t\tunlock(&sched.lock)\n+\t\tgoto top\n+\t}\n+\tif sched.runqsize != 0 {\n+\t\tgp := globrunqget(_p_, 0)\n+\t\tunlock(&sched.lock)\n+\t\treturn gp, false\n+\t}\n+\tif releasep() != _p_ {\n+\t\tthrow(\"findrunnable: wrong p\")\n+\t}\n+\tpidleput(_p_)\n+\tunlock(&sched.lock)\n+\n+\t// Delicate dance: thread transitions from spinning to non-spinning state,\n+\t// potentially concurrently with submission of new goroutines. We must\n+\t// drop nmspinning first and then check all per-P queues again (with\n+\t// #StoreLoad memory barrier in between). If we do it the other way around,\n+\t// another thread can submit a goroutine after we've checked all run queues\n+\t// but before we drop nmspinning; as the result nobody will unpark a thread\n+\t// to run the goroutine.\n+\t// If we discover new work below, we need to restore m.spinning as a signal\n+\t// for resetspinning to unpark a new worker thread (because there can be more\n+\t// than one starving goroutine). However, if after discovering new work\n+\t// we also observe no idle Ps, it is OK to just park the current thread:\n+\t// the system is fully loaded so no spinning threads are required.\n+\t// Also see \"Worker thread parking/unparking\" comment at the top of the file.\n+\twasSpinning := _g_.m.spinning\n+\tif _g_.m.spinning {\n+\t\t_g_.m.spinning = false\n+\t\tif int32(atomic.Xadd(&sched.nmspinning, -1)) < 0 {\n+\t\t\tthrow(\"findrunnable: negative nmspinning\")\n+\t\t}\n+\t}\n+\n+\t// check all runqueues once again\n+\tfor i := 0; i < int(gomaxprocs); i++ {\n+\t\t_p_ := allp[i]\n+\t\tif _p_ != nil && !runqempty(_p_) {\n+\t\t\tlock(&sched.lock)\n+\t\t\t_p_ = pidleget()\n+\t\t\tunlock(&sched.lock)\n+\t\t\tif _p_ != nil {\n+\t\t\t\tacquirep(_p_)\n+\t\t\t\tif wasSpinning {\n+\t\t\t\t\t_g_.m.spinning = true\n+\t\t\t\t\tatomic.Xadd(&sched.nmspinning, 1)\n+\t\t\t\t}\n+\t\t\t\tgoto top\n+\t\t\t}\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\n+\t// poll network\n+\tif netpollinited() && atomic.Xchg64(&sched.lastpoll, 0) != 0 {\n+\t\tif _g_.m.p != 0 {\n+\t\t\tthrow(\"findrunnable: netpoll with p\")\n+\t\t}\n+\t\tif _g_.m.spinning {\n+\t\t\tthrow(\"findrunnable: netpoll with spinning\")\n+\t\t}\n+\t\tgp := netpoll(true) // block until new work is available\n+\t\tatomic.Store64(&sched.lastpoll, uint64(nanotime()))\n+\t\tif gp != nil {\n+\t\t\tlock(&sched.lock)\n+\t\t\t_p_ = pidleget()\n+\t\t\tunlock(&sched.lock)\n+\t\t\tif _p_ != nil {\n+\t\t\t\tacquirep(_p_)\n+\t\t\t\tinjectglist(gp.schedlink.ptr())\n+\t\t\t\tcasgstatus(gp, _Gwaiting, _Grunnable)\n+\t\t\t\tif trace.enabled {\n+\t\t\t\t\ttraceGoUnpark(gp, 0)\n+\t\t\t\t}\n+\t\t\t\treturn gp, false\n+\t\t\t}\n+\t\t\tinjectglist(gp)\n+\t\t}\n+\t}\n+\tstopm()\n+\tgoto top\n+}\n+\n+func resetspinning() {\n+\t_g_ := getg()\n+\tif !_g_.m.spinning {\n+\t\tthrow(\"resetspinning: not a spinning m\")\n+\t}\n+\t_g_.m.spinning = false\n+\tnmspinning := atomic.Xadd(&sched.nmspinning, -1)\n+\tif int32(nmspinning) < 0 {\n+\t\tthrow(\"findrunnable: negative nmspinning\")\n+\t}\n+\t// M wakeup policy is deliberately somewhat conservative, so check if we\n+\t// need to wakeup another P here. See \"Worker thread parking/unparking\"\n+\t// comment at the top of the file for details.\n+\tif nmspinning == 0 && atomic.Load(&sched.npidle) > 0 {\n+\t\twakep()\n+\t}\n+}\n+\n+// Injects the list of runnable G's into the scheduler.\n+// Can run concurrently with GC.\n+func injectglist(glist *g) {\n+\tif glist == nil {\n+\t\treturn\n+\t}\n+\tif trace.enabled {\n+\t\tfor gp := glist; gp != nil; gp = gp.schedlink.ptr() {\n+\t\t\ttraceGoUnpark(gp, 0)\n+\t\t}\n+\t}\n+\tlock(&sched.lock)\n+\tvar n int\n+\tfor n = 0; glist != nil; n++ {\n+\t\tgp := glist\n+\t\tglist = gp.schedlink.ptr()\n+\t\tcasgstatus(gp, _Gwaiting, _Grunnable)\n+\t\tglobrunqput(gp)\n+\t}\n+\tunlock(&sched.lock)\n+\tfor ; n != 0 && sched.npidle != 0; n-- {\n+\t\tstartm(nil, false)\n+\t}\n+}\n+\n+// One round of scheduler: find a runnable goroutine and execute it.\n+// Never returns.\n+func schedule() {\n+\t_g_ := getg()\n+\n+\tif _g_.m.locks != 0 {\n+\t\tthrow(\"schedule: holding locks\")\n+\t}\n+\n+\tif _g_.m.lockedg != nil {\n+\t\tstoplockedm()\n+\t\texecute(_g_.m.lockedg, false) // Never returns.\n+\t}\n+\n+top:\n+\tif sched.gcwaiting != 0 {\n+\t\tgcstopm()\n+\t\tgoto top\n+\t}\n+\tif _g_.m.p.ptr().runSafePointFn != 0 {\n+\t\trunSafePointFn()\n+\t}\n+\n+\tvar gp *g\n+\tvar inheritTime bool\n+\tif trace.enabled || trace.shutdown {\n+\t\tgp = traceReader()\n+\t\tif gp != nil {\n+\t\t\tcasgstatus(gp, _Gwaiting, _Grunnable)\n+\t\t\ttraceGoUnpark(gp, 0)\n+\t\t}\n+\t}\n+\tif gp == nil && gcBlackenEnabled != 0 {\n+\t\tgp = gcController.findRunnableGCWorker(_g_.m.p.ptr())\n+\t}\n+\tif gp == nil {\n+\t\t// Check the global runnable queue once in a while to ensure fairness.\n+\t\t// Otherwise two goroutines can completely occupy the local runqueue\n+\t\t// by constantly respawning each other.\n+\t\tif _g_.m.p.ptr().schedtick%61 == 0 && sched.runqsize > 0 {\n+\t\t\tlock(&sched.lock)\n+\t\t\tgp = globrunqget(_g_.m.p.ptr(), 1)\n+\t\t\tunlock(&sched.lock)\n+\t\t}\n+\t}\n+\tif gp == nil {\n+\t\tgp, inheritTime = runqget(_g_.m.p.ptr())\n+\t\tif gp != nil && _g_.m.spinning {\n+\t\t\tthrow(\"schedule: spinning with local work\")\n+\t\t}\n+\n+\t\t// Because gccgo does not implement preemption as a stack check,\n+\t\t// we need to check for preemption here for fairness.\n+\t\t// Otherwise goroutines on the local queue may starve\n+\t\t// goroutines on the global queue.\n+\t\t// Since we preempt by storing the goroutine on the global\n+\t\t// queue, this is the only place we need to check preempt.\n+\t\tif gp != nil && gp.preempt {\n+\t\t\tgp.preempt = false\n+\t\t\tlock(&sched.lock)\n+\t\t\tglobrunqput(gp)\n+\t\t\tunlock(&sched.lock)\n+\t\t\tgoto top\n+\t\t}\n+\t}\n+\tif gp == nil {\n+\t\tgp, inheritTime = findrunnable() // blocks until work is available\n+\t}\n+\n+\t// This thread is going to run a goroutine and is not spinning anymore,\n+\t// so if it was marked as spinning we need to reset it now and potentially\n+\t// start a new spinning M.\n+\tif _g_.m.spinning {\n+\t\tresetspinning()\n+\t}\n+\n+\tif gp.lockedm != nil {\n+\t\t// Hands off own p to the locked m,\n+\t\t// then blocks waiting for a new p.\n+\t\tstartlockedm(gp)\n+\t\tgoto top\n+\t}\n+\n+\texecute(gp, inheritTime)\n+}\n+\n+// Purge all cached G's from gfree list to the global list.\n+func gfpurge(_p_ *p) {\n+\tlock(&sched.gflock)\n+\tfor _p_.gfreecnt != 0 {\n+\t\t_p_.gfreecnt--\n+\t\tgp := _p_.gfree\n+\t\t_p_.gfree = gp.schedlink.ptr()\n+\t\tgp.schedlink.set(sched.gfree)\n+\t\tsched.gfree = gp\n+\t\tsched.ngfree++\n+\t}\n+\tunlock(&sched.gflock)\n+}\n+\n+// Change number of processors. The world is stopped, sched is locked.\n+// gcworkbufs are not being modified by either the GC or\n+// the write barrier code.\n+// Returns list of Ps with local work, they need to be scheduled by the caller.\n+func procresize(nprocs int32) *p {\n+\told := gomaxprocs\n+\tif old < 0 || old > _MaxGomaxprocs || nprocs <= 0 || nprocs > _MaxGomaxprocs {\n+\t\tthrow(\"procresize: invalid arg\")\n+\t}\n+\tif trace.enabled {\n+\t\ttraceGomaxprocs(nprocs)\n+\t}\n+\n+\t// update statistics\n+\tnow := nanotime()\n+\tif sched.procresizetime != 0 {\n+\t\tsched.totaltime += int64(old) * (now - sched.procresizetime)\n+\t}\n+\tsched.procresizetime = now\n+\n+\t// initialize new P's\n+\tfor i := int32(0); i < nprocs; i++ {\n+\t\tpp := allp[i]\n+\t\tif pp == nil {\n+\t\t\tpp = new(p)\n+\t\t\tpp.id = i\n+\t\t\tpp.status = _Pgcstop\n+\t\t\tpp.sudogcache = pp.sudogbuf[:0]\n+\t\t\tpp.deferpool = pp.deferpoolbuf[:0]\n+\t\t\tatomicstorep(unsafe.Pointer(&allp[i]), unsafe.Pointer(pp))\n+\t\t}\n+\t\tif pp.mcache == nil {\n+\t\t\tif old == 0 && i == 0 {\n+\t\t\t\tif getg().m.mcache == nil {\n+\t\t\t\t\tthrow(\"missing mcache?\")\n+\t\t\t\t}\n+\t\t\t\tpp.mcache = getg().m.mcache // bootstrap\n+\t\t\t} else {\n+\t\t\t\tpp.mcache = allocmcache()\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// free unused P's\n+\tfor i := nprocs; i < old; i++ {\n+\t\tp := allp[i]\n+\t\tif trace.enabled {\n+\t\t\tif p == getg().m.p.ptr() {\n+\t\t\t\t// moving to p[0], pretend that we were descheduled\n+\t\t\t\t// and then scheduled again to keep the trace sane.\n+\t\t\t\ttraceGoSched()\n+\t\t\t\ttraceProcStop(p)\n+\t\t\t}\n+\t\t}\n+\t\t// move all runnable goroutines to the global queue\n+\t\tfor p.runqhead != p.runqtail {\n+\t\t\t// pop from tail of local queue\n+\t\t\tp.runqtail--\n+\t\t\tgp := p.runq[p.runqtail%uint32(len(p.runq))].ptr()\n+\t\t\t// push onto head of global queue\n+\t\t\tglobrunqputhead(gp)\n+\t\t}\n+\t\tif p.runnext != 0 {\n+\t\t\tglobrunqputhead(p.runnext.ptr())\n+\t\t\tp.runnext = 0\n+\t\t}\n+\t\t// if there's a background worker, make it runnable and put\n+\t\t// it on the global queue so it can clean itself up\n+\t\tif gp := p.gcBgMarkWorker.ptr(); gp != nil {\n+\t\t\tcasgstatus(gp, _Gwaiting, _Grunnable)\n+\t\t\tif trace.enabled {\n+\t\t\t\ttraceGoUnpark(gp, 0)\n+\t\t\t}\n+\t\t\tglobrunqput(gp)\n+\t\t\t// This assignment doesn't race because the\n+\t\t\t// world is stopped.\n+\t\t\tp.gcBgMarkWorker.set(nil)\n+\t\t}\n+\t\tfor i := range p.sudogbuf {\n+\t\t\tp.sudogbuf[i] = nil\n+\t\t}\n+\t\tp.sudogcache = p.sudogbuf[:0]\n+\t\tfor i := range p.deferpoolbuf {\n+\t\t\tp.deferpoolbuf[i] = nil\n+\t\t}\n+\t\tp.deferpool = p.deferpoolbuf[:0]\n+\t\tfreemcache(p.mcache)\n+\t\tp.mcache = nil\n+\t\tgfpurge(p)\n+\t\ttraceProcFree(p)\n+\t\tp.status = _Pdead\n+\t\t// can't free P itself because it can be referenced by an M in syscall\n+\t}\n+\n+\t_g_ := getg()\n+\tif _g_.m.p != 0 && _g_.m.p.ptr().id < nprocs {\n+\t\t// continue to use the current P\n+\t\t_g_.m.p.ptr().status = _Prunning\n+\t} else {\n+\t\t// release the current P and acquire allp[0]\n+\t\tif _g_.m.p != 0 {\n+\t\t\t_g_.m.p.ptr().m = 0\n+\t\t}\n+\t\t_g_.m.p = 0\n+\t\t_g_.m.mcache = nil\n+\t\tp := allp[0]\n+\t\tp.m = 0\n+\t\tp.status = _Pidle\n+\t\tacquirep(p)\n+\t\tif trace.enabled {\n+\t\t\ttraceGoStart()\n+\t\t}\n+\t}\n+\tvar runnablePs *p\n+\tfor i := nprocs - 1; i >= 0; i-- {\n+\t\tp := allp[i]\n+\t\tif _g_.m.p.ptr() == p {\n+\t\t\tcontinue\n+\t\t}\n+\t\tp.status = _Pidle\n+\t\tif runqempty(p) {\n+\t\t\tpidleput(p)\n+\t\t} else {\n+\t\t\tp.m.set(mget())\n+\t\t\tp.link.set(runnablePs)\n+\t\t\trunnablePs = p\n+\t\t}\n+\t}\n+\tstealOrder.reset(uint32(nprocs))\n+\tvar int32p *int32 = &gomaxprocs // make compiler check that gomaxprocs is an int32\n+\tatomic.Store((*uint32)(unsafe.Pointer(int32p)), uint32(nprocs))\n+\treturn runnablePs\n+}\n+\n+// Associate p and the current m.\n+func acquirep(_p_ *p) {\n+\tacquirep1(_p_)\n+\n+\t// have p; write barriers now allowed\n+\t_g_ := getg()\n+\t_g_.m.mcache = _p_.mcache\n+\n+\tif trace.enabled {\n+\t\ttraceProcStart()\n+\t}\n+}\n+\n+// May run during STW, so write barriers are not allowed.\n+//go:nowritebarrier\n+func acquirep1(_p_ *p) {\n+\t_g_ := getg()\n+\n+\tif _g_.m.p != 0 || _g_.m.mcache != nil {\n+\t\tthrow(\"acquirep: already in go\")\n+\t}\n+\tif _p_.m != 0 || _p_.status != _Pidle {\n+\t\tid := int32(0)\n+\t\tif _p_.m != 0 {\n+\t\t\tid = _p_.m.ptr().id\n+\t\t}\n+\t\tprint(\"acquirep: p->m=\", _p_.m, \"(\", id, \") p->status=\", _p_.status, \"\\n\")\n+\t\tthrow(\"acquirep: invalid p state\")\n+\t}\n+\t_g_.m.p.set(_p_)\n+\t_p_.m.set(_g_.m)\n+\t_p_.status = _Prunning\n+}\n+\n+// Disassociate p and the current m.\n+func releasep() *p {\n+\t_g_ := getg()\n+\n+\tif _g_.m.p == 0 || _g_.m.mcache == nil {\n+\t\tthrow(\"releasep: invalid arg\")\n+\t}\n+\t_p_ := _g_.m.p.ptr()\n+\tif _p_.m.ptr() != _g_.m || _p_.mcache != _g_.m.mcache || _p_.status != _Prunning {\n+\t\tprint(\"releasep: m=\", _g_.m, \" m->p=\", _g_.m.p.ptr(), \" p->m=\", _p_.m, \" m->mcache=\", _g_.m.mcache, \" p->mcache=\", _p_.mcache, \" p->status=\", _p_.status, \"\\n\")\n+\t\tthrow(\"releasep: invalid p state\")\n+\t}\n+\tif trace.enabled {\n+\t\ttraceProcStop(_g_.m.p.ptr())\n+\t}\n+\t_g_.m.p = 0\n+\t_g_.m.mcache = nil\n+\t_p_.m = 0\n+\t_p_.status = _Pidle\n+\treturn _p_\n+}\n+\n+func incidlelocked(v int32) {\n+\tlock(&sched.lock)\n+\tsched.nmidlelocked += v\n+\tif v > 0 {\n+\t\tcheckdead()\n+\t}\n+\tunlock(&sched.lock)\n+}\n+\n // Check for deadlock situation.\n // The check is based on number of running M's, if 0 -> deadlock.\n func checkdead() {\n@@ -443,6 +1536,236 @@ func checkdead() {\n \tthrow(\"all goroutines are asleep - deadlock!\")\n }\n \n+// forcegcperiod is the maximum time in nanoseconds between garbage\n+// collections. If we go this long without a garbage collection, one\n+// is forced to run.\n+//\n+// This is a variable for testing purposes. It normally doesn't change.\n+var forcegcperiod int64 = 2 * 60 * 1e9\n+\n+// Always runs without a P, so write barriers are not allowed.\n+//\n+//go:nowritebarrierrec\n+func sysmon() {\n+\t// If a heap span goes unused for 5 minutes after a garbage collection,\n+\t// we hand it back to the operating system.\n+\tscavengelimit := int64(5 * 60 * 1e9)\n+\n+\tif debug.scavenge > 0 {\n+\t\t// Scavenge-a-lot for testing.\n+\t\tforcegcperiod = 10 * 1e6\n+\t\tscavengelimit = 20 * 1e6\n+\t}\n+\n+\tlastscavenge := nanotime()\n+\tnscavenge := 0\n+\n+\tlasttrace := int64(0)\n+\tidle := 0 // how many cycles in succession we had not wokeup somebody\n+\tdelay := uint32(0)\n+\tfor {\n+\t\tif idle == 0 { // start with 20us sleep...\n+\t\t\tdelay = 20\n+\t\t} else if idle > 50 { // start doubling the sleep after 1ms...\n+\t\t\tdelay *= 2\n+\t\t}\n+\t\tif delay > 10*1000 { // up to 10ms\n+\t\t\tdelay = 10 * 1000\n+\t\t}\n+\t\tusleep(delay)\n+\t\tif debug.schedtrace <= 0 && (sched.gcwaiting != 0 || atomic.Load(&sched.npidle) == uint32(gomaxprocs)) { // TODO: fast atomic\n+\t\t\tlock(&sched.lock)\n+\t\t\tif atomic.Load(&sched.gcwaiting) != 0 || atomic.Load(&sched.npidle) == uint32(gomaxprocs) {\n+\t\t\t\tatomic.Store(&sched.sysmonwait, 1)\n+\t\t\t\tunlock(&sched.lock)\n+\t\t\t\t// Make wake-up period small enough\n+\t\t\t\t// for the sampling to be correct.\n+\t\t\t\tmaxsleep := forcegcperiod / 2\n+\t\t\t\tif scavengelimit < forcegcperiod {\n+\t\t\t\t\tmaxsleep = scavengelimit / 2\n+\t\t\t\t}\n+\t\t\t\tnotetsleep(&sched.sysmonnote, maxsleep)\n+\t\t\t\tlock(&sched.lock)\n+\t\t\t\tatomic.Store(&sched.sysmonwait, 0)\n+\t\t\t\tnoteclear(&sched.sysmonnote)\n+\t\t\t\tidle = 0\n+\t\t\t\tdelay = 20\n+\t\t\t}\n+\t\t\tunlock(&sched.lock)\n+\t\t}\n+\t\t// poll network if not polled for more than 10ms\n+\t\tlastpoll := int64(atomic.Load64(&sched.lastpoll))\n+\t\tnow := nanotime()\n+\t\tunixnow := unixnanotime()\n+\t\tif lastpoll != 0 && lastpoll+10*1000*1000 < now {\n+\t\t\tatomic.Cas64(&sched.lastpoll, uint64(lastpoll), uint64(now))\n+\t\t\tgp := netpoll(false) // non-blocking - returns list of goroutines\n+\t\t\tif gp != nil {\n+\t\t\t\t// Need to decrement number of idle locked M's\n+\t\t\t\t// (pretending that one more is running) before injectglist.\n+\t\t\t\t// Otherwise it can lead to the following situation:\n+\t\t\t\t// injectglist grabs all P's but before it starts M's to run the P's,\n+\t\t\t\t// another M returns from syscall, finishes running its G,\n+\t\t\t\t// observes that there is no work to do and no other running M's\n+\t\t\t\t// and reports deadlock.\n+\t\t\t\tincidlelocked(-1)\n+\t\t\t\tinjectglist(gp)\n+\t\t\t\tincidlelocked(1)\n+\t\t\t}\n+\t\t}\n+\t\t// retake P's blocked in syscalls\n+\t\t// and preempt long running G's\n+\t\tif retake(now) != 0 {\n+\t\t\tidle = 0\n+\t\t} else {\n+\t\t\tidle++\n+\t\t}\n+\t\t// check if we need to force a GC\n+\t\tlastgc := int64(atomic.Load64(&memstats.last_gc))\n+\t\tif gcphase == _GCoff && lastgc != 0 && unixnow-lastgc > forcegcperiod && atomic.Load(&forcegc.idle) != 0 {\n+\t\t\tlock(&forcegc.lock)\n+\t\t\tforcegc.idle = 0\n+\t\t\tforcegc.g.schedlink = 0\n+\t\t\tinjectglist(forcegc.g)\n+\t\t\tunlock(&forcegc.lock)\n+\t\t}\n+\t\t// scavenge heap once in a while\n+\t\tif lastscavenge+scavengelimit/2 < now {\n+\t\t\tmheap_.scavenge(int32(nscavenge), uint64(now), uint64(scavengelimit))\n+\t\t\tlastscavenge = now\n+\t\t\tnscavenge++\n+\t\t}\n+\t\tif debug.schedtrace > 0 && lasttrace+int64(debug.schedtrace)*1000000 <= now {\n+\t\t\tlasttrace = now\n+\t\t\tschedtrace(debug.scheddetail > 0)\n+\t\t}\n+\t}\n+}\n+\n+var pdesc [_MaxGomaxprocs]struct {\n+\tschedtick   uint32\n+\tschedwhen   int64\n+\tsyscalltick uint32\n+\tsyscallwhen int64\n+}\n+\n+// forcePreemptNS is the time slice given to a G before it is\n+// preempted.\n+const forcePreemptNS = 10 * 1000 * 1000 // 10ms\n+\n+func retake(now int64) uint32 {\n+\tn := 0\n+\tfor i := int32(0); i < gomaxprocs; i++ {\n+\t\t_p_ := allp[i]\n+\t\tif _p_ == nil {\n+\t\t\tcontinue\n+\t\t}\n+\t\tpd := &pdesc[i]\n+\t\ts := _p_.status\n+\t\tif s == _Psyscall {\n+\t\t\t// Retake P from syscall if it's there for more than 1 sysmon tick (at least 20us).\n+\t\t\tt := int64(_p_.syscalltick)\n+\t\t\tif int64(pd.syscalltick) != t {\n+\t\t\t\tpd.syscalltick = uint32(t)\n+\t\t\t\tpd.syscallwhen = now\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// On the one hand we don't want to retake Ps if there is no other work to do,\n+\t\t\t// but on the other hand we want to retake them eventually\n+\t\t\t// because they can prevent the sysmon thread from deep sleep.\n+\t\t\tif runqempty(_p_) && atomic.Load(&sched.nmspinning)+atomic.Load(&sched.npidle) > 0 && pd.syscallwhen+10*1000*1000 > now {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// Need to decrement number of idle locked M's\n+\t\t\t// (pretending that one more is running) before the CAS.\n+\t\t\t// Otherwise the M from which we retake can exit the syscall,\n+\t\t\t// increment nmidle and report deadlock.\n+\t\t\tincidlelocked(-1)\n+\t\t\tif atomic.Cas(&_p_.status, s, _Pidle) {\n+\t\t\t\tif trace.enabled {\n+\t\t\t\t\ttraceGoSysBlock(_p_)\n+\t\t\t\t\ttraceProcStop(_p_)\n+\t\t\t\t}\n+\t\t\t\tn++\n+\t\t\t\t_p_.syscalltick++\n+\t\t\t\thandoffp(_p_)\n+\t\t\t}\n+\t\t\tincidlelocked(1)\n+\t\t} else if s == _Prunning {\n+\t\t\t// Preempt G if it's running for too long.\n+\t\t\tt := int64(_p_.schedtick)\n+\t\t\tif int64(pd.schedtick) != t {\n+\t\t\t\tpd.schedtick = uint32(t)\n+\t\t\t\tpd.schedwhen = now\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tif pd.schedwhen+forcePreemptNS > now {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tpreemptone(_p_)\n+\t\t}\n+\t}\n+\treturn uint32(n)\n+}\n+\n+// Tell all goroutines that they have been preempted and they should stop.\n+// This function is purely best-effort. It can fail to inform a goroutine if a\n+// processor just started running it.\n+// No locks need to be held.\n+// Returns true if preemption request was issued to at least one goroutine.\n+func preemptall() bool {\n+\tres := false\n+\tfor i := int32(0); i < gomaxprocs; i++ {\n+\t\t_p_ := allp[i]\n+\t\tif _p_ == nil || _p_.status != _Prunning {\n+\t\t\tcontinue\n+\t\t}\n+\t\tif preemptone(_p_) {\n+\t\t\tres = true\n+\t\t}\n+\t}\n+\treturn res\n+}\n+\n+// Tell the goroutine running on processor P to stop.\n+// This function is purely best-effort. It can incorrectly fail to inform the\n+// goroutine. It can send inform the wrong goroutine. Even if it informs the\n+// correct goroutine, that goroutine might ignore the request if it is\n+// simultaneously executing newstack.\n+// No lock needs to be held.\n+// Returns true if preemption request was issued.\n+// The actual preemption will happen at some point in the future\n+// and will be indicated by the gp->status no longer being\n+// Grunning\n+func preemptone(_p_ *p) bool {\n+\tmp := _p_.m.ptr()\n+\tif mp == nil || mp == getg().m {\n+\t\treturn false\n+\t}\n+\tgp := mp.curg\n+\tif gp == nil || gp == mp.g0 {\n+\t\treturn false\n+\t}\n+\n+\tgp.preempt = true\n+\n+\t// At this point the gc implementation sets gp.stackguard0 to\n+\t// a value that causes the goroutine to suspend itself.\n+\t// gccgo has no support for this, and it's hard to support.\n+\t// The split stack code reads a value from its TCB.\n+\t// We have no way to set a value in the TCB of a different thread.\n+\t// And, of course, not all systems support split stack anyhow.\n+\t// Checking the field in the g is expensive, since it requires\n+\t// loading the g from TLS.  The best mechanism is likely to be\n+\t// setting a global variable and figuring out a way to efficiently\n+\t// check that global variable.\n+\t//\n+\t// For now we check gp.preempt in schedule and mallocgc,\n+\t// which is at least better than doing nothing at all.\n+\n+\treturn true\n+}\n+\n var starttime int64\n \n func schedtrace(detailed bool) {\n@@ -451,8 +1774,6 @@ func schedtrace(detailed bool) {\n \t\tstarttime = now\n \t}\n \n-\tgomaxprocs := int32(GOMAXPROCS(0))\n-\n \tlock(&sched.lock)\n \tprint(\"SCHED \", (now-starttime)/1e6, \"ms: gomaxprocs=\", gomaxprocs, \" idleprocs=\", sched.npidle, \" threads=\", sched.mcount, \" spinningthreads=\", sched.nmspinning, \" idlethreads=\", sched.nmidle, \" runqueue=\", sched.runqsize)\n \tif detailed {\n@@ -531,3 +1852,416 @@ func schedtrace(detailed bool) {\n \tunlock(&allglock)\n \tunlock(&sched.lock)\n }\n+\n+// Put mp on midle list.\n+// Sched must be locked.\n+// May run during STW, so write barriers are not allowed.\n+//go:nowritebarrier\n+func mput(mp *m) {\n+\tmp.schedlink = sched.midle\n+\tsched.midle.set(mp)\n+\tsched.nmidle++\n+\tcheckdead()\n+}\n+\n+// Try to get an m from midle list.\n+// Sched must be locked.\n+// May run during STW, so write barriers are not allowed.\n+//go:nowritebarrier\n+func mget() *m {\n+\tmp := sched.midle.ptr()\n+\tif mp != nil {\n+\t\tsched.midle = mp.schedlink\n+\t\tsched.nmidle--\n+\t}\n+\treturn mp\n+}\n+\n+// Put gp on the global runnable queue.\n+// Sched must be locked.\n+// May run during STW, so write barriers are not allowed.\n+//go:nowritebarrier\n+func globrunqput(gp *g) {\n+\tgp.schedlink = 0\n+\tif sched.runqtail != 0 {\n+\t\tsched.runqtail.ptr().schedlink.set(gp)\n+\t} else {\n+\t\tsched.runqhead.set(gp)\n+\t}\n+\tsched.runqtail.set(gp)\n+\tsched.runqsize++\n+}\n+\n+// Put gp at the head of the global runnable queue.\n+// Sched must be locked.\n+// May run during STW, so write barriers are not allowed.\n+//go:nowritebarrier\n+func globrunqputhead(gp *g) {\n+\tgp.schedlink = sched.runqhead\n+\tsched.runqhead.set(gp)\n+\tif sched.runqtail == 0 {\n+\t\tsched.runqtail.set(gp)\n+\t}\n+\tsched.runqsize++\n+}\n+\n+// Put a batch of runnable goroutines on the global runnable queue.\n+// Sched must be locked.\n+func globrunqputbatch(ghead *g, gtail *g, n int32) {\n+\tgtail.schedlink = 0\n+\tif sched.runqtail != 0 {\n+\t\tsched.runqtail.ptr().schedlink.set(ghead)\n+\t} else {\n+\t\tsched.runqhead.set(ghead)\n+\t}\n+\tsched.runqtail.set(gtail)\n+\tsched.runqsize += n\n+}\n+\n+// Try get a batch of G's from the global runnable queue.\n+// Sched must be locked.\n+func globrunqget(_p_ *p, max int32) *g {\n+\tif sched.runqsize == 0 {\n+\t\treturn nil\n+\t}\n+\n+\tn := sched.runqsize/gomaxprocs + 1\n+\tif n > sched.runqsize {\n+\t\tn = sched.runqsize\n+\t}\n+\tif max > 0 && n > max {\n+\t\tn = max\n+\t}\n+\tif n > int32(len(_p_.runq))/2 {\n+\t\tn = int32(len(_p_.runq)) / 2\n+\t}\n+\n+\tsched.runqsize -= n\n+\tif sched.runqsize == 0 {\n+\t\tsched.runqtail = 0\n+\t}\n+\n+\tgp := sched.runqhead.ptr()\n+\tsched.runqhead = gp.schedlink\n+\tn--\n+\tfor ; n > 0; n-- {\n+\t\tgp1 := sched.runqhead.ptr()\n+\t\tsched.runqhead = gp1.schedlink\n+\t\trunqput(_p_, gp1, false)\n+\t}\n+\treturn gp\n+}\n+\n+// Put p to on _Pidle list.\n+// Sched must be locked.\n+// May run during STW, so write barriers are not allowed.\n+//go:nowritebarrier\n+func pidleput(_p_ *p) {\n+\tif !runqempty(_p_) {\n+\t\tthrow(\"pidleput: P has non-empty run queue\")\n+\t}\n+\t_p_.link = sched.pidle\n+\tsched.pidle.set(_p_)\n+\tatomic.Xadd(&sched.npidle, 1) // TODO: fast atomic\n+}\n+\n+// Try get a p from _Pidle list.\n+// Sched must be locked.\n+// May run during STW, so write barriers are not allowed.\n+//go:nowritebarrier\n+func pidleget() *p {\n+\t_p_ := sched.pidle.ptr()\n+\tif _p_ != nil {\n+\t\tsched.pidle = _p_.link\n+\t\tatomic.Xadd(&sched.npidle, -1) // TODO: fast atomic\n+\t}\n+\treturn _p_\n+}\n+\n+// runqempty returns true if _p_ has no Gs on its local run queue.\n+// It never returns true spuriously.\n+func runqempty(_p_ *p) bool {\n+\t// Defend against a race where 1) _p_ has G1 in runqnext but runqhead == runqtail,\n+\t// 2) runqput on _p_ kicks G1 to the runq, 3) runqget on _p_ empties runqnext.\n+\t// Simply observing that runqhead == runqtail and then observing that runqnext == nil\n+\t// does not mean the queue is empty.\n+\tfor {\n+\t\thead := atomic.Load(&_p_.runqhead)\n+\t\ttail := atomic.Load(&_p_.runqtail)\n+\t\trunnext := atomic.Loaduintptr((*uintptr)(unsafe.Pointer(&_p_.runnext)))\n+\t\tif tail == atomic.Load(&_p_.runqtail) {\n+\t\t\treturn head == tail && runnext == 0\n+\t\t}\n+\t}\n+}\n+\n+// To shake out latent assumptions about scheduling order,\n+// we introduce some randomness into scheduling decisions\n+// when running with the race detector.\n+// The need for this was made obvious by changing the\n+// (deterministic) scheduling order in Go 1.5 and breaking\n+// many poorly-written tests.\n+// With the randomness here, as long as the tests pass\n+// consistently with -race, they shouldn't have latent scheduling\n+// assumptions.\n+const randomizeScheduler = raceenabled\n+\n+// runqput tries to put g on the local runnable queue.\n+// If next if false, runqput adds g to the tail of the runnable queue.\n+// If next is true, runqput puts g in the _p_.runnext slot.\n+// If the run queue is full, runnext puts g on the global queue.\n+// Executed only by the owner P.\n+func runqput(_p_ *p, gp *g, next bool) {\n+\tif randomizeScheduler && next && fastrand1()%2 == 0 {\n+\t\tnext = false\n+\t}\n+\n+\tif next {\n+\tretryNext:\n+\t\toldnext := _p_.runnext\n+\t\tif !_p_.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) {\n+\t\t\tgoto retryNext\n+\t\t}\n+\t\tif oldnext == 0 {\n+\t\t\treturn\n+\t\t}\n+\t\t// Kick the old runnext out to the regular run queue.\n+\t\tgp = oldnext.ptr()\n+\t}\n+\n+retry:\n+\th := atomic.Load(&_p_.runqhead) // load-acquire, synchronize with consumers\n+\tt := _p_.runqtail\n+\tif t-h < uint32(len(_p_.runq)) {\n+\t\t_p_.runq[t%uint32(len(_p_.runq))].set(gp)\n+\t\tatomic.Store(&_p_.runqtail, t+1) // store-release, makes the item available for consumption\n+\t\treturn\n+\t}\n+\tif runqputslow(_p_, gp, h, t) {\n+\t\treturn\n+\t}\n+\t// the queue is not full, now the put above must succeed\n+\tgoto retry\n+}\n+\n+// Put g and a batch of work from local runnable queue on global queue.\n+// Executed only by the owner P.\n+func runqputslow(_p_ *p, gp *g, h, t uint32) bool {\n+\tvar batch [len(_p_.runq)/2 + 1]*g\n+\n+\t// First, grab a batch from local queue.\n+\tn := t - h\n+\tn = n / 2\n+\tif n != uint32(len(_p_.runq)/2) {\n+\t\tthrow(\"runqputslow: queue is not full\")\n+\t}\n+\tfor i := uint32(0); i < n; i++ {\n+\t\tbatch[i] = _p_.runq[(h+i)%uint32(len(_p_.runq))].ptr()\n+\t}\n+\tif !atomic.Cas(&_p_.runqhead, h, h+n) { // cas-release, commits consume\n+\t\treturn false\n+\t}\n+\tbatch[n] = gp\n+\n+\tif randomizeScheduler {\n+\t\tfor i := uint32(1); i <= n; i++ {\n+\t\t\tj := fastrand1() % (i + 1)\n+\t\t\tbatch[i], batch[j] = batch[j], batch[i]\n+\t\t}\n+\t}\n+\n+\t// Link the goroutines.\n+\tfor i := uint32(0); i < n; i++ {\n+\t\tbatch[i].schedlink.set(batch[i+1])\n+\t}\n+\n+\t// Now put the batch on global queue.\n+\tlock(&sched.lock)\n+\tglobrunqputbatch(batch[0], batch[n], int32(n+1))\n+\tunlock(&sched.lock)\n+\treturn true\n+}\n+\n+// Get g from local runnable queue.\n+// If inheritTime is true, gp should inherit the remaining time in the\n+// current time slice. Otherwise, it should start a new time slice.\n+// Executed only by the owner P.\n+func runqget(_p_ *p) (gp *g, inheritTime bool) {\n+\t// If there's a runnext, it's the next G to run.\n+\tfor {\n+\t\tnext := _p_.runnext\n+\t\tif next == 0 {\n+\t\t\tbreak\n+\t\t}\n+\t\tif _p_.runnext.cas(next, 0) {\n+\t\t\treturn next.ptr(), true\n+\t\t}\n+\t}\n+\n+\tfor {\n+\t\th := atomic.Load(&_p_.runqhead) // load-acquire, synchronize with other consumers\n+\t\tt := _p_.runqtail\n+\t\tif t == h {\n+\t\t\treturn nil, false\n+\t\t}\n+\t\tgp := _p_.runq[h%uint32(len(_p_.runq))].ptr()\n+\t\tif atomic.Cas(&_p_.runqhead, h, h+1) { // cas-release, commits consume\n+\t\t\treturn gp, false\n+\t\t}\n+\t}\n+}\n+\n+// Grabs a batch of goroutines from _p_'s runnable queue into batch.\n+// Batch is a ring buffer starting at batchHead.\n+// Returns number of grabbed goroutines.\n+// Can be executed by any P.\n+func runqgrab(_p_ *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32 {\n+\tfor {\n+\t\th := atomic.Load(&_p_.runqhead) // load-acquire, synchronize with other consumers\n+\t\tt := atomic.Load(&_p_.runqtail) // load-acquire, synchronize with the producer\n+\t\tn := t - h\n+\t\tn = n - n/2\n+\t\tif n == 0 {\n+\t\t\tif stealRunNextG {\n+\t\t\t\t// Try to steal from _p_.runnext.\n+\t\t\t\tif next := _p_.runnext; next != 0 {\n+\t\t\t\t\t// Sleep to ensure that _p_ isn't about to run the g we\n+\t\t\t\t\t// are about to steal.\n+\t\t\t\t\t// The important use case here is when the g running on _p_\n+\t\t\t\t\t// ready()s another g and then almost immediately blocks.\n+\t\t\t\t\t// Instead of stealing runnext in this window, back off\n+\t\t\t\t\t// to give _p_ a chance to schedule runnext. This will avoid\n+\t\t\t\t\t// thrashing gs between different Ps.\n+\t\t\t\t\t// A sync chan send/recv takes ~50ns as of time of writing,\n+\t\t\t\t\t// so 3us gives ~50x overshoot.\n+\t\t\t\t\tif GOOS != \"windows\" {\n+\t\t\t\t\t\tusleep(3)\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t// On windows system timer granularity is 1-15ms,\n+\t\t\t\t\t\t// which is way too much for this optimization.\n+\t\t\t\t\t\t// So just yield.\n+\t\t\t\t\t\tosyield()\n+\t\t\t\t\t}\n+\t\t\t\t\tif !_p_.runnext.cas(next, 0) {\n+\t\t\t\t\t\tcontinue\n+\t\t\t\t\t}\n+\t\t\t\t\tbatch[batchHead%uint32(len(batch))] = next\n+\t\t\t\t\treturn 1\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn 0\n+\t\t}\n+\t\tif n > uint32(len(_p_.runq)/2) { // read inconsistent h and t\n+\t\t\tcontinue\n+\t\t}\n+\t\tfor i := uint32(0); i < n; i++ {\n+\t\t\tg := _p_.runq[(h+i)%uint32(len(_p_.runq))]\n+\t\t\tbatch[(batchHead+i)%uint32(len(batch))] = g\n+\t\t}\n+\t\tif atomic.Cas(&_p_.runqhead, h, h+n) { // cas-release, commits consume\n+\t\t\treturn n\n+\t\t}\n+\t}\n+}\n+\n+// Steal half of elements from local runnable queue of p2\n+// and put onto local runnable queue of p.\n+// Returns one of the stolen elements (or nil if failed).\n+func runqsteal(_p_, p2 *p, stealRunNextG bool) *g {\n+\tt := _p_.runqtail\n+\tn := runqgrab(p2, &_p_.runq, t, stealRunNextG)\n+\tif n == 0 {\n+\t\treturn nil\n+\t}\n+\tn--\n+\tgp := _p_.runq[(t+n)%uint32(len(_p_.runq))].ptr()\n+\tif n == 0 {\n+\t\treturn gp\n+\t}\n+\th := atomic.Load(&_p_.runqhead) // load-acquire, synchronize with consumers\n+\tif t-h+n >= uint32(len(_p_.runq)) {\n+\t\tthrow(\"runqsteal: runq overflow\")\n+\t}\n+\tatomic.Store(&_p_.runqtail, t+n) // store-release, makes the item available for consumption\n+\treturn gp\n+}\n+\n+// Active spinning for sync.Mutex.\n+//go:linkname sync_runtime_canSpin sync.runtime_canSpin\n+//go:nosplit\n+func sync_runtime_canSpin(i int) bool {\n+\t// sync.Mutex is cooperative, so we are conservative with spinning.\n+\t// Spin only few times and only if running on a multicore machine and\n+\t// GOMAXPROCS>1 and there is at least one other running P and local runq is empty.\n+\t// As opposed to runtime mutex we don't do passive spinning here,\n+\t// because there can be work on global runq on on other Ps.\n+\tif i >= active_spin || ncpu <= 1 || gomaxprocs <= int32(sched.npidle+sched.nmspinning)+1 {\n+\t\treturn false\n+\t}\n+\tif p := getg().m.p.ptr(); !runqempty(p) {\n+\t\treturn false\n+\t}\n+\treturn true\n+}\n+\n+//go:linkname sync_runtime_doSpin sync.runtime_doSpin\n+//go:nosplit\n+func sync_runtime_doSpin() {\n+\tprocyield(active_spin_cnt)\n+}\n+\n+var stealOrder randomOrder\n+\n+// randomOrder/randomEnum are helper types for randomized work stealing.\n+// They allow to enumerate all Ps in different pseudo-random orders without repetitions.\n+// The algorithm is based on the fact that if we have X such that X and GOMAXPROCS\n+// are coprime, then a sequences of (i + X) % GOMAXPROCS gives the required enumeration.\n+type randomOrder struct {\n+\tcount    uint32\n+\tcoprimes []uint32\n+}\n+\n+type randomEnum struct {\n+\ti     uint32\n+\tcount uint32\n+\tpos   uint32\n+\tinc   uint32\n+}\n+\n+func (ord *randomOrder) reset(count uint32) {\n+\tord.count = count\n+\tord.coprimes = ord.coprimes[:0]\n+\tfor i := uint32(1); i <= count; i++ {\n+\t\tif gcd(i, count) == 1 {\n+\t\t\tord.coprimes = append(ord.coprimes, i)\n+\t\t}\n+\t}\n+}\n+\n+func (ord *randomOrder) start(i uint32) randomEnum {\n+\treturn randomEnum{\n+\t\tcount: ord.count,\n+\t\tpos:   i % ord.count,\n+\t\tinc:   ord.coprimes[i%uint32(len(ord.coprimes))],\n+\t}\n+}\n+\n+func (enum *randomEnum) done() bool {\n+\treturn enum.i == enum.count\n+}\n+\n+func (enum *randomEnum) next() {\n+\tenum.i++\n+\tenum.pos = (enum.pos + enum.inc) % enum.count\n+}\n+\n+func (enum *randomEnum) position() uint32 {\n+\treturn enum.pos\n+}\n+\n+func gcd(a, b uint32) uint32 {\n+\tfor b != 0 {\n+\t\ta, b = b, a%b\n+\t}\n+\treturn a\n+}"}, {"sha": "813c92912b9bc793d848b2c2d2496236ba6c6571", "filename": "libgo/go/runtime/proc_test.go", "status": "modified", "additions": 2, "deletions": 8, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fproc_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fproc_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fproc_test.go?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -556,19 +556,14 @@ func nonleaf(stop chan int) bool {\n \t}\n }\n \n-/*\n func TestSchedLocalQueue(t *testing.T) {\n-\truntime.TestSchedLocalQueue1()\n+\truntime.RunSchedLocalQueueTest()\n }\n-*/\n \n-/*\n func TestSchedLocalQueueSteal(t *testing.T) {\n-\truntime.TestSchedLocalQueueSteal1()\n+\truntime.RunSchedLocalQueueStealTest()\n }\n-*/\n \n-/*\n func TestSchedLocalQueueEmpty(t *testing.T) {\n \tif runtime.NumCPU() == 1 {\n \t\t// Takes too long and does not trigger the race.\n@@ -586,7 +581,6 @@ func TestSchedLocalQueueEmpty(t *testing.T) {\n \t}\n \truntime.RunSchedLocalQueueEmptyTest(iters)\n }\n-*/\n \n func benchmarkStackGrowth(b *testing.B, rec int) {\n \tb.RunParallel(func(pb *testing.PB) {"}, {"sha": "755bc5f6380331857d13fc8513a3fcbcea0548a6", "filename": "libgo/go/runtime/runtime2.go", "status": "modified", "additions": 15, "deletions": 21, "changes": 36, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fruntime2.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fruntime2.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime2.go?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -5,6 +5,7 @@\n package runtime\n \n import (\n+\t\"runtime/internal/atomic\"\n \t\"runtime/internal/sys\"\n \t\"unsafe\"\n )\n@@ -203,12 +204,10 @@ func (gp guintptr) ptr() *g { return (*g)(unsafe.Pointer(gp)) }\n //go:nosplit\n func (gp *guintptr) set(g *g) { *gp = guintptr(unsafe.Pointer(g)) }\n \n-/*\n //go:nosplit\n func (gp *guintptr) cas(old, new guintptr) bool {\n \treturn atomic.Casuintptr((*uintptr)(unsafe.Pointer(gp)), uintptr(old), uintptr(new))\n }\n-*/\n \n type puintptr uintptr\n \n@@ -358,8 +357,8 @@ type g struct {\n \tsigpc          uintptr\n \tgopc           uintptr // pc of go statement that created this goroutine\n \tstartpc        uintptr // pc of goroutine function\n-\tracectx        uintptr\n-\twaiting        *sudog // sudog structures this g is waiting on (that have a valid elem ptr); in lock order\n+\t// Not for gccgo: racectx        uintptr\n+\twaiting *sudog // sudog structures this g is waiting on (that have a valid elem ptr); in lock order\n \t// Not for gccgo: cgoCtxt        []uintptr // cgo traceback context\n \n \t// Per-G GC state\n@@ -521,16 +520,16 @@ type p struct {\n \tgfreecnt int32\n \n \tsudogcache []*sudog\n-\t// Not for gccgo for now: sudogbuf   [128]*sudog\n+\tsudogbuf   [128]*sudog\n \n-\t// Not for gccgo for now: tracebuf traceBufPtr\n+\ttracebuf traceBufPtr\n \n \t// Not for gccgo for now: palloc persistentAlloc // per-P to avoid mutex\n \n \t// Per-P GC state\n-\t// Not for gccgo for now: gcAssistTime     int64 // Nanoseconds in assistAlloc\n-\t// Not for gccgo for now: gcBgMarkWorker   guintptr\n-\t// Not for gccgo for now: gcMarkWorkerMode gcMarkWorkerMode\n+\tgcAssistTime     int64 // Nanoseconds in assistAlloc\n+\tgcBgMarkWorker   guintptr\n+\tgcMarkWorkerMode gcMarkWorkerMode\n \n \t// gcw is this P's GC work buffer cache. The work buffer is\n \t// filled by write barriers, drained by mutator assists, and\n@@ -760,18 +759,13 @@ var (\n \n \t//\tallm        *m\n \n-\tallp [_MaxGomaxprocs + 1]*p\n-\n-\t//\tgomaxprocs  int32\n-\n-\tpanicking uint32\n-\tncpu      int32\n-\n-\t//\tforcegc     forcegcstate\n-\n-\tsched schedt\n-\n-\t//\tnewprocs    int32\n+\tallp       [_MaxGomaxprocs + 1]*p\n+\tgomaxprocs int32\n+\tpanicking  uint32\n+\tncpu       int32\n+\tforcegc    forcegcstate\n+\tsched      schedt\n+\tnewprocs   int32\n \n \t// Information about what cpu features are available.\n \t// Set on startup."}, {"sha": "6787476456b1e0d8f3ca02b8a49b4c3c6111c383", "filename": "libgo/go/runtime/stubs.go", "status": "modified", "additions": 60, "deletions": 28, "changes": 88, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fstubs.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Fstubs.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstubs.go?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -304,6 +304,7 @@ const (\n \t_64bit              = 1 << (^uintptr(0) >> 63) / 2\n \t_MHeapMap_TotalBits = (_64bit*sys.GoosWindows)*35 + (_64bit*(1-sys.GoosWindows)*(1-sys.GoosDarwin*sys.GoarchArm64))*39 + sys.GoosDarwin*sys.GoarchArm64*31 + (1-_64bit)*32\n \t_MaxMem             = uintptr(1<<_MHeapMap_TotalBits - 1)\n+\t_MaxGcproc          = 32\n )\n \n // Here for gccgo until we port malloc.go.\n@@ -350,7 +351,6 @@ func entersyscallblock(int32)\n func exitsyscall(int32)\n func gopark(func(*g, unsafe.Pointer) bool, unsafe.Pointer, string, byte, int)\n func goparkunlock(*mutex, string, byte, int)\n-func goready(*g, int)\n \n // Temporary hack for gccgo until we port proc.go.\n //go:nosplit\n@@ -411,12 +411,6 @@ func roundupsize(uintptr) uintptr\n // Here for gccgo until we port mgc.go.\n func GC()\n \n-// Here for gccgo until we port proc.go.\n-var worldsema uint32 = 1\n-\n-func stopTheWorldWithSema()\n-func startTheWorldWithSema()\n-\n // For gccgo to call from C code.\n //go:linkname acquireWorldsema runtime.acquireWorldsema\n func acquireWorldsema() {\n@@ -429,26 +423,6 @@ func releaseWorldsema() {\n \tsemrelease(&worldsema)\n }\n \n-// Here for gccgo until we port proc.go.\n-func stopTheWorld(reason string) {\n-\tsemacquire(&worldsema, false)\n-\tgetg().m.preemptoff = reason\n-\tgetg().m.gcing = 1\n-\tsystemstack(stopTheWorldWithSema)\n-}\n-\n-// Here for gccgo until we port proc.go.\n-func startTheWorld() {\n-\tgetg().m.gcing = 0\n-\tgetg().m.locks++\n-\tsystemstack(startTheWorldWithSema)\n-\t// worldsema must be held over startTheWorldWithSema to ensure\n-\t// gomaxprocs cannot change while worldsema is held.\n-\tsemrelease(&worldsema)\n-\tgetg().m.preemptoff = \"\"\n-\tgetg().m.locks--\n-}\n-\n // For gccgo to call from C code, so that the C code and the Go code\n // can share the memstats variable for now.\n //go:linkname getMstats runtime.getMstats\n@@ -461,6 +435,7 @@ func setcpuprofilerate_m(hz int32)\n \n // Temporary for gccgo until we port mem_GOOS.go.\n func sysAlloc(n uintptr, sysStat *uint64) unsafe.Pointer\n+func sysFree(v unsafe.Pointer, n uintptr, sysStat *uint64)\n \n // Temporary for gccgo until we port proc.go, so that the C signal\n // handler can call into cpuprof.\n@@ -522,7 +497,6 @@ func getZerobase() *uintptr {\n func sigprof()\n func mcount() int32\n func goexit1()\n-func freezetheworld()\n \n // Get signal trampoline, written in C.\n func getSigtramp() uintptr\n@@ -592,6 +566,7 @@ func getPanicking() uint32 {\n \n // Temporary for gccgo until we port mcache.go.\n func allocmcache() *mcache\n+func freemcache(*mcache)\n \n // Temporary for gccgo until we port mgc.go.\n // This is just so that allgadd will compile.\n@@ -616,3 +591,60 @@ func gcount() int32 {\n \tunlock(&allglock)\n \treturn n\n }\n+\n+// Temporary for gccgo until we port mgc.go.\n+var gcBlackenEnabled uint32\n+\n+// Temporary for gccgo until we port mgc.go.\n+func gcMarkWorkAvailable(p *p) bool {\n+\treturn false\n+}\n+\n+// Temporary for gccgo until we port mgc.go.\n+var gcController gcControllerState\n+\n+// Temporary for gccgo until we port mgc.go.\n+type gcControllerState struct {\n+}\n+\n+// Temporary for gccgo until we port mgc.go.\n+func (c *gcControllerState) findRunnableGCWorker(_p_ *p) *g {\n+\treturn nil\n+}\n+\n+// Temporary for gccgo until we port mgc.go.\n+var gcphase uint32\n+\n+// Temporary for gccgo until we port mgc.go.\n+const (\n+\t_GCoff = iota\n+\t_GCmark\n+\t_GCmarktermination\n+)\n+\n+// Temporary for gccgo until we port mgc.go.\n+type gcMarkWorkerMode int\n+\n+// Temporary for gccgo until we port mgc.go.\n+const (\n+\tgcMarkWorkerDedicatedMode gcMarkWorkerMode = iota\n+\tgcMarkWorkerFractionalMode\n+\tgcMarkWorkerIdleMode\n+)\n+\n+// Temporary for gccgo until we port mheap.go.\n+type mheap struct {\n+}\n+\n+// Temporary for gccgo until we port mheap.go.\n+var mheap_ mheap\n+\n+// Temporary for gccgo until we port mheap.go.\n+func (h *mheap) scavenge(k int32, now, limit uint64) {\n+}\n+\n+// Temporary for gccgo until we initialize ncpu in Go.\n+//go:linkname setncpu runtime.setncpu\n+func setncpu(n int32) {\n+\tncpu = n\n+}"}, {"sha": "09a150f6e6322c83c679f26f607269b90d58a69c", "filename": "libgo/go/runtime/trace.go", "status": "modified", "additions": 25, "deletions": 50, "changes": 75, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Ftrace.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fgo%2Fruntime%2Ftrace.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ftrace.go?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -127,10 +127,10 @@ var trace struct {\n \n // traceBufHeader is per-P tracing buffer.\n type traceBufHeader struct {\n-\tlink      traceBufPtr             // in trace.empty/full\n-\tlastTicks uint64                  // when we wrote the last event\n-\tpos       int                     // next write offset in arr\n-\tstk       [traceStackSize]uintptr // scratch buffer for traceback\n+\tlink      traceBufPtr              // in trace.empty/full\n+\tlastTicks uint64                   // when we wrote the last event\n+\tpos       int                      // next write offset in arr\n+\tstk       [traceStackSize]location // scratch buffer for traceback\n }\n \n // traceBuf is per-P tracing buffer.\n@@ -152,9 +152,6 @@ func traceBufPtrOf(b *traceBuf) traceBufPtr {\n \treturn traceBufPtr(unsafe.Pointer(b))\n }\n \n-/*\n-Commented out for gccgo for now.\n-\n // StartTrace enables tracing for the current process.\n // While tracing, the data will be buffered and available via ReadTrace.\n // StartTrace returns an error if tracing is already enabled.\n@@ -522,13 +519,7 @@ func traceEvent(ev byte, skip int, args ...uint64) {\n \t\tif gp == _g_ {\n \t\t\tnstk = callers(skip, buf.stk[:])\n \t\t} else if gp != nil {\n-\t\t\tgp = mp.curg\n-\t\t\t// This may happen when tracing a system call,\n-\t\t\t// so we must lock the stack.\n-\t\t\tif gcTryLockStackBarriers(gp) {\n-\t\t\t\tnstk = gcallers(gp, skip, buf.stk[:])\n-\t\t\t\tgcUnlockStackBarriers(gp)\n-\t\t\t}\n+\t\t\t// FIXME: get stack trace of different goroutine.\n \t\t}\n \t\tif nstk > 0 {\n \t\t\tnstk-- // skip runtime.goexit\n@@ -647,8 +638,6 @@ func (buf *traceBuf) byte(v byte) {\n \tbuf.pos++\n }\n \n-*/\n-\n // traceStackTable maps stack traces (arrays of PC's) to unique uint32 ids.\n // It is lock-free for reading.\n type traceStackTable struct {\n@@ -664,28 +653,30 @@ type traceStack struct {\n \thash uintptr\n \tid   uint32\n \tn    int\n-\tstk  [0]uintptr // real type [n]uintptr\n+\tstk  [0]location // real type [n]location\n }\n \n type traceStackPtr uintptr\n \n-/*\n-Commented out for gccgo for now.\n-\n func (tp traceStackPtr) ptr() *traceStack { return (*traceStack)(unsafe.Pointer(tp)) }\n \n // stack returns slice of PCs.\n-func (ts *traceStack) stack() []uintptr {\n-\treturn (*[traceStackSize]uintptr)(unsafe.Pointer(&ts.stk))[:ts.n]\n+func (ts *traceStack) stack() []location {\n+\treturn (*[traceStackSize]location)(unsafe.Pointer(&ts.stk))[:ts.n]\n }\n \n // put returns a unique id for the stack trace pcs and caches it in the table,\n // if it sees the trace for the first time.\n-func (tab *traceStackTable) put(pcs []uintptr) uint32 {\n+func (tab *traceStackTable) put(pcs []location) uint32 {\n \tif len(pcs) == 0 {\n \t\treturn 0\n \t}\n-\thash := memhash(unsafe.Pointer(&pcs[0]), 0, uintptr(len(pcs))*unsafe.Sizeof(pcs[0]))\n+\tvar hash uintptr\n+\tfor _, loc := range pcs {\n+\t\thash += loc.pc\n+\t\thash += hash << 10\n+\t\thash ^= hash >> 6\n+\t}\n \t// First, search the hashtable w/o the mutex.\n \tif id := tab.find(pcs, hash); id != 0 {\n \t\treturn id\n@@ -714,7 +705,7 @@ func (tab *traceStackTable) put(pcs []uintptr) uint32 {\n }\n \n // find checks if the stack trace pcs is already present in the table.\n-func (tab *traceStackTable) find(pcs []uintptr, hash uintptr) uint32 {\n+func (tab *traceStackTable) find(pcs []location, hash uintptr) uint32 {\n \tpart := int(hash % uintptr(len(tab.tab)))\n Search:\n \tfor stk := tab.tab[part].ptr(); stk != nil; stk = stk.link.ptr() {\n@@ -732,13 +723,12 @@ Search:\n \n // newStack allocates a new stack of size n.\n func (tab *traceStackTable) newStack(n int) *traceStack {\n-\treturn (*traceStack)(tab.mem.alloc(unsafe.Sizeof(traceStack{}) + uintptr(n)*sys.PtrSize))\n+\treturn (*traceStack)(tab.mem.alloc(unsafe.Sizeof(traceStack{}) + uintptr(n)*unsafe.Sizeof(location{})))\n }\n \n // dump writes all previously cached stacks to trace buffers,\n // releases all memory and resets state.\n func (tab *traceStackTable) dump() {\n-\tframes := make(map[uintptr]traceFrame)\n \tvar tmp [(2 + 4*traceStackSize) * traceBytesPerNumber]byte\n \tbuf := traceFlush(0).ptr()\n \tfor _, stk := range tab.tab {\n@@ -749,8 +739,8 @@ func (tab *traceStackTable) dump() {\n \t\t\ttmpbuf = traceAppend(tmpbuf, uint64(stk.n))\n \t\t\tfor _, pc := range stk.stack() {\n \t\t\t\tvar frame traceFrame\n-\t\t\t\tframe, buf = traceFrameForPC(buf, frames, pc)\n-\t\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(pc))\n+\t\t\t\tframe, buf = traceFrameForPC(buf, pc)\n+\t\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(pc.pc))\n \t\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(frame.funcID))\n \t\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(frame.fileID))\n \t\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(frame.line))\n@@ -780,25 +770,15 @@ type traceFrame struct {\n \tline   uint64\n }\n \n-func traceFrameForPC(buf *traceBuf, frames map[uintptr]traceFrame, pc uintptr) (traceFrame, *traceBuf) {\n-\tif frame, ok := frames[pc]; ok {\n-\t\treturn frame, buf\n-\t}\n-\n+func traceFrameForPC(buf *traceBuf, loc location) (traceFrame, *traceBuf) {\n \tvar frame traceFrame\n-\tf := findfunc(pc)\n-\tif f == nil {\n-\t\tframes[pc] = frame\n-\t\treturn frame, buf\n-\t}\n-\n-\tfn := funcname(f)\n+\tfn := loc.function\n \tconst maxLen = 1 << 10\n \tif len(fn) > maxLen {\n \t\tfn = fn[len(fn)-maxLen:]\n \t}\n \tframe.funcID, buf = traceString(buf, fn)\n-\tfile, line := funcline(f, pc-sys.PCQuantum)\n+\tfile, line := loc.filename, loc.lineno\n \tframe.line = uint64(line)\n \tif len(file) > maxLen {\n \t\tfile = file[len(file)-maxLen:]\n@@ -807,8 +787,6 @@ func traceFrameForPC(buf *traceBuf, frames map[uintptr]traceFrame, pc uintptr) (\n \treturn frame, buf\n }\n \n-*/\n-\n // traceAlloc is a non-thread-safe region allocator.\n // It holds a linked list of traceAllocBlock.\n type traceAlloc struct {\n@@ -831,16 +809,15 @@ type traceAllocBlockPtr uintptr\n func (p traceAllocBlockPtr) ptr() *traceAllocBlock   { return (*traceAllocBlock)(unsafe.Pointer(p)) }\n func (p *traceAllocBlockPtr) set(x *traceAllocBlock) { *p = traceAllocBlockPtr(unsafe.Pointer(x)) }\n \n-/*\n-Commented out for gccgo for now.\n-\n // alloc allocates n-byte block.\n func (a *traceAlloc) alloc(n uintptr) unsafe.Pointer {\n \tn = round(n, sys.PtrSize)\n \tif a.head == 0 || a.off+n > uintptr(len(a.head.ptr().data)) {\n \t\tif n > uintptr(len(a.head.ptr().data)) {\n \t\t\tthrow(\"trace: alloc too large\")\n \t\t}\n+\t\t// This is only safe because the strings returned by callers\n+\t\t// are stored in a location that is not in the Go heap.\n \t\tblock := (*traceAllocBlock)(sysAlloc(unsafe.Sizeof(traceAllocBlock{}), &memstats.other_sys))\n \t\tif block == nil {\n \t\t\tthrow(\"trace: out of memory\")\n@@ -913,7 +890,7 @@ func traceGoCreate(newg *g, pc uintptr) {\n \tnewg.traceseq = 0\n \tnewg.tracelastp = getg().m.p\n \t// +PCQuantum because traceFrameForPC expects return PCs and subtracts PCQuantum.\n-\tid := trace.stackTab.put([]uintptr{pc + sys.PCQuantum})\n+\tid := trace.stackTab.put([]location{location{pc: pc + sys.PCQuantum}})\n \ttraceEvent(traceEvGoCreate, 2, uint64(newg.goid), uint64(id))\n }\n \n@@ -1004,5 +981,3 @@ func traceHeapAlloc() {\n func traceNextGC() {\n \ttraceEvent(traceEvNextGC, -1, memstats.next_gc)\n }\n-\n-*/"}, {"sha": "c9d50023770788e407cf7603b4217538b79feb30", "filename": "libgo/runtime/heapdump.c", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fheapdump.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fheapdump.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fheapdump.c?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -618,8 +618,7 @@ runtime_debug_WriteHeapDump(uintptr fd)\n \t// Stop the world.\n \truntime_acquireWorldsema();\n \tm = runtime_m();\n-\tm->gcing = 1;\n-\tm->locks++;\n+\tm->preemptoff = runtime_gostringnocopy((const byte*)\"write heap dump\");\n \truntime_stopTheWorldWithSema();\n \n \t// Update stats so we can dump them.\n@@ -640,10 +639,9 @@ runtime_debug_WriteHeapDump(uintptr fd)\n \tdumpfd = 0;\n \n \t// Start up the world again.\n-\tm->gcing = 0;\n-\truntime_releaseWorldsema();\n \truntime_startTheWorldWithSema();\n-\tm->locks--;\n+\truntime_releaseWorldsema();\n+\tm->preemptoff = runtime_gostringnocopy(nil);\n }\n \n // Runs the specified gc program.  Calls the callback for every"}, {"sha": "987431219ca81b72af73d24573da38d9770a318b", "filename": "libgo/runtime/malloc.goc", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fmalloc.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fmalloc.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmalloc.goc?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -99,7 +99,8 @@ runtime_mallocgc(uintptr size, uintptr typ, uint32 flag)\n \t\tflag |= FlagNoInvokeGC;\n \t}\n \n-\tif(runtime_gcwaiting() && g != m->g0 && m->locks == 0 && !(flag & FlagNoInvokeGC) && m->preemptoff.len == 0) {\n+\tif((g->preempt || runtime_gcwaiting()) && g != m->g0 && m->locks == 0 && !(flag & FlagNoInvokeGC) && m->preemptoff.len == 0) {\n+\t\tg->preempt = false;\n \t\truntime_gosched();\n \t\tm = runtime_m();\n \t}"}, {"sha": "00e4166d812b0c41f28e442deabc9c27d7d58838", "filename": "libgo/runtime/malloc.h", "status": "modified", "additions": 10, "deletions": 11, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fmalloc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fmalloc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmalloc.h?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -132,12 +132,6 @@ enum\n #else\n \tMHeapMap_Bits = 32 - PageShift,\n #endif\n-\n-\t// Max number of threads to run garbage collection.\n-\t// 2, 3, and 4 are all plausible maximums depending\n-\t// on the hardware details of the machine.  The garbage\n-\t// collector scales well to 8 cpus.\n-\tMaxGcproc = 8,\n };\n \n // Maximum memory allocation size, a hint for callers.\n@@ -186,7 +180,8 @@ enum\n \n void*\truntime_SysAlloc(uintptr nbytes, uint64 *stat)\n   __asm__ (GOSYM_PREFIX \"runtime.sysAlloc\");\n-void\truntime_SysFree(void *v, uintptr nbytes, uint64 *stat);\n+void\truntime_SysFree(void *v, uintptr nbytes, uint64 *stat)\n+  __asm__ (GOSYM_PREFIX \"runtime.sysFree\");\n void\truntime_SysUnused(void *v, uintptr nbytes);\n void\truntime_SysUsed(void *v, uintptr nbytes);\n void\truntime_SysMap(void *v, uintptr nbytes, bool reserved, uint64 *stat);\n@@ -467,11 +462,15 @@ void\truntime_MProf_GC(void)\n   __asm__ (GOSYM_PREFIX \"runtime.mProf_GC\");\n void\truntime_iterate_memprof(FuncVal* callback)\n   __asm__ (GOSYM_PREFIX \"runtime.iterate_memprof\");\n-int32\truntime_gcprocs(void);\n-void\truntime_helpgc(int32 nproc);\n-void\truntime_gchelper(void);\n+int32\truntime_gcprocs(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.gcprocs\");\n+void\truntime_helpgc(int32 nproc)\n+  __asm__ (GOSYM_PREFIX \"runtime.helpgc\");\n+void\truntime_gchelper(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.gchelper\");\n void\truntime_createfing(void);\n-G*\truntime_wakefing(void);\n+G*\truntime_wakefing(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.wakefing\");\n extern bool\truntime_fingwait;\n extern bool\truntime_fingwake;\n "}, {"sha": "156db0f1f928fb305ce8d35c4ce8465741ffedaf", "filename": "libgo/runtime/mgc0.c", "status": "modified", "additions": 19, "deletions": 5, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fmgc0.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fmgc0.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmgc0.c?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -7,7 +7,7 @@\n // GC is:\n // - mark&sweep\n // - mostly precise (with the exception of some C-allocated objects, assembly frames/arguments, etc)\n-// - parallel (up to MaxGcproc threads)\n+// - parallel (up to _MaxGcproc threads)\n // - partially concurrent (mark is stop-the-world, while sweep is concurrent)\n // - non-moving/non-compacting\n // - full (non-partial)\n@@ -389,7 +389,7 @@ struct BufferList\n \tuint32 busy;\n \tbyte pad[CacheLineSize];\n };\n-static BufferList bufferList[MaxGcproc];\n+static BufferList bufferList[_MaxGcproc];\n \n static void enqueue(Obj obj, Workbuf **_wbuf, Obj **_wp, uintptr *_nobj);\n \n@@ -2228,7 +2228,7 @@ gc(struct gc_args *args)\n \n \tm->locks++;\t// disable gc during mallocs in parforalloc\n \tif(work.markfor == nil)\n-\t\twork.markfor = runtime_parforalloc(MaxGcproc);\n+\t\twork.markfor = runtime_parforalloc(_MaxGcproc);\n \tm->locks--;\n \n \ttm1 = 0;\n@@ -2355,7 +2355,7 @@ gc(struct gc_args *args)\n \t\t\tsweep.g = __go_go(bgsweep, nil);\n \t\telse if(sweep.parked) {\n \t\t\tsweep.parked = false;\n-\t\t\truntime_ready(sweep.g);\n+\t\t\truntime_ready(sweep.g, 0, true);\n \t\t}\n \t\truntime_unlock(&gclock);\n \t} else {\n@@ -2429,7 +2429,7 @@ gchelperstart(void)\n \tM *m;\n \n \tm = runtime_m();\n-\tif(m->helpgc < 0 || m->helpgc >= MaxGcproc)\n+\tif(m->helpgc < 0 || m->helpgc >= _MaxGcproc)\n \t\truntime_throw(\"gchelperstart: bad m->helpgc\");\n \tif(runtime_xchg(&bufferList[m->helpgc].busy, 1))\n \t\truntime_throw(\"gchelperstart: already busy\");\n@@ -2541,6 +2541,20 @@ runtime_createfing(void)\n \truntime_unlock(&gclock);\n }\n \n+bool getfingwait() __asm__(GOSYM_PREFIX \"runtime.getfingwait\");\n+bool\n+getfingwait()\n+{\n+\treturn runtime_fingwait;\n+}\n+\n+bool getfingwake() __asm__(GOSYM_PREFIX \"runtime.getfingwake\");\n+bool\n+getfingwake()\n+{\n+\treturn runtime_fingwake;\n+}\n+\n G*\n runtime_wakefing(void)\n {"}, {"sha": "c4a52839ea8211317668c6e9d555c30171797b0c", "filename": "libgo/runtime/proc.c", "status": "modified", "additions": 60, "deletions": 1340, "changes": 1400, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fproc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fproc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fproc.c?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -365,9 +365,14 @@ extern P** runtime_getAllP()\n   __asm__ (GOSYM_PREFIX \"runtime.getAllP\");\n extern G* allocg(void)\n   __asm__ (GOSYM_PREFIX \"runtime.allocg\");\n+extern bool needaddgcproc(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.needaddgcproc\");\n+extern void startm(P*, bool)\n+  __asm__(GOSYM_PREFIX \"runtime.startm\");\n+extern void newm(void(*)(void), P*)\n+  __asm__(GOSYM_PREFIX \"runtime.newm\");\n \n Sched*\truntime_sched;\n-int32\truntime_gomaxprocs;\n M\truntime_m0;\n G\truntime_g0;\t// idle goroutine for m0\n G*\truntime_lastg;\n@@ -376,51 +381,58 @@ P**\truntime_allp;\n int8*\truntime_goos;\n int32\truntime_ncpu;\n bool\truntime_precisestack;\n-static int32\tnewprocs;\n \n bool\truntime_isarchive;\n \n void* runtime_mstart(void*);\n-static void runqput(P*, G*);\n-static G* runqget(P*);\n-static bool runqputslow(P*, G*, uint32, uint32);\n-static G* runqsteal(P*, P*);\n-static void mput(M*);\n-static M* mget(void);\n static void mcommoninit(M*);\n-static void schedule(void);\n-static void procresize(int32);\n-static void acquirep(P*);\n-static P* releasep(void);\n-static void newm(void(*)(void), P*);\n-static void stopm(void);\n-static void startm(P*, bool);\n-static void handoffp(P*);\n-static void wakep(void);\n-static void stoplockedm(void);\n-static void startlockedm(G*);\n-static void sysmon(void);\n-static uint32 retake(int64);\n-static void incidlelocked(int32);\n static void exitsyscall0(G*);\n static void park0(G*);\n static void goexit0(G*);\n static void gfput(P*, G*);\n static G* gfget(P*);\n-static void gfpurge(P*);\n-static void globrunqput(G*);\n-static void globrunqputbatch(G*, G*, int32);\n-static G* globrunqget(P*, int32);\n-static P* pidleget(void);\n-static void pidleput(P*);\n-static void injectglist(G*);\n-static bool preemptall(void);\n static bool exitsyscallfast(void);\n \n-void allgadd(G*)\n+extern void setncpu(int32)\n+  __asm__(GOSYM_PREFIX \"runtime.setncpu\");\n+extern void allgadd(G*)\n   __asm__(GOSYM_PREFIX \"runtime.allgadd\");\n-void checkdead(void)\n+extern void stopm(void)\n+  __asm__(GOSYM_PREFIX \"runtime.stopm\");\n+extern void handoffp(P*)\n+  __asm__(GOSYM_PREFIX \"runtime.handoffp\");\n+extern void wakep(void)\n+  __asm__(GOSYM_PREFIX \"runtime.wakep\");\n+extern void stoplockedm(void)\n+  __asm__(GOSYM_PREFIX \"runtime.stoplockedm\");\n+extern void schedule(void)\n+  __asm__(GOSYM_PREFIX \"runtime.schedule\");\n+extern void execute(G*, bool)\n+  __asm__(GOSYM_PREFIX \"runtime.execute\");\n+extern void procresize(int32)\n+  __asm__(GOSYM_PREFIX \"runtime.procresize\");\n+extern void acquirep(P*)\n+  __asm__(GOSYM_PREFIX \"runtime.acquirep\");\n+extern P* releasep(void)\n+  __asm__(GOSYM_PREFIX \"runtime.releasep\");\n+extern void incidlelocked(int32)\n+  __asm__(GOSYM_PREFIX \"runtime.incidlelocked\");\n+extern void checkdead(void)\n   __asm__(GOSYM_PREFIX \"runtime.checkdead\");\n+extern void sysmon(void)\n+  __asm__(GOSYM_PREFIX \"runtime.sysmon\");\n+extern void mput(M*)\n+  __asm__(GOSYM_PREFIX \"runtime.mput\");\n+extern M* mget(void)\n+  __asm__(GOSYM_PREFIX \"runtime.mget\");\n+extern void globrunqput(G*)\n+  __asm__(GOSYM_PREFIX \"runtime.globrunqput\");\n+extern P* pidleget(void)\n+  __asm__(GOSYM_PREFIX \"runtime.pidleget\");\n+extern bool runqempty(P*)\n+  __asm__(GOSYM_PREFIX \"runtime.runqempty\");\n+extern void runqput(P*, G*, bool)\n+  __asm__(GOSYM_PREFIX \"runtime.runqput\");\n \n bool runtime_isstarted;\n \n@@ -441,6 +453,7 @@ runtime_schedinit(void)\n \tconst byte *p;\n \tEface i;\n \n+\tsetncpu(runtime_ncpu);\n \truntime_sched = runtime_getsched();\n \n \tm = &runtime_m0;\n@@ -660,234 +673,6 @@ mcommoninit(M *mp)\n \truntime_unlock(&runtime_sched->lock);\n }\n \n-// Mark gp ready to run.\n-void\n-runtime_ready(G *gp)\n-{\n-\t// Mark runnable.\n-\tg->m->locks++;  // disable preemption because it can be holding p in a local var\n-\tif(gp->atomicstatus != _Gwaiting) {\n-\t\truntime_printf(\"goroutine %D has status %d\\n\", gp->goid, gp->atomicstatus);\n-\t\truntime_throw(\"bad g->atomicstatus in ready\");\n-\t}\n-\tgp->atomicstatus = _Grunnable;\n-\trunqput((P*)g->m->p, gp);\n-\tif(runtime_atomicload(&runtime_sched->npidle) != 0 && runtime_atomicload(&runtime_sched->nmspinning) == 0)  // TODO: fast atomic\n-\t\twakep();\n-\tg->m->locks--;\n-}\n-\n-void goready(G*, int) __asm__ (GOSYM_PREFIX \"runtime.goready\");\n-\n-void\n-goready(G* gp, int traceskip __attribute__ ((unused)))\n-{\n-\truntime_ready(gp);\n-}\n-\n-int32\n-runtime_gcprocs(void)\n-{\n-\tint32 n;\n-\n-\t// Figure out how many CPUs to use during GC.\n-\t// Limited by gomaxprocs, number of actual CPUs, and MaxGcproc.\n-\truntime_lock(&runtime_sched->lock);\n-\tn = runtime_gomaxprocs;\n-\tif(n > runtime_ncpu)\n-\t\tn = runtime_ncpu > 0 ? runtime_ncpu : 1;\n-\tif(n > MaxGcproc)\n-\t\tn = MaxGcproc;\n-\tif(n > runtime_sched->nmidle+1) // one M is currently running\n-\t\tn = runtime_sched->nmidle+1;\n-\truntime_unlock(&runtime_sched->lock);\n-\treturn n;\n-}\n-\n-static bool\n-needaddgcproc(void)\n-{\n-\tint32 n;\n-\n-\truntime_lock(&runtime_sched->lock);\n-\tn = runtime_gomaxprocs;\n-\tif(n > runtime_ncpu)\n-\t\tn = runtime_ncpu;\n-\tif(n > MaxGcproc)\n-\t\tn = MaxGcproc;\n-\tn -= runtime_sched->nmidle+1; // one M is currently running\n-\truntime_unlock(&runtime_sched->lock);\n-\treturn n > 0;\n-}\n-\n-void\n-runtime_helpgc(int32 nproc)\n-{\n-\tM *mp;\n-\tint32 n, pos;\n-\n-\truntime_lock(&runtime_sched->lock);\n-\tpos = 0;\n-\tfor(n = 1; n < nproc; n++) {  // one M is currently running\n-\t\tif(runtime_allp[pos]->mcache == g->m->mcache)\n-\t\t\tpos++;\n-\t\tmp = mget();\n-\t\tif(mp == nil)\n-\t\t\truntime_throw(\"runtime_gcprocs inconsistency\");\n-\t\tmp->helpgc = n;\n-\t\tmp->mcache = runtime_allp[pos]->mcache;\n-\t\tpos++;\n-\t\truntime_notewakeup(&mp->park);\n-\t}\n-\truntime_unlock(&runtime_sched->lock);\n-}\n-\n-// Similar to stoptheworld but best-effort and can be called several times.\n-// There is no reverse operation, used during crashing.\n-// This function must not lock any mutexes.\n-void\n-runtime_freezetheworld(void)\n-{\n-\tint32 i;\n-\n-\tif(runtime_gomaxprocs == 1)\n-\t\treturn;\n-\t// stopwait and preemption requests can be lost\n-\t// due to races with concurrently executing threads,\n-\t// so try several times\n-\tfor(i = 0; i < 5; i++) {\n-\t\t// this should tell the scheduler to not start any new goroutines\n-\t\truntime_sched->stopwait = 0x7fffffff;\n-\t\truntime_atomicstore((uint32*)&runtime_sched->gcwaiting, 1);\n-\t\t// this should stop running goroutines\n-\t\tif(!preemptall())\n-\t\t\tbreak;  // no running goroutines\n-\t\truntime_usleep(1000);\n-\t}\n-\t// to be sure\n-\truntime_usleep(1000);\n-\tpreemptall();\n-\truntime_usleep(1000);\n-}\n-\n-void\n-runtime_stopTheWorldWithSema(void)\n-{\n-\tint32 i;\n-\tuint32 s;\n-\tP *p;\n-\tbool wait;\n-\n-\truntime_lock(&runtime_sched->lock);\n-\truntime_sched->stopwait = runtime_gomaxprocs;\n-\truntime_atomicstore((uint32*)&runtime_sched->gcwaiting, 1);\n-\tpreemptall();\n-\t// stop current P\n-\t((P*)g->m->p)->status = _Pgcstop;\n-\truntime_sched->stopwait--;\n-\t// try to retake all P's in _Psyscall status\n-\tfor(i = 0; i < runtime_gomaxprocs; i++) {\n-\t\tp = runtime_allp[i];\n-\t\ts = p->status;\n-\t\tif(s == _Psyscall && runtime_cas(&p->status, s, _Pgcstop))\n-\t\t\truntime_sched->stopwait--;\n-\t}\n-\t// stop idle P's\n-\twhile((p = pidleget()) != nil) {\n-\t\tp->status = _Pgcstop;\n-\t\truntime_sched->stopwait--;\n-\t}\n-\twait = runtime_sched->stopwait > 0;\n-\truntime_unlock(&runtime_sched->lock);\n-\n-\t// wait for remaining P's to stop voluntarily\n-\tif(wait) {\n-\t\truntime_notesleep(&runtime_sched->stopnote);\n-\t\truntime_noteclear(&runtime_sched->stopnote);\n-\t}\n-\tif(runtime_sched->stopwait)\n-\t\truntime_throw(\"stoptheworld: not stopped\");\n-\tfor(i = 0; i < runtime_gomaxprocs; i++) {\n-\t\tp = runtime_allp[i];\n-\t\tif(p->status != _Pgcstop)\n-\t\t\truntime_throw(\"stoptheworld: not stopped\");\n-\t}\n-}\n-\n-static void\n-mhelpgc(void)\n-{\n-\tg->m->helpgc = -1;\n-}\n-\n-void\n-runtime_startTheWorldWithSema(void)\n-{\n-\tP *p, *p1;\n-\tM *mp;\n-\tG *gp;\n-\tbool add;\n-\n-\tg->m->locks++;  // disable preemption because it can be holding p in a local var\n-\tgp = runtime_netpoll(false);  // non-blocking\n-\tinjectglist(gp);\n-\tadd = needaddgcproc();\n-\truntime_lock(&runtime_sched->lock);\n-\tif(newprocs) {\n-\t\tprocresize(newprocs);\n-\t\tnewprocs = 0;\n-\t} else\n-\t\tprocresize(runtime_gomaxprocs);\n-\truntime_sched->gcwaiting = 0;\n-\n-\tp1 = nil;\n-\twhile((p = pidleget()) != nil) {\n-\t\t// procresize() puts p's with work at the beginning of the list.\n-\t\t// Once we reach a p without a run queue, the rest don't have one either.\n-\t\tif(p->runqhead == p->runqtail) {\n-\t\t\tpidleput(p);\n-\t\t\tbreak;\n-\t\t}\n-\t\tp->m = (uintptr)mget();\n-\t\tp->link = (uintptr)p1;\n-\t\tp1 = p;\n-\t}\n-\tif(runtime_sched->sysmonwait) {\n-\t\truntime_sched->sysmonwait = false;\n-\t\truntime_notewakeup(&runtime_sched->sysmonnote);\n-\t}\n-\truntime_unlock(&runtime_sched->lock);\n-\n-\twhile(p1) {\n-\t\tp = p1;\n-\t\tp1 = (P*)p1->link;\n-\t\tif(p->m) {\n-\t\t\tmp = (M*)p->m;\n-\t\t\tp->m = 0;\n-\t\t\tif(mp->nextp)\n-\t\t\t\truntime_throw(\"startTheWorldWithSema: inconsistent mp->nextp\");\n-\t\t\tmp->nextp = (uintptr)p;\n-\t\t\truntime_notewakeup(&mp->park);\n-\t\t} else {\n-\t\t\t// Start M to run P.  Do not start another M below.\n-\t\t\tnewm(nil, p);\n-\t\t\tadd = false;\n-\t\t}\n-\t}\n-\n-\tif(add) {\n-\t\t// If GC could have used another helper proc, start one now,\n-\t\t// in the hope that it will be available next time.\n-\t\t// It would have been even better to start it before the collection,\n-\t\t// but doing so requires allocating memory, so it's tricky to\n-\t\t// coordinate.  This lazy approach works out in practice:\n-\t\t// we don't mind if the first couple gc rounds don't have quite\n-\t\t// the maximum number of procs.\n-\t\tnewm(mhelpgc, nil);\n-\t}\n-\tg->m->locks--;\n-}\n-\n // Called to start an M.\n void*\n runtime_mstart(void* mp)\n@@ -1055,7 +840,7 @@ makeGContext(G* gp, byte* sp, uintptr spsize) {\n }\n \n // Create a new m.  It will start off with a call to fn, or else the scheduler.\n-static void\n+void\n newm(void(*fn)(void), P *p)\n {\n \tM *mp;\n@@ -1067,40 +852,6 @@ newm(void(*fn)(void), P *p)\n \truntime_newosproc(mp);\n }\n \n-// Stops execution of the current m until new work is available.\n-// Returns with acquired P.\n-static void\n-stopm(void)\n-{\n-\tM* m;\n-\n-\tm = g->m;\n-\tif(m->locks)\n-\t\truntime_throw(\"stopm holding locks\");\n-\tif(m->p)\n-\t\truntime_throw(\"stopm holding p\");\n-\tif(m->spinning) {\n-\t\tm->spinning = false;\n-\t\truntime_xadd(&runtime_sched->nmspinning, -1);\n-\t}\n-\n-retry:\n-\truntime_lock(&runtime_sched->lock);\n-\tmput(m);\n-\truntime_unlock(&runtime_sched->lock);\n-\truntime_notesleep(&m->park);\n-\tm = g->m;\n-\truntime_noteclear(&m->park);\n-\tif(m->helpgc) {\n-\t\truntime_gchelper();\n-\t\tm->helpgc = 0;\n-\t\tm->mcache = nil;\n-\t\tgoto retry;\n-\t}\n-\tacquirep((P*)m->nextp);\n-\tm->nextp = 0;\n-}\n-\n static void\n mspinning(void)\n {\n@@ -1109,7 +860,7 @@ mspinning(void)\n \n // Schedules some M to run the p (creates an M if necessary).\n // If p==nil, tries to get an idle P, if no idle P's does nothing.\n-static void\n+void\n startm(P *p, bool spinning)\n {\n \tM *mp;\n@@ -1138,361 +889,12 @@ startm(P *p, bool spinning)\n \t\truntime_throw(\"startm: m is spinning\");\n \tif(mp->nextp)\n \t\truntime_throw(\"startm: m has p\");\n-\tmp->spinning = spinning;\n-\tmp->nextp = (uintptr)p;\n-\truntime_notewakeup(&mp->park);\n-}\n-\n-// Hands off P from syscall or locked M.\n-static void\n-handoffp(P *p)\n-{\n-\t// if it has local work, start it straight away\n-\tif(p->runqhead != p->runqtail || runtime_sched->runqsize) {\n-\t\tstartm(p, false);\n-\t\treturn;\n-\t}\n-\t// no local work, check that there are no spinning/idle M's,\n-\t// otherwise our help is not required\n-\tif(runtime_atomicload(&runtime_sched->nmspinning) + runtime_atomicload(&runtime_sched->npidle) == 0 &&  // TODO: fast atomic\n-\t\truntime_cas(&runtime_sched->nmspinning, 0, 1)) {\n-\t\tstartm(p, true);\n-\t\treturn;\n-\t}\n-\truntime_lock(&runtime_sched->lock);\n-\tif(runtime_sched->gcwaiting) {\n-\t\tp->status = _Pgcstop;\n-\t\tif(--runtime_sched->stopwait == 0)\n-\t\t\truntime_notewakeup(&runtime_sched->stopnote);\n-\t\truntime_unlock(&runtime_sched->lock);\n-\t\treturn;\n-\t}\n-\tif(runtime_sched->runqsize) {\n-\t\truntime_unlock(&runtime_sched->lock);\n-\t\tstartm(p, false);\n-\t\treturn;\n+\tif(spinning && !runqempty(p)) {\n+\t\truntime_throw(\"startm: p has runnable gs\");\n \t}\n-\t// If this is the last running P and nobody is polling network,\n-\t// need to wakeup another M to poll network.\n-\tif(runtime_sched->npidle == (uint32)runtime_gomaxprocs-1 && runtime_atomicload64(&runtime_sched->lastpoll) != 0) {\n-\t\truntime_unlock(&runtime_sched->lock);\n-\t\tstartm(p, false);\n-\t\treturn;\n-\t}\n-\tpidleput(p);\n-\truntime_unlock(&runtime_sched->lock);\n-}\n-\n-// Tries to add one more P to execute G's.\n-// Called when a G is made runnable (newproc, ready).\n-static void\n-wakep(void)\n-{\n-\t// be conservative about spinning threads\n-\tif(!runtime_cas(&runtime_sched->nmspinning, 0, 1))\n-\t\treturn;\n-\tstartm(nil, true);\n-}\n-\n-// Stops execution of the current m that is locked to a g until the g is runnable again.\n-// Returns with acquired P.\n-static void\n-stoplockedm(void)\n-{\n-\tM *m;\n-\tP *p;\n-\n-\tm = g->m;\n-\tif(m->lockedg == nil || m->lockedg->lockedm != m)\n-\t\truntime_throw(\"stoplockedm: inconsistent locking\");\n-\tif(m->p) {\n-\t\t// Schedule another M to run this p.\n-\t\tp = releasep();\n-\t\thandoffp(p);\n-\t}\n-\tincidlelocked(1);\n-\t// Wait until another thread schedules lockedg again.\n-\truntime_notesleep(&m->park);\n-\tm = g->m;\n-\truntime_noteclear(&m->park);\n-\tif(m->lockedg->atomicstatus != _Grunnable)\n-\t\truntime_throw(\"stoplockedm: not runnable\");\n-\tacquirep((P*)m->nextp);\n-\tm->nextp = 0;\n-}\n-\n-// Schedules the locked m to run the locked gp.\n-static void\n-startlockedm(G *gp)\n-{\n-\tM *mp;\n-\tP *p;\n-\n-\tmp = gp->lockedm;\n-\tif(mp == g->m)\n-\t\truntime_throw(\"startlockedm: locked to me\");\n-\tif(mp->nextp)\n-\t\truntime_throw(\"startlockedm: m has p\");\n-\t// directly handoff current P to the locked m\n-\tincidlelocked(-1);\n-\tp = releasep();\n+\tmp->spinning = spinning;\n \tmp->nextp = (uintptr)p;\n \truntime_notewakeup(&mp->park);\n-\tstopm();\n-}\n-\n-// Stops the current m for stoptheworld.\n-// Returns when the world is restarted.\n-static void\n-gcstopm(void)\n-{\n-\tP *p;\n-\n-\tif(!runtime_sched->gcwaiting)\n-\t\truntime_throw(\"gcstopm: not waiting for gc\");\n-\tif(g->m->spinning) {\n-\t\tg->m->spinning = false;\n-\t\truntime_xadd(&runtime_sched->nmspinning, -1);\n-\t}\n-\tp = releasep();\n-\truntime_lock(&runtime_sched->lock);\n-\tp->status = _Pgcstop;\n-\tif(--runtime_sched->stopwait == 0)\n-\t\truntime_notewakeup(&runtime_sched->stopnote);\n-\truntime_unlock(&runtime_sched->lock);\n-\tstopm();\n-}\n-\n-// Schedules gp to run on the current M.\n-// Never returns.\n-static void\n-execute(G *gp)\n-{\n-\tint32 hz;\n-\n-\tif(gp->atomicstatus != _Grunnable) {\n-\t\truntime_printf(\"execute: bad g status %d\\n\", gp->atomicstatus);\n-\t\truntime_throw(\"execute: bad g status\");\n-\t}\n-\tgp->atomicstatus = _Grunning;\n-\tgp->waitsince = 0;\n-\t((P*)g->m->p)->schedtick++;\n-\tg->m->curg = gp;\n-\tgp->m = g->m;\n-\n-\t// Check whether the profiler needs to be turned on or off.\n-\thz = runtime_sched->profilehz;\n-\tif(g->m->profilehz != hz)\n-\t\truntime_resetcpuprofiler(hz);\n-\n-\truntime_gogo(gp);\n-}\n-\n-// Finds a runnable goroutine to execute.\n-// Tries to steal from other P's, get g from global queue, poll network.\n-static G*\n-findrunnable(void)\n-{\n-\tG *gp;\n-\tP *p;\n-\tint32 i;\n-\n-top:\n-\tif(runtime_sched->gcwaiting) {\n-\t\tgcstopm();\n-\t\tgoto top;\n-\t}\n-\tif(runtime_fingwait && runtime_fingwake && (gp = runtime_wakefing()) != nil)\n-\t\truntime_ready(gp);\n-\t// local runq\n-\tgp = runqget((P*)g->m->p);\n-\tif(gp)\n-\t\treturn gp;\n-\t// global runq\n-\tif(runtime_sched->runqsize) {\n-\t\truntime_lock(&runtime_sched->lock);\n-\t\tgp = globrunqget((P*)g->m->p, 0);\n-\t\truntime_unlock(&runtime_sched->lock);\n-\t\tif(gp)\n-\t\t\treturn gp;\n-\t}\n-\t// poll network\n-\tgp = runtime_netpoll(false);  // non-blocking\n-\tif(gp) {\n-\t\tinjectglist((G*)gp->schedlink);\n-\t\tgp->atomicstatus = _Grunnable;\n-\t\treturn gp;\n-\t}\n-\t// If number of spinning M's >= number of busy P's, block.\n-\t// This is necessary to prevent excessive CPU consumption\n-\t// when GOMAXPROCS>>1 but the program parallelism is low.\n-\tif(!g->m->spinning && 2 * runtime_atomicload(&runtime_sched->nmspinning) >= runtime_gomaxprocs - runtime_atomicload(&runtime_sched->npidle))  // TODO: fast atomic\n-\t\tgoto stop;\n-\tif(!g->m->spinning) {\n-\t\tg->m->spinning = true;\n-\t\truntime_xadd(&runtime_sched->nmspinning, 1);\n-\t}\n-\t// random steal from other P's\n-\tfor(i = 0; i < 2*runtime_gomaxprocs; i++) {\n-\t\tif(runtime_sched->gcwaiting)\n-\t\t\tgoto top;\n-\t\tp = runtime_allp[runtime_fastrand1()%runtime_gomaxprocs];\n-\t\tif(p == (P*)g->m->p)\n-\t\t\tgp = runqget(p);\n-\t\telse\n-\t\t\tgp = runqsteal((P*)g->m->p, p);\n-\t\tif(gp)\n-\t\t\treturn gp;\n-\t}\n-stop:\n-\t// return P and block\n-\truntime_lock(&runtime_sched->lock);\n-\tif(runtime_sched->gcwaiting) {\n-\t\truntime_unlock(&runtime_sched->lock);\n-\t\tgoto top;\n-\t}\n-\tif(runtime_sched->runqsize) {\n-\t\tgp = globrunqget((P*)g->m->p, 0);\n-\t\truntime_unlock(&runtime_sched->lock);\n-\t\treturn gp;\n-\t}\n-\tp = releasep();\n-\tpidleput(p);\n-\truntime_unlock(&runtime_sched->lock);\n-\tif(g->m->spinning) {\n-\t\tg->m->spinning = false;\n-\t\truntime_xadd(&runtime_sched->nmspinning, -1);\n-\t}\n-\t// check all runqueues once again\n-\tfor(i = 0; i < runtime_gomaxprocs; i++) {\n-\t\tp = runtime_allp[i];\n-\t\tif(p && p->runqhead != p->runqtail) {\n-\t\t\truntime_lock(&runtime_sched->lock);\n-\t\t\tp = pidleget();\n-\t\t\truntime_unlock(&runtime_sched->lock);\n-\t\t\tif(p) {\n-\t\t\t\tacquirep(p);\n-\t\t\t\tgoto top;\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t}\n-\t// poll network\n-\tif(runtime_xchg64(&runtime_sched->lastpoll, 0) != 0) {\n-\t\tif(g->m->p)\n-\t\t\truntime_throw(\"findrunnable: netpoll with p\");\n-\t\tif(g->m->spinning)\n-\t\t\truntime_throw(\"findrunnable: netpoll with spinning\");\n-\t\tgp = runtime_netpoll(true);  // block until new work is available\n-\t\truntime_atomicstore64(&runtime_sched->lastpoll, runtime_nanotime());\n-\t\tif(gp) {\n-\t\t\truntime_lock(&runtime_sched->lock);\n-\t\t\tp = pidleget();\n-\t\t\truntime_unlock(&runtime_sched->lock);\n-\t\t\tif(p) {\n-\t\t\t\tacquirep(p);\n-\t\t\t\tinjectglist((G*)gp->schedlink);\n-\t\t\t\tgp->atomicstatus = _Grunnable;\n-\t\t\t\treturn gp;\n-\t\t\t}\n-\t\t\tinjectglist(gp);\n-\t\t}\n-\t}\n-\tstopm();\n-\tgoto top;\n-}\n-\n-static void\n-resetspinning(void)\n-{\n-\tint32 nmspinning;\n-\n-\tif(g->m->spinning) {\n-\t\tg->m->spinning = false;\n-\t\tnmspinning = runtime_xadd(&runtime_sched->nmspinning, -1);\n-\t\tif(nmspinning < 0)\n-\t\t\truntime_throw(\"findrunnable: negative nmspinning\");\n-\t} else\n-\t\tnmspinning = runtime_atomicload(&runtime_sched->nmspinning);\n-\n-\t// M wakeup policy is deliberately somewhat conservative (see nmspinning handling),\n-\t// so see if we need to wakeup another P here.\n-\tif (nmspinning == 0 && runtime_atomicload(&runtime_sched->npidle) > 0)\n-\t\twakep();\n-}\n-\n-// Injects the list of runnable G's into the scheduler.\n-// Can run concurrently with GC.\n-static void\n-injectglist(G *glist)\n-{\n-\tint32 n;\n-\tG *gp;\n-\n-\tif(glist == nil)\n-\t\treturn;\n-\truntime_lock(&runtime_sched->lock);\n-\tfor(n = 0; glist; n++) {\n-\t\tgp = glist;\n-\t\tglist = (G*)gp->schedlink;\n-\t\tgp->atomicstatus = _Grunnable;\n-\t\tglobrunqput(gp);\n-\t}\n-\truntime_unlock(&runtime_sched->lock);\n-\n-\tfor(; n && runtime_sched->npidle; n--)\n-\t\tstartm(nil, false);\n-}\n-\n-// One round of scheduler: find a runnable goroutine and execute it.\n-// Never returns.\n-static void\n-schedule(void)\n-{\n-\tG *gp;\n-\tuint32 tick;\n-\n-\tif(g->m->locks)\n-\t\truntime_throw(\"schedule: holding locks\");\n-\n-top:\n-\tif(runtime_sched->gcwaiting) {\n-\t\tgcstopm();\n-\t\tgoto top;\n-\t}\n-\n-\tgp = nil;\n-\t// Check the global runnable queue once in a while to ensure fairness.\n-\t// Otherwise two goroutines can completely occupy the local runqueue\n-\t// by constantly respawning each other.\n-\ttick = ((P*)g->m->p)->schedtick;\n-\t// This is a fancy way to say tick%61==0,\n-\t// it uses 2 MUL instructions instead of a single DIV and so is faster on modern processors.\n-\tif(tick - (((uint64)tick*0x4325c53fu)>>36)*61 == 0 && runtime_sched->runqsize > 0) {\n-\t\truntime_lock(&runtime_sched->lock);\n-\t\tgp = globrunqget((P*)g->m->p, 1);\n-\t\truntime_unlock(&runtime_sched->lock);\n-\t\tif(gp)\n-\t\t\tresetspinning();\n-\t}\n-\tif(gp == nil) {\n-\t\tgp = runqget((P*)g->m->p);\n-\t\tif(gp && g->m->spinning)\n-\t\t\truntime_throw(\"schedule: spinning with local work\");\n-\t}\n-\tif(gp == nil) {\n-\t\tgp = findrunnable();  // blocks until work is available\n-\t\tresetspinning();\n-\t}\n-\n-\tif(gp->lockedm) {\n-\t\t// Hands off own p to the locked m,\n-\t\t// then blocks waiting for a new p.\n-\t\tstartlockedm(gp);\n-\t\tgoto top;\n-\t}\n-\n-\texecute(gp);\n }\n \n // Puts the current goroutine into a waiting state and calls unlockf.\n@@ -1572,12 +974,12 @@ park0(G *gp)\n \t\tm->waitlock = nil;\n \t\tif(!ok) {\n \t\t\tgp->atomicstatus = _Grunnable;\n-\t\t\texecute(gp);  // Schedule it back, never returns.\n+\t\t\texecute(gp, true);  // Schedule it back, never returns.\n \t\t}\n \t}\n \tif(m->lockedg) {\n \t\tstoplockedm();\n-\t\texecute(gp);  // Never returns.\n+\t\texecute(gp, true);  // Never returns.\n \t}\n \tschedule();\n }\n@@ -1606,7 +1008,7 @@ runtime_gosched0(G *gp)\n \truntime_unlock(&runtime_sched->lock);\n \tif(m->lockedg) {\n \t\tstoplockedm();\n-\t\texecute(gp);  // Never returns.\n+\t\texecute(gp, true);  // Never returns.\n \t}\n \tschedule();\n }\n@@ -1643,6 +1045,7 @@ goexit0(G *gp)\n \tgp->writebuf.__capacity = 0;\n \tgp->waitreason = runtime_gostringnocopy(nil);\n \tgp->param = nil;\n+\tm->curg->m = nil;\n \tm->curg = nil;\n \tm->lockedg = nil;\n \tif(m->locked & ~_LockExternal) {\n@@ -1896,12 +1299,12 @@ exitsyscall0(G *gp)\n \truntime_unlock(&runtime_sched->lock);\n \tif(p) {\n \t\tacquirep(p);\n-\t\texecute(gp);  // Never returns.\n+\t\texecute(gp, false);  // Never returns.\n \t}\n \tif(m->lockedg) {\n \t\t// Wait until another thread schedules gp and so m again.\n \t\tstoplockedm();\n-\t\texecute(gp);  // Never returns.\n+\t\texecute(gp, false);  // Never returns.\n \t}\n \tstopm();\n \tschedule();  // Never returns.\n@@ -2069,7 +1472,7 @@ __go_go(void (*fn)(void*), void* arg)\n \n \tmakeGContext(newg, sp, (uintptr)spsize);\n \n-\trunqput(p, newg);\n+\trunqput(p, newg, true);\n \n \tif(runtime_atomicload(&runtime_sched->npidle) != 0 && runtime_atomicload(&runtime_sched->nmspinning) == 0 && fn != runtime_main)  // TODO: fast atomic\n \t\twakep();\n@@ -2126,23 +1529,6 @@ gfget(P *p)\n \treturn gp;\n }\n \n-// Purge all cached G's from gfree list to the global list.\n-static void\n-gfpurge(P *p)\n-{\n-\tG *gp;\n-\n-\truntime_lock(&runtime_sched->gflock);\n-\twhile(p->gfreecnt) {\n-\t\tp->gfreecnt--;\n-\t\tgp = p->gfree;\n-\t\tp->gfree = (G*)gp->schedlink;\n-\t\tgp->schedlink = (uintptr)runtime_sched->gfree;\n-\t\truntime_sched->gfree = gp;\n-\t}\n-\truntime_unlock(&runtime_sched->gflock);\n-}\n-\n void\n runtime_Breakpoint(void)\n {\n@@ -2157,38 +1543,6 @@ runtime_Gosched(void)\n \truntime_gosched();\n }\n \n-// Implementation of runtime.GOMAXPROCS.\n-// delete when scheduler is even stronger\n-\n-intgo runtime_GOMAXPROCS(intgo)\n-  __asm__(GOSYM_PREFIX \"runtime.GOMAXPROCS\");\n-\n-intgo\n-runtime_GOMAXPROCS(intgo n)\n-{\n-\tintgo ret;\n-\n-\tif(n > _MaxGomaxprocs)\n-\t\tn = _MaxGomaxprocs;\n-\truntime_lock(&runtime_sched->lock);\n-\tret = (intgo)runtime_gomaxprocs;\n-\tif(n <= 0 || n == ret) {\n-\t\truntime_unlock(&runtime_sched->lock);\n-\t\treturn ret;\n-\t}\n-\truntime_unlock(&runtime_sched->lock);\n-\n-\truntime_acquireWorldsema();\n-\tg->m->gcing = 1;\n-\truntime_stopTheWorldWithSema();\n-\tnewprocs = (int32)n;\n-\tg->m->gcing = 0;\n-\truntime_releaseWorldsema();\n-\truntime_startTheWorldWithSema();\n-\n-\treturn ret;\n-}\n-\n // lockOSThread is called by runtime.LockOSThread and runtime.lockOSThread below\n // after they modify m->locked. Do not allow preemption during this call,\n // or else the m might be different in this function than in the caller.\n@@ -2365,599 +1719,6 @@ runtime_setcpuprofilerate_m(int32 hz)\n \tg->m->locks--;\n }\n \n-// Change number of processors.  The world is stopped, sched is locked.\n-static void\n-procresize(int32 new)\n-{\n-\tint32 i, old;\n-\tbool pempty;\n-\tG *gp;\n-\tP *p;\n-\tintgo j;\n-\n-\told = runtime_gomaxprocs;\n-\tif(old < 0 || old > _MaxGomaxprocs || new <= 0 || new >_MaxGomaxprocs)\n-\t\truntime_throw(\"procresize: invalid arg\");\n-\t// initialize new P's\n-\tfor(i = 0; i < new; i++) {\n-\t\tp = runtime_allp[i];\n-\t\tif(p == nil) {\n-\t\t\tp = (P*)runtime_mallocgc(sizeof(*p), 0, FlagNoInvokeGC);\n-\t\t\tp->id = i;\n-\t\t\tp->status = _Pgcstop;\n-\t\t\tp->deferpool.__values = &p->deferpoolbuf[0];\n-\t\t\tp->deferpool.__count = 0;\n-\t\t\tp->deferpool.__capacity = nelem(p->deferpoolbuf);\n-\t\t\truntime_atomicstorep(&runtime_allp[i], p);\n-\t\t}\n-\t\tif(p->mcache == nil) {\n-\t\t\tif(old==0 && i==0)\n-\t\t\t\tp->mcache = g->m->mcache;  // bootstrap\n-\t\t\telse\n-\t\t\t\tp->mcache = runtime_allocmcache();\n-\t\t}\n-\t}\n-\n-\t// redistribute runnable G's evenly\n-\t// collect all runnable goroutines in global queue preserving FIFO order\n-\t// FIFO order is required to ensure fairness even during frequent GCs\n-\t// see http://golang.org/issue/7126\n-\tpempty = false;\n-\twhile(!pempty) {\n-\t\tpempty = true;\n-\t\tfor(i = 0; i < old; i++) {\n-\t\t\tp = runtime_allp[i];\n-\t\t\tif(p->runqhead == p->runqtail)\n-\t\t\t\tcontinue;\n-\t\t\tpempty = false;\n-\t\t\t// pop from tail of local queue\n-\t\t\tp->runqtail--;\n-\t\t\tgp = (G*)p->runq[p->runqtail%nelem(p->runq)];\n-\t\t\t// push onto head of global queue\n-\t\t\tgp->schedlink = runtime_sched->runqhead;\n-\t\t\truntime_sched->runqhead = (uintptr)gp;\n-\t\t\tif(runtime_sched->runqtail == 0)\n-\t\t\t\truntime_sched->runqtail = (uintptr)gp;\n-\t\t\truntime_sched->runqsize++;\n-\t\t}\n-\t}\n-\t// fill local queues with at most nelem(p->runq)/2 goroutines\n-\t// start at 1 because current M already executes some G and will acquire allp[0] below,\n-\t// so if we have a spare G we want to put it into allp[1].\n-\tfor(i = 1; (uint32)i < (uint32)new * nelem(p->runq)/2 && runtime_sched->runqsize > 0; i++) {\n-\t\tgp = (G*)runtime_sched->runqhead;\n-\t\truntime_sched->runqhead = gp->schedlink;\n-\t\tif(runtime_sched->runqhead == 0)\n-\t\t\truntime_sched->runqtail = 0;\n-\t\truntime_sched->runqsize--;\n-\t\trunqput(runtime_allp[i%new], gp);\n-\t}\n-\n-\t// free unused P's\n-\tfor(i = new; i < old; i++) {\n-\t\tp = runtime_allp[i];\n-\t\tfor(j = 0; j < p->deferpool.__count; j++) {\n-\t\t\t((struct _defer**)p->deferpool.__values)[j] = nil;\n-\t\t}\n-\t\tp->deferpool.__count = 0;\n-\t\truntime_freemcache(p->mcache);\n-\t\tp->mcache = nil;\n-\t\tgfpurge(p);\n-\t\tp->status = _Pdead;\n-\t\t// can't free P itself because it can be referenced by an M in syscall\n-\t}\n-\n-\tif(g->m->p)\n-\t\t((P*)g->m->p)->m = 0;\n-\tg->m->p = 0;\n-\tg->m->mcache = nil;\n-\tp = runtime_allp[0];\n-\tp->m = 0;\n-\tp->status = _Pidle;\n-\tacquirep(p);\n-\tfor(i = new-1; i > 0; i--) {\n-\t\tp = runtime_allp[i];\n-\t\tp->status = _Pidle;\n-\t\tpidleput(p);\n-\t}\n-\truntime_atomicstore((uint32*)&runtime_gomaxprocs, new);\n-}\n-\n-// Associate p and the current m.\n-static void\n-acquirep(P *p)\n-{\n-\tM *m;\n-\n-\tm = g->m;\n-\tif(m->p || m->mcache)\n-\t\truntime_throw(\"acquirep: already in go\");\n-\tif(p->m || p->status != _Pidle) {\n-\t\truntime_printf(\"acquirep: p->m=%p(%d) p->status=%d\\n\", p->m, p->m ? ((M*)p->m)->id : 0, p->status);\n-\t\truntime_throw(\"acquirep: invalid p state\");\n-\t}\n-\tm->mcache = p->mcache;\n-\tm->p = (uintptr)p;\n-\tp->m = (uintptr)m;\n-\tp->status = _Prunning;\n-}\n-\n-// Disassociate p and the current m.\n-static P*\n-releasep(void)\n-{\n-\tM *m;\n-\tP *p;\n-\n-\tm = g->m;\n-\tif(m->p == 0 || m->mcache == nil)\n-\t\truntime_throw(\"releasep: invalid arg\");\n-\tp = (P*)m->p;\n-\tif((M*)p->m != m || p->mcache != m->mcache || p->status != _Prunning) {\n-\t\truntime_printf(\"releasep: m=%p m->p=%p p->m=%p m->mcache=%p p->mcache=%p p->status=%d\\n\",\n-\t\t\tm, m->p, p->m, m->mcache, p->mcache, p->status);\n-\t\truntime_throw(\"releasep: invalid p state\");\n-\t}\n-\tm->p = 0;\n-\tm->mcache = nil;\n-\tp->m = 0;\n-\tp->status = _Pidle;\n-\treturn p;\n-}\n-\n-static void\n-incidlelocked(int32 v)\n-{\n-\truntime_lock(&runtime_sched->lock);\n-\truntime_sched->nmidlelocked += v;\n-\tif(v > 0)\n-\t\tcheckdead();\n-\truntime_unlock(&runtime_sched->lock);\n-}\n-\n-static void\n-sysmon(void)\n-{\n-\tuint32 idle, delay;\n-\tint64 now, lastpoll, lasttrace;\n-\tG *gp;\n-\n-\tlasttrace = 0;\n-\tidle = 0;  // how many cycles in succession we had not wokeup somebody\n-\tdelay = 0;\n-\tfor(;;) {\n-\t\tif(idle == 0)  // start with 20us sleep...\n-\t\t\tdelay = 20;\n-\t\telse if(idle > 50)  // start doubling the sleep after 1ms...\n-\t\t\tdelay *= 2;\n-\t\tif(delay > 10*1000)  // up to 10ms\n-\t\t\tdelay = 10*1000;\n-\t\truntime_usleep(delay);\n-\t\tif(runtime_debug.schedtrace <= 0 &&\n-\t\t\t(runtime_sched->gcwaiting || runtime_atomicload(&runtime_sched->npidle) == (uint32)runtime_gomaxprocs)) {  // TODO: fast atomic\n-\t\t\truntime_lock(&runtime_sched->lock);\n-\t\t\tif(runtime_atomicload(&runtime_sched->gcwaiting) || runtime_atomicload(&runtime_sched->npidle) == (uint32)runtime_gomaxprocs) {\n-\t\t\t\truntime_atomicstore(&runtime_sched->sysmonwait, 1);\n-\t\t\t\truntime_unlock(&runtime_sched->lock);\n-\t\t\t\truntime_notesleep(&runtime_sched->sysmonnote);\n-\t\t\t\truntime_noteclear(&runtime_sched->sysmonnote);\n-\t\t\t\tidle = 0;\n-\t\t\t\tdelay = 20;\n-\t\t\t} else\n-\t\t\t\truntime_unlock(&runtime_sched->lock);\n-\t\t}\n-\t\t// poll network if not polled for more than 10ms\n-\t\tlastpoll = runtime_atomicload64(&runtime_sched->lastpoll);\n-\t\tnow = runtime_nanotime();\n-\t\tif(lastpoll != 0 && lastpoll + 10*1000*1000 < now) {\n-\t\t\truntime_cas64(&runtime_sched->lastpoll, lastpoll, now);\n-\t\t\tgp = runtime_netpoll(false);  // non-blocking\n-\t\t\tif(gp) {\n-\t\t\t\t// Need to decrement number of idle locked M's\n-\t\t\t\t// (pretending that one more is running) before injectglist.\n-\t\t\t\t// Otherwise it can lead to the following situation:\n-\t\t\t\t// injectglist grabs all P's but before it starts M's to run the P's,\n-\t\t\t\t// another M returns from syscall, finishes running its G,\n-\t\t\t\t// observes that there is no work to do and no other running M's\n-\t\t\t\t// and reports deadlock.\n-\t\t\t\tincidlelocked(-1);\n-\t\t\t\tinjectglist(gp);\n-\t\t\t\tincidlelocked(1);\n-\t\t\t}\n-\t\t}\n-\t\t// retake P's blocked in syscalls\n-\t\t// and preempt long running G's\n-\t\tif(retake(now))\n-\t\t\tidle = 0;\n-\t\telse\n-\t\t\tidle++;\n-\n-\t\tif(runtime_debug.schedtrace > 0 && lasttrace + runtime_debug.schedtrace*1000000ll <= now) {\n-\t\t\tlasttrace = now;\n-\t\t\truntime_schedtrace(runtime_debug.scheddetail);\n-\t\t}\n-\t}\n-}\n-\n-typedef struct Pdesc Pdesc;\n-struct Pdesc\n-{\n-\tuint32\tschedtick;\n-\tint64\tschedwhen;\n-\tuint32\tsyscalltick;\n-\tint64\tsyscallwhen;\n-};\n-static Pdesc pdesc[_MaxGomaxprocs];\n-\n-static uint32\n-retake(int64 now)\n-{\n-\tuint32 i, s, n;\n-\tint64 t;\n-\tP *p;\n-\tPdesc *pd;\n-\n-\tn = 0;\n-\tfor(i = 0; i < (uint32)runtime_gomaxprocs; i++) {\n-\t\tp = runtime_allp[i];\n-\t\tif(p==nil)\n-\t\t\tcontinue;\n-\t\tpd = &pdesc[i];\n-\t\ts = p->status;\n-\t\tif(s == _Psyscall) {\n-\t\t\t// Retake P from syscall if it's there for more than 1 sysmon tick (at least 20us).\n-\t\t\tt = p->syscalltick;\n-\t\t\tif(pd->syscalltick != t) {\n-\t\t\t\tpd->syscalltick = t;\n-\t\t\t\tpd->syscallwhen = now;\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\t// On the one hand we don't want to retake Ps if there is no other work to do,\n-\t\t\t// but on the other hand we want to retake them eventually\n-\t\t\t// because they can prevent the sysmon thread from deep sleep.\n-\t\t\tif(p->runqhead == p->runqtail &&\n-\t\t\t\truntime_atomicload(&runtime_sched->nmspinning) + runtime_atomicload(&runtime_sched->npidle) > 0 &&\n-\t\t\t\tpd->syscallwhen + 10*1000*1000 > now)\n-\t\t\t\tcontinue;\n-\t\t\t// Need to decrement number of idle locked M's\n-\t\t\t// (pretending that one more is running) before the CAS.\n-\t\t\t// Otherwise the M from which we retake can exit the syscall,\n-\t\t\t// increment nmidle and report deadlock.\n-\t\t\tincidlelocked(-1);\n-\t\t\tif(runtime_cas(&p->status, s, _Pidle)) {\n-\t\t\t\tn++;\n-\t\t\t\thandoffp(p);\n-\t\t\t}\n-\t\t\tincidlelocked(1);\n-\t\t} else if(s == _Prunning) {\n-\t\t\t// Preempt G if it's running for more than 10ms.\n-\t\t\tt = p->schedtick;\n-\t\t\tif(pd->schedtick != t) {\n-\t\t\t\tpd->schedtick = t;\n-\t\t\t\tpd->schedwhen = now;\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tif(pd->schedwhen + 10*1000*1000 > now)\n-\t\t\t\tcontinue;\n-\t\t\t// preemptone(p);\n-\t\t}\n-\t}\n-\treturn n;\n-}\n-\n-// Tell all goroutines that they have been preempted and they should stop.\n-// This function is purely best-effort.  It can fail to inform a goroutine if a\n-// processor just started running it.\n-// No locks need to be held.\n-// Returns true if preemption request was issued to at least one goroutine.\n-static bool\n-preemptall(void)\n-{\n-\treturn false;\n-}\n-\n-// Put mp on midle list.\n-// Sched must be locked.\n-static void\n-mput(M *mp)\n-{\n-\tmp->schedlink = runtime_sched->midle;\n-\truntime_sched->midle = (uintptr)mp;\n-\truntime_sched->nmidle++;\n-\tcheckdead();\n-}\n-\n-// Try to get an m from midle list.\n-// Sched must be locked.\n-static M*\n-mget(void)\n-{\n-\tM *mp;\n-\n-\tif((mp = (M*)runtime_sched->midle) != nil){\n-\t\truntime_sched->midle = mp->schedlink;\n-\t\truntime_sched->nmidle--;\n-\t}\n-\treturn mp;\n-}\n-\n-// Put gp on the global runnable queue.\n-// Sched must be locked.\n-static void\n-globrunqput(G *gp)\n-{\n-\tgp->schedlink = 0;\n-\tif(runtime_sched->runqtail)\n-\t\t((G*)runtime_sched->runqtail)->schedlink = (uintptr)gp;\n-\telse\n-\t\truntime_sched->runqhead = (uintptr)gp;\n-\truntime_sched->runqtail = (uintptr)gp;\n-\truntime_sched->runqsize++;\n-}\n-\n-// Put a batch of runnable goroutines on the global runnable queue.\n-// Sched must be locked.\n-static void\n-globrunqputbatch(G *ghead, G *gtail, int32 n)\n-{\n-\tgtail->schedlink = 0;\n-\tif(runtime_sched->runqtail)\n-\t\t((G*)runtime_sched->runqtail)->schedlink = (uintptr)ghead;\n-\telse\n-\t\truntime_sched->runqhead = (uintptr)ghead;\n-\truntime_sched->runqtail = (uintptr)gtail;\n-\truntime_sched->runqsize += n;\n-}\n-\n-// Try get a batch of G's from the global runnable queue.\n-// Sched must be locked.\n-static G*\n-globrunqget(P *p, int32 max)\n-{\n-\tG *gp, *gp1;\n-\tint32 n;\n-\n-\tif(runtime_sched->runqsize == 0)\n-\t\treturn nil;\n-\tn = runtime_sched->runqsize/runtime_gomaxprocs+1;\n-\tif(n > runtime_sched->runqsize)\n-\t\tn = runtime_sched->runqsize;\n-\tif(max > 0 && n > max)\n-\t\tn = max;\n-\tif((uint32)n > nelem(p->runq)/2)\n-\t\tn = nelem(p->runq)/2;\n-\truntime_sched->runqsize -= n;\n-\tif(runtime_sched->runqsize == 0)\n-\t\truntime_sched->runqtail = 0;\n-\tgp = (G*)runtime_sched->runqhead;\n-\truntime_sched->runqhead = gp->schedlink;\n-\tn--;\n-\twhile(n--) {\n-\t\tgp1 = (G*)runtime_sched->runqhead;\n-\t\truntime_sched->runqhead = gp1->schedlink;\n-\t\trunqput(p, gp1);\n-\t}\n-\treturn gp;\n-}\n-\n-// Put p to on pidle list.\n-// Sched must be locked.\n-static void\n-pidleput(P *p)\n-{\n-\tp->link = runtime_sched->pidle;\n-\truntime_sched->pidle = (uintptr)p;\n-\truntime_xadd(&runtime_sched->npidle, 1);  // TODO: fast atomic\n-}\n-\n-// Try get a p from pidle list.\n-// Sched must be locked.\n-static P*\n-pidleget(void)\n-{\n-\tP *p;\n-\n-\tp = (P*)runtime_sched->pidle;\n-\tif(p) {\n-\t\truntime_sched->pidle = p->link;\n-\t\truntime_xadd(&runtime_sched->npidle, -1);  // TODO: fast atomic\n-\t}\n-\treturn p;\n-}\n-\n-// Try to put g on local runnable queue.\n-// If it's full, put onto global queue.\n-// Executed only by the owner P.\n-static void\n-runqput(P *p, G *gp)\n-{\n-\tuint32 h, t;\n-\n-retry:\n-\th = runtime_atomicload(&p->runqhead);  // load-acquire, synchronize with consumers\n-\tt = p->runqtail;\n-\tif(t - h < nelem(p->runq)) {\n-\t\tp->runq[t%nelem(p->runq)] = (uintptr)gp;\n-\t\truntime_atomicstore(&p->runqtail, t+1);  // store-release, makes the item available for consumption\n-\t\treturn;\n-\t}\n-\tif(runqputslow(p, gp, h, t))\n-\t\treturn;\n-\t// the queue is not full, now the put above must suceed\n-\tgoto retry;\n-}\n-\n-// Put g and a batch of work from local runnable queue on global queue.\n-// Executed only by the owner P.\n-static bool\n-runqputslow(P *p, G *gp, uint32 h, uint32 t)\n-{\n-\tG *batch[nelem(p->runq)/2+1];\n-\tuint32 n, i;\n-\n-\t// First, grab a batch from local queue.\n-\tn = t-h;\n-\tn = n/2;\n-\tif(n != nelem(p->runq)/2)\n-\t\truntime_throw(\"runqputslow: queue is not full\");\n-\tfor(i=0; i<n; i++)\n-\t\tbatch[i] = (G*)p->runq[(h+i)%nelem(p->runq)];\n-\tif(!runtime_cas(&p->runqhead, h, h+n))  // cas-release, commits consume\n-\t\treturn false;\n-\tbatch[n] = gp;\n-\t// Link the goroutines.\n-\tfor(i=0; i<n; i++)\n-\t\tbatch[i]->schedlink = (uintptr)batch[i+1];\n-\t// Now put the batch on global queue.\n-\truntime_lock(&runtime_sched->lock);\n-\tglobrunqputbatch(batch[0], batch[n], n+1);\n-\truntime_unlock(&runtime_sched->lock);\n-\treturn true;\n-}\n-\n-// Get g from local runnable queue.\n-// Executed only by the owner P.\n-static G*\n-runqget(P *p)\n-{\n-\tG *gp;\n-\tuint32 t, h;\n-\n-\tfor(;;) {\n-\t\th = runtime_atomicload(&p->runqhead);  // load-acquire, synchronize with other consumers\n-\t\tt = p->runqtail;\n-\t\tif(t == h)\n-\t\t\treturn nil;\n-\t\tgp = (G*)p->runq[h%nelem(p->runq)];\n-\t\tif(runtime_cas(&p->runqhead, h, h+1))  // cas-release, commits consume\n-\t\t\treturn gp;\n-\t}\n-}\n-\n-// Grabs a batch of goroutines from local runnable queue.\n-// batch array must be of size nelem(p->runq)/2. Returns number of grabbed goroutines.\n-// Can be executed by any P.\n-static uint32\n-runqgrab(P *p, G **batch)\n-{\n-\tuint32 t, h, n, i;\n-\n-\tfor(;;) {\n-\t\th = runtime_atomicload(&p->runqhead);  // load-acquire, synchronize with other consumers\n-\t\tt = runtime_atomicload(&p->runqtail);  // load-acquire, synchronize with the producer\n-\t\tn = t-h;\n-\t\tn = n - n/2;\n-\t\tif(n == 0)\n-\t\t\tbreak;\n-\t\tif(n > nelem(p->runq)/2)  // read inconsistent h and t\n-\t\t\tcontinue;\n-\t\tfor(i=0; i<n; i++)\n-\t\t\tbatch[i] = (G*)p->runq[(h+i)%nelem(p->runq)];\n-\t\tif(runtime_cas(&p->runqhead, h, h+n))  // cas-release, commits consume\n-\t\t\tbreak;\n-\t}\n-\treturn n;\n-}\n-\n-// Steal half of elements from local runnable queue of p2\n-// and put onto local runnable queue of p.\n-// Returns one of the stolen elements (or nil if failed).\n-static G*\n-runqsteal(P *p, P *p2)\n-{\n-\tG *gp;\n-\tG *batch[nelem(p->runq)/2];\n-\tuint32 t, h, n, i;\n-\n-\tn = runqgrab(p2, batch);\n-\tif(n == 0)\n-\t\treturn nil;\n-\tn--;\n-\tgp = batch[n];\n-\tif(n == 0)\n-\t\treturn gp;\n-\th = runtime_atomicload(&p->runqhead);  // load-acquire, synchronize with consumers\n-\tt = p->runqtail;\n-\tif(t - h + n >= nelem(p->runq))\n-\t\truntime_throw(\"runqsteal: runq overflow\");\n-\tfor(i=0; i<n; i++, t++)\n-\t\tp->runq[t%nelem(p->runq)] = (uintptr)batch[i];\n-\truntime_atomicstore(&p->runqtail, t);  // store-release, makes the item available for consumption\n-\treturn gp;\n-}\n-\n-void runtime_testSchedLocalQueue(void)\n-  __asm__(\"runtime.testSchedLocalQueue\");\n-\n-void\n-runtime_testSchedLocalQueue(void)\n-{\n-\tP p;\n-\tG gs[nelem(p.runq)];\n-\tint32 i, j;\n-\n-\truntime_memclr((byte*)&p, sizeof(p));\n-\n-\tfor(i = 0; i < (int32)nelem(gs); i++) {\n-\t\tif(runqget(&p) != nil)\n-\t\t\truntime_throw(\"runq is not empty initially\");\n-\t\tfor(j = 0; j < i; j++)\n-\t\t\trunqput(&p, &gs[i]);\n-\t\tfor(j = 0; j < i; j++) {\n-\t\t\tif(runqget(&p) != &gs[i]) {\n-\t\t\t\truntime_printf(\"bad element at iter %d/%d\\n\", i, j);\n-\t\t\t\truntime_throw(\"bad element\");\n-\t\t\t}\n-\t\t}\n-\t\tif(runqget(&p) != nil)\n-\t\t\truntime_throw(\"runq is not empty afterwards\");\n-\t}\n-}\n-\n-void runtime_testSchedLocalQueueSteal(void)\n-  __asm__(\"runtime.testSchedLocalQueueSteal\");\n-\n-void\n-runtime_testSchedLocalQueueSteal(void)\n-{\n-\tP p1, p2;\n-\tG gs[nelem(p1.runq)], *gp;\n-\tint32 i, j, s;\n-\n-\truntime_memclr((byte*)&p1, sizeof(p1));\n-\truntime_memclr((byte*)&p2, sizeof(p2));\n-\n-\tfor(i = 0; i < (int32)nelem(gs); i++) {\n-\t\tfor(j = 0; j < i; j++) {\n-\t\t\tgs[j].sig = 0;\n-\t\t\trunqput(&p1, &gs[j]);\n-\t\t}\n-\t\tgp = runqsteal(&p2, &p1);\n-\t\ts = 0;\n-\t\tif(gp) {\n-\t\t\ts++;\n-\t\t\tgp->sig++;\n-\t\t}\n-\t\twhile((gp = runqget(&p2)) != nil) {\n-\t\t\ts++;\n-\t\t\tgp->sig++;\n-\t\t}\n-\t\twhile((gp = runqget(&p1)) != nil)\n-\t\t\tgp->sig++;\n-\t\tfor(j = 0; j < i; j++) {\n-\t\t\tif(gs[j].sig != 1) {\n-\t\t\t\truntime_printf(\"bad element %d(%d) at iter %d\\n\", j, gs[j].sig, i);\n-\t\t\t\truntime_throw(\"bad element\");\n-\t\t\t}\n-\t\t}\n-\t\tif(s != i/2 && s != i/2+1) {\n-\t\t\truntime_printf(\"bad steal %d, want %d or %d, iter %d\\n\",\n-\t\t\t\ts, i/2, i/2+1, i);\n-\t\t\truntime_throw(\"bad steal\");\n-\t\t}\n-\t}\n-}\n-\n intgo\n runtime_setmaxthreads(intgo in)\n {\n@@ -3041,56 +1802,15 @@ os_beforeExit()\n {\n }\n \n-// Active spinning for sync.Mutex.\n-//go:linkname sync_runtime_canSpin sync.runtime_canSpin\n-\n-enum\n-{\n-\tACTIVE_SPIN = 4,\n-\tACTIVE_SPIN_CNT = 30,\n-};\n-\n-extern _Bool sync_runtime_canSpin(intgo i)\n-  __asm__ (GOSYM_PREFIX \"sync.runtime_canSpin\");\n-\n-_Bool\n-sync_runtime_canSpin(intgo i)\n-{\n-\tP *p;\n-\n-\t// sync.Mutex is cooperative, so we are conservative with spinning.\n-\t// Spin only few times and only if running on a multicore machine and\n-\t// GOMAXPROCS>1 and there is at least one other running P and local runq is empty.\n-\t// As opposed to runtime mutex we don't do passive spinning here,\n-\t// because there can be work on global runq on on other Ps.\n-\tif (i >= ACTIVE_SPIN || runtime_ncpu <= 1 || runtime_gomaxprocs <= (int32)(runtime_sched->npidle+runtime_sched->nmspinning)+1) {\n-\t\treturn false;\n-\t}\n-\tp = (P*)g->m->p;\n-\treturn p != nil && p->runqhead == p->runqtail;\n-}\n-\n-//go:linkname sync_runtime_doSpin sync.runtime_doSpin\n-//go:nosplit\n-\n-extern void sync_runtime_doSpin(void)\n-  __asm__ (GOSYM_PREFIX \"sync.runtime_doSpin\");\n-\n-void\n-sync_runtime_doSpin()\n-{\n-\truntime_procyield(ACTIVE_SPIN_CNT);\n-}\n-\n // For Go code to look at variables, until we port proc.go.\n \n-extern M** runtime_go_allm(void)\n+extern M* runtime_go_allm(void)\n   __asm__ (GOSYM_PREFIX \"runtime.allm\");\n \n-M**\n+M*\n runtime_go_allm()\n {\n-\treturn &runtime_allm;\n+\treturn runtime_allm;\n }\n \n intgo NumCPU(void) __asm__ (GOSYM_PREFIX \"runtime.NumCPU\");"}, {"sha": "438349871c9afa0d0130b55d3a9068356fc79040", "filename": "libgo/runtime/runtime.h", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fruntime.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1/libgo%2Fruntime%2Fruntime.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.h?ref=2193ad7fbf3e917a0ef5a2b48e13f84da1be44f1", "patch": "@@ -240,7 +240,6 @@ extern\tG*\truntime_lastg;\n extern\tM*\truntime_allm;\n extern\tP**\truntime_allp;\n extern\tSched*  runtime_sched;\n-extern\tint32\truntime_gomaxprocs;\n extern\tuint32\truntime_panicking(void)\n   __asm__ (GOSYM_PREFIX \"runtime.getPanicking\");\n extern\tint8*\truntime_goos;\n@@ -260,7 +259,8 @@ extern\tbool\truntime_isarchive;\n intgo\truntime_findnull(const byte*)\n   __asm__ (GOSYM_PREFIX \"runtime.findnull\");\n \n-void\truntime_gogo(G*);\n+void\truntime_gogo(G*)\n+  __asm__ (GOSYM_PREFIX \"runtime.gogo\");\n struct __go_func_type;\n void\truntime_args(int32, byte**)\n   __asm__ (GOSYM_PREFIX \"runtime.args\");\n@@ -294,7 +294,8 @@ void\truntime_printtrace(Slice, G*)\n #define runtime_read(d, v, n) read((d), (v), (n))\n #define runtime_write(d, v, n) write((d), (v), (n))\n #define runtime_close(d) close(d)\n-void\truntime_ready(G*);\n+void\truntime_ready(G*, intgo, bool)\n+  __asm__ (GOSYM_PREFIX \"runtime.ready\");\n String\truntime_getenv(const char*);\n int32\truntime_atoi(const byte*, intgo);\n void*\truntime_mstart(void*);\n@@ -307,7 +308,8 @@ void\truntime_signalstack(byte*, uintptr)\n   __asm__ (GOSYM_PREFIX \"runtime.signalstack\");\n MCache*\truntime_allocmcache(void)\n   __asm__ (GOSYM_PREFIX \"runtime.allocmcache\");\n-void\truntime_freemcache(MCache*);\n+void\truntime_freemcache(MCache*)\n+  __asm__ (GOSYM_PREFIX \"runtime.freemcache\");\n void\truntime_mallocinit(void);\n void\truntime_mprofinit(void);\n #define runtime_getcallersp(p) __builtin_frame_address(0)\n@@ -368,8 +370,6 @@ int64\truntime_unixnanotime(void) // real time, can skip\n void\truntime_dopanic(int32) __attribute__ ((noreturn));\n void\truntime_startpanic(void)\n   __asm__ (GOSYM_PREFIX \"runtime.startpanic\");\n-void\truntime_freezetheworld(void)\n-  __asm__ (GOSYM_PREFIX \"runtime.freezetheworld\");\n void\truntime_unwindstack(G*, byte*);\n void\truntime_sigprof()\n   __asm__ (GOSYM_PREFIX \"runtime.sigprof\");"}]}