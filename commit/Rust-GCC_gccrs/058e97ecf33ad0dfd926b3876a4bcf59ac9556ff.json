{"sha": "058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDU4ZTk3ZWNmMzNhZDBkZmQ5MjZiMzg3NmE0YmNmNTlhYzk1NTZmZg==", "commit": {"author": {"name": "Vladimir Makarov", "email": "vmakarov@redhat.com", "date": "2008-08-26T12:39:58Z"}, "committer": {"name": "Vladimir Makarov", "email": "vmakarov@gcc.gnu.org", "date": "2008-08-26T12:39:58Z"}, "message": "[multiple changes]\n\n2008-08-26  Vladimir Makarov  <vmakarov@redhat.com>\n\n\t* ira-build.c, ira-color.c, ira-costs.c, ira.h, ira-lives.c,\n\tira.c, ira-conflicts.c, ira-emit.c, ira-int.h: New files.\n\n\t* doc/passes.texi: Describe IRA.\n\n\t* doc/tm.texi (IRA_COVER_CLASSES,\n\tIRA_HARD_REGNO_ADD_COST_MULTIPLIER): Describe the new macros.\n\n\t* doc/invoke.texi (ira-max-loops-num): Describe the new parameter.\n\t(-fira, -fira-algorithm, -fira-coalesce, -fno-ira-move-spills,\n\t-fira-propagate-cost, -fno-ira-share-save-slots,\n\t-fno-ira-share-spill-slots, -fira-verbose): Describe new options.\n\n\t* flags.h (ira_algorithm): New enumeration.\n\t(flag_ira_algorithm, flag_ira_verbose): New external variable\n\tdeclarations.\n\n\t* postreload.c (gate_handle_postreload): Don't do post reload\n\toptimizations unless the reload is completed.\n\n\t* reload.c (push_reload, find_dummy_reload): Use DF_LR_OUT for\n\tIRA.\n\n\t* tree-pass.h (pass_ira): New external variable declaration.\n\n\t* reload.h: Add 2008 to the Copyright.\n    \n\t* cfgloopanal.c: Include params.h.\n\t(estimate_reg_pressure_cost): Decrease cost for IRA optimization\n\tmode.\n    \n\t* params.h (IRA_MAX_LOOPS_NUM): New macro.\n\n\t* toplev.c (ira.h): New include.\n\t(flag_ira_algorithm, flag_ira_verbose): New external variables.\n\t(backend_init_target): Call ira_init.\n\t(backend_init): Call ira_init_once.\n\t(finalize): Call finish_ira_once.\n\n\t* toplev.h (flag_ira, flag_ira_coalesce, flag_ira_move_spills,\n\tflag_ira_share_save_slots, flag_ira_share_spill_slots): New\n\texternal variables.\n\n\t* regs.h (contains_reg_of_mode, move_cost, may_move_in_cost,\n\tmay_move_out_cost): New external variable declarations.\n\t(move_table): New typedef.\n    \n\t* caller-save.c: Include headers output.h and ira.h.\n\t(no_caller_save_reg_set): New global variable.\n\t(save_slots_num, save_slots): New variables.\n\t(reg_save_code, reg_restore_code, add_stored_regs): Add\n\tprototypes.\n\t(init_caller_save): Set up no_caller_save_reg_set.\n\t(init_save_areas): Reset save_slots_num.\n\t(saved_hard_reg): New structure.\n\t(hard_reg_map, saved_regs_num, all_saved_regs): New variables.\n\t(initiate_saved_hard_regs, new_saved_hard_reg,\n\tfinish_saved_hard_regs, saved_hard_reg_compare_func): New\n\tfunctions.\n\t(setup_save_areas): Add code for sharing stack slots.\n\t(all_blocks): New variable.\n\t(save_call_clobbered_regs): Process pseudo-register too.\n\t(mark_set_regs): Process pseudo-register too.\n\t(insert_one_insn): Put the insn after bb note in a empty basic\n\tblock.  Add insn check.\n    \n\t* global.c (eliminable_regset): Make it external.\n\t(mark_elimination): Use DF_LR_IN for IRA.\n\t(pseudo_for_reload_consideration_p): New.\n\t(build_insn_chain): Make it external.  Don't ignore spilled\n\tpseudos for IRA.  Use pseudo_for_reload_consideration_p.\n\t(gate_handle_global_alloc): New function.\n\t(pass_global_alloc): Add the gate function.\n\n\t* opts.c (decode_options): Set up flag_ira.  Print the warning for\n\t-fira.\n\t(common_handle_option): Process -fira-algorithm and -fira-verbose.\n\n\t* timevar.def (TV_IRA, TV_RELOAD): New passes.\n\n\t* regmove.c (regmove_optimize): Don't do replacement of output for\n\tIRA.\n\n\t* hard-reg-set.h (no_caller_save_reg_set, reg_class_subclasses):\n\tNew external variable declarations.\n\n\t* local-alloc.c (update_equiv_regs): Make it external.  Return\n\ttrue if jump label rebuilding should be done.  Rescan new_insn for\n\tnotes.\n\t(gate_handle_local_alloc): New function.\n\t(pass_local_alloc): Add the gate function.\n\n\t* alias.c (value_addr_p, stack_addr_p): New functions.\n\t(nonoverlapping_memrefs_p): Use them for IRA.\n\n\t* common.opt (fira, fira-algorithm, fira-coalesce,\n\tfira-move-spills, fira-share-save-slots, fira-share-spill-slots,\n\tfira-verbose): New options.\n\n\t* regclass.c (reg_class_subclasses, contains_reg_of_mode,\n\tmove_cost, may_move_in_cost, may_move_out_cost): Make the\n\tvariables external.\n\t(move_table): Remove typedef.\n\t(init_move_cost): Make it external.\n\t(allocate_reg_info, resize_reg_info, setup_reg_classes): New\n\tfunctions.\n\n\t* rtl.h (init_move_cost, allocate_reg_info, resize_reg_info,\n\tsetup_reg_classes): New function prototypes.\n\t(eliminable_regset): New external variable declaration.\n\t(build_insn_chain, update_equiv_regs): New function prototypes.\n    \n\t* Makefile.in (IRA_INT_H): New definition.\n\t(OBJS-common): Add ira.o, ira-build.o, ira-costs.o,\n\tira-conflicts.o, ira-color.o, ira-emit.o, and ira-lives.o.\n\t(reload1.o, toplev.o): Add dependence on ira.h.\n\t(cfgloopanal.o): Add PARAMS_H.\n\t(caller-save.o): Add dependence on output.h and ira.h.\n\t(ira.o, ira-build.o, ira-costs.o, ira-conflicts.o, ira-color.o,\n\tira-emit.o, ira-lives.o): New entries.\n\n\t* passes.c (pass_ira): New pass.\n\n\t* params.def (PARAM_IRA_MAX_LOOPS_NUM): New parameter.\n\n\t* reload1.c (ira.h): Include the header.\n\t(changed_allocation_pseudos): New bitmap.\n\t(init_reload): Initiate the bitmap.\n\t(compute_use_by_pseudos): Permits spilled registers in FROM.\n\t(temp_pseudo_reg_arr): New variable.\n\t(reload): Allocate and free temp_pseudo_reg_arr.  Sort pseudos for\n\tIRA.  Call alter_reg with the additional parameter.  Don't clear\n\tspilled_pseudos for IRA.  Restore original insn chain for IRA.\n\tClear changed_allocation_pseudos at the end of reload.\n\t(calculate_needs_all_insns): Call IRA's mark_memory_move_deletion.\n\t(hard_regno_to_pseudo_regno): New variable.\n\t(count_pseudo): Check spilled pseudos.  Set up\n\thard_regno_to_pseudo_regno.\n\t(count_spilled_pseudo): Check spilled pseudos. Update\n\thard_regno_to_pseudo_regno.\n\t(find_reg): Use better_spill_reload_regno_p.  Check\n\thard_regno_to_pseudo_regno.\n\t(alter_reg): Set up spilled_pseudos.  Add a new parameter.  Add\n\tcode for IRA.\n\t(eliminate_regs_1): Use additional parameter for alter_reg.\n\t(finish_spills): Set up pseudo_previous_regs only for spilled\n\tpseudos.  Call reassign_pseudos once for all spilled pseudos, pass\n\tmore arguments.  Don't clear live_throughout and dead_or_set for\n\tspilled pseudos.  Use additional parameter for alter_reg.  Call\n\tmark_allocation_change.  Set up changed_allocation_pseudos.\n\tRemove sanity check.\n\t(emit_input_reload_insns, delete_output_reload): Use additional\n\tparameter for alter_reg.  Call mark_allocation_change.\n\t(substitute, gen_reload_chain_without_interm_reg_p): New\n\tfunctions.\n\t(reloads_conflict): Use gen_reload_chain_without_interm_reg_p.\n    \n\t* testsuite/gcc.dg/20080410-1.c: New file.\n\t\n\t* config/s390/s390.h (IRA_COVER_CLASSES,\n\tIRA_HARD_REGNO_ADD_COST_MULTIPLIER): Define.\n\n\t* config/sparc/sparc.h (IRA_COVER_CLASSES): New macro.\n\n\t* config/i386/i386.h (IRA_COVER_CLASSES): Ditto.\n\n\t* config/ia64/ia64.h (IRA_COVER_CLASSES): Ditto.\n\n\t* config/rs6000/rs6000.h (IRA_COVER_CLASSES): Ditto.\n\n\t* config/arm/arm.h (IRA_COVER_CLASSES): Ditto.\n    \n\t* config/alpha/alpha.h (IRA_COVER_CLASSES): Ditto.\n    \n\t2008-08-24  Jeff Law  <law@redhat.com>\n\t* ira.c (setup_reg_class_intersect_union): Prefer smallest class\n\twhen ignoring unavailable registers.\n\n\t2008-08-24  Jeff Law  <law@redhat.com>\n\t* ira-color.c (coalesced_pseudo_reg_slot_compare): Check\n\tFRAME_GROWS_DOWNWARD and STACK_GROWS_DOWNWARD.\n\t* ira.c (setup_eliminable_regset): Check stack_realign_needed.\n\t* config/mn10300/mn10300.h (IRA_COVER_CLASSES): New macro.\n\n\t2008-06-03 Steve Chamberlain <steve.chamberlain@gmail.com>\n\t* ira-build.c (allocno_range_compare_func): Stabilize sort.\n\n\t2008-05-29 Andy Hutchinson <hutchinsonandy@aim.com>\n\t* config/avr/avr.h (IRA_COVER_CLASSES): New macro.\n\t* reload1.c (find_reg): Process registers in register allocation order.\n\n\t2008-05-10 Richard Sandiford <rsandifo@nildram.co.uk>\n\t* toplev.c (backend_init_target): Move ira_init call from\n\there...\n\t(lang_dependent_init_target): ...to here.\n\n\t2008-05-10 Richard Sandiford <rsandifo@nildram.co.uk>\n\t* ira.c (setup_class_subset_and_memory_move_costs): Don't\n\tcalculate memory move costs for NO_REGS.\n\n\t2008-05-05 Kaz Kojima <kkojima@gcc.gnu.org>\n\t* ira-color.c (ira_fast_allocation): Use no_stack_reg_p only if\n\tSTACK_REGS is defined.\n\n\t2008-04-08 Andrew Pinski <andrew_pinski@playstation.sony.com>\n\t* config/spu/spu.h (IRA_COVER_CLASSES): New macro.\n\n\t2008-04-04 Bernd Schmidt <bernd.schmidt@analog.com>\n\t* config/bfin/bfin.h (IRA_COVER_CLASSES): New macro.\n\n\t2008-04-04 Kaz Kojima <kkojima@gcc.gnu.org>\n\t* config/sh/sh.h (IRA_COVER_CLASSES): Define.\n\t* config/sh/sh.md (movsicc_true+3): Check if emit returns a\n\tbarrier.\n\nFrom-SVN: r139590", "tree": {"sha": "6a33af204d23b09732010003bb7079bf0835f4df", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6a33af204d23b09732010003bb7079bf0835f4df"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "html_url": "https://github.com/Rust-GCC/gccrs/commit/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/comments", "author": {"login": "vnmakarov", "id": 9855671, "node_id": "MDQ6VXNlcjk4NTU2NzE=", "avatar_url": "https://avatars.githubusercontent.com/u/9855671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vnmakarov", "html_url": "https://github.com/vnmakarov", "followers_url": "https://api.github.com/users/vnmakarov/followers", "following_url": "https://api.github.com/users/vnmakarov/following{/other_user}", "gists_url": "https://api.github.com/users/vnmakarov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vnmakarov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vnmakarov/subscriptions", "organizations_url": "https://api.github.com/users/vnmakarov/orgs", "repos_url": "https://api.github.com/users/vnmakarov/repos", "events_url": "https://api.github.com/users/vnmakarov/events{/privacy}", "received_events_url": "https://api.github.com/users/vnmakarov/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "8ff27c248ca53aa53d6f2a19d2ee1ce6220013c1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8ff27c248ca53aa53d6f2a19d2ee1ce6220013c1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8ff27c248ca53aa53d6f2a19d2ee1ce6220013c1"}], "stats": {"total": 14886, "additions": 14707, "deletions": 179}, "files": [{"sha": "2c24ea10052e26765aa4a8c041ae73f936fd71b9", "filename": "gcc/ChangeLog", "status": "modified", "additions": 217, "deletions": 0, "changes": 217, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1,3 +1,220 @@\n+2008-08-26  Vladimir Makarov  <vmakarov@redhat.com>\n+\n+\t* ira-build.c, ira-color.c, ira-costs.c, ira.h, ira-lives.c,\n+\tira.c, ira-conflicts.c, ira-emit.c, ira-int.h: New files.\n+\n+\t* doc/passes.texi: Describe IRA.\n+\n+\t* doc/tm.texi (IRA_COVER_CLASSES,\n+\tIRA_HARD_REGNO_ADD_COST_MULTIPLIER): Describe the new macros.\n+\n+\t* doc/invoke.texi (ira-max-loops-num): Describe the new parameter.\n+\t(-fira, -fira-algorithm, -fira-coalesce, -fno-ira-move-spills,\n+\t-fira-propagate-cost, -fno-ira-share-save-slots,\n+\t-fno-ira-share-spill-slots, -fira-verbose): Describe new options.\n+\n+\t* flags.h (ira_algorithm): New enumeration.\n+\t(flag_ira_algorithm, flag_ira_verbose): New external variable\n+\tdeclarations.\n+\n+\t* postreload.c (gate_handle_postreload): Don't do post reload\n+\toptimizations unless the reload is completed.\n+\n+\t* reload.c (push_reload, find_dummy_reload): Use DF_LR_OUT for\n+\tIRA.\n+\n+\t* tree-pass.h (pass_ira): New external variable declaration.\n+\n+\t* reload.h: Add 2008 to the Copyright.\n+    \n+\t* cfgloopanal.c: Include params.h.\n+\t(estimate_reg_pressure_cost): Decrease cost for IRA optimization\n+\tmode.\n+    \n+\t* params.h (IRA_MAX_LOOPS_NUM): New macro.\n+\n+\t* toplev.c (ira.h): New include.\n+\t(flag_ira_algorithm, flag_ira_verbose): New external variables.\n+\t(backend_init_target): Call ira_init.\n+\t(backend_init): Call ira_init_once.\n+\t(finalize): Call finish_ira_once.\n+\n+\t* toplev.h (flag_ira, flag_ira_coalesce, flag_ira_move_spills,\n+\tflag_ira_share_save_slots, flag_ira_share_spill_slots): New\n+\texternal variables.\n+\n+\t* regs.h (contains_reg_of_mode, move_cost, may_move_in_cost,\n+\tmay_move_out_cost): New external variable declarations.\n+\t(move_table): New typedef.\n+    \n+\t* caller-save.c: Include headers output.h and ira.h.\n+\t(no_caller_save_reg_set): New global variable.\n+\t(save_slots_num, save_slots): New variables.\n+\t(reg_save_code, reg_restore_code, add_stored_regs): Add\n+\tprototypes.\n+\t(init_caller_save): Set up no_caller_save_reg_set.\n+\t(init_save_areas): Reset save_slots_num.\n+\t(saved_hard_reg): New structure.\n+\t(hard_reg_map, saved_regs_num, all_saved_regs): New variables.\n+\t(initiate_saved_hard_regs, new_saved_hard_reg,\n+\tfinish_saved_hard_regs, saved_hard_reg_compare_func): New\n+\tfunctions.\n+\t(setup_save_areas): Add code for sharing stack slots.\n+\t(all_blocks): New variable.\n+\t(save_call_clobbered_regs): Process pseudo-register too.\n+\t(mark_set_regs): Process pseudo-register too.\n+\t(insert_one_insn): Put the insn after bb note in a empty basic\n+\tblock.  Add insn check.\n+    \n+\t* global.c (eliminable_regset): Make it external.\n+\t(mark_elimination): Use DF_LR_IN for IRA.\n+\t(pseudo_for_reload_consideration_p): New.\n+\t(build_insn_chain): Make it external.  Don't ignore spilled\n+\tpseudos for IRA.  Use pseudo_for_reload_consideration_p.\n+\t(gate_handle_global_alloc): New function.\n+\t(pass_global_alloc): Add the gate function.\n+\n+\t* opts.c (decode_options): Set up flag_ira.  Print the warning for\n+\t-fira.\n+\t(common_handle_option): Process -fira-algorithm and -fira-verbose.\n+\n+\t* timevar.def (TV_IRA, TV_RELOAD): New passes.\n+\n+\t* regmove.c (regmove_optimize): Don't do replacement of output for\n+\tIRA.\n+\n+\t* hard-reg-set.h (no_caller_save_reg_set, reg_class_subclasses):\n+\tNew external variable declarations.\n+\n+\t* local-alloc.c (update_equiv_regs): Make it external.  Return\n+\ttrue if jump label rebuilding should be done.  Rescan new_insn for\n+\tnotes.\n+\t(gate_handle_local_alloc): New function.\n+\t(pass_local_alloc): Add the gate function.\n+\n+\t* alias.c (value_addr_p, stack_addr_p): New functions.\n+\t(nonoverlapping_memrefs_p): Use them for IRA.\n+\n+\t* common.opt (fira, fira-algorithm, fira-coalesce,\n+\tfira-move-spills, fira-share-save-slots, fira-share-spill-slots,\n+\tfira-verbose): New options.\n+\n+\t* regclass.c (reg_class_subclasses, contains_reg_of_mode,\n+\tmove_cost, may_move_in_cost, may_move_out_cost): Make the\n+\tvariables external.\n+\t(move_table): Remove typedef.\n+\t(init_move_cost): Make it external.\n+\t(allocate_reg_info, resize_reg_info, setup_reg_classes): New\n+\tfunctions.\n+\n+\t* rtl.h (init_move_cost, allocate_reg_info, resize_reg_info,\n+\tsetup_reg_classes): New function prototypes.\n+\t(eliminable_regset): New external variable declaration.\n+\t(build_insn_chain, update_equiv_regs): New function prototypes.\n+    \n+\t* Makefile.in (IRA_INT_H): New definition.\n+\t(OBJS-common): Add ira.o, ira-build.o, ira-costs.o,\n+\tira-conflicts.o, ira-color.o, ira-emit.o, and ira-lives.o.\n+\t(reload1.o, toplev.o): Add dependence on ira.h.\n+\t(cfgloopanal.o): Add PARAMS_H.\n+\t(caller-save.o): Add dependence on output.h and ira.h.\n+\t(ira.o, ira-build.o, ira-costs.o, ira-conflicts.o, ira-color.o,\n+\tira-emit.o, ira-lives.o): New entries.\n+\n+\t* passes.c (pass_ira): New pass.\n+\n+\t* params.def (PARAM_IRA_MAX_LOOPS_NUM): New parameter.\n+\n+\t* reload1.c (ira.h): Include the header.\n+\t(changed_allocation_pseudos): New bitmap.\n+\t(init_reload): Initiate the bitmap.\n+\t(compute_use_by_pseudos): Permits spilled registers in FROM.\n+\t(temp_pseudo_reg_arr): New variable.\n+\t(reload): Allocate and free temp_pseudo_reg_arr.  Sort pseudos for\n+\tIRA.  Call alter_reg with the additional parameter.  Don't clear\n+\tspilled_pseudos for IRA.  Restore original insn chain for IRA.\n+\tClear changed_allocation_pseudos at the end of reload.\n+\t(calculate_needs_all_insns): Call IRA's mark_memory_move_deletion.\n+\t(hard_regno_to_pseudo_regno): New variable.\n+\t(count_pseudo): Check spilled pseudos.  Set up\n+\thard_regno_to_pseudo_regno.\n+\t(count_spilled_pseudo): Check spilled pseudos. Update\n+\thard_regno_to_pseudo_regno.\n+\t(find_reg): Use better_spill_reload_regno_p.  Check\n+\thard_regno_to_pseudo_regno.\n+\t(alter_reg): Set up spilled_pseudos.  Add a new parameter.  Add\n+\tcode for IRA.\n+\t(eliminate_regs_1): Use additional parameter for alter_reg.\n+\t(finish_spills): Set up pseudo_previous_regs only for spilled\n+\tpseudos.  Call reassign_pseudos once for all spilled pseudos, pass\n+\tmore arguments.  Don't clear live_throughout and dead_or_set for\n+\tspilled pseudos.  Use additional parameter for alter_reg.  Call\n+\tmark_allocation_change.  Set up changed_allocation_pseudos.\n+\tRemove sanity check.\n+\t(emit_input_reload_insns, delete_output_reload): Use additional\n+\tparameter for alter_reg.  Call mark_allocation_change.\n+\t(substitute, gen_reload_chain_without_interm_reg_p): New\n+\tfunctions.\n+\t(reloads_conflict): Use gen_reload_chain_without_interm_reg_p.\n+    \n+\t* testsuite/gcc.dg/20080410-1.c: New file.\n+\t\n+\t* config/s390/s390.h (IRA_COVER_CLASSES,\n+\tIRA_HARD_REGNO_ADD_COST_MULTIPLIER): Define.\n+\n+\t* config/sparc/sparc.h (IRA_COVER_CLASSES): New macro.\n+\n+\t* config/i386/i386.h (IRA_COVER_CLASSES): Ditto.\n+\n+\t* config/ia64/ia64.h (IRA_COVER_CLASSES): Ditto.\n+\n+\t* config/rs6000/rs6000.h (IRA_COVER_CLASSES): Ditto.\n+\n+\t* config/arm/arm.h (IRA_COVER_CLASSES): Ditto.\n+    \n+\t* config/alpha/alpha.h (IRA_COVER_CLASSES): Ditto.\n+    \n+\t2008-08-24  Jeff Law  <law@redhat.com>\n+\t* ira.c (setup_reg_class_intersect_union): Prefer smallest class\n+\twhen ignoring unavailable registers.\n+\n+\t2008-08-24  Jeff Law  <law@redhat.com>\n+\t* ira-color.c (coalesced_pseudo_reg_slot_compare): Check\n+\tFRAME_GROWS_DOWNWARD and STACK_GROWS_DOWNWARD.\n+\t* ira.c (setup_eliminable_regset): Check stack_realign_needed.\n+\t* config/mn10300/mn10300.h (IRA_COVER_CLASSES): New macro.\n+\n+\t2008-06-03 Steve Chamberlain <steve.chamberlain@gmail.com>\n+\t* ira-build.c (allocno_range_compare_func): Stabilize sort.\n+\n+\t2008-05-29 Andy Hutchinson <hutchinsonandy@aim.com>\n+\t* config/avr/avr.h (IRA_COVER_CLASSES): New macro.\n+\t* reload1.c (find_reg): Process registers in register allocation order.\n+\n+\t2008-05-10 Richard Sandiford <rsandifo@nildram.co.uk>\n+\t* toplev.c (backend_init_target): Move ira_init call from\n+\there...\n+\t(lang_dependent_init_target): ...to here.\n+\n+\t2008-05-10 Richard Sandiford <rsandifo@nildram.co.uk>\n+\t* ira.c (setup_class_subset_and_memory_move_costs): Don't\n+\tcalculate memory move costs for NO_REGS.\n+\n+\t2008-05-05 Kaz Kojima <kkojima@gcc.gnu.org>\n+\t* ira-color.c (ira_fast_allocation): Use no_stack_reg_p only if\n+\tSTACK_REGS is defined.\n+\n+\t2008-04-08 Andrew Pinski <andrew_pinski@playstation.sony.com>\n+\t* config/spu/spu.h (IRA_COVER_CLASSES): New macro.\n+\n+\t2008-04-04 Bernd Schmidt <bernd.schmidt@analog.com>\n+\t* config/bfin/bfin.h (IRA_COVER_CLASSES): New macro.\n+\n+\t2008-04-04 Kaz Kojima <kkojima@gcc.gnu.org>\n+\t* config/sh/sh.h (IRA_COVER_CLASSES): Define.\n+\t* config/sh/sh.md (movsicc_true+3): Check if emit returns a\n+\tbarrier.\n+\n 2008-08-26  Victor Kaplansky  <victork@il.ibm.com>\n \t    Dorit Nuzman  <dorit@il.ibm.com>\n "}, {"sha": "8edcd949323400f2e3cb302c79297b273a0f5d67", "filename": "gcc/Makefile.in", "status": "modified", "additions": 43, "deletions": 4, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -849,6 +849,7 @@ TREE_DATA_REF_H = tree-data-ref.h $(LAMBDA_H) omega.h graphds.h tree-chrec.h\n VARRAY_H = varray.h $(MACHMODE_H) $(SYSTEM_H) coretypes.h $(TM_H)\n TREE_INLINE_H = tree-inline.h $(VARRAY_H) pointer-set.h\n REAL_H = real.h $(MACHMODE_H)\n+IRA_INT_H = ira.h ira-int.h $(CFGLOOP_H) alloc-pool.h\n DBGCNT_H = dbgcnt.h dbgcnt.def\n EBIMAP_H = ebitmap.h sbitmap.h\n IPA_PROP_H = ipa-prop.h $(TREE_H) vec.h $(CGRAPH_H)\n@@ -1097,6 +1098,13 @@ OBJS-common = \\\n \tinit-regs.o \\\n \tintegrate.o \\\n \tintl.o \\\n+\tira.o \\\n+\tira-build.o \\\n+\tira-costs.o \\\n+\tira-conflicts.o \\\n+\tira-color.o \\\n+\tira-emit.o \\\n+\tira-lives.o \\\n \tjump.o \\\n \tlambda-code.o \\\n \tlambda-mat.o \\\n@@ -2408,7 +2416,7 @@ toplev.o : toplev.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(TREE_H) \\\n    $(INSN_ATTR_H) output.h $(DIAGNOSTIC_H) debug.h insn-config.h intl.h \\\n    $(RECOG_H) Makefile $(TOPLEV_H) dwarf2out.h sdbout.h dbxout.h $(EXPR_H) \\\n    hard-reg-set.h $(BASIC_BLOCK_H) graph.h except.h $(REGS_H) $(TIMEVAR_H) \\\n-   value-prof.h $(PARAMS_H) $(TM_P_H) reload.h dwarf2asm.h $(TARGET_H) \\\n+   value-prof.h $(PARAMS_H) $(TM_P_H) reload.h ira.h dwarf2asm.h $(TARGET_H) \\\n    langhooks.h insn-flags.h $(CFGLAYOUT_H) $(CFGLOOP_H) hosthooks.h \\\n    $(CGRAPH_H) $(COVERAGE_H) alloc-pool.h $(GGC_H) $(INTEGRATE_H) \\\n    opts.h params.def tree-mudflap.h $(REAL_H) tree-pass.h $(GIMPLE_H)\n@@ -2771,7 +2779,7 @@ cfgloop.o : cfgloop.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) coretypes.h $(TM_H) \\\n    $(GGC_H)\n cfgloopanal.o : cfgloopanal.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) \\\n    $(BASIC_BLOCK_H) hard-reg-set.h $(CFGLOOP_H) $(EXPR_H) coretypes.h $(TM_H) \\\n-   $(OBSTACK_H) output.h graphds.h\n+   $(OBSTACK_H) output.h graphds.h $(PARAMS_H)\n graphds.o : graphds.c graphds.h $(CONFIG_H) $(SYSTEM_H) $(BITMAP_H) $(OBSTACK_H) \\\n    coretypes.h vec.h vecprim.h\n loop-iv.o : loop-iv.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(BASIC_BLOCK_H) \\\n@@ -2835,7 +2843,7 @@ reload1.o : reload1.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    $(EXPR_H) $(OPTABS_H) reload.h $(REGS_H) hard-reg-set.h insn-config.h \\\n    $(BASIC_BLOCK_H) $(RECOG_H) output.h $(FUNCTION_H) $(TOPLEV_H) $(TM_P_H) \\\n    addresses.h except.h $(TREE_H) $(REAL_H) $(FLAGS_H) $(MACHMODE_H) \\\n-   $(OBSTACK_H) $(DF_H) $(TARGET_H) dse.h\n+   $(OBSTACK_H) $(DF_H) $(TARGET_H) dse.h ira.h\n rtlhooks.o :  rtlhooks.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    rtlhooks-def.h $(EXPR_H) $(RECOG_H)\n postreload.o : postreload.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n@@ -2851,7 +2859,7 @@ postreload-gcse.o : postreload-gcse.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n caller-save.o : caller-save.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    $(FLAGS_H) $(REGS_H) hard-reg-set.h insn-config.h $(BASIC_BLOCK_H) $(FUNCTION_H) \\\n    addresses.h $(RECOG_H) reload.h $(EXPR_H) $(TOPLEV_H) $(TM_P_H) $(DF_H) \\\n-   gt-caller-save.h $(GGC_H)\n+   output.h ira.h gt-caller-save.h $(GGC_H)\n bt-load.o : bt-load.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) except.h \\\n    $(RTL_H) hard-reg-set.h $(REGS_H) $(TM_P_H) $(FIBHEAP_H) output.h $(EXPR_H) \\\n    $(TARGET_H) $(FLAGS_H) $(INSN_ATTR_H) $(FUNCTION_H) tree-pass.h $(TOPLEV_H) \\\n@@ -2872,6 +2880,37 @@ stack-ptr-mod.o : stack-ptr-mod.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n init-regs.o : init-regs.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n    $(TM_H) $(TREE_H) $(RTL_H) $(REGS_H) $(EXPR_H) tree-pass.h \\\n    $(BASIC_BLOCK_H) $(FLAGS_H) $(DF_H)\n+ira-build.o: ira-build.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(TARGET_H) $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) \\\n+   insn-config.h $(RECOG_H) $(BASIC_BLOCK_H) $(TOPLEV_H) $(TM_P_H) \\\n+   $(PARAMS_H) $(DF_H) sparseset.h $(IRA_INT_H)\n+ira-costs.o: ira-costs.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(TARGET_H) $(RTL_H) insn-config.h $(RECOG_H) \\\n+   $(REGS_H) hard-reg-set.h $(FLAGS_H) errors.h \\\n+   $(EXPR_H) $(BASIC_BLOCK_H) $(TM_P_H) \\\n+   $(IRA_INT_H)\n+ira-conflicts.o: ira-conflicts.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(TARGET_H) $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) \\\n+   insn-config.h $(RECOG_H) $(BASIC_BLOCK_H) $(TOPLEV_H) $(TM_P_H) $(PARAMS_H) \\\n+   $(DF_H) sparseset.h $(IRA_INT_H)\n+ira-color.o: ira-color.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(TARGET_H) $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) \\\n+   $(EXPR_H) $(BASIC_BLOCK_H) $(TOPLEV_H) $(TM_P_H) $(PARAMS_H) \\\n+   $(DF_H) $(SPLAY_TREE_H) $(IRA_INT_H)\n+ira-emit.o: ira-emit.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(TARGET_H) $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) \\\n+   $(EXPR_H) $(BASIC_BLOCK_H) $(TOPLEV_H) $(TM_P_H) $(PARAMS_H) \\\n+   $(IRA_INT_H)\n+ira-lives.o: ira-lives.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(TARGET_H) $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) \\\n+   insn-config.h $(RECOG_H) $(BASIC_BLOCK_H) $(TOPLEV_H) $(TM_P_H) $(PARAMS_H) \\\n+   $(DF_H) sparseset.h $(IRA_INT_H)\n+ira.o: ira.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n+   $(TARGET_H) $(TM_H) $(RTL_H) $(RECOG_H) \\\n+   $(REGS_H) hard-reg-set.h $(FLAGS_H) $(OBSTACK_H) \\\n+   $(EXPR_H) $(BASIC_BLOCK_H) $(TOPLEV_H) $(TM_P_H) \\\n+   $(DF_H) $(IRA_INT_H)  $(PARAMS_H) $(TIMEVAR_H) $(INTEGRATE_H) \\\n+   tree-pass.h output.h\n regmove.o : regmove.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    insn-config.h $(TIMEVAR_H) tree-pass.h $(DF_H)\\\n    $(RECOG_H) output.h $(REGS_H) hard-reg-set.h $(FLAGS_H) $(FUNCTION_H) \\"}, {"sha": "56660ec38d9bd5c61f8797bf856fdfa5f54e8412", "filename": "gcc/alias.c", "status": "modified", "additions": 46, "deletions": 0, "changes": 46, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Falias.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Falias.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Falias.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1975,6 +1975,34 @@ adjust_offset_for_component_ref (tree x, rtx offset)\n   return GEN_INT (ioffset);\n }\n \n+/* The function returns nonzero if X is an address containg VALUE.  */\n+static int\n+value_addr_p (rtx x)\n+{\n+  if (GET_CODE (x) == VALUE)\n+    return 1;\n+  if (GET_CODE (x) == PLUS && GET_CODE (XEXP (x, 0)) == VALUE)\n+    return 1;\n+  return 0;\n+}\n+\n+/* The function returns nonzero if X is a stack address.  */\n+static int\n+stack_addr_p (rtx x)\n+{\n+  if (x == hard_frame_pointer_rtx || x == frame_pointer_rtx\n+      || x == arg_pointer_rtx || x == stack_pointer_rtx)\n+    return 1;\n+  if (GET_CODE (x) == PLUS\n+      && (XEXP (x, 0) == hard_frame_pointer_rtx\n+\t  || XEXP (x, 0) == frame_pointer_rtx\n+\t  || XEXP (x, 0) == arg_pointer_rtx\n+\t  || XEXP (x, 0) == stack_pointer_rtx)\n+      && CONSTANT_P (XEXP (x, 1)))\n+    return 1;\n+  return 0;\n+}\n+\n /* Return nonzero if we can determine the exprs corresponding to memrefs\n    X and Y and they do not overlap.  */\n \n@@ -1984,9 +2012,27 @@ nonoverlapping_memrefs_p (const_rtx x, const_rtx y)\n   tree exprx = MEM_EXPR (x), expry = MEM_EXPR (y);\n   rtx rtlx, rtly;\n   rtx basex, basey;\n+  rtx x_addr, y_addr;\n   rtx moffsetx, moffsety;\n   HOST_WIDE_INT offsetx = 0, offsety = 0, sizex, sizey, tem;\n \n+  if (flag_ira && optimize && reload_completed)\n+    {\n+      /* We need this code for IRA because of stack slot sharing.  RTL\n+\t in decl can be different than RTL used in insns.  It is a\n+\t safe code although it can be conservative sometime.  */\n+      x_addr = canon_rtx (get_addr (XEXP (x, 0)));\n+      y_addr = canon_rtx (get_addr (XEXP (y, 0)));\n+      \n+      if (value_addr_p (x_addr) || value_addr_p (y_addr))\n+\treturn 0;\n+       \n+      if (stack_addr_p (x_addr) && stack_addr_p (y_addr)\n+\t  && memrefs_conflict_p (SIZE_FOR_MODE (y), y_addr,\n+\t\t\t\t SIZE_FOR_MODE (x), x_addr, 0))\n+\treturn 0;\n+    }\n+\n   /* Unless both have exprs, we can't tell anything.  */\n   if (exprx == 0 || expry == 0)\n     return 0;"}, {"sha": "ee8a0dc663122c867ba8a2374dc3f7b19c1dea80", "filename": "gcc/caller-save.c", "status": "modified", "additions": 418, "deletions": 55, "changes": 473, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fcaller-save.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fcaller-save.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcaller-save.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1,6 +1,6 @@\n /* Save and restore call-clobbered registers which are live across a call.\n    Copyright (C) 1989, 1992, 1994, 1995, 1997, 1998, 1999, 2000,\n-   2001, 2002, 2003, 2004, 2005, 2006, 2007 Free Software Foundation, Inc.\n+   2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008 Free Software Foundation, Inc.\n \n This file is part of GCC.\n \n@@ -35,9 +35,14 @@ along with GCC; see the file COPYING3.  If not see\n #include \"toplev.h\"\n #include \"tm_p.h\"\n #include \"addresses.h\"\n+#include \"output.h\"\n #include \"df.h\"\n #include \"ggc.h\"\n \n+/* Call used hard registers which can not be saved because there is no\n+   insn for this.  */\n+HARD_REG_SET no_caller_save_reg_set;\n+\n #ifndef MAX_MOVE_MAX\n #define MAX_MOVE_MAX MOVE_MAX\n #endif\n@@ -62,6 +67,12 @@ static enum machine_mode\n static rtx\n   regno_save_mem[FIRST_PSEUDO_REGISTER][MAX_MOVE_MAX / MIN_UNITS_PER_WORD + 1];\n \n+/* The number of elements in the subsequent array.  */\n+static int save_slots_num;\n+\n+/* Allocated slots so far.  */\n+static rtx save_slots[FIRST_PSEUDO_REGISTER];\n+\n /* We will only make a register eligible for caller-save if it can be\n    saved in its widest mode with a simple SET insn as long as the memory\n    address is valid.  We record the INSN_CODE is those insns here since\n@@ -86,7 +97,17 @@ static int n_regs_saved;\n static HARD_REG_SET referenced_regs;\n \n \n+static int reg_save_code (int, enum machine_mode);\n+static int reg_restore_code (int, enum machine_mode);\n+\n+struct saved_hard_reg;\n+static void initiate_saved_hard_regs (void);\n+static struct saved_hard_reg *new_saved_hard_reg (int, int);\n+static void finish_saved_hard_regs (void);\n+static int saved_hard_reg_compare_func (const void *, const void *);\n+\n static void mark_set_regs (rtx, const_rtx, void *);\n+static void add_stored_regs (rtx, const_rtx, void *);\n static void mark_referenced_regs (rtx);\n static int insert_save (struct insn_chain *, int, int, HARD_REG_SET *,\n \t\t\tenum machine_mode *);\n@@ -95,7 +116,9 @@ static int insert_restore (struct insn_chain *, int, int, int,\n static struct insn_chain *insert_one_insn (struct insn_chain *, int, int,\n \t\t\t\t\t   rtx);\n static void add_stored_regs (rtx, const_rtx, void *);\n+\n \f\n+\n static GTY(()) rtx savepat;\n static GTY(()) rtx restpat;\n static GTY(()) rtx test_reg;\n@@ -180,6 +203,7 @@ init_caller_save (void)\n   rtx address;\n   int i, j;\n \n+  CLEAR_HARD_REG_SET (no_caller_save_reg_set);\n   /* First find all the registers that we need to deal with and all\n      the modes that they can have.  If we can't find a mode to use,\n      we can't have the register live over calls.  */\n@@ -217,7 +241,7 @@ init_caller_save (void)\n   for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n     if (TEST_HARD_REG_BIT\n \t(reg_class_contents\n-\t [(int) base_reg_class (regno_save_mode [i][1], PLUS, CONST_INT)], i))\n+\t [(int) base_reg_class (regno_save_mode[i][1], PLUS, CONST_INT)], i))\n       break;\n \n   gcc_assert (i < FIRST_PSEUDO_REGISTER);\n@@ -264,10 +288,14 @@ init_caller_save (void)\n \t    {\n \t      call_fixed_regs[i] = 1;\n \t      SET_HARD_REG_BIT (call_fixed_reg_set, i);\n+\t      if (call_used_regs[i])\n+\t\tSET_HARD_REG_BIT (no_caller_save_reg_set, i);\n \t    }\n \t}\n }\n+\n \f\n+\n /* Initialize save areas by showing that we haven't allocated any yet.  */\n \n void\n@@ -278,6 +306,100 @@ init_save_areas (void)\n   for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n     for (j = 1; j <= MOVE_MAX_WORDS; j++)\n       regno_save_mem[i][j] = 0;\n+  save_slots_num = 0;\n+    \n+}\n+\n+/* The structure represents a hard register which should be saved\n+   through the call.  It is used when the integrated register\n+   allocator (IRA) is used and sharing save slots is on.  */\n+struct saved_hard_reg\n+{\n+  /* Order number starting with 0.  */\n+  int num;\n+  /* The hard regno.  */\n+  int hard_regno;\n+  /* Execution frequency of all calls through which given hard\n+     register should be saved.  */\n+  int call_freq;\n+  /* Stack slot reserved to save the hard register through calls.  */\n+  rtx slot;\n+  /* True if it is first hard register in the chain of hard registers\n+     sharing the same stack slot.  */\n+  int first_p;\n+  /* Order number of the next hard register structure with the same\n+     slot in the chain.  -1 represents end of the chain.  */\n+  int next;\n+};\n+\n+/* Map: hard register number to the corresponding structure.  */\n+static struct saved_hard_reg *hard_reg_map[FIRST_PSEUDO_REGISTER];\n+\n+/* The number of all structures representing hard registers should be\n+   saved, in order words, the number of used elements in the following\n+   array.  */\n+static int saved_regs_num;\n+\n+/* Pointers to all the structures.  Index is the order number of the\n+   corresponding structure.  */\n+static struct saved_hard_reg *all_saved_regs[FIRST_PSEUDO_REGISTER];\n+\n+/* First called function for work with saved hard registers.  */\n+static void\n+initiate_saved_hard_regs (void)\n+{\n+  int i;\n+\n+  saved_regs_num = 0;\n+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+    hard_reg_map[i] = NULL;\n+}\n+\n+/* Allocate and return new saved hard register with given REGNO and\n+   CALL_FREQ.  */\n+static struct saved_hard_reg *\n+new_saved_hard_reg (int regno, int call_freq)\n+{\n+  struct saved_hard_reg *saved_reg;\n+\n+  saved_reg\n+    = (struct saved_hard_reg *) xmalloc (sizeof (struct saved_hard_reg));\n+  hard_reg_map[regno] = all_saved_regs[saved_regs_num] = saved_reg;\n+  saved_reg->num = saved_regs_num++;\n+  saved_reg->hard_regno = regno;\n+  saved_reg->call_freq = call_freq;\n+  saved_reg->first_p = FALSE;\n+  saved_reg->next = -1;\n+  return saved_reg;\n+}\n+\n+/* Free memory allocated for the saved hard registers.  */\n+static void\n+finish_saved_hard_regs (void)\n+{\n+  int i;\n+\n+  for (i = 0; i < saved_regs_num; i++)\n+    free (all_saved_regs[i]);\n+}\n+\n+/* The function is used to sort the saved hard register structures\n+   according their frequency.  */\n+static int\n+saved_hard_reg_compare_func (const void *v1p, const void *v2p)\n+{\n+  const struct saved_hard_reg *p1 = *(struct saved_hard_reg * const *) v1p;\n+  const struct saved_hard_reg *p2 = *(struct saved_hard_reg * const *) v2p;\n+  \n+  if (flag_omit_frame_pointer)\n+    {\n+      if (p1->call_freq - p2->call_freq != 0)\n+\treturn p1->call_freq - p2->call_freq;\n+    }\n+  else if (p2->call_freq - p1->call_freq != 0)\n+    return p2->call_freq - p1->call_freq;\n+\n+  return p1->num - p2->num;\n }\n \n /* Allocate save areas for any hard registers that might need saving.\n@@ -286,6 +408,10 @@ init_save_areas (void)\n    overestimate slightly (especially if some of these registers are later\n    used as spill registers), but it should not be significant.\n \n+   For IRA we use priority coloring to decrease stack slots needed for\n+   saving hard registers through calls.  We build conflicts for them\n+   to do coloring.\n+\n    Future work:\n \n      In the fallback case we should iterate backwards across all possible\n@@ -317,65 +443,297 @@ setup_save_areas (void)\n \tunsigned int regno = reg_renumber[i];\n \tunsigned int endregno\n \t  = end_hard_regno (GET_MODE (regno_reg_rtx[i]), regno);\n-\n \tfor (r = regno; r < endregno; r++)\n \t  if (call_used_regs[r])\n \t    SET_HARD_REG_BIT (hard_regs_used, r);\n       }\n \n-  /* Now run through all the call-used hard-registers and allocate\n-     space for them in the caller-save area.  Try to allocate space\n-     in a manner which allows multi-register saves/restores to be done.  */\n-\n-  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n-    for (j = MOVE_MAX_WORDS; j > 0; j--)\n-      {\n-\tint do_save = 1;\n-\n-\t/* If no mode exists for this size, try another.  Also break out\n-\t   if we have already saved this hard register.  */\n-\tif (regno_save_mode[i][j] == VOIDmode || regno_save_mem[i][1] != 0)\n-\t  continue;\n-\n-\t/* See if any register in this group has been saved.  */\n-\tfor (k = 0; k < j; k++)\n-\t  if (regno_save_mem[i + k][1])\n+  if (flag_ira && optimize && flag_ira_share_save_slots)\n+    {\n+      rtx insn, slot;\n+      struct insn_chain *chain, *next;\n+      char *saved_reg_conflicts;\n+      unsigned int regno;\n+      int next_k, freq;\n+      struct saved_hard_reg *saved_reg, *saved_reg2, *saved_reg3;\n+      int call_saved_regs_num;\n+      struct saved_hard_reg *call_saved_regs[FIRST_PSEUDO_REGISTER];\n+      HARD_REG_SET hard_regs_to_save, used_regs, this_insn_sets;\n+      reg_set_iterator rsi;\n+      int best_slot_num;\n+      int prev_save_slots_num;\n+      rtx prev_save_slots[FIRST_PSEUDO_REGISTER];\n+      \n+      initiate_saved_hard_regs ();\n+      /* Create hard reg saved regs.  */\n+      for (chain = reload_insn_chain; chain != 0; chain = next)\n+\t{\n+\t  insn = chain->insn;\n+\t  next = chain->next;\n+\t  if (GET_CODE (insn) != CALL_INSN\n+\t      || find_reg_note (insn, REG_NORETURN, NULL))\n+\t    continue;\n+\t  freq = REG_FREQ_FROM_BB (BLOCK_FOR_INSN (insn));\n+\t  REG_SET_TO_HARD_REG_SET (hard_regs_to_save,\n+\t\t\t\t   &chain->live_throughout);\n+\t  COPY_HARD_REG_SET (used_regs, call_used_reg_set);\n+\n+\t  /* Record all registers set in this call insn.  These don't\n+\t     need to be saved.  N.B. the call insn might set a subreg\n+\t     of a multi-hard-reg pseudo; then the pseudo is considered\n+\t     live during the call, but the subreg that is set\n+\t     isn't.  */\n+\t  CLEAR_HARD_REG_SET (this_insn_sets);\n+\t  note_stores (PATTERN (insn), mark_set_regs, &this_insn_sets);\n+\t  /* Sibcalls are considered to set the return value.  */\n+\t  if (SIBLING_CALL_P (insn) && crtl->return_rtx)\n+\t    mark_set_regs (crtl->return_rtx, NULL_RTX, &this_insn_sets);\n+\n+\t  AND_COMPL_HARD_REG_SET (used_regs, call_fixed_reg_set);\n+\t  AND_COMPL_HARD_REG_SET (used_regs, this_insn_sets);\n+\t  AND_HARD_REG_SET (hard_regs_to_save, used_regs);\n+\t  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n+\t    if (TEST_HARD_REG_BIT (hard_regs_to_save, regno))\n+\t      {\n+\t\tif (hard_reg_map[regno] != NULL)\n+\t\t  hard_reg_map[regno]->call_freq += freq;\n+\t\telse\n+\t\t  saved_reg = new_saved_hard_reg (regno, freq);\n+\t      }\n+\t  /* Look through all live pseudos, mark their hard registers.  */\n+\t  EXECUTE_IF_SET_IN_REG_SET\n+\t    (&chain->live_throughout, FIRST_PSEUDO_REGISTER, regno, rsi)\n \t    {\n-\t      do_save = 0;\n-\t      break;\n+\t      int r = reg_renumber[regno];\n+\t      int bound;\n+\t      \n+\t      if (r < 0)\n+\t\tcontinue;\n+\t      \n+\t      bound = r + hard_regno_nregs[r][PSEUDO_REGNO_MODE (regno)];\n+\t      for (; r < bound; r++)\n+\t\tif (TEST_HARD_REG_BIT (used_regs, r))\n+\t\t  {\n+\t\t    if (hard_reg_map[r] != NULL)\n+\t\t      hard_reg_map[r]->call_freq += freq;\n+\t\t    else\n+\t\t      saved_reg = new_saved_hard_reg (r, freq);\n+\t\t    SET_HARD_REG_BIT (hard_regs_to_save, r);\n+\t\t  }\n \t    }\n-\tif (! do_save)\n-\t  continue;\n+\t}\n+      /* Find saved hard register conflicts.  */\n+      saved_reg_conflicts = (char *) xmalloc (saved_regs_num * saved_regs_num);\n+      memset (saved_reg_conflicts, 0, saved_regs_num * saved_regs_num);\n+      for (chain = reload_insn_chain; chain != 0; chain = next)\n+\t{\n+\t  call_saved_regs_num = 0;\n+\t  insn = chain->insn;\n+\t  next = chain->next;\n+\t  if (GET_CODE (insn) != CALL_INSN\n+\t      || find_reg_note (insn, REG_NORETURN, NULL))\n+\t    continue;\n+\t  REG_SET_TO_HARD_REG_SET (hard_regs_to_save,\n+\t\t\t\t   &chain->live_throughout);\n+\t  COPY_HARD_REG_SET (used_regs, call_used_reg_set);\n+\n+\t  /* Record all registers set in this call insn.  These don't\n+\t     need to be saved.  N.B. the call insn might set a subreg\n+\t     of a multi-hard-reg pseudo; then the pseudo is considered\n+\t     live during the call, but the subreg that is set\n+\t     isn't.  */\n+\t  CLEAR_HARD_REG_SET (this_insn_sets);\n+\t  note_stores (PATTERN (insn), mark_set_regs, &this_insn_sets);\n+\t  /* Sibcalls are considered to set the return value,\n+\t     compare flow.c:propagate_one_insn.  */\n+\t  if (SIBLING_CALL_P (insn) && crtl->return_rtx)\n+\t    mark_set_regs (crtl->return_rtx, NULL_RTX, &this_insn_sets);\n+\n+\t  AND_COMPL_HARD_REG_SET (used_regs, call_fixed_reg_set);\n+\t  AND_COMPL_HARD_REG_SET (used_regs, this_insn_sets);\n+\t  AND_HARD_REG_SET (hard_regs_to_save, used_regs);\n+\t  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n+\t    if (TEST_HARD_REG_BIT (hard_regs_to_save, regno))\n+\t      {\n+\t\tgcc_assert (hard_reg_map[regno] != NULL);\n+\t\tcall_saved_regs[call_saved_regs_num++] = hard_reg_map[regno];\n+\t      }\n+\t  /* Look through all live pseudos, mark their hard registers.  */\n+\t  EXECUTE_IF_SET_IN_REG_SET\n+\t    (&chain->live_throughout, FIRST_PSEUDO_REGISTER, regno, rsi)\n+\t    {\n+\t      int r = reg_renumber[regno];\n+\t      int bound;\n+\t      \n+\t      if (r < 0)\n+\t\tcontinue;\n \n-\tfor (k = 0; k < j; k++)\n-\t  if (! TEST_HARD_REG_BIT (hard_regs_used, i + k))\n+\t      bound = r + hard_regno_nregs[r][PSEUDO_REGNO_MODE (regno)];\n+\t      for (; r < bound; r++)\n+\t\tif (TEST_HARD_REG_BIT (used_regs, r))\n+\t\t  call_saved_regs[call_saved_regs_num++] = hard_reg_map[r];\n+\t    }\n+\t  for (i = 0; i < call_saved_regs_num; i++)\n \t    {\n-\t      do_save = 0;\n-\t      break;\n+\t      saved_reg = call_saved_regs[i];\n+\t      for (j = 0; j < call_saved_regs_num; j++)\n+\t\tif (i != j)\n+\t\t  {\n+\t\t    saved_reg2 = call_saved_regs[j];\n+\t\t    saved_reg_conflicts[saved_reg->num * saved_regs_num\n+\t\t\t\t\t+ saved_reg2->num]\n+\t\t      = saved_reg_conflicts[saved_reg2->num * saved_regs_num\n+\t\t\t\t\t    + saved_reg->num]\n+\t\t      = TRUE;\n+\t\t  }\n \t    }\n-\tif (! do_save)\n-\t  continue;\n-\n-\t/* We have found an acceptable mode to store in.  Since hard\n-\t   register is always saved in the widest mode available,\n-\t   the mode may be wider than necessary, it is OK to reduce\n-\t   the alignment of spill space.  We will verify that it is\n-\t   equal to or greater than required when we restore and save\n-\t   the hard register in insert_restore and insert_save.  */\n-\tregno_save_mem[i][j]\n-\t  = assign_stack_local_1 (regno_save_mode[i][j],\n-\t\t\t\tGET_MODE_SIZE (regno_save_mode[i][j]),\n-\t\t\t\t0, true);\n-\n-\t/* Setup single word save area just in case...  */\n-\tfor (k = 0; k < j; k++)\n-\t  /* This should not depend on WORDS_BIG_ENDIAN.\n-\t     The order of words in regs is the same as in memory.  */\n-\t  regno_save_mem[i + k][1]\n-\t    = adjust_address_nv (regno_save_mem[i][j],\n-\t\t\t\t regno_save_mode[i + k][1],\n-\t\t\t\t k * UNITS_PER_WORD);\n-      }\n+\t}\n+      /* Sort saved hard regs.  */\n+      qsort (all_saved_regs, saved_regs_num, sizeof (struct saved_hard_reg *),\n+\t     saved_hard_reg_compare_func);\n+      /* Initiate slots available from the previous reload\n+\t iteration.  */\n+      prev_save_slots_num = save_slots_num;\n+      memcpy (prev_save_slots, save_slots, save_slots_num * sizeof (rtx));\n+      save_slots_num = 0;\n+      /* Allocate stack slots for the saved hard registers.  */\n+      for (i = 0; i < saved_regs_num; i++)\n+\t{\n+\t  saved_reg = all_saved_regs[i];\n+\t  regno = saved_reg->hard_regno;\n+\t  for (j = 0; j < i; j++)\n+\t    {\n+\t      saved_reg2 = all_saved_regs[j];\n+\t      if (! saved_reg2->first_p)\n+\t\tcontinue;\n+\t      slot = saved_reg2->slot;\n+\t      for (k = j; k >= 0; k = next_k)\n+\t\t{\n+\t\t  saved_reg3 = all_saved_regs[k];\n+\t\t  next_k = saved_reg3->next;\n+\t\t  if (saved_reg_conflicts[saved_reg->num * saved_regs_num\n+\t\t\t\t\t  + saved_reg3->num])\n+\t\t    break;\n+\t\t}\n+\t      if (k < 0\n+\t\t  && (GET_MODE_SIZE (regno_save_mode[regno][1])\n+\t\t      <= GET_MODE_SIZE (regno_save_mode\n+\t\t\t\t\t[saved_reg2->hard_regno][1])))\n+\t\t{\n+\t\t  saved_reg->slot\n+\t\t    = adjust_address_nv\n+\t\t      (slot, regno_save_mode[saved_reg->hard_regno][1], 0);\n+\t\t  regno_save_mem[regno][1] = saved_reg->slot;\n+\t\t  saved_reg->next = saved_reg2->next;\n+\t\t  saved_reg2->next = i;\n+\t\t  if (dump_file != NULL)\n+\t\t    fprintf (dump_file, \"%d uses slot of %d\\n\",\n+\t\t\t     regno, saved_reg2->hard_regno);\n+\t\t  break;\n+\t\t}\n+\t    }\n+\t  if (j == i)\n+\t    {\n+\t      saved_reg->first_p = TRUE;\n+\t      for (best_slot_num = -1, j = 0; j < prev_save_slots_num; j++)\n+\t\t{\n+\t\t  slot = prev_save_slots[j];\n+\t\t  if (slot == NULL_RTX)\n+\t\t    continue;\n+\t\t  if (GET_MODE_SIZE (regno_save_mode[regno][1])\n+\t\t      <= GET_MODE_SIZE (GET_MODE (slot))\n+\t\t      && best_slot_num < 0)\n+\t\t    best_slot_num = j;\n+\t\t  if (GET_MODE (slot) == regno_save_mode[regno][1])\n+\t\t    break;\n+\t\t}\n+\t      if (best_slot_num >= 0)\n+\t\t{\n+\t\t  saved_reg->slot = prev_save_slots[best_slot_num];\n+\t\t  saved_reg->slot\n+\t\t    = adjust_address_nv\n+\t\t      (saved_reg->slot,\n+\t\t       regno_save_mode[saved_reg->hard_regno][1], 0);\n+\t\t  if (dump_file != NULL)\n+\t\t    fprintf (dump_file,\n+\t\t\t     \"%d uses a slot from prev iteration\\n\", regno);\n+\t\t  prev_save_slots[best_slot_num] = NULL_RTX;\n+\t\t  if (best_slot_num + 1 == prev_save_slots_num)\n+\t\t    prev_save_slots_num--;\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  saved_reg->slot\n+\t\t    = assign_stack_local_1\n+\t\t      (regno_save_mode[regno][1],\n+\t\t       GET_MODE_SIZE (regno_save_mode[regno][1]), 0, true);\n+\t\t  if (dump_file != NULL)\n+\t\t    fprintf (dump_file, \"%d uses a new slot\\n\", regno);\n+\t\t}\n+\t      regno_save_mem[regno][1] = saved_reg->slot;\n+\t      save_slots[save_slots_num++] = saved_reg->slot;\n+\t    }\n+\t}\n+      free (saved_reg_conflicts);\n+      finish_saved_hard_regs ();\n+    }\n+  else\n+    {\n+      /* Now run through all the call-used hard-registers and allocate\n+\t space for them in the caller-save area.  Try to allocate space\n+\t in a manner which allows multi-register saves/restores to be done.  */\n+      \n+      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+\tfor (j = MOVE_MAX_WORDS; j > 0; j--)\n+\t  {\n+\t    int do_save = 1;\n+\t    \n+\t    /* If no mode exists for this size, try another.  Also break out\n+\t       if we have already saved this hard register.  */\n+\t    if (regno_save_mode[i][j] == VOIDmode || regno_save_mem[i][1] != 0)\n+\t      continue;\n+\t    \n+\t    /* See if any register in this group has been saved.  */\n+\t    for (k = 0; k < j; k++)\n+\t      if (regno_save_mem[i + k][1])\n+\t\t{\n+\t\t  do_save = 0;\n+\t\t  break;\n+\t\t}\n+\t    if (! do_save)\n+\t      continue;\n+\t    \n+\t    for (k = 0; k < j; k++)\n+\t      if (! TEST_HARD_REG_BIT (hard_regs_used, i + k))\n+\t\t{\n+\t\t  do_save = 0;\n+\t\t  break;\n+\t\t}\n+\t    if (! do_save)\n+\t      continue;\n+\t    \n+\t    /* We have found an acceptable mode to store in.  Since\n+\t       hard register is always saved in the widest mode\n+\t       available, the mode may be wider than necessary, it is\n+\t       OK to reduce the alignment of spill space.  We will\n+\t       verify that it is equal to or greater than required\n+\t       when we restore and save the hard register in\n+\t       insert_restore and insert_save.  */\n+\t    regno_save_mem[i][j]\n+\t      = assign_stack_local_1 (regno_save_mode[i][j],\n+\t\t\t\t      GET_MODE_SIZE (regno_save_mode[i][j]),\n+\t\t\t\t      0, true);\n+\t    \n+\t    /* Setup single word save area just in case...  */\n+\t    for (k = 0; k < j; k++)\n+\t      /* This should not depend on WORDS_BIG_ENDIAN.\n+\t\t The order of words in regs is the same as in memory.  */\n+\t      regno_save_mem[i + k][1]\n+\t\t= adjust_address_nv (regno_save_mem[i][j],\n+\t\t\t\t     regno_save_mode[i + k][1],\n+\t\t\t\t     k * UNITS_PER_WORD);\n+\t  }\n+    }\n \n   /* Now loop again and set the alias set of any save areas we made to\n      the alias set used to represent frame objects.  */\n@@ -384,7 +742,9 @@ setup_save_areas (void)\n       if (regno_save_mem[i][j] != 0)\n \tset_mem_alias_set (regno_save_mem[i][j], get_frame_alias_set ());\n }\n+\n \f\n+\n /* Find the places where hard regs are live across calls and save them.  */\n \n void\n@@ -461,7 +821,8 @@ save_call_clobbered_regs (void)\n \t\t  int nregs;\n \t\t  enum machine_mode mode;\n \n-\t\t  gcc_assert (r >= 0);\n+\t\t  if (r < 0)\n+\t\t    continue;\n \t\t  nregs = hard_regno_nregs[r][PSEUDO_REGNO_MODE (regno)];\n \t\t  mode = HARD_REGNO_CALLER_SAVE_MODE\n \t\t    (r, nregs, PSEUDO_REGNO_MODE (regno));\n@@ -497,7 +858,7 @@ save_call_clobbered_regs (void)\n \t    }\n \t}\n \n-      if (chain->next == 0 || chain->next->block > chain->block)\n+      if (chain->next == 0 || chain->next->block != chain->block)\n \t{\n \t  int regno;\n \t  /* At the end of the basic block, we must restore any registers that\n@@ -713,7 +1074,8 @@ insert_restore (struct insn_chain *chain, int before_p, int regno,\n \n   /* Verify that the alignment of spill space is equal to or greater\n      than required.  */\n-  gcc_assert (GET_MODE_ALIGNMENT (GET_MODE (mem)) <= MEM_ALIGN (mem));\n+  gcc_assert (MIN (MAX_SUPPORTED_STACK_ALIGNMENT,\n+\t\t   GET_MODE_ALIGNMENT (GET_MODE (mem))) <= MEM_ALIGN (mem));\n \n   pat = gen_rtx_SET (VOIDmode,\n \t\t     gen_rtx_REG (GET_MODE (mem),\n@@ -790,7 +1152,8 @@ insert_save (struct insn_chain *chain, int before_p, int regno,\n \n   /* Verify that the alignment of spill space is equal to or greater\n      than required.  */\n-  gcc_assert (GET_MODE_ALIGNMENT (GET_MODE (mem)) <= MEM_ALIGN (mem));\n+  gcc_assert (MIN (MAX_SUPPORTED_STACK_ALIGNMENT,\n+\t\t   GET_MODE_ALIGNMENT (GET_MODE (mem))) <= MEM_ALIGN (mem));\n \n   pat = gen_rtx_SET (VOIDmode, mem,\n \t\t     gen_rtx_REG (GET_MODE (mem),"}, {"sha": "db5bd2a62b96993470afdfd567e7a9adecdedbf2", "filename": "gcc/cfgloopanal.c", "status": "modified", "additions": 20, "deletions": 5, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fcfgloopanal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fcfgloopanal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfgloopanal.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -29,6 +29,7 @@ along with GCC; see the file COPYING3.  If not see\n #include \"expr.h\"\n #include \"output.h\"\n #include \"graphds.h\"\n+#include \"params.h\"\n \n /* Checks whether BB is executed exactly once in each LOOP iteration.  */\n \n@@ -372,19 +373,33 @@ init_set_costs (void)\n unsigned\n estimate_reg_pressure_cost (unsigned n_new, unsigned n_old)\n {\n+  unsigned cost;\n   unsigned regs_needed = n_new + n_old;\n \n   /* If we have enough registers, we should use them and not restrict\n      the transformations unnecessarily.  */\n   if (regs_needed + target_res_regs <= target_avail_regs)\n     return 0;\n \n-  /* If we are close to running out of registers, try to preserve them.  */\n   if (regs_needed <= target_avail_regs)\n-    return target_reg_cost * n_new;\n-  \n-  /* If we run out of registers, it is very expensive to add another one.  */\n-  return target_spill_cost * n_new;\n+    /* If we are close to running out of registers, try to preserve\n+       them.  */\n+    cost = target_reg_cost * n_new;\n+  else\n+    /* If we run out of registers, it is very expensive to add another\n+       one.  */\n+    cost = target_spill_cost * n_new;\n+\n+  if (optimize && flag_ira && (flag_ira_algorithm == IRA_ALGORITHM_REGIONAL\n+\t\t\t       || flag_ira_algorithm == IRA_ALGORITHM_MIXED)\n+      && number_of_loops () <= (unsigned) IRA_MAX_LOOPS_NUM)\n+    /* IRA regional allocation deals with high register pressure\n+       better.  So decrease the cost (to do more accurate the cost\n+       calculation for IRA, we need to know how many registers lives\n+       through the loop transparently).  */\n+    cost /= 2;\n+\n+  return cost;\n }\n \n /* Sets EDGE_LOOP_EXIT flag for all loop exits.  */"}, {"sha": "21d1ae3396c00975eaf5887e59276f172dbb0096", "filename": "gcc/common.opt", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fcommon.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fcommon.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcommon.opt?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -653,6 +653,30 @@ Common Report Var(flag_ipa_struct_reorg)\n Perform structure layout optimizations based\n on profiling information.\n \n+fira\n+Common Report Var(flag_ira) Init(0)\n+Use integrated register allocator.\n+\n+fira-algorithm=\n+Common Joined RejectNegative\n+-fira-algorithm=[regional|CB|mixed] Set the used IRA algorithm\n+\n+fira-coalesce\n+Common Report Var(flag_ira_coalesce) Init(0)\n+Do optimistic coalescing.\n+\n+fira-share-save-slots\n+Common Report Var(flag_ira_share_save_slots) Init(1)\n+Share slots for saving different hard registers.\n+\n+fira-share-spill-slots\n+Common Report Var(flag_ira_share_spill_slots) Init(1)\n+Share stack slots for spilled pseudo-registers.\n+\n+fira-verbose=\n+Common RejectNegative Joined UInteger\n+-fira-verbose=<number> Control IRA's level of diagnostic messages.\n+\n fivopts\n Common Report Var(flag_ivopts) Init(1) Optimization\n Optimize induction variables on trees"}, {"sha": "4336e6c9357153253901743763e2b1487c1712cf", "filename": "gcc/config/alpha/alpha.h", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Falpha%2Falpha.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Falpha%2Falpha.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Falpha%2Falpha.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -553,6 +553,19 @@ enum reg_class {\n   {0x00000000, 0x7fffffff},\t/* FLOAT_REGS */\t\\\n   {0xffffffff, 0xffffffff} }\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES\t\t\t\t\t\t     \\\n+{\t\t\t\t\t\t\t\t\t     \\\n+  GENERAL_REGS, FLOAT_REGS, LIM_REG_CLASSES\t\t\t\t     \\\n+}\n+\n /* The same information, inverted:\n    Return the class number of the smallest class containing\n    reg number REGNO.  This could be a conditional expression"}, {"sha": "fd5067adfd89b22b5cbda44c439ae4f0c6c76832", "filename": "gcc/config/arm/arm.h", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Farm%2Farm.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Farm%2Farm.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1185,6 +1185,20 @@ enum reg_class\n    or could index an array.  */\n #define REGNO_REG_CLASS(REGNO)  arm_regno_class (REGNO)\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES\t\t\t\t\t\t     \\\n+{\t\t\t\t\t\t\t\t\t     \\\n+  GENERAL_REGS, FPA_REGS, CIRRUS_REGS, VFP_REGS, IWMMXT_GR_REGS, IWMMXT_REGS,\\\n+  LIM_REG_CLASSES\t\t\t\t\t\t\t     \\\n+}\n+\n /* FPA registers can't do subreg as all values are reformatted to internal\n    precision.  VFP registers may only be accessed in the mode they\n    were set.  */"}, {"sha": "b5132e263136c3048ff483204c47a0be237d09cf", "filename": "gcc/config/avr/avr.h", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Favr%2Favr.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Favr%2Favr.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Favr.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -291,6 +291,19 @@ enum reg_class {\n \n #define REGNO_REG_CLASS(R) avr_regno_reg_class(R)\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES               \\\n+{                                       \\\n+  GENERAL_REGS, LIM_REG_CLASSES         \\\n+}\n+\n #define BASE_REG_CLASS (reload_completed ? BASE_POINTER_REGS : POINTER_REGS)\n \n #define INDEX_REG_CLASS NO_REGS"}, {"sha": "826e60b9e021c38035f16ac650a3ca4cc30b0b26", "filename": "gcc/config/bfin/bfin.h", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fbfin%2Fbfin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fbfin%2Fbfin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fbfin%2Fbfin.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -711,6 +711,19 @@ enum reg_class\n  : (REGNO) >= REG_RETS ? PROLOGUE_REGS\t\t\t\\\n  : NO_REGS)\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES\t\t\t\t\\\n+{\t\t\t\t\t\t\t\\\n+    MOST_REGS, AREGS, CCREGS, LIM_REG_CLASSES\t\t\\\n+}\n+\n /* When defined, the compiler allows registers explicitly used in the\n    rtl to be used as spill registers but prevents the compiler from\n    extending the lifetime of these registers. */"}, {"sha": "69c7472909f062967e947213b10b153cd56530e4", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1274,6 +1274,19 @@ enum reg_class\n { 0xffffffff,0x1fffff }\t\t\t\t\t\t\t\\\n }\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES\t\t\t\t\t\t     \\\n+{\t\t\t\t\t\t\t\t\t     \\\n+  GENERAL_REGS, FLOAT_REGS, MMX_REGS, SSE_REGS, LIM_REG_CLASSES\t\t     \\\n+}\n+\n /* The same information, inverted:\n    Return the class number of the smallest class containing\n    reg number REGNO.  This could be a conditional expression"}, {"sha": "6fca690240821dc3d4b3daf0f5f77df883b70361", "filename": "gcc/config/ia64/ia64.h", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fia64%2Fia64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fia64%2Fia64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fia64%2Fia64.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -800,6 +800,19 @@ enum reg_class\n     0xFFFFFFFF, 0xFFFFFFFF, 0x3FFF },\t\t\t\\\n }\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES\t\t\t\t\t\t     \\\n+{\t\t\t\t\t\t\t\t\t     \\\n+  PR_REGS, BR_REGS, AR_M_REGS, AR_I_REGS, GR_REGS, FR_REGS, LIM_REG_CLASSES  \\\n+}\n+\n /* A C expression whose value is a register class containing hard register\n    REGNO.  In general there is more than one such class; choose a class which\n    is \"minimal\", meaning that no smaller class also contains the register.  */"}, {"sha": "07035fbb06bb36afc9cf1accd4ead3bd2d2b2727", "filename": "gcc/config/mn10300/mn10300.h", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fmn10300%2Fmn10300.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fmn10300%2Fmn10300.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fmn10300%2Fmn10300.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -295,6 +295,19 @@ enum reg_class {\n  { 0xffffffff, 0x3ffff } /* ALL_REGS \t*/\t\\\n }\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES                                                    \\\n+{                                                                            \\\n+  GENERAL_REGS, FP_REGS, LIM_REG_CLASSES \\\n+}\n+\n /* The same information, inverted:\n    Return the class number of the smallest class containing\n    reg number REGNO.  This could be a conditional expression"}, {"sha": "048d163ff14c4e994fd24650ca0a3fa728cd8981", "filename": "gcc/config/rs6000/rs6000.h", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Frs6000%2Frs6000.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Frs6000%2Frs6000.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1128,6 +1128,22 @@ enum reg_class\n   { 0xffffffff, 0xffffffff, 0xffffffff, 0x0003ffff }  /* ALL_REGS */\t     \\\n }\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES\t\t\t\t\t\t     \\\n+{\t\t\t\t\t\t\t\t\t     \\\n+  GENERAL_REGS, SPECIAL_REGS, FLOAT_REGS, ALTIVEC_REGS,\t\t\t     \\\n+  /*VRSAVE_REGS,*/ VSCR_REGS, SPE_ACC_REGS, SPEFSCR_REGS,\t\t     \\\n+  /* MQ_REGS, LINK_REGS, CTR_REGS, */\t\t\t\t\t     \\\n+  CR_REGS, XER_REGS, LIM_REG_CLASSES\t\t\t\t\t     \\\n+}\n+\n /* The same information, inverted:\n    Return the class number of the smallest class containing\n    reg number REGNO.  This could be a conditional expression"}, {"sha": "b96f10026e75ae5c10d8a99f707076239c4c7dd4", "filename": "gcc/config/s390/s390.h", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fs390%2Fs390.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fs390%2Fs390.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fs390.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -478,6 +478,30 @@ enum reg_class\n   { 0xffffffff, 0x0000003f },\t/* ALL_REGS */\t\t\\\n }\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES\t\t\t\t\t\t     \\\n+{\t\t\t\t\t\t\t\t\t     \\\n+  GENERAL_REGS, FP_REGS, CC_REGS, ACCESS_REGS, LIM_REG_CLASSES\t\t     \\\n+}\n+\n+/* In some case register allocation order is not enough for IRA to\n+   generate a good code.  The following macro (if defined) increases\n+   cost of REGNO for a pseudo approximately by pseudo usage frequency\n+   multiplied by the macro value.\n+\n+   We avoid usage of BASE_REGNUM by nonzero macro value because the\n+   reload can decide not to use the hard register because some\n+   constant was forced to be in memory.  */\n+#define IRA_HARD_REGNO_ADD_COST_MULTIPLIER(regno)\t\\\n+  (regno == BASE_REGNUM ? 0.0 : 0.5)\n+\n /* Register -> class mapping.  */\n extern const enum reg_class regclass_map[FIRST_PSEUDO_REGISTER];\n #define REGNO_REG_CLASS(REGNO) (regclass_map[REGNO])"}, {"sha": "8af2f436169967cf4259025f9c55e638f5fe72a0", "filename": "gcc/config/sh/sh.h", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fsh%2Fsh.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fsh%2Fsh.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Fsh.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1499,6 +1499,20 @@ enum reg_class\n extern enum reg_class regno_reg_class[FIRST_PSEUDO_REGISTER];\n #define REGNO_REG_CLASS(REGNO) regno_reg_class[(REGNO)]\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES\t\t\t\t\t\t     \\\n+{\t\t\t\t\t\t\t\t\t     \\\n+  GENERAL_REGS, FP_REGS, PR_REGS, T_REGS, MAC_REGS, TARGET_REGS,  \t     \\\n+  LIM_REG_CLASSES\t\t\t\t\t\t\t     \\\n+}\n+\n /* When defined, the compiler allows registers explicitly used in the\n    rtl to be used as spill registers but prevents the compiler from\n    extending the lifetime of these registers.  */"}, {"sha": "b6e56aa5a5ec733881a0bbb011b648e69363b19e", "filename": "gcc/config/sh/sh.md", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fsh%2Fsh.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fsh%2Fsh.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Fsh.md?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1143,7 +1143,7 @@\n    (set (match_dup 4) (match_dup 5))]\n   \"\n {\n-  rtx set1, set2;\n+  rtx set1, set2, insn2;\n   rtx replacements[4];\n \n   /* We want to replace occurrences of operands[0] with operands[1] and\n@@ -1173,7 +1173,10 @@\n   extract_insn (emit_insn (set1));\n   if (! constrain_operands (1))\n     goto failure;\n-  extract_insn (emit (set2));\n+  insn2 = emit (set2);\n+  if (GET_CODE (insn2) == BARRIER)\n+    goto failure;\n+  extract_insn (insn2);\n   if (! constrain_operands (1))\n     {\n       rtx tmp;"}, {"sha": "42894705361587b076351d5f4e1c3efe32ef8fb5", "filename": "gcc/config/sparc/sparc.h", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fsparc%2Fsparc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fsparc%2Fsparc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsparc%2Fsparc.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1078,6 +1078,19 @@ enum reg_class { NO_REGS, FPCC_REGS, I64_REGS, GENERAL_REGS, FP_REGS,\n    {-1, -1, -1, 0x20},\t/* GENERAL_OR_EXTRA_FP_REGS */\t\\\n    {-1, -1, -1, 0x3f}}\t/* ALL_REGS */\n \n+/* The following macro defines cover classes for Integrated Register\n+   Allocator.  Cover classes is a set of non-intersected register\n+   classes covering all hard registers used for register allocation\n+   purpose.  Any move between two registers of a cover class should be\n+   cheaper than load or store of the registers.  The macro value is\n+   array of register classes with LIM_REG_CLASSES used as the end\n+   marker.  */\n+\n+#define IRA_COVER_CLASSES\t\t\t\t\t\t     \\\n+{\t\t\t\t\t\t\t\t\t     \\\n+  GENERAL_REGS, EXTRA_FP_REGS, FPCC_REGS, LIM_REG_CLASSES\t\t     \\\n+}\n+\n /* Defines invalid mode changes.  Borrowed from pa64-regs.h.\n \n    SImode loads to floating-point registers are not zero-extended."}, {"sha": "86042aacb2fadc07e4ac540901d5be9045a49a7a", "filename": "gcc/config/spu/spu.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fspu%2Fspu.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fconfig%2Fspu%2Fspu.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -196,6 +196,9 @@ enum reg_class {\n    LIM_REG_CLASSES \n };\n \n+/* SPU is simple, it really only has one class of registers.  */\n+#define IRA_COVER_CLASSES { GENERAL_REGS, LIM_REG_CLASSES }\n+\n #define N_REG_CLASSES (int) LIM_REG_CLASSES\n \n #define REG_CLASS_NAMES \\"}, {"sha": "0c4e6b4366e6a1d9925079a8e5a22c3cdf132e28", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 56, "deletions": 2, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -274,7 +274,8 @@ Objective-C and Objective-C++ Dialects}.\n @xref{Debugging Options,,Options for Debugging Your Program or GCC}.\n @gccoptlist{-d@var{letters}  -dumpspecs  -dumpmachine  -dumpversion @gol\n -fdbg-cnt-list -fdbg-cnt=@var{counter-value-list} @gol\n--fdump-noaddr -fdump-unnumbered  -fdump-translation-unit@r{[}-@var{n}@r{]} @gol\n+-fdump-noaddr -fdump-unnumbered @gol\n+-fdump-translation-unit@r{[}-@var{n}@r{]} @gol\n -fdump-class-hierarchy@r{[}-@var{n}@r{]} @gol\n -fdump-ipa-all -fdump-ipa-cgraph -fdump-ipa-inline @gol\n -fdump-statistics @gol\n@@ -332,7 +333,10 @@ Objective-C and Objective-C++ Dialects}.\n -finline-functions -finline-functions-called-once -finline-limit=@var{n} @gol\n -finline-small-functions -fipa-cp -fipa-cp-clone -fipa-marix-reorg -fipa-pta @gol \n -fipa-pure-const -fipa-reference -fipa-struct-reorg @gol\n--fipa-type-escape -fivopts -fkeep-inline-functions -fkeep-static-consts @gol\n+-fipa-type-escape -fira -fira-algorithm=@var{algorithm} @gol\n+-fira-coalesce -fno-ira-share-save-slots @gol\n+-fno-ira-share-spill-slots -fira-verbose=@var{n} @gol\n+-fivopts -fkeep-inline-functions -fkeep-static-consts @gol\n -fmerge-all-constants -fmerge-constants -fmodulo-sched @gol\n -fmodulo-sched-allow-regmoves -fmove-loop-invariants -fmudflap @gol\n -fmudflapir -fmudflapth -fno-branch-count-reg -fno-default-inline @gol\n@@ -5673,6 +5677,49 @@ optimization.\n \n Enabled at levels @option{-O2}, @option{-O3}, @option{-Os}.\n \n+@item -fira\n+@opindex fira\n+Use the integrated register allocator (@acronym{IRA}) for register\n+allocation.  It is a default if @acronym{IRA} has been ported for the\n+target.\n+\n+@item -fira-algorithm=@var{algorithm}\n+Use specified algorithm for the integrated register allocator.  The\n+@var{algorithm} argument should be one of @code{regional}, @code{CB},\n+or @code{mixed}.  The second algorithm specifies Chaitin-Briggs\n+coloring, the first one specifies regional coloring based on\n+Chaitin-Briggs coloring, and the third one which is the default\n+specifies a mix of Chaitin-Briggs and regional algorithms where loops\n+with small register pressure are ignored.  The first algorithm can\n+give best result for machines with small size and irregular register\n+set, the second one is faster and generates decent code and the\n+smallest size code, and the mixed algorithm usually give the best\n+results in most cases and for most architectures.\n+\n+@item -fira-coalesce\n+@opindex fira-coalesce\n+Do optimistic register coalescing.  This option might be profitable for\n+architectures with big regular register files.\n+\n+@item -fno-ira-share-save-slots\n+@opindex fno-ira-share-save-slots\n+Switch off sharing stack slots used for saving call used hard\n+registers living through a call.  Each hard register will get a\n+separate stack slot and as a result function stack frame will be\n+bigger.\n+\n+@item -fno-ira-share-spill-slots\n+@opindex fno-ira-share-spill-slots\n+Switch off sharing stack slots allocated for pseudo-registers.  Each\n+pseudo-register which did not get a hard register will get a separate\n+stack slot and as a result function stack frame will be bigger.\n+\n+@item -fira-verbose=@var{n}\n+@opindex fira-verbose\n+Set up how verbose dump file for the integrated register allocator\n+will be.  Default value is 5.  If the value is greater or equal to 10,\n+the dump file will be stderr as if the value were @var{n} minus 10.\n+\n @item -fdelayed-branch\n @opindex fdelayed-branch\n If supported for the target machine, attempt to reorder instructions\n@@ -7384,6 +7431,13 @@ processing.  If this limit is hit, SCCVN processing for the whole\n function will not be done and optimizations depending on it will\n be disabled.  The default maximum SCC size is 10000.\n \n+@item ira-max-loops-num\n+IRA uses a regional register allocation by default.  If a function\n+contains loops more than number given by the parameter, non-regional\n+register allocator will be used even when option\n+@option{-fira-algorithm} is given.  The default value of the parameter\n+is 20.\n+\n @end table\n @end table\n "}, {"sha": "9004dd763ec53b213c2b6f96e818a65891265258", "filename": "gcc/doc/passes.texi", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fdoc%2Fpasses.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fdoc%2Fpasses.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Fpasses.texi?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -841,6 +841,28 @@ Global register allocation.  This pass allocates hard registers for\n the remaining pseudo registers (those whose life spans are not\n contained in one basic block).  The pass is located in @file{global.c}.\n \n+@item\n+The optional integrated register allocator (@acronym{IRA}).  It can be\n+used instead of the local and global allocator.  It is called\n+integrated because coalescing, register live range splitting, and hard\n+register preferencing are done on-the-fly during coloring.  It also\n+has better integration with the reload pass.  Pseudo-registers spilled\n+by the allocator or the reload have still a chance to get\n+hard-registers if the reload evicts some pseudo-registers from\n+hard-registers.  The allocator helps to choose better pseudos for\n+spilling based on their live ranges and to coalesce stack slots\n+allocated for the spilled pseudo-registers.  IRA is a regional\n+register allocator which is transformed into Chaitin-Briggs allocator\n+if there is one region.  By default, IRA chooses regions using\n+register pressure but the user can force it to use one region or\n+regions corresponding to all loops.\n+\n+Source files of the allocator are @file{ira.c}, @file{ira-build.c},\n+@file{ira-costs.c}, @file{ira-conflicts.c}, @file{ira-color.c},\n+@file{ira-emit.c}, @file{ira-lives}, plus header files @file{ira.h}\n+and @file{ira-int.h} used for the communication between the allocator\n+and the rest of the compiler and between the IRA files.\n+\n @cindex reloading\n @item\n Reloading.  This pass renumbers pseudo registers with the hardware"}, {"sha": "3087694a9cd3037d2037ffd4853ae9df29686eb5", "filename": "gcc/doc/tm.texi", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fdoc%2Ftm.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fdoc%2Ftm.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Ftm.texi?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -2026,6 +2026,18 @@ The macro body should not assume anything about the contents of\n On most machines, it is not necessary to define this macro.\n @end defmac\n \n+@defmac IRA_HARD_REGNO_ADD_COST_MULTIPLIER (@var{regno})\n+In some case register allocation order is not enough for the\n+Integrated Register Allocator (@acronym{IRA}) to generate a good code.\n+If this macro is defined, it should return a floating point value\n+based on @var{regno}.  The cost of using @var{regno} for a pseudo will\n+be increased by approximately the pseudo's usage frequency times the\n+value returned by this macro.  Not defining this macro is equivalent\n+to having it always return @code{0.0}.\n+\n+On most machines, it is not necessary to define this macro.\n+@end defmac\n+\n @node Values in Registers\n @subsection How Values Fit in Registers\n \n@@ -2814,6 +2826,19 @@ as below:\n @end smallexample\n @end defmac\n \n+@defmac IRA_COVER_CLASSES\n+The macro defines cover classes for the Integrated Register Allocator\n+(@acronym{IRA}).  Cover classes are a set of non-intersecting register\n+classes covering all hard registers used for register allocation\n+purposes.  Any move between two registers in the same cover class\n+should be cheaper than load or store of the registers.  The macro\n+value should be the initializer for an array of register class values,\n+with @code{LIM_REG_CLASSES} used as the end marker.\n+\n+You must define this macro in order to use the integrated register\n+allocator for the target.\n+@end defmac\n+\n @node Old Constraints\n @section Obsolete Macros for Defining Constraints\n @cindex defining constraints, obsolete method"}, {"sha": "1d645d9ebfa435ebe53e97e485d1e458b8f0ea75", "filename": "gcc/flags.h", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fflags.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fflags.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fflags.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -205,6 +205,19 @@ extern int flag_debug_asm;\n extern int flag_next_runtime;\n \n extern int flag_dump_rtl_in_asm;\n+\n+/* The algorithm used for the integrated register allocator (IRA).  */\n+enum ira_algorithm\n+{\n+  IRA_ALGORITHM_REGIONAL,\n+  IRA_ALGORITHM_CB,\n+  IRA_ALGORITHM_MIXED\n+};\n+\n+extern enum ira_algorithm flag_ira_algorithm;\n+\n+extern unsigned int flag_ira_verbose;\n+\n \f\n /* Other basic status info about current function.  */\n "}, {"sha": "e0783d5237cf9b87e1516ebc00d0df8e629407be", "filename": "gcc/global.c", "status": "modified", "additions": 30, "deletions": 11, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fglobal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fglobal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fglobal.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -188,7 +188,7 @@ compute_regs_asm_clobbered (char *regs_asm_clobbered)\n \n /* All registers that can be eliminated.  */\n \n-static HARD_REG_SET eliminable_regset;\n+HARD_REG_SET eliminable_regset;\n \n static int regno_compare (const void *, const void *);\n static int allocno_compare (const void *, const void *);\n@@ -197,7 +197,6 @@ static void prune_preferences (void);\n static void set_preferences (void);\n static void find_reg (int, HARD_REG_SET, int, int, int);\n static void dump_conflicts (FILE *);\n-static void build_insn_chain (void);\n \f\n \n /* Look through the list of eliminable registers.  Set ELIM_SET to the\n@@ -1355,7 +1354,8 @@ mark_elimination (int from, int to)\n \n   FOR_EACH_BB (bb)\n     {\n-      regset r = DF_LIVE_IN (bb);\n+      /* We don't use LIVE info in IRA.  */\n+      regset r = (flag_ira ? DF_LR_IN (bb) : DF_LIVE_IN (bb));\n       if (REGNO_REG_SET_P (r, from))\n \t{\n \t  CLEAR_REGNO_REG_SET (r, from);\n@@ -1385,11 +1385,21 @@ print_insn_chains (FILE *file)\n     print_insn_chain (file, c);\n }\n \n+/* Return true if pseudo REGNO should be added to set live_throughout\n+   or dead_or_set of the insn chains for reload consideration.  */\n+\n+static bool\n+pseudo_for_reload_consideration_p (int regno)\n+{\n+  /* Consider spilled pseudos too for IRA because they still have a\n+     chance to get hard-registers in the reload when IRA is used.  */\n+  return reg_renumber[regno] >= 0 || (flag_ira && optimize);\n+}\n \n /* Walk the insns of the current function and build reload_insn_chain,\n    and record register life information.  */\n \n-static void\n+void\n build_insn_chain (void)\n {\n   unsigned int i;\n@@ -1412,7 +1422,6 @@ build_insn_chain (void)\n   for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n     if (TEST_HARD_REG_BIT (eliminable_regset, i))\n       bitmap_set_bit (elim_regset, i);\n-\n   FOR_EACH_BB_REVERSE (bb)\n     {\n       bitmap_iterator bi;\n@@ -1430,7 +1439,7 @@ build_insn_chain (void)\n \n       EXECUTE_IF_SET_IN_BITMAP (df_get_live_out (bb), FIRST_PSEUDO_REGISTER, i, bi)\n \t{\n-\t  if (reg_renumber[i] >= 0)\n+\t  if (pseudo_for_reload_consideration_p (i))\n \t    bitmap_set_bit (live_relevant_regs, i);\n \t}\n \n@@ -1467,11 +1476,13 @@ build_insn_chain (void)\n \t\t\t    if (!fixed_regs[regno])\n \t\t\t      bitmap_set_bit (&c->dead_or_set, regno);\n \t\t\t  }\n-\t\t\telse if (reg_renumber[regno] >= 0)\n+\t\t\telse if (pseudo_for_reload_consideration_p (regno))\n \t\t\t  bitmap_set_bit (&c->dead_or_set, regno);\n \t\t      }\n \n-\t\t    if ((regno < FIRST_PSEUDO_REGISTER || reg_renumber[regno] >= 0)\n+\t\t    if ((regno < FIRST_PSEUDO_REGISTER\n+\t\t\t || reg_renumber[regno] >= 0\n+\t\t\t || (flag_ira && optimize))\n \t\t\t&& (!DF_REF_FLAGS_IS_SET (def, DF_REF_CONDITIONAL)))\n \t\t      {\n \t\t\trtx reg = DF_REF_REG (def);\n@@ -1567,11 +1578,12 @@ build_insn_chain (void)\n \t\t\t    if (!fixed_regs[regno])\n \t\t\t      bitmap_set_bit (&c->dead_or_set, regno);\n \t\t\t  }\n-\t\t\telse if (reg_renumber[regno] >= 0)\n+\t\t\telse if (pseudo_for_reload_consideration_p (regno))\n \t\t\t  bitmap_set_bit (&c->dead_or_set, regno);\n \t\t      }\n \t\t    \n-\t\t    if (regno < FIRST_PSEUDO_REGISTER || reg_renumber[regno] >= 0)\n+\t\t    if (regno < FIRST_PSEUDO_REGISTER\n+\t\t\t|| pseudo_for_reload_consideration_p (regno))\n \t\t      {\n \t\t\tif (GET_CODE (reg) == SUBREG\n \t\t\t    && !DF_REF_FLAGS_IS_SET (use,\n@@ -1748,6 +1760,13 @@ dump_global_regs (FILE *file)\n   fprintf (file, \"\\n\\n\");\n }\n \n+\n+static bool\n+gate_handle_global_alloc (void)\n+{\n+  return ! flag_ira;\n+}\n+\n /* Run old register allocator.  Return TRUE if we must exit\n    rest_of_compilation upon return.  */\n static unsigned int\n@@ -1811,7 +1830,7 @@ struct rtl_opt_pass pass_global_alloc =\n  {\n   RTL_PASS,\n   \"greg\",                               /* name */\n-  NULL,                                 /* gate */\n+  gate_handle_global_alloc,             /* gate */\n   rest_of_handle_global_alloc,          /* execute */\n   NULL,                                 /* sub */\n   NULL,                                 /* next */"}, {"sha": "c4f74a267c0fffc8a88eac99b94a5f0919687347", "filename": "gcc/hard-reg-set.h", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fhard-reg-set.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fhard-reg-set.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fhard-reg-set.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -538,6 +538,11 @@ extern char global_regs[FIRST_PSEUDO_REGISTER];\n \n extern HARD_REG_SET regs_invalidated_by_call;\n \n+/* Call used hard registers which can not be saved because there is no\n+   insn for this.  */\n+\n+extern HARD_REG_SET no_caller_save_reg_set;\n+\n #ifdef REG_ALLOC_ORDER\n /* Table of register numbers in the order in which to try to use them.  */\n \n@@ -556,6 +561,10 @@ extern HARD_REG_SET reg_class_contents[N_REG_CLASSES];\n \n extern unsigned int reg_class_size[N_REG_CLASSES];\n \n+/* For each reg class, table listing all the classes contained in it.  */\n+\n+extern enum reg_class reg_class_subclasses[N_REG_CLASSES][N_REG_CLASSES];\n+\n /* For each pair of reg classes,\n    a largest reg class contained in their union.  */\n "}, {"sha": "979a3c8668553dd9551fb4485c2150b2d9755cf6", "filename": "gcc/ira-build.c", "status": "added", "additions": 2449, "deletions": 0, "changes": 2449, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-build.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-build.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira-build.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -0,0 +1,2449 @@\n+/* Building internal representation for IRA.\n+   Copyright (C) 2006, 2007, 2008\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"target.h\"\n+#include \"regs.h\"\n+#include \"flags.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"toplev.h\"\n+#include \"params.h\"\n+#include \"df.h\"\n+#include \"output.h\"\n+#include \"reload.h\"\n+#include \"sparseset.h\"\n+#include \"ira-int.h\"\n+\n+static ira_copy_t find_allocno_copy (ira_allocno_t, ira_allocno_t, rtx,\n+\t\t\t\t     ira_loop_tree_node_t);\n+\n+/* The root of the loop tree corresponding to the all function.  */\n+ira_loop_tree_node_t ira_loop_tree_root;\n+\n+/* Height of the loop tree.  */\n+int ira_loop_tree_height;\n+\n+/* All nodes representing basic blocks are referred through the\n+   following array.  We can not use basic block member `aux' for this\n+   because it is used for insertion of insns on edges.  */\n+ira_loop_tree_node_t ira_bb_nodes;\n+\n+/* All nodes representing loops are referred through the following\n+   array.  */\n+ira_loop_tree_node_t ira_loop_nodes;\n+\n+/* Map regno -> allocnos with given regno (see comments for \n+   allocno member `next_regno_allocno').  */\n+ira_allocno_t *ira_regno_allocno_map;\n+\n+/* Array of references to all allocnos.  The order number of the\n+   allocno corresponds to the index in the array.  Removed allocnos\n+   have NULL element value.  */\n+ira_allocno_t *ira_allocnos;\n+\n+/* Sizes of the previous array.  */\n+int ira_allocnos_num;\n+\n+/* Map conflict id -> allocno with given conflict id (see comments for\n+   allocno member `conflict_id').  */\n+ira_allocno_t *ira_conflict_id_allocno_map;\n+\n+/* Array of references to all copies.  The order number of the copy\n+   corresponds to the index in the array.  Removed copies have NULL\n+   element value.  */\n+ira_copy_t *ira_copies;\n+\n+/* Size of the previous array.  */\n+int ira_copies_num;\n+\n+\f\n+\n+/* LAST_BASIC_BLOCK before generating additional insns because of live\n+   range splitting.  Emitting insns on a critical edge creates a new\n+   basic block.  */\n+static int last_basic_block_before_change;\n+\n+/* The following function allocates the loop tree nodes.  If LOOPS_P\n+   is FALSE, the nodes corresponding to the loops (except the root\n+   which corresponds the all function) will be not allocated but nodes\n+   will still be allocated for basic blocks.  */\n+static void\n+create_loop_tree_nodes (bool loops_p)\n+{\n+  unsigned int i, j;\n+  int max_regno;\n+  bool skip_p;\n+  edge_iterator ei;\n+  edge e;\n+  VEC (edge, heap) *edges;\n+  loop_p loop;\n+\n+  ira_bb_nodes\n+    = ((struct ira_loop_tree_node *)\n+       ira_allocate (sizeof (struct ira_loop_tree_node) * last_basic_block));\n+  last_basic_block_before_change = last_basic_block;\n+  for (i = 0; i < (unsigned int) last_basic_block; i++)\n+    {\n+      ira_bb_nodes[i].regno_allocno_map = NULL;\n+      memset (ira_bb_nodes[i].reg_pressure, 0,\n+\t      sizeof (ira_bb_nodes[i].reg_pressure));\n+      ira_bb_nodes[i].mentioned_allocnos = NULL;\n+      ira_bb_nodes[i].modified_regnos = NULL;\n+      ira_bb_nodes[i].border_allocnos = NULL;\n+      ira_bb_nodes[i].local_copies = NULL;\n+    }\n+  ira_loop_nodes = ((struct ira_loop_tree_node *)\n+\t\t    ira_allocate (sizeof (struct ira_loop_tree_node)\n+\t\t\t\t  * VEC_length (loop_p, ira_loops.larray)));\n+  max_regno = max_reg_num ();\n+  for (i = 0; VEC_iterate (loop_p, ira_loops.larray, i, loop); i++)\n+    {\n+      if (loop != ira_loops.tree_root)\n+\t{\n+\t  ira_loop_nodes[i].regno_allocno_map = NULL;\n+\t  if (! loops_p)\n+\t    continue;\n+\t  skip_p = false;\n+\t  FOR_EACH_EDGE (e, ei, loop->header->preds)\n+\t    if (e->src != loop->latch\n+\t\t&& (e->flags & EDGE_ABNORMAL) && EDGE_CRITICAL_P (e))\n+\t      {\n+\t\tskip_p = true;\n+\t\tbreak;\n+\t      }\n+\t  if (skip_p)\n+\t    continue;\n+\t  edges = get_loop_exit_edges (loop);\n+\t  for (j = 0; VEC_iterate (edge, edges, j, e); j++)\n+\t    if ((e->flags & EDGE_ABNORMAL) && EDGE_CRITICAL_P (e))\n+\t      {\n+\t\tskip_p = true;\n+\t\tbreak;\n+\t      }\n+\t  VEC_free (edge, heap, edges);\n+\t  if (skip_p)\n+\t    continue;\n+\t}\n+      ira_loop_nodes[i].regno_allocno_map\n+\t= (ira_allocno_t *) ira_allocate (sizeof (ira_allocno_t) * max_regno);\n+      memset (ira_loop_nodes[i].regno_allocno_map, 0,\n+\t      sizeof (ira_allocno_t) * max_regno);\n+      memset (ira_loop_nodes[i].reg_pressure, 0,\n+\t      sizeof (ira_loop_nodes[i].reg_pressure));\n+      ira_loop_nodes[i].mentioned_allocnos = ira_allocate_bitmap ();\n+      ira_loop_nodes[i].modified_regnos = ira_allocate_bitmap ();\n+      ira_loop_nodes[i].border_allocnos = ira_allocate_bitmap ();\n+      ira_loop_nodes[i].local_copies = ira_allocate_bitmap ();\n+    }\n+}\n+\n+/* The function returns TRUE if there are more one allocation\n+   region.  */\n+static bool\n+more_one_region_p (void)\n+{\n+  unsigned int i;\n+  loop_p loop;\n+\n+  for (i = 0; VEC_iterate (loop_p, ira_loops.larray, i, loop); i++)\n+    if (ira_loop_nodes[i].regno_allocno_map != NULL\n+\t&& ira_loop_tree_root != &ira_loop_nodes[i])\n+      return true;\n+  return false;\n+}\n+\n+/* Free the loop tree node of a loop.  */\n+static void\n+finish_loop_tree_node (ira_loop_tree_node_t loop)\n+{\n+  if (loop->regno_allocno_map != NULL)\n+    {\n+      ira_assert (loop->bb == NULL);\n+      ira_free_bitmap (loop->local_copies);\n+      ira_free_bitmap (loop->border_allocnos);\n+      ira_free_bitmap (loop->modified_regnos);\n+      ira_free_bitmap (loop->mentioned_allocnos);\n+      ira_free (loop->regno_allocno_map);\n+      loop->regno_allocno_map = NULL;\n+    }\n+}\n+\n+/* Free the loop tree nodes.  */\n+static void\n+finish_loop_tree_nodes (void)\n+{\n+  unsigned int i;\n+  loop_p loop;\n+\n+  for (i = 0; VEC_iterate (loop_p, ira_loops.larray, i, loop); i++)\n+    finish_loop_tree_node (&ira_loop_nodes[i]);\n+  ira_free (ira_loop_nodes);\n+  for (i = 0; i < (unsigned int) last_basic_block_before_change; i++)\n+    {\n+      if (ira_bb_nodes[i].local_copies != NULL)\n+\tira_free_bitmap (ira_bb_nodes[i].local_copies);\n+      if (ira_bb_nodes[i].border_allocnos != NULL)\n+\tira_free_bitmap (ira_bb_nodes[i].border_allocnos);\n+      if (ira_bb_nodes[i].modified_regnos != NULL)\n+\tira_free_bitmap (ira_bb_nodes[i].modified_regnos);\n+      if (ira_bb_nodes[i].mentioned_allocnos != NULL)\n+\tira_free_bitmap (ira_bb_nodes[i].mentioned_allocnos);\n+      if (ira_bb_nodes[i].regno_allocno_map != NULL)\n+\tira_free (ira_bb_nodes[i].regno_allocno_map);\n+    }\n+  ira_free (ira_bb_nodes);\n+}\n+\n+\f\n+\n+/* The following recursive function adds LOOP to the loop tree\n+   hierarchy.  LOOP is added only once.  */\n+static void\n+add_loop_to_tree (struct loop *loop)\n+{\n+  struct loop *parent;\n+  ira_loop_tree_node_t loop_node, parent_node;\n+\n+  /* We can not use loop node access macros here because of potential\n+     checking and because the nodes are not initialized enough\n+     yet.  */\n+  if (loop_outer (loop) != NULL)\n+    add_loop_to_tree (loop_outer (loop));\n+  if (ira_loop_nodes[loop->num].regno_allocno_map != NULL\n+      && ira_loop_nodes[loop->num].children == NULL)\n+    {\n+      /* We have not added loop node to the tree yet.  */\n+      loop_node = &ira_loop_nodes[loop->num];\n+      loop_node->loop = loop;\n+      loop_node->bb = NULL;\n+      for (parent = loop_outer (loop);\n+\t   parent != NULL;\n+\t   parent = loop_outer (parent))\n+\tif (ira_loop_nodes[parent->num].regno_allocno_map != NULL)\n+\t  break;\n+      if (parent == NULL)\n+\t{\n+\t  loop_node->next = NULL;\n+\t  loop_node->subloop_next = NULL;\n+\t  loop_node->parent = NULL;\n+\t}\n+      else\n+\t{\n+\t  parent_node = &ira_loop_nodes[parent->num];\n+\t  loop_node->next = parent_node->children;\n+\t  parent_node->children = loop_node;\n+\t  loop_node->subloop_next = parent_node->subloops;\n+\t  parent_node->subloops = loop_node;\n+\t  loop_node->parent = parent_node;\n+\t}\n+    }\n+}\n+\n+/* The following recursive function sets up levels of nodes of the\n+   tree given its root LOOP_NODE.  The enumeration starts with LEVEL.\n+   The function returns maximal value of level in the tree + 1.  */\n+static int\n+setup_loop_tree_level (ira_loop_tree_node_t loop_node, int level)\n+{\n+  int height, max_height;\n+  ira_loop_tree_node_t subloop_node;\n+\n+  ira_assert (loop_node->bb == NULL);\n+  loop_node->level = level;\n+  max_height = level + 1;\n+  for (subloop_node = loop_node->subloops;\n+       subloop_node != NULL;\n+       subloop_node = subloop_node->subloop_next)\n+    {\n+      ira_assert (subloop_node->bb == NULL);\n+      height = setup_loop_tree_level (subloop_node, level + 1);\n+      if (height > max_height)\n+\tmax_height = height;\n+    }\n+  return max_height;\n+}\n+\n+/* Create the loop tree.  The algorithm is designed to provide correct\n+   order of loops (they are ordered by their last loop BB) and basic\n+   blocks in the chain formed by member next.  */\n+static void\n+form_loop_tree (void)\n+{\n+  unsigned int i;\n+  basic_block bb;\n+  struct loop *parent;\n+  ira_loop_tree_node_t bb_node, loop_node;\n+  loop_p loop;\n+\n+  /* We can not use loop/bb node access macros because of potential\n+     checking and because the nodes are not initialized enough\n+     yet.  */\n+  for (i = 0; VEC_iterate (loop_p, ira_loops.larray, i, loop); i++)\n+     if (ira_loop_nodes[i].regno_allocno_map != NULL)\n+       {\n+\t ira_loop_nodes[i].children = NULL;\n+\t ira_loop_nodes[i].subloops = NULL;\n+       }\n+  FOR_EACH_BB_REVERSE (bb)\n+    {\n+      bb_node = &ira_bb_nodes[bb->index];\n+      bb_node->bb = bb;\n+      bb_node->loop = NULL;\n+      bb_node->subloops = NULL;\n+      bb_node->children = NULL;\n+      bb_node->subloop_next = NULL;\n+      bb_node->next = NULL;\n+      for (parent = bb->loop_father;\n+\t   parent != NULL;\n+\t   parent = loop_outer (parent))\n+\tif (ira_loop_nodes[parent->num].regno_allocno_map != NULL)\n+\t  break;\n+      add_loop_to_tree (parent);\n+      loop_node = &ira_loop_nodes[parent->num];\n+      bb_node->next = loop_node->children;\n+      bb_node->parent = loop_node;\n+      loop_node->children = bb_node;\n+    }\n+  ira_loop_tree_root = IRA_LOOP_NODE_BY_INDEX (ira_loops.tree_root->num);\n+  ira_loop_tree_height = setup_loop_tree_level (ira_loop_tree_root, 0);\n+  ira_assert (ira_loop_tree_root->regno_allocno_map != NULL);\n+}\n+\n+\f\n+\n+/* Rebuild IRA_REGNO_ALLOCNO_MAP and REGNO_ALLOCNO_MAPs of the loop\n+   tree nodes.  */\n+static void\n+rebuild_regno_allocno_maps (void)\n+{\n+  unsigned int l;\n+  int max_regno, regno;\n+  ira_allocno_t a;\n+  ira_loop_tree_node_t loop_tree_node;\n+  loop_p loop;\n+  ira_allocno_iterator ai;\n+\n+  max_regno = max_reg_num ();\n+  for (l = 0; VEC_iterate (loop_p, ira_loops.larray, l, loop); l++)\n+    if (ira_loop_nodes[l].regno_allocno_map != NULL)\n+      {\n+\tira_free (ira_loop_nodes[l].regno_allocno_map);\n+\tira_loop_nodes[l].regno_allocno_map\n+\t  = (ira_allocno_t *) ira_allocate (sizeof (ira_allocno_t)\n+\t\t\t\t\t    * max_regno);\n+\tmemset (ira_loop_nodes[l].regno_allocno_map, 0,\n+\t\tsizeof (ira_allocno_t) * max_regno);\n+      }\n+  ira_free (ira_regno_allocno_map);\n+  ira_regno_allocno_map\n+    = (ira_allocno_t *) ira_allocate (max_regno * sizeof (ira_allocno_t));\n+  memset (ira_regno_allocno_map, 0, max_regno * sizeof (ira_allocno_t));\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      if (ALLOCNO_CAP_MEMBER (a) != NULL)\n+\t/* Caps are not in the regno allocno maps.  */\n+\tcontinue;\n+      regno = ALLOCNO_REGNO (a);\n+      loop_tree_node = ALLOCNO_LOOP_TREE_NODE (a);\n+      ALLOCNO_NEXT_REGNO_ALLOCNO (a) = ira_regno_allocno_map[regno];\n+      ira_regno_allocno_map[regno] = a;\n+      if (loop_tree_node->regno_allocno_map[regno] == NULL)\n+\t/* Remember that we can create temporary allocnos to break\n+\t   cycles in register shuffle.  */\n+\tloop_tree_node->regno_allocno_map[regno] = a;\n+    }\n+}\n+\n+\f\n+\n+/* Pools for allocnos and allocno live ranges.  */\n+static alloc_pool allocno_pool, allocno_live_range_pool;\n+\n+/* Vec containing references to all created allocnos.  It is a\n+   container of array allocnos.  */\n+static VEC(ira_allocno_t,heap) *allocno_vec;\n+\n+/* Vec containing references to all created allocnos.  It is a\n+   container of ira_conflict_id_allocno_map.  */\n+static VEC(ira_allocno_t,heap) *ira_conflict_id_allocno_map_vec;\n+\n+/* Initialize data concerning allocnos.  */\n+static void\n+initiate_allocnos (void)\n+{\n+  allocno_live_range_pool\n+    = create_alloc_pool (\"allocno live ranges\",\n+\t\t\t sizeof (struct ira_allocno_live_range), 100);\n+  allocno_pool\n+    = create_alloc_pool (\"allocnos\", sizeof (struct ira_allocno), 100);\n+  allocno_vec = VEC_alloc (ira_allocno_t, heap, max_reg_num () * 2);\n+  ira_allocnos = NULL;\n+  ira_allocnos_num = 0;\n+  ira_conflict_id_allocno_map_vec\n+    = VEC_alloc (ira_allocno_t, heap, max_reg_num () * 2);\n+  ira_conflict_id_allocno_map = NULL;\n+  ira_regno_allocno_map\n+    = (ira_allocno_t *) ira_allocate (max_reg_num () * sizeof (ira_allocno_t));\n+  memset (ira_regno_allocno_map, 0, max_reg_num () * sizeof (ira_allocno_t));\n+}\n+\n+/* Create and return the allocno corresponding to REGNO in\n+   LOOP_TREE_NODE.  Add the allocno to the list of allocnos with the\n+   same regno if CAP_P is FALSE.  */\n+ira_allocno_t\n+ira_create_allocno (int regno, bool cap_p, ira_loop_tree_node_t loop_tree_node)\n+{\n+  ira_allocno_t a;\n+\n+  a = (ira_allocno_t) pool_alloc (allocno_pool);\n+  ALLOCNO_REGNO (a) = regno;\n+  ALLOCNO_LOOP_TREE_NODE (a) = loop_tree_node;\n+  if (! cap_p)\n+    {\n+      ALLOCNO_NEXT_REGNO_ALLOCNO (a) = ira_regno_allocno_map[regno];\n+      ira_regno_allocno_map[regno] = a;\n+      if (loop_tree_node->regno_allocno_map[regno] == NULL)\n+\t/* Remember that we can create temporary allocnos to break\n+\t   cycles in register shuffle on region borders (see\n+\t   ira-emit.c).  */\n+\tloop_tree_node->regno_allocno_map[regno] = a;\n+    }\n+  ALLOCNO_CAP (a) = NULL;\n+  ALLOCNO_CAP_MEMBER (a) = NULL;\n+  ALLOCNO_NUM (a) = ira_allocnos_num;\n+  ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a) = NULL;\n+  ALLOCNO_CONFLICT_ALLOCNOS_NUM (a) = 0;\n+  COPY_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (a), ira_no_alloc_regs);\n+  COPY_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a), ira_no_alloc_regs);\n+  ALLOCNO_NREFS (a) = 0;\n+  ALLOCNO_FREQ (a) = 1;\n+  ALLOCNO_HARD_REGNO (a) = -1;\n+  ALLOCNO_CALL_FREQ (a) = 0;\n+  ALLOCNO_CALLS_CROSSED_NUM (a) = 0;\n+#ifdef STACK_REGS\n+  ALLOCNO_NO_STACK_REG_P (a) = false;\n+  ALLOCNO_TOTAL_NO_STACK_REG_P (a) = false;\n+#endif\n+  ALLOCNO_MEM_OPTIMIZED_DEST (a) = NULL;\n+  ALLOCNO_MEM_OPTIMIZED_DEST_P (a) = false;\n+  ALLOCNO_SOMEWHERE_RENAMED_P (a) = false;\n+  ALLOCNO_CHILD_RENAMED_P (a) = false;\n+  ALLOCNO_DONT_REASSIGN_P (a) = false;\n+  ALLOCNO_IN_GRAPH_P (a) = false;\n+  ALLOCNO_ASSIGNED_P (a) = false;\n+  ALLOCNO_MAY_BE_SPILLED_P (a) = false;\n+  ALLOCNO_SPLAY_REMOVED_P (a) = false;\n+  ALLOCNO_CONFLICT_VEC_P (a) = false;\n+  ALLOCNO_MODE (a) = (regno < 0 ? VOIDmode : PSEUDO_REGNO_MODE (regno));\n+  ALLOCNO_COPIES (a) = NULL;\n+  ALLOCNO_HARD_REG_COSTS (a) = NULL;\n+  ALLOCNO_CONFLICT_HARD_REG_COSTS (a) = NULL;\n+  ALLOCNO_UPDATED_HARD_REG_COSTS (a) = NULL;\n+  ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a) = NULL;\n+  ALLOCNO_LEFT_CONFLICTS_NUM (a) = -1;\n+  ALLOCNO_COVER_CLASS (a) = NO_REGS;\n+  ALLOCNO_COVER_CLASS_COST (a) = 0;\n+  ALLOCNO_MEMORY_COST (a) = 0;\n+  ALLOCNO_UPDATED_MEMORY_COST (a) = 0;\n+  ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (a) = 0;\n+  ALLOCNO_NEXT_BUCKET_ALLOCNO (a) = NULL;\n+  ALLOCNO_PREV_BUCKET_ALLOCNO (a) = NULL;\n+  ALLOCNO_FIRST_COALESCED_ALLOCNO (a) = a;\n+  ALLOCNO_NEXT_COALESCED_ALLOCNO (a) = a;\n+  ALLOCNO_LIVE_RANGES (a) = NULL;\n+  ALLOCNO_MIN (a) = INT_MAX;\n+  ALLOCNO_MAX (a) = -1;\n+  ALLOCNO_CONFLICT_ID (a) = ira_allocnos_num;\n+  VEC_safe_push (ira_allocno_t, heap, allocno_vec, a);\n+  ira_allocnos = VEC_address (ira_allocno_t, allocno_vec);\n+  ira_allocnos_num = VEC_length (ira_allocno_t, allocno_vec);\n+  VEC_safe_push (ira_allocno_t, heap, ira_conflict_id_allocno_map_vec, a);\n+  ira_conflict_id_allocno_map\n+    = VEC_address (ira_allocno_t, ira_conflict_id_allocno_map_vec);\n+  return a;\n+}\n+\n+/* Set up cover class for A and update its conflict hard registers.  */\n+void\n+ira_set_allocno_cover_class (ira_allocno_t a, enum reg_class cover_class)\n+{\n+  ALLOCNO_COVER_CLASS (a) = cover_class;\n+  IOR_COMPL_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (a),\n+\t\t\t  reg_class_contents[cover_class]);\n+  IOR_COMPL_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a),\n+\t\t\t  reg_class_contents[cover_class]);\n+}\n+\n+/* Return TRUE if the conflict vector with NUM elements is more\n+   profitable than conflict bit vector for A.  */\n+bool\n+ira_conflict_vector_profitable_p (ira_allocno_t a, int num)\n+{\n+  int nw;\n+\n+  if (ALLOCNO_MAX (a) < ALLOCNO_MIN (a))\n+    /* We prefer bit vector in such case because it does not result in\n+       allocation.  */\n+    return false;\n+\n+  nw = (ALLOCNO_MAX (a) - ALLOCNO_MIN (a) + IRA_INT_BITS) / IRA_INT_BITS;\n+  return (2 * sizeof (ira_allocno_t) * (num + 1)\n+\t  < 3 * nw * sizeof (IRA_INT_TYPE));\n+}\n+\n+/* Allocates and initialize the conflict vector of A for NUM\n+   conflicting allocnos.  */\n+void\n+ira_allocate_allocno_conflict_vec (ira_allocno_t a, int num)\n+{\n+  int size;\n+  ira_allocno_t *vec;\n+\n+  ira_assert (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a) == NULL);\n+  num++; /* for NULL end marker  */\n+  size = sizeof (ira_allocno_t) * num;\n+  ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a) = ira_allocate (size);\n+  vec = (ira_allocno_t *) ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a);\n+  vec[0] = NULL;\n+  ALLOCNO_CONFLICT_ALLOCNOS_NUM (a) = 0;\n+  ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a) = size;\n+  ALLOCNO_CONFLICT_VEC_P (a) = true;\n+}\n+\n+/* Allocate and initialize the conflict bit vector of A.  */\n+static void\n+allocate_allocno_conflict_bit_vec (ira_allocno_t a)\n+{\n+  unsigned int size;\n+\n+  ira_assert (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a) == NULL);\n+  size = ((ALLOCNO_MAX (a) - ALLOCNO_MIN (a) + IRA_INT_BITS)\n+\t  / IRA_INT_BITS * sizeof (IRA_INT_TYPE));\n+  ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a) = ira_allocate (size);\n+  memset (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a), 0, size);\n+  ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a) = size;\n+  ALLOCNO_CONFLICT_VEC_P (a) = false;\n+}\n+\n+/* Allocate and initialize the conflict vector or conflict bit vector\n+   of A for NUM conflicting allocnos whatever is more profitable.  */\n+void\n+ira_allocate_allocno_conflicts (ira_allocno_t a, int num)\n+{\n+  if (ira_conflict_vector_profitable_p (a, num))\n+    ira_allocate_allocno_conflict_vec (a, num);\n+  else\n+    allocate_allocno_conflict_bit_vec (a);\n+}\n+\n+/* Add A2 to the conflicts of A1.  */\n+static void\n+add_to_allocno_conflicts (ira_allocno_t a1, ira_allocno_t a2)\n+{\n+  int num;\n+  unsigned int size;\n+\n+  if (ALLOCNO_CONFLICT_VEC_P (a1))\n+    {\n+      ira_allocno_t *vec;\n+\n+      num = ALLOCNO_CONFLICT_ALLOCNOS_NUM (a1) + 2;\n+      if (ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a1)\n+\t  >=  num * sizeof (ira_allocno_t))\n+\tvec = (ira_allocno_t *) ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1);\n+      else\n+\t{\n+\t  size = (3 * num / 2 + 1) * sizeof (ira_allocno_t);\n+\t  vec = (ira_allocno_t *) ira_allocate (size);\n+\t  memcpy (vec, ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1),\n+\t\t  sizeof (ira_allocno_t) * ALLOCNO_CONFLICT_ALLOCNOS_NUM (a1));\n+\t  ira_free (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1));\n+\t  ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1) = vec;\n+\t  ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a1) = size;\n+\t}\n+      vec[num - 2] = a2;\n+      vec[num - 1] = NULL;\n+      ALLOCNO_CONFLICT_ALLOCNOS_NUM (a1)++;\n+    }\n+  else\n+    {\n+      int nw, added_head_nw, id;\n+      IRA_INT_TYPE *vec;\n+\n+      id = ALLOCNO_CONFLICT_ID (a2);\n+      vec = (IRA_INT_TYPE *) ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1);\n+      if (ALLOCNO_MIN (a1) > id)\n+\t{\n+\t  /* Expand head of the bit vector.  */\n+\t  added_head_nw = (ALLOCNO_MIN (a1) - id - 1) / IRA_INT_BITS + 1;\n+\t  nw = (ALLOCNO_MAX (a1) - ALLOCNO_MIN (a1)) / IRA_INT_BITS + 1;\n+\t  size = (nw + added_head_nw) * sizeof (IRA_INT_TYPE);\n+\t  if (ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a1) >= size)\n+\t    {\n+\t      memmove ((char *) vec + added_head_nw * sizeof (IRA_INT_TYPE),\n+\t\t       vec, nw * sizeof (IRA_INT_TYPE));\n+\t      memset (vec, 0, added_head_nw * sizeof (IRA_INT_TYPE));\n+\t    }\n+\t  else\n+\t    {\n+\t      size\n+\t\t= (3 * (nw + added_head_nw) / 2 + 1) * sizeof (IRA_INT_TYPE);\n+\t      vec = (IRA_INT_TYPE *) ira_allocate (size);\n+\t      memcpy ((char *) vec + added_head_nw * sizeof (IRA_INT_TYPE),\n+\t\t      ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1),\n+\t\t      nw * sizeof (IRA_INT_TYPE));\n+\t      memset (vec, 0, added_head_nw * sizeof (IRA_INT_TYPE));\n+\t      memset ((char *) vec\n+\t\t      + (nw + added_head_nw) * sizeof (IRA_INT_TYPE),\n+\t\t      0, size - (nw + added_head_nw) * sizeof (IRA_INT_TYPE));\n+\t      ira_free (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1));\n+\t      ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1) = vec;\n+\t      ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a1) = size;\n+\t    }\n+\t  ALLOCNO_MIN (a1) -= added_head_nw * IRA_INT_BITS;\n+\t}\n+      else if (ALLOCNO_MAX (a1) < id)\n+\t{\n+\t  nw = (id - ALLOCNO_MIN (a1)) / IRA_INT_BITS + 1;\n+\t  size = nw * sizeof (IRA_INT_TYPE);\n+\t  if (ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a1) < size)\n+\t    {\n+\t      /* Expand tail of the bit vector.  */\n+\t      size = (3 * nw / 2 + 1) * sizeof (IRA_INT_TYPE);\n+\t      vec = (IRA_INT_TYPE *) ira_allocate (size);\n+\t      memcpy (vec, ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1),\n+\t\t      ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a1));\n+\t      memset ((char *) vec + ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a1),\n+\t\t      0, size - ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a1));\n+\t      ira_free (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1));\n+\t      ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a1) = vec;\n+\t      ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a1) = size;\n+\t    }\n+\t  ALLOCNO_MAX (a1) = id;\n+\t}\n+      SET_ALLOCNO_SET_BIT (vec, id, ALLOCNO_MIN (a1), ALLOCNO_MAX (a1));\n+    }\n+}\n+\n+/* Add A1 to the conflicts of A2 and vise versa.  */\n+void\n+ira_add_allocno_conflict (ira_allocno_t a1, ira_allocno_t a2)\n+{\n+  add_to_allocno_conflicts (a1, a2);\n+  add_to_allocno_conflicts (a2, a1);\n+}\n+\n+/* Clear all conflicts of allocno A.  */\n+static void\n+clear_allocno_conflicts (ira_allocno_t a)\n+{\n+  if (ALLOCNO_CONFLICT_VEC_P (a))\n+    {\n+      ALLOCNO_CONFLICT_ALLOCNOS_NUM (a) = 0;\n+      ((ira_allocno_t *) ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a))[0] = NULL;\n+    }\n+  else if (ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a) != 0)\n+    {\n+      int nw;\n+\n+      nw = (ALLOCNO_MAX (a) - ALLOCNO_MIN (a)) / IRA_INT_BITS + 1;\n+      memset (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a), 0,\n+\t      nw * sizeof (IRA_INT_TYPE));\n+    }\n+}\n+\n+/* The array used to find duplications in conflict vectors of\n+   allocnos.  */\n+static int *allocno_conflict_check;\n+\n+/* The value used to mark allocation presence in conflict vector of\n+   the current allocno.  */\n+static int curr_allocno_conflict_check_tick;\n+\n+/* Remove duplications in conflict vector of A.  */\n+static void\n+compress_allocno_conflict_vec (ira_allocno_t a)\n+{\n+  ira_allocno_t *vec, conflict_a;\n+  int i, j;\n+\n+  ira_assert (ALLOCNO_CONFLICT_VEC_P (a));\n+  vec = (ira_allocno_t *) ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a);\n+  curr_allocno_conflict_check_tick++;\n+  for (i = j = 0; (conflict_a = vec[i]) != NULL; i++)\n+    {\n+      if (allocno_conflict_check[ALLOCNO_NUM (conflict_a)]\n+\t  != curr_allocno_conflict_check_tick)\n+\t{\n+\t  allocno_conflict_check[ALLOCNO_NUM (conflict_a)]\n+\t    = curr_allocno_conflict_check_tick;\n+\t  vec[j++] = conflict_a;\n+\t}\n+    }\n+  ALLOCNO_CONFLICT_ALLOCNOS_NUM (a) = j;\n+  vec[j] = NULL;\n+}\n+\n+/* Remove duplications in conflict vectors of all allocnos.  */\n+static void\n+compress_conflict_vecs (void)\n+{\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  allocno_conflict_check\n+    = (int *) ira_allocate (sizeof (int) * ira_allocnos_num);\n+  memset (allocno_conflict_check, 0, sizeof (int) * ira_allocnos_num);\n+  curr_allocno_conflict_check_tick = 0;\n+  FOR_EACH_ALLOCNO (a, ai)\n+    if (ALLOCNO_CONFLICT_VEC_P (a))\n+      compress_allocno_conflict_vec (a);\n+  ira_free (allocno_conflict_check);\n+}\n+\n+/* This recursive function outputs allocno A and if it is a cap the\n+   function outputs its members.  */\n+void\n+ira_print_expanded_allocno (ira_allocno_t a)\n+{\n+  basic_block bb;\n+\n+  fprintf (ira_dump_file, \" a%d(r%d\", ALLOCNO_NUM (a), ALLOCNO_REGNO (a));\n+  if ((bb = ALLOCNO_LOOP_TREE_NODE (a)->bb) != NULL)\n+    fprintf (ira_dump_file, \",b%d\", bb->index);\n+  else\n+    fprintf (ira_dump_file, \",l%d\", ALLOCNO_LOOP_TREE_NODE (a)->loop->num);\n+  if (ALLOCNO_CAP_MEMBER (a) != NULL)\n+    {\n+      fprintf (ira_dump_file, \":\");\n+      ira_print_expanded_allocno (ALLOCNO_CAP_MEMBER (a));\n+    }\n+  fprintf (ira_dump_file, \")\");\n+}\n+\n+/* Create and return the cap representing allocno A in the\n+   parent loop.  */\n+static ira_allocno_t\n+create_cap_allocno (ira_allocno_t a)\n+{\n+  ira_allocno_t cap;\n+  ira_loop_tree_node_t parent;\n+  enum reg_class cover_class;\n+\n+  ira_assert (ALLOCNO_FIRST_COALESCED_ALLOCNO (a) == a\n+\t      && ALLOCNO_NEXT_COALESCED_ALLOCNO (a) == a);\n+  parent = ALLOCNO_LOOP_TREE_NODE (a)->parent;\n+  cap = ira_create_allocno (ALLOCNO_REGNO (a), true, parent);\n+  ALLOCNO_MODE (cap) = ALLOCNO_MODE (a);\n+  cover_class = ALLOCNO_COVER_CLASS (a);\n+  ira_set_allocno_cover_class (cap, cover_class);\n+  ALLOCNO_AVAILABLE_REGS_NUM (cap) = ALLOCNO_AVAILABLE_REGS_NUM (a);\n+  ALLOCNO_CAP_MEMBER (cap) = a;\n+  bitmap_set_bit (parent->mentioned_allocnos, ALLOCNO_NUM (cap));\n+  ALLOCNO_CAP (a) = cap;\n+  ALLOCNO_COVER_CLASS_COST (cap) = ALLOCNO_COVER_CLASS_COST (a);\n+  ALLOCNO_MEMORY_COST (cap) = ALLOCNO_MEMORY_COST (a);\n+  ALLOCNO_UPDATED_MEMORY_COST (cap) = ALLOCNO_UPDATED_MEMORY_COST (a);\n+  ira_allocate_and_copy_costs\n+    (&ALLOCNO_HARD_REG_COSTS (cap), cover_class, ALLOCNO_HARD_REG_COSTS (a));\n+  ira_allocate_and_copy_costs\n+    (&ALLOCNO_CONFLICT_HARD_REG_COSTS (cap), cover_class,\n+     ALLOCNO_CONFLICT_HARD_REG_COSTS (a));\n+  ALLOCNO_NREFS (cap) = ALLOCNO_NREFS (a);\n+  ALLOCNO_FREQ (cap) = ALLOCNO_FREQ (a);\n+  ALLOCNO_CALL_FREQ (cap) = ALLOCNO_CALL_FREQ (a);\n+  IOR_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (cap),\n+\t\t    ALLOCNO_CONFLICT_HARD_REGS (a));\n+  IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (cap),\n+\t\t    ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a));\n+  ALLOCNO_CALLS_CROSSED_NUM (cap) = ALLOCNO_CALLS_CROSSED_NUM (a);\n+#ifdef STACK_REGS\n+  ALLOCNO_NO_STACK_REG_P (cap) = ALLOCNO_NO_STACK_REG_P (a);\n+  ALLOCNO_TOTAL_NO_STACK_REG_P (cap) = ALLOCNO_TOTAL_NO_STACK_REG_P (a);\n+#endif\n+  if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+    {\n+      fprintf (ira_dump_file, \"    Creating cap \");\n+      ira_print_expanded_allocno (cap);\n+      fprintf (ira_dump_file, \"\\n\");\n+    }\n+  return cap;\n+}\n+\n+/* Create and return allocno live range with given attributes.  */\n+allocno_live_range_t\n+ira_create_allocno_live_range (ira_allocno_t a, int start, int finish,\n+\t\t\t       allocno_live_range_t next)\n+{\n+  allocno_live_range_t p;\n+\n+  p = (allocno_live_range_t) pool_alloc (allocno_live_range_pool);\n+  p->allocno = a;\n+  p->start = start;\n+  p->finish = finish;\n+  p->next = next;\n+  return p;\n+}\n+\n+/* Copy allocno live range R and return the result.  */\n+static allocno_live_range_t\n+copy_allocno_live_range (allocno_live_range_t r)\n+{\n+  allocno_live_range_t p;\n+\n+  p = (allocno_live_range_t) pool_alloc (allocno_live_range_pool);\n+  *p = *r;\n+  return p;\n+}\n+\n+/* Copy allocno live range list given by its head R and return the\n+   result.  */\n+static allocno_live_range_t\n+copy_allocno_live_range_list (allocno_live_range_t r)\n+{\n+  allocno_live_range_t p, first, last;\n+\n+  if (r == NULL)\n+    return NULL;\n+  for (first = last = NULL; r != NULL; r = r->next)\n+    {\n+      p = copy_allocno_live_range (r);\n+      if (first == NULL)\n+\tfirst = p;\n+      else\n+\tlast->next = p;\n+      last = p;\n+    }\n+  return first;\n+}\n+\n+/* Free allocno live range R.  */\n+void\n+ira_finish_allocno_live_range (allocno_live_range_t r)\n+{\n+  pool_free (allocno_live_range_pool, r);\n+}\n+\n+/* Free updated register costs of allocno A.  */\n+void\n+ira_free_allocno_updated_costs (ira_allocno_t a)\n+{\n+  enum reg_class cover_class;\n+\n+  cover_class = ALLOCNO_COVER_CLASS (a);\n+  if (ALLOCNO_UPDATED_HARD_REG_COSTS (a) != NULL)\n+    ira_free_cost_vector (ALLOCNO_UPDATED_HARD_REG_COSTS (a), cover_class);\n+  ALLOCNO_UPDATED_HARD_REG_COSTS (a) = NULL;\n+  if (ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a) != NULL)\n+    ira_free_cost_vector (ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a),\n+\t\t\t  cover_class);\n+  ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a) = NULL;\n+}\n+\n+/* Free the memory allocated for allocno A.  */\n+static void\n+finish_allocno (ira_allocno_t a)\n+{\n+  allocno_live_range_t r, next_r;\n+  enum reg_class cover_class = ALLOCNO_COVER_CLASS (a);\n+\n+  ira_allocnos[ALLOCNO_NUM (a)] = NULL;\n+  ira_conflict_id_allocno_map[ALLOCNO_CONFLICT_ID (a)] = NULL;\n+  if (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a) != NULL)\n+    ira_free (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a));\n+  if (ALLOCNO_HARD_REG_COSTS (a) != NULL)\n+    ira_free_cost_vector (ALLOCNO_HARD_REG_COSTS (a), cover_class);\n+  if (ALLOCNO_CONFLICT_HARD_REG_COSTS (a) != NULL)\n+    ira_free_cost_vector (ALLOCNO_CONFLICT_HARD_REG_COSTS (a), cover_class);\n+  if (ALLOCNO_UPDATED_HARD_REG_COSTS (a) != NULL)\n+    ira_free_cost_vector (ALLOCNO_UPDATED_HARD_REG_COSTS (a), cover_class);\n+  if (ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a) != NULL)\n+    ira_free_cost_vector (ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a),\n+\t\t\t  cover_class);\n+  for (r = ALLOCNO_LIVE_RANGES (a); r != NULL; r = next_r)\n+    {\n+      next_r = r->next;\n+      ira_finish_allocno_live_range (r);\n+    }\n+  pool_free (allocno_pool, a);\n+}\n+\n+/* Free the memory allocated for all allocnos.  */\n+static void\n+finish_allocnos (void)\n+{\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    finish_allocno (a);\n+  ira_free (ira_regno_allocno_map);\n+  VEC_free (ira_allocno_t, heap, ira_conflict_id_allocno_map_vec);\n+  VEC_free (ira_allocno_t, heap, allocno_vec);\n+  free_alloc_pool (allocno_pool);\n+  free_alloc_pool (allocno_live_range_pool);\n+}\n+\n+\f\n+\n+/* Pools for copies.  */\n+static alloc_pool copy_pool;\n+\n+/* Vec containing references to all created copies.  It is a\n+   container of array ira_copies.  */\n+static VEC(ira_copy_t,heap) *copy_vec;\n+\n+/* The function initializes data concerning allocno copies.  */\n+static void\n+initiate_copies (void)\n+{\n+  copy_pool\n+    = create_alloc_pool (\"copies\", sizeof (struct ira_allocno_copy), 100);\n+  copy_vec = VEC_alloc (ira_copy_t, heap, get_max_uid ());\n+  ira_copies = NULL;\n+  ira_copies_num = 0;\n+}\n+\n+/* Return copy connecting A1 and A2 and originated from INSN of\n+   LOOP_TREE_NODE if any.  */\n+static ira_copy_t\n+find_allocno_copy (ira_allocno_t a1, ira_allocno_t a2, rtx insn,\n+\t\t   ira_loop_tree_node_t loop_tree_node)\n+{\n+  ira_copy_t cp, next_cp;\n+  ira_allocno_t another_a;\n+\n+  for (cp = ALLOCNO_COPIES (a1); cp != NULL; cp = next_cp)\n+    {\n+      if (cp->first == a1)\n+\t{\n+\t  next_cp = cp->next_first_allocno_copy;\n+\t  another_a = cp->second;\n+\t}\n+      else if (cp->second == a1)\n+\t{\n+\t  next_cp = cp->next_second_allocno_copy;\n+\t  another_a = cp->first;\n+\t}\n+      else\n+\tgcc_unreachable ();\n+      if (another_a == a2 && cp->insn == insn\n+\t  && cp->loop_tree_node == loop_tree_node)\n+\treturn cp;\n+    }\n+  return NULL;\n+}\n+\n+/* Create and return copy with given attributes LOOP_TREE_NODE, FIRST,\n+   SECOND, FREQ, and INSN.  */\n+ira_copy_t\n+ira_create_copy (ira_allocno_t first, ira_allocno_t second, int freq, rtx insn,\n+\t\t ira_loop_tree_node_t loop_tree_node)\n+{\n+  ira_copy_t cp;\n+\n+  cp = (ira_copy_t) pool_alloc (copy_pool);\n+  cp->num = ira_copies_num;\n+  cp->first = first;\n+  cp->second = second;\n+  cp->freq = freq;\n+  cp->insn = insn;\n+  cp->loop_tree_node = loop_tree_node;\n+  VEC_safe_push (ira_copy_t, heap, copy_vec, cp);\n+  ira_copies = VEC_address (ira_copy_t, copy_vec);\n+  ira_copies_num = VEC_length (ira_copy_t, copy_vec);\n+  return cp;\n+}\n+\n+/* Attach a copy CP to allocnos involved into the copy.  */\n+void\n+ira_add_allocno_copy_to_list (ira_copy_t cp)\n+{\n+  ira_allocno_t first = cp->first, second = cp->second;\n+\n+  cp->prev_first_allocno_copy = NULL;\n+  cp->prev_second_allocno_copy = NULL;\n+  cp->next_first_allocno_copy = ALLOCNO_COPIES (first);\n+  if (cp->next_first_allocno_copy != NULL)\n+    {\n+      if (cp->next_first_allocno_copy->first == first)\n+\tcp->next_first_allocno_copy->prev_first_allocno_copy = cp;\n+      else\n+\tcp->next_first_allocno_copy->prev_second_allocno_copy = cp;\n+    }\n+  cp->next_second_allocno_copy = ALLOCNO_COPIES (second);\n+  if (cp->next_second_allocno_copy != NULL)\n+    {\n+      if (cp->next_second_allocno_copy->second == second)\n+\tcp->next_second_allocno_copy->prev_second_allocno_copy = cp;\n+      else\n+\tcp->next_second_allocno_copy->prev_first_allocno_copy = cp;\n+    }\n+  ALLOCNO_COPIES (first) = cp;\n+  ALLOCNO_COPIES (second) = cp;\n+}\n+\n+/* Detach a copy CP from allocnos involved into the copy.  */\n+void\n+ira_remove_allocno_copy_from_list (ira_copy_t cp)\n+{\n+  ira_allocno_t first = cp->first, second = cp->second;\n+  ira_copy_t prev, next;\n+\n+  next = cp->next_first_allocno_copy;\n+  prev = cp->prev_first_allocno_copy;\n+  if (prev == NULL)\n+    ALLOCNO_COPIES (first) = next;\n+  else if (prev->first == first)\n+    prev->next_first_allocno_copy = next;\n+  else\n+    prev->next_second_allocno_copy = next;\n+  if (next != NULL)\n+    {\n+      if (next->first == first)\n+\tnext->prev_first_allocno_copy = prev;\n+      else\n+\tnext->prev_second_allocno_copy = prev;\n+    }\n+  cp->prev_first_allocno_copy = cp->next_first_allocno_copy = NULL;\n+\n+  next = cp->next_second_allocno_copy;\n+  prev = cp->prev_second_allocno_copy;\n+  if (prev == NULL)\n+    ALLOCNO_COPIES (second) = next;\n+  else if (prev->second == second)\n+    prev->next_second_allocno_copy = next;\n+  else\n+    prev->next_first_allocno_copy = next;\n+  if (next != NULL)\n+    {\n+      if (next->second == second)\n+\tnext->prev_second_allocno_copy = prev;\n+      else\n+\tnext->prev_first_allocno_copy = prev;\n+    }\n+  cp->prev_second_allocno_copy = cp->next_second_allocno_copy = NULL;\n+}\n+\n+/* Make a copy CP a canonical copy where number of the\n+   first allocno is less than the second one.  */\n+void\n+ira_swap_allocno_copy_ends_if_necessary (ira_copy_t cp)\n+{\n+  ira_allocno_t temp;\n+  ira_copy_t temp_cp;\n+\n+  if (ALLOCNO_NUM (cp->first) <= ALLOCNO_NUM (cp->second))\n+    return;\n+\n+  temp = cp->first;\n+  cp->first = cp->second;\n+  cp->second = temp;\n+\n+  temp_cp = cp->prev_first_allocno_copy;\n+  cp->prev_first_allocno_copy = cp->prev_second_allocno_copy;\n+  cp->prev_second_allocno_copy = temp_cp;\n+\n+  temp_cp = cp->next_first_allocno_copy;\n+  cp->next_first_allocno_copy = cp->next_second_allocno_copy;\n+  cp->next_second_allocno_copy = temp_cp;\n+}\n+\n+/* Create (or update frequency if the copy already exists) and return\n+   the copy of allocnos FIRST and SECOND with frequency FREQ\n+   corresponding to move insn INSN (if any) and originated from\n+   LOOP_TREE_NODE.  */\n+ira_copy_t\n+ira_add_allocno_copy (ira_allocno_t first, ira_allocno_t second, int freq,\n+\t\t      rtx insn, ira_loop_tree_node_t loop_tree_node)\n+{\n+  ira_copy_t cp;\n+\n+  if ((cp = find_allocno_copy (first, second, insn, loop_tree_node)) != NULL)\n+    {\n+      cp->freq += freq;\n+      return cp;\n+    }\n+  cp = ira_create_copy (first, second, freq, insn, loop_tree_node);\n+  ira_assert (first != NULL && second != NULL);\n+  ira_add_allocno_copy_to_list (cp);\n+  ira_swap_allocno_copy_ends_if_necessary (cp);\n+  return cp;\n+}\n+\n+/* Print info about copies involving allocno A into file F.  */\n+static void\n+print_allocno_copies (FILE *f, ira_allocno_t a)\n+{\n+  ira_allocno_t another_a;\n+  ira_copy_t cp, next_cp;\n+\n+  fprintf (f, \" a%d(r%d):\", ALLOCNO_NUM (a), ALLOCNO_REGNO (a));\n+  for (cp = ALLOCNO_COPIES (a); cp != NULL; cp = next_cp)\n+    {\n+      if (cp->first == a)\n+\t{\n+\t  next_cp = cp->next_first_allocno_copy;\n+\t  another_a = cp->second;\n+\t}\n+      else if (cp->second == a)\n+\t{\n+\t  next_cp = cp->next_second_allocno_copy;\n+\t  another_a = cp->first;\n+\t}\n+      else\n+\tgcc_unreachable ();\n+      fprintf (f, \" cp%d:a%d(r%d)@%d\", cp->num,\n+\t       ALLOCNO_NUM (another_a), ALLOCNO_REGNO (another_a), cp->freq);\n+    }\n+  fprintf (f, \"\\n\");\n+}\n+\n+/* Print info about copies involving allocno A into stderr.  */\n+void\n+ira_debug_allocno_copies (ira_allocno_t a)\n+{\n+  print_allocno_copies (stderr, a);\n+}\n+\n+/* The function frees memory allocated for copy CP.  */\n+static void\n+finish_copy (ira_copy_t cp)\n+{\n+  pool_free (copy_pool, cp);\n+}\n+\n+\n+/* Free memory allocated for all copies.  */\n+static void\n+finish_copies (void)\n+{\n+  ira_copy_t cp;\n+  ira_copy_iterator ci;\n+\n+  FOR_EACH_COPY (cp, ci)\n+    finish_copy (cp);\n+  VEC_free (ira_copy_t, heap, copy_vec);\n+  free_alloc_pool (copy_pool);\n+}\n+\n+\f\n+\n+/* Pools for cost vectors.  It is defined only for cover classes.  */\n+static alloc_pool cost_vector_pool[N_REG_CLASSES];\n+\n+/* The function initiates work with hard register cost vectors.  It\n+   creates allocation pool for each cover class.  */\n+static void\n+initiate_cost_vectors (void)\n+{\n+  int i;\n+  enum reg_class cover_class;\n+\n+  for (i = 0; i < ira_reg_class_cover_size; i++)\n+    {\n+      cover_class = ira_reg_class_cover[i];\n+      cost_vector_pool[cover_class]\n+\t= create_alloc_pool (\"cost vectors\",\n+\t\t\t     sizeof (int)\n+\t\t\t     * ira_class_hard_regs_num[cover_class],\n+\t\t\t     100);\n+    }\n+}\n+\n+/* Allocate and return a cost vector VEC for COVER_CLASS.  */\n+int *\n+ira_allocate_cost_vector (enum reg_class cover_class)\n+{\n+  return (int *) pool_alloc (cost_vector_pool[cover_class]);\n+}\n+\n+/* Free a cost vector VEC for COVER_CLASS.  */\n+void\n+ira_free_cost_vector (int *vec, enum reg_class cover_class)\n+{\n+  ira_assert (vec != NULL);\n+  pool_free (cost_vector_pool[cover_class], vec);\n+}\n+\n+/* Finish work with hard register cost vectors.  Release allocation\n+   pool for each cover class.  */\n+static void\n+finish_cost_vectors (void)\n+{\n+  int i;\n+  enum reg_class cover_class;\n+\n+  for (i = 0; i < ira_reg_class_cover_size; i++)\n+    {\n+      cover_class = ira_reg_class_cover[i];\n+      free_alloc_pool (cost_vector_pool[cover_class]);\n+    }\n+}\n+\n+\f\n+\n+/* The current loop tree node and its regno allocno map.  */\n+ira_loop_tree_node_t ira_curr_loop_tree_node;\n+ira_allocno_t *ira_curr_regno_allocno_map;\n+\n+/* This recursive function traverses loop tree with root LOOP_NODE\n+   calling non-null functions PREORDER_FUNC and POSTORDER_FUNC\n+   correspondingly in preorder and postorder.  The function sets up\n+   IRA_CURR_LOOP_TREE_NODE and IRA_CURR_REGNO_ALLOCNO_MAP.  If BB_P,\n+   basic block nodes of LOOP_NODE is also processed (before its\n+   subloop nodes).  */\n+void\n+ira_traverse_loop_tree (bool bb_p, ira_loop_tree_node_t loop_node,\n+\t\t\tvoid (*preorder_func) (ira_loop_tree_node_t),\n+\t\t\tvoid (*postorder_func) (ira_loop_tree_node_t))\n+{\n+  ira_loop_tree_node_t subloop_node;\n+\n+  ira_assert (loop_node->bb == NULL);\n+  ira_curr_loop_tree_node = loop_node;\n+  ira_curr_regno_allocno_map = ira_curr_loop_tree_node->regno_allocno_map;\n+\n+  if (preorder_func != NULL)\n+    (*preorder_func) (loop_node);\n+  \n+  if (bb_p)\n+    for (subloop_node = loop_node->children;\n+\t subloop_node != NULL;\n+\t subloop_node = subloop_node->next)\n+      if (subloop_node->bb != NULL)\n+\t{\n+\t  if (preorder_func != NULL)\n+\t    (*preorder_func) (subloop_node);\n+  \n+\t  if (postorder_func != NULL)\n+\t    (*postorder_func) (subloop_node);\n+\t}\n+  \n+  for (subloop_node = loop_node->subloops;\n+       subloop_node != NULL;\n+       subloop_node = subloop_node->subloop_next)\n+    {\n+      ira_assert (subloop_node->bb == NULL);\n+      ira_traverse_loop_tree (bb_p, subloop_node,\n+\t\t\t      preorder_func, postorder_func);\n+    }\n+\n+  ira_curr_loop_tree_node = loop_node;\n+  ira_curr_regno_allocno_map = ira_curr_loop_tree_node->regno_allocno_map;\n+\n+  if (postorder_func != NULL)\n+    (*postorder_func) (loop_node);\n+}\n+\n+\f\n+\n+/* The basic block currently being processed.  */\n+static basic_block curr_bb;\n+\n+/* This recursive function creates allocnos corresponding to\n+   pseudo-registers containing in X.  True OUTPUT_P means that X is\n+   a lvalue.  */\n+static void\n+create_insn_allocnos (rtx x, bool output_p)\n+{\n+  int i, j;\n+  const char *fmt;\n+  enum rtx_code code = GET_CODE (x);\n+\n+  if (code == REG)\n+    {\n+      int regno;\n+\n+      if ((regno = REGNO (x)) >= FIRST_PSEUDO_REGISTER)\n+\t{\n+\t  ira_allocno_t a;\n+\n+\t  if ((a = ira_curr_regno_allocno_map[regno]) == NULL)\n+\t    a = ira_create_allocno (regno, false, ira_curr_loop_tree_node);\n+\t  \n+\t  ALLOCNO_NREFS (a)++;\n+\t  ALLOCNO_FREQ (a) += REG_FREQ_FROM_BB (curr_bb);\n+\t  bitmap_set_bit (ira_curr_loop_tree_node->mentioned_allocnos,\n+\t\t\t  ALLOCNO_NUM (a));\n+\t  if (output_p)\n+\t    bitmap_set_bit (ira_curr_loop_tree_node->modified_regnos, regno);\n+\t}\n+      return;\n+    }\n+  else if (code == SET)\n+    {\n+      create_insn_allocnos (SET_DEST (x), true);\n+      create_insn_allocnos (SET_SRC (x), false);\n+      return;\n+    }\n+  else if (code == CLOBBER)\n+    {\n+      create_insn_allocnos (XEXP (x, 0), true);\n+      return;\n+    }\n+  else if (code == MEM)\n+    {\n+      create_insn_allocnos (XEXP (x, 0), false);\n+      return;\n+    }\n+  else if (code == PRE_DEC || code == POST_DEC || code == PRE_INC || \n+\t   code == POST_INC || code == POST_MODIFY || code == PRE_MODIFY)\n+    {\n+      create_insn_allocnos (XEXP (x, 0), true);\n+      create_insn_allocnos (XEXP (x, 0), false);\n+      return;\n+    }\n+\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\tcreate_insn_allocnos (XEXP (x, i), output_p);\n+      else if (fmt[i] == 'E')\n+\tfor (j = 0; j < XVECLEN (x, i); j++)\n+\t  create_insn_allocnos (XVECEXP (x, i, j), output_p);\n+    }\n+}\n+\n+/* Create allocnos corresponding to pseudo-registers living in the\n+   basic block represented by the corresponding loop tree node\n+   BB_NODE.  */\n+static void\n+create_bb_allocnos (ira_loop_tree_node_t bb_node)\n+{\n+  basic_block bb;\n+  rtx insn;\n+  unsigned int i;\n+  bitmap_iterator bi;\n+\n+  curr_bb = bb = bb_node->bb;\n+  ira_assert (bb != NULL);\n+  FOR_BB_INSNS (bb, insn)\n+    if (INSN_P (insn))\n+      create_insn_allocnos (PATTERN (insn), false);\n+  /* It might be a allocno living through from one subloop to\n+     another.  */\n+  EXECUTE_IF_SET_IN_REG_SET (DF_LR_IN (bb), FIRST_PSEUDO_REGISTER, i, bi)\n+    if (ira_curr_regno_allocno_map[i] == NULL)\n+      ira_create_allocno (i, false, ira_curr_loop_tree_node);\n+}\n+\n+/* Create allocnos corresponding to pseudo-registers living on edge E\n+   (a loop entry or exit).  Also mark the allocnos as living on the\n+   loop border. */\n+static void\n+create_loop_allocnos (edge e)\n+{\n+  unsigned int i;\n+  bitmap live_in_regs, border_allocnos;\n+  bitmap_iterator bi;\n+  ira_loop_tree_node_t parent;\n+\n+  live_in_regs = DF_LR_IN (e->dest);\n+  border_allocnos = ira_curr_loop_tree_node->border_allocnos;\n+  EXECUTE_IF_SET_IN_REG_SET (DF_LR_OUT (e->src),\n+\t\t\t     FIRST_PSEUDO_REGISTER, i, bi)\n+    if (bitmap_bit_p (live_in_regs, i))\n+      {\n+\tif (ira_curr_regno_allocno_map[i] == NULL)\n+\t  {\n+\t    /* The order of creations is important for right\n+\t       ira_regno_allocno_map.  */\n+\t    if ((parent = ira_curr_loop_tree_node->parent) != NULL\n+\t\t&& parent->regno_allocno_map[i] == NULL)\n+\t      ira_create_allocno (i, false, parent);\n+\t    ira_create_allocno (i, false, ira_curr_loop_tree_node);\n+\t  }\n+\tbitmap_set_bit (border_allocnos,\n+\t\t\tALLOCNO_NUM (ira_curr_regno_allocno_map[i]));\n+      }\n+}\n+\n+/* Create allocnos corresponding to pseudo-registers living in loop\n+   represented by the corresponding loop tree node LOOP_NODE.  This\n+   function is called by ira_traverse_loop_tree.  */\n+static void\n+create_loop_tree_node_allocnos (ira_loop_tree_node_t loop_node)\n+{\n+  if (loop_node->bb != NULL)\n+    create_bb_allocnos (loop_node);\n+  else if (loop_node != ira_loop_tree_root)\n+    {\n+      int i;\n+      edge_iterator ei;\n+      edge e;\n+      VEC (edge, heap) *edges;\n+\n+      FOR_EACH_EDGE (e, ei, loop_node->loop->header->preds)\n+\tif (e->src != loop_node->loop->latch)\n+\t  create_loop_allocnos (e);\n+      \n+      edges = get_loop_exit_edges (loop_node->loop);\n+      for (i = 0; VEC_iterate (edge, edges, i, e); i++)\n+\tcreate_loop_allocnos (e);\n+      VEC_free (edge, heap, edges);\n+    }\n+}\n+\n+/* Propagate information about allocnos modified inside the loop given\n+   by its LOOP_TREE_NODE to its parent.  */\n+static void\n+propagate_modified_regnos (ira_loop_tree_node_t loop_tree_node)\n+{\n+  if (loop_tree_node == ira_loop_tree_root)\n+    return;\n+  ira_assert (loop_tree_node->bb == NULL);\n+  bitmap_ior_into (loop_tree_node->parent->modified_regnos,\n+\t\t   loop_tree_node->modified_regnos);\n+}\n+\n+/* Propagate new info about allocno A (see comments about accumulated\n+   info in allocno definition) to the corresponding allocno on upper\n+   loop tree level.  So allocnos on upper levels accumulate\n+   information about the corresponding allocnos in nested regions.\n+   The new info means allocno info finally calculated in this\n+   file.  */\n+static void\n+propagate_allocno_info (void)\n+{\n+  int i;\n+  ira_allocno_t a, parent_a;\n+  ira_loop_tree_node_t parent;\n+  enum reg_class cover_class;\n+\n+  if (flag_ira_algorithm != IRA_ALGORITHM_REGIONAL\n+      && flag_ira_algorithm != IRA_ALGORITHM_MIXED)\n+    return;\n+  for (i = max_reg_num () - 1; i >= FIRST_PSEUDO_REGISTER; i--)\n+    for (a = ira_regno_allocno_map[i];\n+\t a != NULL;\n+\t a = ALLOCNO_NEXT_REGNO_ALLOCNO (a))\n+      if ((parent = ALLOCNO_LOOP_TREE_NODE (a)->parent) != NULL\n+\t  && (parent_a = parent->regno_allocno_map[i]) != NULL\n+\t  /* There are no caps yet at this point.  So use\n+\t     border_allocnos to find allocnos for the propagation.  */\n+\t  && bitmap_bit_p (ALLOCNO_LOOP_TREE_NODE (a)->border_allocnos,\n+\t\t\t   ALLOCNO_NUM (a)))\n+\t{\n+\t  ALLOCNO_NREFS (parent_a) += ALLOCNO_NREFS (a);\n+\t  ALLOCNO_FREQ (parent_a) += ALLOCNO_FREQ (a);\n+\t  ALLOCNO_CALL_FREQ (parent_a) += ALLOCNO_CALL_FREQ (a);\n+#ifdef STACK_REGS\n+\t  if (ALLOCNO_TOTAL_NO_STACK_REG_P (a))\n+\t    ALLOCNO_TOTAL_NO_STACK_REG_P (parent_a) = true;\n+#endif\n+\t  IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (parent_a),\n+\t\t\t    ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a));\n+\t  ALLOCNO_CALLS_CROSSED_NUM (parent_a)\n+\t    += ALLOCNO_CALLS_CROSSED_NUM (a);\n+\t  ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (parent_a)\n+\t    += ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (a);\n+\t  cover_class = ALLOCNO_COVER_CLASS (a);\n+\t  ira_assert (cover_class == ALLOCNO_COVER_CLASS (parent_a));\n+\t  ira_allocate_and_accumulate_costs\n+\t    (&ALLOCNO_HARD_REG_COSTS (parent_a), cover_class,\n+\t     ALLOCNO_HARD_REG_COSTS (a));\n+\t  ira_allocate_and_accumulate_costs\n+\t    (&ALLOCNO_CONFLICT_HARD_REG_COSTS (parent_a),\n+\t     cover_class,\n+\t     ALLOCNO_CONFLICT_HARD_REG_COSTS (a));\n+\t  ALLOCNO_COVER_CLASS_COST (parent_a)\n+\t    += ALLOCNO_COVER_CLASS_COST (a);\n+\t  ALLOCNO_MEMORY_COST (parent_a) += ALLOCNO_MEMORY_COST (a);\n+\t  ALLOCNO_UPDATED_MEMORY_COST (parent_a)\n+\t    += ALLOCNO_UPDATED_MEMORY_COST (a);\n+\t}\n+}\n+\n+/* Create allocnos corresponding to pseudo-registers in the current\n+   function.  Traverse the loop tree for this.  */\n+static void\n+create_allocnos (void)\n+{\n+  /* We need to process BB first to correctly link allocnos by member\n+     next_regno_allocno.  */\n+  ira_traverse_loop_tree (true, ira_loop_tree_root,\n+\t\t\t  create_loop_tree_node_allocnos, NULL);\n+  if (optimize)\n+    ira_traverse_loop_tree (false, ira_loop_tree_root, NULL,\n+\t\t\t    propagate_modified_regnos);\n+}\n+\n+\f\n+\n+/* The page contains function to remove some regions from a separate\n+   register allocation.  We remove regions whose separate allocation\n+   will hardly improve the result.  As a result we speed up regional\n+   register allocation.  */\n+\n+/* Merge ranges R1 and R2 and returns the result.  The function\n+   maintains the order of ranges and tries to minimize number of the\n+   result ranges.  */\n+static allocno_live_range_t \n+merge_ranges (allocno_live_range_t r1, allocno_live_range_t r2)\n+{\n+  allocno_live_range_t first, last, temp;\n+\n+  if (r1 == NULL)\n+    return r2;\n+  if (r2 == NULL)\n+    return r1;\n+  for (first = last = NULL; r1 != NULL && r2 != NULL;)\n+    {\n+      if (r1->start < r2->start)\n+\t{\n+\t  temp = r1;\n+\t  r1 = r2;\n+\t  r2 = temp;\n+\t}\n+      if (r1->start <= r2->finish + 1)\n+\t{\n+\t  /* Intersected ranges: merge r1 and r2 into r1.  */\n+\t  r1->start = r2->start;\n+\t  if (r1->finish < r2->finish)\n+\t    r1->finish = r2->finish;\n+\t  temp = r2;\n+\t  r2 = r2->next;\n+\t  ira_finish_allocno_live_range (temp);\n+\t  if (r2 == NULL)\n+\t    {\n+\t      /* To try to merge with subsequent ranges in r1.  */\n+\t      r2 = r1->next;\n+\t      r1->next = NULL;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  /* Add r1 to the result.  */\n+\t  if (first == NULL)\n+\t    first = last = r1;\n+\t  else\n+\t    {\n+\t      last->next = r1;\n+\t      last = r1;\n+\t    }\n+\t  r1 = r1->next;\n+\t  if (r1 == NULL)\n+\t    {\n+\t      /* To try to merge with subsequent ranges in r2.  */\n+\t      r1 = r2->next;\n+\t      r2->next = NULL;\n+\t    }\n+\t}\n+    }\n+  if (r1 != NULL)\n+    {\n+      if (first == NULL)\n+\tfirst = r1;\n+      else\n+\tlast->next = r1;\n+      ira_assert (r1->next == NULL);\n+    }\n+  else if (r2 != NULL)\n+    {\n+      if (first == NULL)\n+\tfirst = r2;\n+      else\n+\tlast->next = r2;\n+      ira_assert (r2->next == NULL);\n+    }\n+  else\n+    {\n+      ira_assert (last->next == NULL);\n+    }\n+  return first;\n+}\n+\n+/* The function changes allocno in range list given by R onto A.  */\n+static void\n+change_allocno_in_range_list (allocno_live_range_t r, ira_allocno_t a)\n+{\n+  for (; r != NULL; r = r->next)\n+    r->allocno = a;\n+}\n+\n+/* Return TRUE if NODE represents a loop with low register\n+   pressure.  */\n+static bool\n+low_pressure_loop_node_p (ira_loop_tree_node_t node)\n+{\n+  int i;\n+  enum reg_class cover_class;\n+  \n+  if (node->bb != NULL)\n+    return false;\n+  \n+  for (i = 0; i < ira_reg_class_cover_size; i++)\n+    {\n+      cover_class = ira_reg_class_cover[i];\n+      if (node->reg_pressure[cover_class]\n+\t  > ira_available_class_regs[cover_class])\n+\treturn false;\n+    }\n+  return true;\n+}\n+\n+/* Return TRUE if NODE represents a loop with should be removed from\n+   regional allocation.  We remove a loop with low register pressure\n+   inside another loop with register pressure.  In this case a\n+   separate allocation of the loop hardly helps (for irregular\n+   register file architecture it could help by choosing a better hard\n+   register in the loop but we prefer faster allocation even in this\n+   case).  */\n+static bool\n+loop_node_to_be_removed_p (ira_loop_tree_node_t node)\n+{\n+  return (node->parent != NULL && low_pressure_loop_node_p (node->parent)\n+\t  && low_pressure_loop_node_p (node));\n+}\n+\n+/* Definition of vector of loop tree nodes.  */\n+DEF_VEC_P(ira_loop_tree_node_t);\n+DEF_VEC_ALLOC_P(ira_loop_tree_node_t, heap);\n+\n+/* Vec containing references to all removed loop tree nodes.  */\n+static VEC(ira_loop_tree_node_t,heap) *removed_loop_vec;\n+\n+/* Vec containing references to all children of loop tree nodes.  */\n+static VEC(ira_loop_tree_node_t,heap) *children_vec;\n+\n+/* Remove subregions of NODE if their separate allocation will not\n+   improve the result.  */\n+static void\n+remove_uneccesary_loop_nodes_from_loop_tree (ira_loop_tree_node_t node)\n+{\n+  unsigned int start;\n+  bool remove_p;\n+  ira_loop_tree_node_t subnode;\n+\n+  remove_p = loop_node_to_be_removed_p (node);\n+  if (! remove_p)\n+    VEC_safe_push (ira_loop_tree_node_t, heap, children_vec, node);\n+  start = VEC_length (ira_loop_tree_node_t, children_vec);\n+  for (subnode = node->children; subnode != NULL; subnode = subnode->next)\n+    if (subnode->bb == NULL)\n+      remove_uneccesary_loop_nodes_from_loop_tree (subnode);\n+    else\n+      VEC_safe_push (ira_loop_tree_node_t, heap, children_vec, subnode);\n+  node->children = node->subloops = NULL;\n+  if (remove_p)\n+    {\n+      VEC_safe_push (ira_loop_tree_node_t, heap, removed_loop_vec, node);\n+      return;\n+    }\n+  while (VEC_length (ira_loop_tree_node_t, children_vec) > start)\n+    {\n+      subnode = VEC_pop (ira_loop_tree_node_t, children_vec);\n+      subnode->parent = node;\n+      subnode->next = node->children;\n+      node->children = subnode;\n+      if (subnode->bb == NULL)\n+\t{\n+\t  subnode->subloop_next = node->subloops;\n+\t  node->subloops = subnode;\n+\t}\n+    }\n+}\n+\n+/* Remove allocnos from loops removed from the allocation\n+   consideration.  */\n+static void\n+remove_unnecessary_allocnos (void)\n+{\n+  int regno;\n+  bool merged_p;\n+  enum reg_class cover_class;\n+  ira_allocno_t a, prev_a, next_a, parent_a;\n+  ira_loop_tree_node_t a_node, parent;\n+  allocno_live_range_t r;\n+\n+  merged_p = false;\n+  for (regno = max_reg_num () - 1; regno >= FIRST_PSEUDO_REGISTER; regno--)\n+    for (prev_a = NULL, a = ira_regno_allocno_map[regno];\n+\t a != NULL;\n+\t a = next_a)\n+      {\n+\tnext_a = ALLOCNO_NEXT_REGNO_ALLOCNO (a);\n+\ta_node = ALLOCNO_LOOP_TREE_NODE (a);\n+\tif (! loop_node_to_be_removed_p (a_node))\n+\t  prev_a = a;\n+\telse\n+\t  {\n+\t    for (parent = a_node->parent;\n+\t\t (parent_a = parent->regno_allocno_map[regno]) == NULL\n+\t\t   && loop_node_to_be_removed_p (parent);\n+\t\t parent = parent->parent)\n+\t      ;\n+\t    if (parent_a == NULL)\n+\t      {\n+\t\t/* There are no allocnos with the same regno in upper\n+\t\t   region -- just move the allocno to the upper\n+\t\t   region.  */\n+\t\tprev_a = a;\n+\t\tALLOCNO_LOOP_TREE_NODE (a) = parent;\n+\t\tparent->regno_allocno_map[regno] = a;\n+\t\tbitmap_set_bit (parent->mentioned_allocnos, ALLOCNO_NUM (a));\n+\t      }\n+\t    else\n+\t      {\n+\t\t/* Remove the allocno and update info of allocno in\n+\t\t   the upper region.  */\n+\t\tif (prev_a == NULL)\n+\t\t  ira_regno_allocno_map[regno] = next_a;\n+\t\telse\n+\t\t  ALLOCNO_NEXT_REGNO_ALLOCNO (prev_a) = next_a;\n+\t\tr = ALLOCNO_LIVE_RANGES (a);\n+\t\tchange_allocno_in_range_list (r, parent_a);\n+\t\tALLOCNO_LIVE_RANGES (parent_a)\n+\t\t  = merge_ranges (r, ALLOCNO_LIVE_RANGES (parent_a));\n+\t\tmerged_p = true;\n+\t\tALLOCNO_LIVE_RANGES (a) = NULL;\n+\t\tIOR_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (parent_a),\n+\t\t\t\t  ALLOCNO_CONFLICT_HARD_REGS (a));\n+#ifdef STACK_REGS\n+\t\tif (ALLOCNO_NO_STACK_REG_P (a))\n+\t\t  ALLOCNO_NO_STACK_REG_P (parent_a) = true;\n+#endif\n+\t\tALLOCNO_NREFS (parent_a) += ALLOCNO_NREFS (a);\n+\t\tALLOCNO_FREQ (parent_a) += ALLOCNO_FREQ (a);\n+\t\tALLOCNO_CALL_FREQ (parent_a) += ALLOCNO_CALL_FREQ (a);\n+\t\tIOR_HARD_REG_SET\n+\t\t  (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (parent_a),\n+\t\t   ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a));\n+\t\tALLOCNO_CALLS_CROSSED_NUM (parent_a)\n+\t\t  += ALLOCNO_CALLS_CROSSED_NUM (a);\n+\t\tALLOCNO_EXCESS_PRESSURE_POINTS_NUM (parent_a)\n+\t\t  += ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (a);\n+#ifdef STACK_REGS\n+\t\tif (ALLOCNO_TOTAL_NO_STACK_REG_P (a))\n+\t\t  ALLOCNO_TOTAL_NO_STACK_REG_P (parent_a) = true;\n+#endif\n+\t\tcover_class = ALLOCNO_COVER_CLASS (a);\n+\t\tira_assert (cover_class == ALLOCNO_COVER_CLASS (parent_a));\n+\t\tira_allocate_and_accumulate_costs\n+\t\t  (&ALLOCNO_HARD_REG_COSTS (parent_a), cover_class,\n+\t\t   ALLOCNO_HARD_REG_COSTS (a));\n+\t\tira_allocate_and_accumulate_costs\n+\t\t  (&ALLOCNO_CONFLICT_HARD_REG_COSTS (parent_a),\n+\t\t   cover_class,\n+\t\t   ALLOCNO_CONFLICT_HARD_REG_COSTS (a));\n+\t\tALLOCNO_COVER_CLASS_COST (parent_a)\n+\t\t  += ALLOCNO_COVER_CLASS_COST (a);\n+\t\tALLOCNO_MEMORY_COST (parent_a) += ALLOCNO_MEMORY_COST (a);\n+\t\tALLOCNO_UPDATED_MEMORY_COST (parent_a)\n+\t\t  += ALLOCNO_UPDATED_MEMORY_COST (a);\n+\t\tfinish_allocno (a);\n+\t      }\n+\t  }\n+      }\n+  if (merged_p)\n+    ira_rebuild_start_finish_chains ();\n+}\n+\n+/* Remove loops from consideration.  We remove loops for which a\n+   separate allocation will not improve the result.  We have to do\n+   this after allocno creation and their costs and cover class\n+   evaluation because only after that the register pressure can be\n+   known and is calculated.  */\n+static void\n+remove_unnecessary_regions (void)\n+{\n+  children_vec\n+    = VEC_alloc (ira_loop_tree_node_t, heap,\n+\t\t last_basic_block + VEC_length (loop_p, ira_loops.larray));\n+  removed_loop_vec\n+    = VEC_alloc (ira_loop_tree_node_t, heap,\n+\t\t last_basic_block + VEC_length (loop_p, ira_loops.larray));\n+  remove_uneccesary_loop_nodes_from_loop_tree (ira_loop_tree_root) ;\n+  VEC_free (ira_loop_tree_node_t, heap, children_vec);\n+  remove_unnecessary_allocnos ();\n+  while (VEC_length (ira_loop_tree_node_t, removed_loop_vec) > 0)\n+    finish_loop_tree_node (VEC_pop (ira_loop_tree_node_t, removed_loop_vec));\n+  VEC_free (ira_loop_tree_node_t, heap, removed_loop_vec);\n+}\n+\n+\f\n+\n+/* Set up minimal and maximal live range points for allocnos.  */\n+static void\n+setup_min_max_allocno_live_range_point (void)\n+{\n+  int i;\n+  ira_allocno_t a, parent_a, cap;\n+  ira_allocno_iterator ai;\n+  allocno_live_range_t r;\n+  ira_loop_tree_node_t parent;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      r = ALLOCNO_LIVE_RANGES (a);\n+      if (r == NULL)\n+\tcontinue;\n+      ALLOCNO_MAX (a) = r->finish;\n+      for (; r->next != NULL; r = r->next)\n+\t;\n+      ALLOCNO_MIN (a) = r->start;\n+    }\n+  for (i = max_reg_num () - 1; i >= FIRST_PSEUDO_REGISTER; i--)\n+    for (a = ira_regno_allocno_map[i];\n+\t a != NULL;\n+\t a = ALLOCNO_NEXT_REGNO_ALLOCNO (a))\n+      {\n+\tif (ALLOCNO_MAX (a) < 0)\n+\t  continue;\n+\tira_assert (ALLOCNO_CAP_MEMBER (a) == NULL);\n+\t/* Accumulation of range info.  */\n+\tif (ALLOCNO_CAP (a) != NULL)\n+\t  {\n+\t    for (cap = ALLOCNO_CAP (a); cap != NULL; cap = ALLOCNO_CAP (cap))\n+\t      {\n+\t\tif (ALLOCNO_MAX (cap) < ALLOCNO_MAX (a))\n+\t\t  ALLOCNO_MAX (cap) = ALLOCNO_MAX (a);\n+\t\tif (ALLOCNO_MIN (cap) > ALLOCNO_MIN (a))\n+\t\t  ALLOCNO_MIN (cap) = ALLOCNO_MIN (a);\n+\t      }\n+\t    continue;\n+\t  }\n+\tif ((parent = ALLOCNO_LOOP_TREE_NODE (a)->parent) == NULL)\n+\t  continue;\n+\tparent_a = parent->regno_allocno_map[i];\n+\tif (ALLOCNO_MAX (parent_a) < ALLOCNO_MAX (a))\n+\t  ALLOCNO_MAX (parent_a) = ALLOCNO_MAX (a);\n+\tif (ALLOCNO_MIN (parent_a) > ALLOCNO_MIN (a))\n+\t  ALLOCNO_MIN (parent_a) = ALLOCNO_MIN (a);\n+      }\n+#ifdef ENABLE_IRA_CHECKING\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      if ((0 <= ALLOCNO_MIN (a) && ALLOCNO_MIN (a) <= ira_max_point)\n+\t  && (0 <= ALLOCNO_MAX (a) && ALLOCNO_MAX (a) <= ira_max_point))\n+\tcontinue;\n+      gcc_unreachable ();\n+    }\n+#endif\n+}\n+\n+/* Sort allocnos according to their live ranges.  Allocnos with\n+   smaller cover class are put first.  Allocnos with the same cove\n+   class are ordered according their start (min).  Allocnos with the\n+   same start are ordered according their finish (max).  */\n+static int\n+allocno_range_compare_func (const void *v1p, const void *v2p)\n+{\n+  int diff;\n+  ira_allocno_t a1 = *(const ira_allocno_t *) v1p;\n+  ira_allocno_t a2 = *(const ira_allocno_t *) v2p;\n+\n+  if ((diff = ALLOCNO_COVER_CLASS (a1) - ALLOCNO_COVER_CLASS (a2)) != 0)\n+    return diff;\n+  if ((diff = ALLOCNO_MIN (a1) - ALLOCNO_MIN (a2)) != 0)\n+    return diff;\n+  if ((diff = ALLOCNO_MAX (a1) - ALLOCNO_MAX (a2)) != 0)\n+     return diff;\n+  return ALLOCNO_NUM (a1) - ALLOCNO_NUM (a2);\n+}\n+\n+/* Sort ira_conflict_id_allocno_map and set up conflict id of\n+   allocnos.  */\n+static void\n+sort_conflict_id_allocno_map (void)\n+{\n+  int i, num;\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  num = 0;\n+  FOR_EACH_ALLOCNO (a, ai)\n+    ira_conflict_id_allocno_map[num++] = a;\n+  qsort (ira_conflict_id_allocno_map, num, sizeof (ira_allocno_t),\n+\t allocno_range_compare_func);\n+  for (i = 0; i < num; i++)\n+    if ((a = ira_conflict_id_allocno_map[i]) != NULL)\n+      ALLOCNO_CONFLICT_ID (a) = i;\n+  for (i = num; i < ira_allocnos_num; i++)\n+    ira_conflict_id_allocno_map[i] = NULL;\n+}\n+\n+/* Set up minimal and maximal conflict ids of allocnos with which\n+   given allocno can conflict.  */\n+static void\n+setup_min_max_conflict_allocno_ids (void)\n+{\n+  enum reg_class cover_class;\n+  int i, j, min, max, start, finish, first_not_finished, filled_area_start;\n+  int *live_range_min, *last_lived;\n+  ira_allocno_t a;\n+\n+  live_range_min = (int *) ira_allocate (sizeof (int) * ira_allocnos_num);\n+  cover_class = -1;\n+  first_not_finished = -1;\n+  for (i = 0; i < ira_allocnos_num; i++)\n+    {\n+      a = ira_conflict_id_allocno_map[i];\n+      if (a == NULL)\n+\tcontinue;\n+      if (cover_class != ALLOCNO_COVER_CLASS (a))\n+\t{\n+\t  cover_class = ALLOCNO_COVER_CLASS (a);\n+\t  min = i;\n+\t  first_not_finished = i;\n+\t}\n+      else\n+\t{\n+\t  start = ALLOCNO_MIN (a);\n+\t  /* If we skip an allocno, the allocno with smaller ids will\n+\t     be also skipped because of the secondary sorting the\n+\t     range finishes (see function\n+\t     allocno_range_compare_func).  */\n+\t  while (first_not_finished < i\n+\t\t && start > ALLOCNO_MAX (ira_conflict_id_allocno_map\n+\t\t\t\t\t [first_not_finished]))\n+\t    first_not_finished++;\n+\t  min = first_not_finished;\n+\t}\t  \n+      if (min == i)\n+\t/* We could increase min further in this case but it is good\n+\t   enough.  */\n+\tmin++;\n+      live_range_min[i] = ALLOCNO_MIN (a);\n+      ALLOCNO_MIN (a) = min;\n+    }\n+  last_lived = (int *) ira_allocate (sizeof (int) * ira_max_point);\n+  cover_class = -1;\n+  filled_area_start = -1;\n+  for (i = ira_allocnos_num - 1; i >= 0; i--)\n+    {\n+      a = ira_conflict_id_allocno_map[i];\n+      if (a == NULL)\n+\tcontinue;\n+      if (cover_class != ALLOCNO_COVER_CLASS (a))\n+\t{\n+\t  cover_class = ALLOCNO_COVER_CLASS (a);\n+\t  for (j = 0; j < ira_max_point; j++)\n+\t    last_lived[j] = -1;\n+\t  filled_area_start = ira_max_point;\n+\t}\n+      min = live_range_min[i];\n+      finish = ALLOCNO_MAX (a);\n+      max = last_lived[finish];\n+      if (max < 0)\n+\t/* We could decrease max further in this case but it is good\n+\t   enough.  */\n+\tmax = ALLOCNO_CONFLICT_ID (a) - 1;\n+      ALLOCNO_MAX (a) = max;\n+      /* In filling, we can go further A range finish to recognize\n+\t intersection quickly because if the finish of subsequently\n+\t processed allocno (it has smaller conflict id) range is\n+\t further A range finish than they are definitely intersected\n+\t (the reason for this is the allocnos with bigger conflict id\n+\t have their range starts not smaller than allocnos with\n+\t smaller ids.  */\n+      for (j = min; j < filled_area_start; j++)\n+\tlast_lived[j] = i;\n+      filled_area_start = min;\n+    }\n+  ira_free (last_lived);\n+  ira_free (live_range_min);\n+}\n+\n+\f\n+\n+static void\n+create_caps (void)\n+{\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+  ira_loop_tree_node_t loop_tree_node;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      if (ALLOCNO_LOOP_TREE_NODE (a) == ira_loop_tree_root)\n+\tcontinue;\n+      if (ALLOCNO_CAP_MEMBER (a) != NULL)\n+\tcreate_cap_allocno (a);\n+      else if (ALLOCNO_CAP (a) == NULL)\n+\t{\n+\t  loop_tree_node = ALLOCNO_LOOP_TREE_NODE (a);\n+\t  if (!bitmap_bit_p (loop_tree_node->border_allocnos, ALLOCNO_NUM (a)))\n+\t    create_cap_allocno (a);\n+\t}\n+    }\n+}\n+\n+\f\n+\n+/* The page contains code transforming more one region internal\n+   representation (IR) to one region IR which is necessary for reload.\n+   This transformation is called IR flattening.  We might just rebuild\n+   the IR for one region but we don't do it because it takes a lot of\n+   time.  */\n+\n+/* This recursive function returns immediate common dominator of two\n+   loop tree nodes N1 and N2.  */\n+static ira_loop_tree_node_t\n+common_loop_tree_node_dominator (ira_loop_tree_node_t n1,\n+\t\t\t\t ira_loop_tree_node_t n2)\n+{\n+  ira_assert (n1 != NULL && n2 != NULL);\n+  if (n1 == n2)\n+    return n1;\n+  if (n1->level < n2->level)\n+    return common_loop_tree_node_dominator (n1, n2->parent);\n+  else if (n1->level > n2->level)\n+    return common_loop_tree_node_dominator (n1->parent, n2);\n+  else\n+    return common_loop_tree_node_dominator (n1->parent, n2->parent);\n+}\n+\n+/* Flatten the IR.  In other words, this function transforms IR as if\n+   it were built with one region (without loops).  We could make it\n+   much simpler by rebuilding IR with one region, but unfortunately it\n+   takes a lot of time.  MAX_REGNO_BEFORE_EMIT and\n+   IRA_MAX_POINT_BEFORE_EMIT are correspondingly MAX_REG_NUM () and\n+   IRA_MAX_POINT before emitting insns on the loop borders.  */\n+void\n+ira_flattening (int max_regno_before_emit, int ira_max_point_before_emit)\n+{\n+  int i, j, num;\n+  bool propagate_p, stop_p, keep_p;\n+  int hard_regs_num;\n+  bool new_pseudos_p, merged_p;\n+  unsigned int n;\n+  enum reg_class cover_class;\n+  ira_allocno_t a, parent_a, first, second, node_first, node_second;\n+  ira_allocno_t dominator_a;\n+  ira_copy_t cp;\n+  ira_loop_tree_node_t parent, node, dominator;\n+  allocno_live_range_t r;\n+  ira_allocno_iterator ai;\n+  ira_copy_iterator ci;\n+  sparseset allocnos_live;\n+  /* Map: regno -> allocnos which will finally represent the regno for\n+     IR with one region.  */\n+  ira_allocno_t *regno_top_level_allocno_map;\n+  bool *allocno_propagated_p;\n+\n+  regno_top_level_allocno_map\n+    = (ira_allocno_t *) ira_allocate (max_reg_num () * sizeof (ira_allocno_t));\n+  memset (regno_top_level_allocno_map, 0,\n+\t  max_reg_num () * sizeof (ira_allocno_t));\n+  allocno_propagated_p\n+    = (bool *) ira_allocate (ira_allocnos_num * sizeof (bool));\n+  memset (allocno_propagated_p, 0, ira_allocnos_num * sizeof (bool));\n+  new_pseudos_p = merged_p = false;\n+  /* Fix final allocno attributes.  */\n+  for (i = max_regno_before_emit - 1; i >= FIRST_PSEUDO_REGISTER; i--)\n+    {\n+      propagate_p = false;\n+      for (a = ira_regno_allocno_map[i];\n+\t   a != NULL;\n+\t   a = ALLOCNO_NEXT_REGNO_ALLOCNO (a))\n+\t{\n+\t  ira_assert (ALLOCNO_CAP_MEMBER (a) == NULL);\n+\t  if (ALLOCNO_SOMEWHERE_RENAMED_P (a))\n+\t    new_pseudos_p = true;\n+\t  if (ALLOCNO_CAP (a) != NULL\n+\t      || (parent = ALLOCNO_LOOP_TREE_NODE (a)->parent) == NULL\n+\t      || ((parent_a = parent->regno_allocno_map[ALLOCNO_REGNO (a)])\n+\t\t  == NULL))\n+\t    {\n+\t      ALLOCNO_COPIES (a) = NULL;\n+\t      regno_top_level_allocno_map[REGNO (ALLOCNO_REG (a))] = a;\n+\t      continue;\n+\t    }\n+\t  ira_assert (ALLOCNO_CAP_MEMBER (parent_a) == NULL);\n+\t  if (propagate_p)\n+\t    {\n+\t      if (!allocno_propagated_p [ALLOCNO_NUM (parent_a)])\n+\t\tCOPY_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (parent_a),\n+\t\t\t\t   ALLOCNO_CONFLICT_HARD_REGS (parent_a));\n+\t      IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (parent_a),\n+\t\t\t\tALLOCNO_TOTAL_CONFLICT_HARD_REGS (a));\n+#ifdef STACK_REGS\n+\t      if (!allocno_propagated_p [ALLOCNO_NUM (parent_a)])\n+\t\tALLOCNO_TOTAL_NO_STACK_REG_P (parent_a)\n+\t\t  = ALLOCNO_NO_STACK_REG_P (parent_a);\n+\t      ALLOCNO_TOTAL_NO_STACK_REG_P (parent_a)\n+\t\t= (ALLOCNO_TOTAL_NO_STACK_REG_P (parent_a)\n+\t\t   || ALLOCNO_TOTAL_NO_STACK_REG_P (a));\n+#endif\n+\t      allocno_propagated_p [ALLOCNO_NUM (parent_a)] = true;\n+\t    }\n+\t  if (REGNO (ALLOCNO_REG (a)) == REGNO (ALLOCNO_REG (parent_a)))\n+\t    {\n+\t      if (internal_flag_ira_verbose > 4 && ira_dump_file != NULL)\n+\t\t{\n+\t\t  fprintf (ira_dump_file,\n+\t\t\t   \"      Moving ranges of a%dr%d to a%dr%d: \",\n+\t\t\t   ALLOCNO_NUM (a), REGNO (ALLOCNO_REG (a)),\n+\t\t\t   ALLOCNO_NUM (parent_a),\n+\t\t\t   REGNO (ALLOCNO_REG (parent_a)));\n+\t\t  ira_print_live_range_list (ira_dump_file,\n+\t\t\t\t\t     ALLOCNO_LIVE_RANGES (a));\n+\t\t}\n+\t      change_allocno_in_range_list (ALLOCNO_LIVE_RANGES (a), parent_a);\n+\t      ALLOCNO_LIVE_RANGES (parent_a)\n+\t\t= merge_ranges (ALLOCNO_LIVE_RANGES (a),\n+\t\t\t\tALLOCNO_LIVE_RANGES (parent_a));\n+\t      merged_p = true;\n+\t      ALLOCNO_LIVE_RANGES (a) = NULL;\n+\t      ALLOCNO_MEM_OPTIMIZED_DEST_P (parent_a)\n+\t\t= (ALLOCNO_MEM_OPTIMIZED_DEST_P (parent_a)\n+\t\t   || ALLOCNO_MEM_OPTIMIZED_DEST_P (a));\n+\t      continue;\n+\t    }\n+\t  new_pseudos_p = true;\n+\t  propagate_p = true;\n+\t  first = ALLOCNO_MEM_OPTIMIZED_DEST (a) == NULL ? NULL : a;\n+\t  stop_p = false;\n+\t  for (;;)\n+\t    {\n+\t      if (first == NULL\n+\t\t  && ALLOCNO_MEM_OPTIMIZED_DEST (parent_a) != NULL)\n+\t\tfirst = parent_a;\n+\t      ALLOCNO_NREFS (parent_a) -= ALLOCNO_NREFS (a);\n+\t      ALLOCNO_FREQ (parent_a) -= ALLOCNO_FREQ (a);\n+\t      if (first != NULL\n+\t\t  && ALLOCNO_MEM_OPTIMIZED_DEST (first) == parent_a)\n+\t\tstop_p = true;\n+\t      else if (!stop_p)\n+\t\t{\n+\t\t  ALLOCNO_CALL_FREQ (parent_a) -= ALLOCNO_CALL_FREQ (a);\n+\t\t  ALLOCNO_CALLS_CROSSED_NUM (parent_a)\n+\t\t    -= ALLOCNO_CALLS_CROSSED_NUM (a);\n+\t\t  ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (parent_a)\n+\t\t    -= ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (a);\n+\t\t}\n+\t      ira_assert (ALLOCNO_CALLS_CROSSED_NUM (parent_a) >= 0\n+\t\t\t  && ALLOCNO_NREFS (parent_a) >= 0\n+\t\t\t  && ALLOCNO_FREQ (parent_a) >= 0);\n+\t      cover_class = ALLOCNO_COVER_CLASS (parent_a);\n+\t      hard_regs_num = ira_class_hard_regs_num[cover_class];\n+\t      if (ALLOCNO_HARD_REG_COSTS (a) != NULL\n+\t\t  && ALLOCNO_HARD_REG_COSTS (parent_a) != NULL)\n+\t\tfor (j = 0; j < hard_regs_num; j++)\n+\t\t  ALLOCNO_HARD_REG_COSTS (parent_a)[j]\n+\t\t    -= ALLOCNO_HARD_REG_COSTS (a)[j];\n+\t      if (ALLOCNO_CONFLICT_HARD_REG_COSTS (a) != NULL\n+\t\t  && ALLOCNO_CONFLICT_HARD_REG_COSTS (parent_a) != NULL)\n+\t\tfor (j = 0; j < hard_regs_num; j++)\n+\t\t  ALLOCNO_CONFLICT_HARD_REG_COSTS (parent_a)[j]\n+\t\t    -= ALLOCNO_CONFLICT_HARD_REG_COSTS (a)[j];\n+\t      ALLOCNO_COVER_CLASS_COST (parent_a)\n+\t\t-= ALLOCNO_COVER_CLASS_COST (a);\n+\t      ALLOCNO_MEMORY_COST (parent_a) -= ALLOCNO_MEMORY_COST (a);\n+\t      if (ALLOCNO_CAP (parent_a) != NULL\n+\t\t  || (parent\n+\t\t      = ALLOCNO_LOOP_TREE_NODE (parent_a)->parent) == NULL\n+\t\t  || (parent_a = (parent->regno_allocno_map\n+\t\t\t\t  [ALLOCNO_REGNO (parent_a)])) == NULL)\n+\t\tbreak;\n+\t    }\n+\t  if (first != NULL)\n+\t    {\n+\t      parent_a = ALLOCNO_MEM_OPTIMIZED_DEST (first);\n+\t      dominator = common_loop_tree_node_dominator\n+\t\t          (ALLOCNO_LOOP_TREE_NODE (parent_a),\n+\t\t\t   ALLOCNO_LOOP_TREE_NODE (first));\n+\t      dominator_a = dominator->regno_allocno_map[ALLOCNO_REGNO (a)];\n+\t      ira_assert (parent_a != NULL);\n+\t      stop_p = first != a;\n+\t      /* Remember that exit can be to a grandparent (not only\n+\t\t to a parent) or a child of the grandparent.  */\n+\t      for (first = a;;)\n+\t\t{\n+\t\t  if (internal_flag_ira_verbose > 4 && ira_dump_file != NULL)\n+\t\t    {\n+\t\t      fprintf\n+\t\t\t(ira_dump_file,\n+\t\t\t \"      Coping ranges of a%dr%d to a%dr%d: \",\n+\t\t\t ALLOCNO_NUM (first), REGNO (ALLOCNO_REG (first)),\n+\t\t\t ALLOCNO_NUM (parent_a),\n+\t\t\t REGNO (ALLOCNO_REG (parent_a)));\n+\t\t      ira_print_live_range_list (ira_dump_file,\n+\t\t\t\t\t\t ALLOCNO_LIVE_RANGES (first));\n+\t\t    }\n+\t\t  r = copy_allocno_live_range_list (ALLOCNO_LIVE_RANGES\n+\t\t\t\t\t\t    (first));\n+\t\t  change_allocno_in_range_list (r, parent_a);\n+\t\t  ALLOCNO_LIVE_RANGES (parent_a)\n+\t\t    = merge_ranges (r, ALLOCNO_LIVE_RANGES (parent_a));\n+\t\t  merged_p = true;\n+\t\t  if (stop_p)\n+\t\t    break;\n+\t\t  parent = ALLOCNO_LOOP_TREE_NODE (first)->parent;\n+\t\t  ira_assert (parent != NULL);\n+\t\t  first = parent->regno_allocno_map[ALLOCNO_REGNO (a)];\n+\t\t  ira_assert (first != NULL);\n+\t\t  if (first == dominator_a)\n+\t\t    break;\n+\t\t}\n+\t    }\n+\t  ALLOCNO_COPIES (a) = NULL;\n+\t  regno_top_level_allocno_map[REGNO (ALLOCNO_REG (a))] = a;\n+\t}\n+    }\n+  ira_free (allocno_propagated_p);\n+  ira_assert (new_pseudos_p || ira_max_point_before_emit == ira_max_point);\n+  if (merged_p || ira_max_point_before_emit != ira_max_point)\n+    ira_rebuild_start_finish_chains ();\n+  if (new_pseudos_p)\n+    {\n+      /* Rebuild conflicts.  */\n+      FOR_EACH_ALLOCNO (a, ai)\n+\t{\n+\t  if (a != regno_top_level_allocno_map[REGNO (ALLOCNO_REG (a))]\n+\t      || ALLOCNO_CAP_MEMBER (a) != NULL)\n+\t    continue;\n+\t  for (r = ALLOCNO_LIVE_RANGES (a); r != NULL; r = r->next)\n+\t    ira_assert (r->allocno == a);\n+\t  clear_allocno_conflicts (a);\n+\t}\n+      allocnos_live = sparseset_alloc (ira_allocnos_num);\n+      for (i = 0; i < ira_max_point; i++)\n+\t{\n+\t  for (r = ira_start_point_ranges[i]; r != NULL; r = r->start_next)\n+\t    {\n+\t      a = r->allocno;\n+\t      if (a != regno_top_level_allocno_map[REGNO (ALLOCNO_REG (a))]\n+\t\t  || ALLOCNO_CAP_MEMBER (a) != NULL)\n+\t\tcontinue;\n+\t      num = ALLOCNO_NUM (a);\n+\t      cover_class = ALLOCNO_COVER_CLASS (a);\n+\t      sparseset_set_bit (allocnos_live, num);\n+\t      EXECUTE_IF_SET_IN_SPARSESET (allocnos_live, n)\n+\t\t{\n+\t\t  ira_allocno_t live_a = ira_allocnos[n];\n+\n+\t\t  if (cover_class == ALLOCNO_COVER_CLASS (live_a)\n+\t\t      /* Don't set up conflict for the allocno with itself.  */\n+\t\t      && num != (int) n)\n+\t\t    ira_add_allocno_conflict (a, live_a);\n+\t\t}\n+\t    }\n+\t  \n+\t  for (r = ira_finish_point_ranges[i]; r != NULL; r = r->finish_next)\n+\t    sparseset_clear_bit (allocnos_live, ALLOCNO_NUM (r->allocno));\n+\t}\n+      sparseset_free (allocnos_live);\n+      compress_conflict_vecs ();\n+    }\n+  /* Mark some copies for removing and change allocnos in the rest\n+     copies.  */\n+  FOR_EACH_COPY (cp, ci)\n+    {\n+      if (ALLOCNO_CAP_MEMBER (cp->first) != NULL\n+\t  || ALLOCNO_CAP_MEMBER (cp->second) != NULL)\n+\t{\n+\t  if (internal_flag_ira_verbose > 4 && ira_dump_file != NULL)\n+\t    fprintf\n+\t      (ira_dump_file, \"      Remove cp%d:%c%dr%d-%c%dr%d\\n\",\n+\t       cp->num, ALLOCNO_CAP_MEMBER (cp->first) != NULL ? 'c' : 'a',\n+\t       ALLOCNO_NUM (cp->first), REGNO (ALLOCNO_REG (cp->first)),\n+\t       ALLOCNO_CAP_MEMBER (cp->second) != NULL ? 'c' : 'a',\n+\t       ALLOCNO_NUM (cp->second), REGNO (ALLOCNO_REG (cp->second)));\n+\t  cp->loop_tree_node = NULL;\n+\t  continue;\n+\t}\n+      first = regno_top_level_allocno_map[REGNO (ALLOCNO_REG (cp->first))];\n+      second = regno_top_level_allocno_map[REGNO (ALLOCNO_REG (cp->second))];\n+      node = cp->loop_tree_node;\n+      if (node == NULL)\n+\tkeep_p = true; /* It copy generated in ira-emit.c.  */\n+      else\n+\t{\n+\t  /* Check that the copy was not propagated from level on\n+\t     which we will have different pseudos.  */\n+\t  node_first = node->regno_allocno_map[ALLOCNO_REGNO (cp->first)];\n+\t  node_second = node->regno_allocno_map[ALLOCNO_REGNO (cp->second)];\n+\t  keep_p = ((REGNO (ALLOCNO_REG (first))\n+\t\t     == REGNO (ALLOCNO_REG (node_first)))\n+\t\t     && (REGNO (ALLOCNO_REG (second))\n+\t\t\t == REGNO (ALLOCNO_REG (node_second))));\n+\t}\n+      if (keep_p)\n+\t{\n+\t  cp->loop_tree_node = ira_loop_tree_root;\n+\t  cp->first = first;\n+\t  cp->second = second;\n+\t}\n+      else\n+\t{\n+\t  cp->loop_tree_node = NULL;\n+\t  if (internal_flag_ira_verbose > 4 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file, \"      Remove cp%d:a%dr%d-a%dr%d\\n\",\n+\t\t     cp->num, ALLOCNO_NUM (cp->first),\n+\t\t     REGNO (ALLOCNO_REG (cp->first)), ALLOCNO_NUM (cp->second),\n+\t\t     REGNO (ALLOCNO_REG (cp->second)));\n+\t}\n+    }\n+  /* Remove unnecessary allocnos on lower levels of the loop tree.  */\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      if (a != regno_top_level_allocno_map[REGNO (ALLOCNO_REG (a))]\n+\t  || ALLOCNO_CAP_MEMBER (a) != NULL)\n+\t{\n+\t  if (internal_flag_ira_verbose > 4 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file, \"      Remove a%dr%d\\n\",\n+\t\t     ALLOCNO_NUM (a), REGNO (ALLOCNO_REG (a)));\n+\t  finish_allocno (a);\n+\t  continue;\n+\t}\n+      ALLOCNO_LOOP_TREE_NODE (a) = ira_loop_tree_root;\n+      ALLOCNO_REGNO (a) = REGNO (ALLOCNO_REG (a));\n+      ALLOCNO_CAP (a) = NULL;\n+      ALLOCNO_UPDATED_MEMORY_COST (a) = ALLOCNO_MEMORY_COST (a);\n+      if (! ALLOCNO_ASSIGNED_P (a))\n+\tira_free_allocno_updated_costs (a);\n+      ira_assert (ALLOCNO_UPDATED_HARD_REG_COSTS (a) == NULL);\n+      ira_assert (ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a) == NULL);\n+    }\n+  /* Remove unnecessary copies.  */\n+  FOR_EACH_COPY (cp, ci)\n+    {\n+      if (cp->loop_tree_node == NULL)\n+\t{\n+\t  ira_copies[cp->num] = NULL;\n+\t  finish_copy (cp);\n+\t  continue;\n+\t}\n+      ira_assert\n+\t(ALLOCNO_LOOP_TREE_NODE (cp->first) == ira_loop_tree_root\n+\t && ALLOCNO_LOOP_TREE_NODE (cp->second) == ira_loop_tree_root);\n+      ira_add_allocno_copy_to_list (cp);\n+      ira_swap_allocno_copy_ends_if_necessary (cp);\n+    }\n+  rebuild_regno_allocno_maps ();\n+  ira_free (regno_top_level_allocno_map);\n+}\n+\n+\f\n+\n+#ifdef ENABLE_IRA_CHECKING\n+/* Check creation of all allocnos.  Allocnos on lower levels should\n+   have allocnos or caps on all upper levels.  */\n+static void\n+check_allocno_creation (void)\n+{\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+  ira_loop_tree_node_t loop_tree_node;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      if (ALLOCNO_LOOP_TREE_NODE (a) == ira_loop_tree_root)\n+\tcontinue;\n+      if (ALLOCNO_CAP_MEMBER (a) != NULL)\n+\t{\n+\t  ira_assert (ALLOCNO_CAP (a) != NULL);\n+\t}\n+      else if (ALLOCNO_CAP (a) == NULL)\n+\t{\n+\t  loop_tree_node = ALLOCNO_LOOP_TREE_NODE (a);\n+\t  ira_assert (loop_tree_node->parent\n+\t\t      ->regno_allocno_map[ALLOCNO_REGNO (a)] != NULL\n+\t\t      && bitmap_bit_p (loop_tree_node->border_allocnos,\n+\t\t\t\t       ALLOCNO_NUM (a)));\n+\t}\n+    }\n+}\n+#endif\n+\n+/* Create a internal representation (IR) for IRA (allocnos, copies,\n+   loop tree nodes).  If LOOPS_P is FALSE the nodes corresponding to\n+   the loops (except the root which corresponds the all function) and\n+   correspondingly allocnos for the loops will be not created.  Such\n+   parameter value is used for Chaitin-Briggs coloring.  The function\n+   returns TRUE if we generate loop structure (besides nodes\n+   representing all function and the basic blocks) for regional\n+   allocation.  A true return means that we really need to flatten IR\n+   before the reload.  */\n+bool\n+ira_build (bool loops_p)\n+{\n+  df_analyze ();\n+\n+  initiate_cost_vectors ();\n+  initiate_allocnos ();\n+  initiate_copies ();\n+  create_loop_tree_nodes (loops_p);\n+  form_loop_tree ();\n+  create_allocnos ();\n+  ira_costs ();\n+  ira_create_allocno_live_ranges ();\n+  remove_unnecessary_regions ();\n+  loops_p = more_one_region_p ();\n+  if (loops_p)\n+    {\n+      propagate_allocno_info ();\n+      create_caps ();\n+    }\n+  ira_tune_allocno_costs_and_cover_classes ();\n+#ifdef ENABLE_IRA_CHECKING\n+  check_allocno_creation ();\n+#endif\n+  setup_min_max_allocno_live_range_point ();\n+  sort_conflict_id_allocno_map ();\n+  setup_min_max_conflict_allocno_ids ();\n+  ira_build_conflicts ();\n+  if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL)\n+    {\n+      int n, nr;\n+      ira_allocno_t a;\n+      allocno_live_range_t r;\n+      ira_allocno_iterator ai;\n+\n+      n = 0;\n+      FOR_EACH_ALLOCNO (a, ai)\n+\tn += ALLOCNO_CONFLICT_ALLOCNOS_NUM (a);\n+      nr = 0;\n+      FOR_EACH_ALLOCNO (a, ai)\n+\tfor (r = ALLOCNO_LIVE_RANGES (a); r != NULL; r = r->next)\n+\t  nr++;\n+      fprintf (ira_dump_file, \"  regions=%d, blocks=%d, points=%d\\n\",\n+\t       VEC_length (loop_p, ira_loops.larray), n_basic_blocks,\n+\t       ira_max_point);\n+      fprintf (ira_dump_file,\n+\t       \"    allocnos=%d, copies=%d, conflicts=%d, ranges=%d\\n\",\n+\t       ira_allocnos_num, ira_copies_num, n, nr);\n+    }\n+  return loops_p;\n+}\n+\n+/* Release the data created by function ira_build.  */\n+void\n+ira_destroy (void)\n+{\n+  finish_loop_tree_nodes ();\n+  finish_copies ();\n+  finish_allocnos ();\n+  finish_cost_vectors ();\n+  ira_finish_allocno_live_ranges ();\n+}"}, {"sha": "f3e4673ad6fdf6c6e7a3a085a238ae991d64b269", "filename": "gcc/ira-color.c", "status": "added", "additions": 2955, "deletions": 0, "changes": 2955, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-color.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-color.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira-color.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -0,0 +1,2955 @@\n+/* IRA allocation based on graph coloring.\n+   Copyright (C) 2006, 2007, 2008\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"target.h\"\n+#include \"regs.h\"\n+#include \"flags.h\"\n+#include \"sbitmap.h\"\n+#include \"bitmap.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"expr.h\"\n+#include \"toplev.h\"\n+#include \"reload.h\"\n+#include \"params.h\"\n+#include \"df.h\"\n+#include \"splay-tree.h\"\n+#include \"ira-int.h\"\n+\n+/* This file contains code for regional graph coloring, spill/restore\n+   code placement optimization, and code helping the reload pass to do\n+   a better job.  */\n+\n+/* Bitmap of allocnos which should be colored.  */\n+static bitmap coloring_allocno_bitmap;\n+\n+/* Bitmap of allocnos which should be taken into account during\n+   coloring.  In general case it contains allocnos from\n+   coloring_allocno_bitmap plus other already colored conflicting\n+   allocnos.  */\n+static bitmap consideration_allocno_bitmap;\n+\n+/* TRUE if we coalesced some allocnos.  In other words, if we got\n+   loops formed by members first_coalesced_allocno and\n+   next_coalesced_allocno containing more one allocno.  */\n+static bool allocno_coalesced_p;\n+\n+/* Bitmap used to prevent a repeated allocno processing because of\n+   coalescing.  */\n+static bitmap processed_coalesced_allocno_bitmap;\n+\n+/* All allocnos sorted according their priorities.  */\n+static ira_allocno_t *sorted_allocnos;\n+\n+/* Vec representing the stack of allocnos used during coloring.  */\n+static VEC(ira_allocno_t,heap) *allocno_stack_vec;\n+\n+/* Array used to choose an allocno for spilling.  */\n+static ira_allocno_t *allocnos_for_spilling;\n+\n+/* Pool for splay tree nodes.  */\n+static alloc_pool splay_tree_node_pool;\n+\n+/* When an allocno is removed from the splay tree, it is put in the\n+   following vector for subsequent inserting it into the splay tree\n+   after putting all colorable allocnos onto the stack.  The allocno\n+   could be removed from and inserted to the splay tree every time\n+   when its spilling priority is changed but such solution would be\n+   more costly although simpler.  */\n+static VEC(ira_allocno_t,heap) *removed_splay_allocno_vec;\n+\n+\f\n+\n+/* This page contains functions used to choose hard registers for\n+   allocnos.  */\n+\n+/* Array whose element value is TRUE if the corresponding hard\n+   register was already allocated for an allocno.  */\n+static bool allocated_hardreg_p[FIRST_PSEUDO_REGISTER];\n+\n+/* Array used to check already processed allocnos during the current\n+   update_copy_costs call.  */\n+static int *allocno_update_cost_check;\n+\n+/* The current value of update_copy_cost call count.  */\n+static int update_cost_check;\n+\n+/* Allocate and initialize data necessary for function\n+   update_copy_costs.  */\n+static void\n+initiate_cost_update (void)\n+{\n+  allocno_update_cost_check\n+    = (int *) ira_allocate (ira_allocnos_num * sizeof (int));\n+  memset (allocno_update_cost_check, 0, ira_allocnos_num * sizeof (int));\n+  update_cost_check = 0;\n+}\n+\n+/* Deallocate data used by function update_copy_costs.  */\n+static void\n+finish_cost_update (void)\n+{\n+  ira_free (allocno_update_cost_check);\n+}\n+\n+/* This recursive function updates costs (decrease if DECR_P) of the\n+   unassigned allocnos connected by copies with ALLOCNO.  This update\n+   increases chances to remove some copies.  Copy cost is proportional\n+   the copy frequency divided by DIVISOR.  */\n+static void\n+update_copy_costs_1 (ira_allocno_t allocno, int hard_regno,\n+\t\t     bool decr_p, int divisor)\n+{\n+  int i, cost, update_cost;\n+  enum machine_mode mode;\n+  enum reg_class rclass, cover_class;\n+  ira_allocno_t another_allocno;\n+  ira_copy_t cp, next_cp;\n+\n+  cover_class = ALLOCNO_COVER_CLASS (allocno);\n+  if (cover_class == NO_REGS)\n+    return;\n+  if (allocno_update_cost_check[ALLOCNO_NUM (allocno)] == update_cost_check)\n+    return;\n+  allocno_update_cost_check[ALLOCNO_NUM (allocno)] = update_cost_check;\n+  ira_assert (hard_regno >= 0);\n+  i = ira_class_hard_reg_index[cover_class][hard_regno];\n+  ira_assert (i >= 0);\n+  rclass = REGNO_REG_CLASS (hard_regno);\n+  mode = ALLOCNO_MODE (allocno);\n+  for (cp = ALLOCNO_COPIES (allocno); cp != NULL; cp = next_cp)\n+    {\n+      if (cp->first == allocno)\n+\t{\n+\t  next_cp = cp->next_first_allocno_copy;\n+\t  another_allocno = cp->second;\n+\t}\n+      else if (cp->second == allocno)\n+\t{\n+\t  next_cp = cp->next_second_allocno_copy;\n+\t  another_allocno = cp->first;\n+\t}\n+      else\n+\tgcc_unreachable ();\n+      if (cover_class\n+\t  != ALLOCNO_COVER_CLASS (another_allocno)\n+\t  || ALLOCNO_ASSIGNED_P (another_allocno))\n+\tcontinue;\n+      cost = (cp->second == allocno\n+\t      ? ira_register_move_cost[mode][rclass]\n+\t        [ALLOCNO_COVER_CLASS (another_allocno)]\n+\t      : ira_register_move_cost[mode]\n+\t        [ALLOCNO_COVER_CLASS (another_allocno)][rclass]);\n+      if (decr_p)\n+\tcost = -cost;\n+      ira_allocate_and_set_or_copy_costs\n+\t(&ALLOCNO_UPDATED_HARD_REG_COSTS (another_allocno), cover_class,\n+\t ALLOCNO_COVER_CLASS_COST (another_allocno),\n+\t ALLOCNO_HARD_REG_COSTS (another_allocno));\n+      ira_allocate_and_set_or_copy_costs\n+\t(&ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (another_allocno),\n+\t cover_class, 0,\n+\t ALLOCNO_CONFLICT_HARD_REG_COSTS (another_allocno));\n+      update_cost = cp->freq * cost / divisor;\n+      ALLOCNO_UPDATED_HARD_REG_COSTS (another_allocno)[i] += update_cost;\n+      ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (another_allocno)[i]\n+\t+= update_cost;\n+      if (update_cost != 0)\n+\tupdate_copy_costs_1 (another_allocno, hard_regno,\n+\t\t\t     decr_p, divisor * 4);\n+    }\n+}\n+\n+/* Update the cost of allocnos to increase chances to remove some\n+   copies as the result of subsequent assignment.  */\n+static void\n+update_copy_costs (ira_allocno_t allocno, bool decr_p)\n+{\n+  update_cost_check++;  \n+  update_copy_costs_1 (allocno, ALLOCNO_HARD_REGNO (allocno), decr_p, 1);\n+}\n+\n+/* Sort allocnos according to the profit of usage of a hard register\n+   instead of memory for them. */\n+static int\n+allocno_cost_compare_func (const void *v1p, const void *v2p)\n+{\n+  ira_allocno_t p1 = *(const ira_allocno_t *) v1p;\n+  ira_allocno_t p2 = *(const ira_allocno_t *) v2p;\n+  int c1, c2;\n+\n+  c1 = ALLOCNO_UPDATED_MEMORY_COST (p1) - ALLOCNO_COVER_CLASS_COST (p1);\n+  c2 = ALLOCNO_UPDATED_MEMORY_COST (p2) - ALLOCNO_COVER_CLASS_COST (p2);\n+  if (c1 - c2)\n+    return c1 - c2;\n+\n+  /* If regs are equally good, sort by allocno numbers, so that the\n+     results of qsort leave nothing to chance.  */\n+  return ALLOCNO_NUM (p1) - ALLOCNO_NUM (p2);\n+}\n+\n+/* Print all allocnos coalesced with ALLOCNO.  */\n+static void\n+print_coalesced_allocno (ira_allocno_t allocno)\n+{\n+  ira_allocno_t a;\n+\n+  for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+       a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+    {\n+      ira_print_expanded_allocno (a);\n+      if (a == allocno)\n+\tbreak;\n+      fprintf (ira_dump_file, \"+\");\n+    }\n+}\n+\n+/* Choose a hard register for ALLOCNO (or for all coalesced allocnos\n+   represented by ALLOCNO).  If RETRY_P is TRUE, it means that the\n+   function called from function `ira_reassign_conflict_allocnos' and\n+   `allocno_reload_assign'.  This function implements the optimistic\n+   coalescing too: if we failed to assign a hard register to set of\n+   the coalesced allocnos, we put them onto the coloring stack for\n+   subsequent separate assigning.  */\n+static bool\n+assign_hard_reg (ira_allocno_t allocno, bool retry_p)\n+{\n+  HARD_REG_SET conflicting_regs;\n+  int i, j, hard_regno, best_hard_regno, class_size;\n+  int cost, mem_cost, min_cost, full_cost, min_full_cost, add_cost;\n+  int *a_costs;\n+  int *conflict_costs;\n+  enum reg_class cover_class, rclass;\n+  enum machine_mode mode;\n+  ira_allocno_t a, conflict_allocno;\n+  ira_allocno_t another_allocno;\n+  ira_allocno_conflict_iterator aci;\n+  ira_copy_t cp, next_cp;\n+  static int costs[FIRST_PSEUDO_REGISTER], full_costs[FIRST_PSEUDO_REGISTER];\n+#ifdef STACK_REGS\n+  bool no_stack_reg_p;\n+#endif\n+\n+  ira_assert (! ALLOCNO_ASSIGNED_P (allocno));\n+  cover_class = ALLOCNO_COVER_CLASS (allocno);\n+  class_size = ira_class_hard_regs_num[cover_class];\n+  mode = ALLOCNO_MODE (allocno);\n+  CLEAR_HARD_REG_SET (conflicting_regs);\n+  best_hard_regno = -1;\n+  memset (full_costs, 0, sizeof (int) * class_size);\n+  mem_cost = 0;\n+  if (allocno_coalesced_p)\n+    bitmap_clear (processed_coalesced_allocno_bitmap);\n+  memset (costs, 0, sizeof (int) * class_size);\n+  memset (full_costs, 0, sizeof (int) * class_size);\n+#ifdef STACK_REGS\n+  no_stack_reg_p = false;\n+#endif\n+  for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+       a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+    {\n+      mem_cost += ALLOCNO_UPDATED_MEMORY_COST (a);\n+      IOR_HARD_REG_SET (conflicting_regs,\n+\t\t\tALLOCNO_TOTAL_CONFLICT_HARD_REGS (a));\n+      ira_allocate_and_copy_costs (&ALLOCNO_UPDATED_HARD_REG_COSTS (a),\n+\t\t\t\t   cover_class, ALLOCNO_HARD_REG_COSTS (a));\n+      a_costs = ALLOCNO_UPDATED_HARD_REG_COSTS (a);\n+#ifdef STACK_REGS\n+      no_stack_reg_p = no_stack_reg_p || ALLOCNO_TOTAL_NO_STACK_REG_P (a);\n+#endif\n+      for (cost = ALLOCNO_COVER_CLASS_COST (a), i = 0; i < class_size; i++)\n+\tif (a_costs != NULL)\n+\t  {\n+\t    costs[i] += a_costs[i];\n+\t    full_costs[i] += a_costs[i];\n+\t  }\n+\telse\n+\t  {\n+\t    costs[i] += cost;\n+\t    full_costs[i] += cost;\n+\t  }\n+      /* Take preferences of conflicting allocnos into account.  */\n+      FOR_EACH_ALLOCNO_CONFLICT (a, conflict_allocno, aci)\n+\t/* Reload can give another class so we need to check all\n+\t   allocnos.  */\n+\tif (retry_p || bitmap_bit_p (consideration_allocno_bitmap,\n+\t\t\t\t     ALLOCNO_NUM (conflict_allocno)))\n+\t  {\n+\t    ira_assert (cover_class == ALLOCNO_COVER_CLASS (conflict_allocno));\n+\t    if (allocno_coalesced_p)\n+\t      {\n+\t\tif (bitmap_bit_p (processed_coalesced_allocno_bitmap,\n+\t\t\t\t  ALLOCNO_NUM (conflict_allocno)))\n+\t\t  continue;\n+\t\tbitmap_set_bit (processed_coalesced_allocno_bitmap,\n+\t\t\t\tALLOCNO_NUM (conflict_allocno));\n+\t      }\n+\t    if (ALLOCNO_ASSIGNED_P (conflict_allocno))\n+\t      {\n+\t\tif ((hard_regno = ALLOCNO_HARD_REGNO (conflict_allocno)) >= 0)\n+\t\t  {\n+\t\t    IOR_HARD_REG_SET\n+\t\t      (conflicting_regs,\n+\t\t       ira_reg_mode_hard_regset\n+\t\t       [hard_regno][ALLOCNO_MODE (conflict_allocno)]);\n+\t\t    if (hard_reg_set_subset_p (reg_class_contents[cover_class],\n+\t\t\t\t\t       conflicting_regs))\n+\t\t      goto fail;\n+\t\t  }\n+\t\tcontinue;\n+\t      }\n+\t    else if (! ALLOCNO_MAY_BE_SPILLED_P (conflict_allocno))\n+\t      {\n+\t\tira_allocate_and_copy_costs\n+\t\t  (&ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (conflict_allocno),\n+\t\t   cover_class,\n+\t\t   ALLOCNO_CONFLICT_HARD_REG_COSTS (conflict_allocno));\n+\t\tconflict_costs\n+\t\t  = ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (conflict_allocno);\n+\t\tif (conflict_costs != NULL)\n+\t\t  for (j = class_size - 1; j >= 0; j--)\n+\t\t    full_costs[j] -= conflict_costs[j];\n+\t      }\n+\t  }\n+      if (a == allocno)\n+\tbreak;\n+    }\n+  /* Take copies into account.  */\n+  for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+       a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+    {\n+      for (cp = ALLOCNO_COPIES (a); cp != NULL; cp = next_cp)\n+\t{\n+\t  if (cp->first == a)\n+\t    {\n+\t      next_cp = cp->next_first_allocno_copy;\n+\t      another_allocno = cp->second;\n+\t    }\n+\t  else if (cp->second == a)\n+\t    {\n+\t      next_cp = cp->next_second_allocno_copy;\n+\t      another_allocno = cp->first;\n+\t    }\n+\t  else\n+\t    gcc_unreachable ();\n+\t  if (cover_class != ALLOCNO_COVER_CLASS (another_allocno)\n+\t      || ALLOCNO_ASSIGNED_P (another_allocno))\n+\t    continue;\n+\t  ira_allocate_and_copy_costs\n+\t    (&ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (another_allocno),\n+\t     cover_class, ALLOCNO_CONFLICT_HARD_REG_COSTS (another_allocno));\n+\t  conflict_costs\n+\t    = ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (another_allocno);\n+\t  if (conflict_costs != NULL\n+\t      && ! ALLOCNO_MAY_BE_SPILLED_P (another_allocno))\n+\t    for (j = class_size - 1; j >= 0; j--)\n+\t      full_costs[j] += conflict_costs[j];\n+\t}\n+      if (a == allocno)\n+\tbreak;\n+    }\n+  min_cost = min_full_cost = INT_MAX;\n+  /* We don't care about giving callee saved registers to allocnos no\n+     living through calls because call clobbered registers are\n+     allocated first (it is usual practice to put them first in\n+     REG_ALLOC_ORDER).  */\n+  for (i = 0; i < class_size; i++)\n+    {\n+      hard_regno = ira_class_hard_regs[cover_class][i];\n+#ifdef STACK_REGS\n+      if (no_stack_reg_p\n+\t  && FIRST_STACK_REG <= hard_regno && hard_regno <= LAST_STACK_REG)\n+\tcontinue;\n+#endif\n+      if (! ira_hard_reg_not_in_set_p (hard_regno, mode, conflicting_regs)\n+\t  || TEST_HARD_REG_BIT (prohibited_class_mode_regs[cover_class][mode],\n+\t\t\t\thard_regno))\n+\tcontinue;\n+      cost = costs[i];\n+      full_cost = full_costs[i];\n+      if (! allocated_hardreg_p[hard_regno]\n+\t  && ira_hard_reg_not_in_set_p (hard_regno, mode, call_used_reg_set))\n+\t/* We need to save/restore the hard register in\n+\t   epilogue/prologue.  Therefore we increase the cost.  */\n+\t{\n+\t  /* ??? If only part is call clobbered.  */\n+\t  rclass = REGNO_REG_CLASS (hard_regno);\n+\t  add_cost = (ira_memory_move_cost[mode][rclass][0]\n+\t\t      + ira_memory_move_cost[mode][rclass][1] - 1);\n+\t  cost += add_cost;\n+\t  full_cost += add_cost;\n+\t}\n+      if (min_cost > cost)\n+\tmin_cost = cost;\n+      if (min_full_cost > full_cost)\n+\t{\n+\t  min_full_cost = full_cost;\n+\t  best_hard_regno = hard_regno;\n+\t  ira_assert (hard_regno >= 0);\n+\t}\n+    }\n+  if (min_full_cost > mem_cost)\n+    {\n+      if (! retry_p && internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\tfprintf (ira_dump_file, \"(memory is more profitable %d vs %d) \",\n+\t\t mem_cost, min_full_cost);\n+      best_hard_regno = -1;\n+    }\n+ fail:\n+  if (best_hard_regno < 0\n+      && ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno) != allocno)\n+    {\n+      for (j = 0, a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+\t   a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+\t{\n+\t  sorted_allocnos[j++] = a;\n+\t  if (a == allocno)\n+\t    break;\n+\t}\n+      qsort (sorted_allocnos, j, sizeof (ira_allocno_t), \n+\t     allocno_cost_compare_func);\n+      for (i = 0; i < j; i++)\n+\t{\n+\t  a = sorted_allocnos[i];\n+\t  ALLOCNO_FIRST_COALESCED_ALLOCNO (a) = a;\n+\t  ALLOCNO_NEXT_COALESCED_ALLOCNO (a) = a;\n+\t  VEC_safe_push (ira_allocno_t, heap, allocno_stack_vec, a);\n+\t  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t    {\n+\t      fprintf (ira_dump_file, \"        Pushing\");\n+\t      print_coalesced_allocno (a);\n+\t      fprintf (ira_dump_file, \"\\n\");\n+\t    }\n+\t}\n+      return false;\n+    }\n+  if (best_hard_regno >= 0)\n+    allocated_hardreg_p[best_hard_regno] = true;\n+  for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+       a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+    {\n+      ALLOCNO_HARD_REGNO (a) = best_hard_regno;\n+      ALLOCNO_ASSIGNED_P (a) = true;\n+      if (best_hard_regno >= 0)\n+\tupdate_copy_costs (a, true);\n+      ira_assert (ALLOCNO_COVER_CLASS (a) == cover_class);\n+      /* We don't need updated costs anymore: */\n+      ira_free_allocno_updated_costs (a);\n+      if (a == allocno)\n+\tbreak;\n+    }\n+  return best_hard_regno >= 0;\n+}\n+\n+\f\n+\n+/* This page contains the allocator based on the Chaitin-Briggs algorithm.  */\n+\n+/* Bucket of allocnos that can colored currently without spilling.  */\n+static ira_allocno_t colorable_allocno_bucket;\n+\n+/* Bucket of allocnos that might be not colored currently without\n+   spilling.  */\n+static ira_allocno_t uncolorable_allocno_bucket;\n+\n+/* Each element of the array contains the current number of allocnos\n+   of given *cover* class in the uncolorable_bucket.  */\n+static int uncolorable_allocnos_num[N_REG_CLASSES];\n+\n+/* Add ALLOCNO to bucket *BUCKET_PTR.  ALLOCNO should be not in a bucket\n+   before the call.  */\n+static void\n+add_ira_allocno_to_bucket (ira_allocno_t allocno, ira_allocno_t *bucket_ptr)\n+{\n+  ira_allocno_t first_allocno;\n+  enum reg_class cover_class;\n+\n+  if (bucket_ptr == &uncolorable_allocno_bucket\n+      && (cover_class = ALLOCNO_COVER_CLASS (allocno)) != NO_REGS)\n+    {\n+      uncolorable_allocnos_num[cover_class]++;\n+      ira_assert (uncolorable_allocnos_num[cover_class] > 0);\n+    }\n+  first_allocno = *bucket_ptr;\n+  ALLOCNO_NEXT_BUCKET_ALLOCNO (allocno) = first_allocno;\n+  ALLOCNO_PREV_BUCKET_ALLOCNO (allocno) = NULL;\n+  if (first_allocno != NULL)\n+    ALLOCNO_PREV_BUCKET_ALLOCNO (first_allocno) = allocno;\n+  *bucket_ptr = allocno;\n+}\n+\n+/* The function returns frequency and number of available hard\n+   registers for allocnos coalesced with ALLOCNO.  */\n+static void\n+get_coalesced_allocnos_attributes (ira_allocno_t allocno, int *freq, int *num)\n+{\n+  ira_allocno_t a;\n+\n+  *freq = 0;\n+  *num = 0;\n+  for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+       a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+    {\n+      *freq += ALLOCNO_FREQ (a);\n+      *num += ALLOCNO_AVAILABLE_REGS_NUM (a);\n+      if (a == allocno)\n+\tbreak;\n+    }\n+}\n+\n+/* Compare two allocnos to define which allocno should be pushed first\n+   into the coloring stack.  If the return is a negative number, the\n+   allocno given by the first parameter will be pushed first.  In this\n+   case such allocno has less priority than the second one and the\n+   hard register will be assigned to it after assignment to the second\n+   one.  As the result of such assignment order, the second allocno\n+   has a better chance to get the best hard register.  */\n+static int\n+bucket_allocno_compare_func (const void *v1p, const void *v2p)\n+{\n+  ira_allocno_t a1 = *(const ira_allocno_t *) v1p;\n+  ira_allocno_t a2 = *(const ira_allocno_t *) v2p;\n+  int diff, a1_freq, a2_freq, a1_num, a2_num;\n+\n+  if ((diff = (int) ALLOCNO_COVER_CLASS (a2) - ALLOCNO_COVER_CLASS (a1)) != 0)\n+    return diff;\n+  get_coalesced_allocnos_attributes (a1, &a1_freq, &a1_num);\n+  get_coalesced_allocnos_attributes (a2, &a2_freq, &a2_num);\n+  if ((diff = a2_num - a1_num) != 0)\n+    return diff;\n+  else if ((diff = a1_freq - a2_freq) != 0)\n+    return diff;\n+  return ALLOCNO_NUM (a2) - ALLOCNO_NUM (a1);\n+}\n+\n+/* Sort bucket *BUCKET_PTR and return the result through\n+   BUCKET_PTR.  */\n+static void\n+sort_bucket (ira_allocno_t *bucket_ptr)\n+{\n+  ira_allocno_t a, head;\n+  int n;\n+\n+  for (n = 0, a = *bucket_ptr; a != NULL; a = ALLOCNO_NEXT_BUCKET_ALLOCNO (a))\n+    sorted_allocnos[n++] = a;\n+  if (n <= 1)\n+    return;\n+  qsort (sorted_allocnos, n, sizeof (ira_allocno_t),\n+\t bucket_allocno_compare_func);\n+  head = NULL;\n+  for (n--; n >= 0; n--)\n+    {\n+      a = sorted_allocnos[n];\n+      ALLOCNO_NEXT_BUCKET_ALLOCNO (a) = head;\n+      ALLOCNO_PREV_BUCKET_ALLOCNO (a) = NULL;\n+      if (head != NULL)\n+\tALLOCNO_PREV_BUCKET_ALLOCNO (head) = a;\n+      head = a;\n+    }\n+  *bucket_ptr = head;\n+}\n+\n+/* Add ALLOCNO to bucket *BUCKET_PTR maintaining the order according\n+   their priority.  ALLOCNO should be not in a bucket before the\n+   call.  */\n+static void\n+add_ira_allocno_to_ordered_bucket (ira_allocno_t allocno,\n+\t\t\t\t   ira_allocno_t *bucket_ptr)\n+{\n+  ira_allocno_t before, after;\n+  enum reg_class cover_class;\n+\n+  if (bucket_ptr == &uncolorable_allocno_bucket\n+      && (cover_class = ALLOCNO_COVER_CLASS (allocno)) != NO_REGS)\n+    {\n+      uncolorable_allocnos_num[cover_class]++;\n+      ira_assert (uncolorable_allocnos_num[cover_class] > 0);\n+    }\n+  for (before = *bucket_ptr, after = NULL;\n+       before != NULL;\n+       after = before, before = ALLOCNO_NEXT_BUCKET_ALLOCNO (before))\n+    if (bucket_allocno_compare_func (&allocno, &before) < 0)\n+      break;\n+  ALLOCNO_NEXT_BUCKET_ALLOCNO (allocno) = before;\n+  ALLOCNO_PREV_BUCKET_ALLOCNO (allocno) = after;\n+  if (after == NULL)\n+    *bucket_ptr = allocno;\n+  else\n+    ALLOCNO_NEXT_BUCKET_ALLOCNO (after) = allocno;\n+  if (before != NULL)\n+    ALLOCNO_PREV_BUCKET_ALLOCNO (before) = allocno;\n+}\n+\n+/* Delete ALLOCNO from bucket *BUCKET_PTR.  It should be there before\n+   the call.  */\n+static void\n+delete_allocno_from_bucket (ira_allocno_t allocno, ira_allocno_t *bucket_ptr)\n+{\n+  ira_allocno_t prev_allocno, next_allocno;\n+  enum reg_class cover_class;\n+\n+  if (bucket_ptr == &uncolorable_allocno_bucket\n+      && (cover_class = ALLOCNO_COVER_CLASS (allocno)) != NO_REGS)\n+    {\n+      uncolorable_allocnos_num[cover_class]--;\n+      ira_assert (uncolorable_allocnos_num[cover_class] >= 0);\n+    }\n+  prev_allocno = ALLOCNO_PREV_BUCKET_ALLOCNO (allocno);\n+  next_allocno = ALLOCNO_NEXT_BUCKET_ALLOCNO (allocno);\n+  if (prev_allocno != NULL)\n+    ALLOCNO_NEXT_BUCKET_ALLOCNO (prev_allocno) = next_allocno;\n+  else\n+    {\n+      ira_assert (*bucket_ptr == allocno);\n+      *bucket_ptr = next_allocno;\n+    }\n+  if (next_allocno != NULL)\n+    ALLOCNO_PREV_BUCKET_ALLOCNO (next_allocno) = prev_allocno;\n+}\n+\n+/* Splay tree for each cover class.  The trees are indexed by the\n+   corresponding cover classes.  Splay trees contain uncolorable\n+   allocnos.  */\n+static splay_tree uncolorable_allocnos_splay_tree[N_REG_CLASSES];\n+\n+/* If the following macro is TRUE, splay tree is used to choose an\n+   allocno of the corresponding cover class for spilling.  When the\n+   number uncolorable allocnos of given cover class decreases to some\n+   threshold, linear array search is used to find the best allocno for\n+   spilling.  This threshold is actually pretty big because, although\n+   splay trees asymptotically is much faster, each splay tree\n+   operation is sufficiently costly especially taking cache locality\n+   into account.  */\n+#define USE_SPLAY_P(CLASS) (uncolorable_allocnos_num[CLASS] > 4000)\n+\n+/* Put ALLOCNO onto the coloring stack without removing it from its\n+   bucket.  Pushing allocno to the coloring stack can result in moving\n+   conflicting allocnos from the uncolorable bucket to the colorable\n+   one.  */\n+static void\n+push_ira_allocno_to_stack (ira_allocno_t allocno)\n+{\n+  int conflicts_num, conflict_size, size;\n+  ira_allocno_t a, conflict_allocno;\n+  enum reg_class cover_class;\n+  ira_allocno_conflict_iterator aci;\n+  \n+  ALLOCNO_IN_GRAPH_P (allocno) = false;\n+  VEC_safe_push (ira_allocno_t, heap, allocno_stack_vec, allocno);\n+  cover_class = ALLOCNO_COVER_CLASS (allocno);\n+  if (cover_class == NO_REGS)\n+    return;\n+  size = ira_reg_class_nregs[cover_class][ALLOCNO_MODE (allocno)];\n+  if (allocno_coalesced_p)\n+    bitmap_clear (processed_coalesced_allocno_bitmap);\n+  for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+       a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+    {\n+      FOR_EACH_ALLOCNO_CONFLICT (a, conflict_allocno, aci)\n+\tif (bitmap_bit_p (coloring_allocno_bitmap,\n+\t\t\t  ALLOCNO_NUM (conflict_allocno)))\n+\t  {\n+\t    ira_assert (cover_class == ALLOCNO_COVER_CLASS (conflict_allocno));\n+\t    if (allocno_coalesced_p)\n+\t      {\n+\t\tif (bitmap_bit_p (processed_coalesced_allocno_bitmap,\n+\t\t\t\t  ALLOCNO_NUM (conflict_allocno)))\n+\t\t  continue;\n+\t\tbitmap_set_bit (processed_coalesced_allocno_bitmap,\n+\t\t\t\tALLOCNO_NUM (conflict_allocno));\n+\t      }\n+\t    if (ALLOCNO_IN_GRAPH_P (conflict_allocno)\n+\t\t&& ! ALLOCNO_ASSIGNED_P (conflict_allocno))\n+\t      {\n+\t\tconflicts_num = ALLOCNO_LEFT_CONFLICTS_NUM (conflict_allocno);\n+\t\tconflict_size\n+\t\t  = (ira_reg_class_nregs\n+\t\t     [cover_class][ALLOCNO_MODE (conflict_allocno)]);\n+\t\tira_assert\n+\t\t  (ALLOCNO_LEFT_CONFLICTS_NUM (conflict_allocno) >= size);\n+\t\tif (conflicts_num + conflict_size\n+\t\t    <= ALLOCNO_AVAILABLE_REGS_NUM (conflict_allocno))\n+\t\t  {\n+\t\t    ALLOCNO_LEFT_CONFLICTS_NUM (conflict_allocno) -= size;\n+\t\t    continue;\n+\t\t  }\n+\t\tconflicts_num\n+\t\t  = ALLOCNO_LEFT_CONFLICTS_NUM (conflict_allocno) - size;\n+\t\tif (uncolorable_allocnos_splay_tree[cover_class] != NULL\n+\t\t    && !ALLOCNO_SPLAY_REMOVED_P (conflict_allocno)\n+\t\t    && USE_SPLAY_P (cover_class))\n+\t\t  {\n+\t\t    ira_assert\n+\t\t      (splay_tree_lookup\n+\t\t       (uncolorable_allocnos_splay_tree[cover_class],\n+\t\t\t(splay_tree_key) conflict_allocno) != NULL);\n+\t\t    splay_tree_remove\n+\t\t      (uncolorable_allocnos_splay_tree[cover_class],\n+\t\t       (splay_tree_key) conflict_allocno);\n+\t\t    ALLOCNO_SPLAY_REMOVED_P (conflict_allocno) = true;\n+\t\t    VEC_safe_push (ira_allocno_t, heap,\n+\t\t\t\t   removed_splay_allocno_vec,\n+\t\t\t\t   conflict_allocno);\n+\t\t  }\n+\t\tALLOCNO_LEFT_CONFLICTS_NUM (conflict_allocno) = conflicts_num;\n+\t\tif (conflicts_num + conflict_size\n+\t\t    <= ALLOCNO_AVAILABLE_REGS_NUM (conflict_allocno))\n+\t\t  {\n+\t\t    delete_allocno_from_bucket (conflict_allocno,\n+\t\t\t\t\t\t&uncolorable_allocno_bucket);\n+\t\t    add_ira_allocno_to_ordered_bucket (conflict_allocno,\n+\t\t\t\t\t\t   &colorable_allocno_bucket);\n+\t\t  }\n+\t      }\n+\t  }\n+      if (a == allocno)\n+\tbreak;\n+    }\n+}\n+\n+/* Put ALLOCNO onto the coloring stack and remove it from its bucket.\n+   The allocno is in the colorable bucket if COLORABLE_P is TRUE.  */\n+static void\n+remove_allocno_from_bucket_and_push (ira_allocno_t allocno, bool colorable_p)\n+{\n+  enum reg_class cover_class;\n+\n+  if (colorable_p)\n+    delete_allocno_from_bucket (allocno, &colorable_allocno_bucket);\n+  else\n+    delete_allocno_from_bucket (allocno, &uncolorable_allocno_bucket);\n+  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+    {\n+      fprintf (ira_dump_file, \"      Pushing\");\n+      print_coalesced_allocno (allocno);\n+      fprintf (ira_dump_file, \"%s\\n\", colorable_p ? \"\" : \"(potential spill)\");\n+    }\n+  cover_class = ALLOCNO_COVER_CLASS (allocno);\n+  ira_assert ((colorable_p\n+\t       && (ALLOCNO_LEFT_CONFLICTS_NUM (allocno)\n+\t\t   + ira_reg_class_nregs[cover_class][ALLOCNO_MODE (allocno)]\n+\t\t   <= ALLOCNO_AVAILABLE_REGS_NUM (allocno)))\n+\t      || (! colorable_p\n+\t\t  && (ALLOCNO_LEFT_CONFLICTS_NUM (allocno)\n+\t\t      + ira_reg_class_nregs[cover_class][ALLOCNO_MODE\n+\t\t\t\t\t\t\t (allocno)]\n+\t\t      > ALLOCNO_AVAILABLE_REGS_NUM (allocno))));\n+  if (! colorable_p)\n+    ALLOCNO_MAY_BE_SPILLED_P (allocno) = true;\n+  push_ira_allocno_to_stack (allocno);\n+}\n+\n+/* Put all allocnos from colorable bucket onto the coloring stack.  */\n+static void\n+push_only_colorable (void)\n+{\n+  sort_bucket (&colorable_allocno_bucket);\n+  for (;colorable_allocno_bucket != NULL;)\n+    remove_allocno_from_bucket_and_push (colorable_allocno_bucket, true);\n+}\n+\n+/* Puts ALLOCNO chosen for potential spilling onto the coloring\n+   stack.  */\n+static void\n+push_ira_allocno_to_spill (ira_allocno_t allocno)\n+{\n+  delete_allocno_from_bucket (allocno, &uncolorable_allocno_bucket);\n+  ALLOCNO_MAY_BE_SPILLED_P (allocno) = true;\n+  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+    fprintf (ira_dump_file, \"      Pushing p%d(%d) (potential spill)\\n\",\n+\t     ALLOCNO_NUM (allocno), ALLOCNO_REGNO (allocno));\n+  push_ira_allocno_to_stack (allocno);\n+}\n+\n+/* Return the frequency of exit edges (if EXIT_P) or entry from/to the\n+   loop given by its LOOP_NODE.  */ \n+int\n+ira_loop_edge_freq (ira_loop_tree_node_t loop_node, int regno, bool exit_p)\n+{\n+  int freq, i;\n+  edge_iterator ei;\n+  edge e;\n+  VEC (edge, heap) *edges;\n+\n+  ira_assert (loop_node->loop != NULL\n+\t      && (regno < 0 || regno >= FIRST_PSEUDO_REGISTER));\n+  freq = 0;\n+  if (! exit_p)\n+    {\n+      FOR_EACH_EDGE (e, ei, loop_node->loop->header->preds)\n+\tif (e->src != loop_node->loop->latch\n+\t    && (regno < 0\n+\t\t|| (bitmap_bit_p (DF_LR_OUT (e->src), regno)\n+\t\t    && bitmap_bit_p (DF_LR_IN (e->dest), regno))))\n+\t  freq += EDGE_FREQUENCY (e);\n+    }\n+  else\n+    {\n+      edges = get_loop_exit_edges (loop_node->loop);\n+      for (i = 0; VEC_iterate (edge, edges, i, e); i++)\n+\tif (regno < 0\n+\t    || (bitmap_bit_p (DF_LR_OUT (e->src), regno)\n+\t\t&& bitmap_bit_p (DF_LR_IN (e->dest), regno)))\n+\t  freq += EDGE_FREQUENCY (e);\n+      VEC_free (edge, heap, edges);\n+    }\n+\n+  return REG_FREQ_FROM_EDGE_FREQ (freq);\n+}\n+\n+/* Calculate and return the cost of putting allocno A into memory.  */\n+static int\n+calculate_allocno_spill_cost (ira_allocno_t a)\n+{\n+  int regno, cost;\n+  enum machine_mode mode;\n+  enum reg_class rclass;\n+  ira_allocno_t parent_allocno;\n+  ira_loop_tree_node_t parent_node, loop_node;\n+\n+  regno = ALLOCNO_REGNO (a);\n+  cost = ALLOCNO_UPDATED_MEMORY_COST (a) - ALLOCNO_COVER_CLASS_COST (a);\n+  if (ALLOCNO_CAP (a) != NULL)\n+    return cost;\n+  loop_node = ALLOCNO_LOOP_TREE_NODE (a);\n+  if ((parent_node = loop_node->parent) == NULL)\n+    return cost;\n+  if ((parent_allocno = parent_node->regno_allocno_map[regno]) == NULL)\n+    return cost;\n+  mode = ALLOCNO_MODE (a);\n+  rclass = ALLOCNO_COVER_CLASS (a);\n+  if (ALLOCNO_HARD_REGNO (parent_allocno) < 0)\n+    cost -= (ira_memory_move_cost[mode][rclass][0]\n+\t     * ira_loop_edge_freq (loop_node, regno, true)\n+\t     + ira_memory_move_cost[mode][rclass][1]\n+\t     * ira_loop_edge_freq (loop_node, regno, false));\n+  else\n+    cost += ((ira_memory_move_cost[mode][rclass][1]\n+\t      * ira_loop_edge_freq (loop_node, regno, true)\n+\t      + ira_memory_move_cost[mode][rclass][0]\n+\t      * ira_loop_edge_freq (loop_node, regno, false))\n+\t     - (ira_register_move_cost[mode][rclass][rclass]\n+\t\t* (ira_loop_edge_freq (loop_node, regno, false)\n+\t\t   + ira_loop_edge_freq (loop_node, regno, true))));\n+  return cost;\n+}\n+\n+/* Compare keys in the splay tree used to choose best allocno for\n+   spilling.  The best allocno has the minimal key.  */\n+static int\n+allocno_spill_priority_compare (splay_tree_key k1, splay_tree_key k2)\n+{\n+  int pri1, pri2, diff;\n+  ira_allocno_t a1 = (ira_allocno_t) k1, a2 = (ira_allocno_t) k2;\n+  \n+  pri1 = (IRA_ALLOCNO_TEMP (a1)\n+\t  / (ALLOCNO_LEFT_CONFLICTS_NUM (a1)\n+\t     * ira_reg_class_nregs[ALLOCNO_COVER_CLASS (a1)][ALLOCNO_MODE (a1)]\n+\t     + 1));\n+  pri2 = (IRA_ALLOCNO_TEMP (a2)\n+\t  / (ALLOCNO_LEFT_CONFLICTS_NUM (a2)\n+\t     * ira_reg_class_nregs[ALLOCNO_COVER_CLASS (a2)][ALLOCNO_MODE (a2)]\n+\t     + 1));\n+  if ((diff = pri1 - pri2) != 0)\n+    return diff;\n+  if ((diff = IRA_ALLOCNO_TEMP (a1) - IRA_ALLOCNO_TEMP (a2)) != 0)\n+    return diff;\n+  return ALLOCNO_NUM (a1) - ALLOCNO_NUM (a2);\n+}\n+\n+/* Allocate data of SIZE for the splay trees.  We allocate only spay\n+   tree roots or splay tree nodes.  If you change this, please rewrite\n+   the function.  */\n+static void *\n+splay_tree_allocate (int size, void *data ATTRIBUTE_UNUSED)\n+{\n+  if (size != sizeof (struct splay_tree_node_s))\n+    return ira_allocate (size);\n+  return pool_alloc (splay_tree_node_pool);\n+}\n+\n+/* Free data NODE for the splay trees.  We allocate and free only spay\n+   tree roots or splay tree nodes.  If you change this, please rewrite\n+   the function.  */\n+static void\n+splay_tree_free (void *node, void *data ATTRIBUTE_UNUSED)\n+{\n+  int i;\n+  enum reg_class cover_class;\n+\n+  for (i = 0; i < ira_reg_class_cover_size; i++)\n+    {\n+      cover_class = ira_reg_class_cover[i];\n+      if (node == uncolorable_allocnos_splay_tree[cover_class])\n+\t{\n+\t  ira_free (node);\n+\t  return;\n+\t}\n+    }\n+  pool_free (splay_tree_node_pool, node);\n+}\n+\n+/* Push allocnos to the coloring stack.  The order of allocnos in the\n+   stack defines the order for the subsequent coloring.  */\n+static void\n+push_allocnos_to_stack (void)\n+{\n+  ira_allocno_t allocno, a, i_allocno, *allocno_vec;\n+  enum reg_class cover_class, rclass;\n+  int allocno_pri, i_allocno_pri, allocno_cost, i_allocno_cost;\n+  int i, j, num, cover_class_allocnos_num[N_REG_CLASSES];\n+  ira_allocno_t *cover_class_allocnos[N_REG_CLASSES];\n+  int cost;\n+\n+  /* Initialize.  */\n+  for (i = 0; i < ira_reg_class_cover_size; i++)\n+    {\n+      cover_class = ira_reg_class_cover[i];\n+      cover_class_allocnos_num[cover_class] = 0;\n+      cover_class_allocnos[cover_class] = NULL;\n+      uncolorable_allocnos_splay_tree[cover_class] = NULL;\n+    }\n+  /* Calculate uncolorable allocno spill costs.  */\n+  for (allocno = uncolorable_allocno_bucket;\n+       allocno != NULL;\n+       allocno = ALLOCNO_NEXT_BUCKET_ALLOCNO (allocno))\n+    if ((cover_class = ALLOCNO_COVER_CLASS (allocno)) != NO_REGS)\n+      {\n+\tcover_class_allocnos_num[cover_class]++;\n+\tcost = 0;\n+\tfor (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+\t     a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+\t  {\n+\t    cost += calculate_allocno_spill_cost (a);\n+\t    if (a == allocno)\n+\t      break;\n+\t  }\n+\t/* ??? Remove cost of copies between the coalesced\n+\t   allocnos.  */\n+\tIRA_ALLOCNO_TEMP (allocno) = cost;\n+      }\n+  /* Define place where to put uncolorable allocnos of the same cover\n+     class.  */\n+  for (num = i = 0; i < ira_reg_class_cover_size; i++)\n+    {\n+      cover_class = ira_reg_class_cover[i];\n+      ira_assert (cover_class_allocnos_num[cover_class]\n+\t\t  == uncolorable_allocnos_num[cover_class]);\n+      if (cover_class_allocnos_num[cover_class] != 0)\n+ \t{\n+\t  cover_class_allocnos[cover_class] = allocnos_for_spilling + num;\n+\t  num += cover_class_allocnos_num[cover_class];\n+\t  cover_class_allocnos_num[cover_class] = 0;\n+\t}\n+      if (USE_SPLAY_P (cover_class))\n+\tuncolorable_allocnos_splay_tree[cover_class]\n+\t  = splay_tree_new_with_allocator (allocno_spill_priority_compare,\n+\t\t\t\t\t   NULL, NULL, splay_tree_allocate,\n+\t\t\t\t\t   splay_tree_free, NULL);\n+    }\n+  ira_assert (num <= ira_allocnos_num);\n+  /* Collect uncolorable allocnos of each cover class.  */\n+  for (allocno = uncolorable_allocno_bucket;\n+       allocno != NULL;\n+       allocno = ALLOCNO_NEXT_BUCKET_ALLOCNO (allocno))\n+    if ((cover_class = ALLOCNO_COVER_CLASS (allocno)) != NO_REGS)\n+      {\n+\tcover_class_allocnos\n+\t  [cover_class][cover_class_allocnos_num[cover_class]++] = allocno;\n+\tif (uncolorable_allocnos_splay_tree[cover_class] != NULL)\n+\t  splay_tree_insert (uncolorable_allocnos_splay_tree[cover_class],\n+\t\t\t     (splay_tree_key) allocno,\n+\t\t\t     (splay_tree_value) allocno);\n+      }\n+  for (;;)\n+    {\n+      push_only_colorable ();\n+      allocno = uncolorable_allocno_bucket;\n+      if (allocno == NULL)\n+\tbreak;\n+      cover_class = ALLOCNO_COVER_CLASS (allocno);\n+      if (cover_class == NO_REGS)\n+\t{\n+\t  push_ira_allocno_to_spill (allocno);\n+\t  continue;\n+\t}\n+      /* Potential spilling.  */\n+      ira_assert\n+\t(ira_reg_class_nregs[cover_class][ALLOCNO_MODE (allocno)] > 0);\n+      if (USE_SPLAY_P (cover_class))\n+\t{\n+\t  for (;VEC_length (ira_allocno_t, removed_splay_allocno_vec) != 0;)\n+\t    {\n+\t      allocno = VEC_pop (ira_allocno_t, removed_splay_allocno_vec);\n+\t      ALLOCNO_SPLAY_REMOVED_P (allocno) = false;\n+\t      rclass = ALLOCNO_COVER_CLASS (allocno);\n+\t      if (ALLOCNO_LEFT_CONFLICTS_NUM (allocno)\n+\t\t  + ira_reg_class_nregs [rclass][ALLOCNO_MODE (allocno)]\n+\t\t  > ALLOCNO_AVAILABLE_REGS_NUM (allocno))\n+\t\tsplay_tree_insert\n+\t\t  (uncolorable_allocnos_splay_tree[rclass],\n+\t\t   (splay_tree_key) allocno, (splay_tree_value) allocno);\n+\t    }\n+\t  allocno = ((ira_allocno_t)\n+\t\t     splay_tree_min\n+\t\t     (uncolorable_allocnos_splay_tree[cover_class])->key);\n+\t  splay_tree_remove (uncolorable_allocnos_splay_tree[cover_class],\n+\t\t\t     (splay_tree_key) allocno);\n+\t}\n+      else\n+\t{\n+\t  num = cover_class_allocnos_num[cover_class];\n+\t  ira_assert (num > 0);\n+\t  allocno_vec = cover_class_allocnos[cover_class];\n+\t  allocno = NULL;\n+\t  allocno_pri = allocno_cost = 0;\n+\t  /* Sort uncolorable allocno to find the one with the lowest\n+\t     spill cost.  */\n+\t  for (i = 0, j = num - 1; i <= j;)\n+\t    {\n+\t      i_allocno = allocno_vec[i];\n+\t      if (! ALLOCNO_IN_GRAPH_P (i_allocno)\n+\t\t  && ALLOCNO_IN_GRAPH_P (allocno_vec[j]))\n+\t\t{\n+\t\t  i_allocno = allocno_vec[j];\n+\t\t  allocno_vec[j] = allocno_vec[i];\n+\t\t  allocno_vec[i] = i_allocno;\n+\t\t}\n+\t      if (ALLOCNO_IN_GRAPH_P (i_allocno))\n+\t\t{\n+\t\t  i++;\n+\t\t  if (IRA_ALLOCNO_TEMP (i_allocno) == INT_MAX)\n+\t\t    {\n+\t\t      ira_allocno_t a;\n+\t\t      int cost = 0;\n+\t\t      \n+\t\t      for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (i_allocno);;\n+\t\t\t   a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+\t\t\t{\n+\t\t\t  cost += calculate_allocno_spill_cost (i_allocno);\n+\t\t\t  if (a == i_allocno)\n+\t\t\t    break;\n+\t\t\t}\n+\t\t      /* ??? Remove cost of copies between the coalesced\n+\t\t\t allocnos.  */\n+\t\t      IRA_ALLOCNO_TEMP (i_allocno) = cost;\n+\t\t    }\n+\t\t  i_allocno_cost = IRA_ALLOCNO_TEMP (i_allocno);\n+\t\t  i_allocno_pri\n+\t\t    = (i_allocno_cost\n+\t\t       / (ALLOCNO_LEFT_CONFLICTS_NUM (i_allocno)\n+\t\t\t  * ira_reg_class_nregs[ALLOCNO_COVER_CLASS\n+\t\t\t\t\t\t(i_allocno)]\n+\t\t\t  [ALLOCNO_MODE (i_allocno)] + 1));\n+\t\t  if (allocno == NULL || allocno_pri > i_allocno_pri\n+\t\t      || (allocno_pri == i_allocno_pri\n+\t\t\t  && (allocno_cost > i_allocno_cost\n+\t\t\t      || (allocno_cost == i_allocno_cost \n+\t\t\t\t  && (ALLOCNO_NUM (allocno)\n+\t\t\t\t      > ALLOCNO_NUM (i_allocno))))))\n+\t\t    {\n+\t\t      allocno = i_allocno;\n+\t\t      allocno_cost = i_allocno_cost;\n+\t\t      allocno_pri = i_allocno_pri;\n+\t\t    }\n+\t\t}\n+\t      if (! ALLOCNO_IN_GRAPH_P (allocno_vec[j]))\n+\t\tj--;\n+\t    }\n+\t  ira_assert (allocno != NULL && j >= 0);\n+\t  cover_class_allocnos_num[cover_class] = j + 1;\n+\t}\n+      ira_assert (ALLOCNO_IN_GRAPH_P (allocno)\n+\t\t  && ALLOCNO_COVER_CLASS (allocno) == cover_class\n+\t\t  && (ALLOCNO_LEFT_CONFLICTS_NUM (allocno)\n+\t\t      + ira_reg_class_nregs[cover_class][ALLOCNO_MODE\n+\t\t\t\t\t\t\t (allocno)]\n+\t\t      > ALLOCNO_AVAILABLE_REGS_NUM (allocno)));\n+      remove_allocno_from_bucket_and_push (allocno, false);\n+    }\n+  ira_assert (colorable_allocno_bucket == NULL\n+\t      && uncolorable_allocno_bucket == NULL);\n+  for (i = 0; i < ira_reg_class_cover_size; i++)\n+    {\n+      cover_class = ira_reg_class_cover[i];\n+      ira_assert (uncolorable_allocnos_num[cover_class] == 0);\n+      if (uncolorable_allocnos_splay_tree[cover_class] != NULL)\n+\tsplay_tree_delete (uncolorable_allocnos_splay_tree[cover_class]);\n+    }\n+}\n+\n+/* Pop the coloring stack and assign hard registers to the popped\n+   allocnos.  */\n+static void\n+pop_allocnos_from_stack (void)\n+{\n+  ira_allocno_t allocno;\n+  enum reg_class cover_class;\n+\n+  for (;VEC_length (ira_allocno_t, allocno_stack_vec) != 0;)\n+    {\n+      allocno = VEC_pop (ira_allocno_t, allocno_stack_vec);\n+      cover_class = ALLOCNO_COVER_CLASS (allocno);\n+      if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t{\n+\t  fprintf (ira_dump_file, \"      Popping\");\n+\t  print_coalesced_allocno (allocno);\n+\t  fprintf (ira_dump_file, \"  -- \");\n+\t}\n+      if (cover_class == NO_REGS)\n+\t{\n+\t  ALLOCNO_HARD_REGNO (allocno) = -1;\n+\t  ALLOCNO_ASSIGNED_P (allocno) = true;\n+\t  ira_assert (ALLOCNO_UPDATED_HARD_REG_COSTS (allocno) == NULL);\n+\t  ira_assert\n+\t    (ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (allocno) == NULL);\n+\t  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file, \"assign memory\\n\");\n+\t}\n+      else if (assign_hard_reg (allocno, false))\n+\t{\n+\t  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file, \"assign reg %d\\n\",\n+\t\t     ALLOCNO_HARD_REGNO (allocno));\n+\t}\n+      else if (ALLOCNO_ASSIGNED_P (allocno))\n+\t{\n+\t  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file, \"spill\\n\");\n+\t}\n+      ALLOCNO_IN_GRAPH_P (allocno) = true;\n+    }\n+}\n+\n+/* Set up number of available hard registers for ALLOCNO.  */\n+static void\n+setup_allocno_available_regs_num (ira_allocno_t allocno)\n+{\n+  int i, n, hard_regs_num;\n+  enum reg_class cover_class;\n+  ira_allocno_t a;\n+  HARD_REG_SET temp_set;\n+\n+  cover_class = ALLOCNO_COVER_CLASS (allocno);\n+  ALLOCNO_AVAILABLE_REGS_NUM (allocno) = ira_available_class_regs[cover_class];\n+  if (cover_class == NO_REGS)\n+    return;\n+  CLEAR_HARD_REG_SET (temp_set);\n+  ira_assert (ALLOCNO_FIRST_COALESCED_ALLOCNO (allocno) == allocno);\n+  hard_regs_num = ira_class_hard_regs_num[cover_class];\n+  for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+       a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+    {\n+      IOR_HARD_REG_SET (temp_set, ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a));\n+      if (a == allocno)\n+\tbreak;\n+    }\n+  for (n = 0, i = hard_regs_num - 1; i >= 0; i--)\n+    if (TEST_HARD_REG_BIT (temp_set, ira_class_hard_regs[cover_class][i]))\n+      n++;\n+  if (internal_flag_ira_verbose > 2 && n > 0 && ira_dump_file != NULL)\n+    fprintf (ira_dump_file, \"    Reg %d of %s has %d regs less\\n\",\n+\t     ALLOCNO_REGNO (allocno), reg_class_names[cover_class], n);\n+  ALLOCNO_AVAILABLE_REGS_NUM (allocno) -= n;\n+}\n+\n+/* Set up ALLOCNO_LEFT_CONFLICTS_NUM for ALLOCNO.  */\n+static void\n+setup_allocno_left_conflicts_num (ira_allocno_t allocno)\n+{\n+  int i, hard_regs_num, hard_regno, conflict_allocnos_size;\n+  ira_allocno_t a, conflict_allocno;\n+  enum reg_class cover_class;\n+  HARD_REG_SET temp_set;\n+  ira_allocno_conflict_iterator aci;\n+\n+  cover_class = ALLOCNO_COVER_CLASS (allocno);\n+  hard_regs_num = ira_class_hard_regs_num[cover_class];\n+  CLEAR_HARD_REG_SET (temp_set);\n+  ira_assert (ALLOCNO_FIRST_COALESCED_ALLOCNO (allocno) == allocno);\n+  for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+       a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+    {\n+      IOR_HARD_REG_SET (temp_set, ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a));\n+      if (a == allocno)\n+\tbreak;\n+    }\n+  AND_HARD_REG_SET (temp_set, reg_class_contents[cover_class]);\n+  AND_COMPL_HARD_REG_SET (temp_set, ira_no_alloc_regs);\n+  conflict_allocnos_size = 0;\n+  if (! hard_reg_set_equal_p (temp_set, ira_zero_hard_reg_set))\n+    for (i = 0; i < (int) hard_regs_num; i++)\n+      {\n+\thard_regno = ira_class_hard_regs[cover_class][i];\n+\tif (TEST_HARD_REG_BIT (temp_set, hard_regno))\n+\t  {\n+\t    conflict_allocnos_size++;\n+\t    CLEAR_HARD_REG_BIT (temp_set, hard_regno);\n+\t    if (hard_reg_set_equal_p (temp_set, ira_zero_hard_reg_set))\n+\t      break;\n+\t  }\n+      }\n+  CLEAR_HARD_REG_SET (temp_set);\n+  if (allocno_coalesced_p)\n+    bitmap_clear (processed_coalesced_allocno_bitmap);\n+  if (cover_class != NO_REGS)\n+    for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+\t a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+      {\n+\tFOR_EACH_ALLOCNO_CONFLICT (a, conflict_allocno, aci)\n+\t  if (bitmap_bit_p (consideration_allocno_bitmap,\n+\t\t\t    ALLOCNO_NUM (conflict_allocno)))\n+\t    {\n+\t      ira_assert (cover_class\n+\t\t\t  == ALLOCNO_COVER_CLASS (conflict_allocno));\n+\t      if (allocno_coalesced_p)\n+\t\t{\n+\t\t  if (bitmap_bit_p (processed_coalesced_allocno_bitmap,\n+\t\t\t\t    ALLOCNO_NUM (conflict_allocno)))\n+\t\t    continue;\n+\t\t  bitmap_set_bit (processed_coalesced_allocno_bitmap,\n+\t\t\t\t  ALLOCNO_NUM (conflict_allocno));\n+\t\t}\n+\t      if (! ALLOCNO_ASSIGNED_P (conflict_allocno))\n+\t\tconflict_allocnos_size\n+\t\t  += (ira_reg_class_nregs\n+\t\t      [cover_class][ALLOCNO_MODE (conflict_allocno)]);\n+\t      else if ((hard_regno = ALLOCNO_HARD_REGNO (conflict_allocno))\n+\t\t       >= 0)\n+\t\t{\n+\t\t  int last = (hard_regno\n+\t\t\t      + hard_regno_nregs\n+\t\t\t        [hard_regno][ALLOCNO_MODE (conflict_allocno)]);\n+\t\t  \n+\t\t  while (hard_regno < last)\n+\t\t    {\n+\t\t      if (! TEST_HARD_REG_BIT (temp_set, hard_regno))\n+\t\t\t{\n+\t\t\t  conflict_allocnos_size++;\n+\t\t\t  SET_HARD_REG_BIT (temp_set, hard_regno);\n+\t\t\t}\n+\t\t      hard_regno++;\n+\t\t    }\n+\t\t}\n+\t    }\n+        if (a == allocno)\n+\t  break;\n+      }\n+  ALLOCNO_LEFT_CONFLICTS_NUM (allocno) = conflict_allocnos_size;\n+}\n+\n+/* Put ALLOCNO in a bucket corresponding to its number and size of its\n+   conflicting allocnos and hard registers.  */\n+static void\n+put_allocno_into_bucket (ira_allocno_t allocno)\n+{\n+  int hard_regs_num;\n+  enum reg_class cover_class;\n+\n+  cover_class = ALLOCNO_COVER_CLASS (allocno);\n+  hard_regs_num = ira_class_hard_regs_num[cover_class];\n+  if (ALLOCNO_FIRST_COALESCED_ALLOCNO (allocno) != allocno)\n+    return;\n+  ALLOCNO_IN_GRAPH_P (allocno) = true;\n+  setup_allocno_left_conflicts_num (allocno);\n+  setup_allocno_available_regs_num (allocno);\n+  if (ALLOCNO_LEFT_CONFLICTS_NUM (allocno)\n+      + ira_reg_class_nregs[cover_class][ALLOCNO_MODE (allocno)]\n+      <= ALLOCNO_AVAILABLE_REGS_NUM (allocno))\n+    add_ira_allocno_to_bucket (allocno, &colorable_allocno_bucket);\n+  else\n+    add_ira_allocno_to_bucket (allocno, &uncolorable_allocno_bucket);\n+}\n+\n+/* The function is used to sort allocnos according to their execution\n+   frequencies.  */\n+static int\n+copy_freq_compare_func (const void *v1p, const void *v2p)\n+{\n+  ira_copy_t cp1 = *(const ira_copy_t *) v1p, cp2 = *(const ira_copy_t *) v2p;\n+  int pri1, pri2;\n+\n+  pri1 = cp1->freq;\n+  pri2 = cp2->freq;\n+  if (pri2 - pri1)\n+    return pri2 - pri1;\n+\n+  /* If freqencies are equal, sort by copies, so that the results of\n+     qsort leave nothing to chance.  */\n+  return cp1->num - cp2->num;\n+}\n+\n+/* Merge two sets of coalesced allocnos given correspondingly by\n+   allocnos A1 and A2 (more accurately merging A2 set into A1\n+   set).  */\n+static void\n+merge_allocnos (ira_allocno_t a1, ira_allocno_t a2)\n+{\n+  ira_allocno_t a, first, last, next;\n+\n+  first = ALLOCNO_FIRST_COALESCED_ALLOCNO (a1);\n+  if (first == ALLOCNO_FIRST_COALESCED_ALLOCNO (a2))\n+    return;\n+  for (last = a2, a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a2);;\n+       a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+    {\n+      ALLOCNO_FIRST_COALESCED_ALLOCNO (a) = first;\n+      if (a == a2)\n+\tbreak;\n+      last = a;\n+    }\n+  next = ALLOCNO_NEXT_COALESCED_ALLOCNO (first);\n+  ALLOCNO_NEXT_COALESCED_ALLOCNO (first) = a2;\n+  ALLOCNO_NEXT_COALESCED_ALLOCNO (last) = next;\n+}\n+\n+/* Return TRUE if there are conflicting allocnos from two sets of\n+   coalesced allocnos given correspondingly by allocnos A1 and A2.  If\n+   RELOAD_P is TRUE, we use live ranges to find conflicts because\n+   conflicts are represented only for allocnos of the same cover class\n+   and during the reload pass we coalesce allocnos for sharing stack\n+   memory slots.  */\n+static bool\n+coalesced_allocno_conflict_p (ira_allocno_t a1, ira_allocno_t a2,\n+\t\t\t      bool reload_p)\n+{\n+  ira_allocno_t a, conflict_allocno;\n+  ira_allocno_conflict_iterator aci;\n+\n+  if (allocno_coalesced_p)\n+    {\n+      bitmap_clear (processed_coalesced_allocno_bitmap);\n+      for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a1);;\n+\t   a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+\t{\n+\t  bitmap_set_bit (processed_coalesced_allocno_bitmap, ALLOCNO_NUM (a));\n+\t  if (a == a1)\n+\t    break;\n+\t}\n+    }\n+  for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a2);;\n+       a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+    {\n+      if (reload_p)\n+\t{\n+\t  for (conflict_allocno = ALLOCNO_NEXT_COALESCED_ALLOCNO (a1);;\n+\t       conflict_allocno\n+\t\t = ALLOCNO_NEXT_COALESCED_ALLOCNO (conflict_allocno))\n+\t    {\n+\t      if (ira_allocno_live_ranges_intersect_p (a, conflict_allocno))\n+\t\treturn true;\n+\t      if (conflict_allocno == a1)\n+\t\tbreak;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  FOR_EACH_ALLOCNO_CONFLICT (a, conflict_allocno, aci)\n+\t    if (conflict_allocno == a1\n+\t\t|| (allocno_coalesced_p\n+\t\t    && bitmap_bit_p (processed_coalesced_allocno_bitmap,\n+\t\t\t\t     ALLOCNO_NUM (conflict_allocno))))\n+\t      return true;\n+\t}\n+      if (a == a2)\n+\tbreak;\n+    }\n+  return false;\n+}\n+\n+/* The major function for aggressive allocno coalescing.  For the\n+   reload pass (RELOAD_P) we coalesce only spilled allocnos.  If some\n+   allocnos have been coalesced, we set up flag\n+   allocno_coalesced_p.  */\n+static void\n+coalesce_allocnos (bool reload_p)\n+{\n+  ira_allocno_t a;\n+  ira_copy_t cp, next_cp, *sorted_copies;\n+  enum reg_class cover_class;\n+  enum machine_mode mode;\n+  unsigned int j;\n+  int i, n, cp_num, regno;\n+  bitmap_iterator bi;\n+\n+  sorted_copies = (ira_copy_t *) ira_allocate (ira_copies_num\n+\t\t\t\t\t       * sizeof (ira_copy_t));\n+  cp_num = 0;\n+  /* Collect copies.  */\n+  EXECUTE_IF_SET_IN_BITMAP (coloring_allocno_bitmap, 0, j, bi)\n+    {\n+      a = ira_allocnos[j];\n+      regno = ALLOCNO_REGNO (a);\n+      if ((! reload_p && ALLOCNO_ASSIGNED_P (a))\n+\t  || (reload_p\n+\t      && (! ALLOCNO_ASSIGNED_P (a) || ALLOCNO_HARD_REGNO (a) >= 0\n+\t\t  || (regno < ira_reg_equiv_len\n+\t\t      && (ira_reg_equiv_const[regno] != NULL_RTX\n+\t\t\t  || ira_reg_equiv_invariant_p[regno])))))\n+\tcontinue;\n+      cover_class = ALLOCNO_COVER_CLASS (a);\n+      mode = ALLOCNO_MODE (a);\n+      for (cp = ALLOCNO_COPIES (a); cp != NULL; cp = next_cp)\n+\t{\n+\t  if (cp->first == a)\n+\t    {\n+\t      next_cp = cp->next_first_allocno_copy;\n+\t      regno = ALLOCNO_REGNO (cp->second);\n+\t      if ((reload_p\n+\t\t   || (ALLOCNO_COVER_CLASS (cp->second) == cover_class\n+\t\t       && ALLOCNO_MODE (cp->second) == mode))\n+\t\t  && cp->insn != NULL\n+\t\t  && ((! reload_p && ! ALLOCNO_ASSIGNED_P (cp->second))\n+\t\t      || (reload_p\n+\t\t\t  && ALLOCNO_ASSIGNED_P (cp->second)\n+\t\t\t  && ALLOCNO_HARD_REGNO (cp->second) < 0\n+\t\t\t  && (regno >= ira_reg_equiv_len\n+\t\t\t      || (! ira_reg_equiv_invariant_p[regno]\n+\t\t\t\t  && ira_reg_equiv_const[regno] == NULL_RTX)))))\n+\t\tsorted_copies[cp_num++] = cp;\n+\t    }\n+\t  else if (cp->second == a)\n+\t    next_cp = cp->next_second_allocno_copy;\n+\t  else\n+\t    gcc_unreachable ();\n+\t}\n+    }\n+  qsort (sorted_copies, cp_num, sizeof (ira_copy_t), copy_freq_compare_func);\n+  /* Coalesced copies, most frequently executed first.  */\n+  for (; cp_num != 0;)\n+    {\n+      for (i = 0; i < cp_num; i++)\n+\t{\n+\t  cp = sorted_copies[i];\n+\t  if (! coalesced_allocno_conflict_p (cp->first, cp->second, reload_p))\n+\t    {\n+\t      allocno_coalesced_p = true;\n+\t      if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t\tfprintf\n+\t\t  (ira_dump_file,\n+\t\t   \"      Coalescing copy %d:a%dr%d-a%dr%d (freq=%d)\\n\",\n+\t\t   cp->num, ALLOCNO_NUM (cp->first), ALLOCNO_REGNO (cp->first),\n+\t\t   ALLOCNO_NUM (cp->second), ALLOCNO_REGNO (cp->second),\n+\t\t   cp->freq);\n+\t      merge_allocnos (cp->first, cp->second);\n+\t      i++;\n+\t      break;\n+\t    }\n+\t}\n+      /* Collect the rest of copies.  */\n+      for (n = 0; i < cp_num; i++)\n+\t{\n+\t  cp = sorted_copies[i];\n+\t  if (ALLOCNO_FIRST_COALESCED_ALLOCNO (cp->first)\n+\t      != ALLOCNO_FIRST_COALESCED_ALLOCNO (cp->second))\n+\t    sorted_copies[n++] = cp;\n+\t}\n+      cp_num = n;\n+    }\n+  ira_free (sorted_copies);\n+}\n+\n+/* Chaitin-Briggs coloring for allocnos in COLORING_ALLOCNO_BITMAP\n+   taking into account allocnos in CONSIDERATION_ALLOCNO_BITMAP.  */\n+static void\n+color_allocnos (void)\n+{\n+  unsigned int i;\n+  bitmap_iterator bi;\n+  ira_allocno_t a;\n+\n+  allocno_coalesced_p = false;\n+  processed_coalesced_allocno_bitmap = ira_allocate_bitmap ();\n+  if (flag_ira_coalesce)\n+    coalesce_allocnos (false);\n+  /* Put the allocnos into the corresponding buckets.  */\n+  colorable_allocno_bucket = NULL;\n+  uncolorable_allocno_bucket = NULL;\n+  EXECUTE_IF_SET_IN_BITMAP (coloring_allocno_bitmap, 0, i, bi)\n+    {\n+      a = ira_allocnos[i];\n+      if (ALLOCNO_COVER_CLASS (a) == NO_REGS)\n+\t{\n+\t  ALLOCNO_HARD_REGNO (a) = -1;\n+\t  ALLOCNO_ASSIGNED_P (a) = true;\n+\t  ira_assert (ALLOCNO_UPDATED_HARD_REG_COSTS (a) == NULL);\n+\t  ira_assert (ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a) == NULL);\n+\t  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t    {\n+\t      fprintf (ira_dump_file, \"      Spill\");\n+\t      print_coalesced_allocno (a);\n+\t      fprintf (ira_dump_file, \"\\n\");\n+\t    }\n+\t  continue;\n+\t}\n+      put_allocno_into_bucket (a);\n+    }\n+  push_allocnos_to_stack ();\n+  pop_allocnos_from_stack ();\n+  if (flag_ira_coalesce)\n+    /* We don't need coalesced allocnos for ira_reassign_pseudos.  */\n+    EXECUTE_IF_SET_IN_BITMAP (coloring_allocno_bitmap, 0, i, bi)\n+      {\n+\ta = ira_allocnos[i];\n+\tALLOCNO_FIRST_COALESCED_ALLOCNO (a) = a;\n+\tALLOCNO_NEXT_COALESCED_ALLOCNO (a) = a;\n+      }\n+  ira_free_bitmap (processed_coalesced_allocno_bitmap);\n+  allocno_coalesced_p = false;\n+}\n+\n+\f\n+\n+/* Output information about the loop given by its LOOP_TREE_NODE. */\n+static void\n+print_loop_title (ira_loop_tree_node_t loop_tree_node)\n+{\n+  unsigned int j;\n+  bitmap_iterator bi;\n+\n+  ira_assert (loop_tree_node->loop != NULL);\n+  fprintf (ira_dump_file,\n+\t   \"\\n  Loop %d (parent %d, header bb%d, depth %d)\\n    ref:\",\n+\t   loop_tree_node->loop->num,\n+\t   (loop_tree_node->parent == NULL\n+\t    ? -1 : loop_tree_node->parent->loop->num),\n+\t   loop_tree_node->loop->header->index,\n+\t   loop_depth (loop_tree_node->loop));\n+  EXECUTE_IF_SET_IN_BITMAP (loop_tree_node->mentioned_allocnos, 0, j, bi)\n+    fprintf (ira_dump_file, \" %dr%d\", j, ALLOCNO_REGNO (ira_allocnos[j]));\n+  fprintf (ira_dump_file, \"\\n    modified regnos:\");\n+  EXECUTE_IF_SET_IN_BITMAP (loop_tree_node->modified_regnos, 0, j, bi)\n+    fprintf (ira_dump_file, \" %d\", j);\n+  fprintf (ira_dump_file, \"\\n    border:\");\n+  EXECUTE_IF_SET_IN_BITMAP (loop_tree_node->border_allocnos, 0, j, bi)\n+    fprintf (ira_dump_file, \" %dr%d\", j, ALLOCNO_REGNO (ira_allocnos[j]));\n+  fprintf (ira_dump_file, \"\\n    Pressure:\");\n+  for (j = 0; (int) j < ira_reg_class_cover_size; j++)\n+    {\n+      enum reg_class cover_class;\n+      \n+      cover_class = ira_reg_class_cover[j];\n+      if (loop_tree_node->reg_pressure[cover_class] == 0)\n+\tcontinue;\n+      fprintf (ira_dump_file, \" %s=%d\", reg_class_names[cover_class],\n+\t       loop_tree_node->reg_pressure[cover_class]);\n+    }\n+  fprintf (ira_dump_file, \"\\n\");\n+}\n+\n+/* Color the allocnos inside loop (in the extreme case it can be all\n+   of the function) given the corresponding LOOP_TREE_NODE.  The\n+   function is called for each loop during top-down traverse of the\n+   loop tree.  */\n+static void\n+color_pass (ira_loop_tree_node_t loop_tree_node)\n+{\n+  int regno, hard_regno, index = -1;\n+  int cost, exit_freq, enter_freq;\n+  unsigned int j;\n+  bitmap_iterator bi;\n+  enum machine_mode mode;\n+  enum reg_class rclass, cover_class;\n+  ira_allocno_t a, subloop_allocno;\n+  ira_loop_tree_node_t subloop_node;\n+\n+  ira_assert (loop_tree_node->bb == NULL);\n+  if (internal_flag_ira_verbose > 1 && ira_dump_file != NULL)\n+    print_loop_title (loop_tree_node);\n+\n+  bitmap_copy (coloring_allocno_bitmap, loop_tree_node->mentioned_allocnos);\n+  bitmap_ior_into (coloring_allocno_bitmap, loop_tree_node->border_allocnos);\n+  bitmap_copy (consideration_allocno_bitmap, coloring_allocno_bitmap);\n+  EXECUTE_IF_SET_IN_BITMAP (consideration_allocno_bitmap, 0, j, bi)\n+    {\n+      a = ira_allocnos[j];\n+      if (! ALLOCNO_ASSIGNED_P (a))\n+\tcontinue;\n+      bitmap_clear_bit (coloring_allocno_bitmap, ALLOCNO_NUM (a));\n+    }\n+  /* Color all mentioned allocnos including transparent ones.  */\n+  color_allocnos ();\n+  /* Process caps.  They are processed just once.  */\n+  if (flag_ira_algorithm == IRA_ALGORITHM_MIXED\n+      || flag_ira_algorithm == IRA_ALGORITHM_REGIONAL)\n+    EXECUTE_IF_SET_IN_BITMAP (loop_tree_node->mentioned_allocnos, 0, j, bi)\n+      {\n+\ta = ira_allocnos[j];\n+\tif (ALLOCNO_CAP_MEMBER (a) == NULL)\n+\t  continue;\n+\t/* Remove from processing in the next loop.  */\n+\tbitmap_clear_bit (consideration_allocno_bitmap, j);\n+\trclass = ALLOCNO_COVER_CLASS (a);\n+\tif ((flag_ira_algorithm == IRA_ALGORITHM_MIXED\n+\t     && loop_tree_node->reg_pressure[rclass]\n+\t     <= ira_available_class_regs[rclass]))\n+\t  {\n+\t    mode = ALLOCNO_MODE (a);\n+\t    hard_regno = ALLOCNO_HARD_REGNO (a);\n+\t    if (hard_regno >= 0)\n+\t      {\n+\t\tindex = ira_class_hard_reg_index[rclass][hard_regno];\n+\t\tira_assert (index >= 0);\n+\t      }\n+\t    regno = ALLOCNO_REGNO (a);\n+\t    subloop_allocno = ALLOCNO_CAP_MEMBER (a);\n+\t    subloop_node = ALLOCNO_LOOP_TREE_NODE (subloop_allocno);\n+\t    ira_assert (!ALLOCNO_ASSIGNED_P (subloop_allocno));\n+\t    ALLOCNO_HARD_REGNO (subloop_allocno) = hard_regno;\n+\t    ALLOCNO_ASSIGNED_P (subloop_allocno) = true;\n+\t    if (hard_regno >= 0)\n+\t      update_copy_costs (subloop_allocno, true);\n+\t    /* We don't need updated costs anymore: */\n+\t    ira_free_allocno_updated_costs (subloop_allocno);\n+\t  }\n+      }\n+  /* Update costs of the corresponding allocnos (not caps) in the\n+     subloops.  */\n+  for (subloop_node = loop_tree_node->subloops;\n+       subloop_node != NULL;\n+       subloop_node = subloop_node->subloop_next)\n+    {\n+      ira_assert (subloop_node->bb == NULL);\n+      EXECUTE_IF_SET_IN_BITMAP (consideration_allocno_bitmap, 0, j, bi)\n+        {\n+\t  a = ira_allocnos[j];\n+\t  ira_assert (ALLOCNO_CAP_MEMBER (a) == NULL);\n+\t  mode = ALLOCNO_MODE (a);\n+\t  rclass = ALLOCNO_COVER_CLASS (a);\n+\t  hard_regno = ALLOCNO_HARD_REGNO (a);\n+\t  if (hard_regno >= 0)\n+\t    {\n+\t      index = ira_class_hard_reg_index[rclass][hard_regno];\n+\t      ira_assert (index >= 0);\n+\t    }\n+\t  regno = ALLOCNO_REGNO (a);\n+\t  /* ??? conflict costs */\n+\t  subloop_allocno = subloop_node->regno_allocno_map[regno];\n+\t  if (subloop_allocno == NULL\n+\t      || ALLOCNO_CAP (subloop_allocno) != NULL)\n+\t    continue;\n+\t  if ((flag_ira_algorithm == IRA_ALGORITHM_MIXED\n+\t       && (loop_tree_node->reg_pressure[rclass]\n+\t\t   <= ira_available_class_regs[rclass]))\n+\t      || (hard_regno < 0\n+\t\t  && ! bitmap_bit_p (subloop_node->mentioned_allocnos,\n+\t\t\t\t     ALLOCNO_NUM (subloop_allocno))))\n+\t    {\n+\t      if (! ALLOCNO_ASSIGNED_P (subloop_allocno))\n+\t\t{\n+\t\t  ALLOCNO_HARD_REGNO (subloop_allocno) = hard_regno;\n+\t\t  ALLOCNO_ASSIGNED_P (subloop_allocno) = true;\n+\t\t  if (hard_regno >= 0)\n+\t\t    update_copy_costs (subloop_allocno, true);\n+\t\t  /* We don't need updated costs anymore: */\n+\t\t  ira_free_allocno_updated_costs (subloop_allocno);\n+\t\t}\n+\t      continue;\n+\t    }\n+\t  exit_freq = ira_loop_edge_freq (subloop_node, regno, true);\n+\t  enter_freq = ira_loop_edge_freq (subloop_node, regno, false);\n+\t  ira_assert (regno < ira_reg_equiv_len);\n+\t  if (ira_reg_equiv_invariant_p[regno]\n+\t      || ira_reg_equiv_const[regno] != NULL_RTX)\n+\t    {\n+\t      if (! ALLOCNO_ASSIGNED_P (subloop_allocno))\n+\t\t{\n+\t\t  ALLOCNO_HARD_REGNO (subloop_allocno) = hard_regno;\n+\t\t  ALLOCNO_ASSIGNED_P (subloop_allocno) = true;\n+\t\t  if (hard_regno >= 0)\n+\t\t    update_copy_costs (subloop_allocno, true);\n+\t\t  /* We don't need updated costs anymore: */\n+\t\t  ira_free_allocno_updated_costs (subloop_allocno);\n+\t\t}\n+\t    }\n+\t  else if (hard_regno < 0)\n+\t    {\n+\t      ALLOCNO_UPDATED_MEMORY_COST (subloop_allocno)\n+\t\t-= ((ira_memory_move_cost[mode][rclass][1] * enter_freq)\n+\t\t    + (ira_memory_move_cost[mode][rclass][0] * exit_freq));\n+\t    }\n+\t  else\n+\t    {\n+\t      cover_class = ALLOCNO_COVER_CLASS (subloop_allocno);\n+\t      ira_allocate_and_set_costs\n+\t\t(&ALLOCNO_HARD_REG_COSTS (subloop_allocno), cover_class,\n+\t\t ALLOCNO_COVER_CLASS_COST (subloop_allocno));\n+\t      ira_allocate_and_set_costs\n+\t\t(&ALLOCNO_CONFLICT_HARD_REG_COSTS (subloop_allocno),\n+\t\t cover_class, 0);\n+\t      cost = (ira_register_move_cost[mode][rclass][rclass] \n+\t\t      * (exit_freq + enter_freq));\n+\t      ALLOCNO_HARD_REG_COSTS (subloop_allocno)[index] -= cost;\n+\t      ALLOCNO_CONFLICT_HARD_REG_COSTS (subloop_allocno)[index]\n+\t\t-= cost;\n+\t      ALLOCNO_UPDATED_MEMORY_COST (subloop_allocno)\n+\t\t+= (ira_memory_move_cost[mode][rclass][0] * enter_freq\n+\t\t    + ira_memory_move_cost[mode][rclass][1] * exit_freq);\n+\t      if (ALLOCNO_COVER_CLASS_COST (subloop_allocno)\n+\t\t  > ALLOCNO_HARD_REG_COSTS (subloop_allocno)[index])\n+\t\tALLOCNO_COVER_CLASS_COST (subloop_allocno)\n+\t\t  = ALLOCNO_HARD_REG_COSTS (subloop_allocno)[index];\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Initialize the common data for coloring and calls functions to do\n+   Chaitin-Briggs and regional coloring.  */\n+static void\n+do_coloring (void)\n+{\n+  coloring_allocno_bitmap = ira_allocate_bitmap ();\n+  allocnos_for_spilling\n+    = (ira_allocno_t *) ira_allocate (sizeof (ira_allocno_t)\n+\t\t\t\t      * ira_allocnos_num);\n+  splay_tree_node_pool = create_alloc_pool (\"splay tree nodes\",\n+\t\t\t\t\t    sizeof (struct splay_tree_node_s),\n+\t\t\t\t\t    100);\n+  if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL)\n+    fprintf (ira_dump_file, \"\\n**** Allocnos coloring:\\n\\n\");\n+  \n+  ira_traverse_loop_tree (false, ira_loop_tree_root, color_pass, NULL);\n+\n+  if (internal_flag_ira_verbose > 1 && ira_dump_file != NULL)\n+    ira_print_disposition (ira_dump_file);\n+\n+  free_alloc_pool (splay_tree_node_pool);\n+  ira_free_bitmap (coloring_allocno_bitmap);\n+  ira_free (allocnos_for_spilling);\n+}\n+\n+\f\n+\n+/* Move spill/restore code, which are to be generated in ira-emit.c,\n+   to less frequent points (if it is profitable) by reassigning some\n+   allocnos (in loop with subloops containing in another loop) to\n+   memory which results in longer live-range where the corresponding\n+   pseudo-registers will be in memory.  */\n+static void\n+move_spill_restore (void)\n+{\n+  int cost, regno, hard_regno, hard_regno2, index;\n+  bool changed_p;\n+  int enter_freq, exit_freq;\n+  enum machine_mode mode;\n+  enum reg_class rclass;\n+  ira_allocno_t a, parent_allocno, subloop_allocno;\n+  ira_loop_tree_node_t parent, loop_node, subloop_node;\n+  ira_allocno_iterator ai;\n+\n+  for (;;)\n+    {\n+      changed_p = false;\n+      if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL)\n+\tfprintf (ira_dump_file, \"New iteration of spill/restore move\\n\");\n+      FOR_EACH_ALLOCNO (a, ai)\n+\t{\n+\t  regno = ALLOCNO_REGNO (a);\n+\t  loop_node = ALLOCNO_LOOP_TREE_NODE (a);\n+\t  if (ALLOCNO_CAP_MEMBER (a) != NULL\n+\t      || ALLOCNO_CAP (a) != NULL\n+\t      || (hard_regno = ALLOCNO_HARD_REGNO (a)) < 0\n+\t      || loop_node->children == NULL\n+\t      /* don't do the optimization because it can create\n+\t\t copies and the reload pass can spill the allocno set\n+\t\t by copy although the allocno will not get memory\n+\t\t slot.  */\n+\t      || ira_reg_equiv_invariant_p[regno]\n+\t      || ira_reg_equiv_const[regno] != NULL_RTX\n+\t      || !bitmap_bit_p (loop_node->border_allocnos, ALLOCNO_NUM (a)))\n+\t    continue;\n+\t  mode = ALLOCNO_MODE (a);\n+\t  rclass = ALLOCNO_COVER_CLASS (a);\n+\t  index = ira_class_hard_reg_index[rclass][hard_regno];\n+\t  ira_assert (index >= 0);\n+\t  cost = (ALLOCNO_MEMORY_COST (a)\n+\t\t  - (ALLOCNO_HARD_REG_COSTS (a) == NULL\n+\t\t     ? ALLOCNO_COVER_CLASS_COST (a)\n+\t\t     : ALLOCNO_HARD_REG_COSTS (a)[index]));\n+\t  for (subloop_node = loop_node->subloops;\n+\t       subloop_node != NULL;\n+\t       subloop_node = subloop_node->subloop_next)\n+\t    {\n+\t      ira_assert (subloop_node->bb == NULL);\n+\t      subloop_allocno = subloop_node->regno_allocno_map[regno];\n+\t      if (subloop_allocno == NULL)\n+\t\tcontinue;\n+\t      /* We have accumulated cost.  To get the real cost of\n+\t\t allocno usage in the loop we should subtract costs of\n+\t\t the subloop allocnos.  */\n+\t      cost -= (ALLOCNO_MEMORY_COST (subloop_allocno)\n+\t\t       - (ALLOCNO_HARD_REG_COSTS (subloop_allocno) == NULL\n+\t\t\t  ? ALLOCNO_COVER_CLASS_COST (subloop_allocno)\n+\t\t\t  : ALLOCNO_HARD_REG_COSTS (subloop_allocno)[index]));\n+\t      exit_freq = ira_loop_edge_freq (subloop_node, regno, true);\n+\t      enter_freq = ira_loop_edge_freq (subloop_node, regno, false);\n+\t      if ((hard_regno2 = ALLOCNO_HARD_REGNO (subloop_allocno)) < 0)\n+\t\tcost -= (ira_memory_move_cost[mode][rclass][0] * exit_freq\n+\t\t\t + ira_memory_move_cost[mode][rclass][1] * enter_freq);\n+\t      else\n+\t\t{\n+\t\t  cost\n+\t\t    += (ira_memory_move_cost[mode][rclass][0] * exit_freq\n+\t\t\t+ ira_memory_move_cost[mode][rclass][1] * enter_freq);\n+\t\t  if (hard_regno2 != hard_regno)\n+\t\t    cost -= (ira_register_move_cost[mode][rclass][rclass]\n+\t\t\t     * (exit_freq + enter_freq));\n+\t\t}\n+\t    }\n+\t  if ((parent = loop_node->parent) != NULL\n+\t      && (parent_allocno = parent->regno_allocno_map[regno]) != NULL)\n+\t    {\n+\t      exit_freq\t= ira_loop_edge_freq (loop_node, regno, true);\n+\t      enter_freq = ira_loop_edge_freq (loop_node, regno, false);\n+\t      if ((hard_regno2 = ALLOCNO_HARD_REGNO (parent_allocno)) < 0)\n+\t\tcost -= (ira_memory_move_cost[mode][rclass][0] * exit_freq\n+\t\t\t + ira_memory_move_cost[mode][rclass][1] * enter_freq);\n+\t      else\n+\t\t{\n+\t\t  cost\n+\t\t    += (ira_memory_move_cost[mode][rclass][1] * exit_freq\n+\t\t\t+ ira_memory_move_cost[mode][rclass][0] * enter_freq);\n+\t\t  if (hard_regno2 != hard_regno)\n+\t\t    cost -= (ira_register_move_cost[mode][rclass][rclass]\n+\t\t\t     * (exit_freq + enter_freq));\n+\t\t}\n+\t    }\n+\t  if (cost < 0)\n+\t    {\n+\t      ALLOCNO_HARD_REGNO (a) = -1;\n+\t      if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t\t{\n+\t\t  fprintf\n+\t\t    (ira_dump_file,\n+\t\t     \"      Moving spill/restore for a%dr%d up from loop %d\",\n+\t\t     ALLOCNO_NUM (a), regno, loop_node->loop->num);\n+\t\t  fprintf (ira_dump_file, \" - profit %d\\n\", -cost);\n+\t\t}\n+\t      changed_p = true;\n+\t    }\n+\t}\n+      if (! changed_p)\n+\tbreak;\n+    }\n+}\n+\n+\f\n+\n+/* Update current hard reg costs and current conflict hard reg costs\n+   for allocno A.  It is done by processing its copies containing\n+   other allocnos already assigned.  */\n+static void\n+update_curr_costs (ira_allocno_t a)\n+{\n+  int i, hard_regno, cost;\n+  enum machine_mode mode;\n+  enum reg_class cover_class, rclass;\n+  ira_allocno_t another_a;\n+  ira_copy_t cp, next_cp;\n+\n+  ira_assert (! ALLOCNO_ASSIGNED_P (a));\n+  cover_class = ALLOCNO_COVER_CLASS (a);\n+  if (cover_class == NO_REGS)\n+    return;\n+  mode = ALLOCNO_MODE (a);\n+  for (cp = ALLOCNO_COPIES (a); cp != NULL; cp = next_cp)\n+    {\n+      if (cp->first == a)\n+\t{\n+\t  next_cp = cp->next_first_allocno_copy;\n+\t  another_a = cp->second;\n+\t}\n+      else if (cp->second == a)\n+\t{\n+\t  next_cp = cp->next_second_allocno_copy;\n+\t  another_a = cp->first;\n+\t}\n+      else\n+\tgcc_unreachable ();\n+      if (cover_class != ALLOCNO_COVER_CLASS (another_a)\n+\t  || ! ALLOCNO_ASSIGNED_P (another_a)\n+\t  || (hard_regno = ALLOCNO_HARD_REGNO (another_a)) < 0)\n+\tcontinue;\n+      rclass = REGNO_REG_CLASS (hard_regno);\n+      i = ira_class_hard_reg_index[cover_class][hard_regno];\n+      ira_assert (i >= 0);\n+      cost = (cp->first == a\n+\t      ? ira_register_move_cost[mode][rclass][cover_class]\n+\t      : ira_register_move_cost[mode][cover_class][rclass]);\n+      ira_allocate_and_set_or_copy_costs\n+\t(&ALLOCNO_UPDATED_HARD_REG_COSTS (a),\n+\t cover_class, ALLOCNO_COVER_CLASS_COST (a),\n+\t ALLOCNO_HARD_REG_COSTS (a));\n+      ira_allocate_and_set_or_copy_costs\n+\t(&ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a),\n+\t cover_class, 0, ALLOCNO_CONFLICT_HARD_REG_COSTS (a));\n+      ALLOCNO_UPDATED_HARD_REG_COSTS (a)[i] -= cp->freq * cost;\n+      ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a)[i] -= cp->freq * cost;\n+    }\n+}\n+\n+/* Map: allocno number -> allocno priority.  */\n+static int *allocno_priorities;\n+\n+/* Allocate array ALLOCNO_PRIORITIES and set up priorities for N allocnos in\n+   array CONSIDERATION_ALLOCNOS.  */\n+static void\n+start_allocno_priorities (ira_allocno_t *consideration_allocnos, int n)\n+{\n+  int i, length;\n+  ira_allocno_t a;\n+  allocno_live_range_t r;\n+\n+  for (i = 0; i < n; i++)\n+    {\n+      a = consideration_allocnos[i];\n+      for (length = 0, r = ALLOCNO_LIVE_RANGES (a); r != NULL; r = r->next)\n+\tlength += r->finish - r->start + 1;\n+      if (length == 0)\n+\t{\n+\t  allocno_priorities[ALLOCNO_NUM (a)] = 0;\n+\t  continue;\n+\t}\n+      ira_assert (length > 0 && ALLOCNO_NREFS (a) >= 0);\n+      allocno_priorities[ALLOCNO_NUM (a)]\n+\t= (((double) (floor_log2 (ALLOCNO_NREFS (a)) * ALLOCNO_FREQ (a))\n+\t    / length)\n+\t   * (10000 / REG_FREQ_MAX) * PSEUDO_REGNO_SIZE (ALLOCNO_REGNO (a)));\n+    }\n+}\n+\n+/* Sort allocnos according to their priorities which are calculated\n+   analogous to ones in file `global.c'.  */\n+static int\n+allocno_priority_compare_func (const void *v1p, const void *v2p)\n+{\n+  ira_allocno_t a1 = *(const ira_allocno_t *) v1p;\n+  ira_allocno_t a2 = *(const ira_allocno_t *) v2p;\n+  int pri1, pri2;\n+\n+  pri1 = allocno_priorities[ALLOCNO_NUM (a1)];\n+  pri2 = allocno_priorities[ALLOCNO_NUM (a2)];\n+  if (pri2 - pri1)\n+    return pri2 - pri1;\n+\n+  /* If regs are equally good, sort by allocnos, so that the results of\n+     qsort leave nothing to chance.  */\n+  return ALLOCNO_NUM (a1) - ALLOCNO_NUM (a2);\n+}\n+\n+/* Try to assign hard registers to the unassigned allocnos and\n+   allocnos conflicting with them or conflicting with allocnos whose\n+   regno >= START_REGNO.  The function is called after ira_flattening,\n+   so more allocnos (including ones created in ira-emit.c) will have a\n+   chance to get a hard register.  We use simple assignment algorithm\n+   based on priorities.  */\n+void\n+ira_reassign_conflict_allocnos (int start_regno)\n+{\n+  int i, allocnos_to_color_num;\n+  ira_allocno_t a, conflict_a;\n+  ira_allocno_conflict_iterator aci;\n+  enum reg_class cover_class;\n+  bitmap allocnos_to_color;\n+  ira_allocno_iterator ai;\n+\n+  allocnos_to_color = ira_allocate_bitmap ();\n+  allocnos_to_color_num = 0;\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      if (! ALLOCNO_ASSIGNED_P (a)\n+\t  && ! bitmap_bit_p (allocnos_to_color, ALLOCNO_NUM (a)))\n+\t{\n+\t  if (ALLOCNO_COVER_CLASS (a) != NO_REGS)\n+\t    sorted_allocnos[allocnos_to_color_num++] = a;\n+\t  else\n+\t    {\n+\t      ALLOCNO_ASSIGNED_P (a) = true;\n+\t      ALLOCNO_HARD_REGNO (a) = -1;\n+\t      ira_assert (ALLOCNO_UPDATED_HARD_REG_COSTS (a) == NULL);\n+\t      ira_assert (ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a) == NULL);\n+\t    }\n+\t  bitmap_set_bit (allocnos_to_color, ALLOCNO_NUM (a));\n+\t}\n+      if (ALLOCNO_REGNO (a) < start_regno\n+\t  || (cover_class = ALLOCNO_COVER_CLASS (a)) == NO_REGS)\n+\tcontinue;\n+      FOR_EACH_ALLOCNO_CONFLICT (a, conflict_a, aci)\n+\t{\n+\t  ira_assert (cover_class == ALLOCNO_COVER_CLASS (conflict_a));\n+\t  if (bitmap_bit_p (allocnos_to_color, ALLOCNO_NUM (conflict_a)))\n+\t    continue;\n+\t  bitmap_set_bit (allocnos_to_color, ALLOCNO_NUM (conflict_a));\n+\t  sorted_allocnos[allocnos_to_color_num++] = conflict_a;\n+\t}\n+    }\n+  ira_free_bitmap (allocnos_to_color);\n+  if (allocnos_to_color_num > 1)\n+    {\n+      start_allocno_priorities (sorted_allocnos, allocnos_to_color_num);\n+      qsort (sorted_allocnos, allocnos_to_color_num, sizeof (ira_allocno_t),\n+\t     allocno_priority_compare_func);\n+    }\n+  for (i = 0; i < allocnos_to_color_num; i++)\n+    {\n+      a = sorted_allocnos[i];\n+      ALLOCNO_ASSIGNED_P (a) = false;\n+      ira_assert (ALLOCNO_UPDATED_HARD_REG_COSTS (a) == NULL);\n+      ira_assert (ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a) == NULL);\n+      update_curr_costs (a);\n+    }\n+  for (i = 0; i < allocnos_to_color_num; i++)\n+    {\n+      a = sorted_allocnos[i];\n+      if (assign_hard_reg (a, true))\n+\t{\n+\t  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t    fprintf\n+\t      (ira_dump_file,\n+\t       \"      Secondary allocation: assign hard reg %d to reg %d\\n\",\n+\t       ALLOCNO_HARD_REGNO (a), ALLOCNO_REGNO (a));\n+\t}\n+    }\n+}\n+\n+\f\n+\n+/* This page contains code to coalesce memory stack slots used by\n+   spilled allocnos.  This results in smaller stack frame, better data\n+   locality, and in smaller code for some architectures like\n+   x86/x86_64 where insn size depends on address displacement value.\n+   On the other hand, it can worsen insn scheduling after the RA but\n+   in practice it is less important than smaller stack frames.  */\n+\n+/* Usage cost and order number of coalesced allocno set to which\n+   given pseudo register belongs to.  */\n+static int *regno_coalesced_allocno_cost;\n+static int *regno_coalesced_allocno_num;\n+\n+/* Sort pseudos according frequencies of coalesced allocno sets they\n+   belong to (putting most frequently ones first), and according to\n+   coalesced allocno set order numbers.  */\n+static int\n+coalesced_pseudo_reg_freq_compare (const void *v1p, const void *v2p)\n+{\n+  const int regno1 = *(const int *) v1p;\n+  const int regno2 = *(const int *) v2p;\n+  int diff;\n+\n+  if ((diff = (regno_coalesced_allocno_cost[regno2]\n+\t       - regno_coalesced_allocno_cost[regno1])) != 0)\n+    return diff;\n+  if ((diff = (regno_coalesced_allocno_num[regno1]\n+\t       - regno_coalesced_allocno_num[regno2])) != 0)\n+    return diff;\n+  return regno1 - regno2;\n+}\n+\n+/* Widest width in which each pseudo reg is referred to (via subreg).\n+   It is used for sorting pseudo registers.  */\n+static unsigned int *regno_max_ref_width;\n+\n+/* Redefine STACK_GROWS_DOWNWARD in terms of 0 or 1.  */\n+#ifdef STACK_GROWS_DOWNWARD\n+# undef STACK_GROWS_DOWNWARD\n+# define STACK_GROWS_DOWNWARD 1\n+#else\n+# define STACK_GROWS_DOWNWARD 0\n+#endif\n+\n+/* Sort pseudos according their slot numbers (putting ones with\n+  smaller numbers first, or last when the frame pointer is not\n+  needed).  */\n+static int\n+coalesced_pseudo_reg_slot_compare (const void *v1p, const void *v2p)\n+{\n+  const int regno1 = *(const int *) v1p;\n+  const int regno2 = *(const int *) v2p;\n+  ira_allocno_t a1 = ira_regno_allocno_map[regno1];\n+  ira_allocno_t a2 = ira_regno_allocno_map[regno2];\n+  int diff, slot_num1, slot_num2;\n+  int total_size1, total_size2;\n+\n+  if (a1 == NULL || ALLOCNO_HARD_REGNO (a1) >= 0)\n+    {\n+      if (a2 == NULL || ALLOCNO_HARD_REGNO (a2) >= 0)\n+\treturn (const int *) v1p - (const int *) v2p; /* Save the order. */\n+      return 1;\n+    }\n+  else if (a2 == NULL || ALLOCNO_HARD_REGNO (a2) >= 0)\n+    return -1;\n+  slot_num1 = -ALLOCNO_HARD_REGNO (a1);\n+  slot_num2 = -ALLOCNO_HARD_REGNO (a2);\n+  if ((diff = slot_num1 - slot_num2) != 0)\n+    return (frame_pointer_needed\n+\t    || !FRAME_GROWS_DOWNWARD == STACK_GROWS_DOWNWARD ? diff : -diff);\n+  total_size1 = MAX (PSEUDO_REGNO_BYTES (regno1), regno_max_ref_width[regno1]);\n+  total_size2 = MAX (PSEUDO_REGNO_BYTES (regno2), regno_max_ref_width[regno2]);\n+  if ((diff = total_size2 - total_size1) != 0)\n+    return diff;\n+  return (const int *) v1p - (const int *) v2p; /* Save the order. */\n+}\n+\n+/* Setup REGNO_COALESCED_ALLOCNO_COST and REGNO_COALESCED_ALLOCNO_NUM\n+   for coalesced allocno sets containing allocnos with their regnos\n+   given in array PSEUDO_REGNOS of length N.  */\n+static void\n+setup_coalesced_allocno_costs_and_nums (int *pseudo_regnos, int n)\n+{\n+  int i, num, regno, cost;\n+  ira_allocno_t allocno, a;\n+\n+  for (num = i = 0; i < n; i++)\n+    {\n+      regno = pseudo_regnos[i];\n+      allocno = ira_regno_allocno_map[regno];\n+      if (allocno == NULL)\n+\t{\n+\t  regno_coalesced_allocno_cost[regno] = 0;\n+\t  regno_coalesced_allocno_num[regno] = ++num;\n+\t  continue;\n+\t}\n+      if (ALLOCNO_FIRST_COALESCED_ALLOCNO (allocno) != allocno)\n+\tcontinue;\n+      num++;\n+      for (cost = 0, a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+\t   a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+\t{\n+\t  cost += ALLOCNO_FREQ (a);\n+\t  if (a == allocno)\n+\t    break;\n+\t}\n+      for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+\t   a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+\t{\n+\t  regno_coalesced_allocno_num[ALLOCNO_REGNO (a)] = num;\n+\t  regno_coalesced_allocno_cost[ALLOCNO_REGNO (a)] = cost;\n+\t  if (a == allocno)\n+\t    break;\n+\t}\n+    }\n+}\n+\n+/* Collect spilled allocnos representing coalesced allocno sets (the\n+   first coalesced allocno).  The collected allocnos are returned\n+   through array SPILLED_COALESCED_ALLOCNOS.  The function returns the\n+   number of the collected allocnos.  The allocnos are given by their\n+   regnos in array PSEUDO_REGNOS of length N.  */\n+static int\n+collect_spilled_coalesced_allocnos (int *pseudo_regnos, int n,\n+\t\t\t\t    ira_allocno_t *spilled_coalesced_allocnos)\n+{\n+  int i, num, regno;\n+  ira_allocno_t allocno;\n+\n+  for (num = i = 0; i < n; i++)\n+    {\n+      regno = pseudo_regnos[i];\n+      allocno = ira_regno_allocno_map[regno];\n+      if (allocno == NULL || ALLOCNO_HARD_REGNO (allocno) >= 0\n+\t  || ALLOCNO_FIRST_COALESCED_ALLOCNO (allocno) != allocno)\n+\tcontinue;\n+      spilled_coalesced_allocnos[num++] = allocno;\n+    }\n+  return num;\n+}\n+\n+/* We have coalesced allocnos involving in copies.  Coalesce allocnos\n+   further in order to share the same memory stack slot.  Allocnos\n+   representing sets of allocnos coalesced before the call are given\n+   in array SPILLED_COALESCED_ALLOCNOS of length NUM.  Return TRUE if\n+   some allocnos were coalesced in the function.  */\n+static bool\n+coalesce_spill_slots (ira_allocno_t *spilled_coalesced_allocnos, int num)\n+{\n+  int i, j;\n+  ira_allocno_t allocno, a;\n+  bool merged_p = false;\n+\n+  /* Coalesce non-conflicting spilled allocnos preferring most\n+     frequently used.  */\n+  for (i = 0; i < num; i++)\n+    {\n+      allocno = spilled_coalesced_allocnos[i];\n+      if (ALLOCNO_FIRST_COALESCED_ALLOCNO (allocno) != allocno\n+\t  || (ALLOCNO_REGNO (allocno) < ira_reg_equiv_len\n+\t      && (ira_reg_equiv_invariant_p[ALLOCNO_REGNO (allocno)]\n+\t\t  || ira_reg_equiv_const[ALLOCNO_REGNO (allocno)] != NULL_RTX)))\n+\tcontinue;\n+      for (j = 0; j < i; j++)\n+\t{\n+\t  a = spilled_coalesced_allocnos[j];\n+\t  if (ALLOCNO_FIRST_COALESCED_ALLOCNO (a) != a\n+\t      || (ALLOCNO_REGNO (a) < ira_reg_equiv_len\n+\t\t  && (ira_reg_equiv_invariant_p[ALLOCNO_REGNO (a)]\n+\t\t      || ira_reg_equiv_const[ALLOCNO_REGNO (a)] != NULL_RTX))\n+\t      || coalesced_allocno_conflict_p (allocno, a, true))\n+\t    continue;\n+\t  allocno_coalesced_p = true;\n+\t  merged_p = true;\n+\t  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file,\n+\t\t     \"      Coalescing spilled allocnos a%dr%d->a%dr%d\\n\",\n+\t\t     ALLOCNO_NUM (allocno), ALLOCNO_REGNO (allocno),\n+\t\t     ALLOCNO_NUM (a), ALLOCNO_REGNO (a));\n+\t  merge_allocnos (a, allocno);\n+\t  ira_assert (ALLOCNO_FIRST_COALESCED_ALLOCNO (a) == a);\n+\t}\n+    }\n+  return merged_p;\n+}\n+\n+/* Sort pseudo-register numbers in array PSEUDO_REGNOS of length N for\n+   subsequent assigning stack slots to them in the reload pass.  To do\n+   this we coalesce spilled allocnos first to decrease the number of\n+   memory-memory move insns.  This function is called by the\n+   reload.  */\n+void\n+ira_sort_regnos_for_alter_reg (int *pseudo_regnos, int n,\n+\t\t\t       unsigned int *reg_max_ref_width)\n+{\n+  int max_regno = max_reg_num ();\n+  int i, regno, num, slot_num;\n+  ira_allocno_t allocno, a;\n+  ira_allocno_iterator ai;\n+  ira_allocno_t *spilled_coalesced_allocnos;\n+\n+  processed_coalesced_allocno_bitmap = ira_allocate_bitmap ();\n+  /* Set up allocnos can be coalesced.  */\n+  coloring_allocno_bitmap = ira_allocate_bitmap ();\n+  for (i = 0; i < n; i++)\n+    {\n+      regno = pseudo_regnos[i];\n+      allocno = ira_regno_allocno_map[regno];\n+      if (allocno != NULL)\n+\tbitmap_set_bit (coloring_allocno_bitmap,\n+\t\t\tALLOCNO_NUM (allocno));\n+    }\n+  allocno_coalesced_p = false;\n+  coalesce_allocnos (true);\n+  ira_free_bitmap (coloring_allocno_bitmap);\n+  regno_coalesced_allocno_cost\n+    = (int *) ira_allocate (max_regno * sizeof (int));\n+  regno_coalesced_allocno_num\n+    = (int *) ira_allocate (max_regno * sizeof (int));\n+  memset (regno_coalesced_allocno_num, 0, max_regno * sizeof (int));\n+  setup_coalesced_allocno_costs_and_nums (pseudo_regnos, n);\n+  /* Sort regnos according frequencies of the corresponding coalesced\n+     allocno sets.  */\n+  qsort (pseudo_regnos, n, sizeof (int), coalesced_pseudo_reg_freq_compare);\n+  spilled_coalesced_allocnos\n+    = (ira_allocno_t *) ira_allocate (ira_allocnos_num\n+\t\t\t\t      * sizeof (ira_allocno_t));\n+  /* Collect allocnos representing the spilled coalesced allocno\n+     sets.  */\n+  num = collect_spilled_coalesced_allocnos (pseudo_regnos, n,\n+\t\t\t\t\t    spilled_coalesced_allocnos);\n+  if (flag_ira_share_spill_slots\n+      && coalesce_spill_slots (spilled_coalesced_allocnos, num))\n+    {\n+      setup_coalesced_allocno_costs_and_nums (pseudo_regnos, n);\n+      qsort (pseudo_regnos, n, sizeof (int),\n+\t     coalesced_pseudo_reg_freq_compare);\n+      num = collect_spilled_coalesced_allocnos (pseudo_regnos, n,\n+\t\t\t\t\t\tspilled_coalesced_allocnos);\n+    }\n+  ira_free_bitmap (processed_coalesced_allocno_bitmap);\n+  allocno_coalesced_p = false;\n+  /* Assign stack slot numbers to spilled allocno sets, use smaller\n+     numbers for most frequently used coalesced allocnos.  -1 is\n+     reserved for dynamic search of stack slots for pseudos spilled by\n+     the reload.  */\n+  slot_num = 1;\n+  for (i = 0; i < num; i++)\n+    {\n+      allocno = spilled_coalesced_allocnos[i];\n+      if (ALLOCNO_FIRST_COALESCED_ALLOCNO (allocno) != allocno\n+\t  || ALLOCNO_HARD_REGNO (allocno) >= 0\n+\t  || (ALLOCNO_REGNO (allocno) < ira_reg_equiv_len\n+\t      && (ira_reg_equiv_invariant_p[ALLOCNO_REGNO (allocno)]\n+\t\t  || ira_reg_equiv_const[ALLOCNO_REGNO (allocno)] != NULL_RTX)))\n+\tcontinue;\n+      if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\tfprintf (ira_dump_file, \"      Slot %d (freq,size):\", slot_num);\n+      slot_num++;\n+      for (a = ALLOCNO_NEXT_COALESCED_ALLOCNO (allocno);;\n+\t   a = ALLOCNO_NEXT_COALESCED_ALLOCNO (a))\n+\t{\n+\t  ira_assert (ALLOCNO_HARD_REGNO (a) < 0);\n+\t  ALLOCNO_HARD_REGNO (a) = -slot_num;\n+\t  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file, \" a%dr%d(%d,%d)\",\n+\t\t     ALLOCNO_NUM (a), ALLOCNO_REGNO (a), ALLOCNO_FREQ (a),\n+\t\t     MAX (PSEUDO_REGNO_BYTES (ALLOCNO_REGNO (a)),\n+\t\t\t  reg_max_ref_width[ALLOCNO_REGNO (a)]));\n+\t      \n+\t  if (a == allocno)\n+\t    break;\n+\t}\n+      if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\tfprintf (ira_dump_file, \"\\n\");\n+    }\n+  ira_spilled_reg_stack_slots_num = slot_num - 1;\n+  ira_free (spilled_coalesced_allocnos);\n+  /* Sort regnos according the slot numbers.  */\n+  regno_max_ref_width = reg_max_ref_width;\n+  qsort (pseudo_regnos, n, sizeof (int), coalesced_pseudo_reg_slot_compare);\n+  /* Uncoalesce allocnos which is necessary for (re)assigning during\n+     the reload pass.  */\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      ALLOCNO_FIRST_COALESCED_ALLOCNO (a) = a;\n+      ALLOCNO_NEXT_COALESCED_ALLOCNO (a) = a;\n+    }\n+  ira_free (regno_coalesced_allocno_num);\n+  ira_free (regno_coalesced_allocno_cost);\n+}\n+\n+\f\n+\n+/* This page contains code used by the reload pass to improve the\n+   final code.  */\n+\n+/* The function is called from reload to mark changes in the\n+   allocation of REGNO made by the reload.  Remember that reg_renumber\n+   reflects the change result.  */\n+void\n+ira_mark_allocation_change (int regno)\n+{\n+  ira_allocno_t a = ira_regno_allocno_map[regno];\n+  int old_hard_regno, hard_regno, cost;\n+  enum reg_class cover_class = ALLOCNO_COVER_CLASS (a);\n+\n+  ira_assert (a != NULL);\n+  hard_regno = reg_renumber[regno];\n+  if ((old_hard_regno = ALLOCNO_HARD_REGNO (a)) == hard_regno)\n+    return;\n+  if (old_hard_regno < 0)\n+    cost = -ALLOCNO_MEMORY_COST (a);\n+  else\n+    {\n+      ira_assert (ira_class_hard_reg_index[cover_class][old_hard_regno] >= 0);\n+      cost = -(ALLOCNO_HARD_REG_COSTS (a) == NULL\n+\t       ? ALLOCNO_COVER_CLASS_COST (a)\n+\t       : ALLOCNO_HARD_REG_COSTS (a)\n+\t         [ira_class_hard_reg_index[cover_class][old_hard_regno]]);\n+      update_copy_costs (a, false);\n+    }\n+  ira_overall_cost -= cost;\n+  ALLOCNO_HARD_REGNO (a) = hard_regno;\n+  if (hard_regno < 0)\n+    {\n+      ALLOCNO_HARD_REGNO (a) = -1;\n+      cost += ALLOCNO_MEMORY_COST (a);\n+    }\n+  else if (ira_class_hard_reg_index[cover_class][hard_regno] >= 0)\n+    {\n+      cost += (ALLOCNO_HARD_REG_COSTS (a) == NULL\n+\t       ? ALLOCNO_COVER_CLASS_COST (a)\n+\t       : ALLOCNO_HARD_REG_COSTS (a)\n+\t         [ira_class_hard_reg_index[cover_class][hard_regno]]);\n+      update_copy_costs (a, true);\n+    }\n+  else\n+    /* Reload changed class of the allocno.  */\n+    cost = 0;\n+  ira_overall_cost += cost;\n+}\n+\n+/* This function is called when reload deletes memory-memory move.  In\n+   this case we marks that the allocation of the corresponding\n+   allocnos should be not changed in future.  Otherwise we risk to get\n+   a wrong code.  */\n+void\n+ira_mark_memory_move_deletion (int dst_regno, int src_regno)\n+{\n+  ira_allocno_t dst = ira_regno_allocno_map[dst_regno];\n+  ira_allocno_t src = ira_regno_allocno_map[src_regno];\n+\n+  ira_assert (dst != NULL && src != NULL\n+\t      && ALLOCNO_HARD_REGNO (dst) < 0\n+\t      && ALLOCNO_HARD_REGNO (src) < 0);\n+  ALLOCNO_DONT_REASSIGN_P (dst) = true;\n+  ALLOCNO_DONT_REASSIGN_P (src) = true;\n+}\n+\n+/* Try to assign a hard register (except for FORBIDDEN_REGS) to\n+   allocno A and return TRUE in the case of success.  That is an\n+   analog of retry_global_alloc for IRA.  */\n+static bool\n+allocno_reload_assign (ira_allocno_t a, HARD_REG_SET forbidden_regs)\n+{\n+  int hard_regno;\n+  enum reg_class cover_class;\n+  int regno = ALLOCNO_REGNO (a);\n+\n+  IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a), forbidden_regs);\n+  if (! flag_caller_saves && ALLOCNO_CALLS_CROSSED_NUM (a) != 0)\n+    IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a), call_used_reg_set);\n+  ALLOCNO_ASSIGNED_P (a) = false;\n+  ira_assert (ALLOCNO_UPDATED_HARD_REG_COSTS (a) == NULL);\n+  ira_assert (ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS (a) == NULL);\n+  cover_class = ALLOCNO_COVER_CLASS (a);\n+  update_curr_costs (a);\n+  assign_hard_reg (a, true);\n+  hard_regno = ALLOCNO_HARD_REGNO (a);\n+  reg_renumber[regno] = hard_regno;\n+  if (hard_regno < 0)\n+    ALLOCNO_HARD_REGNO (a) = -1;\n+  else\n+    {\n+      ira_assert (ira_class_hard_reg_index[cover_class][hard_regno] >= 0);\n+      ira_overall_cost -= (ALLOCNO_MEMORY_COST (a)\n+\t\t\t   - (ALLOCNO_HARD_REG_COSTS (a) == NULL\n+\t\t\t      ? ALLOCNO_COVER_CLASS_COST (a)\n+\t\t\t      : ALLOCNO_HARD_REG_COSTS (a)\n+\t\t\t        [ira_class_hard_reg_index\n+\t\t\t\t [cover_class][hard_regno]]));\n+      if (ALLOCNO_CALLS_CROSSED_NUM (a) != 0\n+\t  && ! ira_hard_reg_not_in_set_p (hard_regno, ALLOCNO_MODE (a),\n+\t\t\t\t\t  call_used_reg_set))\n+\t{\n+\t  ira_assert (flag_caller_saves);\n+\t  caller_save_needed = 1;\n+\t}\n+    }\n+\n+  /* If we found a hard register, modify the RTL for the pseudo\n+     register to show the hard register, and mark the pseudo register\n+     live.  */\n+  if (reg_renumber[regno] >= 0)\n+    {\n+      if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\tfprintf (ira_dump_file, \": reassign to %d\\n\", reg_renumber[regno]);\n+      SET_REGNO (regno_reg_rtx[regno], reg_renumber[regno]);\n+      mark_home_live (regno);\n+    }\n+  else if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+    fprintf (ira_dump_file, \"\\n\");\n+\n+  return reg_renumber[regno] >= 0;\n+}\n+\n+/* Sort pseudos according their usage frequencies (putting most\n+   frequently ones first).  */\n+static int\n+pseudo_reg_compare (const void *v1p, const void *v2p)\n+{\n+  int regno1 = *(const int *) v1p;\n+  int regno2 = *(const int *) v2p;\n+  int diff;\n+\n+  if ((diff = REG_FREQ (regno2) - REG_FREQ (regno1)) != 0)\n+    return diff;\n+  return regno1 - regno2;\n+}\n+\n+/* Try to allocate hard registers to SPILLED_PSEUDO_REGS (there are\n+   NUM of them) or spilled pseudos conflicting with pseudos in\n+   SPILLED_PSEUDO_REGS.  Return TRUE and update SPILLED, if the\n+   allocation has been changed.  The function doesn't use\n+   BAD_SPILL_REGS and hard registers in PSEUDO_FORBIDDEN_REGS and\n+   PSEUDO_PREVIOUS_REGS for the corresponding pseudos.  The function\n+   is called by the reload pass at the end of each reload\n+   iteration.  */\n+bool\n+ira_reassign_pseudos (int *spilled_pseudo_regs, int num,\n+\t\t      HARD_REG_SET bad_spill_regs,\n+\t\t      HARD_REG_SET *pseudo_forbidden_regs,\n+\t\t      HARD_REG_SET *pseudo_previous_regs,  bitmap spilled)\n+{\n+  int i, m, n, regno;\n+  bool changed_p;\n+  ira_allocno_t a, conflict_a;\n+  HARD_REG_SET forbidden_regs;\n+  ira_allocno_conflict_iterator aci;\n+\n+  if (num > 1)\n+    qsort (spilled_pseudo_regs, num, sizeof (int), pseudo_reg_compare);\n+  changed_p = false;\n+  /* Try to assign hard registers to pseudos from\n+     SPILLED_PSEUDO_REGS.  */\n+  for (m = i = 0; i < num; i++)\n+    {\n+      regno = spilled_pseudo_regs[i];\n+      COPY_HARD_REG_SET (forbidden_regs, bad_spill_regs);\n+      IOR_HARD_REG_SET (forbidden_regs, pseudo_forbidden_regs[regno]);\n+      IOR_HARD_REG_SET (forbidden_regs, pseudo_previous_regs[regno]);\n+      gcc_assert (reg_renumber[regno] < 0);\n+      a = ira_regno_allocno_map[regno];\n+      ira_mark_allocation_change (regno);\n+      ira_assert (reg_renumber[regno] < 0);\n+      if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\tfprintf (ira_dump_file,\n+\t\t \"      Spill %d(a%d), cost=%d\", regno, ALLOCNO_NUM (a),\n+\t\t ALLOCNO_MEMORY_COST (a)\n+\t\t - ALLOCNO_COVER_CLASS_COST (a));\n+      allocno_reload_assign (a, forbidden_regs);\n+      if (reg_renumber[regno] >= 0)\n+\t{\n+\t  CLEAR_REGNO_REG_SET (spilled, regno);\n+\t  changed_p = true;\n+\t}\n+      else\n+\tspilled_pseudo_regs[m++] = regno;\n+    }\n+  if (m == 0)\n+    return changed_p;\n+  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+    {\n+      fprintf (ira_dump_file, \"      Spilled regs\");\n+      for (i = 0; i < m; i++)\n+\tfprintf (ira_dump_file, \" %d\", spilled_pseudo_regs[i]);\n+      fprintf (ira_dump_file, \"\\n\");\n+    }\n+  /* Try to assign hard registers to pseudos conflicting with ones\n+     from SPILLED_PSEUDO_REGS.  */\n+  for (i = n = 0; i < m; i++)\n+    {\n+      regno = spilled_pseudo_regs[i];\n+      a = ira_regno_allocno_map[regno];\n+      FOR_EACH_ALLOCNO_CONFLICT (a, conflict_a, aci)\n+\tif (ALLOCNO_HARD_REGNO (conflict_a) < 0\n+\t    && ! ALLOCNO_DONT_REASSIGN_P (conflict_a)\n+\t    && ! bitmap_bit_p (consideration_allocno_bitmap,\n+\t\t\t       ALLOCNO_NUM (conflict_a)))\n+\t  {\n+\t    sorted_allocnos[n++] = conflict_a;\n+\t    bitmap_set_bit (consideration_allocno_bitmap,\n+\t\t\t    ALLOCNO_NUM (conflict_a));\n+\t  }\n+    }\n+  if (n != 0)\n+    {\n+      start_allocno_priorities (sorted_allocnos, n);\n+      qsort (sorted_allocnos, n, sizeof (ira_allocno_t),\n+\t     allocno_priority_compare_func);\n+      for (i = 0; i < n; i++)\n+\t{\n+\t  a = sorted_allocnos[i];\n+\t  regno = ALLOCNO_REGNO (a);\n+\t  COPY_HARD_REG_SET (forbidden_regs, bad_spill_regs);\n+\t  IOR_HARD_REG_SET (forbidden_regs, pseudo_forbidden_regs[regno]);\n+\t  IOR_HARD_REG_SET (forbidden_regs, pseudo_previous_regs[regno]);\n+\t  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file,\n+\t\t     \"        Try assign %d(a%d), cost=%d\",\n+\t\t     regno, ALLOCNO_NUM (a),\n+\t\t     ALLOCNO_MEMORY_COST (a)\n+\t\t     - ALLOCNO_COVER_CLASS_COST (a));\n+\t  if (allocno_reload_assign (a, forbidden_regs))\n+\t    {\n+\t      changed_p = true;\n+\t      bitmap_clear_bit (spilled, regno);\n+\t    }\n+\t}\n+    }\n+  return changed_p;\n+}\n+\n+/* The function is called by reload and returns already allocated\n+   stack slot (if any) for REGNO with given INHERENT_SIZE and\n+   TOTAL_SIZE.  In the case of failure to find a slot which can be\n+   used for REGNO, the function returns NULL.  */\n+rtx\n+ira_reuse_stack_slot (int regno, unsigned int inherent_size,\n+\t\t      unsigned int total_size)\n+{\n+  unsigned int i;\n+  int slot_num, best_slot_num;\n+  int cost, best_cost;\n+  ira_copy_t cp, next_cp;\n+  ira_allocno_t another_allocno, allocno = ira_regno_allocno_map[regno];\n+  rtx x;\n+  bitmap_iterator bi;\n+  struct ira_spilled_reg_stack_slot *slot = NULL;\n+\n+  ira_assert (flag_ira && inherent_size == PSEUDO_REGNO_BYTES (regno)\n+\t      && inherent_size <= total_size\n+\t      && ALLOCNO_HARD_REGNO (allocno) < 0);\n+  if (! flag_ira_share_spill_slots)\n+    return NULL_RTX;\n+  slot_num = -ALLOCNO_HARD_REGNO (allocno) - 2;\n+  if (slot_num != -1)\n+    {\n+      slot = &ira_spilled_reg_stack_slots[slot_num];\n+      x = slot->mem;\n+    }\n+  else\n+    {\n+      best_cost = best_slot_num = -1;\n+      x = NULL_RTX;\n+      /* It means that the pseudo was spilled in the reload pass, try\n+\t to reuse a slot.  */\n+      for (slot_num = 0;\n+\t   slot_num < ira_spilled_reg_stack_slots_num;\n+\t   slot_num++)\n+\t{\n+\t  slot = &ira_spilled_reg_stack_slots[slot_num];\n+\t  if (slot->mem == NULL_RTX)\n+\t    continue;\n+\t  if (slot->width < total_size\n+\t      || GET_MODE_SIZE (GET_MODE (slot->mem)) < inherent_size)\n+\t    continue;\n+\t  \n+\t  EXECUTE_IF_SET_IN_BITMAP (&slot->spilled_regs,\n+\t\t\t\t    FIRST_PSEUDO_REGISTER, i, bi)\n+\t    {\n+\t      another_allocno = ira_regno_allocno_map[i];\n+\t      if (ira_allocno_live_ranges_intersect_p (allocno,\n+\t\t\t\t\t\t       another_allocno))\n+\t\tgoto cont;\n+\t    }\n+\t  for (cost = 0, cp = ALLOCNO_COPIES (allocno);\n+\t       cp != NULL;\n+\t       cp = next_cp)\n+\t    {\n+\t      if (cp->first == allocno)\n+\t\t{\n+\t\t  next_cp = cp->next_first_allocno_copy;\n+\t\t  another_allocno = cp->second;\n+\t\t}\n+\t      else if (cp->second == allocno)\n+\t\t{\n+\t\t  next_cp = cp->next_second_allocno_copy;\n+\t\t  another_allocno = cp->first;\n+\t\t}\n+\t      else\n+\t\tgcc_unreachable ();\n+\t      if (cp->insn == NULL_RTX)\n+\t\tcontinue;\n+\t      if (bitmap_bit_p (&slot->spilled_regs,\n+\t\t\t\tALLOCNO_REGNO (another_allocno)))\n+\t\tcost += cp->freq;\n+\t    }\n+\t  if (cost > best_cost)\n+\t    {\n+\t      best_cost = cost;\n+\t      best_slot_num = slot_num;\n+\t    }\n+\tcont:\n+\t  ;\n+\t}\n+      if (best_cost >= 0)\n+\t{\n+\t  slot = &ira_spilled_reg_stack_slots[best_slot_num];\n+\t  SET_REGNO_REG_SET (&slot->spilled_regs, regno);\n+\t  x = slot->mem;\n+\t  ALLOCNO_HARD_REGNO (allocno) = -best_slot_num - 2;\n+\t}\n+    }\n+  if (x != NULL_RTX)\n+    {\n+      ira_assert (slot->width >= total_size);\n+      EXECUTE_IF_SET_IN_BITMAP (&slot->spilled_regs,\n+\t\t\t\tFIRST_PSEUDO_REGISTER, i, bi)\n+\t{\n+\t  ira_assert (! ira_pseudo_live_ranges_intersect_p (regno, i));\n+\t}\n+      SET_REGNO_REG_SET (&slot->spilled_regs, regno);\n+      if (internal_flag_ira_verbose > 3 && ira_dump_file)\n+\t{\n+\t  fprintf (ira_dump_file, \"      Assigning %d(freq=%d) slot %d of\",\n+\t\t   regno, REG_FREQ (regno), slot_num);\n+\t  EXECUTE_IF_SET_IN_BITMAP (&slot->spilled_regs,\n+\t\t\t\t    FIRST_PSEUDO_REGISTER, i, bi)\n+\t    {\n+\t      if ((unsigned) regno != i)\n+\t\tfprintf (ira_dump_file, \" %d\", i);\n+\t    }\n+\t  fprintf (ira_dump_file, \"\\n\");\n+\t}\n+    }\n+  return x;\n+}\n+\n+/* This is called by reload every time a new stack slot X with\n+   TOTAL_SIZE was allocated for REGNO.  We store this info for\n+   subsequent ira_reuse_stack_slot calls.  */\n+void\n+ira_mark_new_stack_slot (rtx x, int regno, unsigned int total_size)\n+{\n+  struct ira_spilled_reg_stack_slot *slot;\n+  int slot_num;\n+  ira_allocno_t allocno;\n+\n+  ira_assert (flag_ira && PSEUDO_REGNO_BYTES (regno) <= total_size);\n+  allocno = ira_regno_allocno_map[regno];\n+  slot_num = -ALLOCNO_HARD_REGNO (allocno) - 2;\n+  if (slot_num == -1)\n+    {\n+      slot_num = ira_spilled_reg_stack_slots_num++;\n+      ALLOCNO_HARD_REGNO (allocno) = -slot_num - 2;\n+    }\n+  slot = &ira_spilled_reg_stack_slots[slot_num];\n+  INIT_REG_SET (&slot->spilled_regs);\n+  SET_REGNO_REG_SET (&slot->spilled_regs, regno);\n+  slot->mem = x;\n+  slot->width = total_size;\n+  if (internal_flag_ira_verbose > 3 && ira_dump_file)\n+    fprintf (ira_dump_file, \"      Assigning %d(freq=%d) a new slot %d\\n\",\n+\t     regno, REG_FREQ (regno), slot_num);\n+}\n+\n+\n+/* Return spill cost for pseudo-registers whose numbers are in array\n+   REGNOS (with a negative number as an end marker) for reload with\n+   given IN and OUT for INSN.  Return also number points (through\n+   EXCESS_PRESSURE_LIVE_LENGTH) where the pseudo-register lives and\n+   the register pressure is high, number of references of the\n+   pseudo-registers (through NREFS), number of callee-clobbered\n+   hard-registers occupied by the pseudo-registers (through\n+   CALL_USED_COUNT), and the first hard regno occupied by the\n+   pseudo-registers (through FIRST_HARD_REGNO).  */\n+static int\n+calculate_spill_cost (int *regnos, rtx in, rtx out, rtx insn,\n+\t\t      int *excess_pressure_live_length,\n+\t\t      int *nrefs, int *call_used_count, int *first_hard_regno)\n+{\n+  int i, cost, regno, hard_regno, j, count, saved_cost, nregs;\n+  bool in_p, out_p;\n+  int length;\n+  ira_allocno_t a;\n+\n+  *nrefs = 0;\n+  for (length = count = cost = i = 0;; i++)\n+    {\n+      regno = regnos[i];\n+      if (regno < 0)\n+\tbreak;\n+      *nrefs += REG_N_REFS (regno);\n+      hard_regno = reg_renumber[regno];\n+      ira_assert (hard_regno >= 0);\n+      a = ira_regno_allocno_map[regno];\n+      length += ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (a);\n+      cost += ALLOCNO_MEMORY_COST (a) - ALLOCNO_COVER_CLASS_COST (a);\n+      nregs = hard_regno_nregs[hard_regno][ALLOCNO_MODE (a)];\n+      for (j = 0; j < nregs; j++)\n+\tif (! TEST_HARD_REG_BIT (call_used_reg_set, hard_regno + j))\n+\t  break;\n+      if (j == nregs)\n+\tcount++;\n+      in_p = in && REG_P (in) && (int) REGNO (in) == hard_regno;\n+      out_p = out && REG_P (out) && (int) REGNO (out) == hard_regno;\n+      if ((in_p || out_p)\n+\t  && find_regno_note (insn, REG_DEAD, hard_regno) != NULL_RTX)\n+\t{\n+\t  saved_cost = 0;\n+\t  if (in_p)\n+\t    saved_cost += ira_memory_move_cost\n+\t                  [ALLOCNO_MODE (a)][ALLOCNO_COVER_CLASS (a)][1];\n+\t  if (out_p)\n+\t    saved_cost\n+\t      += ira_memory_move_cost\n+\t         [ALLOCNO_MODE (a)][ALLOCNO_COVER_CLASS (a)][0];\n+\t  cost -= REG_FREQ_FROM_BB (BLOCK_FOR_INSN (insn)) * saved_cost;\n+\t}\n+    }\n+  *excess_pressure_live_length = length;\n+  *call_used_count = count;\n+  hard_regno = -1;\n+  if (regnos[0] >= 0)\n+    {\n+      hard_regno = reg_renumber[regnos[0]];\n+    }\n+  *first_hard_regno = hard_regno;\n+  return cost;\n+}\n+\n+/* Return TRUE if spilling pseudo-registers whose numbers are in array\n+   REGNOS is better than spilling pseudo-registers with numbers in\n+   OTHER_REGNOS for reload with given IN and OUT for INSN.  The\n+   function used by the reload pass to make better register spilling\n+   decisions.  */\n+bool\n+ira_better_spill_reload_regno_p (int *regnos, int *other_regnos,\n+\t\t\t\t rtx in, rtx out, rtx insn)\n+{\n+  int cost, other_cost;\n+  int length, other_length;\n+  int nrefs, other_nrefs;\n+  int call_used_count, other_call_used_count;\n+  int hard_regno, other_hard_regno;\n+\n+  cost = calculate_spill_cost (regnos, in, out, insn, \n+\t\t\t       &length, &nrefs, &call_used_count, &hard_regno);\n+  other_cost = calculate_spill_cost (other_regnos, in, out, insn,\n+\t\t\t\t     &other_length, &other_nrefs,\n+\t\t\t\t     &other_call_used_count,\n+\t\t\t\t     &other_hard_regno);\n+  if (nrefs == 0 && other_nrefs != 0)\n+    return true;\n+  if (nrefs != 0 && other_nrefs == 0)\n+    return false;\n+  if (cost != other_cost)\n+    return cost < other_cost;\n+  if (length != other_length)\n+    return length > other_length;\n+#ifdef REG_ALLOC_ORDER\n+  if (hard_regno >= 0 && other_hard_regno >= 0)\n+    return (inv_reg_alloc_order[hard_regno]\n+\t    < inv_reg_alloc_order[other_hard_regno]);\n+#else\n+  if (call_used_count != other_call_used_count)\n+    return call_used_count > other_call_used_count;\n+#endif\n+  return false;\n+}\n+\n+\f\n+\n+/* Allocate and initialize data necessary for assign_hard_reg.  */\n+void\n+ira_initiate_assign (void)\n+{\n+  sorted_allocnos\n+    = (ira_allocno_t *) ira_allocate (sizeof (ira_allocno_t)\n+\t\t\t\t      * ira_allocnos_num);\n+  consideration_allocno_bitmap = ira_allocate_bitmap ();\n+  initiate_cost_update ();\n+  allocno_priorities = (int *) ira_allocate (sizeof (int) * ira_allocnos_num);\n+}\n+\n+/* Deallocate data used by assign_hard_reg.  */\n+void\n+ira_finish_assign (void)\n+{\n+  ira_free (sorted_allocnos);\n+  ira_free_bitmap (consideration_allocno_bitmap);\n+  finish_cost_update ();\n+  ira_free (allocno_priorities);\n+}\n+\n+\f\n+\n+/* Entry function doing color-based register allocation.  */\n+void\n+ira_color (void)\n+{\n+  allocno_stack_vec = VEC_alloc (ira_allocno_t, heap, ira_allocnos_num);\n+  removed_splay_allocno_vec\n+    = VEC_alloc (ira_allocno_t, heap, ira_allocnos_num);\n+  memset (allocated_hardreg_p, 0, sizeof (allocated_hardreg_p));\n+  ira_initiate_assign ();\n+  do_coloring ();\n+  ira_finish_assign ();\n+  VEC_free (ira_allocno_t, heap, removed_splay_allocno_vec);\n+  VEC_free (ira_allocno_t, heap, allocno_stack_vec);\n+  move_spill_restore ();\n+}\n+\n+\f\n+\n+/* This page contains a simple register allocator without usage of\n+   allocno conflicts.  This is used for fast allocation for -O0.  */\n+\n+/* Do register allocation by not using allocno conflicts.  It uses\n+   only allocno live ranges.  The algorithm is close to Chow's\n+   priority coloring.  */\n+void\n+ira_fast_allocation (void)\n+{\n+  int i, j, k, l, num, class_size, hard_regno;\n+#ifdef STACK_REGS\n+  bool no_stack_reg_p;\n+#endif\n+  enum reg_class cover_class;\n+  enum machine_mode mode;\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+  allocno_live_range_t r;\n+  HARD_REG_SET conflict_hard_regs, *used_hard_regs;\n+\n+  allocno_priorities = (int *) ira_allocate (sizeof (int) * ira_allocnos_num);\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      l = ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (a);\n+      if (l <= 0)\n+\tl = 1;\n+      allocno_priorities[ALLOCNO_NUM (a)]\n+\t= (((double) (floor_log2 (ALLOCNO_NREFS (a))\n+\t\t      * (ALLOCNO_MEMORY_COST (a)\n+\t\t\t - ALLOCNO_COVER_CLASS_COST (a))) / l)\n+\t   * (10000 / REG_FREQ_MAX)\n+\t   * ira_reg_class_nregs[ALLOCNO_COVER_CLASS (a)][ALLOCNO_MODE (a)]);\n+    }\n+  used_hard_regs = (HARD_REG_SET *) ira_allocate (sizeof (HARD_REG_SET)\n+\t\t\t\t\t\t  * ira_max_point);\n+  for (i = 0; i < ira_max_point; i++)\n+    CLEAR_HARD_REG_SET (used_hard_regs[i]);\n+  sorted_allocnos = (ira_allocno_t *) ira_allocate (sizeof (ira_allocno_t)\n+\t\t\t\t\t\t    * ira_allocnos_num);\n+  num = 0;\n+  FOR_EACH_ALLOCNO (a, ai)\n+    sorted_allocnos[num++] = a;\n+  qsort (sorted_allocnos, ira_allocnos_num, sizeof (ira_allocno_t), \n+\t allocno_priority_compare_func);\n+  for (i = 0; i < num; i++)\n+    {\n+      a = sorted_allocnos[i];\n+      COPY_HARD_REG_SET (conflict_hard_regs, ALLOCNO_CONFLICT_HARD_REGS (a));\n+      for (r = ALLOCNO_LIVE_RANGES (a); r != NULL; r = r->next)\n+\tfor (j =  r->start; j <= r->finish; j++)\n+\t  IOR_HARD_REG_SET (conflict_hard_regs, used_hard_regs[j]);\n+      cover_class = ALLOCNO_COVER_CLASS (a);\n+      ALLOCNO_ASSIGNED_P (a) = true;\n+      ALLOCNO_HARD_REGNO (a) = -1;\n+      if (hard_reg_set_subset_p (reg_class_contents[cover_class],\n+\t\t\t\t conflict_hard_regs))\n+\tcontinue;\n+      mode = ALLOCNO_MODE (a);\n+#ifdef STACK_REGS\n+      no_stack_reg_p = ALLOCNO_NO_STACK_REG_P (a);\n+#endif\n+      class_size = ira_class_hard_regs_num[cover_class];\n+      for (j = 0; j < class_size; j++)\n+\t{\n+\t  hard_regno = ira_class_hard_regs[cover_class][j];\n+#ifdef STACK_REGS\n+\t  if (no_stack_reg_p && FIRST_STACK_REG <= hard_regno\n+\t      && hard_regno <= LAST_STACK_REG)\n+\t    continue;\n+#endif\n+\t  if (!ira_hard_reg_not_in_set_p (hard_regno, mode, conflict_hard_regs)\n+\t      || (TEST_HARD_REG_BIT\n+\t\t  (prohibited_class_mode_regs[cover_class][mode], hard_regno)))\n+\t    continue;\n+\t  ALLOCNO_HARD_REGNO (a) = hard_regno;\n+\t  for (r = ALLOCNO_LIVE_RANGES (a); r != NULL; r = r->next)\n+\t    for (k = r->start; k <= r->finish; k++)\n+\t      IOR_HARD_REG_SET (used_hard_regs[k],\n+\t\t\t\tira_reg_mode_hard_regset[hard_regno][mode]);\n+\t  break;\n+\t}\n+    }\n+  ira_free (sorted_allocnos);\n+  ira_free (used_hard_regs);\n+  ira_free (allocno_priorities);\n+  if (internal_flag_ira_verbose > 1 && ira_dump_file != NULL)\n+    ira_print_disposition (ira_dump_file);\n+}"}, {"sha": "04d3e42d64d1b2c0110ab7f9f898b7f9708b44dc", "filename": "gcc/ira-conflicts.c", "status": "added", "additions": 777, "deletions": 0, "changes": 777, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-conflicts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-conflicts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira-conflicts.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -0,0 +1,777 @@\n+/* IRA conflict builder.\n+   Copyright (C) 2006, 2007, 2008\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"regs.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"target.h\"\n+#include \"flags.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"toplev.h\"\n+#include \"params.h\"\n+#include \"df.h\"\n+#include \"sparseset.h\"\n+#include \"ira-int.h\"\n+\n+/* This file contains code responsible for allocno conflict creation,\n+   allocno copy creation and allocno info accumulation on upper level\n+   regions.  */\n+\n+/* ira_allocnos_num array of arrays of bits, recording whether two\n+   allocno's conflict (can't go in the same hardware register).\n+\n+   Some arrays will be used as conflict bit vector of the\n+   corresponding allocnos see function build_allocno_conflicts.  */\n+static IRA_INT_TYPE **conflicts;\n+\n+/* Macro to test a conflict of A1 and A2 in `conflicts'.  */\n+#define CONFLICT_ALLOCNO_P(A1, A2)\t\t\t\t\t\\\n+  (ALLOCNO_MIN (A1) <= ALLOCNO_CONFLICT_ID (A2)\t\t\t\t\\\n+   && ALLOCNO_CONFLICT_ID (A2) <= ALLOCNO_MAX (A1)\t\t\t\\\n+   && TEST_ALLOCNO_SET_BIT (conflicts[ALLOCNO_NUM (A1)],\t\t\\\n+\t  \t\t    ALLOCNO_CONFLICT_ID (A2),\t\t\t\\\n+\t\t\t    ALLOCNO_MIN (A1),\t\t\t\t\\\n+\t\t\t    ALLOCNO_MAX (A1)))\n+\n+\f\n+\n+/* Build allocno conflict table by processing allocno live ranges.  */\n+static void\n+build_conflict_bit_table (void)\n+{\n+  int i, num, id, allocated_words_num, conflict_bit_vec_words_num;\n+  unsigned int j;\n+  enum reg_class cover_class;\n+  ira_allocno_t allocno, live_a;\n+  allocno_live_range_t r;\n+  ira_allocno_iterator ai;\n+  sparseset allocnos_live;\n+  int allocno_set_words;\n+\n+  allocno_set_words = (ira_allocnos_num + IRA_INT_BITS - 1) / IRA_INT_BITS;\n+  allocnos_live = sparseset_alloc (ira_allocnos_num);\n+  conflicts = (IRA_INT_TYPE **) ira_allocate (sizeof (IRA_INT_TYPE *)\n+\t\t\t\t\t      * ira_allocnos_num);\n+  allocated_words_num = 0;\n+  FOR_EACH_ALLOCNO (allocno, ai)\n+    {\n+      num = ALLOCNO_NUM (allocno);\n+      if (ALLOCNO_MAX (allocno) < ALLOCNO_MIN (allocno))\n+\t{\n+\t  conflicts[num] = NULL;\n+\t  continue;\n+\t}\n+      conflict_bit_vec_words_num\n+\t= ((ALLOCNO_MAX (allocno) - ALLOCNO_MIN (allocno) + IRA_INT_BITS)\n+\t   / IRA_INT_BITS);\n+      allocated_words_num += conflict_bit_vec_words_num;\n+      conflicts[num]\n+\t= (IRA_INT_TYPE *) ira_allocate (sizeof (IRA_INT_TYPE)\n+\t\t\t\t\t * conflict_bit_vec_words_num);\n+      memset (conflicts[num], 0,\n+\t      sizeof (IRA_INT_TYPE) * conflict_bit_vec_words_num);\n+    }\n+  if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL)\n+    fprintf\n+      (ira_dump_file,\n+       \"+++Allocating %ld bytes for conflict table (uncompressed size %ld)\\n\",\n+       (long) allocated_words_num * sizeof (IRA_INT_TYPE),\n+       (long) allocno_set_words * ira_allocnos_num * sizeof (IRA_INT_TYPE));\n+  for (i = 0; i < ira_max_point; i++)\n+    {\n+      for (r = ira_start_point_ranges[i]; r != NULL; r = r->start_next)\n+\t{\n+\t  allocno = r->allocno;\n+\t  num = ALLOCNO_NUM (allocno);\n+\t  id = ALLOCNO_CONFLICT_ID (allocno);\n+\t  cover_class = ALLOCNO_COVER_CLASS (allocno);\n+\t  sparseset_set_bit (allocnos_live, num);\n+\t  EXECUTE_IF_SET_IN_SPARSESET (allocnos_live, j)\n+\t    {\n+\t      live_a = ira_allocnos[j];\n+\t      if (cover_class == ALLOCNO_COVER_CLASS (live_a)\n+\t\t  /* Don't set up conflict for the allocno with itself.  */\n+\t\t  && num != (int) j)\n+\t\t{\n+\t\t  SET_ALLOCNO_SET_BIT (conflicts[num],\n+\t\t\t\t       ALLOCNO_CONFLICT_ID (live_a),\n+\t\t\t\t       ALLOCNO_MIN (allocno),\n+\t\t\t\t       ALLOCNO_MAX (allocno));\n+\t\t  SET_ALLOCNO_SET_BIT (conflicts[j], id,\n+\t\t\t\t       ALLOCNO_MIN (live_a),\n+\t\t\t\t       ALLOCNO_MAX (live_a));\n+\t\t}\n+\t    }\n+\t}\n+\t  \n+      for (r = ira_finish_point_ranges[i]; r != NULL; r = r->finish_next)\n+\tsparseset_clear_bit (allocnos_live, ALLOCNO_NUM (r->allocno));\n+    }\n+  sparseset_free (allocnos_live);\n+}\n+\n+\f\n+\n+/* Return TRUE if the operand constraint STR is commutative.  */\n+static bool\n+commutative_constraint_p (const char *str)\n+{\n+  bool ignore_p;\n+  int c;\n+\n+  for (ignore_p = false;;)\n+    {\n+      c = *str;\n+      if (c == '\\0')\n+\tbreak;\n+      str += CONSTRAINT_LEN (c, str);\n+      if (c == '#')\n+\tignore_p = true;\n+      else if (c == ',')\n+\tignore_p = false;\n+      else if (! ignore_p)\n+\t{\n+\t  /* Usually `%' is the first constraint character but the\n+\t     documentation does not require this.  */\n+\t  if (c == '%')\n+\t    return true;\n+\t}\n+    }\n+  return false;\n+}\n+\n+/* Return the number of the operand which should be the same in any\n+   case as operand with number OP_NUM (or negative value if there is\n+   no such operand).  If USE_COMMUT_OP_P is TRUE, the function makes\n+   temporarily commutative operand exchange before this.  The function\n+   takes only really possible alternatives into consideration.  */\n+static int\n+get_dup_num (int op_num, bool use_commut_op_p)\n+{\n+  int curr_alt, c, original, dup;\n+  bool ignore_p, commut_op_used_p;\n+  const char *str;\n+  rtx op;\n+\n+  if (op_num < 0 || recog_data.n_alternatives == 0)\n+    return -1;\n+  op = recog_data.operand[op_num];\n+  ira_assert (REG_P (op));\n+  commut_op_used_p = true;\n+  if (use_commut_op_p)\n+    {\n+      if (commutative_constraint_p (recog_data.constraints[op_num]))\n+\top_num++;\n+      else if (op_num > 0 && commutative_constraint_p (recog_data.constraints\n+\t\t\t\t\t\t       [op_num - 1]))\n+\top_num--;\n+      else\n+\tcommut_op_used_p = false;\n+    }\n+  str = recog_data.constraints[op_num];\n+  for (ignore_p = false, original = -1, curr_alt = 0;;)\n+    {\n+      c = *str;\n+      if (c == '\\0')\n+\tbreak;\n+      if (c == '#')\n+\tignore_p = true;\n+      else if (c == ',')\n+\t{\n+\t  curr_alt++;\n+\t  ignore_p = false;\n+\t}\n+      else if (! ignore_p)\n+\tswitch (c)\n+\t  {\n+\t  case 'X':\n+\t    return -1;\n+\t    \n+\t  case 'm':\n+\t  case 'o':\n+\t    /* Accept a register which might be placed in memory.  */\n+\t    return -1;\n+\t    break;\n+\n+\t  case 'V':\n+\t  case '<':\n+\t  case '>':\n+\t    break;\n+\n+\t  case 'p':\n+\t    GO_IF_LEGITIMATE_ADDRESS (VOIDmode, op, win_p);\n+\t    break;\n+\t    \n+\t  win_p:\n+\t    return -1;\n+\t  \n+\t  case 'g':\n+\t    return -1;\n+\t    \n+\t  case 'r':\n+\t  case 'a': case 'b': case 'c': case 'd': case 'e': case 'f':\n+\t  case 'h': case 'j': case 'k': case 'l':\n+\t  case 'q': case 't': case 'u':\n+\t  case 'v': case 'w': case 'x': case 'y': case 'z':\n+\t  case 'A': case 'B': case 'C': case 'D':\n+\t  case 'Q': case 'R': case 'S': case 'T': case 'U':\n+\t  case 'W': case 'Y': case 'Z':\n+\t    {\n+\t      enum reg_class cl;\n+\n+\t      cl = (c == 'r'\n+\t\t    ? GENERAL_REGS : REG_CLASS_FROM_CONSTRAINT (c, str));\n+\t      if (cl != NO_REGS)\n+\t\treturn -1;\n+#ifdef EXTRA_CONSTRAINT_STR\n+\t      else if (EXTRA_CONSTRAINT_STR (op, c, str))\n+\t\treturn -1;\n+#endif\n+\t      break;\n+\t    }\n+\t    \n+\t  case '0': case '1': case '2': case '3': case '4':\n+\t  case '5': case '6': case '7': case '8': case '9':\n+\t    if (original != -1 && original != c)\n+\t      return -1;\n+\t    original = c;\n+\t    break;\n+\t  }\n+      str += CONSTRAINT_LEN (c, str);\n+    }\n+  if (original == -1)\n+    return -1;\n+  dup = original - '0';\n+  if (use_commut_op_p)\n+    {\n+      if (commutative_constraint_p (recog_data.constraints[dup]))\n+\tdup++;\n+      else if (dup > 0\n+\t       && commutative_constraint_p (recog_data.constraints[dup -1]))\n+\tdup--;\n+      else if (! commut_op_used_p)\n+\treturn -1;\n+    }\n+  return dup;\n+}\n+\n+/* Return the operand which should be, in any case, the same as\n+   operand with number OP_NUM.  If USE_COMMUT_OP_P is TRUE, the\n+   function makes temporarily commutative operand exchange before\n+   this.  */\n+static rtx\n+get_dup (int op_num, bool use_commut_op_p)\n+{\n+  int n = get_dup_num (op_num, use_commut_op_p);\n+\n+  if (n < 0)\n+    return NULL_RTX;\n+  else\n+    return recog_data.operand[n];\n+}\n+\n+/* Process registers REG1 and REG2 in move INSN with execution\n+   frequency FREQ.  The function also processes the registers in a\n+   potential move insn (INSN == NULL in this case) with frequency\n+   FREQ.  The function can modify hard register costs of the\n+   corresponding allocnos or create a copy involving the corresponding\n+   allocnos.  The function does nothing if the both registers are hard\n+   registers.  When nothing is changed, the function returns\n+   FALSE.  */\n+static bool\n+process_regs_for_copy (rtx reg1, rtx reg2, rtx insn, int freq)\n+{\n+  int hard_regno, cost, index;\n+  ira_allocno_t a;\n+  enum reg_class rclass, cover_class;\n+  enum machine_mode mode;\n+  ira_copy_t cp;\n+\n+  gcc_assert (REG_P (reg1) && REG_P (reg2));\n+  if (HARD_REGISTER_P (reg1))\n+    {\n+      if (HARD_REGISTER_P (reg2))\n+\treturn false;\n+      hard_regno = REGNO (reg1);\n+      a = ira_curr_regno_allocno_map[REGNO (reg2)];\n+    }\n+  else if (HARD_REGISTER_P (reg2))\n+    {\n+      hard_regno = REGNO (reg2);\n+      a = ira_curr_regno_allocno_map[REGNO (reg1)];\n+    }\n+  else if (!CONFLICT_ALLOCNO_P (ira_curr_regno_allocno_map[REGNO (reg1)],\n+\t\t\t\tira_curr_regno_allocno_map[REGNO (reg2)]))\n+    {\n+      cp = ira_add_allocno_copy (ira_curr_regno_allocno_map[REGNO (reg1)],\n+\t\t\t\t ira_curr_regno_allocno_map[REGNO (reg2)],\n+\t\t\t\t freq, insn, ira_curr_loop_tree_node);\n+      bitmap_set_bit (ira_curr_loop_tree_node->local_copies, cp->num); \n+      return true;\n+    }\n+  else\n+    return false;\n+  rclass = REGNO_REG_CLASS (hard_regno);\n+  mode = ALLOCNO_MODE (a);\n+  cover_class = ALLOCNO_COVER_CLASS (a);\n+  if (! ira_class_subset_p[rclass][cover_class])\n+    return false;\n+  if (reg_class_size[rclass] <= (unsigned) CLASS_MAX_NREGS (rclass, mode))\n+    /* It is already taken into account in ira-costs.c.  */\n+    return false;\n+  index = ira_class_hard_reg_index[cover_class][hard_regno];\n+  if (index < 0)\n+    return false;\n+  if (HARD_REGISTER_P (reg1))\n+    cost = ira_register_move_cost[mode][cover_class][rclass] * freq;\n+  else\n+    cost = ira_register_move_cost[mode][rclass][cover_class] * freq;\n+  ira_allocate_and_set_costs\n+    (&ALLOCNO_HARD_REG_COSTS (a), cover_class,\n+     ALLOCNO_COVER_CLASS_COST (a));\n+  ira_allocate_and_set_costs\n+    (&ALLOCNO_CONFLICT_HARD_REG_COSTS (a), cover_class, 0);\n+  ALLOCNO_HARD_REG_COSTS (a)[index] -= cost;\n+  ALLOCNO_CONFLICT_HARD_REG_COSTS (a)[index] -= cost;\n+  return true;\n+}\n+\n+/* Process all of the output registers of the current insn and\n+   the input register REG (its operand number OP_NUM) which dies in the\n+   insn as if there were a move insn between them with frequency\n+   FREQ.  */\n+static void\n+process_reg_shuffles (rtx reg, int op_num, int freq)\n+{\n+  int i;\n+  rtx another_reg;\n+\n+  gcc_assert (REG_P (reg));\n+  for (i = 0; i < recog_data.n_operands; i++)\n+    {\n+      another_reg = recog_data.operand[i];\n+      \n+      if (!REG_P (another_reg) || op_num == i\n+\t  || recog_data.operand_type[i] != OP_OUT)\n+\tcontinue;\n+      \n+      process_regs_for_copy (reg, another_reg, NULL_RTX, freq);\n+    }\n+}\n+\n+/* Process INSN and create allocno copies if necessary.  For example,\n+   it might be because INSN is a pseudo-register move or INSN is two\n+   operand insn.  */\n+static void\n+add_insn_allocno_copies (rtx insn)\n+{\n+  rtx set, operand, dup;\n+  const char *str;\n+  bool commut_p, bound_p;\n+  int i, j, freq;\n+  \n+  freq = REG_FREQ_FROM_BB (BLOCK_FOR_INSN (insn));\n+  if (freq == 0)\n+    freq = 1;\n+  if ((set = single_set (insn)) != NULL_RTX\n+      && REG_P (SET_DEST (set)) && REG_P (SET_SRC (set))\n+      && ! side_effects_p (set)\n+      && find_reg_note (insn, REG_DEAD, SET_SRC (set)) != NULL_RTX)\n+    process_regs_for_copy (SET_DEST (set), SET_SRC (set), insn, freq);\n+  else\n+    {\n+      extract_insn (insn);\n+      for (i = 0; i < recog_data.n_operands; i++)\n+\t{\n+\t  operand = recog_data.operand[i];\n+\t  if (REG_P (operand)\n+\t      && find_reg_note (insn, REG_DEAD, operand) != NULL_RTX)\n+\t    {\n+\t      str = recog_data.constraints[i];\n+\t      while (*str == ' ' && *str == '\\t')\n+\t\tstr++;\n+\t      bound_p = false;\n+\t      for (j = 0, commut_p = false; j < 2; j++, commut_p = true)\n+\t\tif ((dup = get_dup (i, commut_p)) != NULL_RTX\n+\t\t    && REG_P (dup) && GET_MODE (operand) == GET_MODE (dup)\n+\t\t    && process_regs_for_copy (operand, dup, NULL_RTX, freq))\n+\t\t  bound_p = true;\n+\t      if (bound_p)\n+\t\tcontinue;\n+\t      /* If an operand dies, prefer its hard register for the\n+\t\t output operands by decreasing the hard register cost\n+\t\t or creating the corresponding allocno copies.  The\n+\t\t cost will not correspond to a real move insn cost, so\n+\t\t make the frequency smaller.  */\n+\t      process_reg_shuffles (operand, i, freq < 8 ? 1 : freq / 8);\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Add copies originated from BB given by LOOP_TREE_NODE.  */\n+static void\n+add_copies (ira_loop_tree_node_t loop_tree_node)\n+{\n+  basic_block bb;\n+  rtx insn;\n+\n+  bb = loop_tree_node->bb;\n+  if (bb == NULL)\n+    return;\n+  FOR_BB_INSNS (bb, insn)\n+    if (INSN_P (insn))\n+      add_insn_allocno_copies (insn);\n+}\n+\n+/* Propagate copies the corresponding allocnos on upper loop tree\n+   level.  */\n+static void\n+propagate_copies (void)\n+{\n+  ira_copy_t cp;\n+  ira_copy_iterator ci;\n+  ira_allocno_t a1, a2, parent_a1, parent_a2;\n+  ira_loop_tree_node_t parent;\n+\n+  FOR_EACH_COPY (cp, ci)\n+    {\n+      a1 = cp->first;\n+      a2 = cp->second;\n+      if (ALLOCNO_LOOP_TREE_NODE (a1) == ira_loop_tree_root)\n+\tcontinue;\n+      ira_assert ((ALLOCNO_LOOP_TREE_NODE (a2) != ira_loop_tree_root));\n+      parent = ALLOCNO_LOOP_TREE_NODE (a1)->parent;\n+      if ((parent_a1 = ALLOCNO_CAP (a1)) == NULL)\n+\tparent_a1 = parent->regno_allocno_map[ALLOCNO_REGNO (a1)];\n+      if ((parent_a2 = ALLOCNO_CAP (a2)) == NULL)\n+\tparent_a2 = parent->regno_allocno_map[ALLOCNO_REGNO (a2)];\n+      ira_assert (parent_a1 != NULL && parent_a2 != NULL);\n+      if (! CONFLICT_ALLOCNO_P (parent_a1, parent_a2))\n+\tira_add_allocno_copy (parent_a1, parent_a1, cp->freq,\n+\t\t\t      cp->insn, cp->loop_tree_node);\n+    }\n+}\n+\n+/* Return TRUE if live ranges of allocnos A1 and A2 intersect.  It is\n+   used to find a conflict for new allocnos or allocnos with the\n+   different cover classes.  */\n+bool\n+ira_allocno_live_ranges_intersect_p (ira_allocno_t a1, ira_allocno_t a2)\n+{\n+  allocno_live_range_t r1, r2;\n+\n+  if (a1 == a2)\n+    return false;\n+  if (ALLOCNO_REG (a1) != NULL && ALLOCNO_REG (a2) != NULL\n+      && (ORIGINAL_REGNO (ALLOCNO_REG (a1))\n+\t  == ORIGINAL_REGNO (ALLOCNO_REG (a2))))\n+    return false;\n+  /* Remember the ranges are always kept ordered.  */\n+  for (r1 = ALLOCNO_LIVE_RANGES (a1), r2 = ALLOCNO_LIVE_RANGES (a2);\n+       r1 != NULL && r2 != NULL;)\n+    {\n+      if (r1->start > r2->finish)\n+\tr1 = r1->next;\n+      else if (r2->start > r1->finish)\n+\tr2 = r2->next;\n+      else\n+\treturn true;\n+    }\n+  return false;\n+}\n+\n+/* Return TRUE if live ranges of pseudo-registers REGNO1 and REGNO2\n+   intersect.  This should be used when there is only one region.\n+   Currently this is used during reload.  */\n+bool\n+ira_pseudo_live_ranges_intersect_p (int regno1, int regno2)\n+{\n+  ira_allocno_t a1, a2;\n+\n+  ira_assert (regno1 >= FIRST_PSEUDO_REGISTER\n+\t      && regno2 >= FIRST_PSEUDO_REGISTER);\n+  /* Reg info caclulated by dataflow infrastructure can be different\n+     from one calculated by regclass.  */\n+  if ((a1 = ira_loop_tree_root->regno_allocno_map[regno1]) == NULL\n+      || (a2 = ira_loop_tree_root->regno_allocno_map[regno2]) == NULL)\n+    return false;\n+  return ira_allocno_live_ranges_intersect_p (a1, a2);\n+}\n+\n+/* Array used to collect all conflict allocnos for given allocno.  */\n+static ira_allocno_t *collected_conflict_allocnos;\n+\n+/* Build conflict vectors or bit conflict vectors (whatever is more\n+   profitable) for allocno A from the conflict table and propagate the\n+   conflicts to upper level allocno.  */\n+static void\n+build_allocno_conflicts (ira_allocno_t a)\n+{\n+  int i, px, parent_num;\n+  int conflict_bit_vec_words_num;\n+  ira_loop_tree_node_t parent;\n+  ira_allocno_t parent_a, another_a, another_parent_a;\n+  ira_allocno_t *vec;\n+  IRA_INT_TYPE *allocno_conflicts;\n+  ira_allocno_set_iterator asi;\n+\n+  allocno_conflicts = conflicts[ALLOCNO_NUM (a)];\n+  px = 0;\n+  FOR_EACH_ALLOCNO_IN_SET (allocno_conflicts,\n+\t\t\t   ALLOCNO_MIN (a), ALLOCNO_MAX (a), i, asi)\n+    {\n+      another_a = ira_conflict_id_allocno_map[i];\n+      ira_assert (ALLOCNO_COVER_CLASS (a)\n+\t\t  == ALLOCNO_COVER_CLASS (another_a));\n+      collected_conflict_allocnos[px++] = another_a;\n+    }\n+  if (ira_conflict_vector_profitable_p (a, px))\n+    {\n+      ira_allocate_allocno_conflict_vec (a, px);\n+      vec = (ira_allocno_t*) ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a);\n+      memcpy (vec, collected_conflict_allocnos, sizeof (ira_allocno_t) * px);\n+      vec[px] = NULL;\n+      ALLOCNO_CONFLICT_ALLOCNOS_NUM (a) = px;\n+    }\n+  else\n+    {\n+      ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a) = conflicts[ALLOCNO_NUM (a)];\n+      if (ALLOCNO_MAX (a) < ALLOCNO_MIN (a))\n+\tconflict_bit_vec_words_num = 0;\n+      else\n+\tconflict_bit_vec_words_num\n+\t  = ((ALLOCNO_MAX (a) - ALLOCNO_MIN (a) + IRA_INT_BITS)\n+\t     / IRA_INT_BITS);\n+      ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE (a)\n+\t= conflict_bit_vec_words_num * sizeof (IRA_INT_TYPE);\n+    }\n+  parent = ALLOCNO_LOOP_TREE_NODE (a)->parent;\n+  if ((parent_a = ALLOCNO_CAP (a)) == NULL\n+      && (parent == NULL\n+\t  || (parent_a = parent->regno_allocno_map[ALLOCNO_REGNO (a)])\n+\t  == NULL))\n+    return;\n+  ira_assert (parent != NULL);\n+  ira_assert (ALLOCNO_COVER_CLASS (a) == ALLOCNO_COVER_CLASS (parent_a));\n+  parent_num = ALLOCNO_NUM (parent_a);\n+  FOR_EACH_ALLOCNO_IN_SET (allocno_conflicts,\n+\t\t\t   ALLOCNO_MIN (a), ALLOCNO_MAX (a), i, asi)\n+    {\n+      another_a = ira_conflict_id_allocno_map[i];\n+      ira_assert (ALLOCNO_COVER_CLASS (a)\n+\t\t  == ALLOCNO_COVER_CLASS (another_a));\n+      if ((another_parent_a = ALLOCNO_CAP (another_a)) == NULL\n+\t  && (another_parent_a = (parent->regno_allocno_map\n+\t\t\t\t  [ALLOCNO_REGNO (another_a)])) == NULL)\n+\tcontinue;\n+      ira_assert (ALLOCNO_NUM (another_parent_a) >= 0);\n+      ira_assert (ALLOCNO_COVER_CLASS (another_a)\n+\t\t  == ALLOCNO_COVER_CLASS (another_parent_a));\n+      SET_ALLOCNO_SET_BIT (conflicts[parent_num],\n+\t\t\t   ALLOCNO_CONFLICT_ID (another_parent_a),\n+\t\t\t   ALLOCNO_MIN (parent_a),\n+\t\t\t   ALLOCNO_MAX (parent_a));\n+    }\n+}\n+\n+/* Build conflict vectors or bit conflict vectors (whatever is more\n+   profitable) of all allocnos from the conflict table.  */\n+static void\n+build_conflicts (void)\n+{\n+  int i;\n+  ira_allocno_t a, cap;\n+\n+  collected_conflict_allocnos\n+    = (ira_allocno_t *) ira_allocate (sizeof (ira_allocno_t)\n+\t\t\t\t      * ira_allocnos_num);\n+  for (i = max_reg_num () - 1; i >= FIRST_PSEUDO_REGISTER; i--)\n+    for (a = ira_regno_allocno_map[i];\n+\t a != NULL;\n+\t a = ALLOCNO_NEXT_REGNO_ALLOCNO (a))\n+      {\n+\tbuild_allocno_conflicts (a);\n+\tfor (cap = ALLOCNO_CAP (a); cap != NULL; cap = ALLOCNO_CAP (cap))\n+\t  build_allocno_conflicts (cap);\n+      }\n+  ira_free (collected_conflict_allocnos);\n+}\n+\n+\f\n+\n+/* Print hard reg set SET with TITLE to FILE.  */\n+static void\n+print_hard_reg_set (FILE *file, const char *title, HARD_REG_SET set)\n+{\n+  int i, start;\n+\n+  fprintf (file, title);\n+  for (start = -1, i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+    {\n+      if (TEST_HARD_REG_BIT (set, i))\n+\t{\n+\t  if (i == 0 || ! TEST_HARD_REG_BIT (set, i - 1))\n+\t    start = i;\n+\t}\n+      if (start >= 0\n+\t  && (i == FIRST_PSEUDO_REGISTER - 1 || ! TEST_HARD_REG_BIT (set, i)))\n+\t{\n+\t  if (start == i - 1)\n+\t    fprintf (file, \" %d\", start);\n+\t  else if (start == i - 2)\n+\t    fprintf (file, \" %d %d\", start, start + 1);\n+\t  else\n+\t    fprintf (file, \" %d-%d\", start, i - 1);\n+\t  start = -1;\n+\t}\n+    }\n+  fprintf (file, \"\\n\");\n+}\n+\n+/* Print information about allocno or only regno (if REG_P) conflicts\n+   to FILE.  */\n+static void\n+print_conflicts (FILE *file, bool reg_p)\n+{\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+  HARD_REG_SET conflicting_hard_regs;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      ira_allocno_t conflict_a;\n+      ira_allocno_conflict_iterator aci;\n+      basic_block bb;\n+\n+      if (reg_p)\n+\tfprintf (file, \";; r%d\", ALLOCNO_REGNO (a));\n+      else\n+\t{\n+\t  fprintf (file, \";; a%d(r%d,\", ALLOCNO_NUM (a), ALLOCNO_REGNO (a));\n+\t  if ((bb = ALLOCNO_LOOP_TREE_NODE (a)->bb) != NULL)\n+\t    fprintf (file, \"b%d\", bb->index);\n+\t  else\n+\t    fprintf (file, \"l%d\", ALLOCNO_LOOP_TREE_NODE (a)->loop->num);\n+\t  fprintf (file, \")\");\n+\t}\n+      fprintf (file, \" conflicts:\");\n+      if (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a) != NULL)\n+\tFOR_EACH_ALLOCNO_CONFLICT (a, conflict_a, aci)\n+\t  {\n+\t    if (reg_p)\n+\t      fprintf (file, \" r%d,\", ALLOCNO_REGNO (conflict_a));\n+\t    else\n+\t      {\n+\t\tfprintf (file, \" a%d(r%d,\", ALLOCNO_NUM (conflict_a),\n+\t\t\t ALLOCNO_REGNO (conflict_a));\n+\t\tif ((bb = ALLOCNO_LOOP_TREE_NODE (conflict_a)->bb) != NULL)\n+\t\t  fprintf (file, \"b%d)\", bb->index);\n+\t\telse\n+\t\t  fprintf (file, \"l%d)\",\n+\t\t\t   ALLOCNO_LOOP_TREE_NODE (conflict_a)->loop->num);\n+\t      }\n+\t  }\n+      COPY_HARD_REG_SET (conflicting_hard_regs,\n+\t\t\t ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a));\n+      AND_COMPL_HARD_REG_SET (conflicting_hard_regs, ira_no_alloc_regs);\n+      AND_HARD_REG_SET (conflicting_hard_regs,\n+\t\t\treg_class_contents[ALLOCNO_COVER_CLASS (a)]);\n+      print_hard_reg_set (file, \"\\n;;     total conflict hard regs:\",\n+\t\t\t  conflicting_hard_regs);\n+      COPY_HARD_REG_SET (conflicting_hard_regs,\n+\t\t\t ALLOCNO_CONFLICT_HARD_REGS (a));\n+      AND_COMPL_HARD_REG_SET (conflicting_hard_regs, ira_no_alloc_regs);\n+      AND_HARD_REG_SET (conflicting_hard_regs,\n+\t\t\treg_class_contents[ALLOCNO_COVER_CLASS (a)]);\n+      print_hard_reg_set (file, \";;     conflict hard regs:\",\n+\t\t\t  conflicting_hard_regs);\n+    }\n+  fprintf (file, \"\\n\");\n+}\n+\n+/* Print information about allocno or only regno (if REG_P) conflicts\n+   to stderr.  */\n+void\n+ira_debug_conflicts (bool reg_p)\n+{\n+  print_conflicts (stderr, reg_p);\n+}\n+\n+\f\n+\n+/* Entry function which builds allocno conflicts and allocno copies\n+   and accumulate some allocno info on upper level regions.  */\n+void\n+ira_build_conflicts (void)\n+{\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  if (optimize)\n+    {\n+      build_conflict_bit_table ();\n+      build_conflicts ();\n+      ira_traverse_loop_tree (true, ira_loop_tree_root, NULL, add_copies);\n+      /* We need finished conflict table for the subsequent call.  */\n+      if (flag_ira_algorithm == IRA_ALGORITHM_REGIONAL\n+\t  || flag_ira_algorithm == IRA_ALGORITHM_MIXED)\n+\tpropagate_copies ();\n+      /* Now we can free memory for the conflict table (see function\n+\t build_allocno_conflicts for details).  */\n+      FOR_EACH_ALLOCNO (a, ai)\n+\t{\n+\t  if (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (a) != conflicts[ALLOCNO_NUM (a)])\n+\t    ira_free (conflicts[ALLOCNO_NUM (a)]);\n+\t}\n+      ira_free (conflicts);\n+    }\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      if (ALLOCNO_CALLS_CROSSED_NUM (a) == 0)\n+\tcontinue;\n+      if (! flag_caller_saves)\n+\t{\n+\t  IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a),\n+\t\t\t    call_used_reg_set);\n+\t  if (ALLOCNO_CALLS_CROSSED_NUM (a) != 0)\n+\t    IOR_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (a),\n+\t\t\t      call_used_reg_set);\n+\t}\n+      else\n+\t{\n+\t  IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a),\n+\t\t\t    no_caller_save_reg_set);\n+\t  if (ALLOCNO_CALLS_CROSSED_NUM (a) != 0)\n+\t    IOR_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (a),\n+\t\t\t      no_caller_save_reg_set);\n+\t}\n+    }\n+  if (optimize && internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+    print_conflicts (ira_dump_file, false);\n+}"}, {"sha": "774902035e1f1047f2e6cad5d4b33613b27747c8", "filename": "gcc/ira-costs.c", "status": "added", "additions": 1594, "deletions": 0, "changes": 1594, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-costs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-costs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira-costs.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -0,0 +1,1594 @@\n+/* IRA hard register and memory cost calculation for allocnos.\n+   Copyright (C) 2006, 2007, 2008\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"hard-reg-set.h\"\n+#include \"rtl.h\"\n+#include \"expr.h\"\n+#include \"tm_p.h\"\n+#include \"flags.h\"\n+#include \"basic-block.h\"\n+#include \"regs.h\"\n+#include \"addresses.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"toplev.h\"\n+#include \"target.h\"\n+#include \"params.h\"\n+#include \"ira-int.h\"\n+\n+/* The file contains code is similar to one in regclass but the code\n+   works on the allocno basis.  */\n+\n+#ifdef FORBIDDEN_INC_DEC_CLASSES\n+/* Indexed by n, is TRUE if allocno with number N is used in an\n+   auto-inc or auto-dec context.  */\n+static bool *in_inc_dec;\n+#endif\n+\n+/* The `costs' struct records the cost of using hard registers of each\n+   class considered for the calculation and of using memory for each\n+   allocno.  */\n+struct costs\n+{\n+  int mem_cost;\n+  /* Costs for register classes start here.  We process only some\n+     register classes (cover classes on the 1st cost calculation\n+     iteration and important classes on the 2nd iteration).  */\n+  int cost[1];\n+};\n+\n+/* Initialized once.  It is a maximal possible size of the allocated\n+   struct costs.  */\n+static int max_struct_costs_size;\n+\n+/* Allocated and initialized once, and used to initialize cost values\n+   for each insn.  */\n+static struct costs *init_cost;\n+\n+/* Allocated once, and used for temporary purposes.  */\n+static struct costs *temp_costs;\n+\n+/* Allocated once, and used for the cost calculation.  */\n+static struct costs *op_costs[MAX_RECOG_OPERANDS];\n+static struct costs *this_op_costs[MAX_RECOG_OPERANDS];\n+\n+/* Original and accumulated costs of each class for each allocno.  */\n+static struct costs *allocno_costs, *total_costs;\n+\n+/* Classes used for cost calculation.  They may be different on\n+   different iterations of the cost calculations or in different\n+   optimization modes.  */\n+static enum reg_class *cost_classes;\n+\n+/* The size of the previous array.  */\n+static int cost_classes_num;\n+\n+/* Map: cost class -> order number (they start with 0) of the cost\n+   class.  */\n+static int cost_class_nums[N_REG_CLASSES];\n+\n+/* It is the current size of struct costs.  */\n+static int struct_costs_size;\n+\n+/* Return pointer to structure containing costs of allocno with given\n+   NUM in array ARR.  */\n+#define COSTS_OF_ALLOCNO(arr, num) \\\n+  ((struct costs *) ((char *) (arr) + (num) * struct_costs_size))\n+\n+/* Record register class preferences of each allocno.  Null value\n+   means no preferences.  It happens on the 1st iteration of the cost\n+   calculation.  */\n+static enum reg_class *allocno_pref;\n+\n+/* Allocated buffers for allocno_pref.  */\n+static enum reg_class *allocno_pref_buffer;\n+\n+/* Execution frequency of the current insn.  */\n+static int frequency;\n+\n+\f\n+\n+/* Compute the cost of loading X into (if TO_P is TRUE) or from (if\n+   TO_P is FALSE) a register of class RCLASS in mode MODE.  X must not\n+   be a pseudo register.  */\n+static int\n+copy_cost (rtx x, enum machine_mode mode, enum reg_class rclass, bool to_p,\n+\t   secondary_reload_info *prev_sri)\n+{\n+  secondary_reload_info sri;\n+  enum reg_class secondary_class = NO_REGS;\n+\n+  /* If X is a SCRATCH, there is actually nothing to move since we are\n+     assuming optimal allocation.  */\n+  if (GET_CODE (x) == SCRATCH)\n+    return 0;\n+\n+  /* Get the class we will actually use for a reload.  */\n+  rclass = PREFERRED_RELOAD_CLASS (x, rclass);\n+\n+  /* If we need a secondary reload for an intermediate, the cost is\n+     that to load the input into the intermediate register, then to\n+     copy it.  */\n+  sri.prev_sri = prev_sri;\n+  sri.extra_cost = 0;\n+  secondary_class = targetm.secondary_reload (to_p, x, rclass, mode, &sri);\n+\n+  if (ira_register_move_cost[mode] == NULL)\n+    ira_init_register_move_cost (mode);\n+\n+  if (secondary_class != NO_REGS)\n+    return (move_cost[mode][secondary_class][rclass] + sri.extra_cost\n+\t    + copy_cost (x, mode, secondary_class, to_p, &sri));\n+\n+  /* For memory, use the memory move cost, for (hard) registers, use\n+     the cost to move between the register classes, and use 2 for\n+     everything else (constants).  */\n+  if (MEM_P (x) || rclass == NO_REGS)\n+    return sri.extra_cost + ira_memory_move_cost[mode][rclass][to_p != 0];\n+  else if (REG_P (x))\n+    return\n+      (sri.extra_cost + move_cost[mode][REGNO_REG_CLASS (REGNO (x))][rclass]);\n+  else\n+    /* If this is a constant, we may eventually want to call rtx_cost\n+       here.  */\n+    return sri.extra_cost + COSTS_N_INSNS (1);\n+}\n+\n+\f\n+\n+/* Record the cost of using memory or hard registers of various\n+   classes for the operands in INSN.\n+\n+   N_ALTS is the number of alternatives.\n+   N_OPS is the number of operands.\n+   OPS is an array of the operands.\n+   MODES are the modes of the operands, in case any are VOIDmode.\n+   CONSTRAINTS are the constraints to use for the operands.  This array\n+   is modified by this procedure.\n+\n+   This procedure works alternative by alternative.  For each\n+   alternative we assume that we will be able to allocate all allocnos\n+   to their ideal register class and calculate the cost of using that\n+   alternative.  Then we compute, for each operand that is a\n+   pseudo-register, the cost of having the allocno allocated to each\n+   register class and using it in that alternative.  To this cost is\n+   added the cost of the alternative.\n+\n+   The cost of each class for this insn is its lowest cost among all\n+   the alternatives.  */\n+static void\n+record_reg_classes (int n_alts, int n_ops, rtx *ops,\n+\t\t    enum machine_mode *modes, const char **constraints,\n+\t\t    rtx insn, struct costs **op_costs,\n+\t\t    enum reg_class *allocno_pref)\n+{\n+  int alt;\n+  int i, j, k;\n+  rtx set;\n+\n+  /* Process each alternative, each time minimizing an operand's cost\n+     with the cost for each operand in that alternative.  */\n+  for (alt = 0; alt < n_alts; alt++)\n+    {\n+      enum reg_class classes[MAX_RECOG_OPERANDS];\n+      int allows_mem[MAX_RECOG_OPERANDS];\n+      int rclass;\n+      int alt_fail = 0;\n+      int alt_cost = 0, op_cost_add;\n+\n+      for (i = 0; i < n_ops; i++)\n+\t{\n+\t  unsigned char c;\n+\t  const char *p = constraints[i];\n+\t  rtx op = ops[i];\n+\t  enum machine_mode mode = modes[i];\n+\t  int allows_addr = 0;\n+\t  int win = 0;\n+\n+\t  /* Initially show we know nothing about the register class.  */\n+\t  classes[i] = NO_REGS;\n+\t  allows_mem[i] = 0;\n+\n+\t  /* If this operand has no constraints at all, we can\n+\t     conclude nothing about it since anything is valid.  */\n+\t  if (*p == 0)\n+\t    {\n+\t      if (REG_P (op) && REGNO (op) >= FIRST_PSEUDO_REGISTER)\n+\t\tmemset (this_op_costs[i], 0, struct_costs_size);\n+\t      continue;\n+\t    }\n+\n+\t  /* If this alternative is only relevant when this operand\n+\t     matches a previous operand, we do different things\n+\t     depending on whether this operand is a allocno-reg or not.\n+\t     We must process any modifiers for the operand before we\n+\t     can make this test.  */\n+\t  while (*p == '%' || *p == '=' || *p == '+' || *p == '&')\n+\t    p++;\n+\n+\t  if (p[0] >= '0' && p[0] <= '0' + i && (p[1] == ',' || p[1] == 0))\n+\t    {\n+\t      /* Copy class and whether memory is allowed from the\n+\t\t matching alternative.  Then perform any needed cost\n+\t\t computations and/or adjustments.  */\n+\t      j = p[0] - '0';\n+\t      classes[i] = classes[j];\n+\t      allows_mem[i] = allows_mem[j];\n+\n+\t      if (! REG_P (op) || REGNO (op) < FIRST_PSEUDO_REGISTER)\n+\t\t{\n+\t\t  /* If this matches the other operand, we have no\n+\t\t     added cost and we win.  */\n+\t\t  if (rtx_equal_p (ops[j], op))\n+\t\t    win = 1;\n+\t\t  /* If we can put the other operand into a register,\n+\t\t     add to the cost of this alternative the cost to\n+\t\t     copy this operand to the register used for the\n+\t\t     other operand.  */\n+\t\t  else if (classes[j] != NO_REGS)\n+\t\t    {\n+\t\t      alt_cost += copy_cost (op, mode, classes[j], 1, NULL);\n+\t\t      win = 1;\n+\t\t    }\n+\t\t}\n+\t      else if (! REG_P (ops[j])\n+\t\t       || REGNO (ops[j]) < FIRST_PSEUDO_REGISTER)\n+\t\t{\n+\t\t  /* This op is an allocno but the one it matches is\n+\t\t     not.  */\n+\n+\t\t  /* If we can't put the other operand into a\n+\t\t     register, this alternative can't be used.  */\n+\n+\t\t  if (classes[j] == NO_REGS)\n+\t\t    alt_fail = 1;\n+\t\t  /* Otherwise, add to the cost of this alternative\n+\t\t     the cost to copy the other operand to the hard\n+\t\t     register used for this operand.  */\n+\t\t  else\n+\t\t    alt_cost += copy_cost (ops[j], mode, classes[j], 1, NULL);\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  /* The costs of this operand are not the same as the\n+\t\t     other operand since move costs are not symmetric.\n+\t\t     Moreover, if we cannot tie them, this alternative\n+\t\t     needs to do a copy, which is one insn.  */\n+\t\t  struct costs *pp = this_op_costs[i];\n+\n+\t\t  if (ira_register_move_cost[mode] == NULL)\n+\t\t    ira_init_register_move_cost (mode);\n+\n+\t\t  for (k = 0; k < cost_classes_num; k++)\n+\t\t    {\n+\t\t      rclass = cost_classes[k];\n+\t\t      pp->cost[k]\n+\t\t\t= ((recog_data.operand_type[i] != OP_OUT\n+\t\t\t    ? ira_may_move_in_cost[mode][rclass]\n+\t\t\t      [classes[i]] * frequency : 0)\n+\t\t\t   + (recog_data.operand_type[i] != OP_IN\n+\t\t\t      ? ira_may_move_out_cost[mode][classes[i]]\n+\t\t\t        [rclass] * frequency : 0));\n+\t\t    }\n+\n+\t\t  /* If the alternative actually allows memory, make\n+\t\t     things a bit cheaper since we won't need an extra\n+\t\t     insn to load it.  */\n+\t\t  pp->mem_cost\n+\t\t    = ((recog_data.operand_type[i] != OP_IN\n+\t\t\t? ira_memory_move_cost[mode][classes[i]][0] : 0)\n+\t\t       + (recog_data.operand_type[i] != OP_OUT\n+\t\t\t  ? ira_memory_move_cost[mode][classes[i]][1] : 0)\n+\t\t       - allows_mem[i]) * frequency;\n+\t\t  /* If we have assigned a class to this allocno in our\n+\t\t     first pass, add a cost to this alternative\n+\t\t     corresponding to what we would add if this allocno\n+\t\t     were not in the appropriate class.  We could use\n+\t\t     cover class here but it is less accurate\n+\t\t     approximation.  */\n+\t\t  if (allocno_pref)\n+\t\t    {\n+\t\t      enum reg_class pref_class\n+\t\t\t= allocno_pref[ALLOCNO_NUM\n+\t\t\t\t       (ira_curr_regno_allocno_map\n+\t\t\t\t\t[REGNO (op)])];\n+\n+\t\t      if (pref_class == NO_REGS)\n+\t\t\talt_cost\n+\t\t\t  += ((recog_data.operand_type[i] != OP_IN\n+\t\t\t       ? ira_memory_move_cost[mode][classes[i]][0]\n+\t\t\t       : 0)\n+\t\t\t      + (recog_data.operand_type[i] != OP_OUT\n+\t\t\t\t ? ira_memory_move_cost[mode][classes[i]][1]\n+\t\t\t\t : 0));\n+\t\t      else if (ira_reg_class_intersect\n+\t\t\t       [pref_class][classes[i]] == NO_REGS)\n+\t\t\talt_cost += (ira_register_move_cost\n+\t\t\t\t     [mode][pref_class][classes[i]]);\n+\t\t    }\n+\t\t  if (REGNO (ops[i]) != REGNO (ops[j])\n+\t\t      && ! find_reg_note (insn, REG_DEAD, op))\n+\t\t    alt_cost += 2;\n+\n+\t\t  /* This is in place of ordinary cost computation for\n+\t\t     this operand, so skip to the end of the\n+\t\t     alternative (should be just one character).  */\n+\t\t  while (*p && *p++ != ',')\n+\t\t    ;\n+\n+\t\t  constraints[i] = p;\n+\t\t  continue;\n+\t\t}\n+\t    }\n+\t  \n+\t  /* Scan all the constraint letters.  See if the operand\n+\t     matches any of the constraints.  Collect the valid\n+\t     register classes and see if this operand accepts\n+\t     memory.  */\n+\t  while ((c = *p))\n+\t    {\n+\t      switch (c)\n+\t\t{\n+\t\tcase ',':\n+\t\t  break;\n+\t\tcase '*':\n+\t\t  /* Ignore the next letter for this pass.  */\n+\t\t  c = *++p;\n+\t\t  break;\n+\n+\t\tcase '?':\n+\t\t  alt_cost += 2;\n+\t\tcase '!':  case '#':  case '&':\n+\t\tcase '0':  case '1':  case '2':  case '3':  case '4':\n+\t\tcase '5':  case '6':  case '7':  case '8':  case '9':\n+\t\t  break;\n+\n+\t\tcase 'p':\n+\t\t  allows_addr = 1;\n+\t\t  win = address_operand (op, GET_MODE (op));\n+\t\t  /* We know this operand is an address, so we want it\n+\t\t     to be allocated to a register that can be the\n+\t\t     base of an address, i.e. BASE_REG_CLASS.  */\n+\t\t  classes[i]\n+\t\t    = ira_reg_class_union[classes[i]]\n+\t\t      [base_reg_class (VOIDmode, ADDRESS, SCRATCH)];\n+\t\t  break;\n+\n+\t\tcase 'm':  case 'o':  case 'V':\n+\t\t  /* It doesn't seem worth distinguishing between\n+\t\t     offsettable and non-offsettable addresses\n+\t\t     here.  */\n+\t\t  allows_mem[i] = 1;\n+\t\t  if (MEM_P (op))\n+\t\t    win = 1;\n+\t\t  break;\n+\n+\t\tcase '<':\n+\t\t  if (MEM_P (op)\n+\t\t      && (GET_CODE (XEXP (op, 0)) == PRE_DEC\n+\t\t\t  || GET_CODE (XEXP (op, 0)) == POST_DEC))\n+\t\t    win = 1;\n+\t\t  break;\n+\n+\t\tcase '>':\n+\t\t  if (MEM_P (op)\n+\t\t      && (GET_CODE (XEXP (op, 0)) == PRE_INC\n+\t\t\t  || GET_CODE (XEXP (op, 0)) == POST_INC))\n+\t\t    win = 1;\n+\t\t  break;\n+\n+\t\tcase 'E':\n+\t\tcase 'F':\n+\t\t  if (GET_CODE (op) == CONST_DOUBLE\n+\t\t      || (GET_CODE (op) == CONST_VECTOR\n+\t\t\t  && (GET_MODE_CLASS (GET_MODE (op))\n+\t\t\t      == MODE_VECTOR_FLOAT)))\n+\t\t    win = 1;\n+\t\t  break;\n+\n+\t\tcase 'G':\n+\t\tcase 'H':\n+\t\t  if (GET_CODE (op) == CONST_DOUBLE\n+\t\t      && CONST_DOUBLE_OK_FOR_CONSTRAINT_P (op, c, p))\n+\t\t    win = 1;\n+\t\t  break;\n+\n+\t\tcase 's':\n+\t\t  if (GET_CODE (op) == CONST_INT\n+\t\t      || (GET_CODE (op) == CONST_DOUBLE\n+\t\t\t  && GET_MODE (op) == VOIDmode))\n+\t\t    break;\n+\n+\t\tcase 'i':\n+\t\t  if (CONSTANT_P (op)\n+\t\t      && (! flag_pic || LEGITIMATE_PIC_OPERAND_P (op)))\n+\t\t    win = 1;\n+\t\t  break;\n+\n+\t\tcase 'n':\n+\t\t  if (GET_CODE (op) == CONST_INT\n+\t\t      || (GET_CODE (op) == CONST_DOUBLE\n+\t\t\t  && GET_MODE (op) == VOIDmode))\n+\t\t    win = 1;\n+\t\t  break;\n+\n+\t\tcase 'I':\n+\t\tcase 'J':\n+\t\tcase 'K':\n+\t\tcase 'L':\n+\t\tcase 'M':\n+\t\tcase 'N':\n+\t\tcase 'O':\n+\t\tcase 'P':\n+\t\t  if (GET_CODE (op) == CONST_INT\n+\t\t      && CONST_OK_FOR_CONSTRAINT_P (INTVAL (op), c, p))\n+\t\t    win = 1;\n+\t\t  break;\n+\n+\t\tcase 'X':\n+\t\t  win = 1;\n+\t\t  break;\n+\n+\t\tcase 'g':\n+\t\t  if (MEM_P (op)\n+\t\t      || (CONSTANT_P (op)\n+\t\t\t  && (! flag_pic || LEGITIMATE_PIC_OPERAND_P (op))))\n+\t\t    win = 1;\n+\t\t  allows_mem[i] = 1;\n+\t\tcase 'r':\n+\t\t  classes[i] = ira_reg_class_union[classes[i]][GENERAL_REGS];\n+\t\t  break;\n+\n+\t\tdefault:\n+\t\t  if (REG_CLASS_FROM_CONSTRAINT (c, p) != NO_REGS)\n+\t\t    classes[i] = ira_reg_class_union[classes[i]]\n+\t\t                 [REG_CLASS_FROM_CONSTRAINT (c, p)];\n+#ifdef EXTRA_CONSTRAINT_STR\n+\t\t  else if (EXTRA_CONSTRAINT_STR (op, c, p))\n+\t\t    win = 1;\n+\n+\t\t  if (EXTRA_MEMORY_CONSTRAINT (c, p))\n+\t\t    {\n+\t\t      /* Every MEM can be reloaded to fit.  */\n+\t\t      allows_mem[i] = 1;\n+\t\t      if (MEM_P (op))\n+\t\t\twin = 1;\n+\t\t    }\n+\t\t  if (EXTRA_ADDRESS_CONSTRAINT (c, p))\n+\t\t    {\n+\t\t      /* Every address can be reloaded to fit.  */\n+\t\t      allows_addr = 1;\n+\t\t      if (address_operand (op, GET_MODE (op)))\n+\t\t\twin = 1;\n+\t\t      /* We know this operand is an address, so we\n+\t\t\t want it to be allocated to a hard register\n+\t\t\t that can be the base of an address,\n+\t\t\t i.e. BASE_REG_CLASS.  */\n+\t\t      classes[i]\n+\t\t\t= ira_reg_class_union[classes[i]]\n+\t\t\t  [base_reg_class (VOIDmode, ADDRESS, SCRATCH)];\n+\t\t    }\n+#endif\n+\t\t  break;\n+\t\t}\n+\t      p += CONSTRAINT_LEN (c, p);\n+\t      if (c == ',')\n+\t\tbreak;\n+\t    }\n+\n+\t  constraints[i] = p;\n+\n+\t  /* How we account for this operand now depends on whether it\n+\t     is a pseudo register or not.  If it is, we first check if\n+\t     any register classes are valid.  If not, we ignore this\n+\t     alternative, since we want to assume that all allocnos get\n+\t     allocated for register preferencing.  If some register\n+\t     class is valid, compute the costs of moving the allocno\n+\t     into that class.  */\n+\t  if (REG_P (op) && REGNO (op) >= FIRST_PSEUDO_REGISTER)\n+\t    {\n+\t      if (classes[i] == NO_REGS)\n+\t\t{\n+\t\t  /* We must always fail if the operand is a REG, but\n+\t\t     we did not find a suitable class.\n+\n+\t\t     Otherwise we may perform an uninitialized read\n+\t\t     from this_op_costs after the `continue' statement\n+\t\t     below.  */\n+\t\t  alt_fail = 1;\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  struct costs *pp = this_op_costs[i];\n+\n+\t\t  if (ira_register_move_cost[mode] == NULL)\n+\t\t    ira_init_register_move_cost (mode);\n+\n+\t\t  for (k = 0; k < cost_classes_num; k++)\n+\t\t    {\n+\t\t      rclass = cost_classes[k];\n+\t\t      pp->cost[k]\n+\t\t\t= ((recog_data.operand_type[i] != OP_OUT\n+\t\t\t    ? ira_may_move_in_cost[mode][rclass]\n+\t\t\t      [classes[i]] * frequency : 0)\n+\t\t\t   + (recog_data.operand_type[i] != OP_IN\n+\t\t\t      ? ira_may_move_out_cost[mode][classes[i]]\n+\t\t\t        [rclass] * frequency : 0));\n+\t\t    }\n+\n+\t\t  /* If the alternative actually allows memory, make\n+\t\t     things a bit cheaper since we won't need an extra\n+\t\t     insn to load it.  */\n+\t\t  pp->mem_cost\n+\t\t    = ((recog_data.operand_type[i] != OP_IN\n+\t\t\t? ira_memory_move_cost[mode][classes[i]][0] : 0)\n+\t\t       + (recog_data.operand_type[i] != OP_OUT\n+\t\t\t  ? ira_memory_move_cost[mode][classes[i]][1] : 0)\n+\t\t       - allows_mem[i]) * frequency;\n+\t\t  /* If we have assigned a class to this allocno in our\n+\t\t     first pass, add a cost to this alternative\n+\t\t     corresponding to what we would add if this allocno\n+\t\t     were not in the appropriate class.  We could use\n+\t\t     cover class here but it is less accurate\n+\t\t     approximation.  */\n+\t\t  if (allocno_pref)\n+\t\t    {\n+\t\t      enum reg_class pref_class\n+\t\t\t= allocno_pref[ALLOCNO_NUM\n+\t\t\t\t       (ira_curr_regno_allocno_map\n+\t\t\t\t\t[REGNO (op)])];\n+\n+\t\t      if (pref_class == NO_REGS)\n+\t\t\talt_cost\n+\t\t\t  += ((recog_data.operand_type[i] != OP_IN\n+\t\t\t       ? ira_memory_move_cost[mode][classes[i]][0]\n+\t\t\t       : 0)\n+\t\t\t      + (recog_data.operand_type[i] != OP_OUT\n+\t\t\t\t ? ira_memory_move_cost[mode][classes[i]][1]\n+\t\t\t\t : 0));\n+\t\t      else if (ira_reg_class_intersect[pref_class][classes[i]]\n+\t\t\t       == NO_REGS)\n+\t\t\talt_cost += (ira_register_move_cost\n+\t\t\t\t     [mode][pref_class][classes[i]]);\n+\t\t    }\n+\t\t}\n+\t    }\n+\n+\t  /* Otherwise, if this alternative wins, either because we\n+\t     have already determined that or if we have a hard\n+\t     register of the proper class, there is no cost for this\n+\t     alternative.  */\n+\t  else if (win || (REG_P (op)\n+\t\t\t   && reg_fits_class_p (op, classes[i],\n+\t\t\t\t\t\t0, GET_MODE (op))))\n+\t    ;\n+\n+\t  /* If registers are valid, the cost of this alternative\n+\t     includes copying the object to and/or from a\n+\t     register.  */\n+\t  else if (classes[i] != NO_REGS)\n+\t    {\n+\t      if (recog_data.operand_type[i] != OP_OUT)\n+\t\talt_cost += copy_cost (op, mode, classes[i], 1, NULL);\n+\n+\t      if (recog_data.operand_type[i] != OP_IN)\n+\t\talt_cost += copy_cost (op, mode, classes[i], 0, NULL);\n+\t    }\n+\t  /* The only other way this alternative can be used is if\n+\t     this is a constant that could be placed into memory.  */\n+\t  else if (CONSTANT_P (op) && (allows_addr || allows_mem[i]))\n+\t    alt_cost += ira_memory_move_cost[mode][classes[i]][1];\n+\t  else\n+\t    alt_fail = 1;\n+\t}\n+\n+      if (alt_fail)\n+\tcontinue;\n+\n+      op_cost_add = alt_cost * frequency;\n+      /* Finally, update the costs with the information we've\n+\t calculated about this alternative.  */\n+      for (i = 0; i < n_ops; i++)\n+\tif (REG_P (ops[i]) && REGNO (ops[i]) >= FIRST_PSEUDO_REGISTER)\n+\t  {\n+\t    struct costs *pp = op_costs[i], *qq = this_op_costs[i];\n+\t    int scale = 1 + (recog_data.operand_type[i] == OP_INOUT);\n+\n+\t    pp->mem_cost = MIN (pp->mem_cost,\n+\t\t\t\t(qq->mem_cost + op_cost_add) * scale);\n+\n+\t    for (k = 0; k < cost_classes_num; k++)\n+\t      pp->cost[k]\n+\t\t= MIN (pp->cost[k], (qq->cost[k] + op_cost_add) * scale);\n+\t  }\n+    }\n+\n+  /* If this insn is a single set copying operand 1 to operand 0 and\n+     one operand is an allocno with the other a hard reg or an allocno\n+     that prefers a hard register that is in its own register class\n+     then we may want to adjust the cost of that register class to -1.\n+\n+     Avoid the adjustment if the source does not die to avoid\n+     stressing of register allocator by preferrencing two colliding\n+     registers into single class.\n+\n+     Also avoid the adjustment if a copy between hard registers of the\n+     class is expensive (ten times the cost of a default copy is\n+     considered arbitrarily expensive).  This avoids losing when the\n+     preferred class is very expensive as the source of a copy\n+     instruction.  */\n+  if ((set = single_set (insn)) != 0\n+      && ops[0] == SET_DEST (set) && ops[1] == SET_SRC (set)\n+      && REG_P (ops[0]) && REG_P (ops[1])\n+      && find_regno_note (insn, REG_DEAD, REGNO (ops[1])))\n+    for (i = 0; i <= 1; i++)\n+      if (REGNO (ops[i]) >= FIRST_PSEUDO_REGISTER)\n+\t{\n+\t  unsigned int regno = REGNO (ops[!i]);\n+\t  enum machine_mode mode = GET_MODE (ops[!i]);\n+\t  int rclass;\n+\t  unsigned int nr;\n+\n+\t  if (regno < FIRST_PSEUDO_REGISTER)\n+\t    for (k = 0; k < cost_classes_num; k++)\n+\t      {\n+\t\trclass = cost_classes[k];\n+\t\tif (TEST_HARD_REG_BIT (reg_class_contents[rclass], regno)\n+\t\t    && (reg_class_size[rclass]\n+\t\t\t== (unsigned) CLASS_MAX_NREGS (rclass, mode)))\n+\t\t  {\n+\t\t    if (reg_class_size[rclass] == 1)\n+\t\t      op_costs[i]->cost[k] = -frequency;\n+\t\t    else\n+\t\t      {\n+\t\t\tfor (nr = 0;\n+\t\t\t     nr < (unsigned) hard_regno_nregs[regno][mode];\n+\t\t\t     nr++)\n+\t\t\t  if (! TEST_HARD_REG_BIT (reg_class_contents[rclass],\n+\t\t\t\t\t\t   regno + nr))\n+\t\t\t    break;\n+\t\t\t\n+\t\t\tif (nr == (unsigned) hard_regno_nregs[regno][mode])\n+\t\t\t  op_costs[i]->cost[k] = -frequency;\n+\t\t      }\n+\t\t  }\n+\t      }\n+\t}\n+}\n+\n+\f\n+\n+/* Wrapper around REGNO_OK_FOR_INDEX_P, to allow pseudo registers.  */\n+static inline bool\n+ok_for_index_p_nonstrict (rtx reg)\n+{\n+  unsigned regno = REGNO (reg);\n+\n+  return regno >= FIRST_PSEUDO_REGISTER || REGNO_OK_FOR_INDEX_P (regno);\n+}\n+\n+/* A version of regno_ok_for_base_p for use here, when all\n+   pseudo-registers should count as OK.  Arguments as for\n+   regno_ok_for_base_p.  */\n+static inline bool\n+ok_for_base_p_nonstrict (rtx reg, enum machine_mode mode,\n+\t\t\t enum rtx_code outer_code, enum rtx_code index_code)\n+{\n+  unsigned regno = REGNO (reg);\n+\n+  if (regno >= FIRST_PSEUDO_REGISTER)\n+    return true;\n+  return ok_for_base_p_1 (regno, mode, outer_code, index_code);\n+}\n+\n+/* Record the pseudo registers we must reload into hard registers in a\n+   subexpression of a memory address, X.\n+\n+   If CONTEXT is 0, we are looking at the base part of an address,\n+   otherwise we are looking at the index part.\n+\n+   MODE is the mode of the memory reference; OUTER_CODE and INDEX_CODE\n+   give the context that the rtx appears in.  These three arguments\n+   are passed down to base_reg_class.\n+\n+   SCALE is twice the amount to multiply the cost by (it is twice so\n+   we can represent half-cost adjustments).  */\n+static void\n+record_address_regs (enum machine_mode mode, rtx x, int context,\n+\t\t     enum rtx_code outer_code, enum rtx_code index_code,\n+\t\t     int scale)\n+{\n+  enum rtx_code code = GET_CODE (x);\n+  enum reg_class rclass;\n+\n+  if (context == 1)\n+    rclass = INDEX_REG_CLASS;\n+  else\n+    rclass = base_reg_class (mode, outer_code, index_code);\n+\n+  switch (code)\n+    {\n+    case CONST_INT:\n+    case CONST:\n+    case CC0:\n+    case PC:\n+    case SYMBOL_REF:\n+    case LABEL_REF:\n+      return;\n+\n+    case PLUS:\n+      /* When we have an address that is a sum, we must determine\n+\t whether registers are \"base\" or \"index\" regs.  If there is a\n+\t sum of two registers, we must choose one to be the \"base\".\n+\t Luckily, we can use the REG_POINTER to make a good choice\n+\t most of the time.  We only need to do this on machines that\n+\t can have two registers in an address and where the base and\n+\t index register classes are different.\n+\n+\t ??? This code used to set REGNO_POINTER_FLAG in some cases,\n+\t but that seems bogus since it should only be set when we are\n+\t sure the register is being used as a pointer.  */\n+      {\n+\trtx arg0 = XEXP (x, 0);\n+\trtx arg1 = XEXP (x, 1);\n+\tenum rtx_code code0 = GET_CODE (arg0);\n+\tenum rtx_code code1 = GET_CODE (arg1);\n+\n+\t/* Look inside subregs.  */\n+\tif (code0 == SUBREG)\n+\t  arg0 = SUBREG_REG (arg0), code0 = GET_CODE (arg0);\n+\tif (code1 == SUBREG)\n+\t  arg1 = SUBREG_REG (arg1), code1 = GET_CODE (arg1);\n+\n+\t/* If this machine only allows one register per address, it\n+\t   must be in the first operand.  */\n+\tif (MAX_REGS_PER_ADDRESS == 1)\n+\t  record_address_regs (mode, arg0, 0, PLUS, code1, scale);\n+\n+\t/* If index and base registers are the same on this machine,\n+\t   just record registers in any non-constant operands.  We\n+\t   assume here, as well as in the tests below, that all\n+\t   addresses are in canonical form.  */\n+\telse if (INDEX_REG_CLASS == base_reg_class (VOIDmode, PLUS, SCRATCH))\n+\t  {\n+\t    record_address_regs (mode, arg0, context, PLUS, code1, scale);\n+\t    if (! CONSTANT_P (arg1))\n+\t      record_address_regs (mode, arg1, context, PLUS, code0, scale);\n+\t  }\n+\n+\t/* If the second operand is a constant integer, it doesn't\n+\t   change what class the first operand must be.  */\n+\telse if (code1 == CONST_INT || code1 == CONST_DOUBLE)\n+\t  record_address_regs (mode, arg0, context, PLUS, code1, scale);\n+\t/* If the second operand is a symbolic constant, the first\n+\t   operand must be an index register.  */\n+\telse if (code1 == SYMBOL_REF || code1 == CONST || code1 == LABEL_REF)\n+\t  record_address_regs (mode, arg0, 1, PLUS, code1, scale);\n+\t/* If both operands are registers but one is already a hard\n+\t   register of index or reg-base class, give the other the\n+\t   class that the hard register is not.  */\n+\telse if (code0 == REG && code1 == REG\n+\t\t && REGNO (arg0) < FIRST_PSEUDO_REGISTER\n+\t\t && (ok_for_base_p_nonstrict (arg0, mode, PLUS, REG)\n+\t\t     || ok_for_index_p_nonstrict (arg0)))\n+\t  record_address_regs (mode, arg1,\n+\t\t\t       ok_for_base_p_nonstrict (arg0, mode, PLUS, REG)\n+\t\t\t       ? 1 : 0,\n+\t\t\t       PLUS, REG, scale);\n+\telse if (code0 == REG && code1 == REG\n+\t\t && REGNO (arg1) < FIRST_PSEUDO_REGISTER\n+\t\t && (ok_for_base_p_nonstrict (arg1, mode, PLUS, REG)\n+\t\t     || ok_for_index_p_nonstrict (arg1)))\n+\t  record_address_regs (mode, arg0,\n+\t\t\t       ok_for_base_p_nonstrict (arg1, mode, PLUS, REG)\n+\t\t\t       ? 1 : 0,\n+\t\t\t       PLUS, REG, scale);\n+\t/* If one operand is known to be a pointer, it must be the\n+\t   base with the other operand the index.  Likewise if the\n+\t   other operand is a MULT.  */\n+\telse if ((code0 == REG && REG_POINTER (arg0)) || code1 == MULT)\n+\t  {\n+\t    record_address_regs (mode, arg0, 0, PLUS, code1, scale);\n+\t    record_address_regs (mode, arg1, 1, PLUS, code0, scale);\n+\t  }\n+\telse if ((code1 == REG && REG_POINTER (arg1)) || code0 == MULT)\n+\t  {\n+\t    record_address_regs (mode, arg0, 1, PLUS, code1, scale);\n+\t    record_address_regs (mode, arg1, 0, PLUS, code0, scale);\n+\t  }\n+\t/* Otherwise, count equal chances that each might be a base or\n+\t   index register.  This case should be rare.  */\n+\telse\n+\t  {\n+\t    record_address_regs (mode, arg0, 0, PLUS, code1, scale / 2);\n+\t    record_address_regs (mode, arg0, 1, PLUS, code1, scale / 2);\n+\t    record_address_regs (mode, arg1, 0, PLUS, code0, scale / 2);\n+\t    record_address_regs (mode, arg1, 1, PLUS, code0, scale / 2);\n+\t  }\n+      }\n+      break;\n+\n+      /* Double the importance of an allocno that is incremented or\n+\t decremented, since it would take two extra insns if it ends\n+\t up in the wrong place.  */\n+    case POST_MODIFY:\n+    case PRE_MODIFY:\n+      record_address_regs (mode, XEXP (x, 0), 0, code,\n+\t\t\t   GET_CODE (XEXP (XEXP (x, 1), 1)), 2 * scale);\n+      if (REG_P (XEXP (XEXP (x, 1), 1)))\n+\trecord_address_regs (mode, XEXP (XEXP (x, 1), 1), 1, code, REG,\n+\t\t\t     2 * scale);\n+      break;\n+\n+    case POST_INC:\n+    case PRE_INC:\n+    case POST_DEC:\n+    case PRE_DEC:\n+      /* Double the importance of an allocno that is incremented or\n+\t decremented, since it would take two extra insns if it ends\n+\t up in the wrong place.  If the operand is a pseudo-register,\n+\t show it is being used in an INC_DEC context.  */\n+#ifdef FORBIDDEN_INC_DEC_CLASSES\n+      if (REG_P (XEXP (x, 0))\n+\t  && REGNO (XEXP (x, 0)) >= FIRST_PSEUDO_REGISTER)\n+\tin_inc_dec[ALLOCNO_NUM (ira_curr_regno_allocno_map\n+\t\t\t\t[REGNO (XEXP (x, 0))])] = true;\n+#endif\n+      record_address_regs (mode, XEXP (x, 0), 0, code, SCRATCH, 2 * scale);\n+      break;\n+\n+    case REG:\n+      {\n+\tstruct costs *pp;\n+\tint i, k;\n+\n+\tif (REGNO (x) < FIRST_PSEUDO_REGISTER)\n+\t  break;\n+\n+\tpp = COSTS_OF_ALLOCNO (allocno_costs,\n+\t\t\t       ALLOCNO_NUM (ira_curr_regno_allocno_map\n+\t\t\t\t\t    [REGNO (x)]));\n+\tpp->mem_cost += (ira_memory_move_cost[Pmode][rclass][1] * scale) / 2;\n+\tif (ira_register_move_cost[Pmode] == NULL)\n+\t  ira_init_register_move_cost (Pmode);\n+\tfor (k = 0; k < cost_classes_num; k++)\n+\t  {\n+\t    i = cost_classes[k];\n+\t    pp->cost[k]\n+\t      += (ira_may_move_in_cost[Pmode][i][rclass] * scale) / 2;\n+\t  }\n+      }\n+      break;\n+\n+    default:\n+      {\n+\tconst char *fmt = GET_RTX_FORMAT (code);\n+\tint i;\n+\tfor (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+\t  if (fmt[i] == 'e')\n+\t    record_address_regs (mode, XEXP (x, i), context, code, SCRATCH,\n+\t\t\t\t scale);\n+      }\n+    }\n+}\n+\n+\f\n+\n+/* Calculate the costs of insn operands.  */\n+static void\n+record_operand_costs (rtx insn, struct costs **op_costs,\n+\t\t      enum reg_class *allocno_pref)\n+{\n+  const char *constraints[MAX_RECOG_OPERANDS];\n+  enum machine_mode modes[MAX_RECOG_OPERANDS];\n+  int i;\n+\n+  for (i = 0; i < recog_data.n_operands; i++)\n+    {\n+      constraints[i] = recog_data.constraints[i];\n+      modes[i] = recog_data.operand_mode[i];\n+    }\n+\n+  /* If we get here, we are set up to record the costs of all the\n+     operands for this insn.  Start by initializing the costs.  Then\n+     handle any address registers.  Finally record the desired classes\n+     for any allocnos, doing it twice if some pair of operands are\n+     commutative.  */\n+  for (i = 0; i < recog_data.n_operands; i++)\n+    {\n+      memcpy (op_costs[i], init_cost, struct_costs_size);\n+\n+      if (GET_CODE (recog_data.operand[i]) == SUBREG)\n+\trecog_data.operand[i] = SUBREG_REG (recog_data.operand[i]);\n+\n+      if (MEM_P (recog_data.operand[i]))\n+\trecord_address_regs (GET_MODE (recog_data.operand[i]),\n+\t\t\t     XEXP (recog_data.operand[i], 0),\n+\t\t\t     0, MEM, SCRATCH, frequency * 2);\n+      else if (constraints[i][0] == 'p'\n+\t       || EXTRA_ADDRESS_CONSTRAINT (constraints[i][0],\n+\t\t\t\t\t    constraints[i]))\n+\trecord_address_regs (VOIDmode, recog_data.operand[i], 0, ADDRESS,\n+\t\t\t     SCRATCH, frequency * 2);\n+    }\n+\n+  /* Check for commutative in a separate loop so everything will have\n+     been initialized.  We must do this even if one operand is a\n+     constant--see addsi3 in m68k.md.  */\n+  for (i = 0; i < (int) recog_data.n_operands - 1; i++)\n+    if (constraints[i][0] == '%')\n+      {\n+\tconst char *xconstraints[MAX_RECOG_OPERANDS];\n+\tint j;\n+\n+\t/* Handle commutative operands by swapping the constraints.\n+\t   We assume the modes are the same.  */\n+\tfor (j = 0; j < recog_data.n_operands; j++)\n+\t  xconstraints[j] = constraints[j];\n+\n+\txconstraints[i] = constraints[i+1];\n+\txconstraints[i+1] = constraints[i];\n+\trecord_reg_classes (recog_data.n_alternatives, recog_data.n_operands,\n+\t\t\t    recog_data.operand, modes,\n+\t\t\t    xconstraints, insn, op_costs, allocno_pref);\n+      }\n+  record_reg_classes (recog_data.n_alternatives, recog_data.n_operands,\n+\t\t      recog_data.operand, modes,\n+\t\t      constraints, insn, op_costs, allocno_pref);\n+}\n+\n+\f\n+\n+/* Process one insn INSN.  Scan it and record each time it would save\n+   code to put a certain allocnos in a certain class.  Return the last\n+   insn processed, so that the scan can be continued from there.  */\n+static rtx\n+scan_one_insn (rtx insn)\n+{\n+  enum rtx_code pat_code;\n+  rtx set, note;\n+  int i, k;\n+\n+  if (!INSN_P (insn))\n+    return insn;\n+\n+  pat_code = GET_CODE (PATTERN (insn));\n+  if (pat_code == USE || pat_code == CLOBBER || pat_code == ASM_INPUT\n+      || pat_code == ADDR_VEC || pat_code == ADDR_DIFF_VEC)\n+    return insn;\n+\n+  set = single_set (insn);\n+  extract_insn (insn);\n+\n+  /* If this insn loads a parameter from its stack slot, then it\n+     represents a savings, rather than a cost, if the parameter is\n+     stored in memory.  Record this fact.  */\n+  if (set != 0 && REG_P (SET_DEST (set)) && MEM_P (SET_SRC (set))\n+      && (note = find_reg_note (insn, REG_EQUIV, NULL_RTX)) != NULL_RTX\n+      && MEM_P (XEXP (note, 0)))\n+    {\n+      COSTS_OF_ALLOCNO (allocno_costs,\n+\t\t\tALLOCNO_NUM (ira_curr_regno_allocno_map\n+\t\t\t\t     [REGNO (SET_DEST (set))]))->mem_cost\n+\t-= (ira_memory_move_cost[GET_MODE (SET_DEST (set))][GENERAL_REGS][1]\n+\t    * frequency);\n+      record_address_regs (GET_MODE (SET_SRC (set)), XEXP (SET_SRC (set), 0),\n+\t\t\t   0, MEM, SCRATCH, frequency * 2);\n+    }\n+\n+  record_operand_costs (insn, op_costs, allocno_pref);\n+\n+  /* Now add the cost for each operand to the total costs for its\n+     allocno.  */\n+  for (i = 0; i < recog_data.n_operands; i++)\n+    if (REG_P (recog_data.operand[i])\n+\t&& REGNO (recog_data.operand[i]) >= FIRST_PSEUDO_REGISTER)\n+      {\n+\tint regno = REGNO (recog_data.operand[i]);\n+\tstruct costs *p\n+\t  = COSTS_OF_ALLOCNO (allocno_costs,\n+\t\t\t      ALLOCNO_NUM (ira_curr_regno_allocno_map[regno]));\n+\tstruct costs *q = op_costs[i];\n+\n+\tp->mem_cost += q->mem_cost;\n+\tfor (k = 0; k < cost_classes_num; k++)\n+\t  p->cost[k] += q->cost[k];\n+      }\n+\n+  return insn;\n+}\n+\n+\f\n+\n+/* Print allocnos costs to file F.  */\n+static void\n+print_costs (FILE *f)\n+{\n+  int k;\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  fprintf (f, \"\\n\");\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      int i, rclass;\n+      basic_block bb;\n+      int regno = ALLOCNO_REGNO (a);\n+\n+      i = ALLOCNO_NUM (a);\n+      fprintf (f, \"  a%d(r%d,\", i, regno);\n+      if ((bb = ALLOCNO_LOOP_TREE_NODE (a)->bb) != NULL)\n+\tfprintf (f, \"b%d\", bb->index);\n+      else\n+\tfprintf (f, \"l%d\", ALLOCNO_LOOP_TREE_NODE (a)->loop->num);\n+      fprintf (f, \") costs:\");\n+      for (k = 0; k < cost_classes_num; k++)\n+\t{\n+\t  rclass = cost_classes[k];\n+\t  if (contains_reg_of_mode[rclass][PSEUDO_REGNO_MODE (regno)]\n+#ifdef FORBIDDEN_INC_DEC_CLASSES\n+\t      && (! in_inc_dec[i] || ! forbidden_inc_dec_class[rclass])\n+#endif\n+#ifdef CANNOT_CHANGE_MODE_CLASS\n+\t      && ! invalid_mode_change_p (regno, (enum reg_class) rclass,\n+\t\t\t\t\t  PSEUDO_REGNO_MODE (regno))\n+#endif\n+\t      )\n+\t    {\n+\t      fprintf (f, \" %s:%d\", reg_class_names[rclass],\n+\t\t       COSTS_OF_ALLOCNO (allocno_costs, i)->cost[k]);\n+\t      if (flag_ira_algorithm == IRA_ALGORITHM_REGIONAL\n+\t\t  || flag_ira_algorithm == IRA_ALGORITHM_MIXED)\n+\t\tfprintf (f, \",%d\", COSTS_OF_ALLOCNO (total_costs, i)->cost[k]);\n+\t    }\n+\t}\n+      fprintf (f, \" MEM:%i\\n\", COSTS_OF_ALLOCNO (allocno_costs, i)->mem_cost);\n+    }\n+}\n+\n+/* Traverse the BB represented by LOOP_TREE_NODE to update the allocno\n+   costs.  */\n+static void\n+process_bb_node_for_costs (ira_loop_tree_node_t loop_tree_node)\n+{\n+  basic_block bb;\n+  rtx insn;\n+\n+  bb = loop_tree_node->bb;\n+  if (bb == NULL)\n+    return;\n+  frequency = REG_FREQ_FROM_BB (bb);\n+  if (frequency == 0)\n+    frequency = 1;\n+  FOR_BB_INSNS (bb, insn)\n+    insn = scan_one_insn (insn);\n+}\n+\n+/* Find costs of register classes and memory for allocnos and their\n+   best costs. */\n+static void\n+find_allocno_class_costs (void)\n+{\n+  int i, k;\n+  int pass;\n+  basic_block bb;\n+\n+  init_recog ();\n+#ifdef FORBIDDEN_INC_DEC_CLASSES\n+  in_inc_dec = ira_allocate (sizeof (bool) * ira_allocnos_num);\n+#endif /* FORBIDDEN_INC_DEC_CLASSES */\n+  allocno_pref = NULL;\n+  /* Normally we scan the insns once and determine the best class to\n+     use for each allocno.  However, if -fexpensive-optimizations are\n+     on, we do so twice, the second time using the tentative best\n+     classes to guide the selection.  */\n+  for (pass = 0; pass <= flag_expensive_optimizations; pass++)\n+    {\n+      if (internal_flag_ira_verbose > 0 && ira_dump_file)\n+\tfprintf (ira_dump_file, \"\\nPass %i for finding allocno costs\\n\\n\",\n+\t\t pass);\n+      /* We could use only cover classes.  Unfortunately it does not\n+\t work well for some targets where some subclass of cover class\n+\t is costly and wrong cover class is chosen.  */\n+      for (cost_classes_num = 0;\n+\t   cost_classes_num < ira_important_classes_num;\n+\t   cost_classes_num++)\n+\t{\n+\t  cost_classes[cost_classes_num]\n+\t    = ira_important_classes[cost_classes_num];\n+\t  cost_class_nums[cost_classes[cost_classes_num]]\n+\t    = cost_classes_num;\n+\t}\n+      struct_costs_size\n+\t= sizeof (struct costs) + sizeof (int) * (cost_classes_num - 1);\n+      /* Zero out our accumulation of the cost of each class for each\n+\t allocno.  */\n+      memset (allocno_costs, 0, ira_allocnos_num * struct_costs_size);\n+#ifdef FORBIDDEN_INC_DEC_CLASSES\n+      memset (in_inc_dec, 0, ira_allocnos_num * sizeof (bool));\n+#endif\n+\n+      /* Scan the instructions and record each time it would save code\n+\t to put a certain allocno in a certain class.  */\n+      ira_traverse_loop_tree (true, ira_loop_tree_root,\n+\t\t\t      process_bb_node_for_costs, NULL);\n+\n+      memcpy (total_costs, allocno_costs,\n+\t      max_struct_costs_size * ira_allocnos_num);\n+      if (pass == 0)\n+\tallocno_pref = allocno_pref_buffer;\n+\n+      /* Now for each allocno look at how desirable each class is and\n+\t find which class is preferred.  */\n+      for (i = max_reg_num () - 1; i >= FIRST_PSEUDO_REGISTER; i--)\n+\t{\n+\t  ira_allocno_t a, parent_a;\n+\t  int rclass, a_num, parent_a_num;\n+\t  ira_loop_tree_node_t parent;\n+\t  int best_cost;\n+\t  enum reg_class best, alt_class, common_class;\n+#ifdef FORBIDDEN_INC_DEC_CLASSES\n+\t  int inc_dec_p = false;\n+#endif\n+\n+\t  if (ira_regno_allocno_map[i] == NULL)\n+\t    continue;\n+\t  memset (temp_costs, 0, struct_costs_size);\n+\t  /* Find cost of all allocnos with the same regno.  */\n+\t  for (a = ira_regno_allocno_map[i];\n+\t       a != NULL;\n+\t       a = ALLOCNO_NEXT_REGNO_ALLOCNO (a))\n+\t    {\n+\t      a_num = ALLOCNO_NUM (a);\n+\t      if ((flag_ira_algorithm == IRA_ALGORITHM_REGIONAL\n+\t\t   || flag_ira_algorithm == IRA_ALGORITHM_MIXED)\n+\t\t  && (parent = ALLOCNO_LOOP_TREE_NODE (a)->parent) != NULL\n+\t\t  && (parent_a = parent->regno_allocno_map[i]) != NULL\n+\t\t  /* There are no caps yet.  */\n+\t\t  && bitmap_bit_p (ALLOCNO_LOOP_TREE_NODE (a)->border_allocnos,\n+\t\t\t\t   ALLOCNO_NUM (a)))\n+\t\t{\n+\t\t  /* Propagate costs to upper levels in the region\n+\t\t     tree.  */\n+\t\t  parent_a_num = ALLOCNO_NUM (parent_a);\n+\t\t  for (k = 0; k < cost_classes_num; k++)\n+\t\t    COSTS_OF_ALLOCNO (total_costs, parent_a_num)->cost[k]\n+\t\t      += COSTS_OF_ALLOCNO (total_costs, a_num)->cost[k];\n+\t\t  COSTS_OF_ALLOCNO (total_costs, parent_a_num)->mem_cost\n+\t\t    += COSTS_OF_ALLOCNO (total_costs, a_num)->mem_cost;\n+\t\t}\n+\t      for (k = 0; k < cost_classes_num; k++)\n+\t\ttemp_costs->cost[k]\n+\t\t  += COSTS_OF_ALLOCNO (allocno_costs, a_num)->cost[k];\n+\t      temp_costs->mem_cost\n+\t\t+= COSTS_OF_ALLOCNO (allocno_costs, a_num)->mem_cost;\n+#ifdef FORBIDDEN_INC_DEC_CLASSES\n+\t      if (in_inc_dec[a_num])\n+\t\tinc_dec_p = true;\n+#endif\n+\t    }\n+\t  best_cost = (1 << (HOST_BITS_PER_INT - 2)) - 1;\n+\t  best = ALL_REGS;\n+\t  alt_class = NO_REGS;\n+\t  /* Find best common class for all allocnos with the same\n+\t     regno.  */\n+\t  for (k = 0; k < cost_classes_num; k++)\n+\t    {\n+\t      rclass = cost_classes[k];\n+\t      /* Ignore classes that are too small for this operand or\n+\t\t invalid for an operand that was auto-incremented.  */\n+\t      if (! contains_reg_of_mode[rclass][PSEUDO_REGNO_MODE (i)]\n+#ifdef FORBIDDEN_INC_DEC_CLASSES\n+\t\t  || (inc_dec_p && forbidden_inc_dec_class[rclass])\n+#endif\n+#ifdef CANNOT_CHANGE_MODE_CLASS\n+\t\t  || invalid_mode_change_p (i, (enum reg_class) rclass,\n+\t\t\t\t\t    PSEUDO_REGNO_MODE (i))\n+#endif\n+\t\t  )\n+\t\tcontinue;\n+\t      if (temp_costs->cost[k] < best_cost)\n+\t\t{\n+\t\t  best_cost = temp_costs->cost[k];\n+\t\t  best = (enum reg_class) rclass;\n+\t\t}\n+\t      else if (temp_costs->cost[k] == best_cost)\n+\t\tbest = ira_reg_class_union[best][rclass];\n+\t      if (pass == flag_expensive_optimizations\n+\t\t  && temp_costs->cost[k] < temp_costs->mem_cost\n+\t\t  && (reg_class_size[reg_class_subunion[alt_class][rclass]]\n+\t\t      > reg_class_size[alt_class]))\n+\t\talt_class = reg_class_subunion[alt_class][rclass];\n+\t    }\n+\t  if (pass == flag_expensive_optimizations)\n+\t    {\n+\t      if (best_cost > temp_costs->mem_cost)\n+\t\tbest = alt_class = NO_REGS;\n+\t      else if (best == alt_class)\n+\t\talt_class = NO_REGS;\n+\t      setup_reg_classes (i, best, alt_class);\n+\t      if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+\t\tfprintf (ira_dump_file,\n+\t\t\t \"    r%d: preferred %s, alternative %s\\n\",\n+\t\t\t i, reg_class_names[best], reg_class_names[alt_class]);\n+\t    }\n+\t  if (best_cost > temp_costs->mem_cost)\n+\t    common_class = NO_REGS;\n+\t  else\n+\t    /* Make the common class a cover class.  Remember all\n+\t       allocnos with the same regno should have the same cover\n+\t       class.  */\n+\t    common_class = ira_class_translate[best];\n+\t  for (a = ira_regno_allocno_map[i];\n+\t       a != NULL;\n+\t       a = ALLOCNO_NEXT_REGNO_ALLOCNO (a))\n+\t    {\n+\t      a_num = ALLOCNO_NUM (a);\n+\t      if (common_class == NO_REGS)\n+\t\tbest = NO_REGS;\n+\t      else\n+\t\t{\t      \n+\t\t  /* Finding best class which is subset of the common\n+\t\t     class.  */\n+\t\t  best_cost = (1 << (HOST_BITS_PER_INT - 2)) - 1;\n+\t\t  best = ALL_REGS;\n+\t\t  for (k = 0; k < cost_classes_num; k++)\n+\t\t    {\n+\t\t      rclass = cost_classes[k];\n+\t\t      if (! ira_class_subset_p[rclass][common_class])\n+\t\t\tcontinue;\n+\t\t      /* Ignore classes that are too small for this\n+\t\t\t operand or invalid for an operand that was\n+\t\t\t auto-incremented.  */\n+\t\t      if (! contains_reg_of_mode[rclass][PSEUDO_REGNO_MODE (i)]\n+#ifdef FORBIDDEN_INC_DEC_CLASSES\n+\t\t\t  || (inc_dec_p && forbidden_inc_dec_class[rclass])\n+#endif\n+#ifdef CANNOT_CHANGE_MODE_CLASS\n+\t\t\t  || invalid_mode_change_p (i, (enum reg_class) rclass,\n+\t\t\t\t\t\t    PSEUDO_REGNO_MODE (i))\n+#endif\n+\t\t\t  )\n+\t\t\t;\n+\t\t      else if (COSTS_OF_ALLOCNO (total_costs, a_num)->cost[k]\n+\t\t\t       < best_cost)\n+\t\t\t{\n+\t\t\t  best_cost\n+\t\t\t    = COSTS_OF_ALLOCNO (total_costs, a_num)->cost[k];\n+\t\t\t  best = (enum reg_class) rclass;\n+\t\t\t}\n+\t\t      else if (COSTS_OF_ALLOCNO (total_costs, a_num)->cost[k]\n+\t\t\t       == best_cost)\n+\t\t\tbest = ira_reg_class_union[best][rclass];\n+\t\t    }\n+\t\t}\n+\t      if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL\n+\t\t  && (pass == 0 || allocno_pref[a_num] != best))\n+\t\t{\n+\t\t  fprintf (ira_dump_file, \"    a%d (r%d,\", a_num, i);\n+\t\t  if ((bb = ALLOCNO_LOOP_TREE_NODE (a)->bb) != NULL)\n+\t\t    fprintf (ira_dump_file, \"b%d\", bb->index);\n+\t\t  else\n+\t\t    fprintf (ira_dump_file, \"l%d\",\n+\t\t\t     ALLOCNO_LOOP_TREE_NODE (a)->loop->num);\n+\t\t  fprintf (ira_dump_file, \") best %s, cover %s\\n\",\n+\t\t\t   reg_class_names[best],\n+\t\t\t   reg_class_names[ira_class_translate[best]]);\n+\t\t}\n+\t      allocno_pref[a_num] = best;\n+\t    }\n+\t}\n+      \n+      if (internal_flag_ira_verbose > 4 && ira_dump_file)\n+\t{\n+\t  print_costs (ira_dump_file);\n+\t  fprintf (ira_dump_file,\"\\n\");\n+\t}\n+    }\n+#ifdef FORBIDDEN_INC_DEC_CLASSES\n+  ira_free (in_inc_dec);\n+#endif\n+}\n+\n+\f\n+\n+/* Process moves involving hard regs to modify allocno hard register\n+   costs.  We can do this only after determining allocno cover class.\n+   If a hard register forms a register class, than moves with the hard\n+   register are already taken into account in class costs for the\n+   allocno.  */\n+static void\n+process_bb_node_for_hard_reg_moves (ira_loop_tree_node_t loop_tree_node)\n+{\n+  int i, freq, cost, src_regno, dst_regno, hard_regno;\n+  bool to_p;\n+  ira_allocno_t a;\n+  enum reg_class rclass, hard_reg_class;\n+  enum machine_mode mode;\n+  basic_block bb;\n+  rtx insn, set, src, dst;\n+\n+  bb = loop_tree_node->bb;\n+  if (bb == NULL)\n+    return;\n+  freq = REG_FREQ_FROM_BB (bb);\n+  if (freq == 0)\n+    freq = 1;\n+  FOR_BB_INSNS (bb, insn)\n+    {\n+      if (! INSN_P (insn))\n+\tcontinue;\n+      set = single_set (insn);\n+      if (set == NULL_RTX)\n+\tcontinue;\n+      dst = SET_DEST (set);\n+      src = SET_SRC (set);\n+      if (! REG_P (dst) || ! REG_P (src))\n+\tcontinue;\n+      dst_regno = REGNO (dst);\n+      src_regno = REGNO (src);\n+      if (dst_regno >= FIRST_PSEUDO_REGISTER\n+\t  && src_regno < FIRST_PSEUDO_REGISTER)\n+\t{\n+\t  hard_regno = src_regno;\n+\t  to_p = true;\n+\t  a = ira_curr_regno_allocno_map[dst_regno];\n+\t}\n+      else if (src_regno >= FIRST_PSEUDO_REGISTER\n+\t       && dst_regno < FIRST_PSEUDO_REGISTER)\n+\t{\n+\t  hard_regno = dst_regno;\n+\t  to_p = false;\n+\t  a = ira_curr_regno_allocno_map[src_regno];\n+\t}\n+      else\n+\tcontinue;\n+      rclass = ALLOCNO_COVER_CLASS (a);\n+      if (! TEST_HARD_REG_BIT (reg_class_contents[rclass], hard_regno))\n+\tcontinue;\n+      i = ira_class_hard_reg_index[rclass][hard_regno];\n+      if (i < 0)\n+\tcontinue;\n+      mode = ALLOCNO_MODE (a);\n+      hard_reg_class = REGNO_REG_CLASS (hard_regno);\n+      cost = (to_p ? ira_register_move_cost[mode][hard_reg_class][rclass]\n+\t      : ira_register_move_cost[mode][rclass][hard_reg_class]) * freq;\n+      ira_allocate_and_set_costs (&ALLOCNO_HARD_REG_COSTS (a), rclass,\n+\t\t\t\t  ALLOCNO_COVER_CLASS_COST (a));\n+      ira_allocate_and_set_costs (&ALLOCNO_CONFLICT_HARD_REG_COSTS (a),\n+\t\t\t\t  rclass, 0);\n+      ALLOCNO_HARD_REG_COSTS (a)[i] -= cost;\n+      ALLOCNO_CONFLICT_HARD_REG_COSTS (a)[i] -= cost;\n+      ALLOCNO_COVER_CLASS_COST (a) = MIN (ALLOCNO_COVER_CLASS_COST (a),\n+\t\t\t\t\t  ALLOCNO_HARD_REG_COSTS (a)[i]);\n+    }\n+}\n+\n+/* After we find hard register and memory costs for allocnos, define\n+   its cover class and modify hard register cost because insns moving\n+   allocno to/from hard registers.  */\n+static void\n+setup_allocno_cover_class_and_costs (void)\n+{\n+  int i, j, n, regno;\n+  int *reg_costs;\n+  enum reg_class cover_class, rclass;\n+  enum machine_mode mode;\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      i = ALLOCNO_NUM (a);\n+      mode = ALLOCNO_MODE (a);\n+      cover_class = ira_class_translate[allocno_pref[i]];\n+      ira_assert (allocno_pref[i] == NO_REGS || cover_class != NO_REGS);\n+      ALLOCNO_MEMORY_COST (a) = ALLOCNO_UPDATED_MEMORY_COST (a)\n+\t= COSTS_OF_ALLOCNO (allocno_costs, i)->mem_cost;\n+      ira_set_allocno_cover_class (a, cover_class);\n+      if (cover_class == NO_REGS)\n+\tcontinue;\n+      ALLOCNO_AVAILABLE_REGS_NUM (a) = ira_available_class_regs[cover_class];\n+      ALLOCNO_COVER_CLASS_COST (a)\n+\t= (COSTS_OF_ALLOCNO (allocno_costs, i)\n+\t   ->cost[cost_class_nums[allocno_pref[i]]]);\n+      if (optimize && ALLOCNO_COVER_CLASS (a) != allocno_pref[i])\n+\t{\n+\t  n = ira_class_hard_regs_num[cover_class];\n+\t  ALLOCNO_HARD_REG_COSTS (a)\n+\t    = reg_costs = ira_allocate_cost_vector (cover_class);\n+\t  for (j = n - 1; j >= 0; j--)\n+\t    {\n+\t      regno = ira_class_hard_regs[cover_class][j];\n+\t      rclass = REGNO_REG_CLASS (regno);\n+\t      reg_costs[j] = (COSTS_OF_ALLOCNO (allocno_costs, i)\n+\t\t\t       ->cost[cost_class_nums[rclass]]);\n+\t    }\n+\t}\n+    }\n+  if (optimize)\n+    ira_traverse_loop_tree (true, ira_loop_tree_root,\n+\t\t\t    process_bb_node_for_hard_reg_moves, NULL);\n+}\n+\n+\f\n+\n+/* Function called once during compiler work.  */\n+void\n+ira_init_costs_once (void)\n+{\n+  int i;\n+\n+  init_cost = NULL;\n+  for (i = 0; i < MAX_RECOG_OPERANDS; i++)\n+    {\n+      op_costs[i] = NULL;\n+      this_op_costs[i] = NULL;\n+    }\n+  temp_costs = NULL;\n+  cost_classes = NULL;\n+}\n+\n+/* Free allocated temporary cost vectors.  */\n+static void\n+free_ira_costs (void)\n+{\n+  int i;\n+\n+  if (init_cost != NULL)\n+    free (init_cost);\n+  init_cost = NULL;\n+  for (i = 0; i < MAX_RECOG_OPERANDS; i++)\n+    {\n+      if (op_costs[i] != NULL)\n+\tfree (op_costs[i]);\n+      if (this_op_costs[i] != NULL)\n+\tfree (this_op_costs[i]);\n+      op_costs[i] = this_op_costs[i] = NULL;\n+    }\n+  if (temp_costs != NULL)\n+    free (temp_costs);\n+  temp_costs = NULL;\n+  if (cost_classes != NULL)\n+    free (cost_classes);\n+  cost_classes = NULL;\n+}\n+\n+/* This is called each time register related information is\n+   changed.  */\n+void\n+ira_init_costs (void)\n+{\n+  int i;\n+\n+  free_ira_costs ();\n+  max_struct_costs_size\n+    = sizeof (struct costs) + sizeof (int) * (ira_important_classes_num - 1);\n+  /* Don't use ira_allocate because vectors live through several IRA calls.  */\n+  init_cost = (struct costs *) xmalloc (max_struct_costs_size);\n+  init_cost->mem_cost = 1000000;\n+  for (i = 0; i < ira_important_classes_num; i++)\n+    init_cost->cost[i] = 1000000;\n+  for (i = 0; i < MAX_RECOG_OPERANDS; i++)\n+    {\n+      op_costs[i] = (struct costs *) xmalloc (max_struct_costs_size);\n+      this_op_costs[i] = (struct costs *) xmalloc (max_struct_costs_size);\n+    }\n+  temp_costs = (struct costs *) xmalloc (max_struct_costs_size);\n+  cost_classes = (enum reg_class *) xmalloc (sizeof (enum reg_class)\n+\t\t\t\t\t     * ira_important_classes_num);\n+}\n+\n+/* Function called once at the end of compiler work.  */\n+void\n+ira_finish_costs_once (void)\n+{\n+  free_ira_costs ();\n+}\n+\n+\f\n+\n+/* Entry function which defines cover class, memory and hard register\n+   costs for each allocno.  */\n+void\n+ira_costs (void)\n+{\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  allocno_costs = (struct costs *) ira_allocate (max_struct_costs_size\n+\t\t\t\t\t       * ira_allocnos_num);\n+  total_costs = (struct costs *) ira_allocate (max_struct_costs_size\n+\t\t\t\t\t       * ira_allocnos_num);\n+  allocno_pref_buffer\n+    = (enum reg_class *) ira_allocate (sizeof (enum reg_class)\n+\t\t\t\t       * ira_allocnos_num);\n+  find_allocno_class_costs ();\n+  setup_allocno_cover_class_and_costs ();\n+  /* Because we could process operands only as subregs, check mode of\n+     the registers themselves too.  */\n+  FOR_EACH_ALLOCNO (a, ai)\n+    if (ira_register_move_cost[ALLOCNO_MODE (a)] == NULL\n+\t&& have_regs_of_mode[ALLOCNO_MODE (a)])\n+      ira_init_register_move_cost (ALLOCNO_MODE (a));\n+  ira_free (allocno_pref_buffer);\n+  ira_free (total_costs);\n+  ira_free (allocno_costs);\n+}\n+\n+\f\n+\n+/* Change hard register costs for allocnos which lives through\n+   function calls.  This is called only when we found all intersected\n+   calls during building allocno live ranges.  */\n+void\n+ira_tune_allocno_costs_and_cover_classes (void)\n+{\n+  int j, n, regno;\n+  int cost, min_cost, *reg_costs;\n+  enum reg_class cover_class, rclass;\n+  enum machine_mode mode;\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      cover_class = ALLOCNO_COVER_CLASS (a);\n+      if (cover_class == NO_REGS)\n+\tcontinue;\n+      mode = ALLOCNO_MODE (a);\n+      n = ira_class_hard_regs_num[cover_class];\n+      min_cost = INT_MAX;\n+      if (ALLOCNO_CALLS_CROSSED_NUM (a) != 0)\n+\t{\n+\t  ira_allocate_and_set_costs\n+\t    (&ALLOCNO_HARD_REG_COSTS (a), cover_class,\n+\t     ALLOCNO_COVER_CLASS_COST (a));\n+\t  reg_costs = ALLOCNO_HARD_REG_COSTS (a);\n+\t  for (j = n - 1; j >= 0; j--)\n+\t    {\n+\t      regno = ira_class_hard_regs[cover_class][j];\n+\t      rclass = REGNO_REG_CLASS (regno);\n+\t      cost = 0;\n+\t      /* ??? If only part is call clobbered.  */\n+\t      if (! ira_hard_reg_not_in_set_p (regno, mode, call_used_reg_set))\n+\t\tcost += (ALLOCNO_CALL_FREQ (a)\n+\t\t\t * (ira_memory_move_cost[mode][rclass][0]\n+\t\t\t    + ira_memory_move_cost[mode][rclass][1]));\n+#ifdef IRA_HARD_REGNO_ADD_COST_MULTIPLIER\n+\t      cost += ((ira_memory_move_cost[mode][rclass][0]\n+\t\t\t+ ira_memory_move_cost[mode][rclass][1])\n+\t\t       * ALLOCNO_FREQ (a)\n+\t\t       * IRA_HARD_REGNO_ADD_COST_MULTIPLIER (regno) / 2);\n+#endif\n+\t      reg_costs[j] += cost;\n+\t      if (min_cost > reg_costs[j])\n+\t\tmin_cost = reg_costs[j];\n+\t    }\n+\t}\n+      if (min_cost != INT_MAX)\n+\tALLOCNO_COVER_CLASS_COST (a) = min_cost;\n+    }\n+}"}, {"sha": "d18be8021abbdfb2d3e3e4e787054298c2d7fb21", "filename": "gcc/ira-emit.c", "status": "added", "additions": 1019, "deletions": 0, "changes": 1019, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-emit.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-emit.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira-emit.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -0,0 +1,1019 @@\n+/* Integrated Register Allocator.  Changing code and generating moves.\n+   Copyright (C) 2006, 2007, 2008\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"regs.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"target.h\"\n+#include \"flags.h\"\n+#include \"obstack.h\"\n+#include \"bitmap.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"expr.h\"\n+#include \"recog.h\"\n+#include \"params.h\"\n+#include \"timevar.h\"\n+#include \"tree-pass.h\"\n+#include \"output.h\"\n+#include \"reload.h\"\n+#include \"errors.h\"\n+#include \"df.h\"\n+#include \"ira-int.h\"\n+\n+\n+typedef struct move *move_t;\n+\n+/* The structure represents an allocno move.  Both allocnos have the\n+   same origional regno but different allocation.  */\n+struct move\n+{\n+  /* The allocnos involved in the move.  */\n+  ira_allocno_t from, to;\n+  /* The next move in the move sequence.  */\n+  move_t next;\n+  /* Used for finding dependencies.  */\n+  bool visited_p;\n+  /* The size of the following array. */\n+  int deps_num;\n+  /* Moves on which given move depends on.  Dependency can be cyclic.\n+     It means we need a temporary to generates the moves.  Sequence\n+     A1->A2, B1->B2 where A1 and B2 are assigned to reg R1 and A2 and\n+     B1 are assigned to reg R2 is an example of the cyclic\n+     dependencies.  */\n+  move_t *deps;\n+  /* First insn generated for the move.  */\n+  rtx insn;\n+};\n+\n+/* Array of moves (indexed by BB index) which should be put at the\n+   start/end of the corresponding basic blocks.  */\n+static move_t *at_bb_start, *at_bb_end;\n+\n+/* Max regno before renaming some pseudo-registers.  For example, the\n+   same pseudo-register can be renamed in a loop if its allocation is\n+   different outside the loop.  */\n+static int max_regno_before_changing;\n+\n+/* Return new move of allocnos TO and FROM.  */\n+static move_t\n+create_move (ira_allocno_t to, ira_allocno_t from)\n+{\n+  move_t move;\n+\n+  move = (move_t) ira_allocate (sizeof (struct move));\n+  move->deps = NULL;\n+  move->deps_num = 0;\n+  move->to = to;\n+  move->from = from;\n+  move->next = NULL;\n+  move->insn = NULL_RTX;\n+  move->visited_p = false;\n+  return move;\n+}\n+\n+/* Free memory for MOVE and its dependencies.  */\n+static void\n+free_move (move_t move)\n+{\n+  if (move->deps != NULL)\n+    ira_free (move->deps);\n+  ira_free (move);\n+}\n+\n+/* Free memory for list of the moves given by its HEAD.  */\n+static void\n+free_move_list (move_t head)\n+{\n+  move_t next;\n+  \n+  for (; head != NULL; head = next)\n+    {\n+      next = head->next;\n+      free_move (head);\n+    }\n+}\n+\n+/* Return TRUE if the the move list LIST1 and LIST2 are equal (two\n+   moves are equal if they involve the same allocnos).  */\n+static bool\n+eq_move_lists_p (move_t list1, move_t list2)\n+{\n+  for (; list1 != NULL && list2 != NULL;\n+       list1 = list1->next, list2 = list2->next)\n+    if (list1->from != list2->from || list1->to != list2->to)\n+      return false;\n+  return list1 == list2;\n+}\n+\n+/* This recursive function changes pseudo-registers in *LOC if it is\n+   necessary.  The function returns TRUE if a change was done.  */\n+static bool\n+change_regs (rtx *loc)\n+{\n+  int i, regno, result = false;\n+  const char *fmt;\n+  enum rtx_code code;\n+\n+  if (*loc == NULL_RTX)\n+    return false;\n+  code = GET_CODE (*loc);\n+  if (code == REG)\n+    {\n+      regno = REGNO (*loc);\n+      if (regno < FIRST_PSEUDO_REGISTER)\n+\treturn false;\n+      if (regno >= max_regno_before_changing)\n+\t/* It is a shared register which was changed already.  */\n+\treturn false;\n+      if (ira_curr_regno_allocno_map[regno] == NULL)\n+\treturn false;\n+      *loc = ALLOCNO_REG (ira_curr_regno_allocno_map[regno]);\n+      return true;\n+    }\n+\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\tresult = change_regs (&XEXP (*loc, i)) || result;\n+      else if (fmt[i] == 'E')\n+\t{\n+\t  int j;\n+\n+\t  for (j = XVECLEN (*loc, i) - 1; j >= 0; j--)\n+\t    result = change_regs (&XVECEXP (*loc, i, j)) || result;\n+\t}\n+    }\n+  return result;\n+}\n+\n+/* Attach MOVE to the edge E.  The move is attached to the head of the\n+   list if HEAD_P is TRUE.  */\n+static void\n+add_to_edge_list (edge e, move_t move, bool head_p)\n+{\n+  move_t last;\n+\n+  if (head_p || e->aux == NULL)\n+    {\n+      move->next = (move_t) e->aux;\n+      e->aux = move;\n+    }\n+  else\n+    {\n+      for (last = (move_t) e->aux; last->next != NULL; last = last->next)\n+\t;\n+      last->next = move;\n+      move->next = NULL;\n+    }\n+}\n+\n+/* Create and return new pseudo-register with the same attributes as\n+   ORIGINAL_REG.  */\n+static rtx\n+create_new_reg (rtx original_reg)\n+{\n+  rtx new_reg;\n+\n+  new_reg = gen_reg_rtx (GET_MODE (original_reg));\n+  ORIGINAL_REGNO (new_reg) = ORIGINAL_REGNO (original_reg);\n+  REG_USERVAR_P (new_reg) = REG_USERVAR_P (original_reg);\n+  REG_POINTER (new_reg) = REG_POINTER (original_reg);\n+  REG_ATTRS (new_reg) = REG_ATTRS (original_reg);\n+  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+    fprintf (ira_dump_file, \"      Creating newreg=%i from oldreg=%i\\n\",\n+\t     REGNO (new_reg), REGNO (original_reg));\n+  return new_reg;\n+}\n+\n+/* Return TRUE if loop given by SUBNODE inside the loop given by\n+   NODE.  */\n+static bool\n+subloop_tree_node_p (ira_loop_tree_node_t subnode, ira_loop_tree_node_t node)\n+{\n+  for (; subnode != NULL; subnode = subnode->parent)\n+    if (subnode == node)\n+      return true;\n+  return false;\n+}\n+\n+/* Set up member `reg' to REG for allocnos which has the same regno as\n+   ALLOCNO and which are inside the loop corresponding to ALLOCNO. */\n+static void\n+set_allocno_reg (ira_allocno_t allocno, rtx reg)\n+{\n+  int regno;\n+  ira_allocno_t a;\n+  ira_loop_tree_node_t node;\n+\n+  node = ALLOCNO_LOOP_TREE_NODE (allocno);\n+  for (a = ira_regno_allocno_map[ALLOCNO_REGNO (allocno)];\n+       a != NULL;\n+       a = ALLOCNO_NEXT_REGNO_ALLOCNO (a))\n+    if (subloop_tree_node_p (ALLOCNO_LOOP_TREE_NODE (a), node))\n+      ALLOCNO_REG (a) = reg;\n+  for (a = ALLOCNO_CAP (allocno); a != NULL; a = ALLOCNO_CAP (a))\n+    ALLOCNO_REG (a) = reg;\n+  regno = ALLOCNO_REGNO (allocno);\n+  for (a = allocno;;)\n+    {\n+      if (a == NULL || (a = ALLOCNO_CAP (a)) == NULL)\n+\t{\n+\t  node = node->parent;\n+\t  if (node == NULL)\n+\t    break;\n+\t  a = node->regno_allocno_map[regno];\n+\t}\n+      if (a == NULL)\n+\tcontinue;\n+      if (ALLOCNO_CHILD_RENAMED_P (a))\n+\tbreak;\n+      ALLOCNO_CHILD_RENAMED_P (a) = true;\n+    }\n+}\n+\n+/* Return TRUE if move of SRC_ALLOCNO to DEST_ALLOCNO does not change\n+   value of the destination.  One possible reason for this is the\n+   situation when SRC_ALLOCNO is not modified in the corresponding\n+   loop.  */\n+static bool\n+not_modified_p (ira_allocno_t src_allocno, ira_allocno_t dest_allocno)\n+{\n+  int regno, orig_regno;\n+  ira_allocno_t a;\n+  ira_loop_tree_node_t node;\n+\n+  ira_assert (ALLOCNO_CAP_MEMBER (src_allocno) == NULL\n+\t      && ALLOCNO_CAP_MEMBER (dest_allocno) == NULL);\n+  orig_regno = ALLOCNO_REGNO (src_allocno);\n+  regno = REGNO (ALLOCNO_REG (dest_allocno));\n+  for (node = ALLOCNO_LOOP_TREE_NODE (src_allocno);\n+       node != NULL;\n+       node = node->parent)\n+    if ((a = node->regno_allocno_map[orig_regno]) == NULL)\n+      break;\n+    else if (REGNO (ALLOCNO_REG (a)) == (unsigned) regno)\n+      return true;\n+    else if (bitmap_bit_p (node->modified_regnos, orig_regno))\n+      return false;\n+  return node != NULL;\n+}\n+\n+/* Generate and attach moves to the edge E.  This looks at the final\n+   regnos of allocnos living on the edge with the same original regno\n+   to figure out when moves should be generated.  */\n+static void\n+generate_edge_moves (edge e)\n+{\n+  ira_loop_tree_node_t src_loop_node, dest_loop_node;\n+  unsigned int regno;\n+  bitmap_iterator bi;\n+  ira_allocno_t src_allocno, dest_allocno, *src_map, *dest_map;\n+  move_t move;\n+\n+  src_loop_node = IRA_BB_NODE (e->src)->parent;\n+  dest_loop_node = IRA_BB_NODE (e->dest)->parent;\n+  e->aux = NULL;\n+  if (src_loop_node == dest_loop_node)\n+    return;\n+  src_map = src_loop_node->regno_allocno_map;\n+  dest_map = dest_loop_node->regno_allocno_map;\n+  EXECUTE_IF_SET_IN_REG_SET (DF_LR_IN (e->dest),\n+\t\t\t     FIRST_PSEUDO_REGISTER, regno, bi)\n+    if (bitmap_bit_p (DF_LR_OUT (e->src), regno))\n+      {\n+\tsrc_allocno = src_map[regno];\n+\tdest_allocno = dest_map[regno];\n+\tif (REGNO (ALLOCNO_REG (src_allocno))\n+\t    == REGNO (ALLOCNO_REG (dest_allocno)))\n+\t  continue;\n+\t/* Actually it is not a optimization we need this code because\n+\t   the memory (remember about equivalent memory) might be ROM\n+\t   (or placed in read only section).  */\n+ \tif (ALLOCNO_HARD_REGNO (dest_allocno) < 0\n+\t    && ALLOCNO_HARD_REGNO (src_allocno) >= 0\n+\t    && not_modified_p (src_allocno, dest_allocno))\n+\t  {\n+\t    ALLOCNO_MEM_OPTIMIZED_DEST (src_allocno) = dest_allocno;\n+\t    ALLOCNO_MEM_OPTIMIZED_DEST_P (dest_allocno) = true;\n+\t    if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t      fprintf (ira_dump_file, \"      Remove r%d:a%d->a%d(mem)\\n\",\n+\t\t       regno, ALLOCNO_NUM (src_allocno),\n+\t\t       ALLOCNO_NUM (dest_allocno));\n+\t    continue;\n+\t  }\n+\tmove = create_move (dest_allocno, src_allocno);\n+\tadd_to_edge_list (e, move, true);\n+    }\n+}\n+\n+/* Bitmap of allocnos local for the current loop.  */\n+static bitmap local_allocno_bitmap;\n+\n+/* This bitmap is used to find that we need to generate and to use a\n+   new pseudo-register when processing allocnos with the same original\n+   regno.  */\n+static bitmap used_regno_bitmap;\n+\n+/* This bitmap contains regnos of allocnos which were renamed locally\n+   because the allocnos correspond to disjoint live ranges in loops\n+   with a common parent.  */\n+static bitmap renamed_regno_bitmap;\n+\n+/* Change (if necessary) pseudo-registers inside loop given by loop\n+   tree node NODE.  */\n+static void\n+change_loop (ira_loop_tree_node_t node)\n+{\n+  bitmap_iterator bi;\n+  unsigned int i;\n+  int regno;\n+  bool used_p;\n+  ira_allocno_t allocno, parent_allocno, *map;\n+  rtx insn, original_reg;\n+  enum reg_class cover_class;\n+  ira_loop_tree_node_t parent;\n+\n+  if (node != ira_loop_tree_root)\n+    {\n+      \n+      if (node->bb != NULL)\n+\t{\n+\t  FOR_BB_INSNS (node->bb, insn)\n+\t    if (INSN_P (insn) && change_regs (&insn))\n+\t      {\n+\t\tdf_insn_rescan (insn);\n+\t\tdf_notes_rescan (insn);\n+\t      }\n+\t  return;\n+\t}\n+      \n+      if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\tfprintf (ira_dump_file,\n+\t\t \"      Changing RTL for loop %d (header bb%d)\\n\",\n+\t\t node->loop->num, node->loop->header->index);\n+      \n+      parent = ira_curr_loop_tree_node->parent;\n+      map = parent->regno_allocno_map;\n+      EXECUTE_IF_SET_IN_REG_SET (ira_curr_loop_tree_node->border_allocnos,\n+\t\t\t\t 0, i, bi)\n+\t{\n+\t  allocno = ira_allocnos[i];\n+\t  regno = ALLOCNO_REGNO (allocno);\n+\t  cover_class = ALLOCNO_COVER_CLASS (allocno);\n+\t  parent_allocno = map[regno];\n+\t  ira_assert (regno < ira_reg_equiv_len);\n+\t  /* We generate the same hard register move because the\n+\t     reload pass can put an allocno into memory in this case\n+\t     we will have live range splitting.  If it does not happen\n+\t     such the same hard register moves will be removed.  The\n+\t     worst case when the both allocnos are put into memory by\n+\t     the reload is very rare.  */\n+\t  if (parent_allocno != NULL\n+\t      && (ALLOCNO_HARD_REGNO (allocno)\n+\t\t  == ALLOCNO_HARD_REGNO (parent_allocno))\n+\t      && (ALLOCNO_HARD_REGNO (allocno) < 0\n+\t\t  || (parent->reg_pressure[cover_class] + 1\n+\t\t      <= ira_available_class_regs[cover_class])\n+\t\t  || TEST_HARD_REG_BIT (ira_prohibited_mode_move_regs\n+\t\t\t\t\t[ALLOCNO_MODE (allocno)],\n+\t\t\t\t\tALLOCNO_HARD_REGNO (allocno))\n+\t\t  /* don't create copies because reload can spill an\n+\t\t     allocno set by copy although the allocno will not\n+\t\t     get memory slot.  */\n+\t\t  || ira_reg_equiv_invariant_p[regno]\n+\t\t  || ira_reg_equiv_const[regno] != NULL_RTX))\n+\t    continue;\n+\t  original_reg = ALLOCNO_REG (allocno);\n+\t  if (parent_allocno == NULL\n+\t      || REGNO (ALLOCNO_REG (parent_allocno)) == REGNO (original_reg))\n+\t    {\n+\t      if (internal_flag_ira_verbose > 3 && ira_dump_file)\n+\t\tfprintf (ira_dump_file, \"  %i vs parent %i:\",\n+\t\t\t ALLOCNO_HARD_REGNO (allocno),\n+\t\t\t ALLOCNO_HARD_REGNO (parent_allocno));\n+\t      set_allocno_reg (allocno, create_new_reg (original_reg));\n+\t    }\n+\t}\n+    }\n+  /* Rename locals: Local allocnos with same regno in different loops\n+     might get the different hard register.  So we need to change\n+     ALLOCNO_REG.  */\n+  bitmap_and_compl (local_allocno_bitmap,\n+\t\t    ira_curr_loop_tree_node->mentioned_allocnos,\n+\t\t    ira_curr_loop_tree_node->border_allocnos);\n+  EXECUTE_IF_SET_IN_REG_SET (local_allocno_bitmap, 0, i, bi)\n+    {\n+      allocno = ira_allocnos[i];\n+      regno = ALLOCNO_REGNO (allocno);\n+      if (ALLOCNO_CAP_MEMBER (allocno) != NULL)\n+\tcontinue;\n+      used_p = bitmap_bit_p (used_regno_bitmap, regno);\n+      bitmap_set_bit (used_regno_bitmap, regno);\n+      ALLOCNO_SOMEWHERE_RENAMED_P (allocno) = true;\n+      if (! used_p)\n+\tcontinue;\n+      bitmap_set_bit (renamed_regno_bitmap, regno);\n+      set_allocno_reg (allocno, create_new_reg (ALLOCNO_REG (allocno)));\n+    }\n+}\n+\n+/* Process to set up flag somewhere_renamed_p.  */\n+static void\n+set_allocno_somewhere_renamed_p (void)\n+{\n+  unsigned int regno;\n+  ira_allocno_t allocno;\n+  ira_allocno_iterator ai;\n+\n+  FOR_EACH_ALLOCNO (allocno, ai)\n+    {\n+      regno = ALLOCNO_REGNO (allocno);\n+      if (bitmap_bit_p (renamed_regno_bitmap, regno)\n+\t  && REGNO (ALLOCNO_REG (allocno)) == regno)\n+\tALLOCNO_SOMEWHERE_RENAMED_P (allocno) = true;\n+    }\n+}\n+\n+/* Return TRUE if move lists on all edges given in vector VEC are\n+   equal.  */\n+static bool\n+eq_edge_move_lists_p (VEC(edge,gc) *vec)\n+{\n+  move_t list;\n+  int i;\n+\n+  list = (move_t) EDGE_I (vec, 0)->aux;\n+  for (i = EDGE_COUNT (vec) - 1; i > 0; i--)\n+    if (! eq_move_lists_p (list, (move_t) EDGE_I (vec, i)->aux))\n+      return false;\n+  return true;\n+}\n+\n+/* Look at all entry edges (if START_P) or exit edges of basic block\n+   BB and put move lists at the BB start or end if it is possible.  In\n+   other words, this decreases code duplication of allocno moves.  */\n+static void\n+unify_moves (basic_block bb, bool start_p)\n+{\n+  int i;\n+  edge e;\n+  move_t list;\n+  VEC(edge,gc) *vec;\n+\n+  vec = (start_p ? bb->preds : bb->succs);\n+  if (EDGE_COUNT (vec) == 0 || ! eq_edge_move_lists_p (vec))\n+    return;\n+  e = EDGE_I (vec, 0);\n+  list = (move_t) e->aux;\n+  if (! start_p && control_flow_insn_p (BB_END (bb)))\n+    return;\n+  e->aux = NULL;\n+  for (i = EDGE_COUNT (vec) - 1; i > 0; i--)\n+    {\n+      e = EDGE_I (vec, i);\n+      free_move_list ((move_t) e->aux);\n+      e->aux = NULL;\n+    }\n+  if (start_p)\n+    at_bb_start[bb->index] = list;\n+  else\n+    at_bb_end[bb->index] = list;\n+}\n+\n+/* Last move (in move sequence being processed) setting up the\n+   corresponding hard register.  */\n+static move_t hard_regno_last_set[FIRST_PSEUDO_REGISTER];\n+\n+/* If the element value is equal to CURR_TICK then the corresponding\n+   element in `hard_regno_last_set' is defined and correct.  */\n+static int hard_regno_last_set_check[FIRST_PSEUDO_REGISTER];\n+\n+/* Last move (in move sequence being processed) setting up the\n+   corresponding allocno.  */\n+static move_t *allocno_last_set;\n+\n+/* If the element value is equal to CURR_TICK then the corresponding\n+   element in . `allocno_last_set' is defined and correct.  */\n+static int *allocno_last_set_check;\n+\n+/* Definition of vector of moves.  */\n+DEF_VEC_P(move_t);\n+DEF_VEC_ALLOC_P(move_t, heap);\n+\n+/* This vec contains moves sorted topologically (depth-first) on their\n+   dependency graph.  */\n+static VEC(move_t,heap) *move_vec;\n+\n+/* The variable value is used to check correctness of values of\n+   elements of arrays `hard_regno_last_set' and\n+   `allocno_last_set_check'.  */\n+static int curr_tick;\n+\n+/* This recursive function traverses dependencies of MOVE and produces\n+   topological sorting (in depth-first order).  */\n+static void\n+traverse_moves (move_t move)\n+{\n+  int i;\n+\n+  if (move->visited_p)\n+    return;\n+  move->visited_p = true;\n+  for (i = move->deps_num - 1; i >= 0; i--)\n+    traverse_moves (move->deps[i]);\n+  VEC_safe_push (move_t, heap, move_vec, move);\n+}\n+\n+/* Remove unnecessary moves in the LIST, makes topological sorting,\n+   and removes cycles on hard reg dependencies by introducing new\n+   allocnos assigned to memory and additional moves.  It returns the\n+   result move list.  */\n+static move_t\n+modify_move_list (move_t list)\n+{\n+  int i, n, nregs, hard_regno;\n+  ira_allocno_t to, from, new_allocno;\n+  move_t move, new_move, set_move, first, last;\n+\n+  if (list == NULL)\n+    return NULL;\n+  /* Creat move deps.  */\n+  curr_tick++;\n+  for (move = list; move != NULL; move = move->next)\n+    {\n+      to = move->to;\n+      if ((hard_regno = ALLOCNO_HARD_REGNO (to)) < 0)\n+\tcontinue;\n+      nregs = hard_regno_nregs[hard_regno][ALLOCNO_MODE (to)];\n+      for (i = 0; i < nregs; i++)\n+\t{\n+\t  hard_regno_last_set[hard_regno + i] = move;\n+\t  hard_regno_last_set_check[hard_regno + i] = curr_tick;\n+\t}\n+    }\n+  for (move = list; move != NULL; move = move->next)\n+    {\n+      from = move->from;\n+      to = move->to;\n+      if ((hard_regno = ALLOCNO_HARD_REGNO (from)) >= 0)\n+\t{\n+\t  nregs = hard_regno_nregs[hard_regno][ALLOCNO_MODE (from)];\n+\t  for (n = i = 0; i < nregs; i++)\n+\t    if (hard_regno_last_set_check[hard_regno + i] == curr_tick\n+\t\t&& (ALLOCNO_REGNO (hard_regno_last_set[hard_regno + i]->to)\n+\t\t    != ALLOCNO_REGNO (from)))\n+\t      n++;\n+\t  move->deps = (move_t *) ira_allocate (n * sizeof (move_t));\n+\t  for (n = i = 0; i < nregs; i++)\n+\t    if (hard_regno_last_set_check[hard_regno + i] == curr_tick\n+\t\t&& (ALLOCNO_REGNO (hard_regno_last_set[hard_regno + i]->to)\n+\t\t    != ALLOCNO_REGNO (from)))\n+\t      move->deps[n++] = hard_regno_last_set[hard_regno + i];\n+\t  move->deps_num = n;\n+\t}\n+    }\n+  /* Toplogical sorting:  */\n+  VEC_truncate (move_t, move_vec, 0);\n+  for (move = list; move != NULL; move = move->next)\n+    traverse_moves (move);\n+  last = NULL;\n+  for (i = (int) VEC_length (move_t, move_vec) - 1; i >= 0; i--)\n+    {\n+      move = VEC_index (move_t, move_vec, i);\n+      move->next = NULL;\n+      if (last != NULL)\n+\tlast->next = move;\n+      last = move;\n+    }\n+  first = VEC_last (move_t, move_vec);\n+  /* Removing cycles:  */\n+  curr_tick++;\n+  VEC_truncate (move_t, move_vec, 0);\n+  for (move = first; move != NULL; move = move->next)\n+    {\n+      from = move->from;\n+      to = move->to;\n+      if ((hard_regno = ALLOCNO_HARD_REGNO (from)) >= 0)\n+\t{\n+\t  nregs = hard_regno_nregs[hard_regno][ALLOCNO_MODE (from)];\n+\t  for (i = 0; i < nregs; i++)\n+\t    if (hard_regno_last_set_check[hard_regno + i] == curr_tick\n+\t\t&& ALLOCNO_HARD_REGNO\n+\t\t   (hard_regno_last_set[hard_regno + i]->to) >= 0)\n+\t      {\n+\t\tset_move = hard_regno_last_set[hard_regno + i];\n+\t\t/* It does not matter what loop_tree_node (of TO or\n+\t\t   FROM) to use for the new allocno because of\n+\t\t   subsequent IRA internal representation\n+\t\t   flattening.  */\n+\t\tnew_allocno\n+\t\t  = ira_create_allocno (ALLOCNO_REGNO (set_move->to), false,\n+\t\t\t\t\tALLOCNO_LOOP_TREE_NODE (set_move->to));\n+\t\tALLOCNO_MODE (new_allocno) = ALLOCNO_MODE (set_move->to);\n+\t\tira_set_allocno_cover_class\n+\t\t  (new_allocno, ALLOCNO_COVER_CLASS (set_move->to));\n+\t\tALLOCNO_ASSIGNED_P (new_allocno) = true;\n+\t\tALLOCNO_HARD_REGNO (new_allocno) = -1;\n+\t\tALLOCNO_REG (new_allocno)\n+\t\t  = create_new_reg (ALLOCNO_REG (set_move->to));\n+\t\tALLOCNO_CONFLICT_ID (new_allocno) = ALLOCNO_NUM (new_allocno);\n+\t\t/* Make it possibly conflicting with all earlier\n+\t\t   created allocnos.  Cases where temporary allocnos\n+\t\t   created to remove the cycles are quite rare.  */\n+\t\tALLOCNO_MIN (new_allocno) = 0;\n+\t\tALLOCNO_MAX (new_allocno) = ira_allocnos_num - 1;\n+\t\tnew_move = create_move (set_move->to, new_allocno);\n+\t\tset_move->to = new_allocno;\n+\t\tVEC_safe_push (move_t, heap, move_vec, new_move);\n+\t\tira_move_loops_num++;\n+\t\tif (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+\t\t  fprintf (ira_dump_file,\n+\t\t\t   \"    Creating temporary allocno a%dr%d\\n\",\n+\t\t\t   ALLOCNO_NUM (new_allocno),\n+\t\t\t   REGNO (ALLOCNO_REG (new_allocno)));\n+\t      }\n+\t}\n+      if ((hard_regno = ALLOCNO_HARD_REGNO (to)) < 0)\n+\tcontinue;\n+      nregs = hard_regno_nregs[hard_regno][ALLOCNO_MODE (to)];\n+      for (i = 0; i < nregs; i++)\n+\t{\n+\t  hard_regno_last_set[hard_regno + i] = move;\n+\t  hard_regno_last_set_check[hard_regno + i] = curr_tick;\n+\t}\n+    }\n+  for (i = (int) VEC_length (move_t, move_vec) - 1; i >= 0; i--)\n+    {\n+      move = VEC_index (move_t, move_vec, i);\n+      move->next = NULL;\n+      last->next = move;\n+      last = move;\n+    }\n+  return first;\n+}\n+\n+/* Generate RTX move insns from the move list LIST.  This updates\n+   allocation cost using move execution frequency FREQ.  */\n+static rtx\n+emit_move_list (move_t list, int freq)\n+{\n+  int cost;\n+  rtx result, insn;\n+  enum machine_mode mode;\n+  enum reg_class cover_class;\n+\n+  start_sequence ();\n+  for (; list != NULL; list = list->next)\n+    {\n+      start_sequence ();\n+      emit_move_insn (ALLOCNO_REG (list->to), ALLOCNO_REG (list->from));\n+      list->insn = get_insns ();\n+      end_sequence ();\n+      /* The reload needs to have set up insn codes.  If the reload\n+\t sets up insn codes by itself, it may fail because insns will\n+\t have hard registers instead of pseudos and there may be no\n+\t machine insn with given hard registers.  */\n+      for (insn = list->insn; insn != NULL_RTX; insn = NEXT_INSN (insn))\n+\trecog_memoized (insn);\n+      emit_insn (list->insn);\n+      mode = ALLOCNO_MODE (list->to);\n+      cover_class = ALLOCNO_COVER_CLASS (list->to);\n+      cost = 0;\n+      if (ALLOCNO_HARD_REGNO (list->to) < 0)\n+\t{\n+\t  if (ALLOCNO_HARD_REGNO (list->from) >= 0)\n+\t    {\n+\t      cost = ira_memory_move_cost[mode][cover_class][0] * freq;\n+\t      ira_store_cost += cost;\n+\t    }\n+\t}\n+      else if (ALLOCNO_HARD_REGNO (list->from) < 0)\n+\t{\n+\t  if (ALLOCNO_HARD_REGNO (list->to) >= 0)\n+\t    {\n+\t      cost = ira_memory_move_cost[mode][cover_class][0] * freq;\n+\t      ira_load_cost += cost;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  cost = ira_register_move_cost[mode][cover_class][cover_class] * freq;\n+\t  ira_shuffle_cost += cost;\n+\t}\n+      ira_overall_cost += cost;\n+    }\n+  result = get_insns ();\n+  end_sequence ();\n+  return result;\n+}\n+\n+/* Generate RTX move insns from move lists attached to basic blocks\n+   and edges.  */\n+static void\n+emit_moves (void)\n+{\n+  basic_block bb;\n+  edge_iterator ei;\n+  edge e;\n+  rtx insns, tmp;\n+\n+  FOR_EACH_BB (bb)\n+    {\n+      if (at_bb_start[bb->index] != NULL)\n+\t{\n+\t  at_bb_start[bb->index] = modify_move_list (at_bb_start[bb->index]);\n+\t  insns = emit_move_list (at_bb_start[bb->index],\n+\t\t\t\t  REG_FREQ_FROM_BB (bb));\n+\t  tmp = BB_HEAD (bb);\n+\t  if (LABEL_P (tmp))\n+\t    tmp = NEXT_INSN (tmp);\n+\t  if (NOTE_INSN_BASIC_BLOCK_P (tmp))\n+\t    tmp = NEXT_INSN (tmp);\n+\t  if (tmp == BB_HEAD (bb))\n+\t    emit_insn_before (insns, tmp);\n+\t  else if (tmp != NULL_RTX)\n+\t    emit_insn_after (insns, PREV_INSN (tmp));\n+\t  else\n+\t    emit_insn_after (insns, get_last_insn ());\n+\t}\n+\n+      if (at_bb_end[bb->index] != NULL)\n+\t{\n+\t  at_bb_end[bb->index] = modify_move_list (at_bb_end[bb->index]);\n+\t  insns = emit_move_list (at_bb_end[bb->index], REG_FREQ_FROM_BB (bb));\n+\t  ira_assert (! control_flow_insn_p (BB_END (bb)));\n+\t  emit_insn_after (insns, BB_END (bb));\n+\t}\n+\n+      FOR_EACH_EDGE (e, ei, bb->succs)\n+\t{\n+\t  if (e->aux == NULL)\n+\t    continue;\n+\t  ira_assert ((e->flags & EDGE_ABNORMAL) == 0\n+\t\t      || ! EDGE_CRITICAL_P (e));\n+\t  e->aux = modify_move_list ((move_t) e->aux);\n+\t  insert_insn_on_edge\n+\t    (emit_move_list ((move_t) e->aux,\n+\t\t\t     REG_FREQ_FROM_EDGE_FREQ (EDGE_FREQUENCY (e))),\n+\t     e);\n+\t  if (e->src->next_bb != e->dest)\n+\t    ira_additional_jumps_num++;\n+\t}\n+    }\n+}\n+\n+/* Update costs of A and corresponding allocnos on upper levels on the\n+   loop tree from reading (if READ_P) or writing A on an execution\n+   path with FREQ.  */\n+static void\n+update_costs (ira_allocno_t a, bool read_p, int freq)\n+{\n+  ira_loop_tree_node_t parent;\n+\n+  for (;;)\n+    {\n+      ALLOCNO_NREFS (a)++;\n+      ALLOCNO_FREQ (a) += freq;\n+      ALLOCNO_MEMORY_COST (a)\n+\t+= (ira_memory_move_cost[ALLOCNO_MODE (a)][ALLOCNO_COVER_CLASS (a)]\n+\t    [read_p ? 1 : 0] * freq);\n+      if (ALLOCNO_CAP (a) != NULL)\n+\ta = ALLOCNO_CAP (a);\n+      else if ((parent = ALLOCNO_LOOP_TREE_NODE (a)->parent) == NULL\n+\t       || (a = parent->regno_allocno_map[ALLOCNO_REGNO (a)]) == NULL)\n+\tbreak;\n+    }\n+}\n+\n+/* Process moves from LIST with execution FREQ to add ranges, copies,\n+   and modify costs for allocnos involved in the moves.  All regnos\n+   living through the list is in LIVE_THROUGH, and the loop tree node\n+   used to find corresponding allocnos is NODE.  */\n+static void\n+add_range_and_copies_from_move_list (move_t list, ira_loop_tree_node_t node,\n+\t\t\t\t     bitmap live_through, int freq)\n+{\n+  int start, n;\n+  unsigned int regno;\n+  move_t move;\n+  ira_allocno_t to, from, a;\n+  ira_copy_t cp;\n+  allocno_live_range_t r;\n+  bitmap_iterator bi;\n+  HARD_REG_SET hard_regs_live;\n+\n+  if (list == NULL)\n+    return;\n+  n = 0;\n+  EXECUTE_IF_SET_IN_BITMAP (live_through, FIRST_PSEUDO_REGISTER, regno, bi)\n+    n++;\n+  REG_SET_TO_HARD_REG_SET (hard_regs_live, live_through);\n+  /* This is a trick to guarantee that new ranges is not merged with\n+     the old ones.  */\n+  ira_max_point++;\n+  start = ira_max_point;\n+  for (move = list; move != NULL; move = move->next)\n+    {\n+      from = move->from;\n+      to = move->to;\n+      if (ALLOCNO_CONFLICT_ALLOCNO_ARRAY (to) == NULL)\n+\t{\n+\t  if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file, \"    Allocate conflicts for a%dr%d\\n\",\n+\t\t     ALLOCNO_NUM (to), REGNO (ALLOCNO_REG (to)));\n+\t  ira_allocate_allocno_conflicts (to, n);\n+\t}\n+      bitmap_clear_bit (live_through, ALLOCNO_REGNO (from));\n+      bitmap_clear_bit (live_through, ALLOCNO_REGNO (to));\n+      IOR_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (from), hard_regs_live);\n+      IOR_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (to), hard_regs_live);\n+      IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (from),\n+\t\t\thard_regs_live);\n+      IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (to), hard_regs_live);\n+      update_costs (from, true, freq);\n+      update_costs (to, false, freq);\n+      cp = ira_add_allocno_copy (from, to, freq, move->insn, NULL);\n+      if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+\tfprintf (ira_dump_file, \"    Adding cp%d:a%dr%d-a%dr%d\\n\",\n+\t\t cp->num, ALLOCNO_NUM (cp->first),\n+\t\t REGNO (ALLOCNO_REG (cp->first)), ALLOCNO_NUM (cp->second),\n+\t\t REGNO (ALLOCNO_REG (cp->second)));\n+      r = ALLOCNO_LIVE_RANGES (from);\n+      if (r == NULL || r->finish >= 0)\n+\t{\n+\t  ALLOCNO_LIVE_RANGES (from)\n+\t    = ira_create_allocno_live_range (from, start, ira_max_point, r);\n+\t  if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file,\n+\t\t     \"    Adding range [%d..%d] to allocno a%dr%d\\n\",\n+\t\t     start, ira_max_point, ALLOCNO_NUM (from),\n+\t\t     REGNO (ALLOCNO_REG (from)));\n+\t}\n+      else\n+\tr->finish = ira_max_point;\n+      ira_max_point++;\n+      ALLOCNO_LIVE_RANGES (to)\n+\t= ira_create_allocno_live_range (to, ira_max_point, -1,\n+\t\t\t\t\t ALLOCNO_LIVE_RANGES (to));\n+      ira_max_point++;\n+    }\n+  for (move = list; move != NULL; move = move->next)\n+    {\n+      r = ALLOCNO_LIVE_RANGES (move->to);\n+      if (r->finish < 0)\n+\t{\n+\t  r->finish = ira_max_point - 1;\n+\t  if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file,\n+\t\t     \"    Adding range [%d..%d] to allocno a%dr%d\\n\",\n+\t\t     r->start, r->finish, ALLOCNO_NUM (move->to),\n+\t\t     REGNO (ALLOCNO_REG (move->to)));\n+\t}\n+    }\n+  EXECUTE_IF_SET_IN_BITMAP (live_through, FIRST_PSEUDO_REGISTER, regno, bi)\n+    {\n+      a = node->regno_allocno_map[regno];\n+      if (ALLOCNO_MEM_OPTIMIZED_DEST (a) == NULL)\n+\t{\n+\t  ALLOCNO_LIVE_RANGES (a)\n+\t    = ira_create_allocno_live_range (a, start, ira_max_point - 1,\n+\t\t\t\t\t     ALLOCNO_LIVE_RANGES (a));\n+\t  if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+\t    fprintf\n+\t      (ira_dump_file,\n+\t       \"    Adding range [%d..%d] to live through allocno a%dr%d\\n\",\n+\t       start, ira_max_point - 1, ALLOCNO_NUM (a),\n+\t       REGNO (ALLOCNO_REG (a)));\n+\t}\n+    }\n+}\n+\n+/* Process all move list to add ranges, conflicts, copies, and modify\n+   costs for allocnos involved in the moves.  */\n+static void\n+add_ranges_and_copies (void)\n+{\n+  basic_block bb;\n+  edge_iterator ei;\n+  edge e;\n+  ira_loop_tree_node_t node;\n+  bitmap live_through;\n+\n+  live_through = ira_allocate_bitmap ();\n+  FOR_EACH_BB (bb)\n+    {\n+      /* It does not matter what loop_tree_node (of source or\n+\t destination block) to use for searching allocnos by their\n+\t regnos because of subsequent IR flattening.  */\n+      node = IRA_BB_NODE (bb)->parent;\n+      bitmap_copy (live_through, DF_LR_IN (bb));\n+      add_range_and_copies_from_move_list\n+\t(at_bb_start[bb->index], node, live_through, REG_FREQ_FROM_BB (bb));\n+      bitmap_copy (live_through, DF_LR_OUT (bb));\n+      add_range_and_copies_from_move_list\n+\t(at_bb_end[bb->index], node, live_through, REG_FREQ_FROM_BB (bb));\n+      FOR_EACH_EDGE (e, ei, bb->succs)\n+\t{\n+\t  bitmap_and (live_through, DF_LR_IN (e->dest), DF_LR_OUT (bb));\n+\t  add_range_and_copies_from_move_list\n+\t    ((move_t) e->aux, node, live_through,\n+\t     REG_FREQ_FROM_EDGE_FREQ (EDGE_FREQUENCY (e)));\n+\t}\n+    }\n+  ira_free_bitmap (live_through);\n+}\n+\n+/* The entry function changes code and generates shuffling allocnos on\n+   region borders for the regional (LOOPS_P is TRUE in this case)\n+   register allocation.  */\n+void\n+ira_emit (bool loops_p)\n+{\n+  basic_block bb;\n+  edge_iterator ei;\n+  edge e;\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    ALLOCNO_REG (a) = regno_reg_rtx[ALLOCNO_REGNO (a)];\n+  if (! loops_p)\n+    return;\n+  at_bb_start = (move_t *) ira_allocate (sizeof (move_t) * last_basic_block);\n+  memset (at_bb_start, 0, sizeof (move_t) * last_basic_block);\n+  at_bb_end = (move_t *) ira_allocate (sizeof (move_t) * last_basic_block);\n+  memset (at_bb_end, 0, sizeof (move_t) * last_basic_block);\n+  local_allocno_bitmap = ira_allocate_bitmap ();\n+  used_regno_bitmap = ira_allocate_bitmap ();\n+  renamed_regno_bitmap = ira_allocate_bitmap ();\n+  max_regno_before_changing = max_reg_num ();\n+  ira_traverse_loop_tree (true, ira_loop_tree_root, change_loop, NULL);\n+  set_allocno_somewhere_renamed_p ();\n+  ira_free_bitmap (used_regno_bitmap);\n+  ira_free_bitmap (renamed_regno_bitmap);\n+  ira_free_bitmap (local_allocno_bitmap);\n+  FOR_EACH_BB (bb)\n+    {\n+      at_bb_start[bb->index] = NULL;\n+      at_bb_end[bb->index] = NULL;\n+      FOR_EACH_EDGE (e, ei, bb->succs)\n+\tif (e->dest != EXIT_BLOCK_PTR)\n+\t  generate_edge_moves (e);\n+    }\n+  allocno_last_set\n+    = (move_t *) ira_allocate (sizeof (move_t) * max_reg_num ());\n+  allocno_last_set_check\n+    = (int *) ira_allocate (sizeof (int) * max_reg_num ());\n+  memset (allocno_last_set_check, 0, sizeof (int) * max_reg_num ());\n+  memset (hard_regno_last_set_check, 0, sizeof (hard_regno_last_set_check));\n+  curr_tick = 0;\n+  FOR_EACH_BB (bb)\n+    unify_moves (bb, true);\n+  FOR_EACH_BB (bb)\n+    unify_moves (bb, false);\n+  move_vec = VEC_alloc (move_t, heap, ira_allocnos_num);\n+  emit_moves ();\n+  add_ranges_and_copies ();\n+  /* Clean up: */\n+  FOR_EACH_BB (bb)\n+    {\n+      free_move_list (at_bb_start[bb->index]);\n+      free_move_list (at_bb_end[bb->index]);\n+      FOR_EACH_EDGE (e, ei, bb->succs)\n+\t{\n+\t  free_move_list ((move_t) e->aux);\n+\t  e->aux = NULL;\n+\t}\n+    }\n+  VEC_free (move_t, heap, move_vec);\n+  ira_free (allocno_last_set_check);\n+  ira_free (allocno_last_set);\n+  commit_edge_insertions ();\n+  ira_free (at_bb_end);\n+  ira_free (at_bb_start);\n+}"}, {"sha": "f656cf16737a422864d86530901b46066fde7159", "filename": "gcc/ira-int.h", "status": "added", "additions": 1200, "deletions": 0, "changes": 1200, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-int.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-int.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira-int.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -0,0 +1,1200 @@\n+/* Integrated Register Allocator (IRA) intercommunication header file.\n+   Copyright (C) 2006, 2007, 2008\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"cfgloop.h\"\n+#include \"ira.h\"\n+#include \"alloc-pool.h\"\n+\n+/* To provide consistency in naming, all IRA external variables,\n+   functions, common typedefs start with prefix ira_.  */\n+\n+#ifdef ENABLE_CHECKING\n+#define ENABLE_IRA_CHECKING\n+#endif\n+\n+#ifdef ENABLE_IRA_CHECKING\n+#define ira_assert(c) gcc_assert (c)\n+#else\n+#define ira_assert(c)\n+#endif\n+\n+/* Compute register frequency from edge frequency FREQ.  It is\n+   analogous to REG_FREQ_FROM_BB.  When optimizing for size, or\n+   profile driven feedback is available and the function is never\n+   executed, frequency is always equivalent.  Otherwise rescale the\n+   edge frequency.  */\n+#define REG_FREQ_FROM_EDGE_FREQ(freq)\t\t\t\t\t      \\\n+  (optimize_size || (flag_branch_probabilities && !ENTRY_BLOCK_PTR->count)    \\\n+   ? REG_FREQ_MAX : (freq * REG_FREQ_MAX / BB_FREQ_MAX)\t\t\t      \\\n+   ? (freq * REG_FREQ_MAX / BB_FREQ_MAX) : 1)\n+\n+/* All natural loops.  */\n+extern struct loops ira_loops;\n+\n+/* A modified value of flag `-fira-verbose' used internally.  */\n+extern int internal_flag_ira_verbose;\n+\n+/* Dump file of the allocator if it is not NULL.  */\n+extern FILE *ira_dump_file;\n+\n+/* Typedefs for pointers to allocno live range, allocno, and copy of\n+   allocnos.  */\n+typedef struct ira_allocno_live_range *allocno_live_range_t;\n+typedef struct ira_allocno *ira_allocno_t;\n+typedef struct ira_allocno_copy *ira_copy_t;\n+\n+/* Definition of vector of allocnos and copies.  */\n+DEF_VEC_P(ira_allocno_t);\n+DEF_VEC_ALLOC_P(ira_allocno_t, heap);\n+DEF_VEC_P(ira_copy_t);\n+DEF_VEC_ALLOC_P(ira_copy_t, heap);\n+\n+/* Typedef for pointer to the subsequent structure.  */\n+typedef struct ira_loop_tree_node *ira_loop_tree_node_t;\n+\n+/* In general case, IRA is a regional allocator.  The regions are\n+   nested and form a tree.  Currently regions are natural loops.  The\n+   following structure describes loop tree node (representing basic\n+   block or loop).  We need such tree because the loop tree from\n+   cfgloop.h is not convenient for the optimization: basic blocks are\n+   not a part of the tree from cfgloop.h.  We also use the nodes for\n+   storing additional information about basic blocks/loops for the\n+   register allocation purposes.  */\n+struct ira_loop_tree_node\n+{\n+  /* The node represents basic block if children == NULL.  */\n+  basic_block bb;    /* NULL for loop.  */\n+  struct loop *loop; /* NULL for BB.  */\n+  /* The next (loop) node of with the same parent.  SUBLOOP_NEXT is\n+     always NULL for BBs. */\n+  ira_loop_tree_node_t subloop_next, next;\n+  /* The first (loop) node immediately inside the node.  SUBLOOPS is\n+     always NULL for BBs.  */\n+  ira_loop_tree_node_t subloops, children;\n+  /* The node immediately containing given node.  */\n+  ira_loop_tree_node_t parent;\n+\n+  /* Loop level in range [0, ira_loop_tree_height).  */\n+  int level;\n+\n+  /* All the following members are defined only for nodes representing\n+     loops.  */\n+\n+  /* Allocnos in the loop corresponding to their regnos.  If it is\n+     NULL the loop does not form a separate register allocation region\n+     (e.g. because it has abnormal enter/exit edges and we can not put\n+     code for register shuffling on the edges if a different\n+     allocation is used for a pseudo-register on different sides of\n+     the edges).  Caps are not in the map (remember we can have more\n+     one cap with the same regno in a region).  */\n+  ira_allocno_t *regno_allocno_map;\n+\n+  /* Maximal register pressure inside loop for given register class\n+     (defined only for the cover classes).  */\n+  int reg_pressure[N_REG_CLASSES];\n+\n+  /* Numbers of allocnos referred in the loop node.  */\n+  bitmap mentioned_allocnos;\n+\n+  /* Regnos of pseudos modified in the loop node (including its\n+     subloops).  */\n+  bitmap modified_regnos;\n+\n+  /* Numbers of allocnos living at the loop borders.  */\n+  bitmap border_allocnos;\n+\n+  /* Numbers of copies referred in the corresponding loop.  */\n+  bitmap local_copies;\n+};\n+\n+/* The root of the loop tree corresponding to the all function.  */\n+extern ira_loop_tree_node_t ira_loop_tree_root;\n+\n+/* Height of the loop tree.  */\n+extern int ira_loop_tree_height;\n+\n+/* All nodes representing basic blocks are referred through the\n+   following array.  We can not use basic block member `aux' for this\n+   because it is used for insertion of insns on edges.  */\n+extern ira_loop_tree_node_t ira_bb_nodes;\n+\n+/* Two access macros to the nodes representing basic blocks.  */\n+#if defined ENABLE_IRA_CHECKING && (GCC_VERSION >= 2007)\n+#define IRA_BB_NODE_BY_INDEX(index) __extension__\t\t\t\\\n+(({ ira_loop_tree_node_t _node = (&ira_bb_nodes[index]);\t\\\n+     if (_node->children != NULL || _node->loop != NULL || _node->bb == NULL)\\\n+       {\t\t\t\t\t\t\t\t\\\n+         fprintf (stderr,\t\t\t\t\t\t\\\n+                  \"\\n%s: %d: error in %s: it is not a block node\\n\",\t\\\n+                  __FILE__, __LINE__, __FUNCTION__);\t\t\t\\\n+         gcc_unreachable ();\t\t\t\t\t\t\\\n+       }\t\t\t\t\t\t\t\t\\\n+     _node; }))\n+#else\n+#define IRA_BB_NODE_BY_INDEX(index) (&ira_bb_nodes[index])\n+#endif\n+\n+#define IRA_BB_NODE(bb) IRA_BB_NODE_BY_INDEX ((bb)->index)\n+\n+/* All nodes representing loops are referred through the following\n+   array.  */\n+extern ira_loop_tree_node_t ira_loop_nodes;\n+\n+/* Two access macros to the nodes representing loops.  */\n+#if defined ENABLE_IRA_CHECKING && (GCC_VERSION >= 2007)\n+#define IRA_LOOP_NODE_BY_INDEX(index) __extension__\t\t\t\\\n+(({ ira_loop_tree_node_t const _node = (&ira_loop_nodes[index]);\\\n+     if (_node->children == NULL || _node->bb != NULL || _node->loop == NULL)\\\n+       {\t\t\t\t\t\t\t\t\\\n+         fprintf (stderr,\t\t\t\t\t\t\\\n+                  \"\\n%s: %d: error in %s: it is not a loop node\\n\",\t\\\n+                  __FILE__, __LINE__, __FUNCTION__);\t\t\t\\\n+         gcc_unreachable ();\t\t\t\t\t\t\\\n+       }\t\t\t\t\t\t\t\t\\\n+     _node; }))\n+#else\n+#define IRA_LOOP_NODE_BY_INDEX(index) (&ira_loop_nodes[index])\n+#endif\n+\n+#define IRA_LOOP_NODE(loop) IRA_LOOP_NODE_BY_INDEX ((loop)->num)\n+\n+\f\n+\n+/* The structure describes program points where a given allocno lives.\n+   To save memory we store allocno conflicts only for the same cover\n+   class allocnos which is enough to assign hard registers.  To find\n+   conflicts for other allocnos (e.g. to assign stack memory slot) we\n+   use the live ranges.  If the live ranges of two allocnos are\n+   intersected, the allocnos are in conflict.  */\n+struct ira_allocno_live_range\n+{\n+  /* Allocno whose live range is described by given structure.  */\n+  ira_allocno_t allocno;\n+  /* Program point range.  */\n+  int start, finish;\n+  /* Next structure describing program points where the allocno\n+     lives.  */\n+  allocno_live_range_t next;\n+  /* Pointer to structures with the same start/finish.  */\n+  allocno_live_range_t start_next, finish_next;\n+};\n+\n+/* Program points are enumerated by numbers from range\n+   0..IRA_MAX_POINT-1.  There are approximately two times more program\n+   points than insns.  Program points are places in the program where\n+   liveness info can be changed.  In most general case (there are more\n+   complicated cases too) some program points correspond to places\n+   where input operand dies and other ones correspond to places where\n+   output operands are born.  */\n+extern int ira_max_point;\n+\n+/* Arrays of size IRA_MAX_POINT mapping a program point to the allocno\n+   live ranges with given start/finish point.  */\n+extern allocno_live_range_t *ira_start_point_ranges, *ira_finish_point_ranges;\n+\n+/* A structure representing an allocno (allocation entity).  Allocno\n+   represents a pseudo-register in an allocation region.  If\n+   pseudo-register does not live in a region but it lives in the\n+   nested regions, it is represented in the region by special allocno\n+   called *cap*.  There may be more one cap representing the same\n+   pseudo-register in region.  It means that the corresponding\n+   pseudo-register lives in more one non-intersected subregion.  */\n+struct ira_allocno\n+{\n+  /* The allocno order number starting with 0.  Each allocno has an\n+     unique number and the number is never changed for the\n+     allocno.  */\n+  int num;\n+  /* Regno for allocno or cap.  */\n+  int regno;\n+  /* Mode of the allocno which is the mode of the corresponding\n+     pseudo-register.  */\n+  enum machine_mode mode;\n+  /* Final rtx representation of the allocno.  */\n+  rtx reg;\n+  /* Hard register assigned to given allocno.  Negative value means\n+     that memory was allocated to the allocno.  During the reload,\n+     spilled allocno has value equal to the corresponding stack slot\n+     number (0, ...) - 2.  Value -1 is used for allocnos spilled by the\n+     reload (at this point pseudo-register has only one allocno) which\n+     did not get stack slot yet.  */\n+  int hard_regno;\n+  /* Allocnos with the same regno are linked by the following member.\n+     Allocnos corresponding to inner loops are first in the list (it\n+     corresponds to depth-first traverse of the loops).  */\n+  ira_allocno_t next_regno_allocno;\n+  /* There may be different allocnos with the same regno in different\n+     regions.  Allocnos are bound to the corresponding loop tree node.\n+     Pseudo-register may have only one regular allocno with given loop\n+     tree node but more than one cap (see comments above).  */\n+  ira_loop_tree_node_t loop_tree_node;\n+  /* Accumulated usage references of the allocno.  Here and below,\n+     word 'accumulated' means info for given region and all nested\n+     subregions.  In this case, 'accumulated' means sum of references\n+     of the corresponding pseudo-register in this region and in all\n+     nested subregions recursively. */\n+  int nrefs;\n+  /* Accumulated frequency of usage of the allocno.  */\n+  int freq;\n+  /* Register class which should be used for allocation for given\n+     allocno.  NO_REGS means that we should use memory.  */\n+  enum reg_class cover_class;\n+  /* Minimal accumulated cost of usage register of the cover class for\n+     the allocno.  */\n+  int cover_class_cost;\n+  /* Minimal accumulated, and updated costs of memory for the allocno.\n+     At the allocation start, the original and updated costs are\n+     equal.  The updated cost may be changed after finishing\n+     allocation in a region and starting allocation in a subregion.\n+     The change reflects the cost of spill/restore code on the\n+     subregion border if we assign memory to the pseudo in the\n+     subregion.  */\n+  int memory_cost, updated_memory_cost;\n+  /* Accumulated number of points where the allocno lives and there is\n+     excess pressure for its class.  Excess pressure for a register\n+     class at some point means that there are more allocnos of given\n+     register class living at the point than number of hard-registers\n+     of the class available for the allocation.  */\n+  int excess_pressure_points_num;\n+  /* Copies to other non-conflicting allocnos.  The copies can\n+     represent move insn or potential move insn usually because of two\n+     operand insn constraints.  */\n+  ira_copy_t allocno_copies;\n+  /* It is a allocno (cap) representing given allocno on upper loop tree\n+     level.  */\n+  ira_allocno_t cap;\n+  /* It is a link to allocno (cap) on lower loop level represented by\n+     given cap.  Null if given allocno is not a cap.  */\n+  ira_allocno_t cap_member;\n+  /* Coalesced allocnos form a cyclic list.  One allocno given by\n+     FIRST_COALESCED_ALLOCNO represents all coalesced allocnos.  The\n+     list is chained by NEXT_COALESCED_ALLOCNO.  */\n+  ira_allocno_t first_coalesced_allocno;\n+  ira_allocno_t next_coalesced_allocno;\n+  /* Pointer to structures describing at what program point the\n+     allocno lives.  We always maintain the list in such way that *the\n+     ranges in the list are not intersected and ordered by decreasing\n+     their program points*.  */\n+  allocno_live_range_t live_ranges;\n+  /* Before building conflicts the two member values are\n+     correspondingly minimal and maximal points of the accumulated\n+     allocno live ranges.  After building conflicts the values are\n+     correspondingly minimal and maximal conflict ids of allocnos with\n+     which given allocno can conflict.  */\n+  int min, max;\n+  /* The unique member value represents given allocno in conflict bit\n+     vectors.  */\n+  int conflict_id;\n+  /* Vector of accumulated conflicting allocnos with NULL end marker\n+     (if CONFLICT_VEC_P is true) or conflict bit vector otherwise.\n+     Only allocnos with the same cover class are in the vector or in\n+     the bit vector.  */\n+  void *conflict_allocno_array;\n+  /* Allocated size of the previous array.  */\n+  unsigned int conflict_allocno_array_size;\n+  /* Number of accumulated conflicts in the vector of conflicting\n+     allocnos.  */\n+  int conflict_allocnos_num;\n+  /* Initial and accumulated hard registers conflicting with this\n+     allocno and as a consequences can not be assigned to the allocno.\n+     All non-allocatable hard regs and hard regs of cover classes\n+     different from given allocno one are included in the sets.  */\n+  HARD_REG_SET conflict_hard_regs, total_conflict_hard_regs;\n+  /* Accumulated frequency of calls which given allocno\n+     intersects.  */\n+  int call_freq;\n+  /* Length of the previous array (number of the intersected calls).  */\n+  int calls_crossed_num;\n+  /* Non NULL if we remove restoring value from given allocno to\n+     MEM_OPTIMIZED_DEST at loop exit (see ira-emit.c) because the\n+     allocno value is not changed inside the loop.  */\n+  ira_allocno_t mem_optimized_dest;\n+  /* TRUE if the allocno assigned to memory was a destination of\n+     removed move (see ira-emit.c) at loop exit because the value of\n+     the corresponding pseudo-register is not changed inside the\n+     loop.  */\n+  unsigned int mem_optimized_dest_p : 1;\n+  /* TRUE if the corresponding pseudo-register has disjoint live\n+     ranges and the other allocnos of the pseudo-register except this\n+     one changed REG.  */\n+  unsigned int somewhere_renamed_p : 1;\n+  /* TRUE if allocno with the same REGNO in a subregion has been\n+     renamed, in other words, got a new pseudo-register.  */\n+  unsigned int child_renamed_p : 1;\n+  /* During the reload, value TRUE means that we should not reassign a\n+     hard register to the allocno got memory earlier.  It is set up\n+     when we removed memory-memory move insn before each iteration of\n+     the reload.  */\n+  unsigned int dont_reassign_p : 1;\n+#ifdef STACK_REGS\n+  /* Set to TRUE if allocno can't be assigned to the stack hard\n+     register correspondingly in this region and area including the\n+     region and all its subregions recursively.  */\n+  unsigned int no_stack_reg_p : 1, total_no_stack_reg_p : 1;\n+#endif\n+  /* TRUE value means that the allocno was not removed yet from the\n+     conflicting graph during colouring.  */\n+  unsigned int in_graph_p : 1;\n+  /* TRUE if a hard register or memory has been assigned to the\n+     allocno.  */\n+  unsigned int assigned_p : 1;\n+  /* TRUE if it is put on the stack to make other allocnos\n+     colorable.  */\n+  unsigned int may_be_spilled_p : 1;\n+  /* TRUE if the allocno was removed from the splay tree used to\n+     choose allocn for spilling (see ira-color.c::.  */\n+  unsigned int splay_removed_p : 1;\n+  /* TRUE if conflicts for given allocno are represented by vector of\n+     pointers to the conflicting allocnos.  Otherwise, we use a bit\n+     vector where a bit with given index represents allocno with the\n+     same number.  */\n+  unsigned int conflict_vec_p : 1;\n+  /* Array of usage costs (accumulated and the one updated during\n+     coloring) for each hard register of the allocno cover class.  The\n+     member value can be NULL if all costs are the same and equal to\n+     COVER_CLASS_COST.  For example, the costs of two different hard\n+     registers can be different if one hard register is callee-saved\n+     and another one is callee-used and the allocno lives through\n+     calls.  Another example can be case when for some insn the\n+     corresponding pseudo-register value should be put in specific\n+     register class (e.g. AREG for x86) which is a strict subset of\n+     the allocno cover class (GENERAL_REGS for x86).  We have updated\n+     costs to reflect the situation when the usage cost of a hard\n+     register is decreased because the allocno is connected to another\n+     allocno by a copy and the another allocno has been assigned to\n+     the hard register.  */\n+  int *hard_reg_costs, *updated_hard_reg_costs;\n+  /* Array of decreasing costs (accumulated and the one updated during\n+     coloring) for allocnos conflicting with given allocno for hard\n+     regno of the allocno cover class.  The member value can be NULL\n+     if all costs are the same.  These costs are used to reflect\n+     preferences of other allocnos not assigned yet during assigning\n+     to given allocno.  */\n+  int *conflict_hard_reg_costs, *updated_conflict_hard_reg_costs;\n+  /* Number of the same cover class allocnos with TRUE in_graph_p\n+     value and conflicting with given allocno during each point of\n+     graph coloring.  */\n+  int left_conflicts_num;\n+  /* Number of hard registers of the allocno cover class really\n+     available for the allocno allocation.  */\n+  int available_regs_num;\n+  /* Allocnos in a bucket (used in coloring) chained by the following\n+     two members.  */\n+  ira_allocno_t next_bucket_allocno;\n+  ira_allocno_t prev_bucket_allocno;\n+  /* Used for temporary purposes.  */\n+  int temp;\n+};\n+\n+/* All members of the allocno structures should be accessed only\n+   through the following macros.  */\n+#define ALLOCNO_NUM(A) ((A)->num)\n+#define ALLOCNO_REGNO(A) ((A)->regno)\n+#define ALLOCNO_REG(A) ((A)->reg)\n+#define ALLOCNO_NEXT_REGNO_ALLOCNO(A) ((A)->next_regno_allocno)\n+#define ALLOCNO_LOOP_TREE_NODE(A) ((A)->loop_tree_node)\n+#define ALLOCNO_CAP(A) ((A)->cap)\n+#define ALLOCNO_CAP_MEMBER(A) ((A)->cap_member)\n+#define ALLOCNO_CONFLICT_ALLOCNO_ARRAY(A) ((A)->conflict_allocno_array)\n+#define ALLOCNO_CONFLICT_ALLOCNO_ARRAY_SIZE(A) \\\n+  ((A)->conflict_allocno_array_size)\n+#define ALLOCNO_CONFLICT_ALLOCNOS_NUM(A) \\\n+  ((A)->conflict_allocnos_num)\n+#define ALLOCNO_CONFLICT_HARD_REGS(A) ((A)->conflict_hard_regs)\n+#define ALLOCNO_TOTAL_CONFLICT_HARD_REGS(A) ((A)->total_conflict_hard_regs)\n+#define ALLOCNO_NREFS(A) ((A)->nrefs)\n+#define ALLOCNO_FREQ(A) ((A)->freq)\n+#define ALLOCNO_HARD_REGNO(A) ((A)->hard_regno)\n+#define ALLOCNO_CALL_FREQ(A) ((A)->call_freq)\n+#define ALLOCNO_CALLS_CROSSED_NUM(A) ((A)->calls_crossed_num)\n+#define ALLOCNO_MEM_OPTIMIZED_DEST(A) ((A)->mem_optimized_dest)\n+#define ALLOCNO_MEM_OPTIMIZED_DEST_P(A) ((A)->mem_optimized_dest_p)\n+#define ALLOCNO_SOMEWHERE_RENAMED_P(A) ((A)->somewhere_renamed_p)\n+#define ALLOCNO_CHILD_RENAMED_P(A) ((A)->child_renamed_p)\n+#define ALLOCNO_DONT_REASSIGN_P(A) ((A)->dont_reassign_p)\n+#ifdef STACK_REGS\n+#define ALLOCNO_NO_STACK_REG_P(A) ((A)->no_stack_reg_p)\n+#define ALLOCNO_TOTAL_NO_STACK_REG_P(A) ((A)->total_no_stack_reg_p)\n+#endif\n+#define ALLOCNO_IN_GRAPH_P(A) ((A)->in_graph_p)\n+#define ALLOCNO_ASSIGNED_P(A) ((A)->assigned_p)\n+#define ALLOCNO_MAY_BE_SPILLED_P(A) ((A)->may_be_spilled_p)\n+#define ALLOCNO_SPLAY_REMOVED_P(A) ((A)->splay_removed_p)\n+#define ALLOCNO_CONFLICT_VEC_P(A) ((A)->conflict_vec_p)\n+#define ALLOCNO_MODE(A) ((A)->mode)\n+#define ALLOCNO_COPIES(A) ((A)->allocno_copies)\n+#define ALLOCNO_HARD_REG_COSTS(A) ((A)->hard_reg_costs)\n+#define ALLOCNO_UPDATED_HARD_REG_COSTS(A) ((A)->updated_hard_reg_costs)\n+#define ALLOCNO_CONFLICT_HARD_REG_COSTS(A) \\\n+  ((A)->conflict_hard_reg_costs)\n+#define ALLOCNO_UPDATED_CONFLICT_HARD_REG_COSTS(A) \\\n+  ((A)->updated_conflict_hard_reg_costs)\n+#define ALLOCNO_LEFT_CONFLICTS_NUM(A) ((A)->left_conflicts_num)\n+#define ALLOCNO_COVER_CLASS(A) ((A)->cover_class)\n+#define ALLOCNO_COVER_CLASS_COST(A) ((A)->cover_class_cost)\n+#define ALLOCNO_MEMORY_COST(A) ((A)->memory_cost)\n+#define ALLOCNO_UPDATED_MEMORY_COST(A) ((A)->updated_memory_cost)\n+#define ALLOCNO_EXCESS_PRESSURE_POINTS_NUM(A) ((A)->excess_pressure_points_num)\n+#define ALLOCNO_AVAILABLE_REGS_NUM(A) ((A)->available_regs_num)\n+#define ALLOCNO_NEXT_BUCKET_ALLOCNO(A) ((A)->next_bucket_allocno)\n+#define ALLOCNO_PREV_BUCKET_ALLOCNO(A) ((A)->prev_bucket_allocno)\n+#define IRA_ALLOCNO_TEMP(A) ((A)->temp)\n+#define ALLOCNO_FIRST_COALESCED_ALLOCNO(A) ((A)->first_coalesced_allocno)\n+#define ALLOCNO_NEXT_COALESCED_ALLOCNO(A) ((A)->next_coalesced_allocno)\n+#define ALLOCNO_LIVE_RANGES(A) ((A)->live_ranges)\n+#define ALLOCNO_MIN(A) ((A)->min)\n+#define ALLOCNO_MAX(A) ((A)->max)\n+#define ALLOCNO_CONFLICT_ID(A) ((A)->conflict_id)\n+\n+/* Map regno -> allocnos with given regno (see comments for \n+   allocno member `next_regno_allocno').  */\n+extern ira_allocno_t *ira_regno_allocno_map;\n+\n+/* Array of references to all allocnos.  The order number of the\n+   allocno corresponds to the index in the array.  Removed allocnos\n+   have NULL element value.  */\n+extern ira_allocno_t *ira_allocnos;\n+\n+/* Sizes of the previous array.  */\n+extern int ira_allocnos_num;\n+\n+/* Map conflict id -> allocno with given conflict id (see comments for\n+   allocno member `conflict_id').  */\n+extern ira_allocno_t *ira_conflict_id_allocno_map;\n+\n+/* The following structure represents a copy of two allocnos.  The\n+   copies represent move insns or potential move insns usually because\n+   of two operand insn constraints.  To remove register shuffle, we\n+   also create copies between allocno which is output of an insn and\n+   allocno becoming dead in the insn.  */\n+struct ira_allocno_copy\n+{\n+  /* The unique order number of the copy node starting with 0.  */\n+  int num;\n+  /* Allocnos connected by the copy.  The first allocno should have\n+     smaller order number than the second one.  */\n+  ira_allocno_t first, second;\n+  /* Execution frequency of the copy.  */\n+  int freq;\n+  /* It is a move insn which is an origin of the copy.  The member\n+     value for the copy representing two operand insn constraints or\n+     for the copy created to remove register shuffle is NULL.  In last\n+     case the copy frequency is smaller than the corresponding insn\n+     execution frequency.  */\n+  rtx insn;\n+  /* All copies with the same allocno as FIRST are linked by the two\n+     following members.  */\n+  ira_copy_t prev_first_allocno_copy, next_first_allocno_copy;\n+  /* All copies with the same allocno as SECOND are linked by the two\n+     following members.  */\n+  ira_copy_t prev_second_allocno_copy, next_second_allocno_copy;\n+  /* Region from which given copy is originated.  */\n+  ira_loop_tree_node_t loop_tree_node;\n+};\n+\n+/* Array of references to all copies.  The order number of the copy\n+   corresponds to the index in the array.  Removed copies have NULL\n+   element value.  */\n+extern ira_copy_t *ira_copies;\n+\n+/* Size of the previous array.  */\n+extern int ira_copies_num;\n+\n+/* The following structure describes a stack slot used for spilled\n+   pseudo-registers.  */\n+struct ira_spilled_reg_stack_slot\n+{\n+  /* pseudo-registers assigned to the stack slot.  */\n+  regset_head spilled_regs;\n+  /* RTL representation of the stack slot.  */\n+  rtx mem;\n+  /* Size of the stack slot.  */\n+  unsigned int width;\n+};\n+\n+/* The number of elements in the following array.  */\n+extern int ira_spilled_reg_stack_slots_num;\n+\n+/* The following array contains info about spilled pseudo-registers\n+   stack slots used in current function so far.  */\n+extern struct ira_spilled_reg_stack_slot *ira_spilled_reg_stack_slots;\n+\n+/* Correspondingly overall cost of the allocation, cost of the\n+   allocnos assigned to hard-registers, cost of the allocnos assigned\n+   to memory, cost of loads, stores and register move insns generated\n+   for pseudo-register live range splitting (see ira-emit.c).  */\n+extern int ira_overall_cost;\n+extern int ira_reg_cost, ira_mem_cost;\n+extern int ira_load_cost, ira_store_cost, ira_shuffle_cost;\n+extern int ira_move_loops_num, ira_additional_jumps_num;\n+\n+/* Map: register class x machine mode -> number of hard registers of\n+   given class needed to store value of given mode.  If the number for\n+   some hard-registers of the register class is different, the size\n+   will be negative.  */\n+extern int ira_reg_class_nregs[N_REG_CLASSES][MAX_MACHINE_MODE];\n+\n+/* Maximal value of the previous array elements.  */\n+extern int ira_max_nregs;\n+\n+/* The number of bits in each element of array used to implement a bit\n+   vector of allocnos and what type that element has.  We use the\n+   largest integer format on the host machine.  */\n+#define IRA_INT_BITS HOST_BITS_PER_WIDE_INT\n+#define IRA_INT_TYPE HOST_WIDE_INT\n+\n+/* Set, clear or test bit number I in R, a bit vector of elements with\n+   minimal index and maximal index equal correspondingly to MIN and\n+   MAX.  */\n+#if defined ENABLE_IRA_CHECKING && (GCC_VERSION >= 2007)\n+\n+#define SET_ALLOCNO_SET_BIT(R, I, MIN, MAX) __extension__\t        \\\n+  (({ int _min = (MIN), _max = (MAX), _i = (I);\t\t\t\t\\\n+     if (_i < _min || _i > _max)\t\t\t\t\t\\\n+       {\t\t\t\t\t\t\t\t\\\n+         fprintf (stderr,\t\t\t\t\t\t\\\n+                  \"\\n%s: %d: error in %s: %d not in range [%d,%d]\\n\",   \\\n+                  __FILE__, __LINE__, __FUNCTION__, _i, _min, _max);\t\\\n+         gcc_unreachable ();\t\t\t\t\t\t\\\n+       }\t\t\t\t\t\t\t\t\\\n+     ((R)[(unsigned) (_i - _min) / IRA_INT_BITS]\t\t\t\\\n+      |= ((IRA_INT_TYPE) 1 << ((unsigned) (_i - _min) % IRA_INT_BITS))); }))\n+  \n+\n+#define CLEAR_ALLOCNO_SET_BIT(R, I, MIN, MAX) __extension__\t        \\\n+  (({ int _min = (MIN), _max = (MAX), _i = (I);\t\t\t\t\\\n+     if (_i < _min || _i > _max)\t\t\t\t\t\\\n+       {\t\t\t\t\t\t\t\t\\\n+         fprintf (stderr,\t\t\t\t\t\t\\\n+                  \"\\n%s: %d: error in %s: %d not in range [%d,%d]\\n\",   \\\n+                  __FILE__, __LINE__, __FUNCTION__, _i, _min, _max);\t\\\n+         gcc_unreachable ();\t\t\t\t\t\t\\\n+       }\t\t\t\t\t\t\t\t\\\n+     ((R)[(unsigned) (_i - _min) / IRA_INT_BITS]\t\t\t\\\n+      &= ~((IRA_INT_TYPE) 1 << ((unsigned) (_i - _min) % IRA_INT_BITS))); }))\n+\n+#define TEST_ALLOCNO_SET_BIT(R, I, MIN, MAX) __extension__\t        \\\n+  (({ int _min = (MIN), _max = (MAX), _i = (I);\t\t\t\t\\\n+     if (_i < _min || _i > _max)\t\t\t\t\t\\\n+       {\t\t\t\t\t\t\t\t\\\n+         fprintf (stderr,\t\t\t\t\t\t\\\n+                  \"\\n%s: %d: error in %s: %d not in range [%d,%d]\\n\",   \\\n+                  __FILE__, __LINE__, __FUNCTION__, _i, _min, _max);\t\\\n+         gcc_unreachable ();\t\t\t\t\t\t\\\n+       }\t\t\t\t\t\t\t\t\\\n+     ((R)[(unsigned) (_i - _min) / IRA_INT_BITS]\t\t\t\\\n+      & ((IRA_INT_TYPE) 1 << ((unsigned) (_i - _min) % IRA_INT_BITS))); }))\n+\n+#else\n+\n+#define SET_ALLOCNO_SET_BIT(R, I, MIN, MAX)\t\t\t\\\n+  ((R)[(unsigned) ((I) - (MIN)) / IRA_INT_BITS]\t\t\t\\\n+   |= ((IRA_INT_TYPE) 1 << ((unsigned) ((I) - (MIN)) % IRA_INT_BITS)))\n+\n+#define CLEAR_ALLOCNO_SET_BIT(R, I, MIN, MAX)\t\t\t\\\n+  ((R)[(unsigned) ((I) - (MIN)) / IRA_INT_BITS]\t\t\t\\\n+   &= ~((IRA_INT_TYPE) 1 << ((unsigned) ((I) - (MIN)) % IRA_INT_BITS)))\n+\n+#define TEST_ALLOCNO_SET_BIT(R, I, MIN, MAX)\t\t\t\\\n+  ((R)[(unsigned) ((I) - (MIN)) / IRA_INT_BITS]\t\t\t\\\n+   & ((IRA_INT_TYPE) 1 << ((unsigned) ((I) - (MIN)) % IRA_INT_BITS)))\n+\n+#endif\n+\n+/* The iterator for allocno set implemented ed as allocno bit\n+   vector.  */\n+typedef struct {\n+\n+  /* Array containing the allocno bit vector.  */\n+  IRA_INT_TYPE *vec;\n+\n+  /* The number of the current element in the vector.  */\n+  unsigned int word_num;\n+\n+  /* The number of bits in the bit vector.  */\n+  unsigned int nel;\n+\n+  /* The current bit index of the bit vector.  */\n+  unsigned int bit_num;\n+\n+  /* Index corresponding to the 1st bit of the bit vector.   */\n+  int start_val;\n+\n+  /* The word of the bit vector currently visited.  */\n+  unsigned IRA_INT_TYPE word;\n+} ira_allocno_set_iterator;\n+\n+/* Initialize the iterator I for allocnos bit vector VEC containing\n+   minimal and maximal values MIN and MAX.  */\n+static inline void\n+ira_allocno_set_iter_init (ira_allocno_set_iterator *i,\n+\t\t\t   IRA_INT_TYPE *vec, int min, int max)\n+{\n+  i->vec = vec;\n+  i->word_num = 0;\n+  i->nel = max < min ? 0 : max - min + 1;\n+  i->start_val = min;\n+  i->bit_num = 0;\n+  i->word = i->nel == 0 ? 0 : vec[0];\n+}\n+\n+/* Return TRUE if we have more allocnos to visit, in which case *N is\n+   set to the allocno number to be visited.  Otherwise, return\n+   FALSE.  */\n+static inline bool\n+ira_allocno_set_iter_cond (ira_allocno_set_iterator *i, int *n)\n+{\n+  /* Skip words that are zeros.  */\n+  for (; i->word == 0; i->word = i->vec[i->word_num])\n+    {\n+      i->word_num++;\n+      i->bit_num = i->word_num * IRA_INT_BITS;\n+      \n+      /* If we have reached the end, break.  */\n+      if (i->bit_num >= i->nel)\n+\treturn false;\n+    }\n+  \n+  /* Skip bits that are zero.  */\n+  for (; (i->word & 1) == 0; i->word >>= 1)\n+    i->bit_num++;\n+  \n+  *n = (int) i->bit_num + i->start_val;\n+  \n+  return true;\n+}\n+\n+/* Advance to the next allocno in the set.  */\n+static inline void\n+ira_allocno_set_iter_next (ira_allocno_set_iterator *i)\n+{\n+  i->word >>= 1;\n+  i->bit_num++;\n+}\n+\n+/* Loop over all elements of allocno set given by bit vector VEC and\n+   their minimal and maximal values MIN and MAX.  In each iteration, N\n+   is set to the number of next allocno.  ITER is an instance of\n+   ira_allocno_set_iterator used to iterate the allocnos in the set.  */\n+#define FOR_EACH_ALLOCNO_IN_SET(VEC, MIN, MAX, N, ITER)\t\t\\\n+  for (ira_allocno_set_iter_init (&(ITER), (VEC), (MIN), (MAX));\t\\\n+       ira_allocno_set_iter_cond (&(ITER), &(N));\t\t\t\\\n+       ira_allocno_set_iter_next (&(ITER)))\n+\n+/* ira.c: */\n+\n+/* Hard regsets whose all bits are correspondingly zero or one.  */\n+extern HARD_REG_SET ira_zero_hard_reg_set;\n+extern HARD_REG_SET ira_one_hard_reg_set;\n+\n+/* Map: hard regs X modes -> set of hard registers for storing value\n+   of given mode starting with given hard register.  */\n+extern HARD_REG_SET ira_reg_mode_hard_regset\n+                    [FIRST_PSEUDO_REGISTER][NUM_MACHINE_MODES];\n+\n+/* Arrays analogous to macros MEMORY_MOVE_COST and\n+   REGISTER_MOVE_COST.  */\n+extern short ira_memory_move_cost[MAX_MACHINE_MODE][N_REG_CLASSES][2];\n+extern move_table *ira_register_move_cost[MAX_MACHINE_MODE];\n+\n+/* Similar to may_move_in_cost but it is calculated in IRA instead of\n+   regclass.  Another difference we take only available hard registers\n+   into account to figure out that one register class is a subset of\n+   the another one.  */\n+extern move_table *ira_may_move_in_cost[MAX_MACHINE_MODE];\n+\n+/* Similar to may_move_out_cost but it is calculated in IRA instead of\n+   regclass.  Another difference we take only available hard registers\n+   into account to figure out that one register class is a subset of\n+   the another one.  */\n+extern move_table *ira_may_move_out_cost[MAX_MACHINE_MODE];\n+\n+/* Register class subset relation: TRUE if the first class is a subset\n+   of the second one considering only hard registers available for the\n+   allocation.  */\n+extern int ira_class_subset_p[N_REG_CLASSES][N_REG_CLASSES];\n+\n+/* Array of number of hard registers of given class which are\n+   available for the allocation.  The order is defined by the\n+   allocation order.  */\n+extern short ira_class_hard_regs[N_REG_CLASSES][FIRST_PSEUDO_REGISTER];\n+\n+/* The number of elements of the above array for given register\n+   class.  */\n+extern int ira_class_hard_regs_num[N_REG_CLASSES];\n+\n+/* Index (in ira_class_hard_regs) for given register class and hard\n+   register (in general case a hard register can belong to several\n+   register classes).  The index is negative for hard registers\n+   unavailable for the allocation. */\n+extern short ira_class_hard_reg_index[N_REG_CLASSES][FIRST_PSEUDO_REGISTER];\n+\n+/* Function specific hard registers can not be used for the register\n+   allocation.  */\n+extern HARD_REG_SET ira_no_alloc_regs;\n+\n+/* Number of given class hard registers available for the register\n+   allocation for given classes.  */\n+extern int ira_available_class_regs[N_REG_CLASSES];\n+\n+/* Array whose values are hard regset of hard registers available for\n+   the allocation of given register class whose HARD_REGNO_MODE_OK\n+   values for given mode are zero.  */\n+extern HARD_REG_SET prohibited_class_mode_regs\n+                    [N_REG_CLASSES][NUM_MACHINE_MODES];\n+\n+/* Array whose values are hard regset of hard registers for which\n+   move of the hard register in given mode into itself is\n+   prohibited.  */\n+extern HARD_REG_SET ira_prohibited_mode_move_regs[NUM_MACHINE_MODES];\n+\n+/* Number of cover classes.  Cover classes is non-intersected register\n+   classes containing all hard-registers available for the\n+   allocation.  */\n+extern int ira_reg_class_cover_size;\n+\n+/* The array containing cover classes (see also comments for macro\n+   IRA_COVER_CLASSES).  Only first IRA_REG_CLASS_COVER_SIZE elements are\n+   used for this.  */\n+extern enum reg_class ira_reg_class_cover[N_REG_CLASSES];\n+\n+/* The value is number of elements in the subsequent array.  */\n+extern int ira_important_classes_num;\n+\n+/* The array containing non-empty classes (including non-empty cover\n+   classes) which are subclasses of cover classes.  Such classes is\n+   important for calculation of the hard register usage costs.  */\n+extern enum reg_class ira_important_classes[N_REG_CLASSES];\n+\n+/* The array containing indexes of important classes in the previous\n+   array.  The array elements are defined only for important\n+   classes.  */\n+extern int ira_important_class_nums[N_REG_CLASSES];\n+\n+/* Map of all register classes to corresponding cover class containing\n+   the given class.  If given class is not a subset of a cover class,\n+   we translate it into the cheapest cover class.  */\n+extern enum reg_class ira_class_translate[N_REG_CLASSES];\n+\n+/* The biggest important class inside of intersection of the two\n+   classes (that is calculated taking only hard registers available\n+   for allocation into account).  If the both classes contain no hard\n+   registers available for allocation, the value is calculated with\n+   taking all hard-registers including fixed ones into account.  */\n+extern enum reg_class ira_reg_class_intersect[N_REG_CLASSES][N_REG_CLASSES];\n+\n+/* The biggest important class inside of union of the two classes\n+   (that is calculated taking only hard registers available for\n+   allocation into account).  If the both classes contain no hard\n+   registers available for allocation, the value is calculated with\n+   taking all hard-registers including fixed ones into account.  In\n+   other words, the value is the corresponding reg_class_subunion\n+   value.  */\n+extern enum reg_class ira_reg_class_union[N_REG_CLASSES][N_REG_CLASSES];\n+\n+extern void *ira_allocate (size_t);\n+extern void *ira_reallocate (void *, size_t);\n+extern void ira_free (void *addr);\n+extern bitmap ira_allocate_bitmap (void);\n+extern void ira_free_bitmap (bitmap);\n+extern void ira_print_disposition (FILE *);\n+extern void ira_debug_disposition (void);\n+extern void ira_debug_class_cover (void);\n+extern void ira_init_register_move_cost (enum machine_mode);\n+\n+/* The length of the two following arrays.  */\n+extern int ira_reg_equiv_len;\n+\n+/* The element value is TRUE if the corresponding regno value is\n+   invariant.  */\n+extern bool *ira_reg_equiv_invariant_p;\n+\n+/* The element value is equiv constant of given pseudo-register or\n+   NULL_RTX.  */\n+extern rtx *ira_reg_equiv_const;\n+\n+/* ira-build.c */\n+\n+/* The current loop tree node and its regno allocno map.  */\n+extern ira_loop_tree_node_t ira_curr_loop_tree_node;\n+extern ira_allocno_t *ira_curr_regno_allocno_map;\n+\n+extern void ira_debug_allocno_copies (ira_allocno_t);\n+\n+extern void ira_traverse_loop_tree (bool, ira_loop_tree_node_t,\n+\t\t\t\t    void (*) (ira_loop_tree_node_t),\n+\t\t\t\t    void (*) (ira_loop_tree_node_t));\n+extern ira_allocno_t ira_create_allocno (int, bool, ira_loop_tree_node_t);\n+extern void ira_set_allocno_cover_class (ira_allocno_t, enum reg_class);\n+extern bool ira_conflict_vector_profitable_p (ira_allocno_t, int);\n+extern void ira_allocate_allocno_conflict_vec (ira_allocno_t, int);\n+extern void ira_allocate_allocno_conflicts (ira_allocno_t, int);\n+extern void ira_add_allocno_conflict (ira_allocno_t, ira_allocno_t);\n+extern void ira_print_expanded_allocno (ira_allocno_t);\n+extern allocno_live_range_t ira_create_allocno_live_range\n+\t                    (ira_allocno_t, int, int, allocno_live_range_t);\n+extern void ira_finish_allocno_live_range (allocno_live_range_t);\n+extern void ira_free_allocno_updated_costs (ira_allocno_t);\n+extern ira_copy_t ira_create_copy (ira_allocno_t, ira_allocno_t,\n+\t\t\t\t   int, rtx, ira_loop_tree_node_t);\n+extern void ira_add_allocno_copy_to_list (ira_copy_t);\n+extern void ira_swap_allocno_copy_ends_if_necessary (ira_copy_t);\n+extern void ira_remove_allocno_copy_from_list (ira_copy_t);\n+extern ira_copy_t ira_add_allocno_copy (ira_allocno_t, ira_allocno_t, int, rtx,\n+\t\t\t\t\tira_loop_tree_node_t);\n+\n+extern int *ira_allocate_cost_vector (enum reg_class);\n+extern void ira_free_cost_vector (int *, enum reg_class);\n+\n+extern void ira_flattening (int, int);\n+extern bool ira_build (bool);\n+extern void ira_destroy (void);\n+\n+/* ira-costs.c */\n+extern void ira_init_costs_once (void);\n+extern void ira_init_costs (void);\n+extern void ira_finish_costs_once (void);\n+extern void ira_costs (void);\n+extern void ira_tune_allocno_costs_and_cover_classes (void);\n+\n+/* ira-lives.c */\n+\n+extern void ira_rebuild_start_finish_chains (void);\n+extern void ira_print_live_range_list (FILE *, allocno_live_range_t);\n+extern void ira_debug_live_range_list (allocno_live_range_t);\n+extern void ira_debug_allocno_live_ranges (ira_allocno_t);\n+extern void ira_debug_live_ranges (void);\n+extern void ira_create_allocno_live_ranges (void);\n+extern void ira_finish_allocno_live_ranges (void);\n+\n+/* ira-conflicts.c */\n+extern bool ira_allocno_live_ranges_intersect_p (ira_allocno_t, ira_allocno_t);\n+extern bool ira_pseudo_live_ranges_intersect_p (int, int);\n+extern void ira_debug_conflicts (bool);\n+extern void ira_build_conflicts (void);\n+\n+/* ira-color.c */\n+extern int ira_loop_edge_freq (ira_loop_tree_node_t, int, bool);\n+extern void ira_reassign_conflict_allocnos (int);\n+extern void ira_initiate_assign (void);\n+extern void ira_finish_assign (void);\n+extern void ira_color (void);\n+extern void ira_fast_allocation (void);\n+\n+/* ira-emit.c */\n+extern void ira_emit (bool);\n+\n+\f\n+\n+/* The iterator for all allocnos.  */\n+typedef struct {\n+  /* The number of the current element in IRA_ALLOCNOS.  */\n+  int n;\n+} ira_allocno_iterator;\n+\n+/* Initialize the iterator I.  */\n+static inline void\n+ira_allocno_iter_init (ira_allocno_iterator *i)\n+{\n+  i->n = 0;\n+}\n+\n+/* Return TRUE if we have more allocnos to visit, in which case *A is\n+   set to the allocno to be visited.  Otherwise, return FALSE.  */\n+static inline bool\n+ira_allocno_iter_cond (ira_allocno_iterator *i, ira_allocno_t *a)\n+{\n+  int n;\n+\n+  for (n = i->n; n < ira_allocnos_num; n++)\n+    if (ira_allocnos[n] != NULL)\n+      {\n+\t*a = ira_allocnos[n];\n+\ti->n = n + 1;\n+\treturn true;\n+      }\n+  return false;\n+}\n+\n+/* Loop over all allocnos.  In each iteration, A is set to the next\n+   allocno.  ITER is an instance of ira_allocno_iterator used to iterate\n+   the allocnos.  */\n+#define FOR_EACH_ALLOCNO(A, ITER)\t\t\t\\\n+  for (ira_allocno_iter_init (&(ITER));\t\t\t\\\n+       ira_allocno_iter_cond (&(ITER), &(A));)\n+\n+\n+\f\n+\n+/* The iterator for copies.  */\n+typedef struct {\n+  /* The number of the current element in IRA_COPIES.  */\n+  int n;\n+} ira_copy_iterator;\n+\n+/* Initialize the iterator I.  */\n+static inline void\n+ira_copy_iter_init (ira_copy_iterator *i)\n+{\n+  i->n = 0;\n+}\n+\n+/* Return TRUE if we have more copies to visit, in which case *CP is\n+   set to the copy to be visited.  Otherwise, return FALSE.  */\n+static inline bool\n+ira_copy_iter_cond (ira_copy_iterator *i, ira_copy_t *cp)\n+{\n+  int n;\n+\n+  for (n = i->n; n < ira_copies_num; n++)\n+    if (ira_copies[n] != NULL)\n+      {\n+\t*cp = ira_copies[n];\n+\ti->n = n + 1;\n+\treturn true;\n+      }\n+  return false;\n+}\n+\n+/* Loop over all copies.  In each iteration, C is set to the next\n+   copy.  ITER is an instance of ira_copy_iterator used to iterate\n+   the copies.  */\n+#define FOR_EACH_COPY(C, ITER)\t\t\t\t\\\n+  for (ira_copy_iter_init (&(ITER));\t\t\t\\\n+       ira_copy_iter_cond (&(ITER), &(C));)\n+\n+\n+\f\n+\n+/* The iterator for allocno conflicts.  */\n+typedef struct {\n+\n+  /* TRUE if the conflicts are represented by vector of allocnos.  */\n+  bool allocno_conflict_vec_p;\n+\n+  /* The conflict vector or conflict bit vector.  */\n+  void *vec;\n+\n+  /* The number of the current element in the vector (of type\n+     ira_allocno_t or IRA_INT_TYPE).  */\n+  unsigned int word_num;\n+\n+  /* The bit vector size.  It is defined only if\n+     ALLOCNO_CONFLICT_VEC_P is FALSE.  */\n+  unsigned int size;\n+\n+  /* The current bit index of bit vector.  It is defined only if\n+     ALLOCNO_CONFLICT_VEC_P is FALSE.  */\n+  unsigned int bit_num;\n+\n+  /* Allocno conflict id corresponding to the 1st bit of the bit\n+     vector.  It is defined only if ALLOCNO_CONFLICT_VEC_P is\n+     FALSE.  */\n+  int base_conflict_id;\n+\n+  /* The word of bit vector currently visited.  It is defined only if\n+     ALLOCNO_CONFLICT_VEC_P is FALSE.  */\n+  unsigned IRA_INT_TYPE word;\n+} ira_allocno_conflict_iterator;\n+\n+/* Initialize the iterator I with ALLOCNO conflicts.  */\n+static inline void\n+ira_allocno_conflict_iter_init (ira_allocno_conflict_iterator *i,\n+\t\t\t\tira_allocno_t allocno)\n+{\n+  i->allocno_conflict_vec_p = ALLOCNO_CONFLICT_VEC_P (allocno);\n+  i->vec = ALLOCNO_CONFLICT_ALLOCNO_ARRAY (allocno);\n+  i->word_num = 0;\n+  if (i->allocno_conflict_vec_p)\n+    i->size = i->bit_num = i->base_conflict_id = i->word = 0;\n+  else\n+    {\n+      if (ALLOCNO_MIN (allocno) > ALLOCNO_MAX (allocno))\n+\ti->size = 0;\n+      else\n+\ti->size = ((ALLOCNO_MAX (allocno) - ALLOCNO_MIN (allocno)\n+\t\t    + IRA_INT_BITS)\n+\t\t   / IRA_INT_BITS) * sizeof (IRA_INT_TYPE);\n+      i->bit_num = 0;\n+      i->base_conflict_id = ALLOCNO_MIN (allocno);\n+      i->word = (i->size == 0 ? 0 : ((IRA_INT_TYPE *) i->vec)[0]);\n+    }\n+}\n+\n+/* Return TRUE if we have more conflicting allocnos to visit, in which\n+   case *A is set to the allocno to be visited.  Otherwise, return\n+   FALSE.  */\n+static inline bool\n+ira_allocno_conflict_iter_cond (ira_allocno_conflict_iterator *i,\n+\t\t\t\tira_allocno_t *a)\n+{\n+  ira_allocno_t conflict_allocno;\n+\n+  if (i->allocno_conflict_vec_p)\n+    {\n+      conflict_allocno = ((ira_allocno_t *) i->vec)[i->word_num];\n+      if (conflict_allocno == NULL)\n+\treturn false;\n+      *a = conflict_allocno;\n+      return true;\n+    }\n+  else\n+    {\n+      /* Skip words that are zeros.  */\n+      for (; i->word == 0; i->word = ((IRA_INT_TYPE *) i->vec)[i->word_num])\n+\t{\n+\t  i->word_num++;\n+\t  \n+\t  /* If we have reached the end, break.  */\n+\t  if (i->word_num * sizeof (IRA_INT_TYPE) >= i->size)\n+\t    return false;\n+\t  \n+\t  i->bit_num = i->word_num * IRA_INT_BITS;\n+\t}\n+      \n+      /* Skip bits that are zero.  */\n+      for (; (i->word & 1) == 0; i->word >>= 1)\n+\ti->bit_num++;\n+      \n+      *a = ira_conflict_id_allocno_map[i->bit_num + i->base_conflict_id];\n+      \n+      return true;\n+    }\n+}\n+\n+/* Advance to the next conflicting allocno.  */\n+static inline void\n+ira_allocno_conflict_iter_next (ira_allocno_conflict_iterator *i)\n+{\n+  if (i->allocno_conflict_vec_p)\n+    i->word_num++;\n+  else\n+    {\n+      i->word >>= 1;\n+      i->bit_num++;\n+    }\n+}\n+\n+/* Loop over all allocnos conflicting with ALLOCNO.  In each\n+   iteration, A is set to the next conflicting allocno.  ITER is an\n+   instance of ira_allocno_conflict_iterator used to iterate the\n+   conflicts.  */\n+#define FOR_EACH_ALLOCNO_CONFLICT(ALLOCNO, A, ITER)\t\t\t\\\n+  for (ira_allocno_conflict_iter_init (&(ITER), (ALLOCNO));\t\t\\\n+       ira_allocno_conflict_iter_cond (&(ITER), &(A));\t\t\t\\\n+       ira_allocno_conflict_iter_next (&(ITER)))\n+\n+\f\n+\n+/* The function returns TRUE if hard registers starting with\n+   HARD_REGNO and containing value of MODE are not in set\n+   HARD_REGSET.  */\n+static inline bool\n+ira_hard_reg_not_in_set_p (int hard_regno, enum machine_mode mode,\n+\t\t\t   HARD_REG_SET hard_regset)\n+{\n+  int i;\n+\n+  ira_assert (hard_regno >= 0);\n+  for (i = hard_regno_nregs[hard_regno][mode] - 1; i >= 0; i--)\n+    if (TEST_HARD_REG_BIT (hard_regset, hard_regno + i))\n+      return false;\n+  return true;\n+}\n+\n+\f\n+\n+/* To save memory we use a lazy approach for allocation and\n+   initialization of the cost vectors.  We do this only when it is\n+   really necessary.  */\n+\n+/* Allocate cost vector *VEC for hard registers of COVER_CLASS and\n+   initialize the elements by VAL if it is necessary */\n+static inline void\n+ira_allocate_and_set_costs (int **vec, enum reg_class cover_class, int val)\n+{\n+  int i, *reg_costs;\n+  int len;\n+\n+  if (*vec != NULL)\n+    return;\n+  *vec = reg_costs = ira_allocate_cost_vector (cover_class);\n+  len = ira_class_hard_regs_num[cover_class];\n+  for (i = 0; i < len; i++)\n+    reg_costs[i] = val;\n+}\n+\n+/* Allocate cost vector *VEC for hard registers of COVER_CLASS and\n+   copy values of vector SRC into the vector if it is necessary */\n+static inline void\n+ira_allocate_and_copy_costs (int **vec, enum reg_class cover_class, int *src)\n+{\n+  int len;\n+\n+  if (*vec != NULL || src == NULL)\n+    return;\n+  *vec = ira_allocate_cost_vector (cover_class);\n+  len = ira_class_hard_regs_num[cover_class];\n+  memcpy (*vec, src, sizeof (int) * len);\n+}\n+\n+/* Allocate cost vector *VEC for hard registers of COVER_CLASS and\n+   add values of vector SRC into the vector if it is necessary */\n+static inline void\n+ira_allocate_and_accumulate_costs (int **vec, enum reg_class cover_class,\n+\t\t\t\t   int *src)\n+{\n+  int i, len;\n+\n+  if (src == NULL)\n+    return;\n+  len = ira_class_hard_regs_num[cover_class];\n+  if (*vec == NULL)\n+    {\n+      *vec = ira_allocate_cost_vector (cover_class);\n+      memset (*vec, 0, sizeof (int) * len);\n+    }\n+  for (i = 0; i < len; i++)\n+    (*vec)[i] += src[i];\n+}\n+\n+/* Allocate cost vector *VEC for hard registers of COVER_CLASS and\n+   copy values of vector SRC into the vector or initialize it by VAL\n+   (if SRC is null).  */\n+static inline void\n+ira_allocate_and_set_or_copy_costs (int **vec, enum reg_class cover_class,\n+\t\t\t\t    int val, int *src)\n+{\n+  int i, *reg_costs;\n+  int len;\n+\n+  if (*vec != NULL)\n+    return;\n+  *vec = reg_costs = ira_allocate_cost_vector (cover_class);\n+  len = ira_class_hard_regs_num[cover_class];\n+  if (src != NULL)\n+    memcpy (reg_costs, src, sizeof (int) * len);\n+  else\n+    {\n+      for (i = 0; i < len; i++)\n+\treg_costs[i] = val;\n+    }\n+}"}, {"sha": "7d6b29fedf10b9381c69749fe7402cfadce0b591", "filename": "gcc/ira-lives.c", "status": "added", "additions": 967, "deletions": 0, "changes": 967, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-lives.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira-lives.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira-lives.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -0,0 +1,967 @@\n+/* IRA processing allocno lives to build allocno live ranges.\n+   Copyright (C) 2006, 2007, 2008\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"regs.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"target.h\"\n+#include \"flags.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"toplev.h\"\n+#include \"params.h\"\n+#include \"df.h\"\n+#include \"sparseset.h\"\n+#include \"ira-int.h\"\n+\n+/* The code in this file is similar to one in global but the code\n+   works on the allocno basis and creates live ranges instead of\n+   pseudo-register conflicts.  */\n+\n+/* Program points are enumerated by numbers from range\n+   0..IRA_MAX_POINT-1.  There are approximately two times more program\n+   points than insns.  Program points are places in the program where\n+   liveness info can be changed.  In most general case (there are more\n+   complicated cases too) some program points correspond to places\n+   where input operand dies and other ones correspond to places where\n+   output operands are born.  */\n+int ira_max_point;\n+\n+/* Arrays of size IRA_MAX_POINT mapping a program point to the allocno\n+   live ranges with given start/finish point.  */\n+allocno_live_range_t *ira_start_point_ranges, *ira_finish_point_ranges;\n+\n+/* Number of the current program point.  */\n+static int curr_point;\n+\n+/* Point where register pressure excess started or -1 if there is no\n+   register pressure excess.  Excess pressure for a register class at\n+   some point means that there are more allocnos of given register\n+   class living at the point than number of hard-registers of the\n+   class available for the allocation.  It is defined only for cover\n+   classes.  */\n+static int high_pressure_start_point[N_REG_CLASSES];\n+\n+/* Allocnos live at current point in the scan.  */\n+static sparseset allocnos_live;\n+\n+/* Set of hard regs (except eliminable ones) currently live.  */\n+static HARD_REG_SET hard_regs_live;\n+\n+/* The loop tree node corresponding to the current basic block.  */\n+static ira_loop_tree_node_t curr_bb_node;\n+\n+/* The function processing birth of register REGNO.  It updates living\n+   hard regs and conflict hard regs for living allocnos or starts a\n+   new live range for the allocno corresponding to REGNO if it is\n+   necessary.  */\n+static void\n+make_regno_born (int regno)\n+{\n+  unsigned int i;\n+  ira_allocno_t a;\n+  allocno_live_range_t p;\n+\n+  if (regno < FIRST_PSEUDO_REGISTER)\n+    {\n+      SET_HARD_REG_BIT (hard_regs_live, regno);\n+      EXECUTE_IF_SET_IN_SPARSESET (allocnos_live, i)\n+        {\n+\t  SET_HARD_REG_BIT (ALLOCNO_CONFLICT_HARD_REGS (ira_allocnos[i]),\n+\t\t\t    regno);\n+\t  SET_HARD_REG_BIT (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (ira_allocnos[i]),\n+\t\t\t    regno);\n+\t}\n+      return;\n+    }\n+  a = ira_curr_regno_allocno_map[regno];\n+  if (a == NULL)\n+    return;\n+  if ((p = ALLOCNO_LIVE_RANGES (a)) == NULL\n+      || (p->finish != curr_point && p->finish + 1 != curr_point))\n+    ALLOCNO_LIVE_RANGES (a)\n+      = ira_create_allocno_live_range (a, curr_point, -1,\n+\t\t\t\t       ALLOCNO_LIVE_RANGES (a));\n+}\n+\n+/* Update ALLOCNO_EXCESS_PRESSURE_POINTS_NUM for allocno A.  */\n+static void\n+update_allocno_pressure_excess_length (ira_allocno_t a)\n+{\n+  int start;\n+  enum reg_class cover_class;\n+  allocno_live_range_t p;\n+\n+  cover_class = ALLOCNO_COVER_CLASS (a);\n+  if (high_pressure_start_point[cover_class] < 0)\n+    return;\n+  p = ALLOCNO_LIVE_RANGES (a);\n+  ira_assert (p != NULL);\n+  start = (high_pressure_start_point[cover_class] > p->start\n+\t   ? high_pressure_start_point[cover_class] : p->start);\n+  ALLOCNO_EXCESS_PRESSURE_POINTS_NUM (a) += curr_point - start + 1;\n+}\n+\n+/* Process the death of register REGNO.  This updates hard_regs_live\n+   or finishes the current live range for the allocno corresponding to\n+   REGNO.  */\n+static void\n+make_regno_dead (int regno)\n+{\n+  ira_allocno_t a;\n+  allocno_live_range_t p;\n+\n+  if (regno < FIRST_PSEUDO_REGISTER)\n+    {\n+      CLEAR_HARD_REG_BIT (hard_regs_live, regno);\n+      return;\n+    }\n+  a = ira_curr_regno_allocno_map[regno];\n+  if (a == NULL)\n+    return;\n+  p = ALLOCNO_LIVE_RANGES (a);\n+  ira_assert (p != NULL);\n+  p->finish = curr_point;\n+  update_allocno_pressure_excess_length (a);\n+}\n+\n+/* Process the birth and, right after then, death of register\n+   REGNO.  */\n+static void\n+make_regno_born_and_dead (int regno)\n+{\n+  make_regno_born (regno);\n+  make_regno_dead (regno);\n+}\n+\n+/* The current register pressures for each cover class for the current\n+   basic block.  */\n+static int curr_reg_pressure[N_REG_CLASSES];\n+\n+/* Mark allocno A as currently living and update current register\n+   pressure, maximal register pressure for the current BB, start point\n+   of the register pressure excess, and conflicting hard registers of\n+   A.  */\n+static void\n+set_allocno_live (ira_allocno_t a)\n+{\n+  int nregs;\n+  enum reg_class cover_class;\n+\n+  if (sparseset_bit_p (allocnos_live, ALLOCNO_NUM (a)))\n+    return;\n+  sparseset_set_bit (allocnos_live, ALLOCNO_NUM (a));\n+  IOR_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (a), hard_regs_live);\n+  IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a), hard_regs_live);\n+  cover_class = ALLOCNO_COVER_CLASS (a);\n+  nregs = ira_reg_class_nregs[cover_class][ALLOCNO_MODE (a)];\n+  curr_reg_pressure[cover_class] += nregs;\n+  if (high_pressure_start_point[cover_class] < 0\n+      && (curr_reg_pressure[cover_class]\n+\t  > ira_available_class_regs[cover_class]))\n+    high_pressure_start_point[cover_class] = curr_point;\n+  if (curr_bb_node->reg_pressure[cover_class]\n+      < curr_reg_pressure[cover_class])\n+    curr_bb_node->reg_pressure[cover_class] = curr_reg_pressure[cover_class];\n+}\n+\n+/* Mark allocno A as currently not living and update current register\n+   pressure, start point of the register pressure excess, and register\n+   pressure excess length for living allocnos.  */\n+static void\n+clear_allocno_live (ira_allocno_t a)\n+{\n+  unsigned int i;\n+  enum reg_class cover_class;\n+\n+  if (sparseset_bit_p (allocnos_live, ALLOCNO_NUM (a)))\n+    {\n+      cover_class = ALLOCNO_COVER_CLASS (a);\n+      curr_reg_pressure[cover_class]\n+\t-= ira_reg_class_nregs[cover_class][ALLOCNO_MODE (a)];\n+      ira_assert (curr_reg_pressure[cover_class] >= 0);\n+      if (high_pressure_start_point[cover_class] >= 0\n+\t  && (curr_reg_pressure[cover_class]\n+\t      <= ira_available_class_regs[cover_class]))\n+\t{\n+\t  EXECUTE_IF_SET_IN_SPARSESET (allocnos_live, i)\n+\t    {\n+\t      update_allocno_pressure_excess_length (ira_allocnos[i]);\n+\t    }\n+\t  high_pressure_start_point[cover_class] = -1;\n+\t}\n+    }\n+  sparseset_clear_bit (allocnos_live, ALLOCNO_NUM (a));\n+}\n+\n+/* Record all regs that are set in any one insn.  Communication from\n+   mark_reg_{store,clobber}.  */\n+static VEC(rtx, heap) *regs_set;\n+\n+/* Handle the case where REG is set by the insn being scanned, during\n+   the scan to build live ranges and calculate reg pressure info.\n+   Store a 1 in hard_regs_live or allocnos_live for this register or\n+   the corresponding allocno, record how many consecutive hardware\n+   registers it actually needs.\n+\n+   Note that even if REG does not remain alive after this insn, we\n+   must mark it here as live, to ensure a conflict between REG and any\n+   other reg allocnos set in this insn that really do live.  This is\n+   because those other allocnos could be considered after this.\n+\n+   REG might actually be something other than a register; if so, we do\n+   nothing.\n+\n+   SETTER is 0 if this register was modified by an auto-increment\n+   (i.e., a REG_INC note was found for it).  */\n+static void\n+mark_reg_store (rtx reg, const_rtx setter ATTRIBUTE_UNUSED,\n+\t\tvoid *data ATTRIBUTE_UNUSED)\n+{\n+  int regno;\n+\n+  if (GET_CODE (reg) == SUBREG)\n+    reg = SUBREG_REG (reg);\n+\n+  if (! REG_P (reg))\n+    return;\n+\n+  VEC_safe_push (rtx, heap, regs_set, reg);\n+\n+  regno = REGNO (reg);\n+\n+  if (regno >= FIRST_PSEUDO_REGISTER)\n+    {\n+      ira_allocno_t a = ira_curr_regno_allocno_map[regno];\n+\n+      if (a != NULL)\n+\t{\n+\t  if (sparseset_bit_p (allocnos_live, ALLOCNO_NUM (a)))\n+\t    return;\n+\t  set_allocno_live (a);\n+\t}\n+      make_regno_born (regno);\n+    }\n+  else if (! TEST_HARD_REG_BIT (ira_no_alloc_regs, regno))\n+    {\n+      int last = regno + hard_regno_nregs[regno][GET_MODE (reg)];\n+      enum reg_class cover_class;\n+\n+      while (regno < last)\n+\t{\n+\t  if (! TEST_HARD_REG_BIT (hard_regs_live, regno)\n+\t      && ! TEST_HARD_REG_BIT (eliminable_regset, regno))\n+\t    {\n+\t      cover_class = ira_class_translate[REGNO_REG_CLASS (regno)];\n+\t      if (cover_class != NO_REGS)\n+\t\t{\n+\t\t  curr_reg_pressure[cover_class]++;\n+\t\t  if (high_pressure_start_point[cover_class] < 0\n+\t\t      && (curr_reg_pressure[cover_class]\n+\t\t\t  > ira_available_class_regs[cover_class]))\n+\t\t    high_pressure_start_point[cover_class] = curr_point;\n+\t\t}\n+\t      make_regno_born (regno);\n+\t      if (cover_class != NO_REGS\n+\t\t  && (curr_bb_node->reg_pressure[cover_class]\n+\t\t      < curr_reg_pressure[cover_class]))\n+\t\tcurr_bb_node->reg_pressure[cover_class]\n+\t\t  = curr_reg_pressure[cover_class];\n+\t    }\n+\t  regno++;\n+\t}\n+    }\n+}\n+\n+/* Like mark_reg_store except notice just CLOBBERs; ignore SETs.  */\n+static void\n+mark_reg_clobber (rtx reg, const_rtx setter, void *data)\n+{\n+  if (GET_CODE (setter) == CLOBBER)\n+    mark_reg_store (reg, setter, data);\n+}\n+\n+/* Record that hard register REG (if it is a hard register) has\n+   conflicts with all the allocno currently live or the corresponding\n+   allocno lives at just the current program point.  Do not mark REG\n+   (or the allocno) itself as live.  */\n+static void\n+mark_reg_conflicts (rtx reg)\n+{\n+  int regno;\n+\n+  if (GET_CODE (reg) == SUBREG)\n+    reg = SUBREG_REG (reg);\n+\n+  if (! REG_P (reg))\n+    return;\n+\n+  regno = REGNO (reg);\n+\n+  if (regno >= FIRST_PSEUDO_REGISTER)\n+    make_regno_born_and_dead (regno);\n+  else if (! TEST_HARD_REG_BIT (ira_no_alloc_regs, regno))\n+    {\n+      int last = regno + hard_regno_nregs[regno][GET_MODE (reg)];\n+\n+      while (regno < last)\n+\t{\n+\t  make_regno_born_and_dead (regno);\n+\t  regno++;\n+\t}\n+    }\n+}\n+\n+/* Mark REG (or the corresponding allocno) as being dead (following\n+   the insn being scanned now).  Store a 0 in hard_regs_live or\n+   allocnos_live for the register.  */\n+static void\n+mark_reg_death (rtx reg)\n+{\n+  unsigned int i;\n+  int regno = REGNO (reg);\n+\n+  if (regno >= FIRST_PSEUDO_REGISTER)\n+    {\n+      ira_allocno_t a = ira_curr_regno_allocno_map[regno];\n+\n+      if (a != NULL)\n+\t{\n+\t  if (! sparseset_bit_p (allocnos_live, ALLOCNO_NUM (a)))\n+\t    return;\n+\t  clear_allocno_live (a);\n+\t}\n+      make_regno_dead (regno);\n+    }\n+  else if (! TEST_HARD_REG_BIT (ira_no_alloc_regs, regno))\n+    {\n+      int last = regno + hard_regno_nregs[regno][GET_MODE (reg)];\n+      enum reg_class cover_class;\n+\n+      while (regno < last)\n+\t{\n+\t  if (TEST_HARD_REG_BIT (hard_regs_live, regno))\n+\t    {\n+\t      cover_class = ira_class_translate[REGNO_REG_CLASS (regno)];\n+\t      if (cover_class != NO_REGS)\n+\t\t{\n+\t\t  curr_reg_pressure[cover_class]--;\n+\t\t  if (high_pressure_start_point[cover_class] >= 0\n+\t\t      && (curr_reg_pressure[cover_class]\n+\t\t\t  <= ira_available_class_regs[cover_class]))\n+\t\t    {\n+\t\t      EXECUTE_IF_SET_IN_SPARSESET (allocnos_live, i)\n+\t\t\t{\n+\t\t\t  update_allocno_pressure_excess_length\n+\t\t\t    (ira_allocnos[i]);\n+\t\t\t}\n+\t\t      high_pressure_start_point[cover_class] = -1;\n+\t\t    }\n+\t\t  ira_assert (curr_reg_pressure[cover_class] >= 0);\n+\t\t}\n+\t      make_regno_dead (regno);\n+\t    }\n+\t  regno++;\n+\t}\n+    }\n+}\n+\n+/* Checks that CONSTRAINTS permits to use only one hard register.  If\n+   it is so, the function returns the class of the hard register.\n+   Otherwise it returns NO_REGS.  */\n+static enum reg_class\n+single_reg_class (const char *constraints, rtx op, rtx equiv_const)\n+{\n+  int ignore_p;\n+  enum reg_class cl, next_cl;\n+  int c;\n+\n+  cl = NO_REGS;\n+  for (ignore_p = false;\n+       (c = *constraints);\n+       constraints += CONSTRAINT_LEN (c, constraints))\n+    if (c == '#')\n+      ignore_p = true;\n+    else if (c == ',')\n+      ignore_p = false;\n+    else if (! ignore_p)\n+      switch (c)\n+\t{\n+\tcase ' ':\n+\tcase '\\t':\n+\tcase '=':\n+\tcase '+':\n+\tcase '*':\n+\tcase '&':\n+\tcase '%':\n+\tcase '!':\n+\tcase '?':\n+\t  break;\n+\tcase 'i':\n+\t  if (CONSTANT_P (op)\n+\t      || (equiv_const != NULL_RTX && CONSTANT_P (equiv_const)))\n+\t    return NO_REGS;\n+\t  break;\n+\n+\tcase 'n':\n+\t  if (GET_CODE (op) == CONST_INT\n+\t      || (GET_CODE (op) == CONST_DOUBLE && GET_MODE (op) == VOIDmode)\n+\t      || (equiv_const != NULL_RTX\n+\t\t  && (GET_CODE (equiv_const) == CONST_INT\n+\t\t      || (GET_CODE (equiv_const) == CONST_DOUBLE\n+\t\t\t  && GET_MODE (equiv_const) == VOIDmode))))\n+\t    return NO_REGS;\n+\t  break;\n+\t  \n+\tcase 's':\n+\t  if ((CONSTANT_P (op) && GET_CODE (op) != CONST_INT\n+\t       && (GET_CODE (op) != CONST_DOUBLE || GET_MODE (op) != VOIDmode))\n+\t      || (equiv_const != NULL_RTX\n+\t\t  && CONSTANT_P (equiv_const)\n+\t\t  && GET_CODE (equiv_const) != CONST_INT\n+\t\t  && (GET_CODE (equiv_const) != CONST_DOUBLE\n+\t\t      || GET_MODE (equiv_const) != VOIDmode)))\n+\t    return NO_REGS;\n+\t  break;\n+\t  \n+\tcase 'I':\n+\tcase 'J':\n+\tcase 'K':\n+\tcase 'L':\n+\tcase 'M':\n+\tcase 'N':\n+\tcase 'O':\n+\tcase 'P':\n+\t  if ((GET_CODE (op) == CONST_INT\n+\t       && CONST_OK_FOR_CONSTRAINT_P (INTVAL (op), c, constraints))\n+\t      || (equiv_const != NULL_RTX\n+\t\t  && GET_CODE (equiv_const) == CONST_INT\n+\t\t  && CONST_OK_FOR_CONSTRAINT_P (INTVAL (equiv_const),\n+\t\t\t\t\t\tc, constraints)))\n+\t    return NO_REGS;\n+\t  break;\n+\t  \n+\tcase 'E':\n+\tcase 'F':\n+\t  if (GET_CODE (op) == CONST_DOUBLE\n+\t      || (GET_CODE (op) == CONST_VECTOR\n+\t\t  && GET_MODE_CLASS (GET_MODE (op)) == MODE_VECTOR_FLOAT)\n+\t      || (equiv_const != NULL_RTX\n+\t\t  && (GET_CODE (equiv_const) == CONST_DOUBLE\n+\t\t      || (GET_CODE (equiv_const) == CONST_VECTOR\n+\t\t\t  && (GET_MODE_CLASS (GET_MODE (equiv_const))\n+\t\t\t      == MODE_VECTOR_FLOAT)))))\n+\t    return NO_REGS;\n+\t  break;\n+\t  \n+\tcase 'G':\n+\tcase 'H':\n+\t  if ((GET_CODE (op) == CONST_DOUBLE\n+\t       && CONST_DOUBLE_OK_FOR_CONSTRAINT_P (op, c, constraints))\n+\t      || (equiv_const != NULL_RTX\n+\t\t  && GET_CODE (equiv_const) == CONST_DOUBLE\n+\t\t  && CONST_DOUBLE_OK_FOR_CONSTRAINT_P (equiv_const,\n+\t\t\t\t\t\t       c, constraints)))\n+\t    return NO_REGS;\n+\t  /* ??? what about memory */\n+\tcase 'r':\n+\tcase 'a': case 'b': case 'c': case 'd': case 'e': case 'f':\n+\tcase 'h': case 'j': case 'k': case 'l':\n+\tcase 'q': case 't': case 'u':\n+\tcase 'v': case 'w': case 'x': case 'y': case 'z':\n+\tcase 'A': case 'B': case 'C': case 'D':\n+\tcase 'Q': case 'R': case 'S': case 'T': case 'U':\n+\tcase 'W': case 'Y': case 'Z':\n+\t  next_cl = (c == 'r'\n+\t\t     ? GENERAL_REGS\n+\t\t     : REG_CLASS_FROM_CONSTRAINT (c, constraints));\n+\t  if ((cl != NO_REGS && next_cl != cl)\n+\t      || ira_available_class_regs[next_cl] > 1)\n+\t    return NO_REGS;\n+\t  cl = next_cl;\n+\t  break;\n+\t  \n+\tcase '0': case '1': case '2': case '3': case '4':\n+\tcase '5': case '6': case '7': case '8': case '9':\n+\t  next_cl\n+\t    = single_reg_class (recog_data.constraints[c - '0'],\n+\t\t\t\trecog_data.operand[c - '0'], NULL_RTX);\n+\t  if ((cl != NO_REGS && next_cl != cl) || next_cl == NO_REGS\n+\t      || ira_available_class_regs[next_cl] > 1)\n+\t    return NO_REGS;\n+\t  cl = next_cl;\n+\t  break;\n+\t  \n+\tdefault:\n+\t  return NO_REGS;\n+\t}\n+  return cl;\n+}\n+\n+/* The function checks that operand OP_NUM of the current insn can use\n+   only one hard register.  If it is so, the function returns the\n+   class of the hard register.  Otherwise it returns NO_REGS.  */\n+static enum reg_class\n+single_reg_operand_class (int op_num)\n+{\n+  if (op_num < 0 || recog_data.n_alternatives == 0)\n+    return NO_REGS;\n+  return single_reg_class (recog_data.constraints[op_num],\n+\t\t\t   recog_data.operand[op_num], NULL_RTX);\n+}\n+\n+/* Processes input operands, if IN_P, or output operands otherwise of\n+   the current insn with FREQ to find allocno which can use only one\n+   hard register and makes other currently living allocnos conflicting\n+   with the hard register.  */\n+static void\n+process_single_reg_class_operands (bool in_p, int freq)\n+{\n+  int i, regno, cost;\n+  unsigned int px;\n+  enum reg_class cl, cover_class;\n+  rtx operand;\n+  ira_allocno_t operand_a, a;\n+\n+  for (i = 0; i < recog_data.n_operands; i++)\n+    {\n+      operand = recog_data.operand[i];\n+      if (in_p && recog_data.operand_type[i] != OP_IN\n+\t  && recog_data.operand_type[i] != OP_INOUT)\n+\tcontinue;\n+      if (! in_p && recog_data.operand_type[i] != OP_OUT\n+\t  && recog_data.operand_type[i] != OP_INOUT)\n+\tcontinue;\n+      cl = single_reg_operand_class (i);\n+      if (cl == NO_REGS)\n+\tcontinue;\n+\n+      operand_a = NULL;\n+\n+      if (GET_CODE (operand) == SUBREG)\n+\toperand = SUBREG_REG (operand);\n+      \n+      if (REG_P (operand)\n+\t  && (regno = REGNO (operand)) >= FIRST_PSEUDO_REGISTER)\n+\t{\n+\t  enum machine_mode mode;\n+\t  enum reg_class cover_class;\n+\n+\t  operand_a = ira_curr_regno_allocno_map[regno];\n+\t  mode = ALLOCNO_MODE (operand_a);\n+\t  cover_class = ALLOCNO_COVER_CLASS (operand_a);\n+\t  if (ira_class_subset_p[cl][cover_class]\n+\t      && ira_class_hard_regs_num[cl] != 0\n+\t      && (ira_class_hard_reg_index[cover_class]\n+\t\t  [ira_class_hard_regs[cl][0]]) >= 0\n+\t      && reg_class_size[cl] <= (unsigned) CLASS_MAX_NREGS (cl, mode))\n+\t    {\n+\t      /* ??? FREQ */\n+\t      cost = freq * (in_p\n+\t\t\t     ? ira_register_move_cost[mode][cover_class][cl]\n+\t\t\t     : ira_register_move_cost[mode][cl][cover_class]);\n+\t      ira_allocate_and_set_costs\n+\t\t(&ALLOCNO_CONFLICT_HARD_REG_COSTS (operand_a), cover_class, 0);\n+\t      ALLOCNO_CONFLICT_HARD_REG_COSTS (operand_a)\n+\t\t[ira_class_hard_reg_index\n+\t\t [cover_class][ira_class_hard_regs[cl][0]]]\n+\t\t-= cost;\n+\t    }\n+\t}\n+\n+      EXECUTE_IF_SET_IN_SPARSESET (allocnos_live, px)\n+        {\n+\t  a = ira_allocnos[px];\n+\t  cover_class = ALLOCNO_COVER_CLASS (a);\n+\t  if (a != operand_a)\n+\t    {\n+\t      /* We could increase costs of A instead of making it\n+\t\t conflicting with the hard register.  But it works worse\n+\t\t because it will be spilled in reload in anyway.  */\n+\t      IOR_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (a),\n+\t\t\t\treg_class_contents[cl]);\n+\t      IOR_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a),\n+\t\t\t\treg_class_contents[cl]);\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Process insns of the basic block given by its LOOP_TREE_NODE to\n+   update allocno live ranges, allocno hard register conflicts,\n+   intersected calls, and register pressure info for allocnos for the\n+   basic block for and regions containing the basic block.  */\n+static void\n+process_bb_node_lives (ira_loop_tree_node_t loop_tree_node)\n+{\n+  int i;\n+  unsigned int j;\n+  basic_block bb;\n+  rtx insn;\n+  edge e;\n+  edge_iterator ei;\n+  bitmap_iterator bi;\n+  bitmap reg_live_in;\n+  unsigned int px;\n+\n+  bb = loop_tree_node->bb;\n+  if (bb != NULL)\n+    {\n+      for (i = 0; i < ira_reg_class_cover_size; i++)\n+\t{\n+\t  curr_reg_pressure[ira_reg_class_cover[i]] = 0;\n+\t  high_pressure_start_point[ira_reg_class_cover[i]] = -1;\n+\t}\n+      curr_bb_node = loop_tree_node;\n+      reg_live_in = DF_LR_IN (bb);\n+      sparseset_clear (allocnos_live);\n+      REG_SET_TO_HARD_REG_SET (hard_regs_live, reg_live_in);\n+      AND_COMPL_HARD_REG_SET (hard_regs_live, eliminable_regset);\n+      AND_COMPL_HARD_REG_SET (hard_regs_live, ira_no_alloc_regs);\n+      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+\tif (TEST_HARD_REG_BIT (hard_regs_live, i))\n+\t  {\n+\t    enum reg_class cover_class;\n+\t    \n+\t    cover_class = REGNO_REG_CLASS (i);\n+\t    if (cover_class == NO_REGS)\n+\t      continue;\n+\t    cover_class = ira_class_translate[cover_class];\n+\t    curr_reg_pressure[cover_class]++;\n+\t    if (curr_bb_node->reg_pressure[cover_class]\n+\t\t< curr_reg_pressure[cover_class])\n+\t      curr_bb_node->reg_pressure[cover_class]\n+\t\t= curr_reg_pressure[cover_class];\n+\t    ira_assert (curr_reg_pressure[cover_class]\n+\t\t\t<= ira_available_class_regs[cover_class]);\n+\t  }\n+      EXECUTE_IF_SET_IN_BITMAP (reg_live_in, FIRST_PSEUDO_REGISTER, j, bi)\n+\t{\n+\t  ira_allocno_t a = ira_curr_regno_allocno_map[j];\n+\t  \n+\t  if (a == NULL)\n+\t    continue;\n+\t  ira_assert (! sparseset_bit_p (allocnos_live, ALLOCNO_NUM (a)));\n+\t  set_allocno_live (a);\n+\t  make_regno_born (j);\n+\t}\n+      \n+#ifdef EH_RETURN_DATA_REGNO\n+      if (bb_has_eh_pred (bb))\n+\t{\n+\t  for (j = 0; ; ++j)\n+\t    {\n+\t      unsigned int regno = EH_RETURN_DATA_REGNO (j);\n+\t      \n+\t      if (regno == INVALID_REGNUM)\n+\t\tbreak;\n+\t      make_regno_born_and_dead (regno);\n+\t    }\n+\t}\n+#endif\n+      \n+      /* Allocnos can't go in stack regs at the start of a basic block\n+\t that is reached by an abnormal edge. Likewise for call\n+\t clobbered regs, because caller-save, fixup_abnormal_edges and\n+\t possibly the table driven EH machinery are not quite ready to\n+\t handle such allocnos live across such edges.  */\n+      FOR_EACH_EDGE (e, ei, bb->preds)\n+\tif (e->flags & EDGE_ABNORMAL)\n+\t  break;\n+      \n+      if (e != NULL)\n+\t{\n+#ifdef STACK_REGS\n+\t  EXECUTE_IF_SET_IN_SPARSESET (allocnos_live, px)\n+\t    {\n+\t      ALLOCNO_NO_STACK_REG_P (ira_allocnos[px]) = true;\n+\t      ALLOCNO_TOTAL_NO_STACK_REG_P (ira_allocnos[px]) = true;\n+\t    }\n+\t  for (px = FIRST_STACK_REG; px <= LAST_STACK_REG; px++)\n+\t    make_regno_born_and_dead (px);\n+#endif\n+\t  /* No need to record conflicts for call clobbered regs if we\n+\t     have nonlocal labels around, as we don't ever try to\n+\t     allocate such regs in this case.  */\n+\t  if (!cfun->has_nonlocal_label)\n+\t    for (px = 0; px < FIRST_PSEUDO_REGISTER; px++)\n+\t      if (call_used_regs[px])\n+\t\tmake_regno_born_and_dead (px);\n+\t}\n+  \n+      /* Scan the code of this basic block, noting which allocnos and\n+\t hard regs are born or die.  */\n+      FOR_BB_INSNS (bb, insn)\n+\t{\n+\t  rtx link;\n+\t  int freq;\n+\t  \n+\t  if (! INSN_P (insn))\n+\t    continue;\n+\t  \n+\t  freq = REG_FREQ_FROM_BB (BLOCK_FOR_INSN (insn));\n+\t  if (freq == 0)\n+\t    freq = 1;\n+\n+\t  if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file, \"   Insn %u(l%d): point = %d\\n\",\n+\t\t     INSN_UID (insn), loop_tree_node->parent->loop->num,\n+\t\t     curr_point);\n+\n+\t  /* Check regs_set is an empty set.  */\n+\t  gcc_assert (VEC_empty (rtx, regs_set));\n+      \n+\t  /* Mark any allocnos clobbered by INSN as live, so they\n+\t     conflict with the inputs.  */\n+\t  note_stores (PATTERN (insn), mark_reg_clobber, NULL);\n+\t  \n+\t  extract_insn (insn);\n+\t  process_single_reg_class_operands (true, freq);\n+\t  \n+\t  /* Mark any allocnos dead after INSN as dead now.  */\n+\t  for (link = REG_NOTES (insn); link; link = XEXP (link, 1))\n+\t    if (REG_NOTE_KIND (link) == REG_DEAD)\n+\t      mark_reg_death (XEXP (link, 0));\n+\t  \n+\t  curr_point++;\n+\n+\t  if (CALL_P (insn))\n+\t    {\n+\t      EXECUTE_IF_SET_IN_SPARSESET (allocnos_live, i)\n+\t        {\n+\t\t  ira_allocno_t a = ira_allocnos[i];\n+\t\t  \n+\t\t  ALLOCNO_CALL_FREQ (a) += freq;\n+\t\t  ALLOCNO_CALLS_CROSSED_NUM (a)++;\n+\t\t  /* Don't allocate allocnos that cross calls, if this\n+\t\t     function receives a nonlocal goto.  */\n+\t\t  if (cfun->has_nonlocal_label)\n+\t\t    {\n+\t\t      SET_HARD_REG_SET (ALLOCNO_CONFLICT_HARD_REGS (a));\n+\t\t      SET_HARD_REG_SET (ALLOCNO_TOTAL_CONFLICT_HARD_REGS (a));\n+\t\t    }\n+\t\t}\n+\t    }\n+\t  \n+\t  /* Mark any allocnos set in INSN as live.  Clobbers are\n+\t     processed again, so they will conflict with the reg\n+\t     allocnos that are set.  */\n+\t  note_stores (PATTERN (insn), mark_reg_store, NULL);\n+\t  \n+#ifdef AUTO_INC_DEC\n+\t  for (link = REG_NOTES (insn); link; link = XEXP (link, 1))\n+\t    if (REG_NOTE_KIND (link) == REG_INC)\n+\t      mark_reg_store (XEXP (link, 0), NULL_RTX, NULL);\n+#endif\n+\t  \n+\t  /* If INSN has multiple outputs, then any allocno that dies\n+\t     here and is used inside of an output must conflict with\n+\t     the other outputs.\n+\t     \n+\t     It is unsafe to use !single_set here since it will ignore\n+\t     an unused output.  Just because an output is unused does\n+\t     not mean the compiler can assume the side effect will not\n+\t     occur.  Consider if ALLOCNO appears in the address of an\n+\t     output and we reload the output.  If we allocate ALLOCNO\n+\t     to the same hard register as an unused output we could\n+\t     set the hard register before the output reload insn.  */\n+\t  if (GET_CODE (PATTERN (insn)) == PARALLEL && multiple_sets (insn))\n+\t    for (link = REG_NOTES (insn); link; link = XEXP (link, 1))\n+\t      if (REG_NOTE_KIND (link) == REG_DEAD)\n+\t\t{\n+\t\t  int i;\n+\t\t  int used_in_output = 0;\n+\t\t  rtx reg = XEXP (link, 0);\n+\t\t  \n+\t\t  for (i = XVECLEN (PATTERN (insn), 0) - 1; i >= 0; i--)\n+\t\t    {\n+\t\t      rtx set = XVECEXP (PATTERN (insn), 0, i);\n+\t\t      \n+\t\t      if (GET_CODE (set) == SET\n+\t\t\t  && ! REG_P (SET_DEST (set))\n+\t\t\t  && ! rtx_equal_p (reg, SET_DEST (set))\n+\t\t\t  && reg_overlap_mentioned_p (reg, SET_DEST (set)))\n+\t\t\tused_in_output = 1;\n+\t\t    }\n+\t\t  if (used_in_output)\n+\t\t    mark_reg_conflicts (reg);\n+\t\t}\n+\t  \n+\t  process_single_reg_class_operands (false, freq);\n+\t  \n+\t  /* Mark any allocnos set in INSN and then never used.  */\n+\t  while (! VEC_empty (rtx, regs_set))\n+\t    {\n+\t      rtx reg = VEC_pop (rtx, regs_set);\n+\t      rtx note = find_regno_note (insn, REG_UNUSED, REGNO (reg));\n+\n+\t      if (note)\n+\t\tmark_reg_death (XEXP (note, 0));\n+\t    }\n+\t  curr_point++;\n+\t}\n+      EXECUTE_IF_SET_IN_SPARSESET (allocnos_live, i)\n+       {\n+\t make_regno_dead (ALLOCNO_REGNO (ira_allocnos[i]));\n+       }\n+\n+      curr_point++;\n+\n+    }\n+  /* Propagate register pressure to upper loop tree nodes: */\n+  if (loop_tree_node != ira_loop_tree_root)\n+    for (i = 0; i < ira_reg_class_cover_size; i++)\n+      {\n+\tenum reg_class cover_class;\n+\n+\tcover_class = ira_reg_class_cover[i];\n+\tif (loop_tree_node->reg_pressure[cover_class]\n+\t    > loop_tree_node->parent->reg_pressure[cover_class])\n+\t  loop_tree_node->parent->reg_pressure[cover_class]\n+\t    = loop_tree_node->reg_pressure[cover_class];\n+      }\n+}\n+\n+/* Create and set up IRA_START_POINT_RANGES and\n+   IRA_FINISH_POINT_RANGES.  */\n+static void\n+create_start_finish_chains (void)\n+{\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+  allocno_live_range_t r;\n+\n+  ira_start_point_ranges\n+    = (allocno_live_range_t *) ira_allocate (ira_max_point\n+\t\t\t\t\t     * sizeof (allocno_live_range_t));\n+  memset (ira_start_point_ranges, 0,\n+\t  ira_max_point * sizeof (allocno_live_range_t));\n+  ira_finish_point_ranges\n+    = (allocno_live_range_t *) ira_allocate (ira_max_point\n+\t\t\t\t\t     * sizeof (allocno_live_range_t));\n+  memset (ira_finish_point_ranges, 0,\n+\t  ira_max_point * sizeof (allocno_live_range_t));\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      for (r = ALLOCNO_LIVE_RANGES (a); r != NULL; r = r->next)\n+\t{\n+\t  r->start_next = ira_start_point_ranges[r->start];\n+\t  ira_start_point_ranges[r->start] = r;\n+\t  r->finish_next = ira_finish_point_ranges[r->finish];\n+ \t  ira_finish_point_ranges[r->finish] = r;\n+\t}\n+    }\n+}\n+\n+/* Rebuild IRA_START_POINT_RANGES and IRA_FINISH_POINT_RANGES after\n+   new live ranges and program points were added as a result if new\n+   insn generation.  */\n+void\n+ira_rebuild_start_finish_chains (void)\n+{\n+  ira_free (ira_finish_point_ranges);\n+  ira_free (ira_start_point_ranges);\n+  create_start_finish_chains ();\n+}\n+\n+/* Print live ranges R to file F.  */\n+void\n+ira_print_live_range_list (FILE *f, allocno_live_range_t r)\n+{\n+  for (; r != NULL; r = r->next)\n+    fprintf (f, \" [%d..%d]\", r->start, r->finish);\n+  fprintf (f, \"\\n\");\n+}\n+\n+/* Print live ranges R to stderr.  */\n+void\n+ira_debug_live_range_list (allocno_live_range_t r)\n+{\n+  ira_print_live_range_list (stderr, r);\n+}\n+\n+/* Print live ranges of allocno A to file F.  */\n+static void\n+print_allocno_live_ranges (FILE *f, ira_allocno_t a)\n+{\n+  fprintf (f, \" a%d(r%d):\", ALLOCNO_NUM (a), ALLOCNO_REGNO (a));\n+  ira_print_live_range_list (f, ALLOCNO_LIVE_RANGES (a));\n+}\n+\n+/* Print live ranges of allocno A to stderr.  */\n+void\n+ira_debug_allocno_live_ranges (ira_allocno_t a)\n+{\n+  print_allocno_live_ranges (stderr, a);\n+}\n+\n+/* Print live ranges of all allocnos to file F.  */\n+static void\n+print_live_ranges (FILE *f)\n+{\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    print_allocno_live_ranges (f, a);\n+}\n+\n+/* Print live ranges of all allocnos to stderr.  */\n+void\n+ira_debug_live_ranges (void)\n+{\n+  print_live_ranges (stderr);\n+}\n+\n+/* The main entry function creates live ranges, set up\n+   CONFLICT_HARD_REGS and TOTAL_CONFLICT_HARD_REGS for allocnos, and\n+   calculate register pressure info.  */\n+void\n+ira_create_allocno_live_ranges (void)\n+{\n+  allocnos_live = sparseset_alloc (ira_allocnos_num);\n+  /* Make a vector that mark_reg_{store,clobber} will store in.  */\n+  if (!regs_set)\n+    regs_set = VEC_alloc (rtx, heap, 10);\n+  curr_point = 0;\n+  ira_traverse_loop_tree (true, ira_loop_tree_root, NULL,\n+\t\t\t  process_bb_node_lives);\n+  ira_max_point = curr_point;\n+  create_start_finish_chains ();\n+  if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+    print_live_ranges (ira_dump_file);\n+  /* Clean up.  */\n+  sparseset_free (allocnos_live);\n+}\n+\n+/* Free arrays IRA_START_POINT_RANGES and IRA_FINISH_POINT_RANGES.  */\n+void\n+ira_finish_allocno_live_ranges (void)\n+{\n+  ira_free (ira_finish_point_ranges);\n+  ira_free (ira_start_point_ranges);\n+}"}, {"sha": "c98f0a05e6f77b91914816b77bd520585d1941e8", "filename": "gcc/ira.c", "status": "added", "additions": 2064, "deletions": 0, "changes": 2064, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -0,0 +1,2064 @@\n+/* Integrated Register Allocator (IRA) entry point.\n+   Copyright (C) 2006, 2007, 2008\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+/* The integrated register allocator (IRA) is a\n+   regional register allocator performing graph coloring on a top-down\n+   traversal of nested regions.  Graph coloring in a region is based\n+   on Chaitin-Briggs algorithm.  It is called integrated because\n+   register coalescing, register live range splitting, and choosing a\n+   better hard register are done on-the-fly during coloring.  Register\n+   coalescing and choosing a cheaper hard register is done by hard\n+   register preferencing during hard register assigning.  The live\n+   range splitting is a byproduct of the regional register allocation.\n+\n+   Major IRA notions are:\n+\n+     o *Region* is a part of CFG where graph coloring based on\n+       Chaitin-Briggs algorithm is done.  IRA can work on any set of\n+       nested CFG regions forming a tree.  Currently the regions are\n+       the entire function for the root region and natural loops for\n+       the other regions.  Therefore data structure representing a\n+       region is called loop_tree_node.\n+\n+     o *Cover class* is a register class belonging to a set of\n+       non-intersecting register classes containing all of the\n+       hard-registers available for register allocation.  The set of\n+       all cover classes for a target is defined in the corresponding\n+       machine-description file according some criteria.  Such notion\n+       is needed because Chaitin-Briggs algorithm works on\n+       non-intersected register classes.\n+\n+     o *Allocno* represents the live range of a pseudo-register in a\n+       region.  Besides the obvious attributes like the corresponding\n+       pseudo-register number, cover class, conflicting allocnos and\n+       conflicting hard-registers, there are a few allocno attributes\n+       which are important for understanding the allocation algorithm:\n+\n+       - *Live ranges*.  This is a list of ranges of *program\n+         points* where the allocno lives.  Program points represent\n+         places where a pseudo can be born or become dead (there are\n+         approximately two times more program points than the insns)\n+         and they are represented by integers starting with 0.  The\n+         live ranges are used to find conflicts between allocnos of\n+         different cover classes.  They also play very important role\n+         for the transformation of the IRA internal representation of\n+         several regions into a one region representation.  The later is\n+         used during the reload pass work because each allocno\n+         represents all of the corresponding pseudo-registers.\n+\n+       - *Hard-register costs*.  This is a vector of size equal to the\n+         number of available hard-registers of the allocno's cover\n+         class.  The cost of a callee-clobbered hard-register for an\n+         allocno is increased by the cost of save/restore code around\n+         the calls through the given allocno's life.  If the allocno\n+         is a move instruction operand and another operand is a\n+         hard-register of the allocno's cover class, the cost of the\n+         hard-register is decreased by the move cost.\n+\n+         When an allocno is assigned, the hard-register with minimal\n+         full cost is used.  Initially, a hard-register's full cost is\n+         the corresponding value from the hard-register's cost vector.\n+         If the allocno is connected by a *copy* (see below) to\n+         another allocno which has just received a hard-register, the\n+         cost of the hard-register is decreased.  Before choosing a\n+         hard-register for an allocno, the allocno's current costs of\n+         the hard-registers are modified by the conflict hard-register\n+         costs of all of the conflicting allocnos which are not\n+         assigned yet.\n+\n+       - *Conflict hard-register costs*.  This is a vector of the same\n+         size as the hard-register costs vector.  To permit an\n+         unassigned allocno to get a better hard-register, IRA uses\n+         this vector to calculate the final full cost of the\n+         available hard-registers.  Conflict hard-register costs of an\n+         unassigned allocno are also changed with a change of the\n+         hard-register cost of the allocno when a copy involving the\n+         allocno is processed as described above.  This is done to\n+         show other unassigned allocnos that a given allocno prefers\n+         some hard-registers in order to remove the move instruction\n+         corresponding to the copy.\n+\n+     o *Cap*.  If a pseudo-register does not live in a region but\n+       lives in a nested region, IRA creates a special allocno called\n+       a cap in the outer region.  A region cap is also created for a\n+       subregion cap.\n+\n+     o *Copy*.  Allocnos can be connected by copies.  Copies are used\n+       to modify hard-register costs for allocnos during coloring.\n+       Such modifications reflects a preference to use the same\n+       hard-register for the allocnos connected by copies.  Usually\n+       copies are created for move insns (in this case it results in\n+       register coalescing).  But IRA also creates copies for operands\n+       of an insn which should be assigned to the same hard-register\n+       due to constraints in the machine description (it usually\n+       results in removing a move generated in reload to satisfy\n+       the constraints) and copies referring to the allocno which is\n+       the output operand of an instruction and the allocno which is\n+       an input operand dying in the instruction (creation of such\n+       copies results in less register shuffling).  IRA *does not*\n+       create copies between the same register allocnos from different\n+       regions because we use another technique for propagating\n+       hard-register preference on the borders of regions.\n+\n+   Allocnos (including caps) for the upper region in the region tree\n+   *accumulate* information important for coloring from allocnos with\n+   the same pseudo-register from nested regions.  This includes\n+   hard-register and memory costs, conflicts with hard-registers,\n+   allocno conflicts, allocno copies and more.  *Thus, attributes for\n+   allocnos in a region have the same values as if the region had no\n+   subregions*.  It means that attributes for allocnos in the\n+   outermost region corresponding to the function have the same values\n+   as though the allocation used only one region which is the entire\n+   function.  It also means that we can look at IRA work as if the\n+   first IRA did allocation for all function then it improved the\n+   allocation for loops then their subloops and so on.\n+\n+   IRA major passes are:\n+\n+     o Building IRA internal representation which consists of the\n+       following subpasses:\n+\n+       * First, IRA builds regions and creates allocnos (file\n+         ira-build.c) and initializes most of their attributes.\n+\n+       * Then IRA finds a cover class for each allocno and calculates\n+         its initial (non-accumulated) cost of memory and each\n+         hard-register of its cover class (file ira-cost.c).\n+\n+       * IRA creates live ranges of each allocno, calulates register\n+         pressure for each cover class in each region, sets up\n+         conflict hard registers for each allocno and info about calls\n+         the allocno lives through (file ira-lives.c).\n+\n+       * IRA removes low register pressure loops from the regions\n+         mostly to speed IRA up (file ira-build.c).\n+\n+       * IRA propagates accumulated allocno info from lower region\n+         allocnos to corresponding upper region allocnos (file\n+         ira-build.c).\n+\n+       * IRA creates all caps (file ira-build.c).\n+\n+       * Having live-ranges of allocnos and their cover classes, IRA\n+         creates conflicting allocnos of the same cover class for each\n+         allocno.  Conflicting allocnos are stored as a bit vector or\n+         array of pointers to the conflicting allocnos whatever is\n+         more profitable (file ira-conflicts.c).  At this point IRA\n+         creates allocno copies.\n+\n+     o Coloring.  Now IRA has all necessary info to start graph coloring\n+       process.  It is done in each region on top-down traverse of the\n+       region tree (file ira-color.c).  There are following subpasses:\n+        \n+       * Optional aggressive coalescing of allocnos in the region.\n+\n+       * Putting allocnos onto the coloring stack.  IRA uses Briggs\n+         optimistic coloring which is a major improvement over\n+         Chaitin's coloring.  Therefore IRA does not spill allocnos at\n+         this point.  There is some freedom in the order of putting\n+         allocnos on the stack which can affect the final result of\n+         the allocation.  IRA uses some heuristics to improve the order.\n+\n+       * Popping the allocnos from the stack and assigning them hard\n+         registers.  If IRA can not assign a hard register to an\n+         allocno and the allocno is coalesced, IRA undoes the\n+         coalescing and puts the uncoalesced allocnos onto the stack in\n+         the hope that some such allocnos will get a hard register\n+         separately.  If IRA fails to assign hard register or memory\n+         is more profitable for it, IRA spills the allocno.  IRA\n+         assigns the allocno the hard-register with minimal full\n+         allocation cost which reflects the cost of usage of the\n+         hard-register for the allocno and cost of usage of the\n+         hard-register for allocnos conflicting with given allocno.\n+\n+       * After allono assigning in the region, IRA modifies the hard\n+         register and memory costs for the corresponding allocnos in\n+         the subregions to reflect the cost of possible loads, stores,\n+         or moves on the border of the region and its subregions.\n+         When default regional allocation algorithm is used\n+         (-fira-algorithm=mixed), IRA just propagates the assignment\n+         for allocnos if the register pressure in the region for the\n+         corresponding cover class is less than number of available\n+         hard registers for given cover class.\n+\n+     o Spill/restore code moving.  When IRA performs an allocation\n+       by traversing regions in top-down order, it does not know what\n+       happens below in the region tree.  Therefore, sometimes IRA\n+       misses opportunities to perform a better allocation.  A simple\n+       optimization tries to improve allocation in a region having\n+       subregions and containing in another region.  If the\n+       corresponding allocnos in the subregion are spilled, it spills\n+       the region allocno if it is profitable.  The optimization\n+       implements a simple iterative algorithm performing profitable\n+       transformations while they are still possible.  It is fast in\n+       practice, so there is no real need for a better time complexity\n+       algorithm.\n+\n+     o Code change.  After coloring, two allocnos representing the same\n+       pseudo-register outside and inside a region respectively may be\n+       assigned to different locations (hard-registers or memory).  In\n+       this case IRA creates and uses a new pseudo-register inside the\n+       region and adds code to move allocno values on the region's\n+       borders.  This is done during top-down traversal of the regions\n+       (file ira-emit.c).  In some complicated cases IRA can create a\n+       new allocno to move allocno values (e.g. when a swap of values\n+       stored in two hard-registers is needed).  At this stage, the\n+       new allocno is marked as spilled.  IRA still creates the\n+       pseudo-register and the moves on the region borders even when\n+       both allocnos were assigned to the same hard-register.  If the\n+       reload pass spills a pseudo-register for some reason, the\n+       effect will be smaller because another allocno will still be in\n+       the hard-register.  In most cases, this is better then spilling\n+       both allocnos.  If reload does not change the allocation\n+       for the two pseudo-registers, the trivial move will be removed\n+       by post-reload optimizations.  IRA does not generate moves for\n+       allocnos assigned to the same hard register when the default\n+       regional allocation algorithm is used and the register pressure\n+       in the region for the corresponding allocno cover class is less\n+       than number of available hard registers for given cover class.\n+       IRA also does some optimizations to remove redundant stores and\n+       to reduce code duplication on the region borders.\n+\n+     o Flattening internal representation.  After changing code, IRA\n+       transforms its internal representation for several regions into\n+       one region representation (file ira-build.c).  This process is\n+       called IR flattening.  Such process is more complicated than IR\n+       rebuilding would be, but is much faster.\n+\n+     o After IR flattening, IRA tries to assign hard registers to all\n+       spilled allocnos.  This is impelemented by a simple and fast\n+       priority coloring algorithm (see function\n+       ira_reassign_conflict_allocnos::ira-color.c).  Here new allocnos\n+       created during the code change pass can be assigned to hard\n+       registers.\n+\n+     o At the end IRA calls the reload pass.  The reload pass\n+       communicates with IRA through several functions in file\n+       ira-color.c to improve its decisions in\n+\n+       * sharing stack slots for the spilled pseudos based on IRA info\n+         about pseudo-register conflicts.\n+\n+       * reassigning hard-registers to all spilled pseudos at the end\n+         of each reload iteration.\n+\n+       * choosing a better hard-register to spill based on IRA info\n+         about pseudo-register live ranges and the register pressure\n+         in places where the pseudo-register lives.\n+\n+   IRA uses a lot of data representing the target processors.  These\n+   data are initilized in file ira.c.\n+\n+   If function has no loops (or the loops are ignored when\n+   -fira-algorithm=CB is used), we have classic Chaitin-Briggs\n+   coloring (only instead of separate pass of coalescing, we use hard\n+   register preferencing).  In such case, IRA works much faster\n+   because many things are not made (like IR flattening, the\n+   spill/restore optimization, and the code change).\n+\n+   Literature is worth to read for better understanding the code:\n+\n+   o Preston Briggs, Keith D. Cooper, Linda Torczon.  Improvements to\n+     Graph Coloring Register Allocation.\n+\n+   o David Callahan, Brian Koblenz.  Register allocation via\n+     hierarchical graph coloring.\n+\n+   o Keith Cooper, Anshuman Dasgupta, Jason Eckhardt. Revisiting Graph\n+     Coloring Register Allocation: A Study of the Chaitin-Briggs and\n+     Callahan-Koblenz Algorithms.\n+\n+   o Guei-Yuan Lueh, Thomas Gross, and Ali-Reza Adl-Tabatabai. Global\n+     Register Allocation Based on Graph Fusion.\n+\n+   o Vladimir Makarov. The Integrated Register Allocator for GCC.\n+\n+   o Vladimir Makarov.  The top-down register allocator for irregular\n+     register file architectures.\n+\n+*/\n+\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"regs.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"target.h\"\n+#include \"flags.h\"\n+#include \"obstack.h\"\n+#include \"bitmap.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"expr.h\"\n+#include \"recog.h\"\n+#include \"params.h\"\n+#include \"timevar.h\"\n+#include \"tree-pass.h\"\n+#include \"output.h\"\n+#include \"reload.h\"\n+#include \"errors.h\"\n+#include \"integrate.h\"\n+#include \"df.h\"\n+#include \"ggc.h\"\n+#include \"ira-int.h\"\n+\n+\n+/* A modified value of flag `-fira-verbose' used internally.  */\n+int internal_flag_ira_verbose;\n+\n+/* Dump file of the allocator if it is not NULL.  */\n+FILE *ira_dump_file;\n+\n+/* Pools for allocnos, copies, allocno live ranges.  */\n+alloc_pool allocno_pool, copy_pool, allocno_live_range_pool;\n+\n+/* The number of elements in the following array.  */\n+int ira_spilled_reg_stack_slots_num;\n+\n+/* The following array contains info about spilled pseudo-registers\n+   stack slots used in current function so far.  */\n+struct ira_spilled_reg_stack_slot *ira_spilled_reg_stack_slots;\n+\n+/* Correspondingly overall cost of the allocation, cost of the\n+   allocnos assigned to hard-registers, cost of the allocnos assigned\n+   to memory, cost of loads, stores and register move insns generated\n+   for pseudo-register live range splitting (see ira-emit.c).  */\n+int ira_overall_cost;\n+int ira_reg_cost, ira_mem_cost;\n+int ira_load_cost, ira_store_cost, ira_shuffle_cost;\n+int ira_move_loops_num, ira_additional_jumps_num;\n+\n+/* Map: hard regs X modes -> set of hard registers for storing value\n+   of given mode starting with given hard register.  */\n+HARD_REG_SET ira_reg_mode_hard_regset[FIRST_PSEUDO_REGISTER][NUM_MACHINE_MODES];\n+\n+/* The following two variables are array analogs of the macros\n+   MEMORY_MOVE_COST and REGISTER_MOVE_COST.  */\n+short int ira_memory_move_cost[MAX_MACHINE_MODE][N_REG_CLASSES][2];\n+move_table *ira_register_move_cost[MAX_MACHINE_MODE];\n+\n+/* Similar to may_move_in_cost but it is calculated in IRA instead of\n+   regclass.  Another difference is that we take only available hard\n+   registers into account to figure out that one register class is a\n+   subset of the another one.  */\n+move_table *ira_may_move_in_cost[MAX_MACHINE_MODE];\n+\n+/* Similar to may_move_out_cost but it is calculated in IRA instead of\n+   regclass.  Another difference is that we take only available hard\n+   registers into account to figure out that one register class is a\n+   subset of the another one.  */\n+move_table *ira_may_move_out_cost[MAX_MACHINE_MODE];\n+\n+/* Register class subset relation: TRUE if the first class is a subset\n+   of the second one considering only hard registers available for the\n+   allocation.  */\n+int ira_class_subset_p[N_REG_CLASSES][N_REG_CLASSES];\n+\n+/* Temporary hard reg set used for a different calculation.  */\n+static HARD_REG_SET temp_hard_regset;\n+\n+\f\n+\n+/* The function sets up the map IRA_REG_MODE_HARD_REGSET.  */\n+static void\n+setup_reg_mode_hard_regset (void)\n+{\n+  int i, m, hard_regno;\n+\n+  for (m = 0; m < NUM_MACHINE_MODES; m++)\n+    for (hard_regno = 0; hard_regno < FIRST_PSEUDO_REGISTER; hard_regno++)\n+      {\n+\tCLEAR_HARD_REG_SET (ira_reg_mode_hard_regset[hard_regno][m]);\n+\tfor (i = hard_regno_nregs[hard_regno][m] - 1; i >= 0; i--)\n+\t  if (hard_regno + i < FIRST_PSEUDO_REGISTER)\n+\t    SET_HARD_REG_BIT (ira_reg_mode_hard_regset[hard_regno][m],\n+\t\t\t      hard_regno + i);\n+      }\n+}\n+\n+\f\n+\n+/* Hard registers that can not be used for the register allocator for\n+   all functions of the current compilation unit.  */\n+static HARD_REG_SET no_unit_alloc_regs;\n+\n+/* Array of the number of hard registers of given class which are\n+   available for allocation.  The order is defined by the\n+   allocation order.  */\n+short ira_class_hard_regs[N_REG_CLASSES][FIRST_PSEUDO_REGISTER];\n+\n+/* The number of elements of the above array for given register\n+   class.  */\n+int ira_class_hard_regs_num[N_REG_CLASSES];\n+\n+/* Index (in ira_class_hard_regs) for given register class and hard\n+   register (in general case a hard register can belong to several\n+   register classes).  The index is negative for hard registers\n+   unavailable for the allocation. */\n+short ira_class_hard_reg_index[N_REG_CLASSES][FIRST_PSEUDO_REGISTER];\n+\n+/* The function sets up the three arrays declared above.  */\n+static void\n+setup_class_hard_regs (void)\n+{\n+  int cl, i, hard_regno, n;\n+  HARD_REG_SET processed_hard_reg_set;\n+\n+  ira_assert (SHRT_MAX >= FIRST_PSEUDO_REGISTER);\n+  /* We could call ORDER_REGS_FOR_LOCAL_ALLOC here (it is usually\n+     putting hard callee-used hard registers first).  But our\n+     heuristics work better.  */\n+  for (cl = (int) N_REG_CLASSES - 1; cl >= 0; cl--)\n+    {\n+      COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl]);\n+      AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+      CLEAR_HARD_REG_SET (processed_hard_reg_set);\n+      for (n = 0, i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+\t{\n+#ifdef REG_ALLOC_ORDER\n+\t  hard_regno = reg_alloc_order[i];\n+#else\n+\t  hard_regno = i;\n+#endif\t  \n+\t  if (TEST_HARD_REG_BIT (processed_hard_reg_set, hard_regno))\n+\t    continue;\n+\t  SET_HARD_REG_BIT (processed_hard_reg_set, hard_regno);\n+      \t  if (! TEST_HARD_REG_BIT (temp_hard_regset, hard_regno))\n+\t    ira_class_hard_reg_index[cl][hard_regno] = -1;\n+\t  else\n+\t    {\n+\t      ira_class_hard_reg_index[cl][hard_regno] = n;\n+\t      ira_class_hard_regs[cl][n++] = hard_regno;\n+\t    }\n+\t}\n+      ira_class_hard_regs_num[cl] = n;\n+    }\n+}\n+\n+/* Number of given class hard registers available for the register\n+   allocation for given classes.  */\n+int ira_available_class_regs[N_REG_CLASSES];\n+\n+/* Set up IRA_AVAILABLE_CLASS_REGS.  */\n+static void\n+setup_available_class_regs (void)\n+{\n+  int i, j;\n+\n+  memset (ira_available_class_regs, 0, sizeof (ira_available_class_regs));\n+  for (i = 0; i < N_REG_CLASSES; i++)\n+    {\n+      COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[i]);\n+      AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+      for (j = 0; j < FIRST_PSEUDO_REGISTER; j++)\n+\tif (TEST_HARD_REG_BIT (temp_hard_regset, j))\n+\t  ira_available_class_regs[i]++;\n+    }\n+}\n+\n+/* Set up global variables defining info about hard registers for the\n+   allocation.  These depend on USE_HARD_FRAME_P whose TRUE value means\n+   that we can use the hard frame pointer for the allocation.  */\n+static void\n+setup_alloc_regs (bool use_hard_frame_p)\n+{\n+  COPY_HARD_REG_SET (no_unit_alloc_regs, fixed_reg_set);\n+  if (! use_hard_frame_p)\n+    SET_HARD_REG_BIT (no_unit_alloc_regs, HARD_FRAME_POINTER_REGNUM);\n+  setup_class_hard_regs ();\n+  setup_available_class_regs ();\n+}\n+\n+\f\n+\n+/* Set up IRA_MEMORY_MOVE_COST, IRA_REGISTER_MOVE_COST.  */\n+static void\n+setup_class_subset_and_memory_move_costs (void)\n+{\n+  int cl, cl2;\n+  enum machine_mode mode;\n+  HARD_REG_SET temp_hard_regset2;\n+\n+  for (mode = 0; mode < MAX_MACHINE_MODE; mode++)\n+    ira_memory_move_cost[mode][NO_REGS][0]\n+      = ira_memory_move_cost[mode][NO_REGS][1] = SHRT_MAX;\n+  for (cl = (int) N_REG_CLASSES - 1; cl >= 0; cl--)\n+    {\n+      if (cl != (int) NO_REGS)\n+\tfor (mode = 0; mode < MAX_MACHINE_MODE; mode++)\n+\t  {\n+\t    ira_memory_move_cost[mode][cl][0] = MEMORY_MOVE_COST (mode, cl, 0);\n+\t    ira_memory_move_cost[mode][cl][1] = MEMORY_MOVE_COST (mode, cl, 1);\n+\t    /* Costs for NO_REGS are used in cost calculation on the\n+\t       1st pass when the preferred register classes are not\n+\t       known yet.  In this case we take the best scenario.  */\n+\t    if (ira_memory_move_cost[mode][NO_REGS][0]\n+\t\t> ira_memory_move_cost[mode][cl][0])\n+\t      ira_memory_move_cost[mode][NO_REGS][0]\n+\t\t= ira_memory_move_cost[mode][cl][0];\n+\t    if (ira_memory_move_cost[mode][NO_REGS][1]\n+\t\t> ira_memory_move_cost[mode][cl][1])\n+\t      ira_memory_move_cost[mode][NO_REGS][1]\n+\t\t= ira_memory_move_cost[mode][cl][1];\n+\t  }\n+      for (cl2 = (int) N_REG_CLASSES - 1; cl2 >= 0; cl2--)\n+\t{\n+\t  COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl]);\n+\t  AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+\t  COPY_HARD_REG_SET (temp_hard_regset2, reg_class_contents[cl2]);\n+\t  AND_COMPL_HARD_REG_SET (temp_hard_regset2, no_unit_alloc_regs);\n+\t  ira_class_subset_p[cl][cl2]\n+\t    = hard_reg_set_subset_p (temp_hard_regset, temp_hard_regset2);\n+\t}\n+    }\n+}\n+\n+\f\n+\n+/* Define the following macro if allocation through malloc if\n+   preferable.  */\n+#define IRA_NO_OBSTACK\n+\n+#ifndef IRA_NO_OBSTACK\n+/* Obstack used for storing all dynamic data (except bitmaps) of the\n+   IRA.  */\n+static struct obstack ira_obstack;\n+#endif\n+\n+/* Obstack used for storing all bitmaps of the IRA.  */\n+static struct bitmap_obstack ira_bitmap_obstack;\n+\n+/* Allocate memory of size LEN for IRA data.  */\n+void *\n+ira_allocate (size_t len)\n+{\n+  void *res;\n+\n+#ifndef IRA_NO_OBSTACK\n+  res = obstack_alloc (&ira_obstack, len);\n+#else\n+  res = xmalloc (len);\n+#endif\n+  return res;\n+}\n+\n+/* Reallocate memory PTR of size LEN for IRA data.  */\n+void *\n+ira_reallocate (void *ptr, size_t len)\n+{\n+  void *res;\n+\n+#ifndef IRA_NO_OBSTACK\n+  res = obstack_alloc (&ira_obstack, len);\n+#else\n+  res = xrealloc (ptr, len);\n+#endif\n+  return res;\n+}\n+\n+/* Free memory ADDR allocated for IRA data.  */\n+void\n+ira_free (void *addr ATTRIBUTE_UNUSED)\n+{\n+#ifndef IRA_NO_OBSTACK\n+  /* do nothing */\n+#else\n+  free (addr);\n+#endif\n+}\n+\n+\n+/* Allocate and returns bitmap for IRA.  */\n+bitmap\n+ira_allocate_bitmap (void)\n+{\n+  return BITMAP_ALLOC (&ira_bitmap_obstack);\n+}\n+\n+/* Free bitmap B allocated for IRA.  */\n+void\n+ira_free_bitmap (bitmap b ATTRIBUTE_UNUSED)\n+{\n+  /* do nothing */\n+}\n+\n+\f\n+\n+/* Output information about allocation of all allocnos (except for\n+   caps) into file F.  */\n+void\n+ira_print_disposition (FILE *f)\n+{\n+  int i, n, max_regno;\n+  ira_allocno_t a;\n+  basic_block bb;\n+\n+  fprintf (f, \"Disposition:\");\n+  max_regno = max_reg_num ();\n+  for (n = 0, i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+    for (a = ira_regno_allocno_map[i];\n+\t a != NULL;\n+\t a = ALLOCNO_NEXT_REGNO_ALLOCNO (a))\n+      {\n+\tif (n % 4 == 0)\n+\t  fprintf (f, \"\\n\");\n+\tn++;\n+\tfprintf (f, \" %4d:r%-4d\", ALLOCNO_NUM (a), ALLOCNO_REGNO (a));\n+\tif ((bb = ALLOCNO_LOOP_TREE_NODE (a)->bb) != NULL)\n+\t  fprintf (f, \"b%-3d\", bb->index);\n+\telse\n+\t  fprintf (f, \"l%-3d\", ALLOCNO_LOOP_TREE_NODE (a)->loop->num);\n+\tif (ALLOCNO_HARD_REGNO (a) >= 0)\n+\t  fprintf (f, \" %3d\", ALLOCNO_HARD_REGNO (a));\n+\telse\n+\t  fprintf (f, \" mem\");\n+      }\n+  fprintf (f, \"\\n\");\n+}\n+\n+/* Outputs information about allocation of all allocnos into\n+   stderr.  */\n+void\n+ira_debug_disposition (void)\n+{\n+  ira_print_disposition (stderr);\n+}\n+\n+\f\n+\n+/* For each reg class, table listing all the classes contained in it\n+   (excluding the class itself.  Non-allocatable registers are\n+   excluded from the consideration).  */\n+static enum reg_class alloc_reg_class_subclasses[N_REG_CLASSES][N_REG_CLASSES];\n+\n+/* Initialize the table of subclasses of each reg class.  */\n+static void\n+setup_reg_subclasses (void)\n+{\n+  int i, j;\n+  HARD_REG_SET temp_hard_regset2;\n+\n+  for (i = 0; i < N_REG_CLASSES; i++)\n+    for (j = 0; j < N_REG_CLASSES; j++)\n+      alloc_reg_class_subclasses[i][j] = LIM_REG_CLASSES;\n+\n+  for (i = 0; i < N_REG_CLASSES; i++)\n+    {\n+      if (i == (int) NO_REGS)\n+\tcontinue;\n+\n+      COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[i]);\n+      AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+      if (hard_reg_set_equal_p (temp_hard_regset, ira_zero_hard_reg_set))\n+\tcontinue;\n+      for (j = 0; j < N_REG_CLASSES; j++)\n+\tif (i != j)\n+\t  {\n+\t    enum reg_class *p;\n+\n+\t    COPY_HARD_REG_SET (temp_hard_regset2, reg_class_contents[j]);\n+\t    AND_COMPL_HARD_REG_SET (temp_hard_regset2, no_unit_alloc_regs);\n+\t    if (! hard_reg_set_subset_p (temp_hard_regset,\n+\t\t\t\t\t temp_hard_regset2))\n+\t      continue;\n+\t    p = &alloc_reg_class_subclasses[j][0];\n+\t    while (*p != LIM_REG_CLASSES) p++;\n+\t    *p = (enum reg_class) i;\n+\t  }\n+    }\n+}\n+\n+\f\n+\n+/* Number of cover classes.  Cover classes is non-intersected register\n+   classes containing all hard-registers available for the\n+   allocation.  */\n+int ira_reg_class_cover_size;\n+\n+/* The array containing cover classes (see also comments for macro\n+   IRA_COVER_CLASSES).  Only first IRA_REG_CLASS_COVER_SIZE elements are\n+   used for this.  */\n+enum reg_class ira_reg_class_cover[N_REG_CLASSES];\n+\n+/* The number of elements in the subsequent array.  */\n+int ira_important_classes_num;\n+\n+/* The array containing non-empty classes (including non-empty cover\n+   classes) which are subclasses of cover classes.  Such classes is\n+   important for calculation of the hard register usage costs.  */\n+enum reg_class ira_important_classes[N_REG_CLASSES];\n+\n+/* The array containing indexes of important classes in the previous\n+   array.  The array elements are defined only for important\n+   classes.  */\n+int ira_important_class_nums[N_REG_CLASSES];\n+\n+#ifdef IRA_COVER_CLASSES\n+\n+/* Check IRA_COVER_CLASSES and sets the four global variables defined\n+   above.  */\n+static void\n+setup_cover_and_important_classes (void)\n+{\n+  int i, j;\n+  enum reg_class cl;\n+  static enum reg_class classes[] = IRA_COVER_CLASSES;\n+  HARD_REG_SET temp_hard_regset2;\n+\n+  ira_reg_class_cover_size = 0;\n+  for (i = 0; (cl = classes[i]) != LIM_REG_CLASSES; i++)\n+    {\n+      for (j = 0; j < i; j++)\n+\tif (reg_classes_intersect_p (cl, classes[j]))\n+\t  gcc_unreachable ();\n+      COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl]);\n+      AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+      if (! hard_reg_set_equal_p (temp_hard_regset, ira_zero_hard_reg_set))\n+\tira_reg_class_cover[ira_reg_class_cover_size++] = cl;\n+    }\n+  ira_important_classes_num = 0;\n+  for (cl = 0; cl < N_REG_CLASSES; cl++)\n+    {\n+      COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl]);\n+      AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+      if (! hard_reg_set_equal_p (temp_hard_regset, ira_zero_hard_reg_set))\n+\tfor (j = 0; j < ira_reg_class_cover_size; j++)\n+\t  {\n+\t    COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl]);\n+\t    AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+\t    COPY_HARD_REG_SET (temp_hard_regset2,\n+\t\t\t       reg_class_contents[ira_reg_class_cover[j]]);\n+\t    AND_COMPL_HARD_REG_SET (temp_hard_regset2, no_unit_alloc_regs);\n+\t    if (cl == ira_reg_class_cover[j]\n+\t\t|| (hard_reg_set_subset_p (temp_hard_regset, temp_hard_regset2)\n+\t\t    && ! hard_reg_set_equal_p (temp_hard_regset,\n+\t\t\t\t\t       temp_hard_regset2)))\n+\t      {\n+\t\tira_important_class_nums[cl] = ira_important_classes_num;\n+\t\tira_important_classes[ira_important_classes_num++] = cl;\n+\t      }\n+\t  }\n+    }\n+}\n+#endif\n+\n+/* Map of all register classes to corresponding cover class containing\n+   the given class.  If given class is not a subset of a cover class,\n+   we translate it into the cheapest cover class.  */\n+enum reg_class ira_class_translate[N_REG_CLASSES];\n+\n+#ifdef IRA_COVER_CLASSES\n+\n+/* Set up array IRA_CLASS_TRANSLATE.  */\n+static void\n+setup_class_translate (void)\n+{\n+  enum reg_class cl, cover_class, best_class, *cl_ptr;\n+  enum machine_mode mode;\n+  int i, cost, min_cost, best_cost;\n+\n+  for (cl = 0; cl < N_REG_CLASSES; cl++)\n+    ira_class_translate[cl] = NO_REGS;\n+  for (i = 0; i < ira_reg_class_cover_size; i++)\n+    {\n+      cover_class = ira_reg_class_cover[i];\n+      for (cl_ptr = &alloc_reg_class_subclasses[cover_class][0];\n+\t   (cl = *cl_ptr) != LIM_REG_CLASSES;\n+\t   cl_ptr++)\n+\t{\n+\t  if (ira_class_translate[cl] == NO_REGS)\n+\t    ira_class_translate[cl] = cover_class;\n+#ifdef ENABLE_IRA_CHECKING\n+\t  else\n+\t    {\n+\t      COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl]);\n+\t      AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+\t      if (! hard_reg_set_subset_p (temp_hard_regset,\n+\t\t\t\t\t   ira_zero_hard_reg_set))\n+\t\tgcc_unreachable ();\n+\t    }\n+#endif\n+\t}\n+      ira_class_translate[cover_class] = cover_class;\n+    }\n+  /* For classes which are not fully covered by a cover class (in\n+     other words covered by more one cover class), use the cheapest\n+     cover class.  */\n+  for (cl = 0; cl < N_REG_CLASSES; cl++)\n+    {\n+      if (cl == NO_REGS || ira_class_translate[cl] != NO_REGS)\n+\tcontinue;\n+      best_class = NO_REGS;\n+      best_cost = INT_MAX;\n+      for (i = 0; i < ira_reg_class_cover_size; i++)\n+\t{\n+\t  cover_class = ira_reg_class_cover[i];\n+\t  COPY_HARD_REG_SET (temp_hard_regset,\n+\t\t\t     reg_class_contents[cover_class]);\n+\t  AND_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl]);\n+\t  AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+\t  if (! hard_reg_set_equal_p (temp_hard_regset, ira_zero_hard_reg_set))\n+\t    {\n+\t      min_cost = INT_MAX;\n+\t      for (mode = 0; mode < MAX_MACHINE_MODE; mode++)\n+\t\t{\n+\t\t  cost = (ira_memory_move_cost[mode][cl][0]\n+\t\t\t  + ira_memory_move_cost[mode][cl][1]);\n+\t\t  if (min_cost > cost)\n+\t\t    min_cost = cost;\n+\t\t}\n+\t      if (best_class == NO_REGS || best_cost > min_cost)\n+\t\t{\n+\t\t  best_class = cover_class;\n+\t\t  best_cost = min_cost;\n+\t\t}\n+\t    }\n+\t}\n+      ira_class_translate[cl] = best_class;\n+    }\n+}\n+#endif\n+\n+/* The biggest important reg_class inside of intersection of the two\n+   reg_classes (that is calculated taking only hard registers\n+   available for allocation into account).  If the both reg_classes\n+   contain no hard registers available for allocation, the value is\n+   calculated by taking all hard-registers including fixed ones into\n+   account.  */\n+enum reg_class ira_reg_class_intersect[N_REG_CLASSES][N_REG_CLASSES];\n+\n+/* The biggest important reg_class inside of union of the two\n+   reg_classes (that is calculated taking only hard registers\n+   available for allocation into account).  If the both reg_classes\n+   contain no hard registers available for allocation, the value is\n+   calculated by taking all hard-registers including fixed ones into\n+   account.  In other words, the value is the corresponding\n+   reg_class_subunion value.  */\n+enum reg_class ira_reg_class_union[N_REG_CLASSES][N_REG_CLASSES];\n+\n+#ifdef IRA_COVER_CLASSES\n+\n+/* Set up IRA_REG_CLASS_INTERSECT and IRA_REG_CLASS_UNION.  */\n+static void\n+setup_reg_class_intersect_union (void)\n+{\n+  int i, cl1, cl2, cl3;\n+  HARD_REG_SET intersection_set, union_set, temp_set2;\n+\n+  for (cl1 = 0; cl1 < N_REG_CLASSES; cl1++)\n+    {\n+      for (cl2 = 0; cl2 < N_REG_CLASSES; cl2++)\n+\t{\n+\t  ira_reg_class_intersect[cl1][cl2] = NO_REGS;\n+\t  COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl1]);\n+\t  AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+\t  COPY_HARD_REG_SET (temp_set2, reg_class_contents[cl2]);\n+\t  AND_COMPL_HARD_REG_SET (temp_set2, no_unit_alloc_regs);\n+\t  if (hard_reg_set_equal_p (temp_hard_regset, ira_zero_hard_reg_set)\n+\t      && hard_reg_set_equal_p (temp_set2, ira_zero_hard_reg_set))\n+\t    {\n+\t      for (i = 0;; i++)\n+\t\t{\n+\t\t  cl3 = reg_class_subclasses[cl1][i];\n+\t\t  if (cl3 == LIM_REG_CLASSES)\n+\t\t    break;\n+\t\t  if (reg_class_subset_p (ira_reg_class_intersect[cl1][cl2],\n+\t\t\t\t\t  cl3))\n+\t\t    ira_reg_class_intersect[cl1][cl2] = cl3;\n+\t\t}\n+\t      ira_reg_class_union[cl1][cl2] = reg_class_subunion[cl1][cl2];\n+\t      continue;\n+\t    }\n+\t  ira_reg_class_union[cl1][cl2] = NO_REGS;\n+\t  COPY_HARD_REG_SET (intersection_set, reg_class_contents[cl1]);\n+\t  AND_HARD_REG_SET (intersection_set, reg_class_contents[cl2]);\n+\t  AND_COMPL_HARD_REG_SET (intersection_set, no_unit_alloc_regs);\n+\t  COPY_HARD_REG_SET (union_set, reg_class_contents[cl1]);\n+\t  IOR_HARD_REG_SET (union_set, reg_class_contents[cl2]);\n+\t  AND_COMPL_HARD_REG_SET (union_set, no_unit_alloc_regs);\n+\t  for (i = 0; i < ira_important_classes_num; i++)\n+\t    {\n+\t      cl3 = ira_important_classes[i];\n+\t      COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl3]);\n+\t      AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n+\t      if (hard_reg_set_subset_p (temp_hard_regset, intersection_set))\n+\t\t{\n+\t\t  COPY_HARD_REG_SET\n+\t\t    (temp_set2,\n+\t\t     reg_class_contents[(int)\n+\t\t\t\t\tira_reg_class_intersect[cl1][cl2]]);\n+\t\t  AND_COMPL_HARD_REG_SET (temp_set2, no_unit_alloc_regs);\n+\t \t  if (! hard_reg_set_subset_p (temp_hard_regset, temp_set2)\n+\t\t      /* Ignore unavailable hard registers and prefer\n+\t\t\t smallest class for debugging purposes.  */\n+\t\t      || (hard_reg_set_equal_p (temp_hard_regset, temp_set2)\n+\t\t\t  && hard_reg_set_subset_p\n+\t\t\t     (reg_class_contents[cl3],\n+\t\t\t      reg_class_contents\n+\t\t\t      [(int) ira_reg_class_intersect[cl1][cl2]])))\n+\t\t    ira_reg_class_intersect[cl1][cl2] = (enum reg_class) cl3;\n+\t\t}\n+\t      if (hard_reg_set_subset_p (temp_hard_regset, union_set))\n+\t\t{\n+\t\t  COPY_HARD_REG_SET\n+\t\t    (temp_set2,\n+\t\t     reg_class_contents[(int) ira_reg_class_union[cl1][cl2]]);\n+\t\t  AND_COMPL_HARD_REG_SET (temp_set2, no_unit_alloc_regs);\n+\t \t  if (ira_reg_class_union[cl1][cl2] == NO_REGS\n+\t\t      || (hard_reg_set_subset_p (temp_set2, temp_hard_regset)\n+\t\t      \n+\t\t\t  && (! hard_reg_set_equal_p (temp_set2,\n+\t\t\t\t\t\t      temp_hard_regset)\n+\t\t\t      /* Ignore unavailable hard registers and\n+\t\t\t\t prefer smallest class for debugging\n+\t\t\t\t purposes.  */\n+\t\t\t      || hard_reg_set_subset_p\n+\t\t\t         (reg_class_contents[cl3],\n+\t\t\t\t  reg_class_contents\n+\t\t\t\t  [(int) ira_reg_class_union[cl1][cl2]]))))\n+\t\t    ira_reg_class_union[cl1][cl2] = (enum reg_class) cl3;\n+\t\t}\n+\t    }\n+\t}\n+    }\n+}\n+\n+#endif\n+\n+/* Output all cover classes and the translation map into file F.  */\n+static void\n+print_class_cover (FILE *f)\n+{\n+  static const char *const reg_class_names[] = REG_CLASS_NAMES;\n+  int i;\n+\n+  fprintf (f, \"Class cover:\\n\");\n+  for (i = 0; i < ira_reg_class_cover_size; i++)\n+    fprintf (f, \" %s\", reg_class_names[ira_reg_class_cover[i]]);\n+  fprintf (f, \"\\nClass translation:\\n\");\n+  for (i = 0; i < N_REG_CLASSES; i++)\n+    fprintf (f, \" %s -> %s\\n\", reg_class_names[i],\n+\t     reg_class_names[ira_class_translate[i]]);\n+}\n+\n+/* Output all cover classes and the translation map into\n+   stderr.  */\n+void\n+ira_debug_class_cover (void)\n+{\n+  print_class_cover (stderr);\n+}\n+\n+/* Set up different arrays concerning class subsets, cover and\n+   important classes.  */\n+static void\n+find_reg_class_closure (void)\n+{\n+  setup_reg_subclasses ();\n+#ifdef IRA_COVER_CLASSES\n+  setup_cover_and_important_classes ();\n+  setup_class_translate ();\n+  setup_reg_class_intersect_union ();\n+#endif\n+}\n+\n+\f\n+\n+/* Map: register class x machine mode -> number of hard registers of\n+   given class needed to store value of given mode.  If the number is\n+   different, the size will be negative.  */\n+int ira_reg_class_nregs[N_REG_CLASSES][MAX_MACHINE_MODE];\n+\n+/* Maximal value of the previous array elements.  */\n+int ira_max_nregs;\n+\n+/* Form IRA_REG_CLASS_NREGS map.  */\n+static void\n+setup_reg_class_nregs (void)\n+{\n+  int m;\n+  enum reg_class cl;\n+\n+  ira_max_nregs = -1;\n+  for (cl = 0; cl < N_REG_CLASSES; cl++)\n+    for (m = 0; m < MAX_MACHINE_MODE; m++)\n+      {\n+\tira_reg_class_nregs[cl][m] = CLASS_MAX_NREGS (cl, m);\n+\tif (ira_max_nregs < ira_reg_class_nregs[cl][m])\n+\t  ira_max_nregs = ira_reg_class_nregs[cl][m];\n+      }\n+}\n+\n+\f\n+\n+/* Array whose values are hard regset of hard registers available for\n+   the allocation of given register class whose HARD_REGNO_MODE_OK\n+   values for given mode are zero.  */\n+HARD_REG_SET prohibited_class_mode_regs[N_REG_CLASSES][NUM_MACHINE_MODES];\n+\n+/* Set up PROHIBITED_CLASS_MODE_REGS.  */\n+static void\n+setup_prohibited_class_mode_regs (void)\n+{\n+  int i, j, k, hard_regno;\n+  enum reg_class cl;\n+\n+  for (i = 0; i < ira_reg_class_cover_size; i++)\n+    {\n+      cl = ira_reg_class_cover[i];\n+      for (j = 0; j < NUM_MACHINE_MODES; j++)\n+\t{\n+\t  CLEAR_HARD_REG_SET (prohibited_class_mode_regs[cl][j]);\n+\t  for (k = ira_class_hard_regs_num[cl] - 1; k >= 0; k--)\n+\t    {\n+\t      hard_regno = ira_class_hard_regs[cl][k];\n+\t      if (! HARD_REGNO_MODE_OK (hard_regno, j))\n+\t\tSET_HARD_REG_BIT (prohibited_class_mode_regs[cl][j],\n+\t\t\t\t  hard_regno);\n+\t    }\n+\t}\n+    }\n+}\n+\n+\f\n+\n+/* Allocate and initialize IRA_REGISTER_MOVE_COST,\n+   IRA_MAY_MOVE_IN_COST, and IRA_MAY_MOVE_OUT_COST for MODE if it is\n+   not done yet.  */\n+void\n+ira_init_register_move_cost (enum machine_mode mode)\n+{\n+  int cl1, cl2;\n+\n+  ira_assert (ira_register_move_cost[mode] == NULL\n+\t      && ira_may_move_in_cost[mode] == NULL\n+\t      && ira_may_move_out_cost[mode] == NULL);\n+  if (move_cost[mode] == NULL)\n+    init_move_cost (mode);\n+  ira_register_move_cost[mode] = move_cost[mode];\n+  /* Don't use ira_allocate because the tables exist out of scope of a\n+     IRA call.  */\n+  ira_may_move_in_cost[mode]\n+    = (move_table *) xmalloc (sizeof (move_table) * N_REG_CLASSES);\n+  memcpy (ira_may_move_in_cost[mode], may_move_in_cost[mode],\n+\t  sizeof (move_table) * N_REG_CLASSES);\n+  ira_may_move_out_cost[mode]\n+    = (move_table *) xmalloc (sizeof (move_table) * N_REG_CLASSES);\n+  memcpy (ira_may_move_out_cost[mode], may_move_out_cost[mode],\n+\t  sizeof (move_table) * N_REG_CLASSES);\n+  for (cl1 = 0; cl1 < N_REG_CLASSES; cl1++)\n+    {\n+      for (cl2 = 0; cl2 < N_REG_CLASSES; cl2++)\n+\t{\n+\t  if (ira_class_subset_p[cl1][cl2])\n+\t    ira_may_move_in_cost[mode][cl1][cl2] = 0;\n+\t  if (ira_class_subset_p[cl2][cl1])\n+\t    ira_may_move_out_cost[mode][cl1][cl2] = 0;\n+\t}\n+    }\n+}\n+\n+\f\n+\n+/* Hard regsets whose all bits are correspondingly zero or one.  */\n+HARD_REG_SET ira_zero_hard_reg_set;\n+HARD_REG_SET ira_one_hard_reg_set;\n+\n+/* This is called once during compiler work.  It sets up\n+   different arrays whose values don't depend on the compiled\n+   function.  */\n+void\n+ira_init_once (void)\n+{\n+  enum machine_mode mode;\n+\n+  CLEAR_HARD_REG_SET (ira_zero_hard_reg_set);\n+  SET_HARD_REG_SET (ira_one_hard_reg_set);\n+  for (mode = 0; mode < MAX_MACHINE_MODE; mode++)\n+    {\n+      ira_register_move_cost[mode] = NULL;\n+      ira_may_move_in_cost[mode] = NULL;\n+      ira_may_move_out_cost[mode] = NULL;\n+    }\n+  ira_init_costs_once ();\n+}\n+\n+/* Free ira_register_move_cost, ira_may_move_in_cost, and\n+   ira_may_move_out_cost for each mode.  */\n+static void\n+free_register_move_costs (void)\n+{\n+  enum machine_mode mode;\n+\n+  for (mode = 0; mode < MAX_MACHINE_MODE; mode++)\n+    {\n+      if (ira_may_move_in_cost[mode] != NULL)\n+\tfree (ira_may_move_in_cost[mode]);\n+      if (ira_may_move_out_cost[mode] != NULL)\n+\tfree (ira_may_move_out_cost[mode]);\n+      ira_register_move_cost[mode] = NULL;\n+      ira_may_move_in_cost[mode] = NULL;\n+      ira_may_move_out_cost[mode] = NULL;\n+    }\n+}\n+\n+/* This is called every time when register related information is\n+   changed.  */\n+void\n+ira_init (void)\n+{\n+  free_register_move_costs ();\n+  setup_reg_mode_hard_regset ();\n+  setup_alloc_regs (flag_omit_frame_pointer != 0);\n+  setup_class_subset_and_memory_move_costs ();\n+  find_reg_class_closure ();\n+  setup_reg_class_nregs ();\n+  setup_prohibited_class_mode_regs ();\n+  ira_init_costs ();\n+}\n+\n+/* Function called once at the end of compiler work.  */\n+void\n+ira_finish_once (void)\n+{\n+  ira_finish_costs_once ();\n+  free_register_move_costs ();\n+}\n+\n+\f\n+\n+/* Array whose values are hard regset of hard registers for which\n+   move of the hard register in given mode into itself is\n+   prohibited.  */\n+HARD_REG_SET ira_prohibited_mode_move_regs[NUM_MACHINE_MODES];\n+\n+/* Flag of that the above array has been initialized.  */\n+static bool ira_prohibited_mode_move_regs_initialized_p = false;\n+\n+/* Set up IRA_PROHIBITED_MODE_MOVE_REGS.  */\n+static void\n+setup_prohibited_mode_move_regs (void)\n+{\n+  int i, j;\n+  rtx test_reg1, test_reg2, move_pat, move_insn;\n+\n+  if (ira_prohibited_mode_move_regs_initialized_p)\n+    return;\n+  ira_prohibited_mode_move_regs_initialized_p = true;\n+  test_reg1 = gen_rtx_REG (VOIDmode, 0);\n+  test_reg2 = gen_rtx_REG (VOIDmode, 0);\n+  move_pat = gen_rtx_SET (VOIDmode, test_reg1, test_reg2);\n+  move_insn = gen_rtx_INSN (VOIDmode, 0, 0, 0, 0, 0, move_pat, -1, 0);\n+  for (i = 0; i < NUM_MACHINE_MODES; i++)\n+    {\n+      SET_HARD_REG_SET (ira_prohibited_mode_move_regs[i]);\n+      for (j = 0; j < FIRST_PSEUDO_REGISTER; j++)\n+\t{\n+\t  if (! HARD_REGNO_MODE_OK (j, i))\n+\t    continue;\n+\t  SET_REGNO (test_reg1, j);\n+\t  PUT_MODE (test_reg1, i);\n+\t  SET_REGNO (test_reg2, j);\n+\t  PUT_MODE (test_reg2, i);\n+\t  INSN_CODE (move_insn) = -1;\n+\t  recog_memoized (move_insn);\n+\t  if (INSN_CODE (move_insn) < 0)\n+\t    continue;\n+\t  extract_insn (move_insn);\n+\t  if (! constrain_operands (1))\n+\t    continue;\n+\t  CLEAR_HARD_REG_BIT (ira_prohibited_mode_move_regs[i], j);\n+\t}\n+    }\n+}\n+\n+\f\n+\n+/* Function specific hard registers that can not be used for the\n+   register allocation.  */\n+HARD_REG_SET ira_no_alloc_regs;\n+\n+/* Return TRUE if *LOC contains an asm.  */\n+static int\n+insn_contains_asm_1 (rtx *loc, void *data ATTRIBUTE_UNUSED)\n+{\n+  if ( !*loc)\n+    return FALSE;\n+  if (GET_CODE (*loc) == ASM_OPERANDS)\n+    return TRUE;\n+  return FALSE;\n+}\n+\n+\n+/* Return TRUE if INSN contains an ASM.  */\n+static bool\n+insn_contains_asm (rtx insn)\n+{\n+  return for_each_rtx (&insn, insn_contains_asm_1, NULL);\n+}\n+\n+/* Set up regs_asm_clobbered.  */\n+static void\n+compute_regs_asm_clobbered (char *regs_asm_clobbered)\n+{\n+  basic_block bb;\n+\n+  memset (regs_asm_clobbered, 0, sizeof (char) * FIRST_PSEUDO_REGISTER);\n+  \n+  FOR_EACH_BB (bb)\n+    {\n+      rtx insn;\n+      FOR_BB_INSNS_REVERSE (bb, insn)\n+\t{\n+\t  struct df_ref **def_rec;\n+\n+\t  if (insn_contains_asm (insn))\n+\t    for (def_rec = DF_INSN_DEFS (insn); *def_rec; def_rec++)\n+\t      {\n+\t\tstruct df_ref *def = *def_rec;\n+\t\tunsigned int dregno = DF_REF_REGNO (def);\n+\t\tif (dregno < FIRST_PSEUDO_REGISTER)\n+\t\t  {\n+\t\t    unsigned int i;\n+\t\t    enum machine_mode mode = GET_MODE (DF_REF_REAL_REG (def));\n+\t\t    unsigned int end = dregno \n+\t\t      + hard_regno_nregs[dregno][mode] - 1;\n+\n+\t\t    for (i = dregno; i <= end; ++i)\n+\t\t      regs_asm_clobbered[i] = 1;\n+\t\t  }\n+\t      }\n+\t}\n+    }\n+}\n+\n+\n+/* Set up ELIMINABLE_REGSET, IRA_NO_ALLOC_REGS, and REGS_EVER_LIVE.  */\n+static void\n+setup_eliminable_regset (void)\n+{\n+  int i;\n+  /* Like regs_ever_live, but 1 if a reg is set or clobbered from an\n+     asm.  Unlike regs_ever_live, elements of this array corresponding\n+     to eliminable regs (like the frame pointer) are set if an asm\n+     sets them.  */\n+  char *regs_asm_clobbered\n+    = (char *) alloca (FIRST_PSEUDO_REGISTER * sizeof (char));\n+#ifdef ELIMINABLE_REGS\n+  static const struct {const int from, to; } eliminables[] = ELIMINABLE_REGS;\n+#endif\n+  /* FIXME: If EXIT_IGNORE_STACK is set, we will not save and restore\n+     sp for alloca.  So we can't eliminate the frame pointer in that\n+     case.  At some point, we should improve this by emitting the\n+     sp-adjusting insns for this case.  */\n+  int need_fp\n+    = (! flag_omit_frame_pointer\n+       || (cfun->calls_alloca && EXIT_IGNORE_STACK)\n+       || crtl->accesses_prior_frames\n+       || crtl->stack_realign_needed\n+       || FRAME_POINTER_REQUIRED);\n+\n+  frame_pointer_needed = need_fp;\n+\n+  COPY_HARD_REG_SET (ira_no_alloc_regs, no_unit_alloc_regs);\n+  CLEAR_HARD_REG_SET (eliminable_regset);\n+\n+  compute_regs_asm_clobbered (regs_asm_clobbered);\n+  /* Build the regset of all eliminable registers and show we can't\n+     use those that we already know won't be eliminated.  */\n+#ifdef ELIMINABLE_REGS\n+  for (i = 0; i < (int) ARRAY_SIZE (eliminables); i++)\n+    {\n+      bool cannot_elim\n+\t= (! CAN_ELIMINATE (eliminables[i].from, eliminables[i].to)\n+\t   || (eliminables[i].to == STACK_POINTER_REGNUM && need_fp));\n+\n+      if (! regs_asm_clobbered[eliminables[i].from])\n+\t{\n+\t    SET_HARD_REG_BIT (eliminable_regset, eliminables[i].from);\n+\n+\t    if (cannot_elim)\n+\t      SET_HARD_REG_BIT (ira_no_alloc_regs, eliminables[i].from);\n+\t}\n+      else if (cannot_elim)\n+\terror (\"%s cannot be used in asm here\",\n+\t       reg_names[eliminables[i].from]);\n+      else\n+\tdf_set_regs_ever_live (eliminables[i].from, true);\n+    }\n+#if FRAME_POINTER_REGNUM != HARD_FRAME_POINTER_REGNUM\n+  if (! regs_asm_clobbered[HARD_FRAME_POINTER_REGNUM])\n+    {\n+      SET_HARD_REG_BIT (eliminable_regset, HARD_FRAME_POINTER_REGNUM);\n+      if (need_fp)\n+\tSET_HARD_REG_BIT (ira_no_alloc_regs, HARD_FRAME_POINTER_REGNUM);\n+    }\n+  else if (need_fp)\n+    error (\"%s cannot be used in asm here\",\n+\t   reg_names[HARD_FRAME_POINTER_REGNUM]);\n+  else\n+    df_set_regs_ever_live (HARD_FRAME_POINTER_REGNUM, true);\n+#endif\n+\n+#else\n+  if (! regs_asm_clobbered[FRAME_POINTER_REGNUM])\n+    {\n+      SET_HARD_REG_BIT (eliminable_regset, FRAME_POINTER_REGNUM);\n+      if (need_fp)\n+\tSET_HARD_REG_BIT (ira_no_alloc_regs, FRAME_POINTER_REGNUM);\n+    }\n+  else if (need_fp)\n+    error (\"%s cannot be used in asm here\", reg_names[FRAME_POINTER_REGNUM]);\n+  else\n+    df_set_regs_ever_live (FRAME_POINTER_REGNUM, true);\n+#endif\n+}\n+\n+\f\n+\n+/* The length of the following two arrays.  */\n+int ira_reg_equiv_len;\n+\n+/* The element value is TRUE if the corresponding regno value is\n+   invariant.  */\n+bool *ira_reg_equiv_invariant_p;\n+\n+/* The element value is equiv constant of given pseudo-register or\n+   NULL_RTX.  */\n+rtx *ira_reg_equiv_const;\n+\n+/* Set up the two arrays declared above.  */\n+static void\n+find_reg_equiv_invariant_const (void)\n+{\n+  int i;\n+  bool invariant_p;\n+  rtx list, insn, note, constant, x;\n+\n+  for (i = FIRST_PSEUDO_REGISTER; i < reg_equiv_init_size; i++)\n+    {\n+      constant = NULL_RTX;\n+      invariant_p = false;\n+      for (list = reg_equiv_init[i]; list != NULL_RTX; list = XEXP (list, 1))\n+\t{\n+\t  insn = XEXP (list, 0);\n+\t  note = find_reg_note (insn, REG_EQUIV, NULL_RTX);\n+\t  \n+\t  if (note == NULL_RTX)\n+\t    continue;\n+\n+\t  x = XEXP (note, 0);\n+\t  \n+\t  if (! function_invariant_p (x)\n+\t      || ! flag_pic\n+\t      /* A function invariant is often CONSTANT_P but may\n+\t\t include a register.  We promise to only pass CONSTANT_P\n+\t\t objects to LEGITIMATE_PIC_OPERAND_P.  */\n+\t      || (CONSTANT_P (x) && LEGITIMATE_PIC_OPERAND_P (x)))\n+\t    {\n+\t      /* It can happen that a REG_EQUIV note contains a MEM\n+\t\t that is not a legitimate memory operand.  As later\n+\t\t stages of the reload assume that all addresses found\n+\t\t in the reg_equiv_* arrays were originally legitimate,\n+\t\t we ignore such REG_EQUIV notes.  */\n+\t      if (memory_operand (x, VOIDmode))\n+\t\tinvariant_p = MEM_READONLY_P (x);\n+\t      else if (function_invariant_p (x))\n+\t\t{\n+\t\t  if (GET_CODE (x) == PLUS\n+\t\t      || x == frame_pointer_rtx || x == arg_pointer_rtx)\n+\t\t    invariant_p = true;\n+\t\t  else\n+\t\t    constant = x;\n+\t\t}\n+\t    }\n+\t}\n+      ira_reg_equiv_invariant_p[i] = invariant_p;\n+      ira_reg_equiv_const[i] = constant;\n+    }\n+}\n+\n+\f\n+\n+/* Set up REG_RENUMBER and CALLER_SAVE_NEEDED (used by reload) from\n+   the allocation found by IRA.  */\n+static void\n+setup_reg_renumber (void)\n+{\n+  int regno, hard_regno;\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  caller_save_needed = 0;\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      /* There are no caps at this point.  */\n+      ira_assert (ALLOCNO_CAP_MEMBER (a) == NULL);\n+      if (! ALLOCNO_ASSIGNED_P (a))\n+\t/* It can happen if A is not referenced but partially anticipated\n+\t   somewhere in a region.  */\n+\tALLOCNO_ASSIGNED_P (a) = true;\n+      ira_free_allocno_updated_costs (a);\n+      hard_regno = ALLOCNO_HARD_REGNO (a);\n+      regno = (int) REGNO (ALLOCNO_REG (a));\n+      reg_renumber[regno] = (hard_regno < 0 ? -1 : hard_regno);\n+      if (hard_regno >= 0 && ALLOCNO_CALLS_CROSSED_NUM (a) != 0\n+\t  && ! ira_hard_reg_not_in_set_p (hard_regno, ALLOCNO_MODE (a),\n+\t\t\t\t\t  call_used_reg_set))\n+\t{\n+\t  ira_assert (!optimize || flag_caller_saves\n+\t\t      || regno >= ira_reg_equiv_len\n+\t\t      || ira_reg_equiv_const[regno]\n+\t\t      || ira_reg_equiv_invariant_p[regno]);\n+\t  caller_save_needed = 1;\n+\t}\n+    }\n+}\n+\n+/* Set up allocno assignment flags for further allocation\n+   improvements.  */\n+static void\n+setup_allocno_assignment_flags (void)\n+{\n+  int hard_regno;\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      if (! ALLOCNO_ASSIGNED_P (a))\n+\t/* It can happen if A is not referenced but partially anticipated\n+\t   somewhere in a region.  */\n+\tira_free_allocno_updated_costs (a);\n+      hard_regno = ALLOCNO_HARD_REGNO (a);\n+      /* Don't assign hard registers to allocnos which are destination\n+\t of removed store at the end of loop.  It has no sense to keep\n+\t the same value in different hard registers.  It is also\n+\t impossible to assign hard registers correctly to such\n+\t allocnos because the cost info and info about intersected\n+\t calls are incorrect for them.  */\n+      ALLOCNO_ASSIGNED_P (a) = (hard_regno >= 0\n+\t\t\t\t|| ALLOCNO_MEM_OPTIMIZED_DEST_P (a)\n+\t\t\t\t|| (ALLOCNO_MEMORY_COST (a)\n+\t\t\t\t    - ALLOCNO_COVER_CLASS_COST (a)) < 0);\n+      ira_assert (hard_regno < 0\n+\t\t  || ! ira_hard_reg_not_in_set_p (hard_regno, ALLOCNO_MODE (a),\n+\t\t\t\t\t\t  reg_class_contents\n+\t\t\t\t\t\t  [ALLOCNO_COVER_CLASS (a)]));\n+    }\n+}\n+\n+/* Evaluate overall allocation cost and the costs for using hard\n+   registers and memory for allocnos.  */\n+static void\n+calculate_allocation_cost (void)\n+{\n+  int hard_regno, cost;\n+  ira_allocno_t a;\n+  ira_allocno_iterator ai;\n+\n+  ira_overall_cost = ira_reg_cost = ira_mem_cost = 0;\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      hard_regno = ALLOCNO_HARD_REGNO (a);\n+      ira_assert (hard_regno < 0\n+\t\t  || ! ira_hard_reg_not_in_set_p\n+\t\t       (hard_regno, ALLOCNO_MODE (a),\n+\t\t\treg_class_contents[ALLOCNO_COVER_CLASS (a)])); \n+      if (hard_regno < 0)\n+\t{\n+\t  cost = ALLOCNO_MEMORY_COST (a);\n+\t  ira_mem_cost += cost;\n+\t}\n+      else if (ALLOCNO_HARD_REG_COSTS (a) != NULL)\n+\t{\n+\t  cost = (ALLOCNO_HARD_REG_COSTS (a)\n+\t\t  [ira_class_hard_reg_index\n+\t\t   [ALLOCNO_COVER_CLASS (a)][hard_regno]]);\n+\t  ira_reg_cost += cost;\n+\t}\n+      else\n+\t{\n+\t  cost = ALLOCNO_COVER_CLASS_COST (a);\n+\t  ira_reg_cost += cost;\n+\t}\n+      ira_overall_cost += cost;\n+    }\n+\n+  if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL)\n+    {\n+      fprintf (ira_dump_file,\n+\t       \"+++Costs: overall %d, reg %d, mem %d, ld %d, st %d, move %d\\n\",\n+\t       ira_overall_cost, ira_reg_cost, ira_mem_cost,\n+\t       ira_load_cost, ira_store_cost, ira_shuffle_cost);\n+      fprintf (ira_dump_file, \"+++       move loops %d, new jumps %d\\n\",\n+\t       ira_move_loops_num, ira_additional_jumps_num);\n+    }\n+\n+}\n+\n+#ifdef ENABLE_IRA_CHECKING\n+/* Check the correctness of the allocation.  We do need this because\n+   of complicated code to transform more one region internal\n+   representation into one region representation.  */\n+static void\n+check_allocation (void)\n+{\n+  ira_allocno_t a, conflict_a;\n+  int hard_regno, conflict_hard_regno, nregs, conflict_nregs;\n+  ira_allocno_conflict_iterator aci;\n+  ira_allocno_iterator ai;\n+\n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      if (ALLOCNO_CAP_MEMBER (a) != NULL\n+\t  || (hard_regno = ALLOCNO_HARD_REGNO (a)) < 0)\n+\tcontinue;\n+      nregs = hard_regno_nregs[hard_regno][ALLOCNO_MODE (a)];\n+      FOR_EACH_ALLOCNO_CONFLICT (a, conflict_a, aci)\n+\tif ((conflict_hard_regno = ALLOCNO_HARD_REGNO (conflict_a)) >= 0)\n+\t  {\n+\t    conflict_nregs\n+\t      = (hard_regno_nregs\n+\t\t [conflict_hard_regno][ALLOCNO_MODE (conflict_a)]);\n+\t    if ((conflict_hard_regno <= hard_regno\n+\t\t && hard_regno < conflict_hard_regno + conflict_nregs)\n+\t\t|| (hard_regno <= conflict_hard_regno\n+\t\t    && conflict_hard_regno < hard_regno + nregs))\n+\t      {\n+\t\tfprintf (stderr, \"bad allocation for %d and %d\\n\",\n+\t\t\t ALLOCNO_REGNO (a), ALLOCNO_REGNO (conflict_a));\n+\t\tgcc_unreachable ();\n+\t      }\n+\t  }\n+    }\n+}\n+#endif\n+\n+/* Fix values of array REG_EQUIV_INIT after live range splitting done\n+   by IRA.  */\n+static void\n+fix_reg_equiv_init (void)\n+{\n+  int max_regno = max_reg_num ();\n+  int i, new_regno;\n+  rtx x, prev, next, insn, set;\n+  \n+  if (reg_equiv_init_size < max_regno)\n+    {\n+      reg_equiv_init\n+\t= (rtx *) ggc_realloc (reg_equiv_init, max_regno * sizeof (rtx));\n+      while (reg_equiv_init_size < max_regno)\n+\treg_equiv_init[reg_equiv_init_size++] = NULL_RTX;\n+      for (i = FIRST_PSEUDO_REGISTER; i < reg_equiv_init_size; i++)\n+\tfor (prev = NULL_RTX, x = reg_equiv_init[i]; x != NULL_RTX; x = next)\n+\t  {\n+\t    next = XEXP (x, 1);\n+\t    insn = XEXP (x, 0);\n+\t    set = single_set (insn);\n+\t    ira_assert (set != NULL_RTX\n+\t\t\t&& (REG_P (SET_DEST (set)) || REG_P (SET_SRC (set))));\n+\t    if (REG_P (SET_DEST (set))\n+\t\t&& ((int) REGNO (SET_DEST (set)) == i\n+\t\t    || (int) ORIGINAL_REGNO (SET_DEST (set)) == i))\n+\t      new_regno = REGNO (SET_DEST (set));\n+\t    else if (REG_P (SET_SRC (set))\n+\t\t     && ((int) REGNO (SET_SRC (set)) == i\n+\t\t\t || (int) ORIGINAL_REGNO (SET_SRC (set)) == i))\n+\t      new_regno = REGNO (SET_SRC (set));\n+\t    else\n+ \t      gcc_unreachable ();\n+\t    if (new_regno == i)\n+\t      prev = x;\n+\t    else\n+\t      {\n+\t\tif (prev == NULL_RTX)\n+\t\t  reg_equiv_init[i] = next;\n+\t\telse\n+\t\t  XEXP (prev, 1) = next;\n+\t\tXEXP (x, 1) = reg_equiv_init[new_regno];\n+\t\treg_equiv_init[new_regno] = x;\n+\t      }\n+\t  }\n+    }\n+}\n+\n+#ifdef ENABLE_IRA_CHECKING\n+/* Print redundant memory-memory copies.  */\n+static void\n+print_redundant_copies (void)\n+{\n+  int hard_regno;\n+  ira_allocno_t a;\n+  ira_copy_t cp, next_cp;\n+  ira_allocno_iterator ai;\n+  \n+  FOR_EACH_ALLOCNO (a, ai)\n+    {\n+      if (ALLOCNO_CAP_MEMBER (a) != NULL)\n+\t/* It is a cap. */\n+\tcontinue;\n+      hard_regno = ALLOCNO_HARD_REGNO (a);\n+      if (hard_regno >= 0)\n+\tcontinue;\n+      for (cp = ALLOCNO_COPIES (a); cp != NULL; cp = next_cp)\n+\tif (cp->first == a)\n+\t  next_cp = cp->next_first_allocno_copy;\n+\telse\n+\t  {\n+\t    next_cp = cp->next_second_allocno_copy;\n+\t    if (internal_flag_ira_verbose > 4 && ira_dump_file != NULL\n+\t\t&& cp->insn != NULL_RTX\n+\t\t&& ALLOCNO_HARD_REGNO (cp->first) == hard_regno)\n+\t      fprintf (ira_dump_file,\n+\t\t       \"        Redundant move from %d(freq %d):%d\\n\",\n+\t\t       INSN_UID (cp->insn), cp->freq, hard_regno);\n+\t  }\n+    }\n+}\n+#endif\n+\n+/* Setup preferred and alternative classes for new pseudo-registers\n+   created by IRA starting with START.  */\n+static void\n+setup_preferred_alternate_classes_for_new_pseudos (int start)\n+{\n+  int i, old_regno;\n+  int max_regno = max_reg_num ();\n+\n+  for (i = start; i < max_regno; i++)\n+    {\n+      old_regno = ORIGINAL_REGNO (regno_reg_rtx[i]);\n+      ira_assert (i != old_regno); \n+      setup_reg_classes (i, reg_preferred_class (old_regno),\n+\t\t\t reg_alternate_class (old_regno));\n+      if (internal_flag_ira_verbose > 2 && ira_dump_file != NULL)\n+\tfprintf (ira_dump_file,\n+\t\t \"    New r%d: setting preferred %s, alternative %s\\n\",\n+\t\t i, reg_class_names[reg_preferred_class (old_regno)],\n+\t\t reg_class_names[reg_alternate_class (old_regno)]);\n+    }\n+}\n+\n+\f\n+\n+/* Regional allocation can create new pseudo-registers.  This function\n+   expands some arrays for pseudo-registers.  */\n+static void\n+expand_reg_info (int old_size)\n+{\n+  int i;\n+  int size = max_reg_num ();\n+\n+  resize_reg_info ();\n+  for (i = old_size; i < size; i++)\n+    {\n+      reg_renumber[i] = -1;\n+      setup_reg_classes (i, GENERAL_REGS, ALL_REGS);\n+    }\n+}\n+\n+\f\n+\n+/* This page contains code for sorting the insn chain used by reload.\n+   In the old register allocator, the insn chain order corresponds to\n+   the order of insns in RTL.  By putting insns with higher execution\n+   frequency BBs first, reload has a better chance to generate less\n+   expensive operand reloads for such insns.  */\n+\n+/* Map bb index -> order number in the BB chain in RTL code.  */\n+static int *basic_block_order_nums;\n+\n+/* Map chain insn uid -> order number in the insn chain before sorting\n+   the insn chain.  */\n+static int *chain_insn_order;\n+\n+/* The function is used to sort insn chain according insn execution\n+   frequencies.  */\n+static int\n+chain_freq_compare (const void *v1p, const void *v2p)\n+{\n+  const struct insn_chain *c1 = *(struct insn_chain * const *)v1p;\n+  const struct insn_chain *c2 = *(struct insn_chain * const *)v2p;\n+  int diff;\n+\n+  diff = (BASIC_BLOCK (c2->block)->frequency\n+\t  - BASIC_BLOCK (c1->block)->frequency);\n+  if (diff)\n+    return diff;\n+  /* Keep the same order in BB scope.  */\n+  return (chain_insn_order[INSN_UID(c1->insn)]\n+\t  - chain_insn_order[INSN_UID(c2->insn)]);\n+}\n+\n+/* Sort the insn chain according insn original order.  */\n+static int\n+chain_bb_compare (const void *v1p, const void *v2p)\n+{\n+  const struct insn_chain *c1 = *(struct insn_chain * const *)v1p;\n+  const struct insn_chain *c2 = *(struct insn_chain * const *)v2p;\n+  int diff;\n+\n+  diff = (basic_block_order_nums[c1->block]\n+\t  - basic_block_order_nums[c2->block]);\n+  if (diff)\n+    return diff;\n+  /* Keep the same order in BB scope.  */\n+  return (chain_insn_order[INSN_UID(c1->insn)]\n+\t  - chain_insn_order[INSN_UID(c2->insn)]);\n+}\n+\n+/* Sort the insn chain according to insn frequencies if\n+   FREQ_P or according to insn original order otherwise.  */\n+void\n+ira_sort_insn_chain (bool freq_p)\n+{\n+  struct insn_chain *chain, **chain_arr;\n+  basic_block bb;\n+  int i, n;\n+  \n+  chain_insn_order = (int *) ira_allocate (get_max_uid () * sizeof (int));\n+  for (n = 0, chain = reload_insn_chain; chain != 0; chain = chain->next)\n+    {\n+      chain_insn_order[INSN_UID (chain->insn)] = n;\n+      n++;\n+    }\n+  if (n <= 1)\n+    return;\n+  chain_arr\n+    = (struct insn_chain **) ira_allocate (n * sizeof (struct insn_chain *));\n+  basic_block_order_nums\n+    = (int *) ira_allocate (sizeof (int) * last_basic_block);\n+  n = 0;\n+  FOR_EACH_BB (bb)\n+    {\n+      basic_block_order_nums[bb->index] = n++;\n+    }\n+  for (n = 0, chain = reload_insn_chain; chain != 0; chain = chain->next)\n+    chain_arr[n++] = chain;\n+  qsort (chain_arr, n, sizeof (struct insn_chain *),\n+\t freq_p ? chain_freq_compare : chain_bb_compare);\n+  ira_free (chain_insn_order);\n+  for (i = 1; i < n - 1; i++)\n+    {\n+      chain_arr[i]->next = chain_arr[i + 1];\n+      chain_arr[i]->prev = chain_arr[i - 1];\n+    }\n+  chain_arr[i]->next = NULL;\n+  chain_arr[i]->prev = chain_arr[i - 1];\n+  reload_insn_chain = chain_arr[0];\n+  reload_insn_chain->prev = NULL;\n+  reload_insn_chain->next = chain_arr[1];\n+  ira_free (basic_block_order_nums);\n+  ira_free (chain_arr);\n+}\n+\n+\f\n+\n+/* All natural loops.  */\n+struct loops ira_loops;\n+\n+/* This is the main entry of IRA.  */\n+static void\n+ira (FILE *f)\n+{\n+  int overall_cost_before, allocated_reg_info_size;\n+  bool loops_p;\n+  int max_regno_before_ira, ira_max_point_before_emit;\n+  int rebuild_p;\n+  int saved_flag_ira_algorithm;\n+  basic_block bb;\n+\n+  timevar_push (TV_IRA);\n+\n+  if (flag_ira_verbose < 10)\n+    {\n+      internal_flag_ira_verbose = flag_ira_verbose;\n+      ira_dump_file = f;\n+    }\n+  else\n+    {\n+      internal_flag_ira_verbose = flag_ira_verbose - 10;\n+      ira_dump_file = stderr;\n+    }\n+\n+  setup_prohibited_mode_move_regs ();\n+\n+  df_note_add_problem ();\n+\n+  if (optimize == 1)\n+    {\n+      df_live_add_problem ();\n+      df_live_set_all_dirty ();\n+    }\n+#ifdef ENABLE_CHECKING\n+  df->changeable_flags |= DF_VERIFY_SCHEDULED;\n+#endif\n+  df_analyze ();\n+  df_clear_flags (DF_NO_INSN_RESCAN);\n+  regstat_init_n_sets_and_refs ();\n+  regstat_compute_ri ();\n+\n+  /* If we are not optimizing, then this is the only place before\n+     register allocation where dataflow is done.  And that is needed\n+     to generate these warnings.  */\n+  if (warn_clobbered)\n+    generate_setjmp_warnings ();\n+\n+  rebuild_p = update_equiv_regs ();\n+\n+#ifndef IRA_NO_OBSTACK\n+  gcc_obstack_init (&ira_obstack);\n+#endif\n+  bitmap_obstack_initialize (&ira_bitmap_obstack);\n+  if (optimize)\n+    {      \n+      max_regno = max_reg_num ();\n+      ira_reg_equiv_len = max_regno;\n+      ira_reg_equiv_invariant_p\n+\t= (bool *) ira_allocate (max_regno * sizeof (bool));\n+      memset (ira_reg_equiv_invariant_p, 0, max_regno * sizeof (bool));\n+      ira_reg_equiv_const = (rtx *) ira_allocate (max_regno * sizeof (rtx));\n+      memset (ira_reg_equiv_const, 0, max_regno * sizeof (rtx));\n+      find_reg_equiv_invariant_const ();\n+      if (rebuild_p)\n+\t{\n+\t  timevar_push (TV_JUMP);\n+\t  rebuild_jump_labels (get_insns ());\n+\t  purge_all_dead_edges ();\n+\t  timevar_pop (TV_JUMP);\n+\t}\n+    }\n+\n+  max_regno_before_ira = allocated_reg_info_size = max_reg_num ();\n+  allocate_reg_info ();\n+  setup_eliminable_regset ();\n+      \n+  ira_overall_cost = ira_reg_cost = ira_mem_cost = 0;\n+  ira_load_cost = ira_store_cost = ira_shuffle_cost = 0;\n+  ira_move_loops_num = ira_additional_jumps_num = 0;\n+  \n+  ira_assert (current_loops == NULL);\n+  flow_loops_find (&ira_loops);\n+  current_loops = &ira_loops;\n+  saved_flag_ira_algorithm = flag_ira_algorithm;\n+  if (optimize && number_of_loops () > (unsigned) IRA_MAX_LOOPS_NUM)\n+    flag_ira_algorithm = IRA_ALGORITHM_CB;\n+      \n+  if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL)\n+    fprintf (ira_dump_file, \"Building IRA IR\\n\");\n+  loops_p = ira_build (optimize\n+\t\t       && (flag_ira_algorithm == IRA_ALGORITHM_REGIONAL\n+\t\t\t   || flag_ira_algorithm == IRA_ALGORITHM_MIXED));\n+  if (optimize)\n+    ira_color ();\n+  else\n+    ira_fast_allocation ();\n+      \n+  ira_max_point_before_emit = ira_max_point;\n+      \n+  ira_emit (loops_p);\n+  \n+  if (optimize)\n+    {\n+      max_regno = max_reg_num ();\n+      \n+      if (! loops_p)\n+\tira_initiate_assign ();\n+      else\n+\t{\n+\t  expand_reg_info (allocated_reg_info_size);\n+\t  setup_preferred_alternate_classes_for_new_pseudos\n+\t    (allocated_reg_info_size);\n+\t  allocated_reg_info_size = max_regno;\n+\t  \n+\t  if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL)\n+\t    fprintf (ira_dump_file, \"Flattening IR\\n\");\n+\t  ira_flattening (max_regno_before_ira, ira_max_point_before_emit);\n+\t  /* New insns were generated: add notes and recalculate live\n+\t     info.  */\n+\t  df_analyze ();\n+\t  \n+\t  flow_loops_find (&ira_loops);\n+\t  current_loops = &ira_loops;\n+\n+\t  setup_allocno_assignment_flags ();\n+\t  ira_initiate_assign ();\n+\t  ira_reassign_conflict_allocnos (max_regno);\n+\t}\n+    }\n+\n+  setup_reg_renumber ();\n+  \n+  calculate_allocation_cost ();\n+  \n+#ifdef ENABLE_IRA_CHECKING\n+  if (optimize)\n+    check_allocation ();\n+#endif\n+      \n+  delete_trivially_dead_insns (get_insns (), max_reg_num ());\n+  max_regno = max_reg_num ();\n+  \n+  /* Determine if the current function is a leaf before running IRA\n+     since this can impact optimizations done by the prologue and\n+     epilogue thus changing register elimination offsets.  */\n+  current_function_is_leaf = leaf_function_p ();\n+  \n+  /* And the reg_equiv_memory_loc array.  */\n+  VEC_safe_grow (rtx, gc, reg_equiv_memory_loc_vec, max_regno);\n+  memset (VEC_address (rtx, reg_equiv_memory_loc_vec), 0,\n+\t  sizeof (rtx) * max_regno);\n+  reg_equiv_memory_loc = VEC_address (rtx, reg_equiv_memory_loc_vec);\n+\n+  if (max_regno != max_regno_before_ira)\n+    {\n+      regstat_free_n_sets_and_refs ();\n+      regstat_free_ri ();\n+      regstat_init_n_sets_and_refs ();\n+      regstat_compute_ri ();\n+    }\n+\n+  allocate_initial_values (reg_equiv_memory_loc);\n+\n+  overall_cost_before = ira_overall_cost;\n+  if (optimize)\n+    {\n+      fix_reg_equiv_init ();\n+      \n+#ifdef ENABLE_IRA_CHECKING\n+      print_redundant_copies ();\n+#endif\n+\n+      ira_spilled_reg_stack_slots_num = 0;\n+      ira_spilled_reg_stack_slots\n+\t= ((struct ira_spilled_reg_stack_slot *)\n+\t   ira_allocate (max_regno\n+\t\t\t * sizeof (struct ira_spilled_reg_stack_slot)));\n+      memset (ira_spilled_reg_stack_slots, 0,\n+\t      max_regno * sizeof (struct ira_spilled_reg_stack_slot));\n+    }\n+  \n+  timevar_pop (TV_IRA);\n+\n+  timevar_push (TV_RELOAD);\n+  df_set_flags (DF_NO_INSN_RESCAN);\n+  build_insn_chain ();\n+\n+  if (optimize)\n+    ira_sort_insn_chain (true);\n+\n+  reload_completed = !reload (get_insns (), optimize > 0);\n+\n+  timevar_pop (TV_RELOAD);\n+\n+  timevar_push (TV_IRA);\n+\n+  if (optimize)\n+    {\n+      ira_free (ira_spilled_reg_stack_slots);\n+      \n+      ira_finish_assign ();\n+      \n+    }  \n+  if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL\n+      && overall_cost_before != ira_overall_cost)\n+    fprintf (ira_dump_file, \"+++Overall after reload %d\\n\", ira_overall_cost);\n+  ira_destroy ();\n+  \n+  flow_loops_free (&ira_loops);\n+  free_dominance_info (CDI_DOMINATORS);\n+  FOR_ALL_BB (bb)\n+    bb->loop_father = NULL;\n+  current_loops = NULL;\n+\n+  flag_ira_algorithm = saved_flag_ira_algorithm;\n+\n+  regstat_free_ri ();\n+  regstat_free_n_sets_and_refs ();\n+      \n+  if (optimize)\n+    {\n+      cleanup_cfg (CLEANUP_EXPENSIVE);\n+      \n+      ira_free (ira_reg_equiv_invariant_p);\n+      ira_free (ira_reg_equiv_const);\n+    }\n+\n+  bitmap_obstack_release (&ira_bitmap_obstack);\n+#ifndef IRA_NO_OBSTACK\n+  obstack_free (&ira_obstack, NULL);\n+#endif\n+\n+  /* The code after the reload has changed so much that at this point\n+     we might as well just rescan everything.  Not that\n+     df_rescan_all_insns is not going to help here because it does not\n+     touch the artificial uses and defs.  */\n+  df_finish_pass (true);\n+  if (optimize > 1)\n+    df_live_add_problem ();\n+  df_scan_alloc (NULL);\n+  df_scan_blocks ();\n+\n+  if (optimize)\n+    df_analyze ();\n+\n+  timevar_pop (TV_IRA);\n+}\n+\n+\f\n+\n+static bool\n+gate_ira (void)\n+{\n+  return flag_ira != 0;\n+}\n+\n+/* Run the integrated register allocator.  */\n+static unsigned int\n+rest_of_handle_ira (void)\n+{\n+  ira (dump_file);\n+  return 0;\n+}\n+\n+struct rtl_opt_pass pass_ira =\n+{\n+ {\n+  RTL_PASS,\n+  \"ira\",                                /* name */\n+  gate_ira,                             /* gate */\n+  rest_of_handle_ira,\t\t        /* execute */\n+  NULL,                                 /* sub */\n+  NULL,                                 /* next */\n+  0,                                    /* static_pass_number */\n+  0,\t\t                        /* tv_id */\n+  0,                                    /* properties_required */\n+  0,                                    /* properties_provided */\n+  0,                                    /* properties_destroyed */\n+  0,                                    /* todo_flags_start */\n+  TODO_dump_func |\n+  TODO_ggc_collect                      /* todo_flags_finish */\n+ }\n+};"}, {"sha": "8a90785d5c6e3cdbd979914da64830e17dbf6f6f", "filename": "gcc/ira.h", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fira.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -0,0 +1,37 @@\n+/* Communication between the Integrated Register Allocator (IRA) and\n+   the rest of the compiler.\n+   Copyright (C) 2006, 2007, 2008\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+extern void ira_init_once (void);\n+extern void ira_init (void);\n+extern void ira_finish_once (void);\n+extern rtx ira_eliminate_regs (rtx, enum machine_mode);\n+extern void ira_sort_insn_chain (bool);\n+\n+extern void ira_sort_regnos_for_alter_reg (int *, int, unsigned int *);\n+extern void ira_mark_allocation_change (int);\n+extern void ira_mark_memory_move_deletion (int, int);\n+extern bool ira_reassign_pseudos (int *, int, HARD_REG_SET, HARD_REG_SET *,\n+\t\t\t\t  HARD_REG_SET *, bitmap);\n+extern rtx ira_reuse_stack_slot (int, unsigned int, unsigned int);\n+extern void ira_mark_new_stack_slot (rtx, int, unsigned int);\n+extern bool ira_better_spill_reload_regno_p (int *, int *, rtx, rtx, rtx);\n+"}, {"sha": "e7bbcdaa16926075de89097f6aac1bfa2e31f91d", "filename": "gcc/local-alloc.c", "status": "modified", "additions": 14, "deletions": 4, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Flocal-alloc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Flocal-alloc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flocal-alloc.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -298,7 +298,6 @@ static int equiv_init_movable_p (rtx, int);\n static int contains_replace_regs (rtx);\n static int memref_referenced_p (rtx, rtx);\n static int memref_used_between_p (rtx, rtx, rtx);\n-static void update_equiv_regs (void);\n static void no_equiv (rtx, const_rtx, void *);\n static void block_alloc (int);\n static int qty_sugg_compare (int, int);\n@@ -795,9 +794,11 @@ memref_used_between_p (rtx memref, rtx start, rtx end)\n    into the using insn.  If it succeeds, we can eliminate the register\n    completely.\n \n-   Initialize the REG_EQUIV_INIT array of initializing insns.  */\n+   Initialize the REG_EQUIV_INIT array of initializing insns.\n \n-static void\n+   Return non-zero if jump label rebuilding should be done.  */\n+\n+int\n update_equiv_regs (void)\n {\n   rtx insn;\n@@ -1183,6 +1184,8 @@ update_equiv_regs (void)\n \t\t      new_insn = emit_insn_before (PATTERN (equiv_insn), insn);\n \t\t      REG_NOTES (new_insn) = REG_NOTES (equiv_insn);\n \t\t      REG_NOTES (equiv_insn) = 0;\n+\t\t      /* Rescan it to process the notes.  */\n+\t\t      df_insn_rescan (new_insn);\n \n \t\t      /* Make sure this insn is recognized before\n \t\t\t reload begins, otherwise\n@@ -1227,6 +1230,7 @@ update_equiv_regs (void)\n \n   end_alias_analysis ();\n   free (reg_equiv);\n+  return recorded_label_ref;\n }\n \n /* Mark REG as having no known equivalence.\n@@ -2442,6 +2446,12 @@ find_stack_regs (void)\n }\n #endif\n \n+static bool\n+gate_handle_local_alloc (void)\n+{\n+  return ! flag_ira;\n+}\n+\n /* Run old register allocator.  Return TRUE if we must exit\n    rest_of_compilation upon return.  */\n static unsigned int\n@@ -2517,7 +2527,7 @@ struct rtl_opt_pass pass_local_alloc =\n  {\n   RTL_PASS,\n   \"lreg\",                               /* name */\n-  NULL,                                 /* gate */\n+  gate_handle_local_alloc,              /* gate */\n   rest_of_handle_local_alloc,           /* execute */\n   NULL,                                 /* sub */\n   NULL,                                 /* next */"}, {"sha": "badc0a1a6cd8589b398182be56f2880a6da873ee", "filename": "gcc/opts.c", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fopts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fopts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fopts.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -878,6 +878,11 @@ decode_options (unsigned int argc, const char **argv)\n       flag_section_anchors = 0;\n     }\n \n+#ifdef IRA_COVER_CLASSES\n+  /* Use IRA if it is implemented for the target.  */\n+  flag_ira = 1;\n+#endif\n+\n   /* Originally we just set the variables if a particular optimization level,\n      but with the advent of being able to change the optimization level for a\n      function, we need to reset optimizations.  */\n@@ -1119,6 +1124,14 @@ decode_options (unsigned int argc, const char **argv)\n       flag_reorder_blocks = 1;\n     }\n \n+#ifndef IRA_COVER_CLASSES\n+  if (flag_ira)\n+    {\n+      inform (\"-fira does not work on this architecture\");\n+      flag_ira = 0;\n+    }\n+#endif\n+\n   /* Save the current optimization options if this is the first call.  */\n   if (first_time_p)\n     {\n@@ -1970,6 +1983,21 @@ common_handle_option (size_t scode, const char *arg, int value,\n \twarning (0, \"unknown tls-model \\\"%s\\\"\", arg);\n       break;\n \n+    case OPT_fira_algorithm_:\n+      if (!strcmp (arg, \"regional\"))\n+\tflag_ira_algorithm = IRA_ALGORITHM_REGIONAL;\n+      else if (!strcmp (arg, \"CB\"))\n+\tflag_ira_algorithm = IRA_ALGORITHM_CB;\n+      else if (!strcmp (arg, \"mixed\"))\n+\tflag_ira_algorithm = IRA_ALGORITHM_MIXED;\n+      else\n+\twarning (0, \"unknown ira algorithm \\\"%s\\\"\", arg);\n+      break;\n+\n+    case OPT_fira_verbose_:\n+      flag_ira_verbose = value;\n+      break;\n+\n     case OPT_ftracer:\n       flag_tracer_set = true;\n       break;"}, {"sha": "c71b2b6500e9dba1453f50f7ee978502dfce7196", "filename": "gcc/params.def", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fparams.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fparams.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fparams.def?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -714,6 +714,11 @@ DEFPARAM (PARAM_DF_DOUBLE_QUEUE_THRESHOLD_FACTOR,\n \t  \"Multiplier used for determining the double-queueing threshold\",\n \t  2, 0, 0)\n \n+DEFPARAM (PARAM_IRA_MAX_LOOPS_NUM,\n+\t  \"ira-max-loops-num\",\n+\t  \"max loops number for regional RA\",\n+\t  50, 0, 0)\n+\n /* Switch initialization conversion will refuse to create arrays that are\n    bigger than this parameter times the number of switch branches.  */\n "}, {"sha": "8147b60984847d5dae67ab10d5bb988ffd82313a", "filename": "gcc/params.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fparams.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fparams.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fparams.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -167,6 +167,8 @@ typedef enum compiler_param\n   PARAM_VALUE (PARAM_L2_CACHE_SIZE)\n #define USE_CANONICAL_TYPES \\\n   PARAM_VALUE (PARAM_USE_CANONICAL_TYPES)\n+#define IRA_MAX_LOOPS_NUM \\\n+  PARAM_VALUE (PARAM_IRA_MAX_LOOPS_NUM)\n #define SWITCH_CONVERSION_BRANCH_RATIO \\\n   PARAM_VALUE (PARAM_SWITCH_CONVERSION_BRANCH_RATIO)\n #endif /* ! GCC_PARAMS_H */"}, {"sha": "f45507f27e0a8bed3b1c700bd043f8f681c92a82", "filename": "gcc/passes.c", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fpasses.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fpasses.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fpasses.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -767,6 +767,7 @@ init_optimization_passes (void)\n       NEXT_PASS (pass_subregs_of_mode_init);\n       NEXT_PASS (pass_local_alloc);\n       NEXT_PASS (pass_global_alloc);\n+      NEXT_PASS (pass_ira);\n       NEXT_PASS (pass_subregs_of_mode_finish);\n       NEXT_PASS (pass_postreload);\n \t{"}, {"sha": "7659bab435ac34536c061945bca860858f69ecd9", "filename": "gcc/postreload.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fpostreload.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fpostreload.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fpostreload.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1565,7 +1565,7 @@ move2add_note_store (rtx dst, const_rtx set, void *data ATTRIBUTE_UNUSED)\n static bool\n gate_handle_postreload (void)\n {\n-  return (optimize > 0);\n+  return (optimize > 0 && reload_completed);\n }\n \n "}, {"sha": "b12d41685066b5795df5607592df2ce5fc932196", "filename": "gcc/regclass.c", "status": "modified", "additions": 48, "deletions": 8, "changes": 56, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fregclass.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fregclass.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fregclass.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -178,7 +178,7 @@ static enum reg_class reg_class_superclasses[N_REG_CLASSES][N_REG_CLASSES];\n \n /* For each reg class, table listing all the classes contained in it.  */\n \n-static enum reg_class reg_class_subclasses[N_REG_CLASSES][N_REG_CLASSES];\n+enum reg_class reg_class_subclasses[N_REG_CLASSES][N_REG_CLASSES];\n \n /* For each pair of reg classes,\n    a largest reg class contained in their union.  */\n@@ -211,24 +211,22 @@ bool have_regs_of_mode [MAX_MACHINE_MODE];\n \n /* 1 if class does contain register of given mode.  */\n \n-static char contains_reg_of_mode [N_REG_CLASSES] [MAX_MACHINE_MODE];\n-\n-typedef unsigned short move_table[N_REG_CLASSES];\n+char contains_reg_of_mode [N_REG_CLASSES] [MAX_MACHINE_MODE];\n \n /* Maximum cost of moving from a register in one class to a register in\n    another class.  Based on REGISTER_MOVE_COST.  */\n \n-static move_table *move_cost[MAX_MACHINE_MODE];\n+move_table *move_cost[MAX_MACHINE_MODE];\n \n /* Similar, but here we don't have to move if the first index is a subset\n    of the second so in that case the cost is zero.  */\n \n-static move_table *may_move_in_cost[MAX_MACHINE_MODE];\n+move_table *may_move_in_cost[MAX_MACHINE_MODE];\n \n /* Similar, but here we don't have to move if the first index is a superset\n    of the second so in that case the cost is zero.  */\n \n-static move_table *may_move_out_cost[MAX_MACHINE_MODE];\n+move_table *may_move_out_cost[MAX_MACHINE_MODE];\n \n /* Keep track of the last mode we initialized move costs for.  */\n static int last_mode_for_init_move_cost;\n@@ -313,7 +311,7 @@ init_reg_sets (void)\n \n /* Initialize may_move_cost and friends for mode M.  */\n \n-static void\n+void\n init_move_cost (enum machine_mode m)\n {\n   static unsigned short last_move_cost[N_REG_CLASSES][N_REG_CLASSES];\n@@ -1024,6 +1022,7 @@ reg_preferred_class (int regno)\n {\n   if (reg_pref == 0)\n     return GENERAL_REGS;\n+\n   return (enum reg_class) reg_pref[regno].prefclass;\n }\n \n@@ -2283,6 +2282,32 @@ auto_inc_dec_reg_p (rtx reg, enum machine_mode mode)\n }\n #endif\n \f\n+\n+/* Allocate space for reg info.  */\n+void\n+allocate_reg_info (void)\n+{\n+  int size = max_reg_num ();\n+\n+  gcc_assert (! reg_pref && ! reg_renumber);\n+  reg_renumber = XNEWVEC (short, size);\n+  reg_pref = XCNEWVEC (struct reg_pref, size);\n+  memset (reg_renumber, -1, size * sizeof (short));\n+}\n+\n+\n+/* Resize reg info. The new elements will be uninitialized.  */\n+void\n+resize_reg_info (void)\n+{\n+  int size = max_reg_num ();\n+\n+  gcc_assert (reg_pref && reg_renumber);\n+  reg_renumber = XRESIZEVEC (short, reg_renumber, size);\n+  reg_pref = XRESIZEVEC (struct reg_pref, reg_pref, size);\n+}\n+\n+\n /* Free up the space allocated by allocate_reg_info.  */\n void\n free_reg_info (void)\n@@ -2300,6 +2325,21 @@ free_reg_info (void)\n     }\n }\n \n+\n+\f\n+\n+/* Set up preferred and alternate classes for REGNO as PREFCLASS and\n+   ALTCLASS.  */\n+void\n+setup_reg_classes (int regno,\n+\t\t   enum reg_class prefclass, enum reg_class altclass)\n+{\n+  if (reg_pref == NULL)\n+    return;\n+  reg_pref[regno].prefclass = prefclass;\n+  reg_pref[regno].altclass = altclass;\n+}\n+\n \f\n /* This is the `regscan' pass of the compiler, run just before cse and\n    again just before loop.  It finds the first and last use of each"}, {"sha": "e25dbec7fe99301de4baf6a121674cedabd4ab69", "filename": "gcc/regmove.c", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fregmove.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fregmove.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fregmove.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1117,7 +1117,8 @@ regmove_optimize (rtx f, int nregs)\n \n   for (pass = 0; pass <= 2; pass++)\n     {\n-      if (! flag_regmove && pass >= flag_expensive_optimizations)\n+      /* We need fewer optimizations for IRA.  */\n+      if ((! flag_regmove || flag_ira) && pass >= flag_expensive_optimizations)\n \tgoto done;\n \n       if (dump_file)\n@@ -1165,7 +1166,9 @@ regmove_optimize (rtx f, int nregs)\n \t\t    }\n \t\t}\n \t    }\n-\t  if (! flag_regmove)\n+\n+\t  /* All optimizations important for IRA have been done.  */\n+\t  if (! flag_regmove || flag_ira)\n \t    continue;\n \n \t  if (! find_matches (insn, &match))"}, {"sha": "00be997695fc5189866dd9b40c33dbcd7e808815", "filename": "gcc/regs.h", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fregs.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Fregs.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fregs.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -262,9 +262,27 @@ extern int caller_save_needed;\n #define HARD_REGNO_CALL_PART_CLOBBERED(REGNO, MODE) 0\n #endif\n \n+/* 1 if the corresponding class does contain register of given\n+   mode.  */\n+extern char contains_reg_of_mode [N_REG_CLASSES] [MAX_MACHINE_MODE];\n+\n+typedef unsigned short move_table[N_REG_CLASSES];\n+\n+/* Maximum cost of moving from a register in one class to a register\n+   in another class.  */\n+extern move_table *move_cost[MAX_MACHINE_MODE];\n+\n /* Specify number of hard registers given machine mode occupy.  */\n extern unsigned char hard_regno_nregs[FIRST_PSEUDO_REGISTER][MAX_MACHINE_MODE];\n \n+/* Similar, but here we don't have to move if the first index is a\n+   subset of the second so in that case the cost is zero.  */\n+extern move_table *may_move_in_cost[MAX_MACHINE_MODE];\n+\n+/* Similar, but here we don't have to move if the first index is a\n+   superset of the second so in that case the cost is zero.  */\n+extern move_table *may_move_out_cost[MAX_MACHINE_MODE];\n+\n /* Return an exclusive upper bound on the registers occupied by hard\n    register (reg:MODE REGNO).  */\n "}, {"sha": "816370079954747df6e30655fda3db0785c64cd7", "filename": "gcc/reload.c", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Freload.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Freload.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Freload.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1549,8 +1549,10 @@ push_reload (rtx in, rtx out, rtx *inloc, rtx *outloc,\n \t    && reg_mentioned_p (XEXP (note, 0), in)\n \t    /* Check that a former pseudo is valid; see find_dummy_reload.  */\n \t    && (ORIGINAL_REGNO (XEXP (note, 0)) < FIRST_PSEUDO_REGISTER\n-\t\t|| (!bitmap_bit_p (DF_LIVE_OUT (ENTRY_BLOCK_PTR),\n-\t\t\t\t   ORIGINAL_REGNO (XEXP (note, 0)))\n+\t\t|| (! bitmap_bit_p (flag_ira\n+\t\t\t\t    ? DF_LR_OUT (ENTRY_BLOCK_PTR)\n+\t\t\t\t    : DF_LIVE_OUT (ENTRY_BLOCK_PTR),\n+\t\t\t\t    ORIGINAL_REGNO (XEXP (note, 0)))\n \t\t    && hard_regno_nregs[regno][GET_MODE (XEXP (note, 0))] == 1))\n \t    && ! refers_to_regno_for_reload_p (regno,\n \t\t\t\t\t       end_hard_regno (rel_mode,\n@@ -2027,7 +2029,9 @@ find_dummy_reload (rtx real_in, rtx real_out, rtx *inloc, rtx *outloc,\n \t     can ignore the conflict).  We must never introduce writes\n \t     to such hardregs, as they would clobber the other live\n \t     pseudo.  See PR 20973.  */\n-          || (!bitmap_bit_p (DF_LIVE_OUT (ENTRY_BLOCK_PTR),\n+          || (!bitmap_bit_p (flag_ira\n+\t\t\t     ? DF_LR_OUT (ENTRY_BLOCK_PTR)\n+\t\t\t     : DF_LIVE_OUT (ENTRY_BLOCK_PTR),\n \t\t\t     ORIGINAL_REGNO (in))\n \t      /* Similarly, only do this if we can be sure that the death\n \t\t note is still valid.  global can assign some hardreg to"}, {"sha": "17d8a3e04b2b1bf6400638d34eccb17bee9469f3", "filename": "gcc/reload.h", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Freload.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Freload.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Freload.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1,6 +1,6 @@\n-/* Communication between reload.c and reload1.c.\n-   Copyright (C) 1987, 1991, 1992, 1993, 1994, 1995, 1997, 1998,\n-   1999, 2000, 2001, 2003, 2004, 2007 Free Software Foundation, Inc.\n+/* Communication between reload.c, reload1.c and the rest of compiler.\n+   Copyright (C) 1987, 1991, 1992, 1993, 1994, 1995, 1997, 1998, 1999,\n+   2000, 2001, 2003, 2004, 2007, 2008 Free Software Foundation, Inc.\n \n This file is part of GCC.\n \n@@ -209,8 +209,9 @@ struct insn_chain\n   int block;\n   /* The rtx of the insn.  */\n   rtx insn;\n-  /* Register life information: record all live hard registers, and all\n-     live pseudos that have a hard register.  */\n+  /* Register life information: record all live hard registers, and\n+     all live pseudos that have a hard register.  This set also\n+     contains pseudos spilled by IRA.  */\n   regset_head live_throughout;\n   regset_head dead_or_set;\n "}, {"sha": "bb0d423e91bddafdb053c045feb7be685c69ed1d", "filename": "gcc/reload1.c", "status": "modified", "additions": 357, "deletions": 75, "changes": 432, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Freload1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Freload1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Freload1.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -44,6 +44,7 @@ along with GCC; see the file COPYING3.  If not see\n #include \"toplev.h\"\n #include \"except.h\"\n #include \"tree.h\"\n+#include \"ira.h\"\n #include \"df.h\"\n #include \"target.h\"\n #include \"dse.h\"\n@@ -257,6 +258,9 @@ static unsigned int spill_stack_slot_width[FIRST_PSEUDO_REGISTER];\n /* Record which pseudos needed to be spilled.  */\n static regset_head spilled_pseudos;\n \n+/* Record which pseudos changed their allocation in finish_spills.  */\n+static regset_head changed_allocation_pseudos;\n+\n /* Used for communication between order_regs_for_reload and count_pseudo.\n    Used to avoid counting one pseudo twice.  */\n static regset_head pseudos_counted;\n@@ -389,7 +393,7 @@ static void delete_caller_save_insns (void);\n static void spill_failure (rtx, enum reg_class);\n static void count_spilled_pseudo (int, int, int);\n static void delete_dead_insn (rtx);\n-static void alter_reg (int, int);\n+static void alter_reg (int, int, bool);\n static void set_label_offsets (rtx, rtx, int);\n static void check_eliminable_occurrences (rtx);\n static void elimination_effects (rtx, enum machine_mode);\n@@ -443,6 +447,8 @@ static rtx inc_for_reload (rtx, rtx, rtx, int);\n static void add_auto_inc_notes (rtx, rtx);\n #endif\n static void copy_eh_notes (rtx, rtx);\n+static void substitute (rtx *, const_rtx, rtx);\n+static bool gen_reload_chain_without_interm_reg_p (int, int);\n static int reloads_conflict (int, int);\n static rtx gen_reload (rtx, rtx, int, enum reload_type);\n static rtx emit_insn_if_valid_for_reload (rtx);\n@@ -501,6 +507,7 @@ init_reload (void)\n   reload_startobj = XOBNEWVAR (&reload_obstack, char, 0);\n \n   INIT_REG_SET (&spilled_pseudos);\n+  INIT_REG_SET (&changed_allocation_pseudos);\n   INIT_REG_SET (&pseudos_counted);\n }\n \n@@ -546,11 +553,11 @@ compute_use_by_pseudos (HARD_REG_SET *to, regset from)\n \n       if (r < 0)\n \t{\n-\t  /* reload_combine uses the information from\n-\t     DF_LIVE_IN (BASIC_BLOCK), which might still\n-\t     contain registers that have not actually been allocated\n-\t     since they have an equivalence.  */\n-\t  gcc_assert (reload_completed);\n+\t  /* reload_combine uses the information from DF_LIVE_IN,\n+\t     which might still contain registers that have not\n+\t     actually been allocated since they have an\n+\t     equivalence.  */\n+\t  gcc_assert ((flag_ira && optimize) || reload_completed);\n \t}\n       else\n \tadd_to_hard_reg_set (to, PSEUDO_REGNO_MODE (regno), r);\n@@ -684,6 +691,9 @@ static int something_needs_operands_changed;\n /* Nonzero means we couldn't get enough spill regs.  */\n static int failure;\n \n+/* Temporary array of pseudo-register number.  */\n+static int *temp_pseudo_reg_arr;\n+\n /* Main entry point for the reload pass.\n \n    FIRST is the first insn of the function being compiled.\n@@ -700,7 +710,7 @@ static int failure;\n int\n reload (rtx first, int global)\n {\n-  int i;\n+  int i, n;\n   rtx insn;\n   struct elim_table *ep;\n   basic_block bb;\n@@ -883,12 +893,21 @@ reload (rtx first, int global)\n   offsets_known_at = XNEWVEC (char, num_labels);\n   offsets_at = (HOST_WIDE_INT (*)[NUM_ELIMINABLE_REGS]) xmalloc (num_labels * NUM_ELIMINABLE_REGS * sizeof (HOST_WIDE_INT));\n \n-  /* Alter each pseudo-reg rtx to contain its hard reg number.\n-     Assign stack slots to the pseudos that lack hard regs or equivalents.\n+  /* Alter each pseudo-reg rtx to contain its hard reg number.  Assign\n+     stack slots to the pseudos that lack hard regs or equivalents.\n      Do not touch virtual registers.  */\n \n-  for (i = LAST_VIRTUAL_REGISTER + 1; i < max_regno; i++)\n-    alter_reg (i, -1);\n+  temp_pseudo_reg_arr = XNEWVEC (int, max_regno - LAST_VIRTUAL_REGISTER - 1);\n+  for (n = 0, i = LAST_VIRTUAL_REGISTER + 1; i < max_regno; i++)\n+    temp_pseudo_reg_arr[n++] = i;\n+  \n+  if (flag_ira && optimize)\n+    /* Ask IRA to order pseudo-registers for better stack slot\n+       sharing.  */\n+    ira_sort_regnos_for_alter_reg (temp_pseudo_reg_arr, n, reg_max_ref_width);\n+\n+  for (i = 0; i < n; i++)\n+    alter_reg (temp_pseudo_reg_arr[i], -1, false);\n \n   /* If we have some registers we think can be eliminated, scan all insns to\n      see if there is an insn that sets one of these registers to something\n@@ -1002,7 +1021,7 @@ reload (rtx first, int global)\n \t\t   the loop.  */\n \t\treg_equiv_memory_loc[i] = 0;\n \t\treg_equiv_init[i] = 0;\n-\t\talter_reg (i, -1);\n+\t\talter_reg (i, -1, true);\n \t      }\n \t  }\n \n@@ -1036,7 +1055,12 @@ reload (rtx first, int global)\n \n       calculate_needs_all_insns (global);\n \n-      CLEAR_REG_SET (&spilled_pseudos);\n+      if (! flag_ira || ! optimize)\n+\t/* Don't do it for IRA.  We need this info because we don't\n+\t   change live_throughout and dead_or_set for chains when IRA\n+\t   is used.  */\n+\tCLEAR_REG_SET (&spilled_pseudos);\n+\n       did_spill = 0;\n \n       something_changed = 0;\n@@ -1094,6 +1118,11 @@ reload (rtx first, int global)\n       obstack_free (&reload_obstack, reload_firstobj);\n     }\n \n+  if (flag_ira && optimize)\n+    /* Restore the original insn chain order for correct reload work\n+       (e.g. for correct inheritance).  */\n+    ira_sort_insn_chain (false);\n+\n   /* If global-alloc was run, notify it of any register eliminations we have\n      done.  */\n   if (global)\n@@ -1163,6 +1192,7 @@ reload (rtx first, int global)\n      regs.  */\n  failed:\n \n+  CLEAR_REG_SET (&changed_allocation_pseudos);\n   CLEAR_REG_SET (&spilled_pseudos);\n   reload_in_progress = 0;\n \n@@ -1333,6 +1363,8 @@ reload (rtx first, int global)\n   VEC_free (rtx, gc, reg_equiv_memory_loc_vec);\n   reg_equiv_memory_loc = 0;\n \n+  free (temp_pseudo_reg_arr);\n+\n   if (offsets_known_at)\n     free (offsets_known_at);\n   if (offsets_at)\n@@ -1573,10 +1605,24 @@ calculate_needs_all_insns (int global)\n \t    {\n \t      rtx set = single_set (insn);\n \t      if (set\n-\t\t  && SET_SRC (set) == SET_DEST (set)\n-\t\t  && REG_P (SET_SRC (set))\n-\t\t  && REGNO (SET_SRC (set)) >= FIRST_PSEUDO_REGISTER)\n+\t\t  &&\n+\t\t  ((SET_SRC (set) == SET_DEST (set)\n+\t\t    && REG_P (SET_SRC (set))\n+\t\t    && REGNO (SET_SRC (set)) >= FIRST_PSEUDO_REGISTER)\n+\t\t   || (REG_P (SET_SRC (set)) && REG_P (SET_DEST (set))\n+\t\t       && reg_renumber[REGNO (SET_SRC (set))] < 0\n+\t\t       && reg_renumber[REGNO (SET_DEST (set))] < 0\n+\t\t       && reg_equiv_memory_loc[REGNO (SET_SRC (set))] != NULL\n+\t\t       && reg_equiv_memory_loc[REGNO (SET_DEST (set))] != NULL\n+\t\t       && rtx_equal_p (reg_equiv_memory_loc\n+\t\t\t\t       [REGNO (SET_SRC (set))],\n+\t\t\t\t       reg_equiv_memory_loc\n+\t\t\t\t       [REGNO (SET_DEST (set))]))))\n \t\t{\n+\t\t  if (flag_ira && optimize)\n+\t\t    /* Inform IRA about the insn deletion.  */\n+\t\t    ira_mark_memory_move_deletion (REGNO (SET_DEST (set)),\n+\t\t\t\t\t\t   REGNO (SET_SRC (set)));\n \t\t  delete_insn (insn);\n \t\t  /* Delete it from the reload chain.  */\n \t\t  if (chain->prev)\n@@ -1665,6 +1711,10 @@ static int spill_cost[FIRST_PSEUDO_REGISTER];\n    only the first hard reg for a multi-reg pseudo.  */\n static int spill_add_cost[FIRST_PSEUDO_REGISTER];\n \n+/* Map of hard regno to pseudo regno currently occupying the hard\n+   reg.  */\n+static int hard_regno_to_pseudo_regno[FIRST_PSEUDO_REGISTER];\n+\n /* Update the spill cost arrays, considering that pseudo REG is live.  */\n \n static void\n@@ -1675,18 +1725,23 @@ count_pseudo (int reg)\n   int nregs;\n \n   if (REGNO_REG_SET_P (&pseudos_counted, reg)\n-      || REGNO_REG_SET_P (&spilled_pseudos, reg))\n+      || REGNO_REG_SET_P (&spilled_pseudos, reg)\n+      /* Ignore spilled pseudo-registers which can be here only if IRA\n+\t is used.  */\n+      || (flag_ira && optimize && r < 0))\n     return;\n \n   SET_REGNO_REG_SET (&pseudos_counted, reg);\n \n   gcc_assert (r >= 0);\n \n   spill_add_cost[r] += freq;\n-\n   nregs = hard_regno_nregs[r][PSEUDO_REGNO_MODE (reg)];\n   while (nregs-- > 0)\n-    spill_cost[r + nregs] += freq;\n+    {\n+      hard_regno_to_pseudo_regno[r + nregs] = reg;\n+      spill_cost[r + nregs] += freq;\n+    }\n }\n \n /* Calculate the SPILL_COST and SPILL_ADD_COST arrays and determine the\n@@ -1704,6 +1759,8 @@ order_regs_for_reload (struct insn_chain *chain)\n \n   memset (spill_cost, 0, sizeof spill_cost);\n   memset (spill_add_cost, 0, sizeof spill_add_cost);\n+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+    hard_regno_to_pseudo_regno[i] = -1;\n \n   /* Count number of uses of each hard reg by pseudo regs allocated to it\n      and then order them by decreasing use.  First exclude hard registers\n@@ -1746,18 +1803,25 @@ static HARD_REG_SET used_spill_regs_local;\n static void\n count_spilled_pseudo (int spilled, int spilled_nregs, int reg)\n {\n+  int freq = REG_FREQ (reg);\n   int r = reg_renumber[reg];\n   int nregs = hard_regno_nregs[r][PSEUDO_REGNO_MODE (reg)];\n \n-  if (REGNO_REG_SET_P (&spilled_pseudos, reg)\n+  /* Ignore spilled pseudo-registers which can be here only if IRA is\n+     used.  */\n+  if ((flag_ira && optimize && r < 0)\n+      || REGNO_REG_SET_P (&spilled_pseudos, reg)\n       || spilled + spilled_nregs <= r || r + nregs <= spilled)\n     return;\n \n   SET_REGNO_REG_SET (&spilled_pseudos, reg);\n \n-  spill_add_cost[r] -= REG_FREQ (reg);\n+  spill_add_cost[r] -= freq;\n   while (nregs-- > 0)\n-    spill_cost[r + nregs] -= REG_FREQ (reg);\n+    {\n+      hard_regno_to_pseudo_regno[r + nregs] = -1;\n+      spill_cost[r + nregs] -= freq;\n+    }\n }\n \n /* Find reload register to use for reload number ORDER.  */\n@@ -1769,11 +1833,13 @@ find_reg (struct insn_chain *chain, int order)\n   struct reload *rl = rld + rnum;\n   int best_cost = INT_MAX;\n   int best_reg = -1;\n-  unsigned int i, j;\n+  unsigned int i, j, n;\n   int k;\n   HARD_REG_SET not_usable;\n   HARD_REG_SET used_by_other_reload;\n   reg_set_iterator rsi;\n+  static int regno_pseudo_regs[FIRST_PSEUDO_REGISTER];\n+  static int best_regno_pseudo_regs[FIRST_PSEUDO_REGISTER];\n \n   COPY_HARD_REG_SET (not_usable, bad_spill_regs);\n   IOR_HARD_REG_SET (not_usable, bad_spill_regs_global);\n@@ -1791,7 +1857,11 @@ find_reg (struct insn_chain *chain, int order)\n \n   for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n     {\n+#ifdef REG_ALLOC_ORDER\n+      unsigned int regno = reg_alloc_order[i];\n+#else\n       unsigned int regno = i;\n+#endif\n \n       if (! TEST_HARD_REG_BIT (not_usable, regno)\n \t  && ! TEST_HARD_REG_BIT (used_by_other_reload, regno)\n@@ -1810,6 +1880,38 @@ find_reg (struct insn_chain *chain, int order)\n \t    }\n \t  if (! ok)\n \t    continue;\n+\n+\t  if (flag_ira && optimize)\n+\t    {\n+\t      /* Ask IRA to find a better pseudo-register for\n+\t\t spilling.  */\n+\t      for (n = j = 0; j < this_nregs; j++)\n+\t\t{\n+\t\t  int r = hard_regno_to_pseudo_regno[regno + j];\n+\n+\t\t  if (r < 0)\n+\t\t    continue;\n+\t\t  if (n == 0 || regno_pseudo_regs[n - 1] != r)\n+\t\t    regno_pseudo_regs[n++] = r;\n+\t\t}\n+\t      regno_pseudo_regs[n++] = -1;\n+\t      if (best_reg < 0\n+\t\t  || ira_better_spill_reload_regno_p (regno_pseudo_regs,\n+\t\t\t\t\t\t      best_regno_pseudo_regs,\n+\t\t\t\t\t\t      rl->in, rl->out,\n+\t\t\t\t\t\t      chain->insn))\n+\t\t{\n+\t\t  best_reg = regno;\n+\t\t  for (j = 0;; j++)\n+\t\t    {\n+\t\t      best_regno_pseudo_regs[j] = regno_pseudo_regs[j];\n+\t\t      if (regno_pseudo_regs[j] < 0)\n+\t\t\tbreak;\n+\t\t    }\n+\t\t}\n+\t      continue;\n+\t    }\n+\n \t  if (rl->in && REG_P (rl->in) && REGNO (rl->in) == regno)\n \t    this_cost--;\n \t  if (rl->out && REG_P (rl->out) && REGNO (rl->out) == regno)\n@@ -1857,6 +1959,7 @@ find_reg (struct insn_chain *chain, int order)\n     {\n       gcc_assert (spill_cost[best_reg + i] == 0);\n       gcc_assert (spill_add_cost[best_reg + i] == 0);\n+      gcc_assert (hard_regno_to_pseudo_regno[best_reg + i] == -1);\n       SET_HARD_REG_BIT (used_spill_regs_local, best_reg + i);\n     }\n   return 1;\n@@ -2026,7 +2129,7 @@ delete_dead_insn (rtx insn)\n    can share one stack slot.  */\n \n static void\n-alter_reg (int i, int from_reg)\n+alter_reg (int i, int from_reg, bool dont_share_p)\n {\n   /* When outputting an inline function, this can happen\n      for a reg that isn't actually used.  */\n@@ -2059,7 +2162,15 @@ alter_reg (int i, int from_reg)\n       unsigned int total_size = MAX (inherent_size, reg_max_ref_width[i]);\n       unsigned int min_align = reg_max_ref_width[i] * BITS_PER_UNIT;\n       int adjust = 0;\n-\n+      bool shared_p = false;\n+\n+      if (flag_ira && optimize)\n+\t/* Mark the spill for IRA.  */\n+\tSET_REGNO_REG_SET (&spilled_pseudos, i);\n+      x = (dont_share_p || ! flag_ira || ! optimize\n+\t   ? NULL_RTX : ira_reuse_stack_slot (i, inherent_size, total_size));\n+      if (x)\n+\tshared_p = true;\n       /* Each pseudo reg has an inherent size which comes from its own mode,\n \t and a total size which provides room for paradoxical subregs\n \t which refer to the pseudo reg in wider modes.\n@@ -2068,7 +2179,7 @@ alter_reg (int i, int from_reg)\n \t enough inherent space and enough total space.\n \t Otherwise, we allocate a new slot, making sure that it has no less\n \t inherent space, and no less total space, then the previous slot.  */\n-      if (from_reg == -1)\n+      else if (from_reg == -1 || (! dont_share_p && flag_ira && optimize))\n \t{\n \t  alias_set_type alias_set = new_alias_set ();\n \n@@ -2086,6 +2197,10 @@ alter_reg (int i, int from_reg)\n \t  /* Nothing can alias this slot except this pseudo.  */\n \t  set_mem_alias_set (x, alias_set);\n \t  dse_record_singleton_alias_set (alias_set, mode);\n+\n+\t  if (! dont_share_p && flag_ira && optimize)\n+\t    /* Inform IRA about allocation a new stack slot.  */\n+\t    ira_mark_new_stack_slot (x, i, total_size);\n \t}\n \n       /* Reuse a stack slot if possible.  */\n@@ -2164,8 +2279,13 @@ alter_reg (int i, int from_reg)\n \n       /* If we have a decl for the original register, set it for the\n \t memory.  If this is a shared MEM, make a copy.  */\n-      if (REG_EXPR (regno_reg_rtx[i])\n-\t  && DECL_P (REG_EXPR (regno_reg_rtx[i])))\n+      if (shared_p)\n+\t{\n+\t  x = copy_rtx (x);\n+\t  set_mem_attrs_from_reg (x, regno_reg_rtx[i]);\n+\t}\n+      else if (REG_EXPR (regno_reg_rtx[i])\n+\t       && DECL_P (REG_EXPR (regno_reg_rtx[i])))\n \t{\n \t  rtx decl = DECL_RTL_IF_SET (REG_EXPR (regno_reg_rtx[i]));\n \n@@ -2441,7 +2561,7 @@ eliminate_regs_1 (rtx x, enum machine_mode mem_mode, rtx insn,\n \t  /* There exists at least one use of REGNO that cannot be\n \t     eliminated.  Prevent the defining insn from being deleted.  */\n \t  reg_equiv_init[regno] = NULL_RTX;\n-\t  alter_reg (regno, -1);\n+\t  alter_reg (regno, -1, true);\n \t}\n       return x;\n \n@@ -3817,18 +3937,22 @@ finish_spills (int global)\n       spill_reg_order[i] = -1;\n \n   EXECUTE_IF_SET_IN_REG_SET (&spilled_pseudos, FIRST_PSEUDO_REGISTER, i, rsi)\n-    {\n-      /* Record the current hard register the pseudo is allocated to in\n-\t pseudo_previous_regs so we avoid reallocating it to the same\n-\t hard reg in a later pass.  */\n-      gcc_assert (reg_renumber[i] >= 0);\n-\n-      SET_HARD_REG_BIT (pseudo_previous_regs[i], reg_renumber[i]);\n-      /* Mark it as no longer having a hard register home.  */\n-      reg_renumber[i] = -1;\n-      /* We will need to scan everything again.  */\n-      something_changed = 1;\n-    }\n+    if (! flag_ira || ! optimize || reg_renumber[i] >= 0)\n+      {\n+\t/* Record the current hard register the pseudo is allocated to\n+\t   in pseudo_previous_regs so we avoid reallocating it to the\n+\t   same hard reg in a later pass.  */\n+\tgcc_assert (reg_renumber[i] >= 0);\n+\t\n+\tSET_HARD_REG_BIT (pseudo_previous_regs[i], reg_renumber[i]);\n+\t/* Mark it as no longer having a hard register home.  */\n+\treg_renumber[i] = -1;\n+\tif (flag_ira && optimize)\n+\t  /* Inform IRA about the change.  */\n+\t  ira_mark_allocation_change (i);\n+\t/* We will need to scan everything again.  */\n+\tsomething_changed = 1;\n+      }\n \n   /* Retry global register allocation if possible.  */\n   if (global)\n@@ -3853,24 +3977,50 @@ finish_spills (int global)\n \t    }\n \t}\n \n-      /* Retry allocating the spilled pseudos.  For each reg, merge the\n-\t various reg sets that indicate which hard regs can't be used,\n-\t and call retry_global_alloc.\n-\t We change spill_pseudos here to only contain pseudos that did not\n-\t get a new hard register.  */\n-      for (i = FIRST_PSEUDO_REGISTER; i < (unsigned)max_regno; i++)\n-\tif (reg_old_renumber[i] != reg_renumber[i])\n-\t  {\n-\t    HARD_REG_SET forbidden;\n-\t    COPY_HARD_REG_SET (forbidden, bad_spill_regs_global);\n-\t    IOR_HARD_REG_SET (forbidden, pseudo_forbidden_regs[i]);\n-\t    IOR_HARD_REG_SET (forbidden, pseudo_previous_regs[i]);\n-\t    retry_global_alloc (i, forbidden);\n-\t    if (reg_renumber[i] >= 0)\n-\t      CLEAR_REGNO_REG_SET (&spilled_pseudos, i);\n-\t  }\n+      if (! flag_ira || ! optimize)\n+\t{\n+\t  /* Retry allocating the spilled pseudos.  For each reg,\n+\t     merge the various reg sets that indicate which hard regs\n+\t     can't be used, and call retry_global_alloc.  We change\n+\t     spill_pseudos here to only contain pseudos that did not\n+\t     get a new hard register.  */\n+\t  for (i = FIRST_PSEUDO_REGISTER; i < (unsigned)max_regno; i++)\n+\t    if (reg_old_renumber[i] != reg_renumber[i])\n+\t      {\n+\t\tHARD_REG_SET forbidden;\n+\t\t\n+\t\tCOPY_HARD_REG_SET (forbidden, bad_spill_regs_global);\n+\t\tIOR_HARD_REG_SET (forbidden, pseudo_forbidden_regs[i]);\n+\t\tIOR_HARD_REG_SET (forbidden, pseudo_previous_regs[i]);\n+\t\tretry_global_alloc (i, forbidden);\n+\t\tif (reg_renumber[i] >= 0)\n+\t\t  CLEAR_REGNO_REG_SET (&spilled_pseudos, i);\n+\t      }\n+\t}\n+      else\n+\t{\n+\t  /* Retry allocating the pseudos spilled in IRA and the\n+\t     reload.  For each reg, merge the various reg sets that\n+\t     indicate which hard regs can't be used, and call\n+\t     ira_reassign_pseudos.  */\n+\t  unsigned int n;\n+\n+\t  for (n = 0, i = FIRST_PSEUDO_REGISTER; i < (unsigned) max_regno; i++)\n+\t    if (reg_old_renumber[i] != reg_renumber[i])\n+\t      {\n+\t\tif (reg_renumber[i] < 0)\n+\t\t  temp_pseudo_reg_arr[n++] = i;\n+\t\telse\n+\t\t  CLEAR_REGNO_REG_SET (&spilled_pseudos, i);\n+\t      }\n+\t  if (ira_reassign_pseudos (temp_pseudo_reg_arr, n,\n+\t\t\t\t    bad_spill_regs_global,\n+\t\t\t\t    pseudo_forbidden_regs, pseudo_previous_regs,\n+\t\t\t\t    &spilled_pseudos))\n+\t    something_changed = 1;\n+\t  \n+\t}\n     }\n-\n   /* Fix up the register information in the insn chain.\n      This involves deleting those of the spilled pseudos which did not get\n      a new hard register home from the live_{before,after} sets.  */\n@@ -3879,9 +4029,14 @@ finish_spills (int global)\n       HARD_REG_SET used_by_pseudos;\n       HARD_REG_SET used_by_pseudos2;\n \n-      AND_COMPL_REG_SET (&chain->live_throughout, &spilled_pseudos);\n-      AND_COMPL_REG_SET (&chain->dead_or_set, &spilled_pseudos);\n-\n+      if (! flag_ira || ! optimize)\n+\t{\n+\t  /* Don't do it for IRA because IRA and the reload still can\n+\t     assign hard registers to the spilled pseudos on next\n+\t     reload iterations.  */\n+\t  AND_COMPL_REG_SET (&chain->live_throughout, &spilled_pseudos);\n+\t  AND_COMPL_REG_SET (&chain->dead_or_set, &spilled_pseudos);\n+\t}\n       /* Mark any unallocated hard regs as available for spills.  That\n \t makes inheritance work somewhat better.  */\n       if (chain->need_reload)\n@@ -3890,28 +4045,28 @@ finish_spills (int global)\n \t  REG_SET_TO_HARD_REG_SET (used_by_pseudos2, &chain->dead_or_set);\n \t  IOR_HARD_REG_SET (used_by_pseudos, used_by_pseudos2);\n \n-\t  /* Save the old value for the sanity test below.  */\n-\t  COPY_HARD_REG_SET (used_by_pseudos2, chain->used_spill_regs);\n-\n \t  compute_use_by_pseudos (&used_by_pseudos, &chain->live_throughout);\n \t  compute_use_by_pseudos (&used_by_pseudos, &chain->dead_or_set);\n+\t  /* Value of chain->used_spill_regs from previous iteration\n+\t     may be not included in the value calculated here because\n+\t     of possible removing caller-saves insns (see function\n+\t     delete_caller_save_insns.  */\n \t  COMPL_HARD_REG_SET (chain->used_spill_regs, used_by_pseudos);\n \t  AND_HARD_REG_SET (chain->used_spill_regs, used_spill_regs);\n-\n-\t  /* Make sure we only enlarge the set.  */\n-\t  gcc_assert (hard_reg_set_subset_p (used_by_pseudos2,\n-\t\t\t\t\t    chain->used_spill_regs));\n \t}\n     }\n \n+  CLEAR_REG_SET (&changed_allocation_pseudos);\n   /* Let alter_reg modify the reg rtx's for the modified pseudos.  */\n   for (i = FIRST_PSEUDO_REGISTER; i < (unsigned)max_regno; i++)\n     {\n       int regno = reg_renumber[i];\n       if (reg_old_renumber[i] == regno)\n \tcontinue;\n \n-      alter_reg (i, reg_old_renumber[i]);\n+      SET_REGNO_REG_SET (&changed_allocation_pseudos, i);\n+\n+      alter_reg (i, reg_old_renumber[i], false);\n       reg_old_renumber[i] = regno;\n       if (dump_file)\n \t{\n@@ -4295,8 +4450,8 @@ reload_as_needed (int live_known)\n          be partially clobbered by the call.  */\n       else if (CALL_P (insn))\n \t{\n-\tAND_COMPL_HARD_REG_SET (reg_reloaded_valid, call_used_reg_set);\n-\tAND_COMPL_HARD_REG_SET (reg_reloaded_valid, reg_reloaded_call_part_clobbered);\n+\t  AND_COMPL_HARD_REG_SET (reg_reloaded_valid, call_used_reg_set);\n+\t  AND_COMPL_HARD_REG_SET (reg_reloaded_valid, reg_reloaded_call_part_clobbered);\n \t}\n     }\n \n@@ -4967,6 +5122,126 @@ reloads_unique_chain_p (int r1, int r2)\n   return true;\n }\n \n+\n+/* The recursive function change all occurrences of WHAT in *WHERE\n+   onto REPL.  */\n+static void\n+substitute (rtx *where, const_rtx what, rtx repl)\n+{\n+  const char *fmt;\n+  int i;\n+  enum rtx_code code;\n+\n+  if (*where == 0)\n+    return;\n+\n+  if (*where == what || rtx_equal_p (*where, what))\n+    {\n+      *where = repl;\n+      return;\n+    }\n+\n+  code = GET_CODE (*where);\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'E')\n+\t{\n+\t  int j;\n+\n+\t  for (j = XVECLEN (*where, i) - 1; j >= 0; j--)\n+\t    substitute (&XVECEXP (*where, i, j), what, repl);\n+\t}\n+      else if (fmt[i] == 'e')\n+\tsubstitute (&XEXP (*where, i), what, repl);\n+    }\n+}\n+\n+/* The function returns TRUE if chain of reload R1 and R2 (in any\n+   order) can be evaluated without usage of intermediate register for\n+   the reload containing another reload.  It is important to see\n+   gen_reload to understand what the function is trying to do.  As an\n+   example, let us have reload chain\n+\n+      r2: const\n+      r1: <something> + const\n+\n+   and reload R2 got reload reg HR.  The function returns true if\n+   there is a correct insn HR = HR + <something>.  Otherwise,\n+   gen_reload will use intermediate register (and this is the reload\n+   reg for R1) to reload <something>.\n+\n+   We need this function to find a conflict for chain reloads.  In our\n+   example, if HR = HR + <something> is incorrect insn, then we cannot\n+   use HR as a reload register for R2.  If we do use it then we get a\n+   wrong code:\n+\n+      HR = const\n+      HR = <something>\n+      HR = HR + HR\n+\n+*/\n+static bool\n+gen_reload_chain_without_interm_reg_p (int r1, int r2)\n+{\n+  bool result;\n+  int regno, n, code;\n+  rtx out, in, tem, insn;\n+  rtx last = get_last_insn ();\n+\n+  /* Make r2 a component of r1.  */\n+  if (reg_mentioned_p (rld[r1].in, rld[r2].in))\n+    {\n+      n = r1;\n+      r1 = r2;\n+      r2 = n;\n+    }\n+  gcc_assert (reg_mentioned_p (rld[r2].in, rld[r1].in));\n+  regno = rld[r1].regno >= 0 ? rld[r1].regno : rld[r2].regno;\n+  gcc_assert (regno >= 0);\n+  out = gen_rtx_REG (rld[r1].mode, regno);\n+  in = copy_rtx (rld[r1].in);\n+  substitute (&in, rld[r2].in, gen_rtx_REG (rld[r2].mode, regno));\n+\n+  /* If IN is a paradoxical SUBREG, remove it and try to put the\n+     opposite SUBREG on OUT.  Likewise for a paradoxical SUBREG on OUT.  */\n+  if (GET_CODE (in) == SUBREG\n+      && (GET_MODE_SIZE (GET_MODE (in))\n+\t  > GET_MODE_SIZE (GET_MODE (SUBREG_REG (in))))\n+      && (tem = gen_lowpart_common (GET_MODE (SUBREG_REG (in)), out)) != 0)\n+    in = SUBREG_REG (in), out = tem;\n+\n+  if (GET_CODE (in) == PLUS\n+      && (REG_P (XEXP (in, 0))\n+\t  || GET_CODE (XEXP (in, 0)) == SUBREG\n+\t  || MEM_P (XEXP (in, 0)))\n+      && (REG_P (XEXP (in, 1))\n+\t  || GET_CODE (XEXP (in, 1)) == SUBREG\n+\t  || CONSTANT_P (XEXP (in, 1))\n+\t  || MEM_P (XEXP (in, 1))))\n+    {\n+      insn = emit_insn (gen_rtx_SET (VOIDmode, out, in));\n+      code = recog_memoized (insn);\n+      result = false;\n+\n+      if (code >= 0)\n+\t{\n+\t  extract_insn (insn);\n+\t  /* We want constrain operands to treat this insn strictly in\n+\t     its validity determination, i.e., the way it would after\n+\t     reload has completed.  */\n+\t  result = constrain_operands (1);\n+\t}\n+      \n+      delete_insns_since (last);\n+      return result;\n+    }\n+  \n+  /* It looks like other cases in gen_reload are not possible for\n+     chain reloads or do need an intermediate hard registers.  */\n+  return true;\n+}\n+\n /* Return 1 if the reloads denoted by R1 and R2 cannot share a register.\n    Return 0 otherwise.\n \n@@ -5016,7 +5291,8 @@ reloads_conflict (int r1, int r2)\n     case RELOAD_FOR_OPERAND_ADDRESS:\n       return (r2_type == RELOAD_FOR_INPUT || r2_type == RELOAD_FOR_INSN\n \t      || (r2_type == RELOAD_FOR_OPERAND_ADDRESS\n-\t\t  && !reloads_unique_chain_p (r1, r2)));\n+\t\t  && (!reloads_unique_chain_p (r1, r2)\n+\t\t      || !gen_reload_chain_without_interm_reg_p (r1, r2))));\n \n     case RELOAD_FOR_OPADDR_ADDR:\n       return (r2_type == RELOAD_FOR_INPUT\n@@ -6724,7 +7000,10 @@ emit_input_reload_insns (struct insn_chain *chain, struct reload *rl,\n \t\t  && REG_N_SETS (REGNO (old)) == 1)\n \t\t{\n \t\t  reg_renumber[REGNO (old)] = REGNO (reloadreg);\n-\t\t  alter_reg (REGNO (old), -1);\n+\t\t  if (flag_ira && optimize)\n+\t\t    /* Inform IRA about the change.  */\n+\t\t    ira_mark_allocation_change (REGNO (old));\n+\t\t  alter_reg (REGNO (old), -1, false);\n \t\t}\n \t      special = 1;\n \t    }\n@@ -8161,7 +8440,7 @@ delete_output_reload (rtx insn, int j, int last_reload_reg, rtx new_reload_reg)\n     n_occurrences += count_occurrences (PATTERN (insn),\n \t\t\t\t\teliminate_regs (substed, 0,\n \t\t\t\t\t\t\tNULL_RTX), 0);\n-  for (i1 = reg_equiv_alt_mem_list [REGNO (reg)]; i1; i1 = XEXP (i1, 1))\n+  for (i1 = reg_equiv_alt_mem_list[REGNO (reg)]; i1; i1 = XEXP (i1, 1))\n     {\n       gcc_assert (!rtx_equal_p (XEXP (i1, 0), substed));\n       n_occurrences += count_occurrences (PATTERN (insn), XEXP (i1, 0), 0);\n@@ -8262,7 +8541,10 @@ delete_output_reload (rtx insn, int j, int last_reload_reg, rtx new_reload_reg)\n \n       /* For the debugging info, say the pseudo lives in this reload reg.  */\n       reg_renumber[REGNO (reg)] = REGNO (new_reload_reg);\n-      alter_reg (REGNO (reg), -1);\n+      if (flag_ira && optimize)\n+\t/* Inform IRA about the change.  */\n+\tira_mark_allocation_change (REGNO (reg));\n+      alter_reg (REGNO (reg), -1, false);\n     }\n   else\n     {"}, {"sha": "42fc2ad6b6474b6534e292997789798183460f86", "filename": "gcc/rtl.h", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -1805,6 +1805,12 @@ rtx remove_list_elem (rtx, rtx *);\n \n /* regclass.c */\n \n+/* Initialize may_move_cost and friends for mode M.  */\n+extern void init_move_cost (enum machine_mode);\n+/* Allocate register info memory.  */\n+extern void allocate_reg_info (void);\n+/* Resize reg info.  */\n+extern void resize_reg_info (void);\n /* Free up register info memory.  */\n extern void free_reg_info (void);\n \n@@ -1815,6 +1821,7 @@ extern const char *decode_asm_operands (rtx, rtx *, rtx **, const char **,\n \n extern enum reg_class reg_preferred_class (int);\n extern enum reg_class reg_alternate_class (int);\n+extern void setup_reg_classes (int, enum reg_class, enum reg_class);\n \n extern void split_all_insns (void);\n extern unsigned int split_all_insns_noflow (void);\n@@ -2183,12 +2190,16 @@ extern bool can_copy_p (enum machine_mode);\n extern rtx fis_get_condition (rtx);\n \n /* In global.c */\n+#ifdef HARD_CONST\n+extern HARD_REG_SET eliminable_regset;\n+#endif\n extern void mark_elimination (int, int);\n extern void dump_global_regs (FILE *);\n #ifdef HARD_CONST\n /* Yes, this ifdef is silly, but HARD_REG_SET is not always defined.  */\n extern void retry_global_alloc (int, HARD_REG_SET);\n #endif\n+extern void build_insn_chain (void);\n \n /* In regclass.c */\n extern int reg_classes_intersect_p (enum reg_class, enum reg_class);\n@@ -2214,6 +2225,7 @@ extern void dbr_schedule (rtx);\n \n /* In local-alloc.c */\n extern void dump_local_alloc (FILE *);\n+extern int update_equiv_regs (void);\n \n /* In reload1.c */\n extern int function_invariant_p (const_rtx);"}, {"sha": "ebd783dd05d80bf0e8d553140894d75263f8e250", "filename": "gcc/testsuite/gcc.dg/20080410-1.c", "status": "added", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Ftestsuite%2Fgcc.dg%2F20080410-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Ftestsuite%2Fgcc.dg%2F20080410-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2F20080410-1.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -0,0 +1,28 @@\n+/* { dg-do compile { target \"sh-*-*\" } } */\n+/* { dg-options \"-O0 -m4 -ml -fira\" } */\n+/* { dg-final { scan-assembler-not \"add\\tr0,r0\" } } */\n+\n+/* This test checks that chain reloads conflict.  I they don't\n+   conflict, the same hard register R0 is used for the both reloads\n+   but in this case the second reload needs an intermediate register\n+   (which is the reload register).  As the result we have the\n+   following code\n+\n+   \tmov\t#4,r0   -- first reload\n+\tmov\tr14,r0  -- second reload\n+\tadd\tr0,r0   -- second reload\n+\n+   The right code should be\n+\n+   \tmov\t#4,r0   -- first reload\n+\tmov\tr14,r1  -- second reload\n+\tadd\tr0,r1   -- second reload\n+\n+*/\n+\n+_Complex float foo_float ();\n+\n+void bar_float ()\n+{\n+  __real foo_float ();\n+}"}, {"sha": "94fa087754eb47ed33d0e63b189548319a51b237", "filename": "gcc/timevar.def", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Ftimevar.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Ftimevar.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftimevar.def?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -179,6 +179,8 @@ DEFTIMEVAR (TV_SMS\t\t     , \"sms modulo scheduling\")\n DEFTIMEVAR (TV_SCHED                 , \"scheduling\")\n DEFTIMEVAR (TV_LOCAL_ALLOC           , \"local alloc\")\n DEFTIMEVAR (TV_GLOBAL_ALLOC          , \"global alloc\")\n+DEFTIMEVAR (TV_IRA\t   \t     , \"integrated RA\")\n+DEFTIMEVAR (TV_RELOAD\t   \t     , \"reload\")\n DEFTIMEVAR (TV_RELOAD_CSE_REGS       , \"reload CSE regs\")\n DEFTIMEVAR (TV_SEQABSTR              , \"sequence abstraction\")\n DEFTIMEVAR (TV_GCSE_AFTER_RELOAD      , \"load CSE after reload\")"}, {"sha": "7ace61c2b23c7abe02eaf7626806167ff03559c6", "filename": "gcc/toplev.c", "status": "modified", "additions": 15, "deletions": 2, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Ftoplev.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Ftoplev.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftoplev.c?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -66,6 +66,7 @@ along with GCC; see the file COPYING3.  If not see\n #include \"diagnostic.h\"\n #include \"params.h\"\n #include \"reload.h\"\n+#include \"ira.h\"\n #include \"dwarf2asm.h\"\n #include \"integrate.h\"\n #include \"real.h\"\n@@ -270,6 +271,14 @@ int flag_next_runtime = 0;\n \n enum tls_model flag_tls_default = TLS_MODEL_GLOBAL_DYNAMIC;\n \n+/* Set the default algorithm for the integrated register allocator.  */\n+\n+enum ira_algorithm flag_ira_algorithm = IRA_ALGORITHM_MIXED;\n+\n+/* Set the default value for -fira-verbose.  */\n+\n+unsigned int flag_ira_verbose = 5;\n+\n /* Nonzero means change certain warnings into errors.\n    Usually these are warnings about failure to conform to some standard.  */\n \n@@ -2009,6 +2018,7 @@ backend_init (void)\n   save_register_info ();\n \n   /* Initialize the target-specific back end pieces.  */\n+  ira_init_once ();\n   backend_init_target ();\n }\n \n@@ -2029,9 +2039,10 @@ lang_dependent_init_target (void)\n   /* Do the target-specific parts of expr initialization.  */\n   init_expr_target ();\n \n-  /* Although the actions of init_set_costs are language-independent,\n-     it uses optabs, so we cannot call it from backend_init.  */\n+  /* Although the actions of these functions are language-independent,\n+     they use optabs, so we cannot call them from backend_init.  */\n   init_set_costs ();\n+  ira_init ();\n \n   expand_dummy_function_end ();\n }\n@@ -2132,6 +2143,8 @@ finalize (void)\n   statistics_fini ();\n   finish_optimization_passes ();\n \n+  ira_finish_once ();\n+\n   if (mem_report)\n     dump_memory_report (true);\n "}, {"sha": "5555bf1e093b6e4e5b99f4795c85b817772b5ea0", "filename": "gcc/toplev.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Ftoplev.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Ftoplev.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftoplev.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -139,6 +139,11 @@ extern int flag_unroll_all_loops;\n extern int flag_unswitch_loops;\n extern int flag_cprop_registers;\n extern int time_report;\n+extern int flag_ira;\n+extern int flag_ira_coalesce;\n+extern int flag_ira_move_spills;\n+extern int flag_ira_share_save_slots;\n+extern int flag_ira_share_spill_slots;\n \n /* Things to do with target switches.  */\n extern void print_version (FILE *, const char *);"}, {"sha": "f4e02e86a5ed500d56893e7c552fcdbbd52f3694", "filename": "gcc/tree-pass.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Ftree-pass.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/058e97ecf33ad0dfd926b3876a4bcf59ac9556ff/gcc%2Ftree-pass.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-pass.h?ref=058e97ecf33ad0dfd926b3876a4bcf59ac9556ff", "patch": "@@ -469,6 +469,7 @@ extern struct rtl_opt_pass pass_sms;\n extern struct rtl_opt_pass pass_sched;\n extern struct rtl_opt_pass pass_local_alloc;\n extern struct rtl_opt_pass pass_global_alloc;\n+extern struct rtl_opt_pass pass_ira;\n extern struct rtl_opt_pass pass_postreload;\n extern struct rtl_opt_pass pass_clean_state;\n extern struct rtl_opt_pass pass_branch_prob;"}]}