{"sha": "4a00c76146c2e39d7c56ad93cdee3c4e13561ac6", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NGEwMGM3NjE0NmMyZTM5ZDdjNTZhZDkzY2RlZTNjNGUxMzU2MWFjNg==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2011-11-04T10:35:44Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2011-11-04T10:35:44Z"}, "message": "tree-vect-stmts.c (vectorizable_conversion): Rewritten to handle not just FLOAT_EXPR and FIX_TRUNC_EXPR...\n\n\t* tree-vect-stmts.c (vectorizable_conversion): Rewritten to handle\n\tnot just FLOAT_EXPR and FIX_TRUNC_EXPR, but also CONVERT_EXPR_CODE_P,\n\tWIDEN_MULT_EXPR and WIDEN_LSHIFT_EXPR to handle what\n\tvectorizable_type_demotion and vectorizable_type_promotion did.\n\tAdditionally handle FLOAT_EXPR and FIX_TRUNC_EXPR where the integer\n\tis {,un}signed {char,short}.\n\t(vect_create_vectorized_demotion_stmts): Fix comment typo.  For\n\trecursive calls unconditionally use VEC_PACK_TRUNC_EXPR.\n\tPush vec_dest back to the vec_dsts vector at the end.\n\t(vect_create_vectorized_promotion_stmts): Don't recurse, do just\n\tone step.  Removed multi_step_cvt, vec_dsts, slp_node and\n\tprev_stmt_info arguments, add vec_dest argument.  Push always\n\tinto vec_tmp, not just when multi_step_cvt != 0, replace *vec_oprdn0\n\twith vec_tmp at the end after freeing old *vec_oprnd0 vector.\n\t(vectorizable_type_demotion, vectorizable_type_promotion): Removed.\n\t(vect_analyze_stmt): Don't call vectorizable_type_demotion and\n\tvectorizable_type_promotion.  Call vectorizable_conversion even\n\tfor SLP bb vectorization.\n\t(vect_transform_stmt): Call vectorizable_conversion instead of\n\tvectorizable_type_demotion and vectorizable_type_promotion.\n\t(supportable_widening_operation): Clear *multi_step_cvt first,\n\tsimplify c1/c2 computation, free *interm_types vector on failure.\n\t(supportable_narrowing_operation): Clear *multi_step_cvt first,\n\tfree *interm_types vector on failure, handle multi-step\n\tFIX_TRUNC_EXPR.\n\n\t* gcc.dg/torture/vec-cvt-1.c: New test.\n\nFrom-SVN: r180932", "tree": {"sha": "d84fbb0217ff68e376cce6745950dd5efd5016b4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d84fbb0217ff68e376cce6745950dd5efd5016b4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "25ec1790d7c10ecdd9bbe7bf80a310b8b4d7841d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/25ec1790d7c10ecdd9bbe7bf80a310b8b4d7841d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/25ec1790d7c10ecdd9bbe7bf80a310b8b4d7841d"}], "stats": {"total": 2123, "additions": 1064, "deletions": 1059}, "files": [{"sha": "111efb1a080621c80970102e5ef187e10dc8387a", "filename": "gcc/ChangeLog", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=4a00c76146c2e39d7c56ad93cdee3c4e13561ac6", "patch": "@@ -1,3 +1,31 @@\n+2011-11-04  Jakub Jelinek  <jakub@redhat.com>\n+\n+\t* tree-vect-stmts.c (vectorizable_conversion): Rewritten to handle\n+\tnot just FLOAT_EXPR and FIX_TRUNC_EXPR, but also CONVERT_EXPR_CODE_P,\n+\tWIDEN_MULT_EXPR and WIDEN_LSHIFT_EXPR to handle what\n+\tvectorizable_type_demotion and vectorizable_type_promotion did.\n+\tAdditionally handle FLOAT_EXPR and FIX_TRUNC_EXPR where the integer\n+\tis {,un}signed {char,short}.\n+\t(vect_create_vectorized_demotion_stmts): Fix comment typo.  For\n+\trecursive calls unconditionally use VEC_PACK_TRUNC_EXPR.\n+\tPush vec_dest back to the vec_dsts vector at the end.\n+\t(vect_create_vectorized_promotion_stmts): Don't recurse, do just\n+\tone step.  Removed multi_step_cvt, vec_dsts, slp_node and\n+\tprev_stmt_info arguments, add vec_dest argument.  Push always\n+\tinto vec_tmp, not just when multi_step_cvt != 0, replace *vec_oprdn0\n+\twith vec_tmp at the end after freeing old *vec_oprnd0 vector.\n+\t(vectorizable_type_demotion, vectorizable_type_promotion): Removed.\n+\t(vect_analyze_stmt): Don't call vectorizable_type_demotion and\n+\tvectorizable_type_promotion.  Call vectorizable_conversion even\n+\tfor SLP bb vectorization.\n+\t(vect_transform_stmt): Call vectorizable_conversion instead of\n+\tvectorizable_type_demotion and vectorizable_type_promotion.\n+\t(supportable_widening_operation): Clear *multi_step_cvt first,\n+\tsimplify c1/c2 computation, free *interm_types vector on failure.\n+\t(supportable_narrowing_operation): Clear *multi_step_cvt first,\n+\tfree *interm_types vector on failure, handle multi-step\n+\tFIX_TRUNC_EXPR.\n+\n 2011-11-04  Tristan Gingold  <gingold@adacore.com>\n \n \t* config/alpha/alpha.c (alpha_write_linkage): Remove fundecl"}, {"sha": "7192d526e9cc953ba2986e99874a86da38d84261", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=4a00c76146c2e39d7c56ad93cdee3c4e13561ac6", "patch": "@@ -1,3 +1,7 @@\n+2011-11-04  Jakub Jelinek  <jakub@redhat.com>\n+\n+\t* gcc.dg/torture/vec-cvt-1.c: New test.\n+\n 2011-11-04  Eric Botcazou  <ebotcazou@adacore.com>\n \n \t* gnat.dg/specs/private1[-sub].ads: New test."}, {"sha": "a6d111fd9be25a8f405827bf0f078973a94bcad0", "filename": "gcc/testsuite/gcc.dg/torture/vec-cvt-1.c", "status": "added", "additions": 211, "deletions": 0, "changes": 211, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6/gcc%2Ftestsuite%2Fgcc.dg%2Ftorture%2Fvec-cvt-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6/gcc%2Ftestsuite%2Fgcc.dg%2Ftorture%2Fvec-cvt-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Ftorture%2Fvec-cvt-1.c?ref=4a00c76146c2e39d7c56ad93cdee3c4e13561ac6", "patch": "@@ -0,0 +1,211 @@\n+/* { dg-do run } */\n+\n+#include <stdlib.h>\n+\n+#define N 1024\n+signed char sc[N];\n+short ss[N];\n+int si[N];\n+long long sl[N];\n+unsigned char uc[N];\n+unsigned short us[N];\n+unsigned int ui[N];\n+unsigned long long ul[N];\n+float f[N];\n+double d[N];\n+\n+#define FN1(from, to) \\\n+__attribute__((noinline, noclone)) void\t\t\\\n+from##2##to (void)\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  int i;\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\\\n+    to[i] = from[i];\t\t\t\t\\\n+}\n+#define FN(intt, fltt) FN1 (intt, fltt) FN1 (fltt, intt)\n+\n+FN (sc, f)\n+FN (ss, f)\n+FN (si, f)\n+FN (sl, f)\n+FN (uc, f)\n+FN (us, f)\n+FN (ui, f)\n+FN (ul, f)\n+FN (sc, d)\n+FN (ss, d)\n+FN (si, d)\n+FN (sl, d)\n+FN (uc, d)\n+FN (us, d)\n+FN (ui, d)\n+FN (ul, d)\n+\n+#define FLTTEST(min, max, intt) \\\n+__attribute__((noinline, noclone)) void\t\t\t\t\t\\\n+flttointtest##intt (void)\t\t\t\t\t\t\\\n+{\t\t\t\t\t\t\t\t\t\\\n+  int i;\t\t\t\t\t\t\t\t\\\n+  volatile float fltmin, fltmax, vf, vf2;\t\t\t\t\\\n+  volatile double dblmin, dblmax, vd, vd2;\t\t\t\t\\\n+  if (min == 0)\t\t\t\t\t\t\t\t\\\n+    fltmin = 0.0f;\t\t\t\t\t\t\t\\\n+  else\t\t\t\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      vf2 = fltmin = min - 1.0f;\t\t\t\t\t\\\n+      for (vf = 1.0f; (fltmin = vf2 + vf) == vf2; vf = vf * 2.0f)\t\\\n+\t;\t\t\t\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  vf2 = fltmax = max + 1.0f;\t\t\t\t\t\t\\\n+  for (vf = 1.0f; (fltmax = vf2 - vf) == vf2; vf = vf * 2.0f)\t\t\\\n+    ;\t\t\t\t\t\t\t\t\t\\\n+  if (min == 0)\t\t\t\t\t\t\t\t\\\n+    dblmin = 0.0;\t\t\t\t\t\t\t\\\n+  else\t\t\t\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      vd2 = dblmin = min - 1.0;\t\t\t\t\t\t\\\n+      for (vd = 1.0; (dblmin = vd2 + vd) == vd2; vd = vd * 2.0)\t\t\\\n+\t;\t\t\t\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  vd2 = dblmax = max + 1.0;\t\t\t\t\t\t\\\n+  for (vd = 1.0; (dblmax = vd2 - vd) == vd2; vd = vd * 2.0)\t\t\\\n+    ;\t\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      asm (\"\");\t\t\t\t\t\t\t\t\\\n+      if (i == 0)\t\t\t\t\t\t\t\\\n+\tf[i] = fltmin;\t\t\t\t\t\t\t\\\n+      else if (i < N / 4)\t\t\t\t\t\t\\\n+\tf[i] = fltmin + i + 0.25f;\t\t\t\t\t\\\n+      else if (i < 3 * N / 4)\t\t\t\t\t\t\\\n+\tf[i] = (fltmax + fltmin) / 2.0 - N * 8 + 16.0f * i;\t\t\\\n+      else\t\t\t\t\t\t\t\t\\\n+\tf[i] = fltmax - N + 1 + i;\t\t\t\t\t\\\n+      if (f[i] < fltmin) f[i] = fltmin;\t\t\t\t\t\\\n+      if (f[i] > fltmax) f[i] = fltmax;\t\t\t\t\t\\\n+      if (i == 0)\t\t\t\t\t\t\t\\\n+\td[i] = dblmin;\t\t\t\t\t\t\t\\\n+      else if (i < N / 4)\t\t\t\t\t\t\\\n+\td[i] = dblmin + i + 0.25f;\t\t\t\t\t\\\n+      else if (i < 3 * N / 4)\t\t\t\t\t\t\\\n+\td[i] = (dblmax + dblmin) / 2.0 - N * 8 + 16.0f * i;\t\t\\\n+      else\t\t\t\t\t\t\t\t\\\n+\td[i] = dblmax - N + 1 + i;\t\t\t\t\t\\\n+      if (d[i] < dblmin) d[i] = dblmin;\t\t\t\t\t\\\n+      if (d[i] > dblmax) d[i] = dblmax;\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  f2##intt ();\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    if (intt[i] != (__typeof (intt[0])) f[i])\t\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\t\\\n+  d2##intt ();\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    if (intt[i] != (__typeof (intt[0])) d[i])\t\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      unsigned long long r = random ();\t\t\t\t\t\\\n+      r = (r << 21) ^ (unsigned) random ();\t\t\t\t\\\n+      r = (r << 21) ^ (unsigned) random ();\t\t\t\t\\\n+      asm (\"\");\t\t\t\t\t\t\t\t\\\n+      f[i] = (r >> 59) / 32.0f + (__typeof (intt[0])) r;\t\t\\\n+      if (f[i] < fltmin) f[i] = fltmin;\t\t\t\t\t\\\n+      if (f[i] > fltmax) f[i] = fltmax;\t\t\t\t\t\\\n+      d[i] = (r >> 59) / 32.0 + (__typeof (intt[0])) r;\t\t\t\\\n+      if (d[i] < dblmin) f[i] = dblmin;\t\t\t\t\t\\\n+      if (d[i] > dblmax) f[i] = dblmax;\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  f2##intt ();\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    if (intt[i] != (__typeof (intt[0])) f[i])\t\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\t\\\n+  d2##intt ();\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    if (intt[i] != (__typeof (intt[0])) d[i])\t\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\t\\\n+}\t\t\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+__attribute__((noinline, noclone)) void\t\t\t\t\t\\\n+inttoflttest##intt (void)\t\t\t\t\t\t\\\n+{\t\t\t\t\t\t\t\t\t\\\n+  int i;\t\t\t\t\t\t\t\t\\\n+  volatile float vf;\t\t\t\t\t\t\t\\\n+  volatile double vd;\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      asm (\"\");\t\t\t\t\t\t\t\t\\\n+      if (i < N / 4)\t\t\t\t\t\t\t\\\n+\tintt[i] = min + i;\t\t\t\t\t\t\\\n+      else if (i < 3 * N / 4)\t\t\t\t\t\t\\\n+\tintt[i] = (max + min) / 2 - N * 8 + 16 * i;\t\t\t\\\n+      else\t\t\t\t\t\t\t\t\\\n+\tintt[i] = max - N + 1 + i;\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  intt##2f ();\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      vf = intt[i];\t\t\t\t\t\t\t\\\n+      if (f[i] != vf)\t\t\t\t\t\t\t\\\n+\tabort ();\t\t\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  intt##2d ();\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      vd = intt[i];\t\t\t\t\t\t\t\\\n+      if (d[i] != vd)\t\t\t\t\t\t\t\\\n+\tabort ();\t\t\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      unsigned long long r = random ();\t\t\t\t\t\\\n+      r = (r << 21) ^ (unsigned) random ();\t\t\t\t\\\n+      r = (r << 21) ^ (unsigned) random ();\t\t\t\t\\\n+      asm (\"\");\t\t\t\t\t\t\t\t\\\n+      intt[i] = r;\t\t\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  intt##2f ();\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      vf = intt[i];\t\t\t\t\t\t\t\\\n+      if (f[i] != vf)\t\t\t\t\t\t\t\\\n+\tabort ();\t\t\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  intt##2d ();\t\t\t\t\t\t\t\t\\\n+  for (i = 0; i < N; i++)\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      vd = intt[i];\t\t\t\t\t\t\t\\\n+      if (d[i] != vd)\t\t\t\t\t\t\t\\\n+\tabort ();\t\t\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+}\n+\n+FLTTEST (- __SCHAR_MAX__ - 1, __SCHAR_MAX__, sc)\n+FLTTEST (- __SHRT_MAX__ - 1, __SHRT_MAX__, ss)\n+FLTTEST (- __INT_MAX__ - 1, __INT_MAX__, si)\n+FLTTEST (- __LONG_LONG_MAX__ - 1LL, __LONG_LONG_MAX__, sl)\n+FLTTEST (0, 2U * __SCHAR_MAX__ + 1, uc)\n+FLTTEST (0, 2U * __SHRT_MAX__ + 1, us)\n+FLTTEST (0, 2U * __INT_MAX__ + 1, ui)\n+FLTTEST (0, 2ULL * __LONG_LONG_MAX__ + 1, ul)\n+\n+int\n+main ()\n+{\n+  flttointtestsc ();\n+  flttointtestss ();\n+  flttointtestsi ();\n+  flttointtestsl ();\n+  flttointtestuc ();\n+  flttointtestus ();\n+//  flttointtestui ();\n+  flttointtestul ();\n+  inttoflttestsc ();\n+  inttoflttestss ();\n+  inttoflttestsi ();\n+  inttoflttestsl ();\n+  inttoflttestuc ();\n+  inttoflttestus ();\n+//  inttoflttestui ();\n+  inttoflttestul ();\n+  return 0;\n+}"}, {"sha": "62b88e1cd9555c88c75e5e511fe4a81a39bbe2fc", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 821, "deletions": 1059, "changes": 1880, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4a00c76146c2e39d7c56ad93cdee3c4e13561ac6/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=4a00c76146c2e39d7c56ad93cdee3c4e13561ac6", "patch": "@@ -1843,9 +1843,168 @@ vect_gen_widened_results_half (enum tree_code code,\n   return new_stmt;\n }\n \n+\n+/* Get vectorized definitions for loop-based vectorization.  For the first\n+   operand we call vect_get_vec_def_for_operand() (with OPRND containing\n+   scalar operand), and for the rest we get a copy with\n+   vect_get_vec_def_for_stmt_copy() using the previous vector definition\n+   (stored in OPRND). See vect_get_vec_def_for_stmt_copy() for details.\n+   The vectors are collected into VEC_OPRNDS.  */\n+\n+static void\n+vect_get_loop_based_defs (tree *oprnd, gimple stmt, enum vect_def_type dt,\n+\t\t\t  VEC (tree, heap) **vec_oprnds, int multi_step_cvt)\n+{\n+  tree vec_oprnd;\n+\n+  /* Get first vector operand.  */\n+  /* All the vector operands except the very first one (that is scalar oprnd)\n+     are stmt copies.  */\n+  if (TREE_CODE (TREE_TYPE (*oprnd)) != VECTOR_TYPE)\n+    vec_oprnd = vect_get_vec_def_for_operand (*oprnd, stmt, NULL);\n+  else\n+    vec_oprnd = vect_get_vec_def_for_stmt_copy (dt, *oprnd);\n+\n+  VEC_quick_push (tree, *vec_oprnds, vec_oprnd);\n+\n+  /* Get second vector operand.  */\n+  vec_oprnd = vect_get_vec_def_for_stmt_copy (dt, vec_oprnd);\n+  VEC_quick_push (tree, *vec_oprnds, vec_oprnd);\n+\n+  *oprnd = vec_oprnd;\n+\n+  /* For conversion in multiple steps, continue to get operands\n+     recursively.  */\n+  if (multi_step_cvt)\n+    vect_get_loop_based_defs (oprnd, stmt, dt, vec_oprnds,  multi_step_cvt - 1);\n+}\n+\n+\n+/* Create vectorized demotion statements for vector operands from VEC_OPRNDS.\n+   For multi-step conversions store the resulting vectors and call the function\n+   recursively.  */\n+\n+static void\n+vect_create_vectorized_demotion_stmts (VEC (tree, heap) **vec_oprnds,\n+\t\t\t\t       int multi_step_cvt, gimple stmt,\n+\t\t\t\t       VEC (tree, heap) *vec_dsts,\n+\t\t\t\t       gimple_stmt_iterator *gsi,\n+\t\t\t\t       slp_tree slp_node, enum tree_code code,\n+\t\t\t\t       stmt_vec_info *prev_stmt_info)\n+{\n+  unsigned int i;\n+  tree vop0, vop1, new_tmp, vec_dest;\n+  gimple new_stmt;\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+\n+  vec_dest = VEC_pop (tree, vec_dsts);\n+\n+  for (i = 0; i < VEC_length (tree, *vec_oprnds); i += 2)\n+    {\n+      /* Create demotion operation.  */\n+      vop0 = VEC_index (tree, *vec_oprnds, i);\n+      vop1 = VEC_index (tree, *vec_oprnds, i + 1);\n+      new_stmt = gimple_build_assign_with_ops (code, vec_dest, vop0, vop1);\n+      new_tmp = make_ssa_name (vec_dest, new_stmt);\n+      gimple_assign_set_lhs (new_stmt, new_tmp);\n+      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\n+      if (multi_step_cvt)\n+\t/* Store the resulting vector for next recursive call.  */\n+\tVEC_replace (tree, *vec_oprnds, i/2, new_tmp);\n+      else\n+\t{\n+\t  /* This is the last step of the conversion sequence. Store the\n+\t     vectors in SLP_NODE or in vector info of the scalar statement\n+\t     (or in STMT_VINFO_RELATED_STMT chain).  */\n+\t  if (slp_node)\n+\t    VEC_quick_push (gimple, SLP_TREE_VEC_STMTS (slp_node), new_stmt);\n+\t  else\n+\t    {\n+\t      if (!*prev_stmt_info)\n+\t\tSTMT_VINFO_VEC_STMT (stmt_info) = new_stmt;\n+\t      else\n+\t\tSTMT_VINFO_RELATED_STMT (*prev_stmt_info) = new_stmt;\n+\n+\t      *prev_stmt_info = vinfo_for_stmt (new_stmt);\n+\t    }\n+\t}\n+    }\n+\n+  /* For multi-step demotion operations we first generate demotion operations\n+     from the source type to the intermediate types, and then combine the\n+     results (stored in VEC_OPRNDS) in demotion operation to the destination\n+     type.  */\n+  if (multi_step_cvt)\n+    {\n+      /* At each level of recursion we have half of the operands we had at the\n+\t previous level.  */\n+      VEC_truncate (tree, *vec_oprnds, (i+1)/2);\n+      vect_create_vectorized_demotion_stmts (vec_oprnds, multi_step_cvt - 1,\n+\t\t\t\t\t     stmt, vec_dsts, gsi, slp_node,\n+\t\t\t\t\t     VEC_PACK_TRUNC_EXPR,\n+\t\t\t\t\t     prev_stmt_info);\n+    }\n+\n+  VEC_quick_push (tree, vec_dsts, vec_dest);\n+}\n+\n+\n+/* Create vectorized promotion statements for vector operands from VEC_OPRNDS0\n+   and VEC_OPRNDS1 (for binary operations).  For multi-step conversions store\n+   the resulting vectors and call the function recursively.  */\n+\n+static void\n+vect_create_vectorized_promotion_stmts (VEC (tree, heap) **vec_oprnds0,\n+\t\t\t\t\tVEC (tree, heap) **vec_oprnds1,\n+\t\t\t\t\tgimple stmt, tree vec_dest,\n+\t\t\t\t\tgimple_stmt_iterator *gsi,\n+\t\t\t\t\tenum tree_code code1,\n+\t\t\t\t\tenum tree_code code2, tree decl1,\n+\t\t\t\t\ttree decl2, int op_type)\n+{\n+  int i;\n+  tree vop0, vop1, new_tmp1, new_tmp2;\n+  gimple new_stmt1, new_stmt2;\n+  VEC (tree, heap) *vec_tmp = NULL;\n+\n+  vec_tmp = VEC_alloc (tree, heap, VEC_length (tree, *vec_oprnds0) * 2);\n+  FOR_EACH_VEC_ELT (tree, *vec_oprnds0, i, vop0)\n+    {\n+      if (op_type == binary_op)\n+\tvop1 = VEC_index (tree, *vec_oprnds1, i);\n+      else\n+\tvop1 = NULL_TREE;\n+\n+      /* Generate the two halves of promotion operation.  */\n+      new_stmt1 = vect_gen_widened_results_half (code1, decl1, vop0, vop1,\n+\t\t\t\t\t\t op_type, vec_dest, gsi, stmt);\n+      new_stmt2 = vect_gen_widened_results_half (code2, decl2, vop0, vop1,\n+\t\t\t\t\t\t op_type, vec_dest, gsi, stmt);\n+      if (is_gimple_call (new_stmt1))\n+\t{\n+\t  new_tmp1 = gimple_call_lhs (new_stmt1);\n+\t  new_tmp2 = gimple_call_lhs (new_stmt2);\n+\t}\n+      else\n+\t{\n+\t  new_tmp1 = gimple_assign_lhs (new_stmt1);\n+\t  new_tmp2 = gimple_assign_lhs (new_stmt2);\n+\t}\n+\n+      /* Store the results for the next step.  */\n+      VEC_quick_push (tree, vec_tmp, new_tmp1);\n+      VEC_quick_push (tree, vec_tmp, new_tmp2);\n+    }\n+\n+  VEC_free (tree, heap, *vec_oprnds0);\n+  *vec_oprnds0 = vec_tmp;\n+}\n+\n+\n /* Check if STMT performs a conversion operation, that can be vectorized.\n    If VEC_STMT is also passed, vectorize the STMT: create a vectorized\n-   stmt to replace it, put it in VEC_STMT, and insert it at BSI.\n+   stmt to replace it, put it in VEC_STMT, and insert it at GSI.\n    Return FALSE if not a vectorizable STMT, TRUE otherwise.  */\n \n static bool\n@@ -1854,11 +2013,12 @@ vectorizable_conversion (gimple stmt, gimple_stmt_iterator *gsi,\n {\n   tree vec_dest;\n   tree scalar_dest;\n-  tree op0;\n+  tree op0, op1 = NULL_TREE;\n   tree vec_oprnd0 = NULL_TREE, vec_oprnd1 = NULL_TREE;\n   stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   enum tree_code code, code1 = ERROR_MARK, code2 = ERROR_MARK;\n+  enum tree_code codecvt1 = ERROR_MARK, codecvt2 = ERROR_MARK;\n   tree decl1 = NULL_TREE, decl2 = NULL_TREE;\n   tree new_temp;\n   tree def;\n@@ -1869,21 +2029,22 @@ vectorizable_conversion (gimple stmt, gimple_stmt_iterator *gsi,\n   int nunits_in;\n   int nunits_out;\n   tree vectype_out, vectype_in;\n-  int ncopies, j;\n-  tree rhs_type;\n+  int ncopies, i, j;\n+  tree lhs_type, rhs_type;\n   enum { NARROW, NONE, WIDEN } modifier;\n-  int i;\n-  VEC(tree,heap) *vec_oprnds0 = NULL;\n+  VEC (tree,heap) *vec_oprnds0 = NULL, *vec_oprnds1 = NULL;\n   tree vop0;\n-  VEC(tree,heap) *dummy = NULL;\n-  int dummy_int;\n+  bb_vec_info bb_vinfo = STMT_VINFO_BB_VINFO (stmt_info);\n+  int multi_step_cvt = 0;\n+  VEC (tree, heap) *vec_dsts = NULL, *interm_types = NULL;\n+  tree last_oprnd, intermediate_type, cvt_type = NULL_TREE;\n+  int op_type;\n+  enum machine_mode rhs_mode;\n+  unsigned short fltsz;\n \n   /* Is STMT a vectorizable conversion?   */\n \n-  /* FORNOW: unsupported in basic block SLP.  */\n-  gcc_assert (loop_vinfo);\n-\n-  if (!STMT_VINFO_RELEVANT_P (stmt_info))\n+  if (!STMT_VINFO_RELEVANT_P (stmt_info) && !bb_vinfo)\n     return false;\n \n   if (STMT_VINFO_DEF_TYPE (stmt_info) != vect_internal_def)\n@@ -1896,23 +2057,74 @@ vectorizable_conversion (gimple stmt, gimple_stmt_iterator *gsi,\n     return false;\n \n   code = gimple_assign_rhs_code (stmt);\n-  if (code != FIX_TRUNC_EXPR && code != FLOAT_EXPR)\n+  if (!CONVERT_EXPR_CODE_P (code)\n+      && code != FIX_TRUNC_EXPR\n+      && code != FLOAT_EXPR\n+      && code != WIDEN_MULT_EXPR\n+      && code != WIDEN_LSHIFT_EXPR)\n     return false;\n \n+  op_type = TREE_CODE_LENGTH (code);\n+\n   /* Check types of lhs and rhs.  */\n   scalar_dest = gimple_assign_lhs (stmt);\n+  lhs_type = TREE_TYPE (scalar_dest);\n   vectype_out = STMT_VINFO_VECTYPE (stmt_info);\n \n   op0 = gimple_assign_rhs1 (stmt);\n   rhs_type = TREE_TYPE (op0);\n+\n+  if ((code != FIX_TRUNC_EXPR && code != FLOAT_EXPR)\n+      && !((INTEGRAL_TYPE_P (lhs_type)\n+\t    && INTEGRAL_TYPE_P (rhs_type))\n+\t   || (SCALAR_FLOAT_TYPE_P (lhs_type)\n+\t       && SCALAR_FLOAT_TYPE_P (rhs_type))))\n+    return false;\n+\n+  if ((INTEGRAL_TYPE_P (lhs_type)\n+       && (TYPE_PRECISION (lhs_type)\n+\t   != GET_MODE_PRECISION (TYPE_MODE (lhs_type))))\n+      || (INTEGRAL_TYPE_P (rhs_type)\n+\t  && (TYPE_PRECISION (rhs_type)\n+\t      != GET_MODE_PRECISION (TYPE_MODE (rhs_type)))))\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS))\n+\tfprintf (vect_dump,\n+\t\t \"type conversion to/from bit-precision unsupported.\");\n+      return false;\n+    }\n+\n   /* Check the operands of the operation.  */\n-  if (!vect_is_simple_use_1 (op0, loop_vinfo, NULL,\n+  if (!vect_is_simple_use_1 (op0, loop_vinfo, bb_vinfo,\n \t\t\t     &def_stmt, &def, &dt[0], &vectype_in))\n     {\n       if (vect_print_dump_info (REPORT_DETAILS))\n \tfprintf (vect_dump, \"use not simple.\");\n       return false;\n     }\n+  if (op_type == binary_op)\n+    {\n+      bool ok;\n+\n+      op1 = gimple_assign_rhs2 (stmt);\n+      gcc_assert (code == WIDEN_MULT_EXPR || code == WIDEN_LSHIFT_EXPR);\n+      /* For WIDEN_MULT_EXPR, if OP0 is a constant, use the type of\n+\t OP1.  */\n+      if (CONSTANT_CLASS_P (op0))\n+\tok = vect_is_simple_use_1 (op1, loop_vinfo, NULL,\n+\t\t\t\t   &def_stmt, &def, &dt[1], &vectype_in);\n+      else\n+\tok = vect_is_simple_use (op1, loop_vinfo, NULL, &def_stmt, &def,\n+\t\t\t\t &dt[1]);\n+\n+      if (!ok)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS))\n+\t    fprintf (vect_dump, \"use not simple.\");\n+\t  return false;\n+\t}\n+    }\n+\n   /* If op0 is an external or constant defs use a vector type of\n      the same size as the output vector type.  */\n   if (!vectype_in)\n@@ -1922,82 +2134,222 @@ vectorizable_conversion (gimple stmt, gimple_stmt_iterator *gsi,\n   if (!vectype_in)\n     {\n       if (vect_print_dump_info (REPORT_DETAILS))\n-        {\n-          fprintf (vect_dump, \"no vectype for scalar type \");\n-          print_generic_expr (vect_dump, rhs_type, TDF_SLIM);\n-        }\n+\t{\n+\t  fprintf (vect_dump, \"no vectype for scalar type \");\n+\t  print_generic_expr (vect_dump, rhs_type, TDF_SLIM);\n+\t}\n \n       return false;\n     }\n \n-  /* FORNOW */\n   nunits_in = TYPE_VECTOR_SUBPARTS (vectype_in);\n   nunits_out = TYPE_VECTOR_SUBPARTS (vectype_out);\n-  if (nunits_in == nunits_out / 2)\n+  if (nunits_in < nunits_out)\n     modifier = NARROW;\n   else if (nunits_out == nunits_in)\n     modifier = NONE;\n-  else if (nunits_out == nunits_in / 2)\n-    modifier = WIDEN;\n-  else\n-    return false;\n-\n-  if (modifier == NARROW)\n-    ncopies = LOOP_VINFO_VECT_FACTOR (loop_vinfo) / nunits_out;\n   else\n-    ncopies = LOOP_VINFO_VECT_FACTOR (loop_vinfo) / nunits_in;\n+    modifier = WIDEN;\n \n   /* Multiple types in SLP are handled by creating the appropriate number of\n      vectorized stmts for each SLP node.  Hence, NCOPIES is always 1 in\n      case of SLP.  */\n   if (slp_node || PURE_SLP_STMT (stmt_info))\n     ncopies = 1;\n+  else if (modifier == NARROW)\n+    ncopies = LOOP_VINFO_VECT_FACTOR (loop_vinfo) / nunits_out;\n+  else\n+    ncopies = LOOP_VINFO_VECT_FACTOR (loop_vinfo) / nunits_in;\n \n   /* Sanity check: make sure that at least one copy of the vectorized stmt\n      needs to be generated.  */\n   gcc_assert (ncopies >= 1);\n \n   /* Supportable by target?  */\n-  if ((modifier == NONE\n-       && !supportable_convert_operation (code, vectype_out, vectype_in, &decl1, &code1))\n-      || (modifier == WIDEN\n-\t  && !supportable_widening_operation (code, stmt,\n-\t\t\t\t\t      vectype_out, vectype_in,\n-\t\t\t\t\t      &decl1, &decl2,\n-\t\t\t\t\t      &code1, &code2,\n-                                              &dummy_int, &dummy))\n-      || (modifier == NARROW\n-\t  && !supportable_narrowing_operation (code, vectype_out, vectype_in,\n-\t\t\t\t\t       &code1, &dummy_int, &dummy)))\n+  switch (modifier)\n     {\n+    case NONE:\n+      if (code != FIX_TRUNC_EXPR && code != FLOAT_EXPR)\n+\treturn false;\n+      if (supportable_convert_operation (code, vectype_out, vectype_in,\n+\t\t\t\t\t &decl1, &code1))\n+\tbreak;\n+      /* FALLTHRU */\n+    unsupported:\n       if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"conversion not supported by target.\");\n+\tfprintf (vect_dump, \"conversion not supported by target.\");\n       return false;\n-    }\n \n-  if (modifier != NONE)\n-    {\n-      /* FORNOW: SLP not supported.  */\n-      if (STMT_SLP_TYPE (stmt_info))\n-\treturn false;\n+    case WIDEN:\n+      if (supportable_widening_operation (code, stmt, vectype_out, vectype_in,\n+\t\t\t\t\t  &decl1, &decl2, &code1, &code2,\n+\t\t\t\t\t  &multi_step_cvt, &interm_types))\n+\t{\n+\t  /* Binary widening operation can only be supported directly by the\n+\t     architecture.  */\n+\t  gcc_assert (!(multi_step_cvt && op_type == binary_op));\n+\t  break;\n+\t}\n+\n+      if (code != FLOAT_EXPR\n+\t  || (GET_MODE_SIZE (TYPE_MODE (lhs_type))\n+\t      <= GET_MODE_SIZE (TYPE_MODE (rhs_type))))\n+\tgoto unsupported;\n+\n+      rhs_mode = TYPE_MODE (rhs_type);\n+      fltsz = GET_MODE_SIZE (TYPE_MODE (lhs_type));\n+      for (rhs_mode = GET_MODE_2XWIDER_MODE (TYPE_MODE (rhs_type));\n+\t   rhs_mode != VOIDmode && GET_MODE_SIZE (rhs_mode) <= fltsz;\n+\t   rhs_mode = GET_MODE_2XWIDER_MODE (rhs_mode))\n+\t{\n+\t  cvt_type\n+\t    = build_nonstandard_integer_type (GET_MODE_BITSIZE (rhs_mode), 0);\n+\t  cvt_type = get_same_sized_vectype (cvt_type, vectype_in);\n+\t  if (cvt_type == NULL_TREE)\n+\t    goto unsupported;\n+\n+\t  if (GET_MODE_SIZE (rhs_mode) == fltsz)\n+\t    {\n+\t      if (!supportable_convert_operation (code, vectype_out,\n+\t\t\t\t\t\t  cvt_type, &decl1, &codecvt1))\n+\t\tgoto unsupported;\n+\t    }\n+\t  else if (!supportable_widening_operation (code, stmt, vectype_out,\n+\t\t\t\t\t\t    cvt_type, &decl1, &decl2,\n+\t\t\t\t\t\t    &codecvt1, &codecvt2,\n+\t\t\t\t\t\t    &multi_step_cvt,\n+\t\t\t\t\t\t    &interm_types))\n+\t    continue;\n+\t  else\n+\t    gcc_assert (multi_step_cvt == 0);\n+\n+\t  if (supportable_widening_operation (NOP_EXPR, stmt, cvt_type,\n+\t\t\t\t\t      vectype_in, NULL, NULL, &code1,\n+\t\t\t\t\t      &code2, &multi_step_cvt,\n+\t\t\t\t\t      &interm_types))\n+\t    break;\n+\t}\n+\n+      if (rhs_mode == VOIDmode || GET_MODE_SIZE (rhs_mode) > fltsz)\n+\tgoto unsupported;\n+\n+      if (GET_MODE_SIZE (rhs_mode) == fltsz)\n+\tcodecvt2 = ERROR_MARK;\n+      else\n+\t{\n+\t  multi_step_cvt++;\n+\t  VEC_safe_push (tree, heap, interm_types, cvt_type);\n+\t  cvt_type = NULL_TREE;\n+\t}\n+      break;\n+\n+    case NARROW:\n+      gcc_assert (op_type == unary_op);\n+      if (supportable_narrowing_operation (code, vectype_out, vectype_in,\n+\t\t\t\t\t   &code1, &multi_step_cvt,\n+\t\t\t\t\t   &interm_types))\n+\tbreak;\n+\n+      if (code != FIX_TRUNC_EXPR\n+\t  || (GET_MODE_SIZE (TYPE_MODE (lhs_type))\n+\t      >= GET_MODE_SIZE (TYPE_MODE (rhs_type))))\n+\tgoto unsupported;\n+\n+      rhs_mode = TYPE_MODE (rhs_type);\n+      cvt_type\n+\t= build_nonstandard_integer_type (GET_MODE_BITSIZE (rhs_mode), 0);\n+      cvt_type = get_same_sized_vectype (cvt_type, vectype_in);\n+      if (cvt_type == NULL_TREE)\n+\tgoto unsupported;\n+      if (!supportable_convert_operation (code, cvt_type, vectype_in,\n+\t\t\t\t\t  &decl1, &codecvt1))\n+\tgoto unsupported;\n+      if (supportable_narrowing_operation (NOP_EXPR, vectype_out, cvt_type,\n+\t\t\t\t\t   &code1, &multi_step_cvt,\n+\t\t\t\t\t   &interm_types))\n+\tbreak;\n+      goto unsupported;\n+\n+    default:\n+      gcc_unreachable ();\n     }\n \n   if (!vec_stmt)\t\t/* transformation not required.  */\n     {\n-      STMT_VINFO_TYPE (stmt_info) = type_conversion_vec_info_type;\n+      if (vect_print_dump_info (REPORT_DETAILS))\n+\tfprintf (vect_dump, \"=== vectorizable_conversion ===\");\n+      if (code == FIX_TRUNC_EXPR || code == FLOAT_EXPR)\n+\tSTMT_VINFO_TYPE (stmt_info) = type_conversion_vec_info_type;\n+      else if (modifier == NARROW)\n+\t{\n+\t  STMT_VINFO_TYPE (stmt_info) = type_demotion_vec_info_type;\n+\t  vect_model_simple_cost (stmt_info, ncopies, dt, NULL);\n+\t}\n+      else\n+\t{\n+\t  STMT_VINFO_TYPE (stmt_info) = type_promotion_vec_info_type;\n+\t  vect_model_simple_cost (stmt_info, 2 * ncopies, dt, NULL);\n+\t}\n+      VEC_free (tree, heap, interm_types);\n       return true;\n     }\n \n   /** Transform.  **/\n   if (vect_print_dump_info (REPORT_DETAILS))\n-    fprintf (vect_dump, \"transform conversion.\");\n+    fprintf (vect_dump, \"transform conversion. ncopies = %d.\", ncopies);\n \n-  /* Handle def.  */\n+  if (op_type == binary_op)\n+    {\n+      if (CONSTANT_CLASS_P (op0))\n+\top0 = fold_convert (TREE_TYPE (op1), op0);\n+      else if (CONSTANT_CLASS_P (op1))\n+\top1 = fold_convert (TREE_TYPE (op0), op1);\n+    }\n+\n+  /* In case of multi-step conversion, we first generate conversion operations\n+     to the intermediate types, and then from that types to the final one.\n+     We create vector destinations for the intermediate type (TYPES) received\n+     from supportable_*_operation, and store them in the correct order\n+     for future use in vect_create_vectorized_*_stmts ().  */\n+  vec_dsts = VEC_alloc (tree, heap, multi_step_cvt + 1);\n   vec_dest = vect_create_destination_var (scalar_dest, vectype_out);\n+  VEC_quick_push (tree, vec_dsts, vec_dest);\n+\n+  if (multi_step_cvt)\n+    {\n+      for (i = VEC_length (tree, interm_types) - 1;\n+\t   VEC_iterate (tree, interm_types, i, intermediate_type); i--)\n+\t{\n+\t  vec_dest = vect_create_destination_var (scalar_dest,\n+\t\t\t\t\t\t  intermediate_type);\n+\t  VEC_quick_push (tree, vec_dsts, vec_dest);\n+\t}\n+    }\n \n-  if (modifier == NONE && !slp_node)\n-    vec_oprnds0 = VEC_alloc (tree, heap, 1);\n+  if (cvt_type)\n+    vec_dest = vect_create_destination_var (scalar_dest, cvt_type);\n+\n+  if (!slp_node)\n+    {\n+      if (modifier == NONE)\n+\tvec_oprnds0 = VEC_alloc (tree, heap, 1);\n+      else if (modifier == WIDEN)\n+\t{\n+\t  vec_oprnds0 = VEC_alloc (tree, heap,\n+\t\t\t\t   (multi_step_cvt\n+\t\t\t\t    ? vect_pow2 (multi_step_cvt) : 1));\n+\t  if (op_type == binary_op)\n+\t    vec_oprnds1 = VEC_alloc (tree, heap, 1);\n+\t}\n+      else\n+\tvec_oprnds0 = VEC_alloc (tree, heap,\n+\t\t\t\t 2 * (multi_step_cvt\n+\t\t\t\t      ? vect_pow2 (multi_step_cvt) : 1));\n+    }\n+  else if (code == WIDEN_LSHIFT_EXPR)\n+    vec_oprnds1 = VEC_alloc (tree, heap, slp_node->vec_stmts_size);\n \n+  last_oprnd = op0;\n   prev_stmt_info = NULL;\n   switch (modifier)\n     {\n@@ -2011,28 +2363,29 @@ vectorizable_conversion (gimple stmt, gimple_stmt_iterator *gsi,\n \t    vect_get_vec_defs_for_stmt_copy (dt, &vec_oprnds0, NULL);\n \n \t  FOR_EACH_VEC_ELT (tree, vec_oprnds0, i, vop0)\n-          {\n-            /* Arguments are ready, create the new vector stmt.  */\n-            if (code1 == CALL_EXPR)\n-              {\n-                new_stmt = gimple_build_call (decl1, 1, vop0);\n-                new_temp = make_ssa_name (vec_dest, new_stmt);\n-                gimple_call_set_lhs (new_stmt, new_temp);\n-              }\n-            else\n-              {\n-                gcc_assert (TREE_CODE_LENGTH (code) == unary_op);\n-                new_stmt = gimple_build_assign_with_ops (code, vec_dest, vop0,\n-                                                        NULL);\n-                new_temp = make_ssa_name (vec_dest, new_stmt);\n-                gimple_assign_set_lhs (new_stmt, new_temp);\n-              }\n-\n-            vect_finish_stmt_generation (stmt, new_stmt, gsi);\n-            if (slp_node)\n-              VEC_quick_push (gimple, SLP_TREE_VEC_STMTS (slp_node), new_stmt);\n-          }\n-\n+\t    {\n+\t      /* Arguments are ready, create the new vector stmt.  */\n+\t      if (code1 == CALL_EXPR)\n+\t\t{\n+\t\t  new_stmt = gimple_build_call (decl1, 1, vop0);\n+\t\t  new_temp = make_ssa_name (vec_dest, new_stmt);\n+\t\t  gimple_call_set_lhs (new_stmt, new_temp);\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  gcc_assert (TREE_CODE_LENGTH (code1) == unary_op);\n+\t\t  new_stmt = gimple_build_assign_with_ops (code1, vec_dest,\n+\t\t\t\t\t\t\t   vop0, NULL);\n+\t\t  new_temp = make_ssa_name (vec_dest, new_stmt);\n+\t\t  gimple_assign_set_lhs (new_stmt, new_temp);\n+\t\t}\n+\n+\t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t      if (slp_node)\n+\t\tVEC_quick_push (gimple, SLP_TREE_VEC_STMTS (slp_node),\n+\t\t\t\tnew_stmt);\n+\t    }\n+\n \t  if (j == 0)\n \t    STMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = new_stmt;\n \t  else\n@@ -2048,30 +2401,117 @@ vectorizable_conversion (gimple stmt, gimple_stmt_iterator *gsi,\n \t the vector stmt by a factor VF/nunits.  */\n       for (j = 0; j < ncopies; j++)\n \t{\n+\t  /* Handle uses.  */\n \t  if (j == 0)\n-\t    vec_oprnd0 = vect_get_vec_def_for_operand (op0, stmt, NULL);\n-\t  else\n-\t    vec_oprnd0 = vect_get_vec_def_for_stmt_copy (dt[0], vec_oprnd0);\n+\t    {\n+\t      if (slp_node)\n+\t\t{\n+\t\t  if (code == WIDEN_LSHIFT_EXPR)\n+\t\t    {\n+\t\t      unsigned int k;\n \n-\t  /* Generate first half of the widened result:  */\n-\t  new_stmt\n-\t    = vect_gen_widened_results_half (code1, decl1,\n-\t\t\t\t\t     vec_oprnd0, vec_oprnd1,\n-\t\t\t\t\t     unary_op, vec_dest, gsi, stmt);\n-\t  if (j == 0)\n-\t    STMT_VINFO_VEC_STMT (stmt_info) = new_stmt;\n+\t\t      vec_oprnd1 = op1;\n+\t\t      /* Store vec_oprnd1 for every vector stmt to be created\n+\t\t\t for SLP_NODE.  We check during the analysis that all\n+\t\t\t the shift arguments are the same.  */\n+\t\t      for (k = 0; k < slp_node->vec_stmts_size - 1; k++)\n+\t\t\tVEC_quick_push (tree, vec_oprnds1, vec_oprnd1);\n+\n+\t\t      vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL,\n+\t\t\t\t\t slp_node, -1);\n+\t\t    }\n+\t\t  else\n+\t\t    vect_get_vec_defs (op0, op1, stmt, &vec_oprnds0,\n+\t\t\t\t       &vec_oprnds1, slp_node, -1);\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  vec_oprnd0 = vect_get_vec_def_for_operand (op0, stmt, NULL);\n+\t\t  VEC_quick_push (tree, vec_oprnds0, vec_oprnd0);\n+\t\t  if (op_type == binary_op)\n+\t\t    {\n+\t\t      if (code == WIDEN_LSHIFT_EXPR)\n+\t\t\tvec_oprnd1 = op1;\n+\t\t      else\n+\t\t\tvec_oprnd1 = vect_get_vec_def_for_operand (op1, stmt,\n+\t\t\t\t\t\t\t\t   NULL);\n+\t\t      VEC_quick_push (tree, vec_oprnds1, vec_oprnd1);\n+\t\t    }\n+\t\t}\n+\t    }\n \t  else\n-\t    STMT_VINFO_RELATED_STMT (prev_stmt_info) = new_stmt;\n-\t  prev_stmt_info = vinfo_for_stmt (new_stmt);\n+\t    {\n+\t      vec_oprnd0 = vect_get_vec_def_for_stmt_copy (dt[0], vec_oprnd0);\n+\t      VEC_truncate (tree, vec_oprnds0, 0);\n+\t      VEC_quick_push (tree, vec_oprnds0, vec_oprnd0);\n+\t      if (op_type == binary_op)\n+\t\t{\n+\t\t  if (code == WIDEN_LSHIFT_EXPR)\n+\t\t    vec_oprnd1 = op1;\n+\t\t  else\n+\t\t    vec_oprnd1 = vect_get_vec_def_for_stmt_copy (dt[1],\n+\t\t\t\t\t\t\t\t vec_oprnd1);\n+\t\t  VEC_truncate (tree, vec_oprnds1, 0);\n+\t\t  VEC_quick_push (tree, vec_oprnds1, vec_oprnd1);\n+\t\t}\n+\t    }\n \n-\t  /* Generate second half of the widened result:  */\n-\t  new_stmt\n-\t    = vect_gen_widened_results_half (code2, decl2,\n-\t\t\t\t\t     vec_oprnd0, vec_oprnd1,\n-\t\t\t\t\t     unary_op, vec_dest, gsi, stmt);\n-\t  STMT_VINFO_RELATED_STMT (prev_stmt_info) = new_stmt;\n-\t  prev_stmt_info = vinfo_for_stmt (new_stmt);\n+\t  /* Arguments are ready.  Create the new vector stmts.  */\n+\t  for (i = multi_step_cvt; i >= 0; i--)\n+\t    {\n+\t      tree this_dest = VEC_index (tree, vec_dsts, i);\n+\t      enum tree_code c1 = code1, c2 = code2;\n+\t      if (i == 0 && codecvt2 != ERROR_MARK)\n+\t\t{\n+\t\t  c1 = codecvt1;\n+\t\t  c2 = codecvt2;\n+\t\t}\n+\t      vect_create_vectorized_promotion_stmts (&vec_oprnds0,\n+\t\t\t\t\t\t      &vec_oprnds1,\n+\t\t\t\t\t\t      stmt, this_dest, gsi,\n+\t\t\t\t\t\t      c1, c2, decl1, decl2,\n+\t\t\t\t\t\t      op_type);\n+\t    }\n+\n+\t  FOR_EACH_VEC_ELT (tree, vec_oprnds0, i, vop0)\n+\t    {\n+\t      if (cvt_type)\n+\t\t{\n+\t\t  if (codecvt1 == CALL_EXPR)\n+\t\t    {\n+\t\t      new_stmt = gimple_build_call (decl1, 1, vop0);\n+\t\t      new_temp = make_ssa_name (vec_dest, new_stmt);\n+\t\t      gimple_call_set_lhs (new_stmt, new_temp);\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      gcc_assert (TREE_CODE_LENGTH (codecvt1) == unary_op);\n+\t\t      new_temp = make_ssa_name (vec_dest, NULL);\n+\t\t      new_stmt = gimple_build_assign_with_ops (codecvt1,\n+\t\t\t\t\t\t\t       new_temp,\n+\t\t\t\t\t\t\t       vop0, NULL);\n+\t\t    }\n+\n+\t\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t}\n+\t      else\n+\t\tnew_stmt = SSA_NAME_DEF_STMT (vop0);\n+\n+\t      if (slp_node)\n+\t\tVEC_quick_push (gimple, SLP_TREE_VEC_STMTS (slp_node),\n+\t\t\t\tnew_stmt);\n+\t      else\n+\t\t{\n+\t\t  if (!prev_stmt_info)\n+\t\t    STMT_VINFO_VEC_STMT (stmt_info) = new_stmt;\n+\t\t  else\n+\t\t    STMT_VINFO_RELATED_STMT (prev_stmt_info) = new_stmt;\n+\t\t  prev_stmt_info = vinfo_for_stmt (new_stmt);\n+\t\t}\n+\t    }\n \t}\n+\n+      *vec_stmt = STMT_VINFO_VEC_STMT (stmt_info);\n       break;\n \n     case NARROW:\n@@ -2082,37 +2522,52 @@ vectorizable_conversion (gimple stmt, gimple_stmt_iterator *gsi,\n       for (j = 0; j < ncopies; j++)\n \t{\n \t  /* Handle uses.  */\n-\t  if (j == 0)\n-\t    {\n-\t      vec_oprnd0 = vect_get_vec_def_for_operand (op0, stmt, NULL);\n-\t      vec_oprnd1 = vect_get_vec_def_for_stmt_copy (dt[0], vec_oprnd0);\n-\t    }\n+\t  if (slp_node)\n+\t    vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL,\n+\t\t\t       slp_node, -1);\n \t  else\n \t    {\n-\t      vec_oprnd0 = vect_get_vec_def_for_stmt_copy (dt[0], vec_oprnd1);\n-\t      vec_oprnd1 = vect_get_vec_def_for_stmt_copy (dt[0], vec_oprnd0);\n+\t      VEC_truncate (tree, vec_oprnds0, 0);\n+\t      vect_get_loop_based_defs (&last_oprnd, stmt, dt[0], &vec_oprnds0,\n+\t\t\t\t\tvect_pow2 (multi_step_cvt) - 1);\n \t    }\n \n-\t  /* Arguments are ready.  Create the new vector stmt.  */\n-\t  new_stmt = gimple_build_assign_with_ops (code1, vec_dest, vec_oprnd0,\n-\t\t\t\t\t\t   vec_oprnd1);\n-\t  new_temp = make_ssa_name (vec_dest, new_stmt);\n-\t  gimple_assign_set_lhs (new_stmt, new_temp);\n-\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t  /* Arguments are ready.  Create the new vector stmts.  */\n+\t  if (cvt_type)\n+\t    FOR_EACH_VEC_ELT (tree, vec_oprnds0, i, vop0)\n+\t      {\n+\t\tif (codecvt1 == CALL_EXPR)\n+\t\t  {\n+\t\t    new_stmt = gimple_build_call (decl1, 1, vop0);\n+\t\t    new_temp = make_ssa_name (vec_dest, new_stmt);\n+\t\t    gimple_call_set_lhs (new_stmt, new_temp);\n+\t\t  }\n+\t\telse\n+\t\t  {\n+\t\t    gcc_assert (TREE_CODE_LENGTH (codecvt1) == unary_op);\n+\t\t    new_temp = make_ssa_name (vec_dest, NULL);\n+\t\t    new_stmt = gimple_build_assign_with_ops (codecvt1, new_temp,\n+\t\t\t\t\t\t\t     vop0, NULL);\n+\t\t  }\n \n-\t  if (j == 0)\n-\t    STMT_VINFO_VEC_STMT (stmt_info) = new_stmt;\n-\t  else\n-\t    STMT_VINFO_RELATED_STMT (prev_stmt_info) = new_stmt;\n+\t\tvect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\tVEC_replace (tree, vec_oprnds0, i, new_temp);\n+\t      }\n \n-\t  prev_stmt_info = vinfo_for_stmt (new_stmt);\n+\t  vect_create_vectorized_demotion_stmts (&vec_oprnds0, multi_step_cvt,\n+\t\t\t\t\t\t stmt, vec_dsts, gsi,\n+\t\t\t\t\t\t slp_node, code1,\n+\t\t\t\t\t\t &prev_stmt_info);\n \t}\n \n       *vec_stmt = STMT_VINFO_VEC_STMT (stmt_info);\n+      break;\n     }\n \n-  if (vec_oprnds0)\n-    VEC_free (tree, heap, vec_oprnds0);\n+  VEC_free (tree, heap, vec_oprnds0);\n+  VEC_free (tree, heap, vec_oprnds1);\n+  VEC_free (tree, heap, vec_dsts);\n+  VEC_free (tree, heap, interm_types);\n \n   return true;\n }\n@@ -2855,851 +3310,168 @@ vectorizable_operation (gimple stmt, gimple_stmt_iterator *gsi,\n               && !vec_stmt))\n         return false;\n       if (vect_print_dump_info (REPORT_DETAILS))\n-\tfprintf (vect_dump, \"proceeding using word mode.\");\n-    }\n-\n-  /* Worthwhile without SIMD support?  Check only during analysis.  */\n-  if (!VECTOR_MODE_P (TYPE_MODE (vectype))\n-      && vf < vect_min_worthwhile_factor (code)\n-      && !vec_stmt)\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\tfprintf (vect_dump, \"not worthwhile without SIMD support.\");\n-      return false;\n-    }\n-\n-  if (!vec_stmt) /* transformation not required.  */\n-    {\n-      STMT_VINFO_TYPE (stmt_info) = op_vec_info_type;\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"=== vectorizable_operation ===\");\n-      vect_model_simple_cost (stmt_info, ncopies, dt, NULL);\n-      return true;\n-    }\n-\n-  /** Transform.  **/\n-\n-  if (vect_print_dump_info (REPORT_DETAILS))\n-    fprintf (vect_dump, \"transform binary/unary operation.\");\n-\n-  /* Handle def.  */\n-  vec_dest = vect_create_destination_var (scalar_dest, vectype);\n-\n-  /* Allocate VECs for vector operands.  In case of SLP, vector operands are\n-     created in the previous stages of the recursion, so no allocation is\n-     needed, except for the case of shift with scalar shift argument.  In that\n-     case we store the scalar operand in VEC_OPRNDS1 for every vector stmt to\n-     be created to vectorize the SLP group, i.e., SLP_NODE->VEC_STMTS_SIZE.\n-     In case of loop-based vectorization we allocate VECs of size 1.  We\n-     allocate VEC_OPRNDS1 only in case of binary operation.  */\n-  if (!slp_node)\n-    {\n-      vec_oprnds0 = VEC_alloc (tree, heap, 1);\n-      if (op_type == binary_op || op_type == ternary_op)\n-        vec_oprnds1 = VEC_alloc (tree, heap, 1);\n-      if (op_type == ternary_op)\n-        vec_oprnds2 = VEC_alloc (tree, heap, 1);\n-    }\n-\n-  /* In case the vectorization factor (VF) is bigger than the number\n-     of elements that we can fit in a vectype (nunits), we have to generate\n-     more than one vector stmt - i.e - we need to \"unroll\" the\n-     vector stmt by a factor VF/nunits.  In doing so, we record a pointer\n-     from one copy of the vector stmt to the next, in the field\n-     STMT_VINFO_RELATED_STMT.  This is necessary in order to allow following\n-     stages to find the correct vector defs to be used when vectorizing\n-     stmts that use the defs of the current stmt.  The example below\n-     illustrates the vectorization process when VF=16 and nunits=4 (i.e.,\n-     we need to create 4 vectorized stmts):\n-\n-     before vectorization:\n-                                RELATED_STMT    VEC_STMT\n-        S1:     x = memref      -               -\n-        S2:     z = x + 1       -               -\n-\n-     step 1: vectorize stmt S1 (done in vectorizable_load. See more details\n-             there):\n-                                RELATED_STMT    VEC_STMT\n-        VS1_0:  vx0 = memref0   VS1_1           -\n-        VS1_1:  vx1 = memref1   VS1_2           -\n-        VS1_2:  vx2 = memref2   VS1_3           -\n-        VS1_3:  vx3 = memref3   -               -\n-        S1:     x = load        -               VS1_0\n-        S2:     z = x + 1       -               -\n-\n-     step2: vectorize stmt S2 (done here):\n-        To vectorize stmt S2 we first need to find the relevant vector\n-        def for the first operand 'x'.  This is, as usual, obtained from\n-        the vector stmt recorded in the STMT_VINFO_VEC_STMT of the stmt\n-        that defines 'x' (S1).  This way we find the stmt VS1_0, and the\n-        relevant vector def 'vx0'.  Having found 'vx0' we can generate\n-        the vector stmt VS2_0, and as usual, record it in the\n-        STMT_VINFO_VEC_STMT of stmt S2.\n-        When creating the second copy (VS2_1), we obtain the relevant vector\n-        def from the vector stmt recorded in the STMT_VINFO_RELATED_STMT of\n-        stmt VS1_0.  This way we find the stmt VS1_1 and the relevant\n-        vector def 'vx1'.  Using 'vx1' we create stmt VS2_1 and record a\n-        pointer to it in the STMT_VINFO_RELATED_STMT of the vector stmt VS2_0.\n-        Similarly when creating stmts VS2_2 and VS2_3.  This is the resulting\n-        chain of stmts and pointers:\n-                                RELATED_STMT    VEC_STMT\n-        VS1_0:  vx0 = memref0   VS1_1           -\n-        VS1_1:  vx1 = memref1   VS1_2           -\n-        VS1_2:  vx2 = memref2   VS1_3           -\n-        VS1_3:  vx3 = memref3   -               -\n-        S1:     x = load        -               VS1_0\n-        VS2_0:  vz0 = vx0 + v1  VS2_1           -\n-        VS2_1:  vz1 = vx1 + v1  VS2_2           -\n-        VS2_2:  vz2 = vx2 + v1  VS2_3           -\n-        VS2_3:  vz3 = vx3 + v1  -               -\n-        S2:     z = x + 1       -               VS2_0  */\n-\n-  prev_stmt_info = NULL;\n-  for (j = 0; j < ncopies; j++)\n-    {\n-      /* Handle uses.  */\n-      if (j == 0)\n-\t{\n-\t  if (op_type == binary_op || op_type == ternary_op)\n-\t    vect_get_vec_defs (op0, op1, stmt, &vec_oprnds0, &vec_oprnds1,\n-\t\t\t       slp_node, -1);\n-\t  else\n-\t    vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL,\n-\t\t\t       slp_node, -1);\n-\t  if (op_type == ternary_op)\n-\t    {\n-\t      vec_oprnds2 = VEC_alloc (tree, heap, 1);\n-\t      VEC_quick_push (tree, vec_oprnds2,\n-\t\t\t      vect_get_vec_def_for_operand (op2, stmt, NULL));\n-\t    }\n-\t}\n-      else\n-\t{\n-\t  vect_get_vec_defs_for_stmt_copy (dt, &vec_oprnds0, &vec_oprnds1);\n-\t  if (op_type == ternary_op)\n-\t    {\n-\t      tree vec_oprnd = VEC_pop (tree, vec_oprnds2);\n-\t      VEC_quick_push (tree, vec_oprnds2,\n-\t\t\t      vect_get_vec_def_for_stmt_copy (dt[2],\n-\t\t\t\t\t\t\t      vec_oprnd));\n-\t    }\n-\t}\n-\n-      /* Arguments are ready.  Create the new vector stmt.  */\n-      FOR_EACH_VEC_ELT (tree, vec_oprnds0, i, vop0)\n-        {\n-\t  vop1 = ((op_type == binary_op || op_type == ternary_op)\n-\t\t  ? VEC_index (tree, vec_oprnds1, i) : NULL_TREE);\n-\t  vop2 = ((op_type == ternary_op)\n-\t\t  ? VEC_index (tree, vec_oprnds2, i) : NULL_TREE);\n-\t  new_stmt = gimple_build_assign_with_ops3 (code, vec_dest,\n-\t\t\t\t\t\t    vop0, vop1, vop2);\n-\t  new_temp = make_ssa_name (vec_dest, new_stmt);\n-\t  gimple_assign_set_lhs (new_stmt, new_temp);\n-\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n-          if (slp_node)\n-\t    VEC_quick_push (gimple, SLP_TREE_VEC_STMTS (slp_node), new_stmt);\n-        }\n-\n-      if (slp_node)\n-        continue;\n-\n-      if (j == 0)\n-\tSTMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = new_stmt;\n-      else\n-\tSTMT_VINFO_RELATED_STMT (prev_stmt_info) = new_stmt;\n-      prev_stmt_info = vinfo_for_stmt (new_stmt);\n-    }\n-\n-  VEC_free (tree, heap, vec_oprnds0);\n-  if (vec_oprnds1)\n-    VEC_free (tree, heap, vec_oprnds1);\n-  if (vec_oprnds2)\n-    VEC_free (tree, heap, vec_oprnds2);\n-\n-  return true;\n-}\n-\n-\n-/* Get vectorized definitions for loop-based vectorization.  For the first\n-   operand we call vect_get_vec_def_for_operand() (with OPRND containing\n-   scalar operand), and for the rest we get a copy with\n-   vect_get_vec_def_for_stmt_copy() using the previous vector definition\n-   (stored in OPRND). See vect_get_vec_def_for_stmt_copy() for details.\n-   The vectors are collected into VEC_OPRNDS.  */\n-\n-static void\n-vect_get_loop_based_defs (tree *oprnd, gimple stmt, enum vect_def_type dt,\n-                          VEC (tree, heap) **vec_oprnds, int multi_step_cvt)\n-{\n-  tree vec_oprnd;\n-\n-  /* Get first vector operand.  */\n-  /* All the vector operands except the very first one (that is scalar oprnd)\n-     are stmt copies.  */\n-  if (TREE_CODE (TREE_TYPE (*oprnd)) != VECTOR_TYPE)\n-    vec_oprnd = vect_get_vec_def_for_operand (*oprnd, stmt, NULL);\n-  else\n-    vec_oprnd = vect_get_vec_def_for_stmt_copy (dt, *oprnd);\n-\n-  VEC_quick_push (tree, *vec_oprnds, vec_oprnd);\n-\n-  /* Get second vector operand.  */\n-  vec_oprnd = vect_get_vec_def_for_stmt_copy (dt, vec_oprnd);\n-  VEC_quick_push (tree, *vec_oprnds, vec_oprnd);\n-\n-  *oprnd = vec_oprnd;\n-\n-  /* For conversion in multiple steps, continue to get operands\n-     recursively.  */\n-  if (multi_step_cvt)\n-    vect_get_loop_based_defs (oprnd, stmt, dt, vec_oprnds,  multi_step_cvt - 1);\n-}\n-\n-\n-/* Create vectorized demotion statements for vector operands from VEC_OPRNDS.\n-   For multi-step conversions store the resulting vectors and call the function\n-   recursively.  */\n-\n-static void\n-vect_create_vectorized_demotion_stmts (VEC (tree, heap) **vec_oprnds,\n-                                       int multi_step_cvt, gimple stmt,\n-                                       VEC (tree, heap) *vec_dsts,\n-                                       gimple_stmt_iterator *gsi,\n-                                       slp_tree slp_node, enum tree_code code,\n-                                       stmt_vec_info *prev_stmt_info)\n-{\n-  unsigned int i;\n-  tree vop0, vop1, new_tmp, vec_dest;\n-  gimple new_stmt;\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n-\n-  vec_dest = VEC_pop (tree, vec_dsts);\n-\n-  for (i = 0; i < VEC_length (tree, *vec_oprnds); i += 2)\n-    {\n-      /* Create demotion operation.  */\n-      vop0 = VEC_index (tree, *vec_oprnds, i);\n-      vop1 = VEC_index (tree, *vec_oprnds, i + 1);\n-      new_stmt = gimple_build_assign_with_ops (code, vec_dest, vop0, vop1);\n-      new_tmp = make_ssa_name (vec_dest, new_stmt);\n-      gimple_assign_set_lhs (new_stmt, new_tmp);\n-      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n-\n-      if (multi_step_cvt)\n-        /* Store the resulting vector for next recursive call.  */\n-        VEC_replace (tree, *vec_oprnds, i/2, new_tmp);\n-      else\n-        {\n-          /* This is the last step of the conversion sequence. Store the\n-             vectors in SLP_NODE or in vector info of the scalar statement\n-             (or in STMT_VINFO_RELATED_STMT chain).  */\n-          if (slp_node)\n-            VEC_quick_push (gimple, SLP_TREE_VEC_STMTS (slp_node), new_stmt);\n-          else\n-            {\n-              if (!*prev_stmt_info)\n-                STMT_VINFO_VEC_STMT (stmt_info) = new_stmt;\n-              else\n-                STMT_VINFO_RELATED_STMT (*prev_stmt_info) = new_stmt;\n-\n-              *prev_stmt_info = vinfo_for_stmt (new_stmt);\n-            }\n-        }\n-    }\n-\n-  /* For multi-step demotion operations we first generate demotion operations\n-     from the source type to the intermediate types, and then combine the\n-     results (stored in VEC_OPRNDS) in demotion operation to the destination\n-     type.  */\n-  if (multi_step_cvt)\n-    {\n-      /* At each level of recursion we have have of the operands we had at the\n-         previous level.  */\n-      VEC_truncate (tree, *vec_oprnds, (i+1)/2);\n-      vect_create_vectorized_demotion_stmts (vec_oprnds, multi_step_cvt - 1,\n-                                             stmt, vec_dsts, gsi, slp_node,\n-                                             code, prev_stmt_info);\n-    }\n-}\n-\n-\n-/* Function vectorizable_type_demotion\n-\n-   Check if STMT performs a binary or unary operation that involves\n-   type demotion, and if it can be vectorized.\n-   If VEC_STMT is also passed, vectorize the STMT: create a vectorized\n-   stmt to replace it, put it in VEC_STMT, and insert it at BSI.\n-   Return FALSE if not a vectorizable STMT, TRUE otherwise.  */\n-\n-static bool\n-vectorizable_type_demotion (gimple stmt, gimple_stmt_iterator *gsi,\n-\t\t\t    gimple *vec_stmt, slp_tree slp_node)\n-{\n-  tree vec_dest;\n-  tree scalar_dest;\n-  tree op0;\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n-  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n-  enum tree_code code, code1 = ERROR_MARK;\n-  tree def;\n-  gimple def_stmt;\n-  enum vect_def_type dt[2] = {vect_unknown_def_type, vect_unknown_def_type};\n-  stmt_vec_info prev_stmt_info;\n-  int nunits_in;\n-  int nunits_out;\n-  tree vectype_out;\n-  int ncopies;\n-  int j, i;\n-  tree vectype_in;\n-  int multi_step_cvt = 0;\n-  VEC (tree, heap) *vec_oprnds0 = NULL;\n-  VEC (tree, heap) *vec_dsts = NULL, *interm_types = NULL, *tmp_vec_dsts = NULL;\n-  tree last_oprnd, intermediate_type;\n-  bb_vec_info bb_vinfo = STMT_VINFO_BB_VINFO (stmt_info);\n-\n-  if (!STMT_VINFO_RELEVANT_P (stmt_info) && !bb_vinfo)\n-    return false;\n-\n-  if (STMT_VINFO_DEF_TYPE (stmt_info) != vect_internal_def)\n-    return false;\n-\n-  /* Is STMT a vectorizable type-demotion operation?  */\n-  if (!is_gimple_assign (stmt))\n-    return false;\n-\n-  if (TREE_CODE (gimple_assign_lhs (stmt)) != SSA_NAME)\n-    return false;\n-\n-  code = gimple_assign_rhs_code (stmt);\n-  if (!CONVERT_EXPR_CODE_P (code))\n-    return false;\n-\n-  scalar_dest = gimple_assign_lhs (stmt);\n-  vectype_out = STMT_VINFO_VECTYPE (stmt_info);\n-\n-  /* Check the operands of the operation.  */\n-  op0 = gimple_assign_rhs1 (stmt);\n-  if (! ((INTEGRAL_TYPE_P (TREE_TYPE (scalar_dest))\n-\t  && INTEGRAL_TYPE_P (TREE_TYPE (op0)))\n-\t || (SCALAR_FLOAT_TYPE_P (TREE_TYPE (scalar_dest))\n-\t     && SCALAR_FLOAT_TYPE_P (TREE_TYPE (op0)))))\n-    return false;\n-\n-  if (INTEGRAL_TYPE_P (TREE_TYPE (scalar_dest))\n-      && ((TYPE_PRECISION (TREE_TYPE (scalar_dest))\n-\t   != GET_MODE_PRECISION (TYPE_MODE (TREE_TYPE (scalar_dest))))\n-\t  || ((TYPE_PRECISION (TREE_TYPE (op0))\n-\t       != GET_MODE_PRECISION (TYPE_MODE (TREE_TYPE (op0)))))))\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"type demotion to/from bit-precision unsupported.\");\n-      return false;\n-    }\n-\n-  if (!vect_is_simple_use_1 (op0, loop_vinfo, bb_vinfo,\n-\t\t\t     &def_stmt, &def, &dt[0], &vectype_in))\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"use not simple.\");\n-      return false;\n-    }\n-  /* If op0 is an external def use a vector type with the\n-     same size as the output vector type if possible.  */\n-  if (!vectype_in)\n-    vectype_in = get_same_sized_vectype (TREE_TYPE (op0), vectype_out);\n-  if (vec_stmt)\n-    gcc_assert (vectype_in);\n-  if (!vectype_in)\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        {\n-          fprintf (vect_dump, \"no vectype for scalar type \");\n-          print_generic_expr (vect_dump, TREE_TYPE (op0), TDF_SLIM);\n-        }\n-\n-      return false;\n-    }\n-\n-  nunits_in = TYPE_VECTOR_SUBPARTS (vectype_in);\n-  nunits_out = TYPE_VECTOR_SUBPARTS (vectype_out);\n-  if (nunits_in >= nunits_out)\n-    return false;\n-\n-  /* Multiple types in SLP are handled by creating the appropriate number of\n-     vectorized stmts for each SLP node.  Hence, NCOPIES is always 1 in\n-     case of SLP.  */\n-  if (slp_node || PURE_SLP_STMT (stmt_info))\n-    ncopies = 1;\n-  else\n-    ncopies = LOOP_VINFO_VECT_FACTOR (loop_vinfo) / nunits_out;\n-  gcc_assert (ncopies >= 1);\n-\n-  /* Supportable by target?  */\n-  if (!supportable_narrowing_operation (code, vectype_out, vectype_in,\n-\t\t\t\t\t&code1, &multi_step_cvt, &interm_types))\n-    return false;\n-\n-  if (!vec_stmt) /* transformation not required.  */\n-    {\n-      STMT_VINFO_TYPE (stmt_info) = type_demotion_vec_info_type;\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"=== vectorizable_demotion ===\");\n-      vect_model_simple_cost (stmt_info, ncopies, dt, NULL);\n-      return true;\n-    }\n-\n-  /** Transform.  **/\n-  if (vect_print_dump_info (REPORT_DETAILS))\n-    fprintf (vect_dump, \"transform type demotion operation. ncopies = %d.\",\n-\t     ncopies);\n-\n-  /* In case of multi-step demotion, we first generate demotion operations to\n-     the intermediate types, and then from that types to the final one.\n-     We create vector destinations for the intermediate type (TYPES) received\n-     from supportable_narrowing_operation, and store them in the correct order\n-     for future use in vect_create_vectorized_demotion_stmts().  */\n-  if (multi_step_cvt)\n-    vec_dsts = VEC_alloc (tree, heap, multi_step_cvt + 1);\n-  else\n-    vec_dsts = VEC_alloc (tree, heap, 1);\n-\n-  vec_dest = vect_create_destination_var (scalar_dest, vectype_out);\n-  VEC_quick_push (tree, vec_dsts, vec_dest);\n-\n-  if (multi_step_cvt)\n-    {\n-      for (i = VEC_length (tree, interm_types) - 1;\n-           VEC_iterate (tree, interm_types, i, intermediate_type); i--)\n-        {\n-          vec_dest = vect_create_destination_var (scalar_dest,\n-                                                  intermediate_type);\n-          VEC_quick_push (tree, vec_dsts, vec_dest);\n-        }\n-    }\n-\n-  /* In case the vectorization factor (VF) is bigger than the number\n-     of elements that we can fit in a vectype (nunits), we have to generate\n-     more than one vector stmt - i.e - we need to \"unroll\" the\n-     vector stmt by a factor VF/nunits.   */\n-  last_oprnd = op0;\n-  prev_stmt_info = NULL;\n-  for (j = 0; j < ncopies; j++)\n-    {\n-      /* Handle uses.  */\n-      if (slp_node)\n-        vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL,\n-\t\t\t   slp_node, -1);\n-      else\n-        {\n-          VEC_free (tree, heap, vec_oprnds0);\n-          vec_oprnds0 = VEC_alloc (tree, heap,\n-                        (multi_step_cvt ? vect_pow2 (multi_step_cvt) * 2 : 2));\n-          vect_get_loop_based_defs (&last_oprnd, stmt, dt[0], &vec_oprnds0,\n-                                    vect_pow2 (multi_step_cvt) - 1);\n-        }\n-\n-      /* Arguments are ready.  Create the new vector stmts.  */\n-      tmp_vec_dsts = VEC_copy (tree, heap, vec_dsts);\n-      vect_create_vectorized_demotion_stmts (&vec_oprnds0,\n-                                             multi_step_cvt, stmt, tmp_vec_dsts,\n-                                             gsi, slp_node, code1,\n-                                             &prev_stmt_info);\n-    }\n-\n-  VEC_free (tree, heap, vec_oprnds0);\n-  VEC_free (tree, heap, vec_dsts);\n-  VEC_free (tree, heap, tmp_vec_dsts);\n-  VEC_free (tree, heap, interm_types);\n-\n-  *vec_stmt = STMT_VINFO_VEC_STMT (stmt_info);\n-  return true;\n-}\n-\n-\n-/* Create vectorized promotion statements for vector operands from VEC_OPRNDS0\n-   and VEC_OPRNDS1 (for binary operations).  For multi-step conversions store\n-   the resulting vectors and call the function recursively.  */\n-\n-static void\n-vect_create_vectorized_promotion_stmts (VEC (tree, heap) **vec_oprnds0,\n-                                        VEC (tree, heap) **vec_oprnds1,\n-                                        int multi_step_cvt, gimple stmt,\n-                                        VEC (tree, heap) *vec_dsts,\n-                                        gimple_stmt_iterator *gsi,\n-                                        slp_tree slp_node, enum tree_code code1,\n-                                        enum tree_code code2, tree decl1,\n-                                        tree decl2, int op_type,\n-                                        stmt_vec_info *prev_stmt_info)\n-{\n-  int i;\n-  tree vop0, vop1, new_tmp1, new_tmp2, vec_dest;\n-  gimple new_stmt1, new_stmt2;\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n-  VEC (tree, heap) *vec_tmp;\n-\n-  vec_dest = VEC_pop (tree, vec_dsts);\n-  vec_tmp = VEC_alloc (tree, heap, VEC_length (tree, *vec_oprnds0) * 2);\n-\n-  FOR_EACH_VEC_ELT (tree, *vec_oprnds0, i, vop0)\n-    {\n-      if (op_type == binary_op)\n-        vop1 = VEC_index (tree, *vec_oprnds1, i);\n-      else\n-        vop1 = NULL_TREE;\n-\n-      /* Generate the two halves of promotion operation.  */\n-      new_stmt1 = vect_gen_widened_results_half (code1, decl1, vop0, vop1,\n-                                                 op_type, vec_dest, gsi, stmt);\n-      new_stmt2 = vect_gen_widened_results_half (code2, decl2, vop0, vop1,\n-                                                 op_type, vec_dest, gsi, stmt);\n-      if (is_gimple_call (new_stmt1))\n-        {\n-          new_tmp1 = gimple_call_lhs (new_stmt1);\n-          new_tmp2 = gimple_call_lhs (new_stmt2);\n-        }\n-      else\n-        {\n-          new_tmp1 = gimple_assign_lhs (new_stmt1);\n-          new_tmp2 = gimple_assign_lhs (new_stmt2);\n-        }\n-\n-      if (multi_step_cvt)\n-        {\n-          /* Store the results for the recursive call.  */\n-          VEC_quick_push (tree, vec_tmp, new_tmp1);\n-          VEC_quick_push (tree, vec_tmp, new_tmp2);\n-        }\n-      else\n-        {\n-          /* Last step of promotion sequience - store the results.  */\n-          if (slp_node)\n-            {\n-              VEC_quick_push (gimple, SLP_TREE_VEC_STMTS (slp_node), new_stmt1);\n-              VEC_quick_push (gimple, SLP_TREE_VEC_STMTS (slp_node), new_stmt2);\n-            }\n-          else\n-            {\n-              if (!*prev_stmt_info)\n-                STMT_VINFO_VEC_STMT (stmt_info) = new_stmt1;\n-              else\n-                STMT_VINFO_RELATED_STMT (*prev_stmt_info) = new_stmt1;\n-\n-              *prev_stmt_info = vinfo_for_stmt (new_stmt1);\n-              STMT_VINFO_RELATED_STMT (*prev_stmt_info) = new_stmt2;\n-              *prev_stmt_info = vinfo_for_stmt (new_stmt2);\n-            }\n-        }\n-    }\n-\n-  if (multi_step_cvt)\n-    {\n-      /* For multi-step promotion operation we first generate we call the\n-         function recurcively for every stage.  We start from the input type,\n-         create promotion operations to the intermediate types, and then\n-         create promotions to the output type.  */\n-      *vec_oprnds0 = VEC_copy (tree, heap, vec_tmp);\n-      vect_create_vectorized_promotion_stmts (vec_oprnds0, vec_oprnds1,\n-                                              multi_step_cvt - 1, stmt,\n-                                              vec_dsts, gsi, slp_node, code1,\n-                                              code2, decl2, decl2, op_type,\n-                                              prev_stmt_info);\n-    }\n-\n-  VEC_free (tree, heap, vec_tmp);\n-}\n-\n-\n-/* Function vectorizable_type_promotion\n-\n-   Check if STMT performs a binary or unary operation that involves\n-   type promotion, and if it can be vectorized.\n-   If VEC_STMT is also passed, vectorize the STMT: create a vectorized\n-   stmt to replace it, put it in VEC_STMT, and insert it at BSI.\n-   Return FALSE if not a vectorizable STMT, TRUE otherwise.  */\n-\n-static bool\n-vectorizable_type_promotion (gimple stmt, gimple_stmt_iterator *gsi,\n-\t\t\t     gimple *vec_stmt, slp_tree slp_node)\n-{\n-  tree vec_dest;\n-  tree scalar_dest;\n-  tree op0, op1 = NULL;\n-  tree vec_oprnd0=NULL, vec_oprnd1=NULL;\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n-  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n-  enum tree_code code, code1 = ERROR_MARK, code2 = ERROR_MARK;\n-  tree decl1 = NULL_TREE, decl2 = NULL_TREE;\n-  int op_type;\n-  tree def;\n-  gimple def_stmt;\n-  enum vect_def_type dt[2] = {vect_unknown_def_type, vect_unknown_def_type};\n-  stmt_vec_info prev_stmt_info;\n-  int nunits_in;\n-  int nunits_out;\n-  tree vectype_out;\n-  int ncopies;\n-  int j, i;\n-  tree vectype_in;\n-  tree intermediate_type = NULL_TREE;\n-  int multi_step_cvt = 0;\n-  VEC (tree, heap) *vec_oprnds0 = NULL, *vec_oprnds1 = NULL;\n-  VEC (tree, heap) *vec_dsts = NULL, *interm_types = NULL, *tmp_vec_dsts = NULL;\n-  bb_vec_info bb_vinfo = STMT_VINFO_BB_VINFO (stmt_info);\n-  unsigned int k;\n-\n-  if (!STMT_VINFO_RELEVANT_P (stmt_info) && !bb_vinfo)\n-    return false;\n-\n-  if (STMT_VINFO_DEF_TYPE (stmt_info) != vect_internal_def)\n-    return false;\n-\n-  /* Is STMT a vectorizable type-promotion operation?  */\n-  if (!is_gimple_assign (stmt))\n-    return false;\n-\n-  if (TREE_CODE (gimple_assign_lhs (stmt)) != SSA_NAME)\n-    return false;\n-\n-  code = gimple_assign_rhs_code (stmt);\n-  if (!CONVERT_EXPR_CODE_P (code)\n-      && code != WIDEN_MULT_EXPR\n-      && code != WIDEN_LSHIFT_EXPR)\n-    return false;\n-\n-  scalar_dest = gimple_assign_lhs (stmt);\n-  vectype_out = STMT_VINFO_VECTYPE (stmt_info);\n-\n-  /* Check the operands of the operation.  */\n-  op0 = gimple_assign_rhs1 (stmt);\n-  if (! ((INTEGRAL_TYPE_P (TREE_TYPE (scalar_dest))\n-\t  && INTEGRAL_TYPE_P (TREE_TYPE (op0)))\n-\t || (SCALAR_FLOAT_TYPE_P (TREE_TYPE (scalar_dest))\n-\t     && SCALAR_FLOAT_TYPE_P (TREE_TYPE (op0))\n-\t     && CONVERT_EXPR_CODE_P (code))))\n-    return false;\n-\n-  if (INTEGRAL_TYPE_P (TREE_TYPE (scalar_dest))\n-      && ((TYPE_PRECISION (TREE_TYPE (scalar_dest))\n-\t   != GET_MODE_PRECISION (TYPE_MODE (TREE_TYPE (scalar_dest))))\n-\t  || ((TYPE_PRECISION (TREE_TYPE (op0))\n-\t       != GET_MODE_PRECISION (TYPE_MODE (TREE_TYPE (op0)))))))\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"type promotion to/from bit-precision \"\n-\t\t \"unsupported.\");\n-      return false;\n-    }\n-\n-  if (!vect_is_simple_use_1 (op0, loop_vinfo, bb_vinfo,\n-\t\t\t     &def_stmt, &def, &dt[0], &vectype_in))\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\tfprintf (vect_dump, \"use not simple.\");\n-      return false;\n-    }\n-\n-  op_type = TREE_CODE_LENGTH (code);\n-  if (op_type == binary_op)\n-    {\n-      bool ok;\n-\n-      op1 = gimple_assign_rhs2 (stmt);\n-      if (code == WIDEN_MULT_EXPR || code == WIDEN_LSHIFT_EXPR)\n-        {\n-\t  /* For WIDEN_MULT_EXPR, if OP0 is a constant, use the type of\n-\t     OP1.  */\n-          if (CONSTANT_CLASS_P (op0))\n-            ok = vect_is_simple_use_1 (op1, loop_vinfo, NULL,\n-                             &def_stmt, &def, &dt[1], &vectype_in);\n-          else\n-            ok = vect_is_simple_use (op1, loop_vinfo, NULL, &def_stmt, &def,\n-                                     &dt[1]);\n-\n-          if (!ok)\n-            {\n-\t      if (vect_print_dump_info (REPORT_DETAILS))\n-\t        fprintf (vect_dump, \"use not simple.\");\n-              return false;\n-            }\n-        }        \n+\tfprintf (vect_dump, \"proceeding using word mode.\");\n     }\n \n-  /* If op0 is an external or constant def use a vector type with\n-     the same size as the output vector type.  */\n-  if (!vectype_in)\n-    vectype_in = get_same_sized_vectype (TREE_TYPE (op0), vectype_out);\n-  if (vec_stmt)\n-    gcc_assert (vectype_in);\n-  if (!vectype_in)\n+  /* Worthwhile without SIMD support?  Check only during analysis.  */\n+  if (!VECTOR_MODE_P (TYPE_MODE (vectype))\n+      && vf < vect_min_worthwhile_factor (code)\n+      && !vec_stmt)\n     {\n       if (vect_print_dump_info (REPORT_DETAILS))\n-        {\n-          fprintf (vect_dump, \"no vectype for scalar type \");\n-          print_generic_expr (vect_dump, TREE_TYPE (op0), TDF_SLIM);\n-        }\n-\n+\tfprintf (vect_dump, \"not worthwhile without SIMD support.\");\n       return false;\n     }\n \n-  nunits_in = TYPE_VECTOR_SUBPARTS (vectype_in);\n-  nunits_out = TYPE_VECTOR_SUBPARTS (vectype_out);\n-  if (nunits_in <= nunits_out)\n-    return false;\n-\n-  /* Multiple types in SLP are handled by creating the appropriate number of\n-     vectorized stmts for each SLP node.  Hence, NCOPIES is always 1 in\n-     case of SLP.  */\n-  if (slp_node || PURE_SLP_STMT (stmt_info))\n-    ncopies = 1;\n-  else\n-    ncopies = LOOP_VINFO_VECT_FACTOR (loop_vinfo) / nunits_in;\n-\n-  gcc_assert (ncopies >= 1);\n-\n-  /* Supportable by target?  */\n-  if (!supportable_widening_operation (code, stmt, vectype_out, vectype_in,\n-\t\t\t\t       &decl1, &decl2, &code1, &code2,\n-                                       &multi_step_cvt, &interm_types))\n-    return false;\n-\n-  /* Binary widening operation can only be supported directly by the\n-     architecture.  */\n-  gcc_assert (!(multi_step_cvt && op_type == binary_op));\n-\n   if (!vec_stmt) /* transformation not required.  */\n     {\n-      STMT_VINFO_TYPE (stmt_info) = type_promotion_vec_info_type;\n+      STMT_VINFO_TYPE (stmt_info) = op_vec_info_type;\n       if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"=== vectorizable_promotion ===\");\n-      vect_model_simple_cost (stmt_info, 2*ncopies, dt, NULL);\n+        fprintf (vect_dump, \"=== vectorizable_operation ===\");\n+      vect_model_simple_cost (stmt_info, ncopies, dt, NULL);\n       return true;\n     }\n \n   /** Transform.  **/\n \n   if (vect_print_dump_info (REPORT_DETAILS))\n-    fprintf (vect_dump, \"transform type promotion operation. ncopies = %d.\",\n-                        ncopies);\n-\n-  if (code == WIDEN_MULT_EXPR || code == WIDEN_LSHIFT_EXPR)\n-    {\n-      if (CONSTANT_CLASS_P (op0))\n-\top0 = fold_convert (TREE_TYPE (op1), op0);\n-      else if (CONSTANT_CLASS_P (op1))\n-\top1 = fold_convert (TREE_TYPE (op0), op1);\n-    }\n+    fprintf (vect_dump, \"transform binary/unary operation.\");\n \n   /* Handle def.  */\n-  /* In case of multi-step promotion, we first generate promotion operations\n-     to the intermediate types, and then from that types to the final one.\n-     We store vector destination in VEC_DSTS in the correct order for\n-     recursive creation of promotion operations in\n-     vect_create_vectorized_promotion_stmts(). Vector destinations are created\n-     according to TYPES recieved from supportable_widening_operation().   */\n-  if (multi_step_cvt)\n-    vec_dsts = VEC_alloc (tree, heap, multi_step_cvt + 1);\n-  else\n-    vec_dsts = VEC_alloc (tree, heap, 1);\n-\n-  vec_dest = vect_create_destination_var (scalar_dest, vectype_out);\n-  VEC_quick_push (tree, vec_dsts, vec_dest);\n-\n-  if (multi_step_cvt)\n-    {\n-      for (i = VEC_length (tree, interm_types) - 1;\n-           VEC_iterate (tree, interm_types, i, intermediate_type); i--)\n-        {\n-          vec_dest = vect_create_destination_var (scalar_dest,\n-                                                  intermediate_type);\n-          VEC_quick_push (tree, vec_dsts, vec_dest);\n-        }\n-    }\n+  vec_dest = vect_create_destination_var (scalar_dest, vectype);\n \n+  /* Allocate VECs for vector operands.  In case of SLP, vector operands are\n+     created in the previous stages of the recursion, so no allocation is\n+     needed, except for the case of shift with scalar shift argument.  In that\n+     case we store the scalar operand in VEC_OPRNDS1 for every vector stmt to\n+     be created to vectorize the SLP group, i.e., SLP_NODE->VEC_STMTS_SIZE.\n+     In case of loop-based vectorization we allocate VECs of size 1.  We\n+     allocate VEC_OPRNDS1 only in case of binary operation.  */\n   if (!slp_node)\n     {\n-      vec_oprnds0 = VEC_alloc (tree, heap,\n-                            (multi_step_cvt ? vect_pow2 (multi_step_cvt) : 1));\n-      if (op_type == binary_op)\n+      vec_oprnds0 = VEC_alloc (tree, heap, 1);\n+      if (op_type == binary_op || op_type == ternary_op)\n         vec_oprnds1 = VEC_alloc (tree, heap, 1);\n+      if (op_type == ternary_op)\n+        vec_oprnds2 = VEC_alloc (tree, heap, 1);\n     }\n-  else if (code == WIDEN_LSHIFT_EXPR)\n-    vec_oprnds1 = VEC_alloc (tree, heap, slp_node->vec_stmts_size);\n \n   /* In case the vectorization factor (VF) is bigger than the number\n      of elements that we can fit in a vectype (nunits), we have to generate\n      more than one vector stmt - i.e - we need to \"unroll\" the\n-     vector stmt by a factor VF/nunits.   */\n+     vector stmt by a factor VF/nunits.  In doing so, we record a pointer\n+     from one copy of the vector stmt to the next, in the field\n+     STMT_VINFO_RELATED_STMT.  This is necessary in order to allow following\n+     stages to find the correct vector defs to be used when vectorizing\n+     stmts that use the defs of the current stmt.  The example below\n+     illustrates the vectorization process when VF=16 and nunits=4 (i.e.,\n+     we need to create 4 vectorized stmts):\n+\n+     before vectorization:\n+                                RELATED_STMT    VEC_STMT\n+        S1:     x = memref      -               -\n+        S2:     z = x + 1       -               -\n+\n+     step 1: vectorize stmt S1 (done in vectorizable_load. See more details\n+             there):\n+                                RELATED_STMT    VEC_STMT\n+        VS1_0:  vx0 = memref0   VS1_1           -\n+        VS1_1:  vx1 = memref1   VS1_2           -\n+        VS1_2:  vx2 = memref2   VS1_3           -\n+        VS1_3:  vx3 = memref3   -               -\n+        S1:     x = load        -               VS1_0\n+        S2:     z = x + 1       -               -\n+\n+     step2: vectorize stmt S2 (done here):\n+        To vectorize stmt S2 we first need to find the relevant vector\n+        def for the first operand 'x'.  This is, as usual, obtained from\n+        the vector stmt recorded in the STMT_VINFO_VEC_STMT of the stmt\n+        that defines 'x' (S1).  This way we find the stmt VS1_0, and the\n+        relevant vector def 'vx0'.  Having found 'vx0' we can generate\n+        the vector stmt VS2_0, and as usual, record it in the\n+        STMT_VINFO_VEC_STMT of stmt S2.\n+        When creating the second copy (VS2_1), we obtain the relevant vector\n+        def from the vector stmt recorded in the STMT_VINFO_RELATED_STMT of\n+        stmt VS1_0.  This way we find the stmt VS1_1 and the relevant\n+        vector def 'vx1'.  Using 'vx1' we create stmt VS2_1 and record a\n+        pointer to it in the STMT_VINFO_RELATED_STMT of the vector stmt VS2_0.\n+        Similarly when creating stmts VS2_2 and VS2_3.  This is the resulting\n+        chain of stmts and pointers:\n+                                RELATED_STMT    VEC_STMT\n+        VS1_0:  vx0 = memref0   VS1_1           -\n+        VS1_1:  vx1 = memref1   VS1_2           -\n+        VS1_2:  vx2 = memref2   VS1_3           -\n+        VS1_3:  vx3 = memref3   -               -\n+        S1:     x = load        -               VS1_0\n+        VS2_0:  vz0 = vx0 + v1  VS2_1           -\n+        VS2_1:  vz1 = vx1 + v1  VS2_2           -\n+        VS2_2:  vz2 = vx2 + v1  VS2_3           -\n+        VS2_3:  vz3 = vx3 + v1  -               -\n+        S2:     z = x + 1       -               VS2_0  */\n \n   prev_stmt_info = NULL;\n   for (j = 0; j < ncopies; j++)\n     {\n       /* Handle uses.  */\n       if (j == 0)\n-        {\n-          if (slp_node)\n+\t{\n+\t  if (op_type == binary_op || op_type == ternary_op)\n+\t    vect_get_vec_defs (op0, op1, stmt, &vec_oprnds0, &vec_oprnds1,\n+\t\t\t       slp_node, -1);\n+\t  else\n+\t    vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL,\n+\t\t\t       slp_node, -1);\n+\t  if (op_type == ternary_op)\n \t    {\n-\t      if (code == WIDEN_LSHIFT_EXPR)\n-                {\n-                  vec_oprnd1 = op1;\n-\t\t  /* Store vec_oprnd1 for every vector stmt to be created\n-\t\t     for SLP_NODE.  We check during the analysis that all\n-\t\t     the shift arguments are the same.  */\n-                  for (k = 0; k < slp_node->vec_stmts_size - 1; k++)\n-                    VEC_quick_push (tree, vec_oprnds1, vec_oprnd1);\n-\n-    \t\t  vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL,\n- \t                             slp_node, -1);\n-                }\n-              else\n-                vect_get_vec_defs (op0, op1, stmt, &vec_oprnds0,\n-                                   &vec_oprnds1, slp_node, -1);\n+\t      vec_oprnds2 = VEC_alloc (tree, heap, 1);\n+\t      VEC_quick_push (tree, vec_oprnds2,\n+\t\t\t      vect_get_vec_def_for_operand (op2, stmt, NULL));\n \t    }\n-\t  else\n-            {\n-              vec_oprnd0 = vect_get_vec_def_for_operand (op0, stmt, NULL);\n-              VEC_quick_push (tree, vec_oprnds0, vec_oprnd0);\n-              if (op_type == binary_op)\n-                {\n-                  if (code == WIDEN_LSHIFT_EXPR)\n-                    vec_oprnd1 = op1;\n-                  else\n-                    vec_oprnd1 = vect_get_vec_def_for_operand (op1, stmt, NULL);\n-                  VEC_quick_push (tree, vec_oprnds1, vec_oprnd1);\n-                }\n-            }\n-        }\n+\t}\n       else\n+\t{\n+\t  vect_get_vec_defs_for_stmt_copy (dt, &vec_oprnds0, &vec_oprnds1);\n+\t  if (op_type == ternary_op)\n+\t    {\n+\t      tree vec_oprnd = VEC_pop (tree, vec_oprnds2);\n+\t      VEC_quick_push (tree, vec_oprnds2,\n+\t\t\t      vect_get_vec_def_for_stmt_copy (dt[2],\n+\t\t\t\t\t\t\t      vec_oprnd));\n+\t    }\n+\t}\n+\n+      /* Arguments are ready.  Create the new vector stmt.  */\n+      FOR_EACH_VEC_ELT (tree, vec_oprnds0, i, vop0)\n         {\n-          vec_oprnd0 = vect_get_vec_def_for_stmt_copy (dt[0], vec_oprnd0);\n-          VEC_replace (tree, vec_oprnds0, 0, vec_oprnd0);\n-          if (op_type == binary_op)\n-            {\n-              if (code == WIDEN_LSHIFT_EXPR)\n-                vec_oprnd1 = op1;\n-              else\n-                vec_oprnd1 = vect_get_vec_def_for_stmt_copy (dt[1], vec_oprnd1);\n-              VEC_replace (tree, vec_oprnds1, 0, vec_oprnd1);\n-            }\n+\t  vop1 = ((op_type == binary_op || op_type == ternary_op)\n+\t\t  ? VEC_index (tree, vec_oprnds1, i) : NULL_TREE);\n+\t  vop2 = ((op_type == ternary_op)\n+\t\t  ? VEC_index (tree, vec_oprnds2, i) : NULL_TREE);\n+\t  new_stmt = gimple_build_assign_with_ops3 (code, vec_dest,\n+\t\t\t\t\t\t    vop0, vop1, vop2);\n+\t  new_temp = make_ssa_name (vec_dest, new_stmt);\n+\t  gimple_assign_set_lhs (new_stmt, new_temp);\n+\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+          if (slp_node)\n+\t    VEC_quick_push (gimple, SLP_TREE_VEC_STMTS (slp_node), new_stmt);\n         }\n \n-      /* Arguments are ready.  Create the new vector stmts.  */\n-      tmp_vec_dsts = VEC_copy (tree, heap, vec_dsts);\n-      vect_create_vectorized_promotion_stmts (&vec_oprnds0, &vec_oprnds1,\n-                                              multi_step_cvt, stmt,\n-                                              tmp_vec_dsts,\n-                                              gsi, slp_node, code1, code2,\n-                                              decl1, decl2, op_type,\n-                                              &prev_stmt_info);\n+      if (slp_node)\n+        continue;\n+\n+      if (j == 0)\n+\tSTMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = new_stmt;\n+      else\n+\tSTMT_VINFO_RELATED_STMT (prev_stmt_info) = new_stmt;\n+      prev_stmt_info = vinfo_for_stmt (new_stmt);\n     }\n \n-  VEC_free (tree, heap, vec_dsts);\n-  VEC_free (tree, heap, tmp_vec_dsts);\n-  VEC_free (tree, heap, interm_types);\n   VEC_free (tree, heap, vec_oprnds0);\n-  VEC_free (tree, heap, vec_oprnds1);\n+  if (vec_oprnds1)\n+    VEC_free (tree, heap, vec_oprnds1);\n+  if (vec_oprnds2)\n+    VEC_free (tree, heap, vec_oprnds2);\n \n-  *vec_stmt = STMT_VINFO_VEC_STMT (stmt_info);\n   return true;\n }\n \n@@ -5216,9 +4988,7 @@ vect_analyze_stmt (gimple stmt, bool *need_to_vectorize, slp_tree node)\n    if (!bb_vinfo\n        && (STMT_VINFO_RELEVANT_P (stmt_info)\n            || STMT_VINFO_DEF_TYPE (stmt_info) == vect_reduction_def))\n-      ok = (vectorizable_type_promotion (stmt, NULL, NULL, NULL)\n-            || vectorizable_type_demotion (stmt, NULL, NULL, NULL)\n-            || vectorizable_conversion (stmt, NULL, NULL, NULL)\n+      ok = (vectorizable_conversion (stmt, NULL, NULL, NULL)\n             || vectorizable_shift (stmt, NULL, NULL, NULL)\n             || vectorizable_operation (stmt, NULL, NULL, NULL)\n             || vectorizable_assignment (stmt, NULL, NULL, NULL)\n@@ -5230,9 +5000,8 @@ vect_analyze_stmt (gimple stmt, bool *need_to_vectorize, slp_tree node)\n     else\n       {\n         if (bb_vinfo)\n-          ok = (vectorizable_type_promotion (stmt, NULL, NULL, node)\n-                || vectorizable_type_demotion (stmt, NULL, NULL, node)\n-               || vectorizable_shift (stmt, NULL, NULL, node)\n+\t  ok = (vectorizable_conversion (stmt, NULL, NULL, node)\n+\t\t|| vectorizable_shift (stmt, NULL, NULL, node)\n                 || vectorizable_operation (stmt, NULL, NULL, node)\n                 || vectorizable_assignment (stmt, NULL, NULL, node)\n                 || vectorizable_load (stmt, NULL, NULL, node, NULL)\n@@ -5293,15 +5062,7 @@ vect_transform_stmt (gimple stmt, gimple_stmt_iterator *gsi,\n   switch (STMT_VINFO_TYPE (stmt_info))\n     {\n     case type_demotion_vec_info_type:\n-      done = vectorizable_type_demotion (stmt, gsi, &vec_stmt, slp_node);\n-      gcc_assert (done);\n-      break;\n-\n     case type_promotion_vec_info_type:\n-      done = vectorizable_type_promotion (stmt, gsi, &vec_stmt, slp_node);\n-      gcc_assert (done);\n-      break;\n-\n     case type_conversion_vec_info_type:\n       done = vectorizable_conversion (stmt, gsi, &vec_stmt, slp_node);\n       gcc_assert (done);\n@@ -5877,12 +5638,17 @@ supportable_widening_operation (enum tree_code code, gimple stmt,\n   tree vectype = vectype_in;\n   tree wide_vectype = vectype_out;\n   enum tree_code c1, c2;\n+  int i;\n+  tree prev_type, intermediate_type;\n+  enum machine_mode intermediate_mode, prev_mode;\n+  optab optab3, optab4;\n \n+  *multi_step_cvt = 0;\n   if (loop_info)\n     vect_loop = LOOP_VINFO_LOOP (loop_info);\n \n   /* The result of a vectorized widening operation usually requires two vectors\n-     (because the widened results do not fit int one vector). The generated\n+     (because the widened results do not fit into one vector). The generated\n      vector results would normally be expected to be generated in the same\n      order as in the original scalar computation, i.e. if 8 results are\n      generated in each vector iteration, they are to be organized as follows:\n@@ -5927,55 +5693,23 @@ supportable_widening_operation (enum tree_code code, gimple stmt,\n   switch (code)\n     {\n     case WIDEN_MULT_EXPR:\n-      if (BYTES_BIG_ENDIAN)\n-        {\n-          c1 = VEC_WIDEN_MULT_HI_EXPR;\n-          c2 = VEC_WIDEN_MULT_LO_EXPR;\n-        }\n-      else\n-        {\n-          c2 = VEC_WIDEN_MULT_HI_EXPR;\n-          c1 = VEC_WIDEN_MULT_LO_EXPR;\n-        }\n+      c1 = VEC_WIDEN_MULT_LO_EXPR;\n+      c2 = VEC_WIDEN_MULT_HI_EXPR;\n       break;\n \n     case WIDEN_LSHIFT_EXPR:\n-      if (BYTES_BIG_ENDIAN)\n-        {\n-          c1 = VEC_WIDEN_LSHIFT_HI_EXPR;\n-          c2 = VEC_WIDEN_LSHIFT_LO_EXPR;\n-        }\n-      else\n-        {\n-          c2 = VEC_WIDEN_LSHIFT_HI_EXPR;\n-          c1 = VEC_WIDEN_LSHIFT_LO_EXPR;\n-        }\n+      c1 = VEC_WIDEN_LSHIFT_LO_EXPR;\n+      c2 = VEC_WIDEN_LSHIFT_HI_EXPR;\n       break;\n \n     CASE_CONVERT:\n-      if (BYTES_BIG_ENDIAN)\n-        {\n-          c1 = VEC_UNPACK_HI_EXPR;\n-          c2 = VEC_UNPACK_LO_EXPR;\n-        }\n-      else\n-        {\n-          c2 = VEC_UNPACK_HI_EXPR;\n-          c1 = VEC_UNPACK_LO_EXPR;\n-        }\n+      c1 = VEC_UNPACK_LO_EXPR;\n+      c2 = VEC_UNPACK_HI_EXPR;\n       break;\n \n     case FLOAT_EXPR:\n-      if (BYTES_BIG_ENDIAN)\n-        {\n-          c1 = VEC_UNPACK_FLOAT_HI_EXPR;\n-          c2 = VEC_UNPACK_FLOAT_LO_EXPR;\n-        }\n-      else\n-        {\n-          c2 = VEC_UNPACK_FLOAT_HI_EXPR;\n-          c1 = VEC_UNPACK_FLOAT_LO_EXPR;\n-        }\n+      c1 = VEC_UNPACK_FLOAT_LO_EXPR;\n+      c2 = VEC_UNPACK_FLOAT_HI_EXPR;\n       break;\n \n     case FIX_TRUNC_EXPR:\n@@ -5988,6 +5722,13 @@ supportable_widening_operation (enum tree_code code, gimple stmt,\n       gcc_unreachable ();\n     }\n \n+  if (BYTES_BIG_ENDIAN)\n+    {\n+      enum tree_code ctmp = c1;\n+      c1 = c2;\n+      c2 = ctmp;\n+    }\n+\n   if (code == FIX_TRUNC_EXPR)\n     {\n       /* The signedness is determined from output operand.  */\n@@ -6008,65 +5749,60 @@ supportable_widening_operation (enum tree_code code, gimple stmt,\n        || (icode2 = optab_handler (optab2, vec_mode)) == CODE_FOR_nothing)\n     return false;\n \n+  *code1 = c1;\n+  *code2 = c2;\n+\n+  if (insn_data[icode1].operand[0].mode == TYPE_MODE (wide_vectype)\n+      && insn_data[icode2].operand[0].mode == TYPE_MODE (wide_vectype))\n+    return true;\n+\n   /* Check if it's a multi-step conversion that can be done using intermediate\n      types.  */\n-  if (insn_data[icode1].operand[0].mode != TYPE_MODE (wide_vectype)\n-       || insn_data[icode2].operand[0].mode != TYPE_MODE (wide_vectype))\n-    {\n-      int i;\n-      tree prev_type = vectype, intermediate_type;\n-      enum machine_mode intermediate_mode, prev_mode = vec_mode;\n-      optab optab3, optab4;\n \n-      if (!CONVERT_EXPR_CODE_P (code))\n-        return false;\n+  prev_type = vectype;\n+  prev_mode = vec_mode;\n \n-      *code1 = c1;\n-      *code2 = c2;\n+  if (!CONVERT_EXPR_CODE_P (code))\n+    return false;\n \n-      /* We assume here that there will not be more than MAX_INTERM_CVT_STEPS\n-         intermediate steps in promotion sequence.  We try\n-         MAX_INTERM_CVT_STEPS to get to NARROW_VECTYPE, and fail if we do\n-         not.  */\n-      *interm_types = VEC_alloc (tree, heap, MAX_INTERM_CVT_STEPS);\n-      for (i = 0; i < 3; i++)\n-        {\n-          intermediate_mode = insn_data[icode1].operand[0].mode;\n-          intermediate_type = lang_hooks.types.type_for_mode (intermediate_mode,\n-                                                     TYPE_UNSIGNED (prev_type));\n-          optab3 = optab_for_tree_code (c1, intermediate_type, optab_default);\n-          optab4 = optab_for_tree_code (c2, intermediate_type, optab_default);\n-\n-          if (!optab3 || !optab4\n-              || ((icode1 = optab_handler (optab1, prev_mode))\n-\t\t  == CODE_FOR_nothing)\n-              || insn_data[icode1].operand[0].mode != intermediate_mode\n-              || ((icode2 = optab_handler (optab2, prev_mode))\n-\t\t  == CODE_FOR_nothing)\n-              || insn_data[icode2].operand[0].mode != intermediate_mode\n-              || ((icode1 = optab_handler (optab3, intermediate_mode))\n-\t\t  == CODE_FOR_nothing)\n-              || ((icode2 = optab_handler (optab4, intermediate_mode))\n-\t\t  == CODE_FOR_nothing))\n-            return false;\n-\n-          VEC_quick_push (tree, *interm_types, intermediate_type);\n-          (*multi_step_cvt)++;\n-\n-          if (insn_data[icode1].operand[0].mode == TYPE_MODE (wide_vectype)\n-              && insn_data[icode2].operand[0].mode == TYPE_MODE (wide_vectype))\n-            return true;\n-\n-          prev_type = intermediate_type;\n-          prev_mode = intermediate_mode;\n-        }\n+  /* We assume here that there will not be more than MAX_INTERM_CVT_STEPS\n+     intermediate steps in promotion sequence.  We try\n+     MAX_INTERM_CVT_STEPS to get to NARROW_VECTYPE, and fail if we do\n+     not.  */\n+  *interm_types = VEC_alloc (tree, heap, MAX_INTERM_CVT_STEPS);\n+  for (i = 0; i < MAX_INTERM_CVT_STEPS; i++)\n+    {\n+      intermediate_mode = insn_data[icode1].operand[0].mode;\n+      intermediate_type\n+\t= lang_hooks.types.type_for_mode (intermediate_mode,\n+\t\t\t\t\t  TYPE_UNSIGNED (prev_type));\n+      optab3 = optab_for_tree_code (c1, intermediate_type, optab_default);\n+      optab4 = optab_for_tree_code (c2, intermediate_type, optab_default);\n+\n+      if (!optab3 || !optab4\n+          || (icode1 = optab_handler (optab1, prev_mode)) == CODE_FOR_nothing\n+\t  || insn_data[icode1].operand[0].mode != intermediate_mode\n+\t  || (icode2 = optab_handler (optab2, prev_mode)) == CODE_FOR_nothing\n+\t  || insn_data[icode2].operand[0].mode != intermediate_mode\n+\t  || ((icode1 = optab_handler (optab3, intermediate_mode))\n+\t      == CODE_FOR_nothing)\n+\t  || ((icode2 = optab_handler (optab4, intermediate_mode))\n+\t      == CODE_FOR_nothing))\n+\tbreak;\n \n-       return false;\n+      VEC_quick_push (tree, *interm_types, intermediate_type);\n+      (*multi_step_cvt)++;\n+\n+      if (insn_data[icode1].operand[0].mode == TYPE_MODE (wide_vectype)\n+\t  && insn_data[icode2].operand[0].mode == TYPE_MODE (wide_vectype))\n+\treturn true;\n+\n+      prev_type = intermediate_type;\n+      prev_mode = intermediate_mode;\n     }\n \n-  *code1 = c1;\n-  *code2 = c2;\n-  return true;\n+  VEC_free (tree, heap, *interm_types);\n+  return false;\n }\n \n \n@@ -6102,9 +5838,12 @@ supportable_narrowing_operation (enum tree_code code,\n   tree vectype = vectype_in;\n   tree narrow_vectype = vectype_out;\n   enum tree_code c1;\n-  tree intermediate_type, prev_type;\n+  tree intermediate_type;\n+  enum machine_mode intermediate_mode, prev_mode;\n   int i;\n+  bool uns;\n \n+  *multi_step_cvt = 0;\n   switch (code)\n     {\n     CASE_CONVERT:\n@@ -6137,47 +5876,70 @@ supportable_narrowing_operation (enum tree_code code,\n   if ((icode1 = optab_handler (optab1, vec_mode)) == CODE_FOR_nothing)\n     return false;\n \n+  *code1 = c1;\n+\n+  if (insn_data[icode1].operand[0].mode == TYPE_MODE (narrow_vectype))\n+    return true;\n+\n   /* Check if it's a multi-step conversion that can be done using intermediate\n      types.  */\n-  if (insn_data[icode1].operand[0].mode != TYPE_MODE (narrow_vectype))\n-    {\n-      enum machine_mode intermediate_mode, prev_mode = vec_mode;\n-\n-      *code1 = c1;\n-      prev_type = vectype;\n-      /* We assume here that there will not be more than MAX_INTERM_CVT_STEPS\n-         intermediate steps in promotion sequence.  We try\n-         MAX_INTERM_CVT_STEPS to get to NARROW_VECTYPE, and fail if we do\n-         not.  */\n-      *interm_types = VEC_alloc (tree, heap, MAX_INTERM_CVT_STEPS);\n-      for (i = 0; i < 3; i++)\n-        {\n-          intermediate_mode = insn_data[icode1].operand[0].mode;\n-          intermediate_type = lang_hooks.types.type_for_mode (intermediate_mode,\n-                                                     TYPE_UNSIGNED (prev_type));\n-          interm_optab = optab_for_tree_code (c1, intermediate_type,\n-                                              optab_default);\n-          if (!interm_optab\n-              || ((icode1 = optab_handler (optab1, prev_mode))\n-\t\t  == CODE_FOR_nothing)\n-              || insn_data[icode1].operand[0].mode != intermediate_mode\n-              || ((icode1 = optab_handler (interm_optab, intermediate_mode))\n-\t\t  == CODE_FOR_nothing))\n-            return false;\n-\n-          VEC_quick_push (tree, *interm_types, intermediate_type);\n-          (*multi_step_cvt)++;\n-\n-          if (insn_data[icode1].operand[0].mode == TYPE_MODE (narrow_vectype))\n-            return true;\n-\n-          prev_type = intermediate_type;\n-          prev_mode = intermediate_mode;\n-        }\n+  prev_mode = vec_mode;\n+  if (code == FIX_TRUNC_EXPR)\n+    uns = TYPE_UNSIGNED (vectype_out);\n+  else\n+    uns = TYPE_UNSIGNED (vectype);\n+\n+  /* For multi-step FIX_TRUNC_EXPR prefer signed floating to integer\n+     conversion over unsigned, as unsigned FIX_TRUNC_EXPR is often more\n+     costly than signed.  */\n+  if (code == FIX_TRUNC_EXPR && uns)\n+    {\n+      enum insn_code icode2;\n+\n+      intermediate_type\n+\t= lang_hooks.types.type_for_mode (TYPE_MODE (vectype_out), 0);\n+      interm_optab\n+\t= optab_for_tree_code (c1, intermediate_type, optab_default);\n+      if (interm_optab != NULL\n+\t  && (icode2 = optab_handler (optab1, vec_mode)) != CODE_FOR_nothing\n+\t  && insn_data[icode1].operand[0].mode\n+\t     == insn_data[icode2].operand[0].mode)\n+\t{\n+\t  uns = false;\n+\t  optab1 = interm_optab;\n+\t  icode1 = icode2;\n+\t}\n+    }\n \n-      return false;\n+  /* We assume here that there will not be more than MAX_INTERM_CVT_STEPS\n+     intermediate steps in promotion sequence.  We try\n+     MAX_INTERM_CVT_STEPS to get to NARROW_VECTYPE, and fail if we do not.  */\n+  *interm_types = VEC_alloc (tree, heap, MAX_INTERM_CVT_STEPS);\n+  for (i = 0; i < MAX_INTERM_CVT_STEPS; i++)\n+    {\n+      intermediate_mode = insn_data[icode1].operand[0].mode;\n+      intermediate_type\n+\t= lang_hooks.types.type_for_mode (intermediate_mode, uns);\n+      interm_optab\n+\t= optab_for_tree_code (VEC_PACK_TRUNC_EXPR, intermediate_type,\n+\t\t\t       optab_default);\n+      if (!interm_optab\n+\t  || ((icode1 = optab_handler (optab1, prev_mode)) == CODE_FOR_nothing)\n+\t  || insn_data[icode1].operand[0].mode != intermediate_mode\n+\t  || ((icode1 = optab_handler (interm_optab, intermediate_mode))\n+\t      == CODE_FOR_nothing))\n+\tbreak;\n+\n+      VEC_quick_push (tree, *interm_types, intermediate_type);\n+      (*multi_step_cvt)++;\n+\n+      if (insn_data[icode1].operand[0].mode == TYPE_MODE (narrow_vectype))\n+\treturn true;\n+\n+      prev_mode = intermediate_mode;\n+      optab1 = interm_optab;\n     }\n \n-  *code1 = c1;\n-  return true;\n+  VEC_free (tree, heap, *interm_types);\n+  return false;\n }"}]}