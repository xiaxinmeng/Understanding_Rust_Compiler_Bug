{"sha": "cdc9103cba6f914981e3ba352d22db578c4aa209", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6Y2RjOTEwM2NiYTZmOTE0OTgxZTNiYTM1MmQyMmRiNTc4YzRhYTIwOQ==", "commit": {"author": {"name": "John David Anglin", "email": "dave.anglin@nrc-cnrc.gc.ca", "date": "2003-11-08T20:36:30Z"}, "committer": {"name": "John David Anglin", "email": "danglin@gcc.gnu.org", "date": "2003-11-08T20:36:30Z"}, "message": "re PR rtl-optimization/12630 (Various unrecognizable insns and ICEs at -O3)\n\n\tPR optimization/12630\n\t* pa.c (compute_movstrsi_length): Rename to compute_movstr_length.\n\tHandle length computation 64-bit moves.\n\t(compute_clrstr_length, output_block_clear): Implement block clear.\n\t(output_block_move): Handle 64-bit moves.\n\t(pa_adjust_insn_length): Use compute_movstr_length and\n\tcompute_clrstr_length.\n\t* pa.md (movstrsi): Revise operand order and comments.  Don't use\n\tmatch_scratch.\n\t(movstrsi_internal): Delete.\n\t(movstrsi_prereload, movstrsi_postreload): New insns.  Define splitter\n\tand peephole2 patterns to transform prereload to postreload form.\n\t(movstrdi, movstrdi_prereload, movstrdi_postreload, clrstrsi,\n\tclrstrsi_prereload, clrstrsi_postreload, clrstrdi, clrstrdi_prereload,\n\tclrstrdi_postreload): New patterns for 64-bit block move, and block\n\tclear.\n\t* pa-protos.h (output_block_clear): New prototype.\n\nFrom-SVN: r73375", "tree": {"sha": "63aafd1067bf87ae1d6116cc2f87f72d9b889f0f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/63aafd1067bf87ae1d6116cc2f87f72d9b889f0f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/cdc9103cba6f914981e3ba352d22db578c4aa209", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cdc9103cba6f914981e3ba352d22db578c4aa209", "html_url": "https://github.com/Rust-GCC/gccrs/commit/cdc9103cba6f914981e3ba352d22db578c4aa209", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cdc9103cba6f914981e3ba352d22db578c4aa209/comments", "author": null, "committer": null, "parents": [{"sha": "ac47cc13ab45226d800f5cbc98ad50ed99812b4b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ac47cc13ab45226d800f5cbc98ad50ed99812b4b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ac47cc13ab45226d800f5cbc98ad50ed99812b4b"}], "stats": {"total": 697, "additions": 669, "deletions": 28}, "files": [{"sha": "60d79213f564acef7b4ea3e16f7e6a110cd098e0", "filename": "gcc/ChangeLog", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cdc9103cba6f914981e3ba352d22db578c4aa209/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cdc9103cba6f914981e3ba352d22db578c4aa209/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=cdc9103cba6f914981e3ba352d22db578c4aa209", "patch": "@@ -1,3 +1,23 @@\n+2003-11-08  John David Anglin  <dave.anglin@nrc-cnrc.gc.ca>\n+\n+\tPR optimization/12630\n+\t* pa.c (compute_movstrsi_length): Rename to compute_movstr_length.\n+\tHandle length computation 64-bit moves.\n+\t(compute_clrstr_length, output_block_clear): Implement block clear.\n+\t(output_block_move): Handle 64-bit moves.\n+\t(pa_adjust_insn_length): Use compute_movstr_length and\n+\tcompute_clrstr_length.\n+\t* pa.md (movstrsi): Revise operand order and comments.  Don't use\n+\tmatch_scratch.\n+\t(movstrsi_internal): Delete.\n+\t(movstrsi_prereload, movstrsi_postreload): New insns.  Define splitter\n+\tand peephole2 patterns to transform prereload to postreload form.\n+\t(movstrdi, movstrdi_prereload, movstrdi_postreload, clrstrsi,\n+\tclrstrsi_prereload, clrstrsi_postreload, clrstrdi, clrstrdi_prereload,\n+\tclrstrdi_postreload): New patterns for 64-bit block move, and block\n+\tclear.\n+\t* pa-protos.h (output_block_clear): New prototype.\n+\n 2003-11-08  Andreas Schwab  <schwab@suse.de>\n \n \t* dbxout.c (current_file): Also wrap inside DBX_DEBUGGING_INFO ||"}, {"sha": "4d5ce69b47bbb1babef0ab92381fe2091360dc0a", "filename": "gcc/config/pa/pa-protos.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cdc9103cba6f914981e3ba352d22db578c4aa209/gcc%2Fconfig%2Fpa%2Fpa-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cdc9103cba6f914981e3ba352d22db578c4aa209/gcc%2Fconfig%2Fpa%2Fpa-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fpa%2Fpa-protos.h?ref=cdc9103cba6f914981e3ba352d22db578c4aa209", "patch": "@@ -40,6 +40,7 @@ extern const char *output_ior (rtx *);\n extern const char *output_move_double (rtx *);\n extern const char *output_fp_move_double (rtx *);\n extern const char *output_block_move (rtx *, int);\n+extern const char *output_block_clear (rtx *, int);\n extern const char *output_cbranch (rtx *, int, int, int, rtx);\n extern const char *output_lbranch (rtx, rtx);\n extern const char *output_bb (rtx *, int, int, int, rtx, int);"}, {"sha": "8c8aebbf31b7f3b1f31c4f31e3342dbe558eec83", "filename": "gcc/config/pa/pa.c", "status": "modified", "additions": 188, "deletions": 9, "changes": 197, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cdc9103cba6f914981e3ba352d22db578c4aa209/gcc%2Fconfig%2Fpa%2Fpa.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cdc9103cba6f914981e3ba352d22db578c4aa209/gcc%2Fconfig%2Fpa%2Fpa.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fpa%2Fpa.c?ref=cdc9103cba6f914981e3ba352d22db578c4aa209", "patch": "@@ -103,7 +103,8 @@ static int pa_can_combine_p (rtx, rtx, rtx, int, rtx, rtx, rtx);\n static int forward_branch_p (rtx);\n static int shadd_constant_p (int);\n static void compute_zdepwi_operands (unsigned HOST_WIDE_INT, unsigned *);\n-static int compute_movstrsi_length (rtx);\n+static int compute_movstr_length (rtx);\n+static int compute_clrstr_length (rtx);\n static bool pa_assemble_integer (rtx, unsigned int, int);\n static void remove_useless_addtr_insns (int);\n static void store_reg (int, int, int);\n@@ -2431,8 +2432,8 @@ find_addr_reg (rtx addr)\n    OPERANDS[0] is the destination pointer as a REG, clobbered.\n    OPERANDS[1] is the source pointer as a REG, clobbered.\n    OPERANDS[2] is a register for temporary storage.\n-   OPERANDS[4] is the size as a CONST_INT\n    OPERANDS[3] is a register for temporary storage.\n+   OPERANDS[4] is the size as a CONST_INT\n    OPERANDS[5] is the alignment safe to use, as a CONST_INT.\n    OPERANDS[6] is another temporary register.   */\n \n@@ -2442,15 +2443,43 @@ output_block_move (rtx *operands, int size_is_constant ATTRIBUTE_UNUSED)\n   int align = INTVAL (operands[5]);\n   unsigned long n_bytes = INTVAL (operands[4]);\n \n-  /* We can't move more than four bytes at a time because the PA\n+  /* We can't move more than a word at a time because the PA\n      has no longer integer move insns.  (Could use fp mem ops?)  */\n-  if (align > 4)\n-    align = 4;\n+  if (align > (TARGET_64BIT ? 8 : 4))\n+    align = (TARGET_64BIT ? 8 : 4);\n \n   /* Note that we know each loop below will execute at least twice\n      (else we would have open-coded the copy).  */\n   switch (align)\n     {\n+      case 8:\n+\t/* Pre-adjust the loop counter.  */\n+\toperands[4] = GEN_INT (n_bytes - 16);\n+\toutput_asm_insn (\"ldi %4,%2\", operands);\n+\n+\t/* Copying loop.  */\n+\toutput_asm_insn (\"ldd,ma 8(%1),%3\", operands);\n+\toutput_asm_insn (\"ldd,ma 8(%1),%6\", operands);\n+\toutput_asm_insn (\"std,ma %3,8(%0)\", operands);\n+\toutput_asm_insn (\"addib,>= -16,%2,.-12\", operands);\n+\toutput_asm_insn (\"std,ma %6,8(%0)\", operands);\n+\n+\t/* Handle the residual.  There could be up to 7 bytes of\n+\t   residual to copy!  */\n+\tif (n_bytes % 16 != 0)\n+\t  {\n+\t    operands[4] = GEN_INT (n_bytes % 8);\n+\t    if (n_bytes % 16 >= 8)\n+\t      output_asm_insn (\"ldd,ma 8(%1),%3\", operands);\n+\t    if (n_bytes % 8 != 0)\n+\t      output_asm_insn (\"ldd 0(%1),%6\", operands);\n+\t    if (n_bytes % 16 >= 8)\n+\t      output_asm_insn (\"std,ma %3,8(%0)\", operands);\n+\t    if (n_bytes % 8 != 0)\n+\t      output_asm_insn (\"stdby,e %6,%4(%0)\", operands);\n+\t  }\n+\treturn \"\";\n+\n       case 4:\n \t/* Pre-adjust the loop counter.  */\n \toperands[4] = GEN_INT (n_bytes - 8);\n@@ -2536,7 +2565,7 @@ output_block_move (rtx *operands, int size_is_constant ATTRIBUTE_UNUSED)\n    count insns rather than emit them.  */\n \n static int\n-compute_movstrsi_length (rtx insn)\n+compute_movstr_length (rtx insn)\n {\n   rtx pat = PATTERN (insn);\n   unsigned int align = INTVAL (XEXP (XVECEXP (pat, 0, 7), 0));\n@@ -2545,8 +2574,8 @@ compute_movstrsi_length (rtx insn)\n \n   /* We can't move more than four bytes at a time because the PA\n      has no longer integer move insns.  (Could use fp mem ops?)  */\n-  if (align > 4)\n-    align = 4;\n+  if (align > (TARGET_64BIT ? 8 : 4))\n+    align = (TARGET_64BIT ? 8 : 4);\n \n   /* The basic copying loop.  */\n   n_insns = 6;\n@@ -2564,6 +2593,148 @@ compute_movstrsi_length (rtx insn)\n   /* Lengths are expressed in bytes now; each insn is 4 bytes.  */\n   return n_insns * 4;\n }\n+\n+/* Emit code to perform a block clear.\n+\n+   OPERANDS[0] is the destination pointer as a REG, clobbered.\n+   OPERANDS[1] is a register for temporary storage.\n+   OPERANDS[2] is the size as a CONST_INT\n+   OPERANDS[3] is the alignment safe to use, as a CONST_INT.  */\n+\n+const char *\n+output_block_clear (rtx *operands, int size_is_constant ATTRIBUTE_UNUSED)\n+{\n+  int align = INTVAL (operands[3]);\n+  unsigned long n_bytes = INTVAL (operands[2]);\n+\n+  /* We can't clear more than a word at a time because the PA\n+     has no longer integer move insns.  */\n+  if (align > (TARGET_64BIT ? 8 : 4))\n+    align = (TARGET_64BIT ? 8 : 4);\n+\n+  /* Note that we know each loop below will execute at least twice\n+     (else we would have open-coded the copy).  */\n+  switch (align)\n+    {\n+      case 8:\n+\t/* Pre-adjust the loop counter.  */\n+\toperands[2] = GEN_INT (n_bytes - 16);\n+\toutput_asm_insn (\"ldi %2,%1\", operands);\n+\n+\t/* Loop.  */\n+\toutput_asm_insn (\"std,ma %%r0,8(%0)\", operands);\n+\toutput_asm_insn (\"addib,>= -16,%1,.-4\", operands);\n+\toutput_asm_insn (\"std,ma %%r0,8(%0)\", operands);\n+\n+\t/* Handle the residual.  There could be up to 7 bytes of\n+\t   residual to copy!  */\n+\tif (n_bytes % 16 != 0)\n+\t  {\n+\t    operands[2] = GEN_INT (n_bytes % 8);\n+\t    if (n_bytes % 16 >= 8)\n+\t      output_asm_insn (\"std,ma %%r0,8(%0)\", operands);\n+\t    if (n_bytes % 8 != 0)\n+\t      output_asm_insn (\"stdby,e %%r0,%2(%0)\", operands);\n+\t  }\n+\treturn \"\";\n+\n+      case 4:\n+\t/* Pre-adjust the loop counter.  */\n+\toperands[2] = GEN_INT (n_bytes - 8);\n+\toutput_asm_insn (\"ldi %2,%1\", operands);\n+\n+\t/* Loop.  */\n+\toutput_asm_insn (\"{stws|stw},ma %%r0,4(%0)\", operands);\n+\toutput_asm_insn (\"addib,>= -8,%1,.-4\", operands);\n+\toutput_asm_insn (\"{stws|stw},ma %%r0,4(%0)\", operands);\n+\n+\t/* Handle the residual.  There could be up to 7 bytes of\n+\t   residual to copy!  */\n+\tif (n_bytes % 8 != 0)\n+\t  {\n+\t    operands[2] = GEN_INT (n_bytes % 4);\n+\t    if (n_bytes % 8 >= 4)\n+\t      output_asm_insn (\"{stws|stw},ma %%r0,4(%0)\", operands);\n+\t    if (n_bytes % 4 != 0)\n+\t      output_asm_insn (\"{stbys|stby},e %%r0,%2(%0)\", operands);\n+\t  }\n+\treturn \"\";\n+\n+      case 2:\n+\t/* Pre-adjust the loop counter.  */\n+\toperands[2] = GEN_INT (n_bytes - 4);\n+\toutput_asm_insn (\"ldi %2,%1\", operands);\n+\n+\t/* Loop.  */\n+\toutput_asm_insn (\"{sths|sth},ma %%r0,2(%0)\", operands);\n+\toutput_asm_insn (\"addib,>= -4,%1,.-4\", operands);\n+\toutput_asm_insn (\"{sths|sth},ma %%r0,2(%0)\", operands);\n+\n+\t/* Handle the residual.  */\n+\tif (n_bytes % 4 != 0)\n+\t  {\n+\t    if (n_bytes % 4 >= 2)\n+\t      output_asm_insn (\"{sths|sth},ma %%r0,2(%0)\", operands);\n+\t    if (n_bytes % 2 != 0)\n+\t      output_asm_insn (\"stb %%r0,0(%0)\", operands);\n+\t  }\n+\treturn \"\";\n+\n+      case 1:\n+\t/* Pre-adjust the loop counter.  */\n+\toperands[2] = GEN_INT (n_bytes - 2);\n+\toutput_asm_insn (\"ldi %2,%1\", operands);\n+\n+\t/* Loop.  */\n+\toutput_asm_insn (\"{stbs|stb},ma %%r0,1(%0)\", operands);\n+\toutput_asm_insn (\"addib,>= -2,%1,.-4\", operands);\n+\toutput_asm_insn (\"{stbs|stb},ma %%r0,1(%0)\", operands);\n+\n+\t/* Handle the residual.  */\n+\tif (n_bytes % 2 != 0)\n+\t  output_asm_insn (\"stb %%r0,0(%0)\", operands);\n+\n+\treturn \"\";\n+\n+      default:\n+\tabort ();\n+    }\n+}\n+\n+/* Count the number of insns necessary to handle this block move.\n+\n+   Basic structure is the same as emit_block_move, except that we\n+   count insns rather than emit them.  */\n+\n+static int\n+compute_clrstr_length (rtx insn)\n+{\n+  rtx pat = PATTERN (insn);\n+  unsigned int align = INTVAL (XEXP (XVECEXP (pat, 0, 4), 0));\n+  unsigned long n_bytes = INTVAL (XEXP (XVECEXP (pat, 0, 3), 0));\n+  unsigned int n_insns = 0;\n+\n+  /* We can't clear more than a word at a time because the PA\n+     has no longer integer move insns.  */\n+  if (align > (TARGET_64BIT ? 8 : 4))\n+    align = (TARGET_64BIT ? 8 : 4);\n+\n+  /* The basic loop.  */\n+  n_insns = 4;\n+\n+  /* Residuals.  */\n+  if (n_bytes % (2 * align) != 0)\n+    {\n+      if ((n_bytes % (2 * align)) >= align)\n+\tn_insns++;\n+\n+      if ((n_bytes % align) != 0)\n+\tn_insns++;\n+    }\n+\n+  /* Lengths are expressed in bytes now; each insn is 4 bytes.  */\n+  return n_insns * 4;\n+}\n \f\n \n const char *\n@@ -4337,7 +4508,15 @@ pa_adjust_insn_length (rtx insn, int length)\n \t   && GET_CODE (XEXP (XVECEXP (pat, 0, 0), 1)) == MEM\n \t   && GET_MODE (XEXP (XVECEXP (pat, 0, 0), 0)) == BLKmode\n \t   && GET_MODE (XEXP (XVECEXP (pat, 0, 0), 1)) == BLKmode)\n-    return compute_movstrsi_length (insn) - 4;\n+    return compute_movstr_length (insn) - 4;\n+  /* Block clear pattern.  */\n+  else if (GET_CODE (insn) == INSN\n+\t   && GET_CODE (pat) == PARALLEL\n+\t   && GET_CODE (XVECEXP (pat, 0, 0)) == SET\n+\t   && GET_CODE (XEXP (XVECEXP (pat, 0, 0), 0)) == MEM\n+\t   && XEXP (XVECEXP (pat, 0, 0), 1) == const0_rtx\n+\t   && GET_MODE (XEXP (XVECEXP (pat, 0, 0), 0)) == BLKmode)\n+    return compute_clrstr_length (insn) - 4;\n   /* Conditional branch with an unfilled delay slot.  */\n   else if (GET_CODE (insn) == JUMP_INSN && ! simplejump_p (insn))\n     {"}, {"sha": "aa0bfcbe783a95164ac3de723da06ed2b1622258", "filename": "gcc/config/pa/pa.md", "status": "modified", "additions": 460, "deletions": 19, "changes": 479, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cdc9103cba6f914981e3ba352d22db578c4aa209/gcc%2Fconfig%2Fpa%2Fpa.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cdc9103cba6f914981e3ba352d22db578c4aa209/gcc%2Fconfig%2Fpa%2Fpa.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fpa%2Fpa.md?ref=cdc9103cba6f914981e3ba352d22db578c4aa209", "patch": "@@ -2955,20 +2955,20 @@\n    (set_attr \"length\" \"4\")])\n \n ;; The definition of this insn does not really explain what it does,\n-;; but it should suffice\n-;; that anything generated as this insn will be recognized as one\n-;; and that it will not successfully combine with anything.\n+;; but it should suffice that anything generated as this insn will be\n+;; recognized as a movstrsi operation, and that it will not successfully\n+;; combine with anything.\n (define_expand \"movstrsi\"\n   [(parallel [(set (match_operand:BLK 0 \"\" \"\")\n \t\t   (match_operand:BLK 1 \"\" \"\"))\n-\t      (clobber (match_scratch:SI 7 \"\"))\n-\t      (clobber (match_scratch:SI 8 \"\"))\n \t      (clobber (match_dup 4))\n \t      (clobber (match_dup 5))\n \t      (clobber (match_dup 6))\n+\t      (clobber (match_dup 7))\n+\t      (clobber (match_dup 8))\n \t      (use (match_operand:SI 2 \"arith_operand\" \"\"))\n \t      (use (match_operand:SI 3 \"const_int_operand\" \"\"))])]\n-  \"!TARGET_64BIT\"\n+  \"!TARGET_64BIT && optimize > 0\"\n   \"\n {\n   int size, align;\n@@ -2990,7 +2990,7 @@\n \tIf the size is large in respect to the known alignment, then use\n \tthe library routines.\n \n-\tIf the size is small in repsect to the known alignment, then open\n+\tIf the size is small in respect to the known alignment, then open\n \tcode the copy (since that will lead to better scheduling).\n \n         Else use the block move pattern.   */\n@@ -3003,8 +3003,7 @@\n   align = INTVAL (operands[3]);\n   align = align > 4 ? 4 : align;\n \n-  /* If size/alignment > 8 (eg size is large in respect to alignment),\n-     then use the library routines.  */\n+  /* If size/alignment is large, then use the library routines.  */\n   if (size / align > 16)\n     FAIL;\n \n@@ -3022,28 +3021,470 @@\n   operands[4] = gen_reg_rtx (SImode);\n   operands[5] = gen_reg_rtx (SImode);\n   operands[6] = gen_reg_rtx (SImode);\n-  operands[7] = XEXP (operands[0], 0);\n-  operands[8] = XEXP (operands[1], 0);\n+  operands[7] = gen_reg_rtx (SImode);\n+  operands[8] = gen_reg_rtx (SImode);\n }\")\n \n ;; The operand constraints are written like this to support both compile-time\n-;; and run-time determined byte count.  If the count is run-time determined,\n-;; the register with the byte count is clobbered by the copying code, and\n-;; therefore it is forced to operand 2.  If the count is compile-time\n-;; determined, we need two scratch registers for the unrolled code.\n-(define_insn \"movstrsi_internal\"\n+;; and run-time determined byte counts.  The expander and output_block_move\n+;; only support compile-time determined counts at this time.\n+;;\n+;; If the count is run-time determined, the register with the byte count\n+;; is clobbered by the copying code, and therefore it is forced to operand 2.\n+;;\n+;; We used to clobber operands 0 and 1.  However, a change to regrename.c\n+;; broke this semantic for pseudo registers.  We can't use match_scratch\n+;; as this requires two registers in the class R1_REGS when the MEMs for\n+;; operands 0 and 1 are both equivalent to symbolic MEMs.  Thus, we are\n+;; forced to internally copy operands 0 and 1 to operands 7 and 8,\n+;; respectively.  We then split or peephole optimize after reload.\n+(define_insn \"movstrsi_prereload\"\n   [(set (mem:BLK (match_operand:SI 0 \"register_operand\" \"r,r\"))\n \t(mem:BLK (match_operand:SI 1 \"register_operand\" \"r,r\")))\n-   (clobber (match_scratch:SI 7 \"=0,0\"))\n-   (clobber (match_scratch:SI 8 \"=1,1\"))\n    (clobber (match_operand:SI 2 \"register_operand\" \"=r,r\"))\t;loop cnt/tmp\n-   (clobber (match_operand:SI 3 \"register_operand\" \"=&r,&r\"))\t;item tmp\n+   (clobber (match_operand:SI 3 \"register_operand\" \"=&r,&r\"))\t;item tmp1\n    (clobber (match_operand:SI 6 \"register_operand\" \"=&r,&r\"))\t;item tmp2\n+   (clobber (match_operand:SI 7 \"register_operand\" \"=&r,&r\"))\t;item tmp3\n+   (clobber (match_operand:SI 8 \"register_operand\" \"=&r,&r\"))\t;item tmp4\n    (use (match_operand:SI 4 \"arith_operand\" \"J,2\"))\t ;byte count\n    (use (match_operand:SI 5 \"const_int_operand\" \"n,n\"))] ;alignment\n   \"!TARGET_64BIT\"\n+  \"#\"\n+  [(set_attr \"type\" \"multi,multi\")])\n+\n+(define_split\n+  [(parallel [(set (mem:BLK (match_operand:SI 0 \"register_operand\" \"\"))\n+\t\t   (mem:BLK (match_operand:SI 1 \"register_operand\" \"\")))\n+\t      (clobber (match_operand:SI 2 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:SI 3 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:SI 6 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:SI 7 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:SI 8 \"register_operand\" \"\"))\n+\t      (use (match_operand:SI 4 \"arith_operand\" \"\"))\n+\t      (use (match_operand:SI 5 \"const_int_operand\" \"\"))])]\n+  \"!TARGET_64BIT && reload_completed && !flag_peephole2\"\n+  [(set (match_dup 7) (match_dup 0))\n+   (set (match_dup 8) (match_dup 1))\n+   (parallel [(set (mem:BLK (match_dup 7)) (mem:BLK (match_dup 8)))\n+   \t      (clobber (match_dup 2))\n+   \t      (clobber (match_dup 3))\n+   \t      (clobber (match_dup 6))\n+   \t      (clobber (match_dup 7))\n+   \t      (clobber (match_dup 8))\n+   \t      (use (match_dup 4))\n+   \t      (use (match_dup 5))\n+\t      (const_int 0)])]\n+  \"\")\n+\n+(define_peephole2\n+  [(parallel [(set (mem:BLK (match_operand:SI 0 \"register_operand\" \"\"))\n+\t\t   (mem:BLK (match_operand:SI 1 \"register_operand\" \"\")))\n+\t      (clobber (match_operand:SI 2 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:SI 3 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:SI 6 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:SI 7 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:SI 8 \"register_operand\" \"\"))\n+\t      (use (match_operand:SI 4 \"arith_operand\" \"\"))\n+\t      (use (match_operand:SI 5 \"const_int_operand\" \"\"))])]\n+  \"!TARGET_64BIT\"\n+  [(parallel [(set (mem:BLK (match_dup 7)) (mem:BLK (match_dup 8)))\n+   \t      (clobber (match_dup 2))\n+   \t      (clobber (match_dup 3))\n+   \t      (clobber (match_dup 6))\n+   \t      (clobber (match_dup 7))\n+   \t      (clobber (match_dup 8))\n+   \t      (use (match_dup 4))\n+   \t      (use (match_dup 5))\n+\t      (const_int 0)])]\n+  \"\n+{\n+  if (dead_or_set_p (curr_insn, operands[0]))\n+    operands[7] = operands[0];\n+  else\n+    emit_insn (gen_rtx_SET (VOIDmode, operands[7], operands[0]));\n+\n+  if (dead_or_set_p (curr_insn, operands[1]))\n+    operands[8] = operands[1];\n+  else\n+    emit_insn (gen_rtx_SET (VOIDmode, operands[8], operands[1]));\n+}\")\n+\n+(define_insn \"movstrsi_postreload\"\n+  [(set (mem:BLK (match_operand:SI 0 \"register_operand\" \"r,r\"))\n+\t(mem:BLK (match_operand:SI 1 \"register_operand\" \"r,r\")))\n+   (clobber (match_operand:SI 2 \"register_operand\" \"=r,r\"))\t;loop cnt/tmp\n+   (clobber (match_operand:SI 3 \"register_operand\" \"=&r,&r\"))\t;item tmp1\n+   (clobber (match_operand:SI 6 \"register_operand\" \"=&r,&r\"))\t;item tmp2\n+   (clobber (match_dup 0))\n+   (clobber (match_dup 1))\n+   (use (match_operand:SI 4 \"arith_operand\" \"J,2\"))\t ;byte count\n+   (use (match_operand:SI 5 \"const_int_operand\" \"n,n\"))  ;alignment\n+   (const_int 0)]\n+  \"!TARGET_64BIT && reload_completed\"\n   \"* return output_block_move (operands, !which_alternative);\"\n   [(set_attr \"type\" \"multi,multi\")])\n+\n+(define_expand \"movstrdi\"\n+  [(parallel [(set (match_operand:BLK 0 \"\" \"\")\n+\t\t   (match_operand:BLK 1 \"\" \"\"))\n+\t      (clobber (match_dup 4))\n+\t      (clobber (match_dup 5))\n+\t      (clobber (match_dup 6))\n+\t      (clobber (match_dup 7))\n+\t      (clobber (match_dup 8))\n+\t      (use (match_operand:DI 2 \"arith_operand\" \"\"))\n+\t      (use (match_operand:DI 3 \"const_int_operand\" \"\"))])]\n+  \"TARGET_64BIT && optimize > 0\"\n+  \"\n+{\n+  int size, align;\n+\n+  /* HP provides very fast block move library routine for the PA;\n+     this routine includes:\n+\n+\t4x4 byte at a time block moves,\n+\t1x4 byte at a time with alignment checked at runtime with\n+\t    attempts to align the source and destination as needed\n+\t1x1 byte loop\n+\n+     With that in mind, here's the heuristics to try and guess when\n+     the inlined block move will be better than the library block\n+     move:\n+\n+\tIf the size isn't constant, then always use the library routines.\n+\n+\tIf the size is large in respect to the known alignment, then use\n+\tthe library routines.\n+\n+\tIf the size is small in respect to the known alignment, then open\n+\tcode the copy (since that will lead to better scheduling).\n+\n+        Else use the block move pattern.   */\n+\n+  /* Undetermined size, use the library routine.  */\n+  if (GET_CODE (operands[2]) != CONST_INT)\n+    FAIL;\n+\n+  size = INTVAL (operands[2]);\n+  align = INTVAL (operands[3]);\n+  align = align > 8 ? 8 : align;\n+\n+  /* If size/alignment is large, then use the library routines.  */\n+  if (size / align > 16)\n+    FAIL;\n+\n+  /* This does happen, but not often enough to worry much about.  */\n+  if (size / align < MOVE_RATIO)\n+    FAIL;\n+  \n+  /* Fall through means we're going to use our block move pattern.  */\n+  operands[0]\n+    = replace_equiv_address (operands[0],\n+\t\t\t     copy_to_mode_reg (DImode, XEXP (operands[0], 0)));\n+  operands[1]\n+    = replace_equiv_address (operands[1],\n+\t\t\t     copy_to_mode_reg (DImode, XEXP (operands[1], 0)));\n+  operands[4] = gen_reg_rtx (DImode);\n+  operands[5] = gen_reg_rtx (DImode);\n+  operands[6] = gen_reg_rtx (DImode);\n+  operands[7] = gen_reg_rtx (DImode);\n+  operands[8] = gen_reg_rtx (DImode);\n+}\")\n+\n+;; The operand constraints are written like this to support both compile-time\n+;; and run-time determined byte counts.  The expander and output_block_move\n+;; only support compile-time determined counts at this time.\n+;;\n+;; If the count is run-time determined, the register with the byte count\n+;; is clobbered by the copying code, and therefore it is forced to operand 2.\n+;;\n+;; We used to clobber operands 0 and 1.  However, a change to regrename.c\n+;; broke this semantic for pseudo registers.  We can't use match_scratch\n+;; as this requires two registers in the class R1_REGS when the MEMs for\n+;; operands 0 and 1 are both equivalent to symbolic MEMs.  Thus, we are\n+;; forced to internally copy operands 0 and 1 to operands 7 and 8,\n+;; respectively.  We then split or peephole optimize after reload.\n+(define_insn \"movstrdi_prereload\"\n+  [(set (mem:BLK (match_operand:DI 0 \"register_operand\" \"r,r\"))\n+\t(mem:BLK (match_operand:DI 1 \"register_operand\" \"r,r\")))\n+   (clobber (match_operand:DI 2 \"register_operand\" \"=r,r\"))\t;loop cnt/tmp\n+   (clobber (match_operand:DI 3 \"register_operand\" \"=&r,&r\"))\t;item tmp1\n+   (clobber (match_operand:DI 6 \"register_operand\" \"=&r,&r\"))\t;item tmp2\n+   (clobber (match_operand:DI 7 \"register_operand\" \"=&r,&r\"))\t;item tmp3\n+   (clobber (match_operand:DI 8 \"register_operand\" \"=&r,&r\"))\t;item tmp4\n+   (use (match_operand:DI 4 \"arith_operand\" \"J,2\"))\t ;byte count\n+   (use (match_operand:DI 5 \"const_int_operand\" \"n,n\"))] ;alignment\n+  \"TARGET_64BIT\"\n+  \"#\"\n+  [(set_attr \"type\" \"multi,multi\")])\n+\n+(define_split\n+  [(parallel [(set (mem:BLK (match_operand:DI 0 \"register_operand\" \"\"))\n+\t\t   (mem:BLK (match_operand:DI 1 \"register_operand\" \"\")))\n+\t      (clobber (match_operand:DI 2 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:DI 3 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:DI 6 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:DI 7 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:DI 8 \"register_operand\" \"\"))\n+\t      (use (match_operand:DI 4 \"arith_operand\" \"\"))\n+\t      (use (match_operand:DI 5 \"const_int_operand\" \"\"))])]\n+  \"TARGET_64BIT && reload_completed && !flag_peephole2\"\n+  [(set (match_dup 7) (match_dup 0))\n+   (set (match_dup 8) (match_dup 1))\n+   (parallel [(set (mem:BLK (match_dup 7)) (mem:BLK (match_dup 8)))\n+   \t      (clobber (match_dup 2))\n+   \t      (clobber (match_dup 3))\n+   \t      (clobber (match_dup 6))\n+   \t      (clobber (match_dup 7))\n+   \t      (clobber (match_dup 8))\n+   \t      (use (match_dup 4))\n+   \t      (use (match_dup 5))\n+\t      (const_int 0)])]\n+  \"\")\n+\n+(define_peephole2\n+  [(parallel [(set (mem:BLK (match_operand:DI 0 \"register_operand\" \"\"))\n+\t\t   (mem:BLK (match_operand:DI 1 \"register_operand\" \"\")))\n+\t      (clobber (match_operand:DI 2 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:DI 3 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:DI 6 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:DI 7 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:DI 8 \"register_operand\" \"\"))\n+\t      (use (match_operand:DI 4 \"arith_operand\" \"\"))\n+\t      (use (match_operand:DI 5 \"const_int_operand\" \"\"))])]\n+  \"TARGET_64BIT\"\n+  [(parallel [(set (mem:BLK (match_dup 7)) (mem:BLK (match_dup 8)))\n+   \t      (clobber (match_dup 2))\n+   \t      (clobber (match_dup 3))\n+   \t      (clobber (match_dup 6))\n+   \t      (clobber (match_dup 7))\n+   \t      (clobber (match_dup 8))\n+   \t      (use (match_dup 4))\n+   \t      (use (match_dup 5))\n+\t      (const_int 0)])]\n+  \"\n+{\n+  if (dead_or_set_p (curr_insn, operands[0]))\n+    operands[7] = operands[0];\n+  else\n+    emit_insn (gen_rtx_SET (VOIDmode, operands[7], operands[0]));\n+\n+  if (dead_or_set_p (curr_insn, operands[1]))\n+    operands[8] = operands[1];\n+  else\n+    emit_insn (gen_rtx_SET (VOIDmode, operands[8], operands[1]));\n+}\")\n+\n+(define_insn \"movstrdi_postreload\"\n+  [(set (mem:BLK (match_operand:DI 0 \"register_operand\" \"r,r\"))\n+\t(mem:BLK (match_operand:DI 1 \"register_operand\" \"r,r\")))\n+   (clobber (match_operand:DI 2 \"register_operand\" \"=r,r\"))\t;loop cnt/tmp\n+   (clobber (match_operand:DI 3 \"register_operand\" \"=&r,&r\"))\t;item tmp1\n+   (clobber (match_operand:DI 6 \"register_operand\" \"=&r,&r\"))\t;item tmp2\n+   (clobber (match_dup 0))\n+   (clobber (match_dup 1))\n+   (use (match_operand:DI 4 \"arith_operand\" \"J,2\"))\t ;byte count\n+   (use (match_operand:DI 5 \"const_int_operand\" \"n,n\"))  ;alignment\n+   (const_int 0)]\n+  \"TARGET_64BIT && reload_completed\"\n+  \"* return output_block_move (operands, !which_alternative);\"\n+  [(set_attr \"type\" \"multi,multi\")])\n+\n+(define_expand \"clrstrsi\"\n+  [(parallel [(set (match_operand:BLK 0 \"\" \"\")\n+\t\t   (const_int 0))\n+\t      (clobber (match_dup 3))\n+\t      (clobber (match_dup 4))\n+\t      (use (match_operand:SI 1 \"arith_operand\" \"\"))\n+\t      (use (match_operand:SI 2 \"const_int_operand\" \"\"))])]\n+  \"!TARGET_64BIT && optimize > 0\"\n+  \"\n+{\n+  int size, align;\n+\n+  /* Undetermined size, use the library routine.  */\n+  if (GET_CODE (operands[1]) != CONST_INT)\n+    FAIL;\n+\n+  size = INTVAL (operands[1]);\n+  align = INTVAL (operands[2]);\n+  align = align > 4 ? 4 : align;\n+\n+  /* If size/alignment is large, then use the library routines.  */\n+  if (size / align > 16)\n+    FAIL;\n+\n+  /* This does happen, but not often enough to worry much about.  */\n+  if (size / align < MOVE_RATIO)\n+    FAIL;\n+  \n+  /* Fall through means we're going to use our block clear pattern.  */\n+  operands[0]\n+    = replace_equiv_address (operands[0],\n+\t\t\t     copy_to_mode_reg (SImode, XEXP (operands[0], 0)));\n+  operands[3] = gen_reg_rtx (SImode);\n+  operands[4] = gen_reg_rtx (SImode);\n+}\")\n+\n+(define_insn \"clrstrsi_prereload\"\n+  [(set (mem:BLK (match_operand:SI 0 \"register_operand\" \"r,r\"))\n+\t(const_int 0))\n+   (clobber (match_operand:SI 1 \"register_operand\" \"=r,r\"))\t;loop cnt/tmp\n+   (clobber (match_operand:SI 4 \"register_operand\" \"=&r,&r\"))\t;tmp1\n+   (use (match_operand:SI 2 \"arith_operand\" \"J,1\"))\t ;byte count\n+   (use (match_operand:SI 3 \"const_int_operand\" \"n,n\"))] ;alignment\n+  \"!TARGET_64BIT\"\n+  \"#\"\n+  [(set_attr \"type\" \"multi,multi\")])\n+\n+(define_split\n+  [(parallel [(set (mem:BLK (match_operand:SI 0 \"register_operand\" \"\"))\n+\t\t   (const_int 0))\n+\t      (clobber (match_operand:SI 1 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:SI 4 \"register_operand\" \"\"))\n+\t      (use (match_operand:SI 2 \"arith_operand\" \"\"))\n+\t      (use (match_operand:SI 3 \"const_int_operand\" \"\"))])]\n+  \"!TARGET_64BIT && reload_completed && !flag_peephole2\"\n+  [(set (match_dup 4) (match_dup 0))\n+   (parallel [(set (mem:BLK (match_dup 4)) (const_int 0))\n+   \t      (clobber (match_dup 1))\n+   \t      (clobber (match_dup 4))\n+   \t      (use (match_dup 2))\n+   \t      (use (match_dup 3))\n+\t      (const_int 0)])]\n+  \"\")\n+\n+(define_peephole2\n+  [(parallel [(set (mem:BLK (match_operand:SI 0 \"register_operand\" \"\"))\n+\t\t   (const_int 0))\n+\t      (clobber (match_operand:SI 1 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:SI 4 \"register_operand\" \"\"))\n+\t      (use (match_operand:SI 2 \"arith_operand\" \"\"))\n+\t      (use (match_operand:SI 3 \"const_int_operand\" \"\"))])]\n+  \"!TARGET_64BIT\"\n+  [(parallel [(set (mem:BLK (match_dup 4)) (const_int 0))\n+   \t      (clobber (match_dup 1))\n+   \t      (clobber (match_dup 4))\n+   \t      (use (match_dup 2))\n+   \t      (use (match_dup 3))\n+\t      (const_int 0)])]\n+  \"\n+{\n+  if (dead_or_set_p (curr_insn, operands[0]))\n+    operands[4] = operands[0];\n+  else\n+    emit_insn (gen_rtx_SET (VOIDmode, operands[4], operands[0]));\n+}\")\n+\n+(define_insn \"clrstrsi_postreload\"\n+  [(set (mem:BLK (match_operand:SI 0 \"register_operand\" \"r,r\"))\n+\t(const_int 0))\n+   (clobber (match_operand:SI 1 \"register_operand\" \"=r,r\"))\t;loop cnt/tmp\n+   (clobber (match_dup 0))\n+   (use (match_operand:SI 2 \"arith_operand\" \"J,1\"))\t ;byte count\n+   (use (match_operand:SI 3 \"const_int_operand\" \"n,n\"))  ;alignment\n+   (const_int 0)]\n+  \"!TARGET_64BIT && reload_completed\"\n+  \"* return output_block_clear (operands, !which_alternative);\"\n+  [(set_attr \"type\" \"multi,multi\")])\n+\n+(define_expand \"clrstrdi\"\n+  [(parallel [(set (match_operand:BLK 0 \"\" \"\")\n+\t\t   (const_int 0))\n+\t      (clobber (match_dup 3))\n+\t      (clobber (match_dup 4))\n+\t      (use (match_operand:DI 1 \"arith_operand\" \"\"))\n+\t      (use (match_operand:DI 2 \"const_int_operand\" \"\"))])]\n+  \"TARGET_64BIT && optimize > 0\"\n+  \"\n+{\n+  int size, align;\n+\n+  /* Undetermined size, use the library routine.  */\n+  if (GET_CODE (operands[1]) != CONST_INT)\n+    FAIL;\n+\n+  size = INTVAL (operands[1]);\n+  align = INTVAL (operands[2]);\n+  align = align > 8 ? 8 : align;\n+\n+  /* If size/alignment is large, then use the library routines.  */\n+  if (size / align > 16)\n+    FAIL;\n+\n+  /* This does happen, but not often enough to worry much about.  */\n+  if (size / align < MOVE_RATIO)\n+    FAIL;\n+  \n+  /* Fall through means we're going to use our block clear pattern.  */\n+  operands[0]\n+    = replace_equiv_address (operands[0],\n+\t\t\t     copy_to_mode_reg (DImode, XEXP (operands[0], 0)));\n+  operands[3] = gen_reg_rtx (DImode);\n+  operands[4] = gen_reg_rtx (DImode);\n+}\")\n+\n+(define_insn \"clrstrdi_prereload\"\n+  [(set (mem:BLK (match_operand:DI 0 \"register_operand\" \"r,r\"))\n+\t(const_int 0))\n+   (clobber (match_operand:DI 1 \"register_operand\" \"=r,r\"))\t;loop cnt/tmp\n+   (clobber (match_operand:DI 4 \"register_operand\" \"=&r,&r\"))\t;item tmp1\n+   (use (match_operand:DI 2 \"arith_operand\" \"J,1\"))\t ;byte count\n+   (use (match_operand:DI 3 \"const_int_operand\" \"n,n\"))] ;alignment\n+  \"TARGET_64BIT\"\n+  \"#\"\n+  [(set_attr \"type\" \"multi,multi\")])\n+\n+(define_split\n+  [(parallel [(set (mem:BLK (match_operand:DI 0 \"register_operand\" \"\"))\n+\t\t   (const_int 0))\n+\t      (clobber (match_operand:DI 1 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:DI 4 \"register_operand\" \"\"))\n+\t      (use (match_operand:DI 2 \"arith_operand\" \"\"))\n+\t      (use (match_operand:DI 3 \"const_int_operand\" \"\"))])]\n+  \"TARGET_64BIT && reload_completed && !flag_peephole2\"\n+  [(set (match_dup 4) (match_dup 0))\n+   (parallel [(set (mem:BLK (match_dup 4)) (const_int 0))\n+   \t      (clobber (match_dup 1))\n+   \t      (clobber (match_dup 4))\n+   \t      (use (match_dup 2))\n+   \t      (use (match_dup 3))\n+\t      (const_int 0)])]\n+  \"\")\n+\n+(define_peephole2\n+  [(parallel [(set (mem:BLK (match_operand:DI 0 \"register_operand\" \"\"))\n+\t\t   (const_int 0))\n+\t      (clobber (match_operand:DI 1 \"register_operand\" \"\"))\n+\t      (clobber (match_operand:DI 4 \"register_operand\" \"\"))\n+\t      (use (match_operand:DI 2 \"arith_operand\" \"\"))\n+\t      (use (match_operand:DI 3 \"const_int_operand\" \"\"))])]\n+  \"TARGET_64BIT\"\n+  [(parallel [(set (mem:BLK (match_dup 4)) (const_int 0))\n+   \t      (clobber (match_dup 1))\n+   \t      (clobber (match_dup 4))\n+   \t      (use (match_dup 2))\n+   \t      (use (match_dup 3))\n+\t      (const_int 0)])]\n+  \"\n+{  \n+  if (dead_or_set_p (curr_insn, operands[0]))\n+    operands[4] = operands[0];\n+  else\n+    emit_insn (gen_rtx_SET (VOIDmode, operands[4], operands[0]));\n+}\")\n+\n+(define_insn \"clrstrdi_postreload\"\n+  [(set (mem:BLK (match_operand:DI 0 \"register_operand\" \"r,r\"))\n+\t(const_int 0))\n+   (clobber (match_operand:DI 1 \"register_operand\" \"=r,r\"))\t;loop cnt/tmp\n+   (clobber (match_dup 0))\n+   (use (match_operand:DI 2 \"arith_operand\" \"J,1\"))\t ;byte count\n+   (use (match_operand:DI 3 \"const_int_operand\" \"n,n\"))  ;alignment\n+   (const_int 0)]\n+  \"TARGET_64BIT && reload_completed\"\n+  \"* return output_block_clear (operands, !which_alternative);\"\n+  [(set_attr \"type\" \"multi,multi\")])\n \f\n ;; Floating point move insns\n "}]}