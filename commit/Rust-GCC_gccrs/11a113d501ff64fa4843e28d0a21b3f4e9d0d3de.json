{"sha": "11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "node_id": "C_kwDOANBUbNoAKDExYTExM2Q1MDFmZjY0ZmE0ODQzZTI4ZDBhMjFiM2Y0ZTlkMGQzZGU", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2022-09-29T10:32:54Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2022-09-29T10:32:54Z"}, "message": "aarch64: Simplify feature definitions\n\nCurrently the aarch64-option-extensions.def entries, the\naarch64-cores.def entries, and the AARCH64_FL_FOR_* macros\nhave a transitive closure of dependencies that is maintained by hand.\nThis is a bit error-prone and is becoming less tenable as more features\nare added.  The main point of this patch is to maintain the closure\nautomatically instead.\n\nFor example, the +sve2-aes extension requires sve2 and aes.\nThis is now described using:\n\n  AARCH64_OPT_EXTENSION(\"sve2-aes\", SVE2_AES, (SVE2, AES), ...)\n\nIf life was simple, we could just give the name of the feature\nand the list of features that it requires/depends on.  But sadly\nthings are more complicated.  For example:\n\n- the legacy +crypto option enables aes and sha2 only, but +nocrypto\n  disables all crypto-related extensions, including sm4.\n\n- +fp16fml enables fp16, but armv8.4-a enables fp16fml without fp16.\n  fp16fml only has an effect when fp16 is also present; see the\n  comments for more details.\n\n- +bf16 enables simd, but +bf16+nosimd is valid and enables just the\n  scalar bf16 instructions.  rdma behaves similarly.\n\nTo handle cases like these, the option entries have extra fields to\nspecify what an explicit +foo enables and what an explicit +nofoo\ndisables, in addition to the absolute dependencies.\n\nThe other main changes are:\n\n- AARCH64_FL_* are now defined automatically.\n\n- the feature list for each architecture level moves from aarch64.h\n  to aarch64-arches.def.\n\nAs a consequence, we now have a (redundant) V8A feature flag.\n\nWhile there, the patch uses a new typedef, aarch64_feature_flags,\nfor the set of feature flags.  This should make it easier to switch\nto a class if we run out of bits in the uint64_t.\n\nFor now the patch hardcodes the fact that crypto is the only\nsynthetic option.  A later patch will remove this field.\n\nTo test for things that might not be covered by the testsuite,\nI made the driver print out the all_extensions, all_cores and\nall_archs arrays before and after the patch, with the following\ntweaks:\n\n- renumber the old AARCH64_FL_* bit assignments to match the .def order\n- remove the new V8A flag when printing the new tables\n- treat CRYPTO and CRYPTO | AES | SHA2 the same way when printing the\n  core tables\n\n(On the last point: some cores enabled just CRYPTO while others enabled\nCRYPTO, AES and SHA2.  This doesn't cause a difference in behaviour\nbecause of how the dependent macros are defined.  With the new scheme,\nall entries with CRYPTO automatically get AES and SHA2 too.)\n\nThe only difference is that +nofp now turns off dotprod.  This was\nanother instance of an incomplete transitive closure, but unlike the\ninstances fixed in a previous patch, it had no observable effect.\n\ngcc/\n\t* config/aarch64/aarch64-option-extensions.def: Switch to a new format.\n\t* config/aarch64/aarch64-cores.def: Use the same format to specify\n\tlists of features.\n\t* config/aarch64/aarch64-arches.def: Likewise, moving that information\n\tfrom aarch64.h.\n\t* config/aarch64/aarch64-opts.h (aarch64_feature_flags): New typedef.\n\t* config/aarch64/aarch64.h (aarch64_feature): New class enum.\n\tTurn AARCH64_FL_* macros into constexprs, getting the definitions\n\tfrom aarch64-option-extensions.def.  Remove AARCH64_FL_FOR_* macros.\n\t* common/config/aarch64/aarch64-common.cc: Include\n\taarch64-feature-deps.h.\n\t(all_extensions): Update for new .def format.\n\t(all_extensions_by_on, all_cores, all_architectures): Likewise.\n\t* config/aarch64/driver-aarch64.cc: Include aarch64-feature-deps.h.\n\t(aarch64_extensions): Update for new .def format.\n\t(aarch64_cpu_data, aarch64_arches): Likewise.\n\t* config/aarch64/aarch64.cc: Include aarch64-feature-deps.h.\n\t(all_architectures, all_cores): Update for new .def format.\n\t* config/aarch64/aarch64-sve-builtins.cc\n\t(check_required_extensions): Likewise.", "tree": {"sha": "5a285e3381aa74ea482ead1043d8d7b206526c17", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/5a285e3381aa74ea482ead1043d8d7b206526c17"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "html_url": "https://github.com/Rust-GCC/gccrs/commit/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "c067c474f85b1e9c56fb34dd51ef0eec9221b766", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c067c474f85b1e9c56fb34dd51ef0eec9221b766", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c067c474f85b1e9c56fb34dd51ef0eec9221b766"}], "stats": {"total": 828, "additions": 374, "deletions": 454}, "files": [{"sha": "e5c83547bb2c168d01208ca4611511d2ce5f4302", "filename": "gcc/common/config/aarch64/aarch64-common.cc", "status": "modified", "additions": 18, "deletions": 11, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fcommon%2Fconfig%2Faarch64%2Faarch64-common.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fcommon%2Fconfig%2Faarch64%2Faarch64-common.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcommon%2Fconfig%2Faarch64%2Faarch64-common.cc?ref=11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "patch": "@@ -30,6 +30,7 @@\n #include \"opts.h\"\n #include \"flags.h\"\n #include \"diagnostic.h\"\n+#include \"config/aarch64/aarch64-feature-deps.h\"\n \n #ifdef  TARGET_BIG_ENDIAN_DEFAULT\n #undef  TARGET_DEFAULT_TARGET_FLAGS\n@@ -138,9 +139,12 @@ struct aarch64_option_extension\n /* ISA extensions in AArch64.  */\n static const struct aarch64_option_extension all_extensions[] =\n {\n-#define AARCH64_OPT_EXTENSION(NAME, FLAG_CANONICAL, FLAGS_ON, FLAGS_OFF, \\\n-\t\t\t      SYNTHETIC, Z) \\\n-  {NAME, FLAG_CANONICAL, FLAGS_ON, FLAGS_OFF, SYNTHETIC},\n+#define AARCH64_OPT_EXTENSION(NAME, IDENT, C, D, E, F) \\\n+  {NAME, AARCH64_FL_##IDENT, \\\n+   feature_deps::IDENT ().explicit_on & ~AARCH64_FL_##IDENT, \\\n+   feature_deps::get_flags_off (feature_deps::root_off_##IDENT) \\\n+   & ~AARCH64_FL_##IDENT, \\\n+   AARCH64_FL_##IDENT == AARCH64_FL_CRYPTO},\n #include \"config/aarch64/aarch64-option-extensions.def\"\n   {NULL, 0, 0, 0, false}\n };\n@@ -149,9 +153,12 @@ static const struct aarch64_option_extension all_extensions[] =\n    bits and extension turned on.  Cached for efficiency.  */\n static struct aarch64_option_extension all_extensions_by_on[] =\n {\n-#define AARCH64_OPT_EXTENSION(NAME, FLAG_CANONICAL, FLAGS_ON, FLAGS_OFF, \\\n-\t\t\t      SYNTHETIC, Z) \\\n-  {NAME, FLAG_CANONICAL, FLAGS_ON, FLAGS_OFF, SYNTHETIC},\n+#define AARCH64_OPT_EXTENSION(NAME, IDENT, C, D, E, F) \\\n+  {NAME, AARCH64_FL_##IDENT, \\\n+   feature_deps::IDENT ().explicit_on & ~AARCH64_FL_##IDENT, \\\n+   feature_deps::get_flags_off (feature_deps::root_off_##IDENT) \\\n+   & ~AARCH64_FL_##IDENT, \\\n+   AARCH64_FL_##IDENT == AARCH64_FL_CRYPTO},\n #include \"config/aarch64/aarch64-option-extensions.def\"\n   {NULL, 0, 0, 0, false}\n };\n@@ -174,18 +181,18 @@ struct arch_to_arch_name\n    the default set of architectural feature flags they support.  */\n static const struct processor_name_to_arch all_cores[] =\n {\n-#define AARCH64_CORE(NAME, X, IDENT, ARCH_IDENT, FLAGS, COSTS, IMP, PART, VARIANT) \\\n-  {NAME, AARCH64_ARCH_##ARCH_IDENT, AARCH64_FL_FOR_##ARCH_IDENT | FLAGS},\n+#define AARCH64_CORE(NAME, CORE_IDENT, C, ARCH_IDENT, E, F, G, H, I) \\\n+  {NAME, AARCH64_ARCH_##ARCH_IDENT, feature_deps::cpu_##CORE_IDENT},\n #include \"config/aarch64/aarch64-cores.def\"\n-  {\"generic\", AARCH64_ARCH_V8A, AARCH64_FL_FOR_V8A},\n+  {\"generic\", AARCH64_ARCH_V8A, feature_deps::V8A ().enable},\n   {\"\", aarch64_no_arch, 0}\n };\n \n /* Map architecture revisions to their string representation.  */\n static const struct arch_to_arch_name all_architectures[] =\n {\n-#define AARCH64_ARCH(NAME, CORE, ARCH_IDENT, ARCH, FLAGS) \\\n-  {AARCH64_ARCH_##ARCH_IDENT, NAME, FLAGS},\n+#define AARCH64_ARCH(NAME, B, ARCH_IDENT, D, E)\t\\\n+  {AARCH64_ARCH_##ARCH_IDENT, NAME, feature_deps::ARCH_IDENT ().enable},\n #include \"config/aarch64/aarch64-arches.def\"\n   {aarch64_no_arch, \"\", 0}\n };"}, {"sha": "9f82466181d3d528c35355d49b30989348119a23", "filename": "gcc/config/aarch64/aarch64-arches.def", "status": "modified", "additions": 14, "deletions": 14, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-arches.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-arches.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-arches.def?ref=11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "patch": "@@ -30,19 +30,19 @@\n    Due to the assumptions about the positions of these fields in config.gcc,\n    NAME should be kept as the first argument.  */\n \n-AARCH64_ARCH(\"armv8-a\",\t      generic,\t     V8A,\t8,  AARCH64_FL_FOR_V8A)\n-AARCH64_ARCH(\"armv8.1-a\",     generic,\t     V8_1A,\t8,  AARCH64_FL_FOR_V8_1A)\n-AARCH64_ARCH(\"armv8.2-a\",     generic,\t     V8_2A,\t8,  AARCH64_FL_FOR_V8_2A)\n-AARCH64_ARCH(\"armv8.3-a\",     generic,\t     V8_3A,\t8,  AARCH64_FL_FOR_V8_3A)\n-AARCH64_ARCH(\"armv8.4-a\",     generic,\t     V8_4A,\t8,  AARCH64_FL_FOR_V8_4A)\n-AARCH64_ARCH(\"armv8.5-a\",     generic,\t     V8_5A,\t8,  AARCH64_FL_FOR_V8_5A)\n-AARCH64_ARCH(\"armv8.6-a\",     generic,\t     V8_6A,\t8,  AARCH64_FL_FOR_V8_6A)\n-AARCH64_ARCH(\"armv8.7-a\",     generic,       V8_7A,     8,  AARCH64_FL_FOR_V8_7A)\n-AARCH64_ARCH(\"armv8.8-a\",     generic,       V8_8A,     8,  AARCH64_FL_FOR_V8_8A)\n-AARCH64_ARCH(\"armv8-r\",       generic,\t     V8R  ,\t8,  AARCH64_FL_FOR_V8R)\n-AARCH64_ARCH(\"armv9-a\",       generic,\t     V9A  ,\t9,  AARCH64_FL_FOR_V9A)\n-AARCH64_ARCH(\"armv9.1-a\",     generic,       V9_1A,     9,  AARCH64_FL_FOR_V9_1A)\n-AARCH64_ARCH(\"armv9.2-a\",     generic,       V9_2A,     9,  AARCH64_FL_FOR_V9_2A)\n-AARCH64_ARCH(\"armv9.3-a\",     generic,       V9_3A,     9,  AARCH64_FL_FOR_V9_3A)\n+AARCH64_ARCH(\"armv8-a\",       generic,       V8A,       8,  (SIMD))\n+AARCH64_ARCH(\"armv8.1-a\",     generic,       V8_1A,     8,  (V8A, LSE, CRC, RDMA))\n+AARCH64_ARCH(\"armv8.2-a\",     generic,       V8_2A,     8,  (V8_1A))\n+AARCH64_ARCH(\"armv8.3-a\",     generic,       V8_3A,     8,  (V8_2A, PAUTH))\n+AARCH64_ARCH(\"armv8.4-a\",     generic,       V8_4A,     8,  (V8_3A, F16FML, DOTPROD, FLAGM))\n+AARCH64_ARCH(\"armv8.5-a\",     generic,       V8_5A,     8,  (V8_4A, SB, SSBS, PREDRES))\n+AARCH64_ARCH(\"armv8.6-a\",     generic,       V8_6A,     8,  (V8_5A, I8MM, BF16))\n+AARCH64_ARCH(\"armv8.7-a\",     generic,       V8_7A,     8,  (V8_6A, LS64))\n+AARCH64_ARCH(\"armv8.8-a\",     generic,       V8_8A,     8,  (V8_7A, MOPS))\n+AARCH64_ARCH(\"armv8-r\",       generic,       V8R  ,     8,  (V8_4A))\n+AARCH64_ARCH(\"armv9-a\",       generic,       V9A  ,     9,  (V8_5A, SVE2))\n+AARCH64_ARCH(\"armv9.1-a\",     generic,       V9_1A,     9,  (V8_6A, V9A))\n+AARCH64_ARCH(\"armv9.2-a\",     generic,       V9_2A,     9,  (V8_7A, V9_1A))\n+AARCH64_ARCH(\"armv9.3-a\",     generic,       V9_3A,     9,  (V8_8A, V9_2A))\n \n #undef AARCH64_ARCH"}, {"sha": "60299160bb676903ec6c778219b1a43a50ee2126", "filename": "gcc/config/aarch64/aarch64-cores.def", "status": "modified", "additions": 65, "deletions": 65, "changes": 130, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-cores.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-cores.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-cores.def?ref=11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "patch": "@@ -46,132 +46,132 @@\n /* ARMv8-A Architecture Processors.  */\n \n /* ARM ('A') cores. */\n-AARCH64_CORE(\"cortex-a34\",  cortexa34, cortexa53, V8A,  AARCH64_FL_CRC, cortexa35, 0x41, 0xd02, -1)\n-AARCH64_CORE(\"cortex-a35\",  cortexa35, cortexa53, V8A,  AARCH64_FL_CRC, cortexa35, 0x41, 0xd04, -1)\n-AARCH64_CORE(\"cortex-a53\",  cortexa53, cortexa53, V8A,  AARCH64_FL_CRC, cortexa53, 0x41, 0xd03, -1)\n-AARCH64_CORE(\"cortex-a57\",  cortexa57, cortexa57, V8A,  AARCH64_FL_CRC, cortexa57, 0x41, 0xd07, -1)\n-AARCH64_CORE(\"cortex-a72\",  cortexa72, cortexa57, V8A,  AARCH64_FL_CRC, cortexa72, 0x41, 0xd08, -1)\n-AARCH64_CORE(\"cortex-a73\",  cortexa73, cortexa57, V8A,  AARCH64_FL_CRC, cortexa73, 0x41, 0xd09, -1)\n+AARCH64_CORE(\"cortex-a34\",  cortexa34, cortexa53, V8A,  (CRC), cortexa35, 0x41, 0xd02, -1)\n+AARCH64_CORE(\"cortex-a35\",  cortexa35, cortexa53, V8A,  (CRC), cortexa35, 0x41, 0xd04, -1)\n+AARCH64_CORE(\"cortex-a53\",  cortexa53, cortexa53, V8A,  (CRC), cortexa53, 0x41, 0xd03, -1)\n+AARCH64_CORE(\"cortex-a57\",  cortexa57, cortexa57, V8A,  (CRC), cortexa57, 0x41, 0xd07, -1)\n+AARCH64_CORE(\"cortex-a72\",  cortexa72, cortexa57, V8A,  (CRC), cortexa72, 0x41, 0xd08, -1)\n+AARCH64_CORE(\"cortex-a73\",  cortexa73, cortexa57, V8A,  (CRC), cortexa73, 0x41, 0xd09, -1)\n \n /* Cavium ('C') cores. */\n-AARCH64_CORE(\"thunderx\",      thunderx,      thunderx,  V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderx,  0x43, 0x0a0, -1)\n+AARCH64_CORE(\"thunderx\",      thunderx,      thunderx,  V8A,  (CRC, CRYPTO), thunderx,  0x43, 0x0a0, -1)\n /* Do not swap around \"thunderxt88p1\" and \"thunderxt88\",\n    this order is required to handle variant correctly. */\n-AARCH64_CORE(\"thunderxt88p1\", thunderxt88p1, thunderx,  V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO,\tthunderxt88,  0x43, 0x0a1, 0)\n-AARCH64_CORE(\"thunderxt88\",   thunderxt88,   thunderx,  V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderxt88,  0x43, 0x0a1, -1)\n+AARCH64_CORE(\"thunderxt88p1\", thunderxt88p1, thunderx,  V8A,  (CRC, CRYPTO),\tthunderxt88,  0x43, 0x0a1, 0)\n+AARCH64_CORE(\"thunderxt88\",   thunderxt88,   thunderx,  V8A,  (CRC, CRYPTO), thunderxt88,  0x43, 0x0a1, -1)\n \n /* OcteonTX is the official name for T81/T83. */\n-AARCH64_CORE(\"octeontx\",      octeontx,      thunderx,  V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderx,  0x43, 0x0a0, -1)\n-AARCH64_CORE(\"octeontx81\",    octeontxt81,   thunderx,  V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderx,  0x43, 0x0a2, -1)\n-AARCH64_CORE(\"octeontx83\",    octeontxt83,   thunderx,  V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderx,  0x43, 0x0a3, -1)\n+AARCH64_CORE(\"octeontx\",      octeontx,      thunderx,  V8A,  (CRC, CRYPTO), thunderx,  0x43, 0x0a0, -1)\n+AARCH64_CORE(\"octeontx81\",    octeontxt81,   thunderx,  V8A,  (CRC, CRYPTO), thunderx,  0x43, 0x0a2, -1)\n+AARCH64_CORE(\"octeontx83\",    octeontxt83,   thunderx,  V8A,  (CRC, CRYPTO), thunderx,  0x43, 0x0a3, -1)\n \n-AARCH64_CORE(\"thunderxt81\",   thunderxt81,   thunderx,  V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderx,  0x43, 0x0a2, -1)\n-AARCH64_CORE(\"thunderxt83\",   thunderxt83,   thunderx,  V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO, thunderx,  0x43, 0x0a3, -1)\n+AARCH64_CORE(\"thunderxt81\",   thunderxt81,   thunderx,  V8A,  (CRC, CRYPTO), thunderx,  0x43, 0x0a2, -1)\n+AARCH64_CORE(\"thunderxt83\",   thunderxt83,   thunderx,  V8A,  (CRC, CRYPTO), thunderx,  0x43, 0x0a3, -1)\n \n /* Ampere Computing ('\\xC0') cores. */\n-AARCH64_CORE(\"ampere1\", ampere1, cortexa57, V8_6A, 0, ampere1, 0xC0, 0xac3, -1)\n+AARCH64_CORE(\"ampere1\", ampere1, cortexa57, V8_6A, (), ampere1, 0xC0, 0xac3, -1)\n /* Do not swap around \"emag\" and \"xgene1\",\n    this order is required to handle variant correctly. */\n-AARCH64_CORE(\"emag\",        emag,      xgene1,    V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO, emag, 0x50, 0x000, 3)\n+AARCH64_CORE(\"emag\",        emag,      xgene1,    V8A,  (CRC, CRYPTO), emag, 0x50, 0x000, 3)\n \n /* APM ('P') cores. */\n-AARCH64_CORE(\"xgene1\",      xgene1,    xgene1,    V8A,  0, xgene1, 0x50, 0x000, -1)\n+AARCH64_CORE(\"xgene1\",      xgene1,    xgene1,    V8A,  (), xgene1, 0x50, 0x000, -1)\n \n /* Qualcomm ('Q') cores. */\n-AARCH64_CORE(\"falkor\",      falkor,    falkor,    V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO | AARCH64_FL_RDMA, qdf24xx,   0x51, 0xC00, -1)\n-AARCH64_CORE(\"qdf24xx\",     qdf24xx,   falkor,    V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO | AARCH64_FL_RDMA, qdf24xx,   0x51, 0xC00, -1)\n+AARCH64_CORE(\"falkor\",      falkor,    falkor,    V8A,  (CRC, CRYPTO, RDMA), qdf24xx,   0x51, 0xC00, -1)\n+AARCH64_CORE(\"qdf24xx\",     qdf24xx,   falkor,    V8A,  (CRC, CRYPTO, RDMA), qdf24xx,   0x51, 0xC00, -1)\n \n /* Samsung ('S') cores. */\n-AARCH64_CORE(\"exynos-m1\",   exynosm1,  exynosm1,  V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO, exynosm1,  0x53, 0x001, -1)\n+AARCH64_CORE(\"exynos-m1\",   exynosm1,  exynosm1,  V8A,  (CRC, CRYPTO), exynosm1,  0x53, 0x001, -1)\n \n /* HXT ('h') cores. */\n-AARCH64_CORE(\"phecda\",      phecda,    falkor,    V8A,  AARCH64_FL_CRC | AARCH64_FL_CRYPTO, qdf24xx,   0x68, 0x000, -1)\n+AARCH64_CORE(\"phecda\",      phecda,    falkor,    V8A,  (CRC, CRYPTO), qdf24xx,   0x68, 0x000, -1)\n \n /* ARMv8.1-A Architecture Processors.  */\n \n /* Broadcom ('B') cores. */\n-AARCH64_CORE(\"thunderx2t99p1\",  thunderx2t99p1, thunderx2t99, V8_1A,  AARCH64_FL_CRYPTO, thunderx2t99, 0x42, 0x516, -1)\n-AARCH64_CORE(\"vulcan\",  vulcan, thunderx2t99, V8_1A,  AARCH64_FL_CRYPTO, thunderx2t99, 0x42, 0x516, -1)\n+AARCH64_CORE(\"thunderx2t99p1\",  thunderx2t99p1, thunderx2t99, V8_1A,  (CRYPTO), thunderx2t99, 0x42, 0x516, -1)\n+AARCH64_CORE(\"vulcan\",  vulcan, thunderx2t99, V8_1A,  (CRYPTO), thunderx2t99, 0x42, 0x516, -1)\n \n /* Cavium ('C') cores. */\n-AARCH64_CORE(\"thunderx2t99\",  thunderx2t99,  thunderx2t99, V8_1A,  AARCH64_FL_CRYPTO, thunderx2t99, 0x43, 0x0af, -1)\n+AARCH64_CORE(\"thunderx2t99\",  thunderx2t99,  thunderx2t99, V8_1A,  (CRYPTO), thunderx2t99, 0x43, 0x0af, -1)\n \n /* ARMv8.2-A Architecture Processors.  */\n \n /* ARM ('A') cores. */\n-AARCH64_CORE(\"cortex-a55\",  cortexa55, cortexa53, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD, cortexa53, 0x41, 0xd05, -1)\n-AARCH64_CORE(\"cortex-a75\",  cortexa75, cortexa57, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD, cortexa73, 0x41, 0xd0a, -1)\n-AARCH64_CORE(\"cortex-a76\",  cortexa76, cortexa57, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD, neoversen1, 0x41, 0xd0b, -1)\n-AARCH64_CORE(\"cortex-a76ae\",  cortexa76ae, cortexa57, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_SSBS, neoversen1, 0x41, 0xd0e, -1)\n-AARCH64_CORE(\"cortex-a77\",  cortexa77, cortexa57, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_SSBS, neoversen1, 0x41, 0xd0d, -1)\n-AARCH64_CORE(\"cortex-a78\",  cortexa78, cortexa57, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_SSBS | AARCH64_FL_PROFILE, neoversen1, 0x41, 0xd41, -1)\n-AARCH64_CORE(\"cortex-a78ae\",  cortexa78ae, cortexa57, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_SSBS | AARCH64_FL_PROFILE, neoversen1, 0x41, 0xd42, -1)\n-AARCH64_CORE(\"cortex-a78c\",  cortexa78c, cortexa57, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_SSBS | AARCH64_FL_PROFILE | AARCH64_FL_FLAGM | AARCH64_FL_PAUTH, neoversen1, 0x41, 0xd4b, -1)\n-AARCH64_CORE(\"cortex-a65\",  cortexa65, cortexa53, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_SSBS, cortexa73, 0x41, 0xd06, -1)\n-AARCH64_CORE(\"cortex-a65ae\",  cortexa65ae, cortexa53, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_SSBS, cortexa73, 0x41, 0xd43, -1)\n-AARCH64_CORE(\"cortex-x1\",  cortexx1, cortexa57, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_SSBS | AARCH64_FL_PROFILE, neoversen1, 0x41, 0xd44, -1)\n-AARCH64_CORE(\"ares\",  ares, cortexa57, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_PROFILE, neoversen1, 0x41, 0xd0c, -1)\n-AARCH64_CORE(\"neoverse-n1\",  neoversen1, cortexa57, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_PROFILE, neoversen1, 0x41, 0xd0c, -1)\n-AARCH64_CORE(\"neoverse-e1\",  neoversee1, cortexa53, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD | AARCH64_FL_SSBS, cortexa73, 0x41, 0xd4a, -1)\n+AARCH64_CORE(\"cortex-a55\",  cortexa55, cortexa53, V8_2A,  (F16, RCPC, DOTPROD), cortexa53, 0x41, 0xd05, -1)\n+AARCH64_CORE(\"cortex-a75\",  cortexa75, cortexa57, V8_2A,  (F16, RCPC, DOTPROD), cortexa73, 0x41, 0xd0a, -1)\n+AARCH64_CORE(\"cortex-a76\",  cortexa76, cortexa57, V8_2A,  (F16, RCPC, DOTPROD), neoversen1, 0x41, 0xd0b, -1)\n+AARCH64_CORE(\"cortex-a76ae\",  cortexa76ae, cortexa57, V8_2A,  (F16, RCPC, DOTPROD, SSBS), neoversen1, 0x41, 0xd0e, -1)\n+AARCH64_CORE(\"cortex-a77\",  cortexa77, cortexa57, V8_2A,  (F16, RCPC, DOTPROD, SSBS), neoversen1, 0x41, 0xd0d, -1)\n+AARCH64_CORE(\"cortex-a78\",  cortexa78, cortexa57, V8_2A,  (F16, RCPC, DOTPROD, SSBS, PROFILE), neoversen1, 0x41, 0xd41, -1)\n+AARCH64_CORE(\"cortex-a78ae\",  cortexa78ae, cortexa57, V8_2A,  (F16, RCPC, DOTPROD, SSBS, PROFILE), neoversen1, 0x41, 0xd42, -1)\n+AARCH64_CORE(\"cortex-a78c\",  cortexa78c, cortexa57, V8_2A,  (F16, RCPC, DOTPROD, SSBS, PROFILE, FLAGM, PAUTH), neoversen1, 0x41, 0xd4b, -1)\n+AARCH64_CORE(\"cortex-a65\",  cortexa65, cortexa53, V8_2A,  (F16, RCPC, DOTPROD, SSBS), cortexa73, 0x41, 0xd06, -1)\n+AARCH64_CORE(\"cortex-a65ae\",  cortexa65ae, cortexa53, V8_2A,  (F16, RCPC, DOTPROD, SSBS), cortexa73, 0x41, 0xd43, -1)\n+AARCH64_CORE(\"cortex-x1\",  cortexx1, cortexa57, V8_2A,  (F16, RCPC, DOTPROD, SSBS, PROFILE), neoversen1, 0x41, 0xd44, -1)\n+AARCH64_CORE(\"ares\",  ares, cortexa57, V8_2A,  (F16, RCPC, DOTPROD, PROFILE), neoversen1, 0x41, 0xd0c, -1)\n+AARCH64_CORE(\"neoverse-n1\",  neoversen1, cortexa57, V8_2A,  (F16, RCPC, DOTPROD, PROFILE), neoversen1, 0x41, 0xd0c, -1)\n+AARCH64_CORE(\"neoverse-e1\",  neoversee1, cortexa53, V8_2A,  (F16, RCPC, DOTPROD, SSBS), cortexa73, 0x41, 0xd4a, -1)\n \n /* Cavium ('C') cores. */\n-AARCH64_CORE(\"octeontx2\",      octeontx2,      cortexa57, V8_2A,  AARCH64_FL_CRYPTO | AARCH64_FL_PROFILE, cortexa57, 0x43, 0x0b0, -1)\n-AARCH64_CORE(\"octeontx2t98\",   octeontx2t98,   cortexa57, V8_2A,  AARCH64_FL_CRYPTO | AARCH64_FL_PROFILE, cortexa57, 0x43, 0x0b1, -1)\n-AARCH64_CORE(\"octeontx2t96\",   octeontx2t96,   cortexa57, V8_2A,  AARCH64_FL_CRYPTO | AARCH64_FL_PROFILE, cortexa57, 0x43, 0x0b2, -1)\n+AARCH64_CORE(\"octeontx2\",      octeontx2,      cortexa57, V8_2A,  (CRYPTO, PROFILE), cortexa57, 0x43, 0x0b0, -1)\n+AARCH64_CORE(\"octeontx2t98\",   octeontx2t98,   cortexa57, V8_2A,  (CRYPTO, PROFILE), cortexa57, 0x43, 0x0b1, -1)\n+AARCH64_CORE(\"octeontx2t96\",   octeontx2t96,   cortexa57, V8_2A,  (CRYPTO, PROFILE), cortexa57, 0x43, 0x0b2, -1)\n /* Note OcteonTX2 T93 is an alias to OcteonTX2 T96. */\n-AARCH64_CORE(\"octeontx2t93\",   octeontx2t93,   cortexa57, V8_2A,  AARCH64_FL_CRYPTO | AARCH64_FL_PROFILE, cortexa57, 0x43, 0x0b2, -1)\n-AARCH64_CORE(\"octeontx2f95\",   octeontx2f95,   cortexa57, V8_2A,  AARCH64_FL_CRYPTO | AARCH64_FL_PROFILE, cortexa57, 0x43, 0x0b3, -1)\n-AARCH64_CORE(\"octeontx2f95n\",  octeontx2f95n,  cortexa57, V8_2A,  AARCH64_FL_CRYPTO | AARCH64_FL_PROFILE, cortexa57, 0x43, 0x0b4, -1)\n-AARCH64_CORE(\"octeontx2f95mm\", octeontx2f95mm, cortexa57, V8_2A,  AARCH64_FL_CRYPTO | AARCH64_FL_PROFILE, cortexa57, 0x43, 0x0b5, -1)\n+AARCH64_CORE(\"octeontx2t93\",   octeontx2t93,   cortexa57, V8_2A,  (CRYPTO, PROFILE), cortexa57, 0x43, 0x0b2, -1)\n+AARCH64_CORE(\"octeontx2f95\",   octeontx2f95,   cortexa57, V8_2A,  (CRYPTO, PROFILE), cortexa57, 0x43, 0x0b3, -1)\n+AARCH64_CORE(\"octeontx2f95n\",  octeontx2f95n,  cortexa57, V8_2A,  (CRYPTO, PROFILE), cortexa57, 0x43, 0x0b4, -1)\n+AARCH64_CORE(\"octeontx2f95mm\", octeontx2f95mm, cortexa57, V8_2A,  (CRYPTO, PROFILE), cortexa57, 0x43, 0x0b5, -1)\n \n /* Fujitsu ('F') cores. */\n-AARCH64_CORE(\"a64fx\", a64fx, a64fx, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_SVE, a64fx, 0x46, 0x001, -1)\n+AARCH64_CORE(\"a64fx\", a64fx, a64fx, V8_2A,  (F16, SVE), a64fx, 0x46, 0x001, -1)\n \n /* HiSilicon ('H') cores. */\n-AARCH64_CORE(\"tsv110\",  tsv110, tsv110, V8_2A,  AARCH64_FL_CRYPTO | AARCH64_FL_F16 | AARCH64_FL_AES | AARCH64_FL_SHA2, tsv110,   0x48, 0xd01, -1)\n+AARCH64_CORE(\"tsv110\",  tsv110, tsv110, V8_2A,  (CRYPTO, F16), tsv110,   0x48, 0xd01, -1)\n \n /* ARMv8.3-A Architecture Processors.  */\n \n /* Marvell cores (TX3). */\n-AARCH64_CORE(\"thunderx3t110\",  thunderx3t110,  thunderx3t110, V8_3A,  AARCH64_FL_CRYPTO | AARCH64_FL_RCPC | AARCH64_FL_SM4 | AARCH64_FL_SHA3 | AARCH64_FL_F16FML, thunderx3t110, 0x43, 0x0b8, 0x0a)\n+AARCH64_CORE(\"thunderx3t110\",  thunderx3t110,  thunderx3t110, V8_3A,  (CRYPTO, RCPC, SM4, SHA3, F16FML), thunderx3t110, 0x43, 0x0b8, 0x0a)\n \n /* ARMv8.4-A Architecture Processors.  */\n \n /* Arm ('A') cores.  */\n-AARCH64_CORE(\"zeus\", zeus, cortexa57, V8_4A,  AARCH64_FL_SVE | AARCH64_FL_RCPC | AARCH64_FL_I8MM | AARCH64_FL_BF16 | AARCH64_FL_F16 | AARCH64_FL_PROFILE | AARCH64_FL_SSBS | AARCH64_FL_RNG, neoversev1, 0x41, 0xd40, -1)\n-AARCH64_CORE(\"neoverse-v1\", neoversev1, cortexa57, V8_4A,  AARCH64_FL_SVE | AARCH64_FL_RCPC | AARCH64_FL_I8MM | AARCH64_FL_BF16 | AARCH64_FL_F16 | AARCH64_FL_PROFILE | AARCH64_FL_SSBS | AARCH64_FL_RNG, neoversev1, 0x41, 0xd40, -1)\n-AARCH64_CORE(\"neoverse-512tvb\", neoverse512tvb, cortexa57, V8_4A,  AARCH64_FL_SVE | AARCH64_FL_RCPC | AARCH64_FL_I8MM | AARCH64_FL_BF16 | AARCH64_FL_F16 | AARCH64_FL_PROFILE | AARCH64_FL_SSBS | AARCH64_FL_RNG, neoverse512tvb, INVALID_IMP, INVALID_CORE, -1)\n+AARCH64_CORE(\"zeus\", zeus, cortexa57, V8_4A,  (SVE, RCPC, I8MM, BF16, PROFILE, SSBS, RNG), neoversev1, 0x41, 0xd40, -1)\n+AARCH64_CORE(\"neoverse-v1\", neoversev1, cortexa57, V8_4A,  (SVE, RCPC, I8MM, BF16, PROFILE, SSBS, RNG), neoversev1, 0x41, 0xd40, -1)\n+AARCH64_CORE(\"neoverse-512tvb\", neoverse512tvb, cortexa57, V8_4A,  (SVE, RCPC, I8MM, BF16, PROFILE, SSBS, RNG), neoverse512tvb, INVALID_IMP, INVALID_CORE, -1)\n \n /* Qualcomm ('Q') cores. */\n-AARCH64_CORE(\"saphira\",     saphira,    saphira,    V8_4A,  AARCH64_FL_CRYPTO | AARCH64_FL_RCPC, saphira,   0x51, 0xC01, -1)\n+AARCH64_CORE(\"saphira\",     saphira,    saphira,    V8_4A,  (CRYPTO, RCPC), saphira,   0x51, 0xC01, -1)\n \n /* ARMv8-A big.LITTLE implementations.  */\n \n-AARCH64_CORE(\"cortex-a57.cortex-a53\",  cortexa57cortexa53, cortexa53, V8A,  AARCH64_FL_CRC, cortexa57, 0x41, AARCH64_BIG_LITTLE (0xd07, 0xd03), -1)\n-AARCH64_CORE(\"cortex-a72.cortex-a53\",  cortexa72cortexa53, cortexa53, V8A,  AARCH64_FL_CRC, cortexa72, 0x41, AARCH64_BIG_LITTLE (0xd08, 0xd03), -1)\n-AARCH64_CORE(\"cortex-a73.cortex-a35\",  cortexa73cortexa35, cortexa53, V8A,  AARCH64_FL_CRC, cortexa73, 0x41, AARCH64_BIG_LITTLE (0xd09, 0xd04), -1)\n-AARCH64_CORE(\"cortex-a73.cortex-a53\",  cortexa73cortexa53, cortexa53, V8A,  AARCH64_FL_CRC, cortexa73, 0x41, AARCH64_BIG_LITTLE (0xd09, 0xd03), -1)\n+AARCH64_CORE(\"cortex-a57.cortex-a53\",  cortexa57cortexa53, cortexa53, V8A,  (CRC), cortexa57, 0x41, AARCH64_BIG_LITTLE (0xd07, 0xd03), -1)\n+AARCH64_CORE(\"cortex-a72.cortex-a53\",  cortexa72cortexa53, cortexa53, V8A,  (CRC), cortexa72, 0x41, AARCH64_BIG_LITTLE (0xd08, 0xd03), -1)\n+AARCH64_CORE(\"cortex-a73.cortex-a35\",  cortexa73cortexa35, cortexa53, V8A,  (CRC), cortexa73, 0x41, AARCH64_BIG_LITTLE (0xd09, 0xd04), -1)\n+AARCH64_CORE(\"cortex-a73.cortex-a53\",  cortexa73cortexa53, cortexa53, V8A,  (CRC), cortexa73, 0x41, AARCH64_BIG_LITTLE (0xd09, 0xd03), -1)\n \n /* ARM DynamIQ big.LITTLE configurations.  */\n \n-AARCH64_CORE(\"cortex-a75.cortex-a55\",  cortexa75cortexa55, cortexa53, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD, cortexa73, 0x41, AARCH64_BIG_LITTLE (0xd0a, 0xd05), -1)\n-AARCH64_CORE(\"cortex-a76.cortex-a55\",  cortexa76cortexa55, cortexa53, V8_2A,  AARCH64_FL_F16 | AARCH64_FL_RCPC | AARCH64_FL_DOTPROD, neoversen1, 0x41, AARCH64_BIG_LITTLE (0xd0b, 0xd05), -1)\n+AARCH64_CORE(\"cortex-a75.cortex-a55\",  cortexa75cortexa55, cortexa53, V8_2A,  (F16, RCPC, DOTPROD), cortexa73, 0x41, AARCH64_BIG_LITTLE (0xd0a, 0xd05), -1)\n+AARCH64_CORE(\"cortex-a76.cortex-a55\",  cortexa76cortexa55, cortexa53, V8_2A,  (F16, RCPC, DOTPROD), neoversen1, 0x41, AARCH64_BIG_LITTLE (0xd0b, 0xd05), -1)\n \n /* Armv8-R Architecture Processors.  */\n-AARCH64_CORE(\"cortex-r82\", cortexr82, cortexa53, V8R, 0, cortexa53, 0x41, 0xd15, -1)\n+AARCH64_CORE(\"cortex-r82\", cortexr82, cortexa53, V8R, (), cortexa53, 0x41, 0xd15, -1)\n \n /* Armv9.0-A Architecture Processors.  */\n \n /* Arm ('A') cores. */\n-AARCH64_CORE(\"cortex-a510\",  cortexa510, cortexa55, V9A,  AARCH64_FL_SVE2_BITPERM | AARCH64_FL_MEMTAG | AARCH64_FL_I8MM | AARCH64_FL_BF16, cortexa53, 0x41, 0xd46, -1)\n+AARCH64_CORE(\"cortex-a510\",  cortexa510, cortexa55, V9A,  (SVE2_BITPERM, MEMTAG, I8MM, BF16), cortexa53, 0x41, 0xd46, -1)\n \n-AARCH64_CORE(\"cortex-a710\",  cortexa710, cortexa57, V9A,  AARCH64_FL_SVE2_BITPERM | AARCH64_FL_MEMTAG | AARCH64_FL_I8MM | AARCH64_FL_BF16, neoversen2, 0x41, 0xd47, -1)\n+AARCH64_CORE(\"cortex-a710\",  cortexa710, cortexa57, V9A,  (SVE2_BITPERM, MEMTAG, I8MM, BF16), neoversen2, 0x41, 0xd47, -1)\n \n-AARCH64_CORE(\"cortex-x2\",  cortexx2, cortexa57, V9A,  AARCH64_FL_SVE2_BITPERM | AARCH64_FL_MEMTAG | AARCH64_FL_I8MM | AARCH64_FL_BF16, neoversen2, 0x41, 0xd48, -1)\n+AARCH64_CORE(\"cortex-x2\",  cortexx2, cortexa57, V9A,  (SVE2_BITPERM, MEMTAG, I8MM, BF16), neoversen2, 0x41, 0xd48, -1)\n \n-AARCH64_CORE(\"neoverse-n2\", neoversen2, cortexa57, V9A, AARCH64_FL_I8MM | AARCH64_FL_BF16 | AARCH64_FL_SVE2_BITPERM | AARCH64_FL_RNG | AARCH64_FL_MEMTAG | AARCH64_FL_PROFILE, neoversen2, 0x41, 0xd49, -1)\n+AARCH64_CORE(\"neoverse-n2\", neoversen2, cortexa57, V9A, (I8MM, BF16, SVE2_BITPERM, RNG, MEMTAG, PROFILE), neoversen2, 0x41, 0xd49, -1)\n \n-AARCH64_CORE(\"demeter\", demeter, cortexa57, V9A, AARCH64_FL_I8MM | AARCH64_FL_BF16 | AARCH64_FL_SVE2_BITPERM | AARCH64_FL_RNG | AARCH64_FL_MEMTAG | AARCH64_FL_PROFILE, neoversev2, 0x41, 0xd4f, -1)\n-AARCH64_CORE(\"neoverse-v2\", neoversev2, cortexa57, V9A, AARCH64_FL_I8MM | AARCH64_FL_BF16 | AARCH64_FL_SVE2_BITPERM | AARCH64_FL_RNG | AARCH64_FL_MEMTAG | AARCH64_FL_PROFILE, neoversev2, 0x41, 0xd4f, -1)\n+AARCH64_CORE(\"demeter\", demeter, cortexa57, V9A, (I8MM, BF16, SVE2_BITPERM, RNG, MEMTAG, PROFILE), neoversev2, 0x41, 0xd4f, -1)\n+AARCH64_CORE(\"neoverse-v2\", neoversev2, cortexa57, V9A, (I8MM, BF16, SVE2_BITPERM, RNG, MEMTAG, PROFILE), neoversev2, 0x41, 0xd4f, -1)\n \n #undef AARCH64_CORE"}, {"sha": "3e33cb2ce8467754cfc4a4a34e9d08f54adc2c73", "filename": "gcc/config/aarch64/aarch64-feature-deps.h", "status": "added", "additions": 121, "deletions": 0, "changes": 121, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-feature-deps.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-feature-deps.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-feature-deps.h?ref=11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "patch": "@@ -0,0 +1,121 @@\n+/* Feature dependency helpers for AArch64.\n+   Copyright (C) 2022 Free Software Foundation, Inc.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but\n+   WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+   General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+#ifndef AARCH64_FEATURE_DEPS_H\n+#define AARCH64_FEATURE_DEPS_H 1\n+\n+namespace {\n+namespace feature_deps {\n+\n+/* Together, these definitions of get_flags take a list of\n+   feature names (representing functions that are defined below)\n+   and return the set of associated flags.  */\n+constexpr aarch64_feature_flags get_flags () { return 0; }\n+\n+template<typename T1, typename ...Ts>\n+constexpr aarch64_feature_flags\n+get_flags (T1 i, Ts... args)\n+{\n+  return i ().flag | get_flags (args...);\n+}\n+\n+/* Like get_flags, but return the transitive closure of those features\n+   and the ones that they rely on.  */\n+constexpr aarch64_feature_flags get_enable () { return 0; }\n+\n+template<typename T1, typename ...Ts>\n+constexpr aarch64_feature_flags\n+get_enable (T1 i, Ts... args)\n+{\n+  return i ().enable | get_enable (args...);\n+}\n+\n+/* Define info<FEATURE> such that it has the following static constant\n+   variables:\n+\n+   - flag: the aarch64_feature_flags bit associated with FEATURE\n+\n+   - enable: the transitive closure of the features that FEATURE requires,\n+     plus FLAG itself\n+\n+   - explicit_on: the transitive closure of the features that an\n+     explicit +FEATURE enables, including FLAG itself.  This is\n+     always a superset of ENABLE\n+\n+   Also define a function FEATURE () that returns an info<FEATURE>\n+   (which is an empty structure, since all members are static).\n+\n+   Building up the list feature-by-feature ensures that the definition\n+   files are in topological order.  */\n+template<aarch64_feature> struct info;\n+\n+#define HANDLE(IDENT, REQUIRES, EXPLICIT_ON)\t\t\t\t\\\n+  template<> struct info<aarch64_feature::IDENT> {\t\t\t\\\n+    static constexpr auto flag = AARCH64_FL_##IDENT;\t\t\t\\\n+    static constexpr auto enable = flag | get_enable REQUIRES;\t\t\\\n+    static constexpr auto explicit_on = enable | get_enable EXPLICIT_ON; \\\n+  };\t\t\t\t\t\t\t\t\t\\\n+  constexpr info<aarch64_feature::IDENT> IDENT ()\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    return info<aarch64_feature::IDENT> ();\t\t\t\t\\\n+  }\n+#define AARCH64_OPT_EXTENSION(A, IDENT, REQUIRES, EXPLICIT_ON, E, F) \\\n+  HANDLE (IDENT, REQUIRES, EXPLICIT_ON)\n+#define AARCH64_ARCH(A, B, IDENT, D, REQUIRES) HANDLE (IDENT, REQUIRES, ())\n+#include \"config/aarch64/aarch64-option-extensions.def\"\n+#include \"config/aarch64/aarch64-arches.def\"\n+#undef HANDLE\n+\n+/* Return the set of all features that would need to be disabled if\n+   the features in MASK are disabled.\n+\n+   Note that the size of the expression varies linearly with the number\n+   of features, which means that invoking this function once per feature\n+   is quadratic in the number of features.  However, collecting the same\n+   information at compiler start-up is likely to be quadratic too, so\n+   we're better off paying the cost once per compiler build rather than\n+   once per compiler run.  */\n+constexpr aarch64_feature_flags\n+get_flags_off (aarch64_feature_flags mask)\n+{\n+  return (0\n+#define AARCH64_OPT_EXTENSION(A, IDENT, C, D, E, F) \\\n+\t  | (feature_deps::IDENT ().enable & mask ? AARCH64_FL_##IDENT : 0)\n+#include \"config/aarch64/aarch64-option-extensions.def\"\n+\t  );\n+}\n+\n+/* Define root_off_<IDENT> variables for each feature, giving the set of\n+   features that must be turned off by +noIDENT.  This set is not transitively\n+   closed; use get_flags_off to complete the closure.  */\n+#define AARCH64_OPT_EXTENSION(A, IDENT, C, D, EXPLICIT_OFF, F) \\\n+  constexpr auto root_off_##IDENT \\\n+    = AARCH64_FL_##IDENT | get_flags EXPLICIT_OFF;\n+#include \"config/aarch64/aarch64-option-extensions.def\"\n+\n+/* Define cpu_<NAME> variables for each CPU, giving the transitive\n+   closure of all the features that the CPU supports.  */\n+#define AARCH64_CORE(A, CORE_IDENT, C, ARCH_IDENT, FEATURES, F, G, H, I) \\\n+  constexpr auto cpu_##CORE_IDENT = ARCH_IDENT ().enable | get_enable FEATURES;\n+#include \"config/aarch64/aarch64-cores.def\"\n+\n+}\n+}\n+\n+#endif"}, {"sha": "bdf4baf309c02a08f74eec7b9cb77bc1f3247de3", "filename": "gcc/config/aarch64/aarch64-option-extensions.def", "status": "modified", "additions": 116, "deletions": 207, "changes": 323, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-option-extensions.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-option-extensions.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-option-extensions.def?ref=11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "patch": "@@ -21,23 +21,34 @@\n \n    Before using #include to read this file, define a macro:\n \n-      AARCH64_OPT_EXTENSION(EXT_NAME, FLAG_CANONICAL, FLAGS_ON, FLAGS_OFF,\n-\t\t\t    SYNTHETIC, FEATURE_STRING)\n-\n-   - EXT_NAME is the name of the extension, represented as a string constant.\n-   - FLAGS_CANONICAL is the canonical internal name for this flag.\n-   - FLAGS_ON are the bitwise-or of the features that enabling the extension\n-     adds, or zero if enabling this extension has no effect on other features.\n-   - FLAGS_OFF are the bitwise-or of the features that disabling the extension\n-     removes, or zero if disabling this extension has no effect on other\n-     features.\n-   - SYNTHETIC is a boolean to indicate whether the option is a purely synthetic\n-     grouping of options and that the option itself has no feature bit (e.g.\n-     crypto).  This is used to determine when sum of the individual options in\n-     FLAGS_ON can be replaced by FLAG_CANONICAL in options minimization.  If the\n-     group is synthetic then they can be replaced when all options in FLAGS_ON\n-     are enabled, otherwise they can only be replaced when\n-     FLAGS_ON | FLAG_CANONICAL are enabled.\n+      AARCH64_OPT_EXTENSION(NAME, IDENT, REQUIRES, EXPLICIT_ON,\n+\t\t\t    EXPLICIT_OFF, FEATURE_STRING)\n+\n+   - NAME is the name of the extension, represented as a string constant.\n+\n+   - IDENT is the canonical internal name for this flag.\n+\n+   - REQUIRES is a list of features that must be enabled whenever this\n+     feature is enabled.  The relationship is implicitly transitive:\n+     if A appears in B's REQUIRES and B appears in C's REQUIRES then\n+     A and B must be enabled whenever C is.  Thus, turning on C also\n+     turns on A and B, while turning off A or B also turns off C.\n+\n+   - EXPLICIT_ON is a list of features that are enabled by an explicit\n+     +NAME specification, in addition to those listed in REQUIRES.\n+     Usually this is an empty list; comments below explain the exceptions.\n+     The list is implicitly transitively closed wrt REQUIRES (but *not*\n+     to EXPLICIT_ON, since NAME is the only thing explicit in +NAME).\n+     Thus if A is in B's REQUIRES and B is in C's EXPLICIT_ON, +C will\n+     enable both B and A.  B's EXPLICIT_ON has no effect on +C.\n+\n+   - EXPLICIT_OFF is a list of features that are disabled by an explicit\n+     +noNAME specification, in addition to the features that are transitively\n+     dependent on NAME (according to REQUIRES).  As with EXPLICIT_ON,\n+     this is usually an empty list; comments below explain the exceptions.\n+     If a feature A appears in this list then the list implicitly includes\n+     any features that are transitively dependent on A (according to REQUIRES).\n+\n    - FEAT_STRING is a string containing the entries in the 'Features' field of\n      /proc/cpuinfo on a GNU/Linux system that correspond to this architecture\n      extension being available.  Sometimes multiple entries are needed to enable\n@@ -47,197 +58,95 @@\n      that are required.  Their order is not important.  An empty string means\n      do not detect this feature during auto detection.\n \n-     NOTE: Any changes to the AARCH64_OPT_EXTENSION macro need to be mirrored in\n-     config.gcc.  */\n-\n-/* Enabling \"fp\" just enables \"fp\".\n-   Disabling \"fp\" also disables \"simd\", \"crypto\", \"fp16\", \"aes\", \"sha2\",\n-   \"sha3\", sm3/sm4, \"sve\", \"sve2\", \"sve2-aes\", \"sve2-sha3\", \"sve2-sm4\",\n-   \"sve2-bitperm\", \"i8mm\", \"f32mm\", \"f64mm\", and \"bf16\".  */\n-AARCH64_OPT_EXTENSION(\"fp\", AARCH64_FL_FP, 0, AARCH64_FL_SIMD | \\\n-\t\t      AARCH64_FL_CRYPTO | AARCH64_FL_F16 | AARCH64_FL_AES | \\\n-\t\t      AARCH64_FL_SHA2 | AARCH64_FL_SHA3 | AARCH64_FL_SM4 | \\\n-\t\t      AARCH64_FL_SVE | AARCH64_FL_SVE2 | AARCH64_FL_SVE2_AES | \\\n-\t\t      AARCH64_FL_SVE2_SHA3 | AARCH64_FL_SVE2_SM4 | \\\n-\t\t      AARCH64_FL_SVE2_BITPERM | AARCH64_FL_I8MM | \\\n-\t\t      AARCH64_FL_F32MM | AARCH64_FL_F64MM | AARCH64_FL_BF16,\n-\t\t       false, \"fp\")\n-\n-/* Enabling \"simd\" also enables \"fp\".\n-   Disabling \"simd\" also disables \"crypto\", \"dotprod\", \"aes\", \"sha2\", \"sha3\",\n-   \"sm3/sm4\", \"sve\", \"sve2\", \"sve2-aes\", \"sve2-sha3\", \"sve2-sm4\",\n-   \"sve2-bitperm\", \"i8mm\", \"f32mm\" and \"f64mm\".  */\n-AARCH64_OPT_EXTENSION(\"simd\", AARCH64_FL_SIMD, AARCH64_FL_FP, \\\n-\t\t      AARCH64_FL_CRYPTO | AARCH64_FL_DOTPROD | \\\n-\t\t      AARCH64_FL_AES | AARCH64_FL_SHA2 | AARCH64_FL_SHA3 | \\\n-\t\t      AARCH64_FL_SM4 | AARCH64_FL_SVE | AARCH64_FL_SVE2 | \\\n-\t\t      AARCH64_FL_SVE2_AES | AARCH64_FL_SVE2_SHA3 | \\\n-\t\t      AARCH64_FL_SVE2_SM4 | AARCH64_FL_SVE2_BITPERM | \\\n-\t\t      AARCH64_FL_I8MM | AARCH64_FL_F32MM | AARCH64_FL_F64MM, \\\n-\t\t      false, \"asimd\")\n-\n-/* Enabling or disabling \"crc\" only changes \"crc\".  */\n-AARCH64_OPT_EXTENSION(\"crc\", AARCH64_FL_CRC, 0, 0, false, \"crc32\")\n-\n-/* Enabling or disabling \"lse\" only changes \"lse\".  */\n-AARCH64_OPT_EXTENSION(\"lse\", AARCH64_FL_LSE, 0, 0, false, \"atomics\")\n-\n-/* Enabling \"fp16\" also enables \"fp\".\n-   Disabling \"fp16\" disables \"fp16\", \"fp16fml\", \"sve\", \"sve2\",\n-   \"sve2-aes\", \"sve2-sha3\", \"sve2-sm4\", \"sve2-bitperm\", \"f32mm\" and\n-    \"f64mm\".  */\n-AARCH64_OPT_EXTENSION(\"fp16\", AARCH64_FL_F16, AARCH64_FL_FP, \\\n-\t\t      AARCH64_FL_F16FML | AARCH64_FL_SVE | AARCH64_FL_F32MM | \\\n-\t\t      AARCH64_FL_F64MM | AARCH64_FL_SVE2 | \\\n-\t\t      AARCH64_FL_SVE2_AES | AARCH64_FL_SVE2_SHA3 | \\\n-\t\t      AARCH64_FL_SVE2_SM4 | AARCH64_FL_SVE2_BITPERM, false, \\\n-\t\t      \"fphp asimdhp\")\n-\n-/* Enabling or disabling \"rcpc\" only changes \"rcpc\".  */\n-AARCH64_OPT_EXTENSION(\"rcpc\", AARCH64_FL_RCPC, 0, 0, false, \"lrcpc\")\n-\n-/* Enabling \"rdma\" also enables \"fp\", \"simd\".\n-   Disabling \"rdma\" just disables \"rdma\".  */\n-AARCH64_OPT_EXTENSION(\"rdma\", AARCH64_FL_RDMA, \\\n-\t\t      AARCH64_FL_FP | AARCH64_FL_SIMD, 0, false, \"asimdrdm\")\n-\n-/* Enabling \"dotprod\" also enables \"simd\".\n-   Disabling \"dotprod\" only disables \"dotprod\".  */\n-AARCH64_OPT_EXTENSION(\"dotprod\", AARCH64_FL_DOTPROD, AARCH64_FL_FPSIMD, 0, \\\n-\t\t      false, \"asimddp\")\n-\n-/* Enabling \"aes\" also enables \"simd\".\n-   Disabling \"aes\" disables \"aes\" and \"sve2-aes'.  */\n-AARCH64_OPT_EXTENSION(\"aes\", AARCH64_FL_AES, AARCH64_FL_FPSIMD, \\\n-\t\t      AARCH64_FL_SVE2_AES | AARCH64_FL_CRYPTO, false, \"aes\")\n-\n-/* Enabling \"sha2\" also enables \"simd\".\n-   Disabling \"sha2\" just disables \"sha2\".  */\n-AARCH64_OPT_EXTENSION(\"sha2\", AARCH64_FL_SHA2, AARCH64_FL_FPSIMD, \\\n-\t\t      AARCH64_FL_CRYPTO | AARCH64_FL_SHA3 | \\\n-\t\t      AARCH64_FL_SVE2_SHA3, false, \"sha1 sha2\")\n-\n-/* Enabling \"crypto\" also enables \"fp\", \"simd\", \"aes\" and \"sha2\".\n-   Disabling \"crypto\" disables \"crypto\", \"aes\", \"sha2\", \"sha3\" and \"sm3/sm4\",\n-   \"sve2-aes\", \"sve2-sha3\", \"sve2-sm4\".  */\n-AARCH64_OPT_EXTENSION(\"crypto\", AARCH64_FL_CRYPTO, AARCH64_FL_FP | \\\n-\t\t      AARCH64_FL_SIMD | AARCH64_FL_AES | AARCH64_FL_SHA2, \\\n-\t\t      AARCH64_FL_AES | AARCH64_FL_SHA2 | AARCH64_FL_SHA3 | \\\n-\t\t      AARCH64_FL_SM4 | AARCH64_FL_SVE2_AES | \\\n-\t\t      AARCH64_FL_SVE2_SHA3 | AARCH64_FL_SVE2_SM4, true, \\\n+   The list of features must follow topological order wrt REQUIRES\n+   and EXPLICIT_ON.  For example, if A is in B's REQUIRES list, A must\n+   come before B.  This is enforced by aarch64-feature-deps.h.\n+\n+   NOTE: Any changes to the AARCH64_OPT_EXTENSION macro need to be mirrored in\n+   config.gcc.  */\n+\n+AARCH64_OPT_EXTENSION(\"fp\", FP, (), (), (), \"fp\")\n+\n+AARCH64_OPT_EXTENSION(\"simd\", SIMD, (FP), (), (), \"asimd\")\n+\n+AARCH64_OPT_EXTENSION(\"crc\", CRC, (), (), (), \"crc32\")\n+\n+AARCH64_OPT_EXTENSION(\"lse\", LSE, (), (), (), \"atomics\")\n+\n+/* +nofp16 disables an implicit F16FML, even though an implicit F16FML\n+   does not imply F16.  See F16FML for more details.  */\n+AARCH64_OPT_EXTENSION(\"fp16\", F16, (FP), (), (F16FML), \"fphp asimdhp\")\n+\n+AARCH64_OPT_EXTENSION(\"rcpc\", RCPC, (), (), (), \"lrcpc\")\n+\n+/* An explicit +rdma implies +simd, but +rdma+nosimd still enables scalar\n+   RDMA instructions.  */\n+AARCH64_OPT_EXTENSION(\"rdma\", RDMA, (), (SIMD), (), \"asimdrdm\")\n+\n+AARCH64_OPT_EXTENSION(\"dotprod\", DOTPROD, (SIMD), (), (), \"asimddp\")\n+\n+AARCH64_OPT_EXTENSION(\"aes\", AES, (SIMD), (), (), \"aes\")\n+\n+AARCH64_OPT_EXTENSION(\"sha2\", SHA2, (SIMD), (), (), \"sha1 sha2\")\n+\n+/* +nocrypto disables AES, SHA2 and SM4, and anything that depends on them\n+   (such as SHA3 and the SVE2 crypto extensions).  */\n+AARCH64_OPT_EXTENSION(\"crypto\", CRYPTO, (AES, SHA2), (), (AES, SHA2, SM4),\n \t\t      \"aes pmull sha1 sha2\")\n \n-/* Enabling \"sha3\" enables \"simd\" and \"sha2\".\n-   Disabling \"sha3\" disables \"sha3\" and \"sve2-sha3\".  */\n-AARCH64_OPT_EXTENSION(\"sha3\", AARCH64_FL_SHA3, AARCH64_FL_FPSIMD | \\\n-\t\t      AARCH64_FL_SHA2, AARCH64_FL_SVE2_SHA3, false, \\\n-\t\t      \"sha3 sha512\")\n-\n-/* Enabling \"sm4\" also enables \"simd\".\n-   Disabling \"sm4\" disables \"sm4\" and \"sve2-sm4\".  */\n-AARCH64_OPT_EXTENSION(\"sm4\", AARCH64_FL_SM4, AARCH64_FL_FPSIMD, \\\n-\t\t      AARCH64_FL_SVE2_SM4, false, \"sm3 sm4\")\n-\n-/* Enabling \"fp16fml\" also enables \"fp\" and \"fp16\".\n-   Disabling \"fp16fml\" just disables \"fp16fml\".  */\n-AARCH64_OPT_EXTENSION(\"fp16fml\", AARCH64_FL_F16FML, \\\n-\t\t      AARCH64_FL_FP | AARCH64_FL_F16, 0, false, \"asimdfhm\")\n-\n-/* Enabling \"sve\" also enables \"fp16\", \"fp\" and \"simd\".\n-   Disabling \"sve\" disables \"sve\", \"f32mm\", \"f64mm\", \"sve2\", \"sve2-aes\",\n-   \"sve2-sha3\", \"sve2-sm4\" and \"sve2-bitperm\".  */\n-AARCH64_OPT_EXTENSION(\"sve\", AARCH64_FL_SVE, AARCH64_FL_FP | AARCH64_FL_SIMD | \\\n-\t\t      AARCH64_FL_F16, AARCH64_FL_F32MM | AARCH64_FL_F64MM | \\\n-\t\t      AARCH64_FL_SVE2 | AARCH64_FL_SVE2_AES | \\\n-\t\t      AARCH64_FL_SVE2_SHA3 | AARCH64_FL_SVE2_SM4 | \\\n-\t\t      AARCH64_FL_SVE2_BITPERM, false, \"sve\")\n-\n-/* Enabling/Disabling \"profile\" does not enable/disable any other feature.  */\n-AARCH64_OPT_EXTENSION(\"profile\", AARCH64_FL_PROFILE, 0, 0, false, \"\")\n-\n-/* Enabling/Disabling \"rng\" only changes \"rng\".  */\n-AARCH64_OPT_EXTENSION(\"rng\", AARCH64_FL_RNG, 0, 0, false, \"rng\")\n-\n-/* Enabling/Disabling \"memtag\" only changes \"memtag\".  */\n-AARCH64_OPT_EXTENSION(\"memtag\", AARCH64_FL_MEMTAG, 0, 0, false, \"\")\n-\n-/* Enabling/Disabling \"sb\" only changes \"sb\".  */\n-AARCH64_OPT_EXTENSION(\"sb\", AARCH64_FL_SB, 0, 0, false, \"sb\")\n-\n-/* Enabling/Disabling \"ssbs\" only changes \"ssbs\".  */\n-AARCH64_OPT_EXTENSION(\"ssbs\", AARCH64_FL_SSBS, 0, 0, false, \"ssbs\")\n-\n-/* Enabling/Disabling \"predres\" only changes \"predres\".  */\n-AARCH64_OPT_EXTENSION(\"predres\", AARCH64_FL_PREDRES, 0, 0, false, \"\")\n-\n-/* Enabling \"sve2\" also enables \"sve\", \"fp16\", \"fp\", and \"simd\".\n-   Disabling \"sve2\" disables \"sve2\", \"sve2-aes\", \"sve2-sha3\", \"sve2-sm4\", and\n-   \"sve2-bitperm\".  */\n-AARCH64_OPT_EXTENSION(\"sve2\", AARCH64_FL_SVE2, AARCH64_FL_SVE | \\\n-\t\t      AARCH64_FL_FP | AARCH64_FL_SIMD | AARCH64_FL_F16, \\\n-\t\t      AARCH64_FL_SVE2_AES | AARCH64_FL_SVE2_SHA3 | \\\n-\t\t      AARCH64_FL_SVE2_SM4 | AARCH64_FL_SVE2_BITPERM, false, \"sve2\")\n-\n-/* Enabling \"sve2-sm4\" also enables \"sm4\", \"simd\", \"fp16\", \"fp\", \"sve\", and\n-   \"sve2\". Disabling \"sve2-sm4\" just disables \"sve2-sm4\".  */\n-AARCH64_OPT_EXTENSION(\"sve2-sm4\", AARCH64_FL_SVE2_SM4, AARCH64_FL_SM4 | \\\n-\t\t      AARCH64_FL_SIMD | AARCH64_FL_F16 | AARCH64_FL_FP | \\\n-\t\t      AARCH64_FL_SVE | AARCH64_FL_SVE2, 0, false, \"svesm4\")\n-\n-/* Enabling \"sve2-aes\" also enables \"aes\", \"simd\", \"fp16\", \"fp\", \"sve\", and\n-   \"sve2\". Disabling \"sve2-aes\" just disables \"sve2-aes\".  */\n-AARCH64_OPT_EXTENSION(\"sve2-aes\", AARCH64_FL_SVE2_AES, AARCH64_FL_AES | \\\n-\t\t      AARCH64_FL_SIMD | AARCH64_FL_F16 | AARCH64_FL_FP | \\\n-\t\t      AARCH64_FL_SVE | AARCH64_FL_SVE2, 0, false, \"sveaes\")\n-\n-/* Enabling \"sve2-sha3\" also enables \"sha3\", \"simd\", \"fp16\", \"fp\", \"sve\", and\n-   \"sve2\". Disabling \"sve2-sha3\" just disables \"sve2-sha3\".  */\n-AARCH64_OPT_EXTENSION(\"sve2-sha3\", AARCH64_FL_SVE2_SHA3, AARCH64_FL_SHA3 | \\\n-\t\t      AARCH64_FL_SHA2 | \\\n-\t\t      AARCH64_FL_SIMD | AARCH64_FL_F16 | AARCH64_FL_FP | \\\n-\t\t      AARCH64_FL_SVE | AARCH64_FL_SVE2, 0, false, \"svesha3\")\n-\n-/* Enabling \"sve2-bitperm\" also enables \"simd\", \"fp16\", \"fp\", \"sve\", and\n-   \"sve2\".  Disabling \"sve2-bitperm\" just disables \"sve2-bitperm\".  */\n-AARCH64_OPT_EXTENSION(\"sve2-bitperm\", AARCH64_FL_SVE2_BITPERM, AARCH64_FL_SIMD | \\\n-\t\t      AARCH64_FL_F16 | AARCH64_FL_FP | AARCH64_FL_SVE | \\\n-\t\t      AARCH64_FL_SVE2, 0, false, \"svebitperm\")\n-\n-/* Enabling or disabling \"tme\" only changes \"tme\".  */\n-AARCH64_OPT_EXTENSION(\"tme\", AARCH64_FL_TME, 0, 0, false, \"\")\n-\n-/* Enabling \"i8mm\" also enables \"simd\" and \"fp\".\n-   Disabling \"i8mm\" only disables \"i8mm\".  */\n-AARCH64_OPT_EXTENSION(\"i8mm\", AARCH64_FL_I8MM, \\\n-\t\t      AARCH64_FL_SIMD | AARCH64_FL_FP, 0, false, \"i8mm\")\n-\n-/* Enabling \"f32mm\" also enables \"sve\", \"fp16\", \"fp\", and \"simd\".\n-   Disabling \"f32mm\" only disables \"f32mm\".  */\n-AARCH64_OPT_EXTENSION(\"f32mm\", AARCH64_FL_F32MM, \\\n-\t\t      AARCH64_FL_SVE | AARCH64_FL_F16 | AARCH64_FL_FP | \\\n-\t\t      AARCH64_FL_SIMD, 0, false, \"f32mm\")\n-\n-/* Enabling \"f64mm\" also enables \"sve\", \"fp16\", \"fp\", and \"simd\".\n-   Disabling \"f64mm\" only disables \"f64mm\".  */\n-AARCH64_OPT_EXTENSION(\"f64mm\", AARCH64_FL_F64MM, \\\n-\t\t      AARCH64_FL_SVE | AARCH64_FL_F16 | AARCH64_FL_FP | \\\n-\t\t      AARCH64_FL_SIMD, 0, false, \"f64mm\")\n-\n-/* Enabling \"bf16\" also enables \"simd\" and \"fp\".\n-   Disabling \"bf16\" only disables \"bf16\".  */\n-AARCH64_OPT_EXTENSION(\"bf16\", AARCH64_FL_BF16, \\\n-\t\t      AARCH64_FL_SIMD | AARCH64_FL_FP, 0, false, \"bf16\")\n-\n-/* Enabling/Disabling \"flagm\" only changes \"flagm\".  */\n-AARCH64_OPT_EXTENSION(\"flagm\", AARCH64_FL_FLAGM, 0, 0, false, \"flagm\")\n-\n-/* Enabling/Disabling \"pauth\" only changes \"pauth\".  */\n-AARCH64_OPT_EXTENSION(\"pauth\", AARCH64_FL_PAUTH, 0, 0, false, \"paca pacg\")\n-\n-/* Enabling/Disabling \"ls64\" only changes \"ls64\".  */\n-AARCH64_OPT_EXTENSION(\"ls64\", AARCH64_FL_LS64, 0, 0, false, \"\")\n-\n-/* Enabling/disabling \"mops\" only changes \"mops\".  */\n-AARCH64_OPT_EXTENSION(\"mops\", AARCH64_FL_MOPS, 0, 0, false, \"\")\n+AARCH64_OPT_EXTENSION(\"sha3\", SHA3, (SHA2), (), (), \"sha3 sha512\")\n+\n+AARCH64_OPT_EXTENSION(\"sm4\", SM4, (SIMD), (), (), \"sm3 sm4\")\n+\n+/* An explicit +fp16fml implies +fp16, but a dependence on it does not.\n+   Thus -march=armv8.4-a implies F16FML but not F16.  -march=armv8.4-a+fp16\n+   and -march=armv8.4-a+fp16fml are equivalent and enable both F16FML and F16.\n+   -march=armv8.4-a+nofp16+fp16 enables F16 but not F16FML.  */\n+AARCH64_OPT_EXTENSION(\"fp16fml\", F16FML, (), (F16), (), \"asimdfhm\")\n+\n+AARCH64_OPT_EXTENSION(\"sve\", SVE, (SIMD, F16), (), (), \"sve\")\n+\n+AARCH64_OPT_EXTENSION(\"profile\", PROFILE, (), (), (), \"\")\n+\n+AARCH64_OPT_EXTENSION(\"rng\", RNG, (), (), (), \"rng\")\n+\n+AARCH64_OPT_EXTENSION(\"memtag\", MEMTAG, (), (), (), \"\")\n+\n+AARCH64_OPT_EXTENSION(\"sb\", SB, (), (), (), \"sb\")\n+\n+AARCH64_OPT_EXTENSION(\"ssbs\", SSBS, (), (), (), \"ssbs\")\n+\n+AARCH64_OPT_EXTENSION(\"predres\", PREDRES, (), (), (), \"\")\n+\n+AARCH64_OPT_EXTENSION(\"sve2\", SVE2, (SVE), (), (), \"sve2\")\n+\n+AARCH64_OPT_EXTENSION(\"sve2-sm4\", SVE2_SM4, (SVE2, SM4), (), (), \"svesm4\")\n+\n+AARCH64_OPT_EXTENSION(\"sve2-aes\", SVE2_AES, (SVE2, AES), (), (), \"sveaes\")\n+\n+AARCH64_OPT_EXTENSION(\"sve2-sha3\", SVE2_SHA3, (SVE2, SHA3), (), (), \"svesha3\")\n+\n+AARCH64_OPT_EXTENSION(\"sve2-bitperm\", SVE2_BITPERM, (SVE2), (), (),\n+\t\t      \"svebitperm\")\n+\n+AARCH64_OPT_EXTENSION(\"tme\", TME, (), (), (), \"\")\n+\n+AARCH64_OPT_EXTENSION(\"i8mm\", I8MM, (SIMD), (), (), \"i8mm\")\n+\n+AARCH64_OPT_EXTENSION(\"f32mm\", F32MM, (SVE), (), (), \"f32mm\")\n+\n+AARCH64_OPT_EXTENSION(\"f64mm\", F64MM, (SVE), (), (), \"f64mm\")\n+\n+/* An explicit +bf16 implies +simd, but +bf16+nosimd still enables scalar BF16\n+   instructions.  */\n+AARCH64_OPT_EXTENSION(\"bf16\", BF16, (FP), (SIMD), (), \"bf16\")\n+\n+AARCH64_OPT_EXTENSION(\"flagm\", FLAGM, (), (), (), \"flagm\")\n+\n+AARCH64_OPT_EXTENSION(\"pauth\", PAUTH, (), (), (), \"paca pacg\")\n+\n+AARCH64_OPT_EXTENSION(\"ls64\", LS64, (), (), (), \"\")\n+\n+AARCH64_OPT_EXTENSION(\"mops\", MOPS, (), (), (), \"\")\n \n #undef AARCH64_OPT_EXTENSION"}, {"sha": "ba23c90c411a4e6c909089ad503eff7e34566ec9", "filename": "gcc/config/aarch64/aarch64-opts.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-opts.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-opts.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-opts.h?ref=11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "patch": "@@ -22,6 +22,10 @@\n #ifndef GCC_AARCH64_OPTS_H\n #define GCC_AARCH64_OPTS_H\n \n+#ifndef USED_FOR_TARGET\n+typedef uint64_t aarch64_feature_flags;\n+#endif\n+\n /* The various cores that implement AArch64.  */\n enum aarch64_processor\n {"}, {"sha": "c06e99339e371bd6d36bb53941cc7641ebe5d626", "filename": "gcc/config/aarch64/aarch64-sve-builtins.cc", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-sve-builtins.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64-sve-builtins.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-sve-builtins.cc?ref=11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "patch": "@@ -701,9 +701,8 @@ check_required_extensions (location_t location, tree fndecl,\n     return check_required_registers (location, fndecl);\n \n   static const struct { uint64_t flag; const char *name; } extensions[] = {\n-#define AARCH64_OPT_EXTENSION(EXT_NAME, FLAG_CANONICAL, FLAGS_ON, FLAGS_OFF, \\\n-\t\t\t      SYNTHETIC, FEATURE_STRING) \\\n-    { FLAG_CANONICAL, EXT_NAME },\n+#define AARCH64_OPT_EXTENSION(EXT_NAME, IDENT, C, D, E, F) \\\n+    { AARCH64_FL_##IDENT, EXT_NAME },\n #include \"aarch64-option-extensions.def\"\n   };\n "}, {"sha": "398232433ce5b15e2d9a0f8f85fa4133771c8398", "filename": "gcc/config/aarch64/aarch64.cc", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.cc?ref=11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "patch": "@@ -81,6 +81,7 @@\n #include \"rtlanal.h\"\n #include \"tree-dfa.h\"\n #include \"asan.h\"\n+#include \"aarch64-feature-deps.h\"\n \n /* This file should be included last.  */\n #include \"target-def.h\"\n@@ -2681,21 +2682,22 @@ struct processor\n /* Architectures implementing AArch64.  */\n static const struct processor all_architectures[] =\n {\n-#define AARCH64_ARCH(NAME, CORE, ARCH_IDENT, ARCH_REV, FLAGS) \\\n-  {NAME, CORE, CORE, AARCH64_ARCH_##ARCH_IDENT, FLAGS, NULL},\n+#define AARCH64_ARCH(NAME, CORE, ARCH_IDENT, D, E) \\\n+  {NAME, CORE, CORE, AARCH64_ARCH_##ARCH_IDENT, \\\n+   feature_deps::ARCH_IDENT ().enable, NULL},\n #include \"aarch64-arches.def\"\n   {NULL, aarch64_none, aarch64_none, aarch64_no_arch, 0, NULL}\n };\n \n /* Processor cores implementing AArch64.  */\n static const struct processor all_cores[] =\n {\n-#define AARCH64_CORE(NAME, IDENT, SCHED, ARCH, FLAGS, COSTS, IMP, PART, VARIANT) \\\n-  {NAME, IDENT, SCHED, AARCH64_ARCH_##ARCH,\t\t\t\t\\\n-  AARCH64_FL_FOR_##ARCH | FLAGS, &COSTS##_tunings},\n+#define AARCH64_CORE(NAME, IDENT, SCHED, ARCH, E, COSTS, G, H, I) \\\n+  {NAME, IDENT, SCHED, AARCH64_ARCH_##ARCH, \\\n+   feature_deps::cpu_##IDENT, &COSTS##_tunings},\n #include \"aarch64-cores.def\"\n   {\"generic\", generic, cortexa53, AARCH64_ARCH_V8A,\n-    AARCH64_FL_FOR_V8A, &generic_tunings},\n+   feature_deps::V8A ().enable, &generic_tunings},\n   {NULL, aarch64_none, aarch64_none, aarch64_no_arch, 0, NULL}\n };\n "}, {"sha": "97da721d094515e9b202e9f6ce376417e129e1ee", "filename": "gcc/config/aarch64/aarch64.h", "status": "modified", "additions": 21, "deletions": 143, "changes": 164, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Faarch64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.h?ref=11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "patch": "@@ -144,149 +144,27 @@\n \n #define PCC_BITFIELD_TYPE_MATTERS\t1\n \n-/* Instruction tuning/selection flags.  */\n-\n-/* Bit values used to identify processor capabilities.  */\n-#define AARCH64_FL_SIMD       (1 << 0)\t/* Has SIMD instructions.  */\n-#define AARCH64_FL_FP         (1 << 1)\t/* Has FP.  */\n-#define AARCH64_FL_CRYPTO     (1 << 2)\t/* Has crypto.  */\n-#define AARCH64_FL_CRC        (1 << 3)\t/* Has CRC.  */\n-/* ARMv8.1-A architecture extensions.  */\n-#define AARCH64_FL_LSE\t      (1 << 4)  /* Has Large System Extensions.  */\n-#define AARCH64_FL_RDMA       (1 << 5)  /* Has Round Double Multiply Add.  */\n-#define AARCH64_FL_V8_1A      (1 << 6)  /* Has ARMv8.1-A extensions.  */\n-/* Armv8-R.  */\n-#define AARCH64_FL_V8R        (1 << 7)  /* Armv8-R AArch64.  */\n-/* ARMv8.2-A architecture extensions.  */\n-#define AARCH64_FL_V8_2A      (1 << 8)  /* Has ARMv8.2-A features.  */\n-#define AARCH64_FL_F16\t      (1 << 9)  /* Has ARMv8.2-A FP16 extensions.  */\n-#define AARCH64_FL_SVE        (1 << 10) /* Has Scalable Vector Extensions.  */\n-/* ARMv8.3-A architecture extensions.  */\n-#define AARCH64_FL_V8_3A      (1 << 11)  /* Has ARMv8.3-A features.  */\n-#define AARCH64_FL_RCPC       (1 << 12)  /* Has support for RCpc model.  */\n-#define AARCH64_FL_DOTPROD    (1 << 13)  /* Has ARMv8.2-A Dot Product ins.  */\n-/* New flags to split crypto into aes and sha2.  */\n-#define AARCH64_FL_AES\t      (1 << 14)  /* Has Crypto AES.  */\n-#define AARCH64_FL_SHA2\t      (1 << 15)  /* Has Crypto SHA2.  */\n-/* ARMv8.4-A architecture extensions.  */\n-#define AARCH64_FL_V8_4A      (1 << 16)  /* Has ARMv8.4-A features.  */\n-#define AARCH64_FL_SM4\t      (1 << 17)  /* Has ARMv8.4-A SM3 and SM4.  */\n-#define AARCH64_FL_SHA3\t      (1 << 18)  /* Has ARMv8.4-a SHA3 and SHA512.  */\n-#define AARCH64_FL_F16FML     (1 << 19)  /* Has ARMv8.4-a FP16 extensions.  */\n-\n-/* Statistical Profiling extensions.  */\n-#define AARCH64_FL_PROFILE    (1 << 21)\n-\n-/* ARMv8.5-A architecture extensions.  */\n-#define AARCH64_FL_V8_5A      (1 << 22)  /* Has ARMv8.5-A features.  */\n-#define AARCH64_FL_RNG\t      (1 << 23)  /* ARMv8.5-A Random Number Insns.  */\n-#define AARCH64_FL_MEMTAG     (1 << 24)  /* ARMv8.5-A Memory Tagging\n-\t\t\t\t\t    Extensions.  */\n-\n-/* Speculation Barrier instruction supported.  */\n-#define AARCH64_FL_SB\t      (1 << 25)\n-\n-/* Speculative Store Bypass Safe instruction supported.  */\n-#define AARCH64_FL_SSBS\t      (1 << 26)\n-\n-/* Execution and Data Prediction Restriction instructions supported.  */\n-#define AARCH64_FL_PREDRES    (1 << 27)\n-\n-/* SVE2 instruction supported.  */\n-#define AARCH64_FL_SVE2\t\t(1 << 28)\n-#define AARCH64_FL_SVE2_AES\t(1 << 29)\n-#define AARCH64_FL_SVE2_SM4\t(1 << 30)\n-#define AARCH64_FL_SVE2_SHA3\t(1ULL << 31)\n-#define AARCH64_FL_SVE2_BITPERM\t(1ULL << 32)\n-\n-/* Transactional Memory Extension.  */\n-#define AARCH64_FL_TME\t      (1ULL << 33)  /* Has TME instructions.  */\n-\n-/* Armv8.6-A architecture extensions.  */\n-#define AARCH64_FL_V8_6A      (1ULL << 34)\n-\n-/* 8-bit Integer Matrix Multiply (I8MM) extensions.  */\n-#define AARCH64_FL_I8MM\t      (1ULL << 35)\n-\n-/* Brain half-precision floating-point (BFloat16) Extension.  */\n-#define AARCH64_FL_BF16\t      (1ULL << 36)\n-\n-/* 32-bit Floating-point Matrix Multiply (F32MM) extensions.  */\n-#define AARCH64_FL_F32MM      (1ULL << 37)\n-\n-/* 64-bit Floating-point Matrix Multiply (F64MM) extensions.  */\n-#define AARCH64_FL_F64MM      (1ULL << 38)\n-\n-/* Flag Manipulation Instructions (FLAGM) extension.  */\n-#define AARCH64_FL_FLAGM      (1ULL << 39)\n-\n-/* Pointer Authentication (PAUTH) extension.  */\n-#define AARCH64_FL_PAUTH      (1ULL << 40)\n-\n-/* Armv9.0-A.  */\n-#define AARCH64_FL_V9A        (1ULL << 41)  /* Armv9.0-A Architecture.  */\n-\n-/* 64-byte atomic load/store extensions.  */\n-#define AARCH64_FL_LS64      (1ULL << 42)\n-\n-/* Armv8.7-a architecture extensions.  */\n-#define AARCH64_FL_V8_7A      (1ULL << 43)\n-\n-/* Hardware memory operation instructions.  */\n-#define AARCH64_FL_MOPS       (1ULL << 44)\n-\n-/* Armv8.8-a architecture extensions.  */\n-#define AARCH64_FL_V8_8A      (1ULL << 45)\n-\n-/* Armv9.1-A.  */\n-#define AARCH64_FL_V9_1A      (1ULL << 46)\n-\n-/* Armv9.2-A.  */\n-#define AARCH64_FL_V9_2A      (1ULL << 47)\n-\n-/* Armv9.3-A.  */\n-#define AARCH64_FL_V9_3A      (1ULL << 48)\n-\n-/* Has FP and SIMD.  */\n-#define AARCH64_FL_FPSIMD     (AARCH64_FL_FP | AARCH64_FL_SIMD)\n-\n-/* Has FP without SIMD.  */\n-#define AARCH64_FL_FPQ16      (AARCH64_FL_FP & ~AARCH64_FL_SIMD)\n-\n-/* Architecture flags that effect instruction selection.  */\n-#define AARCH64_FL_FOR_V8A       (AARCH64_FL_FPSIMD)\n-#define AARCH64_FL_FOR_V8_1A\t\t\t       \\\n-  (AARCH64_FL_FOR_V8A | AARCH64_FL_LSE | AARCH64_FL_CRC \\\n-   | AARCH64_FL_RDMA | AARCH64_FL_V8_1A)\n-#define AARCH64_FL_FOR_V8_2A\t\t\t\\\n-  (AARCH64_FL_FOR_V8_1A | AARCH64_FL_V8_2A)\n-#define AARCH64_FL_FOR_V8_3A\t\t\t\\\n-  (AARCH64_FL_FOR_V8_2A | AARCH64_FL_V8_3A | AARCH64_FL_PAUTH)\n-#define AARCH64_FL_FOR_V8_4A\t\t\t\\\n-  (AARCH64_FL_FOR_V8_3A | AARCH64_FL_V8_4A | AARCH64_FL_F16FML \\\n-   | AARCH64_FL_DOTPROD | AARCH64_FL_FLAGM)\n-#define AARCH64_FL_FOR_V8_5A\t\t\t\\\n-  (AARCH64_FL_FOR_V8_4A | AARCH64_FL_V8_5A\t\\\n-   | AARCH64_FL_SB | AARCH64_FL_SSBS | AARCH64_FL_PREDRES)\n-#define AARCH64_FL_FOR_V8_6A\t\t\t\\\n-  (AARCH64_FL_FOR_V8_5A | AARCH64_FL_V8_6A | AARCH64_FL_FPSIMD \\\n-   | AARCH64_FL_I8MM | AARCH64_FL_BF16)\n-#define AARCH64_FL_FOR_V8_7A\t\t\t\\\n-  (AARCH64_FL_FOR_V8_6A | AARCH64_FL_V8_7A | AARCH64_FL_LS64)\n-#define AARCH64_FL_FOR_V8_8A\t\t\t\\\n-  (AARCH64_FL_FOR_V8_7A | AARCH64_FL_V8_8A | AARCH64_FL_MOPS)\n-\n-#define AARCH64_FL_FOR_V8R     \\\n-  (AARCH64_FL_FOR_V8_4A | AARCH64_FL_V8R)\n-#define AARCH64_FL_FOR_V9A       \\\n-  (AARCH64_FL_FOR_V8_5A | AARCH64_FL_SVE | AARCH64_FL_SVE2 | AARCH64_FL_V9A \\\n-   | AARCH64_FL_F16)\n-#define AARCH64_FL_FOR_V9_1A\t\\\n-  (AARCH64_FL_FOR_V9A | AARCH64_FL_FOR_V8_6A | AARCH64_FL_V9_1A)\n-#define AARCH64_FL_FOR_V9_2A\t\\\n-  (AARCH64_FL_FOR_V9_1A | AARCH64_FL_FOR_V8_7A | AARCH64_FL_V9_2A)\n-#define AARCH64_FL_FOR_V9_3A\t\\\n-  (AARCH64_FL_FOR_V9_2A | AARCH64_FL_FOR_V8_8A | AARCH64_FL_V9_3A)\n+#ifndef USED_FOR_TARGET\n+\n+/* Define an enum of all features (architectures and extensions).  */\n+enum class aarch64_feature : unsigned char {\n+#define AARCH64_OPT_EXTENSION(A, IDENT, C, D, E, F) IDENT,\n+#define AARCH64_ARCH(A, B, IDENT, D, E) IDENT,\n+#include \"aarch64-option-extensions.def\"\n+#include \"aarch64-arches.def\"\n+};\n+\n+/* Define unique flags for each of the above.  */\n+#define HANDLE(IDENT) \\\n+  constexpr auto AARCH64_FL_##IDENT \\\n+    = aarch64_feature_flags (1) << int (aarch64_feature::IDENT);\n+#define AARCH64_OPT_EXTENSION(A, IDENT, C, D, E, F) HANDLE (IDENT)\n+#define AARCH64_ARCH(A, B, IDENT, D, E) HANDLE (IDENT)\n+#include \"aarch64-option-extensions.def\"\n+#include \"aarch64-arches.def\"\n+#undef HANDLE\n+\n+#endif\n \n /* Macros to test ISA flags.  */\n "}, {"sha": "1c86d62ef80c0297ff787894f6965beb4476253a", "filename": "gcc/config/aarch64/driver-aarch64.cc", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Fdriver-aarch64.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11a113d501ff64fa4843e28d0a21b3f4e9d0d3de/gcc%2Fconfig%2Faarch64%2Fdriver-aarch64.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fdriver-aarch64.cc?ref=11a113d501ff64fa4843e28d0a21b3f4e9d0d3de", "patch": "@@ -26,6 +26,7 @@\n #include \"coretypes.h\"\n #include \"tm.h\"\n #include \"aarch64-protos.h\"\n+#include \"aarch64-feature-deps.h\"\n \n struct aarch64_arch_extension\n {\n@@ -34,9 +35,8 @@ struct aarch64_arch_extension\n   const char *feat_string;\n };\n \n-#define AARCH64_OPT_EXTENSION(EXT_NAME, FLAG_CANONICAL, FLAGS_ON, FLAGS_OFF, \\\n-\t\t\t      SYNTHETIC, FEATURE_STRING) \\\n-  { EXT_NAME, FLAG_CANONICAL, FEATURE_STRING },\n+#define AARCH64_OPT_EXTENSION(EXT_NAME, IDENT, C, D, E, FEATURE_STRING) \\\n+  { EXT_NAME, AARCH64_FL_##IDENT, FEATURE_STRING },\n static struct aarch64_arch_extension aarch64_extensions[] =\n {\n #include \"aarch64-option-extensions.def\"\n@@ -62,7 +62,7 @@ struct aarch64_core_data\n #define DEFAULT_ARCH \"8A\"\n \n #define AARCH64_CORE(CORE_NAME, CORE_IDENT, SCHED, ARCH, FLAGS, COSTS, IMP, PART, VARIANT) \\\n-  { CORE_NAME, #ARCH, IMP, PART, VARIANT, AARCH64_FL_FOR_##ARCH | FLAGS },\n+  { CORE_NAME, #ARCH, IMP, PART, VARIANT, feature_deps::cpu_##CORE_IDENT },\n \n static struct aarch64_core_data aarch64_cpu_data[] =\n {\n@@ -80,7 +80,7 @@ struct aarch64_arch_driver_info\n \n /* Skip the leading \"V\" in the architecture name.  */\n #define AARCH64_ARCH(NAME, CORE, ARCH_IDENT, ARCH_REV, FLAGS) \\\n-  { #ARCH_IDENT + 1, NAME, FLAGS },\n+  { #ARCH_IDENT + 1, NAME, feature_deps::ARCH_IDENT ().enable },\n \n static struct aarch64_arch_driver_info aarch64_arches[] =\n {"}]}