{"sha": "4cf70c20cb10acd6fb1016611d05540728176b60", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NGNmNzBjMjBjYjEwYWNkNmZiMTAxNjYxMWQwNTU0MDcyODE3NmI2MA==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2020-12-10T12:10:00Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2020-12-10T12:10:00Z"}, "message": "data-ref: Rework integer handling in split_constant_offset [PR98069]\n\nPR98069 is about a case in which split_constant_offset miscategorises\nan expression of the form:\n\n  int foo;\n  \u2026\n  POINTER_PLUS_EXPR<base, (sizetype)(INT_MIN - foo) * size>\n\nas:\n\n  base: base\n  offset: (sizetype) (-foo) * size\n  init: INT_MIN * size\n\n\u201c-foo\u201d overflows when \u201cfoo\u201d is INT_MIN, whereas the original expression\ndidn't overflow in that case.\n\nAs discussed in the PR trail, we could simply ignore the fact that\nint overflow is undefined and treat it as a wrapping type, but that\nis likely to pessimise quite a few cases.\n\nThis patch instead reworks split_constant_offset so that:\n\n- it treats integer operations as having an implicit cast to sizetype\n- for integer operations, the returned VAR has type sizetype\n\nIn other words, the problem becomes to express:\n\n  (sizetype) (OP0 CODE OP1)\n\nas:\n\n  VAR:sizetype + (sizetype) OFF:ssizetype\n\nThe top-level integer split_constant_offset will (usually) be a sizetype\nPOINTER_PLUS operand, so the extra cast to sizetype disappears.  But adding\nthe cast allows the conversion handling to defer a lot of the difficult\ncases to the recursive split_constant_offset call, which can detect\noverflow on individual operations.\n\nThe net effect is to analyse the access above as:\n\n  base: base\n  offset: -(sizetype) foo * size\n  init: INT_MIN * size\n\nSee the comments in the patch for more details.\n\ngcc/\n\tPR tree-optimization/98069\n\t* tree-data-ref.c (compute_distributive_range): New function.\n\t(nop_conversion_for_offset_p): Likewise.\n\t(split_constant_offset): In the internal overload, treat integer\n\texpressions as having an implicit cast to sizetype and express\n\tthem accordingly.  Pass back the range of the original (uncast)\n\texpression in a new range parameter.\n\t(split_constant_offset_1): Likewise.  Rework the handling of\n\tconversions to account for the implicit sizetype casts.", "tree": {"sha": "38f42677ac409708a94e8d82faaf428f6f31c885", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/38f42677ac409708a94e8d82faaf428f6f31c885"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/4cf70c20cb10acd6fb1016611d05540728176b60", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4cf70c20cb10acd6fb1016611d05540728176b60", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4cf70c20cb10acd6fb1016611d05540728176b60", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4cf70c20cb10acd6fb1016611d05540728176b60/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "f5b902a9af9d1cce6c540c7f71e02e22e45c23ef", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f5b902a9af9d1cce6c540c7f71e02e22e45c23ef", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f5b902a9af9d1cce6c540c7f71e02e22e45c23ef"}], "stats": {"total": 449, "additions": 352, "deletions": 97}, "files": [{"sha": "e60549fb30a1be2aea540c221e1f7ec491cf0b22", "filename": "gcc/testsuite/gcc.dg/vect/pr98069.c", "status": "added", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4cf70c20cb10acd6fb1016611d05540728176b60/gcc%2Ftestsuite%2Fgcc.dg%2Fvect%2Fpr98069.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4cf70c20cb10acd6fb1016611d05540728176b60/gcc%2Ftestsuite%2Fgcc.dg%2Fvect%2Fpr98069.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fvect%2Fpr98069.c?ref=4cf70c20cb10acd6fb1016611d05540728176b60", "patch": "@@ -0,0 +1,22 @@\n+long long int var_3 = -166416893043554447LL;\n+short var_8 = (short)27092;\n+unsigned int var_17 = 75036300U;\n+short arr_165[23];\n+\n+static long c(long e, long f) { return f ? e : f; }\n+void __attribute((noipa)) test()\n+{\n+  for (int b = 0; b < 19; b = var_17)\n+    for (int d = (int)(~c(-2147483647 - 1, var_3)) - 2147483647; d < 22; d++)\n+      arr_165[d] = var_8;\n+}\n+\n+int main()\n+{\n+  for (unsigned i_3 = 0; i_3 < 23; ++i_3)\n+    arr_165[i_3] = (short)-8885;\n+  test();\n+  if (arr_165[0] != 27092)\n+    __builtin_abort ();\n+  return 0;\n+}"}, {"sha": "926553b5cac81d20d4a324a2bd6e1547dae0ad61", "filename": "gcc/tree-data-ref.c", "status": "modified", "additions": 330, "deletions": 97, "changes": 427, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4cf70c20cb10acd6fb1016611d05540728176b60/gcc%2Ftree-data-ref.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4cf70c20cb10acd6fb1016611d05540728176b60/gcc%2Ftree-data-ref.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-data-ref.c?ref=4cf70c20cb10acd6fb1016611d05540728176b60", "patch": "@@ -97,6 +97,8 @@ along with GCC; see the file COPYING3.  If not see\n #include \"tree-eh.h\"\n #include \"ssa.h\"\n #include \"internal-fn.h\"\n+#include \"range-op.h\"\n+#include \"vr-values.h\"\n \n static struct datadep_stats\n {\n@@ -581,62 +583,239 @@ debug_ddrs (vec<ddr_p> ddrs)\n   dump_ddrs (stderr, ddrs);\n }\n \n+/* If RESULT_RANGE is nonnull, set *RESULT_RANGE to the range of\n+   OP0 CODE OP1, where:\n+\n+   - OP0 CODE OP1 has integral type TYPE\n+   - the range of OP0 is given by OP0_RANGE and\n+   - the range of OP1 is given by OP1_RANGE.\n+\n+   Independently of RESULT_RANGE, try to compute:\n+\n+     DELTA = ((sizetype) OP0 CODE (sizetype) OP1)\n+\t     - (sizetype) (OP0 CODE OP1)\n+\n+   as a constant and subtract DELTA from the ssizetype constant in *OFF.\n+   Return true on success, or false if DELTA is not known at compile time.\n+\n+   Truncation and sign changes are known to distribute over CODE, i.e.\n+\n+     (itype) (A CODE B) == (itype) A CODE (itype) B\n+\n+   for any integral type ITYPE whose precision is no greater than the\n+   precision of A and B.  */\n+\n+static bool\n+compute_distributive_range (tree type, value_range &op0_range,\n+\t\t\t    tree_code code, value_range &op1_range,\n+\t\t\t    tree *off, value_range *result_range)\n+{\n+  gcc_assert (INTEGRAL_TYPE_P (type) && !TYPE_OVERFLOW_TRAPS (type));\n+  if (result_range)\n+    {\n+      range_operator *op = range_op_handler (code, type);\n+      op->fold_range (*result_range, type, op0_range, op1_range);\n+    }\n+\n+  /* The distributive property guarantees that if TYPE is no narrower\n+     than SIZETYPE,\n+\n+       (sizetype) (OP0 CODE OP1) == (sizetype) OP0 CODE (sizetype) OP1\n+\n+     and so we can treat DELTA as zero.  */\n+  if (TYPE_PRECISION (type) >= TYPE_PRECISION (sizetype))\n+    return true;\n+\n+  /* If overflow is undefined, we can assume that:\n+\n+       X == (ssizetype) OP0 CODE (ssizetype) OP1\n+\n+     is within the range of TYPE, i.e.:\n+\n+       X == (ssizetype) (TYPE) X\n+\n+     Distributing the (TYPE) truncation over X gives:\n+\n+       X == (ssizetype) (OP0 CODE OP1)\n+\n+     Casting both sides to sizetype and distributing the sizetype cast\n+     over X gives:\n+\n+       (sizetype) OP0 CODE (sizetype) OP1 == (sizetype) (OP0 CODE OP1)\n+\n+     and so we can treat DELTA as zero.  */\n+  if (TYPE_OVERFLOW_UNDEFINED (type))\n+    return true;\n+\n+  /* Compute the range of:\n+\n+       (ssizetype) OP0 CODE (ssizetype) OP1\n+\n+     The distributive property guarantees that this has the same bitpattern as:\n+\n+       (sizetype) OP0 CODE (sizetype) OP1\n+\n+     but its range is more conducive to analysis.  */\n+  range_cast (op0_range, ssizetype);\n+  range_cast (op1_range, ssizetype);\n+  value_range wide_range;\n+  range_operator *op = range_op_handler (code, ssizetype);\n+  bool saved_flag_wrapv = flag_wrapv;\n+  flag_wrapv = 1;\n+  op->fold_range (wide_range, ssizetype, op0_range, op1_range);\n+  flag_wrapv = saved_flag_wrapv;\n+  if (wide_range.num_pairs () != 1 || !range_int_cst_p (&wide_range))\n+    return false;\n+\n+  wide_int lb = wide_range.lower_bound ();\n+  wide_int ub = wide_range.upper_bound ();\n+\n+  /* Calculate the number of times that each end of the range overflows or\n+     underflows TYPE.  We can only calculate DELTA if the numbers match.  */\n+  unsigned int precision = TYPE_PRECISION (type);\n+  if (!TYPE_UNSIGNED (type))\n+    {\n+      wide_int type_min = wi::mask (precision - 1, true, lb.get_precision ());\n+      lb -= type_min;\n+      ub -= type_min;\n+    }\n+  wide_int upper_bits = wi::mask (precision, true, lb.get_precision ());\n+  lb &= upper_bits;\n+  ub &= upper_bits;\n+  if (lb != ub)\n+    return false;\n+\n+  /* OP0 CODE OP1 overflows exactly arshift (LB, PRECISION) times, with\n+     negative values indicating underflow.  The low PRECISION bits of LB\n+     are clear, so DELTA is therefore LB (== UB).  */\n+  *off = wide_int_to_tree (ssizetype, wi::to_wide (*off) - lb);\n+  return true;\n+}\n+\n+/* Return true if (sizetype) OP == (sizetype) (TO_TYPE) OP,\n+   given that OP has type FROM_TYPE and range RANGE.  Both TO_TYPE and\n+   FROM_TYPE are integral types.  */\n+\n+static bool\n+nop_conversion_for_offset_p (tree to_type, tree from_type, value_range &range)\n+{\n+  gcc_assert (INTEGRAL_TYPE_P (to_type)\n+\t      && INTEGRAL_TYPE_P (from_type)\n+\t      && !TYPE_OVERFLOW_TRAPS (to_type)\n+\t      && !TYPE_OVERFLOW_TRAPS (from_type));\n+\n+  /* Converting to something no narrower than sizetype and then to sizetype\n+     is equivalent to converting directly to sizetype.  */\n+  if (TYPE_PRECISION (to_type) >= TYPE_PRECISION (sizetype))\n+    return true;\n+\n+  /* Check whether TO_TYPE can represent all values that FROM_TYPE can.  */\n+  if (TYPE_PRECISION (from_type) < TYPE_PRECISION (to_type)\n+      && (TYPE_UNSIGNED (from_type) || !TYPE_UNSIGNED (to_type)))\n+    return true;\n+\n+  /* For narrowing conversions, we could in principle test whether\n+     the bits in FROM_TYPE but not in TO_TYPE have a fixed value\n+     and apply a constant adjustment.\n+\n+     For other conversions (which involve a sign change) we could\n+     check that the signs are always equal, and apply a constant\n+     adjustment if the signs are negative.\n+\n+     However, both cases should be rare.  */\n+  return range_fits_type_p (&range, TYPE_PRECISION (to_type),\n+\t\t\t    TYPE_SIGN (to_type));\n+}\n+\n static void\n-split_constant_offset (tree exp, tree *var, tree *off,\n+split_constant_offset (tree type, tree *var, tree *off,\n+\t\t       value_range *result_range,\n \t\t       hash_map<tree, std::pair<tree, tree> > &cache,\n \t\t       unsigned *limit);\n \n-/* Helper function for split_constant_offset.  Expresses OP0 CODE OP1\n-   (the type of the result is TYPE) as VAR + OFF, where OFF is a nonzero\n-   constant of type ssizetype, and returns true.  If we cannot do this\n-   with OFF nonzero, OFF and VAR are set to NULL_TREE instead and false\n-   is returned.  */\n+/* Helper function for split_constant_offset.  If TYPE is a pointer type,\n+   try to express OP0 CODE OP1 as:\n+\n+     POINTER_PLUS <*VAR, (sizetype) *OFF>\n+\n+   where:\n+\n+   - *VAR has type TYPE\n+   - *OFF is a constant of type ssizetype.\n+\n+   If TYPE is an integral type, try to express (sizetype) (OP0 CODE OP1) as:\n+\n+     *VAR + (sizetype) *OFF\n+\n+   where:\n+\n+   - *VAR has type sizetype\n+   - *OFF is a constant of type ssizetype.\n+\n+   In both cases, OP0 CODE OP1 has type TYPE.\n+\n+   Return true on success.  A false return value indicates that we can't\n+   do better than set *OFF to zero.\n+\n+   When returning true, set RESULT_RANGE to the range of OP0 CODE OP1,\n+   if RESULT_RANGE is nonnull and if we can do better than assume VR_VARYING.\n+\n+   CACHE caches {*VAR, *OFF} pairs for SSA names that we've previously\n+   visited.  LIMIT counts down the number of SSA names that we are\n+   allowed to process before giving up.  */\n \n static bool\n split_constant_offset_1 (tree type, tree op0, enum tree_code code, tree op1,\n-\t\t\t tree *var, tree *off,\n+\t\t\t tree *var, tree *off, value_range *result_range,\n \t\t\t hash_map<tree, std::pair<tree, tree> > &cache,\n \t\t\t unsigned *limit)\n {\n   tree var0, var1;\n   tree off0, off1;\n-  enum tree_code ocode = code;\n+  value_range op0_range, op1_range;\n \n   *var = NULL_TREE;\n   *off = NULL_TREE;\n \n   switch (code)\n     {\n     case INTEGER_CST:\n-      *var = build_int_cst (type, 0);\n+      *var = size_int (0);\n       *off = fold_convert (ssizetype, op0);\n+      if (result_range)\n+\tresult_range->set (op0, op0);\n       return true;\n \n     case POINTER_PLUS_EXPR:\n-      ocode = PLUS_EXPR;\n-      /* FALLTHROUGH */\n+      split_constant_offset (op0, &var0, &off0, nullptr, cache, limit);\n+      split_constant_offset (op1, &var1, &off1, nullptr, cache, limit);\n+      *var = fold_build2 (POINTER_PLUS_EXPR, type, var0, var1);\n+      *off = size_binop (PLUS_EXPR, off0, off1);\n+      return true;\n+\n     case PLUS_EXPR:\n     case MINUS_EXPR:\n-      if (TREE_CODE (op1) == INTEGER_CST)\n-\t{\n-\t  split_constant_offset (op0, &var0, &off0, cache, limit);\n-\t  *var = var0;\n-\t  *off = size_binop (ocode, off0, fold_convert (ssizetype, op1));\n-\t  return true;\n-\t}\n-      split_constant_offset (op0, &var0, &off0, cache, limit);\n-      split_constant_offset (op1, &var1, &off1, cache, limit);\n-      *var = fold_build2 (code, type, var0, var1);\n-      *off = size_binop (ocode, off0, off1);\n+      split_constant_offset (op0, &var0, &off0, &op0_range, cache, limit);\n+      split_constant_offset (op1, &var1, &off1, &op1_range, cache, limit);\n+      *off = size_binop (code, off0, off1);\n+      if (!compute_distributive_range (type, op0_range, code, op1_range,\n+\t\t\t\t       off, result_range))\n+\treturn false;\n+      *var = fold_build2 (code, sizetype, var0, var1);\n       return true;\n \n     case MULT_EXPR:\n       if (TREE_CODE (op1) != INTEGER_CST)\n \treturn false;\n \n-      split_constant_offset (op0, &var0, &off0, cache, limit);\n-      *var = fold_build2 (MULT_EXPR, type, var0, op1);\n+      split_constant_offset (op0, &var0, &off0, &op0_range, cache, limit);\n+      op1_range.set (op1, op1);\n       *off = size_binop (MULT_EXPR, off0, fold_convert (ssizetype, op1));\n+      if (!compute_distributive_range (type, op0_range, code, op1_range,\n+\t\t\t\t       off, result_range))\n+\treturn false;\n+      *var = fold_build2 (MULT_EXPR, sizetype, var0,\n+\t\t\t  fold_convert (sizetype, op1));\n       return true;\n \n     case ADDR_EXPR:\n@@ -658,13 +837,10 @@ split_constant_offset_1 (tree type, tree op0, enum tree_code code, tree op1,\n \n \tif (poffset)\n \t  {\n-\t    split_constant_offset (poffset, &poffset, &off1, cache, limit);\n+\t    split_constant_offset (poffset, &poffset, &off1, nullptr,\n+\t\t\t\t   cache, limit);\n \t    off0 = size_binop (PLUS_EXPR, off0, off1);\n-\t    if (POINTER_TYPE_P (TREE_TYPE (base)))\n-\t      base = fold_build_pointer_plus (base, poffset);\n-\t    else\n-\t      base = fold_build2 (PLUS_EXPR, TREE_TYPE (base), base,\n-\t\t\t\t  fold_convert (TREE_TYPE (base), poffset));\n+\t    base = fold_build_pointer_plus (base, poffset);\n \t  }\n \n \tvar0 = fold_convert (type, base);\n@@ -723,6 +899,7 @@ split_constant_offset_1 (tree type, tree op0, enum tree_code code, tree op1,\n \t\t  return false;\n \t\t*var = e.first;\n \t\t*off = e.second;\n+\t\t/* The caller sets the range in this case.  */\n \t\treturn true;\n \t      }\n \t    e = std::make_pair (op0, ssize_int (0));\n@@ -736,72 +913,80 @@ split_constant_offset_1 (tree type, tree op0, enum tree_code code, tree op1,\n \tvar1 = gimple_assign_rhs2 (def_stmt);\n \n \tbool res = split_constant_offset_1 (type, var0, subcode, var1,\n-\t\t\t\t\t    var, off, cache, limit);\n+\t\t\t\t\t    var, off, nullptr, cache, limit);\n \tif (res && use_cache)\n \t  *cache.get (op0) = std::make_pair (*var, *off);\n+\t/* The caller sets the range in this case.  */\n \treturn res;\n       }\n     CASE_CONVERT:\n       {\n-\t/* We must not introduce undefined overflow, and we must not change\n-\t   the value.  Hence we're okay if the inner type doesn't overflow\n-\t   to start with (pointer or signed), the outer type also is an\n-\t   integer or pointer and the outer precision is at least as large\n-\t   as the inner.  */\n+\t/* We can only handle the following conversions:\n+\n+\t   - Conversions from one pointer type to another pointer type.\n+\n+\t   - Conversions from one non-trapping integral type to another\n+\t     non-trapping integral type.  In this case, the recursive\n+\t     call makes sure that:\n+\n+\t       (sizetype) OP0\n+\n+\t     can be expressed as a sizetype operation involving VAR and OFF,\n+\t     and all we need to do is check whether:\n+\n+\t       (sizetype) OP0 == (sizetype) (TYPE) OP0\n+\n+\t   - Conversions from a non-trapping sizetype-size integral type to\n+\t     a like-sized pointer type.  In this case, the recursive call\n+\t     makes sure that:\n+\n+\t       (sizetype) OP0 == *VAR + (sizetype) *OFF\n+\n+\t     and we can convert that to:\n+\n+\t       POINTER_PLUS <(TYPE) *VAR, (sizetype) *OFF>\n+\n+\t   - Conversions from a sizetype-sized pointer type to a like-sized\n+\t     non-trapping integral type.  In this case, the recursive call\n+\t     makes sure that:\n+\n+\t       OP0 == POINTER_PLUS <*VAR, (sizetype) *OFF>\n+\n+\t     where the POINTER_PLUS and *VAR have the same precision as\n+\t     TYPE (and the same precision as sizetype).  Then:\n+\n+\t       (sizetype) (TYPE) OP0 == (sizetype) *VAR + (sizetype) *OFF.  */\n \ttree itype = TREE_TYPE (op0);\n \tif ((POINTER_TYPE_P (itype)\n \t     || (INTEGRAL_TYPE_P (itype) && !TYPE_OVERFLOW_TRAPS (itype)))\n-\t    && TYPE_PRECISION (type) >= TYPE_PRECISION (itype)\n-\t    && (POINTER_TYPE_P (type) || INTEGRAL_TYPE_P (type)))\n+\t    && (POINTER_TYPE_P (type)\n+\t\t|| (INTEGRAL_TYPE_P (type) && !TYPE_OVERFLOW_TRAPS (type)))\n+\t    && (POINTER_TYPE_P (type) == POINTER_TYPE_P (itype)\n+\t\t|| (TYPE_PRECISION (type) == TYPE_PRECISION (sizetype)\n+\t\t    && TYPE_PRECISION (itype) == TYPE_PRECISION (sizetype))))\n \t  {\n-\t    if (INTEGRAL_TYPE_P (itype) && TYPE_OVERFLOW_WRAPS (itype)\n-\t\t&& (TYPE_PRECISION (type) > TYPE_PRECISION (itype)\n-\t\t    || TYPE_UNSIGNED (itype) != TYPE_UNSIGNED (type)))\n+\t    if (POINTER_TYPE_P (type))\n+\t      {\n+\t\tsplit_constant_offset (op0, var, off, nullptr, cache, limit);\n+\t\t*var = fold_convert (type, *var);\n+\t      }\n+\t    else if (POINTER_TYPE_P (itype))\n+\t      {\n+\t\tsplit_constant_offset (op0, var, off, nullptr, cache, limit);\n+\t\t*var = fold_convert (sizetype, *var);\n+\t      }\n+\t    else\n \t      {\n-\t\t/* Split the unconverted operand and try to prove that\n-\t\t   wrapping isn't a problem.  */\n-\t\ttree tmp_var, tmp_off;\n-\t\tsplit_constant_offset (op0, &tmp_var, &tmp_off, cache, limit);\n-\n-\t\t/* See whether we have an known range [A, B] for tmp_var.  */\n-\t\twide_int var_min, var_max;\n-\t\tsignop sgn = TYPE_SIGN (itype);\n-\t\tif (TREE_CODE (tmp_var) == SSA_NAME)\n+\t\tsplit_constant_offset (op0, var, off, &op0_range,\n+\t\t\t\t       cache, limit);\n+\t\tif (!nop_conversion_for_offset_p (type, itype, op0_range))\n+\t\t  return false;\n+\t\tif (result_range)\n \t\t  {\n-\t\t    value_range_kind vr_type\n-\t\t      = get_range_info (tmp_var, &var_min, &var_max);\n-\t\t    wide_int var_nonzero = get_nonzero_bits (tmp_var);\n-\t\t    if (intersect_range_with_nonzero_bits (vr_type, &var_min,\n-\t\t\t\t\t\t\t   &var_max,\n-\t\t\t\t\t\t\t   var_nonzero,\n-\t\t\t\t\t\t\t   sgn) != VR_RANGE)\n-\t\t      return false;\n+\t\t    *result_range = op0_range;\n+\t\t    range_cast (*result_range, type);\n \t\t  }\n-\t\telse if (determine_value_range (tmp_var, &var_min, &var_max)\n-\t\t\t != VR_RANGE)\n-\t\t  return false;\n-\n-\t\t/* See whether the range of OP0 (i.e. TMP_VAR + TMP_OFF)\n-\t\t   is known to be [A + TMP_OFF, B + TMP_OFF], with all\n-\t\t   operations done in ITYPE.  The addition must overflow\n-\t\t   at both ends of the range or at neither.  */\n-\t\twi::overflow_type overflow[2];\n-\t\tunsigned int prec = TYPE_PRECISION (itype);\n-\t\twide_int woff = wi::to_wide (tmp_off, prec);\n-\t\twide_int op0_min = wi::add (var_min, woff, sgn, &overflow[0]);\n-\t\twi::add (var_max, woff, sgn, &overflow[1]);\n-\t\tif ((overflow[0] != wi::OVF_NONE) != (overflow[1] != wi::OVF_NONE))\n-\t\t  return false;\n-\n-\t\t/* Calculate (ssizetype) OP0 - (ssizetype) TMP_VAR.  */\n-\t\twidest_int diff = (widest_int::from (op0_min, sgn)\n-\t\t\t\t   - widest_int::from (var_min, sgn));\n-\t\tvar0 = tmp_var;\n-\t\t*off = wide_int_to_tree (ssizetype, diff);\n \t      }\n-\t    else\n-\t      split_constant_offset (op0, &var0, off, cache, limit);\n-\t    *var = fold_convert (type, var0);\n \t    return true;\n \t  }\n \treturn false;\n@@ -812,41 +997,89 @@ split_constant_offset_1 (tree type, tree op0, enum tree_code code, tree op1,\n     }\n }\n \n-/* Expresses EXP as VAR + OFF, where off is a constant.  The type of OFF\n-   will be ssizetype.  */\n+/* If EXP has pointer type, try to express it as:\n+\n+     POINTER_PLUS <*VAR, (sizetype) *OFF>\n+\n+   where:\n+\n+   - *VAR has the same type as EXP\n+   - *OFF is a constant of type ssizetype.\n+\n+   If EXP has an integral type, try to express (sizetype) EXP as:\n+\n+     *VAR + (sizetype) *OFF\n+\n+   where:\n+\n+   - *VAR has type sizetype\n+   - *OFF is a constant of type ssizetype.\n+\n+   If EXP_RANGE is nonnull, set it to the range of EXP.\n+\n+   CACHE caches {*VAR, *OFF} pairs for SSA names that we've previously\n+   visited.  LIMIT counts down the number of SSA names that we are\n+   allowed to process before giving up.  */\n \n static void\n-split_constant_offset (tree exp, tree *var, tree *off,\n+split_constant_offset (tree exp, tree *var, tree *off, value_range *exp_range,\n \t\t       hash_map<tree, std::pair<tree, tree> > &cache,\n \t\t       unsigned *limit)\n {\n-  tree type = TREE_TYPE (exp), op0, op1, e, o;\n+  tree type = TREE_TYPE (exp), op0, op1;\n   enum tree_code code;\n \n-  *var = exp;\n-  *off = ssize_int (0);\n+  code = TREE_CODE (exp);\n+  if (exp_range)\n+    {\n+      *exp_range = type;\n+      if (code == SSA_NAME)\n+\t{\n+\t  wide_int var_min, var_max;\n+\t  value_range_kind vr_kind = get_range_info (exp, &var_min, &var_max);\n+\t  wide_int var_nonzero = get_nonzero_bits (exp);\n+\t  vr_kind = intersect_range_with_nonzero_bits (vr_kind,\n+\t\t\t\t\t\t       &var_min, &var_max,\n+\t\t\t\t\t\t       var_nonzero,\n+\t\t\t\t\t\t       TYPE_SIGN (type));\n+\t  if (vr_kind == VR_RANGE)\n+\t    *exp_range = value_range (type, var_min, var_max);\n+\t}\n+    }\n \n-  if (tree_is_chrec (exp)\n-      || get_gimple_rhs_class (TREE_CODE (exp)) == GIMPLE_TERNARY_RHS)\n-    return;\n+  if (!tree_is_chrec (exp)\n+      && get_gimple_rhs_class (TREE_CODE (exp)) != GIMPLE_TERNARY_RHS)\n+    {\n+      extract_ops_from_tree (exp, &code, &op0, &op1);\n+      if (split_constant_offset_1 (type, op0, code, op1, var, off,\n+\t\t\t\t   exp_range, cache, limit))\n+\treturn;\n+    }\n \n-  code = TREE_CODE (exp);\n-  extract_ops_from_tree (exp, &code, &op0, &op1);\n-  if (split_constant_offset_1 (type, op0, code, op1, &e, &o, cache, limit))\n+  *var = exp;\n+  if (INTEGRAL_TYPE_P (type))\n+    *var = fold_convert (sizetype, *var);\n+  *off = ssize_int (0);\n+  if (exp_range && code != SSA_NAME)\n     {\n-      *var = e;\n-      *off = o;\n+      wide_int var_min, var_max;\n+      if (determine_value_range (exp, &var_min, &var_max) == VR_RANGE)\n+\t*exp_range = value_range (type, var_min, var_max);\n     }\n }\n \n+/* Expresses EXP as VAR + OFF, where OFF is a constant.  VAR has the same\n+   type as EXP while OFF has type ssizetype.  */\n+\n void\n split_constant_offset (tree exp, tree *var, tree *off)\n {\n   unsigned limit = param_ssa_name_def_chain_limit;\n   static hash_map<tree, std::pair<tree, tree> > *cache;\n   if (!cache)\n     cache = new hash_map<tree, std::pair<tree, tree> > (37);\n-  split_constant_offset (exp, var, off, *cache, &limit);\n+  split_constant_offset (exp, var, off, nullptr, *cache, &limit);\n+  *var = fold_convert (TREE_TYPE (exp), *var);\n   cache->empty ();\n }\n "}]}