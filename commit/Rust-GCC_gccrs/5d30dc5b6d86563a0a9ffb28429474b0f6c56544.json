{"sha": "5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NWQzMGRjNWI2ZDg2NTYzYTBhOWZmYjI4NDI5NDc0YjBmNmM1NjU0NA==", "commit": {"author": {"name": "Oleg Endo", "email": "olegendo@gcc.gnu.org", "date": "2013-10-12T20:47:22Z"}, "committer": {"name": "Oleg Endo", "email": "olegendo@gcc.gnu.org", "date": "2013-10-12T20:47:22Z"}, "message": "re PR target/51244 ([SH] Inefficient conditional branch and code around T bit)\n\n\tPR target/51244\n\t* config/sh/sh_treg_combine.cc: New SH specific RTL pass.\n\t* config.gcc (SH extra_objs): Add sh_ifcvt.o.\n\t* config/sh/t-sh (sh_treg_combine.o): New entry.\n\t* config/sh/sh.c (sh_fixed_condition_code_regs): New function that\n\timplements the target hook TARGET_FIXED_CONDITION_CODE_REGS.\n\t(register_sh_passes): New function.  Register sh_treg_combine pass.\n\t(sh_option_override): Invoke it.\n\t(sh_canonicalize_comparison): Handle op0_preserve_value.\n\t* sh.md (*cbranch_t\"): Do not try to optimize missed test and branch\n\topportunities.  Canonicalize branch condition.\n\t(nott): Allow only if pseudos can be created for non-SH2A.\n\n\tPR target/51244\n\t* gcc.dg/torture/p51244-21.c: New.\n\t* gcc.target/sh/pr51244-20.c: New.\n\t* gcc.target/sh/pr51244-20-sh2a.c: New.\n\nFrom-SVN: r203492", "tree": {"sha": "d10115063632d11cde88f211517d35808a4261b8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d10115063632d11cde88f211517d35808a4261b8"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/comments", "author": null, "committer": null, "parents": [{"sha": "585a0b99166d96d04c6b65aca3ff797d7316e0ac", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/585a0b99166d96d04c6b65aca3ff797d7316e0ac", "html_url": "https://github.com/Rust-GCC/gccrs/commit/585a0b99166d96d04c6b65aca3ff797d7316e0ac"}], "stats": {"total": 1882, "additions": 1802, "deletions": 80}, "files": [{"sha": "5ad86db2457c5fe9426d54af9585d3f39ca2d897", "filename": "gcc/ChangeLog", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "patch": "@@ -1,3 +1,18 @@\n+2013-10-12  Oleg Endo  <olegendo@gcc.gnu.org>\n+\n+\tPR target/51244\n+\t* config/sh/sh_treg_combine.cc: New SH specific RTL pass.\n+\t* config.gcc (SH extra_objs): Add sh_ifcvt.o.\n+\t* config/sh/t-sh (sh_treg_combine.o): New entry.\n+\t* config/sh/sh.c (sh_fixed_condition_code_regs): New function that\n+\timplements the target hook TARGET_FIXED_CONDITION_CODE_REGS.\n+\t(register_sh_passes): New function.  Register sh_treg_combine pass.\n+\t(sh_option_override): Invoke it.\n+\t(sh_canonicalize_comparison): Handle op0_preserve_value.\n+\t* sh.md (*cbranch_t\"): Do not try to optimize missed test and branch\n+\topportunities.  Canonicalize branch condition.\n+\t(nott): Allow only if pseudos can be created for non-SH2A.\n+\n 2013-10-12  H.J. Lu  <hongjiu.lu@intel.com>\n \n \tPR target/58690"}, {"sha": "c59e2caf7d316de4234e308403ee9d30bbce92db", "filename": "gcc/config.gcc", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "patch": "@@ -465,6 +465,7 @@ sh[123456789lbe]*-*-* | sh-*-*)\n \tcpu_type=sh\n \tneed_64bit_hwint=yes\n \textra_options=\"${extra_options} fused-madd.opt\"\n+\textra_objs=\"${extra_objs} sh_treg_combine.o\"\n \t;;\n v850*-*-*)\n \tcpu_type=v850"}, {"sha": "31d162597b8b6eddc0faba2e1d342c2a867315d1", "filename": "gcc/config/sh/sh.c", "status": "modified", "additions": 49, "deletions": 1, "changes": 50, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Fconfig%2Fsh%2Fsh.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Fconfig%2Fsh%2Fsh.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Fsh.c?ref=5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "patch": "@@ -53,6 +53,9 @@ along with GCC; see the file COPYING3.  If not see\n #include \"alloc-pool.h\"\n #include \"tm-constrs.h\"\n #include \"opts.h\"\n+#include \"tree-pass.h\"\n+#include \"pass_manager.h\"\n+#include \"context.h\"\n \n #include <sstream>\n #include <vector>\n@@ -311,6 +314,7 @@ static bool sequence_insn_p (rtx);\n static void sh_canonicalize_comparison (int *, rtx *, rtx *, bool);\n static void sh_canonicalize_comparison (enum rtx_code&, rtx&, rtx&,\n \t\t\t\t\tenum machine_mode, bool);\n+static bool sh_fixed_condition_code_regs (unsigned int* p1, unsigned int* p2);\n \n static void sh_init_sync_libfuncs (void) ATTRIBUTE_UNUSED;\n \f\n@@ -587,6 +591,9 @@ static const struct attribute_spec sh_attribute_table[] =\n #undef TARGET_CANONICALIZE_COMPARISON\n #define TARGET_CANONICALIZE_COMPARISON\tsh_canonicalize_comparison\n \n+#undef TARGET_FIXED_CONDITION_CODE_REGS\n+#define TARGET_FIXED_CONDITION_CODE_REGS sh_fixed_condition_code_regs\n+\n /* Machine-specific symbol_ref flags.  */\n #define SYMBOL_FLAG_FUNCVEC_FUNCTION\t(SYMBOL_FLAG_MACH_DEP << 0)\n \n@@ -710,6 +717,34 @@ got_mode_name:;\n #undef err_ret\n }\n \n+/* Register SH specific RTL passes.  */\n+extern opt_pass* make_pass_sh_treg_combine (gcc::context* ctx, bool split_insns,\n+\t\t\t\t     const char* name);\n+static void\n+register_sh_passes (void)\n+{\n+  if (!TARGET_SH1)\n+    return;\n+\n+/* Running the sh_treg_combine pass after ce1 generates better code when\n+   comparisons are combined and reg-reg moves are introduced, because\n+   reg-reg moves will be eliminated afterwards.  However, there are quite\n+   some cases where combine will be unable to fold comparison related insns,\n+   thus for now don't do it.\n+  register_pass (make_pass_sh_treg_combine (g, false, \"sh_treg_combine1\"),\n+\t\t PASS_POS_INSERT_AFTER, \"ce1\", 1);\n+*/\n+\n+  /* Run sh_treg_combine pass after combine but before register allocation.  */\n+  register_pass (make_pass_sh_treg_combine (g, true, \"sh_treg_combine2\"),\n+\t\t PASS_POS_INSERT_AFTER, \"split1\", 1);\n+\n+  /* Run sh_treg_combine pass after register allocation and basic block\n+     reordering as this sometimes creates new opportunities.  */\n+  register_pass (make_pass_sh_treg_combine (g, true, \"sh_treg_combine3\"),\n+\t\t PASS_POS_INSERT_AFTER, \"split4\", 1);\n+}\n+\n /* Implement TARGET_OPTION_OVERRIDE macro.  Validate and override \n    various options, and do some machine dependent initialization.  */\n static void\n@@ -1022,6 +1057,8 @@ sh_option_override (void)\n      target CPU.  */\n   selected_atomic_model_\n     = parse_validate_atomic_model_option (sh_atomic_model_str);\n+\n+  register_sh_passes ();\n }\n \f\n /* Print the operand address in x to the stream.  */\n@@ -1908,7 +1945,7 @@ prepare_move_operands (rtx operands[], enum machine_mode mode)\n static void\n sh_canonicalize_comparison (enum rtx_code& cmp, rtx& op0, rtx& op1,\n \t\t\t    enum machine_mode mode,\n-\t\t\t    bool op0_preserve_value ATTRIBUTE_UNUSED)\n+\t\t\t    bool op0_preserve_value)\n {\n   /* When invoked from within the combine pass the mode is not specified,\n      so try to get it from one of the operands.  */\n@@ -1928,6 +1965,9 @@ sh_canonicalize_comparison (enum rtx_code& cmp, rtx& op0, rtx& op1,\n   // Make sure that the constant operand is the second operand.\n   if (CONST_INT_P (op0) && !CONST_INT_P (op1))\n     {\n+      if (op0_preserve_value)\n+\treturn;\n+\n       std::swap (op0, op1);\n       cmp = swap_condition (cmp);\n     }\n@@ -2016,6 +2056,14 @@ sh_canonicalize_comparison (int *code, rtx *op0, rtx *op1,\n   *code = (int)tmp_code;\n }\n \n+bool\n+sh_fixed_condition_code_regs (unsigned int* p1, unsigned int* p2)\n+{\n+  *p1 = T_REG;\n+  *p2 = INVALID_REGNUM;\n+  return true;\n+}\n+\n enum rtx_code\n prepare_cbranch_operands (rtx *operands, enum machine_mode mode,\n \t\t\t  enum rtx_code comparison)"}, {"sha": "d8480cdfe6f82a853952fb86e95e56052c29fcc1", "filename": "gcc/config/sh/sh.md", "status": "modified", "additions": 25, "deletions": 79, "changes": 104, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Fconfig%2Fsh%2Fsh.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Fconfig%2Fsh%2Fsh.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Fsh.md?ref=5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "patch": "@@ -8419,89 +8419,32 @@ label:\n   return output_branch (sh_eval_treg_value (operands[1]), insn, operands);\n }\n   \"&& 1\"\n-  [(set (pc) (if_then_else (eq (reg:SI T_REG) (match_dup 2))\n-\t\t\t   (label_ref (match_dup 0))\n-\t\t\t   (pc)))]\n+  [(const_int 0)]\n {\n-  /* Try to find missed test and branch combine opportunities which result\n-     in redundant T bit tests before conditional branches.\n-     This is done not only after combine (and before reload) but in every\n-     split pass, because some opportunities are formed also after combine.\n-     FIXME: Probably this would not be needed if CCmode was used\n-     together with TARGET_FIXED_CONDITION_CODE_REGS.  */\n-\n-  const int treg_value = sh_eval_treg_value (operands[1]);\n-  operands[2] = NULL_RTX;\n-\n-  /* Scan the insns backwards for an insn that sets the T bit by testing a\n-     reg against zero like:\n-\t(set (reg T_REG) (eq (reg) (const_int 0)))  */\n-  rtx testing_insn = NULL_RTX;\n-  rtx tested_reg = NULL_RTX;\n-\n-  set_of_reg s0 = sh_find_set_of_reg (get_t_reg_rtx (), curr_insn,\n-\t\t\t\t      prev_nonnote_insn_bb);\n-  if (s0.set_src != NULL_RTX\n-      && GET_CODE (s0.set_src) == EQ\n-      && REG_P (XEXP (s0.set_src, 0))\n-      && satisfies_constraint_Z (XEXP (s0.set_src, 1)))\n-    {\n-      testing_insn = s0.insn;\n-      tested_reg = XEXP (s0.set_src, 0);\n-    }\n-  else\n-    FAIL;\n-\n-  /* Continue scanning the insns backwards and try to find the insn that\n-     sets the tested reg which we found above.  If the reg is set by storing\n-     the T bit or the negated T bit we can eliminate the test insn before\n-     the branch.  Notice that the branch condition has to be inverted if the\n-     test is eliminated.  */\n-\n-  /* If the T bit is used between the testing insn and the brach insn\n-     leave it alone.  */\n-  if (reg_used_between_p (get_t_reg_rtx (), testing_insn, curr_insn))\n-    FAIL;\n-\n-  while (true)\n-    {\n-      /* It's not safe to go beyond the current basic block after reload.  */\n-      set_of_reg s1 = sh_find_set_of_reg (tested_reg, s0.insn,\n-\t\t\t\t\t  reload_completed\n-\t\t\t\t\t  ? prev_nonnote_insn_bb\n-\t\t\t\t\t  : prev_nonnote_insn);\n-      if (s1.set_src == NULL_RTX)\n-\tbreak;\n+  /* Try to canonicalize the branch condition if it is not one of:\n+\t(ne (reg:SI T_REG) (const_int 0))\n+\t(eq (reg:SI T_REG) (const_int 0))\n \n-      if (t_reg_operand (s1.set_src, VOIDmode))\n-\toperands[2] = GEN_INT (treg_value ^ 1);\n-      else if (negt_reg_operand (s1.set_src, VOIDmode))\n-\toperands[2] = GEN_INT (treg_value);\n-      else if (REG_P (s1.set_src))\n-\t{\n-\t   /* If it's a reg-reg copy follow the copied reg.  This can\n-\t      happen e.g. when T bit store zero-extensions are\n-\t      eliminated.  */\n-\t  tested_reg = s1.set_src;\n-\t  s0.insn = s1.insn;\n-\t  continue;\n-\t}\n+     Instead of splitting out a new insn, we modify the current insn's\n+     operands as needed.  This preserves things such as REG_DEAD notes.  */\n \n-\t/* It's only safe to remove the testing insn if the T bit is not\n-\t   modified between the testing insn and the insn that stores the\n-\t   T bit.  Notice that some T bit stores such as negc also modify\n-\t   the T bit.  */\n-\tif (modified_between_p (get_t_reg_rtx (), s1.insn, testing_insn)\n-\t    || modified_in_p (get_t_reg_rtx (), s1.insn))\n-\t  operands[2] = NULL_RTX;\n+  if ((GET_CODE (operands[1]) == EQ || GET_CODE (operands[1]) == NE)\n+      && REG_P (XEXP (operands[1], 0)) && REGNO (XEXP (operands[1], 0)) == T_REG\n+      && XEXP (operands[1], 1) == const0_rtx)\n+    DONE;\n \n-\tbreak;\n-    }\n+  int branch_cond = sh_eval_treg_value (operands[1]);\n+  rtx new_cond_rtx = NULL_RTX;\n \n-  if (operands[2] == NULL_RTX)\n-    FAIL;\n+  if (branch_cond == 0)\n+    new_cond_rtx = gen_rtx_EQ (VOIDmode, get_t_reg_rtx (), const0_rtx);\n+  else if (branch_cond == 1)\n+    new_cond_rtx = gen_rtx_NE (VOIDmode, get_t_reg_rtx (), const0_rtx);\n \n-  set_insn_deleted (testing_insn);\n+  if (new_cond_rtx != NULL_RTX)\n+    validate_change (curr_insn, &XEXP (XEXP (PATTERN (curr_insn), 1), 0),\n+\t\t     new_cond_rtx, false);\n+  DONE;\n }\n   [(set_attr \"type\" \"cbranch\")])\n \n@@ -11480,10 +11423,13 @@ label:\n ;; multiple insns like:\n ;;\tmovt\tRn\n ;;\ttst\tRn,Rn\n+;; This requires an additional pseudo.  The SH specific sh_treg_combine RTL\n+;; pass will look for this insn.  Disallow using it if pseudos can't be\n+;; created.\n (define_insn_and_split \"nott\"\n   [(set (reg:SI T_REG)\n-\t(xor:SI (match_operand:SI 0 \"t_reg_operand\" \"\") (const_int 1)))]\n-  \"TARGET_SH1\"\n+\t(xor:SI (match_operand:SI 0 \"t_reg_operand\") (const_int 1)))]\n+  \"TARGET_SH2A || (TARGET_SH1 && can_create_pseudo_p ())\"\n {\n   gcc_assert (TARGET_SH2A);\n   return \"nott\";"}, {"sha": "0f9027ec763b61392fdb6b550da5b22da2b986b4", "filename": "gcc/config/sh/sh_treg_combine.cc", "status": "added", "additions": 1509, "deletions": 0, "changes": 1509, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Fconfig%2Fsh%2Fsh_treg_combine.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Fconfig%2Fsh%2Fsh_treg_combine.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Fsh_treg_combine.cc?ref=5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "patch": "@@ -0,0 +1,1509 @@\n+/* An SH specific RTL pass that tries to combine comparisons and redundant\n+   condition code register stores across multiple basic blocks.\n+   Copyright (C) 2013 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"machmode.h\"\n+#include \"basic-block.h\"\n+#include \"df.h\"\n+#include \"rtl.h\"\n+#include \"insn-config.h\"\n+#include \"insn-codes.h\"\n+#include \"emit-rtl.h\"\n+#include \"recog.h\"\n+#include \"tree-pass.h\"\n+#include \"target.h\"\n+#include \"expr.h\"\n+\n+#include <algorithm>\n+#include <list>\n+#include <vector>\n+\n+/*\n+This pass tries to optimize for example this:\n+\tmov.l\t@(4,r4),r1\n+\ttst\tr1,r1\n+\tmovt\tr1\n+\ttst\tr1,r1\n+\tbt/s\t.L5\n+\n+into something simpler:\n+\tmov.l\t@(4,r4),r1\n+\ttst\tr1,r1\n+\tbf/s\t.L5\n+\n+Such sequences can be identified by looking for conditional branches and\n+checking whether the ccreg is set before the conditional branch\n+by testing another register for != 0, which was set by a ccreg store.\n+This can be optimized by eliminating the redundant comparison and\n+inverting the branch condition.  There can be multiple comparisons in\n+different basic blocks that all end up in the redunant test insn before the\n+conditional branch.  Some example RTL ...\n+\n+Example 1)\n+----------\n+\n+[bb 3]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 173) (const_int 0)))\n+(set (reg:SI 167) (xor:SI (reg:SI 147 t) (const_int 1)))\n+-> bb 5\n+\n+[bb 4]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 177) (const_int 0)))\n+(set (reg:SI 167) (reg:SI 147 t))\n+-> bb 5\n+\n+[bb 5]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 167) (const_int 0)))\n+(set (pc) (if_then_else (ne (reg:SI 147 t) (const_int 0))\n+                        (label_ref:SI 50) (pc)))\n+\n+In [bb 4] elimination of the comparison would require inversion of the branch\n+condition and compensation of other BBs.\n+Instead an inverting reg-move can be used:\n+\n+[bb 3]\n+(set (reg:SI 167) (reg:SI 173))\n+-> bb 5\n+\n+[BB 4]\n+(set (reg:SI 167) (not:SI (reg:SI 177)))\n+-> bb 5\n+\n+[bb 5]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 167) (const_int 0)))\n+(set (pc) (if_then_else (ne (reg:SI 147 t) (const_int 0)))\n+                        (label_ref:SI 50) (pc)))\n+\n+\n+Example 2)\n+----------\n+\n+[bb 3]\n+(set (reg:SI 147 t) (gt:SI (reg:SI 173) (reg:SI 175)))\n+(set (reg:SI 167) (reg:SI 147 t))\n+-> bb 5\n+\n+[bb 4]\n+(set (reg:SI 147 t) (gt:SI (reg:SI 177) (reg:SI 179)))\n+(set (reg:SI 167) (reg:SI 147 t))\n+-> bb 5\n+\n+[bb 5]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 167) (const_int 0)))\n+(set (pc) (if_then_else (ne (reg:SI 147 t) (const_int 0))\n+                        (label_ref:SI 51) (pc)))\n+\n+The common comparison is factored out and the branch condition is inverted:\n+\n+[bb 3]\n+(set (reg:SI 167) (reg:SI 173))\n+(set (reg:SI 200) (reg:SI 175))\n+-> bb 5\n+\n+[bb 4]\n+(set (reg:SI 167) (reg:SI 177))\n+(set (reg:SI 200) (reg:SI 179))\n+-> bb 5\n+\n+[bb 5]\n+(set (reg:SI 147 t) (gt:SI (reg:SI 167) (reg:SI 200)))\n+(set (pc) (if_then_else (eq (reg:SI 147 t) (const_int 0))\n+                        (label_ref:SI 51) (pc)))\n+\n+\n+Example 3)\n+----------\n+\n+[bb 3]\n+(set (reg:SI 147 t) (gt:SI (reg:SI 173) (reg:SI 175)))\n+(set (reg:SI 167) (reg:SI 147 t))\n+-> bb 5\n+\n+[bb 4]\n+(set (reg:SI 147 t) (ge:SI (reg:SI 179) (reg:SI 177)))\n+(set (reg:SI 167) (reg:SI 147 t))\n+-> bb 5\n+\n+[bb 5]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 167) (const_int 0)))\n+(set (pc) (if_then_else (ne (reg:SI 147 t) (const_int 0))\n+                        (label_ref:SI 51) (pc)))\n+\n+The T bit lifetime is extended and the branch condition is inverted:\n+\n+[bb 3]\n+(set (reg:SI 147 t) (gt:SI (reg:SI 173) (reg:SI 175)))\n+-> bb 5\n+\n+[bb 4]\n+(set (reg:SI 147 t) (ge:SI (reg:SI 179) (reg:SI 177)))\n+-> bb 5\n+\n+[bb 5]\n+(set (pc) (if_then_else (eq (reg:SI 147 t) (const_int 0))\n+                        (label_ref:SI 51) (pc)))\n+\n+\n+Example 4)\n+----------\n+\n+[bb 3]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 173) (const_int 5)))\n+(set (reg:SI 167) (reg:SI 147 t))\n+-> bb 5\n+\n+[bb 4]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 176) (const_int 5)))\n+(set (reg:SI 167) (xor:SI (reg:SI 147 t) (const_int 1)))\n+-> bb 5\n+\n+[bb 5]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 167) (const_int 0)))\n+(set (pc) (if_then_else (ne (reg:SI 147 t) (const_int 0))\n+                        (label_ref:SI 50) (pc)))\n+\n+In this case the comparisons are the same and could be combined, but the\n+branch condition is different for [bb 3] and [bb 5].  Since the comparison\n+is not a zero comparison, we can't negate one of the operands.  The best thing\n+we can do here is to eliminate the comparison before the cbranch and invert\n+the ccreg in one of the BBs.  On SH2A this will utilize the 'nott' instruction.\n+\n+[bb 3]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 173) (const_int 5)))\n+-> bb 5\n+\n+[bb 4]\n+(set (reg:SI 147 t) (eq:SI (reg:SI 176) (const_int 5)))\n+(set (reg:SI 147 t) (xor:SI (reg:SI 147 t) (const_int 1)))\n+-> bb 5\n+\n+[bb 5]\n+(set (pc) (if_then_else (eq (reg:SI 147 t) (const_int 0))  // inverted\n+                        (label_ref:SI 50) (pc)))\n+\n+\n+In order to handle cases such as above the RTL pass does the following:\n+\n+- Find the ccreg sets (comparisons) and ccreg stores\n+  (inverting and non-inverting) in all related BBs.\n+\n+- If the comparison types in the BBs are all the same, try to combine the\n+  comparisons in the BBs and replace the zero comparison before the cbranch\n+  with the common comparison.\n+\n+    - If the cstores are the same, move the comparison before the cbranch\n+      and replace the comparisons in the BBs with reg-reg copies to get the\n+      operands in place (create new pseudo regs).\n+\n+    - If the cstores differ, try to apply the special case\n+        (eq (reg) (const_int 0)) -> inverted = (not (reg)).\n+      for the subordinate cstore types and eliminate the dominating ones.\n+\n+- If the comparison types in the BBs are not the same, or the first approach\n+  doesn't work out for some reason, try to eliminate the comparison before the\n+  cbranch by extending the lifetime of the ccreg by leaving the individual\n+  comparisons but eliminating the cstores.\n+  If the cstores are all the same this is straight forward.\n+  If they're not, try to reverse the ccreg for the subordinate cstore type\n+  and eliminate the dominating one.\n+*/\n+\n+// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n+// Helper functions\n+\n+#define log_msg(...)\\\n+  do { if (dump_file != NULL) fprintf (dump_file, __VA_ARGS__); } while (0)\n+\n+#define log_insn(i)\\\n+  do { if (dump_file != NULL) print_rtl_single (dump_file, \\\n+\t\t\t\t\t\t(const_rtx)i); } while (0)\n+\n+#define log_rtx(r)\\\n+  do { if (dump_file != NULL) print_rtl (dump_file, (const_rtx)r); } while (0)\n+\n+#define log_return(retval, ...)\\\n+  do { if (dump_file != NULL) fprintf (dump_file, __VA_ARGS__); \\\n+       return retval; } while (0)\n+\n+#define log_return_void(...)\\\n+  do { if (dump_file != NULL) fprintf (dump_file, __VA_ARGS__); \\\n+       return; } while (0)\n+\n+struct set_of_reg\n+{\n+  // The insn where the search stopped or NULL_RTX.\n+  rtx insn;\n+\n+  // The set rtx of the specified reg if found, NULL_RTX otherwise.\n+  // Notice that the set rtx can also be in a parallel.\n+  const_rtx set_rtx;\n+\n+  // The set source operand rtx if found, NULL_RTX otherwise.\n+  rtx\n+  set_src (void) const\n+  {\n+    return set_rtx == NULL_RTX ? NULL_RTX : XEXP (set_rtx, 1);\n+  }\n+\n+  // The set destination operand rtx if found, NULL_RTX otherwise.\n+  rtx\n+  set_dst (void) const\n+  {\n+    return set_rtx == NULL_RTX ? NULL_RTX : XEXP (set_rtx, 0);\n+  }\n+\n+  bool\n+  empty (void) const\n+  {\n+    return insn == NULL_RTX || set_rtx == NULL_RTX;\n+  }\n+};\n+\n+// Given a reg rtx and a start insn find the insn (in the same basic block)\n+// that sets the reg.\n+static set_of_reg\n+find_set_of_reg_bb (rtx reg, rtx insn)\n+{\n+  set_of_reg result = { insn, NULL_RTX };\n+\n+  if (!REG_P (reg) || insn == NULL_RTX)\n+    return result;\n+\n+  for (result.insn = insn; result.insn != NULL_RTX;\n+       result.insn = prev_nonnote_insn_bb (result.insn))\n+    {\n+      if (BARRIER_P (result.insn))\n+\treturn result;\n+      if (!NONJUMP_INSN_P (result.insn))\n+\tcontinue;\n+      if (reg_set_p (reg, result.insn))\n+\t{\n+\t  result.set_rtx = set_of (reg, result.insn);\n+\t  if (result.set_rtx == NULL_RTX || GET_CODE (result.set_rtx) != SET)\n+\t    result.set_rtx = NULL_RTX;\n+\t  return result;\n+\t}\n+    }\n+\n+  return result;\n+}\n+\n+static bool\n+reg_dead_after_insn (const_rtx reg, const_rtx insn)\n+{\n+  return find_regno_note (insn, REG_DEAD, REGNO (reg)) != NULL_RTX;\n+}\n+\n+static bool\n+reg_unused_after_insn (const_rtx reg, const_rtx insn)\n+{\n+  return find_regno_note (insn, REG_UNUSED, REGNO (reg)) != NULL_RTX;\n+}\n+\n+// Check whether the two specified basic blocks are adjacent, i.e. there's no\n+// other basic block in between them.\n+static bool\n+is_adjacent_bb (basic_block a, basic_block b)\n+{\n+  basic_block bb0[] = { a, b };\n+  basic_block bb1[] = { b, a };\n+\n+  for (int i = 0; i < 2; ++i)\n+    for (edge_iterator ei = ei_start (bb0[i]->succs);\n+\t !ei_end_p (ei); ei_next (&ei))\n+      if (ei_edge (ei)->dest == bb1[i])\n+\treturn true;\n+\n+  return false;\n+}\n+\n+// Internal function of trace_reg_uses.\n+static void\n+trace_reg_uses_1 (rtx reg, rtx start_insn, basic_block bb, int& count,\n+\t\t  std::vector<basic_block>& visited_bb, rtx abort_at_insn)\n+{\n+  if (bb == NULL)\n+    return;\n+\n+  if (std::find (visited_bb.begin (), visited_bb.end (), bb)\n+      != visited_bb.end ())\n+    log_return_void (\"[bb %d] already visited\\n\", bb->index);\n+\n+  visited_bb.push_back (bb);\n+\n+  if (BB_END (bb) == NULL_RTX)\n+    log_return_void (\"[bb %d] BB_END is null\\n\", bb->index);\n+\n+  if (start_insn == NULL_RTX)\n+    log_return_void (\"[bb %d] start_insn is null\\n\", bb->index);\n+\n+  rtx end_insn = NEXT_INSN (BB_END (bb));\n+  if (end_insn == NULL_RTX)\n+    log_return_void (\"[bb %d] end_insn is null\\n\", bb->index);\n+\n+  for (rtx i = NEXT_INSN (start_insn); i != end_insn; i = NEXT_INSN (i))\n+    {\n+      if (INSN_P (i))\n+\t{\n+\t  if (NONDEBUG_INSN_P (i)\n+\t      && (reg_overlap_mentioned_p (reg, PATTERN (i))\n+\t\t  || (CALL_P (i) && find_reg_fusage (i, USE, reg))))\n+\t    {\n+\t      log_msg (\"found use in [bb %d] at insn:\\n\", bb->index);\n+\t      log_insn (i);\n+\t      log_msg (\"\\n\");\n+\t      count += 1;\n+\t    }\n+\n+\t  // Stop following this BB if the reg is set or dies along the way.\n+\t  if (reg_set_p (reg, i) || reg_dead_after_insn (reg, i))\n+\t    return;\n+\t}\n+\n+      if (abort_at_insn != NULL_RTX && abort_at_insn == i)\n+\treturn;\n+    }\n+\n+  for (edge_iterator ei = ei_start (bb->succs); !ei_end_p (ei); ei_next (&ei))\n+    {\n+      basic_block succ_bb = ei_edge (ei)->dest;\n+      trace_reg_uses_1 (reg, BB_HEAD (succ_bb), succ_bb, count, visited_bb,\n+\t\t\tabort_at_insn);\n+    }\n+}\n+\n+// Trace uses of the specified reg in all basic blocks that are reachable from\n+// the specified insn.  If 'abort_at_insn' is not null, abort the trace at\n+// that insn.  If the insn 'abort_at_insn' uses the specified reg, it is also\n+// counted.\n+static int\n+trace_reg_uses (rtx reg, rtx start_insn, rtx abort_at_insn)\n+{\n+  log_msg (\"\\ntrace_reg_uses\\nreg = \");\n+  log_rtx (reg);\n+  log_msg (\"\\nstart_insn = \");\n+  log_insn (start_insn);\n+\n+  int count = 0;\n+  std::vector<basic_block> visited_bb;\n+  visited_bb.reserve (32);\n+\n+  trace_reg_uses_1 (reg, start_insn, BLOCK_FOR_INSN (start_insn),\n+\t\t    count, visited_bb, abort_at_insn);\n+  return count;\n+}\n+\n+// FIXME: Remove dependency on SH predicate function somehow.\n+extern int t_reg_operand (rtx, machine_mode);\n+extern int negt_reg_operand (rtx, machine_mode);\n+\n+// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n+// RTL pass class\n+\n+class sh_treg_combine : public rtl_opt_pass\n+{\n+public:\n+  sh_treg_combine (gcc::context* ctx, bool split_insns, const char* name);\n+  virtual ~sh_treg_combine (void);\n+  virtual bool gate (void);\n+  virtual unsigned int execute (void);\n+\n+private:\n+  // Type of ccreg store that is supported.\n+  enum cstore_type_t\n+  {\n+    cstore_normal = 0,\n+    cstore_inverted = 1,\n+    cstore_unknown = -1\n+  };\n+\n+  // Type of branch condition that is supported.\n+  enum branch_condition_type_t\n+  {\n+    branch_if_true = 1,\n+    branch_if_false = 0,\n+    unknown_branch_condition = -1\n+  };\n+\n+  // For each basic block there can be a trace entry which consists of an\n+  // insn that sets the ccreg (usually a comparison) and a ccreg store.\n+  struct bb_entry\n+  {\n+    basic_block bb;\n+    set_of_reg setcc;\n+    set_of_reg cstore;\n+    cstore_type_t cstore_type;\n+    std::vector<set_of_reg> cstore_reg_reg_copies;\n+\n+    bb_entry (basic_block b)\n+    : bb (b), setcc (), cstore (), cstore_type (cstore_unknown) { }\n+\n+    rtx comparison_rtx (void) const { return setcc.set_src (); }\n+  };\n+\n+  // A ccreg trace for a conditional branch.\n+  struct cbranch_trace\n+  {\n+    rtx cbranch_insn;\n+    branch_condition_type_t cbranch_type;\n+\n+    // The comparison against zero right before the conditional branch.\n+    set_of_reg setcc;\n+\n+    // All BBs that are related to the cbranch.  The last BB in the list is\n+    // the BB of the cbranch itself and might be empty.\n+    std::list<bb_entry> bb_entries;\n+\n+    cbranch_trace (rtx insn)\n+    : cbranch_insn (insn),\n+      cbranch_type (unknown_branch_condition),\n+      setcc ()\n+    {\n+    }\n+\n+    basic_block bb (void) const { return BLOCK_FOR_INSN (cbranch_insn); }\n+\n+    rtx\n+    branch_condition_rtx (void) const\n+    {\n+      rtx x = pc_set (cbranch_insn);\n+      return x == NULL_RTX ? NULL_RTX : XEXP (XEXP (x, 1), 0);\n+    }\n+\n+    bool\n+    can_invert_condition (void) const\n+    {\n+      // The branch condition can be inverted safely only if the condition\n+      // reg is dead after the cbranch.\n+      return reg_dead_after_insn (XEXP (branch_condition_rtx (), 0),\n+\t\t\t\t  cbranch_insn);\n+    }\n+  };\n+\n+  static const pass_data default_pass_data;\n+\n+  // Tells whether modified or newly added insns are to be split at the end\n+  // of the pass.\n+  const bool m_split_insns;\n+\n+  // rtx of the ccreg that is obtained from the target.\n+  rtx m_ccreg;\n+\n+  // Newly added or modified insns.\n+  std::vector<rtx> m_touched_insns;\n+\n+  // Given an rtx determine whether it's a comparison with a constant zero.\n+  static bool is_cmp_eq_zero (const_rtx i);\n+\n+  // Update the stored mode of the ccreg from the given branch condition rtx.\n+  void update_ccreg_mode (const_rtx cond);\n+\n+  // Given an rtx, figure out the branch condition, assuming that it is\n+  // in canonical form:\n+  //   (ne (reg) (const_int 0))\n+  //   (eq (reg) (const_int 0))\n+  branch_condition_type_t branch_condition_type (const_rtx cond) const;\n+\n+  // Return true if the specified rtx is either a normal ccreg or\n+  // a negated form of the ccreg.\n+  bool is_normal_ccreg (const_rtx x) const;\n+  bool is_inverted_ccreg (const_rtx x) const;\n+\n+  // Given a reg rtx and a start insn rtx, try to find the insn in the same\n+  // basic block that sets the specified reg.\n+  // Return how the search ended and the insn where it stopped or NULL_RTX.\n+  enum record_return_t\n+  {\n+    set_found,\n+    set_not_found,\n+    other_set_found\n+  };\n+  record_return_t record_set_of_reg (rtx reg, rtx start_insn, bb_entry& e);\n+\n+  // Tells whether the cbranch insn of the specified bb_entry can be removed\n+  // safely without triggering any side effects.\n+  bool can_remove_cstore (const bb_entry& e,\n+\t\t\t  const cbranch_trace& trace) const;\n+\n+  // Tells whether the setcc insn of the specified bb_entry can be removed\n+  // safely without triggering any side effects.\n+  bool can_remove_comparison (const bb_entry& e,\n+\t\t\t      const cbranch_trace& trace) const;\n+\n+  // Tells whether the two specified comparison rtx can be combined into a\n+  // single comparison.\n+  bool can_combine_comparisons (const_rtx x, const_rtx y) const;\n+\n+  // Tells whether the ccreg usage can be extended from the bb_entry on until\n+  // the final cbranch of the trace.\n+  bool can_extend_ccreg_usage (const bb_entry& e,\n+\t\t\t       const cbranch_trace& trace) const;\n+\n+  // Create an insn rtx that is a negating reg move (not operation).\n+  rtx make_not_reg_insn (rtx dst_reg, rtx src_reg) const;\n+\n+  // Create an insn rtx that inverts the ccreg.\n+  rtx make_inv_ccreg_insn (void) const;\n+\n+  // Adds the specified insn to the set of modified or newly added insns that\n+  // might need splitting at the end of the pass.\n+  rtx touched_insn (rtx i);\n+\n+  // Try to invert the branch condition of the specified trace.\n+  bool try_invert_branch_condition (cbranch_trace& trace);\n+\n+  // Try to optimize a cbranch trace by combining comparisons in BBs and\n+  // eliminate the cstores.\n+  bool try_combine_comparisons (cbranch_trace& trace,\n+\t\t\t\tint cstore_count, int inv_cstore_count,\n+\t\t\t\tcstore_type_t dominating_cstore);\n+\n+  // Try to optimize a cbranch trace by eliminating the cstores in BBs only.\n+  bool try_eliminate_cstores (cbranch_trace& trace,\n+\t\t\t      int cstore_count, int inv_cstore_count,\n+\t\t\t      cstore_type_t dominating_cstore);\n+\n+  // Given a branch insn, try to optimize its branch condition.\n+  // If any insns are modified or added they are added to 'm_touched_insns'.\n+  void try_optimize_cbranch (rtx i);\n+};\n+\n+\n+const pass_data sh_treg_combine::default_pass_data =\n+{\n+  RTL_PASS,\t\t// type\n+  \"\",\t\t\t// name (overwritten by the constructor)\n+  OPTGROUP_NONE,\t// optinfo_flags\n+  true,\t\t\t// has_gate\n+  true,\t\t\t// has_execute\n+  TV_OPTIMIZE,\t\t// tv_id\n+  0,\t\t\t// properties_required\n+  0,\t\t\t// properties_provided\n+  0,\t\t\t// properties_destroyed\n+  0,\t\t\t// todo_flags_start\n+  TODO_df_finish | TODO_df_verify\t// todo_flags_finish\n+  | TODO_verify_rtl_sharing\n+};\n+\n+sh_treg_combine::sh_treg_combine (gcc::context* ctx, bool split_insns,\n+\t\t\t\t  const char* name)\n+: rtl_opt_pass (default_pass_data, ctx),\n+  m_split_insns (split_insns),\n+  m_ccreg (NULL_RTX)\n+{\n+  // Overwrite default name in pass_data base class. \n+  this->name = name;\n+}\n+\n+sh_treg_combine::~sh_treg_combine (void)\n+{\n+}\n+\n+void sh_treg_combine::update_ccreg_mode (const_rtx cond)\n+{\n+  if (REG_P (XEXP (cond, 0)) && REGNO (XEXP (cond, 0)) != REGNO (m_ccreg))\n+    return;\n+\n+  machine_mode m = GET_MODE (XEXP (cond, 0));\n+  if (m == GET_MODE (m_ccreg))\n+    return;\n+\n+  PUT_MODE (m_ccreg, m);\n+  log_msg (\"updated ccreg mode: \");\n+  log_rtx (m_ccreg);\n+  log_msg (\"\\n\");\n+}\n+\n+bool\n+sh_treg_combine::is_cmp_eq_zero (const_rtx i)\n+{\n+  return i != NULL_RTX && GET_CODE (i) == EQ\n+\t && REG_P (XEXP (i, 0)) && XEXP (i, 1) == const0_rtx;\n+}\n+\n+sh_treg_combine::branch_condition_type_t\n+sh_treg_combine::branch_condition_type (const_rtx cond) const\n+{\n+  if (cond == NULL_RTX)\n+    return unknown_branch_condition;\n+\n+  if (GET_CODE (cond) == NE\n+      && REG_P (XEXP (cond, 0)) && REGNO (XEXP (cond, 0)) == REGNO (m_ccreg)\n+      && XEXP (cond, 1) == const0_rtx)\n+    return branch_if_true;\n+\n+  else if (GET_CODE (cond) == EQ\n+      && REG_P (XEXP (cond, 0)) && REGNO (XEXP (cond, 0)) == REGNO (m_ccreg)\n+      && XEXP (cond, 1) == const0_rtx)\n+    return branch_if_false;\n+\n+  else\n+    return unknown_branch_condition;\n+}\n+\n+bool\n+sh_treg_combine::is_normal_ccreg (const_rtx x) const\n+{\n+  return t_reg_operand (const_cast<rtx> (x), VOIDmode);\n+}\n+\n+bool\n+sh_treg_combine::is_inverted_ccreg (const_rtx x) const\n+{\n+  return negt_reg_operand (const_cast<rtx> (x), VOIDmode);\n+}\n+\n+sh_treg_combine::record_return_t\n+sh_treg_combine::record_set_of_reg (rtx reg, rtx start_insn,\n+\t\t\t\t    bb_entry& new_entry)\n+{\n+  log_msg (\"\\n[bb %d]\\n\", new_entry.bb->index);\n+\n+  if (start_insn == NULL_RTX)\n+    log_return (set_not_found, \"set of reg not found.  empty BB?\\n\");\n+\n+  new_entry.cstore_type = cstore_unknown;\n+\n+  for (rtx i = start_insn; i != NULL_RTX; )\n+    {\n+      new_entry.cstore = find_set_of_reg_bb (reg, i);\n+\n+      if (new_entry.cstore.set_src () == NULL_RTX)\n+\tlog_return (set_not_found, \"set of reg not found (cstore)\\n\");\n+\n+      log_insn (new_entry.cstore.insn);\n+      log_msg (\"\\n\");\n+\n+      if (is_normal_ccreg (new_entry.cstore.set_src ()))\n+\t{\n+\t  log_msg (\"normal condition store\\n\");\n+\t  new_entry.cstore_type = cstore_normal;\n+\t}\n+      else if (is_inverted_ccreg (new_entry.cstore.set_src ()))\n+\t{\n+\t  log_msg (\"inverted condition store\\n\");\n+\t  new_entry.cstore_type = cstore_inverted;\n+\t}\n+      else if (REG_P (new_entry.cstore.set_src ()))\n+\t{\n+\t  // If it's a reg-reg copy follow the copied reg.\n+\t  new_entry.cstore_reg_reg_copies.push_back (new_entry.cstore);\n+\t  reg = new_entry.cstore.set_src ();\n+\t  i = new_entry.cstore.insn;\n+\n+\t  log_msg (\"reg-reg copy.  tracing \");\n+\t  log_rtx (reg);\n+\t  log_msg (\"\\n\");\n+\t  continue;\n+\t}\n+      else\n+\tlog_return (other_set_found, \"not a condition store\\n\");\n+\n+      gcc_assert (new_entry.cstore_type != cstore_unknown);\n+\n+      // Now see how the ccreg was set.\n+      // For now it must be in the same BB.\n+      log_msg (\"tracing ccreg\\n\");\n+      new_entry.setcc =\n+\t  find_set_of_reg_bb (m_ccreg,\n+\t\t\t      prev_nonnote_insn_bb (new_entry.cstore.insn));\n+\n+      // If cstore was found but setcc was not found continue anyway, as\n+      // for some of the optimization types the setcc is irrelevant.\n+      if (new_entry.setcc.set_src () == NULL_RTX)\n+\tlog_return (set_found, \"set of ccreg not found\\n\");\n+\n+      else if (GET_CODE (new_entry.setcc.set_rtx) == SET)\n+\t{\n+\t  // Also allow insns that set the ccreg, but are not true comparison\n+\t  // insns, as long as they are sets and not e.g. clobbers.\n+\t  log_insn (new_entry.setcc.insn);\n+\t  log_msg (\"\\n\");\n+\t  return set_found;\n+\t}\n+      else\n+\t// If cstore was found but setcc was not found continue anyway, as\n+\t// for some of the optimization types the setcc is irrelevant.\n+ \tlog_return (set_found, \"unknown set of ccreg\\n\");\n+    }\n+\n+  log_return (set_not_found, \"set of reg not found\\n\");\n+}\n+\n+bool\n+sh_treg_combine::can_remove_cstore (const bb_entry& e,\n+\t\t\t\t    const cbranch_trace& trace) const\n+{\n+  if (volatile_insn_p (PATTERN (e.cstore.insn)))\n+    {\n+      log_msg (\"can't remove insn\\n\");\n+      log_insn (e.cstore.insn);\n+      log_return (false, \"\\nbecause it's volatile\\n\");\n+    }\n+\n+  // On SH there are parallel patterns which store the ccreg multiple times.\n+  // In this case it's not safe.\n+  rtx cstore_pat = PATTERN (e.cstore.insn);\n+  if (GET_CODE (cstore_pat) == PARALLEL)\n+    for (int i = 0; i < XVECLEN (cstore_pat, 0); ++i)\n+      {\n+\trtx x = XVECEXP (cstore_pat, 0, i);\n+\n+\t// It's the cstore set that we're referring to, ignore that one.\n+\tif (x != e.cstore.set_rtx\n+\t    && GET_CODE (x) == SET && reg_referenced_p (m_ccreg, x))\n+\t  {\n+\t    log_msg (\"can't remove insn\\n\");\n+\t    log_insn (e.cstore.insn);\n+\t    log_return (false, \"\\nbecause it's a multiple ccreg store\\n\");\n+\t  }\n+      }\n+\n+  // If the cstore sets the ccreg (e.g. negc) and the ccreg is used afterwards\n+  // it's not safe.\n+  if (modified_in_p (m_ccreg, e.cstore.insn)\n+      && !(reg_dead_after_insn (m_ccreg, e.cstore.insn)\n+\t   || reg_unused_after_insn (m_ccreg, e.cstore.insn)))\n+    {\n+      log_msg (\"can't remove insn\\n\");\n+      log_insn (e.cstore.insn);\n+      log_return (false, \"\\nbecause it sets the ccreg\\n\");\n+    }\n+\n+  // If the cstore destination reg is copied around check the reg-reg\n+  // copies.  At every reg-reg copy the copied reg must be dead and there\n+  // must not be a usage of the copied regs between the reg-reg copies.\n+  // Otherwise we assume that the result of the cstore is used in some\n+  // other way.\n+  rtx prev_insn = e.cstore.insn;\n+  for (std::vector<set_of_reg>::const_reverse_iterator i =\n+\t   e.cstore_reg_reg_copies.rbegin ();\n+       i != e.cstore_reg_reg_copies.rend (); ++i)\n+    {\n+      if (!reg_dead_after_insn (i->set_src (), i->insn))\n+\t{\n+\t  log_msg (\"can't remove insn\\n\");\n+\t  log_insn (i->insn);\n+\t  log_return (false, \"\\nbecause source of reg-reg copy doesn't die\\n\");\n+\t}\n+\n+     if (reg_used_between_p (i->set_src (), prev_insn, i->insn))\n+\t{\n+\t  log_msg (\"can't remove insn\\n\");\n+\t  log_insn (i->insn);\n+\t  log_return (false, \"\\nbecause reg %d is otherwise used\\n\",\n+\t\t\t     REGNO (i->set_src ()));\n+\t}\n+\n+      prev_insn = i->insn;\n+    }\n+\n+  // The cstore_dst reg must die after the test before the cbranch, otherwise\n+  // it's not safe to remove the cstore.\n+  // If the cstore destination reg is copied around check the effective\n+  // destination reg of the cstore.  The reg-reg copies are recorded in\n+  // reverse order, i.e. the most recent reg-reg copy in the insn list\n+  // comes first.\n+  rtx cstore_dst = e.cstore_reg_reg_copies.empty ()\n+\t\t   ? e.cstore.set_dst ()\n+\t\t   : e.cstore_reg_reg_copies.front ().set_dst ();\n+\n+  if (!reg_dead_after_insn (cstore_dst, trace.setcc.insn))\n+    {\n+      log_msg (\"can't remove insn\\n\");\n+      log_insn (e.cstore.insn);\n+      log_return (false, \"\\nbecause its effective target reg %d doesn't die \"\n+\t\t\t \"after trace.setcc.insn\\n\", REGNO (cstore_dst));\n+    }\n+\n+  // Also check that the cstore_dst reg is not used in other reachable code\n+  // paths before it dies.\n+  // Count the uses of the effective cstore_dst reg (i.e. the last known reg\n+  // that holds the cstore value after reg-reg copies) in all BBs that can be\n+  // reached from bb_entry's BB including the BB of the cstore insn.\n+  // If we get more than 1 uses we assume that it's used somewhere else and is\n+  // not safe to be removed.\n+  int cstore_dst_use_count = trace_reg_uses (cstore_dst, e.cstore.insn,\n+\t\t\t\t\t     trace.setcc.insn);\n+  if (cstore_dst_use_count > 1)\n+    {\n+      log_msg (\"can't remove insn\\n\");\n+      log_insn (e.cstore.insn);\n+      log_return (false, \"\\nbecause its effective target reg %d is used \"\n+\t\t\t \"in %d other places\\n\", REGNO (cstore_dst),\n+\t\t\t  cstore_dst_use_count - 1);\n+    }\n+\n+  return true;\n+}\n+\n+bool\n+sh_treg_combine::can_remove_comparison (const bb_entry& e,\n+\t\t\t\t\tconst cbranch_trace&/* trace*/) const\n+{\n+  // If the ccreg is used otherwise between the comparison and the cstore,\n+  // it's not safe.\n+  if (reg_used_between_p (m_ccreg, e.setcc.insn, e.cstore.insn))\n+    {\n+      log_msg (\"can't remove insn\\n\");\n+      log_insn (e.setcc.insn);\n+      log_return (false, \"\\nbecause the ccreg is used otherwise\\n\");\n+    }\n+\n+  if (!reg_dead_after_insn (m_ccreg, e.cstore.insn)\n+      && !reg_unused_after_insn (m_ccreg, e.cstore.insn))\n+    {\n+      log_msg (\"can't remove insn\\n\");\n+      log_insn (e.cstore.insn);\n+      log_return (false, \"\\nbecause ccreg is not dead or unused afterwards\\n\");\n+    }\n+\n+  // On SH there are also multiple set patterns that can be used for\n+  // comparisons, such as \"shll\".  It's not safe to remove those.\n+  if (multiple_sets (e.setcc.insn))\n+    {\n+      log_msg (\"can't remove insn\\n\");\n+      log_insn (e.cstore.insn);\n+      log_return (false, \"\\nbecause it's a multiple set\\n\");\n+    }\n+\n+  return true;\n+}\n+\n+rtx\n+sh_treg_combine::make_not_reg_insn (rtx dst_reg, rtx src_reg) const\n+{\n+  // This will to go through expanders and may output multiple insns\n+  // for multi-word regs.\n+  start_sequence ();\n+  expand_simple_unop (GET_MODE (dst_reg), NOT, src_reg, dst_reg, 0);\n+  rtx i = get_insns ();\n+  end_sequence ();\n+  return i;\n+}\n+\n+rtx\n+sh_treg_combine::make_inv_ccreg_insn (void) const\n+{\n+  start_sequence ();\n+  rtx i = emit_insn (gen_rtx_SET (VOIDmode, m_ccreg,\n+\t\t\t\t  gen_rtx_fmt_ee (XOR, GET_MODE (m_ccreg),\n+\t\t\t\t\t\t  m_ccreg, const1_rtx)));\n+  end_sequence ();\n+  return i;\n+}\n+\n+rtx\n+sh_treg_combine::touched_insn (rtx i)\n+{\n+  m_touched_insns.push_back (i);\n+  return i;\n+}\n+\n+bool\n+sh_treg_combine::can_combine_comparisons (const_rtx x, const_rtx y) const\n+{\n+  if (GET_CODE (x) != GET_CODE (y))\n+    return false;\n+\n+  rtx x_op0 = XEXP (x, 0);\n+  rtx x_op1 = XEXP (x, 1);\n+\n+  rtx y_op0 = XEXP (y, 0);\n+  rtx y_op1 = XEXP (y, 1);\n+\n+  if (!REG_P (x_op0) || !REG_P (y_op0))\n+    return false;\n+\n+  if (GET_MODE (x_op0) != GET_MODE (y_op0))\n+    return false;\n+\n+  // rtx_equal_p also compares the reg numbers which we do not care about\n+  // here, as long as both are regs and the modes are the same.\n+  if (REG_P (x_op1))\n+    return REG_P (y_op1) && GET_MODE (x_op1) == GET_MODE (y_op1);\n+\n+  return rtx_equal_p (x_op1, y_op1);\n+}\n+\n+bool\n+sh_treg_combine::can_extend_ccreg_usage (const bb_entry& e,\n+\t\t\t\t\t const cbranch_trace& trace) const\n+{\n+  // Check if the ccreg is not modified by other insins in the BB path until\n+  // the final cbranch of the trace.\n+  // Start checking after the cstore that follows the setcc, assuming that\n+  // the cstore will be removed.\n+\n+  // The assumption here is that the specified bb_entry's BB is a direct\n+  // predecessor of the trace.cbranch_insn's BB.\n+  if (e.bb != trace.bb () && !is_adjacent_bb (e.bb, trace.bb ()))\n+    log_return (false,\n+\t\"can't extend ccreg usage -- [bb %d] and [bb %d] are not adjacent\\n\",\n+\te.bb->index, trace.bb ()->index);\n+\n+  if (e.cstore.empty ())\n+    log_return (false, \"can't extend ccreg usage -- no cstore\\n\");\n+\n+  // The entry's cstore is in the same BB as the final cbranch.\n+  if (e.bb == trace.bb ())\n+    {\n+      if (reg_set_between_p (m_ccreg, e.cstore.insn, trace.setcc.insn))\n+\tlog_return (false,\n+\t    \"can't extend ccreg usage -- it's modified between e.cstore.insn \"\n+\t    \"and trace.setcc.insn\");\n+      else\n+\treturn true;\n+    }\n+\n+  // The entry's cstore and the final cbranch are in different BBs.\n+  if (reg_set_between_p (m_ccreg, e.cstore.insn, NEXT_INSN (BB_END (e.bb))))\n+    log_return (false,\n+\t\"can't extend ccreg usage -- it's modified in [bb %d]\", e.bb->index);\n+\n+  if (reg_set_between_p (m_ccreg, PREV_INSN (BB_HEAD (trace.bb ())),\n+\t\t\t trace.setcc.insn))\n+    log_return (false,\n+\t\"can't extend ccreg usage -- it's modified in [bb %d]\",\n+\ttrace.bb ()->index);\n+\n+  return true;\n+}\n+\n+bool\n+sh_treg_combine::try_invert_branch_condition (cbranch_trace& trace)\n+{\n+  log_msg (\"inverting branch condition\\n\");\n+\n+  if (!invert_jump_1 (trace.cbranch_insn, JUMP_LABEL (trace.cbranch_insn)))\n+    log_return (false, \"invert_jump_1 failed\\n\");\n+\n+  if (verify_changes (num_validated_changes ()))\n+    confirm_change_group ();\n+  else\n+    log_return (false, \"verify_changed failed\\n\");\n+\n+  touched_insn (trace.cbranch_insn);\n+  return true;\n+}\n+\n+bool\n+sh_treg_combine::try_combine_comparisons (cbranch_trace& trace,\n+\t\t\t\t\t  int cstore_count,\n+\t\t\t\t\t  int inv_cstore_count,\n+\t\t\t\t\t  cstore_type_t dominating_cstore)\n+{\n+  log_msg (\"\\ntry_combine_comparisons\\n\");\n+\n+  // This function will always try to create new pseudos.\n+  if (!can_create_pseudo_p ())\n+    log_return (false, \"can't create pseudos\\n\");\n+\n+  // Check that all ccset insns are comparisons and all comparison types in\n+  // all BBs are the same and could be combined into one single comparison.\n+  rtx comp = NULL_RTX;\n+  rtx comp_insn = NULL_RTX;\n+\n+  for (std::list<bb_entry>::const_iterator i = trace.bb_entries.begin ();\n+       i != trace.bb_entries.end (); ++i)\n+    {\n+      int i_empty_count = i->setcc.empty () + i->cstore.empty ();\n+\n+      // A completly empty entry is OK (could be the BB of the cbranch).\n+      if (i_empty_count == 2)\n+\tcontinue;\n+\n+      // Otherwise we need both, the setcc and the cstore.\n+      if (i_empty_count != 0)\n+\tlog_return (false, \"bb entry is not a setcc cstore pair\\n\");\n+\n+      rtx other_comp = i->comparison_rtx ();\n+\n+      if (!COMPARISON_P (other_comp))\n+\t{\n+\t  log_msg (\"setcc is not a comparison:\\n\");\n+\t  log_rtx (other_comp);\n+\t  log_return (false, \"\\n\");\n+\t}\n+\n+      if (comp_insn == NULL_RTX)\n+\t{\n+\t  comp = other_comp;\n+\t  comp_insn = i->setcc.insn;\n+\t}\n+      else if (!can_combine_comparisons (comp, other_comp))\n+\treturn false;\n+\n+      // The goal here is to eliminate all cstores and comparisons in the BBs.\n+      // Thus check if every cstore can actually be removed safely.\n+      if (!can_remove_cstore (*i, trace) || !can_remove_comparison (*i, trace))\n+\treturn false;\n+    }\n+\n+  // FIXME: The first operand of the comparison must be a simple reg.\n+  // This effectively prohibits combining div0s comparisons such as\n+  //    (lt:SI (xor:SI (reg:SI) (reg:SI)))\n+  if (!REG_P (XEXP (comp, 0)))\n+    {\n+      log_msg (\"comparison operand 0\\n\");\n+      log_rtx (XEXP (comp, 0));\n+      log_return (false, \"\\nis not a reg\\n\");\n+    }\n+\n+  rtx comp_op0 = gen_reg_rtx (GET_MODE (XEXP (comp, 0)));\n+  rtx comp_op1 = REG_P (XEXP (comp, 1))\n+\t\t ? gen_reg_rtx (GET_MODE (XEXP (comp, 1)))\n+\t\t : XEXP (comp, 1);\n+\n+  // If there are both, inverting and non-inverting cstores, they can only\n+  // be eliminated if the comparison can be inverted.  We assume that the\n+  // comparison insns that we find are already minimal and canonicalized.\n+  // There is one special case though, where an integer comparison\n+  //     (eq (reg) (const_int 0))\n+  // can be inverted with a sequence\n+  //     (eq (not (reg)) (const_int 0))\n+  if (inv_cstore_count != 0 && cstore_count != 0)\n+    {\n+      if (make_not_reg_insn (comp_op0, comp_op0) == NULL_RTX)\n+\tlog_return (false, \"make_not_reg_insn failed.\\n\");\n+\n+      for (std::list<bb_entry>::const_iterator i = trace.bb_entries.begin ();\n+\t   i != trace.bb_entries.end (); ++i)\n+\t{\n+\t  if (i->setcc.empty () || i->cstore.empty ())\n+\t    continue;\n+\n+\t  if (i->cstore_type != dominating_cstore\n+\t      && !is_cmp_eq_zero (i->comparison_rtx ()))\n+\t    {\n+\t      log_msg (\"can't invert comparison in insn\\n\");\n+\t      log_insn (i->setcc.insn);\n+\t      log_return (false,\n+\t\t\"\\nbecause it's not a (eq (reg) (const_int 0))\\n\");\n+\t    }\n+\t}\n+    }\n+\n+  if (dominating_cstore == cstore_normal\n+      && !try_invert_branch_condition (trace))\n+    return false;\n+\n+  // Replace the test insn before the cbranch with the common comparison.\n+  // Instead of creating a new insn from scratch we copy the common comparison\n+  // pattern.  This simplifies handling parallel comparison patterns, such as\n+  // FP comparisons on SH, which have an extra use on FPSCR.\n+  log_msg (\"installing common comparison in [bb %d]\\n\", trace.bb ()->index);\n+\n+  rtx common_comp_pat = copy_rtx (PATTERN (comp_insn));\n+  rtx common_comp = const_cast<rtx> (set_of (m_ccreg, common_comp_pat));\n+\n+  gcc_assert (common_comp != NULL_RTX);\n+\n+  XEXP (XEXP (common_comp, 1), 0) = comp_op0;\n+  XEXP (XEXP (common_comp, 1), 1) = comp_op1;\n+\n+  log_rtx (common_comp_pat);\n+  log_msg (\"\\n\");\n+\n+  rtx common_comp_insn = touched_insn (emit_insn_after (common_comp_pat,\n+\t\t\t\t\t\t\ttrace.setcc.insn));\n+\n+  if (REG_P (comp_op0))\n+    add_reg_note (common_comp_insn, REG_DEAD, copy_rtx (comp_op0));\n+  if (REG_P (comp_op1))\n+    add_reg_note (common_comp_insn, REG_DEAD, copy_rtx (comp_op1));\n+\n+  delete_insn (trace.setcc.insn);\n+\n+  // Replace comparison and cstore insns with reg-reg moves in all BBs.\n+  for (std::list<bb_entry>::const_iterator i = trace.bb_entries.begin ();\n+       i != trace.bb_entries.end (); ++i)\n+    {\n+      if (i->setcc.empty () || i->cstore.empty ())\n+\tcontinue;\n+\n+      rtx i_comp_op0 = XEXP (i->comparison_rtx (), 0);\n+      rtx i_comp_op1 = XEXP (i->comparison_rtx (), 1);\n+\n+      if (i->cstore_type == dominating_cstore)\n+\t{\n+\t  log_msg (\"replacing comparison and cstore with reg move \"\n+\t\t   \"in [bb %d]\\n\", i->bb->index);\n+\n+\t  rtx new_i = touched_insn (\n+\t\temit_insn_after (gen_move_insn (comp_op0, i_comp_op0),\n+\t\t\t\t i->setcc.insn));\n+\n+\t  if (REG_P (i_comp_op0)\n+\t      && reg_dead_after_insn (i_comp_op0, i->setcc.insn))\n+\t    add_reg_note (new_i, REG_DEAD, copy_rtx (i_comp_op0));\n+\n+\t  // If the second operand is a reg, have to emit a move insn.\n+\t  // Otherwise assume it's a const_int and just reference it.\n+\t  if (REG_P (comp_op1))\n+\t    {\n+\t      new_i = touched_insn (\n+\t\t  emit_insn_after (gen_move_insn (comp_op1, i_comp_op1),\n+\t\t\t\t   i->setcc.insn));\n+\n+\t      if (reg_dead_after_insn (i_comp_op1, i->setcc.insn))\n+\t\tadd_reg_note (new_i, REG_DEAD, copy_rtx (i_comp_op1));\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  log_msg (\"replacing comparison and cstore with inverting reg move \"\n+\t\t   \"in [bb %d]\\n\", i->bb->index);\n+\n+\t  rtx new_i = make_not_reg_insn (comp_op0, i_comp_op0);\n+\t  if (REG_P (i_comp_op0)\n+\t      && reg_dead_after_insn (i_comp_op0, i->setcc.insn))\n+\t    add_reg_note (new_i, REG_DEAD, copy_rtx (i_comp_op0));\n+\n+\t  touched_insn (emit_insn_after (new_i, i->setcc.insn));\n+\t}\n+\n+      delete_insn (i->cstore.insn);\n+      delete_insn (i->setcc.insn);\n+    }\n+\n+  return true;\n+}\n+\n+bool\n+sh_treg_combine::try_eliminate_cstores (cbranch_trace& trace,\n+\t\t\t\t\tint cstore_count, int inv_cstore_count,\n+\t\t\t\t\tcstore_type_t dominating_cstore)\n+{\n+  log_msg (\"\\ntry_eliminate_cstores\\n\");\n+\n+  for (std::list<bb_entry>::const_iterator i = trace.bb_entries.begin ();\n+       i != trace.bb_entries.end (); ++i)\n+    {\n+      // A completly empty entry is OK (could be the BB of the cbranch).\n+      if (i->setcc.empty () && i->cstore.empty ())\n+\tcontinue;\n+\n+      // We're going to eliminate cstores, but for that they have to be\n+      // there.  We don't care about the setcc in this case.\n+      if (i->cstore.empty ())\n+\tlog_return (false, \"bb entry cstore empty -- aborting\\n\");\n+\n+      // The goal here is to eliminate all cstores in the BBs and extend the\n+      // ccreg usage.\n+      if (!can_extend_ccreg_usage (*i, trace))\n+\treturn false;\n+\n+      // If the cstore can't be removed we can keep it around as long as\n+      // it doesn't modify the ccreg.\n+      if (!can_remove_cstore (*i, trace)\n+\t  && modified_in_p (m_ccreg, i->cstore.insn))\n+\tlog_return (false, \"cstore sets ccreg -- aborting\\n\");\n+    }\n+\n+  // If there are both, inverting and non-inverting cstores, we'll have to\n+  // invert the ccreg as a replacement for one of them.\n+  if (cstore_count != 0 && inv_cstore_count != 0)\n+    {\n+      rtx i = make_inv_ccreg_insn ();\n+      if (recog_memoized (i) < 0)\n+\t{\n+\t  log_msg (\"failed to match ccreg inversion insn:\\n\");\n+\t  log_rtx (PATTERN (i));\n+\t  log_return (false, \"\\naborting\\n\");\n+\t}\n+    }\n+\n+  if (dominating_cstore == cstore_normal\n+      && !try_invert_branch_condition (trace))\n+    return false;\n+\n+  // Eliminate cstores in all BBs.\n+  for (std::list<bb_entry>::const_iterator i = trace.bb_entries.begin ();\n+       i != trace.bb_entries.end (); ++i)\n+    {\n+      if (i->cstore.empty ())\n+\tcontinue;\n+\n+      if (i->cstore_type == dominating_cstore)\n+\tlog_msg (\"removing cstore in [bb %d]\\n\", i->bb->index);\n+      else\n+\t{\n+\t  log_msg (\"replacing cstore with ccreg inversion in [bb %d]\\n\",\n+\t\t   i->bb->index);\n+\n+\t  touched_insn (\n+\t    emit_insn_after (make_inv_ccreg_insn (), i->cstore.insn));\n+\t}\n+\n+      if (can_remove_cstore (*i, trace))\n+\tdelete_insn (i->cstore.insn);\n+    }\n+\n+  log_msg (\"removing test insn before cbranch\\n\");\n+  delete_insn (trace.setcc.insn);\n+  return true;\n+}\n+\n+void\n+sh_treg_combine::try_optimize_cbranch (rtx insn)\n+{\n+  cbranch_trace trace (insn);\n+\n+  log_msg (\"\\n\\n--------------------------------------\\n\");\n+  log_msg (\"found cbranch insn in [bb %d]:\\n\", trace.bb ()->index);\n+  log_insn (insn);\n+\n+  trace.cbranch_type = branch_condition_type (trace.branch_condition_rtx ());\n+\n+  if (trace.cbranch_type == branch_if_true)\n+    log_msg (\"condition: branch if true\\n\");\n+  else if (trace.cbranch_type == branch_if_false)\n+    log_msg (\"condition: branch if false\\n\");\n+  else\n+    {\n+      log_msg (\"unknown branch condition\\n\");\n+      log_rtx (trace.branch_condition_rtx ());\n+      log_return_void (\"\\n\");\n+    }\n+\n+  update_ccreg_mode (trace.branch_condition_rtx ());\n+\n+  // Scan the insns backwards for an insn that sets the ccreg by testing a\n+  // reg against zero like\n+  //   (set (reg ccreg) (eq (reg) (const_int 0)))\n+  // The testing insn could also be outside of the current basic block, but\n+  // for now we limit the search to the current basic block.\n+  trace.setcc = find_set_of_reg_bb (m_ccreg, prev_nonnote_insn_bb (insn));\n+\n+  if (!is_cmp_eq_zero (trace.setcc.set_src ()))\n+    log_return_void (\"could not find set of ccreg in current BB\\n\");\n+\n+  rtx trace_reg = XEXP (trace.setcc.set_src (), 0);\n+\n+  log_msg (\"set of ccreg:\\n\");\n+  log_insn (trace.setcc.insn);\n+\n+  // See if we can remove the trace.setcc insn safely.\n+  if (reg_used_between_p (m_ccreg, trace.setcc.insn, trace.cbranch_insn))\n+    log_return_void (\"ccreg used between testing insn and branch insn\\n\");\n+\n+  if (volatile_insn_p (PATTERN (trace.setcc.insn)))\n+    {\n+      log_msg (\"can't remove insn\\n\");\n+      log_insn (trace.setcc.insn);\n+      log_return_void (\"\\nbecause it's volatile\\n\");\n+    }\n+\n+  // Now that we have an insn which tests some reg and sets the condition\n+  // reg before the conditional branch, try to figure out how that tested\n+  // reg was formed, i.e. find all the insns that set the tested reg in\n+  // some way.\n+  // The tested reg might be set in multiple basic blocks so we need to\n+  // check all basic blocks which can reach this current basic block.\n+  // If the set of reg is an inverting or non-inverting store of the condition\n+  // register, check how the ccreg value was obtained.\n+  log_msg (\"\\ntracing \");\n+  log_rtx (trace_reg);\n+  log_msg (\"\\n\");\n+\n+\n+  // First check the basic block where the conditional branch is in.\n+  // If we find it here there's no point in checking other BBs.\n+  trace.bb_entries.push_front (bb_entry (trace.bb ()));\n+\n+  record_return_t res =\n+      record_set_of_reg (trace_reg, prev_nonnote_insn_bb (trace.setcc.insn),\n+\t\t\t trace.bb_entries.front ());\n+\n+  if (res == other_set_found)\n+    log_return_void (\"other set found - aborting trace\\n\");\n+  else if (res == set_not_found)\n+    {\n+      // It seems the initial search in the BB of the conditional branch\n+      // didn't find anything.  Now look in all predecessor BBs.\n+      for (edge_iterator ei = ei_start (trace.bb ()->preds);\n+\t   !ei_end_p (ei); ei_next (&ei))\n+\t{\n+\t  edge e = ei_edge (ei);\n+\t  trace.bb_entries.push_front (bb_entry (e->src));\n+\n+\t  res = record_set_of_reg (trace_reg, BB_END (e->src),\n+\t\t\t\t   trace.bb_entries.front ());\n+\t  if (res != set_found)\n+\t    log_return_void (\"set not found - aborting trace\\n\");\n+\t}\n+    }\n+\n+  if (dump_file != NULL)\n+    {\n+      log_msg (\"\\ncbranch trace summary:\\n\");\n+      for (std::list<bb_entry>::const_iterator i = trace.bb_entries.begin ();\n+\t   i != trace.bb_entries.end (); ++i)\n+\t{\n+\t  log_msg (\"\\n[bb %d]\\n\", i->bb->index);\n+\t  if (!i->setcc.empty ())\n+\t    {\n+\t      log_rtx (i->setcc.set_rtx);\n+\t      log_msg (\"\\n\");\n+\t    }\n+\t  if (!i->cstore.empty ())\n+\t    {\n+\t      log_rtx (i->cstore.set_rtx);\n+\t      log_msg (\"\\n\");\n+\t    }\n+\n+\t  for (std::vector<set_of_reg>::const_reverse_iterator j =\n+\t\t   i->cstore_reg_reg_copies.rbegin ();\n+\t       j != i->cstore_reg_reg_copies.rend (); ++j)\n+\t    {\n+\t      log_rtx (j->set_rtx);\n+\t      log_msg (\"\\n\");\n+\t    }\n+\t}\n+\n+      log_rtx (trace.setcc.set_rtx);\n+      log_msg (\"\\n\");\n+      log_rtx (PATTERN (trace.cbranch_insn));\n+      log_msg (\"\\n\");\n+    }\n+\n+  // Check that we don't have any empty BBs.\n+  // Only the BB with the cbranch may be empty.\n+  for (std::list<bb_entry>::const_iterator i = trace.bb_entries.begin ();\n+       i != trace.bb_entries.end (); ++i)\n+    if (i->setcc.empty () && i->cstore.empty () && i->bb != trace.bb ())\n+      log_return_void (\"\\n[bb %d] is empty - aborting.\\n\", i->bb->index);\n+\n+  // Determine the dominating cstore type\n+  // FIXME: Try to take the probabilities of the BBs into account somehow.\n+  int cstore_count = 0;\n+  int inv_cstore_count = 0;\n+\n+  for (std::list<bb_entry>::const_iterator i = trace.bb_entries.begin ();\n+       i != trace.bb_entries.end (); ++i)\n+    {\n+      if (i->cstore_type == cstore_normal)\n+\tcstore_count += 1;\n+      else if (i->cstore_type == cstore_inverted)\n+\tinv_cstore_count += 1;\n+    }\n+\n+  log_msg (\"cstore count = %d  inverted cstore count = %d\\n\",\n+\t   cstore_count, inv_cstore_count);\n+\n+  // This puts a priority on inverting cstores.\n+  cstore_type_t dominating_cstore = inv_cstore_count >= cstore_count\n+\t\t\t\t    ? cstore_inverted\n+\t\t\t\t    : cstore_normal;\n+\n+  if (dominating_cstore == cstore_inverted)\n+      log_msg (\"will try to eliminate inverted cstore\\n\");\n+  else if (dominating_cstore == cstore_normal)\n+    {\n+      log_msg (\"will try to eliminate normal cstore\\n\");\n+      if (!trace.can_invert_condition ())\n+\tlog_return_void (\"branch condition can't be inverted - aborting\\n\");\n+    }\n+  else\n+    gcc_unreachable ();\n+\n+  if (try_combine_comparisons (trace, cstore_count, inv_cstore_count,\n+\t\t\t       dominating_cstore))\n+    return;\n+\n+  try_eliminate_cstores (trace, cstore_count, inv_cstore_count,\n+\t\t\t dominating_cstore);\n+}\n+\n+bool\n+sh_treg_combine::gate (void)\n+{\n+  return optimize > 0;\n+}\n+\n+unsigned int\n+sh_treg_combine::execute (void)\n+{\n+  unsigned int ccr0 = INVALID_REGNUM;\n+  unsigned int ccr1 = INVALID_REGNUM;\n+\n+  if (targetm.fixed_condition_code_regs (&ccr0, &ccr1)\n+      && ccr0 != INVALID_REGNUM)\n+    {\n+      // Initially create a reg rtx with VOIDmode.\n+      // When the first conditional branch is discovered, the mode is changed\n+      // to the mode that is actually used by the target.\n+      m_ccreg = gen_rtx_REG (VOIDmode, ccr0);\n+    }\n+\n+  if (m_ccreg == NULL_RTX)\n+    log_return (0, \"no ccreg.\\n\\n\");\n+\n+  if (STORE_FLAG_VALUE != 1)\n+    log_return (0, \"unsupported STORE_FLAG_VALUE %d\", STORE_FLAG_VALUE);\n+\n+  log_msg (\"ccreg: \");\n+  log_rtx (m_ccreg);\n+  log_msg (\"  STORE_FLAG_VALUE = %d\\n\", STORE_FLAG_VALUE);\n+\n+  // Look for basic blocks that end with a conditional branch and try to\n+  // optimize them.\n+  basic_block bb;\n+  FOR_EACH_BB (bb)\n+    {\n+      rtx i = BB_END (bb);\n+      if (any_condjump_p (i) && onlyjump_p (i))\n+\ttry_optimize_cbranch (i);\n+    }\n+\n+  log_msg (\"\\n\\n\");\n+\n+  // If new insns are created and this pass is executed after all insns\n+  // have been split already, we must split the insns we've changed or added\n+  // ourselves here.\n+  // FIXME: Multi-word operations (which emit multiple insns) are not handled\n+  // properly here, since only one insn will end up in 'm_touched_insns'.\n+  // On SH this is not a problem though.\n+  if (m_split_insns)\n+    for (std::vector<rtx>::const_iterator i = m_touched_insns.begin ();\n+\t i != m_touched_insns.end (); ++i)\n+      {\n+\tlog_msg (\"trying to split insn:\\n\");\n+\tlog_insn (*i);\n+\tlog_msg (\"\\n\");\n+\ttry_split (PATTERN (*i), *i, 0);\n+      }\n+\n+  m_touched_insns.clear ();\n+  log_return (0, \"\\n\\n\");\n+}\n+\n+// - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n+// This allows instantiating the pass somewhere else without having to pull\n+// in a header file.\n+opt_pass*\n+make_pass_sh_treg_combine (gcc::context* ctx, bool split_insns,\n+\t\t\t   const char* name)\n+{\n+  return new sh_treg_combine (ctx, split_insns, name);\n+}"}, {"sha": "081ba3cc6ba1f0971fc6a59b7a28fc16a2e114f9", "filename": "gcc/config/sh/t-sh", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Fconfig%2Fsh%2Ft-sh", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Fconfig%2Fsh%2Ft-sh", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Ft-sh?ref=5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "patch": "@@ -21,6 +21,10 @@ sh-c.o: $(srcdir)/config/sh/sh-c.c \\\n \t$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \\\n \t\t$(srcdir)/config/sh/sh-c.c\n \n+sh_treg_combine.o: $(srcdir)/config/sh/sh_treg_combine.cc \\\n+  $(CONFIG_H) $(SYSTEM_H) $(TREE_H) $(TM_H) $(TM_P_H) coretypes.h\n+\t$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) $<\n+\n DEFAULT_ENDIAN = $(word 1,$(TM_ENDIAN_CONFIG))\n OTHER_ENDIAN = $(word 2,$(TM_ENDIAN_CONFIG))\n "}, {"sha": "07e2d4ccdd49c5433de4775f23e1976bca46fd66", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "patch": "@@ -1,3 +1,10 @@\n+2013-10-12  Oleg Endo  <olegendo@gcc.gnu.org>\n+\n+\tPR target/51244\n+\t* gcc.dg/torture/p51244-21.c: New.\n+\t* gcc.target/sh/pr51244-20.c: New.\n+\t* gcc.target/sh/pr51244-20-sh2a.c: New.\n+\n 2013-10-12  Arnaud Charlet  <charlet@adacore.com>\n \n \t* gnat.dg/specs/linker_section.ads: Update test."}, {"sha": "fbb02db28e0e30e68946e113c267865cd2b96aa1", "filename": "gcc/testsuite/gcc.dg/torture/pr51244-21.c", "status": "added", "additions": 75, "deletions": 0, "changes": 75, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Ftestsuite%2Fgcc.dg%2Ftorture%2Fpr51244-21.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Ftestsuite%2Fgcc.dg%2Ftorture%2Fpr51244-21.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Ftorture%2Fpr51244-21.c?ref=5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "patch": "@@ -0,0 +1,75 @@\n+/* { dg-do run } */\n+#include <assert.h>\n+\n+static inline int\n+blk_oversized_queue (int* q)\n+{\n+  if (q[2])\n+    return q[1] != 0;\n+  return q[0] == 0;\n+}\n+\n+int __attribute__ ((noinline))\n+get_request (int* q, int rw)\n+{\n+  if (blk_oversized_queue (q))\n+    {\n+      if ((rw == 1) || (rw == 0))\n+\treturn -33;\n+\n+      return 0;\n+    }\n+\n+  return -100;\n+}\n+\n+int main (void)\n+{\n+  int x[3]; \n+  int r;\n+\n+  x[0] = 0; x[1] = 1; x[2] = 1;\n+  r = get_request (x, 0);\n+  assert (r == -33);\n+\n+  r = get_request (x, 1);\n+  assert (r == -33);\n+\n+  r = get_request (x, 2);\n+  assert (r == 0);\n+\n+\n+  x[0] = 0; x[1] = 0; x[2] = 1;\n+  r = get_request (x, 0);\n+  assert (r == -100);\n+\n+  r = get_request (x, 1);\n+  assert (r == -100);\n+\n+  r = get_request (x, 2);\n+  assert (r == -100);\n+\n+\n+  x[0] = 0; x[1] = 0; x[2] = 0;\n+  r = get_request (x, 0);\n+  assert (r == -33);\n+\n+  r = get_request (x, 1);\n+  assert (r == -33);\n+\n+  r = get_request (x, 2);\n+  assert (r == 0);\n+\n+\n+  x[0] = 0; x[1] = 0; x[2] = 0;\n+  r = get_request (x, 0);\n+  assert (r == -33);\n+\n+  r = get_request (x, 1);\n+  assert (r == -33);\n+\n+  r = get_request (x, 2);\n+  assert (r == 0);\n+\n+  return 0;\n+}"}, {"sha": "6c8c76b78b0d75fcd0dda88c1bea060a31015ce5", "filename": "gcc/testsuite/gcc.target/sh/pr51244-20-sh2a.c", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Ftestsuite%2Fgcc.target%2Fsh%2Fpr51244-20-sh2a.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Ftestsuite%2Fgcc.target%2Fsh%2Fpr51244-20-sh2a.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fsh%2Fpr51244-20-sh2a.c?ref=5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "patch": "@@ -0,0 +1,14 @@\n+/* Check that the SH specific sh_treg_combine RTL optimization pass works as\n+   expected.  */\n+/* { dg-do compile { target \"sh*-*-*\" } } */\n+/* { dg-options \"-O2\" } */\n+/* { dg-skip-if \"\" { \"sh*-*-*\" } { \"*\" } { \"-m2a*\" } } */\n+/* { dg-final { scan-assembler-times \"tst\" 5 } } */\n+/* { dg-final { scan-assembler-times \"movt\" 0 } } */\n+/* { dg-final { scan-assembler-times \"nott\" 1 } } */\n+/* { dg-final { scan-assembler-times \"cmp/eq\" 2 } } */\n+/* { dg-final { scan-assembler-times \"cmp/hi\" 4 } } */\n+/* { dg-final { scan-assembler-times \"cmp/gt\" 3 } } */\n+/* { dg-final { scan-assembler-times \"not\\t\" 1 } } */\n+\n+#include \"pr51244-20.c\""}, {"sha": "57f8197d643904a590820d075e0e3a1c8bb16daa", "filename": "gcc/testsuite/gcc.target/sh/pr51244-20.c", "status": "added", "additions": 103, "deletions": 0, "changes": 103, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Ftestsuite%2Fgcc.target%2Fsh%2Fpr51244-20.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d30dc5b6d86563a0a9ffb28429474b0f6c56544/gcc%2Ftestsuite%2Fgcc.target%2Fsh%2Fpr51244-20.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fsh%2Fpr51244-20.c?ref=5d30dc5b6d86563a0a9ffb28429474b0f6c56544", "patch": "@@ -0,0 +1,103 @@\n+/* Check that the SH specific sh_treg_combine RTL optimization pass works as\n+   expected.  On SH2A the expected insns are slightly different, see\n+   pr51244-21.c.  */\n+/* { dg-do compile { target \"sh*-*-*\" } } */\n+/* { dg-options \"-O2\" } */\n+/* { dg-skip-if \"\" { \"sh*-*-*\" } { \"-m5*\" \"-m2a*\" } { \"\" } } */\n+/* { dg-final { scan-assembler-times \"tst\" 6 } } */\n+/* { dg-final { scan-assembler-times \"movt\" 1 } } */\n+/* { dg-final { scan-assembler-times \"cmp/eq\" 2 } } */\n+/* { dg-final { scan-assembler-times \"cmp/hi\" 4 } } */\n+/* { dg-final { scan-assembler-times \"cmp/gt\" 2 } } */\n+/* { dg-final { scan-assembler-times \"not\\t\" 1 } } */\n+\n+\n+/* non-SH2A: 2x tst, 1x movt, 2x cmp/eq, 1x cmp/hi\n+   SH2A: 1x tst, 1x nott, 2x cmp/eq, 1x cmp/hi  */\n+static inline int\n+blk_oversized_queue_0 (int* q)\n+{\n+  if (q[2])\n+    return q[1] == 5; \n+  return (q[0] != 5);\n+}\n+\n+int __attribute__ ((noinline))\n+get_request_0 (int* q, int rw)\n+{\n+  if (blk_oversized_queue_0 (q))\n+    {\n+      if ((rw == 1) || (rw == 0))\n+\treturn -33;\n+      return 0;\n+    }\n+  return -100;\n+}\n+\n+\n+/* 1x tst, 1x cmp/gt, 1x cmp/hi\n+   On SH2A mem loads/stores have a wrong length of 4 bytes and thus will\n+   not be placed in a delay slot.  This introduces an extra cmp/gt insn.  */\n+static inline int\n+blk_oversized_queue_1 (int* q)\n+{\n+  if (q[2])\n+    return q[1] > 5; \n+  return (q[0] > 5);\n+}\n+\n+int __attribute__ ((noinline))\n+get_request_1 (int* q, int rw)\n+{\n+  if (blk_oversized_queue_1 (q))\n+    {\n+      if ((rw == 1) || (rw == 0))\n+\treturn -33;\n+      return 0;\n+    }\n+  return -100;\n+}\n+\n+\n+/* 1x tst, 1x cmp/gt, 1x cmp/hi, 1x cmp/hi  */\n+static inline int\n+blk_oversized_queue_2 (int* q)\n+{\n+  if (q[2])\n+    return q[1] > 5; \n+  return (q[0] < 5);\n+}\n+\n+int __attribute__ ((noinline))\n+get_request_2 (int* q, int rw)\n+{\n+  if (blk_oversized_queue_2 (q))\n+    {\n+      if ((rw == 1) || (rw == 0))\n+\treturn -33;\n+      return 0;\n+    }\n+  return -100;\n+}\n+\n+\n+/* 2x tst, 1x cmp/hi, 1x not  */\n+static inline int\n+blk_oversized_queue_5 (int* q)\n+{\n+  if (q[2])\n+    return q[1] != 0; \n+  return q[0] == 0;\n+}\n+\n+int __attribute__ ((noinline))\n+get_request_5 (int* q, int rw)\n+{\n+  if (blk_oversized_queue_5 (q))\n+    {\n+      if ((rw == 1) || (rw == 0))\n+\treturn -33;\n+      return 0;\n+    }\n+  return -100;\n+}"}]}