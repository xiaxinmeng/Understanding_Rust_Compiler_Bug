{"sha": "55a2c3226a3e90a6d65f19710bab1ac377054234", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NTVhMmMzMjI2YTNlOTBhNmQ2NWYxOTcxMGJhYjFhYzM3NzA1NDIzNA==", "commit": {"author": {"name": "Vladimir Makarov", "email": "vmakarov@redhat.com", "date": "2012-10-23T15:51:41Z"}, "committer": {"name": "Vladimir Makarov", "email": "vmakarov@gcc.gnu.org", "date": "2012-10-23T15:51:41Z"}, "message": "dbxout.c (dbxout_symbol_location): Pass new argument to alter_subreg.\n\n2012-10-23  Vladimir Makarov  <vmakarov@redhat.com>\n\n\t* dbxout.c (dbxout_symbol_location): Pass new argument to\n\talter_subreg.\n\t* dwarf2out.c: Include ira.h and lra.h.\n\t(based_loc_descr, compute_frame_pointer_to_fb_displacement): Use\n\tlra_eliminate_regs for LRA instead of eliminate_regs.\n\t* expr.c (emit_move_insn_1): Pass an additional argument to\n\temit_move_via_integer.  Use emit_move_via_integer for LRA only if\n\tthe insn is recognized.\n\t* emit-rtl.c (gen_rtx_REG): Add lra_in_progress.\n\t(validate_subreg): Don't check offset for LRA and floating point\n\tmodes.\n\t* final.c (final_scan_insn, cleanup_subreg_operands): Pass new\n\targument to alter_subreg.\n\t(walk_alter_subreg, output_operand): Ditto.\n\t(alter_subreg): Add new argument.\n\t* gcse.c (calculate_bb_reg_pressure): Add parameter to\n\tira_setup_eliminable_regset call.\n\t* ira.c: Include lra.h.\n\t(ira_init_once, ira_init, ira_finish_once): Call lra_start_once,\n\tlra_init, lra_finish_once in anyway.\n\t(ira_setup_eliminable_regset): Add parameter.  Remove need_fp.\n\tCall lra_init_elimination and mark HARD_FRAME_POINTER_REGNUM as\n\tliving forever if frame_pointer_needed.\n\t(setup_reg_class_relations): Set up ira_reg_class_subset.\n\t(ira_reg_equiv_invariant_p, ira_reg_equiv_const): Remove.\n\t(find_reg_equiv_invariant_const): Ditto.\n\t(setup_reg_renumber): Use ira_equiv_no_lvalue_p instead of\n\tira_reg_equiv_invariant_p.  Skip caps for LRA.\n\t(setup_reg_equiv_init, ira_update_equiv_info_by_shuffle_insn): New\n\tfunctions.\n\t(ira_reg_equiv_len, ira_reg_equiv): New externals.\n\t(ira_reg_equiv): New.\n\t(ira_expand_reg_equiv, init_reg_equiv, finish_reg_equiv): New\n\tfunctions.\n\t(no_equiv, update_equiv_regs): Use ira_reg_equiv instead of\n\treg_equiv_init.\n\t(setup_reg_equiv): New function.\n\t(ira_use_lra_p): New global.\n\t(ira): Set up lra_simple_p and ira_conflicts_p.  Set up and\n\trestore flag_caller_saves and flag_ira_region.  Move\n\tinitialization of ira_obstack and ira_bitmap_obstack upper.  Call\n\tinit_reg_equiv, setup_reg_equiv, and setup_reg_equiv_init instead\n\tof initialization of ira_reg_equiv_len, ira_reg_equiv_invariant_p,\n\tand ira_reg_equiv_const.  Call ira_setup_eliminable_regset with a\n\tnew argument.  Don't flatten IRA IRA for LRA.  Don't reassign\n\tconflict allocnos for LRA. Call finish_reg_equiv.\n        (do_reload): Prepare code for LRA call.  Call LRA.\n\t* ira.h (ira_use_lra_p): New external.\n\t(struct target_ira): Add members x_ira_class_subset_p\n\tx_ira_reg_class_subset, and x_ira_reg_classes_intersect_p.\n\t(ira_class_subset_p, ira_reg_class_subset): New macros.\n\t(ira_reg_classes_intersect_p): New macro.\n\t(struct ira_reg_equiv): New.\n\t(ira_setup_eliminable_regset): Add an argument.\n\t(ira_expand_reg_equiv, ira_update_equiv_info_by_shuffle_insn): New\n\tprototypes.\n\t* ira-color.c (color_pass, move_spill_restore, coalesce_allocnos):\n\tUse ira_equiv_no_lvalue_p.\n\t(coalesce_spill_slots, ira_sort_regnos_for_alter_reg): Ditto.\n\t* ira-emit.c (ira_create_new_reg): Call ira_expand_reg_equiv.\n\t(generate_edge_moves, change_loop) Use ira_equiv_no_lvalue_p.\n\t(emit_move_list): Simplify code.  Call\n\tira_update_equiv_info_by_shuffle_insn.  Use ira_reg_equiv instead\n\tof ira_reg_equiv_invariant_p and ira_reg_equiv_const.  Change\n\tassert.\n\t* ira-int.h (struct target_ira_int): Remove x_ira_class_subset_p\n\tand x_ira_reg_classes_intersect_p.\n\t(ira_class_subset_p, ira_reg_classes_intersect_p): Remove.\n\t(ira_reg_equiv_len, ira_reg_equiv_invariant_p): Ditto.\n\t(ira_reg_equiv_const): Ditto.\n\t(ira_equiv_no_lvalue_p): New function.\n\t* jump.c (true_regnum): Always use hard_regno for subreg_get_info\n\twhen lra is in progress.\n\t* haifa-sched.c (sched_init): Pass new argument to\n\tira_setup_eliminable_regset.\n\t* loop-invariant.c (calculate_loop_reg_pressure): Pass new\n\targument to ira_setup_eliminable_regset.\n\t* lra.h: New.\n\t* lra-int.h: Ditto.\n\t* lra.c: Ditto.\n\t* lra-assigns.c: Ditto.\n\t* lra-constraints.c: Ditto.\n\t* lra-coalesce.c: Ditto.\n\t* lra-eliminations.c: Ditto.\n\t* lra-lives.c: Ditto.\n\t* lra-spills.c: Ditto.\n\t* Makefile.in (LRA_INT_H): New.\n\t(OBJS): Add lra.o, lra-assigns.o, lra-coalesce.o,\n\tlra-constraints.o, lra-eliminations.o, lra-lives.o, and\n\tlra-spills.o.\n\t(dwarf2out.o): Add dependence on ira.h and lra.h.\n\t(ira.o): Add dependence on lra.h.\n\t(lra.o, lra-assigns.o, lra-coalesce.o, lra-constraints.o): New\n\tentries.\n\t(lra-eliminations.o, lra-lives.o, lra-spills.o): Ditto.\n\t* output.h (alter_subreg): Add new argument.\n\t* rtlanal.c (simplify_subreg_regno): Permit mode changes for LRA.\n\tPermit ARG_POINTER_REGNUM and STACK_POINTER_REGNUM for LRA.\n\t* recog.c (general_operand, register_operand): Accept paradoxical\n\tFLOAT_MODE subregs for LRA.\n\t(scratch_operand): Accept pseudos for LRA.\n\t* rtl.h (lra_in_progress): New external.\n\t(debug_bb_n_slim, debug_bb_slim, print_value_slim): New\n\tprototypes.\n\t(debug_rtl_slim, debug_insn_slim): Ditto.\n\t* sdbout.c (sdbout_symbol): Pass new argument to alter_subreg.\n\t* sched-vis.c (print_value_slim): New.\n\t* target.def (lra_p): New hook.\n\t(register_priority): Ditto.\n\t(different_addr_displacement_p): Ditto.\n\t(spill_class): Ditto.\n\t* target-globals.h (this_target_lra_int): New external.\n\t(target_globals): New member lra_int.\n\t(restore_target_globals): Restore this_target_lra_int.\n\t* target-globals.c: Include lra-int.h.\n\t(default_target_globals): Add &default_target_lra_int.\n\t* targhooks.c (default_lra_p): New function.\n\t(default_register_priority): Ditto.\n\t(default_different_addr_displacement_p): Ditto.\n\t* targhooks.h (default_lra_p): Declare.\n\t(default_register_priority): Ditto.\n\t(default_different_addr_displacement_p): Ditto.\n\t* timevar.def (TV_LRA, TV_LRA_ELIMINATE, TV_LRA_INHERITANCE): New.\n\t(TV_LRA_CREATE_LIVE_RANGES, TV_LRA_ASSIGN, TV_LRA_COALESCE): New.\n\t* config/arm/arm.c (load_multiple_sequence): Pass new argument to\u001bOB\n\talter_subreg.\n\t(store_multiple_sequence): Ditto.\n\t* config/i386/i386.h (enum ix86_tune_indices): Add\n\tX86_TUNE_GENERAL_REGS_SSE_SPILL.\n\t(TARGET_GENERAL_REGS_SSE_SPILL): New macro.\n\t* config/i386/i386.c (initial_ix86_tune_features): Set up\n\tX86_TUNE_GENERAL_REGS_SSE_SPILL for m_COREI7 and m_CORE2I7.\n\t(ix86_lra_p, ix86_register_priority): New functions.\n\t(ix86_secondary_reload): Add NON_Q_REGS, SIREG, DIREG.\n\t(inline_secondary_memory_needed): Change assert.\n\t(ix86_spill_class): New function.\n\t(TARGET_LRA_P, TARGET_REGISTER_BANK, TARGET_SPILL_CLASS): New\n\tmacros.\n\t* config/m68k/m68k.c (emit_move_sequence): Pass new argument to\n\talter_subreg.\n\t* config/m32r/m32r.c (gen_split_move_double): Ditto.\n\t* config/pa/pa.c (pa_emit_move_sequence): Ditto.\n\t* config/sh/sh.md: Ditto.\n\t* config/v850/v850.c (v850_reorg): Ditto.\n\t* config/xtensa/xtensa.c (fixup_subreg_mem): Ditto.\n\t* doc/md.texi: Add new interpretation of hint * for LRA.\n\t* doc/passes.texi: Describe LRA pass.\n\t* doc/tm.texi.in: Add TARGET_LRA_P, TARGET_REGISTER_PRIORITY,\n\tTARGET_DIFFERENT_ADDR_DISPLACEMENT_P, and TARGET_SPILL_CLASS.\n\t* doc/tm.texi: Update.\n\nFrom-SVN: r192719", "tree": {"sha": "915ce489d01a05653371ff4f7770258ffacab1b4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/915ce489d01a05653371ff4f7770258ffacab1b4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/55a2c3226a3e90a6d65f19710bab1ac377054234", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/55a2c3226a3e90a6d65f19710bab1ac377054234", "html_url": "https://github.com/Rust-GCC/gccrs/commit/55a2c3226a3e90a6d65f19710bab1ac377054234", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/55a2c3226a3e90a6d65f19710bab1ac377054234/comments", "author": {"login": "vnmakarov", "id": 9855671, "node_id": "MDQ6VXNlcjk4NTU2NzE=", "avatar_url": "https://avatars.githubusercontent.com/u/9855671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vnmakarov", "html_url": "https://github.com/vnmakarov", "followers_url": "https://api.github.com/users/vnmakarov/followers", "following_url": "https://api.github.com/users/vnmakarov/following{/other_user}", "gists_url": "https://api.github.com/users/vnmakarov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vnmakarov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vnmakarov/subscriptions", "organizations_url": "https://api.github.com/users/vnmakarov/orgs", "repos_url": "https://api.github.com/users/vnmakarov/repos", "events_url": "https://api.github.com/users/vnmakarov/events{/privacy}", "received_events_url": "https://api.github.com/users/vnmakarov/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "6acf25e4b380e5ad738ffe2830a71635bc5230d1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6acf25e4b380e5ad738ffe2830a71635bc5230d1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6acf25e4b380e5ad738ffe2830a71635bc5230d1"}], "stats": {"total": 14004, "additions": 13722, "deletions": 282}, "files": [{"sha": "049321a91899f6dedd306ec72df00c8b476ecc5f", "filename": "gcc/ChangeLog", "status": "modified", "additions": 153, "deletions": 0, "changes": 153, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -1,3 +1,156 @@\n+2012-10-23  Vladimir Makarov  <vmakarov@redhat.com>\n+\n+\t* dbxout.c (dbxout_symbol_location): Pass new argument to\n+\talter_subreg.\n+\t* dwarf2out.c: Include ira.h and lra.h.\n+\t(based_loc_descr, compute_frame_pointer_to_fb_displacement): Use\n+\tlra_eliminate_regs for LRA instead of eliminate_regs.\n+\t* expr.c (emit_move_insn_1): Pass an additional argument to\n+\temit_move_via_integer.  Use emit_move_via_integer for LRA only if\n+\tthe insn is recognized.\n+\t* emit-rtl.c (gen_rtx_REG): Add lra_in_progress.\n+\t(validate_subreg): Don't check offset for LRA and floating point\n+\tmodes.\n+\t* final.c (final_scan_insn, cleanup_subreg_operands): Pass new\n+\targument to alter_subreg.\n+\t(walk_alter_subreg, output_operand): Ditto.\n+\t(alter_subreg): Add new argument.\n+\t* gcse.c (calculate_bb_reg_pressure): Add parameter to\n+\tira_setup_eliminable_regset call.\n+\t* ira.c: Include lra.h.\n+\t(ira_init_once, ira_init, ira_finish_once): Call lra_start_once,\n+\tlra_init, lra_finish_once in anyway.\n+\t(ira_setup_eliminable_regset): Add parameter.  Remove need_fp.\n+\tCall lra_init_elimination and mark HARD_FRAME_POINTER_REGNUM as\n+\tliving forever if frame_pointer_needed.\n+\t(setup_reg_class_relations): Set up ira_reg_class_subset.\n+\t(ira_reg_equiv_invariant_p, ira_reg_equiv_const): Remove.\n+\t(find_reg_equiv_invariant_const): Ditto.\n+\t(setup_reg_renumber): Use ira_equiv_no_lvalue_p instead of\n+\tira_reg_equiv_invariant_p.  Skip caps for LRA.\n+\t(setup_reg_equiv_init, ira_update_equiv_info_by_shuffle_insn): New\n+\tfunctions.\n+\t(ira_reg_equiv_len, ira_reg_equiv): New externals.\n+\t(ira_reg_equiv): New.\n+\t(ira_expand_reg_equiv, init_reg_equiv, finish_reg_equiv): New\n+\tfunctions.\n+\t(no_equiv, update_equiv_regs): Use ira_reg_equiv instead of\n+\treg_equiv_init.\n+\t(setup_reg_equiv): New function.\n+\t(ira_use_lra_p): New global.\n+\t(ira): Set up lra_simple_p and ira_conflicts_p.  Set up and\n+\trestore flag_caller_saves and flag_ira_region.  Move\n+\tinitialization of ira_obstack and ira_bitmap_obstack upper.  Call\n+\tinit_reg_equiv, setup_reg_equiv, and setup_reg_equiv_init instead\n+\tof initialization of ira_reg_equiv_len, ira_reg_equiv_invariant_p,\n+\tand ira_reg_equiv_const.  Call ira_setup_eliminable_regset with a\n+\tnew argument.  Don't flatten IRA IRA for LRA.  Don't reassign\n+\tconflict allocnos for LRA. Call finish_reg_equiv.\n+        (do_reload): Prepare code for LRA call.  Call LRA.\n+\t* ira.h (ira_use_lra_p): New external.\n+\t(struct target_ira): Add members x_ira_class_subset_p\n+\tx_ira_reg_class_subset, and x_ira_reg_classes_intersect_p.\n+\t(ira_class_subset_p, ira_reg_class_subset): New macros.\n+\t(ira_reg_classes_intersect_p): New macro.\n+\t(struct ira_reg_equiv): New.\n+\t(ira_setup_eliminable_regset): Add an argument.\n+\t(ira_expand_reg_equiv, ira_update_equiv_info_by_shuffle_insn): New\n+\tprototypes.\n+\t* ira-color.c (color_pass, move_spill_restore, coalesce_allocnos):\n+\tUse ira_equiv_no_lvalue_p.\n+\t(coalesce_spill_slots, ira_sort_regnos_for_alter_reg): Ditto.\n+\t* ira-emit.c (ira_create_new_reg): Call ira_expand_reg_equiv.\n+\t(generate_edge_moves, change_loop) Use ira_equiv_no_lvalue_p.\n+\t(emit_move_list): Simplify code.  Call\n+\tira_update_equiv_info_by_shuffle_insn.  Use ira_reg_equiv instead\n+\tof ira_reg_equiv_invariant_p and ira_reg_equiv_const.  Change\n+\tassert.\n+\t* ira-int.h (struct target_ira_int): Remove x_ira_class_subset_p\n+\tand x_ira_reg_classes_intersect_p.\n+\t(ira_class_subset_p, ira_reg_classes_intersect_p): Remove.\n+\t(ira_reg_equiv_len, ira_reg_equiv_invariant_p): Ditto.\n+\t(ira_reg_equiv_const): Ditto.\n+\t(ira_equiv_no_lvalue_p): New function.\n+\t* jump.c (true_regnum): Always use hard_regno for subreg_get_info\n+\twhen lra is in progress.\n+\t* haifa-sched.c (sched_init): Pass new argument to\n+\tira_setup_eliminable_regset.\n+\t* loop-invariant.c (calculate_loop_reg_pressure): Pass new\n+\targument to ira_setup_eliminable_regset.\n+\t* lra.h: New.\n+\t* lra-int.h: Ditto.\n+\t* lra.c: Ditto.\n+\t* lra-assigns.c: Ditto.\n+\t* lra-constraints.c: Ditto.\n+\t* lra-coalesce.c: Ditto.\n+\t* lra-eliminations.c: Ditto.\n+\t* lra-lives.c: Ditto.\n+\t* lra-spills.c: Ditto.\n+\t* Makefile.in (LRA_INT_H): New.\n+\t(OBJS): Add lra.o, lra-assigns.o, lra-coalesce.o,\n+\tlra-constraints.o, lra-eliminations.o, lra-lives.o, and\n+\tlra-spills.o.\n+\t(dwarf2out.o): Add dependence on ira.h and lra.h.\n+\t(ira.o): Add dependence on lra.h.\n+\t(lra.o, lra-assigns.o, lra-coalesce.o, lra-constraints.o): New\n+\tentries.\n+\t(lra-eliminations.o, lra-lives.o, lra-spills.o): Ditto.\n+\t* output.h (alter_subreg): Add new argument.\n+\t* rtlanal.c (simplify_subreg_regno): Permit mode changes for LRA.\n+\tPermit ARG_POINTER_REGNUM and STACK_POINTER_REGNUM for LRA.\n+\t* recog.c (general_operand, register_operand): Accept paradoxical\n+\tFLOAT_MODE subregs for LRA.\n+\t(scratch_operand): Accept pseudos for LRA.\n+\t* rtl.h (lra_in_progress): New external.\n+\t(debug_bb_n_slim, debug_bb_slim, print_value_slim): New\n+\tprototypes.\n+\t(debug_rtl_slim, debug_insn_slim): Ditto.\n+\t* sdbout.c (sdbout_symbol): Pass new argument to alter_subreg.\n+\t* sched-vis.c (print_value_slim): New.\n+\t* target.def (lra_p): New hook.\n+\t(register_priority): Ditto.\n+\t(different_addr_displacement_p): Ditto.\n+\t(spill_class): Ditto.\n+\t* target-globals.h (this_target_lra_int): New external.\n+\t(target_globals): New member lra_int.\n+\t(restore_target_globals): Restore this_target_lra_int.\n+\t* target-globals.c: Include lra-int.h.\n+\t(default_target_globals): Add &default_target_lra_int.\n+\t* targhooks.c (default_lra_p): New function.\n+\t(default_register_priority): Ditto.\n+\t(default_different_addr_displacement_p): Ditto.\n+\t* targhooks.h (default_lra_p): Declare.\n+\t(default_register_priority): Ditto.\n+\t(default_different_addr_displacement_p): Ditto.\n+\t* timevar.def (TV_LRA, TV_LRA_ELIMINATE, TV_LRA_INHERITANCE): New.\n+\t(TV_LRA_CREATE_LIVE_RANGES, TV_LRA_ASSIGN, TV_LRA_COALESCE): New.\n+\t* config/arm/arm.c (load_multiple_sequence): Pass new argument to\u001bOB\n+\talter_subreg.\n+\t(store_multiple_sequence): Ditto.\n+\t* config/i386/i386.h (enum ix86_tune_indices): Add\n+\tX86_TUNE_GENERAL_REGS_SSE_SPILL.\n+\t(TARGET_GENERAL_REGS_SSE_SPILL): New macro.\n+\t* config/i386/i386.c (initial_ix86_tune_features): Set up\n+\tX86_TUNE_GENERAL_REGS_SSE_SPILL for m_COREI7 and m_CORE2I7.\n+\t(ix86_lra_p, ix86_register_priority): New functions.\n+\t(ix86_secondary_reload): Add NON_Q_REGS, SIREG, DIREG.\n+\t(inline_secondary_memory_needed): Change assert.\n+\t(ix86_spill_class): New function.\n+\t(TARGET_LRA_P, TARGET_REGISTER_BANK, TARGET_SPILL_CLASS): New\n+\tmacros.\n+\t* config/m68k/m68k.c (emit_move_sequence): Pass new argument to\n+\talter_subreg.\n+\t* config/m32r/m32r.c (gen_split_move_double): Ditto.\n+\t* config/pa/pa.c (pa_emit_move_sequence): Ditto.\n+\t* config/sh/sh.md: Ditto.\n+\t* config/v850/v850.c (v850_reorg): Ditto.\n+\t* config/xtensa/xtensa.c (fixup_subreg_mem): Ditto.\n+\t* doc/md.texi: Add new interpretation of hint * for LRA.\n+\t* doc/passes.texi: Describe LRA pass.\n+\t* doc/tm.texi.in: Add TARGET_LRA_P, TARGET_REGISTER_PRIORITY,\n+\tTARGET_DIFFERENT_ADDR_DISPLACEMENT_P, and TARGET_SPILL_CLASS.\n+\t* doc/tm.texi: Update.\n+\n 2012-10-23  Jan Hubicka  <jh@suse.cz>\n \n \t* loop-unroll.c (decide_peel_simple): Simple peeling makes sense even"}, {"sha": "c729ee6f7d8daad4520f1f2cff481977e0f1974a", "filename": "gcc/Makefile.in", "status": "modified", "additions": 46, "deletions": 2, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -940,6 +940,7 @@ TREE_DATA_REF_H = tree-data-ref.h $(OMEGA_H) graphds.h $(SCEV_H)\n TREE_INLINE_H = tree-inline.h vecir.h\n REAL_H = real.h $(MACHMODE_H)\n IRA_INT_H = ira.h ira-int.h $(CFGLOOP_H) alloc-pool.h\n+LRA_INT_H = lra.h $(BITMAP_H) $(RECOG_H) $(INSN_ATTR_H) insn-codes.h lra-int.h\n DBGCNT_H = dbgcnt.h dbgcnt.def\n EBITMAP_H = ebitmap.h sbitmap.h\n LTO_STREAMER_H = lto-streamer.h $(LINKER_PLUGIN_API_H) $(TARGET_H) \\\n@@ -1272,6 +1273,13 @@ OBJS = \\\n \tloop-unroll.o \\\n \tloop-unswitch.o \\\n \tlower-subreg.o \\\n+\tlra.o \\\n+\tlra-assigns.o \\\n+\tlra-coalesce.o \\\n+\tlra-constraints.o \\\n+\tlra-eliminations.o \\\n+\tlra-lives.o \\\n+\tlra-spills.o \\\n \tlto-cgraph.o \\\n \tlto-streamer.o \\\n \tlto-streamer-in.o \\\n@@ -2783,7 +2791,7 @@ dwarf2out.o : dwarf2out.c $(CONFIG_H) $(SYSTEM_H) coretypes.h dumpfile.h \\\n    toplev.h $(DIAGNOSTIC_CORE_H) $(DWARF2OUT_H) reload.h \\\n    $(GGC_H) $(EXCEPT_H) dwarf2asm.h $(TM_P_H) langhooks.h $(HASHTAB_H) \\\n    gt-dwarf2out.h $(TARGET_H) $(CGRAPH_H) $(MD5_H) $(INPUT_H) $(FUNCTION_H) \\\n-   $(GIMPLE_H) $(TREE_FLOW_H) \\\n+   $(GIMPLE_H) ira.h lra.h $(TREE_FLOW_H) \\\n    $(TREE_PRETTY_PRINT_H) $(COMMON_TARGET_H) $(OPTS_H)\n dwarf2cfi.o : dwarf2cfi.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    version.h $(RTL_H) $(EXPR_H) $(REGS_H) $(FUNCTION_H) output.h \\\n@@ -3217,7 +3225,43 @@ ira.o: ira.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n    $(TM_H) $(REGS_H) $(RTL_H) $(TM_P_H) $(TARGET_H) $(FLAGS_H) $(OBSTACK_H) \\\n    $(BITMAP_H) hard-reg-set.h $(BASIC_BLOCK_H) $(DBGCNT_H) $(FUNCTION_H) \\\n    $(EXPR_H) $(RECOG_H) $(PARAMS_H) $(TREE_PASS_H) output.h \\\n-   $(EXCEPT_H) reload.h toplev.h $(DIAGNOSTIC_CORE_H) $(DF_H) $(GGC_H) $(IRA_INT_H)\n+   $(EXCEPT_H) reload.h toplev.h $(DIAGNOSTIC_CORE_H) \\\n+   $(DF_H) $(GGC_H) $(IRA_INT_H) lra.h\n+lra.o : lra.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(RTL_H) $(REGS_H) insn-config.h insn-codes.h $(TIMEVAR_H) $(TREE_PASS_H) \\\n+   $(DF_H) $(RECOG_H) output.h addresses.h $(REGS_H) hard-reg-set.h \\\n+   $(FLAGS_H) $(FUNCTION_H) $(EXPR_H) $(BASIC_BLOCK_H) $(TM_P_H) \\\n+   $(EXCEPT_H) ira.h $(LRA_INT_H)\n+lra-assigns.o : lra-assigns.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n+   $(TM_H) $(RTL_H) $(REGS_H) insn-config.h $(DF_H) \\\n+   $(RECOG_H) output.h $(REGS_H) hard-reg-set.h $(FLAGS_H) $(FUNCTION_H) \\\n+   $(EXPR_H) $(BASIC_BLOCK_H) $(TM_P_H) $(EXCEPT_H) ira.h \\\n+   rtl-error.h sparseset.h $(LRA_INT_H)\n+lra-coalesce.o : lra-coalesce.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n+   $(TM_H) $(RTL_H) $(REGS_H) insn-config.h $(DF_H) \\\n+   $(RECOG_H) output.h $(REGS_H) hard-reg-set.h $(FLAGS_H) $(FUNCTION_H) \\\n+   $(EXPR_H) $(BASIC_BLOCK_H) $(TM_P_H) $(EXCEPT_H) ira.h \\\n+   rtl-error.h ira.h $(LRA_INT_H)\n+lra-constraints.o : lra-constraints.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n+   $(TM_H) $(RTL_H) $(REGS_H) insn-config.h insn-codes.h $(DF_H) \\\n+   $(RECOG_H) output.h addresses.h $(REGS_H) hard-reg-set.h $(FLAGS_H) \\\n+   $(FUNCTION_H) $(EXPR_H) $(BASIC_BLOCK_H) $(TM_P_H) $(EXCEPT_H) \\\n+   ira.h rtl-error.h $(LRA_INT_H)\n+lra-eliminations.o : lra-eliminations.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n+   $(TM_H) $(RTL_H) $(REGS_H) insn-config.h $(DF_H) \\\n+   $(RECOG_H) output.h $(REGS_H) hard-reg-set.h $(FLAGS_H) $(FUNCTION_H) \\\n+   $(EXPR_H) $(BASIC_BLOCK_H) $(TM_P_H) $(EXCEPT_H) ira.h \\\n+   rtl-error.h $(LRA_INT_H)\n+lra-lives.o : lra-lives.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(RTL_H) $(REGS_H) insn-config.h $(DF_H) \\\n+   $(RECOG_H) output.h $(REGS_H) hard-reg-set.h $(FLAGS_H) $(FUNCTION_H) \\\n+   $(EXPR_H) $(BASIC_BLOCK_H) $(TM_P_H) $(EXCEPT_H) \\\n+   $(LRA_INT_H)\n+lra-spills.o : lra-spills.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(RTL_H) $(REGS_H) insn-config.h $(DF_H) \\\n+   $(RECOG_H) output.h $(REGS_H) hard-reg-set.h $(FLAGS_H) $(FUNCTION_H) \\\n+   $(EXPR_H) $(BASIC_BLOCK_H) $(TM_P_H) $(EXCEPT_H) \\\n+   ira.h $(LRA_INT_H)\n regmove.o : regmove.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    insn-config.h $(TREE_PASS_H) $(DF_H) \\\n    $(RECOG_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) $(FUNCTION_H) \\"}, {"sha": "b7bec6e0cc4341c8291a3bb545b7d229e8c0547b", "filename": "gcc/config/arm/arm.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Farm%2Farm.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Farm%2Farm.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -10328,7 +10328,7 @@ load_multiple_sequence (rtx *operands, int nops, int *regs, int *saved_order,\n \n       /* Convert a subreg of a mem into the mem itself.  */\n       if (GET_CODE (operands[nops + i]) == SUBREG)\n-\toperands[nops + i] = alter_subreg (operands + (nops + i));\n+\toperands[nops + i] = alter_subreg (operands + (nops + i), true);\n \n       gcc_assert (MEM_P (operands[nops + i]));\n \n@@ -10480,7 +10480,7 @@ store_multiple_sequence (rtx *operands, int nops, int nops_total,\n \n       /* Convert a subreg of a mem into the mem itself.  */\n       if (GET_CODE (operands[nops + i]) == SUBREG)\n-\toperands[nops + i] = alter_subreg (operands + (nops + i));\n+\toperands[nops + i] = alter_subreg (operands + (nops + i), true);\n \n       gcc_assert (MEM_P (operands[nops + i]));\n "}, {"sha": "c98c6b7a52f4618ff1e2e9e60c316dceb34e5ce3", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 66, "deletions": 2, "changes": 68, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -2267,7 +2267,11 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n \n   /* X86_TUNE_REASSOC_FP_TO_PARALLEL: Try to produce parallel computations\n      during reassociation of fp computation.  */\n-  m_ATOM\n+  m_ATOM,\n+\n+  /* X86_TUNE_GENERAL_REGS_SSE_SPILL: Try to spill general regs to SSE\n+     regs instead of memory.  */\n+  m_COREI7 | m_CORE2I7\n };\n \n /* Feature tests against the various architecture variations.  */\n@@ -32046,6 +32050,38 @@ ix86_free_from_memory (enum machine_mode mode)\n     }\n }\n \n+/* Return true if we use LRA instead of reload pass.  */\n+static bool\n+ix86_lra_p (void)\n+{\n+  return true;\n+}\n+\n+/* Return a register priority for hard reg REGNO.  */\n+static int\n+ix86_register_priority (int hard_regno)\n+{\n+  /* ebp and r13 as the base always wants a displacement, r12 as the\n+     base always wants an index.  So discourage their usage in an\n+     address.  */\n+  if (hard_regno == R12_REG || hard_regno == R13_REG)\n+    return 0;\n+  if (hard_regno == BP_REG)\n+    return 1;\n+  /* New x86-64 int registers result in bigger code size.  Discourage\n+     them.  */\n+  if (FIRST_REX_INT_REG <= hard_regno && hard_regno <= LAST_REX_INT_REG)\n+    return 2;\n+  /* New x86-64 SSE registers result in bigger code size.  Discourage\n+     them.  */\n+  if (FIRST_REX_SSE_REG <= hard_regno && hard_regno <= LAST_REX_SSE_REG)\n+    return 2;\n+  /* Usage of AX register results in smaller code.  Prefer it.  */\n+  if (hard_regno == 0)\n+    return 4;\n+  return 3;\n+}\n+\n /* Implement TARGET_PREFERRED_RELOAD_CLASS.\n \n    Put float CONST_DOUBLE in the constant pool instead of fp regs.\n@@ -32179,6 +32215,9 @@ ix86_secondary_reload (bool in_p, rtx x, reg_class_t rclass,\n       && !in_p && mode == QImode\n       && (rclass == GENERAL_REGS\n \t  || rclass == LEGACY_REGS\n+\t  || rclass == NON_Q_REGS\n+\t  || rclass == SIREG\n+\t  || rclass == DIREG\n \t  || rclass == INDEX_REGS))\n     {\n       int regno;\n@@ -32288,7 +32327,7 @@ inline_secondary_memory_needed (enum reg_class class1, enum reg_class class2,\n       || MAYBE_MMX_CLASS_P (class1) != MMX_CLASS_P (class1)\n       || MAYBE_MMX_CLASS_P (class2) != MMX_CLASS_P (class2))\n     {\n-      gcc_assert (!strict);\n+      gcc_assert (!strict || lra_in_progress);\n       return true;\n     }\n \n@@ -40839,6 +40878,22 @@ ix86_autovectorize_vector_sizes (void)\n   return (TARGET_AVX && !TARGET_PREFER_AVX128) ? 32 | 16 : 0;\n }\n \n+\f\n+\n+/* Return class of registers which could be used for pseudo of MODE\n+   and of class RCLASS for spilling instead of memory.  Return NO_REGS\n+   if it is not possible or non-profitable.  */\n+static reg_class_t\n+ix86_spill_class (reg_class_t rclass, enum machine_mode mode)\n+{\n+  if (TARGET_SSE && TARGET_GENERAL_REGS_SSE_SPILL && ! TARGET_MMX\n+      && hard_reg_set_subset_p (reg_class_contents[rclass],\n+\t\t\t\treg_class_contents[GENERAL_REGS])\n+      && (mode == SImode || (TARGET_64BIT && mode == DImode)))\n+    return SSE_REGS;\n+  return NO_REGS;\n+}\n+\n /* Implement targetm.vectorize.init_cost.  */\n \n static void *\n@@ -41241,6 +41296,12 @@ ix86_memmodel_check (unsigned HOST_WIDE_INT val)\n #undef TARGET_LEGITIMATE_ADDRESS_P\n #define TARGET_LEGITIMATE_ADDRESS_P ix86_legitimate_address_p\n \n+#undef TARGET_LRA_P\n+#define TARGET_LRA_P ix86_lra_p\n+\n+#undef TARGET_REGISTER_PRIORITY\n+#define TARGET_REGISTER_PRIORITY ix86_register_priority\n+\n #undef TARGET_LEGITIMATE_CONSTANT_P\n #define TARGET_LEGITIMATE_CONSTANT_P ix86_legitimate_constant_p\n \n@@ -41264,6 +41325,9 @@ ix86_memmodel_check (unsigned HOST_WIDE_INT val)\n #define TARGET_INIT_LIBFUNCS darwin_rename_builtins\n #endif\n \n+#undef TARGET_SPILL_CLASS\n+#define TARGET_SPILL_CLASS ix86_spill_class\n+\n struct gcc_target targetm = TARGET_INITIALIZER;\n \f\n #include \"gt-i386.h\""}, {"sha": "f923a973b645dc54acd2cee816e1ab8d072d9891", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -327,6 +327,7 @@ enum ix86_tune_indices {\n   X86_TUNE_AVX128_OPTIMAL,\n   X86_TUNE_REASSOC_INT_TO_PARALLEL,\n   X86_TUNE_REASSOC_FP_TO_PARALLEL,\n+  X86_TUNE_GENERAL_REGS_SSE_SPILL,\n \n   X86_TUNE_LAST\n };\n@@ -431,6 +432,8 @@ extern unsigned char ix86_tune_features[X86_TUNE_LAST];\n \tix86_tune_features[X86_TUNE_REASSOC_INT_TO_PARALLEL]\n #define TARGET_REASSOC_FP_TO_PARALLEL \\\n \tix86_tune_features[X86_TUNE_REASSOC_FP_TO_PARALLEL]\n+#define TARGET_GENERAL_REGS_SSE_SPILL \\\n+\tix86_tune_features[X86_TUNE_GENERAL_REGS_SSE_SPILL]\n \n /* Feature tests against the various architecture variations.  */\n enum ix86_arch_indices {"}, {"sha": "18b6d8a8a3ee449e5245332e11a858767641522c", "filename": "gcc/config/m32r/m32r.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fm32r%2Fm32r.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fm32r%2Fm32r.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fm32r%2Fm32r.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -1030,9 +1030,9 @@ gen_split_move_double (rtx operands[])\n      subregs to make this code simpler.  It is safe to call\n      alter_subreg any time after reload.  */\n   if (GET_CODE (dest) == SUBREG)\n-    alter_subreg (&dest);\n+    alter_subreg (&dest, true);\n   if (GET_CODE (src) == SUBREG)\n-    alter_subreg (&src);\n+    alter_subreg (&src, true);\n \n   start_sequence ();\n   if (REG_P (dest))"}, {"sha": "e86960efc7b22c4be67ab316cd19dc37f27817d1", "filename": "gcc/config/m68k/m68k.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fm68k%2Fm68k.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fm68k%2Fm68k.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fm68k%2Fm68k.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -3658,7 +3658,7 @@ emit_move_sequence (rtx *operands, enum machine_mode mode, rtx scratch_reg)\n       rtx temp = gen_rtx_SUBREG (GET_MODE (operand0),\n \t\t\t\t reg_equiv_mem (REGNO (SUBREG_REG (operand0))),\n \t\t\t\t SUBREG_BYTE (operand0));\n-      operand0 = alter_subreg (&temp);\n+      operand0 = alter_subreg (&temp, true);\n     }\n \n   if (scratch_reg\n@@ -3675,7 +3675,7 @@ emit_move_sequence (rtx *operands, enum machine_mode mode, rtx scratch_reg)\n       rtx temp = gen_rtx_SUBREG (GET_MODE (operand1),\n \t\t\t\t reg_equiv_mem (REGNO (SUBREG_REG (operand1))),\n \t\t\t\t SUBREG_BYTE (operand1));\n-      operand1 = alter_subreg (&temp);\n+      operand1 = alter_subreg (&temp, true);\n     }\n \n   if (scratch_reg && reload_in_progress && GET_CODE (operand0) == MEM"}, {"sha": "6476daa0fc3d49d8513d57e63c84fb4ed6f9d719", "filename": "gcc/config/pa/pa.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fpa%2Fpa.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fpa%2Fpa.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fpa%2Fpa.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -1616,7 +1616,7 @@ pa_emit_move_sequence (rtx *operands, enum machine_mode mode, rtx scratch_reg)\n       rtx temp = gen_rtx_SUBREG (GET_MODE (operand0),\n \t\t\t\t reg_equiv_mem (REGNO (SUBREG_REG (operand0))),\n \t\t\t\t SUBREG_BYTE (operand0));\n-      operand0 = alter_subreg (&temp);\n+      operand0 = alter_subreg (&temp, true);\n     }\n \n   if (scratch_reg\n@@ -1633,7 +1633,7 @@ pa_emit_move_sequence (rtx *operands, enum machine_mode mode, rtx scratch_reg)\n       rtx temp = gen_rtx_SUBREG (GET_MODE (operand1),\n \t\t\t\t reg_equiv_mem (REGNO (SUBREG_REG (operand1))),\n \t\t\t\t SUBREG_BYTE (operand1));\n-      operand1 = alter_subreg (&temp);\n+      operand1 = alter_subreg (&temp, true);\n     }\n \n   if (scratch_reg && reload_in_progress && GET_CODE (operand0) == MEM"}, {"sha": "d875a63961a3db8a34a5860864e2f1a382303e95", "filename": "gcc/config/sh/sh.md", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fsh%2Fsh.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fsh%2Fsh.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Fsh.md?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -7366,17 +7366,17 @@ label:\n \t  rtx regop = operands[store_p], word0 ,word1;\n \n \t  if (GET_CODE (regop) == SUBREG)\n-\t    alter_subreg (&regop);\n+\t    alter_subreg (&regop, true);\n \t  if (REGNO (XEXP (addr, 0)) == REGNO (XEXP (addr, 1)))\n \t    offset = 2;\n \t  else\n \t    offset = 4;\n \t  mem = copy_rtx (mem);\n \t  PUT_MODE (mem, SImode);\n \t  word0 = gen_rtx_SUBREG (SImode, regop, 0);\n-\t  alter_subreg (&word0);\n+\t  alter_subreg (&word0, true);\n \t  word1 = gen_rtx_SUBREG (SImode, regop, 4);\n-\t  alter_subreg (&word1);\n+\t  alter_subreg (&word1, true);\n \t  if (store_p || ! refers_to_regno_p (REGNO (word0),\n \t\t\t\t\t      REGNO (word0) + 1, addr, 0))\n \t    {\n@@ -7834,7 +7834,7 @@ label:\n       else\n \t{\n \t  x = gen_rtx_SUBREG (V2SFmode, operands[0], i * 8);\n-\t  alter_subreg (&x);\n+\t  alter_subreg (&x, true);\n \t}\n \n       if (MEM_P (operands[1]))\n@@ -7843,7 +7843,7 @@ label:\n       else\n \t{\n \t  y = gen_rtx_SUBREG (V2SFmode, operands[1], i * 8);\n-\t  alter_subreg (&y);\n+\t  alter_subreg (&y, true);\n \t}\n \n       emit_insn (gen_movv2sf_i (x, y));"}, {"sha": "5d297cf3b23ecb67e39bb6f13945e3e2d3d61f36", "filename": "gcc/config/v850/v850.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fv850%2Fv850.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fv850%2Fv850.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fv850%2Fv850.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -1301,11 +1301,11 @@ v850_reorg (void)\n \t      if (GET_CODE (dest) == SUBREG\n \t\t  && (GET_CODE (SUBREG_REG (dest)) == MEM\n \t\t      || GET_CODE (SUBREG_REG (dest)) == REG))\n-\t\talter_subreg (&dest);\n+\t\talter_subreg (&dest, true);\n \t      if (GET_CODE (src) == SUBREG\n \t\t  && (GET_CODE (SUBREG_REG (src)) == MEM\n \t\t      || GET_CODE (SUBREG_REG (src)) == REG))\n-\t\talter_subreg (&src);\n+\t\talter_subreg (&src, true);\n \n \t      if (GET_CODE (dest) == MEM && GET_CODE (src) == MEM)\n \t\tmem = NULL_RTX;"}, {"sha": "38dc7d5b933301be23997c9ca498c151d9592798", "filename": "gcc/config/xtensa/xtensa.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fxtensa%2Fxtensa.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fconfig%2Fxtensa%2Fxtensa.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fxtensa%2Fxtensa.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -1087,7 +1087,7 @@ fixup_subreg_mem (rtx x)\n \tgen_rtx_SUBREG (GET_MODE (x),\n \t\t\treg_equiv_mem (REGNO (SUBREG_REG (x))),\n \t\t\tSUBREG_BYTE (x));\n-      x = alter_subreg (&temp);\n+      x = alter_subreg (&temp, true);\n     }\n   return x;\n }"}, {"sha": "5492c7011ba187dfb31722611f436e7510c63332", "filename": "gcc/dbxout.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdbxout.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdbxout.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdbxout.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -2994,7 +2994,7 @@ dbxout_symbol_location (tree decl, tree type, const char *suffix, rtx home)\n \t  if (REGNO (value) >= FIRST_PSEUDO_REGISTER)\n \t    return 0;\n \t}\n-      home = alter_subreg (&home);\n+      home = alter_subreg (&home, true);\n     }\n   if (REG_P (home))\n     {"}, {"sha": "696ca94c40a39474cfa3febd40b3f5ce8ae55df1", "filename": "gcc/doc/md.texi", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdoc%2Fmd.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdoc%2Fmd.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Fmd.texi?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -1,5 +1,5 @@\n @c Copyright (C) 1988, 1989, 1992, 1993, 1994, 1996, 1998, 1999, 2000, 2001,\n-@c 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011\n+@c 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012\n @c Free Software Foundation, Inc.\n @c This is part of the GCC manual.\n @c For copying conditions, see the file gcc.texi.\n@@ -1622,7 +1622,9 @@ register preferences.\n @item *\n Says that the following character should be ignored when choosing\n register preferences.  @samp{*} has no effect on the meaning of the\n-constraint as a constraint, and no effect on reloading.\n+constraint as a constraint, and no effect on reloading.  For LRA\n+@samp{*} additionally disparages slightly the alternative if the\n+following character matches the operand.\n \n @ifset INTERNALS\n Here is an example: the 68000 has an instruction to sign-extend a"}, {"sha": "693ad31dd813e8bad7882fa5c5024ffa003187cf", "filename": "gcc/doc/passes.texi", "status": "modified", "additions": 21, "deletions": 4, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdoc%2Fpasses.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdoc%2Fpasses.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Fpasses.texi?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -771,7 +771,7 @@ branch instructions.  The source file for this pass is @file{gcse.c}.\n This pass attempts to replace conditional branches and surrounding\n assignments with arithmetic, boolean value producing comparison\n instructions, and conditional move instructions.  In the very last\n-invocation after reload, it will generate predicated instructions\n+invocation after reload/LRA, it will generate predicated instructions\n when supported by the target.  The code is located in @file{ifcvt.c}.\n \n @item Web construction\n@@ -842,9 +842,9 @@ source file is @file{regmove.c}.\n The integrated register allocator (@acronym{IRA}).  It is called\n integrated because coalescing, register live range splitting, and hard\n register preferencing are done on-the-fly during coloring.  It also\n-has better integration with the reload pass.  Pseudo-registers spilled\n-by the allocator or the reload have still a chance to get\n-hard-registers if the reload evicts some pseudo-registers from\n+has better integration with the reload/LRA pass.  Pseudo-registers spilled\n+by the allocator or the reload/LRA have still a chance to get\n+hard-registers if the reload/LRA evicts some pseudo-registers from\n hard-registers.  The allocator helps to choose better pseudos for\n spilling based on their live ranges and to coalesce stack slots\n allocated for the spilled pseudo-registers.  IRA is a regional\n@@ -875,6 +875,23 @@ instructions to save and restore call-clobbered registers around calls.\n \n Source files are @file{reload.c} and @file{reload1.c}, plus the header\n @file{reload.h} used for communication between them.\n+\n+@cindex Local Register Allocator (LRA)\n+@item\n+This pass is a modern replacement of the reload pass.  Source files\n+are @file{lra.c}, @file{lra-assign.c}, @file{lra-coalesce.c},\n+@file{lra-constraints.c}, @file{lra-eliminations.c},\n+@file{lra-equivs.c}, @file{lra-lives.c}, @file{lra-saves.c},\n+@file{lra-spills.c}, the header @file{lra-int.h} used for\n+communication between them, and the header @file{lra.h} used for\n+communication between LRA and the rest of compiler.\n+\n+Unlike the reload pass, intermediate LRA decisions are reflected in\n+RTL as much as possible.  This reduces the number of target-dependent\n+macros and hooks, leaving instruction constraints as the primary\n+source of control.\n+\n+LRA is run on targets for which TARGET_LRA_P returns true.\n @end itemize\n \n @item Basic block reordering"}, {"sha": "68713f7bb2f14ae072b9c823aa9802916076dcc1", "filename": "gcc/doc/tm.texi", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdoc%2Ftm.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdoc%2Ftm.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Ftm.texi?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -2893,6 +2893,22 @@ as below:\n @end smallexample\n @end defmac\n \n+@deftypefn {Target Hook} bool TARGET_LRA_P (void)\n+A target hook which returns true if we use LRA instead of reload pass.  It means that LRA was ported to the target.    The default version of this target hook returns always false.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} int TARGET_REGISTER_PRIORITY (int)\n+A target hook which returns the register priority number to which the  register @var{hard_regno} belongs to.  The bigger the number, the  more preferable the hard register usage (when all other conditions are  the same).  This hook can be used to prefer some hard register over  others in LRA.  For example, some x86-64 register usage needs  additional prefix which makes instructions longer.  The hook can  return lower priority number for such registers make them less favorable  and as result making the generated code smaller.    The default version of this target hook returns always zero.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} bool TARGET_DIFFERENT_ADDR_DISPLACEMENT_P (void)\n+A target hook which returns true if an address with the same structure  can have different maximal legitimate displacement.  For example, the  displacement can depend on memory mode or on operand combinations in  the insn.    The default version of this target hook returns always false.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} reg_class_t TARGET_SPILL_CLASS (reg_class_t, enum @var{machine_mode})\n+This hook defines a class of registers which could be used for spilling  pseudos of the given mode and class, or @code{NO_REGS} if only memory  should be used.  Not defining this hook is equivalent to returning  @code{NO_REGS} for all inputs.\n+@end deftypefn\n+\n @node Old Constraints\n @section Obsolete Macros for Defining Constraints\n @cindex defining constraints, obsolete method"}, {"sha": "c325cd4ae6e73464bf637f61184b4b29b813c621", "filename": "gcc/doc/tm.texi.in", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdoc%2Ftm.texi.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdoc%2Ftm.texi.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Ftm.texi.in?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -2869,6 +2869,14 @@ as below:\n @end smallexample\n @end defmac\n \n+@hook TARGET_LRA_P\n+\n+@hook TARGET_REGISTER_PRIORITY\n+\n+@hook TARGET_DIFFERENT_ADDR_DISPLACEMENT_P\n+\n+@hook TARGET_SPILL_CLASS\n+\n @node Old Constraints\n @section Obsolete Macros for Defining Constraints\n @cindex defining constraints, obsolete method"}, {"sha": "bc5868b6a252c19105585d6860be3e033789e73b", "filename": "gcc/dwarf2out.c", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdwarf2out.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fdwarf2out.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdwarf2out.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -90,6 +90,8 @@ along with GCC; see the file COPYING3.  If not see\n #include \"cgraph.h\"\n #include \"input.h\"\n #include \"gimple.h\"\n+#include \"ira.h\"\n+#include \"lra.h\"\n #include \"dumpfile.h\"\n #include \"opts.h\"\n \n@@ -10162,7 +10164,9 @@ based_loc_descr (rtx reg, HOST_WIDE_INT offset,\n      argument pointer and soft frame pointer rtx's.  */\n   if (reg == arg_pointer_rtx || reg == frame_pointer_rtx)\n     {\n-      rtx elim = eliminate_regs (reg, VOIDmode, NULL_RTX);\n+      rtx elim = (ira_use_lra_p\n+\t\t  ? lra_eliminate_regs (reg, VOIDmode, NULL_RTX)\n+\t\t  : eliminate_regs (reg, VOIDmode, NULL_RTX));\n \n       if (elim != reg)\n \t{\n@@ -15020,7 +15024,9 @@ compute_frame_pointer_to_fb_displacement (HOST_WIDE_INT offset)\n   offset += ARG_POINTER_CFA_OFFSET (current_function_decl);\n #endif\n \n-  elim = eliminate_regs (reg, VOIDmode, NULL_RTX);\n+  elim = (ira_use_lra_p\n+\t  ? lra_eliminate_regs (reg, VOIDmode, NULL_RTX)\n+\t  : eliminate_regs (reg, VOIDmode, NULL_RTX));\n   if (GET_CODE (elim) == PLUS)\n     {\n       offset += INTVAL (XEXP (elim, 1));"}, {"sha": "cb23d5a4e3fdf2d76aca7e9cfa1b518e6c6183d2", "filename": "gcc/emit-rtl.c", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Femit-rtl.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Femit-rtl.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Femit-rtl.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -578,7 +578,7 @@ gen_rtx_REG (enum machine_mode mode, unsigned int regno)\n      Also don't do this when we are making new REGs in reload, since\n      we don't want to get confused with the real pointers.  */\n \n-  if (mode == Pmode && !reload_in_progress)\n+  if (mode == Pmode && !reload_in_progress && !lra_in_progress)\n     {\n       if (regno == FRAME_POINTER_REGNUM\n \t  && (!reload_completed || frame_pointer_needed))\n@@ -720,7 +720,14 @@ validate_subreg (enum machine_mode omode, enum machine_mode imode,\n      (subreg:SI (reg:DF) 0) isn't.  */\n   else if (FLOAT_MODE_P (imode) || FLOAT_MODE_P (omode))\n     {\n-      if (isize != osize)\n+      if (! (isize == osize\n+\t     /* LRA can use subreg to store a floating point value in\n+\t\tan integer mode.  Although the floating point and the\n+\t\tinteger modes need the same number of hard registers,\n+\t\tthe size of floating point mode can be less than the\n+\t\tinteger mode.  LRA also uses subregs for a register\n+\t\tshould be used in different mode in on insn.  */\n+\t     || lra_in_progress))\n \treturn false;\n     }\n \n@@ -753,7 +760,8 @@ validate_subreg (enum machine_mode omode, enum machine_mode imode,\n      of a subword.  A subreg does *not* perform arbitrary bit extraction.\n      Given that we've already checked mode/offset alignment, we only have\n      to check subword subregs here.  */\n-  if (osize < UNITS_PER_WORD)\n+  if (osize < UNITS_PER_WORD\n+      && ! (lra_in_progress && (FLOAT_MODE_P (imode) || FLOAT_MODE_P (omode))))\n     {\n       enum machine_mode wmode = isize > UNITS_PER_WORD ? word_mode : imode;\n       unsigned int low_off = subreg_lowpart_offset (omode, wmode);"}, {"sha": "448596c3396f88dd34413fdf4ce2e78a9e6d94cf", "filename": "gcc/expr.c", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fexpr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fexpr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -3448,9 +3448,13 @@ emit_move_insn_1 (rtx x, rtx y)\n      fits within a HOST_WIDE_INT.  */\n   if (!CONSTANT_P (y) || GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT)\n     {\n-      rtx ret = emit_move_via_integer (mode, x, y, false);\n+      rtx ret = emit_move_via_integer (mode, x, y, lra_in_progress);\n+\n       if (ret)\n-\treturn ret;\n+\t{\n+\t  if (! lra_in_progress || recog (PATTERN (ret), ret, 0) >= 0)\n+\t    return ret;\n+\t}\n     }\n \n   return emit_move_multi_word (mode, x, y);"}, {"sha": "ceb688e5e312e256269bcc82bc890ab640081ef2", "filename": "gcc/final.c", "status": "modified", "additions": 16, "deletions": 13, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ffinal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ffinal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffinal.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -2560,18 +2560,18 @@ final_scan_insn (rtx insn, FILE *file, int optimize_p ATTRIBUTE_UNUSED,\n \t      {\n \t\trtx src1, src2;\n \t\tif (GET_CODE (SET_SRC (set)) == SUBREG)\n-\t\t  SET_SRC (set) = alter_subreg (&SET_SRC (set));\n+\t\t  SET_SRC (set) = alter_subreg (&SET_SRC (set), true);\n \n \t\tsrc1 = SET_SRC (set);\n \t\tsrc2 = NULL_RTX;\n \t\tif (GET_CODE (SET_SRC (set)) == COMPARE)\n \t\t  {\n \t\t    if (GET_CODE (XEXP (SET_SRC (set), 0)) == SUBREG)\n \t\t      XEXP (SET_SRC (set), 0)\n-\t\t\t= alter_subreg (&XEXP (SET_SRC (set), 0));\n+\t\t\t= alter_subreg (&XEXP (SET_SRC (set), 0), true);\n \t\t    if (GET_CODE (XEXP (SET_SRC (set), 1)) == SUBREG)\n \t\t      XEXP (SET_SRC (set), 1)\n-\t\t\t= alter_subreg (&XEXP (SET_SRC (set), 1));\n+\t\t\t= alter_subreg (&XEXP (SET_SRC (set), 1), true);\n \t\t    if (XEXP (SET_SRC (set), 1)\n \t\t\t== CONST0_RTX (GET_MODE (XEXP (SET_SRC (set), 0))))\n \t\t      src2 = XEXP (SET_SRC (set), 0);\n@@ -2974,7 +2974,7 @@ cleanup_subreg_operands (rtx insn)\n \t expression directly.  */\n       if (GET_CODE (*recog_data.operand_loc[i]) == SUBREG)\n \t{\n-\t  recog_data.operand[i] = alter_subreg (recog_data.operand_loc[i]);\n+\t  recog_data.operand[i] = alter_subreg (recog_data.operand_loc[i], true);\n \t  changed = true;\n \t}\n       else if (GET_CODE (recog_data.operand[i]) == PLUS\n@@ -2987,7 +2987,7 @@ cleanup_subreg_operands (rtx insn)\n     {\n       if (GET_CODE (*recog_data.dup_loc[i]) == SUBREG)\n \t{\n-\t  *recog_data.dup_loc[i] = alter_subreg (recog_data.dup_loc[i]);\n+\t  *recog_data.dup_loc[i] = alter_subreg (recog_data.dup_loc[i], true);\n \t  changed = true;\n \t}\n       else if (GET_CODE (*recog_data.dup_loc[i]) == PLUS\n@@ -2999,11 +2999,11 @@ cleanup_subreg_operands (rtx insn)\n     df_insn_rescan (insn);\n }\n \n-/* If X is a SUBREG, replace it with a REG or a MEM,\n-   based on the thing it is a subreg of.  */\n+/* If X is a SUBREG, try to replace it with a REG or a MEM, based on\n+   the thing it is a subreg of.  Do it anyway if FINAL_P.  */\n \n rtx\n-alter_subreg (rtx *xp)\n+alter_subreg (rtx *xp, bool final_p)\n {\n   rtx x = *xp;\n   rtx y = SUBREG_REG (x);\n@@ -3027,16 +3027,19 @@ alter_subreg (rtx *xp)\n             offset += difference % UNITS_PER_WORD;\n         }\n \n-      *xp = adjust_address (y, GET_MODE (x), offset);\n+      if (final_p)\n+\t*xp = adjust_address (y, GET_MODE (x), offset);\n+      else\n+\t*xp = adjust_address_nv (y, GET_MODE (x), offset);\n     }\n   else\n     {\n       rtx new_rtx = simplify_subreg (GET_MODE (x), y, GET_MODE (y),\n-\t\t\t\t SUBREG_BYTE (x));\n+\t\t\t\t     SUBREG_BYTE (x));\n \n       if (new_rtx != 0)\n \t*xp = new_rtx;\n-      else if (REG_P (y))\n+      else if (final_p && REG_P (y))\n \t{\n \t  /* Simplify_subreg can't handle some REG cases, but we have to.  */\n \t  unsigned int regno;\n@@ -3076,7 +3079,7 @@ walk_alter_subreg (rtx *xp, bool *changed)\n \n     case SUBREG:\n       *changed = true;\n-      return alter_subreg (xp);\n+      return alter_subreg (xp, true);\n \n     default:\n       break;\n@@ -3682,7 +3685,7 @@ void\n output_operand (rtx x, int code ATTRIBUTE_UNUSED)\n {\n   if (x && GET_CODE (x) == SUBREG)\n-    x = alter_subreg (&x);\n+    x = alter_subreg (&x, true);\n \n   /* X must not be a pseudo reg.  */\n   gcc_assert (!x || !REG_P (x) || REGNO (x) < FIRST_PSEUDO_REGISTER);"}, {"sha": "90b551bbc4ddebe88533b6b4193e09da99959f64", "filename": "gcc/gcse.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fgcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fgcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgcse.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -3377,7 +3377,7 @@ calculate_bb_reg_pressure (void)\n   bitmap_iterator bi;\n \n \n-  ira_setup_eliminable_regset ();\n+  ira_setup_eliminable_regset (false);\n   curr_regs_live = BITMAP_ALLOC (&reg_obstack);\n   FOR_EACH_BB (bb)\n     {"}, {"sha": "fa80f246b423b5e4443defd5bf861563ed2d0380", "filename": "gcc/haifa-sched.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fhaifa-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fhaifa-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fhaifa-sched.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -6548,7 +6548,7 @@ sched_init (void)\n     sched_pressure = SCHED_PRESSURE_NONE;\n \n   if (sched_pressure != SCHED_PRESSURE_NONE)\n-    ira_setup_eliminable_regset ();\n+    ira_setup_eliminable_regset (false);\n \n   /* Initialize SPEC_INFO.  */\n   if (targetm.sched.set_sched_flags)"}, {"sha": "dd4c73b9482aa7d94846b62840b0b2d9001d568e", "filename": "gcc/ira-color.c", "status": "modified", "additions": 7, "deletions": 20, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fira-color.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fira-color.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira-color.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -2834,8 +2834,7 @@ color_pass (ira_loop_tree_node_t loop_tree_node)\n \t  exit_freq = ira_loop_edge_freq (subloop_node, regno, true);\n \t  enter_freq = ira_loop_edge_freq (subloop_node, regno, false);\n \t  ira_assert (regno < ira_reg_equiv_len);\n-\t  if (ira_reg_equiv_invariant_p[regno]\n-\t      || ira_reg_equiv_const[regno] != NULL_RTX)\n+\t  if (ira_equiv_no_lvalue_p (regno))\n \t    {\n \t      if (! ALLOCNO_ASSIGNED_P (subloop_allocno))\n \t\t{\n@@ -2940,9 +2939,7 @@ move_spill_restore (void)\n \t\t copies and the reload pass can spill the allocno set\n \t\t by copy although the allocno will not get memory\n \t\t slot.  */\n-\t      || (regno < ira_reg_equiv_len\n-\t\t  && (ira_reg_equiv_invariant_p[regno]\n-\t\t      || ira_reg_equiv_const[regno] != NULL_RTX))\n+\t      || ira_equiv_no_lvalue_p (regno)\n \t      || !bitmap_bit_p (loop_node->border_allocnos, ALLOCNO_NUM (a)))\n \t    continue;\n \t  mode = ALLOCNO_MODE (a);\n@@ -3366,9 +3363,7 @@ coalesce_allocnos (void)\n       a = ira_allocnos[j];\n       regno = ALLOCNO_REGNO (a);\n       if (! ALLOCNO_ASSIGNED_P (a) || ALLOCNO_HARD_REGNO (a) >= 0\n-\t  || (regno < ira_reg_equiv_len\n-\t      && (ira_reg_equiv_const[regno] != NULL_RTX\n-\t\t  || ira_reg_equiv_invariant_p[regno])))\n+\t  || ira_equiv_no_lvalue_p (regno))\n \tcontinue;\n       for (cp = ALLOCNO_COPIES (a); cp != NULL; cp = next_cp)\n \t{\n@@ -3383,9 +3378,7 @@ coalesce_allocnos (void)\n \t      if ((cp->insn != NULL || cp->constraint_p)\n \t\t  && ALLOCNO_ASSIGNED_P (cp->second)\n \t\t  && ALLOCNO_HARD_REGNO (cp->second) < 0\n-\t\t  && (regno >= ira_reg_equiv_len\n-\t\t      || (! ira_reg_equiv_invariant_p[regno]\n-\t\t\t  && ira_reg_equiv_const[regno] == NULL_RTX)))\n+\t\t  && ! ira_equiv_no_lvalue_p (regno))\n \t\tsorted_copies[cp_num++] = cp;\n \t    }\n \t  else if (cp->second == a)\n@@ -3651,19 +3644,15 @@ coalesce_spill_slots (ira_allocno_t *spilled_coalesced_allocnos, int num)\n       allocno = spilled_coalesced_allocnos[i];\n       if (ALLOCNO_COALESCE_DATA (allocno)->first != allocno\n \t  || bitmap_bit_p (set_jump_crosses, ALLOCNO_REGNO (allocno))\n-\t  || (ALLOCNO_REGNO (allocno) < ira_reg_equiv_len\n-\t      && (ira_reg_equiv_const[ALLOCNO_REGNO (allocno)] != NULL_RTX\n-\t\t  || ira_reg_equiv_invariant_p[ALLOCNO_REGNO (allocno)])))\n+\t  || ira_equiv_no_lvalue_p (ALLOCNO_REGNO (allocno)))\n \tcontinue;\n       for (j = 0; j < i; j++)\n \t{\n \t  a = spilled_coalesced_allocnos[j];\n \t  n = ALLOCNO_COALESCE_DATA (a)->temp;\n \t  if (ALLOCNO_COALESCE_DATA (a)->first == a\n \t      && ! bitmap_bit_p (set_jump_crosses, ALLOCNO_REGNO (a))\n-\t      && (ALLOCNO_REGNO (a) >= ira_reg_equiv_len\n-\t\t  || (! ira_reg_equiv_invariant_p[ALLOCNO_REGNO (a)]\n-\t\t      && ira_reg_equiv_const[ALLOCNO_REGNO (a)] == NULL_RTX))\n+\t      && ! ira_equiv_no_lvalue_p (ALLOCNO_REGNO (a))\n \t      && ! slot_coalesced_allocno_live_ranges_intersect_p (allocno, n))\n \t    break;\n \t}\n@@ -3771,9 +3760,7 @@ ira_sort_regnos_for_alter_reg (int *pseudo_regnos, int n,\n       allocno = spilled_coalesced_allocnos[i];\n       if (ALLOCNO_COALESCE_DATA (allocno)->first != allocno\n \t  || ALLOCNO_HARD_REGNO (allocno) >= 0\n-\t  || (ALLOCNO_REGNO (allocno) < ira_reg_equiv_len\n-\t      && (ira_reg_equiv_const[ALLOCNO_REGNO (allocno)] != NULL_RTX\n-\t\t  || ira_reg_equiv_invariant_p[ALLOCNO_REGNO (allocno)])))\n+\t  || ira_equiv_no_lvalue_p (ALLOCNO_REGNO (allocno)))\n \tcontinue;\n       if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n \tfprintf (ira_dump_file, \"      Slot %d (freq,size):\", slot_num);"}, {"sha": "683d47eba80d2170e2d20de9cf18a73d5a3a4201", "filename": "gcc/ira-emit.c", "status": "modified", "additions": 21, "deletions": 16, "changes": 37, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fira-emit.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fira-emit.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira-emit.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -340,6 +340,7 @@ ira_create_new_reg (rtx original_reg)\n   if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n     fprintf (ira_dump_file, \"      Creating newreg=%i from oldreg=%i\\n\",\n \t     REGNO (new_reg), REGNO (original_reg));\n+  ira_expand_reg_equiv ();\n   return new_reg;\n }\n \n@@ -518,8 +519,7 @@ generate_edge_moves (edge e)\n \t/* Remove unnecessary stores at the region exit.  We should do\n \t   this for readonly memory for sure and this is guaranteed by\n \t   that we never generate moves on region borders (see\n-\t   checking ira_reg_equiv_invariant_p in function\n-\t   change_loop).  */\n+\t   checking in function change_loop).  */\n  \tif (ALLOCNO_HARD_REGNO (dest_allocno) < 0\n \t    && ALLOCNO_HARD_REGNO (src_allocno) >= 0\n \t    && store_can_be_removed_p (src_allocno, dest_allocno))\n@@ -613,8 +613,7 @@ change_loop (ira_loop_tree_node_t node)\n \t\t  /* don't create copies because reload can spill an\n \t\t     allocno set by copy although the allocno will not\n \t\t     get memory slot.  */\n-\t\t  || ira_reg_equiv_invariant_p[regno]\n-\t\t  || ira_reg_equiv_const[regno] != NULL_RTX))\n+\t\t  || ira_equiv_no_lvalue_p (regno)))\n \t    continue;\n \t  original_reg = allocno_emit_reg (allocno);\n \t  if (parent_allocno == NULL\n@@ -902,17 +901,22 @@ modify_move_list (move_t list)\n static rtx\n emit_move_list (move_t list, int freq)\n {\n-  int cost, regno;\n-  rtx result, insn, set, to;\n+  rtx to, from, dest;\n+  int to_regno, from_regno, cost, regno;\n+  rtx result, insn, set;\n   enum machine_mode mode;\n   enum reg_class aclass;\n \n+  grow_reg_equivs ();\n   start_sequence ();\n   for (; list != NULL; list = list->next)\n     {\n       start_sequence ();\n-      emit_move_insn (allocno_emit_reg (list->to),\n-\t\t      allocno_emit_reg (list->from));\n+      to = allocno_emit_reg (list->to);\n+      to_regno = REGNO (to);\n+      from = allocno_emit_reg (list->from);\n+      from_regno = REGNO (from);\n+      emit_move_insn (to, from);\n       list->insn = get_insns ();\n       end_sequence ();\n       for (insn = list->insn; insn != NULL_RTX; insn = NEXT_INSN (insn))\n@@ -928,21 +932,22 @@ emit_move_list (move_t list, int freq)\n \t     to use the equivalence.  */\n \t  if ((set = single_set (insn)) != NULL_RTX)\n \t    {\n-\t      to = SET_DEST (set);\n-\t      if (GET_CODE (to) == SUBREG)\n-\t\tto = SUBREG_REG (to);\n-\t      ira_assert (REG_P (to));\n-\t      regno = REGNO (to);\n+\t      dest = SET_DEST (set);\n+\t      if (GET_CODE (dest) == SUBREG)\n+\t\tdest = SUBREG_REG (dest);\n+\t      ira_assert (REG_P (dest));\n+\t      regno = REGNO (dest);\n \t      if (regno >= ira_reg_equiv_len\n-\t\t  || (! ira_reg_equiv_invariant_p[regno]\n-\t\t      && ira_reg_equiv_const[regno] == NULL_RTX))\n+\t\t  || (ira_reg_equiv[regno].invariant == NULL_RTX\n+\t\t      && ira_reg_equiv[regno].constant == NULL_RTX))\n \t\tcontinue; /* regno has no equivalence.  */\n \t      ira_assert ((int) VEC_length (reg_equivs_t, reg_equivs)\n-\t\t\t  >= ira_reg_equiv_len);\n+\t\t\t  > regno);\n \t      reg_equiv_init (regno)\n \t\t= gen_rtx_INSN_LIST (VOIDmode, insn, reg_equiv_init (regno));\n \t    }\n \t}\n+      ira_update_equiv_info_by_shuffle_insn (to_regno, from_regno, list->insn);\n       emit_insn (list->insn);\n       mode = ALLOCNO_MODE (list->to);\n       aclass = ALLOCNO_CLASS (list->to);"}, {"sha": "a64e3a14afe423c23d9bcc5a72f1bb3adb06adb1", "filename": "gcc/ira-int.h", "status": "modified", "additions": 16, "deletions": 27, "changes": 43, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fira-int.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fira-int.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira-int.h?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -795,11 +795,6 @@ struct target_ira_int {\n   /* Map class->true if class is a pressure class, false otherwise. */\n   bool x_ira_reg_pressure_class_p[N_REG_CLASSES];\n \n-  /* Register class subset relation: TRUE if the first class is a subset\n-     of the second one considering only hard registers available for the\n-     allocation.  */\n-  int x_ira_class_subset_p[N_REG_CLASSES][N_REG_CLASSES];\n-\n   /* Array of the number of hard registers of given class which are\n      available for allocation.  The order is defined by the hard\n      register numbers.  */\n@@ -852,13 +847,8 @@ struct target_ira_int {\n      taking all hard-registers including fixed ones into account.  */\n   enum reg_class x_ira_reg_class_intersect[N_REG_CLASSES][N_REG_CLASSES];\n \n-  /* True if the two classes (that is calculated taking only hard\n-     registers available for allocation into account; are\n-     intersected.  */\n-  bool x_ira_reg_classes_intersect_p[N_REG_CLASSES][N_REG_CLASSES];\n-\n   /* Classes with end marker LIM_REG_CLASSES which are intersected with\n-     given class (the first index;.  That includes given class itself.\n+     given class (the first index).  That includes given class itself.\n      This is calculated taking only hard registers available for\n      allocation into account.  */\n   enum reg_class x_ira_reg_class_super_classes[N_REG_CLASSES][N_REG_CLASSES];\n@@ -875,7 +865,7 @@ struct target_ira_int {\n \n   /* For each reg class, table listing all the classes contained in it\n      (excluding the class itself.  Non-allocatable registers are\n-     excluded from the consideration;.  */\n+     excluded from the consideration).  */\n   enum reg_class x_alloc_reg_class_subclasses[N_REG_CLASSES][N_REG_CLASSES];\n \n   /* Array whose values are hard regset of hard registers for which\n@@ -908,8 +898,6 @@ extern struct target_ira_int *this_target_ira_int;\n   (this_target_ira_int->x_ira_reg_allocno_class_p)\n #define ira_reg_pressure_class_p \\\n   (this_target_ira_int->x_ira_reg_pressure_class_p)\n-#define ira_class_subset_p \\\n-  (this_target_ira_int->x_ira_class_subset_p)\n #define ira_non_ordered_class_hard_regs \\\n   (this_target_ira_int->x_ira_non_ordered_class_hard_regs)\n #define ira_class_hard_reg_index \\\n@@ -928,8 +916,6 @@ extern struct target_ira_int *this_target_ira_int;\n   (this_target_ira_int->x_ira_uniform_class_p)\n #define ira_reg_class_intersect \\\n   (this_target_ira_int->x_ira_reg_class_intersect)\n-#define ira_reg_classes_intersect_p \\\n-  (this_target_ira_int->x_ira_reg_classes_intersect_p)\n #define ira_reg_class_super_classes \\\n   (this_target_ira_int->x_ira_reg_class_super_classes)\n #define ira_reg_class_subunion \\\n@@ -950,17 +936,6 @@ extern void ira_debug_disposition (void);\n extern void ira_debug_allocno_classes (void);\n extern void ira_init_register_move_cost (enum machine_mode);\n \n-/* The length of the two following arrays.  */\n-extern int ira_reg_equiv_len;\n-\n-/* The element value is TRUE if the corresponding regno value is\n-   invariant.  */\n-extern bool *ira_reg_equiv_invariant_p;\n-\n-/* The element value is equiv constant of given pseudo-register or\n-   NULL_RTX.  */\n-extern rtx *ira_reg_equiv_const;\n-\n /* ira-build.c */\n \n /* The current loop tree node and its regno allocno map.  */\n@@ -1044,6 +1019,20 @@ extern void ira_emit (bool);\n \n \f\n \n+/* Return true if equivalence of pseudo REGNO is not a lvalue.  */\n+static inline bool\n+ira_equiv_no_lvalue_p (int regno)\n+{\n+  if (regno >= ira_reg_equiv_len)\n+    return false;\n+  return (ira_reg_equiv[regno].constant != NULL_RTX\n+\t  || ira_reg_equiv[regno].invariant != NULL_RTX\n+\t  || (ira_reg_equiv[regno].memory != NULL_RTX\n+\t      && MEM_READONLY_P (ira_reg_equiv[regno].memory)));\n+}\n+\n+\f\n+\n /* Initialize register costs for MODE if necessary.  */\n static inline void\n ira_init_register_move_cost_if_necessary (enum machine_mode mode)"}, {"sha": "e91d37ddaa56e0fdcaff885de13ce6547686d172", "filename": "gcc/ira.c", "status": "modified", "additions": 429, "deletions": 155, "changes": 584, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fira.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fira.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -382,6 +382,7 @@ along with GCC; see the file COPYING3.  If not see\n #include \"function.h\"\n #include \"ggc.h\"\n #include \"ira-int.h\"\n+#include \"lra.h\"\n #include \"dce.h\"\n #include \"dbgcnt.h\"\n \n@@ -1201,6 +1202,7 @@ setup_reg_class_relations (void)\n \t{\n \t  ira_reg_classes_intersect_p[cl1][cl2] = false;\n \t  ira_reg_class_intersect[cl1][cl2] = NO_REGS;\n+\t  ira_reg_class_subset[cl1][cl2] = NO_REGS;\n \t  COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl1]);\n \t  AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n \t  COPY_HARD_REG_SET (temp_set2, reg_class_contents[cl2]);\n@@ -1248,35 +1250,54 @@ setup_reg_class_relations (void)\n \t  COPY_HARD_REG_SET (union_set, reg_class_contents[cl1]);\n \t  IOR_HARD_REG_SET (union_set, reg_class_contents[cl2]);\n \t  AND_COMPL_HARD_REG_SET (union_set, no_unit_alloc_regs);\n-\t  for (i = 0; i < ira_important_classes_num; i++)\n+\t  for (cl3 = 0; cl3 < N_REG_CLASSES; cl3++)\n \t    {\n-\t      cl3 = ira_important_classes[i];\n \t      COPY_HARD_REG_SET (temp_hard_regset, reg_class_contents[cl3]);\n \t      AND_COMPL_HARD_REG_SET (temp_hard_regset, no_unit_alloc_regs);\n \t      if (hard_reg_set_subset_p (temp_hard_regset, intersection_set))\n \t\t{\n \t\t  /* CL3 allocatable hard register set is inside of\n \t\t     intersection of allocatable hard register sets\n \t\t     of CL1 and CL2.  */\n+\t\t  if (important_class_p[cl3])\n+\t\t    {\n+\t\t      COPY_HARD_REG_SET\n+\t\t\t(temp_set2,\n+\t\t\t reg_class_contents\n+\t\t\t [(int) ira_reg_class_intersect[cl1][cl2]]);\n+\t\t      AND_COMPL_HARD_REG_SET (temp_set2, no_unit_alloc_regs);\n+\t\t      if (! hard_reg_set_subset_p (temp_hard_regset, temp_set2)\n+\t\t\t  /* If the allocatable hard register sets are\n+\t\t\t     the same, prefer GENERAL_REGS or the\n+\t\t\t     smallest class for debugging\n+\t\t\t     purposes.  */\n+\t\t\t  || (hard_reg_set_equal_p (temp_hard_regset, temp_set2)\n+\t\t\t      && (cl3 == GENERAL_REGS\n+\t\t\t\t  || ((ira_reg_class_intersect[cl1][cl2]\n+\t\t\t\t       != GENERAL_REGS)\n+\t\t\t\t      && hard_reg_set_subset_p\n+\t\t\t\t         (reg_class_contents[cl3],\n+\t\t\t\t\t  reg_class_contents\n+\t\t\t\t\t  [(int)\n+\t\t\t\t\t   ira_reg_class_intersect[cl1][cl2]])))))\n+\t\t\tira_reg_class_intersect[cl1][cl2] = (enum reg_class) cl3;\n+\t\t    }\n \t\t  COPY_HARD_REG_SET\n \t\t    (temp_set2,\n-\t\t     reg_class_contents[(int)\n-\t\t\t\t\tira_reg_class_intersect[cl1][cl2]]);\n+\t\t     reg_class_contents[(int) ira_reg_class_subset[cl1][cl2]]);\n \t\t  AND_COMPL_HARD_REG_SET (temp_set2, no_unit_alloc_regs);\n-\t \t  if (! hard_reg_set_subset_p (temp_hard_regset, temp_set2)\n-\t\t      /* If the allocatable hard register sets are the\n-\t\t\t same, prefer GENERAL_REGS or the smallest\n-\t\t\t class for debugging purposes.  */\n+\t\t  if (! hard_reg_set_subset_p (temp_hard_regset, temp_set2)\n+\t\t      /* Ignore unavailable hard registers and prefer\n+\t\t\t smallest class for debugging purposes.  */\n \t\t      || (hard_reg_set_equal_p (temp_hard_regset, temp_set2)\n-\t\t\t  && (cl3 == GENERAL_REGS\n-\t\t\t      || (ira_reg_class_intersect[cl1][cl2] != GENERAL_REGS\n-\t\t\t\t  && hard_reg_set_subset_p\n-\t\t\t\t     (reg_class_contents[cl3],\n-\t\t\t\t      reg_class_contents\n-\t\t\t\t      [(int) ira_reg_class_intersect[cl1][cl2]])))))\n-\t\t    ira_reg_class_intersect[cl1][cl2] = (enum reg_class) cl3;\n+\t\t\t  && hard_reg_set_subset_p\n+\t\t\t     (reg_class_contents[cl3],\n+\t\t\t      reg_class_contents\n+\t\t\t      [(int) ira_reg_class_subset[cl1][cl2]])))\n+\t\t    ira_reg_class_subset[cl1][cl2] = (enum reg_class) cl3;\n \t\t}\n-\t      if (hard_reg_set_subset_p (temp_hard_regset, union_set))\n+\t      if (important_class_p[cl3]\n+\t\t  && hard_reg_set_subset_p (temp_hard_regset, union_set))\n \t\t{\n \t\t  /* CL3 allocatbale hard register set is inside of\n \t\t     union of allocatable hard register sets of CL1\n@@ -1632,6 +1653,7 @@ void\n ira_init_once (void)\n {\n   ira_init_costs_once ();\n+  lra_init_once ();\n }\n \n /* Free ira_max_register_move_cost, ira_may_move_in_cost and\n@@ -1679,6 +1701,7 @@ ira_init (void)\n   clarify_prohibited_class_mode_regs ();\n   setup_hard_regno_aclass ();\n   ira_init_costs ();\n+  lra_init ();\n }\n \n /* Function called once at the end of compiler work.  */\n@@ -1687,6 +1710,7 @@ ira_finish_once (void)\n {\n   ira_finish_costs_once ();\n   free_register_move_costs ();\n+  lra_finish_once ();\n }\n \n \f\n@@ -1823,9 +1847,11 @@ compute_regs_asm_clobbered (void)\n }\n \n \n-/* Set up ELIMINABLE_REGSET, IRA_NO_ALLOC_REGS, and REGS_EVER_LIVE.  */\n+/* Set up ELIMINABLE_REGSET, IRA_NO_ALLOC_REGS, and REGS_EVER_LIVE.\n+   If the function is called from IRA (not from the insn scheduler or\n+   RTL loop invariant motion), FROM_IRA_P is true.  */\n void\n-ira_setup_eliminable_regset (void)\n+ira_setup_eliminable_regset (bool from_ira_p)\n {\n #ifdef ELIMINABLE_REGS\n   int i;\n@@ -1835,7 +1861,7 @@ ira_setup_eliminable_regset (void)\n      sp for alloca.  So we can't eliminate the frame pointer in that\n      case.  At some point, we should improve this by emitting the\n      sp-adjusting insns for this case.  */\n-  int need_fp\n+  frame_pointer_needed\n     = (! flag_omit_frame_pointer\n        || (cfun->calls_alloca && EXIT_IGNORE_STACK)\n        /* We need the frame pointer to catch stack overflow exceptions\n@@ -1845,8 +1871,14 @@ ira_setup_eliminable_regset (void)\n        || crtl->stack_realign_needed\n        || targetm.frame_pointer_required ());\n \n-  frame_pointer_needed = need_fp;\n+  if (from_ira_p && ira_use_lra_p)\n+    /* It can change FRAME_POINTER_NEEDED.  We call it only from IRA\n+       because it is expensive.  */\n+    lra_init_elimination ();\n \n+  if (frame_pointer_needed)\n+    df_set_regs_ever_live (HARD_FRAME_POINTER_REGNUM, true);\n+    \n   COPY_HARD_REG_SET (ira_no_alloc_regs, no_unit_alloc_regs);\n   CLEAR_HARD_REG_SET (eliminable_regset);\n \n@@ -1859,7 +1891,7 @@ ira_setup_eliminable_regset (void)\n     {\n       bool cannot_elim\n \t= (! targetm.can_eliminate (eliminables[i].from, eliminables[i].to)\n-\t   || (eliminables[i].to == STACK_POINTER_REGNUM && need_fp));\n+\t   || (eliminables[i].to == STACK_POINTER_REGNUM && frame_pointer_needed));\n \n       if (!TEST_HARD_REG_BIT (crtl->asm_clobbers, eliminables[i].from))\n \t{\n@@ -1878,10 +1910,10 @@ ira_setup_eliminable_regset (void)\n   if (!TEST_HARD_REG_BIT (crtl->asm_clobbers, HARD_FRAME_POINTER_REGNUM))\n     {\n       SET_HARD_REG_BIT (eliminable_regset, HARD_FRAME_POINTER_REGNUM);\n-      if (need_fp)\n+      if (frame_pointer_needed)\n \tSET_HARD_REG_BIT (ira_no_alloc_regs, HARD_FRAME_POINTER_REGNUM);\n     }\n-  else if (need_fp)\n+  else if (frame_pointer_needed)\n     error (\"%s cannot be used in asm here\",\n \t   reg_names[HARD_FRAME_POINTER_REGNUM]);\n   else\n@@ -1892,10 +1924,10 @@ ira_setup_eliminable_regset (void)\n   if (!TEST_HARD_REG_BIT (crtl->asm_clobbers, HARD_FRAME_POINTER_REGNUM))\n     {\n       SET_HARD_REG_BIT (eliminable_regset, FRAME_POINTER_REGNUM);\n-      if (need_fp)\n+      if (frame_pointer_needed)\n \tSET_HARD_REG_BIT (ira_no_alloc_regs, FRAME_POINTER_REGNUM);\n     }\n-  else if (need_fp)\n+  else if (frame_pointer_needed)\n     error (\"%s cannot be used in asm here\", reg_names[FRAME_POINTER_REGNUM]);\n   else\n     df_set_regs_ever_live (FRAME_POINTER_REGNUM, true);\n@@ -1904,66 +1936,6 @@ ira_setup_eliminable_regset (void)\n \n \f\n \n-/* The length of the following two arrays.  */\n-int ira_reg_equiv_len;\n-\n-/* The element value is TRUE if the corresponding regno value is\n-   invariant.  */\n-bool *ira_reg_equiv_invariant_p;\n-\n-/* The element value is equiv constant of given pseudo-register or\n-   NULL_RTX.  */\n-rtx *ira_reg_equiv_const;\n-\n-/* Set up the two arrays declared above.  */\n-static void\n-find_reg_equiv_invariant_const (void)\n-{\n-  unsigned int i;\n-  bool invariant_p;\n-  rtx list, insn, note, constant, x;\n-\n-  for (i = FIRST_PSEUDO_REGISTER; i < VEC_length (reg_equivs_t, reg_equivs); i++)\n-    {\n-      constant = NULL_RTX;\n-      invariant_p = false;\n-      for (list = reg_equiv_init (i); list != NULL_RTX; list = XEXP (list, 1))\n-\t{\n-\t  insn = XEXP (list, 0);\n-\t  note = find_reg_note (insn, REG_EQUIV, NULL_RTX);\n-\n-\t  if (note == NULL_RTX)\n-\t    continue;\n-\n-\t  x = XEXP (note, 0);\n-\n-\t  if (! CONSTANT_P (x)\n-\t      || ! flag_pic || LEGITIMATE_PIC_OPERAND_P (x))\n-\t    {\n-\t      /* It can happen that a REG_EQUIV note contains a MEM\n-\t\t that is not a legitimate memory operand.  As later\n-\t\t stages of the reload assume that all addresses found\n-\t\t in the reg_equiv_* arrays were originally legitimate,\n-\t\t we ignore such REG_EQUIV notes.  */\n-\t      if (memory_operand (x, VOIDmode))\n-\t\tinvariant_p = MEM_READONLY_P (x);\n-\t      else if (function_invariant_p (x))\n-\t\t{\n-\t\t  if (GET_CODE (x) == PLUS\n-\t\t      || x == frame_pointer_rtx || x == arg_pointer_rtx)\n-\t\t    invariant_p = true;\n-\t\t  else\n-\t\t    constant = x;\n-\t\t}\n-\t    }\n-\t}\n-      ira_reg_equiv_invariant_p[i] = invariant_p;\n-      ira_reg_equiv_const[i] = constant;\n-    }\n-}\n-\n-\f\n-\n /* Vector of substitutions of register numbers,\n    used to map pseudo regs into hardware regs.\n    This is set up as a result of register allocation.\n@@ -1984,6 +1956,8 @@ setup_reg_renumber (void)\n   caller_save_needed = 0;\n   FOR_EACH_ALLOCNO (a, ai)\n     {\n+      if (ira_use_lra_p && ALLOCNO_CAP_MEMBER (a) != NULL)\n+\tcontinue;\n       /* There are no caps at this point.  */\n       ira_assert (ALLOCNO_CAP_MEMBER (a) == NULL);\n       if (! ALLOCNO_ASSIGNED_P (a))\n@@ -2015,9 +1989,7 @@ setup_reg_renumber (void)\n \t      ira_assert (!optimize || flag_caller_saves\n \t\t\t  || (ALLOCNO_CALLS_CROSSED_NUM (a)\n \t\t\t      == ALLOCNO_CHEAP_CALLS_CROSSED_NUM (a))\n-\t\t\t  || regno >= ira_reg_equiv_len\n-\t\t\t  || ira_reg_equiv_const[regno]\n-\t\t\t  || ira_reg_equiv_invariant_p[regno]);\n+\t\t\t  || ira_equiv_no_lvalue_p (regno));\n \t      caller_save_needed = 1;\n \t    }\n \t}\n@@ -2184,6 +2156,109 @@ check_allocation (void)\n }\n #endif\n \n+/* Allocate REG_EQUIV_INIT.  Set up it from IRA_REG_EQUIV which should\n+   be already calculated.  */\n+static void\n+setup_reg_equiv_init (void)\n+{\n+  int i;\n+  int max_regno = max_reg_num ();\n+\n+  for (i = 0; i < max_regno; i++)\n+    reg_equiv_init (i) = ira_reg_equiv[i].init_insns;\n+}\n+\n+/* Update equiv regno from movement of FROM_REGNO to TO_REGNO.  INSNS\n+   are insns which were generated for such movement.  It is assumed\n+   that FROM_REGNO and TO_REGNO always have the same value at the\n+   point of any move containing such registers. This function is used\n+   to update equiv info for register shuffles on the region borders\n+   and for caller save/restore insns.  */\n+void\n+ira_update_equiv_info_by_shuffle_insn (int to_regno, int from_regno, rtx insns)\n+{\n+  rtx insn, x, note;\n+\n+  if (! ira_reg_equiv[from_regno].defined_p\n+      && (! ira_reg_equiv[to_regno].defined_p\n+\t  || ((x = ira_reg_equiv[to_regno].memory) != NULL_RTX\n+\t      && ! MEM_READONLY_P (x))))\n+      return;\n+  insn = insns;\n+  if (NEXT_INSN (insn) != NULL_RTX)\n+    {\n+      if (! ira_reg_equiv[to_regno].defined_p)\n+\t{\n+\t  ira_assert (ira_reg_equiv[to_regno].init_insns == NULL_RTX);\n+\t  return;\n+\t}\n+      ira_reg_equiv[to_regno].defined_p = false;\n+      ira_reg_equiv[to_regno].memory\n+\t= ira_reg_equiv[to_regno].constant\n+\t= ira_reg_equiv[to_regno].invariant\n+\t= ira_reg_equiv[to_regno].init_insns = NULL_RTX;\n+      if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\tfprintf (ira_dump_file,\n+\t\t \"      Invalidating equiv info for reg %d\\n\", to_regno);\n+      return;\n+    }\n+  /* It is possible that FROM_REGNO still has no equivalence because\n+     in shuffles to_regno<-from_regno and from_regno<-to_regno the 2nd\n+     insn was not processed yet.  */\n+  if (ira_reg_equiv[from_regno].defined_p)\n+    {\n+      ira_reg_equiv[to_regno].defined_p = true;\n+      if ((x = ira_reg_equiv[from_regno].memory) != NULL_RTX)\n+\t{\n+\t  ira_assert (ira_reg_equiv[from_regno].invariant == NULL_RTX\n+\t\t      && ira_reg_equiv[from_regno].constant == NULL_RTX);\n+\t  ira_assert (ira_reg_equiv[to_regno].memory == NULL_RTX\n+\t\t      || rtx_equal_p (ira_reg_equiv[to_regno].memory, x));\n+\t  ira_reg_equiv[to_regno].memory = x;\n+\t  if (! MEM_READONLY_P (x))\n+\t    /* We don't add the insn to insn init list because memory\n+\t       equivalence is just to say what memory is better to use\n+\t       when the pseudo is spilled.  */\n+\t    return;\n+\t}\n+      else if ((x = ira_reg_equiv[from_regno].constant) != NULL_RTX)\n+\t{\n+\t  ira_assert (ira_reg_equiv[from_regno].invariant == NULL_RTX);\n+\t  ira_assert (ira_reg_equiv[to_regno].constant == NULL_RTX\n+\t\t      || rtx_equal_p (ira_reg_equiv[to_regno].constant, x));\n+\t  ira_reg_equiv[to_regno].constant = x;\n+\t}\n+      else\n+\t{\n+\t  x = ira_reg_equiv[from_regno].invariant;\n+\t  ira_assert (x != NULL_RTX);\n+\t  ira_assert (ira_reg_equiv[to_regno].invariant == NULL_RTX\n+\t\t      || rtx_equal_p (ira_reg_equiv[to_regno].invariant, x));\n+\t  ira_reg_equiv[to_regno].invariant = x;\n+\t}\n+      if (find_reg_note (insn, REG_EQUIV, x) == NULL_RTX)\n+\t{\n+\t  note = set_unique_reg_note (insn, REG_EQUIV, x);\n+\t  gcc_assert (note != NULL_RTX);\n+\t  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+\t    {\n+\t      fprintf (ira_dump_file,\n+\t\t       \"      Adding equiv note to insn %u for reg %d \",\n+\t\t       INSN_UID (insn), to_regno);\n+\t      print_value_slim (ira_dump_file, x, 1);\n+\t      fprintf (ira_dump_file, \"\\n\");\n+\t    }\n+\t}\n+    }\n+  ira_reg_equiv[to_regno].init_insns\n+    = gen_rtx_INSN_LIST (VOIDmode, insn,\n+\t\t\t ira_reg_equiv[to_regno].init_insns);\n+  if (internal_flag_ira_verbose > 3 && ira_dump_file != NULL)\n+    fprintf (ira_dump_file,\n+\t     \"      Adding equiv init move insn %u to reg %d\\n\",\n+\t     INSN_UID (insn), to_regno);\n+}\n+\n /* Fix values of array REG_EQUIV_INIT after live range splitting done\n    by IRA.  */\n static void\n@@ -2221,6 +2296,7 @@ fix_reg_equiv_init (void)\n \t      prev = x;\n \t    else\n \t      {\n+\t\t/* Remove the wrong list element.  */\n \t\tif (prev == NULL_RTX)\n \t\t  reg_equiv_init (i) = next;\n \t\telse\n@@ -2360,6 +2436,46 @@ mark_elimination (int from, int to)\n \n \f\n \n+/* The length of the following array.  */\n+int ira_reg_equiv_len;\n+\n+/* Info about equiv. info for each register.  */\n+struct ira_reg_equiv *ira_reg_equiv;\n+\n+/* Expand ira_reg_equiv if necessary.  */\n+void\n+ira_expand_reg_equiv (void)\n+{\n+  int old = ira_reg_equiv_len;\n+\n+  if (ira_reg_equiv_len > max_reg_num ())\n+    return;\n+  ira_reg_equiv_len = max_reg_num () * 3 / 2 + 1;\n+  ira_reg_equiv\n+    = (struct ira_reg_equiv *) xrealloc (ira_reg_equiv,\n+\t\t\t\t\t ira_reg_equiv_len\n+\t\t\t\t\t * sizeof (struct ira_reg_equiv));\n+  gcc_assert (old < ira_reg_equiv_len);\n+  memset (ira_reg_equiv + old, 0,\n+\t  sizeof (struct ira_reg_equiv) * (ira_reg_equiv_len - old));\n+}\n+\n+static void\n+init_reg_equiv (void)\n+{\n+  ira_reg_equiv_len = 0;\n+  ira_reg_equiv = NULL;\n+  ira_expand_reg_equiv ();\n+}\n+\n+static void\n+finish_reg_equiv (void)\n+{\n+  free (ira_reg_equiv);\n+}\n+\n+\f\n+\n struct equivalence\n {\n   /* Set when a REG_EQUIV note is found or created.  Use to\n@@ -2733,7 +2849,8 @@ no_equiv (rtx reg, const_rtx store ATTRIBUTE_UNUSED,\n      should keep their initialization insns.  */\n   if (reg_equiv[regno].is_arg_equivalence)\n     return;\n-  reg_equiv_init (regno) = NULL_RTX;\n+  ira_reg_equiv[regno].defined_p = false;\n+  ira_reg_equiv[regno].init_insns = NULL_RTX;\n   for (; list; list =  XEXP (list, 1))\n     {\n       rtx insn = XEXP (list, 0);\n@@ -2769,7 +2886,7 @@ static int recorded_label_ref;\n    value into the using insn.  If it succeeds, we can eliminate the\n    register completely.\n \n-   Initialize the REG_EQUIV_INIT array of initializing insns.\n+   Initialize init_insns in ira_reg_equiv array.\n \n    Return non-zero if jump label rebuilding should be done.  */\n static int\n@@ -2844,14 +2961,16 @@ update_equiv_regs (void)\n \t      gcc_assert (REG_P (dest));\n \t      regno = REGNO (dest);\n \n-\t      /* Note that we don't want to clear reg_equiv_init even if there\n-\t\t are multiple sets of this register.  */\n+\t      /* Note that we don't want to clear init_insns in\n+\t\t ira_reg_equiv even if there are multiple sets of this\n+\t\t register.  */\n \t      reg_equiv[regno].is_arg_equivalence = 1;\n \n \t      /* Record for reload that this is an equivalencing insn.  */\n \t      if (rtx_equal_p (src, XEXP (note, 0)))\n-\t\treg_equiv_init (regno)\n-\t\t  = gen_rtx_INSN_LIST (VOIDmode, insn, reg_equiv_init (regno));\n+\t\tira_reg_equiv[regno].init_insns\n+\t\t  = gen_rtx_INSN_LIST (VOIDmode, insn,\n+\t\t\t\t       ira_reg_equiv[regno].init_insns);\n \n \t      /* Continue normally in case this is a candidate for\n \t\t replacements.  */\n@@ -2951,8 +3070,9 @@ update_equiv_regs (void)\n \t      /* If we haven't done so, record for reload that this is an\n \t\t equivalencing insn.  */\n \t      if (!reg_equiv[regno].is_arg_equivalence)\n-\t\treg_equiv_init (regno)\n-\t\t  = gen_rtx_INSN_LIST (VOIDmode, insn, reg_equiv_init (regno));\n+\t\tira_reg_equiv[regno].init_insns\n+\t\t  = gen_rtx_INSN_LIST (VOIDmode, insn,\n+\t\t\t\t       ira_reg_equiv[regno].init_insns);\n \n \t      /* Record whether or not we created a REG_EQUIV note for a LABEL_REF.\n \t\t We might end up substituting the LABEL_REF for uses of the\n@@ -3052,7 +3172,7 @@ update_equiv_regs (void)\n \t    {\n \t      /* This insn makes the equivalence, not the one initializing\n \t\t the register.  */\n-\t      reg_equiv_init (regno)\n+\t      ira_reg_equiv[regno].init_insns\n \t\t= gen_rtx_INSN_LIST (VOIDmode, insn, NULL_RTX);\n \t      df_notes_rescan (init_insn);\n \t    }\n@@ -3106,9 +3226,10 @@ update_equiv_regs (void)\n \n \t\t  /* reg_equiv[REGNO].replace gets set only when\n \t\t     REG_N_REFS[REGNO] is 2, i.e. the register is set\n-\t\t     once and used once.  (If it were only set, but not used,\n-\t\t     flow would have deleted the setting insns.)  Hence\n-\t\t     there can only be one insn in reg_equiv[REGNO].init_insns.  */\n+\t\t     once and used once.  (If it were only set, but\n+\t\t     not used, flow would have deleted the setting\n+\t\t     insns.)  Hence there can only be one insn in\n+\t\t     reg_equiv[REGNO].init_insns.  */\n \t\t  gcc_assert (reg_equiv[regno].init_insns\n \t\t\t      && !XEXP (reg_equiv[regno].init_insns, 1));\n \t\t  equiv_insn = XEXP (reg_equiv[regno].init_insns, 0);\n@@ -3155,7 +3276,7 @@ update_equiv_regs (void)\n \t\t      reg_equiv[regno].init_insns\n \t\t\t= XEXP (reg_equiv[regno].init_insns, 1);\n \n-\t\t      reg_equiv_init (regno) = NULL_RTX;\n+\t\t      ira_reg_equiv[regno].init_insns = NULL_RTX;\n \t\t      bitmap_set_bit (cleared_regs, regno);\n \t\t    }\n \t\t  /* Move the initialization of the register to just before\n@@ -3188,7 +3309,7 @@ update_equiv_regs (void)\n \t\t      if (insn == BB_HEAD (bb))\n \t\t\tBB_HEAD (bb) = PREV_INSN (insn);\n \n-\t\t      reg_equiv_init (regno)\n+\t\t      ira_reg_equiv[regno].init_insns\n \t\t\t= gen_rtx_INSN_LIST (VOIDmode, new_insn, NULL_RTX);\n \t\t      bitmap_set_bit (cleared_regs, regno);\n \t\t    }\n@@ -3236,6 +3357,88 @@ update_equiv_regs (void)\n \n \f\n \n+/* Set up fields memory, constant, and invariant from init_insns in\n+   the structures of array ira_reg_equiv.  */\n+static void\n+setup_reg_equiv (void)\n+{\n+  int i;\n+  rtx elem, insn, set, x;\n+\n+  for (i = FIRST_PSEUDO_REGISTER; i < ira_reg_equiv_len; i++)\n+    for (elem = ira_reg_equiv[i].init_insns; elem; elem = XEXP (elem, 1))\n+      {\n+\tinsn = XEXP (elem, 0);\n+\tset = single_set (insn);\n+\t\n+\t/* Init insns can set up equivalence when the reg is a destination or\n+\t   a source (in this case the destination is memory).  */\n+\tif (set != 0 && (REG_P (SET_DEST (set)) || REG_P (SET_SRC (set))))\n+\t  {\n+\t    if ((x = find_reg_note (insn, REG_EQUIV, NULL_RTX)) != NULL)\n+\t      x = XEXP (x, 0);\n+\t    else if (REG_P (SET_DEST (set))\n+\t\t     && REGNO (SET_DEST (set)) == (unsigned int) i)\n+\t      x = SET_SRC (set);\n+\t    else\n+\t      {      \n+\t\tgcc_assert (REG_P (SET_SRC (set))\n+\t\t\t    && REGNO (SET_SRC (set)) == (unsigned int) i);\n+\t\tx = SET_DEST (set);\n+\t      }\n+\t    if (! function_invariant_p (x)\n+\t\t|| ! flag_pic\n+\t\t/* A function invariant is often CONSTANT_P but may\n+\t\t   include a register.  We promise to only pass\n+\t\t   CONSTANT_P objects to LEGITIMATE_PIC_OPERAND_P.  */\n+\t\t|| (CONSTANT_P (x) && LEGITIMATE_PIC_OPERAND_P (x)))\n+\t      {\n+\t\t/* It can happen that a REG_EQUIV note contains a MEM\n+\t\t   that is not a legitimate memory operand.  As later\n+\t\t   stages of reload assume that all addresses found in\n+\t\t   the lra_regno_equiv_* arrays were originally\n+\t\t   legitimate, we ignore such REG_EQUIV notes.  */\n+\t\tif (memory_operand (x, VOIDmode))\n+\t\t  {\n+\t\t    ira_reg_equiv[i].defined_p = true;\n+\t\t    ira_reg_equiv[i].memory = x;\n+\t\t    continue;\n+\t\t  }\n+\t\telse if (function_invariant_p (x))\n+\t\t  {\n+\t\t    enum machine_mode mode;\n+\t\t    \n+\t\t    mode = GET_MODE (SET_DEST (set));\n+\t\t    if (GET_CODE (x) == PLUS\n+\t\t\t|| x == frame_pointer_rtx || x == arg_pointer_rtx)\n+\t\t      /* This is PLUS of frame pointer and a constant,\n+\t\t\t or fp, or argp.  */\n+\t\t      ira_reg_equiv[i].invariant = x;\n+\t\t    else if (targetm.legitimate_constant_p (mode, x))\n+\t\t      ira_reg_equiv[i].constant = x;\n+\t\t    else\n+\t\t      {\n+\t\t\tira_reg_equiv[i].memory = force_const_mem (mode, x);\n+\t\t\tif (ira_reg_equiv[i].memory == NULL_RTX)\n+\t\t\t  {\n+\t\t\t    ira_reg_equiv[i].defined_p = false;\n+\t\t\t    ira_reg_equiv[i].init_insns = NULL_RTX;\n+\t\t\t    break;\n+\t\t\t  }\n+\t\t      }\n+\t\t    ira_reg_equiv[i].defined_p = true;\n+\t\t    continue;\n+\t\t  }\n+\t      }\n+\t  }\n+\tira_reg_equiv[i].defined_p = false;\n+\tira_reg_equiv[i].init_insns = NULL_RTX;\n+\tbreak;\n+      }\n+}\n+\n+\f\n+\n /* Print chain C to FILE.  */\n static void\n print_insn_chain (FILE *file, struct insn_chain *c)\n@@ -4130,6 +4333,11 @@ allocate_initial_values (void)\n     }\n }\n \f\n+\n+/* True when we use LRA instead of reload pass for the current\n+   function.  */\n+bool ira_use_lra_p;\n+\n /* All natural loops.  */\n struct loops ira_loops;\n \n@@ -4147,6 +4355,31 @@ ira (FILE *f)\n   bool loops_p;\n   int max_regno_before_ira, ira_max_point_before_emit;\n   int rebuild_p;\n+  bool saved_flag_caller_saves = flag_caller_saves;\n+  enum ira_region saved_flag_ira_region = flag_ira_region;\n+\n+  ira_conflicts_p = optimize > 0;\n+\n+  ira_use_lra_p = targetm.lra_p ();\n+  /* If there are too many pseudos and/or basic blocks (e.g. 10K\n+     pseudos and 10K blocks or 100K pseudos and 1K blocks), we will\n+     use simplified and faster algorithms in LRA.  */\n+  lra_simple_p\n+    = (ira_use_lra_p && max_reg_num () >= (1 << 26) / last_basic_block);\n+  if (lra_simple_p)\n+    {\n+      /* It permits to skip live range splitting in LRA.  */\n+      flag_caller_saves = false;\n+      /* There is no sense to do regional allocation when we use\n+\t simplified LRA.  */\n+      flag_ira_region = IRA_REGION_ONE;\n+      ira_conflicts_p = false;\n+    }\n+\n+#ifndef IRA_NO_OBSTACK\n+  gcc_obstack_init (&ira_obstack);\n+#endif\n+  bitmap_obstack_initialize (&ira_bitmap_obstack);\n \n   if (flag_caller_saves)\n     init_caller_save ();\n@@ -4162,7 +4395,6 @@ ira (FILE *f)\n       ira_dump_file = stderr;\n     }\n \n-  ira_conflicts_p = optimize > 0;\n   setup_prohibited_mode_move_regs ();\n \n   df_note_add_problem ();\n@@ -4188,30 +4420,18 @@ ira (FILE *f)\n   if (resize_reg_info () && flag_ira_loop_pressure)\n     ira_set_pseudo_classes (true, ira_dump_file);\n \n+  init_reg_equiv ();\n   rebuild_p = update_equiv_regs ();\n+  setup_reg_equiv ();\n+  setup_reg_equiv_init ();\n \n-#ifndef IRA_NO_OBSTACK\n-  gcc_obstack_init (&ira_obstack);\n-#endif\n-  bitmap_obstack_initialize (&ira_bitmap_obstack);\n-  if (optimize)\n+  if (optimize && rebuild_p)\n     {\n-      max_regno = max_reg_num ();\n-      ira_reg_equiv_len = max_regno;\n-      ira_reg_equiv_invariant_p\n-\t= (bool *) ira_allocate (max_regno * sizeof (bool));\n-      memset (ira_reg_equiv_invariant_p, 0, max_regno * sizeof (bool));\n-      ira_reg_equiv_const = (rtx *) ira_allocate (max_regno * sizeof (rtx));\n-      memset (ira_reg_equiv_const, 0, max_regno * sizeof (rtx));\n-      find_reg_equiv_invariant_const ();\n-      if (rebuild_p)\n-\t{\n-\t  timevar_push (TV_JUMP);\n-\t  rebuild_jump_labels (get_insns ());\n-\t  if (purge_all_dead_edges ())\n-\t    delete_unreachable_blocks ();\n-\t  timevar_pop (TV_JUMP);\n-\t}\n+      timevar_push (TV_JUMP);\n+      rebuild_jump_labels (get_insns ());\n+      if (purge_all_dead_edges ())\n+\tdelete_unreachable_blocks ();\n+      timevar_pop (TV_JUMP);\n     }\n \n   allocated_reg_info_size = max_reg_num ();\n@@ -4226,7 +4446,7 @@ ira (FILE *f)\n     find_moveable_pseudos ();\n \n   max_regno_before_ira = max_reg_num ();\n-  ira_setup_eliminable_regset ();\n+  ira_setup_eliminable_regset (true);\n \n   ira_overall_cost = ira_reg_cost = ira_mem_cost = 0;\n   ira_load_cost = ira_store_cost = ira_shuffle_cost = 0;\n@@ -4263,19 +4483,32 @@ ira (FILE *f)\n \n   ira_emit (loops_p);\n \n+  max_regno = max_reg_num ();\n   if (ira_conflicts_p)\n     {\n-      max_regno = max_reg_num ();\n-\n       if (! loops_p)\n-\tira_initiate_assign ();\n+\t{\n+\t  if (! ira_use_lra_p)\n+\t    ira_initiate_assign ();\n+\t}\n       else\n \t{\n \t  expand_reg_info ();\n \n-\t  if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL)\n-\t    fprintf (ira_dump_file, \"Flattening IR\\n\");\n-\t  ira_flattening (max_regno_before_ira, ira_max_point_before_emit);\n+\t  if (ira_use_lra_p)\n+\t    {\n+\t      ira_allocno_t a;\n+\t      ira_allocno_iterator ai;\n+\n+\t      FOR_EACH_ALLOCNO (a, ai)\n+\t\tALLOCNO_REGNO (a) = REGNO (ALLOCNO_EMIT_DATA (a)->reg);\n+\t    }\n+\t  else\n+\t    {\n+\t      if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL)\n+\t\tfprintf (ira_dump_file, \"Flattening IR\\n\");\n+\t      ira_flattening (max_regno_before_ira, ira_max_point_before_emit);\n+\t    }\n \t  /* New insns were generated: add notes and recalculate live\n \t     info.  */\n \t  df_analyze ();\n@@ -4289,9 +4522,12 @@ ira (FILE *f)\n \t  current_loops = &ira_loops;\n \t  record_loop_exits ();\n \n-\t  setup_allocno_assignment_flags ();\n-\t  ira_initiate_assign ();\n-\t  ira_reassign_conflict_allocnos (max_regno);\n+\t  if (! ira_use_lra_p)\n+\t    {\n+\t      setup_allocno_assignment_flags ();\n+\t      ira_initiate_assign ();\n+\t      ira_reassign_conflict_allocnos (max_regno);\n+\t    }\n \t}\n     }\n \n@@ -4338,6 +4574,13 @@ ira (FILE *f)\n   /* See comment for find_moveable_pseudos call.  */\n   if (ira_conflicts_p)\n     move_unallocated_pseudos ();\n+\n+  /* Restore original values.  */\n+  if (lra_simple_p)\n+    {\n+      flag_caller_saves = saved_flag_caller_saves;\n+      flag_ira_region = saved_flag_ira_region;\n+    }\n }\n \n static void\n@@ -4349,46 +4592,77 @@ do_reload (void)\n   if (flag_ira_verbose < 10)\n     ira_dump_file = dump_file;\n \n-  df_set_flags (DF_NO_INSN_RESCAN);\n-  build_insn_chain ();\n+  timevar_push (TV_RELOAD);\n+  if (ira_use_lra_p)\n+    {\n+      if (current_loops != NULL)\n+\t{\n+\t  release_recorded_exits ();\n+\t  flow_loops_free (&ira_loops);\n+\t  free_dominance_info (CDI_DOMINATORS);\n+\t}\n+      FOR_ALL_BB (bb)\n+\tbb->loop_father = NULL;\n+      current_loops = NULL;\n+      \n+      if (ira_conflicts_p)\n+\tira_free (ira_spilled_reg_stack_slots);\n+\n+      ira_destroy ();\n \n-  need_dce = reload (get_insns (), ira_conflicts_p);\n+      lra (ira_dump_file);\n+      /* ???!!! Move it before lra () when we use ira_reg_equiv in\n+\t LRA.  */\n+      VEC_free (reg_equivs_t, gc, reg_equivs);\n+      reg_equivs = NULL;\n+      need_dce = false;\n+    }\n+  else\n+    {\n+      df_set_flags (DF_NO_INSN_RESCAN);\n+      build_insn_chain ();\n+      \n+      need_dce = reload (get_insns (), ira_conflicts_p);\n+\n+    }\n+\n+  timevar_pop (TV_RELOAD);\n \n   timevar_push (TV_IRA);\n \n-  if (ira_conflicts_p)\n+  if (ira_conflicts_p && ! ira_use_lra_p)\n     {\n       ira_free (ira_spilled_reg_stack_slots);\n-\n       ira_finish_assign ();\n     }\n+\n   if (internal_flag_ira_verbose > 0 && ira_dump_file != NULL\n       && overall_cost_before != ira_overall_cost)\n     fprintf (ira_dump_file, \"+++Overall after reload %d\\n\", ira_overall_cost);\n-  ira_destroy ();\n \n   flag_ira_share_spill_slots = saved_flag_ira_share_spill_slots;\n \n-  if (current_loops != NULL)\n+  if (! ira_use_lra_p)\n     {\n-      release_recorded_exits ();\n-      flow_loops_free (&ira_loops);\n-      free_dominance_info (CDI_DOMINATORS);\n+      ira_destroy ();\n+      if (current_loops != NULL)\n+\t{\n+\t  release_recorded_exits ();\n+\t  flow_loops_free (&ira_loops);\n+\t  free_dominance_info (CDI_DOMINATORS);\n+\t}\n+      FOR_ALL_BB (bb)\n+\tbb->loop_father = NULL;\n+      current_loops = NULL;\n+      \n+      regstat_free_ri ();\n+      regstat_free_n_sets_and_refs ();\n     }\n-  FOR_ALL_BB (bb)\n-    bb->loop_father = NULL;\n-  current_loops = NULL;\n-\n-  regstat_free_ri ();\n-  regstat_free_n_sets_and_refs ();\n \n   if (optimize)\n-    {\n-      cleanup_cfg (CLEANUP_EXPENSIVE);\n+    cleanup_cfg (CLEANUP_EXPENSIVE);\n \n-      ira_free (ira_reg_equiv_invariant_p);\n-      ira_free (ira_reg_equiv_const);\n-    }\n+  finish_reg_equiv ();\n \n   bitmap_obstack_release (&ira_bitmap_obstack);\n #ifndef IRA_NO_OBSTACK"}, {"sha": "19852ee934aa000232324c579ac4e978fe48935a", "filename": "gcc/ira.h", "status": "modified", "additions": 54, "deletions": 2, "changes": 56, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fira.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fira.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fira.h?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -20,11 +20,16 @@ You should have received a copy of the GNU General Public License\n along with GCC; see the file COPYING3.  If not see\n <http://www.gnu.org/licenses/>.  */\n \n+/* True when we use LRA instead of reload pass for the current\n+   function.  */\n+extern bool ira_use_lra_p;\n+\n /* True if we have allocno conflicts.  It is false for non-optimized\n    mode or when the conflict table is too big.  */\n extern bool ira_conflicts_p;\n \n-struct target_ira {\n+struct target_ira\n+{\n   /* Map: hard register number -> allocno class it belongs to.  If the\n      corresponding class is NO_REGS, the hard register is not available\n      for allocation.  */\n@@ -79,6 +84,23 @@ struct target_ira {\n      class.  */\n   int x_ira_class_hard_regs_num[N_REG_CLASSES];\n \n+  /* Register class subset relation: TRUE if the first class is a subset\n+     of the second one considering only hard registers available for the\n+     allocation.  */\n+  int x_ira_class_subset_p[N_REG_CLASSES][N_REG_CLASSES];\n+\n+  /* The biggest class inside of intersection of the two classes (that\n+     is calculated taking only hard registers available for allocation\n+     into account.  If the both classes contain no hard registers\n+     available for allocation, the value is calculated with taking all\n+     hard-registers including fixed ones into account.  */\n+  enum reg_class x_ira_reg_class_subset[N_REG_CLASSES][N_REG_CLASSES];\n+\n+  /* True if the two classes (that is calculated taking only hard\n+     registers available for allocation into account; are\n+     intersected.  */\n+  bool x_ira_reg_classes_intersect_p[N_REG_CLASSES][N_REG_CLASSES];\n+\n   /* If class CL has a single allocatable register of mode M,\n      index [CL][M] gives the number of that register, otherwise it is -1.  */\n   short x_ira_class_singleton[N_REG_CLASSES][MAX_MACHINE_MODE];\n@@ -121,18 +143,48 @@ extern struct target_ira *this_target_ira;\n   (this_target_ira->x_ira_class_hard_regs)\n #define ira_class_hard_regs_num \\\n   (this_target_ira->x_ira_class_hard_regs_num)\n+#define ira_class_subset_p \\\n+  (this_target_ira->x_ira_class_subset_p)\n+#define ira_reg_class_subset \\\n+  (this_target_ira->x_ira_reg_class_subset)\n+#define ira_reg_classes_intersect_p \\\n+  (this_target_ira->x_ira_reg_classes_intersect_p)\n #define ira_class_singleton \\\n   (this_target_ira->x_ira_class_singleton)\n #define ira_no_alloc_regs \\\n   (this_target_ira->x_ira_no_alloc_regs)\n \n+/* Major structure describing equivalence info for a pseudo.  */\n+struct ira_reg_equiv\n+{\n+  /* True if we can use this equivalence.  */\n+  bool defined_p;\n+  /* True if the usage of the equivalence is profitable.  */\n+  bool profitable_p;\n+  /* Equiv. memory, constant, invariant, and initializing insns of\n+     given pseudo-register or NULL_RTX.  */\n+  rtx memory;\n+  rtx constant;\n+  rtx invariant;\n+  /* Always NULL_RTX if defined_p is false.  */\n+  rtx init_insns;\n+};\n+\n+/* The length of the following array.  */\n+extern int ira_reg_equiv_len;\n+\n+/* Info about equiv. info for each register.  */\n+extern struct ira_reg_equiv *ira_reg_equiv;\n+\n extern void ira_init_once (void);\n extern void ira_init (void);\n extern void ira_finish_once (void);\n-extern void ira_setup_eliminable_regset (void);\n+extern void ira_setup_eliminable_regset (bool);\n extern rtx ira_eliminate_regs (rtx, enum machine_mode);\n extern void ira_set_pseudo_classes (bool, FILE *);\n extern void ira_implicitly_set_insn_hard_regs (HARD_REG_SET *);\n+extern void ira_expand_reg_equiv (void);\n+extern void ira_update_equiv_info_by_shuffle_insn (int, int, rtx);\n \n extern void ira_sort_regnos_for_alter_reg (int *, int, unsigned int *);\n extern void ira_mark_allocation_change (int);"}, {"sha": "acc96341d651ba030facec37ca249d15106585b0", "filename": "gcc/jump.c", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fjump.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fjump.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fjump.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -1868,7 +1868,8 @@ true_regnum (const_rtx x)\n {\n   if (REG_P (x))\n     {\n-      if (REGNO (x) >= FIRST_PSEUDO_REGISTER && reg_renumber[REGNO (x)] >= 0)\n+      if (REGNO (x) >= FIRST_PSEUDO_REGISTER\n+\t  && (lra_in_progress || reg_renumber[REGNO (x)] >= 0))\n \treturn reg_renumber[REGNO (x)];\n       return REGNO (x);\n     }\n@@ -1880,7 +1881,8 @@ true_regnum (const_rtx x)\n \t{\n \t  struct subreg_info info;\n \n-\t  subreg_get_info (REGNO (SUBREG_REG (x)),\n+\t  subreg_get_info (lra_in_progress\n+\t\t\t   ? (unsigned) base : REGNO (SUBREG_REG (x)),\n \t\t\t   GET_MODE (SUBREG_REG (x)),\n \t\t\t   SUBREG_BYTE (x), GET_MODE (x), &info);\n "}, {"sha": "ba31541f80ffa12a4d1f123055d1d2f7ec705bc4", "filename": "gcc/loop-invariant.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Floop-invariant.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Floop-invariant.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Floop-invariant.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -1824,7 +1824,7 @@ calculate_loop_reg_pressure (void)\n \tbitmap_initialize (&LOOP_DATA (loop)->regs_ref, &reg_obstack);\n \tbitmap_initialize (&LOOP_DATA (loop)->regs_live, &reg_obstack);\n       }\n-  ira_setup_eliminable_regset ();\n+  ira_setup_eliminable_regset (false);\n   bitmap_initialize (&curr_regs_live, &reg_obstack);\n   FOR_EACH_BB (bb)\n     {"}, {"sha": "b957563716f03e50b2149a9d66126ac9ead86a0d", "filename": "gcc/lra-assigns.c", "status": "added", "additions": 1398, "deletions": 0, "changes": 1398, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-assigns.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-assigns.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flra-assigns.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -0,0 +1,1398 @@\n+/* Assign reload pseudos.\n+   Copyright (C) 2010, 2011, 2012\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.\tIf not see\n+<http://www.gnu.org/licenses/>.\t */\n+\n+\n+/* This file's main objective is to assign hard registers to reload\n+   pseudos.  It also tries to allocate hard registers to other\n+   pseudos, but at a lower priority than the reload pseudos.  The pass\n+   does not transform the RTL.\n+\n+   We must allocate a hard register to every reload pseudo.  We try to\n+   increase the chances of finding a viable allocation by assigning\n+   the pseudos in order of fewest available hard registers first.  If\n+   we still fail to find a hard register, we spill other (non-reload)\n+   pseudos in order to make room.\n+\n+   find_hard_regno_for finds hard registers for allocation without\n+   spilling.  spill_for does the same with spilling.  Both functions\n+   use a cost model to determine the most profitable choice of hard\n+   and spill registers.\n+\n+   Once we have finished allocating reload pseudos, we also try to\n+   assign registers to other (non-reload) pseudos.  This is useful if\n+   hard registers were freed up by the spilling just described.\n+\n+   We try to assign hard registers by collecting pseudos into threads.\n+   These threads contain reload and inheritance pseudos that are\n+   connected by copies (move insns).  Doing this improves the chances\n+   of pseudos in the thread getting the same hard register and, as a\n+   result, of allowing some move insns to be deleted.\n+\n+   When we assign a hard register to a pseudo, we decrease the cost of\n+   using the same hard register for pseudos that are connected by\n+   copies.\n+\n+   If two hard registers have the same frequency-derived cost, we\n+   prefer hard registers with higher priorities.  The mapping of\n+   registers to priorities is controlled by the register_priority\n+   target hook.  For example, x86-64 has a few register priorities:\n+   hard registers with and without REX prefixes have different\n+   priorities.  This permits us to generate smaller code as insns\n+   without REX prefixes are shorter.\n+\n+   If a few hard registers are still equally good for the assignment,\n+   we choose the least used hard register.  It is called leveling and\n+   may be profitable for some targets.\n+\n+   Only insns with changed allocation pseudos are processed on the\n+   next constraint pass.\n+\n+   The pseudo live-ranges are used to find conflicting pseudos.\n+\n+   For understanding the code, it is important to keep in mind that\n+   inheritance, split, and reload pseudos created since last\n+   constraint pass have regno >= lra_constraint_new_regno_start.\n+   Inheritance and split pseudos created on any pass are in the\n+   corresponding bitmaps.  Inheritance and split pseudos since the\n+   last constraint pass have also the corresponding non-negative\n+   restore_regno.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"hard-reg-set.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"target.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"output.h\"\n+#include \"regs.h\"\n+#include \"function.h\"\n+#include \"expr.h\"\n+#include \"basic-block.h\"\n+#include \"except.h\"\n+#include \"df.h\"\n+#include \"ira.h\"\n+#include \"sparseset.h\"\n+#include \"lra-int.h\"\n+\n+/* Array containing corresponding values of function\n+   lra_get_allocno_class.  It is used to speed up the code.  */\n+static enum reg_class *regno_allocno_class_array;\n+\n+/* Information about the thread to which a pseudo belongs.  Threads are\n+   a set of connected reload and inheritance pseudos with the same set of\n+   available hard registers.  Lone registers belong to their own threads.  */\n+struct regno_assign_info\n+{\n+  /* First/next pseudo of the same thread.  */\n+  int first, next;\n+  /* Frequency of the thread (execution frequency of only reload\n+     pseudos in the thread when the thread contains a reload pseudo).\n+     Defined only for the first thread pseudo.\t*/\n+  int freq;\n+};\n+\n+/* Map regno to the corresponding regno assignment info.  */\n+static struct regno_assign_info *regno_assign_info;\n+\n+/* Process a pseudo copy with execution frequency COPY_FREQ connecting\n+   REGNO1 and REGNO2 to form threads.  */\n+static void\n+process_copy_to_form_thread (int regno1, int regno2, int copy_freq)\n+{\n+  int last, regno1_first, regno2_first;\n+\n+  lra_assert (regno1 >= lra_constraint_new_regno_start\n+\t      && regno2 >= lra_constraint_new_regno_start);\n+  regno1_first = regno_assign_info[regno1].first;\n+  regno2_first = regno_assign_info[regno2].first;\n+  if (regno1_first != regno2_first)\n+    {\n+      for (last = regno2_first;\n+\t   regno_assign_info[last].next >= 0;\n+\t   last = regno_assign_info[last].next)\n+\tregno_assign_info[last].first = regno1_first;\n+      regno_assign_info[last].first = regno1_first;\n+      regno_assign_info[last].next = regno_assign_info[regno1_first].next;\n+      regno_assign_info[regno1_first].next = regno2_first;\n+      regno_assign_info[regno1_first].freq\n+\t+= regno_assign_info[regno2_first].freq;\n+    }\n+  regno_assign_info[regno1_first].freq -= 2 * copy_freq;\n+  lra_assert (regno_assign_info[regno1_first].freq >= 0);\n+}\n+\n+/* Initialize REGNO_ASSIGN_INFO and form threads.  */\n+static void\n+init_regno_assign_info (void)\n+{\n+  int i, regno1, regno2, max_regno = max_reg_num ();\n+  lra_copy_t cp;\n+  \n+  regno_assign_info = XNEWVEC (struct regno_assign_info, max_regno);\n+  for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+    {\n+      regno_assign_info[i].first = i;\n+      regno_assign_info[i].next = -1;\n+      regno_assign_info[i].freq = lra_reg_info[i].freq;\n+    }\n+  /* Form the threads.\t*/\n+  for (i = 0; (cp = lra_get_copy (i)) != NULL; i++)\n+    if ((regno1 = cp->regno1) >= lra_constraint_new_regno_start\n+\t&& (regno2 = cp->regno2) >= lra_constraint_new_regno_start\n+\t&& reg_renumber[regno1] < 0 && lra_reg_info[regno1].nrefs != 0\n+\t&& reg_renumber[regno2] < 0 && lra_reg_info[regno2].nrefs != 0\n+\t&& (ira_class_hard_regs_num[regno_allocno_class_array[regno1]]\n+\t    == ira_class_hard_regs_num[regno_allocno_class_array[regno2]]))\n+      process_copy_to_form_thread (regno1, regno2, cp->freq);\n+}\n+\n+/* Free REGNO_ASSIGN_INFO.  */\n+static void\n+finish_regno_assign_info (void)\n+{\n+  free (regno_assign_info);\n+}\n+\n+/* The function is used to sort *reload* and *inheritance* pseudos to\n+   try to assign them hard registers.  We put pseudos from the same\n+   thread always nearby.  */\n+static int\n+reload_pseudo_compare_func (const void *v1p, const void *v2p)\n+{\n+  int r1 = *(const int *) v1p, r2 = *(const int *) v2p;\n+  enum reg_class cl1 = regno_allocno_class_array[r1];\n+  enum reg_class cl2 = regno_allocno_class_array[r2];\n+  int diff;\n+  \n+  lra_assert (r1 >= lra_constraint_new_regno_start\n+\t      && r2 >= lra_constraint_new_regno_start);\n+  \n+  /* Prefer to assign reload registers with smaller classes first to\n+     guarantee assignment to all reload registers.  */\n+  if ((diff = (ira_class_hard_regs_num[cl1]\n+\t       - ira_class_hard_regs_num[cl2])) != 0)\n+    return diff;\n+  if ((diff = (regno_assign_info[regno_assign_info[r2].first].freq\n+\t       - regno_assign_info[regno_assign_info[r1].first].freq)) != 0)\n+    return diff;\n+  /* Put pseudos from the thread nearby.  */\n+  if ((diff = regno_assign_info[r1].first - regno_assign_info[r2].first) != 0)\n+    return diff;\n+  /* If regs are equally good, sort by their numbers, so that the\n+     results of qsort leave nothing to chance.\t*/\n+  return r1 - r2;\n+}\n+\n+/* The function is used to sort *non-reload* pseudos to try to assign\n+   them hard registers.\t The order calculation is simpler than in the\n+   previous function and based on the pseudo frequency usage.  */\n+static int\n+pseudo_compare_func (const void *v1p, const void *v2p)\n+{\n+  int r1 = *(const int *) v1p, r2 = *(const int *) v2p;\n+  int diff;\n+\n+  /* Prefer to assign more frequently used registers first.  */\n+  if ((diff = lra_reg_info[r2].freq - lra_reg_info[r1].freq) != 0)\n+    return diff;\n+  \n+  /* If regs are equally good, sort by their numbers, so that the\n+     results of qsort leave nothing to chance.\t*/\n+  return r1 - r2;\n+}\n+\n+/* Arrays of size LRA_LIVE_MAX_POINT mapping a program point to the\n+   pseudo live ranges with given start point.  We insert only live\n+   ranges of pseudos interesting for assignment purposes.  They are\n+   reload pseudos and pseudos assigned to hard registers.  */\n+static lra_live_range_t *start_point_ranges;\n+\n+/* Used as a flag that a live range is not inserted in the start point\n+   chain.  */\n+static struct lra_live_range not_in_chain_mark;\n+\n+/* Create and set up START_POINT_RANGES.  */\n+static void\n+create_live_range_start_chains (void)\n+{\n+  int i, max_regno;\n+  lra_live_range_t r;\n+\n+  start_point_ranges = XCNEWVEC (lra_live_range_t, lra_live_max_point);\n+  max_regno = max_reg_num ();\n+  for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+    if (i >= lra_constraint_new_regno_start || reg_renumber[i] >= 0)\n+      {\n+\tfor (r = lra_reg_info[i].live_ranges; r != NULL; r = r->next)\n+\t  {\n+\t    r->start_next = start_point_ranges[r->start];\n+\t    start_point_ranges[r->start] = r;\n+\t  }\n+      }\n+    else\n+      {\n+\tfor (r = lra_reg_info[i].live_ranges; r != NULL; r = r->next)\n+\t  r->start_next = &not_in_chain_mark;\n+      }\n+}\n+\n+/* Insert live ranges of pseudo REGNO into start chains if they are\n+   not there yet.  */\n+static void\n+insert_in_live_range_start_chain (int regno)\n+{\n+  lra_live_range_t r = lra_reg_info[regno].live_ranges;\n+\n+  if (r->start_next != &not_in_chain_mark)\n+    return;\n+  for (; r != NULL; r = r->next)\n+    {\n+      r->start_next = start_point_ranges[r->start];\n+      start_point_ranges[r->start] = r;\n+    }\n+}\n+\n+/* Free START_POINT_RANGES.  */\n+static void\n+finish_live_range_start_chains (void)\n+{\n+  gcc_assert (start_point_ranges != NULL);\n+  free (start_point_ranges);\n+  start_point_ranges = NULL;\n+}\n+\n+/* Map: program point -> bitmap of all pseudos living at the point and\n+   assigned to hard registers.\t*/\n+static bitmap_head *live_hard_reg_pseudos;\n+static bitmap_obstack live_hard_reg_pseudos_bitmap_obstack;\n+\n+/* reg_renumber corresponding to pseudos marked in\n+   live_hard_reg_pseudos.  reg_renumber might be not matched to\n+   live_hard_reg_pseudos but live_pseudos_reg_renumber always reflects\n+   live_hard_reg_pseudos.  */\n+static int *live_pseudos_reg_renumber;\n+\n+/* Sparseset used to calculate living hard reg pseudos for some program\n+   point range.\t */\n+static sparseset live_range_hard_reg_pseudos;\n+\n+/* Sparseset used to calculate living reload/inheritance pseudos for\n+   some program point range.  */\n+static sparseset live_range_reload_inheritance_pseudos;\n+\n+/* Allocate and initialize the data about living pseudos at program\n+   points.  */\n+static void\n+init_lives (void)\n+{\n+  int i, max_regno = max_reg_num ();\n+\n+  live_range_hard_reg_pseudos = sparseset_alloc (max_regno);\n+  live_range_reload_inheritance_pseudos = sparseset_alloc (max_regno);\n+  live_hard_reg_pseudos = XNEWVEC (bitmap_head, lra_live_max_point);\n+  bitmap_obstack_initialize (&live_hard_reg_pseudos_bitmap_obstack);\n+  for (i = 0; i < lra_live_max_point; i++)\n+    bitmap_initialize (&live_hard_reg_pseudos[i],\n+\t\t       &live_hard_reg_pseudos_bitmap_obstack);\n+  live_pseudos_reg_renumber = XNEWVEC (int, max_regno);\n+  for (i = 0; i < max_regno; i++)\n+    live_pseudos_reg_renumber[i] = -1;\n+}\n+\n+/* Free the data about living pseudos at program points.  */\n+static void\n+finish_lives (void)\n+{\n+  sparseset_free (live_range_hard_reg_pseudos);\n+  sparseset_free (live_range_reload_inheritance_pseudos);\n+  free (live_hard_reg_pseudos);\n+  bitmap_obstack_release (&live_hard_reg_pseudos_bitmap_obstack);\n+  free (live_pseudos_reg_renumber);\n+}\n+\n+/* Update the LIVE_HARD_REG_PSEUDOS and LIVE_PSEUDOS_REG_RENUMBER\n+   entries for pseudo REGNO.  Assume that the register has been\n+   spilled if FREE_P, otherwise assume that it has been assigned\n+   reg_renumber[REGNO] (if >= 0).  We also insert the pseudo live\n+   ranges in the start chains when it is assumed to be assigned to a\n+   hard register because we use the chains of pseudos assigned to hard\n+   registers during allocation.  */\n+static void\n+update_lives (int regno, bool free_p)\n+{\n+  int p;\n+  lra_live_range_t r;\n+\n+  if (reg_renumber[regno] < 0)\n+    return;\n+  live_pseudos_reg_renumber[regno] = free_p ? -1 : reg_renumber[regno];\n+  for (r = lra_reg_info[regno].live_ranges; r != NULL; r = r->next)\n+    {\n+      for (p = r->start; p <= r->finish; p++)\n+\tif (free_p)\n+\t  bitmap_clear_bit (&live_hard_reg_pseudos[p], regno);\n+\telse\n+\t  {\n+\t    bitmap_set_bit (&live_hard_reg_pseudos[p], regno);\n+\t    insert_in_live_range_start_chain (regno);\n+\t  }\n+    }\n+}\n+\n+/* Sparseset used to calculate reload pseudos conflicting with a given\n+   pseudo when we are trying to find a hard register for the given\n+   pseudo.  */\n+static sparseset conflict_reload_and_inheritance_pseudos;\n+\n+/* Map: program point -> bitmap of all reload and inheritance pseudos\n+   living at the point.\t */\n+static bitmap_head *live_reload_and_inheritance_pseudos;\n+static bitmap_obstack live_reload_and_inheritance_pseudos_bitmap_obstack;\n+\n+/* Allocate and initialize data about living reload pseudos at any\n+   given program point.  */\n+static void\n+init_live_reload_and_inheritance_pseudos (void)\n+{\n+  int i, p, max_regno = max_reg_num ();\n+  lra_live_range_t r;\n+  \n+  conflict_reload_and_inheritance_pseudos = sparseset_alloc (max_regno);\n+  live_reload_and_inheritance_pseudos = XNEWVEC (bitmap_head, lra_live_max_point);\n+  bitmap_obstack_initialize (&live_reload_and_inheritance_pseudos_bitmap_obstack);\n+  for (p = 0; p < lra_live_max_point; p++)\n+    bitmap_initialize (&live_reload_and_inheritance_pseudos[p],\n+\t\t       &live_reload_and_inheritance_pseudos_bitmap_obstack);\n+  for (i = lra_constraint_new_regno_start; i < max_regno; i++)\n+    {\n+      for (r = lra_reg_info[i].live_ranges; r != NULL; r = r->next)\n+\tfor (p = r->start; p <= r->finish; p++)\n+\t  bitmap_set_bit (&live_reload_and_inheritance_pseudos[p], i);\n+    }\n+}\n+\n+/* Finalize data about living reload pseudos at any given program\n+   point.  */\n+static void\n+finish_live_reload_and_inheritance_pseudos (void)\n+{\n+  sparseset_free (conflict_reload_and_inheritance_pseudos);\n+  free (live_reload_and_inheritance_pseudos);\n+  bitmap_obstack_release (&live_reload_and_inheritance_pseudos_bitmap_obstack);\n+}\n+\n+/* The value used to check that cost of given hard reg is really\n+   defined currently.  */\n+static int curr_hard_regno_costs_check = 0;\n+/* Array used to check that cost of the corresponding hard reg (the\n+   array element index) is really defined currently.  */\n+static int hard_regno_costs_check[FIRST_PSEUDO_REGISTER];\n+/* The current costs of allocation of hard regs.  Defined only if the\n+   value of the corresponding element of the previous array is equal to\n+   CURR_HARD_REGNO_COSTS_CHECK.\t */\n+static int hard_regno_costs[FIRST_PSEUDO_REGISTER];\n+\n+/* Adjust cost of HARD_REGNO by INCR.  Reset the cost first if it is\n+   not defined yet.  */\n+static inline void\n+adjust_hard_regno_cost (int hard_regno, int incr)\n+{\n+  if (hard_regno_costs_check[hard_regno] != curr_hard_regno_costs_check)\n+    hard_regno_costs[hard_regno] = 0;\n+  hard_regno_costs_check[hard_regno] = curr_hard_regno_costs_check;\n+  hard_regno_costs[hard_regno] += incr;\n+}\n+\n+/* Try to find a free hard register for pseudo REGNO.  Return the\n+   hard register on success and set *COST to the cost of using\n+   that register.  (If several registers have equal cost, the one with\n+   the highest priority wins.)  Return -1 on failure.\n+\n+   If TRY_ONLY_HARD_REGNO >= 0, consider only that hard register,\n+   otherwise consider all hard registers in REGNO's class.  */\n+static int\n+find_hard_regno_for (int regno, int *cost, int try_only_hard_regno)\n+{\n+  HARD_REG_SET conflict_set;\n+  int best_cost = INT_MAX, best_priority = INT_MIN, best_usage = INT_MAX;\n+  lra_live_range_t r;\n+  int p, i, j, rclass_size, best_hard_regno, priority, hard_regno;\n+  int hr, conflict_hr, nregs;\n+  enum machine_mode biggest_mode;\n+  unsigned int k, conflict_regno;\n+  int val, biggest_nregs, nregs_diff;\n+  enum reg_class rclass;\n+  bitmap_iterator bi;\n+  bool *rclass_intersect_p;\n+  HARD_REG_SET impossible_start_hard_regs;\n+\n+  COPY_HARD_REG_SET (conflict_set, lra_no_alloc_regs);\n+  rclass = regno_allocno_class_array[regno];\n+  rclass_intersect_p = ira_reg_classes_intersect_p[rclass];\n+  curr_hard_regno_costs_check++;\n+  sparseset_clear (conflict_reload_and_inheritance_pseudos);\n+  sparseset_clear (live_range_hard_reg_pseudos);\n+  IOR_HARD_REG_SET (conflict_set, lra_reg_info[regno].conflict_hard_regs);\n+  biggest_mode = lra_reg_info[regno].biggest_mode;\n+  for (r = lra_reg_info[regno].live_ranges; r != NULL; r = r->next)\n+    {\n+      EXECUTE_IF_SET_IN_BITMAP (&live_hard_reg_pseudos[r->start], 0, k, bi)\n+\tif (rclass_intersect_p[regno_allocno_class_array[k]])\n+\t  sparseset_set_bit (live_range_hard_reg_pseudos, k);\n+      EXECUTE_IF_SET_IN_BITMAP (&live_reload_and_inheritance_pseudos[r->start],\n+\t\t\t\t0, k, bi)\n+\tif (lra_reg_info[k].preferred_hard_regno1 >= 0\n+\t    && live_pseudos_reg_renumber[k] < 0\n+\t    && rclass_intersect_p[regno_allocno_class_array[k]])\n+\t  sparseset_set_bit (conflict_reload_and_inheritance_pseudos, k);\n+      for (p = r->start + 1; p <= r->finish; p++)\n+\t{\n+\t  lra_live_range_t r2;\n+\t  \n+\t  for (r2 = start_point_ranges[p];\n+\t       r2 != NULL;\n+\t       r2 = r2->start_next)\n+\t    {\n+\t      if (r2->regno >= lra_constraint_new_regno_start\n+\t\t  && lra_reg_info[r2->regno].preferred_hard_regno1 >= 0\n+\t\t  && live_pseudos_reg_renumber[r2->regno] < 0\n+\t\t  && rclass_intersect_p[regno_allocno_class_array[r2->regno]])\n+\t\tsparseset_set_bit (conflict_reload_and_inheritance_pseudos,\n+\t\t\t\t   r2->regno);\n+\t      if (live_pseudos_reg_renumber[r2->regno] >= 0\n+\t\t  && rclass_intersect_p[regno_allocno_class_array[r2->regno]])\n+\t\tsparseset_set_bit (live_range_hard_reg_pseudos, r2->regno);\n+\t    }\n+\t}\n+    }\n+  if ((hard_regno = lra_reg_info[regno].preferred_hard_regno1) >= 0)\n+    {\n+      adjust_hard_regno_cost\n+\t(hard_regno, -lra_reg_info[regno].preferred_hard_regno_profit1);\n+      if ((hard_regno = lra_reg_info[regno].preferred_hard_regno2) >= 0)\n+\tadjust_hard_regno_cost\n+\t  (hard_regno, -lra_reg_info[regno].preferred_hard_regno_profit2);\n+    }\n+#ifdef STACK_REGS\n+  if (lra_reg_info[regno].no_stack_p)\n+    for (i = FIRST_STACK_REG; i <= LAST_STACK_REG; i++)\n+      SET_HARD_REG_BIT (conflict_set, i);\n+#endif\n+  sparseset_clear_bit (conflict_reload_and_inheritance_pseudos, regno);\n+  val = lra_reg_info[regno].val;\n+  CLEAR_HARD_REG_SET (impossible_start_hard_regs);\n+  EXECUTE_IF_SET_IN_SPARSESET (live_range_hard_reg_pseudos, conflict_regno)\n+    if (val == lra_reg_info[conflict_regno].val)\n+      {\n+\tconflict_hr = live_pseudos_reg_renumber[conflict_regno];\n+\tnregs = (hard_regno_nregs[conflict_hr]\n+\t\t [lra_reg_info[conflict_regno].biggest_mode]);\n+\t/* Remember about multi-register pseudos.  For example, 2 hard\n+\t   register pseudos can start on the same hard register but can\n+\t   not start on HR and HR+1/HR-1.  */ \n+\tfor (hr = conflict_hr + 1;\n+\t     hr < FIRST_PSEUDO_REGISTER && hr < conflict_hr + nregs;\n+\t     hr++)\n+\t  SET_HARD_REG_BIT (impossible_start_hard_regs, hr);\n+\tfor (hr = conflict_hr - 1;\n+\t     hr >= 0 && hr + hard_regno_nregs[hr][biggest_mode] > conflict_hr;\n+\t     hr--)\n+\t  SET_HARD_REG_BIT (impossible_start_hard_regs, hr);\n+      }\n+    else\n+      {\n+\tadd_to_hard_reg_set (&conflict_set,\n+\t\t\t     lra_reg_info[conflict_regno].biggest_mode,\n+\t\t\t     live_pseudos_reg_renumber[conflict_regno]);\n+\tif (hard_reg_set_subset_p (reg_class_contents[rclass],\n+\t\t\t\t   conflict_set))\n+\t  return -1;\n+      }\n+  EXECUTE_IF_SET_IN_SPARSESET (conflict_reload_and_inheritance_pseudos,\n+\t\t\t       conflict_regno)\n+    if (val != lra_reg_info[conflict_regno].val)\n+      {\n+\tlra_assert (live_pseudos_reg_renumber[conflict_regno] < 0);\n+\tif ((hard_regno\n+\t     = lra_reg_info[conflict_regno].preferred_hard_regno1) >= 0)\n+\t  {\n+\t    adjust_hard_regno_cost\n+\t      (hard_regno,\n+\t       lra_reg_info[conflict_regno].preferred_hard_regno_profit1);\n+\t    if ((hard_regno\n+\t\t = lra_reg_info[conflict_regno].preferred_hard_regno2) >= 0)\n+\t      adjust_hard_regno_cost\n+\t\t(hard_regno,\n+\t\t lra_reg_info[conflict_regno].preferred_hard_regno_profit2);\n+\t  }\n+      }\n+  /* Make sure that all registers in a multi-word pseudo belong to the\n+     required class.  */\n+  IOR_COMPL_HARD_REG_SET (conflict_set, reg_class_contents[rclass]);\n+  lra_assert (rclass != NO_REGS);\n+  rclass_size = ira_class_hard_regs_num[rclass];\n+  best_hard_regno = -1;\n+  hard_regno = ira_class_hard_regs[rclass][0];\n+  biggest_nregs = hard_regno_nregs[hard_regno][biggest_mode];\n+  nregs_diff = (biggest_nregs\n+\t\t- hard_regno_nregs[hard_regno][PSEUDO_REGNO_MODE (regno)]);\n+  for (i = 0; i < rclass_size; i++)\n+    {\n+      if (try_only_hard_regno >= 0)\n+\thard_regno = try_only_hard_regno;\n+      else\n+\thard_regno = ira_class_hard_regs[rclass][i];\n+      if (! overlaps_hard_reg_set_p (conflict_set,\n+\t\t\t\t     PSEUDO_REGNO_MODE (regno), hard_regno)\n+\t  /* We can not use prohibited_class_mode_regs because it is\n+\t     not defined for all classes.  */\n+\t  && HARD_REGNO_MODE_OK (hard_regno, PSEUDO_REGNO_MODE (regno))\n+\t  && ! TEST_HARD_REG_BIT (impossible_start_hard_regs, hard_regno)\n+\t  && (nregs_diff == 0\n+#ifdef WORDS_BIG_ENDIAN\n+\t      || (hard_regno - nregs_diff >= 0\n+\t\t  && TEST_HARD_REG_BIT (reg_class_contents[rclass],\n+\t\t\t\t\thard_regno - nregs_diff))\n+#else\n+\t      || TEST_HARD_REG_BIT (reg_class_contents[rclass],\n+\t\t\t\t    hard_regno + nregs_diff)\n+#endif\n+\t      ))\n+\t{\n+\t  if (hard_regno_costs_check[hard_regno]\n+\t      != curr_hard_regno_costs_check)\n+\t    {\n+\t      hard_regno_costs_check[hard_regno] = curr_hard_regno_costs_check;\n+\t      hard_regno_costs[hard_regno] = 0;\n+\t    }\n+\t  for (j = 0;\n+\t       j < hard_regno_nregs[hard_regno][PSEUDO_REGNO_MODE (regno)];\n+\t       j++)\n+\t    if (! TEST_HARD_REG_BIT (call_used_reg_set, hard_regno + j)\n+\t\t&& ! df_regs_ever_live_p (hard_regno + j))\n+\t      /* It needs save restore.\t */\n+\t      hard_regno_costs[hard_regno]\n+\t\t+= 2 * ENTRY_BLOCK_PTR->next_bb->frequency;\n+\t  priority = targetm.register_priority (hard_regno);\n+\t  if (best_hard_regno < 0 || hard_regno_costs[hard_regno] < best_cost\n+\t      || (hard_regno_costs[hard_regno] == best_cost\n+\t\t  && (priority > best_priority\n+\t\t      /* Hard register usage leveling actually results\n+\t\t\t in bigger code for targets with conditional\n+\t\t\t execution like ARM because it reduces chance\n+\t\t\t of if-conversion after LRA.  */\n+\t\t      || (! targetm.have_conditional_execution ()\n+\t\t\t  && priority == best_priority\n+\t\t\t  && best_usage > lra_hard_reg_usage[hard_regno]))))\n+\t    {\n+\t      best_hard_regno = hard_regno;\n+\t      best_cost = hard_regno_costs[hard_regno];\n+\t      best_priority = priority;\n+\t      best_usage = lra_hard_reg_usage[hard_regno];\n+\t    }\n+\t}\n+      if (try_only_hard_regno >= 0)\n+\tbreak;\n+    }\n+  if (best_hard_regno >= 0)\n+    *cost = best_cost - lra_reg_info[regno].freq;\n+  return best_hard_regno;\n+}\n+\n+/* Current value used for checking elements in\n+   update_hard_regno_preference_check.\t*/\n+static int curr_update_hard_regno_preference_check;\n+/* If an element value is equal to the above variable value, then the\n+   corresponding regno has been processed for preference\n+   propagation.\t */\n+static int *update_hard_regno_preference_check;\n+\n+/* Update the preference for using HARD_REGNO for pseudos that are\n+   connected directly or indirectly with REGNO.  Apply divisor DIV\n+   to any preference adjustments.\n+\n+   The more indirectly a pseudo is connected, the smaller its effect\n+   should be.  We therefore increase DIV on each \"hop\".  */\n+static void\n+update_hard_regno_preference (int regno, int hard_regno, int div)\n+{\n+  int another_regno, cost;\n+  lra_copy_t cp, next_cp;\n+\n+  /* Search depth 5 seems to be enough.\t */\n+  if (div > (1 << 5))\n+    return;\n+  for (cp = lra_reg_info[regno].copies; cp != NULL; cp = next_cp)\n+    {\n+      if (cp->regno1 == regno)\n+\t{\n+\t  next_cp = cp->regno1_next;\n+\t  another_regno = cp->regno2;\n+\t}\n+      else if (cp->regno2 == regno)\n+\t{\n+\t  next_cp = cp->regno2_next;\n+\t  another_regno = cp->regno1;\n+\t}\n+      else\n+\tgcc_unreachable ();\n+      if (reg_renumber[another_regno] < 0\n+\t  && (update_hard_regno_preference_check[another_regno]\n+\t      != curr_update_hard_regno_preference_check))\n+\t{\n+\t  update_hard_regno_preference_check[another_regno]\n+\t    = curr_update_hard_regno_preference_check;\n+\t  cost = cp->freq < div ? 1 : cp->freq / div;\n+\t  lra_setup_reload_pseudo_preferenced_hard_reg\n+\t    (another_regno, hard_regno, cost);\n+\t  update_hard_regno_preference (another_regno, hard_regno, div * 2);\n+\t}\n+    }\n+}\n+\n+/* Update REG_RENUMBER and other pseudo preferences by assignment of\n+   HARD_REGNO to pseudo REGNO and print about it if PRINT_P.  */\n+void\n+lra_setup_reg_renumber (int regno, int hard_regno, bool print_p)\n+{\n+  int i, hr;\n+\n+  /* We can not just reassign hard register.  */\n+  lra_assert (hard_regno < 0 || reg_renumber[regno] < 0);\n+  if ((hr = hard_regno) < 0)\n+    hr = reg_renumber[regno];\n+  reg_renumber[regno] = hard_regno;\n+  lra_assert (hr >= 0);\n+  for (i = 0; i < hard_regno_nregs[hr][PSEUDO_REGNO_MODE (regno)]; i++)\n+    if (hard_regno < 0)\n+      lra_hard_reg_usage[hr + i] -= lra_reg_info[regno].freq;\n+    else\n+      lra_hard_reg_usage[hr + i] += lra_reg_info[regno].freq;\n+  if (print_p && lra_dump_file != NULL)\n+    fprintf (lra_dump_file, \"\t   Assign %d to %sr%d (freq=%d)\\n\",\n+\t     reg_renumber[regno],\n+\t     regno < lra_constraint_new_regno_start\n+\t     ? \"\"\n+\t     : bitmap_bit_p (&lra_inheritance_pseudos, regno) ? \"inheritance \"\n+\t     : bitmap_bit_p (&lra_split_regs, regno) ? \"split \"\n+\t     : bitmap_bit_p (&lra_optional_reload_pseudos, regno)\n+\t     ? \"optional reload \": \"reload \",\n+\t     regno, lra_reg_info[regno].freq);\n+  if (hard_regno >= 0)\n+    {\n+      curr_update_hard_regno_preference_check++;\n+      update_hard_regno_preference (regno, hard_regno, 1);\n+    }\n+}\n+\n+/* Pseudos which occur in insns containing a particular pseudo.  */\n+static bitmap_head insn_conflict_pseudos;\n+\n+/* Bitmaps used to contain spill pseudos for given pseudo hard regno\n+   and best spill pseudos for given pseudo (and best hard regno).  */\n+static bitmap_head spill_pseudos_bitmap, best_spill_pseudos_bitmap;\n+\n+/* Current pseudo check for validity of elements in\n+   TRY_HARD_REG_PSEUDOS.  */\n+static int curr_pseudo_check;\n+/* Array used for validity of elements in TRY_HARD_REG_PSEUDOS.\t */\n+static int try_hard_reg_pseudos_check[FIRST_PSEUDO_REGISTER];\n+/* Pseudos who hold given hard register at the considered points.  */\n+static bitmap_head try_hard_reg_pseudos[FIRST_PSEUDO_REGISTER];\n+\n+/* Set up try_hard_reg_pseudos for given program point P and class\n+   RCLASS.  Those are pseudos living at P and assigned to a hard\n+   register of RCLASS.\tIn other words, those are pseudos which can be\n+   spilled to assign a hard register of RCLASS to a pseudo living at\n+   P.  */\n+static void\n+setup_try_hard_regno_pseudos (int p, enum reg_class rclass)\n+{\n+  int i, hard_regno;\n+  enum machine_mode mode;\n+  unsigned int spill_regno;\n+  bitmap_iterator bi;\n+\n+  /* Find what pseudos could be spilled.  */\n+  EXECUTE_IF_SET_IN_BITMAP (&live_hard_reg_pseudos[p], 0, spill_regno, bi)\n+    {\n+      mode = PSEUDO_REGNO_MODE (spill_regno);\n+      hard_regno = live_pseudos_reg_renumber[spill_regno];\n+      if (overlaps_hard_reg_set_p (reg_class_contents[rclass],\n+\t\t\t\t   mode, hard_regno))\n+\t{\n+\t  for (i = hard_regno_nregs[hard_regno][mode] - 1; i >= 0; i--)\n+\t    {\n+\t      if (try_hard_reg_pseudos_check[hard_regno + i]\n+\t\t  != curr_pseudo_check)\n+\t\t{\n+\t\t  try_hard_reg_pseudos_check[hard_regno + i]\n+\t\t    = curr_pseudo_check;\n+\t\t  bitmap_clear (&try_hard_reg_pseudos[hard_regno + i]);\n+\t\t}\n+\t      bitmap_set_bit (&try_hard_reg_pseudos[hard_regno + i],\n+\t\t\t      spill_regno);\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Assign temporarily HARD_REGNO to pseudo REGNO.  Temporary\n+   assignment means that we might undo the data change.\t */\n+static void\n+assign_temporarily (int regno, int hard_regno)\n+{\n+  int p;\n+  lra_live_range_t r;\n+\n+  for (r = lra_reg_info[regno].live_ranges; r != NULL; r = r->next)\n+    {\n+      for (p = r->start; p <= r->finish; p++)\n+\tif (hard_regno < 0)\n+\t  bitmap_clear_bit (&live_hard_reg_pseudos[p], regno);\n+\telse\n+\t  {\n+\t    bitmap_set_bit (&live_hard_reg_pseudos[p], regno);\n+\t    insert_in_live_range_start_chain (regno);\n+\t  }\n+    }\n+  live_pseudos_reg_renumber[regno] = hard_regno;\n+}\n+\n+/* Array used for sorting reload pseudos for subsequent allocation\n+   after spilling some pseudo.\t*/\n+static int *sorted_reload_pseudos;\n+\n+/* Spill some pseudos for a reload pseudo REGNO and return hard\n+   register which should be used for pseudo after spilling.  The\n+   function adds spilled pseudos to SPILLED_PSEUDO_BITMAP.  When we\n+   choose hard register (and pseudos occupying the hard registers and\n+   to be spilled), we take into account not only how REGNO will\n+   benefit from the spills but also how other reload pseudos not yet\n+   assigned to hard registers benefit from the spills too.  In very\n+   rare cases, the function can fail and return -1.  */\n+static int\n+spill_for (int regno, bitmap spilled_pseudo_bitmap)\n+{\n+  int i, j, n, p, hard_regno, best_hard_regno, cost, best_cost, rclass_size;\n+  int reload_hard_regno, reload_cost;\n+  enum machine_mode mode, mode2;\n+  enum reg_class rclass;\n+  HARD_REG_SET spilled_hard_regs;\n+  unsigned int spill_regno, reload_regno, uid;\n+  int insn_pseudos_num, best_insn_pseudos_num;\n+  lra_live_range_t r;\n+  bitmap_iterator bi;\n+\n+  rclass = regno_allocno_class_array[regno];\n+  lra_assert (reg_renumber[regno] < 0 && rclass != NO_REGS);\n+  bitmap_clear (&insn_conflict_pseudos);\n+  bitmap_clear (&best_spill_pseudos_bitmap);\n+  EXECUTE_IF_SET_IN_BITMAP (&lra_reg_info[regno].insn_bitmap, 0, uid, bi)\n+    {\n+      struct lra_insn_reg *ir;\n+      \n+      for (ir = lra_get_insn_regs (uid); ir != NULL; ir = ir->next)\n+\tif (ir->regno >= FIRST_PSEUDO_REGISTER)\n+\t  bitmap_set_bit (&insn_conflict_pseudos, ir->regno);\n+    }\n+  best_hard_regno = -1;\n+  best_cost = INT_MAX;\n+  best_insn_pseudos_num = INT_MAX;\n+  rclass_size = ira_class_hard_regs_num[rclass];\n+  mode = PSEUDO_REGNO_MODE (regno);\n+  /* Invalidate try_hard_reg_pseudos elements.  */\n+  curr_pseudo_check++;\n+  for (r = lra_reg_info[regno].live_ranges; r != NULL; r = r->next)\n+    for (p = r->start; p <= r->finish; p++)\n+      setup_try_hard_regno_pseudos (p, rclass);\n+  for (i = 0; i < rclass_size; i++)\n+    {\n+      hard_regno = ira_class_hard_regs[rclass][i];\n+      bitmap_clear (&spill_pseudos_bitmap);\n+      for (j = hard_regno_nregs[hard_regno][mode] - 1; j >= 0; j--)\n+\t{\n+\t  if (try_hard_reg_pseudos_check[hard_regno + j] != curr_pseudo_check)\n+\t    continue;\n+\t  lra_assert (!bitmap_empty_p (&try_hard_reg_pseudos[hard_regno + j]));\n+\t  bitmap_ior_into (&spill_pseudos_bitmap,\n+\t\t\t   &try_hard_reg_pseudos[hard_regno + j]);\n+\t}\n+      /* Spill pseudos.\t */\n+      CLEAR_HARD_REG_SET (spilled_hard_regs);\n+      EXECUTE_IF_SET_IN_BITMAP (&spill_pseudos_bitmap, 0, spill_regno, bi)\n+\tif ((int) spill_regno >= lra_constraint_new_regno_start\n+\t    && ! bitmap_bit_p (&lra_inheritance_pseudos, spill_regno)\n+\t    && ! bitmap_bit_p (&lra_split_regs, spill_regno)\n+\t    && ! bitmap_bit_p (&lra_optional_reload_pseudos, spill_regno))\n+\t  goto fail;\n+      insn_pseudos_num = 0;\n+      if (lra_dump_file != NULL)\n+\tfprintf (lra_dump_file, \"\t Trying %d:\", hard_regno);\n+      sparseset_clear (live_range_reload_inheritance_pseudos);\n+      EXECUTE_IF_SET_IN_BITMAP (&spill_pseudos_bitmap, 0, spill_regno, bi)\n+\t{\n+\t  if (bitmap_bit_p (&insn_conflict_pseudos, spill_regno))\n+\t    insn_pseudos_num++;\n+\t  mode2 = PSEUDO_REGNO_MODE (spill_regno);\n+\t  update_lives (spill_regno, true);\n+\t  if (lra_dump_file != NULL)\n+\t    fprintf (lra_dump_file, \" spill %d(freq=%d)\",\n+\t\t     spill_regno, lra_reg_info[spill_regno].freq);\n+\t  add_to_hard_reg_set (&spilled_hard_regs,\n+\t\t\t       mode2, reg_renumber[spill_regno]);\n+\t  for (r = lra_reg_info[spill_regno].live_ranges;\n+\t       r != NULL;\n+\t       r = r->next)\n+\t    {\n+\t      for (p = r->start; p <= r->finish; p++)\n+\t\t{\n+\t\t  lra_live_range_t r2;\n+\t\t  \n+\t\t  for (r2 = start_point_ranges[p];\n+\t\t       r2 != NULL;\n+\t\t       r2 = r2->start_next)\n+\t\t    if (r2->regno >= lra_constraint_new_regno_start)\n+\t\t      sparseset_set_bit (live_range_reload_inheritance_pseudos,\n+\t\t\t\t\t r2->regno);\n+\t\t}\n+\t    }\n+\t}\n+      hard_regno = find_hard_regno_for (regno, &cost, -1);\n+      if (hard_regno >= 0)\n+\t{\n+\t  assign_temporarily (regno, hard_regno);\n+\t  n = 0;\n+\t  EXECUTE_IF_SET_IN_SPARSESET (live_range_reload_inheritance_pseudos,\n+\t\t\t\t       reload_regno)\n+\t    if (live_pseudos_reg_renumber[reload_regno] < 0\n+\t\t&& (hard_reg_set_intersect_p\n+\t\t    (reg_class_contents\n+\t\t     [regno_allocno_class_array[reload_regno]],\n+\t\t     spilled_hard_regs)))\n+\t      sorted_reload_pseudos[n++] = reload_regno;\n+\t  qsort (sorted_reload_pseudos, n, sizeof (int),\n+\t\t reload_pseudo_compare_func);\n+\t  for (j = 0; j < n; j++)\n+\t    {\n+\t      reload_regno = sorted_reload_pseudos[j];\n+\t      lra_assert (live_pseudos_reg_renumber[reload_regno] < 0);\n+\t      if ((reload_hard_regno\n+\t\t   = find_hard_regno_for (reload_regno,\n+\t\t\t\t\t  &reload_cost, -1)) >= 0\n+\t\t  && (overlaps_hard_reg_set_p\n+\t\t      (spilled_hard_regs,\n+\t\t       PSEUDO_REGNO_MODE (reload_regno), reload_hard_regno)))\n+\t\t{\n+\t\t  if (lra_dump_file != NULL)\n+\t\t    fprintf (lra_dump_file, \" assign %d(cost=%d)\",\n+\t\t\t     reload_regno, reload_cost);\n+\t\t  assign_temporarily (reload_regno, reload_hard_regno);\n+\t\t  cost += reload_cost;\n+\t\t}\n+\t    }\n+\t  EXECUTE_IF_SET_IN_BITMAP (&spill_pseudos_bitmap, 0, spill_regno, bi)\n+\t    {\n+\t      rtx x;\n+\t      \n+\t      cost += lra_reg_info[spill_regno].freq;\n+\t      if (ira_reg_equiv[spill_regno].memory != NULL\n+\t\t  || ira_reg_equiv[spill_regno].constant != NULL)\n+\t\tfor (x = ira_reg_equiv[spill_regno].init_insns;\n+\t\t     x != NULL;\n+\t\t     x = XEXP (x, 1))\n+\t\t  cost -= REG_FREQ_FROM_BB (BLOCK_FOR_INSN (XEXP (x, 0)));\n+\t    }\n+\t  if (best_insn_pseudos_num > insn_pseudos_num\n+\t      || (best_insn_pseudos_num == insn_pseudos_num\n+\t\t  && best_cost > cost))\n+\t    {\n+\t      best_insn_pseudos_num = insn_pseudos_num;\n+\t      best_cost = cost;\n+\t      best_hard_regno = hard_regno;\n+\t      bitmap_copy (&best_spill_pseudos_bitmap, &spill_pseudos_bitmap);\n+\t      if (lra_dump_file != NULL)\n+\t\tfprintf (lra_dump_file, \"\t Now best %d(cost=%d)\\n\",\n+\t\t\t hard_regno, cost);\n+\t    }\n+\t  assign_temporarily (regno, -1);\n+\t  for (j = 0; j < n; j++)\n+\t    {\n+\t      reload_regno = sorted_reload_pseudos[j];\n+\t      if (live_pseudos_reg_renumber[reload_regno] >= 0)\n+\t\tassign_temporarily (reload_regno, -1);\n+\t    }\n+\t}\n+      if (lra_dump_file != NULL)\n+\tfprintf (lra_dump_file, \"\\n\");\n+      /* Restore the live hard reg pseudo info for spilled pseudos.  */\n+      EXECUTE_IF_SET_IN_BITMAP (&spill_pseudos_bitmap, 0, spill_regno, bi)\n+\tupdate_lives (spill_regno, false);\n+    fail:\n+      ;\n+    }\n+  /* Spill: */\n+  EXECUTE_IF_SET_IN_BITMAP (&best_spill_pseudos_bitmap, 0, spill_regno, bi)\n+    {\n+      if (lra_dump_file != NULL)\n+\tfprintf (lra_dump_file, \"      Spill %sr%d(hr=%d, freq=%d) for r%d\\n\",\n+\t\t ((int) spill_regno < lra_constraint_new_regno_start\n+\t\t  ? \"\"\n+\t\t  : bitmap_bit_p (&lra_inheritance_pseudos, spill_regno)\n+\t\t  ? \"inheritance \"\n+\t\t  : bitmap_bit_p (&lra_split_regs, spill_regno)\n+\t\t  ? \"split \"\n+\t\t  : bitmap_bit_p (&lra_optional_reload_pseudos, spill_regno)\n+\t\t  ? \"optional reload \" : \"reload \"),\n+\t\t spill_regno, reg_renumber[spill_regno],\n+\t\t lra_reg_info[spill_regno].freq, regno);\n+      update_lives (spill_regno, true);\n+      lra_setup_reg_renumber (spill_regno, -1, false);\n+    }\n+  bitmap_ior_into (spilled_pseudo_bitmap, &best_spill_pseudos_bitmap);\n+  return best_hard_regno;\n+}\n+\n+/* Assign HARD_REGNO to REGNO.\t*/\n+static void\n+assign_hard_regno (int hard_regno, int regno)\n+{\n+  int i;\n+\n+  lra_assert (hard_regno >= 0);\n+  lra_setup_reg_renumber (regno, hard_regno, true);\n+  update_lives (regno, false);\n+  for (i = 0;\n+       i < hard_regno_nregs[hard_regno][lra_reg_info[regno].biggest_mode];\n+       i++)\n+    df_set_regs_ever_live (hard_regno + i, true);\n+}\n+\n+/* Array used for sorting different pseudos.  */\n+static int *sorted_pseudos;\n+\n+/* The constraints pass is allowed to create equivalences between\n+   pseudos that make the current allocation \"incorrect\" (in the sense\n+   that pseudos are assigned to hard registers from their own conflict\n+   sets).  The global variable lra_risky_transformations_p says\n+   whether this might have happened.\n+\n+   Process pseudos assigned to hard registers (less frequently used\n+   first), spill if a conflict is found, and mark the spilled pseudos\n+   in SPILLED_PSEUDO_BITMAP.  Set up LIVE_HARD_REG_PSEUDOS from\n+   pseudos, assigned to hard registers.\t */\n+static void\n+setup_live_pseudos_and_spill_after_risky_transforms (bitmap\n+\t\t\t\t\t\t     spilled_pseudo_bitmap)\n+{\n+  int p, i, j, n, regno, hard_regno;\n+  unsigned int k, conflict_regno;\n+  int val;\n+  HARD_REG_SET conflict_set;\n+  enum machine_mode mode;\n+  lra_live_range_t r;\n+  bitmap_iterator bi;\n+  int max_regno = max_reg_num ();\n+\n+  if (! lra_risky_transformations_p)\n+    {\n+      for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+\tif (reg_renumber[i] >= 0 && lra_reg_info[i].nrefs > 0)\n+\t  update_lives (i, false);\n+      return;\n+    }\n+  for (n = 0, i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+    if (reg_renumber[i] >= 0 && lra_reg_info[i].nrefs > 0)\n+      sorted_pseudos[n++] = i;\n+  qsort (sorted_pseudos, n, sizeof (int), pseudo_compare_func);\n+  for (i = n - 1; i >= 0; i--)\n+    {\n+      regno = sorted_pseudos[i];\n+      hard_regno = reg_renumber[regno];\n+      lra_assert (hard_regno >= 0);\n+      mode = lra_reg_info[regno].biggest_mode;\n+      sparseset_clear (live_range_hard_reg_pseudos);\n+      for (r = lra_reg_info[regno].live_ranges; r != NULL; r = r->next)\n+\t{\n+\t  EXECUTE_IF_SET_IN_BITMAP (&live_hard_reg_pseudos[r->start], 0, k, bi)\n+\t    sparseset_set_bit (live_range_hard_reg_pseudos, k);\n+\t  for (p = r->start + 1; p <= r->finish; p++)\n+\t    {\n+\t      lra_live_range_t r2;\n+\t      \n+\t      for (r2 = start_point_ranges[p];\n+\t\t   r2 != NULL;\n+\t\t   r2 = r2->start_next)\n+\t\tif (live_pseudos_reg_renumber[r2->regno] >= 0)\n+\t\t  sparseset_set_bit (live_range_hard_reg_pseudos, r2->regno);\n+\t    }\n+\t}\n+      COPY_HARD_REG_SET (conflict_set, lra_no_alloc_regs);\n+      IOR_HARD_REG_SET (conflict_set, lra_reg_info[regno].conflict_hard_regs);\n+      val = lra_reg_info[regno].val;\n+      EXECUTE_IF_SET_IN_SPARSESET (live_range_hard_reg_pseudos, conflict_regno)\n+\tif (val != lra_reg_info[conflict_regno].val\n+\t    /* If it is multi-register pseudos they should start on\n+\t       the same hard register.\t*/\n+\t    || hard_regno != reg_renumber[conflict_regno])\n+\t  add_to_hard_reg_set (&conflict_set,\n+\t\t\t       lra_reg_info[conflict_regno].biggest_mode,\n+\t\t\t       reg_renumber[conflict_regno]);\n+      if (! overlaps_hard_reg_set_p (conflict_set, mode, hard_regno))\n+\t{\n+\t  update_lives (regno, false);\n+\t  continue;\n+\t}\n+      bitmap_set_bit (spilled_pseudo_bitmap, regno);\n+      for (j = 0;\n+\t   j < hard_regno_nregs[hard_regno][PSEUDO_REGNO_MODE (regno)];\n+\t   j++)\n+\tlra_hard_reg_usage[hard_regno + j] -= lra_reg_info[regno].freq;\n+      reg_renumber[regno] = -1;\n+      if (lra_dump_file != NULL)\n+\tfprintf (lra_dump_file, \"    Spill r%d after risky transformations\\n\",\n+\t\t regno);\n+    }\n+}\n+\n+/* Improve allocation by assigning the same hard regno of inheritance\n+   pseudos to the connected pseudos.  We need this because inheritance\n+   pseudos are allocated after reload pseudos in the thread and when\n+   we assign a hard register to a reload pseudo we don't know yet that\n+   the connected inheritance pseudos can get the same hard register.\n+   Add pseudos with changed allocation to bitmap CHANGED_PSEUDOS.  */\n+static void\n+improve_inheritance (bitmap changed_pseudos)\n+{\n+  unsigned int k;\n+  int regno, another_regno, hard_regno, another_hard_regno, cost, i, n;\n+  lra_copy_t cp, next_cp;\n+  bitmap_iterator bi;\n+\n+  n = 0;\n+  EXECUTE_IF_SET_IN_BITMAP (&lra_inheritance_pseudos, 0, k, bi)\n+    if (reg_renumber[k] >= 0 && lra_reg_info[k].nrefs != 0)\n+      sorted_pseudos[n++] = k;\n+  qsort (sorted_pseudos, n, sizeof (int), pseudo_compare_func);\n+  for (i = 0; i < n; i++)\n+    {\n+      regno = sorted_pseudos[i];\n+      hard_regno = reg_renumber[regno];\n+      lra_assert (hard_regno >= 0);\n+      for (cp = lra_reg_info[regno].copies; cp != NULL; cp = next_cp)\n+\t{\n+\t  if (cp->regno1 == regno)\n+\t    {\n+\t      next_cp = cp->regno1_next;\n+\t      another_regno = cp->regno2;\n+\t    }\n+\t  else if (cp->regno2 == regno)\n+\t    {\n+\t      next_cp = cp->regno2_next;\n+\t      another_regno = cp->regno1;\n+\t    }\n+\t  else\n+\t    gcc_unreachable ();\n+\t  /* Don't change reload pseudo allocation.  It might have\n+\t     this allocation for a purpose and changing it can result\n+\t     in LRA cycling.  */\n+\t  if ((another_regno < lra_constraint_new_regno_start\n+\t       || bitmap_bit_p (&lra_inheritance_pseudos, another_regno))\n+\t      && (another_hard_regno = reg_renumber[another_regno]) >= 0\n+\t      && another_hard_regno != hard_regno)\n+\t    {\n+\t      if (lra_dump_file != NULL)\n+\t\tfprintf\n+\t\t  (lra_dump_file,\n+\t\t   \"\tImproving inheritance for %d(%d) and %d(%d)...\\n\",\n+\t\t   regno, hard_regno, another_regno, another_hard_regno);\n+\t      update_lives (another_regno, true);\n+\t      lra_setup_reg_renumber (another_regno, -1, false);\n+\t      if (hard_regno\n+\t\t  == find_hard_regno_for (another_regno, &cost, hard_regno))\n+\t\tassign_hard_regno (hard_regno, another_regno);\n+\t      else\n+\t\tassign_hard_regno (another_hard_regno, another_regno);\n+\t      bitmap_set_bit (changed_pseudos, another_regno);\n+\t    }\n+\t}\n+    }\n+}\n+\n+\n+/* Bitmap finally containing all pseudos spilled on this assignment\n+   pass.  */\n+static bitmap_head all_spilled_pseudos;\n+/* All pseudos whose allocation was changed.  */\n+static bitmap_head changed_pseudo_bitmap;\n+\n+/* Assign hard registers to reload pseudos and other pseudos.  */\n+static void\n+assign_by_spills (void)\n+{\n+  int i, n, nfails, iter, regno, hard_regno, cost, restore_regno;\n+  rtx insn;\n+  basic_block bb;\n+  bitmap_head changed_insns, do_not_assign_nonreload_pseudos;\n+  bitmap_head non_reload_pseudos;\n+  unsigned int u;\n+  bitmap_iterator bi;\n+  int max_regno = max_reg_num ();\n+\n+  for (n = 0, i = lra_constraint_new_regno_start; i < max_regno; i++)\n+    if (reg_renumber[i] < 0 && lra_reg_info[i].nrefs != 0\n+\t&& regno_allocno_class_array[i] != NO_REGS)\n+      sorted_pseudos[n++] = i;\n+  bitmap_initialize (&insn_conflict_pseudos, &reg_obstack);\n+  bitmap_initialize (&spill_pseudos_bitmap, &reg_obstack);\n+  bitmap_initialize (&best_spill_pseudos_bitmap, &reg_obstack);\n+  update_hard_regno_preference_check = XCNEWVEC (int, max_regno);\n+  curr_update_hard_regno_preference_check = 0;\n+  memset (try_hard_reg_pseudos_check, 0, sizeof (try_hard_reg_pseudos_check));\n+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+    bitmap_initialize (&try_hard_reg_pseudos[i], &reg_obstack);\n+  curr_pseudo_check = 0;\n+  bitmap_initialize (&changed_insns, &reg_obstack);\n+  bitmap_initialize (&non_reload_pseudos, &reg_obstack);\n+  bitmap_ior (&non_reload_pseudos, &lra_inheritance_pseudos, &lra_split_regs);\n+  bitmap_ior_into (&non_reload_pseudos, &lra_optional_reload_pseudos);\n+  for (iter = 0; iter <= 1; iter++)\n+    {\n+      qsort (sorted_pseudos, n, sizeof (int), reload_pseudo_compare_func);\n+      nfails = 0;\n+      for (i = 0; i < n; i++)\n+\t{\n+\t  regno = sorted_pseudos[i];\n+\t  if (lra_dump_file != NULL)\n+\t    fprintf (lra_dump_file, \"\t Assigning to %d \"\n+\t\t     \"(cl=%s, orig=%d, freq=%d, tfirst=%d, tfreq=%d)...\\n\",\n+\t\t     regno, reg_class_names[regno_allocno_class_array[regno]],\n+\t\t     ORIGINAL_REGNO (regno_reg_rtx[regno]),\n+\t\t     lra_reg_info[regno].freq, regno_assign_info[regno].first,\n+\t\t     regno_assign_info[regno_assign_info[regno].first].freq);\n+\t  hard_regno = find_hard_regno_for (regno, &cost, -1);\n+\t  if (hard_regno < 0\n+\t      && ! bitmap_bit_p (&non_reload_pseudos, regno))\n+\t    hard_regno = spill_for (regno, &all_spilled_pseudos);\n+\t  if (hard_regno < 0)\n+\t    {\n+\t      if (! bitmap_bit_p (&non_reload_pseudos, regno))\n+\t\tsorted_pseudos[nfails++] = regno;\n+\t    }\n+\t  else\n+\t    {\n+\t      /* This register might have been spilled by the previous\n+\t\t pass.  Indicate that it is no longer spilled.  */\n+\t      bitmap_clear_bit (&all_spilled_pseudos, regno);\n+\t      assign_hard_regno (hard_regno, regno);\n+\t    }\n+\t}\n+      if (nfails == 0)\n+\tbreak;\n+      lra_assert (iter == 0);\n+      /* This is a very rare event.  We can not assign a hard\n+\t register to reload pseudo because the hard register was\n+\t assigned to another reload pseudo on a previous\n+\t assignment pass.  For x86 example, on the 1st pass we\n+\t assigned CX (although another hard register could be used\n+\t for this) to reload pseudo in an insn, on the 2nd pass we\n+\t need CX (and only this) hard register for a new reload\n+\t pseudo in the same insn.  */\n+      if (lra_dump_file != NULL)\n+\tfprintf (lra_dump_file, \"  2nd iter for reload pseudo assignments:\\n\");\n+      for (i = 0; i < nfails; i++)\n+\t{\n+\t  if (lra_dump_file != NULL)\n+\t    fprintf (lra_dump_file, \"\t Reload r%d assignment failure\\n\",\n+\t\t     sorted_pseudos[i]);\n+\t  bitmap_ior_into (&changed_insns,\n+\t\t\t   &lra_reg_info[sorted_pseudos[i]].insn_bitmap);\n+\t}\n+      FOR_EACH_BB (bb)\n+\tFOR_BB_INSNS (bb, insn)\n+\tif (bitmap_bit_p (&changed_insns, INSN_UID (insn)))\n+\t  {\n+\t    lra_insn_recog_data_t data;\n+\t    struct lra_insn_reg *r;\n+\t      \n+\t    data = lra_get_insn_recog_data (insn);\n+\t    for (r = data->regs; r != NULL; r = r->next)\n+\t      {\n+\t\tregno = r->regno;\n+\t\t/* A reload pseudo did not get a hard register on the\n+\t\t   first iteration because of the conflict with\n+\t\t   another reload pseudos in the same insn.  So we\n+\t\t   consider only reload pseudos assigned to hard\n+\t\t   registers.  We shall exclude inheritance pseudos as\n+\t\t   they can occur in original insns (not reload ones).\n+\t\t   We can omit the check for split pseudos because\n+\t\t   they occur only in move insns containing non-reload\n+\t\t   pseudos.  */\n+\t\tif (regno < lra_constraint_new_regno_start\n+\t\t    || bitmap_bit_p (&lra_inheritance_pseudos, regno)\n+\t\t    || reg_renumber[regno] < 0)\n+\t\t  continue;\n+\t\tsorted_pseudos[nfails++] = regno;\n+\t\tif (lra_dump_file != NULL)\n+\t\t  fprintf (lra_dump_file,\n+\t\t\t   \"\t  Spill reload r%d(hr=%d, freq=%d)\\n\",\n+\t\t\t   regno, reg_renumber[regno],\n+\t\t\t   lra_reg_info[regno].freq);\n+\t\tupdate_lives (regno, true);\n+\t\tlra_setup_reg_renumber (regno, -1, false);\n+\t      }\n+\t  }\n+      n = nfails;\n+    }\n+  improve_inheritance (&changed_pseudo_bitmap);\n+  bitmap_clear (&non_reload_pseudos);\n+  bitmap_clear (&changed_insns);\n+  if (! lra_simple_p)\n+    {\n+      /* We should not assign to original pseudos of inheritance\n+\t pseudos or split pseudos if any its inheritance pseudo did\n+\t not get hard register or any its split pseudo was not split\n+\t because undo inheritance/split pass will extend live range of\n+\t such inheritance or split pseudos.  */\n+      bitmap_initialize (&do_not_assign_nonreload_pseudos, &reg_obstack);\n+      EXECUTE_IF_SET_IN_BITMAP (&lra_inheritance_pseudos, 0, u, bi)\n+\tif ((restore_regno = lra_reg_info[u].restore_regno) >= 0\n+\t    && reg_renumber[u] < 0\n+\t    && bitmap_bit_p (&lra_inheritance_pseudos, u))\n+\t  bitmap_set_bit (&do_not_assign_nonreload_pseudos, restore_regno);\n+      EXECUTE_IF_SET_IN_BITMAP (&lra_split_regs, 0, u, bi)\n+\tif ((restore_regno = lra_reg_info[u].restore_regno) >= 0\n+\t    && reg_renumber[u] >= 0)\n+\t  bitmap_set_bit (&do_not_assign_nonreload_pseudos, restore_regno);\n+      for (n = 0, i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+\tif (((i < lra_constraint_new_regno_start\n+\t      && ! bitmap_bit_p (&do_not_assign_nonreload_pseudos, i))\n+\t     || (bitmap_bit_p (&lra_inheritance_pseudos, i)\n+\t\t && lra_reg_info[i].restore_regno >= 0)\n+\t     || (bitmap_bit_p (&lra_split_regs, i)\n+\t\t && lra_reg_info[i].restore_regno >= 0)\n+\t     || bitmap_bit_p (&lra_optional_reload_pseudos, i))\n+\t    && reg_renumber[i] < 0 && lra_reg_info[i].nrefs != 0\n+\t    && regno_allocno_class_array[i] != NO_REGS)\n+\t  sorted_pseudos[n++] = i;\n+      bitmap_clear (&do_not_assign_nonreload_pseudos);\n+      if (n != 0 && lra_dump_file != NULL)\n+\tfprintf (lra_dump_file, \"  Reassigning non-reload pseudos\\n\");\n+      qsort (sorted_pseudos, n, sizeof (int), pseudo_compare_func);\n+      for (i = 0; i < n; i++)\n+\t{\n+\t  regno = sorted_pseudos[i];\n+\t  hard_regno = find_hard_regno_for (regno, &cost, -1);\n+\t  if (hard_regno >= 0)\n+\t    {\n+\t      assign_hard_regno (hard_regno, regno);\n+\t  /* We change allocation for non-reload pseudo on this\n+\t     iteration -- mark the pseudo for invalidation of used\n+\t     alternatives of insns containing the pseudo.  */\n+\t      bitmap_set_bit (&changed_pseudo_bitmap, regno);\n+\t    }\n+\t}\n+    }\n+  free (update_hard_regno_preference_check);\n+  bitmap_clear (&best_spill_pseudos_bitmap);\n+  bitmap_clear (&spill_pseudos_bitmap);\n+  bitmap_clear (&insn_conflict_pseudos);\n+}\n+\n+\n+/* Entry function to assign hard registers to new reload pseudos\n+   starting with LRA_CONSTRAINT_NEW_REGNO_START (by possible spilling\n+   of old pseudos) and possibly to the old pseudos.  The function adds\n+   what insns to process for the next constraint pass.\tThose are all\n+   insns who contains non-reload and non-inheritance pseudos with\n+   changed allocation.\n+\n+   Return true if we did not spill any non-reload and non-inheritance\n+   pseudos.  */\n+bool\n+lra_assign (void)\n+{\n+  int i;\n+  unsigned int u;\n+  bitmap_iterator bi;\n+  bitmap_head insns_to_process;\n+  bool no_spills_p;\n+  int max_regno = max_reg_num ();\n+\n+  timevar_push (TV_LRA_ASSIGN);\n+  init_lives ();\n+  sorted_pseudos = XNEWVEC (int, max_regno);\n+  sorted_reload_pseudos = XNEWVEC (int, max_regno);\n+  regno_allocno_class_array = XNEWVEC (enum reg_class, max_regno);\n+  for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+    regno_allocno_class_array[i] = lra_get_allocno_class (i);\n+  init_regno_assign_info ();\n+  bitmap_initialize (&all_spilled_pseudos, &reg_obstack);\n+  create_live_range_start_chains ();\n+  setup_live_pseudos_and_spill_after_risky_transforms (&all_spilled_pseudos);\n+#ifdef ENABLE_CHECKING\n+  for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+    if (lra_reg_info[i].nrefs != 0 && reg_renumber[i] >= 0\n+\t&& lra_reg_info[i].call_p\n+\t&& overlaps_hard_reg_set_p (call_used_reg_set,\n+\t\t\t\t    PSEUDO_REGNO_MODE (i), reg_renumber[i]))\n+      gcc_unreachable ();\n+#endif\n+  /* Setup insns to process on the next constraint pass.  */\n+  bitmap_initialize (&changed_pseudo_bitmap, &reg_obstack);\n+  init_live_reload_and_inheritance_pseudos ();\n+  assign_by_spills ();\n+  finish_live_reload_and_inheritance_pseudos ();\n+  bitmap_ior_into (&changed_pseudo_bitmap, &all_spilled_pseudos);\n+  no_spills_p = true;\n+  EXECUTE_IF_SET_IN_BITMAP (&all_spilled_pseudos, 0, u, bi)\n+    /* We ignore spilled pseudos created on last inheritance pass\n+       because they will be removed.  */\n+    if (lra_reg_info[u].restore_regno < 0)\n+      {\n+\tno_spills_p = false;\n+\tbreak;\n+      }\n+  finish_live_range_start_chains ();\n+  bitmap_clear (&all_spilled_pseudos);\n+  bitmap_initialize (&insns_to_process, &reg_obstack);\n+  EXECUTE_IF_SET_IN_BITMAP (&changed_pseudo_bitmap, 0, u, bi)\n+    bitmap_ior_into (&insns_to_process, &lra_reg_info[u].insn_bitmap);\n+  bitmap_clear (&changed_pseudo_bitmap);\n+  EXECUTE_IF_SET_IN_BITMAP (&insns_to_process, 0, u, bi)\n+    {\n+      lra_push_insn_by_uid (u);\n+      /* Invalidate alternatives for insn should be processed.\t*/\n+      lra_set_used_insn_alternative_by_uid (u, -1);\n+    }\n+  bitmap_clear (&insns_to_process);\n+  finish_regno_assign_info ();\n+  free (regno_allocno_class_array);\n+  free (sorted_pseudos);\n+  free (sorted_reload_pseudos);\n+  finish_lives ();\n+  timevar_pop (TV_LRA_ASSIGN);\n+  return no_spills_p;\n+}"}, {"sha": "57c3111b9221e450193e0842752a44ee494b5793", "filename": "gcc/lra-coalesce.c", "status": "added", "additions": 351, "deletions": 0, "changes": 351, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-coalesce.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-coalesce.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flra-coalesce.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -0,0 +1,351 @@\n+/* Coalesce spilled pseudos.\n+   Copyright (C) 2010, 2011, 2012\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.\tIf not see\n+<http://www.gnu.org/licenses/>.\t */\n+\n+\n+/* This file contains a pass making some simple RTL code\n+   transformations by coalescing pseudos to remove some move insns.\n+\n+   Spilling pseudos in LRA can create memory-memory moves.  We should\n+   remove potential memory-memory moves before the next constraint\n+   pass because the constraint pass will generate additional insns for\n+   such moves and all these insns will be hard to remove afterwards.\n+\n+   Here we coalesce only spilled pseudos.  Coalescing non-spilled\n+   pseudos (with different hard regs) might result in spilling\n+   additional pseudos because of possible conflicts with other\n+   non-spilled pseudos and, as a consequence, in more constraint\n+   passes and even LRA infinite cycling.  Trivial the same hard\n+   register moves will be removed by subsequent compiler passes.\n+\n+   We don't coalesce special reload pseudos.  It complicates LRA code\n+   a lot without visible generated code improvement.\n+\n+   The pseudo live-ranges are used to find conflicting pseudos during\n+   coalescing.\n+\n+   Most frequently executed moves is tried to be coalesced first.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"output.h\"\n+#include \"regs.h\"\n+#include \"hard-reg-set.h\"\n+#include \"flags.h\"\n+#include \"function.h\"\n+#include \"expr.h\"\n+#include \"basic-block.h\"\n+#include \"except.h\"\n+#include \"timevar.h\"\n+#include \"ira.h\"\n+#include \"lra-int.h\"\n+#include \"df.h\"\n+\n+/* Arrays whose elements represent the first and the next pseudo\n+   (regno) in the coalesced pseudos group to which given pseudo (its\n+   regno is the index) belongs.\t The next of the last pseudo in the\n+   group refers to the first pseudo in the group, in other words the\n+   group is represented by a cyclic list.  */\n+static int *first_coalesced_pseudo, *next_coalesced_pseudo;\n+\n+/* The function is used to sort moves according to their execution\n+   frequencies.\t */\n+static int\n+move_freq_compare_func (const void *v1p, const void *v2p)\n+{\n+  rtx mv1 = *(const rtx *) v1p;\n+  rtx mv2 = *(const rtx *) v2p;\n+  int pri1, pri2;\n+  \n+  pri1 = BLOCK_FOR_INSN (mv1)->frequency;\n+  pri2 = BLOCK_FOR_INSN (mv2)->frequency;\n+  if (pri2 - pri1)\n+    return pri2 - pri1;\n+\n+  /* If frequencies are equal, sort by moves, so that the results of\n+     qsort leave nothing to chance.  */\n+  return (int) INSN_UID (mv1) - (int) INSN_UID (mv2);\n+}\n+\n+/* Pseudos which go away after coalescing.  */\n+static bitmap_head coalesced_pseudos_bitmap;\n+\n+/* Merge two sets of coalesced pseudos given correspondingly by\n+   pseudos REGNO1 and REGNO2 (more accurately merging REGNO2 group\n+   into REGNO1 group).\tSet up COALESCED_PSEUDOS_BITMAP.  */\n+static void\n+merge_pseudos (int regno1, int regno2)\n+{\n+  int regno, first, first2, last, next;\n+\n+  first = first_coalesced_pseudo[regno1];\n+  if ((first2 = first_coalesced_pseudo[regno2]) == first)\n+    return;\n+  for (last = regno2, regno = next_coalesced_pseudo[regno2];;\n+       regno = next_coalesced_pseudo[regno])\n+    {\n+      first_coalesced_pseudo[regno] = first;\n+      bitmap_set_bit (&coalesced_pseudos_bitmap, regno);\n+      if (regno == regno2)\n+\tbreak;\n+      last = regno;\n+    }\n+  next = next_coalesced_pseudo[first];\n+  next_coalesced_pseudo[first] = regno2;\n+  next_coalesced_pseudo[last] = next;\n+  lra_reg_info[first].live_ranges\n+    = (lra_merge_live_ranges\n+       (lra_reg_info[first].live_ranges,\n+\tlra_copy_live_range_list (lra_reg_info[first2].live_ranges)));\n+  if (GET_MODE_SIZE (lra_reg_info[first].biggest_mode)\n+      < GET_MODE_SIZE (lra_reg_info[first2].biggest_mode))\n+    lra_reg_info[first].biggest_mode = lra_reg_info[first2].biggest_mode;\n+}\n+\n+/* Change pseudos in *LOC on their coalescing group\n+   representatives.  */\n+static bool\n+substitute (rtx *loc)\n+{\n+  int i, regno;\n+  const char *fmt;\n+  enum rtx_code code;\n+  bool res;\n+\n+  if (*loc == NULL_RTX)\n+    return false;\n+  code = GET_CODE (*loc);\n+  if (code == REG)\n+    {\n+      regno = REGNO (*loc);\n+      if (regno < FIRST_PSEUDO_REGISTER\n+\t  || first_coalesced_pseudo[regno] == regno)\n+\treturn false;\n+      *loc = regno_reg_rtx[first_coalesced_pseudo[regno]];\n+      return true;\n+    }\n+\n+  res = false;\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\t{\n+\t  if (substitute (&XEXP (*loc, i)))\n+\t    res = true;\n+\t}\n+      else if (fmt[i] == 'E')\n+\t{\n+\t  int j;\n+\n+\t  for (j = XVECLEN (*loc, i) - 1; j >= 0; j--)\n+\t    if (substitute (&XVECEXP (*loc, i, j)))\n+\t      res = true;\n+\t}\n+    }\n+  return res;\n+}\n+\n+/* The current iteration (1, 2, ...) of the coalescing pass.  */\n+int lra_coalesce_iter;\n+\n+/* Return true if the move involving REGNO1 and REGNO2 is a potential\n+   memory-memory move.\t*/\n+static bool\n+mem_move_p (int regno1, int regno2)\n+{\n+  return reg_renumber[regno1] < 0 && reg_renumber[regno2] < 0;\n+}\n+\n+/* Pseudos used instead of the coalesced pseudos.  */\n+static bitmap_head used_pseudos_bitmap;\n+\n+/* Set up USED_PSEUDOS_BITMAP, and update LR_BITMAP (a BB live info\n+   bitmap).  */\n+static void\n+update_live_info (bitmap lr_bitmap)\n+{\n+  unsigned int j;\n+  bitmap_iterator bi;\n+\n+  bitmap_clear (&used_pseudos_bitmap);\n+  EXECUTE_IF_AND_IN_BITMAP (&coalesced_pseudos_bitmap, lr_bitmap,\n+\t\t\t    FIRST_PSEUDO_REGISTER, j, bi)\n+    bitmap_set_bit (&used_pseudos_bitmap, first_coalesced_pseudo[j]);\n+  if (! bitmap_empty_p (&used_pseudos_bitmap))\n+    {\n+      bitmap_and_compl_into (lr_bitmap, &coalesced_pseudos_bitmap);\n+      bitmap_ior_into (lr_bitmap, &used_pseudos_bitmap);\n+    }\n+}\n+\n+/* Return true if pseudo REGNO can be potentially coalesced.  Use\n+   SPLIT_PSEUDO_BITMAP to find pseudos whose live ranges were\n+   split.  */\n+static bool\n+coalescable_pseudo_p (int regno, bitmap split_origin_bitmap)\n+{\n+  lra_assert (regno >= FIRST_PSEUDO_REGISTER);\n+  /* Don't coalesce inheritance pseudos because spilled inheritance\n+     pseudos will be removed in subsequent 'undo inheritance'\n+     pass.  */\n+  return (lra_reg_info[regno].restore_regno < 0\n+\t  /* We undo splits for spilled pseudos whose live ranges were\n+\t     split.  So don't coalesce them, it is not necessary and\n+\t     the undo transformations would be wrong.  */\n+\t  && ! bitmap_bit_p (split_origin_bitmap, regno)\n+\t  /* We don't want to coalesce regnos with equivalences, at\n+\t     least without updating this info.  */\n+\t  && ira_reg_equiv[regno].constant == NULL_RTX\n+\t  && ira_reg_equiv[regno].memory == NULL_RTX\n+\t  && ira_reg_equiv[regno].invariant == NULL_RTX);\n+}\n+\n+/* The major function for aggressive pseudo coalescing of moves only\n+   if the both pseudos were spilled and not special reload pseudos.  */\n+bool\n+lra_coalesce (void)\n+{\n+  basic_block bb;\n+  rtx mv, set, insn, next, *sorted_moves;\n+  int i, mv_num, sregno, dregno, restore_regno;\n+  unsigned int regno;\n+  int coalesced_moves;\n+  int max_regno = max_reg_num ();\n+  bitmap_head involved_insns_bitmap, split_origin_bitmap;\n+  bitmap_iterator bi;\n+\n+  timevar_push (TV_LRA_COALESCE);\n+\n+  if (lra_dump_file != NULL)\n+    fprintf (lra_dump_file,\n+\t     \"\\n********** Pseudos coalescing #%d: **********\\n\\n\",\n+\t     ++lra_coalesce_iter);\n+  first_coalesced_pseudo = XNEWVEC (int, max_regno);\n+  next_coalesced_pseudo = XNEWVEC (int, max_regno);\n+  for (i = 0; i < max_regno; i++)\n+    first_coalesced_pseudo[i] = next_coalesced_pseudo[i] = i;\n+  sorted_moves = XNEWVEC (rtx, get_max_uid ());\n+  mv_num = 0;\n+  /* Collect pseudos whose live ranges were split.  */\n+  bitmap_initialize (&split_origin_bitmap, &reg_obstack);\n+  EXECUTE_IF_SET_IN_BITMAP (&lra_split_regs, 0, regno, bi)\n+    if ((restore_regno = lra_reg_info[regno].restore_regno) >= 0)\n+      bitmap_set_bit (&split_origin_bitmap, restore_regno);\n+  /* Collect moves.  */\n+  coalesced_moves = 0;\n+  FOR_EACH_BB (bb)\n+    {\n+      FOR_BB_INSNS_SAFE (bb, insn, next)\n+\tif (INSN_P (insn)\n+\t    && (set = single_set (insn)) != NULL_RTX\n+\t    && REG_P (SET_DEST (set)) && REG_P (SET_SRC (set))\n+\t    && (sregno = REGNO (SET_SRC (set))) >= FIRST_PSEUDO_REGISTER\n+\t    && (dregno = REGNO (SET_DEST (set))) >= FIRST_PSEUDO_REGISTER\n+\t    && mem_move_p (sregno, dregno)\n+\t    && coalescable_pseudo_p (sregno, &split_origin_bitmap)\n+\t    && coalescable_pseudo_p (dregno, &split_origin_bitmap)\n+\t    && ! side_effects_p (set)\n+\t    && !(lra_intersected_live_ranges_p\n+\t\t (lra_reg_info[sregno].live_ranges,\n+\t\t  lra_reg_info[dregno].live_ranges)))\n+\t  sorted_moves[mv_num++] = insn;\n+    }\n+  bitmap_clear (&split_origin_bitmap);\n+  qsort (sorted_moves, mv_num, sizeof (rtx), move_freq_compare_func);\n+  /* Coalesced copies, most frequently executed first.\t*/\n+  bitmap_initialize (&coalesced_pseudos_bitmap, &reg_obstack);\n+  bitmap_initialize (&involved_insns_bitmap, &reg_obstack);\n+  for (i = 0; i < mv_num; i++)\n+    {\n+      mv = sorted_moves[i];\n+      set = single_set (mv);\n+      lra_assert (set != NULL && REG_P (SET_SRC (set))\n+\t\t  && REG_P (SET_DEST (set)));\n+      sregno = REGNO (SET_SRC (set));\n+      dregno = REGNO (SET_DEST (set));\n+      if (first_coalesced_pseudo[sregno] == first_coalesced_pseudo[dregno])\n+\t{\n+\t  coalesced_moves++;\n+\t  if (lra_dump_file != NULL)\n+\t    fprintf\n+\t      (lra_dump_file, \"      Coalescing move %i:r%d-r%d (freq=%d)\\n\",\n+\t       INSN_UID (mv), sregno, dregno,\n+\t       BLOCK_FOR_INSN (mv)->frequency);\n+\t  /* We updated involved_insns_bitmap when doing the merge.  */\n+\t}\n+      else if (!(lra_intersected_live_ranges_p\n+\t\t (lra_reg_info[first_coalesced_pseudo[sregno]].live_ranges,\n+\t\t  lra_reg_info[first_coalesced_pseudo[dregno]].live_ranges)))\n+\t{\n+\t  coalesced_moves++;\n+\t  if (lra_dump_file != NULL)\n+\t    fprintf\n+\t      (lra_dump_file,\n+\t       \"  Coalescing move %i:r%d(%d)-r%d(%d) (freq=%d)\\n\",\n+\t       INSN_UID (mv), sregno, ORIGINAL_REGNO (SET_SRC (set)),\n+\t       dregno, ORIGINAL_REGNO (SET_DEST (set)),\n+\t       BLOCK_FOR_INSN (mv)->frequency);\n+\t  bitmap_ior_into (&involved_insns_bitmap,\n+\t\t\t   &lra_reg_info[sregno].insn_bitmap);\n+\t  bitmap_ior_into (&involved_insns_bitmap,\n+\t\t\t   &lra_reg_info[dregno].insn_bitmap);\n+\t  merge_pseudos (sregno, dregno);\n+\t}\n+    }\n+  bitmap_initialize (&used_pseudos_bitmap, &reg_obstack);\n+  FOR_EACH_BB (bb)\n+    {\n+      update_live_info (df_get_live_in (bb));\n+      update_live_info (df_get_live_out (bb));\n+      FOR_BB_INSNS_SAFE (bb, insn, next)\n+\tif (INSN_P (insn)\n+\t    && bitmap_bit_p (&involved_insns_bitmap, INSN_UID (insn)))\n+\t  {\n+\t    if (! substitute (&insn))\n+\t      continue;\n+\t    lra_update_insn_regno_info (insn);\n+\t    if ((set = single_set (insn)) != NULL_RTX && set_noop_p (set))\n+\t      {\n+\t\t/* Coalesced move.  */\n+\t\tif (lra_dump_file != NULL)\n+\t\t  fprintf (lra_dump_file, \"\t Removing move %i (freq=%d)\\n\",\n+\t\t\t INSN_UID (insn), BLOCK_FOR_INSN (insn)->frequency);\n+\t\tlra_set_insn_deleted (insn);\n+\t      }\n+\t  }\n+    }\n+  bitmap_clear (&used_pseudos_bitmap);\n+  bitmap_clear (&involved_insns_bitmap);\n+  bitmap_clear (&coalesced_pseudos_bitmap);\n+  if (lra_dump_file != NULL && coalesced_moves != 0)\n+    fprintf (lra_dump_file, \"Coalesced Moves = %d\\n\", coalesced_moves);\n+  free (sorted_moves);\n+  free (next_coalesced_pseudo);\n+  free (first_coalesced_pseudo);\n+  timevar_pop (TV_LRA_COALESCE);\n+  return coalesced_moves != 0;\n+}"}, {"sha": "ec48e9ea02cd2939b443e9f9c2ded3aacf40c178", "filename": "gcc/lra-constraints.c", "status": "added", "additions": 5130, "deletions": 0, "changes": 5130, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-constraints.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-constraints.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flra-constraints.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234"}, {"sha": "2222d805f1b0d366c6ba60396a428d1de5b70717", "filename": "gcc/lra-eliminations.c", "status": "added", "additions": 1301, "deletions": 0, "changes": 1301, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-eliminations.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-eliminations.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flra-eliminations.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -0,0 +1,1301 @@\n+/* Code for RTL register eliminations.\n+   Copyright (C) 2010, 2011, 2012\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.\tIf not see\n+<http://www.gnu.org/licenses/>.\t */\n+\n+/* Eliminable registers (like a soft argument or frame pointer) are\n+   widely used in RTL.  These eliminable registers should be replaced\n+   by real hard registers (like the stack pointer or hard frame\n+   pointer) plus some offset.  The offsets usually change whenever the\n+   stack is expanded.  We know the final offsets only at the very end\n+   of LRA.\n+\n+   Within LRA, we usually keep the RTL in such a state that the\n+   eliminable registers can be replaced by just the corresponding hard\n+   register (without any offset).  To achieve this we should add the\n+   initial elimination offset at the beginning of LRA and update the\n+   offsets whenever the stack is expanded.  We need to do this before\n+   every constraint pass because the choice of offset often affects\n+   whether a particular address or memory constraint is satisfied.\n+\n+   We keep RTL code at most time in such state that the virtual\n+   registers can be changed by just the corresponding hard registers\n+   (with zero offsets) and we have the right RTL code.\tTo achieve this\n+   we should add initial offset at the beginning of LRA work and update\n+   offsets after each stack expanding.\tBut actually we update virtual\n+   registers to the same virtual registers + corresponding offsets\n+   before every constraint pass because it affects constraint\n+   satisfaction (e.g. an address displacement became too big for some\n+   target).\n+\n+   The final change of eliminable registers to the corresponding hard\n+   registers are done at the very end of LRA when there were no change\n+   in offsets anymore:\n+\n+\t\t     fp + 42\t =>\tsp + 42\n+\n+*/\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"hard-reg-set.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"regs.h\"\n+#include \"insn-config.h\"\n+#include \"insn-codes.h\"\n+#include \"recog.h\"\n+#include \"output.h\"\n+#include \"addresses.h\"\n+#include \"target.h\"\n+#include \"function.h\"\n+#include \"expr.h\"\n+#include \"basic-block.h\"\n+#include \"except.h\"\n+#include \"optabs.h\"\n+#include \"df.h\"\n+#include \"ira.h\"\n+#include \"rtl-error.h\"\n+#include \"lra-int.h\"\n+\n+/* This structure is used to record information about hard register\n+   eliminations.  */\n+struct elim_table\n+{\n+  /* Hard register number to be eliminated.  */\n+  int from;\t\t\t\n+  /* Hard register number used as replacement.\t*/\n+  int to;\t\t\t\n+  /* Difference between values of the two hard registers above on\n+     previous iteration.  */\n+  HOST_WIDE_INT previous_offset;\n+  /* Difference between the values on the current iteration.  */\n+  HOST_WIDE_INT offset;\t\t\n+  /* Nonzero if this elimination can be done.  */\n+  bool can_eliminate;\t\t\n+  /* CAN_ELIMINATE since the last check.  */\n+  bool prev_can_eliminate;\n+  /* REG rtx for the register to be eliminated.\t We cannot simply\n+     compare the number since we might then spuriously replace a hard\n+     register corresponding to a pseudo assigned to the reg to be\n+     eliminated.  */\n+  rtx from_rtx;\t\t\t\n+  /* REG rtx for the replacement.  */\n+  rtx to_rtx;\t\t\t\n+};\n+\n+/* The elimination table.  Each array entry describes one possible way\n+   of eliminating a register in favor of another.  If there is more\n+   than one way of eliminating a particular register, the most\n+   preferred should be specified first.\t */\n+static struct elim_table *reg_eliminate = 0;\n+\n+/* This is an intermediate structure to initialize the table.  It has\n+   exactly the members provided by ELIMINABLE_REGS.  */\n+static const struct elim_table_1\n+{\n+  const int from;\n+  const int to;\n+} reg_eliminate_1[] =\n+\n+/* If a set of eliminable hard registers was specified, define the\n+   table from it.  Otherwise, default to the normal case of the frame\n+   pointer being replaced by the stack pointer.\t */\n+\n+#ifdef ELIMINABLE_REGS\n+  ELIMINABLE_REGS;\n+#else\n+  {{ FRAME_POINTER_REGNUM, STACK_POINTER_REGNUM}};\n+#endif\n+\n+#define NUM_ELIMINABLE_REGS ARRAY_SIZE (reg_eliminate_1)\n+\n+/* Print info about elimination table to file F.  */\n+static void\n+print_elim_table (FILE *f)\n+{\n+  struct elim_table *ep;\n+\n+  for (ep = reg_eliminate; ep < &reg_eliminate[NUM_ELIMINABLE_REGS]; ep++)\n+    fprintf (f, \"%s eliminate %d to %d (offset=\" HOST_WIDE_INT_PRINT_DEC\n+\t     \", prev_offset=\" HOST_WIDE_INT_PRINT_DEC \")\\n\",\n+\t     ep->can_eliminate ? \"Can\" : \"Can't\",\n+\t     ep->from, ep->to, ep->offset, ep->previous_offset);\n+}\n+\n+/* Print info about elimination table to stderr.  */\n+void\n+lra_debug_elim_table (void)\n+{\n+  print_elim_table (stderr);\n+}\n+\n+/* Setup possibility of elimination in elimination table element EP to\n+   VALUE.  Setup FRAME_POINTER_NEEDED if elimination from frame\n+   pointer to stack pointer is not possible anymore.  */\n+static void\n+setup_can_eliminate (struct elim_table *ep, bool value)\n+{\n+  ep->can_eliminate = ep->prev_can_eliminate = value;\n+  if (! value\n+      && ep->from == FRAME_POINTER_REGNUM && ep->to == STACK_POINTER_REGNUM)\n+    frame_pointer_needed = 1;\n+}\n+\n+/* Map: eliminable \"from\" register -> its current elimination,\n+   or NULL if none.  The elimination table may contain more than\n+   one elimination for the same hard register, but this map specifies\n+   the one that we are currently using.  */\n+static struct elim_table *elimination_map[FIRST_PSEUDO_REGISTER];\n+\n+/* When an eliminable hard register becomes not eliminable, we use the\n+   following special structure to restore original offsets for the\n+   register.  */\n+static struct elim_table self_elim_table;\n+\n+/* Offsets should be used to restore original offsets for eliminable\n+   hard register which just became not eliminable.  Zero,\n+   otherwise.  */\n+static HOST_WIDE_INT self_elim_offsets[FIRST_PSEUDO_REGISTER];\n+\n+/* Map: hard regno -> RTL presentation.\t RTL presentations of all\n+   potentially eliminable hard registers are stored in the map.\t */\n+static rtx eliminable_reg_rtx[FIRST_PSEUDO_REGISTER];\n+\n+/* Set up ELIMINATION_MAP of the currently used eliminations.  */\n+static void\n+setup_elimination_map (void)\n+{\n+  int i;\n+  struct elim_table *ep;\n+\n+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+    elimination_map[i] = NULL;\n+  for (ep = reg_eliminate; ep < &reg_eliminate[NUM_ELIMINABLE_REGS]; ep++)\n+    if (ep->can_eliminate && elimination_map[ep->from] == NULL)\n+      elimination_map[ep->from] = ep;\n+}\n+\n+\f\n+\n+/* Compute the sum of X and Y, making canonicalizations assumed in an\n+   address, namely: sum constant integers, surround the sum of two\n+   constants with a CONST, put the constant as the second operand, and\n+   group the constant on the outermost sum.\n+\n+   This routine assumes both inputs are already in canonical form.  */\n+static rtx\n+form_sum (rtx x, rtx y)\n+{\n+  rtx tem;\n+  enum machine_mode mode = GET_MODE (x);\n+\n+  if (mode == VOIDmode)\n+    mode = GET_MODE (y);\n+\n+  if (mode == VOIDmode)\n+    mode = Pmode;\n+\n+  if (CONST_INT_P (x))\n+    return plus_constant (mode, y, INTVAL (x));\n+  else if (CONST_INT_P (y))\n+    return plus_constant (mode, x, INTVAL (y));\n+  else if (CONSTANT_P (x))\n+    tem = x, x = y, y = tem;\n+\n+  if (GET_CODE (x) == PLUS && CONSTANT_P (XEXP (x, 1)))\n+    return form_sum (XEXP (x, 0), form_sum (XEXP (x, 1), y));\n+\n+  /* Note that if the operands of Y are specified in the opposite\n+     order in the recursive calls below, infinite recursion will\n+     occur.  */\n+  if (GET_CODE (y) == PLUS && CONSTANT_P (XEXP (y, 1)))\n+    return form_sum (form_sum (x, XEXP (y, 0)), XEXP (y, 1));\n+\n+  /* If both constant, encapsulate sum.\t Otherwise, just form sum.  A\n+     constant will have been placed second.  */\n+  if (CONSTANT_P (x) && CONSTANT_P (y))\n+    {\n+      if (GET_CODE (x) == CONST)\n+\tx = XEXP (x, 0);\n+      if (GET_CODE (y) == CONST)\n+\ty = XEXP (y, 0);\n+\n+      return gen_rtx_CONST (VOIDmode, gen_rtx_PLUS (mode, x, y));\n+    }\n+\n+  return gen_rtx_PLUS (mode, x, y);\n+}\n+\n+/* Return the current substitution hard register of the elimination of\n+   HARD_REGNO.\tIf HARD_REGNO is not eliminable, return itself.\t */\n+int\n+lra_get_elimination_hard_regno (int hard_regno)\n+{\n+  struct elim_table *ep;\n+\n+  if (hard_regno < 0 || hard_regno >= FIRST_PSEUDO_REGISTER)\n+    return hard_regno;\n+  if ((ep = elimination_map[hard_regno]) == NULL)\n+    return hard_regno;\n+  return ep->to;\n+}\n+\n+/* Return elimination which will be used for hard reg REG, NULL\n+   otherwise.  */\n+static struct elim_table *\n+get_elimination (rtx reg)\n+{\n+  int hard_regno;\n+  struct elim_table *ep;\n+  HOST_WIDE_INT offset;\n+\n+  lra_assert (REG_P (reg));\n+  if ((hard_regno = REGNO (reg)) < 0 || hard_regno >= FIRST_PSEUDO_REGISTER)\n+    return NULL;\n+  if ((ep = elimination_map[hard_regno]) != NULL)\n+    return ep->from_rtx != reg ? NULL : ep;\n+  if ((offset = self_elim_offsets[hard_regno]) == 0)\n+    return NULL;\n+  /* This is an iteration to restore offsets just after HARD_REGNO\n+     stopped to be eliminable.\t*/\n+  self_elim_table.from = self_elim_table.to = hard_regno;\n+  self_elim_table.from_rtx\n+    = self_elim_table.to_rtx\n+    = eliminable_reg_rtx[hard_regno];\n+  lra_assert (self_elim_table.from_rtx != NULL);\n+  self_elim_table.offset = offset;\n+  return &self_elim_table;\n+}\n+\n+/* Scan X and replace any eliminable registers (such as fp) with a\n+   replacement (such as sp) if SUBST_P, plus an offset.\t The offset is\n+   a change in the offset between the eliminable register and its\n+   substitution if UPDATE_P, or the full offset if FULL_P, or\n+   otherwise zero.\n+\n+   MEM_MODE is the mode of an enclosing MEM.  We need this to know how\n+   much to adjust a register for, e.g., PRE_DEC.  Also, if we are\n+   inside a MEM, we are allowed to replace a sum of a hard register\n+   and the constant zero with the hard register, which we cannot do\n+   outside a MEM.  In addition, we need to record the fact that a\n+   hard register is referenced outside a MEM.\n+\n+   Alternatively, INSN may be a note (an EXPR_LIST or INSN_LIST).\n+   That's used when we eliminate in expressions stored in notes.  */\n+rtx\n+lra_eliminate_regs_1 (rtx x, enum machine_mode mem_mode,\n+\t\t      bool subst_p, bool update_p, bool full_p)\n+{\n+  enum rtx_code code = GET_CODE (x);\n+  struct elim_table *ep;\n+  rtx new_rtx;\n+  int i, j;\n+  const char *fmt;\n+  int copied = 0;\n+\n+  if (! current_function_decl)\n+    return x;\n+\n+  switch (code)\n+    {\n+    CASE_CONST_ANY:\n+    case CONST:\n+    case SYMBOL_REF:\n+    case CODE_LABEL:\n+    case PC:\n+    case CC0:\n+    case ASM_INPUT:\n+    case ADDR_VEC:\n+    case ADDR_DIFF_VEC:\n+    case RETURN:\n+      return x;\n+\n+    case REG:\n+      /* First handle the case where we encounter a bare hard register\n+\t that is eliminable.  Replace it with a PLUS.  */\n+      if ((ep = get_elimination (x)) != NULL)\n+\t{\n+\t  rtx to = subst_p ? ep->to_rtx : ep->from_rtx;\n+\t  \n+\t  if (update_p)\n+\t    return plus_constant (Pmode, to, ep->offset - ep->previous_offset);\n+\t  else if (full_p)\n+\t    return plus_constant (Pmode, to, ep->offset);\n+\t  else\n+\t    return to;\n+\t}\n+      return x;\n+\n+    case PLUS:\n+      /* If this is the sum of an eliminable register and a constant, rework\n+\t the sum.  */\n+      if (REG_P (XEXP (x, 0)) && CONSTANT_P (XEXP (x, 1)))\n+\t{\n+\t  if ((ep = get_elimination (XEXP (x, 0))) != NULL)\n+\t    {\n+\t      HOST_WIDE_INT offset;\n+\t      rtx to = subst_p ? ep->to_rtx : ep->from_rtx;\n+\t      \n+\t      if (! update_p && ! full_p)\n+\t\treturn gen_rtx_PLUS (Pmode, to, XEXP (x, 1));\n+\t      \n+\t      offset = (update_p\n+\t\t\t? ep->offset - ep->previous_offset : ep->offset);\n+\t      if (CONST_INT_P (XEXP (x, 1))\n+\t\t  && INTVAL (XEXP (x, 1)) == -offset)\n+\t\treturn to;\n+\t      else\n+\t\treturn gen_rtx_PLUS (Pmode, to,\n+\t\t\t\t     plus_constant (Pmode,\n+\t\t\t\t\t\t    XEXP (x, 1), offset));\n+\t    }\n+\n+\t  /* If the hard register is not eliminable, we are done since\n+\t     the other operand is a constant.  */\n+\t  return x;\n+\t}\n+\n+      /* If this is part of an address, we want to bring any constant\n+\t to the outermost PLUS.  We will do this by doing hard\n+\t register replacement in our operands and seeing if a constant\n+\t shows up in one of them.\n+\n+\t Note that there is no risk of modifying the structure of the\n+\t insn, since we only get called for its operands, thus we are\n+\t either modifying the address inside a MEM, or something like\n+\t an address operand of a load-address insn.  */\n+\n+      {\n+\trtx new0 = lra_eliminate_regs_1 (XEXP (x, 0), mem_mode,\n+\t\t\t\t\t subst_p, update_p, full_p);\n+\trtx new1 = lra_eliminate_regs_1 (XEXP (x, 1), mem_mode,\n+\t\t\t\t\t subst_p, update_p, full_p);\n+\n+\tif (new0 != XEXP (x, 0) || new1 != XEXP (x, 1))\n+\t  return form_sum (new0, new1);\n+      }\n+      return x;\n+\n+    case MULT:\n+      /* If this is the product of an eliminable hard register and a\n+\t constant, apply the distribute law and move the constant out\n+\t so that we have (plus (mult ..) ..).  This is needed in order\n+\t to keep load-address insns valid.  This case is pathological.\n+\t We ignore the possibility of overflow here.  */\n+      if (REG_P (XEXP (x, 0)) && CONST_INT_P (XEXP (x, 1))\n+\t  && (ep = get_elimination (XEXP (x, 0))) != NULL)\n+\t{\n+\t  rtx to = subst_p ? ep->to_rtx : ep->from_rtx;\n+\t  \n+\t  if (update_p)\n+\t    return\n+\t      plus_constant (Pmode,\n+\t\t\t     gen_rtx_MULT (Pmode, to, XEXP (x, 1)),\n+\t\t\t     (ep->offset - ep->previous_offset)\n+\t\t\t     * INTVAL (XEXP (x, 1)));\n+\t  else if (full_p)\n+\t    return\n+\t      plus_constant (Pmode,\n+\t\t\t     gen_rtx_MULT (Pmode, to, XEXP (x, 1)),\n+\t\t\t     ep->offset * INTVAL (XEXP (x, 1)));\n+\t  else\n+\t    return gen_rtx_MULT (Pmode, to, XEXP (x, 1));\n+\t}\n+      \n+      /* ... fall through ...  */\n+\n+    case CALL:\n+    case COMPARE:\n+    /* See comments before PLUS about handling MINUS.  */\n+    case MINUS:\n+    case DIV:\t   case UDIV:\n+    case MOD:\t   case UMOD:\n+    case AND:\t   case IOR:\t  case XOR:\n+    case ROTATERT: case ROTATE:\n+    case ASHIFTRT: case LSHIFTRT: case ASHIFT:\n+    case NE:\t   case EQ:\n+    case GE:\t   case GT:\t  case GEU:    case GTU:\n+    case LE:\t   case LT:\t  case LEU:    case LTU:\n+      {\n+\trtx new0 = lra_eliminate_regs_1 (XEXP (x, 0), mem_mode,\n+\t\t\t\t\t subst_p, update_p, full_p);\n+\trtx new1 = XEXP (x, 1)\n+\t\t   ? lra_eliminate_regs_1 (XEXP (x, 1), mem_mode,\n+\t\t\t\t\t   subst_p, update_p, full_p) : 0;\n+\n+\tif (new0 != XEXP (x, 0) || new1 != XEXP (x, 1))\n+\t  return gen_rtx_fmt_ee (code, GET_MODE (x), new0, new1);\n+      }\n+      return x;\n+\n+    case EXPR_LIST:\n+      /* If we have something in XEXP (x, 0), the usual case,\n+\t eliminate it.\t*/\n+      if (XEXP (x, 0))\n+\t{\n+\t  new_rtx = lra_eliminate_regs_1 (XEXP (x, 0), mem_mode,\n+\t\t\t\t\t  subst_p, update_p, full_p);\n+\t  if (new_rtx != XEXP (x, 0))\n+\t    {\n+\t      /* If this is a REG_DEAD note, it is not valid anymore.\n+\t\t Using the eliminated version could result in creating a\n+\t\t REG_DEAD note for the stack or frame pointer.\t*/\n+\t      if (REG_NOTE_KIND (x) == REG_DEAD)\n+\t\treturn (XEXP (x, 1)\n+\t\t\t? lra_eliminate_regs_1 (XEXP (x, 1), mem_mode,\n+\t\t\t\t\t\tsubst_p, update_p, full_p)\n+\t\t\t: NULL_RTX);\n+\n+\t      x = alloc_reg_note (REG_NOTE_KIND (x), new_rtx, XEXP (x, 1));\n+\t    }\n+\t}\n+\n+      /* ... fall through ...  */\n+\n+    case INSN_LIST:\n+      /* Now do eliminations in the rest of the chain.\tIf this was\n+\t an EXPR_LIST, this might result in allocating more memory than is\n+\t strictly needed, but it simplifies the code.  */\n+      if (XEXP (x, 1))\n+\t{\n+\t  new_rtx = lra_eliminate_regs_1 (XEXP (x, 1), mem_mode,\n+\t\t\t\t\t  subst_p, update_p, full_p);\n+\t  if (new_rtx != XEXP (x, 1))\n+\t    return\n+\t      gen_rtx_fmt_ee (GET_CODE (x), GET_MODE (x),\n+\t\t\t      XEXP (x, 0), new_rtx);\n+\t}\n+      return x;\n+\n+    case PRE_INC:\n+    case POST_INC:\n+    case PRE_DEC:\n+    case POST_DEC:\n+      /* We do not support elimination of a register that is modified.\n+\t elimination_effects has already make sure that this does not\n+\t happen.  */\n+      return x;\n+\n+    case PRE_MODIFY:\n+    case POST_MODIFY:\n+      /* We do not support elimination of a hard register that is\n+\t modified.  LRA has already make sure that this does not\n+\t happen. The only remaining case we need to consider here is\n+\t that the increment value may be an eliminable register.  */\n+      if (GET_CODE (XEXP (x, 1)) == PLUS\n+\t  && XEXP (XEXP (x, 1), 0) == XEXP (x, 0))\n+\t{\n+\t  rtx new_rtx = lra_eliminate_regs_1 (XEXP (XEXP (x, 1), 1), mem_mode,\n+\t\t\t\t\t      subst_p, update_p, full_p);\n+\n+\t  if (new_rtx != XEXP (XEXP (x, 1), 1))\n+\t    return gen_rtx_fmt_ee (code, GET_MODE (x), XEXP (x, 0),\n+\t\t\t\t   gen_rtx_PLUS (GET_MODE (x),\n+\t\t\t\t\t\t XEXP (x, 0), new_rtx));\n+\t}\n+      return x;\n+\n+    case STRICT_LOW_PART:\n+    case NEG:\t       case NOT:\n+    case SIGN_EXTEND:  case ZERO_EXTEND:\n+    case TRUNCATE:     case FLOAT_EXTEND: case FLOAT_TRUNCATE:\n+    case FLOAT:\t       case FIX:\n+    case UNSIGNED_FIX: case UNSIGNED_FLOAT:\n+    case ABS:\n+    case SQRT:\n+    case FFS:\n+    case CLZ:\n+    case CTZ:\n+    case POPCOUNT:\n+    case PARITY:\n+    case BSWAP:\n+      new_rtx = lra_eliminate_regs_1 (XEXP (x, 0), mem_mode,\n+\t\t\t\t      subst_p, update_p, full_p);\n+      if (new_rtx != XEXP (x, 0))\n+\treturn gen_rtx_fmt_e (code, GET_MODE (x), new_rtx);\n+      return x;\n+\n+    case SUBREG:\n+      new_rtx = lra_eliminate_regs_1 (SUBREG_REG (x), mem_mode,\n+\t\t\t\t      subst_p, update_p, full_p);\n+\n+      if (new_rtx != SUBREG_REG (x))\n+\t{\n+\t  int x_size = GET_MODE_SIZE (GET_MODE (x));\n+\t  int new_size = GET_MODE_SIZE (GET_MODE (new_rtx));\n+\n+\t  if (MEM_P (new_rtx) && x_size <= new_size)\n+\t    {\n+\t      SUBREG_REG (x) = new_rtx;\n+\t      alter_subreg (&x, false);\n+\t      return x;\n+\t    }\n+\t  else\n+\t    return gen_rtx_SUBREG (GET_MODE (x), new_rtx, SUBREG_BYTE (x));\n+\t}\n+\n+      return x;\n+\n+    case MEM:\n+      /* Our only special processing is to pass the mode of the MEM to our\n+\t recursive call and copy the flags.  While we are here, handle this\n+\t case more efficiently.\t */\n+      return\n+\treplace_equiv_address_nv\n+\t(x,\n+\t lra_eliminate_regs_1 (XEXP (x, 0), GET_MODE (x),\n+\t\t\t       subst_p, update_p, full_p));\n+\n+    case USE:\n+      /* Handle insn_list USE that a call to a pure function may generate.  */\n+      new_rtx = lra_eliminate_regs_1 (XEXP (x, 0), VOIDmode,\n+\t\t\t\t      subst_p, update_p, full_p);\n+      if (new_rtx != XEXP (x, 0))\n+\treturn gen_rtx_USE (GET_MODE (x), new_rtx);\n+      return x;\n+\n+    case CLOBBER:\n+    case SET:\n+      gcc_unreachable ();\n+\n+    default:\n+      break;\n+    }\n+\n+  /* Process each of our operands recursively.\tIf any have changed, make a\n+     copy of the rtx.  */\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = 0; i < GET_RTX_LENGTH (code); i++, fmt++)\n+    {\n+      if (*fmt == 'e')\n+\t{\n+\t  new_rtx = lra_eliminate_regs_1 (XEXP (x, i), mem_mode,\n+\t\t\t\t\t  subst_p, update_p, full_p);\n+\t  if (new_rtx != XEXP (x, i) && ! copied)\n+\t    {\n+\t      x = shallow_copy_rtx (x);\n+\t      copied = 1;\n+\t    }\n+\t  XEXP (x, i) = new_rtx;\n+\t}\n+      else if (*fmt == 'E')\n+\t{\n+\t  int copied_vec = 0;\n+\t  for (j = 0; j < XVECLEN (x, i); j++)\n+\t    {\n+\t      new_rtx = lra_eliminate_regs_1 (XVECEXP (x, i, j), mem_mode,\n+\t\t\t\t\t      subst_p, update_p, full_p);\n+\t      if (new_rtx != XVECEXP (x, i, j) && ! copied_vec)\n+\t\t{\n+\t\t  rtvec new_v = gen_rtvec_v (XVECLEN (x, i),\n+\t\t\t\t\t     XVEC (x, i)->elem);\n+\t\t  if (! copied)\n+\t\t    {\n+\t\t      x = shallow_copy_rtx (x);\n+\t\t      copied = 1;\n+\t\t    }\n+\t\t  XVEC (x, i) = new_v;\n+\t\t  copied_vec = 1;\n+\t\t}\n+\t      XVECEXP (x, i, j) = new_rtx;\n+\t    }\n+\t}\n+    }\n+\n+  return x;\n+}\n+\n+/* This function is used externally in subsequent passes of GCC.  It\n+   always does a full elimination of X.\t */\n+rtx\n+lra_eliminate_regs (rtx x, enum machine_mode mem_mode,\n+\t\t    rtx insn ATTRIBUTE_UNUSED)\n+{\n+  return lra_eliminate_regs_1 (x, mem_mode, true, false, true);\n+}\n+\n+/* Scan rtx X for references to elimination source or target registers\n+   in contexts that would prevent the elimination from happening.\n+   Update the table of eliminables to reflect the changed state.\n+   MEM_MODE is the mode of an enclosing MEM rtx, or VOIDmode if not\n+   within a MEM.  */\n+static void\n+mark_not_eliminable (rtx x)\n+{\n+  enum rtx_code code = GET_CODE (x);\n+  struct elim_table *ep;\n+  int i, j;\n+  const char *fmt;\n+\n+  switch (code)\n+    {\n+    case PRE_INC:\n+    case POST_INC:\n+    case PRE_DEC:\n+    case POST_DEC:\n+    case POST_MODIFY:\n+    case PRE_MODIFY:\n+      if (REG_P (XEXP (x, 0)) && REGNO (XEXP (x, 0)) < FIRST_PSEUDO_REGISTER)\n+\t/* If we modify the source of an elimination rule, disable\n+\t   it. Do the same if it is the source and not the hard frame\n+\t   register.  */\n+\tfor (ep = reg_eliminate;\n+\t     ep < &reg_eliminate[NUM_ELIMINABLE_REGS];\n+\t       ep++)\n+\t  if (ep->from_rtx == XEXP (x, 0)\n+\t      || (ep->to_rtx == XEXP (x, 0)\n+\t\t  && ep->to_rtx != hard_frame_pointer_rtx))\n+\t    setup_can_eliminate (ep, false);\n+      return;\n+\n+    case USE:\n+      if (REG_P (XEXP (x, 0)) && REGNO (XEXP (x, 0)) < FIRST_PSEUDO_REGISTER)\n+\t/* If using a hard register that is the source of an eliminate\n+\t   we still think can be performed, note it cannot be\n+\t   performed since we don't know how this hard register is\n+\t   used.  */\n+\tfor (ep = reg_eliminate;\n+\t     ep < &reg_eliminate[NUM_ELIMINABLE_REGS];\n+\t     ep++)\n+\t  if (ep->from_rtx == XEXP (x, 0)\n+\t      && ep->to_rtx != hard_frame_pointer_rtx)\n+\t    setup_can_eliminate (ep, false);\n+      return;\n+\n+    case CLOBBER:\n+      if (REG_P (XEXP (x, 0)) && REGNO (XEXP (x, 0)) < FIRST_PSEUDO_REGISTER)\n+\t/* If clobbering a hard register that is the replacement\n+\t   register for an elimination we still think can be\n+\t   performed, note that it cannot be performed.\t Otherwise, we\n+\t   need not be concerned about it.  */\n+\tfor (ep = reg_eliminate;\n+\t     ep < &reg_eliminate[NUM_ELIMINABLE_REGS];\n+\t     ep++)\n+\t  if (ep->to_rtx == XEXP (x, 0)\n+\t      && ep->to_rtx != hard_frame_pointer_rtx)\n+\t    setup_can_eliminate (ep, false);\n+      return;\n+\n+    case SET:\n+      /* Check for setting a hard register that we know about.\t*/\n+      if (REG_P (SET_DEST (x)) && REGNO (SET_DEST (x)) < FIRST_PSEUDO_REGISTER)\n+\t{\n+\t  /* See if this is setting the replacement hard register for\n+\t     an elimination.\n+\n+\t     If DEST is the hard frame pointer, we do nothing because\n+\t     we assume that all assignments to the frame pointer are\n+\t     for non-local gotos and are being done at a time when\n+\t     they are valid and do not disturb anything else.  Some\n+\t     machines want to eliminate a fake argument pointer (or\n+\t     even a fake frame pointer) with either the real frame\n+\t     pointer or the stack pointer.  Assignments to the hard\n+\t     frame pointer must not prevent this elimination.  */\n+\n+\t  for (ep = reg_eliminate;\n+\t       ep < &reg_eliminate[NUM_ELIMINABLE_REGS];\n+\t       ep++)\n+\t    if (ep->to_rtx == SET_DEST (x)\n+\t\t&& SET_DEST (x) != hard_frame_pointer_rtx)\n+\t      setup_can_eliminate (ep, false);\n+\t}\n+\n+      mark_not_eliminable (SET_DEST (x));\n+      mark_not_eliminable (SET_SRC (x));\n+      return;\n+\n+    default:\n+      break;\n+    }\n+\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = 0; i < GET_RTX_LENGTH (code); i++, fmt++)\n+    {\n+      if (*fmt == 'e')\n+\tmark_not_eliminable (XEXP (x, i));\n+      else if (*fmt == 'E')\n+\tfor (j = 0; j < XVECLEN (x, i); j++)\n+\t  mark_not_eliminable (XVECEXP (x, i, j));\n+    }\n+}\n+\n+\f\n+\n+/* Scan INSN and eliminate all eliminable hard registers in it.\n+\n+   If REPLACE_P is true, do the replacement destructively.  Also\n+   delete the insn as dead it if it is setting an eliminable register.\n+\n+   If REPLACE_P is false, just update the offsets while keeping the\n+   base register the same.  */\n+\n+static void\n+eliminate_regs_in_insn (rtx insn, bool replace_p)\n+{\n+  int icode = recog_memoized (insn);\n+  rtx old_set = single_set (insn);\n+  bool validate_p;\n+  int i;\n+  rtx substed_operand[MAX_RECOG_OPERANDS];\n+  rtx orig_operand[MAX_RECOG_OPERANDS];\n+  struct elim_table *ep;\n+  rtx plus_src, plus_cst_src;\n+  lra_insn_recog_data_t id;\n+  struct lra_static_insn_data *static_id;\n+\n+  if (icode < 0 && asm_noperands (PATTERN (insn)) < 0 && ! DEBUG_INSN_P (insn))\n+    {\n+      lra_assert (GET_CODE (PATTERN (insn)) == USE\n+\t\t  || GET_CODE (PATTERN (insn)) == CLOBBER\n+\t\t  || GET_CODE (PATTERN (insn)) == ADDR_VEC\n+\t\t  || GET_CODE (PATTERN (insn)) == ADDR_DIFF_VEC\n+\t\t  || GET_CODE (PATTERN (insn)) == ASM_INPUT);\n+      return;\n+    }\n+\n+  /* Check for setting an eliminable register.\t*/\n+  if (old_set != 0 && REG_P (SET_DEST (old_set))\n+      && (ep = get_elimination (SET_DEST (old_set))) != NULL)\n+    {\n+      bool delete_p = replace_p;\n+      \n+#ifdef HARD_FRAME_POINTER_REGNUM\n+      /* If this is setting the frame pointer register to the hardware\n+\t frame pointer register and this is an elimination that will\n+\t be done (tested above), this insn is really adjusting the\n+\t frame pointer downward to compensate for the adjustment done\n+\t before a nonlocal goto.  */\n+      if (ep->from == FRAME_POINTER_REGNUM\n+\t  && ep->to == HARD_FRAME_POINTER_REGNUM)\n+\t{\n+\t  if (replace_p)\n+\t    {\n+\t      SET_DEST (old_set) = ep->to_rtx;\n+\t      lra_update_insn_recog_data (insn);\n+\t      return;\n+\t    }\n+\t  else\n+\t    {\n+\t      rtx base = SET_SRC (old_set);\n+\t      HOST_WIDE_INT offset = 0;\n+\t      rtx base_insn = insn;\n+\t      \n+\t      while (base != ep->to_rtx)\n+\t\t{\n+\t\t  rtx prev_insn, prev_set;\n+\t\t  \n+\t\t  if (GET_CODE (base) == PLUS && CONST_INT_P (XEXP (base, 1)))\n+\t\t    {\n+\t\t      offset += INTVAL (XEXP (base, 1));\n+\t\t      base = XEXP (base, 0);\n+\t\t    }\n+\t\t  else if ((prev_insn = prev_nonnote_insn (base_insn)) != 0\n+\t\t\t   && (prev_set = single_set (prev_insn)) != 0\n+\t\t\t   && rtx_equal_p (SET_DEST (prev_set), base))\n+\t\t    {\n+\t\t      base = SET_SRC (prev_set);\n+\t\t      base_insn = prev_insn;\n+\t\t    }\n+\t\t  else\n+\t\t    break;\n+\t\t}\n+\t      \n+\t      if (base == ep->to_rtx)\n+\t\t{\n+\t\t  rtx src;\n+\t\t  \n+\t\t  offset -= (ep->offset - ep->previous_offset);\n+\t\t  src = plus_constant (Pmode, ep->to_rtx, offset);\n+\t\t  \n+\t\t  /* First see if this insn remains valid when we make\n+\t\t     the change.  If not, keep the INSN_CODE the same\n+\t\t     and let the constraint pass fit it up.  */\n+\t\t  validate_change (insn, &SET_SRC (old_set), src, 1);\n+\t\t  validate_change (insn, &SET_DEST (old_set),\n+\t\t\t\t   ep->from_rtx, 1);\n+\t\t  if (! apply_change_group ())\n+\t\t    {\n+\t\t      SET_SRC (old_set) = src;\n+\t\t      SET_DEST (old_set) = ep->from_rtx;\n+\t\t    }\n+\t\t  lra_update_insn_recog_data (insn);\n+\t\t  return;\n+\t\t}\n+\t    }\n+\t  \n+\t  \n+\t  /* We can't delete this insn, but needn't process it\n+\t     since it won't be used unless something changes.  */\n+\t  delete_p = false;\n+\t}\n+#endif\n+      \n+      /* This insn isn't serving a useful purpose.  We delete it\n+\t when REPLACE is set.  */\n+      if (delete_p)\n+\tlra_delete_dead_insn (insn);\n+      return;\n+    }\n+\n+  /* We allow one special case which happens to work on all machines we\n+     currently support: a single set with the source or a REG_EQUAL\n+     note being a PLUS of an eliminable register and a constant.  */\n+  plus_src = plus_cst_src = 0;\n+  if (old_set && REG_P (SET_DEST (old_set)))\n+    {\n+      if (GET_CODE (SET_SRC (old_set)) == PLUS)\n+\tplus_src = SET_SRC (old_set);\n+      /* First see if the source is of the form (plus (...) CST).  */\n+      if (plus_src\n+\t  && CONST_INT_P (XEXP (plus_src, 1)))\n+\tplus_cst_src = plus_src;\n+      /* Check that the first operand of the PLUS is a hard reg or\n+\t the lowpart subreg of one.  */\n+      if (plus_cst_src)\n+\t{\n+\t  rtx reg = XEXP (plus_cst_src, 0);\n+\n+\t  if (GET_CODE (reg) == SUBREG && subreg_lowpart_p (reg))\n+\t    reg = SUBREG_REG (reg);\n+\n+\t  if (!REG_P (reg) || REGNO (reg) >= FIRST_PSEUDO_REGISTER)\n+\t    plus_cst_src = 0;\n+\t}\n+    }\n+  if (plus_cst_src)\n+    {\n+      rtx reg = XEXP (plus_cst_src, 0);\n+      HOST_WIDE_INT offset = INTVAL (XEXP (plus_cst_src, 1));\n+\n+      if (GET_CODE (reg) == SUBREG)\n+\treg = SUBREG_REG (reg);\n+\n+      if (REG_P (reg) && (ep = get_elimination (reg)) != NULL)\n+\t{\n+\t  rtx to_rtx = replace_p ? ep->to_rtx : ep->from_rtx;\n+\t  \n+\t  if (! replace_p)\n+\t    {\n+\t      offset += (ep->offset - ep->previous_offset);\n+\t      offset = trunc_int_for_mode (offset, GET_MODE (plus_cst_src));\n+\t    }\n+\t  \n+\t  if (GET_CODE (XEXP (plus_cst_src, 0)) == SUBREG)\n+\t    to_rtx = gen_lowpart (GET_MODE (XEXP (plus_cst_src, 0)), to_rtx);\n+\t  /* If we have a nonzero offset, and the source is already a\n+\t     simple REG, the following transformation would increase\n+\t     the cost of the insn by replacing a simple REG with (plus\n+\t     (reg sp) CST).  So try only when we already had a PLUS\n+\t     before.  */\n+\t  if (offset == 0 || plus_src)\n+\t    {\n+\t      rtx new_src = plus_constant (GET_MODE (to_rtx), to_rtx, offset);\n+\t      \n+\t      old_set = single_set (insn);\n+\n+\t      /* First see if this insn remains valid when we make the\n+\t\t change.  If not, try to replace the whole pattern\n+\t\t with a simple set (this may help if the original insn\n+\t\t was a PARALLEL that was only recognized as single_set\n+\t\t due to REG_UNUSED notes).  If this isn't valid\n+\t\t either, keep the INSN_CODE the same and let the\n+\t\t constraint pass fix it up.  */\n+\t      if (! validate_change (insn, &SET_SRC (old_set), new_src, 0))\n+\t\t{\n+\t\t  rtx new_pat = gen_rtx_SET (VOIDmode,\n+\t\t\t\t\t     SET_DEST (old_set), new_src);\n+\t\t  \n+\t\t  if (! validate_change (insn, &PATTERN (insn), new_pat, 0))\n+\t\t    SET_SRC (old_set) = new_src;\n+\t\t}\n+\t      lra_update_insn_recog_data (insn);\n+\t      /* This can't have an effect on elimination offsets, so skip\n+\t\t right to the end.  */\n+\t      return;\n+\t    }\n+\t}\n+    }\n+\n+  /* Eliminate all eliminable registers occurring in operands that\n+     can be handled by the constraint pass.  */\n+  id = lra_get_insn_recog_data (insn);\n+  static_id = id->insn_static_data;\n+  validate_p = false;\n+  for (i = 0; i < static_id->n_operands; i++)\n+    {\n+      orig_operand[i] = *id->operand_loc[i];\n+      substed_operand[i] = *id->operand_loc[i];\n+\n+      /* For an asm statement, every operand is eliminable.  */\n+      if (icode < 0 || insn_data[icode].operand[i].eliminable)\n+\t{\n+\t  /* Check for setting a hard register that we know about.  */\n+\t  if (static_id->operand[i].type != OP_IN\n+\t      && REG_P (orig_operand[i]))\n+\t    {\n+\t      /* If we are assigning to a hard register that can be\n+\t\t eliminated, it must be as part of a PARALLEL, since\n+\t\t the code above handles single SETs.  This reg can not\n+\t\t be longer eliminated -- it is forced by\n+\t\t mark_not_eliminable.  */\n+\t      for (ep = reg_eliminate;\n+\t\t   ep < &reg_eliminate[NUM_ELIMINABLE_REGS];\n+\t\t   ep++)\n+\t\tlra_assert (ep->from_rtx != orig_operand[i]\n+\t\t\t    || ! ep->can_eliminate);\n+\t    }\n+\n+\t  /* Companion to the above plus substitution, we can allow\n+\t     invariants as the source of a plain move.\t*/\n+\t  substed_operand[i]\n+\t    = lra_eliminate_regs_1 (*id->operand_loc[i], VOIDmode,\n+\t\t\t\t    replace_p, ! replace_p, false);\n+\t  if (substed_operand[i] != orig_operand[i])\n+\t    validate_p = true;\n+\t}\n+    }\n+\n+  /* Substitute the operands; the new values are in the substed_operand\n+     array.  */\n+  for (i = 0; i < static_id->n_operands; i++)\n+    *id->operand_loc[i] = substed_operand[i];\n+  for (i = 0; i < static_id->n_dups; i++)\n+    *id->dup_loc[i] = substed_operand[(int) static_id->dup_num[i]];\n+\n+  if (validate_p)\n+    {\n+      /* If we had a move insn but now we don't, re-recognize it.\n+\t This will cause spurious re-recognition if the old move had a\n+\t PARALLEL since the new one still will, but we can't call\n+\t single_set without having put new body into the insn and the\n+\t re-recognition won't hurt in this rare case.  */\n+      id = lra_update_insn_recog_data (insn);\n+      static_id = id->insn_static_data;\n+    }\n+}\n+\n+/* Spill pseudos which are assigned to hard registers in SET.  Add\n+   affected insns for processing in the subsequent constraint\n+   pass.  */\n+static void\n+spill_pseudos (HARD_REG_SET set)\n+{\n+  int i;\n+  bitmap_head to_process;\n+  rtx insn;\n+\n+  if (hard_reg_set_empty_p (set))\n+    return;\n+  if (lra_dump_file != NULL)\n+    {\n+      fprintf (lra_dump_file, \"\t   Spilling non-eliminable hard regs:\");\n+      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+\tif (TEST_HARD_REG_BIT (set, i))\n+\t  fprintf (lra_dump_file, \" %d\", i);\n+      fprintf (lra_dump_file, \"\\n\");\n+    }\n+  bitmap_initialize (&to_process, &reg_obstack);\n+  for (i = FIRST_PSEUDO_REGISTER; i < max_reg_num (); i++)\n+    if (lra_reg_info[i].nrefs != 0 && reg_renumber[i] >= 0\n+\t&& overlaps_hard_reg_set_p (set,\n+\t\t\t\t    PSEUDO_REGNO_MODE (i), reg_renumber[i]))\n+      {\n+\tif (lra_dump_file != NULL)\n+\t  fprintf (lra_dump_file, \"\t Spilling r%d(%d)\\n\",\n+\t\t   i, reg_renumber[i]);\n+\treg_renumber[i] = -1;\n+\tbitmap_ior_into (&to_process, &lra_reg_info[i].insn_bitmap);\n+      }\n+  IOR_HARD_REG_SET (lra_no_alloc_regs, set);\n+  for (insn = get_insns (); insn != NULL_RTX; insn = NEXT_INSN (insn))\n+    if (bitmap_bit_p (&to_process, INSN_UID (insn)))\n+      {\n+\tlra_push_insn (insn);\n+\tlra_set_used_insn_alternative (insn, -1);\n+      }\n+  bitmap_clear (&to_process);\n+}\n+\n+/* Update all offsets and possibility for elimination on eliminable\n+   registers.  Spill pseudos assigned to registers which became\n+   uneliminable, update LRA_NO_ALLOC_REGS and ELIMINABLE_REG_SET.  Add\n+   insns to INSNS_WITH_CHANGED_OFFSETS containing eliminable hard\n+   registers whose offsets should be changed.  */\n+static void\n+update_reg_eliminate (bitmap insns_with_changed_offsets)\n+{\n+  bool prev;\n+  struct elim_table *ep, *ep1;\n+  HARD_REG_SET temp_hard_reg_set;\n+\n+  /* Clear self elimination offsets.  */\n+  for (ep = reg_eliminate; ep < &reg_eliminate[NUM_ELIMINABLE_REGS]; ep++)\n+    self_elim_offsets[ep->from] = 0;\n+  CLEAR_HARD_REG_SET (temp_hard_reg_set);\n+  for (ep = reg_eliminate; ep < &reg_eliminate[NUM_ELIMINABLE_REGS]; ep++)\n+    {\n+      /* If it is a currently used elimination: update the previous\n+\t offset.  */\n+      if (elimination_map[ep->from] == ep)\n+\tep->previous_offset = ep->offset;\n+\n+      prev = ep->prev_can_eliminate;\n+      setup_can_eliminate (ep, targetm.can_eliminate (ep->from, ep->to));\n+      if (ep->can_eliminate && ! prev)\n+\t{\n+\t  /* It is possible that not eliminable register becomes\n+\t     eliminable because we took other reasons into account to\n+\t     set up eliminable regs in the initial set up.  Just\n+\t     ignore new eliminable registers.  */\n+\t  setup_can_eliminate (ep, false);\n+\t  continue;\n+\t}\n+      if (ep->can_eliminate != prev && elimination_map[ep->from] == ep)\n+\t{\n+\t  /* We cannot use this elimination anymore -- find another\n+\t     one.  */\n+\t  if (lra_dump_file != NULL)\n+\t    fprintf (lra_dump_file,\n+\t\t     \"\tElimination %d to %d is not possible anymore\\n\",\n+\t\t     ep->from, ep->to);\n+\t  /* Mark that is not eliminable anymore.  */\n+\t  elimination_map[ep->from] = NULL;\n+\t  for (ep1 = ep + 1; ep1 < &reg_eliminate[NUM_ELIMINABLE_REGS]; ep1++)\n+\t    if (ep1->can_eliminate && ep1->from == ep->from)\n+\t      break;\n+\t  if (ep1 < &reg_eliminate[NUM_ELIMINABLE_REGS])\n+\t    {\n+\t      if (lra_dump_file != NULL)\n+\t\tfprintf (lra_dump_file, \"    Using elimination %d to %d now\\n\",\n+\t\t\t ep1->from, ep1->to);\n+\t      /* Prevent the hard register into which we eliminate now\n+\t\t from the usage for pseudos.  */\n+\t      SET_HARD_REG_BIT (temp_hard_reg_set, ep1->to);\n+\t      lra_assert (ep1->previous_offset == 0);\n+\t      ep1->previous_offset = ep->offset;\n+\t    }\n+\t  else\n+\t    {\n+\t      /* There is no elimination anymore just use the hard\n+\t\t register `from' itself.  Setup self elimination\n+\t\t offset to restore the original offset values.\t*/\n+\t      if (lra_dump_file != NULL)\n+\t\tfprintf (lra_dump_file, \"    %d is not eliminable at all\\n\",\n+\t\t\t ep->from);\n+\t      self_elim_offsets[ep->from] = -ep->offset;\n+\t      SET_HARD_REG_BIT (temp_hard_reg_set, ep->from);\n+\t      if (ep->offset != 0)\n+\t\tbitmap_ior_into (insns_with_changed_offsets,\n+\t\t\t\t &lra_reg_info[ep->from].insn_bitmap);\n+\t    }\n+\t}\n+\n+#ifdef ELIMINABLE_REGS\n+      INITIAL_ELIMINATION_OFFSET (ep->from, ep->to, ep->offset);\n+#else\n+      INITIAL_FRAME_POINTER_OFFSET (ep->offset);\n+#endif\n+    }\n+  IOR_HARD_REG_SET (lra_no_alloc_regs, temp_hard_reg_set);\n+  AND_COMPL_HARD_REG_SET (eliminable_regset, temp_hard_reg_set);\n+  spill_pseudos (temp_hard_reg_set);\n+  setup_elimination_map ();\n+  for (ep = reg_eliminate; ep < &reg_eliminate[NUM_ELIMINABLE_REGS]; ep++)\n+    if (elimination_map[ep->from] == ep && ep->previous_offset != ep->offset)\n+      bitmap_ior_into (insns_with_changed_offsets,\n+\t\t       &lra_reg_info[ep->from].insn_bitmap);\n+}\n+\n+/* Initialize the table of hard registers to eliminate.\n+   Pre-condition: global flag frame_pointer_needed has been set before\n+   calling this function.  */\n+static void\n+init_elim_table (void)\n+{\n+  bool value_p;\n+  struct elim_table *ep;\n+#ifdef ELIMINABLE_REGS\n+  const struct elim_table_1 *ep1;\n+#endif\n+\n+  if (!reg_eliminate)\n+    reg_eliminate = XCNEWVEC (struct elim_table, NUM_ELIMINABLE_REGS);\n+\n+  memset (self_elim_offsets, 0, sizeof (self_elim_offsets));\n+  /* Initiate member values which will be never changed.  */\n+  self_elim_table.can_eliminate = self_elim_table.prev_can_eliminate = true;\n+  self_elim_table.previous_offset = 0;\n+#ifdef ELIMINABLE_REGS\n+  for (ep = reg_eliminate, ep1 = reg_eliminate_1;\n+       ep < &reg_eliminate[NUM_ELIMINABLE_REGS]; ep++, ep1++)\n+    {\n+      ep->offset = ep->previous_offset = 0;\n+      ep->from = ep1->from;\n+      ep->to = ep1->to;\n+      value_p = (targetm.can_eliminate (ep->from, ep->to)\n+\t\t && ! (ep->to == STACK_POINTER_REGNUM\n+\t\t       && frame_pointer_needed \n+\t\t       && (! SUPPORTS_STACK_ALIGNMENT\n+\t\t\t   || ! stack_realign_fp)));\n+      setup_can_eliminate (ep, value_p);\n+    }\n+#else\n+  reg_eliminate[0].offset = reg_eliminate[0].previous_offset = 0;\n+  reg_eliminate[0].from = reg_eliminate_1[0].from;\n+  reg_eliminate[0].to = reg_eliminate_1[0].to;\n+  setup_can_eliminate (&reg_eliminate[0], ! frame_pointer_needed);\n+#endif\n+\n+  /* Count the number of eliminable registers and build the FROM and TO\n+     REG rtx's.\t Note that code in gen_rtx_REG will cause, e.g.,\n+     gen_rtx_REG (Pmode, STACK_POINTER_REGNUM) to equal stack_pointer_rtx.\n+     We depend on this.\t */\n+  for (ep = reg_eliminate; ep < &reg_eliminate[NUM_ELIMINABLE_REGS]; ep++)\n+    {\n+      ep->from_rtx = gen_rtx_REG (Pmode, ep->from);\n+      ep->to_rtx = gen_rtx_REG (Pmode, ep->to);\n+      eliminable_reg_rtx[ep->from] = ep->from_rtx;\n+    }\n+}\n+\n+/* Entry function for initialization of elimination once per\n+   function.  */\n+void\n+lra_init_elimination (void)\n+{\n+  basic_block bb;\n+  rtx insn;\n+\n+  init_elim_table ();\n+  FOR_EACH_BB (bb)\n+    FOR_BB_INSNS (bb, insn)\n+    if (NONDEBUG_INSN_P (insn))\n+      mark_not_eliminable (PATTERN (insn));\n+  setup_elimination_map ();\n+}\n+\n+/* Eliminate hard reg given by its location LOC.  */\n+void\n+lra_eliminate_reg_if_possible (rtx *loc)\n+{\n+  int regno;\n+  struct elim_table *ep;\n+\n+  lra_assert (REG_P (*loc));\n+  if ((regno = REGNO (*loc)) >= FIRST_PSEUDO_REGISTER\n+      || ! TEST_HARD_REG_BIT (lra_no_alloc_regs, regno))\n+    return;\n+  if ((ep = get_elimination (*loc)) != NULL)\n+    *loc = ep->to_rtx;\n+}\n+\n+/* Do (final if FINAL_P) elimination in INSN.  Add the insn for\n+   subsequent processing in the constraint pass, update the insn info.\t*/\n+static void\n+process_insn_for_elimination (rtx insn, bool final_p)\n+{\n+  eliminate_regs_in_insn (insn, final_p);\n+  if (! final_p)\n+    {\n+      /* Check that insn changed its code.  This is a case when a move\n+\t insn becomes an add insn and we do not want to process the\n+\t insn as a move anymore.  */\n+      int icode = recog (PATTERN (insn), insn, 0);\n+\n+      if (icode >= 0 && icode != INSN_CODE (insn))\n+\t{\n+\t  INSN_CODE (insn) = icode;\n+\t  lra_update_insn_recog_data (insn);\n+\t}\n+      lra_update_insn_regno_info (insn);\n+      lra_push_insn (insn);\n+      lra_set_used_insn_alternative (insn, -1);\n+    }\n+}\n+\n+/* Entry function to do final elimination if FINAL_P or to update\n+   elimination register offsets.  */\n+void\n+lra_eliminate (bool final_p)\n+{\n+  int i;\n+  unsigned int uid;\n+  rtx mem_loc, invariant;\n+  bitmap_head insns_with_changed_offsets;\n+  bitmap_iterator bi;\n+  struct elim_table *ep;\n+  int regs_num = max_reg_num ();\n+\n+  timevar_push (TV_LRA_ELIMINATE);\n+\n+  bitmap_initialize (&insns_with_changed_offsets, &reg_obstack);\n+  if (final_p)\n+    {\n+#ifdef ENABLE_CHECKING\n+      update_reg_eliminate (&insns_with_changed_offsets);\n+      if (! bitmap_empty_p (&insns_with_changed_offsets))\n+\tgcc_unreachable ();\n+#endif\n+      /* We change eliminable hard registers in insns so we should do\n+\t this for all insns containing any eliminable hard\n+\t register.  */\n+      for (ep = reg_eliminate; ep < &reg_eliminate[NUM_ELIMINABLE_REGS]; ep++)\n+\tif (elimination_map[ep->from] != NULL)\n+\t  bitmap_ior_into (&insns_with_changed_offsets,\n+\t\t\t   &lra_reg_info[ep->from].insn_bitmap);\n+    }\n+  else\n+    {\n+      update_reg_eliminate (&insns_with_changed_offsets);\n+      if (bitmap_empty_p (&insns_with_changed_offsets))\n+\tgoto lra_eliminate_done;\n+    }\n+  if (lra_dump_file != NULL)\n+    {\n+      fprintf (lra_dump_file, \"New elimination table:\\n\");\n+      print_elim_table (lra_dump_file);\n+    }\n+  for (i = FIRST_PSEUDO_REGISTER; i < regs_num; i++)\n+    if (lra_reg_info[i].nrefs != 0)\n+      {\n+\tmem_loc = ira_reg_equiv[i].memory;\n+\tif (mem_loc != NULL_RTX)\n+\t  mem_loc = lra_eliminate_regs_1 (mem_loc, VOIDmode,\n+\t\t\t\t\t  final_p, ! final_p, false);\n+\tira_reg_equiv[i].memory = mem_loc;\n+\tinvariant = ira_reg_equiv[i].invariant;\n+\tif (invariant != NULL_RTX)\n+\t  invariant = lra_eliminate_regs_1 (invariant, VOIDmode,\n+\t\t\t\t\t    final_p, ! final_p, false);\n+\tira_reg_equiv[i].invariant = invariant;\n+\tif (lra_dump_file != NULL\n+\t    && (mem_loc != NULL_RTX || invariant != NULL))\n+\t  fprintf (lra_dump_file,\n+\t\t   \"Updating elimination of equiv for reg %d\\n\", i);\n+      }\n+  EXECUTE_IF_SET_IN_BITMAP (&insns_with_changed_offsets, 0, uid, bi)\n+    process_insn_for_elimination (lra_insn_recog_data[uid]->insn, final_p);\n+  bitmap_clear (&insns_with_changed_offsets);\n+\n+lra_eliminate_done:\n+  timevar_pop (TV_LRA_ELIMINATE);\n+}"}, {"sha": "d00cc12feff192fc7c47f5e594ee749c64787245", "filename": "gcc/lra-int.h", "status": "added", "additions": 438, "deletions": 0, "changes": 438, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-int.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-int.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flra-int.h?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -0,0 +1,438 @@\n+/* Local Register Allocator (LRA) intercommunication header file.\n+   Copyright (C) 2010, 2011, 2012\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.\tIf not see\n+<http://www.gnu.org/licenses/>.\t */\n+\n+#include \"lra.h\"\n+#include \"bitmap.h\"\n+#include \"recog.h\"\n+#include \"insn-attr.h\"\n+#include \"insn-codes.h\"\n+\n+#ifdef ENABLE_CHECKING\n+#define lra_assert(c) gcc_assert (c)\n+#else\n+/* Always define and include C, so that warnings for empty body in an\n+  \u2018if\u2019 statement and unused variable do not occur.  */\n+#define lra_assert(c) ((void)(0 && (c)))\n+#endif\n+\n+/* The parameter used to prevent infinite reloading for an insn.  Each\n+   insn operands might require a reload and, if it is a memory, its\n+   base and index registers might require a reload too.\t */\n+#define LRA_MAX_INSN_RELOADS (MAX_RECOG_OPERANDS * 3)\n+\n+/* Return the hard register which given pseudo REGNO assigned to.\n+   Negative value means that the register got memory or we don't know\n+   allocation yet.  */\n+static inline int\n+lra_get_regno_hard_regno (int regno)\n+{\n+  resize_reg_info ();\n+  return reg_renumber[regno];\n+}\n+\n+typedef struct lra_live_range *lra_live_range_t;\n+\n+/* The structure describes program points where a given pseudo lives.\n+   The live ranges can be used to find conflicts with other pseudos.\n+   If the live ranges of two pseudos are intersected, the pseudos are\n+   in conflict.\t */\n+struct lra_live_range\n+{\n+  /* Pseudo regno whose live range is described by given\n+     structure.\t */\n+  int regno;\n+  /* Program point range.  */\n+  int start, finish;\n+  /* Next structure describing program points where the pseudo\n+     lives.  */\n+  lra_live_range_t next;\n+  /* Pointer to structures with the same start.\t */\n+  lra_live_range_t start_next;\n+};\n+\n+typedef struct lra_copy *lra_copy_t;\n+\n+/* Copy between pseudos which affects assigning hard registers.\t */\n+struct lra_copy\n+{\n+  /* True if regno1 is the destination of the copy.  */\n+  bool regno1_dest_p;\n+  /* Execution frequency of the copy.  */\n+  int freq;\n+  /* Pseudos connected by the copy.  REGNO1 < REGNO2.  */\n+  int regno1, regno2;\n+  /* Next copy with correspondingly REGNO1 and REGNO2.\t*/\n+  lra_copy_t regno1_next, regno2_next;\n+};\n+\n+/* Common info about a register (pseudo or hard register).  */\n+struct lra_reg\n+{\n+  /* Bitmap of UIDs of insns (including debug insns) referring the\n+     reg.  */\n+  bitmap_head insn_bitmap;\n+  /* The following fields are defined only for pseudos.\t */\n+  /* Hard registers with which the pseudo conflicts.  */\n+  HARD_REG_SET conflict_hard_regs;\n+  /* We assign hard registers to reload pseudos which can occur in few\n+     places.  So two hard register preferences are enough for them.\n+     The following fields define the preferred hard registers.\tIf\n+     there are no such hard registers the first field value is\n+     negative.\tIf there is only one preferred hard register, the 2nd\n+     field is negative.\t */\n+  int preferred_hard_regno1, preferred_hard_regno2;\n+  /* Profits to use the corresponding preferred hard registers.\t If\n+     the both hard registers defined, the first hard register has not\n+     less profit than the second one.  */\n+  int preferred_hard_regno_profit1, preferred_hard_regno_profit2;\n+#ifdef STACK_REGS\n+  /* True if the pseudo should not be assigned to a stack register.  */\n+  bool no_stack_p;\n+#endif\n+#ifdef ENABLE_CHECKING\n+  /* True if the pseudo crosses a call.\t It is setup in lra-lives.c\n+     and used to check that the pseudo crossing a call did not get a\n+     call used hard register.  */\n+  bool call_p;\n+#endif\n+  /* Number of references and execution frequencies of the register in\n+     *non-debug* insns.\t */\n+  int nrefs, freq;\n+  int last_reload;\n+  /* Regno used to undo the inheritance.  It can be non-zero only\n+     between couple of inheritance and undo inheritance passes.\t */\n+  int restore_regno;\n+  /* Value holding by register.\t If the pseudos have the same value\n+     they do not conflict.  */\n+  int val;\n+  /* These members are set up in lra-lives.c and updated in\n+     lra-coalesce.c.  */\n+  /* The biggest size mode in which each pseudo reg is referred in\n+     whole function (possibly via subreg).  */\n+  enum machine_mode biggest_mode;\n+  /* Live ranges of the pseudo.\t */\n+  lra_live_range_t live_ranges;\n+  /* This member is set up in lra-lives.c for subsequent\n+     assignments.  */\n+  lra_copy_t copies;\n+};\n+\n+/* References to the common info about each register.  */\n+extern struct lra_reg *lra_reg_info;\n+\n+/* Static info about each insn operand (common for all insns with the\n+   same ICODE).\t Warning: if the structure definition is changed, the\n+   initializer for debug_operand_data in lra.c should be changed\n+   too.\t */\n+struct lra_operand_data\n+{\n+  /* The machine description constraint string of the operand.\t*/\n+  const char *constraint;\n+  /* It is taken only from machine description (which is different\n+     from recog_data.operand_mode) and can be of VOIDmode.  */\n+  ENUM_BITFIELD(machine_mode) mode : 16;\n+  /* The type of the operand (in/out/inout).  */\n+  ENUM_BITFIELD (op_type) type : 8;\n+  /* Through if accessed through STRICT_LOW.  */\n+  unsigned int strict_low : 1;\n+  /* True if the operand is an operator.  */\n+  unsigned int is_operator : 1;\n+  /* True if there is an early clobber alternative for this operand.\n+     This field is set up every time when corresponding\n+     operand_alternative in lra_static_insn_data is set up.  */\n+  unsigned int early_clobber : 1;\n+  /* True if the operand is an address.  */\n+  unsigned int is_address : 1;\n+};\n+\n+/* Info about register occurrence in an insn.  */\n+struct lra_insn_reg\n+{\n+  /* The biggest mode through which the insn refers to the register\n+     occurrence (remember the register can be accessed through a\n+     subreg in the insn).  */\n+  ENUM_BITFIELD(machine_mode) biggest_mode : 16;\n+  /* The type of the corresponding operand which is the register.  */\n+  ENUM_BITFIELD (op_type) type : 8;\n+  /* True if the reg is accessed through a subreg and the subreg is\n+     just a part of the register.  */\n+  unsigned int subreg_p : 1;\n+  /* True if there is an early clobber alternative for this\n+     operand.  */\n+  unsigned int early_clobber : 1;\n+  /* The corresponding regno of the register.  */\n+  int regno;\n+  /* Next reg info of the same insn.  */\n+  struct lra_insn_reg *next;\n+};\n+\n+/* Static part (common info for insns with the same ICODE) of LRA\n+   internal insn info. It exists in at most one exemplar for each\n+   non-negative ICODE. There is only one exception. Each asm insn has\n+   own structure.  Warning: if the structure definition is changed,\n+   the initializer for debug_insn_static_data in lra.c should be\n+   changed too.  */\n+struct lra_static_insn_data\n+{\n+  /* Static info about each insn operand.  */\n+  struct lra_operand_data *operand;\n+  /* Each duplication refers to the number of the corresponding\n+     operand which is duplicated.  */\n+  int *dup_num;\n+  /* The number of an operand marked as commutative, -1 otherwise.  */\n+  int commutative;\n+  /* Number of operands, duplications, and alternatives of the\n+     insn.  */\n+  char n_operands;\n+  char n_dups;\n+  char n_alternatives;\n+  /* Insns in machine description (or clobbers in asm) may contain\n+     explicit hard regs which are not operands.\t The following list\n+     describes such hard registers.  */\n+  struct lra_insn_reg *hard_regs;\n+  /* Array [n_alternatives][n_operand] of static constraint info for\n+     given operand in given alternative.  This info can be changed if\n+     the target reg info is changed.  */\n+  struct operand_alternative *operand_alternative;\n+};\n+\n+/* LRA internal info about an insn (LRA internal insn\n+   representation).  */\n+struct lra_insn_recog_data\n+{\n+  /* The insn code.  */\n+  int icode;\n+  /* The insn itself.  */\n+  rtx insn;\n+  /* Common data for insns with the same ICODE.  Asm insns (their\n+     ICODE is negative) do not share such structures.  */\n+  struct lra_static_insn_data *insn_static_data;\n+  /* Two arrays of size correspondingly equal to the operand and the\n+     duplication numbers: */\n+  rtx **operand_loc; /* The operand locations, NULL if no operands.  */\n+  rtx **dup_loc; /* The dup locations, NULL if no dups.\t */\n+  /* Number of hard registers implicitly used in given call insn.  The\n+     value can be NULL or points to array of the hard register numbers\n+     ending with a negative value.  */\n+  int *arg_hard_regs;\n+#ifdef HAVE_ATTR_enabled\n+  /* Alternative enabled for the insn.\tNULL for debug insns.  */\n+  bool *alternative_enabled_p;\n+#endif\n+  /* The alternative should be used for the insn, -1 if invalid, or we\n+     should try to use any alternative, or the insn is a debug\n+     insn.  */\n+  int used_insn_alternative;\n+  /* The following member value is always NULL for a debug insn.  */\n+  struct lra_insn_reg *regs;\n+};\n+\n+typedef struct lra_insn_recog_data *lra_insn_recog_data_t;\n+\n+/* lra.c: */\n+\n+extern FILE *lra_dump_file;\n+\n+extern bool lra_reg_spill_p;\n+\n+extern HARD_REG_SET lra_no_alloc_regs;\n+\n+extern int lra_insn_recog_data_len;\n+extern lra_insn_recog_data_t *lra_insn_recog_data;\n+\n+extern int lra_curr_reload_num;\n+\n+extern void lra_push_insn (rtx);\n+extern void lra_push_insn_by_uid (unsigned int);\n+extern void lra_push_insn_and_update_insn_regno_info (rtx);\n+extern rtx lra_pop_insn (void);\n+extern unsigned int lra_insn_stack_length (void);\n+\n+extern rtx lra_create_new_reg_with_unique_value (enum machine_mode, rtx,\n+\t\t\t\t\t\t enum reg_class, const char *);\n+extern void lra_set_regno_unique_value (int);\n+extern void lra_invalidate_insn_data (rtx);\n+extern void lra_set_insn_deleted (rtx);\n+extern void lra_delete_dead_insn (rtx);\n+extern void lra_emit_add (rtx, rtx, rtx);\n+extern void lra_emit_move (rtx, rtx);\n+extern void lra_update_dups (lra_insn_recog_data_t, signed char *);\n+\n+extern void lra_process_new_insns (rtx, rtx, rtx, const char *);\n+\n+extern lra_insn_recog_data_t lra_set_insn_recog_data (rtx);\n+extern lra_insn_recog_data_t lra_update_insn_recog_data (rtx);\n+extern void lra_set_used_insn_alternative (rtx, int);\n+extern void lra_set_used_insn_alternative_by_uid (int, int);\n+\n+extern void lra_invalidate_insn_regno_info (rtx);\n+extern void lra_update_insn_regno_info (rtx);\n+extern struct lra_insn_reg *lra_get_insn_regs (int);\n+\n+extern void lra_free_copies (void);\n+extern void lra_create_copy (int, int, int);\n+extern lra_copy_t lra_get_copy (int);\n+extern bool lra_former_scratch_p (int);\n+extern bool lra_former_scratch_operand_p (rtx, int);\n+\n+extern int lra_constraint_new_regno_start;\n+extern bitmap_head lra_inheritance_pseudos;\n+extern bitmap_head lra_split_regs;\n+extern bitmap_head lra_optional_reload_pseudos;\n+extern int lra_constraint_new_insn_uid_start;\n+\n+/* lra-constraints.c: */\n+\n+extern int lra_constraint_offset (int, enum machine_mode);\n+\n+extern int lra_constraint_iter;\n+extern int lra_constraint_iter_after_spill;\n+extern bool lra_risky_transformations_p;\n+extern int lra_inheritance_iter;\n+extern int lra_undo_inheritance_iter;\n+extern bool lra_constraints (bool);\n+extern void lra_constraints_init (void);\n+extern void lra_constraints_finish (void);\n+extern void lra_inheritance (void);\n+extern bool lra_undo_inheritance (void);\n+\n+/* lra-lives.c: */\n+\n+extern int lra_live_max_point;\n+extern int *lra_point_freq;\n+\n+extern int lra_hard_reg_usage[FIRST_PSEUDO_REGISTER];\n+\n+extern int lra_live_range_iter;\n+extern void lra_create_live_ranges (bool);\n+extern lra_live_range_t lra_copy_live_range_list (lra_live_range_t);\n+extern lra_live_range_t lra_merge_live_ranges (lra_live_range_t,\n+\t\t\t\t\t       lra_live_range_t);\n+extern bool lra_intersected_live_ranges_p (lra_live_range_t,\n+\t\t\t\t\t   lra_live_range_t);\n+extern void lra_print_live_range_list (FILE *, lra_live_range_t);\n+extern void lra_debug_live_range_list (lra_live_range_t);\n+extern void lra_debug_pseudo_live_ranges (int);\n+extern void lra_debug_live_ranges (void);\n+extern void lra_clear_live_ranges (void);\n+extern void lra_live_ranges_init (void);\n+extern void lra_live_ranges_finish (void);\n+extern void lra_setup_reload_pseudo_preferenced_hard_reg (int, int, int);\n+\n+/* lra-assigns.c: */\n+\n+extern void lra_setup_reg_renumber (int, int, bool);\n+extern bool lra_assign (void);\n+\n+\n+/* lra-coalesce.c: */\n+\n+extern int lra_coalesce_iter;\n+extern bool lra_coalesce (void);\n+\n+/* lra-spills.c:  */\n+\n+extern bool lra_need_for_spills_p (void);\n+extern void lra_spill (void);\n+extern void lra_hard_reg_substitution (void);\n+\n+\n+/* lra-elimination.c: */\n+\n+extern void lra_debug_elim_table (void);\n+extern int lra_get_elimination_hard_regno (int);\n+extern rtx lra_eliminate_regs_1 (rtx, enum machine_mode, bool, bool, bool);\n+extern void lra_eliminate (bool);\n+\n+extern void lra_eliminate_reg_if_possible (rtx *);\n+\n+\f\n+\n+/* Update insn operands which are duplication of NOP operand.  The\n+   insn is represented by its LRA internal representation ID.  */\n+static inline void\n+lra_update_dup (lra_insn_recog_data_t id, int nop)\n+{\n+  int i;\n+  struct lra_static_insn_data *static_id = id->insn_static_data;\n+\n+  for (i = 0; i < static_id->n_dups; i++)\n+    if (static_id->dup_num[i] == nop)\n+      *id->dup_loc[i] = *id->operand_loc[nop];\n+}\n+\n+/* Process operator duplications in insn with ID.  We do it after the\n+   operands processing.\t Generally speaking, we could do this probably\n+   simultaneously with operands processing because a common practice\n+   is to enumerate the operators after their operands.\t*/\n+static inline void\n+lra_update_operator_dups (lra_insn_recog_data_t id)\n+{\n+  int i;\n+  struct lra_static_insn_data *static_id = id->insn_static_data;\n+\n+  for (i = 0; i < static_id->n_dups; i++)\n+    {\n+      int ndup = static_id->dup_num[i];\n+      \n+      if (static_id->operand[ndup].is_operator)\n+\t*id->dup_loc[i] = *id->operand_loc[ndup];\n+    }\n+}\n+\n+/* Return info about INSN.  Set up the info if it is not done yet.  */\n+static inline lra_insn_recog_data_t\n+lra_get_insn_recog_data (rtx insn)\n+{\n+  lra_insn_recog_data_t data;\n+  unsigned int uid = INSN_UID (insn);\n+\n+  if (lra_insn_recog_data_len > (int) uid\n+      && (data = lra_insn_recog_data[uid]) != NULL)\n+    {\n+      /* Check that we did not change insn without updating the insn\n+\t info.\t*/\n+      lra_assert (data->insn == insn\n+\t\t  && (INSN_CODE (insn) < 0\n+\t\t      || data->icode == INSN_CODE (insn)));\n+      return data;\n+    }\n+  return lra_set_insn_recog_data (insn);\n+}\n+\n+\f\n+\n+struct target_lra_int\n+{\n+  /* Map INSN_UID -> the operand alternative data (NULL if unknown).\n+     We assume that this data is valid until register info is changed\n+     because classes in the data can be changed.  */\n+  struct operand_alternative *x_op_alt_data[LAST_INSN_CODE];\n+};\n+\n+extern struct target_lra_int default_target_lra_int;\n+#if SWITCHABLE_TARGET\n+extern struct target_lra_int *this_target_lra_int;\n+#else\n+#define this_target_lra_int (&default_target_lra_int)\n+#endif\n+\n+#define op_alt_data (this_target_lra_int->x_op_alt_data)"}, {"sha": "6e00250c7ed03e41ee40c055e6a2ae002a801e16", "filename": "gcc/lra-lives.c", "status": "added", "additions": 1010, "deletions": 0, "changes": 1010, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-lives.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-lives.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flra-lives.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -0,0 +1,1010 @@\n+/* Build live ranges for pseudos.\n+   Copyright (C) 2010, 2011, 2012\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.\tIf not see\n+<http://www.gnu.org/licenses/>.\t */\n+\n+\n+/* This file contains code to build pseudo live-ranges (analogous\n+   structures used in IRA, so read comments about the live-ranges\n+   there) and other info necessary for other passes to assign\n+   hard-registers to pseudos, coalesce the spilled pseudos, and assign\n+   stack memory slots to spilled pseudos.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"hard-reg-set.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"output.h\"\n+#include \"regs.h\"\n+#include \"function.h\"\n+#include \"expr.h\"\n+#include \"basic-block.h\"\n+#include \"except.h\"\n+#include \"df.h\"\n+#include \"ira.h\"\n+#include \"sparseset.h\"\n+#include \"lra-int.h\"\n+\n+/* Program points are enumerated by numbers from range\n+   0..LRA_LIVE_MAX_POINT-1.  There are approximately two times more\n+   program points than insns.  Program points are places in the\n+   program where liveness info can be changed.\tIn most general case\n+   (there are more complicated cases too) some program points\n+   correspond to places where input operand dies and other ones\n+   correspond to places where output operands are born.\t */\n+int lra_live_max_point;\n+\n+/* Accumulated execution frequency of all references for each hard\n+   register.  */\n+int lra_hard_reg_usage[FIRST_PSEUDO_REGISTER];\n+\n+/* A global flag whose true value says to build live ranges for all\n+   pseudos, otherwise the live ranges only for pseudos got memory is\n+   build.  True value means also building copies and setting up hard\n+   register preferences.  The complete info is necessary only for the\n+   assignment pass.  The complete info is not needed for the\n+   coalescing and spill passes.\t */\n+static bool complete_info_p;\n+\n+/* Pseudos live at current point in the RTL scan.  */\n+static sparseset pseudos_live;\n+\n+/* Pseudos probably living through calls and setjumps.\tAs setjump is\n+   a call too, if a bit in PSEUDOS_LIVE_THROUGH_SETJUMPS is set up\n+   then the corresponding bit in PSEUDOS_LIVE_THROUGH_CALLS is set up\n+   too.\t These data are necessary for cases when only one subreg of a\n+   multi-reg pseudo is set up after a call.  So we decide it is\n+   probably live when traversing bb backward.  We are sure about\n+   living when we see its usage or definition of the pseudo.  */\n+static sparseset pseudos_live_through_calls;\n+static sparseset pseudos_live_through_setjumps;\n+\n+/* Set of hard regs (except eliminable ones) currently live.  */\n+static HARD_REG_SET hard_regs_live;\n+\n+/* Set of pseudos and hard registers start living/dying in the current\n+   insn.  These sets are used to update REG_DEAD and REG_UNUSED notes\n+   in the insn.\t */\n+static sparseset start_living, start_dying;\n+\n+/* Set of pseudos and hard regs dead and unused in the current\n+   insn.  */\n+static sparseset unused_set, dead_set;\n+\n+/* Pool for pseudo live ranges.\t */\n+static alloc_pool live_range_pool;\n+\n+/* Free live range LR.\t*/\n+static void\n+free_live_range (lra_live_range_t lr)\n+{\n+  pool_free (live_range_pool, lr);\n+}\n+\n+/* Free live range list LR.  */\n+static void\n+free_live_range_list (lra_live_range_t lr)\n+{\n+  lra_live_range_t next;\n+\n+  while (lr != NULL)\n+    {\n+      next = lr->next;\n+      free_live_range (lr);\n+      lr = next;\n+    }\n+}\n+\n+/* Create and return pseudo live range with given attributes.  */\n+static lra_live_range_t\n+create_live_range (int regno, int start, int finish, lra_live_range_t next)\n+{\n+  lra_live_range_t p;\n+\n+  p = (lra_live_range_t) pool_alloc (live_range_pool);\n+  p->regno = regno;\n+  p->start = start;\n+  p->finish = finish;\n+  p->next = next;\n+  return p;\n+}\n+\n+/* Copy live range R and return the result.  */\n+static lra_live_range_t\n+copy_live_range (lra_live_range_t r)\n+{\n+  lra_live_range_t p;\n+\n+  p = (lra_live_range_t) pool_alloc (live_range_pool);\n+  *p = *r;\n+  return p;\n+}\n+\n+/* Copy live range list given by its head R and return the result.  */\n+lra_live_range_t\n+lra_copy_live_range_list (lra_live_range_t r)\n+{\n+  lra_live_range_t p, first, *chain;\n+\n+  first = NULL;\n+  for (chain = &first; r != NULL; r = r->next)\n+    {\n+      p = copy_live_range (r);\n+      *chain = p;\n+      chain = &p->next;\n+    }\n+  return first;\n+}\n+\n+/* Merge *non-intersected* ranges R1 and R2 and returns the result.\n+   The function maintains the order of ranges and tries to minimize\n+   size of the result range list.  Ranges R1 and R2 may not be used\n+   after the call.  */\n+lra_live_range_t\n+lra_merge_live_ranges (lra_live_range_t r1, lra_live_range_t r2)\n+{\n+  lra_live_range_t first, last, temp;\n+\n+  if (r1 == NULL)\n+    return r2;\n+  if (r2 == NULL)\n+    return r1;\n+  for (first = last = NULL; r1 != NULL && r2 != NULL;)\n+    {\n+      if (r1->start < r2->start)\n+\t{\n+\t  temp = r1;\n+\t  r1 = r2;\n+\t  r2 = temp;\n+\t}\n+      if (r1->start == r2->finish + 1)\n+\t{\n+\t  /* Joint ranges: merge r1 and r2 into r1.  */\n+\t  r1->start = r2->start;\n+\t  temp = r2;\n+\t  r2 = r2->next;\n+\t  pool_free (live_range_pool, temp);\n+\t}\n+      else\n+\t{\n+\t  gcc_assert (r2->finish + 1 < r1->start);\n+\t  /* Add r1 to the result.  */\n+\t  if (first == NULL)\n+\t    first = last = r1;\n+\t  else\n+\t    {\n+\t      last->next = r1;\n+\t      last = r1;\n+\t    }\n+\t  r1 = r1->next;\n+\t}\n+    }\n+  if (r1 != NULL)\n+    {\n+      if (first == NULL)\n+\tfirst = r1;\n+      else\n+\tlast->next = r1;\n+    }\n+  else\n+    {\n+      lra_assert (r2 != NULL);\n+      if (first == NULL)\n+\tfirst = r2;\n+      else\n+\tlast->next = r2;\n+    }\n+  return first;\n+}\n+\n+/* Return TRUE if live ranges R1 and R2 intersect.  */\n+bool\n+lra_intersected_live_ranges_p (lra_live_range_t r1, lra_live_range_t r2)\n+{\n+  /* Remember the live ranges are always kept ordered.\t*/\n+  while (r1 != NULL && r2 != NULL)\n+    {\n+      if (r1->start > r2->finish)\n+\tr1 = r1->next;\n+      else if (r2->start > r1->finish)\n+\tr2 = r2->next;\n+      else\n+\treturn true;\n+    }\n+  return false;\n+}\n+\n+/* The function processing birth of hard register REGNO.  It updates\n+   living hard regs, conflict hard regs for living pseudos, and\n+   START_LIVING.  */\n+static void\n+make_hard_regno_born (int regno)\n+{\n+  unsigned int i;\n+\n+  lra_assert (regno < FIRST_PSEUDO_REGISTER);\n+  if (TEST_HARD_REG_BIT (lra_no_alloc_regs, regno)\n+      || TEST_HARD_REG_BIT (hard_regs_live, regno))\n+    return;\n+  SET_HARD_REG_BIT (hard_regs_live, regno);\n+  sparseset_set_bit (start_living, regno);\n+  EXECUTE_IF_SET_IN_SPARSESET (pseudos_live, i)\n+    SET_HARD_REG_BIT (lra_reg_info[i].conflict_hard_regs, regno);\n+}\n+\n+/* Process the death of hard register REGNO.  This updates\n+   hard_regs_live and START_DYING.  */\n+static void\n+make_hard_regno_dead (int regno)\n+{\n+  lra_assert (regno < FIRST_PSEUDO_REGISTER);\n+  if (TEST_HARD_REG_BIT (lra_no_alloc_regs, regno)\n+      || ! TEST_HARD_REG_BIT (hard_regs_live, regno))\n+    return;\n+  sparseset_set_bit (start_dying, regno);\n+  CLEAR_HARD_REG_BIT (hard_regs_live, regno);\n+}\n+\n+/* Mark pseudo REGNO as living at program point POINT, update conflicting\n+   hard registers of the pseudo and START_LIVING, and start a new live\n+   range for the pseudo corresponding to REGNO if it is necessary.  */\n+static void\n+mark_pseudo_live (int regno, int point)\n+{\n+  lra_live_range_t p;\n+\n+  lra_assert (regno >= FIRST_PSEUDO_REGISTER);\n+  lra_assert (! sparseset_bit_p (pseudos_live, regno));\n+  sparseset_set_bit (pseudos_live, regno);\n+  IOR_HARD_REG_SET (lra_reg_info[regno].conflict_hard_regs, hard_regs_live);\n+\n+  if ((complete_info_p || lra_get_regno_hard_regno (regno) < 0)\n+      && ((p = lra_reg_info[regno].live_ranges) == NULL\n+\t  || (p->finish != point && p->finish + 1 != point)))\n+     lra_reg_info[regno].live_ranges\n+       = create_live_range (regno, point, -1, p);\n+  sparseset_set_bit (start_living, regno);\n+}\n+\n+/* Mark pseudo REGNO as not living at program point POINT and update\n+   START_DYING.\n+   This finishes the current live range for the pseudo corresponding\n+   to REGNO.  */\n+static void\n+mark_pseudo_dead (int regno, int point)\n+{\n+  lra_live_range_t p;\n+\n+  lra_assert (regno >= FIRST_PSEUDO_REGISTER);\n+  lra_assert (sparseset_bit_p (pseudos_live, regno));\n+  sparseset_clear_bit (pseudos_live, regno);\n+  sparseset_set_bit (start_dying, regno);\n+  if (complete_info_p || lra_get_regno_hard_regno (regno) < 0)\n+    {\n+      p = lra_reg_info[regno].live_ranges;\n+      lra_assert (p != NULL);\n+      p->finish = point;\n+    }\n+}\n+\n+/* Mark register REGNO (pseudo or hard register) in MODE as live\n+   at program point POINT.\n+   Return TRUE if the liveness tracking sets were modified,\n+   or FALSE if nothing changed.  */\n+static bool\n+mark_regno_live (int regno, enum machine_mode mode, int point)\n+{\n+  int last;\n+  bool changed = false;\n+\n+  if (regno < FIRST_PSEUDO_REGISTER)\n+    {\n+      for (last = regno + hard_regno_nregs[regno][mode];\n+\t   regno < last;\n+\t   regno++)\n+\tmake_hard_regno_born (regno);\n+    }\n+  else if (! sparseset_bit_p (pseudos_live, regno))\n+    {\n+      mark_pseudo_live (regno, point);\n+      changed = true;\n+    }\n+  return changed;\n+}\n+\n+\n+/* Mark register REGNO in MODE as dead at program point POINT.\n+   Return TRUE if the liveness tracking sets were modified,\n+   or FALSE if nothing changed.  */\n+static bool\n+mark_regno_dead (int regno, enum machine_mode mode, int point)\n+{\n+  int last;\n+  bool changed = false;\n+\n+  if (regno < FIRST_PSEUDO_REGISTER)\n+    {\n+      for (last = regno + hard_regno_nregs[regno][mode];\n+\t   regno < last;\n+\t   regno++)\n+\tmake_hard_regno_dead (regno);\n+    }\n+  else if (sparseset_bit_p (pseudos_live, regno))\n+    {\n+      mark_pseudo_dead (regno, point);\n+      changed = true;\n+    }\n+  return changed;\n+}\n+\n+/* Insn currently scanned.  */\n+static rtx curr_insn;\n+/* The insn data.  */\n+static lra_insn_recog_data_t curr_id;\n+/* The insn static data.  */\n+static struct lra_static_insn_data *curr_static_id;\n+\n+/* Return true when one of the predecessor edges of BB is marked with\n+   EDGE_ABNORMAL_CALL or EDGE_EH.  */\n+static bool\n+bb_has_abnormal_call_pred (basic_block bb)\n+{\n+  edge e;\n+  edge_iterator ei;\n+\n+  FOR_EACH_EDGE (e, ei, bb->preds)\n+    {\n+      if (e->flags & (EDGE_ABNORMAL_CALL | EDGE_EH))\n+\treturn true;\n+    }\n+  return false;\n+}\n+\n+/* Vec containing execution frequencies of program points.  */\n+static VEC(int,heap) *point_freq_vec;\n+\n+/* The start of the above vector elements.  */\n+int *lra_point_freq;\n+\n+/* Increment the current program point POINT to the next point which has\n+   execution frequency FREQ.  */\n+static void\n+next_program_point (int &point, int freq)\n+{\n+  VEC_safe_push (int, heap, point_freq_vec, freq);\n+  lra_point_freq = VEC_address (int, point_freq_vec);\n+  point++;\n+}\n+\n+/* Update the preference of HARD_REGNO for pseudo REGNO by PROFIT.  */\n+void\n+lra_setup_reload_pseudo_preferenced_hard_reg (int regno,\n+\t\t\t\t\t      int hard_regno, int profit)\n+{\n+  lra_assert (regno >= lra_constraint_new_regno_start);\n+  if (lra_reg_info[regno].preferred_hard_regno1 == hard_regno)\n+    lra_reg_info[regno].preferred_hard_regno_profit1 += profit;\n+  else if (lra_reg_info[regno].preferred_hard_regno2 == hard_regno)\n+    lra_reg_info[regno].preferred_hard_regno_profit2 += profit;\n+  else if (lra_reg_info[regno].preferred_hard_regno1 < 0)\n+    {\n+      lra_reg_info[regno].preferred_hard_regno1 = hard_regno;\n+      lra_reg_info[regno].preferred_hard_regno_profit1 = profit;\n+    }\n+  else if (lra_reg_info[regno].preferred_hard_regno2 < 0\n+\t   || profit > lra_reg_info[regno].preferred_hard_regno_profit2)\n+    {\n+      lra_reg_info[regno].preferred_hard_regno2 = hard_regno;\n+      lra_reg_info[regno].preferred_hard_regno_profit2 = profit;\n+    }\n+  else\n+    return;\n+  /* Keep the 1st hard regno as more profitable.  */\n+  if (lra_reg_info[regno].preferred_hard_regno1 >= 0\n+      && lra_reg_info[regno].preferred_hard_regno2 >= 0\n+      && (lra_reg_info[regno].preferred_hard_regno_profit2\n+\t  > lra_reg_info[regno].preferred_hard_regno_profit1))\n+    {\n+      int temp;\n+\n+      temp = lra_reg_info[regno].preferred_hard_regno1;\n+      lra_reg_info[regno].preferred_hard_regno1\n+\t= lra_reg_info[regno].preferred_hard_regno2;\n+      lra_reg_info[regno].preferred_hard_regno2 = temp;\n+      temp = lra_reg_info[regno].preferred_hard_regno_profit1;\n+      lra_reg_info[regno].preferred_hard_regno_profit1\n+\t= lra_reg_info[regno].preferred_hard_regno_profit2;\n+      lra_reg_info[regno].preferred_hard_regno_profit2 = temp;\n+    }\n+  if (lra_dump_file != NULL)\n+    {\n+      if ((hard_regno = lra_reg_info[regno].preferred_hard_regno1) >= 0)\n+\tfprintf (lra_dump_file,\n+\t\t \"\tHard reg %d is preferable by r%d with profit %d\\n\",\n+\t\t hard_regno, regno,\n+\t\t lra_reg_info[regno].preferred_hard_regno_profit1);\n+      if ((hard_regno = lra_reg_info[regno].preferred_hard_regno2) >= 0)\n+\tfprintf (lra_dump_file,\n+\t\t \"\tHard reg %d is preferable by r%d with profit %d\\n\",\n+\t\t hard_regno, regno,\n+\t\t lra_reg_info[regno].preferred_hard_regno_profit2);\n+    }\n+}\n+\n+/* Check that REGNO living through calls and setjumps, set up conflict\n+   regs, and clear corresponding bits in PSEUDOS_LIVE_THROUGH_CALLS and\n+   PSEUDOS_LIVE_THROUGH_SETJUMPS.  */\n+static inline void\n+check_pseudos_live_through_calls (int regno)\n+{\n+  if (! sparseset_bit_p (pseudos_live_through_calls, regno))\n+    return;\n+  sparseset_clear_bit (pseudos_live_through_calls, regno);\n+  IOR_HARD_REG_SET (lra_reg_info[regno].conflict_hard_regs,\n+\t\t    call_used_reg_set);\n+#ifdef ENABLE_CHECKING\n+  lra_reg_info[regno].call_p = true;\n+#endif\n+  if (! sparseset_bit_p (pseudos_live_through_setjumps, regno))\n+    return;\n+  sparseset_clear_bit (pseudos_live_through_setjumps, regno);\n+  /* Don't allocate pseudos that cross setjmps or any call, if this\n+     function receives a nonlocal goto.\t */\n+  SET_HARD_REG_SET (lra_reg_info[regno].conflict_hard_regs);\n+}\n+\n+/* Process insns of the basic block BB to update pseudo live ranges,\n+   pseudo hard register conflicts, and insn notes.  We do it on\n+   backward scan of BB insns.  CURR_POINT is the program point where\n+   BB ends.  The function updates this counter and returns in\n+   CURR_POINT the program point where BB starts.  */\n+static void\n+process_bb_lives (basic_block bb, int &curr_point)\n+{\n+  int i, regno, freq;\n+  unsigned int j;\n+  bitmap_iterator bi;\n+  bitmap reg_live_out;\n+  unsigned int px;\n+  rtx link, *link_loc;\n+  bool need_curr_point_incr;\n+\n+  reg_live_out = df_get_live_out (bb);\n+  sparseset_clear (pseudos_live);\n+  sparseset_clear (pseudos_live_through_calls);\n+  sparseset_clear (pseudos_live_through_setjumps);\n+  REG_SET_TO_HARD_REG_SET (hard_regs_live, reg_live_out);\n+  AND_COMPL_HARD_REG_SET (hard_regs_live, eliminable_regset);\n+  AND_COMPL_HARD_REG_SET (hard_regs_live, lra_no_alloc_regs);\n+  EXECUTE_IF_SET_IN_BITMAP (reg_live_out, FIRST_PSEUDO_REGISTER, j, bi)\n+    mark_pseudo_live (j, curr_point);\n+\n+  freq = REG_FREQ_FROM_BB (bb);\n+\n+  if (lra_dump_file != NULL)\n+    fprintf (lra_dump_file, \"  BB %d\\n\", bb->index);\n+\n+  /* Scan the code of this basic block, noting which pseudos and hard\n+     regs are born or die.\n+\n+     Note that this loop treats uninitialized values as live until the\n+     beginning of the block.  For example, if an instruction uses\n+     (reg:DI foo), and only (subreg:SI (reg:DI foo) 0) is ever set,\n+     FOO will remain live until the beginning of the block.  Likewise\n+     if FOO is not set at all.\tThis is unnecessarily pessimistic, but\n+     it probably doesn't matter much in practice.  */\n+  FOR_BB_INSNS_REVERSE (bb, curr_insn)\n+    {\n+      bool call_p;\n+      int dst_regno, src_regno;\n+      rtx set;\n+      struct lra_insn_reg *reg;\n+\n+      if (!NONDEBUG_INSN_P (curr_insn))\n+\tcontinue;\n+\n+      curr_id = lra_get_insn_recog_data (curr_insn);\n+      curr_static_id = curr_id->insn_static_data;\n+      if (lra_dump_file != NULL)\n+\tfprintf (lra_dump_file, \"   Insn %u: point = %d\\n\",\n+\t\t INSN_UID (curr_insn), curr_point);\n+\n+      /* Update max ref width and hard reg usage.  */\n+      for (reg = curr_id->regs; reg != NULL; reg = reg->next)\n+\tif (reg->regno >= FIRST_PSEUDO_REGISTER\n+\t    && (GET_MODE_SIZE (reg->biggest_mode)\n+\t\t> GET_MODE_SIZE (lra_reg_info[reg->regno].biggest_mode)))\n+\t  lra_reg_info[reg->regno].biggest_mode = reg->biggest_mode;\n+\telse if (reg->regno < FIRST_PSEUDO_REGISTER)\n+\t  lra_hard_reg_usage[reg->regno] += freq;\n+\n+      call_p = CALL_P (curr_insn);\n+      if (complete_info_p\n+\t  && (set = single_set (curr_insn)) != NULL_RTX\n+\t  && REG_P (SET_DEST (set)) && REG_P (SET_SRC (set))\n+\t  /* Check that source regno does not conflict with\n+\t     destination regno to exclude most impossible\n+\t     preferences.  */\n+\t  && ((((src_regno = REGNO (SET_SRC (set))) >= FIRST_PSEUDO_REGISTER\n+\t\t&& ! sparseset_bit_p (pseudos_live, src_regno))\n+\t       || (src_regno < FIRST_PSEUDO_REGISTER\n+\t\t   && ! TEST_HARD_REG_BIT (hard_regs_live, src_regno)))\n+\t      /* It might be 'inheritance pseudo <- reload pseudo'.  */\n+\t      || (src_regno >= lra_constraint_new_regno_start\n+\t\t  && ((int) REGNO (SET_DEST (set))\n+\t\t      >= lra_constraint_new_regno_start))))\n+\t{\n+\t  int hard_regno = -1, regno = -1;\n+\n+\t  dst_regno = REGNO (SET_DEST (set));\n+\t  if (dst_regno >= lra_constraint_new_regno_start\n+\t      && src_regno >= lra_constraint_new_regno_start)\n+\t    lra_create_copy (dst_regno, src_regno, freq);\n+\t  else if (dst_regno >= lra_constraint_new_regno_start)\n+\t    {\n+\t      if ((hard_regno = src_regno) >= FIRST_PSEUDO_REGISTER)\n+\t\thard_regno = reg_renumber[src_regno];\n+\t      regno = dst_regno;\n+\t    }\n+\t  else if (src_regno >= lra_constraint_new_regno_start)\n+\t    {\n+\t      if ((hard_regno = dst_regno) >= FIRST_PSEUDO_REGISTER)\n+\t\thard_regno = reg_renumber[dst_regno];\n+\t      regno = src_regno;\n+\t    }\n+\t  if (regno >= 0 && hard_regno >= 0)\n+\t    lra_setup_reload_pseudo_preferenced_hard_reg\n+\t      (regno, hard_regno, freq);\n+\t}\n+\n+      sparseset_clear (start_living);\n+\n+      /* Try to avoid unnecessary program point increments, this saves\n+\t a lot of time in remove_some_program_points_and_update_live_ranges.\n+\t We only need an increment if something becomes live or dies at this\n+\t program point.  */\n+      need_curr_point_incr = false;\n+\n+      /* Mark each defined value as live.  We need to do this for\n+\t unused values because they still conflict with quantities\n+\t that are live at the time of the definition.  */\n+      for (reg = curr_id->regs; reg != NULL; reg = reg->next)\n+\tif (reg->type != OP_IN)\n+\t  {\n+\t    need_curr_point_incr |= mark_regno_live (reg->regno,\n+\t\t\t\t\t\t     reg->biggest_mode,\n+\t\t\t\t\t\t     curr_point);\n+\t    check_pseudos_live_through_calls (reg->regno);\n+\t  }\n+\n+      for (reg = curr_static_id->hard_regs; reg != NULL; reg = reg->next)\n+\tif (reg->type != OP_IN)\n+\t  make_hard_regno_born (reg->regno);\n+\n+      sparseset_copy (unused_set, start_living);\n+\n+      sparseset_clear (start_dying);\n+\n+      /* See which defined values die here.  */\n+      for (reg = curr_id->regs; reg != NULL; reg = reg->next)\n+\tif (reg->type == OP_OUT && ! reg->early_clobber && ! reg->subreg_p)\n+\t  need_curr_point_incr |= mark_regno_dead (reg->regno,\n+\t\t\t\t\t\t   reg->biggest_mode,\n+\t\t\t\t\t\t   curr_point);\n+\n+      for (reg = curr_static_id->hard_regs; reg != NULL; reg = reg->next)\n+\tif (reg->type == OP_OUT && ! reg->early_clobber && ! reg->subreg_p)\n+\t  make_hard_regno_dead (reg->regno);\n+\n+      if (call_p)\n+\t{\n+\t  sparseset_ior (pseudos_live_through_calls,\n+\t\t\t pseudos_live_through_calls, pseudos_live);\n+\t  if (cfun->has_nonlocal_label\n+\t      || find_reg_note (curr_insn, REG_SETJMP,\n+\t\t\t\tNULL_RTX) != NULL_RTX)\n+\t    sparseset_ior (pseudos_live_through_setjumps,\n+\t\t\t   pseudos_live_through_setjumps, pseudos_live);\n+\t}\n+\n+      /* Increment the current program point if we must.  */\n+      if (need_curr_point_incr)\n+\tnext_program_point (curr_point, freq);\n+\n+      sparseset_clear (start_living);\n+\n+      need_curr_point_incr = false;\n+\n+      /* Mark each used value as live.\t*/\n+      for (reg = curr_id->regs; reg != NULL; reg = reg->next)\n+\tif (reg->type == OP_IN)\n+\t  {\n+\t    need_curr_point_incr |= mark_regno_live (reg->regno,\n+\t\t\t\t\t\t     reg->biggest_mode,\n+\t\t\t\t\t\t     curr_point);\n+\t    check_pseudos_live_through_calls (reg->regno);\n+\t  }\n+\n+      for (reg = curr_static_id->hard_regs; reg != NULL; reg = reg->next)\n+\tif (reg->type == OP_IN)\n+\t  make_hard_regno_born (reg->regno);\n+\n+      if (curr_id->arg_hard_regs != NULL)\n+\t/* Make argument hard registers live.  */\n+\tfor (i = 0; (regno = curr_id->arg_hard_regs[i]) >= 0; i++)\n+\t  make_hard_regno_born (regno);\n+\n+      sparseset_and_compl (dead_set, start_living, start_dying);\n+\n+      /* Mark early clobber outputs dead.  */\n+      for (reg = curr_id->regs; reg != NULL; reg = reg->next)\n+\tif (reg->type == OP_OUT && reg->early_clobber && ! reg->subreg_p)\n+\t  need_curr_point_incr = mark_regno_dead (reg->regno,\n+\t\t\t\t\t\t  reg->biggest_mode,\n+\t\t\t\t\t\t  curr_point);\n+\n+      for (reg = curr_static_id->hard_regs; reg != NULL; reg = reg->next)\n+\tif (reg->type == OP_OUT && reg->early_clobber && ! reg->subreg_p)\n+\t  make_hard_regno_dead (reg->regno);\n+\n+      if (need_curr_point_incr)\n+\tnext_program_point (curr_point, freq);\n+\n+      /* Update notes.\t*/\n+      for (link_loc = &REG_NOTES (curr_insn); (link = *link_loc) != NULL_RTX;)\n+\t{\n+\t  if (REG_NOTE_KIND (link) != REG_DEAD\n+\t      && REG_NOTE_KIND (link) != REG_UNUSED)\n+\t    ;\n+\t  else if (REG_P (XEXP (link, 0)))\n+\t    {\n+\t      regno = REGNO (XEXP (link, 0));\n+\t      if ((REG_NOTE_KIND (link) == REG_DEAD\n+\t\t   && ! sparseset_bit_p (dead_set, regno))\n+\t\t  || (REG_NOTE_KIND (link) == REG_UNUSED\n+\t\t      && ! sparseset_bit_p (unused_set, regno)))\n+\t\t{\n+\t\t  *link_loc = XEXP (link, 1);\n+\t\t  continue;\n+\t\t}\n+\t      if (REG_NOTE_KIND (link) == REG_DEAD)\n+\t\tsparseset_clear_bit (dead_set, regno);\n+\t      else if (REG_NOTE_KIND (link) == REG_UNUSED)\n+\t\tsparseset_clear_bit (unused_set, regno);\n+\t    }\n+\t  link_loc = &XEXP (link, 1);\n+\t}\n+      EXECUTE_IF_SET_IN_SPARSESET (dead_set, j)\n+\tadd_reg_note (curr_insn, REG_DEAD, regno_reg_rtx[j]);\n+      EXECUTE_IF_SET_IN_SPARSESET (unused_set, j)\n+\tadd_reg_note (curr_insn, REG_UNUSED, regno_reg_rtx[j]);\n+    }\n+\n+#ifdef EH_RETURN_DATA_REGNO\n+  if (bb_has_eh_pred (bb))\n+    for (j = 0; ; ++j)\n+      {\n+\tunsigned int regno = EH_RETURN_DATA_REGNO (j);\n+\n+\tif (regno == INVALID_REGNUM)\n+\t  break;\n+\tmake_hard_regno_born (regno);\n+      }\n+#endif\n+\n+  /* Pseudos can't go in stack regs at the start of a basic block that\n+     is reached by an abnormal edge. Likewise for call clobbered regs,\n+     because caller-save, fixup_abnormal_edges and possibly the table\n+     driven EH machinery are not quite ready to handle such pseudos\n+     live across such edges.  */\n+  if (bb_has_abnormal_pred (bb))\n+    {\n+#ifdef STACK_REGS\n+      EXECUTE_IF_SET_IN_SPARSESET (pseudos_live, px)\n+\tlra_reg_info[px].no_stack_p = true;\n+      for (px = FIRST_STACK_REG; px <= LAST_STACK_REG; px++)\n+\tmake_hard_regno_born (px);\n+#endif\n+      /* No need to record conflicts for call clobbered regs if we\n+\t have nonlocal labels around, as we don't ever try to\n+\t allocate such regs in this case.  */\n+      if (!cfun->has_nonlocal_label && bb_has_abnormal_call_pred (bb))\n+\tfor (px = 0; px < FIRST_PSEUDO_REGISTER; px++)\n+\t  if (call_used_regs[px])\n+\t    make_hard_regno_born (px);\n+    }\n+\n+  /* See if we'll need an increment at the end of this basic block.\n+     An increment is needed if the PSEUDOS_LIVE set is not empty,\n+     to make sure the finish points are set up correctly.  */\n+  need_curr_point_incr = (sparseset_cardinality (pseudos_live) > 0);\n+\n+  EXECUTE_IF_SET_IN_SPARSESET (pseudos_live, i)\n+    mark_pseudo_dead (i, curr_point);\n+\n+  EXECUTE_IF_SET_IN_BITMAP (df_get_live_in (bb), FIRST_PSEUDO_REGISTER, j, bi)\n+    {\n+      if (sparseset_cardinality (pseudos_live_through_calls) == 0)\n+\tbreak;\n+      if (sparseset_bit_p (pseudos_live_through_calls, j))\n+\tcheck_pseudos_live_through_calls (j);\n+    }\n+\n+  if (need_curr_point_incr)\n+    next_program_point (curr_point, freq);\n+}\n+\n+/* Compress pseudo live ranges by removing program points where\n+   nothing happens.  Complexity of many algorithms in LRA is linear\n+   function of program points number.  To speed up the code we try to\n+   minimize the number of the program points here.  */\n+static void\n+remove_some_program_points_and_update_live_ranges (void)\n+{\n+  unsigned i;\n+  int n, max_regno;\n+  int *map;\n+  lra_live_range_t r, prev_r, next_r;\n+  sbitmap born_or_dead, born, dead;\n+  sbitmap_iterator sbi;\n+  bool born_p, dead_p, prev_born_p, prev_dead_p;\n+\n+  born = sbitmap_alloc (lra_live_max_point);\n+  dead = sbitmap_alloc (lra_live_max_point);\n+  sbitmap_zero (born);\n+  sbitmap_zero (dead);\n+  max_regno = max_reg_num ();\n+  for (i = FIRST_PSEUDO_REGISTER; i < (unsigned) max_regno; i++)\n+    {\n+      for (r = lra_reg_info[i].live_ranges; r != NULL; r = r->next)\n+\t{\n+\t  lra_assert (r->start <= r->finish);\n+\t  SET_BIT (born, r->start);\n+\t  SET_BIT (dead, r->finish);\n+\t}\n+    }\n+  born_or_dead = sbitmap_alloc (lra_live_max_point);\n+  sbitmap_a_or_b (born_or_dead, born, dead);\n+  map = XCNEWVEC (int, lra_live_max_point);\n+  n = -1;\n+  prev_born_p = prev_dead_p = false;\n+  EXECUTE_IF_SET_IN_SBITMAP (born_or_dead, 0, i, sbi)\n+    {\n+      born_p = TEST_BIT (born, i);\n+      dead_p = TEST_BIT (dead, i);\n+      if ((prev_born_p && ! prev_dead_p && born_p && ! dead_p)\n+\t  || (prev_dead_p && ! prev_born_p && dead_p && ! born_p))\n+\t{\n+\t  map[i] = n;\n+\t  lra_point_freq[n] = MAX (lra_point_freq[n], lra_point_freq[i]);\n+\t}\n+      else\n+\t{\n+\t  map[i] = ++n;\n+\t  lra_point_freq[n] = lra_point_freq[i];\n+\t}\n+      prev_born_p = born_p;\n+      prev_dead_p = dead_p;\n+    }\n+  sbitmap_free (born_or_dead);\n+  sbitmap_free (born);\n+  sbitmap_free (dead);\n+  n++;\n+  if (lra_dump_file != NULL)\n+    fprintf (lra_dump_file, \"Compressing live ranges: from %d to %d - %d%%\\n\",\n+\t     lra_live_max_point, n, 100 * n / lra_live_max_point);\n+  if (n < lra_live_max_point)\n+    {\n+      lra_live_max_point = n;\n+      for (i = FIRST_PSEUDO_REGISTER; i < (unsigned) max_regno; i++)\n+\t{\n+\t  for (prev_r = NULL, r = lra_reg_info[i].live_ranges;\n+\t       r != NULL;\n+\t       r = next_r)\n+\t    {\n+\t      next_r = r->next;\n+\t      r->start = map[r->start];\n+\t      r->finish = map[r->finish];\n+\t      if (prev_r == NULL || prev_r->start > r->finish + 1)\n+\t\t{\n+\t\t  prev_r = r;\n+\t\t  continue;\n+\t\t}\n+\t      prev_r->start = r->start;\n+\t      prev_r->next = next_r;\n+\t      free_live_range (r);\n+\t    }\n+\t}\n+    }\n+  free (map);\n+}\n+\n+/* Print live ranges R to file F.  */\n+void\n+lra_print_live_range_list (FILE *f, lra_live_range_t r)\n+{\n+  for (; r != NULL; r = r->next)\n+    fprintf (f, \" [%d..%d]\", r->start, r->finish);\n+  fprintf (f, \"\\n\");\n+}\n+\n+/* Print live ranges R to stderr.  */\n+void\n+lra_debug_live_range_list (lra_live_range_t r)\n+{\n+  lra_print_live_range_list (stderr, r);\n+}\n+\n+/* Print live ranges of pseudo REGNO to file F.\t */\n+static void\n+print_pseudo_live_ranges (FILE *f, int regno)\n+{\n+  if (lra_reg_info[regno].live_ranges == NULL)\n+    return;\n+  fprintf (f, \" r%d:\", regno);\n+  lra_print_live_range_list (f, lra_reg_info[regno].live_ranges);\n+}\n+\n+/* Print live ranges of pseudo REGNO to stderr.\t */\n+void\n+lra_debug_pseudo_live_ranges (int regno)\n+{\n+  print_pseudo_live_ranges (stderr, regno);\n+}\n+\n+/* Print live ranges of all pseudos to file F.\t*/\n+static void\n+print_live_ranges (FILE *f)\n+{\n+  int i, max_regno;\n+\n+  max_regno = max_reg_num ();\n+  for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+    print_pseudo_live_ranges (f, i);\n+}\n+\n+/* Print live ranges of all pseudos to stderr.\t*/\n+void\n+lra_debug_live_ranges (void)\n+{\n+  print_live_ranges (stderr);\n+}\n+\n+/* Compress pseudo live ranges.\t */\n+static void\n+compress_live_ranges (void)\n+{\n+  remove_some_program_points_and_update_live_ranges ();\n+  if (lra_dump_file != NULL)\n+    {\n+      fprintf (lra_dump_file, \"Ranges after the compression:\\n\");\n+      print_live_ranges (lra_dump_file);\n+    }\n+}\n+\n+/* The number of the current live range pass.  */\n+int lra_live_range_iter;\n+\n+/* The main entry function creates live ranges only for memory pseudos\n+   (or for all ones if ALL_P), set up CONFLICT_HARD_REGS for\n+   the pseudos.\t */\n+void\n+lra_create_live_ranges (bool all_p)\n+{\n+  basic_block bb;\n+  int i, hard_regno, max_regno = max_reg_num ();\n+  int curr_point;\n+\n+  timevar_push (TV_LRA_CREATE_LIVE_RANGES);\n+\n+  complete_info_p = all_p;\n+  if (lra_dump_file != NULL)\n+    fprintf (lra_dump_file,\n+\t     \"\\n********** Pseudo live ranges #%d: **********\\n\\n\",\n+\t     ++lra_live_range_iter);\n+  memset (lra_hard_reg_usage, 0, sizeof (lra_hard_reg_usage));\n+  for (i = 0; i < max_regno; i++)\n+    {\n+      lra_reg_info[i].live_ranges = NULL;\n+      CLEAR_HARD_REG_SET (lra_reg_info[i].conflict_hard_regs);\n+      lra_reg_info[i].preferred_hard_regno1 = -1;\n+      lra_reg_info[i].preferred_hard_regno2 = -1;\n+      lra_reg_info[i].preferred_hard_regno_profit1 = 0;\n+      lra_reg_info[i].preferred_hard_regno_profit2 = 0;\n+#ifdef STACK_REGS\n+      lra_reg_info[i].no_stack_p = false;\n+#endif\n+      if (regno_reg_rtx[i] != NULL_RTX)\n+\tlra_reg_info[i].biggest_mode = GET_MODE (regno_reg_rtx[i]);\n+      else\n+\tlra_reg_info[i].biggest_mode = VOIDmode;\n+#ifdef ENABLE_CHECKING\n+      lra_reg_info[i].call_p = false;\n+#endif\n+      if (i >= FIRST_PSEUDO_REGISTER\n+\t  && lra_reg_info[i].nrefs != 0 && (hard_regno = reg_renumber[i]) >= 0)\n+\tlra_hard_reg_usage[hard_regno] += lra_reg_info[i].freq;\n+    }\n+  lra_free_copies ();\n+  pseudos_live = sparseset_alloc (max_regno);\n+  pseudos_live_through_calls = sparseset_alloc (max_regno);\n+  pseudos_live_through_setjumps = sparseset_alloc (max_regno);\n+  start_living = sparseset_alloc (max_regno);\n+  start_dying = sparseset_alloc (max_regno);\n+  dead_set = sparseset_alloc (max_regno);\n+  unused_set = sparseset_alloc (max_regno);\n+  curr_point = 0;\n+  point_freq_vec = VEC_alloc (int, heap, get_max_uid () * 2);\n+  lra_point_freq = VEC_address (int, point_freq_vec);\n+  int *post_order_rev_cfg = XNEWVEC (int, last_basic_block);\n+  int n_blocks_inverted = inverted_post_order_compute (post_order_rev_cfg);\n+  lra_assert (n_blocks_inverted == n_basic_blocks);\n+  for (i = n_blocks_inverted - 1; i >= 0; --i)\n+    {\n+      bb = BASIC_BLOCK (post_order_rev_cfg[i]);\n+      if (bb == EXIT_BLOCK_PTR || bb == ENTRY_BLOCK_PTR)\n+\tcontinue;\n+      process_bb_lives (bb, curr_point);\n+    }\n+  free (post_order_rev_cfg);\n+  lra_live_max_point = curr_point;\n+  if (lra_dump_file != NULL)\n+    print_live_ranges (lra_dump_file);\n+  /* Clean up.\t*/\n+  sparseset_free (unused_set);\n+  sparseset_free (dead_set);\n+  sparseset_free (start_dying);\n+  sparseset_free (start_living);\n+  sparseset_free (pseudos_live_through_calls);\n+  sparseset_free (pseudos_live_through_setjumps);\n+  sparseset_free (pseudos_live);\n+  compress_live_ranges ();\n+  timevar_pop (TV_LRA_CREATE_LIVE_RANGES);\n+}\n+\n+/* Finish all live ranges.  */\n+void\n+lra_clear_live_ranges (void)\n+{\n+  int i;\n+\n+  for (i = 0; i < max_reg_num (); i++)\n+    free_live_range_list (lra_reg_info[i].live_ranges);\n+  VEC_free (int, heap, point_freq_vec);\n+}\n+\n+/* Initialize live ranges data once per function.  */\n+void\n+lra_live_ranges_init (void)\n+{\n+  live_range_pool = create_alloc_pool (\"live ranges\",\n+\t\t\t\t       sizeof (struct lra_live_range), 100);\n+}\n+\n+/* Finish live ranges data once per function.  */\n+void\n+lra_live_ranges_finish (void)\n+{\n+  free_alloc_pool (live_range_pool);\n+}"}, {"sha": "ecc1de4a4d9bc02b237112b71313d589b133b2e6", "filename": "gcc/lra-spills.c", "status": "added", "additions": 611, "deletions": 0, "changes": 611, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-spills.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra-spills.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flra-spills.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -0,0 +1,611 @@\n+/* Change pseudos by memory.\n+   Copyright (C) 2010, 2011, 2012\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.\tIf not see\n+<http://www.gnu.org/licenses/>.\t */\n+\n+\n+/* This file contains code for a pass to change spilled pseudos into\n+   memory.\n+\n+   The pass creates necessary stack slots and assigns spilled pseudos\n+   to the stack slots in following way:\n+\n+   for all spilled pseudos P most frequently used first do\n+     for all stack slots S do\n+       if P doesn't conflict with pseudos assigned to S then\n+\t assign S to P and goto to the next pseudo process\n+       end\n+     end\n+     create new stack slot S and assign P to S\n+   end\n+ \n+   The actual algorithm is bit more complicated because of different\n+   pseudo sizes.\n+\n+   After that the code changes spilled pseudos (except ones created\n+   from scratches) by corresponding stack slot memory in RTL.\n+\n+   If at least one stack slot was created, we need to run more passes\n+   because we have new addresses which should be checked and because\n+   the old address displacements might change and address constraints\n+   (or insn memory constraints) might not be satisfied any more.\n+\n+   For some targets, the pass can spill some pseudos into hard\n+   registers of different class (usually into vector registers)\n+   instead of spilling them into memory if it is possible and\n+   profitable.  Spilling GENERAL_REGS pseudo into SSE registers for\n+   Intel Corei7 is an example of such optimization.  And this is\n+   actually recommended by Intel optimization guide.\n+\n+   The file also contains code for final change of pseudos on hard\n+   regs correspondingly assigned to them.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"output.h\"\n+#include \"regs.h\"\n+#include \"hard-reg-set.h\"\n+#include \"flags.h\"\n+#include \"function.h\"\n+#include \"expr.h\"\n+#include \"basic-block.h\"\n+#include \"except.h\"\n+#include \"timevar.h\"\n+#include \"target.h\"\n+#include \"lra-int.h\"\n+#include \"ira.h\"\n+#include \"df.h\"\n+\n+\n+/* Max regno at the start of the pass.\t*/\n+static int regs_num;\n+\n+/* Map spilled regno -> hard regno used instead of memory for\n+   spilling.  */\n+static rtx *spill_hard_reg;\n+\n+/* The structure describes stack slot of a spilled pseudo.  */\n+struct pseudo_slot\n+{\n+  /* Number (0, 1, ...) of the stack slot to which given pseudo\n+     belongs.  */\n+  int slot_num;\n+  /* First or next slot with the same slot number.  */\n+  struct pseudo_slot *next, *first;\n+  /* Memory representing the spilled pseudo.  */\n+  rtx mem;\n+};\n+\n+/* The stack slots for each spilled pseudo.  Indexed by regnos.\t */\n+static struct pseudo_slot *pseudo_slots;\n+\n+/* The structure describes a register or a stack slot which can be\n+   used for several spilled pseudos.  */\n+struct slot\n+{\n+  /* First pseudo with given stack slot.  */\n+  int regno;\n+  /* Hard reg into which the slot pseudos are spilled.\tThe value is\n+     negative for pseudos spilled into memory.\t*/\n+  int hard_regno;\n+  /* Memory representing the all stack slot.  It can be different from\n+     memory representing a pseudo belonging to give stack slot because\n+     pseudo can be placed in a part of the corresponding stack slot.\n+     The value is NULL for pseudos spilled into a hard reg.  */\n+  rtx mem;\n+  /* Combined live ranges of all pseudos belonging to given slot.  It\n+     is used to figure out that a new spilled pseudo can use given\n+     stack slot.  */\n+  lra_live_range_t live_ranges;\n+};\n+\n+/* Array containing info about the stack slots.\t The array element is\n+   indexed by the stack slot number in the range [0..slots_num).  */\n+static struct slot *slots;\n+/* The number of the stack slots currently existing.  */\n+static int slots_num;\n+\n+/* Set up memory of the spilled pseudo I.  The function can allocate\n+   the corresponding stack slot if it is not done yet.\t*/\n+static void\n+assign_mem_slot (int i)\n+{\n+  rtx x = NULL_RTX;\n+  enum machine_mode mode = GET_MODE (regno_reg_rtx[i]);\n+  unsigned int inherent_size = PSEUDO_REGNO_BYTES (i);\n+  unsigned int inherent_align = GET_MODE_ALIGNMENT (mode);\n+  unsigned int max_ref_width = GET_MODE_SIZE (lra_reg_info[i].biggest_mode);\n+  unsigned int total_size = MAX (inherent_size, max_ref_width);\n+  unsigned int min_align = max_ref_width * BITS_PER_UNIT;\n+  int adjust = 0;\n+\n+  lra_assert (regno_reg_rtx[i] != NULL_RTX && REG_P (regno_reg_rtx[i])\n+\t      && lra_reg_info[i].nrefs != 0 && reg_renumber[i] < 0);\n+  \n+  x = slots[pseudo_slots[i].slot_num].mem;\n+  \n+  /* We can use a slot already allocated because it is guaranteed the\n+     slot provides both enough inherent space and enough total\n+     space.  */\n+  if (x)\n+    ;\n+  /* Each pseudo has an inherent size which comes from its own mode,\n+     and a total size which provides room for paradoxical subregs\n+     which refer to the pseudo reg in wider modes.  We allocate a new\n+     slot, making sure that it has enough inherent space and total\n+     space.  */\n+  else\n+    {\n+      rtx stack_slot;\n+\n+      /* No known place to spill from => no slot to reuse.  */\n+      x = assign_stack_local (mode, total_size,\n+\t\t\t      min_align > inherent_align\n+\t\t\t      || total_size > inherent_size ? -1 : 0);\n+      x = lra_eliminate_regs_1 (x, GET_MODE (x), false, false, true);\n+      stack_slot = x;\n+      /* Cancel the big-endian correction done in assign_stack_local.\n+\t Get the address of the beginning of the slot.\tThis is so we\n+\t can do a big-endian correction unconditionally below.\t*/\n+      if (BYTES_BIG_ENDIAN)\n+\t{\n+\t  adjust = inherent_size - total_size;\n+\t  if (adjust)\n+\t    stack_slot\n+\t      = adjust_address_nv (x,\n+\t\t\t\t   mode_for_size (total_size * BITS_PER_UNIT,\n+\t\t\t\t\t\t  MODE_INT, 1),\n+\t\t\t\t   adjust);\n+\t}\n+      slots[pseudo_slots[i].slot_num].mem = stack_slot;\n+    }\n+      \n+  /* On a big endian machine, the \"address\" of the slot is the address\n+     of the low part that fits its inherent mode.  */\n+  if (BYTES_BIG_ENDIAN && inherent_size < total_size)\n+    adjust += (total_size - inherent_size);\n+  \n+  x = adjust_address_nv (x, GET_MODE (regno_reg_rtx[i]), adjust);\n+  \n+  /* Set all of the memory attributes as appropriate for a spill.  */\n+  set_mem_attrs_for_spill (x);\n+  pseudo_slots[i].mem = x;\n+}\n+\n+/* Sort pseudos according their usage frequencies.  */\n+static int\n+regno_freq_compare (const void *v1p, const void *v2p)\n+{\n+  const int regno1 = *(const int *) v1p;\n+  const int regno2 = *(const int *) v2p;\n+  int diff;\n+\n+  if ((diff = lra_reg_info[regno2].freq - lra_reg_info[regno1].freq) != 0)\n+    return diff;\n+  return regno1 - regno2;\n+}\n+\n+/* Redefine STACK_GROWS_DOWNWARD in terms of 0 or 1.  */\n+#ifdef STACK_GROWS_DOWNWARD\n+# undef STACK_GROWS_DOWNWARD\n+# define STACK_GROWS_DOWNWARD 1\n+#else\n+# define STACK_GROWS_DOWNWARD 0\n+#endif\n+\n+/* Sort pseudos according to their slots, putting the slots in the order\n+   that they should be allocated.  Slots with lower numbers have the highest\n+   priority and should get the smallest displacement from the stack or\n+   frame pointer (whichever is being used).\n+\n+   The first allocated slot is always closest to the frame pointer,\n+   so prefer lower slot numbers when frame_pointer_needed.  If the stack\n+   and frame grow in the same direction, then the first allocated slot is\n+   always closest to the initial stack pointer and furthest away from the\n+   final stack pointer, so allocate higher numbers first when using the\n+   stack pointer in that case.  The reverse is true if the stack and\n+   frame grow in opposite directions.  */\n+static int\n+pseudo_reg_slot_compare (const void *v1p, const void *v2p)\n+{\n+  const int regno1 = *(const int *) v1p;\n+  const int regno2 = *(const int *) v2p;\n+  int diff, slot_num1, slot_num2;\n+  int total_size1, total_size2;\n+\n+  slot_num1 = pseudo_slots[regno1].slot_num;\n+  slot_num2 = pseudo_slots[regno2].slot_num;\n+  if ((diff = slot_num1 - slot_num2) != 0)\n+    return (frame_pointer_needed\n+\t    || !FRAME_GROWS_DOWNWARD == STACK_GROWS_DOWNWARD ? diff : -diff);\n+  total_size1 = GET_MODE_SIZE (lra_reg_info[regno1].biggest_mode);\n+  total_size2 = GET_MODE_SIZE (lra_reg_info[regno2].biggest_mode);\n+  if ((diff = total_size2 - total_size1) != 0)\n+    return diff;\n+  return regno1 - regno2;\n+}\n+\n+/* Assign spill hard registers to N pseudos in PSEUDO_REGNOS which is\n+   sorted in order of highest frequency first.  Put the pseudos which\n+   did not get a spill hard register at the beginning of array\n+   PSEUDO_REGNOS.  Return the number of such pseudos.  */\n+static int\n+assign_spill_hard_regs (int *pseudo_regnos, int n)\n+{\n+  int i, k, p, regno, res, spill_class_size, hard_regno, nr;\n+  enum reg_class rclass, spill_class;\n+  enum machine_mode mode;\n+  lra_live_range_t r;\n+  rtx insn, set;\n+  basic_block bb;\n+  HARD_REG_SET conflict_hard_regs;\n+  bitmap_head ok_insn_bitmap;\n+  bitmap setjump_crosses = regstat_get_setjmp_crosses ();\n+  /* Hard registers which can not be used for any purpose at given\n+     program point because they are unallocatable or already allocated\n+     for other pseudos.\t */ \n+  HARD_REG_SET *reserved_hard_regs;\n+\n+  if (! lra_reg_spill_p)\n+    return n;\n+  /* Set up reserved hard regs for every program point.\t */\n+  reserved_hard_regs = XNEWVEC (HARD_REG_SET, lra_live_max_point);\n+  for (p = 0; p < lra_live_max_point; p++)\n+    COPY_HARD_REG_SET (reserved_hard_regs[p], lra_no_alloc_regs);\n+  for (i = FIRST_PSEUDO_REGISTER; i < regs_num; i++)\n+    if (lra_reg_info[i].nrefs != 0\n+\t&& (hard_regno = lra_get_regno_hard_regno (i)) >= 0)\n+      for (r = lra_reg_info[i].live_ranges; r != NULL; r = r->next)\n+\tfor (p = r->start; p <= r->finish; p++)\n+\t  add_to_hard_reg_set (&reserved_hard_regs[p],\n+\t\t\t       lra_reg_info[i].biggest_mode, hard_regno);\n+  bitmap_initialize (&ok_insn_bitmap, &reg_obstack);\n+  FOR_EACH_BB (bb)\n+    FOR_BB_INSNS (bb, insn)\n+      if (DEBUG_INSN_P (insn)\n+\t  || ((set = single_set (insn)) != NULL_RTX\n+\t      && REG_P (SET_SRC (set)) && REG_P (SET_DEST (set))))\n+\tbitmap_set_bit (&ok_insn_bitmap, INSN_UID (insn));\n+  for (res = i = 0; i < n; i++)\n+    {\n+      regno = pseudo_regnos[i];\n+      rclass = lra_get_allocno_class (regno);\n+      if (bitmap_bit_p (setjump_crosses, regno)\n+\t  || (spill_class\n+\t      = ((enum reg_class)\n+\t\t targetm.spill_class ((reg_class_t) rclass,\n+\t\t\t\t      PSEUDO_REGNO_MODE (regno)))) == NO_REGS\n+\t  || bitmap_intersect_compl_p (&lra_reg_info[regno].insn_bitmap,\n+\t\t\t\t       &ok_insn_bitmap))\n+\t{\n+\t  pseudo_regnos[res++] = regno;\n+\t  continue;\n+\t}\n+      lra_assert (spill_class != NO_REGS);\n+      COPY_HARD_REG_SET (conflict_hard_regs,\n+\t\t\t lra_reg_info[regno].conflict_hard_regs);\n+      for (r = lra_reg_info[regno].live_ranges; r != NULL; r = r->next)\n+\tfor (p = r->start; p <= r->finish; p++)\n+\t  IOR_HARD_REG_SET (conflict_hard_regs, reserved_hard_regs[p]);\n+      spill_class_size = ira_class_hard_regs_num[spill_class];\n+      mode = lra_reg_info[regno].biggest_mode;\n+      for (k = 0; k < spill_class_size; k++)\n+\t{\n+\t  hard_regno = ira_class_hard_regs[spill_class][k];\n+\t  if (! overlaps_hard_reg_set_p (conflict_hard_regs, mode, hard_regno))\n+\t    break;\n+\t}\n+      if (k >= spill_class_size)\n+\t{\n+\t   /* There is no available regs -- assign memory later.  */\n+\t  pseudo_regnos[res++] = regno;\n+\t  continue;\n+\t}\n+      if (lra_dump_file != NULL)\n+\tfprintf (lra_dump_file, \"  Spill r%d into hr%d\\n\", regno, hard_regno);\n+      /* Update reserved_hard_regs.  */\n+      for (r = lra_reg_info[regno].live_ranges; r != NULL; r = r->next)\n+\tfor (p = r->start; p <= r->finish; p++)\n+\t  add_to_hard_reg_set (&reserved_hard_regs[p],\n+\t\t\t       lra_reg_info[regno].biggest_mode, hard_regno);\n+      spill_hard_reg[regno]\n+\t= gen_raw_REG (PSEUDO_REGNO_MODE (regno), hard_regno);\n+      for (nr = 0;\n+\t   nr < hard_regno_nregs[hard_regno][lra_reg_info[regno].biggest_mode];\n+\t   nr++)\n+\t/* Just loop.  */;\n+      df_set_regs_ever_live (hard_regno + nr, true);\n+    }\n+  bitmap_clear (&ok_insn_bitmap);\n+  free (reserved_hard_regs);\n+  return res;\n+}\n+\n+/* Add pseudo REGNO to slot SLOT_NUM.  */\n+static void\n+add_pseudo_to_slot (int regno, int slot_num)\n+{\n+  struct pseudo_slot *first;\n+\n+  if (slots[slot_num].regno < 0)\n+    {\n+      /* It is the first pseudo in the slot.  */\n+      slots[slot_num].regno = regno;\n+      pseudo_slots[regno].first = &pseudo_slots[regno];\n+      pseudo_slots[regno].next = NULL;\n+    }\n+  else\n+    {\n+      first = pseudo_slots[regno].first = &pseudo_slots[slots[slot_num].regno];\n+      pseudo_slots[regno].next = first->next;\n+      first->next = &pseudo_slots[regno];\n+    }\n+  pseudo_slots[regno].mem = NULL_RTX;\n+  pseudo_slots[regno].slot_num = slot_num;\n+  slots[slot_num].live_ranges\n+    = lra_merge_live_ranges (slots[slot_num].live_ranges,\n+\t\t\t     lra_copy_live_range_list\n+\t\t\t     (lra_reg_info[regno].live_ranges));\n+}\n+\n+/* Assign stack slot numbers to pseudos in array PSEUDO_REGNOS of\n+   length N.  Sort pseudos in PSEUDO_REGNOS for subsequent assigning\n+   memory stack slots.\t*/\n+static void\n+assign_stack_slot_num_and_sort_pseudos (int *pseudo_regnos, int n)\n+{\n+  int i, j, regno;\n+\n+  slots_num = 0;\n+  /* Assign stack slot numbers to spilled pseudos, use smaller numbers\n+     for most frequently used pseudos.\t*/\n+  for (i = 0; i < n; i++)\n+    {\n+      regno = pseudo_regnos[i];\n+      if (! flag_ira_share_spill_slots)\n+\tj = slots_num;\n+      else\n+\t{\n+\t  for (j = 0; j < slots_num; j++)\n+\t    if (slots[j].hard_regno < 0\n+\t\t&& ! (lra_intersected_live_ranges_p\n+\t\t      (slots[j].live_ranges,\n+\t\t       lra_reg_info[regno].live_ranges)))\n+\t      break;\n+\t}\n+      if (j >= slots_num)\n+\t{\n+\t  /* New slot.\t*/\n+\t  slots[j].live_ranges = NULL;\n+\t  slots[j].regno = slots[j].hard_regno = -1;\n+\t  slots[j].mem = NULL_RTX;\n+\t  slots_num++;\n+\t}\n+      add_pseudo_to_slot (regno, j);\n+    }\n+  /* Sort regnos according to their slot numbers.  */\n+  qsort (pseudo_regnos, n, sizeof (int), pseudo_reg_slot_compare);\n+}\n+\n+/* Recursively process LOC in INSN and change spilled pseudos to the\n+   corresponding memory or spilled hard reg.  Ignore spilled pseudos\n+   created from the scratches.\t*/\n+static void\n+remove_pseudos (rtx *loc, rtx insn)\n+{\n+  int i;\n+  rtx hard_reg;\n+  const char *fmt;\n+  enum rtx_code code;\n+\n+  if (*loc == NULL_RTX)\n+    return;\n+  code = GET_CODE (*loc);\n+  if (code == REG && (i = REGNO (*loc)) >= FIRST_PSEUDO_REGISTER\n+      && lra_get_regno_hard_regno (i) < 0\n+      /* We do not want to assign memory for former scratches because\n+\t it might result in an address reload for some targets.\t In\n+\t any case we transform such pseudos not getting hard registers\n+\t into scratches back.  */\n+      && ! lra_former_scratch_p (i))\n+    {\n+      hard_reg = spill_hard_reg[i];\n+      *loc = copy_rtx (hard_reg != NULL_RTX ? hard_reg : pseudo_slots[i].mem);\n+      return;\n+    }\n+\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\tremove_pseudos (&XEXP (*loc, i), insn);\n+      else if (fmt[i] == 'E')\n+\t{\n+\t  int j;\n+\n+\t  for (j = XVECLEN (*loc, i) - 1; j >= 0; j--)\n+\t    remove_pseudos (&XVECEXP (*loc, i, j), insn);\n+\t}\n+    }\n+}\n+\n+/* Convert spilled pseudos into their stack slots or spill hard regs,\n+   put insns to process on the constraint stack (that is all insns in\n+   which pseudos were changed to memory or spill hard regs).   */\n+static void\n+spill_pseudos (void)\n+{\n+  basic_block bb;\n+  rtx insn;\n+  int i;\n+  bitmap_head spilled_pseudos, changed_insns;\n+\n+  bitmap_initialize (&spilled_pseudos, &reg_obstack);\n+  bitmap_initialize (&changed_insns, &reg_obstack);\n+  for (i = FIRST_PSEUDO_REGISTER; i < regs_num; i++)\n+    {\n+      if (lra_reg_info[i].nrefs != 0 && lra_get_regno_hard_regno (i) < 0\n+\t  && ! lra_former_scratch_p (i))\n+\t{\n+\t  bitmap_set_bit (&spilled_pseudos, i);\n+\t  bitmap_ior_into (&changed_insns, &lra_reg_info[i].insn_bitmap);\n+\t}\n+    }\n+  FOR_EACH_BB (bb)\n+    {\n+      FOR_BB_INSNS (bb, insn)\n+\tif (bitmap_bit_p (&changed_insns, INSN_UID (insn)))\n+\t  {\n+\t    remove_pseudos (&PATTERN (insn), insn);\n+\t    if (CALL_P (insn))\n+\t      remove_pseudos (&CALL_INSN_FUNCTION_USAGE (insn), insn);\n+\t    if (lra_dump_file != NULL)\n+\t      fprintf (lra_dump_file,\n+\t\t       \"Changing spilled pseudos to memory in insn #%u\\n\",\n+\t\t       INSN_UID (insn));\n+\t    lra_push_insn (insn);\n+\t    if (lra_reg_spill_p || targetm.different_addr_displacement_p ())\n+\t      lra_set_used_insn_alternative (insn, -1);\n+\t  }\n+\telse if (CALL_P (insn))\n+\t  /* Presence of any pseudo in CALL_INSN_FUNCTION_USAGE does\n+\t     not affect value of insn_bitmap of the corresponding\n+\t     lra_reg_info.  That is because we don't need to reload\n+\t     pseudos in CALL_INSN_FUNCTION_USAGEs.  So if we process\n+\t     only insns in the insn_bitmap of given pseudo here, we\n+\t     can miss the pseudo in some\n+\t     CALL_INSN_FUNCTION_USAGEs.  */\n+\t  remove_pseudos (&CALL_INSN_FUNCTION_USAGE (insn), insn);\n+      bitmap_and_compl_into (df_get_live_in (bb), &spilled_pseudos);\n+      bitmap_and_compl_into (df_get_live_out (bb), &spilled_pseudos);\n+    }\n+  bitmap_clear (&spilled_pseudos);\n+  bitmap_clear (&changed_insns);\n+}\n+\n+/* Return true if we need to change some pseudos into memory.  */\n+bool\n+lra_need_for_spills_p (void)\n+{\n+  int i; max_regno = max_reg_num ();\n+\n+  for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+    if (lra_reg_info[i].nrefs != 0 && lra_get_regno_hard_regno (i) < 0\n+\t&& ! lra_former_scratch_p (i))\n+      return true;\n+  return false;\n+}\n+\n+/* Change spilled pseudos into memory or spill hard regs.  Put changed\n+   insns on the constraint stack (these insns will be considered on\n+   the next constraint pass).  The changed insns are all insns in\n+   which pseudos were changed.  */\n+void\n+lra_spill (void)\n+{\n+  int i, n, curr_regno;\n+  int *pseudo_regnos;\n+\n+  regs_num = max_reg_num ();\n+  spill_hard_reg = XNEWVEC (rtx, regs_num);\n+  pseudo_regnos = XNEWVEC (int, regs_num);\n+  for (n = 0, i = FIRST_PSEUDO_REGISTER; i < regs_num; i++)\n+    if (lra_reg_info[i].nrefs != 0 && lra_get_regno_hard_regno (i) < 0\n+\t/* We do not want to assign memory for former scratches.  */\n+\t&& ! lra_former_scratch_p (i))\n+      {\n+\tspill_hard_reg[i] = NULL_RTX;\n+\tpseudo_regnos[n++] = i;\n+      }\n+  lra_assert (n > 0);\n+  pseudo_slots = XNEWVEC (struct pseudo_slot, regs_num);\n+  slots = XNEWVEC (struct slot, regs_num);\n+  /* Sort regnos according their usage frequencies.  */\n+  qsort (pseudo_regnos, n, sizeof (int), regno_freq_compare);\n+  n = assign_spill_hard_regs (pseudo_regnos, n);\n+  assign_stack_slot_num_and_sort_pseudos (pseudo_regnos, n);\n+  for (i = 0; i < n; i++)\n+    if (pseudo_slots[pseudo_regnos[i]].mem == NULL_RTX)\n+      assign_mem_slot (pseudo_regnos[i]);\n+  if (lra_dump_file != NULL)\n+    {\n+      for (i = 0; i < slots_num; i++)\n+\t{\n+\t  fprintf (lra_dump_file, \"  Slot %d regnos (width = %d):\", i,\n+\t\t   GET_MODE_SIZE (GET_MODE (slots[i].mem)));\n+\t  for (curr_regno = slots[i].regno;;\n+\t       curr_regno = pseudo_slots[curr_regno].next - pseudo_slots)\n+\t    {\n+\t      fprintf (lra_dump_file, \"\t %d\", curr_regno);\n+\t      if (pseudo_slots[curr_regno].next == NULL)\n+\t\tbreak;\n+\t    }\n+\t  fprintf (lra_dump_file, \"\\n\");\n+\t}\n+    }\n+  spill_pseudos ();\n+  free (slots);\n+  free (pseudo_slots);\n+  free (pseudo_regnos);\n+}\n+\n+/* Final change of pseudos got hard registers into the corresponding\n+   hard registers.  */\n+void\n+lra_hard_reg_substitution (void)\n+{\n+  int i, hard_regno;\n+  basic_block bb;\n+  rtx insn;\n+  int max_regno = max_reg_num ();\n+\n+  for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+    if (lra_reg_info[i].nrefs != 0\n+\t&& (hard_regno = lra_get_regno_hard_regno (i)) >= 0)\n+      SET_REGNO (regno_reg_rtx[i], hard_regno);\n+  FOR_EACH_BB (bb)\n+    FOR_BB_INSNS (bb, insn)\n+      if (INSN_P (insn))\n+\t{\n+\t  lra_insn_recog_data_t id;\n+\t  bool insn_change_p = false;\n+\n+\t  id = lra_get_insn_recog_data (insn);\n+\t  for (i = id->insn_static_data->n_operands - 1; i >= 0; i--)\n+\t    {\n+\t      rtx op = *id->operand_loc[i];\n+\n+\t      if (GET_CODE (op) == SUBREG && REG_P (SUBREG_REG (op)))\n+\t\t{\n+\t\t  lra_assert (REGNO (SUBREG_REG (op)) < FIRST_PSEUDO_REGISTER);\n+\t\t  alter_subreg (id->operand_loc[i], ! DEBUG_INSN_P (insn));\n+\t\t  lra_update_dup (id, i);\n+\t\t  insn_change_p = true;\n+\t\t}\n+\t    }\n+\t  if (insn_change_p)\n+\t    lra_update_operator_dups (id);\n+\t}\n+}"}, {"sha": "1897e8593cf7287d7f0344310a1c9fe6f28053b2", "filename": "gcc/lra.c", "status": "added", "additions": 2398, "deletions": 0, "changes": 2398, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flra.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -0,0 +1,2398 @@\n+/* LRA (local register allocator) driver and LRA utilities.\n+   Copyright (C) 2010, 2011, 2012\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.\tIf not see\n+<http://www.gnu.org/licenses/>.\t */\n+\n+\n+/* The Local Register Allocator (LRA) is a replacement of former\n+   reload pass.\t It is focused to simplify code solving the reload\n+   pass tasks, to make the code maintenance easier, and to implement new\n+   perspective optimizations.\n+\n+   The major LRA design solutions are:\n+     o division small manageable, separated sub-tasks\n+     o reflection of all transformations and decisions in RTL as more\n+       as possible\n+     o insn constraints as a primary source of the info (minimizing\n+       number of target-depended macros/hooks)\n+\n+   In brief LRA works by iterative insn process with the final goal is\n+   to satisfy all insn and address constraints:\n+     o New reload insns (in brief reloads) and reload pseudos might be\n+       generated;\n+     o Some pseudos might be spilled to assign hard registers to\n+       new reload pseudos;\n+     o Changing spilled pseudos to stack memory or their equivalences;\n+     o Allocation stack memory changes the address displacement and\n+       new iteration is needed.\n+\n+   Here is block diagram of LRA passes:\n+\n+\t  ---------------------\t\t\t\t    \n+\t | Undo inheritance    |      ---------------\t     --------------- \n+\t | for spilled pseudos)|     | Memory-memory |\t    | New (and old) |\n+\t | and splits (for     |<----| move coalesce |<-----|\t pseudos    |\n+\t | pseudos got the     |      ---------------\t    |\tassignment  |\n+  Start\t |  same  hard regs)   |\t\t\t     --------------- \n+    |\t  ---------------------\t\t\t\t\t    ^\n+    V\t\t  |\t\t ----------------\t\t    |\n+ -----------\t  V\t\t| Update virtual |\t\t    |\n+|  Remove   |----> ------------>|    register\t |\t\t    |\n+| scratches |\t  ^\t\t|  displacements |\t\t    |\n+ -----------\t  |\t\t ----------------\t\t    |\n+\t\t  |\t\t\t |\t\t\t    |\n+\t\t  |\t\t\t V\t   New\t\t    |\n+\t ----------------    No\t   ------------\t pseudos   -------------------\n+\t| Spilled pseudo | change |Constraints:| or insns | Inheritance/split |\n+\t|    to memory\t |<-------|    RTL     |--------->|  transformations  |\n+\t|  substitution\t |\t  | transfor-  |\t  |    in EBB scope   |\n+\t ----------------\t  |  mations   |\t   -------------------\n+\t\t|\t\t    ------------ \n+\t\tV\n+    -------------------------\n+   | Hard regs substitution, |\n+   |  devirtalization, and   |------> Finish\n+   | restoring scratches got |\n+   |\t     memory\t     |\n+    -------------------------\n+\n+   To speed up the process:\n+     o We process only insns affected by changes on previous\n+       iterations;\n+     o We don't use DFA-infrastructure because it results in much slower\n+       compiler speed than a special IR described below does;\n+     o We use a special insn representation for quick access to insn\n+       info which is always *synchronized* with the current RTL;\n+       o Insn IR is minimized by memory.  It is divided on three parts:\n+\t o one specific for each insn in RTL (only operand locations);\n+\t o one common for all insns in RTL with the same insn code\n+\t   (different operand attributes from machine descriptions);\n+\t o one oriented for maintenance of live info (list of pseudos).\n+       o Pseudo data:\n+\t o all insns where the pseudo is referenced;\n+\t o live info (conflicting hard regs, live ranges, # of\n+\t   references etc);\n+\t o data used for assigning (preferred hard regs, costs etc).\n+\n+   This file contains LRA driver, LRA utility functions and data, and\n+   code for dealing with scratches.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"regs.h\"\n+#include \"insn-config.h\"\n+#include \"insn-codes.h\"\n+#include \"recog.h\"\n+#include \"output.h\"\n+#include \"addresses.h\"\n+#include \"hard-reg-set.h\"\n+#include \"flags.h\"\n+#include \"function.h\"\n+#include \"expr.h\"\n+#include \"basic-block.h\"\n+#include \"except.h\"\n+#include \"tree-pass.h\"\n+#include \"timevar.h\"\n+#include \"target.h\"\n+#include \"vec.h\"\n+#include \"ira.h\"\n+#include \"lra-int.h\"\n+#include \"df.h\"\n+\n+/* Hard registers currently not available for allocation.  It can\n+   changed after some hard  registers become not eliminable.  */\n+HARD_REG_SET lra_no_alloc_regs;\n+\n+static int get_new_reg_value (void);\n+static void expand_reg_info (void);\n+static void invalidate_insn_recog_data (int);\n+static int get_insn_freq (rtx);\n+static void invalidate_insn_data_regno_info (lra_insn_recog_data_t, rtx, int);\n+\n+/* Expand all regno related info needed for LRA.  */\n+static void\n+expand_reg_data (void)\n+{\n+  resize_reg_info ();\n+  expand_reg_info ();\n+  ira_expand_reg_equiv ();\n+}\n+\n+/* Create and return a new reg of ORIGINAL mode.  If ORIGINAL is NULL\n+   or of VOIDmode, use MD_MODE for the new reg.  Initialize its\n+   register class to RCLASS.  Print message about assigning class\n+   RCLASS containing new register name TITLE unless it is NULL.  Use\n+   attributes of ORIGINAL if it is a register.  The created register\n+   will have unique held value.  */\n+rtx\n+lra_create_new_reg_with_unique_value (enum machine_mode md_mode, rtx original,\n+\t\t\t\t      enum reg_class rclass, const char *title)\n+{\n+  enum machine_mode mode;\n+  rtx new_reg;\n+\n+  if (original == NULL_RTX || (mode = GET_MODE (original)) == VOIDmode)\n+    mode = md_mode;\n+  lra_assert (mode != VOIDmode);\n+  new_reg = gen_reg_rtx (mode);\n+  if (original == NULL_RTX || ! REG_P (original))\n+    {\n+      if (lra_dump_file != NULL)\n+\tfprintf (lra_dump_file, \"      Creating newreg=%i\", REGNO (new_reg));\n+    }\n+  else\n+    {\n+      if (ORIGINAL_REGNO (original) >= FIRST_PSEUDO_REGISTER)\n+\tORIGINAL_REGNO (new_reg) = ORIGINAL_REGNO (original);\n+      REG_USERVAR_P (new_reg) = REG_USERVAR_P (original);\n+      REG_POINTER (new_reg) = REG_POINTER (original);\n+      REG_ATTRS (new_reg) = REG_ATTRS (original);\n+      if (lra_dump_file != NULL)\n+\tfprintf (lra_dump_file, \"      Creating newreg=%i from oldreg=%i\",\n+\t\t REGNO (new_reg), REGNO (original));\n+    }\n+  if (lra_dump_file != NULL)\n+    {\n+      if (title != NULL)\n+\tfprintf (lra_dump_file, \", assigning class %s to%s%s r%d\",\n+\t\t reg_class_names[rclass], *title == '\\0' ? \"\" : \" \",\n+\t\t title, REGNO (new_reg));\n+      fprintf (lra_dump_file, \"\\n\");\n+    }\n+  expand_reg_data ();\n+  setup_reg_classes (REGNO (new_reg), rclass, NO_REGS, rclass);\n+  return new_reg;\n+}\n+\n+/* Analogous to the previous function but also inherits value of\n+   ORIGINAL.  */\n+rtx\n+lra_create_new_reg (enum machine_mode md_mode, rtx original,\n+\t\t    enum reg_class rclass, const char *title)\n+{\n+  rtx new_reg;\n+\n+  new_reg\n+    = lra_create_new_reg_with_unique_value (md_mode, original, rclass, title);\n+  if (original != NULL_RTX && REG_P (original))\n+    lra_reg_info[REGNO (new_reg)].val = lra_reg_info[REGNO (original)].val;\n+  return new_reg;\n+}\n+\n+/* Set up for REGNO unique hold value.\t*/\n+void\n+lra_set_regno_unique_value (int regno)\n+{\n+  lra_reg_info[regno].val = get_new_reg_value ();\n+}\n+\n+/* Invalidate INSN related info used by LRA.  */\n+void\n+lra_invalidate_insn_data (rtx insn)\n+{\n+  lra_invalidate_insn_regno_info (insn);\n+  invalidate_insn_recog_data (INSN_UID (insn));\n+}\n+\n+/* Mark INSN deleted and invalidate the insn related info used by\n+   LRA.\t */\n+void\n+lra_set_insn_deleted (rtx insn)\n+{\n+  lra_invalidate_insn_data (insn);\n+  SET_INSN_DELETED (insn);\n+}\n+\n+/* Delete an unneeded INSN and any previous insns who sole purpose is\n+   loading data that is dead in INSN.  */\n+void\n+lra_delete_dead_insn (rtx insn)\n+{\n+  rtx prev = prev_real_insn (insn);\n+  rtx prev_dest;\n+\n+  /* If the previous insn sets a register that dies in our insn,\n+     delete it too.  */\n+  if (prev && GET_CODE (PATTERN (prev)) == SET\n+      && (prev_dest = SET_DEST (PATTERN (prev)), REG_P (prev_dest))\n+      && reg_mentioned_p (prev_dest, PATTERN (insn))\n+      && find_regno_note (insn, REG_DEAD, REGNO (prev_dest))\n+      && ! side_effects_p (SET_SRC (PATTERN (prev))))\n+    lra_delete_dead_insn (prev);\n+\n+  lra_set_insn_deleted (insn);\n+}\n+\n+/* Target checks operands through operand predicates to recognize an\n+   insn.  We should have a special precaution to generate add insns\n+   which are frequent results of elimination.\n+\n+   Emit insns for x = y + z.  X can be used to store intermediate\n+   values and should be not in Y and Z when we use X to store an\n+   intermediate value.  Y + Z should form [base] [+ index[ * scale]] [\n+   + disp] where base and index are registers, disp and scale are\n+   constants.  Y should contain base if it is present, Z should\n+   contain disp if any.  index[*scale] can be part of Y or Z.  */\n+void\n+lra_emit_add (rtx x, rtx y, rtx z)\n+{\n+  int old;\n+  rtx insn, last;\n+  rtx a1, a2, base, index, disp, scale, index_scale;\n+  bool ok_p;\n+\n+  insn = gen_add3_insn (x, y, z);\n+  old = max_reg_num ();\n+  if (insn != NULL_RTX)\n+    emit_insn (insn);\n+  else\n+    {\n+      disp = a2 = NULL_RTX;\n+      if (GET_CODE (y) == PLUS)\n+\t{\n+\t  a1 = XEXP (y, 0);\n+\t  a2 = XEXP (y, 1);\n+\t  disp = z;\n+\t}\n+      else\n+\t{\n+\t  a1 = y;\n+\t  if (CONSTANT_P (z))\n+\t    disp = z;\n+\t  else\n+\t    a2 = z;\n+\t}\n+      index_scale = scale = NULL_RTX;\n+      if (GET_CODE (a1) == MULT)\n+\t{\n+\t  index_scale = a1;\n+\t  index = XEXP (a1, 0);\n+\t  scale = XEXP (a1, 1);\n+\t  base = a2;\n+\t}\n+      else if (a2 != NULL_RTX && GET_CODE (a2) == MULT)\n+\t{\n+\t  index_scale = a2;\n+\t  index = XEXP (a2, 0);\n+\t  scale = XEXP (a2, 1);\n+\t  base = a1;\n+\t}\n+      else\n+\t{\n+\t  base = a1;\n+\t  index = a2;\n+\t}\n+      if (! REG_P (base)\n+\t  || (index != NULL_RTX && ! REG_P (index))\n+\t  || (disp != NULL_RTX && ! CONSTANT_P (disp))\n+\t  || (scale != NULL_RTX && ! CONSTANT_P (scale)))\n+\t{\n+\t  /* Its is not an address generation.\tProbably we have no 3 op\n+\t     add.  Last chance is to use 2-op add insn.\t */\n+\t  lra_assert (x != y && x != z);\n+\t  emit_move_insn (x, z);\n+\t  insn = gen_add2_insn (x, y);\n+\t  emit_insn (insn);\n+\t}\n+      else\n+\t{\n+\t  if (index_scale == NULL_RTX)\n+\t    index_scale = index;\n+\t  if (disp == NULL_RTX)\n+\t    {\n+\t      /* Generate x = index_scale; x = x + base.  */\n+\t      lra_assert (index_scale != NULL_RTX && base != NULL_RTX);\n+\t      emit_move_insn (x, index_scale);\n+\t      insn = gen_add2_insn (x, base);\n+\t      emit_insn (insn);\n+\t    }\n+\t  else if (scale == NULL_RTX)\n+\t    {\n+\t      /* Try x = base + disp.  */\n+\t      lra_assert (base != NULL_RTX);\n+\t      last = get_last_insn ();\n+\t      insn = emit_move_insn (x, gen_rtx_PLUS (GET_MODE (base),\n+\t\t\t\t\t\t      base, disp));\n+\t      if (recog_memoized (insn) < 0)\n+\t\t{\n+\t\t  delete_insns_since (last);\n+\t\t  /* Generate x = disp; x = x + base.  */\n+\t\t  emit_move_insn (x, disp);\n+\t\t  insn = gen_add2_insn (x, base);\n+\t\t  emit_insn (insn);\n+\t\t}\n+\t      /* Generate x = x + index.  */\n+\t      if (index != NULL_RTX)\n+\t\t{\n+\t\t  insn = gen_add2_insn (x, index);\n+\t\t  emit_insn (insn);\n+\t\t}\n+\t    }\n+\t  else\n+\t    {\n+\t      /* Try x = index_scale; x = x + disp; x = x + base.  */\n+\t      last = get_last_insn ();\n+\t      insn = emit_move_insn (x, index_scale);\n+\t      ok_p = false;\n+\t      if (recog_memoized (insn) >= 0)\n+\t\t{\n+\t\t  insn = gen_add2_insn (x, disp);\n+\t\t  if (insn != NULL_RTX)\n+\t\t    {\n+\t\t      emit_insn (insn);\n+\t\t      insn = gen_add2_insn (x, disp);\n+\t\t      if (insn != NULL_RTX)\n+\t\t\t{\n+\t\t\t  emit_insn (insn);\n+\t\t\t  ok_p = true;\n+\t\t\t}\n+\t\t    }\n+\t\t}\n+\t      if (! ok_p)\n+\t\t{\n+\t\t  delete_insns_since (last);\n+\t\t  /* Generate x = disp; x = x + base; x = x + index_scale.  */\n+\t\t  emit_move_insn (x, disp);\n+\t\t  insn = gen_add2_insn (x, base);\n+\t\t  emit_insn (insn);\n+\t\t  insn = gen_add2_insn (x, index_scale);\n+\t\t  emit_insn (insn);\n+\t\t}\n+\t    }\n+\t}\n+    }\n+  /* Functions emit_... can create pseudos -- so expand the pseudo\n+     data.  */\n+  if (old != max_reg_num ())\n+    expand_reg_data ();\n+}\n+\n+/* The number of emitted reload insns so far.  */\n+int lra_curr_reload_num;\n+\n+/* Emit x := y, processing special case when y = u + v or y = u + v *\n+   scale + w through emit_add (Y can be an address which is base +\n+   index reg * scale + displacement in general case).  X may be used\n+   as intermediate result therefore it should be not in Y.  */\n+void\n+lra_emit_move (rtx x, rtx y)\n+{\n+  int old;\n+\n+  if (GET_CODE (y) != PLUS)\n+    {\n+      if (rtx_equal_p (x, y))\n+\treturn;\n+      old = max_reg_num ();\n+      emit_move_insn (x, y);\n+      if (REG_P (x))\n+\tlra_reg_info[ORIGINAL_REGNO (x)].last_reload = ++lra_curr_reload_num;\n+      /* Function emit_move can create pseudos -- so expand the pseudo\n+\t data.\t*/\n+      if (old != max_reg_num ())\n+\texpand_reg_data ();\n+      return;\n+    }\n+  lra_emit_add (x, XEXP (y, 0), XEXP (y, 1));\n+}\n+\n+/* Update insn operands which are duplication of operands whose\n+   numbers are in array of NOPS (with end marker -1).  The insn is\n+   represented by its LRA internal representation ID.  */\n+void\n+lra_update_dups (lra_insn_recog_data_t id, signed char *nops)\n+{\n+  int i, j, nop;\n+  struct lra_static_insn_data *static_id = id->insn_static_data;\n+\n+  for (i = 0; i < static_id->n_dups; i++)\n+    for (j = 0; (nop = nops[j]) >= 0; j++)\n+      if (static_id->dup_num[i] == nop)\n+\t*id->dup_loc[i] = *id->operand_loc[nop];\n+}\n+\n+\f\n+\n+/* This page contains code dealing with info about registers in the\n+   insns.  */\n+\n+/* Pools for insn reg info.  */\n+static alloc_pool insn_reg_pool;\n+\n+/* Initiate pool for insn reg info.  */\n+static void\n+init_insn_regs (void)\n+{\n+  insn_reg_pool\n+    = create_alloc_pool (\"insn regs\", sizeof (struct lra_insn_reg), 100);\n+}\n+\n+/* Create LRA insn related info about referenced REGNO with TYPE\n+   (in/out/inout), biggest reference mode MODE, flag that it is\n+   reference through subreg (SUBREG_P), flag that is early clobbered\n+   in the insn (EARLY_CLOBBER), and reference to the next insn reg\n+   info (NEXT).\t */\n+static struct lra_insn_reg *\n+new_insn_reg (int regno, enum op_type type, enum machine_mode mode,\n+\t      bool subreg_p, bool early_clobber, struct lra_insn_reg *next)\n+{\n+  struct lra_insn_reg *ir;\n+\n+  ir = (struct lra_insn_reg *) pool_alloc (insn_reg_pool);\n+  ir->type = type;\n+  ir->biggest_mode = mode;\n+  ir->subreg_p = subreg_p;\n+  ir->early_clobber = early_clobber;\n+  ir->regno = regno;\n+  ir->next = next;\n+  return ir;\n+}\n+\n+/* Free insn reg info IR.  */\n+static void\n+free_insn_reg (struct lra_insn_reg *ir)\n+{\n+  pool_free (insn_reg_pool, ir);\n+}\n+\n+/* Free insn reg info list IR.\t*/\n+static void\n+free_insn_regs (struct lra_insn_reg *ir)\n+{\n+  struct lra_insn_reg *next_ir;\n+\n+  for (; ir != NULL; ir = next_ir)\n+    {\n+      next_ir = ir->next;\n+      free_insn_reg (ir);\n+    }\n+}\n+\n+/* Finish pool for insn reg info.  */\n+static void\n+finish_insn_regs (void)\n+{\n+  free_alloc_pool (insn_reg_pool);\n+}\n+\n+\f\n+\n+/* This page contains code dealing LRA insn info (or in other words\n+   LRA internal insn representation).  */\n+\n+struct target_lra_int default_target_lra_int;\n+#if SWITCHABLE_TARGET\n+struct target_lra_int *this_target_lra_int = &default_target_lra_int;\n+#endif\n+\n+/* Map INSN_CODE -> the static insn data.  This info is valid during\n+   all translation unit.  */\n+struct lra_static_insn_data *insn_code_data[LAST_INSN_CODE];\n+\n+/* Debug insns are represented as a special insn with one input\n+   operand which is RTL expression in var_location.  */\n+\n+/* The following data are used as static insn operand data for all\n+   debug insns.\t If structure lra_operand_data is changed, the\n+   initializer should be changed too.  */\n+static struct lra_operand_data debug_operand_data =\n+  {\n+    NULL, /* alternative  */\n+    VOIDmode, /* We are not interesting in the operand mode.  */\n+    OP_IN,\n+    0, 0, 0, 0\n+  };\n+\n+/* The following data are used as static insn data for all debug\n+   insns.  If structure lra_static_insn_data is changed, the\n+   initializer should be changed too.  */\n+static struct lra_static_insn_data debug_insn_static_data =\n+  {\n+    &debug_operand_data,\n+    0,\t/* Duplication operands #.  */\n+    -1, /* Commutative operand #.  */\n+    1,\t/* Operands #.\tThere is only one operand which is debug RTL\n+\t   expression.\t*/\n+    0,\t/* Duplications #.  */\n+    0,\t/* Alternatives #.  We are not interesting in alternatives\n+\t   because we does not proceed debug_insns for reloads.\t */\n+    NULL, /* Hard registers referenced in machine description.\t*/\n+    NULL  /* Descriptions of operands in alternatives.\t*/\n+  };\n+\n+/* Called once per compiler work to initialize some LRA data related\n+   to insns.  */\n+static void\n+init_insn_code_data_once (void)\n+{\n+  memset (insn_code_data, 0, sizeof (insn_code_data));\n+  memset (op_alt_data, 0, sizeof (op_alt_data));\n+}\n+\n+/* Called once per compiler work to finalize some LRA data related to\n+   insns.  */\n+static void\n+finish_insn_code_data_once (void)\n+{\n+  int i;\n+\n+  for (i = 0; i < LAST_INSN_CODE; i++)\n+    {\n+      if (insn_code_data[i] != NULL)\n+\tfree (insn_code_data[i]);\n+      if (op_alt_data[i] != NULL)\n+\tfree (op_alt_data[i]);\n+    }\n+}\n+\n+/* Initialize LRA info about operands in insn alternatives.  */\n+static void\n+init_op_alt_data (void)\n+{\n+ int i;\n+\n+  for (i = 0; i < LAST_INSN_CODE; i++)\n+    if (op_alt_data[i] != NULL)\n+      {\n+\tfree (op_alt_data[i]);\n+\top_alt_data[i] = NULL;\n+      }\n+}\n+\n+/* Return static insn data, allocate and setup if necessary.  Although\n+   dup_num is static data (it depends only on icode), to set it up we\n+   need to extract insn first.\tSo recog_data should be valid for\n+   normal insn (ICODE >= 0) before the call.  */\n+static struct lra_static_insn_data *\n+get_static_insn_data (int icode, int nop, int ndup, int nalt)\n+{\n+  struct lra_static_insn_data *data;\n+  size_t n_bytes;\n+\n+  lra_assert (icode < LAST_INSN_CODE);\n+  if (icode >= 0 && (data = insn_code_data[icode]) != NULL)\n+    return data;\n+  lra_assert (nop >= 0 && ndup >= 0 && nalt >= 0);\n+  n_bytes = sizeof (struct lra_static_insn_data)\n+\t    + sizeof (struct lra_operand_data) * nop\n+\t    + sizeof (int) * ndup;\n+  data = XNEWVAR (struct lra_static_insn_data, n_bytes);\n+  data->n_operands = nop;\n+  data->n_dups = ndup;\n+  data->n_alternatives = nalt;\n+  data->operand = ((struct lra_operand_data *)\n+\t\t   ((char *) data + sizeof (struct lra_static_insn_data)));\n+  data->dup_num = ((int *) ((char *) data->operand\n+\t\t\t    + sizeof (struct lra_operand_data) * nop));\n+  if (icode >= 0)\n+    {\n+      int i;\n+\n+      insn_code_data[icode] = data;\n+      for (i = 0; i < nop; i++)\n+\t{\n+\t  data->operand[i].constraint\n+\t    = insn_data[icode].operand[i].constraint;\n+\t  data->operand[i].mode = insn_data[icode].operand[i].mode;\n+\t  data->operand[i].strict_low = insn_data[icode].operand[i].strict_low;\n+\t  data->operand[i].is_operator\n+\t    = insn_data[icode].operand[i].is_operator;\n+\t  data->operand[i].type\n+\t    = (data->operand[i].constraint[0] == '=' ? OP_OUT\n+\t       : data->operand[i].constraint[0] == '+' ? OP_INOUT\n+\t       : OP_IN);\n+\t  data->operand[i].is_address = false;\n+\t}\n+      for (i = 0; i < ndup; i++)\n+\tdata->dup_num[i] = recog_data.dup_num[i];\n+    }\n+  return data;\n+}\n+\n+/* The current length of the following array.  */\n+int lra_insn_recog_data_len;\n+\n+/* Map INSN_UID -> the insn recog data (NULL if unknown).  */\n+lra_insn_recog_data_t *lra_insn_recog_data;\n+\n+/* Initialize LRA data about insns.  */\n+static void\n+init_insn_recog_data (void)\n+{\n+  lra_insn_recog_data_len = 0;\n+  lra_insn_recog_data = NULL;\n+  init_insn_regs ();\n+}\n+\n+/* Expand, if necessary, LRA data about insns.\t*/\n+static void\n+check_and_expand_insn_recog_data (int index)\n+{\n+  int i, old;\n+\n+  if (lra_insn_recog_data_len > index)\n+    return;\n+  old = lra_insn_recog_data_len;\n+  lra_insn_recog_data_len = index * 3 / 2 + 1;\n+  lra_insn_recog_data = XRESIZEVEC (lra_insn_recog_data_t,\n+\t\t\t\t    lra_insn_recog_data,\n+\t\t\t\t    lra_insn_recog_data_len);\n+  for (i = old; i < lra_insn_recog_data_len; i++)\n+    lra_insn_recog_data[i] = NULL;\n+}\n+\n+/* Finish LRA DATA about insn.\t*/\n+static void\n+free_insn_recog_data (lra_insn_recog_data_t data)\n+{\n+  if (data->operand_loc != NULL)\n+    free (data->operand_loc);\n+  if (data->dup_loc != NULL)\n+    free (data->dup_loc);\n+  if (data->arg_hard_regs != NULL)\n+    free (data->arg_hard_regs);\n+#ifdef HAVE_ATTR_enabled\n+  if (data->alternative_enabled_p != NULL)\n+    free (data->alternative_enabled_p);\n+#endif\n+  if (data->icode < 0 && NONDEBUG_INSN_P (data->insn))\n+    {\n+      if (data->insn_static_data->operand_alternative != NULL)\n+\tfree (data->insn_static_data->operand_alternative);\n+      free_insn_regs (data->insn_static_data->hard_regs);\n+      free (data->insn_static_data);\n+    }\n+  free_insn_regs (data->regs);\n+  data->regs = NULL;\n+  free (data);\n+}\n+\n+/* Finish LRA data about all insns.  */\n+static void\n+finish_insn_recog_data (void)\n+{\n+  int i;\n+  lra_insn_recog_data_t data;\n+\n+  for (i = 0; i < lra_insn_recog_data_len; i++)\n+    if ((data = lra_insn_recog_data[i]) != NULL)\n+      free_insn_recog_data (data);\n+  finish_insn_regs ();\n+  free (lra_insn_recog_data);\n+}\n+\n+/* Setup info about operands in alternatives of LRA DATA of insn.  */\n+static void\n+setup_operand_alternative (lra_insn_recog_data_t data)\n+{\n+  int i, nop, nalt;\n+  int icode = data->icode;\n+  struct lra_static_insn_data *static_data = data->insn_static_data;\n+\n+  if (icode >= 0\n+      && (static_data->operand_alternative = op_alt_data[icode]) != NULL)\n+    return;\n+  static_data->commutative = -1;\n+  nop = static_data->n_operands;\n+  if (nop == 0)\n+    {\n+      static_data->operand_alternative = NULL;\n+      return;\n+    }\n+  nalt = static_data->n_alternatives;\n+  static_data->operand_alternative = XNEWVEC (struct operand_alternative,\n+\t\t\t\t\t      nalt * nop);\n+  memset (static_data->operand_alternative, 0,\n+\t  nalt * nop * sizeof (struct operand_alternative));\n+  if (icode >= 0)\n+    op_alt_data[icode] = static_data->operand_alternative;\n+  for (i = 0; i < nop; i++)\n+    {\n+      int j;\n+      struct operand_alternative *op_alt_start, *op_alt;\n+      const char *p = static_data->operand[i].constraint;\n+\n+      static_data->operand[i].early_clobber = 0;\n+      op_alt_start = &static_data->operand_alternative[i];\n+\n+      for (j = 0; j < nalt; j++)\n+\t{\n+\t  op_alt = op_alt_start + j * nop;\n+\t  op_alt->cl = NO_REGS;\n+\t  op_alt->constraint = p;\n+\t  op_alt->matches = -1;\n+\t  op_alt->matched = -1;\n+\n+\t  if (*p == '\\0' || *p == ',')\n+\t    {\n+\t      op_alt->anything_ok = 1;\n+\t      continue;\n+\t    }\n+\n+\t  for (;;)\n+\t    {\n+\t      char c = *p;\n+\t      if (c == '#')\n+\t\tdo\n+\t\t  c = *++p;\n+\t\twhile (c != ',' && c != '\\0');\n+\t      if (c == ',' || c == '\\0')\n+\t\t{\n+\t\t  p++;\n+\t\t  break;\n+\t\t}\n+\n+\t      switch (c)\n+\t\t{\n+\t\tcase '=': case '+': case '*':\n+\t\tcase 'E': case 'F': case 'G': case 'H':\n+\t\tcase 's': case 'i': case 'n':\n+\t\tcase 'I': case 'J': case 'K': case 'L':\n+\t\tcase 'M': case 'N': case 'O': case 'P':\n+\t\t  /* These don't say anything we care about.  */\n+\t\t  break;\n+\n+\t\tcase '%':\n+\t\t  /* We currently only support one commutative pair of\n+\t\t     operands.\t*/\n+\t\t  if (static_data->commutative < 0)\n+\t\t    static_data->commutative = i;\n+\t\t  else\n+\t\t    lra_assert (data->icode < 0); /* Asm  */\n+\n+\t\t  /* The last operand should not be marked\n+\t\t     commutative.  */\n+\t\t  lra_assert (i != nop - 1);\n+\t\t  break;\n+\n+\t\tcase '?':\n+\t\t  op_alt->reject += 6;\n+\t\t  break;\n+\t\tcase '!':\n+\t\t  op_alt->reject += 600;\n+\t\t  break;\n+\t\tcase '&':\n+\t\t  op_alt->earlyclobber = 1;\n+\t\t  static_data->operand[i].early_clobber = 1;\n+\t\t  break;\n+\n+\t\tcase '0': case '1': case '2': case '3': case '4':\n+\t\tcase '5': case '6': case '7': case '8': case '9':\n+\t\t  {\n+\t\t    char *end;\n+\t\t    op_alt->matches = strtoul (p, &end, 10);\n+\t\t    static_data->operand_alternative\n+\t\t      [j * nop + op_alt->matches].matched = i;\n+\t\t    p = end;\n+\t\t  }\n+\t\t  continue;\n+\n+\t\tcase TARGET_MEM_CONSTRAINT:\n+\t\t  op_alt->memory_ok = 1;\n+\t\t  break;\n+\t\tcase '<':\n+\t\t  op_alt->decmem_ok = 1;\n+\t\t  break;\n+\t\tcase '>':\n+\t\t  op_alt->incmem_ok = 1;\n+\t\t  break;\n+\t\tcase 'V':\n+\t\t  op_alt->nonoffmem_ok = 1;\n+\t\t  break;\n+\t\tcase 'o':\n+\t\t  op_alt->offmem_ok = 1;\n+\t\t  break;\n+\t\tcase 'X':\n+\t\t  op_alt->anything_ok = 1;\n+\t\t  break;\n+\n+\t\tcase 'p':\n+\t\t  static_data->operand[i].is_address = true;\n+\t\t  op_alt->is_address = 1;\n+\t\t  op_alt->cl = (reg_class_subunion[(int) op_alt->cl]\n+\t\t\t\t[(int) base_reg_class (VOIDmode,\n+\t\t\t\t\t\t       ADDR_SPACE_GENERIC,\n+\t\t\t\t\t\t       ADDRESS, SCRATCH)]);\n+\t\t  break;\n+\n+\t\tcase 'g':\n+\t\tcase 'r':\n+\t\t  op_alt->cl =\n+\t\t   reg_class_subunion[(int) op_alt->cl][(int) GENERAL_REGS];\n+\t\t  break;\n+\n+\t\tdefault:\n+\t\t  if (EXTRA_MEMORY_CONSTRAINT (c, p))\n+\t\t    {\n+\t\t      op_alt->memory_ok = 1;\n+\t\t      break;\n+\t\t    }\n+\t\t  if (EXTRA_ADDRESS_CONSTRAINT (c, p))\n+\t\t    {\n+\t\t      static_data->operand[i].is_address = true;\n+\t\t      op_alt->is_address = 1;\n+\t\t      op_alt->cl\n+\t\t\t= (reg_class_subunion\n+\t\t\t   [(int) op_alt->cl]\n+\t\t\t   [(int) base_reg_class (VOIDmode, ADDR_SPACE_GENERIC,\n+\t\t\t\t\t\t  ADDRESS, SCRATCH)]);\n+\t\t      break;\n+\t\t    }\n+\n+\t\t  op_alt->cl\n+\t\t    = (reg_class_subunion\n+\t\t       [(int) op_alt->cl]\n+\t\t       [(int)\n+\t\t\tREG_CLASS_FROM_CONSTRAINT ((unsigned char) c, p)]);\n+\t\t  break;\n+\t\t}\n+\t      p += CONSTRAINT_LEN (c, p);\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Recursively process X and collect info about registers, which are\n+   not the insn operands, in X with TYPE (in/out/inout) and flag that\n+   it is early clobbered in the insn (EARLY_CLOBBER) and add the info\n+   to LIST.  X is a part of insn given by DATA.\t Return the result\n+   list.  */\n+static struct lra_insn_reg *\n+collect_non_operand_hard_regs (rtx *x, lra_insn_recog_data_t data,\n+\t\t\t       struct lra_insn_reg *list,\n+\t\t\t       enum op_type type, bool early_clobber)\n+{\n+  int i, j, regno, last;\n+  bool subreg_p;\n+  enum machine_mode mode;\n+  struct lra_insn_reg *curr;\n+  rtx op = *x;\n+  enum rtx_code code = GET_CODE (op);\n+  const char *fmt = GET_RTX_FORMAT (code);\n+\n+  for (i = 0; i < data->insn_static_data->n_operands; i++)\n+    if (x == data->operand_loc[i])\n+      /* It is an operand loc. Stop here.  */\n+      return list;\n+  for (i = 0; i < data->insn_static_data->n_dups; i++)\n+    if (x == data->dup_loc[i])\n+      /* It is a dup loc. Stop here.  */\n+      return list;\n+  mode = GET_MODE (op);\n+  subreg_p = false;\n+  if (code == SUBREG)\n+    {\n+      op = SUBREG_REG (op);\n+      code = GET_CODE (op);\n+      if (GET_MODE_SIZE (mode) < GET_MODE_SIZE (GET_MODE (op)))\n+\t{\n+\t  mode = GET_MODE (op);\n+\t  if (GET_MODE_SIZE (mode) > REGMODE_NATURAL_SIZE (mode))\n+\t    subreg_p = true;\n+\t}\n+    }\n+  if (REG_P (op))\n+    {\n+      if ((regno = REGNO (op)) >= FIRST_PSEUDO_REGISTER)\n+\treturn list;\n+      for (last = regno + hard_regno_nregs[regno][mode];\n+\t   regno < last;\n+\t   regno++)\n+\tif (! TEST_HARD_REG_BIT (lra_no_alloc_regs, regno))\n+\t  {\n+\t    for (curr = list; curr != NULL; curr = curr->next)\n+\t      if (curr->regno == regno && curr->subreg_p == subreg_p\n+\t\t  && curr->biggest_mode == mode)\n+\t\t{\n+\t\t  if (curr->type != type)\n+\t\t    curr->type = OP_INOUT;\n+\t\t  if (curr->early_clobber != early_clobber)\n+\t\t    curr->early_clobber = true;\n+\t\t  break;\n+\t\t}\n+\t    if (curr == NULL)\n+\t      {\n+\t\t/* This is a new hard regno or the info can not be\n+\t\t   integrated into the found structure.\t */\n+#ifdef STACK_REGS\n+\t\tearly_clobber\n+\t\t  = (early_clobber\n+\t\t     /* This clobber is to inform popping floating\n+\t\t\tpoint stack only.  */\n+\t\t     && ! (FIRST_STACK_REG <= regno\n+\t\t\t   && regno <= LAST_STACK_REG));\n+#endif\n+\t\tlist = new_insn_reg (regno, type, mode, subreg_p,\n+\t\t\t\t     early_clobber, list);\n+\t      }\n+\t  }\n+      return list;\n+    }\n+  switch (code)\n+    {\n+    case SET:\n+      list = collect_non_operand_hard_regs (&SET_DEST (op), data,\n+\t\t\t\t\t    list, OP_OUT, false);\n+      list = collect_non_operand_hard_regs (&SET_SRC (op), data,\n+\t\t\t\t\t    list, OP_IN, false);\n+      break;\n+    case CLOBBER:\n+      /* We treat clobber of non-operand hard registers as early\n+\t clobber (the behavior is expected from asm).  */ \n+      list = collect_non_operand_hard_regs (&XEXP (op, 0), data,\n+\t\t\t\t\t    list, OP_OUT, true);\n+      break;\n+    case PRE_INC: case PRE_DEC: case POST_INC: case POST_DEC:\n+      list = collect_non_operand_hard_regs (&XEXP (op, 0), data,\n+\t\t\t\t\t    list, OP_INOUT, false);\n+      break;\n+    case PRE_MODIFY: case POST_MODIFY:\n+      list = collect_non_operand_hard_regs (&XEXP (op, 0), data,\n+\t\t\t\t\t    list, OP_INOUT, false);\n+      list = collect_non_operand_hard_regs (&XEXP (op, 1), data,\n+\t\t\t\t\t    list, OP_IN, false);\n+      break;\n+    default:\n+      fmt = GET_RTX_FORMAT (code);\n+      for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+\t{\n+\t  if (fmt[i] == 'e')\n+\t    list = collect_non_operand_hard_regs (&XEXP (op, i), data,\n+\t\t\t\t\t\t  list, OP_IN, false);\n+\t  else if (fmt[i] == 'E')\n+\t    for (j = XVECLEN (op, i) - 1; j >= 0; j--)\n+\t      list = collect_non_operand_hard_regs (&XVECEXP (op, i, j), data,\n+\t\t\t\t\t\t    list, OP_IN, false);\n+\t}\n+    }\n+  return list;\n+}\n+\n+/* Set up and return info about INSN.  Set up the info if it is not set up\n+   yet.\t */\n+lra_insn_recog_data_t\n+lra_set_insn_recog_data (rtx insn)\n+{\n+  lra_insn_recog_data_t data;\n+  int i, n, icode;\n+  rtx **locs;\n+  unsigned int uid = INSN_UID (insn);\n+  struct lra_static_insn_data *insn_static_data;\n+\n+  check_and_expand_insn_recog_data (uid);\n+  if (DEBUG_INSN_P (insn))\n+    icode = -1;\n+  else\n+    {\n+      icode = INSN_CODE (insn);\n+      if (icode < 0)\n+\t/* It might be a new simple insn which is not recognized yet.  */\n+\tINSN_CODE (insn) = icode = recog_memoized (insn);\n+    }\n+  data = XNEW (struct lra_insn_recog_data);\n+  lra_insn_recog_data[uid] = data;\n+  data->insn = insn;\n+  data->used_insn_alternative = -1;\n+  data->icode = icode;\n+  data->regs = NULL;\n+  if (DEBUG_INSN_P (insn))\n+    {\n+      data->insn_static_data = &debug_insn_static_data;\n+      data->dup_loc = NULL;\n+      data->arg_hard_regs = NULL;\n+#ifdef HAVE_ATTR_enabled\n+      data->alternative_enabled_p = NULL;\n+#endif\n+      data->operand_loc = XNEWVEC (rtx *, 1);\n+      data->operand_loc[0] = &INSN_VAR_LOCATION_LOC (insn);\n+      return data;\n+    }\n+  if (icode < 0)\n+    {\n+      int nop;\n+      enum machine_mode operand_mode[MAX_RECOG_OPERANDS];\n+      const char *constraints[MAX_RECOG_OPERANDS];\n+\n+      nop = asm_noperands (PATTERN (insn));\n+      data->operand_loc = data->dup_loc = NULL;\n+      if (nop < 0)\n+\t/* Its is a special insn like USE or CLOBBER.  */\n+\tdata->insn_static_data = insn_static_data\n+\t  = get_static_insn_data (-1, 0, 0, 1);\n+      else\n+\t{\n+\t  /* expand_asm_operands makes sure there aren't too many\n+\t     operands.\t*/\n+\t  lra_assert (nop <= MAX_RECOG_OPERANDS);\n+\t  if (nop != 0)\n+\t    data->operand_loc = XNEWVEC (rtx *, nop);\n+\t  /* Now get the operand values and constraints out of the\n+\t     insn.  */\n+\t  decode_asm_operands (PATTERN (insn), NULL,\n+\t\t\t       data->operand_loc,\n+\t\t\t       constraints, operand_mode, NULL);\n+\t  n = 1;\n+\t  if (nop > 0)\n+\t    {\n+\t      const char *p =  recog_data.constraints[0];\n+\t      \n+\t      for (p =\tconstraints[0]; *p; p++)\n+\t\tn += *p == ',';\n+\t    }\n+\t  data->insn_static_data = insn_static_data\n+\t    = get_static_insn_data (-1, nop, 0, n);\n+\t  for (i = 0; i < nop; i++)\n+\t    {\n+\t      insn_static_data->operand[i].mode = operand_mode[i];\n+\t      insn_static_data->operand[i].constraint = constraints[i];\n+\t      insn_static_data->operand[i].strict_low = false;\n+\t      insn_static_data->operand[i].is_operator = false;\n+\t      insn_static_data->operand[i].is_address = false;\n+\t    }\n+\t}\n+      for (i = 0; i < insn_static_data->n_operands; i++)\n+\tinsn_static_data->operand[i].type\n+\t  = (insn_static_data->operand[i].constraint[0] == '=' ? OP_OUT\n+\t     : insn_static_data->operand[i].constraint[0] == '+' ? OP_INOUT\n+\t     : OP_IN);\n+#ifdef HAVE_ATTR_enabled\n+      data->alternative_enabled_p = NULL;\n+#endif\n+    }\n+  else\n+    {\n+      insn_extract (insn);\n+      data->insn_static_data = insn_static_data\n+\t= get_static_insn_data (icode, insn_data[icode].n_operands,\n+\t\t\t\tinsn_data[icode].n_dups,\n+\t\t\t\tinsn_data[icode].n_alternatives);\n+      n = insn_static_data->n_operands;\n+      if (n == 0)\n+\tlocs = NULL;\n+      else\n+\t{\n+\t  locs = XNEWVEC (rtx *, n);\n+\t  memcpy (locs, recog_data.operand_loc, n * sizeof (rtx *));\n+\t}\n+      data->operand_loc = locs;\n+      n = insn_static_data->n_dups;\n+      if (n == 0)\n+\tlocs = NULL;\n+      else\n+\t{\n+\t  locs = XNEWVEC (rtx *, n);\n+\t  memcpy (locs, recog_data.dup_loc, n * sizeof (rtx *));\n+\t}\n+      data->dup_loc = locs;\n+#ifdef HAVE_ATTR_enabled\n+      {\n+\tbool *bp;\n+\n+\tn = insn_static_data->n_alternatives;\n+\tlra_assert (n >= 0);\n+\tdata->alternative_enabled_p = bp = XNEWVEC (bool, n);\n+\t/* Cache the insn because we don't want to call extract_insn\n+\t   from get_attr_enabled as extract_insn modifies\n+\t   which_alternative.  The attribute enabled should not depend\n+\t   on insn operands, operand modes, operand types, and operand\n+\t   constraints.\t It should depend on the architecture.\tIf it\n+\t   is not true, we should rewrite this file code to use\n+\t   extract_insn instead of less expensive insn_extract.\t */\n+\trecog_data.insn = insn;\n+\tfor (i = 0; i < n; i++)\n+\t  {\n+\t    which_alternative = i;\n+\t    bp[i] = get_attr_enabled (insn);\n+\t  }\n+      }\n+#endif\n+    }\n+  if (GET_CODE (PATTERN (insn)) == CLOBBER || GET_CODE (PATTERN (insn)) == USE)\n+    insn_static_data->hard_regs = NULL;\n+  else\n+    insn_static_data->hard_regs\n+      = collect_non_operand_hard_regs (&PATTERN (insn), data,\n+\t\t\t\t       NULL, OP_IN, false);\n+  setup_operand_alternative (data);\n+  data->arg_hard_regs = NULL;\n+  if (CALL_P (insn))\n+    {\n+      rtx link;\n+      int n_hard_regs, regno, arg_hard_regs[FIRST_PSEUDO_REGISTER];\n+\n+      n_hard_regs = 0;\n+      /* Finding implicit hard register usage.\tWe believe it will be\n+\t not changed whatever transformations are used.\t Call insns\n+\t are such example.  */\n+      for (link = CALL_INSN_FUNCTION_USAGE (insn);\n+\t   link != NULL_RTX;\n+\t   link = XEXP (link, 1))\n+\tif (GET_CODE (XEXP (link, 0)) == USE\n+\t    && REG_P (XEXP (XEXP (link, 0), 0)))\n+\t  {\n+\t    regno = REGNO (XEXP (XEXP (link, 0), 0));\n+\t    lra_assert (regno < FIRST_PSEUDO_REGISTER);\n+\t    /* It is an argument register.  */\n+\t    for (i = (hard_regno_nregs\n+\t\t      [regno][GET_MODE (XEXP (XEXP (link, 0), 0))]) - 1;\n+\t\t i >= 0;\n+\t\t i--)\n+\t      arg_hard_regs[n_hard_regs++] = regno + i;\n+\t  }\n+      if (n_hard_regs != 0)\n+\t{\n+\t  arg_hard_regs[n_hard_regs++] = -1;\n+\t  data->arg_hard_regs = XNEWVEC (int, n_hard_regs);\n+\t  memcpy (data->arg_hard_regs, arg_hard_regs,\n+\t\t  sizeof (int) * n_hard_regs);\n+\t}\n+    }\n+  /* Some output operand can be recognized only from the context not\n+     from the constraints which are empty in this case.\t Call insn may\n+     contain a hard register in set destination with empty constraint\n+     and extract_insn treats them as an input.\t*/\n+  for (i = 0; i < insn_static_data->n_operands; i++)\n+    {\n+      int j;\n+      rtx pat, set;\n+      struct lra_operand_data *operand = &insn_static_data->operand[i];\n+\n+      /* ??? Should we treat 'X' the same way.\tIt looks to me that\n+\t 'X' means anything and empty constraint means we do not\n+\t care.\t*/\n+      if (operand->type != OP_IN || *operand->constraint != '\\0'\n+\t  || operand->is_operator)\n+\tcontinue;\n+      pat = PATTERN (insn);\n+      if (GET_CODE (pat) == SET)\n+\t{\n+\t  if (data->operand_loc[i] != &SET_DEST (pat))\n+\t    continue;\n+\t}\n+      else if (GET_CODE (pat) == PARALLEL)\n+\t{\n+\t  for (j = XVECLEN (pat, 0) - 1; j >= 0; j--)\n+\t    {\n+\t      set = XVECEXP (PATTERN (insn), 0, j);\n+\t      if (GET_CODE (set) == SET\n+\t\t  && &SET_DEST (set) == data->operand_loc[i])\n+\t\tbreak;\n+\t    }\n+\t  if (j < 0)\n+\t    continue;\n+\t}\n+      else\n+\tcontinue;\n+      operand->type = OP_OUT;\n+    }\n+  return data;\n+}\n+\n+/* Return info about insn give by UID.\tThe info should be already set\n+   up.\t*/\n+static lra_insn_recog_data_t\n+get_insn_recog_data_by_uid (int uid)\n+{\n+  lra_insn_recog_data_t data;\n+\n+  data = lra_insn_recog_data[uid];\n+  lra_assert (data != NULL);\n+  return data;\n+}\n+\n+/* Invalidate all info about insn given by its UID.  */\n+static void\n+invalidate_insn_recog_data (int uid)\n+{\n+  lra_insn_recog_data_t data;\n+\n+  data = lra_insn_recog_data[uid];\n+  lra_assert (data != NULL);\n+  free_insn_recog_data (data);\n+  lra_insn_recog_data[uid] = NULL;\n+}\n+\n+/* Update all the insn info about INSN.\t It is usually called when\n+   something in the insn was changed.  Return the updated info.\t */\n+lra_insn_recog_data_t\n+lra_update_insn_recog_data (rtx insn)\n+{\n+  lra_insn_recog_data_t data;\n+  int n;\n+  unsigned int uid = INSN_UID (insn);\n+  struct lra_static_insn_data *insn_static_data;\n+  \n+  check_and_expand_insn_recog_data (uid);\n+  if ((data = lra_insn_recog_data[uid]) != NULL\n+      && data->icode != INSN_CODE (insn))\n+    {\n+      invalidate_insn_data_regno_info (data, insn, get_insn_freq (insn));\n+      invalidate_insn_recog_data (uid);\n+      data = NULL;\n+    }\n+  if (data == NULL)\n+    return lra_get_insn_recog_data (insn);\n+  insn_static_data = data->insn_static_data;\n+  data->used_insn_alternative = -1;\n+  if (DEBUG_INSN_P (insn))\n+    return data;\n+  if (data->icode < 0)\n+    {\n+      int nop;\n+      enum machine_mode operand_mode[MAX_RECOG_OPERANDS];\n+      const char *constraints[MAX_RECOG_OPERANDS];\n+\n+      nop = asm_noperands (PATTERN (insn));\n+      if (nop >= 0)\n+\t{\n+\t  lra_assert (nop == data->insn_static_data->n_operands);\n+\t  /* Now get the operand values and constraints out of the\n+\t     insn.  */\n+\t  decode_asm_operands (PATTERN (insn), NULL,\n+\t\t\t       data->operand_loc,\n+\t\t\t       constraints, operand_mode, NULL);\n+#ifdef ENABLE_CHECKING\n+\t  {\n+\t    int i;\n+\n+\t    for (i = 0; i < nop; i++)\n+\t      lra_assert\n+\t\t(insn_static_data->operand[i].mode == operand_mode[i]\n+\t\t && insn_static_data->operand[i].constraint == constraints[i]\n+\t\t && ! insn_static_data->operand[i].is_operator);\n+\t  }\n+#endif\n+\t}\n+#ifdef ENABLE_CHECKING\n+      {\n+\tint i;\n+\n+\tfor (i = 0; i < insn_static_data->n_operands; i++)\n+\t  lra_assert\n+\t    (insn_static_data->operand[i].type\n+\t     == (insn_static_data->operand[i].constraint[0] == '=' ? OP_OUT\n+\t\t : insn_static_data->operand[i].constraint[0] == '+' ? OP_INOUT\n+\t\t : OP_IN));\n+      }\n+#endif\n+    }\n+  else\n+    {\n+      insn_extract (insn);\n+      n = insn_static_data->n_operands;\n+      if (n != 0)\n+\tmemcpy (data->operand_loc, recog_data.operand_loc, n * sizeof (rtx *));\n+      n = insn_static_data->n_dups;\n+      if (n != 0)\n+\tmemcpy (data->dup_loc, recog_data.dup_loc, n * sizeof (rtx *));\n+#ifdef HAVE_ATTR_enabled\n+#ifdef ENABLE_CHECKING\n+      {\n+\tint i;\n+\tbool *bp;\n+\t\n+\tn = insn_static_data->n_alternatives;\n+\tbp = data->alternative_enabled_p;\n+\tlra_assert (n >= 0 && bp != NULL);\n+\t/* Cache the insn to prevent extract_insn call from\n+\t   get_attr_enabled.  */\n+\trecog_data.insn = insn;\n+\tfor (i = 0; i < n; i++)\n+\t  {\n+\t    which_alternative = i;\n+\t    lra_assert (bp[i] == get_attr_enabled (insn));\n+\t  }\n+      }\n+#endif\n+#endif\n+    }\n+  return data;\n+}\n+\n+/* Set up that INSN is using alternative ALT now.  */\n+void\n+lra_set_used_insn_alternative (rtx insn, int alt)\n+{\n+  lra_insn_recog_data_t data;\n+\n+  data = lra_get_insn_recog_data (insn);\n+  data->used_insn_alternative = alt;\n+}\n+\n+/* Set up that insn with UID is using alternative ALT now.  The insn\n+   info should be already set up.  */\n+void\n+lra_set_used_insn_alternative_by_uid (int uid, int alt)\n+{\n+  lra_insn_recog_data_t data;\n+\n+  check_and_expand_insn_recog_data (uid);\n+  data = lra_insn_recog_data[uid];\n+  lra_assert (data != NULL);\n+  data->used_insn_alternative = alt;\n+}\n+\n+\f\n+\n+/* This page contains code dealing with common register info and\n+   pseudo copies.  */\n+\n+/* The size of the following array.  */\n+static int reg_info_size;\n+/* Common info about each register.  */\n+struct lra_reg *lra_reg_info;\n+\n+/* Last register value.\t */\n+static int last_reg_value;\n+\n+/* Return new register value.  */\n+static int\n+get_new_reg_value (void)\n+{\n+  return ++last_reg_value;\n+}\n+\n+/* Pools for copies.  */\n+static alloc_pool copy_pool;\n+\n+DEF_VEC_P(lra_copy_t);\n+DEF_VEC_ALLOC_P(lra_copy_t, heap);\n+\n+/* Vec referring to pseudo copies.  */\n+static VEC(lra_copy_t,heap) *copy_vec;\n+\n+/* Initialize I-th element of lra_reg_info.  */\n+static inline void\n+initialize_lra_reg_info_element (int i)\n+{\n+  bitmap_initialize (&lra_reg_info[i].insn_bitmap, &reg_obstack);\n+#ifdef STACK_REGS\n+  lra_reg_info[i].no_stack_p = false;\n+#endif\n+  CLEAR_HARD_REG_SET (lra_reg_info[i].conflict_hard_regs);\n+  lra_reg_info[i].preferred_hard_regno1 = -1;\n+  lra_reg_info[i].preferred_hard_regno2 = -1;\n+  lra_reg_info[i].preferred_hard_regno_profit1 = 0;\n+  lra_reg_info[i].preferred_hard_regno_profit2 = 0;\n+  lra_reg_info[i].live_ranges = NULL;\n+  lra_reg_info[i].nrefs = lra_reg_info[i].freq = 0;\n+  lra_reg_info[i].last_reload = 0;\n+  lra_reg_info[i].restore_regno = -1;\n+  lra_reg_info[i].val = get_new_reg_value ();\n+  lra_reg_info[i].copies = NULL;\n+}\n+\n+/* Initialize common reg info and copies.  */\n+static void\n+init_reg_info (void)\n+{\n+  int i;\n+\n+  last_reg_value = 0;\n+  reg_info_size = max_reg_num () * 3 / 2 + 1;\n+  lra_reg_info = XNEWVEC (struct lra_reg, reg_info_size);\n+  for (i = 0; i < reg_info_size; i++)\n+    initialize_lra_reg_info_element (i);\n+  copy_pool\n+    = create_alloc_pool (\"lra copies\", sizeof (struct lra_copy), 100);\n+  copy_vec = VEC_alloc (lra_copy_t, heap, 100);\n+}\n+\n+\n+/* Finish common reg info and copies.  */\n+static void\n+finish_reg_info (void)\n+{\n+  int i;\n+\n+  for (i = 0; i < reg_info_size; i++)\n+    bitmap_clear (&lra_reg_info[i].insn_bitmap);\n+  free (lra_reg_info);\n+  reg_info_size = 0;\n+  free_alloc_pool (copy_pool);\n+  VEC_free (lra_copy_t, heap, copy_vec);\n+}\n+\n+/* Expand common reg info if it is necessary.  */\n+static void\n+expand_reg_info (void)\n+{\n+  int i, old = reg_info_size;\n+\n+  if (reg_info_size > max_reg_num ())\n+    return;\n+  reg_info_size = max_reg_num () * 3 / 2 + 1;\n+  lra_reg_info = XRESIZEVEC (struct lra_reg, lra_reg_info, reg_info_size);\n+  for (i = old; i < reg_info_size; i++)\n+    initialize_lra_reg_info_element (i);\n+}\n+\n+/* Free all copies.  */\n+void\n+lra_free_copies (void)\n+{\n+  lra_copy_t cp;\n+\n+  while (VEC_length (lra_copy_t, copy_vec) != 0)\n+    {\n+      cp = VEC_pop (lra_copy_t, copy_vec);\n+      lra_reg_info[cp->regno1].copies = lra_reg_info[cp->regno2].copies = NULL;\n+      pool_free (copy_pool, cp);\n+    }\n+}\n+\n+/* Create copy of two pseudos REGNO1 and REGNO2.  The copy execution\n+   frequency is FREQ.  */\n+void\n+lra_create_copy (int regno1, int regno2, int freq)\n+{\n+  bool regno1_dest_p;\n+  lra_copy_t cp;\n+\n+  lra_assert (regno1 != regno2);\n+  regno1_dest_p = true;\n+  if (regno1 > regno2)\n+    {\n+      int temp = regno2;\n+\n+      regno1_dest_p = false;\n+      regno2 = regno1;\n+      regno1 = temp;\n+    }\n+  cp = (lra_copy_t) pool_alloc (copy_pool);\n+  VEC_safe_push (lra_copy_t, heap, copy_vec, cp);\n+  cp->regno1_dest_p = regno1_dest_p;\n+  cp->freq = freq;\n+  cp->regno1 = regno1;\n+  cp->regno2 = regno2;\n+  cp->regno1_next = lra_reg_info[regno1].copies;\n+  lra_reg_info[regno1].copies = cp;\n+  cp->regno2_next = lra_reg_info[regno2].copies;\n+  lra_reg_info[regno2].copies = cp;\n+  if (lra_dump_file != NULL)\n+    fprintf (lra_dump_file, \"\t   Creating copy r%d%sr%d@%d\\n\",\n+\t     regno1, regno1_dest_p ? \"<-\" : \"->\", regno2, freq);\n+}\n+\n+/* Return N-th (0, 1, ...) copy.  If there is no copy, return\n+   NULL.  */\n+lra_copy_t\n+lra_get_copy (int n)\n+{\n+  if (n >= (int) VEC_length (lra_copy_t, copy_vec))\n+    return NULL;\n+  return VEC_index (lra_copy_t, copy_vec, n);\n+}\n+\n+\f\n+\n+/* This page contains code dealing with info about registers in\n+   insns.  */\n+\n+/* Process X of insn UID recursively and add info (operand type is\n+   given by TYPE, flag of that it is early clobber is EARLY_CLOBBER)\n+   about registers in X to the insn DATA.  */\n+static void\n+add_regs_to_insn_regno_info (lra_insn_recog_data_t data, rtx x, int uid,\n+\t\t\t     enum op_type type, bool early_clobber)\n+{\n+  int i, j, regno;\n+  bool subreg_p;\n+  enum machine_mode mode;\n+  const char *fmt;\n+  enum rtx_code code;\n+  struct lra_insn_reg *curr;\n+\n+  code = GET_CODE (x);\n+  mode = GET_MODE (x);\n+  subreg_p = false;\n+  if (GET_CODE (x) == SUBREG)\n+    {\n+      x = SUBREG_REG (x);\n+      code = GET_CODE (x);\n+      if (GET_MODE_SIZE (mode) < GET_MODE_SIZE (GET_MODE (x)))\n+\t{\n+\t  mode = GET_MODE (x);\n+\t  if (GET_MODE_SIZE (mode) > REGMODE_NATURAL_SIZE (mode))\n+\t    subreg_p = true;\n+\t}\n+    }\n+  if (REG_P (x))\n+    {\n+      regno = REGNO (x);\n+      expand_reg_info ();\n+      if (bitmap_set_bit (&lra_reg_info[regno].insn_bitmap, uid))\n+\t{\n+\t  data->regs = new_insn_reg (regno, type, mode, subreg_p,\n+\t\t\t\t     early_clobber, data->regs);\n+\t  return;\n+\t}\n+      else\n+\t{\n+\t  for (curr = data->regs; curr != NULL; curr = curr->next)\n+\t    if (curr->regno == regno)\n+\t      {\n+\t\tif (curr->subreg_p != subreg_p || curr->biggest_mode != mode)\n+\t\t  /* The info can not be integrated into the found\n+\t\t     structure.  */\n+\t\t  data->regs = new_insn_reg (regno, type, mode, subreg_p,\n+\t\t\t\t\t     early_clobber, data->regs);\n+\t\telse\n+\t\t  {\n+\t\t    if (curr->type != type)\n+\t\t      curr->type = OP_INOUT;\n+\t\t    if (curr->early_clobber != early_clobber)\n+\t\t      curr->early_clobber = true;\n+\t\t  }\n+\t\treturn;\n+\t      }\n+\t  gcc_unreachable ();\n+\t}\n+    }\n+\n+  switch (code)\n+    {\n+    case SET:\n+      add_regs_to_insn_regno_info (data, SET_DEST (x), uid, OP_OUT, false);\n+      add_regs_to_insn_regno_info (data, SET_SRC (x), uid, OP_IN, false);\n+      break;\n+    case CLOBBER:\n+      /* We treat clobber of non-operand hard registers as early\n+\t clobber (the behavior is expected from asm).  */ \n+      add_regs_to_insn_regno_info (data, XEXP (x, 0), uid, OP_OUT, true);\n+      break;\n+    case PRE_INC: case PRE_DEC: case POST_INC: case POST_DEC:\n+      add_regs_to_insn_regno_info (data, XEXP (x, 0), uid, OP_INOUT, false);\n+      break;\n+    case PRE_MODIFY: case POST_MODIFY:\n+      add_regs_to_insn_regno_info (data, XEXP (x, 0), uid, OP_INOUT, false);\n+      add_regs_to_insn_regno_info (data, XEXP (x, 1), uid, OP_IN, false);\n+      break;\n+    default:\n+      if ((code != PARALLEL && code != EXPR_LIST) || type != OP_OUT)\n+\t/* Some targets place small structures in registers for return\n+\t   values of functions, and those registers are wrapped in\n+\t   PARALLEL that we may see as the destination of a SET.  Here\n+\t   is an example:\n+\n+\t   (call_insn 13 12 14 2 (set (parallel:BLK [\n+\t\t(expr_list:REG_DEP_TRUE (reg:DI 0 ax)\n+\t\t    (const_int 0 [0]))\n+\t\t(expr_list:REG_DEP_TRUE (reg:DI 1 dx)\n+\t\t    (const_int 8 [0x8]))\n+\t       ])\n+\t     (call (mem:QI (symbol_ref:DI (...\t*/\n+\ttype = OP_IN;\n+      fmt = GET_RTX_FORMAT (code);\n+      for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+\t{\n+\t  if (fmt[i] == 'e')\n+\t    add_regs_to_insn_regno_info (data, XEXP (x, i), uid, type, false);\n+\t  else if (fmt[i] == 'E')\n+\t    {\n+\t      for (j = XVECLEN (x, i) - 1; j >= 0; j--)\n+\t\tadd_regs_to_insn_regno_info (data, XVECEXP (x, i, j), uid,\n+\t\t\t\t\t     type, false);\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Return execution frequency of INSN.\t*/\n+static int\n+get_insn_freq (rtx insn)\n+{\n+  basic_block bb;\n+\n+  if ((bb = BLOCK_FOR_INSN (insn)) != NULL)\n+    return REG_FREQ_FROM_BB (bb);\n+  else\n+    {\n+      lra_assert (lra_insn_recog_data[INSN_UID (insn)]\n+\t\t  ->insn_static_data->n_operands == 0);\n+      /* We don't care about such insn, e.g. it might be jump with\n+\t addr_vec.  */\n+      return 1;\n+    }\n+}\n+\n+/* Invalidate all reg info of INSN with DATA and execution frequency\n+   FREQ.  Update common info about the invalidated registers.  */\n+static void\n+invalidate_insn_data_regno_info (lra_insn_recog_data_t data, rtx insn,\n+\t\t\t\t int freq)\n+{\n+  int uid;\n+  bool debug_p;\n+  unsigned int i;\n+  struct lra_insn_reg *ir, *next_ir;\n+\n+  uid = INSN_UID (insn);\n+  debug_p = DEBUG_INSN_P (insn);\n+  for (ir = data->regs; ir != NULL; ir = next_ir)\n+    {\n+      i = ir->regno;\n+      next_ir = ir->next;\n+      free_insn_reg (ir);\n+      bitmap_clear_bit (&lra_reg_info[i].insn_bitmap, uid);\n+      if (i >= FIRST_PSEUDO_REGISTER && ! debug_p)\n+\t{\n+\t  lra_reg_info[i].nrefs--;\n+\t  lra_reg_info[i].freq -= freq;\n+\t  lra_assert (lra_reg_info[i].nrefs >= 0 && lra_reg_info[i].freq >= 0);\n+\t}\n+    }\n+  data->regs = NULL;\n+}\n+\n+/* Invalidate all reg info of INSN.  Update common info about the\n+   invalidated registers.  */\n+void\n+lra_invalidate_insn_regno_info (rtx insn)\n+{\n+  invalidate_insn_data_regno_info (lra_get_insn_recog_data (insn), insn,\n+\t\t\t\t   get_insn_freq (insn));\n+}\n+\n+/* Update common reg info from reg info of insn given by its DATA and\n+   execution frequency FREQ.  */\n+static void\n+setup_insn_reg_info (lra_insn_recog_data_t data, int freq)\n+{\n+  unsigned int i;\n+  struct lra_insn_reg *ir;\n+\n+  for (ir = data->regs; ir != NULL; ir = ir->next)\n+    if ((i = ir->regno) >= FIRST_PSEUDO_REGISTER)\n+      {\n+\tlra_reg_info[i].nrefs++;\n+\tlra_reg_info[i].freq += freq;\n+      }\n+}\n+\n+/* Set up insn reg info of INSN.  Update common reg info from reg info\n+   of INSN.  */\n+void\n+lra_update_insn_regno_info (rtx insn)\n+{\n+  int i, uid, freq;\n+  lra_insn_recog_data_t data;\n+  struct lra_static_insn_data *static_data;\n+  enum rtx_code code;\n+\n+  if (! INSN_P (insn))\n+    return;\n+  data = lra_get_insn_recog_data (insn);\n+  static_data = data->insn_static_data;\n+  freq = get_insn_freq (insn);\n+  invalidate_insn_data_regno_info (data, insn, freq);\n+  uid = INSN_UID (insn);\n+  for (i = static_data->n_operands - 1; i >= 0; i--)\n+    add_regs_to_insn_regno_info (data, *data->operand_loc[i], uid,\n+\t\t\t\t static_data->operand[i].type,\n+\t\t\t\t static_data->operand[i].early_clobber);\n+  if ((code = GET_CODE (PATTERN (insn))) == CLOBBER || code == USE)\n+    add_regs_to_insn_regno_info (data, XEXP (PATTERN (insn), 0), uid,\n+\t\t\t\t code == USE ? OP_IN : OP_OUT, false);\n+  if (NONDEBUG_INSN_P (insn))\n+    setup_insn_reg_info (data, freq);\n+}\n+\n+/* Return reg info of insn given by it UID.  */\n+struct lra_insn_reg *\n+lra_get_insn_regs (int uid)\n+{\n+  lra_insn_recog_data_t data;\n+\n+  data = get_insn_recog_data_by_uid (uid);\n+  return data->regs;\n+}\n+\n+\f\n+\n+/* This page contains code dealing with stack of the insns which\n+   should be processed by the next constraint pass.  */\n+\n+/* Bitmap used to put an insn on the stack only in one exemplar.  */\n+static sbitmap lra_constraint_insn_stack_bitmap;\n+\n+/* The stack itself.  */\n+VEC (rtx, heap) *lra_constraint_insn_stack;\n+\n+/* Put INSN on the stack.  If ALWAYS_UPDATE is true, always update the reg\n+   info for INSN, otherwise only update it if INSN is not already on the\n+   stack.  */\n+static inline void\n+lra_push_insn_1 (rtx insn, bool always_update)\n+{\n+  unsigned int uid = INSN_UID (insn);\n+  if (always_update)\n+    lra_update_insn_regno_info (insn);\n+  if (uid >= SBITMAP_SIZE (lra_constraint_insn_stack_bitmap))\n+    lra_constraint_insn_stack_bitmap =\n+      sbitmap_resize (lra_constraint_insn_stack_bitmap, 3 * uid / 2, 0);\n+  if (TEST_BIT (lra_constraint_insn_stack_bitmap, uid))\n+    return;\n+  SET_BIT (lra_constraint_insn_stack_bitmap, uid);\n+  if (! always_update)\n+    lra_update_insn_regno_info (insn);\n+  VEC_safe_push (rtx, heap, lra_constraint_insn_stack, insn);\n+}\n+\n+/* Put INSN on the stack.  */\n+void\n+lra_push_insn (rtx insn)\n+{\n+  lra_push_insn_1 (insn, false);\n+}\n+\n+/* Put INSN on the stack and update its reg info.  */\n+void\n+lra_push_insn_and_update_insn_regno_info (rtx insn)\n+{\n+  lra_push_insn_1 (insn, true);\n+}\n+\n+/* Put insn with UID on the stack.  */\n+void\n+lra_push_insn_by_uid (unsigned int uid)\n+{\n+  lra_push_insn (lra_insn_recog_data[uid]->insn);\n+}\n+\n+/* Take the last-inserted insns off the stack and return it.  */\n+rtx\n+lra_pop_insn (void)\n+{\n+  rtx insn = VEC_pop (rtx, lra_constraint_insn_stack);\n+  RESET_BIT (lra_constraint_insn_stack_bitmap, INSN_UID (insn));\n+  return insn;\n+}\n+\n+/* Return the current size of the insn stack.  */\n+unsigned int\n+lra_insn_stack_length (void)\n+{\n+  return VEC_length (rtx, lra_constraint_insn_stack);\n+}\n+\n+/* Push insns FROM to TO (excluding it) going in reverse order.\t */\n+static void\n+push_insns (rtx from, rtx to)\n+{\n+  rtx insn;\n+\n+  if (from == NULL_RTX)\n+    return;\n+  for (insn = from; insn != to; insn = PREV_INSN (insn))\n+    if (INSN_P (insn))\n+      lra_push_insn (insn);\n+}\n+\n+/* Emit insns BEFORE before INSN and insns AFTER after INSN.  Put the\n+   insns onto the stack.  Print about emitting the insns with\n+   TITLE.  */\n+void\n+lra_process_new_insns (rtx insn, rtx before, rtx after, const char *title)\n+{\n+  rtx last;\n+\n+  if (lra_dump_file != NULL && (before != NULL_RTX || after != NULL_RTX))\n+    {\n+      debug_rtl_slim (lra_dump_file, insn, insn, -1, 0);\n+      if (before != NULL_RTX)\n+\t{\n+\t  fprintf (lra_dump_file,\"    %s before:\\n\", title);\n+\t  debug_rtl_slim (lra_dump_file, before, NULL_RTX, -1, 0);\n+\t}\n+      if (after != NULL_RTX)\n+\t{\n+\t  fprintf (lra_dump_file, \"    %s after:\\n\", title);\n+\t  debug_rtl_slim (lra_dump_file, after, NULL_RTX, -1, 0);\n+\t}\n+      fprintf (lra_dump_file, \"\\n\");\n+    }\n+  if (before != NULL_RTX)\n+    {\n+      emit_insn_before (before, insn);\n+      push_insns (PREV_INSN (insn), PREV_INSN (before));\n+    }\n+  if (after != NULL_RTX)\n+    {\n+      for (last = after; NEXT_INSN (last) != NULL_RTX; last = NEXT_INSN (last))\n+\t;\n+      emit_insn_after (after, insn);\n+      push_insns (last, insn);\n+    }\n+}\n+\n+\f\n+\n+/* This page contains code dealing with scratches (changing them onto\n+   pseudos and restoring them from the pseudos).\n+\n+   We change scratches into pseudos at the beginning of LRA to\n+   simplify dealing with them (conflicts, hard register assignments).\n+\n+   If the pseudo denoting scratch was spilled it means that we do need\n+   a hard register for it.  Such pseudos are transformed back to\n+   scratches at the end of LRA.\t */\n+\n+/* Description of location of a former scratch operand.\t */\n+struct loc\n+{\n+  rtx insn; /* Insn where the scratch was.  */\n+  int nop;  /* Number of the operand which was a scratch.  */\n+};\n+\n+typedef struct loc *loc_t;\n+\n+DEF_VEC_P(loc_t);\n+DEF_VEC_ALLOC_P(loc_t, heap);\n+\n+/* Locations of the former scratches.  */\n+static VEC (loc_t, heap) *scratches;\n+\n+/* Bitmap of scratch regnos.  */\n+static bitmap_head scratch_bitmap;\n+\n+/* Bitmap of scratch operands.\t*/\n+static bitmap_head scratch_operand_bitmap;\n+\n+/* Return true if pseudo REGNO is made of SCRATCH.  */\n+bool\n+lra_former_scratch_p (int regno)\n+{\n+  return bitmap_bit_p (&scratch_bitmap, regno);\n+}\n+\n+/* Return true if the operand NOP of INSN is a former scratch.\t*/\n+bool\n+lra_former_scratch_operand_p (rtx insn, int nop)\n+{\n+  return bitmap_bit_p (&scratch_operand_bitmap,\n+\t\t       INSN_UID (insn) * MAX_RECOG_OPERANDS + nop) != 0;\n+}\n+\n+/* Change scratches onto pseudos and save their location.  */\n+static void\n+remove_scratches (void)\n+{\n+  int i;\n+  bool insn_changed_p;\n+  basic_block bb;\n+  rtx insn, reg;\n+  loc_t loc;\n+  lra_insn_recog_data_t id;\n+  struct lra_static_insn_data *static_id;\n+\n+  scratches = VEC_alloc (loc_t, heap, get_max_uid ());\n+  bitmap_initialize (&scratch_bitmap, &reg_obstack);\n+  bitmap_initialize (&scratch_operand_bitmap, &reg_obstack);\n+  FOR_EACH_BB (bb)\n+    FOR_BB_INSNS (bb, insn)\n+    if (INSN_P (insn))\n+      {\n+\tid = lra_get_insn_recog_data (insn);\n+\tstatic_id = id->insn_static_data;\n+\tinsn_changed_p = false;\n+\tfor (i = 0; i < static_id->n_operands; i++)\n+\t  if (GET_CODE (*id->operand_loc[i]) == SCRATCH\n+\t      && GET_MODE (*id->operand_loc[i]) != VOIDmode)\n+\t    {\n+\t      insn_changed_p = true;\n+\t      *id->operand_loc[i] = reg\n+\t\t= lra_create_new_reg (static_id->operand[i].mode,\n+\t\t\t\t      *id->operand_loc[i], ALL_REGS, NULL);\n+\t      add_reg_note (insn, REG_UNUSED, reg);\n+\t      lra_update_dup (id, i);\n+\t      loc = XNEW (struct loc);\n+\t      loc->insn = insn;\n+\t      loc->nop = i;\n+\t      VEC_safe_push (loc_t, heap, scratches, loc);\n+\t      bitmap_set_bit (&scratch_bitmap, REGNO (*id->operand_loc[i]));\n+\t      bitmap_set_bit (&scratch_operand_bitmap,\n+\t\t\t      INSN_UID (insn) * MAX_RECOG_OPERANDS + i);\n+\t      if (lra_dump_file != NULL)\n+\t\tfprintf (lra_dump_file,\n+\t\t\t \"Removing SCRATCH in insn #%u (nop %d)\\n\",\n+\t\t\t INSN_UID (insn), i);\n+\t    }\n+\tif (insn_changed_p)\n+\t  /* Because we might use DF right after caller-saves sub-pass\n+\t     we need to keep DF info up to date.  */\n+\t  df_insn_rescan (insn);\n+      }\n+}\n+\n+/* Changes pseudos created by function remove_scratches onto scratches.\t */\n+static void\n+restore_scratches (void)\n+{\n+  int i, regno;\n+  loc_t loc;\n+  rtx last = NULL_RTX;\n+  lra_insn_recog_data_t id = NULL;\n+\n+  for (i = 0; VEC_iterate (loc_t, scratches, i, loc); i++)\n+    {\n+      if (last != loc->insn)\n+\t{\n+\t  last = loc->insn;\n+\t  id = lra_get_insn_recog_data (last);\n+\t}\n+      if (REG_P (*id->operand_loc[loc->nop])\n+\t  && ((regno = REGNO (*id->operand_loc[loc->nop]))\n+\t      >= FIRST_PSEUDO_REGISTER)\n+\t  && lra_get_regno_hard_regno (regno) < 0)\n+\t{\n+\t  /* It should be only case when scratch register with chosen\n+\t     constraint 'X' did not get memory or hard register.  */\n+\t  lra_assert (lra_former_scratch_p (regno));\n+\t  *id->operand_loc[loc->nop]\n+\t    = gen_rtx_SCRATCH (GET_MODE (*id->operand_loc[loc->nop]));\n+\t  lra_update_dup (id, loc->nop);\n+\t  if (lra_dump_file != NULL)\n+\t    fprintf (lra_dump_file, \"Restoring SCRATCH in insn #%u(nop %d)\\n\",\n+\t\t     INSN_UID (loc->insn), loc->nop);\n+\t}\n+    }\n+  for (i = 0; VEC_iterate (loc_t, scratches, i, loc); i++)\n+    free (loc);\n+  VEC_free (loc_t, heap, scratches);\n+  bitmap_clear (&scratch_bitmap);\n+  bitmap_clear (&scratch_operand_bitmap);\n+}\n+\n+\f\n+\n+#ifdef ENABLE_CHECKING\n+\n+/* Function checks RTL for correctness.\t If FINAL_P is true, it is\n+   done at the end of LRA and the check is more rigorous.  */\n+static void\n+check_rtl (bool final_p)\n+{\n+  int i;\n+  basic_block bb;\n+  rtx insn;\n+  lra_insn_recog_data_t id;\n+\n+  lra_assert (! final_p || reload_completed);\n+  FOR_EACH_BB (bb)\n+    FOR_BB_INSNS (bb, insn)\n+    if (NONDEBUG_INSN_P (insn)\n+\t&& GET_CODE (PATTERN (insn)) != USE\n+\t&& GET_CODE (PATTERN (insn)) != CLOBBER\n+\t&& GET_CODE (PATTERN (insn)) != ADDR_VEC\n+\t&& GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC\n+\t&& GET_CODE (PATTERN (insn)) != ASM_INPUT)\n+      {\n+\tif (final_p)\n+\t  {\n+\t    extract_insn (insn);\n+\t    lra_assert (constrain_operands (1));\n+\t    continue;\n+\t  }\n+\tif (insn_invalid_p (insn, false))\n+\t  fatal_insn_not_found (insn);\n+\tif (asm_noperands (PATTERN (insn)) >= 0)\n+\t  continue;\n+\tid = lra_get_insn_recog_data (insn);\n+\t/* The code is based on assumption that all addresses in\n+\t   regular instruction are legitimate before LRA.  The code in\n+\t   lra-constraints.c is based on assumption that there is no\n+\t   subreg of memory as an insn operand.\t */\n+\tfor (i = 0; i < id->insn_static_data->n_operands; i++)\n+\t  {\n+\t    rtx op = *id->operand_loc[i];\n+\t      \n+\t    if (MEM_P (op)\n+\t\t&& (GET_MODE (op) != BLKmode\n+\t\t    || GET_CODE (XEXP (op, 0)) != SCRATCH)\n+\t\t&& ! memory_address_p (GET_MODE (op), XEXP (op, 0))\n+\t\t/* Some ports don't recognize the following addresses\n+\t\t   as legitimate.  Although they are legitimate if\n+\t\t   they satisfies the constraints and will be checked\n+\t\t   by insn constraints which we ignore here.  */\n+\t\t&& GET_CODE (XEXP (op, 0)) != UNSPEC\n+\t\t&& GET_RTX_CLASS (GET_CODE (XEXP (op, 0))) != RTX_AUTOINC)\n+\t      fatal_insn_not_found (insn);\n+\t  }\n+      }\n+}\n+#endif /* #ifdef ENABLE_CHECKING */\n+\n+/* Determine if the current function has an exception receiver block\n+   that reaches the exit block via non-exceptional edges  */\n+static bool\n+has_nonexceptional_receiver (void)\n+{\n+  edge e;\n+  edge_iterator ei;\n+  basic_block *tos, *worklist, bb;\n+\n+  /* If we're not optimizing, then just err on the safe side.  */\n+  if (!optimize)\n+    return true;\n+  \n+  /* First determine which blocks can reach exit via normal paths.  */\n+  tos = worklist = XNEWVEC (basic_block, n_basic_blocks + 1);\n+\n+  FOR_EACH_BB (bb)\n+    bb->flags &= ~BB_REACHABLE;\n+\n+  /* Place the exit block on our worklist.  */\n+  EXIT_BLOCK_PTR->flags |= BB_REACHABLE;\n+  *tos++ = EXIT_BLOCK_PTR;\n+  \n+  /* Iterate: find everything reachable from what we've already seen.  */\n+  while (tos != worklist)\n+    {\n+      bb = *--tos;\n+\n+      FOR_EACH_EDGE (e, ei, bb->preds)\n+\tif (e->flags & EDGE_ABNORMAL)\n+\t  {\n+\t    free (worklist);\n+\t    return true;\n+\t  }\n+\telse\n+\t  {\n+\t    basic_block src = e->src;\n+\n+\t    if (!(src->flags & BB_REACHABLE))\n+\t      {\n+\t\tsrc->flags |= BB_REACHABLE;\n+\t\t*tos++ = src;\n+\t      }\n+\t  }\n+    }\n+  free (worklist);\n+  /* No exceptional block reached exit unexceptionally.\t */\n+  return false;\n+}\n+\n+#ifdef AUTO_INC_DEC\n+\n+/* Process recursively X of INSN and add REG_INC notes if necessary.  */\n+static void\n+add_auto_inc_notes (rtx insn, rtx x)\n+{\n+  enum rtx_code code = GET_CODE (x);\n+  const char *fmt;\n+  int i, j;\n+\n+  if (code == MEM && auto_inc_p (XEXP (x, 0)))\n+    {\n+      add_reg_note (insn, REG_INC, XEXP (XEXP (x, 0), 0));\n+      return;\n+    }\n+\n+  /* Scan all X sub-expressions.  */\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\tadd_auto_inc_notes (insn, XEXP (x, i));\n+      else if (fmt[i] == 'E')\n+\tfor (j = XVECLEN (x, i) - 1; j >= 0; j--)\n+\t  add_auto_inc_notes (insn, XVECEXP (x, i, j));\n+    }\n+}\n+\n+#endif\n+\n+/* Remove all REG_DEAD and REG_UNUSED notes and regenerate REG_INC.\n+   We change pseudos by hard registers without notification of DF and\n+   that can make the notes obsolete.  DF-infrastructure does not deal\n+   with REG_INC notes -- so we should regenerate them here.  */\n+static void\n+update_inc_notes (void)\n+{\n+  rtx *pnote;\n+  basic_block bb;\n+  rtx insn;\n+\n+  FOR_EACH_BB (bb)\n+    FOR_BB_INSNS (bb, insn)\n+    if (NONDEBUG_INSN_P (insn))\n+      {\n+\tpnote = &REG_NOTES (insn);\n+\twhile (*pnote != 0)\n+\t  {\n+\t    if (REG_NOTE_KIND (*pnote) == REG_INC)\n+\t      *pnote = XEXP (*pnote, 1);\n+\t    else\n+\t      pnote = &XEXP (*pnote, 1);\n+\t  }\n+#ifdef AUTO_INC_DEC\n+\tadd_auto_inc_notes (insn, PATTERN (insn));\n+#endif\n+      }\n+}\n+\n+/* Set to 1 while in lra.  */\n+int lra_in_progress;\n+\n+/* Start of reload pseudo regnos before the new spill pass.  */ \n+int lra_constraint_new_regno_start;\n+\n+/* Inheritance pseudo regnos before the new spill pass.\t */ \n+bitmap_head lra_inheritance_pseudos;\n+\n+/* Split regnos before the new spill pass.  */ \n+bitmap_head lra_split_regs;\n+\n+/* Reload pseudo regnos before the new assign pass which still can be\n+   spilled after the assinment pass.  */ \n+bitmap_head lra_optional_reload_pseudos;\n+\n+/* First UID of insns generated before a new spill pass.  */\n+int lra_constraint_new_insn_uid_start;\n+\n+/* File used for output of LRA debug information.  */\n+FILE *lra_dump_file;\n+\n+/* True if we should try spill into registers of different classes\n+   instead of memory.  */\n+bool lra_reg_spill_p;\n+\n+/* Set up value LRA_REG_SPILL_P.  */\n+static void\n+setup_reg_spill_flag (void)\n+{\n+  int cl, mode;\n+\n+  if (targetm.spill_class != NULL)\n+    for (cl = 0; cl < (int) LIM_REG_CLASSES; cl++)\n+      for (mode = 0; mode < MAX_MACHINE_MODE; mode++)\n+\tif (targetm.spill_class ((enum reg_class) cl,\n+\t\t\t\t (enum machine_mode) mode) != NO_REGS)\n+\t  {\n+\t    lra_reg_spill_p = true;\n+\t    return;\n+\t  }\n+  lra_reg_spill_p = false;\n+}\n+\n+/* True if the current function is too big to use regular algorithms\n+   in LRA. In other words, we should use simpler and faster algorithms\n+   in LRA.  It also means we should not worry about generation code\n+   for caller saves.  The value is set up in IRA.  */\n+bool lra_simple_p;\n+\n+/* Major LRA entry function.  F is a file should be used to dump LRA\n+   debug info.  */\n+void\n+lra (FILE *f)\n+{\n+  int i;\n+  bool live_p, scratch_p, inserted_p;\n+\n+  lra_dump_file = f;\n+\n+  timevar_push (TV_LRA);\n+\n+  init_insn_recog_data ();\n+\n+#ifdef ENABLE_CHECKING\n+  check_rtl (false);\n+#endif\n+\n+  COPY_HARD_REG_SET (lra_no_alloc_regs, ira_no_alloc_regs);\n+\n+  lra_live_range_iter = lra_coalesce_iter = 0;\n+  lra_constraint_iter = lra_constraint_iter_after_spill = 0;\n+  lra_inheritance_iter = lra_undo_inheritance_iter = 0;\n+\n+  setup_reg_spill_flag ();\n+\n+  /* We can not set up reload_in_progress because it prevents new\n+     pseudo creation.  */\n+  lra_in_progress = 1;\n+\n+  init_reg_info ();\n+  expand_reg_info ();\n+\n+  /* Function remove_scratches can creates new pseudos for clobbers --\n+     so set up lra_constraint_new_regno_start before its call to\n+     permit changing reg classes for pseudos created by this\n+     simplification.  */\n+  lra_constraint_new_regno_start = max_reg_num ();\n+  remove_scratches ();\n+  scratch_p = lra_constraint_new_regno_start != max_reg_num ();\n+\n+  /* A function that has a non-local label that can reach the exit\n+     block via non-exceptional paths must save all call-saved\n+     registers.\t */\n+  if (cfun->has_nonlocal_label && has_nonexceptional_receiver ())\n+    crtl->saves_all_registers = 1;\n+\n+  if (crtl->saves_all_registers)\n+    for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+      if (! call_used_regs[i] && ! fixed_regs[i] && ! LOCAL_REGNO (i))\n+\tdf_set_regs_ever_live (i, true);\n+\n+  /* We don't DF from now and avoid its using because it is to\n+     expensive when a lot of RTL changes are made.  */\n+  df_set_flags (DF_NO_INSN_RESCAN);\n+  lra_constraint_insn_stack = VEC_alloc (rtx, heap, get_max_uid ());\n+  lra_constraint_insn_stack_bitmap = sbitmap_alloc (get_max_uid ());\n+  sbitmap_zero (lra_constraint_insn_stack_bitmap);\n+  lra_live_ranges_init ();\n+  lra_constraints_init ();\n+  lra_curr_reload_num = 0;\n+  push_insns (get_last_insn (), NULL_RTX);\n+  /* It is needed for the 1st coalescing.  */\n+  lra_constraint_new_insn_uid_start = get_max_uid ();\n+  bitmap_initialize (&lra_inheritance_pseudos, &reg_obstack);\n+  bitmap_initialize (&lra_split_regs, &reg_obstack);\n+  bitmap_initialize (&lra_optional_reload_pseudos, &reg_obstack);\n+  live_p = false;\n+  for (;;)\n+    {\n+      for (;;)\n+\t{\n+\t  bitmap_clear (&lra_optional_reload_pseudos);\n+\t  /* We should try to assign hard registers to scratches even\n+\t     if there were no RTL transformations in\n+\t     lra_constraints.  */\n+\t  if (! lra_constraints (lra_constraint_iter == 0)\n+\t      && (lra_constraint_iter > 1\n+\t\t  || (! scratch_p && ! caller_save_needed)))\n+\t    break;\n+\t  /* Constraint transformations may result in that eliminable\n+\t     hard regs become uneliminable and pseudos which use them\n+\t     should be spilled.\t It is better to do it before pseudo\n+\t     assignments.\n+\n+\t     For example, rs6000 can make\n+\t     RS6000_PIC_OFFSET_TABLE_REGNUM uneliminable if we started\n+\t     to use a constant pool.  */\n+\t  lra_eliminate (false);\n+\t  /* Do inheritance only for regular algorithms.  */\n+\t  if (! lra_simple_p)\n+\t    lra_inheritance ();\n+\t  /* We need live ranges for lra_assign -- so build them.  */\n+\t  lra_create_live_ranges (true);\n+\t  live_p = true;\n+\t  /* If we don't spill non-reload and non-inheritance pseudos,\n+\t     there is no sense to run memory-memory move coalescing.\n+\t     If inheritance pseudos were spilled, the memory-memory\n+\t     moves involving them will be removed by pass undoing\n+\t     inheritance.  */\n+\t  if (lra_simple_p)\n+\t    lra_assign ();\n+\t  else\n+\t    {\n+\t      /* Do coalescing only for regular algorithms.  */\n+\t      if (! lra_assign () && lra_coalesce ())\t\n+\t\tlive_p = false;\n+\t      if (lra_undo_inheritance ())\n+\t\tlive_p = false;\n+\t    }\n+\t}\n+      bitmap_clear (&lra_inheritance_pseudos);\n+      bitmap_clear (&lra_split_regs);\n+      if (! lra_need_for_spills_p ())\n+\tbreak;\n+      if (! live_p)\n+\t{\n+\t  /* We need full live info for spilling pseudos into\n+\t     registers instead of memory.  */\n+\t  lra_create_live_ranges (lra_reg_spill_p);\n+\t  live_p = true;\n+\t}\n+      lra_spill ();\n+      /* Assignment of stack slots changes elimination offsets for\n+\t some eliminations.  So update the offsets here.  */\n+      lra_eliminate (false);\n+      lra_constraint_new_regno_start = max_reg_num ();\n+      lra_constraint_new_insn_uid_start = get_max_uid ();\n+      lra_constraint_iter_after_spill = 0;\n+    }\n+  restore_scratches ();\n+  lra_eliminate (true);\n+  lra_hard_reg_substitution ();\n+  lra_in_progress = 0;\n+  lra_clear_live_ranges ();\n+  lra_live_ranges_finish ();\n+  lra_constraints_finish ();\n+  finish_reg_info ();\n+  sbitmap_free (lra_constraint_insn_stack_bitmap);\n+  VEC_free (rtx, heap, lra_constraint_insn_stack);\n+  finish_insn_recog_data ();\n+  regstat_free_n_sets_and_refs ();\n+  regstat_free_ri ();\n+  reload_completed = 1;\n+  update_inc_notes ();\n+\n+  inserted_p = fixup_abnormal_edges ();\n+\n+  /* We've possibly turned single trapping insn into multiple ones.  */\n+  if (cfun->can_throw_non_call_exceptions)\n+    {\n+      sbitmap blocks;\n+      blocks = sbitmap_alloc (last_basic_block);\n+      sbitmap_ones (blocks);\n+      find_many_sub_basic_blocks (blocks);\n+      sbitmap_free (blocks);\n+    }\n+\n+  if (inserted_p)\n+    commit_edge_insertions ();\n+\n+  /* Replacing pseudos with their memory equivalents might have\n+     created shared rtx.  Subsequent passes would get confused\n+     by this, so unshare everything here.  */\n+  unshare_all_rtl_again (get_insns ());\n+\n+#ifdef ENABLE_CHECKING\n+  check_rtl (true);\n+#endif\n+\n+  timevar_pop (TV_LRA);\n+}\n+\n+/* Called once per compiler to initialize LRA data once.  */\n+void\n+lra_init_once (void)\n+{\n+  init_insn_code_data_once ();\n+}\n+\n+/* Initialize LRA whenever register-related information is changed.  */\n+void\n+lra_init (void)\n+{\n+  init_op_alt_data ();\n+}\n+\n+/* Called once per compiler to finish LRA data which are initialize\n+   once.  */\n+void\n+lra_finish_once (void)\n+{\n+  finish_insn_code_data_once ();\n+}"}, {"sha": "ea275f72a4424098010d1b63ca994b9513d57e7f", "filename": "gcc/lra.h", "status": "added", "additions": 42, "deletions": 0, "changes": 42, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Flra.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flra.h?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -0,0 +1,42 @@\n+/* Communication between the Local Register Allocator (LRA) and\n+   the rest of the compiler.\n+   Copyright (C) 2010, 2011, 2012\n+   Free Software Foundation, Inc.\n+   Contributed by Vladimir Makarov <vmakarov@redhat.com>.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.\tIf not see\n+<http://www.gnu.org/licenses/>.\t */\n+\n+extern bool lra_simple_p;\n+\n+/* Return the allocno reg class of REGNO.  If it is a reload pseudo,\n+   the pseudo should finally get hard register of the allocno\n+   class.  */\n+static inline enum reg_class\n+lra_get_allocno_class (int regno)\n+{\n+  resize_reg_info ();\n+  return reg_allocno_class (regno);\n+}\n+\n+extern rtx lra_create_new_reg (enum machine_mode, rtx, enum reg_class,\n+\t\t\t       const char *);\n+extern void lra_init_elimination (void);\n+extern rtx lra_eliminate_regs (rtx, enum machine_mode, rtx);\n+extern void lra (FILE *);\n+extern void lra_init_once (void);\n+extern void lra_init (void);\n+extern void lra_finish_once (void);"}, {"sha": "3fb743a17e9b72763c136f1d221965fb329b3d22", "filename": "gcc/output.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Foutput.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Foutput.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foutput.h?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -76,7 +76,7 @@ extern rtx final_scan_insn (rtx, FILE *, int, int, int *);\n \n /* Replace a SUBREG with a REG or a MEM, based on the thing it is a\n    subreg of.  */\n-extern rtx alter_subreg (rtx *);\n+extern rtx alter_subreg (rtx *, bool);\n \n /* Print an operand using machine-dependent assembler syntax.  */\n extern void output_operand (rtx, int);"}, {"sha": "466ac6c3e2ac7164cad847e0daefc59d42d2e7b0", "filename": "gcc/recog.c", "status": "modified", "additions": 13, "deletions": 1, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Frecog.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Frecog.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frecog.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -993,6 +993,12 @@ general_operand (rtx op, enum machine_mode mode)\n       /* FLOAT_MODE subregs can't be paradoxical.  Combine will occasionally\n \t create such rtl, and we must reject it.  */\n       if (SCALAR_FLOAT_MODE_P (GET_MODE (op))\n+\t  /* LRA can use subreg to store a floating point value in an\n+\t     integer mode.  Although the floating point and the\n+\t     integer modes need the same number of hard registers, the\n+\t     size of floating point mode can be less than the integer\n+\t     mode.  */\n+\t  && ! lra_in_progress \n \t  && GET_MODE_SIZE (GET_MODE (op)) > GET_MODE_SIZE (GET_MODE (sub)))\n \treturn 0;\n \n@@ -1068,6 +1074,12 @@ register_operand (rtx op, enum machine_mode mode)\n       /* FLOAT_MODE subregs can't be paradoxical.  Combine will occasionally\n \t create such rtl, and we must reject it.  */\n       if (SCALAR_FLOAT_MODE_P (GET_MODE (op))\n+\t  /* LRA can use subreg to store a floating point value in an\n+\t     integer mode.  Although the floating point and the\n+\t     integer modes need the same number of hard registers, the\n+\t     size of floating point mode can be less than the integer\n+\t     mode.  */\n+\t  && ! lra_in_progress \n \t  && GET_MODE_SIZE (GET_MODE (op)) > GET_MODE_SIZE (GET_MODE (sub)))\n \treturn 0;\n \n@@ -1099,7 +1111,7 @@ scratch_operand (rtx op, enum machine_mode mode)\n \n   return (GET_CODE (op) == SCRATCH\n \t  || (REG_P (op)\n-\t      && REGNO (op) < FIRST_PSEUDO_REGISTER));\n+\t      && (lra_in_progress || REGNO (op) < FIRST_PSEUDO_REGISTER)));\n }\n \n /* Return 1 if OP is a valid immediate operand for mode MODE."}, {"sha": "361669ac19126fdcf74f81a8d9a55e7c054116d7", "filename": "gcc/rtl.h", "status": "modified", "additions": 9, "deletions": 1, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -2372,6 +2372,9 @@ extern int epilogue_completed;\n \n extern int reload_in_progress;\n \n+/* Set to 1 while in lra.  */\n+extern int lra_in_progress;\n+\n /* This macro indicates whether you may create a new\n    pseudo-register.  */\n \n@@ -2490,7 +2493,12 @@ extern rtx make_compound_operation (rtx, enum rtx_code);\n extern void delete_dead_jumptables (void);\n \n /* In sched-vis.c.  */\n-extern void dump_insn_slim (FILE *, const_rtx x);\n+extern void debug_bb_n_slim (int);\n+extern void debug_bb_slim (struct basic_block_def *);\n+extern void print_value_slim (FILE *, const_rtx, int);\n+extern void debug_rtl_slim (FILE *, const_rtx, const_rtx, int, int);\n+extern void dump_insn_slim (FILE *f, const_rtx x);\n+extern void debug_insn_slim (const_rtx x);\n \n /* In sched-rgn.c.  */\n extern void schedule_insns (void);"}, {"sha": "a101a2920394a01316c1f78efda0ff64a9fdf2aa", "filename": "gcc/rtlanal.c", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Frtlanal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Frtlanal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtlanal.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -3481,7 +3481,9 @@ simplify_subreg_regno (unsigned int xregno, enum machine_mode xmode,\n   /* Give the backend a chance to disallow the mode change.  */\n   if (GET_MODE_CLASS (xmode) != MODE_COMPLEX_INT\n       && GET_MODE_CLASS (xmode) != MODE_COMPLEX_FLOAT\n-      && REG_CANNOT_CHANGE_MODE_P (xregno, xmode, ymode))\n+      && REG_CANNOT_CHANGE_MODE_P (xregno, xmode, ymode)\n+      /* We can use mode change in LRA for some transformations.  */\n+      && ! lra_in_progress)\n     return -1;\n #endif\n \n@@ -3491,10 +3493,16 @@ simplify_subreg_regno (unsigned int xregno, enum machine_mode xmode,\n     return -1;\n \n   if (FRAME_POINTER_REGNUM != ARG_POINTER_REGNUM\n-      && xregno == ARG_POINTER_REGNUM)\n+      /* We should convert arg register in LRA after the elimination\n+\t if it is possible.  */\n+      && xregno == ARG_POINTER_REGNUM\n+      && ! lra_in_progress)\n     return -1;\n \n-  if (xregno == STACK_POINTER_REGNUM)\n+  if (xregno == STACK_POINTER_REGNUM\n+      /* We should convert hard stack register in LRA if it is\n+\t possible.  */\n+      && ! lra_in_progress)\n     return -1;\n \n   /* Try to get the register offset.  */"}, {"sha": "280b33ad2f55c45f592e9abe2213f929371f053d", "filename": "gcc/sched-vis.c", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fsched-vis.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fsched-vis.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-vis.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -546,6 +546,21 @@ print_value (char *buf, const_rtx x, int verbose)\n     }\n }\t\t\t\t/* print_value */\n \n+/* Print X, an RTL value node, to file F in slim format.  Include\n+   additional information if VERBOSE is nonzero.\n+\n+   Value nodes are constants, registers, labels, symbols and\n+   memory.  */\n+\n+void\n+print_value_slim (FILE *f, const_rtx x, int verbose)\n+{\n+  char buf[BUF_LEN];\n+\n+  print_value (buf, x, verbose);\n+  fprintf (f, \"%s\", buf);\n+}\n+\n /* The next step in insn detalization, its pattern recognition.  */\n \n void"}, {"sha": "5413c6cd153b2ea3195c70c8d1243f92eb891cba", "filename": "gcc/sdbout.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fsdbout.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Fsdbout.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsdbout.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -767,7 +767,7 @@ sdbout_symbol (tree decl, int local)\n \t      if (REGNO (value) >= FIRST_PSEUDO_REGISTER)\n \t\treturn;\n \t    }\n-\t  regno = REGNO (alter_subreg (&value));\n+\t  regno = REGNO (alter_subreg (&value, true));\n \t  SET_DECL_RTL (decl, value);\n \t}\n       /* Don't output anything if an auto variable"}, {"sha": "b3d02a1bc739a931b2605fccdf74835951bb487c", "filename": "gcc/target-globals.c", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftarget-globals.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftarget-globals.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarget-globals.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -37,6 +37,7 @@ along with GCC; see the file COPYING3.  If not see\n #include \"libfuncs.h\"\n #include \"cfgloop.h\"\n #include \"ira-int.h\"\n+#include \"lra-int.h\"\n #include \"builtins.h\"\n #include \"gcse.h\"\n #include \"bb-reorder.h\"\n@@ -55,6 +56,7 @@ struct target_globals default_target_globals = {\n   &default_target_cfgloop,\n   &default_target_ira,\n   &default_target_ira_int,\n+  &default_target_lra_int,\n   &default_target_builtins,\n   &default_target_gcse,\n   &default_target_bb_reorder,"}, {"sha": "42f19d4f64fb755220f18fa45923315e36dd961f", "filename": "gcc/target-globals.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftarget-globals.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftarget-globals.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarget-globals.h?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -32,6 +32,7 @@ extern struct target_libfuncs *this_target_libfuncs;\n extern struct target_cfgloop *this_target_cfgloop;\n extern struct target_ira *this_target_ira;\n extern struct target_ira_int *this_target_ira_int;\n+extern struct target_lra_int *this_target_lra_int;\n extern struct target_builtins *this_target_builtins;\n extern struct target_gcse *this_target_gcse;\n extern struct target_bb_reorder *this_target_bb_reorder;\n@@ -49,6 +50,7 @@ struct GTY(()) target_globals {\n   struct target_cfgloop *GTY((skip)) cfgloop;\n   struct target_ira *GTY((skip)) ira;\n   struct target_ira_int *GTY((skip)) ira_int;\n+  struct target_lra_int *GTY((skip)) lra_int;\n   struct target_builtins *GTY((skip)) builtins;\n   struct target_gcse *GTY((skip)) gcse;\n   struct target_bb_reorder *GTY((skip)) bb_reorder;\n@@ -73,6 +75,7 @@ restore_target_globals (struct target_globals *g)\n   this_target_cfgloop = g->cfgloop;\n   this_target_ira = g->ira;\n   this_target_ira_int = g->ira_int;\n+  this_target_lra_int = g->lra_int;\n   this_target_builtins = g->builtins;\n   this_target_gcse = g->gcse;\n   this_target_bb_reorder = g->bb_reorder;"}, {"sha": "586522435a2ad2f529f8eabad084a226cb5f08b9", "filename": "gcc/target.def", "status": "modified", "additions": 49, "deletions": 0, "changes": 49, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftarget.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftarget.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarget.def?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -2352,6 +2352,55 @@ DEFHOOK\n  tree, (tree type, tree expr),\n  hook_tree_tree_tree_null)\n \n+/* Return true if we use LRA instead of reload.  */\n+DEFHOOK\n+(lra_p,\n+ \"A target hook which returns true if we use LRA instead of reload pass.\\\n+  It means that LRA was ported to the target.\\\n+  \\\n+  The default version of this target hook returns always false.\",\n+ bool, (void),\n+ default_lra_p)\n+\n+/* Return register priority of given hard regno for the current target.  */\n+DEFHOOK\n+(register_priority,\n+ \"A target hook which returns the register priority number to which the\\\n+  register @var{hard_regno} belongs to.  The bigger the number, the\\\n+  more preferable the hard register usage (when all other conditions are\\\n+  the same).  This hook can be used to prefer some hard register over\\\n+  others in LRA.  For example, some x86-64 register usage needs\\\n+  additional prefix which makes instructions longer.  The hook can\\\n+  return lower priority number for such registers make them less favorable\\\n+  and as result making the generated code smaller.\\\n+  \\\n+  The default version of this target hook returns always zero.\",\n+ int, (int),\n+ default_register_priority)\n+\n+/* Return true if maximal address displacement can be different.  */\n+DEFHOOK\n+(different_addr_displacement_p,\n+ \"A target hook which returns true if an address with the same structure\\\n+  can have different maximal legitimate displacement.  For example, the\\\n+  displacement can depend on memory mode or on operand combinations in\\\n+  the insn.\\\n+  \\\n+  The default version of this target hook returns always false.\",\n+ bool, (void),\n+ default_different_addr_displacement_p)\n+\n+/* Determine class for spilling pseudos of given mode into registers\n+   instead of memory.  */\n+DEFHOOK\n+(spill_class,\n+ \"This hook defines a class of registers which could be used for spilling\\\n+  pseudos of the given mode and class, or @code{NO_REGS} if only memory\\\n+  should be used.  Not defining this hook is equivalent to returning\\\n+  @code{NO_REGS} for all inputs.\",\n+ reg_class_t, (reg_class_t, enum machine_mode),\n+ NULL)\n+\n /* True if a structure, union or array with MODE containing FIELD should\n    be accessed using BLKmode.  */\n DEFHOOK"}, {"sha": "be008fdcd5d037408b5f5dae1aa13c75501b7757", "filename": "gcc/targhooks.c", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftarghooks.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftarghooks.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarghooks.c?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -840,6 +840,24 @@ default_branch_target_register_class (void)\n   return NO_REGS;\n }\n \n+extern bool\n+default_lra_p (void)\n+{\n+  return false;\n+}\n+\n+int\n+default_register_priority (int hard_regno ATTRIBUTE_UNUSED)\n+{\n+  return 0;\n+}\n+\n+extern bool\n+default_different_addr_displacement_p (void)\n+{\n+  return false;\n+}\n+\n reg_class_t\n default_secondary_reload (bool in_p ATTRIBUTE_UNUSED, rtx x ATTRIBUTE_UNUSED,\n \t\t\t  reg_class_t reload_class_i ATTRIBUTE_UNUSED,"}, {"sha": "d4196024708d7098bd956bf949b5595ec582831d", "filename": "gcc/targhooks.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftarghooks.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftarghooks.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarghooks.h?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -132,6 +132,9 @@ extern rtx default_static_chain (const_tree, bool);\n extern void default_trampoline_init (rtx, tree, rtx);\n extern int default_return_pops_args (tree, tree, int);\n extern reg_class_t default_branch_target_register_class (void);\n+extern bool default_lra_p (void);\n+extern int default_register_priority (int);\n+extern bool default_different_addr_displacement_p (void);\n extern reg_class_t default_secondary_reload (bool, rtx, reg_class_t,\n \t\t\t\t\t     enum machine_mode,\n \t\t\t\t\t     secondary_reload_info *);"}, {"sha": "3ad8ba2d7b51586a20fdf0ac6b95cf3bc150349e", "filename": "gcc/timevar.def", "status": "modified", "additions": 9, "deletions": 3, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftimevar.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55a2c3226a3e90a6d65f19710bab1ac377054234/gcc%2Ftimevar.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftimevar.def?ref=55a2c3226a3e90a6d65f19710bab1ac377054234", "patch": "@@ -223,10 +223,16 @@ DEFTIMEVAR (TV_REGMOVE               , \"regmove\")\n DEFTIMEVAR (TV_MODE_SWITCH           , \"mode switching\")\n DEFTIMEVAR (TV_SMS\t\t     , \"sms modulo scheduling\")\n DEFTIMEVAR (TV_SCHED                 , \"scheduling\")\n-DEFTIMEVAR (TV_IRA\t   \t     , \"integrated RA\")\n-DEFTIMEVAR (TV_RELOAD\t   \t     , \"reload\")\n+DEFTIMEVAR (TV_IRA\t\t     , \"integrated RA\")\n+DEFTIMEVAR (TV_LRA\t\t     , \"LRA non-specific\")\n+DEFTIMEVAR (TV_LRA_ELIMINATE\t     , \"LRA virtuals elimination\")\n+DEFTIMEVAR (TV_LRA_INHERITANCE\t     , \"LRA reload inheritance\")\n+DEFTIMEVAR (TV_LRA_CREATE_LIVE_RANGES, \"LRA create live ranges\")\n+DEFTIMEVAR (TV_LRA_ASSIGN\t     , \"LRA hard reg assignment\")\n+DEFTIMEVAR (TV_LRA_COALESCE\t     , \"LRA coalesce pseudo regs\")\n+DEFTIMEVAR (TV_RELOAD\t\t     , \"reload\")\n DEFTIMEVAR (TV_RELOAD_CSE_REGS       , \"reload CSE regs\")\n-DEFTIMEVAR (TV_GCSE_AFTER_RELOAD      , \"load CSE after reload\")\n+DEFTIMEVAR (TV_GCSE_AFTER_RELOAD     , \"load CSE after reload\")\n DEFTIMEVAR (TV_REE\t\t     , \"ree\")\n DEFTIMEVAR (TV_THREAD_PROLOGUE_AND_EPILOGUE, \"thread pro- & epilogue\")\n DEFTIMEVAR (TV_IFCVT2\t\t     , \"if-conversion 2\")"}]}