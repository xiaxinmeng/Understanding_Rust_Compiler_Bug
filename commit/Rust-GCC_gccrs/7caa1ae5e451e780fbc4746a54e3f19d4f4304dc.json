{"sha": "7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "node_id": "C_kwDOANBUbNoAKDdjYWExYWU1ZTQ1MWU3ODBmYmM0NzQ2YTU0ZTNmMTlkNGY0MzA0ZGM", "commit": {"author": {"name": "Ju-Zhe Zhong", "email": "juzhe.zhong@rivai.ai", "date": "2023-03-02T08:01:52Z"}, "committer": {"name": "Kito Cheng", "email": "kito.cheng@sifive.com", "date": "2023-03-05T09:16:30Z"}, "message": "RISC-V: Add RVV misc intrinsic support\n\nCo-authored-by: kito-cheng <kito.cheng@sifive.com>\n\ngcc/ChangeLog:\n\n\t* config/riscv/predicates.md (vector_any_register_operand): New predicate.\n\t* config/riscv/riscv-c.cc (riscv_check_builtin_call): New function.\n\t(riscv_register_pragmas): Add builtin function check call.\n\t* config/riscv/riscv-protos.h (RVV_VUNDEF): Adapt macro.\n\t(check_builtin_call): New function.\n\t* config/riscv/riscv-vector-builtins-bases.cc (class vundefined): New class.\n\t(class vreinterpret): Ditto.\n\t(class vlmul_ext): Ditto.\n\t(class vlmul_trunc): Ditto.\n\t(class vset): Ditto.\n\t(class vget): Ditto.\n\t(BASE): Ditto.\n\t* config/riscv/riscv-vector-builtins-bases.h: Ditto.\n\t* config/riscv/riscv-vector-builtins-functions.def (vluxei8): Change name.\n\t(vluxei16): Ditto.\n\t(vluxei32): Ditto.\n\t(vluxei64): Ditto.\n\t(vloxei8): Ditto.\n\t(vloxei16): Ditto.\n\t(vloxei32): Ditto.\n\t(vloxei64): Ditto.\n\t(vsuxei8): Ditto.\n\t(vsuxei16): Ditto.\n\t(vsuxei32): Ditto.\n\t(vsuxei64): Ditto.\n\t(vsoxei8): Ditto.\n\t(vsoxei16): Ditto.\n\t(vsoxei32): Ditto.\n\t(vsoxei64): Ditto.\n\t(vundefined): Add new intrinsic.\n\t(vreinterpret): Ditto.\n\t(vlmul_ext): Ditto.\n\t(vlmul_trunc): Ditto.\n\t(vset): Ditto.\n\t(vget): Ditto.\n\t* config/riscv/riscv-vector-builtins-shapes.cc (struct return_mask_def): New class.\n\t(struct narrow_alu_def): Ditto.\n\t(struct reduc_alu_def): Ditto.\n\t(struct vundefined_def): Ditto.\n\t(struct misc_def): Ditto.\n\t(struct vset_def): Ditto.\n\t(struct vget_def): Ditto.\n\t(SHAPE): Ditto.\n\t* config/riscv/riscv-vector-builtins-shapes.h: Ditto.\n\t* config/riscv/riscv-vector-builtins-types.def (DEF_RVV_EEW8_INTERPRET_OPS): New def.\n\t(DEF_RVV_EEW16_INTERPRET_OPS): Ditto.\n\t(DEF_RVV_EEW32_INTERPRET_OPS): Ditto.\n\t(DEF_RVV_EEW64_INTERPRET_OPS): Ditto.\n\t(DEF_RVV_X2_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_X4_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_X8_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_X16_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_X32_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_X64_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_LMUL1_OPS): Ditto.\n\t(DEF_RVV_LMUL2_OPS): Ditto.\n\t(DEF_RVV_LMUL4_OPS): Ditto.\n\t(vint16mf4_t): Ditto.\n\t(vint16mf2_t): Ditto.\n\t(vint16m1_t): Ditto.\n\t(vint16m2_t): Ditto.\n\t(vint16m4_t): Ditto.\n\t(vint16m8_t): Ditto.\n\t(vint32mf2_t): Ditto.\n\t(vint32m1_t): Ditto.\n\t(vint32m2_t): Ditto.\n\t(vint32m4_t): Ditto.\n\t(vint32m8_t): Ditto.\n\t(vint64m1_t): Ditto.\n\t(vint64m2_t): Ditto.\n\t(vint64m4_t): Ditto.\n\t(vint64m8_t): Ditto.\n\t(vuint16mf4_t): Ditto.\n\t(vuint16mf2_t): Ditto.\n\t(vuint16m1_t): Ditto.\n\t(vuint16m2_t): Ditto.\n\t(vuint16m4_t): Ditto.\n\t(vuint16m8_t): Ditto.\n\t(vuint32mf2_t): Ditto.\n\t(vuint32m1_t): Ditto.\n\t(vuint32m2_t): Ditto.\n\t(vuint32m4_t): Ditto.\n\t(vuint32m8_t): Ditto.\n\t(vuint64m1_t): Ditto.\n\t(vuint64m2_t): Ditto.\n\t(vuint64m4_t): Ditto.\n\t(vuint64m8_t): Ditto.\n\t(vint8mf4_t): Ditto.\n\t(vint8mf2_t): Ditto.\n\t(vint8m1_t): Ditto.\n\t(vint8m2_t): Ditto.\n\t(vint8m4_t): Ditto.\n\t(vint8m8_t): Ditto.\n\t(vuint8mf4_t): Ditto.\n\t(vuint8mf2_t): Ditto.\n\t(vuint8m1_t): Ditto.\n\t(vuint8m2_t): Ditto.\n\t(vuint8m4_t): Ditto.\n\t(vuint8m8_t): Ditto.\n\t(vint8mf8_t): Ditto.\n\t(vuint8mf8_t): Ditto.\n\t(vfloat32mf2_t): Ditto.\n\t(vfloat32m1_t): Ditto.\n\t(vfloat32m2_t): Ditto.\n\t(vfloat32m4_t): Ditto.\n\t(vfloat64m1_t): Ditto.\n\t(vfloat64m2_t): Ditto.\n\t(vfloat64m4_t): Ditto.\n\t* config/riscv/riscv-vector-builtins.cc (DEF_RVV_TYPE): Ditto.\n\t(DEF_RVV_EEW8_INTERPRET_OPS): Ditto.\n\t(DEF_RVV_EEW16_INTERPRET_OPS): Ditto.\n\t(DEF_RVV_EEW32_INTERPRET_OPS): Ditto.\n\t(DEF_RVV_EEW64_INTERPRET_OPS): Ditto.\n\t(DEF_RVV_X2_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_X4_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_X8_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_X16_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_X32_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_X64_VLMUL_EXT_OPS): Ditto.\n\t(DEF_RVV_LMUL1_OPS): Ditto.\n\t(DEF_RVV_LMUL2_OPS): Ditto.\n\t(DEF_RVV_LMUL4_OPS): Ditto.\n\t(DEF_RVV_TYPE_INDEX): Ditto.\n\t(required_extensions_p): Adapt for new intrinsic support/\n\t(get_required_extensions): New function.\n\t(check_required_extensions): Ditto.\n\t(unsigned_base_type_p): Remove.\n\t(rvv_arg_type_info::get_scalar_ptr_type): New function.\n\t(get_mode_for_bitsize): Remove.\n\t(rvv_arg_type_info::get_scalar_const_ptr_type): New function.\n\t(rvv_arg_type_info::get_base_vector_type): Ditto.\n\t(rvv_arg_type_info::get_function_type_index): Ditto.\n\t(DEF_RVV_BASE_TYPE): New def.\n\t(function_builder::apply_predication): New class.\n\t(function_expander::mask_mode): Ditto.\n\t(function_checker::function_checker): Ditto.\n\t(function_checker::report_non_ice): Ditto.\n\t(function_checker::report_out_of_range): Ditto.\n\t(function_checker::require_immediate): Ditto.\n\t(function_checker::require_immediate_range): Ditto.\n\t(function_checker::check): Ditto.\n\t(check_builtin_call): Ditto.\n\t* config/riscv/riscv-vector-builtins.def (DEF_RVV_TYPE): New def.\n\t(DEF_RVV_BASE_TYPE): Ditto.\n\t(DEF_RVV_TYPE_INDEX): Ditto.\n\t(vbool64_t): Ditto.\n\t(vbool32_t): Ditto.\n\t(vbool16_t): Ditto.\n\t(vbool8_t): Ditto.\n\t(vbool4_t): Ditto.\n\t(vbool2_t): Ditto.\n\t(vbool1_t): Ditto.\n\t(vuint8mf8_t): Ditto.\n\t(vuint8mf4_t): Ditto.\n\t(vuint8mf2_t): Ditto.\n\t(vuint8m1_t): Ditto.\n\t(vuint8m2_t): Ditto.\n\t(vint8m4_t): Ditto.\n\t(vuint8m4_t): Ditto.\n\t(vint8m8_t): Ditto.\n\t(vuint8m8_t): Ditto.\n\t(vint16mf4_t): Ditto.\n\t(vuint16mf2_t): Ditto.\n\t(vuint16m1_t): Ditto.\n\t(vuint16m2_t): Ditto.\n\t(vuint16m4_t): Ditto.\n\t(vuint16m8_t): Ditto.\n\t(vint32mf2_t): Ditto.\n\t(vuint32m1_t): Ditto.\n\t(vuint32m2_t): Ditto.\n\t(vuint32m4_t): Ditto.\n\t(vuint32m8_t): Ditto.\n\t(vuint64m1_t): Ditto.\n\t(vuint64m2_t): Ditto.\n\t(vuint64m4_t): Ditto.\n\t(vuint64m8_t): Ditto.\n\t(vfloat32mf2_t): Ditto.\n\t(vfloat32m1_t): Ditto.\n\t(vfloat32m2_t): Ditto.\n\t(vfloat32m4_t): Ditto.\n\t(vfloat32m8_t): Ditto.\n\t(vfloat64m1_t): Ditto.\n\t(vfloat64m4_t): Ditto.\n\t(vector): Move it def.\n\t(scalar): Ditto.\n\t(mask): Ditto.\n\t(signed_vector): Ditto.\n\t(unsigned_vector): Ditto.\n\t(unsigned_scalar): Ditto.\n\t(vector_ptr): Ditto.\n\t(scalar_ptr): Ditto.\n\t(scalar_const_ptr): Ditto.\n\t(void): Ditto.\n\t(size): Ditto.\n\t(ptrdiff): Ditto.\n\t(unsigned_long): Ditto.\n\t(long): Ditto.\n\t(eew8_index): Ditto.\n\t(eew16_index): Ditto.\n\t(eew32_index): Ditto.\n\t(eew64_index): Ditto.\n\t(shift_vector): Ditto.\n\t(double_trunc_vector): Ditto.\n\t(quad_trunc_vector): Ditto.\n\t(oct_trunc_vector): Ditto.\n\t(double_trunc_scalar): Ditto.\n\t(double_trunc_signed_vector): Ditto.\n\t(double_trunc_unsigned_vector): Ditto.\n\t(double_trunc_unsigned_scalar): Ditto.\n\t(double_trunc_float_vector): Ditto.\n\t(float_vector): Ditto.\n\t(lmul1_vector): Ditto.\n\t(widen_lmul1_vector): Ditto.\n\t(eew8_interpret): Ditto.\n\t(eew16_interpret): Ditto.\n\t(eew32_interpret): Ditto.\n\t(eew64_interpret): Ditto.\n\t(vlmul_ext_x2): Ditto.\n\t(vlmul_ext_x4): Ditto.\n\t(vlmul_ext_x8): Ditto.\n\t(vlmul_ext_x16): Ditto.\n\t(vlmul_ext_x32): Ditto.\n\t(vlmul_ext_x64): Ditto.\n\t* config/riscv/riscv-vector-builtins.h (DEF_RVV_BASE_TYPE): New def.\n\t(struct function_type_info): New function.\n\t(struct rvv_arg_type_info): Ditto.\n\t(class function_checker): New class.\n\t(rvv_arg_type_info::get_scalar_type): New function.\n\t(rvv_arg_type_info::get_vector_type): Ditto.\n\t(function_expander::ret_mode): New function.\n\t(function_checker::arg_mode): Ditto.\n\t(function_checker::ret_mode): Ditto.\n\t* config/riscv/t-riscv: Add generator.\n\t* config/riscv/vector-iterators.md: New iterators.\n\t* config/riscv/vector.md (vundefined<mode>): New pattern.\n\t(@vundefined<mode>): Ditto.\n\t(@vreinterpret<mode>): Ditto.\n\t(@vlmul_extx2<mode>): Ditto.\n\t(@vlmul_extx4<mode>): Ditto.\n\t(@vlmul_extx8<mode>): Ditto.\n\t(@vlmul_extx16<mode>): Ditto.\n\t(@vlmul_extx32<mode>): Ditto.\n\t(@vlmul_extx64<mode>): Ditto.\n\t(*vlmul_extx2<mode>): Ditto.\n\t(*vlmul_extx4<mode>): Ditto.\n\t(*vlmul_extx8<mode>): Ditto.\n\t(*vlmul_extx16<mode>): Ditto.\n\t(*vlmul_extx32<mode>): Ditto.\n\t(*vlmul_extx64<mode>): Ditto.\n\t* config/riscv/genrvv-type-indexer.cc: New file.\n\ngcc/testsuite/ChangeLog:\n\n\t* gcc.target/riscv/rvv/base/vlmul_v.c: New test.\n\nCo-authored-by: kito-cheng <kito.cheng@sifive.com>", "tree": {"sha": "667d6c0cb39ca387809124f594107f1106c0b995", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/667d6c0cb39ca387809124f594107f1106c0b995"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/comments", "author": {"login": "zhongjuzhe", "id": 66454988, "node_id": "MDQ6VXNlcjY2NDU0OTg4", "avatar_url": "https://avatars.githubusercontent.com/u/66454988?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhongjuzhe", "html_url": "https://github.com/zhongjuzhe", "followers_url": "https://api.github.com/users/zhongjuzhe/followers", "following_url": "https://api.github.com/users/zhongjuzhe/following{/other_user}", "gists_url": "https://api.github.com/users/zhongjuzhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhongjuzhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhongjuzhe/subscriptions", "organizations_url": "https://api.github.com/users/zhongjuzhe/orgs", "repos_url": "https://api.github.com/users/zhongjuzhe/repos", "events_url": "https://api.github.com/users/zhongjuzhe/events{/privacy}", "received_events_url": "https://api.github.com/users/zhongjuzhe/received_events", "type": "User", "site_admin": false}, "committer": {"login": "kito-cheng", "id": 2723185, "node_id": "MDQ6VXNlcjI3MjMxODU=", "avatar_url": "https://avatars.githubusercontent.com/u/2723185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kito-cheng", "html_url": "https://github.com/kito-cheng", "followers_url": "https://api.github.com/users/kito-cheng/followers", "following_url": "https://api.github.com/users/kito-cheng/following{/other_user}", "gists_url": "https://api.github.com/users/kito-cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/kito-cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kito-cheng/subscriptions", "organizations_url": "https://api.github.com/users/kito-cheng/orgs", "repos_url": "https://api.github.com/users/kito-cheng/repos", "events_url": "https://api.github.com/users/kito-cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/kito-cheng/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "1bff101b7e66feed0efc7f656468647e0b5fb48c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1bff101b7e66feed0efc7f656468647e0b5fb48c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1bff101b7e66feed0efc7f656468647e0b5fb48c"}], "stats": {"total": 3996, "additions": 3610, "deletions": 386}, "files": [{"sha": "0ef1d76600254da136294b29906a07459bf4678f", "filename": "gcc/config/riscv/genrvv-type-indexer.cc", "status": "added", "additions": 313, "deletions": 0, "changes": 313, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Fgenrvv-type-indexer.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Fgenrvv-type-indexer.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Fgenrvv-type-indexer.cc?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -0,0 +1,313 @@\n+/* Generate the RVV type indexer tables.\n+   Copyright (C) 2023-2023 Free Software Foundation, Inc.\n+This file is part of GCC.\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"bconfig.h\"\n+#include \"system.h\"\n+#include \"errors.h\"\n+\n+#include \"coretypes.h\"\n+\n+#include <sstream>\n+#include <assert.h>\n+#include <math.h>\n+\n+std::string\n+to_lmul (int lmul_log2)\n+{\n+  std::stringstream lmul_str;\n+  if (lmul_log2 >= 0)\n+    lmul_str << \"m\";\n+  else\n+    {\n+      lmul_str << \"mf\";\n+      lmul_log2 = -lmul_log2;\n+    }\n+\n+  lmul_str << (1 << lmul_log2);\n+  return lmul_str.str ();\n+}\n+\n+bool\n+valid_type (unsigned sew, int lmul_log2, bool float_p)\n+{\n+  if (lmul_log2 > 3)\n+    return false;\n+\n+  switch (sew)\n+    {\n+    case 8:\n+      return lmul_log2 >= -3 && !float_p;\n+    case 16:\n+      return lmul_log2 >= -2 && !float_p;\n+    case 32:\n+      return lmul_log2 >= -1;\n+    case 64:\n+      return lmul_log2 >= 0;\n+    default:\n+      return false;\n+    }\n+}\n+\n+std::string\n+inttype (unsigned sew, int lmul_log2, bool unsigned_p)\n+{\n+  if (!valid_type (sew, lmul_log2, /*float_t*/ false))\n+    return \"INVALID\";\n+\n+  std::stringstream mode;\n+  mode << \"v\";\n+  if (unsigned_p)\n+    mode << \"u\";\n+  mode << \"int\" << sew << to_lmul (lmul_log2) << \"_t\";\n+  return mode.str ();\n+}\n+\n+std::string\n+floattype (unsigned sew, int lmul_log2)\n+{\n+  if (!valid_type (sew, lmul_log2, /*float_t*/ true))\n+    return \"INVALID\";\n+\n+  std::stringstream mode;\n+  mode << \"vfloat\" << sew << to_lmul (lmul_log2) << \"_t\";\n+  return mode.str ();\n+}\n+\n+std::string\n+maskmode (unsigned sew, int lmul_log2)\n+{\n+  if (!valid_type (sew, lmul_log2, /*float_t*/ false))\n+    return \"INVALID\";\n+\n+  std::stringstream mode;\n+\n+  int mlen;\n+  if (lmul_log2 >= 0)\n+    mlen = sew / (1 << lmul_log2);\n+  else\n+    mlen = sew * (1 << -lmul_log2);\n+\n+  mode << \"vbool\" << mlen << \"_t\";\n+  return mode.str ();\n+}\n+\n+std::string\n+same_ratio_eew_type (unsigned sew, int lmul_log2, unsigned eew, bool unsigned_p,\n+\t\t     bool float_p)\n+{\n+  if (!valid_type (sew, lmul_log2, float_p))\n+    return \"INVALID\";\n+\n+  int elmul_log2;\n+\n+  if (sew == eew)\n+    elmul_log2 = lmul_log2;\n+  else if (sew > eew)\n+    elmul_log2 = lmul_log2 - std::log2 (sew / eew);\n+  else /* sew < eew */\n+    elmul_log2 = lmul_log2 + std::log2 (eew / sew);\n+\n+  if (float_p)\n+    return floattype (eew, elmul_log2);\n+  else\n+    return inttype (eew, elmul_log2, unsigned_p);\n+}\n+\n+int\n+main (int argc, const char **argv)\n+{\n+  // Require at least one argument.\n+  if (argc < 2)\n+    return 1;\n+\n+  FILE *fp = fopen (argv[1], \"w\");\n+\n+  if (!fp)\n+    return 1;\n+\n+  fprintf (fp, \"/* Generated by genrvv-type-indexer */\\n\");\n+\n+  for (unsigned vbool : {64, 32, 16, 8, 4, 2, 1})\n+    {\n+      std::stringstream mode;\n+      mode << \"vbool\" << vbool << \"_t\";\n+      fprintf (fp, \"DEF_RVV_TYPE_INDEX (\\n\");\n+      fprintf (fp, \"  /*VECTOR*/ %s,\\n\", mode.str ().c_str ());\n+      fprintf (fp, \"  /*MASK*/ %s,\\n\", mode.str ().c_str ());\n+      fprintf (fp, \"  /*SIGNED*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*UNSIGNED*/ INVALID,\\n\");\n+      for (unsigned eew : {8, 16, 32, 64})\n+\tfprintf (fp, \"  /*EEW%d_INDEX*/ INVALID,\\n\", eew);\n+      fprintf (fp, \"  /*SHIFT*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*DOUBLE_TRUNC*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*QUAD_TRUNC*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*OCT_TRUNC*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*DOUBLE_TRUNC_SCALAR*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*DOUBLE_TRUNC_SIGNED*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*DOUBLE_TRUNC_UNSIGNED*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*DOUBLE_TRUNC_UNSIGNED_SCALAR*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*DOUBLE_TRUNC_FLOAT*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*FLOAT*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*LMUL1*/ INVALID,\\n\");\n+      fprintf (fp, \"  /*WLMUL1*/ INVALID,\\n\");\n+      for (unsigned eew : {8, 16, 32, 64})\n+\tfprintf (fp, \"  /*EEW%d_INTERPRET*/ INVALID,\\n\", eew);\n+\n+      for (unsigned lmul_log2_offset : {1, 2, 3, 4, 5, 6})\n+\t{\n+\t  unsigned multiple_of_lmul = 1 << lmul_log2_offset;\n+\t  const char *comma = lmul_log2_offset == 6 ? \"\" : \",\";\n+\t  fprintf (fp, \"  /*X%d_INTERPRET*/ INVALID%s\\n\", multiple_of_lmul,\n+\t\t   comma);\n+\t}\n+      fprintf (fp, \")\\n\");\n+    }\n+\n+  // Build for vint and vuint\n+  for (unsigned sew : {8, 16, 32, 64})\n+    for (int lmul_log2 : {-3, -2, -1, 0, 1, 2, 3})\n+      for (bool unsigned_p : {false, true})\n+\t{\n+\t  if (!valid_type (sew, lmul_log2, /*float_t*/ false))\n+\t    continue;\n+\n+\t  fprintf (fp, \"DEF_RVV_TYPE_INDEX (\\n\");\n+\t  fprintf (fp, \"  /*VECTOR*/ %s,\\n\",\n+\t\t   inttype (sew, lmul_log2, unsigned_p).c_str ());\n+\t  fprintf (fp, \"  /*MASK*/ %s,\\n\", maskmode (sew, lmul_log2).c_str ());\n+\t  fprintf (fp, \"  /*SIGNED*/ %s,\\n\",\n+\t\t   inttype (sew, lmul_log2, /*unsigned_p*/ false).c_str ());\n+\t  fprintf (fp, \"  /*UNSIGNED*/ %s,\\n\",\n+\t\t   inttype (sew, lmul_log2, /*unsigned_p*/ true).c_str ());\n+\t  for (unsigned eew : {8, 16, 32, 64})\n+\t    fprintf (fp, \"  /*EEW%d_INDEX*/ %s,\\n\", eew,\n+\t\t     same_ratio_eew_type (sew, lmul_log2, eew,\n+\t\t\t\t\t  /*unsigned_p*/ true, false)\n+\t\t       .c_str ());\n+\t  fprintf (fp, \"  /*SHIFT*/ %s,\\n\",\n+\t\t   inttype (sew, lmul_log2, /*unsigned_p*/ true).c_str ());\n+\t  fprintf (fp, \"  /*DOUBLE_TRUNC*/ %s,\\n\",\n+\t\t   same_ratio_eew_type (sew, lmul_log2, sew / 2, unsigned_p,\n+\t\t\t\t\tfalse)\n+\t\t     .c_str ());\n+\t  fprintf (fp, \"  /*QUAD_TRUNC*/ %s,\\n\",\n+\t\t   same_ratio_eew_type (sew, lmul_log2, sew / 4, unsigned_p,\n+\t\t\t\t\tfalse)\n+\t\t     .c_str ());\n+\t  fprintf (fp, \"  /*OCT_TRUNC*/ %s,\\n\",\n+\t\t   same_ratio_eew_type (sew, lmul_log2, sew / 8, unsigned_p,\n+\t\t\t\t\tfalse)\n+\t\t     .c_str ());\n+\t  fprintf (fp, \"  /*DOUBLE_TRUNC_SCALAR*/ %s,\\n\",\n+\t\t   same_ratio_eew_type (sew, lmul_log2, sew / 2, unsigned_p,\n+\t\t\t\t\tfalse)\n+\t\t     .c_str ());\n+\t  fprintf (fp, \"  /*DOUBLE_TRUNC_SIGNED*/ INVALID,\\n\");\n+\t  fprintf (fp, \"  /*DOUBLE_TRUNC_UNSIGNED*/ %s,\\n\",\n+\t\t   same_ratio_eew_type (sew, lmul_log2, sew / 2, true, false)\n+\t\t     .c_str ());\n+\t  if (unsigned_p)\n+\t    fprintf (fp, \"  /*DOUBLE_TRUNC_UNSIGNED_SCALAR*/ INVALID,\\n\");\n+\t  else\n+\t    fprintf (fp, \"  /*DOUBLE_TRUNC_UNSIGNED_SCALAR*/ %s,\\n\",\n+\t\t     same_ratio_eew_type (sew, lmul_log2, sew / 2, true, false)\n+\t\t       .c_str ());\n+\t  fprintf (fp, \"  /*DOUBLE_TRUNC_FLOAT*/ %s,\\n\",\n+\t\t   same_ratio_eew_type (sew, lmul_log2, sew / 2, false, true)\n+\t\t     .c_str ());\n+\t  fprintf (fp, \"  /*FLOAT*/ %s,\\n\",\n+\t\t   floattype (sew, lmul_log2).c_str ());\n+\t  fprintf (fp, \"  /*LMUL1*/ %s,\\n\",\n+\t\t   inttype (sew, /*lmul_log2*/ 0, unsigned_p).c_str ());\n+\t  fprintf (fp, \"  /*WLMUL1*/ %s,\\n\",\n+\t\t   inttype (sew * 2, /*lmul_log2*/ 0, unsigned_p).c_str ());\n+\t  for (unsigned eew : {8, 16, 32, 64})\n+\t    {\n+\t      if (eew == sew)\n+\t\tfprintf (fp, \"  /*EEW%d_INTERPRET*/ INVALID,\\n\", eew);\n+\t      else\n+\t\tfprintf (fp, \"  /*EEW%d_INTERPRET*/ %s,\\n\", eew,\n+\t\t\t inttype (eew, lmul_log2, unsigned_p).c_str ());\n+\t    }\n+\n+\t  for (unsigned lmul_log2_offset : {1, 2, 3, 4, 5, 6})\n+\t    {\n+\t      unsigned multiple_of_lmul = 1 << lmul_log2_offset;\n+\t      const char *comma = lmul_log2_offset == 6 ? \"\" : \",\";\n+\t      fprintf (fp, \"  /*X%d_VLMUL_EXT*/ %s%s\\n\", multiple_of_lmul,\n+\t\t       inttype (sew, lmul_log2 + lmul_log2_offset, unsigned_p)\n+\t\t\t .c_str (),\n+\t\t       comma);\n+\t    }\n+\t  fprintf (fp, \")\\n\");\n+\t}\n+  // Build for vfloat\n+  for (unsigned sew : {32, 64})\n+    for (int lmul_log2 : {-3, -2, -1, 0, 1, 2, 3})\n+      {\n+\tif (!valid_type (sew, lmul_log2, /*float_t*/ true))\n+\t  continue;\n+\n+\tfprintf (fp, \"DEF_RVV_TYPE_INDEX (\\n\");\n+\tfprintf (fp, \"  /*VECTOR*/ %s,\\n\", floattype (sew, lmul_log2).c_str ());\n+\tfprintf (fp, \"  /*MASK*/ %s,\\n\", maskmode (sew, lmul_log2).c_str ());\n+\tfprintf (fp, \"  /*SIGNED*/ %s,\\n\",\n+\t\t inttype (sew, lmul_log2, /*unsigned_p*/ false).c_str ());\n+\tfprintf (fp, \"  /*UNSIGNED*/ %s,\\n\",\n+\t\t inttype (sew, lmul_log2, /*unsigned_p*/ true).c_str ());\n+\tfor (unsigned eew : {8, 16, 32, 64})\n+\t  fprintf (fp, \"  /*EEW%d_INDEX*/ %s,\\n\", eew,\n+\t\t   same_ratio_eew_type (sew, lmul_log2, eew,\n+\t\t\t\t\t/*unsigned_p*/ true, false)\n+\t\t     .c_str ());\n+\tfprintf (fp, \"  /*SHIFT*/ INVALID,\\n\");\n+\tfprintf (\n+\t  fp, \"  /*DOUBLE_TRUNC*/ %s,\\n\",\n+\t  same_ratio_eew_type (sew, lmul_log2, sew / 2, false, true).c_str ());\n+\tfprintf (fp, \"  /*QUAD_TRUNC*/ INVALID,\\n\");\n+\tfprintf (fp, \"  /*OCT_TRUNC*/ INVALID,\\n\");\n+\tfprintf (\n+\t  fp, \"  /*DOUBLE_TRUNC_SCALAR*/ %s,\\n\",\n+\t  same_ratio_eew_type (sew, lmul_log2, sew / 2, false, true).c_str ());\n+\tfprintf (\n+\t  fp, \"  /*DOUBLE_TRUNC_SIGNED*/ %s,\\n\",\n+\t  same_ratio_eew_type (sew, lmul_log2, sew / 2, false, false).c_str ());\n+\tfprintf (\n+\t  fp, \"  /*DOUBLE_TRUNC_UNSIGNED*/ %s,\\n\",\n+\t  same_ratio_eew_type (sew, lmul_log2, sew / 2, true, false).c_str ());\n+\tfprintf (fp, \"  /*DOUBLE_TRUNC_UNSIGNED_SCALAR*/ INVALID,\\n\");\n+\tfprintf (\n+\t  fp, \"  /*DOUBLE_TRUNC_FLOAT*/ %s,\\n\",\n+\t  same_ratio_eew_type (sew, lmul_log2, sew / 2, false, true).c_str ());\n+\tfprintf (fp, \"  /*FLOAT*/ INVALID,\\n\");\n+\tfprintf (fp, \"  /*LMUL1*/ %s,\\n\",\n+\t\t floattype (sew, /*lmul_log2*/ 0).c_str ());\n+\tfprintf (fp, \"  /*WLMUL1*/ %s,\\n\",\n+\t\t floattype (sew * 2, /*lmul_log2*/ 0).c_str ());\n+\tfor (unsigned eew : {8, 16, 32, 64})\n+\t  fprintf (fp, \"  /*EEW%d_INTERPRET*/ INVALID,\\n\", eew);\n+\tfor (unsigned lmul_log2_offset : {1, 2, 3, 4, 5, 6})\n+\t  {\n+\t    unsigned multiple_of_lmul = 1 << lmul_log2_offset;\n+\t    const char *comma = lmul_log2_offset == 6 ? \"\" : \",\";\n+\t    fprintf (fp, \"  /*X%d_VLMUL_EXT*/ %s%s\\n\", multiple_of_lmul,\n+\t\t     floattype (sew, lmul_log2 + lmul_log2_offset).c_str (),\n+\t\t     comma);\n+\t  }\n+\tfprintf (fp, \")\\n\");\n+      }\n+\n+  return 0;\n+}"}, {"sha": "0d9d7701c7ead1eea22a30355bc5da0264c332de", "filename": "gcc/config/riscv/predicates.md", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Fpredicates.md?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -346,6 +346,10 @@\n   (ior (match_operand 0 \"const_0_operand\")\n        (match_operand 0 \"pmode_register_operand\")))\n \n+;; A special predicate that doesn't match a particular mode.\n+(define_special_predicate \"vector_any_register_operand\"\n+  (match_code \"reg\"))\n+\n ;; The scalar operand can be directly broadcast by RVV instructions.\n (define_predicate \"direct_broadcast_operand\"\n   (and (match_test \"!(reload_completed && !FLOAT_MODE_P (GET_MODE (op))"}, {"sha": "ff07d319d0b4808becca7e3f57e565f41e4da9dc", "filename": "gcc/config/riscv/riscv-c.cc", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-c.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-c.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-c.cc?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -184,10 +184,30 @@ riscv_pragma_intrinsic (cpp_reader *)\n     error (\"unknown %<#pragma riscv intrinsic%> option %qs\", name);\n }\n \n+/* Implement TARGET_CHECK_BUILTIN_CALL.  */\n+static bool\n+riscv_check_builtin_call (location_t loc, vec<location_t> arg_loc, tree fndecl,\n+\t\t\t  tree orig_fndecl, unsigned int nargs, tree *args)\n+{\n+  unsigned int code = DECL_MD_FUNCTION_CODE (fndecl);\n+  unsigned int subcode = code >> RISCV_BUILTIN_SHIFT;\n+  switch (code & RISCV_BUILTIN_CLASS)\n+    {\n+    case RISCV_BUILTIN_GENERAL:\n+      return true;\n+\n+    case RISCV_BUILTIN_VECTOR:\n+      return riscv_vector::check_builtin_call (loc, arg_loc, subcode,\n+\t\t\t\t\t       orig_fndecl, nargs, args);\n+    }\n+  gcc_unreachable ();\n+}\n+\n /* Implement REGISTER_TARGET_PRAGMAS.  */\n \n void\n riscv_register_pragmas (void)\n {\n+  targetm.check_builtin_call = riscv_check_builtin_call;\n   c_register_pragma (\"riscv\", \"intrinsic\", riscv_pragma_intrinsic);\n }"}, {"sha": "88a6bf5442f63b6c584ddb37db840788a6ca004b", "filename": "gcc/config/riscv/riscv-protos.h", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-protos.h?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -122,7 +122,8 @@ void riscv_run_selftests (void);\n namespace riscv_vector {\n #define RVV_VLMAX gen_rtx_REG (Pmode, X0_REGNUM)\n #define RVV_VUNDEF(MODE)                                                       \\\n-  gen_rtx_UNSPEC (MODE, gen_rtvec (1, const0_rtx), UNSPEC_VUNDEF)\n+  gen_rtx_UNSPEC (MODE, gen_rtvec (1, gen_rtx_REG (SImode, X0_REGNUM)),        \\\n+\t\t  UNSPEC_VUNDEF)\n enum vlmul_type\n {\n   LMUL_1 = 0,\n@@ -150,6 +151,8 @@ bool verify_type_context (location_t, type_context_kind, const_tree, bool);\n void handle_pragma_vector (void);\n tree builtin_decl (unsigned, bool);\n rtx expand_builtin (unsigned int, tree, rtx);\n+bool check_builtin_call (location_t, vec<location_t>, unsigned int,\n+\t\t\t   tree, unsigned int, tree *);\n bool const_vec_all_same_in_range_p (rtx, HOST_WIDE_INT, HOST_WIDE_INT);\n bool legitimize_move (rtx, rtx, machine_mode);\n void emit_vlmax_op (unsigned, rtx, rtx, machine_mode);"}, {"sha": "533f40487b674f4540be8d7794103c196548d0cb", "filename": "gcc/config/riscv/riscv-vector-builtins-bases.cc", "status": "modified", "additions": 126, "deletions": 0, "changes": 126, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-bases.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-bases.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-bases.cc?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -1422,6 +1422,120 @@ class vcompress : public function_base\n   }\n };\n \n+class vundefined : public function_base\n+{\n+public:\n+  bool apply_vl_p () const override\n+  {\n+    return false;\n+  }\n+\n+  rtx expand (function_expander &e) const override\n+  {\n+    return e.generate_insn (code_for_vundefined (e.vector_mode ()));\n+  }\n+};\n+\n+class vreinterpret : public function_base\n+{\n+public:\n+  bool apply_vl_p () const override\n+  {\n+    return false;\n+  }\n+\n+  rtx expand (function_expander &e) const override\n+  {\n+    e.add_input_operand (0);\n+    return e.generate_insn (code_for_vreinterpret (e.ret_mode ()));\n+  }\n+};\n+\n+class vlmul_ext : public function_base\n+{\n+public:\n+  bool apply_vl_p () const override\n+  {\n+    return false;\n+  }\n+\n+  rtx expand (function_expander &e) const override\n+  {\n+    e.add_input_operand (0);\n+    switch (e.op_info->ret.base_type)\n+      {\n+      case RVV_BASE_vlmul_ext_x2:\n+\treturn e.generate_insn (\n+\t  code_for_vlmul_extx2 (e.vector_mode ()));\n+      case RVV_BASE_vlmul_ext_x4:\n+\treturn e.generate_insn (\n+\t  code_for_vlmul_extx4 (e.vector_mode ()));\n+      case RVV_BASE_vlmul_ext_x8:\n+\treturn e.generate_insn (\n+\t  code_for_vlmul_extx8 (e.vector_mode ()));\n+      case RVV_BASE_vlmul_ext_x16:\n+\treturn e.generate_insn (\n+\t  code_for_vlmul_extx16 (e.vector_mode ()));\n+      case RVV_BASE_vlmul_ext_x32:\n+\treturn e.generate_insn (\n+\t  code_for_vlmul_extx32 (e.vector_mode ()));\n+      case RVV_BASE_vlmul_ext_x64:\n+\treturn e.generate_insn (\n+\t  code_for_vlmul_extx64 (e.vector_mode ()));\n+      default:\n+\tgcc_unreachable ();\n+      }\n+  }\n+};\n+\n+class vlmul_trunc : public function_base\n+{\n+public:\n+  bool apply_vl_p () const override { return false; }\n+\n+  rtx expand (function_expander &e) const override\n+  {\n+    rtx src = expand_normal (CALL_EXPR_ARG (e.exp, 0));\n+    emit_move_insn (e.target, gen_lowpart (GET_MODE (e.target), src));\n+    return e.target;\n+  }\n+};\n+\n+class vset : public function_base\n+{\n+public:\n+  bool apply_vl_p () const override { return false; }\n+\n+  rtx expand (function_expander &e) const override\n+  {\n+    rtx dest = expand_normal (CALL_EXPR_ARG (e.exp, 0));\n+    rtx index = expand_normal (CALL_EXPR_ARG (e.exp, 1));\n+    rtx src = expand_normal (CALL_EXPR_ARG (e.exp, 2));\n+    poly_int64 offset = INTVAL (index) * GET_MODE_SIZE (GET_MODE (src));\n+    emit_move_insn (e.target, dest);\n+    rtx subreg = simplify_gen_subreg (GET_MODE (src), e.target,\n+\t\t\t\t      GET_MODE (e.target), offset);\n+    emit_move_insn (subreg, src);\n+    return e.target;\n+  }\n+};\n+\n+class vget : public function_base\n+{\n+public:\n+  bool apply_vl_p () const override { return false; }\n+\n+  rtx expand (function_expander &e) const override\n+  {\n+    rtx src = expand_normal (CALL_EXPR_ARG (e.exp, 0));\n+    rtx index = expand_normal (CALL_EXPR_ARG (e.exp, 1));\n+    poly_int64 offset = INTVAL (index) * GET_MODE_SIZE (GET_MODE (src));\n+    rtx subreg\n+      = simplify_gen_subreg (GET_MODE (e.target), src, GET_MODE (src), offset);\n+    return subreg;\n+  }\n+};\n+\n static CONSTEXPR const vsetvl<false> vsetvl_obj;\n static CONSTEXPR const vsetvl<true> vsetvlmax_obj;\n static CONSTEXPR const loadstore<false, LST_UNIT_STRIDE, false> vle_obj;\n@@ -1624,6 +1738,12 @@ static CONSTEXPR const slideop<UNSPEC_VFSLIDE1DOWN> vfslide1down_obj;\n static CONSTEXPR const vrgather vrgather_obj;\n static CONSTEXPR const vrgatherei16 vrgatherei16_obj;\n static CONSTEXPR const vcompress vcompress_obj;\n+static CONSTEXPR const vundefined vundefined_obj;\n+static CONSTEXPR const vreinterpret vreinterpret_obj;\n+static CONSTEXPR const vlmul_ext vlmul_ext_obj;\n+static CONSTEXPR const vlmul_trunc vlmul_trunc_obj;\n+static CONSTEXPR const vset vset_obj;\n+static CONSTEXPR const vget vget_obj;\n \n /* Declare the function base NAME, pointing it to an instance\n    of class <NAME>_obj.  */\n@@ -1832,5 +1952,11 @@ BASE (vfslide1down)\n BASE (vrgather)\n BASE (vrgatherei16)\n BASE (vcompress)\n+BASE (vundefined)\n+BASE (vreinterpret)\n+BASE (vlmul_ext)\n+BASE (vlmul_trunc)\n+BASE (vset)\n+BASE (vget)\n \n } // end namespace riscv_vector"}, {"sha": "5e05b35b084095100c2d884f93852586c98c0fac", "filename": "gcc/config/riscv/riscv-vector-builtins-bases.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-bases.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-bases.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-bases.h?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -232,6 +232,12 @@ extern const function_base *const vfslide1down;\n extern const function_base *const vrgather;\n extern const function_base *const vrgatherei16;\n extern const function_base *const vcompress;\n+extern const function_base *const vundefined;\n+extern const function_base *const vreinterpret;\n+extern const function_base *const vlmul_ext;\n+extern const function_base *const vlmul_trunc;\n+extern const function_base *const vset;\n+extern const function_base *const vget;\n }\n \n } // end namespace riscv_vector"}, {"sha": "c0d752e569f3a06d7e91360c508c042aae0e19a6", "filename": "gcc/config/riscv/riscv-vector-builtins-functions.def", "status": "modified", "additions": 53, "deletions": 16, "changes": 69, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-functions.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-functions.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-functions.def?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -54,22 +54,22 @@ DEF_RVV_FUNCTION (vlse, loadstore, full_preds, all_v_scalar_const_ptr_ptrdiff_op\n DEF_RVV_FUNCTION (vsse, loadstore, none_m_preds, all_v_scalar_ptr_ptrdiff_ops)\n \n // 7.6. Vector Indexed Instructions\n-DEF_RVV_FUNCTION (vluxei8, indexed_loadstore, full_preds, all_v_scalar_const_ptr_uint8_index_ops)\n-DEF_RVV_FUNCTION (vluxei16, indexed_loadstore, full_preds, all_v_scalar_const_ptr_uint16_index_ops)\n-DEF_RVV_FUNCTION (vluxei32, indexed_loadstore, full_preds, all_v_scalar_const_ptr_uint32_index_ops)\n-DEF_RVV_FUNCTION (vluxei64, indexed_loadstore, full_preds, all_v_scalar_const_ptr_uint64_index_ops)\n-DEF_RVV_FUNCTION (vloxei8, indexed_loadstore, full_preds, all_v_scalar_const_ptr_uint8_index_ops)\n-DEF_RVV_FUNCTION (vloxei16, indexed_loadstore, full_preds, all_v_scalar_const_ptr_uint16_index_ops)\n-DEF_RVV_FUNCTION (vloxei32, indexed_loadstore, full_preds, all_v_scalar_const_ptr_uint32_index_ops)\n-DEF_RVV_FUNCTION (vloxei64, indexed_loadstore, full_preds, all_v_scalar_const_ptr_uint64_index_ops)\n-DEF_RVV_FUNCTION (vsuxei8, indexed_loadstore, none_m_preds, all_v_scalar_ptr_uint8_index_ops)\n-DEF_RVV_FUNCTION (vsuxei16, indexed_loadstore, none_m_preds, all_v_scalar_ptr_uint16_index_ops)\n-DEF_RVV_FUNCTION (vsuxei32, indexed_loadstore, none_m_preds, all_v_scalar_ptr_uint32_index_ops)\n-DEF_RVV_FUNCTION (vsuxei64, indexed_loadstore, none_m_preds, all_v_scalar_ptr_uint64_index_ops)\n-DEF_RVV_FUNCTION (vsoxei8, indexed_loadstore, none_m_preds, all_v_scalar_ptr_uint8_index_ops)\n-DEF_RVV_FUNCTION (vsoxei16, indexed_loadstore, none_m_preds, all_v_scalar_ptr_uint16_index_ops)\n-DEF_RVV_FUNCTION (vsoxei32, indexed_loadstore, none_m_preds, all_v_scalar_ptr_uint32_index_ops)\n-DEF_RVV_FUNCTION (vsoxei64, indexed_loadstore, none_m_preds, all_v_scalar_ptr_uint64_index_ops)\n+DEF_RVV_FUNCTION (vluxei8, indexed_loadstore, full_preds, all_v_scalar_const_ptr_eew8_index_ops)\n+DEF_RVV_FUNCTION (vluxei16, indexed_loadstore, full_preds, all_v_scalar_const_ptr_eew16_index_ops)\n+DEF_RVV_FUNCTION (vluxei32, indexed_loadstore, full_preds, all_v_scalar_const_ptr_eew32_index_ops)\n+DEF_RVV_FUNCTION (vluxei64, indexed_loadstore, full_preds, all_v_scalar_const_ptr_eew64_index_ops)\n+DEF_RVV_FUNCTION (vloxei8, indexed_loadstore, full_preds, all_v_scalar_const_ptr_eew8_index_ops)\n+DEF_RVV_FUNCTION (vloxei16, indexed_loadstore, full_preds, all_v_scalar_const_ptr_eew16_index_ops)\n+DEF_RVV_FUNCTION (vloxei32, indexed_loadstore, full_preds, all_v_scalar_const_ptr_eew32_index_ops)\n+DEF_RVV_FUNCTION (vloxei64, indexed_loadstore, full_preds, all_v_scalar_const_ptr_eew64_index_ops)\n+DEF_RVV_FUNCTION (vsuxei8, indexed_loadstore, none_m_preds, all_v_scalar_ptr_eew8_index_ops)\n+DEF_RVV_FUNCTION (vsuxei16, indexed_loadstore, none_m_preds, all_v_scalar_ptr_eew16_index_ops)\n+DEF_RVV_FUNCTION (vsuxei32, indexed_loadstore, none_m_preds, all_v_scalar_ptr_eew32_index_ops)\n+DEF_RVV_FUNCTION (vsuxei64, indexed_loadstore, none_m_preds, all_v_scalar_ptr_eew64_index_ops)\n+DEF_RVV_FUNCTION (vsoxei8, indexed_loadstore, none_m_preds, all_v_scalar_ptr_eew8_index_ops)\n+DEF_RVV_FUNCTION (vsoxei16, indexed_loadstore, none_m_preds, all_v_scalar_ptr_eew16_index_ops)\n+DEF_RVV_FUNCTION (vsoxei32, indexed_loadstore, none_m_preds, all_v_scalar_ptr_eew32_index_ops)\n+DEF_RVV_FUNCTION (vsoxei64, indexed_loadstore, none_m_preds, all_v_scalar_ptr_eew64_index_ops)\n \n // TODO: 7.7. Unit-stride Fault-Only-First Loads\n // TODO: 7.8. Vector Load/Store Segment Instructions\n@@ -490,4 +490,41 @@ DEF_RVV_FUNCTION (vrgatherei16, alu, full_preds, all_gatherei16_vvv_ops)\n // 16.5. Vector Compress Instruction\n DEF_RVV_FUNCTION (vcompress, alu, none_tu_preds, all_vvm_ops)\n \n+/* Miscellaneous Vector Functions.  */\n+DEF_RVV_FUNCTION (vundefined, vundefined, none_preds, all_none_void_ops)\n+DEF_RVV_FUNCTION (vreinterpret, misc, none_preds, i_v_u_ops)\n+DEF_RVV_FUNCTION (vreinterpret, misc, none_preds, u_v_i_ops)\n+DEF_RVV_FUNCTION (vreinterpret, misc, none_preds, f_v_i_ops)\n+DEF_RVV_FUNCTION (vreinterpret, misc, none_preds, f_v_u_ops)\n+DEF_RVV_FUNCTION (vreinterpret, misc, none_preds, i_v_f_ops)\n+DEF_RVV_FUNCTION (vreinterpret, misc, none_preds, u_v_f_ops)\n+DEF_RVV_FUNCTION (vreinterpret, misc, none_preds, iu_v_eew8_interpret_ops)\n+DEF_RVV_FUNCTION (vreinterpret, misc, none_preds, iu_v_eew16_interpret_ops)\n+DEF_RVV_FUNCTION (vreinterpret, misc, none_preds, iu_v_eew32_interpret_ops)\n+DEF_RVV_FUNCTION (vreinterpret, misc, none_preds, iu_v_eew64_interpret_ops)\n+DEF_RVV_FUNCTION (vlmul_ext, misc, none_preds, all_v_vlmul_ext_x2_ops)\n+DEF_RVV_FUNCTION (vlmul_ext, misc, none_preds, all_v_vlmul_ext_x4_ops)\n+DEF_RVV_FUNCTION (vlmul_ext, misc, none_preds, all_v_vlmul_ext_x8_ops)\n+DEF_RVV_FUNCTION (vlmul_ext, misc, none_preds, all_v_vlmul_ext_x16_ops)\n+DEF_RVV_FUNCTION (vlmul_ext, misc, none_preds, all_v_vlmul_ext_x32_ops)\n+DEF_RVV_FUNCTION (vlmul_ext, misc, none_preds, all_v_vlmul_ext_x64_ops)\n+DEF_RVV_FUNCTION (vlmul_trunc, misc, none_preds, all_v_vlmul_trunc_x2_ops)\n+DEF_RVV_FUNCTION (vlmul_trunc, misc, none_preds, all_v_vlmul_trunc_x4_ops)\n+DEF_RVV_FUNCTION (vlmul_trunc, misc, none_preds, all_v_vlmul_trunc_x8_ops)\n+DEF_RVV_FUNCTION (vlmul_trunc, misc, none_preds, all_v_vlmul_trunc_x16_ops)\n+DEF_RVV_FUNCTION (vlmul_trunc, misc, none_preds, all_v_vlmul_trunc_x32_ops)\n+DEF_RVV_FUNCTION (vlmul_trunc, misc, none_preds, all_v_vlmul_trunc_x64_ops)\n+DEF_RVV_FUNCTION (vset, vset, none_preds, all_v_vset_lmul1_x2_ops)\n+DEF_RVV_FUNCTION (vset, vset, none_preds, all_v_vset_lmul1_x4_ops)\n+DEF_RVV_FUNCTION (vset, vset, none_preds, all_v_vset_lmul1_x8_ops)\n+DEF_RVV_FUNCTION (vset, vset, none_preds, all_v_vset_lmul2_x2_ops)\n+DEF_RVV_FUNCTION (vset, vset, none_preds, all_v_vset_lmul2_x4_ops)\n+DEF_RVV_FUNCTION (vset, vset, none_preds, all_v_vset_lmul4_x2_ops)\n+DEF_RVV_FUNCTION (vget, vget, none_preds, all_v_vget_lmul1_x2_ops)\n+DEF_RVV_FUNCTION (vget, vget, none_preds, all_v_vget_lmul1_x4_ops)\n+DEF_RVV_FUNCTION (vget, vget, none_preds, all_v_vget_lmul1_x8_ops)\n+DEF_RVV_FUNCTION (vget, vget, none_preds, all_v_vget_lmul2_x2_ops)\n+DEF_RVV_FUNCTION (vget, vget, none_preds, all_v_vget_lmul2_x4_ops)\n+DEF_RVV_FUNCTION (vget, vget, none_preds, all_v_vget_lmul4_x2_ops)\n+\n #undef DEF_RVV_FUNCTION"}, {"sha": "2bf72e7af0a596626b99eb69bfe2efb51ce373f9", "filename": "gcc/config/riscv/riscv-vector-builtins-shapes.cc", "status": "modified", "additions": 89, "deletions": 6, "changes": 95, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-shapes.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-shapes.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-shapes.cc?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -277,8 +277,7 @@ struct return_mask_def : public build_base\n       {\n \tb.append_name (type_suffixes[instance.type.index].vector);\n \tvector_type_index ret_type_idx\n-\t  = instance.op_info->ret.get_base_vector_type (\n-\t    builtin_types[instance.type.index].vector);\n+\t  = instance.op_info->ret.get_function_type_index (instance.type.index);\n \tb.append_name (type_suffixes[ret_type_idx].vector);\n       }\n \n@@ -303,8 +302,7 @@ struct narrow_alu_def : public build_base\n \tb.append_name (operand_suffixes[instance.op_info->op]);\n \t/* vop_<op> --> vop_<op>_<type>.  */\n \tvector_type_index ret_type_idx\n-\t  = instance.op_info->ret.get_base_vector_type (\n-\t    builtin_types[instance.type.index].vector);\n+\t  = instance.op_info->ret.get_function_type_index (instance.type.index);\n \tb.append_name (type_suffixes[ret_type_idx].vector);\n       }\n \n@@ -388,8 +386,7 @@ struct reduc_alu_def : public build_base\n \tb.append_name (operand_suffixes[instance.op_info->op]);\n \tb.append_name (type_suffixes[instance.type.index].vector);\n \tvector_type_index ret_type_idx\n-\t  = instance.op_info->ret.get_base_vector_type (\n-\t    builtin_types[instance.type.index].vector);\n+\t  = instance.op_info->ret.get_function_type_index (instance.type.index);\n \tb.append_name (type_suffixes[ret_type_idx].vector);\n       }\n \n@@ -418,6 +415,88 @@ struct scalar_move_def : public build_base\n   }\n };\n \n+/* vundefined_def class.  */\n+struct vundefined_def : public build_base\n+{\n+  char *get_name (function_builder &b, const function_instance &instance,\n+\t\t  bool overloaded_p) const override\n+  {\n+    if (overloaded_p)\n+      return nullptr;\n+    b.append_base_name (instance.base_name);\n+    b.append_name (type_suffixes[instance.type.index].vector);\n+    return b.finish_name ();\n+  }\n+};\n+\n+/* misc_def class.  */\n+struct misc_def : public build_base\n+{\n+  char *get_name (function_builder &b, const function_instance &instance,\n+\t\t  bool overloaded_p) const override\n+  {\n+    b.append_base_name (instance.base_name);\n+\n+    if (!overloaded_p)\n+      {\n+\tb.append_name (operand_suffixes[instance.op_info->op]);\n+\tvector_type_index arg0_type_idx\n+\t  = instance.op_info->args[0].get_function_type_index (\n+\t    instance.type.index);\n+\tb.append_name (type_suffixes[arg0_type_idx].vector);\n+      }\n+\n+    vector_type_index ret_type_idx\n+      = instance.op_info->ret.get_function_type_index (instance.type.index);\n+    b.append_name (type_suffixes[ret_type_idx].vector);\n+    return b.finish_name ();\n+  }\n+};\n+\n+/* vset_def class.  */\n+struct vset_def : public build_base\n+{\n+  char *get_name (function_builder &b, const function_instance &instance,\n+\t\t  bool overloaded_p) const override\n+  {\n+    b.append_base_name (instance.base_name);\n+\n+    if (!overloaded_p)\n+      {\n+\tb.append_name (operand_suffixes[instance.op_info->op]);\n+\tvector_type_index arg_type_idx\n+\t  = instance.op_info->args[2].get_function_type_index (\n+\t    instance.type.index);\n+\tb.append_name (type_suffixes[arg_type_idx].vector);\n+\n+\tvector_type_index ret_type_idx\n+\t  = instance.op_info->ret.get_function_type_index (instance.type.index);\n+\tb.append_name (type_suffixes[ret_type_idx].vector);\n+      }\n+    return b.finish_name ();\n+  }\n+\n+  bool check (function_checker &c) const override\n+  {\n+    poly_int64 outer_size = GET_MODE_SIZE (c.arg_mode (0));\n+    poly_int64 inner_size = GET_MODE_SIZE (c.arg_mode (2));\n+    unsigned int nvecs = exact_div (outer_size, inner_size).to_constant ();\n+    return c.require_immediate (1, 0, nvecs - 1);\n+  }\n+};\n+\n+/* vget_def class.  */\n+struct vget_def : public misc_def\n+{\n+  bool check (function_checker &c) const override\n+  {\n+    poly_int64 outer_size = GET_MODE_SIZE (c.arg_mode (0));\n+    poly_int64 inner_size = GET_MODE_SIZE (c.ret_mode ());\n+    unsigned int nvecs = exact_div (outer_size, inner_size).to_constant ();\n+    return c.require_immediate (1, 0, nvecs - 1);\n+  }\n+};\n+\n SHAPE(vsetvl, vsetvl)\n SHAPE(vsetvl, vsetvlmax)\n SHAPE(loadstore, loadstore)\n@@ -431,5 +510,9 @@ SHAPE(move, move)\n SHAPE(mask_alu, mask_alu)\n SHAPE(reduc_alu, reduc_alu)\n SHAPE(scalar_move, scalar_move)\n+SHAPE(vundefined, vundefined)\n+SHAPE(misc, misc)\n+SHAPE(vset, vset)\n+SHAPE(vget, vget)\n \n } // end namespace riscv_vector"}, {"sha": "640ef42f069372eff9358a8937bcd0697b2d7eed", "filename": "gcc/config/riscv/riscv-vector-builtins-shapes.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-shapes.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-shapes.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-shapes.h?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -37,6 +37,10 @@ extern const function_shape *const move;\n extern const function_shape *const mask_alu;\n extern const function_shape *const reduc_alu;\n extern const function_shape *const scalar_move;\n+extern const function_shape *const vundefined;\n+extern const function_shape *const misc;\n+extern const function_shape *const vset;\n+extern const function_shape *const vget;\n }\n \n } // end namespace riscv_vector"}, {"sha": "a55d494f1d94bb444a2e70fe0509f7b6d0247b24", "filename": "gcc/config/riscv/riscv-vector-builtins-types.def", "status": "modified", "additions": 366, "deletions": 0, "changes": 366, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-types.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-types.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins-types.def?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -157,6 +157,84 @@ along with GCC; see the file COPYING3. If not see\n #define DEF_RVV_EI16_OPS(TYPE, REQUIRE)\n #endif\n \n+/* Use \"DEF_RVV_EEW8_INTERPRET_OPS\" macro include all types for EEW8 vinterpret\n+   which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_EEW8_INTERPRET_OPS\n+#define DEF_RVV_EEW8_INTERPRET_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_EEW16_INTERPRET_OPS\" macro include all types for EEW16\n+   vinterpret which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_EEW16_INTERPRET_OPS\n+#define DEF_RVV_EEW16_INTERPRET_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_EEW32_INTERPRET_OPS\" macro include all types for EEW32\n+   vinterpret which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_EEW32_INTERPRET_OPS\n+#define DEF_RVV_EEW32_INTERPRET_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_EEW64_INTERPRET_OPS\" macro include all types for EEW64\n+   vinterpret which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_EEW64_INTERPRET_OPS\n+#define DEF_RVV_EEW64_INTERPRET_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_X2_VLMUL_EXT_OPS\" macro include all types for X2 VLMUL EXT\n+   which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_X2_VLMUL_EXT_OPS\n+#define DEF_RVV_X2_VLMUL_EXT_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_X4_VLMUL_EXT_OPS\" macro include all types for X4 VLMUL EXT\n+   which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_X4_VLMUL_EXT_OPS\n+#define DEF_RVV_X4_VLMUL_EXT_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_X8_VLMUL_EXT_OPS\" macro include all types for X8 VLMUL EXT\n+   which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_X8_VLMUL_EXT_OPS\n+#define DEF_RVV_X8_VLMUL_EXT_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_X16_VLMUL_EXT_OPS\" macro include all types for X16 VLMUL EXT\n+   which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_X16_VLMUL_EXT_OPS\n+#define DEF_RVV_X16_VLMUL_EXT_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_X32_VLMUL_EXT_OPS\" macro include all types for X32 VLMUL EXT\n+   which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_X32_VLMUL_EXT_OPS\n+#define DEF_RVV_X32_VLMUL_EXT_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_X64_VLMUL_EXT_OPS\" macro include all types for X64 VLMUL EXT\n+   which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_X64_VLMUL_EXT_OPS\n+#define DEF_RVV_X64_VLMUL_EXT_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_LMUL1_OPS\" macro include all types for LMUL1\n+   which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_LMUL1_OPS\n+#define DEF_RVV_LMUL1_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_LMUL2_OPS\" macro include all types for LMUL2\n+   which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_LMUL2_OPS\n+#define DEF_RVV_LMUL2_OPS(TYPE, REQUIRE)\n+#endif\n+\n+/* Use \"DEF_RVV_LMUL4_OPS\" macro include all types for LMUL4\n+   which will be iterated and registered as intrinsic functions.  */\n+#ifndef DEF_RVV_LMUL4_OPS\n+#define DEF_RVV_LMUL4_OPS(TYPE, REQUIRE)\n+#endif\n+\n DEF_RVV_I_OPS (vint8mf8_t, RVV_REQUIRE_ZVE64)\n DEF_RVV_I_OPS (vint8mf4_t, 0)\n DEF_RVV_I_OPS (vint8mf2_t, 0)\n@@ -465,6 +543,281 @@ DEF_RVV_EI16_OPS (vfloat64m2_t, RVV_REQUIRE_ELEN_FP_64)\n DEF_RVV_EI16_OPS (vfloat64m4_t, RVV_REQUIRE_ELEN_FP_64)\n DEF_RVV_EI16_OPS (vfloat64m8_t, RVV_REQUIRE_ELEN_FP_64)\n \n+DEF_RVV_EEW8_INTERPRET_OPS (vint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint16mf2_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint16m1_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint16m2_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint16m4_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint16m8_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint32m1_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint32m2_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint32m4_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint32m8_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint64m4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vint64m8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint16mf2_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint16m1_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint16m2_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint16m4_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint16m8_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint32m1_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint32m2_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint32m4_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint32m8_t, 0)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint64m4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW8_INTERPRET_OPS (vuint64m8_t, RVV_REQUIRE_ZVE64)\n+\n+DEF_RVV_EEW16_INTERPRET_OPS (vint8mf4_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint8mf2_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint8m1_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint8m2_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint8m4_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint8m8_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint32m1_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint32m2_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint32m4_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint32m8_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint64m4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW16_INTERPRET_OPS (vint64m8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint8mf4_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint8mf2_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint8m1_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint8m2_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint8m4_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint8m8_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint32m1_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint32m2_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint32m4_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint32m8_t, 0)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint64m4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW16_INTERPRET_OPS (vuint64m8_t, RVV_REQUIRE_ZVE64)\n+\n+DEF_RVV_EEW32_INTERPRET_OPS (vint8mf2_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint8m1_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint8m2_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint8m4_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint8m8_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint16mf2_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint16m1_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint16m2_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint16m4_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint16m8_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint64m4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW32_INTERPRET_OPS (vint64m8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint8mf2_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint8m1_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint8m2_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint8m4_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint8m8_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint16mf2_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint16m1_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint16m2_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint16m4_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint16m8_t, 0)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint64m4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_EEW32_INTERPRET_OPS (vuint64m8_t, RVV_REQUIRE_ZVE64)\n+\n+DEF_RVV_EEW64_INTERPRET_OPS (vint8m1_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint8m2_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint8m4_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint8m8_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint16m1_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint16m2_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint16m4_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint16m8_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint32m1_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint32m2_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint32m4_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vint32m8_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint8m1_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint8m2_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint8m4_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint8m8_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint16m1_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint16m2_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint16m4_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint16m8_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint32m1_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint32m2_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint32m4_t, 0)\n+DEF_RVV_EEW64_INTERPRET_OPS (vuint32m8_t, 0)\n+\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint8mf4_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint8mf2_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint8m1_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint8m2_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint8m4_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint16mf2_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint16m1_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint16m2_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint16m4_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint32m1_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint32m2_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint32m4_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vint64m4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint8mf4_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint8mf2_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint8m1_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint8m2_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint8m4_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint16mf2_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint16m1_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint16m2_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint16m4_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint32m1_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint32m2_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint32m4_t, 0)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vuint64m4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vfloat32mf2_t, RVV_REQUIRE_ELEN_FP_32 | RVV_REQUIRE_ZVE64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vfloat32m1_t, RVV_REQUIRE_ELEN_FP_32)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vfloat32m2_t, RVV_REQUIRE_ELEN_FP_32)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vfloat32m4_t, RVV_REQUIRE_ELEN_FP_32)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vfloat64m1_t, RVV_REQUIRE_ELEN_FP_64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vfloat64m2_t, RVV_REQUIRE_ELEN_FP_64)\n+DEF_RVV_X2_VLMUL_EXT_OPS (vfloat64m4_t, RVV_REQUIRE_ELEN_FP_64)\n+\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint8mf4_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint8mf2_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint8m1_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint8m2_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint16mf2_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint16m1_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint16m2_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint32m1_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint32m2_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint8mf4_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint8mf2_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint8m1_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint8m2_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint16mf2_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint16m1_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint16m2_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint32m1_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint32m2_t, 0)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vuint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vfloat32mf2_t, RVV_REQUIRE_ELEN_FP_32 | RVV_REQUIRE_ZVE64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vfloat32m1_t, RVV_REQUIRE_ELEN_FP_32)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vfloat32m2_t, RVV_REQUIRE_ELEN_FP_32)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vfloat64m1_t, RVV_REQUIRE_ELEN_FP_64)\n+DEF_RVV_X4_VLMUL_EXT_OPS (vfloat64m2_t, RVV_REQUIRE_ELEN_FP_64)\n+\n+DEF_RVV_X8_VLMUL_EXT_OPS (vint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vint8mf4_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vint8mf2_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vint8m1_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vint16mf2_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vint16m1_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vint32m1_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vuint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vuint8mf4_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vuint8mf2_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vuint8m1_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vuint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vuint16mf2_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vuint16m1_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vuint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vuint32m1_t, 0)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vuint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vfloat32mf2_t, RVV_REQUIRE_ELEN_FP_32 | RVV_REQUIRE_ZVE64)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vfloat32m1_t, RVV_REQUIRE_ELEN_FP_32)\n+DEF_RVV_X8_VLMUL_EXT_OPS (vfloat64m1_t, RVV_REQUIRE_ELEN_FP_64)\n+\n+DEF_RVV_X16_VLMUL_EXT_OPS (vint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vint8mf4_t, 0)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vint8mf2_t, 0)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vint16mf2_t, 0)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vuint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vuint8mf4_t, 0)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vuint8mf2_t, 0)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vuint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vuint16mf2_t, 0)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vuint32mf2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X16_VLMUL_EXT_OPS (vfloat32mf2_t, RVV_REQUIRE_ELEN_FP_32 | RVV_REQUIRE_ZVE64)\n+\n+DEF_RVV_X32_VLMUL_EXT_OPS (vint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X32_VLMUL_EXT_OPS (vint8mf4_t, 0)\n+DEF_RVV_X32_VLMUL_EXT_OPS (vint16mf4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X32_VLMUL_EXT_OPS (vuint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X32_VLMUL_EXT_OPS (vuint8mf4_t, 0)\n+DEF_RVV_X32_VLMUL_EXT_OPS (vuint16mf4_t, RVV_REQUIRE_ZVE64)\n+\n+DEF_RVV_X64_VLMUL_EXT_OPS (vint8mf8_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_X64_VLMUL_EXT_OPS (vuint8mf8_t, RVV_REQUIRE_ZVE64)\n+\n+DEF_RVV_LMUL1_OPS (vint8m1_t, 0)\n+DEF_RVV_LMUL1_OPS (vint16m1_t, 0)\n+DEF_RVV_LMUL1_OPS (vint32m1_t, 0)\n+DEF_RVV_LMUL1_OPS (vint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_LMUL1_OPS (vuint8m1_t, 0)\n+DEF_RVV_LMUL1_OPS (vuint16m1_t, 0)\n+DEF_RVV_LMUL1_OPS (vuint32m1_t, 0)\n+DEF_RVV_LMUL1_OPS (vuint64m1_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_LMUL1_OPS (vfloat32m1_t, RVV_REQUIRE_ELEN_FP_32)\n+DEF_RVV_LMUL1_OPS (vfloat64m1_t, RVV_REQUIRE_ELEN_FP_64)\n+\n+DEF_RVV_LMUL2_OPS (vint8m2_t, 0)\n+DEF_RVV_LMUL2_OPS (vint16m2_t, 0)\n+DEF_RVV_LMUL2_OPS (vint32m2_t, 0)\n+DEF_RVV_LMUL2_OPS (vint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_LMUL2_OPS (vuint8m2_t, 0)\n+DEF_RVV_LMUL2_OPS (vuint16m2_t, 0)\n+DEF_RVV_LMUL2_OPS (vuint32m2_t, 0)\n+DEF_RVV_LMUL2_OPS (vuint64m2_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_LMUL2_OPS (vfloat32m2_t, RVV_REQUIRE_ELEN_FP_32)\n+DEF_RVV_LMUL2_OPS (vfloat64m2_t, RVV_REQUIRE_ELEN_FP_64)\n+\n+DEF_RVV_LMUL4_OPS (vint8m4_t, 0)\n+DEF_RVV_LMUL4_OPS (vint16m4_t, 0)\n+DEF_RVV_LMUL4_OPS (vint32m4_t, 0)\n+DEF_RVV_LMUL4_OPS (vint64m4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_LMUL4_OPS (vuint8m4_t, 0)\n+DEF_RVV_LMUL4_OPS (vuint16m4_t, 0)\n+DEF_RVV_LMUL4_OPS (vuint32m4_t, 0)\n+DEF_RVV_LMUL4_OPS (vuint64m4_t, RVV_REQUIRE_ZVE64)\n+DEF_RVV_LMUL4_OPS (vfloat32m4_t, RVV_REQUIRE_ELEN_FP_32)\n+DEF_RVV_LMUL4_OPS (vfloat64m4_t, RVV_REQUIRE_ELEN_FP_64)\n+\n #undef DEF_RVV_I_OPS\n #undef DEF_RVV_U_OPS\n #undef DEF_RVV_F_OPS\n@@ -487,3 +840,16 @@ DEF_RVV_EI16_OPS (vfloat64m8_t, RVV_REQUIRE_ELEN_FP_64)\n #undef DEF_RVV_WU_OPS\n #undef DEF_RVV_WF_OPS\n #undef DEF_RVV_EI16_OPS\n+#undef DEF_RVV_EEW8_INTERPRET_OPS\n+#undef DEF_RVV_EEW16_INTERPRET_OPS\n+#undef DEF_RVV_EEW32_INTERPRET_OPS\n+#undef DEF_RVV_EEW64_INTERPRET_OPS\n+#undef DEF_RVV_X2_VLMUL_EXT_OPS\n+#undef DEF_RVV_X4_VLMUL_EXT_OPS\n+#undef DEF_RVV_X8_VLMUL_EXT_OPS\n+#undef DEF_RVV_X16_VLMUL_EXT_OPS\n+#undef DEF_RVV_X32_VLMUL_EXT_OPS\n+#undef DEF_RVV_X64_VLMUL_EXT_OPS\n+#undef DEF_RVV_LMUL1_OPS\n+#undef DEF_RVV_LMUL2_OPS\n+#undef DEF_RVV_LMUL4_OPS"}, {"sha": "2d57086262b3ad129b822f707f4918c6acec655a", "filename": "gcc/config/riscv/riscv-vector-builtins.cc", "status": "modified", "additions": 688, "deletions": 246, "changes": 934, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins.cc?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -106,20 +106,11 @@ const char *const operand_suffixes[NUM_OP_TYPES] = {\n const rvv_builtin_suffixes type_suffixes[NUM_VECTOR_TYPES + 1] = {\n #define DEF_RVV_TYPE(NAME, NCHARS, ABI_NAME, SCALAR_TYPE, VECTOR_MODE,         \\\n \t\t     VECTOR_MODE_MIN_VLEN_32, VECTOR_SUFFIX, SCALAR_SUFFIX,    \\\n-\t\t     VSETVL_SUFFIX, MASK_TYPE)                                 \\\n+\t\t     VSETVL_SUFFIX)                                            \\\n   {#VECTOR_SUFFIX, #SCALAR_SUFFIX, #VSETVL_SUFFIX},\n #include \"riscv-vector-builtins.def\"\n };\n \n-/* Mask type for each RVV type.  */\n-const vector_type_index mask_types[NUM_VECTOR_TYPES + 1] = {\n-#define DEF_RVV_TYPE(NAME, NCHARS, ABI_NAME, SCALAR_TYPE, VECTOR_MODE,         \\\n-\t\t     VECTOR_MODE_MIN_VLEN_32, VECTOR_SUFFIX, SCALAR_SUFFIX,    \\\n-\t\t     VSETVL_SUFFIX, MASK_TYPE)                                 \\\n-  VECTOR_TYPE_##MASK_TYPE,\n-#include \"riscv-vector-builtins.def\"\n-};\n-\n /* Static information about predication suffix for each RVV type.  */\n const char *const predication_suffixes[NUM_PRED_TYPES] = {\n   \"\", /* PRED_TYPE_none.  */\n@@ -294,6 +285,87 @@ static const rvv_type_info oextu_ops[] = {\n #include \"riscv-vector-builtins-types.def\"\n   {NUM_VECTOR_TYPES, 0}};\n \n+/* A list of eew8 interpret will be registered for intrinsic functions.  */\n+static const rvv_type_info eew8_interpret_ops[] = {\n+#define DEF_RVV_EEW8_INTERPRET_OPS(TYPE, REQUIRE) {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of eew16 interpret will be registered for intrinsic functions.  */\n+static const rvv_type_info eew16_interpret_ops[] = {\n+#define DEF_RVV_EEW16_INTERPRET_OPS(TYPE, REQUIRE)                             \\\n+  {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of eew32 interpret will be registered for intrinsic functions.  */\n+static const rvv_type_info eew32_interpret_ops[] = {\n+#define DEF_RVV_EEW32_INTERPRET_OPS(TYPE, REQUIRE)                             \\\n+  {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of eew64 interpret will be registered for intrinsic functions.  */\n+static const rvv_type_info eew64_interpret_ops[] = {\n+#define DEF_RVV_EEW64_INTERPRET_OPS(TYPE, REQUIRE)                             \\\n+  {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of x2 vlmul ext will be registered for intrinsic functions.  */\n+static const rvv_type_info vlmul_ext_x2_ops[] = {\n+#define DEF_RVV_X2_VLMUL_EXT_OPS(TYPE, REQUIRE) {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of x4 vlmul ext will be registered for intrinsic functions.  */\n+static const rvv_type_info vlmul_ext_x4_ops[] = {\n+#define DEF_RVV_X4_VLMUL_EXT_OPS(TYPE, REQUIRE) {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of x8 vlmul ext will be registered for intrinsic functions.  */\n+static const rvv_type_info vlmul_ext_x8_ops[] = {\n+#define DEF_RVV_X8_VLMUL_EXT_OPS(TYPE, REQUIRE) {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of x16 vlmul ext will be registered for intrinsic functions.  */\n+static const rvv_type_info vlmul_ext_x16_ops[] = {\n+#define DEF_RVV_X16_VLMUL_EXT_OPS(TYPE, REQUIRE) {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of x32 vlmul ext will be registered for intrinsic functions.  */\n+static const rvv_type_info vlmul_ext_x32_ops[] = {\n+#define DEF_RVV_X32_VLMUL_EXT_OPS(TYPE, REQUIRE) {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of x64 vlmul ext will be registered for intrinsic functions.  */\n+static const rvv_type_info vlmul_ext_x64_ops[] = {\n+#define DEF_RVV_X64_VLMUL_EXT_OPS(TYPE, REQUIRE) {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of LMUL1 will be registered for intrinsic functions.  */\n+static const rvv_type_info lmul1_ops[] = {\n+#define DEF_RVV_LMUL1_OPS(TYPE, REQUIRE) {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of LMUL2 will be registered for intrinsic functions.  */\n+static const rvv_type_info lmul2_ops[] = {\n+#define DEF_RVV_LMUL2_OPS(TYPE, REQUIRE) {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n+/* A list of LMUL4 will be registered for intrinsic functions.  */\n+static const rvv_type_info lmul4_ops[] = {\n+#define DEF_RVV_LMUL4_OPS(TYPE, REQUIRE) {VECTOR_TYPE_##TYPE, REQUIRE},\n+#include \"riscv-vector-builtins-types.def\"\n+  {NUM_VECTOR_TYPES, 0}};\n+\n static CONSTEXPR const rvv_arg_type_info rvv_arg_type_info_end\n   = rvv_arg_type_info (NUM_BASE_TYPES);\n \n@@ -330,56 +402,56 @@ static CONSTEXPR const rvv_arg_type_info scalar_ptr_ptrdiff_args[]\n      rvv_arg_type_info (RVV_BASE_ptrdiff), rvv_arg_type_info (RVV_BASE_vector),\n      rvv_arg_type_info_end};\n \n-/* A list of args for vector_type func (const scalar_type *, uint8_index_type)\n+/* A list of args for vector_type func (const scalar_type *, eew8_index_type)\n  * function.  */\n-static CONSTEXPR const rvv_arg_type_info scalar_const_ptr_uint8_index_args[]\n+static CONSTEXPR const rvv_arg_type_info scalar_const_ptr_eew8_index_args[]\n   = {rvv_arg_type_info (RVV_BASE_scalar_const_ptr),\n-     rvv_arg_type_info (RVV_BASE_uint8_index), rvv_arg_type_info_end};\n+     rvv_arg_type_info (RVV_BASE_eew8_index), rvv_arg_type_info_end};\n \n-/* A list of args for vector_type func (const scalar_type *, uint16_index_type)\n+/* A list of args for vector_type func (const scalar_type *, eew16_index_type)\n  * function.  */\n-static CONSTEXPR const rvv_arg_type_info scalar_const_ptr_uint16_index_args[]\n+static CONSTEXPR const rvv_arg_type_info scalar_const_ptr_eew16_index_args[]\n   = {rvv_arg_type_info (RVV_BASE_scalar_const_ptr),\n-     rvv_arg_type_info (RVV_BASE_uint16_index), rvv_arg_type_info_end};\n+     rvv_arg_type_info (RVV_BASE_eew16_index), rvv_arg_type_info_end};\n \n-/* A list of args for vector_type func (const scalar_type *, uint32_index_type)\n+/* A list of args for vector_type func (const scalar_type *, eew32_index_type)\n  * function.  */\n-static CONSTEXPR const rvv_arg_type_info scalar_const_ptr_uint32_index_args[]\n+static CONSTEXPR const rvv_arg_type_info scalar_const_ptr_eew32_index_args[]\n   = {rvv_arg_type_info (RVV_BASE_scalar_const_ptr),\n-     rvv_arg_type_info (RVV_BASE_uint32_index), rvv_arg_type_info_end};\n+     rvv_arg_type_info (RVV_BASE_eew32_index), rvv_arg_type_info_end};\n \n-/* A list of args for vector_type func (const scalar_type *, uint64_index_type)\n+/* A list of args for vector_type func (const scalar_type *, eew64_index_type)\n  * function.  */\n-static CONSTEXPR const rvv_arg_type_info scalar_const_ptr_uint64_index_args[]\n+static CONSTEXPR const rvv_arg_type_info scalar_const_ptr_eew64_index_args[]\n   = {rvv_arg_type_info (RVV_BASE_scalar_const_ptr),\n-     rvv_arg_type_info (RVV_BASE_uint64_index), rvv_arg_type_info_end};\n+     rvv_arg_type_info (RVV_BASE_eew64_index), rvv_arg_type_info_end};\n \n-/* A list of args for void func (scalar_type *, uint8_index_type, vector_type)\n+/* A list of args for void func (scalar_type *, eew8_index_type, vector_type)\n  * function.  */\n-static CONSTEXPR const rvv_arg_type_info scalar_ptr_uint8_index_args[]\n+static CONSTEXPR const rvv_arg_type_info scalar_ptr_eew8_index_args[]\n   = {rvv_arg_type_info (RVV_BASE_scalar_ptr),\n-     rvv_arg_type_info (RVV_BASE_uint8_index),\n+     rvv_arg_type_info (RVV_BASE_eew8_index),\n      rvv_arg_type_info (RVV_BASE_vector), rvv_arg_type_info_end};\n \n-/* A list of args for void func (scalar_type *, uint16_index_type, vector_type)\n+/* A list of args for void func (scalar_type *, eew16_index_type, vector_type)\n  * function.  */\n-static CONSTEXPR const rvv_arg_type_info scalar_ptr_uint16_index_args[]\n+static CONSTEXPR const rvv_arg_type_info scalar_ptr_eew16_index_args[]\n   = {rvv_arg_type_info (RVV_BASE_scalar_ptr),\n-     rvv_arg_type_info (RVV_BASE_uint16_index),\n+     rvv_arg_type_info (RVV_BASE_eew16_index),\n      rvv_arg_type_info (RVV_BASE_vector), rvv_arg_type_info_end};\n \n-/* A list of args for void func (scalar_type *, uint32_index_type, vector_type)\n+/* A list of args for void func (scalar_type *, eew32_index_type, vector_type)\n  * function.  */\n-static CONSTEXPR const rvv_arg_type_info scalar_ptr_uint32_index_args[]\n+static CONSTEXPR const rvv_arg_type_info scalar_ptr_eew32_index_args[]\n   = {rvv_arg_type_info (RVV_BASE_scalar_ptr),\n-     rvv_arg_type_info (RVV_BASE_uint32_index),\n+     rvv_arg_type_info (RVV_BASE_eew32_index),\n      rvv_arg_type_info (RVV_BASE_vector), rvv_arg_type_info_end};\n \n-/* A list of args for void func (scalar_type *, uint64_index_type, vector_type)\n+/* A list of args for void func (scalar_type *, eew64_index_type, vector_type)\n  * function.  */\n-static CONSTEXPR const rvv_arg_type_info scalar_ptr_uint64_index_args[]\n+static CONSTEXPR const rvv_arg_type_info scalar_ptr_eew64_index_args[]\n   = {rvv_arg_type_info (RVV_BASE_scalar_ptr),\n-     rvv_arg_type_info (RVV_BASE_uint64_index),\n+     rvv_arg_type_info (RVV_BASE_eew64_index),\n      rvv_arg_type_info (RVV_BASE_vector), rvv_arg_type_info_end};\n \n /* A list of args for vector_type func (vector_type, vector_type) function.  */\n@@ -447,7 +519,7 @@ static CONSTEXPR const rvv_arg_type_info gather_vv_args[]\n /* A list of args for vector_type func (vector_type, shift_type) function.  */\n static CONSTEXPR const rvv_arg_type_info gatherei16_vv_args[]\n   = {rvv_arg_type_info (RVV_BASE_vector),\n-     rvv_arg_type_info (RVV_BASE_uint16_index), rvv_arg_type_info_end};\n+     rvv_arg_type_info (RVV_BASE_eew16_index), rvv_arg_type_info_end};\n \n /* A list of args for double demote type func (vector_type, shift_type)\n  * function.  */\n@@ -460,6 +532,30 @@ static CONSTEXPR const rvv_arg_type_info shift_wv_args[]\n static CONSTEXPR const rvv_arg_type_info v_args[]\n   = {rvv_arg_type_info (RVV_BASE_vector), rvv_arg_type_info_end};\n \n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info v_x2_trunc_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x2), rvv_arg_type_info_end};\n+\n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info v_x4_trunc_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x4), rvv_arg_type_info_end};\n+\n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info v_x8_trunc_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x8), rvv_arg_type_info_end};\n+\n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info v_x16_trunc_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x16), rvv_arg_type_info_end};\n+\n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info v_x32_trunc_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x32), rvv_arg_type_info_end};\n+\n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info v_x64_trunc_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x64), rvv_arg_type_info_end};\n+\n /* A list of args for vector_type func (vector_type, lmul1_type) function.  */\n static CONSTEXPR const rvv_arg_type_info vs_args[]\n   = {rvv_arg_type_info (RVV_BASE_vector),\n@@ -612,6 +708,39 @@ static CONSTEXPR const rvv_arg_type_info w_xu_v_args[]\n   = {rvv_arg_type_info (RVV_BASE_double_trunc_unsigned_vector),\n      rvv_arg_type_info_end};\n \n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info ext_x2_vset_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x2),\n+     rvv_arg_type_info (RVV_BASE_size), rvv_arg_type_info (RVV_BASE_vector),\n+     rvv_arg_type_info_end};\n+\n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info ext_x4_vset_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x4),\n+     rvv_arg_type_info (RVV_BASE_size), rvv_arg_type_info (RVV_BASE_vector),\n+     rvv_arg_type_info_end};\n+\n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info ext_x8_vset_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x8),\n+     rvv_arg_type_info (RVV_BASE_size), rvv_arg_type_info (RVV_BASE_vector),\n+     rvv_arg_type_info_end};\n+\n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info ext_x2_vget_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x2),\n+     rvv_arg_type_info (RVV_BASE_size), rvv_arg_type_info_end};\n+\n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info ext_x4_vget_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x4),\n+     rvv_arg_type_info (RVV_BASE_size), rvv_arg_type_info_end};\n+\n+/* A list of args for vector_type func (vector_type) function.  */\n+static CONSTEXPR const rvv_arg_type_info ext_x8_vget_args[]\n+  = {rvv_arg_type_info (RVV_BASE_vlmul_ext_x8),\n+     rvv_arg_type_info (RVV_BASE_size), rvv_arg_type_info_end};\n+\n /* A list of none preds that will be registered for intrinsic functions.  */\n static CONSTEXPR const predication_type_index none_preds[]\n   = {PRED_TYPE_none, NUM_PRED_TYPES};\n@@ -637,7 +766,7 @@ static CONSTEXPR const predication_type_index none_m_preds[]\n static CONSTEXPR const predication_type_index none_m_mu_preds[]\n   = {PRED_TYPE_none, PRED_TYPE_m, PRED_TYPE_mu, NUM_PRED_TYPES};\n \n-/* A static operand information for size_t func (void) function registration. */\n+/* A static operand information for size_t func () function registration. */\n static CONSTEXPR const rvv_op_info i_none_size_void_ops\n   = {i_ops,\t\t\t\t/* Types */\n      OP_TYPE_none,\t\t\t/* Suffix */\n@@ -652,6 +781,14 @@ static CONSTEXPR const rvv_op_info i_none_size_size_ops\n      rvv_arg_type_info (RVV_BASE_size), /* Return type */\n      size_args /* Args */};\n \n+/* A static operand information for vector_type func () function registration.\n+ */\n+static CONSTEXPR const rvv_op_info all_none_void_ops\n+  = {all_ops,\t\t\t\t  /* Types */\n+     OP_TYPE_none,\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     void_args /* Args */};\n+\n /* A static operand information for vector_type func (const scalar_type *)\n  * function registration. */\n static CONSTEXPR const rvv_op_info all_v_scalar_const_ptr_ops\n@@ -749,36 +886,36 @@ static CONSTEXPR const rvv_op_info all_v_scalar_const_ptr_ptrdiff_ops\n      scalar_const_ptr_ptrdiff_args /* Args */};\n \n /* A static operand information for vector_type func (const scalar_type *,\n- * uint8_index_type) function registration. */\n-static CONSTEXPR const rvv_op_info all_v_scalar_const_ptr_uint8_index_ops\n+ * eew8_index_type) function registration. */\n+static CONSTEXPR const rvv_op_info all_v_scalar_const_ptr_eew8_index_ops\n   = {all_ops,\t\t\t\t  /* Types */\n      OP_TYPE_v,\t\t\t\t  /* Suffix */\n      rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n-     scalar_const_ptr_uint8_index_args /* Args */};\n+     scalar_const_ptr_eew8_index_args /* Args */};\n \n /* A static operand information for vector_type func (const scalar_type *,\n- * uint16_index_type) function registration. */\n-static CONSTEXPR const rvv_op_info all_v_scalar_const_ptr_uint16_index_ops\n+ * eew16_index_type) function registration. */\n+static CONSTEXPR const rvv_op_info all_v_scalar_const_ptr_eew16_index_ops\n   = {all_ops,\t\t\t\t  /* Types */\n      OP_TYPE_v,\t\t\t\t  /* Suffix */\n      rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n-     scalar_const_ptr_uint16_index_args /* Args */};\n+     scalar_const_ptr_eew16_index_args /* Args */};\n \n /* A static operand information for vector_type func (const scalar_type *,\n- * uint32_index_type) function registration. */\n-static CONSTEXPR const rvv_op_info all_v_scalar_const_ptr_uint32_index_ops\n+ * eew32_index_type) function registration. */\n+static CONSTEXPR const rvv_op_info all_v_scalar_const_ptr_eew32_index_ops\n   = {all_ops,\t\t\t\t  /* Types */\n      OP_TYPE_v,\t\t\t\t  /* Suffix */\n      rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n-     scalar_const_ptr_uint32_index_args /* Args */};\n+     scalar_const_ptr_eew32_index_args /* Args */};\n \n /* A static operand information for vector_type func (const scalar_type *,\n- * uint64_index_type) function registration. */\n-static CONSTEXPR const rvv_op_info all_v_scalar_const_ptr_uint64_index_ops\n+ * eew64_index_type) function registration. */\n+static CONSTEXPR const rvv_op_info all_v_scalar_const_ptr_eew64_index_ops\n   = {all_ops,\t\t\t\t  /* Types */\n      OP_TYPE_v,\t\t\t\t  /* Suffix */\n      rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n-     scalar_const_ptr_uint64_index_args /* Args */};\n+     scalar_const_ptr_eew64_index_args /* Args */};\n \n /* A static operand information for void func (scalar_type *, ptrdiff_t,\n  * vector_type) function registration. */\n@@ -788,37 +925,37 @@ static CONSTEXPR const rvv_op_info all_v_scalar_ptr_ptrdiff_ops\n      rvv_arg_type_info (RVV_BASE_void), /* Return type */\n      scalar_ptr_ptrdiff_args /* Args */};\n \n-/* A static operand information for void func (scalar_type *, uint8_index_type,\n+/* A static operand information for void func (scalar_type *, eew8_index_type,\n  * vector_type) function registration. */\n-static CONSTEXPR const rvv_op_info all_v_scalar_ptr_uint8_index_ops\n+static CONSTEXPR const rvv_op_info all_v_scalar_ptr_eew8_index_ops\n   = {all_ops,\t\t\t\t/* Types */\n      OP_TYPE_v,\t\t\t\t/* Suffix */\n      rvv_arg_type_info (RVV_BASE_void), /* Return type */\n-     scalar_ptr_uint8_index_args /* Args */};\n+     scalar_ptr_eew8_index_args /* Args */};\n \n-/* A static operand information for void func (scalar_type *, uint16_index_type,\n+/* A static operand information for void func (scalar_type *, eew16_index_type,\n  * vector_type) function registration. */\n-static CONSTEXPR const rvv_op_info all_v_scalar_ptr_uint16_index_ops\n+static CONSTEXPR const rvv_op_info all_v_scalar_ptr_eew16_index_ops\n   = {all_ops,\t\t\t\t/* Types */\n      OP_TYPE_v,\t\t\t\t/* Suffix */\n      rvv_arg_type_info (RVV_BASE_void), /* Return type */\n-     scalar_ptr_uint16_index_args /* Args */};\n+     scalar_ptr_eew16_index_args /* Args */};\n \n-/* A static operand information for void func (scalar_type *, uint32_index_type,\n+/* A static operand information for void func (scalar_type *, eew32_index_type,\n  * vector_type) function registration. */\n-static CONSTEXPR const rvv_op_info all_v_scalar_ptr_uint32_index_ops\n+static CONSTEXPR const rvv_op_info all_v_scalar_ptr_eew32_index_ops\n   = {all_ops,\t\t\t\t/* Types */\n      OP_TYPE_v,\t\t\t\t/* Suffix */\n      rvv_arg_type_info (RVV_BASE_void), /* Return type */\n-     scalar_ptr_uint32_index_args /* Args */};\n+     scalar_ptr_eew32_index_args /* Args */};\n \n-/* A static operand information for void func (scalar_type *, uint64_index_type,\n+/* A static operand information for void func (scalar_type *, eew64_index_type,\n  * vector_type) function registration. */\n-static CONSTEXPR const rvv_op_info all_v_scalar_ptr_uint64_index_ops\n+static CONSTEXPR const rvv_op_info all_v_scalar_ptr_eew64_index_ops\n   = {all_ops,\t\t\t\t/* Types */\n      OP_TYPE_v,\t\t\t\t/* Suffix */\n      rvv_arg_type_info (RVV_BASE_void), /* Return type */\n-     scalar_ptr_uint64_index_args /* Args */};\n+     scalar_ptr_eew64_index_args /* Args */};\n \n /* A static operand information for vector_type func (vector_type, vector_type)\n  * function registration. */\n@@ -1374,6 +1511,182 @@ static CONSTEXPR const rvv_op_info all_v_ops\n      rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n      v_args /* Args */};\n \n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info i_v_u_ops\n+  = {i_ops,\t\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_unsigned_vector), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info u_v_i_ops\n+  = {u_ops,\t\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_signed_vector), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info iu_v_eew8_interpret_ops\n+  = {eew8_interpret_ops,\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_eew8_interpret), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info iu_v_eew16_interpret_ops\n+  = {eew16_interpret_ops,\t\t\t   /* Types */\n+     OP_TYPE_v,\t\t\t\t\t   /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_eew16_interpret), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info iu_v_eew32_interpret_ops\n+  = {eew32_interpret_ops,\t\t\t   /* Types */\n+     OP_TYPE_v,\t\t\t\t\t   /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_eew32_interpret), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info iu_v_eew64_interpret_ops\n+  = {eew64_interpret_ops,\t\t\t   /* Types */\n+     OP_TYPE_v,\t\t\t\t\t   /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_eew64_interpret), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_ext_x2_ops\n+  = {vlmul_ext_x2_ops,\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x2), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_ext_x4_ops\n+  = {vlmul_ext_x4_ops,\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x4), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_ext_x8_ops\n+  = {vlmul_ext_x8_ops,\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x8), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_ext_x16_ops\n+  = {vlmul_ext_x16_ops,\t\t\t\t /* Types */\n+     OP_TYPE_v,\t\t\t\t\t /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x16), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_ext_x32_ops\n+  = {vlmul_ext_x32_ops,\t\t\t\t /* Types */\n+     OP_TYPE_v,\t\t\t\t\t /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x32), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_ext_x64_ops\n+  = {vlmul_ext_x64_ops,\t\t\t\t /* Types */\n+     OP_TYPE_v,\t\t\t\t\t /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x64), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_trunc_x2_ops\n+  = {vlmul_ext_x2_ops,\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     v_x2_trunc_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_trunc_x4_ops\n+  = {vlmul_ext_x4_ops,\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     v_x4_trunc_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_trunc_x8_ops\n+  = {vlmul_ext_x8_ops,\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     v_x8_trunc_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_trunc_x16_ops\n+  = {vlmul_ext_x16_ops,\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     v_x16_trunc_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_trunc_x32_ops\n+  = {vlmul_ext_x32_ops,\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     v_x32_trunc_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vlmul_trunc_x64_ops\n+  = {vlmul_ext_x64_ops,\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     v_x64_trunc_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info f_v_i_ops\n+  = {f_ops,\t\t\t\t\t /* Types */\n+     OP_TYPE_v,\t\t\t\t\t /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_signed_vector), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info f_v_u_ops\n+  = {f_ops,\t\t\t\t\t   /* Types */\n+     OP_TYPE_v,\t\t\t\t\t   /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_unsigned_vector), /* Return type */\n+     v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info i_v_f_ops\n+  = {f_ops,\t\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     x_v_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info u_v_f_ops\n+  = {f_ops,\t\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     xu_v_args /* Args */};\n+\n /* A static operand information for vector_type func (scalar_type)\n  * function registration. */\n static CONSTEXPR const rvv_op_info iu_x_ops\n@@ -1694,6 +2007,158 @@ static CONSTEXPR const rvv_op_info iu_trunc_ops\n      rvv_arg_type_info (RVV_BASE_double_trunc_vector), /* Return type */\n      v_args /* Args */};\n \n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vset_lmul1_x2_ops\n+  = {lmul1_ops,\t\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x2), /* Return type */\n+     ext_x2_vset_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vset_lmul1_x4_ops\n+  = {lmul1_ops,\t\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x4), /* Return type */\n+     ext_x4_vset_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vset_lmul1_x8_ops\n+  = {lmul1_ops,\t\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x8), /* Return type */\n+     ext_x8_vset_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vset_lmul2_x2_ops\n+  = {lmul2_ops,\t\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x2), /* Return type */\n+     ext_x2_vset_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vset_lmul2_x4_ops\n+  = {lmul2_ops,\t\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x4), /* Return type */\n+     ext_x4_vset_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vset_lmul4_x2_ops\n+  = {lmul4_ops,\t\t\t\t\t/* Types */\n+     OP_TYPE_v,\t\t\t\t\t/* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vlmul_ext_x2), /* Return type */\n+     ext_x2_vset_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vget_lmul1_x2_ops\n+  = {lmul1_ops,\t\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     ext_x2_vget_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vget_lmul1_x4_ops\n+  = {lmul1_ops,\t\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     ext_x4_vget_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vget_lmul1_x8_ops\n+  = {lmul1_ops,\t\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     ext_x8_vget_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vget_lmul2_x2_ops\n+  = {lmul2_ops,\t\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     ext_x2_vget_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vget_lmul2_x4_ops\n+  = {lmul2_ops,\t\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     ext_x4_vget_args /* Args */};\n+\n+/* A static operand information for vector_type func (vector_type)\n+ * function registration. */\n+static CONSTEXPR const rvv_op_info all_v_vget_lmul4_x2_ops\n+  = {lmul4_ops,\t\t\t\t  /* Types */\n+     OP_TYPE_v,\t\t\t\t  /* Suffix */\n+     rvv_arg_type_info (RVV_BASE_vector), /* Return type */\n+     ext_x2_vget_args /* Args */};\n+\n+/* A list of all RVV base function types.  */\n+static CONSTEXPR const function_type_info function_types[] = {\n+#define DEF_RVV_TYPE_INDEX(VECTOR, MASK, SIGNED, UNSIGNED, EEW8_INDEX, EEW16_INDEX, \\\n+\t\t      EEW32_INDEX, EEW64_INDEX, SHIFT, DOUBLE_TRUNC,           \\\n+\t\t      QUAD_TRUNC, OCT_TRUNC, DOUBLE_TRUNC_SCALAR,              \\\n+\t\t      DOUBLE_TRUNC_SIGNED, DOUBLE_TRUNC_UNSIGNED,              \\\n+\t\t      DOUBLE_TRUNC_UNSIGNED_SCALAR, DOUBLE_TRUNC_FLOAT, FLOAT, \\\n+\t\t      LMUL1, WLMUL1, EEW8_INTERPRET, EEW16_INTERPRET,          \\\n+\t\t      EEW32_INTERPRET, EEW64_INTERPRET, X2_VLMUL_EXT,          \\\n+\t\t      X4_VLMUL_EXT, X8_VLMUL_EXT, X16_VLMUL_EXT,               \\\n+\t\t      X32_VLMUL_EXT, X64_VLMUL_EXT)                            \\\n+  {                                                                            \\\n+    VECTOR_TYPE_##VECTOR,                                                      \\\n+    VECTOR_TYPE_INVALID,                                                       \\\n+    VECTOR_TYPE_##MASK,                                                        \\\n+    VECTOR_TYPE_##SIGNED,                                                      \\\n+    VECTOR_TYPE_##UNSIGNED,                                                    \\\n+    VECTOR_TYPE_INVALID,                                                       \\\n+    VECTOR_TYPE_INVALID,                                                       \\\n+    VECTOR_TYPE_INVALID,                                                       \\\n+    VECTOR_TYPE_INVALID,                                                       \\\n+    VECTOR_TYPE_INVALID,                                                       \\\n+    VECTOR_TYPE_INVALID,                                                       \\\n+    VECTOR_TYPE_INVALID,                                                       \\\n+    VECTOR_TYPE_INVALID,                                                       \\\n+    VECTOR_TYPE_INVALID,                                                       \\\n+    VECTOR_TYPE_##EEW8_INDEX,                                                  \\\n+    VECTOR_TYPE_##EEW16_INDEX,                                                 \\\n+    VECTOR_TYPE_##EEW32_INDEX,                                                 \\\n+    VECTOR_TYPE_##EEW64_INDEX,                                                 \\\n+    VECTOR_TYPE_##SHIFT,                                                       \\\n+    VECTOR_TYPE_##DOUBLE_TRUNC,                                                \\\n+    VECTOR_TYPE_##QUAD_TRUNC,                                                  \\\n+    VECTOR_TYPE_##OCT_TRUNC,                                                   \\\n+    VECTOR_TYPE_##DOUBLE_TRUNC_SCALAR,                                         \\\n+    VECTOR_TYPE_##DOUBLE_TRUNC_SIGNED,                                         \\\n+    VECTOR_TYPE_##DOUBLE_TRUNC_UNSIGNED,                                       \\\n+    VECTOR_TYPE_##DOUBLE_TRUNC_UNSIGNED_SCALAR,                                \\\n+    VECTOR_TYPE_##DOUBLE_TRUNC_FLOAT,                                          \\\n+    VECTOR_TYPE_##FLOAT,                                                       \\\n+    VECTOR_TYPE_##LMUL1,                                                       \\\n+    VECTOR_TYPE_##WLMUL1,                                                      \\\n+    VECTOR_TYPE_##EEW8_INTERPRET,                                              \\\n+    VECTOR_TYPE_##EEW16_INTERPRET,                                             \\\n+    VECTOR_TYPE_##EEW32_INTERPRET,                                             \\\n+    VECTOR_TYPE_##EEW64_INTERPRET,                                             \\\n+    VECTOR_TYPE_##X2_VLMUL_EXT,                                                \\\n+    VECTOR_TYPE_##X4_VLMUL_EXT,                                                \\\n+    VECTOR_TYPE_##X8_VLMUL_EXT,                                                \\\n+    VECTOR_TYPE_##X16_VLMUL_EXT,                                               \\\n+    VECTOR_TYPE_##X32_VLMUL_EXT,                                               \\\n+    VECTOR_TYPE_##X64_VLMUL_EXT,                                               \\\n+  },\n+#include \"riscv-vector-builtins.def\"\n+}; // namespace riscv_vector\n+\n /* A list of all RVV intrinsic functions.  */\n static function_group_info function_groups[] = {\n #define DEF_RVV_FUNCTION(NAME, SHAPE, PREDS, OPS_INFO)                         \\\n@@ -1886,12 +2351,29 @@ register_vector_type (vector_type_index type)\n static bool\n required_extensions_p (enum rvv_base_type type)\n {\n-  return type == RVV_BASE_uint8_index || type == RVV_BASE_uint16_index\n-\t || type == RVV_BASE_uint32_index || type == RVV_BASE_uint64_index\n+  return type == RVV_BASE_eew8_index || type == RVV_BASE_eew16_index\n+\t || type == RVV_BASE_eew32_index || type == RVV_BASE_eew64_index\n \t || type == RVV_BASE_float_vector\n \t || type == RVV_BASE_double_trunc_float_vector\n \t || type == RVV_BASE_double_trunc_vector\n-\t || type == RVV_BASE_widen_lmul1_vector;\n+\t || type == RVV_BASE_widen_lmul1_vector\n+\t || type == RVV_BASE_eew8_interpret || type == RVV_BASE_eew16_interpret\n+\t || type == RVV_BASE_eew32_interpret || type == RVV_BASE_eew64_interpret\n+\t || type == RVV_BASE_vlmul_ext_x2 || type == RVV_BASE_vlmul_ext_x4\n+\t || type == RVV_BASE_vlmul_ext_x8 || type == RVV_BASE_vlmul_ext_x16\n+\t || type == RVV_BASE_vlmul_ext_x32 || type == RVV_BASE_vlmul_ext_x64;\n+}\n+\n+static uint64_t\n+get_required_extensions (vector_type_index type_idx)\n+{\n+  for (unsigned int i = 0; all_ops[i].index != NUM_VECTOR_TYPES; i++)\n+    if (type_idx == all_ops[i].index)\n+      return all_ops[i].required_extensions;\n+  for (unsigned int i = 0; b_ops[i].index != NUM_VECTOR_TYPES; i++)\n+    if (type_idx == b_ops[i].index)\n+      return b_ops[i].required_extensions;\n+  gcc_unreachable ();\n }\n \n /* Check whether all the RVV_REQUIRE_* values in REQUIRED_EXTENSIONS are\n@@ -1902,21 +2384,30 @@ check_required_extensions (const function_instance &instance)\n   rvv_type_info type_info = instance.type;\n   uint64_t required_extensions = type_info.required_extensions;\n   const rvv_op_info *op_info = instance.op_info;\n-  tree type = builtin_types[type_info.index].vector;\n+\n+  if (required_extensions_p (op_info->ret.base_type))\n+    {\n+      enum vector_type_index ret_type_idx\n+\t= op_info->ret.get_function_type_index (type_info.index);\n+      if (ret_type_idx == NUM_VECTOR_TYPES)\n+\treturn false;\n+      required_extensions |= get_required_extensions (ret_type_idx);\n+    }\n+\n   for (unsigned i = 0; op_info->args[i].base_type != NUM_BASE_TYPES; ++i)\n     {\n       if (!required_extensions_p (op_info->args[i].base_type))\n \tcontinue;\n \n       enum vector_type_index vector_type\n-\t= op_info->args[i].get_base_vector_type (type);\n+\t= op_info->args[i].get_function_type_index (type_info.index);\n       if (vector_type == NUM_VECTOR_TYPES)\n \treturn false;\n-      required_extensions |= op_info->types[vector_type].required_extensions;\n+      required_extensions |= get_required_extensions (vector_type);\n \n       /* According to RVV ISA, EEW=64 index of indexed loads/stores require\n \t XLEN = 64.  */\n-      if (op_info->args[i].base_type == RVV_BASE_uint64_index)\n+      if (op_info->args[i].base_type == RVV_BASE_eew64_index)\n \trequired_extensions |= RVV_REQUIRE_RV64BIT;\n     }\n \n@@ -1975,124 +2466,35 @@ get_mask_policy_for_pred (enum predication_type_index pred)\n   return gen_int_mode (get_prefer_mask_policy (), Pmode);\n }\n \n-static bool\n-unsigned_base_type_p (rvv_base_type base_type)\n+tree\n+rvv_arg_type_info::get_scalar_ptr_type (vector_type_index type_idx) const\n {\n-  return base_type == RVV_BASE_double_trunc_unsigned_vector\n-\t || base_type == RVV_BASE_double_trunc_unsigned_scalar\n-\t || base_type == RVV_BASE_unsigned_vector\n-\t || base_type == RVV_BASE_uint8_index\n-\t || base_type == RVV_BASE_uint16_index\n-\t || base_type == RVV_BASE_uint32_index\n-\t || base_type == RVV_BASE_uint64_index\n-\t || base_type == RVV_BASE_shift_vector;\n+  /* According to the latest rvv-intrinsic-doc, it defines vsm.v intrinsic:\n+   __riscv_vsm (uint8_t *base, vbool1_t value, size_t vl).  */\n+  if (type_idx >= VECTOR_TYPE_vbool64_t && type_idx <= VECTOR_TYPE_vbool1_t)\n+    return builtin_types[VECTOR_TYPE_vuint8mf8_t].scalar_ptr;\n+  else\n+    return builtin_types[type_idx].scalar_ptr;\n }\n \n-static machine_mode\n-get_mode_for_bitsize (poly_int64 bitsize, bool float_mode_p)\n+tree\n+rvv_arg_type_info::get_scalar_const_ptr_type (vector_type_index type_idx) const\n {\n-  if (float_mode_p)\n-    return float_mode_for_size (bitsize).require ();\n+  /* According to the latest rvv-intrinsic-doc, it defines vlm.v intrinsic:\n+   __riscv_vlm_v_b1 (const uint8_t *base, size_t vl).  */\n+  if (type_idx >= VECTOR_TYPE_vbool64_t && type_idx <= VECTOR_TYPE_vbool1_t)\n+    return builtin_types[VECTOR_TYPE_vuint8mf8_t].scalar_const_ptr;\n   else\n-    return int_mode_for_size (bitsize, 0).require ();\n+    return builtin_types[type_idx].scalar_const_ptr;\n }\n \n vector_type_index\n-rvv_arg_type_info::get_base_vector_type (tree type) const\n+rvv_arg_type_info::get_function_type_index (vector_type_index type_idx) const\n {\n-  if (!type)\n-    return NUM_VECTOR_TYPES;\n-\n-  poly_int64 nunits = GET_MODE_NUNITS (TYPE_MODE (type));\n-  machine_mode inner_mode = GET_MODE_INNER (TYPE_MODE (type));\n-  poly_int64 bitsize = GET_MODE_BITSIZE (inner_mode);\n-  poly_int64 bytesize = GET_MODE_SIZE (inner_mode);\n-\n-  bool unsigned_p = TYPE_UNSIGNED (type);\n-  if (unsigned_base_type_p (base_type))\n-    unsigned_p = true;\n-\n-  switch (base_type)\n-    {\n-    case RVV_BASE_mask:\n-      inner_mode = E_BImode;\n-      break;\n-    case RVV_BASE_uint8_index:\n-      inner_mode = E_QImode;\n-      break;\n-    case RVV_BASE_uint16_index:\n-      inner_mode = E_HImode;\n-      break;\n-    case RVV_BASE_uint32_index:\n-      inner_mode = E_SImode;\n-      break;\n-    case RVV_BASE_uint64_index:\n-      inner_mode = E_DImode;\n-      break;\n-    case RVV_BASE_shift_vector:\n-      inner_mode = GET_MODE_INNER (TYPE_MODE (type));\n-      break;\n-    case RVV_BASE_double_trunc_vector:\n-    case RVV_BASE_double_trunc_scalar:\n-      inner_mode = get_mode_for_bitsize (exact_div (bitsize, 2),\n-\t\t\t\t\t FLOAT_MODE_P (inner_mode));\n-      break;\n-    case RVV_BASE_double_trunc_unsigned_vector:\n-    case RVV_BASE_double_trunc_unsigned_scalar:\n-    case RVV_BASE_double_trunc_signed_vector:\n-      inner_mode = int_mode_for_size (exact_div (bitsize, 2), 0).require ();\n-      break;\n-    case RVV_BASE_quad_trunc_vector:\n-      inner_mode = get_mode_for_bitsize (exact_div (bitsize, 4),\n-\t\t\t\t\t FLOAT_MODE_P (inner_mode));\n-      break;\n-    case RVV_BASE_oct_trunc_vector:\n-      inner_mode = get_mode_for_bitsize (exact_div (bitsize, 8),\n-\t\t\t\t\t FLOAT_MODE_P (inner_mode));\n-      break;\n-    case RVV_BASE_float_vector:\n-      inner_mode = float_mode_for_size (bitsize).require ();\n-      break;\n-    case RVV_BASE_double_trunc_float_vector:\n-      inner_mode = float_mode_for_size (exact_div (bitsize, 2)).require ();\n-      break;\n-    case RVV_BASE_signed_vector:\n-    case RVV_BASE_unsigned_vector:\n-      inner_mode = int_mode_for_mode (inner_mode).require ();\n-      break;\n-    case RVV_BASE_lmul1_vector:\n-      nunits = exact_div (BYTES_PER_RISCV_VECTOR, bytesize);\n-      break;\n-    case RVV_BASE_widen_lmul1_vector:\n-      inner_mode\n-\t= get_mode_for_bitsize (bitsize * 2, FLOAT_MODE_P (inner_mode));\n-      if (BYTES_PER_RISCV_VECTOR.coeffs[0] < (bytesize * 2).coeffs[0])\n-\treturn NUM_VECTOR_TYPES;\n-      nunits = exact_div (BYTES_PER_RISCV_VECTOR, bytesize * 2);\n-      break;\n-    default:\n-      return NUM_VECTOR_TYPES;\n-    }\n-\n-  opt_machine_mode mode\n-    = get_vector_mode (as_a<scalar_mode> (inner_mode), nunits);\n-\n-  if (!mode.exists ())\n-    return NUM_VECTOR_TYPES;\n-  for (unsigned int i = 0; i < NUM_VECTOR_TYPES + 1; i++)\n-    {\n-      tree vector_type = builtin_types[i].vector;\n-      if (!vector_type)\n-\tcontinue;\n-\n-      if (GET_MODE_CLASS (TYPE_MODE (vector_type)) == MODE_VECTOR_INT\n-\t  && TYPE_UNSIGNED (vector_type) != unsigned_p)\n-\tcontinue;\n-\n-      if (TYPE_MODE (vector_type) == mode.require ())\n-\treturn (enum vector_type_index) i;\n-    }\n-  return NUM_VECTOR_TYPES;\n+  tree type\n+    = builtin_types[function_types[type_idx].type_indexes[base_type]].vector;\n+  return type ? function_types[type_idx].type_indexes[base_type]\n+\t      : NUM_VECTOR_TYPES;\n }\n \n tree\n@@ -2104,79 +2506,17 @@ rvv_arg_type_info::get_tree_type (vector_type_index type_idx) const\n      just return NULL_TREE.  */\n   if (!builtin_types[type_idx].vector)\n     return NULL_TREE;\n+\n   switch (base_type)\n     {\n-    case RVV_BASE_vector:\n-      return builtin_types[type_idx].vector;\n-    case RVV_BASE_scalar:\n-      return builtin_types[type_idx].scalar;\n-    /* According to riscv-vector-builtins-types.def, the unsigned\n-       type is always the signed type + 1 (They have same SEW and LMUL).\n-       For example 'vuint8mf8_t' enum = 'vint8mf8_t' enum + 1.\n-       Note: We dont't allow type_idx to be unsigned type.  */\n-    case RVV_BASE_unsigned_scalar:\n-      gcc_assert (!TYPE_UNSIGNED (builtin_types[type_idx].scalar));\n-      return builtin_types[type_idx + 1].scalar;\n-    case RVV_BASE_vector_ptr:\n-      return builtin_types[type_idx].vector_ptr;\n-    case RVV_BASE_scalar_ptr:\n-      /* According to the latest rvv-intrinsic-doc, it defines vsm.v intrinsic:\n-\t __riscv_vsm (uint8_t *base, vbool1_t value, size_t vl).  */\n-      if (type_idx >= VECTOR_TYPE_vbool64_t && type_idx <= VECTOR_TYPE_vbool1_t)\n-\treturn builtin_types[VECTOR_TYPE_vuint8mf8_t].scalar_ptr;\n-      else\n-\treturn builtin_types[type_idx].scalar_ptr;\n-    case RVV_BASE_scalar_const_ptr:\n-      /* According to the latest rvv-intrinsic-doc, it defines vlm.v intrinsic:\n-\t __riscv_vlm_v_b1 (const uint8_t *base, size_t vl).  */\n-      if (type_idx >= VECTOR_TYPE_vbool64_t && type_idx <= VECTOR_TYPE_vbool1_t)\n-\treturn builtin_types[VECTOR_TYPE_vuint8mf8_t].scalar_const_ptr;\n-      else\n-\treturn builtin_types[type_idx].scalar_const_ptr;\n-    case RVV_BASE_void:\n-      return void_type_node;\n-    case RVV_BASE_size:\n-      return size_type_node;\n-    case RVV_BASE_ptrdiff:\n-      return ptrdiff_type_node;\n-    case RVV_BASE_unsigned_long:\n-      return long_unsigned_type_node;\n-    case RVV_BASE_long:\n-      return long_integer_type_node;\n-    case RVV_BASE_uint8_index:\n-    case RVV_BASE_uint16_index:\n-    case RVV_BASE_uint32_index:\n-    case RVV_BASE_uint64_index:\n-    case RVV_BASE_shift_vector:\n-    case RVV_BASE_double_trunc_vector:\n-    case RVV_BASE_quad_trunc_vector:\n-    case RVV_BASE_oct_trunc_vector:\n-    case RVV_BASE_double_trunc_signed_vector:\n-    case RVV_BASE_double_trunc_unsigned_vector:\n-    case RVV_BASE_mask:\n-    case RVV_BASE_float_vector:\n-    case RVV_BASE_double_trunc_float_vector:\n-    case RVV_BASE_signed_vector:\n-    case RVV_BASE_unsigned_vector:\n-    case RVV_BASE_lmul1_vector:\n-    case RVV_BASE_widen_lmul1_vector:\n-      if (get_base_vector_type (builtin_types[type_idx].vector)\n-\t  != NUM_VECTOR_TYPES)\n-\treturn builtin_types[get_base_vector_type (\n-\t\t\t       builtin_types[type_idx].vector)].vector;\n-      break;\n-    case RVV_BASE_double_trunc_scalar:\n-    case RVV_BASE_double_trunc_unsigned_scalar:\n-      if (get_base_vector_type (builtin_types[type_idx].vector)\n-\t  != NUM_VECTOR_TYPES)\n-\treturn builtin_types[get_base_vector_type (\n-\t\t\t       builtin_types[type_idx].vector)].scalar;\n-      break;\n+#define DEF_RVV_BASE_TYPE(NAME, TYPE)                                          \\\n+  case RVV_BASE_##NAME:                                                        \\\n+    return TYPE;\n+#include \"riscv-vector-builtins.def\"\n     default:\n       gcc_unreachable ();\n     }\n-  /* Return NULL_TREE if the type we don't want to register.  */\n-  return NULL_TREE;\n+  gcc_unreachable ();\n }\n \n function_instance::function_instance (const char *base_name_in,\n@@ -2346,7 +2686,9 @@ function_builder::apply_predication (const function_instance &instance,\n       argument_types.quick_insert (0, return_type);\n \n   /* These predication types need to apply mask type.  */\n-  tree mask_type = builtin_types[mask_types[instance.type.index]].vector;\n+  vector_type_index mask_type_index\n+    = function_types[instance.type.index].type_indexes[RVV_BASE_mask];\n+  tree mask_type = builtin_types[mask_type_index].vector;\n   if (instance.pred == PRED_TYPE_m || instance.pred == PRED_TYPE_tum\n       || instance.pred == PRED_TYPE_tumu || instance.pred == PRED_TYPE_mu)\n     argument_types.quick_insert (0, mask_type);\n@@ -2559,7 +2901,9 @@ function_expander::add_mem_operand (machine_mode mode, unsigned argno)\n machine_mode\n function_expander::mask_mode (void) const\n {\n-  return TYPE_MODE (builtin_types[mask_types[type.index]].vector);\n+  vector_type_index mask_type_index\n+    = function_types[type.index].type_indexes[RVV_BASE_mask];\n+  return TYPE_MODE (builtin_types[mask_type_index].vector);\n }\n \n /* Implement the call using instruction ICODE, with a 1:1 mapping between\n@@ -2850,6 +3194,88 @@ function_expander::generate_insn (insn_code icode)\n   return function_returns_void_p () ? const0_rtx : m_ops[0].value;\n }\n \n+function_checker::function_checker (location_t location,\n+\t\t\t\t    const function_instance &instance,\n+\t\t\t\t    tree fndecl, tree fntype,\n+\t\t\t\t    unsigned int nargs, tree *args)\n+  : function_call_info (location, instance, fndecl), m_fntype (fntype),\n+    m_nargs (nargs), m_args (args)\n+{}\n+\n+/* Report that LOCATION has a call to FNDECL in which argument ARGNO\n+   was not an integer constant expression.  ARGNO counts from zero.  */\n+void\n+function_checker::report_non_ice (unsigned int argno) const\n+{\n+  error_at (location,\n+\t    \"argument %d of %qE must be an integer constant\"\n+\t    \" expression\",\n+\t    argno + 1, fndecl);\n+}\n+\n+/* Report that LOCATION has a call to FNDECL in which argument ARGNO has\n+   the value ACTUAL, whereas the function requires a value in the range\n+   [MIN, MAX].  ARGNO counts from zero.  */\n+void\n+function_checker::report_out_of_range (unsigned int argno, HOST_WIDE_INT actual,\n+\t\t\t\t       HOST_WIDE_INT min,\n+\t\t\t\t       HOST_WIDE_INT max) const\n+{\n+  error_at (location,\n+\t    \"passing %wd to argument %d of %qE, which expects\"\n+\t    \" a value in the range [%wd, %wd]\",\n+\t    actual, argno + 1, fndecl, min, max);\n+}\n+\n+/* Check that argument ARGNO is an integer constant expression and\n+   store its value in VALUE_OUT if so.  The caller should first\n+   check that argument ARGNO exists.  */\n+bool\n+function_checker::require_immediate (unsigned int argno, HOST_WIDE_INT min,\n+\t\t\t\t     HOST_WIDE_INT max) const\n+{\n+  gcc_assert (argno < m_nargs);\n+  tree arg = m_args[argno];\n+\n+  /* The type and range are unsigned, so read the argument as an\n+     unsigned rather than signed HWI.  */\n+  if (!tree_fits_uhwi_p (arg))\n+    {\n+      report_non_ice (argno);\n+      return false;\n+    }\n+  return require_immediate_range (argno, min, max);\n+}\n+\n+/* Check that argument REL_ARGNO is an integer constant expression in the\n+   range [MIN, MAX].  REL_ARGNO counts from the end of the predication\n+   arguments.  */\n+bool\n+function_checker::require_immediate_range (unsigned int argno,\n+\t\t\t\t\t   HOST_WIDE_INT min,\n+\t\t\t\t\t   HOST_WIDE_INT max) const\n+{\n+  gcc_assert (argno < m_nargs);\n+  tree arg = m_args[argno];\n+  HOST_WIDE_INT actual = tree_to_uhwi (arg);\n+\n+  if (!IN_RANGE (actual, min, max))\n+    {\n+      report_out_of_range (argno, actual, min, max);\n+      return false;\n+    }\n+\n+  return true;\n+}\n+\n+/* Perform semantic checks on the call.  Return true if the call is valid,\n+   otherwise report a suitable error.  */\n+bool\n+function_checker::check ()\n+{\n+  return shape->check (*this);\n+}\n+\n inline hashval_t\n registered_function_hasher::hash (value_type value)\n {\n@@ -3013,6 +3439,22 @@ expand_builtin (unsigned int code, tree exp, rtx target)\n   return function_expander (rfn.instance, rfn.decl, exp, target).expand ();\n }\n \n+/* Perform any semantic checks needed for a call to the SVE function\n+   with subcode CODE, such as testing for integer constant expressions.\n+   The call occurs at location LOCATION and has NARGS arguments,\n+   given by ARGS.  FNDECL is the original function decl, before\n+   overload resolution.\n+\n+   Return true if the call is valid, otherwise report a suitable error.  */\n+bool\n+check_builtin_call (location_t location, vec<location_t>, unsigned int code,\n+\t\t    tree fndecl, unsigned int nargs, tree *args)\n+{\n+  const registered_function &rfn = *(*registered_functions)[code];\n+  return function_checker (location, rfn.instance, fndecl,\n+\t\t\t   TREE_TYPE (rfn.decl), nargs, args).check ();\n+}\n+\n } // end namespace riscv_vector\n \n inline void"}, {"sha": "4d7e00de8b4da0517c6eb8a6933ae8e875b1acf9", "filename": "gcc/config/riscv/riscv-vector-builtins.def", "status": "modified", "additions": 156, "deletions": 83, "changes": 239, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins.def?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -44,7 +44,7 @@ along with GCC; see the file COPYING3.  If not see\n #ifndef DEF_RVV_TYPE\n #define DEF_RVV_TYPE(NAME, NCHARS, ABI_NAME, SCALAR_TYPE, VECTOR_MODE,         \\\n \t\t     VECTOR_MODE_MIN_VLEN_32, VECTOR_SUFFIX, SCALAR_SUFFIX,    \\\n-\t\t     VSETVL_SUFFIX, MASK_TYPE)\n+\t\t     VSETVL_SUFFIX)\n #endif\n \n /* Use \"DEF_RVV_OP_TYPE\" macro to define RVV operand types.\n@@ -59,214 +59,234 @@ along with GCC; see the file COPYING3.  If not see\n #define DEF_RVV_PRED_TYPE(NAME)\n #endif\n \n+/* Use \"DEF_RVV_BASE_TYPE\" macro to define RVV base types.\n+   The 'NAME' will be concatenated into intrinsic function name.  */\n+#ifndef DEF_RVV_BASE_TYPE\n+#define DEF_RVV_BASE_TYPE(NAME, TYPE)\n+#endif\n+\n+/* Use \"DEF_RVV_TYPE_INDEX\" macro to define RVV function types.\n+   The 'NAME' will be concatenated into intrinsic function name.  */\n+#ifndef DEF_RVV_TYPE_INDEX\n+#define DEF_RVV_TYPE_INDEX(VECTOR, MASK, SIGNED, UNSIGNED, EEW8_INDEX, EEW16_INDEX, \\\n+\t\t      EEW32_INDEX, EEW64_INDEX, SHIFT, DOUBLE_TRUNC,           \\\n+\t\t      QUAD_TRUNC, OCT_TRUNC, DOUBLE_TRUNC_SCALAR,              \\\n+\t\t      DOUBLE_TRUNC_SIGNED, DOUBLE_TRUNC_UNSIGNED,              \\\n+\t\t      DOUBLE_TRUNC_UNSIGNED_SCALAR, DOUBLE_TRUNC_FLOAT, FLOAT, \\\n+\t\t      LMUL1, WLMUL1, EEW8_INTERPRET, EEW16_INTERPRET,          \\\n+\t\t      EEW32_INTERPRET, EEW64_INTERPRET, X2_VLMUL_EXT,          \\\n+\t\t      X4_VLMUL_EXT, X8_VLMUL_EXT, X16_VLMUL_EXT,               \\\n+\t\t      X32_VLMUL_EXT, X64_VLMUL_EXT)\n+#endif\n+\n /* SEW/LMUL = 64:\n    Only enable when TARGET_MIN_VLEN > 32 and machine mode = VNx1BImode.  */\n-DEF_RVV_TYPE (vbool64_t, 14, __rvv_bool64_t, boolean, VNx1BI, VOID, _b64, , , vbool64_t)\n+DEF_RVV_TYPE (vbool64_t, 14, __rvv_bool64_t, boolean, VNx1BI, VOID, _b64, , )\n /* SEW/LMUL = 32:\n    Machine mode = VNx2BImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx1BImode when TARGET_MIN_VLEN = 32.  */\n-DEF_RVV_TYPE (vbool32_t, 14, __rvv_bool32_t, boolean, VNx2BI, VNx1BI, _b32, , , vbool32_t)\n+DEF_RVV_TYPE (vbool32_t, 14, __rvv_bool32_t, boolean, VNx2BI, VNx1BI, _b32, , )\n /* SEW/LMUL = 16:\n    Machine mode = VNx2BImode when TARGET_MIN_VLEN = 32.\n    Machine mode = VNx4BImode when TARGET_MIN_VLEN > 32.  */\n-DEF_RVV_TYPE (vbool16_t, 14, __rvv_bool16_t, boolean, VNx4BI, VNx2BI, _b16, , , vbool16_t)\n+DEF_RVV_TYPE (vbool16_t, 14, __rvv_bool16_t, boolean, VNx4BI, VNx2BI, _b16, , )\n /* SEW/LMUL = 8:\n    Machine mode = VNx8BImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx4BImode when TARGET_MIN_VLEN = 32.  */\n-DEF_RVV_TYPE (vbool8_t, 13, __rvv_bool8_t, boolean, VNx8BI, VNx4BI, _b8, , , vbool8_t)\n+DEF_RVV_TYPE (vbool8_t, 13, __rvv_bool8_t, boolean, VNx8BI, VNx4BI, _b8, , )\n /* SEW/LMUL = 4:\n    Machine mode = VNx16BImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx8BImode when TARGET_MIN_VLEN = 32.  */\n-DEF_RVV_TYPE (vbool4_t, 13, __rvv_bool4_t, boolean, VNx16BI, VNx8BI, _b4, , , vbool4_t)\n+DEF_RVV_TYPE (vbool4_t, 13, __rvv_bool4_t, boolean, VNx16BI, VNx8BI, _b4, , )\n /* SEW/LMUL = 2:\n    Machine mode = VNx32BImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx16BImode when TARGET_MIN_VLEN = 32.  */\n-DEF_RVV_TYPE (vbool2_t, 13, __rvv_bool2_t, boolean, VNx32BI, VNx16BI, _b2, , , vbool2_t)\n+DEF_RVV_TYPE (vbool2_t, 13, __rvv_bool2_t, boolean, VNx32BI, VNx16BI, _b2, , )\n /* SEW/LMUL = 1:\n    Machine mode = VNx64BImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx32BImode when TARGET_MIN_VLEN = 32.  */\n-DEF_RVV_TYPE (vbool1_t, 13, __rvv_bool1_t, boolean, VNx64BI, VNx32BI, _b1, , , vbool1_t)\n+DEF_RVV_TYPE (vbool1_t, 13, __rvv_bool1_t, boolean, VNx64BI, VNx32BI, _b1, , )\n \n /* LMUL = 1/8:\n    Only enble when TARGET_MIN_VLEN > 32 and machine mode = VNx1QImode.  */\n DEF_RVV_TYPE (vint8mf8_t, 15, __rvv_int8mf8_t, int8, VNx1QI, VOID, _i8mf8, _i8,\n-\t      _e8mf8, vbool64_t)\n-DEF_RVV_TYPE (vuint8mf8_t, 16, __rvv_uint8mf8_t, uint8, VNx1QI, VOID,\n-\t      _u8mf8, _u8, _e8mf8, vbool64_t)\n+\t      _e8mf8)\n+DEF_RVV_TYPE (vuint8mf8_t, 16, __rvv_uint8mf8_t, uint8, VNx1QI, VOID, _u8mf8,\n+\t      _u8, _e8mf8)\n /* LMUL = 1/4:\n    Machine mode = VNx2QImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx1QImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint8mf4_t, 15, __rvv_int8mf4_t, int8, VNx2QI, VNx1QI, _i8mf4,\n-\t      _i8, _e8mf4, vbool32_t)\n-DEF_RVV_TYPE (vuint8mf4_t, 16, __rvv_uint8mf4_t, uint8, VNx2QI, VNx1QI,\n-\t      _u8mf4, _u8, _e8mf4, vbool32_t)\n+\t      _i8, _e8mf4)\n+DEF_RVV_TYPE (vuint8mf4_t, 16, __rvv_uint8mf4_t, uint8, VNx2QI, VNx1QI, _u8mf4,\n+\t      _u8, _e8mf4)\n /* LMUL = 1/2:\n    Machine mode = VNx4QImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx2QImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint8mf2_t, 15, __rvv_int8mf2_t, int8, VNx4QI, VNx2QI, _i8mf2,\n-\t      _i8, _e8mf2, vbool16_t)\n-DEF_RVV_TYPE (vuint8mf2_t, 16, __rvv_uint8mf2_t, uint8, VNx4QI, VNx2QI,\n-\t      _u8mf2, _u8, _e8mf2, vbool16_t)\n+\t      _i8, _e8mf2)\n+DEF_RVV_TYPE (vuint8mf2_t, 16, __rvv_uint8mf2_t, uint8, VNx4QI, VNx2QI, _u8mf2,\n+\t      _u8, _e8mf2)\n /* LMUL = 1:\n    Machine mode = VNx8QImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx4QImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint8m1_t, 14, __rvv_int8m1_t, int8, VNx8QI, VNx4QI, _i8m1, _i8,\n-\t      _e8m1, vbool8_t)\n-DEF_RVV_TYPE (vuint8m1_t, 15, __rvv_uint8m1_t, uint8, VNx8QI, VNx4QI,\n-\t      _u8m1, _u8, _e8m1, vbool8_t)\n+\t      _e8m1)\n+DEF_RVV_TYPE (vuint8m1_t, 15, __rvv_uint8m1_t, uint8, VNx8QI, VNx4QI, _u8m1,\n+\t      _u8, _e8m1)\n /* LMUL = 2:\n    Machine mode = VNx16QImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx8QImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint8m2_t, 14, __rvv_int8m2_t, int8, VNx16QI, VNx8QI, _i8m2, _i8,\n-\t      _e8m2, vbool4_t)\n-DEF_RVV_TYPE (vuint8m2_t, 15, __rvv_uint8m2_t, uint8, VNx16QI, VNx8QI,\n-\t      _u8m2, _u8, _e8m2, vbool4_t)\n+\t      _e8m2)\n+DEF_RVV_TYPE (vuint8m2_t, 15, __rvv_uint8m2_t, uint8, VNx16QI, VNx8QI, _u8m2,\n+\t      _u8, _e8m2)\n /* LMUL = 4:\n    Machine mode = VNx32QImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx16QImode when TARGET_MIN_VLEN = 32.  */\n-DEF_RVV_TYPE (vint8m4_t, 14, __rvv_int8m4_t, int8, VNx32QI, VNx16QI, _i8m4,\n-\t      _i8, _e8m4, vbool2_t)\n-DEF_RVV_TYPE (vuint8m4_t, 15, __rvv_uint8m4_t, uint8, VNx32QI, VNx16QI,\n-\t      _u8m4, _u8, _e8m4, vbool2_t)\n+DEF_RVV_TYPE (vint8m4_t, 14, __rvv_int8m4_t, int8, VNx32QI, VNx16QI, _i8m4, _i8,\n+\t      _e8m4)\n+DEF_RVV_TYPE (vuint8m4_t, 15, __rvv_uint8m4_t, uint8, VNx32QI, VNx16QI, _u8m4,\n+\t      _u8, _e8m4)\n /* LMUL = 8:\n    Machine mode = VNx64QImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx32QImode when TARGET_MIN_VLEN = 32.  */\n-DEF_RVV_TYPE (vint8m8_t, 14, __rvv_int8m8_t, int8, VNx64QI, VNx32QI, _i8m8,\n-\t      _i8, _e8m8, vbool1_t)\n-DEF_RVV_TYPE (vuint8m8_t, 15, __rvv_uint8m8_t, uint8, VNx64QI, VNx32QI,\n-\t      _u8m8, _u8, _e8m8, vbool1_t)\n+DEF_RVV_TYPE (vint8m8_t, 14, __rvv_int8m8_t, int8, VNx64QI, VNx32QI, _i8m8, _i8,\n+\t      _e8m8)\n+DEF_RVV_TYPE (vuint8m8_t, 15, __rvv_uint8m8_t, uint8, VNx64QI, VNx32QI, _u8m8,\n+\t      _u8, _e8m8)\n \n /* LMUL = 1/4:\n    Only enble when TARGET_MIN_VLEN > 32 and machine mode = VNx1HImode.  */\n DEF_RVV_TYPE (vint16mf4_t, 16, __rvv_int16mf4_t, int16, VNx1HI, VOID, _i16mf4,\n-\t      _i16, _e16mf4, vbool64_t)\n+\t      _i16, _e16mf4)\n DEF_RVV_TYPE (vuint16mf4_t, 17, __rvv_uint16mf4_t, uint16, VNx1HI, VOID,\n-\t      _u16mf4, _u16, _e16mf4, vbool64_t)\n+\t      _u16mf4, _u16, _e16mf4)\n /* LMUL = 1/2:\n    Machine mode = VNx2HImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx1HImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint16mf2_t, 16, __rvv_int16mf2_t, int16, VNx2HI, VNx1HI, _i16mf2,\n-\t      _i16, _e16mf2, vbool32_t)\n-DEF_RVV_TYPE (vuint16mf2_t, 17, __rvv_uint16mf2_t, uint16, VNx2HI,\n-\t      VNx1HI, _u16mf2, _u16, _e16mf2, vbool32_t)\n+\t      _i16, _e16mf2)\n+DEF_RVV_TYPE (vuint16mf2_t, 17, __rvv_uint16mf2_t, uint16, VNx2HI, VNx1HI,\n+\t      _u16mf2, _u16, _e16mf2)\n /* LMUL = 1:\n    Machine mode = VNx4HImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx2HImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint16m1_t, 15, __rvv_int16m1_t, int16, VNx4HI, VNx2HI, _i16m1,\n-\t      _i16, _e16m1, vbool16_t)\n-DEF_RVV_TYPE (vuint16m1_t, 16, __rvv_uint16m1_t, uint16, VNx4HI, VNx2HI,\n-\t      _u16m1, _u16, _e16m1, vbool16_t)\n+\t      _i16, _e16m1)\n+DEF_RVV_TYPE (vuint16m1_t, 16, __rvv_uint16m1_t, uint16, VNx4HI, VNx2HI, _u16m1,\n+\t      _u16, _e16m1)\n /* LMUL = 2:\n    Machine mode = VNx8HImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx4HImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint16m2_t, 15, __rvv_int16m2_t, int16, VNx8HI, VNx4HI, _i16m2,\n-\t      _i16, _e16m2, vbool8_t)\n-DEF_RVV_TYPE (vuint16m2_t, 16, __rvv_uint16m2_t, uint16, VNx8HI, VNx4HI,\n-\t      _u16m2, _u16, _e16m2, vbool8_t)\n+\t      _i16, _e16m2)\n+DEF_RVV_TYPE (vuint16m2_t, 16, __rvv_uint16m2_t, uint16, VNx8HI, VNx4HI, _u16m2,\n+\t      _u16, _e16m2)\n /* LMUL = 4:\n    Machine mode = VNx16HImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx8HImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint16m4_t, 15, __rvv_int16m4_t, int16, VNx16HI, VNx8HI, _i16m4,\n-\t      _i16, _e16m4, vbool4_t)\n-DEF_RVV_TYPE (vuint16m4_t, 16, __rvv_uint16m4_t, uint16, VNx16HI,\n-\t      VNx8HI, _u16m4, _u16, _e16m4, vbool4_t)\n+\t      _i16, _e16m4)\n+DEF_RVV_TYPE (vuint16m4_t, 16, __rvv_uint16m4_t, uint16, VNx16HI, VNx8HI,\n+\t      _u16m4, _u16, _e16m4)\n /* LMUL = 8:\n    Machine mode = VNx32HImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx16HImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint16m8_t, 15, __rvv_int16m8_t, int16, VNx32HI, VNx16HI, _i16m8,\n-\t      _i16, _e16m8, vbool2_t)\n-DEF_RVV_TYPE (vuint16m8_t, 16, __rvv_uint16m8_t, uint16, VNx32HI,\n-\t      VNx16HI, _u16m8, _u16, _e16m8, vbool2_t)\n+\t      _i16, _e16m8)\n+DEF_RVV_TYPE (vuint16m8_t, 16, __rvv_uint16m8_t, uint16, VNx32HI, VNx16HI,\n+\t      _u16m8, _u16, _e16m8)\n \n /* LMUL = 1/2:\n    Only enble when TARGET_MIN_VLEN > 32 and machine mode = VNx1SImode.  */\n DEF_RVV_TYPE (vint32mf2_t, 16, __rvv_int32mf2_t, int32, VNx1SI, VOID, _i32mf2,\n-\t      _i32, _e32mf2, vbool64_t)\n+\t      _i32, _e32mf2)\n DEF_RVV_TYPE (vuint32mf2_t, 17, __rvv_uint32mf2_t, uint32, VNx1SI, VOID,\n-\t      _u32mf2, _u32, _e32mf2, vbool64_t)\n+\t      _u32mf2, _u32, _e32mf2)\n /* LMUL = 1:\n    Machine mode = VNx2SImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx1SImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint32m1_t, 15, __rvv_int32m1_t, int32, VNx2SI, VNx1SI, _i32m1,\n-\t      _i32, _e32m1, vbool32_t)\n-DEF_RVV_TYPE (vuint32m1_t, 16, __rvv_uint32m1_t, uint32, VNx2SI, VNx1SI,\n-\t      _u32m1, _u32, _e32m1, vbool32_t)\n+\t      _i32, _e32m1)\n+DEF_RVV_TYPE (vuint32m1_t, 16, __rvv_uint32m1_t, uint32, VNx2SI, VNx1SI, _u32m1,\n+\t      _u32, _e32m1)\n /* LMUL = 2:\n    Machine mode = VNx4SImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx2SImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint32m2_t, 15, __rvv_int32m2_t, int32, VNx4SI, VNx2SI, _i32m2,\n-\t      _i32, _e32m2, vbool16_t)\n-DEF_RVV_TYPE (vuint32m2_t, 16, __rvv_uint32m2_t, uint32, VNx4SI, VNx2SI,\n-\t      _u32m2, _u32, _e32m2, vbool16_t)\n+\t      _i32, _e32m2)\n+DEF_RVV_TYPE (vuint32m2_t, 16, __rvv_uint32m2_t, uint32, VNx4SI, VNx2SI, _u32m2,\n+\t      _u32, _e32m2)\n /* LMUL = 4:\n    Machine mode = VNx8SImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx4SImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint32m4_t, 15, __rvv_int32m4_t, int32, VNx8SI, VNx4SI, _i32m4,\n-\t      _i32, _e32m4, vbool8_t)\n-DEF_RVV_TYPE (vuint32m4_t, 16, __rvv_uint32m4_t, uint32, VNx8SI, VNx4SI,\n-\t      _u32m4, _u32, _e32m4, vbool8_t)\n+\t      _i32, _e32m4)\n+DEF_RVV_TYPE (vuint32m4_t, 16, __rvv_uint32m4_t, uint32, VNx8SI, VNx4SI, _u32m4,\n+\t      _u32, _e32m4)\n /* LMUL = 8:\n    Machine mode = VNx16SImode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx8SImode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vint32m8_t, 15, __rvv_int32m8_t, int32, VNx16SI, VNx8SI, _i32m8,\n-\t      _i32, _e32m8, vbool4_t)\n-DEF_RVV_TYPE (vuint32m8_t, 16, __rvv_uint32m8_t, uint32, VNx16SI,\n-\t      VNx8SI, _u32m8, _u32, _e32m8, vbool4_t)\n+\t      _i32, _e32m8)\n+DEF_RVV_TYPE (vuint32m8_t, 16, __rvv_uint32m8_t, uint32, VNx16SI, VNx8SI,\n+\t      _u32m8, _u32, _e32m8)\n \n /* SEW = 64:\n    Disable when TARGET_MIN_VLEN > 32.  */\n DEF_RVV_TYPE (vint64m1_t, 15, __rvv_int64m1_t, int64, VNx1DI, VOID, _i64m1,\n-\t      _i64, _e64m1, vbool64_t)\n-DEF_RVV_TYPE (vuint64m1_t, 16, __rvv_uint64m1_t, uint64, VNx1DI, VOID,\n-\t      _u64m1, _u64, _e64m1, vbool64_t)\n+\t      _i64, _e64m1)\n+DEF_RVV_TYPE (vuint64m1_t, 16, __rvv_uint64m1_t, uint64, VNx1DI, VOID, _u64m1,\n+\t      _u64, _e64m1)\n DEF_RVV_TYPE (vint64m2_t, 15, __rvv_int64m2_t, int64, VNx2DI, VOID, _i64m2,\n-\t      _i64, _e64m2, vbool32_t)\n-DEF_RVV_TYPE (vuint64m2_t, 16, __rvv_uint64m2_t, uint64, VNx2DI, VOID,\n-\t      _u64m2, _u64, _e64m2, vbool32_t)\n+\t      _i64, _e64m2)\n+DEF_RVV_TYPE (vuint64m2_t, 16, __rvv_uint64m2_t, uint64, VNx2DI, VOID, _u64m2,\n+\t      _u64, _e64m2)\n DEF_RVV_TYPE (vint64m4_t, 15, __rvv_int64m4_t, int64, VNx4DI, VOID, _i64m4,\n-\t      _i64, _e64m4, vbool16_t)\n-DEF_RVV_TYPE (vuint64m4_t, 16, __rvv_uint64m4_t, uint64, VNx4DI, VOID,\n-\t      _u64m4, _u64, _e64m4, vbool16_t)\n+\t      _i64, _e64m4)\n+DEF_RVV_TYPE (vuint64m4_t, 16, __rvv_uint64m4_t, uint64, VNx4DI, VOID, _u64m4,\n+\t      _u64, _e64m4)\n DEF_RVV_TYPE (vint64m8_t, 15, __rvv_int64m8_t, int64, VNx8DI, VOID, _i64m8,\n-\t      _i64, _e64m8, vbool8_t)\n-DEF_RVV_TYPE (vuint64m8_t, 16, __rvv_uint64m8_t, uint64, VNx8DI, VOID,\n-\t      _u64m8, _u64, _e64m8, vbool8_t)\n+\t      _i64, _e64m8)\n+DEF_RVV_TYPE (vuint64m8_t, 16, __rvv_uint64m8_t, uint64, VNx8DI, VOID, _u64m8,\n+\t      _u64, _e64m8)\n \n /* LMUL = 1/2:\n    Only enble when TARGET_MIN_VLEN > 32 and machine mode = VNx1SFmode.  */\n DEF_RVV_TYPE (vfloat32mf2_t, 18, __rvv_float32mf2_t, float, VNx1SF, VOID,\n-\t      _f32mf2, _f32, _e32mf2, vbool64_t)\n+\t      _f32mf2, _f32, _e32mf2)\n /* LMUL = 1:\n    Machine mode = VNx2SFmode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx1SFmode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vfloat32m1_t, 17, __rvv_float32m1_t, float, VNx2SF, VNx1SF,\n-\t      _f32m1, _f32, _e32m1, vbool32_t)\n+\t      _f32m1, _f32, _e32m1)\n /* LMUL = 2:\n    Machine mode = VNx4SFmode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx2SFmode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vfloat32m2_t, 17, __rvv_float32m2_t, float, VNx4SF, VNx2SF,\n-\t      _f32m2, _f32, _e32m2, vbool16_t)\n+\t      _f32m2, _f32, _e32m2)\n /* LMUL = 4:\n    Machine mode = VNx8SFmode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx4SFmode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vfloat32m4_t, 17, __rvv_float32m4_t, float, VNx8SF, VNx4SF,\n-\t      _f32m4, _f32, _e32m4, vbool8_t)\n+\t      _f32m4, _f32, _e32m4)\n /* LMUL = 8:\n    Machine mode = VNx16SFmode when TARGET_MIN_VLEN > 32.\n    Machine mode = VNx8SFmode when TARGET_MIN_VLEN = 32.  */\n DEF_RVV_TYPE (vfloat32m8_t, 17, __rvv_float32m8_t, float, VNx16SF, VNx8SF,\n-\t      _f32m8, _f32, _e32m8, vbool4_t)\n+\t      _f32m8, _f32, _e32m8)\n \n /* SEW = 64:\n    Disable when TARGET_VECTOR_FP64.  */\n DEF_RVV_TYPE (vfloat64m1_t, 17, __rvv_float64m1_t, double, VNx1DF, VOID, _f64m1,\n-\t      _f64, _e64m1, vbool64_t)\n+\t      _f64, _e64m1)\n DEF_RVV_TYPE (vfloat64m2_t, 17, __rvv_float64m2_t, double, VNx2DF, VOID, _f64m2,\n-\t      _f64, _e64m2, vbool32_t)\n+\t      _f64, _e64m2)\n DEF_RVV_TYPE (vfloat64m4_t, 17, __rvv_float64m4_t, double, VNx4DF, VOID, _f64m4,\n-\t      _f64, _e64m4, vbool16_t)\n+\t      _f64, _e64m4)\n DEF_RVV_TYPE (vfloat64m8_t, 17, __rvv_float64m8_t, double, VNx8DF, VOID, _f64m8,\n-\t      _f64, _e64m8, vbool8_t)\n+\t      _f64, _e64m8)\n \n DEF_RVV_OP_TYPE (vv)\n DEF_RVV_OP_TYPE (vx)\n@@ -307,6 +327,59 @@ DEF_RVV_PRED_TYPE (m)\n DEF_RVV_PRED_TYPE (tam)\n DEF_RVV_PRED_TYPE (tum)\n \n+DEF_RVV_BASE_TYPE (vector, builtin_types[type_idx].vector)\n+DEF_RVV_BASE_TYPE (scalar, builtin_types[type_idx].scalar)\n+DEF_RVV_BASE_TYPE (mask, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (signed_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (unsigned_vector, get_vector_type (type_idx))\n+/* According to riscv-vector-builtins-types.def, the unsigned\n+   type is always the signed type + 1 (They have same SEW and LMUL).\n+   For example 'vuint8mf8_t' enum = 'vint8mf8_t' enum + 1.\n+   Note: We dont't allow type_idx to be unsigned type.  */\n+DEF_RVV_BASE_TYPE (unsigned_scalar, builtin_types[type_idx + 1].scalar)\n+DEF_RVV_BASE_TYPE (vector_ptr, builtin_types[type_idx].vector_ptr)\n+/* According to the latest rvv-intrinsic-doc, it defines vsm.v intrinsic:\n+   __riscv_vsm (uint8_t *base, vbool1_t value, size_t vl).  */\n+DEF_RVV_BASE_TYPE (scalar_ptr, get_scalar_ptr_type (type_idx))\n+/* According to the latest rvv-intrinsic-doc, it defines vlm.v intrinsic:\n+   __riscv_vlm_v_b1 (const uint8_t *base, size_t vl).  */\n+DEF_RVV_BASE_TYPE (scalar_const_ptr, get_scalar_const_ptr_type (type_idx))\n+DEF_RVV_BASE_TYPE (void, void_type_node)\n+DEF_RVV_BASE_TYPE (size, size_type_node)\n+DEF_RVV_BASE_TYPE (ptrdiff, ptrdiff_type_node)\n+DEF_RVV_BASE_TYPE (unsigned_long, long_unsigned_type_node)\n+DEF_RVV_BASE_TYPE (long, long_integer_type_node)\n+DEF_RVV_BASE_TYPE (eew8_index, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (eew16_index, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (eew32_index, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (eew64_index, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (shift_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (double_trunc_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (quad_trunc_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (oct_trunc_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (double_trunc_scalar, get_scalar_type (type_idx))\n+DEF_RVV_BASE_TYPE (double_trunc_signed_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (double_trunc_unsigned_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (double_trunc_unsigned_scalar, get_scalar_type (type_idx))\n+DEF_RVV_BASE_TYPE (double_trunc_float_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (float_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (lmul1_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (widen_lmul1_vector, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (eew8_interpret, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (eew16_interpret, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (eew32_interpret, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (eew64_interpret, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (vlmul_ext_x2, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (vlmul_ext_x4, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (vlmul_ext_x8, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (vlmul_ext_x16, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (vlmul_ext_x32, get_vector_type (type_idx))\n+DEF_RVV_BASE_TYPE (vlmul_ext_x64, get_vector_type (type_idx))\n+\n+#include \"riscv-vector-type-indexer.gen.def\"\n+\n #undef DEF_RVV_PRED_TYPE\n #undef DEF_RVV_OP_TYPE\n #undef DEF_RVV_TYPE\n+#undef DEF_RVV_BASE_TYPE\n+#undef DEF_RVV_TYPE_INDEX"}, {"sha": "8464aa9b7e9fa7d50043fc5f79981f3e841895b9", "filename": "gcc/config/riscv/riscv-vector-builtins.h", "status": "modified", "additions": 87, "deletions": 32, "changes": 119, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vector-builtins.h?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -123,7 +123,8 @@ enum vector_type_index\n {\n #define DEF_RVV_TYPE(NAME, ABI_NAME, NCHARS, ARGS...) VECTOR_TYPE_##NAME,\n #include \"riscv-vector-builtins.def\"\n-  NUM_VECTOR_TYPES\n+  NUM_VECTOR_TYPES,\n+  VECTOR_TYPE_INVALID = NUM_VECTOR_TYPES\n };\n \n /* Enumerates the RVV governing predication types.  */\n@@ -138,36 +139,8 @@ enum predication_type_index\n /* Enumerates the RVV base types.  */\n enum rvv_base_type\n {\n-  RVV_BASE_vector,\n-  RVV_BASE_scalar,\n-  RVV_BASE_mask,\n-  RVV_BASE_signed_vector,\n-  RVV_BASE_unsigned_vector,\n-  RVV_BASE_unsigned_scalar,\n-  RVV_BASE_vector_ptr,\n-  RVV_BASE_scalar_ptr,\n-  RVV_BASE_scalar_const_ptr,\n-  RVV_BASE_void,\n-  RVV_BASE_size,\n-  RVV_BASE_ptrdiff,\n-  RVV_BASE_unsigned_long,\n-  RVV_BASE_long,\n-  RVV_BASE_uint8_index,\n-  RVV_BASE_uint16_index,\n-  RVV_BASE_uint32_index,\n-  RVV_BASE_uint64_index,\n-  RVV_BASE_shift_vector,\n-  RVV_BASE_double_trunc_vector,\n-  RVV_BASE_quad_trunc_vector,\n-  RVV_BASE_oct_trunc_vector,\n-  RVV_BASE_double_trunc_scalar,\n-  RVV_BASE_double_trunc_signed_vector,\n-  RVV_BASE_double_trunc_unsigned_vector,\n-  RVV_BASE_double_trunc_unsigned_scalar,\n-  RVV_BASE_double_trunc_float_vector,\n-  RVV_BASE_float_vector,\n-  RVV_BASE_lmul1_vector,\n-  RVV_BASE_widen_lmul1_vector,\n+#define DEF_RVV_BASE_TYPE(NAME, ARGS...) RVV_BASE_##NAME,\n+#include \"riscv-vector-builtins.def\"\n   NUM_BASE_TYPES\n };\n \n@@ -189,6 +162,13 @@ struct rvv_builtin_suffixes\n   const char *vsetvl;\n };\n \n+/* Builtin base type used to specify the type of builtin function\n+   argument or return result.  */\n+struct function_type_info\n+{\n+  enum vector_type_index type_indexes[NUM_BASE_TYPES];\n+};\n+\n /* RVV Builtin argument information.  */\n struct rvv_arg_type_info\n {\n@@ -197,7 +177,11 @@ struct rvv_arg_type_info\n   {}\n   enum rvv_base_type base_type;\n \n-  vector_type_index get_base_vector_type (tree type) const;\n+  tree get_scalar_ptr_type (vector_type_index) const;\n+  tree get_scalar_const_ptr_type (vector_type_index) const;\n+  vector_type_index get_function_type_index (vector_type_index) const;\n+  tree get_scalar_type (vector_type_index) const;\n+  tree get_vector_type (vector_type_index) const;\n   tree get_tree_type (vector_type_index) const;\n };\n \n@@ -352,6 +336,7 @@ class function_expander : public function_call_info\n   machine_mode index_mode (void) const;\n   machine_mode arg_mode (int) const;\n   machine_mode mask_mode (void) const;\n+  machine_mode ret_mode (void) const;\n \n   rtx use_exact_insn (insn_code);\n   rtx use_contiguous_load_insn (insn_code);\n@@ -410,6 +395,37 @@ class function_base\n   virtual rtx expand (function_expander &) const = 0;\n };\n \n+/* A class for checking that the semantic constraints on a function call are\n+   satisfied, such as arguments being integer constant expressions with\n+   a particular range.  The parent class's FNDECL is the decl that was\n+   called in the original source, before overload resolution.  */\n+class function_checker : public function_call_info\n+{\n+public:\n+  function_checker (location_t, const function_instance &, tree, tree,\n+\t\t    unsigned int, tree *);\n+\n+  machine_mode arg_mode (unsigned int) const;\n+  machine_mode ret_mode (void) const;\n+  bool check (void);\n+\n+  bool require_immediate (unsigned int, HOST_WIDE_INT, HOST_WIDE_INT) const;\n+\n+private:\n+  bool require_immediate_range (unsigned int, HOST_WIDE_INT,\n+\t\t\t\tHOST_WIDE_INT) const;\n+  void report_non_ice (unsigned int) const;\n+  void report_out_of_range (unsigned int, HOST_WIDE_INT, HOST_WIDE_INT,\n+\t\t\t    HOST_WIDE_INT) const;\n+\n+  /* The type of the resolved function.  */\n+  tree m_fntype;\n+\n+  /* The arguments to the function.  */\n+  unsigned int m_nargs;\n+  tree *m_args;\n+};\n+\n /* Classifies functions into \"shapes\" base on:\n \n    - Base name of the intrinsic function.\n@@ -430,13 +446,33 @@ class function_shape\n   /* Define all functions associated with the given group.  */\n   virtual void build (function_builder &, const function_group_info &) const\n     = 0;\n+\n+  /* Check whether the given call is semantically valid.  Return true\n+   if it is, otherwise report an error and return false.  */\n+  virtual bool check (function_checker &) const { return true; }\n };\n \n extern const char *const operand_suffixes[NUM_OP_TYPES];\n extern const rvv_builtin_suffixes type_suffixes[NUM_VECTOR_TYPES + 1];\n extern const char *const predication_suffixes[NUM_PRED_TYPES];\n extern rvv_builtin_types_t builtin_types[NUM_VECTOR_TYPES + 1];\n \n+inline tree\n+rvv_arg_type_info::get_scalar_type (vector_type_index type_idx) const\n+{\n+  return get_function_type_index (type_idx) == VECTOR_TYPE_INVALID\n+\t   ? NULL_TREE\n+\t   : builtin_types[get_function_type_index (type_idx)].scalar;\n+}\n+\n+inline tree\n+rvv_arg_type_info::get_vector_type (vector_type_index type_idx) const\n+{\n+  return get_function_type_index (type_idx) == VECTOR_TYPE_INVALID\n+\t   ? NULL_TREE\n+\t   : builtin_types[get_function_type_index (type_idx)].vector;\n+}\n+\n inline bool\n function_instance::operator!= (const function_instance &other) const\n {\n@@ -516,6 +552,25 @@ function_expander::arg_mode (int idx) const\n   return TYPE_MODE (op_info->args[idx].get_tree_type (type.index));\n }\n \n+/* Return the machine_mode of the corresponding return type.  */\n+inline machine_mode\n+function_expander::ret_mode (void) const\n+{\n+  return TYPE_MODE (op_info->ret.get_tree_type (type.index));\n+}\n+\n+inline machine_mode\n+function_checker::arg_mode (unsigned int argno) const\n+{\n+  return TYPE_MODE (TREE_TYPE (m_args[argno]));\n+}\n+\n+inline machine_mode\n+function_checker::ret_mode () const\n+{\n+  return TYPE_MODE (TREE_TYPE (TREE_TYPE (fndecl)));\n+}\n+\n /* Default implementation of function_base::call_properties, with conservatively\n    correct behavior for floating-point instructions.  */\n inline unsigned int"}, {"sha": "c2fc860e4c3aabd737d28172bb5d8e8ad3f4647c", "filename": "gcc/config/riscv/t-riscv", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Ft-riscv", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Ft-riscv", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Ft-riscv?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -80,3 +80,21 @@ PASSES_EXTRA += $(srcdir)/config/riscv/riscv-passes.def\n $(common_out_file): $(srcdir)/config/riscv/riscv-cores.def \\\n     $(srcdir)/config/riscv/riscv-protos.h \\\n     $(srcdir)/config/riscv/riscv-subset.h\n+\n+build/genrvv-type-indexer.o: $(srcdir)/config/riscv/genrvv-type-indexer.cc $(RTL_BASE_H) $(BCONFIG_H) $(SYSTEM_H)\t\\\n+  $(CORETYPES_H) $(GTM_H) errors.h $(GENSUPPORT_H) insn-modes.h\n+\n+build/genrvv-type-indexer$(build_exeext): build/genrvv-type-indexer.o\n+\t+$(LINKER_FOR_BUILD) $(BUILD_LINKERFLAGS) $(BUILD_LDFLAGS) -o $@ \\\n+\t    $(filter-out $(BUILD_LIBDEPS), $^) $(BUILD_LIBS)\n+\n+$(srcdir)/config/riscv/riscv-vector-builtins.def: riscv-vector-type-indexer.gen.def\n+\n+riscv-vector-type-indexer.gen.def: s-riscv-vector-type-indexer.gen.defs ; @true\n+\n+s-riscv-vector-type-indexer.gen.defs: build/genrvv-type-indexer$(build_exeext)\n+\t$(RUN_GEN) build/genrvv-type-indexer$(build_exeext) tmp-riscv-vector-type-indexer.gen.def\n+\t$(SHELL) $(srcdir)/../move-if-change tmp-riscv-vector-type-indexer.gen.def    riscv-vector-type-indexer.gen.def\n+\t$(STAMP) s-riscv-vector-type-indexer.gen.defs\n+\n+genprog+=rvv-type-indexer"}, {"sha": "61e141e7b64afefb105ff93d90629c2e363cdd9f", "filename": "gcc/config/riscv/vector-iterators.md", "status": "modified", "additions": 96, "deletions": 0, "changes": 96, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Fvector-iterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Fvector-iterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Fvector-iterators.md?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -98,6 +98,59 @@\n   (VNx8DF \"TARGET_VECTOR_ELEN_FP_64\")\n ])\n \n+(define_mode_iterator VLMULEXT2 [\n+  VNx1QI VNx2QI VNx4QI VNx8QI VNx16QI VNx32QI\n+  VNx1HI VNx2HI VNx4HI VNx8HI VNx16HI\n+  VNx1SI VNx2SI VNx4SI VNx8SI\n+  (VNx1DI \"TARGET_MIN_VLEN > 32\") (VNx2DI \"TARGET_MIN_VLEN > 32\")\n+  (VNx4DI \"TARGET_MIN_VLEN > 32\")\n+  (VNx1SF \"TARGET_VECTOR_ELEN_FP_32\")\n+  (VNx2SF \"TARGET_VECTOR_ELEN_FP_32\")\n+  (VNx4SF \"TARGET_VECTOR_ELEN_FP_32\")\n+  (VNx8SF \"TARGET_VECTOR_ELEN_FP_32\")\n+  (VNx1DF \"TARGET_VECTOR_ELEN_FP_64\")\n+  (VNx2DF \"TARGET_VECTOR_ELEN_FP_64\")\n+  (VNx4DF \"TARGET_VECTOR_ELEN_FP_64\")\n+])\n+\n+(define_mode_iterator VLMULEXT4 [\n+  VNx1QI VNx2QI VNx4QI VNx8QI VNx16QI\n+  VNx1HI VNx2HI VNx4HI VNx8HI\n+  VNx1SI VNx2SI VNx4SI\n+  (VNx1DI \"TARGET_MIN_VLEN > 32\") (VNx2DI \"TARGET_MIN_VLEN > 32\")\n+  (VNx1SF \"TARGET_VECTOR_ELEN_FP_32\")\n+  (VNx2SF \"TARGET_VECTOR_ELEN_FP_32\")\n+  (VNx4SF \"TARGET_VECTOR_ELEN_FP_32\")\n+  (VNx1DF \"TARGET_VECTOR_ELEN_FP_64\")\n+  (VNx2DF \"TARGET_VECTOR_ELEN_FP_64\")\n+])\n+\n+(define_mode_iterator VLMULEXT8 [\n+  VNx1QI VNx2QI VNx4QI VNx8QI\n+  VNx1HI VNx2HI VNx4HI\n+  VNx1SI VNx2SI\n+  (VNx1DI \"TARGET_MIN_VLEN > 32\")\n+  (VNx1SF \"TARGET_VECTOR_ELEN_FP_32\")\n+  (VNx2SF \"TARGET_VECTOR_ELEN_FP_32\")\n+  (VNx1DF \"TARGET_VECTOR_ELEN_FP_64\")\n+])\n+\n+(define_mode_iterator VLMULEXT16 [\n+  VNx1QI VNx2QI VNx4QI\n+  VNx1HI VNx2HI\n+  VNx1SI\n+  (VNx1SF \"TARGET_VECTOR_ELEN_FP_32\")\n+])\n+\n+(define_mode_iterator VLMULEXT32 [\n+  VNx1QI VNx2QI\n+  VNx1HI\n+])\n+\n+(define_mode_iterator VLMULEXT64 [\n+  VNx1QI\n+])\n+\n (define_mode_iterator VEI16 [\n   VNx1QI VNx2QI VNx4QI VNx8QI VNx16QI VNx32QI\n   VNx1HI VNx2HI VNx4HI VNx8HI VNx16HI (VNx32HI \"TARGET_MIN_VLEN > 32\")\n@@ -317,6 +370,49 @@\n   (VNx4DI \"TARGET_MIN_VLEN > 32\") (VNx8DI \"TARGET_MIN_VLEN > 32\")\n ])\n \n+(define_mode_attr VLMULX2 [\n+  (VNx1QI \"VNx2QI\") (VNx2QI \"VNx4QI\") (VNx4QI \"VNx8QI\") (VNx8QI \"VNx16QI\") (VNx16QI \"VNx32QI\") (VNx32QI \"VNx64QI\")\n+  (VNx1HI \"VNx2HI\") (VNx2HI \"VNx4HI\") (VNx4HI \"VNx8HI\") (VNx8HI \"VNx16HI\") (VNx16HI \"VNx32HI\")\n+  (VNx1SI \"VNx2SI\") (VNx2SI \"VNx4SI\") (VNx4SI \"VNx8SI\") (VNx8SI \"VNx16SI\")\n+  (VNx1DI \"VNx2DI\") (VNx2DI \"VNx4DI\") (VNx4DI \"VNx8DI\")\n+  (VNx1SF \"VNx2SF\") (VNx2SF \"VNx4SF\") (VNx4SF \"VNx8SF\") (VNx8SF \"VNx16SF\")\n+  (VNx1DF \"VNx2DF\") (VNx2DF \"VNx4DF\") (VNx4DF \"VNx8DF\")\n+])\n+\n+(define_mode_attr VLMULX4 [\n+  (VNx1QI \"VNx4QI\") (VNx2QI \"VNx8QI\") (VNx4QI \"VNx16QI\") (VNx8QI \"VNx32QI\") (VNx16QI \"VNx64QI\")\n+  (VNx1HI \"VNx4HI\") (VNx2HI \"VNx8HI\") (VNx4HI \"VNx16HI\") (VNx8HI \"VNx32HI\")\n+  (VNx1SI \"VNx4SI\") (VNx2SI \"VNx8SI\") (VNx4SI \"VNx16SI\")\n+  (VNx1DI \"VNx4DI\") (VNx2DI \"VNx8DI\")\n+  (VNx1SF \"VNx4SF\") (VNx2SF \"VNx8SF\") (VNx4SF \"VNx16SF\")\n+  (VNx1DF \"VNx4DF\") (VNx2DF \"VNx8DF\")\n+])\n+\n+(define_mode_attr VLMULX8 [\n+  (VNx1QI \"VNx8QI\") (VNx2QI \"VNx16QI\") (VNx4QI \"VNx32QI\") (VNx8QI \"VNx64QI\")\n+  (VNx1HI \"VNx8HI\") (VNx2HI \"VNx16HI\") (VNx4HI \"VNx32HI\")\n+  (VNx1SI \"VNx8SI\") (VNx2SI \"VNx16SI\")\n+  (VNx1DI \"VNx8DI\")\n+  (VNx1SF \"VNx8SF\") (VNx2SF \"VNx16SF\")\n+  (VNx1DF \"VNx8DF\")\n+])\n+\n+(define_mode_attr VLMULX16 [\n+  (VNx1QI \"VNx16QI\") (VNx2QI \"VNx32QI\") (VNx4QI \"VNx64QI\")\n+  (VNx1HI \"VNx16HI\") (VNx2HI \"VNx32HI\")\n+  (VNx1SI \"VNx16SI\")\n+  (VNx1SF \"VNx16SF\")\n+])\n+\n+(define_mode_attr VLMULX32 [\n+  (VNx1QI \"VNx32QI\") (VNx2QI \"VNx64QI\")\n+  (VNx1HI \"VNx32HI\")\n+])\n+\n+(define_mode_attr VLMULX64 [\n+  (VNx1QI \"VNx64QI\")\n+])\n+\n (define_mode_attr VINDEX [\n   (VNx1QI \"VNx1QI\") (VNx2QI \"VNx2QI\") (VNx4QI \"VNx4QI\") (VNx8QI \"VNx8QI\")\n   (VNx16QI \"VNx16QI\") (VNx32QI \"VNx32QI\") (VNx64QI \"VNx64QI\")"}, {"sha": "2d4eb8bf1cda71be9546e9552c1b5075276961f6", "filename": "gcc/config/riscv/vector.md", "status": "modified", "additions": 132, "deletions": 2, "changes": 134, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Fvector.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Fconfig%2Friscv%2Fvector.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Fvector.md?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -354,12 +354,142 @@\n ;; ---- Miscellaneous Operations\n ;; -----------------------------------------------------------------\n \n-(define_insn \"vundefined<mode>\"\n+(define_insn \"@vundefined<mode>\"\n   [(set (match_operand:V 0 \"register_operand\" \"=vr\")\n-\t(unspec:V [(const_int 0)] UNSPEC_VUNDEF))]\n+\t(unspec:V [(reg:SI X0_REGNUM)] UNSPEC_VUNDEF))]\n   \"TARGET_VECTOR\"\n   \"\")\n \n+(define_expand \"@vreinterpret<mode>\"\n+  [(set (match_operand:V 0 \"register_operand\")\n+\t(match_operand 1 \"vector_any_register_operand\"))]\n+  \"TARGET_VECTOR\"\n+  {\n+    emit_move_insn (operands[0], gen_lowpart (<MODE>mode, operands[1]));\n+    DONE;\n+  }\n+)\n+\n+(define_expand \"@vlmul_extx2<mode>\"\n+  [(set (match_operand:<VLMULX2> 0 \"register_operand\")\n+  \t(subreg:<VLMULX2>\n+  \t  (match_operand:VLMULEXT2 1 \"register_operand\") 0))]\n+  \"TARGET_VECTOR\"\n+{})\n+\n+(define_expand \"@vlmul_extx4<mode>\"\n+  [(set (match_operand:<VLMULX4> 0 \"register_operand\")\n+  \t(subreg:<VLMULX4>\n+  \t  (match_operand:VLMULEXT4 1 \"register_operand\") 0))]\n+  \"TARGET_VECTOR\"\n+{})\n+\n+(define_expand \"@vlmul_extx8<mode>\"\n+  [(set (match_operand:<VLMULX8> 0 \"register_operand\")\n+  \t(subreg:<VLMULX8>\n+  \t  (match_operand:VLMULEXT8 1 \"register_operand\") 0))]\n+  \"TARGET_VECTOR\"\n+{})\n+\n+(define_expand \"@vlmul_extx16<mode>\"\n+  [(set (match_operand:<VLMULX16> 0 \"register_operand\")\n+  \t(subreg:<VLMULX16>\n+  \t  (match_operand:VLMULEXT16 1 \"register_operand\") 0))]\n+  \"TARGET_VECTOR\"\n+{})\n+\n+(define_expand \"@vlmul_extx32<mode>\"\n+  [(set (match_operand:<VLMULX32> 0 \"register_operand\")\n+  \t(subreg:<VLMULX32>\n+  \t  (match_operand:VLMULEXT32 1 \"register_operand\") 0))]\n+  \"TARGET_VECTOR\"\n+{})\n+\n+(define_expand \"@vlmul_extx64<mode>\"\n+  [(set (match_operand:<VLMULX64> 0 \"register_operand\")\n+  \t(subreg:<VLMULX64>\n+  \t  (match_operand:VLMULEXT64 1 \"register_operand\") 0))]\n+  \"TARGET_VECTOR\"\n+{})\n+\n+(define_insn_and_split \"*vlmul_extx2<mode>\"\n+  [(set (match_operand:<VLMULX2> 0 \"register_operand\"  \"=vr, ?&vr\")\n+\t(subreg:<VLMULX2>\n+\t  (match_operand:VLMULEXT2 1 \"register_operand\" \" 0,   vr\") 0))]\n+  \"TARGET_VECTOR\"\n+  \"#\"\n+  \"&& reload_completed\"\n+  [(const_int 0)]\n+{\n+  emit_insn (gen_rtx_SET (gen_lowpart (<MODE>mode, operands[0]), operands[1]));\n+  DONE;\n+})\n+\n+(define_insn_and_split \"*vlmul_extx4<mode>\"\n+  [(set (match_operand:<VLMULX4> 0 \"register_operand\"  \"=vr, ?&vr\")\n+\t(subreg:<VLMULX4>\n+\t  (match_operand:VLMULEXT4 1 \"register_operand\" \" 0,   vr\") 0))]\n+  \"TARGET_VECTOR\"\n+  \"#\"\n+  \"&& reload_completed\"\n+  [(const_int 0)]\n+{\n+  emit_insn (gen_rtx_SET (gen_lowpart (<MODE>mode, operands[0]), operands[1]));\n+  DONE;\n+})\n+\n+(define_insn_and_split \"*vlmul_extx8<mode>\"\n+  [(set (match_operand:<VLMULX8> 0 \"register_operand\"  \"=vr, ?&vr\")\n+\t(subreg:<VLMULX8>\n+\t  (match_operand:VLMULEXT8 1 \"register_operand\" \" 0,   vr\") 0))]\n+  \"TARGET_VECTOR\"\n+  \"#\"\n+  \"&& reload_completed\"\n+  [(const_int 0)]\n+{\n+  emit_insn (gen_rtx_SET (gen_lowpart (<MODE>mode, operands[0]), operands[1]));\n+  DONE;\n+})\n+\n+(define_insn_and_split \"*vlmul_extx16<mode>\"\n+  [(set (match_operand:<VLMULX16> 0 \"register_operand\"  \"=vr, ?&vr\")\n+\t(subreg:<VLMULX16>\n+\t  (match_operand:VLMULEXT16 1 \"register_operand\" \" 0,   vr\") 0))]\n+  \"TARGET_VECTOR\"\n+  \"#\"\n+  \"&& reload_completed\"\n+  [(const_int 0)]\n+{\n+  emit_insn (gen_rtx_SET (gen_lowpart (<MODE>mode, operands[0]), operands[1]));\n+  DONE;\n+})\n+\n+(define_insn_and_split \"*vlmul_extx32<mode>\"\n+  [(set (match_operand:<VLMULX32> 0 \"register_operand\"  \"=vr, ?&vr\")\n+\t(subreg:<VLMULX32>\n+\t  (match_operand:VLMULEXT32 1 \"register_operand\" \" 0,   vr\") 0))]\n+  \"TARGET_VECTOR\"\n+  \"#\"\n+  \"&& reload_completed\"\n+  [(const_int 0)]\n+{\n+  emit_insn (gen_rtx_SET (gen_lowpart (<MODE>mode, operands[0]), operands[1]));\n+  DONE;\n+})\n+\n+(define_insn_and_split \"*vlmul_extx64<mode>\"\n+  [(set (match_operand:<VLMULX64> 0 \"register_operand\"  \"=vr, ?&vr\")\n+\t(subreg:<VLMULX64>\n+\t  (match_operand:VLMULEXT64 1 \"register_operand\" \" 0,   vr\") 0))]\n+  \"TARGET_VECTOR\"\n+  \"#\"\n+  \"&& reload_completed\"\n+  [(const_int 0)]\n+{\n+  emit_insn (gen_rtx_SET (gen_lowpart (<MODE>mode, operands[0]), operands[1]));\n+  DONE;\n+})\n+\n ;; This pattern is used to hold the AVL operand for\n ;; RVV instructions that implicity use VLMAX AVL.\n ;; RVV instruction implicitly use GPR that is ultimately"}, {"sha": "1925ae37c897b34c69bc4eab481a401e62d8cfc2", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vlmul_v.c", "status": "added", "additions": 1448, "deletions": 0, "changes": 1448, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvlmul_v.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7caa1ae5e451e780fbc4746a54e3f19d4f4304dc/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvlmul_v.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvlmul_v.c?ref=7caa1ae5e451e780fbc4746a54e3f19d4f4304dc", "patch": "@@ -0,0 +1,1448 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vfloat32m1_t test___riscv_vlmul_ext_v_f32mf2_f32m1(vfloat32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f32mf2_f32m1(op1);\n+}\n+\n+\n+vfloat32m2_t test___riscv_vlmul_ext_v_f32mf2_f32m2(vfloat32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f32mf2_f32m2(op1);\n+}\n+\n+\n+vfloat32m4_t test___riscv_vlmul_ext_v_f32mf2_f32m4(vfloat32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f32mf2_f32m4(op1);\n+}\n+\n+\n+vfloat32m8_t test___riscv_vlmul_ext_v_f32mf2_f32m8(vfloat32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f32mf2_f32m8(op1);\n+}\n+\n+\n+vfloat32m2_t test___riscv_vlmul_ext_v_f32m1_f32m2(vfloat32m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f32m1_f32m2(op1);\n+}\n+\n+\n+vfloat32m4_t test___riscv_vlmul_ext_v_f32m1_f32m4(vfloat32m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f32m1_f32m4(op1);\n+}\n+\n+\n+vfloat32m8_t test___riscv_vlmul_ext_v_f32m1_f32m8(vfloat32m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f32m1_f32m8(op1);\n+}\n+\n+\n+vfloat32m4_t test___riscv_vlmul_ext_v_f32m2_f32m4(vfloat32m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f32m2_f32m4(op1);\n+}\n+\n+\n+vfloat32m8_t test___riscv_vlmul_ext_v_f32m2_f32m8(vfloat32m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f32m2_f32m8(op1);\n+}\n+\n+\n+vfloat32m8_t test___riscv_vlmul_ext_v_f32m4_f32m8(vfloat32m4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f32m4_f32m8(op1);\n+}\n+\n+\n+vfloat64m2_t test___riscv_vlmul_ext_v_f64m1_f64m2(vfloat64m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f64m1_f64m2(op1);\n+}\n+\n+\n+vfloat64m4_t test___riscv_vlmul_ext_v_f64m1_f64m4(vfloat64m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f64m1_f64m4(op1);\n+}\n+\n+\n+vfloat64m8_t test___riscv_vlmul_ext_v_f64m1_f64m8(vfloat64m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f64m1_f64m8(op1);\n+}\n+\n+\n+vfloat64m4_t test___riscv_vlmul_ext_v_f64m2_f64m4(vfloat64m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f64m2_f64m4(op1);\n+}\n+\n+\n+vfloat64m8_t test___riscv_vlmul_ext_v_f64m2_f64m8(vfloat64m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f64m2_f64m8(op1);\n+}\n+\n+\n+vfloat64m8_t test___riscv_vlmul_ext_v_f64m4_f64m8(vfloat64m4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_f64m4_f64m8(op1);\n+}\n+\n+\n+vint8mf4_t test___riscv_vlmul_ext_v_i8mf8_i8mf4(vint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf8_i8mf4(op1);\n+}\n+\n+\n+vint8mf2_t test___riscv_vlmul_ext_v_i8mf8_i8mf2(vint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf8_i8mf2(op1);\n+}\n+\n+\n+vint8m1_t test___riscv_vlmul_ext_v_i8mf8_i8m1(vint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf8_i8m1(op1);\n+}\n+\n+\n+vint8m2_t test___riscv_vlmul_ext_v_i8mf8_i8m2(vint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf8_i8m2(op1);\n+}\n+\n+\n+vint8m4_t test___riscv_vlmul_ext_v_i8mf8_i8m4(vint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf8_i8m4(op1);\n+}\n+\n+\n+vint8m8_t test___riscv_vlmul_ext_v_i8mf8_i8m8(vint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf8_i8m8(op1);\n+}\n+\n+\n+vint8mf2_t test___riscv_vlmul_ext_v_i8mf4_i8mf2(vint8mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf4_i8mf2(op1);\n+}\n+\n+\n+vint8m1_t test___riscv_vlmul_ext_v_i8mf4_i8m1(vint8mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf4_i8m1(op1);\n+}\n+\n+\n+vint8m2_t test___riscv_vlmul_ext_v_i8mf4_i8m2(vint8mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf4_i8m2(op1);\n+}\n+\n+\n+vint8m4_t test___riscv_vlmul_ext_v_i8mf4_i8m4(vint8mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf4_i8m4(op1);\n+}\n+\n+\n+vint8m8_t test___riscv_vlmul_ext_v_i8mf4_i8m8(vint8mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf4_i8m8(op1);\n+}\n+\n+\n+vint8m1_t test___riscv_vlmul_ext_v_i8mf2_i8m1(vint8mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf2_i8m1(op1);\n+}\n+\n+\n+vint8m2_t test___riscv_vlmul_ext_v_i8mf2_i8m2(vint8mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf2_i8m2(op1);\n+}\n+\n+\n+vint8m4_t test___riscv_vlmul_ext_v_i8mf2_i8m4(vint8mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf2_i8m4(op1);\n+}\n+\n+\n+vint8m8_t test___riscv_vlmul_ext_v_i8mf2_i8m8(vint8mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8mf2_i8m8(op1);\n+}\n+\n+\n+vint8m2_t test___riscv_vlmul_ext_v_i8m1_i8m2(vint8m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8m1_i8m2(op1);\n+}\n+\n+\n+vint8m4_t test___riscv_vlmul_ext_v_i8m1_i8m4(vint8m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8m1_i8m4(op1);\n+}\n+\n+\n+vint8m8_t test___riscv_vlmul_ext_v_i8m1_i8m8(vint8m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8m1_i8m8(op1);\n+}\n+\n+\n+vint8m4_t test___riscv_vlmul_ext_v_i8m2_i8m4(vint8m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8m2_i8m4(op1);\n+}\n+\n+\n+vint8m8_t test___riscv_vlmul_ext_v_i8m2_i8m8(vint8m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8m2_i8m8(op1);\n+}\n+\n+\n+vint8m8_t test___riscv_vlmul_ext_v_i8m4_i8m8(vint8m4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i8m4_i8m8(op1);\n+}\n+\n+\n+vint16mf2_t test___riscv_vlmul_ext_v_i16mf4_i16mf2(vint16mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16mf4_i16mf2(op1);\n+}\n+\n+\n+vint16m1_t test___riscv_vlmul_ext_v_i16mf4_i16m1(vint16mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16mf4_i16m1(op1);\n+}\n+\n+\n+vint16m2_t test___riscv_vlmul_ext_v_i16mf4_i16m2(vint16mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16mf4_i16m2(op1);\n+}\n+\n+\n+vint16m4_t test___riscv_vlmul_ext_v_i16mf4_i16m4(vint16mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16mf4_i16m4(op1);\n+}\n+\n+\n+vint16m8_t test___riscv_vlmul_ext_v_i16mf4_i16m8(vint16mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16mf4_i16m8(op1);\n+}\n+\n+\n+vint16m1_t test___riscv_vlmul_ext_v_i16mf2_i16m1(vint16mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16mf2_i16m1(op1);\n+}\n+\n+\n+vint16m2_t test___riscv_vlmul_ext_v_i16mf2_i16m2(vint16mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16mf2_i16m2(op1);\n+}\n+\n+\n+vint16m4_t test___riscv_vlmul_ext_v_i16mf2_i16m4(vint16mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16mf2_i16m4(op1);\n+}\n+\n+\n+vint16m8_t test___riscv_vlmul_ext_v_i16mf2_i16m8(vint16mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16mf2_i16m8(op1);\n+}\n+\n+\n+vint16m2_t test___riscv_vlmul_ext_v_i16m1_i16m2(vint16m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16m1_i16m2(op1);\n+}\n+\n+\n+vint16m4_t test___riscv_vlmul_ext_v_i16m1_i16m4(vint16m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16m1_i16m4(op1);\n+}\n+\n+\n+vint16m8_t test___riscv_vlmul_ext_v_i16m1_i16m8(vint16m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16m1_i16m8(op1);\n+}\n+\n+\n+vint16m4_t test___riscv_vlmul_ext_v_i16m2_i16m4(vint16m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16m2_i16m4(op1);\n+}\n+\n+\n+vint16m8_t test___riscv_vlmul_ext_v_i16m2_i16m8(vint16m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16m2_i16m8(op1);\n+}\n+\n+\n+vint16m8_t test___riscv_vlmul_ext_v_i16m4_i16m8(vint16m4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i16m4_i16m8(op1);\n+}\n+\n+\n+vint32m1_t test___riscv_vlmul_ext_v_i32mf2_i32m1(vint32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i32mf2_i32m1(op1);\n+}\n+\n+\n+vint32m2_t test___riscv_vlmul_ext_v_i32mf2_i32m2(vint32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i32mf2_i32m2(op1);\n+}\n+\n+\n+vint32m4_t test___riscv_vlmul_ext_v_i32mf2_i32m4(vint32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i32mf2_i32m4(op1);\n+}\n+\n+\n+vint32m8_t test___riscv_vlmul_ext_v_i32mf2_i32m8(vint32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i32mf2_i32m8(op1);\n+}\n+\n+\n+vint32m2_t test___riscv_vlmul_ext_v_i32m1_i32m2(vint32m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i32m1_i32m2(op1);\n+}\n+\n+\n+vint32m4_t test___riscv_vlmul_ext_v_i32m1_i32m4(vint32m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i32m1_i32m4(op1);\n+}\n+\n+\n+vint32m8_t test___riscv_vlmul_ext_v_i32m1_i32m8(vint32m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i32m1_i32m8(op1);\n+}\n+\n+\n+vint32m4_t test___riscv_vlmul_ext_v_i32m2_i32m4(vint32m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i32m2_i32m4(op1);\n+}\n+\n+\n+vint32m8_t test___riscv_vlmul_ext_v_i32m2_i32m8(vint32m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i32m2_i32m8(op1);\n+}\n+\n+\n+vint32m8_t test___riscv_vlmul_ext_v_i32m4_i32m8(vint32m4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i32m4_i32m8(op1);\n+}\n+\n+\n+vint64m2_t test___riscv_vlmul_ext_v_i64m1_i64m2(vint64m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i64m1_i64m2(op1);\n+}\n+\n+\n+vint64m4_t test___riscv_vlmul_ext_v_i64m1_i64m4(vint64m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i64m1_i64m4(op1);\n+}\n+\n+\n+vint64m8_t test___riscv_vlmul_ext_v_i64m1_i64m8(vint64m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i64m1_i64m8(op1);\n+}\n+\n+\n+vint64m4_t test___riscv_vlmul_ext_v_i64m2_i64m4(vint64m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i64m2_i64m4(op1);\n+}\n+\n+\n+vint64m8_t test___riscv_vlmul_ext_v_i64m2_i64m8(vint64m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i64m2_i64m8(op1);\n+}\n+\n+\n+vint64m8_t test___riscv_vlmul_ext_v_i64m4_i64m8(vint64m4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_i64m4_i64m8(op1);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vlmul_ext_v_u8mf8_u8mf4(vuint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf8_u8mf4(op1);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vlmul_ext_v_u8mf8_u8mf2(vuint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf8_u8mf2(op1);\n+}\n+\n+\n+vuint8m1_t test___riscv_vlmul_ext_v_u8mf8_u8m1(vuint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf8_u8m1(op1);\n+}\n+\n+\n+vuint8m2_t test___riscv_vlmul_ext_v_u8mf8_u8m2(vuint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf8_u8m2(op1);\n+}\n+\n+\n+vuint8m4_t test___riscv_vlmul_ext_v_u8mf8_u8m4(vuint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf8_u8m4(op1);\n+}\n+\n+\n+vuint8m8_t test___riscv_vlmul_ext_v_u8mf8_u8m8(vuint8mf8_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf8_u8m8(op1);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vlmul_ext_v_u8mf4_u8mf2(vuint8mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf4_u8mf2(op1);\n+}\n+\n+\n+vuint8m1_t test___riscv_vlmul_ext_v_u8mf4_u8m1(vuint8mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf4_u8m1(op1);\n+}\n+\n+\n+vuint8m2_t test___riscv_vlmul_ext_v_u8mf4_u8m2(vuint8mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf4_u8m2(op1);\n+}\n+\n+\n+vuint8m4_t test___riscv_vlmul_ext_v_u8mf4_u8m4(vuint8mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf4_u8m4(op1);\n+}\n+\n+\n+vuint8m8_t test___riscv_vlmul_ext_v_u8mf4_u8m8(vuint8mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf4_u8m8(op1);\n+}\n+\n+\n+vuint8m1_t test___riscv_vlmul_ext_v_u8mf2_u8m1(vuint8mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf2_u8m1(op1);\n+}\n+\n+\n+vuint8m2_t test___riscv_vlmul_ext_v_u8mf2_u8m2(vuint8mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf2_u8m2(op1);\n+}\n+\n+\n+vuint8m4_t test___riscv_vlmul_ext_v_u8mf2_u8m4(vuint8mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf2_u8m4(op1);\n+}\n+\n+\n+vuint8m8_t test___riscv_vlmul_ext_v_u8mf2_u8m8(vuint8mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8mf2_u8m8(op1);\n+}\n+\n+\n+vuint8m2_t test___riscv_vlmul_ext_v_u8m1_u8m2(vuint8m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8m1_u8m2(op1);\n+}\n+\n+\n+vuint8m4_t test___riscv_vlmul_ext_v_u8m1_u8m4(vuint8m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8m1_u8m4(op1);\n+}\n+\n+\n+vuint8m8_t test___riscv_vlmul_ext_v_u8m1_u8m8(vuint8m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8m1_u8m8(op1);\n+}\n+\n+\n+vuint8m4_t test___riscv_vlmul_ext_v_u8m2_u8m4(vuint8m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8m2_u8m4(op1);\n+}\n+\n+\n+vuint8m8_t test___riscv_vlmul_ext_v_u8m2_u8m8(vuint8m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8m2_u8m8(op1);\n+}\n+\n+\n+vuint8m8_t test___riscv_vlmul_ext_v_u8m4_u8m8(vuint8m4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u8m4_u8m8(op1);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vlmul_ext_v_u16mf4_u16mf2(vuint16mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16mf4_u16mf2(op1);\n+}\n+\n+\n+vuint16m1_t test___riscv_vlmul_ext_v_u16mf4_u16m1(vuint16mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16mf4_u16m1(op1);\n+}\n+\n+\n+vuint16m2_t test___riscv_vlmul_ext_v_u16mf4_u16m2(vuint16mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16mf4_u16m2(op1);\n+}\n+\n+\n+vuint16m4_t test___riscv_vlmul_ext_v_u16mf4_u16m4(vuint16mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16mf4_u16m4(op1);\n+}\n+\n+\n+vuint16m8_t test___riscv_vlmul_ext_v_u16mf4_u16m8(vuint16mf4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16mf4_u16m8(op1);\n+}\n+\n+\n+vuint16m1_t test___riscv_vlmul_ext_v_u16mf2_u16m1(vuint16mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16mf2_u16m1(op1);\n+}\n+\n+\n+vuint16m2_t test___riscv_vlmul_ext_v_u16mf2_u16m2(vuint16mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16mf2_u16m2(op1);\n+}\n+\n+\n+vuint16m4_t test___riscv_vlmul_ext_v_u16mf2_u16m4(vuint16mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16mf2_u16m4(op1);\n+}\n+\n+\n+vuint16m8_t test___riscv_vlmul_ext_v_u16mf2_u16m8(vuint16mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16mf2_u16m8(op1);\n+}\n+\n+\n+vuint16m2_t test___riscv_vlmul_ext_v_u16m1_u16m2(vuint16m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16m1_u16m2(op1);\n+}\n+\n+\n+vuint16m4_t test___riscv_vlmul_ext_v_u16m1_u16m4(vuint16m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16m1_u16m4(op1);\n+}\n+\n+\n+vuint16m8_t test___riscv_vlmul_ext_v_u16m1_u16m8(vuint16m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16m1_u16m8(op1);\n+}\n+\n+\n+vuint16m4_t test___riscv_vlmul_ext_v_u16m2_u16m4(vuint16m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16m2_u16m4(op1);\n+}\n+\n+\n+vuint16m8_t test___riscv_vlmul_ext_v_u16m2_u16m8(vuint16m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16m2_u16m8(op1);\n+}\n+\n+\n+vuint16m8_t test___riscv_vlmul_ext_v_u16m4_u16m8(vuint16m4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u16m4_u16m8(op1);\n+}\n+\n+\n+vuint32m1_t test___riscv_vlmul_ext_v_u32mf2_u32m1(vuint32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u32mf2_u32m1(op1);\n+}\n+\n+\n+vuint32m2_t test___riscv_vlmul_ext_v_u32mf2_u32m2(vuint32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u32mf2_u32m2(op1);\n+}\n+\n+\n+vuint32m4_t test___riscv_vlmul_ext_v_u32mf2_u32m4(vuint32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u32mf2_u32m4(op1);\n+}\n+\n+\n+vuint32m8_t test___riscv_vlmul_ext_v_u32mf2_u32m8(vuint32mf2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u32mf2_u32m8(op1);\n+}\n+\n+\n+vuint32m2_t test___riscv_vlmul_ext_v_u32m1_u32m2(vuint32m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u32m1_u32m2(op1);\n+}\n+\n+\n+vuint32m4_t test___riscv_vlmul_ext_v_u32m1_u32m4(vuint32m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u32m1_u32m4(op1);\n+}\n+\n+\n+vuint32m8_t test___riscv_vlmul_ext_v_u32m1_u32m8(vuint32m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u32m1_u32m8(op1);\n+}\n+\n+\n+vuint32m4_t test___riscv_vlmul_ext_v_u32m2_u32m4(vuint32m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u32m2_u32m4(op1);\n+}\n+\n+\n+vuint32m8_t test___riscv_vlmul_ext_v_u32m2_u32m8(vuint32m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u32m2_u32m8(op1);\n+}\n+\n+\n+vuint32m8_t test___riscv_vlmul_ext_v_u32m4_u32m8(vuint32m4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u32m4_u32m8(op1);\n+}\n+\n+\n+vuint64m2_t test___riscv_vlmul_ext_v_u64m1_u64m2(vuint64m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u64m1_u64m2(op1);\n+}\n+\n+\n+vuint64m4_t test___riscv_vlmul_ext_v_u64m1_u64m4(vuint64m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u64m1_u64m4(op1);\n+}\n+\n+\n+vuint64m8_t test___riscv_vlmul_ext_v_u64m1_u64m8(vuint64m1_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u64m1_u64m8(op1);\n+}\n+\n+\n+vuint64m4_t test___riscv_vlmul_ext_v_u64m2_u64m4(vuint64m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u64m2_u64m4(op1);\n+}\n+\n+\n+vuint64m8_t test___riscv_vlmul_ext_v_u64m2_u64m8(vuint64m2_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u64m2_u64m8(op1);\n+}\n+\n+\n+vuint64m8_t test___riscv_vlmul_ext_v_u64m4_u64m8(vuint64m4_t op1)\n+{\n+    return __riscv_vlmul_ext_v_u64m4_u64m8(op1);\n+}\n+\n+\n+vfloat32mf2_t test___riscv_vlmul_trunc_v_f32m1_f32mf2(vfloat32m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f32m1_f32mf2(op1);\n+}\n+\n+\n+vfloat32mf2_t test___riscv_vlmul_trunc_v_f32m2_f32mf2(vfloat32m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f32m2_f32mf2(op1);\n+}\n+\n+\n+vfloat32m1_t test___riscv_vlmul_trunc_v_f32m2_f32m1(vfloat32m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f32m2_f32m1(op1);\n+}\n+\n+\n+vfloat32mf2_t test___riscv_vlmul_trunc_v_f32m4_f32mf2(vfloat32m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f32m4_f32mf2(op1);\n+}\n+\n+\n+vfloat32m1_t test___riscv_vlmul_trunc_v_f32m4_f32m1(vfloat32m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f32m4_f32m1(op1);\n+}\n+\n+\n+vfloat32m2_t test___riscv_vlmul_trunc_v_f32m4_f32m2(vfloat32m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f32m4_f32m2(op1);\n+}\n+\n+\n+vfloat32mf2_t test___riscv_vlmul_trunc_v_f32m8_f32mf2(vfloat32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f32m8_f32mf2(op1);\n+}\n+\n+\n+vfloat32m1_t test___riscv_vlmul_trunc_v_f32m8_f32m1(vfloat32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f32m8_f32m1(op1);\n+}\n+\n+\n+vfloat32m2_t test___riscv_vlmul_trunc_v_f32m8_f32m2(vfloat32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f32m8_f32m2(op1);\n+}\n+\n+\n+vfloat32m4_t test___riscv_vlmul_trunc_v_f32m8_f32m4(vfloat32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f32m8_f32m4(op1);\n+}\n+\n+\n+vfloat64m1_t test___riscv_vlmul_trunc_v_f64m2_f64m1(vfloat64m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f64m2_f64m1(op1);\n+}\n+\n+\n+vfloat64m1_t test___riscv_vlmul_trunc_v_f64m4_f64m1(vfloat64m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f64m4_f64m1(op1);\n+}\n+\n+\n+vfloat64m2_t test___riscv_vlmul_trunc_v_f64m4_f64m2(vfloat64m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f64m4_f64m2(op1);\n+}\n+\n+\n+vfloat64m1_t test___riscv_vlmul_trunc_v_f64m8_f64m1(vfloat64m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f64m8_f64m1(op1);\n+}\n+\n+\n+vfloat64m2_t test___riscv_vlmul_trunc_v_f64m8_f64m2(vfloat64m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f64m8_f64m2(op1);\n+}\n+\n+\n+vfloat64m4_t test___riscv_vlmul_trunc_v_f64m8_f64m4(vfloat64m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_f64m8_f64m4(op1);\n+}\n+\n+\n+vint8mf8_t test___riscv_vlmul_trunc_v_i8mf4_i8mf8(vint8mf4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8mf4_i8mf8(op1);\n+}\n+\n+\n+vint8mf8_t test___riscv_vlmul_trunc_v_i8mf2_i8mf8(vint8mf2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8mf2_i8mf8(op1);\n+}\n+\n+\n+vint8mf4_t test___riscv_vlmul_trunc_v_i8mf2_i8mf4(vint8mf2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8mf2_i8mf4(op1);\n+}\n+\n+\n+vint8mf8_t test___riscv_vlmul_trunc_v_i8m1_i8mf8(vint8m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m1_i8mf8(op1);\n+}\n+\n+\n+vint8mf4_t test___riscv_vlmul_trunc_v_i8m1_i8mf4(vint8m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m1_i8mf4(op1);\n+}\n+\n+\n+vint8mf2_t test___riscv_vlmul_trunc_v_i8m1_i8mf2(vint8m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m1_i8mf2(op1);\n+}\n+\n+\n+vint8mf8_t test___riscv_vlmul_trunc_v_i8m2_i8mf8(vint8m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m2_i8mf8(op1);\n+}\n+\n+\n+vint8mf4_t test___riscv_vlmul_trunc_v_i8m2_i8mf4(vint8m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m2_i8mf4(op1);\n+}\n+\n+\n+vint8mf2_t test___riscv_vlmul_trunc_v_i8m2_i8mf2(vint8m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m2_i8mf2(op1);\n+}\n+\n+\n+vint8m1_t test___riscv_vlmul_trunc_v_i8m2_i8m1(vint8m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m2_i8m1(op1);\n+}\n+\n+\n+vint8mf8_t test___riscv_vlmul_trunc_v_i8m4_i8mf8(vint8m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m4_i8mf8(op1);\n+}\n+\n+\n+vint8mf4_t test___riscv_vlmul_trunc_v_i8m4_i8mf4(vint8m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m4_i8mf4(op1);\n+}\n+\n+\n+vint8mf2_t test___riscv_vlmul_trunc_v_i8m4_i8mf2(vint8m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m4_i8mf2(op1);\n+}\n+\n+\n+vint8m1_t test___riscv_vlmul_trunc_v_i8m4_i8m1(vint8m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m4_i8m1(op1);\n+}\n+\n+\n+vint8m2_t test___riscv_vlmul_trunc_v_i8m4_i8m2(vint8m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m4_i8m2(op1);\n+}\n+\n+\n+vint8mf8_t test___riscv_vlmul_trunc_v_i8m8_i8mf8(vint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m8_i8mf8(op1);\n+}\n+\n+\n+vint8mf4_t test___riscv_vlmul_trunc_v_i8m8_i8mf4(vint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m8_i8mf4(op1);\n+}\n+\n+\n+vint8mf2_t test___riscv_vlmul_trunc_v_i8m8_i8mf2(vint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m8_i8mf2(op1);\n+}\n+\n+\n+vint8m1_t test___riscv_vlmul_trunc_v_i8m8_i8m1(vint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m8_i8m1(op1);\n+}\n+\n+\n+vint8m2_t test___riscv_vlmul_trunc_v_i8m8_i8m2(vint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m8_i8m2(op1);\n+}\n+\n+\n+vint8m4_t test___riscv_vlmul_trunc_v_i8m8_i8m4(vint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i8m8_i8m4(op1);\n+}\n+\n+\n+vint16mf4_t test___riscv_vlmul_trunc_v_i16mf2_i16mf4(vint16mf2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16mf2_i16mf4(op1);\n+}\n+\n+\n+vint16mf4_t test___riscv_vlmul_trunc_v_i16m1_i16mf4(vint16m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m1_i16mf4(op1);\n+}\n+\n+\n+vint16mf2_t test___riscv_vlmul_trunc_v_i16m1_i16mf2(vint16m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m1_i16mf2(op1);\n+}\n+\n+\n+vint16mf4_t test___riscv_vlmul_trunc_v_i16m2_i16mf4(vint16m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m2_i16mf4(op1);\n+}\n+\n+\n+vint16mf2_t test___riscv_vlmul_trunc_v_i16m2_i16mf2(vint16m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m2_i16mf2(op1);\n+}\n+\n+\n+vint16m1_t test___riscv_vlmul_trunc_v_i16m2_i16m1(vint16m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m2_i16m1(op1);\n+}\n+\n+\n+vint16mf4_t test___riscv_vlmul_trunc_v_i16m4_i16mf4(vint16m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m4_i16mf4(op1);\n+}\n+\n+\n+vint16mf2_t test___riscv_vlmul_trunc_v_i16m4_i16mf2(vint16m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m4_i16mf2(op1);\n+}\n+\n+\n+vint16m1_t test___riscv_vlmul_trunc_v_i16m4_i16m1(vint16m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m4_i16m1(op1);\n+}\n+\n+\n+vint16m2_t test___riscv_vlmul_trunc_v_i16m4_i16m2(vint16m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m4_i16m2(op1);\n+}\n+\n+\n+vint16mf4_t test___riscv_vlmul_trunc_v_i16m8_i16mf4(vint16m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m8_i16mf4(op1);\n+}\n+\n+\n+vint16mf2_t test___riscv_vlmul_trunc_v_i16m8_i16mf2(vint16m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m8_i16mf2(op1);\n+}\n+\n+\n+vint16m1_t test___riscv_vlmul_trunc_v_i16m8_i16m1(vint16m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m8_i16m1(op1);\n+}\n+\n+\n+vint16m2_t test___riscv_vlmul_trunc_v_i16m8_i16m2(vint16m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m8_i16m2(op1);\n+}\n+\n+\n+vint16m4_t test___riscv_vlmul_trunc_v_i16m8_i16m4(vint16m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i16m8_i16m4(op1);\n+}\n+\n+\n+vint32mf2_t test___riscv_vlmul_trunc_v_i32m1_i32mf2(vint32m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i32m1_i32mf2(op1);\n+}\n+\n+\n+vint32mf2_t test___riscv_vlmul_trunc_v_i32m2_i32mf2(vint32m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i32m2_i32mf2(op1);\n+}\n+\n+\n+vint32m1_t test___riscv_vlmul_trunc_v_i32m2_i32m1(vint32m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i32m2_i32m1(op1);\n+}\n+\n+\n+vint32mf2_t test___riscv_vlmul_trunc_v_i32m4_i32mf2(vint32m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i32m4_i32mf2(op1);\n+}\n+\n+\n+vint32m1_t test___riscv_vlmul_trunc_v_i32m4_i32m1(vint32m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i32m4_i32m1(op1);\n+}\n+\n+\n+vint32m2_t test___riscv_vlmul_trunc_v_i32m4_i32m2(vint32m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i32m4_i32m2(op1);\n+}\n+\n+\n+vint32mf2_t test___riscv_vlmul_trunc_v_i32m8_i32mf2(vint32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i32m8_i32mf2(op1);\n+}\n+\n+\n+vint32m1_t test___riscv_vlmul_trunc_v_i32m8_i32m1(vint32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i32m8_i32m1(op1);\n+}\n+\n+\n+vint32m2_t test___riscv_vlmul_trunc_v_i32m8_i32m2(vint32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i32m8_i32m2(op1);\n+}\n+\n+\n+vint32m4_t test___riscv_vlmul_trunc_v_i32m8_i32m4(vint32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i32m8_i32m4(op1);\n+}\n+\n+\n+vint64m1_t test___riscv_vlmul_trunc_v_i64m2_i64m1(vint64m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i64m2_i64m1(op1);\n+}\n+\n+\n+vint64m1_t test___riscv_vlmul_trunc_v_i64m4_i64m1(vint64m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i64m4_i64m1(op1);\n+}\n+\n+\n+vint64m2_t test___riscv_vlmul_trunc_v_i64m4_i64m2(vint64m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i64m4_i64m2(op1);\n+}\n+\n+\n+vint64m1_t test___riscv_vlmul_trunc_v_i64m8_i64m1(vint64m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i64m8_i64m1(op1);\n+}\n+\n+\n+vint64m2_t test___riscv_vlmul_trunc_v_i64m8_i64m2(vint64m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i64m8_i64m2(op1);\n+}\n+\n+\n+vint64m4_t test___riscv_vlmul_trunc_v_i64m8_i64m4(vint64m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_i64m8_i64m4(op1);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vlmul_trunc_v_u8mf4_u8mf8(vuint8mf4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8mf4_u8mf8(op1);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vlmul_trunc_v_u8mf2_u8mf8(vuint8mf2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8mf2_u8mf8(op1);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vlmul_trunc_v_u8mf2_u8mf4(vuint8mf2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8mf2_u8mf4(op1);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vlmul_trunc_v_u8m1_u8mf8(vuint8m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m1_u8mf8(op1);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vlmul_trunc_v_u8m1_u8mf4(vuint8m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m1_u8mf4(op1);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vlmul_trunc_v_u8m1_u8mf2(vuint8m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m1_u8mf2(op1);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vlmul_trunc_v_u8m2_u8mf8(vuint8m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m2_u8mf8(op1);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vlmul_trunc_v_u8m2_u8mf4(vuint8m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m2_u8mf4(op1);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vlmul_trunc_v_u8m2_u8mf2(vuint8m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m2_u8mf2(op1);\n+}\n+\n+\n+vuint8m1_t test___riscv_vlmul_trunc_v_u8m2_u8m1(vuint8m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m2_u8m1(op1);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vlmul_trunc_v_u8m4_u8mf8(vuint8m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m4_u8mf8(op1);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vlmul_trunc_v_u8m4_u8mf4(vuint8m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m4_u8mf4(op1);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vlmul_trunc_v_u8m4_u8mf2(vuint8m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m4_u8mf2(op1);\n+}\n+\n+\n+vuint8m1_t test___riscv_vlmul_trunc_v_u8m4_u8m1(vuint8m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m4_u8m1(op1);\n+}\n+\n+\n+vuint8m2_t test___riscv_vlmul_trunc_v_u8m4_u8m2(vuint8m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m4_u8m2(op1);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vlmul_trunc_v_u8m8_u8mf8(vuint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m8_u8mf8(op1);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vlmul_trunc_v_u8m8_u8mf4(vuint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m8_u8mf4(op1);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vlmul_trunc_v_u8m8_u8mf2(vuint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m8_u8mf2(op1);\n+}\n+\n+\n+vuint8m1_t test___riscv_vlmul_trunc_v_u8m8_u8m1(vuint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m8_u8m1(op1);\n+}\n+\n+\n+vuint8m2_t test___riscv_vlmul_trunc_v_u8m8_u8m2(vuint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m8_u8m2(op1);\n+}\n+\n+\n+vuint8m4_t test___riscv_vlmul_trunc_v_u8m8_u8m4(vuint8m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u8m8_u8m4(op1);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vlmul_trunc_v_u16mf2_u16mf4(vuint16mf2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16mf2_u16mf4(op1);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vlmul_trunc_v_u16m1_u16mf4(vuint16m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m1_u16mf4(op1);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vlmul_trunc_v_u16m1_u16mf2(vuint16m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m1_u16mf2(op1);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vlmul_trunc_v_u16m2_u16mf4(vuint16m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m2_u16mf4(op1);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vlmul_trunc_v_u16m2_u16mf2(vuint16m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m2_u16mf2(op1);\n+}\n+\n+\n+vuint16m1_t test___riscv_vlmul_trunc_v_u16m2_u16m1(vuint16m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m2_u16m1(op1);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vlmul_trunc_v_u16m4_u16mf4(vuint16m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m4_u16mf4(op1);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vlmul_trunc_v_u16m4_u16mf2(vuint16m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m4_u16mf2(op1);\n+}\n+\n+\n+vuint16m1_t test___riscv_vlmul_trunc_v_u16m4_u16m1(vuint16m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m4_u16m1(op1);\n+}\n+\n+\n+vuint16m2_t test___riscv_vlmul_trunc_v_u16m4_u16m2(vuint16m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m4_u16m2(op1);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vlmul_trunc_v_u16m8_u16mf4(vuint16m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m8_u16mf4(op1);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vlmul_trunc_v_u16m8_u16mf2(vuint16m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m8_u16mf2(op1);\n+}\n+\n+\n+vuint16m1_t test___riscv_vlmul_trunc_v_u16m8_u16m1(vuint16m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m8_u16m1(op1);\n+}\n+\n+\n+vuint16m2_t test___riscv_vlmul_trunc_v_u16m8_u16m2(vuint16m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m8_u16m2(op1);\n+}\n+\n+\n+vuint16m4_t test___riscv_vlmul_trunc_v_u16m8_u16m4(vuint16m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u16m8_u16m4(op1);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vlmul_trunc_v_u32m1_u32mf2(vuint32m1_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u32m1_u32mf2(op1);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vlmul_trunc_v_u32m2_u32mf2(vuint32m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u32m2_u32mf2(op1);\n+}\n+\n+\n+vuint32m1_t test___riscv_vlmul_trunc_v_u32m2_u32m1(vuint32m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u32m2_u32m1(op1);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vlmul_trunc_v_u32m4_u32mf2(vuint32m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u32m4_u32mf2(op1);\n+}\n+\n+\n+vuint32m1_t test___riscv_vlmul_trunc_v_u32m4_u32m1(vuint32m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u32m4_u32m1(op1);\n+}\n+\n+\n+vuint32m2_t test___riscv_vlmul_trunc_v_u32m4_u32m2(vuint32m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u32m4_u32m2(op1);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vlmul_trunc_v_u32m8_u32mf2(vuint32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u32m8_u32mf2(op1);\n+}\n+\n+\n+vuint32m1_t test___riscv_vlmul_trunc_v_u32m8_u32m1(vuint32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u32m8_u32m1(op1);\n+}\n+\n+\n+vuint32m2_t test___riscv_vlmul_trunc_v_u32m8_u32m2(vuint32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u32m8_u32m2(op1);\n+}\n+\n+\n+vuint32m4_t test___riscv_vlmul_trunc_v_u32m8_u32m4(vuint32m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u32m8_u32m4(op1);\n+}\n+\n+\n+vuint64m1_t test___riscv_vlmul_trunc_v_u64m2_u64m1(vuint64m2_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u64m2_u64m1(op1);\n+}\n+\n+\n+vuint64m1_t test___riscv_vlmul_trunc_v_u64m4_u64m1(vuint64m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u64m4_u64m1(op1);\n+}\n+\n+\n+vuint64m2_t test___riscv_vlmul_trunc_v_u64m4_u64m2(vuint64m4_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u64m4_u64m2(op1);\n+}\n+\n+\n+vuint64m1_t test___riscv_vlmul_trunc_v_u64m8_u64m1(vuint64m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u64m8_u64m1(op1);\n+}\n+\n+\n+vuint64m2_t test___riscv_vlmul_trunc_v_u64m8_u64m2(vuint64m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u64m8_u64m2(op1);\n+}\n+\n+\n+vuint64m4_t test___riscv_vlmul_trunc_v_u64m8_u64m4(vuint64m8_t op1)\n+{\n+    return __riscv_vlmul_trunc_v_u64m8_u64m4(op1);\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+\n+\n+"}]}