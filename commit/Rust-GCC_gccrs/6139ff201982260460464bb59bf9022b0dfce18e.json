{"sha": "6139ff201982260460464bb59bf9022b0dfce18e", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NjEzOWZmMjAxOTgyMjYwNDYwNDY0YmI1OWJmOTAyMmIwZGZjZTE4ZQ==", "commit": {"author": {"name": "Richard Kenner", "email": "kenner@gcc.gnu.org", "date": "1993-08-03T21:31:46Z"}, "committer": {"name": "Richard Kenner", "email": "kenner@gcc.gnu.org", "date": "1993-08-03T21:31:46Z"}, "message": "(force_to_mode): Now pass actual AND mask instead of number off;\n\ngeneralize appropriately.\nMove most cases from simplify_and_const_int in.\n(simplify_and_const_int): Remove most code from here; call force_to_mode\ninstead.\n(subst, make_extraction, make_compound_operation): Change calls to\nforce_to_mode.\n(make_field_assignment): Likewise.\n(simplify_comparison): Add call to force_to_mode when doing a sign bit\ncomparison.\n\nFrom-SVN: r5060", "tree": {"sha": "012d005a6cfbd6e7667aedd6d83987a2066d30bd", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/012d005a6cfbd6e7667aedd6d83987a2066d30bd"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6139ff201982260460464bb59bf9022b0dfce18e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6139ff201982260460464bb59bf9022b0dfce18e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6139ff201982260460464bb59bf9022b0dfce18e", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6139ff201982260460464bb59bf9022b0dfce18e/comments", "author": null, "committer": null, "parents": [{"sha": "f5393ab90a86c792fe66b438b1962f2352ddba97", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f5393ab90a86c792fe66b438b1962f2352ddba97", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f5393ab90a86c792fe66b438b1962f2352ddba97"}], "stats": {"total": 737, "additions": 345, "deletions": 392}, "files": [{"sha": "1362dbc1b709af4a1b5206c3488fe23c95d77b7e", "filename": "gcc/combine.c", "status": "modified", "additions": 345, "deletions": 392, "changes": 737, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6139ff201982260460464bb59bf9022b0dfce18e/gcc%2Fcombine.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6139ff201982260460464bb59bf9022b0dfce18e/gcc%2Fcombine.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcombine.c?ref=6139ff201982260460464bb59bf9022b0dfce18e", "patch": "@@ -382,7 +382,8 @@ static rtx make_extraction\tPROTO((enum machine_mode, rtx, int, rtx, int,\n \t\t\t\t       int, int, int));\n static rtx make_compound_operation  PROTO((rtx, enum rtx_code));\n static int get_pos_from_mask\tPROTO((unsigned HOST_WIDE_INT, int *));\n-static rtx force_to_mode\tPROTO((rtx, enum machine_mode, int, rtx));\n+static rtx force_to_mode\tPROTO((rtx, enum machine_mode,\n+\t\t\t\t       unsigned HOST_WIDE_INT, rtx));\n static rtx known_cond\t\tPROTO((rtx, enum rtx_code, rtx, rtx));\n static rtx make_field_assignment  PROTO((rtx));\n static rtx apply_distributive_law  PROTO((rtx));\n@@ -3134,7 +3135,7 @@ subst (x, from, to, in_dest, unique_copy)\n \n       if (GET_MODE_SIZE (mode) < GET_MODE_SIZE (GET_MODE (SUBREG_REG (x)))\n \t  && subreg_lowpart_p (x))\n-\treturn force_to_mode (SUBREG_REG (x), mode, GET_MODE_BITSIZE (mode),\n+\treturn force_to_mode (SUBREG_REG (x), mode, GET_MODE_MASK (mode),\n \t\t\t      NULL_RTX);\n       break;\n \n@@ -4624,7 +4625,9 @@ subst (x, from, to, in_dest, unique_copy)\n       else if (GET_CODE (XEXP (x, 1)) != REG)\n \tSUBST (XEXP (x, 1),\n \t       force_to_mode (XEXP (x, 1), GET_MODE (x),\n-\t\t\t      exact_log2 (GET_MODE_BITSIZE (GET_MODE (x))),\n+\t\t\t      ((HOST_WIDE_INT) 1 \n+\t\t\t       << exact_log2 (GET_MODE_BITSIZE (GET_MODE (x))))\n+\t\t\t      - 1,\n \t\t\t      NULL_RTX));\n #endif\n \n@@ -4914,6 +4917,7 @@ make_extraction (mode, inner, pos, pos_rtx, len,\n   int spans_byte = 0;\n   rtx new = 0;\n   rtx orig_pos_rtx = pos_rtx;\n+  int orig_pos;\n \n   /* Get some information about INNER and get the innermost object.  */\n   if (GET_CODE (inner) == USE)\n@@ -5000,7 +5004,11 @@ make_extraction (mode, inner, pos, pos_rtx, len,\n \t\t\t   / UNITS_PER_WORD)\n \t\t\t: 0));\n       else\n-\tnew = force_to_mode (inner, tmode, len, NULL_RTX);\n+\tnew = force_to_mode (inner, tmode,\n+\t\t\t     len >= HOST_BITS_PER_WIDE_INT\n+\t\t\t     ? GET_MODE_MASK (tmode)\n+\t\t\t     : ((HOST_WIDE_INT) 1 << len) - 1,\n+\t\t\t     NULL_RTX);\n \n       /* If this extraction is going into the destination of a SET, \n \t make a STRICT_LOW_PART unless we made a MEM.  */\n@@ -5073,6 +5081,8 @@ make_extraction (mode, inner, pos, pos_rtx, len,\n \t      || MEM_VOLATILE_P (inner))))\n     wanted_mem_mode = extraction_mode;\n \n+  orig_pos = pos;\n+\n #if BITS_BIG_ENDIAN\n   /* If position is constant, compute new position.  Otherwise, build\n      subtraction.  */\n@@ -5139,8 +5149,9 @@ make_extraction (mode, inner, pos, pos_rtx, len,\n   /* If INNER is not memory, we can always get it into the proper mode. */\n   else if (GET_CODE (inner) != MEM)\n     inner = force_to_mode (inner, extraction_mode,\n-\t\t\t   (pos < 0 ? GET_MODE_BITSIZE (extraction_mode)\n-\t\t\t    : len + pos),\n+\t\t\t   pos_rtx || len + orig_pos >= HOST_BITS_PER_WIDE_INT\n+\t\t\t   ? GET_MODE_MASK (extraction_mode)\n+\t\t\t   : (((HOST_WIDE_INT) 1 << len) - 1) << orig_pos,\n \t\t\t   NULL_RTX);\n \n   /* Adjust mode of POS_RTX, if needed.  If we want a wider mode, we\n@@ -5425,7 +5436,7 @@ make_compound_operation (x, in_code)\n \t  && subreg_lowpart_p (x))\n \t{\n \t  rtx newer = force_to_mode (tem, mode,\n-\t\t\t\t     GET_MODE_BITSIZE (mode), NULL_RTX);\n+\t\t\t\t     GET_MODE_MASK (mode), NULL_RTX);\n \n \t  /* If we have something other than a SUBREG, we might have\n \t     done an expansion, so rerun outselves.  */\n@@ -5482,39 +5493,87 @@ get_pos_from_mask (m, plen)\n   return pos;\n }\n \f\n-/* Rewrite X so that it is an expression in MODE.  We only care about the\n-   low-order BITS bits so we can ignore AND operations that just clear\n-   higher-order bits.\n+/* See if X can be simplified knowing that we will only refer to it in\n+   MODE and will only refer to those bits that are nonzero in MASK.\n+   If other bits are being computed or if masking operations are done\n+   that select a superset of the bits in MASK, they can sometimes be\n+   ignored.\n+\n+   Return a possibly simplified expression, but always convert X to\n+   MODE.  If X is a CONST_INT, AND the CONST_INT with MASK.\n \n    Also, if REG is non-zero and X is a register equal in value to REG, \n    replace X with REG.  */\n \n static rtx\n-force_to_mode (x, mode, bits, reg)\n+force_to_mode (x, mode, mask, reg)\n      rtx x;\n      enum machine_mode mode;\n-     int bits;\n+     unsigned HOST_WIDE_INT mask;\n      rtx reg;\n {\n   enum rtx_code code = GET_CODE (x);\n-  enum machine_mode op_mode = mode;\n+  unsigned HOST_WIDE_INT nonzero = nonzero_bits (x, mode);\n+  rtx op0, op1, temp;\n+\n+  /* We want to perform the operation is its present mode unless we know\n+     that the operation is valid in MODE, in which case we do the operation\n+     in MODE.  */\n+  enum machine_mode op_mode\n+    = ((code_to_optab[(int) code] != 0\n+\t&& (code_to_optab[(int) code]->handlers[(int) mode].insn_code\n+\t    != CODE_FOR_nothing))\n+       ? mode : GET_MODE (x));\n+\n+  /* When we have an arithmetic operation, or a shift whose count we\n+     do not know, we need to assume that all bit the up to the highest-order\n+     bit in MASK will be needed.  This is how we form such a mask.  */\n+  unsigned HOST_WIDE_INT fuller_mask\n+    = (GET_MODE_BITSIZE (op_mode) >= HOST_BITS_PER_WIDE_INT\n+       ? GET_MODE_MASK (op_mode)\n+       : ((HOST_WIDE_INT) 1 << (floor_log2 (mask) + 1)) - 1);\n+\n+  /* If none of the bits in X are needed, return a zero.  */\n+  if ((nonzero & mask) == 0)\n+    return const0_rtx;\n \n-  /* If X is narrower than MODE or if BITS is larger than the size of MODE,\n-     just get X in the proper mode.  */\n+  /* If X is a CONST_INT, return a new one.  Do this here since the\n+     test below will fail.  */\n+  if (GET_CODE (x) == CONST_INT)\n+    return GEN_INT (INTVAL (x) & mask);\n \n-  if (GET_MODE_SIZE (GET_MODE (x)) < GET_MODE_SIZE (mode)\n-      || bits > GET_MODE_BITSIZE (mode))\n+  /* If X is narrower than MODE, just get X in the proper mode.  */\n+  if (GET_MODE_SIZE (GET_MODE (x)) < GET_MODE_SIZE (mode))\n     return gen_lowpart_for_combine (mode, x);\n \n+  /* If we aren't changing the mode and all zero bits in MASK are already\n+     known to be zero in X, we need not do anything.  */\n+  if (GET_MODE (x) == mode && (~ mask & nonzero) == 0)\n+    return x;\n+\n   switch (code)\n     {\n+    case CLOBBER:\n+      /* If X is a (clobber (const_int)), return it since we know we are\n+\t generating something that won't match. */\n+      return x;\n+\n+#if ! BITS_BIG_ENDIAN\n+    case USE:\n+      /* X is a (use (mem ..)) that was made from a bit-field extraction that\n+\t spanned the boundary of the MEM.  If we are now masking so it is\n+\t within that boundary, we don't need the USE any more.  */\n+      if ((mask & ~ GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)\n+\treturn force_to_mode (XEXP (x, 0), mode, mask, reg);\n+#endif\n+\n     case SIGN_EXTEND:\n     case ZERO_EXTEND:\n     case ZERO_EXTRACT:\n     case SIGN_EXTRACT:\n       x = expand_compound_operation (x);\n       if (GET_CODE (x) != code)\n-\treturn force_to_mode (x, mode, bits, reg);\n+\treturn force_to_mode (x, mode, mask, reg);\n       break;\n \n     case REG:\n@@ -5523,90 +5582,106 @@ force_to_mode (x, mode, bits, reg)\n \tx = reg;\n       break;\n \n-    case CONST_INT:\n-      if (bits < HOST_BITS_PER_WIDE_INT)\n-\tx = GEN_INT (INTVAL (x) & (((HOST_WIDE_INT) 1 << bits) - 1));\n-      return x;\n-\n     case SUBREG:\n-      /* Ignore low-order SUBREGs. */\n-      if (subreg_lowpart_p (x))\n-\treturn force_to_mode (SUBREG_REG (x), mode, bits, reg);\n+      if (subreg_lowpart_p (x)\n+\t  /* We can ignore the effect this SUBREG if it narrows the mode or,\n+\t     on machines where byte operations extend, if the constant masks\n+\t     to zero all the bits the mode doesn't have.  */\n+\t  && ((GET_MODE_SIZE (GET_MODE (x))\n+\t       < GET_MODE_SIZE (GET_MODE (SUBREG_REG (x))))\n+#ifdef BYTE_LOADS_EXTEND\n+\t      || (0 == (mask\n+\t\t\t& GET_MODE_MASK (GET_MODE (x))\n+\t\t\t& ~ GET_MODE_MASK (GET_MODE (SUBREG_REG (x)))))\n+#endif\n+\t      ))\n+\treturn force_to_mode (SUBREG_REG (x), mode, mask, reg);\n       break;\n \n     case AND:\n-      /* If this is an AND with a constant.  Otherwise, we fall through to\n-\t do the general binary case.  */\n+      /* If this is an AND with a constant, convert it into an AND\n+\t whose constant is the AND of that constant with MASK.  If it\n+\t remains an AND of MASK, delete it since it is redundant.  */\n \n-      if (GET_CODE (XEXP (x, 1)) == CONST_INT)\n+      if (GET_CODE (XEXP (x, 1)) == CONST_INT\n+\t  && GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT)\n \t{\n-\t  HOST_WIDE_INT mask = INTVAL (XEXP (x, 1));\n-\t  int len = exact_log2 (mask + 1);\n-\t  rtx op = XEXP (x, 0);\n-\n-\t  /* If this is masking some low-order bits, we may be able to\n-\t     impose a stricter constraint on what bits of the operand are\n-\t     required.  */\n-\n-\t  op = force_to_mode (op, mode, len > 0 ? MIN (len, bits) : bits,\n-\t\t\t      reg);\n-\n-\t  if (bits < HOST_BITS_PER_WIDE_INT)\n-\t    mask &= ((HOST_WIDE_INT) 1 << bits) - 1;\n-\n-\t  /* If we have no AND in MODE, use the original mode for the\n-\t     operation.  */\n-\n-\t  if (and_optab->handlers[(int) mode].insn_code == CODE_FOR_nothing)\n-\t    op_mode = GET_MODE (x);\n-\n-\t  x = simplify_and_const_int (x, op_mode, op, mask);\n+\t  x = simplify_and_const_int (x, op_mode, XEXP (x, 0),\n+\t\t\t\t      mask & INTVAL (XEXP (x, 1)));\n \n \t  /* If X is still an AND, see if it is an AND with a mask that\n \t     is just some low-order bits.  If so, and it is BITS wide (it\n \t     can't be wider), we don't need it.  */\n \n \t  if (GET_CODE (x) == AND && GET_CODE (XEXP (x, 1)) == CONST_INT\n-\t      && bits < HOST_BITS_PER_WIDE_INT\n-\t      && INTVAL (XEXP (x, 1)) == ((HOST_WIDE_INT) 1 << bits) - 1)\n+\t      && INTVAL (XEXP (x, 1)) == mask)\n \t    x = XEXP (x, 0);\n \n \t  break;\n \t}\n \n-      /* ... fall through ... */\n+      goto binop;\n \n     case PLUS:\n+      /* In (and (plus FOO C1) M), if M is a mask that just turns off\n+\t low-order bits (as in an alignment operation) and FOO is already\n+\t aligned to that boundary, mask C1 to that boundary as well.\n+\t This may eliminate that PLUS and, later, the AND.  */\n+      if (GET_CODE (XEXP (x, 1)) == CONST_INT\n+\t  && exact_log2 (- mask) >= 0\n+\t  && (nonzero_bits (XEXP (x, 0), mode) & ~ mask) == 0\n+\t  && (INTVAL (XEXP (x, 1)) & ~ mask) != 0)\n+\treturn force_to_mode (plus_constant (XEXP (x, 0),\n+\t\t\t\t\t     INTVAL (XEXP (x, 1)) & mask),\n+\t\t\t      mode, mask, reg);\n+\n+      /* ... fall through ... */\n+\n     case MINUS:\n     case MULT:\n+      /* For PLUS, MINUS and MULT, we need any bits less significant than the\n+\t most significant bit in MASK since carries from those bits will\n+\t affect the bits we are interested in.  */\n+      mask = fuller_mask;\n+      goto binop;\n+\n     case IOR:\n     case XOR:\n+      /* If X is (ior (lshiftrt FOO C1) C2), try to commute the IOR and\n+\t LSHIFTRT so we end up with an (and (lshiftrt (ior ...) ...) ...)\n+\t operation which may be a bitfield extraction.  Ensure that the\n+\t constant we form is not wider than the mode of X.  */\n+\n+      if (GET_CODE (XEXP (x, 0)) == LSHIFTRT\n+\t  && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT\n+\t  && INTVAL (XEXP (XEXP (x, 0), 1)) >= 0\n+\t  && INTVAL (XEXP (XEXP (x, 0), 1)) < HOST_BITS_PER_WIDE_INT\n+\t  && GET_CODE (XEXP (x, 1)) == CONST_INT\n+\t  && ((INTVAL (XEXP (XEXP (x, 0), 1))\n+\t       + floor_log2 (INTVAL (XEXP (x, 1))))\n+\t      < GET_MODE_BITSIZE (GET_MODE (x)))\n+\t  && (INTVAL (XEXP (x, 1))\n+\t      & ~ nonzero_bits (XEXP (x, 0), GET_MODE (x)) == 0))\n+\t{\n+\t  temp = GEN_INT ((INTVAL (XEXP (x, 1)) & mask)\n+\t\t\t      << INTVAL (XEXP (XEXP (x, 0), 1)));\n+\t  temp = gen_binary (GET_CODE (x), GET_MODE (x),\n+\t\t\t     XEXP (XEXP (x, 0), 0), temp);\n+\t  x = gen_binary (LSHIFTRT, GET_MODE (x), temp, XEXP (x, 1));\n+\t  return force_to_mode (x, mode, mask, reg);\n+\t}\n+\n+    binop:\n       /* For most binary operations, just propagate into the operation and\n-\t change the mode if we have an operation of that mode.  */\n-\n-      if ((code == PLUS\n-\t   && add_optab->handlers[(int) mode].insn_code == CODE_FOR_nothing)\n-\t  || (code == MINUS\n-\t      && sub_optab->handlers[(int) mode].insn_code == CODE_FOR_nothing)\n-\t  || (code == MULT && (smul_optab->handlers[(int) mode].insn_code\n-\t\t\t       == CODE_FOR_nothing))\n-\t  || (code == AND\n-\t      && and_optab->handlers[(int) mode].insn_code == CODE_FOR_nothing)\n-\t  || (code == IOR\n-\t      && ior_optab->handlers[(int) mode].insn_code == CODE_FOR_nothing)\n-\t  || (code == XOR && (xor_optab->handlers[(int) mode].insn_code\n-\t\t\t      == CODE_FOR_nothing)))\n-\top_mode = GET_MODE (x);\n-\n-      x = gen_binary (code, op_mode,\n-\t\t      gen_lowpart_for_combine (op_mode,\n-\t\t\t\t\t       force_to_mode (XEXP (x, 0),\n-\t\t\t\t\t\t\t      mode, bits,\n-\t\t\t\t\t\t\t      reg)),\n-\t\t      gen_lowpart_for_combine (op_mode,\n-\t\t\t\t\t       force_to_mode (XEXP (x, 1),\n-\t\t\t\t\t\t\t      mode, bits,\n-\t\t\t\t\t\t\t      reg)));\n+\t change the mode if we have an operation of that mode.   */\n+\n+      op0 = gen_lowpart_for_combine (op_mode, force_to_mode (XEXP (x, 0),\n+\t\t\t\t\t\t\t     mode, mask, reg));\n+      op1 = gen_lowpart_for_combine (op_mode, force_to_mode (XEXP (x, 1),\n+\t\t\t\t\t\t\t     mode, mask, reg));\n+\n+      if (op_mode != GET_MODE (x) || op0 != XEXP (x, 0) || op1 != XEXP (x, 1))\n+\tx = gen_binary (code, op_mode, op0, op1);\n       break;\n \n     case ASHIFT:\n@@ -5615,81 +5690,191 @@ force_to_mode (x, mode, bits, reg)\n \t However, we cannot do anything with shifts where we cannot\n \t guarantee that the counts are smaller than the size of the mode\n \t because such a count will have a different meaning in a\n-\t wider mode.\n-\n-\t If we can narrow the shift and know the count, we need even fewer\n-\t bits of the first operand.  */\n+\t wider mode.  */\n \n       if (! (GET_CODE (XEXP (x, 1)) == CONST_INT\n+\t     && INTVAL (XEXP (x, 1)) >= 0\n \t     && INTVAL (XEXP (x, 1)) < GET_MODE_BITSIZE (mode))\n \t  && ! (GET_MODE (XEXP (x, 1)) != VOIDmode\n \t\t&& (nonzero_bits (XEXP (x, 1), GET_MODE (XEXP (x, 1)))\n \t\t    < (unsigned HOST_WIDE_INT) GET_MODE_BITSIZE (mode))))\n \tbreak;\n \t\n-      if (GET_CODE (XEXP (x, 1)) == CONST_INT && INTVAL (XEXP (x, 1)) < bits)\n-\tbits -= INTVAL (XEXP (x, 1));\n-\n-      if ((code == ASHIFT\n-\t   && ashl_optab->handlers[(int) mode].insn_code == CODE_FOR_nothing)\n-\t  || (code == LSHIFT && (lshl_optab->handlers[(int) mode].insn_code\n-\t\t\t\t == CODE_FOR_nothing)))\n-\top_mode = GET_MODE (x);\n-\n-      x =  gen_binary (code, op_mode,\n-\t\t       gen_lowpart_for_combine (op_mode,\n-\t\t\t\t\t\tforce_to_mode (XEXP (x, 0),\n-\t\t\t\t\t\t\t       mode, bits,\n-\t\t\t\t\t\t\t       reg)),\n-\t\t       XEXP (x, 1));\n+      /* If the shift count is a constant and we can do arithmetic in\n+\t the mode of the shift, refine which bits we need.  Otherwise, use the\n+\t conservative form of the mask.  */\n+      if (GET_CODE (XEXP (x, 1)) == CONST_INT\n+\t  && INTVAL (XEXP (x, 1)) >= 0\n+\t  && INTVAL (XEXP (x, 1)) < GET_MODE_BITSIZE (op_mode)\n+\t  && GET_MODE_BITSIZE (op_mode) <= HOST_BITS_PER_WIDE_INT)\n+\tmask >>= INTVAL (XEXP (x, 1));\n+      else\n+\tmask = fuller_mask;\n+\n+      op0 = gen_lowpart_for_combine (op_mode,\n+\t\t\t\t     force_to_mode (XEXP (x, 0), op_mode,\n+\t\t\t\t\t\t    mask, reg));\n+\n+      if (op_mode != GET_MODE (x) || op0 != XEXP (x, 0))\n+\tx =  gen_binary (code, op_mode, op0, XEXP (x, 1));\n       break;\n \n     case LSHIFTRT:\n       /* Here we can only do something if the shift count is a constant and\n-\t the count plus BITS is no larger than the width of MODE.  In that\n-\t case, we can do the shift in MODE.  */\n+\t we can do arithmetic in OP_MODE.  */\n \n       if (GET_CODE (XEXP (x, 1)) == CONST_INT\n-\t  && INTVAL (XEXP (x, 1)) + bits <= GET_MODE_BITSIZE (mode))\n+\t  && GET_MODE_BITSIZE (op_mode) <= HOST_BITS_PER_WIDE_INT)\n \t{\n-\t  rtx inner = force_to_mode (XEXP (x, 0), mode,\n-\t\t\t\t     bits + INTVAL (XEXP (x, 1)), reg);\n+\t  rtx inner = XEXP (x, 0);\n+\n+\t  /* Select the mask of the bits we need for the shift operand.  */\n+\t  mask <<= INTVAL (XEXP (x, 1));\n \n-\t  if (lshr_optab->handlers[(int) mode].insn_code == CODE_FOR_nothing)\n+\t  /* We can only change the mode of the shift if we can do arithmetic\n+\t     in the mode of the shift and MASK is no wider than the width of\n+\t     OP_MODE.  */\n+\t  if (GET_MODE_BITSIZE (op_mode) > HOST_BITS_PER_WIDE_INT\n+\t      || (mask & ~ GET_MODE_MASK (op_mode)) != 0)\n \t    op_mode = GET_MODE (x);\n \n-\t  x = gen_binary (LSHIFTRT, op_mode,\n-\t\t\t  gen_lowpart_for_combine (op_mode, inner),\n-\t\t\t  XEXP (x, 1));\n+\t  inner = force_to_mode (inner, op_mode, mask, reg);\n+\n+\t  if (GET_MODE (x) != op_mode || inner != XEXP (x, 0))\n+\t    x = gen_binary (LSHIFTRT, op_mode, inner, XEXP (x, 1));\n \t}\n+\n+      /* If we have (and (lshiftrt FOO C1) C2) where the combination of the\n+\t shift and AND produces only copies of the sign bit (C2 is one less\n+\t than a power of two), we can do this with just a shift.  */\n+\n+      if (GET_CODE (x) == LSHIFTRT\n+\t  && GET_CODE (XEXP (x, 1)) == CONST_INT\n+\t  && ((INTVAL (XEXP (x, 1))\n+\t       + num_sign_bit_copies (XEXP (x, 0), GET_MODE (XEXP (x, 0))))\n+\t      >= GET_MODE_BITSIZE (GET_MODE (x)))\n+\t  && exact_log2 (mask + 1) >= 0\n+\t  && (num_sign_bit_copies (XEXP (x, 0), GET_MODE (XEXP (x, 0)))\n+\t      >= exact_log2 (mask + 1)))\n+\tx = gen_binary (LSHIFTRT, GET_MODE (x), XEXP (x, 0),\n+\t\t\tGEN_INT (GET_MODE_BITSIZE (GET_MODE (x))\n+\t\t\t\t - exact_log2 (mask + 1)));\n       break;\n \n     case ASHIFTRT:\n+      /* If we are just looking for the sign bit, we don't need this shift at\n+\t all, even if it has a variable count.  */\n+      if (mask == ((HOST_WIDE_INT) 1\n+\t\t   << (GET_MODE_BITSIZE (GET_MODE (x)) - 1)))\n+\treturn force_to_mode (XEXP (x, 0), mode, mask, reg);\n+\n+      /* If this is a shift by a constant, get a mask that contains those bits\n+\t that are not copies of the sign bit.  We then have two cases:  If\n+\t MASK only includes those bits, this can be a logical shift, which may\n+\t allow simplifications.  If MASK is a single-bit field not within\n+\t those bits, we are requesting a copy of the sign bit and hence can\n+\t shift the sign bit to the appropriate location.  */\n+\n+      if (GET_CODE (XEXP (x, 1)) == CONST_INT && INTVAL (XEXP (x, 1)) >= 0\n+\t  && INTVAL (XEXP (x, 1)) < HOST_BITS_PER_WIDE_INT)\n+\t{\n+\t  int i = -1;\n+\n+\t  nonzero = GET_MODE_MASK (GET_MODE (x));\n+\t  nonzero >>= INTVAL (XEXP (x, 1));\n+\n+\t  if ((mask & ~ nonzero) == 0\n+\t      || (i = exact_log2 (mask)) >= 0)\n+\t    {\n+\t      x = simplify_shift_const\n+\t\t(x, LSHIFTRT, GET_MODE (x), XEXP (x, 0),\n+\t\t i < 0 ? INTVAL (XEXP (x, 1))\n+\t\t : GET_MODE_BITSIZE (GET_MODE (x)) - 1 - i);\n+\n+\t      if (GET_CODE (x) != ASHIFTRT)\n+\t\treturn force_to_mode (x, mode, mask, reg);\n+\t    }\n+\t}\n+\n+      /* If MASK is 1, convert this to a LSHIFTRT.  This can be done\n+\t even if the shift count isn't a constant.  */\n+      if (mask == 1)\n+\tx = gen_binary (LSHIFTRT, GET_MODE (x), XEXP (x, 0), XEXP (x, 1));\n+\n       /* If this is a sign-extension operation that just affects bits\n \t we don't care about, remove it.  */\n \n       if (GET_CODE (XEXP (x, 1)) == CONST_INT\n \t  && INTVAL (XEXP (x, 1)) >= 0\n-\t  && INTVAL (XEXP (x, 1)) <= GET_MODE_BITSIZE (GET_MODE (x)) - bits\n+\t  && (INTVAL (XEXP (x, 1))\n+\t      <= GET_MODE_BITSIZE (GET_MODE (x)) - (floor_log2 (mask) + 1))\n \t  && GET_CODE (XEXP (x, 0)) == ASHIFT\n \t  && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT\n \t  && INTVAL (XEXP (XEXP (x, 0), 1)) == INTVAL (XEXP (x, 1)))\n-\treturn force_to_mode (XEXP (XEXP (x, 0), 0), mode, bits, reg);\n+\treturn force_to_mode (XEXP (XEXP (x, 0), 0), mode, mask, reg);\n+\n       break;\n \n+    case ROTATE:\n+    case ROTATERT:\n+      /* If the shift count is constant and we can do computations\n+\t in the mode of X, compute where the bits we care about are.\n+\t Otherwise, we can't do anything.  Don't change the mode of\n+\t the shift or propagate MODE into the shift, though.  */\n+      if (GET_CODE (XEXP (x, 1)) == CONST_INT\n+\t  && INTVAL (XEXP (x, 1)) >= 0)\n+\t{\n+\t  temp = simplify_binary_operation (code == ROTATE ? ROTATERT : ROTATE,\n+\t\t\t\t\t    GET_MODE (x), GEN_INT (mask),\n+\t\t\t\t\t    XEXP (x, 1));\n+\t  if (temp)\n+\t    SUBST (XEXP (x, 0),\n+\t\t   force_to_mode (XEXP (x, 0), GET_MODE (x),\n+\t\t\t\t  INTVAL (temp), reg));\n+\t}\n+      break;\n+\t\n     case NEG:\n+      /* We need any bits less significant than the most significant bit in\n+\t MASK since carries from those bits will affect the bits we are\n+\t interested in.  */\n+      mask = fuller_mask;\n+      goto unop;\n+\n     case NOT:\n-      if ((code == NEG\n-\t   && neg_optab->handlers[(int) mode].insn_code == CODE_FOR_nothing)\n-\t  || (code == NOT && (one_cmpl_optab->handlers[(int) mode].insn_code\n-\t\t\t      == CODE_FOR_nothing)))\n-\top_mode = GET_MODE (x);\n-\n-      /* Handle these similarly to the way we handle most binary operations. */\n-      x = gen_unary (code, op_mode,\n-\t\t     gen_lowpart_for_combine (op_mode,\n-\t\t\t\t\t      force_to_mode (XEXP (x, 0), mode,\n-\t\t\t\t\t\t\t     bits, reg)));\n+      /* (not FOO) is (xor FOO CONST), so if FOO is an LSHIFTRT, we can do the\n+\t same as the XOR case above.  Ensure that the constant we form is not\n+\t wider than the mode of X.  */\n+\n+      if (GET_CODE (XEXP (x, 0)) == LSHIFTRT\n+\t  && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT\n+\t  && INTVAL (XEXP (XEXP (x, 0), 1)) >= 0\n+\t  && (INTVAL (XEXP (XEXP (x, 0), 1)) + floor_log2 (mask)\n+\t      < GET_MODE_BITSIZE (GET_MODE (x)))\n+\t  && INTVAL (XEXP (XEXP (x, 0), 1)) < HOST_BITS_PER_WIDE_INT)\n+\t{\n+\t  temp = GEN_INT (mask << INTVAL (XEXP (XEXP (x, 0), 1)));\n+\t  temp = gen_binary (XOR, GET_MODE (x), XEXP (XEXP (x, 0), 0), temp);\n+\t  x = gen_binary (LSHIFTRT, GET_MODE (x), temp, XEXP (XEXP (x, 0), 1));\n+\n+\t  return force_to_mode (x, mode, mask, reg);\n+\t}\n+\n+    unop:\n+      op0 = gen_lowpart_for_combine (op_mode, force_to_mode (XEXP (x, 0), mode,\n+\t\t\t\t\t\t\t     mask, reg));\n+      if (op_mode != GET_MODE (x) || op0 != XEXP (x, 0))\n+\tx = gen_unary (code, op_mode, op0);\n+      break;\n+\n+    case NE:\n+      /* (and (ne FOO 0) CONST) can be (and FOO CONST) if CONST is included\n+\t in STORE_FLAG_VALUE and FOO has no bits that might be nonzero not\n+\t in CONST.  */\n+      if ((mask & ~ STORE_FLAG_VALUE) == 0 && XEXP (x, 0) == const0_rtx\n+\t  && (nonzero_bits (XEXP (x, 0), mode) & ~ mask) == 0)\n+\treturn force_to_mode (XEXP (x, 0), mode, mask, reg);\n+\n       break;\n \n     case IF_THEN_ELSE:\n@@ -5699,11 +5884,11 @@ force_to_mode (x, mode, bits, reg)\n       SUBST (XEXP (x, 1),\n \t     gen_lowpart_for_combine (GET_MODE (x),\n \t\t\t\t      force_to_mode (XEXP (x, 1), mode,\n-\t\t\t\t\t\t     bits, reg)));\n+\t\t\t\t\t\t     mask, reg)));\n       SUBST (XEXP (x, 2),\n \t     gen_lowpart_for_combine (GET_MODE (x),\n \t\t\t\t      force_to_mode (XEXP (x, 2), mode,\n-\t\t\t\t\t\t     bits, reg)));\n+\t\t\t\t\t\t     mask, reg)));\n       break;\n     }\n \n@@ -5907,7 +6092,11 @@ make_field_assignment (x)\n \n   src = force_to_mode (simplify_shift_const (NULL_RTX, LSHIFTRT,\n \t\t\t\t\t     GET_MODE (src), other, pos),\n-\t\t       mode, len, dest);\n+\t\t       mode,\n+\t\t       GET_MODE_BITSIZE (mode) >= HOST_BITS_PER_WIDE_INT\n+\t\t       ? GET_MODE_MASK (mode)\n+\t\t       : ((HOST_WIDE_INT) 1 << len) - 1,\n+\t\t       dest);\n \n   return gen_rtx_combine (SET, VOIDmode, assign, src);\n }\n@@ -6053,276 +6242,14 @@ simplify_and_const_int (x, mode, varop, constop)\n   register rtx temp;\n   unsigned HOST_WIDE_INT nonzero;\n \n-  /* There is a large class of optimizations based on the principle that\n-     some operations produce results where certain bits are known to be zero,\n-     and hence are not significant to the AND.  For example, if we have just\n-     done a left shift of one bit, the low-order bit is known to be zero and\n-     hence an AND with a mask of ~1 would not do anything.\n-\n-     At the end of the following loop, we set:\n-\n-     VAROP to be the item to be AND'ed with;\n-     CONSTOP to the constant value to AND it with.  */\n+  /* Simplify VAROP knowing that we will be only looking at some of the\n+     bits in it.  */\n+  varop = force_to_mode (varop, mode, constop, NULL_RTX);\n \n-  while (1)\n-    {\n-      /* If we ever encounter a mode wider than the host machine's widest\n-\t integer size, we can't compute the masks accurately, so give up.  */\n-      if (GET_MODE_BITSIZE (GET_MODE (varop)) > HOST_BITS_PER_WIDE_INT)\n-\tbreak;\n-\n-      /* Unless one of the cases below does a `continue',\n-\t a `break' will be executed to exit the loop.  */\n-\n-      switch (GET_CODE (varop))\n-\t{\n-\tcase CLOBBER:\n-\t  /* If VAROP is a (clobber (const_int)), return it since we know\n-\t     we are generating something that won't match. */\n-\t  return varop;\n-\n-#if ! BITS_BIG_ENDIAN\n-\tcase USE:\n-\t  /* VAROP is a (use (mem ..)) that was made from a bit-field\n-\t     extraction that spanned the boundary of the MEM.  If we are\n-\t     now masking so it is within that boundary, we don't need the\n-\t     USE any more.  */\n-\t  if ((constop & ~ GET_MODE_MASK (GET_MODE (XEXP (varop, 0)))) == 0)\n-\t    {\n-\t      varop = XEXP (varop, 0);\n-\t      continue;\n-\t    }\n-\t  break;\n-#endif\n-\n-\tcase SUBREG:\n-\t  if (subreg_lowpart_p (varop)\n-\t      /* We can ignore the effect this SUBREG if it narrows the mode\n-\t\t or, on machines where byte operations extend, if the\n-\t\t constant masks to zero all the bits the mode doesn't have.  */\n-\t      && ((GET_MODE_SIZE (GET_MODE (varop))\n-\t\t   < GET_MODE_SIZE (GET_MODE (SUBREG_REG (varop))))\n-#ifdef BYTE_LOADS_EXTEND\n-\t\t  || (0 == (constop\n-\t\t\t    & GET_MODE_MASK (GET_MODE (varop))\n-\t\t\t    & ~ GET_MODE_MASK (GET_MODE (SUBREG_REG (varop)))))\n-#endif\n-\t\t  ))\n-\t    {\n-\t      varop = SUBREG_REG (varop);\n-\t      continue;\n-\t    }\n-\t  break;\n-\n-\tcase ZERO_EXTRACT:\n-\tcase SIGN_EXTRACT:\n-\tcase ZERO_EXTEND:\n-\tcase SIGN_EXTEND:\n-\t  /* Try to expand these into a series of shifts and then work\n-\t     with that result.  If we can't, for example, if the extract\n-\t     isn't at a fixed position, give up.  */\n-\t  temp = expand_compound_operation (varop);\n-\t  if (temp != varop)\n-\t    {\n-\t      varop = temp;\n-\t      continue;\n-\t    }\n-\t  break;\n-\n-\tcase AND:\n-\t  if (GET_CODE (XEXP (varop, 1)) == CONST_INT)\n-\t    {\n-\t      constop &= INTVAL (XEXP (varop, 1));\n-\t      varop = XEXP (varop, 0);\n-\t      continue;\n-\t    }\n-\t  break;\n-\n-\tcase IOR:\n-\tcase XOR:\n-\t  /* If VAROP is (ior (lshiftrt FOO C1) C2), try to commute the IOR and\n-\t     LSHIFT so we end up with an (and (lshiftrt (ior ...) ...) ...)\n-\t     operation which may be a bitfield extraction.  Ensure\n-\t     that the constant we form is not wider than the mode of\n-\t     VAROP.  */\n-\n-\t  if (GET_CODE (XEXP (varop, 0)) == LSHIFTRT\n-\t      && GET_CODE (XEXP (XEXP (varop, 0), 1)) == CONST_INT\n-\t      && INTVAL (XEXP (XEXP (varop, 0), 1)) >= 0\n-\t      && INTVAL (XEXP (XEXP (varop, 0), 1)) < HOST_BITS_PER_WIDE_INT\n-\t      && GET_CODE (XEXP (varop, 1)) == CONST_INT\n-\t      && ((INTVAL (XEXP (XEXP (varop, 0), 1))\n-\t\t  + floor_log2 (INTVAL (XEXP (varop, 1))))\n-\t\t  < GET_MODE_BITSIZE (GET_MODE (varop)))\n-\t      && (INTVAL (XEXP (varop, 1))\n-\t\t  & ~ nonzero_bits (XEXP (varop, 0), GET_MODE (varop)) == 0))\n-\t    {\n-\t      temp = GEN_INT ((INTVAL (XEXP (varop, 1)) & constop)\n-\t\t\t      << INTVAL (XEXP (XEXP (varop, 0), 1)));\n-\t      temp = gen_binary (GET_CODE (varop), GET_MODE (varop),\n-\t\t\t\t XEXP (XEXP (varop, 0), 0), temp);\n-\t      varop = gen_rtx_combine (LSHIFTRT, GET_MODE (varop),\n-\t\t\t\t       temp, XEXP (varop, 1));\n-\t      continue;\n-\t    }\n-\n-\t  /* Apply the AND to both branches of the IOR or XOR, then try to\n-\t     apply the distributive law.  This may eliminate operations \n-\t     if either branch can be simplified because of the AND.\n-\t     It may also make some cases more complex, but those cases\n-\t     probably won't match a pattern either with or without this.  */\n-\t  return \n-\t    gen_lowpart_for_combine\n-\t      (mode, apply_distributive_law\n-\t       (gen_rtx_combine\n-\t\t(GET_CODE (varop), GET_MODE (varop),\n-\t\t simplify_and_const_int (NULL_RTX, GET_MODE (varop),\n-\t\t\t\t\t XEXP (varop, 0), constop),\n-\t\t simplify_and_const_int (NULL_RTX, GET_MODE (varop),\n-\t\t\t\t\t XEXP (varop, 1), constop))));\n-\n-\tcase NOT:\n-\t  /* (and (not FOO)) is (and (xor FOO CONST)), so if FOO is an\n-\t     LSHIFTRT, we can do the same as above.  Ensure that the constant\n-\t     we form is not wider than the mode of VAROP.  */\n-\n-\t  if (GET_CODE (XEXP (varop, 0)) == LSHIFTRT\n-\t      && GET_CODE (XEXP (XEXP (varop, 0), 1)) == CONST_INT\n-\t      && INTVAL (XEXP (XEXP (varop, 0), 1)) >= 0\n-\t      && (INTVAL (XEXP (XEXP (varop, 0), 1)) + floor_log2 (constop)\n-\t\t  < GET_MODE_BITSIZE (GET_MODE (varop)))\n-\t      && INTVAL (XEXP (XEXP (varop, 0), 1)) < HOST_BITS_PER_WIDE_INT)\n-\t    {\n-\t      temp = GEN_INT (constop << INTVAL (XEXP (XEXP (varop, 0), 1)));\n-\t      temp = gen_binary (XOR, GET_MODE (varop),\n-\t\t\t\t XEXP (XEXP (varop, 0), 0), temp);\n-\t      varop = gen_rtx_combine (LSHIFTRT, GET_MODE (varop),\n-\t\t\t\t       temp, XEXP (XEXP (varop, 0), 1));\n-\t      continue;\n-\t    }\n-\t  break;\n-\n-\tcase ASHIFTRT:\n-\t  /* If we are just looking for the sign bit, we don't need this\n-\t     shift at all, even if it has a variable count.  */\n-\t  if (constop == ((HOST_WIDE_INT) 1\n-\t\t\t  << (GET_MODE_BITSIZE (GET_MODE (varop)) - 1)))\n-\t    {\n-\t      varop = XEXP (varop, 0);\n-\t      continue;\n-\t    }\n-\n-\t  /* If this is a shift by a constant, get a mask that contains\n-\t     those bits that are not copies of the sign bit.  We then have\n-\t     two cases:  If CONSTOP only includes those bits, this can be\n-\t     a logical shift, which may allow simplifications.  If CONSTOP\n-\t     is a single-bit field not within those bits, we are requesting\n-\t     a copy of the sign bit and hence can shift the sign bit to\n-\t     the appropriate location.  */\n-\t  if (GET_CODE (XEXP (varop, 1)) == CONST_INT\n-\t      && INTVAL (XEXP (varop, 1)) >= 0\n-\t      && INTVAL (XEXP (varop, 1)) < HOST_BITS_PER_WIDE_INT)\n-\t    {\n-\t      int i = -1;\n-\n-\t      nonzero = GET_MODE_MASK (GET_MODE (varop));\n-\t      nonzero >>= INTVAL (XEXP (varop, 1));\n-\n-\t      if ((constop & ~ nonzero) == 0\n-\t\t  || (i = exact_log2 (constop)) >= 0)\n-\t\t{\n-\t\t  varop = simplify_shift_const\n-\t\t    (varop, LSHIFTRT, GET_MODE (varop), XEXP (varop, 0),\n-\t\t     i < 0 ? INTVAL (XEXP (varop, 1))\n-\t\t     : GET_MODE_BITSIZE (GET_MODE (varop)) - 1 - i);\n-\t\t  if (GET_CODE (varop) != ASHIFTRT)\n-\t\t    continue;\n-\t\t}\n-\t    }\n-\n-\t  /* If our mask is 1, convert this to a LSHIFTRT.  This can be done\n-\t     even if the shift count isn't a constant.  */\n-\t  if (constop == 1)\n-\t    varop = gen_rtx_combine (LSHIFTRT, GET_MODE (varop),\n-\t\t\t\t     XEXP (varop, 0), XEXP (varop, 1));\n-\t  break;\n-\n-\tcase LSHIFTRT:\n-\t  /* If we have (and (lshiftrt FOO C1) C2) where the combination of the\n-\t     shift and AND produces only copies of the sign bit (C2 is one less\n-\t     than a power of two), we can do this with just a shift.  */\n-\n-\t  if (GET_CODE (XEXP (varop, 1)) == CONST_INT\n-\t      && ((INTVAL (XEXP (varop, 1))\n-\t\t   + num_sign_bit_copies (XEXP (varop, 0),\n-\t\t\t\t\t  GET_MODE (XEXP (varop, 0))))\n-\t\t  >= GET_MODE_BITSIZE (GET_MODE (varop)))\n-\t      && exact_log2 (constop + 1) >= 0\n-\t      && (num_sign_bit_copies (XEXP (varop, 0),\n-\t\t\t\t       GET_MODE (XEXP (varop, 0)))\n-\t\t  >= exact_log2 (constop + 1)))\n-\t    varop\n-\t      = gen_rtx_combine (LSHIFTRT, GET_MODE (varop), XEXP (varop, 0),\n-\t\t\t\t GEN_INT (GET_MODE_BITSIZE (GET_MODE (varop))\n-\t\t\t\t\t  - exact_log2 (constop + 1)));\n-\t  break;\n-\n-\tcase NE:\n-\t  /* (and (ne FOO 0) CONST) can be (and FOO CONST) if CONST is\n-\t     included in STORE_FLAG_VALUE and FOO has no bits that might be\n-\t     nonzero not in CONST.  */\n-\t  if ((constop & ~ STORE_FLAG_VALUE) == 0\n-\t      && XEXP (varop, 0) == const0_rtx\n-\t      && (nonzero_bits (XEXP (varop, 0), mode) & ~ constop) == 0)\n-\t    {\n-\t      varop = XEXP (varop, 0);\n-\t      continue;\n-\t    }\n-\t  break;\n-\n-\tcase PLUS:\n-\t  /* In (and (plus FOO C1) M), if M is a mask that just turns off\n-\t     low-order bits (as in an alignment operation) and FOO is already\n-\t     aligned to that boundary, we can convert remove this AND\n-\t     and possibly the PLUS if it is now adding zero.  */\n-\t  if (GET_CODE (XEXP (varop, 1)) == CONST_INT\n-\t      && exact_log2 (-constop) >= 0\n-\t      && (nonzero_bits (XEXP (varop, 0), mode) & ~ constop) == 0)\n-\t    {\n-\t      varop = plus_constant (XEXP (varop, 0),\n-\t\t\t\t     INTVAL (XEXP (varop, 1)) & constop);\n-\t      constop = ~0;\n-\t      break;\n-\t    }\n-\n-\t  /* ... fall through ... */\n-\n-\tcase MINUS:\n-\t  /* In (and (plus (and FOO M1) BAR) M2), if M1 and M2 are one\n-\t     less than powers of two and M2 is narrower than M1, we can\n-\t     eliminate the inner AND.  This occurs when incrementing\n-\t     bit fields.  */\n-\n-\t  if (GET_CODE (XEXP (varop, 0)) == ZERO_EXTRACT\n-\t      || GET_CODE (XEXP (varop, 0)) == ZERO_EXTEND)\n-\t    SUBST (XEXP (varop, 0),\n-\t\t   expand_compound_operation (XEXP (varop, 0)));\n-\n-\t  if (GET_CODE (XEXP (varop, 0)) == AND\n-\t      && GET_CODE (XEXP (XEXP (varop, 0), 1)) == CONST_INT\n-\t      && exact_log2 (constop + 1) >= 0\n-\t      && exact_log2 (INTVAL (XEXP (XEXP (varop, 0), 1)) + 1) >= 0\n-\t      && (~ INTVAL (XEXP (XEXP (varop, 0), 1)) & constop) == 0)\n-\t    SUBST (XEXP (varop, 0), XEXP (XEXP (varop, 0), 0));\n-\t  break;\n-\t}\n-\n-      break;\n-    }\n-\n-  /* If we have reached a constant, this whole thing is constant.  */\n-  if (GET_CODE (varop) == CONST_INT)\n-    return GEN_INT (constop & INTVAL (varop));\n+  /* If VAROP is a CLOBBER, we will fail so return it; if it is a\n+     CONST_INT, we are done.  */\n+  if (GET_CODE (varop) == CLOBBER || GET_CODE (varop) == CONST_INT)\n+    return varop;\n \n   /* See what bits may be nonzero in VAROP.  Unlike the general case of\n      a call to nonzero_bits, here we don't care about bits outside\n@@ -6340,6 +6267,23 @@ simplify_and_const_int (x, mode, varop, constop)\n   if (constop == 0)\n     return const0_rtx;\n \n+  /* If VAROP is an IOR or XOR, apply the AND to both branches of the IOR\n+     or XOR, then try to apply the distributive law.  This may eliminate\n+     operations if either branch can be simplified because of the AND.\n+     It may also make some cases more complex, but those cases probably\n+     won't match a pattern either with or without this.  */\n+\n+  if (GET_CODE (varop) == IOR || GET_CODE (varop) == XOR)\n+    return\n+      gen_lowpart_for_combine\n+\t(mode,\n+\t apply_distributive_law\n+\t (gen_binary (GET_CODE (varop), GET_MODE (varop),\n+\t\t      simplify_and_const_int (NULL_RTX, GET_MODE (varop),\n+\t\t\t\t\t      XEXP (varop, 0), constop),\n+\t\t      simplify_and_const_int (NULL_RTX, GET_MODE (varop),\n+\t\t\t\t\t      XEXP (varop, 1), constop))));\n+\n   /* Get VAROP in MODE.  Try to get a SUBREG if not.  Don't make a new SUBREG\n      if we already had one (just check for the simplest cases).  */\n   if (x && GET_CODE (XEXP (x, 0)) == SUBREG\n@@ -6359,7 +6303,7 @@ simplify_and_const_int (x, mode, varop, constop)\n \n   /* Otherwise, return an AND.  See how much, if any, of X we can use.  */\n   else if (x == 0 || GET_CODE (x) != AND || GET_MODE (x) != mode)\n-    x = gen_rtx_combine (AND, mode, varop, GEN_INT (constop));\n+    x = gen_binary (AND, mode, varop, GEN_INT (constop));\n \n   else\n     {\n@@ -8459,6 +8403,15 @@ simplify_comparison (code, pop0, pop1)\n       unsigned_comparison_p = (code == LTU || code == LEU || code == GTU\n \t\t\t       || code == LEU);\n \n+      /* If this is a sign bit comparison and we can do arithmetic in\n+\t MODE, say that we will only be needing the sign bit of OP0.  */\n+      if (sign_bit_comparison_p\n+\t  && GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT)\n+\top0 = force_to_mode (op0, mode,\n+\t\t\t     ((HOST_WIDE_INT) 1\n+\t\t\t      << (GET_MODE_BITSIZE (mode) - 1)),\n+\t\t\t     NULL_RTX);\n+\n       /* Now try cases based on the opcode of OP0.  If none of the cases\n \t does a \"continue\", we exit this loop immediately after the\n \t switch.  */"}]}