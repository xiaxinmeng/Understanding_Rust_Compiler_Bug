{"sha": "55805e5496208b399bc3faa047fc54a5af3f1a36", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NTU4MDVlNTQ5NjIwOGIzOTliYzNmYWEwNDdmYzU0YTVhZjNmMWEzNg==", "commit": {"author": {"name": "Yuri Rumyantsev", "email": "yuri.s.rumyantsev@intel.com", "date": "2013-05-31T15:52:42Z"}, "committer": {"name": "H.J. Lu", "email": "hjl@gcc.gnu.org", "date": "2013-05-31T15:52:42Z"}, "message": "Silvermont (SLM) architecture performance tuning\n\n2013-05-31  Yuri Rumyantsev  <yuri.s.rumyantsev@intel.com>\n\t    Igor Zamyatin  <igor.zamyatin@intel.com>\n\n\t* config/i386/i386.h (enum ix86_tune_indices): Add\n\tX86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS.\n\t(TARGET_SPLIT_MEM_OPND_FOR_FP_CONVERTS): New define.\n\n\t* config/i386/i386.c (initial_ix86_tune_features)\n\t<X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS>: Initialize.\n\t(ix86_lea_outperforms): Handle Silvermont tuning.\n\t(ix86_avoid_lea_for_add): Add new argument to ix86_lea_outperforms\n\tcall.\n\t(ix86_use_lea_for_mov): Likewise.\n\t(ix86_avoid_lea_for_addr): Likewise.\n\t(ix86_lea_for_add_ok): Likewise.\n\t(exact_dependency_1): New function.\n\t(exact_store_load_dependency): Likewise.\n\t(ix86_adjust_cost): Handle Silvermont tuning.\n\t(do_reoder_for_imul): Likewise.\n\t(swap_top_of_ready_list): New function.\n\t(ix86_sched_reorder): Changed to handle Silvermont tuning.\n\n\t* config/i386/i386.md (peepholes that split memory operand in fp\n\tconverts): New.\n\nFrom-SVN: r199546", "tree": {"sha": "042a278a93e3dfcc22ee01de018de0c350b862dd", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/042a278a93e3dfcc22ee01de018de0c350b862dd"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/55805e5496208b399bc3faa047fc54a5af3f1a36", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/55805e5496208b399bc3faa047fc54a5af3f1a36", "html_url": "https://github.com/Rust-GCC/gccrs/commit/55805e5496208b399bc3faa047fc54a5af3f1a36", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/55805e5496208b399bc3faa047fc54a5af3f1a36/comments", "author": null, "committer": null, "parents": [{"sha": "e19c9de23554122a159017cda7df5267575a199c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e19c9de23554122a159017cda7df5267575a199c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e19c9de23554122a159017cda7df5267575a199c"}], "stats": {"total": 373, "additions": 323, "deletions": 50}, "files": [{"sha": "f357e85a759d4fb1ef3fc9d3d2632f2771f805c7", "filename": "gcc/ChangeLog", "status": "modified", "additions": 31, "deletions": 5, "changes": 36, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55805e5496208b399bc3faa047fc54a5af3f1a36/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55805e5496208b399bc3faa047fc54a5af3f1a36/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=55805e5496208b399bc3faa047fc54a5af3f1a36", "patch": "@@ -1,3 +1,29 @@\n+2013-05-31  Yuri Rumyantsev  <yuri.s.rumyantsev@intel.com>\n+\t    Igor Zamyatin  <igor.zamyatin@intel.com>\n+\n+\tSilvermont (SLM) architecture performance tuning.\n+\t* config/i386/i386.h (enum ix86_tune_indices): Add\n+\tX86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS.\n+\t(TARGET_SPLIT_MEM_OPND_FOR_FP_CONVERTS): New define.\n+\n+\t* config/i386/i386.c (initial_ix86_tune_features)\n+\t<X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS>: Initialize.\n+\t(ix86_lea_outperforms): Handle Silvermont tuning.\n+\t(ix86_avoid_lea_for_add): Add new argument to ix86_lea_outperforms\n+\tcall.\n+\t(ix86_use_lea_for_mov): Likewise.\n+\t(ix86_avoid_lea_for_addr): Likewise.\n+\t(ix86_lea_for_add_ok): Likewise.\n+\t(exact_dependency_1): New function.\n+\t(exact_store_load_dependency): Likewise.\n+\t(ix86_adjust_cost): Handle Silvermont tuning.\n+\t(do_reoder_for_imul): Likewise.\n+\t(swap_top_of_ready_list): New function.\n+\t(ix86_sched_reorder): Changed to handle Silvermont tuning.\n+\n+\t* config/i386/i386.md (peepholes that split memory operand in fp\n+\tconverts): New.\n+\n 2013-05-31  Marcus Shawcroft  <marcus.shawcroft@arm.com>\n \n \t* config/aarch64/aarch64.c (aarch64_load_symref_appropriately):\n@@ -718,11 +744,11 @@\n \n 2013-05-24  Vladimir Makarov  <vmakarov@redhat.com>\n \n-        * lra-constraints.c (emit_spill_move): Use smaller mode for\n+\t* lra-constraints.c (emit_spill_move): Use smaller mode for\n \tmem-mem moves.\n-        (check_and_process_move): Consider mem-reg moves for secondary\n+\t(check_and_process_move): Consider mem-reg moves for secondary\n \ttoo.\n-        (curr_insn_transform): Don't lose insns emitted before for\n+\t(curr_insn_transform): Don't lose insns emitted before for\n \tsecondary memory moves.\n \t(inherit_in_ebb): Mark defined reg.  Add usage only if it is not a\n \treg set up in the current insn.\n@@ -1085,8 +1111,8 @@\n \n 2013-05-21  Christian Bruel  <christian.bruel@st.com>\n \n-        * dwarf2out.c (multiple_reg_loc_descriptor): Use dbx_reg_number for\n-        spanning registers. LEAF_REG_REMAP is supported only for contiguous\n+\t* dwarf2out.c (multiple_reg_loc_descriptor): Use dbx_reg_number for\n+\tspanning registers. LEAF_REG_REMAP is supported only for contiguous\n \tregisters. Set register size out of the PARALLEL loop.\n \n 2013-05-20  Oleg Endo  <olegendo@gcc.gnu.org>"}, {"sha": "69c8165390b65681f854b695ff79ff96f7bbc521", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 265, "deletions": 45, "changes": 310, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55805e5496208b399bc3faa047fc54a5af3f1a36/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55805e5496208b399bc3faa047fc54a5af3f1a36/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=55805e5496208b399bc3faa047fc54a5af3f1a36", "patch": "@@ -2108,7 +2108,12 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n \n   /* X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE: Try to avoid memory operands for\n      a conditional move.  */\n-  m_ATOM\n+  m_ATOM,\n+\n+  /* X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS: Try to split memory operand for\n+     fp converts to destination register.  */\n+  m_SLM\n+\n };\n \n /* Feature tests against the various architecture variations.  */\n@@ -17392,10 +17397,24 @@ distance_agu_use (unsigned int regno0, rtx insn)\n \n static bool\n ix86_lea_outperforms (rtx insn, unsigned int regno0, unsigned int regno1,\n-\t\t      unsigned int regno2, int split_cost)\n+\t\t      unsigned int regno2, int split_cost, bool has_scale)\n {\n   int dist_define, dist_use;\n \n+  /* For Silvermont if using a 2-source or 3-source LEA for\n+     non-destructive destination purposes, or due to wanting\n+     ability to use SCALE, the use of LEA is justified.  */\n+  if (ix86_tune == PROCESSOR_SLM)\n+    {\n+      if (has_scale)\n+        return true;\n+      if (split_cost < 1)\n+        return false;\n+      if (regno0 == regno1 || regno0 == regno2)\n+        return false;\n+      return true;\n+    }\n+\n   dist_define = distance_non_agu_define (regno1, regno2, insn);\n   dist_use = distance_agu_use (regno0, insn);\n \n@@ -17484,7 +17503,7 @@ ix86_avoid_lea_for_add (rtx insn, rtx operands[])\n   if (regno0 == regno1 || regno0 == regno2)\n     return false;\n   else\n-    return !ix86_lea_outperforms (insn, regno0, regno1, regno2, 1);\n+    return !ix86_lea_outperforms (insn, regno0, regno1, regno2, 1, false);\n }\n \n /* Return true if we should emit lea instruction instead of mov\n@@ -17506,7 +17525,7 @@ ix86_use_lea_for_mov (rtx insn, rtx operands[])\n   regno0 = true_regnum (operands[0]);\n   regno1 = true_regnum (operands[1]);\n \n-  return ix86_lea_outperforms (insn, regno0, regno1, INVALID_REGNUM, 0);\n+  return ix86_lea_outperforms (insn, regno0, regno1, INVALID_REGNUM, 0, false);\n }\n \n /* Return true if we need to split lea into a sequence of\n@@ -17585,7 +17604,8 @@ ix86_avoid_lea_for_addr (rtx insn, rtx operands[])\n       split_cost -= 1;\n     }\n \n-  return !ix86_lea_outperforms (insn, regno0, regno1, regno2, split_cost);\n+  return !ix86_lea_outperforms (insn, regno0, regno1, regno2, split_cost,\n+                                parts.scale > 1);\n }\n \n /* Emit x86 binary operand CODE in mode MODE, where the first operand\n@@ -17770,7 +17790,7 @@ ix86_lea_for_add_ok (rtx insn, rtx operands[])\n   if (!TARGET_OPT_AGU || optimize_function_for_size_p (cfun))\n     return false;\n \n-  return ix86_lea_outperforms (insn, regno0, regno1, regno2, 0);\n+  return ix86_lea_outperforms (insn, regno0, regno1, regno2, 0, false);\n }\n \n /* Return true if destination reg of SET_BODY is shift count of\n@@ -24368,6 +24388,73 @@ ix86_agi_dependent (rtx set_insn, rtx use_insn)\n   return false;\n }\n \n+/* Helper function for exact_store_load_dependency.\n+   Return true if addr is found in insn.  */\n+static bool\n+exact_dependency_1 (rtx addr, rtx insn)\n+{\n+  enum rtx_code code;\n+  const char *format_ptr;\n+  int i, j;\n+\n+  code = GET_CODE (insn);\n+  switch (code)\n+    {\n+    case MEM:\n+      if (rtx_equal_p (addr, insn))\n+        return true;\n+      break;\n+    case REG:\n+    CASE_CONST_ANY:\n+    case SYMBOL_REF:\n+    case CODE_LABEL:\n+    case PC:\n+    case CC0:\n+    case EXPR_LIST:\n+      return false;\n+    default:\n+      break;\n+    }\n+\n+  format_ptr = GET_RTX_FORMAT (code);\n+  for (i = 0; i < GET_RTX_LENGTH (code); i++)\n+    {\n+      switch (*format_ptr++)\n+       {\n+       case 'e':\n+         if (exact_dependency_1 (addr, XEXP (insn, i)))\n+           return true;\n+         break;\n+       case 'E':\n+         for (j = 0; j < XVECLEN (insn, i); j++)\n+           if (exact_dependency_1 (addr, XVECEXP (insn, i, j)))\n+             return true;\n+         break;\n+       }\n+    }\n+  return false;\n+}\n+\n+/* Return true if there exists exact dependency for store & load, i.e.\n+   the same memory address is used in them.  */\n+static bool\n+exact_store_load_dependency (rtx store, rtx load)\n+{\n+  rtx set1, set2;\n+\n+  set1 = single_set (store);\n+  if (!set1)\n+    return false;\n+  if (!MEM_P (SET_DEST (set1)))\n+    return false;\n+  set2 = single_set (load);\n+  if (!set2)\n+    return false;\n+  if (exact_dependency_1 (SET_DEST (set1), SET_SRC (set2)))\n+    return true;\n+  return false;\n+}\n+\n static int\n ix86_adjust_cost (rtx insn, rtx link, rtx dep_insn, int cost)\n {\n@@ -24519,6 +24606,39 @@ ix86_adjust_cost (rtx insn, rtx link, rtx dep_insn, int cost)\n \t  else\n \t    cost = 0;\n \t}\n+      break;\n+\n+    case PROCESSOR_SLM:\n+      if (!reload_completed)\n+        return cost;\n+\n+      /* Increase cost of integer loads.  */\n+      memory = get_attr_memory (dep_insn);\n+      if (memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n+        {\n+          enum attr_unit unit = get_attr_unit (dep_insn);\n+          if (unit == UNIT_INTEGER && cost == 1)\n+            {\n+              if (memory == MEMORY_LOAD)\n+                cost = 3;\n+              else\n+                {\n+                  /* Increase cost of ld/st for short int types only\n+                     because of store forwarding issue.  */\n+                  rtx set = single_set (dep_insn);\n+                  if (set && (GET_MODE (SET_DEST (set)) == QImode\n+                              || GET_MODE (SET_DEST (set)) == HImode))\n+                    {\n+                      /* Increase cost of store/load insn if exact\n+                         dependence exists and it is load insn.  */\n+                      enum attr_memory insn_memory = get_attr_memory (insn);\n+                      if (insn_memory == MEMORY_LOAD\n+                          && exact_store_load_dependency (dep_insn, insn))\n+                        cost = 3;\n+                    }\n+                }\n+            }\n+        }\n \n     default:\n       break;\n@@ -24565,47 +24685,32 @@ ia32_multipass_dfa_lookahead (void)\n    execution. It is applied if\n    (1) IMUL instruction is on the top of list;\n    (2) There exists the only producer of independent IMUL instruction in\n-       ready list;\n-   (3) Put found producer on the top of ready list.\n-   Returns issue rate.  */\n-\n+       ready list.\n+   Return index of IMUL producer if it was found and -1 otherwise.  */\n static int\n-ix86_sched_reorder(FILE *dump, int sched_verbose, rtx *ready, int *pn_ready,\n-                   int clock_var ATTRIBUTE_UNUSED)\n+do_reoder_for_imul(rtx *ready, int n_ready)\n {\n-  static int issue_rate = -1;\n-  int n_ready = *pn_ready;\n-  rtx insn, insn1, insn2;\n-  int i;\n+  rtx insn, set, insn1, insn2;\n   sd_iterator_def sd_it;\n   dep_t dep;\n   int index = -1;\n+  int i;\n \n-  /* Set up issue rate.  */\n-  issue_rate = ix86_issue_rate();\n-\n-  /* Do reodering for Atom only.  */\n   if (ix86_tune != PROCESSOR_ATOM)\n-    return issue_rate;\n+    return index;\n+\n   /* Do not perform ready list reodering for pre-reload schedule pass.  */\n   if (!reload_completed)\n-    return issue_rate;\n-  /* Nothing to do if ready list contains only 1 instruction.  */\n-  if (n_ready <= 1)\n-    return issue_rate;\n+    return index;\n \n   /* Check that IMUL instruction is on the top of ready list.  */\n   insn = ready[n_ready - 1];\n-  if (!NONDEBUG_INSN_P (insn))\n-    return issue_rate;\n-  insn = PATTERN (insn);\n-  if (GET_CODE (insn) == PARALLEL)\n-    insn = XVECEXP (insn, 0, 0);\n-  if (GET_CODE (insn) != SET)\n-    return issue_rate;\n-  if (!(GET_CODE (SET_SRC (insn)) == MULT\n-      && GET_MODE (SET_SRC (insn)) == SImode))\n-    return issue_rate;\n+  set = single_set (insn);\n+  if (!set)\n+    return index;\n+  if (!(GET_CODE (SET_SRC (set)) == MULT\n+      && GET_MODE (SET_SRC (set)) == SImode))\n+    return index;\n \n   /* Search for producer of independent IMUL instruction.  */\n   for (i = n_ready - 2; i>= 0; i--)\n@@ -24656,19 +24761,134 @@ ix86_sched_reorder(FILE *dump, int sched_verbose, rtx *ready, int *pn_ready,\n       if (index >= 0)\n         break;\n     }\n-  if (index < 0)\n-    return issue_rate; /* Didn't find IMUL producer.  */\n+  return index;\n+}\n+\n+/* Try to find the best candidate on the top of ready list if two insns\n+   have the same priority - candidate is best if its dependees were\n+   scheduled earlier. Applied for Silvermont only.\n+   Return true if top 2 insns must be interchanged.  */\n+static bool\n+swap_top_of_ready_list(rtx *ready, int n_ready)\n+{\n+  rtx top = ready[n_ready - 1];\n+  rtx next = ready[n_ready - 2];\n+  rtx set;\n+  sd_iterator_def sd_it;\n+  dep_t dep;\n+  int clock1 = -1;\n+  int clock2 = -1;\n+  #define INSN_TICK(INSN) (HID (INSN)->tick)\n \n-  if (sched_verbose > 1)\n-    fprintf(dump, \";;\\tatom sched_reorder: swap %d and %d insns\\n\",\n-            INSN_UID (ready[index]), INSN_UID (ready[n_ready - 1]));\n+  if (ix86_tune != PROCESSOR_SLM)\n+    return false;\n+  if (!reload_completed)\n+    return false;\n \n-  /* Put IMUL producer (ready[index]) at the top of ready list.  */\n-  insn1= ready[index];\n-  for (i = index; i < n_ready - 1; i++)\n-    ready[i] = ready[i + 1];\n-  ready[n_ready - 1] = insn1;\n+  if (!NONDEBUG_INSN_P (top))\n+    return false;\n+  if (!NONJUMP_INSN_P (top))\n+    return false;\n+  if (!NONDEBUG_INSN_P (next))\n+    return false;\n+  if (!NONJUMP_INSN_P (next))\n+    return false;\n+  set = single_set (top);\n+  if (!set)\n+    return false;\n+  set = single_set (next);\n+  if (!set)\n+    return false;\n \n+  if (INSN_PRIORITY_KNOWN (top) && INSN_PRIORITY_KNOWN (next))\n+    {\n+      if (INSN_PRIORITY (top) != INSN_PRIORITY (next))\n+        return false;\n+      /* Determine winner more precise.  */\n+      FOR_EACH_DEP (top, SD_LIST_RES_BACK, sd_it, dep)\n+        {\n+          rtx pro;\n+          pro = DEP_PRO (dep);\n+          if (!NONDEBUG_INSN_P (pro))\n+            continue;\n+          if (INSN_TICK (pro) > clock1)\n+            clock1 = INSN_TICK (pro);\n+        }\n+      FOR_EACH_DEP (next, SD_LIST_RES_BACK, sd_it, dep)\n+        {\n+          rtx pro;\n+          pro = DEP_PRO (dep);\n+          if (!NONDEBUG_INSN_P (pro))\n+            continue;\n+          if (INSN_TICK (pro) > clock2)\n+            clock2 = INSN_TICK (pro);\n+        }\n+\n+      if (clock1 == clock2)\n+      {\n+        /* Determine winner - load must win. */\n+        enum attr_memory memory1, memory2;\n+        memory1 = get_attr_memory (top);\n+        memory2 = get_attr_memory (next);\n+        if (memory2 == MEMORY_LOAD && memory1 != MEMORY_LOAD)\n+          return true;\n+      }\n+      return (bool) (clock2 < clock1);\n+    }\n+  return false;\n+  #undef INSN_TICK\n+}\n+\n+/* Perform possible reodering of ready list for Atom/Silvermont only.\n+   Return issue rate.  */\n+static int\n+ix86_sched_reorder(FILE *dump, int sched_verbose, rtx *ready, int *pn_ready,\n+                   int clock_var)\n+{\n+  int issue_rate = -1;\n+  int n_ready = *pn_ready;\n+  int i;\n+  rtx insn;\n+  int index = -1;\n+\n+  /* Set up issue rate.  */\n+  issue_rate = ix86_issue_rate();\n+\n+  /* Do reodering for Atom/SLM only.  */\n+  if (ix86_tune != PROCESSOR_ATOM && ix86_tune != PROCESSOR_SLM)\n+    return issue_rate;\n+\n+  /* Nothing to do if ready list contains only 1 instruction.  */\n+  if (n_ready <= 1)\n+    return issue_rate;\n+\n+  /* Do reodering for post-reload scheduler only.  */\n+  if (!reload_completed)\n+    return issue_rate;\n+\n+  if ((index = do_reoder_for_imul (ready, n_ready)) >= 0)\n+    {\n+      if (sched_verbose > 1)\n+        fprintf(dump, \";;\\tatom sched_reorder: put %d insn on top\\n\",\n+                INSN_UID (ready[index]));\n+\n+      /* Put IMUL producer (ready[index]) at the top of ready list.  */\n+      insn= ready[index];\n+      for (i = index; i < n_ready - 1; i++)\n+        ready[i] = ready[i + 1];\n+      ready[n_ready - 1] = insn;\n+      return issue_rate;\n+    }\n+  if (clock_var != 0 && swap_top_of_ready_list (ready, n_ready))\n+    {\n+      if (sched_verbose > 1)\n+        fprintf(dump, \";;\\tslm sched_reorder: swap %d and %d insns\\n\",\n+                INSN_UID (ready[n_ready - 1]), INSN_UID (ready[n_ready - 2]));\n+      /* Swap 2 top elements of ready list.  */\n+      insn = ready[n_ready - 1];\n+      ready[n_ready - 1] = ready[n_ready - 2];\n+      ready[n_ready - 2] = insn;\n+    }\n   return issue_rate;\n }\n "}, {"sha": "85d1a6895808545a38ea9d7c2521333cf2fe50c7", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55805e5496208b399bc3faa047fc54a5af3f1a36/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55805e5496208b399bc3faa047fc54a5af3f1a36/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=55805e5496208b399bc3faa047fc54a5af3f1a36", "patch": "@@ -333,6 +333,7 @@ enum ix86_tune_indices {\n   X86_TUNE_REASSOC_FP_TO_PARALLEL,\n   X86_TUNE_GENERAL_REGS_SSE_SPILL,\n   X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE,\n+  X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS,\n \n   X86_TUNE_LAST\n };\n@@ -443,6 +444,8 @@ extern unsigned char ix86_tune_features[X86_TUNE_LAST];\n \tix86_tune_features[X86_TUNE_GENERAL_REGS_SSE_SPILL]\n #define TARGET_AVOID_MEM_OPND_FOR_CMOVE \\\n \tix86_tune_features[X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE]\n+#define TARGET_SPLIT_MEM_OPND_FOR_FP_CONVERTS \\\n+        ix86_tune_features[X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS]\n \n /* Feature tests against the various architecture variations.  */\n enum ix86_arch_indices {"}, {"sha": "a9b4dae43d8f1cfe18cd79c4a83a304d2291ff09", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/55805e5496208b399bc3faa047fc54a5af3f1a36/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/55805e5496208b399bc3faa047fc54a5af3f1a36/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=55805e5496208b399bc3faa047fc54a5af3f1a36", "patch": "@@ -3625,6 +3625,18 @@\n \t\t\t\t  CONST0_RTX (V4SFmode), operands[1]));\n })\n \n+;; It's more profitable to split and then extend in the same register.\n+(define_peephole2\n+  [(set (match_operand:DF 0 \"register_operand\")\n+        (float_extend:DF\n+          (match_operand:SF 1 \"memory_operand\")))]\n+  \"TARGET_SPLIT_MEM_OPND_FOR_FP_CONVERTS\n+   && optimize_insn_for_speed_p ()\n+   && SSE_REG_P (operands[0])\"\n+  [(set (match_dup 2) (match_dup 1))\n+   (set (match_dup 0) (float_extend:DF (match_dup 2)))]\n+  \"operands[2] = gen_rtx_REG (SFmode, REGNO (operands[0]));\")\n+\n (define_insn \"*extendsfdf2_mixed\"\n   [(set (match_operand:DF 0 \"nonimmediate_operand\" \"=f,m,x\")\n         (float_extend:DF\n@@ -3766,6 +3778,18 @@\n \t\t\t\t CONST0_RTX (V2DFmode), operands[1]));\n })\n \n+;; It's more profitable to split and then extend in the same register.\n+(define_peephole2\n+  [(set (match_operand:SF 0 \"register_operand\")\n+        (float_truncate:SF\n+          (match_operand:DF 1 \"memory_operand\")))]\n+  \"TARGET_SPLIT_MEM_OPND_FOR_FP_CONVERTS\n+   && optimize_insn_for_speed_p ()\n+   && SSE_REG_P (operands[0])\"\n+  [(set (match_dup 2) (match_dup 1))\n+   (set (match_dup 0) (float_truncate:SF (match_dup 2)))]\n+  \"operands[2] = gen_rtx_REG (DFmode, REGNO (operands[0]));\")\n+\n (define_expand \"truncdfsf2_with_temp\"\n   [(parallel [(set (match_operand:SF 0)\n \t\t   (float_truncate:SF (match_operand:DF 1)))"}]}