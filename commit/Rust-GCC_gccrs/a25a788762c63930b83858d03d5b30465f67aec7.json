{"sha": "a25a788762c63930b83858d03d5b30465f67aec7", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YTI1YTc4ODc2MmM2MzkzMGI4Mzg1OGQwM2Q1YjMwNDY1ZjY3YWVjNw==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2016-11-22T16:53:35Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2016-11-22T16:53:35Z"}, "message": "re PR target/78451 (FAIL: gcc.target/i386/sse-22a.c: error: inlining failed in call to always_inline '_mm512_setzero_ps')\n\n\tPR target/78451\n\t* config/i386/avx512vlintrin.h (_mm_setzero_di): Removed.\n\t(_mm_maskz_mov_epi64): Use _mm_setzero_si128 instead of\n\t_mm_setzero_di.\n\t(_mm_maskz_load_epi64): Likewise.\n\t(_mm_setzero_hi): Removed.\n\t(_mm_maskz_loadu_epi64): Use _mm_setzero_si128 instead of\n\t_mm_setzero_di.\n\t(_mm_abs_epi64, _mm_maskz_abs_epi64, _mm_maskz_srl_epi64,\n\t_mm_maskz_unpackhi_epi64, _mm_maskz_unpacklo_epi64,\n\t_mm_maskz_compress_epi64, _mm_srav_epi64, _mm_maskz_srav_epi64,\n\t_mm_maskz_sllv_epi64, _mm_maskz_srlv_epi64, _mm_rolv_epi64,\n\t_mm_maskz_rolv_epi64, _mm_rorv_epi64, _mm_maskz_rorv_epi64,\n\t_mm_min_epi64, _mm_max_epi64, _mm_max_epu64, _mm_min_epu64,\n\t_mm_lzcnt_epi64, _mm_maskz_lzcnt_epi64, _mm_conflict_epi64,\n\t_mm_maskz_conflict_epi64, _mm_sra_epi64, _mm_maskz_sra_epi64,\n\t_mm_maskz_sll_epi64, _mm_rol_epi64, _mm_maskz_rol_epi64,\n\t_mm_ror_epi64, _mm_maskz_ror_epi64, _mm_alignr_epi64,\n\t_mm_maskz_alignr_epi64, _mm_srai_epi64, _mm_maskz_slli_epi64):\n\tLikewise.\n\t(_mm_cvtepi32_epi8, _mm256_cvtepi32_epi8, _mm_cvtsepi32_epi8,\n\t_mm256_cvtsepi32_epi8, _mm_cvtusepi32_epi8, _mm256_cvtusepi32_epi8,\n\t_mm_cvtepi32_epi16, _mm256_cvtepi32_epi16, _mm_cvtsepi32_epi16,\n\t_mm256_cvtsepi32_epi16, _mm_cvtusepi32_epi16, _mm256_cvtusepi32_epi16,\n\t_mm_cvtepi64_epi8, _mm256_cvtepi64_epi8, _mm_cvtsepi64_epi8,\n\t_mm256_cvtsepi64_epi8, _mm_cvtusepi64_epi8, _mm256_cvtusepi64_epi8,\n\t_mm_cvtepi64_epi16, _mm256_cvtepi64_epi16, _mm_cvtsepi64_epi16,\n\t_mm256_cvtsepi64_epi16, _mm_cvtusepi64_epi16, _mm256_cvtusepi64_epi16,\n\t_mm_cvtepi64_epi32, _mm256_cvtepi64_epi32, _mm_cvtsepi64_epi32,\n\t_mm256_cvtsepi64_epi32, _mm_cvtusepi64_epi32, _mm256_cvtusepi64_epi32,\n\t_mm_maskz_set1_epi32, _mm_maskz_set1_epi64): Formatting fixes.\n\t(_mm_maskz_cvtps_ph, _mm256_maskz_cvtps_ph): Use _mm_setzero_si128\n\tinstead of _mm_setzero_hi.\n\t(_mm256_permutex_pd, _mm256_maskz_permutex_epi64, _mm256_insertf32x4,\n\t_mm256_maskz_insertf32x4, _mm256_inserti32x4, _mm256_maskz_inserti32x4,\n\t_mm256_extractf32x4_ps, _mm256_maskz_extractf32x4_ps,\n\t_mm256_shuffle_i32x4, _mm256_maskz_shuffle_i32x4, _mm256_shuffle_f64x2,\n\t_mm256_maskz_shuffle_f64x2, _mm256_shuffle_f32x4,\n\t_mm256_maskz_shuffle_f32x4, _mm256_maskz_shuffle_pd,\n\t_mm_maskz_shuffle_pd, _mm256_maskz_shuffle_ps, _mm_maskz_shuffle_ps,\n\t_mm256_maskz_srli_epi32, _mm_maskz_srli_epi32, _mm_maskz_srli_epi64,\n\t_mm256_mask_slli_epi32, _mm256_maskz_slli_epi32, _mm256_mask_slli_epi64,\n\t_mm256_maskz_slli_epi64, _mm256_roundscale_ps,\n\t_mm256_maskz_roundscale_ps, _mm256_roundscale_pd,\n\t_mm256_maskz_roundscale_pd, _mm_roundscale_ps, _mm_maskz_roundscale_ps,\n\t_mm_roundscale_pd, _mm_maskz_roundscale_pd, _mm256_getmant_ps,\n\t_mm256_maskz_getmant_ps, _mm_getmant_ps, _mm_maskz_getmant_ps,\n\t_mm256_getmant_pd, _mm256_maskz_getmant_pd, _mm_getmant_pd,\n\t_mm_maskz_getmant_pd, _mm256_maskz_shuffle_epi32,\n\t_mm_maskz_shuffle_epi32, _mm256_rol_epi32, _mm256_maskz_rol_epi32,\n\t_mm_rol_epi32, _mm_maskz_rol_epi32, _mm256_ror_epi32,\n\t_mm256_maskz_ror_epi32, _mm_ror_epi32, _mm_maskz_ror_epi32,\n\t_mm_maskz_alignr_epi32, _mm_maskz_alignr_epi64,\n\t_mm256_maskz_srai_epi32, _mm_maskz_srai_epi32, _mm_srai_epi64,\n\t_mm_maskz_srai_epi64, _mm256_maskz_permutex_pd,\n\t_mm256_maskz_permute_pd, _mm256_maskz_permute_ps, _mm_maskz_permute_pd,\n\t_mm_maskz_permute_ps, _mm256_permutexvar_ps): Formatting fixes.\n\t(_mm_maskz_slli_epi64, _mm_rol_epi64, _mm_maskz_rol_epi64,\n\t_mm_ror_epi64, _mm_maskz_ror_epi64): Use _mm_setzero_si128 instead of\n\t_mm_setzero_di.\n\t(_mm_maskz_cvtps_ph, _mm256_maskz_cvtps_ph): Use _mm_setzero_si128\n\tinstead of _mm_setzero_hi.\n\t* config/i386/avx512dqintrin.h (_mm512_broadcast_f64x2,\n\t_mm512_broadcast_i64x2, _mm512_broadcast_f32x2, _mm512_broadcast_i32x2,\n\t_mm512_broadcast_f32x8, _mm512_broadcast_i32x8): Formatting fixes.\n\t(_mm512_extracti64x2_epi64, _mm512_maskz_extracti64x2_epi64): Use\n\t_mm_setzero_si128 instead of _mm_setzero_di.\n\t(_mm512_cvtt_roundpd_epi64, _mm512_mask_cvtt_roundpd_epi64,\n\t_mm512_maskz_cvtt_roundpd_epi64, _mm512_cvtt_roundpd_epu64,\n\t_mm512_mask_cvtt_roundpd_epu64, _mm512_maskz_cvtt_roundpd_epu64,\n\t_mm512_cvtt_roundps_epi64, _mm512_mask_cvtt_roundps_epi64,\n\t_mm512_maskz_cvtt_roundps_epi64, _mm512_cvtt_roundps_epu64,\n\t_mm512_mask_cvtt_roundps_epu64, _mm512_maskz_cvtt_roundps_epu64,\n\t_mm512_cvt_roundpd_epi64, _mm512_mask_cvt_roundpd_epi64,\n\t_mm512_maskz_cvt_roundpd_epi64, _mm512_cvt_roundpd_epu64,\n\t_mm512_mask_cvt_roundpd_epu64, _mm512_maskz_cvt_roundpd_epu64,\n\t_mm512_cvt_roundps_epi64, _mm512_mask_cvt_roundps_epi64,\n\t_mm512_maskz_cvt_roundps_epi64, _mm512_cvt_roundps_epu64,\n\t_mm512_mask_cvt_roundps_epu64, _mm512_maskz_cvt_roundps_epu64,\n\t_mm512_cvt_roundepi64_ps, _mm512_mask_cvt_roundepi64_ps,\n\t_mm512_maskz_cvt_roundepi64_ps, _mm512_cvt_roundepu64_ps,\n\t_mm512_mask_cvt_roundepu64_ps, _mm512_maskz_cvt_roundepu64_ps,\n\t_mm512_cvt_roundepi64_pd, _mm512_mask_cvt_roundepi64_pd,\n\t_mm512_maskz_cvt_roundepi64_pd, _mm512_cvt_roundepu64_pd,\n\t_mm512_mask_cvt_roundepu64_pd, _mm512_maskz_cvt_roundepu64_pd,\n\t_mm512_reduce_pd, _mm512_maskz_reduce_pd, _mm512_reduce_ps,\n\t_mm512_maskz_reduce_ps, _mm512_extractf32x8_ps,\n\t_mm512_maskz_extractf32x8_ps, _mm512_extractf64x2_pd,\n\t_mm512_maskz_extractf64x2_pd, _mm512_extracti32x8_epi32,\n\t_mm512_maskz_extracti32x8_epi32, _mm512_range_pd,\n\t_mm512_maskz_range_pd, _mm512_range_ps, _mm512_maskz_range_ps,\n\t_mm512_range_round_pd, _mm512_maskz_range_round_pd,\n\t_mm512_range_round_ps, _mm512_maskz_range_round_ps,\n\t_mm512_maskz_insertf64x2, _mm512_insertf32x8,\n\t_mm512_maskz_insertf32x8): Formatting fixes.\n\t(_mm512_extracti64x2_epi64, _mm512_maskz_extracti64x2_epi64): Use\n\t_mm_setzero_si128 instead of _mm_setzero_di.\n\t* config/i386/avx512vldqintrin.h (_mm_cvttpd_epi64,\n\t_mm_cvttpd_epu64, _mm_cvtpd_epi64, _mm_cvtpd_epu64,\n\t_mm_cvttps_epi64, _mm_maskz_cvttps_epi64, _mm_cvttps_epu64,\n\t_mm_maskz_cvttps_epu64, _mm_maskz_mullo_epi64, _mm_cvtps_epi64,\n\t_mm_maskz_cvtps_epi64, _mm_cvtps_epu64, _mm_maskz_cvtps_epu64,\n\t_mm256_extracti64x2_epi64, _mm256_maskz_extracti64x2_epi64): Use\n\t_mm_setzero_si128 instead of _mm_setzero_di.\n\t(_mm256_extracti64x2_epi64, _mm256_maskz_extracti64x2_epi64):\n\tLikewise in macros.\n\t* config/i386/avx512vlbwintrin.h (_mm_maskz_mov_epi8,\n\t_mm_maskz_loadu_epi16, _mm_maskz_mov_epi16, _mm_maskz_loadu_epi8,\n\t_mm_permutexvar_epi16, _mm_maskz_maddubs_epi16): Use\n\t_mm_setzero_si128 instead of _mm_setzero_hi.\n\t(_mm_maskz_min_epu16, _mm_maskz_max_epu8, _mm_maskz_max_epi8,\n\t_mm_maskz_min_epu8, _mm_maskz_min_epi8, _mm_maskz_max_epi16,\n\t_mm_maskz_max_epu16, _mm_maskz_min_epi16): Use _mm_setzero_si128\n\tinstead of _mm_setzero_di.\n\t(_mm_dbsad_epu8, _mm_maskz_shufflehi_epi16,\n\t_mm_maskz_shufflelo_epi16): Use _mm_setzero_si128 instead of\n\t_mm_setzero_hi.\n\t(_mm_maskz_shufflehi_epi16, _mm_maskz_shufflelo_epi16,\n\t_mm_maskz_slli_epi16): Use _mm_setzero_si128 instead of\n\t_mm_setzero_hi.\n\t(_mm_maskz_alignr_epi8): Use _mm_setzero_si128 instead of\n\t_mm_setzero_di.\n\t(_mm_maskz_mulhi_epi16, _mm_maskz_mulhi_epu16, _mm_maskz_mulhrs_epi16,\n\t_mm_maskz_mullo_epi16, _mm_srav_epi16, _mm_srlv_epi16,\n\t_mm_sllv_epi16): Use _mm_setzero_si128 instead of _mm_setzero_hi.\n\nFrom-SVN: r242707", "tree": {"sha": "0354ca07f9a08e86196ff16007ff0b9e989a68f3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/0354ca07f9a08e86196ff16007ff0b9e989a68f3"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a25a788762c63930b83858d03d5b30465f67aec7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a25a788762c63930b83858d03d5b30465f67aec7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a25a788762c63930b83858d03d5b30465f67aec7", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a25a788762c63930b83858d03d5b30465f67aec7/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "a0af8668dcfac575d5c0f426d2dfac76d5a1490a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a0af8668dcfac575d5c0f426d2dfac76d5a1490a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a0af8668dcfac575d5c0f426d2dfac76d5a1490a"}], "stats": {"total": 735, "additions": 444, "deletions": 291}, "files": [{"sha": "179a26dd36b0ded4a8c54fb44bf8f7f52d6db404", "filename": "gcc/ChangeLog", "status": "modified", "additions": 128, "deletions": 0, "changes": 128, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a25a788762c63930b83858d03d5b30465f67aec7/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a25a788762c63930b83858d03d5b30465f67aec7/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=a25a788762c63930b83858d03d5b30465f67aec7", "patch": "@@ -1,3 +1,131 @@\n+2016-11-22  Jakub Jelinek  <jakub@redhat.com>\n+\n+\tPR target/78451\n+\t* config/i386/avx512vlintrin.h (_mm_setzero_di): Removed.\n+\t(_mm_maskz_mov_epi64): Use _mm_setzero_si128 instead of\n+\t_mm_setzero_di.\n+\t(_mm_maskz_load_epi64): Likewise.\n+\t(_mm_setzero_hi): Removed.\n+\t(_mm_maskz_loadu_epi64): Use _mm_setzero_si128 instead of\n+\t_mm_setzero_di.\n+\t(_mm_abs_epi64, _mm_maskz_abs_epi64, _mm_maskz_srl_epi64,\n+\t_mm_maskz_unpackhi_epi64, _mm_maskz_unpacklo_epi64,\n+\t_mm_maskz_compress_epi64, _mm_srav_epi64, _mm_maskz_srav_epi64,\n+\t_mm_maskz_sllv_epi64, _mm_maskz_srlv_epi64, _mm_rolv_epi64,\n+\t_mm_maskz_rolv_epi64, _mm_rorv_epi64, _mm_maskz_rorv_epi64,\n+\t_mm_min_epi64, _mm_max_epi64, _mm_max_epu64, _mm_min_epu64,\n+\t_mm_lzcnt_epi64, _mm_maskz_lzcnt_epi64, _mm_conflict_epi64,\n+\t_mm_maskz_conflict_epi64, _mm_sra_epi64, _mm_maskz_sra_epi64,\n+\t_mm_maskz_sll_epi64, _mm_rol_epi64, _mm_maskz_rol_epi64,\n+\t_mm_ror_epi64, _mm_maskz_ror_epi64, _mm_alignr_epi64,\n+\t_mm_maskz_alignr_epi64, _mm_srai_epi64, _mm_maskz_slli_epi64):\n+\tLikewise.\n+\t(_mm_cvtepi32_epi8, _mm256_cvtepi32_epi8, _mm_cvtsepi32_epi8,\n+\t_mm256_cvtsepi32_epi8, _mm_cvtusepi32_epi8, _mm256_cvtusepi32_epi8,\n+\t_mm_cvtepi32_epi16, _mm256_cvtepi32_epi16, _mm_cvtsepi32_epi16,\n+\t_mm256_cvtsepi32_epi16, _mm_cvtusepi32_epi16, _mm256_cvtusepi32_epi16,\n+\t_mm_cvtepi64_epi8, _mm256_cvtepi64_epi8, _mm_cvtsepi64_epi8,\n+\t_mm256_cvtsepi64_epi8, _mm_cvtusepi64_epi8, _mm256_cvtusepi64_epi8,\n+\t_mm_cvtepi64_epi16, _mm256_cvtepi64_epi16, _mm_cvtsepi64_epi16,\n+\t_mm256_cvtsepi64_epi16, _mm_cvtusepi64_epi16, _mm256_cvtusepi64_epi16,\n+\t_mm_cvtepi64_epi32, _mm256_cvtepi64_epi32, _mm_cvtsepi64_epi32,\n+\t_mm256_cvtsepi64_epi32, _mm_cvtusepi64_epi32, _mm256_cvtusepi64_epi32,\n+\t_mm_maskz_set1_epi32, _mm_maskz_set1_epi64): Formatting fixes.\n+\t(_mm_maskz_cvtps_ph, _mm256_maskz_cvtps_ph): Use _mm_setzero_si128\n+\tinstead of _mm_setzero_hi.\n+\t(_mm256_permutex_pd, _mm256_maskz_permutex_epi64, _mm256_insertf32x4,\n+\t_mm256_maskz_insertf32x4, _mm256_inserti32x4, _mm256_maskz_inserti32x4,\n+\t_mm256_extractf32x4_ps, _mm256_maskz_extractf32x4_ps,\n+\t_mm256_shuffle_i32x4, _mm256_maskz_shuffle_i32x4, _mm256_shuffle_f64x2,\n+\t_mm256_maskz_shuffle_f64x2, _mm256_shuffle_f32x4,\n+\t_mm256_maskz_shuffle_f32x4, _mm256_maskz_shuffle_pd,\n+\t_mm_maskz_shuffle_pd, _mm256_maskz_shuffle_ps, _mm_maskz_shuffle_ps,\n+\t_mm256_maskz_srli_epi32, _mm_maskz_srli_epi32, _mm_maskz_srli_epi64,\n+\t_mm256_mask_slli_epi32, _mm256_maskz_slli_epi32, _mm256_mask_slli_epi64,\n+\t_mm256_maskz_slli_epi64, _mm256_roundscale_ps,\n+\t_mm256_maskz_roundscale_ps, _mm256_roundscale_pd,\n+\t_mm256_maskz_roundscale_pd, _mm_roundscale_ps, _mm_maskz_roundscale_ps,\n+\t_mm_roundscale_pd, _mm_maskz_roundscale_pd, _mm256_getmant_ps,\n+\t_mm256_maskz_getmant_ps, _mm_getmant_ps, _mm_maskz_getmant_ps,\n+\t_mm256_getmant_pd, _mm256_maskz_getmant_pd, _mm_getmant_pd,\n+\t_mm_maskz_getmant_pd, _mm256_maskz_shuffle_epi32,\n+\t_mm_maskz_shuffle_epi32, _mm256_rol_epi32, _mm256_maskz_rol_epi32,\n+\t_mm_rol_epi32, _mm_maskz_rol_epi32, _mm256_ror_epi32,\n+\t_mm256_maskz_ror_epi32, _mm_ror_epi32, _mm_maskz_ror_epi32,\n+\t_mm_maskz_alignr_epi32, _mm_maskz_alignr_epi64,\n+\t_mm256_maskz_srai_epi32, _mm_maskz_srai_epi32, _mm_srai_epi64,\n+\t_mm_maskz_srai_epi64, _mm256_maskz_permutex_pd,\n+\t_mm256_maskz_permute_pd, _mm256_maskz_permute_ps, _mm_maskz_permute_pd,\n+\t_mm_maskz_permute_ps, _mm256_permutexvar_ps): Formatting fixes.\n+\t(_mm_maskz_slli_epi64, _mm_rol_epi64, _mm_maskz_rol_epi64,\n+\t_mm_ror_epi64, _mm_maskz_ror_epi64): Use _mm_setzero_si128 instead of\n+\t_mm_setzero_di.\n+\t(_mm_maskz_cvtps_ph, _mm256_maskz_cvtps_ph): Use _mm_setzero_si128\n+\tinstead of _mm_setzero_hi.\n+\t* config/i386/avx512dqintrin.h (_mm512_broadcast_f64x2,\n+\t_mm512_broadcast_i64x2, _mm512_broadcast_f32x2, _mm512_broadcast_i32x2,\n+\t_mm512_broadcast_f32x8, _mm512_broadcast_i32x8): Formatting fixes.\n+\t(_mm512_extracti64x2_epi64, _mm512_maskz_extracti64x2_epi64): Use\n+\t_mm_setzero_si128 instead of _mm_setzero_di.\n+\t(_mm512_cvtt_roundpd_epi64, _mm512_mask_cvtt_roundpd_epi64,\n+\t_mm512_maskz_cvtt_roundpd_epi64, _mm512_cvtt_roundpd_epu64,\n+\t_mm512_mask_cvtt_roundpd_epu64, _mm512_maskz_cvtt_roundpd_epu64,\n+\t_mm512_cvtt_roundps_epi64, _mm512_mask_cvtt_roundps_epi64,\n+\t_mm512_maskz_cvtt_roundps_epi64, _mm512_cvtt_roundps_epu64,\n+\t_mm512_mask_cvtt_roundps_epu64, _mm512_maskz_cvtt_roundps_epu64,\n+\t_mm512_cvt_roundpd_epi64, _mm512_mask_cvt_roundpd_epi64,\n+\t_mm512_maskz_cvt_roundpd_epi64, _mm512_cvt_roundpd_epu64,\n+\t_mm512_mask_cvt_roundpd_epu64, _mm512_maskz_cvt_roundpd_epu64,\n+\t_mm512_cvt_roundps_epi64, _mm512_mask_cvt_roundps_epi64,\n+\t_mm512_maskz_cvt_roundps_epi64, _mm512_cvt_roundps_epu64,\n+\t_mm512_mask_cvt_roundps_epu64, _mm512_maskz_cvt_roundps_epu64,\n+\t_mm512_cvt_roundepi64_ps, _mm512_mask_cvt_roundepi64_ps,\n+\t_mm512_maskz_cvt_roundepi64_ps, _mm512_cvt_roundepu64_ps,\n+\t_mm512_mask_cvt_roundepu64_ps, _mm512_maskz_cvt_roundepu64_ps,\n+\t_mm512_cvt_roundepi64_pd, _mm512_mask_cvt_roundepi64_pd,\n+\t_mm512_maskz_cvt_roundepi64_pd, _mm512_cvt_roundepu64_pd,\n+\t_mm512_mask_cvt_roundepu64_pd, _mm512_maskz_cvt_roundepu64_pd,\n+\t_mm512_reduce_pd, _mm512_maskz_reduce_pd, _mm512_reduce_ps,\n+\t_mm512_maskz_reduce_ps, _mm512_extractf32x8_ps,\n+\t_mm512_maskz_extractf32x8_ps, _mm512_extractf64x2_pd,\n+\t_mm512_maskz_extractf64x2_pd, _mm512_extracti32x8_epi32,\n+\t_mm512_maskz_extracti32x8_epi32, _mm512_range_pd,\n+\t_mm512_maskz_range_pd, _mm512_range_ps, _mm512_maskz_range_ps,\n+\t_mm512_range_round_pd, _mm512_maskz_range_round_pd,\n+\t_mm512_range_round_ps, _mm512_maskz_range_round_ps,\n+\t_mm512_maskz_insertf64x2, _mm512_insertf32x8,\n+\t_mm512_maskz_insertf32x8): Formatting fixes.\n+\t(_mm512_extracti64x2_epi64, _mm512_maskz_extracti64x2_epi64): Use\n+\t_mm_setzero_si128 instead of _mm_setzero_di.\n+\t* config/i386/avx512vldqintrin.h (_mm_cvttpd_epi64,\n+\t_mm_cvttpd_epu64, _mm_cvtpd_epi64, _mm_cvtpd_epu64,\n+\t_mm_cvttps_epi64, _mm_maskz_cvttps_epi64, _mm_cvttps_epu64,\n+\t_mm_maskz_cvttps_epu64, _mm_maskz_mullo_epi64, _mm_cvtps_epi64,\n+\t_mm_maskz_cvtps_epi64, _mm_cvtps_epu64, _mm_maskz_cvtps_epu64,\n+\t_mm256_extracti64x2_epi64, _mm256_maskz_extracti64x2_epi64): Use\n+\t_mm_setzero_si128 instead of _mm_setzero_di.\n+\t(_mm256_extracti64x2_epi64, _mm256_maskz_extracti64x2_epi64):\n+\tLikewise in macros.\n+\t* config/i386/avx512vlbwintrin.h (_mm_maskz_mov_epi8,\n+\t_mm_maskz_loadu_epi16, _mm_maskz_mov_epi16, _mm_maskz_loadu_epi8,\n+\t_mm_permutexvar_epi16, _mm_maskz_maddubs_epi16): Use\n+\t_mm_setzero_si128 instead of _mm_setzero_hi.\n+\t(_mm_maskz_min_epu16, _mm_maskz_max_epu8, _mm_maskz_max_epi8,\n+\t_mm_maskz_min_epu8, _mm_maskz_min_epi8, _mm_maskz_max_epi16,\n+\t_mm_maskz_max_epu16, _mm_maskz_min_epi16): Use _mm_setzero_si128\n+\tinstead of _mm_setzero_di.\n+\t(_mm_dbsad_epu8, _mm_maskz_shufflehi_epi16,\n+\t_mm_maskz_shufflelo_epi16): Use _mm_setzero_si128 instead of\n+\t_mm_setzero_hi.\n+\t(_mm_maskz_shufflehi_epi16, _mm_maskz_shufflelo_epi16,\n+\t_mm_maskz_slli_epi16): Use _mm_setzero_si128 instead of\n+\t_mm_setzero_hi.\n+\t(_mm_maskz_alignr_epi8): Use _mm_setzero_si128 instead of\n+\t_mm_setzero_di.\n+\t(_mm_maskz_mulhi_epi16, _mm_maskz_mulhi_epu16, _mm_maskz_mulhrs_epi16,\n+\t_mm_maskz_mullo_epi16, _mm_srav_epi16, _mm_srlv_epi16,\n+\t_mm_sllv_epi16): Use _mm_setzero_si128 instead of _mm_setzero_hi.\n+\n 2016-11-22  Carl Love  <cel@us.ibm.com>\n \n \t* config/rs6000/rs6000-c.c: Add built-in support for vector compare"}, {"sha": "4b954f924ed45e3282cb1623190281863a500ada", "filename": "gcc/config/i386/avx512dqintrin.h", "status": "modified", "additions": 89, "deletions": 83, "changes": 172, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a25a788762c63930b83858d03d5b30465f67aec7/gcc%2Fconfig%2Fi386%2Favx512dqintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a25a788762c63930b83858d03d5b30465f67aec7/gcc%2Fconfig%2Fi386%2Favx512dqintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Favx512dqintrin.h?ref=a25a788762c63930b83858d03d5b30465f67aec7", "patch": "@@ -38,10 +38,10 @@ extern __inline __m512d\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_broadcast_f64x2 (__m128d __A)\n {\n-  return (__m512d) __builtin_ia32_broadcastf64x2_512_mask ((__v2df)\n-\t\t\t\t\t\t\t   __A,\n-\t\t\t\t\t\t\t   _mm512_undefined_pd(),\n-\t\t\t\t\t\t\t   (__mmask8) -1);\n+  return (__m512d)\n+\t __builtin_ia32_broadcastf64x2_512_mask ((__v2df) __A,\n+\t\t\t\t\t\t _mm512_undefined_pd (),\n+\t\t\t\t\t\t (__mmask8) -1);\n }\n \n extern __inline __m512d\n@@ -69,10 +69,10 @@ extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_broadcast_i64x2 (__m128i __A)\n {\n-  return (__m512i) __builtin_ia32_broadcasti64x2_512_mask ((__v2di)\n-\t\t\t\t\t\t\t   __A,\n-\t\t\t\t\t\t\t   _mm512_undefined_epi32(),\n-\t\t\t\t\t\t\t   (__mmask8) -1);\n+  return (__m512i)\n+\t __builtin_ia32_broadcasti64x2_512_mask ((__v2di) __A,\n+\t\t\t\t\t\t _mm512_undefined_epi32 (),\n+\t\t\t\t\t\t (__mmask8) -1);\n }\n \n extern __inline __m512i\n@@ -100,9 +100,10 @@ extern __inline __m512\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_broadcast_f32x2 (__m128 __A)\n {\n-  return (__m512) __builtin_ia32_broadcastf32x2_512_mask ((__v4sf) __A,\n-\t\t\t\t\t\t\t  (__v16sf)_mm512_undefined_ps(),\n-\t\t\t\t\t\t\t  (__mmask16) -1);\n+  return (__m512)\n+\t __builtin_ia32_broadcastf32x2_512_mask ((__v4sf) __A,\n+\t\t\t\t\t\t (__v16sf)_mm512_undefined_ps (),\n+\t\t\t\t\t\t (__mmask16) -1);\n }\n \n extern __inline __m512\n@@ -128,10 +129,11 @@ extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_broadcast_i32x2 (__m128i __A)\n {\n-  return (__m512i) __builtin_ia32_broadcasti32x2_512_mask ((__v4si)\n-\t\t\t\t\t\t\t   __A,\n-\t\t\t\t\t\t\t   (__v16si)_mm512_undefined_epi32(),\n-\t\t\t\t\t\t\t   (__mmask16) -1);\n+  return (__m512i)\n+\t __builtin_ia32_broadcasti32x2_512_mask ((__v4si) __A,\n+\t\t\t\t\t\t (__v16si)\n+\t\t\t\t\t\t _mm512_undefined_epi32 (),\n+\t\t\t\t\t\t (__mmask16) -1);\n }\n \n extern __inline __m512i\n@@ -159,9 +161,10 @@ extern __inline __m512\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_broadcast_f32x8 (__m256 __A)\n {\n-  return (__m512) __builtin_ia32_broadcastf32x8_512_mask ((__v8sf) __A,\n-\t\t\t\t\t\t\t  _mm512_undefined_ps(),\n-\t\t\t\t\t\t\t  (__mmask16) -1);\n+  return (__m512)\n+\t __builtin_ia32_broadcastf32x8_512_mask ((__v8sf) __A,\n+\t\t\t\t\t\t _mm512_undefined_ps (),\n+\t\t\t\t\t\t (__mmask16) -1);\n }\n \n extern __inline __m512\n@@ -187,10 +190,11 @@ extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_broadcast_i32x8 (__m256i __A)\n {\n-  return (__m512i) __builtin_ia32_broadcasti32x8_512_mask ((__v8si)\n-\t\t\t\t\t\t\t   __A,\n-\t\t\t\t\t\t\t   (__v16si)_mm512_undefined_epi32(),\n-\t\t\t\t\t\t\t   (__mmask16) -1);\n+  return (__m512i)\n+\t __builtin_ia32_broadcasti32x8_512_mask ((__v8si) __A,\n+\t\t\t\t\t\t (__v16si)\n+\t\t\t\t\t\t _mm512_undefined_epi32 (),\n+\t\t\t\t\t\t (__mmask16) -1);\n }\n \n extern __inline __m512i\n@@ -1632,7 +1636,7 @@ _mm512_extracti64x2_epi64 (__m512i __A, const int __imm)\n   return (__m128i) __builtin_ia32_extracti64x2_512_mask ((__v8di) __A,\n \t\t\t\t\t\t\t __imm,\n \t\t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t\t (__mmask8) -1);\n }\n \n@@ -1656,7 +1660,7 @@ _mm512_maskz_extracti64x2_epi64 (__mmask8 __U, __m512i __A,\n   return (__m128i) __builtin_ia32_extracti64x2_512_mask ((__v8di) __A,\n \t\t\t\t\t\t\t __imm,\n \t\t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t\t (__mmask8)\n \t\t\t\t\t\t\t __U);\n }\n@@ -1946,189 +1950,191 @@ _mm512_fpclass_ps_mask (__m512 __A, const int __imm)\n     (__v4sf)(__m128)(B), (int)(C), (R)))\n \n #define _mm512_cvtt_roundpd_epi64(A, B)\t\t    \\\n-    ((__m512i)__builtin_ia32_cvttpd2qq512_mask((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n+  ((__m512i)__builtin_ia32_cvttpd2qq512_mask ((A), (__v8di)\t\t\\\n+\t\t\t\t\t      _mm512_setzero_si512 (),\t\\\n+\t\t\t\t\t      -1, (B)))\n \n #define _mm512_mask_cvtt_roundpd_epi64(W, U, A, B)  \\\n-    ((__m512i)__builtin_ia32_cvttpd2qq512_mask((A), (__v8di)(W), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvttpd2qq512_mask ((A), (__v8di)(W), (U), (B)))\n \n #define _mm512_maskz_cvtt_roundpd_epi64(U, A, B)    \\\n-    ((__m512i)__builtin_ia32_cvttpd2qq512_mask((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvttpd2qq512_mask ((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n \n #define _mm512_cvtt_roundpd_epu64(A, B)\t\t    \\\n-    ((__m512i)__builtin_ia32_cvttpd2uqq512_mask((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n+    ((__m512i)__builtin_ia32_cvttpd2uqq512_mask ((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n \n #define _mm512_mask_cvtt_roundpd_epu64(W, U, A, B)  \\\n-    ((__m512i)__builtin_ia32_cvttpd2uqq512_mask((A), (__v8di)(W), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvttpd2uqq512_mask ((A), (__v8di)(W), (U), (B)))\n \n #define _mm512_maskz_cvtt_roundpd_epu64(U, A, B)    \\\n-    ((__m512i)__builtin_ia32_cvttpd2uqq512_mask((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvttpd2uqq512_mask ((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n \n #define _mm512_cvtt_roundps_epi64(A, B)\t\t    \\\n-    ((__m512i)__builtin_ia32_cvttps2qq512_mask((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n+    ((__m512i)__builtin_ia32_cvttps2qq512_mask ((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n \n #define _mm512_mask_cvtt_roundps_epi64(W, U, A, B)  \\\n-    ((__m512i)__builtin_ia32_cvttps2qq512_mask((A), (__v8di)(W), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvttps2qq512_mask ((A), (__v8di)(W), (U), (B)))\n \n #define _mm512_maskz_cvtt_roundps_epi64(U, A, B)    \\\n-    ((__m512i)__builtin_ia32_cvttps2qq512_mask((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvttps2qq512_mask ((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n \n #define _mm512_cvtt_roundps_epu64(A, B)\t\t    \\\n-    ((__m512i)__builtin_ia32_cvttps2uqq512_mask((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n+    ((__m512i)__builtin_ia32_cvttps2uqq512_mask ((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n \n #define _mm512_mask_cvtt_roundps_epu64(W, U, A, B)  \\\n-    ((__m512i)__builtin_ia32_cvttps2uqq512_mask((A), (__v8di)(W), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvttps2uqq512_mask ((A), (__v8di)(W), (U), (B)))\n \n #define _mm512_maskz_cvtt_roundps_epu64(U, A, B)    \\\n-    ((__m512i)__builtin_ia32_cvttps2uqq512_mask((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvttps2uqq512_mask ((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n \n #define _mm512_cvt_roundpd_epi64(A, B)\t\t    \\\n-    ((__m512i)__builtin_ia32_cvtpd2qq512_mask((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n+    ((__m512i)__builtin_ia32_cvtpd2qq512_mask ((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n \n #define _mm512_mask_cvt_roundpd_epi64(W, U, A, B)   \\\n-    ((__m512i)__builtin_ia32_cvtpd2qq512_mask((A), (__v8di)(W), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvtpd2qq512_mask ((A), (__v8di)(W), (U), (B)))\n \n #define _mm512_maskz_cvt_roundpd_epi64(U, A, B)     \\\n-    ((__m512i)__builtin_ia32_cvtpd2qq512_mask((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvtpd2qq512_mask ((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n \n #define _mm512_cvt_roundpd_epu64(A, B)\t\t    \\\n-    ((__m512i)__builtin_ia32_cvtpd2uqq512_mask((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n+    ((__m512i)__builtin_ia32_cvtpd2uqq512_mask ((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n \n #define _mm512_mask_cvt_roundpd_epu64(W, U, A, B)   \\\n-    ((__m512i)__builtin_ia32_cvtpd2uqq512_mask((A), (__v8di)(W), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvtpd2uqq512_mask ((A), (__v8di)(W), (U), (B)))\n \n #define _mm512_maskz_cvt_roundpd_epu64(U, A, B)     \\\n-    ((__m512i)__builtin_ia32_cvtpd2uqq512_mask((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvtpd2uqq512_mask ((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n \n #define _mm512_cvt_roundps_epi64(A, B)\t\t    \\\n-    ((__m512i)__builtin_ia32_cvtps2qq512_mask((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n+    ((__m512i)__builtin_ia32_cvtps2qq512_mask ((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n \n #define _mm512_mask_cvt_roundps_epi64(W, U, A, B)   \\\n-    ((__m512i)__builtin_ia32_cvtps2qq512_mask((A), (__v8di)(W), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvtps2qq512_mask ((A), (__v8di)(W), (U), (B)))\n \n #define _mm512_maskz_cvt_roundps_epi64(U, A, B)     \\\n-    ((__m512i)__builtin_ia32_cvtps2qq512_mask((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvtps2qq512_mask ((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n \n #define _mm512_cvt_roundps_epu64(A, B)\t\t    \\\n-    ((__m512i)__builtin_ia32_cvtps2uqq512_mask((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n+    ((__m512i)__builtin_ia32_cvtps2uqq512_mask ((A), (__v8di)_mm512_setzero_si512 (), -1, (B)))\n \n #define _mm512_mask_cvt_roundps_epu64(W, U, A, B)   \\\n-    ((__m512i)__builtin_ia32_cvtps2uqq512_mask((A), (__v8di)(W), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvtps2uqq512_mask ((A), (__v8di)(W), (U), (B)))\n \n #define _mm512_maskz_cvt_roundps_epu64(U, A, B)     \\\n-    ((__m512i)__builtin_ia32_cvtps2uqq512_mask((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n+    ((__m512i)__builtin_ia32_cvtps2uqq512_mask ((A), (__v8di)_mm512_setzero_si512 (), (U), (B)))\n \n #define _mm512_cvt_roundepi64_ps(A, B)\t\t    \\\n-    ((__m256)__builtin_ia32_cvtqq2ps512_mask((__v8di)(A), (__v8sf)_mm256_setzero_ps(), -1, (B)))\n+    ((__m256)__builtin_ia32_cvtqq2ps512_mask ((__v8di)(A), (__v8sf)_mm256_setzero_ps (), -1, (B)))\n \n #define _mm512_mask_cvt_roundepi64_ps(W, U, A, B)   \\\n-    ((__m256)__builtin_ia32_cvtqq2ps512_mask((__v8di)(A), (W), (U), (B)))\n+    ((__m256)__builtin_ia32_cvtqq2ps512_mask ((__v8di)(A), (W), (U), (B)))\n \n #define _mm512_maskz_cvt_roundepi64_ps(U, A, B)     \\\n-    ((__m256)__builtin_ia32_cvtqq2ps512_mask((__v8di)(A), (__v8sf)_mm256_setzero_ps(), (U), (B)))\n+    ((__m256)__builtin_ia32_cvtqq2ps512_mask ((__v8di)(A), (__v8sf)_mm256_setzero_ps (), (U), (B)))\n \n #define _mm512_cvt_roundepu64_ps(A, B)\t\t    \\\n-    ((__m256)__builtin_ia32_cvtuqq2ps512_mask((__v8di)(A), (__v8sf)_mm256_setzero_ps(), -1, (B)))\n+    ((__m256)__builtin_ia32_cvtuqq2ps512_mask ((__v8di)(A), (__v8sf)_mm256_setzero_ps (), -1, (B)))\n \n #define _mm512_mask_cvt_roundepu64_ps(W, U, A, B)   \\\n-    ((__m256)__builtin_ia32_cvtuqq2ps512_mask((__v8di)(A), (W), (U), (B)))\n+    ((__m256)__builtin_ia32_cvtuqq2ps512_mask ((__v8di)(A), (W), (U), (B)))\n \n #define _mm512_maskz_cvt_roundepu64_ps(U, A, B)     \\\n-    ((__m256)__builtin_ia32_cvtuqq2ps512_mask((__v8di)(A), (__v8sf)_mm256_setzero_ps(), (U), (B)))\n+    ((__m256)__builtin_ia32_cvtuqq2ps512_mask ((__v8di)(A), (__v8sf)_mm256_setzero_ps (), (U), (B)))\n \n #define _mm512_cvt_roundepi64_pd(A, B)\t\t    \\\n-    ((__m512d)__builtin_ia32_cvtqq2pd512_mask((__v8di)(A), (__v8df)_mm512_setzero_pd(), -1, (B)))\n+    ((__m512d)__builtin_ia32_cvtqq2pd512_mask ((__v8di)(A), (__v8df)_mm512_setzero_pd (), -1, (B)))\n \n #define _mm512_mask_cvt_roundepi64_pd(W, U, A, B)   \\\n-    ((__m512d)__builtin_ia32_cvtqq2pd512_mask((__v8di)(A), (W), (U), (B)))\n+    ((__m512d)__builtin_ia32_cvtqq2pd512_mask ((__v8di)(A), (W), (U), (B)))\n \n #define _mm512_maskz_cvt_roundepi64_pd(U, A, B)     \\\n-    ((__m512d)__builtin_ia32_cvtqq2pd512_mask((__v8di)(A), (__v8df)_mm512_setzero_pd(), (U), (B)))\n+    ((__m512d)__builtin_ia32_cvtqq2pd512_mask ((__v8di)(A), (__v8df)_mm512_setzero_pd (), (U), (B)))\n \n #define _mm512_cvt_roundepu64_pd(A, B)\t\t    \\\n-    ((__m512d)__builtin_ia32_cvtuqq2pd512_mask((__v8di)(A), (__v8df)_mm512_setzero_pd(), -1, (B)))\n+    ((__m512d)__builtin_ia32_cvtuqq2pd512_mask ((__v8di)(A), (__v8df)_mm512_setzero_pd (), -1, (B)))\n \n #define _mm512_mask_cvt_roundepu64_pd(W, U, A, B)   \\\n-    ((__m512d)__builtin_ia32_cvtuqq2pd512_mask((__v8di)(A), (W), (U), (B)))\n+    ((__m512d)__builtin_ia32_cvtuqq2pd512_mask ((__v8di)(A), (W), (U), (B)))\n \n #define _mm512_maskz_cvt_roundepu64_pd(U, A, B)     \\\n-    ((__m512d)__builtin_ia32_cvtuqq2pd512_mask((__v8di)(A), (__v8df)_mm512_setzero_pd(), (U), (B)))\n+    ((__m512d)__builtin_ia32_cvtuqq2pd512_mask ((__v8di)(A), (__v8df)_mm512_setzero_pd (), (U), (B)))\n \n #define _mm512_reduce_pd(A, B)\t\t\t\t\t\t\\\n   ((__m512d) __builtin_ia32_reducepd512_mask ((__v8df)(__m512d)(A),\t\\\n-    (int)(B), (__v8df)_mm512_setzero_pd(), (__mmask8)-1))\n+    (int)(B), (__v8df)_mm512_setzero_pd (), (__mmask8)-1))\n \n #define _mm512_mask_reduce_pd(W, U, A, B)\t\t\t\t\\\n   ((__m512d) __builtin_ia32_reducepd512_mask ((__v8df)(__m512d)(A),\t\\\n     (int)(B), (__v8df)(__m512d)(W), (__mmask8)(U)))\n \n #define _mm512_maskz_reduce_pd(U, A, B)\t\t\t\t\t\\\n   ((__m512d) __builtin_ia32_reducepd512_mask ((__v8df)(__m512d)(A),\t\\\n-    (int)(B), (__v8df)_mm512_setzero_pd(), (__mmask8)(U)))\n+    (int)(B), (__v8df)_mm512_setzero_pd (), (__mmask8)(U)))\n \n #define _mm512_reduce_ps(A, B)\t\t\t\t\t\t\\\n   ((__m512) __builtin_ia32_reduceps512_mask ((__v16sf)(__m512)(A),\t\\\n-    (int)(B), (__v16sf)_mm512_setzero_ps(), (__mmask16)-1))\n+    (int)(B), (__v16sf)_mm512_setzero_ps (), (__mmask16)-1))\n \n #define _mm512_mask_reduce_ps(W, U, A, B)\t\t\t\t\\\n   ((__m512) __builtin_ia32_reduceps512_mask ((__v16sf)(__m512)(A),\t\\\n     (int)(B), (__v16sf)(__m512)(W), (__mmask16)(U)))\n \n #define _mm512_maskz_reduce_ps(U, A, B)\t\t\t\t\t\\\n   ((__m512) __builtin_ia32_reduceps512_mask ((__v16sf)(__m512)(A),\t\\\n-    (int)(B), (__v16sf)_mm512_setzero_ps(), (__mmask16)(U)))\n+    (int)(B), (__v16sf)_mm512_setzero_ps (), (__mmask16)(U)))\n \n #define _mm512_extractf32x8_ps(X, C)                                    \\\n   ((__m256) __builtin_ia32_extractf32x8_mask ((__v16sf)(__m512) (X),    \\\n-    (int) (C), (__v8sf)(__m256) _mm256_setzero_ps(), (__mmask8)-1))\n+    (int) (C), (__v8sf)(__m256) _mm256_setzero_ps (), (__mmask8)-1))\n \n #define _mm512_mask_extractf32x8_ps(W, U, X, C)                         \\\n   ((__m256) __builtin_ia32_extractf32x8_mask ((__v16sf)(__m512) (X),    \\\n     (int) (C), (__v8sf)(__m256) (W), (__mmask8) (U)))\n \n #define _mm512_maskz_extractf32x8_ps(U, X, C)                           \\\n   ((__m256) __builtin_ia32_extractf32x8_mask ((__v16sf)(__m512) (X),    \\\n-    (int) (C), (__v8sf)(__m256) _mm256_setzero_ps(), (__mmask8) (U)))\n+    (int) (C), (__v8sf)(__m256) _mm256_setzero_ps (), (__mmask8) (U)))\n \n #define _mm512_extractf64x2_pd(X, C)                                    \\\n   ((__m128d) __builtin_ia32_extractf64x2_512_mask ((__v8df)(__m512d) (X),\\\n-    (int) (C), (__v2df)(__m128d) _mm_setzero_pd(), (__mmask8)-1))\n+    (int) (C), (__v2df)(__m128d) _mm_setzero_pd (), (__mmask8)-1))\n \n #define _mm512_mask_extractf64x2_pd(W, U, X, C)                         \\\n   ((__m128d) __builtin_ia32_extractf64x2_512_mask ((__v8df)(__m512d) (X),\\\n     (int) (C), (__v2df)(__m128d) (W), (__mmask8) (U)))\n \n #define _mm512_maskz_extractf64x2_pd(U, X, C)                           \\\n   ((__m128d) __builtin_ia32_extractf64x2_512_mask ((__v8df)(__m512d) (X),\\\n-    (int) (C), (__v2df)(__m128d) _mm_setzero_pd(), (__mmask8) (U)))\n+    (int) (C), (__v2df)(__m128d) _mm_setzero_pd (), (__mmask8) (U)))\n \n #define _mm512_extracti32x8_epi32(X, C)                                 \\\n   ((__m256i) __builtin_ia32_extracti32x8_mask ((__v16si)(__m512i) (X),  \\\n-    (int) (C), (__v8si)(__m256i) _mm256_setzero_si256(), (__mmask8)-1))\n+    (int) (C), (__v8si)(__m256i) _mm256_setzero_si256 (), (__mmask8)-1))\n \n #define _mm512_mask_extracti32x8_epi32(W, U, X, C)                      \\\n   ((__m256i) __builtin_ia32_extracti32x8_mask ((__v16si)(__m512i) (X),  \\\n     (int) (C), (__v8si)(__m256i) (W), (__mmask8) (U)))\n \n #define _mm512_maskz_extracti32x8_epi32(U, X, C)                        \\\n   ((__m256i) __builtin_ia32_extracti32x8_mask ((__v16si)(__m512i) (X),  \\\n-    (int) (C), (__v8si)(__m256i) _mm256_setzero_si256(), (__mmask8) (U)))\n+    (int) (C), (__v8si)(__m256i) _mm256_setzero_si256 (), (__mmask8) (U)))\n \n #define _mm512_extracti64x2_epi64(X, C)                                 \\\n   ((__m128i) __builtin_ia32_extracti64x2_512_mask ((__v8di)(__m512i) (X),\\\n-    (int) (C), (__v2di)(__m128i) _mm_setzero_di(), (__mmask8)-1))\n+    (int) (C), (__v2di)(__m128i) _mm_setzero_si128 (), (__mmask8)-1))\n \n #define _mm512_mask_extracti64x2_epi64(W, U, X, C)                      \\\n   ((__m128i) __builtin_ia32_extracti64x2_512_mask ((__v8di)(__m512i) (X),\\\n     (int) (C), (__v2di)(__m128i) (W), (__mmask8) (U)))\n \n #define _mm512_maskz_extracti64x2_epi64(U, X, C)                        \\\n   ((__m128i) __builtin_ia32_extracti64x2_512_mask ((__v8di)(__m512i) (X),\\\n-    (int) (C), (__v2di)(__m128i) _mm_setzero_di(), (__mmask8) (U)))\n+    (int) (C), (__v2di)(__m128i) _mm_setzero_si128 (), (__mmask8) (U)))\n \n #define _mm512_range_pd(A, B, C)\t\t\t\t\t\\\n   ((__m512d) __builtin_ia32_rangepd512_mask ((__v8df)(__m512d)(A),\t\\\n     (__v8df)(__m512d)(B), (int)(C),\t\t\t\t\t\\\n-    (__v8df)_mm512_setzero_pd(), (__mmask8)-1, _MM_FROUND_CUR_DIRECTION))\n+    (__v8df)_mm512_setzero_pd (), (__mmask8)-1, _MM_FROUND_CUR_DIRECTION))\n \n #define _mm512_mask_range_pd(W, U, A, B, C)\t\t\t\t\\\n   ((__m512d) __builtin_ia32_rangepd512_mask ((__v8df)(__m512d)(A),\t\\\n@@ -2138,12 +2144,12 @@ _mm512_fpclass_ps_mask (__m512 __A, const int __imm)\n #define _mm512_maskz_range_pd(U, A, B, C)\t\t\t\t\\\n   ((__m512d) __builtin_ia32_rangepd512_mask ((__v8df)(__m512d)(A),\t\\\n     (__v8df)(__m512d)(B), (int)(C),\t\t\t\t\t\\\n-    (__v8df)_mm512_setzero_pd(), (__mmask8)(U), _MM_FROUND_CUR_DIRECTION))\n+    (__v8df)_mm512_setzero_pd (), (__mmask8)(U), _MM_FROUND_CUR_DIRECTION))\n \n #define _mm512_range_ps(A, B, C)\t\t\t\t\t\\\n   ((__m512) __builtin_ia32_rangeps512_mask ((__v16sf)(__m512)(A),\t\\\n     (__v16sf)(__m512)(B), (int)(C),\t\t\t\t\t\\\n-    (__v16sf)_mm512_setzero_ps(), (__mmask16)-1, _MM_FROUND_CUR_DIRECTION))\n+    (__v16sf)_mm512_setzero_ps (), (__mmask16)-1, _MM_FROUND_CUR_DIRECTION))\n \n #define _mm512_mask_range_ps(W, U, A, B, C)\t\t\t\t\\\n   ((__m512) __builtin_ia32_rangeps512_mask ((__v16sf)(__m512)(A),\t\\\n@@ -2153,12 +2159,12 @@ _mm512_fpclass_ps_mask (__m512 __A, const int __imm)\n #define _mm512_maskz_range_ps(U, A, B, C)\t\t\t\t\\\n   ((__m512) __builtin_ia32_rangeps512_mask ((__v16sf)(__m512)(A),\t\\\n     (__v16sf)(__m512)(B), (int)(C),\t\t\t\t\t\\\n-    (__v16sf)_mm512_setzero_ps(), (__mmask16)(U), _MM_FROUND_CUR_DIRECTION))\n+    (__v16sf)_mm512_setzero_ps (), (__mmask16)(U), _MM_FROUND_CUR_DIRECTION))\n \n #define _mm512_range_round_pd(A, B, C, R)\t\t\t\t\t\\\n   ((__m512d) __builtin_ia32_rangepd512_mask ((__v8df)(__m512d)(A),\t\\\n     (__v8df)(__m512d)(B), (int)(C),\t\t\t\t\t\\\n-    (__v8df)_mm512_setzero_pd(), (__mmask8)-1, (R)))\n+    (__v8df)_mm512_setzero_pd (), (__mmask8)-1, (R)))\n \n #define _mm512_mask_range_round_pd(W, U, A, B, C, R)\t\t\t\t\\\n   ((__m512d) __builtin_ia32_rangepd512_mask ((__v8df)(__m512d)(A),\t\\\n@@ -2168,12 +2174,12 @@ _mm512_fpclass_ps_mask (__m512 __A, const int __imm)\n #define _mm512_maskz_range_round_pd(U, A, B, C, R)\t\t\t\t\\\n   ((__m512d) __builtin_ia32_rangepd512_mask ((__v8df)(__m512d)(A),\t\\\n     (__v8df)(__m512d)(B), (int)(C),\t\t\t\t\t\\\n-    (__v8df)_mm512_setzero_pd(), (__mmask8)(U), (R)))\n+    (__v8df)_mm512_setzero_pd (), (__mmask8)(U), (R)))\n \n #define _mm512_range_round_ps(A, B, C, R)\t\t\t\t\t\\\n   ((__m512) __builtin_ia32_rangeps512_mask ((__v16sf)(__m512)(A),\t\\\n     (__v16sf)(__m512)(B), (int)(C),\t\t\t\t\t\\\n-    (__v16sf)_mm512_setzero_ps(), (__mmask16)-1, (R)))\n+    (__v16sf)_mm512_setzero_ps (), (__mmask16)-1, (R)))\n \n #define _mm512_mask_range_round_ps(W, U, A, B, C, R)\t\t\t\t\\\n   ((__m512) __builtin_ia32_rangeps512_mask ((__v16sf)(__m512)(A),\t\\\n@@ -2183,7 +2189,7 @@ _mm512_fpclass_ps_mask (__m512 __A, const int __imm)\n #define _mm512_maskz_range_round_ps(U, A, B, C, R)\t\t\t\t\\\n   ((__m512) __builtin_ia32_rangeps512_mask ((__v16sf)(__m512)(A),\t\\\n     (__v16sf)(__m512)(B), (int)(C),\t\t\t\t\t\\\n-    (__v16sf)_mm512_setzero_ps(), (__mmask16)(U), (R)))\n+    (__v16sf)_mm512_setzero_ps (), (__mmask16)(U), (R)))\n \n #define _mm512_insertf64x2(X, Y, C)                                     \\\n   ((__m512d) __builtin_ia32_insertf64x2_512_mask ((__v8df)(__m512d) (X),\\\n@@ -2198,7 +2204,7 @@ _mm512_fpclass_ps_mask (__m512 __A, const int __imm)\n #define _mm512_maskz_insertf64x2(U, X, Y, C)                            \\\n   ((__m512d) __builtin_ia32_insertf64x2_512_mask ((__v8df)(__m512d) (X),\\\n     (__v2df)(__m128d) (Y), (int) (C),                                   \\\n-    (__v8df)(__m512d) _mm512_setzero_pd(), (__mmask8) (U)))\n+    (__v8df)(__m512d) _mm512_setzero_pd (), (__mmask8) (U)))\n \n #define _mm512_inserti64x2(X, Y, C)                                     \\\n   ((__m512i) __builtin_ia32_inserti64x2_512_mask ((__v8di)(__m512i) (X),\\\n@@ -2217,7 +2223,7 @@ _mm512_fpclass_ps_mask (__m512 __A, const int __imm)\n #define _mm512_insertf32x8(X, Y, C)                                     \\\n   ((__m512) __builtin_ia32_insertf32x8_mask ((__v16sf)(__m512) (X),     \\\n     (__v8sf)(__m256) (Y), (int) (C),\\\n-    (__v16sf)(__m512)_mm512_setzero_ps(),\\\n+    (__v16sf)(__m512)_mm512_setzero_ps (),\\\n     (__mmask16)-1))\n \n #define _mm512_mask_insertf32x8(W, U, X, Y, C)                          \\\n@@ -2229,7 +2235,7 @@ _mm512_fpclass_ps_mask (__m512 __A, const int __imm)\n #define _mm512_maskz_insertf32x8(U, X, Y, C)                            \\\n   ((__m512) __builtin_ia32_insertf32x8_mask ((__v16sf)(__m512) (X),     \\\n     (__v8sf)(__m256) (Y), (int) (C),\\\n-    (__v16sf)(__m512)_mm512_setzero_ps(),\\\n+    (__v16sf)(__m512)_mm512_setzero_ps (),\\\n     (__mmask16)(U)))\n \n #define _mm512_inserti32x8(X, Y, C)                                     \\"}, {"sha": "eb384d65a2d8d821656d3dbc5d7af8fd651a01cc", "filename": "gcc/config/i386/avx512vlbwintrin.h", "status": "modified", "additions": 28, "deletions": 28, "changes": 56, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a25a788762c63930b83858d03d5b30465f67aec7/gcc%2Fconfig%2Fi386%2Favx512vlbwintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a25a788762c63930b83858d03d5b30465f67aec7/gcc%2Fconfig%2Fi386%2Favx512vlbwintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Favx512vlbwintrin.h?ref=a25a788762c63930b83858d03d5b30465f67aec7", "patch": "@@ -69,7 +69,7 @@ _mm_maskz_mov_epi8 (__mmask16 __U, __m128i __A)\n {\n   return (__m128i) __builtin_ia32_movdquqi128_mask ((__v16qi) __A,\n \t\t\t\t\t\t    (__v16qi)\n-\t\t\t\t\t\t    _mm_setzero_hi (),\n+\t\t\t\t\t\t    _mm_setzero_si128 (),\n \t\t\t\t\t\t    (__mmask16) __U);\n }\n \n@@ -125,7 +125,7 @@ _mm_maskz_loadu_epi16 (__mmask8 __U, void const *__P)\n {\n   return (__m128i) __builtin_ia32_loaddquhi128_mask ((const short *) __P,\n \t\t\t\t\t\t     (__v8hi)\n-\t\t\t\t\t\t     _mm_setzero_hi (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n \n@@ -164,7 +164,7 @@ _mm_maskz_mov_epi16 (__mmask8 __U, __m128i __A)\n {\n   return (__m128i) __builtin_ia32_movdquhi128_mask ((__v8hi) __A,\n \t\t\t\t\t\t    (__v8hi)\n-\t\t\t\t\t\t    _mm_setzero_hi (),\n+\t\t\t\t\t\t    _mm_setzero_si128 (),\n \t\t\t\t\t\t    (__mmask8) __U);\n }\n \n@@ -202,7 +202,7 @@ _mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)\n {\n   return (__m128i) __builtin_ia32_loaddquqi128_mask ((const char *) __P,\n \t\t\t\t\t\t     (__v16qi)\n-\t\t\t\t\t\t     _mm_setzero_hi (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask16) __U);\n }\n \n@@ -541,7 +541,7 @@ _mm_permutexvar_epi16 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_permvarhi128_mask ((__v8hi) __B,\n \t\t\t\t\t\t     (__v8hi) __A,\n \t\t\t\t\t\t     (__v8hi)\n-\t\t\t\t\t\t     _mm_setzero_hi (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) -1);\n }\n \n@@ -707,7 +707,7 @@ _mm_maskz_maddubs_epi16 (__mmask8 __U, __m128i __X, __m128i __Y)\n   return (__m128i) __builtin_ia32_pmaddubsw128_mask ((__v16qi) __X,\n \t\t\t\t\t\t     (__v16qi) __Y,\n \t\t\t\t\t\t     (__v8hi)\n-\t\t\t\t\t\t     _mm_setzero_hi (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n \n@@ -908,7 +908,7 @@ _mm_maskz_min_epu16 (__mmask8 __M, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pminuw128_mask ((__v8hi) __A,\n \t\t\t\t\t\t  (__v8hi) __B,\n \t\t\t\t\t\t  (__v8hi)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __M);\n }\n \n@@ -974,7 +974,7 @@ _mm_maskz_max_epu8 (__mmask16 __M, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pmaxub128_mask ((__v16qi) __A,\n \t\t\t\t\t\t  (__v16qi) __B,\n \t\t\t\t\t\t  (__v16qi)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask16) __M);\n }\n \n@@ -1018,7 +1018,7 @@ _mm_maskz_max_epi8 (__mmask16 __M, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pmaxsb128_mask ((__v16qi) __A,\n \t\t\t\t\t\t  (__v16qi) __B,\n \t\t\t\t\t\t  (__v16qi)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask16) __M);\n }\n \n@@ -1062,7 +1062,7 @@ _mm_maskz_min_epu8 (__mmask16 __M, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pminub128_mask ((__v16qi) __A,\n \t\t\t\t\t\t  (__v16qi) __B,\n \t\t\t\t\t\t  (__v16qi)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask16) __M);\n }\n \n@@ -1106,7 +1106,7 @@ _mm_maskz_min_epi8 (__mmask16 __M, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pminsb128_mask ((__v16qi) __A,\n \t\t\t\t\t\t  (__v16qi) __B,\n \t\t\t\t\t\t  (__v16qi)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask16) __M);\n }\n \n@@ -1150,7 +1150,7 @@ _mm_maskz_max_epi16 (__mmask8 __M, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pmaxsw128_mask ((__v8hi) __A,\n \t\t\t\t\t\t  (__v8hi) __B,\n \t\t\t\t\t\t  (__v8hi)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __M);\n }\n \n@@ -1194,7 +1194,7 @@ _mm_maskz_max_epu16 (__mmask8 __M, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pmaxuw128_mask ((__v8hi) __A,\n \t\t\t\t\t\t  (__v8hi) __B,\n \t\t\t\t\t\t  (__v8hi)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __M);\n }\n \n@@ -1216,7 +1216,7 @@ _mm_maskz_min_epi16 (__mmask8 __M, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pminsw128_mask ((__v8hi) __A,\n \t\t\t\t\t\t  (__v8hi) __B,\n \t\t\t\t\t\t  (__v8hi)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __M);\n }\n \n@@ -1327,7 +1327,7 @@ _mm_dbsad_epu8 (__m128i __A, __m128i __B, const int __imm)\n \t\t\t\t\t\t    (__v16qi) __B,\n \t\t\t\t\t\t    __imm,\n \t\t\t\t\t\t    (__v8hi)\n-\t\t\t\t\t\t    _mm_setzero_hi (),\n+\t\t\t\t\t\t    _mm_setzero_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -1623,7 +1623,7 @@ _mm_maskz_shufflehi_epi16 (__mmask8 __U, __m128i __A, const int __imm)\n {\n   return (__m128i) __builtin_ia32_pshufhw128_mask ((__v8hi) __A, __imm,\n \t\t\t\t\t\t   (__v8hi)\n-\t\t\t\t\t\t   _mm_setzero_hi (),\n+\t\t\t\t\t\t   _mm_setzero_si128 (),\n \t\t\t\t\t\t   (__mmask8) __U);\n }\n \n@@ -1666,7 +1666,7 @@ _mm_maskz_shufflelo_epi16 (__mmask8 __U, __m128i __A, const int __imm)\n {\n   return (__m128i) __builtin_ia32_pshuflw128_mask ((__v8hi) __A, __imm,\n \t\t\t\t\t\t   (__v8hi)\n-\t\t\t\t\t\t   _mm_setzero_hi (),\n+\t\t\t\t\t\t   _mm_setzero_si128 (),\n \t\t\t\t\t\t   (__mmask8) __U);\n }\n \n@@ -1804,7 +1804,7 @@ _mm_maskz_slli_epi16 (__mmask8 __U, __m128i __A, int __B)\n \n #define _mm_maskz_shufflehi_epi16(U, A, B)                                          \\\n   ((__m128i) __builtin_ia32_pshufhw128_mask ((__v8hi)(__m128i)(A), (int)(B),        \\\n-                                             (__v8hi)(__m128i)_mm_setzero_hi(),     \\\n+\t\t\t\t\t     (__v8hi)(__m128i)_mm_setzero_si128 (), \\\n                                              (__mmask8)(U)))\n \n #define _mm256_mask_shufflelo_epi16(W, U, A, B)                                     \\\n@@ -1824,7 +1824,7 @@ _mm_maskz_slli_epi16 (__mmask8 __U, __m128i __A, int __B)\n \n #define _mm_maskz_shufflelo_epi16(U, A, B)                                          \\\n   ((__m128i) __builtin_ia32_pshuflw128_mask ((__v8hi)(__m128i)(A), (int)(B),        \\\n-                                             (__v8hi)(__m128i)_mm_setzero_hi(),     \\\n+\t\t\t\t\t     (__v8hi)(__m128i)_mm_setzero_si128 (), \\\n                                              (__mmask8)(U)))\n \n #define _mm256_maskz_alignr_epi8(U, X, Y, N)\t\t\t\t\t    \\\n@@ -1841,7 +1841,7 @@ _mm_maskz_slli_epi16 (__mmask8 __U, __m128i __A, int __B)\n #define _mm_maskz_alignr_epi8(U, X, Y, N)\t\t\t\t\t    \\\n   ((__m128i) __builtin_ia32_palignr128_mask ((__v2di)(__m128i)(X),\t\t    \\\n \t\t\t\t\t    (__v2di)(__m128i)(Y), (int)(N * 8),\t    \\\n-\t\t\t\t\t    (__v2di)(__m128i)_mm_setzero_di(),\t    \\\n+\t\t\t\t\t    (__v2di)(__m128i)_mm_setzero_si128 (),  \\\n \t\t\t\t\t    (__mmask16)(U)))\n \n #define _mm_mask_slli_epi16(W, U, X, C)\t\t\t\t\t  \\\n@@ -1851,7 +1851,7 @@ _mm_maskz_slli_epi16 (__mmask8 __U, __m128i __A, int __B)\n \n #define _mm_maskz_slli_epi16(U, X, C)\t\t\t\t\t  \\\n   ((__m128i)__builtin_ia32_psllwi128_mask ((__v8hi)(__m128i)(X), (int)(C),\\\n-    (__v8hi)(__m128i)_mm_setzero_hi(),\\\n+    (__v8hi)(__m128i)_mm_setzero_si128 (),\\\n     (__mmask8)(U)))\n \n #define _mm256_dbsad_epu8(X, Y, C)                                                  \\\n@@ -2301,7 +2301,7 @@ _mm_maskz_mulhi_epi16 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pmulhw128_mask ((__v8hi) __A,\n \t\t\t\t\t\t  (__v8hi) __B,\n \t\t\t\t\t\t  (__v8hi)\n-\t\t\t\t\t\t  _mm_setzero_hi (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n \n@@ -2323,7 +2323,7 @@ _mm_maskz_mulhi_epu16 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pmulhuw128_mask ((__v8hi) __A,\n \t\t\t\t\t\t   (__v8hi) __B,\n \t\t\t\t\t\t   (__v8hi)\n-\t\t\t\t\t\t   _mm_setzero_hi (),\n+\t\t\t\t\t\t   _mm_setzero_si128 (),\n \t\t\t\t\t\t   (__mmask8) __U);\n }\n \n@@ -2345,7 +2345,7 @@ _mm_maskz_mulhrs_epi16 (__mmask8 __U, __m128i __X, __m128i __Y)\n   return (__m128i) __builtin_ia32_pmulhrsw128_mask ((__v8hi) __X,\n \t\t\t\t\t\t    (__v8hi) __Y,\n \t\t\t\t\t\t    (__v8hi)\n-\t\t\t\t\t\t    _mm_setzero_hi (),\n+\t\t\t\t\t\t    _mm_setzero_si128 (),\n \t\t\t\t\t\t    (__mmask8) __U);\n }\n \n@@ -2389,7 +2389,7 @@ _mm_maskz_mullo_epi16 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pmullw128_mask ((__v8hi) __A,\n \t\t\t\t\t\t  (__v8hi) __B,\n \t\t\t\t\t\t  (__v8hi)\n-\t\t\t\t\t\t  _mm_setzero_hi (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n \n@@ -4067,7 +4067,7 @@ _mm_srav_epi16 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_psrav8hi_mask ((__v8hi) __A,\n \t\t\t\t\t\t (__v8hi) __B,\n \t\t\t\t\t\t (__v8hi)\n-\t\t\t\t\t\t _mm_setzero_hi (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) -1);\n }\n \n@@ -4133,7 +4133,7 @@ _mm_srlv_epi16 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_psrlv8hi_mask ((__v8hi) __A,\n \t\t\t\t\t\t (__v8hi) __B,\n \t\t\t\t\t\t (__v8hi)\n-\t\t\t\t\t\t _mm_setzero_hi (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) -1);\n }\n \n@@ -4199,7 +4199,7 @@ _mm_sllv_epi16 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_psllv8hi_mask ((__v8hi) __A,\n \t\t\t\t\t\t (__v8hi) __B,\n \t\t\t\t\t\t (__v8hi)\n-\t\t\t\t\t\t _mm_setzero_hi (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) -1);\n }\n "}, {"sha": "cd0b7143b46efcd85e1f0d6653aa9eae737b3511", "filename": "gcc/config/i386/avx512vldqintrin.h", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a25a788762c63930b83858d03d5b30465f67aec7/gcc%2Fconfig%2Fi386%2Favx512vldqintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a25a788762c63930b83858d03d5b30465f67aec7/gcc%2Fconfig%2Fi386%2Favx512vldqintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Favx512vldqintrin.h?ref=a25a788762c63930b83858d03d5b30465f67aec7", "patch": "@@ -69,7 +69,7 @@ _mm_cvttpd_epi64 (__m128d __A)\n {\n   return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,\n \t\t\t\t\t\t     (__v2di)\n-\t\t\t\t\t\t     _mm_setzero_di (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) -1);\n }\n \n@@ -127,7 +127,7 @@ _mm_cvttpd_epu64 (__m128d __A)\n {\n   return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,\n \t\t\t\t\t\t      (__v2di)\n-\t\t\t\t\t\t      _mm_setzero_di (),\n+\t\t\t\t\t\t      _mm_setzero_si128 (),\n \t\t\t\t\t\t      (__mmask8) -1);\n }\n \n@@ -185,7 +185,7 @@ _mm_cvtpd_epi64 (__m128d __A)\n {\n   return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,\n \t\t\t\t\t\t    (__v2di)\n-\t\t\t\t\t\t    _mm_setzero_di (),\n+\t\t\t\t\t\t    _mm_setzero_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -243,7 +243,7 @@ _mm_cvtpd_epu64 (__m128d __A)\n {\n   return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,\n \t\t\t\t\t\t     (__v2di)\n-\t\t\t\t\t\t     _mm_setzero_di (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) -1);\n }\n \n@@ -301,7 +301,7 @@ _mm_cvttps_epi64 (__m128 __A)\n {\n   return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,\n \t\t\t\t\t\t     (__v2di)\n-\t\t\t\t\t\t     _mm_setzero_di (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) -1);\n }\n \n@@ -320,7 +320,7 @@ _mm_maskz_cvttps_epi64 (__mmask8 __U, __m128 __A)\n {\n   return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,\n \t\t\t\t\t\t     (__v2di)\n-\t\t\t\t\t\t     _mm_setzero_di (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n \n@@ -359,7 +359,7 @@ _mm_cvttps_epu64 (__m128 __A)\n {\n   return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,\n \t\t\t\t\t\t      (__v2di)\n-\t\t\t\t\t\t      _mm_setzero_di (),\n+\t\t\t\t\t\t      _mm_setzero_si128 (),\n \t\t\t\t\t\t      (__mmask8) -1);\n }\n \n@@ -378,7 +378,7 @@ _mm_maskz_cvttps_epu64 (__mmask8 __U, __m128 __A)\n {\n   return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,\n \t\t\t\t\t\t      (__v2di)\n-\t\t\t\t\t\t      _mm_setzero_di (),\n+\t\t\t\t\t\t      _mm_setzero_si128 (),\n \t\t\t\t\t\t      (__mmask8) __U);\n }\n \n@@ -588,7 +588,7 @@ _mm_maskz_mullo_epi64 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pmullq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n \n@@ -714,7 +714,7 @@ _mm_cvtps_epi64 (__m128 __A)\n {\n   return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,\n \t\t\t\t\t\t    (__v2di)\n-\t\t\t\t\t\t    _mm_setzero_di (),\n+\t\t\t\t\t\t    _mm_setzero_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -733,7 +733,7 @@ _mm_maskz_cvtps_epi64 (__mmask8 __U, __m128 __A)\n {\n   return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,\n \t\t\t\t\t\t    (__v2di)\n-\t\t\t\t\t\t    _mm_setzero_di (),\n+\t\t\t\t\t\t    _mm_setzero_si128 (),\n \t\t\t\t\t\t    (__mmask8) __U);\n }\n \n@@ -772,7 +772,7 @@ _mm_cvtps_epu64 (__m128 __A)\n {\n   return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,\n \t\t\t\t\t\t     (__v2di)\n-\t\t\t\t\t\t     _mm_setzero_di (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) -1);\n }\n \n@@ -791,7 +791,7 @@ _mm_maskz_cvtps_epu64 (__mmask8 __U, __m128 __A)\n {\n   return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,\n \t\t\t\t\t\t     (__v2di)\n-\t\t\t\t\t\t     _mm_setzero_di (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n \n@@ -1381,7 +1381,7 @@ _mm256_extracti64x2_epi64 (__m256i __A, const int __imm)\n   return (__m128i) __builtin_ia32_extracti64x2_256_mask ((__v4di) __A,\n \t\t\t\t\t\t\t __imm,\n \t\t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t\t (__mmask8) -1);\n }\n \n@@ -1405,7 +1405,7 @@ _mm256_maskz_extracti64x2_epi64 (__mmask8 __U, __m256i __A,\n   return (__m128i) __builtin_ia32_extracti64x2_256_mask ((__v4di) __A,\n \t\t\t\t\t\t\t __imm,\n \t\t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t\t (__mmask8)\n \t\t\t\t\t\t\t __U);\n }\n@@ -1856,15 +1856,15 @@ _mm256_maskz_insertf64x2 (__mmask8 __U, __m256d __A, __m128d __B,\n \n #define _mm256_extracti64x2_epi64(X, C)                                 \\\n   ((__m128i) __builtin_ia32_extracti64x2_256_mask ((__v4di)(__m256i) (X),\\\n-    (int) (C), (__v2di)(__m128i) _mm_setzero_di(), (__mmask8)-1))\n+    (int) (C), (__v2di)(__m128i) _mm_setzero_si128 (), (__mmask8)-1))\n \n #define _mm256_mask_extracti64x2_epi64(W, U, X, C)                     \\\n   ((__m128i) __builtin_ia32_extracti64x2_256_mask ((__v4di)(__m256i) (X),\\\n     (int) (C), (__v2di)(__m128i) (W), (__mmask8) (U)))\n \n #define _mm256_maskz_extracti64x2_epi64(U, X, C)                        \\\n   ((__m128i) __builtin_ia32_extracti64x2_256_mask ((__v4di)(__m256i) (X),\\\n-    (int) (C), (__v2di)(__m128i) _mm_setzero_di(), (__mmask8) (U)))\n+    (int) (C), (__v2di)(__m128i) _mm_setzero_si128 (), (__mmask8) (U)))\n \n #define _mm256_reduce_pd(A, B)\t\t\t\t\t\t\\\n   ((__m256d) __builtin_ia32_reducepd256_mask ((__v4df)(__m256d)(A),\t\\"}, {"sha": "f83bfe25f192bba8490bc2f64c360a70de639c36", "filename": "gcc/config/i386/avx512vlintrin.h", "status": "modified", "additions": 182, "deletions": 163, "changes": 345, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a25a788762c63930b83858d03d5b30465f67aec7/gcc%2Fconfig%2Fi386%2Favx512vlintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a25a788762c63930b83858d03d5b30465f67aec7/gcc%2Fconfig%2Fi386%2Favx512vlintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Favx512vlintrin.h?ref=a25a788762c63930b83858d03d5b30465f67aec7", "patch": "@@ -28,14 +28,6 @@\n #ifndef _AVX512VLINTRIN_H_INCLUDED\n #define _AVX512VLINTRIN_H_INCLUDED\n \n-/* Doesn't require avx512vl target and is used in avx512dqintrin.h.  */\n-extern __inline __m128i\n-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n-_mm_setzero_di (void)\n-{\n-  return __extension__ (__m128i)(__v2di){ 0LL, 0LL};\n-}\n-\n #ifndef __AVX512VL__\n #pragma GCC push_options\n #pragma GCC target(\"avx512vl\")\n@@ -267,7 +259,7 @@ _mm_maskz_mov_epi64 (__mmask8 __U, __m128i __A)\n {\n   return (__m128i) __builtin_ia32_movdqa64_128_mask ((__v2di) __A,\n \t\t\t\t\t\t     (__v2di)\n-\t\t\t\t\t\t     _mm_setzero_di (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n \n@@ -308,7 +300,7 @@ _mm_maskz_load_epi64 (__mmask8 __U, void const *__P)\n {\n   return (__m128i) __builtin_ia32_movdqa64load128_mask ((__v2di *) __P,\n \t\t\t\t\t\t\t(__v2di)\n-\t\t\t\t\t\t\t_mm_setzero_di (),\n+\t\t\t\t\t\t\t_mm_setzero_si128 (),\n \t\t\t\t\t\t\t(__mmask8)\n \t\t\t\t\t\t\t__U);\n }\n@@ -429,15 +421,6 @@ _mm_mask_store_epi32 (void *__P, __mmask8 __U, __m128i __A)\n \t\t\t\t\t(__mmask8) __U);\n }\n \n-extern __inline __m128i\n-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n-_mm_setzero_hi (void)\n-{\n-  return __extension__ (__m128i) (__v8hi)\n-  {\n-  0, 0, 0, 0, 0, 0, 0, 0};\n-}\n-\n extern __inline __m128d\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_add_pd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)\n@@ -768,7 +751,7 @@ _mm_maskz_loadu_epi64 (__mmask8 __U, void const *__P)\n {\n   return (__m128i) __builtin_ia32_loaddqudi128_mask ((const long long *) __P,\n \t\t\t\t\t\t     (__v2di)\n-\t\t\t\t\t\t     _mm_setzero_di (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n \n@@ -919,7 +902,7 @@ _mm_abs_epi64 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pabsq128_mask ((__v2di) __A,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) -1);\n }\n \n@@ -938,7 +921,7 @@ _mm_maskz_abs_epi64 (__mmask8 __U, __m128i __A)\n {\n   return (__m128i) __builtin_ia32_pabsq128_mask ((__v2di) __A,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) __U);\n }\n \n@@ -1465,7 +1448,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi32_epi8 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,\n-\t\t\t\t\t\t  (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t  (__v16qi)\n+\t\t\t\t\t\t  _mm_undefined_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -1499,7 +1483,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtepi32_epi8 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,\n-\t\t\t\t\t\t  (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t  (__v16qi)\n+\t\t\t\t\t\t  _mm_undefined_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -1533,7 +1518,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsepi32_epi8 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,\n-\t\t\t\t\t\t   (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t   (__v16qi)\n+\t\t\t\t\t\t   _mm_undefined_si128 (),\n \t\t\t\t\t\t   (__mmask8) -1);\n }\n \n@@ -1567,7 +1553,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtsepi32_epi8 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,\n-\t\t\t\t\t\t   (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t   (__v16qi)\n+\t\t\t\t\t\t   _mm_undefined_si128 (),\n \t\t\t\t\t\t   (__mmask8) -1);\n }\n \n@@ -1601,7 +1588,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtusepi32_epi8 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,\n-\t\t\t\t\t\t    (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t    (__v16qi)\n+\t\t\t\t\t\t    _mm_undefined_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -1636,7 +1624,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtusepi32_epi8 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,\n-\t\t\t\t\t\t    (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t    (__v16qi)\n+\t\t\t\t\t\t    _mm_undefined_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -1671,7 +1660,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi32_epi16 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,\n-\t\t\t\t\t\t  (__v8hi) _mm_setzero_si128 (),\n+\t\t\t\t\t\t  (__v8hi)\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -1705,7 +1695,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtepi32_epi16 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,\n-\t\t\t\t\t\t  (__v8hi)_mm_setzero_si128 (),\n+\t\t\t\t\t\t  (__v8hi)\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -1739,7 +1730,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsepi32_epi16 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,\n-\t\t\t\t\t\t   (__v8hi)_mm_setzero_si128 (),\n+\t\t\t\t\t\t   (__v8hi)\n+\t\t\t\t\t\t   _mm_setzero_si128 (),\n \t\t\t\t\t\t   (__mmask8) -1);\n }\n \n@@ -1774,7 +1766,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtsepi32_epi16 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,\n-\t\t\t\t\t\t   (__v8hi)_mm_undefined_si128(),\n+\t\t\t\t\t\t   (__v8hi)\n+\t\t\t\t\t\t   _mm_undefined_si128 (),\n \t\t\t\t\t\t   (__mmask8) -1);\n }\n \n@@ -1808,7 +1801,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtusepi32_epi16 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,\n-\t\t\t\t\t\t    (__v8hi)_mm_undefined_si128(),\n+\t\t\t\t\t\t    (__v8hi)\n+\t\t\t\t\t\t    _mm_undefined_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -1842,7 +1836,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtusepi32_epi16 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,\n-\t\t\t\t\t\t    (__v8hi)_mm_undefined_si128(),\n+\t\t\t\t\t\t    (__v8hi)\n+\t\t\t\t\t\t    _mm_undefined_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -1876,7 +1871,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi64_epi8 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,\n-\t\t\t\t\t\t  (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t  (__v16qi)\n+\t\t\t\t\t\t  _mm_undefined_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -1910,7 +1906,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtepi64_epi8 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,\n-\t\t\t\t\t\t  (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t  (__v16qi)\n+\t\t\t\t\t\t  _mm_undefined_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -1944,7 +1941,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsepi64_epi8 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,\n-\t\t\t\t\t\t   (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t   (__v16qi)\n+\t\t\t\t\t\t   _mm_undefined_si128 (),\n \t\t\t\t\t\t   (__mmask8) -1);\n }\n \n@@ -1978,7 +1976,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtsepi64_epi8 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,\n-\t\t\t\t\t\t   (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t   (__v16qi)\n+\t\t\t\t\t\t   _mm_undefined_si128 (),\n \t\t\t\t\t\t   (__mmask8) -1);\n }\n \n@@ -2012,7 +2011,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtusepi64_epi8 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,\n-\t\t\t\t\t\t    (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t    (__v16qi)\n+\t\t\t\t\t\t    _mm_undefined_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -2047,7 +2047,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtusepi64_epi8 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,\n-\t\t\t\t\t\t    (__v16qi)_mm_undefined_si128(),\n+\t\t\t\t\t\t    (__v16qi)\n+\t\t\t\t\t\t    _mm_undefined_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -2082,7 +2083,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi64_epi16 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,\n-\t\t\t\t\t\t  (__v8hi)_mm_undefined_si128(),\n+\t\t\t\t\t\t  (__v8hi)\n+\t\t\t\t\t\t  _mm_undefined_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -2117,7 +2119,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtepi64_epi16 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,\n-\t\t\t\t\t\t  (__v8hi)_mm_undefined_si128(),\n+\t\t\t\t\t\t  (__v8hi)\n+\t\t\t\t\t\t  _mm_undefined_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -2151,7 +2154,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsepi64_epi16 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,\n-\t\t\t\t\t\t   (__v8hi)_mm_undefined_si128(),\n+\t\t\t\t\t\t   (__v8hi)\n+\t\t\t\t\t\t   _mm_undefined_si128 (),\n \t\t\t\t\t\t   (__mmask8) -1);\n }\n \n@@ -2185,7 +2189,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtsepi64_epi16 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,\n-\t\t\t\t\t\t   (__v8hi)_mm_undefined_si128(),\n+\t\t\t\t\t\t   (__v8hi)\n+\t\t\t\t\t\t   _mm_undefined_si128 (),\n \t\t\t\t\t\t   (__mmask8) -1);\n }\n \n@@ -2219,7 +2224,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtusepi64_epi16 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,\n-\t\t\t\t\t\t    (__v8hi)_mm_undefined_si128(),\n+\t\t\t\t\t\t    (__v8hi)\n+\t\t\t\t\t\t    _mm_undefined_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -2253,7 +2259,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtusepi64_epi16 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,\n-\t\t\t\t\t\t    (__v8hi)_mm_undefined_si128(),\n+\t\t\t\t\t\t    (__v8hi)\n+\t\t\t\t\t\t    _mm_undefined_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -2287,7 +2294,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi64_epi32 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,\n-\t\t\t\t\t\t  (__v4si)_mm_undefined_si128(),\n+\t\t\t\t\t\t  (__v4si)\n+\t\t\t\t\t\t  _mm_undefined_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -2321,7 +2329,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtepi64_epi32 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovqd256_mask ((__v4di) __A,\n-\t\t\t\t\t\t  (__v4si)_mm_undefined_si128(),\n+\t\t\t\t\t\t  (__v4si)\n+\t\t\t\t\t\t  _mm_undefined_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -2355,7 +2364,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsepi64_epi32 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,\n-\t\t\t\t\t\t   (__v4si)_mm_undefined_si128(),\n+\t\t\t\t\t\t   (__v4si)\n+\t\t\t\t\t\t   _mm_undefined_si128 (),\n \t\t\t\t\t\t   (__mmask8) -1);\n }\n \n@@ -2389,7 +2399,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtsepi64_epi32 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,\n-\t\t\t\t\t\t   (__v4si)_mm_undefined_si128(),\n+\t\t\t\t\t\t   (__v4si)\n+\t\t\t\t\t\t   _mm_undefined_si128 (),\n \t\t\t\t\t\t   (__mmask8) -1);\n }\n \n@@ -2424,7 +2435,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtusepi64_epi32 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,\n-\t\t\t\t\t\t    (__v4si)_mm_undefined_si128(),\n+\t\t\t\t\t\t    (__v4si)\n+\t\t\t\t\t\t    _mm_undefined_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -2458,7 +2470,8 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_cvtusepi64_epi32 (__m256i __A)\n {\n   return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,\n-\t\t\t\t\t\t    (__v4si)_mm_undefined_si128(),\n+\t\t\t\t\t\t    (__v4si)\n+\t\t\t\t\t\t    _mm_undefined_si128 (),\n \t\t\t\t\t\t    (__mmask8) -1);\n }\n \n@@ -2612,10 +2625,10 @@ extern __inline __m128i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_set1_epi32 (__mmask8 __M, int __A)\n {\n-  return (__m128i) __builtin_ia32_pbroadcastd128_gpr_mask (__A,\n-\t\t\t\t\t\t\t   (__v4si)\n-\t\t\t\t\t\t\t   _mm_setzero_si128 (),\n-\t\t\t\t\t\t\t   __M);\n+  return (__m128i)\n+\t __builtin_ia32_pbroadcastd128_gpr_mask (__A,\n+\t\t\t\t\t\t (__v4si) _mm_setzero_si128 (),\n+\t\t\t\t\t\t __M);\n }\n \n extern __inline __m256i\n@@ -2686,10 +2699,10 @@ extern __inline __m128i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_set1_epi64 (__mmask8 __M, long long __A)\n {\n-  return (__m128i) __builtin_ia32_pbroadcastq128_gpr_mask (__A,\n-\t\t\t\t\t\t\t   (__v2di)\n-\t\t\t\t\t\t\t   _mm_setzero_si128 (),\n-\t\t\t\t\t\t\t   __M);\n+  return (__m128i)\n+\t __builtin_ia32_pbroadcastq128_gpr_mask (__A,\n+\t\t\t\t\t\t (__v2di) _mm_setzero_si128 (),\n+\t\t\t\t\t\t __M);\n }\n \n extern __inline __m256\n@@ -3815,7 +3828,7 @@ _mm_maskz_srl_epi64 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_psrlq128_mask ((__v2di) __A,\n \t\t\t\t\t\t (__v2di) __B,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) __U);\n }\n \n@@ -5217,7 +5230,7 @@ _mm_maskz_unpackhi_epi64 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_punpckhqdq128_mask ((__v2di) __A,\n \t\t\t\t\t\t      (__v2di) __B,\n \t\t\t\t\t\t      (__v2di)\n-\t\t\t\t\t\t      _mm_setzero_di (),\n+\t\t\t\t\t\t      _mm_setzero_si128 (),\n \t\t\t\t\t\t      (__mmask8) __U);\n }\n \n@@ -5305,7 +5318,7 @@ _mm_maskz_unpacklo_epi64 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_punpcklqdq128_mask ((__v2di) __A,\n \t\t\t\t\t\t      (__v2di) __B,\n \t\t\t\t\t\t      (__v2di)\n-\t\t\t\t\t\t      _mm_setzero_di (),\n+\t\t\t\t\t\t      _mm_setzero_si128 (),\n \t\t\t\t\t\t      (__mmask8) __U);\n }\n \n@@ -5894,7 +5907,7 @@ _mm_maskz_compress_epi64 (__mmask8 __U, __m128i __A)\n {\n   return (__m128i) __builtin_ia32_compressdi128_mask ((__v2di) __A,\n \t\t\t\t\t\t      (__v2di)\n-\t\t\t\t\t\t      _mm_setzero_di (),\n+\t\t\t\t\t\t      _mm_setzero_si128 (),\n \t\t\t\t\t\t      (__mmask8) __U);\n }\n \n@@ -6678,7 +6691,7 @@ _mm_srav_epi64 (__m128i __X, __m128i __Y)\n   return (__m128i) __builtin_ia32_psravq128_mask ((__v2di) __X,\n \t\t\t\t\t\t  (__v2di) __Y,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -6700,7 +6713,7 @@ _mm_maskz_srav_epi64 (__mmask8 __U, __m128i __X, __m128i __Y)\n   return (__m128i) __builtin_ia32_psravq128_mask ((__v2di) __X,\n \t\t\t\t\t\t  (__v2di) __Y,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n \n@@ -6788,7 +6801,7 @@ _mm_maskz_sllv_epi64 (__mmask8 __U, __m128i __X, __m128i __Y)\n   return (__m128i) __builtin_ia32_psllv2di_mask ((__v2di) __X,\n \t\t\t\t\t\t (__v2di) __Y,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) __U);\n }\n \n@@ -6920,7 +6933,7 @@ _mm_maskz_srlv_epi64 (__mmask8 __U, __m128i __X, __m128i __Y)\n   return (__m128i) __builtin_ia32_psrlv2di_mask ((__v2di) __X,\n \t\t\t\t\t\t (__v2di) __Y,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) __U);\n }\n \n@@ -7096,7 +7109,7 @@ _mm_rolv_epi64 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_prolvq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -7118,7 +7131,7 @@ _mm_maskz_rolv_epi64 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_prolvq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n \n@@ -7162,7 +7175,7 @@ _mm_rorv_epi64 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_prorvq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -7184,7 +7197,7 @@ _mm_maskz_rorv_epi64 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_prorvq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n \n@@ -7972,7 +7985,7 @@ _mm_min_epi64 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pminsq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -8015,7 +8028,7 @@ _mm_max_epi64 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pmaxsq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -8026,7 +8039,7 @@ _mm_max_epu64 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pmaxuq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -8047,7 +8060,7 @@ _mm_min_epu64 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_pminuq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -8345,7 +8358,7 @@ _mm_lzcnt_epi64 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_vplzcntq_128_mask ((__v2di) __A,\n \t\t\t\t\t\t     (__v2di)\n-\t\t\t\t\t\t     _mm_setzero_di (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) -1);\n }\n \n@@ -8364,7 +8377,7 @@ _mm_maskz_lzcnt_epi64 (__mmask8 __U, __m128i __A)\n {\n   return (__m128i) __builtin_ia32_vplzcntq_128_mask ((__v2di) __A,\n \t\t\t\t\t\t     (__v2di)\n-\t\t\t\t\t\t     _mm_setzero_di (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n \n@@ -8374,7 +8387,7 @@ _mm_conflict_epi64 (__m128i __A)\n {\n   return (__m128i) __builtin_ia32_vpconflictdi_128_mask ((__v2di) __A,\n \t\t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t\t (__mmask8) -1);\n }\n \n@@ -8394,7 +8407,7 @@ _mm_maskz_conflict_epi64 (__mmask8 __U, __m128i __A)\n {\n   return (__m128i) __builtin_ia32_vpconflictdi_128_mask ((__v2di) __A,\n \t\t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t\t (__mmask8)\n \t\t\t\t\t\t\t __U);\n }\n@@ -8730,7 +8743,7 @@ _mm_sra_epi64 (__m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_psraq128_mask ((__v2di) __A,\n \t\t\t\t\t\t (__v2di) __B,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) -1);\n }\n \n@@ -8752,7 +8765,7 @@ _mm_maskz_sra_epi64 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_psraq128_mask ((__v2di) __A,\n \t\t\t\t\t\t (__v2di) __B,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) __U);\n }\n \n@@ -8796,7 +8809,7 @@ _mm_maskz_sll_epi64 (__mmask8 __U, __m128i __A, __m128i __B)\n   return (__m128i) __builtin_ia32_psllq128_mask ((__v2di) __A,\n \t\t\t\t\t\t (__v2di) __B,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) __U);\n }\n \n@@ -10923,7 +10936,7 @@ _mm_rol_epi64 (__m128i __A, const int __B)\n {\n   return (__m128i) __builtin_ia32_prolq128_mask ((__v2di) __A, __B,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) -1);\n }\n \n@@ -10943,7 +10956,7 @@ _mm_maskz_rol_epi64 (__mmask8 __U, __m128i __A, const int __B)\n {\n   return (__m128i) __builtin_ia32_prolq128_mask ((__v2di) __A, __B,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) __U);\n }\n \n@@ -10983,7 +10996,7 @@ _mm_ror_epi64 (__m128i __A, const int __B)\n {\n   return (__m128i) __builtin_ia32_prorq128_mask ((__v2di) __A, __B,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) -1);\n }\n \n@@ -11003,7 +11016,7 @@ _mm_maskz_ror_epi64 (__mmask8 __U, __m128i __A, const int __B)\n {\n   return (__m128i) __builtin_ia32_prorq128_mask ((__v2di) __A, __B,\n \t\t\t\t\t\t (__v2di)\n-\t\t\t\t\t\t _mm_setzero_di (),\n+\t\t\t\t\t\t _mm_setzero_si128 (),\n \t\t\t\t\t\t (__mmask8) __U);\n }\n \n@@ -11048,7 +11061,7 @@ _mm_alignr_epi64 (__m128i __A, __m128i __B, const int __imm)\n   return (__m128i) __builtin_ia32_alignq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B, __imm,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -11071,7 +11084,7 @@ _mm_maskz_alignr_epi64 (__mmask8 __U, __m128i __A, __m128i __B,\n   return (__m128i) __builtin_ia32_alignq128_mask ((__v2di) __A,\n \t\t\t\t\t\t  (__v2di) __B, __imm,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n \n@@ -11159,7 +11172,7 @@ _mm_maskz_cvtps_ph (__mmask8 __U, __m128 __A, const int __I)\n {\n   return (__m128i) __builtin_ia32_vcvtps2ph_mask ((__v4sf) __A, __I,\n \t\t\t\t\t\t  (__v8hi)\n-\t\t\t\t\t\t  _mm_setzero_hi (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n \n@@ -11179,7 +11192,7 @@ _mm256_maskz_cvtps_ph (__mmask8 __U, __m256 __A, const int __I)\n {\n   return (__m128i) __builtin_ia32_vcvtps2ph256_mask ((__v8sf) __A, __I,\n \t\t\t\t\t\t     (__v8hi)\n-\t\t\t\t\t\t     _mm_setzero_hi (),\n+\t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n \n@@ -11259,7 +11272,7 @@ _mm_srai_epi64 (__m128i __A, const int __imm)\n {\n   return (__m128i) __builtin_ia32_psraqi128_mask ((__v2di) __A, __imm,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) -1);\n }\n \n@@ -11317,7 +11330,7 @@ _mm_maskz_slli_epi64 (__mmask8 __U, __m128i __A, int __B)\n {\n   return (__m128i) __builtin_ia32_psllqi128_mask ((__v2di) __A, __B,\n \t\t\t\t\t\t  (__v2di)\n-\t\t\t\t\t\t  _mm_setzero_di (),\n+\t\t\t\t\t\t  _mm_setzero_si128 (),\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n \n@@ -12350,14 +12363,15 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #else\n #define _mm256_permutex_pd(X, M)\t\t\t\t\t\t\\\n   ((__m256d) __builtin_ia32_permdf256_mask ((__v4df)(__m256d)(X), (int)(M),\t\\\n-\t\t\t\t\t    (__v4df)(__m256d)_mm256_undefined_pd(),\\\n+\t\t\t\t\t    (__v4df)(__m256d)\t\t\t\\\n+\t\t\t\t\t    _mm256_undefined_pd (),\t\t\\\n \t\t\t\t\t    (__mmask8)-1))\n \n #define _mm256_maskz_permutex_epi64(M, X, I)                    \\\n   ((__m256i) __builtin_ia32_permdi256_mask ((__v4di)(__m256i)(X),    \\\n \t\t\t\t\t    (int)(I),                \\\n \t\t\t\t\t    (__v4di)(__m256i)        \\\n-\t\t\t\t\t    (_mm256_setzero_si256()),\\\n+\t\t\t\t\t    (_mm256_setzero_si256 ()),\\\n \t\t\t\t\t    (__mmask8)(M)))\n \n #define _mm256_mask_permutex_epi64(W, M, X, I)               \\\n@@ -12369,7 +12383,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_insertf32x4(X, Y, C)                                     \\\n   ((__m256) __builtin_ia32_insertf32x4_256_mask ((__v8sf)(__m256) (X),  \\\n     (__v4sf)(__m128) (Y), (int) (C),\t\t\t\t\t\\\n-    (__v8sf)(__m256)_mm256_setzero_ps(),\t\t\t\t\\\n+    (__v8sf)(__m256)_mm256_setzero_ps (),\t\t\t\t\\\n     (__mmask8)-1))\n \n #define _mm256_mask_insertf32x4(W, U, X, Y, C)                          \\\n@@ -12381,13 +12395,13 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_maskz_insertf32x4(U, X, Y, C)                            \\\n   ((__m256) __builtin_ia32_insertf32x4_256_mask ((__v8sf)(__m256) (X),\t\\\n     (__v4sf)(__m128) (Y), (int) (C),\t\t\t\t\t\\\n-    (__v8sf)(__m256)_mm256_setzero_ps(),\t\t\t\t\\\n+    (__v8sf)(__m256)_mm256_setzero_ps (),\t\t\t\t\\\n     (__mmask8)(U)))\n \n #define _mm256_inserti32x4(X, Y, C)                                     \\\n   ((__m256i) __builtin_ia32_inserti32x4_256_mask ((__v8si)(__m256i) (X),\\\n     (__v4si)(__m128i) (Y), (int) (C),\t\t\t\t\t\\\n-    (__v8si)(__m256i)_mm256_setzero_si256(),\t\t\t\t\\\n+    (__v8si)(__m256i)_mm256_setzero_si256 (),\t\t\t\t\\\n     (__mmask8)-1))\n \n #define _mm256_mask_inserti32x4(W, U, X, Y, C)                          \\\n@@ -12399,13 +12413,13 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_maskz_inserti32x4(U, X, Y, C)                            \\\n   ((__m256i) __builtin_ia32_inserti32x4_256_mask ((__v8si)(__m256i) (X),\\\n     (__v4si)(__m128i) (Y), (int) (C),\t\t\t\t\t\\\n-    (__v8si)(__m256i)_mm256_setzero_si256(),\t\t\t\t\\\n+    (__v8si)(__m256i)_mm256_setzero_si256 (),\t\t\t\t\\\n     (__mmask8)(U)))\n \n #define _mm256_extractf32x4_ps(X, C)                                    \\\n   ((__m128) __builtin_ia32_extractf32x4_256_mask ((__v8sf)(__m256) (X), \\\n     (int) (C),\t\t\t\t\t\t\t\t\\\n-    (__v4sf)(__m128)_mm_setzero_ps(),\t\t\t\t\t\\\n+    (__v4sf)(__m128)_mm_setzero_ps (),\t\t\t\t\t\\\n     (__mmask8)-1))\n \n #define _mm256_mask_extractf32x4_ps(W, U, X, C)                         \\\n@@ -12417,7 +12431,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_maskz_extractf32x4_ps(U, X, C)                           \\\n   ((__m128) __builtin_ia32_extractf32x4_256_mask ((__v8sf)(__m256) (X), \\\n     (int) (C),\t\t\t\t\t\t\t\t\\\n-    (__v4sf)(__m128)_mm_setzero_ps(),\t\t\t\t\t\\\n+    (__v4sf)(__m128)_mm_setzero_ps (),\t\t\t\t\t\\\n     (__mmask8)(U)))\n \n #define _mm256_extracti32x4_epi32(X, C)                                 \\\n@@ -12453,7 +12467,8 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_shuffle_i32x4(X, Y, C)                                                   \\\n   ((__m256i)  __builtin_ia32_shuf_i32x4_256_mask ((__v8si)(__m256i)(X),                 \\\n                                                   (__v8si)(__m256i)(Y), (int)(C),       \\\n-                                                  (__v8si)(__m256i)_mm256_setzero_si256(), \\\n+\t\t\t\t\t\t  (__v8si)(__m256i)\t\t\t\\\n+\t\t\t\t\t\t  _mm256_setzero_si256 (),\t\t\\\n                                                   (__mmask8)-1))\n \n #define _mm256_mask_shuffle_i32x4(W, U, X, Y, C)                                        \\\n@@ -12465,13 +12480,14 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_maskz_shuffle_i32x4(U, X, Y, C)                                          \\\n   ((__m256i)  __builtin_ia32_shuf_i32x4_256_mask ((__v8si)(__m256i)(X),                 \\\n                                                   (__v8si)(__m256i)(Y), (int)(C),       \\\n-                                                  (__v8si)(__m256i)_mm256_setzero_si256(), \\\n+\t\t\t\t\t\t  (__v8si)(__m256i)\t\t\t\\\n+\t\t\t\t\t\t  _mm256_setzero_si256 (),\t\t\\\n                                                   (__mmask8)(U)))\n \n #define _mm256_shuffle_f64x2(X, Y, C)                                                   \\\n   ((__m256d)  __builtin_ia32_shuf_f64x2_256_mask ((__v4df)(__m256d)(X),                 \\\n                                                   (__v4df)(__m256d)(Y), (int)(C),       \\\n-                                                  (__v4df)(__m256d)_mm256_setzero_pd(), \\\n+\t\t\t\t\t\t  (__v4df)(__m256d)_mm256_setzero_pd (),\\\n                                                   (__mmask8)-1))\n \n #define _mm256_mask_shuffle_f64x2(W, U, X, Y, C)                                        \\\n@@ -12483,13 +12499,13 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_maskz_shuffle_f64x2(U, X, Y, C)                                          \\\n   ((__m256d)  __builtin_ia32_shuf_f64x2_256_mask ((__v4df)(__m256d)(X),                 \\\n                                                   (__v4df)(__m256d)(Y), (int)(C),       \\\n-                                                  (__v4df)(__m256d)_mm256_setzero_pd(), \\\n+\t\t\t\t\t\t  (__v4df)(__m256d)_mm256_setzero_pd( ),\\\n                                                   (__mmask8)(U)))\n \n #define _mm256_shuffle_f32x4(X, Y, C)                                                   \\\n   ((__m256)  __builtin_ia32_shuf_f32x4_256_mask ((__v8sf)(__m256)(X),                   \\\n                                                  (__v8sf)(__m256)(Y), (int)(C),         \\\n-                                                 (__v8sf)(__m256)_mm256_setzero_ps(),   \\\n+\t\t\t\t\t\t (__v8sf)(__m256)_mm256_setzero_ps (),  \\\n                                                  (__mmask8)-1))\n \n #define _mm256_mask_shuffle_f32x4(W, U, X, Y, C)                                        \\\n@@ -12501,7 +12517,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_maskz_shuffle_f32x4(U, X, Y, C)                                          \\\n   ((__m256)  __builtin_ia32_shuf_f32x4_256_mask ((__v8sf)(__m256)(X),                   \\\n                                                  (__v8sf)(__m256)(Y), (int)(C),         \\\n-                                                 (__v8sf)(__m256)_mm256_setzero_ps(),   \\\n+\t\t\t\t\t\t (__v8sf)(__m256)_mm256_setzero_ps (),  \\\n                                                  (__mmask8)(U)))\n \n #define _mm256_mask_shuffle_pd(W, U, A, B, C)                                   \\\n@@ -12513,7 +12529,8 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_maskz_shuffle_pd(U, A, B, C)                                     \\\n   ((__m256d)__builtin_ia32_shufpd256_mask ((__v4df)(__m256d)(A),                \\\n                                            (__v4df)(__m256d)(B), (int)(C),      \\\n-                                           (__v4df)(__m256d)_mm256_setzero_pd(),\\\n+\t\t\t\t\t   (__v4df)(__m256d)\t\t\t\\\n+\t\t\t\t\t   _mm256_setzero_pd (),\t\t\\\n                                            (__mmask8)(U)))\n \n #define _mm_mask_shuffle_pd(W, U, A, B, C)                                      \\\n@@ -12525,7 +12542,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm_maskz_shuffle_pd(U, A, B, C)                                        \\\n   ((__m128d)__builtin_ia32_shufpd128_mask ((__v2df)(__m128d)(A),                \\\n                                            (__v2df)(__m128d)(B), (int)(C),      \\\n-                                           (__v2df)(__m128d)_mm_setzero_pd(),   \\\n+\t\t\t\t\t   (__v2df)(__m128d)_mm_setzero_pd (),  \\\n                                            (__mmask8)(U)))\n \n #define _mm256_mask_shuffle_ps(W, U, A, B, C)                                   \\\n@@ -12537,7 +12554,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_maskz_shuffle_ps(U, A, B, C)                                     \\\n   ((__m256) __builtin_ia32_shufps256_mask ((__v8sf)(__m256)(A),                 \\\n                                            (__v8sf)(__m256)(B), (int)(C),       \\\n-                                           (__v8sf)(__m256)_mm256_setzero_ps(), \\\n+\t\t\t\t\t   (__v8sf)(__m256)_mm256_setzero_ps (),\\\n                                            (__mmask8)(U)))\n \n #define _mm_mask_shuffle_ps(W, U, A, B, C)                                      \\\n@@ -12549,7 +12566,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm_maskz_shuffle_ps(U, A, B, C)                                        \\\n   ((__m128) __builtin_ia32_shufps128_mask ((__v4sf)(__m128)(A),                 \\\n                                            (__v4sf)(__m128)(B), (int)(C),       \\\n-                                           (__v4sf)(__m128)_mm_setzero_ps(),    \\\n+\t\t\t\t\t   (__v4sf)(__m128)_mm_setzero_ps (),   \\\n                                            (__mmask8)(U)))\n \n #define _mm256_fixupimm_pd(X, Y, Z, C)                                          \\\n@@ -12632,15 +12649,15 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm256_maskz_srli_epi32(U, A, B)\t\t\t\t\\\n   ((__m256i) __builtin_ia32_psrldi256_mask ((__v8si)(__m256i)(A),\t\\\n-    (int)(B), (__v8si)_mm256_setzero_si256(), (__mmask8)(U)))\n+    (int)(B), (__v8si)_mm256_setzero_si256 (), (__mmask8)(U)))\n \n #define _mm_mask_srli_epi32(W, U, A, B)                                 \\\n   ((__m128i) __builtin_ia32_psrldi128_mask ((__v4si)(__m128i)(A),       \\\n     (int)(B), (__v4si)(__m128i)(W), (__mmask8)(U)))\n \n #define _mm_maskz_srli_epi32(U, A, B)                                   \\\n   ((__m128i) __builtin_ia32_psrldi128_mask ((__v4si)(__m128i)(A),       \\\n-    (int)(B), (__v4si)_mm_setzero_si128(), (__mmask8)(U)))\n+    (int)(B), (__v4si)_mm_setzero_si128 (), (__mmask8)(U)))\n \n #define _mm256_mask_srli_epi64(W, U, A, B)\t\t\t\t\\\n   ((__m256i) __builtin_ia32_psrlqi256_mask ((__v4di)(__m256i)(A),\t\\\n@@ -12656,26 +12673,26 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_srli_epi64(U, A, B)                                   \\\n   ((__m128i) __builtin_ia32_psrlqi128_mask ((__v2di)(__m128i)(A),       \\\n-    (int)(B), (__v2di)_mm_setzero_si128(), (__mmask8)(U)))\n+    (int)(B), (__v2di)_mm_setzero_si128 (), (__mmask8)(U)))\n \n #define _mm256_mask_slli_epi32(W, U, X, C)                                \\\n   ((__m256i)__builtin_ia32_pslldi256_mask ((__v8si)(__m256i)(X), (int)(C),\\\n-    (__v8si)(__m256i)(W),\\\n+    (__v8si)(__m256i)(W),\t\t\t\t\t\t  \\\n     (__mmask8)(U)))\n \n #define _mm256_maskz_slli_epi32(U, X, C)                                  \\\n   ((__m256i)__builtin_ia32_pslldi256_mask ((__v8si)(__m256i)(X), (int)(C),\\\n-    (__v8si)(__m256i)_mm256_setzero_si256(),\\\n+    (__v8si)(__m256i)_mm256_setzero_si256 (),\t\t\t\t  \\\n     (__mmask8)(U)))\n \n #define _mm256_mask_slli_epi64(W, U, X, C)                                \\\n   ((__m256i)__builtin_ia32_psllqi256_mask ((__v4di)(__m256i)(X), (int)(C),\\\n-    (__v4di)(__m256i)(W),\\\n+    (__v4di)(__m256i)(W),\t\t\t\t\t\t  \\\n     (__mmask8)(U)))\n \n #define _mm256_maskz_slli_epi64(U, X, C)                                  \\\n   ((__m256i)__builtin_ia32_psllqi256_mask ((__v4di)(__m256i)(X), (int)(C),\\\n-    (__v4di)(__m256i)_mm256_setzero_si256 (),\\\n+    (__v4di)(__m256i)_mm256_setzero_si256 (),\t\t\t\t  \\\n     (__mmask8)(U)))\n \n #define _mm_mask_slli_epi32(W, U, X, C)\t\t\t\t\t  \\\n@@ -12695,7 +12712,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_slli_epi64(U, X, C)\t\t\t\t\t  \\\n   ((__m128i)__builtin_ia32_psllqi128_mask ((__v2di)(__m128i)(X), (int)(C),\\\n-    (__v2di)(__m128i)_mm_setzero_di(),\\\n+    (__v2di)(__m128i)_mm_setzero_si128 (),\\\n     (__mmask8)(U)))\n \n #define _mm256_ternarylogic_epi64(A, B, C, I)                           \\\n@@ -12748,56 +12765,56 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm256_roundscale_ps(A, B)\t\t\t\t        \\\n   ((__m256) __builtin_ia32_rndscaleps_256_mask ((__v8sf)(__m256)(A),    \\\n-    (int)(B), (__v8sf)(__m256)_mm256_setzero_ps(), (__mmask8)-1))\n+    (int)(B), (__v8sf)(__m256)_mm256_setzero_ps (), (__mmask8)-1))\n \n #define _mm256_mask_roundscale_ps(W, U, A, B)\t\t\t        \\\n   ((__m256) __builtin_ia32_rndscaleps_256_mask ((__v8sf)(__m256)(A),    \\\n     (int)(B), (__v8sf)(__m256)(W), (__mmask8)(U)))\n \n #define _mm256_maskz_roundscale_ps(U, A, B)\t\t\t        \\\n   ((__m256) __builtin_ia32_rndscaleps_256_mask ((__v8sf)(__m256)(A),    \\\n-    (int)(B), (__v8sf)(__m256)_mm256_setzero_ps(), (__mmask8)(U)))\n+    (int)(B), (__v8sf)(__m256)_mm256_setzero_ps (), (__mmask8)(U)))\n \n #define _mm256_roundscale_pd(A, B)\t\t\t\t        \\\n   ((__m256d) __builtin_ia32_rndscalepd_256_mask ((__v4df)(__m256d)(A),  \\\n-    (int)(B), (__v4df)(__m256d)_mm256_setzero_pd(), (__mmask8)-1))\n+    (int)(B), (__v4df)(__m256d)_mm256_setzero_pd (), (__mmask8)-1))\n \n #define _mm256_mask_roundscale_pd(W, U, A, B)\t\t\t        \\\n   ((__m256d) __builtin_ia32_rndscalepd_256_mask ((__v4df)(__m256d)(A),  \\\n     (int)(B), (__v4df)(__m256d)(W), (__mmask8)(U)))\n \n #define _mm256_maskz_roundscale_pd(U, A, B)\t\t\t        \\\n   ((__m256d) __builtin_ia32_rndscalepd_256_mask ((__v4df)(__m256d)(A),  \\\n-    (int)(B), (__v4df)(__m256d)_mm256_setzero_pd(), (__mmask8)(U)))\n+    (int)(B), (__v4df)(__m256d)_mm256_setzero_pd (), (__mmask8)(U)))\n \n #define _mm_roundscale_ps(A, B)\t\t\t\t\t        \\\n   ((__m128) __builtin_ia32_rndscaleps_128_mask ((__v4sf)(__m128)(A),    \\\n-    (int)(B), (__v4sf)(__m128)_mm_setzero_ps(), (__mmask8)-1))\n+    (int)(B), (__v4sf)(__m128)_mm_setzero_ps (), (__mmask8)-1))\n \n #define _mm_mask_roundscale_ps(W, U, A, B)\t\t\t        \\\n   ((__m128) __builtin_ia32_rndscaleps_128_mask ((__v4sf)(__m128)(A),    \\\n     (int)(B), (__v4sf)(__m128)(W), (__mmask8)(U)))\n \n #define _mm_maskz_roundscale_ps(U, A, B)\t\t\t        \\\n   ((__m128) __builtin_ia32_rndscaleps_128_mask ((__v4sf)(__m128)(A),    \\\n-    (int)(B), (__v4sf)(__m128)_mm_setzero_ps(), (__mmask8)(U)))\n+    (int)(B), (__v4sf)(__m128)_mm_setzero_ps (), (__mmask8)(U)))\n \n #define _mm_roundscale_pd(A, B)\t\t\t\t\t        \\\n   ((__m128d) __builtin_ia32_rndscalepd_128_mask ((__v2df)(__m128d)(A),  \\\n-    (int)(B), (__v2df)(__m128d)_mm_setzero_pd(), (__mmask8)-1))\n+    (int)(B), (__v2df)(__m128d)_mm_setzero_pd (), (__mmask8)-1))\n \n #define _mm_mask_roundscale_pd(W, U, A, B)\t\t\t        \\\n   ((__m128d) __builtin_ia32_rndscalepd_128_mask ((__v2df)(__m128d)(A),  \\\n     (int)(B), (__v2df)(__m128d)(W), (__mmask8)(U)))\n \n #define _mm_maskz_roundscale_pd(U, A, B)\t\t\t        \\\n   ((__m128d) __builtin_ia32_rndscalepd_128_mask ((__v2df)(__m128d)(A),  \\\n-    (int)(B), (__v2df)(__m128d)_mm_setzero_pd(), (__mmask8)(U)))\n+    (int)(B), (__v2df)(__m128d)_mm_setzero_pd (), (__mmask8)(U)))\n \n #define _mm256_getmant_ps(X, B, C)                                              \\\n   ((__m256) __builtin_ia32_getmantps256_mask ((__v8sf)(__m256) (X),             \\\n                                          (int)(((C)<<2) | (B)),                 \\\n-                                         (__v8sf)(__m256)_mm256_setzero_ps(),   \\\n+\t\t\t\t\t (__v8sf)(__m256)_mm256_setzero_ps (),  \\\n                                          (__mmask8)-1))\n \n #define _mm256_mask_getmant_ps(W, U, X, B, C)                                   \\\n@@ -12809,13 +12826,13 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_maskz_getmant_ps(U, X, B, C)                                     \\\n   ((__m256) __builtin_ia32_getmantps256_mask ((__v8sf)(__m256) (X),             \\\n                                          (int)(((C)<<2) | (B)),                 \\\n-                                         (__v8sf)(__m256)_mm256_setzero_ps(),   \\\n+\t\t\t\t\t (__v8sf)(__m256)_mm256_setzero_ps (),  \\\n                                          (__mmask8)(U)))\n \n #define _mm_getmant_ps(X, B, C)                                                 \\\n   ((__m128) __builtin_ia32_getmantps128_mask ((__v4sf)(__m128) (X),             \\\n                                          (int)(((C)<<2) | (B)),                 \\\n-                                         (__v4sf)(__m128)_mm_setzero_ps(),      \\\n+\t\t\t\t\t (__v4sf)(__m128)_mm_setzero_ps (),     \\\n                                          (__mmask8)-1))\n \n #define _mm_mask_getmant_ps(W, U, X, B, C)                                      \\\n@@ -12827,13 +12844,13 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm_maskz_getmant_ps(U, X, B, C)                                        \\\n   ((__m128) __builtin_ia32_getmantps128_mask ((__v4sf)(__m128) (X),             \\\n                                          (int)(((C)<<2) | (B)),                 \\\n-                                         (__v4sf)(__m128)_mm_setzero_ps(),      \\\n+\t\t\t\t\t (__v4sf)(__m128)_mm_setzero_ps (),     \\\n                                          (__mmask8)(U)))\n \n #define _mm256_getmant_pd(X, B, C)                                              \\\n   ((__m256d) __builtin_ia32_getmantpd256_mask ((__v4df)(__m256d) (X),           \\\n                                          (int)(((C)<<2) | (B)),                 \\\n-                                          (__v4df)(__m256d)_mm256_setzero_pd(), \\\n+\t\t\t\t\t  (__v4df)(__m256d)_mm256_setzero_pd (),\\\n                                           (__mmask8)-1))\n \n #define _mm256_mask_getmant_pd(W, U, X, B, C)                                   \\\n@@ -12845,13 +12862,13 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm256_maskz_getmant_pd(U, X, B, C)                                     \\\n   ((__m256d) __builtin_ia32_getmantpd256_mask ((__v4df)(__m256d) (X),           \\\n                                          (int)(((C)<<2) | (B)),                 \\\n-                                          (__v4df)(__m256d)_mm256_setzero_pd(), \\\n+\t\t\t\t\t  (__v4df)(__m256d)_mm256_setzero_pd (),\\\n                                           (__mmask8)(U)))\n \n #define _mm_getmant_pd(X, B, C)                                                 \\\n   ((__m128d) __builtin_ia32_getmantpd128_mask ((__v2df)(__m128d) (X),           \\\n                                          (int)(((C)<<2) | (B)),                 \\\n-                                          (__v2df)(__m128d)_mm_setzero_pd(),    \\\n+\t\t\t\t\t  (__v2df)(__m128d)_mm_setzero_pd (),   \\\n                                           (__mmask8)-1))\n \n #define _mm_mask_getmant_pd(W, U, X, B, C)                                      \\\n@@ -12863,7 +12880,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n #define _mm_maskz_getmant_pd(U, X, B, C)                                        \\\n   ((__m128d) __builtin_ia32_getmantpd128_mask ((__v2df)(__m128d) (X),           \\\n                                          (int)(((C)<<2) | (B)),                 \\\n-                                          (__v2df)(__m128d)_mm_setzero_pd(),    \\\n+\t\t\t\t\t  (__v2df)(__m128d)_mm_setzero_pd (),   \\\n                                           (__mmask8)(U)))\n \n #define _mm256_mmask_i32gather_ps(V1OLD, MASK, INDEX, ADDR, SCALE)\t\\\n@@ -13129,7 +13146,8 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm256_maskz_shuffle_epi32(U, X, C)                                         \\\n   ((__m256i)  __builtin_ia32_pshufd256_mask ((__v8si)(__m256i)(X), (int)(C),        \\\n-                                             (__v8si)(__m256i)_mm256_setzero_si256(),  \\\n+\t\t\t\t\t     (__v8si)(__m256i)\t\t\t    \\\n+\t\t\t\t\t     _mm256_setzero_si256 (),\t\t    \\\n                                              (__mmask8)(U)))\n \n #define _mm_mask_shuffle_epi32(W, U, X, C)                                          \\\n@@ -13139,7 +13157,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_shuffle_epi32(U, X, C)                                            \\\n   ((__m128i)  __builtin_ia32_pshufd128_mask ((__v4si)(__m128i)(X), (int)(C),        \\\n-                                             (__v4si)(__m128i)_mm_setzero_si128 (),     \\\n+\t\t\t\t\t     (__v4si)(__m128i)_mm_setzero_si128 (), \\\n                                              (__mmask8)(U)))\n \n #define _mm256_rol_epi64(A, B)                                                 \\\n@@ -13159,7 +13177,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_rol_epi64(A, B)                                                    \\\n   ((__m128i)__builtin_ia32_prolq128_mask ((__v2di)(__m128i)(A), (int)(B),      \\\n-                                          (__v2di)(__m128i)_mm_setzero_di(),   \\\n+\t\t\t\t\t  (__v2di)(__m128i)_mm_setzero_si128 (),\\\n                                           (__mmask8)-1))\n \n #define _mm_mask_rol_epi64(W, U, A, B)                                         \\\n@@ -13169,7 +13187,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_rol_epi64(U, A, B)                                           \\\n   ((__m128i)__builtin_ia32_prolq128_mask ((__v2di)(__m128i)(A), (int)(B),      \\\n-                                          (__v2di)(__m128i)_mm_setzero_di(),   \\\n+\t\t\t\t\t  (__v2di)(__m128i)_mm_setzero_si128 (),\\\n                                           (__mmask8)(U)))\n \n #define _mm256_ror_epi64(A, B)                                                 \\\n@@ -13189,7 +13207,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_ror_epi64(A, B)                                                    \\\n   ((__m128i)__builtin_ia32_prorq128_mask ((__v2di)(__m128i)(A), (int)(B),      \\\n-                                          (__v2di)(__m128i)_mm_setzero_di(),   \\\n+\t\t\t\t\t  (__v2di)(__m128i)_mm_setzero_si128 (),\\\n                                           (__mmask8)-1))\n \n #define _mm_mask_ror_epi64(W, U, A, B)                                         \\\n@@ -13199,12 +13217,12 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_ror_epi64(U, A, B)                                           \\\n   ((__m128i)__builtin_ia32_prorq128_mask ((__v2di)(__m128i)(A), (int)(B),      \\\n-                                          (__v2di)(__m128i)_mm_setzero_di(),   \\\n+\t\t\t\t\t  (__v2di)(__m128i)_mm_setzero_si128 (),\\\n                                           (__mmask8)(U)))\n \n #define _mm256_rol_epi32(A, B)                                                 \\\n   ((__m256i)__builtin_ia32_prold256_mask ((__v8si)(__m256i)(A), (int)(B),      \\\n-                                          (__v8si)(__m256i)_mm256_setzero_si256(),\\\n+\t\t\t\t\t  (__v8si)(__m256i)_mm256_setzero_si256 (),\\\n                                           (__mmask8)-1))\n \n #define _mm256_mask_rol_epi32(W, U, A, B)                                      \\\n@@ -13214,12 +13232,12 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm256_maskz_rol_epi32(U, A, B)                                        \\\n   ((__m256i)__builtin_ia32_prold256_mask ((__v8si)(__m256i)(A), (int)(B),      \\\n-                                          (__v8si)(__m256i)_mm256_setzero_si256(),\\\n+\t\t\t\t\t  (__v8si)(__m256i)_mm256_setzero_si256 (),\\\n                                           (__mmask8)(U)))\n \n #define _mm_rol_epi32(A, B)                                                    \\\n   ((__m128i)__builtin_ia32_prold128_mask ((__v4si)(__m128i)(A), (int)(B),      \\\n-                                          (__v4si)(__m128i)_mm_setzero_si128 (),   \\\n+\t\t\t\t\t  (__v4si)(__m128i)_mm_setzero_si128 (),\\\n                                           (__mmask8)-1))\n \n #define _mm_mask_rol_epi32(W, U, A, B)                                         \\\n@@ -13229,12 +13247,12 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_rol_epi32(U, A, B)                                           \\\n   ((__m128i)__builtin_ia32_prold128_mask ((__v4si)(__m128i)(A), (int)(B),      \\\n-                                          (__v4si)(__m128i)_mm_setzero_si128 (),   \\\n+\t\t\t\t\t  (__v4si)(__m128i)_mm_setzero_si128 (),\\\n                                           (__mmask8)(U)))\n \n #define _mm256_ror_epi32(A, B)                                                 \\\n   ((__m256i)__builtin_ia32_prord256_mask ((__v8si)(__m256i)(A), (int)(B),      \\\n-                                          (__v8si)(__m256i)_mm256_setzero_si256(),\\\n+\t\t\t\t\t  (__v8si)(__m256i)_mm256_setzero_si256 (),\\\n                                           (__mmask8)-1))\n \n #define _mm256_mask_ror_epi32(W, U, A, B)                                      \\\n@@ -13244,12 +13262,13 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm256_maskz_ror_epi32(U, A, B)                                        \\\n   ((__m256i)__builtin_ia32_prord256_mask ((__v8si)(__m256i)(A), (int)(B),      \\\n-                                          (__v8si)(__m256i)_mm256_setzero_si256(),\\\n+\t\t\t\t\t  (__v8si)(__m256i)\t\t       \\\n+\t\t\t\t\t  _mm256_setzero_si256 (),\t       \\\n                                           (__mmask8)(U)))\n \n #define _mm_ror_epi32(A, B)                                                    \\\n   ((__m128i)__builtin_ia32_prord128_mask ((__v4si)(__m128i)(A), (int)(B),      \\\n-                                          (__v4si)(__m128i)_mm_setzero_si128 (),   \\\n+\t\t\t\t\t  (__v4si)(__m128i)_mm_setzero_si128 (),\\\n                                           (__mmask8)-1))\n \n #define _mm_mask_ror_epi32(W, U, A, B)                                         \\\n@@ -13259,7 +13278,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_ror_epi32(U, A, B)                                           \\\n   ((__m128i)__builtin_ia32_prord128_mask ((__v4si)(__m128i)(A), (int)(B),      \\\n-                                          (__v4si)(__m128i)_mm_setzero_si128 (),   \\\n+\t\t\t\t\t  (__v4si)(__m128i)_mm_setzero_si128 (),\\\n                                           (__mmask8)(U)))\n \n #define _mm256_alignr_epi32(X, Y, C)                                        \\\n@@ -13298,7 +13317,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_alignr_epi32(U, X, Y, C)                                  \\\n     ((__m128i)__builtin_ia32_alignd128_mask ((__v4si)(__m128i)(X),          \\\n-        (__v4si)(__m128i)(Y), (int)(C), (__v4si)(__m128i)_mm_setzero_si128(),\\\n+\t(__v4si)(__m128i)(Y), (int)(C), (__v4si)(__m128i)_mm_setzero_si128 (),\\\n         (__mmask8)(U)))\n \n #define _mm_alignr_epi64(X, Y, C)                                           \\\n@@ -13311,7 +13330,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_alignr_epi64(U, X, Y, C)                                  \\\n     ((__m128i)__builtin_ia32_alignq128_mask ((__v2di)(__m128i)(X),          \\\n-        (__v2di)(__m128i)(Y), (int)(C), (__v2di)(__m128i)_mm_setzero_si128(),\\\n+\t(__v2di)(__m128i)(Y), (int)(C), (__v2di)(__m128i)_mm_setzero_si128 (),\\\n         (__mmask8)(U)))\n \n #define _mm_mask_cvtps_ph(W, U, A, I)\t\t\t\t\t\t\\\n@@ -13320,31 +13339,31 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_cvtps_ph(U, A, I)\t\t\t\t\t\t\\\n   ((__m128i) __builtin_ia32_vcvtps2ph_mask ((__v4sf)(__m128) A, (int) (I),      \\\n-      (__v8hi)(__m128i) _mm_setzero_hi(), (__mmask8) (U)))\n+      (__v8hi)(__m128i) _mm_setzero_si128 (), (__mmask8) (U)))\n \n #define _mm256_mask_cvtps_ph(W, U, A, I)\t\t\t\t\t\\\n   ((__m128i) __builtin_ia32_vcvtps2ph256_mask ((__v8sf)(__m256) A, (int) (I),\t\\\n       (__v8hi)(__m128i) (W), (__mmask8) (U)))\n \n #define _mm256_maskz_cvtps_ph(U, A, I)\t\t\t\t\t\t\\\n   ((__m128i) __builtin_ia32_vcvtps2ph256_mask ((__v8sf)(__m256) A, (int) (I),   \\\n-      (__v8hi)(__m128i) _mm_setzero_hi(), (__mmask8) (U)))\n+      (__v8hi)(__m128i) _mm_setzero_si128 (), (__mmask8) (U)))\n \n #define _mm256_mask_srai_epi32(W, U, A, B)\t\t\t\t\\\n   ((__m256i) __builtin_ia32_psradi256_mask ((__v8si)(__m256i)(A),\t\\\n     (int)(B), (__v8si)(__m256i)(W), (__mmask8)(U)))\n \n #define _mm256_maskz_srai_epi32(U, A, B)\t\t\t\t\\\n   ((__m256i) __builtin_ia32_psradi256_mask ((__v8si)(__m256i)(A),\t\\\n-    (int)(B), (__v8si)_mm256_setzero_si256(), (__mmask8)(U)))\n+    (int)(B), (__v8si)_mm256_setzero_si256 (), (__mmask8)(U)))\n \n #define _mm_mask_srai_epi32(W, U, A, B)                                 \\\n   ((__m128i) __builtin_ia32_psradi128_mask ((__v4si)(__m128i)(A),       \\\n     (int)(B), (__v4si)(__m128i)(W), (__mmask8)(U)))\n \n #define _mm_maskz_srai_epi32(U, A, B)                                   \\\n   ((__m128i) __builtin_ia32_psradi128_mask ((__v4si)(__m128i)(A),       \\\n-    (int)(B), (__v4si)_mm_setzero_si128(), (__mmask8)(U)))\n+    (int)(B), (__v4si)_mm_setzero_si128 (), (__mmask8)(U)))\n \n #define _mm256_srai_epi64(A, B)\t\t\t\t\t\t\\\n   ((__m256i) __builtin_ia32_psraqi256_mask ((__v4di)(__m256i)(A),\t\\\n@@ -13360,23 +13379,23 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_srai_epi64(A, B)\t\t\t\t\t\t\\\n   ((__m128i) __builtin_ia32_psraqi128_mask ((__v2di)(__m128i)(A),       \\\n-    (int)(B), (__v2di)_mm_setzero_si128(), (__mmask8)-1))\n+    (int)(B), (__v2di)_mm_setzero_si128 (), (__mmask8)-1))\n \n #define _mm_mask_srai_epi64(W, U, A, B)                                 \\\n   ((__m128i) __builtin_ia32_psraqi128_mask ((__v2di)(__m128i)(A),       \\\n     (int)(B), (__v2di)(__m128i)(W), (__mmask8)(U)))\n \n #define _mm_maskz_srai_epi64(U, A, B)                                   \\\n   ((__m128i) __builtin_ia32_psraqi128_mask ((__v2di)(__m128i)(A),       \\\n-    (int)(B), (__v2di)_mm_setzero_si128(), (__mmask8)(U)))\n+    (int)(B), (__v2di)_mm_setzero_si128 (), (__mmask8)(U)))\n \n #define _mm256_mask_permutex_pd(W, U, A, B)                             \\\n   ((__m256d) __builtin_ia32_permdf256_mask ((__v4df)(__m256d)(A),       \\\n     (int)(B), (__v4df)(__m256d)(W), (__mmask8)(U)))\n \n #define _mm256_maskz_permutex_pd(U, A, B)\t\t\t\t\\\n   ((__m256d) __builtin_ia32_permdf256_mask ((__v4df)(__m256d)(A),       \\\n-    (int)(B), (__v4df)(__m256d)_mm256_setzero_pd(), (__mmask8)(U)))\n+    (int)(B), (__v4df)(__m256d)_mm256_setzero_pd (), (__mmask8)(U)))\n \n #define _mm256_mask_permute_pd(W, U, X, C)\t\t\t\t\t    \\\n   ((__m256d) __builtin_ia32_vpermilpd256_mask ((__v4df)(__m256d)(X), (int)(C),\t    \\\n@@ -13385,7 +13404,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm256_maskz_permute_pd(U, X, C)\t\t\t\t\t    \\\n   ((__m256d) __builtin_ia32_vpermilpd256_mask ((__v4df)(__m256d)(X), (int)(C),\t    \\\n-\t\t\t\t\t      (__v4df)(__m256d)_mm256_setzero_pd(), \\\n+\t\t\t\t\t      (__v4df)(__m256d)_mm256_setzero_pd (),\\\n \t\t\t\t\t      (__mmask8)(U)))\n \n #define _mm256_mask_permute_ps(W, U, X, C)\t\t\t\t\t    \\\n@@ -13394,7 +13413,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm256_maskz_permute_ps(U, X, C)\t\t\t\t\t    \\\n   ((__m256) __builtin_ia32_vpermilps256_mask ((__v8sf)(__m256)(X), (int)(C),\t    \\\n-\t\t\t\t\t      (__v8sf)(__m256)_mm256_setzero_ps(),  \\\n+\t\t\t\t\t      (__v8sf)(__m256)_mm256_setzero_ps (), \\\n \t\t\t\t\t      (__mmask8)(U)))\n \n #define _mm_mask_permute_pd(W, U, X, C)\t\t\t\t\t\t    \\\n@@ -13403,7 +13422,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_permute_pd(U, X, C)\t\t\t\t\t\t    \\\n   ((__m128d) __builtin_ia32_vpermilpd_mask ((__v2df)(__m128d)(X), (int)(C),\t    \\\n-\t\t\t\t\t    (__v2df)(__m128d)_mm_setzero_pd(),\t    \\\n+\t\t\t\t\t    (__v2df)(__m128d)_mm_setzero_pd (),\t    \\\n \t\t\t\t\t    (__mmask8)(U)))\n \n #define _mm_mask_permute_ps(W, U, X, C)\t\t\t\t\t\t    \\\n@@ -13412,7 +13431,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #define _mm_maskz_permute_ps(U, X, C)\t\t\t\t\t\t    \\\n   ((__m128) __builtin_ia32_vpermilps_mask ((__v4sf)(__m128)(X), (int)(C),\t    \\\n-\t\t\t\t\t  (__v4sf)(__m128)_mm_setzero_ps(),\t    \\\n+\t\t\t\t\t  (__v4sf)(__m128)_mm_setzero_ps (),\t    \\\n \t\t\t\t\t  (__mmask8)(U)))\n \n #define _mm256_mask_blend_pd(__U, __A, __W)\t\t\t      \\\n@@ -13577,7 +13596,7 @@ _mm_cmple_epi64_mask (__m128i __X, __m128i __Y)\n \n #endif\n \n-#define _mm256_permutexvar_ps(A, B)\t_mm256_permutevar8x32_ps((B), (A))\n+#define _mm256_permutexvar_ps(A, B)\t_mm256_permutevar8x32_ps ((B), (A))\n \n #ifdef __DISABLE_AVX512VL__\n #undef __DISABLE_AVX512VL__"}]}