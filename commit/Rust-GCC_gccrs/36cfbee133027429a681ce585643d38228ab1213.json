{"sha": "36cfbee133027429a681ce585643d38228ab1213", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MzZjZmJlZTEzMzAyNzQyOWE2ODFjZTU4NTY0M2QzODIyOGFiMTIxMw==", "commit": {"author": {"name": "Richard Henderson", "email": "rth@redhat.com", "date": "2011-12-13T19:11:25Z"}, "committer": {"name": "Richard Henderson", "email": "rth@gcc.gnu.org", "date": "2011-12-13T19:11:25Z"}, "message": "libitm: Conversion to c++11 atomics.\n\n        * local_atomic: New file.\n        * libitm_i.h: Include it.\n        (gtm_thread::shared_state): Use atomic template.\n        * beginend.cc (GTM::gtm_clock): Use atomic template.\n        (global_tid): Use atomic template if 64-bit atomics available.\n        (gtm_thread::gtm_thread): Update shared_state access.\n        (gtm_thread::trycommit): Likewise.\n        (choose_code_path): Update global_tid access.\n        * method-gl.cc (gl_mg::orec): Use atomic template.  Update all users.\n        * stmlock.h (GTM::gtm_clock): Use atomic template.\n        (gtm_get_clock, gtm_inc_clock): Update accesses.\n        * config/linux/rwlock.cc (gtm_rwlock::read_lock): Remove\n        redundant __sync_synchronize after atomic shared_state access.\n        * config/posix/rwlock.cc (gtm_rwlock::read_lock): Likewise.\n        (gtm_rwlock::write_lock_generic): Likewise.\n        (gtm_rwlock::read_unlock): Likewise.\n        * config/alpha/target.h (atomic_read_barrier): Remove.\n        (atomic_write_barrier): Remove.\n        * config/x86/target.h (atomic_read_barrier): Remove.\n        (atomic_write_barrier): Remove.\n\nFrom-SVN: r182294", "tree": {"sha": "7506d65c4a5b0a1a5cd4450e48e4943360f9ab19", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/7506d65c4a5b0a1a5cd4450e48e4943360f9ab19"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/36cfbee133027429a681ce585643d38228ab1213", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/36cfbee133027429a681ce585643d38228ab1213", "html_url": "https://github.com/Rust-GCC/gccrs/commit/36cfbee133027429a681ce585643d38228ab1213", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/36cfbee133027429a681ce585643d38228ab1213/comments", "author": null, "committer": null, "parents": [{"sha": "c36cc670b57fa6ebfcc387732fb7e34b7881eb14", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c36cc670b57fa6ebfcc387732fb7e34b7881eb14", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c36cc670b57fa6ebfcc387732fb7e34b7881eb14"}], "stats": {"total": 2098, "additions": 1996, "deletions": 102}, "files": [{"sha": "cfb959dd05cb6c90dbe4d68241b61e6d2792a9d8", "filename": "libitm/ChangeLog", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/36cfbee133027429a681ce585643d38228ab1213/libitm%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/36cfbee133027429a681ce585643d38228ab1213/libitm%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2FChangeLog?ref=36cfbee133027429a681ce585643d38228ab1213", "patch": "@@ -1,3 +1,26 @@\n+2011-12-13  Richard Henderson  <rth@redhat.com>\n+\n+\t* local_atomic: New file.\n+\t* libitm_i.h: Include it.\n+\t(gtm_thread::shared_state): Use atomic template.\n+\t* beginend.cc (GTM::gtm_clock): Use atomic template.\n+\t(global_tid): Use atomic template if 64-bit atomics available.\n+\t(gtm_thread::gtm_thread): Update shared_state access.\n+\t(gtm_thread::trycommit): Likewise.\n+\t(choose_code_path): Update global_tid access.\n+\t* method-gl.cc (gl_mg::orec): Use atomic template.  Update all users.\n+\t* stmlock.h (GTM::gtm_clock): Use atomic template.\n+\t(gtm_get_clock, gtm_inc_clock): Update accesses.\n+\t* config/linux/rwlock.cc (gtm_rwlock::read_lock): Remove\n+\tredundant __sync_synchronize after atomic shared_state access.\n+\t* config/posix/rwlock.cc (gtm_rwlock::read_lock): Likewise.\n+\t(gtm_rwlock::write_lock_generic): Likewise.\n+\t(gtm_rwlock::read_unlock): Likewise.\n+\t* config/alpha/target.h (atomic_read_barrier): Remove.\n+\t(atomic_write_barrier): Remove.\n+\t* config/x86/target.h (atomic_read_barrier): Remove.\n+\t(atomic_write_barrier): Remove.\n+\n 2011-11-30  Richard Henderson  <rth@redhat.com>\n \n \t* libitm_i.h (GTM_longjmp): Swap first and second arguments."}, {"sha": "bcc8516be05de2211bae45f03e1cb7a79eccc04b", "filename": "libitm/beginend.cc", "status": "modified", "additions": 12, "deletions": 11, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fbeginend.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fbeginend.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fbeginend.cc?ref=36cfbee133027429a681ce585643d38228ab1213", "patch": "@@ -37,12 +37,18 @@ gtm_thread *GTM::gtm_thread::list_of_threads = 0;\n unsigned GTM::gtm_thread::number_of_threads = 0;\n \n gtm_stmlock GTM::gtm_stmlock_array[LOCK_ARRAY_SIZE];\n-gtm_version GTM::gtm_clock;\n+atomic<gtm_version> GTM::gtm_clock;\n \n /* ??? Move elsewhere when we figure out library initialization.  */\n uint64_t GTM::gtm_spin_count_var = 1000;\n \n+#ifdef HAVE_64BIT_SYNC_BUILTINS\n+static atomic<_ITM_transactionId_t> global_tid;\n+#else\n static _ITM_transactionId_t global_tid;\n+static pthread_mutex_t global_tid_lock = PTHREAD_MUTEX_INITIALIZER;\n+#endif\n+\n \n // Provides a on-thread-exit callback used to release per-thread data.\n static pthread_key_t thr_release_key;\n@@ -114,7 +120,7 @@ GTM::gtm_thread::gtm_thread ()\n   // This object's memory has been set to zero by operator new, so no need\n   // to initialize any of the other primitive-type members that do not have\n   // constructors.\n-  shared_state = ~(typeof shared_state)0;\n+  shared_state.store(-1, memory_order_relaxed);\n \n   // Register this transaction with the list of all threads' transactions.\n   serial_lock.write_lock ();\n@@ -132,13 +138,8 @@ GTM::gtm_thread::gtm_thread ()\n     GTM_fatal(\"Setting thread release TLS key failed.\");\n }\n \n-\n-\n-#ifndef HAVE_64BIT_SYNC_BUILTINS\n-static pthread_mutex_t global_tid_lock = PTHREAD_MUTEX_INITIALIZER;\n-#endif\n-\n-static inline uint32_t choose_code_path(uint32_t prop, abi_dispatch *disp)\n+static inline uint32_t\n+choose_code_path(uint32_t prop, abi_dispatch *disp)\n {\n   if ((prop & pr_uninstrumentedCode) && disp->can_run_uninstrumented_code())\n     return a_runUninstrumentedCode;\n@@ -258,7 +259,7 @@ GTM::gtm_thread::begin_transaction (uint32_t prop, const gtm_jmpbuf *jb)\n   else\n     {\n #ifdef HAVE_64BIT_SYNC_BUILTINS\n-      tx->id = __sync_add_and_fetch (&global_tid, tid_block_size);\n+      tx->id = global_tid.fetch_add(tid_block_size, memory_order_relaxed);\n       tx->local_tid = tx->id + 1;\n #else\n       pthread_mutex_lock (&global_tid_lock);\n@@ -480,7 +481,7 @@ GTM::gtm_thread::trycommit ()\n \t      it = it->next_thread)\n \t    {\n \t      if (it == this) continue;\n-\t      while (it->shared_state < priv_time)\n+\t      while (it->shared_state.load(memory_order_relaxed) < priv_time)\n \t\tcpu_relax();\n \t    }\n \t}"}, {"sha": "12b1d898544e8ae8dc2150b4872b2bb9634986a8", "filename": "libitm/config/alpha/target.h", "status": "modified", "additions": 0, "deletions": 12, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fconfig%2Falpha%2Ftarget.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fconfig%2Falpha%2Ftarget.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Falpha%2Ftarget.h?ref=36cfbee133027429a681ce585643d38228ab1213", "patch": "@@ -45,16 +45,4 @@ cpu_relax (void)\n   __asm volatile (\"\" : : : \"memory\");\n }\n \n-static inline void\n-atomic_read_barrier (void)\n-{\n-  __sync_synchronize ();\n-}\n-\n-static inline void\n-atomic_write_barrier (void)\n-{\n-  __asm volatile (\"wmb\" : : : \"memory\");\n-}\n-\n } // namespace GTM"}, {"sha": "3471049083e03e3e1bee18ad6fa56eaac0e0adad", "filename": "libitm/config/linux/rwlock.cc", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fconfig%2Flinux%2Frwlock.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fconfig%2Flinux%2Frwlock.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Flinux%2Frwlock.cc?ref=36cfbee133027429a681ce585643d38228ab1213", "patch": "@@ -36,10 +36,9 @@ gtm_rwlock::read_lock (gtm_thread *tx)\n   for (;;)\n     {\n       // Fast path: first announce our intent to read, then check for\n-      // conflicting intents to write. The barrier makes sure that this\n-      // happens in exactly this order.\n+      // conflicting intents to write.  Note that direct assignment to\n+      // an atomic object is memory_order_seq_cst.\n       tx->shared_state = 0;\n-      __sync_synchronize();\n       if (likely(writers == 0))\n \treturn;\n \n@@ -51,8 +50,7 @@ gtm_rwlock::read_lock (gtm_thread *tx)\n       // We need the barrier here for the same reason that we need it in\n       // read_unlock().\n       // TODO Potentially too many wake-ups. See comments in read_unlock().\n-      tx->shared_state = ~(typeof tx->shared_state)0;\n-      __sync_synchronize();\n+      tx->shared_state = -1;\n       if (writer_readers > 0)\n \t{\n \t  writer_readers = 0;\n@@ -71,7 +69,7 @@ gtm_rwlock::read_lock (gtm_thread *tx)\n \t  // are no writers anymore after the barrier because this pending\n \t  // store could then lead to lost wake-ups at other readers.\n \t  readers = 1;\n-\t  __sync_synchronize();\n+\t  atomic_thread_fence(memory_order_acq_rel);\n \t  if (writers)\n \t    futex_wait(&readers, 1);\n \t}"}, {"sha": "e1e3dcf22d06f84c071676679a4e6edefbc7a8e8", "filename": "libitm/config/posix/rwlock.cc", "status": "modified", "additions": 8, "deletions": 10, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fconfig%2Fposix%2Frwlock.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fconfig%2Fposix%2Frwlock.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fposix%2Frwlock.cc?ref=36cfbee133027429a681ce585643d38228ab1213", "patch": "@@ -53,10 +53,9 @@ void\n gtm_rwlock::read_lock (gtm_thread *tx)\n {\n   // Fast path: first announce our intent to read, then check for conflicting\n-  // intents to write. The barrier makes sure that this happens in exactly\n-  // this order.\n+  // intents to write.  Note that direct assignment to an atomic object\n+  // is memory_order_seq_cst.\n   tx->shared_state = 0;\n-  __sync_synchronize();\n   unsigned int sum = this->summary;\n   if (likely(!(sum & (a_writer | w_writer))))\n     return;\n@@ -69,7 +68,7 @@ gtm_rwlock::read_lock (gtm_thread *tx)\n   // to happen before we leave the slow path and before we wait for any\n   // writer).\n   // ??? Add a barrier to enforce early visibility of this?\n-  tx->shared_state = ~(typeof tx->shared_state)0;\n+  tx->shared_state.store(-1, memory_order_relaxed);\n \n   pthread_mutex_lock (&this->mutex);\n \n@@ -101,7 +100,7 @@ gtm_rwlock::read_lock (gtm_thread *tx)\n     }\n \n   // Otherwise we can acquire the lock for read.\n-  tx->shared_state = 0;\n+  tx->shared_state.store(0, memory_order_relaxed);\n \n   pthread_mutex_unlock(&this->mutex);\n }\n@@ -153,11 +152,11 @@ gtm_rwlock::write_lock_generic (gtm_thread *tx)\n   // sure that we first set our write intent and check for active readers\n   // after that, in strictly this order (similar to the barrier in the fast\n   // path of read_lock()).\n-  __sync_synchronize();\n+  atomic_thread_fence(memory_order_acq_rel);\n \n   // If this is an upgrade, we are not a reader anymore.\n   if (tx != 0)\n-    tx->shared_state = ~(typeof tx->shared_state)0;\n+    tx->shared_state.store(-1, memory_order_relaxed);\n \n   // Count the number of active readers to be able to decrease the number of\n   // wake-ups and wait calls that are necessary.\n@@ -194,7 +193,7 @@ gtm_rwlock::write_lock_generic (gtm_thread *tx)\n \t  it = it->next_thread)\n \t{\n \t  // Don't count ourself if this is an upgrade.\n-\t  if (it->shared_state != ~(typeof it->shared_state)0)\n+\t  if (it->shared_state.load(memory_order_relaxed) != -1)\n \t    readers++;\n \t}\n \n@@ -236,8 +235,7 @@ gtm_rwlock::write_upgrade (gtm_thread *tx)\n void\n gtm_rwlock::read_unlock (gtm_thread *tx)\n {\n-  tx->shared_state = ~(typeof tx->shared_state)0;\n-  __sync_synchronize();\n+  tx->shared_state = -1;\n   unsigned int sum = this->summary;\n   if (likely(!(sum & (a_writer | w_writer))))\n     return;"}, {"sha": "a59608f388ca65b8ca19f1d4b84fcd80a6fab32b", "filename": "libitm/config/x86/target.h", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fconfig%2Fx86%2Ftarget.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fconfig%2Fx86%2Ftarget.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fx86%2Ftarget.h?ref=36cfbee133027429a681ce585643d38228ab1213", "patch": "@@ -66,20 +66,6 @@ cpu_relax (void)\n   __asm volatile (\"rep; nop\" : : : \"memory\");\n }\n \n-static inline void\n-atomic_read_barrier (void)\n-{\n-  /* x86 is a strong memory ordering machine.  */\n-  __asm volatile (\"\" : : : \"memory\");\n-}\n-\n-static inline void\n-atomic_write_barrier (void)\n-{\n-  /* x86 is a strong memory ordering machine.  */\n-  __asm volatile (\"\" : : : \"memory\");\n-}\n-\n } // namespace GTM\n \n // We'll be using some of the cpu builtins, and their associated types."}, {"sha": "d57872ede7bef1c713fa7bb1676109603d3a7db2", "filename": "libitm/libitm_i.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/36cfbee133027429a681ce585643d38228ab1213/libitm%2Flibitm_i.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/36cfbee133027429a681ce585643d38228ab1213/libitm%2Flibitm_i.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Flibitm_i.h?ref=36cfbee133027429a681ce585643d38228ab1213", "patch": "@@ -37,6 +37,7 @@\n #include <string.h>\n #include <unwind.h>\n #include \"local_type_traits\"\n+#include \"local_atomic\"\n \n #include \"common.h\"\n \n@@ -206,7 +207,7 @@ struct gtm_thread\n \n   // If this transaction is inactive, shared_state is ~0. Otherwise, this is\n   // an active or serial transaction.\n-  gtm_word shared_state;\n+  atomic<gtm_word> shared_state;\n \n   // The lock that provides access to serial mode.  Non-serialized\n   // transactions acquire read locks; a serialized transaction aquires"}, {"sha": "e6644634a73361738f572c4d6a21ad56eff81c5a", "filename": "libitm/local_atomic", "status": "added", "additions": 1903, "deletions": 0, "changes": 1903, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/36cfbee133027429a681ce585643d38228ab1213/libitm%2Flocal_atomic", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/36cfbee133027429a681ce585643d38228ab1213/libitm%2Flocal_atomic", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Flocal_atomic?ref=36cfbee133027429a681ce585643d38228ab1213", "patch": "@@ -0,0 +1,1903 @@\n+// -*- C++ -*- header.\n+\n+// Copyright (C) 2008, 2009, 2010, 2011 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+// ????????????????????????????????????????????????????????????????????\n+//\n+// This is a copy of the libstdc++ header, with the trivial modification\n+// of ignoring the c++config.h include.  If and when the top-level build is\n+// fixed so that target libraries can be built using the newly built, we can\n+// delete this file.\n+//\n+// ????????????????????????????????????????????????????????????????????\n+\n+/** @file include/atomic\n+ *  This is a Standard C++ Library header.\n+ */\n+\n+// Based on \"C++ Atomic Types and Operations\" by Hans Boehm and Lawrence Crowl.\n+// http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2427.html\n+\n+#ifndef _GLIBCXX_ATOMIC\n+#define _GLIBCXX_ATOMIC 1\n+\n+// #pragma GCC system_header\n+\n+// #ifndef __GXX_EXPERIMENTAL_CXX0X__\n+// # include <bits/c++0x_warning.h>\n+// #endif\n+\n+// #include <bits/atomic_base.h>\n+\n+namespace std // _GLIBCXX_VISIBILITY(default)\n+{\n+// _GLIBCXX_BEGIN_NAMESPACE_VERSION\n+\n+  /**\n+   * @defgroup atomics Atomics\n+   *\n+   * Components for performing atomic operations.\n+   * @{\n+   */\n+\n+  /// Enumeration for memory_order\n+  typedef enum memory_order\n+    {\n+      memory_order_relaxed,\n+      memory_order_consume,\n+      memory_order_acquire,\n+      memory_order_release,\n+      memory_order_acq_rel,\n+      memory_order_seq_cst\n+    } memory_order;\n+\n+  inline memory_order\n+  __calculate_memory_order(memory_order __m) noexcept\n+  {\n+    const bool __cond1 = __m == memory_order_release;\n+    const bool __cond2 = __m == memory_order_acq_rel;\n+    memory_order __mo1(__cond1 ? memory_order_relaxed : __m);\n+    memory_order __mo2(__cond2 ? memory_order_acquire : __mo1);\n+    return __mo2;\n+  }\n+\n+  inline void\n+  atomic_thread_fence(memory_order __m) noexcept\n+  {\n+    __atomic_thread_fence (__m);\n+  }\n+\n+  inline void\n+  atomic_signal_fence(memory_order __m) noexcept\n+  {\n+    __atomic_thread_fence (__m);\n+  }\n+\n+  /// kill_dependency\n+  template<typename _Tp>\n+    inline _Tp\n+    kill_dependency(_Tp __y) noexcept\n+    {\n+      _Tp __ret(__y);\n+      return __ret;\n+    }\n+\n+  /// Lock-free Property\n+\n+\n+#define ATOMIC_BOOL_LOCK_FREE\t\t__GCC_ATOMIC_BOOL_LOCK_FREE\n+#define ATOMIC_CHAR_LOCK_FREE\t\t__GCC_ATOMIC_CHAR_LOCK_FREE\n+#define ATOMIC_WCHAR_T_LOCK_FREE\t__GCC_ATOMIC_WCHAR_T_LOCK_FREE\n+#define ATOMIC_CHAR16_T_LOCK_FREE\t__GCC_ATOMIC_CHAR16_T_LOCK_FREE\n+#define ATOMIC_CHAR32_T_LOCK_FREE\t__GCC_ATOMIC_CHAR32_T_LOCK_FREE\n+#define ATOMIC_SHORT_LOCK_FREE\t\t__GCC_ATOMIC_SHORT_LOCK_FREE\n+#define ATOMIC_INT_LOCK_FREE\t\t__GCC_ATOMIC_INT_LOCK_FREE\n+#define ATOMIC_LONG_LOCK_FREE\t\t__GCC_ATOMIC_LONG_LOCK_FREE\n+#define ATOMIC_LLONG_LOCK_FREE\t\t__GCC_ATOMIC_LLONG_LOCK_FREE\n+#define ATOMIC_POINTER_LOCK_FREE\t__GCC_ATOMIC_POINTER_LOCK_FREE\n+\n+  // Base types for atomics.\n+  template<typename _IntTp>\n+    struct __atomic_base;\n+\n+  /// atomic_char\n+  typedef __atomic_base<char>  \t       \t\tatomic_char;\n+\n+  /// atomic_schar\n+  typedef __atomic_base<signed char>\t     \tatomic_schar;\n+\n+  /// atomic_uchar\n+  typedef __atomic_base<unsigned char>\t\tatomic_uchar;\n+\n+  /// atomic_short\n+  typedef __atomic_base<short>\t\t\tatomic_short;\n+\n+  /// atomic_ushort\n+  typedef __atomic_base<unsigned short>\t \tatomic_ushort;\n+\n+  /// atomic_int\n+  typedef __atomic_base<int>  \t       \t\tatomic_int;\n+\n+  /// atomic_uint\n+  typedef __atomic_base<unsigned int>\t     \tatomic_uint;\n+\n+  /// atomic_long\n+  typedef __atomic_base<long>  \t       \t\tatomic_long;\n+\n+  /// atomic_ulong\n+  typedef __atomic_base<unsigned long>\t\tatomic_ulong;\n+\n+  /// atomic_llong\n+  typedef __atomic_base<long long>  \t\tatomic_llong;\n+\n+  /// atomic_ullong\n+  typedef __atomic_base<unsigned long long> \tatomic_ullong;\n+\n+  /// atomic_wchar_t\n+  typedef __atomic_base<wchar_t>  \t\tatomic_wchar_t;\n+\n+  /// atomic_char16_t\n+  typedef __atomic_base<char16_t>  \t\tatomic_char16_t;\n+\n+  /// atomic_char32_t\n+  typedef __atomic_base<char32_t>  \t\tatomic_char32_t;\n+\n+  /// atomic_char32_t\n+  typedef __atomic_base<char32_t>  \t\tatomic_char32_t;\n+\n+\n+  /// atomic_int_least8_t\n+  typedef __atomic_base<int_least8_t>  \t\tatomic_int_least8_t;\n+\n+  /// atomic_uint_least8_t\n+  typedef __atomic_base<uint_least8_t>\t       \tatomic_uint_least8_t;\n+\n+  /// atomic_int_least16_t\n+  typedef __atomic_base<int_least16_t>\t       \tatomic_int_least16_t;\n+\n+  /// atomic_uint_least16_t\n+  typedef __atomic_base<uint_least16_t>\t       \tatomic_uint_least16_t;\n+\n+  /// atomic_int_least32_t\n+  typedef __atomic_base<int_least32_t>\t       \tatomic_int_least32_t;\n+\n+  /// atomic_uint_least32_t\n+  typedef __atomic_base<uint_least32_t>\t       \tatomic_uint_least32_t;\n+\n+  /// atomic_int_least64_t\n+  typedef __atomic_base<int_least64_t>\t       \tatomic_int_least64_t;\n+\n+  /// atomic_uint_least64_t\n+  typedef __atomic_base<uint_least64_t>\t       \tatomic_uint_least64_t;\n+\n+\n+  /// atomic_int_fast8_t\n+  typedef __atomic_base<int_fast8_t>  \t\tatomic_int_fast8_t;\n+\n+  /// atomic_uint_fast8_t\n+  typedef __atomic_base<uint_fast8_t>\t      \tatomic_uint_fast8_t;\n+\n+  /// atomic_int_fast16_t\n+  typedef __atomic_base<int_fast16_t>\t      \tatomic_int_fast16_t;\n+\n+  /// atomic_uint_fast16_t\n+  typedef __atomic_base<uint_fast16_t>\t      \tatomic_uint_fast16_t;\n+\n+  /// atomic_int_fast32_t\n+  typedef __atomic_base<int_fast32_t>\t      \tatomic_int_fast32_t;\n+\n+  /// atomic_uint_fast32_t\n+  typedef __atomic_base<uint_fast32_t>\t      \tatomic_uint_fast32_t;\n+\n+  /// atomic_int_fast64_t\n+  typedef __atomic_base<int_fast64_t>\t      \tatomic_int_fast64_t;\n+\n+  /// atomic_uint_fast64_t\n+  typedef __atomic_base<uint_fast64_t>\t      \tatomic_uint_fast64_t;\n+\n+\n+  /// atomic_intptr_t\n+  typedef __atomic_base<intptr_t>  \t       \tatomic_intptr_t;\n+\n+  /// atomic_uintptr_t\n+  typedef __atomic_base<uintptr_t>  \t       \tatomic_uintptr_t;\n+\n+  /// atomic_size_t\n+  typedef __atomic_base<size_t>\t \t       \tatomic_size_t;\n+\n+  /// atomic_intmax_t\n+  typedef __atomic_base<intmax_t>  \t       \tatomic_intmax_t;\n+\n+  /// atomic_uintmax_t\n+  typedef __atomic_base<uintmax_t>  \t       \tatomic_uintmax_t;\n+\n+  /// atomic_ptrdiff_t\n+  typedef __atomic_base<ptrdiff_t>  \t       \tatomic_ptrdiff_t;\n+\n+\n+#define ATOMIC_VAR_INIT(_VI) { _VI }\n+\n+  template<typename _Tp>\n+    struct atomic;\n+\n+  template<typename _Tp>\n+    struct atomic<_Tp*>;\n+\n+\n+  /**\n+   *  @brief Base type for atomic_flag.\n+   *\n+   *  Base type is POD with data, allowing atomic_flag to derive from\n+   *  it and meet the standard layout type requirement. In addition to\n+   *  compatibilty with a C interface, this allows different\n+   *  implementations of atomic_flag to use the same atomic operation\n+   *  functions, via a standard conversion to the __atomic_flag_base\n+   *  argument.\n+  */\n+  // _GLIBCXX_BEGIN_EXTERN_C\n+\n+  struct __atomic_flag_base\n+  {\n+    bool _M_i;\n+  };\n+\n+  // _GLIBCXX_END_EXTERN_C\n+\n+#define ATOMIC_FLAG_INIT { false }\n+\n+  /// atomic_flag\n+  struct atomic_flag : public __atomic_flag_base\n+  {\n+    atomic_flag() noexcept = default;\n+    ~atomic_flag() noexcept = default;\n+    atomic_flag(const atomic_flag&) = delete;\n+    atomic_flag& operator=(const atomic_flag&) = delete;\n+    atomic_flag& operator=(const atomic_flag&) volatile = delete;\n+\n+    // Conversion to ATOMIC_FLAG_INIT.\n+    atomic_flag(bool __i) noexcept : __atomic_flag_base({ __i }) { }\n+\n+    bool\n+    test_and_set(memory_order __m = memory_order_seq_cst) noexcept\n+    {\n+      return __atomic_test_and_set (&_M_i, __m);\n+    }\n+\n+    bool\n+    test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept\n+    {\n+      return __atomic_test_and_set (&_M_i, __m);\n+    }\n+\n+    void\n+    clear(memory_order __m = memory_order_seq_cst) noexcept\n+    {\n+      // __glibcxx_assert(__m != memory_order_consume);\n+      // __glibcxx_assert(__m != memory_order_acquire);\n+      // __glibcxx_assert(__m != memory_order_acq_rel);\n+\n+      __atomic_clear (&_M_i, __m);\n+    }\n+\n+    void\n+    clear(memory_order __m = memory_order_seq_cst) volatile noexcept\n+    {\n+      // __glibcxx_assert(__m != memory_order_consume);\n+      // __glibcxx_assert(__m != memory_order_acquire);\n+      // __glibcxx_assert(__m != memory_order_acq_rel);\n+\n+      __atomic_clear (&_M_i, __m);\n+    }\n+  };\n+\n+\n+  /// Base class for atomic integrals.\n+  //\n+  // For each of the integral types, define atomic_[integral type] struct\n+  //\n+  // atomic_bool     bool\n+  // atomic_char     char\n+  // atomic_schar    signed char\n+  // atomic_uchar    unsigned char\n+  // atomic_short    short\n+  // atomic_ushort   unsigned short\n+  // atomic_int      int\n+  // atomic_uint     unsigned int\n+  // atomic_long     long\n+  // atomic_ulong    unsigned long\n+  // atomic_llong    long long\n+  // atomic_ullong   unsigned long long\n+  // atomic_char16_t char16_t\n+  // atomic_char32_t char32_t\n+  // atomic_wchar_t  wchar_t\n+  //\n+  // NB: Assuming _ITp is an integral scalar type that is 1, 2, 4, or\n+  // 8 bytes, since that is what GCC built-in functions for atomic\n+  // memory access expect.\n+  template<typename _ITp>\n+    struct __atomic_base\n+    {\n+    private:\n+      typedef _ITp \t__int_type;\n+\n+      __int_type \t_M_i;\n+\n+    public:\n+      __atomic_base() noexcept = default;\n+      ~__atomic_base() noexcept = default;\n+      __atomic_base(const __atomic_base&) = delete;\n+      __atomic_base& operator=(const __atomic_base&) = delete;\n+      __atomic_base& operator=(const __atomic_base&) volatile = delete;\n+\n+      // Requires __int_type convertible to _M_i.\n+      constexpr __atomic_base(__int_type __i) noexcept : _M_i (__i) { }\n+\n+      operator __int_type() const noexcept\n+      { return load(); }\n+\n+      operator __int_type() const volatile noexcept\n+      { return load(); }\n+\n+      __int_type\n+      operator=(__int_type __i) noexcept\n+      {\n+\tstore(__i);\n+\treturn __i;\n+      }\n+\n+      __int_type\n+      operator=(__int_type __i) volatile noexcept\n+      {\n+\tstore(__i);\n+\treturn __i;\n+      }\n+\n+      __int_type\n+      operator++(int) noexcept\n+      { return fetch_add(1); }\n+\n+      __int_type\n+      operator++(int) volatile noexcept\n+      { return fetch_add(1); }\n+\n+      __int_type\n+      operator--(int) noexcept\n+      { return fetch_sub(1); }\n+\n+      __int_type\n+      operator--(int) volatile noexcept\n+      { return fetch_sub(1); }\n+\n+      __int_type\n+      operator++() noexcept\n+      { return __atomic_add_fetch(&_M_i, 1, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator++() volatile noexcept\n+      { return __atomic_add_fetch(&_M_i, 1, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator--() noexcept\n+      { return __atomic_sub_fetch(&_M_i, 1, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator--() volatile noexcept\n+      { return __atomic_sub_fetch(&_M_i, 1, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator+=(__int_type __i) noexcept\n+      { return __atomic_add_fetch(&_M_i, __i, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator+=(__int_type __i) volatile noexcept\n+      { return __atomic_add_fetch(&_M_i, __i, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator-=(__int_type __i) noexcept\n+      { return __atomic_sub_fetch(&_M_i, __i, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator-=(__int_type __i) volatile noexcept\n+      { return __atomic_sub_fetch(&_M_i, __i, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator&=(__int_type __i) noexcept\n+      { return __atomic_and_fetch(&_M_i, __i, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator&=(__int_type __i) volatile noexcept\n+      { return __atomic_and_fetch(&_M_i, __i, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator|=(__int_type __i) noexcept\n+      { return __atomic_or_fetch(&_M_i, __i, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator|=(__int_type __i) volatile noexcept\n+      { return __atomic_or_fetch(&_M_i, __i, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator^=(__int_type __i) noexcept\n+      { return __atomic_xor_fetch(&_M_i, __i, memory_order_seq_cst); }\n+\n+      __int_type\n+      operator^=(__int_type __i) volatile noexcept\n+      { return __atomic_xor_fetch(&_M_i, __i, memory_order_seq_cst); }\n+\n+      bool\n+      is_lock_free() const noexcept\n+      { return __atomic_is_lock_free (sizeof (_M_i), &_M_i); }\n+\n+      bool\n+      is_lock_free() const volatile noexcept\n+      { return __atomic_is_lock_free (sizeof (_M_i), &_M_i); }\n+\n+      void\n+      store(__int_type __i, memory_order __m = memory_order_seq_cst) noexcept\n+      {\n+\t// __glibcxx_assert(__m != memory_order_acquire);\n+\t// __glibcxx_assert(__m != memory_order_acq_rel);\n+\t// __glibcxx_assert(__m != memory_order_consume);\n+\n+\t__atomic_store_n(&_M_i, __i, __m);\n+      }\n+\n+      void\n+      store(__int_type __i,\n+\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n+      {\n+\t// __glibcxx_assert(__m != memory_order_acquire);\n+\t// __glibcxx_assert(__m != memory_order_acq_rel);\n+\t// __glibcxx_assert(__m != memory_order_consume);\n+\n+\t__atomic_store_n(&_M_i, __i, __m);\n+      }\n+\n+      __int_type\n+      load(memory_order __m = memory_order_seq_cst) const noexcept\n+      {\n+\t// __glibcxx_assert(__m != memory_order_release);\n+\t// __glibcxx_assert(__m != memory_order_acq_rel);\n+\n+\treturn __atomic_load_n(&_M_i, __m);\n+      }\n+\n+      __int_type\n+      load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n+      {\n+\t// __glibcxx_assert(__m != memory_order_release);\n+\t// __glibcxx_assert(__m != memory_order_acq_rel);\n+\n+\treturn __atomic_load_n(&_M_i, __m);\n+      }\n+\n+      __int_type\n+      exchange(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n+      {\n+\treturn __atomic_exchange_n(&_M_i, __i, __m);\n+      }\n+\n+\n+      __int_type\n+      exchange(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n+      {\n+\treturn __atomic_exchange_n(&_M_i, __i, __m);\n+      }\n+\n+      bool\n+      compare_exchange_weak(__int_type& __i1, __int_type __i2,\n+\t\t\t    memory_order __m1, memory_order __m2) noexcept\n+      {\n+\t// __glibcxx_assert(__m2 != memory_order_release);\n+\t// __glibcxx_assert(__m2 != memory_order_acq_rel);\n+\t// __glibcxx_assert(__m2 <= __m1);\n+\n+\treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 1, __m1, __m2);\n+      }\n+\n+      bool\n+      compare_exchange_weak(__int_type& __i1, __int_type __i2,\n+\t\t\t    memory_order __m1,\n+\t\t\t    memory_order __m2) volatile noexcept\n+      {\n+\t// __glibcxx_assert(__m2 != memory_order_release);\n+\t// __glibcxx_assert(__m2 != memory_order_acq_rel);\n+\t// __glibcxx_assert(__m2 <= __m1);\n+\n+\treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 1, __m1, __m2);\n+      }\n+\n+      bool\n+      compare_exchange_weak(__int_type& __i1, __int_type __i2,\n+\t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n+      {\n+\treturn compare_exchange_weak(__i1, __i2, __m,\n+\t\t\t\t     __calculate_memory_order(__m));\n+      }\n+\n+      bool\n+      compare_exchange_weak(__int_type& __i1, __int_type __i2,\n+\t\t   memory_order __m = memory_order_seq_cst) volatile noexcept\n+      {\n+\treturn compare_exchange_weak(__i1, __i2, __m,\n+\t\t\t\t     __calculate_memory_order(__m));\n+      }\n+\n+      bool\n+      compare_exchange_strong(__int_type& __i1, __int_type __i2,\n+\t\t\t      memory_order __m1, memory_order __m2) noexcept\n+      {\n+\t// __glibcxx_assert(__m2 != memory_order_release);\n+\t// __glibcxx_assert(__m2 != memory_order_acq_rel);\n+\t// __glibcxx_assert(__m2 <= __m1);\n+\n+\treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0, __m1, __m2);\n+      }\n+\n+      bool\n+      compare_exchange_strong(__int_type& __i1, __int_type __i2,\n+\t\t\t      memory_order __m1,\n+\t\t\t      memory_order __m2) volatile noexcept\n+      {\n+\t// __glibcxx_assert(__m2 != memory_order_release);\n+\t// __glibcxx_assert(__m2 != memory_order_acq_rel);\n+\t// __glibcxx_assert(__m2 <= __m1);\n+\n+\treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0, __m1, __m2);\n+      }\n+\n+      bool\n+      compare_exchange_strong(__int_type& __i1, __int_type __i2,\n+\t\t\t      memory_order __m = memory_order_seq_cst) noexcept\n+      {\n+\treturn compare_exchange_strong(__i1, __i2, __m,\n+\t\t\t\t       __calculate_memory_order(__m));\n+      }\n+\n+      bool\n+      compare_exchange_strong(__int_type& __i1, __int_type __i2,\n+\t\t memory_order __m = memory_order_seq_cst) volatile noexcept\n+      {\n+\treturn compare_exchange_strong(__i1, __i2, __m,\n+\t\t\t\t       __calculate_memory_order(__m));\n+      }\n+\n+      __int_type\n+      fetch_add(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n+      { return __atomic_fetch_add(&_M_i, __i, __m); }\n+\n+      __int_type\n+      fetch_add(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return __atomic_fetch_add(&_M_i, __i, __m); }\n+\n+      __int_type\n+      fetch_sub(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n+      { return __atomic_fetch_sub(&_M_i, __i, __m); }\n+\n+      __int_type\n+      fetch_sub(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return __atomic_fetch_sub(&_M_i, __i, __m); }\n+\n+      __int_type\n+      fetch_and(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n+      { return __atomic_fetch_and(&_M_i, __i, __m); }\n+\n+      __int_type\n+      fetch_and(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return __atomic_fetch_and(&_M_i, __i, __m); }\n+\n+      __int_type\n+      fetch_or(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n+      { return __atomic_fetch_or(&_M_i, __i, __m); }\n+\n+      __int_type\n+      fetch_or(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return __atomic_fetch_or(&_M_i, __i, __m); }\n+\n+      __int_type\n+      fetch_xor(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n+      { return __atomic_fetch_xor(&_M_i, __i, __m); }\n+\n+      __int_type\n+      fetch_xor(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return __atomic_fetch_xor(&_M_i, __i, __m); }\n+    };\n+\n+\n+  /// Partial specialization for pointer types.\n+  template<typename _PTp>\n+    struct __atomic_base<_PTp*>\n+    {\n+    private:\n+      typedef _PTp* \t__pointer_type;\n+\n+      __pointer_type \t_M_p;\n+\n+    public:\n+      __atomic_base() noexcept = default;\n+      ~__atomic_base() noexcept = default;\n+      __atomic_base(const __atomic_base&) = delete;\n+      __atomic_base& operator=(const __atomic_base&) = delete;\n+      __atomic_base& operator=(const __atomic_base&) volatile = delete;\n+\n+      // Requires __pointer_type convertible to _M_p.\n+      constexpr __atomic_base(__pointer_type __p) noexcept : _M_p (__p) { }\n+\n+      operator __pointer_type() const noexcept\n+      { return load(); }\n+\n+      operator __pointer_type() const volatile noexcept\n+      { return load(); }\n+\n+      __pointer_type\n+      operator=(__pointer_type __p) noexcept\n+      {\n+\tstore(__p);\n+\treturn __p;\n+      }\n+\n+      __pointer_type\n+      operator=(__pointer_type __p) volatile noexcept\n+      {\n+\tstore(__p);\n+\treturn __p;\n+      }\n+\n+      __pointer_type\n+      operator++(int) noexcept\n+      { return fetch_add(1); }\n+\n+      __pointer_type\n+      operator++(int) volatile noexcept\n+      { return fetch_add(1); }\n+\n+      __pointer_type\n+      operator--(int) noexcept\n+      { return fetch_sub(1); }\n+\n+      __pointer_type\n+      operator--(int) volatile noexcept\n+      { return fetch_sub(1); }\n+\n+      __pointer_type\n+      operator++() noexcept\n+      { return __atomic_add_fetch(&_M_p, 1, memory_order_seq_cst); }\n+\n+      __pointer_type\n+      operator++() volatile noexcept\n+      { return __atomic_add_fetch(&_M_p, 1, memory_order_seq_cst); }\n+\n+      __pointer_type\n+      operator--() noexcept\n+      { return __atomic_sub_fetch(&_M_p, 1, memory_order_seq_cst); }\n+\n+      __pointer_type\n+      operator--() volatile noexcept\n+      { return __atomic_sub_fetch(&_M_p, 1, memory_order_seq_cst); }\n+\n+      __pointer_type\n+      operator+=(ptrdiff_t __d) noexcept\n+      { return __atomic_add_fetch(&_M_p, __d, memory_order_seq_cst); }\n+\n+      __pointer_type\n+      operator+=(ptrdiff_t __d) volatile noexcept\n+      { return __atomic_add_fetch(&_M_p, __d, memory_order_seq_cst); }\n+\n+      __pointer_type\n+      operator-=(ptrdiff_t __d) noexcept\n+      { return __atomic_sub_fetch(&_M_p, __d, memory_order_seq_cst); }\n+\n+      __pointer_type\n+      operator-=(ptrdiff_t __d) volatile noexcept\n+      { return __atomic_sub_fetch(&_M_p, __d, memory_order_seq_cst); }\n+\n+      bool\n+      is_lock_free() const noexcept\n+      { return __atomic_is_lock_free (sizeof (_M_p), &_M_p); }\n+\n+      bool\n+      is_lock_free() const volatile noexcept\n+      { return __atomic_is_lock_free (sizeof (_M_p), &_M_p); }\n+\n+      void\n+      store(__pointer_type __p,\n+\t    memory_order __m = memory_order_seq_cst) noexcept\n+      {\n+\t// __glibcxx_assert(__m != memory_order_acquire);\n+\t// __glibcxx_assert(__m != memory_order_acq_rel);\n+\t// __glibcxx_assert(__m != memory_order_consume);\n+\n+\t__atomic_store_n(&_M_p, __p, __m);\n+      }\n+\n+      void\n+      store(__pointer_type __p,\n+\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n+      {\n+\t// __glibcxx_assert(__m != memory_order_acquire);\n+\t// __glibcxx_assert(__m != memory_order_acq_rel);\n+\t// __glibcxx_assert(__m != memory_order_consume);\n+\n+\t__atomic_store_n(&_M_p, __p, __m);\n+      }\n+\n+      __pointer_type\n+      load(memory_order __m = memory_order_seq_cst) const noexcept\n+      {\n+\t// __glibcxx_assert(__m != memory_order_release);\n+\t// __glibcxx_assert(__m != memory_order_acq_rel);\n+\n+\treturn __atomic_load_n(&_M_p, __m);\n+      }\n+\n+      __pointer_type\n+      load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n+      {\n+\t// __glibcxx_assert(__m != memory_order_release);\n+\t// __glibcxx_assert(__m != memory_order_acq_rel);\n+\n+\treturn __atomic_load_n(&_M_p, __m);\n+      }\n+\n+      __pointer_type\n+      exchange(__pointer_type __p,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n+      {\n+\treturn __atomic_exchange_n(&_M_p, __p, __m);\n+      }\n+\n+\n+      __pointer_type\n+      exchange(__pointer_type __p,\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n+      {\n+\treturn __atomic_exchange_n(&_M_p, __p, __m);\n+      }\n+\n+      bool\n+      compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n+\t\t\t      memory_order __m1,\n+\t\t\t      memory_order __m2) noexcept\n+      {\n+\t// __glibcxx_assert(__m2 != memory_order_release);\n+\t// __glibcxx_assert(__m2 != memory_order_acq_rel);\n+\t// __glibcxx_assert(__m2 <= __m1);\n+\n+\treturn __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0, __m1, __m2);\n+      }\n+\n+      bool\n+      compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n+\t\t\t      memory_order __m1,\n+\t\t\t      memory_order __m2) volatile noexcept\n+      {\n+\t// __glibcxx_assert(__m2 != memory_order_release);\n+\t// __glibcxx_assert(__m2 != memory_order_acq_rel);\n+\t// __glibcxx_assert(__m2 <= __m1);\n+\n+\treturn __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0, __m1, __m2);\n+      }\n+\n+      __pointer_type\n+      fetch_add(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n+      { return __atomic_fetch_add(&_M_p, __d, __m); }\n+\n+      __pointer_type\n+      fetch_add(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return __atomic_fetch_add(&_M_p, __d, __m); }\n+\n+      __pointer_type\n+      fetch_sub(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n+      { return __atomic_fetch_sub(&_M_p, __d, __m); }\n+\n+      __pointer_type\n+      fetch_sub(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return __atomic_fetch_sub(&_M_p, __d, __m); }\n+    };\n+\n+\n+  /**\n+   * @addtogroup atomics\n+   * @{\n+   */\n+\n+  /// atomic_bool\n+  // NB: No operators or fetch-operations for this type.\n+  struct atomic_bool\n+  {\n+  private:\n+    __atomic_base<bool>\t_M_base;\n+\n+  public:\n+    atomic_bool() noexcept = default;\n+    ~atomic_bool() noexcept = default;\n+    atomic_bool(const atomic_bool&) = delete;\n+    atomic_bool& operator=(const atomic_bool&) = delete;\n+    atomic_bool& operator=(const atomic_bool&) volatile = delete;\n+\n+    constexpr atomic_bool(bool __i) noexcept : _M_base(__i) { }\n+\n+    bool\n+    operator=(bool __i) noexcept\n+    { return _M_base.operator=(__i); }\n+\n+    operator bool() const noexcept\n+    { return _M_base.load(); }\n+\n+    operator bool() const volatile noexcept\n+    { return _M_base.load(); }\n+\n+    bool\n+    is_lock_free() const noexcept { return _M_base.is_lock_free(); }\n+\n+    bool\n+    is_lock_free() const volatile noexcept { return _M_base.is_lock_free(); }\n+\n+    void\n+    store(bool __i, memory_order __m = memory_order_seq_cst) noexcept\n+    { _M_base.store(__i, __m); }\n+\n+    void\n+    store(bool __i, memory_order __m = memory_order_seq_cst) volatile noexcept\n+    { _M_base.store(__i, __m); }\n+\n+    bool\n+    load(memory_order __m = memory_order_seq_cst) const noexcept\n+    { return _M_base.load(__m); }\n+\n+    bool\n+    load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n+    { return _M_base.load(__m); }\n+\n+    bool\n+    exchange(bool __i, memory_order __m = memory_order_seq_cst) noexcept\n+    { return _M_base.exchange(__i, __m); }\n+\n+    bool\n+    exchange(bool __i,\n+\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n+    { return _M_base.exchange(__i, __m); }\n+\n+    bool\n+    compare_exchange_weak(bool& __i1, bool __i2, memory_order __m1,\n+\t\t\t  memory_order __m2) noexcept\n+    { return _M_base.compare_exchange_weak(__i1, __i2, __m1, __m2); }\n+\n+    bool\n+    compare_exchange_weak(bool& __i1, bool __i2, memory_order __m1,\n+\t\t\t  memory_order __m2) volatile noexcept\n+    { return _M_base.compare_exchange_weak(__i1, __i2, __m1, __m2); }\n+\n+    bool\n+    compare_exchange_weak(bool& __i1, bool __i2,\n+\t\t\t  memory_order __m = memory_order_seq_cst) noexcept\n+    { return _M_base.compare_exchange_weak(__i1, __i2, __m); }\n+\n+    bool\n+    compare_exchange_weak(bool& __i1, bool __i2,\n+\t\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n+    { return _M_base.compare_exchange_weak(__i1, __i2, __m); }\n+\n+    bool\n+    compare_exchange_strong(bool& __i1, bool __i2, memory_order __m1,\n+\t\t\t    memory_order __m2) noexcept\n+    { return _M_base.compare_exchange_strong(__i1, __i2, __m1, __m2); }\n+\n+    bool\n+    compare_exchange_strong(bool& __i1, bool __i2, memory_order __m1,\n+\t\t\t    memory_order __m2) volatile noexcept\n+    { return _M_base.compare_exchange_strong(__i1, __i2, __m1, __m2); }\n+\n+    bool\n+    compare_exchange_strong(bool& __i1, bool __i2,\n+\t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n+    { return _M_base.compare_exchange_strong(__i1, __i2, __m); }\n+\n+    bool\n+    compare_exchange_strong(bool& __i1, bool __i2,\n+\t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n+    { return _M_base.compare_exchange_strong(__i1, __i2, __m); }\n+  };\n+\n+\n+  /// atomic\n+  /// 29.4.3, Generic atomic type, primary class template.\n+  template<typename _Tp>\n+    struct atomic\n+    {\n+    private:\n+      _Tp _M_i;\n+\n+    public:\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(_Tp __i) noexcept : _M_i(__i) { }\n+\n+      operator _Tp() const noexcept\n+      { return load(); }\n+\n+      operator _Tp() const volatile noexcept\n+      { return load(); }\n+\n+      _Tp\n+      operator=(_Tp __i) noexcept \n+      { store(__i); return __i; }\n+\n+      _Tp\n+      operator=(_Tp __i) volatile noexcept \n+      { store(__i); return __i; }\n+\n+      bool\n+      is_lock_free() const noexcept\n+      { return __atomic_is_lock_free(sizeof(_M_i), &_M_i); }\n+\n+      bool\n+      is_lock_free() const volatile noexcept\n+      { return __atomic_is_lock_free(sizeof(_M_i), &_M_i); }\n+\n+      void\n+      store(_Tp __i, memory_order _m = memory_order_seq_cst) noexcept\n+      { __atomic_store(&_M_i, &__i, _m); }\n+\n+      void\n+      store(_Tp __i, memory_order _m = memory_order_seq_cst) volatile noexcept\n+      { __atomic_store(&_M_i, &__i, _m); }\n+\n+      _Tp\n+      load(memory_order _m = memory_order_seq_cst) const noexcept\n+      { \n+        _Tp tmp;\n+\t__atomic_load(&_M_i, &tmp, _m); \n+\treturn tmp;\n+      }\n+\n+      _Tp\n+      load(memory_order _m = memory_order_seq_cst) const volatile noexcept\n+      { \n+        _Tp tmp;\n+\t__atomic_load(&_M_i, &tmp, _m); \n+\treturn tmp;\n+      }\n+\n+      _Tp\n+      exchange(_Tp __i, memory_order _m = memory_order_seq_cst) noexcept\n+      { \n+        _Tp tmp;\n+\t__atomic_exchange(&_M_i, &__i, &tmp, _m); \n+\treturn tmp;\n+      }\n+\n+      _Tp\n+      exchange(_Tp __i, \n+\t       memory_order _m = memory_order_seq_cst) volatile noexcept\n+      { \n+        _Tp tmp;\n+\t__atomic_exchange(&_M_i, &__i, &tmp, _m); \n+\treturn tmp;\n+      }\n+\n+      bool\n+      compare_exchange_weak(_Tp& __e, _Tp __i, memory_order __s, \n+\t\t\t    memory_order __f) noexcept\n+      {\n+\treturn __atomic_compare_exchange(&_M_i, &__e, &__i, true, __s, __f); \n+      }\n+\n+      bool\n+      compare_exchange_weak(_Tp& __e, _Tp __i, memory_order __s, \n+\t\t\t    memory_order __f) volatile noexcept\n+      {\n+\treturn __atomic_compare_exchange(&_M_i, &__e, &__i, true, __s, __f); \n+      }\n+\n+      bool\n+      compare_exchange_weak(_Tp& __e, _Tp __i,\n+\t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n+      { return compare_exchange_weak(__e, __i, __m, __m); }\n+\n+      bool\n+      compare_exchange_weak(_Tp& __e, _Tp __i,\n+\t\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return compare_exchange_weak(__e, __i, __m, __m); }\n+\n+      bool\n+      compare_exchange_strong(_Tp& __e, _Tp __i, memory_order __s, \n+\t\t\t      memory_order __f) noexcept\n+      {\n+\treturn __atomic_compare_exchange(&_M_i, &__e, &__i, false, __s, __f); \n+      }\n+\n+      bool\n+      compare_exchange_strong(_Tp& __e, _Tp __i, memory_order __s, \n+\t\t\t      memory_order __f) volatile noexcept\n+      {\n+\treturn __atomic_compare_exchange(&_M_i, &__e, &__i, false, __s, __f); \n+      }\n+\n+      bool\n+      compare_exchange_strong(_Tp& __e, _Tp __i,\n+\t\t\t       memory_order __m = memory_order_seq_cst) noexcept\n+      { return compare_exchange_strong(__e, __i, __m, __m); }\n+\n+      bool\n+      compare_exchange_strong(_Tp& __e, _Tp __i,\n+\t\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return compare_exchange_strong(__e, __i, __m, __m); }\n+    };\n+\n+\n+  /// Partial specialization for pointer types.\n+  template<typename _Tp>\n+    struct atomic<_Tp*>\n+    {\n+      typedef _Tp* \t\t\t__pointer_type;\n+      typedef __atomic_base<_Tp*>\t__base_type;\n+      __base_type\t\t\t_M_b;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__pointer_type __p) noexcept : _M_b(__p) { }\n+\n+      operator __pointer_type() const noexcept\n+      { return __pointer_type(_M_b); }\n+\n+      operator __pointer_type() const volatile noexcept\n+      { return __pointer_type(_M_b); }\n+\n+      __pointer_type\n+      operator=(__pointer_type __p) noexcept\n+      { return _M_b.operator=(__p); }\n+\n+      __pointer_type\n+      operator=(__pointer_type __p) volatile noexcept\n+      { return _M_b.operator=(__p); }\n+\n+      __pointer_type\n+      operator++(int) noexcept\n+      { return _M_b++; }\n+\n+      __pointer_type\n+      operator++(int) volatile noexcept\n+      { return _M_b++; }\n+\n+      __pointer_type\n+      operator--(int) noexcept\n+      { return _M_b--; }\n+\n+      __pointer_type\n+      operator--(int) volatile noexcept\n+      { return _M_b--; }\n+\n+      __pointer_type\n+      operator++() noexcept\n+      { return ++_M_b; }\n+\n+      __pointer_type\n+      operator++() volatile noexcept\n+      { return ++_M_b; }\n+\n+      __pointer_type\n+      operator--() noexcept\n+      { return --_M_b; }\n+\n+      __pointer_type\n+      operator--() volatile noexcept\n+      { return --_M_b; }\n+\n+      __pointer_type\n+      operator+=(ptrdiff_t __d) noexcept\n+      { return _M_b.operator+=(__d); }\n+\n+      __pointer_type\n+      operator+=(ptrdiff_t __d) volatile noexcept\n+      { return _M_b.operator+=(__d); }\n+\n+      __pointer_type\n+      operator-=(ptrdiff_t __d) noexcept\n+      { return _M_b.operator-=(__d); }\n+\n+      __pointer_type\n+      operator-=(ptrdiff_t __d) volatile noexcept\n+      { return _M_b.operator-=(__d); }\n+\n+      bool\n+      is_lock_free() const noexcept\n+      { return _M_b.is_lock_free(); }\n+\n+      bool\n+      is_lock_free() const volatile noexcept\n+      { return _M_b.is_lock_free(); }\n+\n+      void\n+      store(__pointer_type __p,\n+\t    memory_order __m = memory_order_seq_cst) noexcept\n+      { return _M_b.store(__p, __m); }\n+\n+      void\n+      store(__pointer_type __p,\n+\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return _M_b.store(__p, __m); }\n+\n+      __pointer_type\n+      load(memory_order __m = memory_order_seq_cst) const noexcept\n+      { return _M_b.load(__m); }\n+\n+      __pointer_type\n+      load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n+      { return _M_b.load(__m); }\n+\n+      __pointer_type\n+      exchange(__pointer_type __p,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n+      { return _M_b.exchange(__p, __m); }\n+\n+      __pointer_type\n+      exchange(__pointer_type __p,\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return _M_b.exchange(__p, __m); }\n+\n+      bool\n+      compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n+\t\t\t    memory_order __m1, memory_order __m2) noexcept\n+      { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n+\n+      bool\n+      compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n+\t\t\t    memory_order __m1,\n+\t\t\t    memory_order __m2) volatile noexcept\n+      { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n+\n+      bool\n+      compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n+\t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n+      {\n+\treturn compare_exchange_weak(__p1, __p2, __m,\n+\t\t\t\t     __calculate_memory_order(__m));\n+      }\n+\n+      bool\n+      compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n+\t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n+      {\n+\treturn compare_exchange_weak(__p1, __p2, __m,\n+\t\t\t\t     __calculate_memory_order(__m));\n+      }\n+\n+      bool\n+      compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n+\t\t\t      memory_order __m1, memory_order __m2) noexcept\n+      { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n+\n+      bool\n+      compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n+\t\t\t      memory_order __m1,\n+\t\t\t      memory_order __m2) volatile noexcept\n+      { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n+\n+      bool\n+      compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n+\t\t\t      memory_order __m = memory_order_seq_cst) noexcept\n+      {\n+\treturn _M_b.compare_exchange_strong(__p1, __p2, __m,\n+\t\t\t\t\t    __calculate_memory_order(__m));\n+      }\n+\n+      bool\n+      compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n+\t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n+      {\n+\treturn _M_b.compare_exchange_strong(__p1, __p2, __m,\n+\t\t\t\t\t    __calculate_memory_order(__m));\n+      }\n+\n+      __pointer_type\n+      fetch_add(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n+      { return _M_b.fetch_add(__d, __m); }\n+\n+      __pointer_type\n+      fetch_add(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return _M_b.fetch_add(__d, __m); }\n+\n+      __pointer_type\n+      fetch_sub(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n+      { return _M_b.fetch_sub(__d, __m); }\n+\n+      __pointer_type\n+      fetch_sub(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n+      { return _M_b.fetch_sub(__d, __m); }\n+    };\n+\n+\n+  /// Explicit specialization for bool.\n+  template<>\n+    struct atomic<bool> : public atomic_bool\n+    {\n+      typedef bool \t\t\t__integral_type;\n+      typedef atomic_bool \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for char.\n+  template<>\n+    struct atomic<char> : public atomic_char\n+    {\n+      typedef char \t\t\t__integral_type;\n+      typedef atomic_char \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for signed char.\n+  template<>\n+    struct atomic<signed char> : public atomic_schar\n+    {\n+      typedef signed char \t\t__integral_type;\n+      typedef atomic_schar \t\t__base_type;\n+\n+      atomic() noexcept= default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for unsigned char.\n+  template<>\n+    struct atomic<unsigned char> : public atomic_uchar\n+    {\n+      typedef unsigned char \t\t__integral_type;\n+      typedef atomic_uchar \t\t__base_type;\n+\n+      atomic() noexcept= default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for short.\n+  template<>\n+    struct atomic<short> : public atomic_short\n+    {\n+      typedef short \t\t\t__integral_type;\n+      typedef atomic_short \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for unsigned short.\n+  template<>\n+    struct atomic<unsigned short> : public atomic_ushort\n+    {\n+      typedef unsigned short \t      \t__integral_type;\n+      typedef atomic_ushort \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for int.\n+  template<>\n+    struct atomic<int> : atomic_int\n+    {\n+      typedef int \t\t\t__integral_type;\n+      typedef atomic_int \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for unsigned int.\n+  template<>\n+    struct atomic<unsigned int> : public atomic_uint\n+    {\n+      typedef unsigned int\t\t__integral_type;\n+      typedef atomic_uint \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for long.\n+  template<>\n+    struct atomic<long> : public atomic_long\n+    {\n+      typedef long \t\t\t__integral_type;\n+      typedef atomic_long \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for unsigned long.\n+  template<>\n+    struct atomic<unsigned long> : public atomic_ulong\n+    {\n+      typedef unsigned long \t\t__integral_type;\n+      typedef atomic_ulong \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for long long.\n+  template<>\n+    struct atomic<long long> : public atomic_llong\n+    {\n+      typedef long long \t\t__integral_type;\n+      typedef atomic_llong \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for unsigned long long.\n+  template<>\n+    struct atomic<unsigned long long> : public atomic_ullong\n+    {\n+      typedef unsigned long long       \t__integral_type;\n+      typedef atomic_ullong \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for wchar_t.\n+  template<>\n+    struct atomic<wchar_t> : public atomic_wchar_t\n+    {\n+      typedef wchar_t \t\t\t__integral_type;\n+      typedef atomic_wchar_t \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for char16_t.\n+  template<>\n+    struct atomic<char16_t> : public atomic_char16_t\n+    {\n+      typedef char16_t \t\t\t__integral_type;\n+      typedef atomic_char16_t \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+  /// Explicit specialization for char32_t.\n+  template<>\n+    struct atomic<char32_t> : public atomic_char32_t\n+    {\n+      typedef char32_t \t\t\t__integral_type;\n+      typedef atomic_char32_t \t\t__base_type;\n+\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n+      atomic(const atomic&) = delete;\n+      atomic& operator=(const atomic&) = delete;\n+      atomic& operator=(const atomic&) volatile = delete;\n+\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n+\n+      using __base_type::operator __integral_type;\n+      using __base_type::operator=;\n+    };\n+\n+\n+  // Function definitions, atomic_flag operations.\n+  inline bool\n+  atomic_flag_test_and_set_explicit(atomic_flag* __a,\n+\t\t\t\t    memory_order __m) noexcept\n+  { return __a->test_and_set(__m); }\n+\n+  inline bool\n+  atomic_flag_test_and_set_explicit(volatile atomic_flag* __a,\n+\t\t\t\t    memory_order __m) noexcept\n+  { return __a->test_and_set(__m); }\n+\n+  inline void\n+  atomic_flag_clear_explicit(atomic_flag* __a, memory_order __m) noexcept\n+  { __a->clear(__m); }\n+\n+  inline void\n+  atomic_flag_clear_explicit(volatile atomic_flag* __a,\n+\t\t\t     memory_order __m) noexcept\n+  { __a->clear(__m); }\n+\n+  inline bool\n+  atomic_flag_test_and_set(atomic_flag* __a) noexcept\n+  { return atomic_flag_test_and_set_explicit(__a, memory_order_seq_cst); }\n+\n+  inline bool\n+  atomic_flag_test_and_set(volatile atomic_flag* __a) noexcept\n+  { return atomic_flag_test_and_set_explicit(__a, memory_order_seq_cst); }\n+\n+  inline void\n+  atomic_flag_clear(atomic_flag* __a) noexcept\n+  { atomic_flag_clear_explicit(__a, memory_order_seq_cst); }\n+\n+  inline void\n+  atomic_flag_clear(volatile atomic_flag* __a) noexcept\n+  { atomic_flag_clear_explicit(__a, memory_order_seq_cst); }\n+\n+\n+  // Function templates generally applicable to atomic types.\n+  template<typename _ITp>\n+    inline bool\n+    atomic_is_lock_free(const atomic<_ITp>* __a) noexcept\n+    { return __a->is_lock_free(); }\n+\n+  template<typename _ITp>\n+    inline bool\n+    atomic_is_lock_free(const volatile atomic<_ITp>* __a) noexcept\n+    { return __a->is_lock_free(); }\n+\n+  template<typename _ITp>\n+    inline void\n+    atomic_init(atomic<_ITp>* __a, _ITp __i) noexcept;\n+\n+  template<typename _ITp>\n+    inline void\n+    atomic_init(volatile atomic<_ITp>* __a, _ITp __i) noexcept;\n+\n+  template<typename _ITp>\n+    inline void\n+    atomic_store_explicit(atomic<_ITp>* __a, _ITp __i,\n+\t\t\t  memory_order __m) noexcept\n+    { __a->store(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline void\n+    atomic_store_explicit(volatile atomic<_ITp>* __a, _ITp __i,\n+\t\t\t  memory_order __m) noexcept\n+    { __a->store(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_load_explicit(const atomic<_ITp>* __a, memory_order __m) noexcept\n+    { return __a->load(__m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_load_explicit(const volatile atomic<_ITp>* __a,\n+\t\t\t memory_order __m) noexcept\n+    { return __a->load(__m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_exchange_explicit(atomic<_ITp>* __a, _ITp __i,\n+\t\t\t     memory_order __m) noexcept\n+    { return __a->exchange(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_exchange_explicit(volatile atomic<_ITp>* __a, _ITp __i,\n+\t\t\t     memory_order __m) noexcept\n+    { return __a->exchange(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline bool\n+    atomic_compare_exchange_weak_explicit(atomic<_ITp>* __a,\n+\t\t\t\t\t  _ITp* __i1, _ITp __i2,\n+\t\t\t\t\t  memory_order __m1,\n+\t\t\t\t\t  memory_order __m2) noexcept\n+    { return __a->compare_exchange_weak(*__i1, __i2, __m1, __m2); }\n+\n+  template<typename _ITp>\n+    inline bool\n+    atomic_compare_exchange_weak_explicit(volatile atomic<_ITp>* __a,\n+\t\t\t\t\t  _ITp* __i1, _ITp __i2,\n+\t\t\t\t\t  memory_order __m1,\n+\t\t\t\t\t  memory_order __m2) noexcept\n+    { return __a->compare_exchange_weak(*__i1, __i2, __m1, __m2); }\n+\n+  template<typename _ITp>\n+    inline bool\n+    atomic_compare_exchange_strong_explicit(atomic<_ITp>* __a,\n+\t\t\t\t\t    _ITp* __i1, _ITp __i2,\n+\t\t\t\t\t    memory_order __m1,\n+\t\t\t\t\t    memory_order __m2) noexcept\n+    { return __a->compare_exchange_strong(*__i1, __i2, __m1, __m2); }\n+\n+  template<typename _ITp>\n+    inline bool\n+    atomic_compare_exchange_strong_explicit(volatile atomic<_ITp>* __a,\n+\t\t\t\t\t    _ITp* __i1, _ITp __i2,\n+\t\t\t\t\t    memory_order __m1,\n+\t\t\t\t\t    memory_order __m2) noexcept\n+    { return __a->compare_exchange_strong(*__i1, __i2, __m1, __m2); }\n+\n+\n+  template<typename _ITp>\n+    inline void\n+    atomic_store(atomic<_ITp>* __a, _ITp __i) noexcept\n+    { atomic_store_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline void\n+    atomic_store(volatile atomic<_ITp>* __a, _ITp __i) noexcept\n+    { atomic_store_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_load(const atomic<_ITp>* __a) noexcept\n+    { return atomic_load_explicit(__a, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_load(const volatile atomic<_ITp>* __a) noexcept\n+    { return atomic_load_explicit(__a, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_exchange(atomic<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_exchange_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_exchange(volatile atomic<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_exchange_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline bool\n+    atomic_compare_exchange_weak(atomic<_ITp>* __a,\n+\t\t\t\t _ITp* __i1, _ITp __i2) noexcept\n+    {\n+      return atomic_compare_exchange_weak_explicit(__a, __i1, __i2,\n+\t\t\t\t\t\t   memory_order_seq_cst,\n+\t\t\t\t\t\t   memory_order_seq_cst);\n+    }\n+\n+  template<typename _ITp>\n+    inline bool\n+    atomic_compare_exchange_weak(volatile atomic<_ITp>* __a,\n+\t\t\t\t _ITp* __i1, _ITp __i2) noexcept\n+    {\n+      return atomic_compare_exchange_weak_explicit(__a, __i1, __i2,\n+\t\t\t\t\t\t   memory_order_seq_cst,\n+\t\t\t\t\t\t   memory_order_seq_cst);\n+    }\n+\n+  template<typename _ITp>\n+    inline bool\n+    atomic_compare_exchange_strong(atomic<_ITp>* __a,\n+\t\t\t\t   _ITp* __i1, _ITp __i2) noexcept\n+    {\n+      return atomic_compare_exchange_strong_explicit(__a, __i1, __i2,\n+\t\t\t\t\t\t     memory_order_seq_cst,\n+\t\t\t\t\t\t     memory_order_seq_cst);\n+    }\n+\n+  template<typename _ITp>\n+    inline bool\n+    atomic_compare_exchange_strong(volatile atomic<_ITp>* __a,\n+\t\t\t\t   _ITp* __i1, _ITp __i2) noexcept\n+    {\n+      return atomic_compare_exchange_strong_explicit(__a, __i1, __i2,\n+\t\t\t\t\t\t     memory_order_seq_cst,\n+\t\t\t\t\t\t     memory_order_seq_cst);\n+    }\n+\n+  // Function templates for atomic_integral operations only, using\n+  // __atomic_base. Template argument should be constricted to\n+  // intergral types as specified in the standard, excluding address\n+  // types.\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_add_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_add(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_add_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_add(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_sub_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_sub(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_sub_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_sub(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_and_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_and(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_and_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_and(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_or_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n+\t\t\t     memory_order __m) noexcept\n+    { return __a->fetch_or(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_or_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n+\t\t\t     memory_order __m) noexcept\n+    { return __a->fetch_or(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_xor_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_xor(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_xor_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_xor(__i, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_add(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_fetch_add_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_add(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_fetch_add_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_sub(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_fetch_sub_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_sub(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_fetch_sub_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_and(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_fetch_and_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_and(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_fetch_and_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_or(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_fetch_or_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_or(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_fetch_or_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_xor(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_fetch_xor_explicit(__a, __i, memory_order_seq_cst); }\n+\n+  template<typename _ITp>\n+    inline _ITp\n+    atomic_fetch_xor(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n+    { return atomic_fetch_xor_explicit(__a, __i, memory_order_seq_cst); }\n+\n+\n+  // Partial specializations for pointers.\n+  template<typename _ITp>\n+    inline _ITp*\n+    atomic_fetch_add_explicit(atomic<_ITp*>* __a, ptrdiff_t __d,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_add(__d, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp*\n+    atomic_fetch_add_explicit(volatile atomic<_ITp*>* __a, ptrdiff_t __d,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_add(__d, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp*\n+    atomic_fetch_add(volatile atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n+    { return __a->fetch_add(__d); }\n+\n+  template<typename _ITp>\n+    inline _ITp*\n+    atomic_fetch_add(atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n+    { return __a->fetch_add(__d); }\n+\n+  template<typename _ITp>\n+    inline _ITp*\n+    atomic_fetch_sub_explicit(volatile atomic<_ITp*>* __a,\n+\t\t\t      ptrdiff_t __d, memory_order __m) noexcept\n+    { return __a->fetch_sub(__d, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp*\n+    atomic_fetch_sub_explicit(atomic<_ITp*>* __a, ptrdiff_t __d,\n+\t\t\t      memory_order __m) noexcept\n+    { return __a->fetch_sub(__d, __m); }\n+\n+  template<typename _ITp>\n+    inline _ITp*\n+    atomic_fetch_sub(volatile atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n+    { return __a->fetch_sub(__d); }\n+\n+  template<typename _ITp>\n+    inline _ITp*\n+    atomic_fetch_sub(atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n+    { return __a->fetch_sub(__d); }\n+  // @} group atomics\n+\n+// _GLIBCXX_END_NAMESPACE_VERSION\n+} // namespace\n+\n+#endif"}, {"sha": "81045d319fcf106c3a5afff91937c916dee757ec", "filename": "libitm/method-gl.cc", "status": "modified", "additions": 36, "deletions": 39, "changes": 75, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fmethod-gl.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fmethod-gl.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fmethod-gl.cc?ref=36cfbee133027429a681ce585643d38228ab1213", "patch": "@@ -41,10 +41,11 @@ struct gl_mg : public method_group\n   static gtm_word clear_locked(gtm_word l) { return l & ~LOCK_BIT; }\n \n   // The global ownership record.\n-  gtm_word orec;\n+  atomic<gtm_word> orec;\n+\n   virtual void init()\n   {\n-    orec = 0;\n+    orec.store(0, memory_order_relaxed);\n   }\n   virtual void fini() { }\n };\n@@ -84,28 +85,25 @@ class gl_wt_dispatch : public abi_dispatch\n   static void pre_write(const void *addr, size_t len)\n   {\n     gtm_thread *tx = gtm_thr();\n-    if (unlikely(!gl_mg::is_locked(tx->shared_state)))\n+    gtm_word v = tx->shared_state.load(memory_order_acquire);\n+    if (unlikely(!gl_mg::is_locked(v)))\n       {\n \t// Check for and handle version number overflow.\n-\tif (unlikely(tx->shared_state >= gl_mg::VERSION_MAX))\n+\tif (unlikely(v >= gl_mg::VERSION_MAX))\n \t  tx->restart(RESTART_INIT_METHOD_GROUP);\n \n \t// CAS global orec from our snapshot time to the locked state.\n \t// This validates that we have a consistent snapshot, which is also\n \t// for making privatization safety work (see the class' comments).\n-\tgtm_word now = o_gl_mg.orec;\n-\tif (now != tx->shared_state)\n+\tgtm_word now = o_gl_mg.orec.load(memory_order_relaxed);\n+\tif (now != v)\n \t  tx->restart(RESTART_VALIDATE_WRITE);\n-\tif (__sync_val_compare_and_swap(&o_gl_mg.orec, now,\n-\t    gl_mg::set_locked(now)) != now)\n+\tif (!o_gl_mg.orec.compare_exchange_strong (now, gl_mg::set_locked(now),\n+\t\t\t\t\t\t   memory_order_acquire))\n \t  tx->restart(RESTART_LOCKED_WRITE);\n \n-\t// Set shared_state to new value. The CAS is a full barrier, so the\n-\t// acquisition of the global orec is visible before this store here,\n-\t// and the store will not be visible before earlier data loads, which\n-\t// is required to correctly ensure privatization safety (see\n-\t// begin_and_restart() and release_orec() for further comments).\n-\ttx->shared_state = gl_mg::set_locked(now);\n+\t// Set shared_state to new value.\n+\ttx->shared_state.store(gl_mg::set_locked(now), memory_order_release);\n       }\n \n     // TODO Ensure that this gets inlined: Use internal log interface and LTO.\n@@ -115,11 +113,12 @@ class gl_wt_dispatch : public abi_dispatch\n   static void validate()\n   {\n     // Check that snapshot is consistent. The barrier ensures that this\n-    // happens after previous data loads.\n-    atomic_read_barrier();\n+    // happens after previous data loads.  Recall that load cannot itself\n+    // have memory_order_release.\n     gtm_thread *tx = gtm_thr();\n-    gtm_word l = o_gl_mg.orec;\n-    if (l != tx->shared_state)\n+    atomic_thread_fence(memory_order_release);\n+    gtm_word l = o_gl_mg.orec.load(memory_order_relaxed);\n+    if (l != tx->shared_state.load(memory_order_relaxed))\n       tx->restart(RESTART_VALIDATE_READ);\n   }\n \n@@ -180,17 +179,18 @@ class gl_wt_dispatch : public abi_dispatch\n \n     // Spin until global orec is not locked.\n     // TODO This is not necessary if there are no pure loads (check txn props).\n-    gtm_word v;\n     unsigned i = 0;\n-    while (gl_mg::is_locked(v = o_gl_mg.orec))\n+    gtm_word v;\n+    while (1)\n       {\n+        v = o_gl_mg.orec.load(memory_order_acquire);\n+        if (!gl_mg::is_locked(v))\n+\t  break;\n \t// TODO need method-specific max spin count\n-\tif (++i > gtm_spin_count_var) return RESTART_VALIDATE_READ;\n+\tif (++i > gtm_spin_count_var)\n+\t  return RESTART_VALIDATE_READ;\n \tcpu_relax();\n       }\n-    // This barrier ensures that we have read the global orec before later\n-    // data loads.\n-    atomic_read_barrier();\n \n     // Everything is okay, we have a snapshot time.\n     // We don't need to enforce any ordering for the following store. There\n@@ -202,14 +202,14 @@ class gl_wt_dispatch : public abi_dispatch\n     // marking the transaction as active, and restarts enforce immediate\n     // visibility of a smaller or equal value with a barrier (see\n     // release_orec()).\n-    tx->shared_state = v;\n+    tx->shared_state.store(v, memory_order_relaxed);\n     return NO_RESTART;\n   }\n \n   virtual bool trycommit(gtm_word& priv_time)\n   {\n     gtm_thread* tx = gtm_thr();\n-    gtm_word v = tx->shared_state;\n+    gtm_word v = tx->shared_state.load(memory_order_acquire);\n \n     // Special case: If shared_state is ~0, then we have acquired the\n     // serial lock (tx->state is not updated yet). In this case, the previous\n@@ -218,7 +218,7 @@ class gl_wt_dispatch : public abi_dispatch\n     // anymore. In particular, if it is locked, then we are an update\n     // transaction, which is all we care about for commit.\n     if (v == ~(typeof v)0)\n-      v = o_gl_mg.orec;\n+      v = o_gl_mg.orec.load(memory_order_relaxed);\n \n     // Release the orec but do not reset shared_state, which will be modified\n     // by the serial lock right after our commit anyway. Also, resetting\n@@ -227,10 +227,8 @@ class gl_wt_dispatch : public abi_dispatch\n     if (gl_mg::is_locked(v))\n       {\n \t// Release the global orec, increasing its version number / timestamp.\n-\t// TODO replace with C++0x-style atomics (a release in this case)\n-\tatomic_write_barrier();\n \tv = gl_mg::clear_locked(v) + 1;\n-\to_gl_mg.orec = v;\n+\to_gl_mg.orec.store(v, memory_order_release);\n \n \t// Need to ensure privatization safety. Every other transaction must\n \t// have a snapshot time that is at least as high as our commit time\n@@ -247,40 +245,39 @@ class gl_wt_dispatch : public abi_dispatch\n       return;\n \n     gtm_thread *tx = gtm_thr();\n-    gtm_word v = tx->shared_state;\n+    gtm_word v = tx->shared_state.load(memory_order_acquire);\n     // Special case: If shared_state is ~0, then we have acquired the\n     // serial lock (tx->state is not updated yet). In this case, the previous\n     // value isn't available anymore, so grab it from the global lock, which\n     // must have a meaningful value because no other transactions are active\n     // anymore. In particular, if it is locked, then we are an update\n     // transaction, which is all we care about for rollback.\n-    if (v == ~(typeof v)0)\n-      v = o_gl_mg.orec;\n+    bool is_serial = v == ~(typeof v)0;\n+    if (is_serial)\n+      v = o_gl_mg.orec.load(memory_order_relaxed);\n \n     // Release lock and increment version number to prevent dirty reads.\n     // Also reset shared state here, so that begin_or_restart() can expect a\n     // value that is correct wrt. privatization safety.\n     if (gl_mg::is_locked(v))\n       {\n \t// Release the global orec, increasing its version number / timestamp.\n-\t// TODO replace with C++0x-style atomics (a release in this case)\n-\tatomic_write_barrier();\n \tv = gl_mg::clear_locked(v) + 1;\n-\to_gl_mg.orec = v;\n+\to_gl_mg.orec.store(v, memory_order_release);\n \n \t// Also reset the timestamp published via shared_state.\n \t// Special case: Only do this if we are not a serial transaction\n \t// because otherwise, we would interfere with the serial lock.\n-\tif (tx->shared_state != ~(typeof tx->shared_state)0)\n-\t  tx->shared_state = v;\n+\tif (!is_serial)\n+\t  tx->shared_state.store(v, memory_order_relaxed);\n \n \t// We need a store-load barrier after this store to prevent it\n \t// from becoming visible after later data loads because the\n \t// previous value of shared_state has been higher than the actual\n \t// snapshot time (the lock bit had been set), which could break\n \t// privatization safety. We do not need a barrier before this\n \t// store (see pre_write() for an explanation).\n-\t__sync_synchronize();\n+\tatomic_thread_fence(memory_order_acq_rel);\n       }\n \n   }"}, {"sha": "a338258e9f1215d0a290be42841422e019d8615b", "filename": "libitm/stmlock.h", "status": "modified", "additions": 8, "deletions": 9, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fstmlock.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/36cfbee133027429a681ce585643d38228ab1213/libitm%2Fstmlock.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fstmlock.h?ref=36cfbee133027429a681ce585643d38228ab1213", "patch": "@@ -92,24 +92,23 @@ gtm_get_stmlock (const gtm_cacheline *addr)\n }\n \n /* The current global version number.  */\n-extern gtm_version gtm_clock;\n+extern atomic<gtm_version> gtm_clock;\n \n static inline gtm_version\n gtm_get_clock (void)\n {\n-  gtm_version r;\n-\n-  __sync_synchronize ();\n-  r = gtm_clock;\n-  atomic_read_barrier ();\n-\n-  return r;\n+  atomic_thread_fence(memory_order_release);\n+  return gtm_clock.load(memory_order_acquire);\n }\n \n static inline gtm_version\n gtm_inc_clock (void)\n {\n-  gtm_version r = __sync_add_and_fetch (&gtm_clock, 1);\n+  /* ??? Here we have a choice, the pre-inc operator mapping to\n+     __atomic_add_fetch with memory_order_seq_cst, or fetch_add\n+     with memory_order_acq_rel plus another separate increment.\n+     We really ought to recognize and optimize fetch_op(x) op x... */\n+  gtm_version r = ++gtm_clock;\n \n   /* ??? Ought to handle wraparound for 32-bit.  */\n   if (sizeof(r) < 8 && r > GTM_VERSION_MAX)"}]}