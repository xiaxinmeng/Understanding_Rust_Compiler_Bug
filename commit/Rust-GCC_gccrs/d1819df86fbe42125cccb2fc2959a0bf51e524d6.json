{"sha": "d1819df86fbe42125cccb2fc2959a0bf51e524d6", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZDE4MTlkZjg2ZmJlNDIxMjVjY2NiMmZjMjk1OWEwYmY1MWU1MjRkNg==", "commit": {"author": {"name": "Jonathan Wright", "email": "jonathan.wright@arm.com", "date": "2021-08-16T13:37:18Z"}, "committer": {"name": "Jonathan Wright", "email": "jonathan.wright@arm.com", "date": "2021-08-17T10:44:37Z"}, "message": "aarch64: Remove macros for vld4[q]_lane Neon intrinsics\n\nRemove macros for vld4[q]_lane Neon intrinsics. This is a preparatory\nstep before adding new modes for structures of Advanced SIMD vectors.\n\ngcc/ChangeLog:\n\n2021-08-16  Jonathan Wright  <jonathan.wright@arm.com>\n\n\t* config/aarch64/arm_neon.h (__LD4_LANE_FUNC): Delete.\n\t(__LD4Q_LANE_FUNC): Likewise.\n\t(vld4_lane_u8): Define without macro.\n\t(vld4_lane_u16): Likewise.\n\t(vld4_lane_u32): Likewise.\n\t(vld4_lane_u64): Likewise.\n\t(vld4_lane_s8): Likewise.\n\t(vld4_lane_s16): Likewise.\n\t(vld4_lane_s32): Likewise.\n\t(vld4_lane_s64): Likewise.\n\t(vld4_lane_f16): Likewise.\n\t(vld4_lane_f32): Likewise.\n\t(vld4_lane_f64): Likewise.\n\t(vld4_lane_p8): Likewise.\n\t(vld4_lane_p16): Likewise.\n\t(vld4_lane_p64): Likewise.\n\t(vld4q_lane_u8): Likewise.\n\t(vld4q_lane_u16): Likewise.\n\t(vld4q_lane_u32): Likewise.\n\t(vld4q_lane_u64): Likewise.\n\t(vld4q_lane_s8): Likewise.\n\t(vld4q_lane_s16): Likewise.\n\t(vld4q_lane_s32): Likewise.\n\t(vld4q_lane_s64): Likewise.\n\t(vld4q_lane_f16): Likewise.\n\t(vld4q_lane_f32): Likewise.\n\t(vld4q_lane_f64): Likewise.\n\t(vld4q_lane_p8): Likewise.\n\t(vld4q_lane_p16): Likewise.\n\t(vld4q_lane_p64): Likewise.\n\t(vld4_lane_bf16): Likewise.\n\t(vld4q_lane_bf16): Likewise.", "tree": {"sha": "1b2e13c465d8b6c4cf046b0cac740a7120c0b6e4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1b2e13c465d8b6c4cf046b0cac740a7120c0b6e4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/d1819df86fbe42125cccb2fc2959a0bf51e524d6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d1819df86fbe42125cccb2fc2959a0bf51e524d6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d1819df86fbe42125cccb2fc2959a0bf51e524d6", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d1819df86fbe42125cccb2fc2959a0bf51e524d6/comments", "author": {"login": "jwright-arm", "id": 31624044, "node_id": "MDQ6VXNlcjMxNjI0MDQ0", "avatar_url": "https://avatars.githubusercontent.com/u/31624044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwright-arm", "html_url": "https://github.com/jwright-arm", "followers_url": "https://api.github.com/users/jwright-arm/followers", "following_url": "https://api.github.com/users/jwright-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jwright-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwright-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwright-arm/subscriptions", "organizations_url": "https://api.github.com/users/jwright-arm/orgs", "repos_url": "https://api.github.com/users/jwright-arm/repos", "events_url": "https://api.github.com/users/jwright-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jwright-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jwright-arm", "id": 31624044, "node_id": "MDQ6VXNlcjMxNjI0MDQ0", "avatar_url": "https://avatars.githubusercontent.com/u/31624044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwright-arm", "html_url": "https://github.com/jwright-arm", "followers_url": "https://api.github.com/users/jwright-arm/followers", "following_url": "https://api.github.com/users/jwright-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jwright-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwright-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwright-arm/subscriptions", "organizations_url": "https://api.github.com/users/jwright-arm/orgs", "repos_url": "https://api.github.com/users/jwright-arm/repos", "events_url": "https://api.github.com/users/jwright-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jwright-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "08f83812e5c5fdd9a7a4a1b9e46bb33725185c5a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/08f83812e5c5fdd9a7a4a1b9e46bb33725185c5a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/08f83812e5c5fdd9a7a4a1b9e46bb33725185c5a"}], "stats": {"total": 728, "additions": 624, "deletions": 104}, "files": [{"sha": "d8b29706a2078f4be374d4c2b0d5882d820ba8e0", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 624, "deletions": 104, "changes": 728, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d1819df86fbe42125cccb2fc2959a0bf51e524d6/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d1819df86fbe42125cccb2fc2959a0bf51e524d6/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=d1819df86fbe42125cccb2fc2959a0bf51e524d6", "patch": "@@ -20856,110 +20856,595 @@ vld3q_lane_p64 (const poly64_t * __ptr, poly64x2x3_t __b, const int __c)\n \n /* vld4_lane */\n \n-#define __LD4_LANE_FUNC(intype, vectype, largetype, ptrtype, mode,\t   \\\n-\t\t\t qmode, ptrmode, funcsuffix, signedtype)\t   \\\n-__extension__ extern __inline intype \\\n-__attribute__ ((__always_inline__, __gnu_inline__,__artificial__)) \\\n-vld4_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c)  \\\n-{\t\t\t\t\t\t\t\t\t   \\\n-  __builtin_aarch64_simd_xi __o;\t\t\t\t\t   \\\n-  largetype __temp;\t\t\t\t\t\t\t   \\\n-  __temp.val[0] =\t\t\t\t\t\t\t   \\\n-    vcombine_##funcsuffix (__b.val[0], vcreate_##funcsuffix (0));\t   \\\n-  __temp.val[1] =\t\t\t\t\t\t\t   \\\n-    vcombine_##funcsuffix (__b.val[1], vcreate_##funcsuffix (0));\t   \\\n-  __temp.val[2] =\t\t\t\t\t\t\t   \\\n-    vcombine_##funcsuffix (__b.val[2], vcreate_##funcsuffix (0));\t   \\\n-  __temp.val[3] =\t\t\t\t\t\t\t   \\\n-    vcombine_##funcsuffix (__b.val[3], vcreate_##funcsuffix (0));\t   \\\n-  __o = __builtin_aarch64_set_qregxi##qmode (__o,\t\t\t   \\\n-\t\t\t\t\t    (signedtype) __temp.val[0],\t   \\\n-\t\t\t\t\t    0);\t\t\t\t   \\\n-  __o = __builtin_aarch64_set_qregxi##qmode (__o,\t\t\t   \\\n-\t\t\t\t\t    (signedtype) __temp.val[1],\t   \\\n-\t\t\t\t\t    1);\t\t\t\t   \\\n-  __o = __builtin_aarch64_set_qregxi##qmode (__o,\t\t\t   \\\n-\t\t\t\t\t    (signedtype) __temp.val[2],\t   \\\n-\t\t\t\t\t    2);\t\t\t\t   \\\n-  __o = __builtin_aarch64_set_qregxi##qmode (__o,\t\t\t   \\\n-\t\t\t\t\t    (signedtype) __temp.val[3],\t   \\\n-\t\t\t\t\t    3);\t\t\t\t   \\\n-  __o =\t__builtin_aarch64_ld4_lane##mode (\t\t\t\t   \\\n-\t  (__builtin_aarch64_simd_##ptrmode *) __ptr, __o, __c);\t   \\\n-  __b.val[0] = (vectype) __builtin_aarch64_get_dregxidi (__o, 0);\t   \\\n-  __b.val[1] = (vectype) __builtin_aarch64_get_dregxidi (__o, 1);\t   \\\n-  __b.val[2] = (vectype) __builtin_aarch64_get_dregxidi (__o, 2);\t   \\\n-  __b.val[3] = (vectype) __builtin_aarch64_get_dregxidi (__o, 3);\t   \\\n-  return __b;\t\t\t\t\t\t\t\t   \\\n+__extension__ extern __inline uint8x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_u8 (const uint8_t * __ptr, uint8x8x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  uint8x16x4_t __temp;\n+  __temp.val[0] = vcombine_u8 (__b.val[0], vcreate_u8 (0));\n+  __temp.val[1] = vcombine_u8 (__b.val[1], vcreate_u8 (0));\n+  __temp.val[2] = vcombine_u8 (__b.val[2], vcreate_u8 (0));\n+  __temp.val[3] = vcombine_u8 (__b.val[3], vcreate_u8 (0));\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev8qi (\n+\t  (__builtin_aarch64_simd_qi *) __ptr, __o, __c);\n+  __b.val[0] = (uint8x8_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (uint8x8_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (uint8x8_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (uint8x8_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n }\n \n-/* vld4q_lane */\n+__extension__ extern __inline uint16x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_u16 (const uint16_t * __ptr, uint16x4x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  uint16x8x4_t __temp;\n+  __temp.val[0] = vcombine_u16 (__b.val[0], vcreate_u16 (0));\n+  __temp.val[1] = vcombine_u16 (__b.val[1], vcreate_u16 (0));\n+  __temp.val[2] = vcombine_u16 (__b.val[2], vcreate_u16 (0));\n+  __temp.val[3] = vcombine_u16 (__b.val[3], vcreate_u16 (0));\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev4hi (\n+\t  (__builtin_aarch64_simd_hi *) __ptr, __o, __c);\n+  __b.val[0] = (uint16x4_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (uint16x4_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (uint16x4_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (uint16x4_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline uint32x2x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_u32 (const uint32_t * __ptr, uint32x2x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  uint32x4x4_t __temp;\n+  __temp.val[0] = vcombine_u32 (__b.val[0], vcreate_u32 (0));\n+  __temp.val[1] = vcombine_u32 (__b.val[1], vcreate_u32 (0));\n+  __temp.val[2] = vcombine_u32 (__b.val[2], vcreate_u32 (0));\n+  __temp.val[3] = vcombine_u32 (__b.val[3], vcreate_u32 (0));\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev2si (\n+\t  (__builtin_aarch64_simd_si *) __ptr, __o, __c);\n+  __b.val[0] = (uint32x2_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (uint32x2_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (uint32x2_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (uint32x2_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline uint64x1x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_u64 (const uint64_t * __ptr, uint64x1x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  uint64x2x4_t __temp;\n+  __temp.val[0] = vcombine_u64 (__b.val[0], vcreate_u64 (0));\n+  __temp.val[1] = vcombine_u64 (__b.val[1], vcreate_u64 (0));\n+  __temp.val[2] = vcombine_u64 (__b.val[2], vcreate_u64 (0));\n+  __temp.val[3] = vcombine_u64 (__b.val[3], vcreate_u64 (0));\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanedi (\n+\t  (__builtin_aarch64_simd_di *) __ptr, __o, __c);\n+  __b.val[0] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline int8x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_s8 (const int8_t * __ptr, int8x8x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  int8x16x4_t __temp;\n+  __temp.val[0] = vcombine_s8 (__b.val[0], vcreate_s8 (0));\n+  __temp.val[1] = vcombine_s8 (__b.val[1], vcreate_s8 (0));\n+  __temp.val[2] = vcombine_s8 (__b.val[2], vcreate_s8 (0));\n+  __temp.val[3] = vcombine_s8 (__b.val[3], vcreate_s8 (0));\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev8qi (\n+\t  (__builtin_aarch64_simd_qi *) __ptr, __o, __c);\n+  __b.val[0] = (int8x8_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (int8x8_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (int8x8_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (int8x8_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline int16x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_s16 (const int16_t * __ptr, int16x4x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  int16x8x4_t __temp;\n+  __temp.val[0] = vcombine_s16 (__b.val[0], vcreate_s16 (0));\n+  __temp.val[1] = vcombine_s16 (__b.val[1], vcreate_s16 (0));\n+  __temp.val[2] = vcombine_s16 (__b.val[2], vcreate_s16 (0));\n+  __temp.val[3] = vcombine_s16 (__b.val[3], vcreate_s16 (0));\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev4hi (\n+\t  (__builtin_aarch64_simd_hi *) __ptr, __o, __c);\n+  __b.val[0] = (int16x4_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (int16x4_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (int16x4_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (int16x4_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline int32x2x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_s32 (const int32_t * __ptr, int32x2x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  int32x4x4_t __temp;\n+  __temp.val[0] = vcombine_s32 (__b.val[0], vcreate_s32 (0));\n+  __temp.val[1] = vcombine_s32 (__b.val[1], vcreate_s32 (0));\n+  __temp.val[2] = vcombine_s32 (__b.val[2], vcreate_s32 (0));\n+  __temp.val[3] = vcombine_s32 (__b.val[3], vcreate_s32 (0));\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev2si (\n+\t  (__builtin_aarch64_simd_si *) __ptr, __o, __c);\n+  __b.val[0] = (int32x2_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (int32x2_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (int32x2_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (int32x2_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline int64x1x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_s64 (const int64_t * __ptr, int64x1x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  int64x2x4_t __temp;\n+  __temp.val[0] = vcombine_s64 (__b.val[0], vcreate_s64 (0));\n+  __temp.val[1] = vcombine_s64 (__b.val[1], vcreate_s64 (0));\n+  __temp.val[2] = vcombine_s64 (__b.val[2], vcreate_s64 (0));\n+  __temp.val[3] = vcombine_s64 (__b.val[3], vcreate_s64 (0));\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanedi (\n+\t  (__builtin_aarch64_simd_di *) __ptr, __o, __c);\n+  __b.val[0] = (int64x1_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (int64x1_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (int64x1_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (int64x1_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline float16x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_f16 (const float16_t * __ptr, float16x4x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  float16x8x4_t __temp;\n+  __temp.val[0] = vcombine_f16 (__b.val[0], vcreate_f16 (0));\n+  __temp.val[1] = vcombine_f16 (__b.val[1], vcreate_f16 (0));\n+  __temp.val[2] = vcombine_f16 (__b.val[2], vcreate_f16 (0));\n+  __temp.val[3] = vcombine_f16 (__b.val[3], vcreate_f16 (0));\n+  __o = __builtin_aarch64_set_qregxiv8hf (__o, (float16x8_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv8hf (__o, (float16x8_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv8hf (__o, (float16x8_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv8hf (__o, (float16x8_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev4hf (\n+\t  (__builtin_aarch64_simd_hf *) __ptr, __o, __c);\n+  __b.val[0] = (float16x4_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (float16x4_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (float16x4_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (float16x4_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline float32x2x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_f32 (const float32_t * __ptr, float32x2x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  float32x4x4_t __temp;\n+  __temp.val[0] = vcombine_f32 (__b.val[0], vcreate_f32 (0));\n+  __temp.val[1] = vcombine_f32 (__b.val[1], vcreate_f32 (0));\n+  __temp.val[2] = vcombine_f32 (__b.val[2], vcreate_f32 (0));\n+  __temp.val[3] = vcombine_f32 (__b.val[3], vcreate_f32 (0));\n+  __o = __builtin_aarch64_set_qregxiv4sf (__o, (float32x4_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4sf (__o, (float32x4_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4sf (__o, (float32x4_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4sf (__o, (float32x4_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev2si (\n+\t  (__builtin_aarch64_simd_sf *) __ptr, __o, __c);\n+  __b.val[0] = (float32x2_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (float32x2_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (float32x2_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (float32x2_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline float64x1x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_f64 (const float64_t * __ptr, float64x1x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  float64x2x4_t __temp;\n+  __temp.val[0] = vcombine_f64 (__b.val[0], vcreate_f64 (0));\n+  __temp.val[1] = vcombine_f64 (__b.val[1], vcreate_f64 (0));\n+  __temp.val[2] = vcombine_f64 (__b.val[2], vcreate_f64 (0));\n+  __temp.val[3] = vcombine_f64 (__b.val[3], vcreate_f64 (0));\n+  __o = __builtin_aarch64_set_qregxiv2df (__o, (float64x2_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv2df (__o, (float64x2_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv2df (__o, (float64x2_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv2df (__o, (float64x2_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanedf (\n+\t  (__builtin_aarch64_simd_df *) __ptr, __o, __c);\n+  __b.val[0] = (float64x1_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (float64x1_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (float64x1_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (float64x1_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline poly8x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_p8 (const poly8_t * __ptr, poly8x8x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  poly8x16x4_t __temp;\n+  __temp.val[0] = vcombine_p8 (__b.val[0], vcreate_p8 (0));\n+  __temp.val[1] = vcombine_p8 (__b.val[1], vcreate_p8 (0));\n+  __temp.val[2] = vcombine_p8 (__b.val[2], vcreate_p8 (0));\n+  __temp.val[3] = vcombine_p8 (__b.val[3], vcreate_p8 (0));\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv16qi (__o, (int8x16_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev8qi (\n+\t  (__builtin_aarch64_simd_qi *) __ptr, __o, __c);\n+  __b.val[0] = (poly8x8_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (poly8x8_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (poly8x8_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (poly8x8_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n \n-__LD4_LANE_FUNC (float16x4x4_t, float16x4_t, float16x8x4_t, float16_t, v4hf,\n-\t\t v8hf, hf, f16, float16x8_t)\n-__LD4_LANE_FUNC (float32x2x4_t, float32x2_t, float32x4x4_t, float32_t, v2sf, v4sf,\n-\t\t sf, f32, float32x4_t)\n-__LD4_LANE_FUNC (float64x1x4_t, float64x1_t, float64x2x4_t, float64_t, df, v2df,\n-\t\t df, f64, float64x2_t)\n-__LD4_LANE_FUNC (poly8x8x4_t, poly8x8_t, poly8x16x4_t, poly8_t, v8qi, v16qi, qi, p8,\n-\t\t int8x16_t)\n-__LD4_LANE_FUNC (poly16x4x4_t, poly16x4_t, poly16x8x4_t, poly16_t, v4hi, v8hi, hi,\n-\t\t p16, int16x8_t)\n-__LD4_LANE_FUNC (poly64x1x4_t, poly64x1_t, poly64x2x4_t, poly64_t, di,\n-\t\t v2di_ssps, di, p64, poly64x2_t)\n-__LD4_LANE_FUNC (int8x8x4_t, int8x8_t, int8x16x4_t, int8_t, v8qi, v16qi, qi, s8,\n-\t\t int8x16_t)\n-__LD4_LANE_FUNC (int16x4x4_t, int16x4_t, int16x8x4_t, int16_t, v4hi, v8hi, hi, s16,\n-\t\t int16x8_t)\n-__LD4_LANE_FUNC (int32x2x4_t, int32x2_t, int32x4x4_t, int32_t, v2si, v4si, si, s32,\n-\t\t int32x4_t)\n-__LD4_LANE_FUNC (int64x1x4_t, int64x1_t, int64x2x4_t, int64_t, di, v2di, di, s64,\n-\t\t int64x2_t)\n-__LD4_LANE_FUNC (uint8x8x4_t, uint8x8_t, uint8x16x4_t, uint8_t, v8qi, v16qi, qi, u8,\n-\t\t int8x16_t)\n-__LD4_LANE_FUNC (uint16x4x4_t, uint16x4_t, uint16x8x4_t, uint16_t, v4hi, v8hi, hi,\n-\t\t u16, int16x8_t)\n-__LD4_LANE_FUNC (uint32x2x4_t, uint32x2_t, uint32x4x4_t, uint32_t, v2si, v4si, si,\n-\t\t u32, int32x4_t)\n-__LD4_LANE_FUNC (uint64x1x4_t, uint64x1_t, uint64x2x4_t, uint64_t, di, v2di, di,\n-\t\t u64, int64x2_t)\n+__extension__ extern __inline poly16x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_p16 (const poly16_t * __ptr, poly16x4x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  poly16x8x4_t __temp;\n+  __temp.val[0] = vcombine_p16 (__b.val[0], vcreate_p16 (0));\n+  __temp.val[1] = vcombine_p16 (__b.val[1], vcreate_p16 (0));\n+  __temp.val[2] = vcombine_p16 (__b.val[2], vcreate_p16 (0));\n+  __temp.val[3] = vcombine_p16 (__b.val[3], vcreate_p16 (0));\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv8hi (__o, (int16x8_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev4hi (\n+\t  (__builtin_aarch64_simd_hi *) __ptr, __o, __c);\n+  __b.val[0] = (poly16x4_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (poly16x4_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (poly16x4_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (poly16x4_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline poly64x1x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_p64 (const poly64_t * __ptr, poly64x1x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  poly64x2x4_t __temp;\n+  __temp.val[0] = vcombine_p64 (__b.val[0], vcreate_p64 (0));\n+  __temp.val[1] = vcombine_p64 (__b.val[1], vcreate_p64 (0));\n+  __temp.val[2] = vcombine_p64 (__b.val[2], vcreate_p64 (0));\n+  __temp.val[3] = vcombine_p64 (__b.val[3], vcreate_p64 (0));\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv2di (__o, (int64x2_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanedi (\n+\t  (__builtin_aarch64_simd_di *) __ptr, __o, __c);\n+  __b.val[0] = (poly64x1_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (poly64x1_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (poly64x1_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (poly64x1_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n \n /* vld4q_lane */\n \n-#define __LD4Q_LANE_FUNC(intype, vtype, ptrtype, mode, ptrmode, funcsuffix) \\\n-__extension__ extern __inline intype \\\n-__attribute__ ((__always_inline__, __gnu_inline__,__artificial__)) \\\n-vld4q_lane_##funcsuffix (const ptrtype * __ptr, intype __b, const int __c) \\\n-{\t\t\t\t\t\t\t\t\t   \\\n-  __builtin_aarch64_simd_xi __o;\t\t\t\t\t   \\\n-  intype ret;\t\t\t\t\t\t\t\t   \\\n-  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0); \\\n-  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1); \\\n-  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2); \\\n-  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3); \\\n-  __o = __builtin_aarch64_ld4_lane##mode (\t\t\t\t   \\\n-\t(__builtin_aarch64_simd_##ptrmode *) __ptr, __o, __c);\t\t   \\\n-  ret.val[0] = (vtype) __builtin_aarch64_get_qregxiv4si (__o, 0);\t   \\\n-  ret.val[1] = (vtype) __builtin_aarch64_get_qregxiv4si (__o, 1);\t   \\\n-  ret.val[2] = (vtype) __builtin_aarch64_get_qregxiv4si (__o, 2);\t   \\\n-  ret.val[3] = (vtype) __builtin_aarch64_get_qregxiv4si (__o, 3);\t   \\\n-  return ret;\t\t\t\t\t\t\t\t   \\\n-}\n-\n-__LD4Q_LANE_FUNC (float16x8x4_t, float16x8_t, float16_t, v8hf, hf, f16)\n-__LD4Q_LANE_FUNC (float32x4x4_t, float32x4_t, float32_t, v4sf, sf, f32)\n-__LD4Q_LANE_FUNC (float64x2x4_t, float64x2_t, float64_t, v2df, df, f64)\n-__LD4Q_LANE_FUNC (poly8x16x4_t, poly8x16_t, poly8_t, v16qi, qi, p8)\n-__LD4Q_LANE_FUNC (poly16x8x4_t, poly16x8_t, poly16_t, v8hi, hi, p16)\n-__LD4Q_LANE_FUNC (poly64x2x4_t, poly64x2_t, poly64_t, v2di, di, p64)\n-__LD4Q_LANE_FUNC (int8x16x4_t, int8x16_t, int8_t, v16qi, qi, s8)\n-__LD4Q_LANE_FUNC (int16x8x4_t, int16x8_t, int16_t, v8hi, hi, s16)\n-__LD4Q_LANE_FUNC (int32x4x4_t, int32x4_t, int32_t, v4si, si, s32)\n-__LD4Q_LANE_FUNC (int64x2x4_t, int64x2_t, int64_t, v2di, di, s64)\n-__LD4Q_LANE_FUNC (uint8x16x4_t, uint8x16_t, uint8_t, v16qi, qi, u8)\n-__LD4Q_LANE_FUNC (uint16x8x4_t, uint16x8_t, uint16_t, v8hi, hi, u16)\n-__LD4Q_LANE_FUNC (uint32x4x4_t, uint32x4_t, uint32_t, v4si, si, u32)\n-__LD4Q_LANE_FUNC (uint64x2x4_t, uint64x2_t, uint64_t, v2di, di, u64)\n+__extension__ extern __inline uint8x16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_u8 (const uint8_t * __ptr, uint8x16x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  uint8x16x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev16qi (\n+\t(__builtin_aarch64_simd_qi *) __ptr, __o, __c);\n+  ret.val[0] = (uint8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (uint8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (uint8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (uint8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline uint16x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_u16 (const uint16_t * __ptr, uint16x8x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  uint16x8x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev8hi (\n+\t(__builtin_aarch64_simd_hi *) __ptr, __o, __c);\n+  ret.val[0] = (uint16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (uint16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (uint16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (uint16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline uint32x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_u32 (const uint32_t * __ptr, uint32x4x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  uint32x4x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev4si (\n+\t(__builtin_aarch64_simd_si *) __ptr, __o, __c);\n+  ret.val[0] = (uint32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (uint32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (uint32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (uint32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline uint64x2x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_u64 (const uint64_t * __ptr, uint64x2x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  uint64x2x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev2di (\n+\t(__builtin_aarch64_simd_di *) __ptr, __o, __c);\n+  ret.val[0] = (uint64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (uint64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (uint64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (uint64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline int8x16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_s8 (const int8_t * __ptr, int8x16x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  int8x16x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev16qi (\n+\t(__builtin_aarch64_simd_qi *) __ptr, __o, __c);\n+  ret.val[0] = (int8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (int8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (int8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (int8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline int16x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_s16 (const int16_t * __ptr, int16x8x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  int16x8x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev8hi (\n+\t(__builtin_aarch64_simd_hi *) __ptr, __o, __c);\n+  ret.val[0] = (int16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (int16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (int16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (int16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline int32x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_s32 (const int32_t * __ptr, int32x4x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  int32x4x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev4si (\n+\t(__builtin_aarch64_simd_si *) __ptr, __o, __c);\n+  ret.val[0] = (int32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (int32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (int32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (int32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline int64x2x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_s64 (const int64_t * __ptr, int64x2x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  int64x2x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev2di (\n+\t(__builtin_aarch64_simd_di *) __ptr, __o, __c);\n+  ret.val[0] = (int64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (int64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (int64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (int64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline float16x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_f16 (const float16_t * __ptr, float16x8x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  float16x8x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev8hf (\n+\t(__builtin_aarch64_simd_hf *) __ptr, __o, __c);\n+  ret.val[0] = (float16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (float16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (float16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (float16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline float32x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_f32 (const float32_t * __ptr, float32x4x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  float32x4x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev4sf (\n+\t(__builtin_aarch64_simd_sf *) __ptr, __o, __c);\n+  ret.val[0] = (float32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (float32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (float32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (float32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline float64x2x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_f64 (const float64_t * __ptr, float64x2x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  float64x2x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev2df (\n+\t(__builtin_aarch64_simd_di *) __ptr, __o, __c);\n+  ret.val[0] = (float64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (float64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (float64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (float64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline poly8x16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_p8 (const poly8_t * __ptr, poly8x16x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  poly8x16x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev16qi (\n+\t(__builtin_aarch64_simd_qi *) __ptr, __o, __c);\n+  ret.val[0] = (poly8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (poly8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (poly8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (poly8x16_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline poly16x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_p16 (const poly16_t * __ptr, poly16x8x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  poly16x8x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev8hi (\n+\t(__builtin_aarch64_simd_hi *) __ptr, __o, __c);\n+  ret.val[0] = (poly16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (poly16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (poly16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (poly16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline poly64x2x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_p64 (const poly64_t * __ptr, poly64x2x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  poly64x2x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev2di (\n+\t(__builtin_aarch64_simd_di *) __ptr, __o, __c);\n+  ret.val[0] = (poly64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (poly64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (poly64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (poly64x2_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n \n /* vmax */\n \n@@ -35441,9 +35926,47 @@ vld3q_lane_bf16 (const bfloat16_t * __ptr, bfloat16x8x3_t __b, const int __c)\n   return ret;\n }\n \n-__LD4_LANE_FUNC (bfloat16x4x4_t, bfloat16x4_t, bfloat16x8x4_t, bfloat16_t, v4bf,\n-\t\t v8bf, bf, bf16, bfloat16x8_t)\n-__LD4Q_LANE_FUNC (bfloat16x8x4_t, bfloat16x8_t, bfloat16_t, v8bf, bf, bf16)\n+__extension__ extern __inline bfloat16x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4_lane_bf16 (const bfloat16_t * __ptr, bfloat16x4x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  bfloat16x8x4_t __temp;\n+  __temp.val[0] = vcombine_bf16 (__b.val[0], vcreate_bf16 (0));\n+  __temp.val[1] = vcombine_bf16 (__b.val[1], vcreate_bf16 (0));\n+  __temp.val[2] = vcombine_bf16 (__b.val[2], vcreate_bf16 (0));\n+  __temp.val[3] = vcombine_bf16 (__b.val[3], vcreate_bf16 (0));\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __temp.val[3], 3);\n+  __o =\t__builtin_aarch64_ld4_lanev4bf (\n+\t  (__builtin_aarch64_simd_bf *) __ptr, __o, __c);\n+  __b.val[0] = (bfloat16x4_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  __b.val[1] = (bfloat16x4_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  __b.val[2] = (bfloat16x4_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  __b.val[3] = (bfloat16x4_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return __b;\n+}\n+\n+__extension__ extern __inline bfloat16x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__,__artificial__))\n+vld4q_lane_bf16 (const bfloat16_t * __ptr, bfloat16x8x4_t __b, const int __c)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  bfloat16x8x4_t ret;\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv4si (__o, (int32x4_t) __b.val[3], 3);\n+  __o = __builtin_aarch64_ld4_lanev8bf (\n+\t(__builtin_aarch64_simd_bf *) __ptr, __o, __c);\n+  ret.val[0] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n \n __extension__ extern __inline void\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n@@ -35739,7 +36262,4 @@ vaddq_p128 (poly128_t __a, poly128_t __b)\n #undef __aarch64_vdupq_laneq_u32\n #undef __aarch64_vdupq_laneq_u64\n \n-#undef __LD4_LANE_FUNC\n-#undef __LD4Q_LANE_FUNC\n-\n #endif"}]}