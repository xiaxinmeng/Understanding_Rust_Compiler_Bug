{"sha": "43cacb12fc859b671464b63668794158974b2a34", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NDNjYWNiMTJmYzg1OWI2NzE0NjRiNjM2Njg3OTQxNTg5NzRiMmEzNA==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2018-01-13T17:50:35Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2018-01-13T17:50:35Z"}, "message": "[AArch64] Add SVE support\n\nThis patch adds support for ARM's Scalable Vector Extension.\nThe patch just contains the core features that work with the\ncurrent vectoriser framework; later patches will add extra\ncapabilities to both the target-independent code and AArch64 code.\nThe patch doesn't include:\n\n- support for unwinding frames whose size depends on the vector length\n- modelling the effect of __tls_get_addr on the SVE registers\n\nThese are handled by later patches instead.\n\nSome notes:\n\n- The copyright years for aarch64-sve.md start at 2009 because some of\n  the code is based on aarch64.md, which also starts from then.\n\n- The patch inserts spaces between items in the AArch64 section\n  of sourcebuild.texi.  This matches at least the surrounding\n  architectures and looks a little nicer in the info output.\n\n- aarch64-sve.md includes a pattern:\n\n    while_ult<GPI:mode><PRED_ALL:mode>\n\n  A later patch adds a matching \"while_ult\" optab, but the pattern\n  is also needed by the predicate vec_duplicate expander.\n\n2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood  <david.sherwood@arm.com>\n\ngcc/\n\t* doc/invoke.texi (-msve-vector-bits=): Document new option.\n\t(sve): Document new AArch64 extension.\n\t* doc/md.texi (w): Extend the description of the AArch64\n\tconstraint to include SVE vectors.\n\t(Upl, Upa): Document new AArch64 predicate constraints.\n\t* config/aarch64/aarch64-opts.h (aarch64_sve_vector_bits_enum): New\n\tenum.\n\t* config/aarch64/aarch64.opt (sve_vector_bits): New enum.\n\t(msve-vector-bits=): New option.\n\t* config/aarch64/aarch64-option-extensions.def (fp, simd): Disable\n\tSVE when these are disabled.\n\t(sve): New extension.\n\t* config/aarch64/aarch64-modes.def: Define SVE vector and predicate\n\tmodes.  Adjust their number of units based on aarch64_sve_vg.\n\t(MAX_BITSIZE_MODE_ANY_MODE): Define.\n\t* config/aarch64/aarch64-protos.h (ADDR_QUERY_ANY): New\n\taarch64_addr_query_type.\n\t(aarch64_const_vec_all_same_in_range_p, aarch64_sve_pred_mode)\n\t(aarch64_sve_cnt_immediate_p, aarch64_sve_addvl_addpl_immediate_p)\n\t(aarch64_sve_inc_dec_immediate_p, aarch64_add_offset_temporaries)\n\t(aarch64_split_add_offset, aarch64_output_sve_cnt_immediate)\n\t(aarch64_output_sve_addvl_addpl, aarch64_output_sve_inc_dec_immediate)\n\t(aarch64_output_sve_mov_immediate, aarch64_output_ptrue): Declare.\n\t(aarch64_simd_imm_zero_p): Delete.\n\t(aarch64_check_zero_based_sve_index_immediate): Declare.\n\t(aarch64_sve_index_immediate_p, aarch64_sve_arith_immediate_p)\n\t(aarch64_sve_bitmask_immediate_p, aarch64_sve_dup_immediate_p)\n\t(aarch64_sve_cmp_immediate_p, aarch64_sve_float_arith_immediate_p)\n\t(aarch64_sve_float_mul_immediate_p): Likewise.\n\t(aarch64_classify_symbol): Take the offset as a HOST_WIDE_INT\n\trather than an rtx.\n\t(aarch64_sve_ld1r_operand_p, aarch64_sve_ldr_operand_p): Declare.\n\t(aarch64_expand_mov_immediate): Take a gen_vec_duplicate callback.\n\t(aarch64_emit_sve_pred_move, aarch64_expand_sve_mem_move): Declare.\n\t(aarch64_expand_sve_vec_cmp_int, aarch64_expand_sve_vec_cmp_float)\n\t(aarch64_expand_sve_vcond, aarch64_expand_sve_vec_perm): Declare.\n\t(aarch64_regmode_natural_size): Likewise.\n\t* config/aarch64/aarch64.h (AARCH64_FL_SVE): New macro.\n\t(AARCH64_FL_V8_3, AARCH64_FL_RCPC, AARCH64_FL_DOTPROD): Shift\n\tleft one place.\n\t(AARCH64_ISA_SVE, TARGET_SVE): New macros.\n\t(FIXED_REGISTERS, CALL_USED_REGISTERS, REGISTER_NAMES): Add entries\n\tfor VG and the SVE predicate registers.\n\t(V_ALIASES): Add a \"z\"-prefixed alias.\n\t(FIRST_PSEUDO_REGISTER): Change to P15_REGNUM + 1.\n\t(AARCH64_DWARF_VG, AARCH64_DWARF_P0): New macros.\n\t(PR_REGNUM_P, PR_LO_REGNUM_P): Likewise.\n\t(PR_LO_REGS, PR_HI_REGS, PR_REGS): New reg_classes.\n\t(REG_CLASS_NAMES): Add entries for them.\n\t(REG_CLASS_CONTENTS): Likewise.  Update ALL_REGS to include VG\n\tand the predicate registers.\n\t(aarch64_sve_vg): Declare.\n\t(BITS_PER_SVE_VECTOR, BYTES_PER_SVE_VECTOR, BYTES_PER_SVE_PRED)\n\t(SVE_BYTE_MODE, MAX_COMPILE_TIME_VEC_BYTES): New macros.\n\t(REGMODE_NATURAL_SIZE): Define.\n\t* config/aarch64/aarch64-c.c (aarch64_update_cpp_builtins): Handle\n\tSVE macros.\n\t* config/aarch64/aarch64.c: Include cfgrtl.h.\n\t(simd_immediate_info): Add a constructor for series vectors,\n\tand an associated step field.\n\t(aarch64_sve_vg): New variable.\n\t(aarch64_dbx_register_number): Handle VG and the predicate registers.\n\t(aarch64_vect_struct_mode_p, aarch64_vector_mode_p): Delete.\n\t(VEC_ADVSIMD, VEC_SVE_DATA, VEC_SVE_PRED, VEC_STRUCT, VEC_ANY_SVE)\n\t(VEC_ANY_DATA, VEC_STRUCT): New constants.\n\t(aarch64_advsimd_struct_mode_p, aarch64_sve_pred_mode_p)\n\t(aarch64_classify_vector_mode, aarch64_vector_data_mode_p)\n\t(aarch64_sve_data_mode_p, aarch64_sve_pred_mode)\n\t(aarch64_get_mask_mode): New functions.\n\t(aarch64_hard_regno_nregs): Handle SVE data modes for FP_REGS\n\tand FP_LO_REGS.  Handle PR_REGS, PR_LO_REGS and PR_HI_REGS.\n\t(aarch64_hard_regno_mode_ok): Handle VG.  Also handle the SVE\n\tpredicate modes and predicate registers.  Explicitly restrict\n\tGPRs to modes of 16 bytes or smaller.  Only allow FP registers\n\tto store a vector mode if it is recognized by\n\taarch64_classify_vector_mode.\n\t(aarch64_regmode_natural_size): New function.\n\t(aarch64_hard_regno_caller_save_mode): Return the original mode\n\tfor predicates.\n\t(aarch64_sve_cnt_immediate_p, aarch64_output_sve_cnt_immediate)\n\t(aarch64_sve_addvl_addpl_immediate_p, aarch64_output_sve_addvl_addpl)\n\t(aarch64_sve_inc_dec_immediate_p, aarch64_output_sve_inc_dec_immediate)\n\t(aarch64_add_offset_1_temporaries, aarch64_offset_temporaries): New\n\tfunctions.\n\t(aarch64_add_offset): Add a temp2 parameter.  Assert that temp1\n\tdoes not overlap dest if the function is frame-related.  Handle\n\tSVE constants.\n\t(aarch64_split_add_offset): New function.\n\t(aarch64_add_sp, aarch64_sub_sp): Add temp2 parameters and pass\n\tthem aarch64_add_offset.\n\t(aarch64_allocate_and_probe_stack_space): Add a temp2 parameter\n\tand update call to aarch64_sub_sp.\n\t(aarch64_add_cfa_expression): New function.\n\t(aarch64_expand_prologue): Pass extra temporary registers to the\n\tfunctions above.  Handle the case in which we need to emit new\n\tDW_CFA_expressions for registers that were originally saved\n\trelative to the stack pointer, but now have to be expressed\n\trelative to the frame pointer.\n\t(aarch64_output_mi_thunk): Pass extra temporary registers to the\n\tfunctions above.\n\t(aarch64_expand_epilogue): Likewise.  Prevent inheritance of\n\tIP0 and IP1 values for SVE frames.\n\t(aarch64_expand_vec_series): New function.\n\t(aarch64_expand_sve_widened_duplicate): Likewise.\n\t(aarch64_expand_sve_const_vector): Likewise.\n\t(aarch64_expand_mov_immediate): Add a gen_vec_duplicate parameter.\n\tHandle SVE constants.  Use emit_move_insn to move a force_const_mem\n\tinto the register, rather than emitting a SET directly.\n\t(aarch64_emit_sve_pred_move, aarch64_expand_sve_mem_move)\n\t(aarch64_get_reg_raw_mode, offset_4bit_signed_scaled_p)\n\t(offset_6bit_unsigned_scaled_p, aarch64_offset_7bit_signed_scaled_p)\n\t(offset_9bit_signed_scaled_p): New functions.\n\t(aarch64_replicate_bitmask_imm): New function.\n\t(aarch64_bitmask_imm): Use it.\n\t(aarch64_cannot_force_const_mem): Reject expressions involving\n\ta CONST_POLY_INT.  Update call to aarch64_classify_symbol.\n\t(aarch64_classify_index): Handle SVE indices, by requiring\n\ta plain register index with a scale that matches the element size.\n\t(aarch64_classify_address): Handle SVE addresses.  Assert that\n\tthe mode of the address is VOIDmode or an integer mode.\n\tUpdate call to aarch64_classify_symbol.\n\t(aarch64_classify_symbolic_expression): Update call to\n\taarch64_classify_symbol.\n\t(aarch64_const_vec_all_in_range_p): New function.\n\t(aarch64_print_vector_float_operand): Likewise.\n\t(aarch64_print_operand): Handle 'N' and 'C'.  Use \"zN\" rather than\n\t\"vN\" for FP registers with SVE modes.  Handle (const ...) vectors\n\tand the FP immediates 1.0 and 0.5.\n\t(aarch64_print_address_internal): Handle SVE addresses.\n\t(aarch64_print_operand_address): Use ADDR_QUERY_ANY.\n\t(aarch64_regno_regclass): Handle predicate registers.\n\t(aarch64_secondary_reload): Handle big-endian reloads of SVE\n\tdata modes.\n\t(aarch64_class_max_nregs): Handle SVE modes and predicate registers.\n\t(aarch64_rtx_costs): Check for ADDVL and ADDPL instructions.\n\t(aarch64_convert_sve_vector_bits): New function.\n\t(aarch64_override_options): Use it to handle -msve-vector-bits=.\n\t(aarch64_classify_symbol): Take the offset as a HOST_WIDE_INT\n\trather than an rtx.\n\t(aarch64_legitimate_constant_p): Use aarch64_classify_vector_mode.\n\tHandle SVE vector and predicate modes.  Accept VL-based constants\n\tthat need only one temporary register, and VL offsets that require\n\tno temporary registers.\n\t(aarch64_conditional_register_usage): Mark the predicate registers\n\tas fixed if SVE isn't available.\n\t(aarch64_vector_mode_supported_p): Use aarch64_classify_vector_mode.\n\tReturn true for SVE vector and predicate modes.\n\t(aarch64_simd_container_mode): Take the number of bits as a poly_int64\n\trather than an unsigned int.  Handle SVE modes.\n\t(aarch64_preferred_simd_mode): Update call accordingly.  Handle\n\tSVE modes.\n\t(aarch64_autovectorize_vector_sizes): Add BYTES_PER_SVE_VECTOR\n\tif SVE is enabled.\n\t(aarch64_sve_index_immediate_p, aarch64_sve_arith_immediate_p)\n\t(aarch64_sve_bitmask_immediate_p, aarch64_sve_dup_immediate_p)\n\t(aarch64_sve_cmp_immediate_p, aarch64_sve_float_arith_immediate_p)\n\t(aarch64_sve_float_mul_immediate_p): New functions.\n\t(aarch64_sve_valid_immediate): New function.\n\t(aarch64_simd_valid_immediate): Use it as the fallback for SVE vectors.\n\tExplicitly reject structure modes.  Check for INDEX constants.\n\tHandle PTRUE and PFALSE constants.\n\t(aarch64_check_zero_based_sve_index_immediate): New function.\n\t(aarch64_simd_imm_zero_p): Delete.\n\t(aarch64_mov_operand_p): Use aarch64_simd_valid_immediate for\n\tvector modes.  Accept constants in the range of CNT[BHWD].\n\t(aarch64_simd_scalar_immediate_valid_for_move): Explicitly\n\task for an Advanced SIMD mode.\n\t(aarch64_sve_ld1r_operand_p, aarch64_sve_ldr_operand_p): New functions.\n\t(aarch64_simd_vector_alignment): Handle SVE predicates.\n\t(aarch64_vectorize_preferred_vector_alignment): New function.\n\t(aarch64_simd_vector_alignment_reachable): Use it instead of\n\tthe vector size.\n\t(aarch64_shift_truncation_mask): Use aarch64_vector_data_mode_p.\n\t(aarch64_output_sve_mov_immediate, aarch64_output_ptrue): New\n\tfunctions.\n\t(MAX_VECT_LEN): Delete.\n\t(expand_vec_perm_d): Add a vec_flags field.\n\t(emit_unspec2, aarch64_expand_sve_vec_perm): New functions.\n\t(aarch64_evpc_trn, aarch64_evpc_uzp, aarch64_evpc_zip)\n\t(aarch64_evpc_ext): Don't apply a big-endian lane correction\n\tfor SVE modes.\n\t(aarch64_evpc_rev): Rename to...\n\t(aarch64_evpc_rev_local): ...this.  Use a predicated operation for SVE.\n\t(aarch64_evpc_rev_global): New function.\n\t(aarch64_evpc_dup): Enforce a 64-byte range for SVE DUP.\n\t(aarch64_evpc_tbl): Use MAX_COMPILE_TIME_VEC_BYTES instead of\n\tMAX_VECT_LEN.\n\t(aarch64_evpc_sve_tbl): New function.\n\t(aarch64_expand_vec_perm_const_1): Update after rename of\n\taarch64_evpc_rev.  Handle SVE permutes too, trying\n\taarch64_evpc_rev_global and using aarch64_evpc_sve_tbl rather\n\tthan aarch64_evpc_tbl.\n\t(aarch64_vectorize_vec_perm_const): Initialize vec_flags.\n\t(aarch64_sve_cmp_operand_p, aarch64_unspec_cond_code)\n\t(aarch64_gen_unspec_cond, aarch64_expand_sve_vec_cmp_int)\n\t(aarch64_emit_unspec_cond, aarch64_emit_unspec_cond_or)\n\t(aarch64_emit_inverted_unspec_cond, aarch64_expand_sve_vec_cmp_float)\n\t(aarch64_expand_sve_vcond): New functions.\n\t(aarch64_modes_tieable_p): Use aarch64_vector_data_mode_p instead\n\tof aarch64_vector_mode_p.\n\t(aarch64_dwarf_poly_indeterminate_value): New function.\n\t(aarch64_compute_pressure_classes): Likewise.\n\t(aarch64_can_change_mode_class): Likewise.\n\t(TARGET_GET_RAW_RESULT_MODE, TARGET_GET_RAW_ARG_MODE): Redefine.\n\t(TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT): Likewise.\n\t(TARGET_VECTORIZE_GET_MASK_MODE): Likewise.\n\t(TARGET_DWARF_POLY_INDETERMINATE_VALUE): Likewise.\n\t(TARGET_COMPUTE_PRESSURE_CLASSES): Likewise.\n\t(TARGET_CAN_CHANGE_MODE_CLASS): Likewise.\n\t* config/aarch64/constraints.md (Upa, Upl, Uav, Uat, Usv, Usi, Utr)\n\t(Uty, Dm, vsa, vsc, vsd, vsi, vsn, vsl, vsm, vsA, vsM, vsN): New\n\tconstraints.\n\t(Dn, Dl, Dr): Accept const as well as const_vector.\n\t(Dz): Likewise.  Compare against CONST0_RTX.\n\t* config/aarch64/iterators.md: Refer to \"Advanced SIMD\" instead\n\tof \"vector\" where appropriate.\n\t(SVE_ALL, SVE_BH, SVE_BHS, SVE_BHSI, SVE_HSDI, SVE_HSF, SVE_SD)\n\t(SVE_SDI, SVE_I, SVE_F, PRED_ALL, PRED_BHS): New mode iterators.\n\t(UNSPEC_SEL, UNSPEC_ANDF, UNSPEC_IORF, UNSPEC_XORF, UNSPEC_COND_LT)\n\t(UNSPEC_COND_LE, UNSPEC_COND_EQ, UNSPEC_COND_NE, UNSPEC_COND_GE)\n\t(UNSPEC_COND_GT, UNSPEC_COND_LO, UNSPEC_COND_LS, UNSPEC_COND_HS)\n\t(UNSPEC_COND_HI, UNSPEC_COND_UO): New unspecs.\n\t(Vetype, VEL, Vel, VWIDE, Vwide, vw, vwcore, V_INT_EQUIV)\n\t(v_int_equiv): Extend to SVE modes.\n\t(Vesize, V128, v128, Vewtype, V_FP_EQUIV, v_fp_equiv, VPRED): New\n\tmode attributes.\n\t(LOGICAL_OR, SVE_INT_UNARY, SVE_FP_UNARY): New code iterators.\n\t(optab): Handle popcount, smin, smax, umin, umax, abs and sqrt.\n\t(logical_nn, lr, sve_int_op, sve_fp_op): New code attributs.\n\t(LOGICALF, OPTAB_PERMUTE, UNPACK, UNPACK_UNSIGNED, SVE_COND_INT_CMP)\n\t(SVE_COND_FP_CMP): New int iterators.\n\t(perm_hilo): Handle the new unpack unspecs.\n\t(optab, logicalf_op, su, perm_optab, cmp_op, imm_con): New int\n\tattributes.\n\t* config/aarch64/predicates.md (aarch64_sve_cnt_immediate)\n\t(aarch64_sve_addvl_addpl_immediate, aarch64_split_add_offset_immediate)\n\t(aarch64_pluslong_or_poly_operand, aarch64_nonmemory_operand)\n\t(aarch64_equality_operator, aarch64_constant_vector_operand)\n\t(aarch64_sve_ld1r_operand, aarch64_sve_ldr_operand): New predicates.\n\t(aarch64_sve_nonimmediate_operand): Likewise.\n\t(aarch64_sve_general_operand): Likewise.\n\t(aarch64_sve_dup_operand, aarch64_sve_arith_immediate): Likewise.\n\t(aarch64_sve_sub_arith_immediate, aarch64_sve_inc_dec_immediate)\n\t(aarch64_sve_logical_immediate, aarch64_sve_mul_immediate): Likewise.\n\t(aarch64_sve_dup_immediate, aarch64_sve_cmp_vsc_immediate): Likewise.\n\t(aarch64_sve_cmp_vsd_immediate, aarch64_sve_index_immediate): Likewise.\n\t(aarch64_sve_float_arith_immediate): Likewise.\n\t(aarch64_sve_float_arith_with_sub_immediate): Likewise.\n\t(aarch64_sve_float_mul_immediate, aarch64_sve_arith_operand): Likewise.\n\t(aarch64_sve_add_operand, aarch64_sve_logical_operand): Likewise.\n\t(aarch64_sve_lshift_operand, aarch64_sve_rshift_operand): Likewise.\n\t(aarch64_sve_mul_operand, aarch64_sve_cmp_vsc_operand): Likewise.\n\t(aarch64_sve_cmp_vsd_operand, aarch64_sve_index_operand): Likewise.\n\t(aarch64_sve_float_arith_operand): Likewise.\n\t(aarch64_sve_float_arith_with_sub_operand): Likewise.\n\t(aarch64_sve_float_mul_operand): Likewise.\n\t(aarch64_sve_vec_perm_operand): Likewise.\n\t(aarch64_pluslong_operand): Include aarch64_sve_addvl_addpl_immediate.\n\t(aarch64_mov_operand): Accept const_poly_int and const_vector.\n\t(aarch64_simd_lshift_imm, aarch64_simd_rshift_imm): Accept const\n\tas well as const_vector.\n\t(aarch64_simd_imm_zero, aarch64_simd_imm_minus_one): Move earlier\n\tin file.  Use CONST0_RTX and CONSTM1_RTX.\n\t(aarch64_simd_or_scalar_imm_zero): Likewise.  Add match_codes.\n\t(aarch64_simd_reg_or_zero): Accept const as well as const_vector.\n\tUse aarch64_simd_imm_zero.\n\t* config/aarch64/aarch64-sve.md: New file.\n\t* config/aarch64/aarch64.md: Include it.\n\t(VG_REGNUM, P0_REGNUM, P7_REGNUM, P15_REGNUM): New register numbers.\n\t(UNSPEC_REV, UNSPEC_LD1_SVE, UNSPEC_ST1_SVE, UNSPEC_MERGE_PTRUE)\n\t(UNSPEC_PTEST_PTRUE, UNSPEC_UNPACKSHI, UNSPEC_UNPACKUHI)\n\t(UNSPEC_UNPACKSLO, UNSPEC_UNPACKULO, UNSPEC_PACK)\n\t(UNSPEC_FLOAT_CONVERT, UNSPEC_WHILE_LO): New unspec constants.\n\t(sve): New attribute.\n\t(enabled): Disable instructions with the sve attribute unless\n\tTARGET_SVE.\n\t(movqi, movhi): Pass CONST_POLY_INT operaneds through\n\taarch64_expand_mov_immediate.\n\t(*mov<mode>_aarch64, *movsi_aarch64, *movdi_aarch64): Handle\n\tCNT[BHSD] immediates.\n\t(movti): Split CONST_POLY_INT moves into two halves.\n\t(add<mode>3): Accept aarch64_pluslong_or_poly_operand.\n\tSplit additions that need a temporary here if the destination\n\tis the stack pointer.\n\t(*add<mode>3_aarch64): Handle ADDVL and ADDPL immediates.\n\t(*add<mode>3_poly_1): New instruction.\n\t(set_clobber_cc): New expander.\n\nReviewed-by: James Greenhalgh <james.greenhalgh@arm.com>\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r256612", "tree": {"sha": "dfb16013b4bceb9d0886750b889e31f9f7d916e3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/dfb16013b4bceb9d0886750b889e31f9f7d916e3"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/43cacb12fc859b671464b63668794158974b2a34", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/43cacb12fc859b671464b63668794158974b2a34", "html_url": "https://github.com/Rust-GCC/gccrs/commit/43cacb12fc859b671464b63668794158974b2a34", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/43cacb12fc859b671464b63668794158974b2a34/comments", "author": null, "committer": null, "parents": [{"sha": "11e0322aead708df5f572f5d3c50d27103f8c9a8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/11e0322aead708df5f572f5d3c50d27103f8c9a8", "html_url": "https://github.com/Rust-GCC/gccrs/commit/11e0322aead708df5f572f5d3c50d27103f8c9a8"}], "stats": {"total": 5720, "additions": 5367, "deletions": 353}, "files": [{"sha": "40da1eb477aecbe47c5fe94ab388ae0114e565da", "filename": "gcc/ChangeLog", "status": "modified", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -1,3 +1,295 @@\n+2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* doc/invoke.texi (-msve-vector-bits=): Document new option.\n+\t(sve): Document new AArch64 extension.\n+\t* doc/md.texi (w): Extend the description of the AArch64\n+\tconstraint to include SVE vectors.\n+\t(Upl, Upa): Document new AArch64 predicate constraints.\n+\t* config/aarch64/aarch64-opts.h (aarch64_sve_vector_bits_enum): New\n+\tenum.\n+\t* config/aarch64/aarch64.opt (sve_vector_bits): New enum.\n+\t(msve-vector-bits=): New option.\n+\t* config/aarch64/aarch64-option-extensions.def (fp, simd): Disable\n+\tSVE when these are disabled.\n+\t(sve): New extension.\n+\t* config/aarch64/aarch64-modes.def: Define SVE vector and predicate\n+\tmodes.  Adjust their number of units based on aarch64_sve_vg.\n+\t(MAX_BITSIZE_MODE_ANY_MODE): Define.\n+\t* config/aarch64/aarch64-protos.h (ADDR_QUERY_ANY): New\n+\taarch64_addr_query_type.\n+\t(aarch64_const_vec_all_same_in_range_p, aarch64_sve_pred_mode)\n+\t(aarch64_sve_cnt_immediate_p, aarch64_sve_addvl_addpl_immediate_p)\n+\t(aarch64_sve_inc_dec_immediate_p, aarch64_add_offset_temporaries)\n+\t(aarch64_split_add_offset, aarch64_output_sve_cnt_immediate)\n+\t(aarch64_output_sve_addvl_addpl, aarch64_output_sve_inc_dec_immediate)\n+\t(aarch64_output_sve_mov_immediate, aarch64_output_ptrue): Declare.\n+\t(aarch64_simd_imm_zero_p): Delete.\n+\t(aarch64_check_zero_based_sve_index_immediate): Declare.\n+\t(aarch64_sve_index_immediate_p, aarch64_sve_arith_immediate_p)\n+\t(aarch64_sve_bitmask_immediate_p, aarch64_sve_dup_immediate_p)\n+\t(aarch64_sve_cmp_immediate_p, aarch64_sve_float_arith_immediate_p)\n+\t(aarch64_sve_float_mul_immediate_p): Likewise.\n+\t(aarch64_classify_symbol): Take the offset as a HOST_WIDE_INT\n+\trather than an rtx.\n+\t(aarch64_sve_ld1r_operand_p, aarch64_sve_ldr_operand_p): Declare.\n+\t(aarch64_expand_mov_immediate): Take a gen_vec_duplicate callback.\n+\t(aarch64_emit_sve_pred_move, aarch64_expand_sve_mem_move): Declare.\n+\t(aarch64_expand_sve_vec_cmp_int, aarch64_expand_sve_vec_cmp_float)\n+\t(aarch64_expand_sve_vcond, aarch64_expand_sve_vec_perm): Declare.\n+\t(aarch64_regmode_natural_size): Likewise.\n+\t* config/aarch64/aarch64.h (AARCH64_FL_SVE): New macro.\n+\t(AARCH64_FL_V8_3, AARCH64_FL_RCPC, AARCH64_FL_DOTPROD): Shift\n+\tleft one place.\n+\t(AARCH64_ISA_SVE, TARGET_SVE): New macros.\n+\t(FIXED_REGISTERS, CALL_USED_REGISTERS, REGISTER_NAMES): Add entries\n+\tfor VG and the SVE predicate registers.\n+\t(V_ALIASES): Add a \"z\"-prefixed alias.\n+\t(FIRST_PSEUDO_REGISTER): Change to P15_REGNUM + 1.\n+\t(AARCH64_DWARF_VG, AARCH64_DWARF_P0): New macros.\n+\t(PR_REGNUM_P, PR_LO_REGNUM_P): Likewise.\n+\t(PR_LO_REGS, PR_HI_REGS, PR_REGS): New reg_classes.\n+\t(REG_CLASS_NAMES): Add entries for them.\n+\t(REG_CLASS_CONTENTS): Likewise.  Update ALL_REGS to include VG\n+\tand the predicate registers.\n+\t(aarch64_sve_vg): Declare.\n+\t(BITS_PER_SVE_VECTOR, BYTES_PER_SVE_VECTOR, BYTES_PER_SVE_PRED)\n+\t(SVE_BYTE_MODE, MAX_COMPILE_TIME_VEC_BYTES): New macros.\n+\t(REGMODE_NATURAL_SIZE): Define.\n+\t* config/aarch64/aarch64-c.c (aarch64_update_cpp_builtins): Handle\n+\tSVE macros.\n+\t* config/aarch64/aarch64.c: Include cfgrtl.h.\n+\t(simd_immediate_info): Add a constructor for series vectors,\n+\tand an associated step field.\n+\t(aarch64_sve_vg): New variable.\n+\t(aarch64_dbx_register_number): Handle VG and the predicate registers.\n+\t(aarch64_vect_struct_mode_p, aarch64_vector_mode_p): Delete.\n+\t(VEC_ADVSIMD, VEC_SVE_DATA, VEC_SVE_PRED, VEC_STRUCT, VEC_ANY_SVE)\n+\t(VEC_ANY_DATA, VEC_STRUCT): New constants.\n+\t(aarch64_advsimd_struct_mode_p, aarch64_sve_pred_mode_p)\n+\t(aarch64_classify_vector_mode, aarch64_vector_data_mode_p)\n+\t(aarch64_sve_data_mode_p, aarch64_sve_pred_mode)\n+\t(aarch64_get_mask_mode): New functions.\n+\t(aarch64_hard_regno_nregs): Handle SVE data modes for FP_REGS\n+\tand FP_LO_REGS.  Handle PR_REGS, PR_LO_REGS and PR_HI_REGS.\n+\t(aarch64_hard_regno_mode_ok): Handle VG.  Also handle the SVE\n+\tpredicate modes and predicate registers.  Explicitly restrict\n+\tGPRs to modes of 16 bytes or smaller.  Only allow FP registers\n+\tto store a vector mode if it is recognized by\n+\taarch64_classify_vector_mode.\n+\t(aarch64_regmode_natural_size): New function.\n+\t(aarch64_hard_regno_caller_save_mode): Return the original mode\n+\tfor predicates.\n+\t(aarch64_sve_cnt_immediate_p, aarch64_output_sve_cnt_immediate)\n+\t(aarch64_sve_addvl_addpl_immediate_p, aarch64_output_sve_addvl_addpl)\n+\t(aarch64_sve_inc_dec_immediate_p, aarch64_output_sve_inc_dec_immediate)\n+\t(aarch64_add_offset_1_temporaries, aarch64_offset_temporaries): New\n+\tfunctions.\n+\t(aarch64_add_offset): Add a temp2 parameter.  Assert that temp1\n+\tdoes not overlap dest if the function is frame-related.  Handle\n+\tSVE constants.\n+\t(aarch64_split_add_offset): New function.\n+\t(aarch64_add_sp, aarch64_sub_sp): Add temp2 parameters and pass\n+\tthem aarch64_add_offset.\n+\t(aarch64_allocate_and_probe_stack_space): Add a temp2 parameter\n+\tand update call to aarch64_sub_sp.\n+\t(aarch64_add_cfa_expression): New function.\n+\t(aarch64_expand_prologue): Pass extra temporary registers to the\n+\tfunctions above.  Handle the case in which we need to emit new\n+\tDW_CFA_expressions for registers that were originally saved\n+\trelative to the stack pointer, but now have to be expressed\n+\trelative to the frame pointer.\n+\t(aarch64_output_mi_thunk): Pass extra temporary registers to the\n+\tfunctions above.\n+\t(aarch64_expand_epilogue): Likewise.  Prevent inheritance of\n+\tIP0 and IP1 values for SVE frames.\n+\t(aarch64_expand_vec_series): New function.\n+\t(aarch64_expand_sve_widened_duplicate): Likewise.\n+\t(aarch64_expand_sve_const_vector): Likewise.\n+\t(aarch64_expand_mov_immediate): Add a gen_vec_duplicate parameter.\n+\tHandle SVE constants.  Use emit_move_insn to move a force_const_mem\n+\tinto the register, rather than emitting a SET directly.\n+\t(aarch64_emit_sve_pred_move, aarch64_expand_sve_mem_move)\n+\t(aarch64_get_reg_raw_mode, offset_4bit_signed_scaled_p)\n+\t(offset_6bit_unsigned_scaled_p, aarch64_offset_7bit_signed_scaled_p)\n+\t(offset_9bit_signed_scaled_p): New functions.\n+\t(aarch64_replicate_bitmask_imm): New function.\n+\t(aarch64_bitmask_imm): Use it.\n+\t(aarch64_cannot_force_const_mem): Reject expressions involving\n+\ta CONST_POLY_INT.  Update call to aarch64_classify_symbol.\n+\t(aarch64_classify_index): Handle SVE indices, by requiring\n+\ta plain register index with a scale that matches the element size.\n+\t(aarch64_classify_address): Handle SVE addresses.  Assert that\n+\tthe mode of the address is VOIDmode or an integer mode.\n+\tUpdate call to aarch64_classify_symbol.\n+\t(aarch64_classify_symbolic_expression): Update call to\n+\taarch64_classify_symbol.\n+\t(aarch64_const_vec_all_in_range_p): New function.\n+\t(aarch64_print_vector_float_operand): Likewise.\n+\t(aarch64_print_operand): Handle 'N' and 'C'.  Use \"zN\" rather than\n+\t\"vN\" for FP registers with SVE modes.  Handle (const ...) vectors\n+\tand the FP immediates 1.0 and 0.5.\n+\t(aarch64_print_address_internal): Handle SVE addresses.\n+\t(aarch64_print_operand_address): Use ADDR_QUERY_ANY.\n+\t(aarch64_regno_regclass): Handle predicate registers.\n+\t(aarch64_secondary_reload): Handle big-endian reloads of SVE\n+\tdata modes.\n+\t(aarch64_class_max_nregs): Handle SVE modes and predicate registers.\n+\t(aarch64_rtx_costs): Check for ADDVL and ADDPL instructions.\n+\t(aarch64_convert_sve_vector_bits): New function.\n+\t(aarch64_override_options): Use it to handle -msve-vector-bits=.\n+\t(aarch64_classify_symbol): Take the offset as a HOST_WIDE_INT\n+\trather than an rtx.\n+\t(aarch64_legitimate_constant_p): Use aarch64_classify_vector_mode.\n+\tHandle SVE vector and predicate modes.  Accept VL-based constants\n+\tthat need only one temporary register, and VL offsets that require\n+\tno temporary registers.\n+\t(aarch64_conditional_register_usage): Mark the predicate registers\n+\tas fixed if SVE isn't available.\n+\t(aarch64_vector_mode_supported_p): Use aarch64_classify_vector_mode.\n+\tReturn true for SVE vector and predicate modes.\n+\t(aarch64_simd_container_mode): Take the number of bits as a poly_int64\n+\trather than an unsigned int.  Handle SVE modes.\n+\t(aarch64_preferred_simd_mode): Update call accordingly.  Handle\n+\tSVE modes.\n+\t(aarch64_autovectorize_vector_sizes): Add BYTES_PER_SVE_VECTOR\n+\tif SVE is enabled.\n+\t(aarch64_sve_index_immediate_p, aarch64_sve_arith_immediate_p)\n+\t(aarch64_sve_bitmask_immediate_p, aarch64_sve_dup_immediate_p)\n+\t(aarch64_sve_cmp_immediate_p, aarch64_sve_float_arith_immediate_p)\n+\t(aarch64_sve_float_mul_immediate_p): New functions.\n+\t(aarch64_sve_valid_immediate): New function.\n+\t(aarch64_simd_valid_immediate): Use it as the fallback for SVE vectors.\n+\tExplicitly reject structure modes.  Check for INDEX constants.\n+\tHandle PTRUE and PFALSE constants.\n+\t(aarch64_check_zero_based_sve_index_immediate): New function.\n+\t(aarch64_simd_imm_zero_p): Delete.\n+\t(aarch64_mov_operand_p): Use aarch64_simd_valid_immediate for\n+\tvector modes.  Accept constants in the range of CNT[BHWD].\n+\t(aarch64_simd_scalar_immediate_valid_for_move): Explicitly\n+\task for an Advanced SIMD mode.\n+\t(aarch64_sve_ld1r_operand_p, aarch64_sve_ldr_operand_p): New functions.\n+\t(aarch64_simd_vector_alignment): Handle SVE predicates.\n+\t(aarch64_vectorize_preferred_vector_alignment): New function.\n+\t(aarch64_simd_vector_alignment_reachable): Use it instead of\n+\tthe vector size.\n+\t(aarch64_shift_truncation_mask): Use aarch64_vector_data_mode_p.\n+\t(aarch64_output_sve_mov_immediate, aarch64_output_ptrue): New\n+\tfunctions.\n+\t(MAX_VECT_LEN): Delete.\n+\t(expand_vec_perm_d): Add a vec_flags field.\n+\t(emit_unspec2, aarch64_expand_sve_vec_perm): New functions.\n+\t(aarch64_evpc_trn, aarch64_evpc_uzp, aarch64_evpc_zip)\n+\t(aarch64_evpc_ext): Don't apply a big-endian lane correction\n+\tfor SVE modes.\n+\t(aarch64_evpc_rev): Rename to...\n+\t(aarch64_evpc_rev_local): ...this.  Use a predicated operation for SVE.\n+\t(aarch64_evpc_rev_global): New function.\n+\t(aarch64_evpc_dup): Enforce a 64-byte range for SVE DUP.\n+\t(aarch64_evpc_tbl): Use MAX_COMPILE_TIME_VEC_BYTES instead of\n+\tMAX_VECT_LEN.\n+\t(aarch64_evpc_sve_tbl): New function.\n+\t(aarch64_expand_vec_perm_const_1): Update after rename of\n+\taarch64_evpc_rev.  Handle SVE permutes too, trying\n+\taarch64_evpc_rev_global and using aarch64_evpc_sve_tbl rather\n+\tthan aarch64_evpc_tbl.\n+\t(aarch64_vectorize_vec_perm_const): Initialize vec_flags.\n+\t(aarch64_sve_cmp_operand_p, aarch64_unspec_cond_code)\n+\t(aarch64_gen_unspec_cond, aarch64_expand_sve_vec_cmp_int)\n+\t(aarch64_emit_unspec_cond, aarch64_emit_unspec_cond_or)\n+\t(aarch64_emit_inverted_unspec_cond, aarch64_expand_sve_vec_cmp_float)\n+\t(aarch64_expand_sve_vcond): New functions.\n+\t(aarch64_modes_tieable_p): Use aarch64_vector_data_mode_p instead\n+\tof aarch64_vector_mode_p.\n+\t(aarch64_dwarf_poly_indeterminate_value): New function.\n+\t(aarch64_compute_pressure_classes): Likewise.\n+\t(aarch64_can_change_mode_class): Likewise.\n+\t(TARGET_GET_RAW_RESULT_MODE, TARGET_GET_RAW_ARG_MODE): Redefine.\n+\t(TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT): Likewise.\n+\t(TARGET_VECTORIZE_GET_MASK_MODE): Likewise.\n+\t(TARGET_DWARF_POLY_INDETERMINATE_VALUE): Likewise.\n+\t(TARGET_COMPUTE_PRESSURE_CLASSES): Likewise.\n+\t(TARGET_CAN_CHANGE_MODE_CLASS): Likewise.\n+\t* config/aarch64/constraints.md (Upa, Upl, Uav, Uat, Usv, Usi, Utr)\n+\t(Uty, Dm, vsa, vsc, vsd, vsi, vsn, vsl, vsm, vsA, vsM, vsN): New\n+\tconstraints.\n+\t(Dn, Dl, Dr): Accept const as well as const_vector.\n+\t(Dz): Likewise.  Compare against CONST0_RTX.\n+\t* config/aarch64/iterators.md: Refer to \"Advanced SIMD\" instead\n+\tof \"vector\" where appropriate.\n+\t(SVE_ALL, SVE_BH, SVE_BHS, SVE_BHSI, SVE_HSDI, SVE_HSF, SVE_SD)\n+\t(SVE_SDI, SVE_I, SVE_F, PRED_ALL, PRED_BHS): New mode iterators.\n+\t(UNSPEC_SEL, UNSPEC_ANDF, UNSPEC_IORF, UNSPEC_XORF, UNSPEC_COND_LT)\n+\t(UNSPEC_COND_LE, UNSPEC_COND_EQ, UNSPEC_COND_NE, UNSPEC_COND_GE)\n+\t(UNSPEC_COND_GT, UNSPEC_COND_LO, UNSPEC_COND_LS, UNSPEC_COND_HS)\n+\t(UNSPEC_COND_HI, UNSPEC_COND_UO): New unspecs.\n+\t(Vetype, VEL, Vel, VWIDE, Vwide, vw, vwcore, V_INT_EQUIV)\n+\t(v_int_equiv): Extend to SVE modes.\n+\t(Vesize, V128, v128, Vewtype, V_FP_EQUIV, v_fp_equiv, VPRED): New\n+\tmode attributes.\n+\t(LOGICAL_OR, SVE_INT_UNARY, SVE_FP_UNARY): New code iterators.\n+\t(optab): Handle popcount, smin, smax, umin, umax, abs and sqrt.\n+\t(logical_nn, lr, sve_int_op, sve_fp_op): New code attributs.\n+\t(LOGICALF, OPTAB_PERMUTE, UNPACK, UNPACK_UNSIGNED, SVE_COND_INT_CMP)\n+\t(SVE_COND_FP_CMP): New int iterators.\n+\t(perm_hilo): Handle the new unpack unspecs.\n+\t(optab, logicalf_op, su, perm_optab, cmp_op, imm_con): New int\n+\tattributes.\n+\t* config/aarch64/predicates.md (aarch64_sve_cnt_immediate)\n+\t(aarch64_sve_addvl_addpl_immediate, aarch64_split_add_offset_immediate)\n+\t(aarch64_pluslong_or_poly_operand, aarch64_nonmemory_operand)\n+\t(aarch64_equality_operator, aarch64_constant_vector_operand)\n+\t(aarch64_sve_ld1r_operand, aarch64_sve_ldr_operand): New predicates.\n+\t(aarch64_sve_nonimmediate_operand): Likewise.\n+\t(aarch64_sve_general_operand): Likewise.\n+\t(aarch64_sve_dup_operand, aarch64_sve_arith_immediate): Likewise.\n+\t(aarch64_sve_sub_arith_immediate, aarch64_sve_inc_dec_immediate)\n+\t(aarch64_sve_logical_immediate, aarch64_sve_mul_immediate): Likewise.\n+\t(aarch64_sve_dup_immediate, aarch64_sve_cmp_vsc_immediate): Likewise.\n+\t(aarch64_sve_cmp_vsd_immediate, aarch64_sve_index_immediate): Likewise.\n+\t(aarch64_sve_float_arith_immediate): Likewise.\n+\t(aarch64_sve_float_arith_with_sub_immediate): Likewise.\n+\t(aarch64_sve_float_mul_immediate, aarch64_sve_arith_operand): Likewise.\n+\t(aarch64_sve_add_operand, aarch64_sve_logical_operand): Likewise.\n+\t(aarch64_sve_lshift_operand, aarch64_sve_rshift_operand): Likewise.\n+\t(aarch64_sve_mul_operand, aarch64_sve_cmp_vsc_operand): Likewise.\n+\t(aarch64_sve_cmp_vsd_operand, aarch64_sve_index_operand): Likewise.\n+\t(aarch64_sve_float_arith_operand): Likewise.\n+\t(aarch64_sve_float_arith_with_sub_operand): Likewise.\n+\t(aarch64_sve_float_mul_operand): Likewise.\n+\t(aarch64_sve_vec_perm_operand): Likewise.\n+\t(aarch64_pluslong_operand): Include aarch64_sve_addvl_addpl_immediate.\n+\t(aarch64_mov_operand): Accept const_poly_int and const_vector.\n+\t(aarch64_simd_lshift_imm, aarch64_simd_rshift_imm): Accept const\n+\tas well as const_vector.\n+\t(aarch64_simd_imm_zero, aarch64_simd_imm_minus_one): Move earlier\n+\tin file.  Use CONST0_RTX and CONSTM1_RTX.\n+\t(aarch64_simd_or_scalar_imm_zero): Likewise.  Add match_codes.\n+\t(aarch64_simd_reg_or_zero): Accept const as well as const_vector.\n+\tUse aarch64_simd_imm_zero.\n+\t* config/aarch64/aarch64-sve.md: New file.\n+\t* config/aarch64/aarch64.md: Include it.\n+\t(VG_REGNUM, P0_REGNUM, P7_REGNUM, P15_REGNUM): New register numbers.\n+\t(UNSPEC_REV, UNSPEC_LD1_SVE, UNSPEC_ST1_SVE, UNSPEC_MERGE_PTRUE)\n+\t(UNSPEC_PTEST_PTRUE, UNSPEC_UNPACKSHI, UNSPEC_UNPACKUHI)\n+\t(UNSPEC_UNPACKSLO, UNSPEC_UNPACKULO, UNSPEC_PACK)\n+\t(UNSPEC_FLOAT_CONVERT, UNSPEC_WHILE_LO): New unspec constants.\n+\t(sve): New attribute.\n+\t(enabled): Disable instructions with the sve attribute unless\n+\tTARGET_SVE.\n+\t(movqi, movhi): Pass CONST_POLY_INT operaneds through\n+\taarch64_expand_mov_immediate.\n+\t(*mov<mode>_aarch64, *movsi_aarch64, *movdi_aarch64): Handle\n+\tCNT[BHSD] immediates.\n+\t(movti): Split CONST_POLY_INT moves into two halves.\n+\t(add<mode>3): Accept aarch64_pluslong_or_poly_operand.\n+\tSplit additions that need a temporary here if the destination\n+\tis the stack pointer.\n+\t(*add<mode>3_aarch64): Handle ADDVL and ADDPL immediates.\n+\t(*add<mode>3_poly_1): New instruction.\n+\t(set_clobber_cc): New expander.\n+\n 2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n \n \t* simplify-rtx.c (simplify_immed_subreg): Add an inner_bytes"}, {"sha": "40c738c7c3b0fc09378dd8058f09e4e4fff33a6a", "filename": "gcc/config/aarch64/aarch64-c.c", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-c.c?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -136,6 +136,15 @@ aarch64_update_cpp_builtins (cpp_reader *pfile)\n \n   aarch64_def_or_undef (TARGET_CRYPTO, \"__ARM_FEATURE_CRYPTO\", pfile);\n   aarch64_def_or_undef (TARGET_SIMD_RDMA, \"__ARM_FEATURE_QRDMX\", pfile);\n+  aarch64_def_or_undef (TARGET_SVE, \"__ARM_FEATURE_SVE\", pfile);\n+  cpp_undef (pfile, \"__ARM_FEATURE_SVE_BITS\");\n+  if (TARGET_SVE)\n+    {\n+      int bits;\n+      if (!BITS_PER_SVE_VECTOR.is_constant (&bits))\n+\tbits = 0;\n+      builtin_define_with_int_value (\"__ARM_FEATURE_SVE_BITS\", bits);\n+    }\n \n   aarch64_def_or_undef (TARGET_AES, \"__ARM_FEATURE_AES\", pfile);\n   aarch64_def_or_undef (TARGET_SHA2, \"__ARM_FEATURE_SHA2\", pfile);"}, {"sha": "4e9da29d321567cd83ee0012bd96d900e16bad2c", "filename": "gcc/config/aarch64/aarch64-modes.def", "status": "modified", "additions": 50, "deletions": 0, "changes": 50, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -30,6 +30,22 @@ FLOAT_MODE (HF, 2, 0);\n ADJUST_FLOAT_FORMAT (HF, &ieee_half_format);\n \n /* Vector modes.  */\n+\n+VECTOR_BOOL_MODE (VNx16BI, 16, 2);\n+VECTOR_BOOL_MODE (VNx8BI, 8, 2);\n+VECTOR_BOOL_MODE (VNx4BI, 4, 2);\n+VECTOR_BOOL_MODE (VNx2BI, 2, 2);\n+\n+ADJUST_NUNITS (VNx16BI, aarch64_sve_vg * 8);\n+ADJUST_NUNITS (VNx8BI, aarch64_sve_vg * 4);\n+ADJUST_NUNITS (VNx4BI, aarch64_sve_vg * 2);\n+ADJUST_NUNITS (VNx2BI, aarch64_sve_vg);\n+\n+ADJUST_ALIGNMENT (VNx16BI, 2);\n+ADJUST_ALIGNMENT (VNx8BI, 2);\n+ADJUST_ALIGNMENT (VNx4BI, 2);\n+ADJUST_ALIGNMENT (VNx2BI, 2);\n+\n VECTOR_MODES (INT, 8);        /*       V8QI V4HI V2SI.  */\n VECTOR_MODES (INT, 16);       /* V16QI V8HI V4SI V2DI.  */\n VECTOR_MODES (FLOAT, 8);      /*                 V2SF.  */\n@@ -45,9 +61,43 @@ INT_MODE (OI, 32);\n INT_MODE (CI, 48);\n INT_MODE (XI, 64);\n \n+/* Define SVE modes for NVECS vectors.  VB, VH, VS and VD are the prefixes\n+   for 8-bit, 16-bit, 32-bit and 64-bit elements respectively.  It isn't\n+   strictly necessary to set the alignment here, since the default would\n+   be clamped to BIGGEST_ALIGNMENT anyhow, but it seems clearer.  */\n+#define SVE_MODES(NVECS, VB, VH, VS, VD) \\\n+  VECTOR_MODES_WITH_PREFIX (VNx, INT, 16 * NVECS); \\\n+  VECTOR_MODES_WITH_PREFIX (VNx, FLOAT, 16 * NVECS); \\\n+  \\\n+  ADJUST_NUNITS (VB##QI, aarch64_sve_vg * NVECS * 8); \\\n+  ADJUST_NUNITS (VH##HI, aarch64_sve_vg * NVECS * 4); \\\n+  ADJUST_NUNITS (VS##SI, aarch64_sve_vg * NVECS * 2); \\\n+  ADJUST_NUNITS (VD##DI, aarch64_sve_vg * NVECS); \\\n+  ADJUST_NUNITS (VH##HF, aarch64_sve_vg * NVECS * 4); \\\n+  ADJUST_NUNITS (VS##SF, aarch64_sve_vg * NVECS * 2); \\\n+  ADJUST_NUNITS (VD##DF, aarch64_sve_vg * NVECS); \\\n+  \\\n+  ADJUST_ALIGNMENT (VB##QI, 16); \\\n+  ADJUST_ALIGNMENT (VH##HI, 16); \\\n+  ADJUST_ALIGNMENT (VS##SI, 16); \\\n+  ADJUST_ALIGNMENT (VD##DI, 16); \\\n+  ADJUST_ALIGNMENT (VH##HF, 16); \\\n+  ADJUST_ALIGNMENT (VS##SF, 16); \\\n+  ADJUST_ALIGNMENT (VD##DF, 16);\n+\n+/* Give SVE vectors the names normally used for 256-bit vectors.\n+   The actual number depends on command-line flags.  */\n+SVE_MODES (1, VNx16, VNx8, VNx4, VNx2)\n+\n /* Quad float: 128-bit floating mode for long doubles.  */\n FLOAT_MODE (TF, 16, ieee_quad_format);\n \n+/* A 4-tuple of SVE vectors with the maximum -msve-vector-bits= setting.\n+   Note that this is a limit only on the compile-time sizes of modes;\n+   it is not a limit on the runtime sizes, since VL-agnostic code\n+   must work with arbitary vector lengths.  */\n+#define MAX_BITSIZE_MODE_ANY_MODE (2048 * 4)\n+\n /* Coefficient 1 is multiplied by the number of 128-bit chunks in an\n    SVE vector (referred to as \"VQ\") minus one.  */\n #define NUM_POLY_INT_COEFFS 2"}, {"sha": "5fe5e3f7dddf622a48a5b9458ef30449a886f395", "filename": "gcc/config/aarch64/aarch64-option-extensions.def", "status": "modified", "additions": 14, "deletions": 6, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-option-extensions.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-option-extensions.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-option-extensions.def?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -39,16 +39,19 @@\n    that are required.  Their order is not important.  */\n \n /* Enabling \"fp\" just enables \"fp\".\n-   Disabling \"fp\" also disables \"simd\", \"crypto\", \"fp16\", \"aes\", \"sha2\", \"sha3\", and sm3/sm4.  */\n+   Disabling \"fp\" also disables \"simd\", \"crypto\", \"fp16\", \"aes\", \"sha2\",\n+   \"sha3\", sm3/sm4 and \"sve\".  */\n AARCH64_OPT_EXTENSION(\"fp\", AARCH64_FL_FP, 0, AARCH64_FL_SIMD | AARCH64_FL_CRYPTO |\\\n \t\t      AARCH64_FL_F16 | AARCH64_FL_AES | AARCH64_FL_SHA2 |\\\n-\t\t      AARCH64_FL_SHA3 | AARCH64_FL_SM4, \"fp\")\n+\t\t      AARCH64_FL_SHA3 | AARCH64_FL_SM4 | AARCH64_FL_SVE, \"fp\")\n \n /* Enabling \"simd\" also enables \"fp\".\n-   Disabling \"simd\" also disables \"crypto\", \"dotprod\", \"aes\", \"sha2\", \"sha3\" and \"sm3/sm4\".  */\n+   Disabling \"simd\" also disables \"crypto\", \"dotprod\", \"aes\", \"sha2\", \"sha3\",\n+   \"sm3/sm4\" and \"sve\".  */\n AARCH64_OPT_EXTENSION(\"simd\", AARCH64_FL_SIMD, AARCH64_FL_FP, AARCH64_FL_CRYPTO |\\\n \t\t      AARCH64_FL_DOTPROD | AARCH64_FL_AES | AARCH64_FL_SHA2 |\\\n-\t\t      AARCH64_FL_SHA3 | AARCH64_FL_SM4, \"asimd\")\n+\t\t      AARCH64_FL_SHA3 | AARCH64_FL_SM4 | AARCH64_FL_SVE,\n+\t\t      \"asimd\")\n \n /* Enabling \"crypto\" also enables \"fp\" and \"simd\".\n    Disabling \"crypto\" disables \"crypto\", \"aes\", \"sha2\", \"sha3\" and \"sm3/sm4\".  */\n@@ -63,8 +66,9 @@ AARCH64_OPT_EXTENSION(\"crc\", AARCH64_FL_CRC, 0, 0, \"crc32\")\n AARCH64_OPT_EXTENSION(\"lse\", AARCH64_FL_LSE, 0, 0, \"atomics\")\n \n /* Enabling \"fp16\" also enables \"fp\".\n-   Disabling \"fp16\" disables \"fp16\" and \"fp16fml\".  */\n-AARCH64_OPT_EXTENSION(\"fp16\", AARCH64_FL_F16, AARCH64_FL_FP, AARCH64_FL_F16FML, \"fphp asimdhp\")\n+   Disabling \"fp16\" disables \"fp16\", \"fp16fml\" and \"sve\".  */\n+AARCH64_OPT_EXTENSION(\"fp16\", AARCH64_FL_F16, AARCH64_FL_FP,\n+\t\t      AARCH64_FL_F16FML | AARCH64_FL_SVE, \"fphp asimdhp\")\n \n /* Enabling or disabling \"rcpc\" only changes \"rcpc\".  */\n AARCH64_OPT_EXTENSION(\"rcpc\", AARCH64_FL_RCPC, 0, 0, \"lrcpc\")\n@@ -97,4 +101,8 @@ AARCH64_OPT_EXTENSION(\"sm4\", AARCH64_FL_SM4, AARCH64_FL_SIMD, 0, \"sm3 sm4\")\n    Disabling \"fp16fml\" just disables \"fp16fml\".  */\n AARCH64_OPT_EXTENSION(\"fp16fml\", AARCH64_FL_F16FML, AARCH64_FL_FP | AARCH64_FL_F16, 0, \"asimdfml\")\n \n+/* Enabling \"sve\" also enables \"fp16\", \"fp\" and \"simd\".\n+   Disabling \"sve\" just disables \"sve\".  */\n+AARCH64_OPT_EXTENSION(\"sve\", AARCH64_FL_SVE, AARCH64_FL_FP | AARCH64_FL_SIMD | AARCH64_FL_F16, 0, \"sve\")\n+\n #undef AARCH64_OPT_EXTENSION"}, {"sha": "7a5c6d7664f47b220840d7fdd4e68c5fedbb3d6e", "filename": "gcc/config/aarch64/aarch64-opts.h", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-opts.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-opts.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-opts.h?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -81,4 +81,14 @@ enum aarch64_function_type {\n   AARCH64_FUNCTION_ALL\n };\n \n+/* SVE vector register sizes.  */\n+enum aarch64_sve_vector_bits_enum {\n+  SVE_SCALABLE,\n+  SVE_128 = 128,\n+  SVE_256 = 256,\n+  SVE_512 = 512,\n+  SVE_1024 = 1024,\n+  SVE_2048 = 2048\n+};\n+\n #endif"}, {"sha": "4f1fc15d39dbff8741b9b2c698ea63396d62dea0", "filename": "gcc/config/aarch64/aarch64-protos.h", "status": "modified", "additions": 43, "deletions": 5, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -118,10 +118,17 @@ enum aarch64_symbol_type\n       (the rules are the same for both).\n \n    ADDR_QUERY_LDP_STP\n-      Query what is valid for a load/store pair.  */\n+      Query what is valid for a load/store pair.\n+\n+   ADDR_QUERY_ANY\n+      Query what is valid for at least one memory constraint, which may\n+      allow things that \"m\" doesn't.  For example, the SVE LDR and STR\n+      addressing modes allow a wider range of immediate offsets than \"m\"\n+      does.  */\n enum aarch64_addr_query_type {\n   ADDR_QUERY_M,\n-  ADDR_QUERY_LDP_STP\n+  ADDR_QUERY_LDP_STP,\n+  ADDR_QUERY_ANY\n };\n \n /* A set of tuning parameters contains references to size and time\n@@ -344,6 +351,8 @@ int aarch64_branch_cost (bool, bool);\n enum aarch64_symbol_type aarch64_classify_symbolic_expression (rtx);\n bool aarch64_can_const_movi_rtx_p (rtx x, machine_mode mode);\n bool aarch64_const_vec_all_same_int_p (rtx, HOST_WIDE_INT);\n+bool aarch64_const_vec_all_same_in_range_p (rtx, HOST_WIDE_INT,\n+\t\t\t\t\t    HOST_WIDE_INT);\n bool aarch64_constant_address_p (rtx);\n bool aarch64_emit_approx_div (rtx, rtx, rtx);\n bool aarch64_emit_approx_sqrt (rtx, rtx, bool);\n@@ -364,31 +373,49 @@ bool aarch64_legitimate_pic_operand_p (rtx);\n bool aarch64_mask_and_shift_for_ubfiz_p (scalar_int_mode, rtx, rtx);\n bool aarch64_zero_extend_const_eq (machine_mode, rtx, machine_mode, rtx);\n bool aarch64_move_imm (HOST_WIDE_INT, machine_mode);\n+opt_machine_mode aarch64_sve_pred_mode (unsigned int);\n+bool aarch64_sve_cnt_immediate_p (rtx);\n+bool aarch64_sve_addvl_addpl_immediate_p (rtx);\n+bool aarch64_sve_inc_dec_immediate_p (rtx);\n+int aarch64_add_offset_temporaries (rtx);\n+void aarch64_split_add_offset (scalar_int_mode, rtx, rtx, rtx, rtx, rtx);\n bool aarch64_mov_operand_p (rtx, machine_mode);\n rtx aarch64_reverse_mask (machine_mode, unsigned int);\n bool aarch64_offset_7bit_signed_scaled_p (machine_mode, poly_int64);\n+char *aarch64_output_sve_cnt_immediate (const char *, const char *, rtx);\n+char *aarch64_output_sve_addvl_addpl (rtx, rtx, rtx);\n+char *aarch64_output_sve_inc_dec_immediate (const char *, rtx);\n char *aarch64_output_scalar_simd_mov_immediate (rtx, scalar_int_mode);\n char *aarch64_output_simd_mov_immediate (rtx, unsigned,\n \t\t\tenum simd_immediate_check w = AARCH64_CHECK_MOV);\n+char *aarch64_output_sve_mov_immediate (rtx);\n+char *aarch64_output_ptrue (machine_mode, char);\n bool aarch64_pad_reg_upward (machine_mode, const_tree, bool);\n bool aarch64_regno_ok_for_base_p (int, bool);\n bool aarch64_regno_ok_for_index_p (int, bool);\n bool aarch64_reinterpret_float_as_int (rtx value, unsigned HOST_WIDE_INT *fail);\n bool aarch64_simd_check_vect_par_cnst_half (rtx op, machine_mode mode,\n \t\t\t\t\t    bool high);\n-bool aarch64_simd_imm_zero_p (rtx, machine_mode);\n bool aarch64_simd_scalar_immediate_valid_for_move (rtx, scalar_int_mode);\n bool aarch64_simd_shift_imm_p (rtx, machine_mode, bool);\n bool aarch64_simd_valid_immediate (rtx, struct simd_immediate_info *,\n \t\t\tenum simd_immediate_check w = AARCH64_CHECK_MOV);\n+rtx aarch64_check_zero_based_sve_index_immediate (rtx);\n+bool aarch64_sve_index_immediate_p (rtx);\n+bool aarch64_sve_arith_immediate_p (rtx, bool);\n+bool aarch64_sve_bitmask_immediate_p (rtx);\n+bool aarch64_sve_dup_immediate_p (rtx);\n+bool aarch64_sve_cmp_immediate_p (rtx, bool);\n+bool aarch64_sve_float_arith_immediate_p (rtx, bool);\n+bool aarch64_sve_float_mul_immediate_p (rtx);\n bool aarch64_split_dimode_const_store (rtx, rtx);\n bool aarch64_symbolic_address_p (rtx);\n bool aarch64_uimm12_shift (HOST_WIDE_INT);\n bool aarch64_use_return_insn_p (void);\n const char *aarch64_mangle_builtin_type (const_tree);\n const char *aarch64_output_casesi (rtx *);\n \n-enum aarch64_symbol_type aarch64_classify_symbol (rtx, rtx);\n+enum aarch64_symbol_type aarch64_classify_symbol (rtx, HOST_WIDE_INT);\n enum aarch64_symbol_type aarch64_classify_tls_symbol (rtx);\n enum reg_class aarch64_regno_regclass (unsigned);\n int aarch64_asm_preferred_eh_data_format (int, int);\n@@ -403,6 +430,8 @@ const char *aarch64_output_move_struct (rtx *operands);\n rtx aarch64_return_addr (int, rtx);\n rtx aarch64_simd_gen_const_vector_dup (machine_mode, HOST_WIDE_INT);\n bool aarch64_simd_mem_operand_p (rtx);\n+bool aarch64_sve_ld1r_operand_p (rtx);\n+bool aarch64_sve_ldr_operand_p (rtx);\n rtx aarch64_simd_vect_par_cnst_half (machine_mode, int, bool);\n rtx aarch64_tls_get_addr (void);\n tree aarch64_fold_builtin (tree, int, tree *, bool);\n@@ -414,7 +443,9 @@ const char * aarch64_gen_far_branch (rtx *, int, const char *, const char *);\n const char * aarch64_output_probe_stack_range (rtx, rtx);\n void aarch64_err_no_fpadvsimd (machine_mode, const char *);\n void aarch64_expand_epilogue (bool);\n-void aarch64_expand_mov_immediate (rtx, rtx);\n+void aarch64_expand_mov_immediate (rtx, rtx, rtx (*) (rtx, rtx) = 0);\n+void aarch64_emit_sve_pred_move (rtx, rtx, rtx);\n+void aarch64_expand_sve_mem_move (rtx, rtx, machine_mode);\n void aarch64_expand_prologue (void);\n void aarch64_expand_vector_init (rtx, rtx);\n void aarch64_init_cumulative_args (CUMULATIVE_ARGS *, const_tree, rtx,\n@@ -467,6 +498,10 @@ void aarch64_gen_atomic_ldop (enum rtx_code, rtx, rtx, rtx, rtx, rtx);\n void aarch64_split_atomic_op (enum rtx_code, rtx, rtx, rtx, rtx, rtx, rtx);\n \n bool aarch64_gen_adjusted_ldpstp (rtx *, bool, scalar_mode, RTX_CODE);\n+\n+void aarch64_expand_sve_vec_cmp_int (rtx, rtx_code, rtx, rtx);\n+bool aarch64_expand_sve_vec_cmp_float (rtx, rtx_code, rtx, rtx, bool);\n+void aarch64_expand_sve_vcond (machine_mode, machine_mode, rtx *);\n #endif /* RTX_CODE */\n \n void aarch64_init_builtins (void);\n@@ -485,6 +520,7 @@ tree aarch64_builtin_vectorized_function (unsigned int, tree, tree);\n \n extern void aarch64_split_combinev16qi (rtx operands[3]);\n extern void aarch64_expand_vec_perm (rtx, rtx, rtx, rtx, unsigned int);\n+extern void aarch64_expand_sve_vec_perm (rtx, rtx, rtx, rtx);\n extern bool aarch64_madd_needs_nop (rtx_insn *);\n extern void aarch64_final_prescan_insn (rtx_insn *);\n void aarch64_atomic_assign_expand_fenv (tree *, tree *, tree *);\n@@ -508,4 +544,6 @@ std::string aarch64_get_extension_string_for_isa_flags (unsigned long,\n \n rtl_opt_pass *make_pass_fma_steering (gcc::context *ctxt);\n \n+poly_uint64 aarch64_regmode_natural_size (machine_mode);\n+\n #endif /* GCC_AARCH64_PROTOS_H */"}, {"sha": "352c3065094eb6f2032e139a51b4308957f4bbee", "filename": "gcc/config/aarch64/aarch64-sve.md", "status": "added", "additions": 1922, "deletions": 0, "changes": 1922, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -0,0 +1,1922 @@\n+;; Machine description for AArch64 SVE.\n+;; Copyright (C) 2009-2016 Free Software Foundation, Inc.\n+;; Contributed by ARM Ltd.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+;; Note on the handling of big-endian SVE\n+;; --------------------------------------\n+;;\n+;; On big-endian systems, Advanced SIMD mov<mode> patterns act in the\n+;; same way as movdi or movti would: the first byte of memory goes\n+;; into the most significant byte of the register and the last byte\n+;; of memory goes into the least significant byte of the register.\n+;; This is the most natural ordering for Advanced SIMD and matches\n+;; the ABI layout for 64-bit and 128-bit vector types.\n+;;\n+;; As a result, the order of bytes within the register is what GCC\n+;; expects for a big-endian target, and subreg offsets therefore work\n+;; as expected, with the first element in memory having subreg offset 0\n+;; and the last element in memory having the subreg offset associated\n+;; with a big-endian lowpart.  However, this ordering also means that\n+;; GCC's lane numbering does not match the architecture's numbering:\n+;; GCC always treats the element at the lowest address in memory\n+;; (subreg offset 0) as element 0, while the architecture treats\n+;; the least significant end of the register as element 0.\n+;;\n+;; The situation for SVE is different.  We want the layout of the\n+;; SVE register to be same for mov<mode> as it is for maskload<mode>:\n+;; logically, a mov<mode> load must be indistinguishable from a\n+;; maskload<mode> whose mask is all true.  We therefore need the\n+;; register layout to match LD1 rather than LDR.  The ABI layout of\n+;; SVE types also matches LD1 byte ordering rather than LDR byte ordering.\n+;;\n+;; As a result, the architecture lane numbering matches GCC's lane\n+;; numbering, with element 0 always being the first in memory.\n+;; However:\n+;;\n+;; - Applying a subreg offset to a register does not give the element\n+;;   that GCC expects: the first element in memory has the subreg offset\n+;;   associated with a big-endian lowpart while the last element in memory\n+;;   has subreg offset 0.  We handle this via TARGET_CAN_CHANGE_MODE_CLASS.\n+;;\n+;; - We cannot use LDR and STR for spill slots that might be accessed\n+;;   via subregs, since although the elements have the order GCC expects,\n+;;   the order of the bytes within the elements is different.  We instead\n+;;   access spill slots via LD1 and ST1, using secondary reloads to\n+;;   reserve a predicate register.\n+\n+\n+;; SVE data moves.\n+(define_expand \"mov<mode>\"\n+  [(set (match_operand:SVE_ALL 0 \"nonimmediate_operand\")\n+\t(match_operand:SVE_ALL 1 \"general_operand\"))]\n+  \"TARGET_SVE\"\n+  {\n+    /* Use the predicated load and store patterns where possible.\n+       This is required for big-endian targets (see the comment at the\n+       head of the file) and increases the addressing choices for\n+       little-endian.  */\n+    if ((MEM_P (operands[0]) || MEM_P (operands[1]))\n+        && can_create_pseudo_p ())\n+      {\n+\taarch64_expand_sve_mem_move (operands[0], operands[1], <VPRED>mode);\n+\tDONE;\n+      }\n+\n+    if (CONSTANT_P (operands[1]))\n+      {\n+\taarch64_expand_mov_immediate (operands[0], operands[1],\n+\t\t\t\t      gen_vec_duplicate<mode>);\n+\tDONE;\n+      }\n+  }\n+)\n+\n+;; Unpredicated moves (little-endian).  Only allow memory operations\n+;; during and after RA; before RA we want the predicated load and\n+;; store patterns to be used instead.\n+(define_insn \"*aarch64_sve_mov<mode>_le\"\n+  [(set (match_operand:SVE_ALL 0 \"aarch64_sve_nonimmediate_operand\" \"=w, Utr, w, w\")\n+\t(match_operand:SVE_ALL 1 \"aarch64_sve_general_operand\" \"Utr, w, w, Dn\"))]\n+  \"TARGET_SVE\n+   && !BYTES_BIG_ENDIAN\n+   && ((lra_in_progress || reload_completed)\n+       || (register_operand (operands[0], <MODE>mode)\n+\t   && nonmemory_operand (operands[1], <MODE>mode)))\"\n+  \"@\n+   ldr\\t%0, %1\n+   str\\t%1, %0\n+   mov\\t%0.d, %1.d\n+   * return aarch64_output_sve_mov_immediate (operands[1]);\"\n+)\n+\n+;; Unpredicated moves (big-endian).  Memory accesses require secondary\n+;; reloads.\n+(define_insn \"*aarch64_sve_mov<mode>_be\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w, w\")\n+\t(match_operand:SVE_ALL 1 \"aarch64_nonmemory_operand\" \"w, Dn\"))]\n+  \"TARGET_SVE && BYTES_BIG_ENDIAN\"\n+  \"@\n+   mov\\t%0.d, %1.d\n+   * return aarch64_output_sve_mov_immediate (operands[1]);\"\n+)\n+\n+;; Handle big-endian memory reloads.  We use byte PTRUE for all modes\n+;; to try to encourage reuse.\n+(define_expand \"aarch64_sve_reload_be\"\n+  [(parallel\n+     [(set (match_operand 0)\n+           (match_operand 1))\n+      (clobber (match_operand:VNx16BI 2 \"register_operand\" \"=Upl\"))])]\n+  \"TARGET_SVE && BYTES_BIG_ENDIAN\"\n+  {\n+    /* Create a PTRUE.  */\n+    emit_move_insn (operands[2], CONSTM1_RTX (VNx16BImode));\n+\n+    /* Refer to the PTRUE in the appropriate mode for this move.  */\n+    machine_mode mode = GET_MODE (operands[0]);\n+    machine_mode pred_mode\n+      = aarch64_sve_pred_mode (GET_MODE_UNIT_SIZE (mode)).require ();\n+    rtx pred = gen_lowpart (pred_mode, operands[2]);\n+\n+    /* Emit a predicated load or store.  */\n+    aarch64_emit_sve_pred_move (operands[0], pred, operands[1]);\n+    DONE;\n+  }\n+)\n+\n+;; A predicated load or store for which the predicate is known to be\n+;; all-true.  Note that this pattern is generated directly by\n+;; aarch64_emit_sve_pred_move, so changes to this pattern will\n+;; need changes there as well.\n+(define_insn \"*pred_mov<mode>\"\n+  [(set (match_operand:SVE_ALL 0 \"nonimmediate_operand\" \"=w, m\")\n+\t(unspec:SVE_ALL\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (match_operand:SVE_ALL 2 \"nonimmediate_operand\" \"m, w\")]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\n+   && (register_operand (operands[0], <MODE>mode)\n+       || register_operand (operands[2], <MODE>mode))\"\n+  \"@\n+   ld1<Vesize>\\t%0.<Vetype>, %1/z, %2\n+   st1<Vesize>\\t%2.<Vetype>, %1, %0\"\n+)\n+\n+(define_expand \"movmisalign<mode>\"\n+  [(set (match_operand:SVE_ALL 0 \"nonimmediate_operand\")\n+\t(match_operand:SVE_ALL 1 \"general_operand\"))]\n+  \"TARGET_SVE\"\n+  {\n+    /* Equivalent to a normal move for our purpooses.  */\n+    emit_move_insn (operands[0], operands[1]);\n+    DONE;\n+  }\n+)\n+\n+(define_insn \"maskload<mode><vpred>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n+\t  [(match_operand:<VPRED> 2 \"register_operand\" \"Upl\")\n+\t   (match_operand:SVE_ALL 1 \"memory_operand\" \"m\")]\n+\t  UNSPEC_LD1_SVE))]\n+  \"TARGET_SVE\"\n+  \"ld1<Vesize>\\t%0.<Vetype>, %2/z, %1\"\n+)\n+\n+(define_insn \"maskstore<mode><vpred>\"\n+  [(set (match_operand:SVE_ALL 0 \"memory_operand\" \"+m\")\n+\t(unspec:SVE_ALL [(match_operand:<VPRED> 2 \"register_operand\" \"Upl\")\n+\t\t\t (match_operand:SVE_ALL 1 \"register_operand\" \"w\")\n+\t\t\t (match_dup 0)]\n+\t\t\tUNSPEC_ST1_SVE))]\n+  \"TARGET_SVE\"\n+  \"st1<Vesize>\\t%1.<Vetype>, %2, %0\"\n+)\n+\n+(define_expand \"mov<mode>\"\n+  [(set (match_operand:PRED_ALL 0 \"nonimmediate_operand\")\n+\t(match_operand:PRED_ALL 1 \"general_operand\"))]\n+  \"TARGET_SVE\"\n+  {\n+    if (GET_CODE (operands[0]) == MEM)\n+      operands[1] = force_reg (<MODE>mode, operands[1]);\n+  }\n+)\n+\n+(define_insn \"*aarch64_sve_mov<mode>\"\n+  [(set (match_operand:PRED_ALL 0 \"nonimmediate_operand\" \"=Upa, m, Upa, Upa, Upa\")\n+\t(match_operand:PRED_ALL 1 \"general_operand\" \"Upa, Upa, m, Dz, Dm\"))]\n+  \"TARGET_SVE\n+   && (register_operand (operands[0], <MODE>mode)\n+       || register_operand (operands[1], <MODE>mode))\"\n+  \"@\n+   mov\\t%0.b, %1.b\n+   str\\t%1, %0\n+   ldr\\t%0, %1\n+   pfalse\\t%0.b\n+   * return aarch64_output_ptrue (<MODE>mode, '<Vetype>');\"\n+)\n+\n+;; Handle extractions from a predicate by converting to an integer vector\n+;; and extracting from there.\n+(define_expand \"vec_extract<vpred><Vel>\"\n+  [(match_operand:<VEL> 0 \"register_operand\")\n+   (match_operand:<VPRED> 1 \"register_operand\")\n+   (match_operand:SI 2 \"nonmemory_operand\")\n+   ;; Dummy operand to which we can attach the iterator.\n+   (reg:SVE_I V0_REGNUM)]\n+  \"TARGET_SVE\"\n+  {\n+    rtx tmp = gen_reg_rtx (<MODE>mode);\n+    emit_insn (gen_aarch64_sve_dup<mode>_const (tmp, operands[1],\n+\t\t\t\t\t\tCONST1_RTX (<MODE>mode),\n+\t\t\t\t\t\tCONST0_RTX (<MODE>mode)));\n+    emit_insn (gen_vec_extract<mode><Vel> (operands[0], tmp, operands[2]));\n+    DONE;\n+  }\n+)\n+\n+(define_expand \"vec_extract<mode><Vel>\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\")\n+\t(vec_select:<VEL>\n+\t  (match_operand:SVE_ALL 1 \"register_operand\")\n+\t  (parallel [(match_operand:SI 2 \"nonmemory_operand\")])))]\n+  \"TARGET_SVE\"\n+  {\n+    poly_int64 val;\n+    if (poly_int_rtx_p (operands[2], &val)\n+\t&& known_eq (val, GET_MODE_NUNITS (<MODE>mode) - 1))\n+      {\n+\t/* The last element can be extracted with a LASTB and a false\n+\t   predicate.  */\n+\trtx sel = force_reg (<VPRED>mode, CONST0_RTX (<VPRED>mode));\n+\temit_insn (gen_aarch64_sve_lastb<mode> (operands[0], sel,\n+\t\t\t\t\t\toperands[1]));\n+\tDONE;\n+      }\n+    if (!CONST_INT_P (operands[2]))\n+      {\n+\t/* Create an index with operand[2] as the base and -1 as the step.\n+\t   It will then be zero for the element we care about.  */\n+\trtx index = gen_lowpart (<VEL_INT>mode, operands[2]);\n+\tindex = force_reg (<VEL_INT>mode, index);\n+\trtx series = gen_reg_rtx (<V_INT_EQUIV>mode);\n+\temit_insn (gen_vec_series<v_int_equiv> (series, index, constm1_rtx));\n+\n+\t/* Get a predicate that is true for only that element.  */\n+\trtx zero = CONST0_RTX (<V_INT_EQUIV>mode);\n+\trtx cmp = gen_rtx_EQ (<V_INT_EQUIV>mode, series, zero);\n+\trtx sel = gen_reg_rtx (<VPRED>mode);\n+\temit_insn (gen_vec_cmp<v_int_equiv><vpred> (sel, cmp, series, zero));\n+\n+\t/* Select the element using LASTB.  */\n+\temit_insn (gen_aarch64_sve_lastb<mode> (operands[0], sel,\n+\t\t\t\t\t\toperands[1]));\n+\tDONE;\n+      }\n+  }\n+)\n+\n+;; Extract an element from the Advanced SIMD portion of the register.\n+;; We don't just reuse the aarch64-simd.md pattern because we don't\n+;; want any chnage in lane number on big-endian targets.\n+(define_insn \"*vec_extract<mode><Vel>_v128\"\n+  [(set (match_operand:<VEL> 0 \"aarch64_simd_nonimmediate_operand\" \"=r, w, Utv\")\n+\t(vec_select:<VEL>\n+\t  (match_operand:SVE_ALL 1 \"register_operand\" \"w, w, w\")\n+\t  (parallel [(match_operand:SI 2 \"const_int_operand\")])))]\n+  \"TARGET_SVE\n+   && IN_RANGE (INTVAL (operands[2]) * GET_MODE_SIZE (<VEL>mode), 0, 15)\"\n+  {\n+    operands[1] = gen_lowpart (<V128>mode, operands[1]);\n+    switch (which_alternative)\n+      {\n+\tcase 0:\n+\t  return \"umov\\\\t%<vwcore>0, %1.<Vetype>[%2]\";\n+\tcase 1:\n+\t  return \"dup\\\\t%<Vetype>0, %1.<Vetype>[%2]\";\n+\tcase 2:\n+\t  return \"st1\\\\t{%1.<Vetype>}[%2], %0\";\n+\tdefault:\n+\t  gcc_unreachable ();\n+      }\n+  }\n+  [(set_attr \"type\" \"neon_to_gp_q, neon_dup_q, neon_store1_one_lane_q\")]\n+)\n+\n+;; Extract an element in the range of DUP.  This pattern allows the\n+;; source and destination to be different.\n+(define_insn \"*vec_extract<mode><Vel>_dup\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\" \"=w\")\n+\t(vec_select:<VEL>\n+\t  (match_operand:SVE_ALL 1 \"register_operand\" \"w\")\n+\t  (parallel [(match_operand:SI 2 \"const_int_operand\")])))]\n+  \"TARGET_SVE\n+   && IN_RANGE (INTVAL (operands[2]) * GET_MODE_SIZE (<VEL>mode), 16, 63)\"\n+  {\n+    operands[0] = gen_rtx_REG (<MODE>mode, REGNO (operands[0]));\n+    return \"dup\\t%0.<Vetype>, %1.<Vetype>[%2]\";\n+  }\n+)\n+\n+;; Extract an element outside the range of DUP.  This pattern requires the\n+;; source and destination to be the same.\n+(define_insn \"*vec_extract<mode><Vel>_ext\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\" \"=w\")\n+\t(vec_select:<VEL>\n+\t  (match_operand:SVE_ALL 1 \"register_operand\" \"0\")\n+\t  (parallel [(match_operand:SI 2 \"const_int_operand\")])))]\n+  \"TARGET_SVE && INTVAL (operands[2]) * GET_MODE_SIZE (<VEL>mode) >= 64\"\n+  {\n+    operands[0] = gen_rtx_REG (<MODE>mode, REGNO (operands[0]));\n+    operands[2] = GEN_INT (INTVAL (operands[2]) * GET_MODE_SIZE (<VEL>mode));\n+    return \"ext\\t%0.b, %0.b, %0.b, #%2\";\n+  }\n+)\n+\n+;; Extract the last active element of operand 1 into operand 0.\n+;; If no elements are active, extract the last inactive element instead.\n+(define_insn \"aarch64_sve_lastb<mode>\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\" \"=r, w\")\n+\t(unspec:<VEL>\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (match_operand:SVE_ALL 2 \"register_operand\" \"w, w\")]\n+\t  UNSPEC_LASTB))]\n+  \"TARGET_SVE\"\n+  \"@\n+   lastb\\t%<vwcore>0, %1, %2.<Vetype>\n+   lastb\\t%<Vetype>0, %1, %2.<Vetype>\"\n+)\n+\n+(define_expand \"vec_duplicate<mode>\"\n+  [(parallel\n+    [(set (match_operand:SVE_ALL 0 \"register_operand\")\n+\t  (vec_duplicate:SVE_ALL\n+\t    (match_operand:<VEL> 1 \"aarch64_sve_dup_operand\")))\n+     (clobber (scratch:<VPRED>))])]\n+  \"TARGET_SVE\"\n+  {\n+    if (MEM_P (operands[1]))\n+      {\n+\trtx ptrue = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+\temit_insn (gen_sve_ld1r<mode> (operands[0], ptrue, operands[1],\n+\t\t\t\t       CONST0_RTX (<MODE>mode)));\n+\tDONE;\n+      }\n+  }\n+)\n+\n+;; Accept memory operands for the benefit of combine, and also in case\n+;; the scalar input gets spilled to memory during RA.  We want to split\n+;; the load at the first opportunity in order to allow the PTRUE to be\n+;; optimized with surrounding code.\n+(define_insn_and_split \"*vec_duplicate<mode>_reg\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w, w, w\")\n+\t(vec_duplicate:SVE_ALL\n+\t  (match_operand:<VEL> 1 \"aarch64_sve_dup_operand\" \"r, w, Uty\")))\n+   (clobber (match_scratch:<VPRED> 2 \"=X, X, Upl\"))]\n+  \"TARGET_SVE\"\n+  \"@\n+   mov\\t%0.<Vetype>, %<vwcore>1\n+   mov\\t%0.<Vetype>, %<Vetype>1\n+   #\"\n+  \"&& MEM_P (operands[1])\"\n+  [(const_int 0)]\n+  {\n+    if (GET_CODE (operands[2]) == SCRATCH)\n+      operands[2] = gen_reg_rtx (<VPRED>mode);\n+    emit_move_insn (operands[2], CONSTM1_RTX (<VPRED>mode));\n+    emit_insn (gen_sve_ld1r<mode> (operands[0], operands[2], operands[1],\n+\t\t\t\t   CONST0_RTX (<MODE>mode)));\n+    DONE;\n+  }\n+  [(set_attr \"length\" \"4,4,8\")]\n+)\n+\n+;; This is used for vec_duplicate<mode>s from memory, but can also\n+;; be used by combine to optimize selects of a a vec_duplicate<mode>\n+;; with zero.\n+(define_insn \"sve_ld1r<mode>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (vec_duplicate:SVE_ALL\n+\t     (match_operand:<VEL> 2 \"aarch64_sve_ld1r_operand\" \"Uty\"))\n+\t   (match_operand:SVE_ALL 3 \"aarch64_simd_imm_zero\")]\n+\t  UNSPEC_SEL))]\n+  \"TARGET_SVE\"\n+  \"ld1r<Vesize>\\t%0.<Vetype>, %1/z, %2\"\n+)\n+\n+;; Load 128 bits from memory and duplicate to fill a vector.  Since there\n+;; are so few operations on 128-bit \"elements\", we don't define a VNx1TI\n+;; and simply use vectors of bytes instead.\n+(define_insn \"sve_ld1rq\"\n+  [(set (match_operand:VNx16QI 0 \"register_operand\" \"=w\")\n+\t(unspec:VNx16QI\n+\t  [(match_operand:VNx16BI 1 \"register_operand\" \"Upl\")\n+\t   (match_operand:TI 2 \"aarch64_sve_ld1r_operand\" \"Uty\")]\n+\t  UNSPEC_LD1RQ))]\n+  \"TARGET_SVE\"\n+  \"ld1rqb\\t%0.b, %1/z, %2\"\n+)\n+\n+;; Implement a predicate broadcast by shifting the low bit of the scalar\n+;; input into the top bit and using a WHILELO.  An alternative would be to\n+;; duplicate the input and do a compare with zero.\n+(define_expand \"vec_duplicate<mode>\"\n+  [(set (match_operand:PRED_ALL 0 \"register_operand\")\n+\t(vec_duplicate:PRED_ALL (match_operand 1 \"register_operand\")))]\n+  \"TARGET_SVE\"\n+  {\n+    rtx tmp = gen_reg_rtx (DImode);\n+    rtx op1 = gen_lowpart (DImode, operands[1]);\n+    emit_insn (gen_ashldi3 (tmp, op1, gen_int_mode (63, DImode)));\n+    emit_insn (gen_while_ultdi<mode> (operands[0], const0_rtx, tmp));\n+    DONE;\n+  }\n+)\n+\n+(define_insn \"vec_series<mode>\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w, w, w\")\n+\t(vec_series:SVE_I\n+\t  (match_operand:<VEL> 1 \"aarch64_sve_index_operand\" \"Usi, r, r\")\n+\t  (match_operand:<VEL> 2 \"aarch64_sve_index_operand\" \"r, Usi, r\")))]\n+  \"TARGET_SVE\"\n+  \"@\n+   index\\t%0.<Vetype>, #%1, %<vw>2\n+   index\\t%0.<Vetype>, %<vw>1, #%2\n+   index\\t%0.<Vetype>, %<vw>1, %<vw>2\"\n+)\n+\n+;; Optimize {x, x, x, x, ...} + {0, n, 2*n, 3*n, ...} if n is in range\n+;; of an INDEX instruction.\n+(define_insn \"*vec_series<mode>_plus\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w\")\n+\t(plus:SVE_I\n+\t  (vec_duplicate:SVE_I\n+\t    (match_operand:<VEL> 1 \"register_operand\" \"r\"))\n+\t  (match_operand:SVE_I 2 \"immediate_operand\")))]\n+  \"TARGET_SVE && aarch64_check_zero_based_sve_index_immediate (operands[2])\"\n+  {\n+    operands[2] = aarch64_check_zero_based_sve_index_immediate (operands[2]);\n+    return \"index\\t%0.<Vetype>, %<vw>1, #%2\";\n+  }\n+)\n+\n+(define_expand \"vec_perm<mode>\"\n+  [(match_operand:SVE_ALL 0 \"register_operand\")\n+   (match_operand:SVE_ALL 1 \"register_operand\")\n+   (match_operand:SVE_ALL 2 \"register_operand\")\n+   (match_operand:<V_INT_EQUIV> 3 \"aarch64_sve_vec_perm_operand\")]\n+  \"TARGET_SVE && GET_MODE_NUNITS (<MODE>mode).is_constant ()\"\n+  {\n+    aarch64_expand_sve_vec_perm (operands[0], operands[1],\n+\t\t\t\t operands[2], operands[3]);\n+    DONE;\n+  }\n+)\n+\n+(define_insn \"*aarch64_sve_tbl<mode>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n+\t  [(match_operand:SVE_ALL 1 \"register_operand\" \"w\")\n+\t   (match_operand:<V_INT_EQUIV> 2 \"register_operand\" \"w\")]\n+\t  UNSPEC_TBL))]\n+  \"TARGET_SVE\"\n+  \"tbl\\t%0.<Vetype>, %1.<Vetype>, %2.<Vetype>\"\n+)\n+\n+(define_insn \"*aarch64_sve_<perm_insn><perm_hilo><mode>\"\n+  [(set (match_operand:PRED_ALL 0 \"register_operand\" \"=Upa\")\n+\t(unspec:PRED_ALL [(match_operand:PRED_ALL 1 \"register_operand\" \"Upa\")\n+\t\t\t  (match_operand:PRED_ALL 2 \"register_operand\" \"Upa\")]\n+\t\t\t PERMUTE))]\n+  \"TARGET_SVE\"\n+  \"<perm_insn><perm_hilo>\\t%0.<Vetype>, %1.<Vetype>, %2.<Vetype>\"\n+)\n+\n+(define_insn \"*aarch64_sve_<perm_insn><perm_hilo><mode>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL [(match_operand:SVE_ALL 1 \"register_operand\" \"w\")\n+\t\t\t (match_operand:SVE_ALL 2 \"register_operand\" \"w\")]\n+\t\t\tPERMUTE))]\n+  \"TARGET_SVE\"\n+  \"<perm_insn><perm_hilo>\\t%0.<Vetype>, %1.<Vetype>, %2.<Vetype>\"\n+)\n+\n+(define_insn \"*aarch64_sve_rev64<mode>\"\n+  [(set (match_operand:SVE_BHS 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_BHS\n+\t  [(match_operand:VNx2BI 1 \"register_operand\" \"Upl\")\n+\t   (unspec:SVE_BHS [(match_operand:SVE_BHS 2 \"register_operand\" \"w\")]\n+\t\t\t   UNSPEC_REV64)]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"rev<Vesize>\\t%0.d, %1/m, %2.d\"\n+)\n+\n+(define_insn \"*aarch64_sve_rev32<mode>\"\n+  [(set (match_operand:SVE_BH 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_BH\n+\t  [(match_operand:VNx4BI 1 \"register_operand\" \"Upl\")\n+\t   (unspec:SVE_BH [(match_operand:SVE_BH 2 \"register_operand\" \"w\")]\n+\t\t\t  UNSPEC_REV32)]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"rev<Vesize>\\t%0.s, %1/m, %2.s\"\n+)\n+\n+(define_insn \"*aarch64_sve_rev16vnx16qi\"\n+  [(set (match_operand:VNx16QI 0 \"register_operand\" \"=w\")\n+\t(unspec:VNx16QI\n+\t  [(match_operand:VNx8BI 1 \"register_operand\" \"Upl\")\n+\t   (unspec:VNx16QI [(match_operand:VNx16QI 2 \"register_operand\" \"w\")]\n+\t\t\t   UNSPEC_REV16)]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"revb\\t%0.h, %1/m, %2.h\"\n+)\n+\n+(define_insn \"*aarch64_sve_rev<mode>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL [(match_operand:SVE_ALL 1 \"register_operand\" \"w\")]\n+\t\t\tUNSPEC_REV))]\n+  \"TARGET_SVE\"\n+  \"rev\\t%0.<Vetype>, %1.<Vetype>\")\n+\n+(define_insn \"*aarch64_sve_dup_lane<mode>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(vec_duplicate:SVE_ALL\n+\t  (vec_select:<VEL>\n+\t    (match_operand:SVE_ALL 1 \"register_operand\" \"w\")\n+\t    (parallel [(match_operand:SI 2 \"const_int_operand\")]))))]\n+  \"TARGET_SVE\n+   && IN_RANGE (INTVAL (operands[2]) * GET_MODE_SIZE (<VEL>mode), 0, 63)\"\n+  \"dup\\t%0.<Vetype>, %1.<Vetype>[%2]\"\n+)\n+\n+;; Note that the immediate (third) operand is the lane index not\n+;; the byte index.\n+(define_insn \"*aarch64_sve_ext<mode>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL [(match_operand:SVE_ALL 1 \"register_operand\" \"0\")\n+\t\t\t (match_operand:SVE_ALL 2 \"register_operand\" \"w\")\n+\t\t\t (match_operand:SI 3 \"const_int_operand\")]\n+\t\t\tUNSPEC_EXT))]\n+  \"TARGET_SVE\n+   && IN_RANGE (INTVAL (operands[3]) * GET_MODE_SIZE (<VEL>mode), 0, 255)\"\n+  {\n+    operands[3] = GEN_INT (INTVAL (operands[3]) * GET_MODE_SIZE (<VEL>mode));\n+    return \"ext\\\\t%0.b, %0.b, %2.b, #%3\";\n+  }\n+)\n+\n+(define_insn \"add<mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w, w, w, w\")\n+\t(plus:SVE_I\n+\t  (match_operand:SVE_I 1 \"register_operand\" \"%0, 0, 0, w\")\n+\t  (match_operand:SVE_I 2 \"aarch64_sve_add_operand\" \"vsa, vsn, vsi, w\")))]\n+  \"TARGET_SVE\"\n+  \"@\n+   add\\t%0.<Vetype>, %0.<Vetype>, #%D2\n+   sub\\t%0.<Vetype>, %0.<Vetype>, #%N2\n+   * return aarch64_output_sve_inc_dec_immediate (\\\"%0.<Vetype>\\\", operands[2]);\n+   add\\t%0.<Vetype>, %1.<Vetype>, %2.<Vetype>\"\n+)\n+\n+(define_insn \"sub<mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w, w\")\n+\t(minus:SVE_I\n+\t  (match_operand:SVE_I 1 \"aarch64_sve_arith_operand\" \"w, vsa\")\n+\t  (match_operand:SVE_I 2 \"register_operand\" \"w, 0\")))]\n+  \"TARGET_SVE\"\n+  \"@\n+   sub\\t%0.<Vetype>, %1.<Vetype>, %2.<Vetype>\n+   subr\\t%0.<Vetype>, %0.<Vetype>, #%D1\"\n+)\n+\n+;; Unpredicated multiplication.\n+(define_expand \"mul<mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\")\n+\t(unspec:SVE_I\n+\t  [(match_dup 3)\n+\t   (mult:SVE_I\n+\t     (match_operand:SVE_I 1 \"register_operand\")\n+\t     (match_operand:SVE_I 2 \"aarch64_sve_mul_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Multiplication predicated with a PTRUE.  We don't actually need the\n+;; predicate for the first alternative, but using Upa or X isn't likely\n+;; to gain much and would make the instruction seem less uniform to the\n+;; register allocator.\n+(define_insn \"*mul<mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w, w\")\n+\t(unspec:SVE_I\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (mult:SVE_I\n+\t     (match_operand:SVE_I 2 \"register_operand\" \"%0, 0\")\n+\t     (match_operand:SVE_I 3 \"aarch64_sve_mul_operand\" \"vsm, w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"@\n+   mul\\t%0.<Vetype>, %0.<Vetype>, #%3\n+   mul\\t%0.<Vetype>, %1/m, %0.<Vetype>, %3.<Vetype>\"\n+)\n+\n+(define_insn \"*madd<mode>\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w, w\")\n+\t(plus:SVE_I\n+\t  (unspec:SVE_I\n+\t    [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t     (mult:SVE_I (match_operand:SVE_I 2 \"register_operand\" \"%0, w\")\n+\t\t\t (match_operand:SVE_I 3 \"register_operand\" \"w, w\"))]\n+\t    UNSPEC_MERGE_PTRUE)\n+\t  (match_operand:SVE_I 4 \"register_operand\" \"w, 0\")))]\n+  \"TARGET_SVE\"\n+  \"@\n+   mad\\t%0.<Vetype>, %1/m, %3.<Vetype>, %4.<Vetype>\n+   mla\\t%0.<Vetype>, %1/m, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n+(define_insn \"*msub<mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w, w\")\n+\t(minus:SVE_I\n+\t  (match_operand:SVE_I 4 \"register_operand\" \"w, 0\")\n+\t  (unspec:SVE_I\n+\t    [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t     (mult:SVE_I (match_operand:SVE_I 2 \"register_operand\" \"%0, w\")\n+\t\t\t (match_operand:SVE_I 3 \"register_operand\" \"w, w\"))]\n+\t    UNSPEC_MERGE_PTRUE)))]\n+  \"TARGET_SVE\"\n+  \"@\n+   msb\\t%0.<Vetype>, %1/m, %3.<Vetype>, %4.<Vetype>\n+   mls\\t%0.<Vetype>, %1/m, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Unpredicated NEG, NOT and POPCOUNT.\n+(define_expand \"<optab><mode>2\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\")\n+\t(unspec:SVE_I\n+\t  [(match_dup 2)\n+\t   (SVE_INT_UNARY:SVE_I (match_operand:SVE_I 1 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; NEG, NOT and POPCOUNT predicated with a PTRUE.\n+(define_insn \"*<optab><mode>2\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_I\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (SVE_INT_UNARY:SVE_I\n+\t     (match_operand:SVE_I 2 \"register_operand\" \"w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"<sve_int_op>\\t%0.<Vetype>, %1/m, %2.<Vetype>\"\n+)\n+\n+;; Vector AND, ORR and XOR.\n+(define_insn \"<optab><mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w, w\")\n+\t(LOGICAL:SVE_I\n+\t  (match_operand:SVE_I 1 \"register_operand\" \"%0, w\")\n+\t  (match_operand:SVE_I 2 \"aarch64_sve_logical_operand\" \"vsl, w\")))]\n+  \"TARGET_SVE\"\n+  \"@\n+   <logical>\\t%0.<Vetype>, %0.<Vetype>, #%C2\n+   <logical>\\t%0.d, %1.d, %2.d\"\n+)\n+\n+;; Vector AND, ORR and XOR on floating-point modes.  We avoid subregs\n+;; by providing this, but we need to use UNSPECs since rtx logical ops\n+;; aren't defined for floating-point modes.\n+(define_insn \"*<optab><mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_F [(match_operand:SVE_F 1 \"register_operand\" \"w\")\n+\t\t       (match_operand:SVE_F 2 \"register_operand\" \"w\")]\n+\t\t      LOGICALF))]\n+  \"TARGET_SVE\"\n+  \"<logicalf_op>\\t%0.d, %1.d, %2.d\"\n+)\n+\n+;; REG_EQUAL notes on \"not<mode>3\" should ensure that we can generate\n+;; this pattern even though the NOT instruction itself is predicated.\n+(define_insn \"bic<mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w\")\n+\t(and:SVE_I\n+\t  (not:SVE_I (match_operand:SVE_I 1 \"register_operand\" \"w\"))\n+\t  (match_operand:SVE_I 2 \"register_operand\" \"w\")))]\n+  \"TARGET_SVE\"\n+  \"bic\\t%0.d, %2.d, %1.d\"\n+)\n+\n+;; Predicate AND.  We can reuse one of the inputs as the GP.\n+(define_insn \"and<mode>3\"\n+  [(set (match_operand:PRED_ALL 0 \"register_operand\" \"=Upa\")\n+\t(and:PRED_ALL (match_operand:PRED_ALL 1 \"register_operand\" \"Upa\")\n+\t\t      (match_operand:PRED_ALL 2 \"register_operand\" \"Upa\")))]\n+  \"TARGET_SVE\"\n+  \"and\\t%0.b, %1/z, %1.b, %2.b\"\n+)\n+\n+;; Unpredicated predicate ORR and XOR.\n+(define_expand \"<optab><mode>3\"\n+  [(set (match_operand:PRED_ALL 0 \"register_operand\")\n+\t(and:PRED_ALL\n+\t  (LOGICAL_OR:PRED_ALL\n+\t    (match_operand:PRED_ALL 1 \"register_operand\")\n+\t    (match_operand:PRED_ALL 2 \"register_operand\"))\n+\t  (match_dup 3)))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<MODE>mode, CONSTM1_RTX (<MODE>mode));\n+  }\n+)\n+\n+;; Predicated predicate ORR and XOR.\n+(define_insn \"pred_<optab><mode>3\"\n+  [(set (match_operand:PRED_ALL 0 \"register_operand\" \"=Upa\")\n+\t(and:PRED_ALL\n+\t  (LOGICAL:PRED_ALL\n+\t    (match_operand:PRED_ALL 2 \"register_operand\" \"Upa\")\n+\t    (match_operand:PRED_ALL 3 \"register_operand\" \"Upa\"))\n+\t  (match_operand:PRED_ALL 1 \"register_operand\" \"Upa\")))]\n+  \"TARGET_SVE\"\n+  \"<logical>\\t%0.b, %1/z, %2.b, %3.b\"\n+)\n+\n+;; Perform a logical operation on operands 2 and 3, using operand 1 as\n+;; the GP (which is known to be a PTRUE).  Store the result in operand 0\n+;; and set the flags in the same way as for PTEST.  The (and ...) in the\n+;; UNSPEC_PTEST_PTRUE is logically redundant, but means that the tested\n+;; value is structurally equivalent to rhs of the second set.\n+(define_insn \"*<optab><mode>3_cc\"\n+  [(set (reg:CC CC_REGNUM)\n+\t(compare:CC\n+\t  (unspec:SI [(match_operand:PRED_ALL 1 \"register_operand\" \"Upa\")\n+\t\t      (and:PRED_ALL\n+\t\t\t(LOGICAL:PRED_ALL\n+\t\t\t  (match_operand:PRED_ALL 2 \"register_operand\" \"Upa\")\n+\t\t\t  (match_operand:PRED_ALL 3 \"register_operand\" \"Upa\"))\n+\t\t\t(match_dup 1))]\n+\t\t     UNSPEC_PTEST_PTRUE)\n+\t  (const_int 0)))\n+   (set (match_operand:PRED_ALL 0 \"register_operand\" \"=Upa\")\n+\t(and:PRED_ALL (LOGICAL:PRED_ALL (match_dup 2) (match_dup 3))\n+\t\t      (match_dup 1)))]\n+  \"TARGET_SVE\"\n+  \"<logical>s\\t%0.b, %1/z, %2.b, %3.b\"\n+)\n+\n+;; Unpredicated predicate inverse.\n+(define_expand \"one_cmpl<mode>2\"\n+  [(set (match_operand:PRED_ALL 0 \"register_operand\")\n+\t(and:PRED_ALL\n+\t  (not:PRED_ALL (match_operand:PRED_ALL 1 \"register_operand\"))\n+\t  (match_dup 2)))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = force_reg (<MODE>mode, CONSTM1_RTX (<MODE>mode));\n+  }\n+)\n+\n+;; Predicated predicate inverse.\n+(define_insn \"*one_cmpl<mode>3\"\n+  [(set (match_operand:PRED_ALL 0 \"register_operand\" \"=Upa\")\n+\t(and:PRED_ALL\n+\t  (not:PRED_ALL (match_operand:PRED_ALL 2 \"register_operand\" \"Upa\"))\n+\t  (match_operand:PRED_ALL 1 \"register_operand\" \"Upa\")))]\n+  \"TARGET_SVE\"\n+  \"not\\t%0.b, %1/z, %2.b\"\n+)\n+\n+;; Predicated predicate BIC and ORN.\n+(define_insn \"*<nlogical><mode>3\"\n+  [(set (match_operand:PRED_ALL 0 \"register_operand\" \"=Upa\")\n+\t(and:PRED_ALL\n+\t  (NLOGICAL:PRED_ALL\n+\t    (not:PRED_ALL (match_operand:PRED_ALL 2 \"register_operand\" \"Upa\"))\n+\t    (match_operand:PRED_ALL 3 \"register_operand\" \"Upa\"))\n+\t  (match_operand:PRED_ALL 1 \"register_operand\" \"Upa\")))]\n+  \"TARGET_SVE\"\n+  \"<nlogical>\\t%0.b, %1/z, %3.b, %2.b\"\n+)\n+\n+;; Predicated predicate NAND and NOR.\n+(define_insn \"*<logical_nn><mode>3\"\n+  [(set (match_operand:PRED_ALL 0 \"register_operand\" \"=Upa\")\n+\t(and:PRED_ALL\n+\t  (NLOGICAL:PRED_ALL\n+\t    (not:PRED_ALL (match_operand:PRED_ALL 2 \"register_operand\" \"Upa\"))\n+\t    (not:PRED_ALL (match_operand:PRED_ALL 3 \"register_operand\" \"Upa\")))\n+\t  (match_operand:PRED_ALL 1 \"register_operand\" \"Upa\")))]\n+  \"TARGET_SVE\"\n+  \"<logical_nn>\\t%0.b, %1/z, %2.b, %3.b\"\n+)\n+\n+;; Unpredicated LSL, LSR and ASR by a vector.\n+(define_expand \"v<optab><mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\")\n+\t(unspec:SVE_I\n+\t  [(match_dup 3)\n+\t   (ASHIFT:SVE_I\n+\t     (match_operand:SVE_I 1 \"register_operand\")\n+\t     (match_operand:SVE_I 2 \"aarch64_sve_<lr>shift_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; LSL, LSR and ASR by a vector, predicated with a PTRUE.  We don't\n+;; actually need the predicate for the first alternative, but using Upa\n+;; or X isn't likely to gain much and would make the instruction seem\n+;; less uniform to the register allocator.\n+(define_insn \"*v<optab><mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w, w\")\n+\t(unspec:SVE_I\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (ASHIFT:SVE_I\n+\t     (match_operand:SVE_I 2 \"register_operand\" \"w, 0\")\n+\t     (match_operand:SVE_I 3 \"aarch64_sve_<lr>shift_operand\" \"D<lr>, w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"@\n+   <shift>\\t%0.<Vetype>, %2.<Vetype>, #%3\n+   <shift>\\t%0.<Vetype>, %1/m, %0.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; LSL, LSR and ASR by a scalar, which expands into one of the vector\n+;; shifts above.\n+(define_expand \"<ASHIFT:optab><mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\")\n+\t(ASHIFT:SVE_I (match_operand:SVE_I 1 \"register_operand\")\n+\t\t      (match_operand:<VEL> 2 \"general_operand\")))]\n+  \"TARGET_SVE\"\n+  {\n+    rtx amount;\n+    if (CONST_INT_P (operands[2]))\n+      {\n+\tamount = gen_const_vec_duplicate (<MODE>mode, operands[2]);\n+\tif (!aarch64_sve_<lr>shift_operand (operands[2], <MODE>mode))\n+\t  amount = force_reg (<MODE>mode, amount);\n+      }\n+    else\n+      {\n+\tamount = gen_reg_rtx (<MODE>mode);\n+\temit_insn (gen_vec_duplicate<mode> (amount,\n+\t\t\t\t\t    convert_to_mode (<VEL>mode,\n+\t\t\t\t\t\t\t     operands[2], 0)));\n+      }\n+    emit_insn (gen_v<optab><mode>3 (operands[0], operands[1], amount));\n+    DONE;\n+  }\n+)\n+\n+;; Test all bits of operand 1.  Operand 0 is a GP that is known to hold PTRUE.\n+;;\n+;; Using UNSPEC_PTEST_PTRUE allows combine patterns to assume that the GP\n+;; is a PTRUE even if the optimizers haven't yet been able to propagate\n+;; the constant.  We would use a separate unspec code for PTESTs involving\n+;; GPs that might not be PTRUEs.\n+(define_insn \"ptest_ptrue<mode>\"\n+  [(set (reg:CC CC_REGNUM)\n+\t(compare:CC\n+\t  (unspec:SI [(match_operand:PRED_ALL 0 \"register_operand\" \"Upa\")\n+\t\t      (match_operand:PRED_ALL 1 \"register_operand\" \"Upa\")]\n+\t\t     UNSPEC_PTEST_PTRUE)\n+\t  (const_int 0)))]\n+  \"TARGET_SVE\"\n+  \"ptest\\t%0, %1.b\"\n+)\n+\n+;; Set element I of the result if operand1 + J < operand2 for all J in [0, I].\n+;; with the comparison being unsigned.\n+(define_insn \"while_ult<GPI:mode><PRED_ALL:mode>\"\n+  [(set (match_operand:PRED_ALL 0 \"register_operand\" \"=Upa\")\n+\t(unspec:PRED_ALL [(match_operand:GPI 1 \"aarch64_reg_or_zero\" \"rZ\")\n+\t\t\t  (match_operand:GPI 2 \"aarch64_reg_or_zero\" \"rZ\")]\n+\t\t\t UNSPEC_WHILE_LO))\n+   (clobber (reg:CC CC_REGNUM))]\n+  \"TARGET_SVE\"\n+  \"whilelo\\t%0.<PRED_ALL:Vetype>, %<w>1, %<w>2\"\n+)\n+\n+;; WHILELO sets the flags in the same way as a PTEST with a PTRUE GP.\n+;; Handle the case in which both results are useful.  The GP operand\n+;; to the PTEST isn't needed, so we allow it to be anything.\n+(define_insn_and_split \"while_ult<GPI:mode><PRED_ALL:mode>_cc\"\n+  [(set (reg:CC CC_REGNUM)\n+\t(compare:CC\n+\t  (unspec:SI [(match_operand:PRED_ALL 1)\n+\t\t      (unspec:PRED_ALL\n+\t\t\t[(match_operand:GPI 2 \"aarch64_reg_or_zero\" \"rZ\")\n+\t\t\t (match_operand:GPI 3 \"aarch64_reg_or_zero\" \"rZ\")]\n+\t\t\tUNSPEC_WHILE_LO)]\n+\t\t     UNSPEC_PTEST_PTRUE)\n+\t  (const_int 0)))\n+   (set (match_operand:PRED_ALL 0 \"register_operand\" \"=Upa\")\n+\t(unspec:PRED_ALL [(match_dup 2)\n+\t\t\t  (match_dup 3)]\n+\t\t\t UNSPEC_WHILE_LO))]\n+  \"TARGET_SVE\"\n+  \"whilelo\\t%0.<PRED_ALL:Vetype>, %<w>2, %<w>3\"\n+  ;; Force the compiler to drop the unused predicate operand, so that we\n+  ;; don't have an unnecessary PTRUE.\n+  \"&& !CONSTANT_P (operands[1])\"\n+  [(const_int 0)]\n+  {\n+    emit_insn (gen_while_ult<GPI:mode><PRED_ALL:mode>_cc\n+\t       (operands[0], CONSTM1_RTX (<MODE>mode),\n+\t\toperands[2], operands[3]));\n+    DONE;\n+  }\n+)\n+\n+;; Predicated integer comparison.\n+(define_insn \"*vec_cmp<cmp_op>_<mode>\"\n+  [(set (match_operand:<VPRED> 0 \"register_operand\" \"=Upa, Upa\")\n+\t(unspec:<VPRED>\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (match_operand:SVE_I 2 \"register_operand\" \"w, w\")\n+\t   (match_operand:SVE_I 3 \"aarch64_sve_cmp_<imm_con>_operand\" \"<imm_con>, w\")]\n+\t  SVE_COND_INT_CMP))\n+   (clobber (reg:CC CC_REGNUM))]\n+  \"TARGET_SVE\"\n+  \"@\n+   cmp<cmp_op>\\t%0.<Vetype>, %1/z, %2.<Vetype>, #%3\n+   cmp<cmp_op>\\t%0.<Vetype>, %1/z, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Predicated integer comparison in which only the flags result is interesting.\n+(define_insn \"*vec_cmp<cmp_op>_<mode>_ptest\"\n+  [(set (reg:CC CC_REGNUM)\n+\t(compare:CC\n+\t  (unspec:SI\n+\t    [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t     (unspec:<VPRED>\n+\t       [(match_dup 1)\n+\t        (match_operand:SVE_I 2 \"register_operand\" \"w, w\")\n+\t\t(match_operand:SVE_I 3 \"aarch64_sve_cmp_<imm_con>_operand\" \"<imm_con>, w\")]\n+\t       SVE_COND_INT_CMP)]\n+\t    UNSPEC_PTEST_PTRUE)\n+\t  (const_int 0)))\n+   (clobber (match_scratch:<VPRED> 0 \"=Upa, Upa\"))]\n+  \"TARGET_SVE\"\n+  \"@\n+   cmp<cmp_op>\\t%0.<Vetype>, %1/z, %2.<Vetype>, #%3\n+   cmp<cmp_op>\\t%0.<Vetype>, %1/z, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Predicated comparison in which both the flag and predicate results\n+;; are interesting.\n+(define_insn \"*vec_cmp<cmp_op>_<mode>_cc\"\n+  [(set (reg:CC CC_REGNUM)\n+\t(compare:CC\n+\t  (unspec:SI\n+\t    [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t     (unspec:<VPRED>\n+\t       [(match_dup 1)\n+\t\t(match_operand:SVE_I 2 \"register_operand\" \"w, w\")\n+\t\t(match_operand:SVE_I 3 \"aarch64_sve_cmp_<imm_con>_operand\" \"<imm_con>, w\")]\n+\t       SVE_COND_INT_CMP)]\n+\t    UNSPEC_PTEST_PTRUE)\n+\t  (const_int 0)))\n+   (set (match_operand:<VPRED> 0 \"register_operand\" \"=Upa, Upa\")\n+\t(unspec:<VPRED>\n+\t  [(match_dup 1)\n+\t   (match_dup 2)\n+\t   (match_dup 3)]\n+\t  SVE_COND_INT_CMP))]\n+  \"TARGET_SVE\"\n+  \"@\n+   cmp<cmp_op>\\t%0.<Vetype>, %1/z, %2.<Vetype>, #%3\n+   cmp<cmp_op>\\t%0.<Vetype>, %1/z, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Predicated floating-point comparison (excluding FCMUO, which doesn't\n+;; allow #0.0 as an operand).\n+(define_insn \"*vec_fcm<cmp_op><mode>\"\n+  [(set (match_operand:<VPRED> 0 \"register_operand\" \"=Upa, Upa\")\n+\t(unspec:<VPRED>\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (match_operand:SVE_F 2 \"register_operand\" \"w, w\")\n+\t   (match_operand:SVE_F 3 \"aarch64_simd_reg_or_zero\" \"Dz, w\")]\n+\t  SVE_COND_FP_CMP))]\n+  \"TARGET_SVE\"\n+  \"@\n+   fcm<cmp_op>\\t%0.<Vetype>, %1/z, %2.<Vetype>, #0.0\n+   fcm<cmp_op>\\t%0.<Vetype>, %1/z, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Predicated FCMUO.\n+(define_insn \"*vec_fcmuo<mode>\"\n+  [(set (match_operand:<VPRED> 0 \"register_operand\" \"=Upa\")\n+\t(unspec:<VPRED>\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (match_operand:SVE_F 2 \"register_operand\" \"w\")\n+\t   (match_operand:SVE_F 3 \"register_operand\" \"w\")]\n+\t  UNSPEC_COND_UO))]\n+  \"TARGET_SVE\"\n+  \"fcmuo\\t%0.<Vetype>, %1/z, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; vcond_mask operand order: true, false, mask\n+;; UNSPEC_SEL operand order: mask, true, false (as for VEC_COND_EXPR)\n+;; SEL operand order:        mask, true, false\n+(define_insn \"vcond_mask_<mode><vpred>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n+\t  [(match_operand:<VPRED> 3 \"register_operand\" \"Upa\")\n+\t   (match_operand:SVE_ALL 1 \"register_operand\" \"w\")\n+\t   (match_operand:SVE_ALL 2 \"register_operand\" \"w\")]\n+\t  UNSPEC_SEL))]\n+  \"TARGET_SVE\"\n+  \"sel\\t%0.<Vetype>, %3, %1.<Vetype>, %2.<Vetype>\"\n+)\n+\n+;; Selects between a duplicated immediate and zero.\n+(define_insn \"aarch64_sve_dup<mode>_const\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_I\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (match_operand:SVE_I 2 \"aarch64_sve_dup_immediate\")\n+\t   (match_operand:SVE_I 3 \"aarch64_simd_imm_zero\")]\n+\t  UNSPEC_SEL))]\n+  \"TARGET_SVE\"\n+  \"mov\\t%0.<Vetype>, %1/z, #%2\"\n+)\n+\n+;; Integer (signed) vcond.  Don't enforce an immediate range here, since it\n+;; depends on the comparison; leave it to aarch64_expand_sve_vcond instead.\n+(define_expand \"vcond<mode><v_int_equiv>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\")\n+\t(if_then_else:SVE_ALL\n+\t  (match_operator 3 \"comparison_operator\"\n+\t    [(match_operand:<V_INT_EQUIV> 4 \"register_operand\")\n+\t     (match_operand:<V_INT_EQUIV> 5 \"nonmemory_operand\")])\n+\t  (match_operand:SVE_ALL 1 \"register_operand\")\n+\t  (match_operand:SVE_ALL 2 \"register_operand\")))]\n+  \"TARGET_SVE\"\n+  {\n+    aarch64_expand_sve_vcond (<MODE>mode, <V_INT_EQUIV>mode, operands);\n+    DONE;\n+  }\n+)\n+\n+;; Integer vcondu.  Don't enforce an immediate range here, since it\n+;; depends on the comparison; leave it to aarch64_expand_sve_vcond instead.\n+(define_expand \"vcondu<mode><v_int_equiv>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\")\n+\t(if_then_else:SVE_ALL\n+\t  (match_operator 3 \"comparison_operator\"\n+\t    [(match_operand:<V_INT_EQUIV> 4 \"register_operand\")\n+\t     (match_operand:<V_INT_EQUIV> 5 \"nonmemory_operand\")])\n+\t  (match_operand:SVE_ALL 1 \"register_operand\")\n+\t  (match_operand:SVE_ALL 2 \"register_operand\")))]\n+  \"TARGET_SVE\"\n+  {\n+    aarch64_expand_sve_vcond (<MODE>mode, <V_INT_EQUIV>mode, operands);\n+    DONE;\n+  }\n+)\n+\n+;; Floating-point vcond.  All comparisons except FCMUO allow a zero\n+;; operand; aarch64_expand_sve_vcond handles the case of an FCMUO\n+;; with zero.\n+(define_expand \"vcond<mode><v_fp_equiv>\"\n+  [(set (match_operand:SVE_SD 0 \"register_operand\")\n+\t(if_then_else:SVE_SD\n+\t  (match_operator 3 \"comparison_operator\"\n+\t    [(match_operand:<V_FP_EQUIV> 4 \"register_operand\")\n+\t     (match_operand:<V_FP_EQUIV> 5 \"aarch64_simd_reg_or_zero\")])\n+\t  (match_operand:SVE_SD 1 \"register_operand\")\n+\t  (match_operand:SVE_SD 2 \"register_operand\")))]\n+  \"TARGET_SVE\"\n+  {\n+    aarch64_expand_sve_vcond (<MODE>mode, <V_FP_EQUIV>mode, operands);\n+    DONE;\n+  }\n+)\n+\n+;; Signed integer comparisons.  Don't enforce an immediate range here, since\n+;; it depends on the comparison; leave it to aarch64_expand_sve_vec_cmp_int\n+;; instead.\n+(define_expand \"vec_cmp<mode><vpred>\"\n+  [(parallel\n+    [(set (match_operand:<VPRED> 0 \"register_operand\")\n+\t  (match_operator:<VPRED> 1 \"comparison_operator\"\n+\t    [(match_operand:SVE_I 2 \"register_operand\")\n+\t     (match_operand:SVE_I 3 \"nonmemory_operand\")]))\n+     (clobber (reg:CC CC_REGNUM))])]\n+  \"TARGET_SVE\"\n+  {\n+    aarch64_expand_sve_vec_cmp_int (operands[0], GET_CODE (operands[1]),\n+\t\t\t\t    operands[2], operands[3]);\n+    DONE;\n+  }\n+)\n+\n+;; Unsigned integer comparisons.  Don't enforce an immediate range here, since\n+;; it depends on the comparison; leave it to aarch64_expand_sve_vec_cmp_int\n+;; instead.\n+(define_expand \"vec_cmpu<mode><vpred>\"\n+  [(parallel\n+    [(set (match_operand:<VPRED> 0 \"register_operand\")\n+\t  (match_operator:<VPRED> 1 \"comparison_operator\"\n+\t    [(match_operand:SVE_I 2 \"register_operand\")\n+\t     (match_operand:SVE_I 3 \"nonmemory_operand\")]))\n+     (clobber (reg:CC CC_REGNUM))])]\n+  \"TARGET_SVE\"\n+  {\n+    aarch64_expand_sve_vec_cmp_int (operands[0], GET_CODE (operands[1]),\n+\t\t\t\t    operands[2], operands[3]);\n+    DONE;\n+  }\n+)\n+\n+;; Floating-point comparisons.  All comparisons except FCMUO allow a zero\n+;; operand; aarch64_expand_sve_vec_cmp_float handles the case of an FCMUO\n+;; with zero.\n+(define_expand \"vec_cmp<mode><vpred>\"\n+  [(set (match_operand:<VPRED> 0 \"register_operand\")\n+\t(match_operator:<VPRED> 1 \"comparison_operator\"\n+\t  [(match_operand:SVE_F 2 \"register_operand\")\n+\t   (match_operand:SVE_F 3 \"aarch64_simd_reg_or_zero\")]))]\n+  \"TARGET_SVE\"\n+  {\n+    aarch64_expand_sve_vec_cmp_float (operands[0], GET_CODE (operands[1]),\n+\t\t\t\t      operands[2], operands[3], false);\n+    DONE;\n+  }\n+)\n+\n+;; Branch based on predicate equality or inequality.\n+(define_expand \"cbranch<mode>4\"\n+  [(set (pc)\n+\t(if_then_else\n+\t  (match_operator 0 \"aarch64_equality_operator\"\n+\t    [(match_operand:PRED_ALL 1 \"register_operand\")\n+\t     (match_operand:PRED_ALL 2 \"aarch64_simd_reg_or_zero\")])\n+\t  (label_ref (match_operand 3 \"\"))\n+\t  (pc)))]\n+  \"\"\n+  {\n+    rtx ptrue = force_reg (<MODE>mode, CONSTM1_RTX (<MODE>mode));\n+    rtx pred;\n+    if (operands[2] == CONST0_RTX (<MODE>mode))\n+      pred = operands[1];\n+    else\n+      {\n+\tpred = gen_reg_rtx (<MODE>mode);\n+\temit_insn (gen_pred_xor<mode>3 (pred, ptrue, operands[1],\n+\t\t\t\t\toperands[2]));\n+      }\n+    emit_insn (gen_ptest_ptrue<mode> (ptrue, pred));\n+    operands[1] = gen_rtx_REG (CCmode, CC_REGNUM);\n+    operands[2] = const0_rtx;\n+  }\n+)\n+\n+;; Unpredicated integer MIN/MAX.\n+(define_expand \"<su><maxmin><mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\")\n+\t(unspec:SVE_I\n+\t  [(match_dup 3)\n+\t   (MAXMIN:SVE_I (match_operand:SVE_I 1 \"register_operand\")\n+\t\t\t (match_operand:SVE_I 2 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Integer MIN/MAX predicated with a PTRUE.\n+(define_insn \"*<su><maxmin><mode>3\"\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_I\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (MAXMIN:SVE_I (match_operand:SVE_I 2 \"register_operand\" \"%0\")\n+\t\t\t (match_operand:SVE_I 3 \"register_operand\" \"w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"<su><maxmin>\\t%0.<Vetype>, %1/m, %0.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Unpredicated floating-point MIN/MAX.\n+(define_expand \"<su><maxmin><mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 3)\n+\t   (FMAXMIN:SVE_F (match_operand:SVE_F 1 \"register_operand\")\n+\t\t\t  (match_operand:SVE_F 2 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Floating-point MIN/MAX predicated with a PTRUE.\n+(define_insn \"*<su><maxmin><mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (FMAXMIN:SVE_F (match_operand:SVE_F 2 \"register_operand\" \"%0\")\n+\t\t\t  (match_operand:SVE_F 3 \"register_operand\" \"w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"f<maxmin>nm\\t%0.<Vetype>, %1/m, %0.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Unpredicated fmin/fmax.\n+(define_expand \"<maxmin_uns><mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 3)\n+\t   (unspec:SVE_F [(match_operand:SVE_F 1 \"register_operand\")\n+\t\t\t  (match_operand:SVE_F 2 \"register_operand\")]\n+\t\t\t FMAXMIN_UNS)]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; fmin/fmax predicated with a PTRUE.\n+(define_insn \"*<maxmin_uns><mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (unspec:SVE_F [(match_operand:SVE_F 2 \"register_operand\" \"%0\")\n+\t\t\t  (match_operand:SVE_F 3 \"register_operand\" \"w\")]\n+\t\t\t FMAXMIN_UNS)]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"<maxmin_uns_op>\\t%0.<Vetype>, %1/m, %0.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Unpredicated integer add reduction.\n+(define_expand \"reduc_plus_scal_<mode>\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\")\n+\t(unspec:<VEL> [(match_dup 2)\n+\t\t       (match_operand:SVE_I 1 \"register_operand\")]\n+\t\t      UNSPEC_ADDV))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Predicated integer add reduction.  The result is always 64-bits.\n+(define_insn \"*reduc_plus_scal_<mode>\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\" \"=w\")\n+\t(unspec:<VEL> [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t\t       (match_operand:SVE_I 2 \"register_operand\" \"w\")]\n+\t\t      UNSPEC_ADDV))]\n+  \"TARGET_SVE\"\n+  \"uaddv\\t%d0, %1, %2.<Vetype>\"\n+)\n+\n+;; Unpredicated floating-point add reduction.\n+(define_expand \"reduc_plus_scal_<mode>\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\")\n+\t(unspec:<VEL> [(match_dup 2)\n+\t\t       (match_operand:SVE_F 1 \"register_operand\")]\n+\t\t      UNSPEC_FADDV))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Predicated floating-point add reduction.\n+(define_insn \"*reduc_plus_scal_<mode>\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\" \"=w\")\n+\t(unspec:<VEL> [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t\t       (match_operand:SVE_F 2 \"register_operand\" \"w\")]\n+\t\t      UNSPEC_FADDV))]\n+  \"TARGET_SVE\"\n+  \"faddv\\t%<Vetype>0, %1, %2.<Vetype>\"\n+)\n+\n+;; Unpredicated integer MIN/MAX reduction.\n+(define_expand \"reduc_<maxmin_uns>_scal_<mode>\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\")\n+\t(unspec:<VEL> [(match_dup 2)\n+\t\t       (match_operand:SVE_I 1 \"register_operand\")]\n+\t\t      MAXMINV))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Predicated integer MIN/MAX reduction.\n+(define_insn \"*reduc_<maxmin_uns>_scal_<mode>\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\" \"=w\")\n+\t(unspec:<VEL> [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t\t       (match_operand:SVE_I 2 \"register_operand\" \"w\")]\n+\t\t      MAXMINV))]\n+  \"TARGET_SVE\"\n+  \"<maxmin_uns_op>v\\t%<Vetype>0, %1, %2.<Vetype>\"\n+)\n+\n+;; Unpredicated floating-point MIN/MAX reduction.\n+(define_expand \"reduc_<maxmin_uns>_scal_<mode>\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\")\n+\t(unspec:<VEL> [(match_dup 2)\n+\t\t       (match_operand:SVE_F 1 \"register_operand\")]\n+\t\t      FMAXMINV))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Predicated floating-point MIN/MAX reduction.\n+(define_insn \"*reduc_<maxmin_uns>_scal_<mode>\"\n+  [(set (match_operand:<VEL> 0 \"register_operand\" \"=w\")\n+\t(unspec:<VEL> [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t\t       (match_operand:SVE_F 2 \"register_operand\" \"w\")]\n+\t\t      FMAXMINV))]\n+  \"TARGET_SVE\"\n+  \"<maxmin_uns_op>v\\t%<Vetype>0, %1, %2.<Vetype>\"\n+)\n+\n+;; Unpredicated floating-point addition.\n+(define_expand \"add<mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 3)\n+\t   (plus:SVE_F\n+\t     (match_operand:SVE_F 1 \"register_operand\")\n+\t     (match_operand:SVE_F 2 \"aarch64_sve_float_arith_with_sub_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Floating-point addition predicated with a PTRUE.\n+(define_insn \"*add<mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w, w, w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl, Upl\")\n+\t   (plus:SVE_F\n+\t      (match_operand:SVE_F 2 \"register_operand\" \"%0, 0, w\")\n+\t      (match_operand:SVE_F 3 \"aarch64_sve_float_arith_with_sub_operand\" \"vsA, vsN, w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"@\n+   fadd\\t%0.<Vetype>, %1/m, %0.<Vetype>, #%3\n+   fsub\\t%0.<Vetype>, %1/m, %0.<Vetype>, #%N3\n+   fadd\\t%0.<Vetype>, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Unpredicated floating-point subtraction.\n+(define_expand \"sub<mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 3)\n+\t   (minus:SVE_F\n+\t     (match_operand:SVE_F 1 \"aarch64_sve_float_arith_operand\")\n+\t     (match_operand:SVE_F 2 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Floating-point subtraction predicated with a PTRUE.\n+(define_insn \"*sub<mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w, w, w, w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl, Upl, Upl\")\n+\t   (minus:SVE_F\n+\t     (match_operand:SVE_F 2 \"aarch64_sve_float_arith_operand\" \"0, 0, vsA, w\")\n+\t     (match_operand:SVE_F 3 \"aarch64_sve_float_arith_with_sub_operand\" \"vsA, vsN, 0, w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\n+   && (register_operand (operands[2], <MODE>mode)\n+       || register_operand (operands[3], <MODE>mode))\"\n+  \"@\n+   fsub\\t%0.<Vetype>, %1/m, %0.<Vetype>, #%3\n+   fadd\\t%0.<Vetype>, %1/m, %0.<Vetype>, #%N3\n+   fsubr\\t%0.<Vetype>, %1/m, %0.<Vetype>, #%2\n+   fsub\\t%0.<Vetype>, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Unpredicated floating-point multiplication.\n+(define_expand \"mul<mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 3)\n+\t   (mult:SVE_F\n+\t     (match_operand:SVE_F 1 \"register_operand\")\n+\t     (match_operand:SVE_F 2 \"aarch64_sve_float_mul_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Floating-point multiplication predicated with a PTRUE.\n+(define_insn \"*mul<mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w, w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (mult:SVE_F\n+\t     (match_operand:SVE_F 2 \"register_operand\" \"%0, w\")\n+\t     (match_operand:SVE_F 3 \"aarch64_sve_float_mul_operand\" \"vsM, w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"@\n+   fmul\\t%0.<Vetype>, %1/m, %0.<Vetype>, #%3\n+   fmul\\t%0.<Vetype>, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n+;; Unpredicated fma (%0 = (%1 * %2) + %3).\n+(define_expand \"fma<mode>4\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 4)\n+\t   (fma:SVE_F (match_operand:SVE_F 1 \"register_operand\")\n+\t\t      (match_operand:SVE_F 2 \"register_operand\")\n+\t\t      (match_operand:SVE_F 3 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[4] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; fma predicated with a PTRUE.\n+(define_insn \"*fma<mode>4\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w, w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (fma:SVE_F (match_operand:SVE_F 3 \"register_operand\" \"%0, w\")\n+\t\t      (match_operand:SVE_F 4 \"register_operand\" \"w, w\")\n+\t\t      (match_operand:SVE_F 2 \"register_operand\" \"w, 0\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"@\n+   fmad\\t%0.<Vetype>, %1/m, %4.<Vetype>, %2.<Vetype>\n+   fmla\\t%0.<Vetype>, %1/m, %3.<Vetype>, %4.<Vetype>\"\n+)\n+\n+;; Unpredicated fnma (%0 = (-%1 * %2) + %3).\n+(define_expand \"fnma<mode>4\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 4)\n+\t   (fma:SVE_F (neg:SVE_F\n+\t\t\t(match_operand:SVE_F 1 \"register_operand\"))\n+\t\t      (match_operand:SVE_F 2 \"register_operand\")\n+\t\t      (match_operand:SVE_F 3 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[4] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; fnma predicated with a PTRUE.\n+(define_insn \"*fnma<mode>4\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w, w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (fma:SVE_F (neg:SVE_F\n+\t\t\t(match_operand:SVE_F 3 \"register_operand\" \"%0, w\"))\n+\t\t      (match_operand:SVE_F 4 \"register_operand\" \"w, w\")\n+\t\t      (match_operand:SVE_F 2 \"register_operand\" \"w, 0\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"@\n+   fmsb\\t%0.<Vetype>, %1/m, %4.<Vetype>, %2.<Vetype>\n+   fmls\\t%0.<Vetype>, %1/m, %3.<Vetype>, %4.<Vetype>\"\n+)\n+\n+;; Unpredicated fms (%0 = (%1 * %2) - %3).\n+(define_expand \"fms<mode>4\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 4)\n+\t   (fma:SVE_F (match_operand:SVE_F 1 \"register_operand\")\n+\t\t      (match_operand:SVE_F 2 \"register_operand\")\n+\t\t      (neg:SVE_F\n+\t\t\t(match_operand:SVE_F 3 \"register_operand\")))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[4] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; fms predicated with a PTRUE.\n+(define_insn \"*fms<mode>4\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w, w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (fma:SVE_F (match_operand:SVE_F 3 \"register_operand\" \"%0, w\")\n+\t\t      (match_operand:SVE_F 4 \"register_operand\" \"w, w\")\n+\t\t      (neg:SVE_F\n+\t\t\t(match_operand:SVE_F 2 \"register_operand\" \"w, 0\")))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"@\n+   fnmsb\\t%0.<Vetype>, %1/m, %4.<Vetype>, %2.<Vetype>\n+   fnmls\\t%0.<Vetype>, %1/m, %3.<Vetype>, %4.<Vetype>\"\n+)\n+\n+;; Unpredicated fnms (%0 = (-%1 * %2) - %3).\n+(define_expand \"fnms<mode>4\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 4)\n+\t   (fma:SVE_F (neg:SVE_F\n+\t\t\t(match_operand:SVE_F 1 \"register_operand\"))\n+\t\t      (match_operand:SVE_F 2 \"register_operand\")\n+\t\t      (neg:SVE_F\n+\t\t\t(match_operand:SVE_F 3 \"register_operand\")))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[4] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; fnms predicated with a PTRUE.\n+(define_insn \"*fnms<mode>4\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w, w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (fma:SVE_F (neg:SVE_F\n+\t\t\t(match_operand:SVE_F 3 \"register_operand\" \"%0, w\"))\n+\t\t      (match_operand:SVE_F 4 \"register_operand\" \"w, w\")\n+\t\t      (neg:SVE_F\n+\t\t\t(match_operand:SVE_F 2 \"register_operand\" \"w, 0\")))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"@\n+   fnmad\\t%0.<Vetype>, %1/m, %4.<Vetype>, %2.<Vetype>\n+   fnmla\\t%0.<Vetype>, %1/m, %3.<Vetype>, %4.<Vetype>\"\n+)\n+\n+;; Unpredicated floating-point division.\n+(define_expand \"div<mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 3)\n+\t   (div:SVE_F (match_operand:SVE_F 1 \"register_operand\")\n+\t\t      (match_operand:SVE_F 2 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Floating-point division predicated with a PTRUE.\n+(define_insn \"*div<mode>3\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w, w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl\")\n+\t   (div:SVE_F (match_operand:SVE_F 2 \"register_operand\" \"0, w\")\n+\t\t      (match_operand:SVE_F 3 \"register_operand\" \"w, 0\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"@\n+   fdiv\\t%0.<Vetype>, %1/m, %0.<Vetype>, %3.<Vetype>\n+   fdivr\\t%0.<Vetype>, %1/m, %0.<Vetype>, %2.<Vetype>\"\n+)\n+\n+;; Unpredicated FNEG, FABS and FSQRT.\n+(define_expand \"<optab><mode>2\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 2)\n+\t   (SVE_FP_UNARY:SVE_F (match_operand:SVE_F 1 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; FNEG, FABS and FSQRT predicated with a PTRUE.\n+(define_insn \"*<optab><mode>2\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (SVE_FP_UNARY:SVE_F (match_operand:SVE_F 2 \"register_operand\" \"w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"<sve_fp_op>\\t%0.<Vetype>, %1/m, %2.<Vetype>\"\n+)\n+\n+;; Unpredicated FRINTy.\n+(define_expand \"<frint_pattern><mode>2\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 2)\n+\t   (unspec:SVE_F [(match_operand:SVE_F 1 \"register_operand\")]\n+\t\t\t FRINT)]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; FRINTy predicated with a PTRUE.\n+(define_insn \"*<frint_pattern><mode>2\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_F\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (unspec:SVE_F [(match_operand:SVE_F 2 \"register_operand\" \"w\")]\n+\t\t\t FRINT)]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"frint<frint_suffix>\\t%0.<Vetype>, %1/m, %2.<Vetype>\"\n+)\n+\n+;; Unpredicated conversion of floats to integers of the same size (HF to HI,\n+;; SF to SI or DF to DI).\n+(define_expand \"<fix_trunc_optab><mode><v_int_equiv>2\"\n+  [(set (match_operand:<V_INT_EQUIV> 0 \"register_operand\")\n+\t(unspec:<V_INT_EQUIV>\n+\t  [(match_dup 2)\n+\t   (FIXUORS:<V_INT_EQUIV>\n+\t     (match_operand:SVE_F 1 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Conversion of SF to DI, SI or HI, predicated with a PTRUE.\n+(define_insn \"*<fix_trunc_optab>v16hsf<mode>2\"\n+  [(set (match_operand:SVE_HSDI 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_HSDI\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (FIXUORS:SVE_HSDI\n+\t     (match_operand:VNx8HF 2 \"register_operand\" \"w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"fcvtz<su>\\t%0.<Vetype>, %1/m, %2.h\"\n+)\n+\n+;; Conversion of SF to DI or SI, predicated with a PTRUE.\n+(define_insn \"*<fix_trunc_optab>vnx4sf<mode>2\"\n+  [(set (match_operand:SVE_SDI 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_SDI\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (FIXUORS:SVE_SDI\n+\t     (match_operand:VNx4SF 2 \"register_operand\" \"w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"fcvtz<su>\\t%0.<Vetype>, %1/m, %2.s\"\n+)\n+\n+;; Conversion of DF to DI or SI, predicated with a PTRUE.\n+(define_insn \"*<fix_trunc_optab>vnx2df<mode>2\"\n+  [(set (match_operand:SVE_SDI 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_SDI\n+\t  [(match_operand:VNx2BI 1 \"register_operand\" \"Upl\")\n+\t   (FIXUORS:SVE_SDI\n+\t     (match_operand:VNx2DF 2 \"register_operand\" \"w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"fcvtz<su>\\t%0.<Vetype>, %1/m, %2.d\"\n+)\n+\n+;; Unpredicated conversion of integers to floats of the same size\n+;; (HI to HF, SI to SF or DI to DF).\n+(define_expand \"<optab><v_int_equiv><mode>2\"\n+  [(set (match_operand:SVE_F 0 \"register_operand\")\n+\t(unspec:SVE_F\n+\t  [(match_dup 2)\n+\t   (FLOATUORS:SVE_F\n+\t     (match_operand:<V_INT_EQUIV> 1 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = force_reg (<VPRED>mode, CONSTM1_RTX (<VPRED>mode));\n+  }\n+)\n+\n+;; Conversion of DI, SI or HI to the same number of HFs, predicated\n+;; with a PTRUE.\n+(define_insn \"*<optab><mode>vnx8hf2\"\n+  [(set (match_operand:VNx8HF 0 \"register_operand\" \"=w\")\n+\t(unspec:VNx8HF\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (FLOATUORS:VNx8HF\n+\t     (match_operand:SVE_HSDI 2 \"register_operand\" \"w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"<su_optab>cvtf\\t%0.h, %1/m, %2.<Vetype>\"\n+)\n+\n+;; Conversion of DI or SI to the same number of SFs, predicated with a PTRUE.\n+(define_insn \"*<optab><mode>vnx4sf2\"\n+  [(set (match_operand:VNx4SF 0 \"register_operand\" \"=w\")\n+\t(unspec:VNx4SF\n+\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t   (FLOATUORS:VNx4SF\n+\t     (match_operand:SVE_SDI 2 \"register_operand\" \"w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"<su_optab>cvtf\\t%0.s, %1/m, %2.<Vetype>\"\n+)\n+\n+;; Conversion of DI or SI to DF, predicated with a PTRUE.\n+(define_insn \"*<optab><mode>vnx2df2\"\n+  [(set (match_operand:VNx2DF 0 \"register_operand\" \"=w\")\n+\t(unspec:VNx2DF\n+\t  [(match_operand:VNx2BI 1 \"register_operand\" \"Upl\")\n+\t   (FLOATUORS:VNx2DF\n+\t     (match_operand:SVE_SDI 2 \"register_operand\" \"w\"))]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"<su_optab>cvtf\\t%0.d, %1/m, %2.<Vetype>\"\n+)\n+\n+;; Conversion of DFs to the same number of SFs, or SFs to the same number\n+;; of HFs.\n+(define_insn \"*trunc<Vwide><mode>2\"\n+  [(set (match_operand:SVE_HSF 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_HSF\n+\t  [(match_operand:<VWIDE_PRED> 1 \"register_operand\" \"Upl\")\n+\t   (unspec:SVE_HSF\n+\t     [(match_operand:<VWIDE> 2 \"register_operand\" \"w\")]\n+\t     UNSPEC_FLOAT_CONVERT)]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"fcvt\\t%0.<Vetype>, %1/m, %2.<Vewtype>\"\n+)\n+\n+;; Conversion of SFs to the same number of DFs, or HFs to the same number\n+;; of SFs.\n+(define_insn \"*extend<mode><Vwide>2\"\n+  [(set (match_operand:<VWIDE> 0 \"register_operand\" \"=w\")\n+\t(unspec:<VWIDE>\n+\t  [(match_operand:<VWIDE_PRED> 1 \"register_operand\" \"Upl\")\n+\t   (unspec:<VWIDE>\n+\t     [(match_operand:SVE_HSF 2 \"register_operand\" \"w\")]\n+\t     UNSPEC_FLOAT_CONVERT)]\n+\t  UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  \"fcvt\\t%0.<Vewtype>, %1/m, %2.<Vetype>\"\n+)\n+\n+;; PUNPKHI and PUNPKLO.\n+(define_insn \"vec_unpack<su>_<perm_hilo>_<mode>\"\n+  [(set (match_operand:<VWIDE> 0 \"register_operand\" \"=Upa\")\n+\t(unspec:<VWIDE> [(match_operand:PRED_BHS 1 \"register_operand\" \"Upa\")]\n+\t\t\tUNPACK))]\n+  \"TARGET_SVE\"\n+  \"punpk<perm_hilo>\\t%0.h, %1.b\"\n+)\n+\n+;; SUNPKHI, UUNPKHI, SUNPKLO and UUNPKLO.\n+(define_insn \"vec_unpack<su>_<perm_hilo>_<SVE_BHSI:mode>\"\n+  [(set (match_operand:<VWIDE> 0 \"register_operand\" \"=w\")\n+\t(unspec:<VWIDE> [(match_operand:SVE_BHSI 1 \"register_operand\" \"w\")]\n+\t\t\tUNPACK))]\n+  \"TARGET_SVE\"\n+  \"<su>unpk<perm_hilo>\\t%0.<Vewtype>, %1.<Vetype>\"\n+)\n+\n+;; Used by the vec_unpacks_<perm_hilo>_<mode> expander to unpack the bit\n+;; representation of a VNx4SF or VNx8HF without conversion.  The choice\n+;; between signed and unsigned isn't significant.\n+(define_insn \"*vec_unpacku_<perm_hilo>_<mode>_no_convert\"\n+  [(set (match_operand:SVE_HSF 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_HSF [(match_operand:SVE_HSF 1 \"register_operand\" \"w\")]\n+\t\t\tUNPACK_UNSIGNED))]\n+  \"TARGET_SVE\"\n+  \"uunpk<perm_hilo>\\t%0.<Vewtype>, %1.<Vetype>\"\n+)\n+\n+;; Unpack one half of a VNx4SF to VNx2DF, or one half of a VNx8HF to VNx4SF.\n+;; First unpack the source without conversion, then float-convert the\n+;; unpacked source.\n+(define_expand \"vec_unpacks_<perm_hilo>_<mode>\"\n+  [(set (match_dup 2)\n+\t(unspec:SVE_HSF [(match_operand:SVE_HSF 1 \"register_operand\")]\n+\t\t\tUNPACK_UNSIGNED))\n+   (set (match_operand:<VWIDE> 0 \"register_operand\")\n+\t(unspec:<VWIDE> [(match_dup 3)\n+\t\t\t (unspec:<VWIDE> [(match_dup 2)] UNSPEC_FLOAT_CONVERT)]\n+\t\t\tUNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = gen_reg_rtx (<MODE>mode);\n+    operands[3] = force_reg (<VWIDE_PRED>mode, CONSTM1_RTX (<VWIDE_PRED>mode));\n+  }\n+)\n+\n+;; Unpack one half of a VNx4SI to VNx2DF.  First unpack from VNx4SI\n+;; to VNx2DI, reinterpret the VNx2DI as a VNx4SI, then convert the\n+;; unpacked VNx4SI to VNx2DF.\n+(define_expand \"vec_unpack<su_optab>_float_<perm_hilo>_vnx4si\"\n+  [(set (match_dup 2)\n+\t(unspec:VNx2DI [(match_operand:VNx4SI 1 \"register_operand\")]\n+\t\t       UNPACK_UNSIGNED))\n+   (set (match_operand:VNx2DF 0 \"register_operand\")\n+\t(unspec:VNx2DF [(match_dup 3)\n+\t\t\t(FLOATUORS:VNx2DF (match_dup 4))]\n+\t\t       UNSPEC_MERGE_PTRUE))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[2] = gen_reg_rtx (VNx2DImode);\n+    operands[3] = force_reg (VNx2BImode, CONSTM1_RTX (VNx2BImode));\n+    operands[4] = gen_rtx_SUBREG (VNx4SImode, operands[2], 0);\n+  }\n+)\n+\n+;; Predicate pack.  Use UZP1 on the narrower type, which discards\n+;; the high part of each wide element.\n+(define_insn \"vec_pack_trunc_<Vwide>\"\n+  [(set (match_operand:PRED_BHS 0 \"register_operand\" \"=Upa\")\n+\t(unspec:PRED_BHS\n+\t  [(match_operand:<VWIDE> 1 \"register_operand\" \"Upa\")\n+\t   (match_operand:<VWIDE> 2 \"register_operand\" \"Upa\")]\n+\t  UNSPEC_PACK))]\n+  \"TARGET_SVE\"\n+  \"uzp1\\t%0.<Vetype>, %1.<Vetype>, %2.<Vetype>\"\n+)\n+\n+;; Integer pack.  Use UZP1 on the narrower type, which discards\n+;; the high part of each wide element.\n+(define_insn \"vec_pack_trunc_<Vwide>\"\n+  [(set (match_operand:SVE_BHSI 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_BHSI\n+\t  [(match_operand:<VWIDE> 1 \"register_operand\" \"w\")\n+\t   (match_operand:<VWIDE> 2 \"register_operand\" \"w\")]\n+\t  UNSPEC_PACK))]\n+  \"TARGET_SVE\"\n+  \"uzp1\\t%0.<Vetype>, %1.<Vetype>, %2.<Vetype>\"\n+)\n+\n+;; Convert two vectors of DF to SF, or two vectors of SF to HF, and pack\n+;; the results into a single vector.\n+(define_expand \"vec_pack_trunc_<Vwide>\"\n+  [(set (match_dup 4)\n+\t(unspec:SVE_HSF\n+\t  [(match_dup 3)\n+\t   (unspec:SVE_HSF [(match_operand:<VWIDE> 1 \"register_operand\")]\n+\t\t\t   UNSPEC_FLOAT_CONVERT)]\n+\t  UNSPEC_MERGE_PTRUE))\n+   (set (match_dup 5)\n+\t(unspec:SVE_HSF\n+\t  [(match_dup 3)\n+\t   (unspec:SVE_HSF [(match_operand:<VWIDE> 2 \"register_operand\")]\n+\t\t\t   UNSPEC_FLOAT_CONVERT)]\n+\t  UNSPEC_MERGE_PTRUE))\n+   (set (match_operand:SVE_HSF 0 \"register_operand\")\n+\t(unspec:SVE_HSF [(match_dup 4) (match_dup 5)] UNSPEC_UZP1))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (<VWIDE_PRED>mode, CONSTM1_RTX (<VWIDE_PRED>mode));\n+    operands[4] = gen_reg_rtx (<MODE>mode);\n+    operands[5] = gen_reg_rtx (<MODE>mode);\n+  }\n+)\n+\n+;; Convert two vectors of DF to SI and pack the results into a single vector.\n+(define_expand \"vec_pack_<su>fix_trunc_vnx2df\"\n+  [(set (match_dup 4)\n+\t(unspec:VNx4SI\n+\t  [(match_dup 3)\n+\t   (FIXUORS:VNx4SI (match_operand:VNx2DF 1 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))\n+   (set (match_dup 5)\n+\t(unspec:VNx4SI\n+\t  [(match_dup 3)\n+\t   (FIXUORS:VNx4SI (match_operand:VNx2DF 2 \"register_operand\"))]\n+\t  UNSPEC_MERGE_PTRUE))\n+   (set (match_operand:VNx4SI 0 \"register_operand\")\n+\t(unspec:VNx4SI [(match_dup 4) (match_dup 5)] UNSPEC_UZP1))]\n+  \"TARGET_SVE\"\n+  {\n+    operands[3] = force_reg (VNx2BImode, CONSTM1_RTX (VNx2BImode));\n+    operands[4] = gen_reg_rtx (VNx4SImode);\n+    operands[5] = gen_reg_rtx (VNx4SImode);\n+  }\n+)"}, {"sha": "c5ed870ef57a458ae5f8a393cfa20c58c446271e", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 2142, "deletions": 176, "changes": 2318, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=43cacb12fc859b671464b63668794158974b2a34"}, {"sha": "fc99fc4627ec8bc8f8e9dae01489ed0f2ee00459", "filename": "gcc/config/aarch64/aarch64.h", "status": "modified", "additions": 79, "deletions": 17, "changes": 96, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.h?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -144,18 +144,19 @@ extern unsigned aarch64_architecture_version;\n /* ARMv8.2-A architecture extensions.  */\n #define AARCH64_FL_V8_2       (1 << 8)  /* Has ARMv8.2-A features.  */\n #define AARCH64_FL_F16\t      (1 << 9)  /* Has ARMv8.2-A FP16 extensions.  */\n+#define AARCH64_FL_SVE        (1 << 10) /* Has Scalable Vector Extensions.  */\n /* ARMv8.3-A architecture extensions.  */\n-#define AARCH64_FL_V8_3       (1 << 10)  /* Has ARMv8.3-A features.  */\n-#define AARCH64_FL_RCPC       (1 << 11)  /* Has support for RCpc model.  */\n-#define AARCH64_FL_DOTPROD    (1 << 12)  /* Has ARMv8.2-A Dot Product ins.  */\n+#define AARCH64_FL_V8_3       (1 << 11)  /* Has ARMv8.3-A features.  */\n+#define AARCH64_FL_RCPC       (1 << 12)  /* Has support for RCpc model.  */\n+#define AARCH64_FL_DOTPROD    (1 << 13)  /* Has ARMv8.2-A Dot Product ins.  */\n /* New flags to split crypto into aes and sha2.  */\n-#define AARCH64_FL_AES\t      (1 << 13)  /* Has Crypto AES.  */\n-#define AARCH64_FL_SHA2\t      (1 << 14)  /* Has Crypto SHA2.  */\n+#define AARCH64_FL_AES\t      (1 << 14)  /* Has Crypto AES.  */\n+#define AARCH64_FL_SHA2\t      (1 << 15)  /* Has Crypto SHA2.  */\n /* ARMv8.4-A architecture extensions.  */\n-#define AARCH64_FL_V8_4\t      (1 << 15)  /* Has ARMv8.4-A features.  */\n-#define AARCH64_FL_SM4\t      (1 << 16)  /* Has ARMv8.4-A SM3 and SM4.  */\n-#define AARCH64_FL_SHA3\t      (1 << 17)  /* Has ARMv8.4-a SHA3 and SHA512.  */\n-#define AARCH64_FL_F16FML     (1 << 18)  /* Has ARMv8.4-a FP16 extensions.  */\n+#define AARCH64_FL_V8_4\t      (1 << 16)  /* Has ARMv8.4-A features.  */\n+#define AARCH64_FL_SM4\t      (1 << 17)  /* Has ARMv8.4-A SM3 and SM4.  */\n+#define AARCH64_FL_SHA3\t      (1 << 18)  /* Has ARMv8.4-a SHA3 and SHA512.  */\n+#define AARCH64_FL_F16FML     (1 << 19)  /* Has ARMv8.4-a FP16 extensions.  */\n \n /* Has FP and SIMD.  */\n #define AARCH64_FL_FPSIMD     (AARCH64_FL_FP | AARCH64_FL_SIMD)\n@@ -186,6 +187,7 @@ extern unsigned aarch64_architecture_version;\n #define AARCH64_ISA_RDMA\t   (aarch64_isa_flags & AARCH64_FL_RDMA)\n #define AARCH64_ISA_V8_2\t   (aarch64_isa_flags & AARCH64_FL_V8_2)\n #define AARCH64_ISA_F16\t\t   (aarch64_isa_flags & AARCH64_FL_F16)\n+#define AARCH64_ISA_SVE            (aarch64_isa_flags & AARCH64_FL_SVE)\n #define AARCH64_ISA_V8_3\t   (aarch64_isa_flags & AARCH64_FL_V8_3)\n #define AARCH64_ISA_DOTPROD\t   (aarch64_isa_flags & AARCH64_FL_DOTPROD)\n #define AARCH64_ISA_AES\t           (aarch64_isa_flags & AARCH64_FL_AES)\n@@ -226,6 +228,9 @@ extern unsigned aarch64_architecture_version;\n /* Dot Product is an optional extension to AdvSIMD enabled through +dotprod.  */\n #define TARGET_DOTPROD (TARGET_SIMD && AARCH64_ISA_DOTPROD)\n \n+/* SVE instructions, enabled through +sve.  */\n+#define TARGET_SVE (AARCH64_ISA_SVE)\n+\n /* ARMv8.3-A features.  */\n #define TARGET_ARMV8_3\t(AARCH64_ISA_V8_3)\n \n@@ -286,8 +291,17 @@ extern unsigned aarch64_architecture_version;\n    V0-V7\tParameter/result registers\n \n    The vector register V0 holds scalar B0, H0, S0 and D0 in its least\n-   significant bits.  Unlike AArch32 S1 is not packed into D0,\n-   etc.  */\n+   significant bits.  Unlike AArch32 S1 is not packed into D0, etc.\n+\n+   P0-P7        Predicate low registers: valid in all predicate contexts\n+   P8-P15       Predicate high registers: used as scratch space\n+\n+   VG           Pseudo \"vector granules\" register\n+\n+   VG is the number of 64-bit elements in an SVE vector.  We define\n+   it as a hard register so that we can easily map it to the DWARF VG\n+   register.  GCC internally uses the poly_int variable aarch64_sve_vg\n+   instead.  */\n \n /* Note that we don't mark X30 as a call-clobbered register.  The idea is\n    that it's really the call instructions themselves which clobber X30.\n@@ -308,7 +322,9 @@ extern unsigned aarch64_architecture_version;\n     0, 0, 0, 0,   0, 0, 0, 0,   /* V8 - V15 */\t\t\\\n     0, 0, 0, 0,   0, 0, 0, 0,   /* V16 - V23 */         \\\n     0, 0, 0, 0,   0, 0, 0, 0,   /* V24 - V31 */         \\\n-    1, 1, 1,\t\t\t/* SFP, AP, CC */\t\\\n+    1, 1, 1, 1,\t\t\t/* SFP, AP, CC, VG */\t\\\n+    0, 0, 0, 0,   0, 0, 0, 0,   /* P0 - P7 */           \\\n+    0, 0, 0, 0,   0, 0, 0, 0,   /* P8 - P15 */          \\\n   }\n \n #define CALL_USED_REGISTERS\t\t\t\t\\\n@@ -321,7 +337,9 @@ extern unsigned aarch64_architecture_version;\n     0, 0, 0, 0,   0, 0, 0, 0,\t/* V8 - V15 */\t\t\\\n     1, 1, 1, 1,   1, 1, 1, 1,   /* V16 - V23 */         \\\n     1, 1, 1, 1,   1, 1, 1, 1,   /* V24 - V31 */         \\\n-    1, 1, 1,\t\t\t/* SFP, AP, CC */\t\\\n+    1, 1, 1, 1,\t\t\t/* SFP, AP, CC, VG */\t\\\n+    1, 1, 1, 1,   1, 1, 1, 1,\t/* P0 - P7 */\t\t\\\n+    1, 1, 1, 1,   1, 1, 1, 1,\t/* P8 - P15 */\t\t\\\n   }\n \n #define REGISTER_NAMES\t\t\t\t\t\t\\\n@@ -334,7 +352,9 @@ extern unsigned aarch64_architecture_version;\n     \"v8\",  \"v9\",  \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\",\t\\\n     \"v16\", \"v17\", \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\",\t\\\n     \"v24\", \"v25\", \"v26\", \"v27\", \"v28\", \"v29\", \"v30\", \"v31\",\t\\\n-    \"sfp\", \"ap\",  \"cc\",\t\t\t\t\t\t\\\n+    \"sfp\", \"ap\",  \"cc\",  \"vg\",\t\t\t\t\t\\\n+    \"p0\",  \"p1\",  \"p2\",  \"p3\",  \"p4\",  \"p5\",  \"p6\",  \"p7\",\t\\\n+    \"p8\",  \"p9\",  \"p10\", \"p11\", \"p12\", \"p13\", \"p14\", \"p15\",\t\\\n   }\n \n /* Generate the register aliases for core register N */\n@@ -345,7 +365,8 @@ extern unsigned aarch64_architecture_version;\n                      {\"d\" # N, V0_REGNUM + (N)}, \\\n                      {\"s\" # N, V0_REGNUM + (N)}, \\\n                      {\"h\" # N, V0_REGNUM + (N)}, \\\n-                     {\"b\" # N, V0_REGNUM + (N)}\n+                     {\"b\" # N, V0_REGNUM + (N)}, \\\n+                     {\"z\" # N, V0_REGNUM + (N)}\n \n /* Provide aliases for all of the ISA defined register name forms.\n    These aliases are convenient for use in the clobber lists of inline\n@@ -387,7 +408,7 @@ extern unsigned aarch64_architecture_version;\n #define FRAME_POINTER_REGNUM\t\tSFP_REGNUM\n #define STACK_POINTER_REGNUM\t\tSP_REGNUM\n #define ARG_POINTER_REGNUM\t\tAP_REGNUM\n-#define FIRST_PSEUDO_REGISTER\t\t67\n+#define FIRST_PSEUDO_REGISTER\t\t(P15_REGNUM + 1)\n \n /* The number of (integer) argument register available.  */\n #define NUM_ARG_REGS\t\t\t8\n@@ -408,6 +429,8 @@ extern unsigned aarch64_architecture_version;\n #define AARCH64_DWARF_NUMBER_R 31\n \n #define AARCH64_DWARF_SP       31\n+#define AARCH64_DWARF_VG       46\n+#define AARCH64_DWARF_P0       48\n #define AARCH64_DWARF_V0       64\n \n /* The number of V registers.  */\n@@ -472,6 +495,12 @@ extern unsigned aarch64_architecture_version;\n #define FP_LO_REGNUM_P(REGNO)            \\\n   (((unsigned) (REGNO - V0_REGNUM)) <= (V15_REGNUM - V0_REGNUM))\n \n+#define PR_REGNUM_P(REGNO)\\\n+  (((unsigned) (REGNO - P0_REGNUM)) <= (P15_REGNUM - P0_REGNUM))\n+\n+#define PR_LO_REGNUM_P(REGNO)\\\n+  (((unsigned) (REGNO - P0_REGNUM)) <= (P7_REGNUM - P0_REGNUM))\n+\n \f\n /* Register and constant classes.  */\n \n@@ -485,6 +514,9 @@ enum reg_class\n   FP_LO_REGS,\n   FP_REGS,\n   POINTER_AND_FP_REGS,\n+  PR_LO_REGS,\n+  PR_HI_REGS,\n+  PR_REGS,\n   ALL_REGS,\n   LIM_REG_CLASSES\t\t/* Last */\n };\n@@ -501,6 +533,9 @@ enum reg_class\n   \"FP_LO_REGS\",\t\t\t\t\t\\\n   \"FP_REGS\",\t\t\t\t\t\\\n   \"POINTER_AND_FP_REGS\",\t\t\t\\\n+  \"PR_LO_REGS\",\t\t\t\t\t\\\n+  \"PR_HI_REGS\",\t\t\t\t\t\\\n+  \"PR_REGS\",\t\t\t\t\t\\\n   \"ALL_REGS\"\t\t\t\t\t\\\n }\n \n@@ -514,7 +549,10 @@ enum reg_class\n   { 0x00000000, 0x0000ffff, 0x00000000 },       /* FP_LO_REGS  */\t\\\n   { 0x00000000, 0xffffffff, 0x00000000 },       /* FP_REGS  */\t\t\\\n   { 0xffffffff, 0xffffffff, 0x00000003 },\t/* POINTER_AND_FP_REGS */\\\n-  { 0xffffffff, 0xffffffff, 0x00000007 }\t/* ALL_REGS */\t\t\\\n+  { 0x00000000, 0x00000000, 0x00000ff0 },\t/* PR_LO_REGS */\t\\\n+  { 0x00000000, 0x00000000, 0x000ff000 },\t/* PR_HI_REGS */\t\\\n+  { 0x00000000, 0x00000000, 0x000ffff0 },\t/* PR_REGS */\t\t\\\n+  { 0xffffffff, 0xffffffff, 0x000fffff }\t/* ALL_REGS */\t\t\\\n }\n \n #define REGNO_REG_CLASS(REGNO)\taarch64_regno_regclass (REGNO)\n@@ -998,4 +1036,28 @@ extern tree aarch64_fp16_ptr_type_node;\n #define LIBGCC2_UNWIND_ATTRIBUTE \\\n   __attribute__((optimize (\"no-omit-frame-pointer\")))\n \n+#ifndef USED_FOR_TARGET\n+extern poly_uint16 aarch64_sve_vg;\n+\n+/* The number of bits and bytes in an SVE vector.  */\n+#define BITS_PER_SVE_VECTOR (poly_uint16 (aarch64_sve_vg * 64))\n+#define BYTES_PER_SVE_VECTOR (poly_uint16 (aarch64_sve_vg * 8))\n+\n+/* The number of bytes in an SVE predicate.  */\n+#define BYTES_PER_SVE_PRED aarch64_sve_vg\n+\n+/* The SVE mode for a vector of bytes.  */\n+#define SVE_BYTE_MODE VNx16QImode\n+\n+/* The maximum number of bytes in a fixed-size vector.  This is 256 bytes\n+   (for -msve-vector-bits=2048) multiplied by the maximum number of\n+   vectors in a structure mode (4).\n+\n+   This limit must not be used for variable-size vectors, since\n+   VL-agnostic code must work with arbitary vector lengths.  */\n+#define MAX_COMPILE_TIME_VEC_BYTES (256 * 4)\n+#endif\n+\n+#define REGMODE_NATURAL_SIZE(MODE) aarch64_regmode_natural_size (MODE)\n+\n #endif /* GCC_AARCH64_H */"}, {"sha": "728136a7fbaabc7e87a1f77be84e3face4257b3f", "filename": "gcc/config/aarch64/aarch64.md", "status": "modified", "additions": 147, "deletions": 36, "changes": 183, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.md?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -63,6 +63,11 @@\n     (SFP_REGNUM\t\t64)\n     (AP_REGNUM\t\t65)\n     (CC_REGNUM\t\t66)\n+    ;; Defined only to make the DWARF description simpler.\n+    (VG_REGNUM\t\t67)\n+    (P0_REGNUM\t\t68)\n+    (P7_REGNUM\t\t75)\n+    (P15_REGNUM\t\t83)\n   ]\n )\n \n@@ -114,6 +119,7 @@\n     UNSPEC_PACI1716\n     UNSPEC_PACISP\n     UNSPEC_PRLG_STK\n+    UNSPEC_REV\n     UNSPEC_RBIT\n     UNSPEC_SCVTF\n     UNSPEC_SISD_NEG\n@@ -143,6 +149,18 @@\n     UNSPEC_RSQRTS\n     UNSPEC_NZCV\n     UNSPEC_XPACLRI\n+    UNSPEC_LD1_SVE\n+    UNSPEC_ST1_SVE\n+    UNSPEC_LD1RQ\n+    UNSPEC_MERGE_PTRUE\n+    UNSPEC_PTEST_PTRUE\n+    UNSPEC_UNPACKSHI\n+    UNSPEC_UNPACKUHI\n+    UNSPEC_UNPACKSLO\n+    UNSPEC_UNPACKULO\n+    UNSPEC_PACK\n+    UNSPEC_FLOAT_CONVERT\n+    UNSPEC_WHILE_LO\n ])\n \n (define_c_enum \"unspecv\" [\n@@ -194,6 +212,11 @@\n ;; will be disabled when !TARGET_SIMD.\n (define_attr \"simd\" \"no,yes\" (const_string \"no\"))\n \n+;; Attribute that specifies whether or not the instruction uses SVE.\n+;; When this is set to yes for an alternative, that alternative\n+;; will be disabled when !TARGET_SVE.\n+(define_attr \"sve\" \"no,yes\" (const_string \"no\"))\n+\n (define_attr \"length\" \"\"\n   (const_int 4))\n \n@@ -202,13 +225,14 @@\n ;; registers when -mgeneral-regs-only is specified.\n (define_attr \"enabled\" \"no,yes\"\n   (cond [(ior\n-\t    (ior\n-\t\t(and (eq_attr \"fp\" \"yes\")\n-\t\t     (eq (symbol_ref \"TARGET_FLOAT\") (const_int 0)))\n-\t\t(and (eq_attr \"simd\" \"yes\")\n-\t\t     (eq (symbol_ref \"TARGET_SIMD\") (const_int 0))))\n+\t    (and (eq_attr \"fp\" \"yes\")\n+\t\t (eq (symbol_ref \"TARGET_FLOAT\") (const_int 0)))\n+\t    (and (eq_attr \"simd\" \"yes\")\n+\t\t (eq (symbol_ref \"TARGET_SIMD\") (const_int 0)))\n \t    (and (eq_attr \"fp16\" \"yes\")\n-\t\t (eq (symbol_ref \"TARGET_FP_F16INST\") (const_int 0))))\n+\t\t (eq (symbol_ref \"TARGET_FP_F16INST\") (const_int 0)))\n+\t    (and (eq_attr \"sve\" \"yes\")\n+\t\t (eq (symbol_ref \"TARGET_SVE\") (const_int 0))))\n \t    (const_string \"no\")\n \t] (const_string \"yes\")))\n \n@@ -866,12 +890,18 @@\n   \"\n     if (GET_CODE (operands[0]) == MEM && operands[1] != const0_rtx)\n       operands[1] = force_reg (<MODE>mode, operands[1]);\n+\n+    if (GET_CODE (operands[1]) == CONST_POLY_INT)\n+      {\n+\taarch64_expand_mov_immediate (operands[0], operands[1]);\n+\tDONE;\n+      }\n   \"\n )\n \n (define_insn \"*mov<mode>_aarch64\"\n-  [(set (match_operand:SHORT 0 \"nonimmediate_operand\" \"=r,r,   *w,r,*w, m, m, r,*w,*w\")\n-        (match_operand:SHORT 1 \"general_operand\"      \" r,M,D<hq>,m, m,rZ,*w,*w, r,*w\"))]\n+  [(set (match_operand:SHORT 0 \"nonimmediate_operand\" \"=r,r,   *w,r ,r,*w, m, m, r,*w,*w\")\n+\t(match_operand:SHORT 1 \"aarch64_mov_operand\"  \" r,M,D<hq>,Usv,m, m,rZ,*w,*w, r,*w\"))]\n   \"(register_operand (operands[0], <MODE>mode)\n     || aarch64_reg_or_zero (operands[1], <MODE>mode))\"\n {\n@@ -885,26 +915,30 @@\n        return aarch64_output_scalar_simd_mov_immediate (operands[1],\n \t\t\t\t\t\t\t<MODE>mode);\n      case 3:\n-       return \"ldr<size>\\t%w0, %1\";\n+       return aarch64_output_sve_cnt_immediate (\\\"cnt\\\", \\\"%x0\\\", operands[1]);\n      case 4:\n-       return \"ldr\\t%<size>0, %1\";\n+       return \"ldr<size>\\t%w0, %1\";\n      case 5:\n-       return \"str<size>\\t%w1, %0\";\n+       return \"ldr\\t%<size>0, %1\";\n      case 6:\n-       return \"str\\t%<size>1, %0\";\n+       return \"str<size>\\t%w1, %0\";\n      case 7:\n-       return \"umov\\t%w0, %1.<v>[0]\";\n+       return \"str\\t%<size>1, %0\";\n      case 8:\n-       return \"dup\\t%0.<Vallxd>, %w1\";\n+       return \"umov\\t%w0, %1.<v>[0]\";\n      case 9:\n+       return \"dup\\t%0.<Vallxd>, %w1\";\n+     case 10:\n        return \"dup\\t%<Vetype>0, %1.<v>[0]\";\n      default:\n        gcc_unreachable ();\n      }\n }\n-  [(set_attr \"type\" \"mov_reg,mov_imm,neon_move,load_4,load_4,store_4,store_4,\\\n-                     neon_to_gp<q>,neon_from_gp<q>,neon_dup\")\n-   (set_attr \"simd\" \"*,*,yes,*,*,*,*,yes,yes,yes\")]\n+  ;; The \"mov_imm\" type for CNT is just a placeholder.\n+  [(set_attr \"type\" \"mov_reg,mov_imm,neon_move,mov_imm,load_4,load_4,store_4,\n+\t\t     store_4,neon_to_gp<q>,neon_from_gp<q>,neon_dup\")\n+   (set_attr \"simd\" \"*,*,yes,*,*,*,*,*,yes,yes,yes\")\n+   (set_attr \"sve\" \"*,*,*,yes,*,*,*,*,*,*,*\")]\n )\n \n (define_expand \"mov<mode>\"\n@@ -932,8 +966,8 @@\n )\n \n (define_insn_and_split \"*movsi_aarch64\"\n-  [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=r,k,r,r,r,r,w, m, m,  r,  r, w,r,w, w\")\n-\t(match_operand:SI 1 \"aarch64_mov_operand\"  \" r,r,k,M,n,m,m,rZ,*w,Usa,Ush,rZ,w,w,Ds\"))]\n+  [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=r,k,r,r,r,r, r,w, m, m,  r,  r, w,r,w, w\")\n+\t(match_operand:SI 1 \"aarch64_mov_operand\"  \" r,r,k,M,n,Usv,m,m,rZ,*w,Usa,Ush,rZ,w,w,Ds\"))]\n   \"(register_operand (operands[0], SImode)\n     || aarch64_reg_or_zero (operands[1], SImode))\"\n   \"@\n@@ -942,6 +976,7 @@\n    mov\\\\t%w0, %w1\n    mov\\\\t%w0, %1\n    #\n+   * return aarch64_output_sve_cnt_immediate (\\\"cnt\\\", \\\"%x0\\\", operands[1]);\n    ldr\\\\t%w0, %1\n    ldr\\\\t%s0, %1\n    str\\\\t%w1, %0\n@@ -959,15 +994,17 @@\n        aarch64_expand_mov_immediate (operands[0], operands[1]);\n        DONE;\n     }\"\n-  [(set_attr \"type\" \"mov_reg,mov_reg,mov_reg,mov_imm,mov_imm,load_4,load_4,store_4,store_4,\\\n-\t\t    adr,adr,f_mcr,f_mrc,fmov,neon_move\")\n-   (set_attr \"fp\" \"*,*,*,*,*,*,yes,*,yes,*,*,yes,yes,yes,*\")\n-   (set_attr \"simd\" \"*,*,*,*,*,*,*,*,*,*,*,*,*,*,yes\")]\n+  ;; The \"mov_imm\" type for CNT is just a placeholder.\n+  [(set_attr \"type\" \"mov_reg,mov_reg,mov_reg,mov_imm,mov_imm,mov_imm,load_4,\n+\t\t    load_4,store_4,store_4,adr,adr,f_mcr,f_mrc,fmov,neon_move\")\n+   (set_attr \"fp\" \"*,*,*,*,*,*,*,yes,*,yes,*,*,yes,yes,yes,*\")\n+   (set_attr \"simd\" \"*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,yes\")\n+   (set_attr \"sve\" \"*,*,*,*,*,yes,*,*,*,*,*,*,*,*,*,*\")]\n )\n \n (define_insn_and_split \"*movdi_aarch64\"\n-  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=r,k,r,r,r,r,r,w, m,m,  r,  r, w,r,w, w\")\n-\t(match_operand:DI 1 \"aarch64_mov_operand\"  \" r,r,k,N,M,n,m,m,rZ,w,Usa,Ush,rZ,w,w,Dd\"))]\n+  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=r,k,r,r,r,r,r, r,w, m,m,  r,  r, w,r,w, w\")\n+\t(match_operand:DI 1 \"aarch64_mov_operand\"  \" r,r,k,N,M,n,Usv,m,m,rZ,w,Usa,Ush,rZ,w,w,Dd\"))]\n   \"(register_operand (operands[0], DImode)\n     || aarch64_reg_or_zero (operands[1], DImode))\"\n   \"@\n@@ -977,6 +1014,7 @@\n    mov\\\\t%x0, %1\n    mov\\\\t%w0, %1\n    #\n+   * return aarch64_output_sve_cnt_immediate (\\\"cnt\\\", \\\"%x0\\\", operands[1]);\n    ldr\\\\t%x0, %1\n    ldr\\\\t%d0, %1\n    str\\\\t%x1, %0\n@@ -994,10 +1032,13 @@\n        aarch64_expand_mov_immediate (operands[0], operands[1]);\n        DONE;\n     }\"\n-  [(set_attr \"type\" \"mov_reg,mov_reg,mov_reg,mov_imm,mov_imm,mov_imm,load_8,\\\n-                     load_8,store_8,store_8,adr,adr,f_mcr,f_mrc,fmov,neon_move\")\n-   (set_attr \"fp\" \"*,*,*,*,*,*,*,yes,*,yes,*,*,yes,yes,yes,*\")\n-   (set_attr \"simd\" \"*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,yes\")]\n+  ;; The \"mov_imm\" type for CNTD is just a placeholder.\n+  [(set_attr \"type\" \"mov_reg,mov_reg,mov_reg,mov_imm,mov_imm,mov_imm,mov_imm,\n+\t\t     load_8,load_8,store_8,store_8,adr,adr,f_mcr,f_mrc,fmov,\n+\t\t     neon_move\")\n+   (set_attr \"fp\" \"*,*,*,*,*,*,*,*,yes,*,yes,*,*,yes,yes,yes,*\")\n+   (set_attr \"simd\" \"*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,yes\")\n+   (set_attr \"sve\" \"*,*,*,*,*,*,yes,*,*,*,*,*,*,*,*,*,*\")]\n )\n \n (define_insn \"insv_imm<mode>\"\n@@ -1018,6 +1059,14 @@\n   \"\n     if (GET_CODE (operands[0]) == MEM && operands[1] != const0_rtx)\n       operands[1] = force_reg (TImode, operands[1]);\n+\n+    if (GET_CODE (operands[1]) == CONST_POLY_INT)\n+      {\n+\temit_move_insn (gen_lowpart (DImode, operands[0]),\n+\t\t\tgen_lowpart (DImode, operands[1]));\n+\temit_move_insn (gen_highpart (DImode, operands[0]), const0_rtx);\n+\tDONE;\n+      }\n   \"\n )\n \n@@ -1542,7 +1591,7 @@\n   [(set\n     (match_operand:GPI 0 \"register_operand\" \"\")\n     (plus:GPI (match_operand:GPI 1 \"register_operand\" \"\")\n-\t      (match_operand:GPI 2 \"aarch64_pluslong_operand\" \"\")))]\n+\t      (match_operand:GPI 2 \"aarch64_pluslong_or_poly_operand\" \"\")))]\n   \"\"\n {\n   /* If operands[1] is a subreg extract the inner RTX.  */\n@@ -1555,23 +1604,34 @@\n       && (!REG_P (op1)\n \t || !REGNO_PTR_FRAME_P (REGNO (op1))))\n     operands[2] = force_reg (<MODE>mode, operands[2]);\n+  /* Expand polynomial additions now if the destination is the stack\n+     pointer, since we don't want to use that as a temporary.  */\n+  else if (operands[0] == stack_pointer_rtx\n+\t   && aarch64_split_add_offset_immediate (operands[2], <MODE>mode))\n+    {\n+      aarch64_split_add_offset (<MODE>mode, operands[0], operands[1],\n+\t\t\t\toperands[2], NULL_RTX, NULL_RTX);\n+      DONE;\n+    }\n })\n \n (define_insn \"*add<mode>3_aarch64\"\n   [(set\n-    (match_operand:GPI 0 \"register_operand\" \"=rk,rk,w,rk,r\")\n+    (match_operand:GPI 0 \"register_operand\" \"=rk,rk,w,rk,r,rk\")\n     (plus:GPI\n-     (match_operand:GPI 1 \"register_operand\" \"%rk,rk,w,rk,rk\")\n-     (match_operand:GPI 2 \"aarch64_pluslong_operand\" \"I,r,w,J,Uaa\")))]\n+     (match_operand:GPI 1 \"register_operand\" \"%rk,rk,w,rk,rk,rk\")\n+     (match_operand:GPI 2 \"aarch64_pluslong_operand\" \"I,r,w,J,Uaa,Uav\")))]\n   \"\"\n   \"@\n   add\\\\t%<w>0, %<w>1, %2\n   add\\\\t%<w>0, %<w>1, %<w>2\n   add\\\\t%<rtn>0<vas>, %<rtn>1<vas>, %<rtn>2<vas>\n   sub\\\\t%<w>0, %<w>1, #%n2\n-  #\"\n-  [(set_attr \"type\" \"alu_imm,alu_sreg,neon_add,alu_imm,multiple\")\n-   (set_attr \"simd\" \"*,*,yes,*,*\")]\n+  #\n+  * return aarch64_output_sve_addvl_addpl (operands[0], operands[1], operands[2]);\"\n+  ;; The \"alu_imm\" type for ADDVL/ADDPL is just a placeholder.\n+  [(set_attr \"type\" \"alu_imm,alu_sreg,neon_add,alu_imm,multiple,alu_imm\")\n+   (set_attr \"simd\" \"*,*,yes,*,*,*\")]\n )\n \n ;; zero_extend version of above\n@@ -1633,6 +1693,48 @@\n   }\n )\n \n+;; Match addition of polynomial offsets that require one temporary, for which\n+;; we can use the early-clobbered destination register.  This is a separate\n+;; pattern so that the early clobber doesn't affect register allocation\n+;; for other forms of addition.  However, we still need to provide an\n+;; all-register alternative, in case the offset goes out of range after\n+;; elimination.  For completeness we might as well provide all GPR-based\n+;; alternatives from the main pattern.\n+;;\n+;; We don't have a pattern for additions requiring two temporaries since at\n+;; present LRA doesn't allow new scratches to be added during elimination.\n+;; Such offsets should be rare anyway.\n+;;\n+;; ??? But if we added LRA support for new scratches, much of the ugliness\n+;; here would go away.  We could just handle all polynomial constants in\n+;; this pattern.\n+(define_insn_and_split \"*add<mode>3_poly_1\"\n+  [(set\n+    (match_operand:GPI 0 \"register_operand\" \"=r,r,r,r,r,&r\")\n+    (plus:GPI\n+     (match_operand:GPI 1 \"register_operand\" \"%rk,rk,rk,rk,rk,rk\")\n+     (match_operand:GPI 2 \"aarch64_pluslong_or_poly_operand\" \"I,r,J,Uaa,Uav,Uat\")))]\n+  \"TARGET_SVE && operands[0] != stack_pointer_rtx\"\n+  \"@\n+  add\\\\t%<w>0, %<w>1, %2\n+  add\\\\t%<w>0, %<w>1, %<w>2\n+  sub\\\\t%<w>0, %<w>1, #%n2\n+  #\n+  * return aarch64_output_sve_addvl_addpl (operands[0], operands[1], operands[2]);\n+  #\"\n+  \"&& epilogue_completed\n+   && !reg_overlap_mentioned_p (operands[0], operands[1])\n+   && aarch64_split_add_offset_immediate (operands[2], <MODE>mode)\"\n+  [(const_int 0)]\n+  {\n+    aarch64_split_add_offset (<MODE>mode, operands[0], operands[1],\n+\t\t\t      operands[2], operands[0], NULL_RTX);\n+    DONE;\n+  }\n+  ;; The \"alu_imm\" type for ADDVL/ADDPL is just a placeholder.\n+  [(set_attr \"type\" \"alu_imm,alu_sreg,alu_imm,multiple,alu_imm,multiple\")]\n+)\n+\n (define_split\n   [(set (match_operand:DI 0 \"register_operand\")\n \t(zero_extend:DI\n@@ -5797,6 +5899,12 @@\n   DONE;\n })\n \n+;; Helper for aarch64.c code.\n+(define_expand \"set_clobber_cc\"\n+  [(parallel [(set (match_operand 0)\n+\t\t   (match_operand 1))\n+\t      (clobber (reg:CC CC_REGNUM))])])\n+\n ;; AdvSIMD Stuff\n (include \"aarch64-simd.md\")\n \n@@ -5805,3 +5913,6 @@\n \n ;; ldp/stp peephole patterns\n (include \"aarch64-ldpstp.md\")\n+\n+;; SVE.\n+(include \"aarch64-sve.md\")"}, {"sha": "52eaf8c6f408fb640dbc858d4cf4a70054fe8082", "filename": "gcc/config/aarch64/aarch64.opt", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Faarch64.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.opt?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -185,6 +185,32 @@ Enable the division approximation.  Enabling this reduces\n precision of division results to about 16 bits for\n single precision and to 32 bits for double precision.\n \n+Enum\n+Name(sve_vector_bits) Type(enum aarch64_sve_vector_bits_enum)\n+The possible SVE vector lengths:\n+\n+EnumValue\n+Enum(sve_vector_bits) String(scalable) Value(SVE_SCALABLE)\n+\n+EnumValue\n+Enum(sve_vector_bits) String(128) Value(SVE_128)\n+\n+EnumValue\n+Enum(sve_vector_bits) String(256) Value(SVE_256)\n+\n+EnumValue\n+Enum(sve_vector_bits) String(512) Value(SVE_512)\n+\n+EnumValue\n+Enum(sve_vector_bits) String(1024) Value(SVE_1024)\n+\n+EnumValue\n+Enum(sve_vector_bits) String(2048) Value(SVE_2048)\n+\n+msve-vector-bits=\n+Target RejectNegative Joined Enum(sve_vector_bits) Var(aarch64_sve_vector_bits) Init(SVE_SCALABLE)\n+-msve-vector-bits=N\tSet the number of bits in an SVE vector register to N.\n+\n mverbose-cost-dump\n Common Undocumented Var(flag_aarch64_verbose_cost)\n Enables verbose cost model dumping in the debug dump files."}, {"sha": "b004f7888e188c09cee8d74de7850504ac096497", "filename": "gcc/config/aarch64/constraints.md", "status": "modified", "additions": 114, "deletions": 6, "changes": 120, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Fconstraints.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Fconstraints.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fconstraints.md?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -27,6 +27,12 @@\n (define_register_constraint \"w\" \"FP_REGS\"\n   \"Floating point and SIMD vector registers.\")\n \n+(define_register_constraint \"Upa\" \"PR_REGS\"\n+  \"SVE predicate registers p0 - p15.\")\n+\n+(define_register_constraint \"Upl\" \"PR_LO_REGS\"\n+  \"SVE predicate registers p0 - p7.\")\n+\n (define_register_constraint \"x\" \"FP_LO_REGS\"\n   \"Floating point and SIMD vector registers V0 - V15.\")\n \n@@ -40,6 +46,18 @@\n   (and (match_code \"const_int\")\n        (match_test \"aarch64_pluslong_strict_immedate (op, VOIDmode)\")))\n \n+(define_constraint \"Uav\"\n+  \"@internal\n+   A constraint that matches a VG-based constant that can be added by\n+   a single ADDVL or ADDPL.\"\n+ (match_operand 0 \"aarch64_sve_addvl_addpl_immediate\"))\n+\n+(define_constraint \"Uat\"\n+  \"@internal\n+   A constraint that matches a VG-based constant that can be added by\n+   using multiple instructions, with one temporary register.\"\n+ (match_operand 0 \"aarch64_split_add_offset_immediate\"))\n+\n (define_constraint \"J\"\n  \"A constant that can be used with a SUB operation (once negated).\"\n  (and (match_code \"const_int\")\n@@ -134,6 +152,18 @@\n   A constraint that matches the immediate constant -1.\"\n   (match_test \"op == constm1_rtx\"))\n \n+(define_constraint \"Usv\"\n+  \"@internal\n+   A constraint that matches a VG-based constant that can be loaded by\n+   a single CNT[BHWD].\"\n+ (match_operand 0 \"aarch64_sve_cnt_immediate\"))\n+\n+(define_constraint \"Usi\"\n+  \"@internal\n+ A constraint that matches an immediate operand valid for\n+ the SVE INDEX instruction.\"\n+ (match_operand 0 \"aarch64_sve_index_immediate\"))\n+\n (define_constraint \"Ui1\"\n   \"@internal\n   A constraint that matches the immediate constant +1.\"\n@@ -192,6 +222,13 @@\n        (match_test \"aarch64_legitimate_address_p (DFmode, XEXP (op, 0), 1,\n \t\t\t\t\t\t  ADDR_QUERY_LDP_STP)\")))\n \n+(define_memory_constraint \"Utr\"\n+  \"@internal\n+   An address valid for SVE LDR and STR instructions (as distinct from\n+   LD[1234] and ST[1234] patterns).\"\n+  (and (match_code \"mem\")\n+       (match_test \"aarch64_sve_ldr_operand_p (op)\")))\n+\n (define_memory_constraint \"Utv\"\n   \"@internal\n    An address valid for loading/storing opaque structure\n@@ -206,6 +243,12 @@\n        (match_test \"aarch64_legitimate_address_p (V2DImode,\n \t\t\t\t\t\t  XEXP (op, 0), 1)\")))\n \n+(define_memory_constraint \"Uty\"\n+  \"@internal\n+   An address valid for SVE LD1Rs.\"\n+  (and (match_code \"mem\")\n+       (match_test \"aarch64_sve_ld1r_operand_p (op)\")))\n+\n (define_constraint \"Ufc\"\n   \"A floating point constant which can be used with an\\\n    FMOV immediate operation.\"\n@@ -235,7 +278,7 @@\n (define_constraint \"Dn\"\n   \"@internal\n  A constraint that matches vector of immediates.\"\n- (and (match_code \"const_vector\")\n+ (and (match_code \"const,const_vector\")\n       (match_test \"aarch64_simd_valid_immediate (op, NULL)\")))\n \n (define_constraint \"Dh\"\n@@ -257,21 +300,27 @@\n (define_constraint \"Dl\"\n   \"@internal\n  A constraint that matches vector of immediates for left shifts.\"\n- (and (match_code \"const_vector\")\n+ (and (match_code \"const,const_vector\")\n       (match_test \"aarch64_simd_shift_imm_p (op, GET_MODE (op),\n \t\t\t\t\t\t true)\")))\n \n (define_constraint \"Dr\"\n   \"@internal\n  A constraint that matches vector of immediates for right shifts.\"\n- (and (match_code \"const_vector\")\n+ (and (match_code \"const,const_vector\")\n       (match_test \"aarch64_simd_shift_imm_p (op, GET_MODE (op),\n \t\t\t\t\t\t false)\")))\n (define_constraint \"Dz\"\n   \"@internal\n- A constraint that matches vector of immediate zero.\"\n- (and (match_code \"const_vector\")\n-      (match_test \"aarch64_simd_imm_zero_p (op, GET_MODE (op))\")))\n+ A constraint that matches a vector of immediate zero.\"\n+ (and (match_code \"const,const_vector\")\n+      (match_test \"op == CONST0_RTX (GET_MODE (op))\")))\n+\n+(define_constraint \"Dm\"\n+  \"@internal\n+ A constraint that matches a vector of immediate minus one.\"\n+ (and (match_code \"const,const_vector\")\n+      (match_test \"op == CONST1_RTX (GET_MODE (op))\")))\n \n (define_constraint \"Dd\"\n   \"@internal\n@@ -291,3 +340,62 @@\n   \"@internal\n  An address valid for a prefetch instruction.\"\n  (match_test \"aarch64_address_valid_for_prefetch_p (op, true)\"))\n+\n+(define_constraint \"vsa\"\n+  \"@internal\n+   A constraint that matches an immediate operand valid for SVE\n+   arithmetic instructions.\"\n+ (match_operand 0 \"aarch64_sve_arith_immediate\"))\n+\n+(define_constraint \"vsc\"\n+  \"@internal\n+   A constraint that matches a signed immediate operand valid for SVE\n+   CMP instructions.\"\n+ (match_operand 0 \"aarch64_sve_cmp_vsc_immediate\"))\n+\n+(define_constraint \"vsd\"\n+  \"@internal\n+   A constraint that matches an unsigned immediate operand valid for SVE\n+   CMP instructions.\"\n+ (match_operand 0 \"aarch64_sve_cmp_vsd_immediate\"))\n+\n+(define_constraint \"vsi\"\n+  \"@internal\n+   A constraint that matches a vector count operand valid for SVE INC and\n+   DEC instructions.\"\n+ (match_operand 0 \"aarch64_sve_inc_dec_immediate\"))\n+\n+(define_constraint \"vsn\"\n+  \"@internal\n+   A constraint that matches an immediate operand whose negative\n+   is valid for SVE SUB instructions.\"\n+ (match_operand 0 \"aarch64_sve_sub_arith_immediate\"))\n+\n+(define_constraint \"vsl\"\n+  \"@internal\n+   A constraint that matches an immediate operand valid for SVE logical\n+   operations.\"\n+ (match_operand 0 \"aarch64_sve_logical_immediate\"))\n+\n+(define_constraint \"vsm\"\n+  \"@internal\n+   A constraint that matches an immediate operand valid for SVE MUL\n+   operations.\"\n+ (match_operand 0 \"aarch64_sve_mul_immediate\"))\n+\n+(define_constraint \"vsA\"\n+  \"@internal\n+   A constraint that matches an immediate operand valid for SVE FADD\n+   and FSUB operations.\"\n+ (match_operand 0 \"aarch64_sve_float_arith_immediate\"))\n+\n+(define_constraint \"vsM\"\n+  \"@internal\n+   A constraint that matches an imediate operand valid for SVE FMUL\n+   operations.\"\n+ (match_operand 0 \"aarch64_sve_float_mul_immediate\"))\n+\n+(define_constraint \"vsN\"\n+  \"@internal\n+   A constraint that matches the negative of vsA\"\n+ (match_operand 0 \"aarch64_sve_float_arith_with_sub_immediate\"))"}, {"sha": "0fe42edbc6103d83d5db5801cf8c80a2d792b9f5", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 316, "deletions": 84, "changes": 400, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -56,20 +56,20 @@\n ;; Iterator for all scalar floating point modes (SF, DF and TF)\n (define_mode_iterator GPF_TF [SF DF TF])\n \n-;; Integer vector modes.\n+;; Integer Advanced SIMD modes.\n (define_mode_iterator VDQ_I [V8QI V16QI V4HI V8HI V2SI V4SI V2DI])\n \n-;; vector and scalar, 64 & 128-bit container, all integer modes\n+;; Advanced SIMD and scalar, 64 & 128-bit container, all integer modes.\n (define_mode_iterator VSDQ_I [V8QI V16QI V4HI V8HI V2SI V4SI V2DI QI HI SI DI])\n \n-;; vector and scalar, 64 & 128-bit container: all vector integer modes;\n-;; 64-bit scalar integer mode\n+;; Advanced SIMD and scalar, 64 & 128-bit container: all Advanced SIMD\n+;; integer modes; 64-bit scalar integer mode.\n (define_mode_iterator VSDQ_I_DI [V8QI V16QI V4HI V8HI V2SI V4SI V2DI DI])\n \n ;; Double vector modes.\n (define_mode_iterator VD [V8QI V4HI V4HF V2SI V2SF])\n \n-;; vector, 64-bit container, all integer modes\n+;; Advanced SIMD, 64-bit container, all integer modes.\n (define_mode_iterator VD_BHSI [V8QI V4HI V2SI])\n \n ;; 128 and 64-bit container; 8, 16, 32-bit vector integer modes\n@@ -94,16 +94,16 @@\n ;; pointer-sized quantities.  Exactly one of the two alternatives will match.\n (define_mode_iterator PTR [(SI \"ptr_mode == SImode\") (DI \"ptr_mode == DImode\")])\n \n-;; Vector Float modes suitable for moving, loading and storing.\n+;; Advanced SIMD Float modes suitable for moving, loading and storing.\n (define_mode_iterator VDQF_F16 [V4HF V8HF V2SF V4SF V2DF])\n \n-;; Vector Float modes.\n+;; Advanced SIMD Float modes.\n (define_mode_iterator VDQF [V2SF V4SF V2DF])\n (define_mode_iterator VHSDF [(V4HF \"TARGET_SIMD_F16INST\")\n \t\t\t     (V8HF \"TARGET_SIMD_F16INST\")\n \t\t\t     V2SF V4SF V2DF])\n \n-;; Vector Float modes, and DF.\n+;; Advanced SIMD Float modes, and DF.\n (define_mode_iterator VHSDF_DF [(V4HF \"TARGET_SIMD_F16INST\")\n \t\t\t\t(V8HF \"TARGET_SIMD_F16INST\")\n \t\t\t\tV2SF V4SF V2DF DF])\n@@ -113,7 +113,7 @@\n \t\t\t\t  (HF \"TARGET_SIMD_F16INST\")\n \t\t\t\t  SF DF])\n \n-;; Vector single Float modes.\n+;; Advanced SIMD single Float modes.\n (define_mode_iterator VDQSF [V2SF V4SF])\n \n ;; Quad vector Float modes with half/single elements.\n@@ -122,38 +122,38 @@\n ;; Modes suitable to use as the return type of a vcond expression.\n (define_mode_iterator VDQF_COND [V2SF V2SI V4SF V4SI V2DF V2DI])\n \n-;; All Float modes.\n+;; All scalar and Advanced SIMD Float modes.\n (define_mode_iterator VALLF [V2SF V4SF V2DF SF DF])\n \n-;; Vector Float modes with 2 elements.\n+;; Advanced SIMD Float modes with 2 elements.\n (define_mode_iterator V2F [V2SF V2DF])\n \n-;; All vector modes on which we support any arithmetic operations.\n+;; All Advanced SIMD modes on which we support any arithmetic operations.\n (define_mode_iterator VALL [V8QI V16QI V4HI V8HI V2SI V4SI V2DI V2SF V4SF V2DF])\n \n-;; All vector modes suitable for moving, loading, and storing.\n+;; All Advanced SIMD modes suitable for moving, loading, and storing.\n (define_mode_iterator VALL_F16 [V8QI V16QI V4HI V8HI V2SI V4SI V2DI\n \t\t\t\tV4HF V8HF V2SF V4SF V2DF])\n \n ;; The VALL_F16 modes except the 128-bit 2-element ones.\n (define_mode_iterator VALL_F16_NO_V2Q [V8QI V16QI V4HI V8HI V2SI V4SI\n \t\t\t\tV4HF V8HF V2SF V4SF])\n \n-;; All vector modes barring HF modes, plus DI.\n+;; All Advanced SIMD modes barring HF modes, plus DI.\n (define_mode_iterator VALLDI [V8QI V16QI V4HI V8HI V2SI V4SI V2DI V2SF V4SF V2DF DI])\n \n-;; All vector modes and DI.\n+;; All Advanced SIMD modes and DI.\n (define_mode_iterator VALLDI_F16 [V8QI V16QI V4HI V8HI V2SI V4SI V2DI\n \t\t\t\t  V4HF V8HF V2SF V4SF V2DF DI])\n \n-;; All vector modes, plus DI and DF.\n+;; All Advanced SIMD modes, plus DI and DF.\n (define_mode_iterator VALLDIF [V8QI V16QI V4HI V8HI V2SI V4SI\n \t\t\t       V2DI V4HF V8HF V2SF V4SF V2DF DI DF])\n \n-;; Vector modes for Integer reduction across lanes.\n+;; Advanced SIMD modes for Integer reduction across lanes.\n (define_mode_iterator VDQV [V8QI V16QI V4HI V8HI V4SI V2DI])\n \n-;; Vector modes(except V2DI) for Integer reduction across lanes.\n+;; Advanced SIMD modes (except V2DI) for Integer reduction across lanes.\n (define_mode_iterator VDQV_S [V8QI V16QI V4HI V8HI V4SI])\n \n ;; All double integer narrow-able modes.\n@@ -162,7 +162,8 @@\n ;; All quad integer narrow-able modes.\n (define_mode_iterator VQN [V8HI V4SI V2DI])\n \n-;; Vector and scalar 128-bit container: narrowable 16, 32, 64-bit integer modes\n+;; Advanced SIMD and scalar 128-bit container: narrowable 16, 32, 64-bit\n+;; integer modes\n (define_mode_iterator VSQN_HSDI [V8HI V4SI V2DI HI SI DI])\n \n ;; All quad integer widen-able modes.\n@@ -171,54 +172,54 @@\n ;; Double vector modes for combines.\n (define_mode_iterator VDC [V8QI V4HI V4HF V2SI V2SF DI DF])\n \n-;; Vector modes except double int.\n+;; Advanced SIMD modes except double int.\n (define_mode_iterator VDQIF [V8QI V16QI V4HI V8HI V2SI V4SI V2SF V4SF V2DF])\n (define_mode_iterator VDQIF_F16 [V8QI V16QI V4HI V8HI V2SI V4SI\n                                  V4HF V8HF V2SF V4SF V2DF])\n \n-;; Vector modes for S type.\n+;; Advanced SIMD modes for S type.\n (define_mode_iterator VDQ_SI [V2SI V4SI])\n \n-;; Vector modes for S and D\n+;; Advanced SIMD modes for S and D.\n (define_mode_iterator VDQ_SDI [V2SI V4SI V2DI])\n \n-;; Vector modes for H, S and D\n+;; Advanced SIMD modes for H, S and D.\n (define_mode_iterator VDQ_HSDI [(V4HI \"TARGET_SIMD_F16INST\")\n \t\t\t\t(V8HI \"TARGET_SIMD_F16INST\")\n \t\t\t\tV2SI V4SI V2DI])\n \n-;; Scalar and Vector modes for S and D\n+;; Scalar and Advanced SIMD modes for S and D.\n (define_mode_iterator VSDQ_SDI [V2SI V4SI V2DI SI DI])\n \n-;; Scalar and Vector modes for S and D, Vector modes for H.\n+;; Scalar and Advanced SIMD modes for S and D, Advanced SIMD modes for H.\n (define_mode_iterator VSDQ_HSDI [(V4HI \"TARGET_SIMD_F16INST\")\n \t\t\t\t (V8HI \"TARGET_SIMD_F16INST\")\n \t\t\t\t V2SI V4SI V2DI\n \t\t\t\t (HI \"TARGET_SIMD_F16INST\")\n \t\t\t\t SI DI])\n \n-;; Vector modes for Q and H types.\n+;; Advanced SIMD modes for Q and H types.\n (define_mode_iterator VDQQH [V8QI V16QI V4HI V8HI])\n \n-;; Vector modes for H and S types.\n+;; Advanced SIMD modes for H and S types.\n (define_mode_iterator VDQHS [V4HI V8HI V2SI V4SI])\n \n-;; Vector modes for H, S and D types.\n+;; Advanced SIMD modes for H, S and D types.\n (define_mode_iterator VDQHSD [V4HI V8HI V2SI V4SI V2DI])\n \n-;; Vector and scalar integer modes for H and S\n+;; Advanced SIMD and scalar integer modes for H and S.\n (define_mode_iterator VSDQ_HSI [V4HI V8HI V2SI V4SI HI SI])\n \n-;; Vector and scalar 64-bit container: 16, 32-bit integer modes\n+;; Advanced SIMD and scalar 64-bit container: 16, 32-bit integer modes.\n (define_mode_iterator VSD_HSI [V4HI V2SI HI SI])\n \n-;; Vector 64-bit container: 16, 32-bit integer modes\n+;; Advanced SIMD 64-bit container: 16, 32-bit integer modes.\n (define_mode_iterator VD_HSI [V4HI V2SI])\n \n ;; Scalar 64-bit container: 16, 32-bit integer modes\n (define_mode_iterator SD_HSI [HI SI])\n \n-;; Vector 64-bit container: 16, 32-bit integer modes\n+;; Advanced SIMD 64-bit container: 16, 32-bit integer modes.\n (define_mode_iterator VQ_HSI [V8HI V4SI])\n \n ;; All byte modes.\n@@ -229,21 +230,59 @@\n \n (define_mode_iterator TX [TI TF])\n \n-;; Opaque structure modes.\n+;; Advanced SIMD opaque structure modes.\n (define_mode_iterator VSTRUCT [OI CI XI])\n \n ;; Double scalar modes\n (define_mode_iterator DX [DI DF])\n \n-;; Modes available for <f>mul lane operations.\n+;; Modes available for Advanced SIMD <f>mul lane operations.\n (define_mode_iterator VMUL [V4HI V8HI V2SI V4SI\n \t\t\t    (V4HF \"TARGET_SIMD_F16INST\")\n \t\t\t    (V8HF \"TARGET_SIMD_F16INST\")\n \t\t\t    V2SF V4SF V2DF])\n \n-;; Modes available for <f>mul lane operations changing lane count.\n+;; Modes available for Advanced SIMD <f>mul lane operations changing lane\n+;; count.\n (define_mode_iterator VMUL_CHANGE_NLANES [V4HI V8HI V2SI V4SI V2SF V4SF])\n \n+;; All SVE vector modes.\n+(define_mode_iterator SVE_ALL [VNx16QI VNx8HI VNx4SI VNx2DI\n+\t\t\t       VNx8HF VNx4SF VNx2DF])\n+\n+;; All SVE vector modes that have 8-bit or 16-bit elements.\n+(define_mode_iterator SVE_BH [VNx16QI VNx8HI VNx8HF])\n+\n+;; All SVE vector modes that have 8-bit, 16-bit or 32-bit elements.\n+(define_mode_iterator SVE_BHS [VNx16QI VNx8HI VNx4SI VNx8HF VNx4SF])\n+\n+;; All SVE integer vector modes that have 8-bit, 16-bit or 32-bit elements.\n+(define_mode_iterator SVE_BHSI [VNx16QI VNx8HI VNx4SI])\n+\n+;; All SVE integer vector modes that have 16-bit, 32-bit or 64-bit elements.\n+(define_mode_iterator SVE_HSDI [VNx16QI VNx8HI VNx4SI])\n+\n+;; All SVE floating-point vector modes that have 16-bit or 32-bit elements.\n+(define_mode_iterator SVE_HSF [VNx8HF VNx4SF])\n+\n+;; All SVE vector modes that have 32-bit or 64-bit elements.\n+(define_mode_iterator SVE_SD [VNx4SI VNx2DI VNx4SF VNx2DF])\n+\n+;; All SVE integer vector modes that have 32-bit or 64-bit elements.\n+(define_mode_iterator SVE_SDI [VNx4SI VNx2DI])\n+\n+;; All SVE integer vector modes.\n+(define_mode_iterator SVE_I [VNx16QI VNx8HI VNx4SI VNx2DI])\n+\n+;; All SVE floating-point vector modes.\n+(define_mode_iterator SVE_F [VNx8HF VNx4SF VNx2DF])\n+\n+;; All SVE predicate modes.\n+(define_mode_iterator PRED_ALL [VNx16BI VNx8BI VNx4BI VNx2BI])\n+\n+;; SVE predicate modes that control 8-bit, 16-bit or 32-bit elements.\n+(define_mode_iterator PRED_BHS [VNx16BI VNx8BI VNx4BI])\n+\n ;; ------------------------------------------------------------------\n ;; Unspec enumerations for Advance SIMD. These could well go into\n ;; aarch64.md but for their use in int_iterators here.\n@@ -378,6 +417,22 @@\n     UNSPEC_FMLSL\t; Used in aarch64-simd.md.\n     UNSPEC_FMLAL2\t; Used in aarch64-simd.md.\n     UNSPEC_FMLSL2\t; Used in aarch64-simd.md.\n+    UNSPEC_SEL\t\t; Used in aarch64-sve.md.\n+    UNSPEC_ANDF\t\t; Used in aarch64-sve.md.\n+    UNSPEC_IORF\t\t; Used in aarch64-sve.md.\n+    UNSPEC_XORF\t\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_LT\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_LE\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_EQ\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_NE\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_GE\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_GT\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_LO\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_LS\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_HS\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_HI\t; Used in aarch64-sve.md.\n+    UNSPEC_COND_UO\t; Used in aarch64-sve.md.\n+    UNSPEC_LASTB\t; Used in aarch64-sve.md.\n ])\n \n ;; ------------------------------------------------------------------\n@@ -535,17 +590,24 @@\n \t\t\t   (HI   \"\")])\n \n ;; Mode-to-individual element type mapping.\n-(define_mode_attr Vetype [(V8QI \"b\") (V16QI \"b\")\n-\t\t\t  (V4HI \"h\") (V8HI  \"h\")\n-                          (V2SI \"s\") (V4SI  \"s\")\n-\t\t\t  (V2DI \"d\") (V4HF \"h\")\n-\t\t\t  (V8HF \"h\") (V2SF  \"s\")\n-\t\t\t  (V4SF \"s\") (V2DF  \"d\")\n+(define_mode_attr Vetype [(V8QI \"b\") (V16QI \"b\") (VNx16QI \"b\") (VNx16BI \"b\")\n+\t\t\t  (V4HI \"h\") (V8HI  \"h\") (VNx8HI  \"h\") (VNx8BI  \"h\")\n+\t\t\t  (V2SI \"s\") (V4SI  \"s\") (VNx4SI  \"s\") (VNx4BI  \"s\")\n+\t\t\t  (V2DI \"d\")             (VNx2DI  \"d\") (VNx2BI  \"d\")\n+\t\t\t  (V4HF \"h\") (V8HF  \"h\") (VNx8HF  \"h\")\n+\t\t\t  (V2SF \"s\") (V4SF  \"s\") (VNx4SF  \"s\")\n+\t\t\t  (V2DF \"d\")             (VNx2DF  \"d\")\n \t\t\t  (HF   \"h\")\n \t\t\t  (SF   \"s\") (DF  \"d\")\n \t\t\t  (QI \"b\")   (HI \"h\")\n \t\t\t  (SI \"s\")   (DI \"d\")])\n \n+;; Equivalent of \"size\" for a vector element.\n+(define_mode_attr Vesize [(VNx16QI \"b\")\n+\t\t\t  (VNx8HI  \"h\") (VNx8HF \"h\")\n+\t\t\t  (VNx4SI  \"w\") (VNx4SF \"w\")\n+\t\t\t  (VNx2DI  \"d\") (VNx2DF \"d\")])\n+\n ;; Vetype is used everywhere in scheduling type and assembly output,\n ;; sometimes they are not the same, for example HF modes on some\n ;; instructions.  stype is defined to represent scheduling type\n@@ -567,27 +629,45 @@\n \t\t\t  (SI   \"8b\")])\n \n ;; Define element mode for each vector mode.\n-(define_mode_attr VEL [(V8QI \"QI\") (V16QI \"QI\")\n-\t\t\t(V4HI \"HI\") (V8HI \"HI\")\n-                        (V2SI \"SI\") (V4SI \"SI\")\n-                        (DI \"DI\")   (V2DI \"DI\")\n-                        (V4HF \"HF\") (V8HF \"HF\")\n-                        (V2SF \"SF\") (V4SF \"SF\")\n-                        (V2DF \"DF\") (DF \"DF\")\n-\t\t\t(SI   \"SI\") (HI   \"HI\")\n+(define_mode_attr VEL [(V8QI  \"QI\") (V16QI \"QI\") (VNx16QI \"QI\")\n+\t\t\t(V4HI \"HI\") (V8HI  \"HI\") (VNx8HI  \"HI\")\n+\t\t\t(V2SI \"SI\") (V4SI  \"SI\") (VNx4SI  \"SI\")\n+\t\t\t(DI   \"DI\") (V2DI  \"DI\") (VNx2DI  \"DI\")\n+\t\t\t(V4HF \"HF\") (V8HF  \"HF\") (VNx8HF  \"HF\")\n+\t\t\t(V2SF \"SF\") (V4SF  \"SF\") (VNx4SF  \"SF\")\n+\t\t\t(DF   \"DF\") (V2DF  \"DF\") (VNx2DF  \"DF\")\n+\t\t\t(SI   \"SI\") (HI    \"HI\")\n \t\t\t(QI   \"QI\")])\n \n ;; Define element mode for each vector mode (lower case).\n-(define_mode_attr Vel [(V8QI \"qi\") (V16QI \"qi\")\n-\t\t\t(V4HI \"hi\") (V8HI \"hi\")\n-\t\t\t(V2SI \"si\") (V4SI \"si\")\n-\t\t\t(DI \"di\")   (V2DI \"di\")\n-\t\t\t(V4HF \"hf\") (V8HF \"hf\")\n-\t\t\t(V2SF \"sf\") (V4SF \"sf\")\n-\t\t\t(V2DF \"df\") (DF \"df\")\n+(define_mode_attr Vel [(V8QI \"qi\") (V16QI \"qi\") (VNx16QI \"qi\")\n+\t\t\t(V4HI \"hi\") (V8HI \"hi\") (VNx8HI  \"hi\")\n+\t\t\t(V2SI \"si\") (V4SI \"si\") (VNx4SI  \"si\")\n+\t\t\t(DI \"di\")   (V2DI \"di\") (VNx2DI  \"di\")\n+\t\t\t(V4HF \"hf\") (V8HF \"hf\") (VNx8HF  \"hf\")\n+\t\t\t(V2SF \"sf\") (V4SF \"sf\") (VNx4SF  \"sf\")\n+\t\t\t(V2DF \"df\") (DF \"df\")   (VNx2DF  \"df\")\n \t\t\t(SI   \"si\") (HI   \"hi\")\n \t\t\t(QI   \"qi\")])\n \n+;; Element mode with floating-point values replaced by like-sized integers.\n+(define_mode_attr VEL_INT [(VNx16QI \"QI\")\n+\t\t\t   (VNx8HI  \"HI\") (VNx8HF \"HI\")\n+\t\t\t   (VNx4SI  \"SI\") (VNx4SF \"SI\")\n+\t\t\t   (VNx2DI  \"DI\") (VNx2DF \"DI\")])\n+\n+;; Gives the mode of the 128-bit lowpart of an SVE vector.\n+(define_mode_attr V128 [(VNx16QI \"V16QI\")\n+\t\t\t(VNx8HI  \"V8HI\") (VNx8HF \"V8HF\")\n+\t\t\t(VNx4SI  \"V4SI\") (VNx4SF \"V4SF\")\n+\t\t\t(VNx2DI  \"V2DI\") (VNx2DF \"V2DF\")])\n+\n+;; ...and again in lower case.\n+(define_mode_attr v128 [(VNx16QI \"v16qi\")\n+\t\t\t(VNx8HI  \"v8hi\") (VNx8HF \"v8hf\")\n+\t\t\t(VNx4SI  \"v4si\") (VNx4SF \"v4sf\")\n+\t\t\t(VNx2DI  \"v2di\") (VNx2DF \"v2df\")])\n+\n ;; 64-bit container modes the inner or scalar source mode.\n (define_mode_attr VCOND [(HI \"V4HI\") (SI \"V2SI\")\n \t\t\t (V4HI \"V4HI\") (V8HI \"V4HI\")\n@@ -666,23 +746,40 @@\n \t\t\t   (V2DI \"4s\")])\n \n ;; Widened modes of vector modes.\n-(define_mode_attr VWIDE [(V8QI \"V8HI\") (V4HI \"V4SI\")\n-\t\t\t (V2SI \"V2DI\") (V16QI \"V8HI\") \n-\t\t\t (V8HI \"V4SI\") (V4SI \"V2DI\")\n-\t\t\t (HI \"SI\")     (SI \"DI\")\n-\t\t\t (V8HF \"V4SF\") (V4SF \"V2DF\")\n-\t\t\t (V4HF \"V4SF\") (V2SF \"V2DF\")]\n-)\n+(define_mode_attr VWIDE [(V8QI  \"V8HI\")  (V4HI  \"V4SI\")\n+\t\t\t (V2SI  \"V2DI\")  (V16QI \"V8HI\")\n+\t\t\t (V8HI  \"V4SI\")  (V4SI  \"V2DI\")\n+\t\t\t (HI    \"SI\")    (SI    \"DI\")\n+\t\t\t (V8HF  \"V4SF\")  (V4SF  \"V2DF\")\n+\t\t\t (V4HF  \"V4SF\")  (V2SF  \"V2DF\")\n+\t\t\t (VNx8HF  \"VNx4SF\") (VNx4SF \"VNx2DF\")\n+\t\t\t (VNx16QI \"VNx8HI\") (VNx8HI \"VNx4SI\")\n+\t\t\t (VNx4SI  \"VNx2DI\")\n+\t\t\t (VNx16BI \"VNx8BI\") (VNx8BI \"VNx4BI\")\n+\t\t\t (VNx4BI  \"VNx2BI\")])\n+\n+;; Predicate mode associated with VWIDE.\n+(define_mode_attr VWIDE_PRED [(VNx8HF \"VNx4BI\") (VNx4SF \"VNx2BI\")])\n \n ;; Widened modes of vector modes, lowercase\n-(define_mode_attr Vwide [(V2SF \"v2df\") (V4HF \"v4sf\")])\n+(define_mode_attr Vwide [(V2SF \"v2df\") (V4HF \"v4sf\")\n+\t\t\t (VNx16QI \"vnx8hi\") (VNx8HI \"vnx4si\")\n+\t\t\t (VNx4SI  \"vnx2di\")\n+\t\t\t (VNx8HF  \"vnx4sf\") (VNx4SF \"vnx2df\")\n+\t\t\t (VNx16BI \"vnx8bi\") (VNx8BI \"vnx4bi\")\n+\t\t\t (VNx4BI  \"vnx2bi\")])\n \n ;; Widened mode register suffixes for VD_BHSI/VQW/VQ_HSF.\n (define_mode_attr Vwtype [(V8QI \"8h\") (V4HI \"4s\")\n \t\t\t  (V2SI \"2d\") (V16QI \"8h\") \n \t\t\t  (V8HI \"4s\") (V4SI \"2d\")\n \t\t\t  (V8HF \"4s\") (V4SF \"2d\")])\n \n+;; SVE vector after widening\n+(define_mode_attr Vewtype [(VNx16QI \"h\")\n+\t\t\t   (VNx8HI  \"s\") (VNx8HF \"s\")\n+\t\t\t   (VNx4SI  \"d\") (VNx4SF \"d\")])\n+\n ;; Widened mode register suffixes for VDW/VQW.\n (define_mode_attr Vmwtype [(V8QI \".8h\") (V4HI \".4s\")\n \t\t\t   (V2SI \".2d\") (V16QI \".8h\") \n@@ -696,22 +793,23 @@\n \t\t\t     (V4SF \"2s\")])\n \n ;; Define corresponding core/FP element mode for each vector mode.\n-(define_mode_attr vw   [(V8QI \"w\") (V16QI \"w\")\n-                        (V4HI \"w\") (V8HI \"w\")\n-                        (V2SI \"w\") (V4SI \"w\")\n-                        (DI   \"x\") (V2DI \"x\")\n-                        (V2SF \"s\") (V4SF \"s\")\n-                        (V2DF \"d\")])\n+(define_mode_attr vw [(V8QI \"w\") (V16QI \"w\") (VNx16QI \"w\")\n+\t\t      (V4HI \"w\") (V8HI \"w\") (VNx8HI \"w\")\n+\t\t      (V2SI \"w\") (V4SI \"w\") (VNx4SI \"w\")\n+\t\t      (DI   \"x\") (V2DI \"x\") (VNx2DI \"x\")\n+\t\t      (VNx8HF \"h\")\n+\t\t      (V2SF \"s\") (V4SF \"s\") (VNx4SF \"s\")\n+\t\t      (V2DF \"d\") (VNx2DF \"d\")])\n \n ;; Corresponding core element mode for each vector mode.  This is a\n ;; variation on <vw> mapping FP modes to GP regs.\n-(define_mode_attr vwcore  [(V8QI \"w\") (V16QI \"w\")\n-\t\t\t   (V4HI \"w\") (V8HI \"w\")\n-\t\t\t   (V2SI \"w\") (V4SI \"w\")\n-\t\t\t   (DI   \"x\") (V2DI \"x\")\n-\t\t\t   (V4HF \"w\") (V8HF \"w\")\n-\t\t\t   (V2SF \"w\") (V4SF \"w\")\n-\t\t\t   (V2DF \"x\")])\n+(define_mode_attr vwcore [(V8QI \"w\") (V16QI \"w\") (VNx16QI \"w\")\n+\t\t\t  (V4HI \"w\") (V8HI \"w\") (VNx8HI \"w\")\n+\t\t\t  (V2SI \"w\") (V4SI \"w\") (VNx4SI \"w\")\n+\t\t\t  (DI   \"x\") (V2DI \"x\") (VNx2DI \"x\")\n+\t\t\t  (V4HF \"w\") (V8HF \"w\") (VNx8HF \"w\")\n+\t\t\t  (V2SF \"w\") (V4SF \"w\") (VNx4SF \"w\")\n+\t\t\t  (V2DF \"x\") (VNx2DF \"x\")])\n \n ;; Double vector types for ALLX.\n (define_mode_attr Vallxd [(QI \"8b\") (HI \"4h\") (SI \"2s\")])\n@@ -723,8 +821,13 @@\n \t\t\t       (DI   \"DI\")   (V2DI  \"V2DI\")\n \t\t\t       (V4HF \"V4HI\") (V8HF  \"V8HI\")\n \t\t\t       (V2SF \"V2SI\") (V4SF  \"V4SI\")\n-\t\t\t       (V2DF \"V2DI\") (DF    \"DI\")\n-\t\t\t       (SF   \"SI\")   (HF    \"HI\")])\n+\t\t\t       (DF   \"DI\")   (V2DF  \"V2DI\")\n+\t\t\t       (SF   \"SI\")   (HF    \"HI\")\n+\t\t\t       (VNx16QI \"VNx16QI\")\n+\t\t\t       (VNx8HI  \"VNx8HI\") (VNx8HF \"VNx8HI\")\n+\t\t\t       (VNx4SI  \"VNx4SI\") (VNx4SF \"VNx4SI\")\n+\t\t\t       (VNx2DI  \"VNx2DI\") (VNx2DF \"VNx2DI\")\n+])\n \n ;; Lower case mode with floating-point values replaced by like-sized integers.\n (define_mode_attr v_int_equiv [(V8QI \"v8qi\") (V16QI \"v16qi\")\n@@ -733,8 +836,19 @@\n \t\t\t       (DI   \"di\")   (V2DI  \"v2di\")\n \t\t\t       (V4HF \"v4hi\") (V8HF  \"v8hi\")\n \t\t\t       (V2SF \"v2si\") (V4SF  \"v4si\")\n-\t\t\t       (V2DF \"v2di\") (DF    \"di\")\n-\t\t\t       (SF   \"si\")])\n+\t\t\t       (DF   \"di\")   (V2DF  \"v2di\")\n+\t\t\t       (SF   \"si\")\n+\t\t\t       (VNx16QI \"vnx16qi\")\n+\t\t\t       (VNx8HI  \"vnx8hi\") (VNx8HF \"vnx8hi\")\n+\t\t\t       (VNx4SI  \"vnx4si\") (VNx4SF \"vnx4si\")\n+\t\t\t       (VNx2DI  \"vnx2di\") (VNx2DF \"vnx2di\")\n+])\n+\n+;; Floating-point equivalent of selected modes.\n+(define_mode_attr V_FP_EQUIV [(VNx4SI \"VNx4SF\") (VNx4SF \"VNx4SF\")\n+\t\t\t      (VNx2DI \"VNx2DF\") (VNx2DF \"VNx2DF\")])\n+(define_mode_attr v_fp_equiv [(VNx4SI \"vnx4sf\") (VNx4SF \"vnx4sf\")\n+\t\t\t      (VNx2DI \"vnx2df\") (VNx2DF \"vnx2df\")])\n \n ;; Mode for vector conditional operations where the comparison has\n ;; different type from the lhs.\n@@ -869,6 +983,18 @@\n \n (define_code_attr f16mac [(plus \"a\") (minus \"s\")])\n \n+;; The predicate mode associated with an SVE data mode.\n+(define_mode_attr VPRED [(VNx16QI \"VNx16BI\")\n+\t\t\t (VNx8HI \"VNx8BI\") (VNx8HF \"VNx8BI\")\n+\t\t\t (VNx4SI \"VNx4BI\") (VNx4SF \"VNx4BI\")\n+\t\t\t (VNx2DI \"VNx2BI\") (VNx2DF \"VNx2BI\")])\n+\n+;; ...and again in lower case.\n+(define_mode_attr vpred [(VNx16QI \"vnx16bi\")\n+\t\t\t (VNx8HI \"vnx8bi\") (VNx8HF \"vnx8bi\")\n+\t\t\t (VNx4SI \"vnx4bi\") (VNx4SF \"vnx4bi\")\n+\t\t\t (VNx2DI \"vnx2bi\") (VNx2DF \"vnx2bi\")])\n+\n ;; -------------------------------------------------------------------\n ;; Code Iterators\n ;; -------------------------------------------------------------------\n@@ -882,6 +1008,9 @@\n ;; Code iterator for logical operations\n (define_code_iterator LOGICAL [and ior xor])\n \n+;; LOGICAL without AND.\n+(define_code_iterator LOGICAL_OR [ior xor])\n+\n ;; Code iterator for logical operations whose :nlogical works on SIMD registers.\n (define_code_iterator NLOGICAL [and ior])\n \n@@ -940,6 +1069,12 @@\n ;; Unsigned comparison operators.\n (define_code_iterator FAC_COMPARISONS [lt le ge gt])\n \n+;; SVE integer unary operations.\n+(define_code_iterator SVE_INT_UNARY [neg not popcount])\n+\n+;; SVE floating-point unary operations.\n+(define_code_iterator SVE_FP_UNARY [neg abs sqrt])\n+\n ;; -------------------------------------------------------------------\n ;; Code Attributes\n ;; -------------------------------------------------------------------\n@@ -956,6 +1091,7 @@\n \t\t\t (unsigned_fix \"fixuns\")\n \t\t\t (float \"float\")\n \t\t\t (unsigned_float \"floatuns\")\n+\t\t\t (popcount \"popcount\")\n \t\t\t (and \"and\")\n \t\t\t (ior \"ior\")\n \t\t\t (xor \"xor\")\n@@ -969,6 +1105,10 @@\n \t\t\t (us_minus \"qsub\")\n \t\t\t (ss_neg \"qneg\")\n \t\t\t (ss_abs \"qabs\")\n+\t\t\t (smin \"smin\")\n+\t\t\t (smax \"smax\")\n+\t\t\t (umin \"umin\")\n+\t\t\t (umax \"umax\")\n \t\t\t (eq \"eq\")\n \t\t\t (ne \"ne\")\n \t\t\t (lt \"lt\")\n@@ -978,7 +1118,9 @@\n \t\t\t (ltu \"ltu\")\n \t\t\t (leu \"leu\")\n \t\t\t (geu \"geu\")\n-\t\t\t (gtu \"gtu\")])\n+\t\t\t (gtu \"gtu\")\n+\t\t\t (abs \"abs\")\n+\t\t\t (sqrt \"sqrt\")])\n \n ;; For comparison operators we use the FCM* and CM* instructions.\n ;; As there are no CMLE or CMLT instructions which act on 3 vector\n@@ -1021,9 +1163,12 @@\n ;; Operation names for negate and bitwise complement.\n (define_code_attr neg_not_op [(neg \"neg\") (not \"not\")])\n \n-;; Similar, but when not(op)\n+;; Similar, but when the second operand is inverted.\n (define_code_attr nlogical [(and \"bic\") (ior \"orn\") (xor \"eon\")])\n \n+;; Similar, but when both operands are inverted.\n+(define_code_attr logical_nn [(and \"nor\") (ior \"nand\")])\n+\n ;; Sign- or zero-extending data-op\n (define_code_attr su [(sign_extend \"s\") (zero_extend \"u\")\n \t\t      (sign_extract \"s\") (zero_extract \"u\")\n@@ -1032,6 +1177,9 @@\n \t\t      (smax \"s\") (umax \"u\")\n \t\t      (smin \"s\") (umin \"u\")])\n \n+;; Whether a shift is left or right.\n+(define_code_attr lr [(ashift \"l\") (ashiftrt \"r\") (lshiftrt \"r\")])\n+\n ;; Emit conditional branch instructions.\n (define_code_attr bcond [(eq \"beq\") (ne \"bne\") (lt \"bne\") (ge \"beq\")])\n \n@@ -1077,6 +1225,25 @@\n ;; Attribute to describe constants acceptable in atomic logical operations\n (define_mode_attr lconst_atomic [(QI \"K\") (HI \"K\") (SI \"K\") (DI \"L\")])\n \n+;; The integer SVE instruction that implements an rtx code.\n+(define_code_attr sve_int_op [(plus \"add\")\n+\t\t\t      (neg \"neg\")\n+\t\t\t      (smin \"smin\")\n+\t\t\t      (smax \"smax\")\n+\t\t\t      (umin \"umin\")\n+\t\t\t      (umax \"umax\")\n+\t\t\t      (and \"and\")\n+\t\t\t      (ior \"orr\")\n+\t\t\t      (xor \"eor\")\n+\t\t\t      (not \"not\")\n+\t\t\t      (popcount \"cnt\")])\n+\n+;; The floating-point SVE instruction that implements an rtx code.\n+(define_code_attr sve_fp_op [(plus \"fadd\")\n+\t\t\t     (neg \"fneg\")\n+\t\t\t     (abs \"fabs\")\n+\t\t\t     (sqrt \"fsqrt\")])\n+\n ;; -------------------------------------------------------------------\n ;; Int Iterators.\n ;; -------------------------------------------------------------------\n@@ -1086,6 +1253,8 @@\n (define_int_iterator FMAXMINV [UNSPEC_FMAXV UNSPEC_FMINV\n \t\t\t       UNSPEC_FMAXNMV UNSPEC_FMINNMV])\n \n+(define_int_iterator LOGICALF [UNSPEC_ANDF UNSPEC_IORF UNSPEC_XORF])\n+\n (define_int_iterator HADDSUB [UNSPEC_SHADD UNSPEC_UHADD\n \t\t\t      UNSPEC_SRHADD UNSPEC_URHADD\n \t\t\t      UNSPEC_SHSUB UNSPEC_UHSUB\n@@ -1141,6 +1310,9 @@\n \t\t\t      UNSPEC_TRN1 UNSPEC_TRN2\n \t\t\t      UNSPEC_UZP1 UNSPEC_UZP2])\n \n+(define_int_iterator OPTAB_PERMUTE [UNSPEC_ZIP1 UNSPEC_ZIP2\n+\t\t\t\t    UNSPEC_UZP1 UNSPEC_UZP2])\n+\n (define_int_iterator REVERSE [UNSPEC_REV64 UNSPEC_REV32 UNSPEC_REV16])\n \n (define_int_iterator FRINT [UNSPEC_FRINTZ UNSPEC_FRINTP UNSPEC_FRINTM\n@@ -1179,6 +1351,21 @@\n \n (define_int_iterator VFMLA16_HIGH [UNSPEC_FMLAL2 UNSPEC_FMLSL2])\n \n+(define_int_iterator UNPACK [UNSPEC_UNPACKSHI UNSPEC_UNPACKUHI\n+\t\t\t     UNSPEC_UNPACKSLO UNSPEC_UNPACKULO])\n+\n+(define_int_iterator UNPACK_UNSIGNED [UNSPEC_UNPACKULO UNSPEC_UNPACKUHI])\n+\n+(define_int_iterator SVE_COND_INT_CMP [UNSPEC_COND_LT UNSPEC_COND_LE\n+\t\t\t\t       UNSPEC_COND_EQ UNSPEC_COND_NE\n+\t\t\t\t       UNSPEC_COND_GE UNSPEC_COND_GT\n+\t\t\t\t       UNSPEC_COND_LO UNSPEC_COND_LS\n+\t\t\t\t       UNSPEC_COND_HS UNSPEC_COND_HI])\n+\n+(define_int_iterator SVE_COND_FP_CMP [UNSPEC_COND_LT UNSPEC_COND_LE\n+\t\t\t\t      UNSPEC_COND_EQ UNSPEC_COND_NE\n+\t\t\t\t      UNSPEC_COND_GE UNSPEC_COND_GT])\n+\n ;; Iterators for atomic operations.\n \n (define_int_iterator ATOMIC_LDOP\n@@ -1192,6 +1379,14 @@\n ;; -------------------------------------------------------------------\n ;; Int Iterators Attributes.\n ;; -------------------------------------------------------------------\n+\n+;; The optab associated with an operation.  Note that for ANDF, IORF\n+;; and XORF, the optab pattern is not actually defined; we just use this\n+;; name for consistency with the integer patterns.\n+(define_int_attr optab [(UNSPEC_ANDF \"and\")\n+\t\t\t(UNSPEC_IORF \"ior\")\n+\t\t\t(UNSPEC_XORF \"xor\")])\n+\n (define_int_attr  maxmin_uns [(UNSPEC_UMAXV \"umax\")\n \t\t\t      (UNSPEC_UMINV \"umin\")\n \t\t\t      (UNSPEC_SMAXV \"smax\")\n@@ -1218,6 +1413,17 @@\n \t\t\t\t (UNSPEC_FMAXNM \"fmaxnm\")\n \t\t\t\t (UNSPEC_FMINNM \"fminnm\")])\n \n+;; The SVE logical instruction that implements an unspec.\n+(define_int_attr logicalf_op [(UNSPEC_ANDF \"and\")\n+\t\t \t      (UNSPEC_IORF \"orr\")\n+\t\t\t      (UNSPEC_XORF \"eor\")])\n+\n+;; \"s\" for signed operations and \"u\" for unsigned ones.\n+(define_int_attr su [(UNSPEC_UNPACKSHI \"s\")\n+\t\t     (UNSPEC_UNPACKUHI \"u\")\n+\t\t     (UNSPEC_UNPACKSLO \"s\")\n+\t\t     (UNSPEC_UNPACKULO \"u\")])\n+\n (define_int_attr sur [(UNSPEC_SHADD \"s\") (UNSPEC_UHADD \"u\")\n \t\t      (UNSPEC_SRHADD \"sr\") (UNSPEC_URHADD \"ur\")\n \t\t      (UNSPEC_SHSUB \"s\") (UNSPEC_UHSUB \"u\")\n@@ -1328,7 +1534,9 @@\n \n (define_int_attr perm_hilo [(UNSPEC_ZIP1 \"1\") (UNSPEC_ZIP2 \"2\")\n \t\t\t    (UNSPEC_TRN1 \"1\") (UNSPEC_TRN2 \"2\")\n-\t\t\t    (UNSPEC_UZP1 \"1\") (UNSPEC_UZP2 \"2\")])\n+\t\t\t    (UNSPEC_UZP1 \"1\") (UNSPEC_UZP2 \"2\")\n+\t\t\t    (UNSPEC_UNPACKSHI \"hi\") (UNSPEC_UNPACKUHI \"hi\")\n+\t\t\t    (UNSPEC_UNPACKSLO \"lo\") (UNSPEC_UNPACKULO \"lo\")])\n \n (define_int_attr frecp_suffix  [(UNSPEC_FRECPE \"e\") (UNSPEC_FRECPX \"x\")])\n \n@@ -1361,3 +1569,27 @@\n \n (define_int_attr f16mac1 [(UNSPEC_FMLAL \"a\") (UNSPEC_FMLSL \"s\")\n \t\t\t  (UNSPEC_FMLAL2 \"a\") (UNSPEC_FMLSL2 \"s\")])\n+\n+;; The condition associated with an UNSPEC_COND_<xx>.\n+(define_int_attr cmp_op [(UNSPEC_COND_LT \"lt\")\n+\t\t\t (UNSPEC_COND_LE \"le\")\n+\t\t\t (UNSPEC_COND_EQ \"eq\")\n+\t\t\t (UNSPEC_COND_NE \"ne\")\n+\t\t\t (UNSPEC_COND_GE \"ge\")\n+\t\t\t (UNSPEC_COND_GT \"gt\")\n+\t\t\t (UNSPEC_COND_LO \"lo\")\n+\t\t\t (UNSPEC_COND_LS \"ls\")\n+\t\t\t (UNSPEC_COND_HS \"hs\")\n+\t\t\t (UNSPEC_COND_HI \"hi\")])\n+\n+;; The constraint to use for an UNSPEC_COND_<xx>.\n+(define_int_attr imm_con [(UNSPEC_COND_EQ \"vsc\")\n+\t\t\t  (UNSPEC_COND_NE \"vsc\")\n+\t\t\t  (UNSPEC_COND_LT \"vsc\")\n+\t\t\t  (UNSPEC_COND_GE \"vsc\")\n+\t\t\t  (UNSPEC_COND_LE \"vsc\")\n+\t\t\t  (UNSPEC_COND_GT \"vsc\")\n+\t\t\t  (UNSPEC_COND_LO \"vsd\")\n+\t\t\t  (UNSPEC_COND_LS \"vsd\")\n+\t\t\t  (UNSPEC_COND_HS \"vsd\")\n+\t\t\t  (UNSPEC_COND_HI \"vsd\")])"}, {"sha": "7424f506a5c6a289d3afc4472670e57d4029c8f3", "filename": "gcc/config/aarch64/predicates.md", "status": "modified", "additions": 176, "deletions": 22, "changes": 198, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fconfig%2Faarch64%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fpredicates.md?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -93,6 +93,10 @@\n (define_predicate \"aarch64_fp_vec_pow2\"\n   (match_test \"aarch64_vec_fpconst_pow_of_2 (op) > 0\"))\n \n+(define_predicate \"aarch64_sve_cnt_immediate\"\n+  (and (match_code \"const_poly_int\")\n+       (match_test \"aarch64_sve_cnt_immediate_p (op)\")))\n+\n (define_predicate \"aarch64_sub_immediate\"\n   (and (match_code \"const_int\")\n        (match_test \"aarch64_uimm12_shift (-INTVAL (op))\")))\n@@ -114,9 +118,22 @@\n   (and (match_operand 0 \"aarch64_pluslong_immediate\")\n        (not (match_operand 0 \"aarch64_plus_immediate\"))))\n \n+(define_predicate \"aarch64_sve_addvl_addpl_immediate\"\n+  (and (match_code \"const_poly_int\")\n+       (match_test \"aarch64_sve_addvl_addpl_immediate_p (op)\")))\n+\n+(define_predicate \"aarch64_split_add_offset_immediate\"\n+  (and (match_code \"const_poly_int\")\n+       (match_test \"aarch64_add_offset_temporaries (op) == 1\")))\n+\n (define_predicate \"aarch64_pluslong_operand\"\n   (ior (match_operand 0 \"register_operand\")\n-       (match_operand 0 \"aarch64_pluslong_immediate\")))\n+       (match_operand 0 \"aarch64_pluslong_immediate\")\n+       (match_operand 0 \"aarch64_sve_addvl_addpl_immediate\")))\n+\n+(define_predicate \"aarch64_pluslong_or_poly_operand\"\n+  (ior (match_operand 0 \"aarch64_pluslong_operand\")\n+       (match_operand 0 \"aarch64_split_add_offset_immediate\")))\n \n (define_predicate \"aarch64_logical_immediate\"\n   (and (match_code \"const_int\")\n@@ -263,11 +280,18 @@\n })\n \n (define_predicate \"aarch64_mov_operand\"\n-  (and (match_code \"reg,subreg,mem,const,const_int,symbol_ref,label_ref,high\")\n+  (and (match_code \"reg,subreg,mem,const,const_int,symbol_ref,label_ref,high,\n+\t\t    const_poly_int,const_vector\")\n        (ior (match_operand 0 \"register_operand\")\n \t    (ior (match_operand 0 \"memory_operand\")\n \t\t (match_test \"aarch64_mov_operand_p (op, mode)\")))))\n \n+(define_predicate \"aarch64_nonmemory_operand\"\n+  (and (match_code \"reg,subreg,const,const_int,symbol_ref,label_ref,high,\n+\t\t    const_poly_int,const_vector\")\n+       (ior (match_operand 0 \"register_operand\")\n+\t    (match_test \"aarch64_mov_operand_p (op, mode)\"))))\n+\n (define_predicate \"aarch64_movti_operand\"\n   (and (match_code \"reg,subreg,mem,const_int\")\n        (ior (match_operand 0 \"register_operand\")\n@@ -303,6 +327,9 @@\n   return aarch64_get_condition_code (op) >= 0;\n })\n \n+(define_special_predicate \"aarch64_equality_operator\"\n+  (match_code \"eq,ne\"))\n+\n (define_special_predicate \"aarch64_carry_operation\"\n   (match_code \"ne,geu\")\n {\n@@ -342,22 +369,34 @@\n })\n \n (define_special_predicate \"aarch64_simd_lshift_imm\"\n-  (match_code \"const_vector\")\n+  (match_code \"const,const_vector\")\n {\n   return aarch64_simd_shift_imm_p (op, mode, true);\n })\n \n (define_special_predicate \"aarch64_simd_rshift_imm\"\n-  (match_code \"const_vector\")\n+  (match_code \"const,const_vector\")\n {\n   return aarch64_simd_shift_imm_p (op, mode, false);\n })\n \n+(define_predicate \"aarch64_simd_imm_zero\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"op == CONST0_RTX (GET_MODE (op))\")))\n+\n+(define_predicate \"aarch64_simd_or_scalar_imm_zero\"\n+  (and (match_code \"const_int,const_double,const,const_vector\")\n+       (match_test \"op == CONST0_RTX (GET_MODE (op))\")))\n+\n+(define_predicate \"aarch64_simd_imm_minus_one\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"op == CONSTM1_RTX (GET_MODE (op))\")))\n+\n (define_predicate \"aarch64_simd_reg_or_zero\"\n-  (and (match_code \"reg,subreg,const_int,const_double,const_vector\")\n+  (and (match_code \"reg,subreg,const_int,const_double,const,const_vector\")\n        (ior (match_operand 0 \"register_operand\")\n-           (ior (match_test \"op == const0_rtx\")\n-                (match_test \"aarch64_simd_imm_zero_p (op, mode)\")))))\n+\t    (match_test \"op == const0_rtx\")\n+\t    (match_operand 0 \"aarch64_simd_imm_zero\"))))\n \n (define_predicate \"aarch64_simd_struct_operand\"\n   (and (match_code \"mem\")\n@@ -377,21 +416,6 @@\n \t\t    || GET_CODE (XEXP (op, 0)) == POST_INC\n \t\t    || GET_CODE (XEXP (op, 0)) == REG\")))\n \n-(define_special_predicate \"aarch64_simd_imm_zero\"\n-  (match_code \"const_vector\")\n-{\n-  return aarch64_simd_imm_zero_p (op, mode);\n-})\n-\n-(define_special_predicate \"aarch64_simd_or_scalar_imm_zero\"\n-  (match_test \"aarch64_simd_imm_zero_p (op, mode)\"))\n-\n-(define_special_predicate \"aarch64_simd_imm_minus_one\"\n-  (match_code \"const_vector\")\n-{\n-  return aarch64_const_vec_all_same_int_p (op, -1);\n-})\n-\n ;; Predicates used by the various SIMD shift operations.  These\n ;; fall in to 3 categories.\n ;;   Shifts with a range 0-(bit_size - 1) (aarch64_simd_shift_imm)\n@@ -448,3 +472,133 @@\n (define_predicate \"aarch64_constant_pool_symref\"\n    (and (match_code \"symbol_ref\")\n \t(match_test \"CONSTANT_POOL_ADDRESS_P (op)\")))\n+\n+(define_predicate \"aarch64_constant_vector_operand\"\n+  (match_code \"const,const_vector\"))\n+\n+(define_predicate \"aarch64_sve_ld1r_operand\"\n+  (and (match_operand 0 \"memory_operand\")\n+       (match_test \"aarch64_sve_ld1r_operand_p (op)\")))\n+\n+;; Like memory_operand, but restricted to addresses that are valid for\n+;; SVE LDR and STR instructions.\n+(define_predicate \"aarch64_sve_ldr_operand\"\n+  (and (match_code \"mem\")\n+       (match_test \"aarch64_sve_ldr_operand_p (op)\")))\n+\n+(define_predicate \"aarch64_sve_nonimmediate_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_sve_ldr_operand\")))\n+\n+(define_predicate \"aarch64_sve_general_operand\"\n+  (and (match_code \"reg,subreg,mem,const,const_vector\")\n+       (ior (match_operand 0 \"register_operand\")\n+\t    (match_operand 0 \"aarch64_sve_ldr_operand\")\n+\t    (match_test \"aarch64_mov_operand_p (op, mode)\"))))\n+\n+;; Doesn't include immediates, since those are handled by the move\n+;; patterns instead.\n+(define_predicate \"aarch64_sve_dup_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_sve_ld1r_operand\")))\n+\n+(define_predicate \"aarch64_sve_arith_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_sve_arith_immediate_p (op, false)\")))\n+\n+(define_predicate \"aarch64_sve_sub_arith_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_sve_arith_immediate_p (op, true)\")))\n+\n+(define_predicate \"aarch64_sve_inc_dec_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_sve_inc_dec_immediate_p (op)\")))\n+\n+(define_predicate \"aarch64_sve_logical_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_sve_bitmask_immediate_p (op)\")))\n+\n+(define_predicate \"aarch64_sve_mul_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_const_vec_all_same_in_range_p (op, -128, 127)\")))\n+\n+(define_predicate \"aarch64_sve_dup_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_sve_dup_immediate_p (op)\")))\n+\n+(define_predicate \"aarch64_sve_cmp_vsc_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_sve_cmp_immediate_p (op, true)\")))\n+\n+(define_predicate \"aarch64_sve_cmp_vsd_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_sve_cmp_immediate_p (op, false)\")))\n+\n+(define_predicate \"aarch64_sve_index_immediate\"\n+  (and (match_code \"const_int\")\n+       (match_test \"aarch64_sve_index_immediate_p (op)\")))\n+\n+(define_predicate \"aarch64_sve_float_arith_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_sve_float_arith_immediate_p (op, false)\")))\n+\n+(define_predicate \"aarch64_sve_float_arith_with_sub_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_sve_float_arith_immediate_p (op, true)\")))\n+\n+(define_predicate \"aarch64_sve_float_mul_immediate\"\n+  (and (match_code \"const,const_vector\")\n+       (match_test \"aarch64_sve_float_mul_immediate_p (op)\")))\n+\n+(define_predicate \"aarch64_sve_arith_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_sve_arith_immediate\")))\n+\n+(define_predicate \"aarch64_sve_add_operand\"\n+  (ior (match_operand 0 \"aarch64_sve_arith_operand\")\n+       (match_operand 0 \"aarch64_sve_sub_arith_immediate\")\n+       (match_operand 0 \"aarch64_sve_inc_dec_immediate\")))\n+\n+(define_predicate \"aarch64_sve_logical_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_sve_logical_immediate\")))\n+\n+(define_predicate \"aarch64_sve_lshift_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_simd_lshift_imm\")))\n+\n+(define_predicate \"aarch64_sve_rshift_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_simd_rshift_imm\")))\n+\n+(define_predicate \"aarch64_sve_mul_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_sve_mul_immediate\")))\n+\n+(define_predicate \"aarch64_sve_cmp_vsc_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_sve_cmp_vsc_immediate\")))\n+\n+(define_predicate \"aarch64_sve_cmp_vsd_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_sve_cmp_vsd_immediate\")))\n+\n+(define_predicate \"aarch64_sve_index_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_sve_index_immediate\")))\n+\n+(define_predicate \"aarch64_sve_float_arith_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_sve_float_arith_immediate\")))\n+\n+(define_predicate \"aarch64_sve_float_arith_with_sub_operand\"\n+  (ior (match_operand 0 \"aarch64_sve_float_arith_operand\")\n+       (match_operand 0 \"aarch64_sve_float_arith_with_sub_immediate\")))\n+\n+(define_predicate \"aarch64_sve_float_mul_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_sve_float_mul_immediate\")))\n+\n+(define_predicate \"aarch64_sve_vec_perm_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_constant_vector_operand\")))"}, {"sha": "28c61a078d2a9b96af1e7f194dcaeda916da25ef", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -14594,6 +14594,23 @@ Permissible values are @samp{none}, which disables return address signing,\n functions, and @samp{all}, which enables pointer signing for all functions.  The\n default value is @samp{none}.\n \n+@item -msve-vector-bits=@var{bits}\n+@opindex msve-vector-bits\n+Specify the number of bits in an SVE vector register.  This option only has\n+an effect when SVE is enabled.\n+\n+GCC supports two forms of SVE code generation: ``vector-length\n+agnostic'' output that works with any size of vector register and\n+``vector-length specific'' output that only works when the vector\n+registers are a particular size.  Replacing @var{bits} with\n+@samp{scalable} selects vector-length agnostic output while\n+replacing it with a number selects vector-length specific output.\n+The possible lengths in the latter case are: 128, 256, 512, 1024\n+and 2048.  @samp{scalable} is the default.\n+\n+At present, @samp{-msve-vector-bits=128} produces the same output\n+as @samp{-msve-vector-bits=scalable}.\n+\n @end table\n \n @subsubsection @option{-march} and @option{-mcpu} Feature Modifiers\n@@ -14617,6 +14634,9 @@ values for options @option{-march} and @option{-mcpu}.\n Enable Advanced SIMD instructions.  This also enables floating-point\n instructions.  This is on by default for all possible values for options\n @option{-march} and @option{-mcpu}.\n+@item sve\n+Enable Scalable Vector Extension instructions.  This also enables Advanced\n+SIMD and floating-point instructions.\n @item lse\n Enable Large System Extension instructions.  This is on by default for\n @option{-march=armv8.1-a}."}, {"sha": "e956c751b573a8732ba7199bf4ff371e68657aa3", "filename": "gcc/doc/md.texi", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fdoc%2Fmd.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43cacb12fc859b671464b63668794158974b2a34/gcc%2Fdoc%2Fmd.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Fmd.texi?ref=43cacb12fc859b671464b63668794158974b2a34", "patch": "@@ -1735,7 +1735,13 @@ the meanings of that architecture's constraints.\n The stack pointer register (@code{SP})\n \n @item w\n-Floating point or SIMD vector register\n+Floating point register, Advanced SIMD vector register or SVE vector register\n+\n+@item Upl\n+One of the low eight SVE predicate registers (@code{P0} to @code{P7})\n+\n+@item Upa\n+Any of the SVE predicate registers (@code{P0} to @code{P15})\n \n @item I\n Integer constant that is valid as an immediate operand in an @code{ADD}"}]}