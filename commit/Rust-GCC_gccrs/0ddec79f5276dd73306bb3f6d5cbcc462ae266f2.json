{"sha": "0ddec79f5276dd73306bb3f6d5cbcc462ae266f2", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MGRkZWM3OWY1Mjc2ZGQ3MzMwNmJiM2Y2ZDVjYmNjNDYyYWUyNjZmMg==", "commit": {"author": {"name": "James Greenhalgh", "email": "james.greenhalgh@arm.com", "date": "2013-04-22T12:46:38Z"}, "committer": {"name": "James Greenhalgh", "email": "jgreenhalgh@gcc.gnu.org", "date": "2013-04-22T12:46:38Z"}, "message": "[AArch64] Map standard pattern names to NEON intrinsics directly.\n\ngcc/\n\t* config/aarch64/aarch64-builtins.c\n\t(CF): Remove.\n\t(CF0, CF1, CF2, CF3, CF4, CF10): New.\n\t(VAR<1-12>): Add MAP parameter.\n\t(BUILTIN_*): Likewise.\n\t* config/aarch64/aarch64-simd-builtins.def: Set MAP parameter.\n\t* config/aarch64/aarch64-simd.md (aarch64_sshl_n<mode>): Remove.\n\t(aarch64_ushl_n<mode>): Likewise.\n\t(aarch64_sshr_n<mode>): Likewise.\n\t(aarch64_ushr_n<mode>): Likewise.\n\t(aarch64_<maxmin><mode>): Likewise.\n\t(aarch64_sqrt<mode>): Likewise.\n\t* config/aarch64/arm_neon.h (vshl<q>_n_*): Use new builtin names.\n\t(vshr<q>_n_*): Likewise.\n\nFrom-SVN: r198137", "tree": {"sha": "a697b03c3b17616124be30029706b14a1176bbc4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/a697b03c3b17616124be30029706b14a1176bbc4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/comments", "author": {"login": "jgreenhalgh-arm", "id": 6104025, "node_id": "MDQ6VXNlcjYxMDQwMjU=", "avatar_url": "https://avatars.githubusercontent.com/u/6104025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgreenhalgh-arm", "html_url": "https://github.com/jgreenhalgh-arm", "followers_url": "https://api.github.com/users/jgreenhalgh-arm/followers", "following_url": "https://api.github.com/users/jgreenhalgh-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jgreenhalgh-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgreenhalgh-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgreenhalgh-arm/subscriptions", "organizations_url": "https://api.github.com/users/jgreenhalgh-arm/orgs", "repos_url": "https://api.github.com/users/jgreenhalgh-arm/repos", "events_url": "https://api.github.com/users/jgreenhalgh-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jgreenhalgh-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "0050faf8aa1be242fbeda7512c9908638c29f383", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0050faf8aa1be242fbeda7512c9908638c29f383", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0050faf8aa1be242fbeda7512c9908638c29f383"}], "stats": {"total": 878, "additions": 428, "deletions": 450}, "files": [{"sha": "f936a3e36e0e09a1c7db1778f03fe3eb6de18372", "filename": "gcc/ChangeLog", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=0ddec79f5276dd73306bb3f6d5cbcc462ae266f2", "patch": "@@ -1,3 +1,20 @@\n+2013-04-22  James Greenhalgh  <james.greenhalgh@arm.com>\n+\n+\t* config/aarch64/aarch64-builtins.c\n+\t(CF): Remove.\n+\t(CF0, CF1, CF2, CF3, CF4, CF10): New.\n+\t(VAR<1-12>): Add MAP parameter.\n+\t(BUILTIN_*): Likewise.\n+\t* config/aarch64/aarch64-simd-builtins.def: Set MAP parameter.\n+\t* config/aarch64/aarch64-simd.md (aarch64_sshl_n<mode>): Remove.\n+\t(aarch64_ushl_n<mode>): Likewise.\n+\t(aarch64_sshr_n<mode>): Likewise.\n+\t(aarch64_ushr_n<mode>): Likewise.\n+\t(aarch64_<maxmin><mode>): Likewise.\n+\t(aarch64_sqrt<mode>): Likewise.\n+\t* config/aarch64/arm_neon.h (vshl<q>_n_*): Use new builtin names.\n+\t(vshr<q>_n_*): Likewise.\n+\n 2013-04-22  James Greenhalgh  <james.greenhalgh@arm.com>\n \n \t* config/aarch64/aarch64-builtins.c"}, {"sha": "35475ba12bbe7808fea24c1ac13a58e4eb2ce15d", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "modified", "additions": 169, "deletions": 155, "changes": 324, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=0ddec79f5276dd73306bb3f6d5cbcc462ae266f2", "patch": "@@ -130,125 +130,133 @@ typedef struct\n   unsigned int fcode;\n } aarch64_simd_builtin_datum;\n \n-#define CF(N, X) CODE_FOR_aarch64_##N##X\n-\n-#define VAR1(T, N, A) \\\n-  {#N, AARCH64_SIMD_##T, UP (A), CF (N, A), 0},\n-#define VAR2(T, N, A, B) \\\n-  VAR1 (T, N, A) \\\n-  VAR1 (T, N, B)\n-#define VAR3(T, N, A, B, C) \\\n-  VAR2 (T, N, A, B) \\\n-  VAR1 (T, N, C)\n-#define VAR4(T, N, A, B, C, D) \\\n-  VAR3 (T, N, A, B, C) \\\n-  VAR1 (T, N, D)\n-#define VAR5(T, N, A, B, C, D, E) \\\n-  VAR4 (T, N, A, B, C, D) \\\n-  VAR1 (T, N, E)\n-#define VAR6(T, N, A, B, C, D, E, F) \\\n-  VAR5 (T, N, A, B, C, D, E) \\\n-  VAR1 (T, N, F)\n-#define VAR7(T, N, A, B, C, D, E, F, G) \\\n-  VAR6 (T, N, A, B, C, D, E, F) \\\n-  VAR1 (T, N, G)\n-#define VAR8(T, N, A, B, C, D, E, F, G, H) \\\n-  VAR7 (T, N, A, B, C, D, E, F, G) \\\n-  VAR1 (T, N, H)\n-#define VAR9(T, N, A, B, C, D, E, F, G, H, I) \\\n-  VAR8 (T, N, A, B, C, D, E, F, G, H) \\\n-  VAR1 (T, N, I)\n-#define VAR10(T, N, A, B, C, D, E, F, G, H, I, J) \\\n-  VAR9 (T, N, A, B, C, D, E, F, G, H, I) \\\n-  VAR1 (T, N, J)\n-#define VAR11(T, N, A, B, C, D, E, F, G, H, I, J, K) \\\n-  VAR10 (T, N, A, B, C, D, E, F, G, H, I, J) \\\n-  VAR1 (T, N, K)\n-#define VAR12(T, N, A, B, C, D, E, F, G, H, I, J, K, L) \\\n-  VAR11 (T, N, A, B, C, D, E, F, G, H, I, J, K) \\\n-  VAR1 (T, N, L)\n+#define CF0(N, X) CODE_FOR_aarch64_##N##X\n+#define CF1(N, X) CODE_FOR_##N##X##1\n+#define CF2(N, X) CODE_FOR_##N##X##2\n+#define CF3(N, X) CODE_FOR_##N##X##3\n+#define CF4(N, X) CODE_FOR_##N##X##4\n+#define CF10(N, X) CODE_FOR_##N##X\n+\n+#define VAR1(T, N, MAP, A) \\\n+  {#N, AARCH64_SIMD_##T, UP (A), CF##MAP (N, A), 0},\n+#define VAR2(T, N, MAP, A, B) \\\n+  VAR1 (T, N, MAP, A) \\\n+  VAR1 (T, N, MAP, B)\n+#define VAR3(T, N, MAP, A, B, C) \\\n+  VAR2 (T, N, MAP, A, B) \\\n+  VAR1 (T, N, MAP, C)\n+#define VAR4(T, N, MAP, A, B, C, D) \\\n+  VAR3 (T, N, MAP, A, B, C) \\\n+  VAR1 (T, N, MAP, D)\n+#define VAR5(T, N, MAP, A, B, C, D, E) \\\n+  VAR4 (T, N, MAP, A, B, C, D) \\\n+  VAR1 (T, N, MAP, E)\n+#define VAR6(T, N, MAP, A, B, C, D, E, F) \\\n+  VAR5 (T, N, MAP, A, B, C, D, E) \\\n+  VAR1 (T, N, MAP, F)\n+#define VAR7(T, N, MAP, A, B, C, D, E, F, G) \\\n+  VAR6 (T, N, MAP, A, B, C, D, E, F) \\\n+  VAR1 (T, N, MAP, G)\n+#define VAR8(T, N, MAP, A, B, C, D, E, F, G, H) \\\n+  VAR7 (T, N, MAP, A, B, C, D, E, F, G) \\\n+  VAR1 (T, N, MAP, H)\n+#define VAR9(T, N, MAP, A, B, C, D, E, F, G, H, I) \\\n+  VAR8 (T, N, MAP, A, B, C, D, E, F, G, H) \\\n+  VAR1 (T, N, MAP, I)\n+#define VAR10(T, N, MAP, A, B, C, D, E, F, G, H, I, J) \\\n+  VAR9 (T, N, MAP, A, B, C, D, E, F, G, H, I) \\\n+  VAR1 (T, N, MAP, J)\n+#define VAR11(T, N, MAP, A, B, C, D, E, F, G, H, I, J, K) \\\n+  VAR10 (T, N, MAP, A, B, C, D, E, F, G, H, I, J) \\\n+  VAR1 (T, N, MAP, K)\n+#define VAR12(T, N, MAP, A, B, C, D, E, F, G, H, I, J, K, L) \\\n+  VAR11 (T, N, MAP, A, B, C, D, E, F, G, H, I, J, K) \\\n+  VAR1 (T, N, MAP, L)\n \n /* BUILTIN_<ITERATOR> macros should expand to cover the same range of\n    modes as is given for each define_mode_iterator in\n    config/aarch64/iterators.md.  */\n \n-#define BUILTIN_DX(T, N) \\\n-  VAR2 (T, N, di, df)\n-#define BUILTIN_GPF(T, N) \\\n-  VAR2 (T, N, sf, df)\n-#define BUILTIN_SDQ_I(T, N) \\\n-  VAR4 (T, N, qi, hi, si, di)\n-#define BUILTIN_SD_HSI(T, N) \\\n-  VAR2 (T, N, hi, si)\n-#define BUILTIN_V2F(T, N) \\\n-  VAR2 (T, N, v2sf, v2df)\n-#define BUILTIN_VALL(T, N) \\\n-  VAR10 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, v2sf, v4sf, v2df)\n-#define BUILTIN_VB(T, N) \\\n-  VAR2 (T, N, v8qi, v16qi)\n-#define BUILTIN_VD(T, N) \\\n-  VAR4 (T, N, v8qi, v4hi, v2si, v2sf)\n-#define BUILTIN_VDC(T, N) \\\n-  VAR6 (T, N, v8qi, v4hi, v2si, v2sf, di, df)\n-#define BUILTIN_VDIC(T, N) \\\n-  VAR3 (T, N, v8qi, v4hi, v2si)\n-#define BUILTIN_VDN(T, N) \\\n-  VAR3 (T, N, v4hi, v2si, di)\n-#define BUILTIN_VDQ(T, N) \\\n-  VAR7 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di)\n-#define BUILTIN_VDQF(T, N) \\\n-  VAR3 (T, N, v2sf, v4sf, v2df)\n-#define BUILTIN_VDQHS(T, N) \\\n-  VAR4 (T, N, v4hi, v8hi, v2si, v4si)\n-#define BUILTIN_VDQIF(T, N) \\\n-  VAR9 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2sf, v4sf, v2df)\n-#define BUILTIN_VDQM(T, N) \\\n-  VAR6 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si)\n-#define BUILTIN_VDQV(T, N) \\\n-  VAR5 (T, N, v8qi, v16qi, v4hi, v8hi, v4si)\n-#define BUILTIN_VDQ_BHSI(T, N) \\\n-  VAR6 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si)\n-#define BUILTIN_VDQ_I(T, N) \\\n-  VAR7 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di)\n-#define BUILTIN_VDW(T, N) \\\n-  VAR3 (T, N, v8qi, v4hi, v2si)\n-#define BUILTIN_VD_BHSI(T, N) \\\n-  VAR3 (T, N, v8qi, v4hi, v2si)\n-#define BUILTIN_VD_HSI(T, N) \\\n-  VAR2 (T, N, v4hi, v2si)\n-#define BUILTIN_VD_RE(T, N) \\\n-  VAR6 (T, N, v8qi, v4hi, v2si, v2sf, di, df)\n-#define BUILTIN_VQ(T, N) \\\n-  VAR6 (T, N, v16qi, v8hi, v4si, v2di, v4sf, v2df)\n-#define BUILTIN_VQN(T, N) \\\n-  VAR3 (T, N, v8hi, v4si, v2di)\n-#define BUILTIN_VQW(T, N) \\\n-  VAR3 (T, N, v16qi, v8hi, v4si)\n-#define BUILTIN_VQ_HSI(T, N) \\\n-  VAR2 (T, N, v8hi, v4si)\n-#define BUILTIN_VQ_S(T, N) \\\n-  VAR6 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si)\n-#define BUILTIN_VSDQ_HSI(T, N) \\\n-  VAR6 (T, N, v4hi, v8hi, v2si, v4si, hi, si)\n-#define BUILTIN_VSDQ_I(T, N) \\\n-  VAR11 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, qi, hi, si, di)\n-#define BUILTIN_VSDQ_I_BHSI(T, N) \\\n-  VAR10 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, qi, hi, si)\n-#define BUILTIN_VSDQ_I_DI(T, N) \\\n-  VAR8 (T, N, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, di)\n-#define BUILTIN_VSD_HSI(T, N) \\\n-  VAR4 (T, N, v4hi, v2si, hi, si)\n-#define BUILTIN_VSQN_HSDI(T, N) \\\n-  VAR6 (T, N, v8hi, v4si, v2di, hi, si, di)\n-#define BUILTIN_VSTRUCT(T, N) \\\n-  VAR3 (T, N, oi, ci, xi)\n+#define BUILTIN_DX(T, N, MAP) \\\n+  VAR2 (T, N, MAP, di, df)\n+#define BUILTIN_GPF(T, N, MAP) \\\n+  VAR2 (T, N, MAP, sf, df)\n+#define BUILTIN_SDQ_I(T, N, MAP) \\\n+  VAR4 (T, N, MAP, qi, hi, si, di)\n+#define BUILTIN_SD_HSI(T, N, MAP) \\\n+  VAR2 (T, N, MAP, hi, si)\n+#define BUILTIN_V2F(T, N, MAP) \\\n+  VAR2 (T, N, MAP, v2sf, v2df)\n+#define BUILTIN_VALL(T, N, MAP) \\\n+  VAR10 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, \\\n+\t v4si, v2di, v2sf, v4sf, v2df)\n+#define BUILTIN_VB(T, N, MAP) \\\n+  VAR2 (T, N, MAP, v8qi, v16qi)\n+#define BUILTIN_VD(T, N, MAP) \\\n+  VAR4 (T, N, MAP, v8qi, v4hi, v2si, v2sf)\n+#define BUILTIN_VDC(T, N, MAP) \\\n+  VAR6 (T, N, MAP, v8qi, v4hi, v2si, v2sf, di, df)\n+#define BUILTIN_VDIC(T, N, MAP) \\\n+  VAR3 (T, N, MAP, v8qi, v4hi, v2si)\n+#define BUILTIN_VDN(T, N, MAP) \\\n+  VAR3 (T, N, MAP, v4hi, v2si, di)\n+#define BUILTIN_VDQ(T, N, MAP) \\\n+  VAR7 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di)\n+#define BUILTIN_VDQF(T, N, MAP) \\\n+  VAR3 (T, N, MAP, v2sf, v4sf, v2df)\n+#define BUILTIN_VDQH(T, N, MAP) \\\n+  VAR2 (T, N, MAP, v4hi, v8hi)\n+#define BUILTIN_VDQHS(T, N, MAP) \\\n+  VAR4 (T, N, MAP, v4hi, v8hi, v2si, v4si)\n+#define BUILTIN_VDQIF(T, N, MAP) \\\n+  VAR9 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2sf, v4sf, v2df)\n+#define BUILTIN_VDQM(T, N, MAP) \\\n+  VAR6 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si)\n+#define BUILTIN_VDQV(T, N, MAP) \\\n+  VAR5 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v4si)\n+#define BUILTIN_VDQ_BHSI(T, N, MAP) \\\n+  VAR6 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si)\n+#define BUILTIN_VDQ_I(T, N, MAP) \\\n+  VAR7 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di)\n+#define BUILTIN_VDW(T, N, MAP) \\\n+  VAR3 (T, N, MAP, v8qi, v4hi, v2si)\n+#define BUILTIN_VD_BHSI(T, N, MAP) \\\n+  VAR3 (T, N, MAP, v8qi, v4hi, v2si)\n+#define BUILTIN_VD_HSI(T, N, MAP) \\\n+  VAR2 (T, N, MAP, v4hi, v2si)\n+#define BUILTIN_VD_RE(T, N, MAP) \\\n+  VAR6 (T, N, MAP, v8qi, v4hi, v2si, v2sf, di, df)\n+#define BUILTIN_VQ(T, N, MAP) \\\n+  VAR6 (T, N, MAP, v16qi, v8hi, v4si, v2di, v4sf, v2df)\n+#define BUILTIN_VQN(T, N, MAP) \\\n+  VAR3 (T, N, MAP, v8hi, v4si, v2di)\n+#define BUILTIN_VQW(T, N, MAP) \\\n+  VAR3 (T, N, MAP, v16qi, v8hi, v4si)\n+#define BUILTIN_VQ_HSI(T, N, MAP) \\\n+  VAR2 (T, N, MAP, v8hi, v4si)\n+#define BUILTIN_VQ_S(T, N, MAP) \\\n+  VAR6 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si)\n+#define BUILTIN_VSDQ_HSI(T, N, MAP) \\\n+  VAR6 (T, N, MAP, v4hi, v8hi, v2si, v4si, hi, si)\n+#define BUILTIN_VSDQ_I(T, N, MAP) \\\n+  VAR11 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, qi, hi, si, di)\n+#define BUILTIN_VSDQ_I_BHSI(T, N, MAP) \\\n+  VAR10 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, qi, hi, si)\n+#define BUILTIN_VSDQ_I_DI(T, N, MAP) \\\n+  VAR8 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, v4si, v2di, di)\n+#define BUILTIN_VSD_HSI(T, N, MAP) \\\n+  VAR4 (T, N, MAP, v4hi, v2si, hi, si)\n+#define BUILTIN_VSQN_HSDI(T, N, MAP) \\\n+  VAR6 (T, N, MAP, v8hi, v4si, v2di, hi, si, di)\n+#define BUILTIN_VSTRUCT(T, N, MAP) \\\n+  VAR3 (T, N, MAP, oi, ci, xi)\n \n static aarch64_simd_builtin_datum aarch64_simd_builtin_data[] = {\n #include \"aarch64-simd-builtins.def\"\n };\n \n #undef VAR1\n-#define VAR1(T, N, A) \\\n+#define VAR1(T, N, MAP, A) \\\n   AARCH64_SIMD_BUILTIN_##N##A,\n \n enum aarch64_builtins\n@@ -261,53 +269,6 @@ enum aarch64_builtins\n   AARCH64_BUILTIN_MAX\n };\n \n-#undef BUILTIN_DX\n-#undef BUILTIN_SDQ_I\n-#undef BUILTIN_SD_HSI\n-#undef BUILTIN_V2F\n-#undef BUILTIN_VALL\n-#undef BUILTIN_VB\n-#undef BUILTIN_VD\n-#undef BUILTIN_VDC\n-#undef BUILTIN_VDIC\n-#undef BUILTIN_VDN\n-#undef BUILTIN_VDQ\n-#undef BUILTIN_VDQF\n-#undef BUILTIN_VDQHS\n-#undef BUILTIN_VDQIF\n-#undef BUILTIN_VDQM\n-#undef BUILTIN_VDQV\n-#undef BUILTIN_VDQ_BHSI\n-#undef BUILTIN_VDQ_I\n-#undef BUILTIN_VDW\n-#undef BUILTIN_VD_BHSI\n-#undef BUILTIN_VD_HSI\n-#undef BUILTIN_VD_RE\n-#undef BUILTIN_VQ\n-#undef BUILTIN_VQN\n-#undef BUILTIN_VQW\n-#undef BUILTIN_VQ_HSI\n-#undef BUILTIN_VQ_S\n-#undef BUILTIN_VSDQ_HSI\n-#undef BUILTIN_VSDQ_I\n-#undef BUILTIN_VSDQ_I_BHSI\n-#undef BUILTIN_VSDQ_I_DI\n-#undef BUILTIN_VSD_HSI\n-#undef BUILTIN_VSQN_HSDI\n-#undef BUILTIN_VSTRUCT\n-#undef CF\n-#undef VAR1\n-#undef VAR2\n-#undef VAR3\n-#undef VAR4\n-#undef VAR5\n-#undef VAR6\n-#undef VAR7\n-#undef VAR8\n-#undef VAR9\n-#undef VAR10\n-#undef VAR11\n-\n static GTY(()) tree aarch64_builtin_decls[AARCH64_BUILTIN_MAX];\n \n #define NUM_DREG_TYPES 6\n@@ -1295,3 +1256,56 @@ aarch64_builtin_vectorized_function (tree fndecl, tree type_out, tree type_in)\n }\n #undef AARCH64_CHECK_BUILTIN_MODE\n #undef AARCH64_FIND_FRINT_VARIANT\n+#undef BUILTIN_DX\n+#undef BUILTIN_SDQ_I\n+#undef BUILTIN_SD_HSI\n+#undef BUILTIN_V2F\n+#undef BUILTIN_VALL\n+#undef BUILTIN_VB\n+#undef BUILTIN_VD\n+#undef BUILTIN_VDC\n+#undef BUILTIN_VDIC\n+#undef BUILTIN_VDN\n+#undef BUILTIN_VDQ\n+#undef BUILTIN_VDQF\n+#undef BUILTIN_VDQH\n+#undef BUILTIN_VDQHS\n+#undef BUILTIN_VDQIF\n+#undef BUILTIN_VDQM\n+#undef BUILTIN_VDQV\n+#undef BUILTIN_VDQ_BHSI\n+#undef BUILTIN_VDQ_I\n+#undef BUILTIN_VDW\n+#undef BUILTIN_VD_BHSI\n+#undef BUILTIN_VD_HSI\n+#undef BUILTIN_VD_RE\n+#undef BUILTIN_VQ\n+#undef BUILTIN_VQN\n+#undef BUILTIN_VQW\n+#undef BUILTIN_VQ_HSI\n+#undef BUILTIN_VQ_S\n+#undef BUILTIN_VSDQ_HSI\n+#undef BUILTIN_VSDQ_I\n+#undef BUILTIN_VSDQ_I_BHSI\n+#undef BUILTIN_VSDQ_I_DI\n+#undef BUILTIN_VSD_HSI\n+#undef BUILTIN_VSQN_HSDI\n+#undef BUILTIN_VSTRUCT\n+#undef CF0\n+#undef CF1\n+#undef CF2\n+#undef CF3\n+#undef CF4\n+#undef CF10\n+#undef VAR1\n+#undef VAR2\n+#undef VAR3\n+#undef VAR4\n+#undef VAR5\n+#undef VAR6\n+#undef VAR7\n+#undef VAR8\n+#undef VAR9\n+#undef VAR10\n+#undef VAR11\n+"}, {"sha": "43b5b931d77b8164698012a263ead8fb36e67eb8", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 206, "deletions": 192, "changes": 398, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=0ddec79f5276dd73306bb3f6d5cbcc462ae266f2", "patch": "@@ -18,250 +18,264 @@\n    along with GCC; see the file COPYING3.  If not see\n    <http://www.gnu.org/licenses/>.  */\n \n-/* In the list below, the BUILTIN_<ITERATOR> macros should\n-   correspond to the iterator used to construct the instruction's\n-   patterns in aarch64-simd.md.  A helpful idiom to follow when\n-   adding new builtins is to add a line for each pattern in the md\n-   file.  Thus, ADDP, which has one pattern defined for the VD_BHSI\n-   iterator, and one for DImode, has two entries below.  */\n+/* In the list below, the BUILTIN_<ITERATOR> macros expand to create\n+   builtins for each of the modes described by <ITERATOR>.  When adding\n+   new builtins to this list, a helpful idiom to follow is to add\n+   a line for each pattern in the md file.  Thus, ADDP, which has one\n+   pattern defined for the VD_BHSI iterator, and one for DImode, has two\n+   entries below.\n \n-  BUILTIN_VD_RE (CREATE, create)\n-  BUILTIN_VQ_S (GETLANE, get_lane_signed)\n-  BUILTIN_VDQ (GETLANE, get_lane_unsigned)\n-  BUILTIN_VDQF (GETLANE, get_lane)\n-  VAR1 (GETLANE, get_lane, di)\n-  BUILTIN_VDC (COMBINE, combine)\n-  BUILTIN_VB (BINOP, pmul)\n-  BUILTIN_VDQF (UNOP, sqrt)\n-  BUILTIN_VD_BHSI (BINOP, addp)\n-  VAR1 (UNOP, addp, di)\n+   Parameter 1 is the 'type' of the intrinsic.  This is used to\n+   describe the type modifiers (for example; unsigned) applied to\n+   each of the parameters to the intrinsic function.\n \n-  BUILTIN_VD_RE (REINTERP, reinterpretdi)\n-  BUILTIN_VDC (REINTERP, reinterpretv8qi)\n-  BUILTIN_VDC (REINTERP, reinterpretv4hi)\n-  BUILTIN_VDC (REINTERP, reinterpretv2si)\n-  BUILTIN_VDC (REINTERP, reinterpretv2sf)\n-  BUILTIN_VQ (REINTERP, reinterpretv16qi)\n-  BUILTIN_VQ (REINTERP, reinterpretv8hi)\n-  BUILTIN_VQ (REINTERP, reinterpretv4si)\n-  BUILTIN_VQ (REINTERP, reinterpretv4sf)\n-  BUILTIN_VQ (REINTERP, reinterpretv2di)\n-  BUILTIN_VQ (REINTERP, reinterpretv2df)\n+   Parameter 2 is the name of the intrinsic.  This is appended\n+   to `__builtin_aarch64_<name><mode>` to give the intrinsic name\n+   as exported to the front-ends.\n \n-  BUILTIN_VDQ_I (BINOP, dup_lane)\n-  BUILTIN_SDQ_I (BINOP, dup_lane)\n+   Parameter 3 describes how to map from the name to the CODE_FOR_\n+   macro holding the RTL pattern for the intrinsic.  This mapping is:\n+   0 - CODE_FOR_aarch64_<name><mode>\n+   1-9 - CODE_FOR_<name><mode><1-9>\n+   10 - CODE_FOR_<name><mode>.  */\n+\n+  BUILTIN_VD_RE (CREATE, create, 0)\n+  BUILTIN_VQ_S (GETLANE, get_lane_signed, 0)\n+  BUILTIN_VDQ (GETLANE, get_lane_unsigned, 0)\n+  BUILTIN_VDQF (GETLANE, get_lane, 0)\n+  VAR1 (GETLANE, get_lane, 0, di)\n+  BUILTIN_VDC (COMBINE, combine, 0)\n+  BUILTIN_VB (BINOP, pmul, 0)\n+  BUILTIN_VDQF (UNOP, sqrt, 2)\n+  BUILTIN_VD_BHSI (BINOP, addp, 0)\n+  VAR1 (UNOP, addp, 0, di)\n+\n+  BUILTIN_VD_RE (REINTERP, reinterpretdi, 0)\n+  BUILTIN_VDC (REINTERP, reinterpretv8qi, 0)\n+  BUILTIN_VDC (REINTERP, reinterpretv4hi, 0)\n+  BUILTIN_VDC (REINTERP, reinterpretv2si, 0)\n+  BUILTIN_VDC (REINTERP, reinterpretv2sf, 0)\n+  BUILTIN_VQ (REINTERP, reinterpretv16qi, 0)\n+  BUILTIN_VQ (REINTERP, reinterpretv8hi, 0)\n+  BUILTIN_VQ (REINTERP, reinterpretv4si, 0)\n+  BUILTIN_VQ (REINTERP, reinterpretv4sf, 0)\n+  BUILTIN_VQ (REINTERP, reinterpretv2di, 0)\n+  BUILTIN_VQ (REINTERP, reinterpretv2df, 0)\n+\n+  BUILTIN_VDQ_I (BINOP, dup_lane, 0)\n+  BUILTIN_SDQ_I (BINOP, dup_lane, 0)\n   /* Implemented by aarch64_<sur>q<r>shl<mode>.  */\n-  BUILTIN_VSDQ_I (BINOP, sqshl)\n-  BUILTIN_VSDQ_I (BINOP, uqshl)\n-  BUILTIN_VSDQ_I (BINOP, sqrshl)\n-  BUILTIN_VSDQ_I (BINOP, uqrshl)\n+  BUILTIN_VSDQ_I (BINOP, sqshl, 0)\n+  BUILTIN_VSDQ_I (BINOP, uqshl, 0)\n+  BUILTIN_VSDQ_I (BINOP, sqrshl, 0)\n+  BUILTIN_VSDQ_I (BINOP, uqrshl, 0)\n   /* Implemented by aarch64_<su_optab><optab><mode>.  */\n-  BUILTIN_VSDQ_I (BINOP, sqadd)\n-  BUILTIN_VSDQ_I (BINOP, uqadd)\n-  BUILTIN_VSDQ_I (BINOP, sqsub)\n-  BUILTIN_VSDQ_I (BINOP, uqsub)\n+  BUILTIN_VSDQ_I (BINOP, sqadd, 0)\n+  BUILTIN_VSDQ_I (BINOP, uqadd, 0)\n+  BUILTIN_VSDQ_I (BINOP, sqsub, 0)\n+  BUILTIN_VSDQ_I (BINOP, uqsub, 0)\n   /* Implemented by aarch64_<sur>qadd<mode>.  */\n-  BUILTIN_VSDQ_I (BINOP, suqadd)\n-  BUILTIN_VSDQ_I (BINOP, usqadd)\n+  BUILTIN_VSDQ_I (BINOP, suqadd, 0)\n+  BUILTIN_VSDQ_I (BINOP, usqadd, 0)\n \n   /* Implemented by aarch64_get_dreg<VSTRUCT:mode><VDC:mode>.  */\n-  BUILTIN_VDC (GETLANE, get_dregoi)\n-  BUILTIN_VDC (GETLANE, get_dregci)\n-  BUILTIN_VDC (GETLANE, get_dregxi)\n+  BUILTIN_VDC (GETLANE, get_dregoi, 0)\n+  BUILTIN_VDC (GETLANE, get_dregci, 0)\n+  BUILTIN_VDC (GETLANE, get_dregxi, 0)\n   /* Implemented by aarch64_get_qreg<VSTRUCT:mode><VQ:mode>.  */\n-  BUILTIN_VQ (GETLANE, get_qregoi)\n-  BUILTIN_VQ (GETLANE, get_qregci)\n-  BUILTIN_VQ (GETLANE, get_qregxi)\n+  BUILTIN_VQ (GETLANE, get_qregoi, 0)\n+  BUILTIN_VQ (GETLANE, get_qregci, 0)\n+  BUILTIN_VQ (GETLANE, get_qregxi, 0)\n   /* Implemented by aarch64_set_qreg<VSTRUCT:mode><VQ:mode>.  */\n-  BUILTIN_VQ (SETLANE, set_qregoi)\n-  BUILTIN_VQ (SETLANE, set_qregci)\n-  BUILTIN_VQ (SETLANE, set_qregxi)\n+  BUILTIN_VQ (SETLANE, set_qregoi, 0)\n+  BUILTIN_VQ (SETLANE, set_qregci, 0)\n+  BUILTIN_VQ (SETLANE, set_qregxi, 0)\n   /* Implemented by aarch64_ld<VSTRUCT:nregs><VDC:mode>.  */\n-  BUILTIN_VDC (LOADSTRUCT, ld2)\n-  BUILTIN_VDC (LOADSTRUCT, ld3)\n-  BUILTIN_VDC (LOADSTRUCT, ld4)\n+  BUILTIN_VDC (LOADSTRUCT, ld2, 0)\n+  BUILTIN_VDC (LOADSTRUCT, ld3, 0)\n+  BUILTIN_VDC (LOADSTRUCT, ld4, 0)\n   /* Implemented by aarch64_ld<VSTRUCT:nregs><VQ:mode>.  */\n-  BUILTIN_VQ (LOADSTRUCT, ld2)\n-  BUILTIN_VQ (LOADSTRUCT, ld3)\n-  BUILTIN_VQ (LOADSTRUCT, ld4)\n+  BUILTIN_VQ (LOADSTRUCT, ld2, 0)\n+  BUILTIN_VQ (LOADSTRUCT, ld3, 0)\n+  BUILTIN_VQ (LOADSTRUCT, ld4, 0)\n   /* Implemented by aarch64_st<VSTRUCT:nregs><VDC:mode>.  */\n-  BUILTIN_VDC (STORESTRUCT, st2)\n-  BUILTIN_VDC (STORESTRUCT, st3)\n-  BUILTIN_VDC (STORESTRUCT, st4)\n+  BUILTIN_VDC (STORESTRUCT, st2, 0)\n+  BUILTIN_VDC (STORESTRUCT, st3, 0)\n+  BUILTIN_VDC (STORESTRUCT, st4, 0)\n   /* Implemented by aarch64_st<VSTRUCT:nregs><VQ:mode>.  */\n-  BUILTIN_VQ (STORESTRUCT, st2)\n-  BUILTIN_VQ (STORESTRUCT, st3)\n-  BUILTIN_VQ (STORESTRUCT, st4)\n+  BUILTIN_VQ (STORESTRUCT, st2, 0)\n+  BUILTIN_VQ (STORESTRUCT, st3, 0)\n+  BUILTIN_VQ (STORESTRUCT, st4, 0)\n \n-  BUILTIN_VQW (BINOP, saddl2)\n-  BUILTIN_VQW (BINOP, uaddl2)\n-  BUILTIN_VQW (BINOP, ssubl2)\n-  BUILTIN_VQW (BINOP, usubl2)\n-  BUILTIN_VQW (BINOP, saddw2)\n-  BUILTIN_VQW (BINOP, uaddw2)\n-  BUILTIN_VQW (BINOP, ssubw2)\n-  BUILTIN_VQW (BINOP, usubw2)\n+  BUILTIN_VQW (BINOP, saddl2, 0)\n+  BUILTIN_VQW (BINOP, uaddl2, 0)\n+  BUILTIN_VQW (BINOP, ssubl2, 0)\n+  BUILTIN_VQW (BINOP, usubl2, 0)\n+  BUILTIN_VQW (BINOP, saddw2, 0)\n+  BUILTIN_VQW (BINOP, uaddw2, 0)\n+  BUILTIN_VQW (BINOP, ssubw2, 0)\n+  BUILTIN_VQW (BINOP, usubw2, 0)\n   /* Implemented by aarch64_<ANY_EXTEND:su><ADDSUB:optab>l<mode>.  */\n-  BUILTIN_VDW (BINOP, saddl)\n-  BUILTIN_VDW (BINOP, uaddl)\n-  BUILTIN_VDW (BINOP, ssubl)\n-  BUILTIN_VDW (BINOP, usubl)\n+  BUILTIN_VDW (BINOP, saddl, 0)\n+  BUILTIN_VDW (BINOP, uaddl, 0)\n+  BUILTIN_VDW (BINOP, ssubl, 0)\n+  BUILTIN_VDW (BINOP, usubl, 0)\n   /* Implemented by aarch64_<ANY_EXTEND:su><ADDSUB:optab>w<mode>.  */\n-  BUILTIN_VDW (BINOP, saddw)\n-  BUILTIN_VDW (BINOP, uaddw)\n-  BUILTIN_VDW (BINOP, ssubw)\n-  BUILTIN_VDW (BINOP, usubw)\n+  BUILTIN_VDW (BINOP, saddw, 0)\n+  BUILTIN_VDW (BINOP, uaddw, 0)\n+  BUILTIN_VDW (BINOP, ssubw, 0)\n+  BUILTIN_VDW (BINOP, usubw, 0)\n   /* Implemented by aarch64_<sur>h<addsub><mode>.  */\n-  BUILTIN_VQ_S (BINOP, shadd)\n-  BUILTIN_VQ_S (BINOP, uhadd)\n-  BUILTIN_VQ_S (BINOP, srhadd)\n-  BUILTIN_VQ_S (BINOP, urhadd)\n+  BUILTIN_VQ_S (BINOP, shadd, 0)\n+  BUILTIN_VQ_S (BINOP, uhadd, 0)\n+  BUILTIN_VQ_S (BINOP, srhadd, 0)\n+  BUILTIN_VQ_S (BINOP, urhadd, 0)\n   /* Implemented by aarch64_<sur><addsub>hn<mode>.  */\n-  BUILTIN_VQN (BINOP, addhn)\n-  BUILTIN_VQN (BINOP, raddhn)\n+  BUILTIN_VQN (BINOP, addhn, 0)\n+  BUILTIN_VQN (BINOP, raddhn, 0)\n   /* Implemented by aarch64_<sur><addsub>hn2<mode>.  */\n-  BUILTIN_VQN (TERNOP, addhn2)\n-  BUILTIN_VQN (TERNOP, raddhn2)\n+  BUILTIN_VQN (TERNOP, addhn2, 0)\n+  BUILTIN_VQN (TERNOP, raddhn2, 0)\n \n-  BUILTIN_VSQN_HSDI (UNOP, sqmovun)\n+  BUILTIN_VSQN_HSDI (UNOP, sqmovun, 0)\n   /* Implemented by aarch64_<sur>qmovn<mode>.  */\n-  BUILTIN_VSQN_HSDI (UNOP, sqmovn)\n-  BUILTIN_VSQN_HSDI (UNOP, uqmovn)\n+  BUILTIN_VSQN_HSDI (UNOP, sqmovn, 0)\n+  BUILTIN_VSQN_HSDI (UNOP, uqmovn, 0)\n   /* Implemented by aarch64_s<optab><mode>.  */\n-  BUILTIN_VSDQ_I_BHSI (UNOP, sqabs)\n-  BUILTIN_VSDQ_I_BHSI (UNOP, sqneg)\n+  BUILTIN_VSDQ_I_BHSI (UNOP, sqabs, 0)\n+  BUILTIN_VSDQ_I_BHSI (UNOP, sqneg, 0)\n \n-  BUILTIN_VSD_HSI (QUADOP, sqdmlal_lane)\n-  BUILTIN_VSD_HSI (QUADOP, sqdmlsl_lane)\n-  BUILTIN_VSD_HSI (QUADOP, sqdmlal_laneq)\n-  BUILTIN_VSD_HSI (QUADOP, sqdmlsl_laneq)\n-  BUILTIN_VQ_HSI (TERNOP, sqdmlal2)\n-  BUILTIN_VQ_HSI (TERNOP, sqdmlsl2)\n-  BUILTIN_VQ_HSI (QUADOP, sqdmlal2_lane)\n-  BUILTIN_VQ_HSI (QUADOP, sqdmlsl2_lane)\n-  BUILTIN_VQ_HSI (QUADOP, sqdmlal2_laneq)\n-  BUILTIN_VQ_HSI (QUADOP, sqdmlsl2_laneq)\n-  BUILTIN_VQ_HSI (TERNOP, sqdmlal2_n)\n-  BUILTIN_VQ_HSI (TERNOP, sqdmlsl2_n)\n+  BUILTIN_VSD_HSI (QUADOP, sqdmlal_lane, 0)\n+  BUILTIN_VSD_HSI (QUADOP, sqdmlsl_lane, 0)\n+  BUILTIN_VSD_HSI (QUADOP, sqdmlal_laneq, 0)\n+  BUILTIN_VSD_HSI (QUADOP, sqdmlsl_laneq, 0)\n+  BUILTIN_VQ_HSI (TERNOP, sqdmlal2, 0)\n+  BUILTIN_VQ_HSI (TERNOP, sqdmlsl2, 0)\n+  BUILTIN_VQ_HSI (QUADOP, sqdmlal2_lane, 0)\n+  BUILTIN_VQ_HSI (QUADOP, sqdmlsl2_lane, 0)\n+  BUILTIN_VQ_HSI (QUADOP, sqdmlal2_laneq, 0)\n+  BUILTIN_VQ_HSI (QUADOP, sqdmlsl2_laneq, 0)\n+  BUILTIN_VQ_HSI (TERNOP, sqdmlal2_n, 0)\n+  BUILTIN_VQ_HSI (TERNOP, sqdmlsl2_n, 0)\n   /* Implemented by aarch64_sqdml<SBINQOPS:as>l<mode>.  */\n-  BUILTIN_VSD_HSI (TERNOP, sqdmlal)\n-  BUILTIN_VSD_HSI (TERNOP, sqdmlsl)\n+  BUILTIN_VSD_HSI (TERNOP, sqdmlal, 0)\n+  BUILTIN_VSD_HSI (TERNOP, sqdmlsl, 0)\n   /* Implemented by aarch64_sqdml<SBINQOPS:as>l_n<mode>.  */\n-  BUILTIN_VD_HSI (TERNOP, sqdmlal_n)\n-  BUILTIN_VD_HSI (TERNOP, sqdmlsl_n)\n+  BUILTIN_VD_HSI (TERNOP, sqdmlal_n, 0)\n+  BUILTIN_VD_HSI (TERNOP, sqdmlsl_n, 0)\n \n-  BUILTIN_VSD_HSI (BINOP, sqdmull)\n-  BUILTIN_VSD_HSI (TERNOP, sqdmull_lane)\n-  BUILTIN_VD_HSI (TERNOP, sqdmull_laneq)\n-  BUILTIN_VD_HSI (BINOP, sqdmull_n)\n-  BUILTIN_VQ_HSI (BINOP, sqdmull2)\n-  BUILTIN_VQ_HSI (TERNOP, sqdmull2_lane)\n-  BUILTIN_VQ_HSI (TERNOP, sqdmull2_laneq)\n-  BUILTIN_VQ_HSI (BINOP, sqdmull2_n)\n+  BUILTIN_VSD_HSI (BINOP, sqdmull, 0)\n+  BUILTIN_VSD_HSI (TERNOP, sqdmull_lane, 0)\n+  BUILTIN_VD_HSI (TERNOP, sqdmull_laneq, 0)\n+  BUILTIN_VD_HSI (BINOP, sqdmull_n, 0)\n+  BUILTIN_VQ_HSI (BINOP, sqdmull2, 0)\n+  BUILTIN_VQ_HSI (TERNOP, sqdmull2_lane, 0)\n+  BUILTIN_VQ_HSI (TERNOP, sqdmull2_laneq, 0)\n+  BUILTIN_VQ_HSI (BINOP, sqdmull2_n, 0)\n   /* Implemented by aarch64_sq<r>dmulh<mode>.  */\n-  BUILTIN_VSDQ_HSI (BINOP, sqdmulh)\n-  BUILTIN_VSDQ_HSI (BINOP, sqrdmulh)\n+  BUILTIN_VSDQ_HSI (BINOP, sqdmulh, 0)\n+  BUILTIN_VSDQ_HSI (BINOP, sqrdmulh, 0)\n   /* Implemented by aarch64_sq<r>dmulh_lane<q><mode>.  */\n-  BUILTIN_VDQHS (TERNOP, sqdmulh_lane)\n-  BUILTIN_VDQHS (TERNOP, sqdmulh_laneq)\n-  BUILTIN_VDQHS (TERNOP, sqrdmulh_lane)\n-  BUILTIN_VDQHS (TERNOP, sqrdmulh_laneq)\n-  BUILTIN_SD_HSI (TERNOP, sqdmulh_lane)\n-  BUILTIN_SD_HSI (TERNOP, sqrdmulh_lane)\n+  BUILTIN_VDQHS (TERNOP, sqdmulh_lane, 0)\n+  BUILTIN_VDQHS (TERNOP, sqdmulh_laneq, 0)\n+  BUILTIN_VDQHS (TERNOP, sqrdmulh_lane, 0)\n+  BUILTIN_VDQHS (TERNOP, sqrdmulh_laneq, 0)\n+  BUILTIN_SD_HSI (TERNOP, sqdmulh_lane, 0)\n+  BUILTIN_SD_HSI (TERNOP, sqrdmulh_lane, 0)\n \n-  BUILTIN_VSDQ_I_DI (BINOP, sshl_n)\n-  BUILTIN_VSDQ_I_DI (BINOP, ushl_n)\n+  BUILTIN_VSDQ_I_DI (BINOP, ashl, 3)\n   /* Implemented by aarch64_<sur>shl<mode>.  */\n-  BUILTIN_VSDQ_I_DI (BINOP, sshl)\n-  BUILTIN_VSDQ_I_DI (BINOP, ushl)\n-  BUILTIN_VSDQ_I_DI (BINOP, srshl)\n-  BUILTIN_VSDQ_I_DI (BINOP, urshl)\n+  BUILTIN_VSDQ_I_DI (BINOP, sshl, 0)\n+  BUILTIN_VSDQ_I_DI (BINOP, ushl, 0)\n+  BUILTIN_VSDQ_I_DI (BINOP, srshl, 0)\n+  BUILTIN_VSDQ_I_DI (BINOP, urshl, 0)\n \n-  BUILTIN_VSDQ_I_DI (SHIFTIMM, sshr_n)\n-  BUILTIN_VSDQ_I_DI (SHIFTIMM, ushr_n)\n+  BUILTIN_VSDQ_I_DI (SHIFTIMM, ashr, 3)\n+  BUILTIN_VSDQ_I_DI (SHIFTIMM, lshr, 3)\n   /* Implemented by aarch64_<sur>shr_n<mode>.  */\n-  BUILTIN_VSDQ_I_DI (SHIFTIMM, srshr_n)\n-  BUILTIN_VSDQ_I_DI (SHIFTIMM, urshr_n)\n+  BUILTIN_VSDQ_I_DI (SHIFTIMM, srshr_n, 0)\n+  BUILTIN_VSDQ_I_DI (SHIFTIMM, urshr_n, 0)\n   /* Implemented by aarch64_<sur>sra_n<mode>.  */\n-  BUILTIN_VSDQ_I_DI (SHIFTACC, ssra_n)\n-  BUILTIN_VSDQ_I_DI (SHIFTACC, usra_n)\n-  BUILTIN_VSDQ_I_DI (SHIFTACC, srsra_n)\n-  BUILTIN_VSDQ_I_DI (SHIFTACC, ursra_n)\n+  BUILTIN_VSDQ_I_DI (SHIFTACC, ssra_n, 0)\n+  BUILTIN_VSDQ_I_DI (SHIFTACC, usra_n, 0)\n+  BUILTIN_VSDQ_I_DI (SHIFTACC, srsra_n, 0)\n+  BUILTIN_VSDQ_I_DI (SHIFTACC, ursra_n, 0)\n   /* Implemented by aarch64_<sur>shll_n<mode>.  */\n-  BUILTIN_VDW (SHIFTIMM, sshll_n)\n-  BUILTIN_VDW (SHIFTIMM, ushll_n)\n+  BUILTIN_VDW (SHIFTIMM, sshll_n, 0)\n+  BUILTIN_VDW (SHIFTIMM, ushll_n, 0)\n   /* Implemented by aarch64_<sur>shll2_n<mode>.  */\n-  BUILTIN_VQW (SHIFTIMM, sshll2_n)\n-  BUILTIN_VQW (SHIFTIMM, ushll2_n)\n+  BUILTIN_VQW (SHIFTIMM, sshll2_n, 0)\n+  BUILTIN_VQW (SHIFTIMM, ushll2_n, 0)\n   /* Implemented by aarch64_<sur>q<r>shr<u>n_n<mode>.  */\n-  BUILTIN_VSQN_HSDI (SHIFTIMM, sqshrun_n)\n-  BUILTIN_VSQN_HSDI (SHIFTIMM, sqrshrun_n)\n-  BUILTIN_VSQN_HSDI (SHIFTIMM, sqshrn_n)\n-  BUILTIN_VSQN_HSDI (SHIFTIMM, uqshrn_n)\n-  BUILTIN_VSQN_HSDI (SHIFTIMM, sqrshrn_n)\n-  BUILTIN_VSQN_HSDI (SHIFTIMM, uqrshrn_n)\n+  BUILTIN_VSQN_HSDI (SHIFTIMM, sqshrun_n, 0)\n+  BUILTIN_VSQN_HSDI (SHIFTIMM, sqrshrun_n, 0)\n+  BUILTIN_VSQN_HSDI (SHIFTIMM, sqshrn_n, 0)\n+  BUILTIN_VSQN_HSDI (SHIFTIMM, uqshrn_n, 0)\n+  BUILTIN_VSQN_HSDI (SHIFTIMM, sqrshrn_n, 0)\n+  BUILTIN_VSQN_HSDI (SHIFTIMM, uqrshrn_n, 0)\n   /* Implemented by aarch64_<sur>s<lr>i_n<mode>.  */\n-  BUILTIN_VSDQ_I_DI (SHIFTINSERT, ssri_n)\n-  BUILTIN_VSDQ_I_DI (SHIFTINSERT, usri_n)\n-  BUILTIN_VSDQ_I_DI (SHIFTINSERT, ssli_n)\n-  BUILTIN_VSDQ_I_DI (SHIFTINSERT, usli_n)\n+  BUILTIN_VSDQ_I_DI (SHIFTINSERT, ssri_n, 0)\n+  BUILTIN_VSDQ_I_DI (SHIFTINSERT, usri_n, 0)\n+  BUILTIN_VSDQ_I_DI (SHIFTINSERT, ssli_n, 0)\n+  BUILTIN_VSDQ_I_DI (SHIFTINSERT, usli_n, 0)\n   /* Implemented by aarch64_<sur>qshl<u>_n<mode>.  */\n-  BUILTIN_VSDQ_I (SHIFTIMM, sqshlu_n)\n-  BUILTIN_VSDQ_I (SHIFTIMM, sqshl_n)\n-  BUILTIN_VSDQ_I (SHIFTIMM, uqshl_n)\n+  BUILTIN_VSDQ_I (SHIFTIMM, sqshlu_n, 0)\n+  BUILTIN_VSDQ_I (SHIFTIMM, sqshl_n, 0)\n+  BUILTIN_VSDQ_I (SHIFTIMM, uqshl_n, 0)\n \n   /* Implemented by aarch64_cm<cmp><mode>.  */\n-  BUILTIN_VSDQ_I_DI (BINOP, cmeq)\n-  BUILTIN_VSDQ_I_DI (BINOP, cmge)\n-  BUILTIN_VSDQ_I_DI (BINOP, cmgt)\n-  BUILTIN_VSDQ_I_DI (BINOP, cmle)\n-  BUILTIN_VSDQ_I_DI (BINOP, cmlt)\n+  BUILTIN_VSDQ_I_DI (BINOP, cmeq, 0)\n+  BUILTIN_VSDQ_I_DI (BINOP, cmge, 0)\n+  BUILTIN_VSDQ_I_DI (BINOP, cmgt, 0)\n+  BUILTIN_VSDQ_I_DI (BINOP, cmle, 0)\n+  BUILTIN_VSDQ_I_DI (BINOP, cmlt, 0)\n   /* Implemented by aarch64_cm<cmp><mode>.  */\n-  BUILTIN_VSDQ_I_DI (BINOP, cmhs)\n-  BUILTIN_VSDQ_I_DI (BINOP, cmhi)\n-  BUILTIN_VSDQ_I_DI (BINOP, cmtst)\n+  BUILTIN_VSDQ_I_DI (BINOP, cmhs, 0)\n+  BUILTIN_VSDQ_I_DI (BINOP, cmhi, 0)\n+  BUILTIN_VSDQ_I_DI (BINOP, cmtst, 0)\n \n   /* Implemented by aarch64_<fmaxmin><mode>.  */\n-  BUILTIN_VDQF (BINOP, fmax)\n-  BUILTIN_VDQF (BINOP, fmin)\n-  /* Implemented by aarch64_<maxmin><mode>.  */\n-  BUILTIN_VDQ_BHSI (BINOP, smax)\n-  BUILTIN_VDQ_BHSI (BINOP, smin)\n-  BUILTIN_VDQ_BHSI (BINOP, umax)\n-  BUILTIN_VDQ_BHSI (BINOP, umin)\n+  BUILTIN_VDQF (BINOP, fmax, 0)\n+  BUILTIN_VDQF (BINOP, fmin, 0)\n+\n+  /* Implemented by <maxmin><mode>3.  */\n+  BUILTIN_VDQ_BHSI (BINOP, smax, 3)\n+  BUILTIN_VDQ_BHSI (BINOP, smin, 3)\n+  BUILTIN_VDQ_BHSI (BINOP, umax, 3)\n+  BUILTIN_VDQ_BHSI (BINOP, umin, 3)\n \n   /* Implemented by aarch64_frint<frint_suffix><mode>.  */\n-  BUILTIN_VDQF (UNOP, frintz)\n-  BUILTIN_VDQF (UNOP, frintp)\n-  BUILTIN_VDQF (UNOP, frintm)\n-  BUILTIN_VDQF (UNOP, frinti)\n-  BUILTIN_VDQF (UNOP, frintx)\n-  BUILTIN_VDQF (UNOP, frinta)\n+  BUILTIN_VDQF (UNOP, frintz, 0)\n+  BUILTIN_VDQF (UNOP, frintp, 0)\n+  BUILTIN_VDQF (UNOP, frintm, 0)\n+  BUILTIN_VDQF (UNOP, frinti, 0)\n+  BUILTIN_VDQF (UNOP, frintx, 0)\n+  BUILTIN_VDQF (UNOP, frinta, 0)\n \n   /* Implemented by aarch64_fcvt<frint_suffix><su><mode>.  */\n-  BUILTIN_VDQF (UNOP, fcvtzs)\n-  BUILTIN_VDQF (UNOP, fcvtzu)\n-  BUILTIN_VDQF (UNOP, fcvtas)\n-  BUILTIN_VDQF (UNOP, fcvtau)\n-  BUILTIN_VDQF (UNOP, fcvtps)\n-  BUILTIN_VDQF (UNOP, fcvtpu)\n-  BUILTIN_VDQF (UNOP, fcvtms)\n-  BUILTIN_VDQF (UNOP, fcvtmu)\n+  BUILTIN_VDQF (UNOP, fcvtzs, 0)\n+  BUILTIN_VDQF (UNOP, fcvtzu, 0)\n+  BUILTIN_VDQF (UNOP, fcvtas, 0)\n+  BUILTIN_VDQF (UNOP, fcvtau, 0)\n+  BUILTIN_VDQF (UNOP, fcvtps, 0)\n+  BUILTIN_VDQF (UNOP, fcvtpu, 0)\n+  BUILTIN_VDQF (UNOP, fcvtms, 0)\n+  BUILTIN_VDQF (UNOP, fcvtmu, 0)\n \n   /* Implemented by\n      aarch64_<PERMUTE:perm_insn><PERMUTE:perm_hilo><mode>.  */\n-  BUILTIN_VALL (BINOP, zip1)\n-  BUILTIN_VALL (BINOP, zip2)\n-  BUILTIN_VALL (BINOP, uzp1)\n-  BUILTIN_VALL (BINOP, uzp2)\n-  BUILTIN_VALL (BINOP, trn1)\n-  BUILTIN_VALL (BINOP, trn2)\n+  BUILTIN_VALL (BINOP, zip1, 0)\n+  BUILTIN_VALL (BINOP, zip2, 0)\n+  BUILTIN_VALL (BINOP, uzp1, 0)\n+  BUILTIN_VALL (BINOP, uzp2, 0)\n+  BUILTIN_VALL (BINOP, trn1, 0)\n+  BUILTIN_VALL (BINOP, trn2, 0)\n \n   /* Implemented by\n      aarch64_frecp<FRECP:frecp_suffix><mode>.  */\n-  BUILTIN_GPF (UNOP, frecpe)\n-  BUILTIN_GPF (BINOP, frecps)\n-  BUILTIN_GPF (UNOP, frecpx)\n+  BUILTIN_GPF (UNOP, frecpe, 0)\n+  BUILTIN_GPF (BINOP, frecps, 0)\n+  BUILTIN_GPF (UNOP, frecpx, 0)\n \n-  BUILTIN_VDQF (UNOP, frecpe)\n-  BUILTIN_VDQF (BINOP, frecps)\n+  BUILTIN_VDQF (UNOP, frecpe, 0)\n+  BUILTIN_VDQF (BINOP, frecps, 0)"}, {"sha": "9b42365b50b5f0e06d429614b2a3218bb634a3c6", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 0, "deletions": 67, "changes": 67, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=0ddec79f5276dd73306bb3f6d5cbcc462ae266f2", "patch": "@@ -2897,28 +2897,6 @@\n    (set_attr \"simd_mode\" \"<MODE>\")]\n )\n \n-;; vshl_n\n-\n-(define_expand \"aarch64_sshl_n<mode>\"\n-  [(match_operand:VSDQ_I_DI 0 \"register_operand\" \"=w\")\n-   (match_operand:VSDQ_I_DI 1 \"register_operand\" \"w\")\n-   (match_operand:SI 2 \"immediate_operand\" \"i\")]\n-  \"TARGET_SIMD\"\n-{\n-  emit_insn (gen_ashl<mode>3 (operands[0], operands[1], operands[2]));\n-  DONE;\n-})\n-\n-(define_expand \"aarch64_ushl_n<mode>\"\n-  [(match_operand:VSDQ_I_DI 0 \"register_operand\" \"=w\")\n-   (match_operand:VSDQ_I_DI 1 \"register_operand\" \"w\")\n-   (match_operand:SI 2 \"immediate_operand\" \"i\")]\n-  \"TARGET_SIMD\"\n-{\n-  emit_insn (gen_ashl<mode>3 (operands[0], operands[1], operands[2]));\n-  DONE;\n-})\n-\n ;; vshll_n\n \n (define_insn \"aarch64_<sur>shll_n<mode>\"\n@@ -2963,28 +2941,6 @@\n    (set_attr \"simd_mode\" \"<MODE>\")]\n )\n \n-;; vshr_n\n-\n-(define_expand \"aarch64_sshr_n<mode>\"\n-  [(match_operand:VSDQ_I_DI 0 \"register_operand\" \"=w\")\n-   (match_operand:VSDQ_I_DI 1 \"register_operand\" \"w\")\n-   (match_operand:SI 2 \"immediate_operand\" \"i\")]\n-  \"TARGET_SIMD\"\n-{\n-  emit_insn (gen_ashr<mode>3 (operands[0], operands[1], operands[2]));\n-  DONE;\n-})\n-\n-(define_expand \"aarch64_ushr_n<mode>\"\n-  [(match_operand:VSDQ_I_DI 0 \"register_operand\" \"=w\")\n-   (match_operand:VSDQ_I_DI 1 \"register_operand\" \"w\")\n-   (match_operand:SI 2 \"immediate_operand\" \"i\")]\n-  \"TARGET_SIMD\"\n-{\n-  emit_insn (gen_lshr<mode>3 (operands[0], operands[1], operands[2]));\n-  DONE;\n-})\n-\n ;; vrshr_n\n \n (define_insn \"aarch64_<sur>shr_n<mode>\"\n@@ -3141,19 +3097,6 @@\n    (set_attr \"simd_mode\" \"DI\")]\n )\n \n-;; v(max|min)\n-\n-(define_expand \"aarch64_<maxmin><mode>\"\n- [(set (match_operand:VDQ_BHSI 0 \"register_operand\" \"=w\")\n-       (MAXMIN:VDQ_BHSI (match_operand:VDQ_BHSI 1 \"register_operand\" \"w\")\n-\t\t\t(match_operand:VDQ_BHSI 2 \"register_operand\" \"w\")))]\n- \"TARGET_SIMD\"\n-{\n-  emit_insn (gen_<maxmin><mode>3 (operands[0], operands[1], operands[2]));\n-  DONE;\n-})\n-\n-\n (define_insn \"aarch64_<fmaxmin><mode>\"\n   [(set (match_operand:VDQF 0 \"register_operand\" \"=w\")\n         (unspec:VDQF [(match_operand:VDQF 1 \"register_operand\" \"w\")\n@@ -3176,16 +3119,6 @@\n    (set_attr \"simd_mode\" \"<MODE>\")]\n )\n \n-(define_expand \"aarch64_sqrt<mode>\"\n-  [(match_operand:VDQF 0 \"register_operand\" \"=w\")\n-   (match_operand:VDQF 1 \"register_operand\" \"w\")]\n-  \"TARGET_SIMD\"\n-{\n-  emit_insn (gen_sqrt<mode>2 (operands[0], operands[1]));\n-  DONE;\n-})\n-\n-\n ;; Patterns for vector struct loads and stores.\n \n (define_insn \"vec_load_lanesoi<mode>\""}, {"sha": "5e25c77811180d14e8eff0a4d16154071072c653", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 36, "deletions": 36, "changes": 72, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0ddec79f5276dd73306bb3f6d5cbcc462ae266f2/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=0ddec79f5276dd73306bb3f6d5cbcc462ae266f2", "patch": "@@ -23404,109 +23404,109 @@ vrsrad_n_u64 (uint64x1_t __a, uint64x1_t __b, const int __c)\n __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n vshl_n_s8 (int8x8_t __a, const int __b)\n {\n-  return (int8x8_t) __builtin_aarch64_sshl_nv8qi (__a, __b);\n+  return (int8x8_t) __builtin_aarch64_ashlv8qi (__a, __b);\n }\n \n __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n vshl_n_s16 (int16x4_t __a, const int __b)\n {\n-  return (int16x4_t) __builtin_aarch64_sshl_nv4hi (__a, __b);\n+  return (int16x4_t) __builtin_aarch64_ashlv4hi (__a, __b);\n }\n \n __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n vshl_n_s32 (int32x2_t __a, const int __b)\n {\n-  return (int32x2_t) __builtin_aarch64_sshl_nv2si (__a, __b);\n+  return (int32x2_t) __builtin_aarch64_ashlv2si (__a, __b);\n }\n \n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vshl_n_s64 (int64x1_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sshl_ndi (__a, __b);\n+  return (int64x1_t) __builtin_aarch64_ashldi (__a, __b);\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vshl_n_u8 (uint8x8_t __a, const int __b)\n {\n-  return (uint8x8_t) __builtin_aarch64_ushl_nv8qi ((int8x8_t) __a, __b);\n+  return (uint8x8_t) __builtin_aarch64_ashlv8qi ((int8x8_t) __a, __b);\n }\n \n __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n vshl_n_u16 (uint16x4_t __a, const int __b)\n {\n-  return (uint16x4_t) __builtin_aarch64_ushl_nv4hi ((int16x4_t) __a, __b);\n+  return (uint16x4_t) __builtin_aarch64_ashlv4hi ((int16x4_t) __a, __b);\n }\n \n __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n vshl_n_u32 (uint32x2_t __a, const int __b)\n {\n-  return (uint32x2_t) __builtin_aarch64_ushl_nv2si ((int32x2_t) __a, __b);\n+  return (uint32x2_t) __builtin_aarch64_ashlv2si ((int32x2_t) __a, __b);\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vshl_n_u64 (uint64x1_t __a, const int __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_ushl_ndi ((int64x1_t) __a, __b);\n+  return (uint64x1_t) __builtin_aarch64_ashldi ((int64x1_t) __a, __b);\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n vshlq_n_s8 (int8x16_t __a, const int __b)\n {\n-  return (int8x16_t) __builtin_aarch64_sshl_nv16qi (__a, __b);\n+  return (int8x16_t) __builtin_aarch64_ashlv16qi (__a, __b);\n }\n \n __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n vshlq_n_s16 (int16x8_t __a, const int __b)\n {\n-  return (int16x8_t) __builtin_aarch64_sshl_nv8hi (__a, __b);\n+  return (int16x8_t) __builtin_aarch64_ashlv8hi (__a, __b);\n }\n \n __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n vshlq_n_s32 (int32x4_t __a, const int __b)\n {\n-  return (int32x4_t) __builtin_aarch64_sshl_nv4si (__a, __b);\n+  return (int32x4_t) __builtin_aarch64_ashlv4si (__a, __b);\n }\n \n __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n vshlq_n_s64 (int64x2_t __a, const int __b)\n {\n-  return (int64x2_t) __builtin_aarch64_sshl_nv2di (__a, __b);\n+  return (int64x2_t) __builtin_aarch64_ashlv2di (__a, __b);\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n vshlq_n_u8 (uint8x16_t __a, const int __b)\n {\n-  return (uint8x16_t) __builtin_aarch64_ushl_nv16qi ((int8x16_t) __a, __b);\n+  return (uint8x16_t) __builtin_aarch64_ashlv16qi ((int8x16_t) __a, __b);\n }\n \n __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n vshlq_n_u16 (uint16x8_t __a, const int __b)\n {\n-  return (uint16x8_t) __builtin_aarch64_ushl_nv8hi ((int16x8_t) __a, __b);\n+  return (uint16x8_t) __builtin_aarch64_ashlv8hi ((int16x8_t) __a, __b);\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n vshlq_n_u32 (uint32x4_t __a, const int __b)\n {\n-  return (uint32x4_t) __builtin_aarch64_ushl_nv4si ((int32x4_t) __a, __b);\n+  return (uint32x4_t) __builtin_aarch64_ashlv4si ((int32x4_t) __a, __b);\n }\n \n __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n vshlq_n_u64 (uint64x2_t __a, const int __b)\n {\n-  return (uint64x2_t) __builtin_aarch64_ushl_nv2di ((int64x2_t) __a, __b);\n+  return (uint64x2_t) __builtin_aarch64_ashlv2di ((int64x2_t) __a, __b);\n }\n \n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vshld_n_s64 (int64x1_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sshl_ndi (__a, __b);\n+  return (int64x1_t) __builtin_aarch64_ashldi (__a, __b);\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vshld_n_u64 (uint64x1_t __a, const int __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_ushl_ndi (__a, __b);\n+  return (uint64x1_t) __builtin_aarch64_ashldi (__a, __b);\n }\n \n __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n@@ -23694,109 +23694,109 @@ vshll_n_u32 (uint32x2_t __a, const int __b)\n __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n vshr_n_s8 (int8x8_t __a, const int __b)\n {\n-  return (int8x8_t) __builtin_aarch64_sshr_nv8qi (__a, __b);\n+  return (int8x8_t) __builtin_aarch64_ashrv8qi (__a, __b);\n }\n \n __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n vshr_n_s16 (int16x4_t __a, const int __b)\n {\n-  return (int16x4_t) __builtin_aarch64_sshr_nv4hi (__a, __b);\n+  return (int16x4_t) __builtin_aarch64_ashrv4hi (__a, __b);\n }\n \n __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n vshr_n_s32 (int32x2_t __a, const int __b)\n {\n-  return (int32x2_t) __builtin_aarch64_sshr_nv2si (__a, __b);\n+  return (int32x2_t) __builtin_aarch64_ashrv2si (__a, __b);\n }\n \n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vshr_n_s64 (int64x1_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sshr_ndi (__a, __b);\n+  return (int64x1_t) __builtin_aarch64_ashrdi (__a, __b);\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vshr_n_u8 (uint8x8_t __a, const int __b)\n {\n-  return (uint8x8_t) __builtin_aarch64_ushr_nv8qi ((int8x8_t) __a, __b);\n+  return (uint8x8_t) __builtin_aarch64_lshrv8qi ((int8x8_t) __a, __b);\n }\n \n __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n vshr_n_u16 (uint16x4_t __a, const int __b)\n {\n-  return (uint16x4_t) __builtin_aarch64_ushr_nv4hi ((int16x4_t) __a, __b);\n+  return (uint16x4_t) __builtin_aarch64_lshrv4hi ((int16x4_t) __a, __b);\n }\n \n __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n vshr_n_u32 (uint32x2_t __a, const int __b)\n {\n-  return (uint32x2_t) __builtin_aarch64_ushr_nv2si ((int32x2_t) __a, __b);\n+  return (uint32x2_t) __builtin_aarch64_lshrv2si ((int32x2_t) __a, __b);\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vshr_n_u64 (uint64x1_t __a, const int __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_ushr_ndi ((int64x1_t) __a, __b);\n+  return (uint64x1_t) __builtin_aarch64_lshrdi ((int64x1_t) __a, __b);\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n vshrq_n_s8 (int8x16_t __a, const int __b)\n {\n-  return (int8x16_t) __builtin_aarch64_sshr_nv16qi (__a, __b);\n+  return (int8x16_t) __builtin_aarch64_ashrv16qi (__a, __b);\n }\n \n __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n vshrq_n_s16 (int16x8_t __a, const int __b)\n {\n-  return (int16x8_t) __builtin_aarch64_sshr_nv8hi (__a, __b);\n+  return (int16x8_t) __builtin_aarch64_ashrv8hi (__a, __b);\n }\n \n __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n vshrq_n_s32 (int32x4_t __a, const int __b)\n {\n-  return (int32x4_t) __builtin_aarch64_sshr_nv4si (__a, __b);\n+  return (int32x4_t) __builtin_aarch64_ashrv4si (__a, __b);\n }\n \n __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n vshrq_n_s64 (int64x2_t __a, const int __b)\n {\n-  return (int64x2_t) __builtin_aarch64_sshr_nv2di (__a, __b);\n+  return (int64x2_t) __builtin_aarch64_ashrv2di (__a, __b);\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n vshrq_n_u8 (uint8x16_t __a, const int __b)\n {\n-  return (uint8x16_t) __builtin_aarch64_ushr_nv16qi ((int8x16_t) __a, __b);\n+  return (uint8x16_t) __builtin_aarch64_lshrv16qi ((int8x16_t) __a, __b);\n }\n \n __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n vshrq_n_u16 (uint16x8_t __a, const int __b)\n {\n-  return (uint16x8_t) __builtin_aarch64_ushr_nv8hi ((int16x8_t) __a, __b);\n+  return (uint16x8_t) __builtin_aarch64_lshrv8hi ((int16x8_t) __a, __b);\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n vshrq_n_u32 (uint32x4_t __a, const int __b)\n {\n-  return (uint32x4_t) __builtin_aarch64_ushr_nv4si ((int32x4_t) __a, __b);\n+  return (uint32x4_t) __builtin_aarch64_lshrv4si ((int32x4_t) __a, __b);\n }\n \n __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n vshrq_n_u64 (uint64x2_t __a, const int __b)\n {\n-  return (uint64x2_t) __builtin_aarch64_ushr_nv2di ((int64x2_t) __a, __b);\n+  return (uint64x2_t) __builtin_aarch64_lshrv2di ((int64x2_t) __a, __b);\n }\n \n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vshrd_n_s64 (int64x1_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sshr_ndi (__a, __b);\n+  return (int64x1_t) __builtin_aarch64_ashrdi (__a, __b);\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vshrd_n_u64 (uint64x1_t __a, const int __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_ushr_ndi (__a, __b);\n+  return (uint64x1_t) __builtin_aarch64_lshrdi (__a, __b);\n }\n \n /* vsli */"}]}