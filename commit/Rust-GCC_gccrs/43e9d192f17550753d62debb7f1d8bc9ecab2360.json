{"sha": "43e9d192f17550753d62debb7f1d8bc9ecab2360", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NDNlOWQxOTJmMTc1NTA3NTNkNjJkZWJiN2YxZDhiYzllY2FiMjM2MA==", "commit": {"author": {"name": "Ian Bolton", "email": "ian.bolton@arm.com", "date": "2012-10-23T17:02:30Z"}, "committer": {"name": "Marcus Shawcroft", "email": "mshawcroft@gcc.gnu.org", "date": "2012-10-23T17:02:30Z"}, "message": "AArch64 [3/10]\n\n2012-10-23  Ian Bolton  <ian.bolton@arm.com>\n\t    James Greenhalgh  <james.greenhalgh@arm.com>\n\t    Jim MacArthur  <jim.macarthur@arm.com>\n\t    Chris Schlumberger-Socha <chris.schlumberger-socha@arm.com>\n\t    Marcus Shawcroft  <marcus.shawcroft@arm.com>\n\t    Nigel Stephens  <nigel.stephens@arm.com>\n\t    Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>\n\t    Richard Earnshaw  <rearnsha@arm.com>\n\t    Sofiane Naci  <sofiane.naci@arm.com>\n\t    Stephen Thomas  <stephen.thomas@arm.com>\n\t    Tejas Belagod  <tejas.belagod@arm.com>\n\t    Yufeng Zhang  <yufeng.zhang@arm.com>\n\n\t* common/config/aarch64/aarch64-common.c: New file.\n\t* config/aarch64/aarch64-arches.def: New file.\n\t* config/aarch64/aarch64-builtins.c: New file.\n\t* config/aarch64/aarch64-cores.def: New file.\n\t* config/aarch64/aarch64-elf-raw.h: New file.\n\t* config/aarch64/aarch64-elf.h: New file.\n\t* config/aarch64/aarch64-generic.md: New file.\n\t* config/aarch64/aarch64-linux.h: New file.\n\t* config/aarch64/aarch64-modes.def: New file.\n\t* config/aarch64/aarch64-option-extensions.def: New file.\n\t* config/aarch64/aarch64-opts.h: New file.\n\t* config/aarch64/aarch64-protos.h: New file.\n\t* config/aarch64/aarch64-simd.md: New file.\n\t* config/aarch64/aarch64-tune.md: New file.\n\t* config/aarch64/aarch64.c: New file.\n\t* config/aarch64/aarch64.h: New file.\n\t* config/aarch64/aarch64.md: New file.\n\t* config/aarch64/aarch64.opt: New file.\n\t* config/aarch64/arm_neon.h: New file.\n\t* config/aarch64/constraints.md: New file.\n\t* config/aarch64/gentune.sh: New file.\n\t* config/aarch64/iterators.md: New file.\n\t* config/aarch64/large.md: New file.\n\t* config/aarch64/predicates.md: New file.\n\t* config/aarch64/small.md: New file.\n\t* config/aarch64/sync.md: New file.\n\t* config/aarch64/t-aarch64-linux: New file.\n\t* config/aarch64/t-aarch64: New file.\n\n\nCo-Authored-By: Chris Schlumberger-Socha <chris.schlumberger-socha@arm.com>\nCo-Authored-By: James Greenhalgh <james.greenhalgh@arm.com>\nCo-Authored-By: Jim MacArthur <jim.macarthur@arm.com>\nCo-Authored-By: Marcus Shawcroft <marcus.shawcroft@arm.com>\nCo-Authored-By: Nigel Stephens <nigel.stephens@arm.com>\nCo-Authored-By: Ramana Radhakrishnan <ramana.radhakrishnan@arm.com>\nCo-Authored-By: Richard Earnshaw <rearnsha@arm.com>\nCo-Authored-By: Sofiane Naci <sofiane.naci@arm.com>\nCo-Authored-By: Stephen Thomas <stephen.thomas@arm.com>\nCo-Authored-By: Tejas Belagod <tejas.belagod@arm.com>\nCo-Authored-By: Yufeng Zhang <yufeng.zhang@arm.com>\n\nFrom-SVN: r192723", "tree": {"sha": "cd16d0148a24498c87bd7fb3b7eca8ff2bf7658b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/cd16d0148a24498c87bd7fb3b7eca8ff2bf7658b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/43e9d192f17550753d62debb7f1d8bc9ecab2360", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/43e9d192f17550753d62debb7f1d8bc9ecab2360", "html_url": "https://github.com/Rust-GCC/gccrs/commit/43e9d192f17550753d62debb7f1d8bc9ecab2360", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/43e9d192f17550753d62debb7f1d8bc9ecab2360/comments", "author": null, "committer": null, "parents": [{"sha": "0065c7ebdf0c336da82203cf876c09682dbc8b9b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0065c7ebdf0c336da82203cf876c09682dbc8b9b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0065c7ebdf0c336da82203cf876c09682dbc8b9b"}], "stats": {"total": 44200, "additions": 44200, "deletions": 0}, "files": [{"sha": "ee8f79f9d52aa69eff368192f6820e8b8e4b6278", "filename": "gcc/ChangeLog", "status": "modified", "additions": 42, "deletions": 0, "changes": 42, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -1,3 +1,45 @@\n+2012-10-23  Ian Bolton  <ian.bolton@arm.com>\n+\t    James Greenhalgh  <james.greenhalgh@arm.com>\n+\t    Jim MacArthur  <jim.macarthur@arm.com>\n+\t    Chris Schlumberger-Socha <chris.schlumberger-socha@arm.com>\n+\t    Marcus Shawcroft  <marcus.shawcroft@arm.com>\n+\t    Nigel Stephens  <nigel.stephens@arm.com>\n+\t    Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>\n+\t    Richard Earnshaw  <rearnsha@arm.com>\n+\t    Sofiane Naci  <sofiane.naci@arm.com>\n+\t    Stephen Thomas  <stephen.thomas@arm.com>\n+\t    Tejas Belagod  <tejas.belagod@arm.com>\n+\t    Yufeng Zhang  <yufeng.zhang@arm.com>\n+\n+\t* common/config/aarch64/aarch64-common.c: New file.\n+\t* config/aarch64/aarch64-arches.def: New file.\n+\t* config/aarch64/aarch64-builtins.c: New file.\n+\t* config/aarch64/aarch64-cores.def: New file.\n+\t* config/aarch64/aarch64-elf-raw.h: New file.\n+\t* config/aarch64/aarch64-elf.h: New file.\n+\t* config/aarch64/aarch64-generic.md: New file.\n+\t* config/aarch64/aarch64-linux.h: New file.\n+\t* config/aarch64/aarch64-modes.def: New file.\n+\t* config/aarch64/aarch64-option-extensions.def: New file.\n+\t* config/aarch64/aarch64-opts.h: New file.\n+\t* config/aarch64/aarch64-protos.h: New file.\n+\t* config/aarch64/aarch64-simd.md: New file.\n+\t* config/aarch64/aarch64-tune.md: New file.\n+\t* config/aarch64/aarch64.c: New file.\n+\t* config/aarch64/aarch64.h: New file.\n+\t* config/aarch64/aarch64.md: New file.\n+\t* config/aarch64/aarch64.opt: New file.\n+\t* config/aarch64/arm_neon.h: New file.\n+\t* config/aarch64/constraints.md: New file.\n+\t* config/aarch64/gentune.sh: New file.\n+\t* config/aarch64/iterators.md: New file.\n+\t* config/aarch64/large.md: New file.\n+\t* config/aarch64/predicates.md: New file.\n+\t* config/aarch64/small.md: New file.\n+\t* config/aarch64/sync.md: New file.\n+\t* config/aarch64/t-aarch64-linux: New file.\n+\t* config/aarch64/t-aarch64: New file.\n+\n 2012-10-23  Jakub Jelinek  <jakub@redhat.com>\n \n \tPR c++/54988"}, {"sha": "bd249e126ee62224fa699905139ac1f3f2ccc06c", "filename": "gcc/common/config/aarch64/aarch64-common.c", "status": "added", "additions": 88, "deletions": 0, "changes": 88, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fcommon%2Fconfig%2Faarch64%2Faarch64-common.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fcommon%2Fconfig%2Faarch64%2Faarch64-common.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcommon%2Fconfig%2Faarch64%2Faarch64-common.c?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,88 @@\n+/* Common hooks for AArch64.\n+   Copyright (C) 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published\n+   by the Free Software Foundation; either version 3, or (at your\n+   option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT\n+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY\n+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public\n+   License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"tm_p.h\"\n+#include \"common/common-target.h\"\n+#include \"common/common-target-def.h\"\n+#include \"opts.h\"\n+#include \"flags.h\"\n+\n+#ifdef  TARGET_BIG_ENDIAN_DEFAULT\n+#undef  TARGET_DEFAULT_TARGET_FLAGS\n+#define TARGET_DEFAULT_TARGET_FLAGS (MASK_BIG_END)\n+#endif\n+\n+#undef  TARGET_HANDLE_OPTION\n+#define TARGET_HANDLE_OPTION aarch64_handle_option\n+\n+#undef\tTARGET_OPTION_OPTIMIZATION_TABLE\n+#define TARGET_OPTION_OPTIMIZATION_TABLE aarch_option_optimization_table\n+\n+/* Set default optimization options.  */\n+static const struct default_options aarch_option_optimization_table[] =\n+  {\n+    /* Enable section anchors by default at -O1 or higher.  */\n+    { OPT_LEVELS_1_PLUS, OPT_fsection_anchors, NULL, 1 },\n+    { OPT_LEVELS_NONE, 0, NULL, 0 }\n+  };\n+\n+/* Implement TARGET_HANDLE_OPTION.\n+   This function handles the target specific options for CPU/target selection.\n+\n+   march wins over mcpu, so when march is defined, mcpu takes the same value,\n+   otherwise march remains undefined. mtune can be used with either march or\n+   mcpu. If march and mcpu are used together, the rightmost option wins.\n+   mtune can be used with either march or mcpu.  */\n+\n+static bool\n+aarch64_handle_option (struct gcc_options *opts,\n+\t\t       struct gcc_options *opts_set ATTRIBUTE_UNUSED,\n+\t\t       const struct cl_decoded_option *decoded,\n+\t\t       location_t loc ATTRIBUTE_UNUSED)\n+{\n+  size_t code = decoded->opt_index;\n+  const char *arg = decoded->arg;\n+\n+  switch (code)\n+    {\n+    case OPT_march_:\n+      opts->x_aarch64_arch_string = arg;\n+      opts->x_aarch64_cpu_string = arg;\n+      return true;\n+\n+    case OPT_mcpu_:\n+      opts->x_aarch64_cpu_string = arg;\n+      opts->x_aarch64_arch_string = NULL;\n+      return true;\n+\n+    case OPT_mtune_:\n+      opts->x_aarch64_tune_string = arg;\n+      return true;\n+\n+    default:\n+      return true;\n+    }\n+}\n+\n+struct gcc_targetm_common targetm_common = TARGETM_COMMON_INITIALIZER;"}, {"sha": "3ac34baf17ebf2a57fbba85578452ddd68f45108", "filename": "gcc/config/aarch64/aarch64-arches.def", "status": "added", "additions": 29, "deletions": 0, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-arches.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-arches.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-arches.def?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,29 @@\n+/* Copyright (C) 2011, 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published\n+   by the Free Software Foundation; either version 3, or (at your\n+   option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT\n+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY\n+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public\n+   License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+/* Before using #include to read this file, define a macro:\n+\n+      AARCH64_ARCH(NAME, CORE, ARCH, FLAGS)\n+\n+   The NAME is the name of the architecture, represented as a string\n+   constant.  The CORE is the identifier for a core representative of\n+   this architecture.  ARCH is the architecture revision.  FLAGS are\n+   the flags implied by the architecture.  */\n+\n+AARCH64_ARCH(\"armv8-a\",\t      generic,\t     8,  AARCH64_FL_FOR_ARCH8)"}, {"sha": "429a0dfdbfc9b1d277e610f6401c9527bf2c00b7", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "added", "additions": 1320, "deletions": 0, "changes": 1320, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,1320 @@\n+/* Builtins' description for AArch64 SIMD architecture.\n+   Copyright (C) 2011, 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but\n+   WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+   General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"expr.h\"\n+#include \"tm_p.h\"\n+#include \"recog.h\"\n+#include \"langhooks.h\"\n+#include \"diagnostic-core.h\"\n+#include \"optabs.h\"\n+\n+enum aarch64_simd_builtin_type_bits\n+{\n+  T_V8QI = 0x0001,\n+  T_V4HI = 0x0002,\n+  T_V2SI = 0x0004,\n+  T_V2SF = 0x0008,\n+  T_DI = 0x0010,\n+  T_DF = 0x0020,\n+  T_V16QI = 0x0040,\n+  T_V8HI = 0x0080,\n+  T_V4SI = 0x0100,\n+  T_V4SF = 0x0200,\n+  T_V2DI = 0x0400,\n+  T_V2DF = 0x0800,\n+  T_TI = 0x1000,\n+  T_EI = 0x2000,\n+  T_OI = 0x4000,\n+  T_XI = 0x8000,\n+  T_SI = 0x10000,\n+  T_HI = 0x20000,\n+  T_QI = 0x40000\n+};\n+\n+#define v8qi_UP  T_V8QI\n+#define v4hi_UP  T_V4HI\n+#define v2si_UP  T_V2SI\n+#define v2sf_UP  T_V2SF\n+#define di_UP    T_DI\n+#define df_UP    T_DF\n+#define v16qi_UP T_V16QI\n+#define v8hi_UP  T_V8HI\n+#define v4si_UP  T_V4SI\n+#define v4sf_UP  T_V4SF\n+#define v2di_UP  T_V2DI\n+#define v2df_UP  T_V2DF\n+#define ti_UP\t T_TI\n+#define ei_UP\t T_EI\n+#define oi_UP\t T_OI\n+#define xi_UP\t T_XI\n+#define si_UP    T_SI\n+#define hi_UP    T_HI\n+#define qi_UP    T_QI\n+\n+#define UP(X) X##_UP\n+\n+#define T_MAX 19\n+\n+typedef enum\n+{\n+  AARCH64_SIMD_BINOP,\n+  AARCH64_SIMD_TERNOP,\n+  AARCH64_SIMD_QUADOP,\n+  AARCH64_SIMD_UNOP,\n+  AARCH64_SIMD_GETLANE,\n+  AARCH64_SIMD_SETLANE,\n+  AARCH64_SIMD_CREATE,\n+  AARCH64_SIMD_DUP,\n+  AARCH64_SIMD_DUPLANE,\n+  AARCH64_SIMD_COMBINE,\n+  AARCH64_SIMD_SPLIT,\n+  AARCH64_SIMD_LANEMUL,\n+  AARCH64_SIMD_LANEMULL,\n+  AARCH64_SIMD_LANEMULH,\n+  AARCH64_SIMD_LANEMAC,\n+  AARCH64_SIMD_SCALARMUL,\n+  AARCH64_SIMD_SCALARMULL,\n+  AARCH64_SIMD_SCALARMULH,\n+  AARCH64_SIMD_SCALARMAC,\n+  AARCH64_SIMD_CONVERT,\n+  AARCH64_SIMD_FIXCONV,\n+  AARCH64_SIMD_SELECT,\n+  AARCH64_SIMD_RESULTPAIR,\n+  AARCH64_SIMD_REINTERP,\n+  AARCH64_SIMD_VTBL,\n+  AARCH64_SIMD_VTBX,\n+  AARCH64_SIMD_LOAD1,\n+  AARCH64_SIMD_LOAD1LANE,\n+  AARCH64_SIMD_STORE1,\n+  AARCH64_SIMD_STORE1LANE,\n+  AARCH64_SIMD_LOADSTRUCT,\n+  AARCH64_SIMD_LOADSTRUCTLANE,\n+  AARCH64_SIMD_STORESTRUCT,\n+  AARCH64_SIMD_STORESTRUCTLANE,\n+  AARCH64_SIMD_LOGICBINOP,\n+  AARCH64_SIMD_SHIFTINSERT,\n+  AARCH64_SIMD_SHIFTIMM,\n+  AARCH64_SIMD_SHIFTACC\n+} aarch64_simd_itype;\n+\n+typedef struct\n+{\n+  const char *name;\n+  const aarch64_simd_itype itype;\n+  const int bits;\n+  const enum insn_code codes[T_MAX];\n+  const unsigned int num_vars;\n+  unsigned int base_fcode;\n+} aarch64_simd_builtin_datum;\n+\n+#define CF(N, X) CODE_FOR_aarch64_##N##X\n+\n+#define VAR1(T, N, A) \\\n+  #N, AARCH64_SIMD_##T, UP (A), { CF (N, A) }, 1, 0\n+#define VAR2(T, N, A, B) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B), { CF (N, A), CF (N, B) }, 2, 0\n+#define VAR3(T, N, A, B, C) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B) | UP (C), \\\n+  { CF (N, A), CF (N, B), CF (N, C) }, 3, 0\n+#define VAR4(T, N, A, B, C, D) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B) | UP (C) | UP (D), \\\n+  { CF (N, A), CF (N, B), CF (N, C), CF (N, D) }, 4, 0\n+#define VAR5(T, N, A, B, C, D, E) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B) | UP (C) | UP (D) | UP (E), \\\n+  { CF (N, A), CF (N, B), CF (N, C), CF (N, D), CF (N, E) }, 5, 0\n+#define VAR6(T, N, A, B, C, D, E, F) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B) | UP (C) | UP (D) | UP (E) | UP (F), \\\n+  { CF (N, A), CF (N, B), CF (N, C), CF (N, D), CF (N, E), CF (N, F) }, 6, 0\n+#define VAR7(T, N, A, B, C, D, E, F, G) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B) | UP (C) | UP (D) \\\n+\t\t\t| UP (E) | UP (F) | UP (G), \\\n+  { CF (N, A), CF (N, B), CF (N, C), CF (N, D), CF (N, E), CF (N, F), \\\n+    CF (N, G) }, 7, 0\n+#define VAR8(T, N, A, B, C, D, E, F, G, H) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B) | UP (C) | UP (D) \\\n+\t\t| UP (E) | UP (F) | UP (G) \\\n+\t\t| UP (H), \\\n+  { CF (N, A), CF (N, B), CF (N, C), CF (N, D), CF (N, E), CF (N, F), \\\n+    CF (N, G), CF (N, H) }, 8, 0\n+#define VAR9(T, N, A, B, C, D, E, F, G, H, I) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B) | UP (C) | UP (D) \\\n+\t\t| UP (E) | UP (F) | UP (G) \\\n+\t\t| UP (H) | UP (I), \\\n+  { CF (N, A), CF (N, B), CF (N, C), CF (N, D), CF (N, E), CF (N, F), \\\n+    CF (N, G), CF (N, H), CF (N, I) }, 9, 0\n+#define VAR10(T, N, A, B, C, D, E, F, G, H, I, J) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B) | UP (C) | UP (D) \\\n+\t\t| UP (E) | UP (F) | UP (G) \\\n+\t\t| UP (H) | UP (I) | UP (J), \\\n+  { CF (N, A), CF (N, B), CF (N, C), CF (N, D), CF (N, E), CF (N, F), \\\n+    CF (N, G), CF (N, H), CF (N, I), CF (N, J) }, 10, 0\n+\n+#define VAR11(T, N, A, B, C, D, E, F, G, H, I, J, K) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B) | UP (C) | UP (D) \\\n+\t\t| UP (E) | UP (F) | UP (G) \\\n+\t\t| UP (H) | UP (I) | UP (J) | UP (K), \\\n+  { CF (N, A), CF (N, B), CF (N, C), CF (N, D), CF (N, E), CF (N, F), \\\n+    CF (N, G), CF (N, H), CF (N, I), CF (N, J), CF (N, K) }, 11, 0\n+\n+#define VAR12(T, N, A, B, C, D, E, F, G, H, I, J, K, L) \\\n+  #N, AARCH64_SIMD_##T, UP (A) | UP (B) | UP (C) | UP (D) \\\n+\t\t| UP (E) | UP (F) | UP (G) \\\n+\t\t| UP (H) | UP (I) | UP (J) | UP (K) | UP (L), \\\n+  { CF (N, A), CF (N, B), CF (N, C), CF (N, D), CF (N, E), CF (N, F), \\\n+    CF (N, G), CF (N, H), CF (N, I), CF (N, J), CF (N, K), CF (N, L) }, 12, 0\n+\n+\n+/* The mode entries in the following table correspond to the \"key\" type of the\n+   instruction variant, i.e. equivalent to that which would be specified after\n+   the assembler mnemonic, which usually refers to the last vector operand.\n+   (Signed/unsigned/polynomial types are not differentiated between though, and\n+   are all mapped onto the same mode for a given element size.) The modes\n+   listed per instruction should be the same as those defined for that\n+   instruction's pattern in aarch64_simd.md.\n+   WARNING: Variants should be listed in the same increasing order as\n+   aarch64_simd_builtin_type_bits.  */\n+\n+static aarch64_simd_builtin_datum aarch64_simd_builtin_data[] = {\n+  {VAR6 (CREATE, create, v8qi, v4hi, v2si, v2sf, di, df)},\n+  {VAR6 (GETLANE, get_lane_signed,\n+\t  v8qi, v4hi, v2si, v16qi, v8hi, v4si)},\n+  {VAR7 (GETLANE, get_lane_unsigned,\n+\t  v8qi, v4hi, v2si, v16qi, v8hi, v4si, v2di)},\n+  {VAR4 (GETLANE, get_lane, v2sf, di, v4sf, v2df)},\n+  {VAR6 (GETLANE, get_dregoi, v8qi, v4hi, v2si, v2sf, di, df)},\n+  {VAR6 (GETLANE, get_qregoi, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR6 (GETLANE, get_dregci, v8qi, v4hi, v2si, v2sf, di, df)},\n+  {VAR6 (GETLANE, get_qregci, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR6 (GETLANE, get_dregxi, v8qi, v4hi, v2si, v2sf, di, df)},\n+  {VAR6 (GETLANE, get_qregxi, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR6 (SETLANE, set_qregoi, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR6 (SETLANE, set_qregci, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR6 (SETLANE, set_qregxi, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+\n+  {VAR5 (REINTERP, reinterpretv8qi, v8qi, v4hi, v2si, v2sf, di)},\n+  {VAR5 (REINTERP, reinterpretv4hi, v8qi, v4hi, v2si, v2sf, di)},\n+  {VAR5 (REINTERP, reinterpretv2si, v8qi, v4hi, v2si, v2sf, di)},\n+  {VAR5 (REINTERP, reinterpretv2sf, v8qi, v4hi, v2si, v2sf, di)},\n+  {VAR5 (REINTERP, reinterpretdi, v8qi, v4hi, v2si, v2sf, di)},\n+  {VAR6 (REINTERP, reinterpretv16qi, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR6 (REINTERP, reinterpretv8hi, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR6 (REINTERP, reinterpretv4si, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR6 (REINTERP, reinterpretv4sf, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR6 (REINTERP, reinterpretv2di, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR6 (COMBINE, combine, v8qi, v4hi, v2si, v2sf, di, df)},\n+\n+  {VAR3 (BINOP, saddl, v8qi, v4hi, v2si)},\n+  {VAR3 (BINOP, uaddl, v8qi, v4hi, v2si)},\n+  {VAR3 (BINOP, saddl2, v16qi, v8hi, v4si)},\n+  {VAR3 (BINOP, uaddl2, v16qi, v8hi, v4si)},\n+  {VAR3 (BINOP, saddw, v8qi, v4hi, v2si)},\n+  {VAR3 (BINOP, uaddw, v8qi, v4hi, v2si)},\n+  {VAR3 (BINOP, saddw2, v16qi, v8hi, v4si)},\n+  {VAR3 (BINOP, uaddw2, v16qi, v8hi, v4si)},\n+  {VAR6 (BINOP, shadd, v8qi, v4hi, v2si, v16qi, v8hi, v4si)},\n+  {VAR6 (BINOP, uhadd, v8qi, v4hi, v2si, v16qi, v8hi, v4si)},\n+  {VAR6 (BINOP, srhadd, v8qi, v4hi, v2si, v16qi, v8hi, v4si)},\n+  {VAR6 (BINOP, urhadd, v8qi, v4hi, v2si, v16qi, v8hi, v4si)},\n+  {VAR3 (BINOP, addhn, v8hi, v4si, v2di)},\n+  {VAR3 (BINOP, raddhn, v8hi, v4si, v2di)},\n+  {VAR3 (TERNOP, addhn2, v8hi, v4si, v2di)},\n+  {VAR3 (TERNOP, raddhn2, v8hi, v4si, v2di)},\n+  {VAR3 (BINOP, ssubl, v8qi, v4hi, v2si)},\n+  {VAR3 (BINOP, usubl, v8qi, v4hi, v2si)},\n+  {VAR3 (BINOP, ssubl2, v16qi, v8hi, v4si) },\n+  {VAR3 (BINOP, usubl2, v16qi, v8hi, v4si) },\n+  {VAR3 (BINOP, ssubw, v8qi, v4hi, v2si) },\n+  {VAR3 (BINOP, usubw, v8qi, v4hi, v2si) },\n+  {VAR3 (BINOP, ssubw2, v16qi, v8hi, v4si) },\n+  {VAR3 (BINOP, usubw2, v16qi, v8hi, v4si) },\n+  {VAR11 (BINOP, sqadd, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi)},\n+  {VAR11 (BINOP, uqadd, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi)},\n+  {VAR11 (BINOP, sqsub, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi)},\n+  {VAR11 (BINOP, uqsub, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi)},\n+  {VAR11 (BINOP, suqadd, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi)},\n+  {VAR11 (BINOP, usqadd, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi)},\n+  {VAR6 (UNOP, sqmovun, di, v8hi, v4si, v2di, si, hi)},\n+  {VAR6 (UNOP, sqmovn, di, v8hi, v4si, v2di, si, hi)},\n+  {VAR6 (UNOP, uqmovn, di, v8hi, v4si, v2di, si, hi)},\n+  {VAR10 (UNOP, sqabs, v8qi, v4hi, v2si, v16qi, v8hi, v4si, v2di, si, hi, qi)},\n+  {VAR10 (UNOP, sqneg, v8qi, v4hi, v2si, v16qi, v8hi, v4si, v2di, si, hi, qi)},\n+  {VAR2 (BINOP, pmul, v8qi, v16qi)},\n+  {VAR4 (TERNOP, sqdmlal, v4hi, v2si, si, hi)},\n+  {VAR4 (QUADOP, sqdmlal_lane, v4hi, v2si, si, hi) },\n+  {VAR2 (QUADOP, sqdmlal_laneq, v4hi, v2si) },\n+  {VAR2 (TERNOP, sqdmlal_n, v4hi, v2si) },\n+  {VAR2 (TERNOP, sqdmlal2, v8hi, v4si)},\n+  {VAR2 (QUADOP, sqdmlal2_lane, v8hi, v4si) },\n+  {VAR2 (QUADOP, sqdmlal2_laneq, v8hi, v4si) },\n+  {VAR2 (TERNOP, sqdmlal2_n, v8hi, v4si) },\n+  {VAR4 (TERNOP, sqdmlsl, v4hi, v2si, si, hi)},\n+  {VAR4 (QUADOP, sqdmlsl_lane, v4hi, v2si, si, hi) },\n+  {VAR2 (QUADOP, sqdmlsl_laneq, v4hi, v2si) },\n+  {VAR2 (TERNOP, sqdmlsl_n, v4hi, v2si) },\n+  {VAR2 (TERNOP, sqdmlsl2, v8hi, v4si)},\n+  {VAR2 (QUADOP, sqdmlsl2_lane, v8hi, v4si) },\n+  {VAR2 (QUADOP, sqdmlsl2_laneq, v8hi, v4si) },\n+  {VAR2 (TERNOP, sqdmlsl2_n, v8hi, v4si) },\n+  {VAR4 (BINOP, sqdmull, v4hi, v2si, si, hi)},\n+  {VAR4 (TERNOP, sqdmull_lane, v4hi, v2si, si, hi) },\n+  {VAR2 (TERNOP, sqdmull_laneq, v4hi, v2si) },\n+  {VAR2 (BINOP, sqdmull_n, v4hi, v2si) },\n+  {VAR2 (BINOP, sqdmull2, v8hi, v4si) },\n+  {VAR2 (TERNOP, sqdmull2_lane, v8hi, v4si) },\n+  {VAR2 (TERNOP, sqdmull2_laneq, v8hi, v4si) },\n+  {VAR2 (BINOP, sqdmull2_n, v8hi, v4si) },\n+  {VAR6 (BINOP, sqdmulh, v4hi, v2si, v8hi, v4si, si, hi)},\n+  {VAR6 (BINOP, sqrdmulh, v4hi, v2si, v8hi, v4si, si, hi)},\n+  {VAR8 (BINOP, sshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR3 (SHIFTIMM, sshll_n, v8qi, v4hi, v2si) },\n+  {VAR3 (SHIFTIMM, ushll_n, v8qi, v4hi, v2si) },\n+  {VAR3 (SHIFTIMM, sshll2_n, v16qi, v8hi, v4si) },\n+  {VAR3 (SHIFTIMM, ushll2_n, v16qi, v8hi, v4si) },\n+  {VAR8 (BINOP, ushl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (BINOP, sshl_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (BINOP, ushl_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR11 (BINOP, sqshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi) },\n+  {VAR11 (BINOP, uqshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi) },\n+  {VAR8 (BINOP, srshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (BINOP, urshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR11 (BINOP, sqrshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi) },\n+  {VAR11 (BINOP, uqrshl, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi) },\n+  {VAR8 (SHIFTIMM, sshr_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTIMM, ushr_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTIMM, srshr_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTIMM, urshr_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTACC, ssra_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTACC, usra_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTACC, srsra_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTACC, ursra_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTINSERT, ssri_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTINSERT, usri_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTINSERT, ssli_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR8 (SHIFTINSERT, usli_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  {VAR11 (SHIFTIMM, sqshlu_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi) },\n+  {VAR11 (SHIFTIMM, sqshl_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi) },\n+  {VAR11 (SHIFTIMM, uqshl_n, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi) },\n+  { VAR6 (SHIFTIMM, sqshrun_n, di, v8hi, v4si, v2di, si, hi) },\n+  { VAR6 (SHIFTIMM, sqrshrun_n, di, v8hi, v4si, v2di, si, hi) },\n+  { VAR6 (SHIFTIMM, sqshrn_n, di, v8hi, v4si, v2di, si, hi) },\n+  { VAR6 (SHIFTIMM, uqshrn_n, di, v8hi, v4si, v2di, si, hi) },\n+  { VAR6 (SHIFTIMM, sqrshrn_n, di, v8hi, v4si, v2di, si, hi) },\n+  { VAR6 (SHIFTIMM, uqrshrn_n, di, v8hi, v4si, v2di, si, hi) },\n+  { VAR8 (BINOP, cmeq, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  { VAR8 (BINOP, cmge, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  { VAR8 (BINOP, cmgt, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  { VAR8 (BINOP, cmle, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  { VAR8 (BINOP, cmlt, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  { VAR8 (BINOP, cmhs, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  { VAR8 (BINOP, cmhi, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  { VAR8 (BINOP, cmtst, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di) },\n+  { VAR6 (TERNOP, sqdmulh_lane, v4hi, v2si, v8hi, v4si, si, hi) },\n+  { VAR6 (TERNOP, sqrdmulh_lane, v4hi, v2si, v8hi, v4si, si, hi) },\n+  { VAR3 (BINOP, addp, v8qi, v4hi, v2si) },\n+  { VAR1 (UNOP, addp, di) },\n+  { VAR11 (BINOP, dup_lane, v8qi, v4hi, v2si, di, v16qi, v8hi, v4si, v2di,\n+\t  si, hi, qi) },\n+  { VAR3 (BINOP, fmax, v2sf, v4sf, v2df) },\n+  { VAR3 (BINOP, fmin, v2sf, v4sf, v2df) },\n+  { VAR6 (BINOP, smax, v8qi, v4hi, v2si, v16qi, v8hi, v4si) },\n+  { VAR6 (BINOP, smin, v8qi, v4hi, v2si, v16qi, v8hi, v4si) },\n+  { VAR6 (BINOP, umax, v8qi, v4hi, v2si, v16qi, v8hi, v4si) },\n+  { VAR6 (BINOP, umin, v8qi, v4hi, v2si, v16qi, v8hi, v4si) },\n+  { VAR3 (UNOP, sqrt, v2sf, v4sf, v2df) },\n+  {VAR12 (LOADSTRUCT, ld2,\n+\t v8qi, v4hi, v2si, v2sf, di, df, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR12 (LOADSTRUCT, ld3,\n+\t v8qi, v4hi, v2si, v2sf, di, df, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR12 (LOADSTRUCT, ld4,\n+\t v8qi, v4hi, v2si, v2sf, di, df, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR12 (STORESTRUCT, st2,\n+\t v8qi, v4hi, v2si, v2sf, di, df, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR12 (STORESTRUCT, st3,\n+\t v8qi, v4hi, v2si, v2sf, di, df, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+  {VAR12 (STORESTRUCT, st4,\n+\t v8qi, v4hi, v2si, v2sf, di, df, v16qi, v8hi, v4si, v4sf, v2di, v2df)},\n+};\n+\n+#undef CF\n+#undef VAR1\n+#undef VAR2\n+#undef VAR3\n+#undef VAR4\n+#undef VAR5\n+#undef VAR6\n+#undef VAR7\n+#undef VAR8\n+#undef VAR9\n+#undef VAR10\n+#undef VAR11\n+\n+#define NUM_DREG_TYPES 6\n+#define NUM_QREG_TYPES 6\n+\n+void\n+init_aarch64_simd_builtins (void)\n+{\n+  unsigned int i, fcode = AARCH64_SIMD_BUILTIN_BASE;\n+\n+  /* Scalar type nodes.  */\n+  tree aarch64_simd_intQI_type_node;\n+  tree aarch64_simd_intHI_type_node;\n+  tree aarch64_simd_polyQI_type_node;\n+  tree aarch64_simd_polyHI_type_node;\n+  tree aarch64_simd_intSI_type_node;\n+  tree aarch64_simd_intDI_type_node;\n+  tree aarch64_simd_float_type_node;\n+  tree aarch64_simd_double_type_node;\n+\n+  /* Pointer to scalar type nodes.  */\n+  tree intQI_pointer_node;\n+  tree intHI_pointer_node;\n+  tree intSI_pointer_node;\n+  tree intDI_pointer_node;\n+  tree float_pointer_node;\n+  tree double_pointer_node;\n+\n+  /* Const scalar type nodes.  */\n+  tree const_intQI_node;\n+  tree const_intHI_node;\n+  tree const_intSI_node;\n+  tree const_intDI_node;\n+  tree const_float_node;\n+  tree const_double_node;\n+\n+  /* Pointer to const scalar type nodes.  */\n+  tree const_intQI_pointer_node;\n+  tree const_intHI_pointer_node;\n+  tree const_intSI_pointer_node;\n+  tree const_intDI_pointer_node;\n+  tree const_float_pointer_node;\n+  tree const_double_pointer_node;\n+\n+  /* Vector type nodes.  */\n+  tree V8QI_type_node;\n+  tree V4HI_type_node;\n+  tree V2SI_type_node;\n+  tree V2SF_type_node;\n+  tree V16QI_type_node;\n+  tree V8HI_type_node;\n+  tree V4SI_type_node;\n+  tree V4SF_type_node;\n+  tree V2DI_type_node;\n+  tree V2DF_type_node;\n+\n+  /* Scalar unsigned type nodes.  */\n+  tree intUQI_type_node;\n+  tree intUHI_type_node;\n+  tree intUSI_type_node;\n+  tree intUDI_type_node;\n+\n+  /* Opaque integer types for structures of vectors.  */\n+  tree intEI_type_node;\n+  tree intOI_type_node;\n+  tree intCI_type_node;\n+  tree intXI_type_node;\n+\n+  /* Pointer to vector type nodes.  */\n+  tree V8QI_pointer_node;\n+  tree V4HI_pointer_node;\n+  tree V2SI_pointer_node;\n+  tree V2SF_pointer_node;\n+  tree V16QI_pointer_node;\n+  tree V8HI_pointer_node;\n+  tree V4SI_pointer_node;\n+  tree V4SF_pointer_node;\n+  tree V2DI_pointer_node;\n+  tree V2DF_pointer_node;\n+\n+  /* Operations which return results as pairs.  */\n+  tree void_ftype_pv8qi_v8qi_v8qi;\n+  tree void_ftype_pv4hi_v4hi_v4hi;\n+  tree void_ftype_pv2si_v2si_v2si;\n+  tree void_ftype_pv2sf_v2sf_v2sf;\n+  tree void_ftype_pdi_di_di;\n+  tree void_ftype_pv16qi_v16qi_v16qi;\n+  tree void_ftype_pv8hi_v8hi_v8hi;\n+  tree void_ftype_pv4si_v4si_v4si;\n+  tree void_ftype_pv4sf_v4sf_v4sf;\n+  tree void_ftype_pv2di_v2di_v2di;\n+  tree void_ftype_pv2df_v2df_v2df;\n+\n+  tree reinterp_ftype_dreg[NUM_DREG_TYPES][NUM_DREG_TYPES];\n+  tree reinterp_ftype_qreg[NUM_QREG_TYPES][NUM_QREG_TYPES];\n+  tree dreg_types[NUM_DREG_TYPES], qreg_types[NUM_QREG_TYPES];\n+\n+  /* Create distinguished type nodes for AARCH64_SIMD vector element types,\n+     and pointers to values of such types, so we can detect them later.  */\n+  aarch64_simd_intQI_type_node =\n+    make_signed_type (GET_MODE_PRECISION (QImode));\n+  aarch64_simd_intHI_type_node =\n+    make_signed_type (GET_MODE_PRECISION (HImode));\n+  aarch64_simd_polyQI_type_node =\n+    make_signed_type (GET_MODE_PRECISION (QImode));\n+  aarch64_simd_polyHI_type_node =\n+    make_signed_type (GET_MODE_PRECISION (HImode));\n+  aarch64_simd_intSI_type_node =\n+    make_signed_type (GET_MODE_PRECISION (SImode));\n+  aarch64_simd_intDI_type_node =\n+    make_signed_type (GET_MODE_PRECISION (DImode));\n+  aarch64_simd_float_type_node = make_node (REAL_TYPE);\n+  aarch64_simd_double_type_node = make_node (REAL_TYPE);\n+  TYPE_PRECISION (aarch64_simd_float_type_node) = FLOAT_TYPE_SIZE;\n+  TYPE_PRECISION (aarch64_simd_double_type_node) = DOUBLE_TYPE_SIZE;\n+  layout_type (aarch64_simd_float_type_node);\n+  layout_type (aarch64_simd_double_type_node);\n+\n+  /* Define typedefs which exactly correspond to the modes we are basing vector\n+     types on.  If you change these names you'll need to change\n+     the table used by aarch64_mangle_type too.  */\n+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intQI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_qi\");\n+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intHI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_hi\");\n+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intSI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_si\");\n+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_float_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_sf\");\n+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_intDI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_di\");\n+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_double_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_df\");\n+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_polyQI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_poly8\");\n+  (*lang_hooks.types.register_builtin_type) (aarch64_simd_polyHI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_poly16\");\n+\n+  intQI_pointer_node = build_pointer_type (aarch64_simd_intQI_type_node);\n+  intHI_pointer_node = build_pointer_type (aarch64_simd_intHI_type_node);\n+  intSI_pointer_node = build_pointer_type (aarch64_simd_intSI_type_node);\n+  intDI_pointer_node = build_pointer_type (aarch64_simd_intDI_type_node);\n+  float_pointer_node = build_pointer_type (aarch64_simd_float_type_node);\n+  double_pointer_node = build_pointer_type (aarch64_simd_double_type_node);\n+\n+  /* Next create constant-qualified versions of the above types.  */\n+  const_intQI_node = build_qualified_type (aarch64_simd_intQI_type_node,\n+\t\t\t\t\t   TYPE_QUAL_CONST);\n+  const_intHI_node = build_qualified_type (aarch64_simd_intHI_type_node,\n+\t\t\t\t\t   TYPE_QUAL_CONST);\n+  const_intSI_node = build_qualified_type (aarch64_simd_intSI_type_node,\n+\t\t\t\t\t   TYPE_QUAL_CONST);\n+  const_intDI_node = build_qualified_type (aarch64_simd_intDI_type_node,\n+\t\t\t\t\t   TYPE_QUAL_CONST);\n+  const_float_node = build_qualified_type (aarch64_simd_float_type_node,\n+\t\t\t\t\t   TYPE_QUAL_CONST);\n+  const_double_node = build_qualified_type (aarch64_simd_double_type_node,\n+\t\t\t\t\t    TYPE_QUAL_CONST);\n+\n+  const_intQI_pointer_node = build_pointer_type (const_intQI_node);\n+  const_intHI_pointer_node = build_pointer_type (const_intHI_node);\n+  const_intSI_pointer_node = build_pointer_type (const_intSI_node);\n+  const_intDI_pointer_node = build_pointer_type (const_intDI_node);\n+  const_float_pointer_node = build_pointer_type (const_float_node);\n+  const_double_pointer_node = build_pointer_type (const_double_node);\n+\n+  /* Now create vector types based on our AARCH64 SIMD element types.  */\n+  /* 64-bit vectors.  */\n+  V8QI_type_node =\n+    build_vector_type_for_mode (aarch64_simd_intQI_type_node, V8QImode);\n+  V4HI_type_node =\n+    build_vector_type_for_mode (aarch64_simd_intHI_type_node, V4HImode);\n+  V2SI_type_node =\n+    build_vector_type_for_mode (aarch64_simd_intSI_type_node, V2SImode);\n+  V2SF_type_node =\n+    build_vector_type_for_mode (aarch64_simd_float_type_node, V2SFmode);\n+  /* 128-bit vectors.  */\n+  V16QI_type_node =\n+    build_vector_type_for_mode (aarch64_simd_intQI_type_node, V16QImode);\n+  V8HI_type_node =\n+    build_vector_type_for_mode (aarch64_simd_intHI_type_node, V8HImode);\n+  V4SI_type_node =\n+    build_vector_type_for_mode (aarch64_simd_intSI_type_node, V4SImode);\n+  V4SF_type_node =\n+    build_vector_type_for_mode (aarch64_simd_float_type_node, V4SFmode);\n+  V2DI_type_node =\n+    build_vector_type_for_mode (aarch64_simd_intDI_type_node, V2DImode);\n+  V2DF_type_node =\n+    build_vector_type_for_mode (aarch64_simd_double_type_node, V2DFmode);\n+\n+  /* Unsigned integer types for various mode sizes.  */\n+  intUQI_type_node = make_unsigned_type (GET_MODE_PRECISION (QImode));\n+  intUHI_type_node = make_unsigned_type (GET_MODE_PRECISION (HImode));\n+  intUSI_type_node = make_unsigned_type (GET_MODE_PRECISION (SImode));\n+  intUDI_type_node = make_unsigned_type (GET_MODE_PRECISION (DImode));\n+\n+  (*lang_hooks.types.register_builtin_type) (intUQI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_uqi\");\n+  (*lang_hooks.types.register_builtin_type) (intUHI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_uhi\");\n+  (*lang_hooks.types.register_builtin_type) (intUSI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_usi\");\n+  (*lang_hooks.types.register_builtin_type) (intUDI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_udi\");\n+\n+  /* Opaque integer types for structures of vectors.  */\n+  intEI_type_node = make_signed_type (GET_MODE_PRECISION (EImode));\n+  intOI_type_node = make_signed_type (GET_MODE_PRECISION (OImode));\n+  intCI_type_node = make_signed_type (GET_MODE_PRECISION (CImode));\n+  intXI_type_node = make_signed_type (GET_MODE_PRECISION (XImode));\n+\n+  (*lang_hooks.types.register_builtin_type) (intTI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_ti\");\n+  (*lang_hooks.types.register_builtin_type) (intEI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_ei\");\n+  (*lang_hooks.types.register_builtin_type) (intOI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_oi\");\n+  (*lang_hooks.types.register_builtin_type) (intCI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_ci\");\n+  (*lang_hooks.types.register_builtin_type) (intXI_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_xi\");\n+\n+  /* Pointers to vector types.  */\n+  V8QI_pointer_node = build_pointer_type (V8QI_type_node);\n+  V4HI_pointer_node = build_pointer_type (V4HI_type_node);\n+  V2SI_pointer_node = build_pointer_type (V2SI_type_node);\n+  V2SF_pointer_node = build_pointer_type (V2SF_type_node);\n+  V16QI_pointer_node = build_pointer_type (V16QI_type_node);\n+  V8HI_pointer_node = build_pointer_type (V8HI_type_node);\n+  V4SI_pointer_node = build_pointer_type (V4SI_type_node);\n+  V4SF_pointer_node = build_pointer_type (V4SF_type_node);\n+  V2DI_pointer_node = build_pointer_type (V2DI_type_node);\n+  V2DF_pointer_node = build_pointer_type (V2DF_type_node);\n+\n+  /* Operations which return results as pairs.  */\n+  void_ftype_pv8qi_v8qi_v8qi =\n+    build_function_type_list (void_type_node, V8QI_pointer_node,\n+\t\t\t      V8QI_type_node, V8QI_type_node, NULL);\n+  void_ftype_pv4hi_v4hi_v4hi =\n+    build_function_type_list (void_type_node, V4HI_pointer_node,\n+\t\t\t      V4HI_type_node, V4HI_type_node, NULL);\n+  void_ftype_pv2si_v2si_v2si =\n+    build_function_type_list (void_type_node, V2SI_pointer_node,\n+\t\t\t      V2SI_type_node, V2SI_type_node, NULL);\n+  void_ftype_pv2sf_v2sf_v2sf =\n+    build_function_type_list (void_type_node, V2SF_pointer_node,\n+\t\t\t      V2SF_type_node, V2SF_type_node, NULL);\n+  void_ftype_pdi_di_di =\n+    build_function_type_list (void_type_node, intDI_pointer_node,\n+\t\t\t      aarch64_simd_intDI_type_node,\n+\t\t\t      aarch64_simd_intDI_type_node, NULL);\n+  void_ftype_pv16qi_v16qi_v16qi =\n+    build_function_type_list (void_type_node, V16QI_pointer_node,\n+\t\t\t      V16QI_type_node, V16QI_type_node, NULL);\n+  void_ftype_pv8hi_v8hi_v8hi =\n+    build_function_type_list (void_type_node, V8HI_pointer_node,\n+\t\t\t      V8HI_type_node, V8HI_type_node, NULL);\n+  void_ftype_pv4si_v4si_v4si =\n+    build_function_type_list (void_type_node, V4SI_pointer_node,\n+\t\t\t      V4SI_type_node, V4SI_type_node, NULL);\n+  void_ftype_pv4sf_v4sf_v4sf =\n+    build_function_type_list (void_type_node, V4SF_pointer_node,\n+\t\t\t      V4SF_type_node, V4SF_type_node, NULL);\n+  void_ftype_pv2di_v2di_v2di =\n+    build_function_type_list (void_type_node, V2DI_pointer_node,\n+\t\t\t      V2DI_type_node, V2DI_type_node, NULL);\n+  void_ftype_pv2df_v2df_v2df =\n+    build_function_type_list (void_type_node, V2DF_pointer_node,\n+\t\t\t      V2DF_type_node, V2DF_type_node, NULL);\n+\n+  dreg_types[0] = V8QI_type_node;\n+  dreg_types[1] = V4HI_type_node;\n+  dreg_types[2] = V2SI_type_node;\n+  dreg_types[3] = V2SF_type_node;\n+  dreg_types[4] = aarch64_simd_intDI_type_node;\n+  dreg_types[5] = aarch64_simd_double_type_node;\n+\n+  qreg_types[0] = V16QI_type_node;\n+  qreg_types[1] = V8HI_type_node;\n+  qreg_types[2] = V4SI_type_node;\n+  qreg_types[3] = V4SF_type_node;\n+  qreg_types[4] = V2DI_type_node;\n+  qreg_types[5] = V2DF_type_node;\n+\n+  /* If NUM_DREG_TYPES != NUM_QREG_TYPES, we will need separate nested loops\n+     for qreg and dreg reinterp inits.  */\n+  for (i = 0; i < NUM_DREG_TYPES; i++)\n+    {\n+      int j;\n+      for (j = 0; j < NUM_DREG_TYPES; j++)\n+\t{\n+\t  reinterp_ftype_dreg[i][j]\n+\t    = build_function_type_list (dreg_types[i], dreg_types[j], NULL);\n+\t  reinterp_ftype_qreg[i][j]\n+\t    = build_function_type_list (qreg_types[i], qreg_types[j], NULL);\n+\t}\n+    }\n+\n+  for (i = 0; i < ARRAY_SIZE (aarch64_simd_builtin_data); i++)\n+    {\n+      aarch64_simd_builtin_datum *d = &aarch64_simd_builtin_data[i];\n+      unsigned int j, codeidx = 0;\n+\n+      d->base_fcode = fcode;\n+\n+      for (j = 0; j < T_MAX; j++)\n+\t{\n+\t  const char *const modenames[] = {\n+\t    \"v8qi\", \"v4hi\", \"v2si\", \"v2sf\", \"di\", \"df\",\n+\t    \"v16qi\", \"v8hi\", \"v4si\", \"v4sf\", \"v2di\", \"v2df\",\n+\t    \"ti\", \"ei\", \"oi\", \"xi\", \"si\", \"hi\", \"qi\"\n+\t  };\n+\t  char namebuf[60];\n+\t  tree ftype = NULL;\n+\t  enum insn_code icode;\n+\t  int is_load = 0;\n+\t  int is_store = 0;\n+\n+\t  /* Skip if particular mode not supported.  */\n+\t  if ((d->bits & (1 << j)) == 0)\n+\t    continue;\n+\n+\t  icode = d->codes[codeidx++];\n+\n+\t  switch (d->itype)\n+\t    {\n+\t    case AARCH64_SIMD_LOAD1:\n+\t    case AARCH64_SIMD_LOAD1LANE:\n+\t    case AARCH64_SIMD_LOADSTRUCTLANE:\n+\t    case AARCH64_SIMD_LOADSTRUCT:\n+\t      is_load = 1;\n+\t      /* Fall through.  */\n+\t    case AARCH64_SIMD_STORE1:\n+\t    case AARCH64_SIMD_STORE1LANE:\n+\t    case AARCH64_SIMD_STORESTRUCTLANE:\n+\t    case AARCH64_SIMD_STORESTRUCT:\n+\t      if (!is_load)\n+\t\tis_store = 1;\n+\t      /* Fall through.  */\n+\t    case AARCH64_SIMD_UNOP:\n+\t    case AARCH64_SIMD_BINOP:\n+\t    case AARCH64_SIMD_LOGICBINOP:\n+\t    case AARCH64_SIMD_SHIFTINSERT:\n+\t    case AARCH64_SIMD_TERNOP:\n+\t    case AARCH64_SIMD_QUADOP:\n+\t    case AARCH64_SIMD_GETLANE:\n+\t    case AARCH64_SIMD_SETLANE:\n+\t    case AARCH64_SIMD_CREATE:\n+\t    case AARCH64_SIMD_DUP:\n+\t    case AARCH64_SIMD_DUPLANE:\n+\t    case AARCH64_SIMD_SHIFTIMM:\n+\t    case AARCH64_SIMD_SHIFTACC:\n+\t    case AARCH64_SIMD_COMBINE:\n+\t    case AARCH64_SIMD_SPLIT:\n+\t    case AARCH64_SIMD_CONVERT:\n+\t    case AARCH64_SIMD_FIXCONV:\n+\t    case AARCH64_SIMD_LANEMUL:\n+\t    case AARCH64_SIMD_LANEMULL:\n+\t    case AARCH64_SIMD_LANEMULH:\n+\t    case AARCH64_SIMD_LANEMAC:\n+\t    case AARCH64_SIMD_SCALARMUL:\n+\t    case AARCH64_SIMD_SCALARMULL:\n+\t    case AARCH64_SIMD_SCALARMULH:\n+\t    case AARCH64_SIMD_SCALARMAC:\n+\t    case AARCH64_SIMD_SELECT:\n+\t    case AARCH64_SIMD_VTBL:\n+\t    case AARCH64_SIMD_VTBX:\n+\t      {\n+\t\tint k;\n+\t\ttree return_type = void_type_node, args = void_list_node;\n+\n+\t\t/* Build a function type directly from the insn_data for this\n+\t\t   builtin.  The build_function_type() function takes care of\n+\t\t   removing duplicates for us.  */\n+\t\tfor (k = insn_data[icode].n_operands - 1; k >= 0; k--)\n+\t\t  {\n+\t\t    tree eltype;\n+\n+\t\t    /* Skip an internal operand for vget_{low, high}.  */\n+\t\t    if (k == 2 && d->itype == AARCH64_SIMD_SPLIT)\n+\t\t      continue;\n+\n+\t\t    if (is_load && k == 1)\n+\t\t      {\n+\t\t\t/* AdvSIMD load patterns always have the memory operand\n+\t\t\t   (a DImode pointer) in the operand 1 position.  We\n+\t\t\t   want a const pointer to the element type in that\n+\t\t\t   position.  */\n+\t\t\tgcc_assert (insn_data[icode].operand[k].mode ==\n+\t\t\t\t    DImode);\n+\n+\t\t\tswitch (1 << j)\n+\t\t\t  {\n+\t\t\t  case T_V8QI:\n+\t\t\t  case T_V16QI:\n+\t\t\t    eltype = const_intQI_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  case T_V4HI:\n+\t\t\t  case T_V8HI:\n+\t\t\t    eltype = const_intHI_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  case T_V2SI:\n+\t\t\t  case T_V4SI:\n+\t\t\t    eltype = const_intSI_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  case T_V2SF:\n+\t\t\t  case T_V4SF:\n+\t\t\t    eltype = const_float_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  case T_DI:\n+\t\t\t  case T_V2DI:\n+\t\t\t    eltype = const_intDI_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  case T_DF:\n+\t\t\t  case T_V2DF:\n+\t\t\t    eltype = const_double_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  default:\n+\t\t\t    gcc_unreachable ();\n+\t\t\t  }\n+\t\t      }\n+\t\t    else if (is_store && k == 0)\n+\t\t      {\n+\t\t\t/* Similarly, AdvSIMD store patterns use operand 0 as\n+\t\t\t   the memory location to store to (a DImode pointer).\n+\t\t\t   Use a pointer to the element type of the store in\n+\t\t\t   that position.  */\n+\t\t\tgcc_assert (insn_data[icode].operand[k].mode ==\n+\t\t\t\t    DImode);\n+\n+\t\t\tswitch (1 << j)\n+\t\t\t  {\n+\t\t\t  case T_V8QI:\n+\t\t\t  case T_V16QI:\n+\t\t\t    eltype = intQI_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  case T_V4HI:\n+\t\t\t  case T_V8HI:\n+\t\t\t    eltype = intHI_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  case T_V2SI:\n+\t\t\t  case T_V4SI:\n+\t\t\t    eltype = intSI_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  case T_V2SF:\n+\t\t\t  case T_V4SF:\n+\t\t\t    eltype = float_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  case T_DI:\n+\t\t\t  case T_V2DI:\n+\t\t\t    eltype = intDI_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  case T_DF:\n+\t\t\t  case T_V2DF:\n+\t\t\t    eltype = double_pointer_node;\n+\t\t\t    break;\n+\n+\t\t\t  default:\n+\t\t\t    gcc_unreachable ();\n+\t\t\t  }\n+\t\t      }\n+\t\t    else\n+\t\t      {\n+\t\t\tswitch (insn_data[icode].operand[k].mode)\n+\t\t\t  {\n+\t\t\t  case VOIDmode:\n+\t\t\t    eltype = void_type_node;\n+\t\t\t    break;\n+\t\t\t    /* Scalars.  */\n+\t\t\t  case QImode:\n+\t\t\t    eltype = aarch64_simd_intQI_type_node;\n+\t\t\t    break;\n+\t\t\t  case HImode:\n+\t\t\t    eltype = aarch64_simd_intHI_type_node;\n+\t\t\t    break;\n+\t\t\t  case SImode:\n+\t\t\t    eltype = aarch64_simd_intSI_type_node;\n+\t\t\t    break;\n+\t\t\t  case SFmode:\n+\t\t\t    eltype = aarch64_simd_float_type_node;\n+\t\t\t    break;\n+\t\t\t  case DFmode:\n+\t\t\t    eltype = aarch64_simd_double_type_node;\n+\t\t\t    break;\n+\t\t\t  case DImode:\n+\t\t\t    eltype = aarch64_simd_intDI_type_node;\n+\t\t\t    break;\n+\t\t\t  case TImode:\n+\t\t\t    eltype = intTI_type_node;\n+\t\t\t    break;\n+\t\t\t  case EImode:\n+\t\t\t    eltype = intEI_type_node;\n+\t\t\t    break;\n+\t\t\t  case OImode:\n+\t\t\t    eltype = intOI_type_node;\n+\t\t\t    break;\n+\t\t\t  case CImode:\n+\t\t\t    eltype = intCI_type_node;\n+\t\t\t    break;\n+\t\t\t  case XImode:\n+\t\t\t    eltype = intXI_type_node;\n+\t\t\t    break;\n+\t\t\t    /* 64-bit vectors.  */\n+\t\t\t  case V8QImode:\n+\t\t\t    eltype = V8QI_type_node;\n+\t\t\t    break;\n+\t\t\t  case V4HImode:\n+\t\t\t    eltype = V4HI_type_node;\n+\t\t\t    break;\n+\t\t\t  case V2SImode:\n+\t\t\t    eltype = V2SI_type_node;\n+\t\t\t    break;\n+\t\t\t  case V2SFmode:\n+\t\t\t    eltype = V2SF_type_node;\n+\t\t\t    break;\n+\t\t\t    /* 128-bit vectors.  */\n+\t\t\t  case V16QImode:\n+\t\t\t    eltype = V16QI_type_node;\n+\t\t\t    break;\n+\t\t\t  case V8HImode:\n+\t\t\t    eltype = V8HI_type_node;\n+\t\t\t    break;\n+\t\t\t  case V4SImode:\n+\t\t\t    eltype = V4SI_type_node;\n+\t\t\t    break;\n+\t\t\t  case V4SFmode:\n+\t\t\t    eltype = V4SF_type_node;\n+\t\t\t    break;\n+\t\t\t  case V2DImode:\n+\t\t\t    eltype = V2DI_type_node;\n+\t\t\t    break;\n+\t\t\t  case V2DFmode:\n+\t\t\t    eltype = V2DF_type_node;\n+\t\t\t    break;\n+\t\t\t  default:\n+\t\t\t    gcc_unreachable ();\n+\t\t\t  }\n+\t\t      }\n+\n+\t\t    if (k == 0 && !is_store)\n+\t\t      return_type = eltype;\n+\t\t    else\n+\t\t      args = tree_cons (NULL_TREE, eltype, args);\n+\t\t  }\n+\n+\t\tftype = build_function_type (return_type, args);\n+\t      }\n+\t      break;\n+\n+\t    case AARCH64_SIMD_RESULTPAIR:\n+\t      {\n+\t\tswitch (insn_data[icode].operand[1].mode)\n+\t\t  {\n+\t\t  case V8QImode:\n+\t\t    ftype = void_ftype_pv8qi_v8qi_v8qi;\n+\t\t    break;\n+\t\t  case V4HImode:\n+\t\t    ftype = void_ftype_pv4hi_v4hi_v4hi;\n+\t\t    break;\n+\t\t  case V2SImode:\n+\t\t    ftype = void_ftype_pv2si_v2si_v2si;\n+\t\t    break;\n+\t\t  case V2SFmode:\n+\t\t    ftype = void_ftype_pv2sf_v2sf_v2sf;\n+\t\t    break;\n+\t\t  case DImode:\n+\t\t    ftype = void_ftype_pdi_di_di;\n+\t\t    break;\n+\t\t  case V16QImode:\n+\t\t    ftype = void_ftype_pv16qi_v16qi_v16qi;\n+\t\t    break;\n+\t\t  case V8HImode:\n+\t\t    ftype = void_ftype_pv8hi_v8hi_v8hi;\n+\t\t    break;\n+\t\t  case V4SImode:\n+\t\t    ftype = void_ftype_pv4si_v4si_v4si;\n+\t\t    break;\n+\t\t  case V4SFmode:\n+\t\t    ftype = void_ftype_pv4sf_v4sf_v4sf;\n+\t\t    break;\n+\t\t  case V2DImode:\n+\t\t    ftype = void_ftype_pv2di_v2di_v2di;\n+\t\t    break;\n+\t\t  case V2DFmode:\n+\t\t    ftype = void_ftype_pv2df_v2df_v2df;\n+\t\t    break;\n+\t\t  default:\n+\t\t    gcc_unreachable ();\n+\t\t  }\n+\t      }\n+\t      break;\n+\n+\t    case AARCH64_SIMD_REINTERP:\n+\t      {\n+\t\t/* We iterate over 6 doubleword types, then 6 quadword\n+\t\t   types.  */\n+\t\tint rhs_d = j % NUM_DREG_TYPES;\n+\t\tint rhs_q = (j - NUM_DREG_TYPES) % NUM_QREG_TYPES;\n+\t\tswitch (insn_data[icode].operand[0].mode)\n+\t\t  {\n+\t\t  case V8QImode:\n+\t\t    ftype = reinterp_ftype_dreg[0][rhs_d];\n+\t\t    break;\n+\t\t  case V4HImode:\n+\t\t    ftype = reinterp_ftype_dreg[1][rhs_d];\n+\t\t    break;\n+\t\t  case V2SImode:\n+\t\t    ftype = reinterp_ftype_dreg[2][rhs_d];\n+\t\t    break;\n+\t\t  case V2SFmode:\n+\t\t    ftype = reinterp_ftype_dreg[3][rhs_d];\n+\t\t    break;\n+\t\t  case DImode:\n+\t\t    ftype = reinterp_ftype_dreg[4][rhs_d];\n+\t\t    break;\n+\t\t  case DFmode:\n+\t\t    ftype = reinterp_ftype_dreg[5][rhs_d];\n+\t\t    break;\n+\t\t  case V16QImode:\n+\t\t    ftype = reinterp_ftype_qreg[0][rhs_q];\n+\t\t    break;\n+\t\t  case V8HImode:\n+\t\t    ftype = reinterp_ftype_qreg[1][rhs_q];\n+\t\t    break;\n+\t\t  case V4SImode:\n+\t\t    ftype = reinterp_ftype_qreg[2][rhs_q];\n+\t\t    break;\n+\t\t  case V4SFmode:\n+\t\t    ftype = reinterp_ftype_qreg[3][rhs_q];\n+\t\t    break;\n+\t\t  case V2DImode:\n+\t\t    ftype = reinterp_ftype_qreg[4][rhs_q];\n+\t\t    break;\n+\t\t  case V2DFmode:\n+\t\t    ftype = reinterp_ftype_qreg[5][rhs_q];\n+\t\t    break;\n+\t\t  default:\n+\t\t    gcc_unreachable ();\n+\t\t  }\n+\t      }\n+\t      break;\n+\n+\t    default:\n+\t      gcc_unreachable ();\n+\t    }\n+\n+\t  gcc_assert (ftype != NULL);\n+\n+\t  snprintf (namebuf, sizeof (namebuf), \"__builtin_aarch64_%s%s\",\n+\t\t    d->name, modenames[j]);\n+\n+\t  add_builtin_function (namebuf, ftype, fcode++, BUILT_IN_MD, NULL,\n+\t\t\t\tNULL_TREE);\n+\t}\n+    }\n+}\n+\n+static int\n+aarch64_simd_builtin_compare (const void *a, const void *b)\n+{\n+  const aarch64_simd_builtin_datum *const key =\n+    (const aarch64_simd_builtin_datum *) a;\n+  const aarch64_simd_builtin_datum *const memb =\n+    (const aarch64_simd_builtin_datum *) b;\n+  unsigned int soughtcode = key->base_fcode;\n+\n+  if (soughtcode >= memb->base_fcode\n+      && soughtcode < memb->base_fcode + memb->num_vars)\n+    return 0;\n+  else if (soughtcode < memb->base_fcode)\n+    return -1;\n+  else\n+    return 1;\n+}\n+\n+\n+static enum insn_code\n+locate_simd_builtin_icode (int fcode, aarch64_simd_itype * itype)\n+{\n+  aarch64_simd_builtin_datum key\n+    = { NULL, (aarch64_simd_itype) 0, 0, {CODE_FOR_nothing}, 0, 0};\n+  aarch64_simd_builtin_datum *found;\n+  int idx;\n+\n+  key.base_fcode = fcode;\n+  found = (aarch64_simd_builtin_datum *)\n+    bsearch (&key, &aarch64_simd_builtin_data[0],\n+\t     ARRAY_SIZE (aarch64_simd_builtin_data),\n+\t     sizeof (aarch64_simd_builtin_data[0]),\n+\t     aarch64_simd_builtin_compare);\n+  gcc_assert (found);\n+  idx = fcode - (int) found->base_fcode;\n+  gcc_assert (idx >= 0 && idx < T_MAX && idx < (int) found->num_vars);\n+\n+  if (itype)\n+    *itype = found->itype;\n+\n+  return found->codes[idx];\n+}\n+\n+typedef enum\n+{\n+  SIMD_ARG_COPY_TO_REG,\n+  SIMD_ARG_CONSTANT,\n+  SIMD_ARG_STOP\n+} builtin_simd_arg;\n+\n+#define SIMD_MAX_BUILTIN_ARGS 5\n+\n+static rtx\n+aarch64_simd_expand_args (rtx target, int icode, int have_retval,\n+\t\t\t  tree exp, ...)\n+{\n+  va_list ap;\n+  rtx pat;\n+  tree arg[SIMD_MAX_BUILTIN_ARGS];\n+  rtx op[SIMD_MAX_BUILTIN_ARGS];\n+  enum machine_mode tmode = insn_data[icode].operand[0].mode;\n+  enum machine_mode mode[SIMD_MAX_BUILTIN_ARGS];\n+  int argc = 0;\n+\n+  if (have_retval\n+      && (!target\n+\t  || GET_MODE (target) != tmode\n+\t  || !(*insn_data[icode].operand[0].predicate) (target, tmode)))\n+    target = gen_reg_rtx (tmode);\n+\n+  va_start (ap, exp);\n+\n+  for (;;)\n+    {\n+      builtin_simd_arg thisarg = (builtin_simd_arg) va_arg (ap, int);\n+\n+      if (thisarg == SIMD_ARG_STOP)\n+\tbreak;\n+      else\n+\t{\n+\t  arg[argc] = CALL_EXPR_ARG (exp, argc);\n+\t  op[argc] = expand_normal (arg[argc]);\n+\t  mode[argc] = insn_data[icode].operand[argc + have_retval].mode;\n+\n+\t  switch (thisarg)\n+\t    {\n+\t    case SIMD_ARG_COPY_TO_REG:\n+\t      /*gcc_assert (GET_MODE (op[argc]) == mode[argc]); */\n+\t      if (!(*insn_data[icode].operand[argc + have_retval].predicate)\n+\t\t  (op[argc], mode[argc]))\n+\t\top[argc] = copy_to_mode_reg (mode[argc], op[argc]);\n+\t      break;\n+\n+\t    case SIMD_ARG_CONSTANT:\n+\t      if (!(*insn_data[icode].operand[argc + have_retval].predicate)\n+\t\t  (op[argc], mode[argc]))\n+\t\terror_at (EXPR_LOCATION (exp), \"incompatible type for argument %d, \"\n+\t\t       \"expected %<const int%>\", argc + 1);\n+\t      break;\n+\n+\t    case SIMD_ARG_STOP:\n+\t      gcc_unreachable ();\n+\t    }\n+\n+\t  argc++;\n+\t}\n+    }\n+\n+  va_end (ap);\n+\n+  if (have_retval)\n+    switch (argc)\n+      {\n+      case 1:\n+\tpat = GEN_FCN (icode) (target, op[0]);\n+\tbreak;\n+\n+      case 2:\n+\tpat = GEN_FCN (icode) (target, op[0], op[1]);\n+\tbreak;\n+\n+      case 3:\n+\tpat = GEN_FCN (icode) (target, op[0], op[1], op[2]);\n+\tbreak;\n+\n+      case 4:\n+\tpat = GEN_FCN (icode) (target, op[0], op[1], op[2], op[3]);\n+\tbreak;\n+\n+      case 5:\n+\tpat = GEN_FCN (icode) (target, op[0], op[1], op[2], op[3], op[4]);\n+\tbreak;\n+\n+      default:\n+\tgcc_unreachable ();\n+      }\n+  else\n+    switch (argc)\n+      {\n+      case 1:\n+\tpat = GEN_FCN (icode) (op[0]);\n+\tbreak;\n+\n+      case 2:\n+\tpat = GEN_FCN (icode) (op[0], op[1]);\n+\tbreak;\n+\n+      case 3:\n+\tpat = GEN_FCN (icode) (op[0], op[1], op[2]);\n+\tbreak;\n+\n+      case 4:\n+\tpat = GEN_FCN (icode) (op[0], op[1], op[2], op[3]);\n+\tbreak;\n+\n+      case 5:\n+\tpat = GEN_FCN (icode) (op[0], op[1], op[2], op[3], op[4]);\n+\tbreak;\n+\n+      default:\n+\tgcc_unreachable ();\n+      }\n+\n+  if (!pat)\n+    return 0;\n+\n+  emit_insn (pat);\n+\n+  return target;\n+}\n+\n+/* Expand an AArch64 AdvSIMD builtin(intrinsic).  */\n+rtx\n+aarch64_simd_expand_builtin (int fcode, tree exp, rtx target)\n+{\n+  aarch64_simd_itype itype;\n+  enum insn_code icode = locate_simd_builtin_icode (fcode, &itype);\n+\n+  switch (itype)\n+    {\n+    case AARCH64_SIMD_UNOP:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_STOP);\n+\n+    case AARCH64_SIMD_BINOP:\n+      {\n+        rtx arg2 = expand_normal (CALL_EXPR_ARG (exp, 1));\n+        /* Handle constants only if the predicate allows it.  */\n+\tbool op1_const_int_p =\n+\t  (CONST_INT_P (arg2)\n+\t   && (*insn_data[icode].operand[2].predicate)\n+\t\t(arg2, insn_data[icode].operand[2].mode));\n+\treturn aarch64_simd_expand_args\n+\t  (target, icode, 1, exp,\n+\t   SIMD_ARG_COPY_TO_REG,\n+\t   op1_const_int_p ? SIMD_ARG_CONSTANT : SIMD_ARG_COPY_TO_REG,\n+\t   SIMD_ARG_STOP);\n+      }\n+\n+    case AARCH64_SIMD_TERNOP:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_STOP);\n+\n+    case AARCH64_SIMD_QUADOP:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_STOP);\n+    case AARCH64_SIMD_LOAD1:\n+    case AARCH64_SIMD_LOADSTRUCT:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG, SIMD_ARG_STOP);\n+\n+    case AARCH64_SIMD_STORESTRUCT:\n+      return aarch64_simd_expand_args (target, icode, 0, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG, SIMD_ARG_STOP);\n+\n+    case AARCH64_SIMD_REINTERP:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG, SIMD_ARG_STOP);\n+\n+    case AARCH64_SIMD_CREATE:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG, SIMD_ARG_STOP);\n+\n+    case AARCH64_SIMD_COMBINE:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG, SIMD_ARG_STOP);\n+\n+    case AARCH64_SIMD_GETLANE:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_CONSTANT,\n+\t\t\t\t       SIMD_ARG_STOP);\n+\n+    case AARCH64_SIMD_SETLANE:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_CONSTANT,\n+\t\t\t\t       SIMD_ARG_STOP);\n+\n+    case AARCH64_SIMD_SHIFTIMM:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_CONSTANT,\n+\t\t\t\t       SIMD_ARG_STOP);\n+\n+    case AARCH64_SIMD_SHIFTACC:\n+    case AARCH64_SIMD_SHIFTINSERT:\n+      return aarch64_simd_expand_args (target, icode, 1, exp,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_COPY_TO_REG,\n+\t\t\t\t       SIMD_ARG_CONSTANT,\n+\t\t\t\t       SIMD_ARG_STOP);\n+\n+    default:\n+      gcc_unreachable ();\n+    }\n+}"}, {"sha": "06cc9825d3987568253d45ae0ee49ad0c021f24c", "filename": "gcc/config/aarch64/aarch64-cores.def", "status": "added", "additions": 38, "deletions": 0, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-cores.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-cores.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-cores.def?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,38 @@\n+/* Copyright (C) 2011, 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but\n+   WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+   General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+/* This is a list of cores that implement AArch64.\n+\n+   Before using #include to read this file, define a macro:\n+\n+      AARCH64_CORE(CORE_NAME, CORE_IDENT, ARCH, FLAGS, COSTS)\n+\n+   The CORE_NAME is the name of the core, represented as a string constant.\n+   The CORE_IDENT is the name of the core, represented as an identifier.\n+   ARCH is the architecture revision implemented by the chip.\n+   FLAGS are the bitwise-or of the traits that apply to that core.\n+   This need not include flags implied by the architecture.\n+   COSTS is the name of the rtx_costs routine to use.  */\n+\n+/* V8 Architecture Processors.\n+   This list currently contains example CPUs that implement AArch64, and\n+   therefore serves as a template for adding more CPUs in the future.  */\n+\n+AARCH64_CORE(\"example-1\",\t      large,\t     8,  AARCH64_FL_FPSIMD,    generic)\n+AARCH64_CORE(\"example-2\",\t      small,\t     8,  AARCH64_FL_FPSIMD,    generic)"}, {"sha": "d9ec53ff60ea3f25f67c3413e313616570c4592c", "filename": "gcc/config/aarch64/aarch64-elf-raw.h", "status": "added", "additions": 32, "deletions": 0, "changes": 32, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-elf-raw.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-elf-raw.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-elf-raw.h?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,32 @@\n+/* Machine description for AArch64 architecture.\n+   Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but\n+   WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+   General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+/* Support for bare-metal builds.  */\n+#ifndef GCC_AARCH64_ELF_RAW_H\n+#define GCC_AARCH64_ELF_RAW_H\n+\n+#define STARTFILE_SPEC \" crti%O%s crtbegin%O%s crt0%O%s\"\n+#define ENDFILE_SPEC \" crtend%O%s crtn%O%s\"\n+\n+#ifndef LINK_SPEC\n+#define LINK_SPEC \"%{mbig-endian:-EB} %{mlittle-endian:-EL} -X\"\n+#endif\n+\n+#endif /* GCC_AARCH64_ELF_RAW_H */"}, {"sha": "1c021d0ec05431b8225fb305ef66f549556722f6", "filename": "gcc/config/aarch64/aarch64-elf.h", "status": "added", "additions": 132, "deletions": 0, "changes": 132, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-elf.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-elf.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-elf.h?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,132 @@\n+/* Machine description for AArch64 architecture.\n+   Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but\n+   WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+   General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_AARCH64_ELF_H\n+#define GCC_AARCH64_ELF_H\n+\n+\n+#define ASM_OUTPUT_LABELREF(FILE, NAME) \\\n+  aarch64_asm_output_labelref (FILE, NAME)\n+\n+#define ASM_OUTPUT_DEF(FILE, NAME1, NAME2)\t\\\n+  do\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\\\n+      assemble_name (FILE, NAME1);\t\t\\\n+      fputs (\" = \", FILE);\t\t\t\\\n+      assemble_name (FILE, NAME2);\t\t\\\n+      fputc ('\\n', FILE);\t\t\t\\\n+    } while (0)\n+\n+#define TEXT_SECTION_ASM_OP\t\"\\t.text\"\n+#define DATA_SECTION_ASM_OP\t\"\\t.data\"\n+#define BSS_SECTION_ASM_OP\t\"\\t.bss\"\n+\n+#define CTORS_SECTION_ASM_OP \"\\t.section\\t.init_array,\\\"aw\\\",%init_array\"\n+#define DTORS_SECTION_ASM_OP \"\\t.section\\t.fini_array,\\\"aw\\\",%fini_array\"\n+\n+#undef INIT_SECTION_ASM_OP\n+#undef FINI_SECTION_ASM_OP\n+#define INIT_ARRAY_SECTION_ASM_OP CTORS_SECTION_ASM_OP\n+#define FINI_ARRAY_SECTION_ASM_OP DTORS_SECTION_ASM_OP\n+\n+/* Since we use .init_array/.fini_array we don't need the markers at\n+   the start and end of the ctors/dtors arrays.  */\n+#define CTOR_LIST_BEGIN asm (CTORS_SECTION_ASM_OP)\n+#define CTOR_LIST_END\t\t/* empty */\n+#define DTOR_LIST_BEGIN asm (DTORS_SECTION_ASM_OP)\n+#define DTOR_LIST_END\t\t/* empty */\n+\n+#undef TARGET_ASM_CONSTRUCTOR\n+#define TARGET_ASM_CONSTRUCTOR aarch64_elf_asm_constructor\n+\n+#undef TARGET_ASM_DESTRUCTOR\n+#define TARGET_ASM_DESTRUCTOR aarch64_elf_asm_destructor\n+\n+#ifdef HAVE_GAS_MAX_SKIP_P2ALIGN\n+/* Support for -falign-* switches.  Use .p2align to ensure that code\n+   sections are padded with NOP instructions, rather than zeros.  */\n+#define ASM_OUTPUT_MAX_SKIP_ALIGN(FILE, LOG, MAX_SKIP)\t\t\\\n+  do\t\t\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\\\n+      if ((LOG) != 0)\t\t\t\t\t\t\\\n+\t{\t\t\t\t\t\t\t\\\n+\t  if ((MAX_SKIP) == 0)\t\t\t\t\t\\\n+\t    fprintf ((FILE), \"\\t.p2align %d\\n\", (int) (LOG));\t\\\n+\t  else\t\t\t\t\t\t\t\\\n+\t    fprintf ((FILE), \"\\t.p2align %d,,%d\\n\",\t\t\\\n+\t\t     (int) (LOG), (int) (MAX_SKIP));\t\t\\\n+\t}\t\t\t\t\t\t\t\\\n+    } while (0)\n+\n+#endif /* HAVE_GAS_MAX_SKIP_P2ALIGN */\n+\n+#define JUMP_TABLES_IN_TEXT_SECTION 0\n+\n+#define ASM_OUTPUT_ADDR_DIFF_ELT(STREAM, BODY, VALUE, REL)\t\t\\\n+  do {\t\t\t\t\t\t\t\t\t\\\n+    switch (GET_MODE (BODY))\t\t\t\t\t\t\\\n+      {\t\t\t\t\t\t\t\t\t\\\n+      case QImode:\t\t\t\t\t\t\t\\\n+\tasm_fprintf (STREAM, \"\\t.byte\\t(%LL%d - %LLrtx%d) / 4\\n\",\t\\\n+\t\t     VALUE, REL);\t\t\t\t\t\\\n+\tbreak;\t\t\t\t\t\t\t\t\\\n+      case HImode:\t\t\t\t\t\t\t\\\n+\tasm_fprintf (STREAM, \"\\t.2byte\\t(%LL%d - %LLrtx%d) / 4\\n\",\t\\\n+\t\t     VALUE, REL);\t\t\t\t\t\\\n+\tbreak;\t\t\t\t\t\t\t\t\\\n+      case SImode:\t\t\t\t\t\t\t\\\n+      case DImode: /* See comment in aarch64_output_casesi.  */\t\t\\\n+\tasm_fprintf (STREAM, \"\\t.word\\t(%LL%d - %LLrtx%d) / 4\\n\",\t\\\n+\t\t     VALUE, REL);\t\t\t\t\t\\\n+\tbreak;\t\t\t\t\t\t\t\t\\\n+      default:\t\t\t\t\t\t\t\t\\\n+\tgcc_unreachable ();\t\t\t\t\t\t\\\n+      }\t\t\t\t\t\t\t\t\t\\\n+  } while (0)\n+\n+#define ASM_OUTPUT_ALIGN(STREAM, POWER)\t\t\\\n+  fprintf(STREAM, \"\\t.align\\t%d\\n\", (int)POWER)\n+\n+#define ASM_COMMENT_START \"//\"\n+\n+#define REGISTER_PREFIX\t\t\"\"\n+#define LOCAL_LABEL_PREFIX\t\".\"\n+#define USER_LABEL_PREFIX\t\"\"\n+\n+#define GLOBAL_ASM_OP \"\\t.global\\t\"\n+\n+#ifndef ASM_SPEC\n+#define ASM_SPEC \"\\\n+%{mbig-endian:-EB} \\\n+%{mlittle-endian:-EL} \\\n+%{mcpu=*:-mcpu=%*} \\\n+%{march=*:-march=%*}\"\n+#endif\n+\n+#undef TYPE_OPERAND_FMT\n+#define TYPE_OPERAND_FMT\t\"%%%s\"\n+\n+#undef TARGET_ASM_NAMED_SECTION\n+#define TARGET_ASM_NAMED_SECTION  aarch64_elf_asm_named_section\n+\n+/* Stabs debug not required.  */\n+#undef DBX_DEBUGGING_INFO\n+\n+#endif /* GCC_AARCH64_ELF_H */"}, {"sha": "4c9e4555b3fa8400de57e217ec6ab7da998c3bf7", "filename": "gcc/config/aarch64/aarch64-generic.md", "status": "added", "additions": 38, "deletions": 0, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-generic.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-generic.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-generic.md?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,38 @@\n+;; Machine description for AArch64 architecture.\n+;; Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+;; Contributed by ARM Ltd.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+;; Generic scheduler\n+\n+(define_automaton \"aarch64\")\n+\n+(define_cpu_unit \"core\" \"aarch64\")\n+\n+(define_attr \"is_load\" \"yes,no\"\n+  (if_then_else (eq_attr \"v8type\" \"fpsimd_load,fpsimd_load2,load1,load2\")\n+\t(const_string \"yes\")\n+\t(const_string \"no\")))\n+\n+(define_insn_reservation \"load\" 2\n+  (eq_attr \"is_load\" \"yes\")\n+  \"core\")\n+\n+(define_insn_reservation \"nonload\" 1\n+  (eq_attr \"is_load\" \"no\")\n+  \"core\")"}, {"sha": "95aaafab95ab3b7a82a17a57ae310bfc2513565e", "filename": "gcc/config/aarch64/aarch64-linux.h", "status": "added", "additions": 44, "deletions": 0, "changes": 44, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-linux.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-linux.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-linux.h?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,44 @@\n+/* Machine description for AArch64 architecture.\n+   Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but\n+   WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+   General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_AARCH64_LINUX_H\n+#define GCC_AARCH64_LINUX_H\n+\n+#define GLIBC_DYNAMIC_LINKER \"/lib/ld-linux-aarch64.so.1\"\n+\n+#define LINUX_TARGET_LINK_SPEC  \"%{h*}\t\t\\\n+   %{static:-Bstatic}\t\t\t\t\\\n+   %{shared:-shared}\t\t\t\t\\\n+   %{symbolic:-Bsymbolic}\t\t\t\\\n+   %{rdynamic:-export-dynamic}\t\t\t\\\n+   -dynamic-linker \" GNU_USER_DYNAMIC_LINKER \"\t\\\n+   -X\t\t\t\t\t\t\\\n+   %{mbig-endian:-EB} %{mlittle-endian:-EL}\"\n+\n+#define LINK_SPEC LINUX_TARGET_LINK_SPEC\n+\n+#define TARGET_OS_CPP_BUILTINS()\t\t\\\n+  do\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\\\n+\tGNU_USER_TARGET_OS_CPP_BUILTINS();\t\\\n+    }\t\t\t\t\t\t\\\n+  while (0)\n+\n+#endif  /* GCC_AARCH64_LINUX_H */"}, {"sha": "ac05881f9ca62f01cdf4ed3320431bde45f7daf3", "filename": "gcc/config/aarch64/aarch64-modes.def", "status": "added", "additions": 54, "deletions": 0, "changes": 54, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,54 @@\n+/* Machine description for AArch64 architecture.\n+   Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but\n+   WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+   General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+CC_MODE (CCFP);\n+CC_MODE (CCFPE);\n+CC_MODE (CC_SWP);\n+CC_MODE (CC_ZESWP); /* zero-extend LHS (but swap to make it RHS).  */\n+CC_MODE (CC_SESWP); /* sign-extend LHS (but swap to make it RHS).  */\n+CC_MODE (CC_NZ);    /* Only N and Z bits of condition flags are valid.  */\n+\n+/* Vector modes.  */\n+VECTOR_MODES (INT, 8);        /*       V8QI V4HI V2SI.  */\n+VECTOR_MODES (INT, 16);       /* V16QI V8HI V4SI V2DI.  */\n+VECTOR_MODES (FLOAT, 8);      /*                 V2SF.  */\n+VECTOR_MODES (FLOAT, 16);     /*            V4SF V2DF.  */\n+\n+/* Oct Int: 256-bit integer mode needed for 32-byte vector arguments.  */\n+INT_MODE (OI, 32);\n+\n+/* Opaque integer modes for 3, 6 or 8 Neon double registers (2 is\n+   TImode).  */\n+INT_MODE (EI, 24);\n+INT_MODE (CI, 48);\n+INT_MODE (XI, 64);\n+\n+/* Vector modes for register lists.  */\n+VECTOR_MODES (INT, 32);\t\t/* V32QI V16HI V8SI V4DI.  */\n+VECTOR_MODES (FLOAT, 32);\t/* V8SF V4DF.  */\n+\n+VECTOR_MODES (INT, 48);\t\t/* V32QI V16HI V8SI V4DI.  */\n+VECTOR_MODES (FLOAT, 48);\t/* V8SF V4DF.  */\n+\n+VECTOR_MODES (INT, 64);\t\t/* V32QI V16HI V8SI V4DI.  */\n+VECTOR_MODES (FLOAT, 64);\t/* V8SF V4DF.  */\n+\n+/* Quad float: 128-bit floating mode for long doubles.  */\n+FLOAT_MODE (TF, 16, ieee_quad_format);"}, {"sha": "a5d298a610366e60e7ac3c74710823056dc67d17", "filename": "gcc/config/aarch64/aarch64-option-extensions.def", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-option-extensions.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-option-extensions.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-option-extensions.def?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,37 @@\n+/* Copyright (C) 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but\n+   WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+   General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+/* This is a list of ISA extentsions in AArch64.\n+\n+   Before using #include to read this file, define a macro:\n+\n+      AARCH64_OPT_EXTENSION(EXT_NAME, FLAGS_ON, FLAGS_OFF)\n+\n+   EXT_NAME is the name of the extension, represented as a string constant.\n+   FLAGS_ON are the bitwise-or of the features that the extension adds.\n+   FLAGS_OFF are the bitwise-or of the features that the extension removes.  */\n+\n+/* V8 Architecture Extensions.\n+   This list currently contains example extensions for CPUs that implement\n+   AArch64, and therefore serves as a template for adding more CPUs in the\n+   future.  */\n+\n+AARCH64_OPT_EXTENSION(\"fp\",\tAARCH64_FL_FP,\tAARCH64_FL_FPSIMD | AARCH64_FL_CRYPTO)\n+AARCH64_OPT_EXTENSION(\"simd\",\tAARCH64_FL_FPSIMD,\tAARCH64_FL_SIMD | AARCH64_FL_CRYPTO)\n+AARCH64_OPT_EXTENSION(\"crypto\",\tAARCH64_FL_CRYPTO | AARCH64_FL_FPSIMD,\tAARCH64_FL_CRYPTO)"}, {"sha": "6d7a2fdf969be9f47268d83638ae23b788c0fd22", "filename": "gcc/config/aarch64/aarch64-opts.h", "status": "added", "additions": 64, "deletions": 0, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-opts.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-opts.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-opts.h?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,64 @@\n+/* Copyright (C) 2011, 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published\n+   by the Free Software Foundation; either version 3, or (at your\n+   option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT\n+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY\n+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public\n+   License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+/* Definitions for option handling for AArch64.  */\n+\n+#ifndef GCC_AARCH64_OPTS_H\n+#define GCC_AARCH64_OPTS_H\n+\n+/* The various cores that implement AArch64.  */\n+enum aarch64_processor\n+{\n+#define AARCH64_CORE(NAME, IDENT, ARCH, FLAGS, COSTS) \\\n+  IDENT,\n+#include \"aarch64-cores.def\"\n+#undef AARCH64_CORE\n+  /* Used to indicate that no processor has been specified.  */\n+  generic,\n+  /* Used to mark the end of the processor table.  */\n+  aarch64_none\n+};\n+\n+/* TLS types.  */\n+enum aarch64_tls_type {\n+  TLS_TRADITIONAL,\n+  TLS_DESCRIPTORS\n+};\n+\n+/* The code model defines the address generation strategy.\n+   Most have a PIC and non-PIC variant.  */\n+enum aarch64_code_model {\n+  /* Static code and data fit within a 1MB region.\n+     Not fully implemented, mostly treated as SMALL.  */\n+  AARCH64_CMODEL_TINY,\n+  /* Static code, data and GOT/PLT fit within a 1MB region.\n+     Not fully implemented, mostly treated as SMALL_PIC.  */\n+  AARCH64_CMODEL_TINY_PIC,\n+  /* Static code and data fit within a 4GB region.\n+     The default non-PIC code model.  */\n+  AARCH64_CMODEL_SMALL,\n+  /* Static code, data and GOT/PLT fit within a 4GB region.\n+     The default PIC code model.  */\n+  AARCH64_CMODEL_SMALL_PIC,\n+  /* No assumptions about addresses of code and data.\n+     The PIC variant is not yet implemented.  */\n+  AARCH64_CMODEL_LARGE\n+};\n+\n+#endif"}, {"sha": "765d1920a737a4ccf0cc3694c185b4e95b1975bb", "filename": "gcc/config/aarch64/aarch64-protos.h", "status": "added", "additions": 260, "deletions": 0, "changes": 260, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,260 @@\n+/* Machine description for AArch64 architecture.\n+   Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but\n+   WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+   General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+\n+#ifndef GCC_AARCH64_PROTOS_H\n+#define GCC_AARCH64_PROTOS_H\n+\n+ /* This generator struct and enum is used to wrap a function pointer\n+    to a function that generates an RTX fragment but takes either 3 or\n+    4 operands.\n+\n+    The omn flavour, wraps a function that generates a synchronization\n+    instruction from 3 operands: old value, memory and new value.\n+\n+    The omrn flavour, wraps a function that generates a synchronization\n+    instruction from 4 operands: old value, memory, required value and\n+    new value.  */\n+\n+enum aarch64_sync_generator_tag\n+{\n+  aarch64_sync_generator_omn,\n+  aarch64_sync_generator_omrn\n+};\n+\n+ /* Wrapper to pass around a polymorphic pointer to a sync instruction\n+    generator and.  */\n+struct aarch64_sync_generator\n+{\n+  enum aarch64_sync_generator_tag op;\n+  union\n+  {\n+    rtx (*omn) (rtx, rtx, rtx);\n+    rtx (*omrn) (rtx, rtx, rtx, rtx);\n+  } u;\n+};\n+\n+/*\n+  SYMBOL_CONTEXT_ADR\n+  The symbol is used in a load-address operation.\n+  SYMBOL_CONTEXT_MEM\n+  The symbol is used as the address in a MEM.\n+ */\n+enum aarch64_symbol_context\n+{\n+  SYMBOL_CONTEXT_MEM,\n+  SYMBOL_CONTEXT_ADR\n+};\n+\n+/* SYMBOL_SMALL_ABSOLUTE: Generate symbol accesses through\n+   high and lo relocs that calculate the base address using a PC\n+   relative reloc.\n+   So to get the address of foo, we generate\n+   adrp x0, foo\n+   add  x0, x0, :lo12:foo\n+\n+   To load or store something to foo, we could use the corresponding\n+   load store variants that generate an\n+   ldr x0, [x0,:lo12:foo]\n+   or\n+   str x1, [x0, :lo12:foo]\n+\n+   This corresponds to the small code model of the compiler.\n+\n+   SYMBOL_SMALL_GOT: Similar to the one above but this\n+   gives us the GOT entry of the symbol being referred to :\n+   Thus calculating the GOT entry for foo is done using the\n+   following sequence of instructions.  The ADRP instruction\n+   gets us to the page containing the GOT entry of the symbol\n+   and the got_lo12 gets us the actual offset in it.\n+\n+   adrp  x0, :got:foo\n+   ldr   x0, [x0, :gotoff_lo12:foo]\n+\n+   This corresponds to the small PIC model of the compiler.\n+\n+   SYMBOL_SMALL_TLSGD\n+   SYMBOL_SMALL_TLSDESC\n+   SYMBOL_SMALL_GOTTPREL\n+   SYMBOL_SMALL_TPREL\n+   Each of of these represents a thread-local symbol, and corresponds to the\n+   thread local storage relocation operator for the symbol being referred to.\n+\n+   SYMBOL_FORCE_TO_MEM : Global variables are addressed using\n+   constant pool.  All variable addresses are spilled into constant\n+   pools.  The constant pools themselves are addressed using PC\n+   relative accesses.  This only works for the large code model.\n+ */\n+enum aarch64_symbol_type\n+{\n+  SYMBOL_SMALL_ABSOLUTE,\n+  SYMBOL_SMALL_GOT,\n+  SYMBOL_SMALL_TLSGD,\n+  SYMBOL_SMALL_TLSDESC,\n+  SYMBOL_SMALL_GOTTPREL,\n+  SYMBOL_SMALL_TPREL,\n+  SYMBOL_FORCE_TO_MEM\n+};\n+\n+/* A set of tuning parameters contains references to size and time\n+   cost models and vectors for address cost calculations, register\n+   move costs and memory move costs.  */\n+\n+/* Extra costs for specific insns.  Only records the cost above a\n+   single insn.  */\n+\n+struct cpu_rtx_cost_table\n+{\n+  const int memory_load;\n+  const int memory_store;\n+  const int register_shift;\n+  const int int_divide;\n+  const int float_divide;\n+  const int double_divide;\n+  const int int_multiply;\n+  const int int_multiply_extend;\n+  const int int_multiply_add;\n+  const int int_multiply_extend_add;\n+  const int float_multiply;\n+  const int double_multiply;\n+};\n+\n+/* Additional cost for addresses.  */\n+struct cpu_addrcost_table\n+{\n+  const int pre_modify;\n+  const int post_modify;\n+  const int register_offset;\n+  const int register_extend;\n+  const int imm_offset;\n+};\n+\n+/* Additional costs for register copies.  Cost is for one register.  */\n+struct cpu_regmove_cost\n+{\n+  const int GP2GP;\n+  const int GP2FP;\n+  const int FP2GP;\n+  const int FP2FP;\n+};\n+\n+struct tune_params\n+{\n+  const struct cpu_rtx_cost_table *const insn_extra_cost;\n+  const struct cpu_addrcost_table *const addr_cost;\n+  const struct cpu_regmove_cost *const regmove_cost;\n+  const int memmov_cost;\n+};\n+\n+HOST_WIDE_INT aarch64_initial_elimination_offset (unsigned, unsigned);\n+bool aarch64_bitmask_imm (HOST_WIDE_INT val, enum machine_mode);\n+bool aarch64_const_double_zero_rtx_p (rtx);\n+bool aarch64_constant_address_p (rtx);\n+bool aarch64_function_arg_regno_p (unsigned);\n+bool aarch64_gen_movmemqi (rtx *);\n+bool aarch64_is_extend_from_extract (enum machine_mode, rtx, rtx);\n+bool aarch64_is_long_call_p (rtx);\n+bool aarch64_label_mentioned_p (rtx);\n+bool aarch64_legitimate_pic_operand_p (rtx);\n+bool aarch64_move_imm (HOST_WIDE_INT, enum machine_mode);\n+bool aarch64_pad_arg_upward (enum machine_mode, const_tree);\n+bool aarch64_pad_reg_upward (enum machine_mode, const_tree, bool);\n+bool aarch64_regno_ok_for_base_p (int, bool);\n+bool aarch64_regno_ok_for_index_p (int, bool);\n+bool aarch64_simd_imm_scalar_p (rtx x, enum machine_mode mode);\n+bool aarch64_simd_imm_zero_p (rtx, enum machine_mode);\n+bool aarch64_simd_shift_imm_p (rtx, enum machine_mode, bool);\n+bool aarch64_symbolic_address_p (rtx);\n+bool aarch64_symbolic_constant_p (rtx, enum aarch64_symbol_context,\n+\t\t\t\t  enum aarch64_symbol_type *);\n+bool aarch64_uimm12_shift (HOST_WIDE_INT);\n+const char *aarch64_output_casesi (rtx *);\n+const char *aarch64_output_sync_insn (rtx, rtx *);\n+const char *aarch64_output_sync_lock_release (rtx, rtx);\n+enum aarch64_symbol_type aarch64_classify_symbol (rtx,\n+\t\t\t\t\t\t  enum aarch64_symbol_context);\n+enum aarch64_symbol_type aarch64_classify_tls_symbol (rtx);\n+enum reg_class aarch64_regno_regclass (unsigned);\n+int aarch64_asm_preferred_eh_data_format (int, int);\n+int aarch64_hard_regno_mode_ok (unsigned, enum machine_mode);\n+int aarch64_hard_regno_nregs (unsigned, enum machine_mode);\n+int aarch64_simd_attr_length_move (rtx);\n+int aarch64_simd_immediate_valid_for_move (rtx, enum machine_mode, rtx *,\n+\t\t\t\t\t   int *, unsigned char *, int *,\n+\t\t\t\t\t   int *);\n+int aarch64_uxt_size (int, HOST_WIDE_INT);\n+rtx aarch64_final_eh_return_addr (void);\n+rtx aarch64_legitimize_reload_address (rtx *, enum machine_mode, int, int, int);\n+const char *aarch64_output_move_struct (rtx *operands);\n+rtx aarch64_return_addr (int, rtx);\n+rtx aarch64_simd_gen_const_vector_dup (enum machine_mode, int);\n+bool aarch64_simd_mem_operand_p (rtx);\n+rtx aarch64_simd_vect_par_cnst_half (enum machine_mode, bool);\n+rtx aarch64_tls_get_addr (void);\n+unsigned aarch64_dbx_register_number (unsigned);\n+unsigned aarch64_trampoline_size (void);\n+unsigned aarch64_sync_loop_insns (rtx, rtx *);\n+void aarch64_asm_output_labelref (FILE *, const char *);\n+void aarch64_elf_asm_named_section (const char *, unsigned, tree);\n+void aarch64_expand_epilogue (bool);\n+void aarch64_expand_mov_immediate (rtx, rtx);\n+void aarch64_expand_prologue (void);\n+void aarch64_expand_sync (enum machine_mode, struct aarch64_sync_generator *,\n+\t\t\t  rtx, rtx, rtx, rtx);\n+void aarch64_function_profiler (FILE *, int);\n+void aarch64_init_cumulative_args (CUMULATIVE_ARGS *, const_tree, rtx,\n+\t\t\t\t   const_tree, unsigned);\n+void aarch64_init_expanders (void);\n+void aarch64_print_operand (FILE *, rtx, char);\n+void aarch64_print_operand_address (FILE *, rtx);\n+\n+/* Initialize builtins for SIMD intrinsics.  */\n+void init_aarch64_simd_builtins (void);\n+\n+void aarch64_simd_const_bounds (rtx, HOST_WIDE_INT, HOST_WIDE_INT);\n+void aarch64_simd_disambiguate_copy (rtx *, rtx *, rtx *, unsigned int);\n+\n+/* Emit code to place a AdvSIMD pair result in memory locations (with equal\n+   registers).  */\n+void aarch64_simd_emit_pair_result_insn (enum machine_mode,\n+\t\t\t\t\t rtx (*intfn) (rtx, rtx, rtx), rtx,\n+\t\t\t\t\t rtx);\n+\n+/* Expand builtins for SIMD intrinsics.  */\n+rtx aarch64_simd_expand_builtin (int, tree, rtx);\n+\n+void aarch64_simd_lane_bounds (rtx, HOST_WIDE_INT, HOST_WIDE_INT);\n+\n+/* Emit code for reinterprets.  */\n+void aarch64_simd_reinterpret (rtx, rtx);\n+\n+void aarch64_split_128bit_move (rtx, rtx);\n+\n+bool aarch64_split_128bit_move_p (rtx, rtx);\n+\n+#if defined (RTX_CODE)\n+\n+bool aarch64_legitimate_address_p (enum machine_mode, rtx, RTX_CODE, bool);\n+enum machine_mode aarch64_select_cc_mode (RTX_CODE, rtx, rtx);\n+rtx aarch64_gen_compare_reg (RTX_CODE, rtx, rtx);\n+\n+#endif /* RTX_CODE */\n+\n+#endif /* GCC_AARCH64_PROTOS_H */"}, {"sha": "a7ddfb1c1d3f64bbabd939fa5236c5549395a8cc", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "added", "additions": 3264, "deletions": 0, "changes": 3264, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360"}, {"sha": "a654a91b43b531cf7fed6396ec77f0a1f761e06f", "filename": "gcc/config/aarch64/aarch64-tune.md", "status": "added", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-tune.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64-tune.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-tune.md?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,5 @@\n+;; -*- buffer-read-only: t -*-\n+;; Generated automatically by gentune.sh from aarch64-cores.def\n+(define_attr \"tune\"\n+\t\"large,small\"\n+\t(const (symbol_ref \"((enum attr_tune) aarch64_tune)\")))"}, {"sha": "1bc0c8a0b4e93ea210f067b21a4be30832f47c78", "filename": "gcc/config/aarch64/aarch64.c", "status": "added", "additions": 7011, "deletions": 0, "changes": 7011, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360"}, {"sha": "3b8b033da0c377dc3fd55415ebc6e0d4ff3c1848", "filename": "gcc/config/aarch64/aarch64.h", "status": "added", "additions": 837, "deletions": 0, "changes": 837, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.h?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,837 @@\n+/* Machine description for AArch64 architecture.\n+   Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but\n+   WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+   General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+\n+#ifndef GCC_AARCH64_H\n+#define GCC_AARCH64_H\n+\n+/* Target CPU builtins.  */\n+#define TARGET_CPU_CPP_BUILTINS()\t\t\t\\\n+  do\t\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\\\n+      builtin_define (\"__aarch64__\");\t\t\t\\\n+      if (TARGET_BIG_END)\t\t\t\t\\\n+\tbuiltin_define (\"__AARCH64EB__\");\t\t\\\n+      else\t\t\t\t\t\t\\\n+\tbuiltin_define (\"__AARCH64EL__\");\t\t\\\n+\t\t\t\t\t\t\t\\\n+      switch (aarch64_cmodel)\t\t\t\t\\\n+\t{\t\t\t\t\t\t\\\n+\t  case AARCH64_CMODEL_TINY:\t\t\t\\\n+\t  case AARCH64_CMODEL_TINY_PIC:\t\t\t\\\n+\t    builtin_define (\"__AARCH64_CMODEL_TINY__\");\t\\\n+\t    break;\t\t\t\t\t\\\n+\t  case AARCH64_CMODEL_SMALL:\t\t\t\\\n+\t  case AARCH64_CMODEL_SMALL_PIC:\t\t\\\n+\t    builtin_define (\"__AARCH64_CMODEL_SMALL__\");\\\n+\t    break;\t\t\t\t\t\\\n+\t  case AARCH64_CMODEL_LARGE:\t\t\t\\\n+\t    builtin_define (\"__AARCH64_CMODEL_LARGE__\");\t\\\n+\t    break;\t\t\t\t\t\\\n+\t  default:\t\t\t\t\t\\\n+\t    break;\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\\\n+    } while (0)\n+\n+\f\n+\n+/* Target machine storage layout.  */\n+\n+#define PROMOTE_MODE(MODE, UNSIGNEDP, TYPE)\t\\\n+  if (GET_MODE_CLASS (MODE) == MODE_INT\t\t\\\n+      && GET_MODE_SIZE (MODE) < 4)\t\t\\\n+    {\t\t\t\t\t\t\\\n+      if (MODE == QImode || MODE == HImode)\t\\\n+\t{\t\t\t\t\t\\\n+\t  MODE = SImode;\t\t\t\\\n+\t}\t\t\t\t\t\\\n+    }\n+\n+/* Bits are always numbered from the LSBit.  */\n+#define BITS_BIG_ENDIAN 0\n+\n+/* Big/little-endian flavour.  */\n+#define BYTES_BIG_ENDIAN (TARGET_BIG_END != 0)\n+#define WORDS_BIG_ENDIAN (BYTES_BIG_ENDIAN)\n+\n+/* AdvSIMD is supported in the default configuration, unless disabled by\n+   -mgeneral-regs-only.  */\n+#define TARGET_SIMD !TARGET_GENERAL_REGS_ONLY\n+#define TARGET_FLOAT !TARGET_GENERAL_REGS_ONLY\n+\n+#define UNITS_PER_WORD\t\t8\n+\n+#define UNITS_PER_VREG\t\t16\n+\n+#define PARM_BOUNDARY\t\t64\n+\n+#define STACK_BOUNDARY\t\t128\n+\n+#define FUNCTION_BOUNDARY\t32\n+\n+#define EMPTY_FIELD_BOUNDARY\t32\n+\n+#define BIGGEST_ALIGNMENT\t128\n+\n+#define SHORT_TYPE_SIZE\t\t16\n+\n+#define INT_TYPE_SIZE\t\t32\n+\n+#define LONG_TYPE_SIZE\t\t64\t/* XXX This should be an option */\n+\n+#define LONG_LONG_TYPE_SIZE\t64\n+\n+#define FLOAT_TYPE_SIZE\t\t32\n+\n+#define DOUBLE_TYPE_SIZE\t64\n+\n+#define LONG_DOUBLE_TYPE_SIZE\t128\n+\n+/* The architecture reserves all bits of the address for hardware use,\n+   so the vbit must go into the delta field of pointers to member\n+   functions.  This is the same config as that in the AArch32\n+   port.  */\n+#define TARGET_PTRMEMFUNC_VBIT_LOCATION ptrmemfunc_vbit_in_delta\n+\n+/* Make strings word-aligned so that strcpy from constants will be\n+   faster.  */\n+#define CONSTANT_ALIGNMENT(EXP, ALIGN)\t\t\\\n+  ((TREE_CODE (EXP) == STRING_CST\t\t\\\n+    && !optimize_size\t\t\t\t\\\n+    && (ALIGN) < BITS_PER_WORD)\t\t\t\\\n+   ? BITS_PER_WORD : ALIGN)\n+\n+#define DATA_ALIGNMENT(EXP, ALIGN)\t\t\\\n+  ((((ALIGN) < BITS_PER_WORD)\t\t\t\\\n+    && (TREE_CODE (EXP) == ARRAY_TYPE\t\t\\\n+\t|| TREE_CODE (EXP) == UNION_TYPE\t\\\n+\t|| TREE_CODE (EXP) == RECORD_TYPE))\t\\\n+   ? BITS_PER_WORD : (ALIGN))\n+\n+#define LOCAL_ALIGNMENT(EXP, ALIGN) DATA_ALIGNMENT(EXP, ALIGN)\n+\n+#define STRUCTURE_SIZE_BOUNDARY\t\t8\n+\n+/* Defined by the ABI */\n+#define WCHAR_TYPE \"unsigned int\"\n+#define WCHAR_TYPE_SIZE\t\t\t32\n+\n+/* Using long long breaks -ansi and -std=c90, so these will need to be\n+   made conditional for an LLP64 ABI.  */\n+\n+#define SIZE_TYPE\t\"long unsigned int\"\n+\n+#define PTRDIFF_TYPE\t\"long int\"\n+\n+#define PCC_BITFIELD_TYPE_MATTERS\t1\n+\n+\n+/* Instruction tuning/selection flags.  */\n+\n+/* Bit values used to identify processor capabilities.  */\n+#define AARCH64_FL_SIMD       (1 << 0)\t/* Has SIMD instructions.  */\n+#define AARCH64_FL_FP         (1 << 1)\t/* Has FP.  */\n+#define AARCH64_FL_CRYPTO     (1 << 2)\t/* Has crypto.  */\n+#define AARCH64_FL_SLOWMUL    (1 << 3)\t/* A slow multiply core.  */\n+\n+/* Has FP and SIMD.  */\n+#define AARCH64_FL_FPSIMD     (AARCH64_FL_FP | AARCH64_FL_SIMD)\n+\n+/* Has FP without SIMD.  */\n+#define AARCH64_FL_FPQ16      (AARCH64_FL_FP & ~AARCH64_FL_SIMD)\n+\n+/* Architecture flags that effect instruction selection.  */\n+#define AARCH64_FL_FOR_ARCH8       (AARCH64_FL_FPSIMD)\n+\n+/* Macros to test ISA flags.  */\n+extern unsigned long aarch64_isa_flags;\n+#define AARCH64_ISA_CRYPTO         (aarch64_isa_flags & AARCH64_FL_CRYPTO)\n+#define AARCH64_ISA_FP             (aarch64_isa_flags & AARCH64_FL_FP)\n+#define AARCH64_ISA_SIMD           (aarch64_isa_flags & AARCH64_FL_SIMD)\n+\n+/* Macros to test tuning flags.  */\n+extern unsigned long aarch64_tune_flags;\n+#define AARCH64_TUNE_SLOWMUL       (aarch64_tune_flags & AARCH64_FL_SLOWMUL)\n+\n+\n+/* Standard register usage.  */\n+\n+/* 31 64-bit general purpose registers R0-R30:\n+   R30\t\tLR (link register)\n+   R29\t\tFP (frame pointer)\n+   R19-R28\tCallee-saved registers\n+   R18\t\tThe platform register; use as temporary register.\n+   R17\t\tIP1 The second intra-procedure-call temporary register\n+\t\t(can be used by call veneers and PLT code); otherwise use\n+\t\tas a temporary register\n+   R16\t\tIP0 The first intra-procedure-call temporary register (can\n+\t\tbe used by call veneers and PLT code); otherwise use as a\n+\t\ttemporary register\n+   R9-R15\tTemporary registers\n+   R8\t\tStructure value parameter / temporary register\n+   R0-R7\tParameter/result registers\n+\n+   SP\t\tstack pointer, encoded as X/R31 where permitted.\n+   ZR\t\tzero register, encoded as X/R31 elsewhere\n+\n+   32 x 128-bit floating-point/vector registers\n+   V16-V31\tCaller-saved (temporary) registers\n+   V8-V15\tCallee-saved registers\n+   V0-V7\tParameter/result registers\n+\n+   The vector register V0 holds scalar B0, H0, S0 and D0 in its least\n+   significant bits.  Unlike AArch32 S1 is not packed into D0,\n+   etc.  */\n+\n+/* Note that we don't mark X30 as a call-clobbered register.  The idea is\n+   that it's really the call instructions themselves which clobber X30.\n+   We don't care what the called function does with it afterwards.\n+\n+   This approach makes it easier to implement sibcalls.  Unlike normal\n+   calls, sibcalls don't clobber X30, so the register reaches the\n+   called function intact.  EPILOGUE_USES says that X30 is useful\n+   to the called function.  */\n+\n+#define FIXED_REGISTERS\t\t\t\t\t\\\n+  {\t\t\t\t\t\t\t\\\n+    0, 0, 0, 0,   0, 0, 0, 0,\t/* R0 - R7 */\t\t\\\n+    0, 0, 0, 0,   0, 0, 0, 0,\t/* R8 - R15 */\t\t\\\n+    0, 0, 0, 0,   0, 0, 0, 0,\t/* R16 - R23 */\t\t\\\n+    0, 0, 0, 0,   0, 1, 0, 1,\t/* R24 - R30, SP */\t\\\n+    0, 0, 0, 0,   0, 0, 0, 0,   /* V0 - V7 */           \\\n+    0, 0, 0, 0,   0, 0, 0, 0,   /* V8 - V15 */\t\t\\\n+    0, 0, 0, 0,   0, 0, 0, 0,   /* V16 - V23 */         \\\n+    0, 0, 0, 0,   0, 0, 0, 0,   /* V24 - V31 */         \\\n+    1, 1, 1,\t\t\t/* SFP, AP, CC */\t\\\n+  }\n+\n+#define CALL_USED_REGISTERS\t\t\t\t\\\n+  {\t\t\t\t\t\t\t\\\n+    1, 1, 1, 1,   1, 1, 1, 1,\t/* R0 - R7 */\t\t\\\n+    1, 1, 1, 1,   1, 1, 1, 1,\t/* R8 - R15 */\t\t\\\n+    1, 1, 1, 0,   0, 0, 0, 0,\t/* R16 - R23 */\t\t\\\n+    0, 0, 0, 0,   0, 1, 0, 1,\t/* R24 - R30, SP */\t\\\n+    1, 1, 1, 1,   1, 1, 1, 1,\t/* V0 - V7 */\t\t\\\n+    0, 0, 0, 0,   0, 0, 0, 0,\t/* V8 - V15 */\t\t\\\n+    1, 1, 1, 1,   1, 1, 1, 1,   /* V16 - V23 */         \\\n+    1, 1, 1, 1,   1, 1, 1, 1,   /* V24 - V31 */         \\\n+    1, 1, 1,\t\t\t/* SFP, AP, CC */\t\\\n+  }\n+\n+#define REGISTER_NAMES\t\t\t\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\\\n+    \"x0\",  \"x1\",  \"x2\",  \"x3\",  \"x4\",  \"x5\",  \"x6\",  \"x7\",\t\\\n+    \"x8\",  \"x9\",  \"x10\", \"x11\", \"x12\", \"x13\", \"x14\", \"x15\",\t\\\n+    \"x16\", \"x17\", \"x18\", \"x19\", \"x20\", \"x21\", \"x22\", \"x23\",\t\\\n+    \"x24\", \"x25\", \"x26\", \"x27\", \"x28\", \"x29\", \"x30\", \"sp\",\t\\\n+    \"v0\",  \"v1\",  \"v2\",  \"v3\",  \"v4\",  \"v5\",  \"v6\",  \"v7\",\t\\\n+    \"v8\",  \"v9\",  \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\",\t\\\n+    \"v16\", \"v17\", \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\",\t\\\n+    \"v24\", \"v25\", \"v26\", \"v27\", \"v28\", \"v29\", \"v30\", \"v31\",\t\\\n+    \"sfp\", \"ap\",  \"cc\",\t\t\t\t\t\t\\\n+  }\n+\n+/* Generate the register aliases for core register N */\n+#define R_ALIASES(N) {\"r\" # N, R0_REGNUM + (N)}, \\\n+                     {\"w\" # N, R0_REGNUM + (N)}\n+\n+#define V_ALIASES(N) {\"q\" # N, V0_REGNUM + (N)}, \\\n+                     {\"d\" # N, V0_REGNUM + (N)}, \\\n+                     {\"s\" # N, V0_REGNUM + (N)}, \\\n+                     {\"h\" # N, V0_REGNUM + (N)}, \\\n+                     {\"b\" # N, V0_REGNUM + (N)}\n+\n+/* Provide aliases for all of the ISA defined register name forms.\n+   These aliases are convenient for use in the clobber lists of inline\n+   asm statements.  */\n+\n+#define ADDITIONAL_REGISTER_NAMES \\\n+  { R_ALIASES(0),  R_ALIASES(1),  R_ALIASES(2),  R_ALIASES(3),  \\\n+    R_ALIASES(4),  R_ALIASES(5),  R_ALIASES(6),  R_ALIASES(7),  \\\n+    R_ALIASES(8),  R_ALIASES(9),  R_ALIASES(10), R_ALIASES(11), \\\n+    R_ALIASES(12), R_ALIASES(13), R_ALIASES(14), R_ALIASES(15), \\\n+    R_ALIASES(16), R_ALIASES(17), R_ALIASES(18), R_ALIASES(19), \\\n+    R_ALIASES(20), R_ALIASES(21), R_ALIASES(22), R_ALIASES(23), \\\n+    R_ALIASES(24), R_ALIASES(25), R_ALIASES(26), R_ALIASES(27), \\\n+    R_ALIASES(28), R_ALIASES(29), R_ALIASES(30), /* 31 omitted  */ \\\n+    V_ALIASES(0),  V_ALIASES(1),  V_ALIASES(2),  V_ALIASES(3),  \\\n+    V_ALIASES(4),  V_ALIASES(5),  V_ALIASES(6),  V_ALIASES(7),  \\\n+    V_ALIASES(8),  V_ALIASES(9),  V_ALIASES(10), V_ALIASES(11), \\\n+    V_ALIASES(12), V_ALIASES(13), V_ALIASES(14), V_ALIASES(15), \\\n+    V_ALIASES(16), V_ALIASES(17), V_ALIASES(18), V_ALIASES(19), \\\n+    V_ALIASES(20), V_ALIASES(21), V_ALIASES(22), V_ALIASES(23), \\\n+    V_ALIASES(24), V_ALIASES(25), V_ALIASES(26), V_ALIASES(27), \\\n+    V_ALIASES(28), V_ALIASES(29), V_ALIASES(30), V_ALIASES(31)  \\\n+  }\n+\n+/* Say that the epilogue uses the return address register.  Note that\n+   in the case of sibcalls, the values \"used by the epilogue\" are\n+   considered live at the start of the called function.  */\n+\n+#define EPILOGUE_USES(REGNO) \\\n+  ((REGNO) == LR_REGNUM)\n+\n+/* EXIT_IGNORE_STACK should be nonzero if, when returning from a function,\n+   the stack pointer does not matter.  The value is tested only in\n+   functions that have frame pointers.  */\n+#define EXIT_IGNORE_STACK\t1\n+\n+#define STATIC_CHAIN_REGNUM\t\tR18_REGNUM\n+#define HARD_FRAME_POINTER_REGNUM\tR29_REGNUM\n+#define FRAME_POINTER_REGNUM\t\tSFP_REGNUM\n+#define STACK_POINTER_REGNUM\t\tSP_REGNUM\n+#define ARG_POINTER_REGNUM\t\tAP_REGNUM\n+#define FIRST_PSEUDO_REGISTER\t\t67\n+\n+/* The number of (integer) argument register available.  */\n+#define NUM_ARG_REGS\t\t\t8\n+#define NUM_FP_ARG_REGS\t\t\t8\n+\n+/* A Homogeneous Floating-Point or Short-Vector Aggregate may have at most\n+   four members.  */\n+#define HA_MAX_NUM_FLDS\t\t4\n+\n+/* External dwarf register number scheme.  These number are used to\n+   identify registers in dwarf debug information, the values are\n+   defined by the AArch64 ABI.  The numbering scheme is independent of\n+   GCC's internal register numbering scheme.  */\n+\n+#define AARCH64_DWARF_R0        0\n+\n+/* The number of R registers, note 31! not 32.  */\n+#define AARCH64_DWARF_NUMBER_R 31\n+\n+#define AARCH64_DWARF_SP       31\n+#define AARCH64_DWARF_V0       64\n+\n+/* The number of V registers.  */\n+#define AARCH64_DWARF_NUMBER_V 32\n+\n+/* For signal frames we need to use an alternative return column.  This\n+   value must not correspond to a hard register and must be out of the\n+   range of DWARF_FRAME_REGNUM().  */\n+#define DWARF_ALT_FRAME_RETURN_COLUMN   \\\n+  (AARCH64_DWARF_V0 + AARCH64_DWARF_NUMBER_V)\n+\n+/* We add 1 extra frame register for use as the\n+   DWARF_ALT_FRAME_RETURN_COLUMN.  */\n+#define DWARF_FRAME_REGISTERS           (DWARF_ALT_FRAME_RETURN_COLUMN + 1)\n+\n+\n+#define DBX_REGISTER_NUMBER(REGNO)\taarch64_dbx_register_number (REGNO)\n+/* Provide a definition of DWARF_FRAME_REGNUM here so that fallback unwinders\n+   can use DWARF_ALT_FRAME_RETURN_COLUMN defined below.  This is just the same\n+   as the default definition in dwarf2out.c.  */\n+#undef DWARF_FRAME_REGNUM\n+#define DWARF_FRAME_REGNUM(REGNO)\tDBX_REGISTER_NUMBER (REGNO)\n+\n+#define DWARF_FRAME_RETURN_COLUMN\tDWARF_FRAME_REGNUM (LR_REGNUM)\n+\n+#define HARD_REGNO_NREGS(REGNO, MODE)\taarch64_hard_regno_nregs (REGNO, MODE)\n+\n+#define HARD_REGNO_MODE_OK(REGNO, MODE)\taarch64_hard_regno_mode_ok (REGNO, MODE)\n+\n+#define MODES_TIEABLE_P(MODE1, MODE2)\t\t\t\\\n+  (GET_MODE_CLASS (MODE1) == GET_MODE_CLASS (MODE2))\n+\n+#define DWARF2_UNWIND_INFO 1\n+\n+/* Use R0 through R3 to pass exception handling information.  */\n+#define EH_RETURN_DATA_REGNO(N) \\\n+  ((N) < 4 ? ((unsigned int) R0_REGNUM + (N)) : INVALID_REGNUM)\n+\n+/* Select a format to encode pointers in exception handling data.  */\n+#define ASM_PREFERRED_EH_DATA_FORMAT(CODE, GLOBAL) \\\n+  aarch64_asm_preferred_eh_data_format ((CODE), (GLOBAL))\n+\n+/* The register that holds the return address in exception handlers.  */\n+#define AARCH64_EH_STACKADJ_REGNUM\t(R0_REGNUM + 4)\n+#define EH_RETURN_STACKADJ_RTX\tgen_rtx_REG (Pmode, AARCH64_EH_STACKADJ_REGNUM)\n+\n+/* Don't use __builtin_setjmp until we've defined it.  */\n+#undef DONT_USE_BUILTIN_SETJMP\n+#define DONT_USE_BUILTIN_SETJMP 1\n+\n+/* Register in which the structure value is to be returned.  */\n+#define AARCH64_STRUCT_VALUE_REGNUM R8_REGNUM\n+\n+/* Non-zero if REGNO is part of the Core register set.\n+\n+   The rather unusual way of expressing this check is to avoid\n+   warnings when building the compiler when R0_REGNUM is 0 and REGNO\n+   is unsigned.  */\n+#define GP_REGNUM_P(REGNO)\t\t\t\t\t\t\\\n+  (((unsigned) (REGNO - R0_REGNUM)) <= (R30_REGNUM - R0_REGNUM))\n+\n+#define FP_REGNUM_P(REGNO)\t\t\t\\\n+  (((unsigned) (REGNO - V0_REGNUM)) <= (V31_REGNUM - V0_REGNUM))\n+\n+#define FP_LO_REGNUM_P(REGNO)            \\\n+  (((unsigned) (REGNO - V0_REGNUM)) <= (V15_REGNUM - V0_REGNUM))\n+\n+\f\n+/* Register and constant classes.  */\n+\n+enum reg_class\n+{\n+  NO_REGS,\n+  CORE_REGS,\n+  GENERAL_REGS,\n+  STACK_REG,\n+  POINTER_REGS,\n+  FP_LO_REGS,\n+  FP_REGS,\n+  ALL_REGS,\n+  LIM_REG_CLASSES\t\t/* Last */\n+};\n+\n+#define N_REG_CLASSES\t((int) LIM_REG_CLASSES)\n+\n+#define REG_CLASS_NAMES\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  \"NO_REGS\",\t\t\t\t\t\\\n+  \"CORE_REGS\",\t\t\t\t\t\\\n+  \"GENERAL_REGS\",\t\t\t\t\\\n+  \"STACK_REG\",\t\t\t\t\t\\\n+  \"POINTER_REGS\",\t\t\t\t\\\n+  \"FP_LO_REGS\",\t\t\t\t\t\\\n+  \"FP_REGS\",\t\t\t\t\t\\\n+  \"ALL_REGS\"\t\t\t\t\t\\\n+}\n+\n+#define REG_CLASS_CONTENTS\t\t\t\t\t\t\\\n+{\t\t\t\t\t\t\t\t\t\\\n+  { 0x00000000, 0x00000000, 0x00000000 },\t/* NO_REGS */\t\t\\\n+  { 0x7fffffff, 0x00000000, 0x00000003 },\t/* CORE_REGS */\t\t\\\n+  { 0x7fffffff, 0x00000000, 0x00000003 },\t/* GENERAL_REGS */\t\\\n+  { 0x80000000, 0x00000000, 0x00000000 },\t/* STACK_REG */\t\t\\\n+  { 0xffffffff, 0x00000000, 0x00000003 },\t/* POINTER_REGS */\t\\\n+  { 0x00000000, 0x0000ffff, 0x00000000 },       /* FP_LO_REGS  */\t\\\n+  { 0x00000000, 0xffffffff, 0x00000000 },       /* FP_REGS  */\t\t\\\n+  { 0xffffffff, 0xffffffff, 0x00000007 }\t/* ALL_REGS */\t\t\\\n+}\n+\n+#define REGNO_REG_CLASS(REGNO)\taarch64_regno_regclass (REGNO)\n+\n+#define INDEX_REG_CLASS\tCORE_REGS\n+#define BASE_REG_CLASS  POINTER_REGS\n+\n+/* Register pairs used to eliminate unneeded registers that point intoi\n+   the stack frame.  */\n+#define ELIMINABLE_REGS\t\t\t\t\t\t\t\\\n+{\t\t\t\t\t\t\t\t\t\\\n+  { ARG_POINTER_REGNUM,\t\tSTACK_POINTER_REGNUM\t\t},\t\\\n+  { ARG_POINTER_REGNUM,\t\tHARD_FRAME_POINTER_REGNUM\t},\t\\\n+  { FRAME_POINTER_REGNUM,\tSTACK_POINTER_REGNUM\t\t},\t\\\n+  { FRAME_POINTER_REGNUM,\tHARD_FRAME_POINTER_REGNUM\t},\t\\\n+}\n+\n+#define INITIAL_ELIMINATION_OFFSET(FROM, TO, OFFSET) \\\n+  (OFFSET) = aarch64_initial_elimination_offset (FROM, TO)\n+\n+/* CPU/ARCH option handling.  */\n+#include \"config/aarch64/aarch64-opts.h\"\n+\n+enum target_cpus\n+{\n+#define AARCH64_CORE(NAME, IDENT, ARCH, FLAGS, COSTS) \\\n+  TARGET_CPU_##IDENT,\n+#include \"aarch64-cores.def\"\n+#undef AARCH64_CORE\n+  TARGET_CPU_generic\n+};\n+\n+/* If there is no CPU defined at configure, use \"generic\" as default.  */\n+#ifndef TARGET_CPU_DEFAULT\n+#define TARGET_CPU_DEFAULT \\\n+  (TARGET_CPU_generic | (AARCH64_CPU_DEFAULT_FLAGS << 6))\n+#endif\n+\n+/* The processor for which instructions should be scheduled.  */\n+extern enum aarch64_processor aarch64_tune;\n+\n+/* RTL generation support.  */\n+#define INIT_EXPANDERS aarch64_init_expanders ()\n+\f\n+\n+/* Stack layout; function entry, exit and calling.  */\n+#define STACK_GROWS_DOWNWARD\t1\n+\n+#define FRAME_GROWS_DOWNWARD\t0\n+\n+#define STARTING_FRAME_OFFSET\t0\n+\n+#define ACCUMULATE_OUTGOING_ARGS\t1\n+\n+#define FIRST_PARM_OFFSET(FNDECL) 0\n+\n+/* Fix for VFP */\n+#define LIBCALL_VALUE(MODE)  \\\n+  gen_rtx_REG (MODE, FLOAT_MODE_P (MODE) ? V0_REGNUM : R0_REGNUM)\n+\n+#define DEFAULT_PCC_STRUCT_RETURN 0\n+\n+#define AARCH64_ROUND_UP(X, ALIGNMENT) \\\n+  (((X) + ((ALIGNMENT) - 1)) & ~((ALIGNMENT) - 1))\n+\n+#define AARCH64_ROUND_DOWN(X, ALIGNMENT) \\\n+  ((X) & ~((ALIGNMENT) - 1))\n+\n+#ifdef HOST_WIDE_INT\n+struct GTY (()) aarch64_frame\n+{\n+  HOST_WIDE_INT reg_offset[FIRST_PSEUDO_REGISTER];\n+  HOST_WIDE_INT saved_regs_size;\n+  /* Padding if needed after the all the callee save registers have\n+     been saved.  */\n+  HOST_WIDE_INT padding0;\n+  HOST_WIDE_INT hardfp_offset;\t/* HARD_FRAME_POINTER_REGNUM */\n+  HOST_WIDE_INT fp_lr_offset;\t/* Space needed for saving fp and/or lr */\n+\n+  bool laid_out;\n+};\n+\n+typedef struct GTY (()) machine_function\n+{\n+  struct aarch64_frame frame;\n+\n+  /* The number of extra stack bytes taken up by register varargs.\n+     This area is allocated by the callee at the very top of the frame.  */\n+  HOST_WIDE_INT saved_varargs_size;\n+\n+} machine_function;\n+#endif\n+\n+\n+/* Which ABI to use.  */\n+enum arm_abi_type\n+{\n+  ARM_ABI_AAPCS64\n+};\n+\n+enum arm_pcs\n+{\n+  ARM_PCS_AAPCS64,\t\t/* Base standard AAPCS for 64 bit.  */\n+  ARM_PCS_UNKNOWN\n+};\n+\n+\n+extern enum arm_abi_type arm_abi;\n+extern enum arm_pcs arm_pcs_variant;\n+#ifndef ARM_DEFAULT_ABI\n+#define ARM_DEFAULT_ABI ARM_ABI_AAPCS64\n+#endif\n+\n+#ifndef ARM_DEFAULT_PCS\n+#define ARM_DEFAULT_PCS ARM_PCS_AAPCS64\n+#endif\n+\n+/* We can't use enum machine_mode inside a generator file because it\n+   hasn't been created yet; we shouldn't be using any code that\n+   needs the real definition though, so this ought to be safe.  */\n+#ifdef GENERATOR_FILE\n+#define MACHMODE int\n+#else\n+#include \"insn-modes.h\"\n+#define MACHMODE enum machine_mode\n+#endif\n+\n+\n+/* AAPCS related state tracking.  */\n+typedef struct\n+{\n+  enum arm_pcs pcs_variant;\n+  int aapcs_arg_processed;\t/* No need to lay out this argument again.  */\n+  int aapcs_ncrn;\t\t/* Next Core register number.  */\n+  int aapcs_nextncrn;\t\t/* Next next core register number.  */\n+  int aapcs_nvrn;\t\t/* Next Vector register number.  */\n+  int aapcs_nextnvrn;\t\t/* Next Next Vector register number.  */\n+  rtx aapcs_reg;\t\t/* Register assigned to this argument.  This\n+\t\t\t\t   is NULL_RTX if this parameter goes on\n+\t\t\t\t   the stack.  */\n+  MACHMODE aapcs_vfp_rmode;\n+  int aapcs_stack_words;\t/* If the argument is passed on the stack, this\n+\t\t\t\t   is the number of words needed, after rounding\n+\t\t\t\t   up.  Only meaningful when\n+\t\t\t\t   aapcs_reg == NULL_RTX.  */\n+  int aapcs_stack_size;\t\t/* The total size (in words, per 8 byte) of the\n+\t\t\t\t   stack arg area so far.  */\n+} CUMULATIVE_ARGS;\n+\n+#define FUNCTION_ARG_PADDING(MODE, TYPE) \\\n+  (aarch64_pad_arg_upward (MODE, TYPE) ? upward : downward)\n+\n+#define BLOCK_REG_PADDING(MODE, TYPE, FIRST) \\\n+  (aarch64_pad_reg_upward (MODE, TYPE, FIRST) ? upward : downward)\n+\n+#define PAD_VARARGS_DOWN\t0\n+\n+#define INIT_CUMULATIVE_ARGS(CUM, FNTYPE, LIBNAME, FNDECL, N_NAMED_ARGS) \\\n+  aarch64_init_cumulative_args (&(CUM), FNTYPE, LIBNAME, FNDECL, N_NAMED_ARGS)\n+\n+#define FUNCTION_ARG_REGNO_P(REGNO) \\\n+  aarch64_function_arg_regno_p(REGNO)\n+\f\n+\n+/* ISA Features.  */\n+\n+/* Addressing modes, etc.  */\n+#define HAVE_POST_INCREMENT\t1\n+#define HAVE_PRE_INCREMENT\t1\n+#define HAVE_POST_DECREMENT\t1\n+#define HAVE_PRE_DECREMENT\t1\n+#define HAVE_POST_MODIFY_DISP\t1\n+#define HAVE_PRE_MODIFY_DISP\t1\n+\n+#define MAX_REGS_PER_ADDRESS\t2\n+\n+#define CONSTANT_ADDRESS_P(X)\t\taarch64_constant_address_p(X)\n+\n+/* Try a machine-dependent way of reloading an illegitimate address\n+   operand.  If we find one, push the reload and jump to WIN.  This\n+   macro is used in only one place: `find_reloads_address' in reload.c.  */\n+\n+#define LEGITIMIZE_RELOAD_ADDRESS(X, MODE, OPNUM, TYPE, IND_L, WIN)\t     \\\n+do {\t\t\t\t\t\t\t\t\t     \\\n+  rtx new_x = aarch64_legitimize_reload_address (&(X), MODE, OPNUM, TYPE,    \\\n+\t\t\t\t\t\t IND_L);\t\t     \\\n+  if (new_x)\t\t\t\t\t\t\t\t     \\\n+    {\t\t\t\t\t\t\t\t\t     \\\n+      X = new_x;\t\t\t\t\t\t\t     \\\n+      goto WIN;\t\t\t\t\t\t\t\t     \\\n+    }\t\t\t\t\t\t\t\t\t     \\\n+} while (0)\n+\n+#define REGNO_OK_FOR_BASE_P(REGNO)\t\\\n+  aarch64_regno_ok_for_base_p (REGNO, true)\n+\n+#define REGNO_OK_FOR_INDEX_P(REGNO) \\\n+  aarch64_regno_ok_for_index_p (REGNO, true)\n+\n+#define LEGITIMATE_PIC_OPERAND_P(X) \\\n+  aarch64_legitimate_pic_operand_p (X)\n+\n+#define CASE_VECTOR_MODE Pmode\n+\n+#define DEFAULT_SIGNED_CHAR 0\n+\n+/* An integer expression for the size in bits of the largest integer machine\n+   mode that should actually be used.  We allow pairs of registers.  */\n+#define MAX_FIXED_MODE_SIZE GET_MODE_BITSIZE (TImode)\n+\n+/* Maximum bytes moved by a single instruction (load/store pair).  */\n+#define MOVE_MAX (UNITS_PER_WORD * 2)\n+\n+/* The base cost overhead of a memcpy call, for MOVE_RATIO and friends.  */\n+#define AARCH64_CALL_RATIO 8\n+\n+/* When optimizing for size, give a better estimate of the length of a memcpy\n+   call, but use the default otherwise.  But move_by_pieces_ninsns() counts\n+   memory-to-memory moves, and we'll have to generate a load & store for each,\n+   so halve the value to take that into account.  */\n+#define MOVE_RATIO(speed) \\\n+  (((speed) ? 15 : AARCH64_CALL_RATIO) / 2)\n+\n+/* For CLEAR_RATIO, when optimizing for size, give a better estimate\n+   of the length of a memset call, but use the default otherwise.  */\n+#define CLEAR_RATIO(speed) \\\n+  ((speed) ? 15 : AARCH64_CALL_RATIO)\n+\n+/* SET_RATIO is similar to CLEAR_RATIO, but for a non-zero constant, so when\n+   optimizing for size adjust the ratio to account for the overhead of loading\n+   the constant.  */\n+#define SET_RATIO(speed) \\\n+  ((speed) ? 15 : AARCH64_CALL_RATIO - 2)\n+\n+/* STORE_BY_PIECES_P can be used when copying a constant string, but\n+   in that case each 64-bit chunk takes 5 insns instead of 2 (LDR/STR).\n+   For now we always fail this and let the move_by_pieces code copy\n+   the string from read-only memory.  */\n+#define STORE_BY_PIECES_P(SIZE, ALIGN) 0\n+\n+/* Disable auto-increment in move_by_pieces et al.  Use of auto-increment is\n+   rarely a good idea in straight-line code since it adds an extra address\n+   dependency between each instruction.  Better to use incrementing offsets.  */\n+#define USE_LOAD_POST_INCREMENT(MODE)   0\n+#define USE_LOAD_POST_DECREMENT(MODE)   0\n+#define USE_LOAD_PRE_INCREMENT(MODE)    0\n+#define USE_LOAD_PRE_DECREMENT(MODE)    0\n+#define USE_STORE_POST_INCREMENT(MODE)  0\n+#define USE_STORE_POST_DECREMENT(MODE)  0\n+#define USE_STORE_PRE_INCREMENT(MODE)   0\n+#define USE_STORE_PRE_DECREMENT(MODE)   0\n+\n+/* ?? #define WORD_REGISTER_OPERATIONS  */\n+\n+/* Define if loading from memory in MODE, an integral mode narrower than\n+   BITS_PER_WORD will either zero-extend or sign-extend.  The value of this\n+   macro should be the code that says which one of the two operations is\n+   implicitly done, or UNKNOWN if none.  */\n+#define LOAD_EXTEND_OP(MODE) ZERO_EXTEND\n+\n+/* Define this macro to be non-zero if instructions will fail to work\n+   if given data not on the nominal alignment.  */\n+#define STRICT_ALIGNMENT\t\tTARGET_STRICT_ALIGN\n+\n+/* Define this macro to be non-zero if accessing less than a word of\n+   memory is no faster than accessing a word of memory, i.e., if such\n+   accesses require more than one instruction or if there is no\n+   difference in cost.\n+   Although there's no difference in instruction count or cycles,\n+   in AArch64 we don't want to expand to a sub-word to a 64-bit access\n+   if we don't have to, for power-saving reasons.  */\n+#define SLOW_BYTE_ACCESS\t\t0\n+\n+#define TRULY_NOOP_TRUNCATION(OUTPREC, INPREC) 1\n+\n+#define NO_FUNCTION_CSE\t1\n+\n+#define Pmode\t\tDImode\n+#define FUNCTION_MODE\tPmode\n+\n+#define SELECT_CC_MODE(OP, X, Y)\taarch64_select_cc_mode (OP, X, Y)\n+\n+#define REVERSE_CONDITION(CODE, MODE)\t\t\\\n+  (((MODE) == CCFPmode || (MODE) == CCFPEmode)\t\\\n+   ? reverse_condition_maybe_unordered (CODE)\t\\\n+   : reverse_condition (CODE))\n+\n+#define CLZ_DEFINED_VALUE_AT_ZERO(MODE, VALUE) \\\n+  ((VALUE) = ((MODE) == SImode ? 32 : 64), 2)\n+#define CTZ_DEFINED_VALUE_AT_ZERO(MODE, VALUE) \\\n+  ((VALUE) = ((MODE) == SImode ? 32 : 64), 2)\n+\n+#define INCOMING_RETURN_ADDR_RTX gen_rtx_REG (Pmode, LR_REGNUM)\n+\n+#define RETURN_ADDR_RTX aarch64_return_addr\n+\n+#define TRAMPOLINE_SIZE\taarch64_trampoline_size ()\n+\n+/* Trampolines contain dwords, so must be dword aligned.  */\n+#define TRAMPOLINE_ALIGNMENT 64\n+\n+/* Put trampolines in the text section so that mapping symbols work\n+   correctly.  */\n+#define TRAMPOLINE_SECTION text_section\n+\f\n+/* Costs, etc.  */\n+#define MEMORY_MOVE_COST(M, CLASS, IN) \\\n+  (GET_MODE_SIZE (M) < 8 ? 8 : GET_MODE_SIZE (M))\n+\n+/* To start with.  */\n+#define BRANCH_COST(SPEED_P, PREDICTABLE_P) 2\n+\f\n+\n+/* Assembly output.  */\n+\n+/* For now we'll make all jump tables pc-relative.  */\n+#define CASE_VECTOR_PC_RELATIVE\t1\n+\n+#define CASE_VECTOR_SHORTEN_MODE(min, max, body)\t\\\n+  ((min < -0x1fff0 || max > 0x1fff0) ? SImode\t\t\\\n+   : (min < -0x1f0 || max > 0x1f0) ? HImode\t\t\\\n+   : QImode)\n+\n+/* Jump table alignment is explicit in ASM_OUTPUT_CASE_LABEL.  */\n+#define ADDR_VEC_ALIGN(JUMPTABLE) 0\n+\n+#define PRINT_OPERAND(STREAM, X, CODE) aarch64_print_operand (STREAM, X, CODE)\n+\n+#define PRINT_OPERAND_ADDRESS(STREAM, X) \\\n+  aarch64_print_operand_address (STREAM, X)\n+\n+#define FUNCTION_PROFILER(STREAM, LABELNO) \\\n+  aarch64_function_profiler (STREAM, LABELNO)\n+\n+/* For some reason, the Linux headers think they know how to define\n+   these macros.  They don't!!!  */\n+#undef ASM_APP_ON\n+#undef ASM_APP_OFF\n+#define ASM_APP_ON\t\"\\t\" ASM_COMMENT_START \" Start of user assembly\\n\"\n+#define ASM_APP_OFF\t\"\\t\" ASM_COMMENT_START \" End of user assembly\\n\"\n+\n+#define ASM_FPRINTF_EXTENSIONS(FILE, ARGS, P)\t\t\\\n+  case '@':\t\t\t\t\t\t\\\n+    fputs (ASM_COMMENT_START, FILE);\t\t\t\\\n+    break;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\\\n+  case 'r':\t\t\t\t\t\t\\\n+    fputs (REGISTER_PREFIX, FILE);\t\t\t\\\n+    fputs (reg_names[va_arg (ARGS, int)], FILE);\t\\\n+    break;\n+\n+#define CONSTANT_POOL_BEFORE_FUNCTION 0\n+\n+/* This definition should be relocated to aarch64-elf-raw.h.  This macro\n+   should be undefined in aarch64-linux.h and a clear_cache pattern\n+   implmented to emit either the call to __aarch64_sync_cache_range()\n+   directly or preferably the appropriate sycall or cache clear\n+   instructions inline.  */\n+#define CLEAR_INSN_CACHE(beg, end)\t\t\t\t\\\n+  extern void  __aarch64_sync_cache_range (void *, void *);\t\\\n+  __aarch64_sync_cache_range (beg, end)\n+\n+/* This should be integrated with the equivalent in the 32 bit\n+   world.  */\n+enum aarch64_builtins\n+{\n+  AARCH64_BUILTIN_MIN,\n+  AARCH64_BUILTIN_THREAD_POINTER,\n+  AARCH64_SIMD_BUILTIN_BASE\n+};\n+\n+/*  VFP registers may only be accessed in the mode they\n+   were set.  */\n+#define CANNOT_CHANGE_MODE_CLASS(FROM, TO, CLASS)\t\\\n+  (GET_MODE_SIZE (FROM) != GET_MODE_SIZE (TO)\t\t\\\n+   ? reg_classes_intersect_p (FP_REGS, (CLASS))\t\t\\\n+   : 0)\n+\n+\n+#define SHIFT_COUNT_TRUNCATED !TARGET_SIMD\n+\n+/* Callee only saves lower 64-bits of a 128-bit register.  Tell the\n+   compiler the callee clobbers the top 64-bits when restoring the\n+   bottom 64-bits.  */\n+#define HARD_REGNO_CALL_PART_CLOBBERED(REGNO, MODE) \\\n+\t\t(FP_REGNUM_P (REGNO) && GET_MODE_SIZE (MODE) > 8)\n+\n+/* Check TLS Descriptors mechanism is selected.  */\n+#define TARGET_TLS_DESC (aarch64_tls_dialect == TLS_DESCRIPTORS)\n+\n+extern enum aarch64_code_model aarch64_cmodel;\n+\n+/* When using the tiny addressing model conditional and unconditional branches\n+   can span the whole of the available address space (1MB).  */\n+#define HAS_LONG_COND_BRANCH\t\t\t\t\\\n+  (aarch64_cmodel == AARCH64_CMODEL_TINY\t\t\\\n+   || aarch64_cmodel == AARCH64_CMODEL_TINY_PIC)\n+\n+#define HAS_LONG_UNCOND_BRANCH\t\t\t\t\\\n+  (aarch64_cmodel == AARCH64_CMODEL_TINY\t\t\\\n+   || aarch64_cmodel == AARCH64_CMODEL_TINY_PIC)\n+\n+/* Modes valid for AdvSIMD Q registers.  */\n+#define AARCH64_VALID_SIMD_QREG_MODE(MODE) \\\n+  ((MODE) == V4SImode || (MODE) == V8HImode || (MODE) == V16QImode \\\n+   || (MODE) == V4SFmode || (MODE) == V2DImode || mode == V2DFmode)\n+\n+#endif /* GCC_AARCH64_H */"}, {"sha": "b80392298ac3cb945bfc171400a9d65ef7681f74", "filename": "gcc/config/aarch64/aarch64.md", "status": "added", "additions": 2930, "deletions": 0, "changes": 2930, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.md?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,2930 @@\n+;; Machine description for AArch64 architecture.\n+;; Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+;; Contributed by ARM Ltd.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+;; Register numbers\n+(define_constants\n+  [\n+    (R0_REGNUM\t\t0)\n+    (R1_REGNUM\t\t1)\n+    (R2_REGNUM\t\t2)\n+    (R3_REGNUM\t\t3)\n+    (R4_REGNUM\t\t4)\n+    (R5_REGNUM\t\t5)\n+    (R6_REGNUM\t\t6)\n+    (R7_REGNUM\t\t7)\n+    (R8_REGNUM\t\t8)\n+    (R9_REGNUM\t\t9)\n+    (R10_REGNUM\t\t10)\n+    (R11_REGNUM\t\t11)\n+    (R12_REGNUM\t\t12)\n+    (R13_REGNUM\t\t13)\n+    (R14_REGNUM\t\t14)\n+    (R15_REGNUM\t\t15)\n+    (R16_REGNUM\t\t16)\n+    (IP0_REGNUM\t\t16)\n+    (R17_REGNUM\t\t17)\n+    (IP1_REGNUM\t\t17)\n+    (R18_REGNUM\t\t18)\n+    (R19_REGNUM\t\t19)\n+    (R20_REGNUM\t\t20)\n+    (R21_REGNUM\t\t21)\n+    (R22_REGNUM\t\t22)\n+    (R23_REGNUM\t\t23)\n+    (R24_REGNUM\t\t24)\n+    (R25_REGNUM\t\t25)\n+    (R26_REGNUM\t\t26)\n+    (R27_REGNUM\t\t27)\n+    (R28_REGNUM\t\t28)\n+    (R29_REGNUM\t\t29)\n+    (R30_REGNUM\t\t30)\n+    (LR_REGNUM\t\t30)\n+    (SP_REGNUM\t\t31)\n+    (V0_REGNUM\t\t32)\n+    (V15_REGNUM\t\t47)\n+    (V31_REGNUM\t\t63)\n+    (SFP_REGNUM\t\t64)\n+    (AP_REGNUM\t\t65)\n+    (CC_REGNUM\t\t66)\n+  ]\n+)\n+\n+(define_c_enum \"unspec\" [\n+    UNSPEC_CASESI\n+    UNSPEC_CLS\n+    UNSPEC_FRINTA\n+    UNSPEC_FRINTI\n+    UNSPEC_FRINTM\n+    UNSPEC_FRINTP\n+    UNSPEC_FRINTX\n+    UNSPEC_FRINTZ\n+    UNSPEC_GOTSMALLPIC\n+    UNSPEC_GOTSMALLTLS\n+    UNSPEC_LD2\n+    UNSPEC_LD3\n+    UNSPEC_LD4\n+    UNSPEC_MB\n+    UNSPEC_NOP\n+    UNSPEC_PRLG_STK\n+    UNSPEC_RBIT\n+    UNSPEC_ST2\n+    UNSPEC_ST3\n+    UNSPEC_ST4\n+    UNSPEC_TLS\n+    UNSPEC_TLSDESC\n+    UNSPEC_VSTRUCTDUMMY\n+])\n+\n+(define_c_enum \"unspecv\" [\n+    UNSPECV_EH_RETURN\t\t; Represent EH_RETURN\n+  ]\n+)\n+\n+;; If further include files are added the defintion of MD_INCLUDES\n+;; must be updated.\n+\n+(include \"constraints.md\")\n+(include \"predicates.md\")\n+(include \"iterators.md\")\n+\n+;; -------------------------------------------------------------------\n+;; Synchronization Builtins\n+;; -------------------------------------------------------------------\n+\n+;; The following sync_* attributes are applied to sychronization\n+;; instruction patterns to control the way in which the\n+;; synchronization loop is expanded.\n+;; All instruction patterns that call aarch64_output_sync_insn ()\n+;; should define these attributes.  Refer to the comment above\n+;; aarch64.c:aarch64_output_sync_loop () for more detail on the use of\n+;; these attributes.\n+\n+;; Attribute specifies the operand number which contains the\n+;; result of a synchronization operation.  The result is the old value\n+;; loaded from SYNC_MEMORY.\n+(define_attr \"sync_result\"          \"none,0,1,2,3,4,5\" (const_string \"none\"))\n+\n+;; Attribute specifies the operand number which contains the memory\n+;; address to which the synchronization operation is being applied.\n+(define_attr \"sync_memory\"          \"none,0,1,2,3,4,5\" (const_string \"none\"))\n+\n+;; Attribute specifies the operand number which contains the required\n+;; old value expected in the memory location.  This attribute may be\n+;; none if no required value test should be performed in the expanded\n+;; code.\n+(define_attr \"sync_required_value\"  \"none,0,1,2,3,4,5\" (const_string \"none\"))\n+\n+;; Attribute specifies the operand number of the new value to be stored\n+;; into the memory location identitifed by the sync_memory attribute.\n+(define_attr \"sync_new_value\"       \"none,0,1,2,3,4,5\" (const_string \"none\"))\n+\n+;; Attribute specifies the operand number of a temporary register\n+;; which can be clobbered by the synchronization instruction sequence.\n+;; The register provided byn SYNC_T1 may be the same as SYNC_RESULT is\n+;; which case the result value will be clobbered and not available\n+;; after the synchronization loop exits.\n+(define_attr \"sync_t1\"              \"none,0,1,2,3,4,5\" (const_string \"none\"))\n+\n+;; Attribute specifies the operand number of a temporary register\n+;; which can be clobbered by the synchronization instruction sequence.\n+;; This register is used to collect the result of a store exclusive\n+;; instruction.\n+(define_attr \"sync_t2\"              \"none,0,1,2,3,4,5\" (const_string \"none\"))\n+\n+;; Attribute that specifies whether or not the emitted synchronization\n+;; loop must contain a release barrier.\n+(define_attr \"sync_release_barrier\" \"yes,no\"           (const_string \"yes\"))\n+\n+;; Attribute that specifies the operation that the synchronization\n+;; loop should apply to the old and new values to generate the value\n+;; written back to memory.\n+(define_attr \"sync_op\"              \"none,add,sub,ior,xor,and,nand\"\n+                                    (const_string \"none\"))\n+\n+;; -------------------------------------------------------------------\n+;; Instruction types and attributes\n+;; -------------------------------------------------------------------\n+\n+;; Main data types used by the insntructions\n+\n+(define_attr \"mode\" \"unknown,none,QI,HI,SI,DI,TI,SF,DF,TF\"\n+  (const_string \"unknown\"))\n+\n+(define_attr \"mode2\" \"unknown,none,QI,HI,SI,DI,TI,SF,DF,TF\"\n+  (const_string \"unknown\"))\n+\n+; The \"v8type\" attribute is used to for fine grained classification of\n+; AArch64 instructions.  This table briefly explains the meaning of each type.\n+\n+; adc              add/subtract with carry.\n+; adcs             add/subtract with carry (setting condition flags).\n+; adr              calculate address.\n+; alu              simple alu instruction (no memory or fp regs access).\n+; alu_ext          simple alu instruction (sign/zero-extended register).\n+; alu_shift        simple alu instruction, with a source operand shifted by a constant.\n+; alus             simple alu instruction (setting condition flags).\n+; alus_ext         simple alu instruction (sign/zero-extended register, setting condition flags).\n+; alus_shift       simple alu instruction, with a source operand shifted by a constant (setting condition flags).\n+; bfm              bitfield move operation.\n+; branch           branch.\n+; call             subroutine call.\n+; ccmp             conditional compare.\n+; clz              count leading zeros/sign bits.\n+; csel             conditional select.\n+; dmb              data memory barrier.\n+; extend           sign/zero-extend (specialised bitfield move).\n+; extr             extract register-sized bitfield encoding.\n+; fpsimd_load      load single floating point / simd scalar register from memory.\n+; fpsimd_load2     load pair of floating point / simd scalar registers from memory.\n+; fpsimd_store     store single floating point / simd scalar register to memory.\n+; fpsimd_store2    store pair floating point / simd scalar registers to memory.\n+; fadd             floating point add/sub.\n+; fccmp            floating point conditional compare.\n+; fcmp             floating point comparison.\n+; fconst           floating point load immediate.\n+; fcsel            floating point conditional select.\n+; fcvt             floating point convert (float to float).\n+; fcvtf2i          floating point convert (float to integer).\n+; fcvti2f          floating point convert (integer to float).\n+; fdiv             floating point division operation.\n+; ffarith          floating point abs, neg or cpy.\n+; fmadd            floating point multiply-add/sub.\n+; fminmax          floating point min/max.\n+; fmov             floating point move (float to float).\n+; fmovf2i          floating point move (float to integer).\n+; fmovi2f          floating point move (integer to float).\n+; fmul             floating point multiply.\n+; frint            floating point round to integral.\n+; fsqrt            floating point square root.\n+; load_acq         load-acquire.\n+; load             load single general register from memory\n+; load2            load pair of general registers from memory\n+; logic            logical operation (register).\n+; logic_imm        and/or/xor operation (immediate).\n+; logic_shift      logical operation with shift.\n+; logics           logical operation (register, setting condition flags).\n+; logics_imm       and/or/xor operation (immediate, setting condition flags).\n+; logics_shift     logical operation with shift (setting condition flags).\n+; madd             integer multiply-add/sub.\n+; maddl            widening integer multiply-add/sub.\n+; misc             miscellaneous - any type that doesn't fit into the rest.\n+; move             integer move operation.\n+; move2            double integer move operation.\n+; movk             move 16-bit immediate with keep.\n+; movz             move 16-bit immmediate with zero/one.\n+; mrs              system/special register move.\n+; mulh             64x64 to 128-bit multiply (high part).\n+; mull             widening multiply.\n+; mult             integer multiply instruction.\n+; prefetch         memory prefetch.\n+; rbit             reverse bits.\n+; rev              reverse bytes.\n+; sdiv             integer division operation (signed).\n+; shift            variable shift operation.\n+; shift_imm        immediate shift operation (specialised bitfield move).\n+; store_rel        store-release.\n+; store            store single general register to memory.\n+; store2           store pair of general registers to memory.\n+; udiv             integer division operation (unsigned).\n+\n+(define_attr \"v8type\"\n+   \"adc,\\\n+   adcs,\\\n+   adr,\\\n+   alu,\\\n+   alu_ext,\\\n+   alu_shift,\\\n+   alus,\\\n+   alus_ext,\\\n+   alus_shift,\\\n+   bfm,\\\n+   branch,\\\n+   call,\\\n+   ccmp,\\\n+   clz,\\\n+   csel,\\\n+   dmb,\\\n+   div,\\\n+   div64,\\\n+   extend,\\\n+   extr,\\\n+   fpsimd_load,\\\n+   fpsimd_load2,\\\n+   fpsimd_store2,\\\n+   fpsimd_store,\\\n+   fadd,\\\n+   fccmp,\\\n+   fcvt,\\\n+   fcvtf2i,\\\n+   fcvti2f,\\\n+   fcmp,\\\n+   fconst,\\\n+   fcsel,\\\n+   fdiv,\\\n+   ffarith,\\\n+   fmadd,\\\n+   fminmax,\\\n+   fmov,\\\n+   fmovf2i,\\\n+   fmovi2f,\\\n+   fmul,\\\n+   frint,\\\n+   fsqrt,\\\n+   load_acq,\\\n+   load1,\\\n+   load2,\\\n+   logic,\\\n+   logic_imm,\\\n+   logic_shift,\\\n+   logics,\\\n+   logics_imm,\\\n+   logics_shift,\\\n+   madd,\\\n+   maddl,\\\n+   misc,\\\n+   move,\\\n+   move2,\\\n+   movk,\\\n+   movz,\\\n+   mrs,\\\n+   mulh,\\\n+   mull,\\\n+   mult,\\\n+   prefetch,\\\n+   rbit,\\\n+   rev,\\\n+   sdiv,\\\n+   shift,\\\n+   shift_imm,\\\n+   store_rel,\\\n+   store1,\\\n+   store2,\\\n+   udiv\"\n+  (const_string \"alu\"))\n+\n+\n+; The \"type\" attribute is used by the AArch32 backend.  Below is a mapping\n+; from \"v8type\" to \"type\".\n+\n+(define_attr \"type\"\n+  \"alu,alu_shift,block,branch,call,f_2_r,f_cvt,f_flag,f_loads,\n+   f_loadd,f_stored,f_stores,faddd,fadds,fcmpd,fcmps,fconstd,fconsts,\n+   fcpys,fdivd,fdivs,ffarithd,ffariths,fmacd,fmacs,fmuld,fmuls,load_byte,\n+   load1,load2,mult,r_2_f,store1,store2\"\n+  (cond [\n+\t  (eq_attr \"v8type\" \"alu_shift,alus_shift,logic_shift,logics_shift\") (const_string \"alu_shift\")\n+\t  (eq_attr \"v8type\" \"branch\") (const_string \"branch\")\n+\t  (eq_attr \"v8type\" \"call\") (const_string \"call\")\n+\t  (eq_attr \"v8type\" \"fmovf2i\") (const_string \"f_2_r\")\n+\t  (eq_attr \"v8type\" \"fcvt,fcvtf2i,fcvti2f\") (const_string \"f_cvt\")\n+\t  (and (eq_attr \"v8type\" \"fpsimd_load\") (eq_attr \"mode\" \"SF\")) (const_string \"f_loads\")\n+\t  (and (eq_attr \"v8type\" \"fpsimd_load\") (eq_attr \"mode\" \"DF\")) (const_string \"f_loadd\")\n+\t  (and (eq_attr \"v8type\" \"fpsimd_store\") (eq_attr \"mode\" \"SF\")) (const_string \"f_stores\")\n+\t  (and (eq_attr \"v8type\" \"fpsimd_store\") (eq_attr \"mode\" \"DF\")) (const_string \"f_stored\")\n+\t  (and (eq_attr \"v8type\" \"fadd,fminmax\") (eq_attr \"mode\" \"DF\")) (const_string \"faddd\")\n+\t  (and (eq_attr \"v8type\" \"fadd,fminmax\") (eq_attr \"mode\" \"SF\")) (const_string \"fadds\")\n+\t  (and (eq_attr \"v8type\" \"fcmp,fccmp\") (eq_attr \"mode\" \"DF\")) (const_string \"fcmpd\")\n+\t  (and (eq_attr \"v8type\" \"fcmp,fccmp\") (eq_attr \"mode\" \"SF\")) (const_string \"fcmps\")\n+\t  (and (eq_attr \"v8type\" \"fconst\") (eq_attr \"mode\" \"DF\")) (const_string \"fconstd\")\n+\t  (and (eq_attr \"v8type\" \"fconst\") (eq_attr \"mode\" \"SF\")) (const_string \"fconsts\")\n+\t  (and (eq_attr \"v8type\" \"fdiv,fsqrt\") (eq_attr \"mode\" \"DF\")) (const_string \"fdivd\")\n+\t  (and (eq_attr \"v8type\" \"fdiv,fsqrt\") (eq_attr \"mode\" \"SF\")) (const_string \"fdivs\")\n+\t  (and (eq_attr \"v8type\" \"ffarith\") (eq_attr \"mode\" \"DF\")) (const_string \"ffarithd\")\n+\t  (and (eq_attr \"v8type\" \"ffarith\") (eq_attr \"mode\" \"SF\")) (const_string \"ffariths\")\n+\t  (and (eq_attr \"v8type\" \"fmadd\") (eq_attr \"mode\" \"DF\")) (const_string \"fmacd\")\n+\t  (and (eq_attr \"v8type\" \"fmadd\") (eq_attr \"mode\" \"SF\")) (const_string \"fmacs\")\n+\t  (and (eq_attr \"v8type\" \"fmul\") (eq_attr \"mode\" \"DF\")) (const_string \"fmuld\")\n+\t  (and (eq_attr \"v8type\" \"fmul\") (eq_attr \"mode\" \"SF\")) (const_string \"fmuls\")\n+\t  (and (eq_attr \"v8type\" \"load1\") (eq_attr \"mode\" \"QI,HI\")) (const_string \"load_byte\")\n+\t  (and (eq_attr \"v8type\" \"load1\") (eq_attr \"mode\" \"SI,DI,TI\")) (const_string \"load1\")\n+\t  (eq_attr \"v8type\" \"load2\") (const_string \"load2\")\n+\t  (and (eq_attr \"v8type\" \"mulh,mult,mull,madd,sdiv,udiv\") (eq_attr \"mode\" \"SI\")) (const_string \"mult\")\n+\t  (eq_attr \"v8type\" \"fmovi2f\") (const_string \"r_2_f\")\n+\t  (eq_attr \"v8type\" \"store1\") (const_string \"store1\")\n+\t  (eq_attr \"v8type\" \"store2\") (const_string \"store2\")\n+  ]\n+  (const_string \"alu\")))\n+\n+;; Attribute that specifies whether or not the instruction touches fp\n+;; registers.\n+(define_attr \"fp\" \"no,yes\" (const_string \"no\"))\n+\n+;; Attribute that specifies whether or not the instruction touches simd\n+;; registers.\n+(define_attr \"simd\" \"no,yes\" (const_string \"no\"))\n+\n+(define_attr \"length\" \"\"\n+  (cond [(not (eq_attr \"sync_memory\" \"none\"))\n+\t   (symbol_ref \"aarch64_sync_loop_insns (insn, operands) * 4\")\n+\t] (const_int 4)))\n+\n+;; Attribute that controls whether an alternative is enabled or not.\n+;; Currently it is only used to disable alternatives which touch fp or simd\n+;; registers when -mgeneral-regs-only is specified.\n+(define_attr \"enabled\" \"no,yes\"\n+  (cond [(ior\n+\t(and (eq_attr \"fp\" \"yes\")\n+\t     (eq (symbol_ref \"TARGET_FLOAT\") (const_int 0)))\n+\t(and (eq_attr \"simd\" \"yes\")\n+\t     (eq (symbol_ref \"TARGET_SIMD\") (const_int 0))))\n+\t     (const_string \"no\")\n+\t] (const_string \"yes\")))\n+\n+;; -------------------------------------------------------------------\n+;; Pipeline descriptions and scheduling\n+;; -------------------------------------------------------------------\n+\n+;; Processor types.\n+(include \"aarch64-tune.md\")\n+\n+;; Scheduling\n+(include \"aarch64-generic.md\")\n+(include \"large.md\")\n+(include \"small.md\")\n+\n+;; -------------------------------------------------------------------\n+;; Jumps and other miscellaneous insns\n+;; -------------------------------------------------------------------\n+\n+(define_insn \"indirect_jump\"\n+  [(set (pc) (match_operand:DI 0 \"register_operand\" \"r\"))]\n+  \"\"\n+  \"br\\\\t%0\"\n+  [(set_attr \"v8type\" \"branch\")]\n+)\n+\n+(define_insn \"jump\"\n+  [(set (pc) (label_ref (match_operand 0 \"\" \"\")))]\n+  \"\"\n+  \"b\\\\t%l0\"\n+  [(set_attr \"v8type\" \"branch\")]\n+)\n+\n+(define_expand \"cbranch<mode>4\"\n+  [(set (pc) (if_then_else (match_operator 0 \"aarch64_comparison_operator\"\n+\t\t\t    [(match_operand:GPI 1 \"register_operand\" \"\")\n+\t\t\t     (match_operand:GPI 2 \"aarch64_plus_operand\" \"\")])\n+\t\t\t   (label_ref (match_operand 3 \"\" \"\"))\n+\t\t\t   (pc)))]\n+  \"\"\n+  \"\n+  operands[1] = aarch64_gen_compare_reg (GET_CODE (operands[0]), operands[1],\n+\t\t\t\t\t operands[2]);\n+  operands[2] = const0_rtx;\n+  \"\n+)\n+\n+(define_expand \"cbranch<mode>4\"\n+  [(set (pc) (if_then_else (match_operator 0 \"aarch64_comparison_operator\"\n+\t\t\t    [(match_operand:GPF 1 \"register_operand\" \"\")\n+\t\t\t     (match_operand:GPF 2 \"aarch64_reg_or_zero\" \"\")])\n+\t\t\t   (label_ref (match_operand 3 \"\" \"\"))\n+\t\t\t   (pc)))]\n+  \"\"\n+  \"\n+  operands[1] = aarch64_gen_compare_reg (GET_CODE (operands[0]), operands[1],\n+\t\t\t\t\t operands[2]);\n+  operands[2] = const0_rtx;\n+  \"\n+)\n+\n+(define_insn \"*condjump\"\n+  [(set (pc) (if_then_else (match_operator 0 \"aarch64_comparison_operator\"\n+\t\t\t    [(match_operand 1 \"cc_register\" \"\") (const_int 0)])\n+\t\t\t   (label_ref (match_operand 2 \"\" \"\"))\n+\t\t\t   (pc)))]\n+  \"\"\n+  \"b%m0\\\\t%l2\"\n+  [(set_attr \"v8type\" \"branch\")]\n+)\n+\n+(define_expand \"casesi\"\n+  [(match_operand:SI 0 \"register_operand\" \"\")\t; Index\n+   (match_operand:SI 1 \"const_int_operand\" \"\")\t; Lower bound\n+   (match_operand:SI 2 \"const_int_operand\" \"\")\t; Total range\n+   (match_operand:DI 3 \"\" \"\")\t\t\t; Table label\n+   (match_operand:DI 4 \"\" \"\")]\t\t\t; Out of range label\n+  \"\"\n+  {\n+    if (operands[1] != const0_rtx)\n+      {\n+\trtx reg = gen_reg_rtx (SImode);\n+\n+\t/* Canonical RTL says that if you have:\n+\n+\t   (minus (X) (CONST))\n+\n+           then this should be emitted as:\n+\n+           (plus (X) (-CONST))\n+\n+\t   The use of trunc_int_for_mode ensures that the resulting\n+\t   constant can be represented in SImode, this is important\n+\t   for the corner case where operand[1] is INT_MIN.  */\n+\n+\toperands[1] = GEN_INT (trunc_int_for_mode (-INTVAL (operands[1]), SImode));\n+\n+\tif (!(*insn_data[CODE_FOR_addsi3].operand[2].predicate)\n+\t      (operands[1], SImode))\n+\t  operands[1] = force_reg (SImode, operands[1]);\n+\temit_insn (gen_addsi3 (reg, operands[0], operands[1]));\n+\toperands[0] = reg;\n+      }\n+\n+    if (!aarch64_plus_operand (operands[2], SImode))\n+      operands[2] = force_reg (SImode, operands[2]);\n+    emit_jump_insn (gen_cbranchsi4 (gen_rtx_GTU (SImode, const0_rtx,\n+\t\t\t\t\t\t const0_rtx),\n+\t\t\t\t    operands[0], operands[2], operands[4]));\n+\n+    operands[2] = force_reg (DImode, gen_rtx_LABEL_REF (VOIDmode, operands[3]));\n+    emit_jump_insn (gen_casesi_dispatch (operands[2], operands[0],\n+\t\t\t\t\t operands[3]));\n+    DONE;\n+  }\n+)\n+\n+(define_insn \"casesi_dispatch\"\n+  [(parallel\n+    [(set (pc)\n+\t  (mem:DI (unspec [(match_operand:DI 0 \"register_operand\" \"r\")\n+\t\t\t   (match_operand:SI 1 \"register_operand\" \"r\")]\n+\t\t\tUNSPEC_CASESI)))\n+     (clobber (reg:CC CC_REGNUM))\n+     (clobber (match_scratch:DI 3 \"=r\"))\n+     (clobber (match_scratch:DI 4 \"=r\"))\n+     (use (label_ref (match_operand 2 \"\" \"\")))])]\n+  \"\"\n+  \"*\n+  return aarch64_output_casesi (operands);\n+  \"\n+  [(set_attr \"length\" \"16\")\n+   (set_attr \"v8type\" \"branch\")]\n+)\n+\n+(define_insn \"nop\"\n+  [(unspec[(const_int 0)] UNSPEC_NOP)]\n+  \"\"\n+  \"nop\"\n+  [(set_attr \"v8type\" \"misc\")]\n+)\n+\n+(define_expand \"prologue\"\n+  [(clobber (const_int 0))]\n+  \"\"\n+  \"\n+  aarch64_expand_prologue ();\n+  DONE;\n+  \"\n+)\n+\n+(define_expand \"epilogue\"\n+  [(clobber (const_int 0))]\n+  \"\"\n+  \"\n+  aarch64_expand_epilogue (false);\n+  DONE;\n+  \"\n+)\n+\n+(define_expand \"sibcall_epilogue\"\n+  [(clobber (const_int 0))]\n+  \"\"\n+  \"\n+  aarch64_expand_epilogue (true);\n+  DONE;\n+  \"\n+)\n+\n+(define_insn \"*do_return\"\n+  [(return)]\n+  \"\"\n+  \"ret\"\n+  [(set_attr \"v8type\" \"branch\")]\n+)\n+\n+(define_insn \"eh_return\"\n+  [(unspec_volatile [(match_operand:DI 0 \"register_operand\" \"r\")]\n+    UNSPECV_EH_RETURN)]\n+  \"\"\n+  \"#\"\n+  [(set_attr \"v8type\" \"branch\")]\n+)\n+\n+(define_split\n+  [(unspec_volatile [(match_operand:DI 0 \"register_operand\" \"\")]\n+    UNSPECV_EH_RETURN)]\n+  \"reload_completed\"\n+  [(set (match_dup 1) (match_dup 0))]\n+  {\n+    operands[1] = aarch64_final_eh_return_addr ();\n+  }\n+)\n+\n+(define_insn \"*cb<optab><mode>1\"\n+  [(set (pc) (if_then_else (EQL (match_operand:GPI 0 \"register_operand\" \"r\")\n+\t\t\t\t(const_int 0))\n+\t\t\t   (label_ref (match_operand 1 \"\" \"\"))\n+\t\t\t   (pc)))]\n+  \"\"\n+  \"<cbz>\\\\t%<w>0, %l1\"\n+  [(set_attr \"v8type\" \"branch\")]\n+)\n+\n+(define_insn \"*tb<optab><mode>1\"\n+  [(set (pc) (if_then_else\n+\t      (EQL (zero_extract:DI (match_operand:GPI 0 \"register_operand\" \"r\")\n+\t\t\t\t    (const_int 1)\n+\t\t\t\t    (match_operand 1 \"const_int_operand\" \"n\"))\n+\t\t   (const_int 0))\n+\t     (label_ref (match_operand 2 \"\" \"\"))\n+\t     (pc)))\n+   (clobber (match_scratch:DI 3 \"=r\"))]\n+  \"\"\n+  \"*\n+  if (get_attr_length (insn) == 8)\n+    return \\\"ubfx\\\\t%<w>3, %<w>0, %1, #1\\;<cbz>\\\\t%<w>3, %l2\\\";\n+  return \\\"<tbz>\\\\t%<w>0, %1, %l2\\\";\n+  \"\n+  [(set_attr \"v8type\" \"branch\")\n+   (set_attr \"mode\" \"<MODE>\")\n+   (set (attr \"length\")\n+\t(if_then_else (and (ge (minus (match_dup 2) (pc)) (const_int -32768))\n+\t\t\t   (lt (minus (match_dup 2) (pc)) (const_int 32764)))\n+\t\t      (const_int 4)\n+\t\t      (const_int 8)))]\n+)\n+\n+(define_insn \"*cb<optab><mode>1\"\n+  [(set (pc) (if_then_else (LTGE (match_operand:ALLI 0 \"register_operand\" \"r\")\n+\t\t\t\t (const_int 0))\n+\t\t\t   (label_ref (match_operand 1 \"\" \"\"))\n+\t\t\t   (pc)))\n+   (clobber (match_scratch:DI 2 \"=r\"))]\n+  \"\"\n+  \"*\n+  if (get_attr_length (insn) == 8)\n+    return \\\"ubfx\\\\t%<w>2, %<w>0, <sizem1>, #1\\;<cbz>\\\\t%<w>2, %l1\\\";\n+  return \\\"<tbz>\\\\t%<w>0, <sizem1>, %l1\\\";\n+  \"\n+  [(set_attr \"v8type\" \"branch\")\n+   (set_attr \"mode\" \"<MODE>\")\n+   (set (attr \"length\")\n+\t(if_then_else (and (ge (minus (match_dup 1) (pc)) (const_int -32768))\n+\t\t\t   (lt (minus (match_dup 1) (pc)) (const_int 32764)))\n+\t\t      (const_int 4)\n+\t\t      (const_int 8)))]\n+)\n+\n+;; -------------------------------------------------------------------\n+;; Subroutine calls and sibcalls\n+;; -------------------------------------------------------------------\n+\n+(define_expand \"call\"\n+  [(parallel [(call (match_operand 0 \"memory_operand\" \"\")\n+\t\t    (match_operand 1 \"general_operand\" \"\"))\n+\t      (use (match_operand 2 \"\" \"\"))\n+\t      (clobber (reg:DI LR_REGNUM))])]\n+  \"\"\n+  \"\n+  {\n+    rtx callee;\n+\n+    /* In an untyped call, we can get NULL for operand 2.  */\n+    if (operands[2] == NULL)\n+      operands[2] = const0_rtx;\n+\n+    /* Decide if we should generate indirect calls by loading the\n+       64-bit address of the callee into a register before performing\n+       the branch-and-link.  */\n+    callee = XEXP (operands[0], 0);\n+    if (GET_CODE (callee) == SYMBOL_REF\n+\t? aarch64_is_long_call_p (callee)\n+\t: !REG_P (callee))\n+      XEXP (operands[0], 0) = force_reg (Pmode, callee);\n+  }\"\n+)\n+\n+(define_insn \"*call_reg\"\n+  [(call (mem:DI (match_operand:DI 0 \"register_operand\" \"r\"))\n+\t (match_operand 1 \"\" \"\"))\n+   (use (match_operand 2 \"\" \"\"))\n+   (clobber (reg:DI LR_REGNUM))]\n+  \"\"\n+  \"blr\\\\t%0\"\n+  [(set_attr \"v8type\" \"call\")]\n+)\n+\n+(define_insn \"*call_symbol\"\n+  [(call (mem:DI (match_operand:DI 0 \"\" \"\"))\n+\t (match_operand 1 \"\" \"\"))\n+   (use (match_operand 2 \"\" \"\"))\n+   (clobber (reg:DI LR_REGNUM))]\n+  \"GET_CODE (operands[0]) == SYMBOL_REF\n+   && !aarch64_is_long_call_p (operands[0])\"\n+  \"bl\\\\t%a0\"\n+  [(set_attr \"v8type\" \"call\")]\n+)\n+\n+(define_expand \"call_value\"\n+  [(parallel [(set (match_operand 0 \"\" \"\")\n+\t\t   (call (match_operand 1 \"memory_operand\" \"\")\n+\t\t\t (match_operand 2 \"general_operand\" \"\")))\n+\t      (use (match_operand 3 \"\" \"\"))\n+\t      (clobber (reg:DI LR_REGNUM))])]\n+  \"\"\n+  \"\n+  {\n+    rtx callee;\n+\n+    /* In an untyped call, we can get NULL for operand 3.  */\n+    if (operands[3] == NULL)\n+      operands[3] = const0_rtx;\n+\n+    /* Decide if we should generate indirect calls by loading the\n+       64-bit address of the callee into a register before performing\n+       the branch-and-link.  */\n+    callee = XEXP (operands[1], 0);\n+    if (GET_CODE (callee) == SYMBOL_REF\n+\t? aarch64_is_long_call_p (callee)\n+\t: !REG_P (callee))\n+      XEXP (operands[1], 0) = force_reg (Pmode, callee);\n+  }\"\n+)\n+\n+(define_insn \"*call_value_reg\"\n+  [(set (match_operand 0 \"\" \"\")\n+\t(call (mem:DI (match_operand:DI 1 \"register_operand\" \"r\"))\n+\t\t      (match_operand 2 \"\" \"\")))\n+   (use (match_operand 3 \"\" \"\"))\n+   (clobber (reg:DI LR_REGNUM))]\n+  \"\"\n+  \"blr\\\\t%1\"\n+  [(set_attr \"v8type\" \"call\")]\n+)\n+\n+(define_insn \"*call_value_symbol\"\n+  [(set (match_operand 0 \"\" \"\")\n+\t(call (mem:DI (match_operand:DI 1 \"\" \"\"))\n+\t      (match_operand 2 \"\" \"\")))\n+   (use (match_operand 3 \"\" \"\"))\n+   (clobber (reg:DI LR_REGNUM))]\n+  \"GET_CODE (operands[1]) == SYMBOL_REF\n+   && !aarch64_is_long_call_p (operands[1])\"\n+  \"bl\\\\t%a1\"\n+  [(set_attr \"v8type\" \"call\")]\n+)\n+\n+(define_expand \"sibcall\"\n+  [(parallel [(call (match_operand 0 \"memory_operand\" \"\")\n+\t\t    (match_operand 1 \"general_operand\" \"\"))\n+\t      (return)\n+\t      (use (match_operand 2 \"\" \"\"))])]\n+  \"\"\n+  {\n+    if (operands[2] == NULL_RTX)\n+      operands[2] = const0_rtx;\n+  }\n+)\n+\n+(define_expand \"sibcall_value\"\n+  [(parallel [(set (match_operand 0 \"\" \"\")\n+\t\t   (call (match_operand 1 \"memory_operand\" \"\")\n+\t\t\t (match_operand 2 \"general_operand\" \"\")))\n+\t      (return)\n+\t      (use (match_operand 3 \"\" \"\"))])]\n+  \"\"\n+  {\n+    if (operands[3] == NULL_RTX)\n+      operands[3] = const0_rtx;\n+  }\n+)\n+\n+(define_insn \"*sibcall_insn\"\n+  [(call (mem:DI (match_operand:DI 0 \"\" \"X\"))\n+\t (match_operand 1 \"\" \"\"))\n+   (return)\n+   (use (match_operand 2 \"\" \"\"))]\n+  \"GET_CODE (operands[0]) == SYMBOL_REF\"\n+  \"b\\\\t%a0\"\n+  [(set_attr \"v8type\" \"branch\")]\n+)\n+\n+(define_insn \"*sibcall_value_insn\"\n+  [(set (match_operand 0 \"\" \"\")\n+\t(call (mem:DI (match_operand 1 \"\" \"X\"))\n+\t      (match_operand 2 \"\" \"\")))\n+   (return)\n+   (use (match_operand 3 \"\" \"\"))]\n+  \"GET_CODE (operands[1]) == SYMBOL_REF\"\n+  \"b\\\\t%a1\"\n+  [(set_attr \"v8type\" \"branch\")]\n+)\n+\n+;; Call subroutine returning any type.\n+\n+(define_expand \"untyped_call\"\n+  [(parallel [(call (match_operand 0 \"\")\n+\t\t    (const_int 0))\n+\t      (match_operand 1 \"\")\n+\t      (match_operand 2 \"\")])]\n+  \"\"\n+{\n+  int i;\n+\n+  emit_call_insn (GEN_CALL (operands[0], const0_rtx, NULL, const0_rtx));\n+\n+  for (i = 0; i < XVECLEN (operands[2], 0); i++)\n+    {\n+      rtx set = XVECEXP (operands[2], 0, i);\n+      emit_move_insn (SET_DEST (set), SET_SRC (set));\n+    }\n+\n+  /* The optimizer does not know that the call sets the function value\n+     registers we stored in the result block.  We avoid problems by\n+     claiming that all hard registers are used and clobbered at this\n+     point.  */\n+  emit_insn (gen_blockage ());\n+  DONE;\n+})\n+\n+;; -------------------------------------------------------------------\n+;; Moves\n+;; -------------------------------------------------------------------\n+\n+(define_expand \"mov<mode>\"\n+  [(set (match_operand:SHORT 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:SHORT 1 \"general_operand\" \"\"))]\n+  \"\"\n+  \"\n+    if (GET_CODE (operands[0]) == MEM && operands[1] != const0_rtx)\n+      operands[1] = force_reg (<MODE>mode, operands[1]);\n+  \"\n+)\n+\n+(define_insn \"*mov<mode>_aarch64\"\n+  [(set (match_operand:SHORT 0 \"nonimmediate_operand\" \"=r,r,r,m,  r,*w\")\n+        (match_operand:SHORT 1 \"general_operand\"      \" r,M,m,rZ,*w,r\"))]\n+  \"(register_operand (operands[0], <MODE>mode)\n+    || aarch64_reg_or_zero (operands[1], <MODE>mode))\"\n+  \"@\n+   mov\\\\t%w0, %w1\n+   mov\\\\t%w0, %1\n+   ldr<size>\\\\t%w0, %1\n+   str<size>\\\\t%w1, %0\n+   umov\\\\t%w0, %1.<v>[0]\n+   dup\\\\t%0.<Vallxd>, %w1\"\n+  [(set_attr \"v8type\" \"move,alu,load1,store1,*,*\")\n+   (set_attr \"simd_type\" \"*,*,*,*,simd_movgp,simd_dupgp\")\n+   (set_attr \"mode\" \"<MODE>\")\n+   (set_attr \"simd_mode\" \"<MODE>\")]\n+)\n+\n+(define_expand \"mov<mode>\"\n+  [(set (match_operand:GPI 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:GPI 1 \"general_operand\" \"\"))]\n+  \"\"\n+  \"\n+    if (GET_CODE (operands[0]) == MEM && operands[1] != const0_rtx)\n+      operands[1] = force_reg (<MODE>mode, operands[1]);\n+\n+    if (CONSTANT_P (operands[1]))\n+      {\n+\taarch64_expand_mov_immediate (operands[0], operands[1]);\n+\tDONE;\n+      }\n+  \"\n+)\n+\n+(define_insn \"*movsi_aarch64\"\n+  [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=r,r,r,m, *w, r,*w\")\n+\t(match_operand:SI 1 \"aarch64_mov_operand\"     \" r,M,m,rZ,rZ,*w,*w\"))]\n+  \"(register_operand (operands[0], SImode)\n+    || aarch64_reg_or_zero (operands[1], SImode))\"\n+  \"@\n+   mov\\\\t%w0, %w1\n+   mov\\\\t%w0, %1\n+   ldr\\\\t%w0, %1\n+   str\\\\t%w1, %0\n+   fmov\\\\t%s0, %w1\n+   fmov\\\\t%w0, %s1\n+   fmov\\\\t%s0, %s1\"\n+  [(set_attr \"v8type\" \"move,alu,load1,store1,fmov,fmov,fmov\")\n+   (set_attr \"mode\" \"SI\")\n+   (set_attr \"fp\" \"*,*,*,*,yes,yes,yes\")]\n+)\n+\n+(define_insn \"*movdi_aarch64\"\n+  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=r,k,r,r,r,m, r,  r,  *w, r,*w,w\")\n+\t(match_operand:DI 1 \"aarch64_mov_operand\"  \" r,r,k,N,m,rZ,Usa,Ush,rZ,*w,*w,Dd\"))]\n+  \"(register_operand (operands[0], DImode)\n+    || aarch64_reg_or_zero (operands[1], DImode))\"\n+  \"@\n+   mov\\\\t%x0, %x1\n+   mov\\\\t%0, %x1\n+   mov\\\\t%x0, %1\n+   mov\\\\t%x0, %1\n+   ldr\\\\t%x0, %1\n+   str\\\\t%x1, %0\n+   adr\\\\t%x0, %a1\n+   adrp\\\\t%x0, %A1\n+   fmov\\\\t%d0, %x1\n+   fmov\\\\t%x0, %d1\n+   fmov\\\\t%d0, %d1\n+   movi\\\\t%d0, %1\"\n+  [(set_attr \"v8type\" \"move,move,move,alu,load1,store1,adr,adr,fmov,fmov,fmov,fmov\")\n+   (set_attr \"mode\" \"DI\")\n+   (set_attr \"fp\" \"*,*,*,*,*,*,*,*,yes,yes,yes,yes\")]\n+)\n+\n+(define_insn \"insv_imm<mode>\"\n+  [(set (zero_extract:GPI (match_operand:GPI 0 \"register_operand\" \"+r\")\n+\t\t\t  (const_int 16)\n+\t\t\t  (match_operand 1 \"const_int_operand\" \"n\"))\n+\t(match_operand 2 \"const_int_operand\" \"n\"))]\n+  \"INTVAL (operands[1]) < GET_MODE_BITSIZE (<MODE>mode)\n+   && INTVAL (operands[1]) % 16 == 0\n+   && INTVAL (operands[2]) <= 0xffff\"\n+  \"movk\\\\t%<w>0, %2, lsl %1\"\n+  [(set_attr \"v8type\" \"movk\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_expand \"movti\"\n+  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:TI 1 \"general_operand\" \"\"))]\n+  \"\"\n+  \"\n+    if (GET_CODE (operands[0]) == MEM && operands[1] != const0_rtx)\n+      operands[1] = force_reg (TImode, operands[1]);\n+  \"\n+)\n+\n+(define_insn \"*movti_aarch64\"\n+  [(set (match_operand:TI 0\n+\t \"nonimmediate_operand\"  \"=r, *w,r ,*w,r  ,Ump,Ump,*w,m\")\n+\t(match_operand:TI 1\n+\t \"aarch64_movti_operand\" \" rn,r ,*w,*w,Ump,r  ,Z  , m,*w\"))]\n+  \"(register_operand (operands[0], TImode)\n+    || aarch64_reg_or_zero (operands[1], TImode))\"\n+  \"@\n+   #\n+   #\n+   #\n+   orr\\\\t%0.16b, %1.16b, %1.16b\n+   ldp\\\\t%0, %H0, %1\n+   stp\\\\t%1, %H1, %0\n+   stp\\\\txzr, xzr, %0\n+   ldr\\\\t%q0, %1\n+   str\\\\t%q1, %0\"\n+  [(set_attr \"v8type\" \"move2,fmovi2f,fmovf2i,*, \\\n+\t\t       load2,store2,store2,fpsimd_load,fpsimd_store\")\n+   (set_attr \"simd_type\" \"*,*,*,simd_move,*,*,*,*,*\")\n+   (set_attr \"mode\" \"DI,DI,DI,TI,DI,DI,DI,TI,TI\")\n+   (set_attr \"length\" \"8,8,8,4,4,4,4,4,4\")\n+   (set_attr \"fp\" \"*,*,*,*,*,*,*,yes,yes\")\n+   (set_attr \"simd\" \"*,*,*,yes,*,*,*,*,*\")])\n+\n+;; Split a TImode register-register or register-immediate move into\n+;; its component DImode pieces, taking care to handle overlapping\n+;; source and dest registers.\n+(define_split\n+   [(set (match_operand:TI 0 \"register_operand\" \"\")\n+\t (match_operand:TI 1 \"aarch64_reg_or_imm\" \"\"))]\n+  \"reload_completed && aarch64_split_128bit_move_p (operands[0], operands[1])\"\n+  [(const_int 0)]\n+{\n+  aarch64_split_128bit_move (operands[0], operands[1]);\n+  DONE;\n+})\n+\n+(define_expand \"mov<mode>\"\n+  [(set (match_operand:GPF 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:GPF 1 \"general_operand\" \"\"))]\n+  \"\"\n+  \"\n+    if (!TARGET_FLOAT)\n+     {\n+\tsorry (\\\"%qs and floating point code\\\", \\\"-mgeneral-regs-only\\\");\n+\tFAIL;\n+     }\n+\n+    if (GET_CODE (operands[0]) == MEM)\n+      operands[1] = force_reg (<MODE>mode, operands[1]);\n+  \"\n+)\n+\n+(define_insn \"*movsf_aarch64\"\n+  [(set (match_operand:SF 0 \"nonimmediate_operand\" \"= w,?r,w,w,m,r,m ,r\")\n+\t(match_operand:SF 1 \"general_operand\"      \"?rY, w,w,m,w,m,rY,r\"))]\n+  \"TARGET_FLOAT && (register_operand (operands[0], SFmode)\n+    || register_operand (operands[1], SFmode))\"\n+  \"@\n+   fmov\\\\t%s0, %w1\n+   fmov\\\\t%w0, %s1\n+   fmov\\\\t%s0, %s1\n+   ldr\\\\t%s0, %1\n+   str\\\\t%s1, %0\n+   ldr\\\\t%w0, %1\n+   str\\\\t%w1, %0\n+   mov\\\\t%w0, %w1\"\n+  [(set_attr \"v8type\" \"fmovi2f,fmovf2i,fmov,fpsimd_load,fpsimd_store,fpsimd_load,fpsimd_store,fmov\")\n+   (set_attr \"mode\" \"SF\")]\n+)\n+\n+(define_insn \"*movdf_aarch64\"\n+  [(set (match_operand:DF 0 \"nonimmediate_operand\" \"= w,?r,w,w,m,r,m ,r\")\n+\t(match_operand:DF 1 \"general_operand\"      \"?rY, w,w,m,w,m,rY,r\"))]\n+  \"TARGET_FLOAT && (register_operand (operands[0], DFmode)\n+    || register_operand (operands[1], DFmode))\"\n+  \"@\n+   fmov\\\\t%d0, %x1\n+   fmov\\\\t%x0, %d1\n+   fmov\\\\t%d0, %d1\n+   ldr\\\\t%d0, %1\n+   str\\\\t%d1, %0\n+   ldr\\\\t%x0, %1\n+   str\\\\t%x1, %0\n+   mov\\\\t%x0, %x1\"\n+  [(set_attr \"v8type\" \"fmovi2f,fmovf2i,fmov,fpsimd_load,fpsimd_store,fpsimd_load,fpsimd_store,move\")\n+   (set_attr \"mode\" \"DF\")]\n+)\n+\n+(define_expand \"movtf\"\n+  [(set (match_operand:TF 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:TF 1 \"general_operand\" \"\"))]\n+  \"\"\n+  \"\n+    if (!TARGET_FLOAT)\n+     {\n+\tsorry (\\\"%qs and floating point code\\\", \\\"-mgeneral-regs-only\\\");\n+\tFAIL;\n+     }\n+\n+    if (GET_CODE (operands[0]) == MEM)\n+      operands[1] = force_reg (TFmode, operands[1]);\n+  \"\n+)\n+\n+(define_insn \"*movtf_aarch64\"\n+  [(set (match_operand:TF 0\n+\t \"nonimmediate_operand\" \"=w,?&r,w ,?r,w,?w,w,m,?r ,Ump\")\n+\t(match_operand:TF 1\n+\t \"general_operand\"      \" w,?r, ?r,w ,Y,Y ,m,w,Ump,?rY\"))]\n+  \"TARGET_FLOAT && (register_operand (operands[0], TFmode)\n+    || register_operand (operands[1], TFmode))\"\n+  \"@\n+   orr\\\\t%0.16b, %1.16b, %1.16b\n+   mov\\\\t%0, %1\\;mov\\\\t%H0, %H1\n+   fmov\\\\t%d0, %Q1\\;fmov\\\\t%0.d[1], %R1\n+   fmov\\\\t%Q0, %d1\\;fmov\\\\t%R0, %1.d[1]\n+   movi\\\\t%0.2d, #0\n+   fmov\\\\t%s0, wzr\n+   ldr\\\\t%q0, %1\n+   str\\\\t%q1, %0\n+   ldp\\\\t%0, %H0, %1\n+   stp\\\\t%1, %H1, %0\"\n+  [(set_attr \"v8type\" \"logic,move2,fmovi2f,fmovf2i,fconst,fconst,fpsimd_load,fpsimd_store,fpsimd_load2,fpsimd_store2\")\n+   (set_attr \"mode\" \"DF,DF,DF,DF,DF,DF,TF,TF,DF,DF\")\n+   (set_attr \"length\" \"4,8,8,8,4,4,4,4,4,4\")\n+   (set_attr \"fp\" \"*,*,yes,yes,*,yes,yes,yes,*,*\")\n+   (set_attr \"simd\" \"yes,*,*,*,yes,*,*,*,*,*\")]\n+)\n+\n+\n+;; Operands 1 and 3 are tied together by the final condition; so we allow\n+;; fairly lax checking on the second memory operation.\n+(define_insn \"load_pair<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(match_operand:GPI 1 \"aarch64_mem_pair_operand\" \"Ump\"))\n+   (set (match_operand:GPI 2 \"register_operand\" \"=r\")\n+        (match_operand:GPI 3 \"memory_operand\" \"m\"))]\n+  \"rtx_equal_p (XEXP (operands[3], 0),\n+\t\tplus_constant (Pmode,\n+\t\t\t       XEXP (operands[1], 0),\n+\t\t\t       GET_MODE_SIZE (<MODE>mode)))\"\n+  \"ldp\\\\t%<w>0, %<w>2, %1\"\n+  [(set_attr \"v8type\" \"load2\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; Operands 0 and 2 are tied together by the final condition; so we allow\n+;; fairly lax checking on the second memory operation.\n+(define_insn \"store_pair<mode>\"\n+  [(set (match_operand:GPI 0 \"aarch64_mem_pair_operand\" \"=Ump\")\n+\t(match_operand:GPI 1 \"register_operand\" \"r\"))\n+   (set (match_operand:GPI 2 \"memory_operand\" \"=m\")\n+        (match_operand:GPI 3 \"register_operand\" \"r\"))]\n+  \"rtx_equal_p (XEXP (operands[2], 0),\n+\t\tplus_constant (Pmode,\n+\t\t\t       XEXP (operands[0], 0),\n+\t\t\t       GET_MODE_SIZE (<MODE>mode)))\"\n+  \"stp\\\\t%<w>1, %<w>3, %0\"\n+  [(set_attr \"v8type\" \"store2\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; Operands 1 and 3 are tied together by the final condition; so we allow\n+;; fairly lax checking on the second memory operation.\n+(define_insn \"load_pair<mode>\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+\t(match_operand:GPF 1 \"aarch64_mem_pair_operand\" \"Ump\"))\n+   (set (match_operand:GPF 2 \"register_operand\" \"=w\")\n+        (match_operand:GPF 3 \"memory_operand\" \"m\"))]\n+  \"rtx_equal_p (XEXP (operands[3], 0),\n+\t\tplus_constant (Pmode,\n+\t\t\t       XEXP (operands[1], 0),\n+\t\t\t       GET_MODE_SIZE (<MODE>mode)))\"\n+  \"ldp\\\\t%<w>0, %<w>2, %1\"\n+  [(set_attr \"v8type\" \"fpsimd_load2\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; Operands 0 and 2 are tied together by the final condition; so we allow\n+;; fairly lax checking on the second memory operation.\n+(define_insn \"store_pair<mode>\"\n+  [(set (match_operand:GPF 0 \"aarch64_mem_pair_operand\" \"=Ump\")\n+\t(match_operand:GPF 1 \"register_operand\" \"w\"))\n+   (set (match_operand:GPF 2 \"memory_operand\" \"=m\")\n+        (match_operand:GPF 3 \"register_operand\" \"w\"))]\n+  \"rtx_equal_p (XEXP (operands[2], 0),\n+\t\tplus_constant (Pmode,\n+\t\t\t       XEXP (operands[0], 0),\n+\t\t\t       GET_MODE_SIZE (<MODE>mode)))\"\n+  \"stp\\\\t%<w>1, %<w>3, %0\"\n+  [(set_attr \"v8type\" \"fpsimd_load2\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; Load pair with writeback.  This is primarily used in function epilogues\n+;; when restoring [fp,lr]\n+(define_insn \"loadwb_pair<GPI:mode>_<PTR:mode>\"\n+  [(parallel\n+    [(set (match_operand:PTR 0 \"register_operand\" \"=k\")\n+          (plus:PTR (match_operand:PTR 1 \"register_operand\" \"0\")\n+                  (match_operand:PTR 4 \"const_int_operand\" \"n\")))\n+     (set (match_operand:GPI 2 \"register_operand\" \"=r\")\n+          (mem:GPI (plus:PTR (match_dup 1)\n+                   (match_dup 4))))\n+     (set (match_operand:GPI 3 \"register_operand\" \"=r\")\n+          (mem:GPI (plus:PTR (match_dup 1)\n+                   (match_operand:PTR 5 \"const_int_operand\" \"n\"))))])]\n+  \"INTVAL (operands[5]) == INTVAL (operands[4]) + GET_MODE_SIZE (<GPI:MODE>mode)\"\n+  \"ldp\\\\t%<w>2, %<w>3, [%1], %4\"\n+  [(set_attr \"v8type\" \"load2\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+;; Store pair with writeback.  This is primarily used in function prologues\n+;; when saving [fp,lr]\n+(define_insn \"storewb_pair<GPI:mode>_<PTR:mode>\"\n+  [(parallel\n+    [(set (match_operand:PTR 0 \"register_operand\" \"=&k\")\n+          (plus:PTR (match_operand:PTR 1 \"register_operand\" \"0\")\n+                  (match_operand:PTR 4 \"const_int_operand\" \"n\")))\n+     (set (mem:GPI (plus:PTR (match_dup 0)\n+                   (match_dup 4)))\n+          (match_operand:GPI 2 \"register_operand\" \"r\"))\n+     (set (mem:GPI (plus:PTR (match_dup 0)\n+                   (match_operand:PTR 5 \"const_int_operand\" \"n\")))\n+          (match_operand:GPI 3 \"register_operand\" \"r\"))])]\n+  \"INTVAL (operands[5]) == INTVAL (operands[4]) + GET_MODE_SIZE (<GPI:MODE>mode)\"\n+  \"stp\\\\t%<w>2, %<w>3, [%0, %4]!\"\n+  [(set_attr \"v8type\" \"store2\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+;; -------------------------------------------------------------------\n+;; Sign/Zero extension\n+;; -------------------------------------------------------------------\n+\n+(define_expand \"<optab>sidi2\"\n+  [(set (match_operand:DI 0 \"register_operand\")\n+\t(ANY_EXTEND:DI (match_operand:SI 1 \"nonimmediate_operand\")))]\n+  \"\"\n+)\n+\n+(define_insn \"*extendsidi2_aarch64\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r,r\")\n+        (sign_extend:DI (match_operand:SI 1 \"nonimmediate_operand\" \"r,m\")))]\n+  \"\"\n+  \"@\n+   sxtw\\t%0, %w1\n+   ldrsw\\t%0, %1\"\n+  [(set_attr \"v8type\" \"extend,load1\")\n+   (set_attr \"mode\" \"DI\")]\n+)\n+\n+(define_insn \"*zero_extendsidi2_aarch64\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r,r\")\n+        (zero_extend:DI (match_operand:SI 1 \"nonimmediate_operand\" \"r,m\")))]\n+  \"\"\n+  \"@\n+   uxtw\\t%0, %w1\n+   ldr\\t%w0, %1\"\n+  [(set_attr \"v8type\" \"extend,load1\")\n+   (set_attr \"mode\" \"DI\")]\n+)\n+\n+(define_expand \"<ANY_EXTEND:optab><SHORT:mode><GPI:mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\")\n+        (ANY_EXTEND:GPI (match_operand:SHORT 1 \"nonimmediate_operand\")))]\n+  \"\"\n+)\n+\n+(define_insn \"*extend<SHORT:mode><GPI:mode>2_aarch64\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r,r\")\n+        (sign_extend:GPI (match_operand:SHORT 1 \"nonimmediate_operand\" \"r,m\")))]\n+  \"\"\n+  \"@\n+   sxt<SHORT:size>\\t%<GPI:w>0, %w1\n+   ldrs<SHORT:size>\\t%<GPI:w>0, %1\"\n+  [(set_attr \"v8type\" \"extend,load1\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"*zero_extend<SHORT:mode><GPI:mode>2_aarch64\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r,r\")\n+        (zero_extend:GPI (match_operand:SHORT 1 \"nonimmediate_operand\" \"r,m\")))]\n+  \"\"\n+  \"@\n+   uxt<SHORT:size>\\t%<GPI:w>0, %w1\n+   ldr<SHORT:size>\\t%w0, %1\"\n+  [(set_attr \"v8type\" \"extend,load1\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+(define_expand \"<optab>qihi2\"\n+  [(set (match_operand:HI 0 \"register_operand\")\n+        (ANY_EXTEND:HI (match_operand:QI 1 \"nonimmediate_operand\")))]\n+  \"\"\n+)\n+\n+(define_insn \"*<optab>qihi2_aarch64\"\n+  [(set (match_operand:HI 0 \"register_operand\" \"=r,r\")\n+        (ANY_EXTEND:HI (match_operand:QI 1 \"nonimmediate_operand\" \"r,m\")))]\n+  \"\"\n+  \"@\n+   <su>xtb\\t%w0, %w1\n+   <ldrxt>b\\t%w0, %1\"\n+  [(set_attr \"v8type\" \"extend,load1\")\n+   (set_attr \"mode\" \"HI\")]\n+)\n+\n+;; -------------------------------------------------------------------\n+;; Simple arithmetic\n+;; -------------------------------------------------------------------\n+\n+(define_expand \"add<mode>3\"\n+  [(set\n+    (match_operand:GPI 0 \"register_operand\" \"\")\n+    (plus:GPI (match_operand:GPI 1 \"register_operand\" \"\")\n+\t      (match_operand:GPI 2 \"aarch64_pluslong_operand\" \"\")))]\n+  \"\"\n+  \"\n+  if (! aarch64_plus_operand (operands[2], VOIDmode))\n+    {\n+      rtx subtarget = ((optimize && can_create_pseudo_p ())\n+\t\t       ? gen_reg_rtx (<MODE>mode) : operands[0]);\n+      HOST_WIDE_INT imm = INTVAL (operands[2]);\n+\n+      if (imm < 0)\n+\timm = -(-imm & ~0xfff);\n+      else\n+        imm &= ~0xfff;\n+\n+      emit_insn (gen_add<mode>3 (subtarget, operands[1], GEN_INT (imm)));\n+      operands[1] = subtarget;\n+      operands[2] = GEN_INT (INTVAL (operands[2]) - imm);\n+    }\n+  \"\n+)\n+\n+(define_insn \"*addsi3_aarch64\"\n+  [(set\n+    (match_operand:SI 0 \"register_operand\" \"=rk,rk,rk\")\n+    (plus:SI\n+     (match_operand:SI 1 \"register_operand\" \"%rk,rk,rk\")\n+     (match_operand:SI 2 \"aarch64_plus_operand\" \"I,r,J\")))]\n+  \"\"\n+  \"@\n+  add\\\\t%w0, %w1, %2\n+  add\\\\t%w0, %w1, %w2\n+  sub\\\\t%w0, %w1, #%n2\"\n+  [(set_attr \"v8type\" \"alu\")\n+   (set_attr \"mode\" \"SI\")]\n+)\n+\n+(define_insn \"*adddi3_aarch64\"\n+  [(set\n+    (match_operand:DI 0 \"register_operand\" \"=rk,rk,rk,!w\")\n+    (plus:DI\n+     (match_operand:DI 1 \"register_operand\" \"%rk,rk,rk,!w\")\n+     (match_operand:DI 2 \"aarch64_plus_operand\" \"I,r,J,!w\")))]\n+  \"\"\n+  \"@\n+  add\\\\t%x0, %x1, %2\n+  add\\\\t%x0, %x1, %x2\n+  sub\\\\t%x0, %x1, #%n2\n+  add\\\\t%d0, %d1, %d2\"\n+  [(set_attr \"v8type\" \"alu\")\n+   (set_attr \"mode\" \"DI\")\n+   (set_attr \"simd\" \"*,*,*,yes\")]\n+)\n+\n+(define_insn \"*add<mode>3_compare0\"\n+  [(set (reg:CC_NZ CC_REGNUM)\n+\t(compare:CC_NZ\n+\t (plus:GPI (match_operand:GPI 1 \"register_operand\" \"%r,r\")\n+\t\t   (match_operand:GPI 2 \"aarch64_plus_operand\" \"rI,J\"))\n+\t (const_int 0)))\n+   (set (match_operand:GPI 0 \"register_operand\" \"=r,r\")\n+\t(plus:GPI (match_dup 1) (match_dup 2)))]\n+  \"\"\n+  \"@\n+  adds\\\\t%<w>0, %<w>1, %<w>2\n+  subs\\\\t%<w>0, %<w>1, #%n2\"\n+  [(set_attr \"v8type\" \"alus\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*add<mode>3nr_compare0\"\n+  [(set (reg:CC_NZ CC_REGNUM)\n+\t(compare:CC_NZ\n+\t (plus:GPI (match_operand:GPI 0 \"register_operand\" \"%r,r\")\n+\t\t   (match_operand:GPI 1 \"aarch64_plus_operand\" \"rI,J\"))\n+\t (const_int 0)))]\n+  \"\"\n+  \"@\n+  cmn\\\\t%<w>0, %<w>1\n+  cmp\\\\t%<w>0, #%n1\"\n+  [(set_attr \"v8type\" \"alus\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*add_<shift>_<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(plus:GPI (ASHIFT:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t      (match_operand:QI 2 \"aarch64_shift_imm_<mode>\" \"n\"))\n+\t\t  (match_operand:GPI 3 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"add\\\\t%<w>0, %<w>3, %<w>1, <shift> %2\"\n+  [(set_attr \"v8type\" \"alu_shift\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*add_mul_imm_<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(plus:GPI (mult:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t    (match_operand:QI 2 \"aarch64_pwr_2_<mode>\" \"n\"))\n+\t\t  (match_operand:GPI 3 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"add\\\\t%<w>0, %<w>3, %<w>1, lsl %p2\"\n+  [(set_attr \"v8type\" \"alu_shift\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*add_<optab><ALLX:mode>_<GPI:mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(plus:GPI (ANY_EXTEND:GPI (match_operand:ALLX 1 \"register_operand\" \"r\"))\n+\t\t  (match_operand:GPI 2 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"add\\\\t%<GPI:w>0, %<GPI:w>2, %<GPI:w>1, <su>xt<ALLX:size>\"\n+  [(set_attr \"v8type\" \"alu_ext\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"*add_<optab><ALLX:mode>_shft_<GPI:mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(plus:GPI (ashift:GPI (ANY_EXTEND:GPI\n+\t\t\t       (match_operand:ALLX 1 \"register_operand\" \"r\"))\n+\t\t\t      (match_operand 2 \"aarch64_imm3\" \"Ui3\"))\n+\t\t  (match_operand:GPI 3 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"add\\\\t%<GPI:w>0, %<GPI:w>3, %<GPI:w>1, <su>xt<ALLX:size> %2\"\n+  [(set_attr \"v8type\" \"alu_ext\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"*add_<optab><ALLX:mode>_mult_<GPI:mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(plus:GPI (mult:GPI (ANY_EXTEND:GPI\n+\t\t\t     (match_operand:ALLX 1 \"register_operand\" \"r\"))\n+\t\t\t    (match_operand 2 \"aarch64_pwr_imm3\" \"Up3\"))\n+\t\t  (match_operand:GPI 3 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"add\\\\t%<GPI:w>0, %<GPI:w>3, %<GPI:w>1, <su>xt<ALLX:size> %p2\"\n+  [(set_attr \"v8type\" \"alu_ext\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"*add_<optab><mode>_multp2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(plus:GPI (ANY_EXTRACT:GPI\n+\t\t   (mult:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t     (match_operand 2 \"aarch64_pwr_imm3\" \"Up3\"))\n+\t\t   (match_operand 3 \"const_int_operand\" \"n\")\n+\t\t   (const_int 0))\n+\t\t  (match_operand:GPI 4 \"register_operand\" \"r\")))]\n+  \"aarch64_is_extend_from_extract (<MODE>mode, operands[2], operands[3])\"\n+  \"add\\\\t%<w>0, %<w>4, %<w>1, <su>xt%e3 %p2\"\n+  [(set_attr \"v8type\" \"alu_ext\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*add<mode>3_carryin\"\n+  [(set\n+    (match_operand:GPI 0 \"register_operand\" \"=r\")\n+    (plus:GPI (geu:GPI (reg:CC CC_REGNUM) (const_int 0))\n+\t      (plus:GPI\n+\t\t(match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t(match_operand:GPI 2 \"register_operand\" \"r\"))))]\n+   \"\"\n+   \"adc\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"adc\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*add<mode>3_carryin_alt1\"\n+  [(set\n+    (match_operand:GPI 0 \"register_operand\" \"=r\")\n+    (plus:GPI (plus:GPI\n+\t\t(match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t(match_operand:GPI 2 \"register_operand\" \"r\"))\n+              (geu:GPI (reg:CC CC_REGNUM) (const_int 0))))]\n+   \"\"\n+   \"adc\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"adc\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*add<mode>3_carryin_alt2\"\n+  [(set\n+    (match_operand:GPI 0 \"register_operand\" \"=r\")\n+    (plus:GPI (plus:GPI\n+                (geu:GPI (reg:CC CC_REGNUM) (const_int 0))\n+\t\t(match_operand:GPI 1 \"register_operand\" \"r\"))\n+\t      (match_operand:GPI 2 \"register_operand\" \"r\")))]\n+   \"\"\n+   \"adc\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"adc\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*add<mode>3_carryin_alt3\"\n+  [(set\n+    (match_operand:GPI 0 \"register_operand\" \"=r\")\n+    (plus:GPI (plus:GPI\n+                (geu:GPI (reg:CC CC_REGNUM) (const_int 0))\n+\t\t(match_operand:GPI 2 \"register_operand\" \"r\"))\n+\t      (match_operand:GPI 1 \"register_operand\" \"r\")))]\n+   \"\"\n+   \"adc\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"adc\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*add_uxt<mode>_multp2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(plus:GPI (and:GPI\n+\t\t   (mult:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t     (match_operand 2 \"aarch64_pwr_imm3\" \"Up3\"))\n+\t\t   (match_operand 3 \"const_int_operand\" \"n\"))\n+\t\t  (match_operand:GPI 4 \"register_operand\" \"r\")))]\n+  \"aarch64_uxt_size (exact_log2 (INTVAL (operands[2])), INTVAL (operands[3])) != 0\"\n+  \"*\n+  operands[3] = GEN_INT (aarch64_uxt_size (exact_log2 (INTVAL (operands[2])),\n+\t\t\t\t\t   INTVAL (operands[3])));\n+  return \\\"add\\t%<w>0, %<w>4, %<w>1, uxt%e3 %p2\\\";\"\n+  [(set_attr \"v8type\" \"alu_ext\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"subsi3\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=rk\")\n+\t(minus:SI (match_operand:SI 1 \"register_operand\" \"r\")\n+\t\t   (match_operand:SI 2 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"sub\\\\t%w0, %w1, %w2\"\n+  [(set_attr \"v8type\" \"alu\")\n+   (set_attr \"mode\" \"SI\")]\n+)\n+\n+(define_insn \"subdi3\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=rk,!w\")\n+\t(minus:DI (match_operand:DI 1 \"register_operand\" \"r,!w\")\n+\t\t   (match_operand:DI 2 \"register_operand\" \"r,!w\")))]\n+  \"\"\n+  \"@\n+   sub\\\\t%x0, %x1, %x2\n+   sub\\\\t%d0, %d1, %d2\"\n+  [(set_attr \"v8type\" \"alu\")\n+   (set_attr \"mode\" \"DI\")\n+   (set_attr \"simd\" \"*,yes\")]\n+)\n+\n+\n+(define_insn \"*sub<mode>3_compare0\"\n+  [(set (reg:CC_NZ CC_REGNUM)\n+\t(compare:CC_NZ (minus:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t\t  (match_operand:GPI 2 \"register_operand\" \"r\"))\n+\t\t       (const_int 0)))\n+   (set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(minus:GPI (match_dup 1) (match_dup 2)))]\n+  \"\"\n+  \"subs\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"alus\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*sub_<shift>_<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(minus:GPI (match_operand:GPI 3 \"register_operand\" \"r\")\n+\t\t   (ASHIFT:GPI\n+\t\t    (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t    (match_operand:QI 2 \"aarch64_shift_imm_<mode>\" \"n\"))))]\n+  \"\"\n+  \"sub\\\\t%<w>0, %<w>3, %<w>1, <shift> %2\"\n+  [(set_attr \"v8type\" \"alu_shift\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*sub_mul_imm_<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(minus:GPI (match_operand:GPI 3 \"register_operand\" \"r\")\n+\t\t   (mult:GPI\n+\t\t    (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t    (match_operand:QI 2 \"aarch64_pwr_2_<mode>\" \"n\"))))]\n+  \"\"\n+  \"sub\\\\t%<w>0, %<w>3, %<w>1, lsl %p2\"\n+  [(set_attr \"v8type\" \"alu_shift\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*sub_<optab><ALLX:mode>_<GPI:mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(minus:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t   (ANY_EXTEND:GPI\n+\t\t    (match_operand:ALLX 2 \"register_operand\" \"r\"))))]\n+  \"\"\n+  \"sub\\\\t%<GPI:w>0, %<GPI:w>1, %<GPI:w>2, <su>xt<ALLX:size>\"\n+  [(set_attr \"v8type\" \"alu_ext\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"*sub_<optab><ALLX:mode>_shft_<GPI:mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(minus:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t   (ashift:GPI (ANY_EXTEND:GPI\n+\t\t\t\t(match_operand:ALLX 2 \"register_operand\" \"r\"))\n+\t\t\t       (match_operand 3 \"aarch64_imm3\" \"Ui3\"))))]\n+  \"\"\n+  \"sub\\\\t%<GPI:w>0, %<GPI:w>1, %<GPI:w>2, <su>xt<ALLX:size> %3\"\n+  [(set_attr \"v8type\" \"alu_ext\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"*sub_<optab><mode>_multp2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(minus:GPI (match_operand:GPI 4 \"register_operand\" \"r\")\n+\t\t   (ANY_EXTRACT:GPI\n+\t\t    (mult:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t      (match_operand 2 \"aarch64_pwr_imm3\" \"Up3\"))\n+\t\t    (match_operand 3 \"const_int_operand\" \"n\")\n+\t\t    (const_int 0))))]\n+  \"aarch64_is_extend_from_extract (<MODE>mode, operands[2], operands[3])\"\n+  \"sub\\\\t%<w>0, %<w>4, %<w>1, <su>xt%e3 %p2\"\n+  [(set_attr \"v8type\" \"alu_ext\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*sub_uxt<mode>_multp2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=rk\")\n+\t(minus:GPI (match_operand:GPI 4 \"register_operand\" \"r\")\n+\t\t   (and:GPI\n+\t\t    (mult:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t      (match_operand 2 \"aarch64_pwr_imm3\" \"Up3\"))\n+\t\t    (match_operand 3 \"const_int_operand\" \"n\"))))]\n+  \"aarch64_uxt_size (exact_log2 (INTVAL (operands[2])),INTVAL (operands[3])) != 0\"\n+  \"*\n+  operands[3] = GEN_INT (aarch64_uxt_size (exact_log2 (INTVAL (operands[2])),\n+\t\t\t\t\t   INTVAL (operands[3])));\n+  return \\\"sub\\t%<w>0, %<w>4, %<w>1, uxt%e3 %p2\\\";\"\n+  [(set_attr \"v8type\" \"alu_ext\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"neg<mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(neg:GPI (match_operand:GPI 1 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"neg\\\\t%<w>0, %<w>1\"\n+  [(set_attr \"v8type\" \"alu\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*neg<mode>2_compare0\"\n+  [(set (reg:CC_NZ CC_REGNUM)\n+\t(compare:CC_NZ (neg:GPI (match_operand:GPI 1 \"register_operand\" \"r\"))\n+\t\t       (const_int 0)))\n+   (set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(neg:GPI (match_dup 1)))]\n+  \"\"\n+  \"negs\\\\t%<w>0, %<w>1\"\n+  [(set_attr \"v8type\" \"alus\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*neg_<shift>_<mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(neg:GPI (ASHIFT:GPI\n+\t\t  (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t  (match_operand:QI 2 \"aarch64_shift_imm_<mode>\" \"n\"))))]\n+  \"\"\n+  \"neg\\\\t%<w>0, %<w>1, <shift> %2\"\n+  [(set_attr \"v8type\" \"alu_shift\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*neg_mul_imm_<mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(neg:GPI (mult:GPI\n+\t\t  (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t  (match_operand:QI 2 \"aarch64_pwr_2_<mode>\" \"n\"))))]\n+  \"\"\n+  \"neg\\\\t%<w>0, %<w>1, lsl %p2\"\n+  [(set_attr \"v8type\" \"alu_shift\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"mul<mode>3\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(mult:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t  (match_operand:GPI 2 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"mul\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"mult\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*madd<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(plus:GPI (mult:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t    (match_operand:GPI 2 \"register_operand\" \"r\"))\n+\t\t  (match_operand:GPI 3 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"madd\\\\t%<w>0, %<w>1, %<w>2, %<w>3\"\n+  [(set_attr \"v8type\" \"madd\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*msub<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(minus:GPI (match_operand:GPI 3 \"register_operand\" \"r\")\n+\t\t   (mult:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t     (match_operand:GPI 2 \"register_operand\" \"r\"))))]\n+\n+  \"\"\n+  \"msub\\\\t%<w>0, %<w>1, %<w>2, %<w>3\"\n+  [(set_attr \"v8type\" \"madd\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*mul<mode>_neg\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(mult:GPI (neg:GPI (match_operand:GPI 1 \"register_operand\" \"r\"))\n+\t\t  (match_operand:GPI 2 \"register_operand\" \"r\")))]\n+\n+  \"\"\n+  \"mneg\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"mult\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"<su_optab>mulsidi3\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(mult:DI (ANY_EXTEND:DI (match_operand:SI 1 \"register_operand\" \"r\"))\n+\t\t (ANY_EXTEND:DI (match_operand:SI 2 \"register_operand\" \"r\"))))]\n+  \"\"\n+  \"<su>mull\\\\t%0, %w1, %w2\"\n+  [(set_attr \"v8type\" \"mull\")\n+   (set_attr \"mode\" \"DI\")]\n+)\n+\n+(define_insn \"<su_optab>maddsidi4\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(plus:DI (mult:DI\n+\t\t  (ANY_EXTEND:DI (match_operand:SI 1 \"register_operand\" \"r\"))\n+\t\t  (ANY_EXTEND:DI (match_operand:SI 2 \"register_operand\" \"r\")))\n+\t\t (match_operand:DI 3 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"<su>maddl\\\\t%0, %w1, %w2, %3\"\n+  [(set_attr \"v8type\" \"maddl\")\n+   (set_attr \"mode\" \"DI\")]\n+)\n+\n+(define_insn \"<su_optab>msubsidi4\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(minus:DI\n+\t (match_operand:DI 3 \"register_operand\" \"r\")\n+\t (mult:DI (ANY_EXTEND:DI (match_operand:SI 1 \"register_operand\" \"r\"))\n+\t\t  (ANY_EXTEND:DI\n+\t\t   (match_operand:SI 2 \"register_operand\" \"r\")))))]\n+  \"\"\n+  \"<su>msubl\\\\t%0, %w1, %w2, %3\"\n+  [(set_attr \"v8type\" \"maddl\")\n+   (set_attr \"mode\" \"DI\")]\n+)\n+\n+(define_insn \"*<su_optab>mulsidi_neg\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(mult:DI (neg:DI\n+\t\t  (ANY_EXTEND:DI (match_operand:SI 1 \"register_operand\" \"r\")))\n+\t\t  (ANY_EXTEND:DI (match_operand:SI 2 \"register_operand\" \"r\"))))]\n+  \"\"\n+  \"<su>mnegl\\\\t%0, %w1, %w2\"\n+  [(set_attr \"v8type\" \"mull\")\n+   (set_attr \"mode\" \"DI\")]\n+)\n+\n+(define_insn \"<su>muldi3_highpart\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(truncate:DI\n+\t (lshiftrt:TI\n+\t  (mult:TI\n+\t   (ANY_EXTEND:TI (match_operand:DI 1 \"register_operand\" \"r\"))\n+\t   (ANY_EXTEND:TI (match_operand:DI 2 \"register_operand\" \"r\")))\n+\t  (const_int 64))))]\n+  \"\"\n+  \"<su>mulh\\\\t%0, %1, %2\"\n+  [(set_attr \"v8type\" \"mulh\")\n+   (set_attr \"mode\" \"DI\")]\n+)\n+\n+(define_insn \"<su_optab>div<mode>3\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(ANY_DIV:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t     (match_operand:GPI 2 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"<su>div\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"<su>div\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; -------------------------------------------------------------------\n+;; Comparison insns\n+;; -------------------------------------------------------------------\n+\n+(define_insn \"*cmp<mode>\"\n+  [(set (reg:CC CC_REGNUM)\n+\t(compare:CC (match_operand:GPI 0 \"register_operand\" \"r,r\")\n+\t\t    (match_operand:GPI 1 \"aarch64_plus_operand\" \"rI,J\")))]\n+  \"\"\n+  \"@\n+   cmp\\\\t%<w>0, %<w>1\n+   cmn\\\\t%<w>0, #%n1\"\n+  [(set_attr \"v8type\" \"alus\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*cmp<mode>\"\n+  [(set (reg:CCFP CC_REGNUM)\n+        (compare:CCFP (match_operand:GPF 0 \"register_operand\" \"w,w\")\n+\t\t      (match_operand:GPF 1 \"aarch64_fp_compare_operand\" \"Y,w\")))]\n+   \"TARGET_FLOAT\"\n+   \"@\n+    fcmp\\\\t%<s>0, #0.0\n+    fcmp\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"fcmp\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*cmpe<mode>\"\n+  [(set (reg:CCFPE CC_REGNUM)\n+        (compare:CCFPE (match_operand:GPF 0 \"register_operand\" \"w,w\")\n+\t\t       (match_operand:GPF 1 \"aarch64_fp_compare_operand\" \"Y,w\")))]\n+   \"TARGET_FLOAT\"\n+   \"@\n+    fcmpe\\\\t%<s>0, #0.0\n+    fcmpe\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"fcmp\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*cmp_swp_<shift>_reg<mode>\"\n+  [(set (reg:CC_SWP CC_REGNUM)\n+\t(compare:CC_SWP (ASHIFT:GPI\n+\t\t\t (match_operand:GPI 0 \"register_operand\" \"r\")\n+\t\t\t (match_operand:QI 1 \"aarch64_shift_imm_<mode>\" \"n\"))\n+\t\t\t(match_operand:GPI 2 \"aarch64_reg_or_zero\" \"rZ\")))]\n+  \"\"\n+  \"cmp\\\\t%<w>2, %<w>0, <shift> %1\"\n+  [(set_attr \"v8type\" \"alus_shift\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*cmp_swp_<optab><ALLX:mode>_reg<GPI:mode>\"\n+  [(set (reg:CC_SWP CC_REGNUM)\n+\t(compare:CC_SWP (ANY_EXTEND:GPI\n+\t\t\t (match_operand:ALLX 0 \"register_operand\" \"r\"))\n+\t\t\t(match_operand:GPI 1 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"cmp\\\\t%<GPI:w>1, %<GPI:w>0, <su>xt<ALLX:size>\"\n+  [(set_attr \"v8type\" \"alus_ext\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+\n+;; -------------------------------------------------------------------\n+;; Store-flag and conditional select insns\n+;; -------------------------------------------------------------------\n+\n+(define_expand \"cstore<mode>4\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"\")\n+\t(match_operator:SI 1 \"aarch64_comparison_operator\"\n+\t [(match_operand:GPI 2 \"register_operand\" \"\")\n+\t  (match_operand:GPI 3 \"aarch64_plus_operand\" \"\")]))]\n+  \"\"\n+  \"\n+  operands[2] = aarch64_gen_compare_reg (GET_CODE (operands[1]), operands[2],\n+\t\t\t\t      operands[3]);\n+  operands[3] = const0_rtx;\n+  \"\n+)\n+\n+(define_expand \"cstore<mode>4\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"\")\n+\t(match_operator:SI 1 \"aarch64_comparison_operator\"\n+\t [(match_operand:GPF 2 \"register_operand\" \"\")\n+\t  (match_operand:GPF 3 \"register_operand\" \"\")]))]\n+  \"\"\n+  \"\n+  operands[2] = aarch64_gen_compare_reg (GET_CODE (operands[1]), operands[2],\n+\t\t\t\t      operands[3]);\n+  operands[3] = const0_rtx;\n+  \"\n+)\n+\n+(define_insn \"*cstore<mode>_insn\"\n+  [(set (match_operand:ALLI 0 \"register_operand\" \"=r\")\n+\t(match_operator:ALLI 1 \"aarch64_comparison_operator\"\n+\t [(match_operand 2 \"cc_register\" \"\") (const_int 0)]))]\n+  \"\"\n+  \"cset\\\\t%<w>0, %m1\"\n+  [(set_attr \"v8type\" \"csel\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*cstore<mode>_neg\"\n+  [(set (match_operand:ALLI 0 \"register_operand\" \"=r\")\n+\t(neg:ALLI (match_operator:ALLI 1 \"aarch64_comparison_operator\"\n+\t\t  [(match_operand 2 \"cc_register\" \"\") (const_int 0)])))]\n+  \"\"\n+  \"csetm\\\\t%<w>0, %m1\"\n+  [(set_attr \"v8type\" \"csel\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_expand \"cmov<mode>6\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"\")\n+\t(if_then_else:GPI\n+\t (match_operator 1 \"aarch64_comparison_operator\"\n+\t  [(match_operand:GPI 2 \"register_operand\" \"\")\n+\t   (match_operand:GPI 3 \"aarch64_plus_operand\" \"\")])\n+\t (match_operand:GPI 4 \"register_operand\" \"\")\n+\t (match_operand:GPI 5 \"register_operand\" \"\")))]\n+  \"\"\n+  \"\n+  operands[2] = aarch64_gen_compare_reg (GET_CODE (operands[1]), operands[2],\n+\t\t\t\t      operands[3]);\n+  operands[3] = const0_rtx;\n+  \"\n+)\n+\n+(define_expand \"cmov<mode>6\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"\")\n+\t(if_then_else:GPF\n+\t (match_operator 1 \"aarch64_comparison_operator\"\n+\t  [(match_operand:GPF 2 \"register_operand\" \"\")\n+\t   (match_operand:GPF 3 \"register_operand\" \"\")])\n+\t (match_operand:GPF 4 \"register_operand\" \"\")\n+\t (match_operand:GPF 5 \"register_operand\" \"\")))]\n+  \"\"\n+  \"\n+  operands[2] = aarch64_gen_compare_reg (GET_CODE (operands[1]), operands[2],\n+\t\t\t\t      operands[3]);\n+  operands[3] = const0_rtx;\n+  \"\n+)\n+\n+(define_insn \"*cmov<mode>_insn\"\n+  [(set (match_operand:ALLI 0 \"register_operand\" \"=r,r,r,r\")\n+\t(if_then_else:ALLI\n+\t (match_operator 1 \"aarch64_comparison_operator\"\n+\t  [(match_operand 2 \"cc_register\" \"\") (const_int 0)])\n+\t (match_operand:ALLI 3 \"aarch64_reg_zero_or_m1\" \"rZ,rZ,UsM,UsM\")\n+\t (match_operand:ALLI 4 \"aarch64_reg_zero_or_m1\" \"rZ,UsM,rZ,UsM\")))]\n+  \"\"\n+  ;; Final alternative should be unreachable, but included for completeness\n+  \"@\n+   csel\\\\t%<w>0, %<w>3, %<w>4, %m1\n+   csinv\\\\t%<w>0, %<w>3, <w>zr, %m1\n+   csinv\\\\t%<w>0, %<w>4, <w>zr, %M1\n+   mov\\\\t%<w>0, -1\"\n+  [(set_attr \"v8type\" \"csel\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*cmov<mode>_insn\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+\t(if_then_else:GPF\n+\t (match_operator 1 \"aarch64_comparison_operator\"\n+\t  [(match_operand 2 \"cc_register\" \"\") (const_int 0)])\n+\t (match_operand:GPF 3 \"register_operand\" \"w\")\n+\t (match_operand:GPF 4 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fcsel\\\\t%<s>0, %<s>3, %<s>4, %m1\"\n+  [(set_attr \"v8type\" \"fcsel\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_expand \"mov<mode>cc\"\n+  [(set (match_operand:ALLI 0 \"register_operand\" \"\")\n+\t(if_then_else:ALLI (match_operand 1 \"aarch64_comparison_operator\" \"\")\n+\t\t\t   (match_operand:ALLI 2 \"register_operand\" \"\")\n+\t\t\t   (match_operand:ALLI 3 \"register_operand\" \"\")))]\n+  \"\"\n+  {\n+    rtx ccreg;\n+    enum rtx_code code = GET_CODE (operands[1]);\n+\n+    if (code == UNEQ || code == LTGT)\n+      FAIL;\n+\n+    ccreg = aarch64_gen_compare_reg (code, XEXP (operands[1], 0),\n+\t\t\t\t  XEXP (operands[1], 1));\n+    operands[1] = gen_rtx_fmt_ee (code, VOIDmode, ccreg, const0_rtx);\n+  }\n+)\n+\n+(define_expand \"mov<GPF:mode><GPI:mode>cc\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"\")\n+\t(if_then_else:GPI (match_operand 1 \"aarch64_comparison_operator\" \"\")\n+\t\t\t  (match_operand:GPF 2 \"register_operand\" \"\")\n+\t\t\t  (match_operand:GPF 3 \"register_operand\" \"\")))]\n+  \"\"\n+  {\n+    rtx ccreg;\n+    enum rtx_code code = GET_CODE (operands[1]);\n+\n+    if (code == UNEQ || code == LTGT)\n+      FAIL;\n+\n+    ccreg = aarch64_gen_compare_reg (code, XEXP (operands[1], 0),\n+\t\t\t\t  XEXP (operands[1], 1));\n+    operands[1] = gen_rtx_fmt_ee (code, VOIDmode, ccreg, const0_rtx);\n+  }\n+)\n+\n+(define_insn \"*csinc2<mode>_insn\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (plus:GPI (match_operator:GPI 2 \"aarch64_comparison_operator\"\n+\t\t  [(match_operand:CC 3 \"cc_register\" \"\") (const_int 0)])\n+\t\t (match_operand:GPI 1 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"csinc\\\\t%<w>0, %<w>1, %<w>1, %M2\"\n+  [(set_attr \"v8type\" \"csel\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"csinc3<mode>_insn\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (if_then_else:GPI\n+\t  (match_operator:GPI 1 \"aarch64_comparison_operator\"\n+\t   [(match_operand:CC 2 \"cc_register\" \"\") (const_int 0)])\n+\t  (plus:GPI (match_operand:GPI 3 \"register_operand\" \"r\")\n+\t\t    (const_int 1))\n+\t  (match_operand:GPI 4 \"aarch64_reg_or_zero\" \"rZ\")))]\n+  \"\"\n+  \"csinc\\\\t%<w>0, %<w>4, %<w>3, %M1\"\n+  [(set_attr \"v8type\" \"csel\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*csinv3<mode>_insn\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (if_then_else:GPI\n+\t  (match_operator:GPI 1 \"aarch64_comparison_operator\"\n+\t   [(match_operand:CC 2 \"cc_register\" \"\") (const_int 0)])\n+\t  (not:GPI (match_operand:GPI 3 \"register_operand\" \"r\"))\n+\t  (match_operand:GPI 4 \"aarch64_reg_or_zero\" \"rZ\")))]\n+  \"\"\n+  \"csinv\\\\t%<w>0, %<w>4, %<w>3, %M1\"\n+  [(set_attr \"v8type\" \"csel\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"*csneg3<mode>_insn\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (if_then_else:GPI\n+\t  (match_operator:GPI 1 \"aarch64_comparison_operator\"\n+\t   [(match_operand:CC 2 \"cc_register\" \"\") (const_int 0)])\n+\t  (neg:GPI (match_operand:GPI 3 \"register_operand\" \"r\"))\n+\t  (match_operand:GPI 4 \"aarch64_reg_or_zero\" \"rZ\")))]\n+  \"\"\n+  \"csneg\\\\t%<w>0, %<w>4, %<w>3, %M1\"\n+  [(set_attr \"v8type\" \"csel\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+;; -------------------------------------------------------------------\n+;; Logical operations\n+;; -------------------------------------------------------------------\n+\n+(define_insn \"<optab><mode>3\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r,rk\")\n+\t(LOGICAL:GPI (match_operand:GPI 1 \"register_operand\" \"%r,r\")\n+\t\t     (match_operand:GPI 2 \"aarch64_logical_operand\" \"r,<lconst>\")))]\n+  \"\"\n+  \"<logical>\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"logic,logic_imm\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"*<LOGICAL:optab>_<SHIFT:optab><mode>3\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(LOGICAL:GPI (SHIFT:GPI\n+\t\t      (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t      (match_operand:QI 2 \"aarch64_shift_imm_<mode>\" \"n\"))\n+\t\t     (match_operand:GPI 3 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"<LOGICAL:logical>\\\\t%<w>0, %<w>3, %<w>1, <SHIFT:shift> %2\"\n+  [(set_attr \"v8type\" \"logic_shift\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"one_cmpl<mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(not:GPI (match_operand:GPI 1 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"mvn\\\\t%<w>0, %<w>1\"\n+  [(set_attr \"v8type\" \"logic\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"*one_cmpl_<optab><mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(not:GPI (SHIFT:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t    (match_operand:QI 2 \"aarch64_shift_imm_<mode>\" \"n\"))))]\n+  \"\"\n+  \"mvn\\\\t%<w>0, %<w>1, <shift> %2\"\n+  [(set_attr \"v8type\" \"logic_shift\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"*<LOGICAL:optab>_one_cmpl<mode>3\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(LOGICAL:GPI (not:GPI\n+\t\t      (match_operand:GPI 1 \"register_operand\" \"r\"))\n+\t\t     (match_operand:GPI 2 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"<LOGICAL:nlogical>\\\\t%<w>0, %<w>2, %<w>1\"\n+  [(set_attr \"v8type\" \"logic\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"*<LOGICAL:optab>_one_cmpl_<SHIFT:optab><mode>3\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(LOGICAL:GPI (not:GPI\n+\t\t      (SHIFT:GPI\n+\t\t       (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t       (match_operand:QI 2 \"aarch64_shift_imm_<mode>\" \"n\")))\n+\t\t     (match_operand:GPI 3 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"<LOGICAL:nlogical>\\\\t%<w>0, %<w>3, %<w>1, <SHIFT:shift> %2\"\n+  [(set_attr \"v8type\" \"logic_shift\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"clz<mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(clz:GPI (match_operand:GPI 1 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"clz\\\\t%<w>0, %<w>1\"\n+  [(set_attr \"v8type\" \"clz\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_expand \"ffs<mode>2\"\n+  [(match_operand:GPI 0 \"register_operand\")\n+   (match_operand:GPI 1 \"register_operand\")]\n+  \"\"\n+  {\n+    rtx ccreg = aarch64_gen_compare_reg (EQ, operands[1], const0_rtx);\n+    rtx x = gen_rtx_NE (VOIDmode, ccreg, const0_rtx);\n+\n+    emit_insn (gen_rbit<mode>2 (operands[0], operands[1]));\n+    emit_insn (gen_clz<mode>2 (operands[0], operands[0]));\n+    emit_insn (gen_csinc3<mode>_insn (operands[0], x, ccreg, operands[0], const0_rtx));\n+    DONE;\n+  }\n+)\n+\n+(define_insn \"clrsb<mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(unspec:GPI [(match_operand:GPI 1 \"register_operand\" \"r\")] UNSPEC_CLS))]\n+  \"\"\n+  \"cls\\\\t%<w>0, %<w>1\"\n+  [(set_attr \"v8type\" \"clz\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"rbit<mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(unspec:GPI [(match_operand:GPI 1 \"register_operand\" \"r\")] UNSPEC_RBIT))]\n+  \"\"\n+  \"rbit\\\\t%<w>0, %<w>1\"\n+  [(set_attr \"v8type\" \"rbit\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_expand \"ctz<mode>2\"\n+  [(match_operand:GPI 0 \"register_operand\")\n+   (match_operand:GPI 1 \"register_operand\")]\n+  \"\"\n+  {\n+    emit_insn (gen_rbit<mode>2 (operands[0], operands[1]));\n+    emit_insn (gen_clz<mode>2 (operands[0], operands[0]));\n+    DONE;\n+  }\n+)\n+\n+(define_insn \"*and<mode>3nr_compare0\"\n+  [(set (reg:CC CC_REGNUM)\n+\t(compare:CC\n+\t (and:GPI (match_operand:GPI 0 \"register_operand\" \"%r,r\")\n+\t\t  (match_operand:GPI 1 \"aarch64_logical_operand\" \"r,<lconst>\"))\n+\t (const_int 0)))]\n+  \"\"\n+  \"tst\\\\t%<w>0, %<w>1\"\n+  [(set_attr \"v8type\" \"logics\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"*and_<SHIFT:optab><mode>3nr_compare0\"\n+  [(set (reg:CC CC_REGNUM)\n+\t(compare:CC\n+\t (and:GPI (SHIFT:GPI\n+\t\t   (match_operand:GPI 0 \"register_operand\" \"r\")\n+\t\t   (match_operand:QI 1 \"aarch64_shift_imm_<mode>\" \"n\"))\n+\t\t  (match_operand:GPI 2 \"register_operand\" \"r\"))\n+\t(const_int 0)))]\n+  \"\"\n+  \"tst\\\\t%<w>2, %<w>0, <SHIFT:shift> %1\"\n+  [(set_attr \"v8type\" \"logics_shift\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+;; -------------------------------------------------------------------\n+;; Shifts\n+;; -------------------------------------------------------------------\n+\n+(define_expand \"<optab><mode>3\"\n+  [(set (match_operand:GPI 0 \"register_operand\")\n+\t(ASHIFT:GPI (match_operand:GPI 1 \"register_operand\")\n+\t\t    (match_operand:QI 2 \"nonmemory_operand\")))]\n+  \"\"\n+  {\n+    if (CONST_INT_P (operands[2]))\n+      {\n+        operands[2] = GEN_INT (INTVAL (operands[2])\n+                               & (GET_MODE_BITSIZE (<MODE>mode) - 1));\n+\n+        if (operands[2] == const0_rtx)\n+          {\n+\t    emit_insn (gen_mov<mode> (operands[0], operands[1]));\n+\t    DONE;\n+          }\n+      }\n+  }\n+)\n+\n+(define_expand \"ashl<mode>3\"\n+  [(set (match_operand:SHORT 0 \"register_operand\")\n+\t(ashift:SHORT (match_operand:SHORT 1 \"register_operand\")\n+\t\t      (match_operand:QI 2 \"nonmemory_operand\")))]\n+  \"\"\n+  {\n+    if (CONST_INT_P (operands[2]))\n+      {\n+        operands[2] = GEN_INT (INTVAL (operands[2])\n+                               & (GET_MODE_BITSIZE (<MODE>mode) - 1));\n+\n+        if (operands[2] == const0_rtx)\n+          {\n+\t    emit_insn (gen_mov<mode> (operands[0], operands[1]));\n+\t    DONE;\n+          }\n+      }\n+  }\n+)\n+\n+(define_expand \"rotr<mode>3\"\n+  [(set (match_operand:GPI 0 \"register_operand\")\n+\t(rotatert:GPI (match_operand:GPI 1 \"register_operand\")\n+\t\t      (match_operand:QI 2 \"nonmemory_operand\")))]\n+  \"\"\n+  {\n+    if (CONST_INT_P (operands[2]))\n+      {\n+        operands[2] = GEN_INT (INTVAL (operands[2])\n+                               & (GET_MODE_BITSIZE (<MODE>mode) - 1));\n+\n+        if (operands[2] == const0_rtx)\n+          {\n+\t    emit_insn (gen_mov<mode> (operands[0], operands[1]));\n+\t    DONE;\n+          }\n+      }\n+  }\n+)\n+\n+(define_expand \"rotl<mode>3\"\n+  [(set (match_operand:GPI 0 \"register_operand\")\n+\t(rotatert:GPI (match_operand:GPI 1 \"register_operand\")\n+\t\t      (match_operand:QI 2 \"nonmemory_operand\")))]\n+  \"\"\n+  {\n+    /* (SZ - cnt) % SZ == -cnt % SZ */\n+    if (CONST_INT_P (operands[2]))\n+      {\n+        operands[2] = GEN_INT ((-INTVAL (operands[2]))\n+\t\t\t       & (GET_MODE_BITSIZE (<MODE>mode) - 1));\n+        if (operands[2] == const0_rtx)\n+          {\n+\t    emit_insn (gen_mov<mode> (operands[0], operands[1]));\n+\t    DONE;\n+          }\n+      }\n+    else\n+      operands[2] = expand_simple_unop (QImode, NEG, operands[2],\n+\t\t\t\t\tNULL_RTX, 1);\n+  }\n+)\n+\n+(define_insn \"*<optab><mode>3_insn\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(SHIFT:GPI\n+\t (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t (match_operand:QI 2 \"aarch64_reg_or_shift_imm_<mode>\" \"rUs<cmode>\")))]\n+  \"\"\n+  \"<shift>\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"shift\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*ashl<mode>3_insn\"\n+  [(set (match_operand:SHORT 0 \"register_operand\" \"=r\")\n+\t(ashift:SHORT (match_operand:SHORT 1 \"register_operand\" \"r\")\n+\t\t      (match_operand:QI 2 \"aarch64_reg_or_shift_imm_si\" \"rUss\")))]\n+  \"\"\n+  \"lsl\\\\t%<w>0, %<w>1, %<w>2\"\n+  [(set_attr \"v8type\" \"shift\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*<optab><mode>3_insn\"\n+  [(set (match_operand:SHORT 0 \"register_operand\" \"=r\")\n+\t(ASHIFT:SHORT (match_operand:SHORT 1 \"register_operand\" \"r\")\n+\t\t      (match_operand 2 \"const_int_operand\" \"n\")))]\n+  \"UINTVAL (operands[2]) < GET_MODE_BITSIZE (<MODE>mode)\"\n+{\n+  operands[3] = GEN_INT (<sizen> - UINTVAL (operands[2]));\n+  return \"<bfshift>\\t%w0, %w1, %2, %3\";\n+}\n+  [(set_attr \"v8type\" \"bfm\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*<ANY_EXTEND:optab><GPI:mode>_ashl<SHORT:mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(ANY_EXTEND:GPI\n+\t (ashift:SHORT (match_operand:SHORT 1 \"register_operand\" \"r\")\n+\t\t       (match_operand 2 \"const_int_operand\" \"n\"))))]\n+  \"UINTVAL (operands[2]) < GET_MODE_BITSIZE (<SHORT:MODE>mode)\"\n+{\n+  operands[3] = GEN_INT (<SHORT:sizen> - UINTVAL (operands[2]));\n+  return \"<su>bfiz\\t%<GPI:w>0, %<GPI:w>1, %2, %3\";\n+}\n+  [(set_attr \"v8type\" \"bfm\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"*zero_extend<GPI:mode>_lshr<SHORT:mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(zero_extend:GPI\n+\t (lshiftrt:SHORT (match_operand:SHORT 1 \"register_operand\" \"r\")\n+\t\t\t (match_operand 2 \"const_int_operand\" \"n\"))))]\n+  \"UINTVAL (operands[2]) < GET_MODE_BITSIZE (<SHORT:MODE>mode)\"\n+{\n+  operands[3] = GEN_INT (<SHORT:sizen> - UINTVAL (operands[2]));\n+  return \"ubfx\\t%<GPI:w>0, %<GPI:w>1, %2, %3\";\n+}\n+  [(set_attr \"v8type\" \"bfm\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"*extend<GPI:mode>_ashr<SHORT:mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(sign_extend:GPI\n+\t (ashiftrt:SHORT (match_operand:SHORT 1 \"register_operand\" \"r\")\n+\t\t\t (match_operand 2 \"const_int_operand\" \"n\"))))]\n+  \"UINTVAL (operands[2]) < GET_MODE_BITSIZE (<SHORT:MODE>mode)\"\n+{\n+  operands[3] = GEN_INT (<SHORT:sizen> - UINTVAL (operands[2]));\n+  return \"sbfx\\\\t%<GPI:w>0, %<GPI:w>1, %2, %3\";\n+}\n+  [(set_attr \"v8type\" \"bfm\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+;; -------------------------------------------------------------------\n+;; Bitfields\n+;; -------------------------------------------------------------------\n+\n+(define_expand \"<optab>\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(ANY_EXTRACT:DI (match_operand:DI 1 \"register_operand\" \"r\")\n+\t\t\t(match_operand 2 \"const_int_operand\" \"n\")\n+\t\t\t(match_operand 3 \"const_int_operand\" \"n\")))]\n+  \"\"\n+  \"\"\n+)\n+\n+(define_insn \"*<optab><mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(ANY_EXTRACT:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t (match_operand 2 \"const_int_operand\" \"n\")\n+\t\t\t (match_operand 3 \"const_int_operand\" \"n\")))]\n+  \"\"\n+  \"<su>bfx\\\\t%<w>0, %<w>1, %3, %2\"\n+  [(set_attr \"v8type\" \"bfm\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*<optab><ALLX:mode>_shft_<GPI:mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(ashift:GPI (ANY_EXTEND:GPI\n+\t\t     (match_operand:ALLX 1 \"register_operand\" \"r\"))\n+\t\t    (match_operand 2 \"const_int_operand\" \"n\")))]\n+  \"UINTVAL (operands[2]) < <GPI:sizen>\"\n+{\n+  operands[3] = (<ALLX:sizen> <= (<GPI:sizen> - UINTVAL (operands[2])))\n+\t      ? GEN_INT (<ALLX:sizen>)\n+\t      : GEN_INT (<GPI:sizen> - UINTVAL (operands[2]));\n+  return \"<su>bfiz\\t%<GPI:w>0, %<GPI:w>1, %2, %3\";\n+}\n+  [(set_attr \"v8type\" \"bfm\")\n+   (set_attr \"mode\" \"<GPI:MODE>\")]\n+)\n+\n+;; XXX We should match (any_extend (ashift)) here, like (and (ashift)) below\n+\n+(define_insn \"*andim_ashift<mode>_bfiz\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+\t(and:GPI (ashift:GPI (match_operand:GPI 1 \"register_operand\" \"r\")\n+\t\t\t     (match_operand 2 \"const_int_operand\" \"n\"))\n+\t\t (match_operand 3 \"const_int_operand\" \"n\")))]\n+  \"exact_log2 ((INTVAL (operands[3]) >> INTVAL (operands[2])) + 1) >= 0\n+   && (INTVAL (operands[3]) & ((1 << INTVAL (operands[2])) - 1)) == 0\"\n+  \"ubfiz\\\\t%<w>0, %<w>1, %2, %P3\"\n+  [(set_attr \"v8type\" \"bfm\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"bswap<mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (bswap:GPI (match_operand:GPI 1 \"register_operand\" \"r\")))]\n+  \"\"\n+  \"rev\\\\t%<w>0, %<w>1\"\n+  [(set_attr \"v8type\" \"rev\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; -------------------------------------------------------------------\n+;; Floating-point intrinsics\n+;; -------------------------------------------------------------------\n+\n+;; trunc - nothrow\n+\n+(define_insn \"btrunc<mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (unspec:GPF [(match_operand:GPF 1 \"register_operand\" \"w\")]\n+\t UNSPEC_FRINTZ))]\n+  \"TARGET_FLOAT\"\n+  \"frintz\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"frint\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*lbtrunc<su_optab><GPF:mode><GPI:mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (FIXUORS:GPI (unspec:GPF [(match_operand:GPF 1 \"register_operand\" \"w\")]\n+\t\t      UNSPEC_FRINTZ)))]\n+  \"TARGET_FLOAT\"\n+  \"fcvtz<su>\\\\t%<GPI:w>0, %<GPF:s>1\"\n+  [(set_attr \"v8type\" \"fcvtf2i\")\n+   (set_attr \"mode\" \"<GPF:MODE>\")\n+   (set_attr \"mode2\" \"<GPI:MODE>\")]\n+)\n+\n+;; ceil - nothrow\n+\n+(define_insn \"ceil<mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (unspec:GPF [(match_operand:GPF 1 \"register_operand\" \"w\")]\n+\t UNSPEC_FRINTP))]\n+  \"TARGET_FLOAT\"\n+  \"frintp\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"frint\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"lceil<su_optab><GPF:mode><GPI:mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (FIXUORS:GPI (unspec:GPF [(match_operand:GPF 1 \"register_operand\" \"w\")]\n+\t\t      UNSPEC_FRINTP)))]\n+  \"TARGET_FLOAT\"\n+  \"fcvtp<su>\\\\t%<GPI:w>0, %<GPF:s>1\"\n+  [(set_attr \"v8type\" \"fcvtf2i\")\n+   (set_attr \"mode\" \"<GPF:MODE>\")\n+   (set_attr \"mode2\" \"<GPI:MODE>\")]\n+)\n+\n+;; floor - nothrow\n+\n+(define_insn \"floor<mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (unspec:GPF [(match_operand:GPF 1 \"register_operand\" \"w\")]\n+\t UNSPEC_FRINTM))]\n+  \"TARGET_FLOAT\"\n+  \"frintm\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"frint\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"lfloor<su_optab><GPF:mode><GPI:mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (FIXUORS:GPI (unspec:GPF [(match_operand:GPF 1 \"register_operand\" \"w\")]\n+\t\t      UNSPEC_FRINTM)))]\n+  \"TARGET_FLOAT\"\n+  \"fcvtm<su>\\\\t%<GPI:w>0, %<GPF:s>1\"\n+  [(set_attr \"v8type\" \"fcvtf2i\")\n+   (set_attr \"mode\" \"<GPF:MODE>\")\n+   (set_attr \"mode2\" \"<GPI:MODE>\")]\n+)\n+\n+;; nearbyint - nothrow\n+\n+(define_insn \"nearbyint<mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (unspec:GPF [(match_operand:GPF 1 \"register_operand\" \"w\")]\n+\t UNSPEC_FRINTI))]\n+  \"TARGET_FLOAT\"\n+  \"frinti\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"frint\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; rint\n+\n+(define_insn \"rint<mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (unspec:GPF [(match_operand:GPF 1 \"register_operand\" \"w\")]\n+\t UNSPEC_FRINTX))]\n+  \"TARGET_FLOAT\"\n+  \"frintx\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"frint\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; round - nothrow\n+\n+(define_insn \"round<mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (unspec:GPF [(match_operand:GPF 1 \"register_operand\" \"w\")]\n+\t UNSPEC_FRINTA))]\n+  \"TARGET_FLOAT\"\n+  \"frinta\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"frint\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"lround<su_optab><GPF:mode><GPI:mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (FIXUORS:GPI (unspec:GPF [(match_operand:GPF 1 \"register_operand\" \"w\")]\n+\t\t      UNSPEC_FRINTA)))]\n+  \"TARGET_FLOAT\"\n+  \"fcvta<su>\\\\t%<GPI:w>0, %<GPF:s>1\"\n+  [(set_attr \"v8type\" \"fcvtf2i\")\n+   (set_attr \"mode\" \"<GPF:MODE>\")\n+   (set_attr \"mode2\" \"<GPI:MODE>\")]\n+)\n+\n+;; fma - no throw\n+\n+(define_insn \"fma<mode>4\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (fma:GPF (match_operand:GPF 1 \"register_operand\" \"w\")\n+\t\t (match_operand:GPF 2 \"register_operand\" \"w\")\n+\t\t (match_operand:GPF 3 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fmadd\\\\t%<s>0, %<s>1, %<s>2, %<s>3\"\n+  [(set_attr \"v8type\" \"fmadd\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"fnma<mode>4\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+\t(fma:GPF (neg:GPF (match_operand:GPF 1 \"register_operand\" \"w\"))\n+\t\t (match_operand:GPF 2 \"register_operand\" \"w\")\n+\t\t (match_operand:GPF 3 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fmsub\\\\t%<s>0, %<s>1, %<s>2, %<s>3\"\n+  [(set_attr \"v8type\" \"fmadd\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"fms<mode>4\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (fma:GPF (match_operand:GPF 1 \"register_operand\" \"w\")\n+\t\t (match_operand:GPF 2 \"register_operand\" \"w\")\n+\t\t (neg:GPF (match_operand:GPF 3 \"register_operand\" \"w\"))))]\n+  \"TARGET_FLOAT\"\n+  \"fnmsub\\\\t%<s>0, %<s>1, %<s>2, %<s>3\"\n+  [(set_attr \"v8type\" \"fmadd\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"fnms<mode>4\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+\t(fma:GPF (neg:GPF (match_operand:GPF 1 \"register_operand\" \"w\"))\n+\t\t (match_operand:GPF 2 \"register_operand\" \"w\")\n+\t\t (neg:GPF (match_operand:GPF 3 \"register_operand\" \"w\"))))]\n+  \"TARGET_FLOAT\"\n+  \"fnmadd\\\\t%<s>0, %<s>1, %<s>2, %<s>3\"\n+  [(set_attr \"v8type\" \"fmadd\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; If signed zeros are ignored, -(a * b + c) = -a * b - c.\n+(define_insn \"*fnmadd<mode>4\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+\t(neg:GPF (fma:GPF (match_operand:GPF 1 \"register_operand\" \"w\")\n+\t\t\t  (match_operand:GPF 2 \"register_operand\" \"w\")\n+\t\t\t  (match_operand:GPF 3 \"register_operand\" \"w\"))))]\n+  \"!HONOR_SIGNED_ZEROS (<MODE>mode) && TARGET_FLOAT\"\n+  \"fnmadd\\\\t%<s>0, %<s>1, %<s>2, %<s>3\"\n+  [(set_attr \"v8type\" \"fmadd\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; -------------------------------------------------------------------\n+;; Floating-point conversions\n+;; -------------------------------------------------------------------\n+\n+(define_insn \"extendsfdf2\"\n+  [(set (match_operand:DF 0 \"register_operand\" \"=w\")\n+        (float_extend:DF (match_operand:SF 1 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fcvt\\\\t%d0, %s1\"\n+  [(set_attr \"v8type\" \"fcvt\")\n+   (set_attr \"mode\" \"DF\")\n+   (set_attr \"mode2\" \"SF\")]\n+)\n+\n+(define_insn \"truncdfsf2\"\n+  [(set (match_operand:SF 0 \"register_operand\" \"=w\")\n+        (float_truncate:SF (match_operand:DF 1 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fcvt\\\\t%s0, %d1\"\n+  [(set_attr \"v8type\" \"fcvt\")\n+   (set_attr \"mode\" \"SF\")\n+   (set_attr \"mode2\" \"DF\")]\n+)\n+\n+(define_insn \"fix_trunc<GPF:mode><GPI:mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (fix:GPI (match_operand:GPF 1 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fcvtzs\\\\t%<GPI:w>0, %<GPF:s>1\"\n+  [(set_attr \"v8type\" \"fcvtf2i\")\n+   (set_attr \"mode\" \"<GPF:MODE>\")\n+   (set_attr \"mode2\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"fixuns_trunc<GPF:mode><GPI:mode>2\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (unsigned_fix:GPI (match_operand:GPF 1 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fcvtzu\\\\t%<GPI:w>0, %<GPF:s>1\"\n+  [(set_attr \"v8type\" \"fcvtf2i\")\n+   (set_attr \"mode\" \"<GPF:MODE>\")\n+   (set_attr \"mode2\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"float<GPI:mode><GPF:mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (float:GPF (match_operand:GPI 1 \"register_operand\" \"r\")))]\n+  \"TARGET_FLOAT\"\n+  \"scvtf\\\\t%<GPF:s>0, %<GPI:w>1\"\n+  [(set_attr \"v8type\" \"fcvti2f\")\n+   (set_attr \"mode\" \"<GPF:MODE>\")\n+   (set_attr \"mode2\" \"<GPI:MODE>\")]\n+)\n+\n+(define_insn \"floatuns<GPI:mode><GPF:mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (unsigned_float:GPF (match_operand:GPI 1 \"register_operand\" \"r\")))]\n+  \"TARGET_FLOAT\"\n+  \"ucvtf\\\\t%<GPF:s>0, %<GPI:w>1\"\n+  [(set_attr \"v8type\" \"fcvt\")\n+   (set_attr \"mode\" \"<GPF:MODE>\")\n+   (set_attr \"mode2\" \"<GPI:MODE>\")]\n+)\n+\n+;; -------------------------------------------------------------------\n+;; Floating-point arithmetic\n+;; -------------------------------------------------------------------\n+\n+(define_insn \"add<mode>3\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (plus:GPF\n+         (match_operand:GPF 1 \"register_operand\" \"w\")\n+         (match_operand:GPF 2 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fadd\\\\t%<s>0, %<s>1, %<s>2\"\n+  [(set_attr \"v8type\" \"fadd\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"sub<mode>3\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (minus:GPF\n+         (match_operand:GPF 1 \"register_operand\" \"w\")\n+         (match_operand:GPF 2 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fsub\\\\t%<s>0, %<s>1, %<s>2\"\n+  [(set_attr \"v8type\" \"fadd\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"mul<mode>3\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (mult:GPF\n+         (match_operand:GPF 1 \"register_operand\" \"w\")\n+         (match_operand:GPF 2 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fmul\\\\t%<s>0, %<s>1, %<s>2\"\n+  [(set_attr \"v8type\" \"fmul\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*fnmul<mode>3\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (mult:GPF\n+\t\t (neg:GPF (match_operand:GPF 1 \"register_operand\" \"w\"))\n+\t\t (match_operand:GPF 2 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fnmul\\\\t%<s>0, %<s>1, %<s>2\"\n+  [(set_attr \"v8type\" \"fmul\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"div<mode>3\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (div:GPF\n+         (match_operand:GPF 1 \"register_operand\" \"w\")\n+         (match_operand:GPF 2 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fdiv\\\\t%<s>0, %<s>1, %<s>2\"\n+  [(set_attr \"v8type\" \"fdiv\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"neg<mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (neg:GPF (match_operand:GPF 1 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fneg\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"ffarith\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"sqrt<mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (sqrt:GPF (match_operand:GPF 1 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fsqrt\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"fsqrt\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"abs<mode>2\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (abs:GPF (match_operand:GPF 1 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fabs\\\\t%<s>0, %<s>1\"\n+  [(set_attr \"v8type\" \"ffarith\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; Given that smax/smin do not specify the result when either input is NaN,\n+;; we could use either FMAXNM or FMAX for smax, and either FMINNM or FMIN\n+;; for smin.\n+\n+(define_insn \"smax<mode>3\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (smax:GPF (match_operand:GPF 1 \"register_operand\" \"w\")\n+\t\t  (match_operand:GPF 2 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fmaxnm\\\\t%<s>0, %<s>1, %<s>2\"\n+  [(set_attr \"v8type\" \"fminmax\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"smin<mode>3\"\n+  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n+        (smin:GPF (match_operand:GPF 1 \"register_operand\" \"w\")\n+\t\t  (match_operand:GPF 2 \"register_operand\" \"w\")))]\n+  \"TARGET_FLOAT\"\n+  \"fminnm\\\\t%<s>0, %<s>1, %<s>2\"\n+  [(set_attr \"v8type\" \"fminmax\")\n+   (set_attr \"mode\" \"<MODE>\")]\n+)\n+\n+;; -------------------------------------------------------------------\n+;; Reload support\n+;; -------------------------------------------------------------------\n+\n+;; Reload SP+imm where imm cannot be handled by a single ADD instruction.  \n+;; Must load imm into a scratch register and copy SP to the dest reg before\n+;; adding, since SP cannot be used as a source register in an ADD\n+;; instruction.\n+(define_expand \"reload_sp_immediate\"\n+  [(parallel [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t\t   (match_operand:DI 1 \"\" \"\"))\n+\t     (clobber (match_operand:TI 2 \"register_operand\" \"=&r\"))])]\n+  \"\"\n+  {\n+    rtx sp = XEXP (operands[1], 0);\n+    rtx val = XEXP (operands[1], 1);\n+    unsigned regno = REGNO (operands[2]);\n+    rtx scratch = operands[1];\n+    gcc_assert (GET_CODE (operands[1]) == PLUS);\n+    gcc_assert (sp == stack_pointer_rtx);\n+    gcc_assert (CONST_INT_P (val));\n+\n+    /* It is possible that one of the registers we got for operands[2]\n+       might coincide with that of operands[0] (which is why we made\n+       it TImode).  Pick the other one to use as our scratch.  */\n+    if (regno == REGNO (operands[0]))\n+      regno++;\n+    scratch = gen_rtx_REG (DImode, regno);\n+\n+    emit_move_insn (scratch, val);\n+    emit_move_insn (operands[0], sp);\n+    emit_insn (gen_adddi3 (operands[0], operands[0], scratch));\n+    DONE;\n+  }\n+)\n+\n+(define_expand \"aarch64_reload_mov<mode>\"\n+  [(set (match_operand:TX 0 \"register_operand\" \"=w\")\n+        (match_operand:TX 1 \"register_operand\" \"w\"))\n+   (clobber (match_operand:DI 2 \"register_operand\" \"=&r\"))\n+  ]\n+  \"\"\n+  {\n+    rtx op0 = simplify_gen_subreg (TImode, operands[0], <MODE>mode, 0);\n+    rtx op1 = simplify_gen_subreg (TImode, operands[1], <MODE>mode, 0);\n+    gen_aarch64_movtilow_tilow (op0, op1);\n+    gen_aarch64_movdi_tihigh (operands[2], op1);\n+    gen_aarch64_movtihigh_di (op0, operands[2]);\n+    DONE;\n+  }\n+)\n+\n+;; The following secondary reload helpers patterns are invoked\n+;; after or during reload as we don't want these patterns to start\n+;; kicking in during the combiner.\n+ \n+(define_insn \"aarch64_movdi_tilow\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+        (truncate:DI (match_operand:TI 1 \"register_operand\" \"w\")))]\n+  \"reload_completed || reload_in_progress\"\n+  \"fmov\\\\t%x0, %d1\"\n+  [(set_attr \"v8type\" \"fmovf2i\")\n+   (set_attr \"mode\"   \"DI\")\n+   (set_attr \"length\" \"4\")\n+  ])\n+\n+(define_insn \"aarch64_movdi_tihigh\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+        (truncate:DI\n+\t  (lshiftrt:TI (match_operand:TI 1 \"register_operand\" \"w\")\n+\t\t       (const_int 64))))]\n+  \"reload_completed || reload_in_progress\"\n+  \"fmov\\\\t%x0, %1.d[1]\"\n+  [(set_attr \"v8type\" \"fmovf2i\")\n+   (set_attr \"mode\"   \"DI\")\n+   (set_attr \"length\" \"4\")\n+  ])\n+\n+(define_insn \"aarch64_movtihigh_di\"\n+  [(set (zero_extract:TI (match_operand:TI 0 \"register_operand\" \"+w\")\n+                         (const_int 64) (const_int 64))\n+        (zero_extend:TI (match_operand:DI 1 \"register_operand\" \"r\")))]\n+  \"reload_completed || reload_in_progress\"\n+  \"fmov\\\\t%0.d[1], %x1\"\n+\n+  [(set_attr \"v8type\" \"fmovi2f\")\n+   (set_attr \"mode\"   \"DI\")\n+   (set_attr \"length\" \"4\")\n+  ])\n+\n+(define_insn \"aarch64_movtilow_di\"\n+  [(set (match_operand:TI 0 \"register_operand\" \"=w\")\n+        (zero_extend:TI (match_operand:DI 1 \"register_operand\" \"r\")))]\n+  \"reload_completed || reload_in_progress\"\n+  \"fmov\\\\t%d0, %x1\"\n+\n+  [(set_attr \"v8type\" \"fmovi2f\")\n+   (set_attr \"mode\"   \"DI\")\n+   (set_attr \"length\" \"4\")\n+  ])\n+\n+(define_insn \"aarch64_movtilow_tilow\"\n+  [(set (match_operand:TI 0 \"register_operand\" \"=w\")\n+        (zero_extend:TI \n+\t  (truncate:DI (match_operand:TI 1 \"register_operand\" \"w\"))))]\n+  \"reload_completed || reload_in_progress\"\n+  \"fmov\\\\t%d0, %d1\"\n+\n+  [(set_attr \"v8type\" \"fmovi2f\")\n+   (set_attr \"mode\"   \"DI\")\n+   (set_attr \"length\" \"4\")\n+  ])\n+\n+;; There is a deliberate reason why the parameters of high and lo_sum's\n+;; don't have modes for ADRP and ADD instructions.  This is to allow high\n+;; and lo_sum's to be used with the labels defining the jump tables in\n+;; rodata section.\n+\n+(define_insn \"add_losym\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(lo_sum:DI (match_operand:DI 1 \"register_operand\" \"r\")\n+\t\t   (match_operand 2 \"aarch64_valid_symref\" \"S\")))]\n+  \"\"\n+  \"add\\\\t%0, %1, :lo12:%a2\"\n+  [(set_attr \"v8type\" \"alu\")\n+   (set_attr \"mode\" \"DI\")]\n+\n+)\n+\n+(define_insn \"ldr_got_small\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(unspec:DI [(mem:DI (lo_sum:DI\n+\t\t\t      (match_operand:DI 1 \"register_operand\" \"r\")\n+\t\t\t      (match_operand:DI 2 \"aarch64_valid_symref\" \"S\")))]\n+\t\t   UNSPEC_GOTSMALLPIC))]\n+  \"\"\n+  \"ldr\\\\t%0, [%1, #:got_lo12:%a2]\"\n+  [(set_attr \"v8type\" \"load1\")\n+   (set_attr \"mode\" \"DI\")]\n+)\n+\n+(define_insn \"aarch64_load_tp_hard\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(unspec:DI [(const_int 0)] UNSPEC_TLS))]\n+  \"\"\n+  \"mrs\\\\t%0, tpidr_el0\"\n+  [(set_attr \"v8type\" \"mrs\")\n+   (set_attr \"mode\" \"DI\")]\n+)\n+\n+;; The TLS ABI specifically requires that the compiler does not schedule\n+;; instructions in the TLS stubs, in order to enable linker relaxation.\n+;; Therefore we treat the stubs as an atomic sequence.\n+(define_expand \"tlsgd_small\"\n+ [(parallel [(set (match_operand 0 \"register_operand\" \"\")\n+                  (call (mem:DI (match_dup 2)) (const_int 1)))\n+\t     (unspec:DI [(match_operand:DI 1 \"aarch64_valid_symref\" \"\")] UNSPEC_GOTSMALLTLS)\n+\t     (clobber (reg:DI LR_REGNUM))])]\n+ \"\"\n+{\n+  operands[2] = aarch64_tls_get_addr ();\n+})\n+\n+(define_insn \"*tlsgd_small\"\n+  [(set (match_operand 0 \"register_operand\" \"\")\n+\t(call (mem:DI (match_operand:DI 2 \"\" \"\")) (const_int 1)))\n+   (unspec:DI [(match_operand:DI 1 \"aarch64_valid_symref\" \"S\")] UNSPEC_GOTSMALLTLS)\n+   (clobber (reg:DI LR_REGNUM))\n+  ]\n+  \"\"\n+  \"adrp\\\\tx0, %A1\\;add\\\\tx0, x0, %L1\\;bl\\\\t%2\\;nop\"\n+  [(set_attr \"v8type\" \"call\")\n+   (set_attr \"length\" \"16\")])\n+\n+(define_insn \"tlsie_small\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+        (unspec:DI [(match_operand:DI 1 \"aarch64_tls_ie_symref\" \"S\")]\n+\t\t   UNSPEC_GOTSMALLTLS))]\n+  \"\"\n+  \"adrp\\\\t%0, %A1\\;ldr\\\\t%0, [%0, #%L1]\"\n+  [(set_attr \"v8type\" \"load1\")\n+   (set_attr \"mode\" \"DI\")\n+   (set_attr \"length\" \"8\")]\n+)\n+\n+(define_insn \"tlsle_small\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+        (unspec:DI [(match_operand:DI 1 \"register_operand\" \"r\")\n+                   (match_operand:DI 2 \"aarch64_tls_le_symref\" \"S\")]\n+\t\t   UNSPEC_GOTSMALLTLS))]\n+  \"\"\n+  \"add\\\\t%0, %1, #%G2\\;add\\\\t%0, %0, #%L2\"\n+  [(set_attr \"v8type\" \"alu\")\n+   (set_attr \"mode\" \"DI\")\n+   (set_attr \"length\" \"8\")]\n+)\n+\n+(define_insn \"tlsdesc_small\"\n+  [(set (reg:DI R0_REGNUM)\n+        (unspec:DI [(match_operand:DI 0 \"aarch64_valid_symref\" \"S\")]\n+\t\t   UNSPEC_TLSDESC))\n+   (clobber (reg:DI LR_REGNUM))\n+   (clobber (match_scratch:DI 1 \"=r\"))]\n+  \"TARGET_TLS_DESC\"\n+  \"adrp\\\\tx0, %A0\\;ldr\\\\t%1, [x0, #%L0]\\;add\\\\tx0, x0, %L0\\;.tlsdesccall\\\\t%0\\;blr\\\\t%1\"\n+  [(set_attr \"v8type\" \"call\")\n+   (set_attr \"length\" \"16\")])\n+\n+(define_insn \"stack_tie\"\n+  [(set (mem:BLK (scratch))\n+\t(unspec:BLK [(match_operand:DI 0 \"register_operand\" \"rk\")\n+\t\t     (match_operand:DI 1 \"register_operand\" \"rk\")]\n+\t\t    UNSPEC_PRLG_STK))]\n+  \"\"\n+  \"\"\n+  [(set_attr \"length\" \"0\")]\n+)\n+\n+;; AdvSIMD Stuff\n+(include \"aarch64-simd.md\")\n+\n+;; Synchronization Builtins\n+(include \"sync.md\")"}, {"sha": "49d01254914aa5cd1f8759a01e24cb2629b603ca", "filename": "gcc/config/aarch64/aarch64.opt", "status": "added", "additions": 100, "deletions": 0, "changes": 100, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Faarch64.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.opt?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,100 @@\n+; Machine description for AArch64 architecture.\n+; Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+; Contributed by ARM Ltd.\n+;\n+; This file is part of GCC.\n+;\n+; GCC is free software; you can redistribute it and/or modify it\n+; under the terms of the GNU General Public License as published by\n+; the Free Software Foundation; either version 3, or (at your option)\n+; any later version.\n+;\n+; GCC is distributed in the hope that it will be useful, but\n+; WITHOUT ANY WARRANTY; without even the implied warranty of\n+; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+; General Public License for more details.\n+;\n+; You should have received a copy of the GNU General Public License\n+; along with GCC; see the file COPYING3.  If not see\n+; <http://www.gnu.org/licenses/>.\n+\n+HeaderInclude\n+config/aarch64/aarch64-opts.h\n+\n+; The TLS dialect names to use with -mtls-dialect.\n+\n+Enum\n+Name(tls_type) Type(enum aarch64_tls_type)\n+The possible TLS dialects:\n+\n+EnumValue\n+Enum(tls_type) String(trad) Value(TLS_TRADITIONAL)\n+\n+EnumValue\n+Enum(tls_type) String(desc) Value(TLS_DESCRIPTORS)\n+\n+; The code model option names for -mcmodel.\n+\n+Enum\n+Name(cmodel) Type(enum aarch64_code_model)\n+The code model option names for -mcmodel:\n+\n+EnumValue\n+Enum(cmodel) String(tiny) Value(AARCH64_CMODEL_TINY)\n+\n+EnumValue\n+Enum(cmodel) String(small) Value(AARCH64_CMODEL_SMALL)\n+\n+EnumValue\n+Enum(cmodel) String(large) Value(AARCH64_CMODEL_LARGE)\n+\n+; The cpu/arch option names to use in cpu/arch selection.\n+\n+Variable\n+const char *aarch64_arch_string\n+\n+Variable\n+const char *aarch64_cpu_string\n+\n+Variable\n+const char *aarch64_tune_string\n+\n+mbig-endian\n+Target Report RejectNegative Mask(BIG_END)\n+Assume target CPU is configured as big endian\n+\n+mgeneral-regs-only\n+Target Report RejectNegative Mask(GENERAL_REGS_ONLY)\n+Generate code which uses only the general registers\n+\n+mlittle-endian\n+Target Report RejectNegative InverseMask(BIG_END)\n+Assume target CPU is configured as little endian\n+\n+mcmodel=\n+Target RejectNegative Joined Enum(cmodel) Var(aarch64_cmodel_var) Init(AARCH64_CMODEL_SMALL)\n+Specify the code model\n+\n+mstrict-align\n+Target Report RejectNegative Mask(STRICT_ALIGN)\n+Don't assume that unaligned accesses are handled by the system\n+\n+momit-leaf-frame-pointer\n+Target Report Save Var(flag_omit_leaf_frame_pointer) Init(1)\n+Omit the frame pointer in leaf functions\n+\n+mtls-dialect=\n+Target RejectNegative Joined Enum(tls_type) Var(aarch64_tls_dialect) Init(TLS_DESCRIPTORS)\n+Specify TLS dialect\n+\n+march=\n+Target RejectNegative Joined Var(aarch64_arch_string)\n+-march=ARCH\tUse features of architecture ARCH\n+\n+mcpu=\n+Target RejectNegative Joined Var(aarch64_cpu_string)\n+-mcpu=CPU\tUse features of and optimize for CPU\n+\n+mtune=\n+Target RejectNegative Joined Var(aarch64_tune_string)\n+-mtune=CPU\tOptimize for CPU"}, {"sha": "e8fafa6d134733e42f41dcbd1da6313e847157c0", "filename": "gcc/config/aarch64/arm_neon.h", "status": "added", "additions": 25543, "deletions": 0, "changes": 25543, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360"}, {"sha": "fe613070f4e47a3d8988679a6802d836c739a958", "filename": "gcc/config/aarch64/constraints.md", "status": "added", "additions": 167, "deletions": 0, "changes": 167, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fconstraints.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fconstraints.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fconstraints.md?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,167 @@\n+;; Machine description for AArch64 architecture.\n+;; Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+;; Contributed by ARM Ltd.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+(define_register_constraint \"k\" \"STACK_REG\"\n+  \"@internal The stack register.\")\n+\n+(define_register_constraint \"w\" \"FP_REGS\"\n+  \"Floating point and SIMD vector registers.\")\n+\n+(define_register_constraint \"x\" \"FP_LO_REGS\"\n+  \"Floating point and SIMD vector registers V0 - V15.\")\n+\n+(define_constraint \"I\"\n+ \"A constant that can be used with an ADD operation.\"\n+ (and (match_code \"const_int\")\n+      (match_test \"aarch64_uimm12_shift (ival)\")))\n+\n+(define_constraint \"J\"\n+ \"A constant that can be used with a SUB operation (once negated).\"\n+ (and (match_code \"const_int\")\n+      (match_test \"aarch64_uimm12_shift (-ival)\")))\n+\n+;; We can't use the mode of a CONST_INT to determine the context in\n+;; which it is being used, so we must have a separate constraint for\n+;; each context.\n+\n+(define_constraint \"K\"\n+ \"A constant that can be used with a 32-bit logical operation.\"\n+ (and (match_code \"const_int\")\n+      (match_test \"aarch64_bitmask_imm (ival, SImode)\")))\n+\n+(define_constraint \"L\"\n+ \"A constant that can be used with a 64-bit logical operation.\"\n+ (and (match_code \"const_int\")\n+      (match_test \"aarch64_bitmask_imm (ival, DImode)\")))\n+\n+(define_constraint \"M\"\n+ \"A constant that can be used with a 32-bit MOV immediate operation.\"\n+ (and (match_code \"const_int\")\n+      (match_test \"aarch64_move_imm (ival, SImode)\")))\n+\n+(define_constraint \"N\"\n+ \"A constant that can be used with a 64-bit MOV immediate operation.\"\n+ (and (match_code \"const_int\")\n+      (match_test \"aarch64_move_imm (ival, DImode)\")))\n+\n+(define_constraint \"S\"\n+  \"A constraint that matches an absolute symbolic address.\"\n+  (and (match_code \"const,symbol_ref,label_ref\")\n+       (match_test \"aarch64_symbolic_address_p (op)\")))\n+\n+(define_constraint \"Y\"\n+  \"Floating point constant zero.\"\n+  (and (match_code \"const_double\")\n+       (match_test \"aarch64_const_double_zero_rtx_p (op)\")))\n+\n+(define_constraint \"Z\"\n+  \"Integer constant zero.\"\n+  (match_test \"op == const0_rtx\"))\n+\n+(define_constraint \"Usa\"\n+  \"A constraint that matches an absolute symbolic address.\"\n+  (and (match_code \"const,symbol_ref\")\n+       (match_test \"aarch64_symbolic_address_p (op)\")))\n+\n+(define_constraint \"Ush\"\n+  \"A constraint that matches an absolute symbolic address high part.\"\n+  (and (match_code \"high\")\n+       (match_test \"aarch64_valid_symref (XEXP (op, 0), GET_MODE (XEXP (op, 0)))\")))\n+\n+(define_constraint \"Uss\"\n+  \"@internal\n+  A constraint that matches an immediate shift constant in SImode.\"\n+  (and (match_code \"const_int\")\n+       (match_test \"(unsigned HOST_WIDE_INT) ival < 32\")))\n+\n+(define_constraint \"Usd\"\n+  \"@internal\n+  A constraint that matches an immediate shift constant in DImode.\"\n+  (and (match_code \"const_int\")\n+       (match_test \"(unsigned HOST_WIDE_INT) ival < 64\")))\n+\n+(define_constraint \"UsM\"\n+  \"@internal\n+  A constraint that matches the immediate constant -1.\"\n+  (match_test \"op == constm1_rtx\"))\n+\n+(define_constraint \"Ui3\"\n+  \"@internal\n+  A constraint that matches the integers 0...4.\"\n+  (and (match_code \"const_int\")\n+       (match_test \"(unsigned HOST_WIDE_INT) ival <= 4\")))\n+\n+(define_constraint \"Up3\"\n+  \"@internal\n+  A constraint that matches the integers 2^(0...4).\"\n+  (and (match_code \"const_int\")\n+       (match_test \"(unsigned) exact_log2 (ival) <= 4\")))\n+\n+(define_memory_constraint \"Q\"\n+ \"A memory address which uses a single base register with no offset.\"\n+ (and (match_code \"mem\")\n+      (match_test \"REG_P (XEXP (op, 0))\")))\n+\n+(define_memory_constraint \"Ump\"\n+  \"@internal\n+  A memory address suitable for a load/store pair operation.\"\n+  (and (match_code \"mem\")\n+       (match_test \"aarch64_legitimate_address_p (GET_MODE (op), XEXP (op, 0),\n+\t\t\t\t\t\t  PARALLEL, 1)\")))\n+\n+(define_memory_constraint \"Utv\"\n+  \"@internal\n+   An address valid for loading/storing opaque structure\n+   types wider than TImode.\"\n+  (and (match_code \"mem\")\n+       (match_test \"aarch64_simd_mem_operand_p (op)\")))\n+\n+(define_constraint \"Dn\"\n+  \"@internal\n+ A constraint that matches vector of immediates.\"\n+ (and (match_code \"const_vector\")\n+      (match_test \"aarch64_simd_immediate_valid_for_move (op, GET_MODE (op),\n+\t\t\t\t\t\t\t  NULL, NULL, NULL,\n+\t\t\t\t\t\t\t  NULL, NULL) != 0\")))\n+\n+(define_constraint \"Dl\"\n+  \"@internal\n+ A constraint that matches vector of immediates for left shifts.\"\n+ (and (match_code \"const_vector\")\n+      (match_test \"aarch64_simd_shift_imm_p (op, GET_MODE (op),\n+\t\t\t\t\t\t true)\")))\n+\n+(define_constraint \"Dr\"\n+  \"@internal\n+ A constraint that matches vector of immediates for right shifts.\"\n+ (and (match_code \"const_vector\")\n+      (match_test \"aarch64_simd_shift_imm_p (op, GET_MODE (op),\n+\t\t\t\t\t\t false)\")))\n+(define_constraint \"Dz\"\n+  \"@internal\n+ A constraint that matches vector of immediate zero.\"\n+ (and (match_code \"const_vector\")\n+      (match_test \"aarch64_simd_imm_zero_p (op, GET_MODE (op))\")))\n+\n+(define_constraint \"Dd\"\n+  \"@internal\n+ A constraint that matches an immediate operand valid for AdvSIMD scalar.\"\n+ (and (match_code \"const_int\")\n+      (match_test \"aarch64_simd_imm_scalar_p (op, GET_MODE (op))\")))"}, {"sha": "97b378756b660a310bfa79e9ba9e9a45aff2eb34", "filename": "gcc/config/aarch64/gentune.sh", "status": "added", "additions": 32, "deletions": 0, "changes": 32, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fgentune.sh", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fgentune.sh", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fgentune.sh?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,32 @@\n+#!/bin/sh\n+#\n+# Copyright (C) 2011, 2012 Free Software Foundation, Inc.\n+# Contributed by ARM Ltd.\n+#\n+# This file is part of GCC.\n+#\n+# GCC is free software; you can redistribute it and/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation; either version 3, or (at your option)\n+# any later version.\n+#\n+# GCC is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with GCC; see the file COPYING3.  If not see\n+# <http://www.gnu.org/licenses/>.\n+\n+# Generate aarch64-tune.md, a file containing the tune attribute from the list of \n+# CPUs in aarch64-cores.def\n+\n+echo \";; -*- buffer-read-only: t -*-\"\n+echo \";; Generated automatically by gentune.sh from aarch64-cores.def\"\n+\n+allcores=`awk -F'[(, \t]+' '/^AARCH64_CORE/ { cores = cores$3\",\" } END { print cores } ' $1`\n+\n+echo \"(define_attr \\\"tune\\\"\"\n+echo \"\t\\\"$allcores\\\"\" | sed -e 's/,\"$/\"/'\n+echo \"\t(const (symbol_ref \\\"((enum attr_tune) aarch64_tune)\\\")))\""}, {"sha": "bf2041e78e5bd0cb8a59a8fd8c284cd29d2c7a97", "filename": "gcc/config/aarch64/iterators.md", "status": "added", "additions": 716, "deletions": 0, "changes": 716, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,716 @@\n+;; Machine description for AArch64 architecture.\n+;; Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+;; Contributed by ARM Ltd.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+;; -------------------------------------------------------------------\n+;; Mode Iterators\n+;; -------------------------------------------------------------------\n+\n+\n+;; Iterator for General Purpose Integer registers (32- and 64-bit modes)\n+(define_mode_iterator GPI [SI DI])\n+\n+;; Iterator for QI and HI modes\n+(define_mode_iterator SHORT [QI HI])\n+\n+;; Iterator for all integer modes (up to 64-bit)\n+(define_mode_iterator ALLI [QI HI SI DI])\n+\n+;; Iterator scalar modes (up to 64-bit)\n+(define_mode_iterator SDQ_I [QI HI SI DI])\n+\n+;; Iterator for all integer modes that can be extended (up to 64-bit)\n+(define_mode_iterator ALLX [QI HI SI])\n+\n+;; Iterator for General Purpose Floating-point registers (32- and 64-bit modes)\n+(define_mode_iterator GPF [SF DF])\n+\n+;; Integer vector modes.\n+(define_mode_iterator VDQ [V8QI V16QI V4HI V8HI V2SI V4SI V2DI])\n+\n+;; Integer vector modes.\n+(define_mode_iterator VDQ_I [V8QI V16QI V4HI V8HI V2SI V4SI V2DI])\n+\n+;; vector and scalar, 64 & 128-bit container, all integer modes\n+(define_mode_iterator VSDQ_I [V8QI V16QI V4HI V8HI V2SI V4SI V2DI QI HI SI DI])\n+\n+;; vector and scalar, 64 & 128-bit container: all vector integer modes;\n+;; 64-bit scalar integer mode\n+(define_mode_iterator VSDQ_I_DI [V8QI V16QI V4HI V8HI V2SI V4SI V2DI DI])\n+\n+;; Double vector modes.\n+(define_mode_iterator VD [V8QI V4HI V2SI V2SF])\n+\n+;; vector, 64-bit container, all integer modes\n+(define_mode_iterator VD_BHSI [V8QI V4HI V2SI])\n+\n+;; 128 and 64-bit container; 8, 16, 32-bit vector integer modes\n+(define_mode_iterator VDQ_BHSI [V8QI V16QI V4HI V8HI V2SI V4SI])\n+\n+;; Quad vector modes.\n+(define_mode_iterator VQ [V16QI V8HI V4SI V2DI V4SF V2DF])\n+\n+;; All vector modes, except double.\n+(define_mode_iterator VQ_S [V8QI V16QI V4HI V8HI V2SI V4SI])\n+\n+;; Vector and scalar, 64 & 128-bit container: all vector integer mode;\n+;; 8, 16, 32-bit scalar integer modes\n+(define_mode_iterator VSDQ_I_BHSI [V8QI V16QI V4HI V8HI V2SI V4SI V2DI QI HI SI])\n+\n+;; Vector modes for moves.\n+(define_mode_iterator VDQM [V8QI V16QI V4HI V8HI V2SI V4SI])\n+\n+;; This mode iterator allows :PTR to be used for patterns that operate on\n+;; pointer-sized quantities.  Exactly one of the two alternatives will match.\n+(define_mode_iterator PTR [(SI \"Pmode == SImode\") (DI \"Pmode == DImode\")])\n+\n+;; Vector Float modes.\n+(define_mode_iterator VDQF [V2SF V4SF V2DF])\n+\n+;; Vector Float modes with 2 elements.\n+(define_mode_iterator V2F [V2SF V2DF])\n+\n+;; All modes.\n+(define_mode_iterator VALL [V8QI V16QI V4HI V8HI V2SI V4SI V2DI V2SF V4SF V2DF])\n+\n+;; Vector modes for Integer reduction across lanes.\n+(define_mode_iterator VDQV [V8QI V16QI V4HI V8HI V4SI])\n+\n+;; All double integer narrow-able modes.\n+(define_mode_iterator VDN [V4HI V2SI DI])\n+\n+;; All quad integer narrow-able modes.\n+(define_mode_iterator VQN [V8HI V4SI V2DI])\n+\n+;; All double integer widen-able modes.\n+(define_mode_iterator VDW [V8QI V4HI V2SI])\n+\n+;; Vector and scalar 128-bit container: narrowable 16, 32, 64-bit integer modes\n+(define_mode_iterator VSQN_HSDI [V8HI V4SI V2DI HI SI DI])\n+\n+;; All quad integer widen-able modes.\n+(define_mode_iterator VQW [V16QI V8HI V4SI])\n+\n+;; Double vector modes for combines.\n+(define_mode_iterator VDC [V8QI V4HI V2SI V2SF DI DF])\n+\n+;; Double vector modes for combines.\n+(define_mode_iterator VDIC [V8QI V4HI V2SI])\n+\n+;; Double vector modes.\n+(define_mode_iterator VD_RE [V8QI V4HI V2SI DI DF V2SF])\n+\n+;; Vector modes except double int.\n+(define_mode_iterator VDQIF [V8QI V16QI V4HI V8HI V2SI V4SI V2SF V4SF V2DF])\n+\n+;; Vector modes for H and S types.\n+(define_mode_iterator VDQHS [V4HI V8HI V2SI V4SI])\n+\n+;; Vector and scalar integer modes for H and S\n+(define_mode_iterator VSDQ_HSI [V4HI V8HI V2SI V4SI HI SI])\n+\n+;; Vector and scalar 64-bit container: 16, 32-bit integer modes\n+(define_mode_iterator VSD_HSI [V4HI V2SI HI SI])\n+\n+;; Vector 64-bit container: 16, 32-bit integer modes\n+(define_mode_iterator VD_HSI [V4HI V2SI])\n+\n+;; Scalar 64-bit container: 16, 32-bit integer modes\n+(define_mode_iterator SD_HSI [HI SI])\n+\n+;; Vector 64-bit container: 16, 32-bit integer modes\n+(define_mode_iterator VQ_HSI [V8HI V4SI])\n+\n+;; All byte modes.\n+(define_mode_iterator VB [V8QI V16QI])\n+\n+(define_mode_iterator TX [TI TF])\n+\n+;; Opaque structure modes.\n+(define_mode_iterator VSTRUCT [OI CI XI])\n+\n+;; Double scalar modes\n+(define_mode_iterator DX [DI DF])\n+\n+;; ------------------------------------------------------------------\n+;; Unspec enumerations for Advance SIMD. These could well go into\n+;; aarch64.md but for their use in int_iterators here.\n+;; ------------------------------------------------------------------\n+\n+(define_c_enum \"unspec\"\n+ [\n+    UNSPEC_ASHIFT_SIGNED\t; Used in aarch-simd.md.\n+    UNSPEC_ASHIFT_UNSIGNED\t; Used in aarch64-simd.md.\n+    UNSPEC_FMAXV\t; Used in aarch64-simd.md.\n+    UNSPEC_FMINV\t; Used in aarch64-simd.md.\n+    UNSPEC_FADDV\t; Used in aarch64-simd.md.\n+    UNSPEC_ADDV\t\t; Used in aarch64-simd.md.\n+    UNSPEC_SMAXV\t; Used in aarch64-simd.md.\n+    UNSPEC_SMINV\t; Used in aarch64-simd.md.\n+    UNSPEC_UMAXV\t; Used in aarch64-simd.md.\n+    UNSPEC_UMINV\t; Used in aarch64-simd.md.\n+    UNSPEC_SHADD\t; Used in aarch64-simd.md.\n+    UNSPEC_UHADD\t; Used in aarch64-simd.md.\n+    UNSPEC_SRHADD\t; Used in aarch64-simd.md.\n+    UNSPEC_URHADD\t; Used in aarch64-simd.md.\n+    UNSPEC_SHSUB\t; Used in aarch64-simd.md.\n+    UNSPEC_UHSUB\t; Used in aarch64-simd.md.\n+    UNSPEC_SRHSUB\t; Used in aarch64-simd.md.\n+    UNSPEC_URHSUB\t; Used in aarch64-simd.md.\n+    UNSPEC_ADDHN\t; Used in aarch64-simd.md.\n+    UNSPEC_RADDHN\t; Used in aarch64-simd.md.\n+    UNSPEC_SUBHN\t; Used in aarch64-simd.md.\n+    UNSPEC_RSUBHN\t; Used in aarch64-simd.md.\n+    UNSPEC_ADDHN2\t; Used in aarch64-simd.md.\n+    UNSPEC_RADDHN2\t; Used in aarch64-simd.md.\n+    UNSPEC_SUBHN2\t; Used in aarch64-simd.md.\n+    UNSPEC_RSUBHN2\t; Used in aarch64-simd.md.\n+    UNSPEC_SQDMULH\t; Used in aarch64-simd.md.\n+    UNSPEC_SQRDMULH\t; Used in aarch64-simd.md.\n+    UNSPEC_PMUL\t\t; Used in aarch64-simd.md.\n+    UNSPEC_USQADD\t; Used in aarch64-simd.md.\n+    UNSPEC_SUQADD\t; Used in aarch64-simd.md.\n+    UNSPEC_SQXTUN\t; Used in aarch64-simd.md.\n+    UNSPEC_SQXTN\t; Used in aarch64-simd.md.\n+    UNSPEC_UQXTN\t; Used in aarch64-simd.md.\n+    UNSPEC_SSRA\t\t; Used in aarch64-simd.md.\n+    UNSPEC_USRA\t\t; Used in aarch64-simd.md.\n+    UNSPEC_SRSRA\t; Used in aarch64-simd.md.\n+    UNSPEC_URSRA\t; Used in aarch64-simd.md.\n+    UNSPEC_SRSHR\t; Used in aarch64-simd.md.\n+    UNSPEC_URSHR\t; Used in aarch64-simd.md.\n+    UNSPEC_SQSHLU\t; Used in aarch64-simd.md.\n+    UNSPEC_SQSHL\t; Used in aarch64-simd.md.\n+    UNSPEC_UQSHL\t; Used in aarch64-simd.md.\n+    UNSPEC_SQSHRUN\t; Used in aarch64-simd.md.\n+    UNSPEC_SQRSHRUN\t; Used in aarch64-simd.md.\n+    UNSPEC_SQSHRN\t; Used in aarch64-simd.md.\n+    UNSPEC_UQSHRN\t; Used in aarch64-simd.md.\n+    UNSPEC_SQRSHRN\t; Used in aarch64-simd.md.\n+    UNSPEC_UQRSHRN\t; Used in aarch64-simd.md.\n+    UNSPEC_SSHL\t\t; Used in aarch64-simd.md.\n+    UNSPEC_USHL\t\t; Used in aarch64-simd.md.\n+    UNSPEC_SRSHL\t; Used in aarch64-simd.md.\n+    UNSPEC_URSHL\t; Used in aarch64-simd.md.\n+    UNSPEC_SQRSHL\t; Used in aarch64-simd.md.\n+    UNSPEC_UQRSHL\t; Used in aarch64-simd.md.\n+    UNSPEC_CMEQ\t\t; Used in aarch64-simd.md.\n+    UNSPEC_CMLE\t\t; Used in aarch64-simd.md.\n+    UNSPEC_CMLT\t\t; Used in aarch64-simd.md.\n+    UNSPEC_CMGE\t\t; Used in aarch64-simd.md.\n+    UNSPEC_CMGT\t\t; Used in aarch64-simd.md.\n+    UNSPEC_CMHS\t\t; Used in aarch64-simd.md.\n+    UNSPEC_CMHI\t\t; Used in aarch64-simd.md.\n+    UNSPEC_SSLI\t\t; Used in aarch64-simd.md.\n+    UNSPEC_USLI\t\t; Used in aarch64-simd.md.\n+    UNSPEC_SSRI\t\t; Used in aarch64-simd.md.\n+    UNSPEC_USRI\t\t; Used in aarch64-simd.md.\n+    UNSPEC_SSHLL\t; Used in aarch64-simd.md.\n+    UNSPEC_USHLL\t; Used in aarch64-simd.md.\n+    UNSPEC_ADDP\t\t; Used in aarch64-simd.md.\n+    UNSPEC_CMTST\t; Used in aarch64-simd.md.\n+    UNSPEC_FMAX\t\t; Used in aarch64-simd.md.\n+    UNSPEC_FMIN\t\t; Used in aarch64-simd.md.\n+])\n+\n+;; -------------------------------------------------------------------\n+;; Mode attributes\n+;; -------------------------------------------------------------------\n+\n+;; In GPI templates, a string like \"%<w>0\" will expand to \"%w0\" in the\n+;; 32-bit version and \"%x0\" in the 64-bit version.\n+(define_mode_attr w [(QI \"w\") (HI \"w\") (SI \"w\") (DI \"x\") (SF \"s\") (DF \"d\")])\n+\n+;; For scalar usage of vector/FP registers\n+(define_mode_attr v [(QI \"b\") (HI \"h\") (SI \"s\") (DI \"d\")\n+\t\t    (V8QI \"\") (V16QI \"\")\n+\t\t    (V4HI \"\") (V8HI \"\")\n+\t\t    (V2SI \"\") (V4SI  \"\")\n+\t\t    (V2DI \"\") (V2SF \"\")\n+\t\t    (V4SF \"\") (V2DF \"\")])\n+\n+;; For scalar usage of vector/FP registers, narrowing\n+(define_mode_attr vn2 [(QI \"\") (HI \"b\") (SI \"h\") (DI \"s\")\n+\t\t    (V8QI \"\") (V16QI \"\")\n+\t\t    (V4HI \"\") (V8HI \"\")\n+\t\t    (V2SI \"\") (V4SI  \"\")\n+\t\t    (V2DI \"\") (V2SF \"\")\n+\t\t    (V4SF \"\") (V2DF \"\")])\n+\n+;; For scalar usage of vector/FP registers, widening\n+(define_mode_attr vw2 [(DI \"\") (QI \"h\") (HI \"s\") (SI \"d\")\n+\t\t    (V8QI \"\") (V16QI \"\")\n+\t\t    (V4HI \"\") (V8HI \"\")\n+\t\t    (V2SI \"\") (V4SI  \"\")\n+\t\t    (V2DI \"\") (V2SF \"\")\n+\t\t    (V4SF \"\") (V2DF \"\")])\n+\n+;; Map a floating point mode to the appropriate register name prefix\n+(define_mode_attr s [(SF \"s\") (DF \"d\")])\n+\n+;; Give the length suffix letter for a sign- or zero-extension.\n+(define_mode_attr size [(QI \"b\") (HI \"h\") (SI \"w\")])\n+\n+;; Give the number of bits in the mode\n+(define_mode_attr sizen [(QI \"8\") (HI \"16\") (SI \"32\") (DI \"64\")])\n+\n+;; Give the ordinal of the MSB in the mode\n+(define_mode_attr sizem1 [(QI \"#7\") (HI \"#15\") (SI \"#31\") (DI \"#63\")])\n+\n+;; Attribute to describe constants acceptable in logical operations\n+(define_mode_attr lconst [(SI \"K\") (DI \"L\")])\n+\n+;; Map a mode to a specific constraint character.\n+(define_mode_attr cmode [(QI \"q\") (HI \"h\") (SI \"s\") (DI \"d\")])\n+\n+(define_mode_attr Vtype [(V8QI \"8b\") (V16QI \"16b\")\n+\t\t\t (V4HI \"4h\") (V8HI  \"8h\")\n+                         (V2SI \"2s\") (V4SI  \"4s\")\n+                         (DI   \"1d\") (DF    \"1d\")\n+                         (V2DI \"2d\") (V2SF \"2s\")\n+\t\t\t (V4SF \"4s\") (V2DF \"2d\")])\n+\n+(define_mode_attr Vmtype [(V8QI \".8b\") (V16QI \".16b\")\n+\t\t\t (V4HI \".4h\") (V8HI  \".8h\")\n+\t\t\t (V2SI \".2s\") (V4SI  \".4s\")\n+\t\t\t (V2DI \".2d\") (V2SF \".2s\")\n+\t\t\t (V4SF \".4s\") (V2DF \".2d\")\n+\t\t\t (DI   \"\")    (SI   \"\")\n+\t\t\t (HI   \"\")    (QI   \"\")\n+\t\t\t (TI   \"\")])\n+\n+;; Register suffix narrowed modes for VQN.\n+(define_mode_attr Vmntype [(V8HI \".8b\") (V4SI \".4h\")\n+\t\t\t   (V2DI \".2s\")\n+\t\t\t   (DI   \"\")    (SI   \"\")\n+\t\t\t   (HI   \"\")])\n+\n+;; Mode-to-individual element type mapping.\n+(define_mode_attr Vetype [(V8QI \"b\") (V16QI \"b\")\n+\t\t\t  (V4HI \"h\") (V8HI  \"h\")\n+                          (V2SI \"s\") (V4SI  \"s\")\n+\t\t\t  (V2DI \"d\") (V2SF  \"s\")\n+\t\t\t  (V4SF \"s\") (V2DF  \"d\")\n+\t\t\t  (QI \"b\")   (HI \"h\")\n+\t\t\t  (SI \"s\")   (DI \"d\")])\n+\n+;; Mode-to-bitwise operation type mapping.\n+(define_mode_attr Vbtype [(V8QI \"8b\")  (V16QI \"16b\")\n+\t\t\t  (V4HI \"8b\") (V8HI  \"16b\")\n+\t\t\t  (V2SI \"8b\") (V4SI  \"16b\")\n+\t\t\t  (V2DI \"16b\") (V2SF  \"8b\")\n+\t\t\t  (V4SF \"16b\") (V2DF  \"16b\")])\n+\n+;; Define element mode for each vector mode.\n+(define_mode_attr VEL [(V8QI \"QI\") (V16QI \"QI\")\n+\t\t\t(V4HI \"HI\") (V8HI \"HI\")\n+                        (V2SI \"SI\") (V4SI \"SI\")\n+                        (DI \"DI\")   (V2DI \"DI\")\n+                        (V2SF \"SF\") (V4SF \"SF\")\n+                        (V2DF \"DF\")\n+\t\t\t(SI   \"SI\") (HI   \"HI\")\n+\t\t\t(QI   \"QI\")])\n+\n+;; Define container mode for lane selection.\n+(define_mode_attr VCON [(V8QI \"V16QI\") (V16QI \"V16QI\")\n+\t\t\t(V4HI \"V8HI\") (V8HI \"V8HI\")\n+\t\t\t(V2SI \"V4SI\") (V4SI \"V4SI\")\n+\t\t\t(DI   \"V2DI\") (V2DI \"V2DI\")\n+\t\t\t(V2SF \"V2SF\") (V4SF \"V4SF\")\n+\t\t\t(V2DF \"V2DF\") (SI   \"V4SI\")\n+\t\t\t(HI   \"V8HI\") (QI   \"V16QI\")])\n+\n+;; Half modes of all vector modes.\n+(define_mode_attr VHALF [(V8QI \"V4QI\")  (V16QI \"V8QI\")\n+\t\t\t (V4HI \"V2HI\")  (V8HI  \"V4HI\")\n+\t\t\t (V2SI \"SI\")    (V4SI  \"V2SI\")\n+\t\t\t (V2DI \"DI\")    (V2SF  \"SF\")\n+\t\t\t (V4SF \"V2SF\")  (V2DF  \"DF\")])\n+\n+;; Double modes of vector modes.\n+(define_mode_attr VDBL [(V8QI \"V16QI\") (V4HI \"V8HI\")\n+\t\t\t(V2SI \"V4SI\")  (V2SF \"V4SF\")\n+\t\t\t(SI   \"V2SI\")  (DI   \"V2DI\")\n+\t\t\t(DF   \"V2DF\")])\n+\n+;; Double modes of vector modes (lower case).\n+(define_mode_attr Vdbl [(V8QI \"v16qi\") (V4HI \"v8hi\")\n+\t\t\t(V2SI \"v4si\")  (V2SF \"v4sf\")\n+\t\t\t(SI   \"v2si\")  (DI   \"v2di\")])\n+\n+;; Narrowed modes for VDN.\n+(define_mode_attr VNARROWD [(V4HI \"V8QI\") (V2SI \"V4HI\")\n+\t\t\t    (DI   \"V2SI\")])\n+\n+;; Narrowed double-modes for VQN (Used for XTN).\n+(define_mode_attr VNARROWQ [(V8HI \"V8QI\") (V4SI \"V4HI\")\n+\t\t\t    (V2DI \"V2SI\")\n+\t\t\t    (DI\t  \"SI\")\t  (SI\t\"HI\")\n+\t\t\t    (HI\t  \"QI\")])\n+\n+;; Narrowed quad-modes for VQN (Used for XTN2).\n+(define_mode_attr VNARROWQ2 [(V8HI \"V16QI\") (V4SI \"V8HI\")\n+\t\t\t     (V2DI \"V4SI\")])\n+\n+;; Register suffix narrowed modes for VQN.\n+(define_mode_attr Vntype [(V8HI \"8b\") (V4SI \"4h\")\n+\t\t\t  (V2DI \"2s\")])\n+\n+;; Register suffix narrowed modes for VQN.\n+(define_mode_attr V2ntype [(V8HI \"16b\") (V4SI \"8h\")\n+\t\t\t   (V2DI \"4s\")])\n+\n+;; Widened modes of vector modes.\n+(define_mode_attr VWIDE [(V8QI \"V8HI\") (V4HI \"V4SI\")\n+\t\t\t (V2SI \"V2DI\") (V16QI \"V8HI\") \n+\t\t\t (V8HI \"V4SI\") (V4SI \"V2DI\")\n+\t\t\t (HI \"SI\")     (SI \"DI\")]\n+\n+)\n+\n+;; Widened mode register suffixes for VDW/VQW.\n+(define_mode_attr Vwtype [(V8QI \"8h\") (V4HI \"4s\")\n+\t\t\t  (V2SI \"2d\") (V16QI \"8h\") \n+\t\t\t  (V8HI \"4s\") (V4SI \"2d\")])\n+\n+;; Widened mode register suffixes for VDW/VQW.\n+(define_mode_attr Vmwtype [(V8QI \".8h\") (V4HI \".4s\")\n+\t\t\t   (V2SI \".2d\") (V16QI \".8h\") \n+\t\t\t   (V8HI \".4s\") (V4SI \".2d\")\n+\t\t\t   (SI   \"\")    (HI   \"\")])\n+\n+;; Lower part register suffixes for VQW.\n+(define_mode_attr Vhalftype [(V16QI \"8b\") (V8HI \"4h\")\n+\t\t\t     (V4SI \"2s\")])\n+\n+;; Define corresponding core/FP element mode for each vector mode.\n+(define_mode_attr vw   [(V8QI \"w\") (V16QI \"w\")\n+                        (V4HI \"w\") (V8HI \"w\")\n+                        (V2SI \"w\") (V4SI \"w\")\n+                        (DI   \"x\") (V2DI \"x\")\n+                        (V2SF \"s\") (V4SF \"s\")\n+                        (V2DF \"d\")])\n+\n+;; Double vector types for ALLX.\n+(define_mode_attr Vallxd [(QI \"8b\") (HI \"4h\") (SI \"2s\")])\n+\n+;; Mode of result of comparison operations.\n+(define_mode_attr V_cmp_result [(V8QI \"V8QI\") (V16QI \"V16QI\")\n+\t\t\t\t(V4HI \"V4HI\") (V8HI  \"V8HI\")\n+\t\t\t\t(V2SI \"V2SI\") (V4SI  \"V4SI\")\n+\t\t\t\t(V2SF \"V2SI\") (V4SF  \"V4SI\")\n+\t\t\t\t(DI   \"DI\")   (V2DI  \"V2DI\")])\n+\n+;; Vm for lane instructions is restricted to FP_LO_REGS.\n+(define_mode_attr vwx [(V4HI \"x\") (V8HI \"x\") (HI \"x\")\n+\t\t       (V2SI \"w\") (V4SI \"w\") (SI \"w\")])\n+\n+(define_mode_attr Vendreg [(OI \"T\") (CI \"U\") (XI \"V\")])\n+\n+(define_mode_attr nregs [(OI \"2\") (CI \"3\") (XI \"4\")])\n+\n+(define_mode_attr VRL2 [(V8QI \"V32QI\") (V4HI \"V16HI\")\n+\t\t\t(V2SI \"V8SI\")  (V2SF \"V8SF\")\n+\t\t\t(DI   \"V4DI\")  (DF   \"V4DF\")\n+\t\t\t(V16QI \"V32QI\") (V8HI \"V16HI\")\n+\t\t\t(V4SI \"V8SI\")  (V4SF \"V8SF\")\n+\t\t\t(V2DI \"V4DI\")  (V2DF \"V4DF\")])\n+\n+(define_mode_attr VRL3 [(V8QI \"V48QI\") (V4HI \"V24HI\")\n+\t\t\t(V2SI \"V12SI\")  (V2SF \"V12SF\")\n+\t\t\t(DI   \"V6DI\")  (DF   \"V6DF\")\n+\t\t\t(V16QI \"V48QI\") (V8HI \"V24HI\")\n+\t\t\t(V4SI \"V12SI\")  (V4SF \"V12SF\")\n+\t\t\t(V2DI \"V6DI\")  (V2DF \"V6DF\")])\n+\n+(define_mode_attr VRL4 [(V8QI \"V64QI\") (V4HI \"V32HI\")\n+\t\t\t(V2SI \"V16SI\")  (V2SF \"V16SF\")\n+\t\t\t(DI   \"V8DI\")  (DF   \"V8DF\")\n+\t\t\t(V16QI \"V64QI\") (V8HI \"V32HI\")\n+\t\t\t(V4SI \"V16SI\")  (V4SF \"V16SF\")\n+\t\t\t(V2DI \"V8DI\")  (V2DF \"V8DF\")])\n+\n+(define_mode_attr VSTRUCT_DREG [(OI \"TI\") (CI \"EI\") (XI \"OI\")])\n+\n+;; -------------------------------------------------------------------\n+;; Code Iterators\n+;; -------------------------------------------------------------------\n+\n+;; This code iterator allows the various shifts supported on the core\n+(define_code_iterator SHIFT [ashift ashiftrt lshiftrt rotatert])\n+\n+;; This code iterator allows the shifts supported in arithmetic instructions\n+(define_code_iterator ASHIFT [ashift ashiftrt lshiftrt])\n+\n+;; Code iterator for logical operations\n+(define_code_iterator LOGICAL [and ior xor])\n+\n+;; Code iterator for sign/zero extension\n+(define_code_iterator ANY_EXTEND [sign_extend zero_extend])\n+\n+;; All division operations (signed/unsigned)\n+(define_code_iterator ANY_DIV [div udiv])\n+\n+;; Code iterator for sign/zero extraction\n+(define_code_iterator ANY_EXTRACT [sign_extract zero_extract])\n+\n+;; Code iterator for equality comparisons\n+(define_code_iterator EQL [eq ne])\n+\n+;; Code iterator for less-than and greater/equal-to\n+(define_code_iterator LTGE [lt ge])\n+\n+;; Iterator for __sync_<op> operations that where the operation can be\n+;; represented directly RTL.  This is all of the sync operations bar\n+;; nand.\n+(define_code_iterator syncop [plus minus ior xor and])\n+\n+;; Iterator for integer conversions\n+(define_code_iterator FIXUORS [fix unsigned_fix])\n+\n+;; Code iterator for variants of vector max and min.\n+(define_code_iterator MAXMIN [smax smin umax umin])\n+\n+;; Code iterator for variants of vector max and min.\n+(define_code_iterator ADDSUB [plus minus])\n+\n+;; Code iterator for variants of vector saturating binary ops.\n+(define_code_iterator BINQOPS [ss_plus us_plus ss_minus us_minus])\n+\n+;; Code iterator for variants of vector saturating unary ops.\n+(define_code_iterator UNQOPS [ss_neg ss_abs])\n+\n+;; Code iterator for signed variants of vector saturating binary ops.\n+(define_code_iterator SBINQOPS [ss_plus ss_minus])\n+\n+;; -------------------------------------------------------------------\n+;; Code Attributes\n+;; -------------------------------------------------------------------\n+;; Map rtl objects to optab names\n+(define_code_attr optab [(ashift \"ashl\")\n+\t\t\t (ashiftrt \"ashr\")\n+\t\t\t (lshiftrt \"lshr\")\n+\t\t\t (rotatert \"rotr\")\n+\t\t\t (sign_extend \"extend\")\n+\t\t\t (zero_extend \"zero_extend\")\n+\t\t\t (sign_extract \"extv\")\n+\t\t\t (zero_extract \"extzv\")\n+\t\t\t (and \"and\")\n+\t\t\t (ior \"ior\")\n+\t\t\t (xor \"xor\")\n+\t\t\t (not \"one_cmpl\")\n+\t\t\t (neg \"neg\")\n+\t\t\t (plus \"add\")\n+\t\t\t (minus \"sub\")\n+\t\t\t (ss_plus \"qadd\")\n+\t\t\t (us_plus \"qadd\")\n+\t\t\t (ss_minus \"qsub\")\n+\t\t\t (us_minus \"qsub\")\n+\t\t\t (ss_neg \"qneg\")\n+\t\t\t (ss_abs \"qabs\")\n+\t\t\t (eq \"eq\")\n+\t\t\t (ne \"ne\")\n+\t\t\t (lt \"lt\")\n+\t\t\t (ge \"ge\")])\n+\n+;; Optab prefix for sign/zero-extending operations\n+(define_code_attr su_optab [(sign_extend \"\") (zero_extend \"u\")\n+\t\t\t    (div \"\") (udiv \"u\")\n+\t\t\t    (fix \"\") (unsigned_fix \"u\")\n+\t\t\t    (ss_plus \"s\") (us_plus \"u\")\n+\t\t\t    (ss_minus \"s\") (us_minus \"u\")])\n+\n+;; Similar for the instruction mnemonics\n+(define_code_attr shift [(ashift \"lsl\") (ashiftrt \"asr\")\n+\t\t\t (lshiftrt \"lsr\") (rotatert \"ror\")])\n+\n+;; Map shift operators onto underlying bit-field instructions\n+(define_code_attr bfshift [(ashift \"ubfiz\") (ashiftrt \"sbfx\")\n+\t\t\t   (lshiftrt \"ubfx\") (rotatert \"extr\")])\n+\n+;; Logical operator instruction mnemonics\n+(define_code_attr logical [(and \"and\") (ior \"orr\") (xor \"eor\")])\n+\n+;; Similar, but when not(op)\n+(define_code_attr nlogical [(and \"bic\") (ior \"orn\") (xor \"eon\")])\n+\n+;; Sign- or zero-extending load\n+(define_code_attr ldrxt [(sign_extend \"ldrs\") (zero_extend \"ldr\")])\n+\n+;; Sign- or zero-extending data-op\n+(define_code_attr su [(sign_extend \"s\") (zero_extend \"u\")\n+\t\t      (sign_extract \"s\") (zero_extract \"u\")\n+\t\t      (fix \"s\") (unsigned_fix \"u\")\n+\t\t      (div \"s\") (udiv \"u\")])\n+\n+;; Emit cbz/cbnz depending on comparison type.\n+(define_code_attr cbz [(eq \"cbz\") (ne \"cbnz\") (lt \"cbnz\") (ge \"cbz\")])\n+\n+;; Emit tbz/tbnz depending on comparison type.\n+(define_code_attr tbz [(eq \"tbz\") (ne \"tbnz\") (lt \"tbnz\") (ge \"tbz\")])\n+\n+;; Max/min attributes.\n+(define_code_attr maxmin [(smax \"smax\")\n+\t\t\t  (smin \"smin\")\n+\t\t\t  (umax \"umax\")\n+\t\t\t  (umin \"umin\")])\n+\n+;; MLA/MLS attributes.\n+(define_code_attr as [(ss_plus \"a\") (ss_minus \"s\")])\n+\n+\n+;; -------------------------------------------------------------------\n+;; Int Iterators.\n+;; -------------------------------------------------------------------\n+(define_int_iterator MAXMINV [UNSPEC_UMAXV UNSPEC_UMINV\n+\t\t\t      UNSPEC_SMAXV UNSPEC_SMINV])\n+\n+(define_int_iterator FMAXMINV [UNSPEC_FMAXV UNSPEC_FMINV])\n+\n+(define_int_iterator HADDSUB [UNSPEC_SHADD UNSPEC_UHADD\n+\t\t\t      UNSPEC_SRHADD UNSPEC_URHADD\n+\t\t\t      UNSPEC_SHSUB UNSPEC_UHSUB\n+\t\t\t      UNSPEC_SRHSUB UNSPEC_URHSUB])\n+\n+\n+(define_int_iterator ADDSUBHN [UNSPEC_ADDHN UNSPEC_RADDHN\n+\t\t\t       UNSPEC_SUBHN UNSPEC_RSUBHN])\n+\n+(define_int_iterator ADDSUBHN2 [UNSPEC_ADDHN2 UNSPEC_RADDHN2\n+\t\t\t        UNSPEC_SUBHN2 UNSPEC_RSUBHN2])\n+\n+(define_int_iterator FMAXMIN [UNSPEC_FMAX UNSPEC_FMIN])\n+\n+(define_int_iterator VQDMULH [UNSPEC_SQDMULH UNSPEC_SQRDMULH])\n+\n+(define_int_iterator USSUQADD [UNSPEC_SUQADD UNSPEC_USQADD])\n+\n+(define_int_iterator SUQMOVN [UNSPEC_SQXTN UNSPEC_UQXTN])\n+\n+(define_int_iterator VSHL [UNSPEC_SSHL UNSPEC_USHL\n+\t\t           UNSPEC_SRSHL UNSPEC_URSHL])\n+\n+(define_int_iterator VSHLL [UNSPEC_SSHLL UNSPEC_USHLL])\n+\n+(define_int_iterator VQSHL [UNSPEC_SQSHL UNSPEC_UQSHL\n+                            UNSPEC_SQRSHL UNSPEC_UQRSHL])\n+\n+(define_int_iterator VSRA [UNSPEC_SSRA UNSPEC_USRA\n+\t\t\t     UNSPEC_SRSRA UNSPEC_URSRA])\n+\n+(define_int_iterator VSLRI [UNSPEC_SSLI UNSPEC_USLI\n+\t\t\t      UNSPEC_SSRI UNSPEC_USRI])\n+\n+\n+(define_int_iterator VRSHR_N [UNSPEC_SRSHR UNSPEC_URSHR])\n+\n+(define_int_iterator VQSHL_N [UNSPEC_SQSHLU UNSPEC_SQSHL UNSPEC_UQSHL])\n+\n+(define_int_iterator VQSHRN_N [UNSPEC_SQSHRUN UNSPEC_SQRSHRUN\n+                               UNSPEC_SQSHRN UNSPEC_UQSHRN\n+                               UNSPEC_SQRSHRN UNSPEC_UQRSHRN])\n+\n+(define_int_iterator VCMP_S [UNSPEC_CMEQ UNSPEC_CMGE UNSPEC_CMGT\n+\t\t\t     UNSPEC_CMLE UNSPEC_CMLT])\n+\n+(define_int_iterator VCMP_U [UNSPEC_CMHS UNSPEC_CMHI UNSPEC_CMTST])\n+\n+\n+;; -------------------------------------------------------------------\n+;; Int Iterators Attributes.\n+;; -------------------------------------------------------------------\n+(define_int_attr  maxminv [(UNSPEC_UMAXV \"umax\")\n+\t\t\t   (UNSPEC_UMINV \"umin\")\n+\t\t\t   (UNSPEC_SMAXV \"smax\")\n+\t\t\t   (UNSPEC_SMINV \"smin\")])\n+\n+(define_int_attr  fmaxminv [(UNSPEC_FMAXV \"max\")\n+\t\t\t    (UNSPEC_FMINV \"min\")])\n+\n+(define_int_attr  fmaxmin [(UNSPEC_FMAX \"fmax\")\n+\t\t\t   (UNSPEC_FMIN \"fmin\")])\n+\n+(define_int_attr sur [(UNSPEC_SHADD \"s\") (UNSPEC_UHADD \"u\")\n+\t\t      (UNSPEC_SRHADD \"sr\") (UNSPEC_URHADD \"ur\")\n+\t\t      (UNSPEC_SHSUB \"s\") (UNSPEC_UHSUB \"u\")\n+\t\t      (UNSPEC_SRHSUB \"sr\") (UNSPEC_URHSUB \"ur\")\n+\t\t      (UNSPEC_ADDHN \"\") (UNSPEC_RADDHN \"r\")\n+\t\t      (UNSPEC_SUBHN \"\") (UNSPEC_RSUBHN \"r\")\n+\t\t      (UNSPEC_ADDHN2 \"\") (UNSPEC_RADDHN2 \"r\")\n+\t\t      (UNSPEC_SUBHN2 \"\") (UNSPEC_RSUBHN2 \"r\")\n+\t\t      (UNSPEC_SQXTN \"s\") (UNSPEC_UQXTN \"u\")\n+\t\t      (UNSPEC_USQADD \"us\") (UNSPEC_SUQADD \"su\")\n+\t\t      (UNSPEC_SSLI  \"s\") (UNSPEC_USLI  \"u\")\n+\t\t      (UNSPEC_SSRI  \"s\") (UNSPEC_USRI  \"u\")\n+\t\t      (UNSPEC_USRA  \"u\") (UNSPEC_SSRA  \"s\")\n+\t\t      (UNSPEC_URSRA  \"ur\") (UNSPEC_SRSRA  \"sr\")\n+\t\t      (UNSPEC_URSHR  \"ur\") (UNSPEC_SRSHR  \"sr\")\n+\t\t      (UNSPEC_SQSHLU \"s\") (UNSPEC_SQSHL   \"s\")\n+\t\t      (UNSPEC_UQSHL  \"u\")\n+\t\t      (UNSPEC_SQSHRUN \"s\") (UNSPEC_SQRSHRUN \"s\")\n+                      (UNSPEC_SQSHRN \"s\")  (UNSPEC_UQSHRN \"u\")\n+                      (UNSPEC_SQRSHRN \"s\") (UNSPEC_UQRSHRN \"u\")\n+\t\t      (UNSPEC_USHL  \"u\")   (UNSPEC_SSHL  \"s\")\n+\t\t      (UNSPEC_USHLL  \"u\")  (UNSPEC_SSHLL \"s\")\n+\t\t      (UNSPEC_URSHL  \"ur\") (UNSPEC_SRSHL  \"sr\")\n+\t\t      (UNSPEC_UQRSHL  \"u\") (UNSPEC_SQRSHL  \"s\")\n+])\n+\n+(define_int_attr r [(UNSPEC_SQDMULH \"\") (UNSPEC_SQRDMULH \"r\")\n+\t\t    (UNSPEC_SQSHRUN \"\") (UNSPEC_SQRSHRUN \"r\")\n+                    (UNSPEC_SQSHRN \"\")  (UNSPEC_UQSHRN \"\")\n+                    (UNSPEC_SQRSHRN \"r\") (UNSPEC_UQRSHRN \"r\")\n+                    (UNSPEC_SQSHL   \"\")  (UNSPEC_UQSHL  \"\")\n+                    (UNSPEC_SQRSHL   \"r\")(UNSPEC_UQRSHL  \"r\")\n+])\n+\n+(define_int_attr lr [(UNSPEC_SSLI  \"l\") (UNSPEC_USLI  \"l\")\n+\t\t     (UNSPEC_SSRI  \"r\") (UNSPEC_USRI  \"r\")])\n+\n+(define_int_attr u [(UNSPEC_SQSHLU \"u\") (UNSPEC_SQSHL \"\") (UNSPEC_UQSHL \"\")\n+\t\t    (UNSPEC_SQSHRUN \"u\") (UNSPEC_SQRSHRUN \"u\")\n+                    (UNSPEC_SQSHRN \"\")  (UNSPEC_UQSHRN \"\")\n+                    (UNSPEC_SQRSHRN \"\") (UNSPEC_UQRSHRN \"\")])\n+\n+(define_int_attr addsub [(UNSPEC_SHADD \"add\")\n+\t\t\t (UNSPEC_UHADD \"add\")\n+\t\t\t (UNSPEC_SRHADD \"add\")\n+\t\t\t (UNSPEC_URHADD \"add\")\n+\t\t\t (UNSPEC_SHSUB \"sub\")\n+\t\t\t (UNSPEC_UHSUB \"sub\")\n+\t\t\t (UNSPEC_SRHSUB \"sub\")\n+\t\t\t (UNSPEC_URHSUB \"sub\")\n+\t\t\t (UNSPEC_ADDHN \"add\")\n+\t\t\t (UNSPEC_SUBHN \"sub\")\n+\t\t\t (UNSPEC_RADDHN \"add\")\n+\t\t\t (UNSPEC_RSUBHN \"sub\")\n+\t\t\t (UNSPEC_ADDHN2 \"add\")\n+\t\t\t (UNSPEC_SUBHN2 \"sub\")\n+\t\t\t (UNSPEC_RADDHN2 \"add\")\n+\t\t\t (UNSPEC_RSUBHN2 \"sub\")])\n+\n+(define_int_attr cmp [(UNSPEC_CMGE \"ge\") (UNSPEC_CMGT \"gt\")\n+\t\t      (UNSPEC_CMLE \"le\") (UNSPEC_CMLT \"lt\")\n+                      (UNSPEC_CMEQ \"eq\")\n+\t\t      (UNSPEC_CMHS \"hs\") (UNSPEC_CMHI \"hi\")\n+\t\t      (UNSPEC_CMTST \"tst\")])\n+\n+(define_int_attr offsetlr [(UNSPEC_SSLI\t\"1\") (UNSPEC_USLI \"1\")\n+\t\t\t   (UNSPEC_SSRI\t\"0\") (UNSPEC_USRI \"0\")])\n+"}, {"sha": "1e73dc31a0ff42482098c8d7eb9e227f855dea28", "filename": "gcc/config/aarch64/large.md", "status": "added", "additions": 312, "deletions": 0, "changes": 312, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Flarge.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Flarge.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Flarge.md?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,312 @@\n+;; Copyright (C) 2012 Free Software Foundation, Inc.\n+;;\n+;; Contributed by ARM Ltd.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+;; In the absence of any ARMv8-A implementations, two examples derived\n+;; from ARM's most recent ARMv7-A cores (Cortex-A7 and Cortex-A15) are\n+;; included by way of example.  This is a temporary measure.\n+\n+;; Example pipeline description for an example 'large' core\n+;; implementing AArch64\n+\n+;;-------------------------------------------------------\n+;; General Description\n+;;-------------------------------------------------------\n+\n+(define_automaton \"large_cpu\")\n+\n+;; The core is modelled as a triple issue pipeline that has\n+;; the following dispatch units.\n+;; 1. Two pipelines for simple integer operations: int1, int2\n+;; 2. Two pipelines for SIMD and FP data-processing operations: fpsimd1, fpsimd2\n+;; 3. One pipeline for branch operations: br\n+;; 4. One pipeline for integer multiply and divide operations: multdiv\n+;; 5. Two pipelines for load and store operations: ls1, ls2\n+;;\n+;; We can issue into three pipelines per-cycle.\n+;;\n+;; We assume that where we have unit pairs xxx1 is always filled before xxx2.\n+\n+;;-------------------------------------------------------\n+;; CPU Units and Reservations\n+;;-------------------------------------------------------\n+\n+;; The three issue units\n+(define_cpu_unit \"large_cpu_unit_i1, large_cpu_unit_i2, large_cpu_unit_i3\" \"large_cpu\")\n+\n+(define_reservation \"large_cpu_resv_i1\"\n+\t\t    \"(large_cpu_unit_i1 | large_cpu_unit_i2 | large_cpu_unit_i3)\")\n+\n+(define_reservation \"large_cpu_resv_i2\"\n+\t\t    \"((large_cpu_unit_i1 + large_cpu_unit_i2) | (large_cpu_unit_i2 + large_cpu_unit_i3))\")\n+\n+(define_reservation \"large_cpu_resv_i3\"\n+\t\t    \"(large_cpu_unit_i1 + large_cpu_unit_i2 + large_cpu_unit_i3)\")\n+\n+(final_presence_set \"large_cpu_unit_i2\" \"large_cpu_unit_i1\")\n+(final_presence_set \"large_cpu_unit_i3\" \"large_cpu_unit_i2\")\n+\n+;; The main dispatch units\n+(define_cpu_unit \"large_cpu_unit_int1, large_cpu_unit_int2\" \"large_cpu\")\n+(define_cpu_unit \"large_cpu_unit_fpsimd1, large_cpu_unit_fpsimd2\" \"large_cpu\")\n+(define_cpu_unit \"large_cpu_unit_ls1, large_cpu_unit_ls2\" \"large_cpu\")\n+(define_cpu_unit \"large_cpu_unit_br\" \"large_cpu\")\n+(define_cpu_unit \"large_cpu_unit_multdiv\" \"large_cpu\")\n+\n+(define_reservation \"large_cpu_resv_ls\" \"(large_cpu_unit_ls1 | large_cpu_unit_ls2)\")\n+\n+;; The extended load-store pipeline\n+(define_cpu_unit \"large_cpu_unit_load, large_cpu_unit_store\" \"large_cpu\")\n+\n+;; The extended ALU pipeline\n+(define_cpu_unit \"large_cpu_unit_int1_alu, large_cpu_unit_int2_alu\" \"large_cpu\")\n+(define_cpu_unit \"large_cpu_unit_int1_shf, large_cpu_unit_int2_shf\" \"large_cpu\")\n+(define_cpu_unit \"large_cpu_unit_int1_sat, large_cpu_unit_int2_sat\" \"large_cpu\")\n+\n+\n+;;-------------------------------------------------------\n+;; Simple ALU Instructions\n+;;-------------------------------------------------------\n+\n+;; Simple ALU operations without shift\n+(define_insn_reservation \"large_cpu_alu\" 2\n+  (and (eq_attr \"tune\" \"large\") (eq_attr \"v8type\" \"adc,alu,alu_ext\"))\n+  \"large_cpu_resv_i1, \\\n+   (large_cpu_unit_int1, large_cpu_unit_int1_alu) |\\\n+     (large_cpu_unit_int2, large_cpu_unit_int2_alu)\")\n+\n+(define_insn_reservation \"large_cpu_logic\" 2\n+  (and (eq_attr \"tune\" \"large\") (eq_attr \"v8type\" \"logic,logic_imm\"))\n+  \"large_cpu_resv_i1, \\\n+   (large_cpu_unit_int1, large_cpu_unit_int1_alu) |\\\n+     (large_cpu_unit_int2, large_cpu_unit_int2_alu)\")\n+\n+(define_insn_reservation \"large_cpu_shift\" 2\n+  (and (eq_attr \"tune\" \"large\") (eq_attr \"v8type\" \"shift,shift_imm\"))\n+  \"large_cpu_resv_i1, \\\n+   (large_cpu_unit_int1, large_cpu_unit_int1_shf) |\\\n+     (large_cpu_unit_int2, large_cpu_unit_int2_shf)\")\n+\n+;; Simple ALU operations with immediate shift\n+(define_insn_reservation \"large_cpu_alu_shift\" 3\n+  (and (eq_attr \"tune\" \"large\") (eq_attr \"v8type\" \"alu_shift\"))\n+  \"large_cpu_resv_i1, \\\n+   (large_cpu_unit_int1,\n+     large_cpu_unit_int1 + large_cpu_unit_int1_shf, large_cpu_unit_int1_alu) | \\\n+   (large_cpu_unit_int2,\n+     large_cpu_unit_int2 + large_cpu_unit_int2_shf, large_cpu_unit_int2_alu)\")\n+\n+(define_insn_reservation \"large_cpu_logic_shift\" 3\n+  (and (eq_attr \"tune\" \"large\") (eq_attr \"v8type\" \"logic_shift\"))\n+  \"large_cpu_resv_i1, \\\n+   (large_cpu_unit_int1, large_cpu_unit_int1_alu) |\\\n+     (large_cpu_unit_int2, large_cpu_unit_int2_alu)\")\n+\n+\n+;;-------------------------------------------------------\n+;; Multiplication/Division\n+;;-------------------------------------------------------\n+\n+;; Simple multiplication\n+(define_insn_reservation \"large_cpu_mult_single\" 3\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"mult,madd\") (eq_attr \"mode\" \"SI\")))\n+  \"large_cpu_resv_i1, large_cpu_unit_multdiv\")\n+\n+(define_insn_reservation \"large_cpu_mult_double\" 4\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"mult,madd\") (eq_attr \"mode\" \"DI\")))\n+  \"large_cpu_resv_i1, large_cpu_unit_multdiv\")\n+\n+;; 64-bit multiplication\n+(define_insn_reservation \"large_cpu_mull\" 4\n+  (and (eq_attr \"tune\" \"large\") (eq_attr \"v8type\" \"mull,mulh,maddl\"))\n+  \"large_cpu_resv_i1, large_cpu_unit_multdiv * 2\")\n+\n+;; Division\n+(define_insn_reservation \"large_cpu_udiv_single\" 9\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"udiv\") (eq_attr \"mode\" \"SI\")))\n+  \"large_cpu_resv_i1, large_cpu_unit_multdiv\")\n+\n+(define_insn_reservation \"large_cpu_udiv_double\" 18\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"udiv\") (eq_attr \"mode\" \"DI\")))\n+  \"large_cpu_resv_i1, large_cpu_unit_multdiv\")\n+\n+(define_insn_reservation \"large_cpu_sdiv_single\" 10\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"sdiv\") (eq_attr \"mode\" \"SI\")))\n+  \"large_cpu_resv_i1, large_cpu_unit_multdiv\")\n+\n+(define_insn_reservation \"large_cpu_sdiv_double\" 20\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"sdiv\") (eq_attr \"mode\" \"DI\")))\n+  \"large_cpu_resv_i1, large_cpu_unit_multdiv\")\n+\n+\n+;;-------------------------------------------------------\n+;; Branches\n+;;-------------------------------------------------------\n+\n+;; Branches take one issue slot.\n+;; No latency as there is no result\n+(define_insn_reservation \"large_cpu_branch\" 0\n+  (and (eq_attr \"tune\" \"large\") (eq_attr \"v8type\" \"branch\"))\n+  \"large_cpu_resv_i1, large_cpu_unit_br\")\n+\n+\n+;; Calls take up all issue slots, and form a block in the\n+;; pipeline.  The result however is available the next cycle.\n+;; Addition of new units requires this to be updated.\n+(define_insn_reservation \"large_cpu_call\" 1\n+  (and (eq_attr \"tune\" \"large\") (eq_attr \"v8type\" \"call\"))\n+  \"large_cpu_resv_i3 | large_cpu_resv_i2, \\\n+   large_cpu_unit_int1 + large_cpu_unit_int2 + large_cpu_unit_br + \\\n+     large_cpu_unit_multdiv + large_cpu_unit_fpsimd1 + large_cpu_unit_fpsimd2 + \\\n+     large_cpu_unit_ls1 + large_cpu_unit_ls2,\\\n+   large_cpu_unit_int1_alu + large_cpu_unit_int1_shf + large_cpu_unit_int1_sat + \\\n+     large_cpu_unit_int2_alu + large_cpu_unit_int2_shf + \\\n+     large_cpu_unit_int2_sat + large_cpu_unit_load + large_cpu_unit_store\")\n+\n+\n+;;-------------------------------------------------------\n+;; Load/Store Instructions\n+;;-------------------------------------------------------\n+\n+;; Loads of up to two words.\n+(define_insn_reservation \"large_cpu_load1\" 4\n+  (and (eq_attr \"tune\" \"large\") (eq_attr \"v8type\" \"load_acq,load1,load2\"))\n+  \"large_cpu_resv_i1, large_cpu_resv_ls, large_cpu_unit_load, nothing\")\n+\n+;; Stores of up to two words.\n+(define_insn_reservation \"large_cpu_store1\" 0\n+  (and (eq_attr \"tune\" \"large\") (eq_attr \"v8type\" \"store_rel,store1,store2\"))\n+  \"large_cpu_resv_i1, large_cpu_resv_ls, large_cpu_unit_store\")\n+\n+\n+;;-------------------------------------------------------\n+;; Floating-point arithmetic.\n+;;-------------------------------------------------------\n+\n+(define_insn_reservation \"large_cpu_fpalu\" 4\n+  (and (eq_attr \"tune\" \"large\")\n+       (eq_attr \"v8type\" \"ffarith,fadd,fccmp,fcvt,fcmp\"))\n+  \"large_cpu_resv_i1 + large_cpu_unit_fpsimd1\")\n+\n+(define_insn_reservation \"large_cpu_fconst\" 3\n+  (and (eq_attr \"tune\" \"large\")\n+       (eq_attr \"v8type\" \"fconst\"))\n+  \"large_cpu_resv_i1 + large_cpu_unit_fpsimd1\")\n+\n+(define_insn_reservation \"large_cpu_fpmuls\" 4\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"fmul,fmadd\") (eq_attr \"mode\" \"SF\")))\n+  \"large_cpu_resv_i1 + large_cpu_unit_fpsimd1\")\n+\n+(define_insn_reservation \"large_cpu_fpmuld\" 7\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"fmul,fmadd\") (eq_attr \"mode\" \"DF\")))\n+  \"large_cpu_resv_i1 + large_cpu_unit_fpsimd1, large_cpu_unit_fpsimd1 * 2,\\\n+   large_cpu_resv_i1 + large_cpu_unit_fpsimd1\")\n+\n+\n+;;-------------------------------------------------------\n+;; Floating-point Division\n+;;-------------------------------------------------------\n+\n+;; Single-precision divide takes 14 cycles to complete, and this\n+;; includes the time taken for the special instruction used to collect the\n+;; result to travel down the multiply pipeline.\n+\n+(define_insn_reservation \"large_cpu_fdivs\" 14\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"fdiv,fsqrt\") (eq_attr \"mode\" \"SF\")))\n+  \"large_cpu_resv_i1, large_cpu_unit_fpsimd1 * 13\")\n+\n+(define_insn_reservation \"large_cpu_fdivd\" 29\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"fdiv,fsqrt\") (eq_attr \"mode\" \"DF\")))\n+  \"large_cpu_resv_i1, large_cpu_unit_fpsimd1 * 28\")\n+\n+\n+\n+;;-------------------------------------------------------\n+;; Floating-point Transfers\n+;;-------------------------------------------------------\n+\n+(define_insn_reservation \"large_cpu_i2f\" 4\n+  (and (eq_attr \"tune\" \"large\")\n+       (eq_attr \"v8type\" \"fmovi2f\"))\n+  \"large_cpu_resv_i1\")\n+\n+(define_insn_reservation \"large_cpu_f2i\" 2\n+  (and (eq_attr \"tune\" \"large\")\n+       (eq_attr \"v8type\" \"fmovf2i\"))\n+  \"large_cpu_resv_i1\")\n+\n+\n+;;-------------------------------------------------------\n+;; Floating-point Load/Store\n+;;-------------------------------------------------------\n+\n+(define_insn_reservation \"large_cpu_floads\" 4\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"fpsimd_load,fpsimd_load2\") (eq_attr \"mode\" \"SF\")))\n+  \"large_cpu_resv_i1\")\n+\n+(define_insn_reservation \"large_cpu_floadd\" 5\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"fpsimd_load,fpsimd_load2\") (eq_attr \"mode\" \"DF\")))\n+  \"large_cpu_resv_i1 + large_cpu_unit_br, large_cpu_resv_i1\")\n+\n+(define_insn_reservation \"large_cpu_fstores\" 0\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"fpsimd_store,fpsimd_store2\") (eq_attr \"mode\" \"SF\")))\n+  \"large_cpu_resv_i1\")\n+\n+(define_insn_reservation \"large_cpu_fstored\" 0\n+  (and (eq_attr \"tune\" \"large\")\n+       (and (eq_attr \"v8type\" \"fpsimd_store,fpsimd_store2\") (eq_attr \"mode\" \"DF\")))\n+  \"large_cpu_resv_i1 + large_cpu_unit_br, large_cpu_resv_i1\")\n+\n+\n+;;-------------------------------------------------------\n+;; Bypasses\n+;;-------------------------------------------------------\n+\n+(define_bypass 1 \"large_cpu_alu, large_cpu_logic, large_cpu_shift\"\n+  \"large_cpu_alu, large_cpu_alu_shift, large_cpu_logic, large_cpu_logic_shift, large_cpu_shift\")\n+\n+(define_bypass 2 \"large_cpu_alu_shift, large_cpu_logic_shift\"\n+  \"large_cpu_alu, large_cpu_alu_shift, large_cpu_logic, large_cpu_logic_shift, large_cpu_shift\")\n+\n+(define_bypass 1 \"large_cpu_alu, large_cpu_logic, large_cpu_shift\" \"large_cpu_load1\")\n+\n+(define_bypass 2 \"large_cpu_alu_shift, large_cpu_logic_shift\" \"large_cpu_load1\")\n+\n+(define_bypass 2 \"large_cpu_floads\"\n+                 \"large_cpu_fpalu, large_cpu_fpmuld,\\\n+\t\t  large_cpu_fdivs, large_cpu_fdivd,\\\n+\t\t  large_cpu_f2i\")\n+\n+(define_bypass 3 \"large_cpu_floadd\"\n+                 \"large_cpu_fpalu, large_cpu_fpmuld,\\\n+\t\t  large_cpu_fdivs, large_cpu_fdivd,\\\n+\t\t  large_cpu_f2i\")"}, {"sha": "6f79039647c24285b106e45a9716d7d4fd68afd5", "filename": "gcc/config/aarch64/predicates.md", "status": "added", "additions": 297, "deletions": 0, "changes": 297, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fpredicates.md?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,297 @@\n+;; Machine description for AArch64 architecture.\n+;; Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+;; Contributed by ARM Ltd.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+(define_special_predicate \"cc_register\"\n+  (and (match_code \"reg\")\n+       (and (match_test \"REGNO (op) == CC_REGNUM\")\n+\t    (ior (match_test \"mode == GET_MODE (op)\")\n+\t\t (match_test \"mode == VOIDmode\n+\t\t\t      && GET_MODE_CLASS (GET_MODE (op)) == MODE_CC\"))))\n+)\n+\n+(define_predicate \"aarch64_reg_or_zero\"\n+  (and (match_code \"reg,subreg,const_int\")\n+       (ior (match_operand 0 \"register_operand\")\n+\t    (match_test \"op == const0_rtx\"))))\n+\n+(define_predicate \"aarch64_reg_zero_or_m1\"\n+  (and (match_code \"reg,subreg,const_int\")\n+       (ior (match_operand 0 \"register_operand\")\n+\t    (ior (match_test \"op == const0_rtx\")\n+\t\t (match_test \"op == constm1_rtx\")))))\n+\n+(define_predicate \"aarch64_fp_compare_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (and (match_code \"const_double\")\n+\t    (match_test \"aarch64_const_double_zero_rtx_p (op)\"))))\n+\n+(define_predicate \"aarch64_plus_immediate\"\n+  (and (match_code \"const_int\")\n+       (ior (match_test \"aarch64_uimm12_shift (INTVAL (op))\")\n+\t    (match_test \"aarch64_uimm12_shift (-INTVAL (op))\"))))\n+\n+(define_predicate \"aarch64_plus_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_plus_immediate\")))\n+\n+(define_predicate \"aarch64_pluslong_immediate\"\n+  (and (match_code \"const_int\")\n+       (match_test \"(INTVAL (op) < 0xffffff && INTVAL (op) > -0xffffff)\")))\n+\n+(define_predicate \"aarch64_pluslong_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_pluslong_immediate\")))\n+\n+(define_predicate \"aarch64_logical_immediate\"\n+  (and (match_code \"const_int\")\n+       (match_test \"aarch64_bitmask_imm (INTVAL (op), mode)\")))\n+\n+(define_predicate \"aarch64_logical_operand\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_logical_immediate\")))\n+\n+(define_predicate \"aarch64_shift_imm_si\"\n+  (and (match_code \"const_int\")\n+       (match_test \"(unsigned HOST_WIDE_INT) INTVAL (op) < 32\")))\n+\n+(define_predicate \"aarch64_shift_imm_di\"\n+  (and (match_code \"const_int\")\n+       (match_test \"(unsigned HOST_WIDE_INT) INTVAL (op) < 64\")))\n+\n+(define_predicate \"aarch64_reg_or_shift_imm_si\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_shift_imm_si\")))\n+\n+(define_predicate \"aarch64_reg_or_shift_imm_di\"\n+  (ior (match_operand 0 \"register_operand\")\n+       (match_operand 0 \"aarch64_shift_imm_di\")))\n+\n+;; The imm3 field is a 3-bit field that only accepts immediates in the\n+;; range 0..4.\n+(define_predicate \"aarch64_imm3\"\n+  (and (match_code \"const_int\")\n+       (match_test \"(unsigned HOST_WIDE_INT) INTVAL (op) <= 4\")))\n+\n+(define_predicate \"aarch64_pwr_imm3\"\n+  (and (match_code \"const_int\")\n+       (match_test \"INTVAL (op) != 0\n+\t\t    && (unsigned) exact_log2 (INTVAL (op)) <= 4\")))\n+\n+(define_predicate \"aarch64_pwr_2_si\"\n+  (and (match_code \"const_int\")\n+       (match_test \"INTVAL (op) != 0\n+\t\t    && (unsigned) exact_log2 (INTVAL (op)) < 32\")))\n+\n+(define_predicate \"aarch64_pwr_2_di\"\n+  (and (match_code \"const_int\")\n+       (match_test \"INTVAL (op) != 0\n+\t\t    && (unsigned) exact_log2 (INTVAL (op)) < 64\")))\n+\n+(define_predicate \"aarch64_mem_pair_operand\"\n+  (and (match_code \"mem\")\n+       (match_test \"aarch64_legitimate_address_p (mode, XEXP (op, 0), PARALLEL,\n+\t\t\t\t\t       0)\")))\n+\n+(define_predicate \"aarch64_const_address\"\n+  (and (match_code \"symbol_ref\")\n+       (match_test \"mode == DImode && CONSTANT_ADDRESS_P (op)\")))\n+\n+(define_predicate \"aarch64_valid_symref\"\n+  (match_code \"const, symbol_ref, label_ref\")\n+{\n+  enum aarch64_symbol_type symbol_type;\n+  return (aarch64_symbolic_constant_p (op, SYMBOL_CONTEXT_ADR, &symbol_type)\n+\t && symbol_type != SYMBOL_FORCE_TO_MEM);\n+})\n+\n+(define_predicate \"aarch64_tls_ie_symref\"\n+  (match_code \"const, symbol_ref, label_ref\")\n+{\n+  switch (GET_CODE (op))\n+    {\n+    case CONST:\n+      op = XEXP (op, 0);\n+      if (GET_CODE (op) != PLUS\n+\t  || GET_CODE (XEXP (op, 0)) != SYMBOL_REF\n+\t  || GET_CODE (XEXP (op, 1)) != CONST_INT)\n+\treturn false;\n+      op = XEXP (op, 0);\n+\n+    case SYMBOL_REF:\n+      return SYMBOL_REF_TLS_MODEL (op) == TLS_MODEL_INITIAL_EXEC;\n+\n+    default:\n+      gcc_unreachable ();\n+    }\n+})\n+\n+(define_predicate \"aarch64_tls_le_symref\"\n+  (match_code \"const, symbol_ref, label_ref\")\n+{\n+  switch (GET_CODE (op))\n+    {\n+    case CONST:\n+      op = XEXP (op, 0);\n+      if (GET_CODE (op) != PLUS\n+\t  || GET_CODE (XEXP (op, 0)) != SYMBOL_REF\n+\t  || GET_CODE (XEXP (op, 1)) != CONST_INT)\n+\treturn false;\n+      op = XEXP (op, 0);\n+\n+    case SYMBOL_REF:\n+      return SYMBOL_REF_TLS_MODEL (op) == TLS_MODEL_LOCAL_EXEC;\n+\n+    default:\n+      gcc_unreachable ();\n+    }\n+})\n+\n+(define_predicate \"aarch64_mov_operand\"\n+  (and (match_code \"reg,subreg,mem,const_int,symbol_ref,high\")\n+       (ior (match_operand 0 \"register_operand\")\n+\t    (ior (match_operand 0 \"memory_operand\")\n+\t\t (ior (match_test \"GET_CODE (op) == HIGH\n+\t\t\t\t   && aarch64_valid_symref (XEXP (op, 0),\n+\t\t\t\t\t\t\t    GET_MODE (XEXP (op, 0)))\")\n+\t\t      (ior (match_test \"CONST_INT_P (op)\n+\t\t\t\t\t&& aarch64_move_imm (INTVAL (op), mode)\")\n+\t\t\t   (match_test \"aarch64_const_address (op, mode)\")))))))\n+\n+(define_predicate \"aarch64_movti_operand\"\n+  (and (match_code \"reg,subreg,mem,const_int\")\n+       (ior (match_operand 0 \"register_operand\")\n+\t    (ior (match_operand 0 \"memory_operand\")\n+\t\t (match_operand 0 \"const_int_operand\")))))\n+\n+(define_predicate \"aarch64_reg_or_imm\"\n+  (and (match_code \"reg,subreg,const_int\")\n+       (ior (match_operand 0 \"register_operand\")\n+\t    (match_operand 0 \"const_int_operand\"))))\n+\n+;; True for integer comparisons and for FP comparisons other than LTGT or UNEQ.\n+(define_special_predicate \"aarch64_comparison_operator\"\n+  (match_code \"eq,ne,le,lt,ge,gt,geu,gtu,leu,ltu,unordered,ordered,unlt,unle,unge,ungt\"))\n+\n+;; True if the operand is memory reference suitable for a load/store exclusive.\n+(define_predicate \"aarch64_sync_memory_operand\"\n+  (and (match_operand 0 \"memory_operand\")\n+       (match_code \"reg\" \"0\")))\n+\n+;; Predicates for parallel expanders based on mode.\n+(define_special_predicate \"vect_par_cnst_hi_half\"\n+  (match_code \"parallel\")\n+{\n+  HOST_WIDE_INT count = XVECLEN (op, 0);\n+  int nunits = GET_MODE_NUNITS (mode);\n+  int i;\n+\n+  if (count < 1\n+      || count != nunits / 2)\n+    return false;\n+ \n+  if (!VECTOR_MODE_P (mode))\n+    return false;\n+\n+  for (i = 0; i < count; i++)\n+   {\n+     rtx elt = XVECEXP (op, 0, i);\n+     int val;\n+\n+     if (GET_CODE (elt) != CONST_INT)\n+       return false;\n+\n+     val = INTVAL (elt);\n+     if (val != (nunits / 2) + i)\n+       return false;\n+   }\n+  return true;\n+})\n+\n+(define_special_predicate \"vect_par_cnst_lo_half\"\n+  (match_code \"parallel\")\n+{\n+  HOST_WIDE_INT count = XVECLEN (op, 0);\n+  int nunits = GET_MODE_NUNITS (mode);\n+  int i;\n+\n+  if (count < 1\n+      || count != nunits / 2)\n+    return false;\n+\n+  if (!VECTOR_MODE_P (mode))\n+    return false;\n+\n+  for (i = 0; i < count; i++)\n+   {\n+     rtx elt = XVECEXP (op, 0, i);\n+     int val;\n+\n+     if (GET_CODE (elt) != CONST_INT)\n+       return false;\n+\n+     val = INTVAL (elt);\n+     if (val != i)\n+       return false;\n+   }\n+  return true;\n+})\n+\n+\n+(define_special_predicate \"aarch64_simd_lshift_imm\"\n+  (match_code \"const_vector\")\n+{\n+  return aarch64_simd_shift_imm_p (op, mode, true);\n+})\n+\n+(define_special_predicate \"aarch64_simd_rshift_imm\"\n+  (match_code \"const_vector\")\n+{\n+  return aarch64_simd_shift_imm_p (op, mode, false);\n+})\n+\n+(define_predicate \"aarch64_simd_reg_or_zero\"\n+  (and (match_code \"reg,subreg,const_int,const_vector\")\n+       (ior (match_operand 0 \"register_operand\")\n+           (ior (match_test \"op == const0_rtx\")\n+                (match_test \"aarch64_simd_imm_zero_p (op, mode)\")))))\n+\n+(define_predicate \"aarch64_simd_struct_operand\"\n+  (and (match_code \"mem\")\n+       (match_test \"TARGET_SIMD && aarch64_simd_mem_operand_p (op)\")))\n+\n+;; Like general_operand but allow only valid SIMD addressing modes.\n+(define_predicate \"aarch64_simd_general_operand\"\n+  (and (match_operand 0 \"general_operand\")\n+       (match_test \"!MEM_P (op)\n+\t\t    || GET_CODE (XEXP (op, 0)) == POST_INC\n+\t\t    || GET_CODE (XEXP (op, 0)) == REG\")))\n+\n+;; Like nonimmediate_operand but allow only valid SIMD addressing modes.\n+(define_predicate \"aarch64_simd_nonimmediate_operand\"\n+  (and (match_operand 0 \"nonimmediate_operand\")\n+       (match_test \"!MEM_P (op)\n+\t\t    || GET_CODE (XEXP (op, 0)) == POST_INC\n+\t\t    || GET_CODE (XEXP (op, 0)) == REG\")))\n+\n+(define_special_predicate \"aarch64_simd_imm_zero\"\n+  (match_code \"const_vector\")\n+{\n+  return aarch64_simd_imm_zero_p (op, mode);\n+})"}, {"sha": "8f70ca9c02c31b04d577ffcec87fa47f59e4e1bc", "filename": "gcc/config/aarch64/small.md", "status": "added", "additions": 287, "deletions": 0, "changes": 287, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fsmall.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fsmall.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fsmall.md?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,287 @@\n+;; Copyright (C) 2012 Free Software Foundation, Inc.\n+;;\n+;; Contributed by ARM Ltd.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+;; In the absence of any ARMv8-A implementations, two examples derived\n+;; from ARM's most recent ARMv7-A cores (Cortex-A7 and Cortex-A15) are\n+;; included by way of example.  This is a temporary measure.\n+\n+;; Example pipeline description for an example 'small' core\n+;; implementing AArch64\n+\n+;;-------------------------------------------------------\n+;; General Description\n+;;-------------------------------------------------------\n+\n+(define_automaton \"small_cpu\")\n+\n+;; The core is modelled as a single issue pipeline with the following\n+;; dispatch units.\n+;; 1. One pipeline for simple intructions.\n+;; 2. One pipeline for branch intructions.\n+;;\n+;; There are five pipeline stages.\n+;; The decode/issue stages operate the same for all instructions.\n+;; Instructions always advance one stage per cycle in order.\n+;; Only branch instructions may dual-issue with other instructions, except\n+;; when those instructions take multiple cycles to issue.\n+\n+\n+;;-------------------------------------------------------\n+;; CPU Units and Reservations\n+;;-------------------------------------------------------\n+\n+(define_cpu_unit \"small_cpu_unit_i\" \"small_cpu\")\n+(define_cpu_unit \"small_cpu_unit_br\" \"small_cpu\")\n+\n+;; Pseudo-unit for blocking the multiply pipeline when a double-precision\n+;; multiply is in progress.\n+(define_cpu_unit \"small_cpu_unit_fpmul_pipe\" \"small_cpu\")\n+\n+;; The floating-point add pipeline, used to model the usage\n+;; of the add pipeline by fp alu instructions.\n+(define_cpu_unit \"small_cpu_unit_fpadd_pipe\" \"small_cpu\")\n+\n+;; Floating-point division pipeline (long latency, out-of-order completion).\n+(define_cpu_unit \"small_cpu_unit_fpdiv\" \"small_cpu\")\n+\n+\n+;;-------------------------------------------------------\n+;; Simple ALU Instructions\n+;;-------------------------------------------------------\n+\n+;; Simple ALU operations without shift\n+(define_insn_reservation \"small_cpu_alu\" 2\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"adc,alu,alu_ext\"))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_logic\" 2\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"logic,logic_imm\"))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_shift\" 2\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"shift,shift_imm\"))\n+  \"small_cpu_unit_i\")\n+\n+;; Simple ALU operations with immediate shift\n+(define_insn_reservation \"small_cpu_alu_shift\" 2\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"alu_shift\"))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_logic_shift\" 2\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"logic_shift\"))\n+  \"small_cpu_unit_i\")\n+\n+\n+;;-------------------------------------------------------\n+;; Multiplication/Division\n+;;-------------------------------------------------------\n+\n+;; Simple multiplication\n+(define_insn_reservation \"small_cpu_mult_single\" 2\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"mult,madd\") (eq_attr \"mode\" \"SI\")))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_mult_double\" 3\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"mult,madd\") (eq_attr \"mode\" \"DI\")))\n+  \"small_cpu_unit_i\")\n+\n+;; 64-bit multiplication\n+(define_insn_reservation \"small_cpu_mull\" 3\n+  (and (eq_attr \"tune\" \"small\") (eq_attr \"v8type\" \"mull,mulh,maddl\"))\n+  \"small_cpu_unit_i * 2\")\n+\n+;; Division\n+(define_insn_reservation \"small_cpu_udiv_single\" 5\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"udiv\") (eq_attr \"mode\" \"SI\")))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_udiv_double\" 10\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"udiv\") (eq_attr \"mode\" \"DI\")))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_sdiv_single\" 6\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"sdiv\") (eq_attr \"mode\" \"SI\")))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_sdiv_double\" 12\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"sdiv\") (eq_attr \"mode\" \"DI\")))\n+  \"small_cpu_unit_i\")\n+\n+\n+;;-------------------------------------------------------\n+;; Load/Store Instructions\n+;;-------------------------------------------------------\n+\n+(define_insn_reservation \"small_cpu_load1\" 2\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"load_acq,load1\"))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_store1\" 0\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"store_rel,store1\"))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_load2\" 3\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"load2\"))\n+  \"small_cpu_unit_i + small_cpu_unit_br, small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_store2\" 0\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"store2\"))\n+  \"small_cpu_unit_i + small_cpu_unit_br, small_cpu_unit_i\")\n+\n+\n+;;-------------------------------------------------------\n+;; Branches\n+;;-------------------------------------------------------\n+\n+;; Direct branches are the only instructions that can dual-issue.\n+;; The latency here represents when the branch actually takes place.\n+\n+(define_insn_reservation \"small_cpu_unit_br\" 3\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"branch,call\"))\n+  \"small_cpu_unit_br\")\n+\n+\n+;;-------------------------------------------------------\n+;; Floating-point arithmetic.\n+;;-------------------------------------------------------\n+\n+(define_insn_reservation \"small_cpu_fpalu\" 4\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"ffarith,fadd,fccmp,fcvt,fcmp\"))\n+  \"small_cpu_unit_i + small_cpu_unit_fpadd_pipe\")\n+\n+(define_insn_reservation \"small_cpu_fconst\" 3\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"fconst\"))\n+  \"small_cpu_unit_i + small_cpu_unit_fpadd_pipe\")\n+\n+(define_insn_reservation \"small_cpu_fpmuls\" 4\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"fmul\") (eq_attr \"mode\" \"SF\")))\n+  \"small_cpu_unit_i + small_cpu_unit_fpmul_pipe\")\n+\n+(define_insn_reservation \"small_cpu_fpmuld\" 7\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"fmul\") (eq_attr \"mode\" \"DF\")))\n+  \"small_cpu_unit_i + small_cpu_unit_fpmul_pipe, small_cpu_unit_fpmul_pipe * 2,\\\n+   small_cpu_unit_i + small_cpu_unit_fpmul_pipe\")\n+\n+\n+;;-------------------------------------------------------\n+;; Floating-point Division\n+;;-------------------------------------------------------\n+\n+;; Single-precision divide takes 14 cycles to complete, and this\n+;; includes the time taken for the special instruction used to collect the\n+;; result to travel down the multiply pipeline.\n+\n+(define_insn_reservation \"small_cpu_fdivs\" 14\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"fdiv,fsqrt\") (eq_attr \"mode\" \"SF\")))\n+  \"small_cpu_unit_i, small_cpu_unit_fpdiv * 13\")\n+\n+(define_insn_reservation \"small_cpu_fdivd\" 29\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"fdiv,fsqrt\") (eq_attr \"mode\" \"DF\")))\n+  \"small_cpu_unit_i, small_cpu_unit_fpdiv * 28\")\n+\n+\n+;;-------------------------------------------------------\n+;; Floating-point Transfers\n+;;-------------------------------------------------------\n+\n+(define_insn_reservation \"small_cpu_i2f\" 4\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"fmovi2f\"))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_f2i\" 2\n+  (and (eq_attr \"tune\" \"small\")\n+       (eq_attr \"v8type\" \"fmovf2i\"))\n+  \"small_cpu_unit_i\")\n+\n+\n+;;-------------------------------------------------------\n+;; Floating-point Load/Store\n+;;-------------------------------------------------------\n+\n+(define_insn_reservation \"small_cpu_floads\" 4\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"fpsimd_load\") (eq_attr \"mode\" \"SF\")))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_floadd\" 5\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"fpsimd_load\") (eq_attr \"mode\" \"DF\")))\n+  \"small_cpu_unit_i + small_cpu_unit_br, small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_fstores\" 0\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"fpsimd_store\") (eq_attr \"mode\" \"SF\")))\n+  \"small_cpu_unit_i\")\n+\n+(define_insn_reservation \"small_cpu_fstored\" 0\n+  (and (eq_attr \"tune\" \"small\")\n+       (and (eq_attr \"v8type\" \"fpsimd_store\") (eq_attr \"mode\" \"DF\")))\n+  \"small_cpu_unit_i + small_cpu_unit_br, small_cpu_unit_i\")\n+\n+\n+;;-------------------------------------------------------\n+;; Bypasses\n+;;-------------------------------------------------------\n+\n+;; Forwarding path for unshifted operands.\n+\n+(define_bypass 1 \"small_cpu_alu, small_cpu_alu_shift\" \n+  \"small_cpu_alu, small_cpu_alu_shift, small_cpu_logic, small_cpu_logic_shift, small_cpu_shift\")\n+\n+(define_bypass 1 \"small_cpu_logic, small_cpu_logic_shift\" \n+  \"small_cpu_alu, small_cpu_alu_shift, small_cpu_logic, small_cpu_logic_shift, small_cpu_shift\")\n+\n+(define_bypass 1 \"small_cpu_shift\" \n+  \"small_cpu_alu, small_cpu_alu_shift, small_cpu_logic, small_cpu_logic_shift, small_cpu_shift\")\n+\n+;; Load-to-use for floating-point values has a penalty of one cycle.\n+\n+(define_bypass 2 \"small_cpu_floads\"\n+                 \"small_cpu_fpalu, small_cpu_fpmuld,\\\n+\t\t  small_cpu_fdivs, small_cpu_fdivd,\\\n+\t\t  small_cpu_f2i\")\n+\n+(define_bypass 3 \"small_cpu_floadd\"\n+                 \"small_cpu_fpalu, small_cpu_fpmuld,\\\n+\t\t  small_cpu_fdivs, small_cpu_fdivd,\\\n+\t\t  small_cpu_f2i\")"}, {"sha": "61f1f1b78b8e08065026386f222ff4710f634fbb", "filename": "gcc/config/aarch64/sync.md", "status": "added", "additions": 467, "deletions": 0, "changes": 467, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fsync.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Fsync.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fsync.md?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,467 @@\n+;; Machine description for AArch64 processor synchronization primitives.\n+;; Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+;; Contributed by ARM Ltd.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+(define_c_enum \"unspecv\"\n+ [\n+    UNSPECV_SYNC_COMPARE_AND_SWAP       ; Represent a sync_compare_and_swap.\n+    UNSPECV_SYNC_LOCK\t\t\t; Represent a sync_lock_test_and_set.\n+    UNSPECV_SYNC_LOCK_RELEASE\t\t; Represent a sync_lock_release.\n+    UNSPECV_SYNC_OP\t\t\t; Represent a sync_<op>\n+    UNSPECV_SYNC_NEW_OP\t\t\t; Represent a sync_new_<op>\n+    UNSPECV_SYNC_OLD_OP\t\t\t; Represent a sync_old_<op>\n+])\n+\n+(define_expand \"sync_compare_and_swap<mode>\"\n+  [(set (match_operand:ALLI 0 \"register_operand\")\n+        (unspec_volatile:ALLI [(match_operand:ALLI 1 \"memory_operand\")\n+\t\t\t       (match_operand:ALLI 2 \"register_operand\")\n+\t\t\t       (match_operand:ALLI 3 \"register_operand\")]\n+\t\t\t       UNSPECV_SYNC_COMPARE_AND_SWAP))]\n+  \"\"\n+  {\n+    struct aarch64_sync_generator generator;\n+    generator.op = aarch64_sync_generator_omrn;\n+    generator.u.omrn = gen_aarch64_sync_compare_and_swap<mode>;\n+    aarch64_expand_sync (<MODE>mode, &generator, operands[0], operands[1],\n+    \t\t\t operands[2], operands[3]);\n+    DONE;\n+  })\n+\n+(define_expand \"sync_lock_test_and_set<mode>\"\n+  [(match_operand:ALLI 0 \"register_operand\")\n+   (match_operand:ALLI 1 \"memory_operand\")\n+   (match_operand:ALLI 2 \"register_operand\")]\n+  \"\"\n+  {\n+    struct aarch64_sync_generator generator;\n+    generator.op = aarch64_sync_generator_omn;\n+    generator.u.omn = gen_aarch64_sync_lock_test_and_set<mode>;\n+    aarch64_expand_sync (<MODE>mode, &generator, operands[0], operands[1],\n+                         NULL, operands[2]);\n+    DONE;\n+  })\n+\n+(define_expand \"sync_<optab><mode>\"\n+  [(match_operand:ALLI 0 \"memory_operand\")\n+   (match_operand:ALLI 1 \"register_operand\")\n+   (syncop:ALLI (match_dup 0) (match_dup 1))]\n+  \"\"\n+  {\n+    struct aarch64_sync_generator generator;\n+    generator.op = aarch64_sync_generator_omn;\n+    generator.u.omn = gen_aarch64_sync_new_<optab><mode>;\n+    aarch64_expand_sync (<MODE>mode, &generator, NULL, operands[0], NULL,\n+                         operands[1]);\n+    DONE;\n+  })\n+\n+(define_expand \"sync_nand<mode>\"\n+  [(match_operand:ALLI 0 \"memory_operand\")\n+   (match_operand:ALLI 1 \"register_operand\")\n+   (not:ALLI (and:ALLI (match_dup 0) (match_dup 1)))]\n+  \"\"\n+  {\n+    struct aarch64_sync_generator generator;\n+    generator.op = aarch64_sync_generator_omn;\n+    generator.u.omn = gen_aarch64_sync_new_nand<mode>;\n+    aarch64_expand_sync (<MODE>mode, &generator, NULL, operands[0], NULL,\n+                         operands[1]);\n+    DONE;\n+  })\n+\n+(define_expand \"sync_new_<optab><mode>\"\n+  [(match_operand:ALLI 0 \"register_operand\")\n+   (match_operand:ALLI 1 \"memory_operand\")\n+   (match_operand:ALLI 2 \"register_operand\")\n+   (syncop:ALLI (match_dup 1) (match_dup 2))]\n+  \"\"\n+  {\n+    struct aarch64_sync_generator generator;\n+    generator.op = aarch64_sync_generator_omn;\n+    generator.u.omn = gen_aarch64_sync_new_<optab><mode>;\n+    aarch64_expand_sync (<MODE>mode, &generator, operands[0], operands[1],\n+    \t\t    \t NULL, operands[2]);\n+    DONE;\n+  })\n+\n+(define_expand \"sync_new_nand<mode>\"\n+  [(match_operand:ALLI 0 \"register_operand\")\n+   (match_operand:ALLI 1 \"memory_operand\")\n+   (match_operand:ALLI 2 \"register_operand\")\n+   (not:ALLI (and:ALLI (match_dup 1) (match_dup 2)))]\n+  \"\"\n+  {\n+    struct aarch64_sync_generator generator;\n+    generator.op = aarch64_sync_generator_omn;\n+    generator.u.omn = gen_aarch64_sync_new_nand<mode>;\n+    aarch64_expand_sync (<MODE>mode, &generator, operands[0], operands[1],\n+    \t\t\t NULL, operands[2]);\n+    DONE;\n+  });\n+\n+(define_expand \"sync_old_<optab><mode>\"\n+  [(match_operand:ALLI 0 \"register_operand\")\n+   (match_operand:ALLI 1 \"memory_operand\")\n+   (match_operand:ALLI 2 \"register_operand\")\n+   (syncop:ALLI (match_dup 1) (match_dup 2))]\n+  \"\"\n+  {\n+    struct aarch64_sync_generator generator;\n+    generator.op = aarch64_sync_generator_omn;\n+    generator.u.omn = gen_aarch64_sync_old_<optab><mode>;\n+    aarch64_expand_sync (<MODE>mode, &generator, operands[0], operands[1],\n+    \t\t         NULL, operands[2]);\n+    DONE;\n+  })\n+\n+(define_expand \"sync_old_nand<mode>\"\n+  [(match_operand:ALLI 0 \"register_operand\")\n+   (match_operand:ALLI 1 \"memory_operand\")\n+   (match_operand:ALLI 2 \"register_operand\")\n+   (not:ALLI (and:ALLI (match_dup 1) (match_dup 2)))]\n+  \"\"\n+  {\n+    struct aarch64_sync_generator generator;\n+    generator.op = aarch64_sync_generator_omn;\n+    generator.u.omn = gen_aarch64_sync_old_nand<mode>;\n+    aarch64_expand_sync (<MODE>mode, &generator, operands[0], operands[1],\n+                         NULL, operands[2]);\n+    DONE;\n+  })\n+\n+(define_expand \"memory_barrier\"\n+  [(set (match_dup 0) (unspec:BLK [(match_dup 0)] UNSPEC_MB))]\n+  \"\"\n+{\n+  operands[0] = gen_rtx_MEM (BLKmode, gen_rtx_SCRATCH (Pmode));\n+  MEM_VOLATILE_P (operands[0]) = 1;\n+})\n+\n+(define_insn \"aarch64_sync_compare_and_swap<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=&r\")\n+        (unspec_volatile:GPI\n+\t  [(match_operand:GPI 1 \"aarch64_sync_memory_operand\" \"+Q\")\n+   \t   (match_operand:GPI 2 \"register_operand\" \"r\")\n+\t   (match_operand:GPI 3 \"register_operand\" \"r\")]\n+\t  UNSPECV_SYNC_COMPARE_AND_SWAP))\n+   (set (match_dup 1) (unspec_volatile:GPI [(match_dup 2)]\n+                                          UNSPECV_SYNC_COMPARE_AND_SWAP))\n+   (clobber:GPI (match_scratch:GPI 4 \"=&r\"))\n+   (set (reg:CC CC_REGNUM) (unspec_volatile:CC [(match_dup 1)]\n+                                                UNSPECV_SYNC_COMPARE_AND_SWAP))\n+   ]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_required_value\"  \"2\")\n+   (set_attr \"sync_new_value\"       \"3\")\n+   (set_attr \"sync_t1\"              \"0\")\n+   (set_attr \"sync_t2\"              \"4\")\n+   ])\n+\n+(define_insn \"aarch64_sync_compare_and_swap<mode>\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=&r\")\n+        (zero_extend:SI\n+\t  (unspec_volatile:SHORT\n+\t    [(match_operand:SHORT 1 \"aarch64_sync_memory_operand\" \"+Q\")\n+   \t     (match_operand:SI 2 \"register_operand\" \"r\")\n+\t     (match_operand:SI 3 \"register_operand\" \"r\")]\n+\t    UNSPECV_SYNC_COMPARE_AND_SWAP)))\n+   (set (match_dup 1) (unspec_volatile:SHORT [(match_dup 2)]\n+                                             UNSPECV_SYNC_COMPARE_AND_SWAP))\n+   (clobber:SI (match_scratch:SI 4 \"=&r\"))\n+   (set (reg:CC CC_REGNUM) (unspec_volatile:CC [(match_dup 1)]\n+                                                UNSPECV_SYNC_COMPARE_AND_SWAP))\n+   ]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_required_value\"  \"2\")\n+   (set_attr \"sync_new_value\"       \"3\")\n+   (set_attr \"sync_t1\"              \"0\")\n+   (set_attr \"sync_t2\"              \"4\")\n+   ])\n+\n+(define_insn \"aarch64_sync_lock_test_and_set<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=&r\")\n+        (match_operand:GPI 1 \"aarch64_sync_memory_operand\" \"+Q\"))\n+   (set (match_dup 1)\n+        (unspec_volatile:GPI [(match_operand:GPI 2 \"register_operand\" \"r\")]\n+\t                     UNSPECV_SYNC_LOCK))\n+   (clobber (reg:CC CC_REGNUM))\n+   (clobber (match_scratch:GPI 3 \"=&r\"))]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_release_barrier\" \"no\")\n+   (set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_new_value\"       \"2\")\n+   (set_attr \"sync_t1\"              \"0\")\n+   (set_attr \"sync_t2\"              \"3\")\n+   ])\n+\n+(define_insn \"aarch64_sync_lock_test_and_set<mode>\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=&r\")\n+        (zero_extend:SI (match_operand:SHORT 1\n+\t                  \"aarch64_sync_memory_operand\" \"+Q\")))\n+   (set (match_dup 1)\n+        (unspec_volatile:SHORT [(match_operand:SI 2 \"register_operand\" \"r\")]\n+                               UNSPECV_SYNC_LOCK))\n+   (clobber (reg:CC CC_REGNUM))\n+   (clobber (match_scratch:SI 3 \"=&r\"))]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_release_barrier\" \"no\")\n+   (set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_new_value\"       \"2\")\n+   (set_attr \"sync_t1\"              \"0\")\n+   (set_attr \"sync_t2\"              \"3\")\n+   ])\n+\n+(define_insn \"aarch64_sync_new_<optab><mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=&r\")\n+        (unspec_volatile:GPI\n+\t  [(syncop:GPI\n+\t     (match_operand:GPI 1 \"aarch64_sync_memory_operand\" \"+Q\")\n+             (match_operand:GPI 2 \"register_operand\" \"r\"))]\n+           UNSPECV_SYNC_NEW_OP))\n+   (set (match_dup 1)\n+        (unspec_volatile:GPI [(match_dup 1) (match_dup 2)]\n+\t                    UNSPECV_SYNC_NEW_OP))\n+   (clobber (reg:CC CC_REGNUM))\n+   (clobber (match_scratch:GPI 3 \"=&r\"))]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_new_value\"       \"2\")\n+   (set_attr \"sync_t1\"              \"0\")\n+   (set_attr \"sync_t2\"              \"3\")\n+   (set_attr \"sync_op\"              \"<optab>\")\n+   ])\n+\n+(define_insn \"aarch64_sync_new_nand<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=&r\")\n+        (unspec_volatile:GPI\n+\t  [(not:GPI (and:GPI\n+                     (match_operand:GPI 1 \"aarch64_sync_memory_operand\" \"+Q\")\n+                     (match_operand:GPI 2 \"register_operand\" \"r\")))]\n+          UNSPECV_SYNC_NEW_OP))\n+   (set (match_dup 1)\n+        (unspec_volatile:GPI [(match_dup 1) (match_dup 2)]\n+\t                    UNSPECV_SYNC_NEW_OP))\n+   (clobber (reg:CC CC_REGNUM))\n+   (clobber (match_scratch:GPI 3 \"=&r\"))]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_new_value\"       \"2\")\n+   (set_attr \"sync_t1\"              \"0\")\n+   (set_attr \"sync_t2\"              \"3\")\n+   (set_attr \"sync_op\"              \"nand\")\n+   ])\n+\n+(define_insn \"aarch64_sync_new_<optab><mode>\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=&r\")\n+        (unspec_volatile:SI\n+\t  [(syncop:SI\n+             (zero_extend:SI\n+\t       (match_operand:SHORT 1 \"aarch64_sync_memory_operand\" \"+Q\"))\n+               (match_operand:SI 2 \"register_operand\" \"r\"))]\n+          UNSPECV_SYNC_NEW_OP))\n+   (set (match_dup 1)\n+        (unspec_volatile:SHORT [(match_dup 1) (match_dup 2)]\n+\t                       UNSPECV_SYNC_NEW_OP))\n+   (clobber (reg:CC CC_REGNUM))\n+   (clobber (match_scratch:SI 3 \"=&r\"))]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_new_value\"       \"2\")\n+   (set_attr \"sync_t1\"              \"0\")\n+   (set_attr \"sync_t2\"              \"3\")\n+   (set_attr \"sync_op\"              \"<optab>\")\n+   ])\n+\n+(define_insn \"aarch64_sync_new_nand<mode>\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=&r\")\n+        (unspec_volatile:SI\n+\t  [(not:SI\n+\t     (and:SI\n+               (zero_extend:SI\n+\t         (match_operand:SHORT 1 \"aarch64_sync_memory_operand\" \"+Q\"))\n+               (match_operand:SI 2 \"register_operand\" \"r\")))\n+\t  ] UNSPECV_SYNC_NEW_OP))\n+   (set (match_dup 1)\n+        (unspec_volatile:SHORT [(match_dup 1) (match_dup 2)]\n+\t                       UNSPECV_SYNC_NEW_OP))\n+   (clobber (reg:CC CC_REGNUM))\n+   (clobber (match_scratch:SI 3 \"=&r\"))]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_new_value\"       \"2\")\n+   (set_attr \"sync_t1\"              \"0\")\n+   (set_attr \"sync_t2\"              \"3\")\n+   (set_attr \"sync_op\"              \"nand\")\n+   ])\n+\n+(define_insn \"aarch64_sync_old_<optab><mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=&r\")\n+        (unspec_volatile:GPI\n+          [(syncop:GPI\n+             (match_operand:GPI 1 \"aarch64_sync_memory_operand\" \"+Q\")\n+             (match_operand:GPI 2 \"register_operand\" \"r\"))]\n+          UNSPECV_SYNC_OLD_OP))\n+   (set (match_dup 1)\n+        (unspec_volatile:GPI [(match_dup 1) (match_dup 2)]\n+\t                     UNSPECV_SYNC_OLD_OP))\n+   (clobber (reg:CC CC_REGNUM))\n+   (clobber (match_scratch:GPI 3 \"=&r\"))\n+   (clobber (match_scratch:GPI 4 \"=&r\"))]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_new_value\"       \"2\")\n+   (set_attr \"sync_t1\"              \"3\")\n+   (set_attr \"sync_t2\"              \"4\")\n+   (set_attr \"sync_op\"              \"<optab>\")\n+   ])\n+\n+(define_insn \"aarch64_sync_old_nand<mode>\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=&r\")\n+        (unspec_volatile:GPI\n+\t  [(not:GPI (and:GPI\n+                     (match_operand:GPI 1 \"aarch64_sync_memory_operand\" \"+Q\")\n+                     (match_operand:GPI 2 \"register_operand\" \"r\")))]\n+          UNSPECV_SYNC_OLD_OP))\n+   (set (match_dup 1)\n+        (unspec_volatile:GPI [(match_dup 1) (match_dup 2)]\n+\t                     UNSPECV_SYNC_OLD_OP))\n+   (clobber (reg:CC CC_REGNUM))\n+   (clobber (match_scratch:GPI 3 \"=&r\"))\n+   (clobber (match_scratch:GPI 4 \"=&r\"))]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_new_value\"       \"2\")\n+   (set_attr \"sync_t1\"              \"3\")\n+   (set_attr \"sync_t2\"              \"4\")\n+   (set_attr \"sync_op\"              \"nand\")\n+   ])\n+\n+(define_insn \"aarch64_sync_old_<optab><mode>\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=&r\")\n+        (unspec_volatile:SI\n+\t  [(syncop:SI\n+             (zero_extend:SI\n+\t       (match_operand:SHORT 1 \"aarch64_sync_memory_operand\" \"+Q\"))\n+               (match_operand:SI 2 \"register_operand\" \"r\"))]\n+           UNSPECV_SYNC_OLD_OP))\n+   (set (match_dup 1)\n+        (unspec_volatile:SHORT [(match_dup 1) (match_dup 2)]\n+\t                       UNSPECV_SYNC_OLD_OP))\n+   (clobber (reg:CC CC_REGNUM))\n+   (clobber (match_scratch:SI 3 \"=&r\"))\n+   (clobber (match_scratch:SI 4 \"=&r\"))]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_new_value\"       \"2\")\n+   (set_attr \"sync_t1\"              \"3\")\n+   (set_attr \"sync_t2\"              \"4\")\n+   (set_attr \"sync_op\"              \"<optab>\")\n+   ])\n+\n+(define_insn \"aarch64_sync_old_nand<mode>\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=&r\")\n+        (unspec_volatile:SI\n+\t  [(not:SI\n+\t     (and:SI\n+               (zero_extend:SI\n+\t\t (match_operand:SHORT 1 \"aarch64_sync_memory_operand\" \"+Q\"))\n+                 (match_operand:SI 2 \"register_operand\" \"r\")))]\n+          UNSPECV_SYNC_OLD_OP))\n+   (set (match_dup 1)\n+        (unspec_volatile:SHORT [(match_dup 1) (match_dup 2)]\n+\t                       UNSPECV_SYNC_OLD_OP))\n+   (clobber (reg:CC CC_REGNUM))\n+   (clobber (match_scratch:SI 3 \"=&r\"))\n+   (clobber (match_scratch:SI 4 \"=&r\"))]\n+  \"\"\n+  {\n+    return aarch64_output_sync_insn (insn, operands);\n+  }\n+  [(set_attr \"sync_result\"          \"0\")\n+   (set_attr \"sync_memory\"          \"1\")\n+   (set_attr \"sync_new_value\"       \"2\")\n+   (set_attr \"sync_t1\"              \"3\")\n+   (set_attr \"sync_t2\"              \"4\")\n+   (set_attr \"sync_op\"              \"nand\")\n+   ])\n+\n+(define_insn \"*memory_barrier\"\n+  [(set (match_operand:BLK 0 \"\" \"\")\n+\t(unspec:BLK [(match_dup 0)] UNSPEC_MB))]\n+  \"\"\n+  \"dmb\\\\tish\"\n+)\n+\n+(define_insn \"sync_lock_release<mode>\"\n+  [(set (match_operand:ALLI 0 \"memory_operand\" \"+Q\")\n+  \t(unspec_volatile:ALLI [(match_operand:ALLI 1 \"register_operand\" \"r\")]\n+\t                      UNSPECV_SYNC_LOCK_RELEASE))]\n+\n+  \"\"\n+  {\n+    return aarch64_output_sync_lock_release (operands[1], operands[0]);\n+  })\n+"}, {"sha": "715ad1da2c09d321fffe2491a1f672d2a988dce3", "filename": "gcc/config/aarch64/t-aarch64", "status": "added", "additions": 32, "deletions": 0, "changes": 32, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Ft-aarch64", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Ft-aarch64", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Ft-aarch64?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,32 @@\n+# Machine description for AArch64 architecture.\n+#  Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+#  Contributed by ARM Ltd.\n+#\n+#  This file is part of GCC.\n+#\n+#  GCC is free software; you can redistribute it and/or modify it\n+#  under the terms of the GNU General Public License as published by\n+#  the Free Software Foundation; either version 3, or (at your option)\n+#  any later version.\n+#\n+#  GCC is distributed in the hope that it will be useful, but\n+#  WITHOUT ANY WARRANTY; without even the implied warranty of\n+#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+#  General Public License for more details.\n+#\n+#  You should have received a copy of the GNU General Public License\n+#  along with GCC; see the file COPYING3.  If not see\n+#  <http://www.gnu.org/licenses/>.\n+\n+$(srcdir)/config/aarch64/aarch64-tune.md: $(srcdir)/config/aarch64/gentune.sh \\\n+\t$(srcdir)/config/aarch64/aarch64-cores.def\n+\t$(SHELL) $(srcdir)/config/aarch64/gentune.sh \\\n+\t\t$(srcdir)/config/aarch64/aarch64-cores.def > \\\n+\t\t$(srcdir)/config/aarch64/aarch64-tune.md\n+\n+aarch64-builtins.o: $(srcdir)/config/aarch64/aarch64-builtins.c $(CONFIG_H) \\\n+  $(SYSTEM_H) coretypes.h $(TM_H) \\\n+  $(RTL_H) $(TREE_H) expr.h $(TM_P_H) $(RECOG_H) langhooks.h \\\n+  $(DIAGNOSTIC_CORE_H) $(OPTABS_H)\n+\t$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \\\n+\t\t$(srcdir)/config/aarch64/aarch64-builtins.c"}, {"sha": "f6ec5765f7c1831577c05ed2eb182bac8104f4a8", "filename": "gcc/config/aarch64/t-aarch64-linux", "status": "added", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Ft-aarch64-linux", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/43e9d192f17550753d62debb7f1d8bc9ecab2360/gcc%2Fconfig%2Faarch64%2Ft-aarch64-linux", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Ft-aarch64-linux?ref=43e9d192f17550753d62debb7f1d8bc9ecab2360", "patch": "@@ -0,0 +1,22 @@\n+# Machine description for AArch64 architecture.\n+#  Copyright (C) 2009, 2010, 2011, 2012 Free Software Foundation, Inc.\n+#  Contributed by ARM Ltd.\n+#\n+#  This file is part of GCC.\n+#\n+#  GCC is free software; you can redistribute it and/or modify it\n+#  under the terms of the GNU General Public License as published by\n+#  the Free Software Foundation; either version 3, or (at your option)\n+#  any later version.\n+#\n+#  GCC is distributed in the hope that it will be useful, but\n+#  WITHOUT ANY WARRANTY; without even the implied warranty of\n+#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+#  General Public License for more details.\n+#\n+#  You should have received a copy of the GNU General Public License\n+#  along with GCC; see the file COPYING3.  If not see\n+#  <http://www.gnu.org/licenses/>.\n+\n+LIB1ASMSRC   = aarch64/lib1funcs.asm\n+LIB1ASMFUNCS = _aarch64_sync_cache_range"}]}