{"sha": "f30e25a336a39a990c82f8e3cdb507c73e38e84b", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZjMwZTI1YTMzNmEzOWE5OTBjODJmOGUzY2RiNTA3YzczZTM4ZTg0Yg==", "commit": {"author": {"name": "Zhenqiang Chen", "email": "zhenqiang.chen@linaro.org", "date": "2014-05-13T07:05:46Z"}, "committer": {"name": "Zhenqiang Chen", "email": "zqchen@gcc.gnu.org", "date": "2014-05-13T07:05:46Z"}, "message": "Makefile.in: add shrink-wrap.o.\n\n2014-05-13  Zhenqiang Chen  <zhenqiang.chen@linaro.org>\n\n\t* Makefile.in: add shrink-wrap.o.\n\t* config/i386/i386.c: include \"shrink-wrap.h\"\n\t* function.c: Likewise.\n\t(requires_stack_frame_p, next_block_for_reg,\n\tmove_insn_for_shrink_wrap, prepare_shrink_wrap,\n\tdup_block_and_redirect): Move to shrink-wrap.c\n\t(thread_prologue_and_epilogue_insns): Extract three code segments\n\tas functions in shrink-wrap.c\n\t* function.h: Move #ifdef HAVE_simple_return ... #endif block to\n\tshrink-wrap.h\n\t* shrink-wrap.c: New file.\n\t* shrink-wrap.h: New file.\n\nFrom-SVN: r210351", "tree": {"sha": "44f546db36bc06184820b9e3e50274a291e03143", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/44f546db36bc06184820b9e3e50274a291e03143"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f30e25a336a39a990c82f8e3cdb507c73e38e84b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f30e25a336a39a990c82f8e3cdb507c73e38e84b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f30e25a336a39a990c82f8e3cdb507c73e38e84b", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f30e25a336a39a990c82f8e3cdb507c73e38e84b/comments", "author": null, "committer": null, "parents": [{"sha": "3f55e16a298b37e2a482d58ba35687ca07f7824a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3f55e16a298b37e2a482d58ba35687ca07f7824a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3f55e16a298b37e2a482d58ba35687ca07f7824a"}], "stats": {"total": 1756, "additions": 957, "deletions": 799}, "files": [{"sha": "be9e41138e3ee306322a401983240eabc74116ea", "filename": "gcc/ChangeLog", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=f30e25a336a39a990c82f8e3cdb507c73e38e84b", "patch": "@@ -1,3 +1,18 @@\n+2014-05-13  Zhenqiang Chen  <zhenqiang.chen@linaro.org>\n+\n+\t* Makefile.in: add shrink-wrap.o.\n+\t* config/i386/i386.c: include \"shrink-wrap.h\"\n+\t* function.c: Likewise.\n+\t(requires_stack_frame_p, next_block_for_reg,\n+\tmove_insn_for_shrink_wrap, prepare_shrink_wrap,\n+\tdup_block_and_redirect): Move to shrink-wrap.c\n+\t(thread_prologue_and_epilogue_insns): Extract three code segments\n+\tas functions in shrink-wrap.c\n+\t* function.h: Move #ifdef HAVE_simple_return ... #endif block to\n+\tshrink-wrap.h\n+\t* shrink-wrap.c: New file.\n+\t* shrink-wrap.h: New file.\n+\n 2014-05-12  David Wohlferd  <dw@LimeGreenSocks.com>\n \n \t* doc/extend.texi: Reflect current numbers of pragmas.  Remove"}, {"sha": "04c046c5a4462486455d81732981d65d4e14d13c", "filename": "gcc/Makefile.in", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=f30e25a336a39a990c82f8e3cdb507c73e38e84b", "patch": "@@ -1353,6 +1353,7 @@ OBJS = \\\n \tsel-sched-dump.o \\\n \tsel-sched.o \\\n \tsese.o \\\n+\tshrink-wrap.o \\\n \tsimplify-rtx.o \\\n \tsparseset.o \\\n \tsreal.o \\"}, {"sha": "b85015f04f74d7aaf3131c7cac7ed6816b1a007a", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=f30e25a336a39a990c82f8e3cdb507c73e38e84b", "patch": "@@ -83,6 +83,7 @@ along with GCC; see the file COPYING3.  If not see\n #include \"pass_manager.h\"\n #include \"target-globals.h\"\n #include \"tree-vectorizer.h\"\n+#include \"shrink-wrap.h\"\n \n static rtx legitimize_dllimport_symbol (rtx, bool);\n static rtx legitimize_pe_coff_extern_decl (rtx, bool);"}, {"sha": "9be76a9ce15832572b799b3985f1840a9aa53624", "filename": "gcc/function.c", "status": "modified", "additions": 12, "deletions": 795, "changes": 807, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2Ffunction.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2Ffunction.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffunction.c?ref=f30e25a336a39a990c82f8e3cdb507c73e38e84b", "patch": "@@ -63,6 +63,7 @@ along with GCC; see the file COPYING3.  If not see\n #include \"df.h\"\n #include \"params.h\"\n #include \"bb-reorder.h\"\n+#include \"shrink-wrap.h\"\n \n /* So we can assign to cfun in this file.  */\n #undef cfun\n@@ -5321,265 +5322,6 @@ prologue_epilogue_contains (const_rtx insn)\n   return 0;\n }\n \n-#ifdef HAVE_simple_return\n-\n-/* Return true if INSN requires the stack frame to be set up.\n-   PROLOGUE_USED contains the hard registers used in the function\n-   prologue.  SET_UP_BY_PROLOGUE is the set of registers we expect the\n-   prologue to set up for the function.  */\n-bool\n-requires_stack_frame_p (rtx insn, HARD_REG_SET prologue_used,\n-\t\t\tHARD_REG_SET set_up_by_prologue)\n-{\n-  df_ref *df_rec;\n-  HARD_REG_SET hardregs;\n-  unsigned regno;\n-\n-  if (CALL_P (insn))\n-    return !SIBLING_CALL_P (insn);\n-\n-  /* We need a frame to get the unique CFA expected by the unwinder.  */\n-  if (cfun->can_throw_non_call_exceptions && can_throw_internal (insn))\n-    return true;\n-\n-  CLEAR_HARD_REG_SET (hardregs);\n-  for (df_rec = DF_INSN_DEFS (insn); *df_rec; df_rec++)\n-    {\n-      rtx dreg = DF_REF_REG (*df_rec);\n-\n-      if (!REG_P (dreg))\n-\tcontinue;\n-\n-      add_to_hard_reg_set (&hardregs, GET_MODE (dreg),\n-\t\t\t   REGNO (dreg));\n-    }\n-  if (hard_reg_set_intersect_p (hardregs, prologue_used))\n-    return true;\n-  AND_COMPL_HARD_REG_SET (hardregs, call_used_reg_set);\n-  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n-    if (TEST_HARD_REG_BIT (hardregs, regno)\n-\t&& df_regs_ever_live_p (regno))\n-      return true;\n-\n-  for (df_rec = DF_INSN_USES (insn); *df_rec; df_rec++)\n-    {\n-      rtx reg = DF_REF_REG (*df_rec);\n-\n-      if (!REG_P (reg))\n-\tcontinue;\n-\n-      add_to_hard_reg_set (&hardregs, GET_MODE (reg),\n-\t\t\t   REGNO (reg));\n-    }\n-  if (hard_reg_set_intersect_p (hardregs, set_up_by_prologue))\n-    return true;\n-\n-  return false;\n-}\n-\n-/* See whether BB has a single successor that uses [REGNO, END_REGNO),\n-   and if BB is its only predecessor.  Return that block if so,\n-   otherwise return null.  */\n-\n-static basic_block\n-next_block_for_reg (basic_block bb, int regno, int end_regno)\n-{\n-  edge e, live_edge;\n-  edge_iterator ei;\n-  bitmap live;\n-  int i;\n-\n-  live_edge = NULL;\n-  FOR_EACH_EDGE (e, ei, bb->succs)\n-    {\n-      live = df_get_live_in (e->dest);\n-      for (i = regno; i < end_regno; i++)\n-\tif (REGNO_REG_SET_P (live, i))\n-\t  {\n-\t    if (live_edge && live_edge != e)\n-\t      return NULL;\n-\t    live_edge = e;\n-\t  }\n-    }\n-\n-  /* We can sometimes encounter dead code.  Don't try to move it\n-     into the exit block.  */\n-  if (!live_edge || live_edge->dest == EXIT_BLOCK_PTR_FOR_FN (cfun))\n-    return NULL;\n-\n-  /* Reject targets of abnormal edges.  This is needed for correctness\n-     on ports like Alpha and MIPS, whose pic_offset_table_rtx can die on\n-     exception edges even though it is generally treated as call-saved\n-     for the majority of the compilation.  Moving across abnormal edges\n-     isn't going to be interesting for shrink-wrap usage anyway.  */\n-  if (live_edge->flags & EDGE_ABNORMAL)\n-    return NULL;\n-\n-  if (EDGE_COUNT (live_edge->dest->preds) > 1)\n-    return NULL;\n-\n-  return live_edge->dest;\n-}\n-\n-/* Try to move INSN from BB to a successor.  Return true on success.\n-   USES and DEFS are the set of registers that are used and defined\n-   after INSN in BB.  */\n-\n-static bool\n-move_insn_for_shrink_wrap (basic_block bb, rtx insn,\n-\t\t\t   const HARD_REG_SET uses,\n-\t\t\t   const HARD_REG_SET defs)\n-{\n-  rtx set, src, dest;\n-  bitmap live_out, live_in, bb_uses, bb_defs;\n-  unsigned int i, dregno, end_dregno, sregno, end_sregno;\n-  basic_block next_block;\n-\n-  /* Look for a simple register copy.  */\n-  set = single_set (insn);\n-  if (!set)\n-    return false;\n-  src = SET_SRC (set);\n-  dest = SET_DEST (set);\n-  if (!REG_P (dest) || !REG_P (src))\n-    return false;\n-\n-  /* Make sure that the source register isn't defined later in BB.  */\n-  sregno = REGNO (src);\n-  end_sregno = END_REGNO (src);\n-  if (overlaps_hard_reg_set_p (defs, GET_MODE (src), sregno))\n-    return false;\n-\n-  /* Make sure that the destination register isn't referenced later in BB.  */\n-  dregno = REGNO (dest);\n-  end_dregno = END_REGNO (dest);\n-  if (overlaps_hard_reg_set_p (uses, GET_MODE (dest), dregno)\n-      || overlaps_hard_reg_set_p (defs, GET_MODE (dest), dregno))\n-    return false;\n-\n-  /* See whether there is a successor block to which we could move INSN.  */\n-  next_block = next_block_for_reg (bb, dregno, end_dregno);\n-  if (!next_block)\n-    return false;\n-\n-  /* At this point we are committed to moving INSN, but let's try to\n-     move it as far as we can.  */\n-  do\n-    {\n-      live_out = df_get_live_out (bb);\n-      live_in = df_get_live_in (next_block);\n-      bb = next_block;\n-\n-      /* Check whether BB uses DEST or clobbers DEST.  We need to add\n-\t INSN to BB if so.  Either way, DEST is no longer live on entry,\n-\t except for any part that overlaps SRC (next loop).  */\n-      bb_uses = &DF_LR_BB_INFO (bb)->use;\n-      bb_defs = &DF_LR_BB_INFO (bb)->def;\n-      if (df_live)\n-\t{\n-\t  for (i = dregno; i < end_dregno; i++)\n-\t    {\n-\t      if (REGNO_REG_SET_P (bb_uses, i) || REGNO_REG_SET_P (bb_defs, i)\n-\t\t  || REGNO_REG_SET_P (&DF_LIVE_BB_INFO (bb)->gen, i))\n-\t\tnext_block = NULL;\n-\t      CLEAR_REGNO_REG_SET (live_out, i);\n-\t      CLEAR_REGNO_REG_SET (live_in, i);\n-\t    }\n-\n-\t  /* Check whether BB clobbers SRC.  We need to add INSN to BB if so.\n-\t     Either way, SRC is now live on entry.  */\n-\t  for (i = sregno; i < end_sregno; i++)\n-\t    {\n-\t      if (REGNO_REG_SET_P (bb_defs, i)\n-\t\t  || REGNO_REG_SET_P (&DF_LIVE_BB_INFO (bb)->gen, i))\n-\t\tnext_block = NULL;\n-\t      SET_REGNO_REG_SET (live_out, i);\n-\t      SET_REGNO_REG_SET (live_in, i);\n-\t    }\n-\t}\n-      else\n-\t{\n-\t  /* DF_LR_BB_INFO (bb)->def does not comprise the DF_REF_PARTIAL and\n-\t     DF_REF_CONDITIONAL defs.  So if DF_LIVE doesn't exist, i.e.\n-\t     at -O1, just give up searching NEXT_BLOCK.  */\n-\t  next_block = NULL;\n-\t  for (i = dregno; i < end_dregno; i++)\n-\t    {\n-\t      CLEAR_REGNO_REG_SET (live_out, i);\n-\t      CLEAR_REGNO_REG_SET (live_in, i);\n-\t    }\n-\n-\t  for (i = sregno; i < end_sregno; i++)\n-\t    {\n-\t      SET_REGNO_REG_SET (live_out, i);\n-\t      SET_REGNO_REG_SET (live_in, i);\n-\t    }\n-\t}\n-\n-      /* If we don't need to add the move to BB, look for a single\n-\t successor block.  */\n-      if (next_block)\n-\tnext_block = next_block_for_reg (next_block, dregno, end_dregno);\n-    }\n-  while (next_block);\n-\n-  /* BB now defines DEST.  It only uses the parts of DEST that overlap SRC\n-     (next loop).  */\n-  for (i = dregno; i < end_dregno; i++)\n-    {\n-      CLEAR_REGNO_REG_SET (bb_uses, i);\n-      SET_REGNO_REG_SET (bb_defs, i);\n-    }\n-\n-  /* BB now uses SRC.  */\n-  for (i = sregno; i < end_sregno; i++)\n-    SET_REGNO_REG_SET (bb_uses, i);\n-\n-  emit_insn_after (PATTERN (insn), bb_note (bb));\n-  delete_insn (insn);\n-  return true;\n-}\n-\n-/* Look for register copies in the first block of the function, and move\n-   them down into successor blocks if the register is used only on one\n-   path.  This exposes more opportunities for shrink-wrapping.  These\n-   kinds of sets often occur when incoming argument registers are moved\n-   to call-saved registers because their values are live across one or\n-   more calls during the function.  */\n-\n-static void\n-prepare_shrink_wrap (basic_block entry_block)\n-{\n-  rtx insn, curr, x;\n-  HARD_REG_SET uses, defs;\n-  df_ref *ref;\n-\n-  CLEAR_HARD_REG_SET (uses);\n-  CLEAR_HARD_REG_SET (defs);\n-  FOR_BB_INSNS_REVERSE_SAFE (entry_block, insn, curr)\n-    if (NONDEBUG_INSN_P (insn)\n-\t&& !move_insn_for_shrink_wrap (entry_block, insn, uses, defs))\n-      {\n-\t/* Add all defined registers to DEFs.  */\n-\tfor (ref = DF_INSN_DEFS (insn); *ref; ref++)\n-\t  {\n-\t    x = DF_REF_REG (*ref);\n-\t    if (REG_P (x) && HARD_REGISTER_P (x))\n-\t      SET_HARD_REG_BIT (defs, REGNO (x));\n-\t  }\n-\n-\t/* Add all used registers to USESs.  */\n-\tfor (ref = DF_INSN_USES (insn); *ref; ref++)\n-\t  {\n-\t    x = DF_REF_REG (*ref);\n-\t    if (REG_P (x) && HARD_REGISTER_P (x))\n-\t      SET_HARD_REG_BIT (uses, REGNO (x));\n-\t  }\n-      }\n-}\n-\n-#endif\n-\n #ifdef HAVE_return\n /* Insert use of return register before the end of BB.  */\n \n@@ -5618,7 +5360,7 @@ gen_return_pattern (bool simple_p)\n    also means updating block_for_insn appropriately.  SIMPLE_P is\n    the same as in gen_return_pattern and passed to it.  */\n \n-static void\n+void\n emit_return_into_block (bool simple_p, basic_block bb)\n {\n   rtx jump, pat;\n@@ -5645,61 +5387,9 @@ set_return_jump_label (rtx returnjump)\n     JUMP_LABEL (returnjump) = ret_rtx;\n }\n \n-#ifdef HAVE_simple_return\n-/* Create a copy of BB instructions and insert at BEFORE.  Redirect\n-   preds of BB to COPY_BB if they don't appear in NEED_PROLOGUE.  */\n-static void\n-dup_block_and_redirect (basic_block bb, basic_block copy_bb, rtx before,\n-\t\t\tbitmap_head *need_prologue)\n-{\n-  edge_iterator ei;\n-  edge e;\n-  rtx insn = BB_END (bb);\n-\n-  /* We know BB has a single successor, so there is no need to copy a\n-     simple jump at the end of BB.  */\n-  if (simplejump_p (insn))\n-    insn = PREV_INSN (insn);\n-\n-  start_sequence ();\n-  duplicate_insn_chain (BB_HEAD (bb), insn);\n-  if (dump_file)\n-    {\n-      unsigned count = 0;\n-      for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n-\tif (active_insn_p (insn))\n-\t  ++count;\n-      fprintf (dump_file, \"Duplicating bb %d to bb %d, %u active insns.\\n\",\n-\t       bb->index, copy_bb->index, count);\n-    }\n-  insn = get_insns ();\n-  end_sequence ();\n-  emit_insn_before (insn, before);\n-\n-  /* Redirect all the paths that need no prologue into copy_bb.  */\n-  for (ei = ei_start (bb->preds); (e = ei_safe_edge (ei)); )\n-    if (!bitmap_bit_p (need_prologue, e->src->index))\n-      {\n-\tint freq = EDGE_FREQUENCY (e);\n-\tcopy_bb->count += e->count;\n-\tcopy_bb->frequency += EDGE_FREQUENCY (e);\n-\te->dest->count -= e->count;\n-\tif (e->dest->count < 0)\n-\t  e->dest->count = 0;\n-\te->dest->frequency -= freq;\n-\tif (e->dest->frequency < 0)\n-\t  e->dest->frequency = 0;\n-\tredirect_edge_and_branch_force (e, copy_bb);\n-\tcontinue;\n-      }\n-    else\n-      ei_next (&ei);\n-}\n-#endif\n-\n #if defined (HAVE_return) || defined (HAVE_simple_return)\n /* Return true if there are any active insns between HEAD and TAIL.  */\n-static bool\n+bool\n active_insn_between (rtx head, rtx tail)\n {\n   while (tail)\n@@ -5716,7 +5406,7 @@ active_insn_between (rtx head, rtx tail)\n /* LAST_BB is a block that exits, and empty of active instructions.\n    Examine its predecessors for jumps that can be converted to\n    (conditional) returns.  */\n-static vec<edge> \n+vec<edge>\n convert_jumps_to_returns (basic_block last_bb, bool simple_p,\n \t\t\t  vec<edge> unconverted ATTRIBUTE_UNUSED)\n {\n@@ -5816,7 +5506,7 @@ convert_jumps_to_returns (basic_block last_bb, bool simple_p,\n }\n \n /* Emit a return insn for the exit fallthru block.  */\n-static basic_block\n+basic_block\n emit_return_for_exit (edge exit_fallthru_edge, bool simple_p)\n {\n   basic_block last_bb = exit_fallthru_edge->src;\n@@ -5888,9 +5578,7 @@ thread_prologue_and_epilogue_insns (void)\n   bool inserted;\n #ifdef HAVE_simple_return\n   vec<edge> unconverted_simple_returns = vNULL;\n-  bool nonempty_prologue;\n   bitmap_head bb_flags;\n-  unsigned max_grow_size;\n #endif\n   rtx returnjump;\n   rtx seq ATTRIBUTE_UNUSED, epilogue_end ATTRIBUTE_UNUSED;\n@@ -5970,350 +5658,7 @@ thread_prologue_and_epilogue_insns (void)\n      prologue/epilogue is emitted only around those parts of the\n      function that require it.  */\n \n-  nonempty_prologue = false;\n-  for (seq = prologue_seq; seq; seq = NEXT_INSN (seq))\n-    if (!NOTE_P (seq) || NOTE_KIND (seq) != NOTE_INSN_PROLOGUE_END)\n-      {\n-\tnonempty_prologue = true;\n-\tbreak;\n-      }\n-      \n-  if (flag_shrink_wrap && HAVE_simple_return\n-      && (targetm.profile_before_prologue () || !crtl->profile)\n-      && nonempty_prologue && !crtl->calls_eh_return)\n-    {\n-      HARD_REG_SET prologue_clobbered, prologue_used, live_on_edge;\n-      struct hard_reg_set_container set_up_by_prologue;\n-      rtx p_insn;\n-      vec<basic_block> vec;\n-      basic_block bb;\n-      bitmap_head bb_antic_flags;\n-      bitmap_head bb_on_list;\n-      bitmap_head bb_tail;\n-\n-      if (dump_file)\n-\tfprintf (dump_file, \"Attempting shrink-wrapping optimization.\\n\");\n-\n-      /* Compute the registers set and used in the prologue.  */\n-      CLEAR_HARD_REG_SET (prologue_clobbered);\n-      CLEAR_HARD_REG_SET (prologue_used);\n-      for (p_insn = prologue_seq; p_insn; p_insn = NEXT_INSN (p_insn))\n-\t{\n-\t  HARD_REG_SET this_used;\n-\t  if (!NONDEBUG_INSN_P (p_insn))\n-\t    continue;\n-\n-\t  CLEAR_HARD_REG_SET (this_used);\n-\t  note_uses (&PATTERN (p_insn), record_hard_reg_uses,\n-\t\t     &this_used);\n-\t  AND_COMPL_HARD_REG_SET (this_used, prologue_clobbered);\n-\t  IOR_HARD_REG_SET (prologue_used, this_used);\n-\t  note_stores (PATTERN (p_insn), record_hard_reg_sets,\n-\t\t       &prologue_clobbered);\n-\t}\n-\n-      prepare_shrink_wrap (entry_edge->dest);\n-\n-      bitmap_initialize (&bb_antic_flags, &bitmap_default_obstack);\n-      bitmap_initialize (&bb_on_list, &bitmap_default_obstack);\n-      bitmap_initialize (&bb_tail, &bitmap_default_obstack);\n-\n-      /* Find the set of basic blocks that require a stack frame,\n-\t and blocks that are too big to be duplicated.  */\n-\n-      vec.create (n_basic_blocks_for_fn (cfun));\n-\n-      CLEAR_HARD_REG_SET (set_up_by_prologue.set);\n-      add_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n-\t\t\t   STACK_POINTER_REGNUM);\n-      add_to_hard_reg_set (&set_up_by_prologue.set, Pmode, ARG_POINTER_REGNUM);\n-      if (frame_pointer_needed)\n-\tadd_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n-\t\t\t     HARD_FRAME_POINTER_REGNUM);\n-      if (pic_offset_table_rtx)\n-\tadd_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n-\t\t\t     PIC_OFFSET_TABLE_REGNUM);\n-      if (crtl->drap_reg)\n-\tadd_to_hard_reg_set (&set_up_by_prologue.set,\n-\t\t\t     GET_MODE (crtl->drap_reg),\n-\t\t\t     REGNO (crtl->drap_reg));\n-      if (targetm.set_up_by_prologue)\n-\ttargetm.set_up_by_prologue (&set_up_by_prologue);\n-\n-      /* We don't use a different max size depending on\n-\t optimize_bb_for_speed_p because increasing shrink-wrapping\n-\t opportunities by duplicating tail blocks can actually result\n-\t in an overall decrease in code size.  */\n-      max_grow_size = get_uncond_jump_length ();\n-      max_grow_size *= PARAM_VALUE (PARAM_MAX_GROW_COPY_BB_INSNS);\n-\n-      FOR_EACH_BB_FN (bb, cfun)\n-\t{\n-\t  rtx insn;\n-\t  unsigned size = 0;\n-\n-\t  FOR_BB_INSNS (bb, insn)\n-\t    if (NONDEBUG_INSN_P (insn))\n-\t      {\n-\t\tif (requires_stack_frame_p (insn, prologue_used,\n-\t\t\t\t\t    set_up_by_prologue.set))\n-\t\t  {\n-\t\t    if (bb == entry_edge->dest)\n-\t\t      goto fail_shrinkwrap;\n-\t\t    bitmap_set_bit (&bb_flags, bb->index);\n-\t\t    vec.quick_push (bb);\n-\t\t    break;\n-\t\t  }\n-\t\telse if (size <= max_grow_size)\n-\t\t  {\n-\t\t    size += get_attr_min_length (insn);\n-\t\t    if (size > max_grow_size)\n-\t\t      bitmap_set_bit (&bb_on_list, bb->index);\n-\t\t  }\n-\t      }\n-\t}\n-\n-      /* Blocks that really need a prologue, or are too big for tails.  */\n-      bitmap_ior_into (&bb_on_list, &bb_flags);\n-\n-      /* For every basic block that needs a prologue, mark all blocks\n-\t reachable from it, so as to ensure they are also seen as\n-\t requiring a prologue.  */\n-      while (!vec.is_empty ())\n-\t{\n-\t  basic_block tmp_bb = vec.pop ();\n-\n-\t  FOR_EACH_EDGE (e, ei, tmp_bb->succs)\n-\t    if (e->dest != EXIT_BLOCK_PTR_FOR_FN (cfun)\n-\t\t&& bitmap_set_bit (&bb_flags, e->dest->index))\n-\t      vec.quick_push (e->dest);\n-\t}\n-\n-      /* Find the set of basic blocks that need no prologue, have a\n-\t single successor, can be duplicated, meet a max size\n-\t requirement, and go to the exit via like blocks.  */\n-      vec.quick_push (EXIT_BLOCK_PTR_FOR_FN (cfun));\n-      while (!vec.is_empty ())\n-\t{\n-\t  basic_block tmp_bb = vec.pop ();\n-\n-\t  FOR_EACH_EDGE (e, ei, tmp_bb->preds)\n-\t    if (single_succ_p (e->src)\n-\t\t&& !bitmap_bit_p (&bb_on_list, e->src->index)\n-\t\t&& can_duplicate_block_p (e->src))\n-\t      {\n-\t\tedge pe;\n-\t\tedge_iterator pei;\n-\n-\t\t/* If there is predecessor of e->src which doesn't\n-\t\t   need prologue and the edge is complex,\n-\t\t   we might not be able to redirect the branch\n-\t\t   to a copy of e->src.  */\n-\t\tFOR_EACH_EDGE (pe, pei, e->src->preds)\n-\t\t  if ((pe->flags & EDGE_COMPLEX) != 0\n-\t\t      && !bitmap_bit_p (&bb_flags, pe->src->index))\n-\t\t    break;\n-\t\tif (pe == NULL && bitmap_set_bit (&bb_tail, e->src->index))\n-\t\t  vec.quick_push (e->src);\n-\t      }\n-\t}\n-\n-      /* Now walk backwards from every block that is marked as needing\n-\t a prologue to compute the bb_antic_flags bitmap.  Exclude\n-\t tail blocks; They can be duplicated to be used on paths not\n-\t needing a prologue.  */\n-      bitmap_clear (&bb_on_list);\n-      bitmap_and_compl (&bb_antic_flags, &bb_flags, &bb_tail);\n-      FOR_EACH_BB_FN (bb, cfun)\n-\t{\n-\t  if (!bitmap_bit_p (&bb_antic_flags, bb->index))\n-\t    continue;\n-\t  FOR_EACH_EDGE (e, ei, bb->preds)\n-\t    if (!bitmap_bit_p (&bb_antic_flags, e->src->index)\n-\t\t&& bitmap_set_bit (&bb_on_list, e->src->index))\n-\t      vec.quick_push (e->src);\n-\t}\n-      while (!vec.is_empty ())\n-\t{\n-\t  basic_block tmp_bb = vec.pop ();\n-\t  bool all_set = true;\n-\n-\t  bitmap_clear_bit (&bb_on_list, tmp_bb->index);\n-\t  FOR_EACH_EDGE (e, ei, tmp_bb->succs)\n-\t    if (!bitmap_bit_p (&bb_antic_flags, e->dest->index))\n-\t      {\n-\t\tall_set = false;\n-\t\tbreak;\n-\t      }\n-\n-\t  if (all_set)\n-\t    {\n-\t      bitmap_set_bit (&bb_antic_flags, tmp_bb->index);\n-\t      FOR_EACH_EDGE (e, ei, tmp_bb->preds)\n-\t\tif (!bitmap_bit_p (&bb_antic_flags, e->src->index)\n-\t\t    && bitmap_set_bit (&bb_on_list, e->src->index))\n-\t\t  vec.quick_push (e->src);\n-\t    }\n-\t}\n-      /* Find exactly one edge that leads to a block in ANTIC from\n-\t a block that isn't.  */\n-      if (!bitmap_bit_p (&bb_antic_flags, entry_edge->dest->index))\n-\tFOR_EACH_BB_FN (bb, cfun)\n-\t  {\n-\t    if (!bitmap_bit_p (&bb_antic_flags, bb->index))\n-\t      continue;\n-\t    FOR_EACH_EDGE (e, ei, bb->preds)\n-\t      if (!bitmap_bit_p (&bb_antic_flags, e->src->index))\n-\t\t{\n-\t\t  if (entry_edge != orig_entry_edge)\n-\t\t    {\n-\t\t      entry_edge = orig_entry_edge;\n-\t\t      if (dump_file)\n-\t\t\tfprintf (dump_file, \"More than one candidate edge.\\n\");\n-\t\t      goto fail_shrinkwrap;\n-\t\t    }\n-\t\t  if (dump_file)\n-\t\t    fprintf (dump_file, \"Found candidate edge for \"\n-\t\t\t     \"shrink-wrapping, %d->%d.\\n\", e->src->index,\n-\t\t\t     e->dest->index);\n-\t\t  entry_edge = e;\n-\t\t}\n-\t  }\n-\n-      if (entry_edge != orig_entry_edge)\n-\t{\n-\t  /* Test whether the prologue is known to clobber any register\n-\t     (other than FP or SP) which are live on the edge.  */\n-\t  CLEAR_HARD_REG_BIT (prologue_clobbered, STACK_POINTER_REGNUM);\n-\t  if (frame_pointer_needed)\n-\t    CLEAR_HARD_REG_BIT (prologue_clobbered, HARD_FRAME_POINTER_REGNUM);\n-\t  REG_SET_TO_HARD_REG_SET (live_on_edge,\n-\t\t\t\t   df_get_live_in (entry_edge->dest));\n-\t  if (hard_reg_set_intersect_p (live_on_edge, prologue_clobbered))\n-\t    {\n-\t      entry_edge = orig_entry_edge;\n-\t      if (dump_file)\n-\t\tfprintf (dump_file,\n-\t\t\t \"Shrink-wrapping aborted due to clobber.\\n\");\n-\t    }\n-\t}\n-      if (entry_edge != orig_entry_edge)\n-\t{\n-\t  crtl->shrink_wrapped = true;\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \"Performing shrink-wrapping.\\n\");\n-\n-\t  /* Find tail blocks reachable from both blocks needing a\n-\t     prologue and blocks not needing a prologue.  */\n-\t  if (!bitmap_empty_p (&bb_tail))\n-\t    FOR_EACH_BB_FN (bb, cfun)\n-\t      {\n-\t\tbool some_pro, some_no_pro;\n-\t\tif (!bitmap_bit_p (&bb_tail, bb->index))\n-\t\t  continue;\n-\t\tsome_pro = some_no_pro = false;\n-\t\tFOR_EACH_EDGE (e, ei, bb->preds)\n-\t\t  {\n-\t\t    if (bitmap_bit_p (&bb_flags, e->src->index))\n-\t\t      some_pro = true;\n-\t\t    else\n-\t\t      some_no_pro = true;\n-\t\t  }\n-\t\tif (some_pro && some_no_pro)\n-\t\t  vec.quick_push (bb);\n-\t\telse\n-\t\t  bitmap_clear_bit (&bb_tail, bb->index);\n-\t      }\n-\t  /* Find the head of each tail.  */\n-\t  while (!vec.is_empty ())\n-\t    {\n-\t      basic_block tbb = vec.pop ();\n-\n-\t      if (!bitmap_bit_p (&bb_tail, tbb->index))\n-\t\tcontinue;\n-\n-\t      while (single_succ_p (tbb))\n-\t\t{\n-\t\t  tbb = single_succ (tbb);\n-\t\t  bitmap_clear_bit (&bb_tail, tbb->index);\n-\t\t}\n-\t    }\n-\t  /* Now duplicate the tails.  */\n-\t  if (!bitmap_empty_p (&bb_tail))\n-\t    FOR_EACH_BB_REVERSE_FN (bb, cfun)\n-\t      {\n-\t\tbasic_block copy_bb, tbb;\n-\t\trtx insert_point;\n-\t\tint eflags;\n-\n-\t\tif (!bitmap_clear_bit (&bb_tail, bb->index))\n-\t\t  continue;\n-\n-\t\t/* Create a copy of BB, instructions and all, for\n-\t\t   use on paths that don't need a prologue.\n-\t\t   Ideal placement of the copy is on a fall-thru edge\n-\t\t   or after a block that would jump to the copy.  */ \n-\t\tFOR_EACH_EDGE (e, ei, bb->preds)\n-\t\t  if (!bitmap_bit_p (&bb_flags, e->src->index)\n-\t\t      && single_succ_p (e->src))\n-\t\t    break;\n-\t\tif (e)\n-\t\t  {\n-                    /* Make sure we insert after any barriers.  */\n-                    rtx end = get_last_bb_insn (e->src);\n-                    copy_bb = create_basic_block (NEXT_INSN (end),\n-                                                  NULL_RTX, e->src);\n-\t\t    BB_COPY_PARTITION (copy_bb, e->src);\n-\t\t  }\n-\t\telse\n-\t\t  {\n-\t\t    /* Otherwise put the copy at the end of the function.  */\n-\t\t    copy_bb = create_basic_block (NULL_RTX, NULL_RTX,\n-\t\t\t\t\t\t  EXIT_BLOCK_PTR_FOR_FN (cfun)->prev_bb);\n-\t\t    BB_COPY_PARTITION (copy_bb, bb);\n-\t\t  }\n-\n-\t\tinsert_point = emit_note_after (NOTE_INSN_DELETED,\n-\t\t\t\t\t\tBB_END (copy_bb));\n-\t\temit_barrier_after (BB_END (copy_bb));\n-\n-\t\ttbb = bb;\n-\t\twhile (1)\n-\t\t  {\n-\t\t    dup_block_and_redirect (tbb, copy_bb, insert_point,\n-\t\t\t\t\t    &bb_flags);\n-\t\t    tbb = single_succ (tbb);\n-\t\t    if (tbb == EXIT_BLOCK_PTR_FOR_FN (cfun))\n-\t\t      break;\n-\t\t    e = split_block (copy_bb, PREV_INSN (insert_point));\n-\t\t    copy_bb = e->dest;\n-\t\t  }\n-\n-\t\t/* Quiet verify_flow_info by (ab)using EDGE_FAKE.\n-\t\t   We have yet to add a simple_return to the tails,\n-\t\t   as we'd like to first convert_jumps_to_returns in\n-\t\t   case the block is no longer used after that.  */\n-\t\teflags = EDGE_FAKE;\n-\t\tif (CALL_P (PREV_INSN (insert_point))\n-\t\t    && SIBLING_CALL_P (PREV_INSN (insert_point)))\n-\t\t  eflags = EDGE_SIBCALL | EDGE_ABNORMAL;\n-\t\tmake_single_succ_edge (copy_bb, EXIT_BLOCK_PTR_FOR_FN (cfun),\n-\t\t\t\t       eflags);\n-\n-\t\t/* verify_flow_info doesn't like a note after a\n-\t\t   sibling call.  */\n-\t\tdelete_insn (insert_point);\n-\t\tif (bitmap_empty_p (&bb_tail))\n-\t\t  break;\n-\t      }\n-\t}\n-\n-    fail_shrinkwrap:\n-      bitmap_clear (&bb_tail);\n-      bitmap_clear (&bb_antic_flags);\n-      bitmap_clear (&bb_on_list);\n-      vec.release ();\n-    }\n+  try_shrink_wrapping (&entry_edge, orig_entry_edge, &bb_flags, prologue_seq);\n #endif\n \n   if (split_prologue_seq != NULL_RTX)\n@@ -6339,44 +5684,12 @@ thread_prologue_and_epilogue_insns (void)\n \n   exit_fallthru_edge = find_fallthru_edge (EXIT_BLOCK_PTR_FOR_FN (cfun)->preds);\n \n-  /* If we're allowed to generate a simple return instruction, then by\n-     definition we don't need a full epilogue.  If the last basic\n-     block before the exit block does not contain active instructions,\n-     examine its predecessors and try to emit (conditional) return\n-     instructions.  */\n #ifdef HAVE_simple_return\n   if (entry_edge != orig_entry_edge)\n-    {\n-      if (optimize)\n-\t{\n-\t  unsigned i, last;\n-\n-\t  /* convert_jumps_to_returns may add to preds of the exit block\n-\t     (but won't remove).  Stop at end of current preds.  */\n-\t  last = EDGE_COUNT (EXIT_BLOCK_PTR_FOR_FN (cfun)->preds);\n-\t  for (i = 0; i < last; i++)\n-\t    {\n-\t      e = EDGE_I (EXIT_BLOCK_PTR_FOR_FN (cfun)->preds, i);\n-\t      if (LABEL_P (BB_HEAD (e->src))\n-\t\t  && !bitmap_bit_p (&bb_flags, e->src->index)\n-\t\t  && !active_insn_between (BB_HEAD (e->src), BB_END (e->src)))\n-\t\tunconverted_simple_returns\n-\t\t  = convert_jumps_to_returns (e->src, true,\n-\t\t\t\t\t      unconverted_simple_returns);\n-\t    }\n-\t}\n-\n-      if (exit_fallthru_edge != NULL\n-\t  && EDGE_COUNT (exit_fallthru_edge->src->preds) != 0\n-\t  && !bitmap_bit_p (&bb_flags, exit_fallthru_edge->src->index))\n-\t{\n-\t  basic_block last_bb;\n-\n-\t  last_bb = emit_return_for_exit (exit_fallthru_edge, true);\n-\t  returnjump = BB_END (last_bb);\n-\t  exit_fallthru_edge = NULL;\n-\t}\n-    }\n+    exit_fallthru_edge\n+\t= get_unconverted_simple_return (exit_fallthru_edge, bb_flags,\n+\t\t\t\t\t &unconverted_simple_returns,\n+\t\t\t\t\t &returnjump);\n #endif\n #ifdef HAVE_return\n   if (HAVE_return)\n@@ -6520,104 +5833,8 @@ thread_prologue_and_epilogue_insns (void)\n     }\n \n #ifdef HAVE_simple_return\n-  /* If there were branches to an empty LAST_BB which we tried to\n-     convert to conditional simple_returns, but couldn't for some\n-     reason, create a block to hold a simple_return insn and redirect\n-     those remaining edges.  */\n-  if (!unconverted_simple_returns.is_empty ())\n-    {\n-      basic_block simple_return_block_hot = NULL;\n-      basic_block simple_return_block_cold = NULL;\n-      edge pending_edge_hot = NULL;\n-      edge pending_edge_cold = NULL;\n-      basic_block exit_pred;\n-      int i;\n-\n-      gcc_assert (entry_edge != orig_entry_edge);\n-\n-      /* See if we can reuse the last insn that was emitted for the\n-\t epilogue.  */\n-      if (returnjump != NULL_RTX\n-\t  && JUMP_LABEL (returnjump) == simple_return_rtx)\n-\t{\n-\t  e = split_block (BLOCK_FOR_INSN (returnjump), PREV_INSN (returnjump));\n-\t  if (BB_PARTITION (e->src) == BB_HOT_PARTITION)\n-\t    simple_return_block_hot = e->dest;\n-\t  else\n-\t    simple_return_block_cold = e->dest;\n-\t}\n-\n-      /* Also check returns we might need to add to tail blocks.  */\n-      FOR_EACH_EDGE (e, ei, EXIT_BLOCK_PTR_FOR_FN (cfun)->preds)\n-\tif (EDGE_COUNT (e->src->preds) != 0\n-\t    && (e->flags & EDGE_FAKE) != 0\n-\t    && !bitmap_bit_p (&bb_flags, e->src->index))\n-\t  {\n-\t    if (BB_PARTITION (e->src) == BB_HOT_PARTITION)\n-\t      pending_edge_hot = e;\n-\t    else\n-\t      pending_edge_cold = e;\n-\t  }\n-      \n-      /* Save a pointer to the exit's predecessor BB for use in\n-         inserting new BBs at the end of the function. Do this\n-         after the call to split_block above which may split\n-         the original exit pred.  */\n-      exit_pred = EXIT_BLOCK_PTR_FOR_FN (cfun)->prev_bb;\n-\n-      FOR_EACH_VEC_ELT (unconverted_simple_returns, i, e)\n-\t{\n-\t  basic_block *pdest_bb;\n-\t  edge pending;\n-\n-\t  if (BB_PARTITION (e->src) == BB_HOT_PARTITION)\n-\t    {\n-\t      pdest_bb = &simple_return_block_hot;\n-\t      pending = pending_edge_hot;\n-\t    }\n-\t  else\n-\t    {\n-\t      pdest_bb = &simple_return_block_cold;\n-\t      pending = pending_edge_cold;\n-\t    }\n-\n-\t  if (*pdest_bb == NULL && pending != NULL)\n-\t    {\n-\t      emit_return_into_block (true, pending->src);\n-\t      pending->flags &= ~(EDGE_FALLTHRU | EDGE_FAKE);\n-\t      *pdest_bb = pending->src;\n-\t    }\n-\t  else if (*pdest_bb == NULL)\n-\t    {\n-\t      basic_block bb;\n-\t      rtx start;\n-\n-\t      bb = create_basic_block (NULL, NULL, exit_pred);\n-\t      BB_COPY_PARTITION (bb, e->src);\n-\t      start = emit_jump_insn_after (gen_simple_return (),\n-\t\t\t\t\t    BB_END (bb));\n-\t      JUMP_LABEL (start) = simple_return_rtx;\n-\t      emit_barrier_after (start);\n-\n-\t      *pdest_bb = bb;\n-\t      make_edge (bb, EXIT_BLOCK_PTR_FOR_FN (cfun), 0);\n-\t    }\n-\t  redirect_edge_and_branch_force (e, *pdest_bb);\n-\t}\n-      unconverted_simple_returns.release ();\n-    }\n-\n-  if (entry_edge != orig_entry_edge)\n-    {\n-      FOR_EACH_EDGE (e, ei, EXIT_BLOCK_PTR_FOR_FN (cfun)->preds)\n-\tif (EDGE_COUNT (e->src->preds) != 0\n-\t    && (e->flags & EDGE_FAKE) != 0\n-\t    && !bitmap_bit_p (&bb_flags, e->src->index))\n-\t  {\n-\t    emit_return_into_block (true, e->src);\n-\t    e->flags &= ~(EDGE_FALLTHRU | EDGE_FAKE);\n-\t  }\n-    }\n+  convert_to_simple_return (entry_edge, orig_entry_edge, bb_flags, returnjump,\n+\t\t\t    unconverted_simple_returns);\n #endif\n \n #ifdef HAVE_sibcall_epilogue"}, {"sha": "a8294b2675201fc0f50f05a158a6f6233784aa7f", "filename": "gcc/function.h", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2Ffunction.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2Ffunction.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffunction.h?ref=f30e25a336a39a990c82f8e3cdb507c73e38e84b", "patch": "@@ -809,10 +809,6 @@ extern void used_types_insert (tree);\n extern int get_next_funcdef_no (void);\n extern int get_last_funcdef_no (void);\n \n-#ifdef HAVE_simple_return\n-extern bool requires_stack_frame_p (rtx, HARD_REG_SET, HARD_REG_SET);\n-#endif                        \n-\n extern rtx get_hard_reg_initial_val (enum machine_mode, unsigned int);\n extern rtx has_hard_reg_initial_val (enum machine_mode, unsigned int);\n extern rtx get_hard_reg_initial_reg (rtx);"}, {"sha": "b302777736f81ddbc71981aa2688b597a753e3f6", "filename": "gcc/shrink-wrap.c", "status": "added", "additions": 876, "deletions": 0, "changes": 876, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2Fshrink-wrap.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2Fshrink-wrap.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fshrink-wrap.c?ref=f30e25a336a39a990c82f8e3cdb507c73e38e84b", "patch": "@@ -0,0 +1,876 @@\n+/* Expands front end tree to back end RTL for GCC.\n+   Copyright (C) 1987-2014 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+/* This file handles shrink-wrapping related optimizations.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"rtl-error.h\"\n+#include \"tree.h\"\n+#include \"stor-layout.h\"\n+#include \"varasm.h\"\n+#include \"stringpool.h\"\n+#include \"flags.h\"\n+#include \"except.h\"\n+#include \"function.h\"\n+#include \"expr.h\"\n+#include \"optabs.h\"\n+#include \"libfuncs.h\"\n+#include \"regs.h\"\n+#include \"hard-reg-set.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"output.h\"\n+#include \"hashtab.h\"\n+#include \"tm_p.h\"\n+#include \"langhooks.h\"\n+#include \"target.h\"\n+#include \"common/common-target.h\"\n+#include \"gimple-expr.h\"\n+#include \"gimplify.h\"\n+#include \"tree-pass.h\"\n+#include \"predict.h\"\n+#include \"df.h\"\n+#include \"params.h\"\n+#include \"bb-reorder.h\"\n+#include \"shrink-wrap.h\"\n+\n+\n+#ifdef HAVE_simple_return\n+\n+/* Return true if INSN requires the stack frame to be set up.\n+   PROLOGUE_USED contains the hard registers used in the function\n+   prologue.  SET_UP_BY_PROLOGUE is the set of registers we expect the\n+   prologue to set up for the function.  */\n+bool\n+requires_stack_frame_p (rtx insn, HARD_REG_SET prologue_used,\n+\t\t\tHARD_REG_SET set_up_by_prologue)\n+{\n+  df_ref *df_rec;\n+  HARD_REG_SET hardregs;\n+  unsigned regno;\n+\n+  if (CALL_P (insn))\n+    return !SIBLING_CALL_P (insn);\n+\n+  /* We need a frame to get the unique CFA expected by the unwinder.  */\n+  if (cfun->can_throw_non_call_exceptions && can_throw_internal (insn))\n+    return true;\n+\n+  CLEAR_HARD_REG_SET (hardregs);\n+  for (df_rec = DF_INSN_DEFS (insn); *df_rec; df_rec++)\n+    {\n+      rtx dreg = DF_REF_REG (*df_rec);\n+\n+      if (!REG_P (dreg))\n+\tcontinue;\n+\n+      add_to_hard_reg_set (&hardregs, GET_MODE (dreg),\n+\t\t\t   REGNO (dreg));\n+    }\n+  if (hard_reg_set_intersect_p (hardregs, prologue_used))\n+    return true;\n+  AND_COMPL_HARD_REG_SET (hardregs, call_used_reg_set);\n+  for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n+    if (TEST_HARD_REG_BIT (hardregs, regno)\n+\t&& df_regs_ever_live_p (regno))\n+      return true;\n+\n+  for (df_rec = DF_INSN_USES (insn); *df_rec; df_rec++)\n+    {\n+      rtx reg = DF_REF_REG (*df_rec);\n+\n+      if (!REG_P (reg))\n+\tcontinue;\n+\n+      add_to_hard_reg_set (&hardregs, GET_MODE (reg),\n+\t\t\t   REGNO (reg));\n+    }\n+  if (hard_reg_set_intersect_p (hardregs, set_up_by_prologue))\n+    return true;\n+\n+  return false;\n+}\n+\n+/* See whether BB has a single successor that uses [REGNO, END_REGNO),\n+   and if BB is its only predecessor.  Return that block if so,\n+   otherwise return null.  */\n+\n+static basic_block\n+next_block_for_reg (basic_block bb, int regno, int end_regno)\n+{\n+  edge e, live_edge;\n+  edge_iterator ei;\n+  bitmap live;\n+  int i;\n+\n+  live_edge = NULL;\n+  FOR_EACH_EDGE (e, ei, bb->succs)\n+    {\n+      live = df_get_live_in (e->dest);\n+      for (i = regno; i < end_regno; i++)\n+\tif (REGNO_REG_SET_P (live, i))\n+\t  {\n+\t    if (live_edge && live_edge != e)\n+\t      return NULL;\n+\t    live_edge = e;\n+\t  }\n+    }\n+\n+  /* We can sometimes encounter dead code.  Don't try to move it\n+     into the exit block.  */\n+  if (!live_edge || live_edge->dest == EXIT_BLOCK_PTR_FOR_FN (cfun))\n+    return NULL;\n+\n+  /* Reject targets of abnormal edges.  This is needed for correctness\n+     on ports like Alpha and MIPS, whose pic_offset_table_rtx can die on\n+     exception edges even though it is generally treated as call-saved\n+     for the majority of the compilation.  Moving across abnormal edges\n+     isn't going to be interesting for shrink-wrap usage anyway.  */\n+  if (live_edge->flags & EDGE_ABNORMAL)\n+    return NULL;\n+\n+  if (EDGE_COUNT (live_edge->dest->preds) > 1)\n+    return NULL;\n+\n+  return live_edge->dest;\n+}\n+\n+/* Try to move INSN from BB to a successor.  Return true on success.\n+   USES and DEFS are the set of registers that are used and defined\n+   after INSN in BB.  */\n+\n+static bool\n+move_insn_for_shrink_wrap (basic_block bb, rtx insn,\n+\t\t\t   const HARD_REG_SET uses,\n+\t\t\t   const HARD_REG_SET defs)\n+{\n+  rtx set, src, dest;\n+  bitmap live_out, live_in, bb_uses, bb_defs;\n+  unsigned int i, dregno, end_dregno, sregno, end_sregno;\n+  basic_block next_block;\n+\n+  /* Look for a simple register copy.  */\n+  set = single_set (insn);\n+  if (!set)\n+    return false;\n+  src = SET_SRC (set);\n+  dest = SET_DEST (set);\n+  if (!REG_P (dest) || !REG_P (src))\n+    return false;\n+\n+  /* Make sure that the source register isn't defined later in BB.  */\n+  sregno = REGNO (src);\n+  end_sregno = END_REGNO (src);\n+  if (overlaps_hard_reg_set_p (defs, GET_MODE (src), sregno))\n+    return false;\n+\n+  /* Make sure that the destination register isn't referenced later in BB.  */\n+  dregno = REGNO (dest);\n+  end_dregno = END_REGNO (dest);\n+  if (overlaps_hard_reg_set_p (uses, GET_MODE (dest), dregno)\n+      || overlaps_hard_reg_set_p (defs, GET_MODE (dest), dregno))\n+    return false;\n+\n+  /* See whether there is a successor block to which we could move INSN.  */\n+  next_block = next_block_for_reg (bb, dregno, end_dregno);\n+  if (!next_block)\n+    return false;\n+\n+  /* At this point we are committed to moving INSN, but let's try to\n+     move it as far as we can.  */\n+  do\n+    {\n+      live_out = df_get_live_out (bb);\n+      live_in = df_get_live_in (next_block);\n+      bb = next_block;\n+\n+      /* Check whether BB uses DEST or clobbers DEST.  We need to add\n+\t INSN to BB if so.  Either way, DEST is no longer live on entry,\n+\t except for any part that overlaps SRC (next loop).  */\n+      bb_uses = &DF_LR_BB_INFO (bb)->use;\n+      bb_defs = &DF_LR_BB_INFO (bb)->def;\n+      if (df_live)\n+\t{\n+\t  for (i = dregno; i < end_dregno; i++)\n+\t    {\n+\t      if (REGNO_REG_SET_P (bb_uses, i) || REGNO_REG_SET_P (bb_defs, i)\n+\t\t  || REGNO_REG_SET_P (&DF_LIVE_BB_INFO (bb)->gen, i))\n+\t\tnext_block = NULL;\n+\t      CLEAR_REGNO_REG_SET (live_out, i);\n+\t      CLEAR_REGNO_REG_SET (live_in, i);\n+\t    }\n+\n+\t  /* Check whether BB clobbers SRC.  We need to add INSN to BB if so.\n+\t     Either way, SRC is now live on entry.  */\n+\t  for (i = sregno; i < end_sregno; i++)\n+\t    {\n+\t      if (REGNO_REG_SET_P (bb_defs, i)\n+\t\t  || REGNO_REG_SET_P (&DF_LIVE_BB_INFO (bb)->gen, i))\n+\t\tnext_block = NULL;\n+\t      SET_REGNO_REG_SET (live_out, i);\n+\t      SET_REGNO_REG_SET (live_in, i);\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  /* DF_LR_BB_INFO (bb)->def does not comprise the DF_REF_PARTIAL and\n+\t     DF_REF_CONDITIONAL defs.  So if DF_LIVE doesn't exist, i.e.\n+\t     at -O1, just give up searching NEXT_BLOCK.  */\n+\t  next_block = NULL;\n+\t  for (i = dregno; i < end_dregno; i++)\n+\t    {\n+\t      CLEAR_REGNO_REG_SET (live_out, i);\n+\t      CLEAR_REGNO_REG_SET (live_in, i);\n+\t    }\n+\n+\t  for (i = sregno; i < end_sregno; i++)\n+\t    {\n+\t      SET_REGNO_REG_SET (live_out, i);\n+\t      SET_REGNO_REG_SET (live_in, i);\n+\t    }\n+\t}\n+\n+      /* If we don't need to add the move to BB, look for a single\n+\t successor block.  */\n+      if (next_block)\n+\tnext_block = next_block_for_reg (next_block, dregno, end_dregno);\n+    }\n+  while (next_block);\n+\n+  /* BB now defines DEST.  It only uses the parts of DEST that overlap SRC\n+     (next loop).  */\n+  for (i = dregno; i < end_dregno; i++)\n+    {\n+      CLEAR_REGNO_REG_SET (bb_uses, i);\n+      SET_REGNO_REG_SET (bb_defs, i);\n+    }\n+\n+  /* BB now uses SRC.  */\n+  for (i = sregno; i < end_sregno; i++)\n+    SET_REGNO_REG_SET (bb_uses, i);\n+\n+  emit_insn_after (PATTERN (insn), bb_note (bb));\n+  delete_insn (insn);\n+  return true;\n+}\n+\n+/* Look for register copies in the first block of the function, and move\n+   them down into successor blocks if the register is used only on one\n+   path.  This exposes more opportunities for shrink-wrapping.  These\n+   kinds of sets often occur when incoming argument registers are moved\n+   to call-saved registers because their values are live across one or\n+   more calls during the function.  */\n+\n+void\n+prepare_shrink_wrap (basic_block entry_block)\n+{\n+  rtx insn, curr, x;\n+  HARD_REG_SET uses, defs;\n+  df_ref *ref;\n+\n+  CLEAR_HARD_REG_SET (uses);\n+  CLEAR_HARD_REG_SET (defs);\n+  FOR_BB_INSNS_REVERSE_SAFE (entry_block, insn, curr)\n+    if (NONDEBUG_INSN_P (insn)\n+\t&& !move_insn_for_shrink_wrap (entry_block, insn, uses, defs))\n+      {\n+\t/* Add all defined registers to DEFs.  */\n+\tfor (ref = DF_INSN_DEFS (insn); *ref; ref++)\n+\t  {\n+\t    x = DF_REF_REG (*ref);\n+\t    if (REG_P (x) && HARD_REGISTER_P (x))\n+\t      SET_HARD_REG_BIT (defs, REGNO (x));\n+\t  }\n+\n+\t/* Add all used registers to USESs.  */\n+\tfor (ref = DF_INSN_USES (insn); *ref; ref++)\n+\t  {\n+\t    x = DF_REF_REG (*ref);\n+\t    if (REG_P (x) && HARD_REGISTER_P (x))\n+\t      SET_HARD_REG_BIT (uses, REGNO (x));\n+\t  }\n+      }\n+}\n+\n+/* Create a copy of BB instructions and insert at BEFORE.  Redirect\n+   preds of BB to COPY_BB if they don't appear in NEED_PROLOGUE.  */\n+void\n+dup_block_and_redirect (basic_block bb, basic_block copy_bb, rtx before,\n+\t\t\tbitmap_head *need_prologue)\n+{\n+  edge_iterator ei;\n+  edge e;\n+  rtx insn = BB_END (bb);\n+\n+  /* We know BB has a single successor, so there is no need to copy a\n+     simple jump at the end of BB.  */\n+  if (simplejump_p (insn))\n+    insn = PREV_INSN (insn);\n+\n+  start_sequence ();\n+  duplicate_insn_chain (BB_HEAD (bb), insn);\n+  if (dump_file)\n+    {\n+      unsigned count = 0;\n+      for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n+\tif (active_insn_p (insn))\n+\t  ++count;\n+      fprintf (dump_file, \"Duplicating bb %d to bb %d, %u active insns.\\n\",\n+\t       bb->index, copy_bb->index, count);\n+    }\n+  insn = get_insns ();\n+  end_sequence ();\n+  emit_insn_before (insn, before);\n+\n+  /* Redirect all the paths that need no prologue into copy_bb.  */\n+  for (ei = ei_start (bb->preds); (e = ei_safe_edge (ei));)\n+    if (!bitmap_bit_p (need_prologue, e->src->index))\n+      {\n+\tint freq = EDGE_FREQUENCY (e);\n+\tcopy_bb->count += e->count;\n+\tcopy_bb->frequency += EDGE_FREQUENCY (e);\n+\te->dest->count -= e->count;\n+\tif (e->dest->count < 0)\n+\t  e->dest->count = 0;\n+\te->dest->frequency -= freq;\n+\tif (e->dest->frequency < 0)\n+\t  e->dest->frequency = 0;\n+\tredirect_edge_and_branch_force (e, copy_bb);\n+\tcontinue;\n+      }\n+    else\n+      ei_next (&ei);\n+}\n+\n+\n+/* Try to perform a kind of shrink-wrapping, making sure the\n+   prologue/epilogue is emitted only around those parts of the\n+   function that require it.  */\n+\n+void\n+try_shrink_wrapping (edge *entry_edge, edge orig_entry_edge,\n+\t\t     bitmap_head *bb_flags, rtx prologue_seq)\n+{\n+  edge e;\n+  edge_iterator ei;\n+  bool nonempty_prologue = false;\n+  unsigned max_grow_size;\n+  rtx seq;\n+\n+  for (seq = prologue_seq; seq; seq = NEXT_INSN (seq))\n+    if (!NOTE_P (seq) || NOTE_KIND (seq) != NOTE_INSN_PROLOGUE_END)\n+      {\n+\tnonempty_prologue = true;\n+\tbreak;\n+      }\n+\n+  if (flag_shrink_wrap && HAVE_simple_return\n+      && (targetm.profile_before_prologue () || !crtl->profile)\n+      && nonempty_prologue && !crtl->calls_eh_return)\n+    {\n+      HARD_REG_SET prologue_clobbered, prologue_used, live_on_edge;\n+      struct hard_reg_set_container set_up_by_prologue;\n+      rtx p_insn;\n+      vec<basic_block> vec;\n+      basic_block bb;\n+      bitmap_head bb_antic_flags;\n+      bitmap_head bb_on_list;\n+      bitmap_head bb_tail;\n+\n+      if (dump_file)\n+\tfprintf (dump_file, \"Attempting shrink-wrapping optimization.\\n\");\n+\n+      /* Compute the registers set and used in the prologue.  */\n+      CLEAR_HARD_REG_SET (prologue_clobbered);\n+      CLEAR_HARD_REG_SET (prologue_used);\n+      for (p_insn = prologue_seq; p_insn; p_insn = NEXT_INSN (p_insn))\n+\t{\n+\t  HARD_REG_SET this_used;\n+\t  if (!NONDEBUG_INSN_P (p_insn))\n+\t    continue;\n+\n+\t  CLEAR_HARD_REG_SET (this_used);\n+\t  note_uses (&PATTERN (p_insn), record_hard_reg_uses,\n+\t\t     &this_used);\n+\t  AND_COMPL_HARD_REG_SET (this_used, prologue_clobbered);\n+\t  IOR_HARD_REG_SET (prologue_used, this_used);\n+\t  note_stores (PATTERN (p_insn), record_hard_reg_sets,\n+\t\t       &prologue_clobbered);\n+\t}\n+\n+      prepare_shrink_wrap ((*entry_edge)->dest);\n+\n+      bitmap_initialize (&bb_antic_flags, &bitmap_default_obstack);\n+      bitmap_initialize (&bb_on_list, &bitmap_default_obstack);\n+      bitmap_initialize (&bb_tail, &bitmap_default_obstack);\n+\n+      /* Find the set of basic blocks that require a stack frame,\n+\t and blocks that are too big to be duplicated.  */\n+\n+      vec.create (n_basic_blocks_for_fn (cfun));\n+\n+      CLEAR_HARD_REG_SET (set_up_by_prologue.set);\n+      add_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n+\t\t\t   STACK_POINTER_REGNUM);\n+      add_to_hard_reg_set (&set_up_by_prologue.set, Pmode, ARG_POINTER_REGNUM);\n+      if (frame_pointer_needed)\n+\tadd_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n+\t\t\t     HARD_FRAME_POINTER_REGNUM);\n+      if (pic_offset_table_rtx)\n+\tadd_to_hard_reg_set (&set_up_by_prologue.set, Pmode,\n+\t\t\t     PIC_OFFSET_TABLE_REGNUM);\n+      if (crtl->drap_reg)\n+\tadd_to_hard_reg_set (&set_up_by_prologue.set,\n+\t\t\t     GET_MODE (crtl->drap_reg),\n+\t\t\t     REGNO (crtl->drap_reg));\n+      if (targetm.set_up_by_prologue)\n+\ttargetm.set_up_by_prologue (&set_up_by_prologue);\n+\n+      /* We don't use a different max size depending on\n+\t optimize_bb_for_speed_p because increasing shrink-wrapping\n+\t opportunities by duplicating tail blocks can actually result\n+\t in an overall decrease in code size.  */\n+      max_grow_size = get_uncond_jump_length ();\n+      max_grow_size *= PARAM_VALUE (PARAM_MAX_GROW_COPY_BB_INSNS);\n+\n+      FOR_EACH_BB_FN (bb, cfun)\n+\t{\n+\t  rtx insn;\n+\t  unsigned size = 0;\n+\n+\t  FOR_BB_INSNS (bb, insn)\n+\t    if (NONDEBUG_INSN_P (insn))\n+\t      {\n+\t\tif (requires_stack_frame_p (insn, prologue_used,\n+\t\t\t\t\t    set_up_by_prologue.set))\n+\t\t  {\n+\t\t    if (bb == (*entry_edge)->dest)\n+\t\t      goto fail_shrinkwrap;\n+\t\t    bitmap_set_bit (bb_flags, bb->index);\n+\t\t    vec.quick_push (bb);\n+\t\t    break;\n+\t\t  }\n+\t\telse if (size <= max_grow_size)\n+\t\t  {\n+\t\t    size += get_attr_min_length (insn);\n+\t\t    if (size > max_grow_size)\n+\t\t      bitmap_set_bit (&bb_on_list, bb->index);\n+\t\t  }\n+\t      }\n+\t}\n+\n+      /* Blocks that really need a prologue, or are too big for tails.  */\n+      bitmap_ior_into (&bb_on_list, bb_flags);\n+\n+      /* For every basic block that needs a prologue, mark all blocks\n+\t reachable from it, so as to ensure they are also seen as\n+\t requiring a prologue.  */\n+      while (!vec.is_empty ())\n+\t{\n+\t  basic_block tmp_bb = vec.pop ();\n+\n+\t  FOR_EACH_EDGE (e, ei, tmp_bb->succs)\n+\t    if (e->dest != EXIT_BLOCK_PTR_FOR_FN (cfun)\n+\t\t&& bitmap_set_bit (bb_flags, e->dest->index))\n+\t      vec.quick_push (e->dest);\n+\t}\n+\n+      /* Find the set of basic blocks that need no prologue, have a\n+\t single successor, can be duplicated, meet a max size\n+\t requirement, and go to the exit via like blocks.  */\n+      vec.quick_push (EXIT_BLOCK_PTR_FOR_FN (cfun));\n+      while (!vec.is_empty ())\n+\t{\n+\t  basic_block tmp_bb = vec.pop ();\n+\n+\t  FOR_EACH_EDGE (e, ei, tmp_bb->preds)\n+\t    if (single_succ_p (e->src)\n+\t\t&& !bitmap_bit_p (&bb_on_list, e->src->index)\n+\t\t&& can_duplicate_block_p (e->src))\n+\t      {\n+\t\tedge pe;\n+\t\tedge_iterator pei;\n+\n+\t\t/* If there is predecessor of e->src which doesn't\n+\t\t   need prologue and the edge is complex,\n+\t\t   we might not be able to redirect the branch\n+\t\t   to a copy of e->src.  */\n+\t\tFOR_EACH_EDGE (pe, pei, e->src->preds)\n+\t\t  if ((pe->flags & EDGE_COMPLEX) != 0\n+\t\t      && !bitmap_bit_p (bb_flags, pe->src->index))\n+\t\t    break;\n+\t\tif (pe == NULL && bitmap_set_bit (&bb_tail, e->src->index))\n+\t\t  vec.quick_push (e->src);\n+\t      }\n+\t}\n+\n+      /* Now walk backwards from every block that is marked as needing\n+\t a prologue to compute the bb_antic_flags bitmap.  Exclude\n+\t tail blocks; They can be duplicated to be used on paths not\n+\t needing a prologue.  */\n+      bitmap_clear (&bb_on_list);\n+      bitmap_and_compl (&bb_antic_flags, bb_flags, &bb_tail);\n+      FOR_EACH_BB_FN (bb, cfun)\n+\t{\n+\t  if (!bitmap_bit_p (&bb_antic_flags, bb->index))\n+\t    continue;\n+\t  FOR_EACH_EDGE (e, ei, bb->preds)\n+\t    if (!bitmap_bit_p (&bb_antic_flags, e->src->index)\n+\t\t&& bitmap_set_bit (&bb_on_list, e->src->index))\n+\t      vec.quick_push (e->src);\n+\t}\n+      while (!vec.is_empty ())\n+\t{\n+\t  basic_block tmp_bb = vec.pop ();\n+\t  bool all_set = true;\n+\n+\t  bitmap_clear_bit (&bb_on_list, tmp_bb->index);\n+\t  FOR_EACH_EDGE (e, ei, tmp_bb->succs)\n+\t    if (!bitmap_bit_p (&bb_antic_flags, e->dest->index))\n+\t      {\n+\t\tall_set = false;\n+\t\tbreak;\n+\t      }\n+\n+\t  if (all_set)\n+\t    {\n+\t      bitmap_set_bit (&bb_antic_flags, tmp_bb->index);\n+\t      FOR_EACH_EDGE (e, ei, tmp_bb->preds)\n+\t\tif (!bitmap_bit_p (&bb_antic_flags, e->src->index)\n+\t\t    && bitmap_set_bit (&bb_on_list, e->src->index))\n+\t\t  vec.quick_push (e->src);\n+\t    }\n+\t}\n+      /* Find exactly one edge that leads to a block in ANTIC from\n+\t a block that isn't.  */\n+      if (!bitmap_bit_p (&bb_antic_flags, (*entry_edge)->dest->index))\n+\tFOR_EACH_BB_FN (bb, cfun)\n+\t  {\n+\t    if (!bitmap_bit_p (&bb_antic_flags, bb->index))\n+\t      continue;\n+\t    FOR_EACH_EDGE (e, ei, bb->preds)\n+\t      if (!bitmap_bit_p (&bb_antic_flags, e->src->index))\n+\t\t{\n+\t\t  if (*entry_edge != orig_entry_edge)\n+\t\t    {\n+\t\t      *entry_edge = orig_entry_edge;\n+\t\t      if (dump_file)\n+\t\t\tfprintf (dump_file, \"More than one candidate edge.\\n\");\n+\t\t      goto fail_shrinkwrap;\n+\t\t    }\n+\t\t  if (dump_file)\n+\t\t    fprintf (dump_file, \"Found candidate edge for \"\n+\t\t\t     \"shrink-wrapping, %d->%d.\\n\", e->src->index,\n+\t\t\t     e->dest->index);\n+\t\t  *entry_edge = e;\n+\t\t}\n+\t  }\n+\n+      if (*entry_edge != orig_entry_edge)\n+\t{\n+\t  /* Test whether the prologue is known to clobber any register\n+\t     (other than FP or SP) which are live on the edge.  */\n+\t  CLEAR_HARD_REG_BIT (prologue_clobbered, STACK_POINTER_REGNUM);\n+\t  if (frame_pointer_needed)\n+\t    CLEAR_HARD_REG_BIT (prologue_clobbered, HARD_FRAME_POINTER_REGNUM);\n+\t  REG_SET_TO_HARD_REG_SET (live_on_edge,\n+\t\t\t\t   df_get_live_in ((*entry_edge)->dest));\n+\t  if (hard_reg_set_intersect_p (live_on_edge, prologue_clobbered))\n+\t    {\n+\t      *entry_edge = orig_entry_edge;\n+\t      if (dump_file)\n+\t\tfprintf (dump_file,\n+\t\t\t \"Shrink-wrapping aborted due to clobber.\\n\");\n+\t    }\n+\t}\n+      if (*entry_edge != orig_entry_edge)\n+\t{\n+\t  crtl->shrink_wrapped = true;\n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"Performing shrink-wrapping.\\n\");\n+\n+\t  /* Find tail blocks reachable from both blocks needing a\n+\t     prologue and blocks not needing a prologue.  */\n+\t  if (!bitmap_empty_p (&bb_tail))\n+\t    FOR_EACH_BB_FN (bb, cfun)\n+\t      {\n+\t\tbool some_pro, some_no_pro;\n+\t\tif (!bitmap_bit_p (&bb_tail, bb->index))\n+\t\t  continue;\n+\t\tsome_pro = some_no_pro = false;\n+\t\tFOR_EACH_EDGE (e, ei, bb->preds)\n+\t\t  {\n+\t\t    if (bitmap_bit_p (bb_flags, e->src->index))\n+\t\t      some_pro = true;\n+\t\t    else\n+\t\t      some_no_pro = true;\n+\t\t  }\n+\t\tif (some_pro && some_no_pro)\n+\t\t  vec.quick_push (bb);\n+\t\telse\n+\t\t  bitmap_clear_bit (&bb_tail, bb->index);\n+\t      }\n+\t  /* Find the head of each tail.  */\n+\t  while (!vec.is_empty ())\n+\t    {\n+\t      basic_block tbb = vec.pop ();\n+\n+\t      if (!bitmap_bit_p (&bb_tail, tbb->index))\n+\t\tcontinue;\n+\n+\t      while (single_succ_p (tbb))\n+\t\t{\n+\t\t  tbb = single_succ (tbb);\n+\t\t  bitmap_clear_bit (&bb_tail, tbb->index);\n+\t\t}\n+\t    }\n+\t  /* Now duplicate the tails.  */\n+\t  if (!bitmap_empty_p (&bb_tail))\n+\t    FOR_EACH_BB_REVERSE_FN (bb, cfun)\n+\t      {\n+\t\tbasic_block copy_bb, tbb;\n+\t\trtx insert_point;\n+\t\tint eflags;\n+\n+\t\tif (!bitmap_clear_bit (&bb_tail, bb->index))\n+\t\t  continue;\n+\n+\t\t/* Create a copy of BB, instructions and all, for\n+\t\t   use on paths that don't need a prologue.\n+\t\t   Ideal placement of the copy is on a fall-thru edge\n+\t\t   or after a block that would jump to the copy.  */\n+\t\tFOR_EACH_EDGE (e, ei, bb->preds)\n+\t\t  if (!bitmap_bit_p (bb_flags, e->src->index)\n+\t\t      && single_succ_p (e->src))\n+\t\t    break;\n+\t\tif (e)\n+\t\t  {\n+                    /* Make sure we insert after any barriers.  */\n+                    rtx end = get_last_bb_insn (e->src);\n+                    copy_bb = create_basic_block (NEXT_INSN (end),\n+                                                  NULL_RTX, e->src);\n+\t\t    BB_COPY_PARTITION (copy_bb, e->src);\n+\t\t  }\n+\t\telse\n+\t\t  {\n+\t\t    /* Otherwise put the copy at the end of the function.  */\n+\t\t    copy_bb = create_basic_block (NULL_RTX, NULL_RTX,\n+\t\t\t\t\t\t  EXIT_BLOCK_PTR_FOR_FN (cfun)->prev_bb);\n+\t\t    BB_COPY_PARTITION (copy_bb, bb);\n+\t\t  }\n+\n+\t\tinsert_point = emit_note_after (NOTE_INSN_DELETED,\n+\t\t\t\t\t\tBB_END (copy_bb));\n+\t\temit_barrier_after (BB_END (copy_bb));\n+\n+\t\ttbb = bb;\n+\t\twhile (1)\n+\t\t  {\n+\t\t    dup_block_and_redirect (tbb, copy_bb, insert_point,\n+\t\t\t\t\t    bb_flags);\n+\t\t    tbb = single_succ (tbb);\n+\t\t    if (tbb == EXIT_BLOCK_PTR_FOR_FN (cfun))\n+\t\t      break;\n+\t\t    e = split_block (copy_bb, PREV_INSN (insert_point));\n+\t\t    copy_bb = e->dest;\n+\t\t  }\n+\n+\t\t/* Quiet verify_flow_info by (ab)using EDGE_FAKE.\n+\t\t   We have yet to add a simple_return to the tails,\n+\t\t   as we'd like to first convert_jumps_to_returns in\n+\t\t   case the block is no longer used after that.  */\n+\t\teflags = EDGE_FAKE;\n+\t\tif (CALL_P (PREV_INSN (insert_point))\n+\t\t    && SIBLING_CALL_P (PREV_INSN (insert_point)))\n+\t\t  eflags = EDGE_SIBCALL | EDGE_ABNORMAL;\n+\t\tmake_single_succ_edge (copy_bb, EXIT_BLOCK_PTR_FOR_FN (cfun),\n+\t\t\t\t       eflags);\n+\n+\t\t/* verify_flow_info doesn't like a note after a\n+\t\t   sibling call.  */\n+\t\tdelete_insn (insert_point);\n+\t\tif (bitmap_empty_p (&bb_tail))\n+\t\t  break;\n+\t      }\n+\t}\n+\n+    fail_shrinkwrap:\n+      bitmap_clear (&bb_tail);\n+      bitmap_clear (&bb_antic_flags);\n+      bitmap_clear (&bb_on_list);\n+      vec.release ();\n+    }\n+}\n+\n+/* If we're allowed to generate a simple return instruction, then by\n+   definition we don't need a full epilogue.  If the last basic\n+   block before the exit block does not contain active instructions,\n+   examine its predecessors and try to emit (conditional) return\n+   instructions.  */\n+\n+edge\n+get_unconverted_simple_return (edge exit_fallthru_edge, bitmap_head bb_flags,\n+\t\t\t       vec<edge> *unconverted_simple_returns,\n+\t\t\t       rtx *returnjump)\n+{\n+  if (optimize)\n+    {\n+      unsigned i, last;\n+\n+      /* convert_jumps_to_returns may add to preds of the exit block\n+         (but won't remove).  Stop at end of current preds.  */\n+      last = EDGE_COUNT (EXIT_BLOCK_PTR_FOR_FN (cfun)->preds);\n+      for (i = 0; i < last; i++)\n+\t{\n+\t  edge e = EDGE_I (EXIT_BLOCK_PTR_FOR_FN (cfun)->preds, i);\n+\t  if (LABEL_P (BB_HEAD (e->src))\n+\t      && !bitmap_bit_p (&bb_flags, e->src->index)\n+\t      && !active_insn_between (BB_HEAD (e->src), BB_END (e->src)))\n+\t    *unconverted_simple_returns\n+\t\t  = convert_jumps_to_returns (e->src, true,\n+\t\t\t\t\t      *unconverted_simple_returns);\n+\t}\n+    }\n+\n+  if (exit_fallthru_edge != NULL\n+      && EDGE_COUNT (exit_fallthru_edge->src->preds) != 0\n+      && !bitmap_bit_p (&bb_flags, exit_fallthru_edge->src->index))\n+    {\n+      basic_block last_bb;\n+\n+      last_bb = emit_return_for_exit (exit_fallthru_edge, true);\n+      *returnjump = BB_END (last_bb);\n+      exit_fallthru_edge = NULL;\n+    }\n+  return exit_fallthru_edge;\n+}\n+\n+/* If there were branches to an empty LAST_BB which we tried to\n+   convert to conditional simple_returns, but couldn't for some\n+   reason, create a block to hold a simple_return insn and redirect\n+   those remaining edges.  */\n+\n+void\n+convert_to_simple_return (edge entry_edge, edge orig_entry_edge,\n+\t\t\t  bitmap_head bb_flags, rtx returnjump,\n+\t\t\t  vec<edge> unconverted_simple_returns)\n+{\n+  edge e;\n+  edge_iterator ei;\n+\n+  if (!unconverted_simple_returns.is_empty ())\n+    {\n+      basic_block simple_return_block_hot = NULL;\n+      basic_block simple_return_block_cold = NULL;\n+      edge pending_edge_hot = NULL;\n+      edge pending_edge_cold = NULL;\n+      basic_block exit_pred;\n+      int i;\n+\n+      gcc_assert (entry_edge != orig_entry_edge);\n+\n+      /* See if we can reuse the last insn that was emitted for the\n+\t epilogue.  */\n+      if (returnjump != NULL_RTX\n+\t  && JUMP_LABEL (returnjump) == simple_return_rtx)\n+\t{\n+\t  e = split_block (BLOCK_FOR_INSN (returnjump), PREV_INSN (returnjump));\n+\t  if (BB_PARTITION (e->src) == BB_HOT_PARTITION)\n+\t    simple_return_block_hot = e->dest;\n+\t  else\n+\t    simple_return_block_cold = e->dest;\n+\t}\n+\n+      /* Also check returns we might need to add to tail blocks.  */\n+      FOR_EACH_EDGE (e, ei, EXIT_BLOCK_PTR_FOR_FN (cfun)->preds)\n+\tif (EDGE_COUNT (e->src->preds) != 0\n+\t    && (e->flags & EDGE_FAKE) != 0\n+\t    && !bitmap_bit_p (&bb_flags, e->src->index))\n+\t  {\n+\t    if (BB_PARTITION (e->src) == BB_HOT_PARTITION)\n+\t      pending_edge_hot = e;\n+\t    else\n+\t      pending_edge_cold = e;\n+\t  }\n+\n+      /* Save a pointer to the exit's predecessor BB for use in\n+         inserting new BBs at the end of the function.  Do this\n+         after the call to split_block above which may split\n+         the original exit pred.  */\n+      exit_pred = EXIT_BLOCK_PTR_FOR_FN (cfun)->prev_bb;\n+\n+      FOR_EACH_VEC_ELT (unconverted_simple_returns, i, e)\n+\t{\n+\t  basic_block *pdest_bb;\n+\t  edge pending;\n+\n+\t  if (BB_PARTITION (e->src) == BB_HOT_PARTITION)\n+\t    {\n+\t      pdest_bb = &simple_return_block_hot;\n+\t      pending = pending_edge_hot;\n+\t    }\n+\t  else\n+\t    {\n+\t      pdest_bb = &simple_return_block_cold;\n+\t      pending = pending_edge_cold;\n+\t    }\n+\n+\t  if (*pdest_bb == NULL && pending != NULL)\n+\t    {\n+\t      emit_return_into_block (true, pending->src);\n+\t      pending->flags &= ~(EDGE_FALLTHRU | EDGE_FAKE);\n+\t      *pdest_bb = pending->src;\n+\t    }\n+\t  else if (*pdest_bb == NULL)\n+\t    {\n+\t      basic_block bb;\n+\t      rtx start;\n+\n+\t      bb = create_basic_block (NULL, NULL, exit_pred);\n+\t      BB_COPY_PARTITION (bb, e->src);\n+\t      start = emit_jump_insn_after (gen_simple_return (),\n+\t\t\t\t\t    BB_END (bb));\n+\t      JUMP_LABEL (start) = simple_return_rtx;\n+\t      emit_barrier_after (start);\n+\n+\t      *pdest_bb = bb;\n+\t      make_edge (bb, EXIT_BLOCK_PTR_FOR_FN (cfun), 0);\n+\t    }\n+\t  redirect_edge_and_branch_force (e, *pdest_bb);\n+\t}\n+      unconverted_simple_returns.release ();\n+    }\n+\n+  if (entry_edge != orig_entry_edge)\n+    {\n+      FOR_EACH_EDGE (e, ei, EXIT_BLOCK_PTR_FOR_FN (cfun)->preds)\n+\tif (EDGE_COUNT (e->src->preds) != 0\n+\t    && (e->flags & EDGE_FAKE) != 0\n+\t    && !bitmap_bit_p (&bb_flags, e->src->index))\n+\t  {\n+\t    emit_return_into_block (true, e->src);\n+\t    e->flags &= ~(EDGE_FALLTHRU | EDGE_FAKE);\n+\t  }\n+    }\n+}\n+\n+#endif"}, {"sha": "22b1d5c5165221e00fa33c8e2e185858404452ed", "filename": "gcc/shrink-wrap.h", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2Fshrink-wrap.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f30e25a336a39a990c82f8e3cdb507c73e38e84b/gcc%2Fshrink-wrap.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fshrink-wrap.h?ref=f30e25a336a39a990c82f8e3cdb507c73e38e84b", "patch": "@@ -0,0 +1,52 @@\n+/* Structure for saving state for a nested function.\n+   Copyright (C) 1989-2014 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_SHRINK_WRAP_H\n+#define GCC_SHRINK_WRAP_H\n+\n+#include \"hashtab.h\"\n+#include \"vec.h\"\n+#include \"machmode.h\"\n+\n+#ifdef HAVE_simple_return\n+/* In function.c.  */\n+extern void emit_return_into_block (bool simple_p, basic_block bb);\n+extern bool active_insn_between (rtx head, rtx tail);\n+extern vec<edge> convert_jumps_to_returns (basic_block last_bb, bool simple_p,\n+\t\t\t\t\t   vec<edge> unconverted);\n+extern basic_block emit_return_for_exit (edge exit_fallthru_edge,\n+\t\t\t\t\t bool simple_p);\n+\n+/* In shrink-wrap.c.  */\n+extern bool requires_stack_frame_p (rtx, HARD_REG_SET, HARD_REG_SET);\n+extern void prepare_shrink_wrap (basic_block entry_block);\n+extern void dup_block_and_redirect (basic_block bb, basic_block copy_bb,\n+\t\t\t\t    rtx before,\tbitmap_head *need_prologue);\n+extern void try_shrink_wrapping (edge *entry_edge, edge orig_entry_edge,\n+\t\t\t\t bitmap_head *bb_flags, rtx prologue_seq);\n+extern edge get_unconverted_simple_return (edge, bitmap_head,\n+\t\t\t\t\t   vec<edge> *, rtx *);\n+extern void convert_to_simple_return (edge entry_edge, edge orig_entry_edge,\n+\t\t\t\t      bitmap_head bb_flags, rtx returnjump,\n+\t\t\t\t      vec<edge> unconverted_simple_returns);\n+#endif\n+\n+#endif  /* GCC_SHRINK_WRAP_H  */\n+\n+"}]}