{"sha": "1d77928fc49b4f2487fd78db26bbebd00f881414", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MWQ3NzkyOGZjNDliNGYyNDg3ZmQ3OGRiMjZiYmViZDAwZjg4MTQxNA==", "commit": {"author": {"name": "Wilco Dijkstra", "email": "wilco.dijkstra@arm.com", "date": "2020-11-19T15:57:52Z"}, "committer": {"name": "Wilco Dijkstra", "email": "wdijkstr@arm.com", "date": "2020-11-19T16:05:33Z"}, "message": "AArch64: Improve inline memcpy expansion\n\nImprove the inline memcpy expansion.  Use integer load/store for copies <= 24\nbytes instead of SIMD.  Set the maximum copy to expand to 256 by default,\nexcept that -Os or no Neon expands up to 128 bytes.  When using LDP/STP of\nQ-registers, also use Q-register accesses for the unaligned tail, saving 2\ninstructions (eg. all sizes up to 48 bytes emit exactly 4 instructions).\nCleanup code and comments.\n\nThe codesize gain vs the GCC10 expansion is 0.05% on SPECINT2017.\n\n2020-11-03  Wilco Dijkstra  <wdijkstr@arm.com>\n\ngcc/\n\t* config/aarch64/aarch64.c (aarch64_expand_cpymem): Cleanup code and\n\tcomments, tweak expansion decisions and improve tail expansion.", "tree": {"sha": "1121ac93d078a72f50bbb46975f8a6815fef1eb5", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1121ac93d078a72f50bbb46975f8a6815fef1eb5"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/1d77928fc49b4f2487fd78db26bbebd00f881414", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1d77928fc49b4f2487fd78db26bbebd00f881414", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1d77928fc49b4f2487fd78db26bbebd00f881414", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1d77928fc49b4f2487fd78db26bbebd00f881414/comments", "author": {"login": "Wilco1", "id": 58446312, "node_id": "MDQ6VXNlcjU4NDQ2MzEy", "avatar_url": "https://avatars.githubusercontent.com/u/58446312?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Wilco1", "html_url": "https://github.com/Wilco1", "followers_url": "https://api.github.com/users/Wilco1/followers", "following_url": "https://api.github.com/users/Wilco1/following{/other_user}", "gists_url": "https://api.github.com/users/Wilco1/gists{/gist_id}", "starred_url": "https://api.github.com/users/Wilco1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Wilco1/subscriptions", "organizations_url": "https://api.github.com/users/Wilco1/orgs", "repos_url": "https://api.github.com/users/Wilco1/repos", "events_url": "https://api.github.com/users/Wilco1/events{/privacy}", "received_events_url": "https://api.github.com/users/Wilco1/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "2729378d0905a04e476a8bdcaaf0288f417810ec", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2729378d0905a04e476a8bdcaaf0288f417810ec", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2729378d0905a04e476a8bdcaaf0288f417810ec"}], "stats": {"total": 73, "additions": 37, "deletions": 36}, "files": [{"sha": "1cf4900eda5dafd3072b7230b404b9786aef2057", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 37, "deletions": 36, "changes": 73, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1d77928fc49b4f2487fd78db26bbebd00f881414/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1d77928fc49b4f2487fd78db26bbebd00f881414/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=1d77928fc49b4f2487fd78db26bbebd00f881414", "patch": "@@ -21200,76 +21200,77 @@ aarch64_copy_one_block_and_progress_pointers (rtx *src, rtx *dst,\n bool\n aarch64_expand_cpymem (rtx *operands)\n {\n-  /* These need to be signed as we need to perform arithmetic on n as\n-     signed operations.  */\n-  int n, mode_bits;\n+  int mode_bits;\n   rtx dst = operands[0];\n   rtx src = operands[1];\n   rtx base;\n-  machine_mode cur_mode = BLKmode, next_mode;\n-  bool speed_p = !optimize_function_for_size_p (cfun);\n+  machine_mode cur_mode = BLKmode;\n \n-  /* When optimizing for size, give a better estimate of the length of a\n-     memcpy call, but use the default otherwise.  Moves larger than 8 bytes\n-     will always require an even number of instructions to do now.  And each\n-     operation requires both a load+store, so divide the max number by 2.  */\n-  unsigned int max_num_moves = (speed_p ? 16 : AARCH64_CALL_RATIO) / 2;\n-\n-  /* We can't do anything smart if the amount to copy is not constant.  */\n+  /* Only expand fixed-size copies.  */\n   if (!CONST_INT_P (operands[2]))\n     return false;\n \n-  unsigned HOST_WIDE_INT tmp = INTVAL (operands[2]);\n+  unsigned HOST_WIDE_INT size = INTVAL (operands[2]);\n \n-  /* Try to keep the number of instructions low.  For all cases we will do at\n-     most two moves for the residual amount, since we'll always overlap the\n-     remainder.  */\n-  if (((tmp / 16) + (tmp % 16 ? 2 : 0)) > max_num_moves)\n-    return false;\n+  /* Inline up to 256 bytes when optimizing for speed.  */\n+  unsigned HOST_WIDE_INT max_copy_size = 256;\n \n-  /* At this point tmp is known to have to fit inside an int.  */\n-  n = tmp;\n+  if (optimize_function_for_size_p (cfun))\n+    max_copy_size = 128;\n+\n+  int copy_bits = 256;\n+\n+  /* Default to 256-bit LDP/STP on large copies, however small copies, no SIMD\n+     support or slow 256-bit LDP/STP fall back to 128-bit chunks.  */\n+  if (size <= 24\n+      || !TARGET_SIMD\n+      || (aarch64_tune_params.extra_tuning_flags\n+\t  & AARCH64_EXTRA_TUNE_NO_LDP_STP_QREGS))\n+    {\n+      copy_bits = 128;\n+      max_copy_size = max_copy_size / 2;\n+    }\n+\n+  if (size > max_copy_size)\n+    return false;\n \n   base = copy_to_mode_reg (Pmode, XEXP (dst, 0));\n   dst = adjust_automodify_address (dst, VOIDmode, base, 0);\n \n   base = copy_to_mode_reg (Pmode, XEXP (src, 0));\n   src = adjust_automodify_address (src, VOIDmode, base, 0);\n \n-  /* Convert n to bits to make the rest of the code simpler.  */\n-  n = n * BITS_PER_UNIT;\n-\n-  /* Maximum amount to copy in one go.  We allow 256-bit chunks based on the\n-     AARCH64_EXTRA_TUNE_NO_LDP_STP_QREGS tuning parameter and TARGET_SIMD.  */\n-  const int copy_limit = ((aarch64_tune_params.extra_tuning_flags\n-\t\t\t   & AARCH64_EXTRA_TUNE_NO_LDP_STP_QREGS)\n-\t\t\t  || !TARGET_SIMD)\n-\t\t\t ? GET_MODE_BITSIZE (TImode) : 256;\n+  /* Convert size to bits to make the rest of the code simpler.  */\n+  int n = size * BITS_PER_UNIT;\n \n   while (n > 0)\n     {\n       /* Find the largest mode in which to do the copy in without over reading\n \t or writing.  */\n       opt_scalar_int_mode mode_iter;\n       FOR_EACH_MODE_IN_CLASS (mode_iter, MODE_INT)\n-\tif (GET_MODE_BITSIZE (mode_iter.require ()) <= MIN (n, copy_limit))\n+\tif (GET_MODE_BITSIZE (mode_iter.require ()) <= MIN (n, copy_bits))\n \t  cur_mode = mode_iter.require ();\n \n       gcc_assert (cur_mode != BLKmode);\n \n       mode_bits = GET_MODE_BITSIZE (cur_mode).to_constant ();\n+\n+      /* Prefer Q-register accesses for the last bytes.  */\n+      if (mode_bits == 128 && copy_bits == 256)\n+\tcur_mode = V4SImode;\n+\n       aarch64_copy_one_block_and_progress_pointers (&src, &dst, cur_mode);\n \n       n -= mode_bits;\n \n-      /* Do certain trailing copies as overlapping if it's going to be\n-\t cheaper.  i.e. less instructions to do so.  For instance doing a 15\n-\t byte copy it's more efficient to do two overlapping 8 byte copies than\n-\t 8 + 6 + 1.  */\n-      if (n > 0 && n <= 8 * BITS_PER_UNIT)\n+      /* Emit trailing copies using overlapping unaligned accesses - this is\n+\t smaller and faster.  */\n+      if (n > 0 && n < copy_bits / 2)\n \t{\n-\t  next_mode = smallest_mode_for_size (n, MODE_INT);\n+\t  machine_mode next_mode = smallest_mode_for_size (n, MODE_INT);\n \t  int n_bits = GET_MODE_BITSIZE (next_mode).to_constant ();\n+\t  gcc_assert (n_bits <= mode_bits);\n \t  src = aarch64_move_pointer (src, (n - n_bits) / BITS_PER_UNIT);\n \t  dst = aarch64_move_pointer (dst, (n - n_bits) / BITS_PER_UNIT);\n \t  n = n_bits;"}]}