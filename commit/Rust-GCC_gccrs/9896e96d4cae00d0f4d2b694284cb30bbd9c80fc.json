{"sha": "9896e96d4cae00d0f4d2b694284cb30bbd9c80fc", "node_id": "C_kwDOANBUbNoAKDk4OTZlOTZkNGNhZTAwZDBmNGQyYjY5NDI4NGNiMzBiYmQ5YzgwZmM", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2022-01-14T11:04:59Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2022-01-14T11:04:59Z"}, "message": "forwprop: Canonicalize atomic fetch_op op x to op_fetch or vice versa [PR98737]\n\nWhen writing the PR98737 fix, I've handled just the case where people\nuse __atomic_op_fetch (p, x, y) etc.\nBut some people actually use the other builtins, like\n__atomic_fetch_op (p, x, y) op x.\nThe following patch canonicalizes the latter to the former and vice versa\nwhen possible if the result of the builtin is a single use and if\nthat use is a cast with same precision, also that cast's lhs has a single\nuse.\nFor all ops of +, -, &, | and ^ we can do those\n__atomic_fetch_op (p, x, y) op x -> __atomic_op_fetch (p, x, y)\n(and __sync too) opts, but cases of INTEGER_CST and SSA_NAME x\nbehave differently.  For INTEGER_CST, typically - x is\ncanonicalized to + (-x), while for SSA_NAME we need to handle various\ncasts, which sometimes happen on the second argument of the builtin\n(there can be even two subsequent casts for char/short due to the\npromotions we do) and there can be a cast on the argument of op too.\nAnd all ops but - are commutative.\nFor the other direction, i.e.\n__atomic_op_fetch (p, x, y) rop x -> __atomic_fetch_op (p, x, y)\nwe can't handle op of & and |, those aren't reversible, for\nop + rop is -, for - rop is + and for ^ rop is ^, otherwise the same\nstuff as above applies.\nAnd, there is another case, we canonicalize\nx - y == 0 (or != 0) and x ^ y == 0 (or != 0) to x == y (or x != y)\nand for constant y x + y == 0 (or != 0) to x == -y (or != -y),\nso the patch also virtually undoes those canonicalizations, because\ne.g. for the earlier PR98737 patch but even generally, it is better\nif a result of atomic op fetch is compared against 0 than doing\natomic fetch op and compare it to some variable or non-zero constant.\nAs for debug info, for non-reversible operations (& and |) the patch\nresets debug stmts if there are any, for -fnon-call-exceptions too\n(didn't want to include debug temps right before all uses), but\notherwise it emits (on richi's request) the reverse operation from\nthe result as a new setter of the old lhs, so that later DCE fixes\nup the debug info.\n\nOn the emitted assembly for the testcases which are fairly large,\nI see substantial decreases of the *.s size:\n-rw-rw-r--. 1 jakub jakub 116897 Jan 13 09:58 pr98737-1.svanilla\n-rw-rw-r--. 1 jakub jakub  93861 Jan 13 09:57 pr98737-1.spatched\n-rw-rw-r--. 1 jakub jakub  70257 Jan 13 09:57 pr98737-2.svanilla\n-rw-rw-r--. 1 jakub jakub  67537 Jan 13 09:57 pr98737-2.spatched\nThere are some functions where due to RA we get one more instruction\nthan previously, but most of them are smaller even when not hitting\nthe PR98737 previous patch's optimizations.\n\n2022-01-14  Jakub Jelinek  <jakub@redhat.com>\n\n\tPR target/98737\n\t* tree-ssa-forwprop.c (simplify_builtin_call): Canonicalize\n\t__atomic_fetch_op (p, x, y) op x into __atomic_op_fetch (p, x, y)\n\tand __atomic_op_fetch (p, x, y) iop x into\n\t__atomic_fetch_op (p, x, y).\n\n\t* gcc.dg/tree-ssa/pr98737-1.c: New test.\n\t* gcc.dg/tree-ssa/pr98737-2.c: New test.", "tree": {"sha": "86b4a66cd6aab2a7a09c07b27c61f2c998d3dafb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/86b4a66cd6aab2a7a09c07b27c61f2c998d3dafb"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/9896e96d4cae00d0f4d2b694284cb30bbd9c80fc", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9896e96d4cae00d0f4d2b694284cb30bbd9c80fc", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9896e96d4cae00d0f4d2b694284cb30bbd9c80fc", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9896e96d4cae00d0f4d2b694284cb30bbd9c80fc/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "68a650ba57a446fef31722cc2d5ac0752dc1b531", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/68a650ba57a446fef31722cc2d5ac0752dc1b531", "html_url": "https://github.com/Rust-GCC/gccrs/commit/68a650ba57a446fef31722cc2d5ac0752dc1b531"}], "stats": {"total": 584, "additions": 583, "deletions": 1}, "files": [{"sha": "e313a7fa79dd0c6750b2c452134dfcc145f12ee9", "filename": "gcc/testsuite/gcc.dg/tree-ssa/pr98737-1.c", "status": "added", "additions": 148, "deletions": 0, "changes": 148, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9896e96d4cae00d0f4d2b694284cb30bbd9c80fc/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr98737-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9896e96d4cae00d0f4d2b694284cb30bbd9c80fc/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr98737-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr98737-1.c?ref=9896e96d4cae00d0f4d2b694284cb30bbd9c80fc", "patch": "@@ -0,0 +1,148 @@\n+/* PR target/98737 */\n+/* { dg-do compile { target i?86-*-* x86_64-*-* powerpc*-*-* aarch64*-*-* } } */\n+/* { dg-options \"-O2 -fdump-tree-optimized -fcompare-debug\" } */\n+/* { dg-additional-options \"-march=i686\" { target ia32 } } */\n+/* { dg-final { scan-tree-dump-not \"__atomic_fetch_\" \"optimized\" } } */\n+/* { dg-final { scan-tree-dump-not \"__sync_fetch_and_\" \"optimized\" } } */\n+\n+typedef signed char schar;\n+typedef unsigned long ulong;\n+typedef unsigned int uint;\n+typedef unsigned short ushort;\n+typedef unsigned char uchar;\n+long vlong;\n+int vint;\n+short vshort;\n+schar vschar;\n+ulong vulong;\n+uint vuint;\n+ushort vushort;\n+uchar vuchar;\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (t x)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  ut z = f (&v##t, x, ##__VA_ARGS__);\t\t\\\n+  t w = (t) z;\t\t\t\t\t\\\n+  return w o x;\t\t\t\t\t\\\n+}\n+#define B(n, f, o, ...) \\\n+  A(n##0, long, ulong, f, o, ##__VA_ARGS__)\t\\\n+  A(n##1, int, uint, f, o, ##__VA_ARGS__)\t\\\n+  A(n##2, short, ushort, f, o, ##__VA_ARGS__)\t\\\n+  A(n##3, schar, uchar, f, o, ##__VA_ARGS__)\t\\\n+  A(n##4, ulong, ulong, f, o, ##__VA_ARGS__)\t\\\n+  A(n##5, uint, uint, f, o, ##__VA_ARGS__)\t\\\n+  A(n##6, ushort, ushort, f, o, ##__VA_ARGS__)\t\\\n+  A(n##7, uchar, uchar, f, o, ##__VA_ARGS__)\n+\n+B(00, __atomic_fetch_add, +, __ATOMIC_RELAXED)\n+B(01, __atomic_fetch_sub, -, __ATOMIC_RELAXED)\n+B(02, __atomic_fetch_and, &, __ATOMIC_RELAXED)\n+B(03, __atomic_fetch_xor, ^, __ATOMIC_RELAXED)\n+B(04, __atomic_fetch_or, |, __ATOMIC_RELAXED)\n+B(05, __sync_fetch_and_add, +)\n+B(06, __sync_fetch_and_sub, -)\n+B(07, __sync_fetch_and_and, &)\n+B(08, __sync_fetch_and_xor, ^)\n+B(09, __sync_fetch_and_or, |)\n+\n+#undef A\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (void)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  ut z = f (&v##t, 42, ##__VA_ARGS__);\t\t\\\n+  t w = (t) z;\t\t\t\t\t\\\n+  return w o 42;\t\t\t\t\\\n+}\n+\n+B(10, __atomic_fetch_add, +, __ATOMIC_RELAXED)\n+B(11, __atomic_fetch_sub, -, __ATOMIC_RELAXED)\n+B(12, __atomic_fetch_and, &, __ATOMIC_RELAXED)\n+B(13, __atomic_fetch_xor, ^, __ATOMIC_RELAXED)\n+B(14, __atomic_fetch_or, |, __ATOMIC_RELAXED)\n+B(15, __sync_fetch_and_add, +)\n+B(16, __sync_fetch_and_sub, -)\n+B(17, __sync_fetch_and_and, &)\n+B(18, __sync_fetch_and_xor, ^)\n+B(19, __sync_fetch_and_or, |)\n+\n+#undef A\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (t x)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  ut z = f (&v##t, x, ##__VA_ARGS__);\t\t\\\n+  t w = (t) z;\t\t\t\t\t\\\n+  t v = w o x;\t\t\t\t\t\\\n+  return v == 0;\t\t\t\t\\\n+}\n+\n+B(20, __atomic_fetch_add, +, __ATOMIC_RELAXED)\n+B(21, __atomic_fetch_sub, -, __ATOMIC_RELAXED)\n+B(22, __atomic_fetch_and, &, __ATOMIC_RELAXED)\n+B(23, __atomic_fetch_xor, ^, __ATOMIC_RELAXED)\n+B(24, __atomic_fetch_or, |, __ATOMIC_RELAXED)\n+B(25, __sync_fetch_and_add, +)\n+B(26, __sync_fetch_and_sub, -)\n+B(27, __sync_fetch_and_and, &)\n+B(28, __sync_fetch_and_xor, ^)\n+B(29, __sync_fetch_and_or, |)\n+\n+#undef A\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (void)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  ut z = f (&v##t, 42, ##__VA_ARGS__);\t\t\\\n+  t w = (t) z;\t\t\t\t\t\\\n+  t v = w o 42;\t\t\t\t\t\\\n+  return v != 0;\t\t\t\t\\\n+}\n+\n+B(30, __atomic_fetch_add, +, __ATOMIC_RELAXED)\n+B(31, __atomic_fetch_sub, -, __ATOMIC_RELAXED)\n+B(32, __atomic_fetch_and, &, __ATOMIC_RELAXED)\n+B(33, __atomic_fetch_xor, ^, __ATOMIC_RELAXED)\n+B(34, __atomic_fetch_or, |, __ATOMIC_RELAXED)\n+B(35, __sync_fetch_and_add, +)\n+B(36, __sync_fetch_and_sub, -)\n+B(37, __sync_fetch_and_and, &)\n+B(38, __sync_fetch_and_xor, ^)\n+B(39, __sync_fetch_and_or, |)\n+\n+#undef A\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (t x)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  return (t) (((t) f (&v##t, x, ##__VA_ARGS__))\t\\\n+\t      o x) != 0;\t\t\t\\\n+}\n+\n+B(40, __atomic_fetch_add, +, __ATOMIC_RELAXED)\n+B(41, __atomic_fetch_sub, -, __ATOMIC_RELAXED)\n+B(42, __atomic_fetch_and, &, __ATOMIC_RELAXED)\n+B(43, __atomic_fetch_xor, ^, __ATOMIC_RELAXED)\n+B(44, __atomic_fetch_or, |, __ATOMIC_RELAXED)\n+B(45, __sync_fetch_and_add, +)\n+B(46, __sync_fetch_and_sub, -)\n+B(47, __sync_fetch_and_and, &)\n+B(48, __sync_fetch_and_xor, ^)\n+B(49, __sync_fetch_and_or, |)\n+\n+#undef A\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (void)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  return (t) (((t) f (&v##t, 42, ##__VA_ARGS__))\\\n+\t      o 42) == 0;\t\t\t\\\n+}\n+\n+B(50, __atomic_fetch_add, +, __ATOMIC_RELAXED)\n+B(51, __atomic_fetch_sub, -, __ATOMIC_RELAXED)\n+B(52, __atomic_fetch_and, &, __ATOMIC_RELAXED)\n+B(53, __atomic_fetch_xor, ^, __ATOMIC_RELAXED)\n+/* (whatever | 42) == 0 is 0, so we can't test this.  */\n+/* B(54, __atomic_fetch_or, |, __ATOMIC_RELAXED) */\n+B(55, __sync_fetch_and_add, +)\n+B(56, __sync_fetch_and_sub, -)\n+B(57, __sync_fetch_and_and, &)\n+B(58, __sync_fetch_and_xor, ^)\n+/* B(59, __sync_fetch_and_or, |) */"}, {"sha": "09149bcd4fee42ccdbc4feedd766da2a4d3cabb7", "filename": "gcc/testsuite/gcc.dg/tree-ssa/pr98737-2.c", "status": "added", "additions": 123, "deletions": 0, "changes": 123, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9896e96d4cae00d0f4d2b694284cb30bbd9c80fc/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr98737-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9896e96d4cae00d0f4d2b694284cb30bbd9c80fc/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr98737-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr98737-2.c?ref=9896e96d4cae00d0f4d2b694284cb30bbd9c80fc", "patch": "@@ -0,0 +1,123 @@\n+/* PR target/98737 */\n+/* { dg-do compile { target i?86-*-* x86_64-*-* powerpc*-*-* aarch64*-*-* } } */\n+/* { dg-options \"-O2 -fdump-tree-optimized -fcompare-debug\" } */\n+/* { dg-additional-options \"-march=i686\" { target ia32 } } */\n+/* { dg-final { scan-tree-dump-not \"__atomic_\\[^f]\" \"optimized\" } } */\n+/* { dg-final { scan-tree-dump-not \"__sync_\\[^f]\" \"optimized\" } } */\n+\n+typedef signed char schar;\n+typedef unsigned long ulong;\n+typedef unsigned int uint;\n+typedef unsigned short ushort;\n+typedef unsigned char uchar;\n+long vlong;\n+int vint;\n+short vshort;\n+schar vschar;\n+ulong vulong;\n+uint vuint;\n+ushort vushort;\n+uchar vuchar;\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (t x)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  ut z = f (&v##t, x, ##__VA_ARGS__);\t\t\\\n+  t w = (t) z;\t\t\t\t\t\\\n+  return w o x;\t\t\t\t\t\\\n+}\n+#define B(n, f, o, ...) \\\n+  A(n##0, long, ulong, f, o, ##__VA_ARGS__)\t\\\n+  A(n##1, int, uint, f, o, ##__VA_ARGS__)\t\\\n+  A(n##2, short, ushort, f, o, ##__VA_ARGS__)\t\\\n+  A(n##3, schar, uchar, f, o, ##__VA_ARGS__)\t\\\n+  A(n##4, ulong, ulong, f, o, ##__VA_ARGS__)\t\\\n+  A(n##5, uint, uint, f, o, ##__VA_ARGS__)\t\\\n+  A(n##6, ushort, ushort, f, o, ##__VA_ARGS__)\t\\\n+  A(n##7, uchar, uchar, f, o, ##__VA_ARGS__)\n+\n+B(00, __atomic_add_fetch, -, __ATOMIC_RELAXED)\n+B(01, __atomic_sub_fetch, +, __ATOMIC_RELAXED)\n+B(03, __atomic_xor_fetch, ^, __ATOMIC_RELAXED)\n+B(05, __sync_add_and_fetch, -)\n+B(06, __sync_sub_and_fetch, +)\n+B(08, __sync_xor_and_fetch, ^)\n+\n+#undef A\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (void)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  ut z = f (&v##t, 42, ##__VA_ARGS__);\t\t\\\n+  t w = (t) z;\t\t\t\t\t\\\n+  return w o 42;\t\t\t\t\\\n+}\n+\n+B(10, __atomic_add_fetch, -, __ATOMIC_RELAXED)\n+B(11, __atomic_sub_fetch, +, __ATOMIC_RELAXED)\n+B(13, __atomic_xor_fetch, ^, __ATOMIC_RELAXED)\n+B(15, __sync_add_and_fetch, -)\n+B(16, __sync_sub_and_fetch, +)\n+B(18, __sync_xor_and_fetch, ^)\n+\n+#undef A\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (t x)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  ut z = f (&v##t, x, ##__VA_ARGS__);\t\t\\\n+  t w = (t) z;\t\t\t\t\t\\\n+  t v = w o x;\t\t\t\t\t\\\n+  return v == 0;\t\t\t\t\\\n+}\n+\n+B(20, __atomic_add_fetch, -, __ATOMIC_RELAXED)\n+B(21, __atomic_sub_fetch, +, __ATOMIC_RELAXED)\n+B(23, __atomic_xor_fetch, ^, __ATOMIC_RELAXED)\n+B(25, __sync_add_and_fetch, -)\n+B(26, __sync_sub_and_fetch, +)\n+B(28, __sync_xor_and_fetch, ^)\n+\n+#undef A\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (void)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  ut z = f (&v##t, 42, ##__VA_ARGS__);\t\t\\\n+  t w = (t) z;\t\t\t\t\t\\\n+  t v = w o 42;\t\t\t\t\t\\\n+  return v != 0;\t\t\t\t\\\n+}\n+\n+B(30, __atomic_add_fetch, -, __ATOMIC_RELAXED)\n+B(31, __atomic_sub_fetch, +, __ATOMIC_RELAXED)\n+B(33, __atomic_xor_fetch, ^, __ATOMIC_RELAXED)\n+B(35, __sync_add_and_fetch, -)\n+B(36, __sync_sub_and_fetch, +)\n+B(38, __sync_xor_and_fetch, ^)\n+\n+#undef A\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (t x)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  return (t) (((t) f (&v##t, x, ##__VA_ARGS__))\t\\\n+\t      o x) != 0;\t\t\t\\\n+}\n+\n+B(40, __atomic_add_fetch, -, __ATOMIC_RELAXED)\n+B(41, __atomic_sub_fetch, +, __ATOMIC_RELAXED)\n+B(43, __atomic_xor_fetch, ^, __ATOMIC_RELAXED)\n+B(45, __sync_add_and_fetch, -)\n+B(46, __sync_sub_and_fetch, +)\n+B(48, __sync_xor_and_fetch, ^)\n+\n+#undef A\n+#define A(n, t, ut, f, o, ...) \\\n+t fn##n (void)\t\t\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  return (t) (((t) f (&v##t, 42, ##__VA_ARGS__))\\\n+\t      o 42) == 0;\t\t\t\\\n+}\n+\n+B(50, __atomic_add_fetch, -, __ATOMIC_RELAXED)\n+B(51, __atomic_sub_fetch, +, __ATOMIC_RELAXED)\n+B(53, __atomic_xor_fetch, ^, __ATOMIC_RELAXED)\n+B(55, __sync_add_and_fetch, -)\n+B(56, __sync_sub_and_fetch, +)\n+B(58, __sync_xor_and_fetch, ^)"}, {"sha": "709bde6defa72abd1b1d50e47c87992100140ebd", "filename": "gcc/tree-ssa-forwprop.c", "status": "modified", "additions": 312, "deletions": 1, "changes": 313, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9896e96d4cae00d0f4d2b694284cb30bbd9c80fc/gcc%2Ftree-ssa-forwprop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9896e96d4cae00d0f4d2b694284cb30bbd9c80fc/gcc%2Ftree-ssa-forwprop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-forwprop.c?ref=9896e96d4cae00d0f4d2b694284cb30bbd9c80fc", "patch": "@@ -1241,12 +1241,19 @@ constant_pointer_difference (tree p1, tree p2)\n    memset (p + 4, ' ', 3);\n    into\n    memcpy (p, \"abcd   \", 7);\n-   call if the latter can be stored by pieces during expansion.  */\n+   call if the latter can be stored by pieces during expansion.\n+\n+   Also canonicalize __atomic_fetch_op (p, x, y) op x\n+   to __atomic_op_fetch (p, x, y) or\n+   __atomic_op_fetch (p, x, y) iop x\n+   to __atomic_fetch_op (p, x, y) when possible (also __sync).  */\n \n static bool\n simplify_builtin_call (gimple_stmt_iterator *gsi_p, tree callee2)\n {\n   gimple *stmt1, *stmt2 = gsi_stmt (*gsi_p);\n+  enum built_in_function other_atomic = END_BUILTINS;\n+  enum tree_code atomic_op = ERROR_MARK;\n   tree vuse = gimple_vuse (stmt2);\n   if (vuse == NULL)\n     return false;\n@@ -1448,6 +1455,310 @@ simplify_builtin_call (gimple_stmt_iterator *gsi_p, tree callee2)\n \t    }\n \t}\n       break;\n+\n+ #define CASE_ATOMIC(NAME, OTHER, OP) \\\n+    case BUILT_IN_##NAME##_1:\t\t\t\t\t\t\\\n+    case BUILT_IN_##NAME##_2:\t\t\t\t\t\t\\\n+    case BUILT_IN_##NAME##_4:\t\t\t\t\t\t\\\n+    case BUILT_IN_##NAME##_8:\t\t\t\t\t\t\\\n+    case BUILT_IN_##NAME##_16:\t\t\t\t\t\t\\\n+      atomic_op = OP;\t\t\t\t\t\t\t\\\n+      other_atomic\t\t\t\t\t\t\t\\\n+\t= (enum built_in_function) (BUILT_IN_##OTHER##_1\t\t\\\n+\t\t\t\t    + (DECL_FUNCTION_CODE (callee2)\t\\\n+\t\t\t\t       - BUILT_IN_##NAME##_1));\t\t\\\n+      goto handle_atomic_fetch_op;\n+\n+    CASE_ATOMIC (ATOMIC_FETCH_ADD, ATOMIC_ADD_FETCH, PLUS_EXPR)\n+    CASE_ATOMIC (ATOMIC_FETCH_SUB, ATOMIC_SUB_FETCH, MINUS_EXPR)\n+    CASE_ATOMIC (ATOMIC_FETCH_AND, ATOMIC_AND_FETCH, BIT_AND_EXPR)\n+    CASE_ATOMIC (ATOMIC_FETCH_XOR, ATOMIC_XOR_FETCH, BIT_XOR_EXPR)\n+    CASE_ATOMIC (ATOMIC_FETCH_OR, ATOMIC_OR_FETCH, BIT_IOR_EXPR)\n+\n+    CASE_ATOMIC (SYNC_FETCH_AND_ADD, SYNC_ADD_AND_FETCH, PLUS_EXPR)\n+    CASE_ATOMIC (SYNC_FETCH_AND_SUB, SYNC_SUB_AND_FETCH, MINUS_EXPR)\n+    CASE_ATOMIC (SYNC_FETCH_AND_AND, SYNC_AND_AND_FETCH, BIT_AND_EXPR)\n+    CASE_ATOMIC (SYNC_FETCH_AND_XOR, SYNC_XOR_AND_FETCH, BIT_XOR_EXPR)\n+    CASE_ATOMIC (SYNC_FETCH_AND_OR, SYNC_OR_AND_FETCH, BIT_IOR_EXPR)\n+\n+    CASE_ATOMIC (ATOMIC_ADD_FETCH, ATOMIC_FETCH_ADD, MINUS_EXPR)\n+    CASE_ATOMIC (ATOMIC_SUB_FETCH, ATOMIC_FETCH_SUB, PLUS_EXPR)\n+    CASE_ATOMIC (ATOMIC_XOR_FETCH, ATOMIC_FETCH_XOR, BIT_XOR_EXPR)\n+\n+    CASE_ATOMIC (SYNC_ADD_AND_FETCH, SYNC_FETCH_AND_ADD, MINUS_EXPR)\n+    CASE_ATOMIC (SYNC_SUB_AND_FETCH, SYNC_FETCH_AND_SUB, PLUS_EXPR)\n+    CASE_ATOMIC (SYNC_XOR_AND_FETCH, SYNC_FETCH_AND_XOR, BIT_XOR_EXPR)\n+\n+#undef CASE_ATOMIC\n+\n+    handle_atomic_fetch_op:\n+      if (gimple_call_num_args (stmt2) >= 2 && gimple_call_lhs (stmt2))\n+\t{\n+\t  tree lhs2 = gimple_call_lhs (stmt2), lhsc = lhs2;\n+\t  tree arg = gimple_call_arg (stmt2, 1);\n+\t  gimple *use_stmt, *cast_stmt = NULL;\n+\t  use_operand_p use_p;\n+\t  tree ndecl = builtin_decl_explicit (other_atomic);\n+\n+\t  if (ndecl == NULL_TREE || !single_imm_use (lhs2, &use_p, &use_stmt))\n+\t    break;\n+\n+\t  if (gimple_assign_cast_p (use_stmt))\n+\t    {\n+\t      cast_stmt = use_stmt;\n+\t      lhsc = gimple_assign_lhs (cast_stmt);\n+\t      if (lhsc == NULL_TREE\n+\t\t  || !INTEGRAL_TYPE_P (TREE_TYPE (lhsc))\n+\t\t  || (TYPE_PRECISION (TREE_TYPE (lhsc))\n+\t\t      != TYPE_PRECISION (TREE_TYPE (lhs2)))\n+\t\t  || !single_imm_use (lhsc, &use_p, &use_stmt))\n+\t\t{\n+\t\t  use_stmt = cast_stmt;\n+\t\t  cast_stmt = NULL;\n+\t\t  lhsc = lhs2;\n+\t\t}\n+\t    }\n+\n+\t  bool ok = false;\n+\t  tree oarg = NULL_TREE;\n+\t  enum tree_code ccode = ERROR_MARK;\n+\t  tree crhs1 = NULL_TREE, crhs2 = NULL_TREE;\n+\t  if (is_gimple_assign (use_stmt)\n+\t      && gimple_assign_rhs_code (use_stmt) == atomic_op)\n+\t    {\n+\t      if (gimple_assign_rhs1 (use_stmt) == lhsc)\n+\t\toarg = gimple_assign_rhs2 (use_stmt);\n+\t      else if (atomic_op != MINUS_EXPR)\n+\t\toarg = gimple_assign_rhs1 (use_stmt);\n+\t    }\n+\t  else if (atomic_op == MINUS_EXPR\n+\t\t   && is_gimple_assign (use_stmt)\n+\t\t   && gimple_assign_rhs_code (use_stmt) == PLUS_EXPR\n+\t\t   && TREE_CODE (arg) == INTEGER_CST\n+\t\t   && (TREE_CODE (gimple_assign_rhs2 (use_stmt))\n+\t\t       == INTEGER_CST))\n+\t    {\n+\t      tree a = fold_convert (TREE_TYPE (lhs2), arg);\n+\t      tree o = fold_convert (TREE_TYPE (lhs2),\n+\t\t\t\t     gimple_assign_rhs2 (use_stmt));\n+\t      if (wi::to_wide (a) == wi::neg (wi::to_wide (o)))\n+\t\tok = true;\n+\t    }\n+\t  else if (atomic_op == BIT_AND_EXPR || atomic_op == BIT_IOR_EXPR)\n+\t    ;\n+\t  else if (gimple_code (use_stmt) == GIMPLE_COND)\n+\t    {\n+\t      ccode = gimple_cond_code (use_stmt);\n+\t      crhs1 = gimple_cond_lhs (use_stmt);\n+\t      crhs2 = gimple_cond_rhs (use_stmt);\n+\t    }\n+\t  else if (is_gimple_assign (use_stmt))\n+\t    {\n+\t      if (gimple_assign_rhs_class (use_stmt) == GIMPLE_BINARY_RHS)\n+\t\t{\n+\t\t  ccode = gimple_assign_rhs_code (use_stmt);\n+\t\t  crhs1 = gimple_assign_rhs1 (use_stmt);\n+\t\t  crhs2 = gimple_assign_rhs2 (use_stmt);\n+\t\t}\n+\t      else if (gimple_assign_rhs_code (use_stmt) == COND_EXPR)\n+\t\t{\n+\t\t  tree cond = gimple_assign_rhs1 (use_stmt);\n+\t\t  if (COMPARISON_CLASS_P (cond))\n+\t\t    {\n+\t\t      ccode = TREE_CODE (cond);\n+\t\t      crhs1 = TREE_OPERAND (cond, 0);\n+\t\t      crhs2 = TREE_OPERAND (cond, 1);\n+\t\t    }\n+\t\t}\n+\t    }\n+\t  if (ccode == EQ_EXPR || ccode == NE_EXPR)\n+\t    {\n+\t      /* Deal with x - y == 0 or x ^ y == 0\n+\t\t being optimized into x == y and x + cst == 0\n+\t\t into x == -cst.  */\n+\t      tree o = NULL_TREE;\n+\t      if (crhs1 == lhsc)\n+\t\to = crhs2;\n+\t      else if (crhs2 == lhsc)\n+\t\to = crhs1;\n+\t      if (o && atomic_op != PLUS_EXPR)\n+\t\toarg = o;\n+\t      else if (o\n+\t\t       && TREE_CODE (o) == INTEGER_CST\n+\t\t       && TREE_CODE (arg) == INTEGER_CST)\n+\t\t{\n+\t\t  tree a = fold_convert (TREE_TYPE (lhs2), arg);\n+\t\t  o = fold_convert (TREE_TYPE (lhs2), o);\n+\t\t  if (wi::to_wide (a) == wi::neg (wi::to_wide (o)))\n+\t\t    ok = true;\n+\t\t}\n+\t    }\n+\t  if (oarg && !ok)\n+\t    {\n+\t      if (operand_equal_p (arg, oarg, 0))\n+\t\tok = true;\n+\t      else if (TREE_CODE (arg) == SSA_NAME\n+\t\t       && TREE_CODE (oarg) == SSA_NAME)\n+\t\t{\n+\t\t  tree oarg2 = oarg;\n+\t\t  if (gimple_assign_cast_p (SSA_NAME_DEF_STMT (oarg)))\n+\t\t    {\n+\t\t      gimple *g = SSA_NAME_DEF_STMT (oarg);\n+\t\t      oarg2 = gimple_assign_rhs1 (g);\n+\t\t      if (TREE_CODE (oarg2) != SSA_NAME\n+\t\t\t  || !INTEGRAL_TYPE_P (TREE_TYPE (oarg2))\n+\t\t\t  || (TYPE_PRECISION (TREE_TYPE (oarg2))\n+\t\t\t      != TYPE_PRECISION (TREE_TYPE (oarg))))\n+\t\t\toarg2 = oarg;\n+\t\t    }\n+\t\t  if (gimple_assign_cast_p (SSA_NAME_DEF_STMT (arg)))\n+\t\t    {\n+\t\t      gimple *g = SSA_NAME_DEF_STMT (arg);\n+\t\t      tree rhs1 = gimple_assign_rhs1 (g);\n+\t\t      /* Handle e.g.\n+\t\t\t x.0_1 = (long unsigned int) x_4(D);\n+\t\t\t _2 = __atomic_fetch_add_8 (&vlong, x.0_1, 0);\n+\t\t\t _3 = (long int) _2;\n+\t\t\t _7 = x_4(D) + _3;  */\n+\t\t      if (rhs1 == oarg || rhs1 == oarg2)\n+\t\t\tok = true;\n+\t\t      /* Handle e.g.\n+\t\t\t x.18_1 = (short unsigned int) x_5(D);\n+\t\t\t _2 = (int) x.18_1;\n+\t\t\t _3 = __atomic_fetch_xor_2 (&vshort, _2, 0);\n+\t\t\t _4 = (short int) _3;\n+\t\t\t _8 = x_5(D) ^ _4;\n+\t\t\t This happens only for char/short.  */\n+\t\t      else if (TREE_CODE (rhs1) == SSA_NAME\n+\t\t\t       && INTEGRAL_TYPE_P (TREE_TYPE (rhs1))\n+\t\t\t       && (TYPE_PRECISION (TREE_TYPE (rhs1))\n+\t\t\t\t   == TYPE_PRECISION (TREE_TYPE (lhs2))))\n+\t\t\t{\n+\t\t\t  g = SSA_NAME_DEF_STMT (rhs1);\n+\t\t\t  if (gimple_assign_cast_p (g)\n+\t\t\t      && (gimple_assign_rhs1 (g) == oarg\n+\t\t\t\t  || gimple_assign_rhs1 (g) == oarg2))\n+\t\t\t    ok = true;\n+\t\t\t}\n+\t\t    }\n+\t\t  if (!ok && arg == oarg2)\n+\t\t    /* Handle e.g.\n+\t\t       _1 = __sync_fetch_and_add_4 (&v, x_5(D));\n+\t\t       _2 = (int) _1;\n+\t\t       x.0_3 = (int) x_5(D);\n+\t\t       _7 = _2 + x.0_3;  */\n+\t\t    ok = true;\n+\t\t}\n+\t    }\n+\n+\t  if (ok)\n+\t    {\n+\t      tree new_lhs = make_ssa_name (TREE_TYPE (lhs2));\n+\t      gimple_call_set_lhs (stmt2, new_lhs);\n+\t      gimple_call_set_fndecl (stmt2, ndecl);\n+\t      gimple_stmt_iterator gsi = gsi_for_stmt (use_stmt);\n+\t      if (ccode == ERROR_MARK)\n+\t\tgimple_assign_set_rhs_with_ops (&gsi, cast_stmt\n+\t\t\t\t\t\t? NOP_EXPR : SSA_NAME,\n+\t\t\t\t\t\tnew_lhs);\n+\t      else\n+\t\t{\n+\t\t  crhs1 = new_lhs;\n+\t\t  crhs2 = build_zero_cst (TREE_TYPE (lhs2));\n+\t\t  if (gimple_code (use_stmt) == GIMPLE_COND)\n+\t\t    {\n+\t\t      gcond *cond_stmt = as_a <gcond *> (use_stmt);\n+\t\t      gimple_cond_set_lhs (cond_stmt, crhs1);\n+\t\t      gimple_cond_set_rhs (cond_stmt, crhs2);\n+\t\t    }\n+\t\t  else if (gimple_assign_rhs_class (use_stmt)\n+\t\t\t   == GIMPLE_BINARY_RHS)\n+\t\t    {\n+\t\t      gimple_assign_set_rhs1 (use_stmt, crhs1);\n+\t\t      gimple_assign_set_rhs2 (use_stmt, crhs2);\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      gcc_checking_assert (gimple_assign_rhs_code (use_stmt)\n+\t\t\t\t\t   == COND_EXPR);\n+\t\t      tree cond = build2 (ccode, boolean_type_node,\n+\t\t\t\t\t  crhs1, crhs2);\n+\t\t      gimple_assign_set_rhs1 (use_stmt, cond);\n+\t\t    }\n+\t\t}\n+\t      update_stmt (use_stmt);\n+\t      if (atomic_op != BIT_AND_EXPR\n+\t\t  && atomic_op != BIT_IOR_EXPR\n+\t\t  && !stmt_ends_bb_p (stmt2))\n+\t\t{\n+\t\t  /* For the benefit of debug stmts, emit stmt(s) to set\n+\t\t     lhs2 to the value it had from the new builtin.\n+\t\t     E.g. if it was previously:\n+\t\t     lhs2 = __atomic_fetch_add_8 (ptr, arg, 0);\n+\t\t     emit:\n+\t\t     new_lhs = __atomic_add_fetch_8 (ptr, arg, 0);\n+\t\t     lhs2 = new_lhs - arg;\n+\t\t     We also keep cast_stmt if any in the IL for\n+\t\t     the same reasons.\n+\t\t     These stmts will be DCEd later and proper debug info\n+\t\t     will be emitted.\n+\t\t     This is only possible for reversible operations\n+\t\t     (+/-/^) and without -fnon-call-exceptions.  */\n+\t\t  gsi = gsi_for_stmt (stmt2);\n+\t\t  tree type = TREE_TYPE (lhs2);\n+\t\t  if (TREE_CODE (arg) == INTEGER_CST)\n+\t\t    arg = fold_convert (type, arg);\n+\t\t  else if (!useless_type_conversion_p (type, TREE_TYPE (arg)))\n+\t\t    {\n+\t\t      tree narg = make_ssa_name (type);\n+\t\t      gimple *g = gimple_build_assign (narg, NOP_EXPR, arg);\n+\t\t      gsi_insert_after (&gsi, g, GSI_NEW_STMT);\n+\t\t      arg = narg;\n+\t\t    }\n+\t\t  enum tree_code rcode;\n+\t\t  switch (atomic_op)\n+\t\t    {\n+\t\t    case PLUS_EXPR: rcode = MINUS_EXPR; break;\n+\t\t    case MINUS_EXPR: rcode = PLUS_EXPR; break;\n+\t\t    case BIT_XOR_EXPR: rcode = atomic_op; break;\n+\t\t    default: gcc_unreachable ();\n+\t\t    }\n+\t\t  gimple *g = gimple_build_assign (lhs2, rcode, new_lhs, arg);\n+\t\t  gsi_insert_after (&gsi, g, GSI_NEW_STMT);\n+\t\t  update_stmt (stmt2);\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  /* For e.g.\n+\t\t     lhs2 = __atomic_fetch_or_8 (ptr, arg, 0);\n+\t\t     after we change it to\n+\t\t     new_lhs = __atomic_or_fetch_8 (ptr, arg, 0);\n+\t\t     there is no way to find out the lhs2 value (i.e.\n+\t\t     what the atomic memory contained before the operation),\n+\t\t     values of some bits are lost.  We have checked earlier\n+\t\t     that we don't have any non-debug users except for what\n+\t\t     we are already changing, so we need to reset the\n+\t\t     debug stmts and remove the cast_stmt if any.  */\n+\t\t  imm_use_iterator iter;\n+\t\t  FOR_EACH_IMM_USE_STMT (use_stmt, iter, lhs2)\n+\t\t    if (use_stmt != cast_stmt)\n+\t\t      {\n+\t\t\tgcc_assert (is_gimple_debug (use_stmt));\n+\t\t\tgimple_debug_bind_reset_value (use_stmt);\n+\t\t\tupdate_stmt (use_stmt);\n+\t\t      }\n+\t\t  if (cast_stmt)\n+\t\t    {\n+\t\t      gsi = gsi_for_stmt (cast_stmt);\n+\t\t      gsi_remove (&gsi, true);\n+\t\t    }\n+\t\t  update_stmt (stmt2);\n+\t\t  release_ssa_name (lhs2);\n+\t\t}\n+\t    }\n+\t}\n+      break;\n+\n     default:\n       break;\n     }"}]}