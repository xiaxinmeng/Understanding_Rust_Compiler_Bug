{"sha": "f263b26e17c8e39aca86025e0c8bc4f7df74e5bd", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZjI2M2IyNmUxN2M4ZTM5YWNhODYwMjVlMGM4YmM0ZjdkZjc0ZTViZA==", "commit": {"author": {"name": "Benjamin Kosnik", "email": "bkoz@redhat.com", "date": "2004-02-23T15:41:43Z"}, "committer": {"name": "Benjamin Kosnik", "email": "bkoz@gcc.gnu.org", "date": "2004-02-23T15:41:43Z"}, "message": "malloc_allocator.h: Add operators ==, !=.\n\n\n2004-02-20  Benjamin Kosnik  <bkoz@redhat.com>\n\n\t* include/ext/malloc_allocator.h: Add operators ==, !=.\n\t* include/ext/new_allocator.h: Add operators ==, !=.\n\t* include/ext/mt_allocator.h (__mt_alloc::tune): New.\n\t(__mt_alloc::_S_get_options): New.\n\t(__mt_alloc::_S_set_options): New.\n\t(__mt_alloc::_S_thread_key_destr): To _S_destroy_thread_key.\n\t(__mt_alloc::_S_no_of_bins): To _S_bin_size.\n\tMove functions out of line, simplify, format.\n\t* src/allocator.cc: Simplify explicit instantiations.\n\t* include/bits/allocator.h: Tweak.\n\nFrom-SVN: r78314", "tree": {"sha": "2d8f90fd6cd3a49b62e65d0c09e6f383d1fa716e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/2d8f90fd6cd3a49b62e65d0c09e6f383d1fa716e"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/comments", "author": null, "committer": null, "parents": [{"sha": "2b0c1c562350329c766fdf39ab176f3d30b9450c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2b0c1c562350329c766fdf39ab176f3d30b9450c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2b0c1c562350329c766fdf39ab176f3d30b9450c"}], "stats": {"total": 1143, "additions": 521, "deletions": 622}, "files": [{"sha": "9a16649426d3860ec37fec18780982e07d7a4d64", "filename": "libstdc++-v3/ChangeLog", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2FChangeLog?ref=f263b26e17c8e39aca86025e0c8bc4f7df74e5bd", "patch": "@@ -1,3 +1,16 @@\n+2004-02-20  Benjamin Kosnik  <bkoz@redhat.com>\n+\n+\t* include/ext/malloc_allocator.h: Add operators ==, !=.\n+\t* include/ext/new_allocator.h: Add operators ==, !=.\n+\t* include/ext/mt_allocator.h (__mt_alloc::tune): New.\n+\t(__mt_alloc::_S_get_options): New.\n+\t(__mt_alloc::_S_set_options): New.\t\n+\t(__mt_alloc::_S_thread_key_destr): To _S_destroy_thread_key.\n+\t(__mt_alloc::_S_no_of_bins): To _S_bin_size.\n+\tMove functions out of line, simplify, format.\n+\t* src/allocator.cc: Simplify explicit instantiations.\n+\t* include/bits/allocator.h: Tweak.\n+\t\n 2004-02-22  Paolo Carlini  <pcarlini@suse.de>\n \n \t* include/bits/locale_facets.tcc (money_put<>::_M_insert):"}, {"sha": "eca50c84edcc7ceb13afe361784f8e0bfff27f8e", "filename": "libstdc++-v3/include/bits/allocator.h", "status": "modified", "additions": 6, "deletions": 8, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fallocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fallocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fallocator.h?ref=f263b26e17c8e39aca86025e0c8bc4f7df74e5bd", "patch": "@@ -48,15 +48,13 @@\n #ifndef _ALLOCATOR_H\n #define _ALLOCATOR_H 1\n \n-#if 1\n-# include <ext/new_allocator.h>\n-# define __glibcxx_default_allocator  __gnu_cxx::new_allocator\n-#endif\n+// Define the base class to std::allocator.\n \n-#if 0\n-# include <ext/pool_allocator.h>\n-# define __glibcxx_default_allocator  __gnu_cxx::__pool_alloc\n-#endif\n+#include <ext/new_allocator.h>\n+#define __glibcxx_default_allocator  __gnu_cxx::new_allocator\n+\n+//#include <ext/mt_allocator.h>\n+//#define __glibcxx_default_allocator  __gnu_cxx::__mt_alloc\n \n namespace std\n {"}, {"sha": "dc6ae4fb51c89c9e558dbc4bd825d369f73492eb", "filename": "libstdc++-v3/include/ext/malloc_allocator.h", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2Finclude%2Fext%2Fmalloc_allocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2Finclude%2Fext%2Fmalloc_allocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fext%2Fmalloc_allocator.h?ref=f263b26e17c8e39aca86025e0c8bc4f7df74e5bd", "patch": "@@ -98,6 +98,16 @@ namespace __gnu_cxx\n       void \n       destroy(pointer __p) { __p->~_Tp(); }\n     };\n+\n+  template<typename _Tp>\n+    inline bool\n+    operator==(const malloc_allocator<_Tp>&, const malloc_allocator<_Tp>&)\n+    { return true; }\n+  \n+  template<typename _Tp>\n+    inline bool\n+    operator!=(const malloc_allocator<_Tp>&, const malloc_allocator<_Tp>&)\n+    { return false; }\n } // namespace __gnu_cxx\n \n #endif"}, {"sha": "7000c05fe0b72346ca33b8e84fec1f7c81499ad2", "filename": "libstdc++-v3/include/ext/mt_allocator.h", "status": "modified", "additions": 481, "deletions": 601, "changes": 1082, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2Finclude%2Fext%2Fmt_allocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2Finclude%2Fext%2Fmt_allocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fext%2Fmt_allocator.h?ref=f263b26e17c8e39aca86025e0c8bc4f7df74e5bd", "patch": "@@ -50,13 +50,8 @@ namespace __gnu_cxx\n    *  the per thread freelist sizes (by returning excess back to\n    *  \"global\").\n    *\n-   *  Usage examples:\n-   *  @code\n-   *    vector<int, __gnu_cxx::__mt_alloc<int> > v1;\n-   *\n-   *    typedef __gnu_cxx::__mt_alloc<char> > string_allocator;\n-   *    std::basic_string<char, std::char_traits<char>, string_allocator> s1;\n-   *  @endcode\n+   *  Further details:\n+   *  http://gcc.gnu.org/onlinedocs/libstdc++/ext/mt_allocator.html\n    */\n   template<typename _Tp>\n     class __mt_alloc\n@@ -79,13 +74,13 @@ namespace __gnu_cxx\n \t// XXX\n       }\n \n-      __mt_alloc(const __mt_alloc&) throw()\n+      __mt_alloc(const __mt_alloc&) throw() \n       {\n \t// XXX\n       }\n \n       template<typename _Tp1>\n-        __mt_alloc(const __mt_alloc<_Tp1>&) throw()\n+        __mt_alloc(const __mt_alloc<_Tp1>& obj) throw()  \n         {\n \t  // XXX\n \t}\n@@ -111,499 +106,466 @@ namespace __gnu_cxx\n       void \n       destroy(pointer __p) { __p->~_Tp(); }\n \n+      pointer\n+      allocate(size_t __n, const void* = 0);\n+\n+      void\n+      deallocate(pointer __p, size_type __n);\n+\n+      // Variables used to configure the behavior of the allocator,\n+      // assigned and explained in detail below.\n+      struct tune\n+      {\n+\t// Allocation requests (after round-up to power of 2) below\n+\t// this value will be handled by the allocator. A raw new/\n+\t// call will be used for requests larger than this value.\n+\tsize_t\t_M_max_bytes; \n+\n+\t// In order to avoid fragmenting and minimize the number of\n+\t// new() calls we always request new memory using this\n+\t// value. Based on previous discussions on the libstdc++\n+\t// mailing list we have choosen the value below.\n+\t// See http://gcc.gnu.org/ml/libstdc++/2001-07/msg00077.html\n+\tsize_t \t_M_chunk_size;\n+\n+\t// The maximum number of supported threads. Our Linux 2.4.18\n+\t// reports 4070 in /proc/sys/kernel/threads-max\n+\tsize_t \t_M_max_threads;\n+\n+\t// Each time a deallocation occurs in a threaded application\n+\t// we make sure that there are no more than\n+\t// _M_freelist_headroom % of used memory on the freelist. If\n+\t// the number of additional records is more than\n+\t// _M_freelist_headroom % of the freelist, we move these\n+\t// records back to the global pool.\n+\tsize_t \t_M_freelist_headroom;\n+\n+\t// Set to true forces all allocations to use new().\n+\tbool \t_M_force_new; \n+     \n+\texplicit tune() \n+\t: _M_max_bytes(128), _M_chunk_size(4096 - 4 * sizeof(void*)), \n+#ifdef __GTHREADS\n+\t  _M_max_threads(4096), \n+#else\n+\t  _M_max_threads(0), \n+#endif\n+\t  _M_freelist_headroom(10), \n+\t  _M_force_new(getenv(\"GLIBCXX_FORCE_NEW\") ? true : false) \n+\t{ }      \n+\n+\texplicit tune(size_t __maxb, size_t __chunk, size_t __maxthreads, \n+\t\t\t size_t __headroom, bool __force) \n+\t: _M_max_bytes(__maxb), _M_chunk_size(__chunk), \n+\t  _M_max_threads(__maxthreads), _M_freelist_headroom(__headroom), \n+\t  _M_force_new(__force)\n+\t{ }      \n+      };\n+\n     private:\n-      /*\n-       * We need to create the initial lists and set up some variables\n-       * before we can answer to the first request for memory.\n-       * The initialization of these variables is done at file scope\n-       * below class declaration.\n-       */\n+      // We need to create the initial lists and set up some variables\n+      // before we can answer to the first request for memory.\n #ifdef __GTHREADS\n-      static __gthread_once_t _S_once_mt;\n+      static __gthread_once_t \t\t_S_once;\n #endif\n-      static bool volatile _S_initialized;\n-\n-      /*\n-       * If the env var GLIBCXX_FORCE_NEW is set during _S_init()\n-       * we set this var to true which causes all allocations to use new()\n-       */\n-      static bool _S_force_new;\n-\n-      /*\n-       * Using short int as type for the binmap implies we are never caching\n-       * blocks larger than 65535 with this allocator\n-       */\n+      static bool \t\t\t_S_init;\n+\n+      static void \n+      _S_initialize();\n+\n+      // Configuration options.\n+      static tune \t       \t\t_S_options;\n+\n+      static const tune\n+      _S_get_options() { return _S_options; }\n+\n+      static void\n+      _S_set_options(tune __t)\n+      { \n+\tif (!_S_init)\n+\t  _S_options = __t;\n+      }\n+\n+      // Using short int as type for the binmap implies we are never\n+      // caching blocks larger than 65535 with this allocator\n       typedef unsigned short int binmap_type;\n-      static binmap_type* _S_binmap;\n-\n-      static void _S_init();\n-\n-      /*\n-       * Variables used to \"tune\" the behavior of the allocator, assigned\n-       * and explained in detail below.\n-       */\n-      static size_t _S_max_bytes;\n-      static size_t _S_chunk_size;\n-      static size_t _S_max_threads;\n-      static size_t _S_no_of_bins;\n-      static size_t _S_freelist_headroom;\n-\n-      /*\n-       * Each requesting thread is assigned an id ranging from 1 to\n-       * _S_max_threads. Thread id 0 is used as a global memory pool.\n-       * In order to get constant performance on the thread assignment\n-       * routine, we keep a list of free ids. When a thread first requests\n-       * memory we remove the first record in this list and stores the address\n-       * in a __gthread_key. When initializing the __gthread_key\n-       * we specify a destructor. When this destructor (i.e. the thread dies)\n-       * is called, we return the thread id to the front of this list.\n-       */\n+      static binmap_type* \t\t_S_binmap;\n+\n+      // Each requesting thread is assigned an id ranging from 1 to\n+      // _S_max_threads. Thread id 0 is used as a global memory pool.\n+      // In order to get constant performance on the thread assignment\n+      // routine, we keep a list of free ids. When a thread first\n+      // requests memory we remove the first record in this list and\n+      // stores the address in a __gthread_key. When initializing the\n+      // __gthread_key we specify a destructor. When this destructor\n+      // (i.e. the thread dies) is called, we return the thread id to\n+      // the front of this list.\n #ifdef __GTHREADS\n       struct thread_record\n       {\n-        /*\n-         * Points to next free thread id record. NULL if last record in list.\n-         */\n+        // Points to next free thread id record. NULL if last record in list.\n         thread_record* volatile next;\n \n-        /*\n-         * Thread id ranging from 1 to _S_max_threads.\n-         */\n+\t// Thread id ranging from 1 to _S_max_threads.\n         size_t id;\n       };\n \n-      static thread_record* volatile _S_thread_freelist_first;\n-      static __gthread_mutex_t _S_thread_freelist_mutex;\n-      static void _S_thread_key_destr(void* freelist_pos);\n-      static __gthread_key_t _S_thread_key;\n-      static size_t _S_get_thread_id();\n+      static thread_record* volatile \t_S_thread_freelist_first;\n+      static __gthread_mutex_t \t\t_S_thread_freelist_mutex;\n+      static __gthread_key_t \t\t_S_thread_key;\n+\n+      static void \n+      _S_destroy_thread_key(void* freelist_pos);\n+\n+      static size_t \n+      _S_get_thread_id();\n #endif\n \n       struct block_record\n       {\n-        /*\n-         * Points to the next block_record for its thread_id.\n-         */\n+\t// Points to the next block_record for its thread_id.\n         block_record* volatile next;\n \n-        /*\n-         * The thread id of the thread which has requested this block.\n-         */\n+\t// The thread id of the thread which has requested this block.\n #ifdef __GTHREADS\n         size_t thread_id;\n #endif\n       };\n \n       struct bin_record\n       {\n-        /*\n-         * An \"array\" of pointers to the first free block for each\n-         * thread id. Memory to this \"array\" is allocated in _S_init()\n-         * for _S_max_threads + global pool 0.\n-         */\n+\t// An \"array\" of pointers to the first free block for each\n+\t// thread id. Memory to this \"array\" is allocated in _S_initialize()\n+\t// for _S_max_threads + global pool 0.\n         block_record** volatile first;\n \n-        /*\n-         * An \"array\" of counters used to keep track of the amount of blocks\n-         * that are on the freelist/used for each thread id.\n-         * Memory to these \"arrays\" is allocated in _S_init()\n-         * for _S_max_threads + global pool 0.\n-         */\n+\t// An \"array\" of counters used to keep track of the amount of\n+\t// blocks that are on the freelist/used for each thread id.\n+\t// Memory to these \"arrays\" is allocated in _S_initialize() for\n+\t// _S_max_threads + global pool 0.\n         size_t* volatile free;\n         size_t* volatile used;\n \n-        /*\n-         * Each bin has its own mutex which is used to ensure data integrity\n-         * while changing \"ownership\" on a block.\n-         * The mutex is initialized in _S_init().\n-         */\n+\t// Each bin has its own mutex which is used to ensure data\n+\t// integrity while changing \"ownership\" on a block.  The mutex\n+\t// is initialized in _S_initialize().\n #ifdef __GTHREADS\n         __gthread_mutex_t* mutex;\n #endif\n       };\n \n-      /*\n-       * An \"array\" of bin_records each of which represents a specific\n-       * power of 2 size. Memory to this \"array\" is allocated in _S_init().\n-       */\n-      static bin_record* volatile _S_bin;\n+      // An \"array\" of bin_records each of which represents a specific\n+      // power of 2 size. Memory to this \"array\" is allocated in\n+      // _S_initialize().\n+      static bin_record* volatile     \t_S_bin;\n \n-    public:\n-      pointer\n-      allocate(size_t __n, const void* = 0)\n-      {\n-        /*\n-         * Although the test in __gthread_once() would suffice, we\n-         * wrap test of the once condition in our own unlocked\n-         * check. This saves one function call to pthread_once()\n-         * (which itself only tests for the once value unlocked anyway\n-         * and immediately returns if set)\n-         */\n-        if (!_S_initialized)\n-          {\n+      // Actual value calculated in _S_initialize().\n+      static size_t \t       \t     \t_S_bin_size; \n+    };\n+\n+  template<typename _Tp>\n+    typename __mt_alloc<_Tp>::pointer\n+    __mt_alloc<_Tp>::\n+    allocate(size_t __n, const void*)\n+    {\n+      // Although the test in __gthread_once() would suffice, we wrap\n+      // test of the once condition in our own unlocked check. This\n+      // saves one function call to pthread_once() (which itself only\n+      // tests for the once value unlocked anyway and immediately\n+      // returns if set)\n+      if (!_S_init)\n+\t{\n #ifdef __GTHREADS\n-            if (__gthread_active_p())\n-              __gthread_once(&_S_once_mt, _S_init);\n-            else\n+\t  if (__gthread_active_p())\n+\t    __gthread_once(&_S_once, _S_initialize);\n #endif\n-              {\n-                _S_max_threads = 0;\n-                _S_init();\n-              }\n-          }\n-\n-        /*\n-         * Requests larger than _S_max_bytes are handled by\n-         * new/delete directly\n-         */\n-        if (__n * sizeof(_Tp) > _S_max_bytes || _S_force_new)\n-          {\n-            void* __ret = ::operator new(__n * sizeof(_Tp));\n-            if (!__ret)\n-              std::__throw_bad_alloc();\n-            return static_cast<_Tp*>(__ret);\n-          }\n-\n-        /*\n-         * Round up to power of 2 and figure out which bin to use\n-         */\n-        size_t bin = _S_binmap[__n * sizeof(_Tp)];\n-\n+\t  if (!_S_init)\n+\t    _S_initialize();\n+\t}\n+      \n+      // Requests larger than _M_max_bytes are handled by new/delete\n+      // directly.\n+      const size_t __bytes = __n * sizeof(_Tp);\n+      if (__bytes > _S_options._M_max_bytes || _S_options._M_force_new)\n+\t{\n+\t  void* __ret = ::operator new(__bytes);\n+\t  return static_cast<_Tp*>(__ret);\n+\t}\n+      \n+      // Round up to power of 2 and figure out which bin to use.\n+      size_t bin = _S_binmap[__bytes];\n+      \n #ifdef __GTHREADS\n-        size_t thread_id = _S_get_thread_id();\n+      size_t thread_id = _S_get_thread_id();\n #else\n-        size_t thread_id = 0;\n+      size_t thread_id = 0;\n #endif\n-\n-        block_record* block = NULL;\n-\n-        /*\n-         * Find out if we have blocks on our freelist.\n-         * If so, go ahead and use them directly without\n-         * having to lock anything.\n-         */\n-        if (_S_bin[bin].first[thread_id] == NULL)\n-          {\n-            /*\n-             * Are we using threads?\n-             * - Yes, check if there are free blocks on the global\n-             *   list. If so, grab up to block_count blocks in one\n-             *   lock and change ownership. If the global list is \n-             *   empty, we allocate a new chunk and add those blocks \n-             *   directly to our own freelist (with us as owner).\n-             * - No, all operations are made directly to global pool 0\n-             *   no need to lock or change ownership but check for free\n-             *   blocks on global list (and if not add new ones) and\n-             *   get the first one.\n-             */\n+      \n+      // Find out if we have blocks on our freelist.  If so, go ahead\n+      // and use them directly without having to lock anything.\n+      block_record* block = NULL;\n+      if (_S_bin[bin].first[thread_id] == NULL)\n+\t{\n+\t  // Are we using threads?\n+\t  // - Yes, check if there are free blocks on the global\n+\t  //   list. If so, grab up to block_count blocks in one\n+\t  //   lock and change ownership. If the global list is \n+\t  //   empty, we allocate a new chunk and add those blocks \n+\t  //   directly to our own freelist (with us as owner).\n+\t  // - No, all operations are made directly to global pool 0\n+\t  //   no need to lock or change ownership but check for free\n+\t  //   blocks on global list (and if not add new ones) and\n+\t  //   get the first one.\n #ifdef __GTHREADS\n-            if (__gthread_active_p())\n-              {\n-                size_t bin_t = 1 << bin;\n-                size_t block_count =\n-                  _S_chunk_size /(bin_t + sizeof(block_record));\n-\n-                __gthread_mutex_lock(_S_bin[bin].mutex);\n-\n-                if (_S_bin[bin].first[0] == NULL)\n-                  {\n-                    /*\n-                     * No need to hold the lock when we are adding a\n-                     * whole chunk to our own list\n-                     */\n-                    __gthread_mutex_unlock(_S_bin[bin].mutex);\n-\n-                    _S_bin[bin].first[thread_id] =\n-                     static_cast<block_record*>(::operator new(_S_chunk_size));\n-\n-                    if (!_S_bin[bin].first[thread_id])\n-                      std::__throw_bad_alloc();\n-\n-                    _S_bin[bin].free[thread_id] = block_count;\n-\n-                    block_count--;\n-                    block = _S_bin[bin].first[thread_id];\n-\n-                    while (block_count > 0)\n-                      {\n-                        block->next = (block_record*)((char*)block +\n-                                      (bin_t + sizeof(block_record)));\n-                        block->thread_id = thread_id;\n-                        block = block->next;\n-                        block_count--;\n-                      }\n-\n-                    block->next = NULL;\n-                    block->thread_id = thread_id;\n-                  }\n-                else\n-                  {\n-                    size_t global_count = 0;\n-\n-                    block_record* tmp;\n-\n-                    while( _S_bin[bin].first[0] != NULL &&\n-                           global_count < block_count )\n-                      {\n-                        tmp = _S_bin[bin].first[0]->next;\n-\n-                        block = _S_bin[bin].first[0];\n-\n-                        if (_S_bin[bin].first[thread_id] == NULL)\n-                          {\n-                            _S_bin[bin].first[thread_id] = block;\n-                            block->next = NULL;\n-                          }\n-                        else\n-                          {\n-                            block->next = _S_bin[bin].first[thread_id];\n-                            _S_bin[bin].first[thread_id] = block;\n-                          }\n-\n-                        block->thread_id = thread_id;\n-\n-                        _S_bin[bin].free[thread_id]++;\n-\n-                        _S_bin[bin].first[0] = tmp;\n-\n-                        global_count++;\n-                      }\n-\n-                    __gthread_mutex_unlock(_S_bin[bin].mutex);\n-                  }\n-\n-                /*\n-                 * Return the first newly added block in our list and\n-                 * update the counters\n-                 */\n-                block = _S_bin[bin].first[thread_id];\n-                _S_bin[bin].first[thread_id] = \n-                  _S_bin[bin].first[thread_id]->next;\n-\n-                _S_bin[bin].free[thread_id]--;\n-                _S_bin[bin].used[thread_id]++;\n-              }\n-            else\n+\t  if (__gthread_active_p())\n+\t    {\n+\t      size_t bin_t = 1 << bin;\n+\t      size_t block_count =\n+\t\t_S_options._M_chunk_size /(bin_t + sizeof(block_record));\n+\t      \n+\t      __gthread_mutex_lock(_S_bin[bin].mutex);\n+\t      \n+\t      if (_S_bin[bin].first[0] == NULL)\n+\t\t{\n+\t\t  // No need to hold the lock when we are adding a\n+\t\t  // whole chunk to our own list.\n+\t\t  __gthread_mutex_unlock(_S_bin[bin].mutex);\n+\t\t  \n+\t\t  _S_bin[bin].first[thread_id] =\n+\t\t    static_cast<block_record*>(::operator new(_S_options._M_chunk_size));\n+\t\t  \n+\t\t  if (!_S_bin[bin].first[thread_id])\n+\t\t    std::__throw_bad_alloc();\n+\t\t  \n+\t\t  _S_bin[bin].free[thread_id] = block_count;\n+\t\t  \n+\t\t  block_count--;\n+\t\t  block = _S_bin[bin].first[thread_id];\n+\t\t  \n+\t\t  while (block_count > 0)\n+\t\t    {\n+\t\t      block->next = (block_record*)((char*)block +\n+\t\t\t\t\t\t    (bin_t + sizeof(block_record)));\n+\t\t      block->thread_id = thread_id;\n+\t\t      block = block->next;\n+\t\t      block_count--;\n+\t\t    }\n+\t\t  \n+\t\t  block->next = NULL;\n+\t\t  block->thread_id = thread_id;\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  size_t global_count = 0;\t\t  \n+\t\t  block_record* tmp;\t\t  \n+\t\t  while (_S_bin[bin].first[0] != NULL \n+\t\t\t && global_count < block_count)\n+\t\t    {\n+\t\t      tmp = _S_bin[bin].first[0]->next;\n+\t\t      block = _S_bin[bin].first[0];\n+\n+\t\t      if (_S_bin[bin].first[thread_id] == NULL)\n+\t\t\t{\n+\t\t\t  _S_bin[bin].first[thread_id] = block;\n+\t\t\t  block->next = NULL;\n+\t\t\t}\n+\t\t      else\n+\t\t\t{\n+\t\t\t  block->next = _S_bin[bin].first[thread_id];\n+\t\t\t  _S_bin[bin].first[thread_id] = block;\n+\t\t\t}\n+\t\t      \n+\t\t      block->thread_id = thread_id;\n+\t\t      _S_bin[bin].free[thread_id]++;\n+\t\t      _S_bin[bin].first[0] = tmp;\n+\t\t      global_count++;\n+\t\t    }\n+\t\t  __gthread_mutex_unlock(_S_bin[bin].mutex);\n+\t\t}\n+\t      \n+\t      // Return the first newly added block in our list and\n+\t      // update the counters\n+\t      block = _S_bin[bin].first[thread_id];\n+\t      _S_bin[bin].first[thread_id] = \n+\t\t_S_bin[bin].first[thread_id]->next;\t      \n+\t      _S_bin[bin].free[thread_id]--;\n+\t      _S_bin[bin].used[thread_id]++;\n+\t    }\n+\t  else\n #endif\n-              {\n-                _S_bin[bin].first[0] = \n-                  static_cast<block_record*>(::operator new(_S_chunk_size));\n-\n-                if (!_S_bin[bin].first[0])\n-                  std::__throw_bad_alloc();\n-\n-                size_t bin_t = 1 << bin;\n-                size_t block_count = \n-                  _S_chunk_size / (bin_t + sizeof(block_record));\n-\n-                block_count--;\n-                block = _S_bin[bin].first[0];\n-\n-                while (block_count > 0)\n-                  {\n-                    block->next = (block_record*)((char*)block +\n-                                  (bin_t + sizeof(block_record)));\n-                    block = block->next;\n-                    block_count--;\n-                  }\n-\n-                block->next = NULL;\n-\n-                block = _S_bin[bin].first[0];\n-\n-                /*\n-                 * Remove from list\n-                 */\n-                _S_bin[bin].first[0] = _S_bin[bin].first[0]->next;\n-              }\n-          }\n-        else\n-          {\n-            /*\n-             * \"Default\" operation - we have blocks on our own freelist\n-             * grab the first record and update the counters.\n-             */\n-            block = _S_bin[bin].first[thread_id];\n-\n-            _S_bin[bin].first[thread_id] = _S_bin[bin].first[thread_id]->next;\n-\n+\t    {\n+\t      _S_bin[bin].first[0] = \n+\t\tstatic_cast<block_record*>(::operator new(_S_options._M_chunk_size));\n+\t      \n+\t      size_t bin_t = 1 << bin;\n+\t      size_t block_count = \n+\t\t_S_options._M_chunk_size / (bin_t + sizeof(block_record));\n+\t      \n+\t      block_count--;\n+\t      block = _S_bin[bin].first[0];\t      \n+\t      while (block_count > 0)\n+\t\t{\n+\t\t  block->next = (block_record*)((char*)block +\n+\t\t\t\t\t\t(bin_t + sizeof(block_record)));\n+\t\t  block = block->next;\n+\t\t  block_count--;\n+\t\t}\n+\t      block->next = NULL;\n+\t      block = _S_bin[bin].first[0];\n+\t      \n+\t      // Remove from list.\n+\t      _S_bin[bin].first[0] = _S_bin[bin].first[0]->next;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  // \"Default\" operation - we have blocks on our own\n+\t  // freelist grab the first record and update the counters.\n+\t  block = _S_bin[bin].first[thread_id];\n+\t  \n+\t  _S_bin[bin].first[thread_id] = _S_bin[bin].first[thread_id]->next;\n+\t  \n #ifdef __GTHREADS\n-            if (__gthread_active_p())\n-              {\n-                _S_bin[bin].free[thread_id]--;\n-                _S_bin[bin].used[thread_id]++;\n-              }\n+\t  if (__gthread_active_p())\n+\t    {\n+\t      _S_bin[bin].free[thread_id]--;\n+\t      _S_bin[bin].used[thread_id]++;\n+\t    }\n #endif\n-          }\n-\n-        return static_cast<_Tp*>(static_cast<void*>((char*)block + \n-                                                    sizeof(block_record)));\n-      }\n-\n-      void\n-      deallocate(pointer __p, size_type __n)\n-      {\n-        /*\n-         * Requests larger than _S_max_bytes are handled by\n-         * operators new/delete directly\n-         */\n-        if (__n * sizeof(_Tp) > _S_max_bytes || _S_force_new)\n-          {\n-            ::operator delete(__p);\n-            return;\n-          }\n-\n-        /*\n-         * Round up to power of 2 and figure out which bin to use\n-         */\n-        size_t bin = _S_binmap[__n * sizeof(_Tp)];\n-\n+\t}\n+      return static_cast<_Tp*>(static_cast<void*>((char*)block + \n+\t\t\t\t\t\t  sizeof(block_record)));\n+    }\n+  \n+  template<typename _Tp>\n+    void\n+    __mt_alloc<_Tp>::\n+    deallocate(pointer __p, size_type __n)\n+    {\n+      // Requests larger than _M_max_bytes are handled by operators\n+      // new/delete directly.\n+      if (__n * sizeof(_Tp) > _S_options._M_max_bytes \n+\t  || _S_options._M_force_new)\n+\t{\n+\t  ::operator delete(__p);\n+\t  return;\n+\t}\n+      \n+      // Round up to power of 2 and figure out which bin to use.\n+      size_t bin = _S_binmap[__n * sizeof(_Tp)];\n+      \n #ifdef __GTHREADS\n-        size_t thread_id = _S_get_thread_id();\n+      size_t thread_id = _S_get_thread_id();\n #else\n-        size_t thread_id = 0;\n+      size_t thread_id = 0;\n #endif\n-\n-        block_record* block = (block_record*)((char*)__p\n-                                             - sizeof(block_record));\n-\n+      \n+      block_record* block = (block_record*)((char*)__p\n+\t\t\t\t\t    - sizeof(block_record));\n+      \n #ifdef __GTHREADS\n-        if (__gthread_active_p())\n-          {\n-            /*\n-             * Calculate the number of records to remove from our freelist\n-             */\n-            int remove = _S_bin[bin].free[thread_id] -\n-                         (_S_bin[bin].used[thread_id] / _S_freelist_headroom);\n-\n-            /*\n-             * The calculation above will almost always tell us to\n-             * remove one or two records at a time, but this creates\n-             * too much contention when locking and therefore we\n-             * wait until the number of records is \"high enough\".\n-             */\n-            if (remove > (int)(100 * (_S_no_of_bins - bin)) &&\n-                remove > (int)(_S_bin[bin].free[thread_id] /\n-                               _S_freelist_headroom))\n-              {\n-                __gthread_mutex_lock(_S_bin[bin].mutex);\n-\n-                block_record* tmp;\n-\n-                while (remove > 0)\n-                  {\n-                    tmp = _S_bin[bin].first[thread_id]->next;\n-\n-                    if (_S_bin[bin].first[0] == NULL)\n-                      {\n-                        _S_bin[bin].first[0] = _S_bin[bin].first[thread_id];\n-                        _S_bin[bin].first[0]->next = NULL;\n-                      }\n-                    else\n-                      {\n-                        _S_bin[bin].first[thread_id]->next = _S_bin[bin].first[0];\n-                        _S_bin[bin].first[0] = _S_bin[bin].first[thread_id];\n-                      }\n-\n-                    _S_bin[bin].first[thread_id] = tmp;\n-\n-                    _S_bin[bin].free[thread_id]--;\n-\n-                    remove--;\n-                  }\n-\n-                __gthread_mutex_unlock(_S_bin[bin].mutex);\n-              }\n-\n-            /*\n-             * Return this block to our list and update\n-             * counters and owner id as needed\n-             */\n-            if (_S_bin[bin].first[thread_id] == NULL)\n-              {\n-                _S_bin[bin].first[thread_id] = block;\n-                block->next = NULL;\n-              }\n-            else\n-              {\n-                block->next = _S_bin[bin].first[thread_id];\n-                _S_bin[bin].first[thread_id] = block;\n-              }\n-\n-            _S_bin[bin].free[thread_id]++;\n-\n-            if (thread_id == block->thread_id)\n-              _S_bin[bin].used[thread_id]--;\n-            else\n-              {\n-                _S_bin[bin].used[block->thread_id]--;\n-                block->thread_id = thread_id;\n-              }\n-          }\n-        else\n+      if (__gthread_active_p())\n+\t{\n+\t  // Calculate the number of records to remove from our freelist.\n+\t  int remove = _S_bin[bin].free[thread_id] -\n+\t    (_S_bin[bin].used[thread_id] / _S_options._M_freelist_headroom);\n+\n+\t  // The calculation above will almost always tell us to\n+\t  // remove one or two records at a time, but this creates too\n+\t  // much contention when locking and therefore we wait until\n+\t  // the number of records is \"high enough\".\n+\t  if (remove > (int)(100 * (_S_bin_size - bin)) &&\n+\t      remove > (int)(_S_bin[bin].free[thread_id] /\n+                               _S_options._M_freelist_headroom))\n+\t    {\n+\t      __gthread_mutex_lock(_S_bin[bin].mutex);\n+\t      block_record* tmp;\n+\t      while (remove > 0)\n+\t\t{\n+\t\t  tmp = _S_bin[bin].first[thread_id]->next;\n+\t\t  if (_S_bin[bin].first[0] == NULL)\n+\t\t    {\n+\t\t      _S_bin[bin].first[0] = _S_bin[bin].first[thread_id];\n+\t\t      _S_bin[bin].first[0]->next = NULL;\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      _S_bin[bin].first[thread_id]->next = _S_bin[bin].first[0];\n+\t\t      _S_bin[bin].first[0] = _S_bin[bin].first[thread_id];\n+\t\t    }\n+\t\t  \n+\t\t  _S_bin[bin].first[thread_id] = tmp;\n+\t\t  _S_bin[bin].free[thread_id]--;\n+\t\t  remove--;\n+\t\t}\n+\t      __gthread_mutex_unlock(_S_bin[bin].mutex);\n+\t    }\n+\t  \n+\t  // Return this block to our list and update counters and\n+\t  // owner id as needed.\n+\t  if (_S_bin[bin].first[thread_id] == NULL)\n+\t    {\n+\t      _S_bin[bin].first[thread_id] = block;\n+\t      block->next = NULL;\n+\t    }\n+\t  else\n+\t    {\n+\t      block->next = _S_bin[bin].first[thread_id];\n+\t      _S_bin[bin].first[thread_id] = block;\n+\t    }\n+\t  \n+\t  _S_bin[bin].free[thread_id]++;\n+\t  \n+\t  if (thread_id == block->thread_id)\n+\t    _S_bin[bin].used[thread_id]--;\n+\t  else\n+\t    {\n+\t      _S_bin[bin].used[block->thread_id]--;\n+\t      block->thread_id = thread_id;\n+\t    }\n+\t}\n+      else\n #endif\n-          {\n-            /*\n-             * Single threaded application - return to global pool\n-             */\n-            if (_S_bin[bin].first[0] == NULL)\n-              {\n-                _S_bin[bin].first[0] = block;\n-                block->next = NULL;\n-              }\n-            else\n-              {\n-                block->next = _S_bin[bin].first[0];\n-                _S_bin[bin].first[0] = block;\n-              }\n-          }\n-      }\n-    };\n-\n+\t{\n+\t  // Single threaded application - return to global pool.\n+\t  if (_S_bin[bin].first[0] == NULL)\n+\t    {\n+\t      _S_bin[bin].first[0] = block;\n+\t      block->next = NULL;\n+\t    }\n+\t  else\n+\t    {\n+\t      block->next = _S_bin[bin].first[0];\n+\t      _S_bin[bin].first[0] = block;\n+\t    }\n+\t}\n+    }\n+  \n   template<typename _Tp>\n     void\n     __mt_alloc<_Tp>::\n-    _S_init()\n+    _S_initialize()\n     {\n-      if (getenv(\"GLIBCXX_FORCE_NEW\"))\n-        {\n-          _S_force_new = true;\n-          _S_initialized = true;\n-\n-          /*\n-           * Since none of the code in allocate/deallocate ever will be \n-           * executed due to that the GLIBCXX_FORCE_NEW flag is set\n-           * there is no need to create the internal structures either.\n-           */\n-          return;\n-        }\n-\n-      /*\n-       * Calculate the number of bins required based on _S_max_bytes,\n-       * _S_no_of_bins is initialized to 1 below.\n-       */\n-      {\n-        size_t bin_t = 1;\n-        while (_S_max_bytes > bin_t)\n-          {\n-            bin_t = bin_t << 1;\n-            _S_no_of_bins++;\n-          }\n-      }\n+      if (_S_options._M_force_new)\n+\treturn;\n+\t\n+      // Calculate the number of bins required based on _M_max_bytes.\n+      // _S_bin_size is statically-initialized to one.\n+      size_t bin_size = 1;\n+      while (_S_options._M_max_bytes > bin_size)\n+\t{\n+\t  bin_size = bin_size << 1;\n+\t  _S_bin_size++;\n+\t}\n \n-      /*\n-       * Setup the bin map for quick lookup of the relevant bin\n-       */\n+      // Setup the bin map for quick lookup of the relevant bin.\n       _S_binmap = (binmap_type*)\n-        ::operator new ((_S_max_bytes + 1) * sizeof(binmap_type));\n-\n-      if (!_S_binmap)\n-        std::__throw_bad_alloc();\n+        ::operator new ((_S_options._M_max_bytes + 1) * sizeof(binmap_type));\n \n       binmap_type* bp_t = _S_binmap;\n       binmap_type bin_max_t = 1;\n       binmap_type bin_t = 0;\n-      for (binmap_type ct = 0; ct <= _S_max_bytes; ct++)\n+      for (binmap_type ct = 0; ct <= _S_options._M_max_bytes; ct++)\n         {\n           if (ct > bin_max_t)\n             {\n@@ -613,127 +575,95 @@ namespace __gnu_cxx\n           *bp_t++ = bin_t;\n         }\n \n-      /*\n-       * If __gthread_active_p() create and initialize the list of\n-       * free thread ids. Single threaded applications use thread id 0\n-       * directly and have no need for this.\n-       */\n+      // If __gthread_active_p() create and initialize the list of\n+      // free thread ids. Single threaded applications use thread id 0\n+      // directly and have no need for this.\n #ifdef __GTHREADS\n       if (__gthread_active_p())\n         {\n           _S_thread_freelist_first =\n             static_cast<thread_record*>(::operator \n-              new(sizeof(thread_record) * _S_max_threads));\n+              new(sizeof(thread_record) * _S_options._M_max_threads));\n \n-          if (!_S_thread_freelist_first)\n-            std::__throw_bad_alloc();\n-\n-          /*\n-           * NOTE! The first assignable thread id is 1 since the global\n-           * pool uses id 0\n-           */\n+\t  // NOTE! The first assignable thread id is 1 since the\n+\t  // global pool uses id 0\n           size_t i;\n-          for (i = 1; i < _S_max_threads; i++)\n+          for (i = 1; i < _S_options._M_max_threads; i++)\n             {\n-              _S_thread_freelist_first[i - 1].next = \n-                &_S_thread_freelist_first[i];\n-\n-              _S_thread_freelist_first[i - 1].id = i;\n+\t      thread_record& tr = _S_thread_freelist_first[i - 1];\n+              tr.next = &_S_thread_freelist_first[i];\n+              tr.id = i;\n             }\n \n-          /*\n-           * Set last record\n-           */\n+          // Set last record.\n           _S_thread_freelist_first[i - 1].next = NULL;\n           _S_thread_freelist_first[i - 1].id = i;\n \n-          /*\n-           * Initialize per thread key to hold pointer to\n-           * _S_thread_freelist\n-           */\n-          __gthread_key_create(&_S_thread_key, _S_thread_key_destr);\n+          // Initialize per thread key to hold pointer to\n+          // _S_thread_freelist.\n+          __gthread_key_create(&_S_thread_key, _S_destroy_thread_key);\n         }\n #endif\n \n-      /*\n-       * Initialize _S_bin and its members\n-       */\n+      // Initialize _S_bin and its members.\n       _S_bin = static_cast<bin_record*>(::operator \n-        new(sizeof(bin_record) * _S_no_of_bins));\n-\n-      if (!_S_bin)\n-        std::__throw_bad_alloc();\n-\n-      std::size_t __n = 1;\n+\t\t\t\t\tnew(sizeof(bin_record) * _S_bin_size));\n \n+      // Maximum number of threads. \n+      size_t __n = 1;\n #ifdef __GTHREADS\n       if (__gthread_active_p())\n-        __n = _S_max_threads + 1;\n+        __n = _S_options._M_max_threads + 1;\n #endif\n \n-      for (size_t bin = 0; bin < _S_no_of_bins; bin++)\n+      for (size_t bin = 0; bin < _S_bin_size; bin++)\n         {\n-          _S_bin[bin].first = static_cast<block_record**>(::operator \n-            new(sizeof(block_record*) * __n));\n-\n-          if (!_S_bin[bin].first)\n-            std::__throw_bad_alloc();\n+\t  bin_record& br = _S_bin[bin];\n+          br.first = static_cast<block_record**>(::operator new(sizeof(block_record*) * __n));\n \n #ifdef __GTHREADS\n           if (__gthread_active_p())\n             {\n-              _S_bin[bin].free = static_cast<size_t*>(::operator \n-                new(sizeof(size_t) * __n));\n-\n-              if (!_S_bin[bin].free)\n-                std::__throw_bad_alloc();\n-\n-              _S_bin[bin].used = static_cast<size_t*>(::operator \n-                new(sizeof(size_t) * __n));\n-\n-              if (!_S_bin[bin].used)\n-                std::__throw_bad_alloc();\n-\n-              _S_bin[bin].mutex = static_cast<__gthread_mutex_t*>(::operator \n-                new(sizeof(__gthread_mutex_t)));\n+              br.free = static_cast<size_t*>(::operator new(sizeof(size_t) \n+\t\t\t\t\t\t\t    * __n));\n+              br.used = static_cast<size_t*>(::operator new(sizeof(size_t) \n+\t\t\t\t\t\t\t    * __n));\n+              br.mutex = static_cast<__gthread_mutex_t*>(::operator new(sizeof(__gthread_mutex_t)));\n \n #ifdef __GTHREAD_MUTEX_INIT\n               {\n                 // Do not copy a POSIX/gthr mutex once in use.\n                 __gthread_mutex_t __tmp = __GTHREAD_MUTEX_INIT;\n-                *_S_bin[bin].mutex = __tmp;\n+                *br.mutex = __tmp;\n               }\n #else\n-              { __GTHREAD_MUTEX_INIT_FUNCTION (_S_bin[bin].mutex); }\n+              { __GTHREAD_MUTEX_INIT_FUNCTION (br.mutex); }\n #endif\n             }\n #endif\n \n           for (size_t thread = 0; thread < __n; thread++)\n             {\n-              _S_bin[bin].first[thread] = NULL;\n+              br.first[thread] = NULL;\n #ifdef __GTHREADS\n               if (__gthread_active_p())\n                 {\n-                  _S_bin[bin].free[thread] = 0;\n-                  _S_bin[bin].used[thread] = 0;\n+                  br.free[thread] = 0;\n+                  br.used[thread] = 0;\n                 }\n #endif\n             }\n         }\n-\n-      _S_initialized = true;\n+      _S_init = true;\n     }\n \n #ifdef __GTHREADS\n   template<typename _Tp>\n     void\n     __mt_alloc<_Tp>::\n-    _S_thread_key_destr(void* freelist_pos)\n+    _S_destroy_thread_key(void* freelist_pos)\n     {\n-      /*\n-       * Return this thread id record to front of thread_freelist\n-       */\n+      // Return this thread id record to front of thread_freelist.\n       __gthread_mutex_lock(&_S_thread_freelist_mutex);\n       ((thread_record*)freelist_pos)->next = _S_thread_freelist_first;\n       _S_thread_freelist_first = (thread_record*)freelist_pos;\n@@ -745,102 +675,71 @@ namespace __gnu_cxx\n     __mt_alloc<_Tp>::\n     _S_get_thread_id()\n     {\n-      /*\n-       * If we have thread support and it's active we check the thread\n-       * key value and return it's id or if it's not set we take the\n-       * first record from _S_thread_freelist and sets the key and\n-       * returns it's id.\n-       */\n+      // If we have thread support and it's active we check the thread\n+      // key value and return it's id or if it's not set we take the\n+      // first record from _S_thread_freelist and sets the key and\n+      // returns it's id.\n       if (__gthread_active_p())\n         {\n-          thread_record* freelist_pos;\n-\n-          if ((freelist_pos =\n-              (thread_record*)__gthread_getspecific(_S_thread_key)) == NULL)\n+          thread_record* freelist_pos = static_cast<thread_record*>(__gthread_getspecific(_S_thread_key)); \n+\t  if (freelist_pos == NULL)\n             {\n-              /*\n-               * Since _S_max_threads must be larger than the\n-               * theoretical max number of threads of the OS the list\n-               * can never be empty.\n-               */\n+\t      // Since _S_options._M_max_threads must be larger than\n+\t      // the theoretical max number of threads of the OS the\n+\t      // list can never be empty.\n               __gthread_mutex_lock(&_S_thread_freelist_mutex);\n               freelist_pos = _S_thread_freelist_first;\n               _S_thread_freelist_first = _S_thread_freelist_first->next;\n               __gthread_mutex_unlock(&_S_thread_freelist_mutex);\n \n-              __gthread_setspecific(_S_thread_key, (void*)freelist_pos);\n+              __gthread_setspecific(_S_thread_key, \n+\t\t\t\t    static_cast<void*>(freelist_pos));\n             }\n-\n           return freelist_pos->id;\n         }\n \n-      /*\n-       * Otherwise (no thread support or inactive) all requests are\n-       * served from the global pool 0.\n-       */\n+      // Otherwise (no thread support or inactive) all requests are\n+      // served from the global pool 0.\n       return 0;\n     }\n-\n-  template<typename _Tp> __gthread_once_t\n-  __mt_alloc<_Tp>::_S_once_mt = __GTHREAD_ONCE_INIT;\n #endif\n \n-  template<typename _Tp> \n-  bool volatile __mt_alloc<_Tp>::_S_initialized = false;\n+  template<typename _Tp>\n+    inline bool\n+    operator==(const __mt_alloc<_Tp>&, const __mt_alloc<_Tp>&)\n+    { return true; }\n+  \n+  template<typename _Tp>\n+    inline bool\n+    operator!=(const __mt_alloc<_Tp>&, const __mt_alloc<_Tp>&)\n+    { return false; }\n \n-  template<typename _Tp> bool\n-  __mt_alloc<_Tp>::_S_force_new = false;\n+  template<typename _Tp> \n+    bool __mt_alloc<_Tp>::_S_init = false;\n \n-  template<typename _Tp> typename __mt_alloc<_Tp>::binmap_type*\n-  __mt_alloc<_Tp>::_S_binmap = NULL;\n+  template<typename _Tp> \n+    typename __mt_alloc<_Tp>::tune __mt_alloc<_Tp>::_S_options;\n \n-  /*\n-   * Allocation requests (after round-up to power of 2) below this\n-   * value will be handled by the allocator. A raw new/ call\n-   * will be used for requests larger than this value.\n-   */\n-  template<typename _Tp> size_t\n-  __mt_alloc<_Tp>::_S_max_bytes = 128;\n-\n-  /*\n-   * In order to avoid fragmenting and minimize the number of new()\n-   * calls we always request new memory using this value. Based on\n-   * previous discussions on the libstdc++ mailing list we have\n-   * choosen the value below. See\n-   * http://gcc.gnu.org/ml/libstdc++/2001-07/msg00077.html\n-   */\n-  template<typename _Tp> size_t\n-  __mt_alloc<_Tp>::_S_chunk_size = 4096 - 4 * sizeof(void*);\n+  template<typename _Tp> \n+    typename __mt_alloc<_Tp>::binmap_type* __mt_alloc<_Tp>::_S_binmap;\n \n-  /*\n-   * The maximum number of supported threads. Our Linux 2.4.18 reports\n-   * 4070 in /proc/sys/kernel/threads-max\n-   */\n-  template<typename _Tp> size_t\n-  __mt_alloc<_Tp>::_S_max_threads = 4096;\n+  template<typename _Tp> \n+    typename __mt_alloc<_Tp>::bin_record* volatile __mt_alloc<_Tp>::_S_bin;\n \n-  /*\n-   * Actual value calculated in _S_init()\n-   */\n-  template<typename _Tp> size_t\n-  __mt_alloc<_Tp>::_S_no_of_bins = 1;\n-\n-  /*\n-   * Each time a deallocation occurs in a threaded application we make\n-   * sure that there are no more than _S_freelist_headroom % of used\n-   * memory on the freelist. If the number of additional records is\n-   * more than _S_freelist_headroom % of the freelist, we move these\n-   * records back to the global pool.\n-   */\n-  template<typename _Tp> size_t\n-  __mt_alloc<_Tp>::_S_freelist_headroom = 10;\n+  template<typename _Tp> \n+    size_t __mt_alloc<_Tp>::_S_bin_size = 1;\n \n-  /*\n-   * Actual initialization in _S_init()\n-   */\n+  // Actual initialization in _S_initialize().\n #ifdef __GTHREADS\n-  template<typename _Tp> typename __mt_alloc<_Tp>::thread_record*\n-  volatile __mt_alloc<_Tp>::_S_thread_freelist_first = NULL;\n+  template<typename _Tp> \n+    __gthread_once_t __mt_alloc<_Tp>::_S_once = __GTHREAD_ONCE_INIT;\n+\n+  template<typename _Tp> \n+    typename __mt_alloc<_Tp>::thread_record*\n+    volatile __mt_alloc<_Tp>::_S_thread_freelist_first = NULL;\n+\n+  template<typename _Tp> \n+    __gthread_key_t __mt_alloc<_Tp>::_S_thread_key;\n \n   template<typename _Tp> __gthread_mutex_t\n #ifdef __GTHREAD_MUTEX_INIT\n@@ -849,26 +748,7 @@ namespace __gnu_cxx\n   // XXX\n   __mt_alloc<_Tp>::_S_thread_freelist_mutex;\n #endif\n-\n-  /*\n-   * Actual initialization in _S_init()\n-   */\n-  template<typename _Tp> __gthread_key_t\n-  __mt_alloc<_Tp>::_S_thread_key;\n #endif\n-\n-  template<typename _Tp> typename __mt_alloc<_Tp>::bin_record*\n-  volatile __mt_alloc<_Tp>::_S_bin = NULL;\n-\n-  template<typename _Tp>\n-    inline bool\n-    operator==(const __mt_alloc<_Tp>&, const __mt_alloc<_Tp>&)\n-    { return true; }\n-  \n-  template<typename _Tp>\n-    inline bool\n-    operator!=(const __mt_alloc<_Tp>&, const __mt_alloc<_Tp>&)\n-    { return false; }\n } // namespace __gnu_cxx\n \n #endif"}, {"sha": "1b0b4f6107962cb5c82c296f8a9b28b0267e57af", "filename": "libstdc++-v3/include/ext/new_allocator.h", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2Finclude%2Fext%2Fnew_allocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2Finclude%2Fext%2Fnew_allocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fext%2Fnew_allocator.h?ref=f263b26e17c8e39aca86025e0c8bc4f7df74e5bd", "patch": "@@ -98,6 +98,16 @@ namespace __gnu_cxx\n       void \n       destroy(pointer __p) { __p->~_Tp(); }\n     };\n+\n+  template<typename _Tp>\n+    inline bool\n+    operator==(const new_allocator<_Tp>&, const new_allocator<_Tp>&)\n+    { return true; }\n+  \n+  template<typename _Tp>\n+    inline bool\n+    operator!=(const new_allocator<_Tp>&, const new_allocator<_Tp>&)\n+    { return false; }\n } // namespace __gnu_cxx\n \n #endif"}, {"sha": "5e37d8759e215ebb5d0536d4a110ff371752622f", "filename": "libstdc++-v3/src/allocator.cc", "status": "modified", "additions": 1, "deletions": 13, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2Fsrc%2Fallocator.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f263b26e17c8e39aca86025e0c8bc4f7df74e5bd/libstdc%2B%2B-v3%2Fsrc%2Fallocator.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Fsrc%2Fallocator.cc?ref=f263b26e17c8e39aca86025e0c8bc4f7df74e5bd", "patch": "@@ -40,19 +40,7 @@\n // allocator.\n namespace __gnu_cxx\n {\n-  // Static data members and member functions of __mt_alloc.\n-  static template class __mt_alloc<char>;\n-\n-  template\n-    void __mt_alloc<char>::_S_init();\n-\n-#ifdef __GTHREADS\n-   template\n-    size_t __mt_alloc<char>::_S_get_thread_id();\n-\n-   template\n-    void __mt_alloc<char>::_S_thread_key_destr(void*);\n-#endif\n+  template class __mt_alloc<char>;\n \n   // Static members of __pool_alloc.\n   template class __pool_alloc<true, 0>;"}]}