{"sha": "7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6N2M2ZjJmYjljNzUxM2QzNDg3YWIyZDZkNmVkZDIzNWYxNTJjMGVmMw==", "commit": {"author": {"name": "Jan Hubicka", "email": "jh@suse.cz", "date": "2019-11-21T15:23:09Z"}, "committer": {"name": "Jan Hubicka", "email": "hubicka@gcc.gnu.org", "date": "2019-11-21T15:23:09Z"}, "message": "Avoid quadratic behaviour of update_callee_keys.\n\n\t* ipa-inline.c (update_callee_keys): Add parameter UPDATE_SINCE.\n\t(resolve_noninline_speculation, inline_small_functions): Avoid\n\tredundant updates.\n\nFrom-SVN: r278566", "tree": {"sha": "f83e516c05660161a5fc84bd7be2e6517476e01c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/f83e516c05660161a5fc84bd7be2e6517476e01c"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3/comments", "author": null, "committer": null, "parents": [{"sha": "5f5e796c9c986fd135514906a6a8f58b7d7a15d3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5f5e796c9c986fd135514906a6a8f58b7d7a15d3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5f5e796c9c986fd135514906a6a8f58b7d7a15d3"}], "stats": {"total": 79, "additions": 62, "deletions": 17}, "files": [{"sha": "2aa4a78f12d74190034795914a4be957c32e108b", "filename": "gcc/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3", "patch": "@@ -1,3 +1,9 @@\n+2019-11-21  Jan Hubicka  <jh@suse.cz>\n+\n+\t* ipa-inline.c (update_callee_keys): Add parameter UPDATE_SINCE.\n+\t(resolve_noninline_speculation, inline_small_functions): Avoid\n+\tredundant updates.\n+\n 2019-11-21  Richard Biener  <rguenther@suse.de>\n \n \t* lra.c (lra_insn_recog_data_pool): New."}, {"sha": "879da84cfe1856fe0e287aed986828f3c617d21b", "filename": "gcc/ipa-inline.c", "status": "modified", "additions": 56, "deletions": 17, "changes": 73, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3/gcc%2Fipa-inline.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3/gcc%2Fipa-inline.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fipa-inline.c?ref=7c6f2fb9c7513d3487ab2d6d6edd235f152c0ef3", "patch": "@@ -1481,47 +1481,77 @@ update_caller_keys (edge_heap_t *heap, struct cgraph_node *node,\n       }\n }\n \n-/* Recompute HEAP nodes for each uninlined call in NODE.\n+/* Recompute HEAP nodes for each uninlined call in NODE\n+   If UPDATE_SINCE is non-NULL check if edges called within that function\n+   are inlinable (typically UPDATE_SINCE is the inline clone we introduced\n+   where all edges have new context).\n+  \n    This is used when we know that edge badnesses are going only to increase\n    (we introduced new call site) and thus all we need is to insert newly\n    created edges into heap.  */\n \n static void\n update_callee_keys (edge_heap_t *heap, struct cgraph_node *node,\n+\t\t    struct cgraph_node *update_since,\n \t\t    bitmap updated_nodes)\n {\n   struct cgraph_edge *e = node->callees;\n+  bool check_inlinability = update_since == node;\n \n   if (!e)\n     return;\n   while (true)\n     if (!e->inline_failed && e->callee->callees)\n-      e = e->callee->callees;\n+      {\n+\tif (e->callee == update_since)\n+\t  check_inlinability = true;\n+        e = e->callee->callees;\n+      }\n     else\n       {\n \tenum availability avail;\n \tstruct cgraph_node *callee;\n+\tif (!check_inlinability)\n+\t  {\n+\t    if (e->aux\n+\t\t&& !bitmap_bit_p (updated_nodes,\n+\t\t \t\t  e->callee->ultimate_alias_target\n+\t\t\t\t    (&avail, e->caller)->get_uid ()))\n+\t      update_edge_key (heap, e);\n+\t  }\n \t/* We do not reset callee growth cache here.  Since we added a new call,\n \t   growth chould have just increased and consequentely badness metric\n            don't need updating.  */\n-\tif (e->inline_failed\n-\t    && (callee = e->callee->ultimate_alias_target (&avail, e->caller))\n-\t    && ipa_fn_summaries->get (callee) != NULL\n-\t    && ipa_fn_summaries->get (callee)->inlinable\n-\t    && avail >= AVAIL_AVAILABLE\n-\t    && !bitmap_bit_p (updated_nodes, callee->get_uid ()))\n+\telse if (e->inline_failed\n+\t\t && (callee = e->callee->ultimate_alias_target (&avail,\n+\t\t  \t\t\t\t\t\te->caller))\n+\t\t && avail >= AVAIL_AVAILABLE\n+\t\t && ipa_fn_summaries->get (callee) != NULL\n+\t\t && ipa_fn_summaries->get (callee)->inlinable\n+\t\t && !bitmap_bit_p (updated_nodes, callee->get_uid ()))\n \t  {\n \t    if (can_inline_edge_p (e, false)\n \t\t&& want_inline_small_function_p (e, false)\n \t\t&& can_inline_edge_by_limits_p (e, false))\n-\t      update_edge_key (heap, e);\n+\t      {\n+\t\tgcc_checking_assert (check_inlinability || can_inline_edge_p (e, false));\n+\t\tgcc_checking_assert (check_inlinability || e->aux);\n+\t        update_edge_key (heap, e);\n+\t      }\n \t    else if (e->aux)\n \t      {\n \t\treport_inline_failed_reason (e);\n \t\theap->delete_node ((edge_heap_node_t *) e->aux);\n \t\te->aux = NULL;\n \t      }\n \t  }\n+\t/* In case we redirected to unreachable node we only need to remove the\n+\t   fibheap entry.  */\n+\telse if (e->aux)\n+\t  {\n+\t    heap->delete_node ((edge_heap_node_t *) e->aux);\n+\t    e->aux = NULL;\n+\t  }\n \tif (e->next_callee)\n \t  e = e->next_callee;\n \telse\n@@ -1530,6 +1560,8 @@ update_callee_keys (edge_heap_t *heap, struct cgraph_node *node,\n \t      {\n \t\tif (e->caller == node)\n \t\t  return;\n+\t\tif (e->caller == update_since)\n+\t\t  check_inlinability = false;\n \t\te = e->caller->callers;\n \t      }\n \t    while (!e->next_callee);\n@@ -1820,7 +1852,7 @@ resolve_noninline_speculation (edge_heap_t *edge_heap, struct cgraph_edge *edge)\n       ipa_update_overall_fn_summary (where);\n       update_caller_keys (edge_heap, where,\n \t\t\t  updated_nodes, NULL);\n-      update_callee_keys (edge_heap, where,\n+      update_callee_keys (edge_heap, where, NULL,\n \t\t\t  updated_nodes);\n     }\n }\n@@ -1993,7 +2025,7 @@ inline_small_functions (void)\n \t  reset_edge_caches (where);\n           update_caller_keys (&edge_heap, where,\n \t\t\t      updated_nodes, NULL);\n-          update_callee_keys (&edge_heap, where,\n+          update_callee_keys (&edge_heap, where, NULL,\n \t\t\t      updated_nodes);\n           bitmap_clear (updated_nodes);\n \t}\n@@ -2124,6 +2156,8 @@ inline_small_functions (void)\n \t  continue;\n \t}\n \n+      profile_count old_count = callee->count;\n+\n       /* Heuristics for inlining small functions work poorly for\n \t recursive calls where we do effects similar to loop unrolling.\n \t When inlining such edge seems profitable, leave decision on\n@@ -2147,7 +2181,7 @@ inline_small_functions (void)\n \t     at once. Consequently we need to update all callee keys.  */\n \t  if (opt_for_fn (edge->caller->decl, flag_indirect_inlining))\n \t    add_new_edges_to_heap (&edge_heap, new_indirect_edges);\n-          update_callee_keys (&edge_heap, where, updated_nodes);\n+          update_callee_keys (&edge_heap, where, where, updated_nodes);\n \t  bitmap_clear (updated_nodes);\n \t}\n       else\n@@ -2197,9 +2231,12 @@ inline_small_functions (void)\n \t      /* Wrapper penalty may be non-monotonous in this respect.\n \t         Fortunately it only affects small functions.  */\n \t      && !wrapper_heuristics_may_apply (where, old_size))\n-\t    update_callee_keys (&edge_heap, edge->callee, updated_nodes);\n+\t    update_callee_keys (&edge_heap, edge->callee, edge->callee,\n+\t\t\t   \tupdated_nodes);\n \t  else\n-\t    update_callee_keys (&edge_heap, where, updated_nodes);\n+\t    update_callee_keys (&edge_heap, where,\n+\t\t\t\tedge->callee,\n+\t\t\t   \tupdated_nodes);\n \t}\n       where = edge->caller;\n       if (where->inlined_to)\n@@ -2214,9 +2251,11 @@ inline_small_functions (void)\n       update_caller_keys (&edge_heap, where, updated_nodes, NULL);\n       /* Offline copy count has possibly changed, recompute if profile is\n \t available.  */\n-      struct cgraph_node *n = cgraph_node::get (edge->callee->decl);\n-      if (n != edge->callee && n->analyzed && n->count.ipa ().initialized_p ())\n-\tupdate_callee_keys (&edge_heap, n, updated_nodes);\n+      struct cgraph_node *n\n+\t      = cgraph_node::get (edge->callee->decl)->ultimate_alias_target ();\n+      if (n != edge->callee && n->analyzed && !(n->count == old_count)\n+\t  && n->count.ipa_p ())\n+\tupdate_callee_keys (&edge_heap, n, NULL, updated_nodes);\n       bitmap_clear (updated_nodes);\n \n       if (dump_enabled_p ())"}]}