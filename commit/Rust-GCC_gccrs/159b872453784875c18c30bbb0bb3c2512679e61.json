{"sha": "159b872453784875c18c30bbb0bb3c2512679e61", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MTU5Yjg3MjQ1Mzc4NDg3NWMxOGMzMGJiYjBiYjNjMjUxMjY3OWU2MQ==", "commit": {"author": {"name": "Tamar Christina", "email": "tamar.christina@arm.com", "date": "2016-11-28T12:41:03Z"}, "committer": {"name": "Tamar Christina", "email": "tnfchris@gcc.gnu.org", "date": "2016-11-28T12:41:03Z"}, "message": "aarch64-builtins.c (TYPES_SETREGP): Added poly type.\n\n2016-11-28  Tamar Christina  <tamar.christina@arm.com>\n\n\t* config/aarch64/aarch64-builtins.c (TYPES_SETREGP): Added poly type.\n\t(TYPES_GETREGP): Likewise.\n\t(TYPES_SHIFTINSERTP): Likewise.\n\t(TYPES_COMBINEP): Likewise.\n\t(TYPES_STORE1P): Likewise.\n\t* config/aarch64/aarch64-simd-builtins.def\n\t(combine): Added poly generator.\n\t(get_dregoi): Likewise.\n\t(get_dregci): Likewise.\n\t(get_dregxi): Likewise.\n\t(ssli_n): Likewise.\n\t(ld1): Likewise.\n\t(st1): Likewise.\n\t* config/aarch64/arm_neon.h\n\t(poly64x1x2_t, poly64x1x3_t): New.\n\t(poly64x1x4_t, poly64x2x2_t): Likewise.\n\t(poly64x2x3_t, poly64x2x4_t): Likewise.\n\t(poly64x1_t): Likewise.\n\t(vcreate_p64, vcombine_p64): Likewise.\n\t(vdup_n_p64, vdupq_n_p64): Likewise.\n\t(vld2_p64, vld2q_p64): Likewise.\n\t(vld3_p64, vld3q_p64): Likewise.\n\t(vld4_p64, vld4q_p64): Likewise.\n\t(vld2_dup_p64, vld3_dup_p64): Likewise.\n\t(vld4_dup_p64, vsli_n_p64): Likewise.\n\t(vsliq_n_p64, vst1_p64): Likewise.\n\t(vst1q_p64, vst2_p64): Likewise.\n\t(vst3_p64, vst4_p64): Likewise.\n\t(__aarch64_vdup_lane_p64, __aarch64_vdup_laneq_p64): Likewise.\n\t(__aarch64_vdupq_lane_p64, __aarch64_vdupq_laneq_p64): Likewise.\n\t(vget_lane_p64, vgetq_lane_p64): Likewise.\n\t(vreinterpret_p8_p64, vreinterpretq_p8_p64): Likewise.\n\t(vreinterpret_p16_p64, vreinterpretq_p16_p64): Likewise.\n\t(vreinterpret_p64_f16, vreinterpret_p64_f64): Likewise.\n\t(vreinterpret_p64_s8, vreinterpret_p64_s16): Likewise.\n\t(vreinterpret_p64_s32, vreinterpret_p64_s64): Likewise.\n\t(vreinterpret_p64_f32, vreinterpret_p64_u8): Likewise.\n\t(vreinterpret_p64_u16, vreinterpret_p64_u32): Likewise.\n\t(vreinterpret_p64_u64, vreinterpret_p64_p8): Likewise.\n\t(vreinterpretq_p64_f64, vreinterpretq_p64_s8): Likewise.\n\t(vreinterpretq_p64_s16, vreinterpretq_p64_s32): Likewise.\n\t(vreinterpretq_p64_s64, vreinterpretq_p64_f16): Likewise.\n\t(vreinterpretq_p64_f32, vreinterpretq_p64_u8): Likewise.\n\t(vreinterpretq_p64_u16, vreinterpretq_p64_u32): Likewise.\n\t(vreinterpretq_p64_u64, vreinterpretq_p64_p8): Likewise.\n\t(vreinterpret_f16_p64, vreinterpretq_f16_p64): Likewise.\n\t(vreinterpret_f32_p64, vreinterpretq_f32_p64): Likewise.\n\t(vreinterpret_f64_p64, vreinterpretq_f64_p64): Likewise.\n\t(vreinterpret_s64_p64, vreinterpretq_s64_p64): Likewise.\n\t(vreinterpret_u64_p64, vreinterpretq_u64_p64): Likewise.\n\t(vreinterpret_s8_p64, vreinterpretq_s8_p64): Likewise.\n\t(vreinterpret_s16_p64, vreinterpret_s32_p64): Likewise.\n\t(vreinterpretq_s32_p64, vreinterpret_u8_p64): Likewise.\n\t(vreinterpret_u16_p64, vreinterpretq_u16_p64): Likewise.\n\t(vreinterpret_u32_p64, vreinterpretq_u32_p64): Likewise.\n\t(vset_lane_p64, vsetq_lane_p64): Likewise.\n\t(vget_low_p64, vget_high_p64): Likewise.\n\t(vcombine_p64, vst2_lane_p64): Likewise.\n\t(vst3_lane_p64, vst4_lane_p64): Likewise.\n\t(vst2q_lane_p64, vst3q_lane_p64): Likewise.\n\t(vst4q_lane_p64, vget_lane_p64): Likewise.\n\t(vget_laneq_p64, vset_lane_p64): Likewise.\n\t(vset_laneq_p64, vcopy_lane_p64): Likewise.\n\t(vcopy_laneq_p64, vdup_n_p64): Likewise.\n\t(vdupq_n_p64, vdup_lane_p64): Likewise.\n\t(vdup_laneq_p64, vld1_p64): Likewise.\n\t(vld1q_p64, vld1_dup_p64): Likewise.\n\t(vld1q_dup_p64, vld1q_dup_p64): Likewise.\n\t(vmov_n_p64, vmovq_n_p64): Likewise.\n\t(vst3q_p64, vst4q_p64): Likewise.\n\t(vld1_lane_p64, vld1q_lane_p64): Likewise.\n\t(vst1_lane_p64, vst1q_lane_p64): Likewise.\n\t(vcopy_laneq_p64, vcopyq_laneq_p64): Likewise.\n\t(vdupq_laneq_p64): Likewise.\n\nFrom-SVN: r242915", "tree": {"sha": "dd1edb17304cdcb086bd4fba9f06674c93f49b22", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/dd1edb17304cdcb086bd4fba9f06674c93f49b22"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/159b872453784875c18c30bbb0bb3c2512679e61", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/159b872453784875c18c30bbb0bb3c2512679e61", "html_url": "https://github.com/Rust-GCC/gccrs/commit/159b872453784875c18c30bbb0bb3c2512679e61", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/159b872453784875c18c30bbb0bb3c2512679e61/comments", "author": {"login": "TamarChristinaArm", "id": 48126768, "node_id": "MDQ6VXNlcjQ4MTI2NzY4", "avatar_url": "https://avatars.githubusercontent.com/u/48126768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TamarChristinaArm", "html_url": "https://github.com/TamarChristinaArm", "followers_url": "https://api.github.com/users/TamarChristinaArm/followers", "following_url": "https://api.github.com/users/TamarChristinaArm/following{/other_user}", "gists_url": "https://api.github.com/users/TamarChristinaArm/gists{/gist_id}", "starred_url": "https://api.github.com/users/TamarChristinaArm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TamarChristinaArm/subscriptions", "organizations_url": "https://api.github.com/users/TamarChristinaArm/orgs", "repos_url": "https://api.github.com/users/TamarChristinaArm/repos", "events_url": "https://api.github.com/users/TamarChristinaArm/events{/privacy}", "received_events_url": "https://api.github.com/users/TamarChristinaArm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "f76f4b23a4fe974f9cb55077535b1e73bc8011c1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f76f4b23a4fe974f9cb55077535b1e73bc8011c1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f76f4b23a4fe974f9cb55077535b1e73bc8011c1"}], "stats": {"total": 1022, "additions": 1014, "deletions": 8}, "files": [{"sha": "b0594f30d0a8bf5ab3ee9ccc128f72e22946efed", "filename": "gcc/ChangeLog", "status": "modified", "additions": 77, "deletions": 0, "changes": 77, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/159b872453784875c18c30bbb0bb3c2512679e61/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/159b872453784875c18c30bbb0bb3c2512679e61/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=159b872453784875c18c30bbb0bb3c2512679e61", "patch": "@@ -1,3 +1,80 @@\n+2016-11-28  Tamar Christina  <tamar.christina@arm.com>\n+\n+\t* config/aarch64/aarch64-builtins.c (TYPES_SETREGP): Added poly type.\n+\t(TYPES_GETREGP): Likewise.\n+\t(TYPES_SHIFTINSERTP): Likewise.\n+\t(TYPES_COMBINEP): Likewise.\n+\t(TYPES_STORE1P): Likewise.\n+\t* config/aarch64/aarch64-simd-builtins.def\n+\t(combine): Added poly generator.\n+\t(get_dregoi): Likewise.\n+\t(get_dregci): Likewise.\n+\t(get_dregxi): Likewise.\n+\t(ssli_n): Likewise.\n+\t(ld1): Likewise.\n+\t(st1): Likewise.\n+\t* config/aarch64/arm_neon.h\n+\t(poly64x1x2_t, poly64x1x3_t): New.\n+\t(poly64x1x4_t, poly64x2x2_t): Likewise.\n+\t(poly64x2x3_t, poly64x2x4_t): Likewise.\n+\t(poly64x1_t): Likewise.\n+\t(vcreate_p64, vcombine_p64): Likewise.\n+\t(vdup_n_p64, vdupq_n_p64): Likewise.\n+\t(vld2_p64, vld2q_p64): Likewise.\n+\t(vld3_p64, vld3q_p64): Likewise.\n+\t(vld4_p64, vld4q_p64): Likewise.\n+\t(vld2_dup_p64, vld3_dup_p64): Likewise.\n+\t(vld4_dup_p64, vsli_n_p64): Likewise.\n+\t(vsliq_n_p64, vst1_p64): Likewise.\n+\t(vst1q_p64, vst2_p64): Likewise.\n+\t(vst3_p64, vst4_p64): Likewise.\n+\t(__aarch64_vdup_lane_p64, __aarch64_vdup_laneq_p64): Likewise.\n+\t(__aarch64_vdupq_lane_p64, __aarch64_vdupq_laneq_p64): Likewise.\n+\t(vget_lane_p64, vgetq_lane_p64): Likewise.\n+\t(vreinterpret_p8_p64, vreinterpretq_p8_p64): Likewise.\n+\t(vreinterpret_p16_p64, vreinterpretq_p16_p64): Likewise.\n+\t(vreinterpret_p64_f16, vreinterpret_p64_f64): Likewise.\n+\t(vreinterpret_p64_s8, vreinterpret_p64_s16): Likewise.\n+\t(vreinterpret_p64_s32, vreinterpret_p64_s64): Likewise.\n+\t(vreinterpret_p64_f32, vreinterpret_p64_u8): Likewise.\n+\t(vreinterpret_p64_u16, vreinterpret_p64_u32): Likewise.\n+\t(vreinterpret_p64_u64, vreinterpret_p64_p8): Likewise.\n+\t(vreinterpretq_p64_f64, vreinterpretq_p64_s8): Likewise.\n+\t(vreinterpretq_p64_s16, vreinterpretq_p64_s32): Likewise.\n+\t(vreinterpretq_p64_s64, vreinterpretq_p64_f16): Likewise.\n+\t(vreinterpretq_p64_f32, vreinterpretq_p64_u8): Likewise.\n+\t(vreinterpretq_p64_u16, vreinterpretq_p64_u32): Likewise.\n+\t(vreinterpretq_p64_u64, vreinterpretq_p64_p8): Likewise.\n+\t(vreinterpret_f16_p64, vreinterpretq_f16_p64): Likewise.\n+\t(vreinterpret_f32_p64, vreinterpretq_f32_p64): Likewise.\n+\t(vreinterpret_f64_p64, vreinterpretq_f64_p64): Likewise.\n+\t(vreinterpret_s64_p64, vreinterpretq_s64_p64): Likewise.\n+\t(vreinterpret_u64_p64, vreinterpretq_u64_p64): Likewise.\n+\t(vreinterpret_s8_p64, vreinterpretq_s8_p64): Likewise.\n+\t(vreinterpret_s16_p64, vreinterpret_s32_p64): Likewise.\n+\t(vreinterpretq_s32_p64, vreinterpret_u8_p64): Likewise.\n+\t(vreinterpret_u16_p64, vreinterpretq_u16_p64): Likewise.\n+\t(vreinterpret_u32_p64, vreinterpretq_u32_p64): Likewise.\n+\t(vset_lane_p64, vsetq_lane_p64): Likewise.\n+\t(vget_low_p64, vget_high_p64): Likewise.\n+\t(vcombine_p64, vst2_lane_p64): Likewise.\n+\t(vst3_lane_p64, vst4_lane_p64): Likewise.\n+\t(vst2q_lane_p64, vst3q_lane_p64): Likewise.\n+\t(vst4q_lane_p64, vget_lane_p64): Likewise.\n+\t(vget_laneq_p64, vset_lane_p64): Likewise.\n+\t(vset_laneq_p64, vcopy_lane_p64): Likewise.\n+\t(vcopy_laneq_p64, vdup_n_p64): Likewise.\n+\t(vdupq_n_p64, vdup_lane_p64): Likewise.\n+\t(vdup_laneq_p64, vld1_p64): Likewise.\n+\t(vld1q_p64, vld1_dup_p64): Likewise.\n+\t(vld1q_dup_p64, vld1q_dup_p64): Likewise.\n+\t(vmov_n_p64, vmovq_n_p64): Likewise.\n+\t(vst3q_p64, vst4q_p64): Likewise.\n+\t(vld1_lane_p64, vld1q_lane_p64): Likewise.\n+\t(vst1_lane_p64, vst1q_lane_p64): Likewise.\n+\t(vcopy_laneq_p64, vcopyq_laneq_p64): Likewise.\n+\t(vdupq_laneq_p64): Likewise.\n+\n 2016-11-28  Tamar Christina  <tamar.christina@arm.com>\n \t* config/arm/arm_neon.h (vget_lane_p64): New.\n "}, {"sha": "05cc52eba33053f60fb3a590f7f88d178f9fd905", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "modified", "additions": 27, "deletions": 5, "changes": 32, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/159b872453784875c18c30bbb0bb3c2512679e61/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/159b872453784875c18c30bbb0bb3c2512679e61/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=159b872453784875c18c30bbb0bb3c2512679e61", "patch": "@@ -169,6 +169,10 @@ aarch64_types_quadop_lane_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n       qualifier_none, qualifier_lane_index };\n #define TYPES_QUADOP_LANE (aarch64_types_quadop_lane_qualifiers)\n \n+static enum aarch64_type_qualifiers\n+aarch64_types_binop_imm_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_poly, qualifier_none, qualifier_immediate };\n+#define TYPES_GETREGP (aarch64_types_binop_imm_p_qualifiers)\n static enum aarch64_type_qualifiers\n aarch64_types_binop_imm_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_none, qualifier_immediate };\n@@ -188,11 +192,20 @@ aarch64_types_unsigned_shift_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n #define TYPES_USHIFTIMM (aarch64_types_unsigned_shift_qualifiers)\n \n static enum aarch64_type_qualifiers\n-aarch64_types_ternop_imm_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n-  = { qualifier_none, qualifier_none, qualifier_none, qualifier_immediate };\n-#define TYPES_SETREG (aarch64_types_ternop_imm_qualifiers)\n-#define TYPES_SHIFTINSERT (aarch64_types_ternop_imm_qualifiers)\n-#define TYPES_SHIFTACC (aarch64_types_ternop_imm_qualifiers)\n+aarch64_types_ternop_s_imm_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_none, qualifier_none, qualifier_poly, qualifier_immediate};\n+#define TYPES_SETREGP (aarch64_types_ternop_s_imm_p_qualifiers)\n+static enum aarch64_type_qualifiers\n+aarch64_types_ternop_s_imm_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_none, qualifier_none, qualifier_none, qualifier_immediate};\n+#define TYPES_SETREG (aarch64_types_ternop_s_imm_qualifiers)\n+#define TYPES_SHIFTINSERT (aarch64_types_ternop_s_imm_qualifiers)\n+#define TYPES_SHIFTACC (aarch64_types_ternop_s_imm_qualifiers)\n+\n+static enum aarch64_type_qualifiers\n+aarch64_types_ternop_p_imm_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_poly, qualifier_poly, qualifier_poly, qualifier_immediate};\n+#define TYPES_SHIFTINSERTP (aarch64_types_ternop_p_imm_qualifiers)\n \n static enum aarch64_type_qualifiers\n aarch64_types_unsigned_shiftacc_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n@@ -206,6 +219,11 @@ aarch64_types_combine_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_none, qualifier_none };\n #define TYPES_COMBINE (aarch64_types_combine_qualifiers)\n \n+static enum aarch64_type_qualifiers\n+aarch64_types_combine_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_poly, qualifier_poly, qualifier_poly };\n+#define TYPES_COMBINEP (aarch64_types_combine_p_qualifiers)\n+\n static enum aarch64_type_qualifiers\n aarch64_types_load1_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_const_pointer_map_mode };\n@@ -239,6 +257,10 @@ aarch64_types_bsl_u_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n    qualifier_map_mode | qualifier_pointer to build a pointer to the\n    element type of the vector.  */\n static enum aarch64_type_qualifiers\n+aarch64_types_store1_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_void, qualifier_pointer_map_mode, qualifier_poly };\n+#define TYPES_STORE1P (aarch64_types_store1_p_qualifiers)\n+static enum aarch64_type_qualifiers\n aarch64_types_store1_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_void, qualifier_pointer_map_mode, qualifier_none };\n #define TYPES_STORE1 (aarch64_types_store1_qualifiers)"}, {"sha": "bc8a85dcf03cc5e52891ae4300ec721e7a533b9b", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/159b872453784875c18c30bbb0bb3c2512679e61/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/159b872453784875c18c30bbb0bb3c2512679e61/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=159b872453784875c18c30bbb0bb3c2512679e61", "patch": "@@ -40,6 +40,7 @@\n    10 - CODE_FOR_<name><mode>.  */\n \n   BUILTIN_VDC (COMBINE, combine, 0)\n+  VAR1 (COMBINEP, combine, 0, di)\n   BUILTIN_VB (BINOP, pmul, 0)\n   BUILTIN_VHSDF_HSDF (BINOP, fmulx, 0)\n   BUILTIN_VHSDF_DF (UNOP, sqrt, 2)\n@@ -68,14 +69,23 @@\n   BUILTIN_VDC (GETREG, get_dregoi, 0)\n   BUILTIN_VDC (GETREG, get_dregci, 0)\n   BUILTIN_VDC (GETREG, get_dregxi, 0)\n+  VAR1 (GETREGP, get_dregoi, 0, di)\n+  VAR1 (GETREGP, get_dregci, 0, di)\n+  VAR1 (GETREGP, get_dregxi, 0, di)\n   /* Implemented by aarch64_get_qreg<VSTRUCT:mode><VQ:mode>.  */\n   BUILTIN_VQ (GETREG, get_qregoi, 0)\n   BUILTIN_VQ (GETREG, get_qregci, 0)\n   BUILTIN_VQ (GETREG, get_qregxi, 0)\n+  VAR1 (GETREGP, get_qregoi, 0, v2di)\n+  VAR1 (GETREGP, get_qregci, 0, v2di)\n+  VAR1 (GETREGP, get_qregxi, 0, v2di)\n   /* Implemented by aarch64_set_qreg<VSTRUCT:mode><VQ:mode>.  */\n   BUILTIN_VQ (SETREG, set_qregoi, 0)\n   BUILTIN_VQ (SETREG, set_qregci, 0)\n   BUILTIN_VQ (SETREG, set_qregxi, 0)\n+  VAR1 (SETREGP, set_qregoi, 0, v2di)\n+  VAR1 (SETREGP, set_qregci, 0, v2di)\n+  VAR1 (SETREGP, set_qregxi, 0, v2di)\n   /* Implemented by aarch64_ld<VSTRUCT:nregs><VDC:mode>.  */\n   BUILTIN_VDC (LOADSTRUCT, ld2, 0)\n   BUILTIN_VDC (LOADSTRUCT, ld3, 0)\n@@ -224,6 +234,7 @@\n   BUILTIN_VSDQ_I_DI (SHIFTINSERT, ssri_n, 0)\n   BUILTIN_VSDQ_I_DI (USHIFTACC, usri_n, 0)\n   BUILTIN_VSDQ_I_DI (SHIFTINSERT, ssli_n, 0)\n+  VAR2 (SHIFTINSERTP, ssli_n, 0, di, v2di)\n   BUILTIN_VSDQ_I_DI (USHIFTACC, usli_n, 0)\n   /* Implemented by aarch64_<sur>qshl<u>_n<mode>.  */\n   BUILTIN_VSDQ_I (SHIFTIMM_USS, sqshlu_n, 0)\n@@ -416,9 +427,11 @@\n \n   /* Implemented by aarch64_ld1<VALL_F16:mode>.  */\n   BUILTIN_VALL_F16 (LOAD1, ld1, 0)\n+  VAR1(STORE1P, ld1, 0, v2di)\n \n   /* Implemented by aarch64_st1<VALL_F16:mode>.  */\n   BUILTIN_VALL_F16 (STORE1, st1, 0)\n+  VAR1(STORE1P, st1, 0, v2di)\n \n   /* Implemented by fma<mode>4.  */\n   BUILTIN_VHSDF (TERNOP, fma, 4)"}, {"sha": "d39adf1f19a5d82f39b638e14e69906fc3d3f713", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 897, "deletions": 3, "changes": 900, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/159b872453784875c18c30bbb0bb3c2512679e61/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/159b872453784875c18c30bbb0bb3c2512679e61/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=159b872453784875c18c30bbb0bb3c2512679e61", "patch": "@@ -58,6 +58,7 @@ typedef __Float64x2_t float64x2_t;\n typedef __Poly8x16_t poly8x16_t;\n typedef __Poly16x8_t poly16x8_t;\n typedef __Poly64x2_t poly64x2_t;\n+typedef __Poly64x1_t poly64x1_t;\n typedef __Uint8x16_t uint8x16_t;\n typedef __Uint16x8_t uint16x8_t;\n typedef __Uint32x4_t uint32x4_t;\n@@ -202,6 +203,36 @@ typedef struct poly16x8x2_t\n   poly16x8_t val[2];\n } poly16x8x2_t;\n \n+typedef struct poly64x1x2_t\n+{\n+  poly64x1_t val[2];\n+} poly64x1x2_t;\n+\n+typedef struct poly64x1x3_t\n+{\n+  poly64x1_t val[3];\n+} poly64x1x3_t;\n+\n+typedef struct poly64x1x4_t\n+{\n+  poly64x1_t val[4];\n+} poly64x1x4_t;\n+\n+typedef struct poly64x2x2_t\n+{\n+  poly64x2_t val[2];\n+} poly64x2x2_t;\n+\n+typedef struct poly64x2x3_t\n+{\n+  poly64x2_t val[3];\n+} poly64x2x3_t;\n+\n+typedef struct poly64x2x4_t\n+{\n+  poly64x2_t val[4];\n+} poly64x2x4_t;\n+\n typedef struct int8x8x3_t\n {\n   int8x8_t val[3];\n@@ -476,6 +507,8 @@ typedef struct poly16x8x4_t\n    __aarch64_vdup_lane_any (p8, , __a, __b)\n #define __aarch64_vdup_lane_p16(__a, __b) \\\n    __aarch64_vdup_lane_any (p16, , __a, __b)\n+#define __aarch64_vdup_lane_p64(__a, __b) \\\n+   __aarch64_vdup_lane_any (p64, , __a, __b)\n #define __aarch64_vdup_lane_s8(__a, __b) \\\n    __aarch64_vdup_lane_any (s8, , __a, __b)\n #define __aarch64_vdup_lane_s16(__a, __b) \\\n@@ -504,6 +537,8 @@ typedef struct poly16x8x4_t\n    __aarch64_vdup_lane_any (p8, , __a, __b)\n #define __aarch64_vdup_laneq_p16(__a, __b) \\\n    __aarch64_vdup_lane_any (p16, , __a, __b)\n+#define __aarch64_vdup_laneq_p64(__a, __b) \\\n+   __aarch64_vdup_lane_any (p64, , __a, __b)\n #define __aarch64_vdup_laneq_s8(__a, __b) \\\n    __aarch64_vdup_lane_any (s8, , __a, __b)\n #define __aarch64_vdup_laneq_s16(__a, __b) \\\n@@ -532,6 +567,8 @@ typedef struct poly16x8x4_t\n    __aarch64_vdup_lane_any (p8, q, __a, __b)\n #define __aarch64_vdupq_lane_p16(__a, __b) \\\n    __aarch64_vdup_lane_any (p16, q, __a, __b)\n+#define __aarch64_vdupq_lane_p64(__a, __b) \\\n+   __aarch64_vdup_lane_any (p64, q, __a, __b)\n #define __aarch64_vdupq_lane_s8(__a, __b) \\\n    __aarch64_vdup_lane_any (s8, q, __a, __b)\n #define __aarch64_vdupq_lane_s16(__a, __b) \\\n@@ -560,6 +597,8 @@ typedef struct poly16x8x4_t\n    __aarch64_vdup_lane_any (p8, q, __a, __b)\n #define __aarch64_vdupq_laneq_p16(__a, __b) \\\n    __aarch64_vdup_lane_any (p16, q, __a, __b)\n+#define __aarch64_vdupq_laneq_p64(__a, __b) \\\n+   __aarch64_vdup_lane_any (p64, q, __a, __b)\n #define __aarch64_vdupq_laneq_s8(__a, __b) \\\n    __aarch64_vdup_lane_any (s8, q, __a, __b)\n #define __aarch64_vdupq_laneq_s16(__a, __b) \\\n@@ -3076,6 +3115,13 @@ vcreate_p16 (uint64_t __a)\n   return (poly16x4_t) __a;\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcreate_p64 (uint64_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n /* vget_lane  */\n \n __extension__ extern __inline float16_t\n@@ -3113,6 +3159,13 @@ vget_lane_p16 (poly16x4_t __a, const int __b)\n   return __aarch64_vget_lane_any (__a, __b);\n }\n \n+__extension__ extern __inline poly64_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vget_lane_p64 (poly64x1_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_any (__a, __b);\n+}\n+\n __extension__ extern __inline int8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vget_lane_s8 (int8x8_t __a, const int __b)\n@@ -3206,6 +3259,13 @@ vgetq_lane_p16 (poly16x8_t __a, const int __b)\n   return __aarch64_vget_lane_any (__a, __b);\n }\n \n+__extension__ extern __inline poly64_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vgetq_lane_p64 (poly64x2_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_any (__a, __b);\n+}\n+\n __extension__ extern __inline int8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vgetq_lane_s8 (int8x16_t __a, const int __b)\n@@ -3348,6 +3408,13 @@ vreinterpret_p8_p16 (poly16x4_t __a)\n   return (poly8x8_t) __a;\n }\n \n+__extension__ extern __inline poly8x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p8_p64 (poly64x1_t __a)\n+{\n+  return (poly8x8_t) __a;\n+}\n+\n __extension__ extern __inline poly8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_p8_f64 (float64x2_t __a)\n@@ -3432,6 +3499,13 @@ vreinterpretq_p8_p16 (poly16x8_t __a)\n   return (poly8x16_t) __a;\n }\n \n+__extension__ extern __inline poly8x16_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p8_p64 (poly64x2_t __a)\n+{\n+  return (poly8x16_t) __a;\n+}\n+\n __extension__ extern __inline poly16x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_p16_f16 (float16x4_t __a)\n@@ -3516,6 +3590,13 @@ vreinterpret_p16_p8 (poly8x8_t __a)\n   return (poly16x4_t) __a;\n }\n \n+__extension__ extern __inline poly16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p16_p64 (poly64x1_t __a)\n+{\n+  return (poly16x4_t) __a;\n+}\n+\n __extension__ extern __inline poly16x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_p16_f64 (float64x2_t __a)\n@@ -3600,6 +3681,181 @@ vreinterpretq_p16_p8 (poly8x16_t __a)\n   return (poly16x8_t) __a;\n }\n \n+__extension__ extern __inline poly16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p16_p64 (poly64x2_t __a)\n+{\n+  return (poly16x8_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_f16 (float16x4_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_f64 (float64x1_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_s8 (int8x8_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_s16 (int16x4_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_s32 (int32x2_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_s64 (int64x1_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_f32 (float32x2_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_u8 (uint8x8_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_u16 (uint16x4_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_u32 (uint32x2_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_u64 (uint64x1_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_p64_p8 (poly8x8_t __a)\n+{\n+  return (poly64x1_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_f64 (float64x2_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_s8 (int8x16_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_s16 (int16x8_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_s32 (int32x4_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_s64 (int64x2_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_f16 (float16x8_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_f32 (float32x4_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_u8 (uint8x16_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_u16 (uint16x8_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_u32 (uint32x4_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_u64 (uint64x2_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_p64_p8 (poly8x16_t __a)\n+{\n+  return (poly64x2_t) __a;\n+}\n+\n __extension__ extern __inline float16x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_f16_f64 (float64x1_t __a)\n@@ -3684,6 +3940,13 @@ vreinterpret_f16_p16 (poly16x4_t __a)\n   return (float16x4_t) __a;\n }\n \n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_f16_p64 (poly64x1_t __a)\n+{\n+  return (float16x4_t) __a;\n+}\n+\n __extension__ extern __inline float16x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_f16_f64 (float64x2_t __a)\n@@ -3768,6 +4031,13 @@ vreinterpretq_f16_p16 (poly16x8_t __a)\n   return (float16x8_t) __a;\n }\n \n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_f16_p64 (poly64x2_t __a)\n+{\n+  return (float16x8_t) __a;\n+}\n+\n __extension__ extern __inline float32x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_f32_f16 (float16x4_t __a)\n@@ -3852,6 +4122,13 @@ vreinterpret_f32_p16 (poly16x4_t __a)\n   return (float32x2_t) __a;\n }\n \n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_f32_p64 (poly64x1_t __a)\n+{\n+  return (float32x2_t) __a;\n+}\n+\n __extension__ extern __inline float32x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_f32_f16 (float16x8_t __a)\n@@ -3936,6 +4213,13 @@ vreinterpretq_f32_p16 (poly16x8_t __a)\n   return (float32x4_t) __a;\n }\n \n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_f32_p64 (poly64x2_t __a)\n+{\n+  return (float32x4_t) __a;\n+}\n+\n __extension__ extern __inline float64x1_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_f64_f16 (float16x4_t __a)\n@@ -3964,6 +4248,13 @@ vreinterpret_f64_p16 (poly16x4_t __a)\n   return (float64x1_t) __a;\n }\n \n+__extension__ extern __inline float64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_f64_p64 (poly64x1_t __a)\n+{\n+  return (float64x1_t) __a;\n+}\n+\n __extension__ extern __inline float64x1_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_f64_s8 (int8x8_t __a)\n@@ -4048,6 +4339,13 @@ vreinterpretq_f64_p16 (poly16x8_t __a)\n   return (float64x2_t) __a;\n }\n \n+__extension__ extern __inline float64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_f64_p64 (poly64x2_t __a)\n+{\n+  return (float64x2_t) __a;\n+}\n+\n __extension__ extern __inline float64x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_f64_s8 (int8x16_t __a)\n@@ -4188,6 +4486,13 @@ vreinterpret_s64_p16 (poly16x4_t __a)\n   return (int64x1_t) __a;\n }\n \n+__extension__ extern __inline int64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_s64_p64 (poly64x1_t __a)\n+{\n+  return (int64x1_t) __a;\n+}\n+\n __extension__ extern __inline int64x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_s64_f64 (float64x2_t __a)\n@@ -4272,6 +4577,13 @@ vreinterpretq_s64_p16 (poly16x8_t __a)\n   return (int64x2_t) __a;\n }\n \n+__extension__ extern __inline int64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_s64_p64 (poly64x2_t __a)\n+{\n+  return (int64x2_t) __a;\n+}\n+\n __extension__ extern __inline uint64x1_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_u64_f16 (float16x4_t __a)\n@@ -4356,6 +4668,13 @@ vreinterpret_u64_p16 (poly16x4_t __a)\n   return (uint64x1_t) __a;\n }\n \n+__extension__ extern __inline uint64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_u64_p64 (poly64x1_t __a)\n+{\n+  return (uint64x1_t) __a;\n+}\n+\n __extension__ extern __inline uint64x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_u64_f64 (float64x2_t __a)\n@@ -4440,6 +4759,13 @@ vreinterpretq_u64_p16 (poly16x8_t __a)\n   return (uint64x2_t) __a;\n }\n \n+__extension__ extern __inline uint64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_u64_p64 (poly64x2_t __a)\n+{\n+  return (uint64x2_t) __a;\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_s8_f16 (float16x4_t __a)\n@@ -4524,6 +4850,13 @@ vreinterpret_s8_p16 (poly16x4_t __a)\n   return (int8x8_t) __a;\n }\n \n+__extension__ extern __inline int8x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_s8_p64 (poly64x1_t __a)\n+{\n+  return (int8x8_t) __a;\n+}\n+\n __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_s8_f64 (float64x2_t __a)\n@@ -4608,6 +4941,13 @@ vreinterpretq_s8_p16 (poly16x8_t __a)\n   return (int8x16_t) __a;\n }\n \n+__extension__ extern __inline int8x16_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_s8_p64 (poly64x2_t __a)\n+{\n+  return (int8x16_t) __a;\n+}\n+\n __extension__ extern __inline int16x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_s16_f16 (float16x4_t __a)\n@@ -4692,6 +5032,13 @@ vreinterpret_s16_p16 (poly16x4_t __a)\n   return (int16x4_t) __a;\n }\n \n+__extension__ extern __inline int16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_s16_p64 (poly64x1_t __a)\n+{\n+  return (int16x4_t) __a;\n+}\n+\n __extension__ extern __inline int16x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_s16_f64 (float64x2_t __a)\n@@ -4776,6 +5123,13 @@ vreinterpretq_s16_p16 (poly16x8_t __a)\n   return (int16x8_t) __a;\n }\n \n+__extension__ extern __inline int16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_s16_p64 (poly64x2_t __a)\n+{\n+  return (int16x8_t) __a;\n+}\n+\n __extension__ extern __inline int32x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_s32_f16 (float16x4_t __a)\n@@ -4860,6 +5214,13 @@ vreinterpret_s32_p16 (poly16x4_t __a)\n   return (int32x2_t) __a;\n }\n \n+__extension__ extern __inline int32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_s32_p64 (poly64x1_t __a)\n+{\n+  return (int32x2_t) __a;\n+}\n+\n __extension__ extern __inline int32x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_s32_f64 (float64x2_t __a)\n@@ -4944,6 +5305,13 @@ vreinterpretq_s32_p16 (poly16x8_t __a)\n   return (int32x4_t) __a;\n }\n \n+__extension__ extern __inline int32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_s32_p64 (poly64x2_t __a)\n+{\n+  return (int32x4_t) __a;\n+}\n+\n __extension__ extern __inline uint8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_u8_f16 (float16x4_t __a)\n@@ -5028,6 +5396,13 @@ vreinterpret_u8_p16 (poly16x4_t __a)\n   return (uint8x8_t) __a;\n }\n \n+__extension__ extern __inline uint8x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_u8_p64 (poly64x1_t __a)\n+{\n+  return (uint8x8_t) __a;\n+}\n+\n __extension__ extern __inline uint8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_u8_f64 (float64x2_t __a)\n@@ -5112,6 +5487,13 @@ vreinterpretq_u8_p16 (poly16x8_t __a)\n   return (uint8x16_t) __a;\n }\n \n+__extension__ extern __inline uint8x16_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_u8_p64 (poly64x2_t __a)\n+{\n+  return (uint8x16_t) __a;\n+}\n+\n __extension__ extern __inline uint16x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_u16_f16 (float16x4_t __a)\n@@ -5196,6 +5578,13 @@ vreinterpret_u16_p16 (poly16x4_t __a)\n   return (uint16x4_t) __a;\n }\n \n+__extension__ extern __inline uint16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_u16_p64 (poly64x1_t __a)\n+{\n+  return (uint16x4_t) __a;\n+}\n+\n __extension__ extern __inline uint16x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_u16_f64 (float64x2_t __a)\n@@ -5280,6 +5669,13 @@ vreinterpretq_u16_p16 (poly16x8_t __a)\n   return (uint16x8_t) __a;\n }\n \n+__extension__ extern __inline uint16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_u16_p64 (poly64x2_t __a)\n+{\n+  return (uint16x8_t) __a;\n+}\n+\n __extension__ extern __inline uint32x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpret_u32_f16 (float16x4_t __a)\n@@ -5364,6 +5760,13 @@ vreinterpret_u32_p16 (poly16x4_t __a)\n   return (uint32x2_t) __a;\n }\n \n+__extension__ extern __inline uint32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpret_u32_p64 (poly64x1_t __a)\n+{\n+  return (uint32x2_t) __a;\n+}\n+\n __extension__ extern __inline uint32x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vreinterpretq_u32_f64 (float64x2_t __a)\n@@ -5448,6 +5851,13 @@ vreinterpretq_u32_p16 (poly16x8_t __a)\n   return (uint32x4_t) __a;\n }\n \n+__extension__ extern __inline uint32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vreinterpretq_u32_p64 (poly64x2_t __a)\n+{\n+  return (uint32x4_t) __a;\n+}\n+\n /* vset_lane  */\n \n __extension__ extern __inline float16x4_t\n@@ -5485,6 +5895,13 @@ vset_lane_p16 (poly16_t __elem, poly16x4_t __vec, const int __index)\n   return __aarch64_vset_lane_any (__elem, __vec, __index);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vset_lane_p64 (poly64_t __elem, poly64x1_t __vec, const int __index)\n+{\n+  return __aarch64_vset_lane_any (__elem, __vec, __index);\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vset_lane_s8 (int8_t __elem, int8x8_t __vec, const int __index)\n@@ -5571,9 +5988,16 @@ vsetq_lane_p8 (poly8_t __elem, poly8x16_t __vec, const int __index)\n   return __aarch64_vset_lane_any (__elem, __vec, __index);\n }\n \n-__extension__ extern __inline poly16x8_t\n+__extension__ extern __inline poly16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vsetq_lane_p16 (poly16_t __elem, poly16x8_t __vec, const int __index)\n+{\n+  return __aarch64_vset_lane_any (__elem, __vec, __index);\n+}\n+\n+__extension__ extern __inline poly64x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n-vsetq_lane_p16 (poly16_t __elem, poly16x8_t __vec, const int __index)\n+vsetq_lane_p64 (poly64_t __elem, poly64x2_t __vec, const int __index)\n {\n   return __aarch64_vset_lane_any (__elem, __vec, __index);\n }\n@@ -5674,6 +6098,13 @@ vget_low_p16 (poly16x8_t __a)\n   __GET_LOW (p16);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vget_low_p64 (poly64x2_t __a)\n+{\n+  __GET_LOW (p64);\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vget_low_s8 (int8x16_t __a)\n@@ -5772,6 +6203,13 @@ vget_high_p16 (poly16x8_t __a)\n   __GET_HIGH (p16);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vget_high_p64 (poly64x2_t __a)\n+{\n+  __GET_HIGH (p64);\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vget_high_s8 (int8x16_t __a)\n@@ -5926,6 +6364,13 @@ vcombine_p16 (poly16x4_t __a, poly16x4_t __b)\n \t\t\t\t\t\t     (int16x4_t) __b);\n }\n \n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcombine_p64 (poly64x1_t __a, poly64x1_t __b)\n+{\n+  return (poly64x2_t) __builtin_aarch64_combinedi_ppp (__a[0], __b[0]);\n+}\n+\n /* Start of temporary inline asm implementations.  */\n \n __extension__ extern __inline int8x8_t\n@@ -10357,6 +10802,8 @@ __ST2_LANE_FUNC (poly8x8x2_t, poly8x16x2_t, poly8_t, v8qi, v16qi, qi, p8,\n \t\t int8x16_t)\n __ST2_LANE_FUNC (poly16x4x2_t, poly16x8x2_t, poly16_t, v4hi, v8hi, hi, p16,\n \t\t int16x8_t)\n+__ST2_LANE_FUNC (poly64x1x2_t, poly64x2x2_t, poly64_t, di, v2di_ssps, di, p64,\n+\t\t poly64x2_t)\n __ST2_LANE_FUNC (int8x8x2_t, int8x16x2_t, int8_t, v8qi, v16qi, qi, s8,\n \t\t int8x16_t)\n __ST2_LANE_FUNC (int16x4x2_t, int16x8x2_t, int16_t, v4hi, v8hi, hi, s16,\n@@ -10392,6 +10839,7 @@ __ST2_LANE_FUNC (float32x4x2_t, float32_t, v4sf, sf, f32)\n __ST2_LANE_FUNC (float64x2x2_t, float64_t, v2df, df, f64)\n __ST2_LANE_FUNC (poly8x16x2_t, poly8_t, v16qi, qi, p8)\n __ST2_LANE_FUNC (poly16x8x2_t, poly16_t, v8hi, hi, p16)\n+__ST2_LANE_FUNC (poly64x2x2_t, poly64_t, v2di, di, p64)\n __ST2_LANE_FUNC (int8x16x2_t, int8_t, v16qi, qi, s8)\n __ST2_LANE_FUNC (int16x8x2_t, int16_t, v8hi, hi, s16)\n __ST2_LANE_FUNC (int32x4x2_t, int32_t, v4si, si, s32)\n@@ -10439,6 +10887,8 @@ __ST3_LANE_FUNC (poly8x8x3_t, poly8x16x3_t, poly8_t, v8qi, v16qi, qi, p8,\n \t\t int8x16_t)\n __ST3_LANE_FUNC (poly16x4x3_t, poly16x8x3_t, poly16_t, v4hi, v8hi, hi, p16,\n \t\t int16x8_t)\n+__ST3_LANE_FUNC (poly64x1x3_t, poly64x2x3_t, poly64_t, di, v2di_ssps, di, p64,\n+\t\t poly64x2_t)\n __ST3_LANE_FUNC (int8x8x3_t, int8x16x3_t, int8_t, v8qi, v16qi, qi, s8,\n \t\t int8x16_t)\n __ST3_LANE_FUNC (int16x4x3_t, int16x8x3_t, int16_t, v4hi, v8hi, hi, s16,\n@@ -10474,6 +10924,7 @@ __ST3_LANE_FUNC (float32x4x3_t, float32_t, v4sf, sf, f32)\n __ST3_LANE_FUNC (float64x2x3_t, float64_t, v2df, df, f64)\n __ST3_LANE_FUNC (poly8x16x3_t, poly8_t, v16qi, qi, p8)\n __ST3_LANE_FUNC (poly16x8x3_t, poly16_t, v8hi, hi, p16)\n+__ST3_LANE_FUNC (poly64x2x3_t, poly64_t, v2di, di, p64)\n __ST3_LANE_FUNC (int8x16x3_t, int8_t, v16qi, qi, s8)\n __ST3_LANE_FUNC (int16x8x3_t, int16_t, v8hi, hi, s16)\n __ST3_LANE_FUNC (int32x4x3_t, int32_t, v4si, si, s32)\n@@ -10526,6 +10977,8 @@ __ST4_LANE_FUNC (poly8x8x4_t, poly8x16x4_t, poly8_t, v8qi, v16qi, qi, p8,\n \t\t int8x16_t)\n __ST4_LANE_FUNC (poly16x4x4_t, poly16x8x4_t, poly16_t, v4hi, v8hi, hi, p16,\n \t\t int16x8_t)\n+__ST4_LANE_FUNC (poly64x1x4_t, poly64x2x4_t, poly64_t, di, v2di_ssps, di, p64,\n+\t\t poly64x2_t)\n __ST4_LANE_FUNC (int8x8x4_t, int8x16x4_t, int8_t, v8qi, v16qi, qi, s8,\n \t\t int8x16_t)\n __ST4_LANE_FUNC (int16x4x4_t, int16x8x4_t, int16_t, v4hi, v8hi, hi, s16,\n@@ -10561,6 +11014,7 @@ __ST4_LANE_FUNC (float32x4x4_t, float32_t, v4sf, sf, f32)\n __ST4_LANE_FUNC (float64x2x4_t, float64_t, v2df, df, f64)\n __ST4_LANE_FUNC (poly8x16x4_t, poly8_t, v16qi, qi, p8)\n __ST4_LANE_FUNC (poly16x8x4_t, poly16_t, v8hi, hi, p16)\n+__ST4_LANE_FUNC (poly64x2x4_t, poly64_t, v2di, di, p64)\n __ST4_LANE_FUNC (int8x16x4_t, int8_t, v16qi, qi, s8)\n __ST4_LANE_FUNC (int16x8x4_t, int16_t, v8hi, hi, s16)\n __ST4_LANE_FUNC (int32x4x4_t, int32_t, v4si, si, s32)\n@@ -13644,6 +14098,15 @@ vcopy_lane_p16 (poly16x4_t __a, const int __lane1,\n \t\t\t\t  __a, __lane1);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcopy_lane_p64 (poly64x1_t __a, const int __lane1,\n+\t\tpoly64x1_t __b, const int __lane2)\n+{\n+  return __aarch64_vset_lane_any (__aarch64_vget_lane_any (__b, __lane2),\n+\t\t\t\t  __a, __lane1);\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vcopy_lane_s8 (int8x8_t __a, const int __lane1,\n@@ -13754,6 +14217,15 @@ vcopy_laneq_p16 (poly16x4_t __a, const int __lane1,\n \t\t\t\t  __a, __lane1);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcopy_laneq_p64 (poly64x1_t __a, const int __lane1,\n+\t\t poly64x2_t __b, const int __lane2)\n+{\n+  return __aarch64_vset_lane_any (__aarch64_vget_lane_any (__b, __lane2),\n+\t\t\t\t  __a, __lane1);\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vcopy_laneq_s8 (int8x8_t __a, const int __lane1,\n@@ -13864,6 +14336,15 @@ vcopyq_lane_p16 (poly16x8_t __a, const int __lane1,\n \t\t\t\t   __a, __lane1);\n }\n \n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcopyq_lane_p64 (poly64x2_t __a, const int __lane1,\n+\t\t poly64x1_t __b, const int __lane2)\n+{\n+  return __aarch64_vset_lane_any (__aarch64_vget_lane_any (__b, __lane2),\n+\t\t\t\t   __a, __lane1);\n+}\n+\n __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vcopyq_lane_s8 (int8x16_t __a, const int __lane1,\n@@ -13974,6 +14455,15 @@ vcopyq_laneq_p16 (poly16x8_t __a, const int __lane1,\n \t\t\t\t   __a, __lane1);\n }\n \n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcopyq_laneq_p64 (poly64x2_t __a, const int __lane1,\n+\t\t  poly64x2_t __b, const int __lane2)\n+{\n+  return __aarch64_vset_lane_any (__aarch64_vget_lane_any (__b, __lane2),\n+\t\t\t\t   __a, __lane1);\n+}\n+\n __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vcopyq_laneq_s8 (int8x16_t __a, const int __lane1,\n@@ -14836,6 +15326,13 @@ vdup_n_p16 (poly16_t __a)\n   return (poly16x4_t) {__a, __a, __a, __a};\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdup_n_p64 (poly64_t __a)\n+{\n+  return (poly64x1_t) {__a};\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vdup_n_s8 (int8_t __a)\n@@ -14930,6 +15427,13 @@ vdupq_n_p16 (uint32_t __a)\n   return (poly16x8_t) {__a, __a, __a, __a, __a, __a, __a, __a};\n }\n \n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdupq_n_p64 (uint64_t __a)\n+{\n+  return (poly64x2_t) {__a, __a};\n+}\n+\n __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vdupq_n_s8 (int32_t __a)\n@@ -15025,6 +15529,13 @@ vdup_lane_p16 (poly16x4_t __a, const int __b)\n   return __aarch64_vdup_lane_p16 (__a, __b);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdup_lane_p64 (poly64x1_t __a, const int __b)\n+{\n+  return __aarch64_vdup_lane_p64 (__a, __b);\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vdup_lane_s8 (int8x8_t __a, const int __b)\n@@ -15118,6 +15629,13 @@ vdup_laneq_p16 (poly16x8_t __a, const int __b)\n   return __aarch64_vdup_laneq_p16 (__a, __b);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdup_laneq_p64 (poly64x2_t __a, const int __b)\n+{\n+  return __aarch64_vdup_laneq_p64 (__a, __b);\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vdup_laneq_s8 (int8x16_t __a, const int __b)\n@@ -15211,6 +15729,13 @@ vdupq_lane_p16 (poly16x4_t __a, const int __b)\n   return __aarch64_vdupq_lane_p16 (__a, __b);\n }\n \n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdupq_lane_p64 (poly64x1_t __a, const int __b)\n+{\n+  return __aarch64_vdupq_lane_p64 (__a, __b);\n+}\n+\n __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vdupq_lane_s8 (int8x8_t __a, const int __b)\n@@ -15304,6 +15829,13 @@ vdupq_laneq_p16 (poly16x8_t __a, const int __b)\n   return __aarch64_vdupq_laneq_p16 (__a, __b);\n }\n \n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vdupq_laneq_p64 (poly64x2_t __a, const int __b)\n+{\n+  return __aarch64_vdupq_laneq_p64 (__a, __b);\n+}\n+\n __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vdupq_laneq_s8 (int8x16_t __a, const int __b)\n@@ -16283,6 +16815,13 @@ vld1_p16 (const poly16_t *a)\n     __builtin_aarch64_ld1v4hi ((const __builtin_aarch64_simd_hi *) a);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_p64 (const poly64_t *a)\n+{\n+  return (poly64x1_t) {*a};\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld1_s8 (const int8_t *a)\n@@ -16381,6 +16920,14 @@ vld1q_p16 (const poly16_t *a)\n     __builtin_aarch64_ld1v8hi ((const __builtin_aarch64_simd_hi *) a);\n }\n \n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_p64 (const poly64_t *a)\n+{\n+  return (poly64x2_t)\n+    __builtin_aarch64_ld1v2di ((const __builtin_aarch64_simd_di *) a);\n+}\n+\n __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld1q_s8 (const int8_t *a)\n@@ -16478,6 +17025,13 @@ vld1_dup_p16 (const poly16_t* __a)\n   return vdup_n_p16 (*__a);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_dup_p64 (const poly64_t* __a)\n+{\n+  return vdup_n_p64 (*__a);\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld1_dup_s8 (const int8_t* __a)\n@@ -16571,7 +17125,14 @@ vld1q_dup_p16 (const poly16_t* __a)\n   return vdupq_n_p16 (*__a);\n }\n \n-__extension__ extern __inline int8x16_t\n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_dup_p64 (const poly64_t* __a)\n+{\n+  return vdupq_n_p64 (*__a);\n+}\n+\n+ __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld1q_dup_s8 (const int8_t* __a)\n {\n@@ -16664,6 +17225,13 @@ vld1_lane_p16 (const poly16_t *__src, poly16x4_t __vec, const int __lane)\n   return __aarch64_vset_lane_any (*__src, __vec, __lane);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_lane_p64 (const poly64_t *__src, poly64x1_t __vec, const int __lane)\n+{\n+  return __aarch64_vset_lane_any (*__src, __vec, __lane);\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld1_lane_s8 (const int8_t *__src, int8x8_t __vec, const int __lane)\n@@ -16757,6 +17325,13 @@ vld1q_lane_p16 (const poly16_t *__src, poly16x8_t __vec, const int __lane)\n   return __aarch64_vset_lane_any (*__src, __vec, __lane);\n }\n \n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_lane_p64 (const poly64_t *__src, poly64x2_t __vec, const int __lane)\n+{\n+  return __aarch64_vset_lane_any (*__src, __vec, __lane);\n+}\n+\n __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld1q_lane_s8 (const int8_t *__src, int8x16_t __vec, const int __lane)\n@@ -16875,6 +17450,18 @@ vld2_p8 (const poly8_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x1x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld2_p64 (const poly64_t * __a)\n+{\n+  poly64x1x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x1_t) __builtin_aarch64_get_dregoidi_pss (__o, 0);\n+  ret.val[1] = (poly64x1_t) __builtin_aarch64_get_dregoidi_pss (__o, 1);\n+  return ret;\n+}\n+\n __extension__ extern __inline int16x4x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld2_s16 (const int16_t * __a)\n@@ -17019,6 +17606,18 @@ vld2q_p16 (const poly16_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x2x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld2q_p64 (const poly64_t * __a)\n+{\n+  poly64x2x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2v2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x2_t) __builtin_aarch64_get_qregoiv2di_pss (__o, 0);\n+  ret.val[1] = (poly64x2_t) __builtin_aarch64_get_qregoiv2di_pss (__o, 1);\n+  return ret;\n+}\n+\n __extension__ extern __inline int32x4x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld2q_s32 (const int32_t * __a)\n@@ -17296,6 +17895,19 @@ vld3_f32 (const float32_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x1x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld3_p64 (const poly64_t * __a)\n+{\n+  poly64x1x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x1_t) __builtin_aarch64_get_dregcidi_pss (__o, 0);\n+  ret.val[1] = (poly64x1_t) __builtin_aarch64_get_dregcidi_pss (__o, 1);\n+  ret.val[2] = (poly64x1_t) __builtin_aarch64_get_dregcidi_pss (__o, 2);\n+  return ret;\n+}\n+\n __extension__ extern __inline int8x16x3_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld3q_s8 (const int8_t * __a)\n@@ -17465,6 +18077,19 @@ vld3q_f64 (const float64_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x2x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld3q_p64 (const poly64_t * __a)\n+{\n+  poly64x2x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3v2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x2_t) __builtin_aarch64_get_qregciv2di_pss (__o, 0);\n+  ret.val[1] = (poly64x2_t) __builtin_aarch64_get_qregciv2di_pss (__o, 1);\n+  ret.val[2] = (poly64x2_t) __builtin_aarch64_get_qregciv2di_pss (__o, 2);\n+  return ret;\n+}\n+\n __extension__ extern __inline int64x1x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld4_s64 (const int64_t * __a)\n@@ -17647,6 +18272,20 @@ vld4_f32 (const float32_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x1x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld4_p64 (const poly64_t * __a)\n+{\n+  poly64x1x4_t  ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x1_t) __builtin_aarch64_get_dregxidi_pss (__o, 0);\n+  ret.val[1] = (poly64x1_t) __builtin_aarch64_get_dregxidi_pss (__o, 1);\n+  ret.val[2] = (poly64x1_t) __builtin_aarch64_get_dregxidi_pss (__o, 2);\n+  ret.val[3] = (poly64x1_t) __builtin_aarch64_get_dregxidi_pss (__o, 3);\n+  return ret;\n+}\n+\n __extension__ extern __inline int8x16x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld4q_s8 (const int8_t * __a)\n@@ -17829,6 +18468,20 @@ vld4q_f64 (const float64_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x2x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld4q_p64 (const poly64_t * __a)\n+{\n+  poly64x2x4_t  ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4v2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x2_t) __builtin_aarch64_get_qregxiv2di_pss (__o, 0);\n+  ret.val[1] = (poly64x2_t) __builtin_aarch64_get_qregxiv2di_pss (__o, 1);\n+  ret.val[2] = (poly64x2_t) __builtin_aarch64_get_qregxiv2di_pss (__o, 2);\n+  ret.val[3] = (poly64x2_t) __builtin_aarch64_get_qregxiv2di_pss (__o, 3);\n+  return ret;\n+}\n+\n /* vldn_dup */\n \n __extension__ extern __inline int8x8x2_t\n@@ -17963,6 +18616,19 @@ vld2_dup_p16 (const poly16_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x1x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld2_dup_p64 (const poly64_t * __a)\n+{\n+  poly64x1x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x1_t) __builtin_aarch64_get_dregoidi_pss (__o, 0);\n+  ret.val[1] = (poly64x1_t) __builtin_aarch64_get_dregoidi_pss (__o, 1);\n+  return ret;\n+}\n+\n+\n __extension__ extern __inline int64x1x2_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld2_dup_s64 (const int64_t * __a)\n@@ -18143,6 +18809,18 @@ vld2q_dup_f64 (const float64_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x2x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld2q_dup_p64 (const poly64_t * __a)\n+{\n+  poly64x2x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x2_t) __builtin_aarch64_get_qregoiv2di_pss (__o, 0);\n+  ret.val[1] = (poly64x2_t) __builtin_aarch64_get_qregoiv2di_pss (__o, 1);\n+  return ret;\n+}\n+\n __extension__ extern __inline int64x1x3_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld3_dup_s64 (const int64_t * __a)\n@@ -18312,6 +18990,19 @@ vld3_dup_f32 (const float32_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x1x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld3_dup_p64 (const poly64_t * __a)\n+{\n+  poly64x1x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x1_t) __builtin_aarch64_get_dregcidi_pss (__o, 0);\n+  ret.val[1] = (poly64x1_t) __builtin_aarch64_get_dregcidi_pss (__o, 1);\n+  ret.val[2] = (poly64x1_t) __builtin_aarch64_get_dregcidi_pss (__o, 2);\n+  return ret;\n+}\n+\n __extension__ extern __inline int8x16x3_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld3q_dup_s8 (const int8_t * __a)\n@@ -18481,6 +19172,19 @@ vld3q_dup_f64 (const float64_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x2x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld3q_dup_p64 (const poly64_t * __a)\n+{\n+  poly64x2x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x2_t) __builtin_aarch64_get_qregciv2di_pss (__o, 0);\n+  ret.val[1] = (poly64x2_t) __builtin_aarch64_get_qregciv2di_pss (__o, 1);\n+  ret.val[2] = (poly64x2_t) __builtin_aarch64_get_qregciv2di_pss (__o, 2);\n+  return ret;\n+}\n+\n __extension__ extern __inline int64x1x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld4_dup_s64 (const int64_t * __a)\n@@ -18663,6 +19367,20 @@ vld4_dup_f32 (const float32_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x1x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld4_dup_p64 (const poly64_t * __a)\n+{\n+  poly64x1x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x1_t) __builtin_aarch64_get_dregxidi_pss (__o, 0);\n+  ret.val[1] = (poly64x1_t) __builtin_aarch64_get_dregxidi_pss (__o, 1);\n+  ret.val[2] = (poly64x1_t) __builtin_aarch64_get_dregxidi_pss (__o, 2);\n+  ret.val[3] = (poly64x1_t) __builtin_aarch64_get_dregxidi_pss (__o, 3);\n+  return ret;\n+}\n+\n __extension__ extern __inline int8x16x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vld4q_dup_s8 (const int8_t * __a)\n@@ -18845,6 +19563,20 @@ vld4q_dup_f64 (const float64_t * __a)\n   return ret;\n }\n \n+__extension__ extern __inline poly64x2x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld4q_dup_p64 (const poly64_t * __a)\n+{\n+  poly64x2x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (poly64x2_t) __builtin_aarch64_get_qregxiv2di_pss (__o, 0);\n+  ret.val[1] = (poly64x2_t) __builtin_aarch64_get_qregxiv2di_pss (__o, 1);\n+  ret.val[2] = (poly64x2_t) __builtin_aarch64_get_qregxiv2di_pss (__o, 2);\n+  ret.val[3] = (poly64x2_t) __builtin_aarch64_get_qregxiv2di_pss (__o, 3);\n+  return ret;\n+}\n+\n /* vld2_lane */\n \n #define __LD2_LANE_FUNC(intype, vectype, largetype, ptrtype, mode,\t   \\\n@@ -18882,6 +19614,8 @@ __LD2_LANE_FUNC (poly8x8x2_t, poly8x8_t, poly8x16x2_t, poly8_t, v8qi, v16qi, qi,\n \t\t int8x16_t)\n __LD2_LANE_FUNC (poly16x4x2_t, poly16x4_t, poly16x8x2_t, poly16_t, v4hi, v8hi, hi,\n \t\t p16, int16x8_t)\n+__LD2_LANE_FUNC (poly64x1x2_t, poly64x1_t, poly64x2x2_t, poly64_t, di,\n+\t\t v2di_ssps, di, p64, poly64x2_t)\n __LD2_LANE_FUNC (int8x8x2_t, int8x8_t, int8x16x2_t, int8_t, v8qi, v16qi, qi, s8,\n \t\t int8x16_t)\n __LD2_LANE_FUNC (int16x4x2_t, int16x4_t, int16x8x2_t, int16_t, v4hi, v8hi, hi, s16,\n@@ -18924,6 +19658,7 @@ __LD2_LANE_FUNC (float32x4x2_t, float32x4_t, float32_t, v4sf, sf, f32)\n __LD2_LANE_FUNC (float64x2x2_t, float64x2_t, float64_t, v2df, df, f64)\n __LD2_LANE_FUNC (poly8x16x2_t, poly8x16_t, poly8_t, v16qi, qi, p8)\n __LD2_LANE_FUNC (poly16x8x2_t, poly16x8_t, poly16_t, v8hi, hi, p16)\n+__LD2_LANE_FUNC (poly64x2x2_t, poly64x2_t, poly64_t, v2di, di, p64)\n __LD2_LANE_FUNC (int8x16x2_t, int8x16_t, int8_t, v16qi, qi, s8)\n __LD2_LANE_FUNC (int16x8x2_t, int16x8_t, int16_t, v8hi, hi, s16)\n __LD2_LANE_FUNC (int32x4x2_t, int32x4_t, int32_t, v4si, si, s32)\n@@ -18978,6 +19713,8 @@ __LD3_LANE_FUNC (poly8x8x3_t, poly8x8_t, poly8x16x3_t, poly8_t, v8qi, v16qi, qi,\n \t\t int8x16_t)\n __LD3_LANE_FUNC (poly16x4x3_t, poly16x4_t, poly16x8x3_t, poly16_t, v4hi, v8hi, hi,\n \t\t p16, int16x8_t)\n+__LD3_LANE_FUNC (poly64x1x3_t, poly64x1_t, poly64x2x3_t, poly64_t, di,\n+\t\t v2di_ssps, di, p64, poly64x2_t)\n __LD3_LANE_FUNC (int8x8x3_t, int8x8_t, int8x16x3_t, int8_t, v8qi, v16qi, qi, s8,\n \t\t int8x16_t)\n __LD3_LANE_FUNC (int16x4x3_t, int16x4_t, int16x8x3_t, int16_t, v4hi, v8hi, hi, s16,\n@@ -19022,6 +19759,7 @@ __LD3_LANE_FUNC (float32x4x3_t, float32x4_t, float32_t, v4sf, sf, f32)\n __LD3_LANE_FUNC (float64x2x3_t, float64x2_t, float64_t, v2df, df, f64)\n __LD3_LANE_FUNC (poly8x16x3_t, poly8x16_t, poly8_t, v16qi, qi, p8)\n __LD3_LANE_FUNC (poly16x8x3_t, poly16x8_t, poly16_t, v8hi, hi, p16)\n+__LD3_LANE_FUNC (poly64x2x3_t, poly64x2_t, poly64_t, v2di, di, p64)\n __LD3_LANE_FUNC (int8x16x3_t, int8x16_t, int8_t, v16qi, qi, s8)\n __LD3_LANE_FUNC (int16x8x3_t, int16x8_t, int16_t, v8hi, hi, s16)\n __LD3_LANE_FUNC (int32x4x3_t, int32x4_t, int32_t, v4si, si, s32)\n@@ -19084,6 +19822,8 @@ __LD4_LANE_FUNC (poly8x8x4_t, poly8x8_t, poly8x16x4_t, poly8_t, v8qi, v16qi, qi,\n \t\t int8x16_t)\n __LD4_LANE_FUNC (poly16x4x4_t, poly16x4_t, poly16x8x4_t, poly16_t, v4hi, v8hi, hi,\n \t\t p16, int16x8_t)\n+__LD4_LANE_FUNC (poly64x1x4_t, poly64x1_t, poly64x2x4_t, poly64_t, di,\n+\t\t v2di_ssps, di, p64, poly64x2_t)\n __LD4_LANE_FUNC (int8x8x4_t, int8x8_t, int8x16x4_t, int8_t, v8qi, v16qi, qi, s8,\n \t\t int8x16_t)\n __LD4_LANE_FUNC (int16x4x4_t, int16x4_t, int16x8x4_t, int16_t, v4hi, v8hi, hi, s16,\n@@ -19130,6 +19870,7 @@ __LD4_LANE_FUNC (float32x4x4_t, float32x4_t, float32_t, v4sf, sf, f32)\n __LD4_LANE_FUNC (float64x2x4_t, float64x2_t, float64_t, v2df, df, f64)\n __LD4_LANE_FUNC (poly8x16x4_t, poly8x16_t, poly8_t, v16qi, qi, p8)\n __LD4_LANE_FUNC (poly16x8x4_t, poly16x8_t, poly16_t, v8hi, hi, p16)\n+__LD4_LANE_FUNC (poly64x2x4_t, poly64x2_t, poly64_t, v2di, di, p64)\n __LD4_LANE_FUNC (int8x16x4_t, int8x16_t, int8_t, v16qi, qi, s8)\n __LD4_LANE_FUNC (int16x8x4_t, int16x8_t, int16_t, v8hi, hi, s16)\n __LD4_LANE_FUNC (int32x4x4_t, int32x4_t, int32_t, v4si, si, s32)\n@@ -20596,6 +21337,13 @@ vmov_n_p16 (poly16_t __a)\n   return vdup_n_p16 (__a);\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmov_n_p64 (poly64_t __a)\n+{\n+  return vdup_n_p64 (__a);\n+}\n+\n __extension__ extern __inline int8x8_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vmov_n_s8 (int8_t __a)\n@@ -20687,6 +21435,13 @@ vmovq_n_p16 (poly16_t __a)\n   return vdupq_n_p16 (__a);\n }\n \n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmovq_n_p64 (poly64_t __a)\n+{\n+  return vdupq_n_p64 (__a);\n+}\n+\n __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vmovq_n_s8 (int8_t __a)\n@@ -25275,6 +26030,13 @@ vsli_n_u64 (uint64x1_t __a, uint64x1_t __b, const int __c)\n   return (uint64x1_t) {__builtin_aarch64_usli_ndi_uuus (__a[0], __b[0], __c)};\n }\n \n+__extension__ extern __inline poly64x1_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vsli_n_p64 (poly64x1_t __a, poly64x1_t __b, const int __c)\n+{\n+  return (poly64x1_t) {__builtin_aarch64_ssli_ndi_ppps (__a[0], __b[0], __c)};\n+}\n+\n __extension__ extern __inline int8x16_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vsliq_n_s8 (int8x16_t __a, int8x16_t __b, const int __c)\n@@ -25331,6 +26093,13 @@ vsliq_n_u64 (uint64x2_t __a, uint64x2_t __b, const int __c)\n   return __builtin_aarch64_usli_nv2di_uuus (__a, __b, __c);\n }\n \n+__extension__ extern __inline poly64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vsliq_n_p64 (poly64x2_t __a, poly64x2_t __b, const int __c)\n+{\n+  return __builtin_aarch64_ssli_nv2di_ppps (__a, __b, __c);\n+}\n+\n __extension__ extern __inline int64_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vslid_n_s64 (int64_t __a, int64_t __b, const int __c)\n@@ -25755,6 +26524,13 @@ vst1_p16 (poly16_t *a, poly16x4_t b)\n \t\t\t     (int16x4_t) b);\n }\n \n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_p64 (poly64_t *a, poly64x1_t b)\n+{\n+  *a = b[0];\n+}\n+\n __extension__ extern __inline void\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vst1_s8 (int8_t *a, int8x8_t b)\n@@ -25853,6 +26629,14 @@ vst1q_p16 (poly16_t *a, poly16x8_t b)\n \t\t\t     (int16x8_t) b);\n }\n \n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_p64 (poly64_t *a, poly64x2_t b)\n+{\n+  __builtin_aarch64_st1v2di_sp ((__builtin_aarch64_simd_di *) a,\n+\t\t\t\t(poly64x2_t) b);\n+}\n+\n __extension__ extern __inline void\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vst1q_s8 (int8_t *a, int8x16_t b)\n@@ -25950,6 +26734,13 @@ vst1_lane_p16 (poly16_t *__a, poly16x4_t __b, const int __lane)\n   *__a = __aarch64_vget_lane_any (__b, __lane);\n }\n \n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_lane_p64 (poly64_t *__a, poly64x1_t __b, const int __lane)\n+{\n+  *__a = __aarch64_vget_lane_any (__b, __lane);\n+}\n+\n __extension__ extern __inline void\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vst1_lane_s8 (int8_t *__a, int8x8_t __b, const int __lane)\n@@ -26043,6 +26834,13 @@ vst1q_lane_p16 (poly16_t *__a, poly16x8_t __b, const int __lane)\n   *__a = __aarch64_vget_lane_any (__b, __lane);\n }\n \n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_lane_p64 (poly64_t *__a, poly64x2_t __b, const int __lane)\n+{\n+  *__a = __aarch64_vget_lane_any (__b, __lane);\n+}\n+\n __extension__ extern __inline void\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vst1q_lane_s8 (int8_t *__a, int8x16_t __b, const int __lane)\n@@ -26270,6 +27068,21 @@ vst2_f32 (float32_t * __a, float32x2x2_t val)\n   __builtin_aarch64_st2v2sf ((__builtin_aarch64_simd_sf *) __a, __o);\n }\n \n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst2_p64 (poly64_t * __a, poly64x1x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  poly64x2x2_t temp;\n+  temp.val[0] = vcombine_p64 (val.val[0], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_p64 (val.val[1], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[1], 1);\n+  __builtin_aarch64_st2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n __extension__ extern __inline void\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vst2q_s8 (int8_t * __a, int8x16x2_t val)\n@@ -26400,6 +27213,18 @@ vst2q_f64 (float64_t * __a, float64x2x2_t val)\n   __builtin_aarch64_st2v2df ((__builtin_aarch64_simd_df *) __a, __o);\n }\n \n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst2q_p64 (poly64_t * __a, poly64x2x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[1], 1);\n+  __builtin_aarch64_st2v2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n __extension__ extern __inline void\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vst3_s64 (int64_t * __a, int64x1x3_t val)\n@@ -26595,6 +27420,24 @@ vst3_f32 (float32_t * __a, float32x2x3_t val)\n   __builtin_aarch64_st3v2sf ((__builtin_aarch64_simd_sf *) __a, __o);\n }\n \n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst3_p64 (poly64_t * __a, poly64x1x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  poly64x2x3_t temp;\n+  temp.val[0] = vcombine_p64 (val.val[0], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_p64 (val.val[1], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_p64 (val.val[2], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[2], 2);\n+  __builtin_aarch64_st3di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n __extension__ extern __inline void\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vst3q_s8 (int8_t * __a, int8x16x3_t val)\n@@ -26738,6 +27581,20 @@ vst3q_f64 (float64_t * __a, float64x2x3_t val)\n   __builtin_aarch64_st3v2df ((__builtin_aarch64_simd_df *) __a, __o);\n }\n \n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst3q_p64 (poly64_t * __a, poly64x2x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[2], 2);\n+  __builtin_aarch64_st3v2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n __extension__ extern __inline void\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vst4_s64 (int64_t * __a, int64x1x4_t val)\n@@ -26959,6 +27816,27 @@ vst4_f32 (float32_t * __a, float32x2x4_t val)\n   __builtin_aarch64_st4v2sf ((__builtin_aarch64_simd_sf *) __a, __o);\n }\n \n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst4_p64 (poly64_t * __a, poly64x1x4_t val)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  poly64x2x4_t temp;\n+  temp.val[0] = vcombine_p64 (val.val[0], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_p64 (val.val[1], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_p64 (val.val[2], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  temp.val[3] = vcombine_p64 (val.val[3], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregxiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[3], 3);\n+  __builtin_aarch64_st4di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n __extension__ extern __inline void\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n vst4q_s8 (int8_t * __a, int8x16x4_t val)\n@@ -27115,6 +27993,22 @@ vst4q_f64 (float64_t * __a, float64x2x4_t val)\n   __builtin_aarch64_st4v2df ((__builtin_aarch64_simd_df *) __a, __o);\n }\n \n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst4q_p64 (poly64_t * __a, poly64x2x4_t val)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_set_qregxiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[3], 3);\n+  __builtin_aarch64_st4v2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n /* vsub */\n \n __extension__ extern __inline int64_t"}]}