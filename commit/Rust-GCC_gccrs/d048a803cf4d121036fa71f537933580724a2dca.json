{"sha": "d048a803cf4d121036fa71f537933580724a2dca", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZDA0OGE4MDNjZjRkMTIxMDM2ZmE3MWY1Mzc5MzM1ODA3MjRhMmRjYQ==", "commit": {"author": {"name": "Tom Tromey", "email": "tromey@gcc.gnu.org", "date": "1999-04-07T08:01:37Z"}, "committer": {"name": "Tom Tromey", "email": "tromey@gcc.gnu.org", "date": "1999-04-07T08:01:37Z"}, "message": "Initial revision\n\nFrom-SVN: r26252", "tree": {"sha": "83baed2e3f4b039520e169bda5b3ae03b75f3c04", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/83baed2e3f4b039520e169bda5b3ae03b75f3c04"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/d048a803cf4d121036fa71f537933580724a2dca", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d048a803cf4d121036fa71f537933580724a2dca", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d048a803cf4d121036fa71f537933580724a2dca", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d048a803cf4d121036fa71f537933580724a2dca/comments", "author": null, "committer": null, "parents": [{"sha": "fdcddcb1dfa6263074cf7a949f2f670719323fd9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fdcddcb1dfa6263074cf7a949f2f670719323fd9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fdcddcb1dfa6263074cf7a949f2f670719323fd9"}], "stats": {"total": 1561, "additions": 1561, "deletions": 0}, "files": [{"sha": "96ba1da104476113a06ad3574dc6ae12541d1807", "filename": "boehm-gc/gc_priv.h", "status": "added", "additions": 1561, "deletions": 0, "changes": 1561, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d048a803cf4d121036fa71f537933580724a2dca/boehm-gc%2Fgc_priv.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d048a803cf4d121036fa71f537933580724a2dca/boehm-gc%2Fgc_priv.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/boehm-gc%2Fgc_priv.h?ref=d048a803cf4d121036fa71f537933580724a2dca", "patch": "@@ -0,0 +1,1561 @@\n+/* \n+ * Copyright 1988, 1989 Hans-J. Boehm, Alan J. Demers\n+ * Copyright (c) 1991-1994 by Xerox Corporation.  All rights reserved.\n+ *\n+ * THIS MATERIAL IS PROVIDED AS IS, WITH ABSOLUTELY NO WARRANTY EXPRESSED\n+ * OR IMPLIED.  ANY USE IS AT YOUR OWN RISK.\n+ *\n+ * Permission is hereby granted to use or copy this program\n+ * for any purpose,  provided the above notices are retained on all copies.\n+ * Permission to modify the code and to distribute modified code is granted,\n+ * provided the above notices are retained, and a notice that the code was\n+ * modified is included with the above copyright notice.\n+ */\n+/* Boehm, February 16, 1996 2:30 pm PST */\n+ \n+\n+# ifndef GC_PRIVATE_H\n+# define GC_PRIVATE_H\n+\n+#if defined(mips) && defined(SYSTYPE_BSD) && defined(sony_news)\n+    /* sony RISC NEWS, NEWSOS 4 */\n+#   define BSD_TIME\n+/*    typedef long ptrdiff_t;   -- necessary on some really old systems\t*/\n+#endif\n+\n+#if defined(mips) && defined(SYSTYPE_BSD43)\n+    /* MIPS RISCOS 4 */\n+#   define BSD_TIME\n+#endif\n+\n+#ifdef BSD_TIME\n+#   include <sys/types.h>\n+#   include <sys/time.h>\n+#   include <sys/resource.h>\n+#endif /* BSD_TIME */\n+\n+# ifndef GC_H\n+#   include \"gc.h\"\n+# endif\n+\n+typedef GC_word word;\n+typedef GC_signed_word signed_word;\n+\n+# ifndef CONFIG_H\n+#   include \"config.h\"\n+# endif\n+\n+# ifndef HEADERS_H\n+#   include \"gc_hdrs.h\"\n+# endif\n+\n+typedef int GC_bool;\n+# define TRUE 1\n+# define FALSE 0\n+\n+typedef char * ptr_t;\t/* A generic pointer to which we can add\t*/\n+\t\t\t/* byte displacements.\t\t\t\t*/\n+\t\t\t/* Preferably identical to caddr_t, if it \t*/\n+\t\t\t/* exists.\t\t\t\t\t*/\n+\t\t\t\n+#if defined(__STDC__)\n+#   include <stdlib.h>\n+#   if !(defined( sony_news ) )\n+#       include <stddef.h>\n+#   endif\n+#   define VOLATILE volatile\n+#   define CONST const\n+#else\n+#   ifdef MSWIN32\n+#   \tinclude <stdlib.h>\n+#   endif\n+#   define VOLATILE\n+#   define CONST\n+#endif\n+\n+#ifdef AMIGA\n+#   define GC_FAR __far\n+#else\n+#   define GC_FAR\n+#endif\n+\n+/*********************************/\n+/*                               */\n+/* Definitions for conservative  */\n+/* collector                     */\n+/*                               */\n+/*********************************/\n+\n+/*********************************/\n+/*                               */\n+/* Easily changeable parameters  */\n+/*                               */\n+/*********************************/\n+\n+#define STUBBORN_ALLOC\t/* Define stubborn allocation primitives\t*/\n+#if defined(SRC_M3) || defined(SMALL_CONFIG)\n+# undef STUBBORN_ALLOC\n+#endif\n+\n+\n+/* #define ALL_INTERIOR_POINTERS */\n+\t\t    /* Forces all pointers into the interior of an \t*/\n+\t\t    /* object to be considered valid.  Also causes the\t*/\n+\t\t    /* sizes of all objects to be inflated by at least \t*/\n+\t\t    /* one byte.  This should suffice to guarantee\t*/\n+\t\t    /* that in the presence of a compiler that does\t*/\n+\t\t    /* not perform garbage-collector-unsafe\t\t*/\n+\t\t    /* optimizations, all portable, strictly ANSI\t*/\n+\t\t    /* conforming C programs should be safely usable\t*/\n+\t\t    /* with malloc replaced by GC_malloc and free\t*/\n+\t\t    /* calls removed.  There are several disadvantages: */\n+\t\t    /* 1. There are probably no interesting, portable,\t*/\n+\t\t    /*    strictly ANSI\tconforming C programs.\t\t*/\n+\t\t    /* 2. This option makes it hard for the collector\t*/\n+\t\t    /*    to allocate space that is not ``pointed to''  */\n+\t\t    /*    by integers, etc.  Under SunOS 4.X with a \t*/\n+\t\t    /*    statically linked libc, we empiricaly\t\t*/\n+\t\t    /*    observed that it would be difficult to \t*/\n+\t\t    /*\t  allocate individual objects larger than 100K.\t*/\n+\t\t    /* \t  Even if only smaller objects are allocated,\t*/\n+\t\t    /*    more swap space is likely to be needed.       */\n+\t\t    /*    Fortunately, much of this will never be\t*/\n+\t\t    /*    touched.\t\t\t\t\t*/\n+\t\t    /* If you can easily avoid using this option, do.\t*/\n+\t\t    /* If not, try to keep individual objects small.\t*/\n+\t\t    \n+#define PRINTSTATS  /* Print garbage collection statistics          \t*/\n+\t\t    /* For less verbose output, undefine in reclaim.c \t*/\n+\n+#define PRINTTIMES  /* Print the amount of time consumed by each garbage   */\n+\t\t    /* collection.                                         */\n+\n+#define PRINTBLOCKS /* Print object sizes associated with heap blocks,     */\n+\t\t    /* whether the objects are atomic or composite, and    */\n+\t\t    /* whether or not the block was found to be empty      */\n+\t\t    /* during the reclaim phase.  Typically generates       */\n+\t\t    /* about one screenful per garbage collection.         */\n+#undef PRINTBLOCKS\n+\n+#ifdef SILENT\n+#  ifdef PRINTSTATS\n+#    undef PRINTSTATS\n+#  endif\n+#  ifdef PRINTTIMES\n+#    undef PRINTTIMES\n+#  endif\n+#  ifdef PRINTNBLOCKS\n+#    undef PRINTNBLOCKS\n+#  endif\n+#endif\n+\n+#if defined(PRINTSTATS) && !defined(GATHERSTATS)\n+#   define GATHERSTATS\n+#endif\n+\n+#ifdef FINALIZE_ON_DEMAND\n+#   define GC_INVOKE_FINALIZERS()\n+#else\n+#   define GC_INVOKE_FINALIZERS() (void)GC_invoke_finalizers()\n+#endif\n+\n+#define MERGE_SIZES /* Round up some object sizes, so that fewer distinct */\n+\t\t    /* free lists are actually maintained.  This applies  */\n+\t\t    /* only to the top level routines in misc.c, not to   */\n+\t\t    /* user generated code that calls GC_allocobj and     */\n+\t\t    /* GC_allocaobj directly.                             */\n+\t\t    /* Slows down average programs slightly.  May however */\n+\t\t    /* substantially reduce fragmentation if allocation   */\n+\t\t    /* request sizes are widely scattered.                */\n+\t\t    /* May save significant amounts of space for obj_map  */\n+\t\t    /* entries.\t\t\t\t\t\t  */\n+\n+#ifndef OLD_BLOCK_ALLOC\n+   /* Macros controlling large block allocation strategy.\t*/\n+#  define EXACT_FIRST  \t/* Make a complete pass through the large object */\n+\t\t\t/* free list before splitting a block\t\t */\n+#  define PRESERVE_LAST /* Do not divide last allocated heap segment\t */\n+\t\t\t/* unless we would otherwise need to expand the\t */\n+\t\t\t/* heap.\t\t\t\t\t */\n+#endif\n+\n+/* ALIGN_DOUBLE requires MERGE_SIZES at present. */\n+# if defined(ALIGN_DOUBLE) && !defined(MERGE_SIZES)\n+#   define MERGE_SIZES\n+# endif\n+\n+#if defined(ALL_INTERIOR_POINTERS) && !defined(DONT_ADD_BYTE_AT_END)\n+# define ADD_BYTE_AT_END\n+#endif\n+\n+\n+# ifndef LARGE_CONFIG\n+#   define MINHINCR 16\t/* Minimum heap increment, in blocks of HBLKSIZE  */\n+\t\t\t/* Must be multiple of largest page size.\t  */\n+#   define MAXHINCR 512\t/* Maximum heap increment, in blocks              */\n+# else\n+#   define MINHINCR 64\n+#   define MAXHINCR 4096\n+# endif\n+\n+# define TIME_LIMIT 50\t   /* We try to keep pause times from exceeding\t */\n+\t\t\t   /* this by much. In milliseconds.\t\t */\n+\n+# define BL_LIMIT GC_black_list_spacing\n+\t\t\t   /* If we need a block of N bytes, and we have */\n+\t\t\t   /* a block of N + BL_LIMIT bytes available, \t */\n+\t\t\t   /* and N > BL_LIMIT,\t\t\t\t */\n+\t\t\t   /* but all possible positions in it are \t */\n+\t\t\t   /* blacklisted, we just use it anyway (and\t */\n+\t\t\t   /* print a warning, if warnings are enabled). */\n+\t\t\t   /* This risks subsequently leaking the block\t */\n+\t\t\t   /* due to a false reference.  But not using\t */\n+\t\t\t   /* the block risks unreasonable immediate\t */\n+\t\t\t   /* heap growth.\t\t\t\t */\n+\n+/*********************************/\n+/*                               */\n+/* Stack saving for debugging\t */\n+/*                               */\n+/*********************************/\n+\n+#ifdef SAVE_CALL_CHAIN\n+\n+/*\n+ * Number of frames and arguments to save in objects allocated by\n+ * debugging allocator.\n+ */\n+#   define NFRAMES 6\t/* Number of frames to save. Even for\t\t*/\n+\t\t\t/* alignment reasons.\t\t\t\t*/\n+#   define NARGS 2\t/* Mumber of arguments to save for each call.\t*/\n+\n+#   define NEED_CALLINFO\n+\n+/* Fill in the pc and argument information for up to NFRAMES of my\t*/\n+/* callers.  Ignore my frame and my callers frame.\t\t\t*/\n+void GC_save_callers (/* struct callinfo info[NFRAMES] */);\n+\n+void GC_print_callers (/* struct callinfo info[NFRAMES] */);\n+\n+#else\n+\n+# ifdef GC_ADD_CALLER\n+#   define NFRAMES 1\n+#   define NARGS 0\n+#   define NEED_CALLINFO\n+# endif\n+\n+#endif\n+\n+#ifdef NEED_CALLINFO\n+    struct callinfo {\n+\tword ci_pc;\n+#\tif NARGS > 0\n+\t    word ci_arg[NARGS];\t/* bit-wise complement to avoid retention */\n+#\tendif\n+#\tif defined(ALIGN_DOUBLE) && (NFRAMES * (NARGS + 1)) % 2 == 1\n+\t    /* Likely alignment problem. */\n+\t    word ci_dummy;\n+#\tendif\n+    };\n+#endif\n+\n+\n+/*********************************/\n+/*                               */\n+/* OS interface routines\t */\n+/*                               */\n+/*********************************/\n+\n+#ifdef BSD_TIME\n+#   undef CLOCK_TYPE\n+#   undef GET_TIME\n+#   undef MS_TIME_DIFF\n+#   define CLOCK_TYPE struct timeval\n+#   define GET_TIME(x) { struct rusage rusage; \\\n+\t\t\t getrusage (RUSAGE_SELF,  &rusage); \\\n+\t\t\t x = rusage.ru_utime; }\n+#   define MS_TIME_DIFF(a,b) ((double) (a.tv_sec - b.tv_sec) * 1000.0 \\\n+                               + (double) (a.tv_usec - b.tv_usec) / 1000.0)\n+#else /* !BSD_TIME */\n+#   include <time.h>\n+#   if !defined(__STDC__) && defined(SPARC) && defined(SUNOS4)\n+      clock_t clock();\t/* Not in time.h, where it belongs\t*/\n+#   endif\n+#   if defined(FREEBSD) && !defined(CLOCKS_PER_SEC)\n+#     include <machine/limits.h>\n+#     define CLOCKS_PER_SEC CLK_TCK\n+#   endif\n+#   if !defined(CLOCKS_PER_SEC)\n+#     define CLOCKS_PER_SEC 1000000\n+/*\n+ * This is technically a bug in the implementation.  ANSI requires that\n+ * CLOCKS_PER_SEC be defined.  But at least under SunOS4.1.1, it isn't.\n+ * Also note that the combination of ANSI C and POSIX is incredibly gross\n+ * here. The type clock_t is used by both clock() and times().  But on\n+ * some machines these use different notions of a clock tick,  CLOCKS_PER_SEC\n+ * seems to apply only to clock.  Hence we use it here.  On many machines,\n+ * including SunOS, clock actually uses units of microseconds (which are\n+ * not really clock ticks).\n+ */\n+#   endif\n+#   define CLOCK_TYPE clock_t\n+#   define GET_TIME(x) x = clock()\n+#   define MS_TIME_DIFF(a,b) ((unsigned long) \\\n+\t\t(1000.0*(double)((a)-(b))/(double)CLOCKS_PER_SEC))\n+#endif /* !BSD_TIME */\n+\n+/* We use bzero and bcopy internally.  They may not be available.\t*/\n+# if defined(SPARC) && defined(SUNOS4)\n+#   define BCOPY_EXISTS\n+# endif\n+# if defined(M68K) && defined(AMIGA)\n+#   define BCOPY_EXISTS\n+# endif\n+# if defined(M68K) && defined(NEXT)\n+#   define BCOPY_EXISTS\n+# endif\n+# if defined(VAX)\n+#   define BCOPY_EXISTS\n+# endif\n+# if defined(AMIGA)\n+#   include <string.h>\n+#   define BCOPY_EXISTS\n+# endif\n+\n+# ifndef BCOPY_EXISTS\n+#   include <string.h>\n+#   define BCOPY(x,y,n) memcpy(y, x, (size_t)(n))\n+#   define BZERO(x,n)  memset(x, 0, (size_t)(n))\n+# else\n+#   define BCOPY(x,y,n) bcopy((char *)(x),(char *)(y),(int)(n))\n+#   define BZERO(x,n) bzero((char *)(x),(int)(n))\n+# endif\n+\n+/* HBLKSIZE aligned allocation.  0 is taken to mean failure \t*/\n+/* space is assumed to be cleared.\t\t\t\t*/\n+/* In the case os USE_MMAP, the argument must also be a \t*/\n+/* physical page size.\t\t\t\t\t\t*/\n+# ifdef PCR\n+    char * real_malloc();\n+#   define GET_MEM(bytes) HBLKPTR(real_malloc((size_t)bytes + GC_page_size) \\\n+\t\t\t\t  + GC_page_size-1)\n+# else\n+#   ifdef OS2\n+      void * os2_alloc(size_t bytes);\n+#     define GET_MEM(bytes) HBLKPTR((ptr_t)os2_alloc((size_t)bytes \\\n+\t\t\t\t    + GC_page_size) \\\n+                                    + GC_page_size-1)\n+#   else\n+#     if defined(AMIGA) || defined(NEXT) || defined(DOS4GW)\n+#       define GET_MEM(bytes) HBLKPTR((size_t) \\\n+\t\t\t\t      calloc(1, (size_t)bytes + GC_page_size) \\\n+                                      + GC_page_size-1)\n+#     else\n+#\tifdef MSWIN32\n+          extern ptr_t GC_win32_get_mem();\n+#         define GET_MEM(bytes) (struct hblk *)GC_win32_get_mem(bytes)\n+#\telse\n+#\t  ifdef MACOS\n+#\t    if defined(USE_TEMPORARY_MEMORY)\n+\t\textern Ptr GC_MacTemporaryNewPtr(size_t size,\n+\t\t\t\t\t\t Boolean clearMemory);\n+#               define GET_MEM(bytes) HBLKPTR( \\\n+\t\t    GC_MacTemporaryNewPtr(bytes + GC_page_size, true) \\\n+\t\t    + GC_page_size-1)\n+#\t    else\n+#         \t    define GET_MEM(bytes) HBLKPTR( \\\n+\t\t\tNewPtrClear(bytes + GC_page_size) + GC_page_size-1)\n+#\t    endif\n+#\t  else\n+              extern ptr_t GC_unix_get_mem();\n+#             define GET_MEM(bytes) (struct hblk *)GC_unix_get_mem(bytes)\n+#\t  endif\n+#\tendif\n+#     endif\n+#   endif\n+# endif\n+\n+/*\n+ * Mutual exclusion between allocator/collector routines.\n+ * Needed if there is more than one allocator thread.\n+ * FASTLOCK() is assumed to try to acquire the lock in a cheap and\n+ * dirty way that is acceptable for a few instructions, e.g. by\n+ * inhibiting preemption.  This is assumed to have succeeded only\n+ * if a subsequent call to FASTLOCK_SUCCEEDED() returns TRUE.\n+ * FASTUNLOCK() is called whether or not FASTLOCK_SUCCEEDED().\n+ * If signals cannot be tolerated with the FASTLOCK held, then\n+ * FASTLOCK should disable signals.  The code executed under\n+ * FASTLOCK is otherwise immune to interruption, provided it is\n+ * not restarted.\n+ * DCL_LOCK_STATE declares any local variables needed by LOCK and UNLOCK\n+ * and/or DISABLE_SIGNALS and ENABLE_SIGNALS and/or FASTLOCK.\n+ * (There is currently no equivalent for FASTLOCK.)\n+ */  \n+# ifdef THREADS\n+#  ifdef PCR_OBSOLETE\t/* Faster, but broken with multiple lwp's\t*/\n+#    include  \"th/PCR_Th.h\"\n+#    include  \"th/PCR_ThCrSec.h\"\n+     extern struct PCR_Th_MLRep GC_allocate_ml;\n+#    define DCL_LOCK_STATE  PCR_sigset_t GC_old_sig_mask\n+#    define LOCK() PCR_Th_ML_Acquire(&GC_allocate_ml) \n+#    define UNLOCK() PCR_Th_ML_Release(&GC_allocate_ml)\n+#    define FASTLOCK() PCR_ThCrSec_EnterSys()\n+     /* Here we cheat (a lot): */\n+#        define FASTLOCK_SUCCEEDED() (*(int *)(&GC_allocate_ml) == 0)\n+\t\t/* TRUE if nobody currently holds the lock */\n+#    define FASTUNLOCK() PCR_ThCrSec_ExitSys()\n+#  endif\n+#  ifdef PCR\n+#    include <base/PCR_Base.h>\n+#    include <th/PCR_Th.h>\n+     extern PCR_Th_ML GC_allocate_ml;\n+#    define DCL_LOCK_STATE \\\n+\t PCR_ERes GC_fastLockRes; PCR_sigset_t GC_old_sig_mask\n+#    define LOCK() PCR_Th_ML_Acquire(&GC_allocate_ml)\n+#    define UNLOCK() PCR_Th_ML_Release(&GC_allocate_ml)\n+#    define FASTLOCK() (GC_fastLockRes = PCR_Th_ML_Try(&GC_allocate_ml))\n+#    define FASTLOCK_SUCCEEDED() (GC_fastLockRes == PCR_ERes_okay)\n+#    define FASTUNLOCK()  {\\\n+        if( FASTLOCK_SUCCEEDED() ) PCR_Th_ML_Release(&GC_allocate_ml); }\n+#  endif\n+#  ifdef SRC_M3\n+     extern word RT0u__inCritical;\n+#    define LOCK() RT0u__inCritical++\n+#    define UNLOCK() RT0u__inCritical--\n+#  endif\n+#  ifdef SOLARIS_THREADS\n+#    include <thread.h>\n+#    include <signal.h>\n+     extern mutex_t GC_allocate_ml;\n+#    define LOCK() mutex_lock(&GC_allocate_ml);\n+#    define UNLOCK() mutex_unlock(&GC_allocate_ml);\n+#  endif\n+#  ifdef LINUX_THREADS\n+#    include <pthread.h>\n+#    ifdef __i386__\n+       inline static GC_test_and_set(volatile unsigned int *addr) {\n+\t  int oldval;\n+\t  /* Note: the \"xchg\" instruction does not need a \"lock\" prefix */\n+\t  __asm__ __volatile__(\"xchgl %0, %1\"\n+\t\t: \"=r\"(oldval), \"=m\"(*(addr))\n+\t\t: \"0\"(1), \"m\"(*(addr)));\n+\t  return oldval;\n+       }\n+#    else\n+       -- > Need implementation of GC_test_and_set()\n+#    endif\n+#    define GC_clear(addr) (*(addr) = 0)\n+\n+     extern volatile unsigned int GC_allocate_lock;\n+\t/* This is not a mutex because mutexes that obey the (optional)     */\n+\t/* POSIX scheduling rules are subject to convoys in high contention */\n+\t/* applications.  This is basically a spin lock.\t\t    */\n+     extern pthread_t GC_lock_holder;\n+     extern void GC_lock(void);\n+\t/* Allocation lock holder.  Only set if acquired by client through */\n+\t/* GC_call_with_alloc_lock.\t\t\t\t\t   */\n+#    define SET_LOCK_HOLDER() GC_lock_holder = pthread_self()\n+#    define NO_THREAD (pthread_t)(-1)\n+#    define UNSET_LOCK_HOLDER() GC_lock_holder = NO_THREAD\n+#    define I_HOLD_LOCK() (pthread_equal(GC_lock_holder, pthread_self()))\n+#    ifdef UNDEFINED\n+#    \tdefine LOCK() pthread_mutex_lock(&GC_allocate_ml)\n+#    \tdefine UNLOCK() pthread_mutex_unlock(&GC_allocate_ml)\n+#    else\n+#\tdefine LOCK() \\\n+\t\t{ if (GC_test_and_set(&GC_allocate_lock)) GC_lock(); }\n+#\tdefine UNLOCK() \\\n+\t\tGC_clear(&GC_allocate_lock)\n+#    endif\n+     extern GC_bool GC_collecting;\n+#    define ENTER_GC() \\\n+\t\t{ \\\n+\t\t    GC_collecting = 1; \\\n+\t\t}\n+#    define EXIT_GC() GC_collecting = 0;\n+#  endif /* LINUX_THREADS */\n+#  ifdef IRIX_THREADS\n+#    include <pthread.h>\n+#    include <mutex.h>\n+\n+#    if __mips < 3 || !(defined (_ABIN32) || defined(_ABI64))\n+#        define GC_test_and_set(addr, v) test_and_set(addr,v)\n+#    else\n+#\t  define GC_test_and_set(addr, v) __test_and_set(addr,v)\n+#    endif\n+     extern unsigned long GC_allocate_lock;\n+\t/* This is not a mutex because mutexes that obey the (optional) \t*/\n+\t/* POSIX scheduling rules are subject to convoys in high contention\t*/\n+\t/* applications.  This is basically a spin lock.\t\t\t*/\n+     extern pthread_t GC_lock_holder;\n+     extern void GC_lock(void);\n+\t/* Allocation lock holder.  Only set if acquired by client through */\n+\t/* GC_call_with_alloc_lock.\t\t\t\t\t   */\n+#    define SET_LOCK_HOLDER() GC_lock_holder = pthread_self()\n+#    define NO_THREAD (pthread_t)(-1)\n+#    define UNSET_LOCK_HOLDER() GC_lock_holder = NO_THREAD\n+#    define I_HOLD_LOCK() (pthread_equal(GC_lock_holder, pthread_self()))\n+#    ifdef UNDEFINED\n+#    \tdefine LOCK() pthread_mutex_lock(&GC_allocate_ml)\n+#    \tdefine UNLOCK() pthread_mutex_unlock(&GC_allocate_ml)\n+#    else\n+#\tdefine LOCK() { if (GC_test_and_set(&GC_allocate_lock, 1)) GC_lock(); }\n+#       if __mips >= 3 && (defined (_ABIN32) || defined(_ABI64))\n+#\t    define UNLOCK() __lock_release(&GC_allocate_lock)\n+#\telse\n+#           define UNLOCK() GC_allocate_lock = 0\n+#\tendif\n+#    endif\n+     extern GC_bool GC_collecting;\n+#    define ENTER_GC() \\\n+\t\t{ \\\n+\t\t    GC_collecting = 1; \\\n+\t\t}\n+#    define EXIT_GC() GC_collecting = 0;\n+#  endif /* IRIX_THREADS */\n+#  ifdef WIN32_THREADS\n+#    include <windows.h>\n+     GC_API CRITICAL_SECTION GC_allocate_ml;\n+#    define LOCK() EnterCriticalSection(&GC_allocate_ml);\n+#    define UNLOCK() LeaveCriticalSection(&GC_allocate_ml);\n+#  endif\n+#  ifndef SET_LOCK_HOLDER\n+#      define SET_LOCK_HOLDER()\n+#      define UNSET_LOCK_HOLDER()\n+#      define I_HOLD_LOCK() FALSE\n+\t\t/* Used on platforms were locks can be reacquired,\t*/\n+\t\t/* so it doesn't matter if we lie.\t\t\t*/\n+#  endif\n+# else\n+#    define LOCK()\n+#    define UNLOCK()\n+# endif\n+# ifndef SET_LOCK_HOLDER\n+#   define SET_LOCK_HOLDER()\n+#   define UNSET_LOCK_HOLDER()\n+#   define I_HOLD_LOCK() FALSE\n+\t\t/* Used on platforms were locks can be reacquired,\t*/\n+\t\t/* so it doesn't matter if we lie.\t\t\t*/\n+# endif\n+# ifndef ENTER_GC\n+#   define ENTER_GC()\n+#   define EXIT_GC()\n+# endif\n+\n+# ifndef DCL_LOCK_STATE\n+#   define DCL_LOCK_STATE\n+# endif\n+# ifndef FASTLOCK\n+#   define FASTLOCK() LOCK()\n+#   define FASTLOCK_SUCCEEDED() TRUE\n+#   define FASTUNLOCK() UNLOCK()\n+# endif\n+\n+/* Delay any interrupts or signals that may abort this thread.  Data\t*/\n+/* structures are in a consistent state outside this pair of calls.\t*/\n+/* ANSI C allows both to be empty (though the standard isn't very\t*/\n+/* clear on that point).  Standard malloc implementations are usually\t*/\n+/* neither interruptable nor thread-safe, and thus correspond to\t*/\n+/* empty definitions.\t\t\t\t\t\t\t*/\n+# ifdef PCR\n+#   define DISABLE_SIGNALS() \\\n+\t\t PCR_Th_SetSigMask(PCR_allSigsBlocked,&GC_old_sig_mask)\n+#   define ENABLE_SIGNALS() \\\n+\t\tPCR_Th_SetSigMask(&GC_old_sig_mask, NIL)\n+# else\n+#   if defined(SRC_M3) || defined(AMIGA) || defined(SOLARIS_THREADS) \\\n+\t|| defined(MSWIN32) || defined(MACOS) || defined(DJGPP) \\\n+\t|| defined(NO_SIGNALS) || defined(IRIX_THREADS) \\\n+\t|| defined(LINUX_THREADS)\n+\t\t\t/* Also useful for debugging.\t\t*/\n+\t/* Should probably use thr_sigsetmask for SOLARIS_THREADS. */\n+#     define DISABLE_SIGNALS()\n+#     define ENABLE_SIGNALS()\n+#   else\n+#     define DISABLE_SIGNALS() GC_disable_signals()\n+\tvoid GC_disable_signals();\n+#     define ENABLE_SIGNALS() GC_enable_signals()\n+\tvoid GC_enable_signals();\n+#   endif\n+# endif\n+\n+/*\n+ * Stop and restart mutator threads.\n+ */\n+# ifdef PCR\n+#     include \"th/PCR_ThCtl.h\"\n+#     define STOP_WORLD() \\\n+ \tPCR_ThCtl_SetExclusiveMode(PCR_ThCtl_ExclusiveMode_stopNormal, \\\n+ \t\t\t\t   PCR_allSigsBlocked, \\\n+ \t\t\t\t   PCR_waitForever)\n+#     define START_WORLD() \\\n+\tPCR_ThCtl_SetExclusiveMode(PCR_ThCtl_ExclusiveMode_null, \\\n+ \t\t\t\t   PCR_allSigsBlocked, \\\n+ \t\t\t\t   PCR_waitForever);\n+# else\n+#   if defined(SOLARIS_THREADS) || defined(WIN32_THREADS) \\\n+\t|| defined(IRIX_THREADS) || defined(LINUX_THREADS)\n+      void GC_stop_world();\n+      void GC_start_world();\n+#     define STOP_WORLD() GC_stop_world()\n+#     define START_WORLD() GC_start_world()\n+#   else\n+#     define STOP_WORLD()\n+#     define START_WORLD()\n+#   endif\n+# endif\n+\n+/* Abandon ship */\n+# ifdef PCR\n+#   define ABORT(s) PCR_Base_Panic(s)\n+# else\n+#   ifdef SMALL_CONFIG\n+#\tdefine ABORT(msg) abort();\n+#   else\n+\tGC_API void GC_abort();\n+#       define ABORT(msg) GC_abort(msg);\n+#   endif\n+# endif\n+\n+/* Exit abnormally, but without making a mess (e.g. out of memory) */\n+# ifdef PCR\n+#   define EXIT() PCR_Base_Exit(1,PCR_waitForever)\n+# else\n+#   define EXIT() (void)exit(1)\n+# endif\n+\n+/* Print warning message, e.g. almost out of memory.\t*/\n+# define WARN(msg,arg) (*GC_current_warn_proc)(msg, (GC_word)(arg))\n+extern GC_warn_proc GC_current_warn_proc;\n+\n+/*********************************/\n+/*                               */\n+/* Word-size-dependent defines   */\n+/*                               */\n+/*********************************/\n+\n+#if CPP_WORDSZ == 32\n+#  define WORDS_TO_BYTES(x)   ((x)<<2)\n+#  define BYTES_TO_WORDS(x)   ((x)>>2)\n+#  define LOGWL               ((word)5)    /* log[2] of CPP_WORDSZ */\n+#  define modWORDSZ(n) ((n) & 0x1f)        /* n mod size of word\t    */\n+#  if ALIGNMENT != 4\n+#\tdefine UNALIGNED\n+#  endif\n+#endif\n+\n+#if CPP_WORDSZ == 64\n+#  define WORDS_TO_BYTES(x)   ((x)<<3)\n+#  define BYTES_TO_WORDS(x)   ((x)>>3)\n+#  define LOGWL               ((word)6)    /* log[2] of CPP_WORDSZ */\n+#  define modWORDSZ(n) ((n) & 0x3f)        /* n mod size of word\t    */\n+#  if ALIGNMENT != 8\n+#\tdefine UNALIGNED\n+#  endif\n+#endif\n+\n+#define WORDSZ ((word)CPP_WORDSZ)\n+#define SIGNB  ((word)1 << (WORDSZ-1))\n+#define BYTES_PER_WORD      ((word)(sizeof (word)))\n+#define ONES                ((word)(-1))\n+#define divWORDSZ(n) ((n) >> LOGWL)\t   /* divide n by size of word      */\n+\n+/*********************/\n+/*                   */\n+/*  Size Parameters  */\n+/*                   */\n+/*********************/\n+\n+/*  heap block size, bytes. Should be power of 2 */\n+\n+#ifndef HBLKSIZE\n+# ifdef SMALL_CONFIG\n+#   define CPP_LOG_HBLKSIZE 10\n+# else\n+#   if CPP_WORDSZ == 32\n+#     define CPP_LOG_HBLKSIZE 12\n+#   else\n+#     define CPP_LOG_HBLKSIZE 13\n+#   endif\n+# endif\n+#else\n+# if HBLKSIZE == 512\n+#   define CPP_LOG_HBLKSIZE 9\n+# endif\n+# if HBLKSIZE == 1024\n+#   define CPP_LOG_HBLKSIZE 10\n+# endif\n+# if HBLKSIZE == 2048\n+#   define CPP_LOG_HBLKSIZE 11\n+# endif\n+# if HBLKSIZE == 4096\n+#   define CPP_LOG_HBLKSIZE 12\n+# endif\n+# if HBLKSIZE == 8192\n+#   define CPP_LOG_HBLKSIZE 13\n+# endif\n+# if HBLKSIZE == 16384\n+#   define CPP_LOG_HBLKSIZE 14\n+# endif\n+# ifndef CPP_LOG_HBLKSIZE\n+    --> fix HBLKSIZE\n+# endif\n+# undef HBLKSIZE\n+#endif\n+# define CPP_HBLKSIZE (1 << CPP_LOG_HBLKSIZE)\n+# define LOG_HBLKSIZE   ((word)CPP_LOG_HBLKSIZE)\n+# define HBLKSIZE ((word)CPP_HBLKSIZE)\n+\n+\n+/*  max size objects supported by freelist (larger objects may be   */\n+/*  allocated, but less efficiently)                                */\n+\n+#define CPP_MAXOBJSZ    BYTES_TO_WORDS(CPP_HBLKSIZE/2)\n+#define MAXOBJSZ ((word)CPP_MAXOBJSZ)\n+\t\t\n+# define divHBLKSZ(n) ((n) >> LOG_HBLKSIZE)\n+\n+# define HBLK_PTR_DIFF(p,q) divHBLKSZ((ptr_t)p - (ptr_t)q)\n+\t/* Equivalent to subtracting 2 hblk pointers.\t*/\n+\t/* We do it this way because a compiler should\t*/\n+\t/* find it hard to use an integer division\t*/\n+\t/* instead of a shift.  The bundled SunOS 4.1\t*/\n+\t/* o.w. sometimes pessimizes the subtraction to\t*/\n+\t/* involve a call to .div.\t\t\t*/\n+ \n+# define modHBLKSZ(n) ((n) & (HBLKSIZE-1))\n+ \n+# define HBLKPTR(objptr) ((struct hblk *)(((word) (objptr)) & ~(HBLKSIZE-1)))\n+\n+# define HBLKDISPL(objptr) (((word) (objptr)) & (HBLKSIZE-1))\n+\n+/* Round up byte allocation requests to integral number of words, etc. */\n+# ifdef ADD_BYTE_AT_END\n+#   define ROUNDED_UP_WORDS(n) BYTES_TO_WORDS((n) + WORDS_TO_BYTES(1))\n+#   ifdef ALIGN_DOUBLE\n+#       define ALIGNED_WORDS(n) (BYTES_TO_WORDS((n) + WORDS_TO_BYTES(2)) & ~1)\n+#   else\n+#       define ALIGNED_WORDS(n) ROUNDED_UP_WORDS(n)\n+#   endif\n+#   define SMALL_OBJ(bytes) ((bytes) < WORDS_TO_BYTES(MAXOBJSZ))\n+#   define ADD_SLOP(bytes) ((bytes)+1)\n+# else\n+#   define ROUNDED_UP_WORDS(n) BYTES_TO_WORDS((n) + (WORDS_TO_BYTES(1) - 1))\n+#   ifdef ALIGN_DOUBLE\n+#       define ALIGNED_WORDS(n) \\\n+\t\t\t(BYTES_TO_WORDS((n) + WORDS_TO_BYTES(2) - 1) & ~1)\n+#   else\n+#       define ALIGNED_WORDS(n) ROUNDED_UP_WORDS(n)\n+#   endif\n+#   define SMALL_OBJ(bytes) ((bytes) <= WORDS_TO_BYTES(MAXOBJSZ))\n+#   define ADD_SLOP(bytes) (bytes)\n+# endif\n+\n+\n+/*\n+ * Hash table representation of sets of pages.  This assumes it is\n+ * OK to add spurious entries to sets.\n+ * Used by black-listing code, and perhaps by dirty bit maintenance code.\n+ */\n+ \n+# ifdef LARGE_CONFIG\n+#   define LOG_PHT_ENTRIES  17\n+# else\n+#   define LOG_PHT_ENTRIES  14\t/* Collisions are likely if heap grows\t*/\n+\t\t\t\t/* to more than 16K hblks = 64MB.\t*/\n+\t\t\t\t/* Each hash table occupies 2K bytes.   */\n+# endif\n+# define PHT_ENTRIES ((word)1 << LOG_PHT_ENTRIES)\n+# define PHT_SIZE (PHT_ENTRIES >> LOGWL)\n+typedef word page_hash_table[PHT_SIZE];\n+\n+# define PHT_HASH(addr) ((((word)(addr)) >> LOG_HBLKSIZE) & (PHT_ENTRIES - 1))\n+\n+# define get_pht_entry_from_index(bl, index) \\\n+\t\t(((bl)[divWORDSZ(index)] >> modWORDSZ(index)) & 1)\n+# define set_pht_entry_from_index(bl, index) \\\n+\t\t(bl)[divWORDSZ(index)] |= (word)1 << modWORDSZ(index)\n+# define clear_pht_entry_from_index(bl, index) \\\n+\t\t(bl)[divWORDSZ(index)] &= ~((word)1 << modWORDSZ(index))\n+\t\n+\n+\n+/********************************************/\n+/*                                          */\n+/*    H e a p   B l o c k s                 */\n+/*                                          */\n+/********************************************/\n+\n+/*  heap block header */\n+#define HBLKMASK   (HBLKSIZE-1)\n+\n+#define BITS_PER_HBLK (HBLKSIZE * 8)\n+\n+#define MARK_BITS_PER_HBLK (BITS_PER_HBLK/CPP_WORDSZ)\n+\t   /* upper bound                                    */\n+\t   /* We allocate 1 bit/word.  Only the first word   */\n+\t   /* in each object is actually marked.             */\n+\n+# ifdef ALIGN_DOUBLE\n+#   define MARK_BITS_SZ (((MARK_BITS_PER_HBLK + 2*CPP_WORDSZ - 1) \\\n+\t\t\t  / (2*CPP_WORDSZ))*2)\n+# else\n+#   define MARK_BITS_SZ ((MARK_BITS_PER_HBLK + CPP_WORDSZ - 1)/CPP_WORDSZ)\n+# endif\n+\t   /* Upper bound on number of mark words per heap block  */\n+\n+struct hblkhdr {\n+    word hb_sz;  /* If in use, size in words, of objects in the block. */\n+\t\t /* if free, the size in bytes of the whole block      */\n+    struct hblk * hb_next; \t/* Link field for hblk free list\t */\n+    \t\t\t\t/* and for lists of chunks waiting to be */\n+    \t\t\t\t/* reclaimed.\t\t\t\t */\n+    word hb_descr;   \t\t/* object descriptor for marking.  See\t*/\n+    \t\t\t\t/* mark.h.\t\t\t\t*/\n+    char* hb_map;\t/* A pointer to a pointer validity map of the block. */\n+    \t\t      \t/* See GC_obj_map.\t\t\t\t     */\n+    \t\t     \t/* Valid for all blocks with headers.\t\t     */\n+    \t\t     \t/* Free blocks point to GC_invalid_map.\t\t     */\n+    unsigned char hb_obj_kind;\n+    \t\t\t /* Kind of objects in the block.  Each kind \t*/\n+    \t\t\t /* identifies a mark procedure and a set of \t*/\n+    \t\t\t /* list headers.  Sometimes called regions.\t*/\n+    unsigned char hb_flags;\n+#\tdefine IGNORE_OFF_PAGE\t1\t/* Ignore pointers that do not\t*/\n+\t\t\t\t\t/* point to the first page of \t*/\n+\t\t\t\t\t/* this object.\t\t\t*/\n+    unsigned short hb_last_reclaimed;\n+    \t\t\t\t/* Value of GC_gc_no when block was\t*/\n+    \t\t\t\t/* last allocated or swept. May wrap.   */\n+    word hb_marks[MARK_BITS_SZ];\n+\t\t\t    /* Bit i in the array refers to the             */\n+\t\t\t    /* object starting at the ith word (header      */\n+\t\t\t    /* INCLUDED) in the heap block.                 */\n+\t\t\t    /* The lsb of word 0 is numbered 0.\t\t    */\n+};\n+\n+/*  heap block body */\n+\n+# define DISCARD_WORDS 0\n+\t/* Number of words to be dropped at the beginning of each block\t*/\n+\t/* Must be a multiple of WORDSZ.  May reasonably be nonzero\t*/\n+\t/* on machines that don't guarantee longword alignment of\t*/\n+\t/* pointers, so that the number of false hits is minimized.\t*/\n+\t/* 0 and WORDSZ are probably the only reasonable values.\t*/\n+\n+# define BODY_SZ ((HBLKSIZE-WORDS_TO_BYTES(DISCARD_WORDS))/sizeof(word))\n+\n+struct hblk {\n+#   if (DISCARD_WORDS != 0)\n+        word garbage[DISCARD_WORDS];\n+#   endif\n+    word hb_body[BODY_SZ];\n+};\n+\n+# define HDR_WORDS ((word)DISCARD_WORDS)\n+# define HDR_BYTES ((word)WORDS_TO_BYTES(DISCARD_WORDS))\n+\n+# define OBJ_SZ_TO_BLOCKS(sz) \\\n+    divHBLKSZ(HDR_BYTES + WORDS_TO_BYTES(sz) + HBLKSIZE-1)\n+    /* Size of block (in units of HBLKSIZE) needed to hold objects of\t*/\n+    /* given sz (in words).\t\t\t\t\t\t*/\n+\n+/* Object free list link */\n+# define obj_link(p) (*(ptr_t *)(p))\n+\n+/*  lists of all heap blocks and free lists\t*/\n+/* These are grouped together in a struct\t*/\n+/* so that they can be easily skipped by the\t*/\n+/* GC_mark routine.\t\t\t\t*/\n+/* The ordering is weird to make GC_malloc\t*/\n+/* faster by keeping the important fields\t*/\n+/* sufficiently close together that a\t\t*/\n+/* single load of a base register will do.\t*/\n+/* Scalars that could easily appear to\t\t*/\n+/* be pointers are also put here.\t\t*/\n+/* The main fields should precede any \t\t*/\n+/* conditionally included fields, so that\t*/\n+/* gc_inl.h will work even if a different set\t*/\n+/* of macros is defined when the client is\t*/\n+/* compiled.\t\t\t\t\t*/\n+\n+struct _GC_arrays {\n+  word _heapsize;\n+  word _max_heapsize;\n+  ptr_t _last_heap_addr;\n+  ptr_t _prev_heap_addr;\n+  word _words_allocd_before_gc;\n+\t\t/* Number of words allocated before this\t*/\n+\t\t/* collection cycle.\t\t\t\t*/\n+  word _words_allocd;\n+  \t/* Number of words allocated during this collection cycle */\n+  word _words_wasted;\n+  \t/* Number of words wasted due to internal fragmentation\t*/\n+  \t/* in large objects, or due to dropping blacklisted     */\n+\t/* blocks, since last gc.  Approximate.                 */\n+  word _words_finalized;\n+  \t/* Approximate number of words in objects (and headers)\t*/\n+  \t/* That became ready for finalization in the last \t*/\n+  \t/* collection.\t\t\t\t\t\t*/\n+  word _non_gc_bytes_at_gc;\n+  \t/* Number of explicitly managed bytes of storage \t*/\n+  \t/* at last collection.\t\t\t\t\t*/\n+  word _mem_freed;\n+  \t/* Number of explicitly deallocated words of memory\t*/\n+  \t/* since last collection.\t\t\t\t*/\n+  \t\n+  ptr_t _objfreelist[MAXOBJSZ+1];\n+\t\t\t  /* free list for objects */\n+  ptr_t _aobjfreelist[MAXOBJSZ+1];\n+\t\t\t  /* free list for atomic objs \t*/\n+\n+  ptr_t _uobjfreelist[MAXOBJSZ+1];\n+\t\t\t  /* uncollectable but traced objs \t*/\n+\t\t\t  /* objects on this and auobjfreelist  */\n+\t\t\t  /* are always marked, except during   */\n+\t\t\t  /* garbage collections.\t\t*/\n+# ifdef ATOMIC_UNCOLLECTABLE\n+    ptr_t _auobjfreelist[MAXOBJSZ+1];\n+# endif\n+\t\t\t  /* uncollectable but traced objs \t*/\n+\n+# ifdef GATHERSTATS\n+    word _composite_in_use;\n+   \t\t/* Number of words in accessible composite\t*/\n+\t\t/* objects.\t\t\t\t\t*/\n+    word _atomic_in_use;\n+   \t\t/* Number of words in accessible atomic\t\t*/\n+\t\t/* objects.\t\t\t\t\t*/\n+# endif\n+# ifdef MERGE_SIZES\n+    unsigned _size_map[WORDS_TO_BYTES(MAXOBJSZ+1)];\n+    \t/* Number of words to allocate for a given allocation request in */\n+    \t/* bytes.\t\t\t\t\t\t\t */\n+# endif \n+\n+# ifdef STUBBORN_ALLOC\n+    ptr_t _sobjfreelist[MAXOBJSZ+1];\n+# endif\n+  \t\t\t  /* free list for immutable objects\t*/\n+  ptr_t _obj_map[MAXOBJSZ+1];\n+                       /* If not NIL, then a pointer to a map of valid  */\n+    \t\t       /* object addresses. _obj_map[sz][i] is j if the\t*/\n+    \t\t       /* address block_start+i is a valid pointer      */\n+    \t\t       /* to an object at\t\t\t\t*/\n+    \t\t       /* block_start+i&~3 - WORDS_TO_BYTES(j).\t\t*/\n+    \t\t       /* (If ALL_INTERIOR_POINTERS is defined, then\t*/\n+    \t\t       /* instead ((short *)(hbh_map[sz])[i] is j if\t*/\n+    \t\t       /* block_start+WORDS_TO_BYTES(i) is in the\t*/\n+    \t\t       /* interior of an object starting at\t\t*/\n+    \t\t       /* block_start+WORDS_TO_BYTES(i-j)).\t\t*/\n+    \t\t       /* It is OBJ_INVALID if\t\t\t\t*/\n+    \t\t       /* block_start+WORDS_TO_BYTES(i) is not\t\t*/\n+    \t\t       /* valid as a pointer to an object.              */\n+    \t\t       /* We assume all values of j <= OBJ_INVALID.\t*/\n+    \t\t       /* The zeroth entry corresponds to large objects.*/\n+#   ifdef ALL_INTERIOR_POINTERS\n+#\tdefine map_entry_type short\n+#       define OBJ_INVALID 0x7fff\n+#\tdefine MAP_ENTRY(map, bytes) \\\n+\t\t(((map_entry_type *)(map))[BYTES_TO_WORDS(bytes)])\n+#\tdefine MAP_ENTRIES BYTES_TO_WORDS(HBLKSIZE)\n+#\tdefine MAP_SIZE (MAP_ENTRIES * sizeof(map_entry_type))\n+#\tdefine OFFSET_VALID(displ) TRUE\n+#\tdefine CPP_MAX_OFFSET (HBLKSIZE - HDR_BYTES - 1)\n+#\tdefine MAX_OFFSET ((word)CPP_MAX_OFFSET)\n+#   else\n+#\tdefine map_entry_type char\n+#       define OBJ_INVALID 0x7f\n+#\tdefine MAP_ENTRY(map, bytes) \\\n+\t\t(map)[bytes]\n+#\tdefine MAP_ENTRIES HBLKSIZE\n+#\tdefine MAP_SIZE MAP_ENTRIES\n+#\tdefine CPP_MAX_OFFSET (WORDS_TO_BYTES(OBJ_INVALID) - 1)\t\n+#\tdefine MAX_OFFSET ((word)CPP_MAX_OFFSET)\n+# \tdefine VALID_OFFSET_SZ \\\n+\t  (CPP_MAX_OFFSET > WORDS_TO_BYTES(CPP_MAXOBJSZ)? \\\n+\t   CPP_MAX_OFFSET+1 \\\n+\t   : WORDS_TO_BYTES(CPP_MAXOBJSZ)+1)\n+  \tchar _valid_offsets[VALID_OFFSET_SZ];\n+\t\t\t\t/* GC_valid_offsets[i] == TRUE ==> i \t*/\n+\t\t\t\t/* is registered as a displacement.\t*/\n+#\tdefine OFFSET_VALID(displ) GC_valid_offsets[displ]\n+  \tchar _modws_valid_offsets[sizeof(word)];\n+\t\t\t\t/* GC_valid_offsets[i] ==>\t\t  */\n+\t\t\t\t/* GC_modws_valid_offsets[i%sizeof(word)] */\n+#   endif\n+# ifdef STUBBORN_ALLOC\n+      page_hash_table _changed_pages;\n+        /* Stubborn object pages that were changes since last call to\t*/\n+\t/* GC_read_changed.\t\t\t\t\t\t*/\n+      page_hash_table _prev_changed_pages;\n+        /* Stubborn object pages that were changes before last call to\t*/\n+\t/* GC_read_changed.\t\t\t\t\t\t*/\n+# endif\n+# if defined(PROC_VDB) || defined(MPROTECT_VDB)\n+      page_hash_table _grungy_pages; /* Pages that were dirty at last \t   */\n+\t\t\t\t     /* GC_read_dirty.\t\t\t   */\n+# endif\n+# ifdef LARGE_CONFIG\n+#   if CPP_WORDSZ > 32\n+#     define MAX_HEAP_SECTS 4096 \t/* overflows at roughly 64 GB\t   */\n+#   else\n+#     define MAX_HEAP_SECTS 768\t\t/* Separately added heap sections. */\n+#   endif\n+# else\n+#   define MAX_HEAP_SECTS 256\n+# endif\n+  struct HeapSect {\n+      ptr_t hs_start; word hs_bytes;\n+  } _heap_sects[MAX_HEAP_SECTS];\n+# ifdef MSWIN32\n+    ptr_t _heap_bases[MAX_HEAP_SECTS];\n+    \t\t/* Start address of memory regions obtained from kernel. */\n+# endif\n+  /* Block header index; see gc_headers.h */\n+  bottom_index * _all_nils;\n+  bottom_index * _top_index [TOP_SZ];\n+#ifdef SAVE_CALL_CHAIN\n+  struct callinfo _last_stack[NFRAMES];\t/* Stack at last garbage collection.*/\n+  \t\t\t\t\t/* Useful for debugging\tmysterious  */\n+  \t\t\t\t\t/* object disappearances.\t    */\n+  \t\t\t\t\t/* In the multithreaded case, we    */\n+  \t\t\t\t\t/* currently only save the calling  */\n+  \t\t\t\t\t/* stack.\t\t\t    */\n+#endif\n+};\n+\n+GC_API GC_FAR struct _GC_arrays GC_arrays; \n+\n+# define GC_objfreelist GC_arrays._objfreelist\n+# define GC_aobjfreelist GC_arrays._aobjfreelist\n+# define GC_uobjfreelist GC_arrays._uobjfreelist\n+# ifdef ATOMIC_UNCOLLECTABLE\n+#   define GC_auobjfreelist GC_arrays._auobjfreelist\n+# endif\n+# define GC_sobjfreelist GC_arrays._sobjfreelist\n+# define GC_valid_offsets GC_arrays._valid_offsets\n+# define GC_modws_valid_offsets GC_arrays._modws_valid_offsets\n+# ifdef STUBBORN_ALLOC\n+#    define GC_changed_pages GC_arrays._changed_pages\n+#    define GC_prev_changed_pages GC_arrays._prev_changed_pages\n+# endif\n+# define GC_obj_map GC_arrays._obj_map\n+# define GC_last_heap_addr GC_arrays._last_heap_addr\n+# define GC_prev_heap_addr GC_arrays._prev_heap_addr\n+# define GC_words_allocd GC_arrays._words_allocd\n+# define GC_words_wasted GC_arrays._words_wasted\n+# define GC_words_finalized GC_arrays._words_finalized\n+# define GC_non_gc_bytes_at_gc GC_arrays._non_gc_bytes_at_gc\n+# define GC_mem_freed GC_arrays._mem_freed\n+# define GC_heapsize GC_arrays._heapsize\n+# define GC_max_heapsize GC_arrays._max_heapsize\n+# define GC_words_allocd_before_gc GC_arrays._words_allocd_before_gc\n+# define GC_heap_sects GC_arrays._heap_sects\n+# define GC_last_stack GC_arrays._last_stack\n+# ifdef MSWIN32\n+#   define GC_heap_bases GC_arrays._heap_bases\n+# endif\n+# define GC_all_nils GC_arrays._all_nils\n+# define GC_top_index GC_arrays._top_index\n+# if defined(PROC_VDB) || defined(MPROTECT_VDB)\n+#   define GC_grungy_pages GC_arrays._grungy_pages\n+# endif\n+# ifdef GATHERSTATS\n+#   define GC_composite_in_use GC_arrays._composite_in_use\n+#   define GC_atomic_in_use GC_arrays._atomic_in_use\n+# endif\n+# ifdef MERGE_SIZES\n+#   define GC_size_map GC_arrays._size_map\n+# endif\n+\n+# define beginGC_arrays ((ptr_t)(&GC_arrays))\n+# define endGC_arrays (((ptr_t)(&GC_arrays)) + (sizeof GC_arrays))\n+\n+GC_API word GC_fo_entries;\n+\n+# define MAXOBJKINDS 16\n+\n+/* Object kinds: */\n+extern struct obj_kind {\n+   ptr_t *ok_freelist;\t/* Array of free listheaders for this kind of object */\n+   \t\t\t/* Point either to GC_arrays or to storage allocated */\n+   \t\t\t/* with GC_scratch_alloc.\t\t\t     */\n+   struct hblk **ok_reclaim_list;\n+   \t\t\t/* List headers for lists of blocks waiting to be */\n+   \t\t\t/* swept.\t\t\t\t\t  */\n+   word ok_descriptor;  /* Descriptor template for objects in this\t*/\n+   \t\t\t/* block.\t\t\t\t\t*/\n+   GC_bool ok_relocate_descr;\n+   \t\t\t/* Add object size in bytes to descriptor \t*/\n+   \t\t\t/* template to obtain descriptor.  Otherwise\t*/\n+   \t\t\t/* template is used as is.\t\t\t*/\n+   GC_bool ok_init;     /* Clear objects before putting them on the free list. */\n+} GC_obj_kinds[MAXOBJKINDS];\n+/* Predefined kinds: */\n+# define PTRFREE 0\n+# define NORMAL  1\n+# define UNCOLLECTABLE 2\n+# ifdef ATOMIC_UNCOLLECTABLE\n+#   define AUNCOLLECTABLE 3\n+#   define STUBBORN 4\n+#   define IS_UNCOLLECTABLE(k) (((k) & ~1) == UNCOLLECTABLE)\n+# else\n+#   define STUBBORN 3\n+#   define IS_UNCOLLECTABLE(k) ((k) == UNCOLLECTABLE)\n+# endif\n+\n+extern int GC_n_kinds;\n+\n+extern word GC_n_heap_sects;\t/* Number of separately added heap\t*/\n+\t\t\t\t/* sections.\t\t\t\t*/\n+\n+extern word GC_page_size;\n+\n+# ifdef MSWIN32\n+extern word GC_n_heap_bases;\t/* See GC_heap_bases.\t*/\n+# endif\n+\n+extern word GC_total_stack_black_listed;\n+\t\t\t/* Number of bytes on stack blacklist. \t*/\n+\n+extern word GC_black_list_spacing;\n+\t\t\t/* Average number of bytes between blacklisted\t*/\n+\t\t\t/* blocks. Approximate.\t\t\t\t*/\n+\t\t\t/* Counts only blocks that are \t\t\t*/\n+\t\t\t/* \"stack-blacklisted\", i.e. that are \t\t*/\n+\t\t\t/* problematic in the interior of an object.\t*/\n+\n+extern char * GC_invalid_map;\n+\t\t\t/* Pointer to the nowhere valid hblk map */\n+\t\t\t/* Blocks pointing to this map are free. */\n+\n+extern struct hblk * GC_hblkfreelist;\n+\t\t\t\t/* List of completely empty heap blocks\t*/\n+\t\t\t\t/* Linked through hb_next field of \t*/\n+\t\t\t\t/* header structure associated with\t*/\n+\t\t\t\t/* block.\t\t\t\t*/\n+\n+extern GC_bool GC_is_initialized;\t/* GC_init() has been run.\t*/\n+\n+extern GC_bool GC_objects_are_marked;\t/* There are marked objects in  */\n+\t\t\t\t\t/* the heap.\t\t\t*/\n+\n+extern GC_bool GC_incremental; /* Using incremental/generational collection. */\n+\n+extern GC_bool GC_dirty_maintained;\n+\t\t\t\t/* Dirty bits are being maintained, \t*/\n+\t\t\t\t/* either for incremental collection,\t*/\n+\t\t\t\t/* or to limit the root set.\t\t*/\n+\n+# ifndef PCR\n+    extern ptr_t GC_stackbottom;\t/* Cool end of user stack\t*/\n+# endif\n+\n+extern word GC_root_size;\t/* Total size of registered root sections */\n+\n+extern GC_bool GC_debugging_started;\t/* GC_debug_malloc has been called. */ \n+\n+extern ptr_t GC_least_plausible_heap_addr;\n+extern ptr_t GC_greatest_plausible_heap_addr;\n+\t\t\t/* Bounds on the heap.  Guaranteed valid\t*/\n+\t\t\t/* Likely to include future heap expansion.\t*/\n+\t\t\t\n+/* Operations */\n+# ifndef abs\n+#   define abs(x)  ((x) < 0? (-(x)) : (x))\n+# endif\n+\n+\n+/*  Marks are in a reserved area in                          */\n+/*  each heap block.  Each word has one mark bit associated  */\n+/*  with it. Only those corresponding to the beginning of an */\n+/*  object are used.                                         */\n+\n+\n+/* Mark bit operations */\n+\n+/*\n+ * Retrieve, set, clear the mark bit corresponding\n+ * to the nth word in a given heap block.\n+ *\n+ * (Recall that bit n corresponds to object beginning at word n\n+ * relative to the beginning of the block, including unused words)\n+ */\n+\n+# define mark_bit_from_hdr(hhdr,n) (((hhdr)->hb_marks[divWORDSZ(n)] \\\n+\t\t\t    >> (modWORDSZ(n))) & (word)1)\n+# define set_mark_bit_from_hdr(hhdr,n) (hhdr)->hb_marks[divWORDSZ(n)] \\\n+\t\t\t\t|= (word)1 << modWORDSZ(n)\n+\n+# define clear_mark_bit_from_hdr(hhdr,n) (hhdr)->hb_marks[divWORDSZ(n)] \\\n+\t\t\t\t&= ~((word)1 << modWORDSZ(n))\n+\n+/* Important internal collector routines */\n+\n+ptr_t GC_approx_sp();\n+\n+GC_bool GC_should_collect();\n+#ifdef PRESERVE_LAST\n+    GC_bool GC_in_last_heap_sect(/* ptr_t */);\n+\t/* In last added heap section?  If so, avoid breaking up.\t*/\n+#endif\n+void GC_apply_to_all_blocks(/*fn, client_data*/);\n+\t\t\t/* Invoke fn(hbp, client_data) for each \t*/\n+\t\t\t/* allocated heap block.\t\t\t*/\n+struct hblk * GC_next_block(/* struct hblk * h */);\n+void GC_mark_init();\n+void GC_clear_marks();\t/* Clear mark bits for all heap objects. */\n+void GC_invalidate_mark_state();\t/* Tell the marker that\tmarked \t   */\n+\t\t\t\t\t/* objects may point to\tunmarked   */\n+\t\t\t\t\t/* ones, and roots may point to\t   */\n+\t\t\t\t\t/* unmarked objects.\t\t   */\n+\t\t\t\t\t/* Reset mark stack.\t\t   */\n+void GC_mark_from_mark_stack(); /* Mark from everything on the mark stack. */\n+\t\t\t\t/* Return after about one pages worth of   */\n+\t\t\t\t/* work.\t\t\t\t   */\n+GC_bool GC_mark_stack_empty();\n+GC_bool GC_mark_some();\t/* Perform about one pages worth of marking\t*/\n+\t\t\t/* work of whatever kind is needed.  Returns\t*/\n+\t\t\t/* quickly if no collection is in progress.\t*/\n+\t\t\t/* Return TRUE if mark phase finished.\t\t*/\n+void GC_initiate_gc();\t\t/* initiate collection.\t\t\t*/\n+\t\t\t\t/* If the mark state is invalid, this\t*/\n+\t\t\t\t/* becomes full colleection.  Otherwise */\n+\t\t\t\t/* it's partial.\t\t\t*/\n+void GC_push_all(/*b,t*/);\t/* Push everything in a range \t\t*/\n+\t\t\t\t/* onto mark stack.\t\t\t*/\n+void GC_push_dirty(/*b,t*/);      /* Push all possibly changed\t \t*/\n+\t\t\t\t  /* subintervals of [b,t) onto\t\t*/\n+\t\t\t\t  /* mark stack.\t\t\t*/\n+#ifndef SMALL_CONFIG\n+  void GC_push_conditional(/* ptr_t b, ptr_t t, GC_bool all*/);\n+#else\n+# define GC_push_conditional(b, t, all) GC_push_all(b, t)\n+#endif\n+                                /* Do either of the above, depending\t*/\n+\t\t\t\t/* on the third arg.\t\t\t*/\n+void GC_push_all_stack(/*b,t*/);    /* As above, but consider\t\t*/\n+\t\t\t\t    /*  interior pointers as valid  \t*/\n+void GC_push_roots(/* GC_bool all */); /* Push all or dirty roots.\t*/\n+extern void (*GC_push_other_roots)();\n+\t\t\t/* Push system or application specific roots\t*/\n+\t\t\t/* onto the mark stack.  In some environments\t*/\n+\t\t\t/* (e.g. threads environments) this is\t\t*/\n+\t\t\t/* predfined to be non-zero.  A client supplied */\n+\t\t\t/* replacement should also call the original\t*/\n+\t\t\t/* function.\t\t\t\t\t*/\n+extern void (*GC_start_call_back)(/* void */);\n+\t\t\t/* Called at start of full collections.\t\t*/\n+\t\t\t/* Not called if 0.  Called with allocation \t*/\n+\t\t\t/* lock held.\t\t\t\t\t*/\n+\t\t\t/* 0 by default.\t\t\t\t*/\n+void GC_push_regs();\t/* Push register contents onto mark stack.\t*/\n+void GC_remark();\t/* Mark from all marked objects.  Used\t*/\n+\t\t \t/* only if we had to drop something.\t*/\n+# if defined(MSWIN32)\n+  void __cdecl GC_push_one();\n+# else\n+  void GC_push_one(/*p*/);    /* If p points to an object, mark it    */\n+                              /* and push contents on the mark stack  */\n+# endif\n+void GC_push_one_checked(/*p*/); /* Ditto, omits plausibility test\t*/\n+void GC_push_marked(/* struct hblk h, hdr * hhdr */);\n+\t\t/* Push contents of all marked objects in h onto\t*/\n+\t\t/* mark stack.\t\t\t\t\t\t*/\n+#ifdef SMALL_CONFIG\n+# define GC_push_next_marked_dirty(h) GC_push_next_marked(h)\n+#else\n+  struct hblk * GC_push_next_marked_dirty(/* h */);\n+\t\t/* Invoke GC_push_marked on next dirty block above h.\t*/\n+\t\t/* Return a pointer just past the end of this block.\t*/\n+#endif /* !SMALL_CONFIG */\n+struct hblk * GC_push_next_marked(/* h */);\n+\t\t/* Ditto, but also mark from clean pages.\t*/\n+struct hblk * GC_push_next_marked_uncollectable(/* h */);\n+\t\t/* Ditto, but mark only from uncollectable pages.\t*/\n+GC_bool GC_stopped_mark(); /* Stop world and mark from all roots\t*/\n+\t\t\t/* and rescuers.\t\t\t*/\n+void GC_clear_hdr_marks(/* hhdr */);  /* Clear the mark bits in a header */\n+void GC_set_hdr_marks(/* hhdr */);  /* Set the mark bits in a header */\n+void GC_add_roots_inner();\n+GC_bool GC_is_static_root(/* ptr_t p */);\n+\t\t/* Is the address p in one of the registered static\t*/\n+\t\t/* root sections?\t\t\t\t\t*/\n+void GC_register_dynamic_libraries();\n+\t\t/* Add dynamic library data sections to the root set. */\n+\n+/* Machine dependent startup routines */\n+ptr_t GC_get_stack_base();\n+void GC_register_data_segments();\n+\n+/* Black listing: */\n+void GC_bl_init(); \t\n+# ifndef ALL_INTERIOR_POINTERS\n+    void GC_add_to_black_list_normal(/* bits, maybe source */);\n+\t\t\t/* Register bits as a possible future false\t*/\n+\t\t\t/* reference from the heap or static data\t*/\n+#   ifdef PRINT_BLACK_LIST\n+#     define GC_ADD_TO_BLACK_LIST_NORMAL(bits, source) \\\n+\t\t\tGC_add_to_black_list_normal(bits, source)\n+#   else\n+#     define GC_ADD_TO_BLACK_LIST_NORMAL(bits, source) \\\n+\t\t\tGC_add_to_black_list_normal(bits)\n+#   endif\n+# else\n+#   ifdef PRINT_BLACK_LIST\n+#     define GC_ADD_TO_BLACK_LIST_NORMAL(bits, source) \\\n+\t\t\tGC_add_to_black_list_stack(bits, source)\n+#   else\n+#     define GC_ADD_TO_BLACK_LIST_NORMAL(bits, source) \\\n+\t\t\tGC_add_to_black_list_stack(bits)\n+#   endif\n+# endif\n+\n+void GC_add_to_black_list_stack(/* bits, maybe source */);\n+struct hblk * GC_is_black_listed(/* h, len */);\n+\t\t\t/* If there are likely to be false references\t*/\n+\t\t\t/* to a block starting at h of the indicated    */\n+\t\t\t/* length, then return the next plausible\t*/\n+\t\t\t/* starting location for h that might avoid\t*/\n+\t\t\t/* these false references.\t\t\t*/\n+void GC_promote_black_lists();\n+\t\t\t/* Declare an end to a black listing phase.\t*/\n+void GC_unpromote_black_lists();\n+\t\t\t/* Approximately undo the effect of the above.\t*/\n+\t\t\t/* This actually loses some information, but\t*/\n+\t\t\t/* only in a reasonably safe way.\t\t*/\n+word GC_number_stack_black_listed(/*struct hblk *start, struct hblk *endp1 */);\n+\t\t\t/* Return the number of (stack) blacklisted\t*/\n+\t\t\t/* blocks in the range for statistical\t\t*/\n+\t\t\t/* purposes.\t\t\t\t\t*/\n+\t\t \t\n+ptr_t GC_scratch_alloc(/*bytes*/);\n+\t\t\t\t/* GC internal memory allocation for\t*/\n+\t\t\t\t/* small objects.  Deallocation is not  */\n+\t\t\t\t/* possible.\t\t\t\t*/\n+\t\n+/* Heap block layout maps: */\t\t\t\n+void GC_invalidate_map(/* hdr */);\n+\t\t\t\t/* Remove the object map associated\t*/\n+\t\t\t\t/* with the block.  This identifies\t*/\n+\t\t\t\t/* the block as invalid to the mark\t*/\n+\t\t\t\t/* routines.\t\t\t\t*/\n+GC_bool GC_add_map_entry(/*sz*/);\n+\t\t\t\t/* Add a heap block map for objects of\t*/\n+\t\t\t\t/* size sz to obj_map.\t\t\t*/\n+\t\t\t\t/* Return FALSE on failure.\t\t*/\n+void GC_register_displacement_inner(/*offset*/);\n+\t\t\t\t/* Version of GC_register_displacement\t*/\n+\t\t\t\t/* that assumes lock is already held\t*/\n+\t\t\t\t/* and signals are already disabled.\t*/\n+\n+/*  hblk allocation: */\t\t\n+void GC_new_hblk(/*size_in_words, kind*/);\n+\t\t\t\t/* Allocate a new heap block, and build */\n+\t\t\t\t/* a free list in it.\t\t\t*/\t\t\t\t\n+struct hblk * GC_allochblk(/*size_in_words, kind*/);\n+\t\t\t\t/* Allocate a heap block, clear it if\t*/\n+\t\t\t\t/* for composite objects, inform\t*/\n+\t\t\t\t/* the marker that block is valid\t*/\n+\t\t\t\t/* for objects of indicated size.\t*/\n+\t\t\t\t/* sz < 0 ==> atomic.\t\t\t*/ \n+void GC_freehblk();\t\t/* Deallocate a heap block and mark it  */\n+\t\t\t\t/* as invalid.\t\t\t\t*/\n+\t\t\t\t\n+/*  Misc GC: */\n+void GC_init_inner();\n+GC_bool GC_expand_hp_inner();\n+void GC_start_reclaim(/*abort_if_found*/);\n+\t\t\t\t/* Restore unmarked objects to free\t*/\n+\t\t\t\t/* lists, or (if abort_if_found is\t*/\n+\t\t\t\t/* TRUE) report them.\t\t\t*/\n+\t\t\t\t/* Sweeping of small object pages is\t*/\n+\t\t\t\t/* largely deferred.\t\t\t*/\n+void GC_continue_reclaim(/*size, kind*/);\n+\t\t\t\t/* Sweep pages of the given size and\t*/\n+\t\t\t\t/* kind, as long as possible, and\t*/\n+\t\t\t\t/* as long as the corr. free list is    */\n+\t\t\t\t/* empty.\t\t\t\t*/\n+void GC_reclaim_or_delete_all();\n+\t\t\t\t/* Arrange for all reclaim lists to be\t*/\n+\t\t\t\t/* empty.  Judiciously choose between\t*/\n+\t\t\t\t/* sweeping and discarding each page.\t*/\n+GC_bool GC_reclaim_all(/* GC_stop_func f*/);\n+\t\t\t\t/* Reclaim all blocks.  Abort (in a\t*/\n+\t\t\t\t/* consistent state) if f returns TRUE. */\n+GC_bool GC_block_empty(/* hhdr */); /* Block completely unmarked? \t*/\n+GC_bool GC_never_stop_func();\t/* Returns FALSE.\t\t*/\n+GC_bool GC_try_to_collect_inner(/* GC_stop_func f */);\n+\t\t\t\t/* Collect; caller must have acquired\t*/\n+\t\t\t\t/* lock and disabled signals.\t\t*/\n+\t\t\t\t/* Collection is aborted if f returns\t*/\n+\t\t\t\t/* TRUE.  Returns TRUE if it completes\t*/\n+\t\t\t\t/* successfully.\t\t\t*/\n+# define GC_gcollect_inner() \\\n+\t(void) GC_try_to_collect_inner(GC_never_stop_func)\n+void GC_finish_collection();\t/* Finish collection.  Mark bits are\t*/\n+\t\t\t\t/* consistent and lock is still held.\t*/\n+GC_bool GC_collect_or_expand(/* needed_blocks */);\n+\t\t\t\t/* Collect or expand heap in an attempt */\n+\t\t\t\t/* make the indicated number of free\t*/\n+\t\t\t\t/* blocks available.  Should be called\t*/\n+\t\t\t\t/* until the blocks are available or\t*/\n+\t\t\t\t/* until it fails by returning FALSE.\t*/\n+void GC_init();\t\t\t/* Initialize collector.\t\t*/\n+void GC_collect_a_little_inner(/* int n */);\n+\t\t\t\t/* Do n units worth of garbage \t\t*/\n+\t\t\t\t/* collection work, if appropriate.\t*/\n+\t\t\t\t/* A unit is an amount appropriate for  */\n+\t\t\t\t/* HBLKSIZE bytes of allocation.\t*/\n+ptr_t GC_generic_malloc(/* bytes, kind */);\n+\t\t\t\t/* Allocate an object of the given\t*/\n+\t\t\t\t/* kind.  By default, there are only\t*/\n+\t\t\t\t/* a few kinds: composite(pointerfree), */\n+\t\t\t\t/* atomic, uncollectable, etc.\t\t*/\n+\t\t\t\t/* We claim it's possible for clever\t*/\n+\t\t\t\t/* client code that understands GC\t*/\n+\t\t\t\t/* internals to add more, e.g. to\t*/\n+\t\t\t\t/* communicate object layout info\t*/\n+\t\t\t\t/* to the collector.\t\t\t*/\n+ptr_t GC_generic_malloc_ignore_off_page(/* bytes, kind */);\n+\t\t\t\t/* As above, but pointers past the \t*/\n+\t\t\t\t/* first page of the resulting object\t*/\n+\t\t\t\t/* are ignored.\t\t\t\t*/\n+ptr_t GC_generic_malloc_inner(/* bytes, kind */);\n+\t\t\t\t/* Ditto, but I already hold lock, etc.\t*/\n+ptr_t GC_generic_malloc_words_small GC_PROTO((size_t words, int kind));\n+\t\t\t\t/* As above, but size in units of words */\n+\t\t\t\t/* Bypasses MERGE_SIZES.  Assumes\t*/\n+\t\t\t\t/* words <= MAXOBJSZ.\t\t\t*/\n+ptr_t GC_generic_malloc_inner_ignore_off_page(/* bytes, kind */);\n+\t\t\t\t/* Allocate an object, where\t\t*/\n+\t\t\t\t/* the client guarantees that there\t*/\n+\t\t\t\t/* will always be a pointer to the \t*/\n+\t\t\t\t/* beginning of the object while the\t*/\n+\t\t\t\t/* object is live.\t\t\t*/\n+ptr_t GC_allocobj(/* sz_inn_words, kind */);\n+\t\t\t\t/* Make the indicated \t\t\t*/\n+\t\t\t\t/* free list nonempty, and return its\t*/\n+\t\t\t\t/* head.\t\t\t\t*/\n+\n+void GC_init_headers();\n+GC_bool GC_install_header(/*h*/);\n+\t\t\t\t/* Install a header for block h.\t*/\n+\t\t\t\t/* Return FALSE on failure.\t\t*/\n+GC_bool GC_install_counts(/*h, sz*/);\n+\t\t\t\t/* Set up forwarding counts for block\t*/\n+\t\t\t\t/* h of size sz.\t\t\t*/\n+\t\t\t\t/* Return FALSE on failure.\t\t*/\n+void GC_remove_header(/*h*/);\n+\t\t\t\t/* Remove the header for block h.\t*/\n+void GC_remove_counts(/*h, sz*/);\n+\t\t\t\t/* Remove forwarding counts for h.\t*/\n+hdr * GC_find_header(/*p*/);\t/* Debugging only.\t\t\t*/\n+\n+void GC_finalize();\t/* Perform all indicated finalization actions\t*/\n+\t\t\t/* on unmarked objects.\t\t\t\t*/\n+\t\t\t/* Unreachable finalizable objects are enqueued\t*/\n+\t\t\t/* for processing by GC_invoke_finalizers.\t*/\n+\t\t\t/* Invoked with lock.\t\t\t\t*/\n+\t\t\t\n+void GC_add_to_heap(/*p, bytes*/);\n+\t\t\t/* Add a HBLKSIZE aligned chunk to the heap.\t*/\n+\n+void GC_print_obj(/* ptr_t p */);\n+\t\t\t/* P points to somewhere inside an object with\t*/\n+\t\t\t/* debugging info.  Print a human readable\t*/\n+\t\t\t/* description of the object to stderr.\t\t*/\n+extern void (*GC_check_heap)();\n+\t\t\t/* Check that all objects in the heap with \t*/\n+\t\t\t/* debugging info are intact.  Print \t\t*/\n+\t\t\t/* descriptions of any that are not.\t\t*/\n+extern void (*GC_print_heap_obj)(/* ptr_t p */);\n+\t\t\t/* If possible print s followed by a more\t*/\n+\t\t\t/* detailed description of the object \t\t*/\n+\t\t\t/* referred to by p.\t\t\t\t*/\n+\t\t\t\n+/* Virtual dirty bit implementation:\t\t*/\n+/* Each implementation exports the following:\t*/\n+void GC_read_dirty();\t/* Retrieve dirty bits.\t*/\n+GC_bool GC_page_was_dirty(/* struct hblk * h  */);\n+\t\t\t/* Read retrieved dirty bits.\t*/\n+GC_bool GC_page_was_ever_dirty(/* struct hblk * h  */);\n+\t\t\t/* Could the page contain valid heap pointers?\t*/\n+void GC_is_fresh(/* struct hblk * h, word number_of_blocks  */);\n+\t\t\t/* Assert the region currently contains no\t*/\n+\t\t\t/* valid pointers.\t\t\t\t*/\n+void GC_write_hint(/* struct hblk * h  */);\n+\t\t\t/* h is about to be written.\t*/\n+void GC_dirty_init();\n+\n+/* Slow/general mark bit manipulation: */\n+GC_bool GC_is_marked();\n+void GC_clear_mark_bit();\n+void GC_set_mark_bit();\n+\n+/* Stubborn objects: */\n+void GC_read_changed();\t/* Analogous to GC_read_dirty */\n+GC_bool GC_page_was_changed(/* h */);\t/* Analogous to GC_page_was_dirty */\n+void GC_clean_changing_list();\t/* Collect obsolete changing list entries */\n+void GC_stubborn_init();\n+\n+/* Debugging print routines: */\n+void GC_print_block_list();\n+void GC_print_hblkfreelist();\n+void GC_print_heap_sects();\n+void GC_print_static_roots();\n+void GC_dump();\n+\n+/* Make arguments appear live to compiler */\n+# ifdef __WATCOMC__\n+  void GC_noop(void*, ...);\n+# else\n+  GC_API void GC_noop();\n+# endif\n+\n+void GC_noop1(/* word arg */);\n+\n+/* Logging and diagnostic output: \t*/\n+GC_API void GC_printf GC_PROTO((char * format, long, long, long, long, long, long));\n+\t\t\t/* A version of printf that doesn't allocate,\t*/\n+\t\t\t/* is restricted to long arguments, and\t\t*/\n+\t\t\t/* (unfortunately) doesn't use varargs for\t*/\n+\t\t\t/* portability.  Restricted to 6 args and\t*/\n+\t\t\t/* 1K total output length.\t\t\t*/\n+\t\t\t/* (We use sprintf.  Hopefully that doesn't\t*/\n+\t\t\t/* allocate for long arguments.)  \t\t*/\n+# define GC_printf0(f) GC_printf(f, 0l, 0l, 0l, 0l, 0l, 0l)\n+# define GC_printf1(f,a) GC_printf(f, (long)a, 0l, 0l, 0l, 0l, 0l)\n+# define GC_printf2(f,a,b) GC_printf(f, (long)a, (long)b, 0l, 0l, 0l, 0l)\n+# define GC_printf3(f,a,b,c) GC_printf(f, (long)a, (long)b, (long)c, 0l, 0l, 0l)\n+# define GC_printf4(f,a,b,c,d) GC_printf(f, (long)a, (long)b, (long)c, \\\n+\t\t\t\t\t    (long)d, 0l, 0l)\n+# define GC_printf5(f,a,b,c,d,e) GC_printf(f, (long)a, (long)b, (long)c, \\\n+\t\t\t\t\t      (long)d, (long)e, 0l)\n+# define GC_printf6(f,a,b,c,d,e,g) GC_printf(f, (long)a, (long)b, (long)c, \\\n+\t\t\t\t\t\t(long)d, (long)e, (long)g)\n+\n+void GC_err_printf(/* format, a, b, c, d, e, f */);\n+# define GC_err_printf0(f) GC_err_puts(f)\n+# define GC_err_printf1(f,a) GC_err_printf(f, (long)a, 0l, 0l, 0l, 0l, 0l)\n+# define GC_err_printf2(f,a,b) GC_err_printf(f, (long)a, (long)b, 0l, 0l, 0l, 0l)\n+# define GC_err_printf3(f,a,b,c) GC_err_printf(f, (long)a, (long)b, (long)c, \\\n+\t\t\t\t\t\t  0l, 0l, 0l)\n+# define GC_err_printf4(f,a,b,c,d) GC_err_printf(f, (long)a, (long)b, \\\n+\t\t\t\t\t\t    (long)c, (long)d, 0l, 0l)\n+# define GC_err_printf5(f,a,b,c,d,e) GC_err_printf(f, (long)a, (long)b, \\\n+\t\t\t\t\t\t      (long)c, (long)d, \\\n+\t\t\t\t\t\t      (long)e, 0l)\n+# define GC_err_printf6(f,a,b,c,d,e,g) GC_err_printf(f, (long)a, (long)b, \\\n+\t\t\t\t\t\t\t(long)c, (long)d, \\\n+\t\t\t\t\t\t\t(long)e, (long)g)\n+\t\t\t/* Ditto, writes to stderr.\t\t\t*/\n+\t\t\t\n+void GC_err_puts(/* char *s */);\n+\t\t\t/* Write s to stderr, don't buffer, don't add\t*/\n+\t\t\t/* newlines, don't ...\t\t\t\t*/\n+\n+\n+# endif /* GC_PRIVATE_H */"}]}