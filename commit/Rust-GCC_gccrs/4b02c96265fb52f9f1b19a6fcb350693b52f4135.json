{"sha": "4b02c96265fb52f9f1b19a6fcb350693b52f4135", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NGIwMmM5NjI2NWZiNTJmOWYxYjE5YTZmY2IzNTA2OTNiNTJmNDEzNQ==", "commit": {"author": {"name": "Michael Meissner", "email": "meissner@linux.vnet.ibm.com", "date": "2013-06-12T22:41:38Z"}, "committer": {"name": "Michael Meissner", "email": "meissner@gcc.gnu.org", "date": "2013-06-12T22:41:38Z"}, "message": "rs6000.c (emit_load_locked): Add support for power8 byte, half-word, and quad-word atomic instructions.\n\n[gcc]\n2013-06-12  Michael Meissner  <meissner@linux.vnet.ibm.com>\n\t    Pat Haugen <pthaugen@us.ibm.com>\n\t    Peter Bergner <bergner@vnet.ibm.com>\n\n\t* config/rs6000/rs6000.c (emit_load_locked): Add support for\n\tpower8 byte, half-word, and quad-word atomic instructions.\n\t(emit_store_conditional): Likewise.\n\t(rs6000_expand_atomic_compare_and_swap): Likewise.\n\t(rs6000_expand_atomic_op): Likewise.\n\n\t* config/rs6000/sync.md (larx): Add new modes for power8.\n\t(stcx): Likewise.\n\t(AINT): New mode iterator to include TImode as well as normal\n\tinteger modes on power8.\n\t(fetchop_pred): Use int_reg_operand instead of gpc_reg_operand so\n\tthat VSX registers are not considered.  Use AINT mode iterator\n\tinstead of INT1 to allow inclusion of quad word atomic operations\n\ton power8.\n\t(load_locked<mode>): Likewise.\n\t(store_conditional<mode>): Likewise.\n\t(atomic_compare_and_swap<mode>): Likewise.\n\t(atomic_exchange<mode>): Likewise.\n\t(atomic_nand<mode>): Likewise.\n\t(atomic_fetch_<fetchop_name><mode>): Likewise.\n\t(atomic_nand_fetch<mode>): Likewise.\n\t(mem_thread_fence): Use gen_loadsync_<mode> instead of enumerating\n\teach type.\n\t(ATOMIC): On power8, add QImode, HImode modes.\n\t(load_locked<QHI:mode>_si): Varients of load_locked for QI/HI\n\tmodes that promote to SImode.\n\t(load_lockedti): Convert TImode arguments to PTImode, so that we\n\tget a guaranteed even/odd register pair.\n\t(load_lockedpti): Likewise.\n\t(store_conditionalti): Likewise.\n\t(store_conditionalpti): Likewise.\n\n\t* config/rs6000/rs6000.md (QHI): New mode iterator for power8\n\tatomic load/store instructions.\n\t(HSI): Likewise.\n\n[gcc/testsuite]\n2013-06-12  Michael Meissner  <meissner@linux.vnet.ibm.com>\n\t    Pat Haugen <pthaugen@us.ibm.com>\n\t    Peter Bergner <bergner@vnet.ibm.com>\n\n\t* gcc.target/powerpc/atomic-p7.c: New file, add tests for atomic\n\tload/store instructions on power7, power8.\n\t* gcc.target/powerpc/atomic-p8.c: Likewise.\n\n\nCo-Authored-By: Pat Haugen <pthaugen@us.ibm.com>\nCo-Authored-By: Peter Bergner <bergner@vnet.ibm.com>\n\nFrom-SVN: r200044", "tree": {"sha": "ea345c131c43e5b0cfcc0c98ab871b17860e7e27", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ea345c131c43e5b0cfcc0c98ab871b17860e7e27"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/4b02c96265fb52f9f1b19a6fcb350693b52f4135", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4b02c96265fb52f9f1b19a6fcb350693b52f4135", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4b02c96265fb52f9f1b19a6fcb350693b52f4135", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4b02c96265fb52f9f1b19a6fcb350693b52f4135/comments", "author": null, "committer": null, "parents": [{"sha": "73a23b06358ad97298f638c98c5857aa5cf0427c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/73a23b06358ad97298f638c98c5857aa5cf0427c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/73a23b06358ad97298f638c98c5857aa5cf0427c"}], "stats": {"total": 849, "additions": 751, "deletions": 98}, "files": [{"sha": "83ccd25dd0d901b826ee85cbd81808a727b3c359", "filename": "gcc/ChangeLog", "status": "modified", "additions": 40, "deletions": 0, "changes": 40, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=4b02c96265fb52f9f1b19a6fcb350693b52f4135", "patch": "@@ -1,3 +1,43 @@\n+2013-06-12  Michael Meissner  <meissner@linux.vnet.ibm.com>\n+\t    Pat Haugen <pthaugen@us.ibm.com>\n+\t    Peter Bergner <bergner@vnet.ibm.com>\n+\n+\t* config/rs6000/rs6000.c (emit_load_locked): Add support for\n+\tpower8 byte, half-word, and quad-word atomic instructions.\n+\t(emit_store_conditional): Likewise.\n+\t(rs6000_expand_atomic_compare_and_swap): Likewise.\n+\t(rs6000_expand_atomic_op): Likewise.\n+\n+\t* config/rs6000/sync.md (larx): Add new modes for power8.\n+\t(stcx): Likewise.\n+\t(AINT): New mode iterator to include TImode as well as normal\n+\tinteger modes on power8.\n+\t(fetchop_pred): Use int_reg_operand instead of gpc_reg_operand so\n+\tthat VSX registers are not considered.  Use AINT mode iterator\n+\tinstead of INT1 to allow inclusion of quad word atomic operations\n+\ton power8.\n+\t(load_locked<mode>): Likewise.\n+\t(store_conditional<mode>): Likewise.\n+\t(atomic_compare_and_swap<mode>): Likewise.\n+\t(atomic_exchange<mode>): Likewise.\n+\t(atomic_nand<mode>): Likewise.\n+\t(atomic_fetch_<fetchop_name><mode>): Likewise.\n+\t(atomic_nand_fetch<mode>): Likewise.\n+\t(mem_thread_fence): Use gen_loadsync_<mode> instead of enumerating\n+\teach type.\n+\t(ATOMIC): On power8, add QImode, HImode modes.\n+\t(load_locked<QHI:mode>_si): Varients of load_locked for QI/HI\n+\tmodes that promote to SImode.\n+\t(load_lockedti): Convert TImode arguments to PTImode, so that we\n+\tget a guaranteed even/odd register pair.\n+\t(load_lockedpti): Likewise.\n+\t(store_conditionalti): Likewise.\n+\t(store_conditionalpti): Likewise.\n+\n+\t* config/rs6000/rs6000.md (QHI): New mode iterator for power8\n+\tatomic load/store instructions.\n+\t(HSI): Likewise.\n+\n 2013-06-12  Richard Sandiford  <rdsandiford@googlemail.com>\n \n \t* config/mips/mips.md (extended_mips16): Include GOT and constant-pool"}, {"sha": "c5087b1a9157e29d14a10ce611d4603f1737d032", "filename": "gcc/config/rs6000/rs6000.c", "status": "modified", "additions": 133, "deletions": 52, "changes": 185, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Fconfig%2Frs6000%2Frs6000.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Fconfig%2Frs6000%2Frs6000.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.c?ref=4b02c96265fb52f9f1b19a6fcb350693b52f4135", "patch": "@@ -17748,7 +17748,8 @@ emit_unlikely_jump (rtx cond, rtx label)\n }\n \n /* A subroutine of the atomic operation splitters.  Emit a load-locked\n-   instruction in MODE.  */\n+   instruction in MODE.  For QI/HImode, possibly use a pattern than includes\n+   the zero_extend operation.  */\n \n static void\n emit_load_locked (enum machine_mode mode, rtx reg, rtx mem)\n@@ -17757,12 +17758,26 @@ emit_load_locked (enum machine_mode mode, rtx reg, rtx mem)\n \n   switch (mode)\n     {\n+    case QImode:\n+      fn = gen_load_lockedqi;\n+      break;\n+    case HImode:\n+      fn = gen_load_lockedhi;\n+      break;\n     case SImode:\n-      fn = gen_load_lockedsi;\n+      if (GET_MODE (mem) == QImode)\n+\tfn = gen_load_lockedqi_si;\n+      else if (GET_MODE (mem) == HImode)\n+\tfn = gen_load_lockedhi_si;\n+      else\n+\tfn = gen_load_lockedsi;\n       break;\n     case DImode:\n       fn = gen_load_lockeddi;\n       break;\n+    case TImode:\n+      fn = gen_load_lockedti;\n+      break;\n     default:\n       gcc_unreachable ();\n     }\n@@ -17779,12 +17794,21 @@ emit_store_conditional (enum machine_mode mode, rtx res, rtx mem, rtx val)\n \n   switch (mode)\n     {\n+    case QImode:\n+      fn = gen_store_conditionalqi;\n+      break;\n+    case HImode:\n+      fn = gen_store_conditionalhi;\n+      break;\n     case SImode:\n       fn = gen_store_conditionalsi;\n       break;\n     case DImode:\n       fn = gen_store_conditionaldi;\n       break;\n+    case TImode:\n+      fn = gen_store_conditionalti;\n+      break;\n     default:\n       gcc_unreachable ();\n     }\n@@ -17931,7 +17955,7 @@ rs6000_expand_atomic_compare_and_swap (rtx operands[])\n {\n   rtx boolval, retval, mem, oldval, newval, cond;\n   rtx label1, label2, x, mask, shift;\n-  enum machine_mode mode;\n+  enum machine_mode mode, orig_mode;\n   enum memmodel mod_s, mod_f;\n   bool is_weak;\n \n@@ -17943,22 +17967,29 @@ rs6000_expand_atomic_compare_and_swap (rtx operands[])\n   is_weak = (INTVAL (operands[5]) != 0);\n   mod_s = (enum memmodel) INTVAL (operands[6]);\n   mod_f = (enum memmodel) INTVAL (operands[7]);\n-  mode = GET_MODE (mem);\n+  orig_mode = mode = GET_MODE (mem);\n \n   mask = shift = NULL_RTX;\n   if (mode == QImode || mode == HImode)\n     {\n-      mem = rs6000_adjust_atomic_subword (mem, &shift, &mask);\n-\n-      /* Shift and mask OLDVAL into position with the word.  */\n+      /* Before power8, we didn't have access to lbarx/lharx, so generate a\n+\t lwarx and shift/mask operations.  With power8, we need to do the\n+\t comparison in SImode, but the store is still done in QI/HImode.  */\n       oldval = convert_modes (SImode, mode, oldval, 1);\n-      oldval = expand_simple_binop (SImode, ASHIFT, oldval, shift,\n-\t\t\t\t    NULL_RTX, 1, OPTAB_LIB_WIDEN);\n \n-      /* Shift and mask NEWVAL into position within the word.  */\n-      newval = convert_modes (SImode, mode, newval, 1);\n-      newval = expand_simple_binop (SImode, ASHIFT, newval, shift,\n-\t\t\t\t    NULL_RTX, 1, OPTAB_LIB_WIDEN);\n+      if (!TARGET_SYNC_HI_QI)\n+\t{\n+\t  mem = rs6000_adjust_atomic_subword (mem, &shift, &mask);\n+\n+\t  /* Shift and mask OLDVAL into position with the word.  */\n+\t  oldval = expand_simple_binop (SImode, ASHIFT, oldval, shift,\n+\t\t\t\t\tNULL_RTX, 1, OPTAB_LIB_WIDEN);\n+\n+\t  /* Shift and mask NEWVAL into position within the word.  */\n+\t  newval = convert_modes (SImode, mode, newval, 1);\n+\t  newval = expand_simple_binop (SImode, ASHIFT, newval, shift,\n+\t\t\t\t\tNULL_RTX, 1, OPTAB_LIB_WIDEN);\n+\t}\n \n       /* Prepare to adjust the return value.  */\n       retval = gen_reg_rtx (SImode);\n@@ -17987,7 +18018,25 @@ rs6000_expand_atomic_compare_and_swap (rtx operands[])\n     }\n \n   cond = gen_reg_rtx (CCmode);\n-  x = gen_rtx_COMPARE (CCmode, x, oldval);\n+  /* If we have TImode, synthesize a comparison.  */\n+  if (mode != TImode)\n+    x = gen_rtx_COMPARE (CCmode, x, oldval);\n+  else\n+    {\n+      rtx xor1_result = gen_reg_rtx (DImode);\n+      rtx xor2_result = gen_reg_rtx (DImode);\n+      rtx or_result = gen_reg_rtx (DImode);\n+      rtx new_word0 = simplify_gen_subreg (DImode, x, TImode, 0);\n+      rtx new_word1 = simplify_gen_subreg (DImode, x, TImode, 8);\n+      rtx old_word0 = simplify_gen_subreg (DImode, oldval, TImode, 0);\n+      rtx old_word1 = simplify_gen_subreg (DImode, oldval, TImode, 8);\n+\n+      emit_insn (gen_xordi3 (xor1_result, new_word0, old_word0));\n+      emit_insn (gen_xordi3 (xor2_result, new_word1, old_word1));\n+      emit_insn (gen_iordi3 (or_result, xor1_result, xor2_result));\n+      x = gen_rtx_COMPARE (CCmode, or_result, const0_rtx);\n+    }\n+\n   emit_insn (gen_rtx_SET (VOIDmode, cond, x));\n \n   x = gen_rtx_NE (VOIDmode, cond, const0_rtx);\n@@ -17997,7 +18046,7 @@ rs6000_expand_atomic_compare_and_swap (rtx operands[])\n   if (mask)\n     x = rs6000_mask_atomic_subword (retval, newval, mask);\n \n-  emit_store_conditional (mode, cond, mem, x);\n+  emit_store_conditional (orig_mode, cond, mem, x);\n \n   if (!is_weak)\n     {\n@@ -18015,6 +18064,8 @@ rs6000_expand_atomic_compare_and_swap (rtx operands[])\n \n   if (shift)\n     rs6000_finish_atomic_subword (operands[1], retval, shift);\n+  else if (mode != GET_MODE (operands[1]))\n+    convert_move (operands[1], retval, 1);\n \n   /* In all cases, CR0 contains EQ on success, and NE on failure.  */\n   x = gen_rtx_EQ (SImode, cond, const0_rtx);\n@@ -18038,7 +18089,7 @@ rs6000_expand_atomic_exchange (rtx operands[])\n   mode = GET_MODE (mem);\n \n   mask = shift = NULL_RTX;\n-  if (mode == QImode || mode == HImode)\n+  if (!TARGET_SYNC_HI_QI && (mode == QImode || mode == HImode))\n     {\n       mem = rs6000_adjust_atomic_subword (mem, &shift, &mask);\n \n@@ -18087,53 +18138,70 @@ rs6000_expand_atomic_op (enum rtx_code code, rtx mem, rtx val,\n {\n   enum memmodel model = (enum memmodel) INTVAL (model_rtx);\n   enum machine_mode mode = GET_MODE (mem);\n+  enum machine_mode store_mode = mode;\n   rtx label, x, cond, mask, shift;\n   rtx before = orig_before, after = orig_after;\n \n   mask = shift = NULL_RTX;\n+  /* On power8, we want to use SImode for the operation.  On previous systems,\n+     use the operation in a subword and shift/mask to get the proper byte or\n+     halfword.  */\n   if (mode == QImode || mode == HImode)\n     {\n-      mem = rs6000_adjust_atomic_subword (mem, &shift, &mask);\n-\n-      /* Shift and mask VAL into position with the word.  */\n-      val = convert_modes (SImode, mode, val, 1);\n-      val = expand_simple_binop (SImode, ASHIFT, val, shift,\n-\t\t\t\t NULL_RTX, 1, OPTAB_LIB_WIDEN);\n+      if (TARGET_SYNC_HI_QI)\n+\t{\n+\t  val = convert_modes (SImode, mode, val, 1);\n \n-      switch (code)\n+\t  /* Prepare to adjust the return value.  */\n+\t  before = gen_reg_rtx (SImode);\n+\t  if (after)\n+\t    after = gen_reg_rtx (SImode);\n+\t  mode = SImode;\n+\t}\n+      else\n \t{\n-\tcase IOR:\n-\tcase XOR:\n-\t  /* We've already zero-extended VAL.  That is sufficient to\n-\t     make certain that it does not affect other bits.  */\n-\t  mask = NULL;\n-\t  break;\n+\t  mem = rs6000_adjust_atomic_subword (mem, &shift, &mask);\n \n-\tcase AND:\n-\t  /* If we make certain that all of the other bits in VAL are\n-\t     set, that will be sufficient to not affect other bits.  */\n-\t  x = gen_rtx_NOT (SImode, mask);\n-\t  x = gen_rtx_IOR (SImode, x, val);\n-\t  emit_insn (gen_rtx_SET (VOIDmode, val, x));\n-\t  mask = NULL;\n-\t  break;\n+\t  /* Shift and mask VAL into position with the word.  */\n+\t  val = convert_modes (SImode, mode, val, 1);\n+\t  val = expand_simple_binop (SImode, ASHIFT, val, shift,\n+\t\t\t\t     NULL_RTX, 1, OPTAB_LIB_WIDEN);\n \n-\tcase NOT:\n-\tcase PLUS:\n-\tcase MINUS:\n-\t  /* These will all affect bits outside the field and need\n-\t     adjustment via MASK within the loop.  */\n-\t  break;\n+\t  switch (code)\n+\t    {\n+\t    case IOR:\n+\t    case XOR:\n+\t      /* We've already zero-extended VAL.  That is sufficient to\n+\t\t make certain that it does not affect other bits.  */\n+\t      mask = NULL;\n+\t      break;\n \n-\tdefault:\n-\t  gcc_unreachable ();\n-\t}\n+\t    case AND:\n+\t      /* If we make certain that all of the other bits in VAL are\n+\t\t set, that will be sufficient to not affect other bits.  */\n+\t      x = gen_rtx_NOT (SImode, mask);\n+\t      x = gen_rtx_IOR (SImode, x, val);\n+\t      emit_insn (gen_rtx_SET (VOIDmode, val, x));\n+\t      mask = NULL;\n+\t      break;\n \n-      /* Prepare to adjust the return value.  */\n-      before = gen_reg_rtx (SImode);\n-      if (after)\n-\tafter = gen_reg_rtx (SImode);\n-      mode = SImode;\n+\t    case NOT:\n+\t    case PLUS:\n+\t    case MINUS:\n+\t      /* These will all affect bits outside the field and need\n+\t\t adjustment via MASK within the loop.  */\n+\t      break;\n+\n+\t    default:\n+\t      gcc_unreachable ();\n+\t    }\n+\n+\t  /* Prepare to adjust the return value.  */\n+\t  before = gen_reg_rtx (SImode);\n+\t  if (after)\n+\t    after = gen_reg_rtx (SImode);\n+\t  store_mode = mode = SImode;\n+\t}\n     }\n \n   mem = rs6000_pre_atomic_barrier (mem, model);\n@@ -18166,9 +18234,11 @@ rs6000_expand_atomic_op (enum rtx_code code, rtx mem, rtx val,\n \t\t\t       NULL_RTX, 1, OPTAB_LIB_WIDEN);\n       x = rs6000_mask_atomic_subword (before, x, mask);\n     }\n+  else if (store_mode != mode)\n+    x = convert_modes (store_mode, mode, x, 1);\n \n   cond = gen_reg_rtx (CCmode);\n-  emit_store_conditional (mode, cond, mem, x);\n+  emit_store_conditional (store_mode, cond, mem, x);\n \n   x = gen_rtx_NE (VOIDmode, cond, const0_rtx);\n   emit_unlikely_jump (x, label);\n@@ -18177,11 +18247,22 @@ rs6000_expand_atomic_op (enum rtx_code code, rtx mem, rtx val,\n \n   if (shift)\n     {\n+      /* QImode/HImode on machines without lbarx/lharx where we do a lwarx and\n+\t then do the calcuations in a SImode register.  */\n       if (orig_before)\n \trs6000_finish_atomic_subword (orig_before, before, shift);\n       if (orig_after)\n \trs6000_finish_atomic_subword (orig_after, after, shift);\n     }\n+  else if (store_mode != mode)\n+    {\n+      /* QImode/HImode on machines with lbarx/lharx where we do the native\n+\t operation and then do the calcuations in a SImode register.  */\n+      if (orig_before)\n+\tconvert_move (orig_before, before, 1);\n+      if (orig_after)\n+\tconvert_move (orig_after, after, 1);\n+    }\n   else if (orig_after && after != orig_after)\n     emit_move_insn (orig_after, after);\n }"}, {"sha": "8be86a4c15c37be7f4c4f7f7d126cbd8d6fdf2e2", "filename": "gcc/config/rs6000/rs6000.md", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Fconfig%2Frs6000%2Frs6000.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Fconfig%2Frs6000%2Frs6000.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.md?ref=4b02c96265fb52f9f1b19a6fcb350693b52f4135", "patch": "@@ -239,6 +239,12 @@\n ; extend modes for DImode\n (define_mode_iterator QHSI [QI HI SI])\n \n+; QImode or HImode for small atomic ops\n+(define_mode_iterator QHI [QI HI])\n+\n+; HImode or SImode for sign extended fusion ops\n+(define_mode_iterator HSI [HI SI])\n+\n ; SImode or DImode, even if DImode doesn't fit in GPRs.\n (define_mode_iterator SDI [SI DI])\n "}, {"sha": "8616b3eca5f5cbbbd0c95083544c63101e676355", "filename": "gcc/config/rs6000/sync.md", "status": "modified", "additions": 120, "deletions": 46, "changes": 166, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Fconfig%2Frs6000%2Fsync.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Fconfig%2Frs6000%2Fsync.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fsync.md?ref=4b02c96265fb52f9f1b19a6fcb350693b52f4135", "patch": "@@ -18,14 +18,23 @@\n ;; along with GCC; see the file COPYING3.  If not see\n ;; <http://www.gnu.org/licenses/>.\n \n-(define_mode_attr larx [(SI \"lwarx\") (DI \"ldarx\")])\n-(define_mode_attr stcx [(SI \"stwcx.\") (DI \"stdcx.\")])\n+(define_mode_attr larx [(QI \"lbarx\")\n+\t\t\t(HI \"lharx\")\n+\t\t\t(SI \"lwarx\")\n+\t\t\t(DI \"ldarx\")\n+\t\t\t(TI \"lqarx\")])\n+\n+(define_mode_attr stcx [(QI \"stbcx.\")\n+\t\t\t(HI \"sthcx.\")\n+\t\t\t(SI \"stwcx.\")\n+\t\t\t(DI \"stdcx.\")\n+\t\t\t(TI \"stqcx.\")])\n \n (define_code_iterator FETCHOP [plus minus ior xor and])\n (define_code_attr fetchop_name\n   [(plus \"add\") (minus \"sub\") (ior \"or\") (xor \"xor\") (and \"and\")])\n (define_code_attr fetchop_pred\n-  [(plus \"add_operand\") (minus \"gpc_reg_operand\")\n+  [(plus \"add_operand\") (minus \"int_reg_operand\")\n    (ior \"logical_operand\") (xor \"logical_operand\") (and \"and_operand\")])\n \n (define_expand \"mem_thread_fence\"\n@@ -129,16 +138,7 @@\n     case MEMMODEL_CONSUME:\n     case MEMMODEL_ACQUIRE:\n     case MEMMODEL_SEQ_CST:\n-      if (GET_MODE (operands[0]) == QImode)\n-\temit_insn (gen_loadsync_qi (operands[0]));\n-      else if (GET_MODE (operands[0]) == HImode)\n-\temit_insn (gen_loadsync_hi (operands[0]));\n-      else if (GET_MODE (operands[0]) == SImode)\n-\temit_insn (gen_loadsync_si (operands[0]));\n-      else if (GET_MODE (operands[0]) == DImode)\n-\temit_insn (gen_loadsync_di (operands[0]));\n-      else\n-\tgcc_unreachable ();\n+      emit_insn (gen_loadsync_<mode> (operands[0]));\n       break;\n     default:\n       gcc_unreachable ();\n@@ -170,35 +170,109 @@\n   DONE;\n })\n \n-;; ??? Power ISA 2.06B says that there *is* a load-{byte,half}-and-reserve\n-;; opcode that is \"phased-in\".  Not implemented as of Power7, so not yet used,\n-;; but let's prepare the macros anyway.\n+;; Any supported integer mode that has atomic l<x>arx/st<x>cx. instrucitons\n+;; other than the quad memory operations, which have special restrictions.\n+;; Byte/halfword atomic instructions were added in ISA 2.06B, but were phased\n+;; in and did not show up until power8.  TImode atomic lqarx/stqcx. require\n+;; special handling due to even/odd register requirements.\n+(define_mode_iterator ATOMIC [(QI \"TARGET_SYNC_HI_QI\")\n+\t\t\t      (HI \"TARGET_SYNC_HI_QI\")\n+\t\t\t      SI\n+\t\t\t      (DI \"TARGET_POWERPC64\")])\n+\n+;; Types that we should provide atomic instructions for.\n \n-(define_mode_iterator ATOMIC    [SI (DI \"TARGET_POWERPC64\")])\n+(define_mode_iterator AINT [QI\n+\t\t\t    HI\n+\t\t\t    SI\n+\t\t\t    (DI \"TARGET_POWERPC64\")\n+\t\t\t    (TI \"TARGET_SYNC_TI\")])\n \n (define_insn \"load_locked<mode>\"\n-  [(set (match_operand:ATOMIC 0 \"gpc_reg_operand\" \"=r\")\n+  [(set (match_operand:ATOMIC 0 \"int_reg_operand\" \"=r\")\n \t(unspec_volatile:ATOMIC\n          [(match_operand:ATOMIC 1 \"memory_operand\" \"Z\")] UNSPECV_LL))]\n   \"\"\n   \"<larx> %0,%y1\"\n   [(set_attr \"type\" \"load_l\")])\n \n+(define_insn \"load_locked<QHI:mode>_si\"\n+  [(set (match_operand:SI 0 \"int_reg_operand\" \"=r\")\n+\t(unspec_volatile:SI\n+\t  [(match_operand:QHI 1 \"memory_operand\" \"Z\")] UNSPECV_LL))]\n+  \"TARGET_SYNC_HI_QI\"\n+  \"<QHI:larx> %0,%y1\"\n+  [(set_attr \"type\" \"load_l\")])\n+\n+;; Use PTImode to get even/odd register pairs\n+(define_expand \"load_lockedti\"\n+  [(use (match_operand:TI 0 \"quad_int_reg_operand\" \"\"))\n+   (use (match_operand:TI 1 \"memory_operand\" \"\"))]\n+  \"TARGET_SYNC_TI\"\n+{\n+  /* Use a temporary register to force getting an even register for the\n+     lqarx/stqcrx. instructions.  Normal optimizations will eliminate this\n+     extra copy.  */\n+  rtx pti = gen_reg_rtx (PTImode);\n+  emit_insn (gen_load_lockedpti (pti, operands[1]));\n+  emit_move_insn (operands[0], gen_lowpart (TImode, pti));\n+  DONE;\n+})\n+\n+(define_insn \"load_lockedpti\"\n+  [(set (match_operand:PTI 0 \"quad_int_reg_operand\" \"=&r\")\n+\t(unspec_volatile:PTI\n+         [(match_operand:TI 1 \"memory_operand\" \"Z\")] UNSPECV_LL))]\n+  \"TARGET_SYNC_TI\n+   && !reg_mentioned_p (operands[0], operands[1])\n+   && quad_int_reg_operand (operands[0], PTImode)\"\n+  \"lqarx %0,%y1\"\n+  [(set_attr \"type\" \"load_l\")])\n+\n (define_insn \"store_conditional<mode>\"\n   [(set (match_operand:CC 0 \"cc_reg_operand\" \"=x\")\n \t(unspec_volatile:CC [(const_int 0)] UNSPECV_SC))\n    (set (match_operand:ATOMIC 1 \"memory_operand\" \"=Z\")\n-\t(match_operand:ATOMIC 2 \"gpc_reg_operand\" \"r\"))]\n+\t(match_operand:ATOMIC 2 \"int_reg_operand\" \"r\"))]\n   \"\"\n   \"<stcx> %2,%y1\"\n   [(set_attr \"type\" \"store_c\")])\n \n+(define_expand \"store_conditionalti\"\n+  [(use (match_operand:CC 0 \"cc_reg_operand\" \"\"))\n+   (use (match_operand:TI 1 \"memory_operand\" \"\"))\n+   (use (match_operand:TI 2 \"quad_int_reg_operand\" \"\"))]\n+  \"TARGET_SYNC_TI\"\n+{\n+  rtx op0 = operands[0];\n+  rtx op1 = operands[1];\n+  rtx op2 = operands[2];\n+  rtx pti_op1 = change_address (op1, PTImode, XEXP (op1, 0));\n+  rtx pti_op2 = gen_reg_rtx (PTImode);\n+\n+  /* Use a temporary register to force getting an even register for the\n+     lqarx/stqcrx. instructions.  Normal optimizations will eliminate this\n+     extra copy.  */\n+  emit_move_insn (pti_op2, gen_lowpart (PTImode, op2));\n+  emit_insn (gen_store_conditionalpti (op0, pti_op1, pti_op2));\n+  DONE;\n+})\n+\n+(define_insn \"store_conditionalpti\"\n+  [(set (match_operand:CC 0 \"cc_reg_operand\" \"=x\")\n+\t(unspec_volatile:CC [(const_int 0)] UNSPECV_SC))\n+   (set (match_operand:PTI 1 \"memory_operand\" \"=Z\")\n+\t(match_operand:PTI 2 \"quad_int_reg_operand\" \"r\"))]\n+  \"TARGET_SYNC_TI && quad_int_reg_operand (operands[2], PTImode)\"\n+  \"stqcx. %2,%y1\"\n+  [(set_attr \"type\" \"store_c\")])\n+\n (define_expand \"atomic_compare_and_swap<mode>\"\n-  [(match_operand:SI 0 \"gpc_reg_operand\" \"\")\t\t;; bool out\n-   (match_operand:INT1 1 \"gpc_reg_operand\" \"\")\t\t;; val out\n-   (match_operand:INT1 2 \"memory_operand\" \"\")\t\t;; memory\n-   (match_operand:INT1 3 \"reg_or_short_operand\" \"\")\t;; expected\n-   (match_operand:INT1 4 \"gpc_reg_operand\" \"\")\t\t;; desired\n+  [(match_operand:SI 0 \"int_reg_operand\" \"\")\t\t;; bool out\n+   (match_operand:AINT 1 \"int_reg_operand\" \"\")\t\t;; val out\n+   (match_operand:AINT 2 \"memory_operand\" \"\")\t\t;; memory\n+   (match_operand:AINT 3 \"reg_or_short_operand\" \"\")\t;; expected\n+   (match_operand:AINT 4 \"int_reg_operand\" \"\")\t\t;; desired\n    (match_operand:SI 5 \"const_int_operand\" \"\")\t\t;; is_weak\n    (match_operand:SI 6 \"const_int_operand\" \"\")\t\t;; model succ\n    (match_operand:SI 7 \"const_int_operand\" \"\")]\t\t;; model fail\n@@ -209,9 +283,9 @@\n })\n \n (define_expand \"atomic_exchange<mode>\"\n-  [(match_operand:INT1 0 \"gpc_reg_operand\" \"\")\t\t;; output\n-   (match_operand:INT1 1 \"memory_operand\" \"\")\t\t;; memory\n-   (match_operand:INT1 2 \"gpc_reg_operand\" \"\")\t\t;; input\n+  [(match_operand:AINT 0 \"int_reg_operand\" \"\")\t\t;; output\n+   (match_operand:AINT 1 \"memory_operand\" \"\")\t\t;; memory\n+   (match_operand:AINT 2 \"int_reg_operand\" \"\")\t\t;; input\n    (match_operand:SI 3 \"const_int_operand\" \"\")]\t\t;; model\n   \"\"\n {\n@@ -220,9 +294,9 @@\n })\n \n (define_expand \"atomic_<fetchop_name><mode>\"\n-  [(match_operand:INT1 0 \"memory_operand\" \"\")\t\t;; memory\n-   (FETCHOP:INT1 (match_dup 0)\n-     (match_operand:INT1 1 \"<fetchop_pred>\" \"\"))\t;; operand\n+  [(match_operand:AINT 0 \"memory_operand\" \"\")\t\t;; memory\n+   (FETCHOP:AINT (match_dup 0)\n+     (match_operand:AINT 1 \"<fetchop_pred>\" \"\"))\t;; operand\n    (match_operand:SI 2 \"const_int_operand\" \"\")]\t\t;; model\n   \"\"\n {\n@@ -232,8 +306,8 @@\n })\n \n (define_expand \"atomic_nand<mode>\"\n-  [(match_operand:INT1 0 \"memory_operand\" \"\")\t\t;; memory\n-   (match_operand:INT1 1 \"gpc_reg_operand\" \"\")\t\t;; operand\n+  [(match_operand:AINT 0 \"memory_operand\" \"\")\t\t;; memory\n+   (match_operand:AINT 1 \"int_reg_operand\" \"\")\t\t;; operand\n    (match_operand:SI 2 \"const_int_operand\" \"\")]\t\t;; model\n   \"\"\n {\n@@ -243,10 +317,10 @@\n })\n \n (define_expand \"atomic_fetch_<fetchop_name><mode>\"\n-  [(match_operand:INT1 0 \"gpc_reg_operand\" \"\")\t\t;; output\n-   (match_operand:INT1 1 \"memory_operand\" \"\")\t\t;; memory\n-   (FETCHOP:INT1 (match_dup 1)\n-     (match_operand:INT1 2 \"<fetchop_pred>\" \"\"))\t;; operand\n+  [(match_operand:AINT 0 \"int_reg_operand\" \"\")\t\t;; output\n+   (match_operand:AINT 1 \"memory_operand\" \"\")\t\t;; memory\n+   (FETCHOP:AINT (match_dup 1)\n+     (match_operand:AINT 2 \"<fetchop_pred>\" \"\"))\t;; operand\n    (match_operand:SI 3 \"const_int_operand\" \"\")]\t\t;; model\n   \"\"\n { \n@@ -256,9 +330,9 @@\n })\n \n (define_expand \"atomic_fetch_nand<mode>\"\n-  [(match_operand:INT1 0 \"gpc_reg_operand\" \"\")\t\t;; output\n-   (match_operand:INT1 1 \"memory_operand\" \"\")\t\t;; memory\n-   (match_operand:INT1 2 \"gpc_reg_operand\" \"\")\t\t;; operand\n+  [(match_operand:AINT 0 \"int_reg_operand\" \"\")\t\t;; output\n+   (match_operand:AINT 1 \"memory_operand\" \"\")\t\t;; memory\n+   (match_operand:AINT 2 \"int_reg_operand\" \"\")\t\t;; operand\n    (match_operand:SI 3 \"const_int_operand\" \"\")]\t\t;; model\n   \"\"\n {\n@@ -268,10 +342,10 @@\n })\n \n (define_expand \"atomic_<fetchop_name>_fetch<mode>\"\n-  [(match_operand:INT1 0 \"gpc_reg_operand\" \"\")\t\t;; output\n-   (match_operand:INT1 1 \"memory_operand\" \"\")\t\t;; memory\n-   (FETCHOP:INT1 (match_dup 1)\n-     (match_operand:INT1 2 \"<fetchop_pred>\" \"\"))\t;; operand\n+  [(match_operand:AINT 0 \"int_reg_operand\" \"\")\t\t;; output\n+   (match_operand:AINT 1 \"memory_operand\" \"\")\t\t;; memory\n+   (FETCHOP:AINT (match_dup 1)\n+     (match_operand:AINT 2 \"<fetchop_pred>\" \"\"))\t;; operand\n    (match_operand:SI 3 \"const_int_operand\" \"\")]\t\t;; model\n   \"\"\n {\n@@ -281,9 +355,9 @@\n })\n \n (define_expand \"atomic_nand_fetch<mode>\"\n-  [(match_operand:INT1 0 \"gpc_reg_operand\" \"\")\t\t;; output\n-   (match_operand:INT1 1 \"memory_operand\" \"\")\t\t;; memory\n-   (match_operand:INT1 2 \"gpc_reg_operand\" \"\")\t\t;; operand\n+  [(match_operand:AINT 0 \"int_reg_operand\" \"\")\t\t;; output\n+   (match_operand:AINT 1 \"memory_operand\" \"\")\t\t;; memory\n+   (match_operand:AINT 2 \"int_reg_operand\" \"\")\t\t;; operand\n    (match_operand:SI 3 \"const_int_operand\" \"\")]\t\t;; model\n   \"\"\n {"}, {"sha": "3e1fbdbed856259a74b0d7855f973bb46d2007cf", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=4b02c96265fb52f9f1b19a6fcb350693b52f4135", "patch": "@@ -1,3 +1,11 @@\n+2013-06-12  Michael Meissner  <meissner@linux.vnet.ibm.com>\n+\t    Pat Haugen <pthaugen@us.ibm.com>\n+\t    Peter Bergner <bergner@vnet.ibm.com>\n+\n+\t* gcc.target/powerpc/atomic-p7.c: New file, add tests for atomic\n+\tload/store instructions on power7, power8.\n+\t* gcc.target/powerpc/atomic-p8.c: Likewise.\n+\n 2013-06-12  Balaji V. Iyer  <balaji.v.iyer@intel.com>\n \n \tPR c/57577"}, {"sha": "3442bfba4cdba6801e9feb44ff2974b773bec754", "filename": "gcc/testsuite/gcc.target/powerpc/atomic-p7.c", "status": "added", "additions": 207, "deletions": 0, "changes": 207, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fatomic-p7.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fatomic-p7.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fatomic-p7.c?ref=4b02c96265fb52f9f1b19a6fcb350693b52f4135", "patch": "@@ -0,0 +1,207 @@\n+/* { dg-do compile { target { powerpc*-*-* && lp64 } } } */\n+/* { dg-skip-if \"\" { powerpc*-*-darwin* } { \"*\" } { \"\" } } */\n+/* { dg-require-effective-target powerpc_vsx_ok } */\n+/* { dg-options \"-mcpu=power7 -O2\" } */\n+/* { dg-final { scan-assembler-not \"lbarx\" } } */\n+/* { dg-final { scan-assembler-not \"lharx\" } } */\n+/* { dg-final { scan-assembler-times \"lwarx\" 18 } } */\n+/* { dg-final { scan-assembler-times \"ldarx\" 6 } } */\n+/* { dg-final { scan-assembler-not \"lqarx\" } } */\n+/* { dg-final { scan-assembler-not \"stbcx\" } } */\n+/* { dg-final { scan-assembler-not \"sthcx\" } } */\n+/* { dg-final { scan-assembler-times \"stwcx\" 18 } } */\n+/* { dg-final { scan-assembler-times \"stdcx\" 6 } } */\n+/* { dg-final { scan-assembler-not \"stqcx\" } } */\n+/* { dg-final { scan-assembler-times \"bl __atomic\" 6 } } */\n+/* { dg-final { scan-assembler-times \"isync\" 12 } } */\n+/* { dg-final { scan-assembler-times \"lwsync\" 8 } } */\n+/* { dg-final { scan-assembler-not \"mtvsrd\" } } */\n+/* { dg-final { scan-assembler-not \"mtvsrwa\" } } */\n+/* { dg-final { scan-assembler-not \"mtvsrwz\" } } */\n+/* { dg-final { scan-assembler-not \"mfvsrd\" } } */\n+/* { dg-final { scan-assembler-not \"mfvsrwz\" } } */\n+\n+/* Test for the byte atomic operations on power8 using lbarx/stbcx.  */\n+char\n+char_fetch_add_relaxed (char *ptr, int value)\n+{\n+  return __atomic_fetch_add (ptr, value, __ATOMIC_RELAXED);\n+}\n+\n+char\n+char_fetch_sub_consume (char *ptr, int value)\n+{\n+  return __atomic_fetch_sub (ptr, value, __ATOMIC_CONSUME);\n+}\n+\n+char\n+char_fetch_and_acquire (char *ptr, int value)\n+{\n+  return __atomic_fetch_and (ptr, value, __ATOMIC_ACQUIRE);\n+}\n+\n+char\n+char_fetch_ior_release (char *ptr, int value)\n+{\n+  return __atomic_fetch_or (ptr, value, __ATOMIC_RELEASE);\n+}\n+\n+char\n+char_fetch_xor_acq_rel (char *ptr, int value)\n+{\n+  return __atomic_fetch_xor (ptr, value, __ATOMIC_ACQ_REL);\n+}\n+\n+char\n+char_fetch_nand_seq_cst (char *ptr, int value)\n+{\n+  return __atomic_fetch_nand (ptr, value, __ATOMIC_SEQ_CST);\n+}\n+\n+/* Test for the half word atomic operations on power8 using lharx/sthcx.  */\n+short\n+short_fetch_add_relaxed (short *ptr, int value)\n+{\n+  return __atomic_fetch_add (ptr, value, __ATOMIC_RELAXED);\n+}\n+\n+short\n+short_fetch_sub_consume (short *ptr, int value)\n+{\n+  return __atomic_fetch_sub (ptr, value, __ATOMIC_CONSUME);\n+}\n+\n+short\n+short_fetch_and_acquire (short *ptr, int value)\n+{\n+  return __atomic_fetch_and (ptr, value, __ATOMIC_ACQUIRE);\n+}\n+\n+short\n+short_fetch_ior_release (short *ptr, int value)\n+{\n+  return __atomic_fetch_or (ptr, value, __ATOMIC_RELEASE);\n+}\n+\n+short\n+short_fetch_xor_acq_rel (short *ptr, int value)\n+{\n+  return __atomic_fetch_xor (ptr, value, __ATOMIC_ACQ_REL);\n+}\n+\n+short\n+short_fetch_nand_seq_cst (short *ptr, int value)\n+{\n+  return __atomic_fetch_nand (ptr, value, __ATOMIC_SEQ_CST);\n+}\n+\n+/* Test for the word atomic operations on power8 using lwarx/stwcx.  */\n+int\n+int_fetch_add_relaxed (int *ptr, int value)\n+{\n+  return __atomic_fetch_add (ptr, value, __ATOMIC_RELAXED);\n+}\n+\n+int\n+int_fetch_sub_consume (int *ptr, int value)\n+{\n+  return __atomic_fetch_sub (ptr, value, __ATOMIC_CONSUME);\n+}\n+\n+int\n+int_fetch_and_acquire (int *ptr, int value)\n+{\n+  return __atomic_fetch_and (ptr, value, __ATOMIC_ACQUIRE);\n+}\n+\n+int\n+int_fetch_ior_release (int *ptr, int value)\n+{\n+  return __atomic_fetch_or (ptr, value, __ATOMIC_RELEASE);\n+}\n+\n+int\n+int_fetch_xor_acq_rel (int *ptr, int value)\n+{\n+  return __atomic_fetch_xor (ptr, value, __ATOMIC_ACQ_REL);\n+}\n+\n+int\n+int_fetch_nand_seq_cst (int *ptr, int value)\n+{\n+  return __atomic_fetch_nand (ptr, value, __ATOMIC_SEQ_CST);\n+}\n+\n+/* Test for the double word atomic operations on power8 using ldarx/stdcx.  */\n+long\n+long_fetch_add_relaxed (long *ptr, long value)\n+{\n+  return __atomic_fetch_add (ptr, value, __ATOMIC_RELAXED);\n+}\n+\n+long\n+long_fetch_sub_consume (long *ptr, long value)\n+{\n+  return __atomic_fetch_sub (ptr, value, __ATOMIC_CONSUME);\n+}\n+\n+long\n+long_fetch_and_acquire (long *ptr, long value)\n+{\n+  return __atomic_fetch_and (ptr, value, __ATOMIC_ACQUIRE);\n+}\n+\n+long\n+long_fetch_ior_release (long *ptr, long value)\n+{\n+  return __atomic_fetch_or (ptr, value, __ATOMIC_RELEASE);\n+}\n+\n+long\n+long_fetch_xor_acq_rel (long *ptr, long value)\n+{\n+  return __atomic_fetch_xor (ptr, value, __ATOMIC_ACQ_REL);\n+}\n+\n+long\n+long_fetch_nand_seq_cst (long *ptr, long value)\n+{\n+  return __atomic_fetch_nand (ptr, value, __ATOMIC_SEQ_CST);\n+}\n+\n+/* Test for the quad word atomic operations on power8 using ldarx/stdcx.  */\n+__int128_t\n+quad_fetch_add_relaxed (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_add (ptr, value, __ATOMIC_RELAXED);\n+}\n+\n+__int128_t\n+quad_fetch_sub_consume (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_sub (ptr, value, __ATOMIC_CONSUME);\n+}\n+\n+__int128_t\n+quad_fetch_and_acquire (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_and (ptr, value, __ATOMIC_ACQUIRE);\n+}\n+\n+__int128_t\n+quad_fetch_ior_release (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_or (ptr, value, __ATOMIC_RELEASE);\n+}\n+\n+__int128_t\n+quad_fetch_xor_acq_rel (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_xor (ptr, value, __ATOMIC_ACQ_REL);\n+}\n+\n+__int128_t\n+quad_fetch_nand_seq_cst (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_nand (ptr, value, __ATOMIC_SEQ_CST);\n+}"}, {"sha": "17460ac4c73a096074e84b99149c08af15c98e40", "filename": "gcc/testsuite/gcc.target/powerpc/atomic-p8.c", "status": "added", "additions": 237, "deletions": 0, "changes": 237, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fatomic-p8.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b02c96265fb52f9f1b19a6fcb350693b52f4135/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fatomic-p8.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fatomic-p8.c?ref=4b02c96265fb52f9f1b19a6fcb350693b52f4135", "patch": "@@ -0,0 +1,237 @@\n+/* { dg-do compile { target { powerpc*-*-* && lp64 } } } */\n+/* { dg-skip-if \"\" { powerpc*-*-darwin* } { \"*\" } { \"\" } } */\n+/* { dg-require-effective-target powerpc_p8vector_ok } */\n+/* { dg-options \"-mcpu=power8 -O2\" } */\n+/* { dg-final { scan-assembler-times \"lbarx\" 7 } } */\n+/* { dg-final { scan-assembler-times \"lharx\" 7 } } */\n+/* { dg-final { scan-assembler-times \"lwarx\" 7 } } */\n+/* { dg-final { scan-assembler-times \"ldarx\" 7 } } */\n+/* { dg-final { scan-assembler-times \"lqarx\" 7 } } */\n+/* { dg-final { scan-assembler-times \"stbcx\" 7 } } */\n+/* { dg-final { scan-assembler-times \"sthcx\" 7 } } */\n+/* { dg-final { scan-assembler-times \"stwcx\" 7 } } */\n+/* { dg-final { scan-assembler-times \"stdcx\" 7 } } */\n+/* { dg-final { scan-assembler-times \"stqcx\" 7 } } */\n+/* { dg-final { scan-assembler-not \"bl __atomic\" } } */\n+/* { dg-final { scan-assembler-times \"isync\" 20 } } */\n+/* { dg-final { scan-assembler-times \"lwsync\" 10 } } */\n+/* { dg-final { scan-assembler-not \"mtvsrd\" } } */\n+/* { dg-final { scan-assembler-not \"mtvsrwa\" } } */\n+/* { dg-final { scan-assembler-not \"mtvsrwz\" } } */\n+/* { dg-final { scan-assembler-not \"mfvsrd\" } } */\n+/* { dg-final { scan-assembler-not \"mfvsrwz\" } } */\n+\n+/* Test for the byte atomic operations on power8 using lbarx/stbcx.  */\n+char\n+char_fetch_add_relaxed (char *ptr, int value)\n+{\n+  return __atomic_fetch_add (ptr, value, __ATOMIC_RELAXED);\n+}\n+\n+char\n+char_fetch_sub_consume (char *ptr, int value)\n+{\n+  return __atomic_fetch_sub (ptr, value, __ATOMIC_CONSUME);\n+}\n+\n+char\n+char_fetch_and_acquire (char *ptr, int value)\n+{\n+  return __atomic_fetch_and (ptr, value, __ATOMIC_ACQUIRE);\n+}\n+\n+char\n+char_fetch_ior_release (char *ptr, int value)\n+{\n+  return __atomic_fetch_or (ptr, value, __ATOMIC_RELEASE);\n+}\n+\n+char\n+char_fetch_xor_acq_rel (char *ptr, int value)\n+{\n+  return __atomic_fetch_xor (ptr, value, __ATOMIC_ACQ_REL);\n+}\n+\n+char\n+char_fetch_nand_seq_cst (char *ptr, int value)\n+{\n+  return __atomic_fetch_nand (ptr, value, __ATOMIC_SEQ_CST);\n+}\n+\n+void\n+char_val_compare_and_swap (char *p, int i, int j, char *q)\n+{\n+  *q = __sync_val_compare_and_swap (p, i, j);\n+}\n+\n+/* Test for the half word atomic operations on power8 using lharx/sthcx.  */\n+short\n+short_fetch_add_relaxed (short *ptr, int value)\n+{\n+  return __atomic_fetch_add (ptr, value, __ATOMIC_RELAXED);\n+}\n+\n+short\n+short_fetch_sub_consume (short *ptr, int value)\n+{\n+  return __atomic_fetch_sub (ptr, value, __ATOMIC_CONSUME);\n+}\n+\n+short\n+short_fetch_and_acquire (short *ptr, int value)\n+{\n+  return __atomic_fetch_and (ptr, value, __ATOMIC_ACQUIRE);\n+}\n+\n+short\n+short_fetch_ior_release (short *ptr, int value)\n+{\n+  return __atomic_fetch_or (ptr, value, __ATOMIC_RELEASE);\n+}\n+\n+short\n+short_fetch_xor_acq_rel (short *ptr, int value)\n+{\n+  return __atomic_fetch_xor (ptr, value, __ATOMIC_ACQ_REL);\n+}\n+\n+short\n+short_fetch_nand_seq_cst (short *ptr, int value)\n+{\n+  return __atomic_fetch_nand (ptr, value, __ATOMIC_SEQ_CST);\n+}\n+\n+void\n+short_val_compare_and_swap (short *p, int i, int j, short *q)\n+{\n+  *q = __sync_val_compare_and_swap (p, i, j);\n+}\n+\n+/* Test for the word atomic operations on power8 using lwarx/stwcx.  */\n+int\n+int_fetch_add_relaxed (int *ptr, int value)\n+{\n+  return __atomic_fetch_add (ptr, value, __ATOMIC_RELAXED);\n+}\n+\n+int\n+int_fetch_sub_consume (int *ptr, int value)\n+{\n+  return __atomic_fetch_sub (ptr, value, __ATOMIC_CONSUME);\n+}\n+\n+int\n+int_fetch_and_acquire (int *ptr, int value)\n+{\n+  return __atomic_fetch_and (ptr, value, __ATOMIC_ACQUIRE);\n+}\n+\n+int\n+int_fetch_ior_release (int *ptr, int value)\n+{\n+  return __atomic_fetch_or (ptr, value, __ATOMIC_RELEASE);\n+}\n+\n+int\n+int_fetch_xor_acq_rel (int *ptr, int value)\n+{\n+  return __atomic_fetch_xor (ptr, value, __ATOMIC_ACQ_REL);\n+}\n+\n+int\n+int_fetch_nand_seq_cst (int *ptr, int value)\n+{\n+  return __atomic_fetch_nand (ptr, value, __ATOMIC_SEQ_CST);\n+}\n+\n+void\n+int_val_compare_and_swap (int *p, int i, int j, int *q)\n+{\n+  *q = __sync_val_compare_and_swap (p, i, j);\n+}\n+\n+/* Test for the double word atomic operations on power8 using ldarx/stdcx.  */\n+long\n+long_fetch_add_relaxed (long *ptr, long value)\n+{\n+  return __atomic_fetch_add (ptr, value, __ATOMIC_RELAXED);\n+}\n+\n+long\n+long_fetch_sub_consume (long *ptr, long value)\n+{\n+  return __atomic_fetch_sub (ptr, value, __ATOMIC_CONSUME);\n+}\n+\n+long\n+long_fetch_and_acquire (long *ptr, long value)\n+{\n+  return __atomic_fetch_and (ptr, value, __ATOMIC_ACQUIRE);\n+}\n+\n+long\n+long_fetch_ior_release (long *ptr, long value)\n+{\n+  return __atomic_fetch_or (ptr, value, __ATOMIC_RELEASE);\n+}\n+\n+long\n+long_fetch_xor_acq_rel (long *ptr, long value)\n+{\n+  return __atomic_fetch_xor (ptr, value, __ATOMIC_ACQ_REL);\n+}\n+\n+long\n+long_fetch_nand_seq_cst (long *ptr, long value)\n+{\n+  return __atomic_fetch_nand (ptr, value, __ATOMIC_SEQ_CST);\n+}\n+\n+void\n+long_val_compare_and_swap (long *p, long i, long j, long *q)\n+{\n+  *q = __sync_val_compare_and_swap (p, i, j);\n+}\n+\n+/* Test for the quad word atomic operations on power8 using ldarx/stdcx.  */\n+__int128_t\n+quad_fetch_add_relaxed (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_add (ptr, value, __ATOMIC_RELAXED);\n+}\n+\n+__int128_t\n+quad_fetch_sub_consume (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_sub (ptr, value, __ATOMIC_CONSUME);\n+}\n+\n+__int128_t\n+quad_fetch_and_acquire (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_and (ptr, value, __ATOMIC_ACQUIRE);\n+}\n+\n+__int128_t\n+quad_fetch_ior_release (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_or (ptr, value, __ATOMIC_RELEASE);\n+}\n+\n+__int128_t\n+quad_fetch_xor_acq_rel (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_xor (ptr, value, __ATOMIC_ACQ_REL);\n+}\n+\n+__int128_t\n+quad_fetch_nand_seq_cst (__int128_t *ptr, __int128_t value)\n+{\n+  return __atomic_fetch_nand (ptr, value, __ATOMIC_SEQ_CST);\n+}\n+\n+void\n+quad_val_compare_and_swap (__int128_t *p, __int128_t i, __int128_t j, __int128_t *q)\n+{\n+  *q = __sync_val_compare_and_swap (p, i, j);\n+}"}]}