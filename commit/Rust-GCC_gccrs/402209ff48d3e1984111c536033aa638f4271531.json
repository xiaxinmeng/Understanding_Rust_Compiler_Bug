{"sha": "402209ff48d3e1984111c536033aa638f4271531", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NDAyMjA5ZmY0OGQzZTE5ODQxMTFjNTM2MDMzYWE2MzhmNDI3MTUzMQ==", "commit": {"author": {"name": "Jan Hubicka", "email": "jh@suse.cz", "date": "2001-09-10T12:23:08Z"}, "committer": {"name": "Jan Hubicka", "email": "hubicka@gcc.gnu.org", "date": "2001-09-10T12:23:08Z"}, "message": "Makefile.in (cfg.o, [...]): New.\n\n\t* Makefile.in (cfg.o, cfganal.o, cfgloop.o, cfgbuild.o, cfgcleanup.o):\n\tNew.\n\t* basic-block.h (flow_obstack, label_value_list,\n\ttail_recursion_label_list): Declare\n\t(tidy_fallthru_edges): Declare.\n\t(expunge_block, last_loop_beg_note): Delete.\n\t(can_fallthru, flow_nodes_print, flow_edge_list_print): Declare.\n\t* cfg.c: New file\n\t(basic_block_for_insn, label_value_list): Move from flow.c; make global.\n\t(n_basic_blocks, n_edges, basic_block_info, entry_exit_blocks,\n\tinit_flow, clear_edges, can_delete_note_p, can_delete_label_p,\n\tflow_delete_insn, flow_delete_insn_chain, create_basic_block,\n\texpunge_block, flow_delete_block, compute_bb_for_insn,\n\tupdate_bb_for_insn, set_block_for_insn, set_block_for_new_insns,\n\tmake_edge, remove_edge, redirect_edge_succ, redirect_edge_succ_nodup,\n\tredirect_edge_pred, split_block, marge_blocks_nomove, block_label,\n\ttry_redirect_by_replacing_jump, last_loop_beg_note,\n\tredirect_edge_and_branch, redirect_edge_and_branch_force,\n\ttidy_fallthru_edge, tidy_fallthru_edges, back_edge_of_syntactic_loop_p,\n\tsplit_edge, insert_insn_on_edge, commit_one_edge_insertion,\n\tcommit_edge_insertions, dump_flow_info, debug_flow_info,\n\tdump_edge_info, dump_bb, debug_bb, debug_bb_n, print_rtl_with_bb,\n\tverify_flow_info, purge_dead_edges, purge_all_dead_edges):\n\tMove here from flow.c\n\t* cfganal.c: New file.\n\t(forwarder_block_p, can_fallthru, mark_critical_edges,\n\t mark_dfs_back_edges, need_fake_edge_p, flow_call_edges_add,\n\t find_unreachable_blocks, create_edge_list, free_edge_list,\n\t print_edge_list, verify_edge_list, find_edge_index, flow_nodes_print,\n\t flow_edge_list_print, remove_fake_successors, remove_fake_edges,\n\t add_noreturn_fake_exit_edges, connect_infinite_loops_to_exit,\n\t flow_reverse_top_sort_order_compute, flow_depth_first_order_compute,\n\t flow_dfs_compute_reverse_init, flow_dfs-compute_reverse_add_bb,\n\t flow_dfs-compute_reverse_execute, flow_dfs_compute_reverse_finish);\n\tMove here from flow.c\n\t* cfgbuild.c: New file\n\t(count_basic_blocks, find_label_refs, make_label_edge, make_eh_edge,\n\t make_edges, find_basic_blocks_1, find_basic_blocks,\n\t find_sub_basic_blocks): Move here from flow.c\n\t* cfgcleanup.c: New file.\n\t(try_simplify_condjump, try_forward_edges, tail_recursion_label_p,\n\t merge_blocks_move_predecessor_nojumps,\n\t merge_blocks_move_successor_nojumps, merge_blocks,\n\t flow_find_cross_jump, outgoing_edges_match, try_crossjump_to_edge,\n\t try_crossjump_bb, try_optimize_cfg): Move here from flow.c\n\t(delete_unreachable_blocks, cleanup_cfg): Likewise; return true\n\tif succeeded.\n\t* cfgloop.c: New file\n\t(flow_loops_cfg_dump, flow_loop_nested_p, flow_loop_dump,\n\t flow_loops_dump, flow_loops_free, flow_loop_entry_edges_find,\n\t flow_loop_exit_edges_find, flow_loop_nodes_find,\n\t flow_loop_pre_header_scan, flow_loop_pre_header_find,\n\t flow_loop_tree_node_add, flow_loops_tree_build,\n\t flow_loop_level_compute, flow_loops_level_compute, flow_loop_scan,\n\t flow_loops_find, flow_loops_update, flow_loop_outside_edge_p):\n\tMove here from flow.c\n\t* flow.c: Remove everything moved elsewhere\n\t* output.h (cleanup_cfg): Return bool.\n\n\t* bb-reorder.c (reorder_block_def): Remove 'index'.\n\t(insert_intra_1): Add argument BB, set block for new note.\n\t(make_reorder_chain): Do not depdent on BB indexes.\n\t(make_reorder_chain_1): Do not use BB indexes.\n\t(label_for_bb): Likewise; set BB for new insn.\n\t(emit_jump_to_block_after): Likewise.\n\t(fixup_reoder_chain): Sanity check that all basic blocks\n\tare chained; verify newly created insn chain; remove\n\tundocnitional jump simplifying; Do not use BB indexes;\n\tproperly initialize count and frequency information;\n\tdump reordered sequence.\n\t(insert_intra_bb_scope_notes): update call of insert_intra_1.\n\t(insert_inter_bb_scope_notes): Set block for new insn.\n\t(reorder_basic_blocks): Dump flow info before reoredering.\n\nFrom-SVN: r45504", "tree": {"sha": "1de90ed0fe72193706efd4b77aee818dfb646ee7", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1de90ed0fe72193706efd4b77aee818dfb646ee7"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/402209ff48d3e1984111c536033aa638f4271531", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/402209ff48d3e1984111c536033aa638f4271531", "html_url": "https://github.com/Rust-GCC/gccrs/commit/402209ff48d3e1984111c536033aa638f4271531", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/402209ff48d3e1984111c536033aa638f4271531/comments", "author": null, "committer": null, "parents": [{"sha": "5197bd5062d27d1299ca63c3e252dc7b75bc1e1f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5197bd5062d27d1299ca63c3e252dc7b75bc1e1f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5197bd5062d27d1299ca63c3e252dc7b75bc1e1f"}], "stats": {"total": 18897, "additions": 9611, "deletions": 9286}, "files": [{"sha": "98445a2f82ad10cdf7df57eedcd4322c574af9c6", "filename": "gcc/ChangeLog", "status": "modified", "additions": 76, "deletions": 0, "changes": 76, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=402209ff48d3e1984111c536033aa638f4271531", "patch": "@@ -1,3 +1,79 @@\n+Mon Sep 10 14:21:26 CEST 2001  Jan Hubicka  <jh@suse.cz>\n+\n+\t* Makefile.in (cfg.o, cfganal.o, cfgloop.o, cfgbuild.o, cfgcleanup.o):\n+\tNew.\n+\t* basic-block.h (flow_obstack, label_value_list,\n+\ttail_recursion_label_list): Declare\n+\t(tidy_fallthru_edges): Declare.\n+\t(expunge_block, last_loop_beg_note): Delete.\n+\t(can_fallthru, flow_nodes_print, flow_edge_list_print): Declare.\n+\t* cfg.c: New file\n+\t(basic_block_for_insn, label_value_list): Move from flow.c; make global.\n+\t(n_basic_blocks, n_edges, basic_block_info, entry_exit_blocks,\n+\tinit_flow, clear_edges, can_delete_note_p, can_delete_label_p,\n+\tflow_delete_insn, flow_delete_insn_chain, create_basic_block, \n+\texpunge_block, flow_delete_block, compute_bb_for_insn,\n+\tupdate_bb_for_insn, set_block_for_insn, set_block_for_new_insns,\n+\tmake_edge, remove_edge, redirect_edge_succ, redirect_edge_succ_nodup,\n+\tredirect_edge_pred, split_block, marge_blocks_nomove, block_label,\n+\ttry_redirect_by_replacing_jump, last_loop_beg_note,\n+\tredirect_edge_and_branch, redirect_edge_and_branch_force,\n+\ttidy_fallthru_edge, tidy_fallthru_edges, back_edge_of_syntactic_loop_p,\n+\tsplit_edge, insert_insn_on_edge, commit_one_edge_insertion,\n+\tcommit_edge_insertions, dump_flow_info, debug_flow_info, \n+\tdump_edge_info, dump_bb, debug_bb, debug_bb_n, print_rtl_with_bb,\n+\tverify_flow_info, purge_dead_edges, purge_all_dead_edges):\n+\tMove here from flow.c\n+\t* cfganal.c: New file.\n+\t(forwarder_block_p, can_fallthru, mark_critical_edges,\n+\t mark_dfs_back_edges, need_fake_edge_p, flow_call_edges_add,\n+\t find_unreachable_blocks, create_edge_list, free_edge_list,\n+\t print_edge_list, verify_edge_list, find_edge_index, flow_nodes_print,\n+\t flow_edge_list_print, remove_fake_successors, remove_fake_edges,\n+\t add_noreturn_fake_exit_edges, connect_infinite_loops_to_exit,\n+\t flow_reverse_top_sort_order_compute, flow_depth_first_order_compute,\n+\t flow_dfs_compute_reverse_init, flow_dfs-compute_reverse_add_bb,\n+\t flow_dfs-compute_reverse_execute, flow_dfs_compute_reverse_finish);\n+\tMove here from flow.c\n+\t* cfgbuild.c: New file\n+\t(count_basic_blocks, find_label_refs, make_label_edge, make_eh_edge,\n+\t make_edges, find_basic_blocks_1, find_basic_blocks,\n+\t find_sub_basic_blocks): Move here from flow.c\n+\t* cfgcleanup.c: New file.\n+\t(try_simplify_condjump, try_forward_edges, tail_recursion_label_p,\n+\t merge_blocks_move_predecessor_nojumps,\n+\t merge_blocks_move_successor_nojumps, merge_blocks,\n+\t flow_find_cross_jump, outgoing_edges_match, try_crossjump_to_edge,\n+\t try_crossjump_bb, try_optimize_cfg): Move here from flow.c\n+\t(delete_unreachable_blocks, cleanup_cfg): Likewise; return true\n+\tif succeeded.\n+\t* cfgloop.c: New file\n+\t(flow_loops_cfg_dump, flow_loop_nested_p, flow_loop_dump,\n+\t flow_loops_dump, flow_loops_free, flow_loop_entry_edges_find,\n+\t flow_loop_exit_edges_find, flow_loop_nodes_find,\n+\t flow_loop_pre_header_scan, flow_loop_pre_header_find,\n+\t flow_loop_tree_node_add, flow_loops_tree_build,\n+\t flow_loop_level_compute, flow_loops_level_compute, flow_loop_scan,\n+\t flow_loops_find, flow_loops_update, flow_loop_outside_edge_p):\n+\tMove here from flow.c\n+\t* flow.c: Remove everything moved elsewhere\n+\t* output.h (cleanup_cfg): Return bool.\n+\n+\t* bb-reorder.c (reorder_block_def): Remove 'index'.\n+\t(insert_intra_1): Add argument BB, set block for new note.\n+\t(make_reorder_chain): Do not depdent on BB indexes.\n+\t(make_reorder_chain_1): Do not use BB indexes.\n+\t(label_for_bb): Likewise; set BB for new insn.\n+\t(emit_jump_to_block_after): Likewise.\n+\t(fixup_reoder_chain): Sanity check that all basic blocks\n+\tare chained; verify newly created insn chain; remove\n+\tundocnitional jump simplifying; Do not use BB indexes;\n+\tproperly initialize count and frequency information;\n+\tdump reordered sequence.\n+\t(insert_intra_bb_scope_notes): update call of insert_intra_1.\n+\t(insert_inter_bb_scope_notes): Set block for new insn.\n+\t(reorder_basic_blocks): Dump flow info before reoredering.\n+\n Mon Sep 10 06:47:35 2001  Richard Kenner  <kenner@vlsi1.ultra.nyu.edu>\n \n \t* alias.c (clear_reg_alias_info): Use K&R format definition."}, {"sha": "4c29d261e5bc5e2712364a8cb7280db462e7f382", "filename": "gcc/Makefile.in", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=402209ff48d3e1984111c536033aa638f4271531", "patch": "@@ -746,7 +746,8 @@ OBJS =\t\t\t\t\t\t\t\t\t\\\n  rtl-error.o sbitmap.o sched-deps.o sched-ebb.o sched-rgn.o sched-vis.o \\\n  sdbout.o sibcall.o simplify-rtx.o splay-tree.o ssa.o ssa-ccp.o         \\\n  ssa-dce.o stmt.o stor-layout.o stringpool.o timevar.o toplev.o tree.o  \\\n- unroll.o varasm.o varray.o version.o xcoffout.o                        \\\n+ unroll.o varasm.o varray.o version.o xcoffout.o cfg.o cfganal.o\t\\\n+ cfgbuild.o cfgcleanup.o cfgloop.o        \\\n  $(GGC) $(out_object_file) $(EXTRA_OBJS)\n \n BACKEND = main.o libbackend.a\n@@ -1487,6 +1488,18 @@ unroll.o : unroll.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) insn-config.h function.h \\\n flow.o : flow.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(TREE_H) flags.h insn-config.h \\\n    $(BASIC_BLOCK_H) $(REGS_H) hard-reg-set.h output.h toplev.h $(RECOG_H) \\\n    function.h except.h $(EXPR_H) ssa.h $(GGC_H) $(TM_P_H)\n+cfg.o : cfg.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) flags.h insn-config.h \\\n+   $(BASIC_BLOCK_H) $(REGS_H) hard-reg-set.h output.h toplev.h $(RECOG_H) \\\n+   function.h except.h $(GGC_H) \n+cfganal.o : cfganal.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) \\\n+   $(BASIC_BLOCK_H) hard-reg-set.h $(GGC_H)\n+cfgbuild.o : cfgbuild.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) flags.h insn-config.h \\\n+   $(BASIC_BLOCK_H) $(REGS_H) hard-reg-set.h output.h toplev.h $(RECOG_H) \\\n+   function.h except.h $(GGC_H) \n+cfgcleanup.o : cfgcleanup.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) \\\n+   $(BASIC_BLOCK_H) hard-reg-set.h output.h flags.h $(RECOG_H) toplev.h $(GGC_H)\n+cfgloop.o : cfgloop.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) \\\n+   $(BASIC_BLOCK_H) hard-reg-set.h\n dominance.o : dominance.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) hard-reg-set.h \\\n    $(BASIC_BLOCK_H)\n combine.o : combine.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) flags.h function.h \\"}, {"sha": "13241e8ffe656f794abeb1b3471bcea2a84f3315", "filename": "gcc/basic-block.h", "status": "modified", "additions": 12, "deletions": 3, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fbasic-block.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fbasic-block.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fbasic-block.h?ref=402209ff48d3e1984111c536033aa638f4271531", "patch": "@@ -246,6 +246,12 @@ extern varray_type basic_block_info;\n \n extern regset regs_live_at_setjmp;\n \n+/* Special labels found during CFG build.  */\n+\n+extern rtx label_value_list, tail_recursion_label_list;\n+\n+extern struct obstack flow_obstack;\n+\n /* Indexed by n, gives number of basic block that  (REG n) is used in.\n    If the value is REG_BLOCK_GLOBAL (-2),\n    it means (REG n) is used in more than one basic block.\n@@ -310,6 +316,7 @@ extern int flow_delete_block\t\tPARAMS ((basic_block));\n extern void merge_blocks_nomove\t\tPARAMS ((basic_block, basic_block));\n extern void tidy_fallthru_edge\t\tPARAMS ((edge, basic_block,\n \t\t\t\t\t\t basic_block));\n+extern void tidy_fallthru_edges\t\tPARAMS ((void));\n extern void flow_reverse_top_sort_order_compute\tPARAMS ((int *));\n extern int flow_depth_first_order_compute\tPARAMS ((int *, int *));\n extern void dump_edge_info\t\tPARAMS ((FILE *, edge, int));\n@@ -616,17 +623,19 @@ extern void debug_regset\t\tPARAMS ((regset));\n extern void allocate_reg_life_data      PARAMS ((void));\n extern void allocate_bb_life_data\tPARAMS ((void));\n extern void find_unreachable_blocks\tPARAMS ((void));\n-extern void expunge_block\t\tPARAMS ((basic_block));\n extern void delete_noop_moves\t\tPARAMS ((rtx));\n-extern rtx last_loop_beg_note\t\tPARAMS ((rtx));\n extern basic_block redirect_edge_and_branch_force PARAMS ((edge, basic_block));\n extern bool redirect_edge_and_branch\tPARAMS ((edge, basic_block));\n extern rtx block_label\t\t\tPARAMS ((basic_block));\n extern bool forwarder_block_p\t\tPARAMS ((basic_block));\n extern bool purge_all_dead_edges\tPARAMS ((void));\n extern bool purge_dead_edges\t\tPARAMS ((basic_block));\n extern void find_sub_basic_blocks\tPARAMS ((basic_block));\n-\n+extern bool can_fallthru\t\tPARAMS ((basic_block, basic_block));\n+extern void flow_nodes_print\t\tPARAMS ((const char *, const sbitmap,\n+\t\t\t\t\t\t FILE *));\n+extern void flow_edge_list_print\tPARAMS ((const char *, const edge *,\n+\t\t\t\t\t\t int, FILE *));\n \n /* This function is always defined so it can be called from the\n    debugger, and it is declared extern so we don't get warnings about"}, {"sha": "3102132eb3e7a219040ea9fc4b8bc91ff36bfaaf", "filename": "gcc/bb-reorder.c", "status": "modified", "additions": 81, "deletions": 71, "changes": 152, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fbb-reorder.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fbb-reorder.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fbb-reorder.c?ref=402209ff48d3e1984111c536033aa638f4271531", "patch": "@@ -165,7 +165,6 @@ typedef struct reorder_block_def\n   rtx eff_end;\n   scope scope;\n   basic_block next;\n-  int index;\n   int visited;\n } *reorder_block_def;\n \n@@ -187,7 +186,7 @@ static void relate_bbs_with_scopes\tPARAMS ((scope));\n static scope make_new_scope\t\tPARAMS ((int, rtx));\n static void build_scope_forest\t\tPARAMS ((scope_forest_info *));\n static void remove_scope_notes\t\tPARAMS ((void));\n-static void insert_intra_1\t\tPARAMS ((scope, rtx *));\n+static void insert_intra_1\t\tPARAMS ((scope, rtx *, basic_block));\n static void insert_intra_bb_scope_notes PARAMS ((basic_block));\n static void insert_inter_bb_scope_notes PARAMS ((basic_block, basic_block));\n static void rebuild_scope_notes\t\tPARAMS ((scope_forest_info *));\n@@ -323,6 +322,7 @@ make_reorder_chain ()\n   basic_block last_block = NULL;\n   basic_block prev = NULL;\n   int nbb_m1 = n_basic_blocks - 1;\n+  basic_block next;\n \n   /* If we've not got epilogue in RTL, we must fallthru to the exit.\n      Force the last block to be at the end.  */\n@@ -339,7 +339,8 @@ make_reorder_chain ()\n   do\n     {\n       int i;\n-      basic_block next = NULL;\n+\n+      next = NULL;\n \n       /* Find the next unplaced block.  */\n       /* ??? Get rid of this loop, and track which blocks are not yet\n@@ -348,27 +349,21 @@ make_reorder_chain ()\n \t remove from the list as we place.  The head of that list is\n \t what we're looking for here.  */\n \n-      for (i = 0; i <= nbb_m1; ++i)\n+      for (i = 0; i <= nbb_m1 && !next; ++i)\n \t{\n \t  basic_block bb = BASIC_BLOCK (i);\n \t  if (! RBI (bb)->visited)\n-\t    {\n-\t      next = bb;\n-\t      break;\n-\t    }\n+\t    next = bb;\n \t}\n-      if (! next)\n-\tabort ();\n-\n-      prev = make_reorder_chain_1 (next, prev);\n+      if (next)\n+        prev = make_reorder_chain_1 (next, prev);\n     }\n-  while (RBI (prev)->index < nbb_m1);\n+  while (next);\n \n   /* Terminate the chain.  */\n   if (! HAVE_epilogue)\n     {\n       RBI (prev)->next = last_block;\n-      RBI (last_block)->index = RBI (prev)->index + 1;\n       prev = last_block;\n     }\n   RBI (prev)->next = NULL;\n@@ -397,19 +392,18 @@ make_reorder_chain_1 (bb, prev)\n   /* Mark this block visited.  */\n   if (prev)\n     {\n-      int new_index;\n-\n  restart:\n       RBI (prev)->next = bb;\n-      new_index = RBI (prev)->index + 1;\n-      RBI (bb)->index = new_index;\n \n       if (rtl_dump_file && prev->index + 1 != bb->index)\n-\tfprintf (rtl_dump_file, \"Reordering block %d (%d) after %d (%d)\\n\",\n-\t\t bb->index, RBI (bb)->index, prev->index, RBI (prev)->index);\n+\tfprintf (rtl_dump_file, \"Reordering block %d after %d\\n\",\n+\t\t bb->index, prev->index);\n     }\n   else\n-    RBI (bb)->index = 0;\n+    {\n+      if (bb->index != 0)\n+\tabort ();\n+    }\n   RBI (bb)->visited = 1;\n   prev = bb;\n \n@@ -508,13 +502,15 @@ label_for_bb (bb)\n   if (GET_CODE (label) != CODE_LABEL)\n     {\n       if (rtl_dump_file)\n-\tfprintf (rtl_dump_file, \"Emitting label for block %d (%d)\\n\",\n-\t\t bb->index, RBI (bb)->index);\n+\tfprintf (rtl_dump_file, \"Emitting label for block %d\\n\",\n+\t\t bb->index);\n \n       label = emit_label_before (gen_label_rtx (), label);\n       if (bb->head == RBI (bb)->eff_head)\n \tRBI (bb)->eff_head = label;\n       bb->head = label;\n+      if (basic_block_for_insn)\n+\tset_block_for_insn (label, bb);\n     }\n \n   return label;\n@@ -540,15 +536,17 @@ emit_jump_to_block_after (bb, after)\n \tset_block_for_new_insns (jump, bb);\n \n       if (rtl_dump_file)\n-\tfprintf (rtl_dump_file, \"Emitting jump to block %d (%d)\\n\",\n-\t\t bb->index, RBI (bb)->index);\n+\tfprintf (rtl_dump_file, \"Emitting jump to block %d\\n\",\n+\t\t bb->index);\n     }\n   else\n     {\n #ifdef HAVE_return\n       if (! HAVE_return)\n \tabort ();\n       jump = emit_jump_insn_after (gen_return (), after);\n+      if (basic_block_for_insn)\n+\tset_block_for_new_insns (jump, bb);\n \n       if (rtl_dump_file)\n \tfprintf (rtl_dump_file, \"Emitting return\\n\");\n@@ -567,12 +565,16 @@ static void\n fixup_reorder_chain ()\n {\n   basic_block bb, last_bb;\n+  int index;\n+  rtx insn;\n+  int old_n_basic_blocks = n_basic_blocks;\n \n   /* First do the bulk reordering -- rechain the blocks without regard to\n      the needed changes to jumps and labels.  */\n \n   last_bb = BASIC_BLOCK (0);\n   bb = RBI (last_bb)->next;\n+  index = 1;\n   while (bb)\n     {\n       rtx last_e = RBI (last_bb)->eff_end;\n@@ -583,19 +585,24 @@ fixup_reorder_chain ()\n \n       last_bb = bb;\n       bb = RBI (bb)->next;\n+      index++;\n     }\n \n-  {\n-    rtx insn = RBI (last_bb)->eff_end;\n+  if (index != n_basic_blocks)\n+    abort ();\n \n-    NEXT_INSN (insn) = function_tail_eff_head;\n-    if (function_tail_eff_head)\n-      PREV_INSN (function_tail_eff_head) = insn;\n+  insn = RBI (last_bb)->eff_end;\n \n-    while (NEXT_INSN (insn))\n-      insn = NEXT_INSN (insn);\n-    set_last_insn (insn);\n-  }\n+  NEXT_INSN (insn) = function_tail_eff_head;\n+  if (function_tail_eff_head)\n+    PREV_INSN (function_tail_eff_head) = insn;\n+\n+  while (NEXT_INSN (insn))\n+    insn = NEXT_INSN (insn);\n+  set_last_insn (insn);\n+#ifdef ENABLE_CHECKING\n+  verify_insn_chain ();\n+#endif\n \n   /* Now add jumps and labels as needed to match the blocks new\n      outgoing edges.  */\n@@ -621,35 +628,19 @@ fixup_reorder_chain ()\n       bb_end_insn = bb->end;\n       if (GET_CODE (bb_end_insn) == JUMP_INSN)\n \t{\n-\t  if (any_uncondjump_p (bb_end_insn))\n-\t    {\n-\t      /* If the destination is still not next, nothing to do.  */\n-\t      if (RBI (bb)->index + 1 != RBI (e_taken->dest)->index)\n-\t\tcontinue;\n-\n-\t      /* Otherwise, we can remove the jump and cleanup the edge.  */\n-\t      tidy_fallthru_edge (e_taken, bb, e_taken->dest);\n-\t      RBI (bb)->eff_end = skip_insns_after_block (bb);\n-\t      RBI (e_taken->dest)->eff_head = NEXT_INSN (RBI (bb)->eff_end);\n-\n-\t      if (rtl_dump_file)\n-\t\tfprintf (rtl_dump_file, \"Removing jump in block %d (%d)\\n\",\n-\t\t\t bb->index, RBI (bb)->index);\n-\t      continue;\n-\t    }\n-\t  else if (any_condjump_p (bb_end_insn))\n+\t  if (any_condjump_p (bb_end_insn))\n \t    {\n \t      /* If the old fallthru is still next, nothing to do.  */\n-\t      if (RBI (bb)->index + 1 == RBI (e_fall->dest)->index\n-\t          || (RBI (bb)->index == n_basic_blocks - 1\n+\t      if (RBI (bb)->next == e_fall->dest\n+\t          || (!RBI (bb)->next\n \t\t      && e_fall->dest == EXIT_BLOCK_PTR))\n \t\tcontinue;\n \n \t      /* There is one special case: if *neither* block is next,\n \t\t such as happens at the very end of a function, then we'll\n \t\t need to add a new unconditional jump.  Choose the taken\n \t\t edge based on known or assumed probability.  */\n-\t      if (RBI (bb)->index + 1 != RBI (e_taken->dest)->index)\n+\t      if (RBI (bb)->next != e_taken->dest)\n \t\t{\n \t\t  rtx note = find_reg_note (bb_end_insn, REG_BR_PROB, 0);\n \t\t  if (note\n@@ -684,7 +675,7 @@ fixup_reorder_chain ()\n #ifdef CASE_DROPS_THROUGH\n \t      /* Except for VAX.  Since we didn't have predication for the\n \t\t tablejump, the fallthru block should not have moved.  */\n-\t      if (RBI (bb)->index + 1 == RBI (e_fall->dest)->index)\n+\t      if (RBI (bb)->next == e_fall->dest)\n \t\tcontinue;\n \t      bb_end_insn = skip_insns_after_block (bb);\n #else\n@@ -701,9 +692,7 @@ fixup_reorder_chain ()\n \t    continue;\n \n \t  /* If the fallthru block is still next, nothing to do.  */\n-\t  if (RBI (bb)->index + 1 == RBI (e_fall->dest)->index\n-\t      || (RBI (bb)->index == n_basic_blocks - 1\n-\t\t  && e_fall->dest == EXIT_BLOCK_PTR))\n+\t  if (RBI (bb)->next == e_fall->dest)\n \t    continue;\n \n \t  /* We need a new jump insn.  If the block has only one outgoing\n@@ -730,20 +719,19 @@ fixup_reorder_chain ()\n       create_basic_block (n_basic_blocks - 1, jump_insn, jump_insn, NULL);\n \n       nb = BASIC_BLOCK (n_basic_blocks - 1);\n-      nb->global_live_at_start = OBSTACK_ALLOC_REG_SET (&flow_obstack);\n-      nb->global_live_at_end = OBSTACK_ALLOC_REG_SET (&flow_obstack);\n       nb->local_set = 0;\n       nb->count = e_fall->count;\n       nb->frequency = EDGE_FREQUENCY (e_fall);\n \n+      nb->global_live_at_start = OBSTACK_ALLOC_REG_SET (&flow_obstack);\n+      nb->global_live_at_end = OBSTACK_ALLOC_REG_SET (&flow_obstack);\n       COPY_REG_SET (nb->global_live_at_start, bb->global_live_at_start);\n       COPY_REG_SET (nb->global_live_at_end, bb->global_live_at_start);\n \n       nb->aux = xmalloc (sizeof (struct reorder_block_def));\n       RBI (nb)->eff_head = nb->head;\n       RBI (nb)->eff_end = barrier_insn;\n       RBI (nb)->scope = RBI (bb)->scope;\n-      RBI (nb)->index = RBI (bb)->index + 1;\n       RBI (nb)->visited = 1;\n       RBI (nb)->next = RBI (bb)->next;\n       RBI (bb)->next = nb;\n@@ -756,17 +744,26 @@ fixup_reorder_chain ()\n \n       /* Don't process this new block.  */\n       bb = nb;\n-\n-      /* Fix subsequent reorder block indices to reflect new block.  */\n-      while ((nb = RBI (nb)->next) != NULL)\n-\tRBI (nb)->index += 1;\n     }\n \n   /* Put basic_block_info in the new order.  */\n-  for (bb = BASIC_BLOCK (0); bb ; bb = RBI (bb)->next)\n+  bb = BASIC_BLOCK (0);\n+  index = 0;\n+\n+  if (rtl_dump_file)\n+    fprintf (rtl_dump_file, \"Reordered sequence:\\n\");\n+  while (bb)\n     {\n-      bb->index = RBI (bb)->index;\n-      BASIC_BLOCK (bb->index) = bb;\n+      if (rtl_dump_file)\n+\tfprintf (rtl_dump_file, \" %i %sbb %i freq %i\\n\", index,\n+\t\t bb->index >= old_n_basic_blocks ? \"compensation \" : \"\",\n+\t\t bb->index,\n+\t   \t bb->frequency);\n+      bb->index = index;\n+      BASIC_BLOCK (index) = bb;\n+\n+      bb = RBI (bb)->next;\n+      index++;\n     }\n }\n \n@@ -1142,25 +1139,30 @@ remove_scope_notes ()\n /* Insert scope note pairs for a contained scope tree S after insn IP.  */\n \n static void\n-insert_intra_1 (s, ip)\n+insert_intra_1 (s, ip, bb)\n      scope s;\n      rtx *ip;\n+     basic_block bb;\n {\n   scope p;\n \n   if (NOTE_BLOCK (s->note_beg))\n     {  \n       *ip = emit_note_after (NOTE_INSN_BLOCK_BEG, *ip);\n       NOTE_BLOCK (*ip) = NOTE_BLOCK (s->note_beg);\n+      if (basic_block_for_insn)\n+\tset_block_for_insn (*ip, bb);\n     } \n \n   for (p = s->inner; p; p = p->next)\n-    insert_intra_1 (p, ip);\n+    insert_intra_1 (p, ip, bb);\n \n   if (NOTE_BLOCK (s->note_beg))\n     {  \n       *ip = emit_note_after (NOTE_INSN_BLOCK_END, *ip);\n       NOTE_BLOCK (*ip) = NOTE_BLOCK (s->note_end);\n+      if (basic_block_for_insn)\n+\tset_block_for_insn (*ip, bb);\n     }\n }\n \n@@ -1186,7 +1188,7 @@ insert_intra_bb_scope_notes (bb)\n   for (p = s->inner; p; p = p->next)\n     {\n       if (p->bb_beg != NULL && p->bb_beg == p->bb_end && p->bb_beg == bb)\n-\tinsert_intra_1 (p, &ip);\n+\tinsert_intra_1 (p, &ip, bb);\n     }\n }\n \n@@ -1254,6 +1256,8 @@ insert_inter_bb_scope_notes (bb1, bb2)\n \t    {  \n \t      ip = emit_note_after (NOTE_INSN_BLOCK_END, ip);\n \t      NOTE_BLOCK (ip) = NOTE_BLOCK (s->note_end);\n+\t      if (basic_block_for_insn)\n+\t\tset_block_for_insn (ip, bb1);\n \t    }\n \t  s = s->outer;\n \t}\n@@ -1270,6 +1274,8 @@ insert_inter_bb_scope_notes (bb1, bb2)\n \t    {  \n \t      ip = emit_note_before (NOTE_INSN_BLOCK_BEG, ip);\n \t      NOTE_BLOCK (ip) = NOTE_BLOCK (s->note_beg);\n+\t      if (basic_block_for_insn)\n+\t\tset_block_for_insn (ip, bb2);\n \t    }\n \t  s = s->outer;\n \t}\n@@ -1414,6 +1420,10 @@ reorder_basic_blocks ()\n \n   record_effective_endpoints ();\n   make_reorder_chain ();\n+\n+  if (rtl_dump_file)\n+    dump_flow_info (rtl_dump_file);\n+\n   fixup_reorder_chain ();\n \n #ifdef ENABLE_CHECKING"}, {"sha": "3f11a55982fb05b5d67d616a06c102d75d9be6ad", "filename": "gcc/cfg.c", "status": "added", "additions": 2517, "deletions": 0, "changes": 2517, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fcfg.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fcfg.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfg.c?ref=402209ff48d3e1984111c536033aa638f4271531", "patch": "@@ -0,0 +1,2517 @@\n+/* Control flow graph manipulation code for GNU compiler.\n+   Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n+   1999, 2000, 2001 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 2, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+/* This file contains low level functions to manipulate with CFG and analyze it.\n+   All other modules should not transform the datastructure directly and use\n+   abstraction instead.  The file is supposed to be ordered bottom-up.\n+\n+   Available functionality:\n+     - Initialization/deallocation\n+\t init_flow, clear_edges\n+     - CFG aware instruction chain manipulation\n+\t flow_delete_insn, flow_delete_insn_chain\n+     - Basic block manipulation\n+\t create_basic_block, flow_delete_block, split_block, merge_blocks_nomove\n+     - Infrastructure to determine quickly basic block for instruction.\n+\t compute_bb_for_insn, update_bb_for_insn, set_block_for_insn,\n+\t set_block_for_new_insns\n+     - Edge manipulation\n+\t make_edge, remove_edge\n+\t - Low level edge redirection (without updating instruction chain)\n+\t     redirect_edge_succ, redirect_edge_succ_nodup, redirect_edge_pred\n+\t - High level edge redirection (with updating and optimizing instruction\n+\t   chain)\n+\t     block_label, redirect_edge_and_branch,\n+\t     redirect_edge_and_branch_force, tidy_fallthru_edge\n+      - Edge splitting and commiting to edges\n+\t  split_edge, insert_insn_on_edge, commit_edge_insertions\n+      - Dumpipng and debugging\n+\t  dump_flow_info, debug_flow_info, dump_edge_info, dump_bb, debug_bb,\n+\t  debug_bb_n, print_rtl_with_bb\n+      - Consistency checking\n+\t  verify_flow_info\n+      - CFG updating after constant propagation\n+\t  purge_dead_edges, purge_all_dead_edges\n+ */\n+\f\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"tree.h\"\n+#include \"rtl.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"regs.h\"\n+#include \"flags.h\"\n+#include \"output.h\"\n+#include \"function.h\"\n+#include \"except.h\"\n+#include \"toplev.h\"\n+\n+#include \"obstack.h\"\n+\n+/* The obstack on which the flow graph components are allocated.  */\n+\n+struct obstack flow_obstack;\n+static char *flow_firstobj;\n+\n+/* Number of basic blocks in the current function.  */\n+\n+int n_basic_blocks;\n+\n+/* Number of edges in the current function.  */\n+\n+int n_edges;\n+\n+/* The basic block array.  */\n+\n+varray_type basic_block_info;\n+\n+/* The special entry and exit blocks.  */\n+\n+struct basic_block_def entry_exit_blocks[2]\n+= {{NULL,\t\t\t/* head */\n+    NULL,\t\t\t/* end */\n+    NULL,\t\t\t/* head_tree */\n+    NULL,\t\t\t/* end_tree */\n+    NULL,\t\t\t/* pred */\n+    NULL,\t\t\t/* succ */\n+    NULL,\t\t\t/* local_set */\n+    NULL,\t\t\t/* cond_local_set */\n+    NULL,\t\t\t/* global_live_at_start */\n+    NULL,\t\t\t/* global_live_at_end */\n+    NULL,\t\t\t/* aux */\n+    ENTRY_BLOCK,\t\t/* index */\n+    0,\t\t\t\t/* loop_depth */\n+    0,\t\t\t\t/* count */\n+    0,\t\t\t\t/* frequency */\n+    0\t\t\t\t/* flags */\n+  },\n+  {\n+    NULL,\t\t\t/* head */\n+    NULL,\t\t\t/* end */\n+    NULL,\t\t\t/* head_tree */\n+    NULL,\t\t\t/* end_tree */\n+    NULL,\t\t\t/* pred */\n+    NULL,\t\t\t/* succ */\n+    NULL,\t\t\t/* local_set */\n+    NULL,\t\t\t/* cond_local_set */\n+    NULL,\t\t\t/* global_live_at_start */\n+    NULL,\t\t\t/* global_live_at_end */\n+    NULL,\t\t\t/* aux */\n+    EXIT_BLOCK,\t\t\t/* index */\n+    0,\t\t\t\t/* loop_depth */\n+    0,\t\t\t\t/* count */\n+    0,\t\t\t\t/* frequency */\n+    0\t\t\t\t/* flags */\n+  }\n+};\n+\n+/* The basic block structure for every insn, indexed by uid.  */\n+\n+varray_type basic_block_for_insn;\n+\n+/* The labels mentioned in non-jump rtl.  Valid during find_basic_blocks.  */\n+/* ??? Should probably be using LABEL_NUSES instead.  It would take a\n+   bit of surgery to be able to use or co-opt the routines in jump.  */\n+\n+rtx label_value_list;\n+rtx tail_recursion_label_list;\n+\n+void debug_flow_info\t\t\tPARAMS ((void));\n+static int can_delete_note_p\t\tPARAMS ((rtx));\n+static int can_delete_label_p\t\tPARAMS ((rtx));\n+static void commit_one_edge_insertion\tPARAMS ((edge));\n+static bool try_redirect_by_replacing_jump PARAMS ((edge, basic_block));\n+static void expunge_block\t\tPARAMS ((basic_block));\n+static rtx last_loop_beg_note\t\tPARAMS ((rtx));\n+static bool back_edge_of_syntactic_loop_p PARAMS ((basic_block, basic_block));\n+\f\n+/* Called once at intialization time.  */\n+\n+void\n+init_flow ()\n+{\n+  static int initialized;\n+\n+  if (!initialized)\n+    {\n+      gcc_obstack_init (&flow_obstack);\n+      flow_firstobj = (char *) obstack_alloc (&flow_obstack, 0);\n+      initialized = 1;\n+    }\n+  else\n+    {\n+      obstack_free (&flow_obstack, flow_firstobj);\n+      flow_firstobj = (char *) obstack_alloc (&flow_obstack, 0);\n+    }\n+}\n+\f\n+/* Free the memory associated with the edge structures.  */\n+\n+void\n+clear_edges ()\n+{\n+  int i;\n+  edge n, e;\n+\n+  for (i = 0; i < n_basic_blocks; ++i)\n+    {\n+      basic_block bb = BASIC_BLOCK (i);\n+\n+      for (e = bb->succ; e; e = n)\n+\t{\n+\t  n = e->succ_next;\n+\t  free (e);\n+\t}\n+\n+      bb->succ = 0;\n+      bb->pred = 0;\n+    }\n+\n+  for (e = ENTRY_BLOCK_PTR->succ; e; e = n)\n+    {\n+      n = e->succ_next;\n+      free (e);\n+    }\n+\n+  ENTRY_BLOCK_PTR->succ = 0;\n+  EXIT_BLOCK_PTR->pred = 0;\n+\n+  n_edges = 0;\n+}\n+\f\n+/* Return true if NOTE is not one of the ones that must be kept paired,\n+   so that we may simply delete them.  */\n+\n+static int\n+can_delete_note_p (note)\n+     rtx note;\n+{\n+  return (NOTE_LINE_NUMBER (note) == NOTE_INSN_DELETED\n+\t  || NOTE_LINE_NUMBER (note) == NOTE_INSN_BASIC_BLOCK);\n+}\n+\n+/* True if a given label can be deleted.  */\n+\n+static int\n+can_delete_label_p (label)\n+     rtx label;\n+{\n+  rtx x;\n+\n+  if (LABEL_PRESERVE_P (label))\n+    return 0;\n+\n+  for (x = forced_labels; x; x = XEXP (x, 1))\n+    if (label == XEXP (x, 0))\n+      return 0;\n+  for (x = label_value_list; x; x = XEXP (x, 1))\n+    if (label == XEXP (x, 0))\n+      return 0;\n+  for (x = exception_handler_labels; x; x = XEXP (x, 1))\n+    if (label == XEXP (x, 0))\n+      return 0;\n+\n+  /* User declared labels must be preserved.  */\n+  if (LABEL_NAME (label) != 0)\n+    return 0;\n+\n+  return 1;\n+}\n+\n+/* Delete INSN by patching it out.  Return the next insn.  */\n+\n+rtx\n+flow_delete_insn (insn)\n+     rtx insn;\n+{\n+  rtx prev = PREV_INSN (insn);\n+  rtx next = NEXT_INSN (insn);\n+  rtx note;\n+\n+  PREV_INSN (insn) = NULL_RTX;\n+  NEXT_INSN (insn) = NULL_RTX;\n+  INSN_DELETED_P (insn) = 1;\n+\n+  if (prev)\n+    NEXT_INSN (prev) = next;\n+  if (next)\n+    PREV_INSN (next) = prev;\n+  else\n+    set_last_insn (prev);\n+\n+  if (GET_CODE (insn) == CODE_LABEL)\n+    remove_node_from_expr_list (insn, &nonlocal_goto_handler_labels);\n+\n+  /* If deleting a jump, decrement the use count of the label.  Deleting\n+     the label itself should happen in the normal course of block merging.  */\n+  if (GET_CODE (insn) == JUMP_INSN\n+      && JUMP_LABEL (insn)\n+      && GET_CODE (JUMP_LABEL (insn)) == CODE_LABEL)\n+    LABEL_NUSES (JUMP_LABEL (insn))--;\n+\n+  /* Also if deleting an insn that references a label.  */\n+  else if ((note = find_reg_note (insn, REG_LABEL, NULL_RTX)) != NULL_RTX\n+\t   && GET_CODE (XEXP (note, 0)) == CODE_LABEL)\n+    LABEL_NUSES (XEXP (note, 0))--;\n+\n+  if (GET_CODE (insn) == JUMP_INSN\n+      && (GET_CODE (PATTERN (insn)) == ADDR_VEC\n+\t  || GET_CODE (PATTERN (insn)) == ADDR_DIFF_VEC))\n+    {\n+      rtx pat = PATTERN (insn);\n+      int diff_vec_p = GET_CODE (PATTERN (insn)) == ADDR_DIFF_VEC;\n+      int len = XVECLEN (pat, diff_vec_p);\n+      int i;\n+\n+      for (i = 0; i < len; i++)\n+\tLABEL_NUSES (XEXP (XVECEXP (pat, diff_vec_p, i), 0))--;\n+    }\n+\n+  return next;\n+}\n+\n+/* Unlink a chain of insns between START and FINISH, leaving notes\n+   that must be paired.  */\n+\n+void\n+flow_delete_insn_chain (start, finish)\n+     rtx start, finish;\n+{\n+  /* Unchain the insns one by one.  It would be quicker to delete all\n+     of these with a single unchaining, rather than one at a time, but\n+     we need to keep the NOTE's.  */\n+\n+  rtx next;\n+\n+  while (1)\n+    {\n+      next = NEXT_INSN (start);\n+      if (GET_CODE (start) == NOTE && !can_delete_note_p (start))\n+\t;\n+      else if (GET_CODE (start) == CODE_LABEL\n+\t       && ! can_delete_label_p (start))\n+\t{\n+\t  const char *name = LABEL_NAME (start);\n+\t  PUT_CODE (start, NOTE);\n+\t  NOTE_LINE_NUMBER (start) = NOTE_INSN_DELETED_LABEL;\n+\t  NOTE_SOURCE_FILE (start) = name;\n+\t}\n+      else\n+\tnext = flow_delete_insn (start);\n+\n+      if (start == finish)\n+\tbreak;\n+      start = next;\n+    }\n+}\n+\f\n+/* Create a new basic block consisting of the instructions between\n+   HEAD and END inclusive.  Reuses the note and basic block struct\n+   in BB_NOTE, if any.  */\n+\n+void\n+create_basic_block (index, head, end, bb_note)\n+     int index;\n+     rtx head, end, bb_note;\n+{\n+  basic_block bb;\n+\n+  if (bb_note\n+      && ! RTX_INTEGRATED_P (bb_note)\n+      && (bb = NOTE_BASIC_BLOCK (bb_note)) != NULL\n+      && bb->aux == NULL)\n+    {\n+      /* If we found an existing note, thread it back onto the chain.  */\n+\n+      rtx after;\n+\n+      if (GET_CODE (head) == CODE_LABEL)\n+\tafter = head;\n+      else\n+\t{\n+\t  after = PREV_INSN (head);\n+\t  head = bb_note;\n+\t}\n+\n+      if (after != bb_note && NEXT_INSN (after) != bb_note)\n+\treorder_insns (bb_note, bb_note, after);\n+    }\n+  else\n+    {\n+      /* Otherwise we must create a note and a basic block structure.\n+\t Since we allow basic block structs in rtl, give the struct\n+\t the same lifetime by allocating it off the function obstack\n+\t rather than using malloc.  */\n+\n+      bb = (basic_block) obstack_alloc (&flow_obstack, sizeof (*bb));\n+      memset (bb, 0, sizeof (*bb));\n+\n+      if (GET_CODE (head) == CODE_LABEL)\n+\tbb_note = emit_note_after (NOTE_INSN_BASIC_BLOCK, head);\n+      else\n+\t{\n+\t  bb_note = emit_note_before (NOTE_INSN_BASIC_BLOCK, head);\n+\t  head = bb_note;\n+\t}\n+      NOTE_BASIC_BLOCK (bb_note) = bb;\n+    }\n+\n+  /* Always include the bb note in the block.  */\n+  if (NEXT_INSN (end) == bb_note)\n+    end = bb_note;\n+\n+  bb->head = head;\n+  bb->end = end;\n+  bb->index = index;\n+  BASIC_BLOCK (index) = bb;\n+\n+  /* Tag the block so that we know it has been used when considering\n+     other basic block notes.  */\n+  bb->aux = bb;\n+}\n+\n+/* Remove block B from the basic block array and compact behind it.  */\n+\n+static void\n+expunge_block (b)\n+     basic_block b;\n+{\n+  int i, n = n_basic_blocks;\n+\n+  for (i = b->index; i + 1 < n; ++i)\n+    {\n+      basic_block x = BASIC_BLOCK (i + 1);\n+      BASIC_BLOCK (i) = x;\n+      x->index = i;\n+    }\n+\n+  basic_block_info->num_elements--;\n+  n_basic_blocks--;\n+}\n+\n+/* Delete the insns in a (non-live) block.  We physically delete every\n+   non-deleted-note insn, and update the flow graph appropriately.\n+\n+   Return nonzero if we deleted an exception handler.  */\n+\n+/* ??? Preserving all such notes strikes me as wrong.  It would be nice\n+   to post-process the stream to remove empty blocks, loops, ranges, etc.  */\n+\n+int\n+flow_delete_block (b)\n+     basic_block b;\n+{\n+  int deleted_handler = 0;\n+  rtx insn, end, tmp;\n+\n+  /* If the head of this block is a CODE_LABEL, then it might be the\n+     label for an exception handler which can't be reached.\n+\n+     We need to remove the label from the exception_handler_label list\n+     and remove the associated NOTE_INSN_EH_REGION_BEG and\n+     NOTE_INSN_EH_REGION_END notes.  */\n+\n+  insn = b->head;\n+\n+  never_reached_warning (insn);\n+\n+  if (GET_CODE (insn) == CODE_LABEL)\n+    maybe_remove_eh_handler (insn);\n+\n+  /* Include any jump table following the basic block.  */\n+  end = b->end;\n+  if (GET_CODE (end) == JUMP_INSN\n+      && (tmp = JUMP_LABEL (end)) != NULL_RTX\n+      && (tmp = NEXT_INSN (tmp)) != NULL_RTX\n+      && GET_CODE (tmp) == JUMP_INSN\n+      && (GET_CODE (PATTERN (tmp)) == ADDR_VEC\n+\t  || GET_CODE (PATTERN (tmp)) == ADDR_DIFF_VEC))\n+    end = tmp;\n+\n+  /* Include any barrier that may follow the basic block.  */\n+  tmp = next_nonnote_insn (end);\n+  if (tmp && GET_CODE (tmp) == BARRIER)\n+    end = tmp;\n+\n+  /* Selectively delete the entire chain.  */\n+  flow_delete_insn_chain (insn, end);\n+\n+  /* Remove the edges into and out of this block.  Note that there may\n+     indeed be edges in, if we are removing an unreachable loop.  */\n+  {\n+    edge e, next, *q;\n+\n+    for (e = b->pred; e; e = next)\n+      {\n+\tfor (q = &e->src->succ; *q != e; q = &(*q)->succ_next)\n+\t  continue;\n+\t*q = e->succ_next;\n+\tnext = e->pred_next;\n+\tn_edges--;\n+\tfree (e);\n+      }\n+    for (e = b->succ; e; e = next)\n+      {\n+\tfor (q = &e->dest->pred; *q != e; q = &(*q)->pred_next)\n+\t  continue;\n+\t*q = e->pred_next;\n+\tnext = e->succ_next;\n+\tn_edges--;\n+\tfree (e);\n+      }\n+\n+    b->pred = NULL;\n+    b->succ = NULL;\n+  }\n+\n+  /* Remove the basic block from the array, and compact behind it.  */\n+  expunge_block (b);\n+\n+  return deleted_handler;\n+}\n+\f\n+/* Records the basic block struct in BB_FOR_INSN, for every instruction\n+   indexed by INSN_UID.  MAX is the size of the array.  */\n+\n+void\n+compute_bb_for_insn (max)\n+     int max;\n+{\n+  int i;\n+\n+  if (basic_block_for_insn)\n+    VARRAY_FREE (basic_block_for_insn);\n+  VARRAY_BB_INIT (basic_block_for_insn, max, \"basic_block_for_insn\");\n+\n+  for (i = 0; i < n_basic_blocks; ++i)\n+    {\n+      basic_block bb = BASIC_BLOCK (i);\n+      rtx insn, end;\n+\n+      end = bb->end;\n+      insn = bb->head;\n+      while (1)\n+\t{\n+\t  int uid = INSN_UID (insn);\n+\t  if (uid < max)\n+\t    VARRAY_BB (basic_block_for_insn, uid) = bb;\n+\t  if (insn == end)\n+\t    break;\n+\t  insn = NEXT_INSN (insn);\n+\t}\n+    }\n+}\n+\n+/* Update insns block within BB.  */\n+\n+void\n+update_bb_for_insn (bb)\n+     basic_block bb;\n+{\n+  rtx insn;\n+\n+  if (! basic_block_for_insn)\n+    return;\n+\n+  for (insn = bb->head; ; insn = NEXT_INSN (insn))\n+    {\n+      set_block_for_insn (insn, bb);\n+\n+      if (insn == bb->end)\n+\tbreak;\n+    }\n+}\n+\n+/* Record INSN's block as BB.  */\n+\n+void\n+set_block_for_insn (insn, bb)\n+     rtx insn;\n+     basic_block bb;\n+{\n+  size_t uid = INSN_UID (insn);\n+  if (uid >= basic_block_for_insn->num_elements)\n+    {\n+      int new_size;\n+\n+      /* Add one-eighth the size so we don't keep calling xrealloc.  */\n+      new_size = uid + (uid + 7) / 8;\n+\n+      VARRAY_GROW (basic_block_for_insn, new_size);\n+    }\n+  VARRAY_BB (basic_block_for_insn, uid) = bb;\n+}\n+\n+/* When a new insn has been inserted into an existing block, it will\n+   sometimes emit more than a single insn. This routine will set the\n+   block number for the specified insn, and look backwards in the insn\n+   chain to see if there are any other uninitialized insns immediately\n+   previous to this one, and set the block number for them too.  */\n+\n+void\n+set_block_for_new_insns (insn, bb)\n+     rtx insn;\n+     basic_block bb;\n+{\n+  set_block_for_insn (insn, bb);\n+\n+  /* Scan the previous instructions setting the block number until we find\n+     an instruction that has the block number set, or we find a note\n+     of any kind.  */\n+  for (insn = PREV_INSN (insn); insn != NULL_RTX; insn = PREV_INSN (insn))\n+    {\n+      if (GET_CODE (insn) == NOTE)\n+\tbreak;\n+      if ((unsigned) INSN_UID (insn) >= basic_block_for_insn->num_elements\n+\t  || BLOCK_FOR_INSN (insn) == 0)\n+\tset_block_for_insn (insn, bb);\n+      else\n+\tbreak;\n+    }\n+}\n+\f\n+void\n+make_edge (edge_cache, src, dst, flags)\n+     sbitmap *edge_cache;\n+     basic_block src, dst;\n+     int flags;\n+{\n+  int use_edge_cache;\n+  edge e;\n+\n+  /* Don't bother with edge cache for ENTRY or EXIT; there aren't that\n+     many edges to them, and we didn't allocate memory for it.  */\n+  use_edge_cache = (edge_cache\n+\t\t    && src != ENTRY_BLOCK_PTR\n+\t\t    && dst != EXIT_BLOCK_PTR);\n+\n+  /* Make sure we don't add duplicate edges.  */\n+  switch (use_edge_cache)\n+    {\n+    default:\n+      /* Quick test for non-existance of the edge.  */\n+      if (! TEST_BIT (edge_cache[src->index], dst->index))\n+\tbreak;\n+\n+      /* The edge exists; early exit if no work to do.  */\n+      if (flags == 0)\n+\treturn;\n+\n+      /* FALLTHRU */\n+    case 0:\n+      for (e = src->succ; e; e = e->succ_next)\n+\tif (e->dest == dst)\n+\t  {\n+\t    e->flags |= flags;\n+\t    return;\n+\t  }\n+      break;\n+    }\n+\n+  e = (edge) xcalloc (1, sizeof (*e));\n+  n_edges++;\n+\n+  e->succ_next = src->succ;\n+  e->pred_next = dst->pred;\n+  e->src = src;\n+  e->dest = dst;\n+  e->flags = flags;\n+\n+  src->succ = e;\n+  dst->pred = e;\n+\n+  if (use_edge_cache)\n+    SET_BIT (edge_cache[src->index], dst->index);\n+}\n+\n+/* This function will remove an edge from the flow graph.  */\n+\n+void\n+remove_edge (e)\n+     edge e;\n+{\n+  edge last_pred = NULL;\n+  edge last_succ = NULL;\n+  edge tmp;\n+  basic_block src, dest;\n+  src = e->src;\n+  dest = e->dest;\n+  for (tmp = src->succ; tmp && tmp != e; tmp = tmp->succ_next)\n+    last_succ = tmp;\n+\n+  if (!tmp)\n+    abort ();\n+  if (last_succ)\n+    last_succ->succ_next = e->succ_next;\n+  else\n+    src->succ = e->succ_next;\n+\n+  for (tmp = dest->pred; tmp && tmp != e; tmp = tmp->pred_next)\n+    last_pred = tmp;\n+\n+  if (!tmp)\n+    abort ();\n+  if (last_pred)\n+    last_pred->pred_next = e->pred_next;\n+  else\n+    dest->pred = e->pred_next;\n+\n+  n_edges--;\n+  free (e);\n+}\n+\n+/* Redirect an edge's successor from one block to another.  */\n+\n+void\n+redirect_edge_succ (e, new_succ)\n+     edge e;\n+     basic_block new_succ;\n+{\n+  edge *pe;\n+\n+  /* Disconnect the edge from the old successor block.  */\n+  for (pe = &e->dest->pred; *pe != e; pe = &(*pe)->pred_next)\n+    continue;\n+  *pe = (*pe)->pred_next;\n+\n+  /* Reconnect the edge to the new successor block.  */\n+  e->pred_next = new_succ->pred;\n+  new_succ->pred = e;\n+  e->dest = new_succ;\n+}\n+\n+/* Like previous but avoid possible dupplicate edge.  */\n+\n+edge\n+redirect_edge_succ_nodup (e, new_succ)\n+     edge e;\n+     basic_block new_succ;\n+{\n+  edge s;\n+  /* Check whether the edge is already present.  */\n+  for (s = e->src->succ; s; s = s->succ_next)\n+    if (s->dest == new_succ && s != e)\n+      break;\n+  if (s)\n+    {\n+      s->flags |= e->flags;\n+      s->probability += e->probability;\n+      s->count += e->count;\n+      remove_edge (e);\n+      e = s;\n+    }\n+  else\n+    redirect_edge_succ (e, new_succ);\n+  return e;\n+}\n+\n+/* Redirect an edge's predecessor from one block to another.  */\n+\n+void\n+redirect_edge_pred (e, new_pred)\n+     edge e;\n+     basic_block new_pred;\n+{\n+  edge *pe;\n+\n+  /* Disconnect the edge from the old predecessor block.  */\n+  for (pe = &e->src->succ; *pe != e; pe = &(*pe)->succ_next)\n+    continue;\n+  *pe = (*pe)->succ_next;\n+\n+  /* Reconnect the edge to the new predecessor block.  */\n+  e->succ_next = new_pred->succ;\n+  new_pred->succ = e;\n+  e->src = new_pred;\n+}\n+\f\n+/* Split a block BB after insn INSN creating a new fallthru edge.\n+   Return the new edge.  Note that to keep other parts of the compiler happy,\n+   this function renumbers all the basic blocks so that the new\n+   one has a number one greater than the block split.  */\n+\n+edge\n+split_block (bb, insn)\n+     basic_block bb;\n+     rtx insn;\n+{\n+  basic_block new_bb;\n+  edge new_edge;\n+  edge e;\n+  rtx bb_note;\n+  int i, j;\n+\n+  /* There is no point splitting the block after its end.  */\n+  if (bb->end == insn)\n+    return 0;\n+\n+  /* Create the new structures.  */\n+  new_bb = (basic_block) obstack_alloc (&flow_obstack, sizeof (*new_bb));\n+  new_edge = (edge) xcalloc (1, sizeof (*new_edge));\n+  n_edges++;\n+\n+  memset (new_bb, 0, sizeof (*new_bb));\n+\n+  new_bb->head = NEXT_INSN (insn);\n+  new_bb->end = bb->end;\n+  bb->end = insn;\n+\n+  new_bb->succ = bb->succ;\n+  bb->succ = new_edge;\n+  new_bb->pred = new_edge;\n+  new_bb->count = bb->count;\n+  new_bb->frequency = bb->frequency;\n+  new_bb->loop_depth = bb->loop_depth;\n+\n+  new_edge->src = bb;\n+  new_edge->dest = new_bb;\n+  new_edge->flags = EDGE_FALLTHRU;\n+  new_edge->probability = REG_BR_PROB_BASE;\n+  new_edge->count = bb->count;\n+\n+  /* Redirect the src of the successor edges of bb to point to new_bb.  */\n+  for (e = new_bb->succ; e; e = e->succ_next)\n+    e->src = new_bb;\n+\n+  /* Place the new block just after the block being split.  */\n+  VARRAY_GROW (basic_block_info, ++n_basic_blocks);\n+\n+  /* Some parts of the compiler expect blocks to be number in\n+     sequential order so insert the new block immediately after the\n+     block being split..  */\n+  j = bb->index;\n+  for (i = n_basic_blocks - 1; i > j + 1; --i)\n+    {\n+      basic_block tmp = BASIC_BLOCK (i - 1);\n+      BASIC_BLOCK (i) = tmp;\n+      tmp->index = i;\n+    }\n+\n+  BASIC_BLOCK (i) = new_bb;\n+  new_bb->index = i;\n+\n+  if (GET_CODE (new_bb->head) == CODE_LABEL)\n+    {\n+      /* Create the basic block note.  */\n+      bb_note = emit_note_after (NOTE_INSN_BASIC_BLOCK,\n+\t\t\t\t new_bb->head);\n+      NOTE_BASIC_BLOCK (bb_note) = new_bb;\n+\n+      /* If the only thing in this new block was the label, make sure\n+\t the block note gets included.  */\n+      if (new_bb->head == new_bb->end)\n+\tnew_bb->end = bb_note;\n+    }\n+  else\n+    {\n+      /* Create the basic block note.  */\n+      bb_note = emit_note_before (NOTE_INSN_BASIC_BLOCK,\n+\t\t\t\t  new_bb->head);\n+      NOTE_BASIC_BLOCK (bb_note) = new_bb;\n+      new_bb->head = bb_note;\n+    }\n+\n+  update_bb_for_insn (new_bb);\n+\n+  if (bb->global_live_at_start)\n+    {\n+      new_bb->global_live_at_start = OBSTACK_ALLOC_REG_SET (&flow_obstack);\n+      new_bb->global_live_at_end = OBSTACK_ALLOC_REG_SET (&flow_obstack);\n+      COPY_REG_SET (new_bb->global_live_at_end, bb->global_live_at_end);\n+\n+      /* We now have to calculate which registers are live at the end\n+\t of the split basic block and at the start of the new basic\n+\t block.  Start with those registers that are known to be live\n+\t at the end of the original basic block and get\n+\t propagate_block to determine which registers are live.  */\n+      COPY_REG_SET (new_bb->global_live_at_start, bb->global_live_at_end);\n+      propagate_block (new_bb, new_bb->global_live_at_start, NULL, NULL, 0);\n+      COPY_REG_SET (bb->global_live_at_end,\n+\t\t    new_bb->global_live_at_start);\n+    }\n+\n+  return new_edge;\n+}\n+\n+/* Blocks A and B are to be merged into a single block A.  The insns\n+   are already contiguous, hence `nomove'.  */\n+\n+void\n+merge_blocks_nomove (a, b)\n+     basic_block a, b;\n+{\n+  edge e;\n+  rtx b_head, b_end, a_end;\n+  rtx del_first = NULL_RTX, del_last = NULL_RTX;\n+  int b_empty = 0;\n+\n+  /* If there was a CODE_LABEL beginning B, delete it.  */\n+  b_head = b->head;\n+  b_end = b->end;\n+  if (GET_CODE (b_head) == CODE_LABEL)\n+    {\n+      /* Detect basic blocks with nothing but a label.  This can happen\n+\t in particular at the end of a function.  */\n+      if (b_head == b_end)\n+\tb_empty = 1;\n+      del_first = del_last = b_head;\n+      b_head = NEXT_INSN (b_head);\n+    }\n+\n+  /* Delete the basic block note.  */\n+  if (NOTE_INSN_BASIC_BLOCK_P (b_head))\n+    {\n+      if (b_head == b_end)\n+\tb_empty = 1;\n+      if (! del_last)\n+\tdel_first = b_head;\n+      del_last = b_head;\n+      b_head = NEXT_INSN (b_head);\n+    }\n+\n+  /* If there was a jump out of A, delete it.  */\n+  a_end = a->end;\n+  if (GET_CODE (a_end) == JUMP_INSN)\n+    {\n+      rtx prev;\n+\n+      for (prev = PREV_INSN (a_end); ; prev = PREV_INSN (prev))\n+\tif (GET_CODE (prev) != NOTE\n+\t    || NOTE_LINE_NUMBER (prev) == NOTE_INSN_BASIC_BLOCK\n+\t    || prev == a->head)\n+\t  break;\n+\n+      del_first = a_end;\n+\n+#ifdef HAVE_cc0\n+      /* If this was a conditional jump, we need to also delete\n+\t the insn that set cc0.  */\n+      if (only_sets_cc0_p (prev))\n+\t{\n+\t  rtx tmp = prev;\n+\t  prev = prev_nonnote_insn (prev);\n+\t  if (!prev)\n+\t    prev = a->head;\n+\t  del_first = tmp;\n+\t}\n+#endif\n+\n+      a_end = prev;\n+    }\n+  else if (GET_CODE (NEXT_INSN (a_end)) == BARRIER)\n+    del_first = NEXT_INSN (a_end);\n+\n+  /* Delete everything marked above as well as crap that might be\n+     hanging out between the two blocks.  */\n+  flow_delete_insn_chain (del_first, del_last);\n+\n+  /* Normally there should only be one successor of A and that is B, but\n+     partway though the merge of blocks for conditional_execution we'll\n+     be merging a TEST block with THEN and ELSE successors.  Free the\n+     whole lot of them and hope the caller knows what they're doing.  */\n+  while (a->succ)\n+    remove_edge (a->succ);\n+\n+  /* Adjust the edges out of B for the new owner.  */\n+  for (e = b->succ; e; e = e->succ_next)\n+    e->src = a;\n+  a->succ = b->succ;\n+\n+  /* B hasn't quite yet ceased to exist.  Attempt to prevent mishap.  */\n+  b->pred = b->succ = NULL;\n+\n+  /* Reassociate the insns of B with A.  */\n+  if (!b_empty)\n+    {\n+      if (basic_block_for_insn)\n+\t{\n+\t  BLOCK_FOR_INSN (b_head) = a;\n+\t  while (b_head != b_end)\n+\t    {\n+\t      b_head = NEXT_INSN (b_head);\n+\t      BLOCK_FOR_INSN (b_head) = a;\n+\t    }\n+\t}\n+      a_end = b_end;\n+    }\n+  a->end = a_end;\n+\n+  expunge_block (b);\n+}\n+\f\n+/* Return label in the head of basic block.  Create one if it doesn't exist.  */\n+\n+rtx\n+block_label (block)\n+     basic_block block;\n+{\n+  if (block == EXIT_BLOCK_PTR)\n+    return NULL_RTX;\n+  if (GET_CODE (block->head) != CODE_LABEL)\n+    {\n+      block->head = emit_label_before (gen_label_rtx (), block->head);\n+      if (basic_block_for_insn)\n+\tset_block_for_insn (block->head, block);\n+    }\n+  return block->head;\n+}\n+\n+/* Attempt to perform edge redirection by replacing possibly complex jump\n+   instruction by unconditional jump or removing jump completely.\n+   This can apply only if all edges now point to the same block.\n+\n+   The parameters and return values are equivalent to redirect_edge_and_branch.\n+ */\n+\n+static bool\n+try_redirect_by_replacing_jump (e, target)\n+     edge e;\n+     basic_block target;\n+{\n+  basic_block src = e->src;\n+  rtx insn = src->end, kill_from;\n+  edge tmp;\n+  rtx set;\n+  int fallthru = 0;\n+\n+  /* Verify that all targets will be TARGET.  */\n+  for (tmp = src->succ; tmp; tmp = tmp->succ_next)\n+    if (tmp->dest != target && tmp != e)\n+      break;\n+  if (tmp || !onlyjump_p (insn))\n+    return false;\n+\n+  /* Avoid removing branch with side effects.  */\n+  set = single_set (insn);\n+  if (!set || side_effects_p (set))\n+    return false;\n+\n+  /* In case we zap a conditional jump, we'll need to kill\n+     the cc0 setter too.  */\n+  kill_from = insn;\n+#ifdef HAVE_cc0\n+  if (reg_mentioned_p (cc0_rtx, PATTERN (insn)))\n+    kill_from = PREV_INSN (insn);\n+#endif\n+\n+  /* See if we can create the fallthru edge.  */\n+  if (can_fallthru (src, target))\n+    {\n+      src->end = PREV_INSN (kill_from);\n+      if (rtl_dump_file)\n+\tfprintf (rtl_dump_file, \"Removing jump %i.\\n\", INSN_UID (insn));\n+      fallthru = 1;\n+\n+      /* Selectivly unlink whole insn chain.  */\n+      flow_delete_insn_chain (kill_from, PREV_INSN (target->head));\n+    }\n+  /* If this already is simplejump, redirect it.  */\n+  else if (simplejump_p (insn))\n+    {\n+      if (e->dest == target)\n+\treturn false;\n+      if (rtl_dump_file)\n+\tfprintf (rtl_dump_file, \"Redirecting jump %i from %i to %i.\\n\",\n+\t\t INSN_UID (insn), e->dest->index, target->index);\n+      redirect_jump (insn, block_label (target), 0);\n+    }\n+  /* Or replace possibly complicated jump insn by simple jump insn.  */\n+  else\n+    {\n+      rtx target_label = block_label (target);\n+      rtx barrier;\n+\n+      src->end = emit_jump_insn_before (gen_jump (target_label), kill_from);\n+      JUMP_LABEL (src->end) = target_label;\n+      LABEL_NUSES (target_label)++;\n+      if (basic_block_for_insn)\n+\tset_block_for_new_insns (src->end, src);\n+      if (rtl_dump_file)\n+\tfprintf (rtl_dump_file, \"Replacing insn %i by jump %i\\n\",\n+\t\t INSN_UID (insn), INSN_UID (src->end));\n+\n+      flow_delete_insn_chain (kill_from, insn);\n+\n+      barrier = next_nonnote_insn (src->end);\n+      if (!barrier || GET_CODE (barrier) != BARRIER)\n+\temit_barrier_after (src->end);\n+    }\n+\n+  /* Keep only one edge out and set proper flags.  */\n+  while (src->succ->succ_next)\n+    remove_edge (src->succ);\n+  e = src->succ;\n+  if (fallthru)\n+    e->flags = EDGE_FALLTHRU;\n+  else\n+    e->flags = 0;\n+  e->probability = REG_BR_PROB_BASE;\n+  e->count = src->count;\n+\n+  /* We don't want a block to end on a line-number note since that has\n+     the potential of changing the code between -g and not -g.  */\n+  while (GET_CODE (e->src->end) == NOTE\n+\t && NOTE_LINE_NUMBER (e->src->end) >= 0)\n+    {\n+      rtx prev = PREV_INSN (e->src->end);\n+      flow_delete_insn (e->src->end);\n+      e->src->end = prev;\n+    }\n+\n+  if (e->dest != target)\n+    redirect_edge_succ (e, target);\n+  return true;\n+}\n+\n+/* Return last loop_beg note appearing after INSN, before start of next\n+   basic block.  Return INSN if there are no such notes.\n+\n+   When emmiting jump to redirect an fallthru edge, it should always\n+   appear after the LOOP_BEG notes, as loop optimizer expect loop to\n+   eighter start by fallthru edge or jump following the LOOP_BEG note\n+   jumping to the loop exit test.  */\n+\n+static rtx\n+last_loop_beg_note (insn)\n+     rtx insn;\n+{\n+  rtx last = insn;\n+  insn = NEXT_INSN (insn);\n+  while (GET_CODE (insn) == NOTE\n+\t && NOTE_LINE_NUMBER (insn) != NOTE_INSN_BASIC_BLOCK)\n+    {\n+      if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_BEG)\n+\tlast = insn;\n+      insn = NEXT_INSN (insn);\n+    }\n+  return last;\n+}\n+\n+/* Attempt to change code to redirect edge E to TARGET.\n+   Don't do that on expense of adding new instructions or reordering\n+   basic blocks.\n+\n+   Function can be also called with edge destionation equivalent to the\n+   TARGET.  Then it should try the simplifications and do nothing if\n+   none is possible.\n+\n+   Return true if transformation suceeded.  We still return flase in case\n+   E already destinated TARGET and we didn't managed to simplify instruction\n+   stream.  */\n+\n+bool\n+redirect_edge_and_branch (e, target)\n+     edge e;\n+     basic_block target;\n+{\n+  rtx tmp;\n+  rtx old_label = e->dest->head;\n+  basic_block src = e->src;\n+  rtx insn = src->end;\n+\n+  if (e->flags & EDGE_COMPLEX)\n+    return false;\n+\n+  if (try_redirect_by_replacing_jump (e, target))\n+    return true;\n+  /* Do this fast path late, as we want above code to simplify for cases\n+     where called on single edge leaving basic block containing nontrivial\n+     jump insn.  */\n+  else if (e->dest == target)\n+    return false;\n+\n+  /* We can only redirect non-fallthru edges of jump insn.  */\n+  if (e->flags & EDGE_FALLTHRU)\n+    return false;\n+  if (GET_CODE (insn) != JUMP_INSN)\n+    return false;\n+\n+  /* Recognize a tablejump and adjust all matching cases.  */\n+  if ((tmp = JUMP_LABEL (insn)) != NULL_RTX\n+      && (tmp = NEXT_INSN (tmp)) != NULL_RTX\n+      && GET_CODE (tmp) == JUMP_INSN\n+      && (GET_CODE (PATTERN (tmp)) == ADDR_VEC\n+\t  || GET_CODE (PATTERN (tmp)) == ADDR_DIFF_VEC))\n+    {\n+      rtvec vec;\n+      int j;\n+      rtx new_label = block_label (target);\n+\n+      if (GET_CODE (PATTERN (tmp)) == ADDR_VEC)\n+\tvec = XVEC (PATTERN (tmp), 0);\n+      else\n+\tvec = XVEC (PATTERN (tmp), 1);\n+\n+      for (j = GET_NUM_ELEM (vec) - 1; j >= 0; --j)\n+\tif (XEXP (RTVEC_ELT (vec, j), 0) == old_label)\n+\t  {\n+\t    RTVEC_ELT (vec, j) = gen_rtx_LABEL_REF (Pmode, new_label);\n+\t    --LABEL_NUSES (old_label);\n+\t    ++LABEL_NUSES (new_label);\n+\t  }\n+\n+      /* Handle casesi dispatch insns */\n+      if ((tmp = single_set (insn)) != NULL\n+\t  && SET_DEST (tmp) == pc_rtx\n+\t  && GET_CODE (SET_SRC (tmp)) == IF_THEN_ELSE\n+\t  && GET_CODE (XEXP (SET_SRC (tmp), 2)) == LABEL_REF\n+\t  && XEXP (XEXP (SET_SRC (tmp), 2), 0) == old_label)\n+\t{\n+\t  XEXP (SET_SRC (tmp), 2) = gen_rtx_LABEL_REF (VOIDmode,\n+\t\t\t\t\t\t       new_label);\n+\t  --LABEL_NUSES (old_label);\n+\t  ++LABEL_NUSES (new_label);\n+\t}\n+    }\n+  else\n+    {\n+      /* ?? We may play the games with moving the named labels from\n+\t one basic block to the other in case only one computed_jump is\n+\t available.  */\n+      if (computed_jump_p (insn))\n+\treturn false;\n+\n+      /* A return instruction can't be redirected.  */\n+      if (returnjump_p (insn))\n+\treturn false;\n+\n+      /* If the insn doesn't go where we think, we're confused.  */\n+      if (JUMP_LABEL (insn) != old_label)\n+\tabort ();\n+      redirect_jump (insn, block_label (target), 0);\n+    }\n+\n+  if (rtl_dump_file)\n+    fprintf (rtl_dump_file, \"Edge %i->%i redirected to %i\\n\",\n+\t     e->src->index, e->dest->index, target->index);\n+  if (e->dest != target)\n+    redirect_edge_succ_nodup (e, target);\n+  return true;\n+}\n+\n+/* Redirect edge even at the expense of creating new jump insn or\n+   basic block.  Return new basic block if created, NULL otherwise.\n+   Abort if converison is impossible.  */\n+\n+basic_block\n+redirect_edge_and_branch_force (e, target)\n+     edge e;\n+     basic_block target;\n+{\n+  basic_block new_bb;\n+  edge new_edge;\n+  rtx label;\n+  rtx bb_note;\n+  int i, j;\n+\n+  if (redirect_edge_and_branch (e, target))\n+    return NULL;\n+  if (e->dest == target)\n+    return NULL;\n+  if (e->flags & EDGE_ABNORMAL)\n+    abort ();\n+  if (!(e->flags & EDGE_FALLTHRU))\n+    abort ();\n+\n+  e->flags &= ~EDGE_FALLTHRU;\n+  label = block_label (target);\n+  /* Case of the fallthru block.  */\n+  if (!e->src->succ->succ_next)\n+    {\n+      e->src->end = emit_jump_insn_after (gen_jump (label),\n+\t\t\t\t\t  last_loop_beg_note (e->src->end));\n+      JUMP_LABEL (e->src->end) = label;\n+      LABEL_NUSES (label)++;\n+      if (basic_block_for_insn)\n+\tset_block_for_new_insns (e->src->end, e->src);\n+      emit_barrier_after (e->src->end);\n+      if (rtl_dump_file)\n+\tfprintf (rtl_dump_file,\n+\t\t \"Emitting jump insn %i to redirect edge %i->%i to %i\\n\",\n+\t\t INSN_UID (e->src->end), e->src->index, e->dest->index,\n+\t\t target->index);\n+      redirect_edge_succ (e, target);\n+      return NULL;\n+    }\n+  /* Redirecting fallthru edge of the conditional needs extra work.  */\n+\n+  if (rtl_dump_file)\n+    fprintf (rtl_dump_file,\n+\t     \"Emitting jump insn %i in new BB to redirect edge %i->%i to %i\\n\",\n+\t     INSN_UID (e->src->end), e->src->index, e->dest->index,\n+\t     target->index);\n+\n+  /* Create the new structures.  */\n+  new_bb = (basic_block) obstack_alloc (&flow_obstack, sizeof (*new_bb));\n+  new_edge = (edge) xcalloc (1, sizeof (*new_edge));\n+  n_edges++;\n+\n+  memset (new_bb, 0, sizeof (*new_bb));\n+\n+  new_bb->end = new_bb->head = last_loop_beg_note (e->src->end);\n+  new_bb->succ = NULL;\n+  new_bb->pred = new_edge;\n+  new_bb->count = e->count;\n+  new_bb->frequency = EDGE_FREQUENCY (e);\n+  new_bb->loop_depth = e->dest->loop_depth;\n+\n+  new_edge->flags = EDGE_FALLTHRU;\n+  new_edge->probability = e->probability;\n+  new_edge->count = e->count;\n+\n+  if (target->global_live_at_start)\n+    {\n+      new_bb->global_live_at_start = OBSTACK_ALLOC_REG_SET (&flow_obstack);\n+      new_bb->global_live_at_end = OBSTACK_ALLOC_REG_SET (&flow_obstack);\n+      COPY_REG_SET (new_bb->global_live_at_start,\n+\t\t    target->global_live_at_start);\n+      COPY_REG_SET (new_bb->global_live_at_end, new_bb->global_live_at_start);\n+    }\n+\n+  /* Wire edge in.  */\n+  new_edge->src = e->src;\n+  new_edge->dest = new_bb;\n+  new_edge->succ_next = e->src->succ;\n+  e->src->succ = new_edge;\n+  new_edge->pred_next = NULL;\n+\n+  /* Redirect old edge.  */\n+  redirect_edge_succ (e, target);\n+  redirect_edge_pred (e, new_bb);\n+  e->probability = REG_BR_PROB_BASE;\n+\n+  /* Place the new block just after the block being split.  */\n+  VARRAY_GROW (basic_block_info, ++n_basic_blocks);\n+\n+  /* Some parts of the compiler expect blocks to be number in\n+     sequential order so insert the new block immediately after the\n+     block being split..  */\n+  j = new_edge->src->index;\n+  for (i = n_basic_blocks - 1; i > j + 1; --i)\n+    {\n+      basic_block tmp = BASIC_BLOCK (i - 1);\n+      BASIC_BLOCK (i) = tmp;\n+      tmp->index = i;\n+    }\n+\n+  BASIC_BLOCK (i) = new_bb;\n+  new_bb->index = i;\n+\n+  /* Create the basic block note.  */\n+  bb_note = emit_note_after (NOTE_INSN_BASIC_BLOCK, new_bb->head);\n+  NOTE_BASIC_BLOCK (bb_note) = new_bb;\n+  new_bb->head = bb_note;\n+\n+  new_bb->end = emit_jump_insn_after (gen_jump (label), new_bb->head);\n+  JUMP_LABEL (new_bb->end) = label;\n+  LABEL_NUSES (label)++;\n+  if (basic_block_for_insn)\n+    set_block_for_new_insns (new_bb->end, new_bb);\n+  emit_barrier_after (new_bb->end);\n+  return new_bb;\n+}\n+\n+/* The given edge should potentially be a fallthru edge.  If that is in\n+   fact true, delete the jump and barriers that are in the way.  */\n+\n+void\n+tidy_fallthru_edge (e, b, c)\n+     edge e;\n+     basic_block b, c;\n+{\n+  rtx q;\n+\n+  /* ??? In a late-running flow pass, other folks may have deleted basic\n+     blocks by nopping out blocks, leaving multiple BARRIERs between here\n+     and the target label. They ought to be chastized and fixed.\n+\n+     We can also wind up with a sequence of undeletable labels between\n+     one block and the next.\n+\n+     So search through a sequence of barriers, labels, and notes for\n+     the head of block C and assert that we really do fall through.  */\n+\n+  if (next_real_insn (b->end) != next_real_insn (PREV_INSN (c->head)))\n+    return;\n+\n+  /* Remove what will soon cease being the jump insn from the source block.\n+     If block B consisted only of this single jump, turn it into a deleted\n+     note.  */\n+  q = b->end;\n+  if (GET_CODE (q) == JUMP_INSN\n+      && onlyjump_p (q)\n+      && (any_uncondjump_p (q)\n+\t  || (b->succ == e && e->succ_next == NULL)))\n+    {\n+#ifdef HAVE_cc0\n+      /* If this was a conditional jump, we need to also delete\n+\t the insn that set cc0.  */\n+      if (any_condjump_p (q) && only_sets_cc0_p (PREV_INSN (q)))\n+\tq = PREV_INSN (q);\n+#endif\n+\n+      if (b->head == q)\n+\t{\n+\t  PUT_CODE (q, NOTE);\n+\t  NOTE_LINE_NUMBER (q) = NOTE_INSN_DELETED;\n+\t  NOTE_SOURCE_FILE (q) = 0;\n+\t}\n+      else\n+\t{\n+\t  q = PREV_INSN (q);\n+\n+\t  /* We don't want a block to end on a line-number note since that has\n+\t     the potential of changing the code between -g and not -g.  */\n+\t  while (GET_CODE (q) == NOTE && NOTE_LINE_NUMBER (q) >= 0)\n+\t    q = PREV_INSN (q);\n+\t}\n+\n+      b->end = q;\n+    }\n+\n+  /* Selectively unlink the sequence.  */\n+  if (q != PREV_INSN (c->head))\n+    flow_delete_insn_chain (NEXT_INSN (q), PREV_INSN (c->head));\n+\n+  e->flags |= EDGE_FALLTHRU;\n+}\n+\n+/* Fix up edges that now fall through, or rather should now fall through\n+   but previously required a jump around now deleted blocks.  Simplify\n+   the search by only examining blocks numerically adjacent, since this\n+   is how find_basic_blocks created them.  */\n+\n+void\n+tidy_fallthru_edges ()\n+{\n+  int i;\n+\n+  for (i = 1; i < n_basic_blocks; ++i)\n+    {\n+      basic_block b = BASIC_BLOCK (i - 1);\n+      basic_block c = BASIC_BLOCK (i);\n+      edge s;\n+\n+      /* We care about simple conditional or unconditional jumps with\n+\t a single successor.\n+\n+\t If we had a conditional branch to the next instruction when\n+\t find_basic_blocks was called, then there will only be one\n+\t out edge for the block which ended with the conditional\n+\t branch (since we do not create duplicate edges).\n+\n+\t Furthermore, the edge will be marked as a fallthru because we\n+\t merge the flags for the duplicate edges.  So we do not want to\n+\t check that the edge is not a FALLTHRU edge.  */\n+      if ((s = b->succ) != NULL\n+\t  && ! (s->flags & EDGE_COMPLEX)\n+\t  && s->succ_next == NULL\n+\t  && s->dest == c\n+\t  /* If the jump insn has side effects, we can't tidy the edge.  */\n+\t  && (GET_CODE (b->end) != JUMP_INSN\n+\t      || onlyjump_p (b->end)))\n+\ttidy_fallthru_edge (s, b, c);\n+    }\n+}\n+\f\n+/* Helper function for split_edge.  Return true in case edge BB2 to BB1\n+   is back edge of syntactic loop.  */\n+\n+static bool\n+back_edge_of_syntactic_loop_p (bb1, bb2)\n+\tbasic_block bb1, bb2;\n+{\n+  rtx insn;\n+  int count = 0;\n+\n+  if (bb1->index > bb2->index)\n+    return false;\n+\n+  if (bb1->index == bb2->index)\n+    return true;\n+\n+  for (insn = bb1->end; insn != bb2->head && count >= 0;\n+       insn = NEXT_INSN (insn))\n+    if (GET_CODE (insn) == NOTE)\n+      {\n+\tif (NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_BEG)\n+\t  count++;\n+\tif (NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_END)\n+\t  count--;\n+      }\n+\n+  return count >= 0;\n+}\n+\n+/* Split a (typically critical) edge.  Return the new block.\n+   Abort on abnormal edges.\n+\n+   ??? The code generally expects to be called on critical edges.\n+   The case of a block ending in an unconditional jump to a\n+   block with multiple predecessors is not handled optimally.  */\n+\n+basic_block\n+split_edge (edge_in)\n+     edge edge_in;\n+{\n+  basic_block old_pred, bb, old_succ;\n+  edge edge_out;\n+  rtx bb_note;\n+  int i, j;\n+\n+  /* Abnormal edges cannot be split.  */\n+  if ((edge_in->flags & EDGE_ABNORMAL) != 0)\n+    abort ();\n+\n+  old_pred = edge_in->src;\n+  old_succ = edge_in->dest;\n+\n+  /* Create the new structures.  */\n+  bb = (basic_block) obstack_alloc (&flow_obstack, sizeof (*bb));\n+  edge_out = (edge) xcalloc (1, sizeof (*edge_out));\n+  n_edges++;\n+\n+  memset (bb, 0, sizeof (*bb));\n+\n+  /* ??? This info is likely going to be out of date very soon.  */\n+  if (old_succ->global_live_at_start)\n+    {\n+      bb->global_live_at_start = OBSTACK_ALLOC_REG_SET (&flow_obstack);\n+      bb->global_live_at_end = OBSTACK_ALLOC_REG_SET (&flow_obstack);\n+      COPY_REG_SET (bb->global_live_at_start, old_succ->global_live_at_start);\n+      COPY_REG_SET (bb->global_live_at_end, old_succ->global_live_at_start);\n+    }\n+\n+  /* Wire them up.  */\n+  bb->succ = edge_out;\n+  bb->count = edge_in->count;\n+  bb->frequency = EDGE_FREQUENCY (edge_in);\n+\n+  edge_in->flags &= ~EDGE_CRITICAL;\n+\n+  edge_out->pred_next = old_succ->pred;\n+  edge_out->succ_next = NULL;\n+  edge_out->src = bb;\n+  edge_out->dest = old_succ;\n+  edge_out->flags = EDGE_FALLTHRU;\n+  edge_out->probability = REG_BR_PROB_BASE;\n+  edge_out->count = edge_in->count;\n+\n+  old_succ->pred = edge_out;\n+\n+  /* Tricky case -- if there existed a fallthru into the successor\n+     (and we're not it) we must add a new unconditional jump around\n+     the new block we're actually interested in.\n+\n+     Further, if that edge is critical, this means a second new basic\n+     block must be created to hold it.  In order to simplify correct\n+     insn placement, do this before we touch the existing basic block\n+     ordering for the block we were really wanting.  */\n+  if ((edge_in->flags & EDGE_FALLTHRU) == 0)\n+    {\n+      edge e;\n+      for (e = edge_out->pred_next; e; e = e->pred_next)\n+\tif (e->flags & EDGE_FALLTHRU)\n+\t  break;\n+\n+      if (e)\n+\t{\n+\t  basic_block jump_block;\n+\t  rtx pos;\n+\n+\t  if ((e->flags & EDGE_CRITICAL) == 0\n+\t      && e->src != ENTRY_BLOCK_PTR)\n+\t    {\n+\t      /* Non critical -- we can simply add a jump to the end\n+\t\t of the existing predecessor.  */\n+\t      jump_block = e->src;\n+\t    }\n+\t  else\n+\t    {\n+\t      /* We need a new block to hold the jump.  The simplest\n+\t         way to do the bulk of the work here is to recursively\n+\t         call ourselves.  */\n+\t      jump_block = split_edge (e);\n+\t      e = jump_block->succ;\n+\t    }\n+\n+\t  /* Now add the jump insn ...  */\n+\t  pos = emit_jump_insn_after (gen_jump (old_succ->head),\n+\t\t\t\t      last_loop_beg_note (jump_block->end));\n+\t  jump_block->end = pos;\n+\t  if (basic_block_for_insn)\n+\t    set_block_for_new_insns (pos, jump_block);\n+\t  emit_barrier_after (pos);\n+\n+\t  /* ... let jump know that label is in use, ...  */\n+\t  JUMP_LABEL (pos) = old_succ->head;\n+\t  ++LABEL_NUSES (old_succ->head);\n+\n+\t  /* ... and clear fallthru on the outgoing edge.  */\n+\t  e->flags &= ~EDGE_FALLTHRU;\n+\n+\t  /* Continue splitting the interesting edge.  */\n+\t}\n+    }\n+\n+  /* Place the new block just in front of the successor.  */\n+  VARRAY_GROW (basic_block_info, ++n_basic_blocks);\n+  if (old_succ == EXIT_BLOCK_PTR)\n+    j = n_basic_blocks - 1;\n+  else\n+    j = old_succ->index;\n+  for (i = n_basic_blocks - 1; i > j; --i)\n+    {\n+      basic_block tmp = BASIC_BLOCK (i - 1);\n+      BASIC_BLOCK (i) = tmp;\n+      tmp->index = i;\n+    }\n+  BASIC_BLOCK (i) = bb;\n+  bb->index = i;\n+\n+  /* Create the basic block note.\n+\n+     Where we place the note can have a noticable impact on the generated\n+     code.  Consider this cfg:\n+\n+\t\t        E\n+\t\t\t|\n+\t\t\t0\n+\t\t       / \\\n+\t\t   +->1-->2--->E\n+                   |  |\n+\t\t   +--+\n+\n+      If we need to insert an insn on the edge from block 0 to block 1,\n+      we want to ensure the instructions we insert are outside of any\n+      loop notes that physically sit between block 0 and block 1.  Otherwise\n+      we confuse the loop optimizer into thinking the loop is a phony.  */\n+  if (old_succ != EXIT_BLOCK_PTR\n+      && PREV_INSN (old_succ->head)\n+      && GET_CODE (PREV_INSN (old_succ->head)) == NOTE\n+      && NOTE_LINE_NUMBER (PREV_INSN (old_succ->head)) == NOTE_INSN_LOOP_BEG\n+      && !back_edge_of_syntactic_loop_p (old_succ, old_pred))\n+    bb_note = emit_note_before (NOTE_INSN_BASIC_BLOCK,\n+\t\t\t\tPREV_INSN (old_succ->head));\n+  else if (old_succ != EXIT_BLOCK_PTR)\n+    bb_note = emit_note_before (NOTE_INSN_BASIC_BLOCK, old_succ->head);\n+  else\n+    bb_note = emit_note_after (NOTE_INSN_BASIC_BLOCK, get_last_insn ());\n+  NOTE_BASIC_BLOCK (bb_note) = bb;\n+  bb->head = bb->end = bb_note;\n+\n+  /* For non-fallthry edges, we must adjust the predecessor's\n+     jump instruction to target our new block.  */\n+  if ((edge_in->flags & EDGE_FALLTHRU) == 0)\n+    {\n+      if (!redirect_edge_and_branch (edge_in, bb))\n+\tabort ();\n+    }\n+  else\n+    redirect_edge_succ (edge_in, bb);\n+\n+  return bb;\n+}\n+\n+/* Queue instructions for insertion on an edge between two basic blocks.\n+   The new instructions and basic blocks (if any) will not appear in the\n+   CFG until commit_edge_insertions is called.  */\n+\n+void\n+insert_insn_on_edge (pattern, e)\n+     rtx pattern;\n+     edge e;\n+{\n+  /* We cannot insert instructions on an abnormal critical edge.\n+     It will be easier to find the culprit if we die now.  */\n+  if ((e->flags & (EDGE_ABNORMAL|EDGE_CRITICAL))\n+      == (EDGE_ABNORMAL|EDGE_CRITICAL))\n+    abort ();\n+\n+  if (e->insns == NULL_RTX)\n+    start_sequence ();\n+  else\n+    push_to_sequence (e->insns);\n+\n+  emit_insn (pattern);\n+\n+  e->insns = get_insns ();\n+  end_sequence ();\n+}\n+\n+/* Update the CFG for the instructions queued on edge E.  */\n+\n+static void\n+commit_one_edge_insertion (e)\n+     edge e;\n+{\n+  rtx before = NULL_RTX, after = NULL_RTX, insns, tmp, last;\n+  basic_block bb;\n+\n+  /* Pull the insns off the edge now since the edge might go away.  */\n+  insns = e->insns;\n+  e->insns = NULL_RTX;\n+\n+  /* Figure out where to put these things.  If the destination has\n+     one predecessor, insert there.  Except for the exit block.  */\n+  if (e->dest->pred->pred_next == NULL\n+      && e->dest != EXIT_BLOCK_PTR)\n+    {\n+      bb = e->dest;\n+\n+      /* Get the location correct wrt a code label, and \"nice\" wrt\n+\t a basic block note, and before everything else.  */\n+      tmp = bb->head;\n+      if (GET_CODE (tmp) == CODE_LABEL)\n+\ttmp = NEXT_INSN (tmp);\n+      if (NOTE_INSN_BASIC_BLOCK_P (tmp))\n+\ttmp = NEXT_INSN (tmp);\n+      if (tmp == bb->head)\n+\tbefore = tmp;\n+      else\n+\tafter = PREV_INSN (tmp);\n+    }\n+\n+  /* If the source has one successor and the edge is not abnormal,\n+     insert there.  Except for the entry block.  */\n+  else if ((e->flags & EDGE_ABNORMAL) == 0\n+\t   && e->src->succ->succ_next == NULL\n+\t   && e->src != ENTRY_BLOCK_PTR)\n+    {\n+      bb = e->src;\n+      /* It is possible to have a non-simple jump here.  Consider a target\n+\t where some forms of unconditional jumps clobber a register.  This\n+\t happens on the fr30 for example.\n+\n+\t We know this block has a single successor, so we can just emit\n+\t the queued insns before the jump.  */\n+      if (GET_CODE (bb->end) == JUMP_INSN)\n+\t{\n+\t  before = bb->end;\n+\t  while (GET_CODE (PREV_INSN (before)) == NOTE\n+\t\t && NOTE_LINE_NUMBER (PREV_INSN (before)) == NOTE_INSN_LOOP_BEG)\n+\t    before = PREV_INSN (before);\n+\t}\n+      else\n+\t{\n+\t  /* We'd better be fallthru, or we've lost track of what's what.  */\n+\t  if ((e->flags & EDGE_FALLTHRU) == 0)\n+\t    abort ();\n+\n+\t  after = bb->end;\n+\t}\n+    }\n+\n+  /* Otherwise we must split the edge.  */\n+  else\n+    {\n+      bb = split_edge (e);\n+      after = bb->end;\n+    }\n+\n+  /* Now that we've found the spot, do the insertion.  */\n+\n+  /* Set the new block number for these insns, if structure is allocated.  */\n+  if (basic_block_for_insn)\n+    {\n+      rtx i;\n+      for (i = insns; i != NULL_RTX; i = NEXT_INSN (i))\n+\tset_block_for_insn (i, bb);\n+    }\n+\n+  if (before)\n+    {\n+      emit_insns_before (insns, before);\n+      if (before == bb->head)\n+\tbb->head = insns;\n+\n+      last = prev_nonnote_insn (before);\n+    }\n+  else\n+    {\n+      last = emit_insns_after (insns, after);\n+      if (after == bb->end)\n+\tbb->end = last;\n+    }\n+\n+  if (returnjump_p (last))\n+    {\n+      /* ??? Remove all outgoing edges from BB and add one for EXIT.\n+         This is not currently a problem because this only happens\n+\t for the (single) epilogue, which already has a fallthru edge\n+\t to EXIT.  */\n+\n+      e = bb->succ;\n+      if (e->dest != EXIT_BLOCK_PTR\n+\t  || e->succ_next != NULL\n+\t  || (e->flags & EDGE_FALLTHRU) == 0)\n+\tabort ();\n+      e->flags &= ~EDGE_FALLTHRU;\n+\n+      emit_barrier_after (last);\n+      bb->end = last;\n+\n+      if (before)\n+\tflow_delete_insn (before);\n+    }\n+  else if (GET_CODE (last) == JUMP_INSN)\n+    abort ();\n+  find_sub_basic_blocks (bb);\n+}\n+\n+/* Update the CFG for all queued instructions.  */\n+\n+void\n+commit_edge_insertions ()\n+{\n+  int i;\n+  basic_block bb;\n+  compute_bb_for_insn (get_max_uid ());\n+\n+#ifdef ENABLE_CHECKING\n+  verify_flow_info ();\n+#endif\n+\n+  i = -1;\n+  bb = ENTRY_BLOCK_PTR;\n+  while (1)\n+    {\n+      edge e, next;\n+\n+      for (e = bb->succ; e; e = next)\n+\t{\n+\t  next = e->succ_next;\n+\t  if (e->insns)\n+\t    commit_one_edge_insertion (e);\n+\t}\n+\n+      if (++i >= n_basic_blocks)\n+\tbreak;\n+      bb = BASIC_BLOCK (i);\n+    }\n+}\n+\f\n+void\n+dump_flow_info (file)\n+     FILE *file;\n+{\n+  register int i;\n+  static const char * const reg_class_names[] = REG_CLASS_NAMES;\n+\n+  fprintf (file, \"%d registers.\\n\", max_regno);\n+  for (i = FIRST_PSEUDO_REGISTER; i < max_regno; i++)\n+    if (REG_N_REFS (i))\n+      {\n+\tenum reg_class class, altclass;\n+\tfprintf (file, \"\\nRegister %d used %d times across %d insns\",\n+\t\t i, REG_N_REFS (i), REG_LIVE_LENGTH (i));\n+\tif (REG_BASIC_BLOCK (i) >= 0)\n+\t  fprintf (file, \" in block %d\", REG_BASIC_BLOCK (i));\n+\tif (REG_N_SETS (i))\n+\t  fprintf (file, \"; set %d time%s\", REG_N_SETS (i),\n+\t\t   (REG_N_SETS (i) == 1) ? \"\" : \"s\");\n+\tif (REG_USERVAR_P (regno_reg_rtx[i]))\n+\t  fprintf (file, \"; user var\");\n+\tif (REG_N_DEATHS (i) != 1)\n+\t  fprintf (file, \"; dies in %d places\", REG_N_DEATHS (i));\n+\tif (REG_N_CALLS_CROSSED (i) == 1)\n+\t  fprintf (file, \"; crosses 1 call\");\n+\telse if (REG_N_CALLS_CROSSED (i))\n+\t  fprintf (file, \"; crosses %d calls\", REG_N_CALLS_CROSSED (i));\n+\tif (PSEUDO_REGNO_BYTES (i) != UNITS_PER_WORD)\n+\t  fprintf (file, \"; %d bytes\", PSEUDO_REGNO_BYTES (i));\n+\tclass = reg_preferred_class (i);\n+\taltclass = reg_alternate_class (i);\n+\tif (class != GENERAL_REGS || altclass != ALL_REGS)\n+\t  {\n+\t    if (altclass == ALL_REGS || class == ALL_REGS)\n+\t      fprintf (file, \"; pref %s\", reg_class_names[(int) class]);\n+\t    else if (altclass == NO_REGS)\n+\t      fprintf (file, \"; %s or none\", reg_class_names[(int) class]);\n+\t    else\n+\t      fprintf (file, \"; pref %s, else %s\",\n+\t\t       reg_class_names[(int) class],\n+\t\t       reg_class_names[(int) altclass]);\n+\t  }\n+\tif (REG_POINTER (regno_reg_rtx[i]))\n+\t  fprintf (file, \"; pointer\");\n+\tfprintf (file, \".\\n\");\n+      }\n+\n+  fprintf (file, \"\\n%d basic blocks, %d edges.\\n\", n_basic_blocks, n_edges);\n+  for (i = 0; i < n_basic_blocks; i++)\n+    {\n+      register basic_block bb = BASIC_BLOCK (i);\n+      register edge e;\n+\n+      fprintf (file, \"\\nBasic block %d: first insn %d, last %d, loop_depth %d, count \",\n+\t       i, INSN_UID (bb->head), INSN_UID (bb->end), bb->loop_depth);\n+      fprintf (file, HOST_WIDEST_INT_PRINT_DEC, (HOST_WIDEST_INT) bb->count);\n+      fprintf (file, \", freq %i.\\n\", bb->frequency);\n+\n+      fprintf (file, \"Predecessors: \");\n+      for (e = bb->pred; e; e = e->pred_next)\n+\tdump_edge_info (file, e, 0);\n+\n+      fprintf (file, \"\\nSuccessors: \");\n+      for (e = bb->succ; e; e = e->succ_next)\n+\tdump_edge_info (file, e, 1);\n+\n+      fprintf (file, \"\\nRegisters live at start:\");\n+      dump_regset (bb->global_live_at_start, file);\n+\n+      fprintf (file, \"\\nRegisters live at end:\");\n+      dump_regset (bb->global_live_at_end, file);\n+\n+      putc ('\\n', file);\n+    }\n+\n+  putc ('\\n', file);\n+}\n+\n+void\n+debug_flow_info ()\n+{\n+  dump_flow_info (stderr);\n+}\n+\n+void\n+dump_edge_info (file, e, do_succ)\n+     FILE *file;\n+     edge e;\n+     int do_succ;\n+{\n+  basic_block side = (do_succ ? e->dest : e->src);\n+\n+  if (side == ENTRY_BLOCK_PTR)\n+    fputs (\" ENTRY\", file);\n+  else if (side == EXIT_BLOCK_PTR)\n+    fputs (\" EXIT\", file);\n+  else\n+    fprintf (file, \" %d\", side->index);\n+\n+  if (e->probability)\n+    fprintf (file, \" [%.1f%%] \", e->probability * 100.0 / REG_BR_PROB_BASE);\n+\n+  if (e->count)\n+    {\n+      fprintf (file, \" count:\");\n+      fprintf (file, HOST_WIDEST_INT_PRINT_DEC, (HOST_WIDEST_INT) e->count);\n+    }\n+\n+  if (e->flags)\n+    {\n+      static const char * const bitnames[] = {\n+\t\"fallthru\", \"crit\", \"ab\", \"abcall\", \"eh\", \"fake\", \"dfs_back\"\n+      };\n+      int comma = 0;\n+      int i, flags = e->flags;\n+\n+      fputc (' ', file);\n+      fputc ('(', file);\n+      for (i = 0; flags; i++)\n+\tif (flags & (1 << i))\n+\t  {\n+\t    flags &= ~(1 << i);\n+\n+\t    if (comma)\n+\t      fputc (',', file);\n+\t    if (i < (int) ARRAY_SIZE (bitnames))\n+\t      fputs (bitnames[i], file);\n+\t    else\n+\t      fprintf (file, \"%d\", i);\n+\t    comma = 1;\n+\t  }\n+      fputc (')', file);\n+    }\n+}\n+\f\n+/* Print out one basic block with live information at start and end.  */\n+\n+void\n+dump_bb (bb, outf)\n+     basic_block bb;\n+     FILE *outf;\n+{\n+  rtx insn;\n+  rtx last;\n+  edge e;\n+\n+  fprintf (outf, \";; Basic block %d, loop depth %d, count \",\n+\t   bb->index, bb->loop_depth);\n+  fprintf (outf, HOST_WIDEST_INT_PRINT_DEC, (HOST_WIDEST_INT) bb->count);\n+  putc ('\\n', outf);\n+\n+  fputs (\";; Predecessors: \", outf);\n+  for (e = bb->pred; e; e = e->pred_next)\n+    dump_edge_info (outf, e, 0);\n+  putc ('\\n', outf);\n+\n+  fputs (\";; Registers live at start:\", outf);\n+  dump_regset (bb->global_live_at_start, outf);\n+  putc ('\\n', outf);\n+\n+  for (insn = bb->head, last = NEXT_INSN (bb->end);\n+       insn != last;\n+       insn = NEXT_INSN (insn))\n+    print_rtl_single (outf, insn);\n+\n+  fputs (\";; Registers live at end:\", outf);\n+  dump_regset (bb->global_live_at_end, outf);\n+  putc ('\\n', outf);\n+\n+  fputs (\";; Successors: \", outf);\n+  for (e = bb->succ; e; e = e->succ_next)\n+    dump_edge_info (outf, e, 1);\n+  putc ('\\n', outf);\n+}\n+\n+void\n+debug_bb (bb)\n+     basic_block bb;\n+{\n+  dump_bb (bb, stderr);\n+}\n+\n+void\n+debug_bb_n (n)\n+     int n;\n+{\n+  dump_bb (BASIC_BLOCK (n), stderr);\n+}\n+\n+/* Like print_rtl, but also print out live information for the start of each\n+   basic block.  */\n+\n+void\n+print_rtl_with_bb (outf, rtx_first)\n+     FILE *outf;\n+     rtx rtx_first;\n+{\n+  register rtx tmp_rtx;\n+\n+  if (rtx_first == 0)\n+    fprintf (outf, \"(nil)\\n\");\n+  else\n+    {\n+      int i;\n+      enum bb_state { NOT_IN_BB, IN_ONE_BB, IN_MULTIPLE_BB };\n+      int max_uid = get_max_uid ();\n+      basic_block *start = (basic_block *)\n+\txcalloc (max_uid, sizeof (basic_block));\n+      basic_block *end = (basic_block *)\n+\txcalloc (max_uid, sizeof (basic_block));\n+      enum bb_state *in_bb_p = (enum bb_state *)\n+\txcalloc (max_uid, sizeof (enum bb_state));\n+\n+      for (i = n_basic_blocks - 1; i >= 0; i--)\n+\t{\n+\t  basic_block bb = BASIC_BLOCK (i);\n+\t  rtx x;\n+\n+\t  start[INSN_UID (bb->head)] = bb;\n+\t  end[INSN_UID (bb->end)] = bb;\n+\t  for (x = bb->head; x != NULL_RTX; x = NEXT_INSN (x))\n+\t    {\n+\t      enum bb_state state = IN_MULTIPLE_BB;\n+\t      if (in_bb_p[INSN_UID (x)] == NOT_IN_BB)\n+\t\tstate = IN_ONE_BB;\n+\t      in_bb_p[INSN_UID (x)] = state;\n+\n+\t      if (x == bb->end)\n+\t\tbreak;\n+\t    }\n+\t}\n+\n+      for (tmp_rtx = rtx_first; NULL != tmp_rtx; tmp_rtx = NEXT_INSN (tmp_rtx))\n+\t{\n+\t  int did_output;\n+\t  basic_block bb;\n+\n+\t  if ((bb = start[INSN_UID (tmp_rtx)]) != NULL)\n+\t    {\n+\t      fprintf (outf, \";; Start of basic block %d, registers live:\",\n+\t\t       bb->index);\n+\t      dump_regset (bb->global_live_at_start, outf);\n+\t      putc ('\\n', outf);\n+\t    }\n+\n+\t  if (in_bb_p[INSN_UID (tmp_rtx)] == NOT_IN_BB\n+\t      && GET_CODE (tmp_rtx) != NOTE\n+\t      && GET_CODE (tmp_rtx) != BARRIER)\n+\t    fprintf (outf, \";; Insn is not within a basic block\\n\");\n+\t  else if (in_bb_p[INSN_UID (tmp_rtx)] == IN_MULTIPLE_BB)\n+\t    fprintf (outf, \";; Insn is in multiple basic blocks\\n\");\n+\n+\t  did_output = print_rtl_single (outf, tmp_rtx);\n+\n+\t  if ((bb = end[INSN_UID (tmp_rtx)]) != NULL)\n+\t    {\n+\t      fprintf (outf, \";; End of basic block %d, registers live:\\n\",\n+\t\t       bb->index);\n+\t      dump_regset (bb->global_live_at_end, outf);\n+\t      putc ('\\n', outf);\n+\t    }\n+\n+\t  if (did_output)\n+\t    putc ('\\n', outf);\n+\t}\n+\n+      free (start);\n+      free (end);\n+      free (in_bb_p);\n+    }\n+\n+  if (current_function_epilogue_delay_list != 0)\n+    {\n+      fprintf (outf, \"\\n;; Insns in epilogue delay list:\\n\\n\");\n+      for (tmp_rtx = current_function_epilogue_delay_list; tmp_rtx != 0;\n+\t   tmp_rtx = XEXP (tmp_rtx, 1))\n+\tprint_rtl_single (outf, XEXP (tmp_rtx, 0));\n+    }\n+}\n+\f\n+/* Verify the CFG consistency.  This function check some CFG invariants and\n+   aborts when something is wrong.  Hope that this function will help to\n+   convert many optimization passes to preserve CFG consistent.\n+\n+   Currently it does following checks:\n+\n+   - test head/end pointers\n+   - overlapping of basic blocks\n+   - edge list correctness\n+   - headers of basic blocks (the NOTE_INSN_BASIC_BLOCK note)\n+   - tails of basic blocks (ensure that boundary is necesary)\n+   - scans body of the basic block for JUMP_INSN, CODE_LABEL\n+     and NOTE_INSN_BASIC_BLOCK\n+   - check that all insns are in the basic blocks\n+   (except the switch handling code, barriers and notes)\n+   - check that all returns are followed by barriers\n+\n+   In future it can be extended check a lot of other stuff as well\n+   (reachability of basic blocks, life information, etc. etc.).  */\n+\n+void\n+verify_flow_info ()\n+{\n+  const int max_uid = get_max_uid ();\n+  const rtx rtx_first = get_insns ();\n+  rtx last_head = get_last_insn ();\n+  basic_block *bb_info, *last_visited;\n+  size_t *edge_checksum;\n+  rtx x;\n+  int i, last_bb_num_seen, num_bb_notes, err = 0;\n+\n+  bb_info = (basic_block *) xcalloc (max_uid, sizeof (basic_block));\n+  last_visited = (basic_block *) xcalloc (n_basic_blocks + 2,\n+\t\t\t\t\t  sizeof (basic_block));\n+  edge_checksum = (size_t *) xcalloc (n_basic_blocks + 2, sizeof (size_t));\n+\n+  for (i = n_basic_blocks - 1; i >= 0; i--)\n+    {\n+      basic_block bb = BASIC_BLOCK (i);\n+      rtx head = bb->head;\n+      rtx end = bb->end;\n+\n+      /* Verify the end of the basic block is in the INSN chain.  */\n+      for (x = last_head; x != NULL_RTX; x = PREV_INSN (x))\n+\tif (x == end)\n+\t  break;\n+      if (!x)\n+\t{\n+\t  error (\"End insn %d for block %d not found in the insn stream.\",\n+\t\t INSN_UID (end), bb->index);\n+\t  err = 1;\n+\t}\n+\n+      /* Work backwards from the end to the head of the basic block\n+\t to verify the head is in the RTL chain.  */\n+      for (; x != NULL_RTX; x = PREV_INSN (x))\n+\t{\n+\t  /* While walking over the insn chain, verify insns appear\n+\t     in only one basic block and initialize the BB_INFO array\n+\t     used by other passes.  */\n+\t  if (bb_info[INSN_UID (x)] != NULL)\n+\t    {\n+\t      error (\"Insn %d is in multiple basic blocks (%d and %d)\",\n+\t\t     INSN_UID (x), bb->index, bb_info[INSN_UID (x)]->index);\n+\t      err = 1;\n+\t    }\n+\t  bb_info[INSN_UID (x)] = bb;\n+\n+\t  if (x == head)\n+\t    break;\n+\t}\n+      if (!x)\n+\t{\n+\t  error (\"Head insn %d for block %d not found in the insn stream.\",\n+\t\t INSN_UID (head), bb->index);\n+\t  err = 1;\n+\t}\n+\n+      last_head = x;\n+    }\n+\n+  /* Now check the basic blocks (boundaries etc.) */\n+  for (i = n_basic_blocks - 1; i >= 0; i--)\n+    {\n+      basic_block bb = BASIC_BLOCK (i);\n+      int has_fallthru = 0;\n+      edge e;\n+\n+      e = bb->succ;\n+      while (e)\n+\t{\n+\t  if (last_visited [e->dest->index + 2] == bb)\n+\t    {\n+\t      error (\"verify_flow_info: Duplicate edge %i->%i\",\n+\t\t     e->src->index, e->dest->index);\n+\t      err = 1;\n+\t    }\n+\t  last_visited [e->dest->index + 2] = bb;\n+\n+\t  if (e->flags & EDGE_FALLTHRU)\n+\t    has_fallthru = 1;\n+\n+\t  if ((e->flags & EDGE_FALLTHRU)\n+\t      && e->src != ENTRY_BLOCK_PTR\n+\t      && e->dest != EXIT_BLOCK_PTR)\n+\t    {\n+\t      rtx insn;\n+\t      if (e->src->index + 1 != e->dest->index)\n+\t\t{\n+\t\t    error (\"verify_flow_info: Incorrect blocks for fallthru %i->%i\",\n+\t\t\t   e->src->index, e->dest->index);\n+\t\t    err = 1;\n+\t\t}\n+\t      else\n+\t\tfor (insn = NEXT_INSN (e->src->end); insn != e->dest->head;\n+\t\t     insn = NEXT_INSN (insn))\n+\t\t  if (GET_CODE (insn) == BARRIER || INSN_P (insn))\n+\t\t    {\n+\t\t      error (\"verify_flow_info: Incorrect fallthru %i->%i\",\n+\t\t\t     e->src->index, e->dest->index);\n+\t\t      fatal_insn (\"Wrong insn in the fallthru edge\", insn);\n+\t\t      err = 1;\n+\t\t    }\n+\t    }\n+\t  if (e->src != bb)\n+\t    {\n+\t      error (\"verify_flow_info: Basic block %d succ edge is corrupted\",\n+\t\t     bb->index);\n+\t      fprintf (stderr, \"Predecessor: \");\n+\t      dump_edge_info (stderr, e, 0);\n+\t      fprintf (stderr, \"\\nSuccessor: \");\n+\t      dump_edge_info (stderr, e, 1);\n+\t      fprintf (stderr, \"\\n\");\n+\t      err = 1;\n+\t    }\n+\t  edge_checksum[e->dest->index + 2] += (size_t) e;\n+\t  e = e->succ_next;\n+\t}\n+      if (!has_fallthru)\n+\t{\n+\t  rtx insn = bb->end;\n+\n+\t  /* Ensure existence of barrier in BB with no fallthru edges.  */\n+\t  for (insn = bb->end; GET_CODE (insn) != BARRIER;\n+\t       insn = NEXT_INSN (insn))\n+\t    if (!insn\n+\t\t|| (GET_CODE (insn) == NOTE\n+\t\t    && NOTE_LINE_NUMBER (insn) == NOTE_INSN_BASIC_BLOCK))\n+\t\t{\n+\t\t  error (\"Missing barrier after block %i\", bb->index);\n+\t\t  err = 1;\n+\t\t}\n+\t}\n+\n+      e = bb->pred;\n+      while (e)\n+\t{\n+\t  if (e->dest != bb)\n+\t    {\n+\t      error (\"Basic block %d pred edge is corrupted\", bb->index);\n+\t      fputs (\"Predecessor: \", stderr);\n+\t      dump_edge_info (stderr, e, 0);\n+\t      fputs (\"\\nSuccessor: \", stderr);\n+\t      dump_edge_info (stderr, e, 1);\n+\t      fputc ('\\n', stderr);\n+\t      err = 1;\n+\t    }\n+\t  edge_checksum[e->dest->index + 2] -= (size_t) e;\n+\t  e = e->pred_next;\n+\t}\n+\n+      /* OK pointers are correct.  Now check the header of basic\n+         block.  It ought to contain optional CODE_LABEL followed\n+\t by NOTE_BASIC_BLOCK.  */\n+      x = bb->head;\n+      if (GET_CODE (x) == CODE_LABEL)\n+\t{\n+\t  if (bb->end == x)\n+\t    {\n+\t      error (\"NOTE_INSN_BASIC_BLOCK is missing for block %d\",\n+\t\t     bb->index);\n+\t      err = 1;\n+\t    }\n+\t  x = NEXT_INSN (x);\n+\t}\n+      if (!NOTE_INSN_BASIC_BLOCK_P (x) || NOTE_BASIC_BLOCK (x) != bb)\n+\t{\n+\t  error (\"NOTE_INSN_BASIC_BLOCK is missing for block %d\",\n+\t\t bb->index);\n+\t  err = 1;\n+\t}\n+\n+      if (bb->end == x)\n+\t{\n+\t  /* Do checks for empty blocks here */\n+\t}\n+      else\n+\t{\n+\t  x = NEXT_INSN (x);\n+\t  while (x)\n+\t    {\n+\t      if (NOTE_INSN_BASIC_BLOCK_P (x))\n+\t\t{\n+\t\t  error (\"NOTE_INSN_BASIC_BLOCK %d in the middle of basic block %d\",\n+\t\t\t INSN_UID (x), bb->index);\n+\t\t  err = 1;\n+\t\t}\n+\n+\t      if (x == bb->end)\n+\t\tbreak;\n+\n+\t      if (GET_CODE (x) == JUMP_INSN\n+\t\t  || GET_CODE (x) == CODE_LABEL\n+\t\t  || GET_CODE (x) == BARRIER)\n+\t\t{\n+\t\t  error (\"In basic block %d:\", bb->index);\n+\t\t  fatal_insn (\"Flow control insn inside a basic block\", x);\n+\t\t}\n+\n+\t      x = NEXT_INSN (x);\n+\t    }\n+\t}\n+    }\n+\n+  /* Complete edge checksumming for ENTRY and EXIT.  */\n+  {\n+    edge e;\n+    for (e = ENTRY_BLOCK_PTR->succ; e ; e = e->succ_next)\n+      edge_checksum[e->dest->index + 2] += (size_t) e;\n+    for (e = EXIT_BLOCK_PTR->pred; e ; e = e->pred_next)\n+      edge_checksum[e->dest->index + 2] -= (size_t) e;\n+  }\n+\n+  for (i = -2; i < n_basic_blocks; ++i)\n+    if (edge_checksum[i + 2])\n+      {\n+\terror (\"Basic block %i edge lists are corrupted\", i);\n+\terr = 1;\n+      }\n+\n+  last_bb_num_seen = -1;\n+  num_bb_notes = 0;\n+  x = rtx_first;\n+  while (x)\n+    {\n+      if (NOTE_INSN_BASIC_BLOCK_P (x))\n+\t{\n+\t  basic_block bb = NOTE_BASIC_BLOCK (x);\n+\t  num_bb_notes++;\n+\t  if (bb->index != last_bb_num_seen + 1)\n+\t    internal_error (\"Basic blocks not numbered consecutively.\");\n+\n+\t  last_bb_num_seen = bb->index;\n+\t}\n+\n+      if (!bb_info[INSN_UID (x)])\n+\t{\n+\t  switch (GET_CODE (x))\n+\t    {\n+\t    case BARRIER:\n+\t    case NOTE:\n+\t      break;\n+\n+\t    case CODE_LABEL:\n+\t      /* An addr_vec is placed outside any block block.  */\n+\t      if (NEXT_INSN (x)\n+\t\t  && GET_CODE (NEXT_INSN (x)) == JUMP_INSN\n+\t\t  && (GET_CODE (PATTERN (NEXT_INSN (x))) == ADDR_DIFF_VEC\n+\t\t      || GET_CODE (PATTERN (NEXT_INSN (x))) == ADDR_VEC))\n+\t\t{\n+\t\t  x = NEXT_INSN (x);\n+\t\t}\n+\n+\t      /* But in any case, non-deletable labels can appear anywhere.  */\n+\t      break;\n+\n+\t    default:\n+\t      fatal_insn (\"Insn outside basic block\", x);\n+\t    }\n+\t}\n+\n+      if (INSN_P (x)\n+\t  && GET_CODE (x) == JUMP_INSN\n+\t  && returnjump_p (x) && ! condjump_p (x)\n+\t  && ! (NEXT_INSN (x) && GET_CODE (NEXT_INSN (x)) == BARRIER))\n+\t    fatal_insn (\"Return not followed by barrier\", x);\n+\n+      x = NEXT_INSN (x);\n+    }\n+\n+  if (num_bb_notes != n_basic_blocks)\n+    internal_error\n+      (\"number of bb notes in insn chain (%d) != n_basic_blocks (%d)\",\n+       num_bb_notes, n_basic_blocks);\n+\n+  if (err)\n+    internal_error (\"verify_flow_info failed.\");\n+\n+  /* Clean up.  */\n+  free (bb_info);\n+  free (last_visited);\n+  free (edge_checksum);\n+}\n+\f\n+\f\n+/* Assume that the preceeding pass has possibly eliminated jump instructions\n+   or converted the unconditional jumps.  Eliminate the edges from CFG.\n+   Return true if any edges are eliminated.  */\n+\n+bool\n+purge_dead_edges (bb)\n+     basic_block bb;\n+{\n+  edge e, next;\n+  rtx insn = bb->end;\n+  bool purged = false;\n+\n+  if (GET_CODE (insn) == JUMP_INSN && !simplejump_p (insn))\n+    return false;\n+  if (GET_CODE (insn) == JUMP_INSN)\n+    {\n+      rtx note;\n+      edge b,f;\n+      /* We do care only about conditional jumps and simplejumps.  */\n+      if (!any_condjump_p (insn)\n+\t  && !returnjump_p (insn)\n+\t  && !simplejump_p (insn))\n+\treturn false;\n+      for (e = bb->succ; e; e = next)\n+\t{\n+\t  next = e->succ_next;\n+\n+\t  /* Check purposes we can have edge.  */\n+\t  if ((e->flags & EDGE_FALLTHRU)\n+\t      && any_condjump_p (insn))\n+\t    continue;\n+\t  if (e->dest != EXIT_BLOCK_PTR\n+\t      && e->dest->head == JUMP_LABEL (insn))\n+\t    continue;\n+\t  if (e->dest == EXIT_BLOCK_PTR\n+\t      && returnjump_p (insn))\n+\t    continue;\n+\t  purged = true;\n+\t  remove_edge (e);\n+\t}\n+      if (!bb->succ || !purged)\n+\treturn false;\n+      if (rtl_dump_file)\n+\tfprintf (rtl_dump_file, \"Purged edges from bb %i\\n\", bb->index);\n+      if (!optimize)\n+\treturn purged;\n+\n+      /* Redistribute probabilities.  */\n+      if (!bb->succ->succ_next)\n+\t{\n+\t  bb->succ->probability = REG_BR_PROB_BASE;\n+\t  bb->succ->count = bb->count;\n+        }\n+      else\n+\t{\n+\t  note = find_reg_note (insn, REG_BR_PROB, NULL);\n+\t  if (!note)\n+\t    return purged;\n+\t  b = BRANCH_EDGE (bb);\n+\t  f = FALLTHRU_EDGE (bb);\n+\t  b->probability = INTVAL (XEXP (note, 0));\n+\t  f->probability = REG_BR_PROB_BASE - b->probability;\n+\t  b->count = bb->count * b->probability / REG_BR_PROB_BASE;\n+\t  f->count = bb->count * f->probability / REG_BR_PROB_BASE;\n+\t}\n+      return purged;\n+    }\n+\n+  /* Cleanup abnormal edges caused by throwing insns that have been\n+     eliminated.  */\n+  if (! can_throw_internal (bb->end))\n+    for (e = bb->succ; e; e = next)\n+      {\n+\tnext = e->succ_next;\n+\tif (e->flags & EDGE_EH)\n+\t  {\n+\t    remove_edge (e);\n+\t    purged = true;\n+\t  }\n+      }\n+\n+  /* If we don't see a jump insn, we don't know exactly why the block would\n+     have been broken at this point.  Look for a simple, non-fallthru edge,\n+     as these are only created by conditional branches.  If we find such an\n+     edge we know that there used to be a jump here and can then safely\n+     remove all non-fallthru edges.  */\n+  for (e = bb->succ; e && (e->flags & (EDGE_COMPLEX | EDGE_FALLTHRU));\n+       e = e->succ_next);\n+  if (!e)\n+    return purged;\n+  for (e = bb->succ; e; e = next)\n+    {\n+      next = e->succ_next;\n+      if (!(e->flags & EDGE_FALLTHRU))\n+\tremove_edge (e), purged = true;\n+    }\n+  if (!bb->succ || bb->succ->succ_next)\n+    abort ();\n+  bb->succ->probability = REG_BR_PROB_BASE;\n+  bb->succ->count = bb->count;\n+\n+  if (rtl_dump_file)\n+    fprintf (rtl_dump_file, \"Purged non-fallthru edges from bb %i\\n\",\n+\t     bb->index);\n+  return purged;\n+}\n+\n+/* Search all basic blocks for potentionally dead edges and purge them.\n+\n+   Return true ifif some edge has been elliminated.\n+ */\n+\n+bool\n+purge_all_dead_edges ()\n+{\n+  int i, purged = false;\n+  for (i = 0; i < n_basic_blocks; i++)\n+    purged |= purge_dead_edges (BASIC_BLOCK (i));\n+  return purged;\n+}"}, {"sha": "96a20b8f3c3e7adee2fb2dd477c3806b69ed622b", "filename": "gcc/cfganal.c", "status": "added", "additions": 1074, "deletions": 0, "changes": 1074, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fcfganal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fcfganal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfganal.c?ref=402209ff48d3e1984111c536033aa638f4271531", "patch": "@@ -0,0 +1,1074 @@\n+/* Control flow graph analysis code for GNU compiler.\n+   Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n+   1999, 2000, 2001 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 2, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+/* This file contains various simple utilities to analyze the CFG.  */\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"rtl.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"toplev.h\"\n+\n+#include \"obstack.h\"\n+\n+/* Store the data structures necessary for depth-first search.  */\n+struct depth_first_search_dsS {\n+  /* stack for backtracking during the algorithm */\n+  basic_block *stack;\n+\n+  /* number of edges in the stack.  That is, positions 0, ..., sp-1\n+     have edges.  */\n+  unsigned int sp;\n+\n+  /* record of basic blocks already seen by depth-first search */\n+  sbitmap visited_blocks;\n+};\n+typedef struct depth_first_search_dsS *depth_first_search_ds;\n+\n+static void flow_dfs_compute_reverse_init\n+  PARAMS ((depth_first_search_ds));\n+static void flow_dfs_compute_reverse_add_bb\n+  PARAMS ((depth_first_search_ds, basic_block));\n+static basic_block flow_dfs_compute_reverse_execute\n+  PARAMS ((depth_first_search_ds));\n+static void flow_dfs_compute_reverse_finish\n+  PARAMS ((depth_first_search_ds));\n+static void remove_fake_successors\tPARAMS ((basic_block));\n+static bool need_fake_edge_p\t\tPARAMS ((rtx));\n+\f\n+/* Return true if the block has no effect and only forwards control flow to\n+   its single destination.  */\n+bool\n+forwarder_block_p (bb)\n+     basic_block bb;\n+{\n+  rtx insn = bb->head;\n+  if (bb == EXIT_BLOCK_PTR || bb == ENTRY_BLOCK_PTR\n+      || !bb->succ || bb->succ->succ_next)\n+    return false;\n+\n+  while (insn != bb->end)\n+    {\n+      if (active_insn_p (insn))\n+\treturn false;\n+      insn = NEXT_INSN (insn);\n+    }\n+  return (!active_insn_p (insn)\n+\t  || (GET_CODE (insn) == JUMP_INSN && onlyjump_p (insn)));\n+}\n+\n+/* Return nonzero if we can reach target from src by falling trought.  */\n+bool\n+can_fallthru (src, target)\n+     basic_block src, target;\n+{\n+  rtx insn = src->end;\n+  rtx insn2 = target->head;\n+\n+  if (src->index + 1 == target->index && !active_insn_p (insn2))\n+    insn2 = next_active_insn (insn2);\n+  /* ??? Later we may add code to move jump tables offline.  */\n+  return next_active_insn (insn) == insn2;\n+}\n+\f\n+/* Identify critical edges and set the bits appropriately.  */\n+\n+void\n+mark_critical_edges ()\n+{\n+  int i, n = n_basic_blocks;\n+  basic_block bb;\n+\n+  /* We begin with the entry block.  This is not terribly important now,\n+     but could be if a front end (Fortran) implemented alternate entry\n+     points.  */\n+  bb = ENTRY_BLOCK_PTR;\n+  i = -1;\n+\n+  while (1)\n+    {\n+      edge e;\n+\n+      /* (1) Critical edges must have a source with multiple successors.  */\n+      if (bb->succ && bb->succ->succ_next)\n+\t{\n+\t  for (e = bb->succ; e; e = e->succ_next)\n+\t    {\n+\t      /* (2) Critical edges must have a destination with multiple\n+\t\t predecessors.  Note that we know there is at least one\n+\t\t predecessor -- the edge we followed to get here.  */\n+\t      if (e->dest->pred->pred_next)\n+\t\te->flags |= EDGE_CRITICAL;\n+\t      else\n+\t\te->flags &= ~EDGE_CRITICAL;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  for (e = bb->succ; e; e = e->succ_next)\n+\t    e->flags &= ~EDGE_CRITICAL;\n+\t}\n+\n+      if (++i >= n)\n+\tbreak;\n+      bb = BASIC_BLOCK (i);\n+    }\n+}\n+\n+/* Mark the back edges in DFS traversal.\n+   Return non-zero if a loop (natural or otherwise) is present.\n+   Inspired by Depth_First_Search_PP described in:\n+\n+     Advanced Compiler Design and Implementation\n+     Steven Muchnick\n+     Morgan Kaufmann, 1997\n+\n+   and heavily borrowed from flow_depth_first_order_compute.  */\n+\n+bool\n+mark_dfs_back_edges ()\n+{\n+  edge *stack;\n+  int *pre;\n+  int *post;\n+  int sp;\n+  int prenum = 1;\n+  int postnum = 1;\n+  sbitmap visited;\n+  bool found = false;\n+\n+  /* Allocate the preorder and postorder number arrays.  */\n+  pre = (int *) xcalloc (n_basic_blocks, sizeof (int));\n+  post = (int *) xcalloc (n_basic_blocks, sizeof (int));\n+\n+  /* Allocate stack for back-tracking up CFG.  */\n+  stack = (edge *) xmalloc ((n_basic_blocks + 1) * sizeof (edge));\n+  sp = 0;\n+\n+  /* Allocate bitmap to track nodes that have been visited.  */\n+  visited = sbitmap_alloc (n_basic_blocks);\n+\n+  /* None of the nodes in the CFG have been visited yet.  */\n+  sbitmap_zero (visited);\n+\n+  /* Push the first edge on to the stack.  */\n+  stack[sp++] = ENTRY_BLOCK_PTR->succ;\n+\n+  while (sp)\n+    {\n+      edge e;\n+      basic_block src;\n+      basic_block dest;\n+\n+      /* Look at the edge on the top of the stack.  */\n+      e = stack[sp - 1];\n+      src = e->src;\n+      dest = e->dest;\n+      e->flags &= ~EDGE_DFS_BACK;\n+\n+      /* Check if the edge destination has been visited yet.  */\n+      if (dest != EXIT_BLOCK_PTR && ! TEST_BIT (visited, dest->index))\n+\t{\n+\t  /* Mark that we have visited the destination.  */\n+\t  SET_BIT (visited, dest->index);\n+\n+\t  pre[dest->index] = prenum++;\n+\n+\t  if (dest->succ)\n+\t    {\n+\t      /* Since the DEST node has been visited for the first\n+\t\t time, check its successors.  */\n+\t      stack[sp++] = dest->succ;\n+\t    }\n+\t  else\n+\t    post[dest->index] = postnum++;\n+\t}\n+      else\n+\t{\n+\t  if (dest != EXIT_BLOCK_PTR && src != ENTRY_BLOCK_PTR\n+\t      && pre[src->index] >= pre[dest->index]\n+\t      && post[dest->index] == 0)\n+\t    e->flags |= EDGE_DFS_BACK, found = true;\n+\n+\t  if (! e->succ_next && src != ENTRY_BLOCK_PTR)\n+\t    post[src->index] = postnum++;\n+\n+\t  if (e->succ_next)\n+\t    stack[sp - 1] = e->succ_next;\n+\t  else\n+\t    sp--;\n+\t}\n+    }\n+\n+  free (pre);\n+  free (post);\n+  free (stack);\n+  sbitmap_free (visited);\n+\n+  return found;\n+}\n+\n+/* Return true if we need to add fake edge to exit.\n+   Helper function for the flow_call_edges_add.  */\n+\n+static bool\n+need_fake_edge_p (insn)\n+     rtx insn;\n+{\n+  if (!INSN_P (insn))\n+    return false;\n+\n+  if ((GET_CODE (insn) == CALL_INSN\n+       && !SIBLING_CALL_P (insn)\n+       && !find_reg_note (insn, REG_NORETURN, NULL)\n+       && !find_reg_note (insn, REG_ALWAYS_RETURN, NULL)\n+       && !CONST_OR_PURE_CALL_P (insn)))\n+    return true;\n+\n+  return ((GET_CODE (PATTERN (insn)) == ASM_OPERANDS\n+\t   && MEM_VOLATILE_P (PATTERN (insn)))\n+\t  || (GET_CODE (PATTERN (insn)) == PARALLEL\n+\t      && asm_noperands (insn) != -1\n+\t      && MEM_VOLATILE_P (XVECEXP (PATTERN (insn), 0, 0)))\n+\t  || GET_CODE (PATTERN (insn)) == ASM_INPUT);\n+}\n+\n+/* Add fake edges to the function exit for any non constant and non noreturn\n+   calls, volatile inline assembly in the bitmap of blocks specified by\n+   BLOCKS or to the whole CFG if BLOCKS is zero.  Return the nuber of blocks\n+   that were split.\n+\n+   The goal is to expose cases in which entering a basic block does not imply\n+   that all subsequent instructions must be executed.  */\n+\n+int\n+flow_call_edges_add (blocks)\n+     sbitmap blocks;\n+{\n+  int i;\n+  int blocks_split = 0;\n+  int bb_num = 0;\n+  basic_block *bbs;\n+  bool check_last_block = false;\n+\n+  /* Map bb indicies into basic block pointers since split_block\n+     will renumber the basic blocks.  */\n+\n+  bbs = xmalloc (n_basic_blocks * sizeof (*bbs));\n+\n+  if (! blocks)\n+    {\n+      for (i = 0; i < n_basic_blocks; i++)\n+\tbbs[bb_num++] = BASIC_BLOCK (i);\n+      check_last_block = true;\n+    }\n+  else\n+    {\n+      EXECUTE_IF_SET_IN_SBITMAP (blocks, 0, i,\n+      {\n+\tbbs[bb_num++] = BASIC_BLOCK (i);\n+\tif (i == n_basic_blocks - 1)\n+\t  check_last_block = true;\n+      });\n+    }\n+\n+  /* In the last basic block, before epilogue generation, there will be\n+     a fallthru edge to EXIT.  Special care is required if the last insn\n+     of the last basic block is a call because make_edge folds duplicate\n+     edges, which would result in the fallthru edge also being marked\n+     fake, which would result in the fallthru edge being removed by\n+     remove_fake_edges, which would result in an invalid CFG.\n+\n+     Moreover, we can't elide the outgoing fake edge, since the block\n+     profiler needs to take this into account in order to solve the minimal\n+     spanning tree in the case that the call doesn't return.\n+\n+     Handle this by adding a dummy instruction in a new last basic block.  */\n+  if (check_last_block\n+      && need_fake_edge_p (BASIC_BLOCK (n_basic_blocks - 1)->end))\n+    {\n+       edge e;\n+       for (e = BASIC_BLOCK (n_basic_blocks - 1)->succ; e; e = e->succ_next)\n+\t if (e->dest == EXIT_BLOCK_PTR)\n+\t    break;\n+       insert_insn_on_edge (gen_rtx_USE (VOIDmode, const0_rtx), e);\n+       commit_edge_insertions ();\n+    }\n+\n+\n+  /* Now add fake edges to the function exit for any non constant\n+     calls since there is no way that we can determine if they will\n+     return or not...  */\n+\n+  for (i = 0; i < bb_num; i++)\n+    {\n+      basic_block bb = bbs[i];\n+      rtx insn;\n+      rtx prev_insn;\n+\n+      for (insn = bb->end; ; insn = prev_insn)\n+\t{\n+\t  prev_insn = PREV_INSN (insn);\n+\t  if (need_fake_edge_p (insn))\n+\t    {\n+\t      edge e;\n+\n+\t      /* The above condition should be enought to verify that there is\n+\t\t no edge to the exit block in CFG already.  Calling make_edge in\n+\t\t such case would make us to mark that edge as fake and remove it\n+\t\t later.  */\n+#ifdef ENABLE_CHECKING\n+\t      if (insn == bb->end)\n+\t\tfor (e = bb->succ; e; e = e->succ_next)\n+\t\t  if (e->dest == EXIT_BLOCK_PTR)\n+\t\t    abort ();\n+#endif\n+\n+\t      /* Note that the following may create a new basic block\n+\t\t and renumber the existing basic blocks.  */\n+\t      e = split_block (bb, insn);\n+\t      if (e)\n+\t\tblocks_split++;\n+\n+\t      make_edge (NULL, bb, EXIT_BLOCK_PTR, EDGE_FAKE);\n+\t    }\n+\t  if (insn == bb->head)\n+\t    break;\n+\t}\n+    }\n+\n+  if (blocks_split)\n+    verify_flow_info ();\n+\n+  free (bbs);\n+  return blocks_split;\n+}\n+/* Find unreachable blocks.  An unreachable block will have 0 in\n+   the reachable bit in block->flags.  A non-zero value indicates the\n+   block is reachable.  */\n+\n+void\n+find_unreachable_blocks ()\n+{\n+  edge e;\n+  int i, n;\n+  basic_block *tos, *worklist;\n+\n+  n = n_basic_blocks;\n+  tos = worklist = (basic_block *) xmalloc (sizeof (basic_block) * n);\n+\n+  /* Clear all the reachability flags.  */\n+\n+  for (i = 0; i < n; ++i)\n+    BASIC_BLOCK (i)->flags &= ~BB_REACHABLE;\n+\n+  /* Add our starting points to the worklist.  Almost always there will\n+     be only one.  It isn't inconcievable that we might one day directly\n+     support Fortran alternate entry points.  */\n+\n+  for (e = ENTRY_BLOCK_PTR->succ; e; e = e->succ_next)\n+    {\n+      *tos++ = e->dest;\n+\n+      /* Mark the block reachable.  */\n+      e->dest->flags |= BB_REACHABLE;\n+    }\n+\n+  /* Iterate: find everything reachable from what we've already seen.  */\n+\n+  while (tos != worklist)\n+    {\n+      basic_block b = *--tos;\n+\n+      for (e = b->succ; e; e = e->succ_next)\n+\tif (!(e->dest->flags & BB_REACHABLE))\n+\t  {\n+\t    *tos++ = e->dest;\n+\t    e->dest->flags |= BB_REACHABLE;\n+\t  }\n+    }\n+\n+  free (worklist);\n+}\n+\f\n+/* Functions to access an edge list with a vector representation.\n+   Enough data is kept such that given an index number, the\n+   pred and succ that edge represents can be determined, or\n+   given a pred and a succ, its index number can be returned.\n+   This allows algorithms which consume a lot of memory to\n+   represent the normally full matrix of edge (pred,succ) with a\n+   single indexed vector,  edge (EDGE_INDEX (pred, succ)), with no\n+   wasted space in the client code due to sparse flow graphs.  */\n+\n+/* This functions initializes the edge list. Basically the entire\n+   flowgraph is processed, and all edges are assigned a number,\n+   and the data structure is filled in.  */\n+\n+struct edge_list *\n+create_edge_list ()\n+{\n+  struct edge_list *elist;\n+  edge e;\n+  int num_edges;\n+  int x;\n+  int block_count;\n+\n+  block_count = n_basic_blocks + 2;   /* Include the entry and exit blocks.  */\n+\n+  num_edges = 0;\n+\n+  /* Determine the number of edges in the flow graph by counting successor\n+     edges on each basic block.  */\n+  for (x = 0; x < n_basic_blocks; x++)\n+    {\n+      basic_block bb = BASIC_BLOCK (x);\n+\n+      for (e = bb->succ; e; e = e->succ_next)\n+\tnum_edges++;\n+    }\n+  /* Don't forget successors of the entry block.  */\n+  for (e = ENTRY_BLOCK_PTR->succ; e; e = e->succ_next)\n+    num_edges++;\n+\n+  elist = (struct edge_list *) xmalloc (sizeof (struct edge_list));\n+  elist->num_blocks = block_count;\n+  elist->num_edges = num_edges;\n+  elist->index_to_edge = (edge *) xmalloc (sizeof (edge) * num_edges);\n+\n+  num_edges = 0;\n+\n+  /* Follow successors of the entry block, and register these edges.  */\n+  for (e = ENTRY_BLOCK_PTR->succ; e; e = e->succ_next)\n+    {\n+      elist->index_to_edge[num_edges] = e;\n+      num_edges++;\n+    }\n+\n+  for (x = 0; x < n_basic_blocks; x++)\n+    {\n+      basic_block bb = BASIC_BLOCK (x);\n+\n+      /* Follow all successors of blocks, and register these edges.  */\n+      for (e = bb->succ; e; e = e->succ_next)\n+\t{\n+\t  elist->index_to_edge[num_edges] = e;\n+\t  num_edges++;\n+\t}\n+    }\n+  return elist;\n+}\n+\n+/* This function free's memory associated with an edge list.  */\n+\n+void\n+free_edge_list (elist)\n+     struct edge_list *elist;\n+{\n+  if (elist)\n+    {\n+      free (elist->index_to_edge);\n+      free (elist);\n+    }\n+}\n+\n+/* This function provides debug output showing an edge list.  */\n+\n+void\n+print_edge_list (f, elist)\n+     FILE *f;\n+     struct edge_list *elist;\n+{\n+  int x;\n+  fprintf (f, \"Compressed edge list, %d BBs + entry & exit, and %d edges\\n\",\n+\t   elist->num_blocks - 2, elist->num_edges);\n+\n+  for (x = 0; x < elist->num_edges; x++)\n+    {\n+      fprintf (f, \" %-4d - edge(\", x);\n+      if (INDEX_EDGE_PRED_BB (elist, x) == ENTRY_BLOCK_PTR)\n+\tfprintf (f, \"entry,\");\n+      else\n+\tfprintf (f, \"%d,\", INDEX_EDGE_PRED_BB (elist, x)->index);\n+\n+      if (INDEX_EDGE_SUCC_BB (elist, x) == EXIT_BLOCK_PTR)\n+\tfprintf (f, \"exit)\\n\");\n+      else\n+\tfprintf (f, \"%d)\\n\", INDEX_EDGE_SUCC_BB (elist, x)->index);\n+    }\n+}\n+\n+/* This function provides an internal consistency check of an edge list,\n+   verifying that all edges are present, and that there are no\n+   extra edges.  */\n+\n+void\n+verify_edge_list (f, elist)\n+     FILE *f;\n+     struct edge_list *elist;\n+{\n+  int x, pred, succ, index;\n+  edge e;\n+\n+  for (x = 0; x < n_basic_blocks; x++)\n+    {\n+      basic_block bb = BASIC_BLOCK (x);\n+\n+      for (e = bb->succ; e; e = e->succ_next)\n+\t{\n+\t  pred = e->src->index;\n+\t  succ = e->dest->index;\n+\t  index = EDGE_INDEX (elist, e->src, e->dest);\n+\t  if (index == EDGE_INDEX_NO_EDGE)\n+\t    {\n+\t      fprintf (f, \"*p* No index for edge from %d to %d\\n\", pred, succ);\n+\t      continue;\n+\t    }\n+\t  if (INDEX_EDGE_PRED_BB (elist, index)->index != pred)\n+\t    fprintf (f, \"*p* Pred for index %d should be %d not %d\\n\",\n+\t\t     index, pred, INDEX_EDGE_PRED_BB (elist, index)->index);\n+\t  if (INDEX_EDGE_SUCC_BB (elist, index)->index != succ)\n+\t    fprintf (f, \"*p* Succ for index %d should be %d not %d\\n\",\n+\t\t     index, succ, INDEX_EDGE_SUCC_BB (elist, index)->index);\n+\t}\n+    }\n+  for (e = ENTRY_BLOCK_PTR->succ; e; e = e->succ_next)\n+    {\n+      pred = e->src->index;\n+      succ = e->dest->index;\n+      index = EDGE_INDEX (elist, e->src, e->dest);\n+      if (index == EDGE_INDEX_NO_EDGE)\n+\t{\n+\t  fprintf (f, \"*p* No index for edge from %d to %d\\n\", pred, succ);\n+\t  continue;\n+\t}\n+      if (INDEX_EDGE_PRED_BB (elist, index)->index != pred)\n+\tfprintf (f, \"*p* Pred for index %d should be %d not %d\\n\",\n+\t\t index, pred, INDEX_EDGE_PRED_BB (elist, index)->index);\n+      if (INDEX_EDGE_SUCC_BB (elist, index)->index != succ)\n+\tfprintf (f, \"*p* Succ for index %d should be %d not %d\\n\",\n+\t\t index, succ, INDEX_EDGE_SUCC_BB (elist, index)->index);\n+    }\n+  /* We've verified that all the edges are in the list, no lets make sure\n+     there are no spurious edges in the list.  */\n+\n+  for (pred = 0; pred < n_basic_blocks; pred++)\n+    for (succ = 0; succ < n_basic_blocks; succ++)\n+      {\n+\tbasic_block p = BASIC_BLOCK (pred);\n+\tbasic_block s = BASIC_BLOCK (succ);\n+\n+\tint found_edge = 0;\n+\n+\tfor (e = p->succ; e; e = e->succ_next)\n+\t  if (e->dest == s)\n+\t    {\n+\t      found_edge = 1;\n+\t      break;\n+\t    }\n+\tfor (e = s->pred; e; e = e->pred_next)\n+\t  if (e->src == p)\n+\t    {\n+\t      found_edge = 1;\n+\t      break;\n+\t    }\n+\tif (EDGE_INDEX (elist, BASIC_BLOCK (pred), BASIC_BLOCK (succ))\n+\t    == EDGE_INDEX_NO_EDGE && found_edge != 0)\n+\t  fprintf (f, \"*** Edge (%d, %d) appears to not have an index\\n\",\n+\t\t   pred, succ);\n+\tif (EDGE_INDEX (elist, BASIC_BLOCK (pred), BASIC_BLOCK (succ))\n+\t    != EDGE_INDEX_NO_EDGE && found_edge == 0)\n+\t  fprintf (f, \"*** Edge (%d, %d) has index %d, but there is no edge\\n\",\n+\t\t   pred, succ, EDGE_INDEX (elist, BASIC_BLOCK (pred),\n+\t\t\t\t\t   BASIC_BLOCK (succ)));\n+      }\n+  for (succ = 0; succ < n_basic_blocks; succ++)\n+    {\n+      basic_block p = ENTRY_BLOCK_PTR;\n+      basic_block s = BASIC_BLOCK (succ);\n+\n+      int found_edge = 0;\n+\n+      for (e = p->succ; e; e = e->succ_next)\n+\tif (e->dest == s)\n+\t  {\n+\t    found_edge = 1;\n+\t    break;\n+\t  }\n+      for (e = s->pred; e; e = e->pred_next)\n+\tif (e->src == p)\n+\t  {\n+\t    found_edge = 1;\n+\t    break;\n+\t  }\n+      if (EDGE_INDEX (elist, ENTRY_BLOCK_PTR, BASIC_BLOCK (succ))\n+\t  == EDGE_INDEX_NO_EDGE && found_edge != 0)\n+\tfprintf (f, \"*** Edge (entry, %d) appears to not have an index\\n\",\n+\t\t succ);\n+      if (EDGE_INDEX (elist, ENTRY_BLOCK_PTR, BASIC_BLOCK (succ))\n+\t  != EDGE_INDEX_NO_EDGE && found_edge == 0)\n+\tfprintf (f, \"*** Edge (entry, %d) has index %d, but no edge exists\\n\",\n+\t\t succ, EDGE_INDEX (elist, ENTRY_BLOCK_PTR,\n+\t\t\t\t   BASIC_BLOCK (succ)));\n+    }\n+  for (pred = 0; pred < n_basic_blocks; pred++)\n+    {\n+      basic_block p = BASIC_BLOCK (pred);\n+      basic_block s = EXIT_BLOCK_PTR;\n+\n+      int found_edge = 0;\n+\n+      for (e = p->succ; e; e = e->succ_next)\n+\tif (e->dest == s)\n+\t  {\n+\t    found_edge = 1;\n+\t    break;\n+\t  }\n+      for (e = s->pred; e; e = e->pred_next)\n+\tif (e->src == p)\n+\t  {\n+\t    found_edge = 1;\n+\t    break;\n+\t  }\n+      if (EDGE_INDEX (elist, BASIC_BLOCK (pred), EXIT_BLOCK_PTR)\n+\t  == EDGE_INDEX_NO_EDGE && found_edge != 0)\n+\tfprintf (f, \"*** Edge (%d, exit) appears to not have an index\\n\",\n+\t\t pred);\n+      if (EDGE_INDEX (elist, BASIC_BLOCK (pred), EXIT_BLOCK_PTR)\n+\t  != EDGE_INDEX_NO_EDGE && found_edge == 0)\n+\tfprintf (f, \"*** Edge (%d, exit) has index %d, but no edge exists\\n\",\n+\t\t pred, EDGE_INDEX (elist, BASIC_BLOCK (pred),\n+\t\t\t\t   EXIT_BLOCK_PTR));\n+    }\n+}\n+\n+/* This routine will determine what, if any, edge there is between\n+   a specified predecessor and successor.  */\n+\n+int\n+find_edge_index (edge_list, pred, succ)\n+     struct edge_list *edge_list;\n+     basic_block pred, succ;\n+{\n+  int x;\n+  for (x = 0; x < NUM_EDGES (edge_list); x++)\n+    {\n+      if (INDEX_EDGE_PRED_BB (edge_list, x) == pred\n+\t  && INDEX_EDGE_SUCC_BB (edge_list, x) == succ)\n+\treturn x;\n+    }\n+  return (EDGE_INDEX_NO_EDGE);\n+}\n+\n+/* Dump the list of basic blocks in the bitmap NODES.  */\n+\n+void\n+flow_nodes_print (str, nodes, file)\n+     const char *str;\n+     const sbitmap nodes;\n+     FILE *file;\n+{\n+  int node;\n+\n+  if (! nodes)\n+    return;\n+\n+  fprintf (file, \"%s { \", str);\n+  EXECUTE_IF_SET_IN_SBITMAP (nodes, 0, node, {fprintf (file, \"%d \", node);});\n+  fputs (\"}\\n\", file);\n+}\n+\n+/* Dump the list of edges in the array EDGE_LIST.  */\n+\n+void\n+flow_edge_list_print (str, edge_list, num_edges, file)\n+     const char *str;\n+     const edge *edge_list;\n+     int num_edges;\n+     FILE *file;\n+{\n+  int i;\n+\n+  if (! edge_list)\n+    return;\n+\n+  fprintf (file, \"%s { \", str);\n+  for (i = 0; i < num_edges; i++)\n+    fprintf (file, \"%d->%d \", edge_list[i]->src->index,\n+\t     edge_list[i]->dest->index);\n+  fputs (\"}\\n\", file);\n+}\n+\n+\f\n+/* This routine will remove any fake successor edges for a basic block.\n+   When the edge is removed, it is also removed from whatever predecessor\n+   list it is in.  */\n+\n+static void\n+remove_fake_successors (bb)\n+     basic_block bb;\n+{\n+  edge e;\n+  for (e = bb->succ; e;)\n+    {\n+      edge tmp = e;\n+      e = e->succ_next;\n+      if ((tmp->flags & EDGE_FAKE) == EDGE_FAKE)\n+\tremove_edge (tmp);\n+    }\n+}\n+\n+/* This routine will remove all fake edges from the flow graph.  If\n+   we remove all fake successors, it will automatically remove all\n+   fake predecessors.  */\n+\n+void\n+remove_fake_edges ()\n+{\n+  int x;\n+\n+  for (x = 0; x < n_basic_blocks; x++)\n+    remove_fake_successors (BASIC_BLOCK (x));\n+\n+  /* We've handled all successors except the entry block's.  */\n+  remove_fake_successors (ENTRY_BLOCK_PTR);\n+}\n+\n+/* This function will add a fake edge between any block which has no\n+   successors, and the exit block. Some data flow equations require these\n+   edges to exist.  */\n+\n+void\n+add_noreturn_fake_exit_edges ()\n+{\n+  int x;\n+\n+  for (x = 0; x < n_basic_blocks; x++)\n+    if (BASIC_BLOCK (x)->succ == NULL)\n+      make_edge (NULL, BASIC_BLOCK (x), EXIT_BLOCK_PTR, EDGE_FAKE);\n+}\n+\n+/* This function adds a fake edge between any infinite loops to the\n+   exit block.  Some optimizations require a path from each node to\n+   the exit node.\n+\n+   See also Morgan, Figure 3.10, pp. 82-83.\n+\n+   The current implementation is ugly, not attempting to minimize the\n+   number of inserted fake edges.  To reduce the number of fake edges\n+   to insert, add fake edges from _innermost_ loops containing only\n+   nodes not reachable from the exit block.  */\n+\n+void\n+connect_infinite_loops_to_exit ()\n+{\n+  basic_block unvisited_block;\n+\n+  /* Perform depth-first search in the reverse graph to find nodes\n+     reachable from the exit block.  */\n+  struct depth_first_search_dsS dfs_ds;\n+\n+  flow_dfs_compute_reverse_init (&dfs_ds);\n+  flow_dfs_compute_reverse_add_bb (&dfs_ds, EXIT_BLOCK_PTR);\n+\n+  /* Repeatedly add fake edges, updating the unreachable nodes.  */\n+  while (1)\n+    {\n+      unvisited_block = flow_dfs_compute_reverse_execute (&dfs_ds);\n+      if (!unvisited_block)\n+\tbreak;\n+      make_edge (NULL, unvisited_block, EXIT_BLOCK_PTR, EDGE_FAKE);\n+      flow_dfs_compute_reverse_add_bb (&dfs_ds, unvisited_block);\n+    }\n+\n+  flow_dfs_compute_reverse_finish (&dfs_ds);\n+\n+  return;\n+}\n+\f\n+/* Compute reverse top sort order */\n+void\n+flow_reverse_top_sort_order_compute (rts_order)\n+     int *rts_order;\n+{\n+  edge *stack;\n+  int sp;\n+  int postnum = 0;\n+  sbitmap visited;\n+\n+  /* Allocate stack for back-tracking up CFG.  */\n+  stack = (edge *) xmalloc ((n_basic_blocks + 1) * sizeof (edge));\n+  sp = 0;\n+\n+  /* Allocate bitmap to track nodes that have been visited.  */\n+  visited = sbitmap_alloc (n_basic_blocks);\n+\n+  /* None of the nodes in the CFG have been visited yet.  */\n+  sbitmap_zero (visited);\n+\n+  /* Push the first edge on to the stack.  */\n+  stack[sp++] = ENTRY_BLOCK_PTR->succ;\n+\n+  while (sp)\n+    {\n+      edge e;\n+      basic_block src;\n+      basic_block dest;\n+\n+      /* Look at the edge on the top of the stack.  */\n+      e = stack[sp - 1];\n+      src = e->src;\n+      dest = e->dest;\n+\n+      /* Check if the edge destination has been visited yet.  */\n+      if (dest != EXIT_BLOCK_PTR && ! TEST_BIT (visited, dest->index))\n+\t{\n+\t  /* Mark that we have visited the destination.  */\n+\t  SET_BIT (visited, dest->index);\n+\n+\t  if (dest->succ)\n+\t    {\n+\t      /* Since the DEST node has been visited for the first\n+\t\t time, check its successors.  */\n+\t      stack[sp++] = dest->succ;\n+\t    }\n+\t  else\n+\t    rts_order[postnum++] = dest->index;\n+\t}\n+      else\n+\t{\n+\t  if (! e->succ_next && src != ENTRY_BLOCK_PTR)\n+\t   rts_order[postnum++] = src->index;\n+\n+\t  if (e->succ_next)\n+\t    stack[sp - 1] = e->succ_next;\n+\t  else\n+\t    sp--;\n+\t}\n+    }\n+\n+  free (stack);\n+  sbitmap_free (visited);\n+}\n+\n+/* Compute the depth first search order and store in the array\n+  DFS_ORDER if non-zero, marking the nodes visited in VISITED.  If\n+  RC_ORDER is non-zero, return the reverse completion number for each\n+  node.  Returns the number of nodes visited.  A depth first search\n+  tries to get as far away from the starting point as quickly as\n+  possible.  */\n+\n+int\n+flow_depth_first_order_compute (dfs_order, rc_order)\n+     int *dfs_order;\n+     int *rc_order;\n+{\n+  edge *stack;\n+  int sp;\n+  int dfsnum = 0;\n+  int rcnum = n_basic_blocks - 1;\n+  sbitmap visited;\n+\n+  /* Allocate stack for back-tracking up CFG.  */\n+  stack = (edge *) xmalloc ((n_basic_blocks + 1) * sizeof (edge));\n+  sp = 0;\n+\n+  /* Allocate bitmap to track nodes that have been visited.  */\n+  visited = sbitmap_alloc (n_basic_blocks);\n+\n+  /* None of the nodes in the CFG have been visited yet.  */\n+  sbitmap_zero (visited);\n+\n+  /* Push the first edge on to the stack.  */\n+  stack[sp++] = ENTRY_BLOCK_PTR->succ;\n+\n+  while (sp)\n+    {\n+      edge e;\n+      basic_block src;\n+      basic_block dest;\n+\n+      /* Look at the edge on the top of the stack.  */\n+      e = stack[sp - 1];\n+      src = e->src;\n+      dest = e->dest;\n+\n+      /* Check if the edge destination has been visited yet.  */\n+      if (dest != EXIT_BLOCK_PTR && ! TEST_BIT (visited, dest->index))\n+\t{\n+\t  /* Mark that we have visited the destination.  */\n+\t  SET_BIT (visited, dest->index);\n+\n+\t  if (dfs_order)\n+\t    dfs_order[dfsnum++] = dest->index;\n+\n+\t  if (dest->succ)\n+\t    {\n+\t      /* Since the DEST node has been visited for the first\n+\t\t time, check its successors.  */\n+\t      stack[sp++] = dest->succ;\n+\t    }\n+\t  else\n+\t    {\n+\t      /* There are no successors for the DEST node so assign\n+\t\t its reverse completion number.  */\n+\t      if (rc_order)\n+\t\trc_order[rcnum--] = dest->index;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  if (! e->succ_next && src != ENTRY_BLOCK_PTR)\n+\t    {\n+\t      /* There are no more successors for the SRC node\n+\t\t so assign its reverse completion number.  */\n+\t      if (rc_order)\n+\t\trc_order[rcnum--] = src->index;\n+\t    }\n+\n+\t  if (e->succ_next)\n+\t    stack[sp - 1] = e->succ_next;\n+\t  else\n+\t    sp--;\n+\t}\n+    }\n+\n+  free (stack);\n+  sbitmap_free (visited);\n+\n+  /* The number of nodes visited should not be greater than\n+     n_basic_blocks.  */\n+  if (dfsnum > n_basic_blocks)\n+    abort ();\n+\n+  /* There are some nodes left in the CFG that are unreachable.  */\n+  if (dfsnum < n_basic_blocks)\n+    abort ();\n+  return dfsnum;\n+}\n+\n+/* Compute the depth first search order on the _reverse_ graph and\n+   store in the array DFS_ORDER, marking the nodes visited in VISITED.\n+   Returns the number of nodes visited.\n+\n+   The computation is split into three pieces:\n+\n+   flow_dfs_compute_reverse_init () creates the necessary data\n+   structures.\n+\n+   flow_dfs_compute_reverse_add_bb () adds a basic block to the data\n+   structures.  The block will start the search.\n+\n+   flow_dfs_compute_reverse_execute () continues (or starts) the\n+   search using the block on the top of the stack, stopping when the\n+   stack is empty.\n+\n+   flow_dfs_compute_reverse_finish () destroys the necessary data\n+   structures.\n+\n+   Thus, the user will probably call ..._init(), call ..._add_bb() to\n+   add a beginning basic block to the stack, call ..._execute(),\n+   possibly add another bb to the stack and again call ..._execute(),\n+   ..., and finally call _finish().  */\n+\n+/* Initialize the data structures used for depth-first search on the\n+   reverse graph.  If INITIALIZE_STACK is nonzero, the exit block is\n+   added to the basic block stack.  DATA is the current depth-first\n+   search context.  If INITIALIZE_STACK is non-zero, there is an\n+   element on the stack.  */\n+\n+static void\n+flow_dfs_compute_reverse_init (data)\n+     depth_first_search_ds data;\n+{\n+  /* Allocate stack for back-tracking up CFG.  */\n+  data->stack =\n+    (basic_block *) xmalloc ((n_basic_blocks - (INVALID_BLOCK + 1))\n+\t\t\t     * sizeof (basic_block));\n+  data->sp = 0;\n+\n+  /* Allocate bitmap to track nodes that have been visited.  */\n+  data->visited_blocks = sbitmap_alloc (n_basic_blocks - (INVALID_BLOCK + 1));\n+\n+  /* None of the nodes in the CFG have been visited yet.  */\n+  sbitmap_zero (data->visited_blocks);\n+\n+  return;\n+}\n+\n+/* Add the specified basic block to the top of the dfs data\n+   structures.  When the search continues, it will start at the\n+   block.  */\n+\n+static void\n+flow_dfs_compute_reverse_add_bb (data, bb)\n+     depth_first_search_ds data;\n+     basic_block bb;\n+{\n+  data->stack[data->sp++] = bb;\n+  return;\n+}\n+\n+/* Continue the depth-first search through the reverse graph starting\n+   with the block at the stack's top and ending when the stack is\n+   empty.  Visited nodes are marked.  Returns an unvisited basic\n+   block, or NULL if there is none available.  */\n+\n+static basic_block\n+flow_dfs_compute_reverse_execute (data)\n+     depth_first_search_ds data;\n+{\n+  basic_block bb;\n+  edge e;\n+  int i;\n+\n+  while (data->sp > 0)\n+    {\n+      bb = data->stack[--data->sp];\n+\n+      /* Mark that we have visited this node.  */\n+      if (!TEST_BIT (data->visited_blocks, bb->index - (INVALID_BLOCK + 1)))\n+\t{\n+\t  SET_BIT (data->visited_blocks, bb->index - (INVALID_BLOCK + 1));\n+\n+\t  /* Perform depth-first search on adjacent vertices.  */\n+\t  for (e = bb->pred; e; e = e->pred_next)\n+\t    flow_dfs_compute_reverse_add_bb (data, e->src);\n+\t}\n+    }\n+\n+  /* Determine if there are unvisited basic blocks.  */\n+  for (i = n_basic_blocks - (INVALID_BLOCK + 1); --i >= 0;)\n+    if (!TEST_BIT (data->visited_blocks, i))\n+      return BASIC_BLOCK (i + (INVALID_BLOCK + 1));\n+  return NULL;\n+}\n+\n+/* Destroy the data structures needed for depth-first search on the\n+   reverse graph.  */\n+\n+static void\n+flow_dfs_compute_reverse_finish (data)\n+     depth_first_search_ds data;\n+{\n+  free (data->stack);\n+  sbitmap_free (data->visited_blocks);\n+  return;\n+}"}, {"sha": "2f62b067ff57107af1120070632d7efbdc59e6df", "filename": "gcc/cfgbuild.c", "status": "added", "additions": 791, "deletions": 0, "changes": 791, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fcfgbuild.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fcfgbuild.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfgbuild.c?ref=402209ff48d3e1984111c536033aa638f4271531", "patch": "@@ -0,0 +1,791 @@\n+/* Control flow graph building code for GNU compiler.\n+   Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n+   1999, 2000, 2001 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 2, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+/* find_basic_blocks divides the current function's rtl into basic\n+   blocks and constructs the CFG.  The blocks are recorded in the\n+   basic_block_info array; the CFG exists in the edge structures\n+   referenced by the blocks.\n+\n+   find_basic_blocks also finds any unreachable loops and deletes them.\n+\n+   Available functionality:\n+     - CFG construction\n+         find_basic_blocks\n+     - Local CFG construction\n+         find_sub_basic_blocks\n+ */\n+\f\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"tree.h\"\n+#include \"rtl.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"regs.h\"\n+#include \"flags.h\"\n+#include \"output.h\"\n+#include \"function.h\"\n+#include \"except.h\"\n+#include \"toplev.h\"\n+#include \"timevar.h\"\n+\n+#include \"obstack.h\"\n+static int count_basic_blocks\t\tPARAMS ((rtx));\n+static void find_basic_blocks_1\t\tPARAMS ((rtx));\n+static rtx find_label_refs\t\tPARAMS ((rtx, rtx));\n+static void make_edges\t\t\tPARAMS ((rtx, int, int, int));\n+static void make_label_edge\t\tPARAMS ((sbitmap *, basic_block,\n+\t\t\t\t\t\t rtx, int));\n+static void make_eh_edge\t\tPARAMS ((sbitmap *, basic_block, rtx));\n+\n+/* Count the basic blocks of the function.  */\n+\n+static int\n+count_basic_blocks (f)\n+     rtx f;\n+{\n+  register rtx insn;\n+  register RTX_CODE prev_code;\n+  register int count = 0;\n+  int saw_abnormal_edge = 0;\n+\n+  prev_code = JUMP_INSN;\n+  for (insn = f; insn; insn = NEXT_INSN (insn))\n+    {\n+      enum rtx_code code = GET_CODE (insn);\n+\n+      if (code == CODE_LABEL\n+\t  || (GET_RTX_CLASS (code) == 'i'\n+\t      && (prev_code == JUMP_INSN\n+\t\t  || prev_code == BARRIER\n+\t\t  || saw_abnormal_edge)))\n+\t{\n+\t  saw_abnormal_edge = 0;\n+\t  count++;\n+\t}\n+\n+      /* Record whether this insn created an edge.  */\n+      if (code == CALL_INSN)\n+\t{\n+\t  rtx note;\n+\n+\t  /* If there is a nonlocal goto label and the specified\n+\t     region number isn't -1, we have an edge.  */\n+\t  if (nonlocal_goto_handler_labels\n+\t      && ((note = find_reg_note (insn, REG_EH_REGION, NULL_RTX)) == 0\n+\t\t  || INTVAL (XEXP (note, 0)) >= 0))\n+\t    saw_abnormal_edge = 1;\n+\n+\t  else if (can_throw_internal (insn))\n+\t    saw_abnormal_edge = 1;\n+\t}\n+      else if (flag_non_call_exceptions\n+\t       && code == INSN\n+\t       && can_throw_internal (insn))\n+\tsaw_abnormal_edge = 1;\n+\n+      if (code != NOTE)\n+\tprev_code = code;\n+    }\n+\n+  /* The rest of the compiler works a bit smoother when we don't have to\n+     check for the edge case of do-nothing functions with no basic blocks.  */\n+  if (count == 0)\n+    {\n+      emit_insn (gen_rtx_USE (VOIDmode, const0_rtx));\n+      count = 1;\n+    }\n+\n+  return count;\n+}\n+\n+/* Scan a list of insns for labels referred to other than by jumps.\n+   This is used to scan the alternatives of a call placeholder.  */\n+static rtx\n+find_label_refs (f, lvl)\n+     rtx f;\n+     rtx lvl;\n+{\n+  rtx insn;\n+\n+  for (insn = f; insn; insn = NEXT_INSN (insn))\n+    if (INSN_P (insn) && GET_CODE (insn) != JUMP_INSN)\n+      {\n+\trtx note;\n+\n+\t/* Make a list of all labels referred to other than by jumps\n+\t   (which just don't have the REG_LABEL notes).\n+\n+\t   Make a special exception for labels followed by an ADDR*VEC,\n+\t   as this would be a part of the tablejump setup code.\n+\n+\t   Make a special exception to registers loaded with label\n+\t   values just before jump insns that use them.  */\n+\n+\tfor (note = REG_NOTES (insn); note; note = XEXP (note, 1))\n+\t  if (REG_NOTE_KIND (note) == REG_LABEL)\n+\t    {\n+\t      rtx lab = XEXP (note, 0), next;\n+\n+\t      if ((next = next_nonnote_insn (lab)) != NULL\n+\t\t       && GET_CODE (next) == JUMP_INSN\n+\t\t       && (GET_CODE (PATTERN (next)) == ADDR_VEC\n+\t\t\t   || GET_CODE (PATTERN (next)) == ADDR_DIFF_VEC))\n+\t\t;\n+\t      else if (GET_CODE (lab) == NOTE)\n+\t\t;\n+\t      else if (GET_CODE (NEXT_INSN (insn)) == JUMP_INSN\n+\t\t       && find_reg_note (NEXT_INSN (insn), REG_LABEL, lab))\n+\t\t;\n+\t      else\n+\t\tlvl = alloc_EXPR_LIST (0, XEXP (note, 0), lvl);\n+\t    }\n+      }\n+\n+  return lvl;\n+}\n+\f\n+/* Create an edge between two basic blocks.  FLAGS are auxiliary information\n+   about the edge that is accumulated between calls.  */\n+\n+/* Create an edge from a basic block to a label.  */\n+\n+static void\n+make_label_edge (edge_cache, src, label, flags)\n+     sbitmap *edge_cache;\n+     basic_block src;\n+     rtx label;\n+     int flags;\n+{\n+  if (GET_CODE (label) != CODE_LABEL)\n+    abort ();\n+\n+  /* If the label was never emitted, this insn is junk, but avoid a\n+     crash trying to refer to BLOCK_FOR_INSN (label).  This can happen\n+     as a result of a syntax error and a diagnostic has already been\n+     printed.  */\n+\n+  if (INSN_UID (label) == 0)\n+    return;\n+\n+  make_edge (edge_cache, src, BLOCK_FOR_INSN (label), flags);\n+}\n+\n+/* Create the edges generated by INSN in REGION.  */\n+\n+static void\n+make_eh_edge (edge_cache, src, insn)\n+     sbitmap *edge_cache;\n+     basic_block src;\n+     rtx insn;\n+{\n+  int is_call = (GET_CODE (insn) == CALL_INSN ? EDGE_ABNORMAL_CALL : 0);\n+  rtx handlers, i;\n+\n+  handlers = reachable_handlers (insn);\n+\n+  for (i = handlers; i; i = XEXP (i, 1))\n+    make_label_edge (edge_cache, src, XEXP (i, 0),\n+\t\t     EDGE_ABNORMAL | EDGE_EH | is_call);\n+\n+  free_INSN_LIST_list (&handlers);\n+}\n+/* Identify the edges between basic blocks MIN to MAX.\n+\n+   NONLOCAL_LABEL_LIST is a list of non-local labels in the function.  Blocks\n+   that are otherwise unreachable may be reachable with a non-local goto.\n+\n+   BB_EH_END is an array indexed by basic block number in which we record\n+   the list of exception regions active at the end of the basic block.  */\n+\n+static void\n+make_edges (label_value_list, min, max, update_p)\n+     rtx label_value_list;\n+     int min, max, update_p;\n+{\n+  int i;\n+  sbitmap *edge_cache = NULL;\n+\n+  /* Assume no computed jump; revise as we create edges.  */\n+  current_function_has_computed_jump = 0;\n+\n+  /* Heavy use of computed goto in machine-generated code can lead to\n+     nearly fully-connected CFGs.  In that case we spend a significant\n+     amount of time searching the edge lists for duplicates.  */\n+  if (forced_labels || label_value_list)\n+    {\n+      edge_cache = sbitmap_vector_alloc (n_basic_blocks, n_basic_blocks);\n+      sbitmap_vector_zero (edge_cache, n_basic_blocks);\n+\n+      if (update_p)\n+\tfor (i = min; i <= max; ++i)\n+\t  {\n+\t    edge e;\n+\t    for (e = BASIC_BLOCK (i)->succ; e ; e = e->succ_next)\n+\t      if (e->dest != EXIT_BLOCK_PTR)\n+\t        SET_BIT (edge_cache[i], e->dest->index);\n+\t  }\n+    }\n+\n+  /* By nature of the way these get numbered, block 0 is always the entry.  */\n+  make_edge (edge_cache, ENTRY_BLOCK_PTR, BASIC_BLOCK (0), EDGE_FALLTHRU);\n+\n+  for (i = min; i <= max; ++i)\n+    {\n+      basic_block bb = BASIC_BLOCK (i);\n+      rtx insn, x;\n+      enum rtx_code code;\n+      int force_fallthru = 0;\n+\n+      if (GET_CODE (bb->head) == CODE_LABEL\n+\t  && LABEL_ALTERNATE_NAME (bb->head))\n+\tmake_edge (NULL, ENTRY_BLOCK_PTR, bb, 0);\n+\n+      /* Examine the last instruction of the block, and discover the\n+\t ways we can leave the block.  */\n+\n+      insn = bb->end;\n+      code = GET_CODE (insn);\n+\n+      /* A branch.  */\n+      if (code == JUMP_INSN)\n+\t{\n+\t  rtx tmp;\n+\n+\t  /* Recognize exception handling placeholders.  */\n+\t  if (GET_CODE (PATTERN (insn)) == RESX)\n+\t    make_eh_edge (edge_cache, bb, insn);\n+\n+\t  /* Recognize a non-local goto as a branch outside the\n+\t     current function.  */\n+\t  else if (find_reg_note (insn, REG_NON_LOCAL_GOTO, NULL_RTX))\n+\t    ;\n+\n+\t  /* ??? Recognize a tablejump and do the right thing.  */\n+\t  else if ((tmp = JUMP_LABEL (insn)) != NULL_RTX\n+\t\t   && (tmp = NEXT_INSN (tmp)) != NULL_RTX\n+\t\t   && GET_CODE (tmp) == JUMP_INSN\n+\t\t   && (GET_CODE (PATTERN (tmp)) == ADDR_VEC\n+\t\t       || GET_CODE (PATTERN (tmp)) == ADDR_DIFF_VEC))\n+\t    {\n+\t      rtvec vec;\n+\t      int j;\n+\n+\t      if (GET_CODE (PATTERN (tmp)) == ADDR_VEC)\n+\t\tvec = XVEC (PATTERN (tmp), 0);\n+\t      else\n+\t\tvec = XVEC (PATTERN (tmp), 1);\n+\n+\t      for (j = GET_NUM_ELEM (vec) - 1; j >= 0; --j)\n+\t\tmake_label_edge (edge_cache, bb,\n+\t\t\t\t XEXP (RTVEC_ELT (vec, j), 0), 0);\n+\n+\t      /* Some targets (eg, ARM) emit a conditional jump that also\n+\t\t contains the out-of-range target.  Scan for these and\n+\t\t add an edge if necessary.  */\n+\t      if ((tmp = single_set (insn)) != NULL\n+\t\t  && SET_DEST (tmp) == pc_rtx\n+\t\t  && GET_CODE (SET_SRC (tmp)) == IF_THEN_ELSE\n+\t\t  && GET_CODE (XEXP (SET_SRC (tmp), 2)) == LABEL_REF)\n+\t\tmake_label_edge (edge_cache, bb,\n+\t\t\t\t XEXP (XEXP (SET_SRC (tmp), 2), 0), 0);\n+\n+#ifdef CASE_DROPS_THROUGH\n+\t      /* Silly VAXen.  The ADDR_VEC is going to be in the way of\n+\t\t us naturally detecting fallthru into the next block.  */\n+\t      force_fallthru = 1;\n+#endif\n+\t    }\n+\n+\t  /* If this is a computed jump, then mark it as reaching\n+\t     everything on the label_value_list and forced_labels list.  */\n+\t  else if (computed_jump_p (insn))\n+\t    {\n+\t      current_function_has_computed_jump = 1;\n+\n+\t      for (x = label_value_list; x; x = XEXP (x, 1))\n+\t\tmake_label_edge (edge_cache, bb, XEXP (x, 0), EDGE_ABNORMAL);\n+\n+\t      for (x = forced_labels; x; x = XEXP (x, 1))\n+\t\tmake_label_edge (edge_cache, bb, XEXP (x, 0), EDGE_ABNORMAL);\n+\t    }\n+\n+\t  /* Returns create an exit out.  */\n+\t  else if (returnjump_p (insn))\n+\t    make_edge (edge_cache, bb, EXIT_BLOCK_PTR, 0);\n+\n+\t  /* Otherwise, we have a plain conditional or unconditional jump.  */\n+\t  else\n+\t    {\n+\t      if (! JUMP_LABEL (insn))\n+\t\tabort ();\n+\t      make_label_edge (edge_cache, bb, JUMP_LABEL (insn), 0);\n+\t    }\n+\t}\n+\n+      /* If this is a sibling call insn, then this is in effect a\n+\t combined call and return, and so we need an edge to the\n+\t exit block.  No need to worry about EH edges, since we\n+\t wouldn't have created the sibling call in the first place.  */\n+\n+      if (code == CALL_INSN && SIBLING_CALL_P (insn))\n+\tmake_edge (edge_cache, bb, EXIT_BLOCK_PTR,\n+\t\t   EDGE_ABNORMAL | EDGE_ABNORMAL_CALL);\n+\n+      /* If this is a CALL_INSN, then mark it as reaching the active EH\n+\t handler for this CALL_INSN.  If we're handling non-call\n+\t exceptions then any insn can reach any of the active handlers.\n+\n+\t Also mark the CALL_INSN as reaching any nonlocal goto handler.  */\n+\n+      else if (code == CALL_INSN || flag_non_call_exceptions)\n+\t{\n+\t  /* Add any appropriate EH edges.  */\n+\t  make_eh_edge (edge_cache, bb, insn);\n+\n+\t  if (code == CALL_INSN && nonlocal_goto_handler_labels)\n+\t    {\n+\t      /* ??? This could be made smarter: in some cases it's possible\n+\t\t to tell that certain calls will not do a nonlocal goto.\n+\n+\t\t For example, if the nested functions that do the nonlocal\n+\t\t gotos do not have their addresses taken, then only calls to\n+\t\t those functions or to other nested functions that use them\n+\t\t could possibly do nonlocal gotos.  */\n+\t      /* We do know that a REG_EH_REGION note with a value less\n+\t\t than 0 is guaranteed not to perform a non-local goto.  */\n+\t      rtx note = find_reg_note (insn, REG_EH_REGION, NULL_RTX);\n+\t      if (!note || INTVAL (XEXP (note, 0)) >=  0)\n+\t\tfor (x = nonlocal_goto_handler_labels; x; x = XEXP (x, 1))\n+\t\t  make_label_edge (edge_cache, bb, XEXP (x, 0),\n+\t\t\t\t   EDGE_ABNORMAL | EDGE_ABNORMAL_CALL);\n+\t    }\n+\t}\n+\n+      /* Find out if we can drop through to the next block.  */\n+      insn = next_nonnote_insn (insn);\n+      if (!insn || (i + 1 == n_basic_blocks && force_fallthru))\n+\tmake_edge (edge_cache, bb, EXIT_BLOCK_PTR, EDGE_FALLTHRU);\n+      else if (i + 1 < n_basic_blocks)\n+\t{\n+\t  rtx tmp = BLOCK_HEAD (i + 1);\n+\t  if (GET_CODE (tmp) == NOTE)\n+\t    tmp = next_nonnote_insn (tmp);\n+\t  if (force_fallthru || insn == tmp)\n+\t    make_edge (edge_cache, bb, BASIC_BLOCK (i + 1), EDGE_FALLTHRU);\n+\t}\n+    }\n+\n+  if (edge_cache)\n+    sbitmap_vector_free (edge_cache);\n+}\n+\f\n+/* Find all basic blocks of the function whose first insn is F.\n+\n+   Collect and return a list of labels whose addresses are taken.  This\n+   will be used in make_edges for use with computed gotos.  */\n+\n+static void\n+find_basic_blocks_1 (f)\n+     rtx f;\n+{\n+  register rtx insn, next;\n+  int i = 0;\n+  rtx bb_note = NULL_RTX;\n+  rtx lvl = NULL_RTX;\n+  rtx trll = NULL_RTX;\n+  rtx head = NULL_RTX;\n+  rtx end = NULL_RTX;\n+\n+  /* We process the instructions in a slightly different way than we did\n+     previously.  This is so that we see a NOTE_BASIC_BLOCK after we have\n+     closed out the previous block, so that it gets attached at the proper\n+     place.  Since this form should be equivalent to the previous,\n+     count_basic_blocks continues to use the old form as a check.  */\n+\n+  for (insn = f; insn; insn = next)\n+    {\n+      enum rtx_code code = GET_CODE (insn);\n+\n+      next = NEXT_INSN (insn);\n+\n+      switch (code)\n+\t{\n+\tcase NOTE:\n+\t  {\n+\t    int kind = NOTE_LINE_NUMBER (insn);\n+\n+\t    /* Look for basic block notes with which to keep the\n+\t       basic_block_info pointers stable.  Unthread the note now;\n+\t       we'll put it back at the right place in create_basic_block.\n+\t       Or not at all if we've already found a note in this block.  */\n+\t    if (kind == NOTE_INSN_BASIC_BLOCK)\n+\t      {\n+\t\tif (bb_note == NULL_RTX)\n+\t\t  bb_note = insn;\n+\t\telse\n+\t\t  next = flow_delete_insn (insn);\n+\t      }\n+\t    break;\n+\t  }\n+\n+\tcase CODE_LABEL:\n+\t  /* A basic block starts at a label.  If we've closed one off due\n+\t     to a barrier or some such, no need to do it again.  */\n+\t  if (head != NULL_RTX)\n+\t    {\n+\t      create_basic_block (i++, head, end, bb_note);\n+\t      bb_note = NULL_RTX;\n+\t    }\n+\n+\t  head = end = insn;\n+\t  break;\n+\n+\tcase JUMP_INSN:\n+\t  /* A basic block ends at a jump.  */\n+\t  if (head == NULL_RTX)\n+\t    head = insn;\n+\t  else\n+\t    {\n+\t      /* ??? Make a special check for table jumps.  The way this\n+\t\t happens is truly and amazingly gross.  We are about to\n+\t\t create a basic block that contains just a code label and\n+\t\t an addr*vec jump insn.  Worse, an addr_diff_vec creates\n+\t\t its own natural loop.\n+\n+\t\t Prevent this bit of brain damage, pasting things together\n+\t\t correctly in make_edges.\n+\n+\t\t The correct solution involves emitting the table directly\n+\t\t on the tablejump instruction as a note, or JUMP_LABEL.  */\n+\n+\t      if (GET_CODE (PATTERN (insn)) == ADDR_VEC\n+\t\t  || GET_CODE (PATTERN (insn)) == ADDR_DIFF_VEC)\n+\t\t{\n+\t\t  head = end = NULL;\n+\t\t  n_basic_blocks--;\n+\t\t  break;\n+\t\t}\n+\t    }\n+\t  end = insn;\n+\t  goto new_bb_inclusive;\n+\n+\tcase BARRIER:\n+\t  /* A basic block ends at a barrier.  It may be that an unconditional\n+\t     jump already closed the basic block -- no need to do it again.  */\n+\t  if (head == NULL_RTX)\n+\t    break;\n+\t  goto new_bb_exclusive;\n+\n+\tcase CALL_INSN:\n+\t  {\n+\t    /* Record whether this call created an edge.  */\n+\t    rtx note = find_reg_note (insn, REG_EH_REGION, NULL_RTX);\n+\t    int region = (note ? INTVAL (XEXP (note, 0)) : 0);\n+\n+\t    if (GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+\t      {\n+\t\t/* Scan each of the alternatives for label refs.  */\n+\t\tlvl = find_label_refs (XEXP (PATTERN (insn), 0), lvl);\n+\t\tlvl = find_label_refs (XEXP (PATTERN (insn), 1), lvl);\n+\t\tlvl = find_label_refs (XEXP (PATTERN (insn), 2), lvl);\n+\t\t/* Record its tail recursion label, if any.  */\n+\t\tif (XEXP (PATTERN (insn), 3) != NULL_RTX)\n+\t\t  trll = alloc_EXPR_LIST (0, XEXP (PATTERN (insn), 3), trll);\n+\t      }\n+\n+\t    /* A basic block ends at a call that can either throw or\n+\t       do a non-local goto.  */\n+\t    if ((nonlocal_goto_handler_labels && region >= 0)\n+\t\t|| can_throw_internal (insn))\n+\t      {\n+\t      new_bb_inclusive:\n+\t\tif (head == NULL_RTX)\n+\t\t  head = insn;\n+\t\tend = insn;\n+\n+\t      new_bb_exclusive:\n+\t\tcreate_basic_block (i++, head, end, bb_note);\n+\t\thead = end = NULL_RTX;\n+\t\tbb_note = NULL_RTX;\n+\t\tbreak;\n+\t      }\n+\t  }\n+\t  /* Fall through.  */\n+\n+\tcase INSN:\n+\t  /* Non-call exceptions generate new blocks just like calls.  */\n+\t  if (flag_non_call_exceptions && can_throw_internal (insn))\n+\t    goto new_bb_inclusive;\n+\n+\t  if (head == NULL_RTX)\n+\t    head = insn;\n+\t  end = insn;\n+\t  break;\n+\n+\tdefault:\n+\t  abort ();\n+\t}\n+\n+      if (GET_CODE (insn) == INSN || GET_CODE (insn) == CALL_INSN)\n+\t{\n+\t  rtx note;\n+\n+\t  /* Make a list of all labels referred to other than by jumps.\n+\n+\t     Make a special exception for labels followed by an ADDR*VEC,\n+\t     as this would be a part of the tablejump setup code.\n+\n+\t     Make a special exception to registers loaded with label\n+\t     values just before jump insns that use them.  */\n+\n+\t  for (note = REG_NOTES (insn); note; note = XEXP (note, 1))\n+\t    if (REG_NOTE_KIND (note) == REG_LABEL)\n+\t      {\n+\t\trtx lab = XEXP (note, 0), next;\n+\n+\t\tif ((next = next_nonnote_insn (lab)) != NULL\n+\t\t\t && GET_CODE (next) == JUMP_INSN\n+\t\t\t && (GET_CODE (PATTERN (next)) == ADDR_VEC\n+\t\t\t     || GET_CODE (PATTERN (next)) == ADDR_DIFF_VEC))\n+\t\t  ;\n+\t\telse if (GET_CODE (lab) == NOTE)\n+\t\t  ;\n+\t\telse if (GET_CODE (NEXT_INSN (insn)) == JUMP_INSN\n+\t\t\t && find_reg_note (NEXT_INSN (insn), REG_LABEL, lab))\n+\t\t  ;\n+\t\telse\n+\t\t  lvl = alloc_EXPR_LIST (0, XEXP (note, 0), lvl);\n+\t      }\n+\t}\n+    }\n+\n+  if (head != NULL_RTX)\n+    create_basic_block (i++, head, end, bb_note);\n+  else if (bb_note)\n+    flow_delete_insn (bb_note);\n+\n+  if (i != n_basic_blocks)\n+    abort ();\n+\n+  label_value_list = lvl;\n+  tail_recursion_label_list = trll;\n+}\n+\n+\n+/* Find basic blocks of the current function.\n+   F is the first insn of the function and NREGS the number of register\n+   numbers in use.  */\n+\n+void\n+find_basic_blocks (f, nregs, file)\n+     rtx f;\n+     int nregs ATTRIBUTE_UNUSED;\n+     FILE *file ATTRIBUTE_UNUSED;\n+{\n+  int max_uid;\n+  timevar_push (TV_CFG);\n+\n+  /* Flush out existing data.  */\n+  if (basic_block_info != NULL)\n+    {\n+      int i;\n+\n+      clear_edges ();\n+\n+      /* Clear bb->aux on all extant basic blocks.  We'll use this as a\n+\t tag for reuse during create_basic_block, just in case some pass\n+\t copies around basic block notes improperly.  */\n+      for (i = 0; i < n_basic_blocks; ++i)\n+\tBASIC_BLOCK (i)->aux = NULL;\n+\n+      VARRAY_FREE (basic_block_info);\n+    }\n+\n+  n_basic_blocks = count_basic_blocks (f);\n+\n+  /* Size the basic block table.  The actual structures will be allocated\n+     by find_basic_blocks_1, since we want to keep the structure pointers\n+     stable across calls to find_basic_blocks.  */\n+  /* ??? This whole issue would be much simpler if we called find_basic_blocks\n+     exactly once, and thereafter we don't have a single long chain of\n+     instructions at all until close to the end of compilation when we\n+     actually lay them out.  */\n+\n+  VARRAY_BB_INIT (basic_block_info, n_basic_blocks, \"basic_block_info\");\n+\n+  find_basic_blocks_1 (f);\n+\n+  /* Record the block to which an insn belongs.  */\n+  /* ??? This should be done another way, by which (perhaps) a label is\n+     tagged directly with the basic block that it starts.  It is used for\n+     more than that currently, but IMO that is the only valid use.  */\n+\n+  max_uid = get_max_uid ();\n+#ifdef AUTO_INC_DEC\n+  /* Leave space for insns life_analysis makes in some cases for auto-inc.\n+     These cases are rare, so we don't need too much space.  */\n+  max_uid += max_uid / 10;\n+#endif\n+\n+  compute_bb_for_insn (max_uid);\n+\n+  /* Discover the edges of our cfg.  */\n+  make_edges (label_value_list, 0, n_basic_blocks - 1, 0);\n+\n+  /* Do very simple cleanup now, for the benefit of code that runs between\n+     here and cleanup_cfg, e.g. thread_prologue_and_epilogue_insns.  */\n+  tidy_fallthru_edges ();\n+\n+  mark_critical_edges ();\n+\n+#ifdef ENABLE_CHECKING\n+  verify_flow_info ();\n+#endif\n+  timevar_pop (TV_CFG);\n+}\n+\f\n+/* Assume that someone emitted code with control flow instructions to the\n+   basic block.  Update the data structure.  */\n+void\n+find_sub_basic_blocks (bb)\n+     basic_block bb;\n+{\n+  rtx insn = bb->head;\n+  rtx end = bb->end;\n+  rtx jump_insn = NULL_RTX;\n+  edge falltru = 0;\n+  basic_block first_bb = bb;\n+  int i;\n+\n+  if (insn == bb->end)\n+    return;\n+\n+  if (GET_CODE (insn) == CODE_LABEL)\n+    insn = NEXT_INSN (insn);\n+\n+  /* Scan insn chain and try to find new basic block boundaries.  */\n+  while (1)\n+    {\n+      enum rtx_code code = GET_CODE (insn);\n+      switch (code)\n+\t{\n+\tcase BARRIER:\n+\t  if (!jump_insn)\n+\t    abort ();\n+\t  break;\n+\t/* On code label, split current basic block.  */\n+\tcase CODE_LABEL:\n+\t  falltru = split_block (bb, PREV_INSN (insn));\n+\t  if (jump_insn)\n+\t    bb->end = jump_insn;\n+\t  bb = falltru->dest;\n+\t  remove_edge (falltru);\n+\t  jump_insn = 0;\n+\t  if (LABEL_ALTERNATE_NAME (insn))\n+\t    make_edge (NULL, ENTRY_BLOCK_PTR, bb, 0);\n+\t  break;\n+\tcase INSN:\n+\tcase JUMP_INSN:\n+\t  /* In case we've previously split insn on the JUMP_INSN, move the\n+\t     block header to proper place.  */\n+\t  if (jump_insn)\n+\t    {\n+\t      falltru = split_block (bb, PREV_INSN (insn));\n+\t      bb->end = jump_insn;\n+\t      bb = falltru->dest;\n+\t      remove_edge (falltru);\n+\t      jump_insn = 0;\n+\t    }\n+\t  /* We need some special care for those expressions.  */\n+\t  if (GET_CODE (insn) == JUMP_INSN)\n+\t    {\n+\t      if (GET_CODE (PATTERN (insn)) == ADDR_VEC\n+\t\t  || GET_CODE (PATTERN (insn)) == ADDR_DIFF_VEC)\n+\t\tabort();\n+\t      jump_insn = insn;\n+\t    }\n+\t  break;\n+\tdefault:\n+\t  break;\n+\t}\n+      if (insn == end)\n+\tbreak;\n+      insn = NEXT_INSN (insn);\n+    }\n+\n+  /* In case expander replaced normal insn by sequence terminating by\n+     return and barrier, or possibly other sequence not behaving like\n+     ordinary jump, we need to take care and move basic block boundary.  */\n+  if (jump_insn && GET_CODE (bb->end) != JUMP_INSN)\n+    bb->end = jump_insn;\n+\n+  /* We've possibly replaced the conditional jump by conditional jump\n+     followed by cleanup at fallthru edge, so the outgoing edges may\n+     be dead.  */\n+  purge_dead_edges (bb);\n+\n+  /* Now re-scan and wire in all edges.  This expect simple (conditional)\n+     jumps at the end of each new basic blocks.  */\n+  make_edges (NULL, first_bb->index, bb->index, 1);\n+\n+  /* Update branch probabilities.  Expect only (un)conditional jumps\n+     to be created with only the forward edges.  */\n+  for (i = first_bb->index; i <= bb->index; i++)\n+    {\n+      edge e,f;\n+      basic_block b = BASIC_BLOCK (i);\n+      if (b != first_bb)\n+\t{\n+\t  b->count = 0;\n+\t  b->frequency = 0;\n+\t  for (e = b->pred; e; e=e->pred_next)\n+\t    {\n+\t      b->count += e->count;\n+\t      b->frequency += EDGE_FREQUENCY (e);\n+\t    }\n+\t}\n+      if (b->succ && b->succ->succ_next && !b->succ->succ_next->succ_next)\n+\t{\n+\t  rtx note = find_reg_note (b->end, REG_BR_PROB, NULL);\n+\t  int probability;\n+\n+\t  if (!note)\n+\t    continue;\n+\t  probability = INTVAL (XEXP (find_reg_note (b->end,\n+\t\t\t\t\t\t     REG_BR_PROB,\n+\t\t\t\t\t\t     NULL), 0));\n+\t  e = BRANCH_EDGE (b);\n+\t  e->probability = probability;\n+\t  e->count = ((b->count * probability + REG_BR_PROB_BASE / 2)\n+\t\t      / REG_BR_PROB_BASE);\n+\t  f = FALLTHRU_EDGE (b);\n+\t  f->probability = REG_BR_PROB_BASE - probability;\n+\t  f->count = b->count - e->count;\n+\t}\n+      if (b->succ && !b->succ->succ_next)\n+\t{\n+\t  e = b->succ;\n+\t  e->probability = REG_BR_PROB_BASE;\n+\t  e->count = b->count;\n+\t}\n+    }\n+}"}, {"sha": "00eb80adc7c115264b02ab164327604b50a87953", "filename": "gcc/cfgcleanup.c", "status": "added", "additions": 1248, "deletions": 0, "changes": 1248, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fcfgcleanup.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fcfgcleanup.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfgcleanup.c?ref=402209ff48d3e1984111c536033aa638f4271531", "patch": "@@ -0,0 +1,1248 @@\n+/* Control flow optimization code for GNU compiler.\n+   Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n+   1999, 2000, 2001 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 2, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+/* This file contains optimizer of the control flow.  The main entrypoint is\n+   cleanup_cfg.  Following optimizations are performed:\n+\n+   - Unreachable blocks removal\n+   - Edge forwarding (edge to the forwarder block is forwarded to it's\n+     succesor.  Simplification of the branch instruction is performed by\n+     underlying infrastructure so branch can be converted to simplejump or\n+     elliminated).\n+   - Cross jumping (tail merging)\n+   - Conditional jump-around-simplejump simplification\n+   - Basic block merging.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"rtl.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"timevar.h\"\n+#include \"output.h\"\n+#include \"insn-config.h\"\n+#include \"flags.h\"\n+#include \"recog.h\"\n+#include \"toplev.h\"\n+\n+#include \"obstack.h\"\n+\n+static bool try_crossjump_to_edge\tPARAMS ((int, edge, edge));\n+static bool try_crossjump_bb\t\tPARAMS ((int, basic_block));\n+static bool outgoing_edges_match\tPARAMS ((basic_block, basic_block));\n+static int flow_find_cross_jump\t\tPARAMS ((int, basic_block, basic_block,\n+\t\t\t\t\t\t rtx *, rtx *));\n+\n+static bool delete_unreachable_blocks\tPARAMS ((void));\n+static int tail_recursion_label_p\tPARAMS ((rtx));\n+static int merge_blocks_move_predecessor_nojumps PARAMS ((basic_block,\n+\t\t\t\t\t\t\t  basic_block));\n+static int merge_blocks_move_successor_nojumps PARAMS ((basic_block,\n+\t\t\t\t\t\t\tbasic_block));\n+static int merge_blocks\t\t\tPARAMS ((edge,basic_block,basic_block,\n+\t\t\t\t\t\t int));\n+static bool try_optimize_cfg\t\tPARAMS ((int));\n+static bool try_simplify_condjump\tPARAMS ((basic_block));\n+static bool try_forward_edges\t\tPARAMS ((int, basic_block));\n+\f\n+/* Simplify a conditional jump around an unconditional jump.\n+   Return true if something changed.  */\n+\n+static bool\n+try_simplify_condjump (cbranch_block)\n+     basic_block cbranch_block;\n+{\n+  basic_block jump_block, jump_dest_block, cbranch_dest_block;\n+  edge cbranch_jump_edge, cbranch_fallthru_edge;\n+  rtx cbranch_insn;\n+\n+  /* Verify that there are exactly two successors.  */\n+  if (!cbranch_block->succ\n+      || !cbranch_block->succ->succ_next\n+      || cbranch_block->succ->succ_next->succ_next)\n+    return false;\n+\n+  /* Verify that we've got a normal conditional branch at the end\n+     of the block.  */\n+  cbranch_insn = cbranch_block->end;\n+  if (!any_condjump_p (cbranch_insn))\n+    return false;\n+\n+  cbranch_fallthru_edge = FALLTHRU_EDGE (cbranch_block);\n+  cbranch_jump_edge = BRANCH_EDGE (cbranch_block);\n+\n+  /* The next block must not have multiple predecessors, must not\n+     be the last block in the function, and must contain just the\n+     unconditional jump.  */\n+  jump_block = cbranch_fallthru_edge->dest;\n+  if (jump_block->pred->pred_next\n+      || jump_block->index == n_basic_blocks - 1\n+      || !forwarder_block_p (jump_block))\n+    return false;\n+  jump_dest_block = jump_block->succ->dest;\n+\n+  /* The conditional branch must target the block after the\n+     unconditional branch.  */\n+  cbranch_dest_block = cbranch_jump_edge->dest;\n+\n+  if (!can_fallthru (jump_block, cbranch_dest_block))\n+    return false;\n+\n+  /* Invert the conditional branch.  Prevent jump.c from deleting\n+     \"unreachable\" instructions.  */\n+  LABEL_NUSES (JUMP_LABEL (cbranch_insn))++;\n+  if (!invert_jump (cbranch_insn, block_label (jump_dest_block), 1))\n+    {\n+      LABEL_NUSES (JUMP_LABEL (cbranch_insn))--;\n+      return false;\n+    }\n+\n+  if (rtl_dump_file)\n+    fprintf (rtl_dump_file, \"Simplifying condjump %i around jump %i\\n\",\n+\t     INSN_UID (cbranch_insn), INSN_UID (jump_block->end));\n+\n+  /* Success.  Update the CFG to match.  Note that after this point\n+     the edge variable names appear backwards; the redirection is done\n+     this way to preserve edge profile data.  */\n+  cbranch_jump_edge = redirect_edge_succ_nodup (cbranch_jump_edge,\n+\t\t\t\t\t\tcbranch_dest_block);\n+  cbranch_fallthru_edge = redirect_edge_succ_nodup (cbranch_fallthru_edge,\n+\t\t\t\t\t\t    jump_dest_block);\n+  cbranch_jump_edge->flags |= EDGE_FALLTHRU;\n+  cbranch_fallthru_edge->flags &= ~EDGE_FALLTHRU;\n+\n+  /* Delete the block with the unconditional jump, and clean up the mess.  */\n+  flow_delete_block (jump_block);\n+  tidy_fallthru_edge (cbranch_jump_edge, cbranch_block, cbranch_dest_block);\n+\n+  return true;\n+}\n+\f\n+/* Attempt to forward edges leaving basic block B.\n+   Return true if sucessful.  */\n+\n+static bool\n+try_forward_edges (mode, b)\n+     basic_block b;\n+     int mode;\n+{\n+  bool changed = false;\n+  edge e, next;\n+\n+  for (e = b->succ; e ; e = next)\n+    {\n+      basic_block target, first;\n+      int counter;\n+\n+      next = e->succ_next;\n+\n+      /* Skip complex edges because we don't know how to update them.\n+\n+         Still handle fallthru edges, as we can suceed to forward fallthru\n+         edge to the same place as the branch edge of conditional branch\n+         and turn conditional branch to an unconditonal branch.  */\n+      if (e->flags & EDGE_COMPLEX)\n+\tcontinue;\n+\n+      target = first = e->dest;\n+      counter = 0;\n+\n+      /* Look for the real destination of the jump.\n+         Avoid inifinite loop in the infinite empty loop by counting\n+         up to n_basic_blocks.  */\n+      while (forwarder_block_p (target)\n+\t     && target->succ->dest != EXIT_BLOCK_PTR\n+\t     && counter < n_basic_blocks)\n+\t{\n+\t  /* Bypass trivial infinite loops.  */\n+\t  if (target == target->succ->dest)\n+\t    counter = n_basic_blocks;\n+\n+\t  /* Avoid killing of loop pre-headers, as it is the place loop\n+\t     optimizer wants to hoist code to.\n+\n+\t     For fallthru forwarders, the LOOP_BEG note must appear between\n+\t     the header of block and CODE_LABEL of the loop, for non forwarders\n+\t     it must appear before the JUMP_INSN.  */\n+\t  if (mode & CLEANUP_PRE_LOOP)\n+\t    {\n+\t      rtx insn = (target->succ->flags & EDGE_FALLTHRU\n+\t\t          ? target->head : prev_nonnote_insn (target->end));\n+\n+\t      if (GET_CODE (insn) != NOTE)\n+\t\tinsn = NEXT_INSN (insn);\n+\n+\t      for (;insn && GET_CODE (insn) != CODE_LABEL && !INSN_P (insn);\n+\t\t   insn = NEXT_INSN (insn))\n+\t\tif (GET_CODE (insn) == NOTE\n+\t\t    && NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_BEG)\n+\t\t  break;\n+\n+\t      if (GET_CODE (insn) == NOTE)\n+\t\tbreak;\n+\t    }\n+\t  target = target->succ->dest, counter++;\n+\t}\n+\n+      if (counter >= n_basic_blocks)\n+\t{\n+\t  if (rtl_dump_file)\n+\t    fprintf (rtl_dump_file, \"Infinite loop in BB %i.\\n\",\n+\t\t     target->index);\n+\t}\n+      else if (target == first)\n+\t; /* We didn't do anything.  */\n+      else\n+\t{\n+\t  /* Save the values now, as the edge may get removed.  */\n+\t  gcov_type edge_count = e->count;\n+\t  int edge_probability = e->probability;\n+\n+\t  if (redirect_edge_and_branch (e, target))\n+\t    {\n+\t      /* We successfully forwarded the edge.  Now update profile\n+\t\t data: for each edge we traversed in the chain, remove\n+\t\t the original edge's execution count.  */\n+\t      int edge_frequency = ((edge_probability * b->frequency\n+\t\t\t\t     + REG_BR_PROB_BASE / 2)\n+\t\t\t\t    / REG_BR_PROB_BASE);\n+\n+\t      do\n+\t\t{\n+\t\t  first->count -= edge_count;\n+\t\t  first->succ->count -= edge_count;\n+\t\t  first->frequency -= edge_frequency;\n+\t\t  first = first->succ->dest;\n+\t\t}\n+\t      while (first != target);\n+\n+\t      changed = true;\n+\t    }\n+\t  else\n+\t    {\n+\t      if (rtl_dump_file)\n+\t\tfprintf (rtl_dump_file, \"Forwarding edge %i->%i to %i failed.\\n\",\n+\t\t\t b->index, e->dest->index, target->index);\n+\t    }\n+\t}\n+    }\n+\n+  return changed;\n+}\n+\f\n+static int\n+tail_recursion_label_p (label)\n+     rtx label;\n+{\n+  rtx x;\n+\n+  for (x = tail_recursion_label_list; x; x = XEXP (x, 1))\n+    if (label == XEXP (x, 0))\n+      return 1;\n+\n+  return 0;\n+}\n+\n+/* Blocks A and B are to be merged into a single block.  A has no incoming\n+   fallthru edge, so it can be moved before B without adding or modifying\n+   any jumps (aside from the jump from A to B).  */\n+\n+static int\n+merge_blocks_move_predecessor_nojumps (a, b)\n+     basic_block a, b;\n+{\n+  rtx barrier;\n+  int index;\n+\n+  barrier = next_nonnote_insn (a->end);\n+  if (GET_CODE (barrier) != BARRIER)\n+    abort ();\n+  flow_delete_insn (barrier);\n+\n+  /* Move block and loop notes out of the chain so that we do not\n+     disturb their order.\n+\n+     ??? A better solution would be to squeeze out all the non-nested notes\n+     and adjust the block trees appropriately.   Even better would be to have\n+     a tighter connection between block trees and rtl so that this is not\n+     necessary.  */\n+  squeeze_notes (&a->head, &a->end);\n+\n+  /* Scramble the insn chain.  */\n+  if (a->end != PREV_INSN (b->head))\n+    reorder_insns (a->head, a->end, PREV_INSN (b->head));\n+\n+  if (rtl_dump_file)\n+    {\n+      fprintf (rtl_dump_file, \"Moved block %d before %d and merged.\\n\",\n+\t       a->index, b->index);\n+    }\n+\n+  /* Swap the records for the two blocks around.  Although we are deleting B,\n+     A is now where B was and we want to compact the BB array from where\n+     A used to be.  */\n+  BASIC_BLOCK (a->index) = b;\n+  BASIC_BLOCK (b->index) = a;\n+  index = a->index;\n+  a->index = b->index;\n+  b->index = index;\n+\n+  /* Now blocks A and B are contiguous.  Merge them.  */\n+  merge_blocks_nomove (a, b);\n+\n+  return 1;\n+}\n+\n+/* Blocks A and B are to be merged into a single block.  B has no outgoing\n+   fallthru edge, so it can be moved after A without adding or modifying\n+   any jumps (aside from the jump from A to B).  */\n+\n+static int\n+merge_blocks_move_successor_nojumps (a, b)\n+     basic_block a, b;\n+{\n+  rtx barrier;\n+\n+  barrier = NEXT_INSN (b->end);\n+\n+  /* Recognize a jump table following block B.  */\n+  if (barrier\n+      && GET_CODE (barrier) == CODE_LABEL\n+      && NEXT_INSN (barrier)\n+      && GET_CODE (NEXT_INSN (barrier)) == JUMP_INSN\n+      && (GET_CODE (PATTERN (NEXT_INSN (barrier))) == ADDR_VEC\n+\t  || GET_CODE (PATTERN (NEXT_INSN (barrier))) == ADDR_DIFF_VEC))\n+    {\n+      b->end = NEXT_INSN (barrier);\n+      barrier = NEXT_INSN (b->end);\n+    }\n+\n+  /* There had better have been a barrier there.  Delete it.  */\n+  if (barrier && GET_CODE (barrier) == BARRIER)\n+    flow_delete_insn (barrier);\n+\n+  /* Move block and loop notes out of the chain so that we do not\n+     disturb their order.\n+\n+     ??? A better solution would be to squeeze out all the non-nested notes\n+     and adjust the block trees appropriately.   Even better would be to have\n+     a tighter connection between block trees and rtl so that this is not\n+     necessary.  */\n+  squeeze_notes (&b->head, &b->end);\n+\n+  /* Scramble the insn chain.  */\n+  reorder_insns (b->head, b->end, a->end);\n+\n+  /* Now blocks A and B are contiguous.  Merge them.  */\n+  merge_blocks_nomove (a, b);\n+\n+  if (rtl_dump_file)\n+    {\n+      fprintf (rtl_dump_file, \"Moved block %d after %d and merged.\\n\",\n+\t       b->index, a->index);\n+    }\n+\n+  return 1;\n+}\n+\n+/* Attempt to merge basic blocks that are potentially non-adjacent.\n+   Return true iff the attempt succeeded.  */\n+\n+static int\n+merge_blocks (e, b, c, mode)\n+     edge e;\n+     basic_block b, c;\n+     int mode;\n+{\n+  /* If C has a tail recursion label, do not merge.  There is no\n+     edge recorded from the call_placeholder back to this label, as\n+     that would make optimize_sibling_and_tail_recursive_calls more\n+     complex for no gain.  */\n+  if (GET_CODE (c->head) == CODE_LABEL\n+      && tail_recursion_label_p (c->head))\n+    return 0;\n+\n+  /* If B has a fallthru edge to C, no need to move anything.  */\n+  if (e->flags & EDGE_FALLTHRU)\n+    {\n+      merge_blocks_nomove (b, c);\n+\n+      if (rtl_dump_file)\n+\t{\n+\t  fprintf (rtl_dump_file, \"Merged %d and %d without moving.\\n\",\n+\t\t   b->index, c->index);\n+\t}\n+\n+      return 1;\n+    }\n+  /* Otherwise we will need to move code around.  Do that only if expensive\n+     transformations are allowed.  */\n+  else if (mode & CLEANUP_EXPENSIVE)\n+    {\n+      edge tmp_edge, c_fallthru_edge;\n+      int c_has_outgoing_fallthru;\n+      int b_has_incoming_fallthru;\n+\n+      /* Avoid overactive code motion, as the forwarder blocks should be\n+         eliminated by edge redirection instead.  One exception might have\n+\t been if B is a forwarder block and C has no fallthru edge, but\n+\t that should be cleaned up by bb-reorder instead.  */\n+      if (forwarder_block_p (b) || forwarder_block_p (c))\n+\treturn 0;\n+\n+      /* We must make sure to not munge nesting of lexical blocks,\n+\t and loop notes.  This is done by squeezing out all the notes\n+\t and leaving them there to lie.  Not ideal, but functional.  */\n+\n+      for (tmp_edge = c->succ; tmp_edge; tmp_edge = tmp_edge->succ_next)\n+\tif (tmp_edge->flags & EDGE_FALLTHRU)\n+\t  break;\n+      c_has_outgoing_fallthru = (tmp_edge != NULL);\n+      c_fallthru_edge = tmp_edge;\n+\n+      for (tmp_edge = b->pred; tmp_edge; tmp_edge = tmp_edge->pred_next)\n+\tif (tmp_edge->flags & EDGE_FALLTHRU)\n+\t  break;\n+      b_has_incoming_fallthru = (tmp_edge != NULL);\n+\n+      /* If B does not have an incoming fallthru, then it can be moved\n+\t immediately before C without introducing or modifying jumps.\n+\t C cannot be the first block, so we do not have to worry about\n+\t accessing a non-existent block.  */\n+      if (! b_has_incoming_fallthru)\n+\treturn merge_blocks_move_predecessor_nojumps (b, c);\n+\n+      /* Otherwise, we're going to try to move C after B.  If C does\n+\t not have an outgoing fallthru, then it can be moved\n+\t immediately after B without introducing or modifying jumps.  */\n+      if (! c_has_outgoing_fallthru)\n+\treturn merge_blocks_move_successor_nojumps (b, c);\n+\n+      /* Otherwise, we'll need to insert an extra jump, and possibly\n+\t a new block to contain it.  We can't redirect to EXIT_BLOCK_PTR,\n+\t as we don't have explicit return instructions before epilogues\n+\t are generated, so give up on that case.  */\n+\n+      if (c_fallthru_edge->dest != EXIT_BLOCK_PTR\n+\t  && merge_blocks_move_successor_nojumps (b, c))\n+        {\n+\t  basic_block target = c_fallthru_edge->dest;\n+\t  rtx barrier;\n+\t  basic_block new;\n+\n+\t  /* This is a dirty hack to avoid code duplication.\n+\n+\t     Set edge to point to wrong basic block, so\n+\t     redirect_edge_and_branch_force will do the trick\n+\t     and rewire edge back to the original location.  */\n+\t  redirect_edge_succ (c_fallthru_edge, ENTRY_BLOCK_PTR);\n+\t  new = redirect_edge_and_branch_force (c_fallthru_edge, target);\n+\n+\t  /* We've just created barrier, but another barrier is\n+\t     already present in the stream.  Avoid the duplicate.  */\n+\t  barrier = next_nonnote_insn (new ? new->end : b->end);\n+\t  if (GET_CODE (barrier) != BARRIER)\n+\t    abort ();\n+\t  flow_delete_insn (barrier);\n+\n+\t  return 1;\n+        }\n+\n+      return 0;\n+    }\n+  return 0;\n+}\n+\f\n+/* Look through the insns at the end of BB1 and BB2 and find the longest\n+   sequence that are equivalent.  Store the first insns for that sequence\n+   in *F1 and *F2 and return the sequence length.\n+\n+   To simplify callers of this function, if the blocks match exactly,\n+   store the head of the blocks in *F1 and *F2.  */\n+\n+static int\n+flow_find_cross_jump (mode, bb1, bb2, f1, f2)\n+     int mode ATTRIBUTE_UNUSED;\n+     basic_block bb1, bb2;\n+     rtx *f1, *f2;\n+{\n+  rtx i1, i2, p1, p2, last1, last2, afterlast1, afterlast2;\n+  int ninsns = 0;\n+\n+  /* Skip simple jumps at the end of the blocks.  Complex jumps still\n+     need to be compared for equivalence, which we'll do below.  */\n+\n+  i1 = bb1->end;\n+  if (onlyjump_p (i1)\n+      || (returnjump_p (i1) && !side_effects_p (PATTERN (i1))))\n+    i1 = PREV_INSN (i1);\n+  i2 = bb2->end;\n+  if (onlyjump_p (i2)\n+      || (returnjump_p (i2) && !side_effects_p (PATTERN (i2))))\n+    i2 = PREV_INSN (i2);\n+\n+  last1 = afterlast1 = last2 = afterlast2 = NULL_RTX;\n+  while (true)\n+    {\n+      /* Ignore notes.  */\n+      while ((GET_CODE (i1) == NOTE && i1 != bb1->head))\n+\ti1 = PREV_INSN (i1);\n+      while ((GET_CODE (i2) == NOTE && i2 != bb2->head))\n+\ti2 = PREV_INSN (i2);\n+\n+      if (i1 == bb1->head || i2 == bb2->head)\n+\tbreak;\n+\n+      /* Verify that I1 and I2 are equivalent.  */\n+\n+      if (GET_CODE (i1) != GET_CODE (i2))\n+\tbreak;\n+\n+      p1 = PATTERN (i1);\n+      p2 = PATTERN (i2);\n+\n+      /* If this is a CALL_INSN, compare register usage information.\n+\t If we don't check this on stack register machines, the two\n+\t CALL_INSNs might be merged leaving reg-stack.c with mismatching\n+\t numbers of stack registers in the same basic block.\n+\t If we don't check this on machines with delay slots, a delay slot may\n+\t be filled that clobbers a parameter expected by the subroutine.\n+\n+\t ??? We take the simple route for now and assume that if they're\n+\t equal, they were constructed identically.  */\n+\n+      if (GET_CODE (i1) == CALL_INSN\n+\t  && ! rtx_equal_p (CALL_INSN_FUNCTION_USAGE (i1),\n+\t\t\t    CALL_INSN_FUNCTION_USAGE (i2)))\n+\tbreak;\n+\n+#ifdef STACK_REGS\n+      /* If cross_jump_death_matters is not 0, the insn's mode\n+\t indicates whether or not the insn contains any stack-like\n+\t regs.  */\n+\n+      if ((mode & CLEANUP_POST_REGSTACK) && stack_regs_mentioned (i1))\n+\t{\n+\t  /* If register stack conversion has already been done, then\n+\t     death notes must also be compared before it is certain that\n+\t     the two instruction streams match.  */\n+\n+\t  rtx note;\n+\t  HARD_REG_SET i1_regset, i2_regset;\n+\n+\t  CLEAR_HARD_REG_SET (i1_regset);\n+\t  CLEAR_HARD_REG_SET (i2_regset);\n+\n+\t  for (note = REG_NOTES (i1); note; note = XEXP (note, 1))\n+\t    if (REG_NOTE_KIND (note) == REG_DEAD\n+\t\t&& STACK_REG_P (XEXP (note, 0)))\n+\t      SET_HARD_REG_BIT (i1_regset, REGNO (XEXP (note, 0)));\n+\n+\t  for (note = REG_NOTES (i2); note; note = XEXP (note, 1))\n+\t    if (REG_NOTE_KIND (note) == REG_DEAD\n+\t\t&& STACK_REG_P (XEXP (note, 0)))\n+\t      SET_HARD_REG_BIT (i2_regset, REGNO (XEXP (note, 0)));\n+\n+\t  GO_IF_HARD_REG_EQUAL (i1_regset, i2_regset, done);\n+\n+\t  break;\n+\n+\tdone:\n+\t  ;\n+\t}\n+#endif\n+\n+      if (GET_CODE (p1) != GET_CODE (p2))\n+\tbreak;\n+\n+      if (! rtx_renumbered_equal_p (p1, p2))\n+\t{\n+\t  /* The following code helps take care of G++ cleanups.  */\n+\t  rtx equiv1 = find_reg_equal_equiv_note (i1);\n+\t  rtx equiv2 = find_reg_equal_equiv_note (i2);\n+\n+\t  if (equiv1 && equiv2\n+\t      /* If the equivalences are not to a constant, they may\n+\t\t reference pseudos that no longer exist, so we can't\n+\t\t use them.  */\n+\t      && CONSTANT_P (XEXP (equiv1, 0))\n+\t      && rtx_equal_p (XEXP (equiv1, 0), XEXP (equiv2, 0)))\n+\t    {\n+\t      rtx s1 = single_set (i1);\n+\t      rtx s2 = single_set (i2);\n+\t      if (s1 != 0 && s2 != 0\n+\t\t  && rtx_renumbered_equal_p (SET_DEST (s1), SET_DEST (s2)))\n+\t\t{\n+\t\t  validate_change (i1, &SET_SRC (s1), XEXP (equiv1, 0), 1);\n+\t\t  validate_change (i2, &SET_SRC (s2), XEXP (equiv2, 0), 1);\n+\t\t  if (! rtx_renumbered_equal_p (p1, p2))\n+\t\t    cancel_changes (0);\n+\t\t  else if (apply_change_group ())\n+\t\t    goto win;\n+\t\t}\n+\t    }\n+\t  break;\n+\t}\n+\n+    win:\n+      /* Don't begin a cross-jump with a USE or CLOBBER insn.  */\n+      if (GET_CODE (p1) != USE && GET_CODE (p1) != CLOBBER)\n+\t{\n+\t  afterlast1 = last1, afterlast2 = last2;\n+\t  last1 = i1, last2 = i2;\n+          ninsns++;\n+\t}\n+      i1 = PREV_INSN (i1);\n+      i2 = PREV_INSN (i2);\n+    }\n+\n+#ifdef HAVE_cc0\n+  if (ninsns)\n+    {\n+      /* Don't allow the insn after a compare to be shared by\n+\t cross-jumping unless the compare is also shared.  */\n+      if (reg_mentioned_p (cc0_rtx, last1) && ! sets_cc0_p (last1))\n+\tlast1 = afterlast1, last2 = afterlast2, ninsns--;\n+    }\n+#endif\n+\n+  /* Include preceeding notes and labels in the cross-jump.  One,\n+     this may bring us to the head of the blocks as requested above.\n+     Two, it keeps line number notes as matched as may be.  */\n+  if (ninsns)\n+    {\n+      while (last1 != bb1->head && GET_CODE (PREV_INSN (last1)) == NOTE)\n+\tlast1 = PREV_INSN (last1);\n+      if (last1 != bb1->head && GET_CODE (PREV_INSN (last1)) == CODE_LABEL)\n+\tlast1 = PREV_INSN (last1);\n+      while (last2 != bb2->head && GET_CODE (PREV_INSN (last2)) == NOTE)\n+\tlast2 = PREV_INSN (last2);\n+      if (last2 != bb2->head && GET_CODE (PREV_INSN (last2)) == CODE_LABEL)\n+\tlast2 = PREV_INSN (last2);\n+\n+      *f1 = last1;\n+      *f2 = last2;\n+    }\n+\n+  return ninsns;\n+}\n+\n+/* Return true iff outgoing edges of BB1 and BB2 match, together with\n+   the branch instruction.  This means that if we commonize the control\n+   flow before end of the basic block, the semantic remains unchanged.\n+\n+   We may assume that there exists one edge with a common destination.  */\n+\n+static bool\n+outgoing_edges_match (bb1, bb2)\n+     basic_block bb1;\n+     basic_block bb2;\n+{\n+  /* If BB1 has only one successor, we must be looking at an unconditional\n+     jump.  Which, by the assumption above, means that we only need to check\n+     that BB2 has one successor.  */\n+  if (bb1->succ && !bb1->succ->succ_next)\n+    return (bb2->succ && !bb2->succ->succ_next);\n+\n+  /* Match conditional jumps - this may get tricky when fallthru and branch\n+     edges are crossed.  */\n+  if (bb1->succ\n+      && bb1->succ->succ_next\n+      && !bb1->succ->succ_next->succ_next\n+      && any_condjump_p (bb1->end))\n+    {\n+      edge b1, f1, b2, f2;\n+      bool reverse, match;\n+      rtx set1, set2, cond1, cond2;\n+      enum rtx_code code1, code2;\n+\n+      if (!bb2->succ\n+          || !bb2->succ->succ_next\n+\t  || bb1->succ->succ_next->succ_next\n+\t  || !any_condjump_p (bb2->end))\n+\treturn false;\n+\n+      b1 = BRANCH_EDGE (bb1);\n+      b2 = BRANCH_EDGE (bb2);\n+      f1 = FALLTHRU_EDGE (bb1);\n+      f2 = FALLTHRU_EDGE (bb2);\n+\n+      /* Get around possible forwarders on fallthru edges.  Other cases\n+         should be optimized out already.  */\n+      if (forwarder_block_p (f1->dest))\n+\tf1 = f1->dest->succ;\n+      if (forwarder_block_p (f2->dest))\n+\tf2 = f2->dest->succ;\n+\n+      /* To simplify use of this function, return false if there are\n+\t unneeded forwarder blocks.  These will get eliminated later\n+\t during cleanup_cfg.  */\n+      if (forwarder_block_p (f1->dest)\n+\t  || forwarder_block_p (f2->dest)\n+\t  || forwarder_block_p (b1->dest)\n+\t  || forwarder_block_p (b2->dest))\n+\treturn false;\n+\n+      if (f1->dest == f2->dest && b1->dest == b2->dest)\n+\treverse = false;\n+      else if (f1->dest == b2->dest && b1->dest == f2->dest)\n+\treverse = true;\n+      else\n+\treturn false;\n+\n+      set1 = pc_set (bb1->end);\n+      set2 = pc_set (bb2->end);\n+      if ((XEXP (SET_SRC (set1), 1) == pc_rtx)\n+\t  != (XEXP (SET_SRC (set2), 1) == pc_rtx))\n+\treverse = !reverse;\n+\n+      cond1 = XEXP (SET_SRC (set1), 0);\n+      cond2 = XEXP (SET_SRC (set2), 0);\n+      code1 = GET_CODE (cond1);\n+      if (reverse)\n+\tcode2 = reversed_comparison_code (cond2, bb2->end);\n+      else\n+\tcode2 = GET_CODE (cond2);\n+      if (code2 == UNKNOWN)\n+\treturn false;\n+\n+      /* Verify codes and operands match.  */\n+      match = ((code1 == code2\n+\t\t&& rtx_renumbered_equal_p (XEXP (cond1, 0), XEXP (cond2, 0))\n+\t\t&& rtx_renumbered_equal_p (XEXP (cond1, 1), XEXP (cond2, 1)))\n+\t       || (code1 == swap_condition (code2)\n+\t\t   && rtx_renumbered_equal_p (XEXP (cond1, 1),\n+\t\t\t\t\t      XEXP (cond2, 0))\n+\t\t   && rtx_renumbered_equal_p (XEXP (cond1, 0),\n+\t\t\t\t\t      XEXP (cond2, 1))));\n+\n+      /* If we return true, we will join the blocks.  Which means that\n+\t we will only have one branch prediction bit to work with.  Thus\n+\t we require the existing branches to have probabilities that are\n+\t roughly similar.  */\n+      /* ??? We should use bb->frequency to allow merging in infrequently\n+\t executed blocks, but at the moment it is not available when\n+\t cleanup_cfg is run.  */\n+      if (match && !optimize_size)\n+\t{\n+\t  rtx note1, note2;\n+\t  int prob1, prob2;\n+\t  note1 = find_reg_note (bb1->end, REG_BR_PROB, 0);\n+\t  note2 = find_reg_note (bb2->end, REG_BR_PROB, 0);\n+\n+\t  if (note1 && note2)\n+\t    {\n+\t      prob1 = INTVAL (XEXP (note1, 0));\n+\t      prob2 = INTVAL (XEXP (note2, 0));\n+\t      if (reverse)\n+\t\tprob2 = REG_BR_PROB_BASE - prob2;\n+\n+\t      /* Fail if the difference in probabilities is\n+\t\t greater than 5%.  */\n+\t      if (abs (prob1 - prob2) > REG_BR_PROB_BASE / 20)\n+\t\treturn false;\n+\t    }\n+\t  else if (note1 || note2)\n+\t    return false;\n+\t}\n+\n+      if (rtl_dump_file && match)\n+\tfprintf (rtl_dump_file, \"Conditionals in bb %i and %i match.\\n\",\n+\t\t bb1->index, bb2->index);\n+\n+      return match;\n+    }\n+\n+  /* ??? We can handle computed jumps too.  This may be important for\n+     inlined functions containing switch statements.  Also jumps w/o\n+     fallthru edges can be handled by simply matching whole insn.  */\n+  return false;\n+}\n+\n+/* E1 and E2 are edges with the same destination block.  Search their\n+   predecessors for common code.  If found, redirect control flow from\n+   (maybe the middle of) E1->SRC to (maybe the middle of) E2->SRC.  */\n+\n+static bool\n+try_crossjump_to_edge (mode, e1, e2)\n+     int mode;\n+     edge e1, e2;\n+{\n+  int nmatch;\n+  basic_block src1 = e1->src, src2 = e2->src;\n+  basic_block redirect_to;\n+  rtx newpos1, newpos2;\n+  edge s;\n+  rtx last;\n+  rtx label;\n+  rtx note;\n+\n+  /* Search backward through forwarder blocks.  We don't need to worry\n+     about multiple entry or chained forwarders, as they will be optimized\n+     away.  We do this to look past the unconditional jump following a\n+     conditional jump that is required due to the current CFG shape.  */\n+  if (src1->pred\n+      && !src1->pred->pred_next\n+      && forwarder_block_p (src1))\n+    {\n+      e1 = src1->pred;\n+      src1 = e1->src;\n+    }\n+  if (src2->pred\n+      && !src2->pred->pred_next\n+      && forwarder_block_p (src2))\n+    {\n+      e2 = src2->pred;\n+      src2 = e2->src;\n+    }\n+\n+  /* Nothing to do if we reach ENTRY, or a common source block.  */\n+  if (src1 == ENTRY_BLOCK_PTR || src2 == ENTRY_BLOCK_PTR)\n+    return false;\n+  if (src1 == src2)\n+    return false;\n+\n+  /* Seeing more than 1 forwarder blocks would confuse us later...  */\n+  if (forwarder_block_p (e1->dest)\n+      && forwarder_block_p (e1->dest->succ->dest))\n+    return false;\n+  if (forwarder_block_p (e2->dest)\n+      && forwarder_block_p (e2->dest->succ->dest))\n+    return false;\n+\n+  /* Likewise with dead code (possibly newly created by the other optimizations\n+     of cfg_cleanup).  */\n+  if (!src1->pred || !src2->pred)\n+    return false;\n+\n+  /* Likewise with complex edges.\n+     ??? We should be able to handle most complex edges later with some\n+     care.  */\n+  if (e1->flags & EDGE_COMPLEX)\n+    return false;\n+\n+  /* Look for the common insn sequence, part the first ...  */\n+  if (!outgoing_edges_match (src1, src2))\n+    return false;\n+\n+  /* ... and part the second.  */\n+  nmatch = flow_find_cross_jump (mode, src1, src2, &newpos1, &newpos2);\n+  if (!nmatch)\n+    return false;\n+\n+  /* Avoid splitting if possible.  */\n+  if (newpos2 == src2->head)\n+    redirect_to = src2;\n+  else\n+    {\n+      if (rtl_dump_file)\n+\tfprintf (rtl_dump_file, \"Splitting bb %i before %i insns\\n\",\n+\t\t src2->index, nmatch);\n+      redirect_to = split_block (src2, PREV_INSN (newpos2))->dest;\n+    }\n+\n+  if (rtl_dump_file)\n+    fprintf (rtl_dump_file,\n+\t     \"Cross jumping from bb %i to bb %i; %i common insns\\n\",\n+\t     src1->index, src2->index, nmatch);\n+\n+  redirect_to->count += src1->count;\n+  redirect_to->frequency += src1->frequency;\n+\n+  /* Recompute the frequencies and counts of outgoing edges.  */\n+  for (s = redirect_to->succ; s; s = s->succ_next)\n+    {\n+      edge s2;\n+      basic_block d = s->dest;\n+\n+      if (forwarder_block_p (d))\n+\td = d->succ->dest;\n+      for (s2 = src1->succ; ; s2 = s2->succ_next)\n+\t{\n+\t  basic_block d2 = s2->dest;\n+\t  if (forwarder_block_p (d2))\n+\t    d2 = d2->succ->dest;\n+\t  if (d == d2)\n+\t    break;\n+\t}\n+      s->count += s2->count;\n+\n+      /* Take care to update possible forwarder blocks.  We verified\n+         that there is no more than one in the chain, so we can't run\n+         into infinite loop.  */\n+      if (forwarder_block_p (s->dest))\n+\t{\n+\t  s->dest->succ->count += s2->count;\n+\t  s->dest->count += s2->count;\n+\t  s->dest->frequency += EDGE_FREQUENCY (s);\n+\t}\n+      if (forwarder_block_p (s2->dest))\n+\t{\n+\t  s2->dest->succ->count -= s2->count;\n+\t  s2->dest->count -= s2->count;\n+\t  s2->dest->frequency -= EDGE_FREQUENCY (s);\n+\t}\n+      if (!redirect_to->frequency && !src1->frequency)\n+\ts->probability = (s->probability + s2->probability) / 2;\n+      else\n+\ts->probability =\n+\t  ((s->probability * redirect_to->frequency +\n+\t    s2->probability * src1->frequency)\n+\t   / (redirect_to->frequency + src1->frequency));\n+    }\n+\n+  note = find_reg_note (redirect_to->end, REG_BR_PROB, 0);\n+  if (note)\n+    XEXP (note, 0) = GEN_INT (BRANCH_EDGE (redirect_to)->probability);\n+\n+  /* Edit SRC1 to go to REDIRECT_TO at NEWPOS1.  */\n+\n+  /* Skip possible basic block header.  */\n+  if (GET_CODE (newpos1) == CODE_LABEL)\n+    newpos1 = NEXT_INSN (newpos1);\n+  if (GET_CODE (newpos1) == NOTE)\n+    newpos1 = NEXT_INSN (newpos1);\n+  last = src1->end;\n+\n+  /* Emit the jump insn.   */\n+  label = block_label (redirect_to);\n+  src1->end = emit_jump_insn_before (gen_jump (label), newpos1);\n+  JUMP_LABEL (src1->end) = label;\n+  LABEL_NUSES (label)++;\n+  if (basic_block_for_insn)\n+    set_block_for_new_insns (src1->end, src1);\n+\n+  /* Delete the now unreachable instructions.  */\n+  flow_delete_insn_chain (newpos1, last);\n+\n+  /* Make sure there is a barrier after the new jump.  */\n+  last = next_nonnote_insn (src1->end);\n+  if (!last || GET_CODE (last) != BARRIER)\n+    emit_barrier_after (src1->end);\n+\n+  /* Update CFG.  */\n+  while (src1->succ)\n+    remove_edge (src1->succ);\n+  make_edge (NULL, src1, redirect_to, 0);\n+  src1->succ->probability = REG_BR_PROB_BASE;\n+  src1->succ->count = src1->count;\n+\n+  return true;\n+}\n+\n+/* Search the predecessors of BB for common insn sequences.  When found,\n+   share code between them by redirecting control flow.  Return true if\n+   any changes made.  */\n+\n+static bool\n+try_crossjump_bb (mode, bb)\n+     int mode;\n+     basic_block bb;\n+{\n+  edge e, e2, nexte2, nexte, fallthru;\n+  bool changed;\n+\n+  /* Nothing to do if there is not at least two incomming edges.  */\n+  if (!bb->pred || !bb->pred->pred_next)\n+    return false;\n+\n+  /* It is always cheapest to redirect a block that ends in a branch to\n+     a block that falls through into BB, as that adds no branches to the\n+     program.  We'll try that combination first.  */\n+  for (fallthru = bb->pred; fallthru; fallthru = fallthru->pred_next)\n+    if (fallthru->flags & EDGE_FALLTHRU)\n+      break;\n+\n+  changed = false;\n+  for (e = bb->pred; e; e = nexte)\n+    {\n+      nexte = e->pred_next;\n+\n+      /* Elide complex edges now, as neither try_crossjump_to_edge\n+\t nor outgoing_edges_match can handle them.  */\n+      if (e->flags & EDGE_COMPLEX)\n+\tcontinue;\n+\n+      /* As noted above, first try with the fallthru predecessor.  */\n+      if (fallthru)\n+\t{\n+\t  /* Don't combine the fallthru edge into anything else.\n+\t     If there is a match, we'll do it the other way around.  */\n+\t  if (e == fallthru)\n+\t    continue;\n+\n+\t  if (try_crossjump_to_edge (mode, e, fallthru))\n+\t    {\n+\t      changed = true;\n+\t      nexte = bb->pred;\n+\t      continue;\n+\t    }\n+\t}\n+\n+      /* Non-obvious work limiting check: Recognize that we're going\n+\t to call try_crossjump_bb on every basic block.  So if we have\n+\t two blocks with lots of outgoing edges (a switch) and they\n+\t share lots of common destinations, then we would do the\n+\t cross-jump check once for each common destination.\n+\n+\t Now, if the blocks actually are cross-jump candidates, then\n+\t all of their destinations will be shared.  Which means that\n+\t we only need check them for cross-jump candidacy once.  We\n+\t can eliminate redundant checks of crossjump(A,B) by arbitrarily\n+\t choosing to do the check from the block for which the edge\n+\t in question is the first successor of A.  */\n+      if (e->src->succ != e)\n+\tcontinue;\n+\n+      for (e2 = bb->pred; e2; e2 = nexte2)\n+\t{\n+\t  nexte2 = e2->pred_next;\n+\n+\t  if (e2 == e)\n+\t    continue;\n+\n+\t  /* We've already checked the fallthru edge above.  */\n+\t  if (e2 == fallthru)\n+\t    continue;\n+\n+\t  /* Again, neither try_crossjump_to_edge nor outgoing_edges_match\n+\t     can handle complex edges.  */\n+\t  if (e2->flags & EDGE_COMPLEX)\n+\t    continue;\n+\n+\t  /* The \"first successor\" check above only prevents multiple\n+\t     checks of crossjump(A,B).  In order to prevent redundant\n+\t     checks of crossjump(B,A), require that A be the block\n+\t     with the lowest index.  */\n+\t  if (e->src->index > e2->src->index)\n+\t    continue;\n+\n+\t  if (try_crossjump_to_edge (mode, e, e2))\n+\t    {\n+\t      changed = true;\n+\t      nexte = bb->pred;\n+\t      break;\n+\t    }\n+\t}\n+    }\n+\n+  return changed;\n+}\n+\n+/* Do simple CFG optimizations - basic block merging, simplifying of jump\n+   instructions etc.  Return nonzero if changes were made.  */\n+\n+static bool\n+try_optimize_cfg (mode)\n+     int mode;\n+{\n+  int i;\n+  bool changed_overall = false;\n+  bool changed;\n+  int iterations = 0;\n+\n+  /* Attempt to merge blocks as made possible by edge removal.  If a block\n+     has only one successor, and the successor has only one predecessor,\n+     they may be combined.  */\n+\n+  do\n+    {\n+      changed = false;\n+      iterations++;\n+\n+      if (rtl_dump_file)\n+\tfprintf (rtl_dump_file, \"\\n\\ntry_optimize_cfg iteration %i\\n\\n\",\n+\t\t iterations);\n+\n+      for (i = 0; i < n_basic_blocks;)\n+\t{\n+\t  basic_block c, b = BASIC_BLOCK (i);\n+\t  edge s;\n+\t  bool changed_here = false;\n+\n+\t  /* Delete trivially dead basic blocks.  */\n+\t  while (b->pred == NULL)\n+\t    {\n+\t      c = BASIC_BLOCK (b->index - 1);\n+\t      if (rtl_dump_file)\n+\t\tfprintf (rtl_dump_file, \"Deleting block %i.\\n\", b->index);\n+\t      flow_delete_block (b);\n+\t      changed = true;\n+\t      b = c;\n+\t    }\n+\n+\t  /* Remove code labels no longer used.  Don't do this before\n+\t     CALL_PLACEHOLDER is removed, as some branches may be hidden\n+\t     within.  */\n+\t  if (b->pred->pred_next == NULL\n+\t      && (b->pred->flags & EDGE_FALLTHRU)\n+\t      && !(b->pred->flags & EDGE_COMPLEX)\n+\t      && GET_CODE (b->head) == CODE_LABEL\n+\t      && (!(mode & CLEANUP_PRE_SIBCALL)\n+\t\t  || !tail_recursion_label_p (b->head))\n+\t      /* If previous block ends with condjump jumping to next BB,\n+\t         we can't delete the label.  */\n+\t      && (b->pred->src == ENTRY_BLOCK_PTR\n+\t\t  || !reg_mentioned_p (b->head, b->pred->src->end)))\n+\t    {\n+\t      rtx label = b->head;\n+\t      b->head = NEXT_INSN (b->head);\n+\t      flow_delete_insn_chain (label, label);\n+\t      if (rtl_dump_file)\n+\t\tfprintf (rtl_dump_file, \"Deleted label in block %i.\\n\",\n+\t\t\t b->index);\n+\t    }\n+\n+\t  /* If we fall through an empty block, we can remove it.  */\n+\t  if (b->pred->pred_next == NULL\n+\t      && (b->pred->flags & EDGE_FALLTHRU)\n+\t      && GET_CODE (b->head) != CODE_LABEL\n+\t      && forwarder_block_p (b)\n+\t      /* Note that forwarder_block_p true ensures that there\n+\t\t is a successor for this block.  */\n+\t      && (b->succ->flags & EDGE_FALLTHRU)\n+\t      && n_basic_blocks > 1)\n+\t    {\n+\t      if (rtl_dump_file)\n+\t\tfprintf (rtl_dump_file, \"Deleting fallthru block %i.\\n\",\n+\t\t\t b->index);\n+\t      c = BASIC_BLOCK (b->index ? b->index - 1 : 1);\n+\t      redirect_edge_succ_nodup (b->pred, b->succ->dest);\n+\t      flow_delete_block (b);\n+\t      changed = true;\n+\t      b = c;\n+\t    }\n+\n+\t  /* Merge blocks.  Loop because chains of blocks might be\n+\t     combineable.  */\n+\t  while ((s = b->succ) != NULL\n+\t\t && s->succ_next == NULL\n+\t         && !(s->flags & EDGE_COMPLEX)\n+\t\t && (c = s->dest) != EXIT_BLOCK_PTR\n+\t\t && c->pred->pred_next == NULL\n+\t\t /* If the jump insn has side effects,\n+\t\t    we can't kill the edge.  */\n+\t\t && (GET_CODE (b->end) != JUMP_INSN\n+\t\t     || onlyjump_p (b->end))\n+\t\t && merge_blocks (s, b, c, mode))\n+\t    changed_here = true;\n+\n+\t  /* Simplify branch over branch.  */\n+\t  if ((mode & CLEANUP_EXPENSIVE) && try_simplify_condjump (b))\n+\t    changed_here = true;\n+\n+\t  /* If B has a single outgoing edge, but uses a non-trivial jump\n+\t     instruction without side-effects, we can either delete the\n+\t     jump entirely, or replace it with a simple unconditional jump.\n+\t     Use redirect_edge_and_branch to do the dirty work.  */\n+\t  if (b->succ\n+\t      && ! b->succ->succ_next\n+\t      && b->succ->dest != EXIT_BLOCK_PTR\n+\t      && onlyjump_p (b->end)\n+\t      && redirect_edge_and_branch (b->succ, b->succ->dest))\n+\t    changed_here = true;\n+\n+\t  /* Simplify branch to branch.  */\n+\t  if (try_forward_edges (mode, b))\n+\t    changed_here = true;\n+\n+\t  /* Look for shared code between blocks.  */\n+\t  if ((mode & CLEANUP_CROSSJUMP)\n+\t      && try_crossjump_bb (mode, b))\n+\t    changed_here = true;\n+\n+\t  /* Don't get confused by the index shift caused by deleting\n+\t     blocks.  */\n+\t  if (!changed_here)\n+\t    i = b->index + 1;\n+\t  else\n+\t    changed = true;\n+\t}\n+\n+      if ((mode & CLEANUP_CROSSJUMP)\n+\t  && try_crossjump_bb (mode, EXIT_BLOCK_PTR))\n+\tchanged = true;\n+\n+#ifdef ENABLE_CHECKING\n+      if (changed)\n+\tverify_flow_info ();\n+#endif\n+\n+      changed_overall |= changed;\n+    }\n+  while (changed);\n+  return changed_overall;\n+}\n+\f\n+/* Delete all unreachable basic blocks.   */\n+static bool\n+delete_unreachable_blocks ()\n+{\n+  int i;\n+  bool changed = false;\n+\n+  find_unreachable_blocks ();\n+\n+  /* Delete all unreachable basic blocks.  Count down so that we\n+     don't interfere with the block renumbering that happens in\n+     flow_delete_block.  */\n+\n+  for (i = n_basic_blocks - 1; i >= 0; --i)\n+    {\n+      basic_block b = BASIC_BLOCK (i);\n+\n+      if (!(b->flags & BB_REACHABLE))\n+\tflow_delete_block (b), changed = true;\n+    }\n+\n+  if (changed)\n+    tidy_fallthru_edges ();\n+  return changed;\n+}\n+\n+\f\n+/* Tidy the CFG by deleting unreachable code and whatnot.  */\n+\n+bool\n+cleanup_cfg (mode)\n+     int mode;\n+{\n+  int i;\n+  bool changed = false;\n+\n+  timevar_push (TV_CLEANUP_CFG);\n+  changed = delete_unreachable_blocks ();\n+  if (try_optimize_cfg (mode))\n+    delete_unreachable_blocks (), changed = true;\n+\n+  if (changed)\n+    mark_critical_edges ();\n+\n+  /* Kill the data we won't maintain.  */\n+  free_EXPR_LIST_list (&label_value_list);\n+  free_EXPR_LIST_list (&tail_recursion_label_list);\n+  timevar_pop (TV_CLEANUP_CFG);\n+\n+  /* Clear bb->aux on all basic blocks.  */\n+  for (i = 0; i < n_basic_blocks; ++i)\n+    BASIC_BLOCK (i)->aux = NULL;\n+  return changed;\n+}"}, {"sha": "d8b5b4d46fb1f3d70c0f616eced91c6a6c36a01c", "filename": "gcc/cfgloop.c", "status": "added", "additions": 854, "deletions": 0, "changes": 854, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fcfgloop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fcfgloop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfgloop.c?ref=402209ff48d3e1984111c536033aa638f4271531", "patch": "@@ -0,0 +1,854 @@\n+/* Natural loop discovery code for GNU compiler.\n+   Copyright (C) 2000, 2001 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 2, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"rtl.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+\n+static void flow_loops_cfg_dump\t\tPARAMS ((const struct loops *,\n+\t\t\t\t\t\t FILE *));\n+static int flow_loop_nested_p\t\tPARAMS ((struct loop *,\n+\t\t\t\t\t\t struct loop *));\n+static int flow_loop_entry_edges_find\tPARAMS ((basic_block, const sbitmap,\n+\t\t\t\t\t\t edge **));\n+static int flow_loop_exit_edges_find\tPARAMS ((const sbitmap, edge **));\n+static int flow_loop_nodes_find\tPARAMS ((basic_block, basic_block, sbitmap));\n+static void flow_loop_pre_header_scan PARAMS ((struct loop *));\n+static basic_block flow_loop_pre_header_find PARAMS ((basic_block,\n+\t\t\t\t\t\t      const sbitmap *));\n+static void flow_loop_tree_node_add\tPARAMS ((struct loop *, struct loop *));\n+static void flow_loops_tree_build\tPARAMS ((struct loops *));\n+static int flow_loop_level_compute\tPARAMS ((struct loop *, int));\n+static int flow_loops_level_compute\tPARAMS ((struct loops *));\n+\f\n+/* Dump loop related CFG information.  */\n+\n+static void\n+flow_loops_cfg_dump (loops, file)\n+     const struct loops *loops;\n+     FILE *file;\n+{\n+  int i;\n+\n+  if (! loops->num || ! file || ! loops->cfg.dom)\n+    return;\n+\n+  for (i = 0; i < n_basic_blocks; i++)\n+    {\n+      edge succ;\n+\n+      fprintf (file, \";; %d succs { \", i);\n+      for (succ = BASIC_BLOCK (i)->succ; succ; succ = succ->succ_next)\n+\tfprintf (file, \"%d \", succ->dest->index);\n+      flow_nodes_print (\"} dom\", loops->cfg.dom[i], file);\n+    }\n+\n+  /* Dump the DFS node order.  */\n+  if (loops->cfg.dfs_order)\n+    {\n+      fputs (\";; DFS order: \", file);\n+      for (i = 0; i < n_basic_blocks; i++)\n+\tfprintf (file, \"%d \", loops->cfg.dfs_order[i]);\n+      fputs (\"\\n\", file);\n+    }\n+  /* Dump the reverse completion node order.  */\n+  if (loops->cfg.rc_order)\n+    {\n+      fputs (\";; RC order: \", file);\n+      for (i = 0; i < n_basic_blocks; i++)\n+\tfprintf (file, \"%d \", loops->cfg.rc_order[i]);\n+      fputs (\"\\n\", file);\n+    }\n+}\n+\n+/* Return non-zero if the nodes of LOOP are a subset of OUTER.  */\n+\n+static int\n+flow_loop_nested_p (outer, loop)\n+     struct loop *outer;\n+     struct loop *loop;\n+{\n+  return sbitmap_a_subset_b_p (loop->nodes, outer->nodes);\n+}\n+\n+/* Dump the loop information specified by LOOP to the stream FILE\n+   using auxiliary dump callback function LOOP_DUMP_AUX if non null.  */\n+\n+void\n+flow_loop_dump (loop, file, loop_dump_aux, verbose)\n+     const struct loop *loop;\n+     FILE *file;\n+     void (*loop_dump_aux) PARAMS((const struct loop *, FILE *, int));\n+     int verbose;\n+{\n+  if (! loop || ! loop->header)\n+    return;\n+\n+  if (loop->first->head && loop->last->end)\n+    fprintf (file, \";;\\n;; Loop %d (%d to %d):%s%s\\n\",\n+\t    loop->num, INSN_UID (loop->first->head),\n+\t    INSN_UID (loop->last->end),\n+\t    loop->shared ? \" shared\" : \"\",\n+\t    loop->invalid ? \" invalid\" : \"\");\n+  else\n+    fprintf (file, \";;\\n;; Loop %d:%s%s\\n\", loop->num,\n+\t     loop->shared ? \" shared\" : \"\",\n+\t     loop->invalid ? \" invalid\" : \"\");\n+\n+  fprintf (file, \";;  header %d, latch %d, pre-header %d, first %d, last %d\\n\",\n+\t   loop->header->index, loop->latch->index,\n+\t   loop->pre_header ? loop->pre_header->index : -1,\n+\t   loop->first->index, loop->last->index);\n+  fprintf (file, \";;  depth %d, level %d, outer %ld\\n\",\n+\t   loop->depth, loop->level,\n+\t   (long) (loop->outer ? loop->outer->num : -1));\n+\n+  if (loop->pre_header_edges)\n+    flow_edge_list_print (\";;  pre-header edges\", loop->pre_header_edges,\n+\t\t\t  loop->num_pre_header_edges, file);\n+  flow_edge_list_print (\";;  entry edges\", loop->entry_edges,\n+\t\t\tloop->num_entries, file);\n+  fprintf (file, \";;  %d\", loop->num_nodes);\n+  flow_nodes_print (\" nodes\", loop->nodes, file);\n+  flow_edge_list_print (\";;  exit edges\", loop->exit_edges,\n+\t\t\tloop->num_exits, file);\n+  if (loop->exits_doms)\n+    flow_nodes_print (\";;  exit doms\", loop->exits_doms, file);\n+  if (loop_dump_aux)\n+    loop_dump_aux (loop, file, verbose);\n+}\n+\n+/* Dump the loop information specified by LOOPS to the stream FILE,\n+   using auxiliary dump callback function LOOP_DUMP_AUX if non null.  */\n+\n+void\n+flow_loops_dump (loops, file, loop_dump_aux, verbose)\n+     const struct loops *loops;\n+     FILE *file;\n+     void (*loop_dump_aux) PARAMS((const struct loop *, FILE *, int));\n+     int verbose;\n+{\n+  int i;\n+  int num_loops;\n+\n+  num_loops = loops->num;\n+  if (! num_loops || ! file)\n+    return;\n+\n+  fprintf (file, \";; %d loops found, %d levels\\n\",\n+\t   num_loops, loops->levels);\n+\n+  for (i = 0; i < num_loops; i++)\n+    {\n+      struct loop *loop = &loops->array[i];\n+\n+      flow_loop_dump (loop, file, loop_dump_aux, verbose);\n+\n+      if (loop->shared)\n+\t{\n+\t  int j;\n+\n+\t  for (j = 0; j < i; j++)\n+\t    {\n+\t      struct loop *oloop = &loops->array[j];\n+\n+\t      if (loop->header == oloop->header)\n+\t\t{\n+\t\t  int disjoint;\n+\t\t  int smaller;\n+\n+\t\t  smaller = loop->num_nodes < oloop->num_nodes;\n+\n+\t\t  /* If the union of LOOP and OLOOP is different than\n+\t\t     the larger of LOOP and OLOOP then LOOP and OLOOP\n+\t\t     must be disjoint.  */\n+\t\t  disjoint = ! flow_loop_nested_p (smaller ? loop : oloop,\n+\t\t\t\t\t\t   smaller ? oloop : loop);\n+\t\t  fprintf (file,\n+\t\t\t   \";; loop header %d shared by loops %d, %d %s\\n\",\n+\t\t\t   loop->header->index, i, j,\n+\t\t\t   disjoint ? \"disjoint\" : \"nested\");\n+\t\t}\n+\t    }\n+\t}\n+    }\n+\n+  if (verbose)\n+    flow_loops_cfg_dump (loops, file);\n+}\n+\n+/* Free all the memory allocated for LOOPS.  */\n+\n+void\n+flow_loops_free (loops)\n+     struct loops *loops;\n+{\n+  if (loops->array)\n+    {\n+      int i;\n+\n+      if (! loops->num)\n+\tabort ();\n+\n+      /* Free the loop descriptors.  */\n+      for (i = 0; i < loops->num; i++)\n+\t{\n+\t  struct loop *loop = &loops->array[i];\n+\n+\t  if (loop->pre_header_edges)\n+\t    free (loop->pre_header_edges);\n+\t  if (loop->nodes)\n+\t    sbitmap_free (loop->nodes);\n+\t  if (loop->entry_edges)\n+\t    free (loop->entry_edges);\n+\t  if (loop->exit_edges)\n+\t    free (loop->exit_edges);\n+\t  if (loop->exits_doms)\n+\t    sbitmap_free (loop->exits_doms);\n+\t}\n+      free (loops->array);\n+      loops->array = NULL;\n+\n+      if (loops->cfg.dom)\n+\tsbitmap_vector_free (loops->cfg.dom);\n+      if (loops->cfg.dfs_order)\n+\tfree (loops->cfg.dfs_order);\n+\n+      if (loops->shared_headers)\n+\tsbitmap_free (loops->shared_headers);\n+    }\n+}\n+\n+/* Find the entry edges into the loop with header HEADER and nodes\n+   NODES and store in ENTRY_EDGES array.  Return the number of entry\n+   edges from the loop.  */\n+\n+static int\n+flow_loop_entry_edges_find (header, nodes, entry_edges)\n+     basic_block header;\n+     const sbitmap nodes;\n+     edge **entry_edges;\n+{\n+  edge e;\n+  int num_entries;\n+\n+  *entry_edges = NULL;\n+\n+  num_entries = 0;\n+  for (e = header->pred; e; e = e->pred_next)\n+    {\n+      basic_block src = e->src;\n+\n+      if (src == ENTRY_BLOCK_PTR || ! TEST_BIT (nodes, src->index))\n+\tnum_entries++;\n+    }\n+\n+  if (! num_entries)\n+    abort ();\n+\n+  *entry_edges = (edge *) xmalloc (num_entries * sizeof (edge *));\n+\n+  num_entries = 0;\n+  for (e = header->pred; e; e = e->pred_next)\n+    {\n+      basic_block src = e->src;\n+\n+      if (src == ENTRY_BLOCK_PTR || ! TEST_BIT (nodes, src->index))\n+\t(*entry_edges)[num_entries++] = e;\n+    }\n+\n+  return num_entries;\n+}\n+\n+/* Find the exit edges from the loop using the bitmap of loop nodes\n+   NODES and store in EXIT_EDGES array.  Return the number of\n+   exit edges from the loop.  */\n+\n+static int\n+flow_loop_exit_edges_find (nodes, exit_edges)\n+     const sbitmap nodes;\n+     edge **exit_edges;\n+{\n+  edge e;\n+  int node;\n+  int num_exits;\n+\n+  *exit_edges = NULL;\n+\n+  /* Check all nodes within the loop to see if there are any\n+     successors not in the loop.  Note that a node may have multiple\n+     exiting edges ?????  A node can have one jumping edge and one fallthru\n+     edge so only one of these can exit the loop.  */\n+  num_exits = 0;\n+  EXECUTE_IF_SET_IN_SBITMAP (nodes, 0, node, {\n+    for (e = BASIC_BLOCK (node)->succ; e; e = e->succ_next)\n+      {\n+\tbasic_block dest = e->dest;\n+\n+\tif (dest == EXIT_BLOCK_PTR || ! TEST_BIT (nodes, dest->index))\n+\t    num_exits++;\n+      }\n+  });\n+\n+  if (! num_exits)\n+    return 0;\n+\n+  *exit_edges = (edge *) xmalloc (num_exits * sizeof (edge *));\n+\n+  /* Store all exiting edges into an array.  */\n+  num_exits = 0;\n+  EXECUTE_IF_SET_IN_SBITMAP (nodes, 0, node, {\n+    for (e = BASIC_BLOCK (node)->succ; e; e = e->succ_next)\n+      {\n+\tbasic_block dest = e->dest;\n+\n+\tif (dest == EXIT_BLOCK_PTR || ! TEST_BIT (nodes, dest->index))\n+\t  (*exit_edges)[num_exits++] = e;\n+      }\n+  });\n+\n+  return num_exits;\n+}\n+\n+/* Find the nodes contained within the loop with header HEADER and\n+   latch LATCH and store in NODES.  Return the number of nodes within\n+   the loop.  */\n+\n+static int\n+flow_loop_nodes_find (header, latch, nodes)\n+     basic_block header;\n+     basic_block latch;\n+     sbitmap nodes;\n+{\n+  basic_block *stack;\n+  int sp;\n+  int num_nodes = 0;\n+\n+  stack = (basic_block *) xmalloc (n_basic_blocks * sizeof (basic_block));\n+  sp = 0;\n+\n+  /* Start with only the loop header in the set of loop nodes.  */\n+  sbitmap_zero (nodes);\n+  SET_BIT (nodes, header->index);\n+  num_nodes++;\n+  header->loop_depth++;\n+\n+  /* Push the loop latch on to the stack.  */\n+  if (! TEST_BIT (nodes, latch->index))\n+    {\n+      SET_BIT (nodes, latch->index);\n+      latch->loop_depth++;\n+      num_nodes++;\n+      stack[sp++] = latch;\n+    }\n+\n+  while (sp)\n+    {\n+      basic_block node;\n+      edge e;\n+\n+      node = stack[--sp];\n+      for (e = node->pred; e; e = e->pred_next)\n+\t{\n+\t  basic_block ancestor = e->src;\n+\n+\t  /* If each ancestor not marked as part of loop, add to set of\n+\t     loop nodes and push on to stack.  */\n+\t  if (ancestor != ENTRY_BLOCK_PTR\n+\t      && ! TEST_BIT (nodes, ancestor->index))\n+\t    {\n+\t      SET_BIT (nodes, ancestor->index);\n+\t      ancestor->loop_depth++;\n+\t      num_nodes++;\n+\t      stack[sp++] = ancestor;\n+\t    }\n+\t}\n+    }\n+  free (stack);\n+  return num_nodes;\n+}\n+\n+/* Find the root node of the loop pre-header extended basic block and\n+   the edges along the trace from the root node to the loop header.  */\n+\n+static void\n+flow_loop_pre_header_scan (loop)\n+     struct loop *loop;\n+{\n+  int num = 0;\n+  basic_block ebb;\n+\n+  loop->num_pre_header_edges = 0;\n+\n+  if (loop->num_entries != 1)\n+     return;\n+\n+  ebb = loop->entry_edges[0]->src;\n+\n+  if (ebb != ENTRY_BLOCK_PTR)\n+    {\n+      edge e;\n+\n+      /* Count number of edges along trace from loop header to\n+\t root of pre-header extended basic block.  Usually this is\n+\t only one or two edges.  */\n+      num++;\n+      while (ebb->pred->src != ENTRY_BLOCK_PTR && ! ebb->pred->pred_next)\n+\t{\n+\t  ebb = ebb->pred->src;\n+\t  num++;\n+\t}\n+\n+      loop->pre_header_edges = (edge *) xmalloc (num * sizeof (edge *));\n+      loop->num_pre_header_edges = num;\n+\n+      /* Store edges in order that they are followed.   The source\n+\t of the first edge is the root node of the pre-header extended\n+\t basic block and the destination of the last last edge is\n+\t the loop header.  */\n+      for (e = loop->entry_edges[0]; num; e = e->src->pred)\n+\t{\n+\t  loop->pre_header_edges[--num] = e;\n+\t}\n+    }\n+}\n+\n+/* Return the block for the pre-header of the loop with header\n+   HEADER where DOM specifies the dominator information.  Return NULL if\n+   there is no pre-header.  */\n+\n+static basic_block\n+flow_loop_pre_header_find (header, dom)\n+     basic_block header;\n+     const sbitmap *dom;\n+{\n+  basic_block pre_header;\n+  edge e;\n+\n+  /* If block p is a predecessor of the header and is the only block\n+     that the header does not dominate, then it is the pre-header.  */\n+  pre_header = NULL;\n+  for (e = header->pred; e; e = e->pred_next)\n+    {\n+      basic_block node = e->src;\n+\n+      if (node != ENTRY_BLOCK_PTR\n+\t  && ! TEST_BIT (dom[node->index], header->index))\n+\t{\n+\t  if (pre_header == NULL)\n+\t    pre_header = node;\n+\t  else\n+\t    {\n+\t      /* There are multiple edges into the header from outside\n+\t\t the loop so there is no pre-header block.  */\n+\t      pre_header = NULL;\n+\t      break;\n+\t    }\n+\t}\n+    }\n+  return pre_header;\n+}\n+\n+/* Add LOOP to the loop hierarchy tree where PREVLOOP was the loop\n+   previously added.  The insertion algorithm assumes that the loops\n+   are added in the order found by a depth first search of the CFG.  */\n+\n+static void\n+flow_loop_tree_node_add (prevloop, loop)\n+     struct loop *prevloop;\n+     struct loop *loop;\n+{\n+\n+  if (flow_loop_nested_p (prevloop, loop))\n+    {\n+      prevloop->inner = loop;\n+      loop->outer = prevloop;\n+      return;\n+    }\n+\n+  while (prevloop->outer)\n+    {\n+      if (flow_loop_nested_p (prevloop->outer, loop))\n+\t{\n+\t  prevloop->next = loop;\n+\t  loop->outer = prevloop->outer;\n+\t  return;\n+\t}\n+      prevloop = prevloop->outer;\n+    }\n+\n+  prevloop->next = loop;\n+  loop->outer = NULL;\n+}\n+\n+/* Build the loop hierarchy tree for LOOPS.  */\n+\n+static void\n+flow_loops_tree_build (loops)\n+     struct loops *loops;\n+{\n+  int i;\n+  int num_loops;\n+\n+  num_loops = loops->num;\n+  if (! num_loops)\n+    return;\n+\n+  /* Root the loop hierarchy tree with the first loop found.\n+     Since we used a depth first search this should be the\n+     outermost loop.  */\n+  loops->tree_root = &loops->array[0];\n+  loops->tree_root->outer = loops->tree_root->inner = loops->tree_root->next = NULL;\n+\n+  /* Add the remaining loops to the tree.  */\n+  for (i = 1; i < num_loops; i++)\n+    flow_loop_tree_node_add (&loops->array[i - 1], &loops->array[i]);\n+}\n+\n+/* Helper function to compute loop nesting depth and enclosed loop level\n+   for the natural loop specified by LOOP at the loop depth DEPTH.\n+   Returns the loop level.  */\n+\n+static int\n+flow_loop_level_compute (loop, depth)\n+     struct loop *loop;\n+     int depth;\n+{\n+  struct loop *inner;\n+  int level = 1;\n+\n+  if (! loop)\n+    return 0;\n+\n+  /* Traverse loop tree assigning depth and computing level as the\n+     maximum level of all the inner loops of this loop.  The loop\n+     level is equivalent to the height of the loop in the loop tree\n+     and corresponds to the number of enclosed loop levels (including\n+     itself).  */\n+  for (inner = loop->inner; inner; inner = inner->next)\n+    {\n+      int ilevel;\n+\n+      ilevel = flow_loop_level_compute (inner, depth + 1) + 1;\n+\n+      if (ilevel > level)\n+\tlevel = ilevel;\n+    }\n+  loop->level = level;\n+  loop->depth = depth;\n+  return level;\n+}\n+\n+/* Compute the loop nesting depth and enclosed loop level for the loop\n+   hierarchy tree specfied by LOOPS.  Return the maximum enclosed loop\n+   level.  */\n+\n+static int\n+flow_loops_level_compute (loops)\n+     struct loops *loops;\n+{\n+  struct loop *loop;\n+  int level;\n+  int levels = 0;\n+\n+  /* Traverse all the outer level loops.  */\n+  for (loop = loops->tree_root; loop; loop = loop->next)\n+    {\n+      level = flow_loop_level_compute (loop, 1);\n+      if (level > levels)\n+\tlevels = level;\n+    }\n+  return levels;\n+}\n+\n+/* Scan a single natural loop specified by LOOP collecting information\n+   about it specified by FLAGS.  */\n+\n+int\n+flow_loop_scan (loops, loop, flags)\n+     struct loops *loops;\n+     struct loop *loop;\n+     int flags;\n+{\n+  /* Determine prerequisites.  */\n+  if ((flags & LOOP_EXITS_DOMS) && ! loop->exit_edges)\n+    flags |= LOOP_EXIT_EDGES;\n+\n+  if (flags & LOOP_ENTRY_EDGES)\n+    {\n+      /* Find edges which enter the loop header.\n+\t Note that the entry edges should only\n+\t enter the header of a natural loop.  */\n+      loop->num_entries\n+\t= flow_loop_entry_edges_find (loop->header,\n+\t\t\t\t      loop->nodes,\n+\t\t\t\t      &loop->entry_edges);\n+    }\n+\n+  if (flags & LOOP_EXIT_EDGES)\n+    {\n+      /* Find edges which exit the loop.  */\n+      loop->num_exits\n+\t= flow_loop_exit_edges_find (loop->nodes,\n+\t\t\t\t     &loop->exit_edges);\n+    }\n+\n+  if (flags & LOOP_EXITS_DOMS)\n+    {\n+      int j;\n+\n+      /* Determine which loop nodes dominate all the exits\n+\t of the loop.  */\n+      loop->exits_doms = sbitmap_alloc (n_basic_blocks);\n+      sbitmap_copy (loop->exits_doms, loop->nodes);\n+      for (j = 0; j < loop->num_exits; j++)\n+\tsbitmap_a_and_b (loop->exits_doms, loop->exits_doms,\n+\t\t\t loops->cfg.dom[loop->exit_edges[j]->src->index]);\n+\n+      /* The header of a natural loop must dominate\n+\t all exits.  */\n+      if (! TEST_BIT (loop->exits_doms, loop->header->index))\n+\tabort ();\n+    }\n+\n+  if (flags & LOOP_PRE_HEADER)\n+    {\n+      /* Look to see if the loop has a pre-header node.  */\n+      loop->pre_header\n+\t= flow_loop_pre_header_find (loop->header, loops->cfg.dom);\n+\n+      /* Find the blocks within the extended basic block of\n+\t the loop pre-header.  */\n+      flow_loop_pre_header_scan (loop);\n+    }\n+  return 1;\n+}\n+\n+/* Find all the natural loops in the function and save in LOOPS structure\n+   and recalculate loop_depth information in basic block structures.\n+   FLAGS controls which loop information is collected.\n+   Return the number of natural loops found.  */\n+\n+int\n+flow_loops_find (loops, flags)\n+     struct loops *loops;\n+     int flags;\n+{\n+  int i;\n+  int b;\n+  int num_loops;\n+  edge e;\n+  sbitmap headers;\n+  sbitmap *dom;\n+  int *dfs_order;\n+  int *rc_order;\n+\n+  /* This function cannot be repeatedly called with different\n+     flags to build up the loop information.  The loop tree\n+     must always be built if this function is called.  */\n+  if (! (flags & LOOP_TREE))\n+    abort ();\n+\n+  memset (loops, 0, sizeof (*loops));\n+\n+  /* Taking care of this degenerate case makes the rest of\n+     this code simpler.  */\n+  if (n_basic_blocks == 0)\n+    return 0;\n+\n+  dfs_order = NULL;\n+  rc_order = NULL;\n+\n+  /* Compute the dominators.  */\n+  dom = sbitmap_vector_alloc (n_basic_blocks, n_basic_blocks);\n+  calculate_dominance_info (NULL, dom, CDI_DOMINATORS);\n+\n+  /* Count the number of loop edges (back edges).  This should be the\n+     same as the number of natural loops.  */\n+\n+  num_loops = 0;\n+  for (b = 0; b < n_basic_blocks; b++)\n+    {\n+      basic_block header;\n+\n+      header = BASIC_BLOCK (b);\n+      header->loop_depth = 0;\n+\n+      for (e = header->pred; e; e = e->pred_next)\n+\t{\n+\t  basic_block latch = e->src;\n+\n+\t  /* Look for back edges where a predecessor is dominated\n+\t     by this block.  A natural loop has a single entry\n+\t     node (header) that dominates all the nodes in the\n+\t     loop.  It also has single back edge to the header\n+\t     from a latch node.  Note that multiple natural loops\n+\t     may share the same header.  */\n+\t  if (b != header->index)\n+\t    abort ();\n+\n+\t  if (latch != ENTRY_BLOCK_PTR && TEST_BIT (dom[latch->index], b))\n+\t    num_loops++;\n+\t}\n+    }\n+\n+  if (num_loops)\n+    {\n+      /* Compute depth first search order of the CFG so that outer\n+\t natural loops will be found before inner natural loops.  */\n+      dfs_order = (int *) xmalloc (n_basic_blocks * sizeof (int));\n+      rc_order = (int *) xmalloc (n_basic_blocks * sizeof (int));\n+      flow_depth_first_order_compute (dfs_order, rc_order);\n+\n+      /* Save CFG derived information to avoid recomputing it.  */\n+      loops->cfg.dom = dom;\n+      loops->cfg.dfs_order = dfs_order;\n+      loops->cfg.rc_order = rc_order;\n+\n+      /* Allocate loop structures.  */\n+      loops->array\n+\t= (struct loop *) xcalloc (num_loops, sizeof (struct loop));\n+\n+      headers = sbitmap_alloc (n_basic_blocks);\n+      sbitmap_zero (headers);\n+\n+      loops->shared_headers = sbitmap_alloc (n_basic_blocks);\n+      sbitmap_zero (loops->shared_headers);\n+\n+      /* Find and record information about all the natural loops\n+\t in the CFG.  */\n+      num_loops = 0;\n+      for (b = 0; b < n_basic_blocks; b++)\n+\t{\n+\t  basic_block header;\n+\n+\t  /* Search the nodes of the CFG in reverse completion order\n+\t     so that we can find outer loops first.  */\n+\t  header = BASIC_BLOCK (rc_order[b]);\n+\n+\t  /* Look for all the possible latch blocks for this header.  */\n+\t  for (e = header->pred; e; e = e->pred_next)\n+\t    {\n+\t      basic_block latch = e->src;\n+\n+\t      /* Look for back edges where a predecessor is dominated\n+\t\t by this block.  A natural loop has a single entry\n+\t\t node (header) that dominates all the nodes in the\n+\t\t loop.  It also has single back edge to the header\n+\t\t from a latch node.  Note that multiple natural loops\n+\t\t may share the same header.  */\n+\t      if (latch != ENTRY_BLOCK_PTR\n+\t\t  && TEST_BIT (dom[latch->index], header->index))\n+\t\t{\n+\t\t  struct loop *loop;\n+\n+\t\t  loop = loops->array + num_loops;\n+\n+\t\t  loop->header = header;\n+\t\t  loop->latch = latch;\n+\t\t  loop->num = num_loops;\n+\n+\t\t  num_loops++;\n+\t\t}\n+\t    }\n+\t}\n+\n+      for (i = 0; i < num_loops; i++)\n+\t{\n+\t  struct loop *loop = &loops->array[i];\n+\n+\t  /* Keep track of blocks that are loop headers so\n+\t     that we can tell which loops should be merged.  */\n+\t  if (TEST_BIT (headers, loop->header->index))\n+\t    SET_BIT (loops->shared_headers, loop->header->index);\n+\t  SET_BIT (headers, loop->header->index);\n+\n+\t  /* Find nodes contained within the loop.  */\n+\t  loop->nodes = sbitmap_alloc (n_basic_blocks);\n+\t  loop->num_nodes\n+\t    = flow_loop_nodes_find (loop->header, loop->latch, loop->nodes);\n+\n+\t  /* Compute first and last blocks within the loop.\n+\t     These are often the same as the loop header and\n+\t     loop latch respectively, but this is not always\n+\t     the case.  */\n+\t  loop->first\n+\t    = BASIC_BLOCK (sbitmap_first_set_bit (loop->nodes));\n+\t  loop->last\n+\t    = BASIC_BLOCK (sbitmap_last_set_bit (loop->nodes));\n+\n+\t  flow_loop_scan (loops, loop, flags);\n+\t}\n+\n+      /* Natural loops with shared headers may either be disjoint or\n+\t nested.  Disjoint loops with shared headers cannot be inner\n+\t loops and should be merged.  For now just mark loops that share\n+\t headers.  */\n+      for (i = 0; i < num_loops; i++)\n+\tif (TEST_BIT (loops->shared_headers, loops->array[i].header->index))\n+\t  loops->array[i].shared = 1;\n+\n+      sbitmap_free (headers);\n+    }\n+  else\n+    {\n+      sbitmap_vector_free (dom);\n+    }\n+\n+  loops->num = num_loops;\n+\n+  /* Build the loop hierarchy tree.  */\n+  flow_loops_tree_build (loops);\n+\n+  /* Assign the loop nesting depth and enclosed loop level for each\n+     loop.  */\n+  loops->levels = flow_loops_level_compute (loops);\n+\n+  return num_loops;\n+}\n+\n+/* Update the information regarding the loops in the CFG\n+   specified by LOOPS.  */\n+int\n+flow_loops_update (loops, flags)\n+     struct loops *loops;\n+     int flags;\n+{\n+  /* One day we may want to update the current loop data.  For now\n+     throw away the old stuff and rebuild what we need.  */\n+  if (loops->array)\n+    flow_loops_free (loops);\n+\n+  return flow_loops_find (loops, flags);\n+}\n+\n+/* Return non-zero if edge E enters header of LOOP from outside of LOOP.  */\n+\n+int\n+flow_loop_outside_edge_p (loop, e)\n+     const struct loop *loop;\n+     edge e;\n+{\n+  if (e->dest != loop->header)\n+    abort ();\n+  return (e->src == ENTRY_BLOCK_PTR) || ! TEST_BIT (loop->nodes, e->src->index);\n+}"}, {"sha": "b5a6e3775ac1ad81076c9a88dd89979204c8b84f", "filename": "gcc/flow.c", "status": "modified", "additions": 2943, "deletions": 9210, "changes": 12153, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fflow.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2Fflow.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fflow.c?ref=402209ff48d3e1984111c536033aa638f4271531"}, {"sha": "2edfb050794be3faeab49eea6220b4d203492bcd", "filename": "gcc/output.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/402209ff48d3e1984111c536033aa638f4271531/gcc%2Foutput.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/402209ff48d3e1984111c536033aa638f4271531/gcc%2Foutput.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foutput.h?ref=402209ff48d3e1984111c536033aa638f4271531", "patch": "@@ -144,7 +144,7 @@ extern void allocate_for_life_analysis\tPARAMS ((void));\n extern int regno_uninitialized\t\tPARAMS ((int));\n extern int regno_clobbered_at_setjmp\tPARAMS ((int));\n extern void find_basic_blocks\t\tPARAMS ((rtx, int, FILE *));\n-extern void cleanup_cfg\t\t\tPARAMS ((int));\n+extern bool cleanup_cfg\t\t\tPARAMS ((int));\n extern void check_function_return_warnings PARAMS ((void));\n #endif\n "}]}