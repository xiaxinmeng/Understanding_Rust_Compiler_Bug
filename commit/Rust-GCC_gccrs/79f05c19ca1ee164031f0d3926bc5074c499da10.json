{"sha": "79f05c19ca1ee164031f0d3926bc5074c499da10", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NzlmMDVjMTljYTFlZTE2NDAzMWYwZDM5MjZiYzUwNzRjNDk5ZGExMA==", "commit": {"author": {"name": "Jan Hubicka", "email": "jh@suse.cz", "date": "2000-02-03T14:10:02Z"}, "committer": {"name": "Jan Hubicka", "email": "hubicka@gcc.gnu.org", "date": "2000-02-03T14:10:02Z"}, "message": "i386.md (movstrsi, clrstrsi): Support variable sized copies, align destination when needed.\n\n\t* i386.md (movstrsi, clrstrsi): Support variable sized copies, align\n\tdestination when needed.\n\t(strmovsi, strsetsi): New expander.\n\t(strmovsi_1, strsetsi_1): New pattern.\n\t* i386.h (MASK_NO_ALIGN_STROP, MASK_INLINE_ALL_STROP,\n\tTARGET_ALIGN_STRINGOPS, TARGET_INLINE_ALL_STRINGOPS): New macros.\n\t(TARGET_SWITCHES) Add align-stringops and inline-all-stringops.\n\t* invoke.texi (align-stringops, inline-all-stringops): Document.\n\nFrom-SVN: r31773", "tree": {"sha": "e836a7b6ad85ed399075edd002c6b38840018694", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/e836a7b6ad85ed399075edd002c6b38840018694"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/79f05c19ca1ee164031f0d3926bc5074c499da10", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/79f05c19ca1ee164031f0d3926bc5074c499da10", "html_url": "https://github.com/Rust-GCC/gccrs/commit/79f05c19ca1ee164031f0d3926bc5074c499da10", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/79f05c19ca1ee164031f0d3926bc5074c499da10/comments", "author": null, "committer": null, "parents": [{"sha": "31a72d3f3e81140f66f4c140114147de5b913398", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/31a72d3f3e81140f66f4c140114147de5b913398", "html_url": "https://github.com/Rust-GCC/gccrs/commit/31a72d3f3e81140f66f4c140114147de5b913398"}], "stats": {"total": 399, "additions": 384, "deletions": 15}, "files": [{"sha": "8bb5dd68b41a20c2734013c13bf7d6d07c11d6ee", "filename": "gcc/ChangeLog", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79f05c19ca1ee164031f0d3926bc5074c499da10/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79f05c19ca1ee164031f0d3926bc5074c499da10/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=79f05c19ca1ee164031f0d3926bc5074c499da10", "patch": "@@ -1,3 +1,14 @@\n+Thu Feb  3 15:08:13 MET 2000  Jan Hubicka  <jh@suse.cz>\n+\n+\t* i386.md (movstrsi, clrstrsi): Support variable sized copies, align\n+\tdestination when needed.\n+\t(strmovsi, strsetsi): New expander.\n+\t(strmovsi_1, strsetsi_1): New pattern.\n+\t* i386.h (MASK_NO_ALIGN_STROP, MASK_INLINE_ALL_STROP,\n+\tTARGET_ALIGN_STRINGOPS, TARGET_INLINE_ALL_STRINGOPS): New macros.\n+\t(TARGET_SWITCHES) Add align-stringops and inline-all-stringops.\n+\t* invoke.texi (align-stringops, inline-all-stringops): Document.\n+\n Wed Feb  2 23:04:47 2000   Krister Walfridsson <cato@df.lth.se>\n \n \t* i386/netbsd.h (INT_ASM_OP): Define."}, {"sha": "8c33c66f50747584bf042061ef8420177ba56a2f", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79f05c19ca1ee164031f0d3926bc5074c499da10/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79f05c19ca1ee164031f0d3926bc5074c499da10/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=79f05c19ca1ee164031f0d3926bc5074c499da10", "patch": "@@ -101,6 +101,8 @@ extern int target_flags;\n #define MASK_NO_FANCY_MATH_387\t0x00000040\t/* Disable sin, cos, sqrt */\n #define MASK_OMIT_LEAF_FRAME_POINTER 0x080      /* omit leaf frame pointers */\n #define MASK_STACK_PROBE\t0x00000100\t/* Enable stack probing */\n+#define MASK_NO_ALIGN_STROPS\t0x00001000\t/* Enable aligning of string ops. */\n+#define MASK_INLINE_ALL_STROPS\t0x00002000\t/* Inline stringops in all cases */\n \n /* Temporary codegen switches */\n #define MASK_INTEL_SYNTAX\t0x00000200\n@@ -190,6 +192,9 @@ extern const int x86_promote_QImode, x86_single_stringop;\n \n #define TARGET_STACK_PROBE (target_flags & MASK_STACK_PROBE)\n \n+#define TARGET_ALIGN_STRINGOPS (!(target_flags & MASK_NO_ALIGN_STROPS))\n+#define TARGET_INLINE_ALL_STRINGOPS (target_flags & MASK_INLINE_ALL_STROPS)\n+\n #define ASSEMBLER_DIALECT ((target_flags & MASK_INTEL_SYNTAX) != 0)\n \n #define TARGET_SWITCHES\t\t\t\t\t\t\t      \\\n@@ -238,6 +243,14 @@ extern const int x86_promote_QImode, x86_single_stringop;\n   { \"intel-syntax\",\t\tMASK_INTEL_SYNTAX,\t\t\t      \\\n     \"Emit Intel syntax assembler opcodes\" },\t\t\t\t      \\\n   { \"no-intel-syntax\",\t\t-MASK_INTEL_SYNTAX, \"\" },\t\t      \\\n+  { \"align-stringops\",\t\t-MASK_NO_ALIGN_STROPS,\t\t\t      \\\n+    \"Align destination of the string operations\" },\t\t\t      \\\n+  { \"no-align-stringops\",\t MASK_NO_ALIGN_STROPS,\t\t\t      \\\n+    \"Do not align destination of the string operations\" },\t\t      \\\n+  { \"inline-all-strinops\",\t MASK_INLINE_ALL_STROPS,\t\t      \\\n+    \"Inline all known string operations\" },\t\t\t\t      \\\n+  { \"no-inline-all-stringops\",\t-MASK_INLINE_ALL_STROPS,\t\t      \\\n+    \"Do not inline all known string operations\" },\t\t\t      \\\n   SUBTARGET_SWITCHES\t\t\t\t\t\t\t      \\\n   { \"\", TARGET_DEFAULT, 0 }}\n "}, {"sha": "c5454d7b77b18b155ed3eac30adfe96699c9e4d2", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 346, "deletions": 14, "changes": 360, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79f05c19ca1ee164031f0d3926bc5074c499da10/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79f05c19ca1ee164031f0d3926bc5074c499da10/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=79f05c19ca1ee164031f0d3926bc5074c499da10", "patch": "@@ -7838,49 +7838,208 @@\n (define_expand \"movstrsi\"\n   [(use (match_operand:BLK 0 \"memory_operand\" \"\"))\n    (use (match_operand:BLK 1 \"memory_operand\" \"\"))\n-   (use (match_operand:SI 2 \"const_int_operand\" \"\"))\n+   (use (match_operand:SI 2 \"nonmemory_operand\" \"\"))\n    (use (match_operand:SI 3 \"const_int_operand\" \"\"))]\n   \"\"\n   \"\n {\n   rtx srcreg, destreg, countreg;\n+  int align = 0;\n+  int count = -1;\n \n-  if (GET_CODE (operands[2]) != CONST_INT)\n-    FAIL;\n+  if (GET_CODE (operands[3]) == CONST_INT)\n+    align = INTVAL (operands[3]);\n+\n+  /* This simple hack avoids all inlining code and simplifies code bellow.  */\n+  if (!TARGET_ALIGN_STRINGOPS)\n+    align = 32;\n+\n+  if (GET_CODE (operands[2]) == CONST_INT)\n+    count = INTVAL (operands[2]);\n \n   destreg = copy_to_mode_reg (Pmode, XEXP (operands[0], 0));\n   srcreg = copy_to_mode_reg (Pmode, XEXP (operands[1], 0));\n \n   emit_insn (gen_cld());\n+\n   /* When optimizing for size emit simple rep ; movsb instruction for\n      counts not divisible by 4.  */\n-  if ((!optimize || optimize_size) && (INTVAL (operands[2]) & 0x03))\n+\n+  if ((!optimize || optimize_size) \n+      && (count < 0 || (count & 0x03)))\n     {\n       countreg = copy_to_mode_reg (SImode, operands[2]);\n       emit_insn (gen_rep_movqi (destreg, srcreg, countreg,\n       \t\t\t\tdestreg, srcreg, countreg));\n     }\n-  else\n+\n+  /* For constant aligned (or small unaligned) copies use rep movsl\n+     followed by code copying the rest.  For PentiumPro ensure 8 byte\n+     alignment to allow rep movsl acceleration.  */\n+\n+  else if (count >= 0 \n+\t   && (align >= 8\n+\t       || (!TARGET_PENTIUMPRO && align >= 4)\n+\t       || optimize_size || count < 64))\n     {\n-      if (INTVAL (operands[2]) & ~0x03)\n+      if (count & ~0x03)\n \t{\n \t  countreg = copy_to_mode_reg (SImode,\n-\t  \t\t\t       GEN_INT ((INTVAL (operands[2]) >> 2)\n+\t  \t\t\t       GEN_INT ((count >> 2)\n \t\t\t\t\t\t& 0x3fffffff));\n \t  emit_insn (gen_rep_movsi (destreg, srcreg, countreg,\n \t\t\t\t    destreg, srcreg, countreg));\n \t}\n-      if (INTVAL (operands[2]) & 0x02)\n+      if (count & 0x02)\n \temit_insn (gen_strmovhi (destreg, srcreg));\n-      if (INTVAL (operands[2]) & 0x01)\n+      if (count & 0x01)\n \temit_insn (gen_strmovqi (destreg, srcreg));\n     }\n+  /* The generic code based on the glibc implementation:\n+     - align destination to 4 bytes (8 byte alignment is used for PentiumPro\n+       allowing accelerated copying there)\n+     - copy the data using rep movsl\n+     - copy the rest.  */\n+  else\n+    {\n+      rtx countreg2;\n+      rtx label = NULL;\n+\n+      /* In case we don't know anything about the alignment, default to\n+         library version, since it is usually equally fast and result in\n+\t shorter code.  */\n+      if (!TARGET_INLINE_ALL_STRINGOPS && align < 4)\n+\tFAIL;\n+\n+      if (TARGET_SINGLE_STRINGOP)\n+\temit_insn (gen_cld());\n+\n+      countreg2 = gen_reg_rtx (SImode);\n+      countreg = copy_to_mode_reg (SImode, operands[2]);\n+\n+      /* We don't use loops to align destination and to copy parts smaller\n+\t than 4 bytes, because gcc is able to optimize such code better (in\n+\t the case the destination or the count really is aligned, gcc is often\n+\t able to predict the branches) and also it is friendlier to the\n+\t hardware branch prediction.  \n+\n+\t Using loops is benefical for generic case, because we can\n+\t handle small counts using the loops.  Many CPUs (such as Athlon)\n+\t have large REP prefix setup costs.\n+\n+\t This is quite costy.  Maybe we can revisit this decision later or\n+\t add some customizability to this code.  */\n+\n+      if (count < 0\n+\t  && align < (TARGET_PENTIUMPRO && (count < 0 || count >= 260) ? 8 : 4))\n+\t{\n+\t  label = gen_label_rtx ();\n+\t  emit_cmp_and_jump_insns (countreg, GEN_INT (3),\n+\t\t\t\t   LEU, 0, SImode, 1, 0, label);\n+\t}\n+      if (align <= 1)\n+\t{\n+\t  rtx label = gen_label_rtx ();\n+\t  rtx tmpcount = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_andsi3 (tmpcount, destreg, GEN_INT (1)));\n+\t  emit_cmp_and_jump_insns (tmpcount, GEN_INT (0), EQ, 0,\n+\t\t\t\t   SImode, 1, 0, label);\n+\t  emit_insn (gen_strmovqi (destreg, srcreg));\n+\t  emit_insn (gen_addsi3 (countreg, countreg, constm1_rtx));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+      if (align <= 2)\n+\t{\n+\t  rtx label = gen_label_rtx ();\n+\t  rtx tmpcount = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_andsi3 (tmpcount, destreg, GEN_INT (2)));\n+\t  emit_cmp_and_jump_insns (tmpcount, GEN_INT (0), EQ, 0,\n+\t\t\t\t   SImode, 1, 0, label);\n+\t  emit_insn (gen_strmovhi (destreg, srcreg));\n+\t  emit_insn (gen_addsi3 (countreg, countreg, GEN_INT (-2)));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+      if (align <= 4 && TARGET_PENTIUMPRO && (count < 1 || count >= 260))\n+\t{\n+\t  rtx label = gen_label_rtx ();\n+\t  rtx tmpcount = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_andsi3 (tmpcount, destreg, GEN_INT (4)));\n+\t  emit_cmp_and_jump_insns (tmpcount, GEN_INT (0), EQ, 0,\n+\t\t\t\t   SImode, 1, 0, label);\n+\t  emit_insn (gen_strmovsi (destreg, srcreg));\n+\t  emit_insn (gen_addsi3 (countreg, countreg, GEN_INT (-4)));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+\n+      if (!TARGET_SINGLE_STRINGOP)\n+\temit_insn (gen_cld());\n+      emit_insn (gen_lshrsi3 (countreg2, countreg, GEN_INT (2)));\n+      emit_insn (gen_rep_movsi (destreg, srcreg, countreg2,\n+\t\t\t\tdestreg, srcreg, countreg2));\n+\n+      if (label)\n+\t{\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+      if (align > 2 && count > 0 && (count & 2))\n+\temit_insn (gen_strmovhi (destreg, srcreg));\n+      if (align <= 2 || count < 0)\n+\t{\n+\t  rtx label = gen_label_rtx ();\n+\t  rtx tmpcount = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_andsi3 (tmpcount, countreg, GEN_INT (2)));\n+\t  emit_cmp_and_jump_insns (tmpcount, GEN_INT (0), EQ, 0,\n+\t\t\t\t   SImode, 1, 0, label);\n+\t  emit_insn (gen_strmovhi (destreg, srcreg));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+      if (align > 1 && count > 0 && (count & 1))\n+\temit_insn (gen_strmovsi (destreg, srcreg));\n+      if (align <= 1 || count < 0)\n+\t{\n+\t  rtx label = gen_label_rtx ();\n+\t  rtx tmpcount = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_andsi3 (tmpcount, countreg, GEN_INT (1)));\n+\t  emit_cmp_and_jump_insns (tmpcount, GEN_INT (0), EQ, 0,\n+\t\t\t\t   SImode, 1, 0, label);\n+\t  emit_insn (gen_strmovqi (destreg, srcreg));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+    }\n   DONE;\n }\")\n \n ;; Most CPUs don't like single string operations\n ;; Handle this case here to simplify previous expander.\n \n+(define_expand \"strmovsi\"\n+  [(set (match_dup 2)\n+  \t(mem:SI (match_operand:SI 1 \"register_operand\" \"\")))\n+   (set (mem:SI (match_operand:SI 0 \"register_operand\" \"\"))\n+        (match_dup 2))\n+   (parallel [(set (match_dup 0) (plus:SI (match_dup 0) (const_int 4)))\n+\t      (clobber (reg:CC 17))])\n+   (parallel [(set (match_dup 1) (plus:SI (match_dup 1) (const_int 4)))\n+\t      (clobber (reg:CC 17))])]\n+  \"\"\n+  \"\n+{\n+  if (TARGET_SINGLE_STRINGOP || optimize_size)\n+    {\n+      emit_insn (gen_strmovsi_1 (operands[0], operands[1], operands[0],\n+\t\t\t\toperands[1]));\n+      DONE;\n+    }\n+  else \n+    operands[2] = gen_reg_rtx (SImode);\n+}\")\n+\n (define_expand \"strmovhi\"\n   [(set (match_dup 2)\n   \t(mem:HI (match_operand:SI 1 \"register_operand\" \"\")))\n@@ -7925,6 +8084,21 @@\n     operands[2] = gen_reg_rtx (QImode);\n }\")\n \n+(define_insn \"strmovsi_1\"\n+  [(set (mem:SI (match_operand:SI 2 \"register_operand\" \"0\"))\n+\t(mem:SI (match_operand:SI 3 \"register_operand\" \"1\")))\n+   (set (match_operand:SI 0 \"register_operand\" \"=D\")\n+\t(plus:SI (match_dup 0)\n+\t\t (const_int 4)))\n+   (set (match_operand:SI 1 \"register_operand\" \"=S\")\n+\t(plus:SI (match_dup 1)\n+\t\t (const_int 4)))\n+   (use (reg:SI 19))]\n+  \"TARGET_SINGLE_STRINGOP || optimize_size\"\n+  \"movsl\"\n+  [(set_attr \"type\" \"str\")\n+   (set_attr \"memory\" \"both\")])\n+\n (define_insn \"strmovhi_1\"\n   [(set (mem:HI (match_operand:SI 2 \"register_operand\" \"0\"))\n \t(mem:HI (match_operand:SI 3 \"register_operand\" \"1\")))\n@@ -7996,30 +8170,46 @@\n \n (define_expand \"clrstrsi\"\n    [(use (match_operand:BLK 0 \"memory_operand\" \"\"))\n-    (use (match_operand:SI 1 \"const_int_operand\" \"\"))\n+    (use (match_operand:SI 1 \"nonmemory_operand\" \"\"))\n     (use (match_operand:SI 2 \"const_int_operand\" \"\"))]\n   \"\"\n   \"\n {\n+  /* See comments in movstr expanders.  The code is mostly identical.  */\n+\n   rtx destreg, zeroreg, countreg;\n+  int align = 0;\n+  int count = -1;\n \n-  if (GET_CODE (operands[1]) != CONST_INT)\n-    FAIL;\n+  if (GET_CODE (operands[2]) == CONST_INT)\n+    align = INTVAL (operands[2]);\n+\n+  /* This simple hack avoids all inlining code and simplifies code bellow.  */\n+  if (!TARGET_ALIGN_STRINGOPS)\n+    align = 32;\n+\n+  if (GET_CODE (operands[1]) == CONST_INT)\n+    count = INTVAL (operands[1]);\n \n   destreg = copy_to_mode_reg (Pmode, XEXP (operands[0], 0));\n \n   emit_insn (gen_cld());\n \n   /* When optimizing for size emit simple rep ; movsb instruction for\n      counts not divisible by 4.  */\n-  if ((!optimize || optimize_size) && (INTVAL (operands[1]) & 0x03))\n+\n+  if ((!optimize || optimize_size) \n+      && (count < 0 || (count & 0x03)))\n     {\n       countreg = copy_to_mode_reg (SImode, operands[1]);\n       zeroreg = copy_to_mode_reg (QImode, const0_rtx);\n       emit_insn (gen_rep_stosqi (destreg, countreg, zeroreg,\n \t\t\t\t destreg, countreg));\n     }\n-  else\n+  else if (count >= 0 \n+\t   && (align >= 8\n+\t       || (!TARGET_PENTIUMPRO && align >= 4)\n+\t       || optimize_size || count < 64))\n     {\n       zeroreg = copy_to_mode_reg (SImode, const0_rtx);\n       if (INTVAL (operands[1]) & ~0x03)\n@@ -8037,12 +8227,133 @@\n \temit_insn (gen_strsetqi (destreg,\n \t\t\t\t gen_rtx_SUBREG (QImode, zeroreg, 0)));\n     }\n+  else\n+    {\n+      rtx countreg2;\n+      rtx label = NULL;\n+\n+      /* In case we don't know anything about the alignment, default to\n+         library version, since it is usually equally fast and result in\n+\t shorter code.  */\n+      if (!TARGET_INLINE_ALL_STRINGOPS && align < 4)\n+\tFAIL;\n+\n+      if (TARGET_SINGLE_STRINGOP)\n+\temit_insn (gen_cld());\n+\n+      countreg2 = gen_reg_rtx (SImode);\n+      countreg = copy_to_mode_reg (SImode, operands[1]);\n+      zeroreg = copy_to_mode_reg (SImode, const0_rtx);\n+\n+      if (count < 0\n+\t  && align < (TARGET_PENTIUMPRO && (count < 0 || count >= 260) ? 8 : 4))\n+\t{\n+\t  label = gen_label_rtx ();\n+\t  emit_cmp_and_jump_insns (countreg, GEN_INT (3),\n+\t\t\t\t   LEU, 0, SImode, 1, 0, label);\n+\t}\n+      if (align <= 1)\n+\t{\n+\t  rtx label = gen_label_rtx ();\n+\t  rtx tmpcount = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_andsi3 (tmpcount, destreg, GEN_INT (1)));\n+\t  emit_cmp_and_jump_insns (tmpcount, GEN_INT (0), EQ, 0,\n+\t\t\t\t   SImode, 1, 0, label);\n+\t  emit_insn (gen_strsetqi (destreg,\n+\t\t\t\t   gen_rtx_SUBREG (QImode, zeroreg, 0)));\n+\t  emit_insn (gen_addsi3 (countreg, countreg, constm1_rtx));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+      if (align <= 2)\n+\t{\n+\t  rtx label = gen_label_rtx ();\n+\t  rtx tmpcount = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_andsi3 (tmpcount, destreg, GEN_INT (2)));\n+\t  emit_cmp_and_jump_insns (tmpcount, GEN_INT (0), EQ, 0,\n+\t\t\t\t   SImode, 1, 0, label);\n+\t  emit_insn (gen_strsethi (destreg,\n+\t\t\t\t   gen_rtx_SUBREG (HImode, zeroreg, 0)));\n+\t  emit_insn (gen_addsi3 (countreg, countreg, GEN_INT (-2)));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+      if (align <= 4 && TARGET_PENTIUMPRO && (count < 1 || count >= 260))\n+\t{\n+\t  rtx label = gen_label_rtx ();\n+\t  rtx tmpcount = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_andsi3 (tmpcount, destreg, GEN_INT (4)));\n+\t  emit_cmp_and_jump_insns (tmpcount, GEN_INT (0), EQ, 0,\n+\t\t\t\t   SImode, 1, 0, label);\n+\t  emit_insn (gen_strsethi (destreg, zeroreg));\n+\t  emit_insn (gen_addsi3 (countreg, countreg, GEN_INT (-4)));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+\n+      if (!TARGET_SINGLE_STRINGOP)\n+\temit_insn (gen_cld());\n+      emit_insn (gen_lshrsi3 (countreg2, countreg, GEN_INT (2)));\n+      emit_insn (gen_rep_stossi (destreg, countreg2, zeroreg,\n+\t\t\t\t destreg, countreg2));\n+\n+      if (label)\n+\t{\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+      if (align > 2 && count > 0 && (count & 2))\n+\temit_insn (gen_strsethi (destreg,\n+\t\t\t\t gen_rtx_SUBREG (HImode, zeroreg, 0)));\n+      if (align <= 2 || count < 0)\n+\t{\n+\t  rtx label = gen_label_rtx ();\n+\t  rtx tmpcount = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_andsi3 (tmpcount, countreg, GEN_INT (2)));\n+\t  emit_cmp_and_jump_insns (tmpcount, GEN_INT (0), EQ, 0,\n+\t\t\t\t   SImode, 1, 0, label);\n+\t  emit_insn (gen_strsethi (destreg,\n+\t\t\t\t   gen_rtx_SUBREG (HImode, zeroreg, 0)));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+      if (align > 1 && count > 0 && (count & 1))\n+\temit_insn (gen_strsetqi (destreg,\n+\t\t\t\t gen_rtx_SUBREG (QImode, zeroreg, 0)));\n+      if (align <= 1 || count < 0)\n+\t{\n+\t  rtx label = gen_label_rtx ();\n+\t  rtx tmpcount = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_andsi3 (tmpcount, countreg, GEN_INT (1)));\n+\t  emit_cmp_and_jump_insns (tmpcount, GEN_INT (0), EQ, 0,\n+\t\t\t\t   SImode, 1, 0, label);\n+\t  emit_insn (gen_strsetqi (destreg,\n+\t\t\t\t   gen_rtx_SUBREG (QImode, zeroreg, 0)));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+    }\n   DONE;\n }\")\n \n ;; Most CPUs don't like single string operations\n ;; Handle this case here to simplify previous expander.\n \n+(define_expand \"strsetsi\"\n+  [(set (mem:SI (match_operand:SI 0 \"register_operand\" \"\"))\n+\t(match_operand:SI 1 \"register_operand\" \"\"))\n+   (parallel [(set (match_dup 0) (plus:SI (match_dup 0) (const_int 4)))\n+\t      (clobber (reg:CC 17))])]\n+  \"\"\n+  \"\n+{\n+  if (TARGET_SINGLE_STRINGOP || optimize_size)\n+    {\n+      emit_insn (gen_strsetsi_1 (operands[0], operands[0], operands[1]));\n+      DONE;\n+    }\n+}\")\n+\n (define_expand \"strsethi\"\n   [(set (mem:HI (match_operand:SI 0 \"register_operand\" \"\"))\n \t(match_operand:HI 1 \"register_operand\" \"\"))\n@@ -8073,6 +8384,18 @@\n     }\n }\")\n \n+(define_insn \"strsetsi_1\"\n+  [(set (mem:SI (match_operand:SI 1 \"register_operand\" \"0\"))\n+\t(match_operand:SI 2 \"register_operand\" \"a\"))\n+   (set (match_operand:SI 0 \"register_operand\" \"=D\")\n+\t(plus:SI (match_dup 0)\n+\t\t (const_int 4)))\n+   (use (reg:SI 19))]\n+  \"TARGET_SINGLE_STRINGOP || optimize_size\"\n+  \"stosl\"\n+  [(set_attr \"type\" \"str\")\n+   (set_attr \"memory\" \"store\")])\n+\n (define_insn \"strsethi_1\"\n   [(set (mem:HI (match_operand:SI 1 \"register_operand\" \"0\"))\n \t(match_operand:HI 2 \"register_operand\" \"a\"))\n@@ -8252,6 +8575,14 @@\n {\n   rtx out, addr, eoschar, align, scratch1, scratch2, scratch3;\n \n+  /* The generic case of strlen expander is long.  Avoid it's\n+     expanding unless TARGET_INLINE_ALL_STRINGOPS.  */\n+\n+  if (TARGET_UNROLL_STRLEN && eoschar == const0_rtx && optimize > 1\n+      && !optimize_size\n+      && (GET_CODE (align) != CONST_INT || INTVAL (align) < 4))\n+    FAIL;\n+\n   out = operands[0];\n   addr = force_reg (Pmode, XEXP (operands[1], 0));\n   eoschar = operands[2];\n@@ -8271,6 +8602,7 @@\n \n       if (GET_CODE (align) != CONST_INT || INTVAL (align) < 4)\n \temit_move_insn (scratch1, addr);\n+\n       emit_move_insn (out, addr);\n \n       ix86_expand_strlensi_unroll_1 (out, align, scratch1);"}, {"sha": "f09ef55776209715b5e7f818ae5953f17bded367", "filename": "gcc/invoke.texi", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79f05c19ca1ee164031f0d3926bc5074c499da10/gcc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79f05c19ca1ee164031f0d3926bc5074c499da10/gcc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Finvoke.texi?ref=79f05c19ca1ee164031f0d3926bc5074c499da10", "patch": "@@ -360,7 +360,7 @@ in the following sections.\n -mreg-alloc=@var{list}  -mregparm=@var{num}\n -malign-jumps=@var{num}  -malign-loops=@var{num}\n -malign-functions=@var{num} -mpreferred-stack-boundary=@var{num}\n--mthreads\n+-mthreads -mno-align-stringops -minline-all-stringops\n \n @emph{HPPA Options}\n -march=@var{architecture type}\n@@ -5954,6 +5954,19 @@ on thread-safe exception handling must compile and link all code with the\n @samp{-mthreads} option. When compiling, @samp{-mthreads} defines \n @samp{-D_MT}; when linking, it links in a special thread helper library \n @samp{-lmingwthrd} which cleans up per thread exception handling data.\n+\n+@item -mno-align-stringops\n+@kindex -mno-align-stringops\n+Do not align destination of inlined string operations. This switch reduces\n+code size and improves performance in case the destination is already aligned,\n+but gcc don't know about it.\n+\n+@item -minline-all-stringops\n+@kindex -minline-all-stringops\n+By default GCC inlines string operations only when destination is known to be\n+aligned at least to 4 byte boundary. This enables more inlining, increase code\n+size, but may improve performance of code that depends on fast memcpy, strlen\n+and memset for short lengths.\n @end table\n \n @node HPPA Options"}]}