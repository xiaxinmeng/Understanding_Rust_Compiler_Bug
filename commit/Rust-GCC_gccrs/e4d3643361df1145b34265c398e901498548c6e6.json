{"sha": "e4d3643361df1145b34265c398e901498548c6e6", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZTRkMzY0MzM2MWRmMTE0NWIzNDI2NWMzOThlOTAxNDk4NTQ4YzZlNg==", "commit": {"author": {"name": "Hongyu Wang", "email": "hongyu.wang@intel.com", "date": "2021-09-15T08:46:59Z"}, "committer": {"name": "Hongyu Wang", "email": "hongyu.wang@intel.com", "date": "2021-09-15T09:58:15Z"}, "message": "AVX512FP16: Adjust builtin name for FP16 builtins to match AVX512F style\n\nFor AVX512FP16 builtins, they all contain format like vaddph_v8hf,\nwhile AVX512F builtins use addps128 which succeeded SSE/AVX style.\nAdjust AVX512FP16 builtins to match such format.\n\ngcc/ChangeLog:\n\n\t* config/i386/avx512fp16intrin.h: Adjust all builtin calls.\n\t* config/i386/avx512fp16vlintrin.h: Likewise.\n\t* config/i386/i386-builtin.def: Adjust builtin name and\n\tenumeration to match AVX512F style.\n\ngcc/testsuite/ChangeLog:\n\n\t* gcc.target/i386/avx-1.c: Adjust builtin macros.\n\t* gcc.target/i386/sse-13.c: Likewise.\n\t* gcc.target/i386/sse-23.c: Likewise.", "tree": {"sha": "fc27b9a0a249fc1e1b6ba80c2bcbd98019ed039f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/fc27b9a0a249fc1e1b6ba80c2bcbd98019ed039f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/e4d3643361df1145b34265c398e901498548c6e6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e4d3643361df1145b34265c398e901498548c6e6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e4d3643361df1145b34265c398e901498548c6e6", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e4d3643361df1145b34265c398e901498548c6e6/comments", "author": {"login": "wwwhhhyyy", "id": 5366075, "node_id": "MDQ6VXNlcjUzNjYwNzU=", "avatar_url": "https://avatars.githubusercontent.com/u/5366075?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wwwhhhyyy", "html_url": "https://github.com/wwwhhhyyy", "followers_url": "https://api.github.com/users/wwwhhhyyy/followers", "following_url": "https://api.github.com/users/wwwhhhyyy/following{/other_user}", "gists_url": "https://api.github.com/users/wwwhhhyyy/gists{/gist_id}", "starred_url": "https://api.github.com/users/wwwhhhyyy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wwwhhhyyy/subscriptions", "organizations_url": "https://api.github.com/users/wwwhhhyyy/orgs", "repos_url": "https://api.github.com/users/wwwhhhyyy/repos", "events_url": "https://api.github.com/users/wwwhhhyyy/events{/privacy}", "received_events_url": "https://api.github.com/users/wwwhhhyyy/received_events", "type": "User", "site_admin": false}, "committer": {"login": "wwwhhhyyy", "id": 5366075, "node_id": "MDQ6VXNlcjUzNjYwNzU=", "avatar_url": "https://avatars.githubusercontent.com/u/5366075?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wwwhhhyyy", "html_url": "https://github.com/wwwhhhyyy", "followers_url": "https://api.github.com/users/wwwhhhyyy/followers", "following_url": "https://api.github.com/users/wwwhhhyyy/following{/other_user}", "gists_url": "https://api.github.com/users/wwwhhhyyy/gists{/gist_id}", "starred_url": "https://api.github.com/users/wwwhhhyyy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wwwhhhyyy/subscriptions", "organizations_url": "https://api.github.com/users/wwwhhhyyy/orgs", "repos_url": "https://api.github.com/users/wwwhhhyyy/repos", "events_url": "https://api.github.com/users/wwwhhhyyy/events{/privacy}", "received_events_url": "https://api.github.com/users/wwwhhhyyy/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b6d8fa66e1bf08756cb4134735b5034e171f49d1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b6d8fa66e1bf08756cb4134735b5034e171f49d1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b6d8fa66e1bf08756cb4134735b5034e171f49d1"}], "stats": {"total": 1994, "additions": 997, "deletions": 997}, "files": [{"sha": "5d66ca5c8208f4b645340207cb91edb1c3591218", "filename": "gcc/config/i386/avx512fp16intrin.h", "status": "modified", "additions": 618, "deletions": 618, "changes": 1236, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Fconfig%2Fi386%2Favx512fp16intrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Fconfig%2Fi386%2Favx512fp16intrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Favx512fp16intrin.h?ref=e4d3643361df1145b34265c398e901498548c6e6", "patch": "@@ -229,15 +229,15 @@ extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_add_ph (__m512h __A, __mmask32 __B, __m512h __C, __m512h __D)\n {\n-  return __builtin_ia32_vaddph_v32hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_addph512_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_add_ph (__mmask32 __A, __m512h __B, __m512h __C)\n {\n-  return __builtin_ia32_vaddph_v32hf_mask (__B, __C,\n-\t\t\t\t\t   _mm512_setzero_ph (), __A);\n+  return __builtin_ia32_addph512_mask (__B, __C,\n+\t\t\t\t       _mm512_setzero_ph (), __A);\n }\n \n extern __inline __m512h\n@@ -251,15 +251,15 @@ extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_sub_ph (__m512h __A, __mmask32 __B, __m512h __C, __m512h __D)\n {\n-  return __builtin_ia32_vsubph_v32hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_subph512_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_sub_ph (__mmask32 __A, __m512h __B, __m512h __C)\n {\n-  return __builtin_ia32_vsubph_v32hf_mask (__B, __C,\n-\t\t\t\t\t   _mm512_setzero_ph (), __A);\n+  return __builtin_ia32_subph512_mask (__B, __C,\n+\t\t\t\t       _mm512_setzero_ph (), __A);\n }\n \n extern __inline __m512h\n@@ -273,15 +273,15 @@ extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_mul_ph (__m512h __A, __mmask32 __B, __m512h __C, __m512h __D)\n {\n-  return __builtin_ia32_vmulph_v32hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_mulph512_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_mul_ph (__mmask32 __A, __m512h __B, __m512h __C)\n {\n-  return __builtin_ia32_vmulph_v32hf_mask (__B, __C,\n-\t\t\t\t\t   _mm512_setzero_ph (), __A);\n+  return __builtin_ia32_mulph512_mask (__B, __C,\n+\t\t\t\t       _mm512_setzero_ph (), __A);\n }\n \n extern __inline __m512h\n@@ -295,182 +295,182 @@ extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_div_ph (__m512h __A, __mmask32 __B, __m512h __C, __m512h __D)\n {\n-  return __builtin_ia32_vdivph_v32hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_divph512_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_div_ph (__mmask32 __A, __m512h __B, __m512h __C)\n {\n-  return __builtin_ia32_vdivph_v32hf_mask (__B, __C,\n-\t\t\t\t\t   _mm512_setzero_ph (), __A);\n+  return __builtin_ia32_divph512_mask (__B, __C,\n+\t\t\t\t       _mm512_setzero_ph (), __A);\n }\n \n #ifdef __OPTIMIZE__\n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_add_round_ph (__m512h __A, __m512h __B, const int __C)\n {\n-  return __builtin_ia32_vaddph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t (__mmask32) -1, __C);\n+  return __builtin_ia32_addph512_mask_round (__A, __B,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     (__mmask32) -1, __C);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_add_round_ph (__m512h __A, __mmask32 __B, __m512h __C,\n \t\t\t  __m512h __D, const int __E)\n {\n-  return __builtin_ia32_vaddph_v32hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_addph512_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_add_round_ph (__mmask32 __A, __m512h __B, __m512h __C,\n \t\t\t   const int __D)\n {\n-  return __builtin_ia32_vaddph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t __A, __D);\n+  return __builtin_ia32_addph512_mask_round (__B, __C,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     __A, __D);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_sub_round_ph (__m512h __A, __m512h __B, const int __C)\n {\n-  return __builtin_ia32_vsubph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t (__mmask32) -1, __C);\n+  return __builtin_ia32_subph512_mask_round (__A, __B,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     (__mmask32) -1, __C);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_sub_round_ph (__m512h __A, __mmask32 __B, __m512h __C,\n \t\t\t  __m512h __D, const int __E)\n {\n-  return __builtin_ia32_vsubph_v32hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_subph512_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_sub_round_ph (__mmask32 __A, __m512h __B, __m512h __C,\n \t\t\t   const int __D)\n {\n-  return __builtin_ia32_vsubph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t __A, __D);\n+  return __builtin_ia32_subph512_mask_round (__B, __C,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     __A, __D);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mul_round_ph (__m512h __A, __m512h __B, const int __C)\n {\n-  return __builtin_ia32_vmulph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t (__mmask32) -1, __C);\n+  return __builtin_ia32_mulph512_mask_round (__A, __B,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     (__mmask32) -1, __C);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_mul_round_ph (__m512h __A, __mmask32 __B, __m512h __C,\n \t\t\t  __m512h __D, const int __E)\n {\n-  return __builtin_ia32_vmulph_v32hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_mulph512_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_mul_round_ph (__mmask32 __A, __m512h __B, __m512h __C,\n \t\t\t   const int __D)\n {\n-  return __builtin_ia32_vmulph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t __A, __D);\n+  return __builtin_ia32_mulph512_mask_round (__B, __C,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     __A, __D);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_div_round_ph (__m512h __A, __m512h __B, const int __C)\n {\n-  return __builtin_ia32_vdivph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t (__mmask32) -1, __C);\n+  return __builtin_ia32_divph512_mask_round (__A, __B,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     (__mmask32) -1, __C);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_div_round_ph (__m512h __A, __mmask32 __B, __m512h __C,\n \t\t\t  __m512h __D, const int __E)\n {\n-  return __builtin_ia32_vdivph_v32hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_divph512_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_div_round_ph (__mmask32 __A, __m512h __B, __m512h __C,\n \t\t\t   const int __D)\n {\n-  return __builtin_ia32_vdivph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t __A, __D);\n+  return __builtin_ia32_divph512_mask_round (__B, __C,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     __A, __D);\n }\n #else\n #define _mm512_add_round_ph(A, B, C)\t\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_vaddph_v32hf_mask_round((A), (B),\t\t\\\n-\t\t\t\t\t\t   _mm512_setzero_ph (),\\\n-\t\t\t\t\t\t   (__mmask32)-1, (C)))\n+  ((__m512h)__builtin_ia32_addph512_mask_round((A), (B),\t\t\\\n+\t\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t       (__mmask32)-1, (C)))\n \n-#define _mm512_mask_add_round_ph(A, B, C, D, E)\t\t\t\\\n-  ((__m512h)__builtin_ia32_vaddph_v32hf_mask_round((C), (D), (A), (B), (E)))\n+#define _mm512_mask_add_round_ph(A, B, C, D, E)\t\t\t\t\\\n+  ((__m512h)__builtin_ia32_addph512_mask_round((C), (D), (A), (B), (E)))\n \n #define _mm512_maskz_add_round_ph(A, B, C, D)\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_vaddph_v32hf_mask_round((B), (C),\t\t\\\n-\t\t\t\t\t\t   _mm512_setzero_ph (),\\\n-\t\t\t\t\t\t   (A), (D)))\n+  ((__m512h)__builtin_ia32_addph512_mask_round((B), (C),\t\t\\\n+\t\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t       (A), (D)))\n \n #define _mm512_sub_round_ph(A, B, C)\t\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_vsubph_v32hf_mask_round((A), (B),\t\t\\\n-\t\t\t\t\t\t   _mm512_setzero_ph (),\\\n-\t\t\t\t\t\t   (__mmask32)-1, (C)))\n+  ((__m512h)__builtin_ia32_subph512_mask_round((A), (B),\t\t\\\n+\t\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t       (__mmask32)-1, (C)))\n \n-#define _mm512_mask_sub_round_ph(A, B, C, D, E)\t\t\t\\\n-  ((__m512h)__builtin_ia32_vsubph_v32hf_mask_round((C), (D), (A), (B), (E)))\n+#define _mm512_mask_sub_round_ph(A, B, C, D, E)\t\t\t\t\\\n+  ((__m512h)__builtin_ia32_subph512_mask_round((C), (D), (A), (B), (E)))\n \n #define _mm512_maskz_sub_round_ph(A, B, C, D)\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_vsubph_v32hf_mask_round((B), (C),\t\t\\\n-\t\t\t\t\t\t   _mm512_setzero_ph (),\\\n-\t\t\t\t\t\t   (A), (D)))\n+  ((__m512h)__builtin_ia32_subph512_mask_round((B), (C),\t\t\\\n+\t\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t       (A), (D)))\n \n #define _mm512_mul_round_ph(A, B, C)\t\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_vmulph_v32hf_mask_round((A), (B),\t\t\\\n-\t\t\t\t\t\t   _mm512_setzero_ph (),\\\n-\t\t\t\t\t\t   (__mmask32)-1, (C)))\n+  ((__m512h)__builtin_ia32_mulph512_mask_round((A), (B),\t\t\\\n+\t\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t       (__mmask32)-1, (C)))\n \n-#define _mm512_mask_mul_round_ph(A, B, C, D, E)\t\t\t\\\n-  ((__m512h)__builtin_ia32_vmulph_v32hf_mask_round((C), (D), (A), (B), (E)))\n+#define _mm512_mask_mul_round_ph(A, B, C, D, E)\t\t\t\t\\\n+  ((__m512h)__builtin_ia32_mulph512_mask_round((C), (D), (A), (B), (E)))\n \n #define _mm512_maskz_mul_round_ph(A, B, C, D)\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_vmulph_v32hf_mask_round((B), (C),\t\t\\\n-\t\t\t\t\t\t   _mm512_setzero_ph (),\\\n-\t\t\t\t\t\t   (A), (D)))\n+  ((__m512h)__builtin_ia32_mulph512_mask_round((B), (C),\t\t\\\n+\t\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t       (A), (D)))\n \n #define _mm512_div_round_ph(A, B, C)\t\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_vdivph_v32hf_mask_round((A), (B),\t\t\\\n-\t\t\t\t\t\t   _mm512_setzero_ph (),\\\n-\t\t\t\t\t\t   (__mmask32)-1, (C)))\n+  ((__m512h)__builtin_ia32_divph512_mask_round((A), (B),\t\t\\\n+\t\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t       (__mmask32)-1, (C)))\n \n-#define _mm512_mask_div_round_ph(A, B, C, D, E)\t\t\t\\\n-  ((__m512h)__builtin_ia32_vdivph_v32hf_mask_round((C), (D), (A), (B), (E)))\n+#define _mm512_mask_div_round_ph(A, B, C, D, E)\t\t\t\t\\\n+  ((__m512h)__builtin_ia32_divph512_mask_round((C), (D), (A), (B), (E)))\n \n #define _mm512_maskz_div_round_ph(A, B, C, D)\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_vdivph_v32hf_mask_round((B), (C),\t\t\\\n-\t\t\t\t\t\t   _mm512_setzero_ph (),\\\n-\t\t\t\t\t\t   (A), (D)))\n+  ((__m512h)__builtin_ia32_divph512_mask_round((B), (C),\t\t\\\n+\t\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t       (A), (D)))\n #endif  /* __OPTIMIZE__  */\n \n /* Intrinsics of v[add,sub,mul,div]sh.  */\n extern __inline __m128h\n-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n+  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_sh (__m128h __A, __m128h __B)\n {\n   __A[0] += __B[0];\n@@ -481,15 +481,15 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_add_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vaddsh_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_addsh_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_add_sh (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vaddsh_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_addsh_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t    __A);\n }\n \n extern __inline __m128h\n@@ -504,15 +504,15 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_sub_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vsubsh_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_subsh_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_sub_sh (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vsubsh_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_subsh_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t    __A);\n }\n \n extern __inline __m128h\n@@ -527,14 +527,14 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_mul_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vmulsh_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_mulsh_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_mul_sh (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vmulsh_v8hf_mask (__B, __C, _mm_setzero_ph (), __A);\n+  return __builtin_ia32_mulsh_mask (__B, __C, _mm_setzero_ph (), __A);\n }\n \n extern __inline __m128h\n@@ -549,309 +549,309 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_div_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vdivsh_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_divsh_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_div_sh (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vdivsh_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_divsh_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t    __A);\n }\n \n #ifdef __OPTIMIZE__\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_round_sh (__m128h __A, __m128h __B, const int __C)\n {\n-  return __builtin_ia32_vaddsh_v8hf_mask_round (__A, __B,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t(__mmask8) -1, __C);\n+  return __builtin_ia32_addsh_mask_round (__A, __B,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  (__mmask8) -1, __C);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_add_round_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t       __m128h __D, const int __E)\n {\n-  return __builtin_ia32_vaddsh_v8hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_addsh_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_add_round_sh (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\tconst int __D)\n {\n-  return __builtin_ia32_vaddsh_v8hf_mask_round (__B, __C,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t__A, __D);\n+  return __builtin_ia32_addsh_mask_round (__B, __C,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  __A, __D);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_round_sh (__m128h __A, __m128h __B, const int __C)\n {\n-  return __builtin_ia32_vsubsh_v8hf_mask_round (__A, __B,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t(__mmask8) -1, __C);\n+  return __builtin_ia32_subsh_mask_round (__A, __B,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  (__mmask8) -1, __C);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_sub_round_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t       __m128h __D, const int __E)\n {\n-  return __builtin_ia32_vsubsh_v8hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_subsh_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_sub_round_sh (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\tconst int __D)\n {\n-  return __builtin_ia32_vsubsh_v8hf_mask_round (__B, __C,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t__A, __D);\n+  return __builtin_ia32_subsh_mask_round (__B, __C,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  __A, __D);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mul_round_sh (__m128h __A, __m128h __B, const int __C)\n {\n-  return __builtin_ia32_vmulsh_v8hf_mask_round (__A, __B,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t(__mmask8) -1, __C);\n+  return __builtin_ia32_mulsh_mask_round (__A, __B,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  (__mmask8) -1, __C);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_mul_round_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t       __m128h __D, const int __E)\n {\n-  return __builtin_ia32_vmulsh_v8hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_mulsh_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_mul_round_sh (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\tconst int __D)\n {\n-  return __builtin_ia32_vmulsh_v8hf_mask_round (__B, __C,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t__A, __D);\n+  return __builtin_ia32_mulsh_mask_round (__B, __C,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  __A, __D);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_div_round_sh (__m128h __A, __m128h __B, const int __C)\n {\n-  return __builtin_ia32_vdivsh_v8hf_mask_round (__A, __B,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t(__mmask8) -1, __C);\n+  return __builtin_ia32_divsh_mask_round (__A, __B,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  (__mmask8) -1, __C);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_div_round_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t       __m128h __D, const int __E)\n {\n-  return __builtin_ia32_vdivsh_v8hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_divsh_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_div_round_sh (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\tconst int __D)\n {\n-  return __builtin_ia32_vdivsh_v8hf_mask_round (__B, __C,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t__A, __D);\n+  return __builtin_ia32_divsh_mask_round (__B, __C,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  __A, __D);\n }\n #else\n #define _mm_add_round_sh(A, B, C)\t\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vaddsh_v8hf_mask_round ((A), (B),\t\t\\\n-\t\t\t\t\t\t   _mm_setzero_ph (),\t\\\n-\t\t\t\t\t\t   (__mmask8)-1, (C)))\n+  ((__m128h)__builtin_ia32_addsh_mask_round ((A), (B),\t\t\t\\\n+\t\t\t\t\t     _mm_setzero_ph (),\t\t\\\n+\t\t\t\t\t     (__mmask8)-1, (C)))\n \n #define _mm_mask_add_round_sh(A, B, C, D, E)\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vaddsh_v8hf_mask_round ((C), (D), (A), (B), (E)))\n+  ((__m128h)__builtin_ia32_addsh_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm_maskz_add_round_sh(A, B, C, D)\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vaddsh_v8hf_mask_round ((B), (C),\t\t\\\n-\t\t\t\t\t\t   _mm_setzero_ph (),\t\\\n-\t\t\t\t\t\t   (A), (D)))\n+#define _mm_maskz_add_round_sh(A, B, C, D)\t\t\t\\\n+  ((__m128h)__builtin_ia32_addsh_mask_round ((B), (C),\t\t\\\n+\t\t\t\t\t     _mm_setzero_ph (),\t\\\n+\t\t\t\t\t     (A), (D)))\n \n #define _mm_sub_round_sh(A, B, C)\t\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vsubsh_v8hf_mask_round ((A), (B),\t\t\\\n-\t\t\t\t\t\t   _mm_setzero_ph (),\t\\\n-\t\t\t\t\t\t   (__mmask8)-1, (C)))\n+  ((__m128h)__builtin_ia32_subsh_mask_round ((A), (B),\t\t\t\\\n+\t\t\t\t\t     _mm_setzero_ph (),\t\t\\\n+\t\t\t\t\t     (__mmask8)-1, (C)))\n \n #define _mm_mask_sub_round_sh(A, B, C, D, E)\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vsubsh_v8hf_mask_round ((C), (D), (A), (B), (E)))\n+  ((__m128h)__builtin_ia32_subsh_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm_maskz_sub_round_sh(A, B, C, D)\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vsubsh_v8hf_mask_round ((B), (C),\t\t\\\n-\t\t\t\t\t\t   _mm_setzero_ph (),\t\\\n-\t\t\t\t\t\t   (A), (D)))\n+#define _mm_maskz_sub_round_sh(A, B, C, D)\t\t\t\\\n+  ((__m128h)__builtin_ia32_subsh_mask_round ((B), (C),\t\t\\\n+\t\t\t\t\t     _mm_setzero_ph (),\t\\\n+\t\t\t\t\t     (A), (D)))\n \n #define _mm_mul_round_sh(A, B, C)\t\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vmulsh_v8hf_mask_round ((A), (B),\t\t\\\n-\t\t\t\t\t\t   _mm_setzero_ph (),\t\\\n-\t\t\t\t\t\t   (__mmask8)-1, (C)))\n+  ((__m128h)__builtin_ia32_mulsh_mask_round ((A), (B),\t\t\t\\\n+\t\t\t\t\t     _mm_setzero_ph (),\t\t\\\n+\t\t\t\t\t     (__mmask8)-1, (C)))\n \n #define _mm_mask_mul_round_sh(A, B, C, D, E)\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vmulsh_v8hf_mask_round ((C), (D), (A), (B), (E)))\n+  ((__m128h)__builtin_ia32_mulsh_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm_maskz_mul_round_sh(A, B, C, D)\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vmulsh_v8hf_mask_round ((B), (C),\t\t\\\n-\t\t\t\t\t\t   _mm_setzero_ph (),\t\\\n-\t\t\t\t\t\t   (A), (D)))\n+#define _mm_maskz_mul_round_sh(A, B, C, D)\t\t\t\\\n+  ((__m128h)__builtin_ia32_mulsh_mask_round ((B), (C),\t\t\\\n+\t\t\t\t\t     _mm_setzero_ph (),\t\\\n+\t\t\t\t\t     (A), (D)))\n \n #define _mm_div_round_sh(A, B, C)\t\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vdivsh_v8hf_mask_round ((A), (B),\t\t\\\n-\t\t\t\t\t\t   _mm_setzero_ph (),\t\\\n-\t\t\t\t\t\t   (__mmask8)-1, (C)))\n+  ((__m128h)__builtin_ia32_divsh_mask_round ((A), (B),\t\t\t\\\n+\t\t\t\t\t     _mm_setzero_ph (),\t\t\\\n+\t\t\t\t\t     (__mmask8)-1, (C)))\n \n #define _mm_mask_div_round_sh(A, B, C, D, E)\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vdivsh_v8hf_mask_round ((C), (D), (A), (B), (E)))\n+  ((__m128h)__builtin_ia32_divsh_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm_maskz_div_round_sh(A, B, C, D)\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_vdivsh_v8hf_mask_round ((B), (C),\t\t\\\n-\t\t\t\t\t\t   _mm_setzero_ph (),\t\\\n-\t\t\t\t\t\t   (A), (D)))\n+#define _mm_maskz_div_round_sh(A, B, C, D)\t\t\t\\\n+  ((__m128h)__builtin_ia32_divsh_mask_round ((B), (C),\t\t\\\n+\t\t\t\t\t     _mm_setzero_ph (),\t\\\n+\t\t\t\t\t     (A), (D)))\n #endif /* __OPTIMIZE__ */\n \n /* Intrinsic vmaxph vminph.  */\n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_max_ph (__m512h __A, __m512h __B)\n {\n-  return __builtin_ia32_vmaxph_v32hf_mask (__A, __B,\n-\t\t\t\t\t   _mm512_setzero_ph (),\n-\t\t\t\t\t   (__mmask32) -1);\n+  return __builtin_ia32_maxph512_mask (__A, __B,\n+\t\t\t\t       _mm512_setzero_ph (),\n+\t\t\t\t       (__mmask32) -1);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_max_ph (__m512h __A, __mmask32 __B, __m512h __C, __m512h __D)\n {\n-  return __builtin_ia32_vmaxph_v32hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_maxph512_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_max_ph (__mmask32 __A, __m512h __B, __m512h __C)\n {\n-  return __builtin_ia32_vmaxph_v32hf_mask (__B, __C,\n-\t\t\t\t\t   _mm512_setzero_ph (), __A);\n+  return __builtin_ia32_maxph512_mask (__B, __C,\n+\t\t\t\t       _mm512_setzero_ph (), __A);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_min_ph (__m512h __A, __m512h __B)\n {\n-  return __builtin_ia32_vminph_v32hf_mask (__A, __B,\n-\t\t\t\t\t   _mm512_setzero_ph (),\n-\t\t\t\t\t   (__mmask32) -1);\n+  return __builtin_ia32_minph512_mask (__A, __B,\n+\t\t\t\t       _mm512_setzero_ph (),\n+\t\t\t\t       (__mmask32) -1);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_min_ph (__m512h __A, __mmask32 __B, __m512h __C, __m512h __D)\n {\n-  return __builtin_ia32_vminph_v32hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_minph512_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_min_ph (__mmask32 __A, __m512h __B, __m512h __C)\n {\n-  return __builtin_ia32_vminph_v32hf_mask (__B, __C,\n-\t\t\t\t\t   _mm512_setzero_ph (), __A);\n+  return __builtin_ia32_minph512_mask (__B, __C,\n+\t\t\t\t       _mm512_setzero_ph (), __A);\n }\n \n #ifdef __OPTIMIZE__\n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_max_round_ph (__m512h __A, __m512h __B, const int __C)\n {\n-  return __builtin_ia32_vmaxph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t (__mmask32) -1, __C);\n+  return __builtin_ia32_maxph512_mask_round (__A, __B,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     (__mmask32) -1, __C);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_max_round_ph (__m512h __A, __mmask32 __B, __m512h __C,\n \t\t\t  __m512h __D, const int __E)\n {\n-  return __builtin_ia32_vmaxph_v32hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_maxph512_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_max_round_ph (__mmask32 __A, __m512h __B, __m512h __C,\n \t\t\t   const int __D)\n {\n-  return __builtin_ia32_vmaxph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t __A, __D);\n+  return __builtin_ia32_maxph512_mask_round (__B, __C,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     __A, __D);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_min_round_ph (__m512h __A, __m512h __B, const int __C)\n {\n-  return __builtin_ia32_vminph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t (__mmask32) -1, __C);\n+  return __builtin_ia32_minph512_mask_round (__A, __B,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     (__mmask32) -1, __C);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_min_round_ph (__m512h __A, __mmask32 __B, __m512h __C,\n \t\t\t  __m512h __D, const int __E)\n {\n-  return __builtin_ia32_vminph_v32hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_minph512_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_min_round_ph (__mmask32 __A, __m512h __B, __m512h __C,\n \t\t\t   const int __D)\n {\n-  return __builtin_ia32_vminph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t _mm512_setzero_ph (),\n-\t\t\t\t\t\t __A, __D);\n+  return __builtin_ia32_minph512_mask_round (__B, __C,\n+\t\t\t\t\t     _mm512_setzero_ph (),\n+\t\t\t\t\t     __A, __D);\n }\n \n #else\n-#define _mm512_max_round_ph(A, B, C)\t\t\t\t\t\\\n-  (__builtin_ia32_vmaxph_v32hf_mask_round ((A), (B),\t\t\t\\\n-\t\t\t\t\t   _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t   (__mmask32)-1, (C)))\n+#define _mm512_max_round_ph(A, B, C)\t\t\t\t\\\n+  (__builtin_ia32_maxph512_mask_round ((A), (B),\t\t\\\n+\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t       (__mmask32)-1, (C)))\n \n #define _mm512_mask_max_round_ph(A, B, C, D, E)\t\t\t\t\\\n-  (__builtin_ia32_vmaxph_v32hf_mask_round ((C), (D), (A), (B), (E)))\n+  (__builtin_ia32_maxph512_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm512_maskz_max_round_ph(A, B, C, D)\t\t\t\t\\\n-  (__builtin_ia32_vmaxph_v32hf_mask_round ((B), (C),\t\t\t\\\n-\t\t\t\t\t   _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t   (A), (D)))\n+#define _mm512_maskz_max_round_ph(A, B, C, D)\t\t\t\\\n+  (__builtin_ia32_maxph512_mask_round ((B), (C),\t\t\\\n+\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t       (A), (D)))\n \n-#define _mm512_min_round_ph(A, B, C)\t\t\t\t\t\\\n-  (__builtin_ia32_vminph_v32hf_mask_round ((A), (B),\t\t\t\\\n-\t\t\t\t\t   _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t   (__mmask32)-1, (C)))\n+#define _mm512_min_round_ph(A, B, C)\t\t\t\t\\\n+  (__builtin_ia32_minph512_mask_round ((A), (B),\t\t\\\n+\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t       (__mmask32)-1, (C)))\n \n #define _mm512_mask_min_round_ph(A, B, C, D, E)\t\t\t\t\\\n-  (__builtin_ia32_vminph_v32hf_mask_round ((C), (D), (A), (B), (E)))\n+  (__builtin_ia32_minph512_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm512_maskz_min_round_ph(A, B, C, D)\t\t\t\t\\\n-  (__builtin_ia32_vminph_v32hf_mask_round ((B), (C),\t\t\t\\\n-\t\t\t\t\t   _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t   (A), (D)))\n+#define _mm512_maskz_min_round_ph(A, B, C, D)\t\t\t\\\n+  (__builtin_ia32_minph512_mask_round ((B), (C),\t\t\\\n+\t\t\t\t       _mm512_setzero_ph (),\t\\\n+\t\t\t\t       (A), (D)))\n #endif /* __OPTIMIZE__ */\n \n /* Intrinsic vmaxsh vminsh.  */\n@@ -867,15 +867,15 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_max_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vmaxsh_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_maxsh_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_max_sh (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vmaxsh_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_maxsh_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t    __A);\n }\n \n extern __inline __m128h\n@@ -890,98 +890,98 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_min_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vminsh_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_minsh_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_min_sh (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vminsh_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_minsh_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t    __A);\n }\n \n #ifdef __OPTIMIZE__\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_round_sh (__m128h __A, __m128h __B, const int __C)\n {\n-  return __builtin_ia32_vmaxsh_v8hf_mask_round (__A, __B,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t(__mmask8) -1, __C);\n+  return __builtin_ia32_maxsh_mask_round (__A, __B,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  (__mmask8) -1, __C);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_max_round_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t       __m128h __D, const int __E)\n {\n-  return __builtin_ia32_vmaxsh_v8hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_maxsh_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_max_round_sh (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\tconst int __D)\n {\n-  return __builtin_ia32_vmaxsh_v8hf_mask_round (__B, __C,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t__A, __D);\n+  return __builtin_ia32_maxsh_mask_round (__B, __C,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  __A, __D);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_round_sh (__m128h __A, __m128h __B, const int __C)\n {\n-  return __builtin_ia32_vminsh_v8hf_mask_round (__A, __B,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t(__mmask8) -1, __C);\n+  return __builtin_ia32_minsh_mask_round (__A, __B,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  (__mmask8) -1, __C);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_min_round_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t       __m128h __D, const int __E)\n {\n-  return __builtin_ia32_vminsh_v8hf_mask_round (__C, __D, __A, __B, __E);\n+  return __builtin_ia32_minsh_mask_round (__C, __D, __A, __B, __E);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_min_round_sh (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\tconst int __D)\n {\n-  return __builtin_ia32_vminsh_v8hf_mask_round (__B, __C,\n-\t\t\t\t\t\t_mm_setzero_ph (),\n-\t\t\t\t\t\t__A, __D);\n+  return __builtin_ia32_minsh_mask_round (__B, __C,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  __A, __D);\n }\n \n #else\n-#define _mm_max_round_sh(A, B, C)\t\t\t\t\t\\\n-  (__builtin_ia32_vmaxsh_v8hf_mask_round ((A), (B),\t\t\t\\\n-\t\t\t\t\t  _mm_setzero_ph (),\t\t\\\n-\t\t\t\t\t  (__mmask8)-1, (C)))\n+#define _mm_max_round_sh(A, B, C)\t\t\t\\\n+  (__builtin_ia32_maxsh_mask_round ((A), (B),\t\t\\\n+\t\t\t\t    _mm_setzero_ph (),\t\\\n+\t\t\t\t    (__mmask8)-1, (C)))\n \n-#define _mm_mask_max_round_sh(A, B, C, D, E)\t\t\t\t\\\n-  (__builtin_ia32_vmaxsh_v8hf_mask_round ((C), (D), (A), (B), (E)))\n+#define _mm_mask_max_round_sh(A, B, C, D, E)\t\t\t\\\n+  (__builtin_ia32_maxsh_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm_maskz_max_round_sh(A, B, C, D)\t\t\t\t\\\n-  (__builtin_ia32_vmaxsh_v8hf_mask_round ((B), (C),\t\t\t\\\n-\t\t\t\t\t  _mm_setzero_ph (),\t\t\\\n-\t\t\t\t\t  (A), (D)))\n+#define _mm_maskz_max_round_sh(A, B, C, D)\t\t\\\n+  (__builtin_ia32_maxsh_mask_round ((B), (C),\t\t\\\n+\t\t\t\t    _mm_setzero_ph (),\t\\\n+\t\t\t\t    (A), (D)))\n \n-#define _mm_min_round_sh(A, B, C)\t\t\t\t\t\\\n-  (__builtin_ia32_vminsh_v8hf_mask_round ((A), (B),\t\t\t\\\n-\t\t\t\t\t  _mm_setzero_ph (),\t\t\\\n-\t\t\t\t\t  (__mmask8)-1, (C)))\n+#define _mm_min_round_sh(A, B, C)\t\t\t\\\n+  (__builtin_ia32_minsh_mask_round ((A), (B),\t\t\\\n+\t\t\t\t    _mm_setzero_ph (),\t\\\n+\t\t\t\t    (__mmask8)-1, (C)))\n \n-#define _mm_mask_min_round_sh(A, B, C, D, E)\t\t\t\t\\\n-  (__builtin_ia32_vminsh_v8hf_mask_round ((C), (D), (A), (B), (E)))\n+#define _mm_mask_min_round_sh(A, B, C, D, E)\t\t\t\\\n+  (__builtin_ia32_minsh_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm_maskz_min_round_sh(A, B, C, D)\t\t\t\t\\\n-  (__builtin_ia32_vminsh_v8hf_mask_round ((B), (C),\t\t\t\\\n-\t\t\t\t\t  _mm_setzero_ph (),\t\t\\\n-\t\t\t\t\t  (A), (D)))\n+#define _mm_maskz_min_round_sh(A, B, C, D)\t\t\\\n+  (__builtin_ia32_minsh_mask_round ((B), (C),\t\t\\\n+\t\t\t\t    _mm_setzero_ph (),\t\\\n+\t\t\t\t    (A), (D)))\n \n #endif /* __OPTIMIZE__ */\n \n@@ -991,51 +991,51 @@ extern __inline __mmask32\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_cmp_ph_mask (__m512h __A, __m512h __B, const int __C)\n {\n-  return (__mmask32) __builtin_ia32_vcmpph_v32hf_mask (__A, __B, __C,\n-\t\t\t\t\t\t       (__mmask32) -1);\n+  return (__mmask32) __builtin_ia32_cmpph512_mask (__A, __B, __C,\n+\t\t\t\t\t\t   (__mmask32) -1);\n }\n \n extern __inline __mmask32\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_cmp_ph_mask (__mmask32 __A, __m512h __B, __m512h __C,\n \t\t\t const int __D)\n {\n-  return (__mmask32) __builtin_ia32_vcmpph_v32hf_mask (__B, __C, __D,\n-\t\t\t\t\t\t       __A);\n+  return (__mmask32) __builtin_ia32_cmpph512_mask (__B, __C, __D,\n+\t\t\t\t\t\t   __A);\n }\n \n extern __inline __mmask32\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_cmp_round_ph_mask (__m512h __A, __m512h __B, const int __C,\n \t\t\t  const int __D)\n {\n-  return (__mmask32) __builtin_ia32_vcmpph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t\t     __C, (__mmask32) -1,\n-\t\t\t\t\t\t\t     __D);\n+  return (__mmask32) __builtin_ia32_cmpph512_mask_round (__A, __B,\n+\t\t\t\t\t\t\t __C, (__mmask32) -1,\n+\t\t\t\t\t\t\t __D);\n }\n \n extern __inline __mmask32\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_cmp_round_ph_mask (__mmask32 __A, __m512h __B, __m512h __C,\n \t\t\t       const int __D, const int __E)\n {\n-  return (__mmask32) __builtin_ia32_vcmpph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t\t     __D, __A,\n-\t\t\t\t\t\t\t     __E);\n+  return (__mmask32) __builtin_ia32_cmpph512_mask_round (__B, __C,\n+\t\t\t\t\t\t\t __D, __A,\n+\t\t\t\t\t\t\t __E);\n }\n \n #else\n #define _mm512_cmp_ph_mask(A, B, C)\t\t\t\\\n-  (__builtin_ia32_vcmpph_v32hf_mask ((A), (B), (C), (-1)))\n+  (__builtin_ia32_cmpph512_mask ((A), (B), (C), (-1)))\n \n #define _mm512_mask_cmp_ph_mask(A, B, C, D)\t\t\\\n-  (__builtin_ia32_vcmpph_v32hf_mask ((B), (C), (D), (A)))\n+  (__builtin_ia32_cmpph512_mask ((B), (C), (D), (A)))\n \n-#define _mm512_cmp_round_ph_mask(A, B, C, D)\t\t\\\n-  (__builtin_ia32_vcmpph_v32hf_mask_round ((A), (B), (C), (-1), (D)))\n+#define _mm512_cmp_round_ph_mask(A, B, C, D)\t\t\t\t\\\n+  (__builtin_ia32_cmpph512_mask_round ((A), (B), (C), (-1), (D)))\n \n-#define _mm512_mask_cmp_round_ph_mask(A, B, C, D, E)\t\\\n-  (__builtin_ia32_vcmpph_v32hf_mask_round ((B), (C), (D), (A), (E)))\n+#define _mm512_mask_cmp_round_ph_mask(A, B, C, D, E)\t\t\t\\\n+  (__builtin_ia32_cmpph512_mask_round ((B), (C), (D), (A), (E)))\n \n #endif /* __OPTIMIZE__ */\n \n@@ -1046,9 +1046,9 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmp_sh_mask (__m128h __A, __m128h __B, const int __C)\n {\n   return (__mmask8)\n-    __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B,\n-\t\t\t\t\t   __C, (__mmask8) -1,\n-\t\t\t\t\t   _MM_FROUND_CUR_DIRECTION);\n+    __builtin_ia32_cmpsh_mask_round (__A, __B,\n+\t\t\t\t     __C, (__mmask8) -1,\n+\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __mmask8\n@@ -1057,45 +1057,45 @@ _mm_mask_cmp_sh_mask (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t      const int __D)\n {\n   return (__mmask8)\n-    __builtin_ia32_vcmpsh_v8hf_mask_round (__B, __C,\n-\t\t\t\t\t   __D, __A,\n-\t\t\t\t\t   _MM_FROUND_CUR_DIRECTION);\n+    __builtin_ia32_cmpsh_mask_round (__B, __C,\n+\t\t\t\t     __D, __A,\n+\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __mmask8\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmp_round_sh_mask (__m128h __A, __m128h __B, const int __C,\n \t\t       const int __D)\n {\n-  return (__mmask8) __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B,\n-\t\t\t\t\t\t\t   __C, (__mmask8) -1,\n-\t\t\t\t\t\t\t   __D);\n+  return (__mmask8) __builtin_ia32_cmpsh_mask_round (__A, __B,\n+\t\t\t\t\t\t     __C, (__mmask8) -1,\n+\t\t\t\t\t\t     __D);\n }\n \n extern __inline __mmask8\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_cmp_round_sh_mask (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\t    const int __D, const int __E)\n {\n-  return (__mmask8) __builtin_ia32_vcmpsh_v8hf_mask_round (__B, __C,\n-\t\t\t\t\t\t\t   __D, __A,\n-\t\t\t\t\t\t\t   __E);\n+  return (__mmask8) __builtin_ia32_cmpsh_mask_round (__B, __C,\n+\t\t\t\t\t\t     __D, __A,\n+\t\t\t\t\t\t     __E);\n }\n \n #else\n-#define _mm_cmp_sh_mask(A, B, C)\t\t\\\n-  (__builtin_ia32_vcmpsh_v8hf_mask_round ((A), (B), (C), (-1), \\\n-\t\t\t\t\t  (_MM_FROUND_CUR_DIRECTION)))\n+#define _mm_cmp_sh_mask(A, B, C)\t\t\t\t\t\\\n+  (__builtin_ia32_cmpsh_mask_round ((A), (B), (C), (-1),\t\t\\\n+\t\t\t\t    (_MM_FROUND_CUR_DIRECTION)))\n \n-#define _mm_mask_cmp_sh_mask(A, B, C, D)\t\\\n-  (__builtin_ia32_vcmpsh_v8hf_mask_round ((B), (C), (D), (A),\t\t\\\n-\t\t\t\t\t  (_MM_FROUND_CUR_DIRECTION)))\n+#define _mm_mask_cmp_sh_mask(A, B, C, D)\t\t\t\t\\\n+  (__builtin_ia32_cmpsh_mask_round ((B), (C), (D), (A),\t\t\t\\\n+\t\t\t\t    (_MM_FROUND_CUR_DIRECTION)))\n \n-#define _mm_cmp_round_sh_mask(A, B, C, D)\t\t\t\t\\\n-  (__builtin_ia32_vcmpsh_v8hf_mask_round ((A), (B), (C), (-1), (D)))\n+#define _mm_cmp_round_sh_mask(A, B, C, D)\t\t\t\\\n+  (__builtin_ia32_cmpsh_mask_round ((A), (B), (C), (-1), (D)))\n \n-#define _mm_mask_cmp_round_sh_mask(A, B, C, D, E)\t\\\n-  (__builtin_ia32_vcmpsh_v8hf_mask_round ((B), (C), (D), (A), (E)))\n+#define _mm_mask_cmp_round_sh_mask(A, B, C, D, E)\t\t\\\n+  (__builtin_ia32_cmpsh_mask_round ((B), (C), (D), (A), (E)))\n \n #endif /* __OPTIMIZE__ */\n \n@@ -1104,134 +1104,134 @@ extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comieq_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_EQ_OS,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_EQ_OS,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comilt_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_LT_OS,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_LT_OS,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comile_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_LE_OS,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_LE_OS,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comigt_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_GT_OS,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_GT_OS,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comige_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_GE_OS,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_GE_OS,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comineq_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_NEQ_US,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_NEQ_US,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomieq_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_EQ_OQ,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_EQ_OQ,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomilt_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_LT_OQ,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_LT_OQ,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomile_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_LE_OQ,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_LE_OQ,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomigt_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_GT_OQ,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_GT_OQ,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomige_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_GE_OQ,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_GE_OQ,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomineq_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, _CMP_NEQ_UQ,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, _CMP_NEQ_UQ,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n #ifdef __OPTIMIZE__\n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n-  _mm_comi_sh (__m128h __A, __m128h __B, const int __P)\n+_mm_comi_sh (__m128h __A, __m128h __B, const int __P)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, __P,\n-\t\t\t\t\t\t(__mmask8) -1,\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, __P,\n+\t\t\t\t\t  (__mmask8) -1,\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline int\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comi_round_sh (__m128h __A, __m128h __B, const int __P, const int __R)\n {\n-  return __builtin_ia32_vcmpsh_v8hf_mask_round (__A, __B, __P,\n-\t\t\t\t\t\t(__mmask8) -1,__R);\n+  return __builtin_ia32_cmpsh_mask_round (__A, __B, __P,\n+\t\t\t\t\t  (__mmask8) -1,__R);\n }\n \n #else\n-#define _mm_comi_round_sh(A, B, P, R)\t\t\\\n-  (__builtin_ia32_vcmpsh_v8hf_mask_round ((A), (B), (P), (__mmask8) (-1), (R)))\n-#define _mm_comi_sh(A, B, P)\t\t\\\n-  (__builtin_ia32_vcmpsh_v8hf_mask_round ((A), (B), (P), (__mmask8) (-1), \\\n-\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION))\n+#define _mm_comi_round_sh(A, B, P, R)\t\t\t\t\t\\\n+  (__builtin_ia32_cmpsh_mask_round ((A), (B), (P), (__mmask8) (-1), (R)))\n+#define _mm_comi_sh(A, B, P)\t\t\t\t\t\t\\\n+  (__builtin_ia32_cmpsh_mask_round ((A), (B), (P), (__mmask8) (-1),\t\\\n+\t\t\t\t    _MM_FROUND_CUR_DIRECTION))\n \n #endif /* __OPTIMIZE__  */\n \n@@ -1240,70 +1240,70 @@ extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_sqrt_ph (__m512h __A)\n {\n-  return __builtin_ia32_vsqrtph_v32hf_mask_round (__A,\n-\t\t\t\t\t\t  _mm512_setzero_ph(),\n-\t\t\t\t\t\t  (__mmask32) -1,\n-\t\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_sqrtph512_mask_round (__A,\n+\t\t\t\t\t      _mm512_setzero_ph(),\n+\t\t\t\t\t      (__mmask32) -1,\n+\t\t\t\t\t      _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_sqrt_ph (__m512h __A, __mmask32 __B, __m512h __C)\n {\n-  return __builtin_ia32_vsqrtph_v32hf_mask_round (__C, __A, __B,\n-\t\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_sqrtph512_mask_round (__C, __A, __B,\n+\t\t\t\t\t      _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_sqrt_ph (__mmask32 __A, __m512h __B)\n {\n-  return __builtin_ia32_vsqrtph_v32hf_mask_round (__B,\n-\t\t\t\t\t\t  _mm512_setzero_ph (),\n-\t\t\t\t\t\t  __A,\n-\t\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_sqrtph512_mask_round (__B,\n+\t\t\t\t\t      _mm512_setzero_ph (),\n+\t\t\t\t\t      __A,\n+\t\t\t\t\t      _MM_FROUND_CUR_DIRECTION);\n }\n \n #ifdef __OPTIMIZE__\n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_sqrt_round_ph (__m512h __A, const int __B)\n {\n-  return __builtin_ia32_vsqrtph_v32hf_mask_round (__A,\n-\t\t\t\t\t\t  _mm512_setzero_ph(),\n-\t\t\t\t\t\t  (__mmask32) -1, __B);\n+  return __builtin_ia32_sqrtph512_mask_round (__A,\n+\t\t\t\t\t      _mm512_setzero_ph(),\n+\t\t\t\t\t      (__mmask32) -1, __B);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_sqrt_round_ph (__m512h __A, __mmask32 __B, __m512h __C,\n \t\t\t   const int __D)\n {\n-  return __builtin_ia32_vsqrtph_v32hf_mask_round (__C, __A, __B, __D);\n+  return __builtin_ia32_sqrtph512_mask_round (__C, __A, __B, __D);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_sqrt_round_ph (__mmask32 __A, __m512h __B, const int __C)\n {\n-  return __builtin_ia32_vsqrtph_v32hf_mask_round (__B,\n-\t\t\t\t\t\t  _mm512_setzero_ph (),\n-\t\t\t\t\t\t  __A, __C);\n+  return __builtin_ia32_sqrtph512_mask_round (__B,\n+\t\t\t\t\t      _mm512_setzero_ph (),\n+\t\t\t\t\t      __A, __C);\n }\n \n #else\n-#define _mm512_sqrt_round_ph(A, B)\t\t\t\t\t\\\n-  (__builtin_ia32_vsqrtph_v32hf_mask_round ((A),\t\t\t\\\n-\t\t\t\t\t    _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t    (__mmask32)-1, (B)))\n+#define _mm512_sqrt_round_ph(A, B)\t\t\t\t\\\n+  (__builtin_ia32_sqrtph512_mask_round ((A),\t\t\t\\\n+\t\t\t\t\t_mm512_setzero_ph (),\t\\\n+\t\t\t\t\t(__mmask32)-1, (B)))\n \n-#define _mm512_mask_sqrt_round_ph(A, B, C, D)\t\t\t\t\\\n-  (__builtin_ia32_vsqrtph_v32hf_mask_round ((C), (A), (B), (D)))\n+#define _mm512_mask_sqrt_round_ph(A, B, C, D)\t\t\t\\\n+  (__builtin_ia32_sqrtph512_mask_round ((C), (A), (B), (D)))\n \n-#define _mm512_maskz_sqrt_round_ph(A, B, C)\t\t\t\t\\\n-  (__builtin_ia32_vsqrtph_v32hf_mask_round ((B),\t\t\t\\\n-\t\t\t\t\t    _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t    (A), (C)))\n+#define _mm512_maskz_sqrt_round_ph(A, B, C)\t\t\t\\\n+  (__builtin_ia32_sqrtph512_mask_round ((B),\t\t\t\\\n+\t\t\t\t\t_mm512_setzero_ph (),\t\\\n+\t\t\t\t\t(A), (C)))\n \n #endif /* __OPTIMIZE__ */\n \n@@ -1312,119 +1312,119 @@ extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_rsqrt_ph (__m512h __A)\n {\n-  return __builtin_ia32_vrsqrtph_v32hf_mask (__A, _mm512_setzero_ph (),\n-\t\t\t\t\t     (__mmask32) -1);\n+  return __builtin_ia32_rsqrtph512_mask (__A, _mm512_setzero_ph (),\n+\t\t\t\t\t (__mmask32) -1);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_rsqrt_ph (__m512h __A, __mmask32 __B, __m512h __C)\n {\n-  return __builtin_ia32_vrsqrtph_v32hf_mask (__C, __A, __B);\n+  return __builtin_ia32_rsqrtph512_mask (__C, __A, __B);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_rsqrt_ph (__mmask32 __A, __m512h __B)\n {\n-  return __builtin_ia32_vrsqrtph_v32hf_mask (__B, _mm512_setzero_ph (),\n-\t\t\t\t\t     __A);\n+  return __builtin_ia32_rsqrtph512_mask (__B, _mm512_setzero_ph (),\n+\t\t\t\t\t __A);\n }\n \n /* Intrinsics vrsqrtsh.  */\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rsqrt_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vrsqrtsh_v8hf_mask (__B, __A, _mm_setzero_ph (),\n-\t\t\t\t\t    (__mmask8) -1);\n+  return __builtin_ia32_rsqrtsh_mask (__B, __A, _mm_setzero_ph (),\n+\t\t\t\t      (__mmask8) -1);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_rsqrt_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vrsqrtsh_v8hf_mask (__D, __C, __A, __B);\n+  return __builtin_ia32_rsqrtsh_mask (__D, __C, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_rsqrt_sh (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vrsqrtsh_v8hf_mask (__C, __B, _mm_setzero_ph (),\n-\t\t\t\t\t    __A);\n+  return __builtin_ia32_rsqrtsh_mask (__C, __B, _mm_setzero_ph (),\n+\t\t\t\t      __A);\n }\n \n /* Intrinsics vsqrtsh.  */\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sqrt_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vsqrtsh_v8hf_mask_round (__B, __A,\n-\t\t\t\t\t\t _mm_setzero_ph (),\n-\t\t\t\t\t\t (__mmask8) -1,\n-\t\t\t\t\t\t _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_sqrtsh_mask_round (__B, __A,\n+\t\t\t\t\t   _mm_setzero_ph (),\n+\t\t\t\t\t   (__mmask8) -1,\n+\t\t\t\t\t   _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_sqrt_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vsqrtsh_v8hf_mask_round (__D, __C, __A, __B,\n-\t\t\t\t\t\t _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_sqrtsh_mask_round (__D, __C, __A, __B,\n+\t\t\t\t\t   _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_sqrt_sh (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vsqrtsh_v8hf_mask_round (__C, __B,\n-\t\t\t\t\t\t _mm_setzero_ph (),\n-\t\t\t\t\t\t __A, _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_sqrtsh_mask_round (__C, __B,\n+\t\t\t\t\t   _mm_setzero_ph (),\n+\t\t\t\t\t   __A, _MM_FROUND_CUR_DIRECTION);\n }\n \n #ifdef __OPTIMIZE__\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sqrt_round_sh (__m128h __A, __m128h __B, const int __C)\n {\n-  return __builtin_ia32_vsqrtsh_v8hf_mask_round (__B, __A,\n-\t\t\t\t\t\t _mm_setzero_ph (),\n-\t\t\t\t\t\t (__mmask8) -1, __C);\n+  return __builtin_ia32_sqrtsh_mask_round (__B, __A,\n+\t\t\t\t\t   _mm_setzero_ph (),\n+\t\t\t\t\t   (__mmask8) -1, __C);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_sqrt_round_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t\t__m128h __D, const int __E)\n {\n-  return __builtin_ia32_vsqrtsh_v8hf_mask_round (__D, __C, __A, __B,\n-\t\t\t\t\t\t __E);\n+  return __builtin_ia32_sqrtsh_mask_round (__D, __C, __A, __B,\n+\t\t\t\t\t   __E);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_sqrt_round_sh (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\t const int __D)\n {\n-  return __builtin_ia32_vsqrtsh_v8hf_mask_round (__C, __B,\n-\t\t\t\t\t\t _mm_setzero_ph (),\n-\t\t\t\t\t\t __A, __D);\n+  return __builtin_ia32_sqrtsh_mask_round (__C, __B,\n+\t\t\t\t\t   _mm_setzero_ph (),\n+\t\t\t\t\t   __A, __D);\n }\n \n #else\n #define _mm_sqrt_round_sh(A, B, C)\t\t\t\t\\\n-  (__builtin_ia32_vsqrtsh_v8hf_mask_round ((B), (A),\t\t\\\n-\t\t\t\t\t   _mm_setzero_ph (),\t\\\n-\t\t\t\t\t   (__mmask8)-1, (C)))\n+  (__builtin_ia32_sqrtsh_mask_round ((B), (A),\t\t\t\\\n+\t\t\t\t     _mm_setzero_ph (),\t\t\\\n+\t\t\t\t     (__mmask8)-1, (C)))\n \n #define _mm_mask_sqrt_round_sh(A, B, C, D, E)\t\t\t\\\n-  (__builtin_ia32_vsqrtsh_v8hf_mask_round ((D), (C), (A), (B), (E)))\n+  (__builtin_ia32_sqrtsh_mask_round ((D), (C), (A), (B), (E)))\n \n-#define _mm_maskz_sqrt_round_sh(A, B, C, D)\t\t\t\\\n-  (__builtin_ia32_vsqrtsh_v8hf_mask_round ((C), (B),\t\t\\\n-\t\t\t\t\t   _mm_setzero_ph (),\t\\\n-\t\t\t\t\t   (A), (D)))\n+#define _mm_maskz_sqrt_round_sh(A, B, C, D)\t\t\\\n+  (__builtin_ia32_sqrtsh_mask_round ((C), (B),\t\t\\\n+\t\t\t\t     _mm_setzero_ph (),\t\\\n+\t\t\t\t     (A), (D)))\n \n #endif /* __OPTIMIZE__ */\n \n@@ -1433,120 +1433,120 @@ extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_rcp_ph (__m512h __A)\n {\n-  return __builtin_ia32_vrcpph_v32hf_mask (__A, _mm512_setzero_ph (),\n-\t\t\t\t\t   (__mmask32) -1);\n+  return __builtin_ia32_rcpph512_mask (__A, _mm512_setzero_ph (),\n+\t\t\t\t       (__mmask32) -1);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_rcp_ph (__m512h __A, __mmask32 __B, __m512h __C)\n {\n-  return __builtin_ia32_vrcpph_v32hf_mask (__C, __A, __B);\n+  return __builtin_ia32_rcpph512_mask (__C, __A, __B);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_rcp_ph (__mmask32 __A, __m512h __B)\n {\n-  return __builtin_ia32_vrcpph_v32hf_mask (__B, _mm512_setzero_ph (),\n-\t\t\t\t\t   __A);\n+  return __builtin_ia32_rcpph512_mask (__B, _mm512_setzero_ph (),\n+\t\t\t\t       __A);\n }\n \n /* Intrinsics vrcpsh.  */\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rcp_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vrcpsh_v8hf_mask (__B, __A, _mm_setzero_ph (),\n-\t\t\t\t\t  (__mmask8) -1);\n+  return __builtin_ia32_rcpsh_mask (__B, __A, _mm_setzero_ph (),\n+\t\t\t\t    (__mmask8) -1);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_rcp_sh (__m128h __A, __mmask32 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vrcpsh_v8hf_mask (__D, __C, __A, __B);\n+  return __builtin_ia32_rcpsh_mask (__D, __C, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_rcp_sh (__mmask32 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vrcpsh_v8hf_mask (__C, __B, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_rcpsh_mask (__C, __B, _mm_setzero_ph (),\n+\t\t\t\t    __A);\n }\n \n /* Intrinsics vscalefph.  */\n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_scalef_ph (__m512h __A, __m512h __B)\n {\n-  return __builtin_ia32_vscalefph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t    _mm512_setzero_ph (),\n-\t\t\t\t\t\t    (__mmask32) -1,\n-\t\t\t\t\t\t    _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_scalefph512_mask_round (__A, __B,\n+\t\t\t\t\t\t_mm512_setzero_ph (),\n+\t\t\t\t\t\t(__mmask32) -1,\n+\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_scalef_ph (__m512h __A, __mmask32 __B, __m512h __C, __m512h __D)\n {\n-  return __builtin_ia32_vscalefph_v32hf_mask_round (__C, __D, __A, __B,\n-\t\t\t\t\t\t    _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_scalefph512_mask_round (__C, __D, __A, __B,\n+\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_scalef_ph (__mmask32 __A, __m512h __B, __m512h __C)\n {\n-  return __builtin_ia32_vscalefph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t    _mm512_setzero_ph (),\n-\t\t\t\t\t\t    __A,\n-\t\t\t\t\t\t    _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_scalefph512_mask_round (__B, __C,\n+\t\t\t\t\t\t_mm512_setzero_ph (),\n+\t\t\t\t\t\t__A,\n+\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n }\n \n #ifdef __OPTIMIZE__\n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_scalef_round_ph (__m512h __A, __m512h __B, const int __C)\n {\n-  return __builtin_ia32_vscalefph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t    _mm512_setzero_ph (),\n-\t\t\t\t\t\t    (__mmask32) -1, __C);\n+  return __builtin_ia32_scalefph512_mask_round (__A, __B,\n+\t\t\t\t\t\t_mm512_setzero_ph (),\n+\t\t\t\t\t\t(__mmask32) -1, __C);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_scalef_round_ph (__m512h __A, __mmask32 __B, __m512h __C,\n \t\t\t     __m512h __D, const int __E)\n {\n-  return __builtin_ia32_vscalefph_v32hf_mask_round (__C, __D, __A, __B,\n-\t\t\t\t\t\t    __E);\n+  return __builtin_ia32_scalefph512_mask_round (__C, __D, __A, __B,\n+\t\t\t\t\t\t__E);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_scalef_round_ph (__mmask32 __A, __m512h __B, __m512h __C,\n \t\t\t      const int __D)\n {\n-  return __builtin_ia32_vscalefph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t    _mm512_setzero_ph (),\n-\t\t\t\t\t\t    __A, __D);\n+  return __builtin_ia32_scalefph512_mask_round (__B, __C,\n+\t\t\t\t\t\t_mm512_setzero_ph (),\n+\t\t\t\t\t\t__A, __D);\n }\n \n #else\n-#define _mm512_scalef_round_ph(A, B, C)\t\t\t\t\t\\\n-  (__builtin_ia32_vscalefph_v32hf_mask_round ((A), (B),\t\t\t\\\n-\t\t\t\t\t      _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t      (__mmask32)-1, (C)))\n+#define _mm512_scalef_round_ph(A, B, C)\t\t\t\t\\\n+  (__builtin_ia32_scalefph512_mask_round ((A), (B),\t\t\\\n+\t\t\t\t\t  _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t  (__mmask32)-1, (C)))\n \n #define _mm512_mask_scalef_round_ph(A, B, C, D, E)\t\t\t\\\n-  (__builtin_ia32_vscalefph_v32hf_mask_round ((C), (D), (A), (B), (E)))\n+  (__builtin_ia32_scalefph512_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm512_maskz_scalef_round_ph(A, B, C, D)\t\t\t\\\n-  (__builtin_ia32_vscalefph_v32hf_mask_round ((B), (C),\t\t\t\\\n-\t\t\t\t\t      _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t      (A), (D)))\n+#define _mm512_maskz_scalef_round_ph(A, B, C, D)\t\t\\\n+  (__builtin_ia32_scalefph512_mask_round ((B), (C),\t\t\\\n+\t\t\t\t\t  _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t  (A), (D)))\n \n #endif  /* __OPTIMIZE__ */\n \n@@ -1555,71 +1555,71 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_scalef_sh (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vscalefsh_v8hf_mask_round (__A, __B,\n-\t\t\t\t\t\t   _mm_setzero_ph (),\n-\t\t\t\t\t\t   (__mmask8) -1,\n-\t\t\t\t\t\t   _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_scalefsh_mask_round (__A, __B,\n+\t\t\t\t\t     _mm_setzero_ph (),\n+\t\t\t\t\t     (__mmask8) -1,\n+\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_scalef_sh (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vscalefsh_v8hf_mask_round (__C, __D, __A, __B,\n-\t\t\t\t\t\t   _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_scalefsh_mask_round (__C, __D, __A, __B,\n+\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_scalef_sh (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vscalefsh_v8hf_mask_round (__B, __C,\n-\t\t\t\t\t\t   _mm_setzero_ph (),\n-\t\t\t\t\t\t   __A,\n-\t\t\t\t\t\t   _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_scalefsh_mask_round (__B, __C,\n+\t\t\t\t\t     _mm_setzero_ph (),\n+\t\t\t\t\t     __A,\n+\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n }\n \n #ifdef __OPTIMIZE__\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_scalef_round_sh (__m128h __A, __m128h __B, const int __C)\n {\n-  return __builtin_ia32_vscalefsh_v8hf_mask_round (__A, __B,\n-\t\t\t\t\t\t   _mm_setzero_ph (),\n-\t\t\t\t\t\t   (__mmask8) -1, __C);\n+  return __builtin_ia32_scalefsh_mask_round (__A, __B,\n+\t\t\t\t\t     _mm_setzero_ph (),\n+\t\t\t\t\t     (__mmask8) -1, __C);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_scalef_round_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t\t  __m128h __D, const int __E)\n {\n-  return __builtin_ia32_vscalefsh_v8hf_mask_round (__C, __D, __A, __B,\n-\t\t\t\t\t\t   __E);\n+  return __builtin_ia32_scalefsh_mask_round (__C, __D, __A, __B,\n+\t\t\t\t\t     __E);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_scalef_round_sh (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\t   const int __D)\n {\n-  return __builtin_ia32_vscalefsh_v8hf_mask_round (__B, __C,\n-\t\t\t\t\t\t   _mm_setzero_ph (),\n-\t\t\t\t\t\t   __A, __D);\n+  return __builtin_ia32_scalefsh_mask_round (__B, __C,\n+\t\t\t\t\t     _mm_setzero_ph (),\n+\t\t\t\t\t     __A, __D);\n }\n \n #else\n-#define _mm_scalef_round_sh(A, B, C)\t\t\t\t\t  \\\n-  (__builtin_ia32_vscalefsh_v8hf_mask_round ((A), (B),\t\t\t  \\\n-\t\t\t\t\t     _mm_setzero_ph (),\t\t  \\\n-\t\t\t\t\t     (__mmask8)-1, (C)))\n+#define _mm_scalef_round_sh(A, B, C)\t\t\t\t\\\n+  (__builtin_ia32_scalefsh_mask_round ((A), (B),\t\t\\\n+\t\t\t\t       _mm_setzero_ph (),\t\\\n+\t\t\t\t       (__mmask8)-1, (C)))\n \n-#define _mm_mask_scalef_round_sh(A, B, C, D, E)\t\t\t\t  \\\n-  (__builtin_ia32_vscalefsh_v8hf_mask_round ((C), (D), (A), (B), (E)))\n+#define _mm_mask_scalef_round_sh(A, B, C, D, E)\t\t\t\t\\\n+  (__builtin_ia32_scalefsh_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm_maskz_scalef_round_sh(A, B, C, D)\t\t\t\t  \\\n-  (__builtin_ia32_vscalefsh_v8hf_mask_round ((B), (C), _mm_setzero_ph (), \\\n-\t\t\t\t\t     (A), (D)))\n+#define _mm_maskz_scalef_round_sh(A, B, C, D)\t\t\t\t\\\n+  (__builtin_ia32_scalefsh_mask_round ((B), (C), _mm_setzero_ph (),\t\\\n+\t\t\t\t       (A), (D)))\n \n #endif /* __OPTIMIZE__ */\n \n@@ -1629,86 +1629,86 @@ extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_reduce_ph (__m512h __A, int __B)\n {\n-  return __builtin_ia32_vreduceph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t    _mm512_setzero_ph (),\n-\t\t\t\t\t\t    (__mmask32) -1,\n-\t\t\t\t\t\t    _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_reduceph512_mask_round (__A, __B,\n+\t\t\t\t\t\t_mm512_setzero_ph (),\n+\t\t\t\t\t\t(__mmask32) -1,\n+\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_reduce_ph (__m512h __A, __mmask32 __B, __m512h __C, int __D)\n {\n-  return __builtin_ia32_vreduceph_v32hf_mask_round (__C, __D, __A, __B,\n-\t\t\t\t\t\t    _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_reduceph512_mask_round (__C, __D, __A, __B,\n+\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_reduce_ph (__mmask32 __A, __m512h __B, int __C)\n {\n-  return __builtin_ia32_vreduceph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t    _mm512_setzero_ph (),\n-\t\t\t\t\t\t    __A,\n-\t\t\t\t\t\t    _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_reduceph512_mask_round (__B, __C,\n+\t\t\t\t\t\t_mm512_setzero_ph (),\n+\t\t\t\t\t\t__A,\n+\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_reduce_round_ph (__m512h __A, int __B, const int __C)\n {\n-  return __builtin_ia32_vreduceph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t    _mm512_setzero_ph (),\n-\t\t\t\t\t\t    (__mmask32) -1, __C);\n+  return __builtin_ia32_reduceph512_mask_round (__A, __B,\n+\t\t\t\t\t\t_mm512_setzero_ph (),\n+\t\t\t\t\t\t(__mmask32) -1, __C);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_reduce_round_ph (__m512h __A, __mmask32 __B, __m512h __C,\n \t\t\t     int __D, const int __E)\n {\n-  return __builtin_ia32_vreduceph_v32hf_mask_round (__C, __D, __A, __B,\n-\t\t\t\t\t\t    __E);\n+  return __builtin_ia32_reduceph512_mask_round (__C, __D, __A, __B,\n+\t\t\t\t\t\t__E);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_reduce_round_ph (__mmask32 __A, __m512h __B, int __C,\n \t\t\t      const int __D)\n {\n-  return __builtin_ia32_vreduceph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t    _mm512_setzero_ph (),\n-\t\t\t\t\t\t    __A, __D);\n+  return __builtin_ia32_reduceph512_mask_round (__B, __C,\n+\t\t\t\t\t\t_mm512_setzero_ph (),\n+\t\t\t\t\t\t__A, __D);\n }\n \n #else\n #define _mm512_reduce_ph(A, B)\t\t\t\t\t\t\\\n-  (__builtin_ia32_vreduceph_v32hf_mask_round ((A), (B),\t\t\t\\\n-\t\t\t\t\t      _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t      (__mmask32)-1,\t\t\\\n-\t\t\t\t\t      _MM_FROUND_CUR_DIRECTION))\n+  (__builtin_ia32_reduceph512_mask_round ((A), (B),\t\t\t\\\n+\t\t\t\t\t  _mm512_setzero_ph (),\t\t\\\n+\t\t\t\t\t  (__mmask32)-1,\t\t\\\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION))\n \n #define _mm512_mask_reduce_ph(A, B, C, D)\t\t\t\t\\\n-  (__builtin_ia32_vreduceph_v32hf_mask_round ((C), (D), (A), (B),\t\\\n-\t\t\t\t\t      _MM_FROUND_CUR_DIRECTION))\n+  (__builtin_ia32_reduceph512_mask_round ((C), (D), (A), (B),\t\t\\\n+\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION))\n \n #define _mm512_maskz_reduce_ph(A, B, C)\t\t\t\t\t\\\n-  (__builtin_ia32_vreduceph_v32hf_mask_round ((B), (C),\t\t\t\\\n-\t\t\t\t\t      _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t      (A), _MM_FROUND_CUR_DIRECTION))\n+  (__builtin_ia32_reduceph512_mask_round ((B), (C),\t\t\t\\\n+\t\t\t\t\t  _mm512_setzero_ph (),\t\t\\\n+\t\t\t\t\t  (A), _MM_FROUND_CUR_DIRECTION))\n \n-#define _mm512_reduce_round_ph(A, B, C)\t\t\t\t\t\\\n-  (__builtin_ia32_vreduceph_v32hf_mask_round ((A), (B),\t\t\t\\\n-\t\t\t\t\t      _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t      (__mmask32)-1, (C)))\n+#define _mm512_reduce_round_ph(A, B, C)\t\t\t\t\\\n+  (__builtin_ia32_reduceph512_mask_round ((A), (B),\t\t\\\n+\t\t\t\t\t  _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t  (__mmask32)-1, (C)))\n \n #define _mm512_mask_reduce_round_ph(A, B, C, D, E)\t\t\t\\\n-  (__builtin_ia32_vreduceph_v32hf_mask_round ((C), (D), (A), (B), (E)))\n+  (__builtin_ia32_reduceph512_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm512_maskz_reduce_round_ph(A, B, C, D)\t\t\t\\\n-  (__builtin_ia32_vreduceph_v32hf_mask_round ((B), (C),\t\t\t\\\n-\t\t\t\t\t      _mm512_setzero_ph (),\t\\\n-\t\t\t\t\t      (A), (D)))\n+#define _mm512_maskz_reduce_round_ph(A, B, C, D)\t\t\\\n+  (__builtin_ia32_reduceph512_mask_round ((B), (C),\t\t\\\n+\t\t\t\t\t  _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t  (A), (D)))\n \n #endif /* __OPTIMIZE__ */\n \n@@ -1718,274 +1718,274 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_reduce_sh (__m128h __A, __m128h __B, int __C)\n {\n-  return __builtin_ia32_vreducesh_v8hf_mask_round (__A, __B, __C,\n-\t\t\t\t\t\t   _mm_setzero_ph (),\n-\t\t\t\t\t\t   (__mmask8) -1,\n-\t\t\t\t\t\t   _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_reducesh_mask_round (__A, __B, __C,\n+\t\t\t\t\t     _mm_setzero_ph (),\n+\t\t\t\t\t     (__mmask8) -1,\n+\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_reduce_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t    __m128h __D, int __E)\n {\n-  return __builtin_ia32_vreducesh_v8hf_mask_round (__C, __D, __E, __A, __B,\n-\t\t\t\t\t\t   _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_reducesh_mask_round (__C, __D, __E, __A, __B,\n+\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_reduce_sh (__mmask8 __A, __m128h __B, __m128h __C, int __D)\n {\n-  return __builtin_ia32_vreducesh_v8hf_mask_round (__B, __C, __D,\n-\t\t\t\t\t\t   _mm_setzero_ph (), __A,\n-\t\t\t\t\t\t   _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_reducesh_mask_round (__B, __C, __D,\n+\t\t\t\t\t     _mm_setzero_ph (), __A,\n+\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_reduce_round_sh (__m128h __A, __m128h __B, int __C, const int __D)\n {\n-  return __builtin_ia32_vreducesh_v8hf_mask_round (__A, __B, __C,\n-\t\t\t\t\t\t   _mm_setzero_ph (),\n-\t\t\t\t\t\t   (__mmask8) -1, __D);\n+  return __builtin_ia32_reducesh_mask_round (__A, __B, __C,\n+\t\t\t\t\t     _mm_setzero_ph (),\n+\t\t\t\t\t     (__mmask8) -1, __D);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_reduce_round_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t\t  __m128h __D, int __E, const int __F)\n {\n-  return __builtin_ia32_vreducesh_v8hf_mask_round (__C, __D, __E, __A,\n-\t\t\t\t\t\t   __B, __F);\n+  return __builtin_ia32_reducesh_mask_round (__C, __D, __E, __A,\n+\t\t\t\t\t     __B, __F);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_reduce_round_sh (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\t   int __D, const int __E)\n {\n-  return __builtin_ia32_vreducesh_v8hf_mask_round (__B, __C, __D,\n-\t\t\t\t\t\t   _mm_setzero_ph (),\n-\t\t\t\t\t\t   __A, __E);\n+  return __builtin_ia32_reducesh_mask_round (__B, __C, __D,\n+\t\t\t\t\t     _mm_setzero_ph (),\n+\t\t\t\t\t     __A, __E);\n }\n \n #else\n #define _mm_reduce_sh(A, B, C)\t\t\t\t\t\t\\\n-  (__builtin_ia32_vreducesh_v8hf_mask_round ((A), (B), (C),\t\t\\\n-\t\t\t\t\t     _mm_setzero_ph (),\t\\\n-\t\t\t\t\t     (__mmask8)-1,\t\t\\\n-\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION))\n+  (__builtin_ia32_reducesh_mask_round ((A), (B), (C),\t\t\t\\\n+\t\t\t\t       _mm_setzero_ph (),\t\t\\\n+\t\t\t\t       (__mmask8)-1,\t\t\t\\\n+\t\t\t\t       _MM_FROUND_CUR_DIRECTION))\n \n #define _mm_mask_reduce_sh(A, B, C, D, E)\t\t\t\t\\\n-  (__builtin_ia32_vreducesh_v8hf_mask_round ((C), (D), (E), (A), (B),\t\\\n-\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION))\n+  (__builtin_ia32_reducesh_mask_round ((C), (D), (E), (A), (B),\t\t\\\n+\t\t\t\t       _MM_FROUND_CUR_DIRECTION))\n \n #define _mm_maskz_reduce_sh(A, B, C, D)\t\t\t\t\t\\\n-  (__builtin_ia32_vreducesh_v8hf_mask_round ((B), (C), (D),\t\t\\\n-\t\t\t\t\t     _mm_setzero_ph (),\t\\\n-\t\t\t\t\t     (A), _MM_FROUND_CUR_DIRECTION))\n+  (__builtin_ia32_reducesh_mask_round ((B), (C), (D),\t\t\t\\\n+\t\t\t\t       _mm_setzero_ph (),\t\t\\\n+\t\t\t\t       (A), _MM_FROUND_CUR_DIRECTION))\n \n #define _mm_reduce_round_sh(A, B, C, D)\t\t\t\t\\\n-  (__builtin_ia32_vreducesh_v8hf_mask_round ((A), (B), (C),\t\\\n-\t\t\t\t\t     _mm_setzero_ph (),\t\\\n-\t\t\t\t\t     (__mmask8)-1, (D)))\n+  (__builtin_ia32_reducesh_mask_round ((A), (B), (C),\t\t\\\n+\t\t\t\t       _mm_setzero_ph (),\t\\\n+\t\t\t\t       (__mmask8)-1, (D)))\n \n #define _mm_mask_reduce_round_sh(A, B, C, D, E, F)\t\t\t\\\n-  (__builtin_ia32_vreducesh_v8hf_mask_round ((C), (D), (E), (A), (B), (F)))\n+  (__builtin_ia32_reducesh_mask_round ((C), (D), (E), (A), (B), (F)))\n \n #define _mm_maskz_reduce_round_sh(A, B, C, D, E)\t\t\\\n-  (__builtin_ia32_vreducesh_v8hf_mask_round ((B), (C), (D),\t\\\n-\t\t\t\t\t     _mm_setzero_ph (),\t\\\n-\t\t\t\t\t     (A), (E)))\n+  (__builtin_ia32_reducesh_mask_round ((B), (C), (D),\t\t\\\n+\t\t\t\t       _mm_setzero_ph (),\t\\\n+\t\t\t\t       (A), (E)))\n \n #endif /* __OPTIMIZE__ */\n \n /* Intrinsics vrndscaleph.  */\n #ifdef __OPTIMIZE__\n extern __inline __m512h\n-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n+  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_roundscale_ph (__m512h __A, int __B)\n {\n-  return __builtin_ia32_vrndscaleph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t      _mm512_setzero_ph (),\n-\t\t\t\t\t\t      (__mmask32) -1,\n-\t\t\t\t\t\t      _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_rndscaleph512_mask_round (__A, __B,\n+\t\t\t\t\t\t  _mm512_setzero_ph (),\n+\t\t\t\t\t\t  (__mmask32) -1,\n+\t\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_roundscale_ph (__m512h __A, __mmask32 __B,\n-\t\t\t\t __m512h __C, int __D)\n+\t\t\t   __m512h __C, int __D)\n {\n-  return __builtin_ia32_vrndscaleph_v32hf_mask_round (__C, __D, __A, __B,\n-\t\t\t\t\t\t      _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_rndscaleph512_mask_round (__C, __D, __A, __B,\n+\t\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_roundscale_ph (__mmask32 __A, __m512h __B, int __C)\n {\n-  return __builtin_ia32_vrndscaleph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t      _mm512_setzero_ph (),\n-\t\t\t\t\t\t      __A,\n-\t\t\t\t\t\t      _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_rndscaleph512_mask_round (__B, __C,\n+\t\t\t\t\t\t  _mm512_setzero_ph (),\n+\t\t\t\t\t\t  __A,\n+\t\t\t\t\t\t  _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_roundscale_round_ph (__m512h __A, int __B, const int __C)\n {\n-  return __builtin_ia32_vrndscaleph_v32hf_mask_round (__A, __B,\n-\t\t\t\t\t\t      _mm512_setzero_ph (),\n-\t\t\t\t\t\t      (__mmask32) -1,\n-\t\t\t\t\t\t      __C);\n+  return __builtin_ia32_rndscaleph512_mask_round (__A, __B,\n+\t\t\t\t\t\t  _mm512_setzero_ph (),\n+\t\t\t\t\t\t  (__mmask32) -1,\n+\t\t\t\t\t\t  __C);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_roundscale_round_ph (__m512h __A, __mmask32 __B,\n \t\t\t\t __m512h __C, int __D, const int __E)\n {\n-  return __builtin_ia32_vrndscaleph_v32hf_mask_round (__C, __D, __A,\n-\t\t\t\t\t\t      __B, __E);\n+  return __builtin_ia32_rndscaleph512_mask_round (__C, __D, __A,\n+\t\t\t\t\t\t  __B, __E);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_roundscale_round_ph (__mmask32 __A, __m512h __B, int __C,\n \t\t\t\t  const int __D)\n {\n-  return __builtin_ia32_vrndscaleph_v32hf_mask_round (__B, __C,\n-\t\t\t\t\t\t      _mm512_setzero_ph (),\n-\t\t\t\t\t\t      __A, __D);\n+  return __builtin_ia32_rndscaleph512_mask_round (__B, __C,\n+\t\t\t\t\t\t  _mm512_setzero_ph (),\n+\t\t\t\t\t\t  __A, __D);\n }\n \n #else\n-#define _mm512_roundscale_ph(A, B) \\\n-  (__builtin_ia32_vrndscaleph_v32hf_mask_round ((A), (B),\t\t\\\n-\t\t\t\t\t\t_mm512_setzero_ph (),\t\\\n-\t\t\t\t\t\t(__mmask32)-1,\t\t\\\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION))\n-\n-#define _mm512_mask_roundscale_ph(A, B, C, D) \\\n-  (__builtin_ia32_vrndscaleph_v32hf_mask_round ((C), (D), (A), (B),\t\\\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION))\n-\n-#define _mm512_maskz_roundscale_ph(A, B, C) \\\n-  (__builtin_ia32_vrndscaleph_v32hf_mask_round ((B), (C),\t\t\\\n-\t\t\t\t\t\t_mm512_setzero_ph (),\t\\\n-\t\t\t\t\t\t(A),\t\t\t\\\n-\t\t\t\t\t\t_MM_FROUND_CUR_DIRECTION))\n-#define _mm512_roundscale_round_ph(A, B, C) \\\n-  (__builtin_ia32_vrndscaleph_v32hf_mask_round ((A), (B),\t\t\\\n-\t\t\t\t\t\t_mm512_setzero_ph (),\t\\\n-\t\t\t\t\t\t(__mmask32)-1, (C)))\n+#define _mm512_roundscale_ph(A, B)\t\t\t\t\t\\\n+  (__builtin_ia32_rndscaleph512_mask_round ((A), (B),\t\t\t\\\n+\t\t\t\t\t    _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t    (__mmask32)-1,\t\t\\\n+\t\t\t\t\t    _MM_FROUND_CUR_DIRECTION))\n+\n+#define _mm512_mask_roundscale_ph(A, B, C, D)\t\t\t\t\\\n+  (__builtin_ia32_rndscaleph512_mask_round ((C), (D), (A), (B),\t\t\\\n+\t\t\t\t\t    _MM_FROUND_CUR_DIRECTION))\n+\n+#define _mm512_maskz_roundscale_ph(A, B, C)\t\t\t\t\\\n+  (__builtin_ia32_rndscaleph512_mask_round ((B), (C),\t\t\t\\\n+\t\t\t\t\t    _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t    (A),\t\t\t\\\n+\t\t\t\t\t    _MM_FROUND_CUR_DIRECTION))\n+#define _mm512_roundscale_round_ph(A, B, C)\t\t\t\t\\\n+  (__builtin_ia32_rndscaleph512_mask_round ((A), (B),\t\t\t\\\n+\t\t\t\t\t    _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t    (__mmask32)-1, (C)))\n \n #define _mm512_mask_roundscale_round_ph(A, B, C, D, E)\t\t\t\\\n-  (__builtin_ia32_vrndscaleph_v32hf_mask_round ((C), (D), (A), (B), (E)))\n+  (__builtin_ia32_rndscaleph512_mask_round ((C), (D), (A), (B), (E)))\n \n-#define _mm512_maskz_roundscale_round_ph(A, B, C, D) \\\n-  (__builtin_ia32_vrndscaleph_v32hf_mask_round ((B), (C),\t\t\\\n-\t\t\t\t\t\t_mm512_setzero_ph (),\t\\\n-\t\t\t\t\t\t(A), (D)))\n+#define _mm512_maskz_roundscale_round_ph(A, B, C, D)\t\t\t\\\n+  (__builtin_ia32_rndscaleph512_mask_round ((B), (C),\t\t\t\\\n+\t\t\t\t\t    _mm512_setzero_ph (),\t\\\n+\t\t\t\t\t    (A), (D)))\n \n #endif /* __OPTIMIZE__ */\n \n /* Intrinsics vrndscalesh.  */\n #ifdef __OPTIMIZE__\n extern __inline __m128h\n-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n+  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_roundscale_sh (__m128h __A, __m128h __B, int __C)\n {\n-  return __builtin_ia32_vrndscalesh_v8hf_mask_round (__A, __B, __C,\n-\t\t\t\t\t\t     _mm_setzero_ph (),\n-\t\t\t\t\t\t     (__mmask8) -1,\n-\t\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_rndscalesh_mask_round (__A, __B, __C,\n+\t\t\t\t\t       _mm_setzero_ph (),\n+\t\t\t\t\t       (__mmask8) -1,\n+\t\t\t\t\t       _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_roundscale_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t\t__m128h __D, int __E)\n {\n-  return __builtin_ia32_vrndscalesh_v8hf_mask_round (__C, __D, __E, __A, __B,\n-\t\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_rndscalesh_mask_round (__C, __D, __E, __A, __B,\n+\t\t\t\t\t       _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_roundscale_sh (__mmask8 __A, __m128h __B, __m128h __C, int __D)\n {\n-  return __builtin_ia32_vrndscalesh_v8hf_mask_round (__B, __C, __D,\n-\t\t\t\t\t\t     _mm_setzero_ph (), __A,\n-\t\t\t\t\t\t     _MM_FROUND_CUR_DIRECTION);\n+  return __builtin_ia32_rndscalesh_mask_round (__B, __C, __D,\n+\t\t\t\t\t       _mm_setzero_ph (), __A,\n+\t\t\t\t\t       _MM_FROUND_CUR_DIRECTION);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_roundscale_round_sh (__m128h __A, __m128h __B, int __C, const int __D)\n {\n-  return __builtin_ia32_vrndscalesh_v8hf_mask_round (__A, __B, __C,\n-\t\t\t\t\t\t     _mm_setzero_ph (),\n-\t\t\t\t\t\t     (__mmask8) -1,\n-\t\t\t\t\t\t     __D);\n+  return __builtin_ia32_rndscalesh_mask_round (__A, __B, __C,\n+\t\t\t\t\t       _mm_setzero_ph (),\n+\t\t\t\t\t       (__mmask8) -1,\n+\t\t\t\t\t       __D);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_roundscale_round_sh (__m128h __A, __mmask8 __B, __m128h __C,\n \t\t\t      __m128h __D, int __E, const int __F)\n {\n-  return __builtin_ia32_vrndscalesh_v8hf_mask_round (__C, __D, __E,\n-\t\t\t\t\t\t     __A, __B, __F);\n+  return __builtin_ia32_rndscalesh_mask_round (__C, __D, __E,\n+\t\t\t\t\t       __A, __B, __F);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_roundscale_round_sh (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t\t       int __D, const int __E)\n {\n-  return __builtin_ia32_vrndscalesh_v8hf_mask_round (__B, __C, __D,\n-\t\t\t\t\t\t     _mm_setzero_ph (),\n-\t\t\t\t\t\t     __A, __E);\n+  return __builtin_ia32_rndscalesh_mask_round (__B, __C, __D,\n+\t\t\t\t\t       _mm_setzero_ph (),\n+\t\t\t\t\t       __A, __E);\n }\n \n #else\n #define _mm_roundscale_sh(A, B, C)\t\t\t\t\t\\\n-  (__builtin_ia32_vrndscalesh_v8hf_mask_round ((A), (B), (C),\t\t\\\n-\t\t\t\t\t       _mm_setzero_ph (),\t\\\n-\t\t\t\t\t       (__mmask8)-1, \\\n-\t\t\t\t\t       _MM_FROUND_CUR_DIRECTION))\n+  (__builtin_ia32_rndscalesh_mask_round ((A), (B), (C),\t\t\t\\\n+\t\t\t\t\t _mm_setzero_ph (),\t\t\\\n+\t\t\t\t\t (__mmask8)-1,\t\t\t\\\n+\t\t\t\t\t _MM_FROUND_CUR_DIRECTION))\n \n #define _mm_mask_roundscale_sh(A, B, C, D, E)\t\t\t\t\\\n-  (__builtin_ia32_vrndscalesh_v8hf_mask_round ((C), (D), (E), (A), (B), \\\n-\t\t\t\t\t       _MM_FROUND_CUR_DIRECTION))\n+  (__builtin_ia32_rndscalesh_mask_round ((C), (D), (E), (A), (B),\t\\\n+\t\t\t\t\t _MM_FROUND_CUR_DIRECTION))\n \n #define _mm_maskz_roundscale_sh(A, B, C, D)\t\t\t\t\\\n-  (__builtin_ia32_vrndscalesh_v8hf_mask_round ((B), (C), (D),\t\t\\\n-\t\t\t\t\t       _mm_setzero_ph (),\t\\\n-\t\t\t\t\t       (A), _MM_FROUND_CUR_DIRECTION))\n+  (__builtin_ia32_rndscalesh_mask_round ((B), (C), (D),\t\t\t\\\n+\t\t\t\t\t _mm_setzero_ph (),\t\t\\\n+\t\t\t\t\t (A), _MM_FROUND_CUR_DIRECTION))\n \n-#define _mm_roundscale_round_sh(A, B, C, D)\t\t\t\t\\\n-  (__builtin_ia32_vrndscalesh_v8hf_mask_round ((A), (B), (C),\t\t\\\n-\t\t\t\t\t       _mm_setzero_ph (),\t\\\n-\t\t\t\t\t       (__mmask8)-1, (D)))\n+#define _mm_roundscale_round_sh(A, B, C, D)\t\t\t\\\n+  (__builtin_ia32_rndscalesh_mask_round ((A), (B), (C),\t\t\\\n+\t\t\t\t\t _mm_setzero_ph (),\t\\\n+\t\t\t\t\t (__mmask8)-1, (D)))\n \n #define _mm_mask_roundscale_round_sh(A, B, C, D, E, F)\t\t\t\\\n-  (__builtin_ia32_vrndscalesh_v8hf_mask_round ((C), (D), (E), (A), (B), (F)))\n+  (__builtin_ia32_rndscalesh_mask_round ((C), (D), (E), (A), (B), (F)))\n \n-#define _mm_maskz_roundscale_round_sh(A, B, C, D, E)\t\t\t\\\n-  (__builtin_ia32_vrndscalesh_v8hf_mask_round ((B), (C), (D),\t\t\\\n-\t\t\t\t\t       _mm_setzero_ph (),\t\\\n-\t\t\t\t\t       (A), (E)))\n+#define _mm_maskz_roundscale_round_sh(A, B, C, D, E)\t\t\\\n+  (__builtin_ia32_rndscalesh_mask_round ((B), (C), (D),\t\t\\\n+\t\t\t\t\t _mm_setzero_ph (),\t\\\n+\t\t\t\t\t (A), (E)))\n \n #endif /* __OPTIMIZE__ */\n \n /* Intrinsics vfpclasssh.  */\n #ifdef __OPTIMIZE__\n extern __inline __mmask8\n-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n+  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_fpclass_sh_mask (__m128h __A, const int __imm)\n {\n   return (__mmask8) __builtin_ia32_fpclasssh_mask ((__v8hf) __A, __imm,\n@@ -2031,11 +2031,11 @@ _mm512_fpclass_ph_mask (__m512h __A, const int __imm)\n \n #else\n #define _mm512_mask_fpclass_ph_mask(u, x, c)\t\t\t\t\\\n-  ((__mmask32) __builtin_ia32_fpclassph512_mask ((__v32hf) (__m512h) (x),\\\n+  ((__mmask32) __builtin_ia32_fpclassph512_mask ((__v32hf) (__m512h) (x), \\\n \t\t\t\t\t\t (int) (c),(__mmask8)(u)))\n \n #define _mm512_fpclass_ph_mask(x, c)                                    \\\n-  ((__mmask32) __builtin_ia32_fpclassph512_mask ((__v32hf) (__m512h) (x),\\\n+  ((__mmask32) __builtin_ia32_fpclassph512_mask ((__v32hf) (__m512h) (x), \\\n \t\t\t\t\t\t (int) (c),(__mmask8)-1))\n #endif /* __OPIMTIZE__ */\n \n@@ -2141,9 +2141,9 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_getexp_round_ph (__m512h __A, const int __R)\n {\n   return (__m512h) __builtin_ia32_getexpph512_mask ((__v32hf) __A,\n-\t\t\t\t\t\t   (__v32hf)\n-\t\t\t\t\t\t   _mm512_setzero_ph (),\n-\t\t\t\t\t\t   (__mmask32) -1, __R);\n+\t\t\t\t\t\t    (__v32hf)\n+\t\t\t\t\t\t    _mm512_setzero_ph (),\n+\t\t\t\t\t\t    (__mmask32) -1, __R);\n }\n \n extern __inline __m512h\n@@ -2152,46 +2152,46 @@ _mm512_mask_getexp_round_ph (__m512h __W, __mmask32 __U, __m512h __A,\n \t\t\t     const int __R)\n {\n   return (__m512h) __builtin_ia32_getexpph512_mask ((__v32hf) __A,\n-\t\t\t\t\t\t   (__v32hf) __W,\n-\t\t\t\t\t\t   (__mmask32) __U, __R);\n+\t\t\t\t\t\t    (__v32hf) __W,\n+\t\t\t\t\t\t    (__mmask32) __U, __R);\n }\n \n extern __inline __m512h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_getexp_round_ph (__mmask32 __U, __m512h __A, const int __R)\n {\n   return (__m512h) __builtin_ia32_getexpph512_mask ((__v32hf) __A,\n-\t\t\t\t\t\t   (__v32hf)\n-\t\t\t\t\t\t   _mm512_setzero_ph (),\n-\t\t\t\t\t\t   (__mmask32) __U, __R);\n+\t\t\t\t\t\t    (__v32hf)\n+\t\t\t\t\t\t    _mm512_setzero_ph (),\n+\t\t\t\t\t\t    (__mmask32) __U, __R);\n }\n \n #else\n-#define _mm_getexp_round_sh(A, B, R)\t\t\t\t\t\t\\\n-  ((__m128h)__builtin_ia32_getexpsh_mask_round((__v8hf)(__m128h)(A),\t\t\\\n-\t\t\t\t\t       (__v8hf)(__m128h)(B),\t\t\\\n-\t\t\t\t\t       (__v8hf)_mm_setzero_ph(),\t\\\n+#define _mm_getexp_round_sh(A, B, R)\t\t\t\t\t\\\n+  ((__m128h)__builtin_ia32_getexpsh_mask_round((__v8hf)(__m128h)(A),\t\\\n+\t\t\t\t\t       (__v8hf)(__m128h)(B),\t\\\n+\t\t\t\t\t       (__v8hf)_mm_setzero_ph(), \\\n \t\t\t\t\t       (__mmask8)-1, R))\n \n-#define _mm_mask_getexp_round_sh(W, U, A, B, C)\t\t\t\t\t\\\n+#define _mm_mask_getexp_round_sh(W, U, A, B, C)\t\t\t\\\n   (__m128h)__builtin_ia32_getexpsh_mask_round(A, B, W, U, C)\n \n-#define _mm_maskz_getexp_round_sh(U, A, B, C)\t\t\t\t\t\\\n-  (__m128h)__builtin_ia32_getexpsh_mask_round(A, B,\t\t\t\t\\\n-\t\t\t\t\t      (__v8hf)_mm_setzero_ph(),\t\t\\\n+#define _mm_maskz_getexp_round_sh(U, A, B, C)\t\t\t\t\\\n+  (__m128h)__builtin_ia32_getexpsh_mask_round(A, B,\t\t\t\\\n+\t\t\t\t\t      (__v8hf)_mm_setzero_ph(),\t\\\n \t\t\t\t\t      U, C)\n \n-#define _mm512_getexp_round_ph(A, R)\t\t\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_getexpph512_mask((__v32hf)(__m512h)(A),\t\t\\\n-  (__v32hf)_mm512_setzero_ph(), (__mmask32)-1, R))\n+#define _mm512_getexp_round_ph(A, R)\t\t\t\t\t\\\n+  ((__m512h)__builtin_ia32_getexpph512_mask((__v32hf)(__m512h)(A),\t\\\n+\t\t\t\t\t    (__v32hf)_mm512_setzero_ph(), (__mmask32)-1, R))\n \n-#define _mm512_mask_getexp_round_ph(W, U, A, R)\t\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_getexpph512_mask((__v32hf)(__m512h)(A),\t\t\\\n-  (__v32hf)(__m512h)(W), (__mmask32)(U), R))\n+#define _mm512_mask_getexp_round_ph(W, U, A, R)\t\t\t\t\\\n+  ((__m512h)__builtin_ia32_getexpph512_mask((__v32hf)(__m512h)(A),\t\\\n+\t\t\t\t\t    (__v32hf)(__m512h)(W), (__mmask32)(U), R))\n \n-#define _mm512_maskz_getexp_round_ph(U, A, R)\t\t\t\t\t\\\n-  ((__m512h)__builtin_ia32_getexpph512_mask((__v32hf)(__m512h)(A),\t\t\\\n-  (__v32hf)_mm512_setzero_ph(), (__mmask32)(U), R))\n+#define _mm512_maskz_getexp_round_ph(U, A, R)\t\t\t\t\\\n+  ((__m512h)__builtin_ia32_getexpph512_mask((__v32hf)(__m512h)(A),\t\\\n+\t\t\t\t\t    (__v32hf)_mm512_setzero_ph(), (__mmask32)(U), R))\n \n #endif /* __OPTIMIZE__ */\n "}, {"sha": "e9478792a033b3681d8c5e6c3ddc044e5812841a", "filename": "gcc/config/i386/avx512fp16vlintrin.h", "status": "modified", "additions": 227, "deletions": 227, "changes": 454, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Fconfig%2Fi386%2Favx512fp16vlintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Fconfig%2Fi386%2Favx512fp16vlintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Favx512fp16vlintrin.h?ref=e4d3643361df1145b34265c398e901498548c6e6", "patch": "@@ -53,30 +53,30 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_add_ph (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vaddph_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_addph128_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_add_ph (__m256h __A, __mmask16 __B, __m256h __C, __m256h __D)\n {\n-  return __builtin_ia32_vaddph_v16hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_addph256_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_add_ph (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vaddph_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_addph128_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t       __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_add_ph (__mmask16 __A, __m256h __B, __m256h __C)\n {\n-  return __builtin_ia32_vaddph_v16hf_mask (__B, __C,\n-\t\t\t\t\t   _mm256_setzero_ph (), __A);\n+  return __builtin_ia32_addph256_mask (__B, __C,\n+\t\t\t\t       _mm256_setzero_ph (), __A);\n }\n \n extern __inline __m128h\n@@ -97,30 +97,30 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_sub_ph (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vsubph_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_subph128_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_sub_ph (__m256h __A, __mmask16 __B, __m256h __C, __m256h __D)\n {\n-  return __builtin_ia32_vsubph_v16hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_subph256_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_sub_ph (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vsubph_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_subph128_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t       __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_sub_ph (__mmask16 __A, __m256h __B, __m256h __C)\n {\n-  return __builtin_ia32_vsubph_v16hf_mask (__B, __C,\n-\t\t\t\t\t   _mm256_setzero_ph (), __A);\n+  return __builtin_ia32_subph256_mask (__B, __C,\n+\t\t\t\t       _mm256_setzero_ph (), __A);\n }\n \n extern __inline __m128h\n@@ -141,30 +141,30 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_mul_ph (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vmulph_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_mulph128_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_mul_ph (__m256h __A, __mmask16 __B, __m256h __C, __m256h __D)\n {\n-  return __builtin_ia32_vmulph_v16hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_mulph256_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_mul_ph (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vmulph_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_mulph128_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t       __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_mul_ph (__mmask16 __A, __m256h __B, __m256h __C)\n {\n-  return __builtin_ia32_vmulph_v16hf_mask (__B, __C,\n-\t\t\t\t\t   _mm256_setzero_ph (), __A);\n+  return __builtin_ia32_mulph256_mask (__B, __C,\n+\t\t\t\t       _mm256_setzero_ph (), __A);\n }\n \n extern __inline __m128h\n@@ -185,127 +185,127 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_div_ph (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vdivph_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_divph128_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_div_ph (__m256h __A, __mmask16 __B, __m256h __C, __m256h __D)\n {\n-  return __builtin_ia32_vdivph_v16hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_divph256_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_div_ph (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vdivph_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_divph128_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t       __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_div_ph (__mmask16 __A, __m256h __B, __m256h __C)\n {\n-  return __builtin_ia32_vdivph_v16hf_mask (__B, __C,\n-\t\t\t\t\t   _mm256_setzero_ph (), __A);\n+  return __builtin_ia32_divph256_mask (__B, __C,\n+\t\t\t\t       _mm256_setzero_ph (), __A);\n }\n \n /* Intrinsics v[max,min]ph.  */\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_ph (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vmaxph_v8hf_mask (__A, __B,\n-\t\t\t\t\t  _mm_setzero_ph (),\n-\t\t\t\t\t  (__mmask8) -1);\n+  return __builtin_ia32_maxph128_mask (__A, __B,\n+\t\t\t\t       _mm_setzero_ph (),\n+\t\t\t\t       (__mmask8) -1);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_max_ph (__m256h __A, __m256h __B)\n {\n-  return __builtin_ia32_vmaxph_v16hf_mask (__A, __B,\n-\t\t\t\t\t  _mm256_setzero_ph (),\n-\t\t\t\t\t  (__mmask16) -1);\n+  return __builtin_ia32_maxph256_mask (__A, __B,\n+\t\t\t\t       _mm256_setzero_ph (),\n+\t\t\t\t       (__mmask16) -1);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_max_ph (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vmaxph_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_maxph128_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_max_ph (__m256h __A, __mmask16 __B, __m256h __C, __m256h __D)\n {\n-  return __builtin_ia32_vmaxph_v16hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_maxph256_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_max_ph (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vmaxph_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_maxph128_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t       __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_max_ph (__mmask16 __A, __m256h __B, __m256h __C)\n {\n-  return __builtin_ia32_vmaxph_v16hf_mask (__B, __C,\n-\t\t\t\t\t   _mm256_setzero_ph (), __A);\n+  return __builtin_ia32_maxph256_mask (__B, __C,\n+\t\t\t\t       _mm256_setzero_ph (), __A);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_ph (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vminph_v8hf_mask (__A, __B,\n-\t\t\t\t\t  _mm_setzero_ph (),\n-\t\t\t\t\t  (__mmask8) -1);\n+  return __builtin_ia32_minph128_mask (__A, __B,\n+\t\t\t\t       _mm_setzero_ph (),\n+\t\t\t\t       (__mmask8) -1);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_min_ph (__m256h __A, __m256h __B)\n {\n-  return __builtin_ia32_vminph_v16hf_mask (__A, __B,\n-\t\t\t\t\t  _mm256_setzero_ph (),\n-\t\t\t\t\t  (__mmask16) -1);\n+  return __builtin_ia32_minph256_mask (__A, __B,\n+\t\t\t\t       _mm256_setzero_ph (),\n+\t\t\t\t       (__mmask16) -1);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_min_ph (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vminph_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_minph128_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_min_ph (__m256h __A, __mmask16 __B, __m256h __C, __m256h __D)\n {\n-  return __builtin_ia32_vminph_v16hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_minph256_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_min_ph (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vminph_v8hf_mask (__B, __C, _mm_setzero_ph (),\n-\t\t\t\t\t  __A);\n+  return __builtin_ia32_minph128_mask (__B, __C, _mm_setzero_ph (),\n+\t\t\t\t       __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_min_ph (__mmask16 __A, __m256h __B, __m256h __C)\n {\n-  return __builtin_ia32_vminph_v16hf_mask (__B, __C,\n-\t\t\t\t\t   _mm256_setzero_ph (), __A);\n+  return __builtin_ia32_minph256_mask (__B, __C,\n+\t\t\t\t       _mm256_setzero_ph (), __A);\n }\n \n /* vcmpph */\n@@ -314,47 +314,47 @@ extern __inline __mmask8\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmp_ph_mask (__m128h __A, __m128h __B, const int __C)\n {\n-  return (__mmask8) __builtin_ia32_vcmpph_v8hf_mask (__A, __B, __C,\n-\t\t\t\t\t\t     (__mmask8) -1);\n+  return (__mmask8) __builtin_ia32_cmpph128_mask (__A, __B, __C,\n+\t\t\t\t\t\t  (__mmask8) -1);\n }\n \n extern __inline __mmask8\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_cmp_ph_mask (__mmask8 __A, __m128h __B, __m128h __C,\n \t\t      const int __D)\n {\n-  return (__mmask8) __builtin_ia32_vcmpph_v8hf_mask (__B, __C, __D, __A);\n+  return (__mmask8) __builtin_ia32_cmpph128_mask (__B, __C, __D, __A);\n }\n \n extern __inline __mmask16\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmp_ph_mask (__m256h __A, __m256h __B, const int __C)\n {\n-  return (__mmask16) __builtin_ia32_vcmpph_v16hf_mask (__A, __B, __C,\n-\t\t\t\t\t\t       (__mmask16) -1);\n+  return (__mmask16) __builtin_ia32_cmpph256_mask (__A, __B, __C,\n+\t\t\t\t\t\t   (__mmask16) -1);\n }\n \n extern __inline __mmask16\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_cmp_ph_mask (__mmask16 __A, __m256h __B, __m256h __C,\n \t\t      const int __D)\n {\n-  return (__mmask16) __builtin_ia32_vcmpph_v16hf_mask (__B, __C, __D,\n-\t\t\t\t\t\t       __A);\n+  return (__mmask16) __builtin_ia32_cmpph256_mask (__B, __C, __D,\n+\t\t\t\t\t\t   __A);\n }\n \n #else\n-#define _mm_cmp_ph_mask(A, B, C)\t\t\\\n-  (__builtin_ia32_vcmpph_v8hf_mask ((A), (B), (C), (-1)))\n+#define _mm_cmp_ph_mask(A, B, C)\t\t\t\\\n+  (__builtin_ia32_cmpph128_mask ((A), (B), (C), (-1)))\n \n-#define _mm_mask_cmp_ph_mask(A, B, C, D)\t\\\n-  (__builtin_ia32_vcmpph_v8hf_mask ((B), (C), (D), (A)))\n+#define _mm_mask_cmp_ph_mask(A, B, C, D)\t\t\\\n+  (__builtin_ia32_cmpph128_mask ((B), (C), (D), (A)))\n \n-#define _mm256_cmp_ph_mask(A, B, C)\t\t\\\n-  (__builtin_ia32_vcmpph_v16hf_mask ((A), (B), (C), (-1)))\n+#define _mm256_cmp_ph_mask(A, B, C)\t\t\t\\\n+  (__builtin_ia32_cmpph256_mask ((A), (B), (C), (-1)))\n \n-#define _mm256_mask_cmp_ph_mask(A, B, C, D)\t\\\n-  (__builtin_ia32_vcmpph_v16hf_mask ((B), (C), (D), (A)))\n+#define _mm256_mask_cmp_ph_mask(A, B, C, D)\t\t\\\n+  (__builtin_ia32_cmpph256_mask ((B), (C), (D), (A)))\n \n #endif /* __OPTIMIZE__ */\n \n@@ -363,189 +363,189 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sqrt_ph (__m128h __A)\n {\n-  return __builtin_ia32_vsqrtph_v8hf_mask (__A, _mm_setzero_ph (),\n-\t\t\t\t\t   (__mmask8) -1);\n+  return __builtin_ia32_sqrtph128_mask (__A, _mm_setzero_ph (),\n+\t\t\t\t\t(__mmask8) -1);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_sqrt_ph (__m256h __A)\n {\n-  return __builtin_ia32_vsqrtph_v16hf_mask (__A, _mm256_setzero_ph (),\n-\t\t\t\t\t    (__mmask16) -1);\n+  return __builtin_ia32_sqrtph256_mask (__A, _mm256_setzero_ph (),\n+\t\t\t\t\t(__mmask16) -1);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_sqrt_ph (__m128h __A, __mmask8 __B, __m128h __C)\n {\n-  return __builtin_ia32_vsqrtph_v8hf_mask (__C, __A, __B);\n+  return __builtin_ia32_sqrtph128_mask (__C, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_sqrt_ph (__m256h __A, __mmask16 __B, __m256h __C)\n {\n-  return __builtin_ia32_vsqrtph_v16hf_mask (__C, __A, __B);\n+  return __builtin_ia32_sqrtph256_mask (__C, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_sqrt_ph (__mmask8 __A, __m128h __B)\n {\n-  return __builtin_ia32_vsqrtph_v8hf_mask (__B, _mm_setzero_ph (),\n-\t\t\t\t\t   __A);\n+  return __builtin_ia32_sqrtph128_mask (__B, _mm_setzero_ph (),\n+\t\t\t\t\t__A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_sqrt_ph (__mmask16 __A, __m256h __B)\n {\n-  return __builtin_ia32_vsqrtph_v16hf_mask (__B, _mm256_setzero_ph (),\n-\t\t\t\t\t    __A);\n+  return __builtin_ia32_sqrtph256_mask (__B, _mm256_setzero_ph (),\n+\t\t\t\t\t__A);\n }\n \n /* Intrinsics vrsqrtph.  */\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rsqrt_ph (__m128h __A)\n {\n-  return __builtin_ia32_vrsqrtph_v8hf_mask (__A, _mm_setzero_ph (),\n-\t\t\t\t\t    (__mmask8) -1);\n+  return __builtin_ia32_rsqrtph128_mask (__A, _mm_setzero_ph (),\n+\t\t\t\t\t (__mmask8) -1);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_rsqrt_ph (__m256h __A)\n {\n-  return __builtin_ia32_vrsqrtph_v16hf_mask (__A, _mm256_setzero_ph (),\n-\t\t\t\t\t     (__mmask16) -1);\n+  return __builtin_ia32_rsqrtph256_mask (__A, _mm256_setzero_ph (),\n+\t\t\t\t\t (__mmask16) -1);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_rsqrt_ph (__m128h __A, __mmask8 __B, __m128h __C)\n {\n-  return __builtin_ia32_vrsqrtph_v8hf_mask (__C, __A, __B);\n+  return __builtin_ia32_rsqrtph128_mask (__C, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_rsqrt_ph (__m256h __A, __mmask16 __B, __m256h __C)\n {\n-  return __builtin_ia32_vrsqrtph_v16hf_mask (__C, __A, __B);\n+  return __builtin_ia32_rsqrtph256_mask (__C, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_rsqrt_ph (__mmask8 __A, __m128h __B)\n {\n-  return __builtin_ia32_vrsqrtph_v8hf_mask (__B, _mm_setzero_ph (), __A);\n+  return __builtin_ia32_rsqrtph128_mask (__B, _mm_setzero_ph (), __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_rsqrt_ph (__mmask16 __A, __m256h __B)\n {\n-  return __builtin_ia32_vrsqrtph_v16hf_mask (__B, _mm256_setzero_ph (),\n-\t\t\t\t\t     __A);\n+  return __builtin_ia32_rsqrtph256_mask (__B, _mm256_setzero_ph (),\n+\t\t\t\t\t __A);\n }\n \n /* Intrinsics vrcpph.  */\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rcp_ph (__m128h __A)\n {\n-  return __builtin_ia32_vrcpph_v8hf_mask (__A, _mm_setzero_ph (),\n-\t\t\t\t\t  (__mmask8) -1);\n+  return __builtin_ia32_rcpph128_mask (__A, _mm_setzero_ph (),\n+\t\t\t\t       (__mmask8) -1);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_rcp_ph (__m256h __A)\n {\n-  return __builtin_ia32_vrcpph_v16hf_mask (__A, _mm256_setzero_ph (),\n-\t\t\t\t\t   (__mmask16) -1);\n+  return __builtin_ia32_rcpph256_mask (__A, _mm256_setzero_ph (),\n+\t\t\t\t       (__mmask16) -1);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_rcp_ph (__m128h __A, __mmask8 __B, __m128h __C)\n {\n-  return __builtin_ia32_vrcpph_v8hf_mask (__C, __A, __B);\n+  return __builtin_ia32_rcpph128_mask (__C, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_rcp_ph (__m256h __A, __mmask16 __B, __m256h __C)\n {\n-  return __builtin_ia32_vrcpph_v16hf_mask (__C, __A, __B);\n+  return __builtin_ia32_rcpph256_mask (__C, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_rcp_ph (__mmask8 __A, __m128h __B)\n {\n-  return __builtin_ia32_vrcpph_v8hf_mask (__B, _mm_setzero_ph (), __A);\n+  return __builtin_ia32_rcpph128_mask (__B, _mm_setzero_ph (), __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_rcp_ph (__mmask16 __A, __m256h __B)\n {\n-  return __builtin_ia32_vrcpph_v16hf_mask (__B, _mm256_setzero_ph (),\n-\t\t\t\t\t   __A);\n+  return __builtin_ia32_rcpph256_mask (__B, _mm256_setzero_ph (),\n+\t\t\t\t       __A);\n }\n \n /* Intrinsics vscalefph.  */\n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_scalef_ph (__m128h __A, __m128h __B)\n {\n-  return __builtin_ia32_vscalefph_v8hf_mask (__A, __B,\n-\t\t\t\t\t     _mm_setzero_ph (),\n-\t\t\t\t\t     (__mmask8) -1);\n+  return __builtin_ia32_scalefph128_mask (__A, __B,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  (__mmask8) -1);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_scalef_ph (__m256h __A, __m256h __B)\n {\n-  return __builtin_ia32_vscalefph_v16hf_mask (__A, __B,\n-\t\t\t\t\t      _mm256_setzero_ph (),\n-\t\t\t\t\t      (__mmask16) -1);\n+  return __builtin_ia32_scalefph256_mask (__A, __B,\n+\t\t\t\t\t  _mm256_setzero_ph (),\n+\t\t\t\t\t  (__mmask16) -1);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_scalef_ph (__m128h __A, __mmask8 __B, __m128h __C, __m128h __D)\n {\n-  return __builtin_ia32_vscalefph_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_scalefph128_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_scalef_ph (__m256h __A, __mmask16 __B, __m256h __C,\n \t\t       __m256h __D)\n {\n-  return __builtin_ia32_vscalefph_v16hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_scalefph256_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_scalef_ph (__mmask8 __A, __m128h __B, __m128h __C)\n {\n-  return __builtin_ia32_vscalefph_v8hf_mask (__B, __C,\n-\t\t\t\t\t     _mm_setzero_ph (), __A);\n+  return __builtin_ia32_scalefph128_mask (__B, __C,\n+\t\t\t\t\t  _mm_setzero_ph (), __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_scalef_ph (__mmask16 __A, __m256h __B, __m256h __C)\n {\n-  return __builtin_ia32_vscalefph_v16hf_mask (__B, __C,\n-\t\t\t\t\t      _mm256_setzero_ph (),\n-\t\t\t\t\t      __A);\n+  return __builtin_ia32_scalefph256_mask (__B, __C,\n+\t\t\t\t\t  _mm256_setzero_ph (),\n+\t\t\t\t\t  __A);\n }\n \n /* Intrinsics vreduceph.  */\n@@ -554,158 +554,158 @@ extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_reduce_ph (__m128h __A, int __B)\n {\n-  return __builtin_ia32_vreduceph_v8hf_mask (__A, __B,\n-\t\t\t\t\t     _mm_setzero_ph (),\n-\t\t\t\t\t     (__mmask8) -1);\n+  return __builtin_ia32_reduceph128_mask (__A, __B,\n+\t\t\t\t\t  _mm_setzero_ph (),\n+\t\t\t\t\t  (__mmask8) -1);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_reduce_ph (__m128h __A, __mmask8 __B, __m128h __C, int __D)\n {\n-  return __builtin_ia32_vreduceph_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_reduceph128_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_reduce_ph (__mmask8 __A, __m128h __B, int __C)\n {\n-  return __builtin_ia32_vreduceph_v8hf_mask (__B, __C,\n-\t\t\t\t\t     _mm_setzero_ph (), __A);\n+  return __builtin_ia32_reduceph128_mask (__B, __C,\n+\t\t\t\t\t  _mm_setzero_ph (), __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_reduce_ph (__m256h __A, int __B)\n {\n-  return __builtin_ia32_vreduceph_v16hf_mask (__A, __B,\n-\t\t\t\t\t      _mm256_setzero_ph (),\n-\t\t\t\t\t      (__mmask16) -1);\n+  return __builtin_ia32_reduceph256_mask (__A, __B,\n+\t\t\t\t\t  _mm256_setzero_ph (),\n+\t\t\t\t\t  (__mmask16) -1);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_reduce_ph (__m256h __A, __mmask16 __B, __m256h __C, int __D)\n {\n-  return __builtin_ia32_vreduceph_v16hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_reduceph256_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_reduce_ph (__mmask16 __A, __m256h __B, int __C)\n {\n-  return __builtin_ia32_vreduceph_v16hf_mask (__B, __C,\n-\t\t\t\t\t      _mm256_setzero_ph (),\n-\t\t\t\t\t      __A);\n+  return __builtin_ia32_reduceph256_mask (__B, __C,\n+\t\t\t\t\t  _mm256_setzero_ph (),\n+\t\t\t\t\t  __A);\n }\n \n #else\n-#define _mm_reduce_ph(A, B)\t\t\t\t\t\\\n-  (__builtin_ia32_vreduceph_v8hf_mask ((A), (B),\\\n-\t\t\t\t       _mm_setzero_ph (),\t\\\n-\t\t\t\t       ((__mmask8)-1)))\n+#define _mm_reduce_ph(A, B)\t\t\t\t\\\n+  (__builtin_ia32_reduceph128_mask ((A), (B),\t\t\\\n+\t\t\t\t    _mm_setzero_ph (),\t\\\n+\t\t\t\t    ((__mmask8)-1)))\n \n-#define _mm_mask_reduce_ph(A,  B,  C, D)\t\t\\\n-  (__builtin_ia32_vreduceph_v8hf_mask ((C), (D), (A), (B)))\n+#define _mm_mask_reduce_ph(A,  B,  C, D)\t\t\t\\\n+  (__builtin_ia32_reduceph128_mask ((C), (D), (A), (B)))\n \n-#define _mm_maskz_reduce_ph(A,  B, C)\t\t\t\t\\\n-  (__builtin_ia32_vreduceph_v8hf_mask ((B), (C), _mm_setzero_ph (), (A)))\n+#define _mm_maskz_reduce_ph(A,  B, C)\t\t\t\t\t\\\n+  (__builtin_ia32_reduceph128_mask ((B), (C), _mm_setzero_ph (), (A)))\n \n #define _mm256_reduce_ph(A, B)\t\t\t\t\t\\\n-  (__builtin_ia32_vreduceph_v16hf_mask ((A), (B),\\\n-\t\t\t\t\t_mm256_setzero_ph (),\t\\\n-\t\t\t\t\t((__mmask16)-1)))\n+  (__builtin_ia32_reduceph256_mask ((A), (B),\t\t\t\\\n+\t\t\t\t    _mm256_setzero_ph (),\t\\\n+\t\t\t\t    ((__mmask16)-1)))\n \n-#define _mm256_mask_reduce_ph(A, B, C, D)\t\t\\\n-  (__builtin_ia32_vreduceph_v16hf_mask ((C), (D), (A), (B)))\n+#define _mm256_mask_reduce_ph(A, B, C, D)\t\t\t\\\n+  (__builtin_ia32_reduceph256_mask ((C), (D), (A), (B)))\n \n-#define _mm256_maskz_reduce_ph(A, B, C)\t\t\t\t\\\n-  (__builtin_ia32_vreduceph_v16hf_mask ((B), (C), _mm256_setzero_ph (), (A)))\n+#define _mm256_maskz_reduce_ph(A, B, C)\t\t\t\t\t\\\n+  (__builtin_ia32_reduceph256_mask ((B), (C), _mm256_setzero_ph (), (A)))\n \n #endif /* __OPTIMIZE__ */\n \n /* Intrinsics vrndscaleph.  */\n #ifdef __OPTIMIZE__\n-extern __inline __m128h\n-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n-_mm_roundscale_ph (__m128h __A, int __B)\n-{\n-  return __builtin_ia32_vrndscaleph_v8hf_mask (__A, __B,\n-\t\t\t\t\t       _mm_setzero_ph (),\n-\t\t\t\t\t       (__mmask8) -1);\n-}\n+  extern __inline __m128h\n+  __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n+  _mm_roundscale_ph (__m128h __A, int __B)\n+  {\n+    return __builtin_ia32_rndscaleph128_mask (__A, __B,\n+\t\t\t\t\t      _mm_setzero_ph (),\n+\t\t\t\t\t      (__mmask8) -1);\n+  }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_roundscale_ph (__m128h __A, __mmask8 __B, __m128h __C, int __D)\n {\n-  return __builtin_ia32_vrndscaleph_v8hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_rndscaleph128_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_roundscale_ph (__mmask8 __A, __m128h __B, int __C)\n {\n-  return __builtin_ia32_vrndscaleph_v8hf_mask (__B, __C,\n-\t\t\t\t\t       _mm_setzero_ph (), __A);\n+  return __builtin_ia32_rndscaleph128_mask (__B, __C,\n+\t\t\t\t\t    _mm_setzero_ph (), __A);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_roundscale_ph (__m256h __A, int __B)\n {\n-  return __builtin_ia32_vrndscaleph_v16hf_mask (__A, __B,\n-\t\t\t\t\t\t_mm256_setzero_ph (),\n-\t\t\t\t\t\t(__mmask16) -1);\n+  return __builtin_ia32_rndscaleph256_mask (__A, __B,\n+\t\t\t\t\t    _mm256_setzero_ph (),\n+\t\t\t\t\t    (__mmask16) -1);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_roundscale_ph (__m256h __A, __mmask16 __B, __m256h __C,\n \t\t\t   int __D)\n {\n-  return __builtin_ia32_vrndscaleph_v16hf_mask (__C, __D, __A, __B);\n+  return __builtin_ia32_rndscaleph256_mask (__C, __D, __A, __B);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_roundscale_ph (__mmask16 __A, __m256h __B, int __C)\n {\n-  return __builtin_ia32_vrndscaleph_v16hf_mask (__B, __C,\n-\t\t\t\t\t\t_mm256_setzero_ph (),\n-\t\t\t\t\t\t__A);\n+  return __builtin_ia32_rndscaleph256_mask (__B, __C,\n+\t\t\t\t\t    _mm256_setzero_ph (),\n+\t\t\t\t\t    __A);\n }\n \n #else\n-#define _mm_roundscale_ph(A, B) \\\n-  (__builtin_ia32_vrndscaleph_v8hf_mask ((A), (B), _mm_setzero_ph (),\t\\\n-\t\t\t\t\t ((__mmask8)-1)))\n+#define _mm_roundscale_ph(A, B)\t\t\t\t\t\t\\\n+  (__builtin_ia32_rndscaleph128_mask ((A), (B), _mm_setzero_ph (),\t\\\n+\t\t\t\t      ((__mmask8)-1)))\n \n-#define _mm_mask_roundscale_ph(A, B, C, D) \\\n-  (__builtin_ia32_vrndscaleph_v8hf_mask ((C), (D), (A), (B)))\n+#define _mm_mask_roundscale_ph(A, B, C, D)\t\t\t\\\n+  (__builtin_ia32_rndscaleph128_mask ((C), (D), (A), (B)))\n \n-#define _mm_maskz_roundscale_ph(A, B, C) \\\n-  (__builtin_ia32_vrndscaleph_v8hf_mask ((B), (C), _mm_setzero_ph (), (A)))\n+#define _mm_maskz_roundscale_ph(A, B, C)\t\t\t\t\\\n+  (__builtin_ia32_rndscaleph128_mask ((B), (C), _mm_setzero_ph (), (A)))\n \n-#define _mm256_roundscale_ph(A, B) \\\n-  (__builtin_ia32_vrndscaleph_v16hf_mask ((A), (B),\t      \\\n-\t\t\t\t\t _mm256_setzero_ph(), \\\n-\t\t\t\t\t  ((__mmask16)-1)))\n+#define _mm256_roundscale_ph(A, B)\t\t\t\t\\\n+  (__builtin_ia32_rndscaleph256_mask ((A), (B),\t\t\t\\\n+\t\t\t\t      _mm256_setzero_ph(),\t\\\n+\t\t\t\t      ((__mmask16)-1)))\n \n-#define _mm256_mask_roundscale_ph(A, B, C, D) \\\n-  (__builtin_ia32_vrndscaleph_v16hf_mask ((C), (D), (A), (B)))\n+#define _mm256_mask_roundscale_ph(A, B, C, D)\t\t\t\\\n+  (__builtin_ia32_rndscaleph256_mask ((C), (D), (A), (B)))\n \n-#define _mm256_maskz_roundscale_ph(A, B, C) \\\n-  (__builtin_ia32_vrndscaleph_v16hf_mask ((B), (C),\t\t\t\\\n-\t\t\t\t\t  _mm256_setzero_ph (), (A)))\n+#define _mm256_maskz_roundscale_ph(A, B, C)\t\t\t\t\\\n+  (__builtin_ia32_rndscaleph256_mask ((B), (C),\t\t\t\t\\\n+\t\t\t\t      _mm256_setzero_ph (), (A)))\n \n #endif /* __OPTIMIZE__ */\n \n /* Intrinsics vfpclassph.  */\n #ifdef __OPTIMIZE__\n extern __inline __mmask8\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n-_mm_mask_fpclass_ph_mask (__mmask8 __U, __m128h __A, const int __imm)\n+  _mm_mask_fpclass_ph_mask (__mmask8 __U, __m128h __A, const int __imm)\n {\n   return (__mmask8) __builtin_ia32_fpclassph128_mask ((__v8hf) __A,\n \t\t\t\t\t\t      __imm, __U);\n@@ -725,34 +725,34 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_fpclass_ph_mask (__mmask16 __U, __m256h __A, const int __imm)\n {\n   return (__mmask16) __builtin_ia32_fpclassph256_mask ((__v16hf) __A,\n-\t\t\t\t\t\t      __imm, __U);\n+\t\t\t\t\t\t       __imm, __U);\n }\n \n extern __inline __mmask16\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_fpclass_ph_mask (__m256h __A, const int __imm)\n {\n   return (__mmask16) __builtin_ia32_fpclassph256_mask ((__v16hf) __A,\n-\t\t\t\t\t\t      __imm,\n-\t\t\t\t\t\t      (__mmask16) -1);\n+\t\t\t\t\t\t       __imm,\n+\t\t\t\t\t\t       (__mmask16) -1);\n }\n \n #else\n #define _mm_fpclass_ph_mask(X, C)                                       \\\n-  ((__mmask8) __builtin_ia32_fpclassph128_mask ((__v8hf) (__m128h) (X),  \\\n+  ((__mmask8) __builtin_ia32_fpclassph128_mask ((__v8hf) (__m128h) (X),\t\\\n \t\t\t\t\t\t(int) (C),(__mmask8)-1))\n \n #define _mm_mask_fpclass_ph_mask(u, X, C)                               \\\n-  ((__mmask8) __builtin_ia32_fpclassph128_mask ((__v8hf) (__m128h) (X),  \\\n+  ((__mmask8) __builtin_ia32_fpclassph128_mask ((__v8hf) (__m128h) (X),\t\\\n \t\t\t\t\t\t(int) (C),(__mmask8)(u)))\n \n #define _mm256_fpclass_ph_mask(X, C)                                    \\\n-  ((__mmask16) __builtin_ia32_fpclassph256_mask ((__v16hf) (__m256h) (X),  \\\n-\t\t\t\t\t\t(int) (C),(__mmask16)-1))\n+  ((__mmask16) __builtin_ia32_fpclassph256_mask ((__v16hf) (__m256h) (X), \\\n+\t\t\t\t\t\t (int) (C),(__mmask16)-1))\n \n #define _mm256_mask_fpclass_ph_mask(u, X, C)\t\t\t\t\\\n-  ((__mmask16) __builtin_ia32_fpclassph256_mask ((__v16hf) (__m256h) (X),  \\\n-\t\t\t\t\t\t(int) (C),(__mmask16)(u)))\n+  ((__mmask16) __builtin_ia32_fpclassph256_mask ((__v16hf) (__m256h) (X), \\\n+\t\t\t\t\t\t (int) (C),(__mmask16)(u)))\n #endif /* __OPTIMIZE__ */\n \n /* Intrinsics vgetexpph, vgetexpsh.  */\n@@ -761,57 +761,57 @@ __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_getexp_ph (__m256h __A)\n {\n   return (__m256h) __builtin_ia32_getexpph256_mask ((__v16hf) __A,\n-\t\t\t\t\t\t   (__v16hf)\n-\t\t\t\t\t\t   _mm256_setzero_ph (),\n-\t\t\t\t\t\t   (__mmask16) -1);\n+\t\t\t\t\t\t    (__v16hf)\n+\t\t\t\t\t\t    _mm256_setzero_ph (),\n+\t\t\t\t\t\t    (__mmask16) -1);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_getexp_ph (__m256h __W, __mmask16 __U, __m256h __A)\n {\n   return (__m256h) __builtin_ia32_getexpph256_mask ((__v16hf) __A,\n-\t\t\t\t\t\t   (__v16hf) __W,\n-\t\t\t\t\t\t   (__mmask16) __U);\n+\t\t\t\t\t\t    (__v16hf) __W,\n+\t\t\t\t\t\t    (__mmask16) __U);\n }\n \n extern __inline __m256h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_getexp_ph (__mmask16 __U, __m256h __A)\n {\n   return (__m256h) __builtin_ia32_getexpph256_mask ((__v16hf) __A,\n-\t\t\t\t\t\t   (__v16hf)\n-\t\t\t\t\t\t   _mm256_setzero_ph (),\n-\t\t\t\t\t\t   (__mmask16) __U);\n+\t\t\t\t\t\t    (__v16hf)\n+\t\t\t\t\t\t    _mm256_setzero_ph (),\n+\t\t\t\t\t\t    (__mmask16) __U);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_getexp_ph (__m128h __A)\n {\n   return (__m128h) __builtin_ia32_getexpph128_mask ((__v8hf) __A,\n-\t\t\t\t\t\t   (__v8hf)\n-\t\t\t\t\t\t   _mm_setzero_ph (),\n-\t\t\t\t\t\t   (__mmask8) -1);\n+\t\t\t\t\t\t    (__v8hf)\n+\t\t\t\t\t\t    _mm_setzero_ph (),\n+\t\t\t\t\t\t    (__mmask8) -1);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_getexp_ph (__m128h __W, __mmask8 __U, __m128h __A)\n {\n   return (__m128h) __builtin_ia32_getexpph128_mask ((__v8hf) __A,\n-\t\t\t\t\t\t   (__v8hf) __W,\n-\t\t\t\t\t\t   (__mmask8) __U);\n+\t\t\t\t\t\t    (__v8hf) __W,\n+\t\t\t\t\t\t    (__mmask8) __U);\n }\n \n extern __inline __m128h\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_getexp_ph (__mmask8 __U, __m128h __A)\n {\n   return (__m128h) __builtin_ia32_getexpph128_mask ((__v8hf) __A,\n-\t\t\t\t\t\t   (__v8hf)\n-\t\t\t\t\t\t   _mm_setzero_ph (),\n-\t\t\t\t\t\t   (__mmask8) __U);\n+\t\t\t\t\t\t    (__v8hf)\n+\t\t\t\t\t\t    _mm_setzero_ph (),\n+\t\t\t\t\t\t    (__mmask8) __U);\n }\n \n \n@@ -892,41 +892,41 @@ _mm_maskz_getmant_ph (__mmask8 __U, __m128h __A,\n }\n \n #else\n-#define _mm256_getmant_ph(X, B, C)                                              \\\n-  ((__m256h) __builtin_ia32_getmantph256_mask ((__v16hf)(__m256h) (X),           \\\n-\t\t\t\t\t (int)(((C)<<2) | (B)),                 \\\n-\t\t\t\t\t  (__v16hf)(__m256h)_mm256_setzero_ph (),\\\n-\t\t\t\t\t  (__mmask16)-1))\n-\n-#define _mm256_mask_getmant_ph(W, U, X, B, C)                                   \\\n-  ((__m256h) __builtin_ia32_getmantph256_mask ((__v16hf)(__m256h) (X),           \\\n-\t\t\t\t\t (int)(((C)<<2) | (B)),                 \\\n-\t\t\t\t\t  (__v16hf)(__m256h)(W),                 \\\n-\t\t\t\t\t  (__mmask16)(U)))\n-\n-#define _mm256_maskz_getmant_ph(U, X, B, C)                                     \\\n-  ((__m256h) __builtin_ia32_getmantph256_mask ((__v16hf)(__m256h) (X),           \\\n-\t\t\t\t\t (int)(((C)<<2) | (B)),                 \\\n-\t\t\t\t\t  (__v16hf)(__m256h)_mm256_setzero_ph (),\\\n-\t\t\t\t\t  (__mmask16)(U)))\n-\n-#define _mm_getmant_ph(X, B, C)                                                 \\\n-  ((__m128h) __builtin_ia32_getmantph128_mask ((__v8hf)(__m128h) (X),           \\\n-\t\t\t\t\t (int)(((C)<<2) | (B)),                 \\\n-\t\t\t\t\t  (__v8hf)(__m128h)_mm_setzero_ph (),   \\\n-\t\t\t\t\t  (__mmask8)-1))\n-\n-#define _mm_mask_getmant_ph(W, U, X, B, C)                                      \\\n-  ((__m128h) __builtin_ia32_getmantph128_mask ((__v8hf)(__m128h) (X),           \\\n-\t\t\t\t\t (int)(((C)<<2) | (B)),                 \\\n-\t\t\t\t\t  (__v8hf)(__m128h)(W),                 \\\n-\t\t\t\t\t  (__mmask8)(U)))\n-\n-#define _mm_maskz_getmant_ph(U, X, B, C)                                        \\\n-  ((__m128h) __builtin_ia32_getmantph128_mask ((__v8hf)(__m128h) (X),           \\\n-\t\t\t\t\t (int)(((C)<<2) | (B)),                 \\\n-\t\t\t\t\t  (__v8hf)(__m128h)_mm_setzero_ph (),   \\\n-\t\t\t\t\t  (__mmask8)(U)))\n+#define _mm256_getmant_ph(X, B, C)\t\t\t\t\t\\\n+  ((__m256h) __builtin_ia32_getmantph256_mask ((__v16hf)(__m256h) (X),\t\\\n+\t\t\t\t\t       (int)(((C)<<2) | (B)),\t\\\n+\t\t\t\t\t       (__v16hf)(__m256h)_mm256_setzero_ph (), \\\n+\t\t\t\t\t       (__mmask16)-1))\n+\n+#define _mm256_mask_getmant_ph(W, U, X, B, C)\t\t\t\t\\\n+  ((__m256h) __builtin_ia32_getmantph256_mask ((__v16hf)(__m256h) (X),\t\\\n+\t\t\t\t\t       (int)(((C)<<2) | (B)),\t\\\n+\t\t\t\t\t       (__v16hf)(__m256h)(W),\t\\\n+\t\t\t\t\t       (__mmask16)(U)))\n+\n+#define _mm256_maskz_getmant_ph(U, X, B, C)\t\t\t\t\\\n+  ((__m256h) __builtin_ia32_getmantph256_mask ((__v16hf)(__m256h) (X),\t\\\n+\t\t\t\t\t       (int)(((C)<<2) | (B)),\t\\\n+\t\t\t\t\t       (__v16hf)(__m256h)_mm256_setzero_ph (), \\\n+\t\t\t\t\t       (__mmask16)(U)))\n+\n+#define _mm_getmant_ph(X, B, C)\t\t\t\t\t\t\\\n+  ((__m128h) __builtin_ia32_getmantph128_mask ((__v8hf)(__m128h) (X),\t\\\n+\t\t\t\t\t       (int)(((C)<<2) | (B)),\t\\\n+\t\t\t\t\t       (__v8hf)(__m128h)_mm_setzero_ph (), \\\n+\t\t\t\t\t       (__mmask8)-1))\n+\n+#define _mm_mask_getmant_ph(W, U, X, B, C)\t\t\t\t\\\n+  ((__m128h) __builtin_ia32_getmantph128_mask ((__v8hf)(__m128h) (X),\t\\\n+\t\t\t\t\t       (int)(((C)<<2) | (B)),\t\\\n+\t\t\t\t\t       (__v8hf)(__m128h)(W),\t\\\n+\t\t\t\t\t       (__mmask8)(U)))\n+\n+#define _mm_maskz_getmant_ph(U, X, B, C)\t\t\t\t\\\n+  ((__m128h) __builtin_ia32_getmantph128_mask ((__v8hf)(__m128h) (X),\t\\\n+\t\t\t\t\t       (int)(((C)<<2) | (B)),\t\\\n+\t\t\t\t\t       (__v8hf)(__m128h)_mm_setzero_ph (), \\\n+\t\t\t\t\t       (__mmask8)(U)))\n \n #endif /* __OPTIMIZE__ */\n "}, {"sha": "0292059c068076fd54ab084c5a44ee0be4b0c389", "filename": "gcc/config/i386/i386-builtin.def", "status": "modified", "additions": 65, "deletions": 65, "changes": 130, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Fconfig%2Fi386%2Fi386-builtin.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Fconfig%2Fi386%2Fi386-builtin.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-builtin.def?ref=e4d3643361df1145b34265c398e901498548c6e6", "patch": "@@ -2775,49 +2775,49 @@ BDESC (0, OPTION_MASK_ISA2_AVX512BF16, CODE_FOR_avx512f_dpbf16ps_v4sf_mask, \"__b\n BDESC (0, OPTION_MASK_ISA2_AVX512BF16, CODE_FOR_avx512f_dpbf16ps_v4sf_maskz, \"__builtin_ia32_dpbf16ps_v4sf_maskz\", IX86_BUILTIN_DPHI16PS_V4SF_MASKZ, UNKNOWN, (int) V4SF_FTYPE_V4SF_V8HI_V8HI_UQI)\n \n /* AVX512FP16.  */\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_addv8hf3_mask, \"__builtin_ia32_vaddph_v8hf_mask\", IX86_BUILTIN_VADDPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_addv16hf3_mask, \"__builtin_ia32_vaddph_v16hf_mask\", IX86_BUILTIN_VADDPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_addv32hf3_mask, \"__builtin_ia32_vaddph_v32hf_mask\", IX86_BUILTIN_VADDPH_V32HF_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_subv8hf3_mask, \"__builtin_ia32_vsubph_v8hf_mask\", IX86_BUILTIN_VSUBPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_subv16hf3_mask, \"__builtin_ia32_vsubph_v16hf_mask\", IX86_BUILTIN_VSUBPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_subv32hf3_mask, \"__builtin_ia32_vsubph_v32hf_mask\", IX86_BUILTIN_VSUBPH_V32HF_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_mulv8hf3_mask, \"__builtin_ia32_vmulph_v8hf_mask\", IX86_BUILTIN_VMULPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_mulv16hf3_mask, \"__builtin_ia32_vmulph_v16hf_mask\", IX86_BUILTIN_VMULPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_mulv32hf3_mask, \"__builtin_ia32_vmulph_v32hf_mask\", IX86_BUILTIN_VMULPH_V32HF_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_divv8hf3_mask, \"__builtin_ia32_vdivph_v8hf_mask\", IX86_BUILTIN_VDIVPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_divv16hf3_mask, \"__builtin_ia32_vdivph_v16hf_mask\", IX86_BUILTIN_VDIVPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_divv32hf3_mask, \"__builtin_ia32_vdivph_v32hf_mask\", IX86_BUILTIN_VDIVPH_V32HF_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmaddv8hf3_mask, \"__builtin_ia32_vaddsh_v8hf_mask\", IX86_BUILTIN_VADDSH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsubv8hf3_mask, \"__builtin_ia32_vsubsh_v8hf_mask\", IX86_BUILTIN_VSUBSH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmmulv8hf3_mask, \"__builtin_ia32_vmulsh_v8hf_mask\", IX86_BUILTIN_VMULSH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmdivv8hf3_mask, \"__builtin_ia32_vdivsh_v8hf_mask\", IX86_BUILTIN_VDIVSH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_smaxv8hf3_mask, \"__builtin_ia32_vmaxph_v8hf_mask\", IX86_BUILTIN_VMAXPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_smaxv16hf3_mask, \"__builtin_ia32_vmaxph_v16hf_mask\", IX86_BUILTIN_VMAXPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_smaxv32hf3_mask, \"__builtin_ia32_vmaxph_v32hf_mask\", IX86_BUILTIN_VMAXPH_V32HF_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_sminv8hf3_mask, \"__builtin_ia32_vminph_v8hf_mask\", IX86_BUILTIN_VMINPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_sminv16hf3_mask, \"__builtin_ia32_vminph_v16hf_mask\", IX86_BUILTIN_VMINPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_sminv32hf3_mask, \"__builtin_ia32_vminph_v32hf_mask\", IX86_BUILTIN_VMINPH_V32HF_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsmaxv8hf3_mask, \"__builtin_ia32_vmaxsh_v8hf_mask\", IX86_BUILTIN_VMAXSH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsminv8hf3_mask, \"__builtin_ia32_vminsh_v8hf_mask\", IX86_BUILTIN_VMINSH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_cmpv8hf3_mask, \"__builtin_ia32_vcmpph_v8hf_mask\", IX86_BUILTIN_VCMPPH_V8HF_MASK, UNKNOWN, (int) UQI_FTYPE_V8HF_V8HF_INT_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512vl_cmpv16hf3_mask, \"__builtin_ia32_vcmpph_v16hf_mask\", IX86_BUILTIN_VCMPPH_V16HF_MASK, UNKNOWN, (int) UHI_FTYPE_V16HF_V16HF_INT_UHI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512bw_cmpv32hf3_mask, \"__builtin_ia32_vcmpph_v32hf_mask\", IX86_BUILTIN_VCMPPH_V32HF_MASK, UNKNOWN, (int) USI_FTYPE_V32HF_V32HF_INT_USI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_sqrtv8hf2_mask, \"__builtin_ia32_vsqrtph_v8hf_mask\", IX86_BUILTIN_VSQRTPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_sqrtv16hf2_mask, \"__builtin_ia32_vsqrtph_v16hf_mask\", IX86_BUILTIN_VSQRTPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_UHI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rsqrtv8hf2_mask, \"__builtin_ia32_vrsqrtph_v8hf_mask\", IX86_BUILTIN_VRSQRTPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rsqrtv16hf2_mask, \"__builtin_ia32_vrsqrtph_v16hf_mask\", IX86_BUILTIN_VRSQRTPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_UHI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rsqrtv32hf2_mask, \"__builtin_ia32_vrsqrtph_v32hf_mask\", IX86_BUILTIN_VRSQRTPH_V32HF_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_USI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmrsqrtv8hf2_mask, \"__builtin_ia32_vrsqrtsh_v8hf_mask\", IX86_BUILTIN_VRSQRTSH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rcpv8hf2_mask, \"__builtin_ia32_vrcpph_v8hf_mask\", IX86_BUILTIN_VRCPPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rcpv16hf2_mask, \"__builtin_ia32_vrcpph_v16hf_mask\", IX86_BUILTIN_VRCPPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_UHI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rcpv32hf2_mask, \"__builtin_ia32_vrcpph_v32hf_mask\", IX86_BUILTIN_VRCPPH_V32HF_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_USI)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmrcpv8hf2_mask, \"__builtin_ia32_vrcpsh_v8hf_mask\", IX86_BUILTIN_VRCPSH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_scalefv8hf_mask, \"__builtin_ia32_vscalefph_v8hf_mask\", IX86_BUILTIN_VSCALEFPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512vl_scalefv16hf_mask, \"__builtin_ia32_vscalefph_v16hf_mask\", IX86_BUILTIN_VSCALEFPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_reducepv8hf_mask, \"__builtin_ia32_vreduceph_v8hf_mask\", IX86_BUILTIN_VREDUCEPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_INT_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_reducepv16hf_mask, \"__builtin_ia32_vreduceph_v16hf_mask\", IX86_BUILTIN_VREDUCEPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_INT_V16HF_UHI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rndscalev8hf_mask, \"__builtin_ia32_vrndscaleph_v8hf_mask\", IX86_BUILTIN_VRNDSCALEPH_V8HF_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_INT_V8HF_UQI)\n-BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512vl_rndscalev16hf_mask, \"__builtin_ia32_vrndscaleph_v16hf_mask\", IX86_BUILTIN_VRNDSCALEPH_V16HF_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_INT_V16HF_UHI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_addv8hf3_mask, \"__builtin_ia32_addph128_mask\", IX86_BUILTIN_ADDPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_addv16hf3_mask, \"__builtin_ia32_addph256_mask\", IX86_BUILTIN_ADDPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_addv32hf3_mask, \"__builtin_ia32_addph512_mask\", IX86_BUILTIN_ADDPH512_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_subv8hf3_mask, \"__builtin_ia32_subph128_mask\", IX86_BUILTIN_SUBPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_subv16hf3_mask, \"__builtin_ia32_subph256_mask\", IX86_BUILTIN_SUBPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_subv32hf3_mask, \"__builtin_ia32_subph512_mask\", IX86_BUILTIN_SUBPH512_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_mulv8hf3_mask, \"__builtin_ia32_mulph128_mask\", IX86_BUILTIN_MULPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_mulv16hf3_mask, \"__builtin_ia32_mulph256_mask\", IX86_BUILTIN_MULPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_mulv32hf3_mask, \"__builtin_ia32_mulph512_mask\", IX86_BUILTIN_MULPH512_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_divv8hf3_mask, \"__builtin_ia32_divph128_mask\", IX86_BUILTIN_DIVPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_divv16hf3_mask, \"__builtin_ia32_divph256_mask\", IX86_BUILTIN_DIVPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_divv32hf3_mask, \"__builtin_ia32_divph512_mask\", IX86_BUILTIN_DIVPH512_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmaddv8hf3_mask, \"__builtin_ia32_addsh_mask\", IX86_BUILTIN_ADDSH_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsubv8hf3_mask, \"__builtin_ia32_subsh_mask\", IX86_BUILTIN_SUBSH_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmmulv8hf3_mask, \"__builtin_ia32_mulsh_mask\", IX86_BUILTIN_MULSH_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmdivv8hf3_mask, \"__builtin_ia32_divsh_mask\", IX86_BUILTIN_DIVSH_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_smaxv8hf3_mask, \"__builtin_ia32_maxph128_mask\", IX86_BUILTIN_MAXPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_smaxv16hf3_mask, \"__builtin_ia32_maxph256_mask\", IX86_BUILTIN_MAXPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_smaxv32hf3_mask, \"__builtin_ia32_maxph512_mask\", IX86_BUILTIN_MAXPH512_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_sminv8hf3_mask, \"__builtin_ia32_minph128_mask\", IX86_BUILTIN_MINPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_sminv16hf3_mask, \"__builtin_ia32_minph256_mask\", IX86_BUILTIN_MINPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_sminv32hf3_mask, \"__builtin_ia32_minph512_mask\", IX86_BUILTIN_MINPH512_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsmaxv8hf3_mask, \"__builtin_ia32_maxsh_mask\", IX86_BUILTIN_MAXSH_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsminv8hf3_mask, \"__builtin_ia32_minsh_mask\", IX86_BUILTIN_MINSH_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_cmpv8hf3_mask, \"__builtin_ia32_cmpph128_mask\", IX86_BUILTIN_CMPPH128_MASK, UNKNOWN, (int) UQI_FTYPE_V8HF_V8HF_INT_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512vl_cmpv16hf3_mask, \"__builtin_ia32_cmpph256_mask\", IX86_BUILTIN_CMPPH256_MASK, UNKNOWN, (int) UHI_FTYPE_V16HF_V16HF_INT_UHI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512bw_cmpv32hf3_mask, \"__builtin_ia32_cmpph512_mask\", IX86_BUILTIN_CMPPH512_MASK, UNKNOWN, (int) USI_FTYPE_V32HF_V32HF_INT_USI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_sqrtv8hf2_mask, \"__builtin_ia32_sqrtph128_mask\", IX86_BUILTIN_SQRTPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_sqrtv16hf2_mask, \"__builtin_ia32_sqrtph256_mask\", IX86_BUILTIN_SQRTPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_UHI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rsqrtv8hf2_mask, \"__builtin_ia32_rsqrtph128_mask\", IX86_BUILTIN_RSQRTPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rsqrtv16hf2_mask, \"__builtin_ia32_rsqrtph256_mask\", IX86_BUILTIN_RSQRTPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_UHI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rsqrtv32hf2_mask, \"__builtin_ia32_rsqrtph512_mask\", IX86_BUILTIN_RSQRTPH512_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_USI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmrsqrtv8hf2_mask, \"__builtin_ia32_rsqrtsh_mask\", IX86_BUILTIN_RSQRTSH_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rcpv8hf2_mask, \"__builtin_ia32_rcpph128_mask\", IX86_BUILTIN_RCPPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rcpv16hf2_mask, \"__builtin_ia32_rcpph256_mask\", IX86_BUILTIN_RCPPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_UHI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rcpv32hf2_mask, \"__builtin_ia32_rcpph512_mask\", IX86_BUILTIN_RCPPH512_MASK, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_USI)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmrcpv8hf2_mask, \"__builtin_ia32_rcpsh_mask\", IX86_BUILTIN_RCPSH_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_scalefv8hf_mask, \"__builtin_ia32_scalefph128_mask\", IX86_BUILTIN_SCALEFPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512vl_scalefv16hf_mask, \"__builtin_ia32_scalefph256_mask\", IX86_BUILTIN_SCALEFPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_V16HF_V16HF_UHI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_reducepv8hf_mask, \"__builtin_ia32_reduceph128_mask\", IX86_BUILTIN_REDUCEPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_INT_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_reducepv16hf_mask, \"__builtin_ia32_reduceph256_mask\", IX86_BUILTIN_REDUCEPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_INT_V16HF_UHI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_rndscalev8hf_mask, \"__builtin_ia32_rndscaleph128_mask\", IX86_BUILTIN_RNDSCALEPH128_MASK, UNKNOWN, (int) V8HF_FTYPE_V8HF_INT_V8HF_UQI)\n+BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512vl_rndscalev16hf_mask, \"__builtin_ia32_rndscaleph256_mask\", IX86_BUILTIN_RNDSCALEPH256_MASK, UNKNOWN, (int) V16HF_FTYPE_V16HF_INT_V16HF_UHI)\n BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512dq_fpclassv16hf_mask, \"__builtin_ia32_fpclassph256_mask\", IX86_BUILTIN_FPCLASSPH256, UNKNOWN, (int) HI_FTYPE_V16HF_INT_UHI)\n BDESC (OPTION_MASK_ISA_AVX512VL, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512dq_fpclassv8hf_mask, \"__builtin_ia32_fpclassph128_mask\", IX86_BUILTIN_FPCLASSPH128, UNKNOWN, (int) QI_FTYPE_V8HF_INT_UQI)\n BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512dq_fpclassv32hf_mask, \"__builtin_ia32_fpclassph512_mask\", IX86_BUILTIN_FPCLASSPH512, UNKNOWN, (int) SI_FTYPE_V32HF_INT_USI)\n@@ -3027,28 +3027,28 @@ BDESC (OPTION_MASK_ISA_AVX512DQ, 0, CODE_FOR_avx512dq_rangepv16sf_mask_round, \"_\n BDESC (OPTION_MASK_ISA_AVX512DQ, 0, CODE_FOR_avx512dq_rangepv8df_mask_round, \"__builtin_ia32_rangepd512_mask\", IX86_BUILTIN_RANGEPD512, UNKNOWN, (int) V8DF_FTYPE_V8DF_V8DF_INT_V8DF_QI_INT)\n \n /* AVX512FP16.  */\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_addv32hf3_mask_round, \"__builtin_ia32_vaddph_v32hf_mask_round\", IX86_BUILTIN_VADDPH_V32HF_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_subv32hf3_mask_round, \"__builtin_ia32_vsubph_v32hf_mask_round\", IX86_BUILTIN_VSUBPH_V32HF_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_mulv32hf3_mask_round, \"__builtin_ia32_vmulph_v32hf_mask_round\", IX86_BUILTIN_VMULPH_V32HF_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_divv32hf3_mask_round, \"__builtin_ia32_vdivph_v32hf_mask_round\", IX86_BUILTIN_VDIVPH_V32HF_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmaddv8hf3_mask_round, \"__builtin_ia32_vaddsh_v8hf_mask_round\", IX86_BUILTIN_VADDSH_V8HF_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsubv8hf3_mask_round, \"__builtin_ia32_vsubsh_v8hf_mask_round\", IX86_BUILTIN_VSUBSH_V8HF_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmmulv8hf3_mask_round, \"__builtin_ia32_vmulsh_v8hf_mask_round\", IX86_BUILTIN_VMULSH_V8HF_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmdivv8hf3_mask_round, \"__builtin_ia32_vdivsh_v8hf_mask_round\", IX86_BUILTIN_VDIVSH_V8HF_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_smaxv32hf3_mask_round, \"__builtin_ia32_vmaxph_v32hf_mask_round\", IX86_BUILTIN_VMAXPH_V32HF_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_sminv32hf3_mask_round, \"__builtin_ia32_vminph_v32hf_mask_round\", IX86_BUILTIN_VMINPH_V32HF_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsmaxv8hf3_mask_round, \"__builtin_ia32_vmaxsh_v8hf_mask_round\", IX86_BUILTIN_VMAXSH_V8HF_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsminv8hf3_mask_round, \"__builtin_ia32_vminsh_v8hf_mask_round\", IX86_BUILTIN_VMINSH_V8HF_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512bw_cmpv32hf3_mask_round, \"__builtin_ia32_vcmpph_v32hf_mask_round\", IX86_BUILTIN_VCMPPH_V32HF_MASK_ROUND, UNKNOWN, (int) USI_FTYPE_V32HF_V32HF_INT_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512f_vmcmpv8hf3_mask_round, \"__builtin_ia32_vcmpsh_v8hf_mask_round\", IX86_BUILTIN_VCMPSH_V8HF_MASK_ROUND, UNKNOWN, (int) UQI_FTYPE_V8HF_V8HF_INT_UQI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_sqrtv32hf2_mask_round, \"__builtin_ia32_vsqrtph_v32hf_mask_round\", IX86_BUILTIN_VSQRTPH_V32HF_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsqrtv8hf2_mask_round, \"__builtin_ia32_vsqrtsh_v8hf_mask_round\", IX86_BUILTIN_VSQRTSH_V8HF_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512bw_scalefv32hf_mask_round, \"__builtin_ia32_vscalefph_v32hf_mask_round\", IX86_BUILTIN_VSCALEFPH_V32HF_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512f_vmscalefv8hf_mask_round, \"__builtin_ia32_vscalefsh_v8hf_mask_round\", IX86_BUILTIN_VSCALEFSH_V8HF_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_reducepv32hf_mask_round, \"__builtin_ia32_vreduceph_v32hf_mask_round\", IX86_BUILTIN_VREDUCEPH_V32HF_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_INT_V32HF_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_reducesv8hf_mask_round, \"__builtin_ia32_vreducesh_v8hf_mask_round\", IX86_BUILTIN_VREDUCESH_V8HF_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_INT_V8HF_UQI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512bw_rndscalev32hf_mask_round, \"__builtin_ia32_vrndscaleph_v32hf_mask_round\", IX86_BUILTIN_VRNDSCALEPH_V32HF_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_INT_V32HF_USI_INT)\n-BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512f_rndscalev8hf_mask_round, \"__builtin_ia32_vrndscalesh_v8hf_mask_round\", IX86_BUILTIN_VRNDSCALESH_V8HF_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_INT_V8HF_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_addv32hf3_mask_round, \"__builtin_ia32_addph512_mask_round\", IX86_BUILTIN_ADDPH512_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_subv32hf3_mask_round, \"__builtin_ia32_subph512_mask_round\", IX86_BUILTIN_SUBPH512_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_mulv32hf3_mask_round, \"__builtin_ia32_mulph512_mask_round\", IX86_BUILTIN_MULPH512_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_divv32hf3_mask_round, \"__builtin_ia32_divph512_mask_round\", IX86_BUILTIN_DIVPH512_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmaddv8hf3_mask_round, \"__builtin_ia32_addsh_mask_round\", IX86_BUILTIN_ADDSH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsubv8hf3_mask_round, \"__builtin_ia32_subsh_mask_round\", IX86_BUILTIN_SUBSH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmmulv8hf3_mask_round, \"__builtin_ia32_mulsh_mask_round\", IX86_BUILTIN_MULSH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmdivv8hf3_mask_round, \"__builtin_ia32_divsh_mask_round\", IX86_BUILTIN_DIVSH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_smaxv32hf3_mask_round, \"__builtin_ia32_maxph512_mask_round\", IX86_BUILTIN_MAXPH512_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_sminv32hf3_mask_round, \"__builtin_ia32_minph512_mask_round\", IX86_BUILTIN_MINPH512_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsmaxv8hf3_mask_round, \"__builtin_ia32_maxsh_mask_round\", IX86_BUILTIN_MAXSH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsminv8hf3_mask_round, \"__builtin_ia32_minsh_mask_round\", IX86_BUILTIN_MINSH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512bw_cmpv32hf3_mask_round, \"__builtin_ia32_cmpph512_mask_round\", IX86_BUILTIN_CMPPH512_MASK_ROUND, UNKNOWN, (int) USI_FTYPE_V32HF_V32HF_INT_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512f_vmcmpv8hf3_mask_round, \"__builtin_ia32_cmpsh_mask_round\", IX86_BUILTIN_CMPSH_MASK_ROUND, UNKNOWN, (int) UQI_FTYPE_V8HF_V8HF_INT_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_sqrtv32hf2_mask_round, \"__builtin_ia32_sqrtph512_mask_round\", IX86_BUILTIN_SQRTPH512_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512fp16_vmsqrtv8hf2_mask_round, \"__builtin_ia32_sqrtsh_mask_round\", IX86_BUILTIN_SQRTSH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512bw_scalefv32hf_mask_round, \"__builtin_ia32_scalefph512_mask_round\", IX86_BUILTIN_SCALEFPH512_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_V32HF_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512f_vmscalefv8hf_mask_round, \"__builtin_ia32_scalefsh_mask_round\", IX86_BUILTIN_SCALEFSH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_reducepv32hf_mask_round, \"__builtin_ia32_reduceph512_mask_round\", IX86_BUILTIN_REDUCEPH512_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_INT_V32HF_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_reducesv8hf_mask_round, \"__builtin_ia32_reducesh_mask_round\", IX86_BUILTIN_REDUCESH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_INT_V8HF_UQI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512bw_rndscalev32hf_mask_round, \"__builtin_ia32_rndscaleph512_mask_round\", IX86_BUILTIN_RNDSCALEPH512_MASK_ROUND, UNKNOWN, (int) V32HF_FTYPE_V32HF_INT_V32HF_USI_INT)\n+BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512f_rndscalev8hf_mask_round, \"__builtin_ia32_rndscalesh_mask_round\", IX86_BUILTIN_RNDSCALESH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_INT_V8HF_UQI_INT)\n BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512bw_getexpv32hf_mask_round, \"__builtin_ia32_getexpph512_mask\", IX86_BUILTIN_GETEXPPH512, UNKNOWN, (int) V32HF_FTYPE_V32HF_V32HF_USI_INT)\n BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512f_sgetexpv8hf_mask_round, \"__builtin_ia32_getexpsh_mask_round\", IX86_BUILTIN_GETEXPSH_MASK_ROUND, UNKNOWN, (int) V8HF_FTYPE_V8HF_V8HF_V8HF_UQI_INT)\n BDESC (0, OPTION_MASK_ISA2_AVX512FP16, CODE_FOR_avx512bw_getmantv32hf_mask_round, \"__builtin_ia32_getmantph512_mask\", IX86_BUILTIN_GETMANTPH512, UNKNOWN, (int) V32HF_FTYPE_V32HF_INT_V32HF_USI_INT)"}, {"sha": "3a96e5864185f7618fe9007081a7ff482a0475e9", "filename": "gcc/testsuite/gcc.target/i386/avx-1.c", "status": "modified", "additions": 29, "deletions": 29, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx-1.c?ref=e4d3643361df1145b34265c398e901498548c6e6", "patch": "@@ -686,33 +686,33 @@\n #define __builtin_ia32_vpshld_v2di_mask(A, B, C, D, E)  __builtin_ia32_vpshld_v2di_mask(A, B, 1, D, E)\n \n /* avx512fp16intrin.h */\n-#define __builtin_ia32_vaddph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vaddph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vsubph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vsubph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmulph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vmulph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vdivph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vdivph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vaddsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vaddsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vsubsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vsubsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmulsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vmulsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vdivsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vdivsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmaxph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vmaxph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vminph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vminph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmaxsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vmaxsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vminsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vminsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vcmpph_v32hf_mask(A, B, C, D) __builtin_ia32_vcmpph_v32hf_mask(A, B, 1, D)\n-#define __builtin_ia32_vcmpph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vcmpph_v32hf_mask_round(A, B, 1, D, 8)\n-#define __builtin_ia32_vcmpsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vcmpsh_v8hf_mask_round(A, B, 1, D, 8)\n-#define __builtin_ia32_vsqrtph_v32hf_mask_round(C, A, B, D) __builtin_ia32_vsqrtph_v32hf_mask_round(C, A, B, 8)\n-#define __builtin_ia32_vsqrtsh_v8hf_mask_round(D, C, A, B, E) __builtin_ia32_vsqrtsh_v8hf_mask_round(D, C, A, B, 8)\n-#define __builtin_ia32_vscalefph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vscalefph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vscalefsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vscalefsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vreduceph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vreduceph_v32hf_mask_round(A, 123, C, D, 8)\n-#define __builtin_ia32_vreduceph_v8hf_mask(A, B, C, D) __builtin_ia32_vreduceph_v8hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vreduceph_v16hf_mask(A, B, C, D) __builtin_ia32_vreduceph_v16hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vreducesh_v8hf_mask_round(A, B, C, D, E, F) __builtin_ia32_vreducesh_v8hf_mask_round(A, B, 123, D, E, 8)\n-#define __builtin_ia32_vrndscaleph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vrndscaleph_v32hf_mask_round(A, 123, C, D, 8)\n-#define __builtin_ia32_vrndscaleph_v8hf_mask(A, B, C, D) __builtin_ia32_vrndscaleph_v8hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vrndscaleph_v16hf_mask(A, B, C, D) __builtin_ia32_vrndscaleph_v16hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vrndscalesh_v8hf_mask_round(A, B, C, D, E, F) __builtin_ia32_vrndscalesh_v8hf_mask_round(A, B, 123, D, E, 8)\n+#define __builtin_ia32_addph512_mask_round(A, B, C, D, E) __builtin_ia32_addph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_subph512_mask_round(A, B, C, D, E) __builtin_ia32_subph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_mulph512_mask_round(A, B, C, D, E) __builtin_ia32_mulph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_divph512_mask_round(A, B, C, D, E) __builtin_ia32_divph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_addsh_mask_round(A, B, C, D, E) __builtin_ia32_addsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_subsh_mask_round(A, B, C, D, E) __builtin_ia32_subsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_mulsh_mask_round(A, B, C, D, E) __builtin_ia32_mulsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_divsh_mask_round(A, B, C, D, E) __builtin_ia32_divsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_maxph512_mask_round(A, B, C, D, E) __builtin_ia32_maxph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_minph512_mask_round(A, B, C, D, E) __builtin_ia32_minph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_maxsh_mask_round(A, B, C, D, E) __builtin_ia32_maxsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_minsh_mask_round(A, B, C, D, E) __builtin_ia32_minsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_cmpph512_mask(A, B, C, D) __builtin_ia32_cmpph512_mask(A, B, 1, D)\n+#define __builtin_ia32_cmpph512_mask_round(A, B, C, D, E) __builtin_ia32_cmpph512_mask_round(A, B, 1, D, 8)\n+#define __builtin_ia32_cmpsh_mask_round(A, B, C, D, E) __builtin_ia32_cmpsh_mask_round(A, B, 1, D, 8)\n+#define __builtin_ia32_sqrtph512_mask_round(C, A, B, D) __builtin_ia32_sqrtph512_mask_round(C, A, B, 8)\n+#define __builtin_ia32_sqrtsh_mask_round(D, C, A, B, E) __builtin_ia32_sqrtsh_mask_round(D, C, A, B, 8)\n+#define __builtin_ia32_scalefph512_mask_round(A, B, C, D, E) __builtin_ia32_scalefph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_scalefsh_mask_round(A, B, C, D, E) __builtin_ia32_scalefsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_reduceph512_mask_round(A, B, C, D, E) __builtin_ia32_reduceph512_mask_round(A, 123, C, D, 8)\n+#define __builtin_ia32_reduceph128_mask(A, B, C, D) __builtin_ia32_reduceph128_mask(A, 123, C, D)\n+#define __builtin_ia32_reduceph256_mask(A, B, C, D) __builtin_ia32_reduceph256_mask(A, 123, C, D)\n+#define __builtin_ia32_reducesh_mask_round(A, B, C, D, E, F) __builtin_ia32_reducesh_mask_round(A, B, 123, D, E, 8)\n+#define __builtin_ia32_rndscaleph512_mask_round(A, B, C, D, E) __builtin_ia32_rndscaleph512_mask_round(A, 123, C, D, 8)\n+#define __builtin_ia32_rndscaleph128_mask(A, B, C, D) __builtin_ia32_rndscaleph128_mask(A, 123, C, D)\n+#define __builtin_ia32_rndscaleph256_mask(A, B, C, D) __builtin_ia32_rndscaleph256_mask(A, 123, C, D)\n+#define __builtin_ia32_rndscalesh_mask_round(A, B, C, D, E, F) __builtin_ia32_rndscalesh_mask_round(A, B, 123, D, E, 8)\n #define __builtin_ia32_fpclassph512_mask(A, D, C) __builtin_ia32_fpclassph512_mask(A, 1, C)\n #define __builtin_ia32_fpclasssh_mask(A, D, U) __builtin_ia32_fpclasssh_mask(A, 1, U)\n #define __builtin_ia32_getexpph512_mask(A, B, C, D) __builtin_ia32_getexpph512_mask(A, B, C, 8)\n@@ -721,8 +721,8 @@\n #define __builtin_ia32_getmantsh_mask_round(A, B, C, W, U, D) __builtin_ia32_getmantsh_mask_round(A, B, 1, W, U, 4)\n \n /* avx512fp16vlintrin.h */\n-#define __builtin_ia32_vcmpph_v8hf_mask(A, B, C, D) __builtin_ia32_vcmpph_v8hf_mask(A, B, 1, D)\n-#define __builtin_ia32_vcmpph_v16hf_mask(A, B, C, D) __builtin_ia32_vcmpph_v16hf_mask(A, B, 1, D)\n+#define __builtin_ia32_cmpph128_mask(A, B, C, D) __builtin_ia32_cmpph128_mask(A, B, 1, D)\n+#define __builtin_ia32_cmpph256_mask(A, B, C, D) __builtin_ia32_cmpph256_mask(A, B, 1, D)\n #define __builtin_ia32_fpclassph256_mask(A, D, C) __builtin_ia32_fpclassph256_mask(A, 1, C)\n #define __builtin_ia32_fpclassph128_mask(A, D, C) __builtin_ia32_fpclassph128_mask(A, 1, C)\n #define __builtin_ia32_getmantph256_mask(A, E, C, D) __builtin_ia32_getmantph256_mask(A, 1, C, D)"}, {"sha": "aafcd4145308f77eea202c5d3e4c0cceddd5fb6a", "filename": "gcc/testsuite/gcc.target/i386/sse-13.c", "status": "modified", "additions": 29, "deletions": 29, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-13.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-13.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-13.c?ref=e4d3643361df1145b34265c398e901498548c6e6", "patch": "@@ -703,33 +703,33 @@\n #define __builtin_ia32_vpshld_v2di_mask(A, B, C, D, E)  __builtin_ia32_vpshld_v2di_mask(A, B, 1, D, E)\n \n /* avx512fp16intrin.h */\n-#define __builtin_ia32_vaddph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vaddph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vsubph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vsubph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmulph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vmulph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vdivph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vdivph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vaddsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vaddsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vsubsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vsubsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmulsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vmulsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vdivsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vdivsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmaxph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vmaxph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vminph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vminph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmaxsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vmaxsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vminsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vminsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vcmpph_v32hf_mask(A, B, C, D) __builtin_ia32_vcmpph_v32hf_mask(A, B, 1, D)\n-#define __builtin_ia32_vcmpph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vcmpph_v32hf_mask_round(A, B, 1, D, 8)\n-#define __builtin_ia32_vcmpsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vcmpsh_v8hf_mask_round(A, B, 1, D, 8)\n-#define __builtin_ia32_vsqrtph_v32hf_mask_round(C, A, B, D) __builtin_ia32_vsqrtph_v32hf_mask_round(C, A, B, 8)\n-#define __builtin_ia32_vsqrtsh_v8hf_mask_round(D, C, A, B, E) __builtin_ia32_vsqrtsh_v8hf_mask_round(D, C, A, B, 8)\n-#define __builtin_ia32_vscalefph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vscalefph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vscalefsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vscalefsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vreduceph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vreduceph_v32hf_mask_round(A, 123, C, D, 8)\n-#define __builtin_ia32_vreduceph_v8hf_mask(A, B, C, D) __builtin_ia32_vreduceph_v8hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vreduceph_v16hf_mask(A, B, C, D) __builtin_ia32_vreduceph_v16hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vreducesh_v8hf_mask_round(A, B, C, D, E, F) __builtin_ia32_vreducesh_v8hf_mask_round(A, B, 123, D, E, 8)\n-#define __builtin_ia32_vrndscaleph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vrndscaleph_v32hf_mask_round(A, 123, C, D, 8)\n-#define __builtin_ia32_vrndscaleph_v8hf_mask(A, B, C, D) __builtin_ia32_vrndscaleph_v8hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vrndscaleph_v16hf_mask(A, B, C, D) __builtin_ia32_vrndscaleph_v16hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vrndscalesh_v8hf_mask_round(A, B, C, D, E, F) __builtin_ia32_vrndscalesh_v8hf_mask_round(A, B, 123, D, E, 8)\n+#define __builtin_ia32_addph512_mask_round(A, B, C, D, E) __builtin_ia32_addph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_subph512_mask_round(A, B, C, D, E) __builtin_ia32_subph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_mulph512_mask_round(A, B, C, D, E) __builtin_ia32_mulph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_divph512_mask_round(A, B, C, D, E) __builtin_ia32_divph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_addsh_mask_round(A, B, C, D, E) __builtin_ia32_addsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_subsh_mask_round(A, B, C, D, E) __builtin_ia32_subsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_mulsh_mask_round(A, B, C, D, E) __builtin_ia32_mulsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_divsh_mask_round(A, B, C, D, E) __builtin_ia32_divsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_maxph512_mask_round(A, B, C, D, E) __builtin_ia32_maxph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_minph512_mask_round(A, B, C, D, E) __builtin_ia32_minph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_maxsh_mask_round(A, B, C, D, E) __builtin_ia32_maxsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_minsh_mask_round(A, B, C, D, E) __builtin_ia32_minsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_cmpph512_mask(A, B, C, D) __builtin_ia32_cmpph512_mask(A, B, 1, D)\n+#define __builtin_ia32_cmpph512_mask_round(A, B, C, D, E) __builtin_ia32_cmpph512_mask_round(A, B, 1, D, 8)\n+#define __builtin_ia32_cmpsh_mask_round(A, B, C, D, E) __builtin_ia32_cmpsh_mask_round(A, B, 1, D, 8)\n+#define __builtin_ia32_sqrtph512_mask_round(C, A, B, D) __builtin_ia32_sqrtph512_mask_round(C, A, B, 8)\n+#define __builtin_ia32_sqrtsh_mask_round(D, C, A, B, E) __builtin_ia32_sqrtsh_mask_round(D, C, A, B, 8)\n+#define __builtin_ia32_scalefph512_mask_round(A, B, C, D, E) __builtin_ia32_scalefph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_scalefsh_mask_round(A, B, C, D, E) __builtin_ia32_scalefsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_reduceph512_mask_round(A, B, C, D, E) __builtin_ia32_reduceph512_mask_round(A, 123, C, D, 8)\n+#define __builtin_ia32_reduceph128_mask(A, B, C, D) __builtin_ia32_reduceph128_mask(A, 123, C, D)\n+#define __builtin_ia32_reduceph256_mask(A, B, C, D) __builtin_ia32_reduceph256_mask(A, 123, C, D)\n+#define __builtin_ia32_reducesh_mask_round(A, B, C, D, E, F) __builtin_ia32_reducesh_mask_round(A, B, 123, D, E, 8)\n+#define __builtin_ia32_rndscaleph512_mask_round(A, B, C, D, E) __builtin_ia32_rndscaleph512_mask_round(A, 123, C, D, 8)\n+#define __builtin_ia32_rndscaleph128_mask(A, B, C, D) __builtin_ia32_rndscaleph128_mask(A, 123, C, D)\n+#define __builtin_ia32_rndscaleph256_mask(A, B, C, D) __builtin_ia32_rndscaleph256_mask(A, 123, C, D)\n+#define __builtin_ia32_rndscalesh_mask_round(A, B, C, D, E, F) __builtin_ia32_rndscalesh_mask_round(A, B, 123, D, E, 8)\n #define __builtin_ia32_fpclassph512_mask(A, D, C) __builtin_ia32_fpclassph512_mask(A, 1, C)\n #define __builtin_ia32_fpclasssh_mask(A, D, U) __builtin_ia32_fpclasssh_mask(A, 1, U)\n #define __builtin_ia32_getexpph512_mask(A, B, C, D) __builtin_ia32_getexpph512_mask(A, B, C, 8)\n@@ -738,8 +738,8 @@\n #define __builtin_ia32_getmantsh_mask_round(A, B, C, W, U, D) __builtin_ia32_getmantsh_mask_round(A, B, 1, W, U, 4)\n \n /* avx512fp16vlintrin.h */\n-#define __builtin_ia32_vcmpph_v8hf_mask(A, B, C, D) __builtin_ia32_vcmpph_v8hf_mask(A, B, 1, D)\n-#define __builtin_ia32_vcmpph_v16hf_mask(A, B, C, D) __builtin_ia32_vcmpph_v16hf_mask(A, B, 1, D)\n+#define __builtin_ia32_cmpph128_mask(A, B, C, D) __builtin_ia32_cmpph128_mask(A, B, 1, D)\n+#define __builtin_ia32_cmpph256_mask(A, B, C, D) __builtin_ia32_cmpph256_mask(A, B, 1, D)\n #define __builtin_ia32_fpclassph256_mask(A, D, C) __builtin_ia32_fpclassph256_mask(A, 1, C)\n #define __builtin_ia32_fpclassph128_mask(A, D, C) __builtin_ia32_fpclassph128_mask(A, 1, C)\n #define __builtin_ia32_getmantph256_mask(A, E, C, D) __builtin_ia32_getmantph256_mask(A, 1, C, D)"}, {"sha": "8b600282c6729c8bcec11e0964c3234c29427974", "filename": "gcc/testsuite/gcc.target/i386/sse-23.c", "status": "modified", "additions": 29, "deletions": 29, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-23.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e4d3643361df1145b34265c398e901498548c6e6/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-23.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-23.c?ref=e4d3643361df1145b34265c398e901498548c6e6", "patch": "@@ -704,33 +704,33 @@\n #define __builtin_ia32_vpshld_v2di_mask(A, B, C, D, E)  __builtin_ia32_vpshld_v2di_mask(A, B, 1, D, E)\n \n /* avx512fp16intrin.h */\n-#define __builtin_ia32_vaddph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vaddph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vsubph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vsubph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmulph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vmulph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vdivph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vdivph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vaddsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vaddsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vsubsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vsubsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmulsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vmulsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vdivsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vdivsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmaxph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vmaxph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vminph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vminph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vmaxsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vmaxsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vminsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vminsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vcmpph_v32hf_mask(A, B, C, D) __builtin_ia32_vcmpph_v32hf_mask(A, B, 1, D)\n-#define __builtin_ia32_vcmpph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vcmpph_v32hf_mask_round(A, B, 1, D, 8)\n-#define __builtin_ia32_vcmpsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vcmpsh_v8hf_mask_round(A, B, 1, D, 8)\n-#define __builtin_ia32_vsqrtph_v32hf_mask_round(C, A, B, D) __builtin_ia32_vsqrtph_v32hf_mask_round(C, A, B, 8)\n-#define __builtin_ia32_vsqrtsh_v8hf_mask_round(D, C, A, B, E) __builtin_ia32_vsqrtsh_v8hf_mask_round(D, C, A, B, 8)\n-#define __builtin_ia32_vscalefph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vscalefph_v32hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vscalefsh_v8hf_mask_round(A, B, C, D, E) __builtin_ia32_vscalefsh_v8hf_mask_round(A, B, C, D, 8)\n-#define __builtin_ia32_vreduceph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vreduceph_v32hf_mask_round(A, 123, C, D, 8)\n-#define __builtin_ia32_vreduceph_v8hf_mask(A, B, C, D) __builtin_ia32_vreduceph_v8hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vreduceph_v16hf_mask(A, B, C, D) __builtin_ia32_vreduceph_v16hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vreducesh_v8hf_mask_round(A, B, C, D, E, F) __builtin_ia32_vreducesh_v8hf_mask_round(A, B, 123, D, E, 8)\n-#define __builtin_ia32_vrndscaleph_v32hf_mask_round(A, B, C, D, E) __builtin_ia32_vrndscaleph_v32hf_mask_round(A, 123, C, D, 8)\n-#define __builtin_ia32_vrndscaleph_v8hf_mask(A, B, C, D) __builtin_ia32_vrndscaleph_v8hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vrndscaleph_v16hf_mask(A, B, C, D) __builtin_ia32_vrndscaleph_v16hf_mask(A, 123, C, D)\n-#define __builtin_ia32_vrndscalesh_v8hf_mask_round(A, B, C, D, E, F) __builtin_ia32_vrndscalesh_v8hf_mask_round(A, B, 123, D, E, 8)\n+#define __builtin_ia32_addph512_mask_round(A, B, C, D, E) __builtin_ia32_addph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_subph512_mask_round(A, B, C, D, E) __builtin_ia32_subph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_mulph512_mask_round(A, B, C, D, E) __builtin_ia32_mulph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_divph512_mask_round(A, B, C, D, E) __builtin_ia32_divph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_addsh_mask_round(A, B, C, D, E) __builtin_ia32_addsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_subsh_mask_round(A, B, C, D, E) __builtin_ia32_subsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_mulsh_mask_round(A, B, C, D, E) __builtin_ia32_mulsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_divsh_mask_round(A, B, C, D, E) __builtin_ia32_divsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_maxph512_mask_round(A, B, C, D, E) __builtin_ia32_maxph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_minph512_mask_round(A, B, C, D, E) __builtin_ia32_minph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_maxsh_mask_round(A, B, C, D, E) __builtin_ia32_maxsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_minsh_mask_round(A, B, C, D, E) __builtin_ia32_minsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_cmpph512_mask(A, B, C, D) __builtin_ia32_cmpph512_mask(A, B, 1, D)\n+#define __builtin_ia32_cmpph512_mask_round(A, B, C, D, E) __builtin_ia32_cmpph512_mask_round(A, B, 1, D, 8)\n+#define __builtin_ia32_cmpsh_mask_round(A, B, C, D, E) __builtin_ia32_cmpsh_mask_round(A, B, 1, D, 8)\n+#define __builtin_ia32_sqrtph512_mask_round(C, A, B, D) __builtin_ia32_sqrtph512_mask_round(C, A, B, 8)\n+#define __builtin_ia32_sqrtsh_mask_round(D, C, A, B, E) __builtin_ia32_sqrtsh_mask_round(D, C, A, B, 8)\n+#define __builtin_ia32_scalefph512_mask_round(A, B, C, D, E) __builtin_ia32_scalefph512_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_scalefsh_mask_round(A, B, C, D, E) __builtin_ia32_scalefsh_mask_round(A, B, C, D, 8)\n+#define __builtin_ia32_reduceph512_mask_round(A, B, C, D, E) __builtin_ia32_reduceph512_mask_round(A, 123, C, D, 8)\n+#define __builtin_ia32_reduceph128_mask(A, B, C, D) __builtin_ia32_reduceph128_mask(A, 123, C, D)\n+#define __builtin_ia32_reduceph256_mask(A, B, C, D) __builtin_ia32_reduceph256_mask(A, 123, C, D)\n+#define __builtin_ia32_reducesh_mask_round(A, B, C, D, E, F) __builtin_ia32_reducesh_mask_round(A, B, 123, D, E, 8)\n+#define __builtin_ia32_rndscaleph512_mask_round(A, B, C, D, E) __builtin_ia32_rndscaleph512_mask_round(A, 123, C, D, 8)\n+#define __builtin_ia32_rndscaleph128_mask(A, B, C, D) __builtin_ia32_rndscaleph128_mask(A, 123, C, D)\n+#define __builtin_ia32_rndscaleph256_mask(A, B, C, D) __builtin_ia32_rndscaleph256_mask(A, 123, C, D)\n+#define __builtin_ia32_rndscalesh_mask_round(A, B, C, D, E, F) __builtin_ia32_rndscalesh_mask_round(A, B, 123, D, E, 8)\n #define __builtin_ia32_fpclassph512_mask(A, D, C) __builtin_ia32_fpclassph512_mask(A, 1, C)\n #define __builtin_ia32_fpclasssh_mask(A, D, U) __builtin_ia32_fpclasssh_mask(A, 1, U)\n #define __builtin_ia32_getexpph512_mask(A, B, C, D) __builtin_ia32_getexpph512_mask(A, B, C, 8)\n@@ -739,8 +739,8 @@\n #define __builtin_ia32_getmantsh_mask_round(A, B, C, W, U, D) __builtin_ia32_getmantsh_mask_round(A, B, 1, W, U, 4)\n \n /* avx512fp16vlintrin.h */\n-#define __builtin_ia32_vcmpph_v8hf_mask(A, B, C, D) __builtin_ia32_vcmpph_v8hf_mask(A, B, 1, D)\n-#define __builtin_ia32_vcmpph_v16hf_mask(A, B, C, D) __builtin_ia32_vcmpph_v16hf_mask(A, B, 1, D)\n+#define __builtin_ia32_cmpph128_mask(A, B, C, D) __builtin_ia32_cmpph128_mask(A, B, 1, D)\n+#define __builtin_ia32_cmpph256_mask(A, B, C, D) __builtin_ia32_cmpph256_mask(A, B, 1, D)\n #define __builtin_ia32_fpclassph256_mask(A, D, C) __builtin_ia32_fpclassph256_mask(A, 1, C)\n #define __builtin_ia32_fpclassph128_mask(A, D, C) __builtin_ia32_fpclassph128_mask(A, 1, C)\n #define __builtin_ia32_getmantph256_mask(A, E, C, D) __builtin_ia32_getmantph256_mask(A, 1, C, D)"}]}