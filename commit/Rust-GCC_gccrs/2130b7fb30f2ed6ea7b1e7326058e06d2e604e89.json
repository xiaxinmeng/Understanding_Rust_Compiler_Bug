{"sha": "2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjEzMGI3ZmIzMGYyZWQ2ZWE3YjFlNzMyNjA1OGUwNmQyZTYwNGU4OQ==", "commit": {"author": {"name": "Bernd Schmidt", "email": "bernds@redhat.com", "date": "2000-12-21T18:26:07Z"}, "committer": {"name": "Bernd Schmidt", "email": "bernds@gcc.gnu.org", "date": "2000-12-21T18:26:07Z"}, "message": "ia64 specific scheduling bits\n\nFrom-SVN: r38419", "tree": {"sha": "1117103e48ca7bedfe39afe37a9ddcaad38397de", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1117103e48ca7bedfe39afe37a9ddcaad38397de"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/comments", "author": null, "committer": null, "parents": [{"sha": "5f446d2172c1ca3e776b91a40127de9efa2f62d9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5f446d2172c1ca3e776b91a40127de9efa2f62d9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5f446d2172c1ca3e776b91a40127de9efa2f62d9"}], "stats": {"total": 1916, "additions": 1674, "deletions": 242}, "files": [{"sha": "3343904186b78a1a0f62b40f0cce87eb86e329e1", "filename": "gcc/ChangeLog", "status": "modified", "additions": 51, "deletions": 0, "changes": 51, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "patch": "@@ -1,3 +1,54 @@\n+2000-12-21  Bernd Schmidt  <bernds@redhat.com>\n+\n+\t* Makefile.in (out_object_file): Depend on sched-int.h.\n+\t* rtl.h (single_set_1): New macro.\n+\t(single_set_2): Renamed from single_set_1 and extra argument added.\n+\t* rtlanal.c (single_set_2): Likewise.\n+\n+\t* config/ia64/ia64-protos.h (get_bundle_name, ia64_issue_rate,\n+\tia64_adjust_cost, ia64_sched_init, ia64_sched_finish,\n+\tia64_sched_reorder, ia64_sched_reorder2, ia64_variable_issue):\n+\tDeclare.\n+\t* config/ia64/ia64.c: Include \"sched-int.h\".\n+\t(hard_regno_rename_ok): Also disallow renaming from the various\n+\treg_save_* regs.\n+\t(ia64_safe_itanium_requiers_unit0, ia64_safe_itanium_class,\n+\tia64_safe_type, init_insn_group_barriers, group_barrier_needed_p,\n+\tsafe_group_barrier_needed_p, fixup_errata): New static functions.\n+\t(rtx_needs_barrier):  Handle bundle selector and cycle display\n+\tinsns.\n+\t(emit_insn_group_barriers): Accept additional FILE * arg.  All\n+\tcallers changed.  Rework to only generate stop bits between\n+\tbasic blocks that haven't been inserted by scheduling.\n+\t(struct bundle, struct ia64_packet): New structures.\n+\t(NR_BUNDLES, NR_PACKETS): New macros.\n+\t(bundle, packets, type_names): New static arrays.\n+\t(ia64_final_schedule): New variable.\n+\t(ia64_single_set, insn_matches_slot, ia64_emit_insn_before,\n+\tgen_nop_type, finish_last_head, rotate_one_bundle, rotate_two_bundles,\n+\tcycle_end_fill_slots, packet_matches_p, get_split, find_best_insn,\n+\tfind_best_packet, itanium_reorder, dump_current_packet, schedule_stop):\n+\tNew static functions.\n+\t(ia64_issue_rate, ia64_sched_init, ia64_sched_reorder,\n+\tia64_sched_finish, ia64_sched_reorder2, ia64_variable_issue): New\n+\tfunctions.\n+\t(ia64_reorg): Perform a final scheduling pass.\n+\t* config/ia64/ia64.h (CONST_COSTS): Slightly increase SYMBOL_REF costs.\n+\t(MAX_CONDITIONAL_EXECUTE, ADJUST_COST, ISSUE_RATE, MD_SCHED_INIT,\n+\tMD_SCHED_REORDER, MD_SCHED_REORDER2, MD_SCHED_FINISH,\n+\tMD_SCHED_VARIABLE_ISSUE): Define macros.\n+\t(ia64_final_schedule): Declare variable.\n+\t* config/ia64/ia64.md (attr itanium_class): Add some more classes.\n+\t(attr type): Account for them.\n+\t(itanium_requires_unit0): New attribute.\n+\t(function units): Rewrite.\n+\t(some splitters): Don't create scheduling barriers here.\n+\t(gr_spill_internal, gr_restore_internal): Don't predicate the\n+\tpseudo-op.\n+\t(nop_m, nop_i, nop_f, nop_b, nop_x, cycle_display, cycle_display_1,\n+\tbundle_selector): New patterns.\n+\t(insn_group_barrier): Now has an operand.\n+\t\n 2000-12-21  DJ Delorie  <dj@redhat.com>\n \n \t* dwarf2out.c (simple_decl_align_in_bits): new"}, {"sha": "96394578d20df2c4f73ce61e7a13700328109ca6", "filename": "gcc/Makefile.in", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "patch": "@@ -1499,7 +1499,8 @@ dependence.o : dependence.c $(CONFIG_H) system.h $(RTL_H) $(TREE_H) \\\n \n $(out_object_file): $(out_file) $(CONFIG_H) $(TREE_H) $(GGC_H) \\\n    $(RTL_H) $(REGS_H) hard-reg-set.h real.h insn-config.h conditions.h \\\n-   insn-flags.h output.h $(INSN_ATTR_H) insn-codes.h system.h toplev.h function.h\n+   insn-flags.h output.h $(INSN_ATTR_H) insn-codes.h system.h toplev.h \\\n+   function.h sched-int.h\n \t$(CC) -c $(ALL_CFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \\\n \t\t$(out_file) $(OUTPUT_OPTION)\n "}, {"sha": "cb076c779a48cb6b425ea80e18df10f8c51bff65", "filename": "gcc/config/ia64/ia64-protos.h", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Fconfig%2Fia64%2Fia64-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Fconfig%2Fia64%2Fia64-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fia64%2Fia64-protos.h?ref=2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "patch": "@@ -92,6 +92,14 @@ extern enum reg_class ia64_secondary_reload_class PARAMS((enum reg_class,\n \t\t\t\t\t\t\t  rtx));\n extern void ia64_reorg PARAMS((rtx));\n extern void process_for_unwind_directive PARAMS ((FILE *, rtx));\n+extern const char *get_bundle_name PARAMS ((int));\n+extern int ia64_issue_rate PARAMS ((void));\n+extern int ia64_adjust_cost PARAMS ((rtx, rtx, rtx, int));\n+extern void ia64_sched_init PARAMS ((FILE *, int, int));\n+extern void ia64_sched_finish PARAMS ((FILE *, int));\n+extern int ia64_sched_reorder PARAMS ((FILE *, int, rtx *, int *, int));\n+extern int ia64_sched_reorder2 PARAMS ((FILE *, int, rtx *, int *, int));\n+extern int ia64_variable_issue PARAMS ((FILE *, int, rtx, int));\n #endif /* RTX_CODE */\n \n #ifdef TREE_CODE"}, {"sha": "3478883d1a7cd54a15d9f8219dcedb8396d76c6d", "filename": "gcc/config/ia64/ia64.c", "status": "modified", "additions": 1428, "deletions": 162, "changes": 1590, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Fconfig%2Fia64%2Fia64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Fconfig%2Fia64%2Fia64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fia64%2Fia64.c?ref=2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "patch": "@@ -42,6 +42,7 @@ Boston, MA 02111-1307, USA.  */\n #include \"ggc.h\"\n #include \"basic-block.h\"\n #include \"toplev.h\"\n+#include \"sched-int.h\"\n \n /* This is used for communication between ASM_OUTPUT_LABEL and\n    ASM_OUTPUT_LABELREF.  */\n@@ -114,7 +115,7 @@ static void fix_range PARAMS ((const char *));\n static void ia64_add_gc_roots PARAMS ((void));\n static void ia64_init_machine_status PARAMS ((struct function *));\n static void ia64_mark_machine_status PARAMS ((struct function *));\n-static void emit_insn_group_barriers PARAMS ((rtx));\n+static void emit_insn_group_barriers PARAMS ((FILE *, rtx));\n static void emit_predicate_relation_info PARAMS ((void));\n static int process_set PARAMS ((FILE *, rtx));\n \n@@ -127,7 +128,6 @@ static rtx ia64_expand_compare_and_swap PARAMS ((enum machine_mode, int,\n static rtx ia64_expand_lock_test_and_set PARAMS ((enum machine_mode,\n \t\t\t\t\t\t  tree, rtx));\n static rtx ia64_expand_lock_release PARAMS ((enum machine_mode, tree, rtx));\n-\n \f\n /* Return 1 if OP is a valid operand for the MEM of a CALL insn.  */\n \n@@ -2401,6 +2401,14 @@ ia64_hard_regno_rename_ok (from, to)\n       || to == current_frame_info.reg_save_ar_lc)\n     return 0;\n \n+  if (from == current_frame_info.reg_fp\n+      || from == current_frame_info.reg_save_b0\n+      || from == current_frame_info.reg_save_pr\n+      || from == current_frame_info.reg_save_ar_pfs\n+      || from == current_frame_info.reg_save_ar_unat\n+      || from == current_frame_info.reg_save_ar_lc)\n+    return 0;\n+\n   /* Don't use output registers outside the register frame.  */\n   if (OUT_REGNO_P (to) && to >= OUT_REG (current_frame_info.n_output_regs))\n     return 0;\n@@ -3674,6 +3682,40 @@ ia64_override_options ()\n   ia64_add_gc_roots ();\n }\n \f\n+static enum attr_itanium_requires_unit0 ia64_safe_itanium_requires_unit0 PARAMS((rtx));\n+static enum attr_itanium_class ia64_safe_itanium_class PARAMS((rtx));\n+static enum attr_type ia64_safe_type PARAMS((rtx));\n+\n+static enum attr_itanium_requires_unit0\n+ia64_safe_itanium_requires_unit0 (insn)\n+     rtx insn;\n+{\n+  if (recog_memoized (insn) >= 0)\n+    return get_attr_itanium_requires_unit0 (insn);\n+  else\n+    return ITANIUM_REQUIRES_UNIT0_NO;\n+}\n+\n+static enum attr_itanium_class\n+ia64_safe_itanium_class (insn)\n+     rtx insn;\n+{\n+  if (recog_memoized (insn) >= 0)\n+    return get_attr_itanium_class (insn);\n+  else\n+    return ITANIUM_CLASS_UNKNOWN;\n+}\n+\n+static enum attr_type\n+ia64_safe_type (insn)\n+     rtx insn;\n+{\n+  if (recog_memoized (insn) >= 0)\n+    return get_attr_type (insn);\n+  else\n+    return TYPE_UNKNOWN;\n+}\n+\f\n /* The following collection of routines emit instruction group stop bits as\n    necessary to avoid dependencies.  */\n \n@@ -3744,6 +3786,9 @@ static void rws_update PARAMS ((struct reg_write_state *, int,\n static int rws_access_regno PARAMS ((int, struct reg_flags, int));\n static int rws_access_reg PARAMS ((rtx, struct reg_flags, int));\n static int rtx_needs_barrier PARAMS ((rtx, struct reg_flags, int));\n+static void init_insn_group_barriers PARAMS ((void));\n+static int group_barrier_needed_p PARAMS ((rtx));\n+static int safe_group_barrier_needed_p PARAMS ((rtx));\n \n /* Update *RWS for REGNO, which is being written by the current instruction,\n    with predicate PRED, and associated register flags in FLAGS.  */\n@@ -4189,6 +4234,8 @@ rtx_needs_barrier (x, flags, pred)\n         case 19: /* fetchadd_acq */\n \tcase 20: /* mov = ar.bsp */\n \tcase 21: /* flushrs */\n+\tcase 22: /* bundle selector */\n+\tcase 23: /* cycle display */\n           break;\n \n \tcase 5: /* recip_approx */\n@@ -4279,6 +4326,179 @@ rtx_needs_barrier (x, flags, pred)\n   return need_barrier;\n }\n \n+/* Clear out the state for group_barrier_needed_p at the start of a\n+   sequence of insns.  */\n+\n+static void\n+init_insn_group_barriers ()\n+{\n+  memset (rws_sum, 0, sizeof (rws_sum));\n+}\n+\n+/* Cumulative info for the current instruction group.  */\n+struct reg_write_state rws_sum[NUM_REGS];\n+\n+/* Given the current state, recorded by previous calls to this function,\n+   determine whether a group barrier (a stop bit) is necessary before INSN.\n+   Return nonzero if so.  */\n+\n+static int\n+group_barrier_needed_p (insn)\n+     rtx insn;\n+{\n+  rtx pat;\n+  int need_barrier = 0;\n+  struct reg_flags flags;\n+\n+  memset (&flags, 0, sizeof (flags));\n+  switch (GET_CODE (insn))\n+    {\n+    case NOTE:\n+      break;\n+\n+    case BARRIER:\n+      /* A barrier doesn't imply an instruction group boundary.  */\n+      break;\n+\n+    case CODE_LABEL:\n+      memset (rws_insn, 0, sizeof (rws_insn));\n+      return 1;\n+\n+    case CALL_INSN:\n+      flags.is_branch = 1;\n+      flags.is_sibcall = SIBLING_CALL_P (insn);\n+      memset (rws_insn, 0, sizeof (rws_insn));\n+      need_barrier = rtx_needs_barrier (PATTERN (insn), flags, 0);\n+      break;\n+\n+    case JUMP_INSN:\n+      flags.is_branch = 1;\n+      /* FALLTHRU */\n+\n+    case INSN:\n+      if (GET_CODE (PATTERN (insn)) == USE\n+\t  || GET_CODE (PATTERN (insn)) == CLOBBER)\n+\t/* Don't care about USE and CLOBBER \"insns\"---those are used to\n+\t   indicate to the optimizer that it shouldn't get rid of\n+\t   certain operations.  */\n+\tbreak;\n+\n+      pat = PATTERN (insn);\n+\n+      /* Ug.  Hack hacks hacked elsewhere.  */\n+      switch (recog_memoized (insn))\n+\t{\n+\t  /* We play dependency tricks with the epilogue in order\n+\t     to get proper schedules.  Undo this for dv analysis.  */\n+\tcase CODE_FOR_epilogue_deallocate_stack:\n+\t  pat = XVECEXP (pat, 0, 0);\n+\t  break;\n+\n+\t  /* The pattern we use for br.cloop confuses the code above.\n+\t     The second element of the vector is representative.  */\n+\tcase CODE_FOR_doloop_end_internal:\n+\t  pat = XVECEXP (pat, 0, 1);\n+\t  break;\n+\n+\t  /* Doesn't generate code.  */\n+\tcase CODE_FOR_pred_rel_mutex:\n+\t  return 0;\n+\n+\tdefault:\n+\t  break;\n+\t}\n+\n+      memset (rws_insn, 0, sizeof (rws_insn));\n+      need_barrier = rtx_needs_barrier (pat, flags, 0);\n+\n+      /* Check to see if the previous instruction was a volatile\n+\t asm.  */\n+      if (! need_barrier)\n+\tneed_barrier = rws_access_regno (REG_VOLATILE, flags, 0);\n+\n+      break;\n+\n+    default:\n+      abort ();\n+    }\n+  return need_barrier;\n+}\n+\n+/* Like group_barrier_needed_p, but do not clobber the current state.  */\n+\n+static int\n+safe_group_barrier_needed_p (insn)\n+     rtx insn;\n+{\n+  struct reg_write_state rws_saved[NUM_REGS];\n+  int t;\n+  memcpy (rws_saved, rws_sum, NUM_REGS * sizeof *rws_saved);\n+  t = group_barrier_needed_p (insn);\n+  memcpy (rws_sum, rws_saved, NUM_REGS * sizeof *rws_saved);\n+  return t;\n+}\n+\n+/* INSNS is an chain of instructions.  Scan the chain, and insert stop bits\n+   as necessary to eliminate dependendencies.  */\n+\n+static void\n+emit_insn_group_barriers (dump, insns)\n+     FILE *dump;\n+     rtx insns;\n+{\n+  rtx insn;\n+  rtx last_label = 0;\n+  int insns_since_last_label = 0;\n+\n+  init_insn_group_barriers ();\n+\n+  for (insn = insns; insn; insn = NEXT_INSN (insn))\n+    {\n+      if (GET_CODE (insn) == CODE_LABEL)\n+\t{\n+\t  if (insns_since_last_label)\n+\t    last_label = insn;\n+\t  insns_since_last_label = 0;\n+\t}\n+      else if (GET_CODE (insn) == NOTE\n+\t       && NOTE_LINE_NUMBER (insn) == NOTE_INSN_BASIC_BLOCK)\n+\t{\n+\t  if (insns_since_last_label)\n+\t    last_label = insn;\n+\t  insns_since_last_label = 0;\n+\t}\n+      else if (GET_CODE (insn) == INSN\n+\t       && GET_CODE (PATTERN (insn)) == UNSPEC_VOLATILE\n+\t       && XINT (PATTERN (insn), 1) == 2)\n+\t{\n+\t  init_insn_group_barriers ();\n+\t  last_label = 0;\n+\t}\n+      else if (INSN_P (insn))\n+\t{\n+\t  insns_since_last_label = 1;\n+\n+\t  if (group_barrier_needed_p (insn))\n+\t    {\n+\t      if (last_label)\n+\t\t{\n+\t\t  if (dump)\n+\t\t    fprintf (dump, \"Emitting stop before label %d\\n\",\n+\t\t\t     INSN_UID (last_label));\n+\t\t  emit_insn_before (gen_insn_group_barrier (GEN_INT (3)), last_label);\n+\t\t  insn = last_label;\n+\t\t}\n+\t      init_insn_group_barriers ();\n+\t      last_label = 0;\n+\t    }\n+\t}\n+    }\n+}\n+\f\n+static int errata_find_address_regs PARAMS ((rtx *, void *));\n+static void errata_emit_nops PARAMS ((rtx));\n+static void fixup_errata PARAMS ((void));\n+\n /* This structure is used to track some details about the previous insns\n    groups so we can determine if it may be necessary to insert NOPs to\n    workaround hardware errata.  */\n@@ -4291,20 +4511,6 @@ static struct group\n /* Index into the last_group array.  */\n static int group_idx;\n \n-static void emit_group_barrier_after PARAMS ((rtx));\n-static int errata_find_address_regs PARAMS ((rtx *, void *));\n-static void errata_emit_nops PARAMS ((rtx));\n-\n-/* Create a new group barrier, emit it after AFTER, and advance group_idx.  */\n-static void\n-emit_group_barrier_after (after)\n-     rtx after;\n-{\n-  emit_insn_after (gen_insn_group_barrier (), after);\n-  group_idx = (group_idx + 1) % 3;\n-  memset (last_group + group_idx, 0, sizeof last_group[group_idx]);\n-}\n-\n /* Called through for_each_rtx; determines if a hard register that was\n    conditionally set in the previous group is used as an address register.\n    It ensures that for_each_rtx returns 1 in that case.  */\n@@ -4395,194 +4601,1246 @@ errata_emit_nops (insn)\n     }\n   if (for_each_rtx (&real_pat, errata_find_address_regs, NULL))\n     {\n-      emit_insn_before (gen_insn_group_barrier (), insn);\n+      emit_insn_before (gen_insn_group_barrier (GEN_INT (3)), insn);\n       emit_insn_before (gen_nop (), insn);\n-      emit_insn_before (gen_insn_group_barrier (), insn);\n+      emit_insn_before (gen_insn_group_barrier (GEN_INT (3)), insn);\n     }\n }\n \n-/* INSNS is an chain of instructions.  Scan the chain, and insert stop bits\n-   as necessary to eliminate dependendencies.  */\n+/* Emit extra nops if they are required to work around hardware errata.  */\n \n static void\n-emit_insn_group_barriers (insns)\n-     rtx insns;\n+fixup_errata ()\n {\n-  rtx insn, prev_insn;\n-\n-  memset (rws_sum, 0, sizeof (rws_sum));\n+  rtx insn;\n \n   group_idx = 0;\n   memset (last_group, 0, sizeof last_group);\n \n-  prev_insn = 0;\n-  for (insn = insns; insn; insn = NEXT_INSN (insn))\n+  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n     {\n-      int need_barrier = 0;\n-      struct reg_flags flags;\n-\n+      if (INSN_P (insn) && ia64_safe_type (insn) == TYPE_S)\n+\t{\n+\t  group_idx = (group_idx + 1) % 3;\n+\t  memset (last_group + group_idx, 0, sizeof last_group[group_idx]);\n+\t}\n       if ((TARGET_B_STEP || TARGET_A_STEP) && INSN_P (insn))\n \terrata_emit_nops (insn);\n+    }\n+}\n+\f\n+/* Instruction scheduling support.  */\n+/* Describe one bundle.  */\n+\n+struct bundle\n+{\n+  /* Zero if there's no possibility of a stop in this bundle other than\n+     at the end, otherwise the position of the optional stop bit.  */\n+  int possible_stop;\n+  /* The types of the three slots.  */\n+  enum attr_type t[3];\n+  /* The pseudo op to be emitted into the assembler output.  */\n+  const char *name;\n+};\n \n-      memset (&flags, 0, sizeof (flags));\n-      switch (GET_CODE (insn))\n+#define NR_BUNDLES 10\n+\n+/* A list of all available bundles.  */\n+\n+static const struct bundle bundle[NR_BUNDLES] =\n+{\n+  { 2, { TYPE_M, TYPE_I, TYPE_I }, \".mii\" },\n+  { 1, { TYPE_M, TYPE_M, TYPE_I }, \".mmi\" },\n+  { 0, { TYPE_M, TYPE_F, TYPE_I }, \".mfi\" },\n+  { 0, { TYPE_M, TYPE_M, TYPE_F }, \".mmf\" },\n+#if NR_BUNDLES == 10\n+  { 0, { TYPE_B, TYPE_B, TYPE_B }, \".bbb\" },\n+  { 0, { TYPE_M, TYPE_B, TYPE_B }, \".mbb\" },\n+#endif\n+  { 0, { TYPE_M, TYPE_I, TYPE_B }, \".mib\" },\n+  { 0, { TYPE_M, TYPE_M, TYPE_B }, \".mmb\" },\n+  { 0, { TYPE_M, TYPE_F, TYPE_B }, \".mfb\" },\n+  /* .mfi needs to occur earlier than .mlx, so that we only generate it if\n+     it matches an L type insn.  Otherwise we'll try to generate L type\n+     nops.  */\n+  { 0, { TYPE_M, TYPE_L, TYPE_X }, \".mlx\" }\n+};\n+\n+/* Describe a packet of instructions.  Packets consist of two bundles that\n+   are visible to the hardware in one scheduling window.  */\n+\n+struct ia64_packet\n+{\n+  const struct bundle *t1, *t2;\n+  /* Precomputed value of the first split issue in this packet if a cycle\n+     starts at its beginning.  */\n+  int first_split;\n+  /* For convenience, the insn types are replicated here so we don't have\n+     to go through T1 and T2 all the time.  */\n+  enum attr_type t[6];\n+};\n+\n+/* An array containing all possible packets.  */\n+#define NR_PACKETS (NR_BUNDLES * NR_BUNDLES)\n+static struct ia64_packet packets[NR_PACKETS];\n+\n+/* Map attr_type to a string with the name.  */\n+\n+static const char *type_names[] =\n+{\n+  \"UNKNOWN\", \"A\", \"I\", \"M\", \"F\", \"B\", \"L\", \"X\", \"S\"\n+};\n+\n+/* Nonzero if we should insert stop bits into the schedule.  */\n+int ia64_final_schedule = 0;\n+\n+static rtx ia64_single_set PARAMS ((rtx));\n+static int insn_matches_slot PARAMS ((const struct ia64_packet *, enum attr_type, int, rtx));\n+static void ia64_emit_insn_before PARAMS ((rtx, rtx));\n+static rtx gen_nop_type PARAMS ((enum attr_type));\n+static void finish_last_head PARAMS ((FILE *, int));\n+static void rotate_one_bundle PARAMS ((FILE *));\n+static void rotate_two_bundles PARAMS ((FILE *));\n+static void cycle_end_fill_slots PARAMS ((FILE *));\n+static int packet_matches_p PARAMS ((const struct ia64_packet *, int, int *));\n+static int get_split PARAMS ((const struct ia64_packet *, int));\n+static int find_best_insn PARAMS ((rtx *, enum attr_type *, int,\n+\t\t\t\t   const struct ia64_packet *, int));\n+static void find_best_packet PARAMS ((int *, const struct ia64_packet **,\n+\t\t\t\t      rtx *, enum attr_type *, int));\n+static int itanium_reorder PARAMS ((FILE *, rtx *, rtx *, int));\n+static void dump_current_packet PARAMS ((FILE *));\n+static void schedule_stop PARAMS ((FILE *));\n+\n+/* Map a bundle number to its pseudo-op.  */\n+\n+const char *\n+get_bundle_name (b)\n+     int b;\n+{\n+  return bundle[b].name;\n+}\n+\n+/* Compute the slot which will cause a split issue in packet P if the\n+   current cycle begins at slot BEGIN.  */\n+\n+static int\n+itanium_split_issue (p, begin)\n+     const struct ia64_packet *p;\n+     int begin;\n+{\n+  int type_count[TYPE_S];\n+  int i;\n+  int split = 6;\n+\n+  if (begin < 3)\n+    {\n+      /* Always split before and after MMF.  */\n+      if (p->t[0] == TYPE_M && p->t[1] == TYPE_M && p->t[2] == TYPE_F)\n+\treturn 3;\n+      if (p->t[3] == TYPE_M && p->t[4] == TYPE_M && p->t[5] == TYPE_F)\n+\treturn 3;\n+      /* Always split after MBB and BBB.  */\n+      if (p->t[1] == TYPE_B)\n+\treturn 3;\n+      /* Split after first bundle in MIB BBB combination.  */\n+      if (p->t[2] == TYPE_B && p->t[3] == TYPE_B)\n+\treturn 3;\n+    }\n+\n+  memset (type_count, 0, sizeof type_count);\n+  for (i = begin; i < split; i++)\n+    {\n+      enum attr_type t0 = p->t[i];\n+      /* An MLX bundle reserves the same units as an MFI bundle.  */\n+      enum attr_type t = (t0 == TYPE_L ? TYPE_F\n+\t\t\t  : t0 == TYPE_X ? TYPE_I\n+\t\t\t  : t0);\n+      int max = (t == TYPE_B ? 3 : t == TYPE_F ? 1 : 2);\n+      if (type_count[t] == max)\n+\treturn i;\n+      type_count[t]++;\n+    }\n+  return split;\n+}\n+\n+/* Return the maximum number of instructions a cpu can issue.  */\n+\n+int\n+ia64_issue_rate ()\n+{\n+  return 6;\n+}\n+\n+/* Helper function - like single_set, but look inside COND_EXEC.  */\n+\n+static rtx\n+ia64_single_set (insn)\n+     rtx insn;\n+{\n+  rtx x = PATTERN (insn);\n+  if (GET_CODE (x) == COND_EXEC)\n+    x = COND_EXEC_CODE (x);\n+  if (GET_CODE (x) == SET)\n+    return x;\n+  return single_set_2 (insn, x);\n+}\n+\n+/* Adjust the cost of a scheduling dependency.  Return the new cost of\n+   a dependency LINK or INSN on DEP_INSN.  COST is the current cost.  */\n+\n+int\n+ia64_adjust_cost (insn, link, dep_insn, cost)\n+     rtx insn, link, dep_insn;\n+     int cost;\n+{\n+  enum attr_type dep_type;\n+  enum attr_itanium_class dep_class;\n+  enum attr_itanium_class insn_class;\n+  rtx dep_set, set, src, addr;\n+\n+  if (GET_CODE (PATTERN (insn)) == CLOBBER\n+      || GET_CODE (PATTERN (insn)) == USE\n+      || GET_CODE (PATTERN (dep_insn)) == CLOBBER\n+      || GET_CODE (PATTERN (dep_insn)) == USE\n+      /* @@@ Not accurate for indirect calls.  */\n+      || GET_CODE (insn) == CALL_INSN\n+      || ia64_safe_type (insn) == TYPE_S)\n+    return 0;\n+\n+  if (REG_NOTE_KIND (link) == REG_DEP_OUTPUT\n+      || REG_NOTE_KIND (link) == REG_DEP_ANTI)\n+    return 0;\n+\n+  dep_type = ia64_safe_type (dep_insn);\n+  dep_class = ia64_safe_itanium_class (dep_insn);\n+  insn_class = ia64_safe_itanium_class (insn);\n+\n+  /* Compares that feed a conditional branch can execute in the same\n+     cycle.  */\n+  dep_set = ia64_single_set (dep_insn);\n+  set = ia64_single_set (insn);\n+\n+  if (dep_type != TYPE_F\n+      && dep_set\n+      && GET_CODE (SET_DEST (dep_set)) == REG\n+      && PR_REG (REGNO (SET_DEST (dep_set)))\n+      && GET_CODE (insn) == JUMP_INSN)\n+    return 0;\n+\n+  if (dep_set && GET_CODE (SET_DEST (dep_set)) == MEM)\n+    {\n+      /* ??? Can't find any information in the documenation about whether\n+\t a sequence\n+\t   st [rx] = ra\n+\t   ld rb = [ry]\n+\t splits issue.  Assume it doesn't.  */\n+      return 0;\n+    }\n+\n+  src = set ? SET_SRC (set) : 0;\n+  addr = 0;\n+  if (set && GET_CODE (SET_DEST (set)) == MEM)\n+    addr = XEXP (SET_DEST (set), 0);\n+  else if (set && GET_CODE (src) == MEM)\n+    addr = XEXP (src, 0);\n+  else if (set && GET_CODE (src) == ZERO_EXTEND\n+\t   && GET_CODE (XEXP (src, 0)) == MEM)\n+    addr = XEXP (XEXP (src, 0), 0);\n+  else if (set && GET_CODE (src) == UNSPEC\n+\t   && XVECLEN (XEXP (src, 0), 0) > 0\n+\t   && GET_CODE (XVECEXP (src, 0, 0)) == MEM)\n+    addr = XEXP (XVECEXP (src, 0, 0), 0);\n+  if (addr && GET_CODE (addr) == POST_MODIFY)\n+    addr = XEXP (addr, 0);\n+\n+  set = ia64_single_set (dep_insn);\n+\n+  if ((dep_class == ITANIUM_CLASS_IALU\n+       || dep_class == ITANIUM_CLASS_ILOG\n+       || dep_class == ITANIUM_CLASS_LD)\n+      && (insn_class == ITANIUM_CLASS_LD\n+\t  || insn_class == ITANIUM_CLASS_ST))\n+    {\n+      if (! addr || ! set)\n+\tabort ();\n+      /* This isn't completely correct - an IALU that feeds an address has\n+\t a latency of 1 cycle if it's issued in an M slot, but 2 cycles\n+\t otherwise.  Unfortunately there's no good way to describe this.  */\n+      if (reg_overlap_mentioned_p (SET_DEST (set), addr))\n+\treturn cost + 1;\n+    }\n+  if ((dep_class == ITANIUM_CLASS_IALU\n+       || dep_class == ITANIUM_CLASS_ILOG\n+       || dep_class == ITANIUM_CLASS_LD)\n+      && (insn_class == ITANIUM_CLASS_MMMUL\n+\t  || insn_class == ITANIUM_CLASS_MMSHF\n+\t  || insn_class == ITANIUM_CLASS_MMSHFI))\n+    return 3;\n+  if (dep_class == ITANIUM_CLASS_FMAC\n+      && (insn_class == ITANIUM_CLASS_FMISC\n+\t  || insn_class == ITANIUM_CLASS_FCVTFX\n+\t  || insn_class == ITANIUM_CLASS_XMPY))\n+    return 7;\n+  if ((dep_class == ITANIUM_CLASS_FMAC\n+       || dep_class == ITANIUM_CLASS_FMISC\n+       || dep_class == ITANIUM_CLASS_FCVTFX\n+       || dep_class == ITANIUM_CLASS_XMPY)\n+      && insn_class == ITANIUM_CLASS_STF)\n+    return 8;\n+  if ((dep_class == ITANIUM_CLASS_MMMUL\n+       || dep_class == ITANIUM_CLASS_MMSHF\n+       || dep_class == ITANIUM_CLASS_MMSHFI)\n+      && (insn_class == ITANIUM_CLASS_LD\n+\t  || insn_class == ITANIUM_CLASS_ST\n+\t  || insn_class == ITANIUM_CLASS_IALU\n+\t  || insn_class == ITANIUM_CLASS_ILOG\n+\t  || insn_class == ITANIUM_CLASS_ISHF))\n+    return 4;\n+\n+  return cost;\n+}\n+\n+/* Describe the current state of the Itanium pipeline.  */\n+static struct\n+{\n+  /* The first slot that is used in the current cycle.  */\n+  int first_slot;\n+  /* The next slot to fill.  */\n+  int cur;\n+  /* The packet we have selected for the current issue window.  */\n+  const struct ia64_packet *packet;\n+  /* The position of the split issue that occurs due to issue width\n+     limitations (6 if there's no split issue).  */\n+  int split;\n+  /* Record data about the insns scheduled so far in the same issue\n+     window.  The elements up to but not including FIRST_SLOT belong\n+     to the previous cycle, the ones starting with FIRST_SLOT belong\n+     to the current cycle.  */\n+  enum attr_type types[6];\n+  rtx insns[6];\n+  int stopbit[6];\n+  /* Nonzero if we decided to schedule a stop bit.  */\n+  int last_was_stop;\n+} sched_data;\n+\n+/* Temporary arrays; they have enough elements to hold all insns that\n+   can be ready at the same time while scheduling of the current block.\n+   SCHED_READY can hold ready insns, SCHED_TYPES their types.  */\n+static rtx *sched_ready;\n+static enum attr_type *sched_types;\n+\n+/* Determine whether an insn INSN of type ITYPE can fit into slot SLOT\n+   of packet P.  */\n+\n+static int\n+insn_matches_slot (p, itype, slot, insn)\n+     const struct ia64_packet *p;\n+     enum attr_type itype;\n+     int slot;\n+     rtx insn;\n+{\n+  enum attr_itanium_requires_unit0 u0;\n+  enum attr_type stype = p->t[slot];\n+\n+  if (insn)\n+    {\n+      u0 = ia64_safe_itanium_requires_unit0 (insn);\n+      if (u0 == ITANIUM_REQUIRES_UNIT0_YES)\n \t{\n-\tcase NOTE:\n-\t  /* For very small loops we can wind up with extra stop bits\n-\t     inside the loop because of not putting a stop after the\n-\t     assignment to ar.lc before the loop label.  */\n-\t  /* ??? Ideally we'd do this for any register used in the first\n-\t     insn group that's been written recently.  */\n-          if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_BEG)\n+\t  int i;\n+\t  for (i = sched_data.first_slot; i < slot; i++)\n+\t    if (p->t[i] == stype)\n+\t      return 0;\n+\t}\n+      if (GET_CODE (insn) == CALL_INSN)\n+\t{\n+\t  /* Reject calls in multiway branch packets.  We want to limit\n+\t     the number of multiway branches we generate (since the branch\n+\t     predictor is limited), and this seems to work fairly well.\n+\t     (If we didn't do this, we'd have to add another test here to\n+\t     force calls into the third slot of the bundle.)  */\n+\t  if (slot < 3)\n \t    {\n-\t      need_barrier = rws_access_regno (AR_LC_REGNUM, flags, 0);\n-\t      if (need_barrier)\n-\t\t{\n-\t\t  emit_group_barrier_after (insn);\n-\t\t  memset (rws_sum, 0, sizeof(rws_sum));\n-\t\t  prev_insn = NULL_RTX;\n-\t\t}\n+\t      if (p->t[1] == TYPE_B)\n+\t\treturn 0;\n \t    }\n-\t  break;\n+\t  else\n+\t    {\n+\t      if (p->t[4] == TYPE_B)\n+\t\treturn 0;\n+\t    }\n+\t}\n+    }\n+\n+  if (itype == stype)\n+    return 1;\n+  if (itype == TYPE_A)\n+    return stype == TYPE_M || stype == TYPE_I;\n+  return 0;\n+}\n+\n+/* Like emit_insn_before, but skip cycle_display insns.  This makes the\n+   assembly output a bit prettier.  */\n+\n+static void\n+ia64_emit_insn_before (insn, before)\n+     rtx insn, before;\n+{\n+  rtx prev = PREV_INSN (before);\n+  if (prev && GET_CODE (prev) == INSN\n+      && GET_CODE (PATTERN (prev)) == UNSPEC\n+      && XINT (PATTERN (prev), 1) == 23)\n+    before = prev;\n+  emit_insn_before (insn, before);\n+}\n+\n+/* Generate a nop insn of the given type.  Note we never generate L type\n+   nops.  */\n+\n+static rtx\n+gen_nop_type (t)\n+     enum attr_type t;\n+{\n+  switch (t)\n+    {\n+    case TYPE_M:\n+      return gen_nop_m ();\n+    case TYPE_I:\n+      return gen_nop_i ();\n+    case TYPE_B:\n+      return gen_nop_b ();\n+    case TYPE_F:\n+      return gen_nop_f ();\n+    case TYPE_X:\n+      return gen_nop_x ();\n+    default:\n+      abort ();\n+    }\n+}\n \n-\tcase CALL_INSN:\n-\t  flags.is_branch = 1;\n-\t  flags.is_sibcall = SIBLING_CALL_P (insn);\n-\t  memset (rws_insn, 0, sizeof (rws_insn));\n-\t  need_barrier = rtx_needs_barrier (PATTERN (insn), flags, 0);\n+/* When rotating a bundle out of the issue window, insert a bundle selector\n+   insn in front of it.  DUMP is the scheduling dump file or NULL.  START\n+   is either 0 or 3, depending on whether we want to emit a bundle selector\n+   for the first bundle or the second bundle in the current issue window.\n \n-\t  if (need_barrier)\n+   The selector insns are emitted this late because the selected packet can\n+   be changed until parts of it get rotated out.  */\n+\n+static void\n+finish_last_head (dump, start)\n+     FILE *dump;\n+     int start;\n+{\n+  const struct ia64_packet *p = sched_data.packet;\n+  const struct bundle *b = start == 0 ? p->t1 : p->t2;\n+  int bundle_type = b - bundle;\n+  rtx insn;\n+  int i;\n+\n+  if (! ia64_final_schedule)\n+    return;\n+\n+  for (i = start; sched_data.insns[i] == 0; i++)\n+    if (i == start + 3)\n+      abort ();\n+  insn = sched_data.insns[i];\n+\n+  if (dump)\n+    fprintf (dump, \"//    Emitting template before %d: %s\\n\",\n+\t     INSN_UID (insn), b->name);\n+\n+  ia64_emit_insn_before (gen_bundle_selector (GEN_INT (bundle_type)), insn);\n+}\n+\n+/* We can't schedule more insns this cycle.  Fix up the scheduling state\n+   and advance FIRST_SLOT and CUR.\n+   We have to distribute the insns that are currently found between\n+   FIRST_SLOT and CUR into the slots of the packet we have selected.  So\n+   far, they are stored successively in the fields starting at FIRST_SLOT;\n+   now they must be moved to the correct slots.\n+   DUMP is the current scheduling dump file, or NULL.  */\n+\n+static void\n+cycle_end_fill_slots (dump)\n+     FILE *dump;\n+{\n+  const struct ia64_packet *packet = sched_data.packet;\n+  int slot, i;\n+  enum attr_type tmp_types[6];\n+  rtx tmp_insns[6];\n+\n+  memcpy (tmp_types, sched_data.types, 6 * sizeof (enum attr_type));\n+  memcpy (tmp_insns, sched_data.insns, 6 * sizeof (rtx));\n+\n+  for (i = slot = sched_data.first_slot; i < sched_data.cur; i++)\n+    {\n+      enum attr_type t = tmp_types[i];\n+      if (t != ia64_safe_type (tmp_insns[i]))\n+\tabort ();\n+      while (! insn_matches_slot (packet, t, slot, tmp_insns[i]))\n+\t{\n+\t  if (slot > sched_data.split)\n+\t    abort ();\n+\t  if (dump)\n+\t    fprintf (dump, \"// Packet needs %s, have %s\\n\", type_names[packet->t[slot]],\n+\t\t     type_names[t]);\n+\t  sched_data.types[slot] = packet->t[slot];\n+\t  sched_data.insns[slot] = 0;\n+\t  sched_data.stopbit[slot] = 0;\n+\t  slot++;\n+\t}\n+      /* Do _not_ use T here.  If T == TYPE_A, then we'd risk changing the\n+\t actual slot type later.  */\n+      sched_data.types[slot] = packet->t[slot];\n+      sched_data.insns[slot] = tmp_insns[i];\n+      sched_data.stopbit[slot] = 0;\n+      slot++;\n+    }\n+\n+  /* This isn't right - there's no need to pad out until the forced split;\n+     the CPU will automatically split if an insn isn't ready.  */\n+#if 0\n+  while (slot < sched_data.split)\n+    {\n+      sched_data.types[slot] = packet->t[slot];\n+      sched_data.insns[slot] = 0;\n+      sched_data.stopbit[slot] = 0;\n+      slot++;\n+    }\n+#endif\n+\n+  sched_data.first_slot = sched_data.cur = slot;\n+}\n+\n+/* Bundle rotations, as described in the Itanium optimization manual.\n+   We can rotate either one or both bundles out of the issue window.\n+   DUMP is the current scheduling dump file, or NULL.  */\n+\n+static void\n+rotate_one_bundle (dump)\n+     FILE *dump;\n+{\n+  if (dump)\n+    fprintf (dump, \"// Rotating one bundle.\\n\");\n+\n+  finish_last_head (dump, 0);\n+  if (sched_data.cur > 3)\n+    {\n+      sched_data.cur -= 3;\n+      sched_data.first_slot -= 3;\n+      memmove (sched_data.types,\n+\t       sched_data.types + 3,\n+\t       sched_data.cur * sizeof *sched_data.types);\n+      memmove (sched_data.stopbit,\n+\t       sched_data.stopbit + 3,\n+\t       sched_data.cur * sizeof *sched_data.stopbit);\n+      memmove (sched_data.insns,\n+\t       sched_data.insns + 3,\n+\t       sched_data.cur * sizeof *sched_data.insns);\n+    }\n+  else\n+    {\n+      sched_data.cur = 0;\n+      sched_data.first_slot = 0;\n+    }\n+}\n+\n+static void\n+rotate_two_bundles (dump)\n+     FILE *dump;\n+{\n+  if (dump)\n+    fprintf (dump, \"// Rotating two bundles.\\n\");\n+\n+  if (sched_data.cur == 0)\n+    return;\n+\n+  finish_last_head (dump, 0);\n+  if (sched_data.cur > 3)\n+    finish_last_head (dump, 3);\n+  sched_data.cur = 0;\n+  sched_data.first_slot = 0;\n+}\n+\n+/* We're beginning a new block.  Initialize data structures as necessary.  */\n+\n+void\n+ia64_sched_init (dump, sched_verbose, max_ready)\n+     FILE *dump ATTRIBUTE_UNUSED;\n+     int sched_verbose ATTRIBUTE_UNUSED;\n+     int max_ready;\n+{\n+  static int initialized = 0;\n+\n+  if (! initialized)\n+    {\n+      int b1, b2, i;\n+\n+      initialized = 1;\n+\n+      for (i = b1 = 0; b1 < NR_BUNDLES; b1++)\n+\t{\n+\t  const struct bundle *t1 = bundle + b1;\n+\t  for (b2 = 0; b2 < NR_BUNDLES; b2++, i++)\n \t    {\n-\t      /* PREV_INSN null can happen if the very first insn is a\n-\t\t volatile asm.  */\n-\t      if (prev_insn)\n-\t\temit_group_barrier_after (prev_insn);\n-\t      memcpy (rws_sum, rws_insn, sizeof (rws_sum));\n+\t      const struct bundle *t2 = bundle + b2;\n+\n+\t      packets[i].t1 = t1;\n+\t      packets[i].t2 = t2;\n \t    }\n+\t}\n+      for (i = 0; i < NR_PACKETS; i++)\n+\t{\n+\t  int j;\n+\t  for (j = 0; j < 3; j++)\n+\t    packets[i].t[j] = packets[i].t1->t[j];\n+\t  for (j = 0; j < 3; j++)\n+\t    packets[i].t[j + 3] = packets[i].t2->t[j];\n+\t  packets[i].first_split = itanium_split_issue (packets + i, 0);\n+\t}\n+\t\n+    }\n \n-\t  /* A call must end a bundle, otherwise the assembler might pack\n-\t     it in with a following branch and then the function return\n-\t     goes to the wrong place.  Do this unconditionally for \n-\t     unconditional calls, simply because it (1) looks nicer and\n-\t     (2) keeps the data structures more accurate for the insns\n-\t     following the call.  */\n-\t  /* ??? A call doesn't have to end a bundle if it is followed by\n-\t     a mutex call or branch.  Two mutex calls/branches can be put in\n-\t     the same bundle.  */\n+  init_insn_group_barriers ();\n \n-\t  need_barrier = 1;\n-\t  if (GET_CODE (PATTERN (insn)) == COND_EXEC)\n+  memset (&sched_data, 0, sizeof sched_data);\n+  sched_types = (enum attr_type *) xmalloc (max_ready\n+\t\t\t\t\t    * sizeof (enum attr_type));\n+  sched_ready = (rtx *) xmalloc (max_ready * sizeof (rtx));\n+}\n+\n+/* See if the packet P can match the insns we have already scheduled.  Return\n+   nonzero if so.  In *PSLOT, we store the first slot that is available for\n+   more instructions if we choose this packet.\n+   SPLIT holds the last slot we can use, there's a split issue after it so\n+   scheduling beyond it would cause us to use more than one cycle.  */\n+\n+static int\n+packet_matches_p (p, split, pslot)\n+     const struct ia64_packet *p;\n+     int split;\n+     int *pslot;\n+{\n+  int filled = sched_data.cur;\n+  int first = sched_data.first_slot;\n+  int i, slot;\n+\n+  /* First, check if the first of the two bundles must be a specific one (due\n+     to stop bits).  */\n+  if (first > 0 && sched_data.stopbit[0] && p->t1->possible_stop != 1)\n+    return 0;\n+  if (first > 1 && sched_data.stopbit[1] && p->t1->possible_stop != 2)\n+    return 0;\n+\n+  for (i = 0; i < first; i++)\n+    if (! insn_matches_slot (p, sched_data.types[i], i,\n+\t\t\t     sched_data.insns[i]))\n+      return 0;\n+  for (i = slot = first; i < filled; i++)\n+    {\n+      while (slot < split)\n+\t{\n+\t  if (insn_matches_slot (p, sched_data.types[i], slot,\n+\t\t\t\t sched_data.insns[i]))\n+\t    break;\n+\t  slot++;\n+\t}\n+      if (slot == split)\n+\treturn 0;\n+      slot++;\n+    }\n+\n+  if (pslot)\n+    *pslot = slot;\n+  return 1;\n+}\n+\n+/* A frontend for itanium_split_issue.  For a packet P and a slot\n+   number FIRST that describes the start of the current clock cycle,\n+   return the slot number of the first split issue.  This function\n+   uses the cached number found in P if possible.  */\n+\n+static int\n+get_split (p, first)\n+     const struct ia64_packet *p;\n+     int first;\n+{\n+  if (first == 0)\n+    return p->first_split;\n+  return itanium_split_issue (p, first);\n+}\n+\n+/* Given N_READY insns in the array READY, whose types are found in the\n+   corresponding array TYPES, return the insn that is best suited to be\n+   scheduled in slot SLOT of packet P.  */\n+\n+static int\n+find_best_insn (ready, types, n_ready, p, slot)\n+     rtx *ready;\n+     enum attr_type *types;\n+     int n_ready;\n+     const struct ia64_packet *p;\n+     int slot;\n+{\n+  int best = -1;\n+  int best_pri = 0;\n+  while (n_ready-- > 0)\n+    {\n+      rtx insn = ready[n_ready];\n+      if (! insn)\n+\tcontinue;\n+      if (best >= 0 && INSN_PRIORITY (ready[n_ready]) < best_pri)\n+\tbreak;\n+      /* If we have equally good insns, one of which has a stricter\n+\t slot requirement, prefer the one with the stricter requirement.  */\n+      if (best >= 0 && types[n_ready] == TYPE_A)\n+\tcontinue;\n+      if (insn_matches_slot (p, types[n_ready], slot, insn))\n+\t{\n+\t  best = n_ready;\n+\t  best_pri = INSN_PRIORITY (ready[best]);\n+\n+\t  /* If there's no way we could get a stricter requirement, stop\n+\t     looking now.  */\n+\t  if (types[n_ready] != TYPE_A\n+\t      && ia64_safe_itanium_requires_unit0 (ready[n_ready]))\n+\t    break;\n+\t  break;\n+\t}\n+    }\n+  return best;\n+}\n+\n+/* Select the best packet to use given the current scheduler state and the\n+   current ready list.\n+   READY is an array holding N_READY ready insns; TYPES is a corresponding\n+   array that holds their types.  Store the best packet in *PPACKET and the\n+   number of insns that can be scheduled in the current cycle in *PBEST.  */\n+\n+static void\n+find_best_packet (pbest, ppacket, ready, types, n_ready)\n+     int *pbest;\n+     const struct ia64_packet **ppacket;\n+     rtx *ready;\n+     enum attr_type *types;\n+     int n_ready;\n+{\n+  int first = sched_data.first_slot;\n+  int best = 0;\n+  int lowest_end = 6;\n+  const struct ia64_packet *best_packet;\n+  int i;\n+\n+  for (i = 0; i < NR_PACKETS; i++)\n+    {\n+      const struct ia64_packet *p = packets + i;\n+      int slot;\n+      int split = get_split (p, first);\n+      int win = 0;\n+      int first_slot, last_slot;\n+      int b_nops = 0;\n+\n+      if (! packet_matches_p (p, split, &first_slot))\n+\tcontinue;\n+\n+      memcpy (sched_ready, ready, n_ready * sizeof (rtx));\n+\n+      win = 0;\n+      last_slot = 6;\n+      for (slot = first_slot; slot < split; slot++)\n+\t{\n+\t  int insn_nr;\n+\n+\t  /* Disallow a degenerate case where the first bundle doesn't\n+\t     contain anything but NOPs!  */\n+\t  if (first_slot == 0 && win == 0 && slot == 3)\n \t    {\n-\t      rtx next_insn = insn;\n-\t      enum attr_type type = TYPE_A;\n-\n-\t      do\n-\t\tnext_insn = next_nonnote_insn (next_insn);\n-\t      while (next_insn\n-\t\t     && GET_CODE (next_insn) == INSN\n-\t\t     && (GET_CODE (PATTERN (next_insn)) == USE\n-\t\t\t || GET_CODE (PATTERN (next_insn)) == CLOBBER));\n-\n-\t      /* A call ends a bundle if there is a stop bit after it,\n-\t\t or if it is followed by a non-B-type instruction.\n-\t\t In the later case, we can elide the stop bit, and get faster\n-\t\t code when the predicate is false.  */\n-\t      /* ??? The proper solution for this problem is to make gcc\n-\t\t explicitly bundle instructions.  Then we don't need to\n-\t\t emit stop bits to force the assembler to start a new\n-\t\t bundle.  */\n-\n-\t      /* Check the instruction type if it is not a branch or call.  */\n-\t      if (next_insn && GET_CODE (next_insn) == INSN)\n-\t\ttype = get_attr_type (next_insn);\n-\n-\t      if (next_insn && GET_CODE (next_insn) != JUMP_INSN\n-\t\t  && GET_CODE (next_insn) != CALL_INSN\n-\t\t  && type != TYPE_B && type != TYPE_UNKNOWN)\n-\t\tneed_barrier = 0;\n+\t      win = -1;\n+\t      break;\n \t    }\n-\t  if (need_barrier)\n+\n+\t  insn_nr = find_best_insn (sched_ready, types, n_ready, p, slot);\n+\t  if (insn_nr >= 0)\n \t    {\n-\t      emit_group_barrier_after (insn);\n-\t      memset (rws_sum, 0, sizeof (rws_sum));\n-\t      prev_insn = NULL_RTX;\n+\t      sched_ready[insn_nr] = 0;\n+\t      last_slot = slot;\n+\t      win++;\n \t    }\n-\t  else\n-\t    prev_insn = insn;\n-\t  break;\n-\t\n-\tcase JUMP_INSN:\n-\t  flags.is_branch = 1;\n-\t  /* FALLTHRU */\n-\n-\tcase INSN:\n-\t  if (GET_CODE (PATTERN (insn)) == USE)\n-\t    /* Don't care about USE \"insns\"---those are used to\n-\t       indicate to the optimizer that it shouldn't get rid of\n-\t       certain operations.  */\n-\t    break;\n-\t  else\n-\t    {\n-\t      rtx pat = PATTERN (insn);\n+\t  else if (p->t[slot] == TYPE_B)\n+\t    b_nops++;\n+\t}\n+      /* We must disallow MBB/BBB packets if any of their B slots would be\n+\t filled with nops.  */\n+      if (last_slot < 3)\n+\t{\n+\t  if (p->t[1] == TYPE_B && (b_nops || last_slot < 2))\n+\t    win = -1;\n+\t}\n+      else\n+\t{\n+\t  if (p->t[4] == TYPE_B && (b_nops || last_slot < 5))\n+\t    win = -1;\n+\t}\n \n-\t      /* Ug.  Hack hacks hacked elsewhere.  */\n-\t      switch (recog_memoized (insn))\n-\t\t{\n-\t\t  /* We play dependency tricks with the epilogue in order\n-\t\t     to get proper schedules.  Undo this for dv analysis.  */\n-\t\tcase CODE_FOR_epilogue_deallocate_stack:\n-\t\t  pat = XVECEXP (pat, 0, 0);\n-\t\t  break;\n+      if (win > best\n+\t  || (win == best && last_slot < lowest_end))\n+\t{\n+\t  best = win;\n+\t  lowest_end = last_slot;\n+\t  best_packet = p;\n+\t}\n+    }\n+  *pbest = best;\n+  *ppacket = best_packet;\n+}\n \n-\t\t  /* The pattern we use for br.cloop confuses the code above.\n-\t\t     The second element of the vector is representative.  */\n-\t\tcase CODE_FOR_doloop_end_internal:\n-\t\t  pat = XVECEXP (pat, 0, 1);\n-\t\t  break;\n+/* Reorder the ready list so that the insns that can be issued in this cycle\n+   are found in the correct order at the end of the list.\n+   DUMP is the scheduling dump file, or NULL.  READY points to the start,\n+   E_READY to the end of the ready list.  MAY_FAIL determines what should be\n+   done if no insns can be scheduled in this cycle: if it is zero, we abort,\n+   otherwise we return 0.\n+   Return 1 if any insns can be scheduled in this cycle.  */\n+\n+static int\n+itanium_reorder (dump, ready, e_ready, may_fail)\n+     FILE *dump;\n+     rtx *ready;\n+     rtx *e_ready;\n+     int may_fail;\n+{\n+  const struct ia64_packet *best_packet;\n+  int n_ready = e_ready - ready;\n+  int first = sched_data.first_slot;\n+  int i, best, best_split, filled;\n+\n+  for (i = 0; i < n_ready; i++)\n+    sched_types[i] = ia64_safe_type (ready[i]);\n+\n+  find_best_packet (&best, &best_packet, ready, sched_types, n_ready);\n+\n+  if (best == 0)\n+    {\n+      if (may_fail)\n+\treturn 0;\n+      abort ();\n+    }\n+\n+  if (dump)\n+    {\n+      fprintf (dump, \"// Selected bundles: %s %s (%d insns)\\n\",\n+\t       best_packet->t1->name,\n+\t       best_packet->t2 ? best_packet->t2->name : NULL, best);\n+    }\n+\n+  best_split = itanium_split_issue (best_packet, first);\n+  packet_matches_p (best_packet, best_split, &filled);\n+\n+  for (i = filled; i < best_split; i++)\n+    {\n+      int insn_nr;\n+\n+      insn_nr = find_best_insn (ready, sched_types, n_ready, best_packet, i);\n+      if (insn_nr >= 0)\n+\t{\n+\t  rtx insn = ready[insn_nr];\n+\t  memmove (ready + insn_nr, ready + insn_nr + 1,\n+\t\t   (n_ready - insn_nr - 1) * sizeof (rtx));\n+\t  memmove (sched_types + insn_nr, sched_types + insn_nr + 1,\n+\t\t   (n_ready - insn_nr - 1) * sizeof (enum attr_type));\n+\t  ready[--n_ready] = insn;\n+\t}\n+    }\n+\n+  sched_data.packet = best_packet;\n+  sched_data.split = best_split;\n+  return 1;\n+}\n+\n+/* Dump information about the current scheduling state to file DUMP.  */\n+\n+static void\n+dump_current_packet (dump)\n+     FILE *dump;\n+{\n+  int i;\n+  fprintf (dump, \"//    %d slots filled:\", sched_data.cur);\n+  for (i = 0; i < sched_data.first_slot; i++)\n+    {\n+      rtx insn = sched_data.insns[i];\n+      fprintf (dump, \" %s\", type_names[sched_data.types[i]]);\n+      if (insn)\n+\tfprintf (dump, \"/%s\", type_names[ia64_safe_type (insn)]);\n+      if (sched_data.stopbit[i])\n+\tfprintf (dump, \" ;;\");\n+    }\n+  fprintf (dump, \" :::\");\n+  for (i = sched_data.first_slot; i < sched_data.cur; i++)\n+    {\n+      rtx insn = sched_data.insns[i];\n+      enum attr_type t = ia64_safe_type (insn);\n+      fprintf (dump, \" (%d) %s\", INSN_UID (insn), type_names[t]);\n+    }\n+  fprintf (dump, \"\\n\");\n+}\n+\n+/* Schedule a stop bit.  DUMP is the current scheduling dump file, or\n+   NULL.  */\n+\n+static void\n+schedule_stop (dump)\n+     FILE *dump;\n+{\n+  const struct ia64_packet *best = sched_data.packet;\n+  int i;\n+  int best_stop = 6;\n+\n+  if (dump)\n+    fprintf (dump, \"// Stop bit, cur = %d.\\n\", sched_data.cur);\n+\n+  if (sched_data.cur == 0)\n+    {\n+      if (dump)\n+\tfprintf (dump, \"//   At start of bundle, so nothing to do.\\n\");\n+\n+      rotate_two_bundles (NULL);\n+      return;\n+    }\n+\n+  for (i = -1; i < NR_PACKETS; i++)\n+    {\n+      /* This is a slight hack to give the current packet the first chance.\n+\t This is done to avoid e.g. switching from MIB to MBB bundles.  */\n+      const struct ia64_packet *p = (i >= 0 ? packets + i : sched_data.packet);\n+      int split = get_split (p, sched_data.first_slot);\n+      const struct bundle *compare;\n+      int next, stoppos;\n+\n+      if (! packet_matches_p (p, split, &next))\n+\tcontinue;\n+\n+      compare = next > 3 ? p->t2 : p->t1;\n+\n+      stoppos = 3;\n+      if (compare->possible_stop)\n+\tstoppos = compare->possible_stop;\n+      if (next > 3)\n+\tstoppos += 3;\n+\n+      if (stoppos < next || stoppos >= best_stop)\n+\t{\n+\t  if (compare->possible_stop == 0)\n+\t    continue;\n+\t  stoppos = (next > 3 ? 6 : 3);\n+\t}\n+      if (stoppos < next || stoppos >= best_stop)\n+\tcontinue;\n+\n+      if (dump)\n+\tfprintf (dump, \"//   switching from %s %s to %s %s (stop at %d)\\n\",\n+\t\t best->t1->name, best->t2->name, p->t1->name, p->t2->name,\n+\t\t stoppos);\n+\n+      best_stop = stoppos;\n+      best = p;\n+    }\n+\n+  sched_data.packet = best;\n+  cycle_end_fill_slots (dump);\n+  while (sched_data.cur < best_stop)\n+    {\n+      sched_data.types[sched_data.cur] = best->t[sched_data.cur];\n+      sched_data.insns[sched_data.cur] = 0;\n+      sched_data.stopbit[sched_data.cur] = 0;\n+      sched_data.cur++;\n+    }\n+  sched_data.stopbit[sched_data.cur - 1] = 1;\n+  sched_data.first_slot = best_stop;\n+\n+  if (dump)\n+    dump_current_packet (dump);\n+}\n+\n+/* We are about to being issuing insns for this clock cycle.\n+   Override the default sort algorithm to better slot instructions.  */\n+\n+int\n+ia64_sched_reorder (dump, sched_verbose, ready, pn_ready, reorder_type)\n+     FILE *dump ATTRIBUTE_UNUSED;\n+     int sched_verbose ATTRIBUTE_UNUSED;\n+     rtx *ready;\n+     int *pn_ready;\n+     int reorder_type;\n+{\n+  int n_ready = *pn_ready;\n+  rtx *e_ready = ready + n_ready;\n+  rtx *insnp;\n+  rtx highest;\n \n-\t\t  /* Doesn't generate code.  */\n-\t\tcase CODE_FOR_pred_rel_mutex:\n-\t\t  continue;\n+  if (sched_verbose)\n+    {\n+      fprintf (dump, \"// ia64_sched_reorder (type %d):\\n\", reorder_type);\n+      dump_current_packet (dump);\n+    }\n+\n+  /* First, move all USEs, CLOBBERs and other crud out of the way.  */\n+  highest = ready[n_ready - 1];\n+  for (insnp = ready; insnp < e_ready; insnp++)\n+    if (insnp < e_ready)\n+      {\n+\trtx insn = *insnp;\n+\tenum attr_type t = ia64_safe_type (insn);\n+\tif (t == TYPE_UNKNOWN)\n+\t  {\n+\t    highest = ready[n_ready - 1];\n+\t    ready[n_ready - 1] = insn;\n+\t    *insnp = highest;\n+\t    if (group_barrier_needed_p (insn))\n+\t      {\n+\t\tschedule_stop (sched_verbose ? dump : NULL);\n+\t\tsched_data.last_was_stop = 1;\n+\t      }\n+\t    return 1;\n+\t  }\n+      }\n \n-\t\tdefault:\n+  if (ia64_final_schedule)\n+    {\n+      int nr_need_stop = 0;\n+\n+      for (insnp = ready; insnp < e_ready; insnp++)\n+\tif (safe_group_barrier_needed_p (*insnp))\n+\t  nr_need_stop++;\n+\n+      /* Schedule a stop bit if\n+          - all insns require a stop bit, or\n+          - we are starting a new cycle and _any_ insns require a stop bit.\n+         The reason for the latter is that if our schedule is accurate, then\n+         the additional stop won't decrease performance at this point (since\n+\t there's a split issue at this point anyway), but it gives us more\n+         freedom when scheduling the currently ready insns.  */\n+      if ((reorder_type == 0 && nr_need_stop)\n+\t  || (reorder_type == 1 && n_ready == nr_need_stop))\n+\t{\n+\t  schedule_stop (sched_verbose ? dump : NULL);\n+\t  sched_data.last_was_stop = 1;\n+\t  if (reorder_type == 1)\n+\t    return 0;\n+\t}\n+      else\n+\t{\n+\t  int deleted = 0;\n+\t  insnp = e_ready;\n+\t  /* Move down everything that needs a stop bit, preserving relative\n+\t     order.  */\n+\t  while (insnp-- > ready + deleted)\n+\t    while (insnp >= ready + deleted)\n+\t      {\n+\t\trtx insn = *insnp;\n+\t\tif (! safe_group_barrier_needed_p (insn))\n \t\t  break;\n-\t\t}\n+\t\tmemmove (ready + 1, ready, (insnp - ready) * sizeof (rtx));\n+\t\t*ready = insn;\n+\t\tdeleted++;\n+\t      }\n+\t  n_ready -= deleted;\n+\t  ready += deleted;\n+\t  if (deleted != nr_need_stop)\n+\t    abort ();\n+\t}\n+    }\n \n-\t      memset (rws_insn, 0, sizeof (rws_insn));\n-\t      need_barrier |= rtx_needs_barrier (pat, flags, 0);\n+  if (reorder_type == 0)\n+    {\n+      if (sched_data.cur == 6)\n+\trotate_two_bundles (sched_verbose ? dump : NULL);\n+      else if (sched_data.cur >= 3)\n+\trotate_one_bundle (sched_verbose ? dump : NULL);\n+      sched_data.first_slot = sched_data.cur;\n+    }\n \n-\t      /* Check to see if the previous instruction was a volatile\n-\t\t asm.  */\n-\t      if (! need_barrier)\n-\t\tneed_barrier = rws_access_regno (REG_VOLATILE, flags, 0);\n+  return itanium_reorder (sched_verbose ? dump : NULL,\n+\t\t\t  ready, e_ready, reorder_type == 1);\n+}\n \n-\t      if (need_barrier)\n-\t\t{\n-\t\t  /* PREV_INSN null can happen if the very first insn is a\n-\t\t     volatile asm.  */\n-\t\t  if (prev_insn)\n-\t\t    emit_group_barrier_after (prev_insn);\n-\t\t  memcpy (rws_sum, rws_insn, sizeof (rws_sum));\n-\t\t}\n-\t      prev_insn = insn;\n+/* Like ia64_sched_reorder, but called after issuing each insn.\n+   Override the default sort algorithm to better slot instructions.  */\n+\n+int\n+ia64_sched_reorder2 (dump, sched_verbose, ready, pn_ready, clock_var)\n+     FILE *dump ATTRIBUTE_UNUSED;\n+     int sched_verbose ATTRIBUTE_UNUSED;\n+     rtx *ready;\n+     int *pn_ready;\n+     int clock_var ATTRIBUTE_UNUSED;\n+{\n+  if (sched_data.last_was_stop)\n+    return 0;\n+\n+  /* Detect one special case and try to optimize it.\n+     If we have 1.M;;MI 2.MIx, and slots 2.1 (M) and 2.2 (I) are both NOPs,\n+     then we can get better code by transforming this to 1.MFB;; 2.MIx.  */\n+  if (sched_data.first_slot == 1\n+      && sched_data.stopbit[0]\n+      && ((sched_data.cur == 4\n+\t   && (sched_data.types[1] == TYPE_M || sched_data.types[1] == TYPE_A)\n+\t   && (sched_data.types[2] == TYPE_I || sched_data.types[2] == TYPE_A)\n+\t   && (sched_data.types[3] != TYPE_M && sched_data.types[3] != TYPE_A))\n+\t  || (sched_data.cur == 3\n+\t      && (sched_data.types[1] == TYPE_M || sched_data.types[1] == TYPE_A)\n+\t      && (sched_data.types[2] != TYPE_M && sched_data.types[2] != TYPE_I\n+\t\t  && sched_data.types[2] != TYPE_A))))\n+      \n+    {\n+      int i, best;\n+      rtx stop = PREV_INSN (sched_data.insns[1]);\n+      rtx pat;\n+\n+      sched_data.stopbit[0] = 0;\n+      sched_data.stopbit[2] = 1;\n+      if (GET_CODE (stop) != INSN)\n+\tabort ();\n+\n+      pat = PATTERN (stop);\n+      /* Ignore cycle displays.  */\n+      if (GET_CODE (pat) == UNSPEC && XINT (pat, 1) == 23)\n+\tstop = PREV_INSN (stop);\n+      pat = PATTERN (stop);\n+      if (GET_CODE (pat) != UNSPEC_VOLATILE\n+\t  || XINT (pat, 1) != 2\n+\t  || INTVAL (XVECEXP (pat, 0, 0)) != 1)\n+\tabort ();\n+      XVECEXP (pat, 0, 0) = GEN_INT (3);\n+\n+      sched_data.types[5] = sched_data.types[3];\n+      sched_data.types[4] = sched_data.types[2];\n+      sched_data.types[3] = sched_data.types[1];\n+      sched_data.insns[5] = sched_data.insns[3];\n+      sched_data.insns[4] = sched_data.insns[2];\n+      sched_data.insns[3] = sched_data.insns[1];\n+      sched_data.stopbit[5] = sched_data.stopbit[4] = sched_data.stopbit[3] = 0;\n+      sched_data.cur += 2;\n+      sched_data.first_slot = 3;\n+      for (i = 0; i < NR_PACKETS; i++)\n+\t{\n+\t  const struct ia64_packet *p = packets + i;\n+\t  if (p->t[0] == TYPE_M && p->t[1] == TYPE_F && p->t[2] == TYPE_B)\n+\t    {\n+\t      sched_data.packet = p;\n+\t      break;\n \t    }\n-\t  break;\n+\t}\n+      rotate_one_bundle (sched_verbose ? dump : NULL);\n \n-\tcase BARRIER:\n-\t  /* A barrier doesn't imply an instruction group boundary.  */\n-\t  break;\n+      best = 6;\n+      for (i = 0; i < NR_PACKETS; i++)\n+\t{\n+\t  const struct ia64_packet *p = packets + i;\n+\t  int split = get_split (p, sched_data.first_slot);\n+\t  int next;\n \n-\tcase CODE_LABEL:\n-\t  /* Leave prev_insn alone so the barrier gets generated in front\n-\t     of the label, if one is needed.  */\n-\t  break;\n+\t  /* Disallow multiway branches here.  */\n+\t  if (p->t[1] == TYPE_B)\n+\t    continue;\n \n-\tdefault:\n-\t  abort ();\n+\t  if (packet_matches_p (p, split, &next) && next < best)\n+\t    {\n+\t      best = next;\n+\t      sched_data.packet = p;\n+\t      sched_data.split = split;\n+\t    }\n \t}\n+      if (best == 6)\n+\tabort ();\n+    }\n+\n+  if (*pn_ready > 0)\n+    {\n+      int more = ia64_sched_reorder (dump, sched_verbose, ready, pn_ready, 1);\n+      if (more)\n+\treturn more;\n+      /* Did we schedule a stop?  If so, finish this cycle.  */\n+      if (sched_data.cur == sched_data.first_slot)\n+\treturn 0;\n     }\n+\n+  if (sched_verbose)\n+    fprintf (dump, \"//   Can't issue more this cycle; updating type array.\\n\");\n+\n+  cycle_end_fill_slots (sched_verbose ? dump : NULL);\n+  if (sched_verbose)\n+    dump_current_packet (dump);\n+  return 0;\n }\n \n+/* We are about to issue INSN.  Return the number of insns left on the\n+   ready queue that can be issued this cycle.  */\n+\n+int\n+ia64_variable_issue (dump, sched_verbose, insn, can_issue_more)\n+     FILE *dump;\n+     int sched_verbose;\n+     rtx insn;\n+     int can_issue_more ATTRIBUTE_UNUSED;\n+{\n+  enum attr_type t = ia64_safe_type (insn);\n+\n+  if (sched_data.last_was_stop)\n+    {\n+      int t = sched_data.first_slot;\n+      if (t == 0)\n+\tt = 3;\n+      ia64_emit_insn_before (gen_insn_group_barrier (GEN_INT (t)), insn);\n+      init_insn_group_barriers ();\n+      sched_data.last_was_stop = 0;\n+    }\n+\n+  if (t == TYPE_UNKNOWN)\n+    {\n+      if (sched_verbose)\n+\tfprintf (dump, \"// Ignoring type %s\\n\", type_names[t]);\n+      return 1;\n+    }\n+\n+  /* This is _not_ just a sanity check.  group_barrier_needed_p will update\n+     important state info.  Don't delete this test.  */\n+  if (ia64_final_schedule\n+      && group_barrier_needed_p (insn))\n+    abort ();\n+\n+  sched_data.stopbit[sched_data.cur] = 0;\n+  sched_data.insns[sched_data.cur] = insn;\n+  sched_data.types[sched_data.cur] = t;\n+\n+  sched_data.cur++;\n+  if (sched_verbose)\n+    fprintf (dump, \"// Scheduling insn %d of type %s\\n\",\n+\t     INSN_UID (insn), type_names[t]);\n+\n+  if (GET_CODE (insn) == CALL_INSN && ia64_final_schedule)\n+    {\n+      schedule_stop (sched_verbose ? dump : NULL);\n+      sched_data.last_was_stop = 1;\n+    }\n+\n+  return 1;\n+}\n+\n+/* Free data allocated by ia64_sched_init.  */\n+\n+void\n+ia64_sched_finish (dump, sched_verbose)\n+     FILE *dump;\n+     int sched_verbose;\n+{\n+  if (sched_verbose)\n+    fprintf (dump, \"// Finishing schedule.\\n\");\n+  rotate_two_bundles (NULL);\n+  free (sched_types);\n+  free (sched_ready);\n+}\n+\f\n /* Emit pseudo-ops for the assembler to describe predicate relations.\n    At present this assumes that we only consider predicate pairs to\n    be mutex, and that the assembler can deduce proper values from\n@@ -4660,9 +5918,17 @@ ia64_reorg (insns)\n   /* Make sure the CFG and global_live_at_start are correct\n      for emit_predicate_relation_info.  */\n   find_basic_blocks (insns, max_reg_num (), NULL);\n-  life_analysis (insns, NULL, 0);\n+  life_analysis (insns, NULL, PROP_DEATH_NOTES);\n+\n+  ia64_final_schedule = 1;\n+  schedule_ebbs (rtl_dump_file);\n+  ia64_final_schedule = 0;\n+\n+  /* This relies on the NOTE_INSN_BASIC_BLOCK notes to be in the same\n+     place as they were during scheduling.  */\n+  emit_insn_group_barriers (rtl_dump_file, insns);\n \n-  emit_insn_group_barriers (insns);\n+  fixup_errata ();\n   emit_predicate_relation_info ();\n }\n \f"}, {"sha": "424fa778d41eb41e3a590462ccf89ba419d8a1e3", "filename": "gcc/config/ia64/ia64.h", "status": "modified", "additions": 35, "deletions": 18, "changes": 53, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Fconfig%2Fia64%2Fia64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Fconfig%2Fia64%2Fia64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fia64%2Fia64.h?ref=2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "patch": "@@ -1849,7 +1849,7 @@ do {\t\t\t\t\t\t\t\t\t\\\n   case CONST:\t\t\t\t\t\t\t\t\\\n   case SYMBOL_REF:\t\t\t\t\t\t\t\\\n   case LABEL_REF:\t\t\t\t\t\t\t\\\n-    return COSTS_N_INSNS (2);\n+    return COSTS_N_INSNS (3);\n \n /* Like `CONST_COSTS' but applies to nonconstant RTL expressions.  */\n \n@@ -1916,19 +1916,6 @@ do {\t\t\t\t\t\t\t\t\t\\\n \n #define NO_FUNCTION_CSE\n \n-/* A C statement (sans semicolon) to update the integer variable COST based on\n-   the relationship between INSN that is dependent on DEP_INSN through the\n-   dependence LINK.  */\n-\n-/* ??? Investigate.  */\n-/* #define ADJUST_COST(INSN, LINK, DEP_INSN, COST) */\n-\n-/* A C statement (sans semicolon) to update the integer scheduling\n-   priority `INSN_PRIORITY(INSN)'.  */\n-\n-/* ??? Investigate.  */\n-/* #define ADJUST_PRIORITY (INSN) */\n-\n \f\n /* Dividing the output into sections.  */\n \n@@ -2816,13 +2803,43 @@ do {\t\t\t\t\t\t\t\t\t\\\n    BRANCH_COST+1 is the default if the machine does not use\n    cc0, and 1 if it does use cc0.  */\n /* ??? Investigate.  */\n-/* #define MAX_CONDITIONAL_EXECUTE */\n+#define MAX_CONDITIONAL_EXECUTE 12\n+\n+/* A C statement (sans semicolon) to update the integer scheduling\n+   priority `INSN_PRIORITY(INSN)'.  */\n+\n+/* ??? Investigate.  */\n+/* #define ADJUST_PRIORITY (INSN) */\n+\n+/* A C statement (sans semicolon) to update the integer variable COST\n+   based on the relationship between INSN that is dependent on\n+   DEP_INSN through the dependence LINK.  The default is to make no\n+   adjustment to COST.  This can be used for example to specify to\n+   the scheduler that an output- or anti-dependence does not incur\n+   the same cost as a data-dependence.  */\n+\n+#define ADJUST_COST(insn,link,dep_insn,cost) \\\n+  (cost) = ia64_adjust_cost(insn, link, dep_insn, cost)\n+\n+#define ISSUE_RATE ia64_issue_rate ()\n+\n+#define MD_SCHED_INIT(DUMP, SCHED_VERBOSE, MAX_READY) \\\n+  ia64_sched_init (DUMP, SCHED_VERBOSE, MAX_READY)\n+\n+#define MD_SCHED_REORDER(DUMP, SCHED_VERBOSE, READY, N_READY, CLOCK, CIM) \\\n+  (CIM) = ia64_sched_reorder (DUMP, SCHED_VERBOSE, READY, &N_READY, 0)\n+\n+#define MD_SCHED_REORDER2(DUMP, SCHED_VERBOSE, READY, N_READY, CLOCK, CIM) \\\n+  (CIM) = ia64_sched_reorder2 (DUMP, SCHED_VERBOSE, READY, &N_READY, 1)\n \n-/* Indicate how many instructions can be issued at the same time.  */\n+#define MD_SCHED_FINISH(DUMP, SCHED_VERBOSE) \\\n+  ia64_sched_finish (DUMP, SCHED_VERBOSE)\n \n-/* ??? For now, we just schedule to fill bundles.  */\n+#define MD_SCHED_VARIABLE_ISSUE(DUMP, SCHED_VERBOSE, INSN, CAN_ISSUE_MORE) \\\n+  ((CAN_ISSUE_MORE)\t\t\t\t\t\t\t   \\\n+   = ia64_variable_issue (DUMP, SCHED_VERBOSE, INSN, CAN_ISSUE_MORE))\n \n-#define ISSUE_RATE 3\n+extern int ia64_final_schedule;\n \n #define IA64_UNWIND_INFO\t1\n #define HANDLER_SECTION fprintf (asm_out_file, \"\\t.personality\\t__ia64_personality_v1\\n\\t.handlerdata\\n\");"}, {"sha": "25660fe1082c8761051ef6bdf78bd093b44396a3", "filename": "gcc/config/ia64/ia64.md", "status": "modified", "additions": 146, "deletions": 57, "changes": 203, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Fconfig%2Fia64%2Fia64.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Fconfig%2Fia64%2Fia64.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fia64%2Fia64.md?ref=2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "patch": "@@ -68,6 +68,8 @@\n ;;\t19\tfetchadd_acq\n ;;\t20\tbsp_value\n ;;\t21\tflushrs\n+;;\t22      bundle selector\n+;;\t23      cycle display\n ;;\n ;; unspec_volatile:\n ;;\t0\talloc\n@@ -99,23 +101,35 @@\n ;; multiple instructions, patterns which emit 0 instructions, and patterns\n ;; which emit instruction that can go in any slot (e.g. nop).\n \n-(define_attr \"itanium_class\" \"unknown,ignore,stop_bit,br,fcmp,fcvtfx,fld,fmac,fmisc,frar_i,frar_m,frbr,frfr,frpr,ialu,icmp,ilog,ishf,ld,long_i,mmmul,mmshf,mmshfi,rse_m,scall,sem,stf,st,syst_m0,syst_m,tbit,toar_i,toar_m,tobr,tofr,topr,xmpy,xtd\"\n+(define_attr \"itanium_class\" \"unknown,ignore,stop_bit,br,fcmp,fcvtfx,fld,fmac,fmisc,frar_i,frar_m,frbr,frfr,frpr,ialu,icmp,ilog,ishf,ld,chk_s,long_i,mmmul,mmshf,mmshfi,rse_m,scall,sem,stf,st,syst_m0,syst_m,tbit,toar_i,toar_m,tobr,tofr,topr,xmpy,xtd,nop_b,nop_f,nop_i,nop_m,nop_x\"\n          (const_string \"unknown\"))\n \n-(define_attr \"type\" \"unknown,A,I,M,F,B,L,S\"\n-  (cond [(eq_attr \"itanium_class\" \"ld,st,fld,stf,sem\") (const_string \"M\")\n+;; chk_s has an I and an M form; use type A for convenience.\n+(define_attr \"type\" \"unknown,A,I,M,F,B,L,X,S\"\n+  (cond [(eq_attr \"itanium_class\" \"ld,st,fld,stf,sem,nop_m\") (const_string \"M\")\n \t (eq_attr \"itanium_class\" \"rse_m,syst_m,syst_m0\") (const_string \"M\")\n \t (eq_attr \"itanium_class\" \"frar_m,toar_m,frfr,tofr\") (const_string \"M\")\n-\t (eq_attr \"itanium_class\" \"ialu,icmp,ilog\") (const_string \"A\")\n-\t (eq_attr \"itanium_class\" \"fmisc,fmac,fcmp,xmpy,fcvtfx\") (const_string \"F\")\n+\t (eq_attr \"itanium_class\" \"chk_s,ialu,icmp,ilog\") (const_string \"A\")\n+\t (eq_attr \"itanium_class\" \"fmisc,fmac,fcmp,xmpy\") (const_string \"F\")\n+\t (eq_attr \"itanium_class\" \"fcvtfx,nop_f\") (const_string \"F\")\n \t (eq_attr \"itanium_class\" \"frar_i,toar_i,frbr,tobr\") (const_string \"I\")\n \t (eq_attr \"itanium_class\" \"frpr,topr,ishf,xtd,tbit\") (const_string \"I\")\n-\t (eq_attr \"itanium_class\" \"mmmul,mmshf,mmshfi\") (const_string \"I\")\n-\t (eq_attr \"itanium_class\" \"br,scall\") (const_string \"B\")\n+\t (eq_attr \"itanium_class\" \"mmmul,mmshf,mmshfi,nop_i\") (const_string \"I\")\n+\t (eq_attr \"itanium_class\" \"br,scall,nop_b\") (const_string \"B\")\n \t (eq_attr \"itanium_class\" \"stop_bit\") (const_string \"S\")\n+\t (eq_attr \"itanium_class\" \"nop_x\") (const_string \"X\")\n \t (eq_attr \"itanium_class\" \"long_i\") (const_string \"L\")]\n \t(const_string \"unknown\")))\n \n+(define_attr \"itanium_requires_unit0\" \"no,yes\"\n+  (cond [(eq_attr \"itanium_class\" \"syst_m0,sem,frfr,rse_m\") (const_string \"yes\")\n+\t (eq_attr \"itanium_class\" \"toar_m,frar_m\") (const_string \"yes\")\n+\t (eq_attr \"itanium_class\" \"frbr,tobr,mmmul\") (const_string \"yes\")\n+\t (eq_attr \"itanium_class\" \"tbit,ishf,topr,frpr\") (const_string \"yes\")\n+\t (eq_attr \"itanium_class\" \"toar_i,frar_i\") (const_string \"yes\")\n+\t (eq_attr \"itanium_class\" \"fmisc,fcmp\") (const_string \"yes\")]\n+\t(const_string \"no\")))\n+\n ;; Predication.  True iff this instruction can be predicated.\n \n (define_attr \"predicable\" \"no,yes\" (const_string \"yes\"))\n@@ -127,47 +141,70 @@\n ;; ::\n ;; ::::::::::::::::::::\n \n-;; Each usage of a function units by a class of insns is specified with a\n-;; `define_function_unit' expression, which looks like this:\n-;; (define_function_unit NAME MULTIPLICITY SIMULTANEITY TEST READY-DELAY\n-;;   ISSUE-DELAY [CONFLICT-LIST])\n-\n-;; This default scheduling info seeks to pack instructions into bundles\n-;; efficiently to reduce code size, so we just list how many of each\n-;; instruction type can go in a bundle.  ISSUE_RATE is set to 3.\n-\n-;; ??? Add scheduler ready-list hook (MD_SCHED_REORDER) that orders\n-;; instructions, so that the next instruction can fill the next bundle slot.\n-;; This really needs to know where the stop bits are though.\n-\n-;; ??? Use MD_SCHED_REORDER to put alloc first instead of using an unspec\n-;; volatile.  Use ADJUST_PRIORITY to set the priority of alloc very high to\n-;; make it schedule first.\n-\n-;; ??? Modify the md_reorg code that emits stop bits so that instead of putting\n-;; them in the last possible place, we put them in places where bundles allow\n-;; them.  This should reduce code size, but may decrease performance if we end\n-;; up with more stop bits than the minimum we need.\n-\n-;; Alu instructions can execute on either the integer or memory function\n-;; unit.  We indicate this by defining an alu function unit, and then marking\n-;; it as busy everytime we issue a integer or memory type instruction.\n-\n-(define_function_unit \"alu\" 3 1 (eq_attr \"type\" \"A,I,M\") 1 0)\n-\n-(define_function_unit \"integer\" 2 1 (eq_attr \"type\" \"I\") 1 0)\n-\n-(define_function_unit \"memory\" 3 1 (eq_attr \"type\" \"M\") 1 0)\n-\n-(define_function_unit \"floating_point\" 1 1 (eq_attr \"type\" \"F\") 1 0)\n-\n-(define_function_unit \"branch\" 3 1 (eq_attr \"type\" \"B\") 1 0)\n-\n-;; ??? This isn't quite right, because we can only fit two insns in a bundle\n-;; when using an L type instruction.  That isn't modeled currently.\n-\n-(define_function_unit \"long_immediate\" 1 1 (eq_attr \"type\" \"L\") 1 0)\n-\n+;; We define 6 \"dummy\" functional units.  All the real work to decide which\n+;; insn uses which unit is done by our MD_SCHED_REORDER hooks.  We only\n+;; have to ensure here that there are enough copies of the dummy unit so\n+;; that the scheduler doesn't get confused by MD_SCHED_REORDER.\n+;; Other than the 6 dummies for normal insns, we also add a single dummy unit\n+;; for stop bits.\n+\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"br\")     0 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"scall\")  0 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"fcmp\")   2 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"fcvtfx\") 7 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"fld\")    9 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"fmac\")   5 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"fmisc\")  5 0)\n+\n+;; There is only one insn `mov = ar.bsp' for frar_i:\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"frar_i\") 13 0)\n+;; There is only ony insn `mov = ar.unat' for frar_m:\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"frar_m\") 6 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"frbr\")   2 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"frfr\")   2 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"frpr\")   2 0)\n+\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"ialu\")   1 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"icmp\")   1 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"ilog\")   1 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"ishf\")   1 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"ld\")     2 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"long_i\") 1 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"mmmul\")  2 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"mmshf\")  2 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"mmshfi\")  2 0)\n+\n+;; Now we have only one insn (flushrs) of such class.  We assume that flushrs\n+;; is the 1st syllable of the bundle after stop bit.\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"rse_m\")  0 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"sem\")   11 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"stf\")    1 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"st\")     1 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"syst_m0\") 1 0)\n+;; Now we use only one insn `mf'.  Therfore latency time is set up to 0.\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"syst_m\") 0 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"tbit\")   1 0)\n+\n+;; There is only one insn `mov ar.pfs =' for toar_i therefore we use\n+;; latency time equal to 0:\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"toar_i\") 0 0)\n+;; There are only ony 2 insns `mov ar.ccv =' and `mov ar.unat =' for toar_m:\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"toar_m\") 5 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"tobr\")   1 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"tofr\")   9 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"topr\")   1 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"xmpy\")   7 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"xtd\")    1 0)\n+\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"nop_m\")  0 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"nop_i\")  0 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"nop_f\")  0 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"nop_b\")  0 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"nop_x\")  0 0)\n+\n+(define_function_unit \"stop_bit\" 1 1 (eq_attr \"itanium_class\" \"stop_bit\") 0 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"ignore\") 0 0)\n+(define_function_unit \"dummy\" 6 1 (eq_attr \"itanium_class\" \"unknown\") 0 0)\n \f\n ;; ::::::::::::::::::::\n ;; ::\n@@ -1411,7 +1448,6 @@\n    (clobber (match_operand:DI 2 \"register_operand\" \"\"))]\n   \"reload_completed\"\n   [(set (match_dup 3) (ashift:DI (match_dup 1) (const_int 32)))\n-   (unspec_volatile [(const_int 0)] 2)\n    (set (zero_extract:DI (match_dup 0) (const_int 32) (const_int 0))\n \t(lshiftrt:DI (match_dup 3) (const_int 32)))]\n   \"operands[3] = operands[2];\")\n@@ -2408,9 +2444,6 @@\n   \"#\"\n   [(set_attr \"itanium_class\" \"unknown\")])\n \n-;; ??? Need to emit an instruction group barrier here because this gets split\n-;; after md_reorg.\n-\n (define_split\n   [(set (match_operand:DI 0 \"register_operand\" \"\")\n \t(plus:DI (plus:DI (mult:DI (match_operand:DI 1 \"register_operand\" \"\")\n@@ -2422,9 +2455,7 @@\n   [(parallel [(set (match_dup 5) (plus:DI (mult:DI (match_dup 1) (match_dup 2))\n \t\t\t\t\t  (match_dup 3)))\n \t      (clobber (match_dup 0))])\n-   (unspec_volatile [(const_int 0)] 2)\n    (set (match_dup 0) (match_dup 5))\n-   (unspec_volatile [(const_int 0)] 2)\n    (set (match_dup 0) (plus:DI (match_dup 0) (match_dup 4)))]\n   \"\")\n \n@@ -5122,7 +5153,10 @@\n \t\t    (match_operand:DI 2 \"const_int_operand\" \"\")] 1))\n    (clobber (match_operand:DI 3 \"register_operand\" \"\"))]\n   \"\"\n-  \".mem.offset %2, 0\\;st8.spill %0 = %1%P0\"\n+  \"*\n+{\n+  return \\\".mem.offset %2, 0\\;%,st8.spill %0 = %1%P0\\\";\n+}\"\n   [(set_attr \"itanium_class\" \"st\")])\n \n ;; Reads ar.unat\n@@ -5140,7 +5174,10 @@\n \t\t    (match_operand:DI 2 \"const_int_operand\" \"\")] 2))\n    (use (match_operand:DI 3 \"register_operand\" \"\"))]\n   \"\"\n-  \".mem.offset %2, 0\\;ld8.fill %0 = %1%P1\"\n+  \"*\n+{\n+  return \\\".mem.offset %2, 0\\;%,ld8.fill %0 = %1%P1\\\";\n+}\"\n   [(set_attr \"itanium_class\" \"ld\")])\n \n (define_insn \"fr_spill\"\n@@ -5193,6 +5230,58 @@\n   \"nop 0\"\n   [(set_attr \"itanium_class\" \"unknown\")])\n \n+(define_insn \"nop_m\"\n+  [(const_int 1)]\n+  \"\"\n+  \"nop.m 0\"\n+  [(set_attr \"itanium_class\" \"nop_m\")])\n+\n+(define_insn \"nop_i\"\n+  [(const_int 2)]\n+  \"\"\n+  \"nop.i 0\"\n+  [(set_attr \"itanium_class\" \"nop_i\")])\n+\n+(define_insn \"nop_f\"\n+  [(const_int 3)]\n+  \"\"\n+  \"nop.f 0\"\n+  [(set_attr \"itanium_class\" \"nop_f\")])\n+\n+(define_insn \"nop_b\"\n+  [(const_int 4)]\n+  \"\"\n+  \"nop.b 0\"\n+  [(set_attr \"itanium_class\" \"nop_b\")])\n+\n+(define_insn \"nop_x\"\n+  [(const_int 5)]\n+  \"\"\n+  \"\"\n+  [(set_attr \"itanium_class\" \"nop_x\")])\n+\n+(define_expand \"cycle_display\"\n+  [(unspec [(match_operand 0 \"const_int_operand\" \"\")] 23)]\n+  \"ia64_final_schedule\"\n+  \"\")\n+\n+(define_insn \"*cycle_display_1\"\n+  [(unspec [(match_operand 0 \"const_int_operand\" \"\")] 23)]\n+  \"\"\n+  \"// cycle %0\"\n+  [(set_attr \"itanium_class\" \"ignore\")\n+   (set_attr \"predicable\" \"no\")])\n+\n+(define_insn \"bundle_selector\"\n+  [(unspec [(match_operand 0 \"const_int_operand\" \"\")] 22)]\n+  \"\"\n+  \"*\n+{\n+  return get_bundle_name (INTVAL (operands[0]));\n+}\"\n+  [(set_attr \"itanium_class\" \"ignore\")\n+   (set_attr \"predicable\" \"no\")])\n+\n ;; Pseudo instruction that prevents the scheduler from moving code above this\n ;; point.\n (define_insn \"blockage\"\n@@ -5203,7 +5292,7 @@\n    (set_attr \"predicable\" \"no\")])\n \n (define_insn \"insn_group_barrier\"\n-  [(unspec_volatile [(const_int 0)] 2)]\n+  [(unspec_volatile [(match_operand 0 \"const_int_operand\" \"\")] 2)]\n   \"\"\n   \";;\"\n   [(set_attr \"itanium_class\" \"stop_bit\")"}, {"sha": "e1fd687e1c7610b7ac82ee63a2ff830c2955c6c9", "filename": "gcc/rtl.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "patch": "@@ -1346,6 +1346,7 @@ extern void set_unique_reg_note         PARAMS ((rtx, enum reg_note, rtx));\n \t\t       ? (GET_CODE (PATTERN (I)) == SET \\\n \t\t\t  ? PATTERN (I) : single_set_1 (I)) \\\n \t\t       : NULL_RTX)\n+#define single_set_1(I) single_set_2 (I, PATTERN (I))\n \n extern int rtx_unstable_p\t\tPARAMS ((rtx));\n extern int rtx_varies_p\t\t\tPARAMS ((rtx));\n@@ -1365,7 +1366,7 @@ extern int no_jumps_between_p\t\tPARAMS ((rtx, rtx));\n extern int modified_in_p\t\tPARAMS ((rtx, rtx));\n extern int insn_dependent_p\t\tPARAMS ((rtx, rtx));\n extern int reg_set_p\t\t\tPARAMS ((rtx, rtx));\n-extern rtx single_set_1\t\t\tPARAMS ((rtx));\n+extern rtx single_set_2\t\t\tPARAMS ((rtx, rtx));\n extern int multiple_sets\t\tPARAMS ((rtx));\n extern rtx find_last_value\t\tPARAMS ((rtx, rtx *, rtx, int));\n extern int refers_to_regno_p\t\tPARAMS ((unsigned int, unsigned int,"}, {"sha": "00aebf47558f3b746f2f0c981a66dcbd2a3ea33c", "filename": "gcc/rtlanal.c", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Frtlanal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2130b7fb30f2ed6ea7b1e7326058e06d2e604e89/gcc%2Frtlanal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtlanal.c?ref=2130b7fb30f2ed6ea7b1e7326058e06d2e604e89", "patch": "@@ -860,12 +860,11 @@ insn_dependent_p_1 (x, pat, data)\n    will not be used, which we ignore.  */\n \n rtx\n-single_set_1 (insn)\n-     rtx insn;\n+single_set_2 (insn, pat)\n+     rtx insn, pat;\n {\n   rtx set = NULL;\n   int set_verified = 1;\n-  rtx pat = PATTERN (insn);\n   int i;\n \n   if (GET_CODE (pat) == PARALLEL)"}]}