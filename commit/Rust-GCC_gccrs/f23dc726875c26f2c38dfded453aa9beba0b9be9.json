{"sha": "f23dc726875c26f2c38dfded453aa9beba0b9be9", "node_id": "C_kwDOANBUbNoAKGYyM2RjNzI2ODc1YzI2ZjJjMzhkZmRlZDQ1M2FhOWJlYmEwYjliZTk", "commit": {"author": {"name": "Tamar Christina", "email": "tamar.christina@arm.com", "date": "2023-03-12T18:42:59Z"}, "committer": {"name": "Tamar Christina", "email": "tamar.christina@arm.com", "date": "2023-03-12T18:42:59Z"}, "message": "AArch64: Update div-bitmask to implement new optab instead of target hook [PR108583]\n\nThis replaces the custom division hook with just an implementation through\nadd_highpart.  For NEON we implement the add highpart (Addition + extraction of\nthe upper highpart of the register in the same precision) as ADD + LSR.\n\nThis representation allows us to easily optimize the sequence using existing\nsequences. This gets us a pretty decent sequence using SRA:\n\n        umull   v1.8h, v0.8b, v3.8b\n        umull2  v0.8h, v0.16b, v3.16b\n        add     v5.8h, v1.8h, v2.8h\n        add     v4.8h, v0.8h, v2.8h\n        usra    v1.8h, v5.8h, 8\n        usra    v0.8h, v4.8h, 8\n        uzp2    v1.16b, v1.16b, v0.16b\n\nTo get the most optimal sequence however we match (a + ((b + c) >> n)) where n\nis half the precision of the mode of the operation into addhn + uaddw which is\na general good optimization on its own and gets us back to:\n\n.L4:\n        ldr     q0, [x3]\n        umull   v1.8h, v0.8b, v5.8b\n        umull2  v0.8h, v0.16b, v5.16b\n        addhn   v3.8b, v1.8h, v4.8h\n        addhn   v2.8b, v0.8h, v4.8h\n        uaddw   v1.8h, v1.8h, v3.8b\n        uaddw   v0.8h, v0.8h, v2.8b\n        uzp2    v1.16b, v1.16b, v0.16b\n        str     q1, [x3], 16\n        cmp     x3, x4\n        bne     .L4\n\nFor SVE2 we optimize the initial sequence to the same ADD + LSR which gets us:\n\n.L3:\n        ld1b    z0.h, p0/z, [x0, x3]\n        mul     z0.h, p1/m, z0.h, z2.h\n        add     z1.h, z0.h, z3.h\n        usra    z0.h, z1.h, #8\n        lsr     z0.h, z0.h, #8\n        st1b    z0.h, p0, [x0, x3]\n        inch    x3\n        whilelo p0.h, w3, w2\n        b.any   .L3\n.L1:\n        ret\n\nand to get the most optimal sequence I match (a + b) >> n (same constraint on n)\nto addhnb which gets us to:\n\n.L3:\n        ld1b    z0.h, p0/z, [x0, x3]\n        mul     z0.h, p1/m, z0.h, z2.h\n        addhnb  z1.b, z0.h, z3.h\n        addhnb  z0.b, z0.h, z1.h\n        st1b    z0.h, p0, [x0, x3]\n        inch    x3\n        whilelo p0.h, w3, w2\n        b.any   .L3\n\nThere are multiple RTL representations possible for these optimizations, I did\nnot represent them using a zero_extend because we seem very inconsistent in this\nin the backend.  Since they are unspecs we won't match them from vector ops\nanyway. I figured maintainers would prefer this, but my maintainer ouija board\nis still out for repairs :)\n\nThere are no new test as new correctness tests were added to the mid-end and\nthe existing codegen tests for this already exist.\n\ngcc/ChangeLog:\n\n\tPR target/108583\n\t* config/aarch64/aarch64-simd.md (@aarch64_bitmask_udiv<mode>3): Remove.\n\t(*bitmask_shift_plus<mode>): New.\n\t* config/aarch64/aarch64-sve2.md (*bitmask_shift_plus<mode>): New.\n\t(@aarch64_bitmask_udiv<mode>3): Remove.\n\t* config/aarch64/aarch64.cc\n\t(aarch64_vectorize_can_special_div_by_constant,\n\tTARGET_VECTORIZE_CAN_SPECIAL_DIV_BY_CONST): Removed.\n\t(TARGET_VECTORIZE_PREFERRED_DIV_AS_SHIFTS_OVER_MULT,\n\taarch64_vectorize_preferred_div_as_shifts_over_mult): New.", "tree": {"sha": "a83c43779c456b5acf203efafeb639a443642a0b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/a83c43779c456b5acf203efafeb639a443642a0b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f23dc726875c26f2c38dfded453aa9beba0b9be9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f23dc726875c26f2c38dfded453aa9beba0b9be9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f23dc726875c26f2c38dfded453aa9beba0b9be9", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f23dc726875c26f2c38dfded453aa9beba0b9be9/comments", "author": {"login": "TamarChristinaArm", "id": 48126768, "node_id": "MDQ6VXNlcjQ4MTI2NzY4", "avatar_url": "https://avatars.githubusercontent.com/u/48126768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TamarChristinaArm", "html_url": "https://github.com/TamarChristinaArm", "followers_url": "https://api.github.com/users/TamarChristinaArm/followers", "following_url": "https://api.github.com/users/TamarChristinaArm/following{/other_user}", "gists_url": "https://api.github.com/users/TamarChristinaArm/gists{/gist_id}", "starred_url": "https://api.github.com/users/TamarChristinaArm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TamarChristinaArm/subscriptions", "organizations_url": "https://api.github.com/users/TamarChristinaArm/orgs", "repos_url": "https://api.github.com/users/TamarChristinaArm/repos", "events_url": "https://api.github.com/users/TamarChristinaArm/events{/privacy}", "received_events_url": "https://api.github.com/users/TamarChristinaArm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "TamarChristinaArm", "id": 48126768, "node_id": "MDQ6VXNlcjQ4MTI2NzY4", "avatar_url": "https://avatars.githubusercontent.com/u/48126768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TamarChristinaArm", "html_url": "https://github.com/TamarChristinaArm", "followers_url": "https://api.github.com/users/TamarChristinaArm/followers", "following_url": "https://api.github.com/users/TamarChristinaArm/following{/other_user}", "gists_url": "https://api.github.com/users/TamarChristinaArm/gists{/gist_id}", "starred_url": "https://api.github.com/users/TamarChristinaArm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TamarChristinaArm/subscriptions", "organizations_url": "https://api.github.com/users/TamarChristinaArm/orgs", "repos_url": "https://api.github.com/users/TamarChristinaArm/repos", "events_url": "https://api.github.com/users/TamarChristinaArm/events{/privacy}", "received_events_url": "https://api.github.com/users/TamarChristinaArm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "81fd62d1378b7ddc1fa0967cbddcdcdcdd2d8d8c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/81fd62d1378b7ddc1fa0967cbddcdcdcdd2d8d8c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/81fd62d1378b7ddc1fa0967cbddcdcdcdd2d8d8c"}], "stats": {"total": 189, "additions": 52, "deletions": 137}, "files": [{"sha": "b63c1fe154362cdf31b2965d48f599be60387056", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 19, "deletions": 52, "changes": 71, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f23dc726875c26f2c38dfded453aa9beba0b9be9/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f23dc726875c26f2c38dfded453aa9beba0b9be9/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=f23dc726875c26f2c38dfded453aa9beba0b9be9", "patch": "@@ -4867,60 +4867,27 @@\n   }\n )\n \n-;; div optimizations using narrowings\n-;; we can do the division e.g. shorts by 255 faster by calculating it as\n-;; (x + ((x + 257) >> 8)) >> 8 assuming the operation is done in\n-;; double the precision of x.\n-;;\n-;; If we imagine a short as being composed of two blocks of bytes then\n-;; adding 257 or 0b0000_0001_0000_0001 to the number is equivalent to\n-;; adding 1 to each sub component:\n-;;\n-;;      short value of 16-bits\n-;; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n-;; \u2502              \u2502                \u2502\n-;; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n-;;   8-bit part1 \u25b2  8-bit part2   \u25b2\n-;;               \u2502                \u2502\n-;;               \u2502                \u2502\n-;;              +1               +1\n-;;\n-;; after the first addition, we have to shift right by 8, and narrow the\n-;; results back to a byte.  Remember that the addition must be done in\n-;; double the precision of the input.  Since 8 is half the size of a short\n-;; we can use a narrowing halfing instruction in AArch64, addhn which also\n-;; does the addition in a wider precision and narrows back to a byte.  The\n-;; shift itself is implicit in the operation as it writes back only the top\n-;; half of the result. i.e. bits 2*esize-1:esize.\n-;;\n-;; Since we have narrowed the result of the first part back to a byte, for\n-;; the second addition we can use a widening addition, uaddw.\n-;;\n-;; For the final shift, since it's unsigned arithmetic we emit an ushr by 8.\n-;;\n-;; The shift is later optimized by combine to a uzp2 with movi #0.\n-(define_expand \"@aarch64_bitmask_udiv<mode>3\"\n-  [(match_operand:VQN 0 \"register_operand\")\n-   (match_operand:VQN 1 \"register_operand\")\n-   (match_operand:VQN 2 \"immediate_operand\")]\n+;; Optimize ((a + b) >> n) + c where n is half the bitsize of the vector\n+(define_insn_and_split \"*bitmask_shift_plus<mode>\"\n+  [(set (match_operand:VQN 0 \"register_operand\" \"=&w\")\n+\t(plus:VQN\n+\t  (lshiftrt:VQN\n+\t    (plus:VQN (match_operand:VQN 1 \"register_operand\" \"w\")\n+\t\t      (match_operand:VQN 2 \"register_operand\" \"w\"))\n+\t    (match_operand:VQN 3 \"aarch64_simd_shift_imm_vec_exact_top\" \"\"))\n+\t  (match_operand:VQN 4 \"register_operand\" \"w\")))]\n   \"TARGET_SIMD\"\n+  \"#\"\n+  \"&& true\"\n+  [(const_int 0)]\n {\n-  unsigned HOST_WIDE_INT size\n-    = (1ULL << GET_MODE_UNIT_BITSIZE (<VNARROWQ>mode)) - 1;\n-  rtx elt = unwrap_const_vec_duplicate (operands[2]);\n-  if (!CONST_INT_P (elt) || UINTVAL (elt) != size)\n-    FAIL;\n-\n-  rtx addend = gen_reg_rtx (<MODE>mode);\n-  rtx val = aarch64_simd_gen_const_vector_dup (<VNARROWQ2>mode, 1);\n-  emit_move_insn (addend, lowpart_subreg (<MODE>mode, val, <VNARROWQ2>mode));\n-  rtx tmp1 = gen_reg_rtx (<VNARROWQ>mode);\n-  rtx tmp2 = gen_reg_rtx (<MODE>mode);\n-  emit_insn (gen_aarch64_addhn<mode> (tmp1, operands[1], addend));\n-  unsigned bitsize = GET_MODE_UNIT_BITSIZE (<VNARROWQ>mode);\n-  rtx shift_vector = aarch64_simd_gen_const_vector_dup (<MODE>mode, bitsize);\n-  emit_insn (gen_aarch64_uaddw<Vnarrowq> (tmp2, operands[1], tmp1));\n-  emit_insn (gen_aarch64_simd_lshr<mode> (operands[0], tmp2, shift_vector));\n+  rtx tmp;\n+  if (can_create_pseudo_p ())\n+    tmp = gen_reg_rtx (<VNARROWQ>mode);\n+  else\n+    tmp = gen_rtx_REG (<VNARROWQ>mode, REGNO (operands[0]));\n+  emit_insn (gen_aarch64_addhn<mode> (tmp, operands[1], operands[2]));\n+  emit_insn (gen_aarch64_uaddw<Vnarrowq> (operands[0], operands[4], tmp));\n   DONE;\n })\n "}, {"sha": "2346f9f835d26f5b87afd47cdc9e44f9f47604ed", "filename": "gcc/config/aarch64/aarch64-sve2.md", "status": "modified", "additions": 16, "deletions": 41, "changes": 57, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f23dc726875c26f2c38dfded453aa9beba0b9be9/gcc%2Fconfig%2Faarch64%2Faarch64-sve2.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f23dc726875c26f2c38dfded453aa9beba0b9be9/gcc%2Fconfig%2Faarch64%2Faarch64-sve2.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-sve2.md?ref=f23dc726875c26f2c38dfded453aa9beba0b9be9", "patch": "@@ -71,7 +71,6 @@\n ;; ---- [INT] Reciprocal approximation\n ;; ---- [INT<-FP] Base-2 logarithm\n ;; ---- [INT] Polynomial multiplication\n-;; ---- [INT] Misc optab implementations\n ;;\n ;; == Permutation\n ;; ---- [INT,FP] General permutes\n@@ -1600,6 +1599,22 @@\n   \"<sve_int_op>\\t%0.<Ventype>, %2.<Vetype>, %3.<Vetype>\"\n )\n \n+;; Optimize ((a + b) >> n) where n is half the bitsize of the vector\n+(define_insn \"*bitmask_shift_plus<mode>\"\n+  [(set (match_operand:SVE_FULL_HSDI 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_FULL_HSDI\n+\t   [(match_operand:<VPRED> 1)\n+\t    (lshiftrt:SVE_FULL_HSDI\n+\t      (plus:SVE_FULL_HSDI\n+\t\t(match_operand:SVE_FULL_HSDI 2 \"register_operand\" \"w\")\n+\t\t(match_operand:SVE_FULL_HSDI 3 \"register_operand\" \"w\"))\n+\t      (match_operand:SVE_FULL_HSDI 4\n+\t\t \"aarch64_simd_shift_imm_vec_exact_top\" \"\"))]\n+          UNSPEC_PRED_X))]\n+  \"TARGET_SVE2\"\n+  \"addhnb\\t%0.<Ventype>, %2.<Vetype>, %3.<Vetype>\"\n+)\n+\n ;; -------------------------------------------------------------------------\n ;; ---- [INT] Narrowing right shifts\n ;; -------------------------------------------------------------------------\n@@ -2313,46 +2328,6 @@\n   \"<sve_int_op>\\t%0.<Vewtype>, %1.<Vetype>, %2.<Vetype>\"\n )\n \n-;; -------------------------------------------------------------------------\n-;; ---- [INT] Misc optab implementations\n-;; -------------------------------------------------------------------------\n-;; Includes:\n-;; - aarch64_bitmask_udiv\n-;; -------------------------------------------------------------------------\n-\n-;; div optimizations using narrowings\n-;; we can do the division e.g. shorts by 255 faster by calculating it as\n-;; (x + ((x + 257) >> 8)) >> 8 assuming the operation is done in\n-;; double the precision of x.\n-;;\n-;; See aarch64-simd.md for bigger explanation.\n-(define_expand \"@aarch64_bitmask_udiv<mode>3\"\n-  [(match_operand:SVE_FULL_HSDI 0 \"register_operand\")\n-   (match_operand:SVE_FULL_HSDI 1 \"register_operand\")\n-   (match_operand:SVE_FULL_HSDI 2 \"immediate_operand\")]\n-  \"TARGET_SVE2\"\n-{\n-  unsigned HOST_WIDE_INT size\n-    = (1ULL << GET_MODE_UNIT_BITSIZE (<VNARROW>mode)) - 1;\n-  rtx elt = unwrap_const_vec_duplicate (operands[2]);\n-  if (!CONST_INT_P (elt) || UINTVAL (elt) != size)\n-    FAIL;\n-\n-  rtx addend = gen_reg_rtx (<MODE>mode);\n-  rtx tmp1 = gen_reg_rtx (<VNARROW>mode);\n-  rtx tmp2 = gen_reg_rtx (<VNARROW>mode);\n-  rtx val = aarch64_simd_gen_const_vector_dup (<VNARROW>mode, 1);\n-  emit_move_insn (addend, lowpart_subreg (<MODE>mode, val, <VNARROW>mode));\n-  emit_insn (gen_aarch64_sve (UNSPEC_ADDHNB, <MODE>mode, tmp1, operands[1],\n-\t\t\t      addend));\n-  emit_insn (gen_aarch64_sve (UNSPEC_ADDHNB, <MODE>mode, tmp2, operands[1],\n-\t\t\t      lowpart_subreg (<MODE>mode, tmp1,\n-\t\t\t\t\t      <VNARROW>mode)));\n-  emit_move_insn (operands[0],\n-\t\t  lowpart_subreg (<MODE>mode, tmp2, <VNARROW>mode));\n-  DONE;\n-})\n-\n ;; =========================================================================\n ;; == Permutation\n ;; ========================================================================="}, {"sha": "8a06879e94f538aaadb57479dcd4ca0531982f56", "filename": "gcc/config/aarch64/aarch64.cc", "status": "modified", "additions": 17, "deletions": 44, "changes": 61, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f23dc726875c26f2c38dfded453aa9beba0b9be9/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f23dc726875c26f2c38dfded453aa9beba0b9be9/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.cc?ref=f23dc726875c26f2c38dfded453aa9beba0b9be9", "patch": "@@ -3847,6 +3847,19 @@ aarch64_vectorize_related_mode (machine_mode vector_mode,\n   return default_vectorize_related_mode (vector_mode, element_mode, nunits);\n }\n \n+/* Implement TARGET_VECTORIZE_PREFERRED_DIV_AS_SHIFTS_OVER_MULT.  */\n+\n+static bool\n+aarch64_vectorize_preferred_div_as_shifts_over_mult (const_tree type)\n+{\n+  machine_mode mode = TYPE_MODE (type);\n+  unsigned int vec_flags = aarch64_classify_vector_mode (mode);\n+  bool sve_p = (vec_flags & VEC_ANY_SVE);\n+  bool simd_p = (vec_flags & VEC_ADVSIMD);\n+\n+  return (sve_p && TARGET_SVE2) || (simd_p && TARGET_SIMD);\n+}\n+\n /* Implement TARGET_PREFERRED_ELSE_VALUE.  For binary operations,\n    prefer to use the first arithmetic operand as the else value if\n    the else value doesn't matter, since that exactly matches the SVE\n@@ -24361,46 +24374,6 @@ aarch64_vectorize_vec_perm_const (machine_mode vmode, machine_mode op_mode,\n \n   return ret;\n }\n-\n-/* Implement TARGET_VECTORIZE_CAN_SPECIAL_DIV_BY_CONST.  */\n-\n-bool\n-aarch64_vectorize_can_special_div_by_constant (enum tree_code code,\n-\t\t\t\t\t       tree vectype, wide_int cst,\n-\t\t\t\t\t       rtx *output, rtx in0, rtx in1)\n-{\n-  if (code != TRUNC_DIV_EXPR\n-      || !TYPE_UNSIGNED (vectype))\n-    return false;\n-\n-  machine_mode mode = TYPE_MODE (vectype);\n-  unsigned int flags = aarch64_classify_vector_mode (mode);\n-  if ((flags & VEC_ANY_SVE) && !TARGET_SVE2)\n-    return false;\n-\n-  int pow = wi::exact_log2 (cst + 1);\n-  auto insn_code = maybe_code_for_aarch64_bitmask_udiv3 (TYPE_MODE (vectype));\n-  /* SVE actually has a div operator, we may have gotten here through\n-     that route.  */\n-  if (pow != (int) (element_precision (vectype) / 2)\n-      || insn_code == CODE_FOR_nothing)\n-    return false;\n-\n-  /* We can use the optimized pattern.  */\n-  if (in0 == NULL_RTX && in1 == NULL_RTX)\n-    return true;\n-\n-  gcc_assert (output);\n-\n-  expand_operand ops[3];\n-  create_output_operand (&ops[0], *output, mode);\n-  create_input_operand (&ops[1], in0, mode);\n-  create_fixed_operand (&ops[2], in1);\n-  expand_insn (insn_code, 3, ops);\n-  *output = ops[0].value;\n-  return true;\n-}\n-\n /* Generate a byte permute mask for a register of mode MODE,\n    which has NUNITS units.  */\n \n@@ -27902,13 +27875,13 @@ aarch64_libgcc_floating_mode_supported_p\n #undef TARGET_MAX_ANCHOR_OFFSET\n #define TARGET_MAX_ANCHOR_OFFSET 4095\n \n+#undef TARGET_VECTORIZE_PREFERRED_DIV_AS_SHIFTS_OVER_MULT\n+#define TARGET_VECTORIZE_PREFERRED_DIV_AS_SHIFTS_OVER_MULT \\\n+  aarch64_vectorize_preferred_div_as_shifts_over_mult\n+\n #undef TARGET_VECTOR_ALIGNMENT\n #define TARGET_VECTOR_ALIGNMENT aarch64_simd_vector_alignment\n \n-#undef TARGET_VECTORIZE_CAN_SPECIAL_DIV_BY_CONST\n-#define TARGET_VECTORIZE_CAN_SPECIAL_DIV_BY_CONST \\\n-  aarch64_vectorize_can_special_div_by_constant\n-\n #undef TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT\n #define TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT \\\n   aarch64_vectorize_preferred_vector_alignment"}]}