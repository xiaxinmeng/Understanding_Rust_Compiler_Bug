{"sha": "bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YmI2MGVmZDliZjQ3MmY0YTZjYTRlMjA3MTkzMWQzNzYxYmQ4YjZjNA==", "commit": {"author": {"name": "James Greenhalgh", "email": "james.greenhalgh@arm.com", "date": "2013-05-01T10:37:28Z"}, "committer": {"name": "James Greenhalgh", "email": "jgreenhalgh@gcc.gnu.org", "date": "2013-05-01T10:37:28Z"}, "message": "[AArch64] Remap neon vcmp functions to C/TREE\n\ngcc/\n\t* config/aarch64/aarch64-builtins.c (BUILTIN_VALLDI): Define.\n\t(aarch64_fold_builtin): Add folding for cm<eq,ge,gt,tst>.\n\t* config/aarch64/aarch64-simd-builtins.def\n\t(cmeq): Update to BUILTIN_VALLDI.\n\t(cmgt): Likewise.\n\t(cmge): Likewise.\n\t(cmle): Likewise.\n\t(cmlt): Likewise.\n\t* config/aarch64/arm_neon.h\n\t(vc<eq, lt, le, gt, ge, tst><z><qsd>_<fpsu><8,16,32,64>): Remap\n\tto builtins or C as appropriate.\n\nFrom-SVN: r198491", "tree": {"sha": "534a612f13b512b6ec75b93b3ae303961027b6eb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/534a612f13b512b6ec75b93b3ae303961027b6eb"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4", "html_url": "https://github.com/Rust-GCC/gccrs/commit/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4/comments", "author": {"login": "jgreenhalgh-arm", "id": 6104025, "node_id": "MDQ6VXNlcjYxMDQwMjU=", "avatar_url": "https://avatars.githubusercontent.com/u/6104025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgreenhalgh-arm", "html_url": "https://github.com/jgreenhalgh-arm", "followers_url": "https://api.github.com/users/jgreenhalgh-arm/followers", "following_url": "https://api.github.com/users/jgreenhalgh-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jgreenhalgh-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgreenhalgh-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgreenhalgh-arm/subscriptions", "organizations_url": "https://api.github.com/users/jgreenhalgh-arm/orgs", "repos_url": "https://api.github.com/users/jgreenhalgh-arm/repos", "events_url": "https://api.github.com/users/jgreenhalgh-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jgreenhalgh-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "889b9412392bbde643b9fce035a8b0025c54ebe6", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/889b9412392bbde643b9fce035a8b0025c54ebe6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/889b9412392bbde643b9fce035a8b0025c54ebe6"}], "stats": {"total": 1576, "additions": 1199, "deletions": 377}, "files": [{"sha": "94959e0a0cab5a3d718d1d7c71335e66d15f2e65", "filename": "gcc/ChangeLog", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4", "patch": "@@ -1,3 +1,17 @@\n+2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>\n+\n+\t* config/aarch64/aarch64-builtins.c (BUILTIN_VALLDI): Define.\n+\t(aarch64_fold_builtin): Add folding for cm<eq,ge,gt,tst>.\n+\t* config/aarch64/aarch64-simd-builtins.def\n+\t(cmeq): Update to BUILTIN_VALLDI.\n+\t(cmgt): Likewise.\n+\t(cmge): Likewise.\n+\t(cmle): Likewise.\n+\t(cmlt): Likewise.\n+\t* config/aarch64/arm_neon.h\n+\t(vc<eq, lt, le, gt, ge, tst><z><qsd>_<fpsu><8,16,32,64>): Remap\n+\tto builtins or C as appropriate.\n+\n 2013-05-01  James Greenhalgh  <james.greenhalgh@arm.com>\n \n \t* config/aarch64/aarch64-simd-builtins.def (cmhs): Rename to..."}, {"sha": "3016f256869a232ef4fcb18f8d55e49c569b692b", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4", "patch": "@@ -191,6 +191,9 @@ typedef struct\n #define BUILTIN_VALL(T, N, MAP) \\\n   VAR10 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, \\\n \t v4si, v2di, v2sf, v4sf, v2df)\n+#define BUILTIN_VALLDI(T, N, MAP) \\\n+  VAR11 (T, N, MAP, v8qi, v16qi, v4hi, v8hi, v2si, \\\n+\t v4si, v2di, v2sf, v4sf, v2df, di)\n #define BUILTIN_VB(T, N, MAP) \\\n   VAR2 (T, N, MAP, v8qi, v16qi)\n #define BUILTIN_VD(T, N, MAP) \\\n@@ -1314,11 +1317,26 @@ aarch64_fold_builtin (tree fndecl, int n_args ATTRIBUTE_UNUSED, tree *args,\n       BUILTIN_VDQF (UNOP, abs, 2)\n \treturn fold_build1 (ABS_EXPR, type, args[0]);\n \tbreak;\n+      BUILTIN_VALLDI (BINOP, cmge, 0)\n+\treturn fold_build2 (GE_EXPR, type, args[0], args[1]);\n+\tbreak;\n+      BUILTIN_VALLDI (BINOP, cmgt, 0)\n+\treturn fold_build2 (GT_EXPR, type, args[0], args[1]);\n+\tbreak;\n+      BUILTIN_VALLDI (BINOP, cmeq, 0)\n+\treturn fold_build2 (EQ_EXPR, type, args[0], args[1]);\n+\tbreak;\n+      BUILTIN_VSDQ_I_DI (BINOP, cmtst, 0)\n+\t{\n+\t  tree and_node = fold_build2 (BIT_AND_EXPR, type, args[0], args[1]);\n+\t  tree vec_zero_node = build_zero_cst (type);\n+\t  return fold_build2 (NE_EXPR, type, and_node, vec_zero_node);\n+\t  break;\n+\t}\n       VAR1 (UNOP, floatv2si, 2, v2sf)\n       VAR1 (UNOP, floatv4si, 2, v4sf)\n       VAR1 (UNOP, floatv2di, 2, v2df)\n \treturn fold_build1 (FLOAT_EXPR, type, args[0]);\n-\tbreak;\n       default:\n \tbreak;\n     }"}, {"sha": "620406b449d92eb3278e7bd26af7e91a40abcf6e", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4", "patch": "@@ -224,11 +224,11 @@\n   BUILTIN_VSDQ_I (SHIFTIMM, uqshl_n, 0)\n \n   /* Implemented by aarch64_cm<cmp><mode>.  */\n-  BUILTIN_VSDQ_I_DI (BINOP, cmeq, 0)\n-  BUILTIN_VSDQ_I_DI (BINOP, cmge, 0)\n-  BUILTIN_VSDQ_I_DI (BINOP, cmgt, 0)\n-  BUILTIN_VSDQ_I_DI (BINOP, cmle, 0)\n-  BUILTIN_VSDQ_I_DI (BINOP, cmlt, 0)\n+  BUILTIN_VALLDI (BINOP, cmeq, 0)\n+  BUILTIN_VALLDI (BINOP, cmge, 0)\n+  BUILTIN_VALLDI (BINOP, cmgt, 0)\n+  BUILTIN_VALLDI (BINOP, cmle, 0)\n+  BUILTIN_VALLDI (BINOP, cmlt, 0)\n   /* Implemented by aarch64_cm<cmp><mode>.  */\n   BUILTIN_VSDQ_I_DI (BINOP, cmgeu, 0)\n   BUILTIN_VSDQ_I_DI (BINOP, cmgtu, 0)"}, {"sha": "d822130ccea2a51fb6629424c68b00c86f1b1f4a", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 1161, "deletions": 371, "changes": 1532, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=bb60efd9bf472f4a6ca4e2071931d3761bd8b6c4", "patch": "@@ -5194,226 +5194,6 @@ vcaltq_f64 (float64x2_t a, float64x2_t b)\n   return result;\n }\n \n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vceq_f32 (float32x2_t a, float32x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"fcmeq %0.2s, %1.2s, %2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vceq_f64 (float64x1_t a, float64x1_t b)\n-{\n-  uint64x1_t result;\n-  __asm__ (\"fcmeq %d0, %d1, %d2\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n-vceqd_f64 (float64_t a, float64_t b)\n-{\n-  float64_t result;\n-  __asm__ (\"fcmeq %d0,%d1,%d2\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vceqq_f32 (float32x4_t a, float32x4_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"fcmeq %0.4s, %1.4s, %2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vceqq_f64 (float64x2_t a, float64x2_t b)\n-{\n-  uint64x2_t result;\n-  __asm__ (\"fcmeq %0.2d, %1.2d, %2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32_t __attribute__ ((__always_inline__))\n-vceqs_f32 (float32_t a, float32_t b)\n-{\n-  float32_t result;\n-  __asm__ (\"fcmeq %s0,%s1,%s2\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n-vceqzd_f64 (float64_t a)\n-{\n-  float64_t result;\n-  __asm__ (\"fcmeq %d0,%d1,#0\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32_t __attribute__ ((__always_inline__))\n-vceqzs_f32 (float32_t a)\n-{\n-  float32_t result;\n-  __asm__ (\"fcmeq %s0,%s1,#0\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vcge_f32 (float32x2_t a, float32x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"fcmge %0.2s, %1.2s, %2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcge_f64 (float64x1_t a, float64x1_t b)\n-{\n-  uint64x1_t result;\n-  __asm__ (\"fcmge %d0, %d1, %d2\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vcgeq_f32 (float32x4_t a, float32x4_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"fcmge %0.4s, %1.4s, %2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vcgeq_f64 (float64x2_t a, float64x2_t b)\n-{\n-  uint64x2_t result;\n-  __asm__ (\"fcmge %0.2d, %1.2d, %2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vcgt_f32 (float32x2_t a, float32x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"fcmgt %0.2s, %1.2s, %2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcgt_f64 (float64x1_t a, float64x1_t b)\n-{\n-  uint64x1_t result;\n-  __asm__ (\"fcmgt %d0, %d1, %d2\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vcgtq_f32 (float32x4_t a, float32x4_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"fcmgt %0.4s, %1.4s, %2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vcgtq_f64 (float64x2_t a, float64x2_t b)\n-{\n-  uint64x2_t result;\n-  __asm__ (\"fcmgt %0.2d, %1.2d, %2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vcle_f32 (float32x2_t a, float32x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"fcmge %0.2s, %2.2s, %1.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcle_f64 (float64x1_t a, float64x1_t b)\n-{\n-  uint64x1_t result;\n-  __asm__ (\"fcmge %d0, %d2, %d1\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vcleq_f32 (float32x4_t a, float32x4_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"fcmge %0.4s, %2.4s, %1.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vcleq_f64 (float64x2_t a, float64x2_t b)\n-{\n-  uint64x2_t result;\n-  __asm__ (\"fcmge %0.2d, %2.2d, %1.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n vcls_s8 (int8x8_t a)\n {\n@@ -5480,50 +5260,6 @@ vclsq_s32 (int32x4_t a)\n   return result;\n }\n \n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vclt_f32 (float32x2_t a, float32x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"fcmgt %0.2s, %2.2s, %1.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vclt_f64 (float64x1_t a, float64x1_t b)\n-{\n-  uint64x1_t result;\n-  __asm__ (\"fcmgt %d0, %d2, %d1\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vcltq_f32 (float32x4_t a, float32x4_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"fcmgt %0.4s, %2.4s, %1.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vcltq_f64 (float64x2_t a, float64x2_t b)\n-{\n-  uint64x2_t result;\n-  __asm__ (\"fcmgt %0.2d, %2.2d, %1.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n vclz_s8 (int8x8_t a)\n {\n@@ -18848,7 +18584,19 @@ vaddvq_f64 (float64x2_t __a)\n   return vgetq_lane_f64 (t, 0);\n }\n \n-/* vceq */\n+/* vceq - vector.  */\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vceq_f32 (float32x2_t __a, float32x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_cmeqv2sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vceq_f64 (float64x1_t __a, float64x1_t __b)\n+{\n+  return __a == __b ? -1ll : 0ll;\n+}\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vceq_p8 (poly8x8_t __a, poly8x8_t __b)\n@@ -18878,7 +18626,7 @@ vceq_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vceq_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmeqdi (__a, __b);\n+  return __a == __b ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -18905,8 +18653,19 @@ vceq_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vceq_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmeqdi ((int64x1_t) __a,\n-\t\t\t\t\t\t(int64x1_t) __b);\n+  return __a == __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vceqq_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+  return (uint32x4_t) __builtin_aarch64_cmeqv4sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vceqq_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+  return (uint64x2_t) __builtin_aarch64_cmeqv2df (__a, __b);\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n@@ -18968,76 +18727,312 @@ vceqq_u64 (uint64x2_t __a, uint64x2_t __b)\n \t\t\t\t\t\t  (int64x2_t) __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vceqd_s64 (int64x1_t __a, int64x1_t __b)\n+/* vceq - scalar.  */\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vceqs_f32 (float32_t __a, float32_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmeqdi (__a, __b);\n+  return __a == __b ? -1 : 0;\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vceqd_u64 (uint64x1_t __a, uint64x1_t __b)\n+vceqd_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmeqdi (__a, __b);\n+  return __a == __b ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vceqzd_s64 (int64x1_t __a)\n+vceqd_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmeqdi (__a, 0);\n+  return __a == __b ? -1ll : 0ll;\n }\n \n-/* vcge */\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vcge_s8 (int8x8_t __a, int8x8_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vceqd_f64 (float64_t __a, float64_t __b)\n {\n-  return (uint8x8_t) __builtin_aarch64_cmgev8qi (__a, __b);\n+  return __a == __b ? -1ll : 0ll;\n }\n \n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vcge_s16 (int16x4_t __a, int16x4_t __b)\n-{\n-  return (uint16x4_t) __builtin_aarch64_cmgev4hi (__a, __b);\n-}\n+/* vceqz - vector.  */\n \n __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vcge_s32 (int32x2_t __a, int32x2_t __b)\n+vceqz_f32 (float32x2_t __a)\n {\n-  return (uint32x2_t) __builtin_aarch64_cmgev2si (__a, __b);\n+  float32x2_t __b = {0.0f, 0.0f};\n+  return (uint32x2_t) __builtin_aarch64_cmeqv2sf (__a, __b);\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcge_s64 (int64x1_t __a, int64x1_t __b)\n+vceqz_f64 (float64x1_t __a)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmgedi (__a, __b);\n+  return __a == 0.0 ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vcge_u8 (uint8x8_t __a, uint8x8_t __b)\n+vceqz_p8 (poly8x8_t __a)\n {\n-  return (uint8x8_t) __builtin_aarch64_cmhsv8qi ((int8x8_t) __a,\n+  poly8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmeqv8qi ((int8x8_t) __a,\n \t\t\t\t\t\t (int8x8_t) __b);\n }\n \n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vceqz_s8 (int8x8_t __a)\n+{\n+  int8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmeqv8qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vceqz_s16 (int16x4_t __a)\n+{\n+  int16x4_t __b = {0, 0, 0, 0};\n+  return (uint16x4_t) __builtin_aarch64_cmeqv4hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vceqz_s32 (int32x2_t __a)\n+{\n+  int32x2_t __b = {0, 0};\n+  return (uint32x2_t) __builtin_aarch64_cmeqv2si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vceqz_s64 (int64x1_t __a)\n+{\n+  return __a == 0ll ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vceqz_u8 (uint8x8_t __a)\n+{\n+  uint8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmeqv8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vceqz_u16 (uint16x4_t __a)\n+{\n+  uint16x4_t __b = {0, 0, 0, 0};\n+  return (uint16x4_t) __builtin_aarch64_cmeqv4hi ((int16x4_t) __a,\n+\t\t\t\t\t\t  (int16x4_t) __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vceqz_u32 (uint32x2_t __a)\n+{\n+  uint32x2_t __b = {0, 0};\n+  return (uint32x2_t) __builtin_aarch64_cmeqv2si ((int32x2_t) __a,\n+\t\t\t\t\t\t  (int32x2_t) __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vceqz_u64 (uint64x1_t __a)\n+{\n+  return __a == 0ll ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vceqzq_f32 (float32x4_t __a)\n+{\n+  float32x4_t __b = {0.0f, 0.0f, 0.0f, 0.0f};\n+  return (uint32x4_t) __builtin_aarch64_cmeqv4sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vceqzq_f64 (float64x2_t __a)\n+{\n+  float64x2_t __b = {0.0, 0.0};\n+  return (uint64x2_t) __builtin_aarch64_cmeqv2df (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vceqzq_p8 (poly8x16_t __a)\n+{\n+  poly8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t    0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmeqv16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vceqzq_s8 (int8x16_t __a)\n+{\n+  int8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t   0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmeqv16qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vceqzq_s16 (int16x8_t __a)\n+{\n+  int16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint16x8_t) __builtin_aarch64_cmeqv8hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vceqzq_s32 (int32x4_t __a)\n+{\n+  int32x4_t __b = {0, 0, 0, 0};\n+  return (uint32x4_t) __builtin_aarch64_cmeqv4si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vceqzq_s64 (int64x2_t __a)\n+{\n+  int64x2_t __b = {0, 0};\n+  return (uint64x2_t) __builtin_aarch64_cmeqv2di (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vceqzq_u8 (uint8x16_t __a)\n+{\n+  uint8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t    0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmeqv16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vceqzq_u16 (uint16x8_t __a)\n+{\n+  uint16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint16x8_t) __builtin_aarch64_cmeqv8hi ((int16x8_t) __a,\n+\t\t\t\t\t\t  (int16x8_t) __b);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vceqzq_u32 (uint32x4_t __a)\n+{\n+  uint32x4_t __b = {0, 0, 0, 0};\n+  return (uint32x4_t) __builtin_aarch64_cmeqv4si ((int32x4_t) __a,\n+\t\t\t\t\t\t  (int32x4_t) __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vceqzq_u64 (uint64x2_t __a)\n+{\n+  uint64x2_t __b = {0, 0};\n+  return (uint64x2_t) __builtin_aarch64_cmeqv2di ((int64x2_t) __a,\n+\t\t\t\t\t\t  (int64x2_t) __b);\n+}\n+\n+/* vceqz - scalar.  */\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vceqzs_f32 (float32_t __a)\n+{\n+  return __a == 0.0f ? -1 : 0;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vceqzd_s64 (int64x1_t __a)\n+{\n+  return __a == 0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vceqzd_u64 (int64x1_t __a)\n+{\n+  return __a == 0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vceqzd_f64 (float64_t __a)\n+{\n+  return __a == 0.0 ? -1ll : 0ll;\n+}\n+\n+/* vcge - vector.  */\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcge_f32 (float32x2_t __a, float32x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_cmgev2sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcge_f64 (float64x1_t __a, float64x1_t __b)\n+{\n+  return __a >= __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcge_p8 (poly8x8_t __a, poly8x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_cmgev8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcge_s8 (int8x8_t __a, int8x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_cmgev8qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vcge_s16 (int16x4_t __a, int16x4_t __b)\n+{\n+  return (uint16x4_t) __builtin_aarch64_cmgev4hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcge_s32 (int32x2_t __a, int32x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_cmgev2si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcge_s64 (int64x1_t __a, int64x1_t __b)\n+{\n+  return __a >= __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcge_u8 (uint8x8_t __a, uint8x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_cmgeuv8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n vcge_u16 (uint16x4_t __a, uint16x4_t __b)\n {\n-  return (uint16x4_t) __builtin_aarch64_cmhsv4hi ((int16x4_t) __a,\n+  return (uint16x4_t) __builtin_aarch64_cmgeuv4hi ((int16x4_t) __a,\n \t\t\t\t\t\t  (int16x4_t) __b);\n }\n \n __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n vcge_u32 (uint32x2_t __a, uint32x2_t __b)\n {\n-  return (uint32x2_t) __builtin_aarch64_cmhsv2si ((int32x2_t) __a,\n+  return (uint32x2_t) __builtin_aarch64_cmgeuv2si ((int32x2_t) __a,\n \t\t\t\t\t\t  (int32x2_t) __b);\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcge_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmhsdi ((int64x1_t) __a,\n-\t\t\t\t\t\t(int64x1_t) __b);\n+  return __a >= __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcgeq_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+  return (uint32x4_t) __builtin_aarch64_cmgev4sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcgeq_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+  return (uint64x2_t) __builtin_aarch64_cmgev2df (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcgeq_p8 (poly8x16_t __a, poly8x16_t __b)\n+{\n+  return (uint8x16_t) __builtin_aarch64_cmgev16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n@@ -19067,51 +19062,268 @@ vcgeq_s64 (int64x2_t __a, int64x2_t __b)\n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n vcgeq_u8 (uint8x16_t __a, uint8x16_t __b)\n {\n-  return (uint8x16_t) __builtin_aarch64_cmhsv16qi ((int8x16_t) __a,\n+  return (uint8x16_t) __builtin_aarch64_cmgeuv16qi ((int8x16_t) __a,\n \t\t\t\t\t\t   (int8x16_t) __b);\n }\n \n __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n vcgeq_u16 (uint16x8_t __a, uint16x8_t __b)\n {\n-  return (uint16x8_t) __builtin_aarch64_cmhsv8hi ((int16x8_t) __a,\n+  return (uint16x8_t) __builtin_aarch64_cmgeuv8hi ((int16x8_t) __a,\n \t\t\t\t\t\t  (int16x8_t) __b);\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n vcgeq_u32 (uint32x4_t __a, uint32x4_t __b)\n {\n-  return (uint32x4_t) __builtin_aarch64_cmhsv4si ((int32x4_t) __a,\n+  return (uint32x4_t) __builtin_aarch64_cmgeuv4si ((int32x4_t) __a,\n \t\t\t\t\t\t  (int32x4_t) __b);\n }\n \n __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n vcgeq_u64 (uint64x2_t __a, uint64x2_t __b)\n {\n-  return (uint64x2_t) __builtin_aarch64_cmhsv2di ((int64x2_t) __a,\n+  return (uint64x2_t) __builtin_aarch64_cmgeuv2di ((int64x2_t) __a,\n \t\t\t\t\t\t  (int64x2_t) __b);\n }\n \n+/* vcge - scalar.  */\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vcges_f32 (float32_t __a, float32_t __b)\n+{\n+  return __a >= __b ? -1 : 0;\n+}\n+\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcged_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmgedi (__a, __b);\n+  return __a >= __b ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcged_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmhsdi ((int64x1_t) __a,\n-\t\t\t\t\t\t(int64x1_t) __b);\n+  return __a >= __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcged_f64 (float64_t __a, float64_t __b)\n+{\n+  return __a >= __b ? -1ll : 0ll;\n+}\n+\n+/* vcgez - vector.  */\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcgez_f32 (float32x2_t __a)\n+{\n+  float32x2_t __b = {0.0f, 0.0f};\n+  return (uint32x2_t) __builtin_aarch64_cmgev2sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgez_f64 (float64x1_t __a)\n+{\n+  return __a >= 0.0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcgez_p8 (poly8x8_t __a)\n+{\n+  poly8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmgev8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcgez_s8 (int8x8_t __a)\n+{\n+  int8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmgev8qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vcgez_s16 (int16x4_t __a)\n+{\n+  int16x4_t __b = {0, 0, 0, 0};\n+  return (uint16x4_t) __builtin_aarch64_cmgev4hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcgez_s32 (int32x2_t __a)\n+{\n+  int32x2_t __b = {0, 0};\n+  return (uint32x2_t) __builtin_aarch64_cmgev2si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgez_s64 (int64x1_t __a)\n+{\n+  return __a >= 0ll ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcgez_u8 (uint8x8_t __a)\n+{\n+  uint8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmgeuv8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vcgez_u16 (uint16x4_t __a)\n+{\n+  uint16x4_t __b = {0, 0, 0, 0};\n+  return (uint16x4_t) __builtin_aarch64_cmgeuv4hi ((int16x4_t) __a,\n+\t\t\t\t\t\t  (int16x4_t) __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcgez_u32 (uint32x2_t __a)\n+{\n+  uint32x2_t __b = {0, 0};\n+  return (uint32x2_t) __builtin_aarch64_cmgeuv2si ((int32x2_t) __a,\n+\t\t\t\t\t\t  (int32x2_t) __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgez_u64 (uint64x1_t __a)\n+{\n+  return __a >= 0ll ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcgezq_f32 (float32x4_t __a)\n+{\n+  float32x4_t __b = {0.0f, 0.0f, 0.0f, 0.0f};\n+  return (uint32x4_t) __builtin_aarch64_cmgev4sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcgezq_f64 (float64x2_t __a)\n+{\n+  float64x2_t __b = {0.0, 0.0};\n+  return (uint64x2_t) __builtin_aarch64_cmgev2df (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcgezq_p8 (poly8x16_t __a)\n+{\n+  poly8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t    0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmgev16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcgezq_s8 (int8x16_t __a)\n+{\n+  int8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t   0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmgev16qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vcgezq_s16 (int16x8_t __a)\n+{\n+  int16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint16x8_t) __builtin_aarch64_cmgev8hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcgezq_s32 (int32x4_t __a)\n+{\n+  int32x4_t __b = {0, 0, 0, 0};\n+  return (uint32x4_t) __builtin_aarch64_cmgev4si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcgezq_s64 (int64x2_t __a)\n+{\n+  int64x2_t __b = {0, 0};\n+  return (uint64x2_t) __builtin_aarch64_cmgev2di (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcgezq_u8 (uint8x16_t __a)\n+{\n+  uint8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t    0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmgeuv16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vcgezq_u16 (uint16x8_t __a)\n+{\n+  uint16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint16x8_t) __builtin_aarch64_cmgeuv8hi ((int16x8_t) __a,\n+\t\t\t\t\t\t  (int16x8_t) __b);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcgezq_u32 (uint32x4_t __a)\n+{\n+  uint32x4_t __b = {0, 0, 0, 0};\n+  return (uint32x4_t) __builtin_aarch64_cmgeuv4si ((int32x4_t) __a,\n+\t\t\t\t\t\t  (int32x4_t) __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcgezq_u64 (uint64x2_t __a)\n+{\n+  uint64x2_t __b = {0, 0};\n+  return (uint64x2_t) __builtin_aarch64_cmgeuv2di ((int64x2_t) __a,\n+\t\t\t\t\t\t  (int64x2_t) __b);\n+}\n+\n+/* vcgez - scalar.  */\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vcgezs_f32 (float32_t __a)\n+{\n+  return __a >= 0.0f ? -1 : 0;\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcgezd_s64 (int64x1_t __a)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmgedi (__a, 0);\n+  return __a >= 0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgezd_u64 (int64x1_t __a)\n+{\n+  return __a >= 0 ? -1ll : 0ll;\n }\n \n-/* vcgt */\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcgezd_f64 (float64_t __a)\n+{\n+  return __a >= 0.0 ? -1ll : 0ll;\n+}\n+\n+/* vcgt - vector.  */\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcgt_f32 (float32x2_t __a, float32x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_cmgtv2sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgt_f64 (float64x1_t __a, float64x1_t __b)\n+{\n+  return __a > __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcgt_p8 (poly8x8_t __a, poly8x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_cmgtv8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vcgt_s8 (int8x8_t __a, int8x8_t __b)\n@@ -19134,109 +19346,344 @@ vcgt_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcgt_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmgtdi (__a, __b);\n+  return __a > __b ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vcgt_u8 (uint8x8_t __a, uint8x8_t __b)\n {\n-  return (uint8x8_t) __builtin_aarch64_cmhiv8qi ((int8x8_t) __a,\n-\t\t\t\t\t\t (int8x8_t) __b);\n+  return (uint8x8_t) __builtin_aarch64_cmgtuv8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vcgt_u16 (uint16x4_t __a, uint16x4_t __b)\n+{\n+  return (uint16x4_t) __builtin_aarch64_cmgtuv4hi ((int16x4_t) __a,\n+\t\t\t\t\t\t  (int16x4_t) __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcgt_u32 (uint32x2_t __a, uint32x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_cmgtuv2si ((int32x2_t) __a,\n+\t\t\t\t\t\t  (int32x2_t) __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgt_u64 (uint64x1_t __a, uint64x1_t __b)\n+{\n+  return __a > __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcgtq_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+  return (uint32x4_t) __builtin_aarch64_cmgtv4sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcgtq_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+  return (uint64x2_t) __builtin_aarch64_cmgtv2df (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcgtq_p8 (poly8x16_t __a, poly8x16_t __b)\n+{\n+  return (uint8x16_t) __builtin_aarch64_cmgtv16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcgtq_s8 (int8x16_t __a, int8x16_t __b)\n+{\n+  return (uint8x16_t) __builtin_aarch64_cmgtv16qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vcgtq_s16 (int16x8_t __a, int16x8_t __b)\n+{\n+  return (uint16x8_t) __builtin_aarch64_cmgtv8hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcgtq_s32 (int32x4_t __a, int32x4_t __b)\n+{\n+  return (uint32x4_t) __builtin_aarch64_cmgtv4si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcgtq_s64 (int64x2_t __a, int64x2_t __b)\n+{\n+  return (uint64x2_t) __builtin_aarch64_cmgtv2di (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcgtq_u8 (uint8x16_t __a, uint8x16_t __b)\n+{\n+  return (uint8x16_t) __builtin_aarch64_cmgtuv16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vcgtq_u16 (uint16x8_t __a, uint16x8_t __b)\n+{\n+  return (uint16x8_t) __builtin_aarch64_cmgtuv8hi ((int16x8_t) __a,\n+\t\t\t\t\t\t  (int16x8_t) __b);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcgtq_u32 (uint32x4_t __a, uint32x4_t __b)\n+{\n+  return (uint32x4_t) __builtin_aarch64_cmgtuv4si ((int32x4_t) __a,\n+\t\t\t\t\t\t  (int32x4_t) __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcgtq_u64 (uint64x2_t __a, uint64x2_t __b)\n+{\n+  return (uint64x2_t) __builtin_aarch64_cmgtuv2di ((int64x2_t) __a,\n+\t\t\t\t\t\t  (int64x2_t) __b);\n+}\n+\n+/* vcgt - scalar.  */\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vcgts_f32 (float32_t __a, float32_t __b)\n+{\n+  return __a > __b ? -1 : 0;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgtd_s64 (int64x1_t __a, int64x1_t __b)\n+{\n+  return __a > __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgtd_u64 (uint64x1_t __a, uint64x1_t __b)\n+{\n+  return __a > __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcgtd_f64 (float64_t __a, float64_t __b)\n+{\n+  return __a > __b ? -1ll : 0ll;\n+}\n+\n+/* vcgtz - vector.  */\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcgtz_f32 (float32x2_t __a)\n+{\n+  float32x2_t __b = {0.0f, 0.0f};\n+  return (uint32x2_t) __builtin_aarch64_cmgtv2sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgtz_f64 (float64x1_t __a)\n+{\n+  return __a > 0.0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcgtz_p8 (poly8x8_t __a)\n+{\n+  poly8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmgtv8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcgtz_s8 (int8x8_t __a)\n+{\n+  int8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmgtv8qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vcgtz_s16 (int16x4_t __a)\n+{\n+  int16x4_t __b = {0, 0, 0, 0};\n+  return (uint16x4_t) __builtin_aarch64_cmgtv4hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcgtz_s32 (int32x2_t __a)\n+{\n+  int32x2_t __b = {0, 0};\n+  return (uint32x2_t) __builtin_aarch64_cmgtv2si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgtz_s64 (int64x1_t __a)\n+{\n+  return __a > 0ll ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcgtz_u8 (uint8x8_t __a)\n+{\n+  uint8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmgtuv8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vcgtz_u16 (uint16x4_t __a)\n+{\n+  uint16x4_t __b = {0, 0, 0, 0};\n+  return (uint16x4_t) __builtin_aarch64_cmgtuv4hi ((int16x4_t) __a,\n+\t\t\t\t\t\t  (int16x4_t) __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcgtz_u32 (uint32x2_t __a)\n+{\n+  uint32x2_t __b = {0, 0};\n+  return (uint32x2_t) __builtin_aarch64_cmgtuv2si ((int32x2_t) __a,\n+\t\t\t\t\t\t  (int32x2_t) __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcgtz_u64 (uint64x1_t __a)\n+{\n+  return __a > 0ll ? -1ll : 0ll;\n }\n \n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vcgt_u16 (uint16x4_t __a, uint16x4_t __b)\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcgtzq_f32 (float32x4_t __a)\n {\n-  return (uint16x4_t) __builtin_aarch64_cmhiv4hi ((int16x4_t) __a,\n-\t\t\t\t\t\t  (int16x4_t) __b);\n+  float32x4_t __b = {0.0f, 0.0f, 0.0f, 0.0f};\n+  return (uint32x4_t) __builtin_aarch64_cmgtv4sf (__a, __b);\n }\n \n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vcgt_u32 (uint32x2_t __a, uint32x2_t __b)\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcgtzq_f64 (float64x2_t __a)\n {\n-  return (uint32x2_t) __builtin_aarch64_cmhiv2si ((int32x2_t) __a,\n-\t\t\t\t\t\t  (int32x2_t) __b);\n+  float64x2_t __b = {0.0, 0.0};\n+  return (uint64x2_t) __builtin_aarch64_cmgtv2df (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcgt_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcgtzq_p8 (poly8x16_t __a)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmhidi ((int64x1_t) __a,\n-\t\t\t\t\t\t(int64x1_t) __b);\n+  poly8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t    0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmgtv16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vcgtq_s8 (int8x16_t __a, int8x16_t __b)\n+vcgtzq_s8 (int8x16_t __a)\n {\n+  int8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t   0, 0, 0, 0, 0, 0, 0, 0};\n   return (uint8x16_t) __builtin_aarch64_cmgtv16qi (__a, __b);\n }\n \n __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vcgtq_s16 (int16x8_t __a, int16x8_t __b)\n+vcgtzq_s16 (int16x8_t __a)\n {\n+  int16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n   return (uint16x8_t) __builtin_aarch64_cmgtv8hi (__a, __b);\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vcgtq_s32 (int32x4_t __a, int32x4_t __b)\n+vcgtzq_s32 (int32x4_t __a)\n {\n+  int32x4_t __b = {0, 0, 0, 0};\n   return (uint32x4_t) __builtin_aarch64_cmgtv4si (__a, __b);\n }\n \n __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vcgtq_s64 (int64x2_t __a, int64x2_t __b)\n+vcgtzq_s64 (int64x2_t __a)\n {\n+  int64x2_t __b = {0, 0};\n   return (uint64x2_t) __builtin_aarch64_cmgtv2di (__a, __b);\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vcgtq_u8 (uint8x16_t __a, uint8x16_t __b)\n+vcgtzq_u8 (uint8x16_t __a)\n {\n-  return (uint8x16_t) __builtin_aarch64_cmhiv16qi ((int8x16_t) __a,\n+  uint8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t    0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmgtuv16qi ((int8x16_t) __a,\n \t\t\t\t\t\t   (int8x16_t) __b);\n }\n \n __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vcgtq_u16 (uint16x8_t __a, uint16x8_t __b)\n+vcgtzq_u16 (uint16x8_t __a)\n {\n-  return (uint16x8_t) __builtin_aarch64_cmhiv8hi ((int16x8_t) __a,\n+  uint16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint16x8_t) __builtin_aarch64_cmgtuv8hi ((int16x8_t) __a,\n \t\t\t\t\t\t  (int16x8_t) __b);\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vcgtq_u32 (uint32x4_t __a, uint32x4_t __b)\n+vcgtzq_u32 (uint32x4_t __a)\n {\n-  return (uint32x4_t) __builtin_aarch64_cmhiv4si ((int32x4_t) __a,\n+  uint32x4_t __b = {0, 0, 0, 0};\n+  return (uint32x4_t) __builtin_aarch64_cmgtuv4si ((int32x4_t) __a,\n \t\t\t\t\t\t  (int32x4_t) __b);\n }\n \n __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vcgtq_u64 (uint64x2_t __a, uint64x2_t __b)\n+vcgtzq_u64 (uint64x2_t __a)\n {\n-  return (uint64x2_t) __builtin_aarch64_cmhiv2di ((int64x2_t) __a,\n+  uint64x2_t __b = {0, 0};\n+  return (uint64x2_t) __builtin_aarch64_cmgtuv2di ((int64x2_t) __a,\n \t\t\t\t\t\t  (int64x2_t) __b);\n }\n \n+/* vcgtz - scalar.  */\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vcgtzs_f32 (float32_t __a)\n+{\n+  return __a > 0.0f ? -1 : 0;\n+}\n+\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcgtd_s64 (int64x1_t __a, int64x1_t __b)\n+vcgtzd_s64 (int64x1_t __a)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmgtdi (__a, __b);\n+  return __a > 0 ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcgtd_u64 (uint64x1_t __a, uint64x1_t __b)\n+vcgtzd_u64 (int64x1_t __a)\n+{\n+  return __a > 0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcgtzd_f64 (float64_t __a)\n+{\n+  return __a > 0.0 ? -1ll : 0ll;\n+}\n+\n+/* vcle - vector.  */\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcle_f32 (float32x2_t __a, float32x2_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmhidi ((int64x1_t) __a,\n-\t\t\t\t\t\t(int64x1_t) __b);\n+  return (uint32x2_t) __builtin_aarch64_cmgev2sf (__b, __a);\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcgtzd_s64 (int64x1_t __a)\n+vcle_f64 (float64x1_t __a, float64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmgtdi (__a, 0);\n+  return __a <= __b ? -1ll : 0ll;\n }\n \n-/* vcle */\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcle_p8 (poly8x8_t __a, poly8x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_cmgev8qi ((int8x8_t) __b,\n+\t\t\t\t\t\t (int8x8_t) __a);\n+}\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vcle_s8 (int8x8_t __a, int8x8_t __b)\n@@ -19259,35 +19706,53 @@ vcle_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcle_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmgedi (__b, __a);\n+  return __a <= __b ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vcle_u8 (uint8x8_t __a, uint8x8_t __b)\n {\n-  return (uint8x8_t) __builtin_aarch64_cmhsv8qi ((int8x8_t) __b,\n+  return (uint8x8_t) __builtin_aarch64_cmgeuv8qi ((int8x8_t) __b,\n \t\t\t\t\t\t (int8x8_t) __a);\n }\n \n __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n vcle_u16 (uint16x4_t __a, uint16x4_t __b)\n {\n-  return (uint16x4_t) __builtin_aarch64_cmhsv4hi ((int16x4_t) __b,\n+  return (uint16x4_t) __builtin_aarch64_cmgeuv4hi ((int16x4_t) __b,\n \t\t\t\t\t\t  (int16x4_t) __a);\n }\n \n __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n vcle_u32 (uint32x2_t __a, uint32x2_t __b)\n {\n-  return (uint32x2_t) __builtin_aarch64_cmhsv2si ((int32x2_t) __b,\n+  return (uint32x2_t) __builtin_aarch64_cmgeuv2si ((int32x2_t) __b,\n \t\t\t\t\t\t  (int32x2_t) __a);\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcle_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmhsdi ((int64x1_t) __b,\n-\t\t\t\t\t\t(int64x1_t) __a);\n+  return __a <= __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcleq_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+  return (uint32x4_t) __builtin_aarch64_cmgev4sf (__b, __a);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcleq_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+  return (uint64x2_t) __builtin_aarch64_cmgev2df (__b, __a);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcleq_p8 (poly8x16_t __a, poly8x16_t __b)\n+{\n+  return (uint8x16_t) __builtin_aarch64_cmgev16qi ((int8x16_t) __b,\n+\t\t\t\t\t\t   (int8x16_t) __a);\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n@@ -19317,44 +19782,211 @@ vcleq_s64 (int64x2_t __a, int64x2_t __b)\n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n vcleq_u8 (uint8x16_t __a, uint8x16_t __b)\n {\n-  return (uint8x16_t) __builtin_aarch64_cmhsv16qi ((int8x16_t) __b,\n+  return (uint8x16_t) __builtin_aarch64_cmgeuv16qi ((int8x16_t) __b,\n \t\t\t\t\t\t   (int8x16_t) __a);\n }\n \n __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n vcleq_u16 (uint16x8_t __a, uint16x8_t __b)\n {\n-  return (uint16x8_t) __builtin_aarch64_cmhsv8hi ((int16x8_t) __b,\n+  return (uint16x8_t) __builtin_aarch64_cmgeuv8hi ((int16x8_t) __b,\n \t\t\t\t\t\t  (int16x8_t) __a);\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n vcleq_u32 (uint32x4_t __a, uint32x4_t __b)\n {\n-  return (uint32x4_t) __builtin_aarch64_cmhsv4si ((int32x4_t) __b,\n+  return (uint32x4_t) __builtin_aarch64_cmgeuv4si ((int32x4_t) __b,\n \t\t\t\t\t\t  (int32x4_t) __a);\n }\n \n __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n vcleq_u64 (uint64x2_t __a, uint64x2_t __b)\n {\n-  return (uint64x2_t) __builtin_aarch64_cmhsv2di ((int64x2_t) __b,\n+  return (uint64x2_t) __builtin_aarch64_cmgeuv2di ((int64x2_t) __b,\n \t\t\t\t\t\t  (int64x2_t) __a);\n }\n \n+/* vcle - scalar.  */\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vcles_f32 (float32_t __a, float32_t __b)\n+{\n+  return __a <= __b ? -1 : 0;\n+}\n+\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcled_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmgedi (__b, __a);\n+  return __a <= __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcled_u64 (uint64x1_t __a, uint64x1_t __b)\n+{\n+  return __a <= __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcled_f64 (float64_t __a, float64_t __b)\n+{\n+  return __a <= __b ? -1ll : 0ll;\n+}\n+\n+/* vclez - vector.  */\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vclez_f32 (float32x2_t __a)\n+{\n+  float32x2_t __b = {0.0f, 0.0f};\n+  return (uint32x2_t) __builtin_aarch64_cmlev2sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vclez_f64 (float64x1_t __a)\n+{\n+  return __a <= 0.0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vclez_p8 (poly8x8_t __a)\n+{\n+  poly8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmlev8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vclez_s8 (int8x8_t __a)\n+{\n+  int8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmlev8qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vclez_s16 (int16x4_t __a)\n+{\n+  int16x4_t __b = {0, 0, 0, 0};\n+  return (uint16x4_t) __builtin_aarch64_cmlev4hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vclez_s32 (int32x2_t __a)\n+{\n+  int32x2_t __b = {0, 0};\n+  return (uint32x2_t) __builtin_aarch64_cmlev2si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vclez_s64 (int64x1_t __a)\n+{\n+  return __a <= 0ll ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vclez_u64 (uint64x1_t __a)\n+{\n+  return __a <= 0ll ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vclezq_f32 (float32x4_t __a)\n+{\n+  float32x4_t __b = {0.0f, 0.0f, 0.0f, 0.0f};\n+  return (uint32x4_t) __builtin_aarch64_cmlev4sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vclezq_f64 (float64x2_t __a)\n+{\n+  float64x2_t __b = {0.0, 0.0};\n+  return (uint64x2_t) __builtin_aarch64_cmlev2df (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vclezq_p8 (poly8x16_t __a)\n+{\n+  poly8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t    0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmlev16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vclezq_s8 (int8x16_t __a)\n+{\n+  int8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t   0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmlev16qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vclezq_s16 (int16x8_t __a)\n+{\n+  int16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint16x8_t) __builtin_aarch64_cmlev8hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vclezq_s32 (int32x4_t __a)\n+{\n+  int32x4_t __b = {0, 0, 0, 0};\n+  return (uint32x4_t) __builtin_aarch64_cmlev4si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vclezq_s64 (int64x2_t __a)\n+{\n+  int64x2_t __b = {0, 0};\n+  return (uint64x2_t) __builtin_aarch64_cmlev2di (__a, __b);\n+}\n+\n+/* vclez - scalar.  */\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vclezs_f32 (float32_t __a)\n+{\n+  return __a <= 0.0f ? -1 : 0;\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vclezd_s64 (int64x1_t __a)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmledi (__a, 0);\n+  return __a <= 0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vclezd_u64 (int64x1_t __a)\n+{\n+  return __a <= 0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vclezd_f64 (float64_t __a)\n+{\n+  return __a <= 0.0 ? -1ll : 0ll;\n+}\n+\n+/* vclt - vector.  */\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vclt_f32 (float32x2_t __a, float32x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_cmgtv2sf (__b, __a);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vclt_f64 (float64x1_t __a, float64x1_t __b)\n+{\n+  return __a < __b ? -1ll : 0ll;\n }\n \n-/* vclt */\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vclt_p8 (poly8x8_t __a, poly8x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_cmgtv8qi ((int8x8_t) __b,\n+\t\t\t\t\t\t (int8x8_t) __a);\n+}\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vclt_s8 (int8x8_t __a, int8x8_t __b)\n@@ -19377,35 +20009,53 @@ vclt_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vclt_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmgtdi (__b, __a);\n+  return __a < __b ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vclt_u8 (uint8x8_t __a, uint8x8_t __b)\n {\n-  return (uint8x8_t) __builtin_aarch64_cmhiv8qi ((int8x8_t) __b,\n+  return (uint8x8_t) __builtin_aarch64_cmgtuv8qi ((int8x8_t) __b,\n \t\t\t\t\t\t (int8x8_t) __a);\n }\n \n __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n vclt_u16 (uint16x4_t __a, uint16x4_t __b)\n {\n-  return (uint16x4_t) __builtin_aarch64_cmhiv4hi ((int16x4_t) __b,\n+  return (uint16x4_t) __builtin_aarch64_cmgtuv4hi ((int16x4_t) __b,\n \t\t\t\t\t\t  (int16x4_t) __a);\n }\n \n __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n vclt_u32 (uint32x2_t __a, uint32x2_t __b)\n {\n-  return (uint32x2_t) __builtin_aarch64_cmhiv2si ((int32x2_t) __b,\n+  return (uint32x2_t) __builtin_aarch64_cmgtuv2si ((int32x2_t) __b,\n \t\t\t\t\t\t  (int32x2_t) __a);\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vclt_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmhidi ((int64x1_t) __b,\n-\t\t\t\t\t\t(int64x1_t) __a);\n+  return __a < __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcltq_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+  return (uint32x4_t) __builtin_aarch64_cmgtv4sf (__b, __a);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcltq_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+  return (uint64x2_t) __builtin_aarch64_cmgtv2df (__b, __a);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcltq_p8 (poly8x16_t __a, poly8x16_t __b)\n+{\n+  return (uint8x16_t) __builtin_aarch64_cmgtv16qi ((int8x16_t) __b,\n+\t\t\t\t\t\t   (int8x16_t) __a);\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n@@ -19435,41 +20085,183 @@ vcltq_s64 (int64x2_t __a, int64x2_t __b)\n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n vcltq_u8 (uint8x16_t __a, uint8x16_t __b)\n {\n-  return (uint8x16_t) __builtin_aarch64_cmhiv16qi ((int8x16_t) __b,\n+  return (uint8x16_t) __builtin_aarch64_cmgtuv16qi ((int8x16_t) __b,\n \t\t\t\t\t\t   (int8x16_t) __a);\n }\n \n __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n vcltq_u16 (uint16x8_t __a, uint16x8_t __b)\n {\n-  return (uint16x8_t) __builtin_aarch64_cmhiv8hi ((int16x8_t) __b,\n+  return (uint16x8_t) __builtin_aarch64_cmgtuv8hi ((int16x8_t) __b,\n \t\t\t\t\t\t  (int16x8_t) __a);\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n vcltq_u32 (uint32x4_t __a, uint32x4_t __b)\n {\n-  return (uint32x4_t) __builtin_aarch64_cmhiv4si ((int32x4_t) __b,\n+  return (uint32x4_t) __builtin_aarch64_cmgtuv4si ((int32x4_t) __b,\n \t\t\t\t\t\t  (int32x4_t) __a);\n }\n \n __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n vcltq_u64 (uint64x2_t __a, uint64x2_t __b)\n {\n-  return (uint64x2_t) __builtin_aarch64_cmhiv2di ((int64x2_t) __b,\n+  return (uint64x2_t) __builtin_aarch64_cmgtuv2di ((int64x2_t) __b,\n \t\t\t\t\t\t  (int64x2_t) __a);\n }\n \n+/* vclt - scalar.  */\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vclts_f32 (float32_t __a, float32_t __b)\n+{\n+  return __a < __b ? -1 : 0;\n+}\n+\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcltd_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmgtdi (__b, __a);\n+  return __a < __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcltd_u64 (uint64x1_t __a, uint64x1_t __b)\n+{\n+  return __a < __b ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcltd_f64 (float64_t __a, float64_t __b)\n+{\n+  return __a < __b ? -1ll : 0ll;\n+}\n+\n+/* vcltz - vector.  */\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcltz_f32 (float32x2_t __a)\n+{\n+  float32x2_t __b = {0.0f, 0.0f};\n+  return (uint32x2_t) __builtin_aarch64_cmltv2sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcltz_f64 (float64x1_t __a)\n+{\n+  return __a < 0.0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcltz_p8 (poly8x8_t __a)\n+{\n+  poly8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmltv8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vcltz_s8 (int8x8_t __a)\n+{\n+  int8x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x8_t) __builtin_aarch64_cmltv8qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vcltz_s16 (int16x4_t __a)\n+{\n+  int16x4_t __b = {0, 0, 0, 0};\n+  return (uint16x4_t) __builtin_aarch64_cmltv4hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vcltz_s32 (int32x2_t __a)\n+{\n+  int32x2_t __b = {0, 0};\n+  return (uint32x2_t) __builtin_aarch64_cmltv2si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcltz_s64 (int64x1_t __a)\n+{\n+  return __a < 0ll ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcltzq_f32 (float32x4_t __a)\n+{\n+  float32x4_t __b = {0.0f, 0.0f, 0.0f, 0.0f};\n+  return (uint32x4_t) __builtin_aarch64_cmltv4sf (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcltzq_f64 (float64x2_t __a)\n+{\n+  float64x2_t __b = {0.0, 0.0};\n+  return (uint64x2_t) __builtin_aarch64_cmltv2df (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcltzq_p8 (poly8x16_t __a)\n+{\n+  poly8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t    0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmltv16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vcltzq_s8 (int8x16_t __a)\n+{\n+  int8x16_t __b = {0, 0, 0, 0, 0, 0, 0, 0,\n+\t\t   0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint8x16_t) __builtin_aarch64_cmltv16qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vcltzq_s16 (int16x8_t __a)\n+{\n+  int16x8_t __b = {0, 0, 0, 0, 0, 0, 0, 0};\n+  return (uint16x8_t) __builtin_aarch64_cmltv8hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vcltzq_s32 (int32x4_t __a)\n+{\n+  int32x4_t __b = {0, 0, 0, 0};\n+  return (uint32x4_t) __builtin_aarch64_cmltv4si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vcltzq_s64 (int64x2_t __a)\n+{\n+  int64x2_t __b = {0, 0};\n+  return (uint64x2_t) __builtin_aarch64_cmltv2di (__a, __b);\n+}\n+\n+/* vcltz - scalar.  */\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vcltzs_f32 (float32_t __a)\n+{\n+  return __a < 0.0f ? -1 : 0;\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcltzd_s64 (int64x1_t __a)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmltdi (__a, 0);\n+  return __a < 0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vcltzd_u64 (int64x1_t __a)\n+{\n+  return __a < 0 ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcltzd_f64 (float64_t __a)\n+{\n+  return __a < 0.0 ? -1ll : 0ll;\n }\n \n /* vcvt (double -> float).  */\n@@ -24953,7 +25745,7 @@ vtst_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vtst_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmtstdi (__a, __b);\n+  return (__a & __b) ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -24980,8 +25772,7 @@ vtst_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vtst_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmtstdi ((int64x1_t) __a,\n-\t\t\t\t\t\t(int64x1_t) __b);\n+  return (__a & __b) ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n@@ -25039,14 +25830,13 @@ vtstq_u64 (uint64x2_t __a, uint64x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vtstd_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmtstdi (__a, __b);\n+  return (__a & __b) ? -1ll : 0ll;\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vtstd_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_cmtstdi ((int64x1_t) __a,\n-\t\t\t\t\t\t(int64x1_t) __b);\n+  return (__a & __b) ? -1ll : 0ll;\n }\n \n /* vuqadd */"}]}