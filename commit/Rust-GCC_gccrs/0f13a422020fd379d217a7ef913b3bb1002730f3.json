{"sha": "0f13a422020fd379d217a7ef913b3bb1002730f3", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MGYxM2E0MjIwMjBmZDM3OWQyMTdhN2VmOTEzYjNiYjEwMDI3MzBmMw==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "1996-10-29T00:01:53Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "1996-10-29T00:01:53Z"}, "message": "Add some optimizations for TRUNCATE and ZERO_EXTEND\n\nFrom-SVN: r13058", "tree": {"sha": "1ed7714bea75fe3fc71c5b94771ebeac2a84519f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1ed7714bea75fe3fc71c5b94771ebeac2a84519f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0f13a422020fd379d217a7ef913b3bb1002730f3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0f13a422020fd379d217a7ef913b3bb1002730f3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0f13a422020fd379d217a7ef913b3bb1002730f3", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0f13a422020fd379d217a7ef913b3bb1002730f3/comments", "author": null, "committer": null, "parents": [{"sha": "8cc2ddb666cfba01a80f6d91cb5921958008125c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8cc2ddb666cfba01a80f6d91cb5921958008125c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8cc2ddb666cfba01a80f6d91cb5921958008125c"}], "stats": {"total": 121, "additions": 121, "deletions": 0}, "files": [{"sha": "c9a14a70fac2a520d54612a0fa4fd08e6f2a7f66", "filename": "gcc/combine.c", "status": "modified", "additions": 121, "deletions": 0, "changes": 121, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f13a422020fd379d217a7ef913b3bb1002730f3/gcc%2Fcombine.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f13a422020fd379d217a7ef913b3bb1002730f3/gcc%2Fcombine.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcombine.c?ref=0f13a422020fd379d217a7ef913b3bb1002730f3", "patch": "@@ -3539,6 +3539,55 @@ simplify_rtx (x, op0_mode, last, in_dest)\n \tSUBST (XEXP (x, 0),\n \t       force_to_mode (XEXP (x, 0), GET_MODE (XEXP (x, 0)),\n \t\t\t      GET_MODE_MASK (mode), NULL_RTX, 0));\n+\n+      /* (truncate:SI ({sign,zero}_extend:DI foo:SI)) == foo:SI.  */\n+      if ((GET_CODE (XEXP (x, 0)) == SIGN_EXTEND\n+\t   || GET_CODE (XEXP (x, 0)) == ZERO_EXTEND)\n+\t  && GET_MODE (XEXP (XEXP (x, 0), 0)) == mode)\n+\treturn XEXP (XEXP (x, 0), 0);\n+\n+      /* (truncate:SI (OP:DI ({sign,zero}_extend:DI foo:SI))) is\n+\t (OP:SI foo:SI) if OP is NEG or ABS.  */\n+      if ((GET_CODE (XEXP (x, 0)) == ABS\n+\t   || GET_CODE (XEXP (x, 0)) == NEG)\n+\t  && (GET_CODE (XEXP (XEXP (x, 0), 0)) == SIGN_EXTEND\n+\t      || GET_CODE (XEXP (XEXP (x, 0), 0)) == ZERO_EXTEND)\n+\t  && GET_MODE (XEXP (XEXP (XEXP (x, 0), 0), 0)) == mode)\n+\treturn gen_unary (GET_CODE (XEXP (x, 0)), mode, mode,\n+\t\t\t  XEXP (XEXP (XEXP (x, 0), 0), 0));\n+\n+      /* (truncate:SI (subreg:DI (truncate:SI X) 0)) is\n+\t (truncate:SI x).  */\n+      if (GET_CODE (XEXP (x, 0)) == SUBREG\n+\t  && GET_CODE (SUBREG_REG (XEXP (x, 0))) == TRUNCATE\n+\t  && subreg_lowpart_p (XEXP (x, 0)))\n+\treturn SUBREG_REG (XEXP (x, 0));\n+\n+      /* If we know that the value is already truncated, we can\n+         replace the TRUNCATE with a SUBREG.  */\n+      if (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0))) <= HOST_BITS_PER_WIDE_INT\n+\t  && (nonzero_bits (XEXP (x, 0), GET_MODE (XEXP (x, 0)))\n+\t      &~ GET_MODE_MASK (mode)) == 0)\n+\treturn gen_lowpart_for_combine (mode, XEXP (x, 0));\n+\n+      /* A truncate of a comparison can be replaced with a subreg if\n+         STORE_FLAG_VALUE permits.  This is like the previous test,\n+         but it works even if the comparison is done in a mode larger\n+         than HOST_BITS_PER_WIDE_INT.  */\n+      if (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT\n+\t  && GET_RTX_CLASS (GET_CODE (XEXP (x, 0))) == '<'\n+\t  && ((HOST_WIDE_INT) STORE_FLAG_VALUE &~ GET_MODE_MASK (mode)) == 0)\n+\treturn gen_lowpart_for_combine (mode, XEXP (x, 0));\n+\n+      /* Similarly, a truncate of a register whose value is a\n+         comparison can be replaced with a subreg if STORE_FLAG_VALUE\n+         permits.  */\n+      if (GET_MODE_BITSIZE (mode) <= HOST_BITS_PER_WIDE_INT\n+\t  && ((HOST_WIDE_INT) STORE_FLAG_VALUE &~ GET_MODE_MASK (mode)) == 0\n+\t  && (temp = get_last_value (XEXP (x, 0)))\n+\t  && GET_RTX_CLASS (GET_CODE (temp)) == '<')\n+\treturn gen_lowpart_for_combine (mode, XEXP (x, 0));\n+\n       break;\n \n     case FLOAT_TRUNCATE:\n@@ -4915,6 +4964,78 @@ expand_compound_operation (x)\n       return x;\n     }\n \n+  /* We can optimize some special cases of ZERO_EXTEND.  */\n+  if (GET_CODE (x) == ZERO_EXTEND)\n+    {\n+      /* (zero_extend:DI (truncate:SI foo:DI)) is just foo:DI if we\n+         know that the last value didn't have any inappropriate bits\n+         set.  */\n+      if (GET_CODE (XEXP (x, 0)) == TRUNCATE\n+\t  && GET_MODE (XEXP (XEXP (x, 0), 0)) == GET_MODE (x)\n+\t  && GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT\n+\t  && (nonzero_bits (XEXP (XEXP (x, 0), 0), GET_MODE (x))\n+\t      & ~ GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)\n+\treturn XEXP (XEXP (x, 0), 0);\n+\n+      /* Likewise for (zero_extend:DI (subreg:SI foo:DI 0)).  */\n+      if (GET_CODE (XEXP (x, 0)) == SUBREG\n+\t  && GET_MODE (SUBREG_REG (XEXP (x, 0))) == GET_MODE (x)\n+\t  && subreg_lowpart_p (XEXP (x, 0))\n+\t  && GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT\n+\t  && (nonzero_bits (SUBREG_REG (XEXP (x, 0)), GET_MODE (x))\n+\t      & ~ GET_MODE_MASK (GET_MODE (SUBREG_REG (x)))) == 0)\n+\treturn SUBREG_REG (XEXP (x, 0));\n+\n+      /* (zero_extend:DI (truncate:SI foo:DI)) is just foo:DI when foo\n+         is a comparison and STORE_FLAG_VALUE permits.  This is like\n+         the first case, but it works even when GET_MODE (x) is larger\n+         than HOST_WIDE_INT.  */\n+      if (GET_CODE (XEXP (x, 0)) == TRUNCATE\n+\t  && GET_MODE (XEXP (XEXP (x, 0), 0)) == GET_MODE (x)\n+\t  && GET_RTX_CLASS (GET_CODE (XEXP (XEXP (x, 0), 0))) == '<'\n+\t  && (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))\n+\t      <= HOST_BITS_PER_WIDE_INT)\n+ \t  && ((HOST_WIDE_INT) STORE_FLAG_VALUE\n+\t      & ~ GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)\n+\treturn XEXP (XEXP (x, 0), 0);\n+\n+      /* Likewise for (zero_extend:DI (subreg:SI foo:DI 0)).  */\n+      if (GET_CODE (XEXP (x, 0)) == SUBREG\n+\t  && GET_MODE (SUBREG_REG (XEXP (x, 0))) == GET_MODE (x)\n+\t  && subreg_lowpart_p (XEXP (x, 0))\n+\t  && GET_RTX_CLASS (GET_CODE (SUBREG_REG (XEXP (x, 0)))) == '<'\n+\t  && (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))\n+\t      <= HOST_BITS_PER_WIDE_INT)\n+\t  && ((HOST_WIDE_INT) STORE_FLAG_VALUE\n+\t      & ~ GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)\n+\treturn SUBREG_REG (XEXP (x, 0));\n+\n+      /* If sign extension is cheaper than zero extension, then use it\n+\t if we know that no extraneous bits are set, and that the high\n+\t bit is not set.  */\n+      if (flag_expensive_optimizations\n+\t  && ((GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT\n+\t       && ((nonzero_bits (XEXP (x, 0), GET_MODE (x))\n+\t\t    & ~ (((unsigned HOST_WIDE_INT)\n+\t\t\t  GET_MODE_MASK (GET_MODE (XEXP (x, 0))))\n+\t\t\t >> 1))\n+\t\t   == 0))\n+\t      || (GET_RTX_CLASS (GET_CODE (XEXP (x, 0))) == '<'\n+\t\t  && (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))\n+\t\t      <= HOST_BITS_PER_WIDE_INT)\n+\t\t  && (((HOST_WIDE_INT) STORE_FLAG_VALUE\n+\t\t       & ~ (((unsigned HOST_WIDE_INT)\n+\t\t\t     GET_MODE_MASK (GET_MODE (XEXP (x, 0))))\n+\t\t\t    >> 1))\n+\t\t      == 0))))\n+\t{\n+\t  rtx temp = gen_rtx (SIGN_EXTEND, GET_MODE (x), XEXP (x, 0));\n+\n+\t  if (rtx_cost (temp, SET) < rtx_cost (x, SET))\n+\t    return expand_compound_operation (temp);\n+\t}\n+    }\n+\n   /* If we reach here, we want to return a pair of shifts.  The inner\n      shift is a left shift of BITSIZE - POS - LEN bits.  The outer\n      shift is a right shift of BITSIZE - LEN bits.  It is arithmetic or"}]}