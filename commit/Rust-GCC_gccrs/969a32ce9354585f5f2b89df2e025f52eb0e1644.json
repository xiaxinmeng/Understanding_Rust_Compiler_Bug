{"sha": "969a32ce9354585f5f2b89df2e025f52eb0e1644", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OTY5YTMyY2U5MzU0NTg1ZjVmMmI4OWRmMmUwMjVmNTJlYjBlMTY0NA==", "commit": {"author": {"name": "Torvald Riegel", "email": "torvald@gcc.gnu.org", "date": "2017-02-01T17:21:59Z"}, "committer": {"name": "Torvald Riegel", "email": "torvald@gcc.gnu.org", "date": "2017-02-01T17:21:59Z"}, "message": "Fix __atomic to not implement atomic loads with CAS.\n\ngcc/\n\t* builtins.c (fold_builtin_atomic_always_lock_free): Make \"lock-free\"\n\tconditional on existance of a fast atomic load.\n\t* optabs-query.c (can_atomic_load_p): New function.\n\t* optabs-query.h (can_atomic_load_p): Declare it.\n\t* optabs.c (expand_atomic_exchange): Always delegate to libatomic if\n\tno fast atomic load is available for the particular size of access.\n\t(expand_atomic_compare_and_swap): Likewise.\n\t(expand_atomic_load): Likewise.\n\t(expand_atomic_store): Likewise.\n\t(expand_atomic_fetch_op): Likewise.\n\t* testsuite/lib/target-supports.exp\n\t(check_effective_target_sync_int_128): Remove x86 because it provides\n\tno fast atomic load.\n\t(check_effective_target_sync_int_128_runtime): Likewise.\n\nlibatomic/\n\t* acinclude.m4: Add #define FAST_ATOMIC_LDST_*.\n\t* auto-config.h.in: Regenerate.\n\t* config/x86/host-config.h (FAST_ATOMIC_LDST_16): Define to 0.\n\t(atomic_compare_exchange_n): New.\n\t* glfree.c (EXACT, LARGER): Change condition and add comments.\n\nFrom-SVN: r245098", "tree": {"sha": "ba5dc4787f7d4f9d23224810508207f4fcc188dc", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ba5dc4787f7d4f9d23224810508207f4fcc188dc"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/969a32ce9354585f5f2b89df2e025f52eb0e1644", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/969a32ce9354585f5f2b89df2e025f52eb0e1644", "html_url": "https://github.com/Rust-GCC/gccrs/commit/969a32ce9354585f5f2b89df2e025f52eb0e1644", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/969a32ce9354585f5f2b89df2e025f52eb0e1644/comments", "author": null, "committer": null, "parents": [{"sha": "55e75c7c6bcfe386d0ecbf4611cff81040af00b3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/55e75c7c6bcfe386d0ecbf4611cff81040af00b3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/55e75c7c6bcfe386d0ecbf4611cff81040af00b3"}], "stats": {"total": 206, "additions": 152, "deletions": 54}, "files": [{"sha": "594cc3bf1666cfd2f6886e5f03c14364c92838a0", "filename": "gcc/ChangeLog", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -1,3 +1,21 @@\n+2017-02-01  Torvald Riegel  <triegel@redhat.com>\n+            Richard Henderson  <rth@redhat.com>\n+\n+\t* builtins.c (fold_builtin_atomic_always_lock_free): Make \"lock-free\"\n+\tconditional on existance of a fast atomic load.\n+\t* optabs-query.c (can_atomic_load_p): New function.\n+\t* optabs-query.h (can_atomic_load_p): Declare it.\n+\t* optabs.c (expand_atomic_exchange): Always delegate to libatomic if\n+\tno fast atomic load is available for the particular size of access.\n+\t(expand_atomic_compare_and_swap): Likewise.\n+\t(expand_atomic_load): Likewise.\n+\t(expand_atomic_store): Likewise.\n+\t(expand_atomic_fetch_op): Likewise.\n+\t* testsuite/lib/target-supports.exp\n+\t(check_effective_target_sync_int_128): Remove x86 because it provides\n+\tno fast atomic load.\n+\t(check_effective_target_sync_int_128_runtime): Likewise.\n+\n 2017-02-01  Richard Biener  <rguenther@suse.de>\n \n \t* graphite.c: Include tree-vectorizer.h for find_loop_location."}, {"sha": "0a0e8b9e2fa7bcfe2c4bc4e4d46c02228bf418d1", "filename": "gcc/builtins.c", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2Fbuiltins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2Fbuiltins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fbuiltins.c?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -6157,8 +6157,9 @@ fold_builtin_atomic_always_lock_free (tree arg0, tree arg1)\n \n   /* Check if a compare_and_swap pattern exists for the mode which represents\n      the required size.  The pattern is not allowed to fail, so the existence\n-     of the pattern indicates support is present.  */\n-  if (can_compare_and_swap_p (mode, true))\n+     of the pattern indicates support is present.  Also require that an\n+     atomic load exists for the required size.  */\n+  if (can_compare_and_swap_p (mode, true) && can_atomic_load_p (mode))\n     return boolean_true_node;\n   else\n     return boolean_false_node;"}, {"sha": "4899333096e5d327bcd98fa70ddfb7b2dd386801", "filename": "gcc/optabs-query.c", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2Foptabs-query.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2Foptabs-query.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foptabs-query.c?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -584,6 +584,25 @@ can_atomic_exchange_p (machine_mode mode, bool allow_libcall)\n   return can_compare_and_swap_p (mode, allow_libcall);\n }\n \n+/* Return true if an atomic load can be performed without falling back to\n+   a compare-and-swap.  */\n+\n+bool\n+can_atomic_load_p (machine_mode mode)\n+{\n+  enum insn_code icode;\n+\n+  /* Does the target supports the load directly?  */\n+  icode = direct_optab_handler (atomic_load_optab, mode);\n+  if (icode != CODE_FOR_nothing)\n+    return true;\n+\n+  /* If the size of the object is greater than word size on this target,\n+     then we assume that a load will not be atomic.  Also see\n+     expand_atomic_load.  */\n+  return GET_MODE_PRECISION (mode) <= BITS_PER_WORD;\n+}\n+\n /* Determine whether \"1 << x\" is relatively cheap in word_mode.  */\n \n bool"}, {"sha": "e85a7f11b95905295cc35107ad9538934e22de76", "filename": "gcc/optabs-query.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2Foptabs-query.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2Foptabs-query.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foptabs-query.h?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -176,6 +176,7 @@ int can_mult_highpart_p (machine_mode, bool);\n bool can_vec_mask_load_store_p (machine_mode, machine_mode, bool);\n bool can_compare_and_swap_p (machine_mode, bool);\n bool can_atomic_exchange_p (machine_mode, bool);\n+bool can_atomic_load_p (machine_mode);\n bool lshift_cheap_p (bool);\n \n #endif"}, {"sha": "1afd593ae1520475b13c789ff6df2602b41be024", "filename": "gcc/optabs.c", "status": "modified", "additions": 42, "deletions": 21, "changes": 63, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2Foptabs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2Foptabs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foptabs.c?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -6086,8 +6086,15 @@ expand_atomic_test_and_set (rtx target, rtx mem, enum memmodel model)\n rtx\n expand_atomic_exchange (rtx target, rtx mem, rtx val, enum memmodel model)\n {\n+  machine_mode mode = GET_MODE (mem);\n   rtx ret;\n \n+  /* If loads are not atomic for the required size and we are not called to\n+     provide a __sync builtin, do not do anything so that we stay consistent\n+     with atomic loads of the same size.  */\n+  if (!can_atomic_load_p (mode) && !is_mm_sync (model))\n+    return NULL_RTX;\n+\n   ret = maybe_emit_atomic_exchange (target, mem, val, model);\n \n   /* Next try a compare-and-swap loop for the exchange.  */\n@@ -6121,6 +6128,12 @@ expand_atomic_compare_and_swap (rtx *ptarget_bool, rtx *ptarget_oval,\n   rtx target_oval, target_bool = NULL_RTX;\n   rtx libfunc;\n \n+  /* If loads are not atomic for the required size and we are not called to\n+     provide a __sync builtin, do not do anything so that we stay consistent\n+     with atomic loads of the same size.  */\n+  if (!can_atomic_load_p (mode) && !is_mm_sync (succ_model))\n+    return false;\n+\n   /* Load expected into a register for the compare and swap.  */\n   if (MEM_P (expected))\n     expected = copy_to_reg (expected);\n@@ -6316,19 +6329,13 @@ expand_atomic_load (rtx target, rtx mem, enum memmodel model)\n     }\n \n   /* If the size of the object is greater than word size on this target,\n-     then we assume that a load will not be atomic.  */\n+     then we assume that a load will not be atomic.  We could try to\n+     emulate a load with a compare-and-swap operation, but the store that\n+     doing this could result in would be incorrect if this is a volatile\n+     atomic load or targetting read-only-mapped memory.  */\n   if (GET_MODE_PRECISION (mode) > BITS_PER_WORD)\n-    {\n-      /* Issue val = compare_and_swap (mem, 0, 0).\n-\t This may cause the occasional harmless store of 0 when the value is\n-\t already 0, but it seems to be OK according to the standards guys.  */\n-      if (expand_atomic_compare_and_swap (NULL, &target, mem, const0_rtx,\n-\t\t\t\t\t  const0_rtx, false, model, model))\n-\treturn target;\n-      else\n-      /* Otherwise there is no atomic load, leave the library call.  */\n-        return NULL_RTX;\n-    }\n+    /* If there is no atomic load, leave the library call.  */\n+    return NULL_RTX;\n \n   /* Otherwise assume loads are atomic, and emit the proper barriers.  */\n   if (!target || target == const0_rtx)\n@@ -6370,7 +6377,9 @@ expand_atomic_store (rtx mem, rtx val, enum memmodel model, bool use_release)\n \treturn const0_rtx;\n     }\n \n-  /* If using __sync_lock_release is a viable alternative, try it.  */\n+  /* If using __sync_lock_release is a viable alternative, try it.\n+     Note that this will not be set to true if we are expanding a generic\n+     __atomic_store_n.  */\n   if (use_release)\n     {\n       icode = direct_optab_handler (sync_lock_release_optab, mode);\n@@ -6389,16 +6398,22 @@ expand_atomic_store (rtx mem, rtx val, enum memmodel model, bool use_release)\n     }\n \n   /* If the size of the object is greater than word size on this target,\n-     a default store will not be atomic, Try a mem_exchange and throw away\n-     the result.  If that doesn't work, don't do anything.  */\n+     a default store will not be atomic.  */\n   if (GET_MODE_PRECISION (mode) > BITS_PER_WORD)\n     {\n-      rtx target = maybe_emit_atomic_exchange (NULL_RTX, mem, val, model);\n-      if (!target)\n-        target = maybe_emit_compare_and_swap_exchange_loop (NULL_RTX, mem, val);\n-      if (target)\n-        return const0_rtx;\n-      else\n+      /* If loads are atomic or we are called to provide a __sync builtin,\n+\t we can try a atomic_exchange and throw away the result.  Otherwise,\n+\t don't do anything so that we do not create an inconsistency between\n+\t loads and stores.  */\n+      if (can_atomic_load_p (mode) || is_mm_sync (model))\n+\t{\n+\t  rtx target = maybe_emit_atomic_exchange (NULL_RTX, mem, val, model);\n+\t  if (!target)\n+\t    target = maybe_emit_compare_and_swap_exchange_loop (NULL_RTX, mem,\n+\t\t\t\t\t\t\t\tval);\n+\t  if (target)\n+\t    return const0_rtx;\n+\t}\n         return NULL_RTX;\n     }\n \n@@ -6713,6 +6728,12 @@ expand_atomic_fetch_op (rtx target, rtx mem, rtx val, enum rtx_code code,\n   rtx result;\n   bool unused_result = (target == const0_rtx);\n \n+  /* If loads are not atomic for the required size and we are not called to\n+     provide a __sync builtin, do not do anything so that we stay consistent\n+     with atomic loads of the same size.  */\n+  if (!can_atomic_load_p (mode) && !is_mm_sync (model))\n+    return NULL_RTX;\n+\n   result = expand_atomic_fetch_op_no_fallback (target, mem, val, code, model,\n \t\t\t\t\t       after);\n   "}, {"sha": "7a260085405c850ef4042faffc4ad71fdb25c830", "filename": "gcc/testsuite/lib/target-supports.exp", "status": "modified", "additions": 3, "deletions": 18, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2Ftestsuite%2Flib%2Ftarget-supports.exp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/gcc%2Ftestsuite%2Flib%2Ftarget-supports.exp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Flib%2Ftarget-supports.exp?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -6514,9 +6514,7 @@ proc check_effective_target_section_anchors { } {\n # Return 1 if the target supports atomic operations on \"int_128\" values.\n \n proc check_effective_target_sync_int_128 { } {\n-    if { (([istarget i?86-*-*] || [istarget x86_64-*-*])\n-\t  && ![is-effective-target ia32])\n-\t || [istarget spu-*-*] } {\n+    if { [istarget spu-*-*] } {\n \treturn 1\n     } else {\n \treturn 0\n@@ -6525,23 +6523,10 @@ proc check_effective_target_sync_int_128 { } {\n \n # Return 1 if the target supports atomic operations on \"int_128\" values\n # and can execute them.\n+# This requires support for both compare-and-swap and true atomic loads.\n \n proc check_effective_target_sync_int_128_runtime { } {\n-    if { (([istarget i?86-*-*] || [istarget x86_64-*-*])\n-\t  && ![is-effective-target ia32]\n-\t  && [check_cached_effective_target sync_int_128_available {\n-\t      check_runtime_nocache sync_int_128_available {\n-\t\t  #include \"cpuid.h\"\n-\t\t  int main ()\n-\t\t  {\n-\t\t      unsigned int eax, ebx, ecx, edx;\n-\t\t      if (__get_cpuid (1, &eax, &ebx, &ecx, &edx))\n-\t\t\treturn !(ecx & bit_CMPXCHG16B);\n-\t\t      return 1;\n-\t\t  }\n-\t      } \"\"\n-\t  }])\n-\t || [istarget spu-*-*] } {\n+    if { [istarget spu-*-*] } {\n \treturn 1\n     } else {\n \treturn 0"}, {"sha": "d2b83369b74c36459140f1cea5e44e4ed408a98d", "filename": "libatomic/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/libatomic%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/libatomic%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libatomic%2FChangeLog?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -1,3 +1,12 @@\n+2017-02-01  Richard Henderson  <rth@redhat.com>\n+            Torvald Riegel  <triegel@redhat.com>\n+\n+\t* acinclude.m4: Add #define FAST_ATOMIC_LDST_*.\n+\t* auto-config.h.in: Regenerate.\n+\t* config/x86/host-config.h (FAST_ATOMIC_LDST_16): Define to 0.\n+\t(atomic_compare_exchange_n): New.\n+\t* glfree.c (EXACT, LARGER): Change condition and add comments.\n+\n 2017-01-30  Szabolcs Nagy  <szabolcs.nagy@arm.com>\n \n \tPR target/78945"}, {"sha": "485d731df55177afbe5c05c21fc7ac6661b5e0cc", "filename": "libatomic/acinclude.m4", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/libatomic%2Facinclude.m4", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/libatomic%2Facinclude.m4", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libatomic%2Facinclude.m4?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -96,6 +96,7 @@ AC_DEFUN([LIBAT_HAVE_ATOMIC_LOADSTORE],[\n   LIBAT_DEFINE_YESNO([HAVE_ATOMIC_LDST_$2], [$libat_cv_have_at_ldst_$2],\n \t[Have __atomic_load/store for $2 byte integers.])\n   AH_BOTTOM([#define MAYBE_HAVE_ATOMIC_LDST_$2 HAVE_ATOMIC_LDST_$2])\n+  AH_BOTTOM([#define FAST_ATOMIC_LDST_$2 HAVE_ATOMIC_LDST_$2])\n ])\n \n dnl"}, {"sha": "d5b8a26e33e172cf18177bad5e2b8355ad0a764e", "filename": "libatomic/auto-config.h.in", "status": "modified", "additions": 20, "deletions": 10, "changes": 30, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/libatomic%2Fauto-config.h.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/libatomic%2Fauto-config.h.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libatomic%2Fauto-config.h.in?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -222,6 +222,16 @@\n \n #define MAYBE_HAVE_ATOMIC_LDST_1 HAVE_ATOMIC_LDST_1\n \n+#define FAST_ATOMIC_LDST_16 HAVE_ATOMIC_LDST_16\n+\n+#define MAYBE_HAVE_ATOMIC_TAS_1 HAVE_ATOMIC_TAS_1\n+\n+#define MAYBE_HAVE_ATOMIC_TAS_2 HAVE_ATOMIC_TAS_2\n+\n+#define MAYBE_HAVE_ATOMIC_TAS_4 HAVE_ATOMIC_TAS_4\n+\n+#define MAYBE_HAVE_ATOMIC_TAS_8 HAVE_ATOMIC_TAS_8\n+\n #define MAYBE_HAVE_ATOMIC_TAS_16 HAVE_ATOMIC_TAS_16\n \n #define MAYBE_HAVE_ATOMIC_EXCHANGE_1 HAVE_ATOMIC_EXCHANGE_1\n@@ -232,6 +242,8 @@\n \n #define MAYBE_HAVE_ATOMIC_EXCHANGE_8 HAVE_ATOMIC_EXCHANGE_8\n \n+#define FAST_ATOMIC_LDST_1 HAVE_ATOMIC_LDST_1\n+\n #define MAYBE_HAVE_ATOMIC_EXCHANGE_16 HAVE_ATOMIC_EXCHANGE_16\n \n #define MAYBE_HAVE_ATOMIC_CAS_1 HAVE_ATOMIC_CAS_1\n@@ -242,8 +254,6 @@\n \n #define MAYBE_HAVE_ATOMIC_CAS_8 HAVE_ATOMIC_CAS_8\n \n-#define MAYBE_HAVE_ATOMIC_LDST_2 HAVE_ATOMIC_LDST_2\n-\n #define MAYBE_HAVE_ATOMIC_CAS_16 HAVE_ATOMIC_CAS_16\n \n #define MAYBE_HAVE_ATOMIC_FETCH_ADD_1 HAVE_ATOMIC_FETCH_ADD_1\n@@ -254,6 +264,8 @@\n \n #define MAYBE_HAVE_ATOMIC_FETCH_ADD_8 HAVE_ATOMIC_FETCH_ADD_8\n \n+#define MAYBE_HAVE_ATOMIC_LDST_2 HAVE_ATOMIC_LDST_2\n+\n #define MAYBE_HAVE_ATOMIC_FETCH_ADD_16 HAVE_ATOMIC_FETCH_ADD_16\n \n #define MAYBE_HAVE_ATOMIC_FETCH_OP_1 HAVE_ATOMIC_FETCH_OP_1\n@@ -264,22 +276,20 @@\n \n #define MAYBE_HAVE_ATOMIC_FETCH_OP_8 HAVE_ATOMIC_FETCH_OP_8\n \n-#define MAYBE_HAVE_ATOMIC_LDST_4 HAVE_ATOMIC_LDST_4\n-\n #define MAYBE_HAVE_ATOMIC_FETCH_OP_16 HAVE_ATOMIC_FETCH_OP_16\n \n #ifndef WORDS_BIGENDIAN\n #define WORDS_BIGENDIAN 0\n #endif\n \n-#define MAYBE_HAVE_ATOMIC_LDST_8 HAVE_ATOMIC_LDST_8\n+#define FAST_ATOMIC_LDST_2 HAVE_ATOMIC_LDST_2\n \n-#define MAYBE_HAVE_ATOMIC_LDST_16 HAVE_ATOMIC_LDST_16\n+#define MAYBE_HAVE_ATOMIC_LDST_4 HAVE_ATOMIC_LDST_4\n \n-#define MAYBE_HAVE_ATOMIC_TAS_1 HAVE_ATOMIC_TAS_1\n+#define FAST_ATOMIC_LDST_4 HAVE_ATOMIC_LDST_4\n \n-#define MAYBE_HAVE_ATOMIC_TAS_2 HAVE_ATOMIC_TAS_2\n+#define MAYBE_HAVE_ATOMIC_LDST_8 HAVE_ATOMIC_LDST_8\n \n-#define MAYBE_HAVE_ATOMIC_TAS_4 HAVE_ATOMIC_TAS_4\n+#define FAST_ATOMIC_LDST_8 HAVE_ATOMIC_LDST_8\n \n-#define MAYBE_HAVE_ATOMIC_TAS_8 HAVE_ATOMIC_TAS_8\n+#define MAYBE_HAVE_ATOMIC_LDST_16 HAVE_ATOMIC_LDST_16"}, {"sha": "2e9f85aee5f9fe69a298a7efa17389b5120226a2", "filename": "libatomic/config/x86/host-config.h", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/libatomic%2Fconfig%2Fx86%2Fhost-config.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/libatomic%2Fconfig%2Fx86%2Fhost-config.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libatomic%2Fconfig%2Fx86%2Fhost-config.h?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -47,6 +47,9 @@ extern unsigned int libat_feat1_edx HIDDEN;\n # define MAYBE_HAVE_ATOMIC_EXCHANGE_16\tIFUNC_COND_1\n # undef MAYBE_HAVE_ATOMIC_LDST_16\n # define MAYBE_HAVE_ATOMIC_LDST_16\tIFUNC_COND_1\n+/* Since load and store are implemented with CAS, they are not fast.  */\n+# undef FAST_ATOMIC_LDST_16\n+# define FAST_ATOMIC_LDST_16\t\t0\n # if IFUNC_ALT == 1\n #  undef HAVE_ATOMIC_CAS_16\n #  define HAVE_ATOMIC_CAS_16 1\n@@ -64,6 +67,21 @@ extern unsigned int libat_feat1_edx HIDDEN;\n # endif\n #endif\n \n+#if defined(__x86_64__) && N == 16 && IFUNC_ALT == 1\n+static inline bool\n+atomic_compare_exchange_n (UTYPE *mptr, UTYPE *eptr, UTYPE newval,\n+                           bool weak_p UNUSED, int sm UNUSED, int fm UNUSED)\n+{\n+  UTYPE cmpval = *eptr;\n+  UTYPE oldval = __sync_val_compare_and_swap_16 (mptr, cmpval, newval);\n+  if (oldval == cmpval)\n+    return true;\n+  *eptr = oldval;\n+  return false;\n+}\n+# define atomic_compare_exchange_n atomic_compare_exchange_n\n+#endif /* Have CAS 16 */\n+\n #endif /* HAVE_IFUNC */\n \n #include_next <host-config.h>"}, {"sha": "59fe533bc308444bf02c7468d2eeaba5ccd94bd1", "filename": "libatomic/glfree.c", "status": "modified", "additions": 18, "deletions": 3, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/969a32ce9354585f5f2b89df2e025f52eb0e1644/libatomic%2Fglfree.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/969a32ce9354585f5f2b89df2e025f52eb0e1644/libatomic%2Fglfree.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libatomic%2Fglfree.c?ref=969a32ce9354585f5f2b89df2e025f52eb0e1644", "patch": "@@ -24,26 +24,41 @@\n \n #include \"libatomic_i.h\"\n \n-\n+/* Accesses with a power-of-two size are not lock-free if we don't have an\n+   integer type of this size or if they are not naturally aligned.  They\n+   are lock-free if such a naturally aligned access is always lock-free\n+   according to the compiler, which requires that both atomic loads and CAS\n+   are available.\n+   In all other cases, we fall through to LARGER (see below).  */\n #define EXACT(N)\t\t\t\t\t\t\\\n   do {\t\t\t\t\t\t\t\t\\\n     if (!C2(HAVE_INT,N)) break;\t\t\t\t\t\\\n     if ((uintptr_t)ptr & (N - 1)) break;\t\t\t\\\n     if (__atomic_always_lock_free(N, 0)) return true;\t\t\\\n-    if (C2(MAYBE_HAVE_ATOMIC_CAS_,N)) return true;\t\t\\\n+    if (!C2(MAYBE_HAVE_ATOMIC_CAS_,N)) break;\t\t\t\\\n+    if (C2(FAST_ATOMIC_LDST_,N)) return true;\t\t\t\\\n   } while (0)\n \n \n+/* We next check to see if an access of a larger size is lock-free.  We use\n+   a similar check as in EXACT, except that we also check that the alignment\n+   of the access is so that the data to be accessed is completely covered\n+   by the larger access.  */\n #define LARGER(N)\t\t\t\t\t\t\\\n   do {\t\t\t\t\t\t\t\t\\\n     uintptr_t r = (uintptr_t)ptr & (N - 1);\t\t\t\\\n     if (!C2(HAVE_INT,N)) break;\t\t\t\t\t\\\n-    if (!C2(HAVE_ATOMIC_LDST_,N)) break;\t\t\t\\\n+    if (!C2(FAST_ATOMIC_LDST_,N)) break;\t\t\t\\\n     if (!C2(MAYBE_HAVE_ATOMIC_CAS_,N)) break;\t\t\t\\\n     if (r + n <= N) return true;\t\t\t\t\\\n   } while (0)\n \n \n+/* Note that this can return that a size/alignment is not lock-free even if\n+   all the operations that we use to implement the respective accesses provide\n+   lock-free forward progress as specified in C++14:  Users likely expect\n+   \"lock-free\" to also mean \"fast\", which is why we do not return true if, for\n+   example, we implement loads with this size/alignment using a CAS.  */\n bool\n libat_is_lock_free (size_t n, void *ptr)\n {"}]}