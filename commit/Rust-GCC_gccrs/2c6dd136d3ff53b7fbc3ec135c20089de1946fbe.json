{"sha": "2c6dd136d3ff53b7fbc3ec135c20089de1946fbe", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MmM2ZGQxMzZkM2ZmNTNiN2ZiYzNlYzEzNWMyMDA4OWRlMTk0NmZiZQ==", "commit": {"author": {"name": "Ghassan Shobaki", "email": "ghassan.shobaki@amd.com", "date": "2009-08-13T21:37:24Z"}, "committer": {"name": "Ghassan Shobaki", "email": "gshobaki@gcc.gnu.org", "date": "2009-08-13T21:37:24Z"}, "message": "2009-08-13  Ghassan Shobaki  <ghassan.shobaki@amd.com>\n\n\t* tree-ssa-loop-prefetch.c \n\t(prune_ref_by_group_reuse): Enhance probabilistic analysis \n\tfor long-stride pruning.\n\t(compute_miss_rate): New function to compute the probability\n\tthat two memory references access different cache lines.\n\nFrom-SVN: r150726", "tree": {"sha": "1ef7a9372f20be41988ce118b5ab5b5840262f37", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1ef7a9372f20be41988ce118b5ab5b5840262f37"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/2c6dd136d3ff53b7fbc3ec135c20089de1946fbe", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2c6dd136d3ff53b7fbc3ec135c20089de1946fbe", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2c6dd136d3ff53b7fbc3ec135c20089de1946fbe", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2c6dd136d3ff53b7fbc3ec135c20089de1946fbe/comments", "author": null, "committer": null, "parents": [{"sha": "bc21d3152f7644fcbd2acf98adbba270c0408c91", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bc21d3152f7644fcbd2acf98adbba270c0408c91", "html_url": "https://github.com/Rust-GCC/gccrs/commit/bc21d3152f7644fcbd2acf98adbba270c0408c91"}], "stats": {"total": 87, "additions": 72, "deletions": 15}, "files": [{"sha": "70905a8fe1ba38f9f53540a4bd562d4e6867ef8d", "filename": "gcc/ChangeLog", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2c6dd136d3ff53b7fbc3ec135c20089de1946fbe/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2c6dd136d3ff53b7fbc3ec135c20089de1946fbe/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=2c6dd136d3ff53b7fbc3ec135c20089de1946fbe", "patch": "@@ -1,3 +1,11 @@\n+2009-08-13  Ghassan Shobaki  <ghassan.shobaki@amd.com>\n+\n+\t* tree-ssa-loop-prefetch.c \n+\t(prune_ref_by_group_reuse): Enhance probabilistic analysis \n+\tfor long-stride pruning.\n+\t(compute_miss_rate): New function to compute the probability\n+\tthat two memory references access different cache lines. \n+\n 2009-08-13  Dave Korn  <dave.korn.cygwin@gmail.com>\n \n \t* gcc/config/i386/cygwin.h (LINK_SPEC): Add --enable-auto-image-base."}, {"sha": "60f5a2f9b0dd9c1643c8ee3cf2966ecbdd60a713", "filename": "gcc/tree-ssa-loop-prefetch.c", "status": "modified", "additions": 64, "deletions": 15, "changes": 79, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2c6dd136d3ff53b7fbc3ec135c20089de1946fbe/gcc%2Ftree-ssa-loop-prefetch.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2c6dd136d3ff53b7fbc3ec135c20089de1946fbe/gcc%2Ftree-ssa-loop-prefetch.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-loop-prefetch.c?ref=2c6dd136d3ff53b7fbc3ec135c20089de1946fbe", "patch": "@@ -593,6 +593,45 @@ ddown (HOST_WIDE_INT x, unsigned HOST_WIDE_INT by)\n     return (x + by - 1) / by;\n }\n \n+/* Given a CACHE_LINE_SIZE and two inductive memory references \n+   with a common STEP greater than CACHE_LINE_SIZE and an address \n+   difference DELTA, compute the probability that they will fall \n+   in different cache lines.  DISTINCT_ITERS is the number of \n+   distinct iterations after which the pattern repeats itself.  \n+   ALIGN_UNIT is the unit of alignment in bytes.  */\n+\n+static int\n+compute_miss_rate (unsigned HOST_WIDE_INT cache_line_size, \n+\t\t   HOST_WIDE_INT step, HOST_WIDE_INT delta,\n+\t\t   unsigned HOST_WIDE_INT distinct_iters,\n+\t\t   int align_unit)\n+{\n+  unsigned align, iter;\n+  int total_positions, miss_positions, miss_rate;\n+  int address1, address2, cache_line1, cache_line2;\n+\n+  total_positions = 0;\n+  miss_positions = 0;\n+  \n+  /* Iterate through all possible alignments of the first\n+     memory reference within its cache line.  */\n+  for (align = 0; align < cache_line_size; align += align_unit)\n+\n+    /* Iterate through all distinct iterations.  */\n+    for (iter = 0; iter < distinct_iters; iter++)\n+      {\n+\taddress1 = align + step * iter;\n+\taddress2 = address1 + delta;\n+\tcache_line1 = address1 / cache_line_size;\n+\tcache_line2 = address2 / cache_line_size;\n+\ttotal_positions += 1;\n+\tif (cache_line1 != cache_line2)\n+\t  miss_positions += 1;\n+      }\n+  miss_rate = 1000 * miss_positions / total_positions;\n+  return miss_rate;\n+}\n+\n /* Prune the prefetch candidate REF using the reuse with BY.\n    If BY_IS_BEFORE is true, BY is before REF in the loop.  */\n \n@@ -606,6 +645,11 @@ prune_ref_by_group_reuse (struct mem_ref *ref, struct mem_ref *by,\n   HOST_WIDE_INT delta = delta_b - delta_r;\n   HOST_WIDE_INT hit_from;\n   unsigned HOST_WIDE_INT prefetch_before, prefetch_block;\n+  int miss_rate;\n+  HOST_WIDE_INT reduced_step;\n+  unsigned HOST_WIDE_INT reduced_prefetch_block;\n+  tree ref_type;\n+  int align_unit;\n \n   if (delta == 0)\n     {\n@@ -667,25 +711,29 @@ prune_ref_by_group_reuse (struct mem_ref *ref, struct mem_ref *by,\n       return;\n     }\n \n-  /* A more complicated case.  First let us ensure that size of cache line\n-     and step are coprime (here we assume that PREFETCH_BLOCK is a power\n-     of two.  */\n+  /* A more complicated case with step > prefetch_block.  First reduce \n+     the ratio between the step and the cache line size to its simplest\n+     terms.  The resulting denominator will then represent the number of \n+     distinct iterations after which each address will go back to its \n+     initial location within the cache line.  This computation assumes \n+     that PREFETCH_BLOCK is a power of two.  */\n   prefetch_block = PREFETCH_BLOCK;\n-  while ((step & 1) == 0\n-\t && prefetch_block > 1)\n+  reduced_prefetch_block = prefetch_block;\n+  reduced_step = step;\n+  while ((reduced_step & 1) == 0\n+\t && reduced_prefetch_block > 1)\n     {\n-      step >>= 1;\n-      prefetch_block >>= 1;\n-      delta >>= 1;\n+      reduced_step >>= 1;\n+      reduced_prefetch_block >>= 1;\n     }\n \n-  /* Now step > prefetch_block, and step and prefetch_block are coprime.\n-     Determine the probability that the accesses hit the same cache line.  */\n-\n   prefetch_before = delta / step;\n   delta %= step;\n-  if ((unsigned HOST_WIDE_INT) delta\n-      <= (prefetch_block * ACCEPTABLE_MISS_RATE / 1000))\n+  ref_type = TREE_TYPE (ref->mem);\n+  align_unit = TYPE_ALIGN (ref_type) / 8;\n+  miss_rate = compute_miss_rate(prefetch_block, step, delta, \n+\t\t\t\treduced_prefetch_block, align_unit);\n+  if (miss_rate <= ACCEPTABLE_MISS_RATE)\n     {\n       if (prefetch_before < ref->prefetch_before)\n \tref->prefetch_before = prefetch_before;\n@@ -696,8 +744,9 @@ prune_ref_by_group_reuse (struct mem_ref *ref, struct mem_ref *by,\n   /* Try also the following iteration.  */\n   prefetch_before++;\n   delta = step - delta;\n-  if ((unsigned HOST_WIDE_INT) delta\n-      <= (prefetch_block * ACCEPTABLE_MISS_RATE / 1000))\n+  miss_rate = compute_miss_rate(prefetch_block, step, delta, \n+\t\t\t\treduced_prefetch_block, align_unit);\n+  if (miss_rate <= ACCEPTABLE_MISS_RATE) \n     {\n       if (prefetch_before < ref->prefetch_before)\n \tref->prefetch_before = prefetch_before;"}]}