{"sha": "434641a57b90584bffa4e8def3f900c7d102bfdf", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NDM0NjQxYTU3YjkwNTg0YmZmYTRlOGRlZjNmOTAwYzdkMTAyYmZkZg==", "commit": {"author": {"name": "Christophe Lyon", "email": "christophe.lyon@linaro.org", "date": "2012-09-04T08:32:39Z"}, "committer": {"name": "Christophe Lyon", "email": "clyon@gcc.gnu.org", "date": "2012-09-04T08:32:39Z"}, "message": "arm.c (arm_evpc_neon_vext): New function.\n\n2012-09-04  Christophe Lyon  <christophe.lyon@linaro.org>\n\n\tgcc/\n\t* config/arm/arm.c (arm_evpc_neon_vext): New\n\tfunction.\n\t(arm_expand_vec_perm_const_1): Add call to\n\tarm_evpc_neon_vext.\n\n\tgcc/testsuite/\n\t* gcc.target/arm/neon-vext.c: New test.\n\t* gcc.target/arm/neon-vext-execute.c: Ditto.\n\nFrom-SVN: r190911", "tree": {"sha": "d905cbb1980a6d296367ddf04bf6aad0a3354e02", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d905cbb1980a6d296367ddf04bf6aad0a3354e02"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/434641a57b90584bffa4e8def3f900c7d102bfdf", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/434641a57b90584bffa4e8def3f900c7d102bfdf", "html_url": "https://github.com/Rust-GCC/gccrs/commit/434641a57b90584bffa4e8def3f900c7d102bfdf", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/434641a57b90584bffa4e8def3f900c7d102bfdf/comments", "author": null, "committer": null, "parents": [{"sha": "ee3bea0b28549727df923754c8d5e9590f53bba2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ee3bea0b28549727df923754c8d5e9590f53bba2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ee3bea0b28549727df923754c8d5e9590f53bba2"}], "stats": {"total": 539, "additions": 539, "deletions": 0}, "files": [{"sha": "cda27fd74dfb06fbd66a38bdd753075df5f8ada6", "filename": "gcc/ChangeLog", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/434641a57b90584bffa4e8def3f900c7d102bfdf/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/434641a57b90584bffa4e8def3f900c7d102bfdf/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=434641a57b90584bffa4e8def3f900c7d102bfdf", "patch": "@@ -1,3 +1,10 @@\n+2012-09-04  Christophe Lyon  <christophe.lyon@linaro.org>\n+\n+\t* config/arm/arm.c (arm_evpc_neon_vext): New\n+\tfunction.\n+\t(arm_expand_vec_perm_const_1): Add call to\n+\tarm_evpc_neon_vext.\n+\n 2012-09-04  Oleg Endo  <olegendo@gcc.gnu.org>\n \n \tPR target/51244"}, {"sha": "36937d2b90d597dc878a8f4b253981c22c8fca58", "filename": "gcc/config/arm/arm.c", "status": "modified", "additions": 72, "deletions": 0, "changes": 72, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/434641a57b90584bffa4e8def3f900c7d102bfdf/gcc%2Fconfig%2Farm%2Farm.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/434641a57b90584bffa4e8def3f900c7d102bfdf/gcc%2Fconfig%2Farm%2Farm.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.c?ref=434641a57b90584bffa4e8def3f900c7d102bfdf", "patch": "@@ -25937,6 +25937,72 @@ arm_evpc_neon_vtrn (struct expand_vec_perm_d *d)\n   return true;\n }\n \n+/* Recognize patterns for the VEXT insns.  */\n+\n+static bool\n+arm_evpc_neon_vext (struct expand_vec_perm_d *d)\n+{\n+  unsigned int i, nelt = d->nelt;\n+  rtx (*gen) (rtx, rtx, rtx, rtx);\n+  rtx offset;\n+\n+  unsigned int location;\n+\n+  unsigned int next  = d->perm[0] + 1;\n+\n+  /* TODO: Handle GCC's numbering of elements for big-endian.  */\n+  if (BYTES_BIG_ENDIAN)\n+    return false;\n+\n+  /* Check if the extracted indexes are increasing by one.  */\n+  for (i = 1; i < nelt; next++, i++)\n+    {\n+      /* If we hit the most significant element of the 2nd vector in\n+\t the previous iteration, no need to test further.  */\n+      if (next == 2 * nelt)\n+\treturn false;\n+\n+      /* If we are operating on only one vector: it could be a\n+\t rotation.  If there are only two elements of size < 64, let\n+\t arm_evpc_neon_vrev catch it.  */\n+      if (d->one_vector_p && (next == nelt))\n+\t{\n+\t  if ((nelt == 2) && (d->vmode != V2DImode))\n+\t    return false;\n+\t  else\n+\t    next = 0;\n+\t}\n+\n+      if (d->perm[i] != next)\n+\treturn false;\n+    }\n+\n+  location = d->perm[0];\n+\n+  switch (d->vmode)\n+    {\n+    case V16QImode: gen = gen_neon_vextv16qi; break;\n+    case V8QImode: gen = gen_neon_vextv8qi; break;\n+    case V4HImode: gen = gen_neon_vextv4hi; break;\n+    case V8HImode: gen = gen_neon_vextv8hi; break;\n+    case V2SImode: gen = gen_neon_vextv2si; break;\n+    case V4SImode: gen = gen_neon_vextv4si; break;\n+    case V2SFmode: gen = gen_neon_vextv2sf; break;\n+    case V4SFmode: gen = gen_neon_vextv4sf; break;\n+    case V2DImode: gen = gen_neon_vextv2di; break;\n+    default:\n+      return false;\n+    }\n+\n+  /* Success! */\n+  if (d->testing_p)\n+    return true;\n+\n+  offset = GEN_INT (location);\n+  emit_insn (gen (d->target, d->op0, d->op1, offset));\n+  return true;\n+}\n+\n /* The NEON VTBL instruction is a fully variable permuation that's even\n    stronger than what we expose via VEC_PERM_EXPR.  What it doesn't do\n    is mask the index operand as VEC_PERM_EXPR requires.  Therefore we\n@@ -25976,6 +26042,12 @@ arm_evpc_neon_vtbl (struct expand_vec_perm_d *d)\n static bool\n arm_expand_vec_perm_const_1 (struct expand_vec_perm_d *d)\n {\n+  /* Check if the input mask matches vext before reordering the\n+     operands.  */\n+  if (TARGET_NEON)\n+    if (arm_evpc_neon_vext (d))\n+      return true;\n+\n   /* The pattern matching functions above are written to look for a small\n      number to begin the sequence (0, 1, N/2).  If we begin with an index\n      from the second operand, we can swap the operands.  */"}, {"sha": "4a0532188cdd4e520af5beb5ffd7527af92a13c8", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/434641a57b90584bffa4e8def3f900c7d102bfdf/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/434641a57b90584bffa4e8def3f900c7d102bfdf/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=434641a57b90584bffa4e8def3f900c7d102bfdf", "patch": "@@ -1,3 +1,8 @@\n+2012-09-04  Christophe Lyon  <christophe.lyon@linaro.org>\n+\n+\t* gcc.target/arm/neon-vext.c: New test.\n+\t* gcc.target/arm/neon-vext-execute.c: Ditto.\n+\n 2012-09-04  Janus Weil  <janus@gcc.gnu.org>\n \n \tPR fortran/54243"}, {"sha": "3d6c28cca89e1d86bd7536bf1ae978adcdaee65c", "filename": "gcc/testsuite/gcc.target/arm/neon-vext-execute.c", "status": "added", "additions": 340, "deletions": 0, "changes": 340, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/434641a57b90584bffa4e8def3f900c7d102bfdf/gcc%2Ftestsuite%2Fgcc.target%2Farm%2Fneon-vext-execute.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/434641a57b90584bffa4e8def3f900c7d102bfdf/gcc%2Ftestsuite%2Fgcc.target%2Farm%2Fneon-vext-execute.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Farm%2Fneon-vext-execute.c?ref=434641a57b90584bffa4e8def3f900c7d102bfdf", "patch": "@@ -0,0 +1,340 @@\n+/* { dg-do run } */\n+/* { dg-require-effective-target arm_neon_ok } */\n+/* { dg-require-effective-target arm_little_endian } */\n+/* { dg-options \"-O2\" } */\n+/* { dg-add-options arm_neon } */\n+\n+#include <arm_neon.h>\n+#include <stdlib.h>\n+#include <stdio.h>\n+\n+uint8x8_t\n+tst_vext_u8 (uint8x8_t __a, uint8x8_t __b)\n+{\n+  uint8x8_t __mask1 = {2, 3, 4, 5, 6, 7, 8, 9};\n+\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint8x8_t\n+tst_vext_u8_rotate (uint8x8_t __a)\n+{\n+  uint8x8_t __mask1 = {2, 3, 4, 5, 6, 7, 0, 1};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint16x4_t\n+tst_vext_u16 (uint16x4_t __a, uint16x4_t __b)\n+{\n+  uint16x4_t __mask1 = {2, 3, 4, 5};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint16x4_t\n+tst_vext_u16_rotate (uint16x4_t __a)\n+{\n+  uint16x4_t __mask1 = {2, 3, 0, 1};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint32x2_t\n+tst_vext_u32 (uint32x2_t __a, uint32x2_t __b)\n+{\n+  uint32x2_t __mask1 = {1, 2};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+/* This one is mapped into vrev64.32.  */\n+uint32x2_t\n+tst_vext_u32_rotate (uint32x2_t __a)\n+{\n+  uint32x2_t __mask1 = {1, 0};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint8x16_t\n+tst_vextq_u8 (uint8x16_t __a, uint8x16_t __b)\n+{\n+  uint8x16_t __mask1 = {4, 5, 6, 7, 8, 9, 10, 11,\n+\t\t\t12, 13, 14, 15, 16, 17, 18, 19};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint8x16_t\n+tst_vextq_u8_rotate (uint8x16_t __a)\n+{\n+  uint8x16_t __mask1 = {4, 5, 6, 7, 8, 9, 10, 11,\n+\t\t\t12, 13, 14, 15, 0, 1, 2, 3};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint16x8_t\n+tst_vextq_u16 (uint16x8_t __a, uint16x8_t __b)\n+{\n+  uint16x8_t __mask1 = {2, 3, 4, 5, 6, 7, 8, 9};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint16x8_t\n+tst_vextq_u16_rotate (uint16x8_t __a)\n+{\n+  uint16x8_t __mask1 = {2, 3, 4, 5, 6, 7, 0, 1};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint32x4_t\n+tst_vextq_u32 (uint32x4_t __a, uint32x4_t __b)\n+{\n+  uint32x4_t __mask1 = {1, 2, 3, 4};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint32x4_t\n+tst_vextq_u32_rotate (uint32x4_t __a)\n+{\n+  uint32x4_t __mask1 = {1, 2, 3, 0};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint64x2_t\n+tst_vextq_u64 (uint64x2_t __a, uint64x2_t __b)\n+{\n+  uint64x2_t __mask1 = {1, 2};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint64x2_t\n+tst_vextq_u64_rotate (uint64x2_t __a)\n+{\n+  uint64x2_t __mask1 = {1, 0};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+int main (void)\n+{\n+  uint8_t arr_u8x8[] = {0, 1, 2, 3, 4, 5, 6, 7};\n+  uint8_t arr2_u8x8[] = {8, 9, 10, 11, 12, 13, 14, 15};\n+  uint16_t arr_u16x4[] = {0, 1, 2, 3};\n+  uint16_t arr2_u16x4[] = {4, 5, 6, 7};\n+  uint32_t arr_u32x2[] = {0, 1};\n+  uint32_t arr2_u32x2[] = {2, 3};\n+  uint8_t arr_u8x16[] = {0, 1, 2, 3, 4, 5, 6, 7,\n+\t\t\t 8, 9, 10, 11, 12, 13, 14, 15};\n+  uint8_t arr2_u8x16[] = {16, 17, 18, 19, 20, 21, 22, 23,\n+\t\t\t  24, 25, 26, 27, 28, 29, 30, 31};\n+  uint16_t arr_u16x8[] = {0, 1, 2, 3, 4, 5, 6, 7};\n+  uint16_t arr2_u16x8[] = {8, 9, 10, 11, 12, 13, 14, 15};\n+  uint32_t arr_u32x4[] = {0, 1, 2, 3};\n+  uint32_t arr2_u32x4[] = {4, 5, 6, 7};\n+  uint64_t arr_u64x2[] = {0, 1};\n+  uint64_t arr2_u64x2[] = {2, 3};\n+\n+  uint8_t expected_u8x8[] = {2, 3, 4, 5, 6, 7, 8, 9};\n+  uint8_t expected_rot_u8x8[] = {2, 3, 4, 5, 6, 7, 0, 1};\n+  uint16_t expected_u16x4[] = {2, 3, 4, 5};\n+  uint16_t expected_rot_u16x4[] = {2, 3, 0, 1};\n+  uint32_t expected_u32x2[] = {1, 2};\n+  uint32_t expected_rot_u32x2[] = {1, 0};\n+  uint8_t expected_u8x16[] = {4, 5, 6, 7, 8, 9, 10, 11,\n+\t\t\t      12, 13, 14, 15, 16, 17, 18, 19};\n+  uint8_t expected_rot_u8x16[] = {4, 5, 6, 7, 8, 9, 10, 11,\n+\t\t\t\t  12, 13, 14, 15, 0, 1, 2, 3,};\n+  uint16_t expected_u16x8[] = {2, 3, 4, 5, 6, 7, 8, 9};\n+  uint16_t expected_rot_u16x8[] = {2, 3, 4, 5, 6, 7, 0, 1};\n+  uint32_t expected_u32x4[] = {1, 2, 3, 4};\n+  uint32_t expected_rot_u32x4[] = {1, 2, 3, 0};\n+  uint64_t expected_u64x2[] = {1, 2};\n+  uint64_t expected_rot_u64x2[] = {1, 0};\n+\n+  uint8x8_t vec_u8x8 = vld1_u8 (arr_u8x8);\n+  uint8x8_t vec2_u8x8 = vld1_u8 (arr2_u8x8);\n+  uint16x4_t vec_u16x4 = vld1_u16 (arr_u16x4);\n+  uint16x4_t vec2_u16x4 = vld1_u16 (arr2_u16x4);\n+  uint32x2_t vec_u32x2 = vld1_u32 (arr_u32x2);\n+  uint32x2_t vec2_u32x2 = vld1_u32 (arr2_u32x2);\n+  uint8x16_t vec_u8x16 = vld1q_u8 (arr_u8x16);\n+  uint8x16_t vec2_u8x16 = vld1q_u8 (arr2_u8x16);\n+  uint16x8_t vec_u16x8 = vld1q_u16 (arr_u16x8);\n+  uint16x8_t vec2_u16x8 = vld1q_u16 (arr2_u16x8);\n+  uint32x4_t vec_u32x4 = vld1q_u32 (arr_u32x4);\n+  uint32x4_t vec2_u32x4 = vld1q_u32 (arr2_u32x4);\n+  uint64x2_t vec_u64x2 = vld1q_u64 (arr_u64x2);\n+  uint64x2_t vec2_u64x2 = vld1q_u64 (arr2_u64x2);\n+\n+  uint8x8_t result_u8x8;\n+  uint16x4_t result_u16x4;\n+  uint32x2_t result_u32x2;\n+  uint8x16_t result_u8x16;\n+  uint16x8_t result_u16x8;\n+  uint32x4_t result_u32x4;\n+  uint64x2_t result_u64x2;\n+\n+  union {uint8x8_t v; uint8_t buf[8];} mem_u8x8;\n+  union {uint16x4_t v; uint16_t buf[4];} mem_u16x4;\n+  union {uint32x2_t v; uint32_t buf[2];} mem_u32x2;\n+  union {uint8x16_t v; uint8_t buf[16];} mem_u8x16;\n+  union {uint16x8_t v; uint16_t buf[8];} mem_u16x8;\n+  union {uint32x4_t v; uint32_t buf[4];} mem_u32x4;\n+  union {uint64x2_t v; uint64_t buf[2];} mem_u64x2;\n+\n+  int i;\n+\n+  result_u8x8 = tst_vext_u8 (vec_u8x8, vec2_u8x8);\n+  vst1_u8 (mem_u8x8.buf, result_u8x8);\n+\n+  for (i=0; i<8; i++)\n+      if (mem_u8x8.buf[i] != expected_u8x8[i])\n+\t{\n+\t  printf (\"tst_vext_u8[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u8x8.buf[i], expected_u8x8[i]);\n+\t  abort ();\n+\t}\n+\n+  result_u8x8 = tst_vext_u8_rotate (vec_u8x8);\n+  vst1_u8 (mem_u8x8.buf, result_u8x8);\n+\n+  for (i=0; i<8; i++)\n+      if (mem_u8x8.buf[i] != expected_rot_u8x8[i])\n+\t{\n+\t  printf (\"tst_vext_u8_rotate[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u8x8.buf[i], expected_rot_u8x8[i]);\n+\t  abort ();\n+\t}\n+\n+\n+  result_u16x4 = tst_vext_u16 (vec_u16x4, vec2_u16x4);\n+  vst1_u16 (mem_u16x4.buf, result_u16x4);\n+\n+  for (i=0; i<4; i++)\n+      if (mem_u16x4.buf[i] != expected_u16x4[i])\n+\t{\n+\t  printf (\"tst_vext_u16[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u16x4.buf[i], expected_u16x4[i]);\n+\t  abort ();\n+\t}\n+\n+  result_u16x4 = tst_vext_u16_rotate (vec_u16x4);\n+  vst1_u16 (mem_u16x4.buf, result_u16x4);\n+\n+  for (i=0; i<4; i++)\n+      if (mem_u16x4.buf[i] != expected_rot_u16x4[i])\n+\t{\n+\t  printf (\"tst_vext_u16_rotate[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u16x4.buf[i], expected_rot_u16x4[i]);\n+\t  abort ();\n+\t}\n+\n+\n+  result_u32x2 = tst_vext_u32 (vec_u32x2, vec2_u32x2);\n+  vst1_u32 (mem_u32x2.buf, result_u32x2);\n+\n+  for (i=0; i<2; i++)\n+      if (mem_u32x2.buf[i] != expected_u32x2[i])\n+\t{\n+\t  printf (\"tst_vext_u32[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u32x2.buf[i], expected_u32x2[i]);\n+\t  abort ();\n+\t}\n+\n+  result_u32x2 = tst_vext_u32_rotate (vec_u32x2);\n+  vst1_u32 (mem_u32x2.buf, result_u32x2);\n+\n+  for (i=0; i<2; i++)\n+      if (mem_u32x2.buf[i] != expected_rot_u32x2[i])\n+\t{\n+\t  printf (\"tst_vext_u32_rotate[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u32x2.buf[i], expected_rot_u32x2[i]);\n+\t  abort ();\n+\t}\n+\n+\n+  result_u8x16 = tst_vextq_u8 (vec_u8x16, vec2_u8x16);\n+  vst1q_u8 (mem_u8x16.buf, result_u8x16);\n+\n+  for (i=0; i<16; i++)\n+      if (mem_u8x16.buf[i] != expected_u8x16[i])\n+\t{\n+\t  printf (\"tst_vextq_u8[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u8x16.buf[i], expected_u8x16[i]);\n+\t  abort ();\n+\t}\n+\n+  result_u8x16 = tst_vextq_u8_rotate (vec_u8x16);\n+  vst1q_u8 (mem_u8x16.buf, result_u8x16);\n+\n+  for (i=0; i<16; i++)\n+      if (mem_u8x16.buf[i] != expected_rot_u8x16[i])\n+\t{\n+\t  printf (\"tst_vextq_u8_rotate[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u8x16.buf[i], expected_rot_u8x16[i]);\n+\t  abort ();\n+\t}\n+\n+  result_u16x8 = tst_vextq_u16 (vec_u16x8, vec2_u16x8);\n+  vst1q_u16 (mem_u16x8.buf, result_u16x8);\n+\n+  for (i=0; i<8; i++)\n+      if (mem_u16x8.buf[i] != expected_u16x8[i])\n+\t{\n+\t  printf (\"tst_vextq_u16[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u16x8.buf[i], expected_u16x8[i]);\n+\t  abort ();\n+\t}\n+\n+  result_u16x8 = tst_vextq_u16_rotate (vec_u16x8);\n+  vst1q_u16 (mem_u16x8.buf, result_u16x8);\n+\n+  for (i=0; i<8; i++)\n+      if (mem_u16x8.buf[i] != expected_rot_u16x8[i])\n+\t{\n+\t  printf (\"tst_vextq_u16_rotate[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u16x8.buf[i], expected_rot_u16x8[i]);\n+\t  abort ();\n+\t}\n+\n+  result_u32x4 = tst_vextq_u32 (vec_u32x4, vec2_u32x4);\n+  vst1q_u32 (mem_u32x4.buf, result_u32x4);\n+\n+  for (i=0; i<4; i++)\n+      if (mem_u32x4.buf[i] != expected_u32x4[i])\n+\t{\n+\t  printf (\"tst_vextq_u32[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u32x4.buf[i], expected_u32x4[i]);\n+\t  abort ();\n+\t}\n+\n+  result_u32x4 = tst_vextq_u32_rotate (vec_u32x4);\n+  vst1q_u32 (mem_u32x4.buf, result_u32x4);\n+\n+  for (i=0; i<4; i++)\n+      if (mem_u32x4.buf[i] != expected_rot_u32x4[i])\n+\t{\n+\t  printf (\"tst_vextq_u32_rotate[%d]=%d expected %d\\n\",\n+\t\t  i, mem_u32x4.buf[i], expected_rot_u32x4[i]);\n+\t  abort ();\n+\t}\n+\n+  result_u64x2 = tst_vextq_u64 (vec_u64x2, vec2_u64x2);\n+  vst1q_u64 (mem_u64x2.buf, result_u64x2);\n+\n+  for (i=0; i<2; i++)\n+      if (mem_u64x2.buf[i] != expected_u64x2[i])\n+\t{\n+\t  printf (\"tst_vextq_u64[%d]=%lld expected %lld\\n\",\n+\t\t  i, mem_u64x2.buf[i], expected_u64x2[i]);\n+\t  abort ();\n+\t}\n+\n+  result_u64x2 = tst_vextq_u64_rotate (vec_u64x2);\n+  vst1q_u64 (mem_u64x2.buf, result_u64x2);\n+\n+  for (i=0; i<2; i++)\n+      if (mem_u64x2.buf[i] != expected_rot_u64x2[i])\n+\t{\n+\t  printf (\"tst_vextq_u64_rotate[%d]=%lld expected %lld\\n\",\n+\t\t  i, mem_u64x2.buf[i], expected_rot_u64x2[i]);\n+\t  abort ();\n+\t}\n+\n+  return 0;\n+}"}, {"sha": "4a012a996a81ddbee528ac9d7e247a8a0ab25494", "filename": "gcc/testsuite/gcc.target/arm/neon-vext.c", "status": "added", "additions": 115, "deletions": 0, "changes": 115, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/434641a57b90584bffa4e8def3f900c7d102bfdf/gcc%2Ftestsuite%2Fgcc.target%2Farm%2Fneon-vext.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/434641a57b90584bffa4e8def3f900c7d102bfdf/gcc%2Ftestsuite%2Fgcc.target%2Farm%2Fneon-vext.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Farm%2Fneon-vext.c?ref=434641a57b90584bffa4e8def3f900c7d102bfdf", "patch": "@@ -0,0 +1,115 @@\n+/* { dg-do compile } */\n+/* { dg-require-effective-target arm_neon_ok } */\n+/* { dg-require-effective-target arm_little_endian } */\n+/* { dg-options \"-O2\" } */\n+/* { dg-add-options arm_neon } */\n+\n+#include <arm_neon.h>\n+\n+uint8x8_t\n+tst_vext_u8 (uint8x8_t __a, uint8x8_t __b)\n+{\n+  uint8x8_t __mask1 = {2, 3, 4, 5, 6, 7, 8, 9};\n+\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint8x8_t\n+tst_vext_u8_rotate (uint8x8_t __a)\n+{\n+  uint8x8_t __mask1 = {2, 3, 4, 5, 6, 7, 0, 1};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint16x4_t\n+tst_vext_u16 (uint16x4_t __a, uint16x4_t __b)\n+{\n+  uint16x4_t __mask1 = {2, 3, 4, 5};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint16x4_t\n+tst_vext_u16_rotate (uint16x4_t __a)\n+{\n+  uint16x4_t __mask1 = {2, 3, 0, 1};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint32x2_t\n+tst_vext_u32 (uint32x2_t __a, uint32x2_t __b)\n+{\n+  uint32x2_t __mask1 = {1, 2};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+/* This one is mapped into vrev64.32.  */\n+uint32x2_t\n+tst_vext_u32_rotate (uint32x2_t __a)\n+{\n+  uint32x2_t __mask1 = {1, 0};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint8x16_t\n+tst_vextq_u8 (uint8x16_t __a, uint8x16_t __b)\n+{\n+  uint8x16_t __mask1 = {4, 5, 6, 7, 8, 9, 10, 11,\n+\t\t\t12, 13, 14, 15, 16, 17, 18, 19};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint8x16_t\n+tst_vextq_u8_rotate (uint8x16_t __a)\n+{\n+  uint8x16_t __mask1 = {4, 5, 6, 7, 8, 9, 10, 11,\n+\t\t\t12, 13, 14, 15, 0, 1, 2, 3};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint16x8_t\n+tst_vextq_u16 (uint16x8_t __a, uint16x8_t __b)\n+{\n+  uint16x8_t __mask1 = {2, 3, 4, 5, 6, 7, 8, 9};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint16x8_t\n+tst_vextq_u16_rotate (uint16x8_t __a)\n+{\n+  uint16x8_t __mask1 = {2, 3, 4, 5, 6, 7, 0, 1};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint32x4_t\n+tst_vextq_u32 (uint32x4_t __a, uint32x4_t __b)\n+{\n+  uint32x4_t __mask1 = {1, 2, 3, 4};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint32x4_t\n+tst_vextq_u32_rotate (uint32x4_t __a)\n+{\n+  uint32x4_t __mask1 = {1, 2, 3, 0};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+uint64x2_t\n+tst_vextq_u64 (uint64x2_t __a, uint64x2_t __b)\n+{\n+  uint64x2_t __mask1 = {1, 2};\n+  return __builtin_shuffle ( __a, __b, __mask1) ;\n+}\n+\n+uint64x2_t\n+tst_vextq_u64_rotate (uint64x2_t __a)\n+{\n+  uint64x2_t __mask1 = {1, 0};\n+  return __builtin_shuffle ( __a, __mask1) ;\n+}\n+\n+/* { dg-final {scan-assembler-times \"vext\\.8\\\\t\" 4} }  */\n+/* { dg-final {scan-assembler-times \"vext\\.16\\\\t\" 4} }  */\n+/* { dg-final {scan-assembler-times \"vext\\.32\\\\t\" 3} }  */\n+/* { dg-final {scan-assembler-times \"vrev64\\.32\\\\t\" 1} }  */\n+/* { dg-final {scan-assembler-times \"vext\\.64\\\\t\" 2} }  */"}]}