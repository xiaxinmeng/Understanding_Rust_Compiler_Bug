{"sha": "096c59be14c385b96c186dd1a1b45f95d80e102e", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDk2YzU5YmUxNGMzODViOTZjMTg2ZGQxYTFiNDVmOTVkODBlMTAyZQ==", "commit": {"author": {"name": "Alan Lawrence", "email": "alan.lawrence@arm.com", "date": "2014-06-23T14:07:42Z"}, "committer": {"name": "Alan Lawrence", "email": "alalaw01@gcc.gnu.org", "date": "2014-06-23T14:07:42Z"}, "message": "PR/60825 Make {int,uint}64x1_t in arm_neon.h a proper vector type\n\ngcc/ChangeLog:\n \tPR target/60825\n\t* config/aarch64/aarch64-builtins.c (aarch64_types_unop_qualifiers):\n\tIgnore third operand if present by marking qualifier_internal.\n\n\t* config/aarch64/aarch64-simd-builtins.def (abs): Comment.\n\n\t* config/aarch64/arm_neon.h (int64x1_t, uint64x1_t): Typedef to GCC\n\tvector extension.\n\t(aarch64_vget_lane_s64, aarch64_vdup_lane_s64,\n\tarch64_vdupq_lane_s64, aarch64_vdupq_lane_u64): Remove macro.\n\t(vqadd_s64, vqadd_u64, vqsub_s64, vqsub_u64, vqneg_s64, vqabs_s64,\n\tvcreate_s64, vcreate_u64, vreinterpret_s64_f64, vreinterpret_u64_f64,\n\tvcombine_u64, vbsl_s64, vbsl_u64, vceq_s64, vceq_u64, vceqz_s64,\n\tvceqz_u64, vcge_s64, vcge_u64, vcgez_s64, vcgt_s64, vcgt_u64,\n\tvcgtz_s64, vcle_s64, vcle_u64, vclez_s64, vclt_s64, vclt_u64,\n\tvcltz_s64, vdup_n_s64, vdup_n_u64, vld1_s64, vld1_u64, vmov_n_s64,\n\tvmov_n_u64, vqdmlals_lane_s32, vqdmlsls_lane_s32,\n\tvqdmulls_lane_s32, vqrshl_s64, vqrshl_u64, vqrshl_u64, vqshl_s64,\n\tvqshl_u64, vqshl_n_s64, vqshl_n_u64, vqshl_n_s64, vqshl_n_u64,\n\tvqshlu_n_s64, vrshl_s64, vrshl_u64, vrshr_n_s64, vrshr_n_u64,\n\tvrsra_n_s64, vrsra_n_u64, vshl_n_s64, vshl_n_u64, vshl_s64,\n\tvshl_u64, vshr_n_s64, vshr_n_u64, vsli_n_s64, vsli_n_u64,\n\tvsqadd_u64, vsra_n_s64, vsra_n_u64, vsri_n_s64, vsri_n_u64,\n\tvst1_s64, vst1_u64, vtst_s64, vtst_u64, vuqadd_s64): Wrap existing\n\tlogic in GCC vector extensions\n\t\n\t(vpaddd_s64, vaddd_s64, vaddd_u64, vceqd_s64, vceqd_u64, vceqzd_s64\n\tvceqzd_u64, vcged_s64, vcged_u64, vcgezd_s64, vcgtd_s64, vcgtd_u64,\n\tvcgtzd_s64, vcled_s64, vcled_u64, vclezd_s64, vcltd_s64, vcltd_u64,\n\tvcltzd_s64, vqdmlals_s32, vqdmlsls_s32, vqmovnd_s64, vqmovnd_u64\n\tvqmovund_s64, vqrshld_s64, vqrshld_u64, vqrshrnd_n_s64,\n\tvqrshrnd_n_u64, vqrshrund_n_s64, vqshld_s64, vqshld_u64,\n\tvqshld_n_u64, vqshrnd_n_s64, vqshrnd_n_u64, vqshrund_n_s64,\n\tvrshld_u64, vrshrd_n_u64, vrsrad_n_u64, vshld_n_u64, vshld_s64,\n\tvshld_u64, vslid_n_u64, vsqaddd_u64, vsrad_n_u64, vsrid_n_u64,\n\tvsubd_s64, vsubd_u64, vtstd_s64, vtstd_u64): Fix type signature.\n\n\t(vabs_s64): Use GCC vector extensions; call __builtin_aarch64_absdi.\n\n\t(vget_high_s64, vget_high_u64): Reimplement with GCC vector\n\textensions.\n\n\t(__GET_LOW, vget_low_u64): Wrap result using vcreate_u64.\n\t(vget_low_s64): Use __GET_LOW macro.\n\t(vget_lane_s64, vget_lane_u64, vdupq_lane_s64, vdupq_lane_u64): Use\n\tgcc vector extensions, add call to __builtin_aarch64_lane_boundsi.\n\t(vdup_lane_s64, vdup_lane_u64,): Add __builtin_aarch64_lane_bound_si.\n\t(vdupd_lane_s64, vdupd_lane_u64): Fix type signature, add\n\t__builtin_aarch64_lane_boundsi, use GCC vector extensions.\n\n\t(vcombine_s64): Use GCC vector extensions; remove cast.\n\t(vqaddd_s64, vqaddd_u64, vqdmulls_s32, vqshld_n_s64, vqshlud_n_s64,\n\tvqsubd_s64, vqsubd_u64, vrshld_s64, vrshrd_n_s64, vrsrad_n_s64,\n\tvshld_n_s64, vshrd_n_s64, vslid_n_s64, vsrad_n_s64, vsrid_n_s64):\n\tFix type signature; remove cast.\n\ngcc/testsuite/ChangeLog:\n\t* g++.dg/abi/mangle-neon-aarch64.C (f22, f23): New tests of \n\t[u]int64x1_t.\n\n\t* gcc.target/aarch64/aapcs64/func-ret-64x1_1.c: Add {u,}int64x1 cases.\n\t* gcc.target/aarch64/aapcs64/test_64x1_1.c: Likewise.\n\n\t* gcc.target/aarch64/scalar_intrinsics.c (test_vaddd_u64,\n\ttest_vaddd_s64, test_vceqd_s64, test_vceqzd_s64, test_vcged_s64,\n\ttest_vcled_s64, test_vcgezd_s64, test_vcged_u64, test_vcgtd_s64,\n\ttest_vcltd_s64, test_vcgtzd_s64, test_vcgtd_u64, test_vclezd_s64,\n\ttest_vcltzd_s64, test_vqaddd_u64, test_vqaddd_s64, test_vqdmlals_s32,\n\ttest_vqdmlsls_s32, test_vqdmulls_s32, test_vuqaddd_s64,\n\ttest_vsqaddd_u64, test_vqmovund_s64, test_vqmovnd_s64,\n\ttest_vqmovnd_u64, test_vsubd_u64, test_vsubd_s64, test_vqsubd_u64,\n\ttest_vqsubd_s64, test_vshld_s64, test_vshld_u64, test_vrshld_s64,\n\ttest_vrshld_u64, test_vshrd_n_s64, test_vshrd_n_u64, test_vsrad_n_s64,\n\ttest_vsrad_n_u64, test_vrshrd_n_s64, test_vrshrd_n_u64,\n\ttest_vrsrad_n_s64, test_vrsrad_n_u64, test_vqrshld_s64,\n\ttest_vqrshld_u64, test_vqshlud_n_s64, test_vqshld_s64, test_vqshld_u64,\n\ttest_vqshld_n_u64, test_vqshrund_n_s64, test_vqrshrund_n_s64,\n\ttest_vqshrnd_n_s64, test_vqshrnd_n_u64, test_vqrshrnd_n_s64,\n\ttest_vqrshrnd_n_u64, test_vshld_n_s64, test_vshdl_n_u64,\n\ttest_vslid_n_s64, test_vslid_n_u64, test_vsrid_n_s64,\n\ttest_vsrid_n_u64): Fix signature to match intrinsic.\n\t\n\t(test_vabs_s64): Remove.\n\t(test_vaddd_s64_2, test_vsubd_s64_2): Use force_simd.\n\n\t(test_vdupd_lane_s64): Rename to...\n\t(test_vdupd_laneq_s64): ...and remove a call to force_simd.\n\n\t(test_vdupd_lane_u64): Rename to...\n\t(test_vdupd_laneq_u64): ...and remove a call to force_simd.\n\n\t(test_vtst_s64): Rename to...\n\t(test_vtstd_s64): ...and change int64x1_t to int64_t.\n\n\t(test_vtst_u64): Rename to...\n\t(test_vtstd_u64): ...and change uint64x1_t to uint64_t.\n\n\t* gcc.target/aarch64/singleton_intrinsics_1.c: New file.\n\t* gcc.target/aarch64/vdup_lane_1.c, gcc.target/aarch64/vdup_lane_2.c:\n\tRemove out-of-bounds tests.\n\t* gcc.target/aarch64/vneg_s.c (INDEX*, RUN_TEST): Remove INDEX macro.\n\nFrom-SVN: r211894", "tree": {"sha": "72eb63c796c4f9c2a426547084ade5f5060cbeca", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/72eb63c796c4f9c2a426547084ade5f5060cbeca"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/096c59be14c385b96c186dd1a1b45f95d80e102e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/096c59be14c385b96c186dd1a1b45f95d80e102e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/096c59be14c385b96c186dd1a1b45f95d80e102e", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/096c59be14c385b96c186dd1a1b45f95d80e102e/comments", "author": null, "committer": null, "parents": [{"sha": "c6a29a091a11b6337c78130b738a0f5a96b2a2f5", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c6a29a091a11b6337c78130b738a0f5a96b2a2f5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c6a29a091a11b6337c78130b738a0f5a96b2a2f5"}], "stats": {"total": 1349, "additions": 922, "deletions": 427}, "files": [{"sha": "4e567e36d5b724ed3ce7d651f9e269fb47a13402", "filename": "gcc/ChangeLog", "status": "modified", "additions": 58, "deletions": 0, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -1,3 +1,61 @@\n+2014-06-19  Alan Lawrence  <alan.lawrence@arm.com>\n+\n+\tPR target/60825\n+\t* config/aarch64/aarch64-builtins.c (aarch64_types_unop_qualifiers):\n+\tIgnore third operand if present by marking qualifier_internal.\n+\n+\t* config/aarch64/aarch64-simd-builtins.def (abs): Comment.\n+\n+\t* config/aarch64/arm_neon.h (int64x1_t, uint64x1_t): Typedef to GCC\n+\tvector extension.\n+\t(aarch64_vget_lane_s64, aarch64_vdup_lane_s64,\n+\tarch64_vdupq_lane_s64, aarch64_vdupq_lane_u64): Remove macro.\n+\t(vqadd_s64, vqadd_u64, vqsub_s64, vqsub_u64, vqneg_s64, vqabs_s64,\n+\tvcreate_s64, vcreate_u64, vreinterpret_s64_f64, vreinterpret_u64_f64,\n+\tvcombine_u64, vbsl_s64, vbsl_u64, vceq_s64, vceq_u64, vceqz_s64,\n+\tvceqz_u64, vcge_s64, vcge_u64, vcgez_s64, vcgt_s64, vcgt_u64,\n+\tvcgtz_s64, vcle_s64, vcle_u64, vclez_s64, vclt_s64, vclt_u64,\n+\tvcltz_s64, vdup_n_s64, vdup_n_u64, vld1_s64, vld1_u64, vmov_n_s64,\n+\tvmov_n_u64, vqdmlals_lane_s32, vqdmlsls_lane_s32,\n+\tvqdmulls_lane_s32, vqrshl_s64, vqrshl_u64, vqrshl_u64, vqshl_s64,\n+\tvqshl_u64, vqshl_n_s64, vqshl_n_u64, vqshl_n_s64, vqshl_n_u64,\n+\tvqshlu_n_s64, vrshl_s64, vrshl_u64, vrshr_n_s64, vrshr_n_u64,\n+\tvrsra_n_s64, vrsra_n_u64, vshl_n_s64, vshl_n_u64, vshl_s64,\n+\tvshl_u64, vshr_n_s64, vshr_n_u64, vsli_n_s64, vsli_n_u64,\n+\tvsqadd_u64, vsra_n_s64, vsra_n_u64, vsri_n_s64, vsri_n_u64,\n+\tvst1_s64, vst1_u64, vtst_s64, vtst_u64, vuqadd_s64): Wrap existing\n+\tlogic in GCC vector extensions\n+\t\n+\t(vpaddd_s64, vaddd_s64, vaddd_u64, vceqd_s64, vceqd_u64, vceqzd_s64\n+\tvceqzd_u64, vcged_s64, vcged_u64, vcgezd_s64, vcgtd_s64, vcgtd_u64,\n+\tvcgtzd_s64, vcled_s64, vcled_u64, vclezd_s64, vcltd_s64, vcltd_u64,\n+\tvcltzd_s64, vqdmlals_s32, vqdmlsls_s32, vqmovnd_s64, vqmovnd_u64\n+\tvqmovund_s64, vqrshld_s64, vqrshld_u64, vqrshrnd_n_s64,\n+\tvqrshrnd_n_u64, vqrshrund_n_s64, vqshld_s64, vqshld_u64,\n+\tvqshld_n_u64, vqshrnd_n_s64, vqshrnd_n_u64, vqshrund_n_s64,\n+\tvrshld_u64, vrshrd_n_u64, vrsrad_n_u64, vshld_n_u64, vshld_s64,\n+\tvshld_u64, vslid_n_u64, vsqaddd_u64, vsrad_n_u64, vsrid_n_u64,\n+\tvsubd_s64, vsubd_u64, vtstd_s64, vtstd_u64): Fix type signature.\n+\n+\t(vabs_s64): Use GCC vector extensions; call __builtin_aarch64_absdi.\n+\n+\t(vget_high_s64, vget_high_u64): Reimplement with GCC vector\n+\textensions.\n+\n+\t(__GET_LOW, vget_low_u64): Wrap result using vcreate_u64.\n+\t(vget_low_s64): Use __GET_LOW macro.\n+\t(vget_lane_s64, vget_lane_u64, vdupq_lane_s64, vdupq_lane_u64): Use\n+\tgcc vector extensions, add call to __builtin_aarch64_lane_boundsi.\n+\t(vdup_lane_s64, vdup_lane_u64,): Add __builtin_aarch64_lane_bound_si.\n+\t(vdupd_lane_s64, vdupd_lane_u64): Fix type signature, add\n+\t__builtin_aarch64_lane_boundsi, use GCC vector extensions.\n+\n+\t(vcombine_s64): Use GCC vector extensions; remove cast.\n+\t(vqaddd_s64, vqaddd_u64, vqdmulls_s32, vqshld_n_s64, vqshlud_n_s64,\n+\tvqsubd_s64, vqsubd_u64, vrshld_s64, vrshrd_n_s64, vrsrad_n_s64,\n+\tvshld_n_s64, vshrd_n_s64, vslid_n_s64, vsrad_n_s64, vsrid_n_s64):\n+\tFix type signature; remove cast.\n+\n 2014-06-19  Alan Lawrence  <alan.lawrence@arm.com>\n \n \tPR target/60825"}, {"sha": "fee17ecf637436c8704f565be2eb9ef23891209a", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -140,9 +140,11 @@ typedef struct\n   enum aarch64_type_qualifiers *qualifiers;\n } aarch64_simd_builtin_datum;\n \n+/*  The qualifier_internal allows generation of a unary builtin from\n+    a pattern with a third pseudo-operand such as a match_scratch.  */\n static enum aarch64_type_qualifiers\n aarch64_types_unop_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n-  = { qualifier_none, qualifier_none };\n+  = { qualifier_none, qualifier_none, qualifier_internal };\n #define TYPES_UNOP (aarch64_types_unop_qualifiers)\n static enum aarch64_type_qualifiers\n aarch64_types_unopu_qualifiers[SIMD_MAX_BUILTIN_ARGS]"}, {"sha": "268432cc117b7027ee9472fc5a4f9b1ea13bea0f", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -365,6 +365,8 @@\n   BUILTIN_VDQF (UNOP, frecpe, 0)\n   BUILTIN_VDQF (BINOP, frecps, 0)\n \n+  /* Implemented by a mixture of abs2 patterns.  Note the DImode builtin is\n+     only ever used for the int64x1_t intrinsic, there is no scalar version.  */\n   BUILTIN_VALLDI (UNOP, abs, 2)\n \n   VAR1 (UNOP, vec_unpacks_hi_, 10, v4sf)"}, {"sha": "befdfbbe08032af777f1e1dd753fd8fd19346c08", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -7358,6 +7358,8 @@ static aarch64_simd_mangle_map_entry aarch64_simd_mangle_map[] = {\n   { V2SImode,  \"__builtin_aarch64_simd_si\",     \"11__Int32x2_t\" },\n   { V2SImode,  \"__builtin_aarch64_simd_usi\",    \"12__Uint32x2_t\" },\n   { V2SFmode,  \"__builtin_aarch64_simd_sf\",     \"13__Float32x2_t\" },\n+  { DImode,    \"__builtin_aarch64_simd_di\",     \"11__Int64x1_t\" },\n+  { DImode,    \"__builtin_aarch64_simd_udi\",    \"12__Uint64x1_t\" },\n   { V1DFmode,  \"__builtin_aarch64_simd_df\",\t\"13__Float64x1_t\" },\n   { V8QImode,  \"__builtin_aarch64_simd_poly8\",  \"11__Poly8x8_t\" },\n   { V4HImode,  \"__builtin_aarch64_simd_poly16\", \"12__Poly16x4_t\" },"}, {"sha": "d5d8c23acd75b6f2a4e8cd6cc4daca418372f883", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 240, "deletions": 226, "changes": 466, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -38,7 +38,8 @@ typedef __builtin_aarch64_simd_hi int16x4_t\n   __attribute__ ((__vector_size__ (8)));\n typedef __builtin_aarch64_simd_si int32x2_t\n   __attribute__ ((__vector_size__ (8)));\n-typedef int64_t int64x1_t;\n+typedef __builtin_aarch64_simd_di int64x1_t\n+  __attribute__ ((__vector_size__ (8)));\n typedef int32_t int32x1_t;\n typedef int16_t int16x1_t;\n typedef int8_t int8x1_t;\n@@ -56,7 +57,8 @@ typedef __builtin_aarch64_simd_uhi uint16x4_t\n   __attribute__ ((__vector_size__ (8)));\n typedef __builtin_aarch64_simd_usi uint32x2_t\n   __attribute__ ((__vector_size__ (8)));\n-typedef uint64_t uint64x1_t;\n+typedef __builtin_aarch64_simd_udi uint64x1_t\n+  __attribute__ ((__vector_size__ (8)));\n typedef uint32_t uint32x1_t;\n typedef uint16_t uint16x1_t;\n typedef uint8_t uint8x1_t;\n@@ -479,15 +481,23 @@ typedef struct poly16x8x4_t\n   __aarch64_vget_lane_any (v4hi, , ,__a, __b)\n #define __aarch64_vget_lane_s32(__a, __b) \\\n   __aarch64_vget_lane_any (v2si, , ,__a, __b)\n-#define __aarch64_vget_lane_s64(__a, __b) (__a)\n+#define __aarch64_vget_lane_s64(__a, __b) __extension__\t\\\n+  ({\t\t\t\t\t\t\t\\\n+    __builtin_aarch64_im_lane_boundsi (__b, 1);\t\t\\\n+    __a[0];\t\t\t\t\t\t\\\n+  })\n \n #define __aarch64_vget_lane_u8(__a, __b) \\\n   __aarch64_vget_lane_any (v8qi, (uint8_t), (int8x8_t), __a, __b)\n #define __aarch64_vget_lane_u16(__a, __b) \\\n   __aarch64_vget_lane_any (v4hi, (uint16_t), (int16x4_t), __a, __b)\n #define __aarch64_vget_lane_u32(__a, __b) \\\n   __aarch64_vget_lane_any (v2si, (uint32_t), (int32x2_t), __a, __b)\n-#define __aarch64_vget_lane_u64(__a, __b) (__a)\n+#define __aarch64_vget_lane_u64(__a, __b) __extension__\t\\\n+  ({\t\t\t\t\t\t\t\\\n+    __builtin_aarch64_im_lane_boundsi (__b, 1);\t\t\\\n+    __a[0];\t\t\t\t\t\t\\\n+  })\n \n #define __aarch64_vgetq_lane_f32(__a, __b) \\\n   __aarch64_vget_lane_any (v4sf, , , __a, __b)\n@@ -535,14 +545,16 @@ typedef struct poly16x8x4_t\n    __aarch64_vdup_lane_any (s16, , , __a, __b)\n #define __aarch64_vdup_lane_s32(__a, __b) \\\n    __aarch64_vdup_lane_any (s32, , , __a, __b)\n-#define __aarch64_vdup_lane_s64(__a, __b) (__a)\n+#define __aarch64_vdup_lane_s64(__a, __b) \\\n+  __aarch64_vdup_lane_any (s64, , , __a, __b)\n #define __aarch64_vdup_lane_u8(__a, __b) \\\n    __aarch64_vdup_lane_any (u8, , , __a, __b)\n #define __aarch64_vdup_lane_u16(__a, __b) \\\n    __aarch64_vdup_lane_any (u16, , , __a, __b)\n #define __aarch64_vdup_lane_u32(__a, __b) \\\n    __aarch64_vdup_lane_any (u32, , , __a, __b)\n-#define __aarch64_vdup_lane_u64(__a, __b) (__a)\n+#define __aarch64_vdup_lane_u64(__a, __b) \\\n+   __aarch64_vdup_lane_any (u64, , , __a, __b)\n \n /* __aarch64_vdup_laneq internal macros.  */\n #define __aarch64_vdup_laneq_f32(__a, __b) \\\n@@ -585,14 +597,16 @@ typedef struct poly16x8x4_t\n    __aarch64_vdup_lane_any (s16, q, , __a, __b)\n #define __aarch64_vdupq_lane_s32(__a, __b) \\\n    __aarch64_vdup_lane_any (s32, q, , __a, __b)\n-#define __aarch64_vdupq_lane_s64(__a, __b) (vdupq_n_s64 (__a))\n+#define __aarch64_vdupq_lane_s64(__a, __b) \\\n+   __aarch64_vdup_lane_any (s64, q, , __a, __b)\n #define __aarch64_vdupq_lane_u8(__a, __b) \\\n    __aarch64_vdup_lane_any (u8, q, , __a, __b)\n #define __aarch64_vdupq_lane_u16(__a, __b) \\\n    __aarch64_vdup_lane_any (u16, q, , __a, __b)\n #define __aarch64_vdupq_lane_u32(__a, __b) \\\n    __aarch64_vdup_lane_any (u32, q, , __a, __b)\n-#define __aarch64_vdupq_lane_u64(__a, __b) (vdupq_n_u64 (__a))\n+#define __aarch64_vdupq_lane_u64(__a, __b) \\\n+   __aarch64_vdup_lane_any (u64, q, , __a, __b)\n \n /* __aarch64_vdupq_laneq internal macros.  */\n #define __aarch64_vdupq_laneq_f32(__a, __b) \\\n@@ -2120,7 +2134,7 @@ vqadd_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vqadd_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sqadddi (__a, __b);\n+  return (int64x1_t) {__builtin_aarch64_sqadddi (__a[0], __b[0])};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -2144,8 +2158,7 @@ vqadd_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vqadd_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_uqadddi_uuu ((uint64_t) __a,\n-\t\t\t\t\t\t     (uint64_t) __b);\n+  return (uint64x1_t) {__builtin_aarch64_uqadddi_uuu (__a[0], __b[0])};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -2217,7 +2230,7 @@ vqsub_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vqsub_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sqsubdi (__a, __b);\n+  return (int64x1_t) {__builtin_aarch64_sqsubdi (__a[0], __b[0])};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -2241,8 +2254,7 @@ vqsub_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vqsub_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_uqsubdi_uuu ((uint64_t) __a,\n-\t\t\t\t\t\t     (uint64_t) __b);\n+  return (uint64x1_t) {__builtin_aarch64_uqsubdi_uuu (__a[0], __b[0])};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -2314,7 +2326,7 @@ vqneg_s32 (int32x2_t __a)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vqneg_s64 (int64x1_t __a)\n {\n-  return __builtin_aarch64_sqnegdi (__a);\n+  return (int64x1_t) {__builtin_aarch64_sqnegdi (__a[0])};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -2356,7 +2368,7 @@ vqabs_s32 (int32x2_t __a)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vqabs_s64 (int64x1_t __a)\n {\n-  return __builtin_aarch64_sqabsdi (__a);\n+  return (int64x1_t) {__builtin_aarch64_sqabsdi (__a[0])};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -2446,7 +2458,7 @@ vcreate_s32 (uint64_t __a)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vcreate_s64 (uint64_t __a)\n {\n-  return (int64x1_t) __a;\n+  return (int64x1_t) {__a};\n }\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n@@ -2476,7 +2488,7 @@ vcreate_u32 (uint64_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcreate_u64 (uint64_t __a)\n {\n-  return (uint64x1_t) __a;\n+  return (uint64x1_t) {__a};\n }\n \n __extension__ static __inline float64x1_t __attribute__ ((__always_inline__))\n@@ -3178,7 +3190,7 @@ vreinterpretq_f64_u64 (uint64x2_t __a)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vreinterpret_s64_f64 (float64x1_t __a)\n {\n-  return __builtin_aarch64_reinterpretdiv1df (__a);\n+  return (int64x1_t) {__builtin_aarch64_reinterpretdiv1df (__a)};\n }\n \n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n@@ -3310,7 +3322,7 @@ vreinterpretq_s64_p16 (poly16x8_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vreinterpret_u64_f64 (float64x1_t __a)\n {\n-  return __builtin_aarch64_reinterpretdiv1df_us (__a);\n+  return (uint64x1_t) {__builtin_aarch64_reinterpretdiv1df_us (__a)};\n }\n \n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n@@ -4233,7 +4245,7 @@ vreinterpretq_u32_p16 (poly16x8_t __a)\n \n #define __GET_LOW(__TYPE) \\\n   uint64x2_t tmp = vreinterpretq_u64_##__TYPE (__a);  \\\n-  uint64_t lo = vgetq_lane_u64 (tmp, 0);  \\\n+  uint64x1_t lo = vcreate_u64 (vgetq_lane_u64 (tmp, 0));  \\\n   return vreinterpret_##__TYPE##_u64 (lo);\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n@@ -4281,7 +4293,7 @@ vget_low_s32 (int32x4_t __a)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vget_low_s64 (int64x2_t __a)\n {\n-  return vgetq_lane_s64 (__a, 0);\n+  __GET_LOW (s64);\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -4305,7 +4317,7 @@ vget_low_u32 (uint32x4_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vget_low_u64 (uint64x2_t __a)\n {\n-  return vgetq_lane_u64 (__a, 0);\n+  return vcreate_u64 (vgetq_lane_u64 (__a, 0));\n }\n \n #undef __GET_LOW\n@@ -4331,7 +4343,7 @@ vcombine_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n vcombine_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (int64x2_t) __builtin_aarch64_combinedi (__a, __b);\n+  return __builtin_aarch64_combinedi (__a[0], __b[0]);\n }\n \n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n@@ -4364,8 +4376,7 @@ vcombine_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n vcombine_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (uint64x2_t) __builtin_aarch64_combinedi ((int64x1_t) __a,\n-\t\t\t\t\t\t   (int64x1_t) __b);\n+  return (uint64x2_t) __builtin_aarch64_combinedi (__a[0], __b[0]);\n }\n \n __extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n@@ -12552,7 +12563,7 @@ vaddlv_u32 (uint32x2_t a)\n   return result;\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n vpaddd_s64 (int64x2_t __a)\n {\n   return __builtin_aarch64_addpdi (__a);\n@@ -13463,7 +13474,7 @@ vabs_s32 (int32x2_t __a)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vabs_s64 (int64x1_t __a)\n {\n-  return __builtin_llabs (__a);\n+  return (int64x1_t) {__builtin_aarch64_absdi (__a[0])};\n }\n \n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n@@ -13504,14 +13515,14 @@ vabsq_s64 (int64x2_t __a)\n \n /* vadd */\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vaddd_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vaddd_s64 (int64_t __a, int64_t __b)\n {\n   return __a + __b;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vaddd_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vaddd_u64 (uint64_t __a, uint64_t __b)\n {\n   return __a + __b;\n }\n@@ -13679,7 +13690,8 @@ vbsl_s32 (uint32x2_t __a, int32x2_t __b, int32x2_t __c)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vbsl_s64 (uint64x1_t __a, int64x1_t __b, int64x1_t __c)\n {\n-  return __builtin_aarch64_simd_bsldi_suss (__a, __b, __c);\n+  return (int64x1_t)\n+      {__builtin_aarch64_simd_bsldi_suss (__a[0], __b[0], __c[0])};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -13703,7 +13715,8 @@ vbsl_u32 (uint32x2_t __a, uint32x2_t __b, uint32x2_t __c)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vbsl_u64 (uint64x1_t __a, uint64x1_t __b, uint64x1_t __c)\n {\n-  return __builtin_aarch64_simd_bsldi_uuuu (__a, __b, __c);\n+  return (uint64x1_t)\n+      {__builtin_aarch64_simd_bsldi_uuuu (__a[0], __b[0], __c[0])};\n }\n \n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n@@ -13954,7 +13967,7 @@ vceq_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vceq_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return __a == __b ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] == __b[0] ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -13981,7 +13994,7 @@ vceq_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vceq_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return __a == __b ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] == __b[0] ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n@@ -14063,14 +14076,14 @@ vceqs_f32 (float32_t __a, float32_t __b)\n   return __a == __b ? -1 : 0;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vceqd_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vceqd_s64 (int64_t __a, int64_t __b)\n {\n   return __a == __b ? -1ll : 0ll;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vceqd_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vceqd_u64 (uint64_t __a, uint64_t __b)\n {\n   return __a == __b ? -1ll : 0ll;\n }\n@@ -14128,7 +14141,7 @@ vceqz_s32 (int32x2_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vceqz_s64 (int64x1_t __a)\n {\n-  return __a == 0ll ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] == 0ll ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -14158,7 +14171,7 @@ vceqz_u32 (uint32x2_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vceqz_u64 (uint64x1_t __a)\n {\n-  return __a == 0ll ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] == 0ll ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n@@ -14254,14 +14267,14 @@ vceqzs_f32 (float32_t __a)\n   return __a == 0.0f ? -1 : 0;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vceqzd_s64 (int64x1_t __a)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vceqzd_s64 (int64_t __a)\n {\n   return __a == 0 ? -1ll : 0ll;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vceqzd_u64 (int64x1_t __a)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vceqzd_u64 (uint64_t __a)\n {\n   return __a == 0 ? -1ll : 0ll;\n }\n@@ -14307,7 +14320,7 @@ vcge_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcge_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return __a >= __b ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] >= __b[0] ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -14334,7 +14347,7 @@ vcge_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcge_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return __a >= __b ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] >= __b[0] ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n@@ -14409,14 +14422,14 @@ vcges_f32 (float32_t __a, float32_t __b)\n   return __a >= __b ? -1 : 0;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcged_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcged_s64 (int64_t __a, int64_t __b)\n {\n   return __a >= __b ? -1ll : 0ll;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcged_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcged_u64 (uint64_t __a, uint64_t __b)\n {\n   return __a >= __b ? -1ll : 0ll;\n }\n@@ -14466,7 +14479,7 @@ vcgez_s32 (int32x2_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcgez_s64 (int64x1_t __a)\n {\n-  return __a >= 0ll ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] >= 0ll ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n@@ -14520,8 +14533,8 @@ vcgezs_f32 (float32_t __a)\n   return __a >= 0.0f ? -1 : 0;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcgezd_s64 (int64x1_t __a)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcgezd_s64 (int64_t __a)\n {\n   return __a >= 0 ? -1ll : 0ll;\n }\n@@ -14567,7 +14580,7 @@ vcgt_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcgt_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return __a > __b ? -1ll : 0ll;\n+  return (uint64x1_t) (__a[0] > __b[0] ? -1ll : 0ll);\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -14594,7 +14607,7 @@ vcgt_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcgt_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return __a > __b ? -1ll : 0ll;\n+  return (uint64x1_t) (__a[0] > __b[0] ? -1ll : 0ll);\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n@@ -14669,14 +14682,14 @@ vcgts_f32 (float32_t __a, float32_t __b)\n   return __a > __b ? -1 : 0;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcgtd_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcgtd_s64 (int64_t __a, int64_t __b)\n {\n   return __a > __b ? -1ll : 0ll;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcgtd_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcgtd_u64 (uint64_t __a, uint64_t __b)\n {\n   return __a > __b ? -1ll : 0ll;\n }\n@@ -14726,7 +14739,7 @@ vcgtz_s32 (int32x2_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcgtz_s64 (int64x1_t __a)\n {\n-  return __a > 0ll ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] > 0ll ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n@@ -14780,8 +14793,8 @@ vcgtzs_f32 (float32_t __a)\n   return __a > 0.0f ? -1 : 0;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcgtzd_s64 (int64x1_t __a)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcgtzd_s64 (int64_t __a)\n {\n   return __a > 0 ? -1ll : 0ll;\n }\n@@ -14827,7 +14840,7 @@ vcle_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcle_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return __a <= __b ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] <= __b[0] ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -14854,7 +14867,7 @@ vcle_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcle_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return __a <= __b ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] <= __b[0] ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n@@ -14929,14 +14942,14 @@ vcles_f32 (float32_t __a, float32_t __b)\n   return __a <= __b ? -1 : 0;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcled_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcled_s64 (int64_t __a, int64_t __b)\n {\n   return __a <= __b ? -1ll : 0ll;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcled_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcled_u64 (uint64_t __a, uint64_t __b)\n {\n   return __a <= __b ? -1ll : 0ll;\n }\n@@ -14986,7 +14999,7 @@ vclez_s32 (int32x2_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vclez_s64 (int64x1_t __a)\n {\n-  return __a <= 0ll ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] <= 0ll ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n@@ -15040,8 +15053,8 @@ vclezs_f32 (float32_t __a)\n   return __a <= 0.0f ? -1 : 0;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vclezd_s64 (int64x1_t __a)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vclezd_s64 (int64_t __a)\n {\n   return __a <= 0 ? -1ll : 0ll;\n }\n@@ -15087,7 +15100,7 @@ vclt_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vclt_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return __a < __b ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] < __b[0] ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -15114,7 +15127,7 @@ vclt_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vclt_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return __a < __b ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] < __b[0] ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n@@ -15189,14 +15202,14 @@ vclts_f32 (float32_t __a, float32_t __b)\n   return __a < __b ? -1 : 0;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcltd_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcltd_s64 (int64_t __a, int64_t __b)\n {\n   return __a < __b ? -1ll : 0ll;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcltd_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcltd_u64 (uint64_t __a, uint64_t __b)\n {\n   return __a < __b ? -1ll : 0ll;\n }\n@@ -15246,7 +15259,7 @@ vcltz_s32 (int32x2_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vcltz_s64 (int64x1_t __a)\n {\n-  return __a < 0ll ? -1ll : 0ll;\n+  return (uint64x1_t) {__a[0] < 0ll ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n@@ -15300,8 +15313,8 @@ vcltzs_f32 (float32_t __a)\n   return __a < 0.0f ? -1 : 0;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vcltzd_s64 (int64x1_t __a)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vcltzd_s64 (int64_t __a)\n {\n   return __a < 0 ? -1ll : 0ll;\n }\n@@ -15864,7 +15877,7 @@ vdup_n_s32 (int32_t __a)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vdup_n_s64 (int64_t __a)\n {\n-  return __a;\n+  return (int64x1_t) {__a};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -15888,7 +15901,7 @@ vdup_n_u32 (uint32_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vdup_n_u64 (uint64_t __a)\n {\n-  return __a;\n+  return (uint64x1_t) {__a};\n }\n \n /* vdupq_n  */\n@@ -16328,15 +16341,17 @@ vdupd_lane_f64 (float64x1_t __a, const int __b)\n }\n \n __extension__ static __inline int64_t __attribute__ ((__always_inline__))\n-vdupd_lane_s64 (int64x1_t __a, const int __attribute__ ((unused)) __b)\n+vdupd_lane_s64 (int64x1_t __a, const int __b)\n {\n-  return __a;\n+  __builtin_aarch64_im_lane_boundsi (__b, 1);\n+  return __a[0];\n }\n \n __extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n-vdupd_lane_u64 (uint64x1_t __a, const int __attribute__ ((unused)) __b)\n+vdupd_lane_u64 (uint64x1_t __a, const int __b)\n {\n-  return __a;\n+  __builtin_aarch64_im_lane_boundsi (__b, 1);\n+  return __a[0];\n }\n \n /* vdupb_laneq  */\n@@ -16956,7 +16971,7 @@ vld1_s32 (const int32_t *a)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vld1_s64 (const int64_t *a)\n {\n-  return *a;\n+  return (int64x1_t) {*a};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -16983,7 +16998,7 @@ vld1_u32 (const uint32_t *a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vld1_u64 (const uint64_t *a)\n {\n-  return *a;\n+  return (uint64x1_t) {*a};\n }\n \n /* vld1q */\n@@ -18806,7 +18821,7 @@ vmov_n_s32 (int32_t __a)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vmov_n_s64 (int64_t __a)\n {\n-  return __a;\n+  return (int64x1_t) {__a};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -18830,7 +18845,7 @@ vmov_n_u32 (uint32_t __a)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vmov_n_u64 (uint64_t __a)\n {\n-   return __a;\n+  return (uint64x1_t) {__a};\n }\n \n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n@@ -19184,10 +19199,10 @@ vqadds_s32 (int32x1_t __a, int32x1_t __b)\n   return (int32x1_t) __builtin_aarch64_sqaddsi (__a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vqaddd_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vqaddd_s64 (int64_t __a, int64_t __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sqadddi (__a, __b);\n+  return __builtin_aarch64_sqadddi (__a, __b);\n }\n \n __extension__ static __inline uint8x1_t __attribute__ ((__always_inline__))\n@@ -19208,11 +19223,10 @@ vqadds_u32 (uint32x1_t __a, uint32x1_t __b)\n   return (uint32x1_t) __builtin_aarch64_uqaddsi_uuu (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vqaddd_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vqaddd_u64 (uint64_t __a, uint64_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_uqadddi_uuu ((uint64_t) __a,\n-\t\t\t\t\t\t     (uint64_t) __b);\n+  return __builtin_aarch64_uqadddi_uuu (__a, __b);\n }\n \n /* vqdmlal */\n@@ -19329,16 +19343,17 @@ vqdmlalh_lane_s16 (int32x1_t __a, int16x1_t __b, int16x4_t __c, const int __d)\n   return __builtin_aarch64_sqdmlal_lanehi (__a, __b, __c, __d);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vqdmlals_s32 (int64x1_t __a, int32x1_t __b, int32x1_t __c)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vqdmlals_s32 (int64_t __a, int32x1_t __b, int32x1_t __c)\n {\n   return __builtin_aarch64_sqdmlalsi (__a, __b, __c);\n }\n \n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vqdmlals_lane_s32 (int64x1_t __a, int32x1_t __b, int32x2_t __c, const int __d)\n {\n-  return __builtin_aarch64_sqdmlal_lanesi (__a, __b, __c, __d);\n+  return (int64x1_t)\n+      {__builtin_aarch64_sqdmlal_lanesi (__a[0], __b, __c, __d)};\n }\n \n /* vqdmlsl */\n@@ -19455,16 +19470,16 @@ vqdmlslh_lane_s16 (int32x1_t __a, int16x1_t __b, int16x4_t __c, const int __d)\n   return __builtin_aarch64_sqdmlsl_lanehi (__a, __b, __c, __d);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vqdmlsls_s32 (int64x1_t __a, int32x1_t __b, int32x1_t __c)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vqdmlsls_s32 (int64_t __a, int32x1_t __b, int32x1_t __c)\n {\n   return __builtin_aarch64_sqdmlslsi (__a, __b, __c);\n }\n \n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vqdmlsls_lane_s32 (int64x1_t __a, int32x1_t __b, int32x2_t __c, const int __d)\n {\n-  return __builtin_aarch64_sqdmlsl_lanesi (__a, __b, __c, __d);\n+  return (int64x1_t) {__builtin_aarch64_sqdmlsl_lanesi (__a[0], __b, __c, __d)};\n }\n \n /* vqdmulh */\n@@ -19627,16 +19642,16 @@ vqdmullh_lane_s16 (int16x1_t __a, int16x4_t __b, const int __c)\n   return __builtin_aarch64_sqdmull_lanehi (__a, __b, __c);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n vqdmulls_s32 (int32x1_t __a, int32x1_t __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sqdmullsi (__a, __b);\n+  return __builtin_aarch64_sqdmullsi (__a, __b);\n }\n \n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vqdmulls_lane_s32 (int32x1_t __a, int32x2_t __b, const int __c)\n {\n-  return __builtin_aarch64_sqdmull_lanesi (__a, __b, __c);\n+  return (int64x1_t) {__builtin_aarch64_sqdmull_lanesi (__a, __b, __c)};\n }\n \n /* vqmovn */\n@@ -19690,7 +19705,7 @@ vqmovns_s32 (int32x1_t __a)\n }\n \n __extension__ static __inline int32x1_t __attribute__ ((__always_inline__))\n-vqmovnd_s64 (int64x1_t __a)\n+vqmovnd_s64 (int64_t __a)\n {\n   return (int32x1_t) __builtin_aarch64_sqmovndi (__a);\n }\n@@ -19708,7 +19723,7 @@ vqmovns_u32 (uint32x1_t __a)\n }\n \n __extension__ static __inline uint32x1_t __attribute__ ((__always_inline__))\n-vqmovnd_u64 (uint64x1_t __a)\n+vqmovnd_u64 (uint64_t __a)\n {\n   return (uint32x1_t) __builtin_aarch64_uqmovndi (__a);\n }\n@@ -19746,7 +19761,7 @@ vqmovuns_s32 (int32x1_t __a)\n }\n \n __extension__ static __inline int32x1_t __attribute__ ((__always_inline__))\n-vqmovund_s64 (int64x1_t __a)\n+vqmovund_s64 (int64_t __a)\n {\n   return (int32x1_t) __builtin_aarch64_sqmovundi (__a);\n }\n@@ -19856,7 +19871,7 @@ vqrshl_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vqrshl_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return __builtin_aarch64_sqrshldi (__a, __b);\n+  return (int64x1_t) {__builtin_aarch64_sqrshldi (__a[0], __b[0])};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -19880,7 +19895,7 @@ vqrshl_u32 (uint32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vqrshl_u64 (uint64x1_t __a, int64x1_t __b)\n {\n-  return __builtin_aarch64_uqrshldi_uus ( __a, __b);\n+  return (uint64x1_t) {__builtin_aarch64_uqrshldi_uus (__a[0], __b[0])};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -19949,8 +19964,8 @@ vqrshls_s32 (int32x1_t __a, int32x1_t __b)\n   return __builtin_aarch64_sqrshlsi (__a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vqrshld_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vqrshld_s64 (int64_t __a, int64_t __b)\n {\n   return __builtin_aarch64_sqrshldi (__a, __b);\n }\n@@ -19973,8 +19988,8 @@ vqrshls_u32 (uint32x1_t __a, uint32x1_t __b)\n   return __builtin_aarch64_uqrshlsi_uus (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vqrshld_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vqrshld_u64 (uint64_t __a, uint64_t __b)\n {\n   return __builtin_aarch64_uqrshldi_uus (__a, __b);\n }\n@@ -20030,7 +20045,7 @@ vqrshrns_n_s32 (int32x1_t __a, const int __b)\n }\n \n __extension__ static __inline int32x1_t __attribute__ ((__always_inline__))\n-vqrshrnd_n_s64 (int64x1_t __a, const int __b)\n+vqrshrnd_n_s64 (int64_t __a, const int __b)\n {\n   return (int32x1_t) __builtin_aarch64_sqrshrn_ndi (__a, __b);\n }\n@@ -20048,7 +20063,7 @@ vqrshrns_n_u32 (uint32x1_t __a, const int __b)\n }\n \n __extension__ static __inline uint32x1_t __attribute__ ((__always_inline__))\n-vqrshrnd_n_u64 (uint64x1_t __a, const int __b)\n+vqrshrnd_n_u64 (uint64_t __a, const int __b)\n {\n   return __builtin_aarch64_uqrshrn_ndi_uus (__a, __b);\n }\n@@ -20086,7 +20101,7 @@ vqrshruns_n_s32 (int32x1_t __a, const int __b)\n }\n \n __extension__ static __inline int32x1_t __attribute__ ((__always_inline__))\n-vqrshrund_n_s64 (int64x1_t __a, const int __b)\n+vqrshrund_n_s64 (int64_t __a, const int __b)\n {\n   return (int32x1_t) __builtin_aarch64_sqrshrun_ndi (__a, __b);\n }\n@@ -20114,7 +20129,7 @@ vqshl_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vqshl_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return __builtin_aarch64_sqshldi (__a, __b);\n+  return (int64x1_t) {__builtin_aarch64_sqshldi (__a[0], __b[0])};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -20138,7 +20153,7 @@ vqshl_u32 (uint32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vqshl_u64 (uint64x1_t __a, int64x1_t __b)\n {\n-  return __builtin_aarch64_uqshldi_uus ( __a, __b);\n+  return (uint64x1_t) {__builtin_aarch64_uqshldi_uus (__a[0], __b[0])};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -20207,8 +20222,8 @@ vqshls_s32 (int32x1_t __a, int32x1_t __b)\n   return __builtin_aarch64_sqshlsi (__a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vqshld_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vqshld_s64 (int64_t __a, int64_t __b)\n {\n   return __builtin_aarch64_sqshldi (__a, __b);\n }\n@@ -20231,8 +20246,8 @@ vqshls_u32 (uint32x1_t __a, uint32x1_t __b)\n   return __builtin_aarch64_uqshlsi_uus (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vqshld_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vqshld_u64 (uint64_t __a, uint64_t __b)\n {\n   return __builtin_aarch64_uqshldi_uus (__a, __b);\n }\n@@ -20258,7 +20273,7 @@ vqshl_n_s32 (int32x2_t __a, const int __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vqshl_n_s64 (int64x1_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sqshl_ndi (__a, __b);\n+  return (int64x1_t) {__builtin_aarch64_sqshl_ndi (__a[0], __b)};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -20282,7 +20297,7 @@ vqshl_n_u32 (uint32x2_t __a, const int __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vqshl_n_u64 (uint64x1_t __a, const int __b)\n {\n-  return __builtin_aarch64_uqshl_ndi_uus (__a, __b);\n+  return (uint64x1_t) {__builtin_aarch64_uqshl_ndi_uus (__a[0], __b)};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -20351,10 +20366,10 @@ vqshls_n_s32 (int32x1_t __a, const int __b)\n   return (int32x1_t) __builtin_aarch64_sqshl_nsi (__a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vqshld_n_s64 (int64x1_t __a, const int __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vqshld_n_s64 (int64_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sqshl_ndi (__a, __b);\n+  return __builtin_aarch64_sqshl_ndi (__a, __b);\n }\n \n __extension__ static __inline uint8x1_t __attribute__ ((__always_inline__))\n@@ -20375,8 +20390,8 @@ vqshls_n_u32 (uint32x1_t __a, const int __b)\n   return __builtin_aarch64_uqshl_nsi_uus (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vqshld_n_u64 (uint64x1_t __a, const int __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vqshld_n_u64 (uint64_t __a, const int __b)\n {\n   return __builtin_aarch64_uqshl_ndi_uus (__a, __b);\n }\n@@ -20404,7 +20419,7 @@ vqshlu_n_s32 (int32x2_t __a, const int __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vqshlu_n_s64 (int64x1_t __a, const int __b)\n {\n-  return __builtin_aarch64_sqshlu_ndi_uss (__a, __b);\n+  return (uint64x1_t) {__builtin_aarch64_sqshlu_ndi_uss (__a[0], __b)};\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n@@ -20449,10 +20464,10 @@ vqshlus_n_s32 (int32x1_t __a, const int __b)\n   return (int32x1_t) __builtin_aarch64_sqshlu_nsi_uss (__a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vqshlud_n_s64 (int64x1_t __a, const int __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vqshlud_n_s64 (int64_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sqshlu_ndi_uss (__a, __b);\n+  return __builtin_aarch64_sqshlu_ndi_uss (__a, __b);\n }\n \n /* vqshrn */\n@@ -20506,7 +20521,7 @@ vqshrns_n_s32 (int32x1_t __a, const int __b)\n }\n \n __extension__ static __inline int32x1_t __attribute__ ((__always_inline__))\n-vqshrnd_n_s64 (int64x1_t __a, const int __b)\n+vqshrnd_n_s64 (int64_t __a, const int __b)\n {\n   return (int32x1_t) __builtin_aarch64_sqshrn_ndi (__a, __b);\n }\n@@ -20524,7 +20539,7 @@ vqshrns_n_u32 (uint32x1_t __a, const int __b)\n }\n \n __extension__ static __inline uint32x1_t __attribute__ ((__always_inline__))\n-vqshrnd_n_u64 (uint64x1_t __a, const int __b)\n+vqshrnd_n_u64 (uint64_t __a, const int __b)\n {\n   return __builtin_aarch64_uqshrn_ndi_uus (__a, __b);\n }\n@@ -20562,7 +20577,7 @@ vqshruns_n_s32 (int32x1_t __a, const int __b)\n }\n \n __extension__ static __inline int32x1_t __attribute__ ((__always_inline__))\n-vqshrund_n_s64 (int64x1_t __a, const int __b)\n+vqshrund_n_s64 (int64_t __a, const int __b)\n {\n   return (int32x1_t) __builtin_aarch64_sqshrun_ndi (__a, __b);\n }\n@@ -20587,10 +20602,10 @@ vqsubs_s32 (int32x1_t __a, int32x1_t __b)\n   return (int32x1_t) __builtin_aarch64_sqsubsi (__a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vqsubd_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vqsubd_s64 (int64_t __a, int64_t __b)\n {\n-  return (int64x1_t) __builtin_aarch64_sqsubdi (__a, __b);\n+  return __builtin_aarch64_sqsubdi (__a, __b);\n }\n \n __extension__ static __inline uint8x1_t __attribute__ ((__always_inline__))\n@@ -20611,11 +20626,10 @@ vqsubs_u32 (uint32x1_t __a, uint32x1_t __b)\n   return (uint32x1_t) __builtin_aarch64_uqsubsi_uuu (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vqsubd_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vqsubd_u64 (uint64_t __a, uint64_t __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_uqsubdi_uuu ((uint64_t) __a,\n-\t\t\t\t\t\t     (uint64_t) __b);\n+  return __builtin_aarch64_uqsubdi_uuu (__a, __b);\n }\n \n /* vrecpe  */\n@@ -21129,7 +21143,7 @@ vrshl_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vrshl_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (int64x1_t) __builtin_aarch64_srshldi (__a, __b);\n+  return (int64x1_t) {__builtin_aarch64_srshldi (__a[0], __b[0])};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -21153,7 +21167,7 @@ vrshl_u32 (uint32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vrshl_u64 (uint64x1_t __a, int64x1_t __b)\n {\n-  return __builtin_aarch64_urshldi_uus (__a, __b);\n+  return (uint64x1_t) {__builtin_aarch64_urshldi_uus (__a[0], __b[0])};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -21204,14 +21218,14 @@ vrshlq_u64 (uint64x2_t __a, int64x2_t __b)\n   return __builtin_aarch64_urshlv2di_uus (__a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vrshld_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vrshld_s64 (int64_t __a, int64_t __b)\n {\n-  return (int64x1_t) __builtin_aarch64_srshldi (__a, __b);\n+  return __builtin_aarch64_srshldi (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vrshld_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vrshld_u64 (uint64_t __a, int64_t __b)\n {\n   return __builtin_aarch64_urshldi_uus (__a, __b);\n }\n@@ -21239,7 +21253,7 @@ vrshr_n_s32 (int32x2_t __a, const int __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vrshr_n_s64 (int64x1_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_srshr_ndi (__a, __b);\n+  return (int64x1_t) {__builtin_aarch64_srshr_ndi (__a[0], __b)};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -21263,7 +21277,7 @@ vrshr_n_u32 (uint32x2_t __a, const int __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vrshr_n_u64 (uint64x1_t __a, const int __b)\n {\n-  return __builtin_aarch64_urshr_ndi_uus (__a, __b);\n+  return (uint64x1_t) {__builtin_aarch64_urshr_ndi_uus (__a[0], __b)};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -21314,14 +21328,14 @@ vrshrq_n_u64 (uint64x2_t __a, const int __b)\n   return __builtin_aarch64_urshr_nv2di_uus (__a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vrshrd_n_s64 (int64x1_t __a, const int __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vrshrd_n_s64 (int64_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_srshr_ndi (__a, __b);\n+  return __builtin_aarch64_srshr_ndi (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vrshrd_n_u64 (uint64x1_t __a, const int __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vrshrd_n_u64 (uint64_t __a, const int __b)\n {\n   return __builtin_aarch64_urshr_ndi_uus (__a, __b);\n }\n@@ -21349,7 +21363,7 @@ vrsra_n_s32 (int32x2_t __a, int32x2_t __b, const int __c)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vrsra_n_s64 (int64x1_t __a, int64x1_t __b, const int __c)\n {\n-  return (int64x1_t) __builtin_aarch64_srsra_ndi (__a, __b, __c);\n+  return (int64x1_t) {__builtin_aarch64_srsra_ndi (__a[0], __b[0], __c)};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -21373,7 +21387,7 @@ vrsra_n_u32 (uint32x2_t __a, uint32x2_t __b, const int __c)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vrsra_n_u64 (uint64x1_t __a, uint64x1_t __b, const int __c)\n {\n-  return __builtin_aarch64_ursra_ndi_uuus (__a, __b, __c);\n+  return (uint64x1_t) {__builtin_aarch64_ursra_ndi_uuus (__a[0], __b[0], __c)};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -21424,14 +21438,14 @@ vrsraq_n_u64 (uint64x2_t __a, uint64x2_t __b, const int __c)\n   return __builtin_aarch64_ursra_nv2di_uuus (__a, __b, __c);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vrsrad_n_s64 (int64x1_t __a, int64x1_t __b, const int __c)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vrsrad_n_s64 (int64_t __a, int64_t __b, const int __c)\n {\n-  return (int64x1_t) __builtin_aarch64_srsra_ndi (__a, __b, __c);\n+  return __builtin_aarch64_srsra_ndi (__a, __b, __c);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vrsrad_n_u64 (uint64x1_t __a, uint64x1_t __b, const int __c)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vrsrad_n_u64 (uint64_t __a, uint64_t __b, const int __c)\n {\n   return __builtin_aarch64_ursra_ndi_uuus (__a, __b, __c);\n }\n@@ -21536,7 +21550,7 @@ vshl_n_s32 (int32x2_t __a, const int __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vshl_n_s64 (int64x1_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_ashldi (__a, __b);\n+  return (int64x1_t) {__builtin_aarch64_ashldi (__a[0], __b)};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -21560,7 +21574,7 @@ vshl_n_u32 (uint32x2_t __a, const int __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vshl_n_u64 (uint64x1_t __a, const int __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_ashldi ((int64x1_t) __a, __b);\n+  return (uint64x1_t) {__builtin_aarch64_ashldi ((int64_t) __a[0], __b)};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -21611,16 +21625,16 @@ vshlq_n_u64 (uint64x2_t __a, const int __b)\n   return (uint64x2_t) __builtin_aarch64_ashlv2di ((int64x2_t) __a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vshld_n_s64 (int64x1_t __a, const int __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vshld_n_s64 (int64_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_ashldi (__a, __b);\n+  return __builtin_aarch64_ashldi (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vshld_n_u64 (uint64x1_t __a, const int __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vshld_n_u64 (uint64_t __a, const int __b)\n {\n-  return (uint64x1_t) __builtin_aarch64_ashldi (__a, __b);\n+  return (uint64_t) __builtin_aarch64_ashldi (__a, __b);\n }\n \n __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n@@ -21644,7 +21658,7 @@ vshl_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vshl_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return __builtin_aarch64_sshldi (__a, __b);\n+  return (int64x1_t) {__builtin_aarch64_sshldi (__a[0], __b[0])};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -21668,7 +21682,7 @@ vshl_u32 (uint32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vshl_u64 (uint64x1_t __a, int64x1_t __b)\n {\n-  return __builtin_aarch64_ushldi_uus (__a, __b);\n+  return (uint64x1_t) {__builtin_aarch64_ushldi_uus (__a[0], __b[0])};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -21719,14 +21733,14 @@ vshlq_u64 (uint64x2_t __a, int64x2_t __b)\n   return __builtin_aarch64_ushlv2di_uus (__a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vshld_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vshld_s64 (int64_t __a, int64_t __b)\n {\n   return __builtin_aarch64_sshldi (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vshld_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vshld_u64 (uint64_t __a, uint64_t __b)\n {\n   return __builtin_aarch64_ushldi_uus (__a, __b);\n }\n@@ -21826,7 +21840,7 @@ vshr_n_s32 (int32x2_t __a, const int __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vshr_n_s64 (int64x1_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_ashr_simddi (__a, __b);\n+  return (int64x1_t) {__builtin_aarch64_ashr_simddi (__a[0], __b)};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -21850,7 +21864,7 @@ vshr_n_u32 (uint32x2_t __a, const int __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vshr_n_u64 (uint64x1_t __a, const int __b)\n {\n-  return __builtin_aarch64_lshr_simddi_uus ( __a, __b);\n+  return (uint64x1_t) {__builtin_aarch64_lshr_simddi_uus ( __a[0], __b)};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -21901,10 +21915,10 @@ vshrq_n_u64 (uint64x2_t __a, const int __b)\n   return (uint64x2_t) __builtin_aarch64_lshrv2di ((int64x2_t) __a, __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vshrd_n_s64 (int64x1_t __a, const int __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vshrd_n_s64 (int64_t __a, const int __b)\n {\n-  return (int64x1_t) __builtin_aarch64_ashr_simddi (__a, __b);\n+  return __builtin_aarch64_ashr_simddi (__a, __b);\n }\n \n __extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n@@ -21936,7 +21950,7 @@ vsli_n_s32 (int32x2_t __a, int32x2_t __b, const int __c)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vsli_n_s64 (int64x1_t __a, int64x1_t __b, const int __c)\n {\n-  return (int64x1_t) __builtin_aarch64_ssli_ndi (__a, __b, __c);\n+  return (int64x1_t) {__builtin_aarch64_ssli_ndi (__a[0], __b[0], __c)};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -21960,7 +21974,7 @@ vsli_n_u32 (uint32x2_t __a, uint32x2_t __b, const int __c)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vsli_n_u64 (uint64x1_t __a, uint64x1_t __b, const int __c)\n {\n-  return __builtin_aarch64_usli_ndi_uuus (__a, __b, __c);\n+  return (uint64x1_t) {__builtin_aarch64_usli_ndi_uuus (__a[0], __b[0], __c)};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -22011,14 +22025,14 @@ vsliq_n_u64 (uint64x2_t __a, uint64x2_t __b, const int __c)\n   return __builtin_aarch64_usli_nv2di_uuus (__a, __b, __c);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vslid_n_s64 (int64x1_t __a, int64x1_t __b, const int __c)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vslid_n_s64 (int64_t __a, int64_t __b, const int __c)\n {\n-  return (int64x1_t) __builtin_aarch64_ssli_ndi (__a, __b, __c);\n+  return __builtin_aarch64_ssli_ndi (__a, __b, __c);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vslid_n_u64 (uint64x1_t __a, uint64x1_t __b, const int __c)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vslid_n_u64 (uint64_t __a, uint64_t __b, const int __c)\n {\n   return __builtin_aarch64_usli_ndi_uuus (__a, __b, __c);\n }\n@@ -22046,7 +22060,7 @@ vsqadd_u32 (uint32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vsqadd_u64 (uint64x1_t __a, int64x1_t __b)\n {\n-  return __builtin_aarch64_usqadddi_uus (__a, __b);\n+  return (uint64x1_t) {__builtin_aarch64_usqadddi_uus (__a[0], __b[0])};\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n@@ -22091,8 +22105,8 @@ vsqadds_u32 (uint32x1_t __a, int32x1_t __b)\n   return __builtin_aarch64_usqaddsi_uus (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vsqaddd_u64 (uint64x1_t __a, int64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vsqaddd_u64 (uint64_t __a, int64_t __b)\n {\n   return __builtin_aarch64_usqadddi_uus (__a, __b);\n }\n@@ -22139,7 +22153,7 @@ vsra_n_s32 (int32x2_t __a, int32x2_t __b, const int __c)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vsra_n_s64 (int64x1_t __a, int64x1_t __b, const int __c)\n {\n-  return (int64x1_t) __builtin_aarch64_ssra_ndi (__a, __b, __c);\n+  return (int64x1_t) {__builtin_aarch64_ssra_ndi (__a[0], __b[0], __c)};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -22163,7 +22177,7 @@ vsra_n_u32 (uint32x2_t __a, uint32x2_t __b, const int __c)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vsra_n_u64 (uint64x1_t __a, uint64x1_t __b, const int __c)\n {\n-  return __builtin_aarch64_usra_ndi_uuus (__a, __b, __c);\n+  return (uint64x1_t) {__builtin_aarch64_usra_ndi_uuus (__a[0], __b[0], __c)};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -22214,14 +22228,14 @@ vsraq_n_u64 (uint64x2_t __a, uint64x2_t __b, const int __c)\n   return __builtin_aarch64_usra_nv2di_uuus (__a, __b, __c);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vsrad_n_s64 (int64x1_t __a, int64x1_t __b, const int __c)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vsrad_n_s64 (int64_t __a, int64_t __b, const int __c)\n {\n-  return (int64x1_t) __builtin_aarch64_ssra_ndi (__a, __b, __c);\n+  return __builtin_aarch64_ssra_ndi (__a, __b, __c);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vsrad_n_u64 (uint64x1_t __a, uint64x1_t __b, const int __c)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vsrad_n_u64 (uint64_t __a, uint64_t __b, const int __c)\n {\n   return __builtin_aarch64_usra_ndi_uuus (__a, __b, __c);\n }\n@@ -22249,7 +22263,7 @@ vsri_n_s32 (int32x2_t __a, int32x2_t __b, const int __c)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vsri_n_s64 (int64x1_t __a, int64x1_t __b, const int __c)\n {\n-  return (int64x1_t) __builtin_aarch64_ssri_ndi (__a, __b, __c);\n+  return (int64x1_t) {__builtin_aarch64_ssri_ndi (__a[0], __b[0], __c)};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -22273,7 +22287,7 @@ vsri_n_u32 (uint32x2_t __a, uint32x2_t __b, const int __c)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vsri_n_u64 (uint64x1_t __a, uint64x1_t __b, const int __c)\n {\n-  return __builtin_aarch64_usri_ndi_uuus (__a, __b, __c);\n+  return (uint64x1_t) {__builtin_aarch64_usri_ndi_uuus (__a[0], __b[0], __c)};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -22324,14 +22338,14 @@ vsriq_n_u64 (uint64x2_t __a, uint64x2_t __b, const int __c)\n   return __builtin_aarch64_usri_nv2di_uuus (__a, __b, __c);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vsrid_n_s64 (int64x1_t __a, int64x1_t __b, const int __c)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vsrid_n_s64 (int64_t __a, int64_t __b, const int __c)\n {\n-  return (int64x1_t) __builtin_aarch64_ssri_ndi (__a, __b, __c);\n+  return __builtin_aarch64_ssri_ndi (__a, __b, __c);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vsrid_n_u64 (uint64x1_t __a, uint64x1_t __b, const int __c)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vsrid_n_u64 (uint64_t __a, uint64_t __b, const int __c)\n {\n   return __builtin_aarch64_usri_ndi_uuus (__a, __b, __c);\n }\n@@ -22385,7 +22399,7 @@ vst1_s32 (int32_t *a, int32x2_t b)\n __extension__ static __inline void __attribute__ ((__always_inline__))\n vst1_s64 (int64_t *a, int64x1_t b)\n {\n-  *a = b;\n+  *a = b[0];\n }\n \n __extension__ static __inline void __attribute__ ((__always_inline__))\n@@ -22412,7 +22426,7 @@ vst1_u32 (uint32_t *a, uint32x2_t b)\n __extension__ static __inline void __attribute__ ((__always_inline__))\n vst1_u64 (uint64_t *a, uint64x1_t b)\n {\n-  *a = b;\n+  *a = b[0];\n }\n \n __extension__ static __inline void __attribute__ ((__always_inline__))\n@@ -23363,14 +23377,14 @@ vst4q_f64 (float64_t * __a, float64x2x4_t val)\n \n /* vsub */\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vsubd_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vsubd_s64 (int64_t __a, int64_t __b)\n {\n   return __a - __b;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vsubd_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vsubd_u64 (uint64_t __a, uint64_t __b)\n {\n   return __a - __b;\n }\n@@ -24000,7 +24014,7 @@ vtst_s32 (int32x2_t __a, int32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vtst_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return (__a & __b) ? -1ll : 0ll;\n+  return (uint64x1_t) {(__a[0] & __b[0]) ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n@@ -24027,7 +24041,7 @@ vtst_u32 (uint32x2_t __a, uint32x2_t __b)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vtst_u64 (uint64x1_t __a, uint64x1_t __b)\n {\n-  return (__a & __b) ? -1ll : 0ll;\n+  return (uint64x1_t) {(__a[0] & __b[0]) ? -1ll : 0ll};\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n@@ -24082,14 +24096,14 @@ vtstq_u64 (uint64x2_t __a, uint64x2_t __b)\n \t\t\t\t\t\t  (int64x2_t) __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vtstd_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vtstd_s64 (int64_t __a, int64_t __b)\n {\n   return (__a & __b) ? -1ll : 0ll;\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vtstd_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vtstd_u64 (uint64_t __a, uint64_t __b)\n {\n   return (__a & __b) ? -1ll : 0ll;\n }\n@@ -24117,7 +24131,7 @@ vuqadd_s32 (int32x2_t __a, uint32x2_t __b)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vuqadd_s64 (int64x1_t __a, uint64x1_t __b)\n {\n-  return __builtin_aarch64_suqadddi_ssu (__a,  __b);\n+  return (int64x1_t) {__builtin_aarch64_suqadddi_ssu (__a[0], __b[0])};\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n@@ -24162,8 +24176,8 @@ vuqadds_s32 (int32x1_t __a, uint32x1_t __b)\n   return __builtin_aarch64_suqaddsi_ssu (__a,  __b);\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vuqaddd_s64 (int64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vuqaddd_s64 (int64_t __a, uint64_t __b)\n {\n   return __builtin_aarch64_suqadddi_ssu (__a,  __b);\n }"}, {"sha": "29aa16cae7bba6bb075914d5203061ea911389e3", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 47, "deletions": 0, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -1,3 +1,50 @@\n+2014-06-19  Alan Lawrence  <alan.lawrence@arm.com>\n+\n+\t* g++.dg/abi/mangle-neon-aarch64.C (f22, f23): New tests of \n+\t[u]int64x1_t.\n+\n+\t* gcc.target/aarch64/aapcs64/func-ret-64x1_1.c: Add {u,}int64x1 cases.\n+\t* gcc.target/aarch64/aapcs64/test_64x1_1.c: Likewise.\n+\n+\t* gcc.target/aarch64/scalar_intrinsics.c (test_vaddd_u64,\n+\ttest_vaddd_s64, test_vceqd_s64, test_vceqzd_s64, test_vcged_s64,\n+\ttest_vcled_s64, test_vcgezd_s64, test_vcged_u64, test_vcgtd_s64,\n+\ttest_vcltd_s64, test_vcgtzd_s64, test_vcgtd_u64, test_vclezd_s64,\n+\ttest_vcltzd_s64, test_vqaddd_u64, test_vqaddd_s64, test_vqdmlals_s32,\n+\ttest_vqdmlsls_s32, test_vqdmulls_s32, test_vuqaddd_s64,\n+\ttest_vsqaddd_u64, test_vqmovund_s64, test_vqmovnd_s64,\n+\ttest_vqmovnd_u64, test_vsubd_u64, test_vsubd_s64, test_vqsubd_u64,\n+\ttest_vqsubd_s64, test_vshld_s64, test_vshld_u64, test_vrshld_s64,\n+\ttest_vrshld_u64, test_vshrd_n_s64, test_vshrd_n_u64, test_vsrad_n_s64,\n+\ttest_vsrad_n_u64, test_vrshrd_n_s64, test_vrshrd_n_u64,\n+\ttest_vrsrad_n_s64, test_vrsrad_n_u64, test_vqrshld_s64,\n+\ttest_vqrshld_u64, test_vqshlud_n_s64, test_vqshld_s64, test_vqshld_u64,\n+\ttest_vqshld_n_u64, test_vqshrund_n_s64, test_vqrshrund_n_s64,\n+\ttest_vqshrnd_n_s64, test_vqshrnd_n_u64, test_vqrshrnd_n_s64,\n+\ttest_vqrshrnd_n_u64, test_vshld_n_s64, test_vshdl_n_u64,\n+\ttest_vslid_n_s64, test_vslid_n_u64, test_vsrid_n_s64,\n+\ttest_vsrid_n_u64): Fix signature to match intrinsic.\n+\t\n+\t(test_vabs_s64): Remove.\n+\t(test_vaddd_s64_2, test_vsubd_s64_2): Use force_simd.\n+\n+\t(test_vdupd_lane_s64): Rename to...\n+\t(test_vdupd_laneq_s64): ...and remove a call to force_simd.\n+\n+\t(test_vdupd_lane_u64): Rename to...\n+\t(test_vdupd_laneq_u64): ...and remove a call to force_simd.\n+\n+\t(test_vtst_s64): Rename to...\n+\t(test_vtstd_s64): ...and change int64x1_t to int64_t.\n+\n+\t(test_vtst_u64): Rename to...\n+\t(test_vtstd_u64): ...and change uint64x1_t to uint64_t.\n+\n+\t* gcc.target/aarch64/singleton_intrinsics_1.c: New file.\n+\t* gcc.target/aarch64/vdup_lane_1.c, gcc.target/aarch64/vdup_lane_2.c:\n+\tRemove out-of-bounds tests.\n+\t* gcc.target/aarch64/vneg_s.c (INDEX*, RUN_TEST): Remove INDEX macro.\n+\n 2014-06-19  Alan Lawrence  <alan.lawrence@arm.com>\n \n \t* g++.dg/abi/mangle-neon-aarch64.C: Also test mangling of float64x1_t."}, {"sha": "09a20dc985ef04314e3435b5eb899035429400c4", "filename": "gcc/testsuite/g++.dg/abi/mangle-neon-aarch64.C", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fg%2B%2B.dg%2Fabi%2Fmangle-neon-aarch64.C", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fg%2B%2B.dg%2Fabi%2Fmangle-neon-aarch64.C", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fg%2B%2B.dg%2Fabi%2Fmangle-neon-aarch64.C?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -8,9 +8,11 @@\n void f0 (int8x8_t a) {}\n void f1 (int16x4_t a) {}\n void f2 (int32x2_t a) {}\n+void f22 (int64x1_t a) {}\n void f3 (uint8x8_t a) {}\n void f4 (uint16x4_t a) {}\n void f5 (uint32x2_t a) {}\n+void f23 (uint64x1_t a) {}\n void f6 (float32x2_t a) {}\n void f7 (poly8x8_t a) {}\n void f8 (poly16x4_t a) {}\n@@ -35,9 +37,11 @@ void g1 (int8x16_t, int8x16_t) {}\n // { dg-final { scan-assembler \"_Z2f010__Int8x8_t:\" } }\n // { dg-final { scan-assembler \"_Z2f111__Int16x4_t:\" } }\n // { dg-final { scan-assembler \"_Z2f211__Int32x2_t:\" } }\n+// { dg-final { scan-assembler \"_Z3f2211__Int64x1_t:\" } }\n // { dg-final { scan-assembler \"_Z2f311__Uint8x8_t:\" } }\n // { dg-final { scan-assembler \"_Z2f412__Uint16x4_t:\" } }\n // { dg-final { scan-assembler \"_Z2f512__Uint32x2_t:\" } }\n+// { dg-final { scan-assembler \"_Z3f2312__Uint64x1_t:\" } }\n // { dg-final { scan-assembler \"_Z2f613__Float32x2_t:\" } }\n // { dg-final { scan-assembler \"_Z2f711__Poly8x8_t:\" } }\n // { dg-final { scan-assembler \"_Z2f812__Poly16x4_t:\" } }"}, {"sha": "05957e2dcae1d830a404814062b993fad7030712", "filename": "gcc/testsuite/gcc.target/aarch64/aapcs64/func-ret-64x1_1.c", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Faapcs64%2Ffunc-ret-64x1_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Faapcs64%2Ffunc-ret-64x1_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Faapcs64%2Ffunc-ret-64x1_1.c?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -11,5 +11,7 @@\n #include \"abitest-2.h\"\n #else\n FUNC_VAL_CHECK ( 0, float64x1_t, (float64x1_t) {123456.789}, D0, flat)\n+FUNC_VAL_CHECK ( 1, int64x1_t, (int64x1_t) {0xdeadbeefcafebabeLL}, D0, flat)\n+FUNC_VAL_CHECK ( 2, uint64x1_t, (uint64x1_t) {0xaaaabbbbccccddddULL}, D0, flat)\n #endif\n "}, {"sha": "b5281d5a545b877b6831bed396bdd502486ce389", "filename": "gcc/testsuite/gcc.target/aarch64/aapcs64/test_64x1_1.c", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Faapcs64%2Ftest_64x1_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Faapcs64%2Ftest_64x1_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Faapcs64%2Ftest_64x1_1.c?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -12,5 +12,9 @@\n #else\n ARG (float64x1_t, (float64x1_t) {123456.789}, D0)\n ARG (float64_t, 987654.321, D1)\n-LAST_ARG (float64x1_t, (float64x1_t) {13579.2468}, D2)\n+ARG (float64x1_t, (float64x1_t) {13579.2468}, D2)\n+ARG (int64x1_t, (int64x1_t) {0xcafebabe0cabfaffLL}, D3)\n+ARG (uint64_t, 0xdeadbeefdeafbeeb, X0)\n+ARG (int64_t, 0x0123456789abcdef, X1)\n+LAST_ARG (uint64x1_t, (uint64x1_t) {0xaaaabbbbccccddddULL}, D4)\n #endif"}, {"sha": "624348eb449a7f197800f29a8229250f23e71379", "filename": "gcc/testsuite/gcc.target/aarch64/scalar_intrinsics.c", "status": "modified", "additions": 148, "deletions": 156, "changes": 304, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fscalar_intrinsics.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fscalar_intrinsics.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fscalar_intrinsics.c?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -11,45 +11,37 @@\n \n /* { dg-final { scan-assembler-times \"\\\\tadd\\\\tx\\[0-9\\]+\" 2 } } */\n \n-uint64x1_t\n-test_vaddd_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vaddd_u64 (uint64_t a, uint64_t b)\n {\n   return vaddd_u64 (a, b);\n }\n \n-int64x1_t\n-test_vaddd_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vaddd_s64 (int64_t a, int64_t b)\n {\n   return vaddd_s64 (a, b);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tadd\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vaddd_s64_2 (int64x1_t a, int64x1_t b, int64x1_t c, int64x1_t d)\n-{\n-  return vqaddd_s64 (vaddd_s64 (vqaddd_s64 (a, b), vqaddd_s64 (c, d)),\n-\t\t     vqaddd_s64 (a, d));\n-}\n-\n-/* { dg-final { scan-assembler-times \"\\\\tabs\\\\td\\[0-9\\]+, d\\[0-9\\]+\" 1 } } */\n-\n-int64x1_t\n-test_vabs_s64 (int64x1_t a)\n+int64_t\n+test_vaddd_s64_2 (int64_t a, int64_t b)\n {\n-  uint64x1_t res;\n+  int64_t res;\n   force_simd (a);\n-  res = vabs_s64 (a);\n+  force_simd (b);\n+  res = vaddd_s64 (a, b);\n   force_simd (res);\n   return res;\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tcmeq\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vceqd_s64 (int64x1_t a, int64x1_t b)\n+uint64_t\n+test_vceqd_s64 (int64_t a, int64_t b)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   force_simd (b);\n   res = vceqd_s64 (a, b);\n@@ -59,10 +51,10 @@ test_vceqd_s64 (int64x1_t a, int64x1_t b)\n \n /* { dg-final { scan-assembler-times \"\\\\tcmeq\\\\td\\[0-9\\]+, d\\[0-9\\]+, #?0\" 1 } } */\n \n-uint64x1_t\n-test_vceqzd_s64 (int64x1_t a)\n+uint64_t\n+test_vceqzd_s64 (int64_t a)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   res = vceqzd_s64 (a);\n   force_simd (res);\n@@ -71,21 +63,21 @@ test_vceqzd_s64 (int64x1_t a)\n \n /* { dg-final { scan-assembler-times \"\\\\tcmge\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 2 } } */\n \n-uint64x1_t\n-test_vcged_s64 (int64x1_t a, int64x1_t b)\n+uint64_t\n+test_vcged_s64 (int64_t a, int64_t b)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   force_simd (b);\n   res = vcged_s64 (a, b);\n   force_simd (res);\n   return res;\n }\n \n-uint64x1_t\n-test_vcled_s64 (int64x1_t a, int64x1_t b)\n+uint64_t\n+test_vcled_s64 (int64_t a, int64_t b)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   force_simd (b);\n   res = vcled_s64 (a, b);\n@@ -96,10 +88,10 @@ test_vcled_s64 (int64x1_t a, int64x1_t b)\n /* Idiom recognition will cause this testcase not to generate\n    the expected cmge instruction, so do not check for it.  */\n \n-uint64x1_t\n-test_vcgezd_s64 (int64x1_t a)\n+uint64_t\n+test_vcgezd_s64 (int64_t a)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   res = vcgezd_s64 (a);\n   force_simd (res);\n@@ -108,10 +100,10 @@ test_vcgezd_s64 (int64x1_t a)\n \n /* { dg-final { scan-assembler-times \"\\\\tcmhs\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vcged_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vcged_u64 (uint64_t a, uint64_t b)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   force_simd (b);\n   res = vcged_u64 (a, b);\n@@ -121,21 +113,21 @@ test_vcged_u64 (uint64x1_t a, uint64x1_t b)\n \n /* { dg-final { scan-assembler-times \"\\\\tcmgt\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 2 } } */\n \n-uint64x1_t\n-test_vcgtd_s64 (int64x1_t a, int64x1_t b)\n+uint64_t\n+test_vcgtd_s64 (int64_t a, int64_t b)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   force_simd (b);\n   res = vcgtd_s64 (a, b);\n   force_simd (res);\n   return res;\n }\n \n-uint64x1_t\n-test_vcltd_s64 (int64x1_t a, int64x1_t b)\n+uint64_t\n+test_vcltd_s64 (int64_t a, int64_t b)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   force_simd (b);\n   res = vcltd_s64 (a, b);\n@@ -145,10 +137,10 @@ test_vcltd_s64 (int64x1_t a, int64x1_t b)\n \n /* { dg-final { scan-assembler-times \"\\\\tcmgt\\\\td\\[0-9\\]+, d\\[0-9\\]+, #?0\" 1 } } */\n \n-uint64x1_t\n-test_vcgtzd_s64 (int64x1_t a)\n+uint64_t\n+test_vcgtzd_s64 (int64_t a)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   res = vcgtzd_s64 (a);\n   force_simd (res);\n@@ -157,10 +149,10 @@ test_vcgtzd_s64 (int64x1_t a)\n \n /* { dg-final { scan-assembler-times \"\\\\tcmhi\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vcgtd_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vcgtd_u64 (uint64_t a, uint64_t b)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   force_simd (b);\n   res = vcgtd_u64 (a, b);\n@@ -170,10 +162,10 @@ test_vcgtd_u64 (uint64x1_t a, uint64x1_t b)\n \n /* { dg-final { scan-assembler-times \"\\\\tcmle\\\\td\\[0-9\\]+, d\\[0-9\\]+, #?0\" 1 } } */\n \n-uint64x1_t\n-test_vclezd_s64 (int64x1_t a)\n+uint64_t\n+test_vclezd_s64 (int64_t a)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   res = vclezd_s64 (a);\n   force_simd (res);\n@@ -183,10 +175,10 @@ test_vclezd_s64 (int64x1_t a)\n /* Idiom recognition will cause this testcase not to generate\n    the expected cmlt instruction, so do not check for it.  */\n \n-uint64x1_t\n-test_vcltzd_s64 (int64x1_t a)\n+uint64_t\n+test_vcltzd_s64 (int64_t a)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   res = vcltzd_s64 (a);\n   force_simd (res);\n@@ -261,46 +253,42 @@ test_vdups_lane_u32 (uint32x4_t a)\n \n /* { dg-final { scan-assembler-times \"aarch64_get_lanev2di\" 2 } } */\n \n-int64x1_t\n-test_vdupd_lane_s64 (int64x2_t a)\n+int64_t\n+test_vdupd_laneq_s64 (int64x2_t a)\n {\n-  int64x1_t res;\n-  force_simd (a);\n-  res = vdupd_laneq_s64 (a, 1);\n+  int64_t res = vdupd_laneq_s64 (a, 1);\n   force_simd (res);\n   return res;\n }\n \n-uint64x1_t\n-test_vdupd_lane_u64 (uint64x2_t a)\n+uint64_t\n+test_vdupd_laneq_u64 (uint64x2_t a)\n {\n-  uint64x1_t res;\n-  force_simd (a);\n-  res = vdupd_laneq_u64 (a, 1);\n+  uint64_t res = vdupd_laneq_u64 (a, 1);\n   force_simd (res);\n   return res;\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tcmtst\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 2 } } */\n \n-int64x1_t\n-test_vtst_s64 (int64x1_t a, int64x1_t b)\n+uint64_t\n+test_vtstd_s64 (int64_t a, int64_t b)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   force_simd (b);\n   res = vtstd_s64 (a, b);\n   force_simd (res);\n   return res;\n }\n \n-uint64x1_t\n-test_vtst_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vtstd_u64 (uint64_t a, uint64_t b)\n {\n-  uint64x1_t res;\n+  uint64_t res;\n   force_simd (a);\n   force_simd (b);\n-  res = vtstd_s64 (a, b);\n+  res = vtstd_u64 (a, b);\n   force_simd (res);\n   return res;\n }\n@@ -314,8 +302,8 @@ test_vpaddd_s64 (int64x2_t a)\n \n /* { dg-final { scan-assembler-times \"\\\\tuqadd\\\\td\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vqaddd_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vqaddd_u64 (uint64_t a, uint64_t b)\n {\n   return vqaddd_u64 (a, b);\n }\n@@ -344,10 +332,10 @@ test_vqaddb_u8 (uint8x1_t a, uint8x1_t b)\n   return vqaddb_u8 (a, b);\n }\n \n-/* { dg-final { scan-assembler-times \"\\\\tsqadd\\\\td\\[0-9\\]+\" 5 } } */\n+/* { dg-final { scan-assembler-times \"\\\\tsqadd\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vqaddd_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vqaddd_s64 (int64_t a, int64_t b)\n {\n   return vqaddd_s64 (a, b);\n }\n@@ -394,8 +382,8 @@ test_vqdmlalh_lane_s16 (int32x1_t a, int16x1_t b, int16x4_t c)\n \n /* { dg-final { scan-assembler-times \"\\\\tsqdmlal\\\\td\\[0-9\\]+, s\\[0-9\\]+, s\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vqdmlals_s32 (int64x1_t a, int32x1_t b, int32x1_t c)\n+int64_t\n+test_vqdmlals_s32 (int64_t a, int32x1_t b, int32x1_t c)\n {\n   return vqdmlals_s32 (a, b, c);\n }\n@@ -426,8 +414,8 @@ test_vqdmlslh_lane_s16 (int32x1_t a, int16x1_t b, int16x4_t c)\n \n /* { dg-final { scan-assembler-times \"\\\\tsqdmlsl\\\\td\\[0-9\\]+, s\\[0-9\\]+, s\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vqdmlsls_s32 (int64x1_t a, int32x1_t b, int32x1_t c)\n+int64_t\n+test_vqdmlsls_s32 (int64_t a, int32x1_t b, int32x1_t c)\n {\n   return vqdmlsls_s32 (a, b, c);\n }\n@@ -490,7 +478,7 @@ test_vqdmullh_lane_s16 (int16x1_t a, int16x4_t b)\n \n /* { dg-final { scan-assembler-times \"\\\\tsqdmull\\\\td\\[0-9\\]+, s\\[0-9\\]+, s\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n+int64_t\n test_vqdmulls_s32 (int32x1_t a, int32x1_t b)\n {\n   return vqdmulls_s32 (a, b);\n@@ -562,8 +550,8 @@ test_vuqadds_s32 (int32x1_t a, int8x1_t b)\n \n /* { dg-final { scan-assembler-times \"\\\\tsuqadd\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vuqaddd_s64 (int64x1_t a, int8x1_t b)\n+int64_t\n+test_vuqaddd_s64 (int64_t a, uint64_t b)\n {\n   return vuqaddd_s64 (a, b);\n }\n@@ -594,8 +582,8 @@ test_vsqadds_u32 (uint32x1_t a, int8x1_t b)\n \n /* { dg-final { scan-assembler-times \"\\\\tusqadd\\\\td\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vsqaddd_u64 (uint64x1_t a, int8x1_t b)\n+uint64_t\n+test_vsqaddd_u64 (uint64_t a, int64_t b)\n {\n   return vsqaddd_u64 (a, b);\n }\n@@ -667,7 +655,7 @@ test_vqmovuns_s32 (int32x1_t a)\n /* { dg-final { scan-assembler-times \"\\\\tsqxtun\\\\ts\\[0-9\\]+\" 1 } } */\n \n int32x1_t\n-test_vqmovund_s64 (int64x1_t a)\n+test_vqmovund_s64 (int64_t a)\n {\n   return vqmovund_s64 (a);\n }\n@@ -691,7 +679,7 @@ test_vqmovns_s32 (int32x1_t a)\n /* { dg-final { scan-assembler-times \"\\\\tsqxtn\\\\ts\\[0-9\\]+\" 1 } } */\n \n int32x1_t\n-test_vqmovnd_s64 (int64x1_t a)\n+test_vqmovnd_s64 (int64_t a)\n {\n   return vqmovnd_s64 (a);\n }\n@@ -715,38 +703,42 @@ test_vqmovns_u32 (uint32x1_t a)\n /* { dg-final { scan-assembler-times \"\\\\tuqxtn\\\\ts\\[0-9\\]+\" 1 } } */\n \n uint32x1_t\n-test_vqmovnd_u64 (uint64x1_t a)\n+test_vqmovnd_u64 (uint64_t a)\n {\n   return vqmovnd_u64 (a);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tsub\\\\tx\\[0-9\\]+\" 2 } } */\n \n-uint64x1_t\n-test_vsubd_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vsubd_u64 (uint64_t a, uint64_t b)\n {\n   return vsubd_u64 (a, b);\n }\n \n-int64x1_t\n-test_vsubd_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vsubd_s64 (int64_t a, int64_t b)\n {\n   return vsubd_s64 (a, b);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tsub\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vsubd_s64_2 (int64x1_t a, int64x1_t b, int64x1_t c, int64x1_t d)\n+int64_t\n+test_vsubd_s64_2 (int64_t a, int64_t b)\n {\n-  return vqsubd_s64 (vsubd_s64 (vqsubd_s64 (a, b), vqsubd_s64 (c, d)),\n-\t\t     vqsubd_s64 (a, d));\n+  int64_t res;\n+  force_simd (a);\n+  force_simd (b);\n+  res = vsubd_s64 (a, b);\n+  force_simd (res);\n+  return res;\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tuqsub\\\\td\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vqsubd_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vqsubd_u64 (uint64_t a, uint64_t b)\n {\n   return vqsubd_u64 (a, b);\n }\n@@ -775,10 +767,10 @@ test_vqsubb_u8 (uint8x1_t a, uint8x1_t b)\n   return vqsubb_u8 (a, b);\n }\n \n-/* { dg-final { scan-assembler-times \"\\\\tsqsub\\\\td\\[0-9\\]+\" 5 } } */\n+/* { dg-final { scan-assembler-times \"\\\\tsqsub\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vqsubd_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vqsubd_s64 (int64_t a, int64_t b)\n {\n   return vqsubd_s64 (a, b);\n }\n@@ -809,32 +801,32 @@ test_vqsubb_s8 (int8x1_t a, int8x1_t b)\n \n /* { dg-final { scan-assembler-times \"\\\\tsshl\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vshld_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vshld_s64 (int64_t a, int64_t b)\n {\n   return vshld_s64 (a, b);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tushl\\\\td\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vshld_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vshld_u64 (uint64_t a, uint64_t b)\n {\n   return vshld_u64 (a, b);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tsrshl\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vrshld_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vrshld_s64 (int64_t a, int64_t b)\n {\n   return vrshld_s64 (a, b);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\turshl\\\\td\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vrshld_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vrshld_u64 (uint64_t a, int64_t b)\n {\n   return vrshld_u64 (a, b);\n }\n@@ -844,64 +836,64 @@ test_vrshld_u64 (uint64x1_t a, uint64x1_t b)\n \n /* { dg-final { scan-assembler \"\\\\tasr\\\\tx\\[0-9\\]+\" } } */\n \n-int64x1_t\n-test_vshrd_n_s64 (int64x1_t a)\n+int64_t\n+test_vshrd_n_s64 (int64_t a)\n {\n   return vshrd_n_s64 (a, 5);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tlsr\\\\tx\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vshrd_n_u64 (uint64x1_t a)\n+uint64_t\n+test_vshrd_n_u64 (uint64_t a)\n {\n   return vshrd_n_u64 (a, 3);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tssra\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vsrad_n_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vsrad_n_s64 (int64_t a, int64_t b)\n {\n   return vsrad_n_s64 (a, b, 2);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tusra\\\\td\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vsrad_n_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vsrad_n_u64 (uint64_t a, uint64_t b)\n {\n   return vsrad_n_u64 (a, b, 5);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tsrshr\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vrshrd_n_s64 (int64x1_t a)\n+int64_t\n+test_vrshrd_n_s64 (int64_t a)\n {\n   return vrshrd_n_s64 (a, 5);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\turshr\\\\td\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vrshrd_n_u64 (uint64x1_t a)\n+uint64_t\n+test_vrshrd_n_u64 (uint64_t a)\n {\n   return vrshrd_n_u64 (a, 3);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tsrsra\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vrsrad_n_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vrsrad_n_s64 (int64_t a, int64_t b)\n {\n   return vrsrad_n_s64 (a, b, 3);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tsrsra\\\\td\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vrsrad_n_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vrsrad_n_u64 (uint64_t a, uint64_t b)\n {\n   return vrsrad_n_u64 (a, b, 4);\n }\n@@ -932,8 +924,8 @@ test_vqrshls_s32 (int32x1_t a, int32x1_t b)\n \n /* { dg-final { scan-assembler-times \"\\\\tsqrshl\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vqrshld_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vqrshld_s64 (int64_t a, int64_t b)\n {\n   return vqrshld_s64 (a, b);\n }\n@@ -964,8 +956,8 @@ test_vqrshls_u32 (uint32x1_t a, uint32x1_t b)\n \n /* { dg-final { scan-assembler-times \"\\\\tuqrshl\\\\td\\[0-9\\]+\" 1 } } */\n \n-uint64x1_t\n-test_vqrshld_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vqrshld_u64 (uint64_t a, uint64_t b)\n {\n   return vqrshld_u64 (a, b);\n }\n@@ -996,8 +988,8 @@ test_vqshlus_n_s32 (int32x1_t a)\n \n /* { dg-final { scan-assembler-times \"\\\\tsqshlu\\\\td\\[0-9\\]+\" 1 } } */\n \n-int64x1_t\n-test_vqshlud_n_s64 (int64x1_t a)\n+int64_t\n+test_vqshlud_n_s64 (int64_t a)\n {\n   return vqshlud_n_s64 (a, 6);\n }\n@@ -1046,14 +1038,14 @@ test_vqshls_n_s32 (int32x1_t a)\n \n /* { dg-final { scan-assembler-times \"\\\\tsqshl\\\\td\\[0-9\\]+\" 2 } } */\n \n-int64x1_t\n-test_vqshld_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vqshld_s64 (int64_t a, int64_t b)\n {\n   return vqshld_s64 (a, b);\n }\n \n-int64x1_t\n-test_vqshld_n_s64 (int64x1_t a)\n+int64_t\n+test_vqshld_n_s64 (int64_t a)\n {\n   return vqshld_n_s64 (a, 5);\n }\n@@ -1102,14 +1094,14 @@ test_vqshls_n_u32 (uint32x1_t a)\n \n /* { dg-final { scan-assembler-times \"\\\\tuqshl\\\\td\\[0-9\\]+\" 2 } } */\n \n-uint64x1_t\n-test_vqshld_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vqshld_u64 (uint64_t a, int64_t b)\n {\n   return vqshld_u64 (a, b);\n }\n \n-uint64x1_t\n-test_vqshld_n_u64 (uint64x1_t a)\n+uint64_t\n+test_vqshld_n_u64 (uint64_t a)\n {\n   return vqshld_n_u64 (a, 5);\n }\n@@ -1133,7 +1125,7 @@ test_vqshruns_n_s32 (int32x1_t a)\n /* { dg-final { scan-assembler-times \"\\\\tsqshrun\\\\ts\\[0-9\\]+\" 1 } } */\n \n int32x1_t\n-test_vqshrund_n_s64 (int64x1_t a)\n+test_vqshrund_n_s64 (int64_t a)\n {\n   return vqshrund_n_s64 (a, 4);\n }\n@@ -1157,7 +1149,7 @@ test_vqrshruns_n_s32 (int32x1_t a)\n /* { dg-final { scan-assembler-times \"\\\\tsqrshrun\\\\ts\\[0-9\\]+\" 1 } } */\n \n int32x1_t\n-test_vqrshrund_n_s64 (int64x1_t a)\n+test_vqrshrund_n_s64 (int64_t a)\n {\n   return vqrshrund_n_s64 (a, 4);\n }\n@@ -1181,7 +1173,7 @@ test_vqshrns_n_s32 (int32x1_t a)\n /* { dg-final { scan-assembler-times \"\\\\tsqshrn\\\\ts\\[0-9\\]+\" 1 } } */\n \n int32x1_t\n-test_vqshrnd_n_s64 (int64x1_t a)\n+test_vqshrnd_n_s64 (int64_t a)\n {\n   return vqshrnd_n_s64 (a, 4);\n }\n@@ -1205,7 +1197,7 @@ test_vqshrns_n_u32 (uint32x1_t a)\n /* { dg-final { scan-assembler-times \"\\\\tuqshrn\\\\ts\\[0-9\\]+\" 1 } } */\n \n uint32x1_t\n-test_vqshrnd_n_u64 (uint64x1_t a)\n+test_vqshrnd_n_u64 (uint64_t a)\n {\n   return vqshrnd_n_u64 (a, 4);\n }\n@@ -1229,7 +1221,7 @@ test_vqrshrns_n_s32 (int32x1_t a)\n /* { dg-final { scan-assembler-times \"\\\\tsqrshrn\\\\ts\\[0-9\\]+\" 1 } } */\n \n int32x1_t\n-test_vqrshrnd_n_s64 (int64x1_t a)\n+test_vqrshrnd_n_s64 (int64_t a)\n {\n   return vqrshrnd_n_s64 (a, 4);\n }\n@@ -1253,49 +1245,49 @@ test_vqrshrns_n_u32 (uint32x1_t a)\n /* { dg-final { scan-assembler-times \"\\\\tuqrshrn\\\\ts\\[0-9\\]+\" 1 } } */\n \n uint32x1_t\n-test_vqrshrnd_n_u64 (uint64x1_t a)\n+test_vqrshrnd_n_u64 (uint64_t a)\n {\n   return vqrshrnd_n_u64 (a, 4);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tlsl\\\\tx\\[0-9\\]+\" 2 } } */\n \n-int64x1_t\n-test_vshl_n_s64 (int64x1_t a)\n+int64_t\n+test_vshld_n_s64 (int64_t a)\n {\n   return vshld_n_s64 (a, 9);\n }\n \n-uint64x1_t\n-test_vshl_n_u64 (uint64x1_t a)\n+uint64_t\n+test_vshdl_n_u64 (uint64_t a)\n {\n   return vshld_n_u64 (a, 9);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tsli\\\\td\\[0-9\\]+\" 2 } } */\n \n-int64x1_t\n-test_vsli_n_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vslid_n_s64 (int64_t a, int64_t b)\n {\n   return vslid_n_s64 (a, b, 9);\n }\n \n-uint64x1_t\n-test_vsli_n_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vslid_n_u64 (uint64_t a, uint64_t b)\n {\n   return vslid_n_u64 (a, b, 9);\n }\n \n /* { dg-final { scan-assembler-times \"\\\\tsri\\\\td\\[0-9\\]+\" 2 } } */\n \n-int64x1_t\n-test_vsri_n_s64 (int64x1_t a, int64x1_t b)\n+int64_t\n+test_vsrid_n_s64 (int64_t a, int64_t b)\n {\n   return vsrid_n_s64 (a, b, 9);\n }\n \n-uint64x1_t\n-test_vsri_n_u64 (uint64x1_t a, uint64x1_t b)\n+uint64_t\n+test_vsrid_n_u64 (uint64_t a, uint64_t b)\n {\n   return vsrid_n_u64 (a, b, 9);\n }"}, {"sha": "b879fdacaa6544790e4d3ff98ca0055073d6d1d1", "filename": "gcc/testsuite/gcc.target/aarch64/simd/ext_s64.x", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fext_s64.x", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fext_s64.x", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fext_s64.x?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -9,7 +9,7 @@ main (int argc, char **argv)\n   int64_t arr2[] = {1};\n   int64x1_t in2 = vld1_s64 (arr2);\n   int64x1_t actual = vext_s64 (in1, in2, 0);\n-  if (actual != in1)\n+  if (actual[0] != in1[0])\n     abort ();\n \n   return 0;"}, {"sha": "bd51e27c2156bfcaca6b26798c449369b2894c08", "filename": "gcc/testsuite/gcc.target/aarch64/simd/ext_u64.x", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fext_u64.x", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fext_u64.x", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fext_u64.x?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -9,7 +9,7 @@ main (int argc, char **argv)\n   uint64_t arr2[] = {1};\n   uint64x1_t in2 = vld1_u64 (arr2);\n   uint64x1_t actual = vext_u64 (in1, in2, 0);\n-  if (actual != in1)\n+  if (actual[0] != in1[0])\n     abort ();\n \n   return 0;"}, {"sha": "329af947a46d2276493845bd38cd5c0e1f39e93a", "filename": "gcc/testsuite/gcc.target/aarch64/singleton_intrinsics_1.c", "status": "added", "additions": 402, "deletions": 0, "changes": 402, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsingleton_intrinsics_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsingleton_intrinsics_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsingleton_intrinsics_1.c?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -0,0 +1,402 @@\n+/* { dg-do assemble } */\n+/* { dg-options \"-O2 -dp\" } */\n+\n+/* Test the [u]int64x1_t intrinsics.  */\n+\n+#include <arm_neon.h>\n+\n+/* { dg-final { scan-assembler-times \"\\\\tadd\\\\td\\[0-9\\]+\" 2 } } */\n+\n+uint64x1_t\n+test_vadd_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vadd_u64 (a, b);\n+}\n+\n+int64x1_t\n+test_vadd_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vadd_s64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tabs\\\\td\\[0-9\\]+, d\\[0-9\\]+\" 1 } } */\n+\n+int64x1_t\n+test_vabs_s64 (int64x1_t a)\n+{\n+  return vabs_s64 (a);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tcmeq\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vceq_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vceq_s64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tcmeq\\\\td\\[0-9\\]+, d\\[0-9\\]+, #?0\" 1 } } */\n+\n+uint64x1_t\n+test_vceqz_s64 (int64x1_t a)\n+{\n+  return vceqz_s64 (a);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tcmge\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 2 } } */\n+\n+uint64x1_t\n+test_vcge_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vcge_s64 (a, b);\n+}\n+\n+uint64x1_t\n+test_vcle_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vcle_s64 (a, b);\n+}\n+\n+/* Idiom recognition will cause this testcase not to generate\n+   the expected cmge instruction, so do not check for it.  */\n+\n+uint64x1_t\n+test_vcgez_s64 (int64x1_t a)\n+{\n+  return vcgez_s64 (a);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tcmhs\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vcge_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vcge_u64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tcmgt\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 2 } } */\n+\n+uint64x1_t\n+test_vcgt_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vcgt_s64 (a, b);\n+}\n+\n+uint64x1_t\n+test_vclt_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vclt_s64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tcmgt\\\\td\\[0-9\\]+, d\\[0-9\\]+, #?0\" 1 } } */\n+\n+uint64x1_t\n+test_vcgtz_s64 (int64x1_t a)\n+{\n+  return vcgtz_s64 (a);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tcmhi\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vcgt_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vcgt_u64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tcmle\\\\td\\[0-9\\]+, d\\[0-9\\]+, #?0\" 1 } } */\n+\n+uint64x1_t\n+test_vclez_s64 (int64x1_t a)\n+{\n+  return vclez_s64 (a);\n+}\n+\n+/* Compiling with \"-dp\" outputs the name of each .md pattern into the assembler.\n+   This is what we look for here.  */\n+/* { dg-final { scan-assembler-times \"aarch64_get_lanev2di\" 2 } } */\n+\n+int64x1_t\n+test_vdup_laneq_s64 (int64x2_t a)\n+{\n+  return vdup_laneq_s64 (a, 1);\n+}\n+\n+uint64x1_t\n+test_vdup_laneq_u64 (uint64x2_t a)\n+{\n+  return vdup_laneq_u64 (a, 1);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tcmtst\\\\td\\[0-9\\]+, d\\[0-9\\]+, d\\[0-9\\]+\" 2 } } */\n+\n+uint64x1_t\n+test_vtst_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vtst_s64 (a, b);\n+}\n+\n+uint64x1_t\n+test_vtst_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vtst_u64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tuqadd\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vqadd_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vqadd_u64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsqadd\\\\td\\[0-9\\]+\" 1 } } */\n+\n+int64x1_t\n+test_vqadd_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vqadd_s64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsuqadd\\\\td\\[0-9\\]+\" 1 } } */\n+\n+int64x1_t\n+test_vuqadd_s64 (int64x1_t a, uint64x1_t b)\n+{\n+  return vuqadd_s64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tusqadd\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vsqadd_u64 (uint64x1_t a, int64x1_t b)\n+{\n+  return vsqadd_u64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsub\\\\td\\[0-9\\]+\" 2 } } */\n+\n+uint64x1_t\n+test_vsub_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vsub_u64 (a, b);\n+}\n+\n+int64x1_t\n+test_vsub_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vsub_s64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tuqsub\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vqsub_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vqsub_u64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsqsub\\\\td\\[0-9\\]+\" 1 } } */\n+\n+int64x1_t\n+test_vqsub_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vqsub_s64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsshl\\\\td\\[0-9\\]+\" 1 } } */\n+\n+int64x1_t\n+test_vshl_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vshl_s64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tushl\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vshl_u64 (uint64x1_t a, int64x1_t b)\n+{\n+  return vshl_u64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsrshl\\\\td\\[0-9\\]+\" 1 } } */\n+\n+int64x1_t\n+test_vrshl_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vrshl_s64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\turshl\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vrshl_u64 (uint64x1_t a, int64x1_t b)\n+{\n+  return vrshl_u64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsshr\\\\td\\[0-9\\]+\" 3 } } */\n+/* Idiom recognition compiles vcltz and vcgez to sshr rather than cmlt/cmge.  */\n+\n+int64x1_t\n+test_vshr_n_s64 (int64x1_t a)\n+{\n+  return vshr_n_s64 (a, 5);\n+}\n+\n+uint64x1_t\n+test_vcltz_s64 (int64x1_t a)\n+{\n+  return vcltz_s64 (a);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tushr\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vshr_n_u64 (uint64x1_t a)\n+{\n+  return vshr_n_u64 (a, 3);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tssra\\\\td\\[0-9\\]+\" 1 } } */\n+\n+int64x1_t\n+test_vsra_n_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vsra_n_s64 (a, b, 2);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tusra\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vsra_n_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vsra_n_u64 (a, b, 5);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsrshr\\\\td\\[0-9\\]+\" 1 } } */\n+\n+int64x1_t\n+test_vrshr_n_s64 (int64x1_t a)\n+{\n+  return vrshr_n_s64 (a, 5);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\turshr\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vrshr_n_u64 (uint64x1_t a)\n+{\n+  return vrshr_n_u64 (a, 3);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsrsra\\\\td\\[0-9\\]+\" 1 } } */\n+\n+int64x1_t\n+test_vrsra_n_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vrsra_n_s64 (a, b, 3);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsrsra\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vrsra_n_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vrsra_n_u64 (a, b, 4);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsqrshl\\\\td\\[0-9\\]+\" 1 } } */\n+\n+int64x1_t\n+test_vqrshl_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vqrshl_s64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tuqrshl\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vqrshl_u64 (uint64x1_t a, int64x1_t b)\n+{\n+  return vqrshl_u64 (a, b);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsqshlu\\\\td\\[0-9\\]+\" 1 } } */\n+\n+uint64x1_t\n+test_vqshlu_n_s64 (int64x1_t a)\n+{\n+  return vqshlu_n_s64 (a, 6);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsqshl\\\\td\\[0-9\\]+\" 2 } } */\n+\n+int64x1_t\n+test_vqshl_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vqshl_s64 (a, b);\n+}\n+\n+int64x1_t\n+test_vqshl_n_s64 (int64x1_t a)\n+{\n+  return vqshl_n_s64 (a, 5);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tuqshl\\\\td\\[0-9\\]+\" 2 } } */\n+\n+uint64x1_t\n+test_vqshl_u64 (uint64x1_t a, int64x1_t b)\n+{\n+  return vqshl_u64 (a, b);\n+}\n+\n+uint64x1_t\n+test_vqshl_n_u64 (uint64x1_t a)\n+{\n+  return vqshl_n_u64 (a, 5);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tshl\\\\td\\[0-9\\]+\" 2 } } */\n+\n+int64x1_t\n+test_vshl_n_s64 (int64x1_t a)\n+{\n+  return vshl_n_s64 (a, 9);\n+}\n+\n+uint64x1_t\n+test_vshl_n_u64 (uint64x1_t a)\n+{\n+  return vshl_n_u64 (a, 9);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsli\\\\td\\[0-9\\]+\" 2 } } */\n+\n+int64x1_t\n+test_vsli_n_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vsli_n_s64 (a, b, 9);\n+}\n+\n+uint64x1_t\n+test_vsli_n_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vsli_n_u64 (a, b, 9);\n+}\n+\n+/* { dg-final { scan-assembler-times \"\\\\tsri\\\\td\\[0-9\\]+\" 2 } } */\n+\n+int64x1_t\n+test_vsri_n_s64 (int64x1_t a, int64x1_t b)\n+{\n+  return vsri_n_s64 (a, b, 9);\n+}\n+\n+uint64x1_t\n+test_vsri_n_u64 (uint64x1_t a, uint64x1_t b)\n+{\n+  return vsri_n_u64 (a, b, 9);\n+}"}, {"sha": "b1ddc89bf798990524534ba25ea15daf63159cd8", "filename": "gcc/testsuite/gcc.target/aarch64/vdup_lane_1.c", "status": "modified", "additions": 0, "deletions": 26, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_1.c?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -304,12 +304,6 @@ wrap_vdup_lane_s64_0 (int64x1_t a)\n   return vdup_lane_s64 (a, 0);\n }\n \n-int64x1_t __attribute__ ((noinline))\n-wrap_vdup_lane_s64_1 (int64x1_t a)\n-{\n-  return vdup_lane_s64 (a, 1);\n-}\n-\n int __attribute__ ((noinline))\n test_vdup_lane_s64 ()\n {\n@@ -325,12 +319,6 @@ test_vdup_lane_s64 ()\n   if (c[0] != d[0])\n     return 1;\n \n-  c[0] = 1;\n-  a = vld1_s64 (c);\n-  b = wrap_vdup_lane_s64_1 (a);\n-  vst1_s64 (d, b);\n-  if (c[0] != d[0])\n-    return 1;\n   return 0;\n }\n \n@@ -340,12 +328,6 @@ wrap_vdupq_lane_s64_0 (int64x1_t a)\n   return vdupq_lane_s64 (a, 0);\n }\n \n-int64x2_t __attribute__ ((noinline))\n-wrap_vdupq_lane_s64_1 (int64x1_t a)\n-{\n-  return vdupq_lane_s64 (a, 1);\n-}\n-\n int __attribute__ ((noinline))\n test_vdupq_lane_s64 ()\n {\n@@ -359,14 +341,6 @@ test_vdupq_lane_s64 ()\n   a = vld1_s64 (c);\n   b = wrap_vdupq_lane_s64_0 (a);\n   vst1q_s64 (d, b);\n-  for (i = 0; i < 2; i++)\n-    if (c[0] != d[i])\n-      return 1;\n-\n-  c[0] = 1;\n-  a = vld1_s64 (c);\n-  b = wrap_vdupq_lane_s64_1 (a);\n-  vst1q_s64 (d, b);\n   for (i = 0; i < 2; i++)\n     if (c[0] != d[i])\n       return 1;"}, {"sha": "c4183ce1a321fed892cd67fd002de09697a86ed5", "filename": "gcc/testsuite/gcc.target/aarch64/vdup_lane_2.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_2.c?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -278,9 +278,9 @@ test_vdupd_lane_u64 ()\n }\n \n int64_t __attribute__ ((noinline))\n-wrap_vdupd_lane_s64_0 (uint64x1_t dummy, int64x1_t a)\n+wrap_vdupd_lane_s64_0 (int64x1_t dummy, int64x1_t a)\n {\n-  return vdupd_lane_u64 (a, 0);\n+  return vdupd_lane_s64 (a, 0);\n }\n \n int __attribute__ ((noinline))"}, {"sha": "e818ab9e96ac7c8af2d96d900828fd6d34fa185a", "filename": "gcc/testsuite/gcc.target/aarch64/vneg_s.c", "status": "modified", "additions": 5, "deletions": 13, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvneg_s.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/096c59be14c385b96c186dd1a1b45f95d80e102e/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvneg_s.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvneg_s.c?ref=096c59be14c385b96c186dd1a1b45f95d80e102e", "patch": "@@ -5,7 +5,10 @@\n #include <arm_neon.h>\n #include <limits.h>\n \n-/* Used to force a variable to a SIMD register.  */\n+/* Used to force a variable to a SIMD register.  Also acts as a stronger\n+   inhibitor of optimization than the below - necessary for int64x1_t\n+   because more of the implementation is in terms of gcc vector extensions\n+   (which support constant propagation) than for other types.  */\n #define force_simd(V1)   asm volatile (\"mov %d0, %1.d[0]\"\t\\\n \t   : \"=w\"(V1)\t\t\t\t\t\t\\\n \t   : \"w\"(V1)\t\t\t\t\t\t\\\n@@ -38,14 +41,6 @@ extern void abort (void);\n #define DATA_TYPE_32 float\n #define DATA_TYPE_64 double\n #define DATA_TYPE(data_len) DATA_TYPE_##data_len\n-#define INDEX64_8 [i]\n-#define INDEX64_16 [i]\n-#define INDEX64_32 [i]\n-#define INDEX64_64\n-#define INDEX128_8 [i]\n-#define INDEX128_16 [i]\n-#define INDEX128_32 [i]\n-#define INDEX128_64 [i]\n \n #define FORCE_SIMD_INST64_8(data)\n #define FORCE_SIMD_INST64_16(data)\n@@ -56,8 +51,6 @@ extern void abort (void);\n #define FORCE_SIMD_INST128_32(data)\n #define FORCE_SIMD_INST128_64(data)\n \n-#define INDEX(reg_len, data_len) \\\n-  CONCAT1 (INDEX, reg_len##_##data_len)\n #define FORCE_SIMD_INST(reg_len, data_len, data) \\\n   CONCAT1 (FORCE_SIMD_INST, reg_len##_##data_len) (data)\n #define LOAD_INST(reg_len, data_len) \\\n@@ -77,8 +70,7 @@ extern void abort (void);\n     for (i = 0; i < n; i++)\t\t\t\t\t\t\\\n       {\t\t\t\t\t\t\t\t\t\\\n         INHIB_OPTIMIZATION;\t\t\t\t\t\t\\\n-\tif (a INDEX (reg_len, data_len)\t\t\t\t\t\\\n-\t    != b INDEX (reg_len, data_len))\t\t\t\t\\\n+\tif (a[i] != b[i])\t\t\t\t\t\t\\\n \t  return 1;\t\t\t\t\t\t\t\\\n       }\t\t\t\t\t\t\t\t\t\\\n   }"}]}