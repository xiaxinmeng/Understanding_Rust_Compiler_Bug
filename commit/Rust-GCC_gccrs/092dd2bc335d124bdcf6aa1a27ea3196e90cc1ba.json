{"sha": "092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDkyZGQyYmMzMzVkMTI0YmRjZjZhYTFhMjdlYTMxOTZlOTBjYzFiYQ==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-11-18T17:48:29Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-11-18T17:48:29Z"}, "message": "runtime: move schedt type and sched var from C to Go\n    \n    This doesn't change any actual code, it just starts using the Go\n    definition of the schedt type and the sched variable rather than the C\n    definitions.\n    \n    The schedt type is tweaked slightly for gccgo.  We aren't going to\n    release goroutine stacks, so we don't need separate gfreeStack and\n    gfreeNostack lists.  We only have one size of defer function, so we\n    don't need a list of 5 different pools.\n    \n    Reviewed-on: https://go-review.googlesource.com/33364\n\nFrom-SVN: r242600", "tree": {"sha": "fb1782f935ac669d4b7a6987b556622df8aef1a6", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/fb1782f935ac669d4b7a6987b556622df8aef1a6"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba", "html_url": "https://github.com/Rust-GCC/gccrs/commit/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba/comments", "author": null, "committer": null, "parents": [{"sha": "70e73d3c4277fa602999b3e3c104a06024932c70", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/70e73d3c4277fa602999b3e3c104a06024932c70", "html_url": "https://github.com/Rust-GCC/gccrs/commit/70e73d3c4277fa602999b3e3c104a06024932c70"}], "stats": {"total": 512, "additions": 243, "deletions": 269}, "files": [{"sha": "f0cbae6c3c2d66e159754d490c05e7c929a777f8", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba", "patch": "@@ -1,4 +1,4 @@\n-fc4ca600b2fc6de81fd3c4014542d6a50593db1a\n+bf4762823c4543229867436399be3ae30b4d13bb\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "7a0cc4316e4358027dc3607ed4305f7515eab89d", "filename": "libgo/go/runtime/runtime2.go", "status": "modified", "additions": 9, "deletions": 12, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba/libgo%2Fgo%2Fruntime%2Fruntime2.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba/libgo%2Fgo%2Fruntime%2Fruntime2.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime2.go?ref=092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba", "patch": "@@ -550,9 +550,6 @@ const (\n \t_MaxGomaxprocs = 1 << 8\n )\n \n-/*\n-Commented out for gccgo for now.\n-\n type schedt struct {\n \t// accessed atomically. keep at top to ensure alignment on 32-bit systems.\n \tgoidgen  uint64\n@@ -578,18 +575,17 @@ type schedt struct {\n \trunqsize int32\n \n \t// Global cache of dead G's.\n-\tgflock       mutex\n-\tgfreeStack   *g\n-\tgfreeNoStack *g\n-\tngfree       int32\n+\tgflock mutex\n+\tgfree  *g\n+\tngfree int32\n \n \t// Central cache of sudog structs.\n \tsudoglock  mutex\n \tsudogcache *sudog\n \n-\t// Central pool of available defer structs of different sizes.\n+\t// Central pool of available defer structs.\n \tdeferlock mutex\n-\tdeferpool [5]*_defer\n+\tdeferpool *_defer\n \n \tgcwaiting  uint32 // gc is waiting to run\n \tstopwait   int32\n@@ -608,7 +604,6 @@ type schedt struct {\n \tprocresizetime int64 // nanotime() of last change to gomaxprocs\n \ttotaltime      int64 // \u222bgomaxprocs dt up to procresizetime\n }\n-*/\n \n // The m.locked word holds two pieces of state counting active calls to LockOSThread/lockOSThread.\n // The low bit (LockExternal) is a boolean reporting whether any LockOSThread call is active.\n@@ -772,8 +767,10 @@ var (\n \n \tncpu int32\n \n-//\tforcegc     forcegcstate\n-//\tsched       schedt\n+\t//\tforcegc     forcegcstate\n+\n+\tsched schedt\n+\n //\tnewprocs    int32\n \n // Information about what cpu features are available."}, {"sha": "b01bc7c315315f3b5d483f131e3d17b924cd6d65", "filename": "libgo/go/runtime/stubs.go", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba/libgo%2Fgo%2Fruntime%2Fstubs.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba/libgo%2Fgo%2Fruntime%2Fstubs.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstubs.go?ref=092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba", "patch": "@@ -520,3 +520,9 @@ func dumpregs(*_siginfo_t, unsafe.Pointer)\n \n // Temporary for gccgo until we port panic.go.\n func startpanic()\n+\n+// Temporary for gccgo until we port proc.go.\n+//go:linkname getsched runtime.getsched\n+func getsched() *schedt {\n+\treturn &sched\n+}"}, {"sha": "d8834ad485f98db33d372f0a6e26350ff43421b1", "filename": "libgo/runtime/proc.c", "status": "modified", "additions": 227, "deletions": 256, "changes": 483, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba/libgo%2Fruntime%2Fproc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba/libgo%2Fruntime%2Fproc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fproc.c?ref=092dd2bc335d124bdcf6aa1a27ea3196e90cc1ba", "patch": "@@ -351,48 +351,18 @@ runtime_mcall(void (*pfn)(G*))\n //\n // Design doc at http://golang.org/s/go11sched.\n \n-typedef struct Sched Sched;\n-struct Sched {\n-\tLock;\n-\n-\tuint64\tgoidgen;\n-\tM*\tmidle;\t // idle m's waiting for work\n-\tint32\tnmidle;\t // number of idle m's waiting for work\n-\tint32\tnmidlelocked; // number of locked m's waiting for work\n-\tint32\tmcount;\t // number of m's that have been created\n-\tint32\tmaxmcount;\t// maximum number of m's allowed (or die)\n-\n-\tP*\tpidle;  // idle P's\n-\tuint32\tnpidle;\n-\tuint32\tnmspinning;\n-\n-\t// Global runnable queue.\n-\tG*\trunqhead;\n-\tG*\trunqtail;\n-\tint32\trunqsize;\n-\n-\t// Global cache of dead G's.\n-\tLock\tgflock;\n-\tG*\tgfree;\n-\n-\tuint32\tgcwaiting;\t// gc is waiting to run\n-\tint32\tstopwait;\n-\tNote\tstopnote;\n-\tuint32\tsysmonwait;\n-\tNote\tsysmonnote;\n-\tuint64\tlastpoll;\n-\n-\tint32\tprofilehz;\t// cpu profiling rate\n-};\n+typedef struct schedt Sched;\n \n enum\n {\n-\t// Number of goroutine ids to grab from runtime_sched.goidgen to local per-P cache at once.\n+\t// Number of goroutine ids to grab from runtime_sched->goidgen to local per-P cache at once.\n \t// 16 seems to provide enough amortization, but other than that it's mostly arbitrary number.\n \tGoidCacheBatch = 16,\n };\n \n-Sched\truntime_sched;\n+extern Sched* runtime_getsched() __asm__ (GOSYM_PREFIX \"runtime.getsched\");\n+\n+static Sched*\truntime_sched;\n int32\truntime_gomaxprocs;\n uint32\truntime_needextram = 1;\n M\truntime_m0;\n@@ -471,6 +441,8 @@ runtime_schedinit(void)\n \tconst byte *p;\n \tEface i;\n \n+\truntime_sched = runtime_getsched();\n+\n \tm = &runtime_m0;\n \tg = &runtime_g0;\n \tm->g0 = g;\n@@ -479,7 +451,7 @@ runtime_schedinit(void)\n \n \tinitcontext();\n \n-\truntime_sched.maxmcount = 10000;\n+\truntime_sched->maxmcount = 10000;\n \truntime_precisestack = 0;\n \n \t// runtime_symtabinit();\n@@ -500,7 +472,7 @@ runtime_schedinit(void)\n \truntime_goenvs();\n \truntime_parsedebugvars();\n \n-\truntime_sched.lastpoll = runtime_nanotime();\n+\truntime_sched->lastpoll = runtime_nanotime();\n \tprocs = 1;\n \ts = runtime_getenv(\"GOMAXPROCS\");\n \tp = s.str;\n@@ -747,8 +719,8 @@ static void\n checkmcount(void)\n {\n \t// sched lock is held\n-\tif(runtime_sched.mcount > runtime_sched.maxmcount) {\n-\t\truntime_printf(\"runtime: program exceeds %d-thread limit\\n\", runtime_sched.maxmcount);\n+\tif(runtime_sched->mcount > runtime_sched->maxmcount) {\n+\t\truntime_printf(\"runtime: program exceeds %d-thread limit\\n\", runtime_sched->maxmcount);\n \t\truntime_throw(\"thread exhaustion\");\n \t}\n }\n@@ -782,8 +754,8 @@ mcommoninit(M *mp)\n \n \tmp->fastrand = 0x49f6428aUL + mp->id + runtime_cputicks();\n \n-\truntime_lock(&runtime_sched);\n-\tmp->id = runtime_sched.mcount++;\n+\truntime_lock(&runtime_sched->lock);\n+\tmp->id = runtime_sched->mcount++;\n \tcheckmcount();\n \truntime_mpreinit(mp);\n \n@@ -793,7 +765,7 @@ mcommoninit(M *mp)\n \t// runtime_NumCgoCall() iterates over allm w/o schedlock,\n \t// so we need to publish it safely.\n \truntime_atomicstorep(&runtime_allm, mp);\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n }\n \n // Mark gp ready to run.\n@@ -808,7 +780,7 @@ runtime_ready(G *gp)\n \t}\n \tgp->atomicstatus = _Grunnable;\n \trunqput((P*)g->m->p, gp);\n-\tif(runtime_atomicload(&runtime_sched.npidle) != 0 && runtime_atomicload(&runtime_sched.nmspinning) == 0)  // TODO: fast atomic\n+\tif(runtime_atomicload(&runtime_sched->npidle) != 0 && runtime_atomicload(&runtime_sched->nmspinning) == 0)  // TODO: fast atomic\n \t\twakep();\n \tg->m->locks--;\n }\n@@ -828,15 +800,15 @@ runtime_gcprocs(void)\n \n \t// Figure out how many CPUs to use during GC.\n \t// Limited by gomaxprocs, number of actual CPUs, and MaxGcproc.\n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tn = runtime_gomaxprocs;\n \tif(n > runtime_ncpu)\n \t\tn = runtime_ncpu > 0 ? runtime_ncpu : 1;\n \tif(n > MaxGcproc)\n \t\tn = MaxGcproc;\n-\tif(n > runtime_sched.nmidle+1) // one M is currently running\n-\t\tn = runtime_sched.nmidle+1;\n-\truntime_unlock(&runtime_sched);\n+\tif(n > runtime_sched->nmidle+1) // one M is currently running\n+\t\tn = runtime_sched->nmidle+1;\n+\truntime_unlock(&runtime_sched->lock);\n \treturn n;\n }\n \n@@ -845,14 +817,14 @@ needaddgcproc(void)\n {\n \tint32 n;\n \n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tn = runtime_gomaxprocs;\n \tif(n > runtime_ncpu)\n \t\tn = runtime_ncpu;\n \tif(n > MaxGcproc)\n \t\tn = MaxGcproc;\n-\tn -= runtime_sched.nmidle+1; // one M is currently running\n-\truntime_unlock(&runtime_sched);\n+\tn -= runtime_sched->nmidle+1; // one M is currently running\n+\truntime_unlock(&runtime_sched->lock);\n \treturn n > 0;\n }\n \n@@ -862,7 +834,7 @@ runtime_helpgc(int32 nproc)\n \tM *mp;\n \tint32 n, pos;\n \n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tpos = 0;\n \tfor(n = 1; n < nproc; n++) {  // one M is currently running\n \t\tif(runtime_allp[pos]->mcache == g->m->mcache)\n@@ -875,7 +847,7 @@ runtime_helpgc(int32 nproc)\n \t\tpos++;\n \t\truntime_notewakeup(&mp->park);\n \t}\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n }\n \n // Similar to stoptheworld but best-effort and can be called several times.\n@@ -893,8 +865,8 @@ runtime_freezetheworld(void)\n \t// so try several times\n \tfor(i = 0; i < 5; i++) {\n \t\t// this should tell the scheduler to not start any new goroutines\n-\t\truntime_sched.stopwait = 0x7fffffff;\n-\t\truntime_atomicstore((uint32*)&runtime_sched.gcwaiting, 1);\n+\t\truntime_sched->stopwait = 0x7fffffff;\n+\t\truntime_atomicstore((uint32*)&runtime_sched->gcwaiting, 1);\n \t\t// this should stop running goroutines\n \t\tif(!preemptall())\n \t\t\tbreak;  // no running goroutines\n@@ -914,34 +886,34 @@ runtime_stopTheWorldWithSema(void)\n \tP *p;\n \tbool wait;\n \n-\truntime_lock(&runtime_sched);\n-\truntime_sched.stopwait = runtime_gomaxprocs;\n-\truntime_atomicstore((uint32*)&runtime_sched.gcwaiting, 1);\n+\truntime_lock(&runtime_sched->lock);\n+\truntime_sched->stopwait = runtime_gomaxprocs;\n+\truntime_atomicstore((uint32*)&runtime_sched->gcwaiting, 1);\n \tpreemptall();\n \t// stop current P\n \t((P*)g->m->p)->status = _Pgcstop;\n-\truntime_sched.stopwait--;\n+\truntime_sched->stopwait--;\n \t// try to retake all P's in _Psyscall status\n \tfor(i = 0; i < runtime_gomaxprocs; i++) {\n \t\tp = runtime_allp[i];\n \t\ts = p->status;\n \t\tif(s == _Psyscall && runtime_cas(&p->status, s, _Pgcstop))\n-\t\t\truntime_sched.stopwait--;\n+\t\t\truntime_sched->stopwait--;\n \t}\n \t// stop idle P's\n \twhile((p = pidleget()) != nil) {\n \t\tp->status = _Pgcstop;\n-\t\truntime_sched.stopwait--;\n+\t\truntime_sched->stopwait--;\n \t}\n-\twait = runtime_sched.stopwait > 0;\n-\truntime_unlock(&runtime_sched);\n+\twait = runtime_sched->stopwait > 0;\n+\truntime_unlock(&runtime_sched->lock);\n \n \t// wait for remaining P's to stop voluntarily\n \tif(wait) {\n-\t\truntime_notesleep(&runtime_sched.stopnote);\n-\t\truntime_noteclear(&runtime_sched.stopnote);\n+\t\truntime_notesleep(&runtime_sched->stopnote);\n+\t\truntime_noteclear(&runtime_sched->stopnote);\n \t}\n-\tif(runtime_sched.stopwait)\n+\tif(runtime_sched->stopwait)\n \t\truntime_throw(\"stoptheworld: not stopped\");\n \tfor(i = 0; i < runtime_gomaxprocs; i++) {\n \t\tp = runtime_allp[i];\n@@ -968,13 +940,13 @@ runtime_startTheWorldWithSema(void)\n \tgp = runtime_netpoll(false);  // non-blocking\n \tinjectglist(gp);\n \tadd = needaddgcproc();\n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tif(newprocs) {\n \t\tprocresize(newprocs);\n \t\tnewprocs = 0;\n \t} else\n \t\tprocresize(runtime_gomaxprocs);\n-\truntime_sched.gcwaiting = 0;\n+\truntime_sched->gcwaiting = 0;\n \n \tp1 = nil;\n \twhile((p = pidleget()) != nil) {\n@@ -988,11 +960,11 @@ runtime_startTheWorldWithSema(void)\n \t\tp->link = (uintptr)p1;\n \t\tp1 = p;\n \t}\n-\tif(runtime_sched.sysmonwait) {\n-\t\truntime_sched.sysmonwait = false;\n-\t\truntime_notewakeup(&runtime_sched.sysmonnote);\n+\tif(runtime_sched->sysmonwait) {\n+\t\truntime_sched->sysmonwait = false;\n+\t\truntime_notewakeup(&runtime_sched->sysmonnote);\n \t}\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n \n \twhile(p1) {\n \t\tp = p1;\n@@ -1285,7 +1257,7 @@ runtime_newextram(void)\n \tmp->locked = _LockInternal;\n \tmp->lockedg = gp;\n \tgp->lockedm = mp;\n-\tgp->goid = runtime_xadd64(&runtime_sched.goidgen, 1);\n+\tgp->goid = runtime_xadd64(&runtime_sched->goidgen, 1);\n \t// put on allg for garbage collector\n \tallgadd(gp);\n \n@@ -1439,13 +1411,13 @@ stopm(void)\n \t\truntime_throw(\"stopm holding p\");\n \tif(m->spinning) {\n \t\tm->spinning = false;\n-\t\truntime_xadd(&runtime_sched.nmspinning, -1);\n+\t\truntime_xadd(&runtime_sched->nmspinning, -1);\n \t}\n \n retry:\n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tmput(m);\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n \truntime_notesleep(&m->park);\n \tm = g->m;\n \truntime_noteclear(&m->park);\n@@ -1473,18 +1445,18 @@ startm(P *p, bool spinning)\n \tM *mp;\n \tvoid (*fn)(void);\n \n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tif(p == nil) {\n \t\tp = pidleget();\n \t\tif(p == nil) {\n-\t\t\truntime_unlock(&runtime_sched);\n+\t\t\truntime_unlock(&runtime_sched->lock);\n \t\t\tif(spinning)\n-\t\t\t\truntime_xadd(&runtime_sched.nmspinning, -1);\n+\t\t\t\truntime_xadd(&runtime_sched->nmspinning, -1);\n \t\t\treturn;\n \t\t}\n \t}\n \tmp = mget();\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n \tif(mp == nil) {\n \t\tfn = nil;\n \t\tif(spinning)\n@@ -1506,39 +1478,39 @@ static void\n handoffp(P *p)\n {\n \t// if it has local work, start it straight away\n-\tif(p->runqhead != p->runqtail || runtime_sched.runqsize) {\n+\tif(p->runqhead != p->runqtail || runtime_sched->runqsize) {\n \t\tstartm(p, false);\n \t\treturn;\n \t}\n \t// no local work, check that there are no spinning/idle M's,\n \t// otherwise our help is not required\n-\tif(runtime_atomicload(&runtime_sched.nmspinning) + runtime_atomicload(&runtime_sched.npidle) == 0 &&  // TODO: fast atomic\n-\t\truntime_cas(&runtime_sched.nmspinning, 0, 1)) {\n+\tif(runtime_atomicload(&runtime_sched->nmspinning) + runtime_atomicload(&runtime_sched->npidle) == 0 &&  // TODO: fast atomic\n+\t\truntime_cas(&runtime_sched->nmspinning, 0, 1)) {\n \t\tstartm(p, true);\n \t\treturn;\n \t}\n-\truntime_lock(&runtime_sched);\n-\tif(runtime_sched.gcwaiting) {\n+\truntime_lock(&runtime_sched->lock);\n+\tif(runtime_sched->gcwaiting) {\n \t\tp->status = _Pgcstop;\n-\t\tif(--runtime_sched.stopwait == 0)\n-\t\t\truntime_notewakeup(&runtime_sched.stopnote);\n-\t\truntime_unlock(&runtime_sched);\n+\t\tif(--runtime_sched->stopwait == 0)\n+\t\t\truntime_notewakeup(&runtime_sched->stopnote);\n+\t\truntime_unlock(&runtime_sched->lock);\n \t\treturn;\n \t}\n-\tif(runtime_sched.runqsize) {\n-\t\truntime_unlock(&runtime_sched);\n+\tif(runtime_sched->runqsize) {\n+\t\truntime_unlock(&runtime_sched->lock);\n \t\tstartm(p, false);\n \t\treturn;\n \t}\n \t// If this is the last running P and nobody is polling network,\n \t// need to wakeup another M to poll network.\n-\tif(runtime_sched.npidle == (uint32)runtime_gomaxprocs-1 && runtime_atomicload64(&runtime_sched.lastpoll) != 0) {\n-\t\truntime_unlock(&runtime_sched);\n+\tif(runtime_sched->npidle == (uint32)runtime_gomaxprocs-1 && runtime_atomicload64(&runtime_sched->lastpoll) != 0) {\n+\t\truntime_unlock(&runtime_sched->lock);\n \t\tstartm(p, false);\n \t\treturn;\n \t}\n \tpidleput(p);\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n }\n \n // Tries to add one more P to execute G's.\n@@ -1547,7 +1519,7 @@ static void\n wakep(void)\n {\n \t// be conservative about spinning threads\n-\tif(!runtime_cas(&runtime_sched.nmspinning, 0, 1))\n+\tif(!runtime_cas(&runtime_sched->nmspinning, 0, 1))\n \t\treturn;\n \tstartm(nil, true);\n }\n@@ -1606,18 +1578,18 @@ gcstopm(void)\n {\n \tP *p;\n \n-\tif(!runtime_sched.gcwaiting)\n+\tif(!runtime_sched->gcwaiting)\n \t\truntime_throw(\"gcstopm: not waiting for gc\");\n \tif(g->m->spinning) {\n \t\tg->m->spinning = false;\n-\t\truntime_xadd(&runtime_sched.nmspinning, -1);\n+\t\truntime_xadd(&runtime_sched->nmspinning, -1);\n \t}\n \tp = releasep();\n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tp->status = _Pgcstop;\n-\tif(--runtime_sched.stopwait == 0)\n-\t\truntime_notewakeup(&runtime_sched.stopnote);\n-\truntime_unlock(&runtime_sched);\n+\tif(--runtime_sched->stopwait == 0)\n+\t\truntime_notewakeup(&runtime_sched->stopnote);\n+\truntime_unlock(&runtime_sched->lock);\n \tstopm();\n }\n \n@@ -1639,7 +1611,7 @@ execute(G *gp)\n \tgp->m = g->m;\n \n \t// Check whether the profiler needs to be turned on or off.\n-\thz = runtime_sched.profilehz;\n+\thz = runtime_sched->profilehz;\n \tif(g->m->profilehz != hz)\n \t\truntime_resetcpuprofiler(hz);\n \n@@ -1656,7 +1628,7 @@ findrunnable(void)\n \tint32 i;\n \n top:\n-\tif(runtime_sched.gcwaiting) {\n+\tif(runtime_sched->gcwaiting) {\n \t\tgcstopm();\n \t\tgoto top;\n \t}\n@@ -1667,10 +1639,10 @@ findrunnable(void)\n \tif(gp)\n \t\treturn gp;\n \t// global runq\n-\tif(runtime_sched.runqsize) {\n-\t\truntime_lock(&runtime_sched);\n+\tif(runtime_sched->runqsize) {\n+\t\truntime_lock(&runtime_sched->lock);\n \t\tgp = globrunqget((P*)g->m->p, 0);\n-\t\truntime_unlock(&runtime_sched);\n+\t\truntime_unlock(&runtime_sched->lock);\n \t\tif(gp)\n \t\t\treturn gp;\n \t}\n@@ -1684,15 +1656,15 @@ findrunnable(void)\n \t// If number of spinning M's >= number of busy P's, block.\n \t// This is necessary to prevent excessive CPU consumption\n \t// when GOMAXPROCS>>1 but the program parallelism is low.\n-\tif(!g->m->spinning && 2 * runtime_atomicload(&runtime_sched.nmspinning) >= runtime_gomaxprocs - runtime_atomicload(&runtime_sched.npidle))  // TODO: fast atomic\n+\tif(!g->m->spinning && 2 * runtime_atomicload(&runtime_sched->nmspinning) >= runtime_gomaxprocs - runtime_atomicload(&runtime_sched->npidle))  // TODO: fast atomic\n \t\tgoto stop;\n \tif(!g->m->spinning) {\n \t\tg->m->spinning = true;\n-\t\truntime_xadd(&runtime_sched.nmspinning, 1);\n+\t\truntime_xadd(&runtime_sched->nmspinning, 1);\n \t}\n \t// random steal from other P's\n \tfor(i = 0; i < 2*runtime_gomaxprocs; i++) {\n-\t\tif(runtime_sched.gcwaiting)\n+\t\tif(runtime_sched->gcwaiting)\n \t\t\tgoto top;\n \t\tp = runtime_allp[runtime_fastrand1()%runtime_gomaxprocs];\n \t\tif(p == (P*)g->m->p)\n@@ -1704,30 +1676,30 @@ findrunnable(void)\n \t}\n stop:\n \t// return P and block\n-\truntime_lock(&runtime_sched);\n-\tif(runtime_sched.gcwaiting) {\n-\t\truntime_unlock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n+\tif(runtime_sched->gcwaiting) {\n+\t\truntime_unlock(&runtime_sched->lock);\n \t\tgoto top;\n \t}\n-\tif(runtime_sched.runqsize) {\n+\tif(runtime_sched->runqsize) {\n \t\tgp = globrunqget((P*)g->m->p, 0);\n-\t\truntime_unlock(&runtime_sched);\n+\t\truntime_unlock(&runtime_sched->lock);\n \t\treturn gp;\n \t}\n \tp = releasep();\n \tpidleput(p);\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n \tif(g->m->spinning) {\n \t\tg->m->spinning = false;\n-\t\truntime_xadd(&runtime_sched.nmspinning, -1);\n+\t\truntime_xadd(&runtime_sched->nmspinning, -1);\n \t}\n \t// check all runqueues once again\n \tfor(i = 0; i < runtime_gomaxprocs; i++) {\n \t\tp = runtime_allp[i];\n \t\tif(p && p->runqhead != p->runqtail) {\n-\t\t\truntime_lock(&runtime_sched);\n+\t\t\truntime_lock(&runtime_sched->lock);\n \t\t\tp = pidleget();\n-\t\t\truntime_unlock(&runtime_sched);\n+\t\t\truntime_unlock(&runtime_sched->lock);\n \t\t\tif(p) {\n \t\t\t\tacquirep(p);\n \t\t\t\tgoto top;\n@@ -1736,17 +1708,17 @@ findrunnable(void)\n \t\t}\n \t}\n \t// poll network\n-\tif(runtime_xchg64(&runtime_sched.lastpoll, 0) != 0) {\n+\tif(runtime_xchg64(&runtime_sched->lastpoll, 0) != 0) {\n \t\tif(g->m->p)\n \t\t\truntime_throw(\"findrunnable: netpoll with p\");\n \t\tif(g->m->spinning)\n \t\t\truntime_throw(\"findrunnable: netpoll with spinning\");\n \t\tgp = runtime_netpoll(true);  // block until new work is available\n-\t\truntime_atomicstore64(&runtime_sched.lastpoll, runtime_nanotime());\n+\t\truntime_atomicstore64(&runtime_sched->lastpoll, runtime_nanotime());\n \t\tif(gp) {\n-\t\t\truntime_lock(&runtime_sched);\n+\t\t\truntime_lock(&runtime_sched->lock);\n \t\t\tp = pidleget();\n-\t\t\truntime_unlock(&runtime_sched);\n+\t\t\truntime_unlock(&runtime_sched->lock);\n \t\t\tif(p) {\n \t\t\t\tacquirep(p);\n \t\t\t\tinjectglist((G*)gp->schedlink);\n@@ -1767,15 +1739,15 @@ resetspinning(void)\n \n \tif(g->m->spinning) {\n \t\tg->m->spinning = false;\n-\t\tnmspinning = runtime_xadd(&runtime_sched.nmspinning, -1);\n+\t\tnmspinning = runtime_xadd(&runtime_sched->nmspinning, -1);\n \t\tif(nmspinning < 0)\n \t\t\truntime_throw(\"findrunnable: negative nmspinning\");\n \t} else\n-\t\tnmspinning = runtime_atomicload(&runtime_sched.nmspinning);\n+\t\tnmspinning = runtime_atomicload(&runtime_sched->nmspinning);\n \n \t// M wakeup policy is deliberately somewhat conservative (see nmspinning handling),\n \t// so see if we need to wakeup another P here.\n-\tif (nmspinning == 0 && runtime_atomicload(&runtime_sched.npidle) > 0)\n+\tif (nmspinning == 0 && runtime_atomicload(&runtime_sched->npidle) > 0)\n \t\twakep();\n }\n \n@@ -1789,16 +1761,16 @@ injectglist(G *glist)\n \n \tif(glist == nil)\n \t\treturn;\n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tfor(n = 0; glist; n++) {\n \t\tgp = glist;\n \t\tglist = (G*)gp->schedlink;\n \t\tgp->atomicstatus = _Grunnable;\n \t\tglobrunqput(gp);\n \t}\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n \n-\tfor(; n && runtime_sched.npidle; n--)\n+\tfor(; n && runtime_sched->npidle; n--)\n \t\tstartm(nil, false);\n }\n \n@@ -1814,7 +1786,7 @@ schedule(void)\n \t\truntime_throw(\"schedule: holding locks\");\n \n top:\n-\tif(runtime_sched.gcwaiting) {\n+\tif(runtime_sched->gcwaiting) {\n \t\tgcstopm();\n \t\tgoto top;\n \t}\n@@ -1826,10 +1798,10 @@ schedule(void)\n \ttick = ((P*)g->m->p)->schedtick;\n \t// This is a fancy way to say tick%61==0,\n \t// it uses 2 MUL instructions instead of a single DIV and so is faster on modern processors.\n-\tif(tick - (((uint64)tick*0x4325c53fu)>>36)*61 == 0 && runtime_sched.runqsize > 0) {\n-\t\truntime_lock(&runtime_sched);\n+\tif(tick - (((uint64)tick*0x4325c53fu)>>36)*61 == 0 && runtime_sched->runqsize > 0) {\n+\t\truntime_lock(&runtime_sched->lock);\n \t\tgp = globrunqget((P*)g->m->p, 1);\n-\t\truntime_unlock(&runtime_sched);\n+\t\truntime_unlock(&runtime_sched->lock);\n \t\tif(gp)\n \t\t\tresetspinning();\n \t}\n@@ -1959,9 +1931,9 @@ runtime_gosched0(G *gp)\n \tgp->atomicstatus = _Grunnable;\n \tgp->m = nil;\n \tm->curg = nil;\n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tglobrunqput(gp);\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n \tif(m->lockedg) {\n \t\tstoplockedm();\n \t\texecute(gp);  // Never returns.\n@@ -2074,25 +2046,25 @@ doentersyscall(uintptr pc, uintptr sp)\n \n \tg->atomicstatus = _Gsyscall;\n \n-\tif(runtime_atomicload(&runtime_sched.sysmonwait)) {  // TODO: fast atomic\n-\t\truntime_lock(&runtime_sched);\n-\t\tif(runtime_atomicload(&runtime_sched.sysmonwait)) {\n-\t\t\truntime_atomicstore(&runtime_sched.sysmonwait, 0);\n-\t\t\truntime_notewakeup(&runtime_sched.sysmonnote);\n+\tif(runtime_atomicload(&runtime_sched->sysmonwait)) {  // TODO: fast atomic\n+\t\truntime_lock(&runtime_sched->lock);\n+\t\tif(runtime_atomicload(&runtime_sched->sysmonwait)) {\n+\t\t\truntime_atomicstore(&runtime_sched->sysmonwait, 0);\n+\t\t\truntime_notewakeup(&runtime_sched->sysmonnote);\n \t\t}\n-\t\truntime_unlock(&runtime_sched);\n+\t\truntime_unlock(&runtime_sched->lock);\n \t}\n \n \tg->m->mcache = nil;\n \t((P*)(g->m->p))->m = 0;\n \truntime_atomicstore(&((P*)g->m->p)->status, _Psyscall);\n-\tif(runtime_atomicload(&runtime_sched.gcwaiting)) {\n-\t\truntime_lock(&runtime_sched);\n-\t\tif (runtime_sched.stopwait > 0 && runtime_cas(&((P*)g->m->p)->status, _Psyscall, _Pgcstop)) {\n-\t\t\tif(--runtime_sched.stopwait == 0)\n-\t\t\t\truntime_notewakeup(&runtime_sched.stopnote);\n+\tif(runtime_atomicload(&runtime_sched->gcwaiting)) {\n+\t\truntime_lock(&runtime_sched->lock);\n+\t\tif (runtime_sched->stopwait > 0 && runtime_cas(&((P*)g->m->p)->status, _Psyscall, _Pgcstop)) {\n+\t\t\tif(--runtime_sched->stopwait == 0)\n+\t\t\t\truntime_notewakeup(&runtime_sched->stopnote);\n \t\t}\n-\t\truntime_unlock(&runtime_sched);\n+\t\truntime_unlock(&runtime_sched->lock);\n \t}\n \n \tg->m->locks--;\n@@ -2201,7 +2173,7 @@ exitsyscallfast(void)\n \tgp = g;\n \n \t// Freezetheworld sets stopwait but does not retake P's.\n-\tif(runtime_sched.stopwait) {\n+\tif(runtime_sched->stopwait) {\n \t\tgp->m->p = 0;\n \t\treturn false;\n \t}\n@@ -2215,14 +2187,14 @@ exitsyscallfast(void)\n \t}\n \t// Try to get any other idle P.\n \tgp->m->p = 0;\n-\tif(runtime_sched.pidle) {\n-\t\truntime_lock(&runtime_sched);\n+\tif(runtime_sched->pidle) {\n+\t\truntime_lock(&runtime_sched->lock);\n \t\tp = pidleget();\n-\t\tif(p && runtime_atomicload(&runtime_sched.sysmonwait)) {\n-\t\t\truntime_atomicstore(&runtime_sched.sysmonwait, 0);\n-\t\t\truntime_notewakeup(&runtime_sched.sysmonnote);\n+\t\tif(p && runtime_atomicload(&runtime_sched->sysmonwait)) {\n+\t\t\truntime_atomicstore(&runtime_sched->sysmonwait, 0);\n+\t\t\truntime_notewakeup(&runtime_sched->sysmonnote);\n \t\t}\n-\t\truntime_unlock(&runtime_sched);\n+\t\truntime_unlock(&runtime_sched->lock);\n \t\tif(p) {\n \t\t\tacquirep(p);\n \t\t\treturn true;\n@@ -2243,15 +2215,15 @@ exitsyscall0(G *gp)\n \tgp->atomicstatus = _Grunnable;\n \tgp->m = nil;\n \tm->curg = nil;\n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tp = pidleget();\n \tif(p == nil)\n \t\tglobrunqput(gp);\n-\telse if(runtime_atomicload(&runtime_sched.sysmonwait)) {\n-\t\truntime_atomicstore(&runtime_sched.sysmonwait, 0);\n-\t\truntime_notewakeup(&runtime_sched.sysmonnote);\n+\telse if(runtime_atomicload(&runtime_sched->sysmonwait)) {\n+\t\truntime_atomicstore(&runtime_sched->sysmonwait, 0);\n+\t\truntime_notewakeup(&runtime_sched->sysmonnote);\n \t}\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n \tif(p) {\n \t\tacquirep(p);\n \t\texecute(gp);  // Never returns.\n@@ -2308,7 +2280,7 @@ syscall_runtime_AfterFork(void)\n {\n \tint32 hz;\n \n-\thz = runtime_sched.profilehz;\n+\thz = runtime_sched->profilehz;\n \tif(hz != 0)\n \t\truntime_resetcpuprofiler(hz);\n \truntime_m()->locks--;\n@@ -2400,7 +2372,7 @@ __go_go(void (*fn)(void*), void* arg)\n \tnewg->gopc = (uintptr)__builtin_return_address(0);\n \tnewg->atomicstatus = _Grunnable;\n \tif(p->goidcache == p->goidcacheend) {\n-\t\tp->goidcache = runtime_xadd64(&runtime_sched.goidgen, GoidCacheBatch);\n+\t\tp->goidcache = runtime_xadd64(&runtime_sched->goidgen, GoidCacheBatch);\n \t\tp->goidcacheend = p->goidcache + GoidCacheBatch;\n \t}\n \tnewg->goid = p->goidcache++;\n@@ -2421,7 +2393,7 @@ __go_go(void (*fn)(void*), void* arg)\n \n \t\trunqput(p, vnewg);\n \n-\t\tif(runtime_atomicload(&runtime_sched.npidle) != 0 && runtime_atomicload(&runtime_sched.nmspinning) == 0 && fn != runtime_main)  // TODO: fast atomic\n+\t\tif(runtime_atomicload(&runtime_sched->npidle) != 0 && runtime_atomicload(&runtime_sched->nmspinning) == 0 && fn != runtime_main)  // TODO: fast atomic\n \t\t\twakep();\n \t\tg->m->locks--;\n \t\treturn vnewg;\n@@ -2462,15 +2434,15 @@ gfput(P *p, G *gp)\n \tp->gfree = gp;\n \tp->gfreecnt++;\n \tif(p->gfreecnt >= 64) {\n-\t\truntime_lock(&runtime_sched.gflock);\n+\t\truntime_lock(&runtime_sched->gflock);\n \t\twhile(p->gfreecnt >= 32) {\n \t\t\tp->gfreecnt--;\n \t\t\tgp = p->gfree;\n \t\t\tp->gfree = (G*)gp->schedlink;\n-\t\t\tgp->schedlink = (uintptr)runtime_sched.gfree;\n-\t\t\truntime_sched.gfree = gp;\n+\t\t\tgp->schedlink = (uintptr)runtime_sched->gfree;\n+\t\t\truntime_sched->gfree = gp;\n \t\t}\n-\t\truntime_unlock(&runtime_sched.gflock);\n+\t\truntime_unlock(&runtime_sched->gflock);\n \t}\n }\n \n@@ -2483,16 +2455,16 @@ gfget(P *p)\n \n retry:\n \tgp = p->gfree;\n-\tif(gp == nil && runtime_sched.gfree) {\n-\t\truntime_lock(&runtime_sched.gflock);\n-\t\twhile(p->gfreecnt < 32 && runtime_sched.gfree) {\n+\tif(gp == nil && runtime_sched->gfree) {\n+\t\truntime_lock(&runtime_sched->gflock);\n+\t\twhile(p->gfreecnt < 32 && runtime_sched->gfree) {\n \t\t\tp->gfreecnt++;\n-\t\t\tgp = runtime_sched.gfree;\n-\t\t\truntime_sched.gfree = (G*)gp->schedlink;\n+\t\t\tgp = runtime_sched->gfree;\n+\t\t\truntime_sched->gfree = (G*)gp->schedlink;\n \t\t\tgp->schedlink = (uintptr)p->gfree;\n \t\t\tp->gfree = gp;\n \t\t}\n-\t\truntime_unlock(&runtime_sched.gflock);\n+\t\truntime_unlock(&runtime_sched->gflock);\n \t\tgoto retry;\n \t}\n \tif(gp) {\n@@ -2508,15 +2480,15 @@ gfpurge(P *p)\n {\n \tG *gp;\n \n-\truntime_lock(&runtime_sched.gflock);\n+\truntime_lock(&runtime_sched->gflock);\n \twhile(p->gfreecnt) {\n \t\tp->gfreecnt--;\n \t\tgp = p->gfree;\n \t\tp->gfree = (G*)gp->schedlink;\n-\t\tgp->schedlink = (uintptr)runtime_sched.gfree;\n-\t\truntime_sched.gfree = gp;\n+\t\tgp->schedlink = (uintptr)runtime_sched->gfree;\n+\t\truntime_sched->gfree = gp;\n \t}\n-\truntime_unlock(&runtime_sched.gflock);\n+\truntime_unlock(&runtime_sched->gflock);\n }\n \n void\n@@ -2546,13 +2518,13 @@ runtime_GOMAXPROCS(intgo n)\n \n \tif(n > _MaxGomaxprocs)\n \t\tn = _MaxGomaxprocs;\n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tret = (intgo)runtime_gomaxprocs;\n \tif(n <= 0 || n == ret) {\n-\t\truntime_unlock(&runtime_sched);\n+\t\truntime_unlock(&runtime_sched->lock);\n \t\treturn ret;\n \t}\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n \n \truntime_acquireWorldsema();\n \tg->m->gcing = 1;\n@@ -2653,7 +2625,7 @@ runtime_gcount(void)\n int32\n runtime_mcount(void)\n {\n-\treturn runtime_sched.mcount;\n+\treturn runtime_sched->mcount;\n }\n \n static struct {\n@@ -2754,9 +2726,9 @@ runtime_setcpuprofilerate_m(int32 hz)\n \tprof.hz = hz;\n \truntime_atomicstore(&prof.lock, 0);\n \n-\truntime_lock(&runtime_sched);\n-\truntime_sched.profilehz = hz;\n-\truntime_unlock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n+\truntime_sched->profilehz = hz;\n+\truntime_unlock(&runtime_sched->lock);\n \n \tif(hz != 0)\n \t\truntime_resetcpuprofiler(hz);\n@@ -2809,22 +2781,22 @@ procresize(int32 new)\n \t\t\tp->runqtail--;\n \t\t\tgp = (G*)p->runq[p->runqtail%nelem(p->runq)];\n \t\t\t// push onto head of global queue\n-\t\t\tgp->schedlink = (uintptr)runtime_sched.runqhead;\n-\t\t\truntime_sched.runqhead = gp;\n-\t\t\tif(runtime_sched.runqtail == nil)\n-\t\t\t\truntime_sched.runqtail = gp;\n-\t\t\truntime_sched.runqsize++;\n+\t\t\tgp->schedlink = runtime_sched->runqhead;\n+\t\t\truntime_sched->runqhead = (uintptr)gp;\n+\t\t\tif(runtime_sched->runqtail == 0)\n+\t\t\t\truntime_sched->runqtail = (uintptr)gp;\n+\t\t\truntime_sched->runqsize++;\n \t\t}\n \t}\n \t// fill local queues with at most nelem(p->runq)/2 goroutines\n \t// start at 1 because current M already executes some G and will acquire allp[0] below,\n \t// so if we have a spare G we want to put it into allp[1].\n-\tfor(i = 1; (uint32)i < (uint32)new * nelem(p->runq)/2 && runtime_sched.runqsize > 0; i++) {\n-\t\tgp = runtime_sched.runqhead;\n-\t\truntime_sched.runqhead = (G*)gp->schedlink;\n-\t\tif(runtime_sched.runqhead == nil)\n-\t\t\truntime_sched.runqtail = nil;\n-\t\truntime_sched.runqsize--;\n+\tfor(i = 1; (uint32)i < (uint32)new * nelem(p->runq)/2 && runtime_sched->runqsize > 0; i++) {\n+\t\tgp = (G*)runtime_sched->runqhead;\n+\t\truntime_sched->runqhead = gp->schedlink;\n+\t\tif(runtime_sched->runqhead == 0)\n+\t\t\truntime_sched->runqtail = 0;\n+\t\truntime_sched->runqsize--;\n \t\trunqput(runtime_allp[i%new], gp);\n \t}\n \n@@ -2899,11 +2871,11 @@ releasep(void)\n static void\n incidlelocked(int32 v)\n {\n-\truntime_lock(&runtime_sched);\n-\truntime_sched.nmidlelocked += v;\n+\truntime_lock(&runtime_sched->lock);\n+\truntime_sched->nmidlelocked += v;\n \tif(v > 0)\n \t\tcheckdead();\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n }\n \n // Check for deadlock situation.\n@@ -2923,7 +2895,7 @@ checkdead(void)\n \t}\n \n \t// -1 for sysmon\n-\trun = runtime_sched.mcount - runtime_sched.nmidle - runtime_sched.nmidlelocked - 1 - countextra();\n+\trun = runtime_sched->mcount - runtime_sched->nmidle - runtime_sched->nmidlelocked - 1 - countextra();\n \tif(run > 0)\n \t\treturn;\n \t// If we are dying because of a signal caught on an already idle thread,\n@@ -2934,7 +2906,7 @@ checkdead(void)\n \t\treturn;\n \tif(run < 0) {\n \t\truntime_printf(\"runtime: checkdead: nmidle=%d nmidlelocked=%d mcount=%d\\n\",\n-\t\t\truntime_sched.nmidle, runtime_sched.nmidlelocked, runtime_sched.mcount);\n+\t\t\truntime_sched->nmidle, runtime_sched->nmidlelocked, runtime_sched->mcount);\n \t\truntime_throw(\"checkdead: inconsistent counts\");\n \t}\n \tgrunning = 0;\n@@ -2978,23 +2950,23 @@ sysmon(void)\n \t\t\tdelay = 10*1000;\n \t\truntime_usleep(delay);\n \t\tif(runtime_debug.schedtrace <= 0 &&\n-\t\t\t(runtime_sched.gcwaiting || runtime_atomicload(&runtime_sched.npidle) == (uint32)runtime_gomaxprocs)) {  // TODO: fast atomic\n-\t\t\truntime_lock(&runtime_sched);\n-\t\t\tif(runtime_atomicload(&runtime_sched.gcwaiting) || runtime_atomicload(&runtime_sched.npidle) == (uint32)runtime_gomaxprocs) {\n-\t\t\t\truntime_atomicstore(&runtime_sched.sysmonwait, 1);\n-\t\t\t\truntime_unlock(&runtime_sched);\n-\t\t\t\truntime_notesleep(&runtime_sched.sysmonnote);\n-\t\t\t\truntime_noteclear(&runtime_sched.sysmonnote);\n+\t\t\t(runtime_sched->gcwaiting || runtime_atomicload(&runtime_sched->npidle) == (uint32)runtime_gomaxprocs)) {  // TODO: fast atomic\n+\t\t\truntime_lock(&runtime_sched->lock);\n+\t\t\tif(runtime_atomicload(&runtime_sched->gcwaiting) || runtime_atomicload(&runtime_sched->npidle) == (uint32)runtime_gomaxprocs) {\n+\t\t\t\truntime_atomicstore(&runtime_sched->sysmonwait, 1);\n+\t\t\t\truntime_unlock(&runtime_sched->lock);\n+\t\t\t\truntime_notesleep(&runtime_sched->sysmonnote);\n+\t\t\t\truntime_noteclear(&runtime_sched->sysmonnote);\n \t\t\t\tidle = 0;\n \t\t\t\tdelay = 20;\n \t\t\t} else\n-\t\t\t\truntime_unlock(&runtime_sched);\n+\t\t\t\truntime_unlock(&runtime_sched->lock);\n \t\t}\n \t\t// poll network if not polled for more than 10ms\n-\t\tlastpoll = runtime_atomicload64(&runtime_sched.lastpoll);\n+\t\tlastpoll = runtime_atomicload64(&runtime_sched->lastpoll);\n \t\tnow = runtime_nanotime();\n \t\tif(lastpoll != 0 && lastpoll + 10*1000*1000 < now) {\n-\t\t\truntime_cas64(&runtime_sched.lastpoll, lastpoll, now);\n+\t\t\truntime_cas64(&runtime_sched->lastpoll, lastpoll, now);\n \t\t\tgp = runtime_netpoll(false);  // non-blocking\n \t\t\tif(gp) {\n \t\t\t\t// Need to decrement number of idle locked M's\n@@ -3060,7 +3032,7 @@ retake(int64 now)\n \t\t\t// but on the other hand we want to retake them eventually\n \t\t\t// because they can prevent the sysmon thread from deep sleep.\n \t\t\tif(p->runqhead == p->runqtail &&\n-\t\t\t\truntime_atomicload(&runtime_sched.nmspinning) + runtime_atomicload(&runtime_sched.npidle) > 0 &&\n+\t\t\t\truntime_atomicload(&runtime_sched->nmspinning) + runtime_atomicload(&runtime_sched->npidle) > 0 &&\n \t\t\t\tpd->syscallwhen + 10*1000*1000 > now)\n \t\t\t\tcontinue;\n \t\t\t// Need to decrement number of idle locked M's\n@@ -3117,14 +3089,14 @@ runtime_schedtrace(bool detailed)\n \tif(starttime == 0)\n \t\tstarttime = now;\n \n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \truntime_printf(\"SCHED %Dms: gomaxprocs=%d idleprocs=%d threads=%d idlethreads=%d runqueue=%d\",\n-\t\t(now-starttime)/1000000, runtime_gomaxprocs, runtime_sched.npidle, runtime_sched.mcount,\n-\t\truntime_sched.nmidle, runtime_sched.runqsize);\n+\t\t(now-starttime)/1000000, runtime_gomaxprocs, runtime_sched->npidle, runtime_sched->mcount,\n+\t\truntime_sched->nmidle, runtime_sched->runqsize);\n \tif(detailed) {\n \t\truntime_printf(\" gcwaiting=%d nmidlelocked=%d nmspinning=%d stopwait=%d sysmonwait=%d\\n\",\n-\t\t\truntime_sched.gcwaiting, runtime_sched.nmidlelocked, runtime_sched.nmspinning,\n-\t\t\truntime_sched.stopwait, runtime_sched.sysmonwait);\n+\t\t\truntime_sched->gcwaiting, runtime_sched->nmidlelocked, runtime_sched->nmspinning,\n+\t\t\truntime_sched->stopwait, runtime_sched->sysmonwait);\n \t}\n \t// We must be careful while reading data from P's, M's and G's.\n \t// Even if we hold schedlock, most data can be changed concurrently.\n@@ -3153,7 +3125,7 @@ runtime_schedtrace(bool detailed)\n \t\t}\n \t}\n \tif(!detailed) {\n-\t\truntime_unlock(&runtime_sched);\n+\t\truntime_unlock(&runtime_sched->lock);\n \t\treturn;\n \t}\n \tfor(mp = runtime_allm; mp; mp = mp->alllink) {\n@@ -3185,17 +3157,17 @@ runtime_schedtrace(bool detailed)\n \t\t\tlockedm ? lockedm->id : -1);\n \t}\n \truntime_unlock(&allglock);\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n }\n \n // Put mp on midle list.\n // Sched must be locked.\n static void\n mput(M *mp)\n {\n-\tmp->schedlink = (uintptr)runtime_sched.midle;\n-\truntime_sched.midle = mp;\n-\truntime_sched.nmidle++;\n+\tmp->schedlink = runtime_sched->midle;\n+\truntime_sched->midle = (uintptr)mp;\n+\truntime_sched->nmidle++;\n \tcheckdead();\n }\n \n@@ -3206,9 +3178,9 @@ mget(void)\n {\n \tM *mp;\n \n-\tif((mp = runtime_sched.midle) != nil){\n-\t\truntime_sched.midle = (M*)mp->schedlink;\n-\t\truntime_sched.nmidle--;\n+\tif((mp = (M*)runtime_sched->midle) != nil){\n+\t\truntime_sched->midle = mp->schedlink;\n+\t\truntime_sched->nmidle--;\n \t}\n \treturn mp;\n }\n@@ -3219,12 +3191,12 @@ static void\n globrunqput(G *gp)\n {\n \tgp->schedlink = 0;\n-\tif(runtime_sched.runqtail)\n-\t\truntime_sched.runqtail->schedlink = (uintptr)gp;\n+\tif(runtime_sched->runqtail)\n+\t\t((G*)runtime_sched->runqtail)->schedlink = (uintptr)gp;\n \telse\n-\t\truntime_sched.runqhead = gp;\n-\truntime_sched.runqtail = gp;\n-\truntime_sched.runqsize++;\n+\t\truntime_sched->runqhead = (uintptr)gp;\n+\truntime_sched->runqtail = (uintptr)gp;\n+\truntime_sched->runqsize++;\n }\n \n // Put a batch of runnable goroutines on the global runnable queue.\n@@ -3233,12 +3205,12 @@ static void\n globrunqputbatch(G *ghead, G *gtail, int32 n)\n {\n \tgtail->schedlink = 0;\n-\tif(runtime_sched.runqtail)\n-\t\truntime_sched.runqtail->schedlink = (uintptr)ghead;\n+\tif(runtime_sched->runqtail)\n+\t\t((G*)runtime_sched->runqtail)->schedlink = (uintptr)ghead;\n \telse\n-\t\truntime_sched.runqhead = ghead;\n-\truntime_sched.runqtail = gtail;\n-\truntime_sched.runqsize += n;\n+\t\truntime_sched->runqhead = (uintptr)ghead;\n+\truntime_sched->runqtail = (uintptr)gtail;\n+\truntime_sched->runqsize += n;\n }\n \n // Try get a batch of G's from the global runnable queue.\n@@ -3249,24 +3221,24 @@ globrunqget(P *p, int32 max)\n \tG *gp, *gp1;\n \tint32 n;\n \n-\tif(runtime_sched.runqsize == 0)\n+\tif(runtime_sched->runqsize == 0)\n \t\treturn nil;\n-\tn = runtime_sched.runqsize/runtime_gomaxprocs+1;\n-\tif(n > runtime_sched.runqsize)\n-\t\tn = runtime_sched.runqsize;\n+\tn = runtime_sched->runqsize/runtime_gomaxprocs+1;\n+\tif(n > runtime_sched->runqsize)\n+\t\tn = runtime_sched->runqsize;\n \tif(max > 0 && n > max)\n \t\tn = max;\n \tif((uint32)n > nelem(p->runq)/2)\n \t\tn = nelem(p->runq)/2;\n-\truntime_sched.runqsize -= n;\n-\tif(runtime_sched.runqsize == 0)\n-\t\truntime_sched.runqtail = nil;\n-\tgp = runtime_sched.runqhead;\n-\truntime_sched.runqhead = (G*)gp->schedlink;\n+\truntime_sched->runqsize -= n;\n+\tif(runtime_sched->runqsize == 0)\n+\t\truntime_sched->runqtail = 0;\n+\tgp = (G*)runtime_sched->runqhead;\n+\truntime_sched->runqhead = gp->schedlink;\n \tn--;\n \twhile(n--) {\n-\t\tgp1 = runtime_sched.runqhead;\n-\t\truntime_sched.runqhead = (G*)gp1->schedlink;\n+\t\tgp1 = (G*)runtime_sched->runqhead;\n+\t\truntime_sched->runqhead = gp1->schedlink;\n \t\trunqput(p, gp1);\n \t}\n \treturn gp;\n@@ -3277,9 +3249,9 @@ globrunqget(P *p, int32 max)\n static void\n pidleput(P *p)\n {\n-\tp->link = (uintptr)runtime_sched.pidle;\n-\truntime_sched.pidle = p;\n-\truntime_xadd(&runtime_sched.npidle, 1);  // TODO: fast atomic\n+\tp->link = runtime_sched->pidle;\n+\truntime_sched->pidle = (uintptr)p;\n+\truntime_xadd(&runtime_sched->npidle, 1);  // TODO: fast atomic\n }\n \n // Try get a p from pidle list.\n@@ -3289,10 +3261,10 @@ pidleget(void)\n {\n \tP *p;\n \n-\tp = runtime_sched.pidle;\n+\tp = (P*)runtime_sched->pidle;\n \tif(p) {\n-\t\truntime_sched.pidle = (P*)p->link;\n-\t\truntime_xadd(&runtime_sched.npidle, -1);  // TODO: fast atomic\n+\t\truntime_sched->pidle = p->link;\n+\t\truntime_xadd(&runtime_sched->npidle, -1);  // TODO: fast atomic\n \t}\n \treturn p;\n }\n@@ -3341,9 +3313,9 @@ runqputslow(P *p, G *gp, uint32 h, uint32 t)\n \tfor(i=0; i<n; i++)\n \t\tbatch[i]->schedlink = (uintptr)batch[i+1];\n \t// Now put the batch on global queue.\n-\truntime_lock(&runtime_sched);\n+\truntime_lock(&runtime_sched->lock);\n \tglobrunqputbatch(batch[0], batch[n], n+1);\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n \treturn true;\n }\n \n@@ -3495,11 +3467,11 @@ runtime_setmaxthreads(intgo in)\n {\n \tintgo out;\n \n-\truntime_lock(&runtime_sched);\n-\tout = (intgo)runtime_sched.maxmcount;\n-\truntime_sched.maxmcount = (int32)in;\n+\truntime_lock(&runtime_sched->lock);\n+\tout = (intgo)runtime_sched->maxmcount;\n+\truntime_sched->maxmcount = (int32)in;\n \tcheckmcount();\n-\truntime_unlock(&runtime_sched);\n+\truntime_unlock(&runtime_sched->lock);\n \treturn out;\n }\n \n@@ -3558,7 +3530,6 @@ sync_atomic_runtime_procUnpin()\n void\n runtime_proc_scan(struct Workbuf** wbufp, void (*enqueue1)(struct Workbuf**, Obj))\n {\n-\tenqueue1(wbufp, (Obj){(byte*)&runtime_sched, sizeof runtime_sched, 0});\n \tenqueue1(wbufp, (Obj){(byte*)&runtime_main_init_done, sizeof runtime_main_init_done, 0});\n }\n \n@@ -3567,7 +3538,7 @@ runtime_proc_scan(struct Workbuf** wbufp, void (*enqueue1)(struct Workbuf**, Obj\n bool\n runtime_gcwaiting(void)\n {\n-\treturn runtime_sched.gcwaiting;\n+\treturn runtime_sched->gcwaiting;\n }\n \n // os_beforeExit is called from os.Exit(0).\n@@ -3602,7 +3573,7 @@ sync_runtime_canSpin(intgo i)\n \t// GOMAXPROCS>1 and there is at least one other running P and local runq is empty.\n \t// As opposed to runtime mutex we don't do passive spinning here,\n \t// because there can be work on global runq on on other Ps.\n-\tif (i >= ACTIVE_SPIN || runtime_ncpu <= 1 || runtime_gomaxprocs <= (int32)(runtime_sched.npidle+runtime_sched.nmspinning)+1) {\n+\tif (i >= ACTIVE_SPIN || runtime_ncpu <= 1 || runtime_gomaxprocs <= (int32)(runtime_sched->npidle+runtime_sched->nmspinning)+1) {\n \t\treturn false;\n \t}\n \tp = (P*)g->m->p;"}]}