{"sha": "6cdd5aecfb4e062354db8f7253240a371ba418af", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NmNkZDVhZWNmYjRlMDYyMzU0ZGI4ZjcyNTMyNDBhMzcxYmE0MThhZg==", "commit": {"author": {"name": "Richard Biener", "email": "rguenther@suse.de", "date": "2018-10-10T07:05:47Z"}, "committer": {"name": "Richard Biener", "email": "rguenth@gcc.gnu.org", "date": "2018-10-10T07:05:47Z"}, "message": "sse.md (reduc_plus_scal_v8df, [...]): Merge into pattern reducing to half width and recursing and pattern terminating...\n\n2018-10-10  Richard Biener  <rguenther@suse.de>\n\n\t* config/i386/sse.md (reduc_plus_scal_v8df, reduc_plus_scal_v4df,\n\treduc_plus_scal_v2df, reduc_plus_scal_v16sf, reduc_plus_scal_v8sf,\n\treduc_plus_scal_v4sf): Merge into pattern reducing to half width\n\tand recursing and pattern terminating the recursion on SSE\n\tvector width using ix86_expand_reduc.\n\t(reduc_sminmax_scal_<mode>): Split into part reducing to half\n\twidth and recursing and SSE2 vector variant doing the final\n\treduction with ix86_expand_reduc.\n\t(reduc_uminmax_scal_<mode>): Likewise for the AVX512 variants\n\twith terminating the recursion at AVX level, splitting that\n\tto SSE there.\n\nFrom-SVN: r265004", "tree": {"sha": "c039977eac3392c84b8e276012a795dbd5da47ec", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/c039977eac3392c84b8e276012a795dbd5da47ec"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6cdd5aecfb4e062354db8f7253240a371ba418af", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6cdd5aecfb4e062354db8f7253240a371ba418af", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6cdd5aecfb4e062354db8f7253240a371ba418af", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6cdd5aecfb4e062354db8f7253240a371ba418af/comments", "author": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "fa23d4e1224e572470c1798c9742d2081c752500", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fa23d4e1224e572470c1798c9742d2081c752500", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fa23d4e1224e572470c1798c9742d2081c752500"}], "stats": {"total": 164, "additions": 77, "deletions": 87}, "files": [{"sha": "8227395dd99e30be7b250ad311b9c969584a8cfd", "filename": "gcc/ChangeLog", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6cdd5aecfb4e062354db8f7253240a371ba418af/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6cdd5aecfb4e062354db8f7253240a371ba418af/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=6cdd5aecfb4e062354db8f7253240a371ba418af", "patch": "@@ -1,3 +1,17 @@\n+2018-10-10  Richard Biener  <rguenther@suse.de>\n+\n+\t* config/i386/sse.md (reduc_plus_scal_v8df, reduc_plus_scal_v4df,\n+\treduc_plus_scal_v2df, reduc_plus_scal_v16sf, reduc_plus_scal_v8sf,\n+\treduc_plus_scal_v4sf): Merge into pattern reducing to half width\n+\tand recursing and pattern terminating the recursion on SSE\n+\tvector width using ix86_expand_reduc.\n+\t(reduc_sminmax_scal_<mode>): Split into part reducing to half\n+\twidth and recursing and SSE2 vector variant doing the final\n+\treduction with ix86_expand_reduc.\n+\t(reduc_uminmax_scal_<mode>): Likewise for the AVX512 variants\n+\twith terminating the recursion at AVX level, splitting that\n+\tto SSE there.\n+\n 2018-10-09  David Malcolm  <dmalcolm@redhat.com>\n \n \t* genmatch.c (error_cb): Rename to..."}, {"sha": "9fc5819a863710481a05e3185e12fc26a686fa65", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 63, "deletions": 87, "changes": 150, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6cdd5aecfb4e062354db8f7253240a371ba418af/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6cdd5aecfb4e062354db8f7253240a371ba418af/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=6cdd5aecfb4e062354db8f7253240a371ba418af", "patch": "@@ -2457,98 +2457,65 @@\n    (set_attr \"prefix_rep\" \"1,*\")\n    (set_attr \"mode\" \"V4SF\")])\n \n-(define_expand \"reduc_plus_scal_v8df\"\n-  [(match_operand:DF 0 \"register_operand\")\n-   (match_operand:V8DF 1 \"register_operand\")]\n-  \"TARGET_AVX512F\"\n-{\n-  rtx tmp = gen_reg_rtx (V8DFmode);\n-  ix86_expand_reduc (gen_addv8df3, tmp, operands[1]);\n-  emit_insn (gen_vec_extractv8dfdf (operands[0], tmp, const0_rtx));\n-  DONE;\n-})\n+(define_mode_iterator REDUC_SSE_PLUS_MODE\n+ [(V2DF \"TARGET_SSE\") (V4SF \"TARGET_SSE\")])\n \n-(define_expand \"reduc_plus_scal_v4df\"\n-  [(match_operand:DF 0 \"register_operand\")\n-   (match_operand:V4DF 1 \"register_operand\")]\n-  \"TARGET_AVX\"\n+(define_expand \"reduc_plus_scal_<mode>\"\n+ [(plus:REDUC_SSE_PLUS_MODE\n+   (match_operand:<ssescalarmode> 0 \"register_operand\")\n+   (match_operand:REDUC_SSE_PLUS_MODE 1 \"register_operand\"))]\n+ \"\"\n {\n-  rtx tmp = gen_reg_rtx (V2DFmode);\n-  emit_insn (gen_vec_extract_hi_v4df (tmp, operands[1]));\n-  rtx tmp2 = gen_reg_rtx (V2DFmode);\n-  emit_insn (gen_addv2df3 (tmp2, tmp, gen_lowpart (V2DFmode, operands[1])));\n-  rtx tmp3 = gen_reg_rtx (V2DFmode);\n-  emit_insn (gen_vec_interleave_highv2df (tmp3, tmp2, tmp2));\n-  emit_insn (gen_adddf3 (operands[0],\n-\t\t\t gen_lowpart (DFmode, tmp2),\n-\t\t\t gen_lowpart (DFmode, tmp3)));\n-  DONE;\n-})\n-\n-(define_expand \"reduc_plus_scal_v2df\"\n-  [(match_operand:DF 0 \"register_operand\")\n-   (match_operand:V2DF 1 \"register_operand\")]\n-  \"TARGET_SSE2\"\n-{\n-  rtx tmp = gen_reg_rtx (V2DFmode);\n-  emit_insn (gen_vec_interleave_highv2df (tmp, operands[1], operands[1]));\n-  emit_insn (gen_adddf3 (operands[0],\n-\t\t\t gen_lowpart (DFmode, tmp),\n-\t\t\t gen_lowpart (DFmode, operands[1])));\n+  rtx tmp = gen_reg_rtx (<MODE>mode);\n+  ix86_expand_reduc (gen_add<mode>3, tmp, operands[1]);\n+  emit_insn (gen_vec_extract<mode><ssescalarmodelower> (operands[0], tmp,\n+                                                        const0_rtx));\n   DONE;\n })\n \n-(define_expand \"reduc_plus_scal_v16sf\"\n-  [(match_operand:SF 0 \"register_operand\")\n-   (match_operand:V16SF 1 \"register_operand\")]\n-  \"TARGET_AVX512F\"\n-{\n-  rtx tmp = gen_reg_rtx (V16SFmode);\n-  ix86_expand_reduc (gen_addv16sf3, tmp, operands[1]);\n-  emit_insn (gen_vec_extractv16sfsf (operands[0], tmp, const0_rtx));\n+(define_mode_iterator REDUC_PLUS_MODE\n+ [(V4DF \"TARGET_AVX\") (V8SF \"TARGET_AVX\")\n+  (V8DF \"TARGET_AVX512F\") (V16SF \"TARGET_AVX512F\")])\n+\n+(define_expand \"reduc_plus_scal_<mode>\"\n+ [(plus:REDUC_PLUS_MODE\n+   (match_operand:<ssescalarmode> 0 \"register_operand\")\n+   (match_operand:REDUC_PLUS_MODE 1 \"register_operand\"))]\n+ \"\"\n+{\n+  rtx tmp = gen_reg_rtx (<ssehalfvecmode>mode);\n+  emit_insn (gen_vec_extract_hi_<mode> (tmp, operands[1]));\n+  rtx tmp2 = gen_reg_rtx (<ssehalfvecmode>mode);\n+  emit_insn (gen_add<ssehalfvecmodelower>3\n+    (tmp2, tmp, gen_lowpart (<ssehalfvecmode>mode, operands[1])));\n+  emit_insn (gen_reduc_plus_scal_<ssehalfvecmodelower> (operands[0], tmp2));\n   DONE;\n })\n \n-(define_expand \"reduc_plus_scal_v8sf\"\n-  [(match_operand:SF 0 \"register_operand\")\n-   (match_operand:V8SF 1 \"register_operand\")]\n-  \"TARGET_AVX\"\n-{\n-  rtx tmp = gen_reg_rtx (V8SFmode);\n-  rtx tmp2 = gen_reg_rtx (V8SFmode);\n-  rtx vec_res = gen_reg_rtx (V8SFmode);\n-  emit_insn (gen_avx_haddv8sf3 (tmp, operands[1], operands[1]));\n-  emit_insn (gen_avx_haddv8sf3 (tmp2, tmp, tmp));\n-  emit_insn (gen_avx_vperm2f128v8sf3 (tmp, tmp2, tmp2, GEN_INT (1)));\n-  emit_insn (gen_addv8sf3 (vec_res, tmp, tmp2));\n-  emit_insn (gen_vec_extractv8sfsf (operands[0], vec_res, const0_rtx));\n-  DONE;\n-})\n+;; Modes handled by reduc_sm{in,ax}* patterns.\n+(define_mode_iterator REDUC_SSE_SMINMAX_MODE\n+  [(V4SF \"TARGET_SSE\") (V2DF \"TARGET_SSE\")\n+   (V2DI \"TARGET_SSE\") (V4SI \"TARGET_SSE\") (V8HI \"TARGET_SSE\")\n+   (V16QI \"TARGET_SSE\")])\n \n-(define_expand \"reduc_plus_scal_v4sf\"\n-  [(match_operand:SF 0 \"register_operand\")\n-   (match_operand:V4SF 1 \"register_operand\")]\n-  \"TARGET_SSE\"\n+(define_expand \"reduc_<code>_scal_<mode>\"\n+  [(smaxmin:REDUC_SSE_SMINMAX_MODE\n+     (match_operand:<ssescalarmode> 0 \"register_operand\")\n+     (match_operand:REDUC_SSE_SMINMAX_MODE 1 \"register_operand\"))]\n+  \"\"\n {\n-  rtx vec_res = gen_reg_rtx (V4SFmode);\n-  if (TARGET_SSE3)\n-    {\n-      rtx tmp = gen_reg_rtx (V4SFmode);\n-      emit_insn (gen_sse3_haddv4sf3 (tmp, operands[1], operands[1]));\n-      emit_insn (gen_sse3_haddv4sf3 (vec_res, tmp, tmp));\n-    }\n-  else\n-    ix86_expand_reduc (gen_addv4sf3, vec_res, operands[1]);\n-  emit_insn (gen_vec_extractv4sfsf (operands[0], vec_res, const0_rtx));\n+  rtx tmp = gen_reg_rtx (<MODE>mode);\n+  ix86_expand_reduc (gen_<code><mode>3, tmp, operands[1]);\n+  emit_insn (gen_vec_extract<mode><ssescalarmodelower> (operands[0], tmp,\n+\t\t\t\t\t\t\tconst0_rtx));\n   DONE;\n })\n \n-;; Modes handled by reduc_sm{in,ax}* patterns.\n (define_mode_iterator REDUC_SMINMAX_MODE\n   [(V32QI \"TARGET_AVX2\") (V16HI \"TARGET_AVX2\")\n    (V8SI \"TARGET_AVX2\") (V4DI \"TARGET_AVX2\")\n    (V8SF \"TARGET_AVX\") (V4DF \"TARGET_AVX\")\n-   (V4SF \"TARGET_SSE\") (V64QI \"TARGET_AVX512BW\")\n+   (V64QI \"TARGET_AVX512BW\")\n    (V32HI \"TARGET_AVX512BW\") (V16SI \"TARGET_AVX512F\")\n    (V8DI \"TARGET_AVX512F\") (V16SF \"TARGET_AVX512F\")\n    (V8DF \"TARGET_AVX512F\")])\n@@ -2559,10 +2526,12 @@\n      (match_operand:REDUC_SMINMAX_MODE 1 \"register_operand\"))]\n   \"\"\n {\n-  rtx tmp = gen_reg_rtx (<MODE>mode);\n-  ix86_expand_reduc (gen_<code><mode>3, tmp, operands[1]);\n-  emit_insn (gen_vec_extract<mode><ssescalarmodelower> (operands[0], tmp,\n-\t\t\t\t\t\t\tconst0_rtx));\n+  rtx tmp = gen_reg_rtx (<ssehalfvecmode>mode);\n+  emit_insn (gen_vec_extract_hi_<mode> (tmp, operands[1]));\n+  rtx tmp2 = gen_reg_rtx (<ssehalfvecmode>mode);\n+  emit_insn (gen_<code><ssehalfvecmodelower>3\n+    (tmp2, tmp, gen_lowpart (<ssehalfvecmode>mode, operands[1])));\n+  emit_insn (gen_reduc_<code>_scal_<ssehalfvecmodelower> (operands[0], tmp2));\n   DONE;\n })\n \n@@ -2572,10 +2541,12 @@\n      (match_operand:VI_AVX512BW 1 \"register_operand\"))]\n   \"TARGET_AVX512F\"\n {\n-  rtx tmp = gen_reg_rtx (<MODE>mode);\n-  ix86_expand_reduc (gen_<code><mode>3, tmp, operands[1]);\n-  emit_insn (gen_vec_extract<mode><ssescalarmodelower> (operands[0], tmp,\n-  \t\t\t\t\t\t\tconst0_rtx));\n+  rtx tmp = gen_reg_rtx (<ssehalfvecmode>mode);\n+  emit_insn (gen_vec_extract_hi_<mode> (tmp, operands[1]));\n+  rtx tmp2 = gen_reg_rtx (<ssehalfvecmode>mode);\n+  emit_insn (gen_<code><ssehalfvecmodelower>3\n+    (tmp2, tmp, gen_lowpart (<ssehalfvecmode>mode, operands[1])));\n+  emit_insn (gen_reduc_<code>_scal_<ssehalfvecmodelower> (operands[0], tmp2));\n   DONE;\n })\n \n@@ -2585,10 +2556,15 @@\n      (match_operand:VI_256 1 \"register_operand\"))]\n   \"TARGET_AVX2\"\n {\n-  rtx tmp = gen_reg_rtx (<MODE>mode);\n-  ix86_expand_reduc (gen_<code><mode>3, tmp, operands[1]);\n-  emit_insn (gen_vec_extract<mode><ssescalarmodelower> (operands[0], tmp,\n-\t\t\t\t\t\t\tconst0_rtx));\n+  rtx tmp = gen_reg_rtx (<ssehalfvecmode>mode);\n+  emit_insn (gen_vec_extract_hi_<mode> (tmp, operands[1]));\n+  rtx tmp2 = gen_reg_rtx (<ssehalfvecmode>mode);\n+  emit_insn (gen_<code><ssehalfvecmodelower>3\n+    (tmp2, tmp, gen_lowpart (<ssehalfvecmode>mode, operands[1])));\n+  rtx tmp3 = gen_reg_rtx (<ssehalfvecmode>mode);\n+  ix86_expand_reduc (gen_<code><ssehalfvecmodelower>3, tmp3, tmp2);\n+  emit_insn (gen_vec_extract<ssehalfvecmodelower><ssescalarmodelower>\n+\t\t(operands[0], tmp3, const0_rtx));\n   DONE;\n })\n "}]}