{"sha": "805903b5251042186344830f35161eaec7fedb69", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODA1OTAzYjUyNTEwNDIxODYzNDQ4MzBmMzUxNjFlYWVjN2ZlZGI2OQ==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2008-12-22T23:34:07Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2008-12-22T23:34:07Z"}, "message": "re PR target/38488 (x86_64 generates much larger and slightly slower code for memset)\n\n\t* config/i386/i386.c (expand_setmem_via_rep_stos): Add ORIG_VALUE\n\targument.  If ORIG_VALUE is const0_rtx and COUNT is constant,\n\tset MEM_SIZE on DESTMEM.\n\t(ix86_expand_setmem): Adjust callers.\n\n\tPR target/38488\n\t* expr.h (get_mem_align_offset): New prototype.\n\t* emit-rtl.c (get_mem_align_offset): New function.\n\t* config/i386/i386.c (expand_movmem_via_rep_mov): Set MEM_SIZE correctly.\n\t(expand_constant_movmem_prologue, expand_constant_setmem_prologue):\n\tNew functions.\n\t(ix86_expand_movmem): Optimize if COUNT_EXP\n\tis constant, desired_align > align and dst & (desired_align - 1)\n\tis computable at compile time.\n\t(ix86_expand_setmem): Likewise.\n\n\t* builtins.c (get_memory_rtx): Try to derive MEM_ATTRS from not yet\n\tresolved SAVE_EXPR or POINTER_PLUS_EXPR.\n\nFrom-SVN: r142891", "tree": {"sha": "5971e3d6c2cd233e84f89d0395f6d5c1beecc681", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/5971e3d6c2cd233e84f89d0395f6d5c1beecc681"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/805903b5251042186344830f35161eaec7fedb69", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/805903b5251042186344830f35161eaec7fedb69", "html_url": "https://github.com/Rust-GCC/gccrs/commit/805903b5251042186344830f35161eaec7fedb69", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/805903b5251042186344830f35161eaec7fedb69/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "d797a4ed797b6ba140e7fc9059835b74335cbcf5", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d797a4ed797b6ba140e7fc9059835b74335cbcf5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d797a4ed797b6ba140e7fc9059835b74335cbcf5"}], "stats": {"total": 390, "additions": 359, "deletions": 31}, "files": [{"sha": "61b5a807666614c7334a1ad6b7889dd1308a8225", "filename": "gcc/ChangeLog", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/805903b5251042186344830f35161eaec7fedb69/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/805903b5251042186344830f35161eaec7fedb69/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=805903b5251042186344830f35161eaec7fedb69", "patch": "@@ -1,3 +1,24 @@\n+2008-12-23  Jakub Jelinek  <jakub@redhat.com>\n+\n+\t* config/i386/i386.c (expand_setmem_via_rep_stos): Add ORIG_VALUE\n+\targument.  If ORIG_VALUE is const0_rtx and COUNT is constant,\n+\tset MEM_SIZE on DESTMEM.\n+\t(ix86_expand_setmem): Adjust callers.\n+\n+\tPR target/38488\n+\t* expr.h (get_mem_align_offset): New prototype.\n+\t* emit-rtl.c (get_mem_align_offset): New function.\n+\t* config/i386/i386.c (expand_movmem_via_rep_mov): Set MEM_SIZE correctly.\n+\t(expand_constant_movmem_prologue, expand_constant_setmem_prologue):\n+\tNew functions.\n+\t(ix86_expand_movmem): Optimize if COUNT_EXP\n+\tis constant, desired_align > align and dst & (desired_align - 1)\n+\tis computable at compile time.\n+\t(ix86_expand_setmem): Likewise.\n+\n+\t* builtins.c (get_memory_rtx): Try to derive MEM_ATTRS from not yet\n+\tresolved SAVE_EXPR or POINTER_PLUS_EXPR.\n+\n 2008-12-22  Uros Bizjak  <ubizjak@gmail.com>\n \n \t* config/alpha/alpha.h (ASM_OUTPUT_EXTERNAL): New macro."}, {"sha": "607d7dd36b1e699f9b256de7d149ddf03e9b718b", "filename": "gcc/builtins.c", "status": "modified", "additions": 21, "deletions": 3, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/805903b5251042186344830f35161eaec7fedb69/gcc%2Fbuiltins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/805903b5251042186344830f35161eaec7fedb69/gcc%2Fbuiltins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fbuiltins.c?ref=805903b5251042186344830f35161eaec7fedb69", "patch": "@@ -1094,8 +1094,17 @@ expand_builtin_prefetch (tree exp)\n static rtx\n get_memory_rtx (tree exp, tree len)\n {\n-  rtx addr = expand_expr (exp, NULL_RTX, ptr_mode, EXPAND_NORMAL);\n-  rtx mem = gen_rtx_MEM (BLKmode, memory_address (BLKmode, addr));\n+  tree orig_exp = exp;\n+  rtx addr, mem;\n+  HOST_WIDE_INT off;\n+\n+  /* When EXP is not resolved SAVE_EXPR, MEM_ATTRS can be still derived\n+     from its expression, for expr->a.b only <variable>.a.b is recorded.  */\n+  if (TREE_CODE (exp) == SAVE_EXPR && !SAVE_EXPR_RESOLVED_P (exp))\n+    exp = TREE_OPERAND (exp, 0);\n+\n+  addr = expand_expr (orig_exp, NULL_RTX, ptr_mode, EXPAND_NORMAL);\n+  mem = gen_rtx_MEM (BLKmode, memory_address (BLKmode, addr));\n \n   /* Get an expression we can use to find the attributes to assign to MEM.\n      If it is an ADDR_EXPR, use the operand.  Otherwise, dereference it if\n@@ -1104,7 +1113,13 @@ get_memory_rtx (tree exp, tree len)\n \t && POINTER_TYPE_P (TREE_TYPE (TREE_OPERAND (exp, 0))))\n     exp = TREE_OPERAND (exp, 0);\n \n-  if (TREE_CODE (exp) == ADDR_EXPR)\n+  off = 0;\n+  if (TREE_CODE (exp) == POINTER_PLUS_EXPR\n+      && TREE_CODE (TREE_OPERAND (exp, 0)) == ADDR_EXPR\n+      && host_integerp (TREE_OPERAND (exp, 1), 0)\n+      && (off = tree_low_cst (TREE_OPERAND (exp, 1), 0)) > 0)\n+    exp = TREE_OPERAND (TREE_OPERAND (exp, 0), 0);\n+  else if (TREE_CODE (exp) == ADDR_EXPR)\n     exp = TREE_OPERAND (exp, 0);\n   else if (POINTER_TYPE_P (TREE_TYPE (exp)))\n     exp = build1 (INDIRECT_REF, TREE_TYPE (TREE_TYPE (exp)), exp);\n@@ -1118,6 +1133,9 @@ get_memory_rtx (tree exp, tree len)\n     {\n       set_mem_attributes (mem, exp, 0);\n \n+      if (off)\n+\tmem = adjust_automodify_address_nv (mem, BLKmode, NULL, off);\n+\n       /* Allow the string and memory builtins to overflow from one\n \t field into another, see http://gcc.gnu.org/PR23561.\n \t Thus avoid COMPONENT_REFs in MEM_EXPR unless we know the whole"}, {"sha": "9ad964be78d85ea412598c5c5ce69a1fae4b2555", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 228, "deletions": 28, "changes": 256, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/805903b5251042186344830f35161eaec7fedb69/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/805903b5251042186344830f35161eaec7fedb69/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=805903b5251042186344830f35161eaec7fedb69", "patch": "@@ -16636,6 +16636,22 @@ expand_movmem_via_rep_mov (rtx destmem, rtx srcmem,\n       destexp = gen_rtx_PLUS (Pmode, destptr, countreg);\n       srcexp = gen_rtx_PLUS (Pmode, srcptr, countreg);\n     }\n+  if (CONST_INT_P (count))\n+    {\n+      count = GEN_INT (INTVAL (count)\n+\t\t       & ~((HOST_WIDE_INT) GET_MODE_SIZE (mode) - 1));\n+      destmem = shallow_copy_rtx (destmem);\n+      srcmem = shallow_copy_rtx (srcmem);\n+      set_mem_size (destmem, count);\n+      set_mem_size (srcmem, count);\n+    }\n+  else\n+    {\n+      if (MEM_SIZE (destmem))\n+\tset_mem_size (destmem, NULL_RTX);\n+      if (MEM_SIZE (srcmem))\n+\tset_mem_size (srcmem, NULL_RTX);\n+    }\n   emit_insn (gen_rep_mov (destptr, destmem, srcptr, srcmem, countreg,\n \t\t\t  destexp, srcexp));\n }\n@@ -16644,8 +16660,8 @@ expand_movmem_via_rep_mov (rtx destmem, rtx srcmem,\n    Arguments have same meaning as for previous function */\n static void\n expand_setmem_via_rep_stos (rtx destmem, rtx destptr, rtx value,\n-\t\t\t    rtx count,\n-\t\t\t    enum machine_mode mode)\n+\t\t\t    rtx count, enum machine_mode mode,\n+\t\t\t    rtx orig_value)\n {\n   rtx destexp;\n   rtx countreg;\n@@ -16662,6 +16678,15 @@ expand_setmem_via_rep_stos (rtx destmem, rtx destptr, rtx value,\n     }\n   else\n     destexp = gen_rtx_PLUS (Pmode, destptr, countreg);\n+  if (orig_value == const0_rtx && CONST_INT_P (count))\n+    {\n+      count = GEN_INT (INTVAL (count)\n+\t\t       & ~((HOST_WIDE_INT) GET_MODE_SIZE (mode) - 1));\n+      destmem = shallow_copy_rtx (destmem);\n+      set_mem_size (destmem, count);\n+    }\n+  else if (MEM_SIZE (destmem))\n+    set_mem_size (destmem, NULL_RTX);\n   emit_insn (gen_rep_stos (destptr, countreg, destmem, value, destexp));\n }\n \n@@ -16995,6 +17020,85 @@ expand_movmem_prologue (rtx destmem, rtx srcmem,\n   gcc_assert (desired_alignment <= 8);\n }\n \n+/* Copy enough from DST to SRC to align DST known to DESIRED_ALIGN.\n+   ALIGN_BYTES is how many bytes need to be copied.  */\n+static rtx\n+expand_constant_movmem_prologue (rtx dst, rtx *srcp, rtx destreg, rtx srcreg,\n+\t\t\t\t int desired_align, int align_bytes)\n+{\n+  rtx src = *srcp;\n+  rtx src_size, dst_size;\n+  int off = 0;\n+  int src_align_bytes = get_mem_align_offset (src, desired_align * BITS_PER_UNIT);\n+  if (src_align_bytes >= 0)\n+    src_align_bytes = desired_align - src_align_bytes;\n+  src_size = MEM_SIZE (src);\n+  dst_size = MEM_SIZE (dst);\n+  if (align_bytes & 1)\n+    {\n+      dst = adjust_automodify_address_nv (dst, QImode, destreg, 0);\n+      src = adjust_automodify_address_nv (src, QImode, srcreg, 0);\n+      off = 1;\n+      emit_insn (gen_strmov (destreg, dst, srcreg, src));\n+    }\n+  if (align_bytes & 2)\n+    {\n+      dst = adjust_automodify_address_nv (dst, HImode, destreg, off);\n+      src = adjust_automodify_address_nv (src, HImode, srcreg, off);\n+      if (MEM_ALIGN (dst) < 2 * BITS_PER_UNIT)\n+\tset_mem_align (dst, 2 * BITS_PER_UNIT);\n+      if (src_align_bytes >= 0\n+\t  && (src_align_bytes & 1) == (align_bytes & 1)\n+\t  && MEM_ALIGN (src) < 2 * BITS_PER_UNIT)\n+\tset_mem_align (src, 2 * BITS_PER_UNIT);\n+      off = 2;\n+      emit_insn (gen_strmov (destreg, dst, srcreg, src));\n+    }\n+  if (align_bytes & 4)\n+    {\n+      dst = adjust_automodify_address_nv (dst, SImode, destreg, off);\n+      src = adjust_automodify_address_nv (src, SImode, srcreg, off);\n+      if (MEM_ALIGN (dst) < 4 * BITS_PER_UNIT)\n+\tset_mem_align (dst, 4 * BITS_PER_UNIT);\n+      if (src_align_bytes >= 0)\n+\t{\n+\t  unsigned int src_align = 0;\n+\t  if ((src_align_bytes & 3) == (align_bytes & 3))\n+\t    src_align = 4;\n+\t  else if ((src_align_bytes & 1) == (align_bytes & 1))\n+\t    src_align = 2;\n+\t  if (MEM_ALIGN (src) < src_align * BITS_PER_UNIT)\n+\t    set_mem_align (src, src_align * BITS_PER_UNIT);\n+\t}\n+      off = 4;\n+      emit_insn (gen_strmov (destreg, dst, srcreg, src));\n+    }\n+  dst = adjust_automodify_address_nv (dst, BLKmode, destreg, off);\n+  src = adjust_automodify_address_nv (src, BLKmode, srcreg, off);\n+  if (MEM_ALIGN (dst) < (unsigned int) desired_align * BITS_PER_UNIT)\n+    set_mem_align (dst, desired_align * BITS_PER_UNIT);\n+  if (src_align_bytes >= 0)\n+    {\n+      unsigned int src_align = 0;\n+      if ((src_align_bytes & 7) == (align_bytes & 7))\n+\tsrc_align = 8;\n+      else if ((src_align_bytes & 3) == (align_bytes & 3))\n+\tsrc_align = 4;\n+      else if ((src_align_bytes & 1) == (align_bytes & 1))\n+\tsrc_align = 2;\n+      if (src_align > (unsigned int) desired_align)\n+\tsrc_align = desired_align;\n+      if (MEM_ALIGN (src) < src_align * BITS_PER_UNIT)\n+\tset_mem_align (src, src_align * BITS_PER_UNIT);\n+    }\n+  if (dst_size)\n+    set_mem_size (dst, GEN_INT (INTVAL (dst_size) - align_bytes));\n+  if (src_size)\n+    set_mem_size (dst, GEN_INT (INTVAL (src_size) - align_bytes));\n+  *srcp = src;\n+  return dst;\n+}\n+\n /* Set enough from DEST to align DEST known to by aligned by ALIGN to\n    DESIRED_ALIGNMENT.  */\n static void\n@@ -17031,6 +17135,47 @@ expand_setmem_prologue (rtx destmem, rtx destptr, rtx value, rtx count,\n   gcc_assert (desired_alignment <= 8);\n }\n \n+/* Set enough from DST to align DST known to by aligned by ALIGN to\n+   DESIRED_ALIGN.  ALIGN_BYTES is how many bytes need to be stored.  */\n+static rtx\n+expand_constant_setmem_prologue (rtx dst, rtx destreg, rtx value,\n+\t\t\t\t int desired_align, int align_bytes)\n+{\n+  int off = 0;\n+  rtx dst_size = MEM_SIZE (dst);\n+  if (align_bytes & 1)\n+    {\n+      dst = adjust_automodify_address_nv (dst, QImode, destreg, 0);\n+      off = 1;\n+      emit_insn (gen_strset (destreg, dst,\n+\t\t\t     gen_lowpart (QImode, value)));\n+    }\n+  if (align_bytes & 2)\n+    {\n+      dst = adjust_automodify_address_nv (dst, HImode, destreg, off);\n+      if (MEM_ALIGN (dst) < 2 * BITS_PER_UNIT)\n+\tset_mem_align (dst, 2 * BITS_PER_UNIT);\n+      off = 2;\n+      emit_insn (gen_strset (destreg, dst,\n+\t\t\t     gen_lowpart (HImode, value)));\n+    }\n+  if (align_bytes & 4)\n+    {\n+      dst = adjust_automodify_address_nv (dst, SImode, destreg, off);\n+      if (MEM_ALIGN (dst) < 4 * BITS_PER_UNIT)\n+\tset_mem_align (dst, 4 * BITS_PER_UNIT);\n+      off = 4;\n+      emit_insn (gen_strset (destreg, dst,\n+\t\t\t     gen_lowpart (SImode, value)));\n+    }\n+  dst = adjust_automodify_address_nv (dst, BLKmode, destreg, off);\n+  if (MEM_ALIGN (dst) < (unsigned int) desired_align * BITS_PER_UNIT)\n+    set_mem_align (dst, desired_align * BITS_PER_UNIT);\n+  if (dst_size)\n+    set_mem_size (dst, GEN_INT (INTVAL (dst_size) - align_bytes));\n+  return dst;\n+}\n+\n /* Given COUNT and EXPECTED_SIZE, decide on codegen of string operation.  */\n static enum stringop_alg\n decide_alg (HOST_WIDE_INT count, HOST_WIDE_INT expected_size, bool memset,\n@@ -17262,7 +17407,7 @@ ix86_expand_movmem (rtx dst, rtx src, rtx count_exp, rtx align_exp,\n   unsigned HOST_WIDE_INT count = 0;\n   HOST_WIDE_INT expected_size = -1;\n   int size_needed = 0, epilogue_size_needed;\n-  int desired_align = 0;\n+  int desired_align = 0, align_bytes = 0;\n   enum stringop_alg alg;\n   int dynamic_check;\n   bool need_zero_guard = false;\n@@ -17273,6 +17418,11 @@ ix86_expand_movmem (rtx dst, rtx src, rtx count_exp, rtx align_exp,\n   if (CONST_INT_P (expected_align_exp)\n       && INTVAL (expected_align_exp) > align)\n     align = INTVAL (expected_align_exp);\n+  /* ALIGN is the minimum of destination and source alignment, but we care here\n+     just about destination alignment.  */\n+  else if (MEM_ALIGN (dst) > (unsigned HOST_WIDE_INT) align * BITS_PER_UNIT)\n+    align = MEM_ALIGN (dst) / BITS_PER_UNIT;\n+\n   if (CONST_INT_P (count_exp))\n     count = expected_size = INTVAL (count_exp);\n   if (CONST_INT_P (expected_size_exp) && count == 0)\n@@ -17332,7 +17482,20 @@ ix86_expand_movmem (rtx dst, rtx src, rtx count_exp, rtx align_exp,\n \n   /* Alignment code needs count to be in register.  */\n   if (CONST_INT_P (count_exp) && desired_align > align)\n-    count_exp = force_reg (counter_mode (count_exp), count_exp);\n+    {\n+      if (INTVAL (count_exp) > desired_align\n+\t  && INTVAL (count_exp) > size_needed)\n+\t{\n+\t  align_bytes\n+\t    = get_mem_align_offset (dst, desired_align * BITS_PER_UNIT);\n+\t  if (align_bytes <= 0)\n+\t    align_bytes = 0;\n+\t  else\n+\t    align_bytes = desired_align - align_bytes;\n+\t}\n+      if (align_bytes == 0)\n+\tcount_exp = force_reg (counter_mode (count_exp), count_exp);\n+    }\n   gcc_assert (desired_align >= 1 && align >= 1);\n \n   /* Ensure that alignment prologue won't copy past end of block.  */\n@@ -17391,14 +17554,26 @@ ix86_expand_movmem (rtx dst, rtx src, rtx count_exp, rtx align_exp,\n \n   if (desired_align > align)\n     {\n-      /* Except for the first move in epilogue, we no longer know\n-         constant offset in aliasing info.  It don't seems to worth\n-\t the pain to maintain it for the first move, so throw away\n-\t the info early.  */\n-      src = change_address (src, BLKmode, srcreg);\n-      dst = change_address (dst, BLKmode, destreg);\n-      expand_movmem_prologue (dst, src, destreg, srcreg, count_exp, align,\n-\t\t\t      desired_align);\n+      if (align_bytes == 0)\n+\t{\n+\t  /* Except for the first move in epilogue, we no longer know\n+\t     constant offset in aliasing info.  It don't seems to worth\n+\t     the pain to maintain it for the first move, so throw away\n+\t     the info early.  */\n+\t  src = change_address (src, BLKmode, srcreg);\n+\t  dst = change_address (dst, BLKmode, destreg);\n+\t  expand_movmem_prologue (dst, src, destreg, srcreg, count_exp, align,\n+\t\t\t\t  desired_align);\n+\t}\n+      else\n+\t{\n+\t  /* If we know how many bytes need to be stored before dst is\n+\t     sufficiently aligned, maintain aliasing info accurately.  */\n+\t  dst = expand_constant_movmem_prologue (dst, &src, destreg, srcreg,\n+\t\t\t\t\t\t desired_align, align_bytes);\n+\t  count_exp = plus_constant (count_exp, -align_bytes);\n+\t  count -= align_bytes;\n+\t}\n       if (need_zero_guard && !count)\n \t{\n \t  /* It is possible that we copied enough so the main loop will not\n@@ -17607,7 +17782,7 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n   unsigned HOST_WIDE_INT count = 0;\n   HOST_WIDE_INT expected_size = -1;\n   int size_needed = 0, epilogue_size_needed;\n-  int desired_align = 0;\n+  int desired_align = 0, align_bytes = 0;\n   enum stringop_alg alg;\n   rtx promoted_val = NULL;\n   bool force_loopy_epilogue = false;\n@@ -17678,10 +17853,23 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n   /* Alignment code needs count to be in register.  */\n   if (CONST_INT_P (count_exp) && desired_align > align)\n     {\n-      enum machine_mode mode = SImode;\n-      if (TARGET_64BIT && (count & ~0xffffffff))\n-\tmode = DImode;\n-      count_exp = force_reg (mode, count_exp);\n+      if (INTVAL (count_exp) > desired_align\n+\t  && INTVAL (count_exp) > size_needed)\n+\t{\n+\t  align_bytes\n+\t    = get_mem_align_offset (dst, desired_align * BITS_PER_UNIT);\n+\t  if (align_bytes <= 0)\n+\t    align_bytes = 0;\n+\t  else\n+\t    align_bytes = desired_align - align_bytes;\n+\t}\n+      if (align_bytes == 0)\n+\t{\n+\t  enum machine_mode mode = SImode;\n+\t  if (TARGET_64BIT && (count & ~0xffffffff))\n+\t    mode = DImode;\n+\t  count_exp = force_reg (mode, count_exp);\n+\t}\n     }\n   /* Do the cheap promotion to allow better CSE across the\n      main loop and epilogue (ie one load of the big constant in the\n@@ -17693,7 +17881,7 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n   if (size_needed > 1 || (desired_align > 1 && desired_align > align))\n     {\n       epilogue_size_needed = MAX (size_needed - 1, desired_align - align);\n-      /* Epilogue always copies COUNT_EXP & EPILOGUE_SIZE_NEEDED bytes.\n+      /* Epilogue always copies COUNT_EXP & (EPILOGUE_SIZE_NEEDED - 1) bytes.\n \t Make sure it is power of 2.  */\n       epilogue_size_needed = smallest_pow2_greater_than (epilogue_size_needed);\n \n@@ -17736,13 +17924,25 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n \n   if (desired_align > align)\n     {\n-      /* Except for the first move in epilogue, we no longer know\n-         constant offset in aliasing info.  It don't seems to worth\n-\t the pain to maintain it for the first move, so throw away\n-\t the info early.  */\n-      dst = change_address (dst, BLKmode, destreg);\n-      expand_setmem_prologue (dst, destreg, promoted_val, count_exp, align,\n-\t\t\t      desired_align);\n+      if (align_bytes == 0)\n+\t{\n+\t  /* Except for the first move in epilogue, we no longer know\n+\t     constant offset in aliasing info.  It don't seems to worth\n+\t     the pain to maintain it for the first move, so throw away\n+\t     the info early.  */\n+\t  dst = change_address (dst, BLKmode, destreg);\n+\t  expand_setmem_prologue (dst, destreg, promoted_val, count_exp, align,\n+\t\t\t\t  desired_align);\n+\t}\n+      else\n+\t{\n+\t  /* If we know how many bytes need to be stored before dst is\n+\t     sufficiently aligned, maintain aliasing info accurately.  */\n+\t  dst = expand_constant_setmem_prologue (dst, destreg, promoted_val,\n+\t\t\t\t\t\t desired_align, align_bytes);\n+\t  count_exp = plus_constant (count_exp, -align_bytes);\n+\t  count -= align_bytes;\n+\t}\n       if (need_zero_guard && !count)\n \t{\n \t  /* It is possible that we copied enough so the main loop will not\n@@ -17785,15 +17985,15 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n       break;\n     case rep_prefix_8_byte:\n       expand_setmem_via_rep_stos (dst, destreg, promoted_val, count_exp,\n-\t\t\t\t  DImode);\n+\t\t\t\t  DImode, val_exp);\n       break;\n     case rep_prefix_4_byte:\n       expand_setmem_via_rep_stos (dst, destreg, promoted_val, count_exp,\n-\t\t\t\t  SImode);\n+\t\t\t\t  SImode, val_exp);\n       break;\n     case rep_prefix_1_byte:\n       expand_setmem_via_rep_stos (dst, destreg, promoted_val, count_exp,\n-\t\t\t\t  QImode);\n+\t\t\t\t  QImode, val_exp);\n       break;\n     }\n   /* Adjust properly the offset of src and dest memory for aliasing.  */"}, {"sha": "830ce1dc91050b2c3aa177cbf2f28d863a2320c2", "filename": "gcc/emit-rtl.c", "status": "modified", "additions": 84, "deletions": 0, "changes": 84, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/805903b5251042186344830f35161eaec7fedb69/gcc%2Femit-rtl.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/805903b5251042186344830f35161eaec7fedb69/gcc%2Femit-rtl.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Femit-rtl.c?ref=805903b5251042186344830f35161eaec7fedb69", "patch": "@@ -1490,6 +1490,90 @@ mem_expr_equal_p (const_tree expr1, const_tree expr2)\n   return 0;\n }\n \n+/* Return OFFSET if XEXP (MEM, 0) - OFFSET is known to be ALIGN\n+   bits aligned for 0 <= OFFSET < ALIGN / BITS_PER_UNIT, or\n+   -1 if not known.  */\n+\n+int\n+get_mem_align_offset (rtx mem, int align)\n+{\n+  tree expr;\n+  unsigned HOST_WIDE_INT offset;\n+\n+  /* This function can't use\n+     if (!MEM_EXPR (mem) || !MEM_OFFSET (mem)\n+\t || !CONST_INT_P (MEM_OFFSET (mem))\n+\t || (get_object_alignment (MEM_EXPR (mem), MEM_ALIGN (mem), align)\n+\t     < align))\n+       return -1;\n+     else\n+       return (- INTVAL (MEM_OFFSET (mem))) & (align / BITS_PER_UNIT - 1);\n+     for two reasons:\n+     - COMPONENT_REFs in MEM_EXPR can have NULL first operand,\n+       for <variable>.  get_inner_reference doesn't handle it and\n+       even if it did, the alignment in that case needs to be determined\n+       from DECL_FIELD_CONTEXT's TYPE_ALIGN.\n+     - it would do suboptimal job for COMPONENT_REFs, even if MEM_EXPR\n+       isn't sufficiently aligned, the object it is in might be.  */\n+  gcc_assert (MEM_P (mem));\n+  expr = MEM_EXPR (mem);\n+  if (expr == NULL_TREE\n+      || MEM_OFFSET (mem) == NULL_RTX\n+      || !CONST_INT_P (MEM_OFFSET (mem)))\n+    return -1;\n+\n+  offset = INTVAL (MEM_OFFSET (mem));\n+  if (DECL_P (expr))\n+    {\n+      if (DECL_ALIGN (expr) < align)\n+\treturn -1;\n+    }\n+  else if (INDIRECT_REF_P (expr))\n+    {\n+      if (TYPE_ALIGN (TREE_TYPE (expr)) < (unsigned int) align)\n+\treturn -1;\n+    }\n+  else if (TREE_CODE (expr) == COMPONENT_REF)\n+    {\n+      while (1)\n+\t{\n+\t  tree inner = TREE_OPERAND (expr, 0);\n+\t  tree field = TREE_OPERAND (expr, 1);\n+\t  tree byte_offset = component_ref_field_offset (expr);\n+\t  tree bit_offset = DECL_FIELD_BIT_OFFSET (field);\n+\n+\t  if (!byte_offset\n+\t      || !host_integerp (byte_offset, 1)\n+\t      || !host_integerp (bit_offset, 1))\n+\t    return -1;\n+\n+\t  offset += tree_low_cst (byte_offset, 1);\n+\t  offset += tree_low_cst (bit_offset, 1) / BITS_PER_UNIT;\n+\n+\t  if (inner == NULL_TREE)\n+\t    {\n+\t      if (TYPE_ALIGN (DECL_FIELD_CONTEXT (field))\n+\t\t  < (unsigned int) align)\n+\t\treturn -1;\n+\t      break;\n+\t    }\n+\t  else if (DECL_P (inner))\n+\t    {\n+\t      if (DECL_ALIGN (inner) < align)\n+\t\treturn -1;\n+\t      break;\n+\t    }\n+\t  else if (TREE_CODE (inner) != COMPONENT_REF)\n+\t    return -1;\n+\t  expr = inner;\n+\t}\n+    }\n+  else\n+    return -1;\n+\n+  return offset & ((align / BITS_PER_UNIT) - 1);\n+}\n+\n /* Given REF (a MEM) and T, either the type of X or the expression\n    corresponding to REF, set the memory attributes.  OBJECTP is nonzero\n    if we are making a new object of this type.  BITPOS is nonzero if"}, {"sha": "f002da2e499434f407c8c12cc47d42330357ae43", "filename": "gcc/expr.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/805903b5251042186344830f35161eaec7fedb69/gcc%2Fexpr.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/805903b5251042186344830f35161eaec7fedb69/gcc%2Fexpr.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.h?ref=805903b5251042186344830f35161eaec7fedb69", "patch": "@@ -695,6 +695,11 @@ extern void set_mem_attributes (rtx, tree, int);\n    expecting that it'll be added back in later.  */\n extern void set_mem_attributes_minus_bitpos (rtx, tree, int, HOST_WIDE_INT);\n \n+/* Return OFFSET if XEXP (MEM, 0) - OFFSET is known to be ALIGN\n+   bits aligned for 0 <= OFFSET < ALIGN / BITS_PER_UNIT, or\n+   -1 if not known.  */\n+extern int get_mem_align_offset (rtx, int);\n+\n /* Assemble the static constant template for function entry trampolines.  */\n extern rtx assemble_trampoline_template (void);\n "}]}