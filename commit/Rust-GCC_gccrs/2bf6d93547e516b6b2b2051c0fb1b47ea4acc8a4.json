{"sha": "2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MmJmNmQ5MzU0N2U1MTZiNmIyYjIwNTFjMGZiMWI0N2VhNGFjYzhhNA==", "commit": {"author": {"name": "Martin Liska", "email": "mliska@suse.cz", "date": "2019-05-06T07:18:26Z"}, "committer": {"name": "Martin Liska", "email": "marxin@gcc.gnu.org", "date": "2019-05-06T07:18:26Z"}, "message": "Split i386.c.\n\n2019-05-06  Martin Liska  <mliska@suse.cz>\n\n\t* config/i386/i386-builtins.c: New file.\n\t* config/i386/i386-builtins.h: New file.\n\t* config/i386/i386-expand.c: New file.\n\t* config/i386/i386-expand.h: New file.\n\t* config/i386/i386-features.c: New file.\n\t* config/i386/i386-features.h: New file.\n\t* config/i386/i386-options.c: New file.\n\t* config/i386/i386-options.h: New file.\n\t* config.gcc: Add new files into extra_objs and\n\ttarget_gtfiles.\n\t* config/i386/i386.c: Split content of the file\n\tinto newly introduced files.\n\t* config/i386/i386.h: Declare common variables\n\tand macros.\n\t* config/i386/t-i386: Define dependencies for new files.\n\nFrom-SVN: r270895", "tree": {"sha": "33008ff45c985d258a227c712900ba46336573d2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/33008ff45c985d258a227c712900ba46336573d2"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/comments", "author": {"login": "marxin", "id": 2658545, "node_id": "MDQ6VXNlcjI2NTg1NDU=", "avatar_url": "https://avatars.githubusercontent.com/u/2658545?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marxin", "html_url": "https://github.com/marxin", "followers_url": "https://api.github.com/users/marxin/followers", "following_url": "https://api.github.com/users/marxin/following{/other_user}", "gists_url": "https://api.github.com/users/marxin/gists{/gist_id}", "starred_url": "https://api.github.com/users/marxin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marxin/subscriptions", "organizations_url": "https://api.github.com/users/marxin/orgs", "repos_url": "https://api.github.com/users/marxin/repos", "events_url": "https://api.github.com/users/marxin/events{/privacy}", "received_events_url": "https://api.github.com/users/marxin/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "94adc935af1e1e6a90c0d6127707f7b3a53b692d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/94adc935af1e1e6a90c0d6127707f7b3a53b692d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/94adc935af1e1e6a90c0d6127707f7b3a53b692d"}], "stats": {"total": 92648, "additions": 46639, "deletions": 46009}, "files": [{"sha": "4ca2162072352e1648c1960117af070a47956057", "filename": "gcc/ChangeLog", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "patch": "@@ -1,3 +1,21 @@\n+2019-05-06  Martin Liska  <mliska@suse.cz>\n+\n+\t* config/i386/i386-builtins.c: New file.\n+\t* config/i386/i386-builtins.h: New file.\n+\t* config/i386/i386-expand.c: New file.\n+\t* config/i386/i386-expand.h: New file.\n+\t* config/i386/i386-features.c: New file.\n+\t* config/i386/i386-features.h: New file.\n+\t* config/i386/i386-options.c: New file.\n+\t* config/i386/i386-options.h: New file.\n+\t* config.gcc: Add new files into extra_objs and\n+\ttarget_gtfiles.\n+\t* config/i386/i386.c: Split content of the file\n+\tinto newly introduced files.\n+\t* config/i386/i386.h: Declare common variables\n+\tand macros.\n+\t* config/i386/t-i386: Define dependencies for new files.\n+\n 2019-05-06  Li Jia He  <helijia@linux.ibm.com>\n \n \t* tree-ssa-phiopt.c (two_value_replacement): Fix a typo in parameter"}, {"sha": "5124ea0079296bbd03e773f9f89c29ec476f4f77", "filename": "gcc/config.gcc", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "patch": "@@ -382,7 +382,8 @@ i[34567]86-*-*)\n \tc_target_objs=\"i386-c.o\"\n \tcxx_target_objs=\"i386-c.o\"\n \td_target_objs=\"i386-d.o\"\n-\textra_objs=\"x86-tune-sched.o x86-tune-sched-bd.o x86-tune-sched-atom.o x86-tune-sched-core.o\"\n+\textra_objs=\"x86-tune-sched.o x86-tune-sched-bd.o x86-tune-sched-atom.o x86-tune-sched-core.o i386-options.o i386-builtins.o i386-expand.o i386-features.o\"\n+  target_gtfiles=\"\\$(srcdir)/config/i386/i386-builtins.c \\$(srcdir)/config/i386/i386-expand.c \\$(srcdir)/config/i386/i386-options.c\"\n \textra_options=\"${extra_options} fused-madd.opt\"\n \textra_headers=\"cpuid.h mmintrin.h mm3dnow.h xmmintrin.h emmintrin.h\n \t\t       pmmintrin.h tmmintrin.h ammintrin.h smmintrin.h\n@@ -414,7 +415,8 @@ x86_64-*-*)\n \tcxx_target_objs=\"i386-c.o\"\n \td_target_objs=\"i386-d.o\"\n \textra_options=\"${extra_options} fused-madd.opt\"\n-\textra_objs=\"x86-tune-sched.o x86-tune-sched-bd.o x86-tune-sched-atom.o x86-tune-sched-core.o\"\n+\textra_objs=\"x86-tune-sched.o x86-tune-sched-bd.o x86-tune-sched-atom.o x86-tune-sched-core.o i386-options.o i386-builtins.o i386-expand.o i386-features.o\"\n+  target_gtfiles=\"\\$(srcdir)/config/i386/i386-builtins.c \\$(srcdir)/config/i386/i386-expand.c \\$(srcdir)/config/i386/i386-options.c\"\n \textra_headers=\"cpuid.h mmintrin.h mm3dnow.h xmmintrin.h emmintrin.h\n \t\t       pmmintrin.h tmmintrin.h ammintrin.h smmintrin.h\n \t\t       nmmintrin.h bmmintrin.h fma4intrin.h wmmintrin.h"}, {"sha": "9779727480f9656468796f628171225afc8a39c2", "filename": "gcc/config/i386/i386-builtins.c", "status": "added", "additions": 2539, "deletions": 0, "changes": 2539, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-builtins.c?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "patch": "@@ -0,0 +1,2539 @@\n+/* Copyright (C) 1988-2019 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#define IN_TARGET_CODE 1\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"backend.h\"\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"memmodel.h\"\n+#include \"gimple.h\"\n+#include \"cfghooks.h\"\n+#include \"cfgloop.h\"\n+#include \"df.h\"\n+#include \"tm_p.h\"\n+#include \"stringpool.h\"\n+#include \"expmed.h\"\n+#include \"optabs.h\"\n+#include \"regs.h\"\n+#include \"emit-rtl.h\"\n+#include \"recog.h\"\n+#include \"cgraph.h\"\n+#include \"diagnostic.h\"\n+#include \"cfgbuild.h\"\n+#include \"alias.h\"\n+#include \"fold-const.h\"\n+#include \"attribs.h\"\n+#include \"calls.h\"\n+#include \"stor-layout.h\"\n+#include \"varasm.h\"\n+#include \"output.h\"\n+#include \"insn-attr.h\"\n+#include \"flags.h\"\n+#include \"except.h\"\n+#include \"explow.h\"\n+#include \"expr.h\"\n+#include \"cfgrtl.h\"\n+#include \"common/common-target.h\"\n+#include \"langhooks.h\"\n+#include \"reload.h\"\n+#include \"gimplify.h\"\n+#include \"dwarf2.h\"\n+#include \"tm-constrs.h\"\n+#include \"params.h\"\n+#include \"cselib.h\"\n+#include \"sched-int.h\"\n+#include \"opts.h\"\n+#include \"tree-pass.h\"\n+#include \"context.h\"\n+#include \"pass_manager.h\"\n+#include \"target-globals.h\"\n+#include \"gimple-iterator.h\"\n+#include \"tree-vectorizer.h\"\n+#include \"shrink-wrap.h\"\n+#include \"builtins.h\"\n+#include \"rtl-iter.h\"\n+#include \"tree-iterator.h\"\n+#include \"dbgcnt.h\"\n+#include \"case-cfn-macros.h\"\n+#include \"dojump.h\"\n+#include \"fold-const-call.h\"\n+#include \"tree-vrp.h\"\n+#include \"tree-ssanames.h\"\n+#include \"selftest.h\"\n+#include \"selftest-rtl.h\"\n+#include \"print-rtl.h\"\n+#include \"intl.h\"\n+#include \"ifcvt.h\"\n+#include \"symbol-summary.h\"\n+#include \"ipa-prop.h\"\n+#include \"ipa-fnsummary.h\"\n+#include \"wide-int-bitmask.h\"\n+#include \"tree-vector-builder.h\"\n+#include \"debug.h\"\n+#include \"dwarf2out.h\"\n+#include \"i386-builtins.h\"\n+\n+#undef BDESC\n+#undef BDESC_FIRST\n+#undef BDESC_END\n+\n+/* Macros for verification of enum ix86_builtins order.  */\n+#define BDESC_VERIFY(x, y, z) \\\n+  gcc_checking_assert ((x) == (enum ix86_builtins) ((y) + (z)))\n+#define BDESC_VERIFYS(x, y, z) \\\n+  STATIC_ASSERT ((x) == (enum ix86_builtins) ((y) + (z)))\n+\n+BDESC_VERIFYS (IX86_BUILTIN__BDESC_PCMPESTR_FIRST,\n+\t       IX86_BUILTIN__BDESC_COMI_LAST, 1);\n+BDESC_VERIFYS (IX86_BUILTIN__BDESC_PCMPISTR_FIRST,\n+\t       IX86_BUILTIN__BDESC_PCMPESTR_LAST, 1);\n+BDESC_VERIFYS (IX86_BUILTIN__BDESC_SPECIAL_ARGS_FIRST,\n+\t       IX86_BUILTIN__BDESC_PCMPISTR_LAST, 1);\n+BDESC_VERIFYS (IX86_BUILTIN__BDESC_ARGS_FIRST,\n+\t       IX86_BUILTIN__BDESC_SPECIAL_ARGS_LAST, 1);\n+BDESC_VERIFYS (IX86_BUILTIN__BDESC_ROUND_ARGS_FIRST,\n+\t       IX86_BUILTIN__BDESC_ARGS_LAST, 1);\n+BDESC_VERIFYS (IX86_BUILTIN__BDESC_MULTI_ARG_FIRST,\n+\t       IX86_BUILTIN__BDESC_ROUND_ARGS_LAST, 1);\n+BDESC_VERIFYS (IX86_BUILTIN__BDESC_CET_FIRST,\n+\t       IX86_BUILTIN__BDESC_MULTI_ARG_LAST, 1);\n+BDESC_VERIFYS (IX86_BUILTIN__BDESC_CET_NORMAL_FIRST,\n+\t       IX86_BUILTIN__BDESC_CET_LAST, 1);\n+BDESC_VERIFYS (IX86_BUILTIN_MAX,\n+\t       IX86_BUILTIN__BDESC_CET_NORMAL_LAST, 1);\n+\n+\n+/* Table for the ix86 builtin non-function types.  */\n+static GTY(()) tree ix86_builtin_type_tab[(int) IX86_BT_LAST_CPTR + 1];\n+\n+/* Retrieve an element from the above table, building some of\n+   the types lazily.  */\n+\n+static tree\n+ix86_get_builtin_type (enum ix86_builtin_type tcode)\n+{\n+  unsigned int index;\n+  tree type, itype;\n+\n+  gcc_assert ((unsigned)tcode < ARRAY_SIZE(ix86_builtin_type_tab));\n+\n+  type = ix86_builtin_type_tab[(int) tcode];\n+  if (type != NULL)\n+    return type;\n+\n+  gcc_assert (tcode > IX86_BT_LAST_PRIM);\n+  if (tcode <= IX86_BT_LAST_VECT)\n+    {\n+      machine_mode mode;\n+\n+      index = tcode - IX86_BT_LAST_PRIM - 1;\n+      itype = ix86_get_builtin_type (ix86_builtin_type_vect_base[index]);\n+      mode = ix86_builtin_type_vect_mode[index];\n+\n+      type = build_vector_type_for_mode (itype, mode);\n+    }\n+  else\n+    {\n+      int quals;\n+\n+      index = tcode - IX86_BT_LAST_VECT - 1;\n+      if (tcode <= IX86_BT_LAST_PTR)\n+\tquals = TYPE_UNQUALIFIED;\n+      else\n+\tquals = TYPE_QUAL_CONST;\n+\n+      itype = ix86_get_builtin_type (ix86_builtin_type_ptr_base[index]);\n+      if (quals != TYPE_UNQUALIFIED)\n+\titype = build_qualified_type (itype, quals);\n+\n+      type = build_pointer_type (itype);\n+    }\n+\n+  ix86_builtin_type_tab[(int) tcode] = type;\n+  return type;\n+}\n+\n+/* Table for the ix86 builtin function types.  */\n+static GTY(()) tree ix86_builtin_func_type_tab[(int) IX86_BT_LAST_ALIAS + 1];\n+\n+/* Retrieve an element from the above table, building some of\n+   the types lazily.  */\n+\n+static tree\n+ix86_get_builtin_func_type (enum ix86_builtin_func_type tcode)\n+{\n+  tree type;\n+\n+  gcc_assert ((unsigned)tcode < ARRAY_SIZE (ix86_builtin_func_type_tab));\n+\n+  type = ix86_builtin_func_type_tab[(int) tcode];\n+  if (type != NULL)\n+    return type;\n+\n+  if (tcode <= IX86_BT_LAST_FUNC)\n+    {\n+      unsigned start = ix86_builtin_func_start[(int) tcode];\n+      unsigned after = ix86_builtin_func_start[(int) tcode + 1];\n+      tree rtype, atype, args = void_list_node;\n+      unsigned i;\n+\n+      rtype = ix86_get_builtin_type (ix86_builtin_func_args[start]);\n+      for (i = after - 1; i > start; --i)\n+\t{\n+\t  atype = ix86_get_builtin_type (ix86_builtin_func_args[i]);\n+\t  args = tree_cons (NULL, atype, args);\n+\t}\n+\n+      type = build_function_type (rtype, args);\n+    }\n+  else\n+    {\n+      unsigned index = tcode - IX86_BT_LAST_FUNC - 1;\n+      enum ix86_builtin_func_type icode;\n+\n+      icode = ix86_builtin_func_alias_base[index];\n+      type = ix86_get_builtin_func_type (icode);\n+    }\n+\n+  ix86_builtin_func_type_tab[(int) tcode] = type;\n+  return type;\n+}\n+\n+/* Table for the ix86 builtin decls.  */\n+static GTY(()) tree ix86_builtins[(int) IX86_BUILTIN_MAX];\n+\n+struct builtin_isa ix86_builtins_isa[(int) IX86_BUILTIN_MAX];\n+\n+tree get_ix86_builtin (enum ix86_builtins c)\n+{\n+  return ix86_builtins[c];\n+}\n+\n+/* Bits that can still enable any inclusion of a builtin.  */\n+HOST_WIDE_INT deferred_isa_values = 0;\n+HOST_WIDE_INT deferred_isa_values2 = 0;\n+\n+/* Add an ix86 target builtin function with CODE, NAME and TYPE.  Save the\n+   MASK and MASK2 of which isa_flags and ix86_isa_flags2 to use in the\n+   ix86_builtins_isa array.  Stores the function decl in the ix86_builtins\n+   array.  Returns the function decl or NULL_TREE, if the builtin was not\n+   added.\n+\n+   If the front end has a special hook for builtin functions, delay adding\n+   builtin functions that aren't in the current ISA until the ISA is changed\n+   with function specific optimization.  Doing so, can save about 300K for the\n+   default compiler.  When the builtin is expanded, check at that time whether\n+   it is valid.\n+\n+   If the front end doesn't have a special hook, record all builtins, even if\n+   it isn't an instruction set in the current ISA in case the user uses\n+   function specific options for a different ISA, so that we don't get scope\n+   errors if a builtin is added in the middle of a function scope.  */\n+\n+static inline tree\n+def_builtin (HOST_WIDE_INT mask, HOST_WIDE_INT mask2,\n+\t     const char *name,\n+\t     enum ix86_builtin_func_type tcode,\n+\t     enum ix86_builtins code)\n+{\n+  tree decl = NULL_TREE;\n+\n+  /* An instruction may be 64bit only regardless of ISAs.  */\n+  if (!(mask & OPTION_MASK_ISA_64BIT) || TARGET_64BIT)\n+    {\n+      ix86_builtins_isa[(int) code].isa = mask;\n+      ix86_builtins_isa[(int) code].isa2 = mask2;\n+\n+      mask &= ~OPTION_MASK_ISA_64BIT;\n+\n+      /* Filter out the masks most often ored together with others.  */\n+      if ((mask & ix86_isa_flags & OPTION_MASK_ISA_AVX512VL)\n+\t  && mask != OPTION_MASK_ISA_AVX512VL)\n+\tmask &= ~OPTION_MASK_ISA_AVX512VL;\n+      if ((mask & ix86_isa_flags & OPTION_MASK_ISA_AVX512BW)\n+\t  && mask != OPTION_MASK_ISA_AVX512BW)\n+\tmask &= ~OPTION_MASK_ISA_AVX512BW;\n+\n+      if (((mask2 == 0 || (mask2 & ix86_isa_flags2) != 0)\n+\t   && (mask == 0 || (mask & ix86_isa_flags) != 0))\n+\t  || (lang_hooks.builtin_function\n+\t      == lang_hooks.builtin_function_ext_scope))\n+\t{\n+\t  tree type = ix86_get_builtin_func_type (tcode);\n+\t  decl = add_builtin_function (name, type, code, BUILT_IN_MD,\n+\t\t\t\t       NULL, NULL_TREE);\n+\t  ix86_builtins[(int) code] = decl;\n+\t  ix86_builtins_isa[(int) code].set_and_not_built_p = false;\n+\t}\n+      else\n+\t{\n+\t  /* Just MASK and MASK2 where set_and_not_built_p == true can potentially\n+\t     include a builtin.  */\n+\t  deferred_isa_values |= mask;\n+\t  deferred_isa_values2 |= mask2;\n+\t  ix86_builtins[(int) code] = NULL_TREE;\n+\t  ix86_builtins_isa[(int) code].tcode = tcode;\n+\t  ix86_builtins_isa[(int) code].name = name;\n+\t  ix86_builtins_isa[(int) code].const_p = false;\n+\t  ix86_builtins_isa[(int) code].pure_p = false;\n+\t  ix86_builtins_isa[(int) code].set_and_not_built_p = true;\n+\t}\n+    }\n+\n+  return decl;\n+}\n+\n+/* Like def_builtin, but also marks the function decl \"const\".  */\n+\n+static inline tree\n+def_builtin_const (HOST_WIDE_INT mask, HOST_WIDE_INT mask2, const char *name,\n+\t\t   enum ix86_builtin_func_type tcode, enum ix86_builtins code)\n+{\n+  tree decl = def_builtin (mask, mask2, name, tcode, code);\n+  if (decl)\n+    TREE_READONLY (decl) = 1;\n+  else\n+    ix86_builtins_isa[(int) code].const_p = true;\n+\n+  return decl;\n+}\n+\n+/* Like def_builtin, but also marks the function decl \"pure\".  */\n+\n+static inline tree\n+def_builtin_pure (HOST_WIDE_INT mask, HOST_WIDE_INT mask2, const char *name,\n+\t\t  enum ix86_builtin_func_type tcode, enum ix86_builtins code)\n+{\n+  tree decl = def_builtin (mask, mask2, name, tcode, code);\n+  if (decl)\n+    DECL_PURE_P (decl) = 1;\n+  else\n+    ix86_builtins_isa[(int) code].pure_p = true;\n+\n+  return decl;\n+}\n+\n+/* Add any new builtin functions for a given ISA that may not have been\n+   declared.  This saves a bit of space compared to adding all of the\n+   declarations to the tree, even if we didn't use them.  */\n+\n+void\n+ix86_add_new_builtins (HOST_WIDE_INT isa, HOST_WIDE_INT isa2)\n+{\n+  isa &= ~OPTION_MASK_ISA_64BIT;\n+\n+  if ((isa & deferred_isa_values) == 0\n+      && (isa2 & deferred_isa_values2) == 0)\n+    return;\n+\n+  /* Bits in ISA value can be removed from potential isa values.  */\n+  deferred_isa_values &= ~isa;\n+  deferred_isa_values2 &= ~isa2;\n+\n+  int i;\n+  tree saved_current_target_pragma = current_target_pragma;\n+  current_target_pragma = NULL_TREE;\n+\n+  for (i = 0; i < (int)IX86_BUILTIN_MAX; i++)\n+    {\n+      if (((ix86_builtins_isa[i].isa & isa) != 0\n+\t   || (ix86_builtins_isa[i].isa2 & isa2) != 0)\n+\t  && ix86_builtins_isa[i].set_and_not_built_p)\n+\t{\n+\t  tree decl, type;\n+\n+\t  /* Don't define the builtin again.  */\n+\t  ix86_builtins_isa[i].set_and_not_built_p = false;\n+\n+\t  type = ix86_get_builtin_func_type (ix86_builtins_isa[i].tcode);\n+\t  decl = add_builtin_function_ext_scope (ix86_builtins_isa[i].name,\n+\t\t\t\t\t\t type, i, BUILT_IN_MD, NULL,\n+\t\t\t\t\t\t NULL_TREE);\n+\n+\t  ix86_builtins[i] = decl;\n+\t  if (ix86_builtins_isa[i].const_p)\n+\t    TREE_READONLY (decl) = 1;\n+\t}\n+    }\n+\n+  current_target_pragma = saved_current_target_pragma;\n+}\n+\f\n+/* TM vector builtins.  */\n+\n+/* Reuse the existing x86-specific `struct builtin_description' cause\n+   we're lazy.  Add casts to make them fit.  */\n+static const struct builtin_description bdesc_tm[] =\n+{\n+  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_WM64\", (enum ix86_builtins) BUILT_IN_TM_STORE_M64, UNKNOWN, VOID_FTYPE_PV2SI_V2SI },\n+  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_WaRM64\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAR_M64, UNKNOWN, VOID_FTYPE_PV2SI_V2SI },\n+  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_WaWM64\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAW_M64, UNKNOWN, VOID_FTYPE_PV2SI_V2SI },\n+  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_RM64\", (enum ix86_builtins) BUILT_IN_TM_LOAD_M64, UNKNOWN, V2SI_FTYPE_PCV2SI },\n+  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_RaRM64\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAR_M64, UNKNOWN, V2SI_FTYPE_PCV2SI },\n+  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_RaWM64\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAW_M64, UNKNOWN, V2SI_FTYPE_PCV2SI },\n+  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_RfWM64\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RFW_M64, UNKNOWN, V2SI_FTYPE_PCV2SI },\n+\n+  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_WM128\", (enum ix86_builtins) BUILT_IN_TM_STORE_M128, UNKNOWN, VOID_FTYPE_PV4SF_V4SF },\n+  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_WaRM128\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAR_M128, UNKNOWN, VOID_FTYPE_PV4SF_V4SF },\n+  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_WaWM128\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAW_M128, UNKNOWN, VOID_FTYPE_PV4SF_V4SF },\n+  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_RM128\", (enum ix86_builtins) BUILT_IN_TM_LOAD_M128, UNKNOWN, V4SF_FTYPE_PCV4SF },\n+  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_RaRM128\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAR_M128, UNKNOWN, V4SF_FTYPE_PCV4SF },\n+  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_RaWM128\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAW_M128, UNKNOWN, V4SF_FTYPE_PCV4SF },\n+  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_RfWM128\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RFW_M128, UNKNOWN, V4SF_FTYPE_PCV4SF },\n+\n+  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_WM256\", (enum ix86_builtins) BUILT_IN_TM_STORE_M256, UNKNOWN, VOID_FTYPE_PV8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_WaRM256\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAR_M256, UNKNOWN, VOID_FTYPE_PV8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_WaWM256\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAW_M256, UNKNOWN, VOID_FTYPE_PV8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_RM256\", (enum ix86_builtins) BUILT_IN_TM_LOAD_M256, UNKNOWN, V8SF_FTYPE_PCV8SF },\n+  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_RaRM256\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAR_M256, UNKNOWN, V8SF_FTYPE_PCV8SF },\n+  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_RaWM256\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAW_M256, UNKNOWN, V8SF_FTYPE_PCV8SF },\n+  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_RfWM256\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RFW_M256, UNKNOWN, V8SF_FTYPE_PCV8SF },\n+\n+  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_LM64\", (enum ix86_builtins) BUILT_IN_TM_LOG_M64, UNKNOWN, VOID_FTYPE_PCVOID },\n+  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_LM128\", (enum ix86_builtins) BUILT_IN_TM_LOG_M128, UNKNOWN, VOID_FTYPE_PCVOID },\n+  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_LM256\", (enum ix86_builtins) BUILT_IN_TM_LOG_M256, UNKNOWN, VOID_FTYPE_PCVOID },\n+};\n+\n+/* Initialize the transactional memory vector load/store builtins.  */\n+\n+static void\n+ix86_init_tm_builtins (void)\n+{\n+  enum ix86_builtin_func_type ftype;\n+  const struct builtin_description *d;\n+  size_t i;\n+  tree decl;\n+  tree attrs_load, attrs_type_load, attrs_store, attrs_type_store;\n+  tree attrs_log, attrs_type_log;\n+\n+  if (!flag_tm)\n+    return;\n+\n+  /* If there are no builtins defined, we must be compiling in a\n+     language without trans-mem support.  */\n+  if (!builtin_decl_explicit_p (BUILT_IN_TM_LOAD_1))\n+    return;\n+\n+  /* Use whatever attributes a normal TM load has.  */\n+  decl = builtin_decl_explicit (BUILT_IN_TM_LOAD_1);\n+  attrs_load = DECL_ATTRIBUTES (decl);\n+  attrs_type_load = TYPE_ATTRIBUTES (TREE_TYPE (decl));\n+  /* Use whatever attributes a normal TM store has.  */\n+  decl = builtin_decl_explicit (BUILT_IN_TM_STORE_1);\n+  attrs_store = DECL_ATTRIBUTES (decl);\n+  attrs_type_store = TYPE_ATTRIBUTES (TREE_TYPE (decl));\n+  /* Use whatever attributes a normal TM log has.  */\n+  decl = builtin_decl_explicit (BUILT_IN_TM_LOG);\n+  attrs_log = DECL_ATTRIBUTES (decl);\n+  attrs_type_log = TYPE_ATTRIBUTES (TREE_TYPE (decl));\n+\n+  for (i = 0, d = bdesc_tm;\n+       i < ARRAY_SIZE (bdesc_tm);\n+       i++, d++)\n+    {\n+      if ((d->mask & ix86_isa_flags) != 0\n+\t  || (lang_hooks.builtin_function\n+\t      == lang_hooks.builtin_function_ext_scope))\n+\t{\n+\t  tree type, attrs, attrs_type;\n+\t  enum built_in_function code = (enum built_in_function) d->code;\n+\n+\t  ftype = (enum ix86_builtin_func_type) d->flag;\n+\t  type = ix86_get_builtin_func_type (ftype);\n+\n+\t  if (BUILTIN_TM_LOAD_P (code))\n+\t    {\n+\t      attrs = attrs_load;\n+\t      attrs_type = attrs_type_load;\n+\t    }\n+\t  else if (BUILTIN_TM_STORE_P (code))\n+\t    {\n+\t      attrs = attrs_store;\n+\t      attrs_type = attrs_type_store;\n+\t    }\n+\t  else\n+\t    {\n+\t      attrs = attrs_log;\n+\t      attrs_type = attrs_type_log;\n+\t    }\n+\t  decl = add_builtin_function (d->name, type, code, BUILT_IN_NORMAL,\n+\t\t\t\t       /* The builtin without the prefix for\n+\t\t\t\t\t  calling it directly.  */\n+\t\t\t\t       d->name + strlen (\"__builtin_\"),\n+\t\t\t\t       attrs);\n+\t  /* add_builtin_function() will set the DECL_ATTRIBUTES, now\n+\t     set the TYPE_ATTRIBUTES.  */\n+\t  decl_attributes (&TREE_TYPE (decl), attrs_type, ATTR_FLAG_BUILT_IN);\n+\n+\t  set_builtin_decl (code, decl, false);\n+\t}\n+    }\n+}\n+\n+/* Set up all the MMX/SSE builtins, even builtins for instructions that are not\n+   in the current target ISA to allow the user to compile particular modules\n+   with different target specific options that differ from the command line\n+   options.  */\n+static void\n+ix86_init_mmx_sse_builtins (void)\n+{\n+  const struct builtin_description * d;\n+  enum ix86_builtin_func_type ftype;\n+  size_t i;\n+\n+  /* Add all special builtins with variable number of operands.  */\n+  for (i = 0, d = bdesc_special_args;\n+       i < ARRAY_SIZE (bdesc_special_args);\n+       i++, d++)\n+    {\n+      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_SPECIAL_ARGS_FIRST, i);\n+      if (d->name == 0)\n+\tcontinue;\n+\n+      ftype = (enum ix86_builtin_func_type) d->flag;\n+      def_builtin (d->mask, d->mask2, d->name, ftype, d->code);\n+    }\n+  BDESC_VERIFYS (IX86_BUILTIN__BDESC_SPECIAL_ARGS_LAST,\n+\t\t IX86_BUILTIN__BDESC_SPECIAL_ARGS_FIRST,\n+\t\t ARRAY_SIZE (bdesc_special_args) - 1);\n+\n+  /* Add all builtins with variable number of operands.  */\n+  for (i = 0, d = bdesc_args;\n+       i < ARRAY_SIZE (bdesc_args);\n+       i++, d++)\n+    {\n+      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_ARGS_FIRST, i);\n+      if (d->name == 0)\n+\tcontinue;\n+\n+      ftype = (enum ix86_builtin_func_type) d->flag;\n+      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n+    }\n+  BDESC_VERIFYS (IX86_BUILTIN__BDESC_ARGS_LAST,\n+\t\t IX86_BUILTIN__BDESC_ARGS_FIRST,\n+\t\t ARRAY_SIZE (bdesc_args) - 1);\n+\n+  /* Add all builtins with rounding.  */\n+  for (i = 0, d = bdesc_round_args;\n+       i < ARRAY_SIZE (bdesc_round_args);\n+       i++, d++)\n+    {\n+      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_ROUND_ARGS_FIRST, i);\n+      if (d->name == 0)\n+\tcontinue;\n+\n+      ftype = (enum ix86_builtin_func_type) d->flag;\n+      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n+    }\n+  BDESC_VERIFYS (IX86_BUILTIN__BDESC_ROUND_ARGS_LAST,\n+\t\t IX86_BUILTIN__BDESC_ROUND_ARGS_FIRST,\n+\t\t ARRAY_SIZE (bdesc_round_args) - 1);\n+\n+  /* pcmpestr[im] insns.  */\n+  for (i = 0, d = bdesc_pcmpestr;\n+       i < ARRAY_SIZE (bdesc_pcmpestr);\n+       i++, d++)\n+    {\n+      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_PCMPESTR_FIRST, i);\n+      if (d->code == IX86_BUILTIN_PCMPESTRM128)\n+\tftype = V16QI_FTYPE_V16QI_INT_V16QI_INT_INT;\n+      else\n+\tftype = INT_FTYPE_V16QI_INT_V16QI_INT_INT;\n+      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n+    }\n+  BDESC_VERIFYS (IX86_BUILTIN__BDESC_PCMPESTR_LAST,\n+\t\t IX86_BUILTIN__BDESC_PCMPESTR_FIRST,\n+\t\t ARRAY_SIZE (bdesc_pcmpestr) - 1);\n+\n+  /* pcmpistr[im] insns.  */\n+  for (i = 0, d = bdesc_pcmpistr;\n+       i < ARRAY_SIZE (bdesc_pcmpistr);\n+       i++, d++)\n+    {\n+      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_PCMPISTR_FIRST, i);\n+      if (d->code == IX86_BUILTIN_PCMPISTRM128)\n+\tftype = V16QI_FTYPE_V16QI_V16QI_INT;\n+      else\n+\tftype = INT_FTYPE_V16QI_V16QI_INT;\n+      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n+    }\n+  BDESC_VERIFYS (IX86_BUILTIN__BDESC_PCMPISTR_LAST,\n+\t\t IX86_BUILTIN__BDESC_PCMPISTR_FIRST,\n+\t\t ARRAY_SIZE (bdesc_pcmpistr) - 1);\n+\n+  /* comi/ucomi insns.  */\n+  for (i = 0, d = bdesc_comi; i < ARRAY_SIZE (bdesc_comi); i++, d++)\n+    {\n+      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_COMI_FIRST, i);\n+      if (d->mask == OPTION_MASK_ISA_SSE2)\n+\tftype = INT_FTYPE_V2DF_V2DF;\n+      else\n+\tftype = INT_FTYPE_V4SF_V4SF;\n+      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n+    }\n+  BDESC_VERIFYS (IX86_BUILTIN__BDESC_COMI_LAST,\n+\t\t IX86_BUILTIN__BDESC_COMI_FIRST,\n+\t\t ARRAY_SIZE (bdesc_comi) - 1);\n+\n+  /* SSE */\n+  def_builtin (OPTION_MASK_ISA_SSE, 0,  \"__builtin_ia32_ldmxcsr\",\n+\t       VOID_FTYPE_UNSIGNED, IX86_BUILTIN_LDMXCSR);\n+  def_builtin_pure (OPTION_MASK_ISA_SSE, 0, \"__builtin_ia32_stmxcsr\",\n+\t\t    UNSIGNED_FTYPE_VOID, IX86_BUILTIN_STMXCSR);\n+\n+  /* SSE or 3DNow!A */\n+  def_builtin (OPTION_MASK_ISA_SSE | OPTION_MASK_ISA_3DNOW_A\n+\t       /* As it uses V4HImode, we have to require -mmmx too.  */\n+\t       | OPTION_MASK_ISA_MMX, 0,\n+\t       \"__builtin_ia32_maskmovq\", VOID_FTYPE_V8QI_V8QI_PCHAR,\n+\t       IX86_BUILTIN_MASKMOVQ);\n+\n+  /* SSE2 */\n+  def_builtin (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_maskmovdqu\",\n+\t       VOID_FTYPE_V16QI_V16QI_PCHAR, IX86_BUILTIN_MASKMOVDQU);\n+\n+  def_builtin (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_clflush\",\n+\t       VOID_FTYPE_PCVOID, IX86_BUILTIN_CLFLUSH);\n+  x86_mfence = def_builtin (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_mfence\",\n+\t\t\t    VOID_FTYPE_VOID, IX86_BUILTIN_MFENCE);\n+\n+  /* SSE3.  */\n+  def_builtin (OPTION_MASK_ISA_SSE3, 0, \"__builtin_ia32_monitor\",\n+\t       VOID_FTYPE_PCVOID_UNSIGNED_UNSIGNED, IX86_BUILTIN_MONITOR);\n+  def_builtin (OPTION_MASK_ISA_SSE3, 0, \"__builtin_ia32_mwait\",\n+\t       VOID_FTYPE_UNSIGNED_UNSIGNED, IX86_BUILTIN_MWAIT);\n+\n+  /* AES */\n+  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n+\t\t     \"__builtin_ia32_aesenc128\",\n+\t\t     V2DI_FTYPE_V2DI_V2DI, IX86_BUILTIN_AESENC128);\n+  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n+\t\t     \"__builtin_ia32_aesenclast128\",\n+\t\t     V2DI_FTYPE_V2DI_V2DI, IX86_BUILTIN_AESENCLAST128);\n+  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n+\t\t     \"__builtin_ia32_aesdec128\",\n+\t\t     V2DI_FTYPE_V2DI_V2DI, IX86_BUILTIN_AESDEC128);\n+  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n+\t\t     \"__builtin_ia32_aesdeclast128\",\n+\t\t     V2DI_FTYPE_V2DI_V2DI, IX86_BUILTIN_AESDECLAST128);\n+  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n+\t\t     \"__builtin_ia32_aesimc128\",\n+\t\t     V2DI_FTYPE_V2DI, IX86_BUILTIN_AESIMC128);\n+  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n+\t\t     \"__builtin_ia32_aeskeygenassist128\",\n+\t\t     V2DI_FTYPE_V2DI_INT, IX86_BUILTIN_AESKEYGENASSIST128);\n+\n+  /* PCLMUL */\n+  def_builtin_const (OPTION_MASK_ISA_PCLMUL | OPTION_MASK_ISA_SSE2, 0,\n+\t\t     \"__builtin_ia32_pclmulqdq128\",\n+\t\t     V2DI_FTYPE_V2DI_V2DI_INT, IX86_BUILTIN_PCLMULQDQ128);\n+\n+  /* RDRND */\n+  def_builtin (OPTION_MASK_ISA_RDRND, 0, \"__builtin_ia32_rdrand16_step\",\n+\t       INT_FTYPE_PUSHORT, IX86_BUILTIN_RDRAND16_STEP);\n+  def_builtin (OPTION_MASK_ISA_RDRND, 0,  \"__builtin_ia32_rdrand32_step\",\n+\t       INT_FTYPE_PUNSIGNED, IX86_BUILTIN_RDRAND32_STEP);\n+  def_builtin (OPTION_MASK_ISA_RDRND | OPTION_MASK_ISA_64BIT, 0,\n+\t       \"__builtin_ia32_rdrand64_step\", INT_FTYPE_PULONGLONG,\n+\t       IX86_BUILTIN_RDRAND64_STEP);\n+\n+  /* AVX2 */\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv2df\",\n+\t\t    V2DF_FTYPE_V2DF_PCDOUBLE_V4SI_V2DF_INT,\n+\t\t    IX86_BUILTIN_GATHERSIV2DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv4df\",\n+\t\t    V4DF_FTYPE_V4DF_PCDOUBLE_V4SI_V4DF_INT,\n+\t\t    IX86_BUILTIN_GATHERSIV4DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv2df\",\n+\t\t    V2DF_FTYPE_V2DF_PCDOUBLE_V2DI_V2DF_INT,\n+\t\t    IX86_BUILTIN_GATHERDIV2DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4df\",\n+\t\t    V4DF_FTYPE_V4DF_PCDOUBLE_V4DI_V4DF_INT,\n+\t\t    IX86_BUILTIN_GATHERDIV4DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv4sf\",\n+\t\t    V4SF_FTYPE_V4SF_PCFLOAT_V4SI_V4SF_INT,\n+\t\t    IX86_BUILTIN_GATHERSIV4SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv8sf\",\n+\t\t    V8SF_FTYPE_V8SF_PCFLOAT_V8SI_V8SF_INT,\n+\t\t    IX86_BUILTIN_GATHERSIV8SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4sf\",\n+\t\t    V4SF_FTYPE_V4SF_PCFLOAT_V2DI_V4SF_INT,\n+\t\t    IX86_BUILTIN_GATHERDIV4SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4sf256\",\n+\t\t    V4SF_FTYPE_V4SF_PCFLOAT_V4DI_V4SF_INT,\n+\t\t    IX86_BUILTIN_GATHERDIV8SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv2di\",\n+\t\t    V2DI_FTYPE_V2DI_PCINT64_V4SI_V2DI_INT,\n+\t\t    IX86_BUILTIN_GATHERSIV2DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv4di\",\n+\t\t    V4DI_FTYPE_V4DI_PCINT64_V4SI_V4DI_INT,\n+\t\t    IX86_BUILTIN_GATHERSIV4DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv2di\",\n+\t\t    V2DI_FTYPE_V2DI_PCINT64_V2DI_V2DI_INT,\n+\t\t    IX86_BUILTIN_GATHERDIV2DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4di\",\n+\t\t    V4DI_FTYPE_V4DI_PCINT64_V4DI_V4DI_INT,\n+\t\t    IX86_BUILTIN_GATHERDIV4DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv4si\",\n+\t\t    V4SI_FTYPE_V4SI_PCINT_V4SI_V4SI_INT,\n+\t\t    IX86_BUILTIN_GATHERSIV4SI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv8si\",\n+\t\t    V8SI_FTYPE_V8SI_PCINT_V8SI_V8SI_INT,\n+\t\t    IX86_BUILTIN_GATHERSIV8SI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4si\",\n+\t\t    V4SI_FTYPE_V4SI_PCINT_V2DI_V4SI_INT,\n+\t\t    IX86_BUILTIN_GATHERDIV4SI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4si256\",\n+\t\t    V4SI_FTYPE_V4SI_PCINT_V4DI_V4SI_INT,\n+\t\t    IX86_BUILTIN_GATHERDIV8SI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatheraltsiv4df \",\n+\t\t    V4DF_FTYPE_V4DF_PCDOUBLE_V8SI_V4DF_INT,\n+\t\t    IX86_BUILTIN_GATHERALTSIV4DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatheraltdiv8sf \",\n+\t\t    V8SF_FTYPE_V8SF_PCFLOAT_V4DI_V8SF_INT,\n+\t\t    IX86_BUILTIN_GATHERALTDIV8SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatheraltsiv4di \",\n+\t\t    V4DI_FTYPE_V4DI_PCINT64_V8SI_V4DI_INT,\n+\t\t    IX86_BUILTIN_GATHERALTSIV4DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatheraltdiv8si \",\n+\t\t    V8SI_FTYPE_V8SI_PCINT_V4DI_V8SI_INT,\n+\t\t    IX86_BUILTIN_GATHERALTDIV8SI);\n+\n+  /* AVX512F */\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gathersiv16sf\",\n+\t\t    V16SF_FTYPE_V16SF_PCVOID_V16SI_HI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV16SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gathersiv8df\",\n+\t\t    V8DF_FTYPE_V8DF_PCVOID_V8SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV8DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gatherdiv16sf\",\n+\t\t    V8SF_FTYPE_V8SF_PCVOID_V8DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV16SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gatherdiv8df\",\n+\t\t    V8DF_FTYPE_V8DF_PCVOID_V8DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV8DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gathersiv16si\",\n+\t\t    V16SI_FTYPE_V16SI_PCVOID_V16SI_HI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV16SI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gathersiv8di\",\n+\t\t    V8DI_FTYPE_V8DI_PCVOID_V8SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV8DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gatherdiv16si\",\n+\t\t    V8SI_FTYPE_V8SI_PCVOID_V8DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV16SI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gatherdiv8di\",\n+\t\t    V8DI_FTYPE_V8DI_PCVOID_V8DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV8DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gather3altsiv8df \",\n+\t\t    V8DF_FTYPE_V8DF_PCDOUBLE_V16SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3ALTSIV8DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gather3altdiv16sf \",\n+\t\t    V16SF_FTYPE_V16SF_PCFLOAT_V8DI_HI_INT,\n+\t\t    IX86_BUILTIN_GATHER3ALTDIV16SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gather3altsiv8di \",\n+\t\t    V8DI_FTYPE_V8DI_PCINT64_V16SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3ALTSIV8DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gather3altdiv16si \",\n+\t\t    V16SI_FTYPE_V16SI_PCINT_V8DI_HI_INT,\n+\t\t    IX86_BUILTIN_GATHER3ALTDIV16SI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scattersiv16sf\",\n+\t       VOID_FTYPE_PVOID_HI_V16SI_V16SF_INT,\n+\t       IX86_BUILTIN_SCATTERSIV16SF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scattersiv8df\",\n+\t       VOID_FTYPE_PVOID_QI_V8SI_V8DF_INT,\n+\t       IX86_BUILTIN_SCATTERSIV8DF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatterdiv16sf\",\n+\t       VOID_FTYPE_PVOID_QI_V8DI_V8SF_INT,\n+\t       IX86_BUILTIN_SCATTERDIV16SF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatterdiv8df\",\n+\t       VOID_FTYPE_PVOID_QI_V8DI_V8DF_INT,\n+\t       IX86_BUILTIN_SCATTERDIV8DF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scattersiv16si\",\n+\t       VOID_FTYPE_PVOID_HI_V16SI_V16SI_INT,\n+\t       IX86_BUILTIN_SCATTERSIV16SI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scattersiv8di\",\n+\t       VOID_FTYPE_PVOID_QI_V8SI_V8DI_INT,\n+\t       IX86_BUILTIN_SCATTERSIV8DI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatterdiv16si\",\n+\t       VOID_FTYPE_PVOID_QI_V8DI_V8SI_INT,\n+\t       IX86_BUILTIN_SCATTERDIV16SI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatterdiv8di\",\n+\t       VOID_FTYPE_PVOID_QI_V8DI_V8DI_INT,\n+\t       IX86_BUILTIN_SCATTERDIV8DI);\n+\n+  /* AVX512VL */\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv2df\",\n+\t\t    V2DF_FTYPE_V2DF_PCVOID_V4SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV2DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv4df\",\n+\t\t    V4DF_FTYPE_V4DF_PCVOID_V4SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV4DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div2df\",\n+\t\t    V2DF_FTYPE_V2DF_PCVOID_V2DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV2DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div4df\",\n+\t\t    V4DF_FTYPE_V4DF_PCVOID_V4DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV4DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv4sf\",\n+\t\t    V4SF_FTYPE_V4SF_PCVOID_V4SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV4SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv8sf\",\n+\t\t    V8SF_FTYPE_V8SF_PCVOID_V8SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV8SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div4sf\",\n+\t\t    V4SF_FTYPE_V4SF_PCVOID_V2DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV4SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div8sf\",\n+\t\t    V4SF_FTYPE_V4SF_PCVOID_V4DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV8SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv2di\",\n+\t\t    V2DI_FTYPE_V2DI_PCVOID_V4SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV2DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv4di\",\n+\t\t    V4DI_FTYPE_V4DI_PCVOID_V4SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV4DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div2di\",\n+\t\t    V2DI_FTYPE_V2DI_PCVOID_V2DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV2DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div4di\",\n+\t\t    V4DI_FTYPE_V4DI_PCVOID_V4DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV4DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv4si\",\n+\t\t    V4SI_FTYPE_V4SI_PCVOID_V4SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV4SI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv8si\",\n+\t\t    V8SI_FTYPE_V8SI_PCVOID_V8SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3SIV8SI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div4si\",\n+\t\t    V4SI_FTYPE_V4SI_PCVOID_V2DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV4SI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div8si\",\n+\t\t    V4SI_FTYPE_V4SI_PCVOID_V4DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3DIV8SI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3altsiv4df \",\n+\t\t    V4DF_FTYPE_V4DF_PCDOUBLE_V8SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3ALTSIV4DF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3altdiv8sf \",\n+\t\t    V8SF_FTYPE_V8SF_PCFLOAT_V4DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3ALTDIV8SF);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3altsiv4di \",\n+\t\t    V4DI_FTYPE_V4DI_PCINT64_V8SI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3ALTSIV4DI);\n+\n+  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3altdiv8si \",\n+\t\t    V8SI_FTYPE_V8SI_PCINT_V4DI_QI_INT,\n+\t\t    IX86_BUILTIN_GATHER3ALTDIV8SI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv8sf\",\n+\t       VOID_FTYPE_PVOID_QI_V8SI_V8SF_INT,\n+\t       IX86_BUILTIN_SCATTERSIV8SF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv4sf\",\n+\t       VOID_FTYPE_PVOID_QI_V4SI_V4SF_INT,\n+\t       IX86_BUILTIN_SCATTERSIV4SF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv4df\",\n+\t       VOID_FTYPE_PVOID_QI_V4SI_V4DF_INT,\n+\t       IX86_BUILTIN_SCATTERSIV4DF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv2df\",\n+\t       VOID_FTYPE_PVOID_QI_V4SI_V2DF_INT,\n+\t       IX86_BUILTIN_SCATTERSIV2DF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv8sf\",\n+\t       VOID_FTYPE_PVOID_QI_V4DI_V4SF_INT,\n+\t       IX86_BUILTIN_SCATTERDIV8SF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv4sf\",\n+\t       VOID_FTYPE_PVOID_QI_V2DI_V4SF_INT,\n+\t       IX86_BUILTIN_SCATTERDIV4SF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv4df\",\n+\t       VOID_FTYPE_PVOID_QI_V4DI_V4DF_INT,\n+\t       IX86_BUILTIN_SCATTERDIV4DF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv2df\",\n+\t       VOID_FTYPE_PVOID_QI_V2DI_V2DF_INT,\n+\t       IX86_BUILTIN_SCATTERDIV2DF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv8si\",\n+\t       VOID_FTYPE_PVOID_QI_V8SI_V8SI_INT,\n+\t       IX86_BUILTIN_SCATTERSIV8SI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv4si\",\n+\t       VOID_FTYPE_PVOID_QI_V4SI_V4SI_INT,\n+\t       IX86_BUILTIN_SCATTERSIV4SI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv4di\",\n+\t       VOID_FTYPE_PVOID_QI_V4SI_V4DI_INT,\n+\t       IX86_BUILTIN_SCATTERSIV4DI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv2di\",\n+\t       VOID_FTYPE_PVOID_QI_V4SI_V2DI_INT,\n+\t       IX86_BUILTIN_SCATTERSIV2DI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv8si\",\n+\t       VOID_FTYPE_PVOID_QI_V4DI_V4SI_INT,\n+\t       IX86_BUILTIN_SCATTERDIV8SI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv4si\",\n+\t       VOID_FTYPE_PVOID_QI_V2DI_V4SI_INT,\n+\t       IX86_BUILTIN_SCATTERDIV4SI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv4di\",\n+\t       VOID_FTYPE_PVOID_QI_V4DI_V4DI_INT,\n+\t       IX86_BUILTIN_SCATTERDIV4DI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv2di\",\n+\t       VOID_FTYPE_PVOID_QI_V2DI_V2DI_INT,\n+\t       IX86_BUILTIN_SCATTERDIV2DI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatteraltsiv8df \",\n+\t       VOID_FTYPE_PDOUBLE_QI_V16SI_V8DF_INT,\n+\t       IX86_BUILTIN_SCATTERALTSIV8DF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatteraltdiv16sf \",\n+\t       VOID_FTYPE_PFLOAT_HI_V8DI_V16SF_INT,\n+\t       IX86_BUILTIN_SCATTERALTDIV16SF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatteraltsiv8di \",\n+\t       VOID_FTYPE_PLONGLONG_QI_V16SI_V8DI_INT,\n+\t       IX86_BUILTIN_SCATTERALTSIV8DI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatteraltdiv16si \",\n+\t       VOID_FTYPE_PINT_HI_V8DI_V16SI_INT,\n+\t       IX86_BUILTIN_SCATTERALTDIV16SI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltsiv4df \",\n+\t       VOID_FTYPE_PDOUBLE_QI_V8SI_V4DF_INT,\n+\t       IX86_BUILTIN_SCATTERALTSIV4DF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltdiv8sf \",\n+\t       VOID_FTYPE_PFLOAT_QI_V4DI_V8SF_INT,\n+\t       IX86_BUILTIN_SCATTERALTDIV8SF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltsiv4di \",\n+\t       VOID_FTYPE_PLONGLONG_QI_V8SI_V4DI_INT,\n+\t       IX86_BUILTIN_SCATTERALTSIV4DI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltdiv8si \",\n+\t       VOID_FTYPE_PINT_QI_V4DI_V8SI_INT,\n+\t       IX86_BUILTIN_SCATTERALTDIV8SI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltsiv2df \",\n+\t       VOID_FTYPE_PDOUBLE_QI_V4SI_V2DF_INT,\n+\t       IX86_BUILTIN_SCATTERALTSIV2DF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltdiv4sf \",\n+\t       VOID_FTYPE_PFLOAT_QI_V2DI_V4SF_INT,\n+\t       IX86_BUILTIN_SCATTERALTDIV4SF);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltsiv2di \",\n+\t       VOID_FTYPE_PLONGLONG_QI_V4SI_V2DI_INT,\n+\t       IX86_BUILTIN_SCATTERALTSIV2DI);\n+\n+  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltdiv4si \",\n+\t       VOID_FTYPE_PINT_QI_V2DI_V4SI_INT,\n+\t       IX86_BUILTIN_SCATTERALTDIV4SI);\n+\n+  /* AVX512PF */\n+  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_gatherpfdpd\",\n+\t       VOID_FTYPE_QI_V8SI_PCVOID_INT_INT,\n+\t       IX86_BUILTIN_GATHERPFDPD);\n+  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_gatherpfdps\",\n+\t       VOID_FTYPE_HI_V16SI_PCVOID_INT_INT,\n+\t       IX86_BUILTIN_GATHERPFDPS);\n+  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_gatherpfqpd\",\n+\t       VOID_FTYPE_QI_V8DI_PCVOID_INT_INT,\n+\t       IX86_BUILTIN_GATHERPFQPD);\n+  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_gatherpfqps\",\n+\t       VOID_FTYPE_QI_V8DI_PCVOID_INT_INT,\n+\t       IX86_BUILTIN_GATHERPFQPS);\n+  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_scatterpfdpd\",\n+\t       VOID_FTYPE_QI_V8SI_PCVOID_INT_INT,\n+\t       IX86_BUILTIN_SCATTERPFDPD);\n+  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_scatterpfdps\",\n+\t       VOID_FTYPE_HI_V16SI_PCVOID_INT_INT,\n+\t       IX86_BUILTIN_SCATTERPFDPS);\n+  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_scatterpfqpd\",\n+\t       VOID_FTYPE_QI_V8DI_PCVOID_INT_INT,\n+\t       IX86_BUILTIN_SCATTERPFQPD);\n+  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_scatterpfqps\",\n+\t       VOID_FTYPE_QI_V8DI_PCVOID_INT_INT,\n+\t       IX86_BUILTIN_SCATTERPFQPS);\n+\n+  /* SHA */\n+  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha1msg1\",\n+\t\t     V4SI_FTYPE_V4SI_V4SI, IX86_BUILTIN_SHA1MSG1);\n+  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha1msg2\",\n+\t\t     V4SI_FTYPE_V4SI_V4SI, IX86_BUILTIN_SHA1MSG2);\n+  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha1nexte\",\n+\t\t     V4SI_FTYPE_V4SI_V4SI, IX86_BUILTIN_SHA1NEXTE);\n+  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha1rnds4\",\n+\t\t     V4SI_FTYPE_V4SI_V4SI_INT, IX86_BUILTIN_SHA1RNDS4);\n+  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha256msg1\",\n+\t\t     V4SI_FTYPE_V4SI_V4SI, IX86_BUILTIN_SHA256MSG1);\n+  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha256msg2\",\n+\t\t     V4SI_FTYPE_V4SI_V4SI, IX86_BUILTIN_SHA256MSG2);\n+  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha256rnds2\",\n+\t\t     V4SI_FTYPE_V4SI_V4SI_V4SI, IX86_BUILTIN_SHA256RNDS2);\n+\n+  /* RTM.  */\n+  def_builtin (OPTION_MASK_ISA_RTM, 0, \"__builtin_ia32_xabort\",\n+\t       VOID_FTYPE_UNSIGNED, IX86_BUILTIN_XABORT);\n+\n+  /* MMX access to the vec_init patterns.  */\n+  def_builtin_const (OPTION_MASK_ISA_MMX, 0, \"__builtin_ia32_vec_init_v2si\",\n+\t\t     V2SI_FTYPE_INT_INT, IX86_BUILTIN_VEC_INIT_V2SI);\n+\n+  def_builtin_const (OPTION_MASK_ISA_MMX, 0, \"__builtin_ia32_vec_init_v4hi\",\n+\t\t     V4HI_FTYPE_HI_HI_HI_HI,\n+\t\t     IX86_BUILTIN_VEC_INIT_V4HI);\n+\n+  def_builtin_const (OPTION_MASK_ISA_MMX, 0, \"__builtin_ia32_vec_init_v8qi\",\n+\t\t     V8QI_FTYPE_QI_QI_QI_QI_QI_QI_QI_QI,\n+\t\t     IX86_BUILTIN_VEC_INIT_V8QI);\n+\n+  /* Access to the vec_extract patterns.  */\n+  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_ext_v2df\",\n+\t\t     DOUBLE_FTYPE_V2DF_INT, IX86_BUILTIN_VEC_EXT_V2DF);\n+  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_ext_v2di\",\n+\t\t     DI_FTYPE_V2DI_INT, IX86_BUILTIN_VEC_EXT_V2DI);\n+  def_builtin_const (OPTION_MASK_ISA_SSE, 0, \"__builtin_ia32_vec_ext_v4sf\",\n+\t\t     FLOAT_FTYPE_V4SF_INT, IX86_BUILTIN_VEC_EXT_V4SF);\n+  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_ext_v4si\",\n+\t\t     SI_FTYPE_V4SI_INT, IX86_BUILTIN_VEC_EXT_V4SI);\n+  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_ext_v8hi\",\n+\t\t     HI_FTYPE_V8HI_INT, IX86_BUILTIN_VEC_EXT_V8HI);\n+\n+  def_builtin_const (OPTION_MASK_ISA_SSE | OPTION_MASK_ISA_3DNOW_A\n+\t\t     /* As it uses V4HImode, we have to require -mmmx too.  */\n+\t\t     | OPTION_MASK_ISA_MMX, 0,\n+\t\t     \"__builtin_ia32_vec_ext_v4hi\",\n+\t\t     HI_FTYPE_V4HI_INT, IX86_BUILTIN_VEC_EXT_V4HI);\n+\n+  def_builtin_const (OPTION_MASK_ISA_MMX, 0, \"__builtin_ia32_vec_ext_v2si\",\n+\t\t     SI_FTYPE_V2SI_INT, IX86_BUILTIN_VEC_EXT_V2SI);\n+\n+  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_ext_v16qi\",\n+\t\t     QI_FTYPE_V16QI_INT, IX86_BUILTIN_VEC_EXT_V16QI);\n+\n+  /* Access to the vec_set patterns.  */\n+  def_builtin_const (OPTION_MASK_ISA_SSE4_1 | OPTION_MASK_ISA_64BIT, 0,\n+\t\t     \"__builtin_ia32_vec_set_v2di\",\n+\t\t     V2DI_FTYPE_V2DI_DI_INT, IX86_BUILTIN_VEC_SET_V2DI);\n+\n+  def_builtin_const (OPTION_MASK_ISA_SSE4_1, 0, \"__builtin_ia32_vec_set_v4sf\",\n+\t\t     V4SF_FTYPE_V4SF_FLOAT_INT, IX86_BUILTIN_VEC_SET_V4SF);\n+\n+  def_builtin_const (OPTION_MASK_ISA_SSE4_1, 0, \"__builtin_ia32_vec_set_v4si\",\n+\t\t     V4SI_FTYPE_V4SI_SI_INT, IX86_BUILTIN_VEC_SET_V4SI);\n+\n+  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_set_v8hi\",\n+\t\t     V8HI_FTYPE_V8HI_HI_INT, IX86_BUILTIN_VEC_SET_V8HI);\n+\n+  def_builtin_const (OPTION_MASK_ISA_SSE | OPTION_MASK_ISA_3DNOW_A\n+\t\t     /* As it uses V4HImode, we have to require -mmmx too.  */\n+\t\t     | OPTION_MASK_ISA_MMX, 0,\n+\t\t     \"__builtin_ia32_vec_set_v4hi\",\n+\t\t     V4HI_FTYPE_V4HI_HI_INT, IX86_BUILTIN_VEC_SET_V4HI);\n+\n+  def_builtin_const (OPTION_MASK_ISA_SSE4_1, 0, \"__builtin_ia32_vec_set_v16qi\",\n+\t\t     V16QI_FTYPE_V16QI_QI_INT, IX86_BUILTIN_VEC_SET_V16QI);\n+\n+  /* RDSEED */\n+  def_builtin (OPTION_MASK_ISA_RDSEED, 0, \"__builtin_ia32_rdseed_hi_step\",\n+\t       INT_FTYPE_PUSHORT, IX86_BUILTIN_RDSEED16_STEP);\n+  def_builtin (OPTION_MASK_ISA_RDSEED, 0, \"__builtin_ia32_rdseed_si_step\",\n+\t       INT_FTYPE_PUNSIGNED, IX86_BUILTIN_RDSEED32_STEP);\n+  def_builtin (OPTION_MASK_ISA_RDSEED | OPTION_MASK_ISA_64BIT, 0,\n+\t       \"__builtin_ia32_rdseed_di_step\",\n+\t       INT_FTYPE_PULONGLONG, IX86_BUILTIN_RDSEED64_STEP);\n+\n+  /* ADCX */\n+  def_builtin (0, 0, \"__builtin_ia32_addcarryx_u32\",\n+\t       UCHAR_FTYPE_UCHAR_UINT_UINT_PUNSIGNED, IX86_BUILTIN_ADDCARRYX32);\n+  def_builtin (OPTION_MASK_ISA_64BIT, 0,\n+\t       \"__builtin_ia32_addcarryx_u64\",\n+\t       UCHAR_FTYPE_UCHAR_ULONGLONG_ULONGLONG_PULONGLONG,\n+\t       IX86_BUILTIN_ADDCARRYX64);\n+\n+  /* SBB */\n+  def_builtin (0, 0, \"__builtin_ia32_sbb_u32\",\n+\t       UCHAR_FTYPE_UCHAR_UINT_UINT_PUNSIGNED, IX86_BUILTIN_SBB32);\n+  def_builtin (OPTION_MASK_ISA_64BIT, 0,\n+\t       \"__builtin_ia32_sbb_u64\",\n+\t       UCHAR_FTYPE_UCHAR_ULONGLONG_ULONGLONG_PULONGLONG,\n+\t       IX86_BUILTIN_SBB64);\n+\n+  /* Read/write FLAGS.  */\n+  if (TARGET_64BIT)\n+    {\n+      def_builtin (OPTION_MASK_ISA_64BIT, 0, \"__builtin_ia32_readeflags_u64\",\n+\t\t   UINT64_FTYPE_VOID, IX86_BUILTIN_READ_FLAGS);\n+      def_builtin (OPTION_MASK_ISA_64BIT, 0, \"__builtin_ia32_writeeflags_u64\",\n+\t\t   VOID_FTYPE_UINT64, IX86_BUILTIN_WRITE_FLAGS);\n+    }\n+  else\n+    {\n+      def_builtin (0, 0, \"__builtin_ia32_readeflags_u32\",\n+\t\t   UNSIGNED_FTYPE_VOID, IX86_BUILTIN_READ_FLAGS);\n+      def_builtin (0, 0, \"__builtin_ia32_writeeflags_u32\",\n+\t\t   VOID_FTYPE_UNSIGNED, IX86_BUILTIN_WRITE_FLAGS);\n+    }\n+\n+  /* CLFLUSHOPT.  */\n+  def_builtin (OPTION_MASK_ISA_CLFLUSHOPT, 0, \"__builtin_ia32_clflushopt\",\n+\t       VOID_FTYPE_PCVOID, IX86_BUILTIN_CLFLUSHOPT);\n+\n+  /* CLWB.  */\n+  def_builtin (OPTION_MASK_ISA_CLWB, 0, \"__builtin_ia32_clwb\",\n+\t       VOID_FTYPE_PCVOID, IX86_BUILTIN_CLWB);\n+\n+  /* MONITORX and MWAITX.  */\n+  def_builtin (0, OPTION_MASK_ISA_MWAITX, \"__builtin_ia32_monitorx\",\n+\t\tVOID_FTYPE_PCVOID_UNSIGNED_UNSIGNED, IX86_BUILTIN_MONITORX);\n+  def_builtin (0, OPTION_MASK_ISA_MWAITX, \"__builtin_ia32_mwaitx\",\n+\t\tVOID_FTYPE_UNSIGNED_UNSIGNED_UNSIGNED, IX86_BUILTIN_MWAITX);\n+\n+  /* CLZERO.  */\n+  def_builtin (0, OPTION_MASK_ISA_CLZERO, \"__builtin_ia32_clzero\",\n+\t\tVOID_FTYPE_PCVOID, IX86_BUILTIN_CLZERO);\n+\n+  /* WAITPKG.  */\n+  def_builtin (0, OPTION_MASK_ISA_WAITPKG, \"__builtin_ia32_umonitor\",\n+\t       VOID_FTYPE_PVOID, IX86_BUILTIN_UMONITOR);\n+  def_builtin (0, OPTION_MASK_ISA_WAITPKG, \"__builtin_ia32_umwait\",\n+\t       UINT8_FTYPE_UNSIGNED_UINT64, IX86_BUILTIN_UMWAIT);\n+  def_builtin (0, OPTION_MASK_ISA_WAITPKG, \"__builtin_ia32_tpause\",\n+\t       UINT8_FTYPE_UNSIGNED_UINT64, IX86_BUILTIN_TPAUSE);\n+\n+  /* CLDEMOTE.  */\n+  def_builtin (0, OPTION_MASK_ISA_CLDEMOTE, \"__builtin_ia32_cldemote\",\n+\t       VOID_FTYPE_PCVOID, IX86_BUILTIN_CLDEMOTE);\n+\n+  /* Add FMA4 multi-arg argument instructions */\n+  for (i = 0, d = bdesc_multi_arg; i < ARRAY_SIZE (bdesc_multi_arg); i++, d++)\n+    {\n+      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_MULTI_ARG_FIRST, i);\n+      if (d->name == 0)\n+\tcontinue;\n+\n+      ftype = (enum ix86_builtin_func_type) d->flag;\n+      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n+    }\n+  BDESC_VERIFYS (IX86_BUILTIN__BDESC_MULTI_ARG_LAST,\n+\t\t IX86_BUILTIN__BDESC_MULTI_ARG_FIRST,\n+\t\t ARRAY_SIZE (bdesc_multi_arg) - 1);\n+\n+  /* Add CET inrinsics.  */\n+  for (i = 0, d = bdesc_cet; i < ARRAY_SIZE (bdesc_cet); i++, d++)\n+    {\n+      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_CET_FIRST, i);\n+      if (d->name == 0)\n+\tcontinue;\n+\n+      ftype = (enum ix86_builtin_func_type) d->flag;\n+      def_builtin (d->mask, d->mask2, d->name, ftype, d->code);\n+    }\n+  BDESC_VERIFYS (IX86_BUILTIN__BDESC_CET_LAST,\n+\t\t IX86_BUILTIN__BDESC_CET_FIRST,\n+\t\t ARRAY_SIZE (bdesc_cet) - 1);\n+\n+  for (i = 0, d = bdesc_cet_rdssp;\n+       i < ARRAY_SIZE (bdesc_cet_rdssp);\n+       i++, d++)\n+    {\n+      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_CET_NORMAL_FIRST, i);\n+      if (d->name == 0)\n+\tcontinue;\n+\n+      ftype = (enum ix86_builtin_func_type) d->flag;\n+      def_builtin (d->mask, d->mask2, d->name, ftype, d->code);\n+    }\n+  BDESC_VERIFYS (IX86_BUILTIN__BDESC_CET_NORMAL_LAST,\n+\t\t IX86_BUILTIN__BDESC_CET_NORMAL_FIRST,\n+\t\t ARRAY_SIZE (bdesc_cet_rdssp) - 1);\n+}\n+\n+#undef BDESC_VERIFY\n+#undef BDESC_VERIFYS\n+\n+/* Make builtins to detect cpu type and features supported.  NAME is\n+   the builtin name, CODE is the builtin code, and FTYPE is the function\n+   type of the builtin.  */\n+\n+static void\n+make_cpu_type_builtin (const char* name, int code,\n+\t\t       enum ix86_builtin_func_type ftype, bool is_const)\n+{\n+  tree decl;\n+  tree type;\n+\n+  type = ix86_get_builtin_func_type (ftype);\n+  decl = add_builtin_function (name, type, code, BUILT_IN_MD,\n+\t\t\t       NULL, NULL_TREE);\n+  gcc_assert (decl != NULL_TREE);\n+  ix86_builtins[(int) code] = decl;\n+  TREE_READONLY (decl) = is_const;\n+}\n+\n+/* Make builtins to get CPU type and features supported.  The created\n+   builtins are :\n+\n+   __builtin_cpu_init (), to detect cpu type and features,\n+   __builtin_cpu_is (\"<CPUNAME>\"), to check if cpu is of type <CPUNAME>,\n+   __builtin_cpu_supports (\"<FEATURE>\"), to check if cpu supports <FEATURE>\n+   */\n+\n+static void\n+ix86_init_platform_type_builtins (void)\n+{\n+  make_cpu_type_builtin (\"__builtin_cpu_init\", IX86_BUILTIN_CPU_INIT,\n+\t\t\t INT_FTYPE_VOID, false);\n+  make_cpu_type_builtin (\"__builtin_cpu_is\", IX86_BUILTIN_CPU_IS,\n+\t\t\t INT_FTYPE_PCCHAR, true);\n+  make_cpu_type_builtin (\"__builtin_cpu_supports\", IX86_BUILTIN_CPU_SUPPORTS,\n+\t\t\t INT_FTYPE_PCCHAR, true);\n+}\n+\n+/* Internal method for ix86_init_builtins.  */\n+\n+static void\n+ix86_init_builtins_va_builtins_abi (void)\n+{\n+  tree ms_va_ref, sysv_va_ref;\n+  tree fnvoid_va_end_ms, fnvoid_va_end_sysv;\n+  tree fnvoid_va_start_ms, fnvoid_va_start_sysv;\n+  tree fnvoid_va_copy_ms, fnvoid_va_copy_sysv;\n+  tree fnattr_ms = NULL_TREE, fnattr_sysv = NULL_TREE;\n+\n+  if (!TARGET_64BIT)\n+    return;\n+  fnattr_ms = build_tree_list (get_identifier (\"ms_abi\"), NULL_TREE);\n+  fnattr_sysv = build_tree_list (get_identifier (\"sysv_abi\"), NULL_TREE);\n+  ms_va_ref = build_reference_type (ms_va_list_type_node);\n+  sysv_va_ref = build_pointer_type (TREE_TYPE (sysv_va_list_type_node));\n+\n+  fnvoid_va_end_ms = build_function_type_list (void_type_node, ms_va_ref,\n+\t\t\t\t\t       NULL_TREE);\n+  fnvoid_va_start_ms\n+    = build_varargs_function_type_list (void_type_node, ms_va_ref, NULL_TREE);\n+  fnvoid_va_end_sysv\n+    = build_function_type_list (void_type_node, sysv_va_ref, NULL_TREE);\n+  fnvoid_va_start_sysv\n+    = build_varargs_function_type_list (void_type_node, sysv_va_ref,\n+\t\t\t\t\tNULL_TREE);\n+  fnvoid_va_copy_ms\n+    = build_function_type_list (void_type_node, ms_va_ref,\n+\t\t\t\tms_va_list_type_node, NULL_TREE);\n+  fnvoid_va_copy_sysv\n+    = build_function_type_list (void_type_node, sysv_va_ref,\n+\t\t\t\tsysv_va_ref, NULL_TREE);\n+\n+  add_builtin_function (\"__builtin_ms_va_start\", fnvoid_va_start_ms,\n+  \t\t\tBUILT_IN_VA_START, BUILT_IN_NORMAL, NULL, fnattr_ms);\n+  add_builtin_function (\"__builtin_ms_va_end\", fnvoid_va_end_ms,\n+  \t\t\tBUILT_IN_VA_END, BUILT_IN_NORMAL, NULL, fnattr_ms);\n+  add_builtin_function (\"__builtin_ms_va_copy\", fnvoid_va_copy_ms,\n+\t\t\tBUILT_IN_VA_COPY, BUILT_IN_NORMAL, NULL, fnattr_ms);\n+  add_builtin_function (\"__builtin_sysv_va_start\", fnvoid_va_start_sysv,\n+  \t\t\tBUILT_IN_VA_START, BUILT_IN_NORMAL, NULL, fnattr_sysv);\n+  add_builtin_function (\"__builtin_sysv_va_end\", fnvoid_va_end_sysv,\n+  \t\t\tBUILT_IN_VA_END, BUILT_IN_NORMAL, NULL, fnattr_sysv);\n+  add_builtin_function (\"__builtin_sysv_va_copy\", fnvoid_va_copy_sysv,\n+\t\t\tBUILT_IN_VA_COPY, BUILT_IN_NORMAL, NULL, fnattr_sysv);\n+}\n+\n+static void\n+ix86_init_builtin_types (void)\n+{\n+  tree float80_type_node, const_string_type_node;\n+\n+  /* The __float80 type.  */\n+  float80_type_node = long_double_type_node;\n+  if (TYPE_MODE (float80_type_node) != XFmode)\n+    {\n+      if (float64x_type_node != NULL_TREE\n+\t  && TYPE_MODE (float64x_type_node) == XFmode)\n+\tfloat80_type_node = float64x_type_node;\n+      else\n+\t{\n+\t  /* The __float80 type.  */\n+\t  float80_type_node = make_node (REAL_TYPE);\n+\n+\t  TYPE_PRECISION (float80_type_node) = 80;\n+\t  layout_type (float80_type_node);\n+\t}\n+    }\n+  lang_hooks.types.register_builtin_type (float80_type_node, \"__float80\");\n+\n+  /* The __float128 type.  The node has already been created as\n+     _Float128, so we only need to register the __float128 name for\n+     it.  */\n+  lang_hooks.types.register_builtin_type (float128_type_node, \"__float128\");\n+\n+  const_string_type_node\n+    = build_pointer_type (build_qualified_type\n+\t\t\t  (char_type_node, TYPE_QUAL_CONST));\n+\n+  /* This macro is built by i386-builtin-types.awk.  */\n+  DEFINE_BUILTIN_PRIMITIVE_TYPES;\n+}\n+\n+void\n+ix86_init_builtins (void)\n+{\n+  tree ftype, decl;\n+\n+  ix86_init_builtin_types ();\n+\n+  /* Builtins to get CPU type and features. */\n+  ix86_init_platform_type_builtins ();\n+\n+  /* TFmode support builtins.  */\n+  def_builtin_const (0, 0, \"__builtin_infq\",\n+\t\t     FLOAT128_FTYPE_VOID, IX86_BUILTIN_INFQ);\n+  def_builtin_const (0, 0, \"__builtin_huge_valq\",\n+\t\t     FLOAT128_FTYPE_VOID, IX86_BUILTIN_HUGE_VALQ);\n+\n+  ftype = ix86_get_builtin_func_type (FLOAT128_FTYPE_CONST_STRING);\n+  decl = add_builtin_function (\"__builtin_nanq\", ftype, IX86_BUILTIN_NANQ,\n+\t\t\t       BUILT_IN_MD, \"nanq\", NULL_TREE);\n+  TREE_READONLY (decl) = 1;\n+  ix86_builtins[(int) IX86_BUILTIN_NANQ] = decl;\n+\n+  decl = add_builtin_function (\"__builtin_nansq\", ftype, IX86_BUILTIN_NANSQ,\n+\t\t\t       BUILT_IN_MD, \"nansq\", NULL_TREE);\n+  TREE_READONLY (decl) = 1;\n+  ix86_builtins[(int) IX86_BUILTIN_NANSQ] = decl;\n+\n+  /* We will expand them to normal call if SSE isn't available since\n+     they are used by libgcc. */\n+  ftype = ix86_get_builtin_func_type (FLOAT128_FTYPE_FLOAT128);\n+  decl = add_builtin_function (\"__builtin_fabsq\", ftype, IX86_BUILTIN_FABSQ,\n+\t\t\t       BUILT_IN_MD, \"__fabstf2\", NULL_TREE);\n+  TREE_READONLY (decl) = 1;\n+  ix86_builtins[(int) IX86_BUILTIN_FABSQ] = decl;\n+\n+  ftype = ix86_get_builtin_func_type (FLOAT128_FTYPE_FLOAT128_FLOAT128);\n+  decl = add_builtin_function (\"__builtin_copysignq\", ftype,\n+\t\t\t       IX86_BUILTIN_COPYSIGNQ, BUILT_IN_MD,\n+\t\t\t       \"__copysigntf3\", NULL_TREE);\n+  TREE_READONLY (decl) = 1;\n+  ix86_builtins[(int) IX86_BUILTIN_COPYSIGNQ] = decl;\n+\n+  ix86_init_tm_builtins ();\n+  ix86_init_mmx_sse_builtins ();\n+\n+  if (TARGET_LP64)\n+    ix86_init_builtins_va_builtins_abi ();\n+\n+#ifdef SUBTARGET_INIT_BUILTINS\n+  SUBTARGET_INIT_BUILTINS;\n+#endif\n+}\n+\n+/* Return the ix86 builtin for CODE.  */\n+\n+tree\n+ix86_builtin_decl (unsigned code, bool)\n+{\n+  if (code >= IX86_BUILTIN_MAX)\n+    return error_mark_node;\n+\n+  return ix86_builtins[code];\n+}\n+\n+/* This returns the target-specific builtin with code CODE if\n+   current_function_decl has visibility on this builtin, which is checked\n+   using isa flags.  Returns NULL_TREE otherwise.  */\n+\n+static tree ix86_get_builtin (enum ix86_builtins code)\n+{\n+  struct cl_target_option *opts;\n+  tree target_tree = NULL_TREE;\n+\n+  /* Determine the isa flags of current_function_decl.  */\n+\n+  if (current_function_decl)\n+    target_tree = DECL_FUNCTION_SPECIFIC_TARGET (current_function_decl);\n+\n+  if (target_tree == NULL)\n+    target_tree = target_option_default_node;\n+\n+  opts = TREE_TARGET_OPTION (target_tree);\n+\n+  if ((ix86_builtins_isa[(int) code].isa & opts->x_ix86_isa_flags)\n+      || (ix86_builtins_isa[(int) code].isa2 & opts->x_ix86_isa_flags2))\n+    return ix86_builtin_decl (code, true);\n+  else\n+    return NULL_TREE;\n+}\n+\n+/* Vectorization library interface and handlers.  */\n+tree (*ix86_veclib_handler) (combined_fn, tree, tree);\n+\n+/* Returns a function decl for a vectorized version of the combined function\n+   with combined_fn code FN and the result vector type TYPE, or NULL_TREE\n+   if it is not available.  */\n+\n+tree\n+ix86_builtin_vectorized_function (unsigned int fn, tree type_out,\n+\t\t\t\t  tree type_in)\n+{\n+  machine_mode in_mode, out_mode;\n+  int in_n, out_n;\n+\n+  if (TREE_CODE (type_out) != VECTOR_TYPE\n+      || TREE_CODE (type_in) != VECTOR_TYPE)\n+    return NULL_TREE;\n+\n+  out_mode = TYPE_MODE (TREE_TYPE (type_out));\n+  out_n = TYPE_VECTOR_SUBPARTS (type_out);\n+  in_mode = TYPE_MODE (TREE_TYPE (type_in));\n+  in_n = TYPE_VECTOR_SUBPARTS (type_in);\n+\n+  switch (fn)\n+    {\n+    CASE_CFN_EXP2:\n+      if (out_mode == SFmode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_EXP2PS);\n+\t}\n+      break;\n+\n+    CASE_CFN_IFLOOR:\n+    CASE_CFN_LFLOOR:\n+    CASE_CFN_LLFLOOR:\n+      /* The round insn does not trap on denormals.  */\n+      if (flag_trapping_math || !TARGET_SSE4_1)\n+\tbreak;\n+\n+      if (out_mode == SImode && in_mode == DFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 2)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD_VEC_PACK_SFIX);\n+\t  else if (out_n == 8 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD_VEC_PACK_SFIX256);\n+\t  else if (out_n == 16 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD_VEC_PACK_SFIX512);\n+\t}\n+      if (out_mode == SImode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS_SFIX);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS_SFIX256);\n+\t  else if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS_SFIX512);\n+\t}\n+      break;\n+\n+    CASE_CFN_ICEIL:\n+    CASE_CFN_LCEIL:\n+    CASE_CFN_LLCEIL:\n+      /* The round insn does not trap on denormals.  */\n+      if (flag_trapping_math || !TARGET_SSE4_1)\n+\tbreak;\n+\n+      if (out_mode == SImode && in_mode == DFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 2)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD_VEC_PACK_SFIX);\n+\t  else if (out_n == 8 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD_VEC_PACK_SFIX256);\n+\t  else if (out_n == 16 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD_VEC_PACK_SFIX512);\n+\t}\n+      if (out_mode == SImode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS_SFIX);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS_SFIX256);\n+\t  else if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS_SFIX512);\n+\t}\n+      break;\n+\n+    CASE_CFN_IRINT:\n+    CASE_CFN_LRINT:\n+    CASE_CFN_LLRINT:\n+      if (out_mode == SImode && in_mode == DFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 2)\n+\t    return ix86_get_builtin (IX86_BUILTIN_VEC_PACK_SFIX);\n+\t  else if (out_n == 8 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_VEC_PACK_SFIX256);\n+\t  else if (out_n == 16 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_VEC_PACK_SFIX512);\n+\t}\n+      if (out_mode == SImode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CVTPS2DQ);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CVTPS2DQ256);\n+\t  else if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CVTPS2DQ512);\n+\t}\n+      break;\n+\n+    CASE_CFN_IROUND:\n+    CASE_CFN_LROUND:\n+    CASE_CFN_LLROUND:\n+      /* The round insn does not trap on denormals.  */\n+      if (flag_trapping_math || !TARGET_SSE4_1)\n+\tbreak;\n+\n+      if (out_mode == SImode && in_mode == DFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 2)\n+\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX);\n+\t  else if (out_n == 8 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX256);\n+\t  else if (out_n == 16 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX512);\n+\t}\n+      if (out_mode == SImode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPS_AZ_SFIX);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPS_AZ_SFIX256);\n+\t  else if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPS_AZ_SFIX512);\n+\t}\n+      break;\n+\n+    CASE_CFN_FLOOR:\n+      /* The round insn does not trap on denormals.  */\n+      if (flag_trapping_math || !TARGET_SSE4_1)\n+\tbreak;\n+\n+      if (out_mode == DFmode && in_mode == DFmode)\n+\t{\n+\t  if (out_n == 2 && in_n == 2)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD);\n+\t  else if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD256);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD512);\n+\t}\n+      if (out_mode == SFmode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS256);\n+\t  else if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS512);\n+\t}\n+      break;\n+\n+    CASE_CFN_CEIL:\n+      /* The round insn does not trap on denormals.  */\n+      if (flag_trapping_math || !TARGET_SSE4_1)\n+\tbreak;\n+\n+      if (out_mode == DFmode && in_mode == DFmode)\n+\t{\n+\t  if (out_n == 2 && in_n == 2)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD);\n+\t  else if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD256);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD512);\n+\t}\n+      if (out_mode == SFmode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS256);\n+\t  else if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS512);\n+\t}\n+      break;\n+\n+    CASE_CFN_TRUNC:\n+      /* The round insn does not trap on denormals.  */\n+      if (flag_trapping_math || !TARGET_SSE4_1)\n+\tbreak;\n+\n+      if (out_mode == DFmode && in_mode == DFmode)\n+\t{\n+\t  if (out_n == 2 && in_n == 2)\n+\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPD);\n+\t  else if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPD256);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPD512);\n+\t}\n+      if (out_mode == SFmode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPS);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPS256);\n+\t  else if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPS512);\n+\t}\n+      break;\n+\n+    CASE_CFN_RINT:\n+      /* The round insn does not trap on denormals.  */\n+      if (flag_trapping_math || !TARGET_SSE4_1)\n+\tbreak;\n+\n+      if (out_mode == DFmode && in_mode == DFmode)\n+\t{\n+\t  if (out_n == 2 && in_n == 2)\n+\t    return ix86_get_builtin (IX86_BUILTIN_RINTPD);\n+\t  else if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_RINTPD256);\n+\t}\n+      if (out_mode == SFmode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_RINTPS);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_RINTPS256);\n+\t}\n+      break;\n+\n+    CASE_CFN_FMA:\n+      if (out_mode == DFmode && in_mode == DFmode)\n+\t{\n+\t  if (out_n == 2 && in_n == 2)\n+\t    return ix86_get_builtin (IX86_BUILTIN_VFMADDPD);\n+\t  if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_VFMADDPD256);\n+\t}\n+      if (out_mode == SFmode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 4 && in_n == 4)\n+\t    return ix86_get_builtin (IX86_BUILTIN_VFMADDPS);\n+\t  if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_VFMADDPS256);\n+\t}\n+      break;\n+\n+    default:\n+      break;\n+    }\n+\n+  /* Dispatch to a handler for a vectorization library.  */\n+  if (ix86_veclib_handler)\n+    return ix86_veclib_handler (combined_fn (fn), type_out, type_in);\n+\n+  return NULL_TREE;\n+}\n+\n+/* Returns a decl of a function that implements gather load with\n+   memory type MEM_VECTYPE and index type INDEX_VECTYPE and SCALE.\n+   Return NULL_TREE if it is not available.  */\n+\n+tree\n+ix86_vectorize_builtin_gather (const_tree mem_vectype,\n+\t\t\t       const_tree index_type, int scale)\n+{\n+  bool si;\n+  enum ix86_builtins code;\n+\n+  if (! TARGET_AVX2 || !TARGET_USE_GATHER)\n+    return NULL_TREE;\n+\n+  if ((TREE_CODE (index_type) != INTEGER_TYPE\n+       && !POINTER_TYPE_P (index_type))\n+      || (TYPE_MODE (index_type) != SImode\n+\t  && TYPE_MODE (index_type) != DImode))\n+    return NULL_TREE;\n+\n+  if (TYPE_PRECISION (index_type) > POINTER_SIZE)\n+    return NULL_TREE;\n+\n+  /* v*gather* insn sign extends index to pointer mode.  */\n+  if (TYPE_PRECISION (index_type) < POINTER_SIZE\n+      && TYPE_UNSIGNED (index_type))\n+    return NULL_TREE;\n+\n+  if (scale <= 0\n+      || scale > 8\n+      || (scale & (scale - 1)) != 0)\n+    return NULL_TREE;\n+\n+  si = TYPE_MODE (index_type) == SImode;\n+  switch (TYPE_MODE (mem_vectype))\n+    {\n+    case E_V2DFmode:\n+      if (TARGET_AVX512VL)\n+\tcode = si ? IX86_BUILTIN_GATHER3SIV2DF : IX86_BUILTIN_GATHER3DIV2DF;\n+      else\n+\tcode = si ? IX86_BUILTIN_GATHERSIV2DF : IX86_BUILTIN_GATHERDIV2DF;\n+      break;\n+    case E_V4DFmode:\n+      if (TARGET_AVX512VL)\n+\tcode = si ? IX86_BUILTIN_GATHER3ALTSIV4DF : IX86_BUILTIN_GATHER3DIV4DF;\n+      else\n+\tcode = si ? IX86_BUILTIN_GATHERALTSIV4DF : IX86_BUILTIN_GATHERDIV4DF;\n+      break;\n+    case E_V2DImode:\n+      if (TARGET_AVX512VL)\n+\tcode = si ? IX86_BUILTIN_GATHER3SIV2DI : IX86_BUILTIN_GATHER3DIV2DI;\n+      else\n+\tcode = si ? IX86_BUILTIN_GATHERSIV2DI : IX86_BUILTIN_GATHERDIV2DI;\n+      break;\n+    case E_V4DImode:\n+      if (TARGET_AVX512VL)\n+\tcode = si ? IX86_BUILTIN_GATHER3ALTSIV4DI : IX86_BUILTIN_GATHER3DIV4DI;\n+      else\n+\tcode = si ? IX86_BUILTIN_GATHERALTSIV4DI : IX86_BUILTIN_GATHERDIV4DI;\n+      break;\n+    case E_V4SFmode:\n+      if (TARGET_AVX512VL)\n+\tcode = si ? IX86_BUILTIN_GATHER3SIV4SF : IX86_BUILTIN_GATHER3DIV4SF;\n+      else\n+\tcode = si ? IX86_BUILTIN_GATHERSIV4SF : IX86_BUILTIN_GATHERDIV4SF;\n+      break;\n+    case E_V8SFmode:\n+      if (TARGET_AVX512VL)\n+\tcode = si ? IX86_BUILTIN_GATHER3SIV8SF : IX86_BUILTIN_GATHER3ALTDIV8SF;\n+      else\n+\tcode = si ? IX86_BUILTIN_GATHERSIV8SF : IX86_BUILTIN_GATHERALTDIV8SF;\n+      break;\n+    case E_V4SImode:\n+      if (TARGET_AVX512VL)\n+\tcode = si ? IX86_BUILTIN_GATHER3SIV4SI : IX86_BUILTIN_GATHER3DIV4SI;\n+      else\n+\tcode = si ? IX86_BUILTIN_GATHERSIV4SI : IX86_BUILTIN_GATHERDIV4SI;\n+      break;\n+    case E_V8SImode:\n+      if (TARGET_AVX512VL)\n+\tcode = si ? IX86_BUILTIN_GATHER3SIV8SI : IX86_BUILTIN_GATHER3ALTDIV8SI;\n+      else\n+\tcode = si ? IX86_BUILTIN_GATHERSIV8SI : IX86_BUILTIN_GATHERALTDIV8SI;\n+      break;\n+    case E_V8DFmode:\n+      if (TARGET_AVX512F)\n+\tcode = si ? IX86_BUILTIN_GATHER3ALTSIV8DF : IX86_BUILTIN_GATHER3DIV8DF;\n+      else\n+\treturn NULL_TREE;\n+      break;\n+    case E_V8DImode:\n+      if (TARGET_AVX512F)\n+\tcode = si ? IX86_BUILTIN_GATHER3ALTSIV8DI : IX86_BUILTIN_GATHER3DIV8DI;\n+      else\n+\treturn NULL_TREE;\n+      break;\n+    case E_V16SFmode:\n+      if (TARGET_AVX512F)\n+\tcode = si ? IX86_BUILTIN_GATHER3SIV16SF : IX86_BUILTIN_GATHER3ALTDIV16SF;\n+      else\n+\treturn NULL_TREE;\n+      break;\n+    case E_V16SImode:\n+      if (TARGET_AVX512F)\n+\tcode = si ? IX86_BUILTIN_GATHER3SIV16SI : IX86_BUILTIN_GATHER3ALTDIV16SI;\n+      else\n+\treturn NULL_TREE;\n+      break;\n+    default:\n+      return NULL_TREE;\n+    }\n+\n+  return ix86_get_builtin (code);\n+}\n+\n+/* Returns a code for a target-specific builtin that implements\n+   reciprocal of the function, or NULL_TREE if not available.  */\n+\n+tree\n+ix86_builtin_reciprocal (tree fndecl)\n+{\n+  enum ix86_builtins fn_code\n+    = (enum ix86_builtins) DECL_FUNCTION_CODE (fndecl);\n+  switch (fn_code)\n+    {\n+      /* Vectorized version of sqrt to rsqrt conversion.  */\n+    case IX86_BUILTIN_SQRTPS_NR:\n+      return ix86_get_builtin (IX86_BUILTIN_RSQRTPS_NR);\n+\n+    case IX86_BUILTIN_SQRTPS_NR256:\n+      return ix86_get_builtin (IX86_BUILTIN_RSQRTPS_NR256);\n+\n+    default:\n+      return NULL_TREE;\n+    }\n+}\n+\n+/* Priority of i386 features, greater value is higher priority.   This is\n+   used to decide the order in which function dispatch must happen.  For\n+   instance, a version specialized for SSE4.2 should be checked for dispatch\n+   before a version for SSE3, as SSE4.2 implies SSE3.  */\n+enum feature_priority\n+{\n+  P_ZERO = 0,\n+  P_MMX,\n+  P_SSE,\n+  P_SSE2,\n+  P_SSE3,\n+  P_SSSE3,\n+  P_PROC_SSSE3,\n+  P_SSE4_A,\n+  P_PROC_SSE4_A,\n+  P_SSE4_1,\n+  P_SSE4_2,\n+  P_PROC_SSE4_2,\n+  P_POPCNT,\n+  P_AES,\n+  P_PCLMUL,\n+  P_AVX,\n+  P_PROC_AVX,\n+  P_BMI,\n+  P_PROC_BMI,\n+  P_FMA4,\n+  P_XOP,\n+  P_PROC_XOP,\n+  P_FMA,\n+  P_PROC_FMA,\n+  P_BMI2,\n+  P_AVX2,\n+  P_PROC_AVX2,\n+  P_AVX512F,\n+  P_PROC_AVX512F\n+};\n+\n+/* This is the order of bit-fields in __processor_features in cpuinfo.c */\n+enum processor_features\n+{\n+  F_CMOV = 0,\n+  F_MMX,\n+  F_POPCNT,\n+  F_SSE,\n+  F_SSE2,\n+  F_SSE3,\n+  F_SSSE3,\n+  F_SSE4_1,\n+  F_SSE4_2,\n+  F_AVX,\n+  F_AVX2,\n+  F_SSE4_A,\n+  F_FMA4,\n+  F_XOP,\n+  F_FMA,\n+  F_AVX512F,\n+  F_BMI,\n+  F_BMI2,\n+  F_AES,\n+  F_PCLMUL,\n+  F_AVX512VL,\n+  F_AVX512BW,\n+  F_AVX512DQ,\n+  F_AVX512CD,\n+  F_AVX512ER,\n+  F_AVX512PF,\n+  F_AVX512VBMI,\n+  F_AVX512IFMA,\n+  F_AVX5124VNNIW,\n+  F_AVX5124FMAPS,\n+  F_AVX512VPOPCNTDQ,\n+  F_AVX512VBMI2,\n+  F_GFNI,\n+  F_VPCLMULQDQ,\n+  F_AVX512VNNI,\n+  F_AVX512BITALG,\n+  F_MAX\n+};\n+\n+/* These are the values for vendor types and cpu types  and subtypes\n+   in cpuinfo.c.  Cpu types and subtypes should be subtracted by\n+   the corresponding start value.  */\n+enum processor_model\n+{\n+  M_INTEL = 1,\n+  M_AMD,\n+  M_CPU_TYPE_START,\n+  M_INTEL_BONNELL,\n+  M_INTEL_CORE2,\n+  M_INTEL_COREI7,\n+  M_AMDFAM10H,\n+  M_AMDFAM15H,\n+  M_INTEL_SILVERMONT,\n+  M_INTEL_KNL,\n+  M_AMD_BTVER1,\n+  M_AMD_BTVER2,\n+  M_AMDFAM17H,\n+  M_INTEL_KNM,\n+  M_INTEL_GOLDMONT,\n+  M_INTEL_GOLDMONT_PLUS,\n+  M_INTEL_TREMONT,\n+  M_CPU_SUBTYPE_START,\n+  M_INTEL_COREI7_NEHALEM,\n+  M_INTEL_COREI7_WESTMERE,\n+  M_INTEL_COREI7_SANDYBRIDGE,\n+  M_AMDFAM10H_BARCELONA,\n+  M_AMDFAM10H_SHANGHAI,\n+  M_AMDFAM10H_ISTANBUL,\n+  M_AMDFAM15H_BDVER1,\n+  M_AMDFAM15H_BDVER2,\n+  M_AMDFAM15H_BDVER3,\n+  M_AMDFAM15H_BDVER4,\n+  M_AMDFAM17H_ZNVER1,\n+  M_INTEL_COREI7_IVYBRIDGE,\n+  M_INTEL_COREI7_HASWELL,\n+  M_INTEL_COREI7_BROADWELL,\n+  M_INTEL_COREI7_SKYLAKE,\n+  M_INTEL_COREI7_SKYLAKE_AVX512,\n+  M_INTEL_COREI7_CANNONLAKE,\n+  M_INTEL_COREI7_ICELAKE_CLIENT,\n+  M_INTEL_COREI7_ICELAKE_SERVER,\n+  M_AMDFAM17H_ZNVER2,\n+  M_INTEL_COREI7_CASCADELAKE\n+};\n+\n+struct _arch_names_table\n+{\n+  const char *const name;\n+  const enum processor_model model;\n+};\n+\n+static const _arch_names_table arch_names_table[] =\n+{\n+  {\"amd\", M_AMD},\n+  {\"intel\", M_INTEL},\n+  {\"atom\", M_INTEL_BONNELL},\n+  {\"slm\", M_INTEL_SILVERMONT},\n+  {\"core2\", M_INTEL_CORE2},\n+  {\"corei7\", M_INTEL_COREI7},\n+  {\"nehalem\", M_INTEL_COREI7_NEHALEM},\n+  {\"westmere\", M_INTEL_COREI7_WESTMERE},\n+  {\"sandybridge\", M_INTEL_COREI7_SANDYBRIDGE},\n+  {\"ivybridge\", M_INTEL_COREI7_IVYBRIDGE},\n+  {\"haswell\", M_INTEL_COREI7_HASWELL},\n+  {\"broadwell\", M_INTEL_COREI7_BROADWELL},\n+  {\"skylake\", M_INTEL_COREI7_SKYLAKE},\n+  {\"skylake-avx512\", M_INTEL_COREI7_SKYLAKE_AVX512},\n+  {\"cannonlake\", M_INTEL_COREI7_CANNONLAKE},\n+  {\"icelake-client\", M_INTEL_COREI7_ICELAKE_CLIENT},\n+  {\"icelake-server\", M_INTEL_COREI7_ICELAKE_SERVER},\n+  {\"cascadelake\", M_INTEL_COREI7_CASCADELAKE},\n+  {\"bonnell\", M_INTEL_BONNELL},\n+  {\"silvermont\", M_INTEL_SILVERMONT},\n+  {\"goldmont\", M_INTEL_GOLDMONT},\n+  {\"goldmont-plus\", M_INTEL_GOLDMONT_PLUS},\n+  {\"tremont\", M_INTEL_TREMONT},\n+  {\"knl\", M_INTEL_KNL},\n+  {\"knm\", M_INTEL_KNM},\n+  {\"amdfam10h\", M_AMDFAM10H},\n+  {\"barcelona\", M_AMDFAM10H_BARCELONA},\n+  {\"shanghai\", M_AMDFAM10H_SHANGHAI},\n+  {\"istanbul\", M_AMDFAM10H_ISTANBUL},\n+  {\"btver1\", M_AMD_BTVER1},\n+  {\"amdfam15h\", M_AMDFAM15H},\n+  {\"bdver1\", M_AMDFAM15H_BDVER1},\n+  {\"bdver2\", M_AMDFAM15H_BDVER2},\n+  {\"bdver3\", M_AMDFAM15H_BDVER3},\n+  {\"bdver4\", M_AMDFAM15H_BDVER4},\n+  {\"btver2\", M_AMD_BTVER2},\n+  {\"amdfam17h\", M_AMDFAM17H},\n+  {\"znver1\", M_AMDFAM17H_ZNVER1},\n+  {\"znver2\", M_AMDFAM17H_ZNVER2},\n+};\n+\n+/* These are the target attribute strings for which a dispatcher is\n+   available, from fold_builtin_cpu.  */\n+struct _isa_names_table\n+{\n+  const char *const name;\n+  const enum processor_features feature;\n+  const enum feature_priority priority;\n+};\n+\n+static const _isa_names_table isa_names_table[] =\n+{\n+  {\"cmov\",    F_CMOV,\tP_ZERO},\n+  {\"mmx\",     F_MMX,\tP_MMX},\n+  {\"popcnt\",  F_POPCNT,\tP_POPCNT},\n+  {\"sse\",     F_SSE,\tP_SSE},\n+  {\"sse2\",    F_SSE2,\tP_SSE2},\n+  {\"sse3\",    F_SSE3,\tP_SSE3},\n+  {\"ssse3\",   F_SSSE3,\tP_SSSE3},\n+  {\"sse4a\",   F_SSE4_A,\tP_SSE4_A},\n+  {\"sse4.1\",  F_SSE4_1,\tP_SSE4_1},\n+  {\"sse4.2\",  F_SSE4_2,\tP_SSE4_2},\n+  {\"avx\",     F_AVX,\tP_AVX},\n+  {\"fma4\",    F_FMA4,\tP_FMA4},\n+  {\"xop\",     F_XOP,\tP_XOP},\n+  {\"fma\",     F_FMA,\tP_FMA},\n+  {\"avx2\",    F_AVX2,\tP_AVX2},\n+  {\"avx512f\", F_AVX512F, P_AVX512F},\n+  {\"bmi\",     F_BMI,\tP_BMI},\n+  {\"bmi2\",    F_BMI2,\tP_BMI2},\n+  {\"aes\",     F_AES,\tP_AES},\n+  {\"pclmul\",  F_PCLMUL,\tP_PCLMUL},\n+  {\"avx512vl\",F_AVX512VL, P_ZERO},\n+  {\"avx512bw\",F_AVX512BW, P_ZERO},\n+  {\"avx512dq\",F_AVX512DQ, P_ZERO},\n+  {\"avx512cd\",F_AVX512CD, P_ZERO},\n+  {\"avx512er\",F_AVX512ER, P_ZERO},\n+  {\"avx512pf\",F_AVX512PF, P_ZERO},\n+  {\"avx512vbmi\",F_AVX512VBMI, P_ZERO},\n+  {\"avx512ifma\",F_AVX512IFMA, P_ZERO},\n+  {\"avx5124vnniw\",F_AVX5124VNNIW, P_ZERO},\n+  {\"avx5124fmaps\",F_AVX5124FMAPS, P_ZERO},\n+  {\"avx512vpopcntdq\",F_AVX512VPOPCNTDQ,\tP_ZERO},\n+  {\"avx512vbmi2\", F_AVX512VBMI2, P_ZERO},\n+  {\"gfni\",\tF_GFNI,\tP_ZERO},\n+  {\"vpclmulqdq\", F_VPCLMULQDQ, P_ZERO},\n+  {\"avx512vnni\", F_AVX512VNNI, P_ZERO},\n+  {\"avx512bitalg\", F_AVX512BITALG, P_ZERO}\n+};\n+\n+/* This parses the attribute arguments to target in DECL and determines\n+   the right builtin to use to match the platform specification.\n+   It returns the priority value for this version decl.  If PREDICATE_LIST\n+   is not NULL, it stores the list of cpu features that need to be checked\n+   before dispatching this function.  */\n+\n+unsigned int\n+get_builtin_code_for_version (tree decl, tree *predicate_list)\n+{\n+  tree attrs;\n+  struct cl_target_option cur_target;\n+  tree target_node;\n+  struct cl_target_option *new_target;\n+  const char *arg_str = NULL;\n+  const char *attrs_str = NULL;\n+  char *tok_str = NULL;\n+  char *token;\n+\n+  enum feature_priority priority = P_ZERO;\n+\n+  static unsigned int NUM_FEATURES\n+    = sizeof (isa_names_table) / sizeof (_isa_names_table);\n+\n+  unsigned int i;\n+\n+  tree predicate_chain = NULL_TREE;\n+  tree predicate_decl, predicate_arg;\n+\n+  attrs = lookup_attribute (\"target\", DECL_ATTRIBUTES (decl));\n+  gcc_assert (attrs != NULL);\n+\n+  attrs = TREE_VALUE (TREE_VALUE (attrs));\n+\n+  gcc_assert (TREE_CODE (attrs) == STRING_CST);\n+  attrs_str = TREE_STRING_POINTER (attrs);\n+\n+  /* Return priority zero for default function.  */\n+  if (strcmp (attrs_str, \"default\") == 0)\n+    return 0;\n+\n+  /* Handle arch= if specified.  For priority, set it to be 1 more than\n+     the best instruction set the processor can handle.  For instance, if\n+     there is a version for atom and a version for ssse3 (the highest ISA\n+     priority for atom), the atom version must be checked for dispatch\n+     before the ssse3 version. */\n+  if (strstr (attrs_str, \"arch=\") != NULL)\n+    {\n+      cl_target_option_save (&cur_target, &global_options);\n+      target_node\n+\t= ix86_valid_target_attribute_tree (decl, attrs, &global_options,\n+\t\t\t\t\t    &global_options_set, 0);\n+    \n+      gcc_assert (target_node);\n+      if (target_node == error_mark_node)\n+\treturn 0;\n+      new_target = TREE_TARGET_OPTION (target_node);\n+      gcc_assert (new_target);\n+      \n+      if (new_target->arch_specified && new_target->arch > 0)\n+\t{\n+\t  switch (new_target->arch)\n+\t    {\n+\t    case PROCESSOR_CORE2:\n+\t      arg_str = \"core2\";\n+\t      priority = P_PROC_SSSE3;\n+\t      break;\n+\t    case PROCESSOR_NEHALEM:\n+\t      if (new_target->x_ix86_isa_flags & OPTION_MASK_ISA_PCLMUL)\n+\t\t{\n+\t\t  arg_str = \"westmere\";\n+\t\t  priority = P_PCLMUL;\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  /* We translate \"arch=corei7\" and \"arch=nehalem\" to\n+\t\t     \"corei7\" so that it will be mapped to M_INTEL_COREI7\n+\t\t     as cpu type to cover all M_INTEL_COREI7_XXXs.  */\n+\t\t  arg_str = \"corei7\";\n+\t\t  priority = P_PROC_SSE4_2;\n+\t\t}\n+\t      break;\n+\t    case PROCESSOR_SANDYBRIDGE:\n+\t      if (new_target->x_ix86_isa_flags & OPTION_MASK_ISA_F16C)\n+\t\targ_str = \"ivybridge\";\n+\t      else\n+\t\targ_str = \"sandybridge\";\n+\t      priority = P_PROC_AVX;\n+\t      break;\n+\t    case PROCESSOR_HASWELL:\n+\t      if (new_target->x_ix86_isa_flags & OPTION_MASK_ISA_ADX)\n+\t\targ_str = \"broadwell\";\n+\t      else\n+\t\targ_str = \"haswell\";\n+\t      priority = P_PROC_AVX2;\n+\t      break;\n+\t    case PROCESSOR_SKYLAKE:\n+\t      arg_str = \"skylake\";\n+\t      priority = P_PROC_AVX2;\n+\t      break;\n+\t    case PROCESSOR_SKYLAKE_AVX512:\n+\t      arg_str = \"skylake-avx512\";\n+\t      priority = P_PROC_AVX512F;\n+\t      break;\n+\t    case PROCESSOR_CANNONLAKE:\n+\t      arg_str = \"cannonlake\";\n+\t      priority = P_PROC_AVX512F;\n+\t      break;\n+\t    case PROCESSOR_ICELAKE_CLIENT:\n+\t      arg_str = \"icelake-client\";\n+\t      priority = P_PROC_AVX512F;\n+\t      break;\n+\t    case PROCESSOR_ICELAKE_SERVER:\n+\t      arg_str = \"icelake-server\";\n+\t      priority = P_PROC_AVX512F;\n+\t      break;\n+\t    case PROCESSOR_CASCADELAKE:\n+\t      arg_str = \"cascadelake\";\n+\t      priority = P_PROC_AVX512F;\n+\t      break;\n+\t    case PROCESSOR_BONNELL:\n+\t      arg_str = \"bonnell\";\n+\t      priority = P_PROC_SSSE3;\n+\t      break;\n+\t    case PROCESSOR_KNL:\n+\t      arg_str = \"knl\";\n+\t      priority = P_PROC_AVX512F;\n+\t      break;\n+\t    case PROCESSOR_KNM:\n+\t      arg_str = \"knm\";\n+\t      priority = P_PROC_AVX512F;\n+\t      break;\n+\t    case PROCESSOR_SILVERMONT:\n+\t      arg_str = \"silvermont\";\n+\t      priority = P_PROC_SSE4_2;\n+\t      break;\n+\t    case PROCESSOR_GOLDMONT:\n+\t      arg_str = \"goldmont\";\n+\t      priority = P_PROC_SSE4_2;\n+\t      break;\n+\t    case PROCESSOR_GOLDMONT_PLUS:\n+\t      arg_str = \"goldmont-plus\";\n+\t      priority = P_PROC_SSE4_2;\n+\t      break;\n+\t    case PROCESSOR_TREMONT:\n+\t      arg_str = \"tremont\";\n+\t      priority = P_PROC_SSE4_2;\n+\t      break;\n+\t    case PROCESSOR_AMDFAM10:\n+\t      arg_str = \"amdfam10h\";\n+\t      priority = P_PROC_SSE4_A;\n+\t      break;\n+\t    case PROCESSOR_BTVER1:\n+\t      arg_str = \"btver1\";\n+\t      priority = P_PROC_SSE4_A;\n+\t      break;\n+\t    case PROCESSOR_BTVER2:\n+\t      arg_str = \"btver2\";\n+\t      priority = P_PROC_BMI;\n+\t      break;\n+\t    case PROCESSOR_BDVER1:\n+\t      arg_str = \"bdver1\";\n+\t      priority = P_PROC_XOP;\n+\t      break;\n+\t    case PROCESSOR_BDVER2:\n+\t      arg_str = \"bdver2\";\n+\t      priority = P_PROC_FMA;\n+\t      break;\n+\t    case PROCESSOR_BDVER3:\n+\t      arg_str = \"bdver3\";\n+\t      priority = P_PROC_FMA;\n+\t      break;\n+\t    case PROCESSOR_BDVER4:\n+\t      arg_str = \"bdver4\";\n+\t      priority = P_PROC_AVX2;\n+\t      break;\n+\t    case PROCESSOR_ZNVER1:\n+\t      arg_str = \"znver1\";\n+\t      priority = P_PROC_AVX2;\n+\t      break;\n+\t    case PROCESSOR_ZNVER2:\n+\t      arg_str = \"znver2\";\n+\t      priority = P_PROC_AVX2;\n+\t      break;\n+\t    }\n+\t}\n+\n+      cl_target_option_restore (&global_options, &cur_target);\n+\t\n+      if (predicate_list && arg_str == NULL)\n+\t{\n+\t  error_at (DECL_SOURCE_LOCATION (decl),\n+\t\t    \"no dispatcher found for the versioning attributes\");\n+\t  return 0;\n+\t}\n+    \n+      if (predicate_list)\n+\t{\n+          predicate_decl = ix86_builtins [(int) IX86_BUILTIN_CPU_IS];\n+          /* For a C string literal the length includes the trailing NULL.  */\n+          predicate_arg = build_string_literal (strlen (arg_str) + 1, arg_str);\n+          predicate_chain = tree_cons (predicate_decl, predicate_arg,\n+\t\t\t\t       predicate_chain);\n+\t}\n+    }\n+\n+  /* Process feature name.  */\n+  tok_str =  (char *) xmalloc (strlen (attrs_str) + 1);\n+  strcpy (tok_str, attrs_str);\n+  token = strtok (tok_str, \",\");\n+  predicate_decl = ix86_builtins [(int) IX86_BUILTIN_CPU_SUPPORTS];\n+\n+  while (token != NULL)\n+    {\n+      /* Do not process \"arch=\"  */\n+      if (strncmp (token, \"arch=\", 5) == 0)\n+\t{\n+\t  token = strtok (NULL, \",\");\n+\t  continue;\n+\t}\n+      for (i = 0; i < NUM_FEATURES; ++i)\n+\t{\n+\t  if (strcmp (token, isa_names_table[i].name) == 0)\n+\t    {\n+\t      if (predicate_list)\n+\t\t{\n+\t\t  predicate_arg = build_string_literal (\n+\t\t\t\t  strlen (isa_names_table[i].name) + 1,\n+\t\t\t\t  isa_names_table[i].name);\n+\t\t  predicate_chain = tree_cons (predicate_decl, predicate_arg,\n+\t\t\t\t\t       predicate_chain);\n+\t\t}\n+\t      /* Find the maximum priority feature.  */\n+\t      if (isa_names_table[i].priority > priority)\n+\t\tpriority = isa_names_table[i].priority;\n+\n+\t      break;\n+\t    }\n+\t}\n+      if (predicate_list && priority == P_ZERO)\n+\t{\n+\t  error_at (DECL_SOURCE_LOCATION (decl),\n+\t\t    \"ISA %qs is not supported in %<target%> attribute, \"\n+\t\t    \"use %<arch=%> syntax\", token);\n+\t  return 0;\n+\t}\n+      token = strtok (NULL, \",\");\n+    }\n+  free (tok_str);\n+\n+  if (predicate_list && predicate_chain == NULL_TREE)\n+    {\n+      error_at (DECL_SOURCE_LOCATION (decl),\n+\t        \"no dispatcher found for the versioning attributes: %s\",\n+\t        attrs_str);\n+      return 0;\n+    }\n+  else if (predicate_list)\n+    {\n+      predicate_chain = nreverse (predicate_chain);\n+      *predicate_list = predicate_chain;\n+    }\n+\n+  return priority; \n+}\n+\n+/* This builds the processor_model struct type defined in\n+   libgcc/config/i386/cpuinfo.c  */\n+\n+static tree\n+build_processor_model_struct (void)\n+{\n+  const char *field_name[] = {\"__cpu_vendor\", \"__cpu_type\", \"__cpu_subtype\",\n+\t\t\t      \"__cpu_features\"};\n+  tree field = NULL_TREE, field_chain = NULL_TREE;\n+  int i;\n+  tree type = make_node (RECORD_TYPE);\n+\n+  /* The first 3 fields are unsigned int.  */\n+  for (i = 0; i < 3; ++i)\n+    {\n+      field = build_decl (UNKNOWN_LOCATION, FIELD_DECL,\n+\t\t\t  get_identifier (field_name[i]), unsigned_type_node);\n+      if (field_chain != NULL_TREE)\n+\tDECL_CHAIN (field) = field_chain;\n+      field_chain = field;\n+    }\n+\n+  /* The last field is an array of unsigned integers of size one.  */\n+  field = build_decl (UNKNOWN_LOCATION, FIELD_DECL,\n+\t\t      get_identifier (field_name[3]),\n+\t\t      build_array_type (unsigned_type_node,\n+\t\t\t\t\tbuild_index_type (size_one_node)));\n+  if (field_chain != NULL_TREE)\n+    DECL_CHAIN (field) = field_chain;\n+  field_chain = field;\n+\n+  finish_builtin_struct (type, \"__processor_model\", field_chain, NULL_TREE);\n+  return type;\n+}\n+\n+/* Returns a extern, comdat VAR_DECL of type TYPE and name NAME. */\n+\n+static tree\n+make_var_decl (tree type, const char *name)\n+{\n+  tree new_decl;\n+\n+  new_decl = build_decl (UNKNOWN_LOCATION,\n+\t                 VAR_DECL,\n+\t  \t         get_identifier(name),\n+\t\t         type);\n+\n+  DECL_EXTERNAL (new_decl) = 1;\n+  TREE_STATIC (new_decl) = 1;\n+  TREE_PUBLIC (new_decl) = 1;\n+  DECL_INITIAL (new_decl) = 0;\n+  DECL_ARTIFICIAL (new_decl) = 0;\n+  DECL_PRESERVE_P (new_decl) = 1;\n+\n+  make_decl_one_only (new_decl, DECL_ASSEMBLER_NAME (new_decl));\n+  assemble_variable (new_decl, 0, 0, 0);\n+\n+  return new_decl;\n+}\n+\n+/* FNDECL is a __builtin_cpu_is or a __builtin_cpu_supports call that is folded\n+   into an integer defined in libgcc/config/i386/cpuinfo.c */\n+\n+tree\n+fold_builtin_cpu (tree fndecl, tree *args)\n+{\n+  unsigned int i;\n+  enum ix86_builtins fn_code = (enum ix86_builtins)\n+\t\t\t\tDECL_FUNCTION_CODE (fndecl);\n+  tree param_string_cst = NULL;\n+\n+  tree __processor_model_type = build_processor_model_struct ();\n+  tree __cpu_model_var = make_var_decl (__processor_model_type,\n+\t\t\t\t\t\"__cpu_model\");\n+\n+\n+  varpool_node::add (__cpu_model_var);\n+\n+  gcc_assert ((args != NULL) && (*args != NULL));\n+\n+  param_string_cst = *args;\n+  while (param_string_cst\n+\t && TREE_CODE (param_string_cst) !=  STRING_CST)\n+    {\n+      /* *args must be a expr that can contain other EXPRS leading to a\n+\t STRING_CST.   */\n+      if (!EXPR_P (param_string_cst))\n+ \t{\n+\t  error (\"parameter to builtin must be a string constant or literal\");\n+\t  return integer_zero_node;\n+\t}\n+      param_string_cst = TREE_OPERAND (EXPR_CHECK (param_string_cst), 0);\n+    }\n+\n+  gcc_assert (param_string_cst);\n+\n+  if (fn_code == IX86_BUILTIN_CPU_IS)\n+    {\n+      tree ref;\n+      tree field;\n+      tree final;\n+\n+      unsigned int field_val = 0;\n+      unsigned int NUM_ARCH_NAMES\n+\t= sizeof (arch_names_table) / sizeof (struct _arch_names_table);\n+\n+      for (i = 0; i < NUM_ARCH_NAMES; i++)\n+\tif (strcmp (arch_names_table[i].name,\n+\t    TREE_STRING_POINTER (param_string_cst)) == 0)\n+\t  break;\n+\n+      if (i == NUM_ARCH_NAMES)\n+\t{\n+\t  error (\"parameter to builtin not valid: %s\",\n+\t         TREE_STRING_POINTER (param_string_cst));\n+\t  return integer_zero_node;\n+\t}\n+\n+      field = TYPE_FIELDS (__processor_model_type);\n+      field_val = arch_names_table[i].model;\n+\n+      /* CPU types are stored in the next field.  */\n+      if (field_val > M_CPU_TYPE_START\n+\t  && field_val < M_CPU_SUBTYPE_START)\n+\t{\n+\t  field = DECL_CHAIN (field);\n+\t  field_val -= M_CPU_TYPE_START;\n+\t}\n+\n+      /* CPU subtypes are stored in the next field.  */\n+      if (field_val > M_CPU_SUBTYPE_START)\n+\t{\n+\t  field = DECL_CHAIN ( DECL_CHAIN (field));\n+\t  field_val -= M_CPU_SUBTYPE_START;\n+\t}\n+\n+      /* Get the appropriate field in __cpu_model.  */\n+      ref = build3 (COMPONENT_REF, TREE_TYPE (field), __cpu_model_var,\n+\t\t    field, NULL_TREE);\n+\n+      /* Check the value.  */\n+      final = build2 (EQ_EXPR, unsigned_type_node, ref,\n+\t\t      build_int_cstu (unsigned_type_node, field_val));\n+      return build1 (CONVERT_EXPR, integer_type_node, final);\n+    }\n+  else if (fn_code == IX86_BUILTIN_CPU_SUPPORTS)\n+    {\n+      tree ref;\n+      tree array_elt;\n+      tree field;\n+      tree final;\n+\n+      unsigned int field_val = 0;\n+      unsigned int NUM_ISA_NAMES\n+\t= sizeof (isa_names_table) / sizeof (struct _isa_names_table);\n+\n+      for (i = 0; i < NUM_ISA_NAMES; i++)\n+\tif (strcmp (isa_names_table[i].name,\n+\t    TREE_STRING_POINTER (param_string_cst)) == 0)\n+\t  break;\n+\n+      if (i == NUM_ISA_NAMES)\n+\t{\n+\t  error (\"parameter to builtin not valid: %s\",\n+\t       \t TREE_STRING_POINTER (param_string_cst));\n+\t  return integer_zero_node;\n+\t}\n+\n+      if (isa_names_table[i].feature >= 32)\n+\t{\n+\t  tree __cpu_features2_var = make_var_decl (unsigned_type_node,\n+\t\t\t\t\t\t    \"__cpu_features2\");\n+\n+\t  varpool_node::add (__cpu_features2_var);\n+\t  field_val = (1U << (isa_names_table[i].feature - 32));\n+\t  /* Return __cpu_features2 & field_val  */\n+\t  final = build2 (BIT_AND_EXPR, unsigned_type_node,\n+\t\t\t  __cpu_features2_var,\n+\t\t\t  build_int_cstu (unsigned_type_node, field_val));\n+\t  return build1 (CONVERT_EXPR, integer_type_node, final);\n+\t}\n+\n+      field = TYPE_FIELDS (__processor_model_type);\n+      /* Get the last field, which is __cpu_features.  */\n+      while (DECL_CHAIN (field))\n+        field = DECL_CHAIN (field);\n+\n+      /* Get the appropriate field: __cpu_model.__cpu_features  */\n+      ref = build3 (COMPONENT_REF, TREE_TYPE (field), __cpu_model_var,\n+\t\t    field, NULL_TREE);\n+\n+      /* Access the 0th element of __cpu_features array.  */\n+      array_elt = build4 (ARRAY_REF, unsigned_type_node, ref,\n+\t\t\t  integer_zero_node, NULL_TREE, NULL_TREE);\n+\n+      field_val = (1U << isa_names_table[i].feature);\n+      /* Return __cpu_model.__cpu_features[0] & field_val  */\n+      final = build2 (BIT_AND_EXPR, unsigned_type_node, array_elt,\n+\t\t      build_int_cstu (unsigned_type_node, field_val));\n+      return build1 (CONVERT_EXPR, integer_type_node, final);\n+    }\n+  gcc_unreachable ();\n+}\n+\n+#include \"gt-i386-builtins.h\""}, {"sha": "c0264e5bf1d1ef27670e4c4a9e7bb13d608a08ca", "filename": "gcc/config/i386/i386-builtins.h", "status": "added", "additions": 330, "deletions": 0, "changes": 330, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-builtins.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-builtins.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-builtins.h?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "patch": "@@ -0,0 +1,330 @@\n+/* Copyright (C) 1988-2019 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_I386_BUILTINS_H\n+#define GCC_I386_BUILTINS_H\n+\n+/* The following file contains several enumerations and data structures\n+   built from the definitions in i386-builtin-types.def.  */\n+\n+#include \"i386-builtin-types.inc\"\n+\n+/* Codes for all the SSE/MMX builtins.  Builtins not mentioned in any\n+   bdesc_* arrays below should come first, then builtins for each bdesc_*\n+   array in ascending order, so that we can use direct array accesses.  */\n+enum ix86_builtins\n+{\n+  IX86_BUILTIN_MASKMOVQ,\n+  IX86_BUILTIN_LDMXCSR,\n+  IX86_BUILTIN_STMXCSR,\n+  IX86_BUILTIN_MASKMOVDQU,\n+  IX86_BUILTIN_PSLLDQ128,\n+  IX86_BUILTIN_CLFLUSH,\n+  IX86_BUILTIN_MONITOR,\n+  IX86_BUILTIN_MWAIT,\n+  IX86_BUILTIN_UMONITOR,\n+  IX86_BUILTIN_UMWAIT,\n+  IX86_BUILTIN_TPAUSE,\n+  IX86_BUILTIN_CLZERO,\n+  IX86_BUILTIN_CLDEMOTE,\n+  IX86_BUILTIN_VEC_INIT_V2SI,\n+  IX86_BUILTIN_VEC_INIT_V4HI,\n+  IX86_BUILTIN_VEC_INIT_V8QI,\n+  IX86_BUILTIN_VEC_EXT_V2DF,\n+  IX86_BUILTIN_VEC_EXT_V2DI,\n+  IX86_BUILTIN_VEC_EXT_V4SF,\n+  IX86_BUILTIN_VEC_EXT_V4SI,\n+  IX86_BUILTIN_VEC_EXT_V8HI,\n+  IX86_BUILTIN_VEC_EXT_V2SI,\n+  IX86_BUILTIN_VEC_EXT_V4HI,\n+  IX86_BUILTIN_VEC_EXT_V16QI,\n+  IX86_BUILTIN_VEC_SET_V2DI,\n+  IX86_BUILTIN_VEC_SET_V4SF,\n+  IX86_BUILTIN_VEC_SET_V4SI,\n+  IX86_BUILTIN_VEC_SET_V8HI,\n+  IX86_BUILTIN_VEC_SET_V4HI,\n+  IX86_BUILTIN_VEC_SET_V16QI,\n+  IX86_BUILTIN_GATHERSIV2DF,\n+  IX86_BUILTIN_GATHERSIV4DF,\n+  IX86_BUILTIN_GATHERDIV2DF,\n+  IX86_BUILTIN_GATHERDIV4DF,\n+  IX86_BUILTIN_GATHERSIV4SF,\n+  IX86_BUILTIN_GATHERSIV8SF,\n+  IX86_BUILTIN_GATHERDIV4SF,\n+  IX86_BUILTIN_GATHERDIV8SF,\n+  IX86_BUILTIN_GATHERSIV2DI,\n+  IX86_BUILTIN_GATHERSIV4DI,\n+  IX86_BUILTIN_GATHERDIV2DI,\n+  IX86_BUILTIN_GATHERDIV4DI,\n+  IX86_BUILTIN_GATHERSIV4SI,\n+  IX86_BUILTIN_GATHERSIV8SI,\n+  IX86_BUILTIN_GATHERDIV4SI,\n+  IX86_BUILTIN_GATHERDIV8SI,\n+  IX86_BUILTIN_GATHER3SIV8SF,\n+  IX86_BUILTIN_GATHER3SIV4SF,\n+  IX86_BUILTIN_GATHER3SIV4DF,\n+  IX86_BUILTIN_GATHER3SIV2DF,\n+  IX86_BUILTIN_GATHER3DIV8SF,\n+  IX86_BUILTIN_GATHER3DIV4SF,\n+  IX86_BUILTIN_GATHER3DIV4DF,\n+  IX86_BUILTIN_GATHER3DIV2DF,\n+  IX86_BUILTIN_GATHER3SIV8SI,\n+  IX86_BUILTIN_GATHER3SIV4SI,\n+  IX86_BUILTIN_GATHER3SIV4DI,\n+  IX86_BUILTIN_GATHER3SIV2DI,\n+  IX86_BUILTIN_GATHER3DIV8SI,\n+  IX86_BUILTIN_GATHER3DIV4SI,\n+  IX86_BUILTIN_GATHER3DIV4DI,\n+  IX86_BUILTIN_GATHER3DIV2DI,\n+  IX86_BUILTIN_SCATTERSIV8SF,\n+  IX86_BUILTIN_SCATTERSIV4SF,\n+  IX86_BUILTIN_SCATTERSIV4DF,\n+  IX86_BUILTIN_SCATTERSIV2DF,\n+  IX86_BUILTIN_SCATTERDIV8SF,\n+  IX86_BUILTIN_SCATTERDIV4SF,\n+  IX86_BUILTIN_SCATTERDIV4DF,\n+  IX86_BUILTIN_SCATTERDIV2DF,\n+  IX86_BUILTIN_SCATTERSIV8SI,\n+  IX86_BUILTIN_SCATTERSIV4SI,\n+  IX86_BUILTIN_SCATTERSIV4DI,\n+  IX86_BUILTIN_SCATTERSIV2DI,\n+  IX86_BUILTIN_SCATTERDIV8SI,\n+  IX86_BUILTIN_SCATTERDIV4SI,\n+  IX86_BUILTIN_SCATTERDIV4DI,\n+  IX86_BUILTIN_SCATTERDIV2DI,\n+  /* Alternate 4 and 8 element gather/scatter for the vectorizer\n+     where all operands are 32-byte or 64-byte wide respectively.  */\n+  IX86_BUILTIN_GATHERALTSIV4DF,\n+  IX86_BUILTIN_GATHERALTDIV8SF,\n+  IX86_BUILTIN_GATHERALTSIV4DI,\n+  IX86_BUILTIN_GATHERALTDIV8SI,\n+  IX86_BUILTIN_GATHER3ALTDIV16SF,\n+  IX86_BUILTIN_GATHER3ALTDIV16SI,\n+  IX86_BUILTIN_GATHER3ALTSIV4DF,\n+  IX86_BUILTIN_GATHER3ALTDIV8SF,\n+  IX86_BUILTIN_GATHER3ALTSIV4DI,\n+  IX86_BUILTIN_GATHER3ALTDIV8SI,\n+  IX86_BUILTIN_GATHER3ALTSIV8DF,\n+  IX86_BUILTIN_GATHER3ALTSIV8DI,\n+  IX86_BUILTIN_GATHER3DIV16SF,\n+  IX86_BUILTIN_GATHER3DIV16SI,\n+  IX86_BUILTIN_GATHER3DIV8DF,\n+  IX86_BUILTIN_GATHER3DIV8DI,\n+  IX86_BUILTIN_GATHER3SIV16SF,\n+  IX86_BUILTIN_GATHER3SIV16SI,\n+  IX86_BUILTIN_GATHER3SIV8DF,\n+  IX86_BUILTIN_GATHER3SIV8DI,\n+  IX86_BUILTIN_SCATTERALTSIV8DF,\n+  IX86_BUILTIN_SCATTERALTDIV16SF,\n+  IX86_BUILTIN_SCATTERALTSIV8DI,\n+  IX86_BUILTIN_SCATTERALTDIV16SI,\n+  IX86_BUILTIN_SCATTERALTSIV4DF,\n+  IX86_BUILTIN_SCATTERALTDIV8SF,\n+  IX86_BUILTIN_SCATTERALTSIV4DI,\n+  IX86_BUILTIN_SCATTERALTDIV8SI,\n+  IX86_BUILTIN_SCATTERALTSIV2DF,\n+  IX86_BUILTIN_SCATTERALTDIV4SF,\n+  IX86_BUILTIN_SCATTERALTSIV2DI,\n+  IX86_BUILTIN_SCATTERALTDIV4SI,\n+  IX86_BUILTIN_SCATTERDIV16SF,\n+  IX86_BUILTIN_SCATTERDIV16SI,\n+  IX86_BUILTIN_SCATTERDIV8DF,\n+  IX86_BUILTIN_SCATTERDIV8DI,\n+  IX86_BUILTIN_SCATTERSIV16SF,\n+  IX86_BUILTIN_SCATTERSIV16SI,\n+  IX86_BUILTIN_SCATTERSIV8DF,\n+  IX86_BUILTIN_SCATTERSIV8DI,\n+  IX86_BUILTIN_GATHERPFQPD,\n+  IX86_BUILTIN_GATHERPFDPS,\n+  IX86_BUILTIN_GATHERPFDPD,\n+  IX86_BUILTIN_GATHERPFQPS,\n+  IX86_BUILTIN_SCATTERPFDPD,\n+  IX86_BUILTIN_SCATTERPFDPS,\n+  IX86_BUILTIN_SCATTERPFQPD,\n+  IX86_BUILTIN_SCATTERPFQPS,\n+  IX86_BUILTIN_CLWB,\n+  IX86_BUILTIN_CLFLUSHOPT,\n+  IX86_BUILTIN_INFQ,\n+  IX86_BUILTIN_HUGE_VALQ,\n+  IX86_BUILTIN_NANQ,\n+  IX86_BUILTIN_NANSQ,\n+  IX86_BUILTIN_XABORT,\n+  IX86_BUILTIN_ADDCARRYX32,\n+  IX86_BUILTIN_ADDCARRYX64,\n+  IX86_BUILTIN_SBB32,\n+  IX86_BUILTIN_SBB64,\n+  IX86_BUILTIN_RDRAND16_STEP,\n+  IX86_BUILTIN_RDRAND32_STEP,\n+  IX86_BUILTIN_RDRAND64_STEP,\n+  IX86_BUILTIN_RDSEED16_STEP,\n+  IX86_BUILTIN_RDSEED32_STEP,\n+  IX86_BUILTIN_RDSEED64_STEP,\n+  IX86_BUILTIN_MONITORX,\n+  IX86_BUILTIN_MWAITX,\n+  IX86_BUILTIN_CFSTRING,\n+  IX86_BUILTIN_CPU_INIT,\n+  IX86_BUILTIN_CPU_IS,\n+  IX86_BUILTIN_CPU_SUPPORTS,\n+  IX86_BUILTIN_READ_FLAGS,\n+  IX86_BUILTIN_WRITE_FLAGS,\n+\n+  /* All the remaining builtins are tracked in bdesc_* arrays in\n+     i386-builtin.def.  Don't add any IX86_BUILTIN_* enumerators after\n+     this point.  */\n+#define BDESC(mask, mask2, icode, name, code, comparison, flag)\t\\\n+  code,\n+#define BDESC_FIRST(kind, kindu, mask, mask2, icode, name, code, comparison, flag) \\\n+  code,\t\t\t\t\t\t\t\t\t\\\n+  IX86_BUILTIN__BDESC_##kindu##_FIRST = code,\n+#define BDESC_END(kind, next_kind)\n+\n+#include \"i386-builtin.def\"\n+\n+#undef BDESC\n+#undef BDESC_FIRST\n+#undef BDESC_END\n+\n+  IX86_BUILTIN_MAX,\n+\n+  IX86_BUILTIN__BDESC_MAX_FIRST = IX86_BUILTIN_MAX,\n+\n+  /* Now just the aliases for bdesc_* start/end.  */\n+#define BDESC(mask, mask2, icode, name, code, comparison, flag)\n+#define BDESC_FIRST(kind, kindu, mask, mask2, icode, name, code, comparison, flag)\n+#define BDESC_END(kind, next_kind) \\\n+  IX86_BUILTIN__BDESC_##kind##_LAST\t\t\t\t\t    \\\n+    = IX86_BUILTIN__BDESC_##next_kind##_FIRST - 1,\n+\n+#include \"i386-builtin.def\"\n+\n+#undef BDESC\n+#undef BDESC_FIRST\n+#undef BDESC_END\n+\n+  /* Just to make sure there is no comma after the last enumerator.  */\n+  IX86_BUILTIN__BDESC_MAX_LAST = IX86_BUILTIN__BDESC_MAX_FIRST\n+};\n+\n+/* Table of all of the builtin functions that are possible with different ISA's\n+   but are waiting to be built until a function is declared to use that\n+   ISA.  */\n+struct builtin_isa {\n+  HOST_WIDE_INT isa;\t\t/* isa_flags this builtin is defined for */\n+  HOST_WIDE_INT isa2;\t\t/* additional isa_flags this builtin is defined for */\n+  const char *name;\t\t/* function name */\n+  enum ix86_builtin_func_type tcode; /* type to use in the declaration */\n+  unsigned char const_p:1;\t/* true if the declaration is constant */\n+  unsigned char pure_p:1;\t/* true if the declaration has pure attribute */\n+  bool set_and_not_built_p;\n+};\n+\n+/* Bits for builtin_description.flag.  */\n+\n+/* Set when we don't support the comparison natively, and should\n+   swap_comparison in order to support it.  */\n+#define BUILTIN_DESC_SWAP_OPERANDS\t1\n+\n+struct builtin_description\n+{\n+  const HOST_WIDE_INT mask;\n+  const HOST_WIDE_INT mask2;\n+  const enum insn_code icode;\n+  const char *const name;\n+  const enum ix86_builtins code;\n+  const enum rtx_code comparison;\n+  const int flag;\n+};\n+\n+#define MULTI_ARG_4_DF2_DI_I\tV2DF_FTYPE_V2DF_V2DF_V2DI_INT\n+#define MULTI_ARG_4_DF2_DI_I1\tV4DF_FTYPE_V4DF_V4DF_V4DI_INT\n+#define MULTI_ARG_4_SF2_SI_I\tV4SF_FTYPE_V4SF_V4SF_V4SI_INT\n+#define MULTI_ARG_4_SF2_SI_I1\tV8SF_FTYPE_V8SF_V8SF_V8SI_INT\n+#define MULTI_ARG_3_SF\t\tV4SF_FTYPE_V4SF_V4SF_V4SF\n+#define MULTI_ARG_3_DF\t\tV2DF_FTYPE_V2DF_V2DF_V2DF\n+#define MULTI_ARG_3_SF2\t\tV8SF_FTYPE_V8SF_V8SF_V8SF\n+#define MULTI_ARG_3_DF2\t\tV4DF_FTYPE_V4DF_V4DF_V4DF\n+#define MULTI_ARG_3_DI\t\tV2DI_FTYPE_V2DI_V2DI_V2DI\n+#define MULTI_ARG_3_SI\t\tV4SI_FTYPE_V4SI_V4SI_V4SI\n+#define MULTI_ARG_3_SI_DI\tV4SI_FTYPE_V4SI_V4SI_V2DI\n+#define MULTI_ARG_3_HI\t\tV8HI_FTYPE_V8HI_V8HI_V8HI\n+#define MULTI_ARG_3_HI_SI\tV8HI_FTYPE_V8HI_V8HI_V4SI\n+#define MULTI_ARG_3_QI\t\tV16QI_FTYPE_V16QI_V16QI_V16QI\n+#define MULTI_ARG_3_DI2\t\tV4DI_FTYPE_V4DI_V4DI_V4DI\n+#define MULTI_ARG_3_SI2\t\tV8SI_FTYPE_V8SI_V8SI_V8SI\n+#define MULTI_ARG_3_HI2\t\tV16HI_FTYPE_V16HI_V16HI_V16HI\n+#define MULTI_ARG_3_QI2\t\tV32QI_FTYPE_V32QI_V32QI_V32QI\n+#define MULTI_ARG_2_SF\t\tV4SF_FTYPE_V4SF_V4SF\n+#define MULTI_ARG_2_DF\t\tV2DF_FTYPE_V2DF_V2DF\n+#define MULTI_ARG_2_DI\t\tV2DI_FTYPE_V2DI_V2DI\n+#define MULTI_ARG_2_SI\t\tV4SI_FTYPE_V4SI_V4SI\n+#define MULTI_ARG_2_HI\t\tV8HI_FTYPE_V8HI_V8HI\n+#define MULTI_ARG_2_QI\t\tV16QI_FTYPE_V16QI_V16QI\n+#define MULTI_ARG_2_DI_IMM\tV2DI_FTYPE_V2DI_SI\n+#define MULTI_ARG_2_SI_IMM\tV4SI_FTYPE_V4SI_SI\n+#define MULTI_ARG_2_HI_IMM\tV8HI_FTYPE_V8HI_SI\n+#define MULTI_ARG_2_QI_IMM\tV16QI_FTYPE_V16QI_SI\n+#define MULTI_ARG_2_DI_CMP\tV2DI_FTYPE_V2DI_V2DI_CMP\n+#define MULTI_ARG_2_SI_CMP\tV4SI_FTYPE_V4SI_V4SI_CMP\n+#define MULTI_ARG_2_HI_CMP\tV8HI_FTYPE_V8HI_V8HI_CMP\n+#define MULTI_ARG_2_QI_CMP\tV16QI_FTYPE_V16QI_V16QI_CMP\n+#define MULTI_ARG_2_SF_TF\tV4SF_FTYPE_V4SF_V4SF_TF\n+#define MULTI_ARG_2_DF_TF\tV2DF_FTYPE_V2DF_V2DF_TF\n+#define MULTI_ARG_2_DI_TF\tV2DI_FTYPE_V2DI_V2DI_TF\n+#define MULTI_ARG_2_SI_TF\tV4SI_FTYPE_V4SI_V4SI_TF\n+#define MULTI_ARG_2_HI_TF\tV8HI_FTYPE_V8HI_V8HI_TF\n+#define MULTI_ARG_2_QI_TF\tV16QI_FTYPE_V16QI_V16QI_TF\n+#define MULTI_ARG_1_SF\t\tV4SF_FTYPE_V4SF\n+#define MULTI_ARG_1_DF\t\tV2DF_FTYPE_V2DF\n+#define MULTI_ARG_1_SF2\t\tV8SF_FTYPE_V8SF\n+#define MULTI_ARG_1_DF2\t\tV4DF_FTYPE_V4DF\n+#define MULTI_ARG_1_DI\t\tV2DI_FTYPE_V2DI\n+#define MULTI_ARG_1_SI\t\tV4SI_FTYPE_V4SI\n+#define MULTI_ARG_1_HI\t\tV8HI_FTYPE_V8HI\n+#define MULTI_ARG_1_QI\t\tV16QI_FTYPE_V16QI\n+#define MULTI_ARG_1_SI_DI\tV2DI_FTYPE_V4SI\n+#define MULTI_ARG_1_HI_DI\tV2DI_FTYPE_V8HI\n+#define MULTI_ARG_1_HI_SI\tV4SI_FTYPE_V8HI\n+#define MULTI_ARG_1_QI_DI\tV2DI_FTYPE_V16QI\n+#define MULTI_ARG_1_QI_SI\tV4SI_FTYPE_V16QI\n+#define MULTI_ARG_1_QI_HI\tV8HI_FTYPE_V16QI\n+\n+#define BDESC(mask, mask2, icode, name, code, comparison, flag)\t\\\n+  { mask, mask2, icode, name, code, comparison, flag },\n+#define BDESC_FIRST(kind, kindu, mask, mask2, icode, name, code, comparison, flag) \\\n+static const struct builtin_description bdesc_##kind[] =\t\t    \\\n+{\t\t\t\t\t\t\t\t\t    \\\n+  BDESC (mask, mask2, icode, name, code, comparison, flag)\n+#define BDESC_END(kind, next_kind) \\\n+};\n+\n+#include \"i386-builtin.def\"\n+\n+extern builtin_isa ix86_builtins_isa[(int) IX86_BUILTIN_MAX];\n+\n+tree ix86_builtin_vectorized_function (unsigned int fn, tree type_out,\n+\t\t\t\t\t      tree type_in);\n+void ix86_init_builtins (void);\n+tree ix86_vectorize_builtin_gather (const_tree mem_vectype,\n+\t\t\t\t\t   const_tree index_type, int scale);\n+tree ix86_builtin_decl (unsigned code, bool);\n+tree ix86_builtin_reciprocal (tree fndecl);\n+unsigned int get_builtin_code_for_version (tree decl, tree *predicate_list);\n+tree fold_builtin_cpu (tree fndecl, tree *args);\n+tree get_ix86_builtin (enum ix86_builtins c);\n+\n+#endif  /* GCC_I386_BUILTINS_H */"}, {"sha": "0835ebf74b72cde8c1f298bdf0f8693b3e7154b7", "filename": "gcc/config/i386/i386-expand.c", "status": "added", "additions": 19840, "deletions": 0, "changes": 19840, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-expand.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-expand.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-expand.c?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4"}, {"sha": "9271bb85ac5d4c02539523afc41e0dffc5cf13e4", "filename": "gcc/config/i386/i386-expand.h", "status": "added", "additions": 58, "deletions": 0, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-expand.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-expand.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-expand.h?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "patch": "@@ -0,0 +1,58 @@\n+/* Copyright (C) 1988-2019 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_I386_EXPAND_H\n+#define GCC_I386_EXPAND_H\n+\n+/* AVX512F does support 64-byte integer vector operations,\n+   thus the longest vector we are faced with is V64QImode.  */\n+#define MAX_VECT_LEN\t64\n+\n+struct expand_vec_perm_d\n+{\n+  rtx target, op0, op1;\n+  unsigned char perm[MAX_VECT_LEN];\n+  machine_mode vmode;\n+  unsigned char nelt;\n+  bool one_operand_p;\n+  bool testing_p;\n+};\n+\n+rtx legitimize_tls_address (rtx x, enum tls_model model, bool for_mov);\n+alias_set_type ix86_GOT_alias_set (void);\n+rtx legitimize_pic_address (rtx orig, rtx reg);\n+rtx legitimize_pe_coff_symbol (rtx addr, bool inreg);\n+\n+bool insn_defines_reg (unsigned int regno1, unsigned int regno2,\n+\t\t       rtx_insn *insn);\n+void ix86_emit_binop (enum rtx_code code, machine_mode mode, rtx dst, rtx src);\n+enum calling_abi ix86_function_abi (const_tree fndecl);\n+bool ix86_function_ms_hook_prologue (const_tree fn);\n+void warn_once_call_ms2sysv_xlogues (const char *feature);\n+rtx gen_push (rtx arg);\n+rtx gen_pop (rtx arg);\n+rtx ix86_expand_builtin (tree exp, rtx target, rtx subtarget,\n+\t\t\t machine_mode mode, int ignore);\n+bool ix86_vectorize_vec_perm_const (machine_mode vmode, rtx target, rtx op0,\n+\t\t\t\t    rtx op1, const vec_perm_indices &sel);\n+bool ix86_notrack_prefixed_insn_p (rtx insn);\n+machine_mode ix86_split_reduction (machine_mode mode);\n+void ix86_expand_divmod_libfunc (rtx libfunc, machine_mode mode, rtx op0,\n+\t\t\t\t rtx op1, rtx *quot_p, rtx *rem_p);\n+\n+#endif  /* GCC_I386_EXPAND_H */"}, {"sha": "67f45d66c48a033620d2b76fd39df6ebf2c31dbd", "filename": "gcc/config/i386/i386-features.c", "status": "added", "additions": 2742, "deletions": 0, "changes": 2742, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-features.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-features.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-features.c?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "patch": "@@ -0,0 +1,2742 @@\n+/* Copyright (C) 1988-2019 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#define IN_TARGET_CODE 1\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"backend.h\"\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"memmodel.h\"\n+#include \"gimple.h\"\n+#include \"cfghooks.h\"\n+#include \"cfgloop.h\"\n+#include \"df.h\"\n+#include \"tm_p.h\"\n+#include \"stringpool.h\"\n+#include \"expmed.h\"\n+#include \"optabs.h\"\n+#include \"regs.h\"\n+#include \"emit-rtl.h\"\n+#include \"recog.h\"\n+#include \"cgraph.h\"\n+#include \"diagnostic.h\"\n+#include \"cfgbuild.h\"\n+#include \"alias.h\"\n+#include \"fold-const.h\"\n+#include \"attribs.h\"\n+#include \"calls.h\"\n+#include \"stor-layout.h\"\n+#include \"varasm.h\"\n+#include \"output.h\"\n+#include \"insn-attr.h\"\n+#include \"flags.h\"\n+#include \"except.h\"\n+#include \"explow.h\"\n+#include \"expr.h\"\n+#include \"cfgrtl.h\"\n+#include \"common/common-target.h\"\n+#include \"langhooks.h\"\n+#include \"reload.h\"\n+#include \"gimplify.h\"\n+#include \"dwarf2.h\"\n+#include \"tm-constrs.h\"\n+#include \"params.h\"\n+#include \"cselib.h\"\n+#include \"sched-int.h\"\n+#include \"opts.h\"\n+#include \"tree-pass.h\"\n+#include \"context.h\"\n+#include \"pass_manager.h\"\n+#include \"target-globals.h\"\n+#include \"gimple-iterator.h\"\n+#include \"tree-vectorizer.h\"\n+#include \"shrink-wrap.h\"\n+#include \"builtins.h\"\n+#include \"rtl-iter.h\"\n+#include \"tree-iterator.h\"\n+#include \"dbgcnt.h\"\n+#include \"case-cfn-macros.h\"\n+#include \"dojump.h\"\n+#include \"fold-const-call.h\"\n+#include \"tree-vrp.h\"\n+#include \"tree-ssanames.h\"\n+#include \"selftest.h\"\n+#include \"selftest-rtl.h\"\n+#include \"print-rtl.h\"\n+#include \"intl.h\"\n+#include \"ifcvt.h\"\n+#include \"symbol-summary.h\"\n+#include \"ipa-prop.h\"\n+#include \"ipa-fnsummary.h\"\n+#include \"wide-int-bitmask.h\"\n+#include \"tree-vector-builder.h\"\n+#include \"debug.h\"\n+#include \"dwarf2out.h\"\n+#include \"i386-builtins.h\"\n+#include \"i386-features.h\"\n+\n+const char * const xlogue_layout::STUB_BASE_NAMES[XLOGUE_STUB_COUNT] = {\n+  \"savms64\",\n+  \"resms64\",\n+  \"resms64x\",\n+  \"savms64f\",\n+  \"resms64f\",\n+  \"resms64fx\"\n+};\n+\n+const unsigned xlogue_layout::REG_ORDER[xlogue_layout::MAX_REGS] = {\n+/* The below offset values are where each register is stored for the layout\n+   relative to incoming stack pointer.  The value of each m_regs[].offset will\n+   be relative to the incoming base pointer (rax or rsi) used by the stub.\n+\n+    s_instances:   0\t\t1\t\t2\t\t3\n+    Offset:\t\t\t\t\trealigned or\taligned + 8\n+    Register\t   aligned\taligned + 8\taligned w/HFP\tw/HFP\t*/\n+    XMM15_REG,\t/* 0x10\t\t0x18\t\t0x10\t\t0x18\t*/\n+    XMM14_REG,\t/* 0x20\t\t0x28\t\t0x20\t\t0x28\t*/\n+    XMM13_REG,\t/* 0x30\t\t0x38\t\t0x30\t\t0x38\t*/\n+    XMM12_REG,\t/* 0x40\t\t0x48\t\t0x40\t\t0x48\t*/\n+    XMM11_REG,\t/* 0x50\t\t0x58\t\t0x50\t\t0x58\t*/\n+    XMM10_REG,\t/* 0x60\t\t0x68\t\t0x60\t\t0x68\t*/\n+    XMM9_REG,\t/* 0x70\t\t0x78\t\t0x70\t\t0x78\t*/\n+    XMM8_REG,\t/* 0x80\t\t0x88\t\t0x80\t\t0x88\t*/\n+    XMM7_REG,\t/* 0x90\t\t0x98\t\t0x90\t\t0x98\t*/\n+    XMM6_REG,\t/* 0xa0\t\t0xa8\t\t0xa0\t\t0xa8\t*/\n+    SI_REG,\t/* 0xa8\t\t0xb0\t\t0xa8\t\t0xb0\t*/\n+    DI_REG,\t/* 0xb0\t\t0xb8\t\t0xb0\t\t0xb8\t*/\n+    BX_REG,\t/* 0xb8\t\t0xc0\t\t0xb8\t\t0xc0\t*/\n+    BP_REG,\t/* 0xc0\t\t0xc8\t\tN/A\t\tN/A\t*/\n+    R12_REG,\t/* 0xc8\t\t0xd0\t\t0xc0\t\t0xc8\t*/\n+    R13_REG,\t/* 0xd0\t\t0xd8\t\t0xc8\t\t0xd0\t*/\n+    R14_REG,\t/* 0xd8\t\t0xe0\t\t0xd0\t\t0xd8\t*/\n+    R15_REG,\t/* 0xe0\t\t0xe8\t\t0xd8\t\t0xe0\t*/\n+};\n+\n+/* Instantiate static const values.  */\n+const HOST_WIDE_INT xlogue_layout::STUB_INDEX_OFFSET;\n+const unsigned xlogue_layout::MIN_REGS;\n+const unsigned xlogue_layout::MAX_REGS;\n+const unsigned xlogue_layout::MAX_EXTRA_REGS;\n+const unsigned xlogue_layout::VARIANT_COUNT;\n+const unsigned xlogue_layout::STUB_NAME_MAX_LEN;\n+\n+/* Initialize xlogue_layout::s_stub_names to zero.  */\n+char xlogue_layout::s_stub_names[2][XLOGUE_STUB_COUNT][VARIANT_COUNT]\n+\t\t\t\t[STUB_NAME_MAX_LEN];\n+\n+/* Instantiates all xlogue_layout instances.  */\n+const xlogue_layout xlogue_layout::s_instances[XLOGUE_SET_COUNT] = {\n+  xlogue_layout (0, false),\n+  xlogue_layout (8, false),\n+  xlogue_layout (0, true),\n+  xlogue_layout (8, true)\n+};\n+\n+/* Return an appropriate const instance of xlogue_layout based upon values\n+   in cfun->machine and crtl.  */\n+const struct xlogue_layout &\n+xlogue_layout::get_instance ()\n+{\n+  enum xlogue_stub_sets stub_set;\n+  bool aligned_plus_8 = cfun->machine->call_ms2sysv_pad_in;\n+\n+  if (stack_realign_fp)\n+    stub_set = XLOGUE_SET_HFP_ALIGNED_OR_REALIGN;\n+  else if (frame_pointer_needed)\n+    stub_set = aligned_plus_8\n+\t      ? XLOGUE_SET_HFP_ALIGNED_PLUS_8\n+\t      : XLOGUE_SET_HFP_ALIGNED_OR_REALIGN;\n+  else\n+    stub_set = aligned_plus_8 ? XLOGUE_SET_ALIGNED_PLUS_8 : XLOGUE_SET_ALIGNED;\n+\n+  return s_instances[stub_set];\n+}\n+\n+/* Determine how many clobbered registers can be saved by the stub.\n+   Returns the count of registers the stub will save and restore.  */\n+unsigned\n+xlogue_layout::count_stub_managed_regs ()\n+{\n+  bool hfp = frame_pointer_needed || stack_realign_fp;\n+  unsigned i, count;\n+  unsigned regno;\n+\n+  for (count = i = MIN_REGS; i < MAX_REGS; ++i)\n+    {\n+      regno = REG_ORDER[i];\n+      if (regno == BP_REG && hfp)\n+\tcontinue;\n+      if (!ix86_save_reg (regno, false, false))\n+\tbreak;\n+      ++count;\n+    }\n+  return count;\n+}\n+\n+/* Determine if register REGNO is a stub managed register given the\n+   total COUNT of stub managed registers.  */\n+bool\n+xlogue_layout::is_stub_managed_reg (unsigned regno, unsigned count)\n+{\n+  bool hfp = frame_pointer_needed || stack_realign_fp;\n+  unsigned i;\n+\n+  for (i = 0; i < count; ++i)\n+    {\n+      gcc_assert (i < MAX_REGS);\n+      if (REG_ORDER[i] == BP_REG && hfp)\n+\t++count;\n+      else if (REG_ORDER[i] == regno)\n+\treturn true;\n+    }\n+  return false;\n+}\n+\n+/* Constructor for xlogue_layout.  */\n+xlogue_layout::xlogue_layout (HOST_WIDE_INT stack_align_off_in, bool hfp)\n+  : m_hfp (hfp) , m_nregs (hfp ? 17 : 18),\n+    m_stack_align_off_in (stack_align_off_in)\n+{\n+  HOST_WIDE_INT offset = stack_align_off_in;\n+  unsigned i, j;\n+\n+  for (i = j = 0; i < MAX_REGS; ++i)\n+    {\n+      unsigned regno = REG_ORDER[i];\n+\n+      if (regno == BP_REG && hfp)\n+\tcontinue;\n+      if (SSE_REGNO_P (regno))\n+\t{\n+\t  offset += 16;\n+\t  /* Verify that SSE regs are always aligned.  */\n+\t  gcc_assert (!((stack_align_off_in + offset) & 15));\n+\t}\n+      else\n+\toffset += 8;\n+\n+      m_regs[j].regno    = regno;\n+      m_regs[j++].offset = offset - STUB_INDEX_OFFSET;\n+    }\n+  gcc_assert (j == m_nregs);\n+}\n+\n+const char *\n+xlogue_layout::get_stub_name (enum xlogue_stub stub,\n+\t\t\t      unsigned n_extra_regs)\n+{\n+  const int have_avx = TARGET_AVX;\n+  char *name = s_stub_names[!!have_avx][stub][n_extra_regs];\n+\n+  /* Lazy init */\n+  if (!*name)\n+    {\n+      int res = snprintf (name, STUB_NAME_MAX_LEN, \"__%s_%s_%u\",\n+\t\t\t  (have_avx ? \"avx\" : \"sse\"),\n+\t\t\t  STUB_BASE_NAMES[stub],\n+\t\t\t  MIN_REGS + n_extra_regs);\n+      gcc_checking_assert (res < (int)STUB_NAME_MAX_LEN);\n+    }\n+\n+  return name;\n+}\n+\n+/* Return rtx of a symbol ref for the entry point (based upon\n+   cfun->machine->call_ms2sysv_extra_regs) of the specified stub.  */\n+rtx\n+xlogue_layout::get_stub_rtx (enum xlogue_stub stub)\n+{\n+  const unsigned n_extra_regs = cfun->machine->call_ms2sysv_extra_regs;\n+  gcc_checking_assert (n_extra_regs <= MAX_EXTRA_REGS);\n+  gcc_assert (stub < XLOGUE_STUB_COUNT);\n+  gcc_assert (crtl->stack_realign_finalized);\n+\n+  return gen_rtx_SYMBOL_REF (Pmode, get_stub_name (stub, n_extra_regs));\n+}\n+\n+unsigned scalar_chain::max_id = 0;\n+\n+/* Initialize new chain.  */\n+\n+scalar_chain::scalar_chain ()\n+{\n+  chain_id = ++max_id;\n+\n+   if (dump_file)\n+    fprintf (dump_file, \"Created a new instruction chain #%d\\n\", chain_id);\n+\n+  bitmap_obstack_initialize (NULL);\n+  insns = BITMAP_ALLOC (NULL);\n+  defs = BITMAP_ALLOC (NULL);\n+  defs_conv = BITMAP_ALLOC (NULL);\n+  queue = NULL;\n+}\n+\n+/* Free chain's data.  */\n+\n+scalar_chain::~scalar_chain ()\n+{\n+  BITMAP_FREE (insns);\n+  BITMAP_FREE (defs);\n+  BITMAP_FREE (defs_conv);\n+  bitmap_obstack_release (NULL);\n+}\n+\n+/* Add instruction into chains' queue.  */\n+\n+void\n+scalar_chain::add_to_queue (unsigned insn_uid)\n+{\n+  if (bitmap_bit_p (insns, insn_uid)\n+      || bitmap_bit_p (queue, insn_uid))\n+    return;\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"  Adding insn %d into chain's #%d queue\\n\",\n+\t     insn_uid, chain_id);\n+  bitmap_set_bit (queue, insn_uid);\n+}\n+\n+/* For DImode conversion, mark register defined by DEF as requiring\n+   conversion.  */\n+\n+void\n+dimode_scalar_chain::mark_dual_mode_def (df_ref def)\n+{\n+  gcc_assert (DF_REF_REG_DEF_P (def));\n+\n+  if (bitmap_bit_p (defs_conv, DF_REF_REGNO (def)))\n+    return;\n+\n+  if (dump_file)\n+    fprintf (dump_file,\n+\t     \"  Mark r%d def in insn %d as requiring both modes in chain #%d\\n\",\n+\t     DF_REF_REGNO (def), DF_REF_INSN_UID (def), chain_id);\n+\n+  bitmap_set_bit (defs_conv, DF_REF_REGNO (def));\n+}\n+\n+/* For TImode conversion, it is unused.  */\n+\n+void\n+timode_scalar_chain::mark_dual_mode_def (df_ref)\n+{\n+  gcc_unreachable ();\n+}\n+\n+/* Check REF's chain to add new insns into a queue\n+   and find registers requiring conversion.  */\n+\n+void\n+scalar_chain::analyze_register_chain (bitmap candidates, df_ref ref)\n+{\n+  df_link *chain;\n+\n+  gcc_assert (bitmap_bit_p (insns, DF_REF_INSN_UID (ref))\n+\t      || bitmap_bit_p (candidates, DF_REF_INSN_UID (ref)));\n+  add_to_queue (DF_REF_INSN_UID (ref));\n+\n+  for (chain = DF_REF_CHAIN (ref); chain; chain = chain->next)\n+    {\n+      unsigned uid = DF_REF_INSN_UID (chain->ref);\n+\n+      if (!NONDEBUG_INSN_P (DF_REF_INSN (chain->ref)))\n+\tcontinue;\n+\n+      if (!DF_REF_REG_MEM_P (chain->ref))\n+\t{\n+\t  if (bitmap_bit_p (insns, uid))\n+\t    continue;\n+\n+\t  if (bitmap_bit_p (candidates, uid))\n+\t    {\n+\t      add_to_queue (uid);\n+\t      continue;\n+\t    }\n+\t}\n+\n+      if (DF_REF_REG_DEF_P (chain->ref))\n+\t{\n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"  r%d def in insn %d isn't convertible\\n\",\n+\t\t     DF_REF_REGNO (chain->ref), uid);\n+\t  mark_dual_mode_def (chain->ref);\n+\t}\n+      else\n+\t{\n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"  r%d use in insn %d isn't convertible\\n\",\n+\t\t     DF_REF_REGNO (chain->ref), uid);\n+\t  mark_dual_mode_def (ref);\n+\t}\n+    }\n+}\n+\n+/* Add instruction into a chain.  */\n+\n+void\n+scalar_chain::add_insn (bitmap candidates, unsigned int insn_uid)\n+{\n+  if (bitmap_bit_p (insns, insn_uid))\n+    return;\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"  Adding insn %d to chain #%d\\n\", insn_uid, chain_id);\n+\n+  bitmap_set_bit (insns, insn_uid);\n+\n+  rtx_insn *insn = DF_INSN_UID_GET (insn_uid)->insn;\n+  rtx def_set = single_set (insn);\n+  if (def_set && REG_P (SET_DEST (def_set))\n+      && !HARD_REGISTER_P (SET_DEST (def_set)))\n+    bitmap_set_bit (defs, REGNO (SET_DEST (def_set)));\n+\n+  df_ref ref;\n+  df_ref def;\n+  for (ref = DF_INSN_UID_DEFS (insn_uid); ref; ref = DF_REF_NEXT_LOC (ref))\n+    if (!HARD_REGISTER_P (DF_REF_REG (ref)))\n+      for (def = DF_REG_DEF_CHAIN (DF_REF_REGNO (ref));\n+\t   def;\n+\t   def = DF_REF_NEXT_REG (def))\n+\tanalyze_register_chain (candidates, def);\n+  for (ref = DF_INSN_UID_USES (insn_uid); ref; ref = DF_REF_NEXT_LOC (ref))\n+    if (!DF_REF_REG_MEM_P (ref))\n+      analyze_register_chain (candidates, ref);\n+}\n+\n+/* Build new chain starting from insn INSN_UID recursively\n+   adding all dependent uses and definitions.  */\n+\n+void\n+scalar_chain::build (bitmap candidates, unsigned insn_uid)\n+{\n+  queue = BITMAP_ALLOC (NULL);\n+  bitmap_set_bit (queue, insn_uid);\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Building chain #%d...\\n\", chain_id);\n+\n+  while (!bitmap_empty_p (queue))\n+    {\n+      insn_uid = bitmap_first_set_bit (queue);\n+      bitmap_clear_bit (queue, insn_uid);\n+      bitmap_clear_bit (candidates, insn_uid);\n+      add_insn (candidates, insn_uid);\n+    }\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"Collected chain #%d...\\n\", chain_id);\n+      fprintf (dump_file, \"  insns: \");\n+      dump_bitmap (dump_file, insns);\n+      if (!bitmap_empty_p (defs_conv))\n+\t{\n+\t  bitmap_iterator bi;\n+\t  unsigned id;\n+\t  const char *comma = \"\";\n+\t  fprintf (dump_file, \"  defs to convert: \");\n+\t  EXECUTE_IF_SET_IN_BITMAP (defs_conv, 0, id, bi)\n+\t    {\n+\t      fprintf (dump_file, \"%sr%d\", comma, id);\n+\t      comma = \", \";\n+\t    }\n+\t  fprintf (dump_file, \"\\n\");\n+\t}\n+    }\n+\n+  BITMAP_FREE (queue);\n+}\n+\n+/* Return a cost of building a vector costant\n+   instead of using a scalar one.  */\n+\n+int\n+dimode_scalar_chain::vector_const_cost (rtx exp)\n+{\n+  gcc_assert (CONST_INT_P (exp));\n+\n+  if (standard_sse_constant_p (exp, V2DImode))\n+    return COSTS_N_INSNS (1);\n+  return ix86_cost->sse_load[1];\n+}\n+\n+/* Compute a gain for chain conversion.  */\n+\n+int\n+dimode_scalar_chain::compute_convert_gain ()\n+{\n+  bitmap_iterator bi;\n+  unsigned insn_uid;\n+  int gain = 0;\n+  int cost = 0;\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Computing gain for chain #%d...\\n\", chain_id);\n+\n+  EXECUTE_IF_SET_IN_BITMAP (insns, 0, insn_uid, bi)\n+    {\n+      rtx_insn *insn = DF_INSN_UID_GET (insn_uid)->insn;\n+      rtx def_set = single_set (insn);\n+      rtx src = SET_SRC (def_set);\n+      rtx dst = SET_DEST (def_set);\n+\n+      if (REG_P (src) && REG_P (dst))\n+\tgain += COSTS_N_INSNS (2) - ix86_cost->xmm_move;\n+      else if (REG_P (src) && MEM_P (dst))\n+\tgain += 2 * ix86_cost->int_store[2] - ix86_cost->sse_store[1];\n+      else if (MEM_P (src) && REG_P (dst))\n+\tgain += 2 * ix86_cost->int_load[2] - ix86_cost->sse_load[1];\n+      else if (GET_CODE (src) == ASHIFT\n+\t       || GET_CODE (src) == ASHIFTRT\n+\t       || GET_CODE (src) == LSHIFTRT)\n+\t{\n+    \t  if (CONST_INT_P (XEXP (src, 0)))\n+\t    gain -= vector_const_cost (XEXP (src, 0));\n+\t  gain += ix86_cost->shift_const;\n+\t  if (INTVAL (XEXP (src, 1)) >= 32)\n+\t    gain -= COSTS_N_INSNS (1);\n+\t}\n+      else if (GET_CODE (src) == PLUS\n+\t       || GET_CODE (src) == MINUS\n+\t       || GET_CODE (src) == IOR\n+\t       || GET_CODE (src) == XOR\n+\t       || GET_CODE (src) == AND)\n+\t{\n+\t  gain += ix86_cost->add;\n+\t  /* Additional gain for andnot for targets without BMI.  */\n+\t  if (GET_CODE (XEXP (src, 0)) == NOT\n+\t      && !TARGET_BMI)\n+\t    gain += 2 * ix86_cost->add;\n+\n+\t  if (CONST_INT_P (XEXP (src, 0)))\n+\t    gain -= vector_const_cost (XEXP (src, 0));\n+\t  if (CONST_INT_P (XEXP (src, 1)))\n+\t    gain -= vector_const_cost (XEXP (src, 1));\n+\t}\n+      else if (GET_CODE (src) == NEG\n+\t       || GET_CODE (src) == NOT)\n+\tgain += ix86_cost->add - COSTS_N_INSNS (1);\n+      else if (GET_CODE (src) == COMPARE)\n+\t{\n+\t  /* Assume comparison cost is the same.  */\n+\t}\n+      else if (CONST_INT_P (src))\n+\t{\n+\t  if (REG_P (dst))\n+\t    gain += COSTS_N_INSNS (2);\n+\t  else if (MEM_P (dst))\n+\t    gain += 2 * ix86_cost->int_store[2] - ix86_cost->sse_store[1];\n+\t  gain -= vector_const_cost (src);\n+\t}\n+      else\n+\tgcc_unreachable ();\n+    }\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"  Instruction conversion gain: %d\\n\", gain);\n+\n+  EXECUTE_IF_SET_IN_BITMAP (defs_conv, 0, insn_uid, bi)\n+    cost += DF_REG_DEF_COUNT (insn_uid) * ix86_cost->mmxsse_to_integer;\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"  Registers conversion cost: %d\\n\", cost);\n+\n+  gain -= cost;\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"  Total gain: %d\\n\", gain);\n+\n+  return gain;\n+}\n+\n+/* Replace REG in X with a V2DI subreg of NEW_REG.  */\n+\n+rtx\n+dimode_scalar_chain::replace_with_subreg (rtx x, rtx reg, rtx new_reg)\n+{\n+  if (x == reg)\n+    return gen_rtx_SUBREG (V2DImode, new_reg, 0);\n+\n+  const char *fmt = GET_RTX_FORMAT (GET_CODE (x));\n+  int i, j;\n+  for (i = GET_RTX_LENGTH (GET_CODE (x)) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\tXEXP (x, i) = replace_with_subreg (XEXP (x, i), reg, new_reg);\n+      else if (fmt[i] == 'E')\n+\tfor (j = XVECLEN (x, i) - 1; j >= 0; j--)\n+\t  XVECEXP (x, i, j) = replace_with_subreg (XVECEXP (x, i, j),\n+\t\t\t\t\t\t   reg, new_reg);\n+    }\n+\n+  return x;\n+}\n+\n+/* Replace REG in INSN with a V2DI subreg of NEW_REG.  */\n+\n+void\n+dimode_scalar_chain::replace_with_subreg_in_insn (rtx_insn *insn,\n+\t\t\t\t\t\t  rtx reg, rtx new_reg)\n+{\n+  replace_with_subreg (single_set (insn), reg, new_reg);\n+}\n+\n+/* Insert generated conversion instruction sequence INSNS\n+   after instruction AFTER.  New BB may be required in case\n+   instruction has EH region attached.  */\n+\n+void\n+scalar_chain::emit_conversion_insns (rtx insns, rtx_insn *after)\n+{\n+  if (!control_flow_insn_p (after))\n+    {\n+      emit_insn_after (insns, after);\n+      return;\n+    }\n+\n+  basic_block bb = BLOCK_FOR_INSN (after);\n+  edge e = find_fallthru_edge (bb->succs);\n+  gcc_assert (e);\n+\n+  basic_block new_bb = split_edge (e);\n+  emit_insn_after (insns, BB_HEAD (new_bb));\n+}\n+\n+/* Make vector copies for all register REGNO definitions\n+   and replace its uses in a chain.  */\n+\n+void\n+dimode_scalar_chain::make_vector_copies (unsigned regno)\n+{\n+  rtx reg = regno_reg_rtx[regno];\n+  rtx vreg = gen_reg_rtx (DImode);\n+  df_ref ref;\n+\n+  for (ref = DF_REG_DEF_CHAIN (regno); ref; ref = DF_REF_NEXT_REG (ref))\n+    if (!bitmap_bit_p (insns, DF_REF_INSN_UID (ref)))\n+      {\n+\tstart_sequence ();\n+\tif (!TARGET_INTER_UNIT_MOVES_TO_VEC)\n+\t  {\n+\t    rtx tmp = assign_386_stack_local (DImode, SLOT_STV_TEMP);\n+\t    emit_move_insn (adjust_address (tmp, SImode, 0),\n+\t\t\t    gen_rtx_SUBREG (SImode, reg, 0));\n+\t    emit_move_insn (adjust_address (tmp, SImode, 4),\n+\t\t\t    gen_rtx_SUBREG (SImode, reg, 4));\n+\t    emit_move_insn (vreg, tmp);\n+\t  }\n+\telse if (TARGET_SSE4_1)\n+\t  {\n+\t    emit_insn (gen_sse2_loadld (gen_rtx_SUBREG (V4SImode, vreg, 0),\n+\t\t\t\t\tCONST0_RTX (V4SImode),\n+\t\t\t\t\tgen_rtx_SUBREG (SImode, reg, 0)));\n+\t    emit_insn (gen_sse4_1_pinsrd (gen_rtx_SUBREG (V4SImode, vreg, 0),\n+\t\t\t\t\t  gen_rtx_SUBREG (V4SImode, vreg, 0),\n+\t\t\t\t\t  gen_rtx_SUBREG (SImode, reg, 4),\n+\t\t\t\t\t  GEN_INT (2)));\n+\t  }\n+\telse\n+\t  {\n+\t    rtx tmp = gen_reg_rtx (DImode);\n+\t    emit_insn (gen_sse2_loadld (gen_rtx_SUBREG (V4SImode, vreg, 0),\n+\t\t\t\t\tCONST0_RTX (V4SImode),\n+\t\t\t\t\tgen_rtx_SUBREG (SImode, reg, 0)));\n+\t    emit_insn (gen_sse2_loadld (gen_rtx_SUBREG (V4SImode, tmp, 0),\n+\t\t\t\t\tCONST0_RTX (V4SImode),\n+\t\t\t\t\tgen_rtx_SUBREG (SImode, reg, 4)));\n+\t    emit_insn (gen_vec_interleave_lowv4si\n+\t\t       (gen_rtx_SUBREG (V4SImode, vreg, 0),\n+\t\t\tgen_rtx_SUBREG (V4SImode, vreg, 0),\n+\t\t\tgen_rtx_SUBREG (V4SImode, tmp, 0)));\n+\t  }\n+\trtx_insn *seq = get_insns ();\n+\tend_sequence ();\n+\trtx_insn *insn = DF_REF_INSN (ref);\n+\temit_conversion_insns (seq, insn);\n+\n+\tif (dump_file)\n+\t  fprintf (dump_file,\n+\t\t   \"  Copied r%d to a vector register r%d for insn %d\\n\",\n+\t\t   regno, REGNO (vreg), INSN_UID (insn));\n+      }\n+\n+  for (ref = DF_REG_USE_CHAIN (regno); ref; ref = DF_REF_NEXT_REG (ref))\n+    if (bitmap_bit_p (insns, DF_REF_INSN_UID (ref)))\n+      {\n+\trtx_insn *insn = DF_REF_INSN (ref);\n+\treplace_with_subreg_in_insn (insn, reg, vreg);\n+\n+\tif (dump_file)\n+\t  fprintf (dump_file, \"  Replaced r%d with r%d in insn %d\\n\",\n+\t\t   regno, REGNO (vreg), INSN_UID (insn));\n+      }\n+}\n+\n+/* Convert all definitions of register REGNO\n+   and fix its uses.  Scalar copies may be created\n+   in case register is used in not convertible insn.  */\n+\n+void\n+dimode_scalar_chain::convert_reg (unsigned regno)\n+{\n+  bool scalar_copy = bitmap_bit_p (defs_conv, regno);\n+  rtx reg = regno_reg_rtx[regno];\n+  rtx scopy = NULL_RTX;\n+  df_ref ref;\n+  bitmap conv;\n+\n+  conv = BITMAP_ALLOC (NULL);\n+  bitmap_copy (conv, insns);\n+\n+  if (scalar_copy)\n+    scopy = gen_reg_rtx (DImode);\n+\n+  for (ref = DF_REG_DEF_CHAIN (regno); ref; ref = DF_REF_NEXT_REG (ref))\n+    {\n+      rtx_insn *insn = DF_REF_INSN (ref);\n+      rtx def_set = single_set (insn);\n+      rtx src = SET_SRC (def_set);\n+      rtx reg = DF_REF_REG (ref);\n+\n+      if (!MEM_P (src))\n+\t{\n+\t  replace_with_subreg_in_insn (insn, reg, reg);\n+\t  bitmap_clear_bit (conv, INSN_UID (insn));\n+\t}\n+\n+      if (scalar_copy)\n+\t{\n+\t  start_sequence ();\n+\t  if (!TARGET_INTER_UNIT_MOVES_FROM_VEC)\n+\t    {\n+\t      rtx tmp = assign_386_stack_local (DImode, SLOT_STV_TEMP);\n+\t      emit_move_insn (tmp, reg);\n+\t      emit_move_insn (gen_rtx_SUBREG (SImode, scopy, 0),\n+\t\t\t      adjust_address (tmp, SImode, 0));\n+\t      emit_move_insn (gen_rtx_SUBREG (SImode, scopy, 4),\n+\t\t\t      adjust_address (tmp, SImode, 4));\n+\t    }\n+\t  else if (TARGET_SSE4_1)\n+\t    {\n+\t      rtx tmp = gen_rtx_PARALLEL (VOIDmode, gen_rtvec (1, const0_rtx));\n+\t      emit_insn\n+\t\t(gen_rtx_SET\n+\t\t (gen_rtx_SUBREG (SImode, scopy, 0),\n+\t\t  gen_rtx_VEC_SELECT (SImode,\n+\t\t\t\t      gen_rtx_SUBREG (V4SImode, reg, 0), tmp)));\n+\n+\t      tmp = gen_rtx_PARALLEL (VOIDmode, gen_rtvec (1, const1_rtx));\n+\t      emit_insn\n+\t\t(gen_rtx_SET\n+\t\t (gen_rtx_SUBREG (SImode, scopy, 4),\n+\t\t  gen_rtx_VEC_SELECT (SImode,\n+\t\t\t\t      gen_rtx_SUBREG (V4SImode, reg, 0), tmp)));\n+\t    }\n+\t  else\n+\t    {\n+\t      rtx vcopy = gen_reg_rtx (V2DImode);\n+\t      emit_move_insn (vcopy, gen_rtx_SUBREG (V2DImode, reg, 0));\n+\t      emit_move_insn (gen_rtx_SUBREG (SImode, scopy, 0),\n+\t\t\t      gen_rtx_SUBREG (SImode, vcopy, 0));\n+\t      emit_move_insn (vcopy,\n+\t\t\t      gen_rtx_LSHIFTRT (V2DImode, vcopy, GEN_INT (32)));\n+\t      emit_move_insn (gen_rtx_SUBREG (SImode, scopy, 4),\n+\t\t\t      gen_rtx_SUBREG (SImode, vcopy, 0));\n+\t    }\n+\t  rtx_insn *seq = get_insns ();\n+\t  end_sequence ();\n+\t  emit_conversion_insns (seq, insn);\n+\n+\t  if (dump_file)\n+\t    fprintf (dump_file,\n+\t\t     \"  Copied r%d to a scalar register r%d for insn %d\\n\",\n+\t\t     regno, REGNO (scopy), INSN_UID (insn));\n+\t}\n+    }\n+\n+  for (ref = DF_REG_USE_CHAIN (regno); ref; ref = DF_REF_NEXT_REG (ref))\n+    if (bitmap_bit_p (insns, DF_REF_INSN_UID (ref)))\n+      {\n+\tif (bitmap_bit_p (conv, DF_REF_INSN_UID (ref)))\n+\t  {\n+\t    rtx_insn *insn = DF_REF_INSN (ref);\n+\n+\t    rtx def_set = single_set (insn);\n+\t    gcc_assert (def_set);\n+\n+\t    rtx src = SET_SRC (def_set);\n+\t    rtx dst = SET_DEST (def_set);\n+\n+\t    if (!MEM_P (dst) || !REG_P (src))\n+\t      replace_with_subreg_in_insn (insn, reg, reg);\n+\n+\t    bitmap_clear_bit (conv, INSN_UID (insn));\n+\t  }\n+      }\n+    /* Skip debug insns and uninitialized uses.  */\n+    else if (DF_REF_CHAIN (ref)\n+\t     && NONDEBUG_INSN_P (DF_REF_INSN (ref)))\n+      {\n+\tgcc_assert (scopy);\n+\treplace_rtx (DF_REF_INSN (ref), reg, scopy);\n+\tdf_insn_rescan (DF_REF_INSN (ref));\n+      }\n+\n+  BITMAP_FREE (conv);\n+}\n+\n+/* Convert operand OP in INSN.  We should handle\n+   memory operands and uninitialized registers.\n+   All other register uses are converted during\n+   registers conversion.  */\n+\n+void\n+dimode_scalar_chain::convert_op (rtx *op, rtx_insn *insn)\n+{\n+  *op = copy_rtx_if_shared (*op);\n+\n+  if (GET_CODE (*op) == NOT)\n+    {\n+      convert_op (&XEXP (*op, 0), insn);\n+      PUT_MODE (*op, V2DImode);\n+    }\n+  else if (MEM_P (*op))\n+    {\n+      rtx tmp = gen_reg_rtx (DImode);\n+\n+      emit_insn_before (gen_move_insn (tmp, *op), insn);\n+      *op = gen_rtx_SUBREG (V2DImode, tmp, 0);\n+\n+      if (dump_file)\n+\tfprintf (dump_file, \"  Preloading operand for insn %d into r%d\\n\",\n+\t\t INSN_UID (insn), REGNO (tmp));\n+    }\n+  else if (REG_P (*op))\n+    {\n+      /* We may have not converted register usage in case\n+\t this register has no definition.  Otherwise it\n+\t should be converted in convert_reg.  */\n+      df_ref ref;\n+      FOR_EACH_INSN_USE (ref, insn)\n+\tif (DF_REF_REGNO (ref) == REGNO (*op))\n+\t  {\n+\t    gcc_assert (!DF_REF_CHAIN (ref));\n+\t    break;\n+\t  }\n+      *op = gen_rtx_SUBREG (V2DImode, *op, 0);\n+    }\n+  else if (CONST_INT_P (*op))\n+    {\n+      rtx vec_cst;\n+      rtx tmp = gen_rtx_SUBREG (V2DImode, gen_reg_rtx (DImode), 0);\n+\n+      /* Prefer all ones vector in case of -1.  */\n+      if (constm1_operand (*op, GET_MODE (*op)))\n+\tvec_cst = CONSTM1_RTX (V2DImode);\n+      else\n+\tvec_cst = gen_rtx_CONST_VECTOR (V2DImode,\n+\t\t\t\t\tgen_rtvec (2, *op, const0_rtx));\n+\n+      if (!standard_sse_constant_p (vec_cst, V2DImode))\n+\t{\n+\t  start_sequence ();\n+\t  vec_cst = validize_mem (force_const_mem (V2DImode, vec_cst));\n+\t  rtx_insn *seq = get_insns ();\n+\t  end_sequence ();\n+\t  emit_insn_before (seq, insn);\n+\t}\n+\n+      emit_insn_before (gen_move_insn (copy_rtx (tmp), vec_cst), insn);\n+      *op = tmp;\n+    }\n+  else\n+    {\n+      gcc_assert (SUBREG_P (*op));\n+      gcc_assert (GET_MODE (*op) == V2DImode);\n+    }\n+}\n+\n+/* Convert INSN to vector mode.  */\n+\n+void\n+dimode_scalar_chain::convert_insn (rtx_insn *insn)\n+{\n+  rtx def_set = single_set (insn);\n+  rtx src = SET_SRC (def_set);\n+  rtx dst = SET_DEST (def_set);\n+  rtx subreg;\n+\n+  if (MEM_P (dst) && !REG_P (src))\n+    {\n+      /* There are no scalar integer instructions and therefore\n+\t temporary register usage is required.  */\n+      rtx tmp = gen_reg_rtx (DImode);\n+      emit_conversion_insns (gen_move_insn (dst, tmp), insn);\n+      dst = gen_rtx_SUBREG (V2DImode, tmp, 0);\n+    }\n+\n+  switch (GET_CODE (src))\n+    {\n+    case ASHIFT:\n+    case ASHIFTRT:\n+    case LSHIFTRT:\n+      convert_op (&XEXP (src, 0), insn);\n+      PUT_MODE (src, V2DImode);\n+      break;\n+\n+    case PLUS:\n+    case MINUS:\n+    case IOR:\n+    case XOR:\n+    case AND:\n+      convert_op (&XEXP (src, 0), insn);\n+      convert_op (&XEXP (src, 1), insn);\n+      PUT_MODE (src, V2DImode);\n+      break;\n+\n+    case NEG:\n+      src = XEXP (src, 0);\n+      convert_op (&src, insn);\n+      subreg = gen_reg_rtx (V2DImode);\n+      emit_insn_before (gen_move_insn (subreg, CONST0_RTX (V2DImode)), insn);\n+      src = gen_rtx_MINUS (V2DImode, subreg, src);\n+      break;\n+\n+    case NOT:\n+      src = XEXP (src, 0);\n+      convert_op (&src, insn);\n+      subreg = gen_reg_rtx (V2DImode);\n+      emit_insn_before (gen_move_insn (subreg, CONSTM1_RTX (V2DImode)), insn);\n+      src = gen_rtx_XOR (V2DImode, src, subreg);\n+      break;\n+\n+    case MEM:\n+      if (!REG_P (dst))\n+\tconvert_op (&src, insn);\n+      break;\n+\n+    case REG:\n+      if (!MEM_P (dst))\n+\tconvert_op (&src, insn);\n+      break;\n+\n+    case SUBREG:\n+      gcc_assert (GET_MODE (src) == V2DImode);\n+      break;\n+\n+    case COMPARE:\n+      src = SUBREG_REG (XEXP (XEXP (src, 0), 0));\n+\n+      gcc_assert ((REG_P (src) && GET_MODE (src) == DImode)\n+\t\t  || (SUBREG_P (src) && GET_MODE (src) == V2DImode));\n+\n+      if (REG_P (src))\n+\tsubreg = gen_rtx_SUBREG (V2DImode, src, 0);\n+      else\n+\tsubreg = copy_rtx_if_shared (src);\n+      emit_insn_before (gen_vec_interleave_lowv2di (copy_rtx_if_shared (subreg),\n+\t\t\t\t\t\t    copy_rtx_if_shared (subreg),\n+\t\t\t\t\t\t    copy_rtx_if_shared (subreg)),\n+\t\t\tinsn);\n+      dst = gen_rtx_REG (CCmode, FLAGS_REG);\n+      src = gen_rtx_UNSPEC (CCmode, gen_rtvec (2, copy_rtx_if_shared (src),\n+\t\t\t\t\t       copy_rtx_if_shared (src)),\n+\t\t\t    UNSPEC_PTEST);\n+      break;\n+\n+    case CONST_INT:\n+      convert_op (&src, insn);\n+      break;\n+\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+  SET_SRC (def_set) = src;\n+  SET_DEST (def_set) = dst;\n+\n+  /* Drop possible dead definitions.  */\n+  PATTERN (insn) = def_set;\n+\n+  INSN_CODE (insn) = -1;\n+  recog_memoized (insn);\n+  df_insn_rescan (insn);\n+}\n+\n+/* Fix uses of converted REG in debug insns.  */\n+\n+void\n+timode_scalar_chain::fix_debug_reg_uses (rtx reg)\n+{\n+  if (!flag_var_tracking)\n+    return;\n+\n+  df_ref ref, next;\n+  for (ref = DF_REG_USE_CHAIN (REGNO (reg)); ref; ref = next)\n+    {\n+      rtx_insn *insn = DF_REF_INSN (ref);\n+      /* Make sure the next ref is for a different instruction,\n+         so that we're not affected by the rescan.  */\n+      next = DF_REF_NEXT_REG (ref);\n+      while (next && DF_REF_INSN (next) == insn)\n+\tnext = DF_REF_NEXT_REG (next);\n+\n+      if (DEBUG_INSN_P (insn))\n+\t{\n+\t  /* It may be a debug insn with a TImode variable in\n+\t     register.  */\n+\t  bool changed = false;\n+\t  for (; ref != next; ref = DF_REF_NEXT_REG (ref))\n+\t    {\n+\t      rtx *loc = DF_REF_LOC (ref);\n+\t      if (REG_P (*loc) && GET_MODE (*loc) == V1TImode)\n+\t\t{\n+\t\t  *loc = gen_rtx_SUBREG (TImode, *loc, 0);\n+\t\t  changed = true;\n+\t\t}\n+\t    }\n+\t  if (changed)\n+\t    df_insn_rescan (insn);\n+\t}\n+    }\n+}\n+\n+/* Convert INSN from TImode to V1T1mode.  */\n+\n+void\n+timode_scalar_chain::convert_insn (rtx_insn *insn)\n+{\n+  rtx def_set = single_set (insn);\n+  rtx src = SET_SRC (def_set);\n+  rtx dst = SET_DEST (def_set);\n+\n+  switch (GET_CODE (dst))\n+    {\n+    case REG:\n+      {\n+\trtx tmp = find_reg_equal_equiv_note (insn);\n+\tif (tmp)\n+\t  PUT_MODE (XEXP (tmp, 0), V1TImode);\n+\tPUT_MODE (dst, V1TImode);\n+\tfix_debug_reg_uses (dst);\n+      }\n+      break;\n+    case MEM:\n+      PUT_MODE (dst, V1TImode);\n+      break;\n+\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+  switch (GET_CODE (src))\n+    {\n+    case REG:\n+      PUT_MODE (src, V1TImode);\n+      /* Call fix_debug_reg_uses only if SRC is never defined.  */\n+      if (!DF_REG_DEF_CHAIN (REGNO (src)))\n+\tfix_debug_reg_uses (src);\n+      break;\n+\n+    case MEM:\n+      PUT_MODE (src, V1TImode);\n+      break;\n+\n+    case CONST_WIDE_INT:\n+      if (NONDEBUG_INSN_P (insn))\n+\t{\n+\t  /* Since there are no instructions to store 128-bit constant,\n+\t     temporary register usage is required.  */\n+\t  rtx tmp = gen_reg_rtx (V1TImode);\n+\t  start_sequence ();\n+\t  src = gen_rtx_CONST_VECTOR (V1TImode, gen_rtvec (1, src));\n+\t  src = validize_mem (force_const_mem (V1TImode, src));\n+\t  rtx_insn *seq = get_insns ();\n+\t  end_sequence ();\n+\t  if (seq)\n+\t    emit_insn_before (seq, insn);\n+\t  emit_conversion_insns (gen_rtx_SET (dst, tmp), insn);\n+\t  dst = tmp;\n+\t}\n+      break;\n+\n+    case CONST_INT:\n+      switch (standard_sse_constant_p (src, TImode))\n+\t{\n+\tcase 1:\n+\t  src = CONST0_RTX (GET_MODE (dst));\n+\t  break;\n+\tcase 2:\n+\t  src = CONSTM1_RTX (GET_MODE (dst));\n+\t  break;\n+\tdefault:\n+\t  gcc_unreachable ();\n+\t}\n+      if (NONDEBUG_INSN_P (insn))\n+\t{\n+\t  rtx tmp = gen_reg_rtx (V1TImode);\n+\t  /* Since there are no instructions to store standard SSE\n+\t     constant, temporary register usage is required.  */\n+\t  emit_conversion_insns (gen_rtx_SET (dst, tmp), insn);\n+\t  dst = tmp;\n+\t}\n+      break;\n+\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+  SET_SRC (def_set) = src;\n+  SET_DEST (def_set) = dst;\n+\n+  /* Drop possible dead definitions.  */\n+  PATTERN (insn) = def_set;\n+\n+  INSN_CODE (insn) = -1;\n+  recog_memoized (insn);\n+  df_insn_rescan (insn);\n+}\n+\n+void\n+dimode_scalar_chain::convert_registers ()\n+{\n+  bitmap_iterator bi;\n+  unsigned id;\n+\n+  EXECUTE_IF_SET_IN_BITMAP (defs, 0, id, bi)\n+    convert_reg (id);\n+\n+  EXECUTE_IF_AND_COMPL_IN_BITMAP (defs_conv, defs, 0, id, bi)\n+    make_vector_copies (id);\n+}\n+\n+/* Convert whole chain creating required register\n+   conversions and copies.  */\n+\n+int\n+scalar_chain::convert ()\n+{\n+  bitmap_iterator bi;\n+  unsigned id;\n+  int converted_insns = 0;\n+\n+  if (!dbg_cnt (stv_conversion))\n+    return 0;\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Converting chain #%d...\\n\", chain_id);\n+\n+  convert_registers ();\n+\n+  EXECUTE_IF_SET_IN_BITMAP (insns, 0, id, bi)\n+    {\n+      convert_insn (DF_INSN_UID_GET (id)->insn);\n+      converted_insns++;\n+    }\n+\n+  return converted_insns;\n+}\n+\n+/* Return 1 if INSN uses or defines a hard register.\n+   Hard register uses in a memory address are ignored.\n+   Clobbers and flags definitions are ignored.  */\n+\n+static bool\n+has_non_address_hard_reg (rtx_insn *insn)\n+{\n+  df_ref ref;\n+  FOR_EACH_INSN_DEF (ref, insn)\n+    if (HARD_REGISTER_P (DF_REF_REAL_REG (ref))\n+\t&& !DF_REF_FLAGS_IS_SET (ref, DF_REF_MUST_CLOBBER)\n+\t&& DF_REF_REGNO (ref) != FLAGS_REG)\n+      return true;\n+\n+  FOR_EACH_INSN_USE (ref, insn)\n+    if (!DF_REF_REG_MEM_P (ref) && HARD_REGISTER_P (DF_REF_REAL_REG (ref)))\n+      return true;\n+\n+  return false;\n+}\n+\n+/* Check if comparison INSN may be transformed\n+   into vector comparison.  Currently we transform\n+   zero checks only which look like:\n+\n+   (set (reg:CCZ 17 flags)\n+        (compare:CCZ (ior:SI (subreg:SI (reg:DI x) 4)\n+                             (subreg:SI (reg:DI x) 0))\n+\t\t     (const_int 0 [0])))  */\n+\n+static bool\n+convertible_comparison_p (rtx_insn *insn)\n+{\n+  if (!TARGET_SSE4_1)\n+    return false;\n+\n+  rtx def_set = single_set (insn);\n+\n+  gcc_assert (def_set);\n+\n+  rtx src = SET_SRC (def_set);\n+  rtx dst = SET_DEST (def_set);\n+\n+  gcc_assert (GET_CODE (src) == COMPARE);\n+\n+  if (GET_CODE (dst) != REG\n+      || REGNO (dst) != FLAGS_REG\n+      || GET_MODE (dst) != CCZmode)\n+    return false;\n+\n+  rtx op1 = XEXP (src, 0);\n+  rtx op2 = XEXP (src, 1);\n+\n+  if (op2 != CONST0_RTX (GET_MODE (op2)))\n+    return false;\n+\n+  if (GET_CODE (op1) != IOR)\n+    return false;\n+\n+  op2 = XEXP (op1, 1);\n+  op1 = XEXP (op1, 0);\n+\n+  if (!SUBREG_P (op1)\n+      || !SUBREG_P (op2)\n+      || GET_MODE (op1) != SImode\n+      || GET_MODE (op2) != SImode\n+      || ((SUBREG_BYTE (op1) != 0\n+\t   || SUBREG_BYTE (op2) != GET_MODE_SIZE (SImode))\n+\t  && (SUBREG_BYTE (op2) != 0\n+\t      || SUBREG_BYTE (op1) != GET_MODE_SIZE (SImode))))\n+    return false;\n+\n+  op1 = SUBREG_REG (op1);\n+  op2 = SUBREG_REG (op2);\n+\n+  if (op1 != op2\n+      || !REG_P (op1)\n+      || GET_MODE (op1) != DImode)\n+    return false;\n+\n+  return true;\n+}\n+\n+/* The DImode version of scalar_to_vector_candidate_p.  */\n+\n+static bool\n+dimode_scalar_to_vector_candidate_p (rtx_insn *insn)\n+{\n+  rtx def_set = single_set (insn);\n+\n+  if (!def_set)\n+    return false;\n+\n+  if (has_non_address_hard_reg (insn))\n+    return false;\n+\n+  rtx src = SET_SRC (def_set);\n+  rtx dst = SET_DEST (def_set);\n+\n+  if (GET_CODE (src) == COMPARE)\n+    return convertible_comparison_p (insn);\n+\n+  /* We are interested in DImode promotion only.  */\n+  if ((GET_MODE (src) != DImode\n+       && !CONST_INT_P (src))\n+      || GET_MODE (dst) != DImode)\n+    return false;\n+\n+  if (!REG_P (dst) && !MEM_P (dst))\n+    return false;\n+\n+  switch (GET_CODE (src))\n+    {\n+    case ASHIFTRT:\n+      if (!TARGET_AVX512VL)\n+\treturn false;\n+      /* FALLTHRU */\n+\n+    case ASHIFT:\n+    case LSHIFTRT:\n+      if (!CONST_INT_P (XEXP (src, 1))\n+\t  || !IN_RANGE (INTVAL (XEXP (src, 1)), 0, 63))\n+\treturn false;\n+      break;\n+\n+    case PLUS:\n+    case MINUS:\n+    case IOR:\n+    case XOR:\n+    case AND:\n+      if (!REG_P (XEXP (src, 1))\n+\t  && !MEM_P (XEXP (src, 1))\n+\t  && !CONST_INT_P (XEXP (src, 1)))\n+\treturn false;\n+\n+      if (GET_MODE (XEXP (src, 1)) != DImode\n+\t  && !CONST_INT_P (XEXP (src, 1)))\n+\treturn false;\n+      break;\n+\n+    case NEG:\n+    case NOT:\n+      break;\n+\n+    case REG:\n+      return true;\n+\n+    case MEM:\n+    case CONST_INT:\n+      return REG_P (dst);\n+\n+    default:\n+      return false;\n+    }\n+\n+  if (!REG_P (XEXP (src, 0))\n+      && !MEM_P (XEXP (src, 0))\n+      && !CONST_INT_P (XEXP (src, 0))\n+      /* Check for andnot case.  */\n+      && (GET_CODE (src) != AND\n+\t  || GET_CODE (XEXP (src, 0)) != NOT\n+\t  || !REG_P (XEXP (XEXP (src, 0), 0))))\n+      return false;\n+\n+  if (GET_MODE (XEXP (src, 0)) != DImode\n+      && !CONST_INT_P (XEXP (src, 0)))\n+    return false;\n+\n+  return true;\n+}\n+\n+/* The TImode version of scalar_to_vector_candidate_p.  */\n+\n+static bool\n+timode_scalar_to_vector_candidate_p (rtx_insn *insn)\n+{\n+  rtx def_set = single_set (insn);\n+\n+  if (!def_set)\n+    return false;\n+\n+  if (has_non_address_hard_reg (insn))\n+    return false;\n+\n+  rtx src = SET_SRC (def_set);\n+  rtx dst = SET_DEST (def_set);\n+\n+  /* Only TImode load and store are allowed.  */\n+  if (GET_MODE (dst) != TImode)\n+    return false;\n+\n+  if (MEM_P (dst))\n+    {\n+      /* Check for store.  Memory must be aligned or unaligned store\n+\t is optimal.  Only support store from register, standard SSE\n+\t constant or CONST_WIDE_INT generated from piecewise store.\n+\n+\t ??? Verify performance impact before enabling CONST_INT for\n+\t __int128 store.  */\n+      if (misaligned_operand (dst, TImode)\n+\t  && !TARGET_SSE_UNALIGNED_STORE_OPTIMAL)\n+\treturn false;\n+\n+      switch (GET_CODE (src))\n+\t{\n+\tdefault:\n+\t  return false;\n+\n+\tcase REG:\n+\tcase CONST_WIDE_INT:\n+\t  return true;\n+\n+\tcase CONST_INT:\n+\t  return standard_sse_constant_p (src, TImode);\n+\t}\n+    }\n+  else if (MEM_P (src))\n+    {\n+      /* Check for load.  Memory must be aligned or unaligned load is\n+\t optimal.  */\n+      return (REG_P (dst)\n+\t      && (!misaligned_operand (src, TImode)\n+\t\t  || TARGET_SSE_UNALIGNED_LOAD_OPTIMAL));\n+    }\n+\n+  return false;\n+}\n+\n+/* Return 1 if INSN may be converted into vector\n+   instruction.  */\n+\n+static bool\n+scalar_to_vector_candidate_p (rtx_insn *insn)\n+{\n+  if (TARGET_64BIT)\n+    return timode_scalar_to_vector_candidate_p (insn);\n+  else\n+    return dimode_scalar_to_vector_candidate_p (insn);\n+}\n+\n+/* The DImode version of remove_non_convertible_regs.  */\n+\n+static void\n+dimode_remove_non_convertible_regs (bitmap candidates)\n+{\n+  bitmap_iterator bi;\n+  unsigned id;\n+  bitmap regs = BITMAP_ALLOC (NULL);\n+\n+  EXECUTE_IF_SET_IN_BITMAP (candidates, 0, id, bi)\n+    {\n+      rtx def_set = single_set (DF_INSN_UID_GET (id)->insn);\n+      rtx reg = SET_DEST (def_set);\n+\n+      if (!REG_P (reg)\n+\t  || bitmap_bit_p (regs, REGNO (reg))\n+\t  || HARD_REGISTER_P (reg))\n+\tcontinue;\n+\n+      for (df_ref def = DF_REG_DEF_CHAIN (REGNO (reg));\n+\t   def;\n+\t   def = DF_REF_NEXT_REG (def))\n+\t{\n+\t  if (!bitmap_bit_p (candidates, DF_REF_INSN_UID (def)))\n+\t    {\n+\t      if (dump_file)\n+\t\tfprintf (dump_file,\n+\t\t\t \"r%d has non convertible definition in insn %d\\n\",\n+\t\t\t REGNO (reg), DF_REF_INSN_UID (def));\n+\n+\t      bitmap_set_bit (regs, REGNO (reg));\n+\t      break;\n+\t    }\n+\t}\n+    }\n+\n+  EXECUTE_IF_SET_IN_BITMAP (regs, 0, id, bi)\n+    {\n+      for (df_ref def = DF_REG_DEF_CHAIN (id);\n+\t   def;\n+\t   def = DF_REF_NEXT_REG (def))\n+\tif (bitmap_bit_p (candidates, DF_REF_INSN_UID (def)))\n+\t  {\n+\t    if (dump_file)\n+\t      fprintf (dump_file, \"Removing insn %d from candidates list\\n\",\n+\t\t       DF_REF_INSN_UID (def));\n+\n+\t    bitmap_clear_bit (candidates, DF_REF_INSN_UID (def));\n+\t  }\n+    }\n+\n+  BITMAP_FREE (regs);\n+}\n+\n+/* For a register REGNO, scan instructions for its defs and uses.\n+   Put REGNO in REGS if a def or use isn't in CANDIDATES.  */\n+\n+static void\n+timode_check_non_convertible_regs (bitmap candidates, bitmap regs,\n+\t\t\t\t   unsigned int regno)\n+{\n+  for (df_ref def = DF_REG_DEF_CHAIN (regno);\n+       def;\n+       def = DF_REF_NEXT_REG (def))\n+    {\n+      if (!bitmap_bit_p (candidates, DF_REF_INSN_UID (def)))\n+\t{\n+\t  if (dump_file)\n+\t    fprintf (dump_file,\n+\t\t     \"r%d has non convertible def in insn %d\\n\",\n+\t\t     regno, DF_REF_INSN_UID (def));\n+\n+\t  bitmap_set_bit (regs, regno);\n+\t  break;\n+\t}\n+    }\n+\n+  for (df_ref ref = DF_REG_USE_CHAIN (regno);\n+       ref;\n+       ref = DF_REF_NEXT_REG (ref))\n+    {\n+      /* Debug instructions are skipped.  */\n+      if (NONDEBUG_INSN_P (DF_REF_INSN (ref))\n+\t  && !bitmap_bit_p (candidates, DF_REF_INSN_UID (ref)))\n+\t{\n+\t  if (dump_file)\n+\t    fprintf (dump_file,\n+\t\t     \"r%d has non convertible use in insn %d\\n\",\n+\t\t     regno, DF_REF_INSN_UID (ref));\n+\n+\t  bitmap_set_bit (regs, regno);\n+\t  break;\n+\t}\n+    }\n+}\n+\n+/* The TImode version of remove_non_convertible_regs.  */\n+\n+static void\n+timode_remove_non_convertible_regs (bitmap candidates)\n+{\n+  bitmap_iterator bi;\n+  unsigned id;\n+  bitmap regs = BITMAP_ALLOC (NULL);\n+\n+  EXECUTE_IF_SET_IN_BITMAP (candidates, 0, id, bi)\n+    {\n+      rtx def_set = single_set (DF_INSN_UID_GET (id)->insn);\n+      rtx dest = SET_DEST (def_set);\n+      rtx src = SET_SRC (def_set);\n+\n+      if ((!REG_P (dest)\n+\t   || bitmap_bit_p (regs, REGNO (dest))\n+\t   || HARD_REGISTER_P (dest))\n+\t  && (!REG_P (src)\n+\t      || bitmap_bit_p (regs, REGNO (src))\n+\t      || HARD_REGISTER_P (src)))\n+\tcontinue;\n+\n+      if (REG_P (dest))\n+\ttimode_check_non_convertible_regs (candidates, regs,\n+\t\t\t\t\t   REGNO (dest));\n+\n+      if (REG_P (src))\n+\ttimode_check_non_convertible_regs (candidates, regs,\n+\t\t\t\t\t   REGNO (src));\n+    }\n+\n+  EXECUTE_IF_SET_IN_BITMAP (regs, 0, id, bi)\n+    {\n+      for (df_ref def = DF_REG_DEF_CHAIN (id);\n+\t   def;\n+\t   def = DF_REF_NEXT_REG (def))\n+\tif (bitmap_bit_p (candidates, DF_REF_INSN_UID (def)))\n+\t  {\n+\t    if (dump_file)\n+\t      fprintf (dump_file, \"Removing insn %d from candidates list\\n\",\n+\t\t       DF_REF_INSN_UID (def));\n+\n+\t    bitmap_clear_bit (candidates, DF_REF_INSN_UID (def));\n+\t  }\n+\n+      for (df_ref ref = DF_REG_USE_CHAIN (id);\n+\t   ref;\n+\t   ref = DF_REF_NEXT_REG (ref))\n+\tif (bitmap_bit_p (candidates, DF_REF_INSN_UID (ref)))\n+\t  {\n+\t    if (dump_file)\n+\t      fprintf (dump_file, \"Removing insn %d from candidates list\\n\",\n+\t\t       DF_REF_INSN_UID (ref));\n+\n+\t    bitmap_clear_bit (candidates, DF_REF_INSN_UID (ref));\n+\t  }\n+    }\n+\n+  BITMAP_FREE (regs);\n+}\n+\n+/* For a given bitmap of insn UIDs scans all instruction and\n+   remove insn from CANDIDATES in case it has both convertible\n+   and not convertible definitions.\n+\n+   All insns in a bitmap are conversion candidates according to\n+   scalar_to_vector_candidate_p.  Currently it implies all insns\n+   are single_set.  */\n+\n+static void\n+remove_non_convertible_regs (bitmap candidates)\n+{\n+  if (TARGET_64BIT)\n+    timode_remove_non_convertible_regs (candidates);\n+  else\n+    dimode_remove_non_convertible_regs (candidates);\n+}\n+\n+/* Main STV pass function.  Find and convert scalar\n+   instructions into vector mode when profitable.  */\n+\n+static unsigned int\n+convert_scalars_to_vector ()\n+{\n+  basic_block bb;\n+  bitmap candidates;\n+  int converted_insns = 0;\n+\n+  bitmap_obstack_initialize (NULL);\n+  candidates = BITMAP_ALLOC (NULL);\n+\n+  calculate_dominance_info (CDI_DOMINATORS);\n+  df_set_flags (DF_DEFER_INSN_RESCAN);\n+  df_chain_add_problem (DF_DU_CHAIN | DF_UD_CHAIN);\n+  df_md_add_problem ();\n+  df_analyze ();\n+\n+  /* Find all instructions we want to convert into vector mode.  */\n+  if (dump_file)\n+    fprintf (dump_file, \"Searching for mode conversion candidates...\\n\");\n+\n+  FOR_EACH_BB_FN (bb, cfun)\n+    {\n+      rtx_insn *insn;\n+      FOR_BB_INSNS (bb, insn)\n+\tif (scalar_to_vector_candidate_p (insn))\n+\t  {\n+\t    if (dump_file)\n+\t      fprintf (dump_file, \"  insn %d is marked as a candidate\\n\",\n+\t\t       INSN_UID (insn));\n+\n+\t    bitmap_set_bit (candidates, INSN_UID (insn));\n+\t  }\n+    }\n+\n+  remove_non_convertible_regs (candidates);\n+\n+  if (bitmap_empty_p (candidates))\n+    if (dump_file)\n+      fprintf (dump_file, \"There are no candidates for optimization.\\n\");\n+\n+  while (!bitmap_empty_p (candidates))\n+    {\n+      unsigned uid = bitmap_first_set_bit (candidates);\n+      scalar_chain *chain;\n+\n+      if (TARGET_64BIT)\n+\tchain = new timode_scalar_chain;\n+      else\n+\tchain = new dimode_scalar_chain;\n+\n+      /* Find instructions chain we want to convert to vector mode.\n+\t Check all uses and definitions to estimate all required\n+\t conversions.  */\n+      chain->build (candidates, uid);\n+\n+      if (chain->compute_convert_gain () > 0)\n+\tconverted_insns += chain->convert ();\n+      else\n+\tif (dump_file)\n+\t  fprintf (dump_file, \"Chain #%d conversion is not profitable\\n\",\n+\t\t   chain->chain_id);\n+\n+      delete chain;\n+    }\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Total insns converted: %d\\n\", converted_insns);\n+\n+  BITMAP_FREE (candidates);\n+  bitmap_obstack_release (NULL);\n+  df_process_deferred_rescans ();\n+\n+  /* Conversion means we may have 128bit register spills/fills\n+     which require aligned stack.  */\n+  if (converted_insns)\n+    {\n+      if (crtl->stack_alignment_needed < 128)\n+\tcrtl->stack_alignment_needed = 128;\n+      if (crtl->stack_alignment_estimated < 128)\n+\tcrtl->stack_alignment_estimated = 128;\n+      /* Fix up DECL_RTL/DECL_INCOMING_RTL of arguments.  */\n+      if (TARGET_64BIT)\n+\tfor (tree parm = DECL_ARGUMENTS (current_function_decl);\n+\t     parm; parm = DECL_CHAIN (parm))\n+\t  {\n+\t    if (TYPE_MODE (TREE_TYPE (parm)) != TImode)\n+\t      continue;\n+\t    if (DECL_RTL_SET_P (parm)\n+\t\t&& GET_MODE (DECL_RTL (parm)) == V1TImode)\n+\t      {\n+\t\trtx r = DECL_RTL (parm);\n+\t\tif (REG_P (r))\n+\t\t  SET_DECL_RTL (parm, gen_rtx_SUBREG (TImode, r, 0));\n+\t      }\n+\t    if (DECL_INCOMING_RTL (parm)\n+\t\t&& GET_MODE (DECL_INCOMING_RTL (parm)) == V1TImode)\n+\t      {\n+\t\trtx r = DECL_INCOMING_RTL (parm);\n+\t\tif (REG_P (r))\n+\t\t  DECL_INCOMING_RTL (parm) = gen_rtx_SUBREG (TImode, r, 0);\n+\t      }\n+\t  }\n+    }\n+\n+  return 0;\n+}\n+\n+static unsigned int\n+rest_of_handle_insert_vzeroupper (void)\n+{\n+  int i;\n+\n+  /* vzeroupper instructions are inserted immediately after reload to\n+     account for possible spills from 256bit or 512bit registers.  The pass\n+     reuses mode switching infrastructure by re-running mode insertion\n+     pass, so disable entities that have already been processed.  */\n+  for (i = 0; i < MAX_386_ENTITIES; i++)\n+    ix86_optimize_mode_switching[i] = 0;\n+\n+  ix86_optimize_mode_switching[AVX_U128] = 1;\n+\n+  /* Call optimize_mode_switching.  */\n+  g->get_passes ()->execute_pass_mode_switching ();\n+  return 0;\n+}\n+\n+namespace {\n+\n+const pass_data pass_data_insert_vzeroupper =\n+{\n+  RTL_PASS, /* type */\n+  \"vzeroupper\", /* name */\n+  OPTGROUP_NONE, /* optinfo_flags */\n+  TV_MACH_DEP, /* tv_id */\n+  0, /* properties_required */\n+  0, /* properties_provided */\n+  0, /* properties_destroyed */\n+  0, /* todo_flags_start */\n+  TODO_df_finish, /* todo_flags_finish */\n+};\n+\n+class pass_insert_vzeroupper : public rtl_opt_pass\n+{\n+public:\n+  pass_insert_vzeroupper(gcc::context *ctxt)\n+    : rtl_opt_pass(pass_data_insert_vzeroupper, ctxt)\n+  {}\n+\n+  /* opt_pass methods: */\n+  virtual bool gate (function *)\n+    {\n+      return TARGET_AVX\n+\t     && TARGET_VZEROUPPER && flag_expensive_optimizations\n+\t     && !optimize_size;\n+    }\n+\n+  virtual unsigned int execute (function *)\n+    {\n+      return rest_of_handle_insert_vzeroupper ();\n+    }\n+\n+}; // class pass_insert_vzeroupper\n+\n+const pass_data pass_data_stv =\n+{\n+  RTL_PASS, /* type */\n+  \"stv\", /* name */\n+  OPTGROUP_NONE, /* optinfo_flags */\n+  TV_MACH_DEP, /* tv_id */\n+  0, /* properties_required */\n+  0, /* properties_provided */\n+  0, /* properties_destroyed */\n+  0, /* todo_flags_start */\n+  TODO_df_finish, /* todo_flags_finish */\n+};\n+\n+class pass_stv : public rtl_opt_pass\n+{\n+public:\n+  pass_stv (gcc::context *ctxt)\n+    : rtl_opt_pass (pass_data_stv, ctxt),\n+      timode_p (false)\n+  {}\n+\n+  /* opt_pass methods: */\n+  virtual bool gate (function *)\n+    {\n+      return (timode_p == !!TARGET_64BIT\n+\t      && TARGET_STV && TARGET_SSE2 && optimize > 1);\n+    }\n+\n+  virtual unsigned int execute (function *)\n+    {\n+      return convert_scalars_to_vector ();\n+    }\n+\n+  opt_pass *clone ()\n+    {\n+      return new pass_stv (m_ctxt);\n+    }\n+\n+  void set_pass_param (unsigned int n, bool param)\n+    {\n+      gcc_assert (n == 0);\n+      timode_p = param;\n+    }\n+\n+private:\n+  bool timode_p;\n+}; // class pass_stv\n+\n+} // anon namespace\n+\n+rtl_opt_pass *\n+make_pass_insert_vzeroupper (gcc::context *ctxt)\n+{\n+  return new pass_insert_vzeroupper (ctxt);\n+}\n+\n+rtl_opt_pass *\n+make_pass_stv (gcc::context *ctxt)\n+{\n+  return new pass_stv (ctxt);\n+}\n+\n+/* Inserting ENDBRANCH instructions.  */\n+\n+static unsigned int\n+rest_of_insert_endbranch (void)\n+{\n+  timevar_push (TV_MACH_DEP);\n+\n+  rtx cet_eb;\n+  rtx_insn *insn;\n+  basic_block bb;\n+\n+  /* Currently emit EB if it's a tracking function, i.e. 'nocf_check' is\n+     absent among function attributes.  Later an optimization will be\n+     introduced to make analysis if an address of a static function is\n+     taken.  A static function whose address is not taken will get a\n+     nocf_check attribute.  This will allow to reduce the number of EB.  */\n+\n+  if (!lookup_attribute (\"nocf_check\",\n+\t\t\t TYPE_ATTRIBUTES (TREE_TYPE (cfun->decl)))\n+      && (!flag_manual_endbr\n+\t  || lookup_attribute (\"cf_check\",\n+\t\t\t       DECL_ATTRIBUTES (cfun->decl)))\n+      && !cgraph_node::get (cfun->decl)->only_called_directly_p ())\n+    {\n+      /* Queue ENDBR insertion to x86_function_profiler.  */\n+      if (crtl->profile && flag_fentry)\n+\tcfun->machine->endbr_queued_at_entrance = true;\n+      else\n+\t{\n+\t  cet_eb = gen_nop_endbr ();\n+\n+\t  bb = ENTRY_BLOCK_PTR_FOR_FN (cfun)->next_bb;\n+\t  insn = BB_HEAD (bb);\n+\t  emit_insn_before (cet_eb, insn);\n+\t}\n+    }\n+\n+  bb = 0;\n+  FOR_EACH_BB_FN (bb, cfun)\n+    {\n+      for (insn = BB_HEAD (bb); insn != NEXT_INSN (BB_END (bb));\n+\t   insn = NEXT_INSN (insn))\n+\t{\n+\t  if (CALL_P (insn))\n+\t    {\n+\t      bool need_endbr;\n+\t      need_endbr = find_reg_note (insn, REG_SETJMP, NULL) != NULL;\n+\t      if (!need_endbr && !SIBLING_CALL_P (insn))\n+\t\t{\n+\t\t  rtx call = get_call_rtx_from (insn);\n+\t\t  rtx fnaddr = XEXP (call, 0);\n+\t\t  tree fndecl = NULL_TREE;\n+\n+\t\t  /* Also generate ENDBRANCH for non-tail call which\n+\t\t     may return via indirect branch.  */\n+\t\t  if (GET_CODE (XEXP (fnaddr, 0)) == SYMBOL_REF)\n+\t\t    fndecl = SYMBOL_REF_DECL (XEXP (fnaddr, 0));\n+\t\t  if (fndecl == NULL_TREE)\n+\t\t    fndecl = MEM_EXPR (fnaddr);\n+\t\t  if (fndecl\n+\t\t      && TREE_CODE (TREE_TYPE (fndecl)) != FUNCTION_TYPE\n+\t\t      && TREE_CODE (TREE_TYPE (fndecl)) != METHOD_TYPE)\n+\t\t    fndecl = NULL_TREE;\n+\t\t  if (fndecl && TYPE_ARG_TYPES (TREE_TYPE (fndecl)))\n+\t\t    {\n+\t\t      tree fntype = TREE_TYPE (fndecl);\n+\t\t      if (lookup_attribute (\"indirect_return\",\n+\t\t\t\t\t    TYPE_ATTRIBUTES (fntype)))\n+\t\t\tneed_endbr = true;\n+\t\t    }\n+\t\t}\n+\t      if (!need_endbr)\n+\t\tcontinue;\n+\t      /* Generate ENDBRANCH after CALL, which can return more than\n+\t\t twice, setjmp-like functions.  */\n+\n+\t      cet_eb = gen_nop_endbr ();\n+\t      emit_insn_after_setloc (cet_eb, insn, INSN_LOCATION (insn));\n+\t      continue;\n+\t    }\n+\n+\t  if (JUMP_P (insn) && flag_cet_switch)\n+\t    {\n+\t      rtx target = JUMP_LABEL (insn);\n+\t      if (target == NULL_RTX || ANY_RETURN_P (target))\n+\t\tcontinue;\n+\n+\t      /* Check the jump is a switch table.  */\n+\t      rtx_insn *label = as_a<rtx_insn *> (target);\n+\t      rtx_insn *table = next_insn (label);\n+\t      if (table == NULL_RTX || !JUMP_TABLE_DATA_P (table))\n+\t\tcontinue;\n+\n+\t      /* For the indirect jump find out all places it jumps and insert\n+\t\t ENDBRANCH there.  It should be done under a special flag to\n+\t\t control ENDBRANCH generation for switch stmts.  */\n+\t      edge_iterator ei;\n+\t      edge e;\n+\t      basic_block dest_blk;\n+\n+\t      FOR_EACH_EDGE (e, ei, bb->succs)\n+\t\t{\n+\t\t  rtx_insn *insn;\n+\n+\t\t  dest_blk = e->dest;\n+\t\t  insn = BB_HEAD (dest_blk);\n+\t\t  gcc_assert (LABEL_P (insn));\n+\t\t  cet_eb = gen_nop_endbr ();\n+\t\t  emit_insn_after (cet_eb, insn);\n+\t\t}\n+\t      continue;\n+\t    }\n+\n+\t  if ((LABEL_P (insn) && LABEL_PRESERVE_P (insn))\n+\t      || (NOTE_P (insn)\n+\t\t  && NOTE_KIND (insn) == NOTE_INSN_DELETED_LABEL))\n+\t    /* TODO.  Check /s bit also.  */\n+\t    {\n+\t      cet_eb = gen_nop_endbr ();\n+\t      emit_insn_after (cet_eb, insn);\n+\t      continue;\n+\t    }\n+\t}\n+    }\n+\n+  timevar_pop (TV_MACH_DEP);\n+  return 0;\n+}\n+\n+namespace {\n+\n+const pass_data pass_data_insert_endbranch =\n+{\n+  RTL_PASS, /* type.  */\n+  \"cet\", /* name.  */\n+  OPTGROUP_NONE, /* optinfo_flags.  */\n+  TV_MACH_DEP, /* tv_id.  */\n+  0, /* properties_required.  */\n+  0, /* properties_provided.  */\n+  0, /* properties_destroyed.  */\n+  0, /* todo_flags_start.  */\n+  0, /* todo_flags_finish.  */\n+};\n+\n+class pass_insert_endbranch : public rtl_opt_pass\n+{\n+public:\n+  pass_insert_endbranch (gcc::context *ctxt)\n+    : rtl_opt_pass (pass_data_insert_endbranch, ctxt)\n+  {}\n+\n+  /* opt_pass methods: */\n+  virtual bool gate (function *)\n+    {\n+      return ((flag_cf_protection & CF_BRANCH));\n+    }\n+\n+  virtual unsigned int execute (function *)\n+    {\n+      return rest_of_insert_endbranch ();\n+    }\n+\n+}; // class pass_insert_endbranch\n+\n+} // anon namespace\n+\n+rtl_opt_pass *\n+make_pass_insert_endbranch (gcc::context *ctxt)\n+{\n+  return new pass_insert_endbranch (ctxt);\n+}\n+\n+/* At entry of the nearest common dominator for basic blocks with\n+   conversions, generate a single\n+\tvxorps %xmmN, %xmmN, %xmmN\n+   for all\n+\tvcvtss2sd  op, %xmmN, %xmmX\n+\tvcvtsd2ss  op, %xmmN, %xmmX\n+\tvcvtsi2ss  op, %xmmN, %xmmX\n+\tvcvtsi2sd  op, %xmmN, %xmmX\n+\n+   NB: We want to generate only a single vxorps to cover the whole\n+   function.  The LCM algorithm isn't appropriate here since it may\n+   place a vxorps inside the loop.  */\n+\n+static unsigned int\n+remove_partial_avx_dependency (void)\n+{\n+  timevar_push (TV_MACH_DEP);\n+\n+  bitmap_obstack_initialize (NULL);\n+  bitmap convert_bbs = BITMAP_ALLOC (NULL);\n+\n+  basic_block bb;\n+  rtx_insn *insn, *set_insn;\n+  rtx set;\n+  rtx v4sf_const0 = NULL_RTX;\n+\n+  auto_vec<rtx_insn *> control_flow_insns;\n+\n+  FOR_EACH_BB_FN (bb, cfun)\n+    {\n+      FOR_BB_INSNS (bb, insn)\n+\t{\n+\t  if (!NONDEBUG_INSN_P (insn))\n+\t    continue;\n+\n+\t  set = single_set (insn);\n+\t  if (!set)\n+\t    continue;\n+\n+\t  if (get_attr_avx_partial_xmm_update (insn)\n+\t      != AVX_PARTIAL_XMM_UPDATE_TRUE)\n+\t    continue;\n+\n+\t  if (!v4sf_const0)\n+\t    {\n+\t      calculate_dominance_info (CDI_DOMINATORS);\n+\t      df_set_flags (DF_DEFER_INSN_RESCAN);\n+\t      df_chain_add_problem (DF_DU_CHAIN | DF_UD_CHAIN);\n+\t      df_md_add_problem ();\n+\t      df_analyze ();\n+\t      v4sf_const0 = gen_reg_rtx (V4SFmode);\n+\t    }\n+\n+\t  /* Convert PARTIAL_XMM_UPDATE_TRUE insns, DF -> SF, SF -> DF,\n+\t     SI -> SF, SI -> DF, DI -> SF, DI -> DF, to vec_dup and\n+\t     vec_merge with subreg.  */\n+\t  rtx src = SET_SRC (set);\n+\t  rtx dest = SET_DEST (set);\n+\t  machine_mode dest_mode = GET_MODE (dest);\n+\n+\t  rtx zero;\n+\t  machine_mode dest_vecmode;\n+\t  if (dest_mode == E_SFmode)\n+\t    {\n+\t      dest_vecmode = V4SFmode;\n+\t      zero = v4sf_const0;\n+\t    }\n+\t  else\n+\t    {\n+\t      dest_vecmode = V2DFmode;\n+\t      zero = gen_rtx_SUBREG (V2DFmode, v4sf_const0, 0);\n+\t    }\n+\n+\t  /* Change source to vector mode.  */\n+\t  src = gen_rtx_VEC_DUPLICATE (dest_vecmode, src);\n+\t  src = gen_rtx_VEC_MERGE (dest_vecmode, src, zero,\n+\t\t\t\t   GEN_INT (HOST_WIDE_INT_1U));\n+\t  /* Change destination to vector mode.  */\n+\t  rtx vec = gen_reg_rtx (dest_vecmode);\n+\t  /* Generate an XMM vector SET.  */\n+\t  set = gen_rtx_SET (vec, src);\n+\t  set_insn = emit_insn_before (set, insn);\n+\t  df_insn_rescan (set_insn);\n+\n+\t  if (cfun->can_throw_non_call_exceptions)\n+\t    {\n+\t      /* Handle REG_EH_REGION note.  */\n+\t      rtx note = find_reg_note (insn, REG_EH_REGION, NULL_RTX);\n+\t      if (note)\n+\t\t{\n+\t\t  control_flow_insns.safe_push (set_insn);\n+\t\t  add_reg_note (set_insn, REG_EH_REGION, XEXP (note, 0));\n+\t\t}\n+\t    }\n+\n+\t  src = gen_rtx_SUBREG (dest_mode, vec, 0);\n+\t  set = gen_rtx_SET (dest, src);\n+\n+\t  /* Drop possible dead definitions.  */\n+\t  PATTERN (insn) = set;\n+\n+\t  INSN_CODE (insn) = -1;\n+\t  recog_memoized (insn);\n+\t  df_insn_rescan (insn);\n+\t  bitmap_set_bit (convert_bbs, bb->index);\n+\t}\n+    }\n+\n+  if (v4sf_const0)\n+    {\n+      /* (Re-)discover loops so that bb->loop_father can be used in the\n+\t analysis below.  */\n+      loop_optimizer_init (AVOID_CFG_MODIFICATIONS);\n+\n+      /* Generate a vxorps at entry of the nearest dominator for basic\n+\t blocks with conversions, which is in the the fake loop that\n+\t contains the whole function, so that there is only a single\n+\t vxorps in the whole function.   */\n+      bb = nearest_common_dominator_for_set (CDI_DOMINATORS,\n+\t\t\t\t\t     convert_bbs);\n+      while (bb->loop_father->latch\n+\t     != EXIT_BLOCK_PTR_FOR_FN (cfun))\n+\tbb = get_immediate_dominator (CDI_DOMINATORS,\n+\t\t\t\t      bb->loop_father->header);\n+\n+      set = gen_rtx_SET (v4sf_const0, CONST0_RTX (V4SFmode));\n+\n+      insn = BB_HEAD (bb);\n+      while (insn && !NONDEBUG_INSN_P (insn))\n+\t{\n+\t  if (insn == BB_END (bb))\n+\t    {\n+\t      insn = NULL;\n+\t      break;\n+\t    }\n+\t  insn = NEXT_INSN (insn);\n+\t}\n+      if (insn == BB_HEAD (bb))\n+        set_insn = emit_insn_before (set, insn);\n+      else\n+\tset_insn = emit_insn_after (set,\n+\t\t\t\t    insn ? PREV_INSN (insn) : BB_END (bb));\n+      df_insn_rescan (set_insn);\n+      df_process_deferred_rescans ();\n+      loop_optimizer_finalize ();\n+\n+      if (!control_flow_insns.is_empty ())\n+\t{\n+\t  free_dominance_info (CDI_DOMINATORS);\n+\n+\t  unsigned int i;\n+\t  FOR_EACH_VEC_ELT (control_flow_insns, i, insn)\n+\t    if (control_flow_insn_p (insn))\n+\t      {\n+\t\t/* Split the block after insn.  There will be a fallthru\n+\t\t   edge, which is OK so we keep it.  We have to create\n+\t\t   the exception edges ourselves.  */\n+\t\tbb = BLOCK_FOR_INSN (insn);\n+\t\tsplit_block (bb, insn);\n+\t\trtl_make_eh_edge (NULL, bb, BB_END (bb));\n+\t      }\n+\t}\n+    }\n+\n+  bitmap_obstack_release (NULL);\n+  BITMAP_FREE (convert_bbs);\n+\n+  timevar_pop (TV_MACH_DEP);\n+  return 0;\n+}\n+\n+namespace {\n+\n+const pass_data pass_data_remove_partial_avx_dependency =\n+{\n+  RTL_PASS, /* type */\n+  \"rpad\", /* name */\n+  OPTGROUP_NONE, /* optinfo_flags */\n+  TV_MACH_DEP, /* tv_id */\n+  0, /* properties_required */\n+  0, /* properties_provided */\n+  0, /* properties_destroyed */\n+  0, /* todo_flags_start */\n+  TODO_df_finish, /* todo_flags_finish */\n+};\n+\n+class pass_remove_partial_avx_dependency : public rtl_opt_pass\n+{\n+public:\n+  pass_remove_partial_avx_dependency (gcc::context *ctxt)\n+    : rtl_opt_pass (pass_data_remove_partial_avx_dependency, ctxt)\n+  {}\n+\n+  /* opt_pass methods: */\n+  virtual bool gate (function *)\n+    {\n+      return (TARGET_AVX\n+\t      && TARGET_SSE_PARTIAL_REG_DEPENDENCY\n+\t      && TARGET_SSE_MATH\n+\t      && optimize\n+\t      && optimize_function_for_speed_p (cfun));\n+    }\n+\n+  virtual unsigned int execute (function *)\n+    {\n+      return remove_partial_avx_dependency ();\n+    }\n+}; // class pass_rpad\n+\n+} // anon namespace\n+\n+rtl_opt_pass *\n+make_pass_remove_partial_avx_dependency (gcc::context *ctxt)\n+{\n+  return new pass_remove_partial_avx_dependency (ctxt);\n+}\n+\n+/* This compares the priority of target features in function DECL1\n+   and DECL2.  It returns positive value if DECL1 is higher priority,\n+   negative value if DECL2 is higher priority and 0 if they are the\n+   same.  */\n+\n+int\n+ix86_compare_version_priority (tree decl1, tree decl2)\n+{\n+  unsigned int priority1 = get_builtin_code_for_version (decl1, NULL);\n+  unsigned int priority2 = get_builtin_code_for_version (decl2, NULL);\n+\n+  return (int)priority1 - (int)priority2;\n+}\n+\n+/* V1 and V2 point to function versions with different priorities\n+   based on the target ISA.  This function compares their priorities.  */\n+ \n+static int\n+feature_compare (const void *v1, const void *v2)\n+{\n+  typedef struct _function_version_info\n+    {\n+      tree version_decl;\n+      tree predicate_chain;\n+      unsigned int dispatch_priority;\n+    } function_version_info;\n+\n+  const function_version_info c1 = *(const function_version_info *)v1;\n+  const function_version_info c2 = *(const function_version_info *)v2;\n+  return (c2.dispatch_priority - c1.dispatch_priority);\n+}\n+\n+/* This adds a condition to the basic_block NEW_BB in function FUNCTION_DECL\n+   to return a pointer to VERSION_DECL if the outcome of the expression\n+   formed by PREDICATE_CHAIN is true.  This function will be called during\n+   version dispatch to decide which function version to execute.  It returns\n+   the basic block at the end, to which more conditions can be added.  */\n+\n+static basic_block\n+add_condition_to_bb (tree function_decl, tree version_decl,\n+\t\t     tree predicate_chain, basic_block new_bb)\n+{\n+  gimple *return_stmt;\n+  tree convert_expr, result_var;\n+  gimple *convert_stmt;\n+  gimple *call_cond_stmt;\n+  gimple *if_else_stmt;\n+\n+  basic_block bb1, bb2, bb3;\n+  edge e12, e23;\n+\n+  tree cond_var, and_expr_var = NULL_TREE;\n+  gimple_seq gseq;\n+\n+  tree predicate_decl, predicate_arg;\n+\n+  push_cfun (DECL_STRUCT_FUNCTION (function_decl));\n+\n+  gcc_assert (new_bb != NULL);\n+  gseq = bb_seq (new_bb);\n+\n+\n+  convert_expr = build1 (CONVERT_EXPR, ptr_type_node,\n+\t     \t\t build_fold_addr_expr (version_decl));\n+  result_var = create_tmp_var (ptr_type_node);\n+  convert_stmt = gimple_build_assign (result_var, convert_expr); \n+  return_stmt = gimple_build_return (result_var);\n+\n+  if (predicate_chain == NULL_TREE)\n+    {\n+      gimple_seq_add_stmt (&gseq, convert_stmt);\n+      gimple_seq_add_stmt (&gseq, return_stmt);\n+      set_bb_seq (new_bb, gseq);\n+      gimple_set_bb (convert_stmt, new_bb);\n+      gimple_set_bb (return_stmt, new_bb);\n+      pop_cfun ();\n+      return new_bb;\n+    }\n+\n+  while (predicate_chain != NULL)\n+    {\n+      cond_var = create_tmp_var (integer_type_node);\n+      predicate_decl = TREE_PURPOSE (predicate_chain);\n+      predicate_arg = TREE_VALUE (predicate_chain);\n+      call_cond_stmt = gimple_build_call (predicate_decl, 1, predicate_arg);\n+      gimple_call_set_lhs (call_cond_stmt, cond_var);\n+\n+      gimple_set_block (call_cond_stmt, DECL_INITIAL (function_decl));\n+      gimple_set_bb (call_cond_stmt, new_bb);\n+      gimple_seq_add_stmt (&gseq, call_cond_stmt);\n+\n+      predicate_chain = TREE_CHAIN (predicate_chain);\n+      \n+      if (and_expr_var == NULL)\n+        and_expr_var = cond_var;\n+      else\n+\t{\n+\t  gimple *assign_stmt;\n+\t  /* Use MIN_EXPR to check if any integer is zero?.\n+\t     and_expr_var = min_expr <cond_var, and_expr_var>  */\n+\t  assign_stmt = gimple_build_assign (and_expr_var,\n+\t\t\t  build2 (MIN_EXPR, integer_type_node,\n+\t\t\t\t  cond_var, and_expr_var));\n+\n+\t  gimple_set_block (assign_stmt, DECL_INITIAL (function_decl));\n+\t  gimple_set_bb (assign_stmt, new_bb);\n+\t  gimple_seq_add_stmt (&gseq, assign_stmt);\n+\t}\n+    }\n+\n+  if_else_stmt = gimple_build_cond (GT_EXPR, and_expr_var,\n+\t  \t\t            integer_zero_node,\n+\t\t\t\t    NULL_TREE, NULL_TREE);\n+  gimple_set_block (if_else_stmt, DECL_INITIAL (function_decl));\n+  gimple_set_bb (if_else_stmt, new_bb);\n+  gimple_seq_add_stmt (&gseq, if_else_stmt);\n+\n+  gimple_seq_add_stmt (&gseq, convert_stmt);\n+  gimple_seq_add_stmt (&gseq, return_stmt);\n+  set_bb_seq (new_bb, gseq);\n+\n+  bb1 = new_bb;\n+  e12 = split_block (bb1, if_else_stmt);\n+  bb2 = e12->dest;\n+  e12->flags &= ~EDGE_FALLTHRU;\n+  e12->flags |= EDGE_TRUE_VALUE;\n+\n+  e23 = split_block (bb2, return_stmt);\n+\n+  gimple_set_bb (convert_stmt, bb2);\n+  gimple_set_bb (return_stmt, bb2);\n+\n+  bb3 = e23->dest;\n+  make_edge (bb1, bb3, EDGE_FALSE_VALUE); \n+\n+  remove_edge (e23);\n+  make_edge (bb2, EXIT_BLOCK_PTR_FOR_FN (cfun), 0);\n+\n+  pop_cfun ();\n+\n+  return bb3;\n+}\n+\n+/* This function generates the dispatch function for\n+   multi-versioned functions.  DISPATCH_DECL is the function which will\n+   contain the dispatch logic.  FNDECLS are the function choices for\n+   dispatch, and is a tree chain.  EMPTY_BB is the basic block pointer\n+   in DISPATCH_DECL in which the dispatch code is generated.  */\n+\n+static int\n+dispatch_function_versions (tree dispatch_decl,\n+\t\t\t    void *fndecls_p,\n+\t\t\t    basic_block *empty_bb)\n+{\n+  tree default_decl;\n+  gimple *ifunc_cpu_init_stmt;\n+  gimple_seq gseq;\n+  int ix;\n+  tree ele;\n+  vec<tree> *fndecls;\n+  unsigned int num_versions = 0;\n+  unsigned int actual_versions = 0;\n+  unsigned int i;\n+\n+  struct _function_version_info\n+    {\n+      tree version_decl;\n+      tree predicate_chain;\n+      unsigned int dispatch_priority;\n+    }*function_version_info;\n+\n+  gcc_assert (dispatch_decl != NULL\n+\t      && fndecls_p != NULL\n+\t      && empty_bb != NULL);\n+\n+  /*fndecls_p is actually a vector.  */\n+  fndecls = static_cast<vec<tree> *> (fndecls_p);\n+\n+  /* At least one more version other than the default.  */\n+  num_versions = fndecls->length ();\n+  gcc_assert (num_versions >= 2);\n+\n+  function_version_info = (struct _function_version_info *)\n+    XNEWVEC (struct _function_version_info, (num_versions - 1));\n+\n+  /* The first version in the vector is the default decl.  */\n+  default_decl = (*fndecls)[0];\n+\n+  push_cfun (DECL_STRUCT_FUNCTION (dispatch_decl));\n+\n+  gseq = bb_seq (*empty_bb);\n+  /* Function version dispatch is via IFUNC.  IFUNC resolvers fire before\n+     constructors, so explicity call __builtin_cpu_init here.  */\n+  ifunc_cpu_init_stmt\n+    = gimple_build_call_vec (get_ix86_builtin (IX86_BUILTIN_CPU_INIT), vNULL);\n+  gimple_seq_add_stmt (&gseq, ifunc_cpu_init_stmt);\n+  gimple_set_bb (ifunc_cpu_init_stmt, *empty_bb);\n+  set_bb_seq (*empty_bb, gseq);\n+\n+  pop_cfun ();\n+\n+\n+  for (ix = 1; fndecls->iterate (ix, &ele); ++ix)\n+    {\n+      tree version_decl = ele;\n+      tree predicate_chain = NULL_TREE;\n+      unsigned int priority;\n+      /* Get attribute string, parse it and find the right predicate decl.\n+         The predicate function could be a lengthy combination of many\n+\t features, like arch-type and various isa-variants.  */\n+      priority = get_builtin_code_for_version (version_decl,\n+\t \t\t\t               &predicate_chain);\n+\n+      if (predicate_chain == NULL_TREE)\n+\tcontinue;\n+\n+      function_version_info [actual_versions].version_decl = version_decl;\n+      function_version_info [actual_versions].predicate_chain\n+\t = predicate_chain;\n+      function_version_info [actual_versions].dispatch_priority = priority;\n+      actual_versions++;\n+    }\n+\n+  /* Sort the versions according to descending order of dispatch priority.  The\n+     priority is based on the ISA.  This is not a perfect solution.  There\n+     could still be ambiguity.  If more than one function version is suitable\n+     to execute,  which one should be dispatched?  In future, allow the user\n+     to specify a dispatch  priority next to the version.  */\n+  qsort (function_version_info, actual_versions,\n+         sizeof (struct _function_version_info), feature_compare);\n+\n+  for  (i = 0; i < actual_versions; ++i)\n+    *empty_bb = add_condition_to_bb (dispatch_decl,\n+\t\t\t\t     function_version_info[i].version_decl,\n+\t\t\t\t     function_version_info[i].predicate_chain,\n+\t\t\t\t     *empty_bb);\n+\n+  /* dispatch default version at the end.  */\n+  *empty_bb = add_condition_to_bb (dispatch_decl, default_decl,\n+\t\t\t\t   NULL, *empty_bb);\n+\n+  free (function_version_info);\n+  return 0;\n+}\n+\n+/* This function changes the assembler name for functions that are\n+   versions.  If DECL is a function version and has a \"target\"\n+   attribute, it appends the attribute string to its assembler name.  */\n+\n+static tree\n+ix86_mangle_function_version_assembler_name (tree decl, tree id)\n+{\n+  tree version_attr;\n+  const char *orig_name, *version_string;\n+  char *attr_str, *assembler_name;\n+\n+  if (DECL_DECLARED_INLINE_P (decl)\n+      && lookup_attribute (\"gnu_inline\",\n+\t\t\t   DECL_ATTRIBUTES (decl)))\n+    error_at (DECL_SOURCE_LOCATION (decl),\n+\t      \"function versions cannot be marked as gnu_inline,\"\n+\t      \" bodies have to be generated\");\n+\n+  if (DECL_VIRTUAL_P (decl)\n+      || DECL_VINDEX (decl))\n+    sorry (\"virtual function multiversioning not supported\");\n+\n+  version_attr = lookup_attribute (\"target\", DECL_ATTRIBUTES (decl));\n+\n+  /* target attribute string cannot be NULL.  */\n+  gcc_assert (version_attr != NULL_TREE);\n+\n+  orig_name = IDENTIFIER_POINTER (id);\n+  version_string\n+    = TREE_STRING_POINTER (TREE_VALUE (TREE_VALUE (version_attr)));\n+\n+  if (strcmp (version_string, \"default\") == 0)\n+    return id;\n+\n+  attr_str = sorted_attr_string (TREE_VALUE (version_attr));\n+  assembler_name = XNEWVEC (char, strlen (orig_name) + strlen (attr_str) + 2);\n+\n+  sprintf (assembler_name, \"%s.%s\", orig_name, attr_str);\n+\n+  /* Allow assembler name to be modified if already set.  */\n+  if (DECL_ASSEMBLER_NAME_SET_P (decl))\n+    SET_DECL_RTL (decl, NULL);\n+\n+  tree ret = get_identifier (assembler_name);\n+  XDELETEVEC (attr_str);\n+  XDELETEVEC (assembler_name);\n+  return ret;\n+}\n+\n+tree \n+ix86_mangle_decl_assembler_name (tree decl, tree id)\n+{\n+  /* For function version, add the target suffix to the assembler name.  */\n+  if (TREE_CODE (decl) == FUNCTION_DECL\n+      && DECL_FUNCTION_VERSIONED (decl))\n+    id = ix86_mangle_function_version_assembler_name (decl, id);\n+#ifdef SUBTARGET_MANGLE_DECL_ASSEMBLER_NAME\n+  id = SUBTARGET_MANGLE_DECL_ASSEMBLER_NAME (decl, id);\n+#endif\n+\n+  return id;\n+}\n+\n+/* Make a dispatcher declaration for the multi-versioned function DECL.\n+   Calls to DECL function will be replaced with calls to the dispatcher\n+   by the front-end.  Returns the decl of the dispatcher function.  */\n+\n+tree\n+ix86_get_function_versions_dispatcher (void *decl)\n+{\n+  tree fn = (tree) decl;\n+  struct cgraph_node *node = NULL;\n+  struct cgraph_node *default_node = NULL;\n+  struct cgraph_function_version_info *node_v = NULL;\n+  struct cgraph_function_version_info *first_v = NULL;\n+\n+  tree dispatch_decl = NULL;\n+\n+  struct cgraph_function_version_info *default_version_info = NULL;\n+ \n+  gcc_assert (fn != NULL && DECL_FUNCTION_VERSIONED (fn));\n+\n+  node = cgraph_node::get (fn);\n+  gcc_assert (node != NULL);\n+\n+  node_v = node->function_version ();\n+  gcc_assert (node_v != NULL);\n+ \n+  if (node_v->dispatcher_resolver != NULL)\n+    return node_v->dispatcher_resolver;\n+\n+  /* Find the default version and make it the first node.  */\n+  first_v = node_v;\n+  /* Go to the beginning of the chain.  */\n+  while (first_v->prev != NULL)\n+    first_v = first_v->prev;\n+  default_version_info = first_v;\n+  while (default_version_info != NULL)\n+    {\n+      if (is_function_default_version\n+\t    (default_version_info->this_node->decl))\n+        break;\n+      default_version_info = default_version_info->next;\n+    }\n+\n+  /* If there is no default node, just return NULL.  */\n+  if (default_version_info == NULL)\n+    return NULL;\n+\n+  /* Make default info the first node.  */\n+  if (first_v != default_version_info)\n+    {\n+      default_version_info->prev->next = default_version_info->next;\n+      if (default_version_info->next)\n+        default_version_info->next->prev = default_version_info->prev;\n+      first_v->prev = default_version_info;\n+      default_version_info->next = first_v;\n+      default_version_info->prev = NULL;\n+    }\n+\n+  default_node = default_version_info->this_node;\n+\n+#if defined (ASM_OUTPUT_TYPE_DIRECTIVE)\n+  if (targetm.has_ifunc_p ())\n+    {\n+      struct cgraph_function_version_info *it_v = NULL;\n+      struct cgraph_node *dispatcher_node = NULL;\n+      struct cgraph_function_version_info *dispatcher_version_info = NULL;\n+\n+      /* Right now, the dispatching is done via ifunc.  */\n+      dispatch_decl = make_dispatcher_decl (default_node->decl);\n+\n+      dispatcher_node = cgraph_node::get_create (dispatch_decl);\n+      gcc_assert (dispatcher_node != NULL);\n+      dispatcher_node->dispatcher_function = 1;\n+      dispatcher_version_info\n+\t= dispatcher_node->insert_new_function_version ();\n+      dispatcher_version_info->next = default_version_info;\n+      dispatcher_node->definition = 1;\n+\n+      /* Set the dispatcher for all the versions.  */\n+      it_v = default_version_info;\n+      while (it_v != NULL)\n+\t{\n+\t  it_v->dispatcher_resolver = dispatch_decl;\n+\t  it_v = it_v->next;\n+\t}\n+    }\n+  else\n+#endif\n+    {\n+      error_at (DECL_SOURCE_LOCATION (default_node->decl),\n+\t\t\"multiversioning needs ifunc which is not supported \"\n+\t\t\"on this target\");\n+    }\n+\n+  return dispatch_decl;\n+}\n+\n+/* Make the resolver function decl to dispatch the versions of\n+   a multi-versioned function,  DEFAULT_DECL.  IFUNC_ALIAS_DECL is\n+   ifunc alias that will point to the created resolver.  Create an\n+   empty basic block in the resolver and store the pointer in\n+   EMPTY_BB.  Return the decl of the resolver function.  */\n+\n+static tree\n+make_resolver_func (const tree default_decl,\n+\t\t    const tree ifunc_alias_decl,\n+\t\t    basic_block *empty_bb)\n+{\n+  char *resolver_name;\n+  tree decl, type, decl_name, t;\n+\n+  /* IFUNC's have to be globally visible.  So, if the default_decl is\n+     not, then the name of the IFUNC should be made unique.  */\n+  if (TREE_PUBLIC (default_decl) == 0)\n+    {\n+      char *ifunc_name = make_unique_name (default_decl, \"ifunc\", true);\n+      symtab->change_decl_assembler_name (ifunc_alias_decl,\n+\t\t\t\t\t  get_identifier (ifunc_name));\n+      XDELETEVEC (ifunc_name);\n+    }\n+\n+  resolver_name = make_unique_name (default_decl, \"resolver\", false);\n+\n+  /* The resolver function should return a (void *). */\n+  type = build_function_type_list (ptr_type_node, NULL_TREE);\n+\n+  decl = build_fn_decl (resolver_name, type);\n+  decl_name = get_identifier (resolver_name);\n+  SET_DECL_ASSEMBLER_NAME (decl, decl_name);\n+\n+  DECL_NAME (decl) = decl_name;\n+  TREE_USED (decl) = 1;\n+  DECL_ARTIFICIAL (decl) = 1;\n+  DECL_IGNORED_P (decl) = 1;\n+  TREE_PUBLIC (decl) = 0;\n+  DECL_UNINLINABLE (decl) = 1;\n+\n+  /* Resolver is not external, body is generated.  */\n+  DECL_EXTERNAL (decl) = 0;\n+  DECL_EXTERNAL (ifunc_alias_decl) = 0;\n+\n+  DECL_CONTEXT (decl) = NULL_TREE;\n+  DECL_INITIAL (decl) = make_node (BLOCK);\n+  DECL_STATIC_CONSTRUCTOR (decl) = 0;\n+\n+  if (DECL_COMDAT_GROUP (default_decl)\n+      || TREE_PUBLIC (default_decl))\n+    {\n+      /* In this case, each translation unit with a call to this\n+\t versioned function will put out a resolver.  Ensure it\n+\t is comdat to keep just one copy.  */\n+      DECL_COMDAT (decl) = 1;\n+      make_decl_one_only (decl, DECL_ASSEMBLER_NAME (decl));\n+    }\n+  /* Build result decl and add to function_decl. */\n+  t = build_decl (UNKNOWN_LOCATION, RESULT_DECL, NULL_TREE, ptr_type_node);\n+  DECL_CONTEXT (t) = decl;\n+  DECL_ARTIFICIAL (t) = 1;\n+  DECL_IGNORED_P (t) = 1;\n+  DECL_RESULT (decl) = t;\n+\n+  gimplify_function_tree (decl);\n+  push_cfun (DECL_STRUCT_FUNCTION (decl));\n+  *empty_bb = init_lowered_empty_function (decl, false,\n+\t\t\t\t\t   profile_count::uninitialized ());\n+\n+  cgraph_node::add_new_function (decl, true);\n+  symtab->call_cgraph_insertion_hooks (cgraph_node::get_create (decl));\n+\n+  pop_cfun ();\n+\n+  gcc_assert (ifunc_alias_decl != NULL);\n+  /* Mark ifunc_alias_decl as \"ifunc\" with resolver as resolver_name.  */\n+  DECL_ATTRIBUTES (ifunc_alias_decl)\n+    = make_attribute (\"ifunc\", resolver_name,\n+\t\t      DECL_ATTRIBUTES (ifunc_alias_decl));\n+\n+  /* Create the alias for dispatch to resolver here.  */\n+  cgraph_node::create_same_body_alias (ifunc_alias_decl, decl);\n+  XDELETEVEC (resolver_name);\n+  return decl;\n+}\n+\n+/* Generate the dispatching code body to dispatch multi-versioned function\n+   DECL.  The target hook is called to process the \"target\" attributes and\n+   provide the code to dispatch the right function at run-time.  NODE points\n+   to the dispatcher decl whose body will be created.  */\n+\n+tree \n+ix86_generate_version_dispatcher_body (void *node_p)\n+{\n+  tree resolver_decl;\n+  basic_block empty_bb;\n+  tree default_ver_decl;\n+  struct cgraph_node *versn;\n+  struct cgraph_node *node;\n+\n+  struct cgraph_function_version_info *node_version_info = NULL;\n+  struct cgraph_function_version_info *versn_info = NULL;\n+\n+  node = (cgraph_node *)node_p;\n+\n+  node_version_info = node->function_version ();\n+  gcc_assert (node->dispatcher_function\n+\t      && node_version_info != NULL);\n+\n+  if (node_version_info->dispatcher_resolver)\n+    return node_version_info->dispatcher_resolver;\n+\n+  /* The first version in the chain corresponds to the default version.  */\n+  default_ver_decl = node_version_info->next->this_node->decl;\n+\n+  /* node is going to be an alias, so remove the finalized bit.  */\n+  node->definition = false;\n+\n+  resolver_decl = make_resolver_func (default_ver_decl,\n+\t\t\t\t      node->decl, &empty_bb);\n+\n+  node_version_info->dispatcher_resolver = resolver_decl;\n+\n+  push_cfun (DECL_STRUCT_FUNCTION (resolver_decl));\n+\n+  auto_vec<tree, 2> fn_ver_vec;\n+\n+  for (versn_info = node_version_info->next; versn_info;\n+       versn_info = versn_info->next)\n+    {\n+      versn = versn_info->this_node;\n+      /* Check for virtual functions here again, as by this time it should\n+\t have been determined if this function needs a vtable index or\n+\t not.  This happens for methods in derived classes that override\n+\t virtual methods in base classes but are not explicitly marked as\n+\t virtual.  */\n+      if (DECL_VINDEX (versn->decl))\n+\tsorry (\"virtual function multiversioning not supported\");\n+\n+      fn_ver_vec.safe_push (versn->decl);\n+    }\n+\n+  dispatch_function_versions (resolver_decl, &fn_ver_vec, &empty_bb);\n+  cgraph_edge::rebuild_edges ();\n+  pop_cfun ();\n+  return resolver_decl;\n+}\n+\n+"}, {"sha": "358122249978dd4eaecf01db47fa71cece3fdb22", "filename": "gcc/config/i386/i386-features.h", "status": "added", "additions": 201, "deletions": 0, "changes": 201, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-features.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-features.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-features.h?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "patch": "@@ -0,0 +1,201 @@\n+/* Copyright (C) 1988-2019 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_I386_FEATURES_H\n+#define GCC_I386_FEATURES_H\n+\n+enum xlogue_stub {\n+  XLOGUE_STUB_SAVE,\n+  XLOGUE_STUB_RESTORE,\n+  XLOGUE_STUB_RESTORE_TAIL,\n+  XLOGUE_STUB_SAVE_HFP,\n+  XLOGUE_STUB_RESTORE_HFP,\n+  XLOGUE_STUB_RESTORE_HFP_TAIL,\n+\n+  XLOGUE_STUB_COUNT\n+};\n+\n+enum xlogue_stub_sets {\n+  XLOGUE_SET_ALIGNED,\n+  XLOGUE_SET_ALIGNED_PLUS_8,\n+  XLOGUE_SET_HFP_ALIGNED_OR_REALIGN,\n+  XLOGUE_SET_HFP_ALIGNED_PLUS_8,\n+\n+  XLOGUE_SET_COUNT\n+};\n+\n+/* Register save/restore layout used by out-of-line stubs.  */\n+class xlogue_layout {\n+public:\n+  struct reginfo\n+  {\n+    unsigned regno;\n+    HOST_WIDE_INT offset;\t/* Offset used by stub base pointer (rax or\n+\t\t\t\t   rsi) to where each register is stored.  */\n+  };\n+\n+  unsigned get_nregs () const\t\t\t{return m_nregs;}\n+  HOST_WIDE_INT get_stack_align_off_in () const\t{return m_stack_align_off_in;}\n+\n+  const reginfo &get_reginfo (unsigned reg) const\n+  {\n+    gcc_assert (reg < m_nregs);\n+    return m_regs[reg];\n+  }\n+\n+  static const char *get_stub_name (enum xlogue_stub stub,\n+\t\t\t\t    unsigned n_extra_args);\n+\n+  /* Returns an rtx for the stub's symbol based upon\n+       1.) the specified stub (save, restore or restore_ret) and\n+       2.) the value of cfun->machine->call_ms2sysv_extra_regs and\n+       3.) rather or not stack alignment is being performed.  */\n+  static rtx get_stub_rtx (enum xlogue_stub stub);\n+\n+  /* Returns the amount of stack space (including padding) that the stub\n+     needs to store registers based upon data in the machine_function.  */\n+  HOST_WIDE_INT get_stack_space_used () const\n+  {\n+    const struct machine_function *m = cfun->machine;\n+    unsigned last_reg = m->call_ms2sysv_extra_regs + MIN_REGS - 1;\n+\n+    gcc_assert (m->call_ms2sysv_extra_regs <= MAX_EXTRA_REGS);\n+    return m_regs[last_reg].offset + STUB_INDEX_OFFSET;\n+  }\n+\n+  /* Returns the offset for the base pointer used by the stub.  */\n+  HOST_WIDE_INT get_stub_ptr_offset () const\n+  {\n+    return STUB_INDEX_OFFSET + m_stack_align_off_in;\n+  }\n+\n+  static const struct xlogue_layout &get_instance ();\n+  static unsigned count_stub_managed_regs ();\n+  static bool is_stub_managed_reg (unsigned regno, unsigned count);\n+\n+  static const HOST_WIDE_INT STUB_INDEX_OFFSET = 0x70;\n+  static const unsigned MIN_REGS = NUM_X86_64_MS_CLOBBERED_REGS;\n+  static const unsigned MAX_REGS = 18;\n+  static const unsigned MAX_EXTRA_REGS = MAX_REGS - MIN_REGS;\n+  static const unsigned VARIANT_COUNT = MAX_EXTRA_REGS + 1;\n+  static const unsigned STUB_NAME_MAX_LEN = 20;\n+  static const char * const STUB_BASE_NAMES[XLOGUE_STUB_COUNT];\n+  static const unsigned REG_ORDER[MAX_REGS];\n+  static const unsigned REG_ORDER_REALIGN[MAX_REGS];\n+\n+private:\n+  xlogue_layout ();\n+  xlogue_layout (HOST_WIDE_INT stack_align_off_in, bool hfp);\n+  xlogue_layout (const xlogue_layout &);\n+\n+  /* True if hard frame pointer is used.  */\n+  bool m_hfp;\n+\n+  /* Max number of register this layout manages.  */\n+  unsigned m_nregs;\n+\n+  /* Incoming offset from 16-byte alignment.  */\n+  HOST_WIDE_INT m_stack_align_off_in;\n+\n+  /* Register order and offsets.  */\n+  struct reginfo m_regs[MAX_REGS];\n+\n+  /* Lazy-inited cache of symbol names for stubs.  */\n+  static char s_stub_names[2][XLOGUE_STUB_COUNT][VARIANT_COUNT]\n+\t\t\t  [STUB_NAME_MAX_LEN];\n+\n+  static const xlogue_layout s_instances[XLOGUE_SET_COUNT];\n+};\n+\n+namespace {\n+\n+class scalar_chain\n+{\n+ public:\n+  scalar_chain ();\n+  virtual ~scalar_chain ();\n+\n+  static unsigned max_id;\n+\n+  /* ID of a chain.  */\n+  unsigned int chain_id;\n+  /* A queue of instructions to be included into a chain.  */\n+  bitmap queue;\n+  /* Instructions included into a chain.  */\n+  bitmap insns;\n+  /* All registers defined by a chain.  */\n+  bitmap defs;\n+  /* Registers used in both vector and sclar modes.  */\n+  bitmap defs_conv;\n+\n+  void build (bitmap candidates, unsigned insn_uid);\n+  virtual int compute_convert_gain () = 0;\n+  int convert ();\n+\n+ protected:\n+  void add_to_queue (unsigned insn_uid);\n+  void emit_conversion_insns (rtx insns, rtx_insn *pos);\n+\n+ private:\n+  void add_insn (bitmap candidates, unsigned insn_uid);\n+  void analyze_register_chain (bitmap candidates, df_ref ref);\n+  virtual void mark_dual_mode_def (df_ref def) = 0;\n+  virtual void convert_insn (rtx_insn *insn) = 0;\n+  virtual void convert_registers () = 0;\n+};\n+\n+class dimode_scalar_chain : public scalar_chain\n+{\n+ public:\n+  int compute_convert_gain ();\n+ private:\n+  void mark_dual_mode_def (df_ref def);\n+  rtx replace_with_subreg (rtx x, rtx reg, rtx subreg);\n+  void replace_with_subreg_in_insn (rtx_insn *insn, rtx reg, rtx subreg);\n+  void convert_insn (rtx_insn *insn);\n+  void convert_op (rtx *op, rtx_insn *insn);\n+  void convert_reg (unsigned regno);\n+  void make_vector_copies (unsigned regno);\n+  void convert_registers ();\n+  int vector_const_cost (rtx exp);\n+};\n+\n+class timode_scalar_chain : public scalar_chain\n+{\n+ public:\n+  /* Convert from TImode to V1TImode is always faster.  */\n+  int compute_convert_gain () { return 1; }\n+\n+ private:\n+  void mark_dual_mode_def (df_ref def);\n+  void fix_debug_reg_uses (rtx reg);\n+  void convert_insn (rtx_insn *insn);\n+  /* We don't convert registers to difference size.  */\n+  void convert_registers () {}\n+};\n+\n+} // anon namespace\n+\n+bool ix86_save_reg (unsigned int regno, bool maybe_eh_return, bool ignore_outlined);\n+int ix86_compare_version_priority (tree decl1, tree decl2);\n+tree ix86_generate_version_dispatcher_body (void *node_p);\n+tree ix86_get_function_versions_dispatcher (void *decl);\n+tree ix86_mangle_decl_assembler_name (tree decl, tree id);\n+\n+\n+#endif  /* GCC_I386_FEATURES_H */"}, {"sha": "1a673d278eeac2ac151eaaff5eca32add607883c", "filename": "gcc/config/i386/i386-options.c", "status": "added", "additions": 3688, "deletions": 0, "changes": 3688, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-options.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-options.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-options.c?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4"}, {"sha": "817ddda5c221ae60030ca2e9e82966e97ffff80e", "filename": "gcc/config/i386/i386-options.h", "status": "added", "additions": 95, "deletions": 0, "changes": 95, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-options.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386-options.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-options.h?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "patch": "@@ -0,0 +1,95 @@\n+/* Copyright (C) 1988-2019 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_I386_OPTIONS_H\n+#define GCC_I386_OPTIONS_H\n+\n+char *ix86_target_string (HOST_WIDE_INT isa, HOST_WIDE_INT isa2,\n+\t\t\t  int flags, int flags2,\n+\t\t\t  const char *arch, const char *tune,\n+\t\t\t  enum fpmath_unit fpmath, bool add_nl_p,\n+\t\t\t  bool add_abi_p);\n+\n+extern enum attr_cpu ix86_schedule;\n+\n+extern enum processor_type ix86_tune;\n+extern enum processor_type ix86_arch;\n+extern unsigned char x86_prefetch_sse;\n+extern const struct processor_costs *ix86_tune_cost;\n+\n+extern int ix86_tune_defaulted;\n+extern int ix86_arch_specified;\n+\n+extern unsigned int ix86_default_incoming_stack_boundary;\n+extern HOST_WIDE_INT deferred_isa_values;\n+extern HOST_WIDE_INT deferred_isa_values2;\n+\n+extern unsigned int ix86_preferred_stack_boundary;\n+extern unsigned int ix86_user_incoming_stack_boundary;\n+extern unsigned int ix86_default_incoming_stack_boundary;\n+extern unsigned int ix86_incoming_stack_boundary;\n+\n+extern char *ix86_offload_options (void);\n+extern void ix86_option_override (void);\n+extern void ix86_override_options_after_change (void);\n+void ix86_set_current_function (tree fndecl);\n+bool ix86_function_naked (const_tree fn);\n+void ix86_simd_clone_adjust (struct cgraph_node *node);\n+\n+extern tree (*ix86_veclib_handler) (combined_fn, tree, tree);\n+extern tree ix86_veclibabi_svml (combined_fn, tree, tree);\n+extern tree ix86_veclibabi_acml (combined_fn, tree, tree);\n+\n+extern rtx (*ix86_gen_leave) (void);\n+extern rtx (*ix86_gen_add3) (rtx, rtx, rtx);\n+extern rtx (*ix86_gen_sub3) (rtx, rtx, rtx);\n+extern rtx (*ix86_gen_sub3_carry) (rtx, rtx, rtx, rtx, rtx);\n+extern rtx (*ix86_gen_one_cmpl2) (rtx, rtx);\n+extern rtx (*ix86_gen_monitor) (rtx, rtx, rtx);\n+extern rtx (*ix86_gen_monitorx) (rtx, rtx, rtx);\n+extern rtx (*ix86_gen_clzero) (rtx);\n+extern rtx (*ix86_gen_andsp) (rtx, rtx, rtx);\n+extern rtx (*ix86_gen_allocate_stack_worker) (rtx, rtx);\n+extern rtx (*ix86_gen_adjust_stack_and_probe) (rtx, rtx, rtx);\n+extern rtx (*ix86_gen_probe_stack_range) (rtx, rtx, rtx);\n+extern rtx (*ix86_gen_tls_global_dynamic_64) (rtx, rtx, rtx);\n+extern rtx (*ix86_gen_tls_local_dynamic_base_64) (rtx, rtx);\n+\n+enum ix86_function_specific_strings\n+{\n+  IX86_FUNCTION_SPECIFIC_ARCH,\n+  IX86_FUNCTION_SPECIFIC_TUNE,\n+  IX86_FUNCTION_SPECIFIC_MAX\n+};\n+\n+extern const char *stringop_alg_names[];\n+\n+void ix86_add_new_builtins (HOST_WIDE_INT isa, HOST_WIDE_INT isa2);\n+void ix86_function_specific_save (struct cl_target_option *,\n+\t\t\t\t  struct gcc_options *opts);\n+void ix86_function_specific_restore (struct gcc_options *opts,\n+\t\t\t\t     struct cl_target_option *);\n+void ix86_function_specific_post_stream_in (struct cl_target_option *);\n+void ix86_function_specific_print (FILE *, int,\n+\t\t\t\t   struct cl_target_option *);\n+bool ix86_valid_target_attribute_p (tree, tree, tree, int);\n+\n+extern const struct attribute_spec ix86_attribute_table[];\n+\n+\n+#endif  /* GCC_I386_OPTIONS_H */"}, {"sha": "2f23986a9ddff8f39301cee9b0ab2fc7563174fe", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 17099, "deletions": 46007, "changes": 63106, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4"}, {"sha": "ad6c36ba26532f2dc6502e06eaedb5b6b8152c97", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "patch": "@@ -2759,6 +2759,9 @@ struct GTY(()) machine_function {\n   /* During SEH output, this is non-null.  */\n   struct seh_frame_state * GTY((skip(\"\"))) seh;\n };\n+\n+extern GTY(()) tree sysv_va_list_type_node;\n+extern GTY(()) tree ms_va_list_type_node;\n #endif\n \n #define ix86_stack_locals (cfun->machine->stack_locals)\n@@ -2856,6 +2859,12 @@ extern void debug_dispatch_window (int);\n \n #define TARGET_SUPPORTS_WIDE_INT 1\n \n+#if !defined(GENERATOR_FILE) && !defined(IN_LIBGCC2)\n+extern enum attr_cpu ix86_schedule;\n+\n+#define NUM_X86_64_MS_CLOBBERED_REGS 12\n+#endif\n+\n /*\n Local variables:\n version-control: t"}, {"sha": "50caf2c69611de3c2187e459e96dbadd198e3b58", "filename": "gcc/config/i386/t-i386", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Ft-i386", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4/gcc%2Fconfig%2Fi386%2Ft-i386", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Ft-i386?ref=2bf6d93547e516b6b2b2051c0fb1b47ea4acc8a4", "patch": "@@ -44,6 +44,22 @@ i386-d.o: $(srcdir)/config/i386/i386-d.c\n \t$(COMPILE) $<\n \t$(POSTCOMPILE)\n \n+i386-options.o: $(srcdir)/config/i386/i386-options.c\n+\t$(COMPILE) $<\n+\t$(POSTCOMPILE)\n+\n+i386-builtins.o: $(srcdir)/config/i386/i386-builtins.c\n+\t$(COMPILE) $<\n+\t$(POSTCOMPILE)\n+\n+i386-expand.o: $(srcdir)/config/i386/i386-expand.c\n+\t$(COMPILE) $<\n+\t$(POSTCOMPILE)\n+\n+i386-features.o: $(srcdir)/config/i386/i386-features.c\n+\t$(COMPILE) $<\n+\t$(POSTCOMPILE)\n+\n i386.o: i386-builtin-types.inc\n \n i386-builtin-types.inc: s-i386-bt ; @true"}]}