{"sha": "f732bf6a603721f61102a08ad2d023c7c2670870", "node_id": "C_kwDOANBUbNoAKGY3MzJiZjZhNjAzNzIxZjYxMTAyYTA4YWQyZDAyM2M3YzI2NzA4NzA", "commit": {"author": {"name": "Martin Liska", "email": "mliska@suse.cz", "date": "2022-05-03T10:56:26Z"}, "committer": {"name": "Martin Liska", "email": "mliska@suse.cz", "date": "2022-05-04T09:00:48Z"}, "message": "libsanitizer: merge from upstream (0a1bcab9f3bf75c4c5d3e53bafb3eeb80320af46).", "tree": {"sha": "d0d8dafedac59ab6d55b678e53afe19fdd113fba", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d0d8dafedac59ab6d55b678e53afe19fdd113fba"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f732bf6a603721f61102a08ad2d023c7c2670870", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f732bf6a603721f61102a08ad2d023c7c2670870", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f732bf6a603721f61102a08ad2d023c7c2670870", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f732bf6a603721f61102a08ad2d023c7c2670870/comments", "author": {"login": "marxin", "id": 2658545, "node_id": "MDQ6VXNlcjI2NTg1NDU=", "avatar_url": "https://avatars.githubusercontent.com/u/2658545?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marxin", "html_url": "https://github.com/marxin", "followers_url": "https://api.github.com/users/marxin/followers", "following_url": "https://api.github.com/users/marxin/following{/other_user}", "gists_url": "https://api.github.com/users/marxin/gists{/gist_id}", "starred_url": "https://api.github.com/users/marxin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marxin/subscriptions", "organizations_url": "https://api.github.com/users/marxin/orgs", "repos_url": "https://api.github.com/users/marxin/repos", "events_url": "https://api.github.com/users/marxin/events{/privacy}", "received_events_url": "https://api.github.com/users/marxin/received_events", "type": "User", "site_admin": false}, "committer": {"login": "marxin", "id": 2658545, "node_id": "MDQ6VXNlcjI2NTg1NDU=", "avatar_url": "https://avatars.githubusercontent.com/u/2658545?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marxin", "html_url": "https://github.com/marxin", "followers_url": "https://api.github.com/users/marxin/followers", "following_url": "https://api.github.com/users/marxin/following{/other_user}", "gists_url": "https://api.github.com/users/marxin/gists{/gist_id}", "starred_url": "https://api.github.com/users/marxin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marxin/subscriptions", "organizations_url": "https://api.github.com/users/marxin/orgs", "repos_url": "https://api.github.com/users/marxin/repos", "events_url": "https://api.github.com/users/marxin/events{/privacy}", "received_events_url": "https://api.github.com/users/marxin/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "e2285af309000b74da0f7dc756a0b55e5f0b1b56", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e2285af309000b74da0f7dc756a0b55e5f0b1b56", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e2285af309000b74da0f7dc756a0b55e5f0b1b56"}], "stats": {"total": 12342, "additions": 6955, "deletions": 5387}, "files": [{"sha": "b92d082f9ae428572f47a0685f61da7dcfce52c8", "filename": "libsanitizer/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2FMERGE?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -1,4 +1,4 @@\n-82bc6a094e85014f1891ef9407496f44af8fe442\n+0a1bcab9f3bf75c4c5d3e53bafb3eeb80320af46\n \n The first line of this file holds the git revision number of the\n last merge done from the master library sources."}, {"sha": "1757838600ca6e556be955bcc44721e395b293ad", "filename": "libsanitizer/asan/asan_activation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_activation.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_activation.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_activation.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -112,7 +112,7 @@ void AsanDeactivate() {\n   disabled.quarantine_size_mb = 0;\n   disabled.thread_local_quarantine_size_kb = 0;\n   // Redzone must be at least Max(16, granularity) bytes long.\n-  disabled.min_redzone = Max(16, (int)SHADOW_GRANULARITY);\n+  disabled.min_redzone = Max(16, (int)ASAN_SHADOW_GRANULARITY);\n   disabled.max_redzone = disabled.min_redzone;\n   disabled.alloc_dealloc_mismatch = false;\n   disabled.may_return_null = true;"}, {"sha": "7b7a289c2d25ac5a48c741d0d5e5318a9bcd49bc", "filename": "libsanitizer/asan/asan_allocator.cpp", "status": "modified", "additions": 15, "deletions": 41, "changes": 56, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_allocator.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_allocator.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_allocator.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -210,8 +210,7 @@ struct QuarantineCallback {\n       CHECK_EQ(old_chunk_state, CHUNK_QUARANTINE);\n     }\n \n-    PoisonShadow(m->Beg(),\n-                 RoundUpTo(m->UsedSize(), SHADOW_GRANULARITY),\n+    PoisonShadow(m->Beg(), RoundUpTo(m->UsedSize(), ASAN_SHADOW_GRANULARITY),\n                  kAsanHeapLeftRedzoneMagic);\n \n     // Statistics.\n@@ -305,7 +304,6 @@ struct Allocator {\n   QuarantineCache fallback_quarantine_cache;\n \n   uptr max_user_defined_malloc_size;\n-  atomic_uint8_t rss_limit_exceeded;\n \n   // ------------------- Options --------------------------\n   atomic_uint16_t min_redzone;\n@@ -345,14 +343,6 @@ struct Allocator {\n                                        : kMaxAllowedMallocSize;\n   }\n \n-  bool RssLimitExceeded() {\n-    return atomic_load(&rss_limit_exceeded, memory_order_relaxed);\n-  }\n-\n-  void SetRssLimitExceeded(bool limit_exceeded) {\n-    atomic_store(&rss_limit_exceeded, limit_exceeded, memory_order_relaxed);\n-  }\n-\n   void RePoisonChunk(uptr chunk) {\n     // This could be a user-facing chunk (with redzones), or some internal\n     // housekeeping chunk, like TransferBatch. Start by assuming the former.\n@@ -366,7 +356,7 @@ struct Allocator {\n       if (chunk < beg && beg < end && end <= chunk_end) {\n         // Looks like a valid AsanChunk in use, poison redzones only.\n         PoisonShadow(chunk, beg - chunk, kAsanHeapLeftRedzoneMagic);\n-        uptr end_aligned_down = RoundDownTo(end, SHADOW_GRANULARITY);\n+        uptr end_aligned_down = RoundDownTo(end, ASAN_SHADOW_GRANULARITY);\n         FastPoisonShadowPartialRightRedzone(\n             end_aligned_down, end - end_aligned_down,\n             chunk_end - end_aligned_down, kAsanHeapLeftRedzoneMagic);\n@@ -484,14 +474,14 @@ struct Allocator {\n                  AllocType alloc_type, bool can_fill) {\n     if (UNLIKELY(!asan_inited))\n       AsanInitFromRtl();\n-    if (RssLimitExceeded()) {\n+    if (UNLIKELY(IsRssLimitExceeded())) {\n       if (AllocatorMayReturnNull())\n         return nullptr;\n       ReportRssLimitExceeded(stack);\n     }\n     Flags &fl = *flags();\n     CHECK(stack);\n-    const uptr min_alignment = SHADOW_GRANULARITY;\n+    const uptr min_alignment = ASAN_SHADOW_GRANULARITY;\n     const uptr user_requested_alignment_log =\n         ComputeUserRequestedAlignmentLog(alignment);\n     if (alignment < min_alignment)\n@@ -572,15 +562,15 @@ struct Allocator {\n     m->SetAllocContext(t ? t->tid() : kMainTid, StackDepotPut(*stack));\n \n     uptr size_rounded_down_to_granularity =\n-        RoundDownTo(size, SHADOW_GRANULARITY);\n+        RoundDownTo(size, ASAN_SHADOW_GRANULARITY);\n     // Unpoison the bulk of the memory region.\n     if (size_rounded_down_to_granularity)\n       PoisonShadow(user_beg, size_rounded_down_to_granularity, 0);\n     // Deal with the end of the region if size is not aligned to granularity.\n     if (size != size_rounded_down_to_granularity && CanPoisonMemory()) {\n       u8 *shadow =\n           (u8 *)MemToShadow(user_beg + size_rounded_down_to_granularity);\n-      *shadow = fl.poison_partial ? (size & (SHADOW_GRANULARITY - 1)) : 0;\n+      *shadow = fl.poison_partial ? (size & (ASAN_SHADOW_GRANULARITY - 1)) : 0;\n     }\n \n     AsanStats &thread_stats = GetCurrentThreadStats();\n@@ -607,7 +597,7 @@ struct Allocator {\n       CHECK_LE(alloc_beg + sizeof(LargeChunkHeader), chunk_beg);\n       reinterpret_cast<LargeChunkHeader *>(alloc_beg)->Set(m);\n     }\n-    ASAN_MALLOC_HOOK(res, size);\n+    RunMallocHooks(res, size);\n     return res;\n   }\n \n@@ -650,8 +640,7 @@ struct Allocator {\n     }\n \n     // Poison the region.\n-    PoisonShadow(m->Beg(),\n-                 RoundUpTo(m->UsedSize(), SHADOW_GRANULARITY),\n+    PoisonShadow(m->Beg(), RoundUpTo(m->UsedSize(), ASAN_SHADOW_GRANULARITY),\n                  kAsanHeapFreeMagic);\n \n     AsanStats &thread_stats = GetCurrentThreadStats();\n@@ -689,7 +678,7 @@ struct Allocator {\n       return;\n     }\n \n-    ASAN_FREE_HOOK(ptr);\n+    RunFreeHooks(ptr);\n \n     // Must mark the chunk as quarantined before any changes to its metadata.\n     // Do not quarantine given chunk if we failed to set CHUNK_QUARANTINE flag.\n@@ -851,12 +840,12 @@ struct Allocator {\n     quarantine.PrintStats();\n   }\n \n-  void ForceLock() ACQUIRE(fallback_mutex) {\n+  void ForceLock() SANITIZER_ACQUIRE(fallback_mutex) {\n     allocator.ForceLock();\n     fallback_mutex.Lock();\n   }\n \n-  void ForceUnlock() RELEASE(fallback_mutex) {\n+  void ForceUnlock() SANITIZER_RELEASE(fallback_mutex) {\n     fallback_mutex.Unlock();\n     allocator.ForceUnlock();\n   }\n@@ -1065,14 +1054,12 @@ uptr asan_mz_size(const void *ptr) {\n   return instance.AllocationSize(reinterpret_cast<uptr>(ptr));\n }\n \n-void asan_mz_force_lock() NO_THREAD_SAFETY_ANALYSIS { instance.ForceLock(); }\n-\n-void asan_mz_force_unlock() NO_THREAD_SAFETY_ANALYSIS {\n-  instance.ForceUnlock();\n+void asan_mz_force_lock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  instance.ForceLock();\n }\n \n-void AsanSoftRssLimitExceededCallback(bool limit_exceeded) {\n-  instance.SetRssLimitExceeded(limit_exceeded);\n+void asan_mz_force_unlock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  instance.ForceUnlock();\n }\n \n }  // namespace __asan\n@@ -1230,16 +1217,3 @@ int __asan_update_allocation_context(void* addr) {\n   GET_STACK_TRACE_MALLOC;\n   return instance.UpdateAllocationStack((uptr)addr, &stack);\n }\n-\n-#if !SANITIZER_SUPPORTS_WEAK_HOOKS\n-// Provide default (no-op) implementation of malloc hooks.\n-SANITIZER_INTERFACE_WEAK_DEF(void, __sanitizer_malloc_hook,\n-                             void *ptr, uptr size) {\n-  (void)ptr;\n-  (void)size;\n-}\n-\n-SANITIZER_INTERFACE_WEAK_DEF(void, __sanitizer_free_hook, void *ptr) {\n-  (void)ptr;\n-}\n-#endif"}, {"sha": "f078f1041a87f059f20a9d61be8285378eed2b0e", "filename": "libsanitizer/asan/asan_debugging.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_debugging.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_debugging.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_debugging.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -141,7 +141,7 @@ uptr __asan_get_free_stack(uptr addr, uptr *trace, uptr size, u32 *thread_id) {\n SANITIZER_INTERFACE_ATTRIBUTE\n void __asan_get_shadow_mapping(uptr *shadow_scale, uptr *shadow_offset) {\n   if (shadow_scale)\n-    *shadow_scale = SHADOW_SCALE;\n+    *shadow_scale = ASAN_SHADOW_SCALE;\n   if (shadow_offset)\n-    *shadow_offset = SHADOW_OFFSET;\n+    *shadow_offset = ASAN_SHADOW_OFFSET;\n }"}, {"sha": "a22bf130d8233b753ee8d2130e99b5f574e64651", "filename": "libsanitizer/asan/asan_errors.cpp", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_errors.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_errors.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_errors.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -329,7 +329,7 @@ void ErrorBadParamsToAnnotateContiguousContainer::Print() {\n       \"      old_mid : %p\\n\"\n       \"      new_mid : %p\\n\",\n       (void *)beg, (void *)end, (void *)old_mid, (void *)new_mid);\n-  uptr granularity = SHADOW_GRANULARITY;\n+  uptr granularity = ASAN_SHADOW_GRANULARITY;\n   if (!IsAligned(beg, granularity))\n     Report(\"ERROR: beg is not aligned by %zu\\n\", granularity);\n   stack->Print();\n@@ -410,7 +410,8 @@ ErrorGeneric::ErrorGeneric(u32 tid, uptr pc_, uptr bp_, uptr sp_, uptr addr,\n     if (AddrIsInMem(addr)) {\n       u8 *shadow_addr = (u8 *)MemToShadow(addr);\n       // If we are accessing 16 bytes, look at the second shadow byte.\n-      if (*shadow_addr == 0 && access_size > SHADOW_GRANULARITY) shadow_addr++;\n+      if (*shadow_addr == 0 && access_size > ASAN_SHADOW_GRANULARITY)\n+        shadow_addr++;\n       // If we are in the partial right redzone, look at the next shadow byte.\n       if (*shadow_addr > 0 && *shadow_addr < 128) shadow_addr++;\n       bool far_from_bounds = false;\n@@ -501,10 +502,11 @@ static void PrintLegend(InternalScopedString *str) {\n   str->append(\n       \"Shadow byte legend (one shadow byte represents %d \"\n       \"application bytes):\\n\",\n-      (int)SHADOW_GRANULARITY);\n+      (int)ASAN_SHADOW_GRANULARITY);\n   PrintShadowByte(str, \"  Addressable:           \", 0);\n   str->append(\"  Partially addressable: \");\n-  for (u8 i = 1; i < SHADOW_GRANULARITY; i++) PrintShadowByte(str, \"\", i, \" \");\n+  for (u8 i = 1; i < ASAN_SHADOW_GRANULARITY; i++)\n+    PrintShadowByte(str, \"\", i, \" \");\n   str->append(\"\\n\");\n   PrintShadowByte(str, \"  Heap left redzone:       \",\n                   kAsanHeapLeftRedzoneMagic);"}, {"sha": "c6ac88f6dc2ae4204b8fa4b6b409ffe452aa30bb", "filename": "libsanitizer/asan/asan_errors.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_errors.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_errors.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_errors.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -53,9 +53,9 @@ struct ErrorDeadlySignal : ErrorBase {\n       scariness.Scare(10, \"null-deref\");\n     } else if (signal.addr == signal.pc) {\n       scariness.Scare(60, \"wild-jump\");\n-    } else if (signal.write_flag == SignalContext::WRITE) {\n+    } else if (signal.write_flag == SignalContext::Write) {\n       scariness.Scare(30, \"wild-addr-write\");\n-    } else if (signal.write_flag == SignalContext::READ) {\n+    } else if (signal.write_flag == SignalContext::Read) {\n       scariness.Scare(20, \"wild-addr-read\");\n     } else {\n       scariness.Scare(25, \"wild-addr\");\n@@ -372,7 +372,7 @@ struct ErrorGeneric : ErrorBase {\n   u8 shadow_val;\n \n   ErrorGeneric() = default;  // (*)\n-  ErrorGeneric(u32 tid, uptr addr, uptr pc_, uptr bp_, uptr sp_, bool is_write_,\n+  ErrorGeneric(u32 tid, uptr pc_, uptr bp_, uptr sp_, uptr addr, bool is_write_,\n                uptr access_size_);\n   void Print();\n };"}, {"sha": "74a039b6579856974d2cc359d28977e136cad6d2", "filename": "libsanitizer/asan/asan_fake_stack.cpp", "status": "modified", "additions": 7, "deletions": 8, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_fake_stack.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_fake_stack.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_fake_stack.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -28,8 +28,8 @@ static const u64 kAllocaRedzoneMask = 31UL;\n // For small size classes inline PoisonShadow for better performance.\n ALWAYS_INLINE void SetShadow(uptr ptr, uptr size, uptr class_id, u64 magic) {\n   u64 *shadow = reinterpret_cast<u64*>(MemToShadow(ptr));\n-  if (SHADOW_SCALE == 3 && class_id <= 6) {\n-    // This code expects SHADOW_SCALE=3.\n+  if (ASAN_SHADOW_SCALE == 3 && class_id <= 6) {\n+    // This code expects ASAN_SHADOW_SCALE=3.\n     for (uptr i = 0; i < (((uptr)1) << class_id); i++) {\n       shadow[i] = magic;\n       // Make sure this does not become memset.\n@@ -140,7 +140,6 @@ void FakeStack::HandleNoReturn() {\n // We do it based on their 'real_stack' values -- everything that is lower\n // than the current real_stack is garbage.\n NOINLINE void FakeStack::GC(uptr real_stack) {\n-  uptr collected = 0;\n   for (uptr class_id = 0; class_id < kNumberOfSizeClasses; class_id++) {\n     u8 *flags = GetFlags(stack_size_log(), class_id);\n     for (uptr i = 0, n = NumberOfFrames(stack_size_log(), class_id); i < n;\n@@ -150,7 +149,6 @@ NOINLINE void FakeStack::GC(uptr real_stack) {\n           GetFrame(stack_size_log(), class_id, i));\n       if (ff->real_stack < real_stack) {\n         flags[i] = 0;\n-        collected++;\n       }\n     }\n   }\n@@ -294,18 +292,19 @@ void __asan_alloca_poison(uptr addr, uptr size) {\n   uptr LeftRedzoneAddr = addr - kAllocaRedzoneSize;\n   uptr PartialRzAddr = addr + size;\n   uptr RightRzAddr = (PartialRzAddr + kAllocaRedzoneMask) & ~kAllocaRedzoneMask;\n-  uptr PartialRzAligned = PartialRzAddr & ~(SHADOW_GRANULARITY - 1);\n+  uptr PartialRzAligned = PartialRzAddr & ~(ASAN_SHADOW_GRANULARITY - 1);\n   FastPoisonShadow(LeftRedzoneAddr, kAllocaRedzoneSize, kAsanAllocaLeftMagic);\n   FastPoisonShadowPartialRightRedzone(\n-      PartialRzAligned, PartialRzAddr % SHADOW_GRANULARITY,\n+      PartialRzAligned, PartialRzAddr % ASAN_SHADOW_GRANULARITY,\n       RightRzAddr - PartialRzAligned, kAsanAllocaRightMagic);\n   FastPoisonShadow(RightRzAddr, kAllocaRedzoneSize, kAsanAllocaRightMagic);\n }\n \n SANITIZER_INTERFACE_ATTRIBUTE\n void __asan_allocas_unpoison(uptr top, uptr bottom) {\n   if ((!top) || (top > bottom)) return;\n-  REAL(memset)(reinterpret_cast<void*>(MemToShadow(top)), 0,\n-               (bottom - top) / SHADOW_GRANULARITY);\n+  REAL(memset)\n+  (reinterpret_cast<void *>(MemToShadow(top)), 0,\n+   (bottom - top) / ASAN_SHADOW_GRANULARITY);\n }\n } // extern \"C\""}, {"sha": "9ea899f84b4b7a59821a22b65c7a60ddf834c7e4", "filename": "libsanitizer/asan/asan_flags.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_flags.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_flags.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_flags.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -140,9 +140,9 @@ void InitializeFlags() {\n            SanitizerToolName);\n     Die();\n   }\n-  // Ensure that redzone is at least SHADOW_GRANULARITY.\n-  if (f->redzone < (int)SHADOW_GRANULARITY)\n-    f->redzone = SHADOW_GRANULARITY;\n+  // Ensure that redzone is at least ASAN_SHADOW_GRANULARITY.\n+  if (f->redzone < (int)ASAN_SHADOW_GRANULARITY)\n+    f->redzone = ASAN_SHADOW_GRANULARITY;\n   // Make \"strict_init_order\" imply \"check_initialization_order\".\n   // TODO(samsonov): Use a single runtime flag for an init-order checker.\n   if (f->strict_init_order) {"}, {"sha": "314ed19353586ec38b3f93fb9e88d8860e7addd6", "filename": "libsanitizer/asan/asan_flags.inc", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_flags.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_flags.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_flags.inc?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -49,9 +49,10 @@ ASAN_FLAG(\n     \"to find more errors.\")\n ASAN_FLAG(bool, replace_intrin, true,\n           \"If set, uses custom wrappers for memset/memcpy/memmove intrinsics.\")\n-ASAN_FLAG(bool, detect_stack_use_after_return, false,\n+ASAN_FLAG(bool, detect_stack_use_after_return,\n+          SANITIZER_LINUX && !SANITIZER_ANDROID,\n           \"Enables stack-use-after-return checking at run-time.\")\n-ASAN_FLAG(int, min_uar_stack_size_log, 16, // We can't do smaller anyway.\n+ASAN_FLAG(int, min_uar_stack_size_log, 16,  // We can't do smaller anyway.\n           \"Minimum fake stack size log.\")\n ASAN_FLAG(int, max_uar_stack_size_log,\n           20, // 1Mb per size class, i.e. ~11Mb per thread"}, {"sha": "2b15504123bee7c95908ffb7dc0f99290d469327", "filename": "libsanitizer/asan/asan_fuchsia.cpp", "status": "modified", "additions": 20, "deletions": 12, "changes": 32, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_fuchsia.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_fuchsia.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_fuchsia.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -14,16 +14,17 @@\n #include \"sanitizer_common/sanitizer_fuchsia.h\"\n #if SANITIZER_FUCHSIA\n \n-#include \"asan_interceptors.h\"\n-#include \"asan_internal.h\"\n-#include \"asan_stack.h\"\n-#include \"asan_thread.h\"\n-\n #include <limits.h>\n #include <zircon/sanitizer.h>\n #include <zircon/syscalls.h>\n #include <zircon/threads.h>\n \n+#  include \"asan_interceptors.h\"\n+#  include \"asan_internal.h\"\n+#  include \"asan_stack.h\"\n+#  include \"asan_thread.h\"\n+#  include \"lsan/lsan_common.h\"\n+\n namespace __asan {\n \n // The system already set up the shadow memory for us.\n@@ -118,14 +119,12 @@ struct AsanThread::InitOptions {\n \n // Shared setup between thread creation and startup for the initial thread.\n static AsanThread *CreateAsanThread(StackTrace *stack, u32 parent_tid,\n-                                    uptr user_id, bool detached,\n-                                    const char *name) {\n+                                    bool detached, const char *name) {\n   // In lieu of AsanThread::Create.\n   AsanThread *thread = (AsanThread *)MmapOrDie(AsanThreadMmapSize(), __func__);\n \n   AsanThreadContext::CreateThreadContextArgs args = {thread, stack};\n-  u32 tid =\n-      asanThreadRegistry().CreateThread(user_id, detached, parent_tid, &args);\n+  u32 tid = asanThreadRegistry().CreateThread(0, detached, parent_tid, &args);\n   asanThreadRegistry().SetThreadName(tid, name);\n \n   return thread;\n@@ -152,7 +151,7 @@ AsanThread *CreateMainThread() {\n   CHECK_NE(__sanitizer::MainThreadStackBase, 0);\n   CHECK_GT(__sanitizer::MainThreadStackSize, 0);\n   AsanThread *t = CreateAsanThread(\n-      nullptr, 0, reinterpret_cast<uptr>(self), true,\n+      nullptr, 0, true,\n       _zx_object_get_property(thrd_get_zx_handle(self), ZX_PROP_NAME, name,\n                               sizeof(name)) == ZX_OK\n           ? name\n@@ -182,8 +181,7 @@ static void *BeforeThreadCreateHook(uptr user_id, bool detached,\n   GET_STACK_TRACE_THREAD;\n   u32 parent_tid = GetCurrentTidOrInvalid();\n \n-  AsanThread *thread =\n-      CreateAsanThread(&stack, parent_tid, user_id, detached, name);\n+  AsanThread *thread = CreateAsanThread(&stack, parent_tid, detached, name);\n \n   // On other systems, AsanThread::Init() is called from the new\n   // thread itself.  But on Fuchsia we already know the stack address\n@@ -238,8 +236,18 @@ void FlushUnneededASanShadowMemory(uptr p, uptr size) {\n   __sanitizer_fill_shadow(p, size, 0, 0);\n }\n \n+// On Fuchsia, leak detection is done by a special hook after atexit hooks.\n+// So this doesn't install any atexit hook like on other platforms.\n+void InstallAtExitCheckLeaks() {}\n+\n }  // namespace __asan\n \n+namespace __lsan {\n+\n+bool UseExitcodeOnLeak() { return __asan::flags()->halt_on_error; }\n+\n+}  // namespace __lsan\n+\n // These are declared (in extern \"C\") by <zircon/sanitizer.h>.\n // The system runtime will call our definitions directly.\n "}, {"sha": "ecc2600f039a11ec738fffad9f3307e2187b97b2", "filename": "libsanitizer/asan/asan_globals.cpp", "status": "modified", "additions": 22, "deletions": 4, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_globals.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_globals.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_globals.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -61,14 +61,13 @@ ALWAYS_INLINE void PoisonShadowForGlobal(const Global *g, u8 value) {\n }\n \n ALWAYS_INLINE void PoisonRedZones(const Global &g) {\n-  uptr aligned_size = RoundUpTo(g.size, SHADOW_GRANULARITY);\n+  uptr aligned_size = RoundUpTo(g.size, ASAN_SHADOW_GRANULARITY);\n   FastPoisonShadow(g.beg + aligned_size, g.size_with_redzone - aligned_size,\n                    kAsanGlobalRedzoneMagic);\n   if (g.size != aligned_size) {\n     FastPoisonShadowPartialRightRedzone(\n-        g.beg + RoundDownTo(g.size, SHADOW_GRANULARITY),\n-        g.size % SHADOW_GRANULARITY,\n-        SHADOW_GRANULARITY,\n+        g.beg + RoundDownTo(g.size, ASAN_SHADOW_GRANULARITY),\n+        g.size % ASAN_SHADOW_GRANULARITY, ASAN_SHADOW_GRANULARITY,\n         kAsanGlobalRedzoneMagic);\n   }\n }\n@@ -154,6 +153,23 @@ static void CheckODRViolationViaIndicator(const Global *g) {\n   }\n }\n \n+// Check ODR violation for given global G by checking if it's already poisoned.\n+// We use this method in case compiler doesn't use private aliases for global\n+// variables.\n+static void CheckODRViolationViaPoisoning(const Global *g) {\n+  if (__asan_region_is_poisoned(g->beg, g->size_with_redzone)) {\n+    // This check may not be enough: if the first global is much larger\n+    // the entire redzone of the second global may be within the first global.\n+    for (ListOfGlobals *l = list_of_all_globals; l; l = l->next) {\n+      if (g->beg == l->g->beg &&\n+          (flags()->detect_odr_violation >= 2 || g->size != l->g->size) &&\n+          !IsODRViolationSuppressed(g->name))\n+        ReportODRViolation(g, FindRegistrationSite(g),\n+                           l->g, FindRegistrationSite(l->g));\n+    }\n+  }\n+}\n+\n // Clang provides two different ways for global variables protection:\n // it can poison the global itself or its private alias. In former\n // case we may poison same symbol multiple times, that can help us to\n@@ -199,6 +215,8 @@ static void RegisterGlobal(const Global *g) {\n     // where two globals with the same name are defined in different modules.\n     if (UseODRIndicator(g))\n       CheckODRViolationViaIndicator(g);\n+    else\n+      CheckODRViolationViaPoisoning(g);\n   }\n   if (CanPoisonMemory())\n     PoisonRedZones(*g);"}, {"sha": "2ff314a5a9cbdd1031d1ad809a23a60981568bcf", "filename": "libsanitizer/asan/asan_interceptors.cpp", "status": "modified", "additions": 18, "deletions": 17, "changes": 35, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_interceptors.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_interceptors.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_interceptors.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -130,23 +130,24 @@ DECLARE_REAL_AND_INTERCEPTOR(void, free, void *)\n #define COMMON_INTERCEPTOR_BLOCK_REAL(name) REAL(name)\n // Strict init-order checking is dlopen-hostile:\n // https://github.com/google/sanitizers/issues/178\n-#define COMMON_INTERCEPTOR_ON_DLOPEN(filename, flag)                           \\\n-  do {                                                                         \\\n-    if (flags()->strict_init_order)                                            \\\n-      StopInitOrderChecking();                                                 \\\n-    CheckNoDeepBind(filename, flag);                                           \\\n-  } while (false)\n-#define COMMON_INTERCEPTOR_ON_EXIT(ctx) OnExit()\n-#define COMMON_INTERCEPTOR_LIBRARY_LOADED(filename, handle)\n-#define COMMON_INTERCEPTOR_LIBRARY_UNLOADED()\n-#define COMMON_INTERCEPTOR_NOTHING_IS_INITIALIZED (!asan_inited)\n-#define COMMON_INTERCEPTOR_GET_TLS_RANGE(begin, end)                           \\\n-  if (AsanThread *t = GetCurrentThread()) {                                    \\\n-    *begin = t->tls_begin();                                                   \\\n-    *end = t->tls_end();                                                       \\\n-  } else {                                                                     \\\n-    *begin = *end = 0;                                                         \\\n-  }\n+#  define COMMON_INTERCEPTOR_DLOPEN(filename, flag) \\\n+    ({                                              \\\n+      if (flags()->strict_init_order)               \\\n+        StopInitOrderChecking();                    \\\n+      CheckNoDeepBind(filename, flag);              \\\n+      REAL(dlopen)(filename, flag);                 \\\n+    })\n+#  define COMMON_INTERCEPTOR_ON_EXIT(ctx) OnExit()\n+#  define COMMON_INTERCEPTOR_LIBRARY_LOADED(filename, handle)\n+#  define COMMON_INTERCEPTOR_LIBRARY_UNLOADED()\n+#  define COMMON_INTERCEPTOR_NOTHING_IS_INITIALIZED (!asan_inited)\n+#  define COMMON_INTERCEPTOR_GET_TLS_RANGE(begin, end) \\\n+    if (AsanThread *t = GetCurrentThread()) {          \\\n+      *begin = t->tls_begin();                         \\\n+      *end = t->tls_end();                             \\\n+    } else {                                           \\\n+      *begin = *end = 0;                               \\\n+    }\n \n #define COMMON_INTERCEPTOR_MEMMOVE_IMPL(ctx, to, from, size) \\\n   do {                                                       \\"}, {"sha": "047b044c8bf47da51e74436dc088b2740fd162dc", "filename": "libsanitizer/asan/asan_interceptors.h", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_interceptors.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_interceptors.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_interceptors.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -81,12 +81,7 @@ void InitializePlatformInterceptors();\n #if ASAN_HAS_EXCEPTIONS && !SANITIZER_WINDOWS && !SANITIZER_SOLARIS && \\\n     !SANITIZER_NETBSD\n # define ASAN_INTERCEPT___CXA_THROW 1\n-# if ! defined(ASAN_HAS_CXA_RETHROW_PRIMARY_EXCEPTION) \\\n-     || ASAN_HAS_CXA_RETHROW_PRIMARY_EXCEPTION\n-#   define ASAN_INTERCEPT___CXA_RETHROW_PRIMARY_EXCEPTION 1\n-# else\n-#   define ASAN_INTERCEPT___CXA_RETHROW_PRIMARY_EXCEPTION 0\n-# endif\n+# define ASAN_INTERCEPT___CXA_RETHROW_PRIMARY_EXCEPTION 1\n # if defined(_GLIBCXX_SJLJ_EXCEPTIONS) || (SANITIZER_IOS && defined(__arm__))\n #  define ASAN_INTERCEPT__UNWIND_SJLJ_RAISEEXCEPTION 1\n # else"}, {"sha": "89ef552b7117367406c6526ee11db32162f5c046", "filename": "libsanitizer/asan/asan_interface.inc", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_interface.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_interface.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_interface.inc?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -7,6 +7,7 @@\n //===----------------------------------------------------------------------===//\n // Asan interface list.\n //===----------------------------------------------------------------------===//\n+\n INTERFACE_FUNCTION(__asan_addr_is_in_fake_stack)\n INTERFACE_FUNCTION(__asan_address_is_poisoned)\n INTERFACE_FUNCTION(__asan_after_dynamic_init)"}, {"sha": "7468f126d37b1a28e8d981679ab95e8f56ce5ff3", "filename": "libsanitizer/asan/asan_internal.h", "status": "modified", "additions": 15, "deletions": 23, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_internal.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_internal.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_internal.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -17,19 +17,19 @@\n #include \"asan_interface_internal.h\"\n #include \"sanitizer_common/sanitizer_common.h\"\n #include \"sanitizer_common/sanitizer_internal_defs.h\"\n-#include \"sanitizer_common/sanitizer_stacktrace.h\"\n #include \"sanitizer_common/sanitizer_libc.h\"\n+#include \"sanitizer_common/sanitizer_stacktrace.h\"\n \n #if __has_feature(address_sanitizer) || defined(__SANITIZE_ADDRESS__)\n-# error \"The AddressSanitizer run-time should not be\"\n-        \" instrumented by AddressSanitizer\"\n+#  error \\\n+      \"The AddressSanitizer run-time should not be instrumented by AddressSanitizer\"\n #endif\n \n // Build-time configuration options.\n \n // If set, asan will intercept C++ exception api call(s).\n #ifndef ASAN_HAS_EXCEPTIONS\n-# define ASAN_HAS_EXCEPTIONS 1\n+#  define ASAN_HAS_EXCEPTIONS 1\n #endif\n \n // If set, values like allocator chunk size, as well as defaults for some flags\n@@ -43,11 +43,11 @@\n #endif\n \n #ifndef ASAN_DYNAMIC\n-# ifdef PIC\n-#  define ASAN_DYNAMIC 1\n-# else\n-#  define ASAN_DYNAMIC 0\n-# endif\n+#  ifdef PIC\n+#    define ASAN_DYNAMIC 1\n+#  else\n+#    define ASAN_DYNAMIC 0\n+#  endif\n #endif\n \n // All internal functions in asan reside inside the __asan namespace\n@@ -123,26 +123,18 @@ void *AsanDlSymNext(const char *sym);\n // `dlopen()` specific initialization inside this function.\n bool HandleDlopenInit();\n \n-// Add convenient macro for interface functions that may be represented as\n-// weak hooks.\n-#define ASAN_MALLOC_HOOK(ptr, size)                                   \\\n-  do {                                                                \\\n-    if (&__sanitizer_malloc_hook) __sanitizer_malloc_hook(ptr, size); \\\n-    RunMallocHooks(ptr, size);                                        \\\n-  } while (false)\n-#define ASAN_FREE_HOOK(ptr)                                 \\\n-  do {                                                      \\\n-    if (&__sanitizer_free_hook) __sanitizer_free_hook(ptr); \\\n-    RunFreeHooks(ptr);                                      \\\n-  } while (false)\n+void InstallAtExitCheckLeaks();\n+\n #define ASAN_ON_ERROR() \\\n-  if (&__asan_on_error) __asan_on_error()\n+  if (&__asan_on_error) \\\n+  __asan_on_error()\n \n extern int asan_inited;\n // Used to avoid infinite recursion in __asan_init().\n extern bool asan_init_is_running;\n extern void (*death_callback)(void);\n-// These magic values are written to shadow for better error reporting.\n+// These magic values are written to shadow for better error\n+// reporting.\n const int kAsanHeapLeftRedzoneMagic = 0xfa;\n const int kAsanHeapFreeMagic = 0xfd;\n const int kAsanStackLeftRedzoneMagic = 0xf1;"}, {"sha": "defd81bc19e221ea392975772605d0facce21c5e", "filename": "libsanitizer/asan/asan_linux.cpp", "status": "modified", "additions": 13, "deletions": 19, "changes": 32, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_linux.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_linux.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_linux.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -107,7 +107,7 @@ uptr FindDynamicShadowStart() {\n     return FindPremappedShadowStart(shadow_size_bytes);\n #endif\n \n-  return MapDynamicShadow(shadow_size_bytes, SHADOW_SCALE,\n+  return MapDynamicShadow(shadow_size_bytes, ASAN_SHADOW_SCALE,\n                           /*min_shadow_base_alignment*/ 0, kHighMemEnd);\n }\n \n@@ -131,30 +131,24 @@ static int FindFirstDSOCallback(struct dl_phdr_info *info, size_t size,\n   VReport(2, \"info->dlpi_name = %s\\tinfo->dlpi_addr = %p\\n\", info->dlpi_name,\n           (void *)info->dlpi_addr);\n \n-  // Continue until the first dynamic library is found\n-  if (!info->dlpi_name || info->dlpi_name[0] == 0)\n-    return 0;\n-\n-  // Ignore vDSO\n-  if (internal_strncmp(info->dlpi_name, \"linux-\", sizeof(\"linux-\") - 1) == 0)\n-    return 0;\n+  const char **name = (const char **)data;\n \n-#if SANITIZER_FREEBSD || SANITIZER_NETBSD\n   // Ignore first entry (the main program)\n-  char **p = (char **)data;\n-  if (!(*p)) {\n-    *p = (char *)-1;\n+  if (!*name) {\n+    *name = \"\";\n     return 0;\n   }\n-#endif\n \n-#if SANITIZER_SOLARIS\n-  // Ignore executable on Solaris\n-  if (info->dlpi_addr == 0)\n+#    if SANITIZER_LINUX\n+  // Ignore vDSO. glibc versions earlier than 2.15 (and some patched\n+  // by distributors) return an empty name for the vDSO entry, so\n+  // detect this as well.\n+  if (!info->dlpi_name[0] ||\n+      internal_strncmp(info->dlpi_name, \"linux-\", sizeof(\"linux-\") - 1) == 0)\n     return 0;\n-#endif\n+#    endif\n \n-  *(const char **)data = info->dlpi_name;\n+  *name = info->dlpi_name;\n   return 1;\n }\n \n@@ -175,7 +169,7 @@ void AsanCheckDynamicRTPrereqs() {\n   // Ensure that dynamic RT is the first DSO in the list\n   const char *first_dso_name = nullptr;\n   dl_iterate_phdr(FindFirstDSOCallback, &first_dso_name);\n-  if (first_dso_name && !IsDynamicRTName(first_dso_name)) {\n+  if (first_dso_name && first_dso_name[0] && !IsDynamicRTName(first_dso_name)) {\n     Report(\"ASan runtime does not come first in initial library list; \"\n            \"you should either link runtime to your application or \"\n            \"manually preload it with LD_PRELOAD.\\n\");"}, {"sha": "9161f728d44c8a563607ccbd333cdad52e167aa8", "filename": "libsanitizer/asan/asan_mac.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_mac.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_mac.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_mac.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -55,7 +55,7 @@ void *AsanDoesNotSupportStaticLinkage() {\n }\n \n uptr FindDynamicShadowStart() {\n-  return MapDynamicShadow(MemToShadowSize(kHighMemEnd), SHADOW_SCALE,\n+  return MapDynamicShadow(MemToShadowSize(kHighMemEnd), ASAN_SHADOW_SCALE,\n                           /*min_shadow_base_alignment*/ 0, kHighMemEnd);\n }\n "}, {"sha": "4ff09b103d5f294ee169029bb430c70b3454841f", "filename": "libsanitizer/asan/asan_mapping.h", "status": "modified", "additions": 97, "deletions": 97, "changes": 194, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_mapping.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_mapping.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_mapping.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -13,7 +13,7 @@\n #ifndef ASAN_MAPPING_H\n #define ASAN_MAPPING_H\n \n-#include \"asan_internal.h\"\n+#include \"sanitizer_common/sanitizer_platform.h\"\n \n // The full explanation of the memory mapping could be found here:\n // https://github.com/google/sanitizers/wiki/AddressSanitizerAlgorithm\n@@ -151,149 +151,145 @@\n // || `[0x30000000, 0x35ffffff]` || LowShadow  ||\n // || `[0x00000000, 0x2fffffff]` || LowMem     ||\n \n-#if defined(ASAN_SHADOW_SCALE)\n-static const u64 kDefaultShadowScale = ASAN_SHADOW_SCALE;\n-#else\n-static const u64 kDefaultShadowScale = 3;\n-#endif\n-static const u64 kDefaultShadowSentinel = ~(uptr)0;\n-static const u64 kDefaultShadowOffset32 = 1ULL << 29;  // 0x20000000\n-static const u64 kDefaultShadowOffset64 = 1ULL << 44;\n-static const u64 kDefaultShort64bitShadowOffset =\n-    0x7FFFFFFF & (~0xFFFULL << kDefaultShadowScale);  // < 2G.\n-static const u64 kAArch64_ShadowOffset64 = 1ULL << 36;\n-static const u64 kRiscv64_ShadowOffset64 = 0xd55550000;\n-static const u64 kMIPS32_ShadowOffset32 = 0x0aaa0000;\n-static const u64 kMIPS64_ShadowOffset64 = 1ULL << 37;\n-static const u64 kPPC64_ShadowOffset64 = 1ULL << 41;\n-static const u64 kSystemZ_ShadowOffset64 = 1ULL << 52;\n-static const u64 kSPARC64_ShadowOffset64 = 1ULL << 43;  // 0x80000000000\n-static const u64 kFreeBSD_ShadowOffset32 = 1ULL << 30;  // 0x40000000\n-static const u64 kFreeBSD_ShadowOffset64 = 1ULL << 46;  // 0x400000000000\n-static const u64 kNetBSD_ShadowOffset32 = 1ULL << 30;  // 0x40000000\n-static const u64 kNetBSD_ShadowOffset64 = 1ULL << 46;  // 0x400000000000\n-static const u64 kWindowsShadowOffset32 = 3ULL << 28;  // 0x30000000\n-\n-#define SHADOW_SCALE kDefaultShadowScale\n+#define ASAN_SHADOW_SCALE 3\n \n #if SANITIZER_FUCHSIA\n-#  define SHADOW_OFFSET (0)\n+#  define ASAN_SHADOW_OFFSET_CONST (0)\n #elif SANITIZER_WORDSIZE == 32\n #  if SANITIZER_ANDROID\n-#    define SHADOW_OFFSET __asan_shadow_memory_dynamic_address\n+#    define ASAN_SHADOW_OFFSET_DYNAMIC\n #  elif defined(__mips__)\n-#    define SHADOW_OFFSET kMIPS32_ShadowOffset32\n+#    define ASAN_SHADOW_OFFSET_CONST 0x0aaa0000\n #  elif SANITIZER_FREEBSD\n-#    define SHADOW_OFFSET kFreeBSD_ShadowOffset32\n+#    define ASAN_SHADOW_OFFSET_CONST 0x40000000\n #  elif SANITIZER_NETBSD\n-#    define SHADOW_OFFSET kNetBSD_ShadowOffset32\n+#    define ASAN_SHADOW_OFFSET_CONST 0x40000000\n #  elif SANITIZER_WINDOWS\n-#    define SHADOW_OFFSET kWindowsShadowOffset32\n+#    define ASAN_SHADOW_OFFSET_CONST 0x30000000\n #  elif SANITIZER_IOS\n-#    define SHADOW_OFFSET __asan_shadow_memory_dynamic_address\n+#    define ASAN_SHADOW_OFFSET_DYNAMIC\n #  else\n-#    define SHADOW_OFFSET kDefaultShadowOffset32\n+#    define ASAN_SHADOW_OFFSET_CONST 0x20000000\n #  endif\n #else\n #  if SANITIZER_IOS\n-#    define SHADOW_OFFSET __asan_shadow_memory_dynamic_address\n+#    define ASAN_SHADOW_OFFSET_DYNAMIC\n #  elif SANITIZER_MAC && defined(__aarch64__)\n-#    define SHADOW_OFFSET __asan_shadow_memory_dynamic_address\n-#elif SANITIZER_RISCV64\n-#define SHADOW_OFFSET kRiscv64_ShadowOffset64\n+#    define ASAN_SHADOW_OFFSET_DYNAMIC\n+#  elif SANITIZER_RISCV64\n+#    define ASAN_SHADOW_OFFSET_CONST 0x0000000d55550000\n #  elif defined(__aarch64__)\n-#    define SHADOW_OFFSET kAArch64_ShadowOffset64\n+#    define ASAN_SHADOW_OFFSET_CONST 0x0000001000000000\n #  elif defined(__powerpc64__)\n-#    define SHADOW_OFFSET kPPC64_ShadowOffset64\n+#    define ASAN_SHADOW_OFFSET_CONST 0x0000100000000000\n #  elif defined(__s390x__)\n-#    define SHADOW_OFFSET kSystemZ_ShadowOffset64\n+#    define ASAN_SHADOW_OFFSET_CONST 0x0010000000000000\n #  elif SANITIZER_FREEBSD\n-#    define SHADOW_OFFSET kFreeBSD_ShadowOffset64\n+#    define ASAN_SHADOW_OFFSET_CONST 0x0000400000000000\n #  elif SANITIZER_NETBSD\n-#    define SHADOW_OFFSET kNetBSD_ShadowOffset64\n+#    define ASAN_SHADOW_OFFSET_CONST 0x0000400000000000\n #  elif SANITIZER_MAC\n-#   define SHADOW_OFFSET kDefaultShadowOffset64\n+#    define ASAN_SHADOW_OFFSET_CONST 0x0000100000000000\n #  elif defined(__mips64)\n-#   define SHADOW_OFFSET kMIPS64_ShadowOffset64\n-#elif defined(__sparc__)\n-#define SHADOW_OFFSET kSPARC64_ShadowOffset64\n+#    define ASAN_SHADOW_OFFSET_CONST 0x0000002000000000\n+#  elif defined(__sparc__)\n+#    define ASAN_SHADOW_OFFSET_CONST 0x0000080000000000\n #  elif SANITIZER_WINDOWS64\n-#   define SHADOW_OFFSET __asan_shadow_memory_dynamic_address\n+#    define ASAN_SHADOW_OFFSET_DYNAMIC\n #  else\n-#   define SHADOW_OFFSET kDefaultShort64bitShadowOffset\n+#    if ASAN_SHADOW_SCALE != 3\n+#      error \"Value below is based on shadow scale = 3.\"\n+#      error \"Original formula was: 0x7FFFFFFF & (~0xFFFULL << SHADOW_SCALE).\"\n+#    endif\n+#    define ASAN_SHADOW_OFFSET_CONST 0x000000007fff8000\n #  endif\n #endif\n \n-#if SANITIZER_ANDROID && defined(__arm__)\n-# define ASAN_PREMAP_SHADOW 1\n-#else\n-# define ASAN_PREMAP_SHADOW 0\n-#endif\n+#if defined(__cplusplus)\n+#  include \"asan_internal.h\"\n+\n+static const u64 kDefaultShadowSentinel = ~(uptr)0;\n \n-#define SHADOW_GRANULARITY (1ULL << SHADOW_SCALE)\n+#  if defined(ASAN_SHADOW_OFFSET_CONST)\n+static const u64 kConstShadowOffset = ASAN_SHADOW_OFFSET_CONST;\n+#    define ASAN_SHADOW_OFFSET kConstShadowOffset\n+#  elif defined(ASAN_SHADOW_OFFSET_DYNAMIC)\n+#    define ASAN_SHADOW_OFFSET __asan_shadow_memory_dynamic_address\n+#  else\n+#    error \"ASAN_SHADOW_OFFSET can't be determined.\"\n+#  endif\n \n-#define DO_ASAN_MAPPING_PROFILE 0  // Set to 1 to profile the functions below.\n+#  if SANITIZER_ANDROID && defined(__arm__)\n+#    define ASAN_PREMAP_SHADOW 1\n+#  else\n+#    define ASAN_PREMAP_SHADOW 0\n+#  endif\n \n-#if DO_ASAN_MAPPING_PROFILE\n-# define PROFILE_ASAN_MAPPING() AsanMappingProfile[__LINE__]++;\n-#else\n-# define PROFILE_ASAN_MAPPING()\n-#endif\n+#  define ASAN_SHADOW_GRANULARITY (1ULL << ASAN_SHADOW_SCALE)\n+\n+#  define DO_ASAN_MAPPING_PROFILE 0  // Set to 1 to profile the functions below.\n+\n+#  if DO_ASAN_MAPPING_PROFILE\n+#    define PROFILE_ASAN_MAPPING() AsanMappingProfile[__LINE__]++;\n+#  else\n+#    define PROFILE_ASAN_MAPPING()\n+#  endif\n \n // If 1, all shadow boundaries are constants.\n // Don't set to 1 other than for testing.\n-#define ASAN_FIXED_MAPPING 0\n+#  define ASAN_FIXED_MAPPING 0\n \n namespace __asan {\n \n extern uptr AsanMappingProfile[];\n \n-#if ASAN_FIXED_MAPPING\n+#  if ASAN_FIXED_MAPPING\n // Fixed mapping for 64-bit Linux. Mostly used for performance comparison\n // with non-fixed mapping. As of r175253 (Feb 2013) the performance\n // difference between fixed and non-fixed mapping is below the noise level.\n static uptr kHighMemEnd = 0x7fffffffffffULL;\n-static uptr kMidMemBeg =    0x3000000000ULL;\n-static uptr kMidMemEnd =    0x4fffffffffULL;\n-#else\n+static uptr kMidMemBeg = 0x3000000000ULL;\n+static uptr kMidMemEnd = 0x4fffffffffULL;\n+#  else\n extern uptr kHighMemEnd, kMidMemBeg, kMidMemEnd;  // Initialized in __asan_init.\n-#endif\n+#  endif\n \n }  // namespace __asan\n \n-#if defined(__sparc__) && SANITIZER_WORDSIZE == 64\n-#  include \"asan_mapping_sparc64.h\"\n-#else\n-#define MEM_TO_SHADOW(mem) (((mem) >> SHADOW_SCALE) + (SHADOW_OFFSET))\n+#  if defined(__sparc__) && SANITIZER_WORDSIZE == 64\n+#    include \"asan_mapping_sparc64.h\"\n+#  else\n+#    define MEM_TO_SHADOW(mem) \\\n+      (((mem) >> ASAN_SHADOW_SCALE) + (ASAN_SHADOW_OFFSET))\n \n-#define kLowMemBeg      0\n-#define kLowMemEnd      (SHADOW_OFFSET ? SHADOW_OFFSET - 1 : 0)\n+#    define kLowMemBeg 0\n+#    define kLowMemEnd (ASAN_SHADOW_OFFSET ? ASAN_SHADOW_OFFSET - 1 : 0)\n \n-#define kLowShadowBeg   SHADOW_OFFSET\n-#define kLowShadowEnd   MEM_TO_SHADOW(kLowMemEnd)\n+#    define kLowShadowBeg ASAN_SHADOW_OFFSET\n+#    define kLowShadowEnd MEM_TO_SHADOW(kLowMemEnd)\n \n-#define kHighMemBeg     (MEM_TO_SHADOW(kHighMemEnd) + 1)\n+#    define kHighMemBeg (MEM_TO_SHADOW(kHighMemEnd) + 1)\n \n-#define kHighShadowBeg  MEM_TO_SHADOW(kHighMemBeg)\n-#define kHighShadowEnd  MEM_TO_SHADOW(kHighMemEnd)\n+#    define kHighShadowBeg MEM_TO_SHADOW(kHighMemBeg)\n+#    define kHighShadowEnd MEM_TO_SHADOW(kHighMemEnd)\n \n-# define kMidShadowBeg MEM_TO_SHADOW(kMidMemBeg)\n-# define kMidShadowEnd MEM_TO_SHADOW(kMidMemEnd)\n+#    define kMidShadowBeg MEM_TO_SHADOW(kMidMemBeg)\n+#    define kMidShadowEnd MEM_TO_SHADOW(kMidMemEnd)\n \n // With the zero shadow base we can not actually map pages starting from 0.\n // This constant is somewhat arbitrary.\n-#define kZeroBaseShadowStart 0\n-#define kZeroBaseMaxShadowStart (1 << 18)\n+#    define kZeroBaseShadowStart 0\n+#    define kZeroBaseMaxShadowStart (1 << 18)\n \n-#define kShadowGapBeg   (kLowShadowEnd ? kLowShadowEnd + 1 \\\n-                                       : kZeroBaseShadowStart)\n-#define kShadowGapEnd   ((kMidMemBeg ? kMidShadowBeg : kHighShadowBeg) - 1)\n+#    define kShadowGapBeg \\\n+      (kLowShadowEnd ? kLowShadowEnd + 1 : kZeroBaseShadowStart)\n+#    define kShadowGapEnd ((kMidMemBeg ? kMidShadowBeg : kHighShadowBeg) - 1)\n \n-#define kShadowGap2Beg (kMidMemBeg ? kMidShadowEnd + 1 : 0)\n-#define kShadowGap2End (kMidMemBeg ? kMidMemBeg - 1 : 0)\n+#    define kShadowGap2Beg (kMidMemBeg ? kMidShadowEnd + 1 : 0)\n+#    define kShadowGap2End (kMidMemBeg ? kMidMemBeg - 1 : 0)\n \n-#define kShadowGap3Beg (kMidMemBeg ? kMidMemEnd + 1 : 0)\n-#define kShadowGap3End (kMidMemBeg ? kHighShadowBeg - 1 : 0)\n+#    define kShadowGap3Beg (kMidMemBeg ? kMidMemEnd + 1 : 0)\n+#    define kShadowGap3End (kMidMemBeg ? kHighShadowBeg - 1 : 0)\n \n namespace __asan {\n \n@@ -331,29 +327,31 @@ static inline bool AddrIsInShadowGap(uptr a) {\n   PROFILE_ASAN_MAPPING();\n   if (kMidMemBeg) {\n     if (a <= kShadowGapEnd)\n-      return SHADOW_OFFSET == 0 || a >= kShadowGapBeg;\n+      return ASAN_SHADOW_OFFSET == 0 || a >= kShadowGapBeg;\n     return (a >= kShadowGap2Beg && a <= kShadowGap2End) ||\n            (a >= kShadowGap3Beg && a <= kShadowGap3End);\n   }\n   // In zero-based shadow mode we treat addresses near zero as addresses\n   // in shadow gap as well.\n-  if (SHADOW_OFFSET == 0)\n+  if (ASAN_SHADOW_OFFSET == 0)\n     return a <= kShadowGapEnd;\n   return a >= kShadowGapBeg && a <= kShadowGapEnd;\n }\n \n }  // namespace __asan\n \n-#endif\n+#  endif\n \n namespace __asan {\n \n-static inline uptr MemToShadowSize(uptr size) { return size >> SHADOW_SCALE; }\n+static inline uptr MemToShadowSize(uptr size) {\n+  return size >> ASAN_SHADOW_SCALE;\n+}\n \n static inline bool AddrIsInMem(uptr a) {\n   PROFILE_ASAN_MAPPING();\n   return AddrIsInLowMem(a) || AddrIsInMidMem(a) || AddrIsInHighMem(a) ||\n-      (flags()->protect_shadow_gap == 0 && AddrIsInShadowGap(a));\n+         (flags()->protect_shadow_gap == 0 && AddrIsInShadowGap(a));\n }\n \n static inline uptr MemToShadow(uptr p) {\n@@ -369,17 +367,17 @@ static inline bool AddrIsInShadow(uptr a) {\n \n static inline bool AddrIsAlignedByGranularity(uptr a) {\n   PROFILE_ASAN_MAPPING();\n-  return (a & (SHADOW_GRANULARITY - 1)) == 0;\n+  return (a & (ASAN_SHADOW_GRANULARITY - 1)) == 0;\n }\n \n static inline bool AddressIsPoisoned(uptr a) {\n   PROFILE_ASAN_MAPPING();\n   const uptr kAccessSize = 1;\n-  u8 *shadow_address = (u8*)MEM_TO_SHADOW(a);\n+  u8 *shadow_address = (u8 *)MEM_TO_SHADOW(a);\n   s8 shadow_value = *shadow_address;\n   if (shadow_value) {\n-    u8 last_accessed_byte = (a & (SHADOW_GRANULARITY - 1))\n-                                 + kAccessSize - 1;\n+    u8 last_accessed_byte =\n+        (a & (ASAN_SHADOW_GRANULARITY - 1)) + kAccessSize - 1;\n     return (last_accessed_byte >= shadow_value);\n   }\n   return false;\n@@ -390,4 +388,6 @@ static const uptr kAsanMappingProfileSize = __LINE__;\n \n }  // namespace __asan\n \n+#endif  // __cplusplus\n+\n #endif  // ASAN_MAPPING_H"}, {"sha": "90261d301f7f8c1c2a2b2d1cfdaa76cdcd9e34a6", "filename": "libsanitizer/asan/asan_mapping_sparc64.h", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_mapping_sparc64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_mapping_sparc64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_mapping_sparc64.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -25,13 +25,14 @@\n // The idea is to chop the high bits before doing the scaling, so the two\n // parts become contiguous again and the usual scheme can be applied.\n \n-#define MEM_TO_SHADOW(mem) \\\n-  ((((mem) << HIGH_BITS) >> (HIGH_BITS + (SHADOW_SCALE))) + (SHADOW_OFFSET))\n+#define MEM_TO_SHADOW(mem)                                       \\\n+  ((((mem) << HIGH_BITS) >> (HIGH_BITS + (ASAN_SHADOW_SCALE))) + \\\n+   (ASAN_SHADOW_OFFSET))\n \n #define kLowMemBeg 0\n-#define kLowMemEnd (SHADOW_OFFSET - 1)\n+#define kLowMemEnd (ASAN_SHADOW_OFFSET - 1)\n \n-#define kLowShadowBeg SHADOW_OFFSET\n+#define kLowShadowBeg ASAN_SHADOW_OFFSET\n #define kLowShadowEnd MEM_TO_SHADOW(kLowMemEnd)\n \n // But of course there is the huge hole between the high shadow memory,"}, {"sha": "3b7c9d1312d6187f47784c4e745c032a8b7a2b04", "filename": "libsanitizer/asan/asan_poisoning.cpp", "status": "modified", "additions": 16, "deletions": 14, "changes": 30, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_poisoning.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_poisoning.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_poisoning.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -12,11 +12,13 @@\n //===----------------------------------------------------------------------===//\n \n #include \"asan_poisoning.h\"\n+\n #include \"asan_report.h\"\n #include \"asan_stack.h\"\n #include \"sanitizer_common/sanitizer_atomic.h\"\n-#include \"sanitizer_common/sanitizer_libc.h\"\n #include \"sanitizer_common/sanitizer_flags.h\"\n+#include \"sanitizer_common/sanitizer_interface_internal.h\"\n+#include \"sanitizer_common/sanitizer_libc.h\"\n \n namespace __asan {\n \n@@ -35,7 +37,7 @@ void PoisonShadow(uptr addr, uptr size, u8 value) {\n   CHECK(AddrIsAlignedByGranularity(addr));\n   CHECK(AddrIsInMem(addr));\n   CHECK(AddrIsAlignedByGranularity(addr + size));\n-  CHECK(AddrIsInMem(addr + size - SHADOW_GRANULARITY));\n+  CHECK(AddrIsInMem(addr + size - ASAN_SHADOW_GRANULARITY));\n   CHECK(REAL(memset));\n   FastPoisonShadow(addr, size, value);\n }\n@@ -52,12 +54,12 @@ void PoisonShadowPartialRightRedzone(uptr addr,\n \n struct ShadowSegmentEndpoint {\n   u8 *chunk;\n-  s8 offset;  // in [0, SHADOW_GRANULARITY)\n+  s8 offset;  // in [0, ASAN_SHADOW_GRANULARITY)\n   s8 value;  // = *chunk;\n \n   explicit ShadowSegmentEndpoint(uptr address) {\n     chunk = (u8*)MemToShadow(address);\n-    offset = address & (SHADOW_GRANULARITY - 1);\n+    offset = address & (ASAN_SHADOW_GRANULARITY - 1);\n     value = *chunk;\n   }\n };\n@@ -72,14 +74,14 @@ void AsanPoisonOrUnpoisonIntraObjectRedzone(uptr ptr, uptr size, bool poison) {\n   }\n   CHECK(size);\n   CHECK_LE(size, 4096);\n-  CHECK(IsAligned(end, SHADOW_GRANULARITY));\n-  if (!IsAligned(ptr, SHADOW_GRANULARITY)) {\n+  CHECK(IsAligned(end, ASAN_SHADOW_GRANULARITY));\n+  if (!IsAligned(ptr, ASAN_SHADOW_GRANULARITY)) {\n     *(u8 *)MemToShadow(ptr) =\n-        poison ? static_cast<u8>(ptr % SHADOW_GRANULARITY) : 0;\n-    ptr |= SHADOW_GRANULARITY - 1;\n+        poison ? static_cast<u8>(ptr % ASAN_SHADOW_GRANULARITY) : 0;\n+    ptr |= ASAN_SHADOW_GRANULARITY - 1;\n     ptr++;\n   }\n-  for (; ptr < end; ptr += SHADOW_GRANULARITY)\n+  for (; ptr < end; ptr += ASAN_SHADOW_GRANULARITY)\n     *(u8*)MemToShadow(ptr) = poison ? kAsanIntraObjectRedzone : 0;\n }\n \n@@ -181,12 +183,12 @@ uptr __asan_region_is_poisoned(uptr beg, uptr size) {\n   if (!AddrIsInMem(end))\n     return end;\n   CHECK_LT(beg, end);\n-  uptr aligned_b = RoundUpTo(beg, SHADOW_GRANULARITY);\n-  uptr aligned_e = RoundDownTo(end, SHADOW_GRANULARITY);\n+  uptr aligned_b = RoundUpTo(beg, ASAN_SHADOW_GRANULARITY);\n+  uptr aligned_e = RoundDownTo(end, ASAN_SHADOW_GRANULARITY);\n   uptr shadow_beg = MemToShadow(aligned_b);\n   uptr shadow_end = MemToShadow(aligned_e);\n   // First check the first and the last application bytes,\n-  // then check the SHADOW_GRANULARITY-aligned region by calling\n+  // then check the ASAN_SHADOW_GRANULARITY-aligned region by calling\n   // mem_is_zero on the corresponding shadow.\n   if (!__asan::AddressIsPoisoned(beg) && !__asan::AddressIsPoisoned(end - 1) &&\n       (shadow_end <= shadow_beg ||\n@@ -285,7 +287,7 @@ uptr __asan_load_cxx_array_cookie(uptr *p) {\n // assumes that left border of region to be poisoned is properly aligned.\n static void PoisonAlignedStackMemory(uptr addr, uptr size, bool do_poison) {\n   if (size == 0) return;\n-  uptr aligned_size = size & ~(SHADOW_GRANULARITY - 1);\n+  uptr aligned_size = size & ~(ASAN_SHADOW_GRANULARITY - 1);\n   PoisonShadow(addr, aligned_size,\n                do_poison ? kAsanStackUseAfterScopeMagic : 0);\n   if (size == aligned_size)\n@@ -351,7 +353,7 @@ void __sanitizer_annotate_contiguous_container(const void *beg_p,\n   uptr end = reinterpret_cast<uptr>(end_p);\n   uptr old_mid = reinterpret_cast<uptr>(old_mid_p);\n   uptr new_mid = reinterpret_cast<uptr>(new_mid_p);\n-  uptr granularity = SHADOW_GRANULARITY;\n+  uptr granularity = ASAN_SHADOW_GRANULARITY;\n   if (!(beg <= old_mid && beg <= new_mid && old_mid <= end && new_mid <= end &&\n         IsAligned(beg, granularity))) {\n     GET_STACK_TRACE_FATAL_HERE;"}, {"sha": "600bd011f304cbc23e46afc8397689a9bc1c7514", "filename": "libsanitizer/asan/asan_poisoning.h", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_poisoning.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_poisoning.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_poisoning.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -44,8 +44,8 @@ ALWAYS_INLINE void FastPoisonShadow(uptr aligned_beg, uptr aligned_size,\n                           common_flags()->clear_shadow_mmap_threshold);\n #else\n   uptr shadow_beg = MEM_TO_SHADOW(aligned_beg);\n-  uptr shadow_end = MEM_TO_SHADOW(\n-      aligned_beg + aligned_size - SHADOW_GRANULARITY) + 1;\n+  uptr shadow_end =\n+      MEM_TO_SHADOW(aligned_beg + aligned_size - ASAN_SHADOW_GRANULARITY) + 1;\n   // FIXME: Page states are different on Windows, so using the same interface\n   // for mapping shadow and zeroing out pages doesn't \"just work\", so we should\n   // probably provide higher-level interface for these operations.\n@@ -78,11 +78,12 @@ ALWAYS_INLINE void FastPoisonShadowPartialRightRedzone(\n   DCHECK(CanPoisonMemory());\n   bool poison_partial = flags()->poison_partial;\n   u8 *shadow = (u8*)MEM_TO_SHADOW(aligned_addr);\n-  for (uptr i = 0; i < redzone_size; i += SHADOW_GRANULARITY, shadow++) {\n-    if (i + SHADOW_GRANULARITY <= size) {\n+  for (uptr i = 0; i < redzone_size; i += ASAN_SHADOW_GRANULARITY, shadow++) {\n+    if (i + ASAN_SHADOW_GRANULARITY <= size) {\n       *shadow = 0;  // fully addressable\n     } else if (i >= size) {\n-      *shadow = (SHADOW_GRANULARITY == 128) ? 0xff : value;  // unaddressable\n+      *shadow =\n+          (ASAN_SHADOW_GRANULARITY == 128) ? 0xff : value;  // unaddressable\n     } else {\n       // first size-i bytes are addressable\n       *shadow = poison_partial ? static_cast<u8>(size - i) : 0;"}, {"sha": "765f4a26cd7ab8ab49575114b80a6b4a285eec5a", "filename": "libsanitizer/asan/asan_posix.cpp", "status": "modified", "additions": 30, "deletions": 17, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_posix.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_posix.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_posix.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -14,22 +14,23 @@\n #include \"sanitizer_common/sanitizer_platform.h\"\n #if SANITIZER_POSIX\n \n-#include \"asan_internal.h\"\n-#include \"asan_interceptors.h\"\n-#include \"asan_mapping.h\"\n-#include \"asan_poisoning.h\"\n-#include \"asan_report.h\"\n-#include \"asan_stack.h\"\n-#include \"sanitizer_common/sanitizer_libc.h\"\n-#include \"sanitizer_common/sanitizer_posix.h\"\n-#include \"sanitizer_common/sanitizer_procmaps.h\"\n-\n-#include <pthread.h>\n-#include <signal.h>\n-#include <stdlib.h>\n-#include <sys/time.h>\n-#include <sys/resource.h>\n-#include <unistd.h>\n+#  include <pthread.h>\n+#  include <signal.h>\n+#  include <stdlib.h>\n+#  include <sys/resource.h>\n+#  include <sys/time.h>\n+#  include <unistd.h>\n+\n+#  include \"asan_interceptors.h\"\n+#  include \"asan_internal.h\"\n+#  include \"asan_mapping.h\"\n+#  include \"asan_poisoning.h\"\n+#  include \"asan_report.h\"\n+#  include \"asan_stack.h\"\n+#  include \"lsan/lsan_common.h\"\n+#  include \"sanitizer_common/sanitizer_libc.h\"\n+#  include \"sanitizer_common/sanitizer_posix.h\"\n+#  include \"sanitizer_common/sanitizer_procmaps.h\"\n \n namespace __asan {\n \n@@ -131,7 +132,7 @@ void AsanTSDSet(void *tsd) {\n }\n \n void PlatformTSDDtor(void *tsd) {\n-  AsanThreadContext *context = (AsanThreadContext*)tsd;\n+  AsanThreadContext *context = (AsanThreadContext *)tsd;\n   if (context->destructor_iterations > 1) {\n     context->destructor_iterations--;\n     CHECK_EQ(0, pthread_setspecific(tsd_key, tsd));\n@@ -140,6 +141,18 @@ void PlatformTSDDtor(void *tsd) {\n   AsanThread::TSDDtor(tsd);\n }\n #endif\n+\n+void InstallAtExitCheckLeaks() {\n+  if (CAN_SANITIZE_LEAKS) {\n+    if (common_flags()->detect_leaks && common_flags()->leak_check_at_exit) {\n+      if (flags()->halt_on_error)\n+        Atexit(__lsan::DoLeakCheck);\n+      else\n+        Atexit(__lsan::DoRecoverableLeakCheckVoid);\n+    }\n+  }\n+}\n+\n }  // namespace __asan\n \n #endif  // SANITIZER_POSIX"}, {"sha": "bed2f62a2251107b82c639063e7069922c745104", "filename": "libsanitizer/asan/asan_premap_shadow.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_premap_shadow.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_premap_shadow.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_premap_shadow.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -26,7 +26,7 @@ namespace __asan {\n // Conservative upper limit.\n uptr PremapShadowSize() {\n   uptr granularity = GetMmapGranularity();\n-  return RoundUpTo(GetMaxVirtualAddress() >> SHADOW_SCALE, granularity);\n+  return RoundUpTo(GetMaxVirtualAddress() >> ASAN_SHADOW_SCALE, granularity);\n }\n \n // Returns an address aligned to 8 pages, such that one page on the left and"}, {"sha": "2a55d6c0978d88c282ddc77f5d4708568c89b616", "filename": "libsanitizer/asan/asan_report.cpp", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_report.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_report.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_report.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -11,17 +11,19 @@\n // This file contains error reporting code.\n //===----------------------------------------------------------------------===//\n \n+#include \"asan_report.h\"\n+\n+#include \"asan_descriptions.h\"\n #include \"asan_errors.h\"\n #include \"asan_flags.h\"\n-#include \"asan_descriptions.h\"\n #include \"asan_internal.h\"\n #include \"asan_mapping.h\"\n-#include \"asan_report.h\"\n #include \"asan_scariness_score.h\"\n #include \"asan_stack.h\"\n #include \"asan_thread.h\"\n #include \"sanitizer_common/sanitizer_common.h\"\n #include \"sanitizer_common/sanitizer_flags.h\"\n+#include \"sanitizer_common/sanitizer_interface_internal.h\"\n #include \"sanitizer_common/sanitizer_report_decorator.h\"\n #include \"sanitizer_common/sanitizer_stackdepot.h\"\n #include \"sanitizer_common/sanitizer_symbolizer.h\"\n@@ -460,6 +462,10 @@ static bool SuppressErrorReport(uptr pc) {\n \n void ReportGenericError(uptr pc, uptr bp, uptr sp, uptr addr, bool is_write,\n                         uptr access_size, u32 exp, bool fatal) {\n+  if (__asan_test_only_reported_buggy_pointer) {\n+    *__asan_test_only_reported_buggy_pointer = addr;\n+    return;\n+  }\n   if (!fatal && SuppressErrorReport(pc)) return;\n   ENABLE_FRAME_POINTER;\n "}, {"sha": "3a5261474b2980ef7e0ae343406b05cffddafc68", "filename": "libsanitizer/asan/asan_rtl.cpp", "status": "modified", "additions": 26, "deletions": 47, "changes": 73, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_rtl.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_rtl.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_rtl.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -27,6 +27,7 @@\n #include \"lsan/lsan_common.h\"\n #include \"sanitizer_common/sanitizer_atomic.h\"\n #include \"sanitizer_common/sanitizer_flags.h\"\n+#include \"sanitizer_common/sanitizer_interface_internal.h\"\n #include \"sanitizer_common/sanitizer_libc.h\"\n #include \"sanitizer_common/sanitizer_symbolizer.h\"\n #include \"ubsan/ubsan_init.h\"\n@@ -44,7 +45,9 @@ static void AsanDie() {\n   static atomic_uint32_t num_calls;\n   if (atomic_fetch_add(&num_calls, 1, memory_order_relaxed) != 0) {\n     // Don't die twice - run a busy loop.\n-    while (1) { }\n+    while (1) {\n+      internal_sched_yield();\n+    }\n   }\n   if (common_flags()->print_module_map >= 1)\n     DumpProcessMap();\n@@ -85,12 +88,8 @@ void ShowStatsAndAbort() {\n NOINLINE\n static void ReportGenericErrorWrapper(uptr addr, bool is_write, int size,\n                                       int exp_arg, bool fatal) {\n-  if (__asan_test_only_reported_buggy_pointer) {\n-    *__asan_test_only_reported_buggy_pointer = addr;\n-  } else {\n-    GET_CALLER_PC_BP_SP;\n-    ReportGenericError(pc, bp, sp, addr, is_write, size, exp_arg, fatal);\n-  }\n+  GET_CALLER_PC_BP_SP;\n+  ReportGenericError(pc, bp, sp, addr, is_write, size, exp_arg, fatal);\n }\n \n // --------------- LowLevelAllocateCallbac ---------- {{{1\n@@ -150,11 +149,11 @@ ASAN_REPORT_ERROR_N(store, true)\n \n #define ASAN_MEMORY_ACCESS_CALLBACK_BODY(type, is_write, size, exp_arg, fatal) \\\n   uptr sp = MEM_TO_SHADOW(addr);                                               \\\n-  uptr s = size <= SHADOW_GRANULARITY ? *reinterpret_cast<u8 *>(sp)            \\\n-                                      : *reinterpret_cast<u16 *>(sp);          \\\n+  uptr s = size <= ASAN_SHADOW_GRANULARITY ? *reinterpret_cast<u8 *>(sp)       \\\n+                                           : *reinterpret_cast<u16 *>(sp);     \\\n   if (UNLIKELY(s)) {                                                           \\\n-    if (UNLIKELY(size >= SHADOW_GRANULARITY ||                                 \\\n-                 ((s8)((addr & (SHADOW_GRANULARITY - 1)) + size - 1)) >=       \\\n+    if (UNLIKELY(size >= ASAN_SHADOW_GRANULARITY ||                            \\\n+                 ((s8)((addr & (ASAN_SHADOW_GRANULARITY - 1)) + size - 1)) >=  \\\n                      (s8)s)) {                                                 \\\n       ReportGenericErrorWrapper(addr, is_write, size, exp_arg, fatal);         \\\n     }                                                                          \\\n@@ -188,7 +187,7 @@ ASAN_MEMORY_ACCESS_CALLBACK(store, true, 16)\n extern \"C\"\n NOINLINE INTERFACE_ATTRIBUTE\n void __asan_loadN(uptr addr, uptr size) {\n-  if (__asan_region_is_poisoned(addr, size)) {\n+  if ((addr = __asan_region_is_poisoned(addr, size))) {\n     GET_CALLER_PC_BP_SP;\n     ReportGenericError(pc, bp, sp, addr, false, size, 0, true);\n   }\n@@ -197,7 +196,7 @@ void __asan_loadN(uptr addr, uptr size) {\n extern \"C\"\n NOINLINE INTERFACE_ATTRIBUTE\n void __asan_exp_loadN(uptr addr, uptr size, u32 exp) {\n-  if (__asan_region_is_poisoned(addr, size)) {\n+  if ((addr = __asan_region_is_poisoned(addr, size))) {\n     GET_CALLER_PC_BP_SP;\n     ReportGenericError(pc, bp, sp, addr, false, size, exp, true);\n   }\n@@ -206,7 +205,7 @@ void __asan_exp_loadN(uptr addr, uptr size, u32 exp) {\n extern \"C\"\n NOINLINE INTERFACE_ATTRIBUTE\n void __asan_loadN_noabort(uptr addr, uptr size) {\n-  if (__asan_region_is_poisoned(addr, size)) {\n+  if ((addr = __asan_region_is_poisoned(addr, size))) {\n     GET_CALLER_PC_BP_SP;\n     ReportGenericError(pc, bp, sp, addr, false, size, 0, false);\n   }\n@@ -215,7 +214,7 @@ void __asan_loadN_noabort(uptr addr, uptr size) {\n extern \"C\"\n NOINLINE INTERFACE_ATTRIBUTE\n void __asan_storeN(uptr addr, uptr size) {\n-  if (__asan_region_is_poisoned(addr, size)) {\n+  if ((addr = __asan_region_is_poisoned(addr, size))) {\n     GET_CALLER_PC_BP_SP;\n     ReportGenericError(pc, bp, sp, addr, true, size, 0, true);\n   }\n@@ -224,7 +223,7 @@ void __asan_storeN(uptr addr, uptr size) {\n extern \"C\"\n NOINLINE INTERFACE_ATTRIBUTE\n void __asan_exp_storeN(uptr addr, uptr size, u32 exp) {\n-  if (__asan_region_is_poisoned(addr, size)) {\n+  if ((addr = __asan_region_is_poisoned(addr, size))) {\n     GET_CALLER_PC_BP_SP;\n     ReportGenericError(pc, bp, sp, addr, true, size, exp, true);\n   }\n@@ -233,7 +232,7 @@ void __asan_exp_storeN(uptr addr, uptr size, u32 exp) {\n extern \"C\"\n NOINLINE INTERFACE_ATTRIBUTE\n void __asan_storeN_noabort(uptr addr, uptr size) {\n-  if (__asan_region_is_poisoned(addr, size)) {\n+  if ((addr = __asan_region_is_poisoned(addr, size))) {\n     GET_CALLER_PC_BP_SP;\n     ReportGenericError(pc, bp, sp, addr, true, size, 0, false);\n   }\n@@ -313,7 +312,7 @@ static void InitializeHighMemEnd() {\n   kHighMemEnd = GetMaxUserVirtualAddress();\n   // Increase kHighMemEnd to make sure it's properly\n   // aligned together with kHighMemBeg:\n-  kHighMemEnd |= (GetMmapGranularity() << SHADOW_SCALE) - 1;\n+  kHighMemEnd |= (GetMmapGranularity() << ASAN_SHADOW_SCALE) - 1;\n #endif  // !ASAN_FIXED_MAPPING\n   CHECK_EQ((kHighMemBeg % GetMmapGranularity()), 0);\n }\n@@ -365,29 +364,16 @@ void PrintAddressSpaceLayout() {\n   Printf(\"malloc_context_size=%zu\\n\",\n          (uptr)common_flags()->malloc_context_size);\n \n-  Printf(\"SHADOW_SCALE: %d\\n\", (int)SHADOW_SCALE);\n-  Printf(\"SHADOW_GRANULARITY: %d\\n\", (int)SHADOW_GRANULARITY);\n-  Printf(\"SHADOW_OFFSET: 0x%zx\\n\", (uptr)SHADOW_OFFSET);\n-  CHECK(SHADOW_SCALE >= 3 && SHADOW_SCALE <= 7);\n+  Printf(\"SHADOW_SCALE: %d\\n\", (int)ASAN_SHADOW_SCALE);\n+  Printf(\"SHADOW_GRANULARITY: %d\\n\", (int)ASAN_SHADOW_GRANULARITY);\n+  Printf(\"SHADOW_OFFSET: 0x%zx\\n\", (uptr)ASAN_SHADOW_OFFSET);\n+  CHECK(ASAN_SHADOW_SCALE >= 3 && ASAN_SHADOW_SCALE <= 7);\n   if (kMidMemBeg)\n     CHECK(kMidShadowBeg > kLowShadowEnd &&\n           kMidMemBeg > kMidShadowEnd &&\n           kHighShadowBeg > kMidMemEnd);\n }\n \n-#if defined(__thumb__) && defined(__linux__)\n-#define START_BACKGROUND_THREAD_IN_ASAN_INTERNAL\n-#endif\n-\n-#ifndef START_BACKGROUND_THREAD_IN_ASAN_INTERNAL\n-static bool UNUSED __local_asan_dyninit = [] {\n-  MaybeStartBackgroudThread();\n-  SetSoftRssLimitExceededCallback(AsanSoftRssLimitExceededCallback);\n-\n-  return false;\n-}();\n-#endif\n-\n static void AsanInitInternal() {\n   if (LIKELY(asan_inited)) return;\n   SanitizerToolName = \"AddressSanitizer\";\n@@ -438,7 +424,7 @@ static void AsanInitInternal() {\n   MaybeReexec();\n \n   // Setup internal allocator callback.\n-  SetLowLevelAllocateMinAlignment(SHADOW_GRANULARITY);\n+  SetLowLevelAllocateMinAlignment(ASAN_SHADOW_GRANULARITY);\n   SetLowLevelAllocateCallback(OnLowLevelAllocate);\n \n   InitializeAsanInterceptors();\n@@ -462,10 +448,8 @@ static void AsanInitInternal() {\n   allocator_options.SetFrom(flags(), common_flags());\n   InitializeAllocator(allocator_options);\n \n-#ifdef START_BACKGROUND_THREAD_IN_ASAN_INTERNAL\n-  MaybeStartBackgroudThread();\n-  SetSoftRssLimitExceededCallback(AsanSoftRssLimitExceededCallback);\n-#endif\n+  if (SANITIZER_START_BACKGROUND_THREAD_IN_ASAN_INTERNAL)\n+    MaybeStartBackgroudThread();\n \n   // On Linux AsanThread::ThreadStart() calls malloc() that's why asan_inited\n   // should be set to 1 prior to initializing the threads.\n@@ -493,12 +477,7 @@ static void AsanInitInternal() {\n \n   if (CAN_SANITIZE_LEAKS) {\n     __lsan::InitCommonLsan();\n-    if (common_flags()->detect_leaks && common_flags()->leak_check_at_exit) {\n-      if (flags()->halt_on_error)\n-        Atexit(__lsan::DoLeakCheck);\n-      else\n-        Atexit(__lsan::DoRecoverableLeakCheckVoid);\n-    }\n+    InstallAtExitCheckLeaks();\n   }\n \n #if CAN_SANITIZE_UB\n@@ -561,7 +540,7 @@ void UnpoisonStack(uptr bottom, uptr top, const char *type) {\n         top - bottom);\n     return;\n   }\n-  PoisonShadow(bottom, RoundUpTo(top - bottom, SHADOW_GRANULARITY), 0);\n+  PoisonShadow(bottom, RoundUpTo(top - bottom, ASAN_SHADOW_GRANULARITY), 0);\n }\n \n static void UnpoisonDefaultStack() {"}, {"sha": "a6f812bb89151d8540e8b96ee6777ef37fc81940", "filename": "libsanitizer/asan/asan_rtl_static.cpp", "status": "added", "additions": 36, "deletions": 0, "changes": 36, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_rtl_static.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_rtl_static.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_rtl_static.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -0,0 +1,36 @@\n+//===-- asan_static_rtl.cpp -----------------------------------------------===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+//\n+// This file is a part of AddressSanitizer, an address sanity checker.\n+//\n+// Main file of the ASan run-time library.\n+//===----------------------------------------------------------------------===//\n+\n+// This file is empty for now. Main reason to have it is workaround for Windows\n+// build, which complains because no files are part of the asan_static lib.\n+\n+#include \"sanitizer_common/sanitizer_common.h\"\n+\n+#define REPORT_FUNCTION(Name)                                       \\\n+  extern \"C\" SANITIZER_WEAK_ATTRIBUTE void Name(__asan::uptr addr); \\\n+  extern \"C\" void Name##_asm(uptr addr) { Name(addr); }\n+\n+namespace __asan {\n+\n+REPORT_FUNCTION(__asan_report_load1)\n+REPORT_FUNCTION(__asan_report_load2)\n+REPORT_FUNCTION(__asan_report_load4)\n+REPORT_FUNCTION(__asan_report_load8)\n+REPORT_FUNCTION(__asan_report_load16)\n+REPORT_FUNCTION(__asan_report_store1)\n+REPORT_FUNCTION(__asan_report_store2)\n+REPORT_FUNCTION(__asan_report_store4)\n+REPORT_FUNCTION(__asan_report_store8)\n+REPORT_FUNCTION(__asan_report_store16)\n+\n+}  // namespace __asan"}, {"sha": "d93b5ed2a7fe34ccc1753597a8aecfc9db8e868e", "filename": "libsanitizer/asan/asan_rtl_x86_64.S", "status": "added", "additions": 146, "deletions": 0, "changes": 146, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_rtl_x86_64.S", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_rtl_x86_64.S", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_rtl_x86_64.S?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -0,0 +1,146 @@\n+#include \"asan_mapping.h\"\n+#include \"sanitizer_common/sanitizer_asm.h\"\n+\n+#if defined(__x86_64__)\n+#include \"sanitizer_common/sanitizer_platform.h\"\n+\n+.file \"asan_rtl_x86_64.S\"\n+\n+#define NAME(n, reg, op, s, i) n##_##op##_##i##_##s##_##reg\n+\n+#define FNAME(reg, op, s, i) NAME(__asan_check, reg, op, s, i)\n+#define RLABEL(reg, op, s, i) NAME(.return, reg, op, s, i)\n+#define CLABEL(reg, op, s, i) NAME(.check, reg, op, s, i)\n+#define FLABEL(reg, op, s, i) NAME(.fail, reg, op, s, i)\n+\n+#define BEGINF(reg, op, s, i) \\\n+.section .text.FNAME(reg, op, s, i),\"ax\",@progbits ;\\\n+.globl  FNAME(reg, op, s, i) ;\\\n+.hidden  FNAME(reg, op, s, i) ;\\\n+ASM_TYPE_FUNCTION(FNAME(reg, op, s, i)) ;\\\n+.cfi_startproc ;\\\n+FNAME(reg, op, s, i): ;\\\n+\n+#define ENDF .cfi_endproc ;\\\n+\n+// Access check functions for 1,2 and 4 byte types, which require extra checks.\n+#define ASAN_MEMORY_ACCESS_INITIAL_CHECK_ADD(reg, op, s) \\\n+        mov    %##reg,%r10 ;\\\n+        shr    $0x3,%r10 ;\\\n+        movsbl ASAN_SHADOW_OFFSET_CONST(%r10),%r10d ;\\\n+        test   %r10d,%r10d ;\\\n+        jne    CLABEL(reg, op, s, add) ;\\\n+RLABEL(reg, op, s, add): ;\\\n+        retq  ;\\\n+\n+#define ASAN_MEMORY_ACCESS_EXTRA_CHECK_1(reg, op, i) \\\n+CLABEL(reg, op, 1, i): ;\\\n+        push   %rcx ;\\\n+        mov    %##reg,%rcx ;\\\n+        and    $0x7,%ecx ;\\\n+        cmp    %r10d,%ecx ;\\\n+        pop    %rcx ;\\\n+        jl     RLABEL(reg, op, 1, i);\\\n+        mov    %##reg,%rdi ;\\\n+        jmp    __asan_report_##op##1_asm ;\\\n+\n+#define ASAN_MEMORY_ACCESS_EXTRA_CHECK_2(reg, op, i) \\\n+CLABEL(reg, op, 2, i): ;\\\n+        push   %rcx ;\\\n+        mov    %##reg,%rcx ;\\\n+        and    $0x7,%ecx ;\\\n+        add    $0x1,%ecx ;\\\n+        cmp    %r10d,%ecx ;\\\n+        pop    %rcx ;\\\n+        jl     RLABEL(reg, op, 2, i);\\\n+        mov    %##reg,%rdi ;\\\n+        jmp    __asan_report_##op##2_asm ;\\\n+\n+#define ASAN_MEMORY_ACCESS_EXTRA_CHECK_4(reg, op, i) \\\n+CLABEL(reg, op, 4, i): ;\\\n+        push   %rcx ;\\\n+        mov    %##reg,%rcx ;\\\n+        and    $0x7,%ecx ;\\\n+        add    $0x3,%ecx ;\\\n+        cmp    %r10d,%ecx ;\\\n+        pop    %rcx ;\\\n+        jl     RLABEL(reg, op, 4, i);\\\n+        mov    %##reg,%rdi ;\\\n+        jmp    __asan_report_##op##4_asm ;\\\n+\n+#define ASAN_MEMORY_ACCESS_CALLBACK_ADD_1(reg, op) \\\n+BEGINF(reg, op, 1, add) ;\\\n+        ASAN_MEMORY_ACCESS_INITIAL_CHECK_ADD(reg, op, 1) ;\\\n+        ASAN_MEMORY_ACCESS_EXTRA_CHECK_1(reg, op, add) ;\\\n+ENDF\n+\n+#define ASAN_MEMORY_ACCESS_CALLBACK_ADD_2(reg, op) \\\n+BEGINF(reg, op, 2, add) ;\\\n+        ASAN_MEMORY_ACCESS_INITIAL_CHECK_ADD(reg, op, 2) ;\\\n+        ASAN_MEMORY_ACCESS_EXTRA_CHECK_2(reg, op, add) ;\\\n+ENDF\n+\n+#define ASAN_MEMORY_ACCESS_CALLBACK_ADD_4(reg, op) \\\n+BEGINF(reg, op, 4, add) ;\\\n+        ASAN_MEMORY_ACCESS_INITIAL_CHECK_ADD(reg, op, 4) ;\\\n+        ASAN_MEMORY_ACCESS_EXTRA_CHECK_4(reg, op, add) ;\\\n+ENDF\n+\n+// Access check functions for 8 and 16 byte types: no extra checks required.\n+#define ASAN_MEMORY_ACCESS_CHECK_ADD(reg, op, s, c) \\\n+        mov    %##reg,%r10 ;\\\n+        shr    $0x3,%r10 ;\\\n+        ##c    $0x0,ASAN_SHADOW_OFFSET_CONST(%r10) ;\\\n+        jne    FLABEL(reg, op, s, add) ;\\\n+        retq  ;\\\n+\n+#define ASAN_MEMORY_ACCESS_FAIL(reg, op, s, i) \\\n+FLABEL(reg, op, s, i): ;\\\n+        mov    %##reg,%rdi ;\\\n+        jmp    __asan_report_##op##s##_asm;\\\n+\n+#define ASAN_MEMORY_ACCESS_CALLBACK_ADD_8(reg, op) \\\n+BEGINF(reg, op, 8, add) ;\\\n+        ASAN_MEMORY_ACCESS_CHECK_ADD(reg, op, 8, cmpb) ;\\\n+        ASAN_MEMORY_ACCESS_FAIL(reg, op, 8, add) ;\\\n+ENDF\n+\n+#define ASAN_MEMORY_ACCESS_CALLBACK_ADD_16(reg, op) \\\n+BEGINF(reg, op, 16, add) ;\\\n+        ASAN_MEMORY_ACCESS_CHECK_ADD(reg, op, 16, cmpw) ;\\\n+        ASAN_MEMORY_ACCESS_FAIL(reg, op, 16, add) ;\\\n+ENDF\n+\n+#define ASAN_MEMORY_ACCESS_CALLBACKS_ADD(reg) \\\n+ASAN_MEMORY_ACCESS_CALLBACK_ADD_1(reg, load) \\\n+ASAN_MEMORY_ACCESS_CALLBACK_ADD_1(reg, store) \\\n+ASAN_MEMORY_ACCESS_CALLBACK_ADD_2(reg, load) \\\n+ASAN_MEMORY_ACCESS_CALLBACK_ADD_2(reg, store) \\\n+ASAN_MEMORY_ACCESS_CALLBACK_ADD_4(reg, load) \\\n+ASAN_MEMORY_ACCESS_CALLBACK_ADD_4(reg, store) \\\n+ASAN_MEMORY_ACCESS_CALLBACK_ADD_8(reg, load) \\\n+ASAN_MEMORY_ACCESS_CALLBACK_ADD_8(reg, store) \\\n+ASAN_MEMORY_ACCESS_CALLBACK_ADD_16(reg, load) \\\n+ASAN_MEMORY_ACCESS_CALLBACK_ADD_16(reg, store) \\\n+\n+\n+// Instantiate all but R10 and R11 callbacks. We are using PLTSafe class with\n+// the intrinsic, which guarantees that the code generation will never emit\n+// R10 or R11 callback.\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(RAX)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(RBX)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(RCX)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(RDX)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(RSI)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(RDI)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(RBP)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(R8)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(R9)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(R12)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(R13)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(R14)\n+ASAN_MEMORY_ACCESS_CALLBACKS_ADD(R15)\n+\n+#endif\n+\n+NO_EXEC_STACK_DIRECTIVE"}, {"sha": "c15963e141832768a9eeaf01123045a658fe85be", "filename": "libsanitizer/asan/asan_thread.cpp", "status": "modified", "additions": 9, "deletions": 12, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_thread.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_thread.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_thread.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -83,8 +83,7 @@ AsanThread *AsanThread::Create(thread_callback_t start_routine, void *arg,\n   thread->start_routine_ = start_routine;\n   thread->arg_ = arg;\n   AsanThreadContext::CreateThreadContextArgs args = {thread, stack};\n-  asanThreadRegistry().CreateThread(*reinterpret_cast<uptr *>(thread), detached,\n-                                    parent_tid, &args);\n+  asanThreadRegistry().CreateThread(0, detached, parent_tid, &args);\n \n   return thread;\n }\n@@ -306,7 +305,7 @@ void AsanThread::SetThreadStackAndTls(const InitOptions *options) {\n   uptr stack_size = 0;\n   GetThreadStackAndTls(tid() == kMainTid, &stack_bottom_, &stack_size,\n                        &tls_begin_, &tls_size);\n-  stack_top_ = RoundDownTo(stack_bottom_ + stack_size, SHADOW_GRANULARITY);\n+  stack_top_ = RoundDownTo(stack_bottom_ + stack_size, ASAN_SHADOW_GRANULARITY);\n   tls_end_ = tls_begin_ + tls_size;\n   dtls_ = DTLS_Get();\n \n@@ -322,11 +321,9 @@ void AsanThread::ClearShadowForThreadStackAndTLS() {\n   if (stack_top_ != stack_bottom_)\n     PoisonShadow(stack_bottom_, stack_top_ - stack_bottom_, 0);\n   if (tls_begin_ != tls_end_) {\n-    uptr tls_begin_aligned = RoundDownTo(tls_begin_, SHADOW_GRANULARITY);\n-    uptr tls_end_aligned = RoundUpTo(tls_end_, SHADOW_GRANULARITY);\n-    FastPoisonShadowPartialRightRedzone(tls_begin_aligned,\n-                                        tls_end_ - tls_begin_aligned,\n-                                        tls_end_aligned - tls_end_, 0);\n+    uptr tls_begin_aligned = RoundDownTo(tls_begin_, ASAN_SHADOW_GRANULARITY);\n+    uptr tls_end_aligned = RoundUpTo(tls_end_, ASAN_SHADOW_GRANULARITY);\n+    FastPoisonShadow(tls_begin_aligned, tls_end_aligned - tls_begin_aligned, 0);\n   }\n }\n \n@@ -347,27 +344,27 @@ bool AsanThread::GetStackFrameAccessByAddr(uptr addr,\n     return true;\n   }\n   uptr aligned_addr = RoundDownTo(addr, SANITIZER_WORDSIZE / 8);  // align addr.\n-  uptr mem_ptr = RoundDownTo(aligned_addr, SHADOW_GRANULARITY);\n+  uptr mem_ptr = RoundDownTo(aligned_addr, ASAN_SHADOW_GRANULARITY);\n   u8 *shadow_ptr = (u8*)MemToShadow(aligned_addr);\n   u8 *shadow_bottom = (u8*)MemToShadow(bottom);\n \n   while (shadow_ptr >= shadow_bottom &&\n          *shadow_ptr != kAsanStackLeftRedzoneMagic) {\n     shadow_ptr--;\n-    mem_ptr -= SHADOW_GRANULARITY;\n+    mem_ptr -= ASAN_SHADOW_GRANULARITY;\n   }\n \n   while (shadow_ptr >= shadow_bottom &&\n          *shadow_ptr == kAsanStackLeftRedzoneMagic) {\n     shadow_ptr--;\n-    mem_ptr -= SHADOW_GRANULARITY;\n+    mem_ptr -= ASAN_SHADOW_GRANULARITY;\n   }\n \n   if (shadow_ptr < shadow_bottom) {\n     return false;\n   }\n \n-  uptr* ptr = (uptr*)(mem_ptr + SHADOW_GRANULARITY);\n+  uptr *ptr = (uptr *)(mem_ptr + ASAN_SHADOW_GRANULARITY);\n   CHECK(ptr[0] == kCurrentStackFrameMagic);\n   access->offset = addr - (uptr)ptr;\n   access->frame_pc = ptr[2];"}, {"sha": "81958038fb1ce9aa0eb6f8b5f6ad1996b9785a68", "filename": "libsanitizer/asan/asan_win.cpp", "status": "modified", "additions": 21, "deletions": 19, "changes": 40, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_win.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_win.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_win.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -1,4 +1,5 @@\n-//===-- asan_win.cpp ------------------------------------------------------===//\n+//===-- asan_win.cpp\n+//------------------------------------------------------===//>\n //\n // Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n // See https://llvm.org/LICENSE.txt for license information.\n@@ -13,21 +14,20 @@\n \n #include \"sanitizer_common/sanitizer_platform.h\"\n #if SANITIZER_WINDOWS\n-#define WIN32_LEAN_AND_MEAN\n-#include <windows.h>\n-\n-#include <stdlib.h>\n-\n-#include \"asan_interceptors.h\"\n-#include \"asan_internal.h\"\n-#include \"asan_mapping.h\"\n-#include \"asan_report.h\"\n-#include \"asan_stack.h\"\n-#include \"asan_thread.h\"\n-#include \"sanitizer_common/sanitizer_libc.h\"\n-#include \"sanitizer_common/sanitizer_mutex.h\"\n-#include \"sanitizer_common/sanitizer_win.h\"\n-#include \"sanitizer_common/sanitizer_win_defs.h\"\n+#  define WIN32_LEAN_AND_MEAN\n+#  include <stdlib.h>\n+#  include <windows.h>\n+\n+#  include \"asan_interceptors.h\"\n+#  include \"asan_internal.h\"\n+#  include \"asan_mapping.h\"\n+#  include \"asan_report.h\"\n+#  include \"asan_stack.h\"\n+#  include \"asan_thread.h\"\n+#  include \"sanitizer_common/sanitizer_libc.h\"\n+#  include \"sanitizer_common/sanitizer_mutex.h\"\n+#  include \"sanitizer_common/sanitizer_win.h\"\n+#  include \"sanitizer_common/sanitizer_win_defs.h\"\n \n using namespace __asan;\n \n@@ -49,8 +49,8 @@ uptr __asan_get_shadow_memory_dynamic_address() {\n static LPTOP_LEVEL_EXCEPTION_FILTER default_seh_handler;\n static LPTOP_LEVEL_EXCEPTION_FILTER user_seh_handler;\n \n-extern \"C\" SANITIZER_INTERFACE_ATTRIBUTE\n-long __asan_unhandled_exception_filter(EXCEPTION_POINTERS *info) {\n+extern \"C\" SANITIZER_INTERFACE_ATTRIBUTE long __asan_unhandled_exception_filter(\n+    EXCEPTION_POINTERS *info) {\n   EXCEPTION_RECORD *exception_record = info->ExceptionRecord;\n   CONTEXT *context = info->ContextRecord;\n \n@@ -187,6 +187,8 @@ void InitializePlatformInterceptors() {\n   }\n }\n \n+void InstallAtExitCheckLeaks() {}\n+\n void AsanApplyToGlobals(globals_op_fptr op, const void *needle) {\n   UNIMPLEMENTED();\n }\n@@ -253,7 +255,7 @@ void *AsanDoesNotSupportStaticLinkage() {\n }\n \n uptr FindDynamicShadowStart() {\n-  return MapDynamicShadow(MemToShadowSize(kHighMemEnd), SHADOW_SCALE,\n+  return MapDynamicShadow(MemToShadowSize(kHighMemEnd), ASAN_SHADOW_SCALE,\n                           /*min_shadow_base_alignment*/ 0, kHighMemEnd);\n }\n "}, {"sha": "e3a90f18ed81ac4be2d5bd9d744f9e43dae8aadb", "filename": "libsanitizer/asan/asan_win_dll_thunk.cpp", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_win_dll_thunk.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fasan%2Fasan_win_dll_thunk.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_win_dll_thunk.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -56,6 +56,13 @@ INTERCEPT_WRAP_W_W(_expand_dbg)\n \n // TODO(timurrrr): Do we need to add _Crt* stuff here? (see asan_malloc_win.cpp)\n \n+#  if defined(_MSC_VER) && !defined(__clang__)\n+// Disable warnings such as: 'void memchr(void)': incorrect number of arguments\n+// for intrinsic function, expected '3' arguments.\n+#    pragma warning(push)\n+#    pragma warning(disable : 4392)\n+#  endif\n+\n INTERCEPT_LIBRARY_FUNCTION(atoi);\n INTERCEPT_LIBRARY_FUNCTION(atol);\n INTERCEPT_LIBRARY_FUNCTION(frexp);\n@@ -87,6 +94,10 @@ INTERCEPT_LIBRARY_FUNCTION(strtol);\n INTERCEPT_LIBRARY_FUNCTION(wcslen);\n INTERCEPT_LIBRARY_FUNCTION(wcsnlen);\n \n+#  if defined(_MSC_VER) && !defined(__clang__)\n+#    pragma warning(pop)\n+#  endif\n+\n #ifdef _WIN64\n INTERCEPT_LIBRARY_FUNCTION(__C_specific_handler);\n #else"}, {"sha": "69a3d8620f9241ad187b284de89c188324f73a40", "filename": "libsanitizer/builtins/assembly.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fbuiltins%2Fassembly.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fbuiltins%2Fassembly.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fbuiltins%2Fassembly.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -14,6 +14,12 @@\n #ifndef COMPILERRT_ASSEMBLY_H\n #define COMPILERRT_ASSEMBLY_H\n \n+#if defined(__linux__) && defined(__CET__)\n+#if __has_include(<cet.h>)\n+#include <cet.h>\n+#endif\n+#endif\n+\n #if defined(__APPLE__) && defined(__aarch64__)\n #define SEPARATOR %%\n #else"}, {"sha": "f8725a17343265c264ba6316fdbdd5478db3ea1c", "filename": "libsanitizer/hwasan/hwasan.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fhwasan%2Fhwasan.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -25,6 +25,7 @@\n #include \"sanitizer_common/sanitizer_common.h\"\n #include \"sanitizer_common/sanitizer_flag_parser.h\"\n #include \"sanitizer_common/sanitizer_flags.h\"\n+#include \"sanitizer_common/sanitizer_interface_internal.h\"\n #include \"sanitizer_common/sanitizer_libc.h\"\n #include \"sanitizer_common/sanitizer_procmaps.h\"\n #include \"sanitizer_common/sanitizer_stackdepot.h\""}, {"sha": "3cc2fc40b5f68f59bf8383aa877e1008bfe760be", "filename": "libsanitizer/hwasan/hwasan.h", "status": "modified", "additions": 0, "deletions": 15, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fhwasan%2Fhwasan.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -172,21 +172,6 @@ void HwasanTagMismatch(uptr addr, uptr access_info, uptr *registers_frame,\n \n }  // namespace __hwasan\n \n-#define HWASAN_MALLOC_HOOK(ptr, size)       \\\n-  do {                                    \\\n-    if (&__sanitizer_malloc_hook) {       \\\n-      __sanitizer_malloc_hook(ptr, size); \\\n-    }                                     \\\n-    RunMallocHooks(ptr, size);            \\\n-  } while (false)\n-#define HWASAN_FREE_HOOK(ptr)       \\\n-  do {                            \\\n-    if (&__sanitizer_free_hook) { \\\n-      __sanitizer_free_hook(ptr); \\\n-    }                             \\\n-    RunFreeHooks(ptr);            \\\n-  } while (false)\n-\n #if HWASAN_WITH_INTERCEPTORS\n // For both bionic and glibc __sigset_t is an unsigned long.\n typedef unsigned long __hw_sigset_t;"}, {"sha": "842455150c7b3362f3940e14e10f69b544c75e4f", "filename": "libsanitizer/hwasan/hwasan_allocator.cpp", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan_allocator.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan_allocator.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fhwasan%2Fhwasan_allocator.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -132,6 +132,11 @@ static void *HwasanAllocate(StackTrace *stack, uptr orig_size, uptr alignment,\n     }\n     ReportAllocationSizeTooBig(orig_size, kMaxAllowedMallocSize, stack);\n   }\n+  if (UNLIKELY(IsRssLimitExceeded())) {\n+    if (AllocatorMayReturnNull())\n+      return nullptr;\n+    ReportRssLimitExceeded(stack);\n+  }\n \n   alignment = Max(alignment, kShadowAlignment);\n   uptr size = TaggedSize(orig_size);\n@@ -194,7 +199,7 @@ static void *HwasanAllocate(StackTrace *stack, uptr orig_size, uptr alignment,\n     }\n   }\n \n-  HWASAN_MALLOC_HOOK(user_ptr, size);\n+  RunMallocHooks(user_ptr, size);\n   return user_ptr;\n }\n \n@@ -221,7 +226,7 @@ static bool CheckInvalidFree(StackTrace *stack, void *untagged_ptr,\n \n static void HwasanDeallocate(StackTrace *stack, void *tagged_ptr) {\n   CHECK(tagged_ptr);\n-  HWASAN_FREE_HOOK(tagged_ptr);\n+  RunFreeHooks(tagged_ptr);\n \n   bool in_taggable_region =\n       InTaggableRegion(reinterpret_cast<uptr>(tagged_ptr));"}, {"sha": "8dc886e587e75010f10acebc0f2bac831a92fcf8", "filename": "libsanitizer/hwasan/hwasan_interceptors.cpp", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan_interceptors.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan_interceptors.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fhwasan%2Fhwasan_interceptors.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -47,6 +47,12 @@ INTERCEPTOR(int, pthread_create, void *th, void *attr, void *(*callback)(void*),\n   return res;\n }\n \n+INTERCEPTOR(int, pthread_join, void *t, void **arg) {\n+  return REAL(pthread_join)(t, arg);\n+}\n+\n+DEFINE_REAL_PTHREAD_FUNCTIONS\n+\n DEFINE_REAL(int, vfork)\n DECLARE_EXTERN_INTERCEPTOR_AND_WRAPPER(int, vfork)\n \n@@ -189,7 +195,8 @@ void InitializeInterceptors() {\n   INTERCEPT_FUNCTION(vfork);\n #endif  // __linux__\n   INTERCEPT_FUNCTION(pthread_create);\n-#endif\n+  INTERCEPT_FUNCTION(pthread_join);\n+#  endif\n \n   inited = 1;\n }"}, {"sha": "ea7f5ce40b0744ed6b51eb51fd505f5ee12d8f54", "filename": "libsanitizer/hwasan/hwasan_memintrinsics.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan_memintrinsics.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan_memintrinsics.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fhwasan%2Fhwasan_memintrinsics.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -40,5 +40,5 @@ void *__hwasan_memmove(void *to, const void *from, uptr size) {\n       reinterpret_cast<uptr>(to), size);\n   CheckAddressSized<ErrorAction::Recover, AccessType::Load>(\n       reinterpret_cast<uptr>(from), size);\n-  return memmove(UntagPtr(to), UntagPtr(from), size);\n+  return memmove(to, from, size);\n }"}, {"sha": "495046a754f107153a3fcf16a167b2bd60db8820", "filename": "libsanitizer/hwasan/hwasan_new_delete.cpp", "status": "modified", "additions": 33, "deletions": 30, "changes": 63, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan_new_delete.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan_new_delete.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fhwasan%2Fhwasan_new_delete.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -22,30 +22,32 @@\n #if HWASAN_REPLACE_OPERATORS_NEW_AND_DELETE\n \n // TODO(alekseys): throw std::bad_alloc instead of dying on OOM.\n-#define OPERATOR_NEW_BODY(nothrow) \\\n-  GET_MALLOC_STACK_TRACE; \\\n-  void *res = hwasan_malloc(size, &stack);\\\n-  if (!nothrow && UNLIKELY(!res)) ReportOutOfMemory(size, &stack);\\\n-  return res\n-#define OPERATOR_NEW_ALIGN_BODY(nothrow)                                    \\\n-  GET_MALLOC_STACK_TRACE;                                                   \\\n-  void *res = hwasan_aligned_alloc(static_cast<uptr>(align), size, &stack); \\\n-  if (!nothrow && UNLIKELY(!res))                                           \\\n-    ReportOutOfMemory(size, &stack);                                        \\\n-  return res\n-\n-#define OPERATOR_DELETE_BODY \\\n-  GET_MALLOC_STACK_TRACE; \\\n-  if (ptr) hwasan_free(ptr, &stack)\n+#  define OPERATOR_NEW_BODY(nothrow)         \\\n+    GET_MALLOC_STACK_TRACE;                  \\\n+    void *res = hwasan_malloc(size, &stack); \\\n+    if (!nothrow && UNLIKELY(!res))          \\\n+      ReportOutOfMemory(size, &stack);       \\\n+    return res\n+#  define OPERATOR_NEW_ALIGN_BODY(nothrow)                               \\\n+    GET_MALLOC_STACK_TRACE;                                              \\\n+    void *res = hwasan_memalign(static_cast<uptr>(align), size, &stack); \\\n+    if (!nothrow && UNLIKELY(!res))                                      \\\n+      ReportOutOfMemory(size, &stack);                                   \\\n+    return res\n+\n+#  define OPERATOR_DELETE_BODY \\\n+    GET_MALLOC_STACK_TRACE;    \\\n+    if (ptr)                   \\\n+    hwasan_free(ptr, &stack)\n \n #elif defined(__ANDROID__)\n \n // We don't actually want to intercept operator new and delete on Android, but\n // since we previously released a runtime that intercepted these functions,\n // removing the interceptors would break ABI. Therefore we simply forward to\n // malloc and free.\n-#define OPERATOR_NEW_BODY(nothrow) return malloc(size)\n-#define OPERATOR_DELETE_BODY free(ptr)\n+#  define OPERATOR_NEW_BODY(nothrow) return malloc(size)\n+#  define OPERATOR_DELETE_BODY free(ptr)\n \n #endif\n \n@@ -55,26 +57,27 @@ using namespace __hwasan;\n \n // Fake std::nothrow_t to avoid including <new>.\n namespace std {\n-  struct nothrow_t {};\n+struct nothrow_t {};\n }  // namespace std\n \n-\n-\n-INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-void *operator new(size_t size) { OPERATOR_NEW_BODY(false /*nothrow*/); }\n-INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-void *operator new[](size_t size) { OPERATOR_NEW_BODY(false /*nothrow*/); }\n-INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-void *operator new(size_t size, std::nothrow_t const&) {\n+INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void *operator new(size_t size) {\n+  OPERATOR_NEW_BODY(false /*nothrow*/);\n+}\n+INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void *operator new[](\n+    size_t size) {\n+  OPERATOR_NEW_BODY(false /*nothrow*/);\n+}\n+INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void *operator new(\n+    size_t size, std::nothrow_t const &) {\n   OPERATOR_NEW_BODY(true /*nothrow*/);\n }\n-INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-void *operator new[](size_t size, std::nothrow_t const&) {\n+INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void *operator new[](\n+    size_t size, std::nothrow_t const &) {\n   OPERATOR_NEW_BODY(true /*nothrow*/);\n }\n \n-INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void operator delete(void *ptr)\n-    NOEXCEPT {\n+INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void operator delete(\n+    void *ptr) NOEXCEPT {\n   OPERATOR_DELETE_BODY;\n }\n INTERCEPTOR_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void operator delete[]("}, {"sha": "8c9c95f413be3f5cffce0ae9eff6f58dc9247744", "filename": "libsanitizer/hwasan/hwasan_preinit.cpp", "status": "added", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan_preinit.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fhwasan%2Fhwasan_preinit.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fhwasan%2Fhwasan_preinit.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -0,0 +1,23 @@\n+//===-- hwasan_preinit.cpp ------------------------------------------------===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+//\n+// This file is a part of HWAddressSanitizer, an address sanity checker.\n+//\n+// Call __hwasan_init at the very early stage of process startup.\n+//===----------------------------------------------------------------------===//\n+#include \"hwasan_interface_internal.h\"\n+#include \"sanitizer_common/sanitizer_internal_defs.h\"\n+\n+#if SANITIZER_CAN_USE_PREINIT_ARRAY\n+// The symbol is called __local_hwasan_preinit, because it's not intended to\n+// be exported.\n+// This code linked into the main executable when -fsanitize=hwaddress is in\n+// the link flags. It can only use exported interface functions.\n+__attribute__((section(\".preinit_array\"), used)) static void (\n+    *__local_hwasan_preinit)(void) = __hwasan_init;\n+#endif"}, {"sha": "ba58ad46f32d0cf6a084c0ab5784e7b09011f3a4", "filename": "libsanitizer/include/sanitizer/common_interface_defs.h", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Finclude%2Fsanitizer%2Fcommon_interface_defs.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Finclude%2Fsanitizer%2Fcommon_interface_defs.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Finclude%2Fsanitizer%2Fcommon_interface_defs.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -211,6 +211,15 @@ void __sanitizer_symbolize_pc(void *pc, const char *fmt, char *out_buf,\n // Same as __sanitizer_symbolize_pc, but for data section (i.e. globals).\n void __sanitizer_symbolize_global(void *data_ptr, const char *fmt,\n                                   char *out_buf, size_t out_buf_size);\n+// Determine the return address.\n+#if !defined(_MSC_VER) || defined(__clang__)\n+#define __sanitizer_return_address()                                           \\\n+  __builtin_extract_return_addr(__builtin_return_address(0))\n+#else\n+extern \"C\" void *_ReturnAddress(void);\n+#pragma intrinsic(_ReturnAddress)\n+#define __sanitizer_return_address() _ReturnAddress()\n+#endif\n \n /// Sets the callback to be called immediately before death on error.\n ///"}, {"sha": "8e581a67572d3287b5ea792b3af3f417082a3985", "filename": "libsanitizer/include/sanitizer/dfsan_interface.h", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Finclude%2Fsanitizer%2Fdfsan_interface.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Finclude%2Fsanitizer%2Fdfsan_interface.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Finclude%2Fsanitizer%2Fdfsan_interface.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -27,6 +27,10 @@ typedef uint32_t dfsan_origin;\n /// Signature of the callback argument to dfsan_set_write_callback().\n typedef void (*dfsan_write_callback_t)(int fd, const void *buf, size_t count);\n \n+/// Signature of the callback argument to dfsan_set_conditional_callback().\n+typedef void (*dfsan_conditional_callback_t)(dfsan_label label,\n+                                             dfsan_origin origin);\n+\n /// Computes the union of \\c l1 and \\c l2, resulting in a union label.\n dfsan_label dfsan_union(dfsan_label l1, dfsan_label l2);\n \n@@ -54,6 +58,10 @@ dfsan_origin dfsan_get_origin(long data);\n /// Retrieves the label associated with the data at the given address.\n dfsan_label dfsan_read_label(const void *addr, size_t size);\n \n+/// Return the origin associated with the first taint byte in the size bytes\n+/// from the address addr.\n+dfsan_origin dfsan_read_origin_of_first_taint(const void *addr, size_t size);\n+\n /// Returns whether the given label label contains the label elem.\n int dfsan_has_label(dfsan_label label, dfsan_label elem);\n \n@@ -70,6 +78,19 @@ void dfsan_flush(void);\n /// callback executes.  Pass in NULL to remove any callback.\n void dfsan_set_write_callback(dfsan_write_callback_t labeled_write_callback);\n \n+/// Sets a callback to be invoked on any conditional expressions which have a\n+/// taint label set. This can be used to find where tainted data influences\n+/// the behavior of the program.\n+/// These callbacks will only be added when -dfsan-conditional-callbacks=true.\n+void dfsan_set_conditional_callback(dfsan_conditional_callback_t callback);\n+\n+/// Conditional expressions occur during signal handlers.\n+/// Making callbacks that handle signals well is tricky, so when\n+/// -dfsan-conditional-callbacks=true, conditional expressions used in signal\n+/// handlers will add the labels they see into a global (bitwise-or together).\n+/// This function returns all label bits seen in signal handler conditions.\n+dfsan_label dfsan_get_labels_in_signal_conditional();\n+\n /// Interceptor hooks.\n /// Whenever a dfsan's custom function is called the corresponding\n /// hook is called it non-zero. The hooks should be defined by the user.\n@@ -87,6 +108,9 @@ void dfsan_weak_hook_strncmp(void *caller_pc, const char *s1, const char *s2,\n /// prints description at the beginning of the trace. If origin tracking is not\n /// on, or the address is not labeled, it prints nothing.\n void dfsan_print_origin_trace(const void *addr, const char *description);\n+/// As above, but use an origin id from dfsan_get_origin() instead of address.\n+/// Does not include header line with taint label and address information.\n+void dfsan_print_origin_id_trace(dfsan_origin origin);\n \n /// Prints the origin trace of the label at the address \\p addr to a\n /// pre-allocated output buffer. If origin tracking is not on, or the address is\n@@ -124,6 +148,10 @@ void dfsan_print_origin_trace(const void *addr, const char *description);\n /// return value is not less than \\p out_buf_size.\n size_t dfsan_sprint_origin_trace(const void *addr, const char *description,\n                                  char *out_buf, size_t out_buf_size);\n+/// As above, but use an origin id from dfsan_get_origin() instead of address.\n+/// Does not include header line with taint label and address information.\n+size_t dfsan_sprint_origin_id_trace(dfsan_origin origin, char *out_buf,\n+                                    size_t out_buf_size);\n \n /// Prints the stack trace leading to this call to a pre-allocated output\n /// buffer."}, {"sha": "10b893391f47a1726f4e483f822f836d8d085491", "filename": "libsanitizer/interception/interception_win.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Finterception%2Finterception_win.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Finterception%2Finterception_win.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Finterception%2Finterception_win.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -401,6 +401,7 @@ static uptr AllocateMemoryForTrampoline(uptr image_address, size_t size) {\n // The following prologues cannot be patched because of the short jump\n // jumping to the patching region.\n \n+#if SANITIZER_WINDOWS64\n // ntdll!wcslen in Win11\n //   488bc1          mov     rax,rcx\n //   0fb710          movzx   edx,word ptr [rax]\n@@ -422,6 +423,7 @@ static const u8 kPrologueWithShortJump2[] = {\n     0x4c, 0x8b, 0xc1, 0x8a, 0x01, 0x48, 0xff, 0xc1,\n     0x84, 0xc0, 0x75, 0xf7,\n };\n+#endif\n \n // Returns 0 on error.\n static size_t GetInstructionSize(uptr address, size_t* rel_offset = nullptr) {\n@@ -602,6 +604,7 @@ static size_t GetInstructionSize(uptr address, size_t* rel_offset = nullptr) {\n     case 0x246c8948:  // 48 89 6C 24 XX : mov QWORD ptr [rsp + XX], rbp\n     case 0x245c8948:  // 48 89 5c 24 XX : mov QWORD PTR [rsp + XX], rbx\n     case 0x24748948:  // 48 89 74 24 XX : mov QWORD PTR [rsp + XX], rsi\n+    case 0x247c8948:  // 48 89 7c 24 XX : mov QWORD PTR [rsp + XX], rdi\n     case 0x244C8948:  // 48 89 4C 24 XX : mov QWORD PTR [rsp + XX], rcx\n     case 0x24548948:  // 48 89 54 24 XX : mov QWORD PTR [rsp + XX], rdx\n     case 0x244c894c:  // 4c 89 4c 24 XX : mov QWORD PTR [rsp + XX], r9"}, {"sha": "489c5ca01fed85bc110bea76227d0bd3fde2ec8a", "filename": "libsanitizer/lsan/lsan.cpp", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -13,11 +13,12 @@\n \n #include \"lsan.h\"\n \n-#include \"sanitizer_common/sanitizer_flags.h\"\n-#include \"sanitizer_common/sanitizer_flag_parser.h\"\n #include \"lsan_allocator.h\"\n #include \"lsan_common.h\"\n #include \"lsan_thread.h\"\n+#include \"sanitizer_common/sanitizer_flag_parser.h\"\n+#include \"sanitizer_common/sanitizer_flags.h\"\n+#include \"sanitizer_common/sanitizer_interface_internal.h\"\n \n bool lsan_inited;\n bool lsan_init_is_running;\n@@ -99,9 +100,7 @@ extern \"C\" void __lsan_init() {\n   InitializeThreadRegistry();\n   InstallDeadlySignalHandlers(LsanOnDeadlySignal);\n   InitializeMainThread();\n-\n-  if (common_flags()->detect_leaks && common_flags()->leak_check_at_exit)\n-    Atexit(DoLeakCheck);\n+  InstallAtExitCheckLeaks();\n \n   InitializeCoverage(common_flags()->coverage, common_flags()->coverage_dir);\n "}, {"sha": "757edec8e104f906206520ce6207d99b6a02cd06", "filename": "libsanitizer/lsan/lsan.h", "status": "modified", "additions": 14, "deletions": 12, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -13,17 +13,17 @@\n \n #include \"lsan_thread.h\"\n #if SANITIZER_POSIX\n-#include \"lsan_posix.h\"\n+#  include \"lsan_posix.h\"\n #elif SANITIZER_FUCHSIA\n-#include \"lsan_fuchsia.h\"\n+#  include \"lsan_fuchsia.h\"\n #endif\n #include \"sanitizer_common/sanitizer_flags.h\"\n #include \"sanitizer_common/sanitizer_stacktrace.h\"\n \n-#define GET_STACK_TRACE(max_size, fast)                       \\\n-  __sanitizer::BufferedStackTrace stack;                      \\\n-  stack.Unwind(StackTrace::GetCurrentPc(),                    \\\n-               GET_CURRENT_FRAME(), nullptr, fast, max_size);\n+#define GET_STACK_TRACE(max_size, fast)                                        \\\n+  __sanitizer::BufferedStackTrace stack;                                       \\\n+  stack.Unwind(StackTrace::GetCurrentPc(), GET_CURRENT_FRAME(), nullptr, fast, \\\n+               max_size);\n \n #define GET_STACK_TRACE_FATAL \\\n   GET_STACK_TRACE(kStackTraceMax, common_flags()->fast_unwind_on_fatal)\n@@ -39,12 +39,14 @@ namespace __lsan {\n void InitializeInterceptors();\n void ReplaceSystemMalloc();\n void LsanOnDeadlySignal(int signo, void *siginfo, void *context);\n-\n-#define ENSURE_LSAN_INITED do {   \\\n-  CHECK(!lsan_init_is_running);   \\\n-  if (!lsan_inited)               \\\n-    __lsan_init();                \\\n-} while (0)\n+void InstallAtExitCheckLeaks();\n+\n+#define ENSURE_LSAN_INITED        \\\n+  do {                            \\\n+    CHECK(!lsan_init_is_running); \\\n+    if (!lsan_inited)             \\\n+      __lsan_init();              \\\n+  } while (0)\n \n }  // namespace __lsan\n "}, {"sha": "b4fd7e904be0f7cbea8c833fcb7992b8df1c55fa", "filename": "libsanitizer/lsan/lsan_allocator.cpp", "status": "modified", "additions": 8, "deletions": 17, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_allocator.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_allocator.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_allocator.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -27,11 +27,11 @@ extern \"C\" void *memset(void *ptr, int value, uptr num);\n \n namespace __lsan {\n #if defined(__i386__) || defined(__arm__)\n-static const uptr kMaxAllowedMallocSize = 1UL << 30;\n+static const uptr kMaxAllowedMallocSize = 1ULL << 30;\n #elif defined(__mips64) || defined(__aarch64__)\n-static const uptr kMaxAllowedMallocSize = 4UL << 30;\n+static const uptr kMaxAllowedMallocSize = 4ULL << 30;\n #else\n-static const uptr kMaxAllowedMallocSize = 8UL << 30;\n+static const uptr kMaxAllowedMallocSize = 8ULL << 30;\n #endif\n \n static Allocator allocator;\n@@ -88,6 +88,11 @@ void *Allocate(const StackTrace &stack, uptr size, uptr alignment,\n     size = 1;\n   if (size > max_malloc_size)\n     return ReportAllocationSizeTooBig(size, stack);\n+  if (UNLIKELY(IsRssLimitExceeded())) {\n+    if (AllocatorMayReturnNull())\n+      return nullptr;\n+    ReportRssLimitExceeded(&stack);\n+  }\n   void *p = allocator.Allocate(GetAllocatorCache(), size, alignment);\n   if (UNLIKELY(!p)) {\n     SetAllocatorOutOfMemory();\n@@ -99,7 +104,6 @@ void *Allocate(const StackTrace &stack, uptr size, uptr alignment,\n   if (cleared && allocator.FromPrimary(p))\n     memset(p, 0, size);\n   RegisterAllocation(stack, p, size);\n-  if (&__sanitizer_malloc_hook) __sanitizer_malloc_hook(p, size);\n   RunMallocHooks(p, size);\n   return p;\n }\n@@ -115,7 +119,6 @@ static void *Calloc(uptr nmemb, uptr size, const StackTrace &stack) {\n }\n \n void Deallocate(void *p) {\n-  if (&__sanitizer_free_hook) __sanitizer_free_hook(p);\n   RunFreeHooks(p);\n   RegisterDeallocation(p);\n   allocator.Deallocate(GetAllocatorCache(), p);\n@@ -359,16 +362,4 @@ uptr __sanitizer_get_allocated_size(const void *p) {\n   return GetMallocUsableSize(p);\n }\n \n-#if !SANITIZER_SUPPORTS_WEAK_HOOKS\n-// Provide default (no-op) implementation of malloc hooks.\n-SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-void __sanitizer_malloc_hook(void *ptr, uptr size) {\n-  (void)ptr;\n-  (void)size;\n-}\n-SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-void __sanitizer_free_hook(void *ptr) {\n-  (void)ptr;\n-}\n-#endif\n } // extern \"C\""}, {"sha": "539330491b02ad8e540830b714a771a14f452bf3", "filename": "libsanitizer/lsan/lsan_allocator.h", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_allocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_allocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_allocator.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -66,12 +66,9 @@ template <typename AddressSpaceView>\n using PrimaryAllocatorASVT = SizeClassAllocator32<AP32<AddressSpaceView>>;\n using PrimaryAllocator = PrimaryAllocatorASVT<LocalAddressSpaceView>;\n #elif defined(__x86_64__) || defined(__powerpc64__) || defined(__s390x__)\n-# if SANITIZER_FUCHSIA\n+# if SANITIZER_FUCHSIA || defined(__powerpc64__)\n const uptr kAllocatorSpace = ~(uptr)0;\n const uptr kAllocatorSize  =  0x40000000000ULL;  // 4T.\n-# elif defined(__powerpc64__)\n-const uptr kAllocatorSpace = 0xa0000000000ULL;\n-const uptr kAllocatorSize  = 0x20000000000ULL;  // 2T.\n #elif defined(__s390x__)\n const uptr kAllocatorSpace = 0x40000000000ULL;\n const uptr kAllocatorSize = 0x40000000000ULL;  // 4T."}, {"sha": "8d1bf11fdab6262ad50cbefa52ef6cc471b2cd0d", "filename": "libsanitizer/lsan/lsan_common.cpp", "status": "modified", "additions": 243, "deletions": 238, "changes": 481, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_common.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_common.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_common.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -34,7 +34,6 @@ Mutex global_mutex;\n \n Flags lsan_flags;\n \n-\n void DisableCounterUnderflow() {\n   if (common_flags()->detect_leaks) {\n     Report(\"Unmatched call to __lsan_enable().\\n\");\n@@ -43,44 +42,48 @@ void DisableCounterUnderflow() {\n }\n \n void Flags::SetDefaults() {\n-#define LSAN_FLAG(Type, Name, DefaultValue, Description) Name = DefaultValue;\n-#include \"lsan_flags.inc\"\n-#undef LSAN_FLAG\n+#  define LSAN_FLAG(Type, Name, DefaultValue, Description) Name = DefaultValue;\n+#  include \"lsan_flags.inc\"\n+#  undef LSAN_FLAG\n }\n \n void RegisterLsanFlags(FlagParser *parser, Flags *f) {\n-#define LSAN_FLAG(Type, Name, DefaultValue, Description) \\\n-  RegisterFlag(parser, #Name, Description, &f->Name);\n-#include \"lsan_flags.inc\"\n-#undef LSAN_FLAG\n+#  define LSAN_FLAG(Type, Name, DefaultValue, Description) \\\n+    RegisterFlag(parser, #Name, Description, &f->Name);\n+#  include \"lsan_flags.inc\"\n+#  undef LSAN_FLAG\n }\n \n-#define LOG_POINTERS(...)                           \\\n-  do {                                              \\\n-    if (flags()->log_pointers) Report(__VA_ARGS__); \\\n-  } while (0)\n+#  define LOG_POINTERS(...)      \\\n+    do {                         \\\n+      if (flags()->log_pointers) \\\n+        Report(__VA_ARGS__);     \\\n+    } while (0)\n \n-#define LOG_THREADS(...)                           \\\n-  do {                                             \\\n-    if (flags()->log_threads) Report(__VA_ARGS__); \\\n-  } while (0)\n+#  define LOG_THREADS(...)      \\\n+    do {                        \\\n+      if (flags()->log_threads) \\\n+        Report(__VA_ARGS__);    \\\n+    } while (0)\n \n class LeakSuppressionContext {\n   bool parsed = false;\n   SuppressionContext context;\n   bool suppressed_stacks_sorted = true;\n   InternalMmapVector<u32> suppressed_stacks;\n+  const LoadedModule *suppress_module = nullptr;\n \n-  Suppression *GetSuppressionForAddr(uptr addr);\n   void LazyInit();\n+  Suppression *GetSuppressionForAddr(uptr addr);\n+  bool SuppressInvalid(const StackTrace &stack);\n+  bool SuppressByRule(const StackTrace &stack, uptr hit_count, uptr total_size);\n \n  public:\n   LeakSuppressionContext(const char *supprression_types[],\n                          int suppression_types_num)\n       : context(supprression_types, suppression_types_num) {}\n \n-  Suppression *GetSuppressionForStack(u32 stack_trace_id,\n-                                      const StackTrace &stack);\n+  bool Suppress(u32 stack_trace_id, uptr hit_count, uptr total_size);\n \n   const InternalMmapVector<u32> &GetSortedSuppressedStacks() {\n     if (!suppressed_stacks_sorted) {\n@@ -95,17 +98,17 @@ class LeakSuppressionContext {\n ALIGNED(64) static char suppression_placeholder[sizeof(LeakSuppressionContext)];\n static LeakSuppressionContext *suppression_ctx = nullptr;\n static const char kSuppressionLeak[] = \"leak\";\n-static const char *kSuppressionTypes[] = { kSuppressionLeak };\n+static const char *kSuppressionTypes[] = {kSuppressionLeak};\n static const char kStdSuppressions[] =\n-#if SANITIZER_SUPPRESS_LEAK_ON_PTHREAD_EXIT\n+#  if SANITIZER_SUPPRESS_LEAK_ON_PTHREAD_EXIT\n     // For more details refer to the SANITIZER_SUPPRESS_LEAK_ON_PTHREAD_EXIT\n     // definition.\n     \"leak:*pthread_exit*\\n\"\n-#endif  // SANITIZER_SUPPRESS_LEAK_ON_PTHREAD_EXIT\n-#if SANITIZER_MAC\n+#  endif  // SANITIZER_SUPPRESS_LEAK_ON_PTHREAD_EXIT\n+#  if SANITIZER_MAC\n     // For Darwin and os_log/os_trace: https://reviews.llvm.org/D35173\n     \"leak:*_os_trace*\\n\"\n-#endif\n+#  endif\n     // TLS leak in some glibc versions, described in\n     // https://sourceware.org/bugzilla/show_bug.cgi?id=12650.\n     \"leak:*tls_get_addr*\\n\";\n@@ -123,9 +126,93 @@ void LeakSuppressionContext::LazyInit() {\n     if (&__lsan_default_suppressions)\n       context.Parse(__lsan_default_suppressions());\n     context.Parse(kStdSuppressions);\n+    if (flags()->use_tls && flags()->use_ld_allocations)\n+      suppress_module = GetLinker();\n   }\n }\n \n+Suppression *LeakSuppressionContext::GetSuppressionForAddr(uptr addr) {\n+  Suppression *s = nullptr;\n+\n+  // Suppress by module name.\n+  const char *module_name = Symbolizer::GetOrInit()->GetModuleNameForPc(addr);\n+  if (!module_name)\n+    module_name = \"<unknown module>\";\n+  if (context.Match(module_name, kSuppressionLeak, &s))\n+    return s;\n+\n+  // Suppress by file or function name.\n+  SymbolizedStack *frames = Symbolizer::GetOrInit()->SymbolizePC(addr);\n+  for (SymbolizedStack *cur = frames; cur; cur = cur->next) {\n+    if (context.Match(cur->info.function, kSuppressionLeak, &s) ||\n+        context.Match(cur->info.file, kSuppressionLeak, &s)) {\n+      break;\n+    }\n+  }\n+  frames->ClearAll();\n+  return s;\n+}\n+\n+static uptr GetCallerPC(const StackTrace &stack) {\n+  // The top frame is our malloc/calloc/etc. The next frame is the caller.\n+  if (stack.size >= 2)\n+    return stack.trace[1];\n+  return 0;\n+}\n+\n+// On Linux, treats all chunks allocated from ld-linux.so as reachable, which\n+// covers dynamically allocated TLS blocks, internal dynamic loader's loaded\n+// modules accounting etc.\n+// Dynamic TLS blocks contain the TLS variables of dynamically loaded modules.\n+// They are allocated with a __libc_memalign() call in allocate_and_init()\n+// (elf/dl-tls.c). Glibc won't tell us the address ranges occupied by those\n+// blocks, but we can make sure they come from our own allocator by intercepting\n+// __libc_memalign(). On top of that, there is no easy way to reach them. Their\n+// addresses are stored in a dynamically allocated array (the DTV) which is\n+// referenced from the static TLS. Unfortunately, we can't just rely on the DTV\n+// being reachable from the static TLS, and the dynamic TLS being reachable from\n+// the DTV. This is because the initial DTV is allocated before our interception\n+// mechanism kicks in, and thus we don't recognize it as allocated memory. We\n+// can't special-case it either, since we don't know its size.\n+// Our solution is to include in the root set all allocations made from\n+// ld-linux.so (which is where allocate_and_init() is implemented). This is\n+// guaranteed to include all dynamic TLS blocks (and possibly other allocations\n+// which we don't care about).\n+// On all other platforms, this simply checks to ensure that the caller pc is\n+// valid before reporting chunks as leaked.\n+bool LeakSuppressionContext::SuppressInvalid(const StackTrace &stack) {\n+  uptr caller_pc = GetCallerPC(stack);\n+  // If caller_pc is unknown, this chunk may be allocated in a coroutine. Mark\n+  // it as reachable, as we can't properly report its allocation stack anyway.\n+  return !caller_pc ||\n+         (suppress_module && suppress_module->containsAddress(caller_pc));\n+}\n+\n+bool LeakSuppressionContext::SuppressByRule(const StackTrace &stack,\n+                                            uptr hit_count, uptr total_size) {\n+  for (uptr i = 0; i < stack.size; i++) {\n+    Suppression *s = GetSuppressionForAddr(\n+        StackTrace::GetPreviousInstructionPc(stack.trace[i]));\n+    if (s) {\n+      s->weight += total_size;\n+      atomic_fetch_add(&s->hit_count, hit_count, memory_order_relaxed);\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool LeakSuppressionContext::Suppress(u32 stack_trace_id, uptr hit_count,\n+                                      uptr total_size) {\n+  LazyInit();\n+  StackTrace stack = StackDepotGet(stack_trace_id);\n+  if (!SuppressInvalid(stack) && !SuppressByRule(stack, hit_count, total_size))\n+    return false;\n+  suppressed_stacks_sorted = false;\n+  suppressed_stacks.push_back(stack_trace_id);\n+  return true;\n+}\n+\n static LeakSuppressionContext *GetSuppressionContext() {\n   CHECK(suppression_ctx);\n   return suppression_ctx;\n@@ -146,9 +233,9 @@ void InitCommonLsan() {\n   }\n }\n \n-class Decorator: public __sanitizer::SanitizerCommonDecorator {\n+class Decorator : public __sanitizer::SanitizerCommonDecorator {\n  public:\n-  Decorator() : SanitizerCommonDecorator() { }\n+  Decorator() : SanitizerCommonDecorator() {}\n   const char *Error() { return Red(); }\n   const char *Leak() { return Blue(); }\n };\n@@ -157,19 +244,19 @@ static inline bool CanBeAHeapPointer(uptr p) {\n   // Since our heap is located in mmap-ed memory, we can assume a sensible lower\n   // bound on heap addresses.\n   const uptr kMinAddress = 4 * 4096;\n-  if (p < kMinAddress) return false;\n-#if defined(__x86_64__)\n+  if (p < kMinAddress)\n+    return false;\n+#  if defined(__x86_64__)\n   // Accept only canonical form user-space addresses.\n   return ((p >> 47) == 0);\n-#elif defined(__mips64)\n+#  elif defined(__mips64)\n   return ((p >> 40) == 0);\n-#elif defined(__aarch64__)\n-  unsigned runtimeVMA =\n-    (MostSignificantSetBitIndex(GET_CURRENT_FRAME()) + 1);\n+#  elif defined(__aarch64__)\n+  unsigned runtimeVMA = (MostSignificantSetBitIndex(GET_CURRENT_FRAME()) + 1);\n   return ((p >> runtimeVMA) == 0);\n-#else\n+#  else\n   return true;\n-#endif\n+#  endif\n }\n \n // Scans the memory range, looking for byte patterns that point into allocator\n@@ -178,8 +265,7 @@ static inline bool CanBeAHeapPointer(uptr p) {\n // (|tag| = kReachable) and finding indirectly leaked chunks\n // (|tag| = kIndirectlyLeaked). In the second case, there's no flood fill,\n // so |frontier| = 0.\n-void ScanRangeForPointers(uptr begin, uptr end,\n-                          Frontier *frontier,\n+void ScanRangeForPointers(uptr begin, uptr end, Frontier *frontier,\n                           const char *region_type, ChunkTag tag) {\n   CHECK(tag == kReachable || tag == kIndirectlyLeaked);\n   const uptr alignment = flags()->pointer_alignment();\n@@ -190,13 +276,17 @@ void ScanRangeForPointers(uptr begin, uptr end,\n     pp = pp + alignment - pp % alignment;\n   for (; pp + sizeof(void *) <= end; pp += alignment) {\n     void *p = *reinterpret_cast<void **>(pp);\n-    if (!CanBeAHeapPointer(reinterpret_cast<uptr>(p))) continue;\n+    if (!CanBeAHeapPointer(reinterpret_cast<uptr>(p)))\n+      continue;\n     uptr chunk = PointsIntoChunk(p);\n-    if (!chunk) continue;\n+    if (!chunk)\n+      continue;\n     // Pointers to self don't count. This matters when tag == kIndirectlyLeaked.\n-    if (chunk == begin) continue;\n+    if (chunk == begin)\n+      continue;\n     LsanMetadata m(chunk);\n-    if (m.tag() == kReachable || m.tag() == kIgnored) continue;\n+    if (m.tag() == kReachable || m.tag() == kIgnored)\n+      continue;\n \n     // Do this check relatively late so we can log only the interesting cases.\n     if (!flags()->use_poisoned && WordIsPoisoned(pp)) {\n@@ -234,23 +324,23 @@ void ScanGlobalRange(uptr begin, uptr end, Frontier *frontier) {\n   }\n }\n \n-void ForEachExtraStackRangeCb(uptr begin, uptr end, void* arg) {\n+void ForEachExtraStackRangeCb(uptr begin, uptr end, void *arg) {\n   Frontier *frontier = reinterpret_cast<Frontier *>(arg);\n   ScanRangeForPointers(begin, end, frontier, \"FAKE STACK\", kReachable);\n }\n \n-#if SANITIZER_FUCHSIA\n+#  if SANITIZER_FUCHSIA\n \n // Fuchsia handles all threads together with its own callback.\n static void ProcessThreads(SuspendedThreadsList const &, Frontier *) {}\n \n-#else\n+#  else\n \n-#if SANITIZER_ANDROID\n+#    if SANITIZER_ANDROID\n // FIXME: Move this out into *libcdep.cpp\n extern \"C\" SANITIZER_WEAK_ATTRIBUTE void __libc_iterate_dynamic_tls(\n     pid_t, void (*cb)(void *, void *, uptr, void *), void *);\n-#endif\n+#    endif\n \n static void ProcessThreadRegistry(Frontier *frontier) {\n   InternalMmapVector<uptr> ptrs;\n@@ -282,9 +372,9 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n     LOG_THREADS(\"Processing thread %llu.\\n\", os_id);\n     uptr stack_begin, stack_end, tls_begin, tls_end, cache_begin, cache_end;\n     DTLS *dtls;\n-    bool thread_found = GetThreadRangesLocked(os_id, &stack_begin, &stack_end,\n-                                              &tls_begin, &tls_end,\n-                                              &cache_begin, &cache_end, &dtls);\n+    bool thread_found =\n+        GetThreadRangesLocked(os_id, &stack_begin, &stack_end, &tls_begin,\n+                              &tls_end, &cache_begin, &cache_end, &dtls);\n     if (!thread_found) {\n       // If a thread can't be found in the thread registry, it's probably in the\n       // process of destruction. Log this event and move on.\n@@ -298,7 +388,8 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n       Report(\"Unable to get registers from thread %llu.\\n\", os_id);\n       // If unable to get SP, consider the entire stack to be reachable unless\n       // GetRegistersAndSP failed with ESRCH.\n-      if (have_registers == REGISTERS_UNAVAILABLE_FATAL) continue;\n+      if (have_registers == REGISTERS_UNAVAILABLE_FATAL)\n+        continue;\n       sp = stack_begin;\n     }\n \n@@ -353,7 +444,7 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n                                  kReachable);\n         }\n       }\n-#if SANITIZER_ANDROID\n+#    if SANITIZER_ANDROID\n       auto *cb = +[](void *dtls_begin, void *dtls_end, uptr /*dso_idd*/,\n                      void *arg) -> void {\n         ScanRangeForPointers(reinterpret_cast<uptr>(dtls_begin),\n@@ -366,7 +457,7 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n       // thread is suspended in the middle of updating its DTLS. IOWs, we\n       // could scan already freed memory. (probably fine for now)\n       __libc_iterate_dynamic_tls(os_id, cb, frontier);\n-#else\n+#    else\n       if (dtls && !DTLSInDestruction(dtls)) {\n         ForEachDVT(dtls, [&](const DTLS::DTV &dtv, int id) {\n           uptr dtls_beg = dtv.beg;\n@@ -383,21 +474,22 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n         // this and continue.\n         LOG_THREADS(\"Thread %llu has DTLS under destruction.\\n\", os_id);\n       }\n-#endif\n+#    endif\n     }\n   }\n \n   // Add pointers reachable from ThreadContexts\n   ProcessThreadRegistry(frontier);\n }\n \n-#endif  // SANITIZER_FUCHSIA\n+#  endif  // SANITIZER_FUCHSIA\n \n void ScanRootRegion(Frontier *frontier, const RootRegion &root_region,\n                     uptr region_begin, uptr region_end, bool is_readable) {\n   uptr intersection_begin = Max(root_region.begin, region_begin);\n   uptr intersection_end = Min(region_end, root_region.begin + root_region.size);\n-  if (intersection_begin >= intersection_end) return;\n+  if (intersection_begin >= intersection_end)\n+    return;\n   LOG_POINTERS(\"Root region %p-%p intersects with mapped region %p-%p (%s)\\n\",\n                (void *)root_region.begin,\n                (void *)(root_region.begin + root_region.size),\n@@ -420,7 +512,8 @@ static void ProcessRootRegion(Frontier *frontier,\n \n // Scans root regions for heap pointers.\n static void ProcessRootRegions(Frontier *frontier) {\n-  if (!flags()->use_root_regions) return;\n+  if (!flags()->use_root_regions)\n+    return;\n   for (uptr i = 0; i < root_regions.size(); i++)\n     ProcessRootRegion(frontier, root_regions[i]);\n }\n@@ -477,68 +570,6 @@ static void CollectIgnoredCb(uptr chunk, void *arg) {\n   }\n }\n \n-static uptr GetCallerPC(const StackTrace &stack) {\n-  // The top frame is our malloc/calloc/etc. The next frame is the caller.\n-  if (stack.size >= 2)\n-    return stack.trace[1];\n-  return 0;\n-}\n-\n-struct InvalidPCParam {\n-  Frontier *frontier;\n-  bool skip_linker_allocations;\n-};\n-\n-// ForEachChunk callback. If the caller pc is invalid or is within the linker,\n-// mark as reachable. Called by ProcessPlatformSpecificAllocations.\n-static void MarkInvalidPCCb(uptr chunk, void *arg) {\n-  CHECK(arg);\n-  InvalidPCParam *param = reinterpret_cast<InvalidPCParam *>(arg);\n-  chunk = GetUserBegin(chunk);\n-  LsanMetadata m(chunk);\n-  if (m.allocated() && m.tag() != kReachable && m.tag() != kIgnored) {\n-    u32 stack_id = m.stack_trace_id();\n-    uptr caller_pc = 0;\n-    if (stack_id > 0)\n-      caller_pc = GetCallerPC(StackDepotGet(stack_id));\n-    // If caller_pc is unknown, this chunk may be allocated in a coroutine. Mark\n-    // it as reachable, as we can't properly report its allocation stack anyway.\n-    if (caller_pc == 0 || (param->skip_linker_allocations &&\n-                           GetLinker()->containsAddress(caller_pc))) {\n-      m.set_tag(kReachable);\n-      param->frontier->push_back(chunk);\n-    }\n-  }\n-}\n-\n-// On Linux, treats all chunks allocated from ld-linux.so as reachable, which\n-// covers dynamically allocated TLS blocks, internal dynamic loader's loaded\n-// modules accounting etc.\n-// Dynamic TLS blocks contain the TLS variables of dynamically loaded modules.\n-// They are allocated with a __libc_memalign() call in allocate_and_init()\n-// (elf/dl-tls.c). Glibc won't tell us the address ranges occupied by those\n-// blocks, but we can make sure they come from our own allocator by intercepting\n-// __libc_memalign(). On top of that, there is no easy way to reach them. Their\n-// addresses are stored in a dynamically allocated array (the DTV) which is\n-// referenced from the static TLS. Unfortunately, we can't just rely on the DTV\n-// being reachable from the static TLS, and the dynamic TLS being reachable from\n-// the DTV. This is because the initial DTV is allocated before our interception\n-// mechanism kicks in, and thus we don't recognize it as allocated memory. We\n-// can't special-case it either, since we don't know its size.\n-// Our solution is to include in the root set all allocations made from\n-// ld-linux.so (which is where allocate_and_init() is implemented). This is\n-// guaranteed to include all dynamic TLS blocks (and possibly other allocations\n-// which we don't care about).\n-// On all other platforms, this simply checks to ensure that the caller pc is\n-// valid before reporting chunks as leaked.\n-static void ProcessPC(Frontier *frontier) {\n-  InvalidPCParam arg;\n-  arg.frontier = frontier;\n-  arg.skip_linker_allocations =\n-      flags()->use_tls && flags()->use_ld_allocations && GetLinker() != nullptr;\n-  ForEachChunk(MarkInvalidPCCb, &arg);\n-}\n-\n // Sets the appropriate tag on each chunk.\n static void ClassifyAllChunks(SuspendedThreadsList const &suspended_threads,\n                               Frontier *frontier) {\n@@ -554,9 +585,6 @@ static void ClassifyAllChunks(SuspendedThreadsList const &suspended_threads,\n   ProcessRootRegions(frontier);\n   FloodFillTag(frontier, kReachable);\n \n-  CHECK_EQ(0, frontier->size());\n-  ProcessPC(frontier);\n-\n   // The check here is relatively expensive, so we do this in a separate flood\n   // fill. That way we can skip the check for chunks that are reachable\n   // otherwise.\n@@ -583,14 +611,13 @@ static void ResetTagsCb(uptr chunk, void *arg) {\n // a LeakReport.\n static void CollectLeaksCb(uptr chunk, void *arg) {\n   CHECK(arg);\n-  LeakReport *leak_report = reinterpret_cast<LeakReport *>(arg);\n+  LeakedChunks *leaks = reinterpret_cast<LeakedChunks *>(arg);\n   chunk = GetUserBegin(chunk);\n   LsanMetadata m(chunk);\n-  if (!m.allocated()) return;\n-  if (m.tag() == kDirectlyLeaked || m.tag() == kIndirectlyLeaked) {\n-    leak_report->AddLeakedChunk(chunk, m.stack_trace_id(), m.requested_size(),\n-                                m.tag());\n-  }\n+  if (!m.allocated())\n+    return;\n+  if (m.tag() == kDirectlyLeaked || m.tag() == kIndirectlyLeaked)\n+    leaks->push_back({chunk, m.stack_trace_id(), m.requested_size(), m.tag()});\n }\n \n void LeakSuppressionContext::PrintMatchedSuppressions() {\n@@ -622,13 +649,13 @@ static void ReportIfNotSuspended(ThreadContextBase *tctx, void *arg) {\n   }\n }\n \n-#if SANITIZER_FUCHSIA\n+#  if SANITIZER_FUCHSIA\n \n // Fuchsia provides a libc interface that guarantees all threads are\n // covered, and SuspendedThreadList is never really used.\n static void ReportUnsuspendedThreads(const SuspendedThreadsList &) {}\n \n-#else  // !SANITIZER_FUCHSIA\n+#  else  // !SANITIZER_FUCHSIA\n \n static void ReportUnsuspendedThreads(\n     const SuspendedThreadsList &suspended_threads) {\n@@ -642,7 +669,7 @@ static void ReportUnsuspendedThreads(\n       &ReportIfNotSuspended, &threads);\n }\n \n-#endif  // !SANITIZER_FUCHSIA\n+#  endif  // !SANITIZER_FUCHSIA\n \n static void CheckForLeaksCallback(const SuspendedThreadsList &suspended_threads,\n                                   void *arg) {\n@@ -651,7 +678,7 @@ static void CheckForLeaksCallback(const SuspendedThreadsList &suspended_threads,\n   CHECK(!param->success);\n   ReportUnsuspendedThreads(suspended_threads);\n   ClassifyAllChunks(suspended_threads, &param->frontier);\n-  ForEachChunk(CollectLeaksCb, &param->leak_report);\n+  ForEachChunk(CollectLeaksCb, &param->leaks);\n   // Clean up for subsequent leak checks. This assumes we did not overwrite any\n   // kIgnored tags.\n   ForEachChunk(ResetTagsCb, nullptr);\n@@ -700,17 +727,20 @@ static bool CheckForLeaks() {\n           \"etc)\\n\");\n       Die();\n     }\n+    LeakReport leak_report;\n+    leak_report.AddLeakedChunks(param.leaks);\n+\n     // No new suppressions stacks, so rerun will not help and we can report.\n-    if (!param.leak_report.ApplySuppressions())\n-      return PrintResults(param.leak_report);\n+    if (!leak_report.ApplySuppressions())\n+      return PrintResults(leak_report);\n \n     // No indirect leaks to report, so we are done here.\n-    if (!param.leak_report.IndirectUnsuppressedLeakCount())\n-      return PrintResults(param.leak_report);\n+    if (!leak_report.IndirectUnsuppressedLeakCount())\n+      return PrintResults(leak_report);\n \n     if (i >= 8) {\n       Report(\"WARNING: LeakSanitizer gave up on indirect leaks suppression.\\n\");\n-      return PrintResults(param.leak_report);\n+      return PrintResults(leak_report);\n     }\n \n     // We found a new previously unseen suppressed call stack. Rerun to make\n@@ -726,10 +756,12 @@ bool HasReportedLeaks() { return has_reported_leaks; }\n void DoLeakCheck() {\n   Lock l(&global_mutex);\n   static bool already_done;\n-  if (already_done) return;\n+  if (already_done)\n+    return;\n   already_done = true;\n   has_reported_leaks = CheckForLeaks();\n-  if (has_reported_leaks) HandleLeaks();\n+  if (has_reported_leaks)\n+    HandleLeaks();\n }\n \n static int DoRecoverableLeakCheck() {\n@@ -740,80 +772,50 @@ static int DoRecoverableLeakCheck() {\n \n void DoRecoverableLeakCheckVoid() { DoRecoverableLeakCheck(); }\n \n-Suppression *LeakSuppressionContext::GetSuppressionForAddr(uptr addr) {\n-  Suppression *s = nullptr;\n-\n-  // Suppress by module name.\n-  if (const char *module_name =\n-          Symbolizer::GetOrInit()->GetModuleNameForPc(addr))\n-    if (context.Match(module_name, kSuppressionLeak, &s))\n-      return s;\n-\n-  // Suppress by file or function name.\n-  SymbolizedStack *frames = Symbolizer::GetOrInit()->SymbolizePC(addr);\n-  for (SymbolizedStack *cur = frames; cur; cur = cur->next) {\n-    if (context.Match(cur->info.function, kSuppressionLeak, &s) ||\n-        context.Match(cur->info.file, kSuppressionLeak, &s)) {\n-      break;\n-    }\n-  }\n-  frames->ClearAll();\n-  return s;\n-}\n-\n-Suppression *LeakSuppressionContext::GetSuppressionForStack(\n-    u32 stack_trace_id, const StackTrace &stack) {\n-  LazyInit();\n-  for (uptr i = 0; i < stack.size; i++) {\n-    Suppression *s = GetSuppressionForAddr(\n-        StackTrace::GetPreviousInstructionPc(stack.trace[i]));\n-    if (s) {\n-      suppressed_stacks_sorted = false;\n-      suppressed_stacks.push_back(stack_trace_id);\n-      return s;\n-    }\n-  }\n-  return nullptr;\n-}\n-\n ///// LeakReport implementation. /////\n \n // A hard limit on the number of distinct leaks, to avoid quadratic complexity\n // in LeakReport::AddLeakedChunk(). We don't expect to ever see this many leaks\n // in real-world applications.\n-// FIXME: Get rid of this limit by changing the implementation of LeakReport to\n-// use a hash table.\n+// FIXME: Get rid of this limit by moving logic into DedupLeaks.\n const uptr kMaxLeaksConsidered = 5000;\n \n-void LeakReport::AddLeakedChunk(uptr chunk, u32 stack_trace_id,\n-                                uptr leaked_size, ChunkTag tag) {\n-  CHECK(tag == kDirectlyLeaked || tag == kIndirectlyLeaked);\n-\n-  if (u32 resolution = flags()->resolution) {\n-    StackTrace stack = StackDepotGet(stack_trace_id);\n-    stack.size = Min(stack.size, resolution);\n-    stack_trace_id = StackDepotPut(stack);\n-  }\n+void LeakReport::AddLeakedChunks(const LeakedChunks &chunks) {\n+  for (const LeakedChunk &leak : chunks) {\n+    uptr chunk = leak.chunk;\n+    u32 stack_trace_id = leak.stack_trace_id;\n+    uptr leaked_size = leak.leaked_size;\n+    ChunkTag tag = leak.tag;\n+    CHECK(tag == kDirectlyLeaked || tag == kIndirectlyLeaked);\n+\n+    if (u32 resolution = flags()->resolution) {\n+      StackTrace stack = StackDepotGet(stack_trace_id);\n+      stack.size = Min(stack.size, resolution);\n+      stack_trace_id = StackDepotPut(stack);\n+    }\n \n-  bool is_directly_leaked = (tag == kDirectlyLeaked);\n-  uptr i;\n-  for (i = 0; i < leaks_.size(); i++) {\n-    if (leaks_[i].stack_trace_id == stack_trace_id &&\n-        leaks_[i].is_directly_leaked == is_directly_leaked) {\n-      leaks_[i].hit_count++;\n-      leaks_[i].total_size += leaked_size;\n-      break;\n+    bool is_directly_leaked = (tag == kDirectlyLeaked);\n+    uptr i;\n+    for (i = 0; i < leaks_.size(); i++) {\n+      if (leaks_[i].stack_trace_id == stack_trace_id &&\n+          leaks_[i].is_directly_leaked == is_directly_leaked) {\n+        leaks_[i].hit_count++;\n+        leaks_[i].total_size += leaked_size;\n+        break;\n+      }\n+    }\n+    if (i == leaks_.size()) {\n+      if (leaks_.size() == kMaxLeaksConsidered)\n+        return;\n+      Leak leak = {next_id_++,         /* hit_count */ 1,\n+                   leaked_size,        stack_trace_id,\n+                   is_directly_leaked, /* is_suppressed */ false};\n+      leaks_.push_back(leak);\n+    }\n+    if (flags()->report_objects) {\n+      LeakedObject obj = {leaks_[i].id, chunk, leaked_size};\n+      leaked_objects_.push_back(obj);\n     }\n-  }\n-  if (i == leaks_.size()) {\n-    if (leaks_.size() == kMaxLeaksConsidered) return;\n-    Leak leak = { next_id_++, /* hit_count */ 1, leaked_size, stack_trace_id,\n-                  is_directly_leaked, /* is_suppressed */ false };\n-    leaks_.push_back(leak);\n-  }\n-  if (flags()->report_objects) {\n-    LeakedObject obj = {leaks_[i].id, chunk, leaked_size};\n-    leaked_objects_.push_back(obj);\n   }\n }\n \n@@ -828,20 +830,23 @@ void LeakReport::ReportTopLeaks(uptr num_leaks_to_report) {\n   CHECK(leaks_.size() <= kMaxLeaksConsidered);\n   Printf(\"\\n\");\n   if (leaks_.size() == kMaxLeaksConsidered)\n-    Printf(\"Too many leaks! Only the first %zu leaks encountered will be \"\n-           \"reported.\\n\",\n-           kMaxLeaksConsidered);\n+    Printf(\n+        \"Too many leaks! Only the first %zu leaks encountered will be \"\n+        \"reported.\\n\",\n+        kMaxLeaksConsidered);\n \n   uptr unsuppressed_count = UnsuppressedLeakCount();\n   if (num_leaks_to_report > 0 && num_leaks_to_report < unsuppressed_count)\n     Printf(\"The %zu top leak(s):\\n\", num_leaks_to_report);\n   Sort(leaks_.data(), leaks_.size(), &LeakComparator);\n   uptr leaks_reported = 0;\n   for (uptr i = 0; i < leaks_.size(); i++) {\n-    if (leaks_[i].is_suppressed) continue;\n+    if (leaks_[i].is_suppressed)\n+      continue;\n     PrintReportForLeak(i);\n     leaks_reported++;\n-    if (leaks_reported == num_leaks_to_report) break;\n+    if (leaks_reported == num_leaks_to_report)\n+      break;\n   }\n   if (leaks_reported < unsuppressed_count) {\n     uptr remaining = unsuppressed_count - leaks_reported;\n@@ -880,9 +885,10 @@ void LeakReport::PrintSummary() {\n   CHECK(leaks_.size() <= kMaxLeaksConsidered);\n   uptr bytes = 0, allocations = 0;\n   for (uptr i = 0; i < leaks_.size(); i++) {\n-      if (leaks_[i].is_suppressed) continue;\n-      bytes += leaks_[i].total_size;\n-      allocations += leaks_[i].hit_count;\n+    if (leaks_[i].is_suppressed)\n+      continue;\n+    bytes += leaks_[i].total_size;\n+    allocations += leaks_[i].hit_count;\n   }\n   InternalScopedString summary;\n   summary.append(\"%zu byte(s) leaked in %zu allocation(s).\", bytes,\n@@ -894,12 +900,8 @@ uptr LeakReport::ApplySuppressions() {\n   LeakSuppressionContext *suppressions = GetSuppressionContext();\n   uptr new_suppressions = false;\n   for (uptr i = 0; i < leaks_.size(); i++) {\n-    Suppression *s = suppressions->GetSuppressionForStack(\n-        leaks_[i].stack_trace_id, StackDepotGet(leaks_[i].stack_trace_id));\n-    if (s) {\n-      s->weight += leaks_[i].total_size;\n-      atomic_store_relaxed(&s->hit_count, atomic_load_relaxed(&s->hit_count) +\n-          leaks_[i].hit_count);\n+    if (suppressions->Suppress(leaks_[i].stack_trace_id, leaks_[i].hit_count,\n+                               leaks_[i].total_size)) {\n       leaks_[i].is_suppressed = true;\n       ++new_suppressions;\n     }\n@@ -910,7 +912,8 @@ uptr LeakReport::ApplySuppressions() {\n uptr LeakReport::UnsuppressedLeakCount() {\n   uptr result = 0;\n   for (uptr i = 0; i < leaks_.size(); i++)\n-    if (!leaks_[i].is_suppressed) result++;\n+    if (!leaks_[i].is_suppressed)\n+      result++;\n   return result;\n }\n \n@@ -922,16 +925,16 @@ uptr LeakReport::IndirectUnsuppressedLeakCount() {\n   return result;\n }\n \n-} // namespace __lsan\n-#else // CAN_SANITIZE_LEAKS\n+}  // namespace __lsan\n+#else   // CAN_SANITIZE_LEAKS\n namespace __lsan {\n-void InitCommonLsan() { }\n-void DoLeakCheck() { }\n-void DoRecoverableLeakCheckVoid() { }\n-void DisableInThisThread() { }\n-void EnableInThisThread() { }\n-}\n-#endif // CAN_SANITIZE_LEAKS\n+void InitCommonLsan() {}\n+void DoLeakCheck() {}\n+void DoRecoverableLeakCheckVoid() {}\n+void DisableInThisThread() {}\n+void EnableInThisThread() {}\n+}  // namespace __lsan\n+#endif  // CAN_SANITIZE_LEAKS\n \n using namespace __lsan;\n \n@@ -948,11 +951,13 @@ void __lsan_ignore_object(const void *p) {\n   if (res == kIgnoreObjectInvalid)\n     VReport(1, \"__lsan_ignore_object(): no heap object found at %p\", p);\n   if (res == kIgnoreObjectAlreadyIgnored)\n-    VReport(1, \"__lsan_ignore_object(): \"\n-           \"heap object at %p is already being ignored\\n\", p);\n+    VReport(1,\n+            \"__lsan_ignore_object(): \"\n+            \"heap object at %p is already being ignored\\n\",\n+            p);\n   if (res == kIgnoreObjectSuccess)\n     VReport(1, \"__lsan_ignore_object(): ignoring heap object at %p\\n\", p);\n-#endif // CAN_SANITIZE_LEAKS\n+#endif  // CAN_SANITIZE_LEAKS\n }\n \n SANITIZER_INTERFACE_ATTRIBUTE\n@@ -962,7 +967,7 @@ void __lsan_register_root_region(const void *begin, uptr size) {\n   RootRegion region = {reinterpret_cast<uptr>(begin), size};\n   root_regions.push_back(region);\n   VReport(1, \"Registered root region at %p of size %zu\\n\", begin, size);\n-#endif // CAN_SANITIZE_LEAKS\n+#endif  // CAN_SANITIZE_LEAKS\n }\n \n SANITIZER_INTERFACE_ATTRIBUTE\n@@ -988,7 +993,7 @@ void __lsan_unregister_root_region(const void *begin, uptr size) {\n         begin, size);\n     Die();\n   }\n-#endif // CAN_SANITIZE_LEAKS\n+#endif  // CAN_SANITIZE_LEAKS\n }\n \n SANITIZER_INTERFACE_ATTRIBUTE\n@@ -1010,15 +1015,15 @@ void __lsan_do_leak_check() {\n #if CAN_SANITIZE_LEAKS\n   if (common_flags()->detect_leaks)\n     __lsan::DoLeakCheck();\n-#endif // CAN_SANITIZE_LEAKS\n+#endif  // CAN_SANITIZE_LEAKS\n }\n \n SANITIZER_INTERFACE_ATTRIBUTE\n int __lsan_do_recoverable_leak_check() {\n #if CAN_SANITIZE_LEAKS\n   if (common_flags()->detect_leaks)\n     return __lsan::DoRecoverableLeakCheck();\n-#endif // CAN_SANITIZE_LEAKS\n+#endif  // CAN_SANITIZE_LEAKS\n   return 0;\n }\n \n@@ -1027,14 +1032,14 @@ SANITIZER_INTERFACE_WEAK_DEF(const char *, __lsan_default_options, void) {\n }\n \n #if !SANITIZER_SUPPORTS_WEAK_HOOKS\n-SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-int __lsan_is_turned_off() {\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE int\n+__lsan_is_turned_off() {\n   return 0;\n }\n \n-SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-const char *__lsan_default_suppressions() {\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE const char *\n+__lsan_default_suppressions() {\n   return \"\";\n }\n #endif\n-} // extern \"C\"\n+}  // extern \"C\""}, {"sha": "6b06c4517cd5de17484a9fe82a74a65026730b14", "filename": "libsanitizer/lsan/lsan_common.h", "status": "modified", "additions": 36, "deletions": 12, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_common.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_common.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_common.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -33,21 +33,21 @@\n // Exclude leak-detection on arm32 for Android because `__aeabi_read_tp`\n // is missing. This caused a link error.\n #if SANITIZER_ANDROID && (__ANDROID_API__ < 28 || defined(__arm__))\n-#define CAN_SANITIZE_LEAKS 0\n+#  define CAN_SANITIZE_LEAKS 0\n #elif (SANITIZER_LINUX || SANITIZER_MAC) && (SANITIZER_WORDSIZE == 64) && \\\n     (defined(__x86_64__) || defined(__mips64) || defined(__aarch64__) ||  \\\n      defined(__powerpc64__) || defined(__s390x__))\n-#define CAN_SANITIZE_LEAKS 1\n+#  define CAN_SANITIZE_LEAKS 1\n #elif defined(__i386__) && (SANITIZER_LINUX || SANITIZER_MAC)\n-#define CAN_SANITIZE_LEAKS 1\n+#  define CAN_SANITIZE_LEAKS 1\n #elif defined(__arm__) && SANITIZER_LINUX\n-#define CAN_SANITIZE_LEAKS 1\n+#  define CAN_SANITIZE_LEAKS 1\n #elif SANITIZER_RISCV64 && SANITIZER_LINUX\n-#define CAN_SANITIZE_LEAKS 1\n+#  define CAN_SANITIZE_LEAKS 1\n #elif SANITIZER_NETBSD || SANITIZER_FUCHSIA\n-#define CAN_SANITIZE_LEAKS 1\n+#  define CAN_SANITIZE_LEAKS 1\n #else\n-#define CAN_SANITIZE_LEAKS 0\n+#  define CAN_SANITIZE_LEAKS 0\n #endif\n \n namespace __sanitizer {\n@@ -82,6 +82,15 @@ extern Flags lsan_flags;\n inline Flags *flags() { return &lsan_flags; }\n void RegisterLsanFlags(FlagParser *parser, Flags *f);\n \n+struct LeakedChunk {\n+  uptr chunk;\n+  u32 stack_trace_id;\n+  uptr leaked_size;\n+  ChunkTag tag;\n+};\n+\n+using LeakedChunks = InternalMmapVector<LeakedChunk>;\n+\n struct Leak {\n   u32 id;\n   uptr hit_count;\n@@ -101,8 +110,7 @@ struct LeakedObject {\n class LeakReport {\n  public:\n   LeakReport() {}\n-  void AddLeakedChunk(uptr chunk, u32 stack_trace_id, uptr leaked_size,\n-                      ChunkTag tag);\n+  void AddLeakedChunks(const LeakedChunks &chunks);\n   void ReportTopLeaks(uptr max_leaks);\n   void PrintSummary();\n   uptr ApplySuppressions();\n@@ -136,7 +144,7 @@ struct RootRegion {\n // threads and enumerating roots.\n struct CheckForLeaksParam {\n   Frontier frontier;\n-  LeakReport leak_report;\n+  LeakedChunks leaks;\n   bool success = false;\n };\n \n@@ -222,8 +230,24 @@ void UnlockAllocator();\n // Returns true if [addr, addr + sizeof(void *)) is poisoned.\n bool WordIsPoisoned(uptr addr);\n // Wrappers for ThreadRegistry access.\n-void LockThreadRegistry() NO_THREAD_SAFETY_ANALYSIS;\n-void UnlockThreadRegistry() NO_THREAD_SAFETY_ANALYSIS;\n+void LockThreadRegistry() SANITIZER_NO_THREAD_SAFETY_ANALYSIS;\n+void UnlockThreadRegistry() SANITIZER_NO_THREAD_SAFETY_ANALYSIS;\n+\n+struct ScopedStopTheWorldLock {\n+  ScopedStopTheWorldLock() {\n+    LockThreadRegistry();\n+    LockAllocator();\n+  }\n+\n+  ~ScopedStopTheWorldLock() {\n+    UnlockAllocator();\n+    UnlockThreadRegistry();\n+  }\n+\n+  ScopedStopTheWorldLock &operator=(const ScopedStopTheWorldLock &) = delete;\n+  ScopedStopTheWorldLock(const ScopedStopTheWorldLock &) = delete;\n+};\n+\n ThreadRegistry *GetThreadRegistryLocked();\n bool GetThreadRangesLocked(tid_t os_id, uptr *stack_begin, uptr *stack_end,\n                            uptr *tls_begin, uptr *tls_end, uptr *cache_begin,"}, {"sha": "edb4ca6c8578e03c2c266f6e970ec6b67e34c164", "filename": "libsanitizer/lsan/lsan_common_fuchsia.cpp", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_common_fuchsia.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_common_fuchsia.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_common_fuchsia.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -52,14 +52,22 @@ void ProcessPlatformSpecificAllocations(Frontier *frontier) {}\n // behavior and causes rare race conditions.\n void HandleLeaks() {}\n \n+// This is defined differently in asan_fuchsia.cpp and lsan_fuchsia.cpp.\n+bool UseExitcodeOnLeak();\n+\n int ExitHook(int status) {\n+  if (common_flags()->detect_leaks && common_flags()->leak_check_at_exit) {\n+    if (UseExitcodeOnLeak())\n+      DoLeakCheck();\n+    else\n+      DoRecoverableLeakCheckVoid();\n+  }\n   return status == 0 && HasReportedLeaks() ? common_flags()->exitcode : status;\n }\n \n void LockStuffAndStopTheWorld(StopTheWorldCallback callback,\n                               CheckForLeaksParam *argument) {\n-  LockThreadRegistry();\n-  LockAllocator();\n+  ScopedStopTheWorldLock lock;\n \n   struct Params {\n     InternalMmapVector<uptr> allocator_caches;\n@@ -149,9 +157,6 @@ void LockStuffAndStopTheWorld(StopTheWorldCallback callback,\n         params->callback(SuspendedThreadsListFuchsia(), params->argument);\n       },\n       &params);\n-\n-  UnlockAllocator();\n-  UnlockThreadRegistry();\n }\n \n }  // namespace __lsan"}, {"sha": "692ad35169e1d85d2ec48f95e1e555ee057d6b70", "filename": "libsanitizer/lsan/lsan_common_linux.cpp", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_common_linux.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_common_linux.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_common_linux.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -122,12 +122,9 @@ void HandleLeaks() {\n \n static int LockStuffAndStopTheWorldCallback(struct dl_phdr_info *info,\n                                             size_t size, void *data) {\n-  LockThreadRegistry();\n-  LockAllocator();\n+  ScopedStopTheWorldLock lock;\n   DoStopTheWorldParam *param = reinterpret_cast<DoStopTheWorldParam *>(data);\n   StopTheWorld(param->callback, param->argument);\n-  UnlockAllocator();\n-  UnlockThreadRegistry();\n   return 1;\n }\n "}, {"sha": "a4204740c7fab11e669dc819a1494df0a7ca7572", "filename": "libsanitizer/lsan/lsan_common_mac.cpp", "status": "modified", "additions": 4, "deletions": 7, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_common_mac.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_common_mac.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_common_mac.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -143,16 +143,16 @@ void ProcessGlobalRegions(Frontier *frontier) {\n }\n \n void ProcessPlatformSpecificAllocations(Frontier *frontier) {\n-  unsigned depth = 1;\n-  vm_size_t size = 0;\n   vm_address_t address = 0;\n   kern_return_t err = KERN_SUCCESS;\n-  mach_msg_type_number_t count = VM_REGION_SUBMAP_INFO_COUNT_64;\n \n   InternalMmapVectorNoCtor<RootRegion> const *root_regions = GetRootRegions();\n \n   while (err == KERN_SUCCESS) {\n+    vm_size_t size = 0;\n+    unsigned depth = 1;\n     struct vm_region_submap_info_64 info;\n+    mach_msg_type_number_t count = VM_REGION_SUBMAP_INFO_COUNT_64;\n     err = vm_region_recurse_64(mach_task_self(), &address, &size, &depth,\n                                (vm_region_info_t)&info, &count);\n \n@@ -195,11 +195,8 @@ void HandleLeaks() {}\n \n void LockStuffAndStopTheWorld(StopTheWorldCallback callback,\n                               CheckForLeaksParam *argument) {\n-  LockThreadRegistry();\n-  LockAllocator();\n+  ScopedStopTheWorldLock lock;\n   StopTheWorld(callback, argument);\n-  UnlockAllocator();\n-  UnlockThreadRegistry();\n }\n \n } // namespace __lsan"}, {"sha": "2d96206754a94d6a24d049450e0b9718c0c4269e", "filename": "libsanitizer/lsan/lsan_fuchsia.cpp", "status": "modified", "additions": 9, "deletions": 3, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_fuchsia.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_fuchsia.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_fuchsia.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -62,7 +62,7 @@ void InitializeMainThread() {\n   OnCreatedArgs args;\n   __sanitizer::GetThreadStackTopAndBottom(true, &args.stack_end,\n                                           &args.stack_begin);\n-  u32 tid = ThreadCreate(0, GetThreadSelf(), true, &args);\n+  u32 tid = ThreadCreate(kMainTid, true, &args);\n   CHECK_EQ(tid, 0);\n   ThreadStart(tid);\n }\n@@ -76,6 +76,13 @@ void GetAllThreadAllocatorCachesLocked(InternalMmapVector<uptr> *caches) {\n       caches);\n }\n \n+// On Fuchsia, leak detection is done by a special hook after atexit hooks.\n+// So this doesn't install any atexit hook like on other platforms.\n+void InstallAtExitCheckLeaks() {}\n+\n+// ASan defines this to check its `halt_on_error` flag.\n+bool UseExitcodeOnLeak() { return true; }\n+\n }  // namespace __lsan\n \n // These are declared (in extern \"C\") by <zircon/sanitizer.h>.\n@@ -86,14 +93,13 @@ void GetAllThreadAllocatorCachesLocked(InternalMmapVector<uptr> *caches) {\n void *__sanitizer_before_thread_create_hook(thrd_t thread, bool detached,\n                                             const char *name, void *stack_base,\n                                             size_t stack_size) {\n-  uptr user_id = reinterpret_cast<uptr>(thread);\n   ENSURE_LSAN_INITED;\n   EnsureMainThreadIDIsCorrect();\n   OnCreatedArgs args;\n   args.stack_begin = reinterpret_cast<uptr>(stack_base);\n   args.stack_end = args.stack_begin + stack_size;\n   u32 parent_tid = GetCurrentThread();\n-  u32 tid = ThreadCreate(parent_tid, user_id, detached, &args);\n+  u32 tid = ThreadCreate(parent_tid, detached, &args);\n   return reinterpret_cast<void *>(static_cast<uptr>(tid));\n }\n "}, {"sha": "205e85685a7fafc8e82cdbe35256ac9652846838", "filename": "libsanitizer/lsan/lsan_interceptors.cpp", "status": "modified", "additions": 4, "deletions": 18, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_interceptors.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_interceptors.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_interceptors.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -468,8 +468,7 @@ INTERCEPTOR(int, pthread_create, void *th, void *attr,\n     res = REAL(pthread_create)(th, attr, __lsan_thread_start_func, &p);\n   }\n   if (res == 0) {\n-    int tid = ThreadCreate(GetCurrentThread(), *(uptr *)th,\n-                           IsStateDetached(detached));\n+    int tid = ThreadCreate(GetCurrentThread(), IsStateDetached(detached));\n     CHECK_NE(tid, kMainTid);\n     atomic_store(&p.tid, tid, memory_order_release);\n     while (atomic_load(&p.tid, memory_order_acquire) != 0)\n@@ -480,23 +479,11 @@ INTERCEPTOR(int, pthread_create, void *th, void *attr,\n   return res;\n }\n \n-INTERCEPTOR(int, pthread_join, void *th, void **ret) {\n-  ENSURE_LSAN_INITED;\n-  int tid = ThreadTid((uptr)th);\n-  int res = REAL(pthread_join)(th, ret);\n-  if (res == 0)\n-    ThreadJoin(tid);\n-  return res;\n+INTERCEPTOR(int, pthread_join, void *t, void **arg) {\n+  return REAL(pthread_join)(t, arg);\n }\n \n-INTERCEPTOR(int, pthread_detach, void *th) {\n-  ENSURE_LSAN_INITED;\n-  int tid = ThreadTid((uptr)th);\n-  int res = REAL(pthread_detach)(th);\n-  if (res == 0)\n-    ThreadDetach(tid);\n-  return res;\n-}\n+DEFINE_REAL_PTHREAD_FUNCTIONS\n \n INTERCEPTOR(void, _exit, int status) {\n   if (status == 0 && HasReportedLeaks()) status = common_flags()->exitcode;\n@@ -530,7 +517,6 @@ void InitializeInterceptors() {\n   LSAN_MAYBE_INTERCEPT_MALLINFO;\n   LSAN_MAYBE_INTERCEPT_MALLOPT;\n   INTERCEPT_FUNCTION(pthread_create);\n-  INTERCEPT_FUNCTION(pthread_detach);\n   INTERCEPT_FUNCTION(pthread_join);\n   INTERCEPT_FUNCTION(_exit);\n "}, {"sha": "10a73f8fa93dff5e0323da5f0828f3ec99deaa51", "filename": "libsanitizer/lsan/lsan_mac.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_mac.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_mac.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_mac.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -68,7 +68,7 @@ typedef struct {\n ALWAYS_INLINE\n void lsan_register_worker_thread(int parent_tid) {\n   if (GetCurrentThread() == kInvalidTid) {\n-    u32 tid = ThreadCreate(parent_tid, 0, true);\n+    u32 tid = ThreadCreate(parent_tid, true);\n     ThreadStart(tid, GetTid());\n     SetCurrentThread(tid);\n   }"}, {"sha": "8f277db22375956b21d0dad89f195c4a0d8871dd", "filename": "libsanitizer/lsan/lsan_posix.cpp", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_posix.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_posix.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_posix.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -75,7 +75,7 @@ bool GetThreadRangesLocked(tid_t os_id, uptr *stack_begin, uptr *stack_end,\n }\n \n void InitializeMainThread() {\n-  u32 tid = ThreadCreate(kMainTid, 0, true);\n+  u32 tid = ThreadCreate(kMainTid, true);\n   CHECK_EQ(tid, kMainTid);\n   ThreadStart(tid, GetTid());\n }\n@@ -91,6 +91,11 @@ void LsanOnDeadlySignal(int signo, void *siginfo, void *context) {\n                      nullptr);\n }\n \n+void InstallAtExitCheckLeaks() {\n+  if (common_flags()->detect_leaks && common_flags()->leak_check_at_exit)\n+    Atexit(DoLeakCheck);\n+}\n+\n }  // namespace __lsan\n \n #endif  // SANITIZER_POSIX"}, {"sha": "ca3dfd03b109df8b983462224834dd1f3a643f18", "filename": "libsanitizer/lsan/lsan_thread.cpp", "status": "modified", "additions": 2, "deletions": 24, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_thread.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_thread.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_thread.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -44,8 +44,8 @@ void ThreadContextLsanBase::OnFinished() {\n   DTLS_Destroy();\n }\n \n-u32 ThreadCreate(u32 parent_tid, uptr user_id, bool detached, void *arg) {\n-  return thread_registry->CreateThread(user_id, detached, parent_tid, arg);\n+u32 ThreadCreate(u32 parent_tid, bool detached, void *arg) {\n+  return thread_registry->CreateThread(0, detached, parent_tid, arg);\n }\n \n void ThreadContextLsanBase::ThreadStart(u32 tid, tid_t os_id,\n@@ -68,28 +68,6 @@ ThreadContext *CurrentThreadContext() {\n   return (ThreadContext *)thread_registry->GetThreadLocked(GetCurrentThread());\n }\n \n-static bool FindThreadByUid(ThreadContextBase *tctx, void *arg) {\n-  uptr uid = (uptr)arg;\n-  if (tctx->user_id == uid && tctx->status != ThreadStatusInvalid) {\n-    return true;\n-  }\n-  return false;\n-}\n-\n-u32 ThreadTid(uptr uid) {\n-  return thread_registry->FindThread(FindThreadByUid, (void *)uid);\n-}\n-\n-void ThreadDetach(u32 tid) {\n-  CHECK_NE(tid, kInvalidTid);\n-  thread_registry->DetachThread(tid, /* arg */ nullptr);\n-}\n-\n-void ThreadJoin(u32 tid) {\n-  CHECK_NE(tid, kInvalidTid);\n-  thread_registry->JoinThread(tid, /* arg */ nullptr);\n-}\n-\n void EnsureMainThreadIDIsCorrect() {\n   if (GetCurrentThread() == kMainTid)\n     CurrentThreadContext()->os_id = GetTid();"}, {"sha": "6ab4172092ae9bdc970e60dd0ade48db93bdb0a9", "filename": "libsanitizer/lsan/lsan_thread.h", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_thread.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Flsan%2Flsan_thread.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_thread.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -45,11 +45,8 @@ class ThreadContext;\n void InitializeThreadRegistry();\n void InitializeMainThread();\n \n-u32 ThreadCreate(u32 tid, uptr uid, bool detached, void *arg = nullptr);\n+u32 ThreadCreate(u32 tid, bool detached, void *arg = nullptr);\n void ThreadFinish();\n-void ThreadDetach(u32 tid);\n-void ThreadJoin(u32 tid);\n-u32 ThreadTid(uptr uid);\n \n u32 GetCurrentThread();\n void SetCurrentThread(u32 tid);"}, {"sha": "fe48b9caf0670b7533f03845601a0d2ea3a6b1af", "filename": "libsanitizer/sanitizer_common/sanitizer_addrhashmap.h", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_addrhashmap.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_addrhashmap.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_addrhashmap.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -201,7 +201,8 @@ AddrHashMap<T, kSize>::AddrHashMap() {\n }\n \n template <typename T, uptr kSize>\n-void AddrHashMap<T, kSize>::acquire(Handle *h) NO_THREAD_SAFETY_ANALYSIS {\n+void AddrHashMap<T, kSize>::acquire(Handle *h)\n+    SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n   uptr addr = h->addr_;\n   uptr hash = calcHash(addr);\n   Bucket *b = &table_[hash];\n@@ -330,7 +331,8 @@ void AddrHashMap<T, kSize>::acquire(Handle *h) NO_THREAD_SAFETY_ANALYSIS {\n  }\n \n  template <typename T, uptr kSize>\n- void AddrHashMap<T, kSize>::release(Handle *h) NO_THREAD_SAFETY_ANALYSIS {\n+ void AddrHashMap<T, kSize>::release(Handle *h)\n+     SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n    if (!h->cell_)\n      return;\n    Bucket *b = h->bucket_;"}, {"sha": "25a43a59f0475a37d516325b3b52203a47c2e454", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator.cpp", "status": "modified", "additions": 21, "deletions": 62, "changes": 83, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -17,73 +17,14 @@\n #include \"sanitizer_allocator_internal.h\"\n #include \"sanitizer_atomic.h\"\n #include \"sanitizer_common.h\"\n+#include \"sanitizer_platform.h\"\n \n namespace __sanitizer {\n \n // Default allocator names.\n const char *PrimaryAllocatorName = \"SizeClassAllocator\";\n const char *SecondaryAllocatorName = \"LargeMmapAllocator\";\n \n-// ThreadSanitizer for Go uses libc malloc/free.\n-#if defined(SANITIZER_USE_MALLOC)\n-# if SANITIZER_LINUX && !SANITIZER_ANDROID\n-extern \"C\" void *__libc_malloc(uptr size);\n-#  if !SANITIZER_GO\n-extern \"C\" void *__libc_memalign(uptr alignment, uptr size);\n-#  endif\n-extern \"C\" void *__libc_realloc(void *ptr, uptr size);\n-extern \"C\" void __libc_free(void *ptr);\n-# else\n-#  include <stdlib.h>\n-#  define __libc_malloc malloc\n-#  if !SANITIZER_GO\n-static void *__libc_memalign(uptr alignment, uptr size) {\n-  void *p;\n-  uptr error = posix_memalign(&p, alignment, size);\n-  if (error) return nullptr;\n-  return p;\n-}\n-#  endif\n-#  define __libc_realloc realloc\n-#  define __libc_free free\n-# endif\n-\n-static void *RawInternalAlloc(uptr size, InternalAllocatorCache *cache,\n-                              uptr alignment) {\n-  (void)cache;\n-#if !SANITIZER_GO\n-  if (alignment == 0)\n-    return __libc_malloc(size);\n-  else\n-    return __libc_memalign(alignment, size);\n-#else\n-  // Windows does not provide __libc_memalign/posix_memalign. It provides\n-  // __aligned_malloc, but the allocated blocks can't be passed to free,\n-  // they need to be passed to __aligned_free. InternalAlloc interface does\n-  // not account for such requirement. Alignemnt does not seem to be used\n-  // anywhere in runtime, so just call __libc_malloc for now.\n-  DCHECK_EQ(alignment, 0);\n-  return __libc_malloc(size);\n-#endif\n-}\n-\n-static void *RawInternalRealloc(void *ptr, uptr size,\n-                                InternalAllocatorCache *cache) {\n-  (void)cache;\n-  return __libc_realloc(ptr, size);\n-}\n-\n-static void RawInternalFree(void *ptr, InternalAllocatorCache *cache) {\n-  (void)cache;\n-  __libc_free(ptr);\n-}\n-\n-InternalAllocator *internal_allocator() {\n-  return 0;\n-}\n-\n-#else  // SANITIZER_GO || defined(SANITIZER_USE_MALLOC)\n-\n static ALIGNED(64) char internal_alloc_placeholder[sizeof(InternalAllocator)];\n static atomic_uint8_t internal_allocator_initialized;\n static StaticSpinMutex internal_alloc_init_mu;\n@@ -135,8 +76,6 @@ static void RawInternalFree(void *ptr, InternalAllocatorCache *cache) {\n   internal_allocator()->Deallocate(cache, ptr);\n }\n \n-#endif  // SANITIZER_GO || defined(SANITIZER_USE_MALLOC)\n-\n static void NORETURN ReportInternalAllocatorOutOfMemory(uptr requested_size) {\n   SetAllocatorOutOfMemory();\n   Report(\"FATAL: %s: internal allocator is out of memory trying to allocate \"\n@@ -187,6 +126,16 @@ void InternalFree(void *addr, InternalAllocatorCache *cache) {\n   RawInternalFree(addr, cache);\n }\n \n+void InternalAllocatorLock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  internal_allocator_cache_mu.Lock();\n+  internal_allocator()->ForceLock();\n+}\n+\n+void InternalAllocatorUnlock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  internal_allocator()->ForceUnlock();\n+  internal_allocator_cache_mu.Unlock();\n+}\n+\n // LowLevelAllocator\n constexpr uptr kLowLevelAllocatorDefaultAlignment = 8;\n static uptr low_level_alloc_min_alignment = kLowLevelAllocatorDefaultAlignment;\n@@ -247,4 +196,14 @@ void PrintHintAllocatorCannotReturnNull() {\n          \"allocator_may_return_null=1\\n\");\n }\n \n+static atomic_uint8_t rss_limit_exceeded;\n+\n+bool IsRssLimitExceeded() {\n+  return atomic_load(&rss_limit_exceeded, memory_order_relaxed);\n+}\n+\n+void SetRssLimitExceeded(bool limit_exceeded) {\n+  atomic_store(&rss_limit_exceeded, limit_exceeded, memory_order_relaxed);\n+}\n+\n } // namespace __sanitizer"}, {"sha": "76b936ff5eaa6c8a2b18cc00b89f338e4b668d6a", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -70,6 +70,9 @@ inline void RandomShuffle(T *a, u32 n, u32 *rand_state) {\n #include \"sanitizer_allocator_secondary.h\"\n #include \"sanitizer_allocator_combined.h\"\n \n+bool IsRssLimitExceeded();\n+void SetRssLimitExceeded(bool limit_exceeded);\n+\n } // namespace __sanitizer\n \n #endif // SANITIZER_ALLOCATOR_H"}, {"sha": "b92cfa5bf4c4be96274e7f1d188686ba0a943984", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator_combined.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_combined.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_combined.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_combined.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -175,12 +175,12 @@ class CombinedAllocator {\n \n   // ForceLock() and ForceUnlock() are needed to implement Darwin malloc zone\n   // introspection API.\n-  void ForceLock() NO_THREAD_SAFETY_ANALYSIS {\n+  void ForceLock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n     primary_.ForceLock();\n     secondary_.ForceLock();\n   }\n \n-  void ForceUnlock() NO_THREAD_SAFETY_ANALYSIS {\n+  void ForceUnlock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n     secondary_.ForceUnlock();\n     primary_.ForceUnlock();\n   }"}, {"sha": "38994736877acb66b0fd588d6639e517aa314107", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator_internal.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_internal.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_internal.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_internal.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -48,6 +48,8 @@ void *InternalReallocArray(void *p, uptr count, uptr size,\n void *InternalCalloc(uptr count, uptr size,\n                      InternalAllocatorCache *cache = nullptr);\n void InternalFree(void *p, InternalAllocatorCache *cache = nullptr);\n+void InternalAllocatorLock();\n+void InternalAllocatorUnlock();\n InternalAllocator *internal_allocator();\n \n } // namespace __sanitizer"}, {"sha": "f2471efced613681808f0a81f68924c61e98315a", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator_primary32.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary32.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary32.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary32.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -238,13 +238,13 @@ class SizeClassAllocator32 {\n \n   // ForceLock() and ForceUnlock() are needed to implement Darwin malloc zone\n   // introspection API.\n-  void ForceLock() NO_THREAD_SAFETY_ANALYSIS {\n+  void ForceLock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n     for (uptr i = 0; i < kNumClasses; i++) {\n       GetSizeClassInfo(i)->mutex.Lock();\n     }\n   }\n \n-  void ForceUnlock() NO_THREAD_SAFETY_ANALYSIS {\n+  void ForceUnlock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n     for (int i = kNumClasses - 1; i >= 0; i--) {\n       GetSizeClassInfo(i)->mutex.Unlock();\n     }"}, {"sha": "66ba71d325dad94283032a9bc04a1083ecd0d6a4", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator_primary64.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary64.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -354,13 +354,13 @@ class SizeClassAllocator64 {\n \n   // ForceLock() and ForceUnlock() are needed to implement Darwin malloc zone\n   // introspection API.\n-  void ForceLock() NO_THREAD_SAFETY_ANALYSIS {\n+  void ForceLock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n     for (uptr i = 0; i < kNumClasses; i++) {\n       GetRegionInfo(i)->mutex.Lock();\n     }\n   }\n \n-  void ForceUnlock() NO_THREAD_SAFETY_ANALYSIS {\n+  void ForceUnlock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n     for (int i = (int)kNumClasses - 1; i >= 0; i--) {\n       GetRegionInfo(i)->mutex.Unlock();\n     }"}, {"sha": "48afb2a2983419e28c75a39f474780d4d2119266", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator_secondary.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_secondary.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_secondary.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_secondary.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -267,9 +267,9 @@ class LargeMmapAllocator {\n \n   // ForceLock() and ForceUnlock() are needed to implement Darwin malloc zone\n   // introspection API.\n-  void ForceLock() ACQUIRE(mutex_) { mutex_.Lock(); }\n+  void ForceLock() SANITIZER_ACQUIRE(mutex_) { mutex_.Lock(); }\n \n-  void ForceUnlock() RELEASE(mutex_) { mutex_.Unlock(); }\n+  void ForceUnlock() SANITIZER_RELEASE(mutex_) { mutex_.Unlock(); }\n \n   // Iterate over all existing chunks.\n   // The allocator must be locked when calling this function."}, {"sha": "4318d64d16cfa21114dc50de6a45b3b15318d8b6", "filename": "libsanitizer/sanitizer_common/sanitizer_atomic_clang.h", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_atomic_clang.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_atomic_clang.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_atomic_clang.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -74,13 +74,12 @@ template <typename T>\n inline bool atomic_compare_exchange_strong(volatile T *a, typename T::Type *cmp,\n                                            typename T::Type xchg,\n                                            memory_order mo) {\n-  typedef typename T::Type Type;\n-  Type cmpv = *cmp;\n-  Type prev;\n-  prev = __sync_val_compare_and_swap(&a->val_dont_use, cmpv, xchg);\n-  if (prev == cmpv) return true;\n-  *cmp = prev;\n-  return false;\n+  // Transitioned from __sync_val_compare_and_swap to support targets like\n+  // SPARC V8 that cannot inline atomic cmpxchg.  __atomic_compare_exchange\n+  // can then be resolved from libatomic.  __ATOMIC_SEQ_CST is used to best\n+  // match the __sync builtin memory order.\n+  return __atomic_compare_exchange(&a->val_dont_use, cmp, &xchg, false,\n+                                   __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST);\n }\n \n template<typename T>"}, {"sha": "472b83d63a08a499be768f6a4325ebd6afa06c4d", "filename": "libsanitizer/sanitizer_common/sanitizer_chained_origin_depot.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_chained_origin_depot.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_chained_origin_depot.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_chained_origin_depot.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -11,7 +11,6 @@\n \n #include \"sanitizer_chained_origin_depot.h\"\n \n-#include \"sanitizer_persistent_allocator.h\"\n #include \"sanitizer_stackdepotbase.h\"\n \n namespace __sanitizer {"}, {"sha": "e30a93da5b598d502bab430acdf69a0b1cf616db", "filename": "libsanitizer/sanitizer_common/sanitizer_common.cpp", "status": "modified", "additions": 32, "deletions": 7, "changes": 39, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -11,10 +11,12 @@\n //===----------------------------------------------------------------------===//\n \n #include \"sanitizer_common.h\"\n+\n #include \"sanitizer_allocator_interface.h\"\n #include \"sanitizer_allocator_internal.h\"\n #include \"sanitizer_atomic.h\"\n #include \"sanitizer_flags.h\"\n+#include \"sanitizer_interface_internal.h\"\n #include \"sanitizer_libc.h\"\n #include \"sanitizer_placement_new.h\"\n \n@@ -138,13 +140,21 @@ void LoadedModule::set(const char *module_name, uptr base_address,\n   set(module_name, base_address);\n   arch_ = arch;\n   internal_memcpy(uuid_, uuid, sizeof(uuid_));\n+  uuid_size_ = kModuleUUIDSize;\n   instrumented_ = instrumented;\n }\n \n+void LoadedModule::setUuid(const char *uuid, uptr size) {\n+  if (size > kModuleUUIDSize)\n+    size = kModuleUUIDSize;\n+  internal_memcpy(uuid_, uuid, size);\n+  uuid_size_ = size;\n+}\n+\n void LoadedModule::clear() {\n   InternalFree(full_name_);\n   base_address_ = 0;\n-  max_executable_address_ = 0;\n+  max_address_ = 0;\n   full_name_ = nullptr;\n   arch_ = kModuleArchUnknown;\n   internal_memset(uuid_, 0, kModuleUUIDSize);\n@@ -162,8 +172,7 @@ void LoadedModule::addAddressRange(uptr beg, uptr end, bool executable,\n   AddressRange *r =\n       new(mem) AddressRange(beg, end, executable, writable, name);\n   ranges_.push_back(r);\n-  if (executable && end > max_executable_address_)\n-    max_executable_address_ = end;\n+  max_address_ = Max(max_address_, end);\n }\n \n bool LoadedModule::containsAddress(uptr address) const {\n@@ -301,18 +310,22 @@ struct MallocFreeHook {\n \n static MallocFreeHook MFHooks[kMaxMallocFreeHooks];\n \n-void RunMallocHooks(const void *ptr, uptr size) {\n+void RunMallocHooks(void *ptr, uptr size) {\n+  __sanitizer_malloc_hook(ptr, size);\n   for (int i = 0; i < kMaxMallocFreeHooks; i++) {\n     auto hook = MFHooks[i].malloc_hook;\n-    if (!hook) return;\n+    if (!hook)\n+      break;\n     hook(ptr, size);\n   }\n }\n \n-void RunFreeHooks(const void *ptr) {\n+void RunFreeHooks(void *ptr) {\n+  __sanitizer_free_hook(ptr);\n   for (int i = 0; i < kMaxMallocFreeHooks; i++) {\n     auto hook = MFHooks[i].free_hook;\n-    if (!hook) return;\n+    if (!hook)\n+      break;\n     hook(ptr);\n   }\n }\n@@ -360,4 +373,16 @@ int __sanitizer_install_malloc_and_free_hooks(void (*malloc_hook)(const void *,\n                                               void (*free_hook)(const void *)) {\n   return InstallMallocFreeHooks(malloc_hook, free_hook);\n }\n+\n+// Provide default (no-op) implementation of malloc hooks.\n+SANITIZER_INTERFACE_WEAK_DEF(void, __sanitizer_malloc_hook, void *ptr,\n+                             uptr size) {\n+  (void)ptr;\n+  (void)size;\n+}\n+\n+SANITIZER_INTERFACE_WEAK_DEF(void, __sanitizer_free_hook, void *ptr) {\n+  (void)ptr;\n+}\n+\n } // extern \"C\""}, {"sha": "17570d606885608217e5aedb225cfb83faff3da2", "filename": "libsanitizer/sanitizer_common/sanitizer_common.h", "status": "modified", "additions": 26, "deletions": 24, "changes": 50, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -16,7 +16,6 @@\n #define SANITIZER_COMMON_H\n \n #include \"sanitizer_flags.h\"\n-#include \"sanitizer_interface_internal.h\"\n #include \"sanitizer_internal_defs.h\"\n #include \"sanitizer_libc.h\"\n #include \"sanitizer_list.h\"\n@@ -171,8 +170,8 @@ void SetShadowRegionHugePageMode(uptr addr, uptr length);\n bool DontDumpShadowMemory(uptr addr, uptr length);\n // Check if the built VMA size matches the runtime one.\n void CheckVMASize();\n-void RunMallocHooks(const void *ptr, uptr size);\n-void RunFreeHooks(const void *ptr);\n+void RunMallocHooks(void *ptr, uptr size);\n+void RunFreeHooks(void *ptr);\n \n class ReservedAddressRange {\n  public:\n@@ -238,12 +237,12 @@ void SetPrintfAndReportCallback(void (*callback)(const char *));\n // Lock sanitizer error reporting and protects against nested errors.\n class ScopedErrorReportLock {\n  public:\n-  ScopedErrorReportLock() ACQUIRE(mutex_) { Lock(); }\n-  ~ScopedErrorReportLock() RELEASE(mutex_) { Unlock(); }\n+  ScopedErrorReportLock() SANITIZER_ACQUIRE(mutex_) { Lock(); }\n+  ~ScopedErrorReportLock() SANITIZER_RELEASE(mutex_) { Unlock(); }\n \n-  static void Lock() ACQUIRE(mutex_);\n-  static void Unlock() RELEASE(mutex_);\n-  static void CheckLocked() CHECK_LOCKED(mutex_);\n+  static void Lock() SANITIZER_ACQUIRE(mutex_);\n+  static void Unlock() SANITIZER_RELEASE(mutex_);\n+  static void CheckLocked() SANITIZER_CHECK_LOCKED(mutex_);\n \n  private:\n   static atomic_uintptr_t reporting_thread_;\n@@ -286,7 +285,7 @@ void SetStackSizeLimitInBytes(uptr limit);\n bool AddressSpaceIsUnlimited();\n void SetAddressSpaceUnlimited();\n void AdjustStackSize(void *attr);\n-void PlatformPrepareForSandboxing(__sanitizer_sandbox_arguments *args);\n+void PlatformPrepareForSandboxing(void *args);\n void SetSandboxingCallback(void (*f)());\n \n void InitializeCoverage(bool enabled, const char *coverage_dir);\n@@ -326,12 +325,6 @@ void SetUserDieCallback(DieCallbackType callback);\n \n void SetCheckUnwindCallback(void (*callback)());\n \n-// Callback will be called if soft_rss_limit_mb is given and the limit is\n-// exceeded (exceeded==true) or if rss went down below the limit\n-// (exceeded==false).\n-// The callback should be registered once at the tool init time.\n-void SetSoftRssLimitExceededCallback(void (*Callback)(bool exceeded));\n-\n // Functions related to signal handling.\n typedef void (*SignalHandlerType)(int, void *, void *);\n HandleSignalMode GetHandleSignalMode(int signum);\n@@ -460,6 +453,10 @@ template <class T>\n constexpr T Max(T a, T b) {\n   return a > b ? a : b;\n }\n+template <class T>\n+constexpr T Abs(T a) {\n+  return a < 0 ? -a : a;\n+}\n template<class T> void Swap(T& a, T& b) {\n   T tmp = a;\n   a = b;\n@@ -669,11 +666,9 @@ void Sort(T *v, uptr size, Compare comp = {}) {\n \n // Works like std::lower_bound: finds the first element that is not less\n // than the val.\n-template <class Container,\n+template <class Container, class T,\n           class Compare = CompareLess<typename Container::value_type>>\n-uptr InternalLowerBound(const Container &v,\n-                        const typename Container::value_type &val,\n-                        Compare comp = {}) {\n+uptr InternalLowerBound(const Container &v, const T &val, Compare comp = {}) {\n   uptr first = 0;\n   uptr last = v.size();\n   while (last > first) {\n@@ -743,6 +738,9 @@ bool ReadFileToBuffer(const char *file_name, char **buff, uptr *buff_size,\n                       uptr *read_len, uptr max_len = kDefaultFileMaxSize,\n                       error_t *errno_p = nullptr);\n \n+int GetModuleAndOffsetForPc(uptr pc, char *module_name, uptr module_name_len,\n+                            uptr *pc_offset);\n+\n // When adding a new architecture, don't forget to also update\n // script/asan_symbolize.py and sanitizer_symbolizer_libcdep.cpp.\n inline const char *ModuleArchToString(ModuleArch arch) {\n@@ -774,7 +772,7 @@ inline const char *ModuleArchToString(ModuleArch arch) {\n   return \"\";\n }\n \n-const uptr kModuleUUIDSize = 16;\n+const uptr kModuleUUIDSize = 32;\n const uptr kMaxSegName = 16;\n \n // Represents a binary loaded into virtual memory (e.g. this can be an\n@@ -784,25 +782,28 @@ class LoadedModule {\n   LoadedModule()\n       : full_name_(nullptr),\n         base_address_(0),\n-        max_executable_address_(0),\n+        max_address_(0),\n         arch_(kModuleArchUnknown),\n+        uuid_size_(0),\n         instrumented_(false) {\n     internal_memset(uuid_, 0, kModuleUUIDSize);\n     ranges_.clear();\n   }\n   void set(const char *module_name, uptr base_address);\n   void set(const char *module_name, uptr base_address, ModuleArch arch,\n            u8 uuid[kModuleUUIDSize], bool instrumented);\n+  void setUuid(const char *uuid, uptr size);\n   void clear();\n   void addAddressRange(uptr beg, uptr end, bool executable, bool writable,\n                        const char *name = nullptr);\n   bool containsAddress(uptr address) const;\n \n   const char *full_name() const { return full_name_; }\n   uptr base_address() const { return base_address_; }\n-  uptr max_executable_address() const { return max_executable_address_; }\n+  uptr max_address() const { return max_address_; }\n   ModuleArch arch() const { return arch_; }\n   const u8 *uuid() const { return uuid_; }\n+  uptr uuid_size() const { return uuid_size_; }\n   bool instrumented() const { return instrumented_; }\n \n   struct AddressRange {\n@@ -829,8 +830,9 @@ class LoadedModule {\n  private:\n   char *full_name_;  // Owned.\n   uptr base_address_;\n-  uptr max_executable_address_;\n+  uptr max_address_;\n   ModuleArch arch_;\n+  uptr uuid_size_;\n   u8 uuid_[kModuleUUIDSize];\n   bool instrumented_;\n   IntrusiveList<AddressRange> ranges_;\n@@ -956,7 +958,7 @@ struct SignalContext {\n   uptr sp;\n   uptr bp;\n   bool is_memory_access;\n-  enum WriteFlag { UNKNOWN, READ, WRITE } write_flag;\n+  enum WriteFlag { Unknown, Read, Write } write_flag;\n \n   // In some cases the kernel cannot provide the true faulting address; `addr`\n   // will be zero then.  This field allows to distinguish between these cases"}, {"sha": "43296e6c1f6a727c49c00e9b898cdab6f7f85f86", "filename": "libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc", "status": "modified", "additions": 148, "deletions": 11, "changes": 159, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interceptors.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interceptors.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interceptors.inc?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -21,7 +21,7 @@\n //   COMMON_INTERCEPTOR_FD_RELEASE\n //   COMMON_INTERCEPTOR_FD_ACCESS\n //   COMMON_INTERCEPTOR_SET_THREAD_NAME\n-//   COMMON_INTERCEPTOR_ON_DLOPEN\n+//   COMMON_INTERCEPTOR_DLOPEN\n //   COMMON_INTERCEPTOR_ON_EXIT\n //   COMMON_INTERCEPTOR_MUTEX_PRE_LOCK\n //   COMMON_INTERCEPTOR_MUTEX_POST_LOCK\n@@ -132,6 +132,76 @@ extern const short *_toupper_tab_;\n extern const short *_tolower_tab_;\n #endif\n \n+#if SANITIZER_MUSL && \\\n+  (defined(__i386__) || defined(__arm__) || SANITIZER_MIPS32 || SANITIZER_PPC32)\n+// musl 1.2.0 on existing 32-bit architectures uses new symbol names for the\n+// time-related functions that take 64-bit time_t values.  See\n+// https://musl.libc.org/time64.html\n+#define adjtime __adjtime64\n+#define adjtimex __adjtimex_time64\n+#define aio_suspend __aio_suspend_time64\n+#define clock_adjtime __clock_adjtime64\n+#define clock_getres __clock_getres_time64\n+#define clock_gettime __clock_gettime64\n+#define clock_nanosleep __clock_nanosleep_time64\n+#define clock_settime __clock_settime64\n+#define cnd_timedwait __cnd_timedwait_time64\n+#define ctime __ctime64\n+#define ctime_r __ctime64_r\n+#define difftime __difftime64\n+#define dlsym __dlsym_time64\n+#define fstatat __fstatat_time64\n+#define fstat __fstat_time64\n+#define ftime __ftime64\n+#define futimens __futimens_time64\n+#define futimesat __futimesat_time64\n+#define futimes __futimes_time64\n+#define getitimer __getitimer_time64\n+#define getrusage __getrusage_time64\n+#define gettimeofday __gettimeofday_time64\n+#define gmtime __gmtime64\n+#define gmtime_r __gmtime64_r\n+#define localtime __localtime64\n+#define localtime_r __localtime64_r\n+#define lstat __lstat_time64\n+#define lutimes __lutimes_time64\n+#define mktime __mktime64\n+#define mq_timedreceive __mq_timedreceive_time64\n+#define mq_timedsend __mq_timedsend_time64\n+#define mtx_timedlock __mtx_timedlock_time64\n+#define nanosleep __nanosleep_time64\n+#define ppoll __ppoll_time64\n+#define pselect __pselect_time64\n+#define pthread_cond_timedwait __pthread_cond_timedwait_time64\n+#define pthread_mutex_timedlock __pthread_mutex_timedlock_time64\n+#define pthread_rwlock_timedrdlock __pthread_rwlock_timedrdlock_time64\n+#define pthread_rwlock_timedwrlock __pthread_rwlock_timedwrlock_time64\n+#define pthread_timedjoin_np __pthread_timedjoin_np_time64\n+#define recvmmsg __recvmmsg_time64\n+#define sched_rr_get_interval __sched_rr_get_interval_time64\n+#define select __select_time64\n+#define semtimedop __semtimedop_time64\n+#define sem_timedwait __sem_timedwait_time64\n+#define setitimer __setitimer_time64\n+#define settimeofday __settimeofday_time64\n+#define sigtimedwait __sigtimedwait_time64\n+#define stat __stat_time64\n+#define stime __stime64\n+#define thrd_sleep __thrd_sleep_time64\n+#define timegm __timegm_time64\n+#define timerfd_gettime __timerfd_gettime64\n+#define timerfd_settime __timerfd_settime64\n+#define timer_gettime __timer_gettime64\n+#define timer_settime __timer_settime64\n+#define timespec_get __timespec_get_time64\n+#define time __time64\n+#define utimensat __utimensat_time64\n+#define utimes __utimes_time64\n+#define utime __utime64\n+#define wait3 __wait3_time64\n+#define wait4 __wait4_time64\n+#endif\n+\n // Platform-specific options.\n #if SANITIZER_MAC\n #define PLATFORM_HAS_DIFFERENT_MEMCPY_AND_MEMMOVE 0\n@@ -206,9 +276,9 @@ extern const short *_tolower_tab_;\n     COMMON_INTERCEPTOR_READ_RANGE((ctx), (s),                       \\\n       common_flags()->strict_string_checks ? (internal_strlen(s)) + 1 : (n) )\n \n-#ifndef COMMON_INTERCEPTOR_ON_DLOPEN\n-#define COMMON_INTERCEPTOR_ON_DLOPEN(filename, flag) \\\n-  CheckNoDeepBind(filename, flag);\n+#ifndef COMMON_INTERCEPTOR_DLOPEN\n+#define COMMON_INTERCEPTOR_DLOPEN(filename, flag) \\\n+  ({ CheckNoDeepBind(filename, flag); REAL(dlopen)(filename, flag); })\n #endif\n \n #ifndef COMMON_INTERCEPTOR_GET_TLS_RANGE\n@@ -1295,12 +1365,16 @@ INTERCEPTOR(int, prctl, int option, unsigned long arg2, unsigned long arg3,\n   void *ctx;\n   COMMON_INTERCEPTOR_ENTER(ctx, prctl, option, arg2, arg3, arg4, arg5);\n   static const int PR_SET_NAME = 15;\n+  static const int PR_SCHED_CORE = 62;\n+  static const int PR_SCHED_CORE_GET = 0;\n   int res = REAL(prctl(option, arg2, arg3, arg4, arg5));\n   if (option == PR_SET_NAME) {\n     char buff[16];\n     internal_strncpy(buff, (char *)arg2, 15);\n     buff[15] = 0;\n     COMMON_INTERCEPTOR_SET_THREAD_NAME(ctx, buff);\n+  } else if (res != -1 && option == PR_SCHED_CORE && arg2 == PR_SCHED_CORE_GET) {\n+    COMMON_INTERCEPTOR_WRITE_RANGE(ctx, (u64*)(arg5), sizeof(u64));\n   }\n   return res;\n }\n@@ -2422,6 +2496,34 @@ INTERCEPTOR(int, glob64, const char *pattern, int flags,\n #define INIT_GLOB64\n #endif  // SANITIZER_INTERCEPT_GLOB64\n \n+#if SANITIZER_INTERCEPT___B64_TO\n+INTERCEPTOR(int, __b64_ntop, unsigned char const *src, SIZE_T srclength,\n+            char *target, SIZE_T targsize) {\n+  void *ctx;\n+  COMMON_INTERCEPTOR_ENTER(ctx, __b64_ntop, src, srclength, target, targsize);\n+  COMMON_INTERCEPTOR_READ_RANGE(ctx, src, srclength);\n+  int res = REAL(__b64_ntop)(src, srclength, target, targsize);\n+  if (res >= 0)\n+    COMMON_INTERCEPTOR_WRITE_RANGE(ctx, target, res + 1);\n+  return res;\n+}\n+INTERCEPTOR(int, __b64_pton, char const *src, char *target, SIZE_T targsize) {\n+  void *ctx;\n+  COMMON_INTERCEPTOR_ENTER(ctx, __b64_pton, src, target, targsize);\n+  COMMON_INTERCEPTOR_READ_RANGE(ctx, src, internal_strlen(src) + 1);\n+  int res = REAL(__b64_pton)(src, target, targsize);\n+  if (res >= 0)\n+    COMMON_INTERCEPTOR_WRITE_RANGE(ctx, target, res);\n+  return res;\n+}\n+#  define INIT___B64_TO                    \\\n+    COMMON_INTERCEPT_FUNCTION(__b64_ntop); \\\n+    COMMON_INTERCEPT_FUNCTION(__b64_pton);\n+#else  // SANITIZER_INTERCEPT___B64_TO\n+#define INIT___B64_TO\n+#endif  // SANITIZER_INTERCEPT___B64_TO\n+\n+\n #if SANITIZER_INTERCEPT_POSIX_SPAWN\n \n template <class RealSpawnPtr>\n@@ -6380,8 +6482,7 @@ INTERCEPTOR(void*, dlopen, const char *filename, int flag) {\n   void *ctx;\n   COMMON_INTERCEPTOR_ENTER_NOIGNORE(ctx, dlopen, filename, flag);\n   if (filename) COMMON_INTERCEPTOR_READ_STRING(ctx, filename, 0);\n-  COMMON_INTERCEPTOR_ON_DLOPEN(filename, flag);\n-  void *res = REAL(dlopen)(filename, flag);\n+  void *res = COMMON_INTERCEPTOR_DLOPEN(filename, flag);\n   Symbolizer::GetOrInit()->InvalidateModuleList();\n   COMMON_INTERCEPTOR_LIBRARY_LOADED(filename, res);\n   return res;\n@@ -6872,6 +6973,23 @@ INTERCEPTOR(int, stat, const char *path, void *buf) {\n #define INIT_STAT\n #endif\n \n+#if SANITIZER_INTERCEPT_STAT64\n+INTERCEPTOR(int, stat64, const char *path, void *buf) {\n+  void *ctx;\n+  COMMON_INTERCEPTOR_ENTER(ctx, stat64, path, buf);\n+  if (common_flags()->intercept_stat)\n+    COMMON_INTERCEPTOR_READ_STRING(ctx, path, 0);\n+  int res = REAL(stat64)(path, buf);\n+  if (!res)\n+    COMMON_INTERCEPTOR_WRITE_RANGE(ctx, buf, __sanitizer::struct_stat64_sz);\n+  return res;\n+}\n+#define INIT_STAT64 COMMON_INTERCEPT_FUNCTION(stat64)\n+#else\n+#define INIT_STAT64\n+#endif\n+\n+\n #if SANITIZER_INTERCEPT_LSTAT\n INTERCEPTOR(int, lstat, const char *path, void *buf) {\n   void *ctx;\n@@ -6888,6 +7006,22 @@ INTERCEPTOR(int, lstat, const char *path, void *buf) {\n #define INIT_LSTAT\n #endif\n \n+#if SANITIZER_INTERCEPT_STAT64\n+INTERCEPTOR(int, lstat64, const char *path, void *buf) {\n+  void *ctx;\n+  COMMON_INTERCEPTOR_ENTER(ctx, lstat64, path, buf);\n+  if (common_flags()->intercept_stat)\n+    COMMON_INTERCEPTOR_READ_STRING(ctx, path, 0);\n+  int res = REAL(lstat64)(path, buf);\n+  if (!res)\n+    COMMON_INTERCEPTOR_WRITE_RANGE(ctx, buf, __sanitizer::struct_stat64_sz);\n+  return res;\n+}\n+#define INIT_LSTAT64 COMMON_INTERCEPT_FUNCTION(lstat64)\n+#else\n+#define INIT_LSTAT64\n+#endif\n+\n #if SANITIZER_INTERCEPT___XSTAT\n INTERCEPTOR(int, __xstat, int version, const char *path, void *buf) {\n   void *ctx;\n@@ -7858,12 +7992,12 @@ INTERCEPTOR(void, setbuf, __sanitizer_FILE *stream, char *buf) {\n       unpoison_file(stream);\n }\n \n-INTERCEPTOR(void, setbuffer, __sanitizer_FILE *stream, char *buf, int mode) {\n+INTERCEPTOR(void, setbuffer, __sanitizer_FILE *stream, char *buf, SIZE_T size) {\n   void *ctx;\n-  COMMON_INTERCEPTOR_ENTER(ctx, setbuffer, stream, buf, mode);\n-  REAL(setbuffer)(stream, buf, mode);\n+  COMMON_INTERCEPTOR_ENTER(ctx, setbuffer, stream, buf, size);\n+  REAL(setbuffer)(stream, buf, size);\n   if (buf) {\n-    COMMON_INTERCEPTOR_WRITE_RANGE(ctx, buf, __sanitizer_bufsiz);\n+    COMMON_INTERCEPTOR_WRITE_RANGE(ctx, buf, size);\n   }\n   if (stream)\n     unpoison_file(stream);\n@@ -7905,7 +8039,7 @@ INTERCEPTOR(int, regcomp, void *preg, const char *pattern, int cflags) {\n   if (pattern)\n     COMMON_INTERCEPTOR_READ_RANGE(ctx, pattern, internal_strlen(pattern) + 1);\n   int res = REAL(regcomp)(preg, pattern, cflags);\n-  if (!res)\n+  if (preg)\n     COMMON_INTERCEPTOR_WRITE_RANGE(ctx, preg, struct_regex_sz);\n   return res;\n }\n@@ -10290,6 +10424,7 @@ static void InitializeCommonInterceptors() {\n   INIT_TIME;\n   INIT_GLOB;\n   INIT_GLOB64;\n+  INIT___B64_TO;\n   INIT_POSIX_SPAWN;\n   INIT_WAIT;\n   INIT_WAIT4;\n@@ -10447,8 +10582,10 @@ static void InitializeCommonInterceptors() {\n   INIT_RECV_RECVFROM;\n   INIT_SEND_SENDTO;\n   INIT_STAT;\n+  INIT_STAT64;\n   INIT_EVENTFD_READ_WRITE;\n   INIT_LSTAT;\n+  INIT_LSTAT64;\n   INIT___XSTAT;\n   INIT___XSTAT64;\n   INIT___LXSTAT;"}, {"sha": "49ec4097c900bd7fd23efaa7ac3bf32438d67ee8", "filename": "libsanitizer/sanitizer_common/sanitizer_common_interceptors_ioctl.inc", "status": "modified", "additions": 9, "deletions": 1, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interceptors_ioctl.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interceptors_ioctl.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interceptors_ioctl.inc?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -115,11 +115,19 @@ static void ioctl_table_fill() {\n   // _(SOUND_MIXER_WRITE_MUTE, WRITE, sizeof(int)); // same as ...WRITE_ENHANCE\n   _(BLKFLSBUF, NONE, 0);\n   _(BLKGETSIZE, WRITE, sizeof(uptr));\n-  _(BLKRAGET, WRITE, sizeof(int));\n+  _(BLKRAGET, WRITE, sizeof(uptr));\n   _(BLKRASET, NONE, 0);\n   _(BLKROGET, WRITE, sizeof(int));\n   _(BLKROSET, READ, sizeof(int));\n   _(BLKRRPART, NONE, 0);\n+  _(BLKFRASET, NONE, 0);\n+  _(BLKFRAGET, WRITE, sizeof(uptr));\n+  _(BLKSECTSET, READ, sizeof(short));\n+  _(BLKSECTGET, WRITE, sizeof(short));\n+  _(BLKSSZGET, WRITE, sizeof(int));\n+  _(BLKBSZGET, WRITE, sizeof(int));\n+  _(BLKBSZSET, READ, sizeof(uptr));\n+  _(BLKGETSIZE64, WRITE, sizeof(u64));\n   _(CDROMEJECT, NONE, 0);\n   _(CDROMEJECT_SW, NONE, 0);\n   _(CDROMMULTISESSION, WRITE, struct_cdrom_multisession_sz);"}, {"sha": "a5259be9335aca8434b37397bbb78af2af65a424", "filename": "libsanitizer/sanitizer_common/sanitizer_common_interface_posix.inc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interface_posix.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interface_posix.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interface_posix.inc?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -11,3 +11,5 @@ INTERFACE_WEAK_FUNCTION(__sanitizer_symbolize_code)\n INTERFACE_WEAK_FUNCTION(__sanitizer_symbolize_data)\n INTERFACE_WEAK_FUNCTION(__sanitizer_symbolize_demangle)\n INTERFACE_WEAK_FUNCTION(__sanitizer_symbolize_flush)\n+INTERFACE_WEAK_FUNCTION(__sanitizer_symbolize_set_demangle)\n+INTERFACE_WEAK_FUNCTION(__sanitizer_symbolize_set_inline_frames)"}, {"sha": "8fd3985642809426ca94d7069ffd537b25dfd3af", "filename": "libsanitizer/sanitizer_common/sanitizer_common_libcdep.cpp", "status": "modified", "additions": 54, "deletions": 23, "changes": 77, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_libcdep.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_libcdep.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_libcdep.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -10,25 +10,22 @@\n // run-time libraries.\n //===----------------------------------------------------------------------===//\n \n+#include \"sanitizer_allocator.h\"\n #include \"sanitizer_allocator_interface.h\"\n #include \"sanitizer_common.h\"\n #include \"sanitizer_flags.h\"\n+#include \"sanitizer_interface_internal.h\"\n #include \"sanitizer_procmaps.h\"\n-\n+#include \"sanitizer_stackdepot.h\"\n \n namespace __sanitizer {\n \n-static void (*SoftRssLimitExceededCallback)(bool exceeded);\n-void SetSoftRssLimitExceededCallback(void (*Callback)(bool exceeded)) {\n-  CHECK_EQ(SoftRssLimitExceededCallback, nullptr);\n-  SoftRssLimitExceededCallback = Callback;\n-}\n-\n #if (SANITIZER_LINUX || SANITIZER_NETBSD) && !SANITIZER_GO\n // Weak default implementation for when sanitizer_stackdepot is not linked in.\n SANITIZER_WEAK_ATTRIBUTE StackDepotStats StackDepotGetStats() { return {}; }\n \n void *BackgroundThread(void *arg) {\n+  VPrintf(1, \"%s: Started BackgroundThread\\n\", SanitizerToolName);\n   const uptr hard_rss_limit_mb = common_flags()->hard_rss_limit_mb;\n   const uptr soft_rss_limit_mb = common_flags()->soft_rss_limit_mb;\n   const bool heap_profile = common_flags()->heap_profile;\n@@ -66,13 +63,11 @@ void *BackgroundThread(void *arg) {\n         reached_soft_rss_limit = true;\n         Report(\"%s: soft rss limit exhausted (%zdMb vs %zdMb)\\n\",\n                SanitizerToolName, soft_rss_limit_mb, current_rss_mb);\n-        if (SoftRssLimitExceededCallback)\n-          SoftRssLimitExceededCallback(true);\n+        SetRssLimitExceeded(true);\n       } else if (soft_rss_limit_mb >= current_rss_mb &&\n                  reached_soft_rss_limit) {\n         reached_soft_rss_limit = false;\n-        if (SoftRssLimitExceededCallback)\n-          SoftRssLimitExceededCallback(false);\n+        SetRssLimitExceeded(false);\n       }\n     }\n     if (heap_profile &&\n@@ -83,6 +78,42 @@ void *BackgroundThread(void *arg) {\n     }\n   }\n }\n+\n+void MaybeStartBackgroudThread() {\n+  // Need to implement/test on other platforms.\n+  // Start the background thread if one of the rss limits is given.\n+  if (!common_flags()->hard_rss_limit_mb &&\n+      !common_flags()->soft_rss_limit_mb &&\n+      !common_flags()->heap_profile) return;\n+  if (!&real_pthread_create) {\n+    VPrintf(1, \"%s: real_pthread_create undefined\\n\", SanitizerToolName);\n+    return;  // Can't spawn the thread anyway.\n+  }\n+\n+  static bool started = false;\n+  if (!started) {\n+    started = true;\n+    internal_start_thread(BackgroundThread, nullptr);\n+  }\n+}\n+\n+#  if !SANITIZER_START_BACKGROUND_THREAD_IN_ASAN_INTERNAL\n+#    ifdef __clang__\n+#    pragma clang diagnostic push\n+// We avoid global-constructors to be sure that globals are ready when\n+// sanitizers need them. This can happend before global constructors executed.\n+// Here we don't mind if thread is started on later stages.\n+#    pragma clang diagnostic ignored \"-Wglobal-constructors\"\n+#    endif\n+static struct BackgroudThreadStarted {\n+  BackgroudThreadStarted() { MaybeStartBackgroudThread(); }\n+} background_thread_strarter UNUSED;\n+#    ifdef __clang__\n+#    pragma clang diagnostic pop\n+#    endif\n+#  endif\n+#else\n+void MaybeStartBackgroudThread() {}\n #endif\n \n void WriteToSyslog(const char *msg) {\n@@ -105,18 +136,6 @@ void WriteToSyslog(const char *msg) {\n     WriteOneLineToSyslog(p);\n }\n \n-void MaybeStartBackgroudThread() {\n-#if (SANITIZER_LINUX || SANITIZER_NETBSD) && \\\n-    !SANITIZER_GO  // Need to implement/test on other platforms.\n-  // Start the background thread if one of the rss limits is given.\n-  if (!common_flags()->hard_rss_limit_mb &&\n-      !common_flags()->soft_rss_limit_mb &&\n-      !common_flags()->heap_profile) return;\n-  if (!&real_pthread_create) return;  // Can't spawn the thread anyway.\n-  internal_start_thread(BackgroundThread, nullptr);\n-#endif\n-}\n-\n static void (*sandboxing_callback)();\n void SetSandboxingCallback(void (*f)()) {\n   sandboxing_callback = f;\n@@ -185,10 +204,22 @@ void ProtectGap(uptr addr, uptr size, uptr zero_base_shadow_start,\n \n #endif  // !SANITIZER_FUCHSIA\n \n+#if !SANITIZER_WINDOWS && !SANITIZER_GO\n+// Weak default implementation for when sanitizer_stackdepot is not linked in.\n+SANITIZER_WEAK_ATTRIBUTE void StackDepotStopBackgroundThread() {}\n+static void StopStackDepotBackgroundThread() {\n+  StackDepotStopBackgroundThread();\n+}\n+#else\n+// SANITIZER_WEAK_ATTRIBUTE is unsupported.\n+static void StopStackDepotBackgroundThread() {}\n+#endif\n+\n }  // namespace __sanitizer\n \n SANITIZER_INTERFACE_WEAK_DEF(void, __sanitizer_sandbox_on_notify,\n                              __sanitizer_sandbox_arguments *args) {\n+  __sanitizer::StopStackDepotBackgroundThread();\n   __sanitizer::PlatformPrepareForSandboxing(args);\n   if (__sanitizer::sandboxing_callback)\n     __sanitizer::sandboxing_callback();"}, {"sha": "35c325359148ab776b1d587cdb38ae52eada3f73", "filename": "libsanitizer/sanitizer_common/sanitizer_coverage_fuchsia.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_coverage_fuchsia.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_coverage_fuchsia.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_coverage_fuchsia.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -33,6 +33,7 @@\n \n #include \"sanitizer_atomic.h\"\n #include \"sanitizer_common.h\"\n+#include \"sanitizer_interface_internal.h\"\n #include \"sanitizer_internal_defs.h\"\n #include \"sanitizer_symbolizer_fuchsia.h\"\n "}, {"sha": "3dcb39f32f6c2f57a215dc41f35d5df1f27faddc", "filename": "libsanitizer/sanitizer_common/sanitizer_coverage_libcdep_new.cpp", "status": "modified", "additions": 11, "deletions": 9, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_coverage_libcdep_new.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_coverage_libcdep_new.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_coverage_libcdep_new.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -10,11 +10,13 @@\n #include \"sanitizer_platform.h\"\n \n #if !SANITIZER_FUCHSIA\n-#include \"sancov_flags.h\"\n-#include \"sanitizer_allocator_internal.h\"\n-#include \"sanitizer_atomic.h\"\n-#include \"sanitizer_common.h\"\n-#include \"sanitizer_file.h\"\n+#  include \"sancov_flags.h\"\n+#  include \"sanitizer_allocator_internal.h\"\n+#  include \"sanitizer_atomic.h\"\n+#  include \"sanitizer_common.h\"\n+#  include \"sanitizer_common/sanitizer_stacktrace.h\"\n+#  include \"sanitizer_file.h\"\n+#  include \"sanitizer_interface_internal.h\"\n \n using namespace __sanitizer;\n \n@@ -72,7 +74,7 @@ static void SanitizerDumpCoverage(const uptr* unsorted_pcs, uptr len) {\n     const uptr pc = pcs[i];\n     if (!pc) continue;\n \n-    if (!__sanitizer_get_module_and_offset_for_pc(pc, nullptr, 0, &pcs[i])) {\n+    if (!GetModuleAndOffsetForPc(pc, nullptr, 0, &pcs[i])) {\n       Printf(\"ERROR: unknown pc 0x%zx (may happen if dlclose is used)\\n\", pc);\n       continue;\n     }\n@@ -87,8 +89,7 @@ static void SanitizerDumpCoverage(const uptr* unsorted_pcs, uptr len) {\n       last_base = module_base;\n       module_start_idx = i;\n       module_found = true;\n-      __sanitizer_get_module_and_offset_for_pc(pc, module_name, kMaxPathLength,\n-                                               &pcs[i]);\n+      GetModuleAndOffsetForPc(pc, module_name, kMaxPathLength, &pcs[i]);\n     }\n   }\n \n@@ -222,7 +223,8 @@ SANITIZER_INTERFACE_ATTRIBUTE void __sanitizer_dump_coverage(const uptr* pcs,\n \n SANITIZER_INTERFACE_WEAK_DEF(void, __sanitizer_cov_trace_pc_guard, u32* guard) {\n   if (!*guard) return;\n-  __sancov::pc_guard_controller.TracePcGuard(guard, GET_CALLER_PC() - 1);\n+  __sancov::pc_guard_controller.TracePcGuard(\n+      guard, StackTrace::GetPreviousInstructionPc(GET_CALLER_PC()));\n }\n \n SANITIZER_INTERFACE_WEAK_DEF(void, __sanitizer_cov_trace_pc_guard_init,"}, {"sha": "046d77dddc9c11720f11c422660cf2ffbb6a75b2", "filename": "libsanitizer/sanitizer_common/sanitizer_dense_map.h", "status": "added", "additions": 705, "deletions": 0, "changes": 705, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_dense_map.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_dense_map.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_dense_map.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -0,0 +1,705 @@\n+//===- sanitizer_dense_map.h - Dense probed hash table ----------*- C++ -*-===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+//\n+// This is fork of llvm/ADT/DenseMap.h class with the following changes:\n+//  * Use mmap to allocate.\n+//  * No iterators.\n+//  * Does not shrink.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef SANITIZER_DENSE_MAP_H\n+#define SANITIZER_DENSE_MAP_H\n+\n+#include \"sanitizer_common.h\"\n+#include \"sanitizer_dense_map_info.h\"\n+#include \"sanitizer_internal_defs.h\"\n+#include \"sanitizer_type_traits.h\"\n+\n+namespace __sanitizer {\n+\n+template <typename DerivedT, typename KeyT, typename ValueT, typename KeyInfoT,\n+          typename BucketT>\n+class DenseMapBase {\n+ public:\n+  using size_type = unsigned;\n+  using key_type = KeyT;\n+  using mapped_type = ValueT;\n+  using value_type = BucketT;\n+\n+  WARN_UNUSED_RESULT bool empty() const { return getNumEntries() == 0; }\n+  unsigned size() const { return getNumEntries(); }\n+\n+  /// Grow the densemap so that it can contain at least \\p NumEntries items\n+  /// before resizing again.\n+  void reserve(size_type NumEntries) {\n+    auto NumBuckets = getMinBucketToReserveForEntries(NumEntries);\n+    if (NumBuckets > getNumBuckets())\n+      grow(NumBuckets);\n+  }\n+\n+  void clear() {\n+    if (getNumEntries() == 0 && getNumTombstones() == 0)\n+      return;\n+\n+    const KeyT EmptyKey = getEmptyKey(), TombstoneKey = getTombstoneKey();\n+    if (__sanitizer::is_trivially_destructible<ValueT>::value) {\n+      // Use a simpler loop when values don't need destruction.\n+      for (BucketT *P = getBuckets(), *E = getBucketsEnd(); P != E; ++P)\n+        P->getFirst() = EmptyKey;\n+    } else {\n+      unsigned NumEntries = getNumEntries();\n+      for (BucketT *P = getBuckets(), *E = getBucketsEnd(); P != E; ++P) {\n+        if (!KeyInfoT::isEqual(P->getFirst(), EmptyKey)) {\n+          if (!KeyInfoT::isEqual(P->getFirst(), TombstoneKey)) {\n+            P->getSecond().~ValueT();\n+            --NumEntries;\n+          }\n+          P->getFirst() = EmptyKey;\n+        }\n+      }\n+      CHECK_EQ(NumEntries, 0);\n+    }\n+    setNumEntries(0);\n+    setNumTombstones(0);\n+  }\n+\n+  /// Return 1 if the specified key is in the map, 0 otherwise.\n+  size_type count(const KeyT &Key) const {\n+    const BucketT *TheBucket;\n+    return LookupBucketFor(Key, TheBucket) ? 1 : 0;\n+  }\n+\n+  value_type *find(const KeyT &Key) {\n+    BucketT *TheBucket;\n+    if (LookupBucketFor(Key, TheBucket))\n+      return TheBucket;\n+    return nullptr;\n+  }\n+  const value_type *find(const KeyT &Key) const {\n+    const BucketT *TheBucket;\n+    if (LookupBucketFor(Key, TheBucket))\n+      return TheBucket;\n+    return nullptr;\n+  }\n+\n+  /// Alternate version of find() which allows a different, and possibly\n+  /// less expensive, key type.\n+  /// The DenseMapInfo is responsible for supplying methods\n+  /// getHashValue(LookupKeyT) and isEqual(LookupKeyT, KeyT) for each key\n+  /// type used.\n+  template <class LookupKeyT>\n+  value_type *find_as(const LookupKeyT &Key) {\n+    BucketT *TheBucket;\n+    if (LookupBucketFor(Key, TheBucket))\n+      return TheBucket;\n+    return nullptr;\n+  }\n+  template <class LookupKeyT>\n+  const value_type *find_as(const LookupKeyT &Key) const {\n+    const BucketT *TheBucket;\n+    if (LookupBucketFor(Key, TheBucket))\n+      return TheBucket;\n+    return nullptr;\n+  }\n+\n+  /// lookup - Return the entry for the specified key, or a default\n+  /// constructed value if no such entry exists.\n+  ValueT lookup(const KeyT &Key) const {\n+    const BucketT *TheBucket;\n+    if (LookupBucketFor(Key, TheBucket))\n+      return TheBucket->getSecond();\n+    return ValueT();\n+  }\n+\n+  // Inserts key,value pair into the map if the key isn't already in the map.\n+  // If the key is already in the map, it returns false and doesn't update the\n+  // value.\n+  detail::DenseMapPair<value_type *, bool> insert(const value_type &KV) {\n+    return try_emplace(KV.first, KV.second);\n+  }\n+\n+  // Inserts key,value pair into the map if the key isn't already in the map.\n+  // If the key is already in the map, it returns false and doesn't update the\n+  // value.\n+  detail::DenseMapPair<value_type *, bool> insert(value_type &&KV) {\n+    return try_emplace(__sanitizer::move(KV.first),\n+                       __sanitizer::move(KV.second));\n+  }\n+\n+  // Inserts key,value pair into the map if the key isn't already in the map.\n+  // The value is constructed in-place if the key is not in the map, otherwise\n+  // it is not moved.\n+  template <typename... Ts>\n+  detail::DenseMapPair<value_type *, bool> try_emplace(KeyT &&Key,\n+                                                       Ts &&...Args) {\n+    BucketT *TheBucket;\n+    if (LookupBucketFor(Key, TheBucket))\n+      return {TheBucket, false};  // Already in map.\n+\n+    // Otherwise, insert the new element.\n+    TheBucket = InsertIntoBucket(TheBucket, __sanitizer::move(Key),\n+                                 __sanitizer::forward<Ts>(Args)...);\n+    return {TheBucket, true};\n+  }\n+\n+  // Inserts key,value pair into the map if the key isn't already in the map.\n+  // The value is constructed in-place if the key is not in the map, otherwise\n+  // it is not moved.\n+  template <typename... Ts>\n+  detail::DenseMapPair<value_type *, bool> try_emplace(const KeyT &Key,\n+                                                       Ts &&...Args) {\n+    BucketT *TheBucket;\n+    if (LookupBucketFor(Key, TheBucket))\n+      return {TheBucket, false};  // Already in map.\n+\n+    // Otherwise, insert the new element.\n+    TheBucket =\n+        InsertIntoBucket(TheBucket, Key, __sanitizer::forward<Ts>(Args)...);\n+    return {TheBucket, true};\n+  }\n+\n+  /// Alternate version of insert() which allows a different, and possibly\n+  /// less expensive, key type.\n+  /// The DenseMapInfo is responsible for supplying methods\n+  /// getHashValue(LookupKeyT) and isEqual(LookupKeyT, KeyT) for each key\n+  /// type used.\n+  template <typename LookupKeyT>\n+  detail::DenseMapPair<value_type *, bool> insert_as(value_type &&KV,\n+                                                     const LookupKeyT &Val) {\n+    BucketT *TheBucket;\n+    if (LookupBucketFor(Val, TheBucket))\n+      return {TheBucket, false};  // Already in map.\n+\n+    // Otherwise, insert the new element.\n+    TheBucket =\n+        InsertIntoBucketWithLookup(TheBucket, __sanitizer::move(KV.first),\n+                                   __sanitizer::move(KV.second), Val);\n+    return {TheBucket, true};\n+  }\n+\n+  bool erase(const KeyT &Val) {\n+    BucketT *TheBucket;\n+    if (!LookupBucketFor(Val, TheBucket))\n+      return false;  // not in map.\n+\n+    TheBucket->getSecond().~ValueT();\n+    TheBucket->getFirst() = getTombstoneKey();\n+    decrementNumEntries();\n+    incrementNumTombstones();\n+    return true;\n+  }\n+\n+  void erase(value_type *I) {\n+    CHECK_NE(I, nullptr);\n+    BucketT *TheBucket = &*I;\n+    TheBucket->getSecond().~ValueT();\n+    TheBucket->getFirst() = getTombstoneKey();\n+    decrementNumEntries();\n+    incrementNumTombstones();\n+  }\n+\n+  value_type &FindAndConstruct(const KeyT &Key) {\n+    BucketT *TheBucket;\n+    if (LookupBucketFor(Key, TheBucket))\n+      return *TheBucket;\n+\n+    return *InsertIntoBucket(TheBucket, Key);\n+  }\n+\n+  ValueT &operator[](const KeyT &Key) { return FindAndConstruct(Key).second; }\n+\n+  value_type &FindAndConstruct(KeyT &&Key) {\n+    BucketT *TheBucket;\n+    if (LookupBucketFor(Key, TheBucket))\n+      return *TheBucket;\n+\n+    return *InsertIntoBucket(TheBucket, __sanitizer::move(Key));\n+  }\n+\n+  ValueT &operator[](KeyT &&Key) {\n+    return FindAndConstruct(__sanitizer::move(Key)).second;\n+  }\n+\n+  /// Iterate over active entries of the container.\n+  ///\n+  /// Function can return fast to stop the process.\n+  template <class Fn>\n+  void forEach(Fn fn) {\n+    const KeyT EmptyKey = getEmptyKey(), TombstoneKey = getTombstoneKey();\n+    for (auto *P = getBuckets(), *E = getBucketsEnd(); P != E; ++P) {\n+      const KeyT K = P->getFirst();\n+      if (!KeyInfoT::isEqual(K, EmptyKey) &&\n+          !KeyInfoT::isEqual(K, TombstoneKey)) {\n+        if (!fn(*P))\n+          return;\n+      }\n+    }\n+  }\n+\n+  template <class Fn>\n+  void forEach(Fn fn) const {\n+    const_cast<DenseMapBase *>(this)->forEach(\n+        [&](const value_type &KV) { return fn(KV); });\n+  }\n+\n+ protected:\n+  DenseMapBase() = default;\n+\n+  void destroyAll() {\n+    if (getNumBuckets() == 0)  // Nothing to do.\n+      return;\n+\n+    const KeyT EmptyKey = getEmptyKey(), TombstoneKey = getTombstoneKey();\n+    for (BucketT *P = getBuckets(), *E = getBucketsEnd(); P != E; ++P) {\n+      if (!KeyInfoT::isEqual(P->getFirst(), EmptyKey) &&\n+          !KeyInfoT::isEqual(P->getFirst(), TombstoneKey))\n+        P->getSecond().~ValueT();\n+      P->getFirst().~KeyT();\n+    }\n+  }\n+\n+  void initEmpty() {\n+    setNumEntries(0);\n+    setNumTombstones(0);\n+\n+    CHECK_EQ((getNumBuckets() & (getNumBuckets() - 1)), 0);\n+    const KeyT EmptyKey = getEmptyKey();\n+    for (BucketT *B = getBuckets(), *E = getBucketsEnd(); B != E; ++B)\n+      ::new (&B->getFirst()) KeyT(EmptyKey);\n+  }\n+\n+  /// Returns the number of buckets to allocate to ensure that the DenseMap can\n+  /// accommodate \\p NumEntries without need to grow().\n+  unsigned getMinBucketToReserveForEntries(unsigned NumEntries) {\n+    // Ensure that \"NumEntries * 4 < NumBuckets * 3\"\n+    if (NumEntries == 0)\n+      return 0;\n+    // +1 is required because of the strict equality.\n+    // For example if NumEntries is 48, we need to return 401.\n+    return RoundUpToPowerOfTwo((NumEntries * 4 / 3 + 1) + /* NextPowerOf2 */ 1);\n+  }\n+\n+  void moveFromOldBuckets(BucketT *OldBucketsBegin, BucketT *OldBucketsEnd) {\n+    initEmpty();\n+\n+    // Insert all the old elements.\n+    const KeyT EmptyKey = getEmptyKey();\n+    const KeyT TombstoneKey = getTombstoneKey();\n+    for (BucketT *B = OldBucketsBegin, *E = OldBucketsEnd; B != E; ++B) {\n+      if (!KeyInfoT::isEqual(B->getFirst(), EmptyKey) &&\n+          !KeyInfoT::isEqual(B->getFirst(), TombstoneKey)) {\n+        // Insert the key/value into the new table.\n+        BucketT *DestBucket;\n+        bool FoundVal = LookupBucketFor(B->getFirst(), DestBucket);\n+        (void)FoundVal;  // silence warning.\n+        CHECK(!FoundVal);\n+        DestBucket->getFirst() = __sanitizer::move(B->getFirst());\n+        ::new (&DestBucket->getSecond())\n+            ValueT(__sanitizer::move(B->getSecond()));\n+        incrementNumEntries();\n+\n+        // Free the value.\n+        B->getSecond().~ValueT();\n+      }\n+      B->getFirst().~KeyT();\n+    }\n+  }\n+\n+  template <typename OtherBaseT>\n+  void copyFrom(\n+      const DenseMapBase<OtherBaseT, KeyT, ValueT, KeyInfoT, BucketT> &other) {\n+    CHECK_NE(&other, this);\n+    CHECK_EQ(getNumBuckets(), other.getNumBuckets());\n+\n+    setNumEntries(other.getNumEntries());\n+    setNumTombstones(other.getNumTombstones());\n+\n+    if (__sanitizer::is_trivially_copyable<KeyT>::value &&\n+        __sanitizer::is_trivially_copyable<ValueT>::value)\n+      internal_memcpy(reinterpret_cast<void *>(getBuckets()),\n+                      other.getBuckets(), getNumBuckets() * sizeof(BucketT));\n+    else\n+      for (uptr i = 0; i < getNumBuckets(); ++i) {\n+        ::new (&getBuckets()[i].getFirst())\n+            KeyT(other.getBuckets()[i].getFirst());\n+        if (!KeyInfoT::isEqual(getBuckets()[i].getFirst(), getEmptyKey()) &&\n+            !KeyInfoT::isEqual(getBuckets()[i].getFirst(), getTombstoneKey()))\n+          ::new (&getBuckets()[i].getSecond())\n+              ValueT(other.getBuckets()[i].getSecond());\n+      }\n+  }\n+\n+  static unsigned getHashValue(const KeyT &Val) {\n+    return KeyInfoT::getHashValue(Val);\n+  }\n+\n+  template <typename LookupKeyT>\n+  static unsigned getHashValue(const LookupKeyT &Val) {\n+    return KeyInfoT::getHashValue(Val);\n+  }\n+\n+  static const KeyT getEmptyKey() { return KeyInfoT::getEmptyKey(); }\n+\n+  static const KeyT getTombstoneKey() { return KeyInfoT::getTombstoneKey(); }\n+\n+ private:\n+  unsigned getNumEntries() const {\n+    return static_cast<const DerivedT *>(this)->getNumEntries();\n+  }\n+\n+  void setNumEntries(unsigned Num) {\n+    static_cast<DerivedT *>(this)->setNumEntries(Num);\n+  }\n+\n+  void incrementNumEntries() { setNumEntries(getNumEntries() + 1); }\n+\n+  void decrementNumEntries() { setNumEntries(getNumEntries() - 1); }\n+\n+  unsigned getNumTombstones() const {\n+    return static_cast<const DerivedT *>(this)->getNumTombstones();\n+  }\n+\n+  void setNumTombstones(unsigned Num) {\n+    static_cast<DerivedT *>(this)->setNumTombstones(Num);\n+  }\n+\n+  void incrementNumTombstones() { setNumTombstones(getNumTombstones() + 1); }\n+\n+  void decrementNumTombstones() { setNumTombstones(getNumTombstones() - 1); }\n+\n+  const BucketT *getBuckets() const {\n+    return static_cast<const DerivedT *>(this)->getBuckets();\n+  }\n+\n+  BucketT *getBuckets() { return static_cast<DerivedT *>(this)->getBuckets(); }\n+\n+  unsigned getNumBuckets() const {\n+    return static_cast<const DerivedT *>(this)->getNumBuckets();\n+  }\n+\n+  BucketT *getBucketsEnd() { return getBuckets() + getNumBuckets(); }\n+\n+  const BucketT *getBucketsEnd() const {\n+    return getBuckets() + getNumBuckets();\n+  }\n+\n+  void grow(unsigned AtLeast) { static_cast<DerivedT *>(this)->grow(AtLeast); }\n+\n+  template <typename KeyArg, typename... ValueArgs>\n+  BucketT *InsertIntoBucket(BucketT *TheBucket, KeyArg &&Key,\n+                            ValueArgs &&...Values) {\n+    TheBucket = InsertIntoBucketImpl(Key, Key, TheBucket);\n+\n+    TheBucket->getFirst() = __sanitizer::forward<KeyArg>(Key);\n+    ::new (&TheBucket->getSecond())\n+        ValueT(__sanitizer::forward<ValueArgs>(Values)...);\n+    return TheBucket;\n+  }\n+\n+  template <typename LookupKeyT>\n+  BucketT *InsertIntoBucketWithLookup(BucketT *TheBucket, KeyT &&Key,\n+                                      ValueT &&Value, LookupKeyT &Lookup) {\n+    TheBucket = InsertIntoBucketImpl(Key, Lookup, TheBucket);\n+\n+    TheBucket->getFirst() = __sanitizer::move(Key);\n+    ::new (&TheBucket->getSecond()) ValueT(__sanitizer::move(Value));\n+    return TheBucket;\n+  }\n+\n+  template <typename LookupKeyT>\n+  BucketT *InsertIntoBucketImpl(const KeyT &Key, const LookupKeyT &Lookup,\n+                                BucketT *TheBucket) {\n+    // If the load of the hash table is more than 3/4, or if fewer than 1/8 of\n+    // the buckets are empty (meaning that many are filled with tombstones),\n+    // grow the table.\n+    //\n+    // The later case is tricky.  For example, if we had one empty bucket with\n+    // tons of tombstones, failing lookups (e.g. for insertion) would have to\n+    // probe almost the entire table until it found the empty bucket.  If the\n+    // table completely filled with tombstones, no lookup would ever succeed,\n+    // causing infinite loops in lookup.\n+    unsigned NewNumEntries = getNumEntries() + 1;\n+    unsigned NumBuckets = getNumBuckets();\n+    if (UNLIKELY(NewNumEntries * 4 >= NumBuckets * 3)) {\n+      this->grow(NumBuckets * 2);\n+      LookupBucketFor(Lookup, TheBucket);\n+      NumBuckets = getNumBuckets();\n+    } else if (UNLIKELY(NumBuckets - (NewNumEntries + getNumTombstones()) <=\n+                        NumBuckets / 8)) {\n+      this->grow(NumBuckets);\n+      LookupBucketFor(Lookup, TheBucket);\n+    }\n+    CHECK(TheBucket);\n+\n+    // Only update the state after we've grown our bucket space appropriately\n+    // so that when growing buckets we have self-consistent entry count.\n+    incrementNumEntries();\n+\n+    // If we are writing over a tombstone, remember this.\n+    const KeyT EmptyKey = getEmptyKey();\n+    if (!KeyInfoT::isEqual(TheBucket->getFirst(), EmptyKey))\n+      decrementNumTombstones();\n+\n+    return TheBucket;\n+  }\n+\n+  /// LookupBucketFor - Lookup the appropriate bucket for Val, returning it in\n+  /// FoundBucket.  If the bucket contains the key and a value, this returns\n+  /// true, otherwise it returns a bucket with an empty marker or tombstone and\n+  /// returns false.\n+  template <typename LookupKeyT>\n+  bool LookupBucketFor(const LookupKeyT &Val,\n+                       const BucketT *&FoundBucket) const {\n+    const BucketT *BucketsPtr = getBuckets();\n+    const unsigned NumBuckets = getNumBuckets();\n+\n+    if (NumBuckets == 0) {\n+      FoundBucket = nullptr;\n+      return false;\n+    }\n+\n+    // FoundTombstone - Keep track of whether we find a tombstone while probing.\n+    const BucketT *FoundTombstone = nullptr;\n+    const KeyT EmptyKey = getEmptyKey();\n+    const KeyT TombstoneKey = getTombstoneKey();\n+    CHECK(!KeyInfoT::isEqual(Val, EmptyKey));\n+    CHECK(!KeyInfoT::isEqual(Val, TombstoneKey));\n+\n+    unsigned BucketNo = getHashValue(Val) & (NumBuckets - 1);\n+    unsigned ProbeAmt = 1;\n+    while (true) {\n+      const BucketT *ThisBucket = BucketsPtr + BucketNo;\n+      // Found Val's bucket?  If so, return it.\n+      if (LIKELY(KeyInfoT::isEqual(Val, ThisBucket->getFirst()))) {\n+        FoundBucket = ThisBucket;\n+        return true;\n+      }\n+\n+      // If we found an empty bucket, the key doesn't exist in the set.\n+      // Insert it and return the default value.\n+      if (LIKELY(KeyInfoT::isEqual(ThisBucket->getFirst(), EmptyKey))) {\n+        // If we've already seen a tombstone while probing, fill it in instead\n+        // of the empty bucket we eventually probed to.\n+        FoundBucket = FoundTombstone ? FoundTombstone : ThisBucket;\n+        return false;\n+      }\n+\n+      // If this is a tombstone, remember it.  If Val ends up not in the map, we\n+      // prefer to return it than something that would require more probing.\n+      if (KeyInfoT::isEqual(ThisBucket->getFirst(), TombstoneKey) &&\n+          !FoundTombstone)\n+        FoundTombstone = ThisBucket;  // Remember the first tombstone found.\n+\n+      // Otherwise, it's a hash collision or a tombstone, continue quadratic\n+      // probing.\n+      BucketNo += ProbeAmt++;\n+      BucketNo &= (NumBuckets - 1);\n+    }\n+  }\n+\n+  template <typename LookupKeyT>\n+  bool LookupBucketFor(const LookupKeyT &Val, BucketT *&FoundBucket) {\n+    const BucketT *ConstFoundBucket;\n+    bool Result = const_cast<const DenseMapBase *>(this)->LookupBucketFor(\n+        Val, ConstFoundBucket);\n+    FoundBucket = const_cast<BucketT *>(ConstFoundBucket);\n+    return Result;\n+  }\n+\n+ public:\n+  /// Return the approximate size (in bytes) of the actual map.\n+  /// This is just the raw memory used by DenseMap.\n+  /// If entries are pointers to objects, the size of the referenced objects\n+  /// are not included.\n+  uptr getMemorySize() const {\n+    return RoundUpTo(getNumBuckets() * sizeof(BucketT), GetPageSizeCached());\n+  }\n+};\n+\n+/// Equality comparison for DenseMap.\n+///\n+/// Iterates over elements of LHS confirming that each (key, value) pair in LHS\n+/// is also in RHS, and that no additional pairs are in RHS.\n+/// Equivalent to N calls to RHS.find and N value comparisons. Amortized\n+/// complexity is linear, worst case is O(N^2) (if every hash collides).\n+template <typename DerivedT, typename KeyT, typename ValueT, typename KeyInfoT,\n+          typename BucketT>\n+bool operator==(\n+    const DenseMapBase<DerivedT, KeyT, ValueT, KeyInfoT, BucketT> &LHS,\n+    const DenseMapBase<DerivedT, KeyT, ValueT, KeyInfoT, BucketT> &RHS) {\n+  if (LHS.size() != RHS.size())\n+    return false;\n+\n+  bool R = true;\n+  LHS.forEach(\n+      [&](const typename DenseMapBase<DerivedT, KeyT, ValueT, KeyInfoT,\n+                                      BucketT>::value_type &KV) -> bool {\n+        const auto *I = RHS.find(KV.first);\n+        if (!I || I->second != KV.second) {\n+          R = false;\n+          return false;\n+        }\n+        return true;\n+      });\n+\n+  return R;\n+}\n+\n+/// Inequality comparison for DenseMap.\n+///\n+/// Equivalent to !(LHS == RHS). See operator== for performance notes.\n+template <typename DerivedT, typename KeyT, typename ValueT, typename KeyInfoT,\n+          typename BucketT>\n+bool operator!=(\n+    const DenseMapBase<DerivedT, KeyT, ValueT, KeyInfoT, BucketT> &LHS,\n+    const DenseMapBase<DerivedT, KeyT, ValueT, KeyInfoT, BucketT> &RHS) {\n+  return !(LHS == RHS);\n+}\n+\n+template <typename KeyT, typename ValueT,\n+          typename KeyInfoT = DenseMapInfo<KeyT>,\n+          typename BucketT = detail::DenseMapPair<KeyT, ValueT>>\n+class DenseMap : public DenseMapBase<DenseMap<KeyT, ValueT, KeyInfoT, BucketT>,\n+                                     KeyT, ValueT, KeyInfoT, BucketT> {\n+  friend class DenseMapBase<DenseMap, KeyT, ValueT, KeyInfoT, BucketT>;\n+\n+  // Lift some types from the dependent base class into this class for\n+  // simplicity of referring to them.\n+  using BaseT = DenseMapBase<DenseMap, KeyT, ValueT, KeyInfoT, BucketT>;\n+\n+  BucketT *Buckets = nullptr;\n+  unsigned NumEntries = 0;\n+  unsigned NumTombstones = 0;\n+  unsigned NumBuckets = 0;\n+\n+ public:\n+  /// Create a DenseMap with an optional \\p InitialReserve that guarantee that\n+  /// this number of elements can be inserted in the map without grow()\n+  explicit DenseMap(unsigned InitialReserve) { init(InitialReserve); }\n+  constexpr DenseMap() = default;\n+\n+  DenseMap(const DenseMap &other) : BaseT() {\n+    init(0);\n+    copyFrom(other);\n+  }\n+\n+  DenseMap(DenseMap &&other) : BaseT() {\n+    init(0);\n+    swap(other);\n+  }\n+\n+  ~DenseMap() {\n+    this->destroyAll();\n+    deallocate_buffer(Buckets, sizeof(BucketT) * NumBuckets);\n+  }\n+\n+  void swap(DenseMap &RHS) {\n+    Swap(Buckets, RHS.Buckets);\n+    Swap(NumEntries, RHS.NumEntries);\n+    Swap(NumTombstones, RHS.NumTombstones);\n+    Swap(NumBuckets, RHS.NumBuckets);\n+  }\n+\n+  DenseMap &operator=(const DenseMap &other) {\n+    if (&other != this)\n+      copyFrom(other);\n+    return *this;\n+  }\n+\n+  DenseMap &operator=(DenseMap &&other) {\n+    this->destroyAll();\n+    deallocate_buffer(Buckets, sizeof(BucketT) * NumBuckets, alignof(BucketT));\n+    init(0);\n+    swap(other);\n+    return *this;\n+  }\n+\n+  void copyFrom(const DenseMap &other) {\n+    this->destroyAll();\n+    deallocate_buffer(Buckets, sizeof(BucketT) * NumBuckets);\n+    if (allocateBuckets(other.NumBuckets)) {\n+      this->BaseT::copyFrom(other);\n+    } else {\n+      NumEntries = 0;\n+      NumTombstones = 0;\n+    }\n+  }\n+\n+  void init(unsigned InitNumEntries) {\n+    auto InitBuckets = BaseT::getMinBucketToReserveForEntries(InitNumEntries);\n+    if (allocateBuckets(InitBuckets)) {\n+      this->BaseT::initEmpty();\n+    } else {\n+      NumEntries = 0;\n+      NumTombstones = 0;\n+    }\n+  }\n+\n+  void grow(unsigned AtLeast) {\n+    unsigned OldNumBuckets = NumBuckets;\n+    BucketT *OldBuckets = Buckets;\n+\n+    allocateBuckets(RoundUpToPowerOfTwo(Max<unsigned>(64, AtLeast)));\n+    CHECK(Buckets);\n+    if (!OldBuckets) {\n+      this->BaseT::initEmpty();\n+      return;\n+    }\n+\n+    this->moveFromOldBuckets(OldBuckets, OldBuckets + OldNumBuckets);\n+\n+    // Free the old table.\n+    deallocate_buffer(OldBuckets, sizeof(BucketT) * OldNumBuckets);\n+  }\n+\n+ private:\n+  unsigned getNumEntries() const { return NumEntries; }\n+\n+  void setNumEntries(unsigned Num) { NumEntries = Num; }\n+\n+  unsigned getNumTombstones() const { return NumTombstones; }\n+\n+  void setNumTombstones(unsigned Num) { NumTombstones = Num; }\n+\n+  BucketT *getBuckets() const { return Buckets; }\n+\n+  unsigned getNumBuckets() const { return NumBuckets; }\n+\n+  bool allocateBuckets(unsigned Num) {\n+    NumBuckets = Num;\n+    if (NumBuckets == 0) {\n+      Buckets = nullptr;\n+      return false;\n+    }\n+\n+    uptr Size = sizeof(BucketT) * NumBuckets;\n+    if (Size * 2 <= GetPageSizeCached()) {\n+      // We always allocate at least a page, so use entire space.\n+      unsigned Log2 = MostSignificantSetBitIndex(GetPageSizeCached() / Size);\n+      Size <<= Log2;\n+      NumBuckets <<= Log2;\n+      CHECK_EQ(Size, sizeof(BucketT) * NumBuckets);\n+      CHECK_GT(Size * 2, GetPageSizeCached());\n+    }\n+    Buckets = static_cast<BucketT *>(allocate_buffer(Size));\n+    return true;\n+  }\n+\n+  static void *allocate_buffer(uptr Size) {\n+    return MmapOrDie(RoundUpTo(Size, GetPageSizeCached()), \"DenseMap\");\n+  }\n+\n+  static void deallocate_buffer(void *Ptr, uptr Size) {\n+    UnmapOrDie(Ptr, RoundUpTo(Size, GetPageSizeCached()));\n+  }\n+};\n+\n+}  // namespace __sanitizer\n+\n+#endif  // SANITIZER_DENSE_MAP_H"}, {"sha": "f4640369ae588a7f79593eb558c1a7779a42e888", "filename": "libsanitizer/sanitizer_common/sanitizer_dense_map_info.h", "status": "added", "additions": 282, "deletions": 0, "changes": 282, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_dense_map_info.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_dense_map_info.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_dense_map_info.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -0,0 +1,282 @@\n+//===- sanitizer_dense_map_info.h - Type traits for DenseMap ----*- C++ -*-===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef SANITIZER_DENSE_MAP_INFO_H\n+#define SANITIZER_DENSE_MAP_INFO_H\n+\n+#include \"sanitizer_common.h\"\n+#include \"sanitizer_internal_defs.h\"\n+#include \"sanitizer_type_traits.h\"\n+\n+namespace __sanitizer {\n+\n+namespace detail {\n+\n+/// Simplistic combination of 32-bit hash values into 32-bit hash values.\n+static constexpr unsigned combineHashValue(unsigned a, unsigned b) {\n+  u64 key = (u64)a << 32 | (u64)b;\n+  key += ~(key << 32);\n+  key ^= (key >> 22);\n+  key += ~(key << 13);\n+  key ^= (key >> 8);\n+  key += (key << 3);\n+  key ^= (key >> 15);\n+  key += ~(key << 27);\n+  key ^= (key >> 31);\n+  return (unsigned)key;\n+}\n+\n+// We extend a pair to allow users to override the bucket type with their own\n+// implementation without requiring two members.\n+template <typename KeyT, typename ValueT>\n+struct DenseMapPair {\n+  KeyT first = {};\n+  ValueT second = {};\n+  constexpr DenseMapPair() = default;\n+  constexpr DenseMapPair(const KeyT &f, const ValueT &s)\n+      : first(f), second(s) {}\n+\n+  template <typename KeyT2, typename ValueT2>\n+  constexpr DenseMapPair(KeyT2 &&f, ValueT2 &&s)\n+      : first(__sanitizer::forward<KeyT2>(f)),\n+        second(__sanitizer::forward<ValueT2>(s)) {}\n+\n+  constexpr DenseMapPair(const DenseMapPair &other) = default;\n+  constexpr DenseMapPair &operator=(const DenseMapPair &other) = default;\n+  constexpr DenseMapPair(DenseMapPair &&other) = default;\n+  constexpr DenseMapPair &operator=(DenseMapPair &&other) = default;\n+\n+  KeyT &getFirst() { return first; }\n+  const KeyT &getFirst() const { return first; }\n+  ValueT &getSecond() { return second; }\n+  const ValueT &getSecond() const { return second; }\n+};\n+\n+}  // end namespace detail\n+\n+template <typename T>\n+struct DenseMapInfo {\n+  // static T getEmptyKey();\n+  // static T getTombstoneKey();\n+  // static unsigned getHashValue(const T &Val);\n+  // static bool isEqual(const T &LHS, const T &RHS);\n+};\n+\n+// Provide DenseMapInfo for all pointers. Come up with sentinel pointer values\n+// that are aligned to alignof(T) bytes, but try to avoid requiring T to be\n+// complete. This allows clients to instantiate DenseMap<T*, ...> with forward\n+// declared key types. Assume that no pointer key type requires more than 4096\n+// bytes of alignment.\n+template <typename T>\n+struct DenseMapInfo<T *> {\n+  // The following should hold, but it would require T to be complete:\n+  // static_assert(alignof(T) <= (1 << Log2MaxAlign),\n+  //               \"DenseMap does not support pointer keys requiring more than \"\n+  //               \"Log2MaxAlign bits of alignment\");\n+  static constexpr uptr Log2MaxAlign = 12;\n+\n+  static constexpr T *getEmptyKey() {\n+    uptr Val = static_cast<uptr>(-1);\n+    Val <<= Log2MaxAlign;\n+    return reinterpret_cast<T *>(Val);\n+  }\n+\n+  static constexpr T *getTombstoneKey() {\n+    uptr Val = static_cast<uptr>(-2);\n+    Val <<= Log2MaxAlign;\n+    return reinterpret_cast<T *>(Val);\n+  }\n+\n+  static constexpr unsigned getHashValue(const T *PtrVal) {\n+    return (unsigned((uptr)PtrVal) >> 4) ^ (unsigned((uptr)PtrVal) >> 9);\n+  }\n+\n+  static constexpr bool isEqual(const T *LHS, const T *RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for chars.\n+template <>\n+struct DenseMapInfo<char> {\n+  static constexpr char getEmptyKey() { return ~0; }\n+  static constexpr char getTombstoneKey() { return ~0 - 1; }\n+  static constexpr unsigned getHashValue(const char &Val) { return Val * 37U; }\n+\n+  static constexpr bool isEqual(const char &LHS, const char &RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for unsigned chars.\n+template <>\n+struct DenseMapInfo<unsigned char> {\n+  static constexpr unsigned char getEmptyKey() { return ~0; }\n+  static constexpr unsigned char getTombstoneKey() { return ~0 - 1; }\n+  static constexpr unsigned getHashValue(const unsigned char &Val) {\n+    return Val * 37U;\n+  }\n+\n+  static constexpr bool isEqual(const unsigned char &LHS,\n+                                const unsigned char &RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for unsigned shorts.\n+template <>\n+struct DenseMapInfo<unsigned short> {\n+  static constexpr unsigned short getEmptyKey() { return 0xFFFF; }\n+  static constexpr unsigned short getTombstoneKey() { return 0xFFFF - 1; }\n+  static constexpr unsigned getHashValue(const unsigned short &Val) {\n+    return Val * 37U;\n+  }\n+\n+  static constexpr bool isEqual(const unsigned short &LHS,\n+                                const unsigned short &RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for unsigned ints.\n+template <>\n+struct DenseMapInfo<unsigned> {\n+  static constexpr unsigned getEmptyKey() { return ~0U; }\n+  static constexpr unsigned getTombstoneKey() { return ~0U - 1; }\n+  static constexpr unsigned getHashValue(const unsigned &Val) {\n+    return Val * 37U;\n+  }\n+\n+  static constexpr bool isEqual(const unsigned &LHS, const unsigned &RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for unsigned longs.\n+template <>\n+struct DenseMapInfo<unsigned long> {\n+  static constexpr unsigned long getEmptyKey() { return ~0UL; }\n+  static constexpr unsigned long getTombstoneKey() { return ~0UL - 1L; }\n+\n+  static constexpr unsigned getHashValue(const unsigned long &Val) {\n+    return (unsigned)(Val * 37UL);\n+  }\n+\n+  static constexpr bool isEqual(const unsigned long &LHS,\n+                                const unsigned long &RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for unsigned long longs.\n+template <>\n+struct DenseMapInfo<unsigned long long> {\n+  static constexpr unsigned long long getEmptyKey() { return ~0ULL; }\n+  static constexpr unsigned long long getTombstoneKey() { return ~0ULL - 1ULL; }\n+\n+  static constexpr unsigned getHashValue(const unsigned long long &Val) {\n+    return (unsigned)(Val * 37ULL);\n+  }\n+\n+  static constexpr bool isEqual(const unsigned long long &LHS,\n+                                const unsigned long long &RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for shorts.\n+template <>\n+struct DenseMapInfo<short> {\n+  static constexpr short getEmptyKey() { return 0x7FFF; }\n+  static constexpr short getTombstoneKey() { return -0x7FFF - 1; }\n+  static constexpr unsigned getHashValue(const short &Val) { return Val * 37U; }\n+  static constexpr bool isEqual(const short &LHS, const short &RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for ints.\n+template <>\n+struct DenseMapInfo<int> {\n+  static constexpr int getEmptyKey() { return 0x7fffffff; }\n+  static constexpr int getTombstoneKey() { return -0x7fffffff - 1; }\n+  static constexpr unsigned getHashValue(const int &Val) {\n+    return (unsigned)(Val * 37U);\n+  }\n+\n+  static constexpr bool isEqual(const int &LHS, const int &RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for longs.\n+template <>\n+struct DenseMapInfo<long> {\n+  static constexpr long getEmptyKey() {\n+    return (1UL << (sizeof(long) * 8 - 1)) - 1UL;\n+  }\n+\n+  static constexpr long getTombstoneKey() { return getEmptyKey() - 1L; }\n+\n+  static constexpr unsigned getHashValue(const long &Val) {\n+    return (unsigned)(Val * 37UL);\n+  }\n+\n+  static constexpr bool isEqual(const long &LHS, const long &RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for long longs.\n+template <>\n+struct DenseMapInfo<long long> {\n+  static constexpr long long getEmptyKey() { return 0x7fffffffffffffffLL; }\n+  static constexpr long long getTombstoneKey() {\n+    return -0x7fffffffffffffffLL - 1;\n+  }\n+\n+  static constexpr unsigned getHashValue(const long long &Val) {\n+    return (unsigned)(Val * 37ULL);\n+  }\n+\n+  static constexpr bool isEqual(const long long &LHS, const long long &RHS) {\n+    return LHS == RHS;\n+  }\n+};\n+\n+// Provide DenseMapInfo for all pairs whose members have info.\n+template <typename T, typename U>\n+struct DenseMapInfo<detail::DenseMapPair<T, U>> {\n+  using Pair = detail::DenseMapPair<T, U>;\n+  using FirstInfo = DenseMapInfo<T>;\n+  using SecondInfo = DenseMapInfo<U>;\n+\n+  static constexpr Pair getEmptyKey() {\n+    return detail::DenseMapPair<T, U>(FirstInfo::getEmptyKey(),\n+                                      SecondInfo::getEmptyKey());\n+  }\n+\n+  static constexpr Pair getTombstoneKey() {\n+    return detail::DenseMapPair<T, U>(FirstInfo::getTombstoneKey(),\n+                                      SecondInfo::getTombstoneKey());\n+  }\n+\n+  static constexpr unsigned getHashValue(const Pair &PairVal) {\n+    return detail::combineHashValue(FirstInfo::getHashValue(PairVal.first),\n+                                    SecondInfo::getHashValue(PairVal.second));\n+  }\n+\n+  static constexpr bool isEqual(const Pair &LHS, const Pair &RHS) {\n+    return FirstInfo::isEqual(LHS.first, RHS.first) &&\n+           SecondInfo::isEqual(LHS.second, RHS.second);\n+  }\n+};\n+\n+}  // namespace __sanitizer\n+\n+#endif  // SANITIZER_DENSE_MAP_INFO_H"}, {"sha": "7ef499ce07b13331e8cb0142df288d4e0f37bc1a", "filename": "libsanitizer/sanitizer_common/sanitizer_file.cpp", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_file.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_file.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_file.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -19,6 +19,7 @@\n \n #include \"sanitizer_common.h\"\n #include \"sanitizer_file.h\"\n+#  include \"sanitizer_interface_internal.h\"\n \n namespace __sanitizer {\n \n@@ -83,8 +84,12 @@ static void RecursiveCreateParentDirs(char *path) {\n     if (!IsPathSeparator(path[i]))\n       continue;\n     path[i] = '\\0';\n-    /* Some of these will fail, because the directory exists, ignore it. */\n-    CreateDir(path);\n+    if (!DirExists(path) && !CreateDir(path)) {\n+      const char *ErrorMsgPrefix = \"ERROR: Can't create directory: \";\n+      WriteToFile(kStderrFd, ErrorMsgPrefix, internal_strlen(ErrorMsgPrefix));\n+      WriteToFile(kStderrFd, path, internal_strlen(path));\n+      Die();\n+    }\n     path[i] = save;\n   }\n }"}, {"sha": "810c1e452f61199c764cac367c8fad71c20c19a5", "filename": "libsanitizer/sanitizer_common/sanitizer_file.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_file.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_file.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_file.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -15,7 +15,6 @@\n #ifndef SANITIZER_FILE_H\n #define SANITIZER_FILE_H\n \n-#include \"sanitizer_interface_internal.h\"\n #include \"sanitizer_internal_defs.h\"\n #include \"sanitizer_libc.h\"\n #include \"sanitizer_mutex.h\"\n@@ -78,6 +77,7 @@ bool SupportsColoredOutput(fd_t fd);\n // OS\n const char *GetPwd();\n bool FileExists(const char *filename);\n+bool DirExists(const char *path);\n char *FindPathToBinary(const char *name);\n bool IsPathSeparator(const char c);\n bool IsAbsolutePath(const char *path);"}, {"sha": "0ca91aff8dd4b4c2a1beb7b67bee5e2a9befa437", "filename": "libsanitizer/sanitizer_common/sanitizer_flags.inc", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_flags.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_flags.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_flags.inc?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -179,6 +179,7 @@ COMMON_FLAG(bool, use_madv_dontdump, true,\n           \"in core file.\")\n COMMON_FLAG(bool, symbolize_inline_frames, true,\n             \"Print inlined frames in stacktraces. Defaults to true.\")\n+COMMON_FLAG(bool, demangle, true, \"Print demangled symbols.\")\n COMMON_FLAG(bool, symbolize_vs_style, false,\n             \"Print file locations in Visual Studio style (e.g: \"\n             \" file(10,42): ...\")\n@@ -191,6 +192,8 @@ COMMON_FLAG(const char *, stack_trace_format, \"DEFAULT\",\n             \"Format string used to render stack frames. \"\n             \"See sanitizer_stacktrace_printer.h for the format description. \"\n             \"Use DEFAULT to get default format.\")\n+COMMON_FLAG(int, compress_stack_depot, 0,\n+            \"Compress stack depot to save memory.\")\n COMMON_FLAG(bool, no_huge_pages_for_shadow, true,\n             \"If true, the shadow is not allowed to use huge pages. \")\n COMMON_FLAG(bool, strict_string_checks, false,"}, {"sha": "848953a6ab007852a7729d5be679742828d92d4e", "filename": "libsanitizer/sanitizer_common/sanitizer_fuchsia.cpp", "status": "modified", "additions": 27, "deletions": 35, "changes": 62, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_fuchsia.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_fuchsia.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_fuchsia.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -14,17 +14,18 @@\n #include \"sanitizer_fuchsia.h\"\n #if SANITIZER_FUCHSIA\n \n-#include <pthread.h>\n-#include <stdlib.h>\n-#include <unistd.h>\n-#include <zircon/errors.h>\n-#include <zircon/process.h>\n-#include <zircon/syscalls.h>\n-#include <zircon/utc.h>\n-\n-#include \"sanitizer_common.h\"\n-#include \"sanitizer_libc.h\"\n-#include \"sanitizer_mutex.h\"\n+#  include <pthread.h>\n+#  include <stdlib.h>\n+#  include <unistd.h>\n+#  include <zircon/errors.h>\n+#  include <zircon/process.h>\n+#  include <zircon/syscalls.h>\n+#  include <zircon/utc.h>\n+\n+#  include \"sanitizer_common.h\"\n+#  include \"sanitizer_interface_internal.h\"\n+#  include \"sanitizer_libc.h\"\n+#  include \"sanitizer_mutex.h\"\n \n namespace __sanitizer {\n \n@@ -89,7 +90,7 @@ void InitializePlatformEarly() {}\n void MaybeReexec() {}\n void CheckASLR() {}\n void CheckMPROTECT() {}\n-void PlatformPrepareForSandboxing(__sanitizer_sandbox_arguments *args) {}\n+void PlatformPrepareForSandboxing(void *args) {}\n void DisableCoreDumperIfNecessary() {}\n void InstallDeadlySignalHandlers(SignalHandlerType handler) {}\n void SetAlternateSignalStack() {}\n@@ -274,6 +275,15 @@ void *MmapFixedNoAccess(uptr fixed_addr, uptr size, const char *name) {\n   UNIMPLEMENTED();\n }\n \n+bool MprotectNoAccess(uptr addr, uptr size) {\n+  return _zx_vmar_protect(_zx_vmar_root_self(), 0, addr, size) == ZX_OK;\n+}\n+\n+bool MprotectReadOnly(uptr addr, uptr size) {\n+  return _zx_vmar_protect(_zx_vmar_root_self(), ZX_VM_PERM_READ, addr, size) ==\n+         ZX_OK;\n+}\n+\n void *MmapAlignedOrDieOnFatalError(uptr size, uptr alignment,\n                                    const char *mem_type) {\n   CHECK_GE(size, GetPageSize());\n@@ -376,29 +386,8 @@ void GetMemoryProfile(fill_profile_f cb, uptr *stats) {}\n \n bool ReadFileToBuffer(const char *file_name, char **buff, uptr *buff_size,\n                       uptr *read_len, uptr max_len, error_t *errno_p) {\n-  zx_handle_t vmo;\n-  zx_status_t status = __sanitizer_get_configuration(file_name, &vmo);\n-  if (status == ZX_OK) {\n-    uint64_t vmo_size;\n-    status = _zx_vmo_get_size(vmo, &vmo_size);\n-    if (status == ZX_OK) {\n-      if (vmo_size < max_len)\n-        max_len = vmo_size;\n-      size_t map_size = RoundUpTo(max_len, GetPageSize());\n-      uintptr_t addr;\n-      status = _zx_vmar_map(_zx_vmar_root_self(), ZX_VM_PERM_READ, 0, vmo, 0,\n-                            map_size, &addr);\n-      if (status == ZX_OK) {\n-        *buff = reinterpret_cast<char *>(addr);\n-        *buff_size = map_size;\n-        *read_len = max_len;\n-      }\n-    }\n-    _zx_handle_close(vmo);\n-  }\n-  if (status != ZX_OK && errno_p)\n-    *errno_p = status;\n-  return status == ZX_OK;\n+  *errno_p = ZX_ERR_NOT_SUPPORTED;\n+  return false;\n }\n \n void RawWrite(const char *buffer) {\n@@ -475,6 +464,9 @@ u32 GetNumberOfCPUs() { return zx_system_get_num_cpus(); }\n \n uptr GetRSS() { UNIMPLEMENTED(); }\n \n+void *internal_start_thread(void *(*func)(void *arg), void *arg) { return 0; }\n+void internal_join_thread(void *th) {}\n+\n void InitializePlatformCommonFlags(CommonFlags *cf) {}\n \n }  // namespace __sanitizer"}, {"sha": "e9dc78c6354e51667e1f862eae9c191162d6b856", "filename": "libsanitizer/sanitizer_common/sanitizer_interface_internal.h", "status": "modified", "additions": 88, "deletions": 90, "changes": 178, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_interface_internal.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_interface_internal.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_interface_internal.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -20,104 +20,102 @@\n #include \"sanitizer_internal_defs.h\"\n \n extern \"C\" {\n-  // Tell the tools to write their reports to \"path.<pid>\" instead of stderr.\n-  // The special values are \"stdout\" and \"stderr\".\n-  SANITIZER_INTERFACE_ATTRIBUTE\n-  void __sanitizer_set_report_path(const char *path);\n-  // Tell the tools to write their reports to the provided file descriptor\n-  // (casted to void *).\n-  SANITIZER_INTERFACE_ATTRIBUTE\n-  void __sanitizer_set_report_fd(void *fd);\n-  // Get the current full report file path, if a path was specified by\n-  // an earlier call to __sanitizer_set_report_path. Returns null otherwise.\n-  SANITIZER_INTERFACE_ATTRIBUTE\n-  const char *__sanitizer_get_report_path();\n+// Tell the tools to write their reports to \"path.<pid>\" instead of stderr.\n+// The special values are \"stdout\" and \"stderr\".\n+SANITIZER_INTERFACE_ATTRIBUTE\n+void __sanitizer_set_report_path(const char *path);\n+// Tell the tools to write their reports to the provided file descriptor\n+// (casted to void *).\n+SANITIZER_INTERFACE_ATTRIBUTE\n+void __sanitizer_set_report_fd(void *fd);\n+// Get the current full report file path, if a path was specified by\n+// an earlier call to __sanitizer_set_report_path. Returns null otherwise.\n+SANITIZER_INTERFACE_ATTRIBUTE\n+const char *__sanitizer_get_report_path();\n \n-  typedef struct {\n-      int coverage_sandboxed;\n-      __sanitizer::sptr coverage_fd;\n-      unsigned int coverage_max_block_size;\n-  } __sanitizer_sandbox_arguments;\n+typedef struct {\n+  int coverage_sandboxed;\n+  __sanitizer::sptr coverage_fd;\n+  unsigned int coverage_max_block_size;\n+} __sanitizer_sandbox_arguments;\n \n-  // Notify the tools that the sandbox is going to be turned on.\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n-      __sanitizer_sandbox_on_notify(__sanitizer_sandbox_arguments *args);\n+// Notify the tools that the sandbox is going to be turned on.\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_sandbox_on_notify(__sanitizer_sandbox_arguments *args);\n \n-  // This function is called by the tool when it has just finished reporting\n-  // an error. 'error_summary' is a one-line string that summarizes\n-  // the error message. This function can be overridden by the client.\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_report_error_summary(const char *error_summary);\n+// This function is called by the tool when it has just finished reporting\n+// an error. 'error_summary' is a one-line string that summarizes\n+// the error message. This function can be overridden by the client.\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_report_error_summary(const char *error_summary);\n \n-  SANITIZER_INTERFACE_ATTRIBUTE void __sanitizer_cov_dump();\n-  SANITIZER_INTERFACE_ATTRIBUTE void __sanitizer_dump_coverage(\n-      const __sanitizer::uptr *pcs, const __sanitizer::uptr len);\n-  SANITIZER_INTERFACE_ATTRIBUTE void __sanitizer_dump_trace_pc_guard_coverage();\n+SANITIZER_INTERFACE_ATTRIBUTE void __sanitizer_cov_dump();\n+SANITIZER_INTERFACE_ATTRIBUTE void __sanitizer_dump_coverage(\n+    const __sanitizer::uptr *pcs, const __sanitizer::uptr len);\n+SANITIZER_INTERFACE_ATTRIBUTE void __sanitizer_dump_trace_pc_guard_coverage();\n \n-  SANITIZER_INTERFACE_ATTRIBUTE void __sanitizer_cov(__sanitizer::u32 *guard);\n+SANITIZER_INTERFACE_ATTRIBUTE void __sanitizer_cov(__sanitizer::u32 *guard);\n \n-  // Returns 1 on the first call, then returns 0 thereafter.  Called by the tool\n-  // to ensure only one report is printed when multiple errors occur\n-  // simultaneously.\n-  SANITIZER_INTERFACE_ATTRIBUTE int __sanitizer_acquire_crash_state();\n+// Returns 1 on the first call, then returns 0 thereafter.  Called by the tool\n+// to ensure only one report is printed when multiple errors occur\n+// simultaneously.\n+SANITIZER_INTERFACE_ATTRIBUTE int __sanitizer_acquire_crash_state();\n \n-  SANITIZER_INTERFACE_ATTRIBUTE\n-  void __sanitizer_annotate_contiguous_container(const void *beg,\n-                                                 const void *end,\n-                                                 const void *old_mid,\n-                                                 const void *new_mid);\n-  SANITIZER_INTERFACE_ATTRIBUTE\n-  int __sanitizer_verify_contiguous_container(const void *beg, const void *mid,\n-                                              const void *end);\n-  SANITIZER_INTERFACE_ATTRIBUTE\n-  const void *__sanitizer_contiguous_container_find_bad_address(\n-      const void *beg, const void *mid, const void *end);\n+SANITIZER_INTERFACE_ATTRIBUTE\n+void __sanitizer_annotate_contiguous_container(const void *beg, const void *end,\n+                                               const void *old_mid,\n+                                               const void *new_mid);\n+SANITIZER_INTERFACE_ATTRIBUTE\n+int __sanitizer_verify_contiguous_container(const void *beg, const void *mid,\n+                                            const void *end);\n+SANITIZER_INTERFACE_ATTRIBUTE\n+const void *__sanitizer_contiguous_container_find_bad_address(const void *beg,\n+                                                              const void *mid,\n+                                                              const void *end);\n \n-  SANITIZER_INTERFACE_ATTRIBUTE\n-  int __sanitizer_get_module_and_offset_for_pc(\n-      __sanitizer::uptr pc, char *module_path,\n-      __sanitizer::uptr module_path_len, __sanitizer::uptr *pc_offset);\n+SANITIZER_INTERFACE_ATTRIBUTE\n+int __sanitizer_get_module_and_offset_for_pc(void *pc, char *module_path,\n+                                             __sanitizer::uptr module_path_len,\n+                                             void **pc_offset);\n \n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_cmp();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_cmp1();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_cmp2();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_cmp4();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_cmp8();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_const_cmp1();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_const_cmp2();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_const_cmp4();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_const_cmp8();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_switch();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_div4();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_div8();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_gep();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_pc_indir();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_pc_guard(__sanitizer::u32*);\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-  void __sanitizer_cov_trace_pc_guard_init(__sanitizer::u32*,\n-                                           __sanitizer::u32*);\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n-  __sanitizer_cov_8bit_counters_init(char *, char *);\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n-  __sanitizer_cov_bool_flag_init();\n-  SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n-  __sanitizer_cov_pcs_init(const __sanitizer::uptr *,\n-                           const __sanitizer::uptr *);\n-} // extern \"C\"\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_cmp();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_cmp1();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_cmp2();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_cmp4();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_cmp8();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_const_cmp1();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_const_cmp2();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_const_cmp4();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_const_cmp8();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_switch();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_div4();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_div8();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_gep();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_pc_indir();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_pc_guard(__sanitizer::u32 *);\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_trace_pc_guard_init(__sanitizer::u32 *, __sanitizer::u32 *);\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_8bit_counters_init(char *, char *);\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_bool_flag_init();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_cov_pcs_init(const __sanitizer::uptr *, const __sanitizer::uptr *);\n+}  // extern \"C\"\n \n #endif  // SANITIZER_INTERFACE_INTERNAL_H"}, {"sha": "95a80b4629c7782f924b826154e062be27280960", "filename": "libsanitizer/sanitizer_common/sanitizer_internal_defs.h", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_internal_defs.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_internal_defs.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_internal_defs.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -179,6 +179,7 @@ typedef int pid_t;\n \n #if SANITIZER_FREEBSD || SANITIZER_NETBSD || SANITIZER_MAC ||             \\\n     (SANITIZER_SOLARIS && (defined(_LP64) || _FILE_OFFSET_BITS == 64)) || \\\n+    (SANITIZER_LINUX && !SANITIZER_GLIBC && !SANITIZER_ANDROID) ||        \\\n     (SANITIZER_LINUX && (defined(__x86_64__) || defined(__hexagon__)))\n typedef u64 OFF_T;\n #else\n@@ -258,7 +259,9 @@ typedef u64 tid_t;\n # define NOEXCEPT throw()\n #endif\n \n-#if __has_cpp_attribute(clang::fallthrough)\n+#if __has_cpp_attribute(fallthrough)\n+#  define FALLTHROUGH [[fallthrough]]\n+#elif __has_cpp_attribute(clang::fallthrough)\n #  define FALLTHROUGH [[clang::fallthrough]]\n #else\n #  define FALLTHROUGH\n@@ -300,7 +303,8 @@ void NORETURN CheckFailed(const char *file, int line, const char *cond,\n     }                                          \\\n   } while (0)\n \n-#define RAW_CHECK(expr, ...) RAW_CHECK_MSG(expr, #expr \"\\n\", __VA_ARGS__)\n+#define RAW_CHECK(expr) RAW_CHECK_MSG(expr, #expr \"\\n\", )\n+#define RAW_CHECK_VA(expr, ...) RAW_CHECK_MSG(expr, #expr \"\\n\", __VA_ARGS__)\n \n #define CHECK_IMPL(c1, op, c2) \\\n   do { \\"}, {"sha": "553550d295529e18a837b98678ffa243ffe5722a", "filename": "libsanitizer/sanitizer_common/sanitizer_leb128.h", "status": "added", "additions": 87, "deletions": 0, "changes": 87, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_leb128.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_leb128.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_leb128.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -0,0 +1,87 @@\n+//===-- sanitizer_leb128.h --------------------------------------*- C++ -*-===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef SANITIZER_LEB128_H\n+#define SANITIZER_LEB128_H\n+\n+#include \"sanitizer_common.h\"\n+#include \"sanitizer_internal_defs.h\"\n+\n+namespace __sanitizer {\n+\n+template <typename T, typename It>\n+It EncodeSLEB128(T value, It begin, It end) {\n+  bool more;\n+  do {\n+    u8 byte = value & 0x7f;\n+    // NOTE: this assumes that this signed shift is an arithmetic right shift.\n+    value >>= 7;\n+    more = !((((value == 0) && ((byte & 0x40) == 0)) ||\n+              ((value == -1) && ((byte & 0x40) != 0))));\n+    if (more)\n+      byte |= 0x80;\n+    if (UNLIKELY(begin == end))\n+      break;\n+    *(begin++) = byte;\n+  } while (more);\n+  return begin;\n+}\n+\n+template <typename T, typename It>\n+It DecodeSLEB128(It begin, It end, T* v) {\n+  T value = 0;\n+  unsigned shift = 0;\n+  u8 byte;\n+  do {\n+    if (UNLIKELY(begin == end))\n+      return begin;\n+    byte = *(begin++);\n+    T slice = byte & 0x7f;\n+    value |= slice << shift;\n+    shift += 7;\n+  } while (byte >= 128);\n+  if (shift < 64 && (byte & 0x40))\n+    value |= (-1ULL) << shift;\n+  *v = value;\n+  return begin;\n+}\n+\n+template <typename T, typename It>\n+It EncodeULEB128(T value, It begin, It end) {\n+  do {\n+    u8 byte = value & 0x7f;\n+    value >>= 7;\n+    if (value)\n+      byte |= 0x80;\n+    if (UNLIKELY(begin == end))\n+      break;\n+    *(begin++) = byte;\n+  } while (value);\n+  return begin;\n+}\n+\n+template <typename T, typename It>\n+It DecodeULEB128(It begin, It end, T* v) {\n+  T value = 0;\n+  unsigned shift = 0;\n+  u8 byte;\n+  do {\n+    if (UNLIKELY(begin == end))\n+      return begin;\n+    byte = *(begin++);\n+    T slice = byte & 0x7f;\n+    value += slice << shift;\n+    shift += 7;\n+  } while (byte >= 128);\n+  *v = value;\n+  return begin;\n+}\n+\n+}  // namespace __sanitizer\n+\n+#endif  // SANITIZER_LEB128_H"}, {"sha": "8e144a4e9a0e8a82531e6262fbdd8388beb5e77d", "filename": "libsanitizer/sanitizer_common/sanitizer_linux.cpp", "status": "modified", "additions": 110, "deletions": 105, "changes": 215, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -80,6 +80,7 @@\n \n #if SANITIZER_FREEBSD\n #include <sys/exec.h>\n+#include <sys/procctl.h>\n #include <sys/sysctl.h>\n #include <machine/atomic.h>\n extern \"C\" {\n@@ -162,6 +163,12 @@ ScopedBlockSignals::ScopedBlockSignals(__sanitizer_sigset_t *copy) {\n   // on any thread, setuid call hangs.\n   // See test/sanitizer_common/TestCases/Linux/setuid.c.\n   internal_sigdelset(&set, 33);\n+#  endif\n+#  if SANITIZER_LINUX\n+  // Seccomp-BPF-sandboxed processes rely on SIGSYS to handle trapped syscalls.\n+  // If this signal is blocked, such calls cannot be handled and the process may\n+  // hang.\n+  internal_sigdelset(&set, 31);\n #  endif\n   SetSigProcMask(&set, &saved_);\n   if (copy)\n@@ -226,15 +233,15 @@ uptr internal_close(fd_t fd) {\n }\n \n uptr internal_open(const char *filename, int flags) {\n-#if SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n+#    if SANITIZER_LINUX\n   return internal_syscall(SYSCALL(openat), AT_FDCWD, (uptr)filename, flags);\n #else\n   return internal_syscall(SYSCALL(open), (uptr)filename, flags);\n #endif\n }\n \n uptr internal_open(const char *filename, int flags, u32 mode) {\n-#if SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n+#    if SANITIZER_LINUX\n   return internal_syscall(SYSCALL(openat), AT_FDCWD, (uptr)filename, flags,\n                           mode);\n #else\n@@ -335,50 +342,46 @@ static void kernel_stat_to_stat(struct kernel_stat *in, struct stat *out) {\n uptr internal_stat(const char *path, void *buf) {\n #if SANITIZER_FREEBSD\n   return internal_syscall(SYSCALL(fstatat), AT_FDCWD, (uptr)path, (uptr)buf, 0);\n-#elif SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n+#    elif SANITIZER_LINUX\n+#      if SANITIZER_WORDSIZE == 64\n   return internal_syscall(SYSCALL(newfstatat), AT_FDCWD, (uptr)path, (uptr)buf,\n                           0);\n-#elif SANITIZER_LINUX_USES_64BIT_SYSCALLS\n-# if defined(__mips64)\n-  // For mips64, stat syscall fills buffer in the format of kernel_stat\n-  struct kernel_stat kbuf;\n-  int res = internal_syscall(SYSCALL(stat), path, &kbuf);\n-  kernel_stat_to_stat(&kbuf, (struct stat *)buf);\n+#      else\n+  struct stat64 buf64;\n+  int res = internal_syscall(SYSCALL(fstatat64), AT_FDCWD, (uptr)path,\n+                             (uptr)&buf64, 0);\n+  stat64_to_stat(&buf64, (struct stat *)buf);\n   return res;\n-# else\n-  return internal_syscall(SYSCALL(stat), (uptr)path, (uptr)buf);\n-# endif\n-#else\n+#      endif\n+#    else\n   struct stat64 buf64;\n   int res = internal_syscall(SYSCALL(stat64), path, &buf64);\n   stat64_to_stat(&buf64, (struct stat *)buf);\n   return res;\n-#endif\n+#    endif\n }\n \n uptr internal_lstat(const char *path, void *buf) {\n #if SANITIZER_FREEBSD\n   return internal_syscall(SYSCALL(fstatat), AT_FDCWD, (uptr)path, (uptr)buf,\n                           AT_SYMLINK_NOFOLLOW);\n-#elif SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n+#    elif SANITIZER_LINUX\n+#      if defined(_LP64)\n   return internal_syscall(SYSCALL(newfstatat), AT_FDCWD, (uptr)path, (uptr)buf,\n                           AT_SYMLINK_NOFOLLOW);\n-#elif SANITIZER_LINUX_USES_64BIT_SYSCALLS\n-# if SANITIZER_MIPS64\n-  // For mips64, lstat syscall fills buffer in the format of kernel_stat\n-  struct kernel_stat kbuf;\n-  int res = internal_syscall(SYSCALL(lstat), path, &kbuf);\n-  kernel_stat_to_stat(&kbuf, (struct stat *)buf);\n+#      else\n+  struct stat64 buf64;\n+  int res = internal_syscall(SYSCALL(fstatat64), AT_FDCWD, (uptr)path,\n+                             (uptr)&buf64, AT_SYMLINK_NOFOLLOW);\n+  stat64_to_stat(&buf64, (struct stat *)buf);\n   return res;\n-# else\n-  return internal_syscall(SYSCALL(lstat), (uptr)path, (uptr)buf);\n-# endif\n-#else\n+#      endif\n+#    else\n   struct stat64 buf64;\n   int res = internal_syscall(SYSCALL(lstat64), path, &buf64);\n   stat64_to_stat(&buf64, (struct stat *)buf);\n   return res;\n-#endif\n+#    endif\n }\n \n uptr internal_fstat(fd_t fd, void *buf) {\n@@ -412,15 +415,15 @@ uptr internal_dup(int oldfd) {\n }\n \n uptr internal_dup2(int oldfd, int newfd) {\n-#if SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n+#    if SANITIZER_LINUX\n   return internal_syscall(SYSCALL(dup3), oldfd, newfd, 0);\n #else\n   return internal_syscall(SYSCALL(dup2), oldfd, newfd);\n #endif\n }\n \n uptr internal_readlink(const char *path, char *buf, uptr bufsize) {\n-#if SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n+#    if SANITIZER_LINUX\n   return internal_syscall(SYSCALL(readlinkat), AT_FDCWD, (uptr)path, (uptr)buf,\n                           bufsize);\n #else\n@@ -429,7 +432,7 @@ uptr internal_readlink(const char *path, char *buf, uptr bufsize) {\n }\n \n uptr internal_unlink(const char *path) {\n-#if SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n+#    if SANITIZER_LINUX\n   return internal_syscall(SYSCALL(unlinkat), AT_FDCWD, (uptr)path, 0);\n #else\n   return internal_syscall(SYSCALL(unlink), (uptr)path);\n@@ -440,12 +443,12 @@ uptr internal_rename(const char *oldpath, const char *newpath) {\n #if defined(__riscv) && defined(__linux__)\n   return internal_syscall(SYSCALL(renameat2), AT_FDCWD, (uptr)oldpath, AT_FDCWD,\n                           (uptr)newpath, 0);\n-#elif SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n+#    elif SANITIZER_LINUX\n   return internal_syscall(SYSCALL(renameat), AT_FDCWD, (uptr)oldpath, AT_FDCWD,\n                           (uptr)newpath);\n-#else\n+#    else\n   return internal_syscall(SYSCALL(rename), (uptr)oldpath, (uptr)newpath);\n-#endif\n+#    endif\n }\n \n uptr internal_sched_yield() {\n@@ -482,17 +485,20 @@ bool FileExists(const char *filename) {\n   if (ShouldMockFailureToOpen(filename))\n     return false;\n   struct stat st;\n-#if SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n-  if (internal_syscall(SYSCALL(newfstatat), AT_FDCWD, filename, &st, 0))\n-#else\n   if (internal_stat(filename, &st))\n-#endif\n     return false;\n   // Sanity check: filename is a regular file.\n   return S_ISREG(st.st_mode);\n }\n \n-#if !SANITIZER_NETBSD\n+bool DirExists(const char *path) {\n+  struct stat st;\n+  if (internal_stat(path, &st))\n+    return false;\n+  return S_ISDIR(st.st_mode);\n+}\n+\n+#  if !SANITIZER_NETBSD\n tid_t GetTid() {\n #if SANITIZER_FREEBSD\n   long Tid;\n@@ -691,17 +697,17 @@ void FutexWake(atomic_uint32_t *p, u32 count) {\n // Not used\n #else\n struct linux_dirent {\n-#if SANITIZER_X32 || defined(__aarch64__) || SANITIZER_RISCV64\n+#    if SANITIZER_X32 || SANITIZER_LINUX\n   u64 d_ino;\n   u64 d_off;\n-#else\n+#    else\n   unsigned long      d_ino;\n   unsigned long      d_off;\n-#endif\n+#    endif\n   unsigned short     d_reclen;\n-#if defined(__aarch64__) || SANITIZER_RISCV64\n+#    if SANITIZER_LINUX\n   unsigned char      d_type;\n-#endif\n+#    endif\n   char               d_name[256];\n };\n #endif\n@@ -737,11 +743,11 @@ int internal_dlinfo(void *handle, int request, void *p) {\n uptr internal_getdents(fd_t fd, struct linux_dirent *dirp, unsigned int count) {\n #if SANITIZER_FREEBSD\n   return internal_syscall(SYSCALL(getdirentries), fd, (uptr)dirp, count, NULL);\n-#elif SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n+#    elif SANITIZER_LINUX\n   return internal_syscall(SYSCALL(getdents64), fd, (uptr)dirp, count);\n-#else\n+#    else\n   return internal_syscall(SYSCALL(getdents), fd, (uptr)dirp, count);\n-#endif\n+#    endif\n }\n \n uptr internal_lseek(fd_t fd, OFF_T offset, int whence) {\n@@ -759,11 +765,15 @@ uptr internal_sigaltstack(const void *ss, void *oss) {\n }\n \n int internal_fork() {\n-#if SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n+#    if SANITIZER_LINUX\n+#      if SANITIZER_S390\n+  return internal_syscall(SYSCALL(clone), 0, SIGCHLD);\n+#      else\n   return internal_syscall(SYSCALL(clone), SIGCHLD, 0);\n-#else\n+#      endif\n+#    else\n   return internal_syscall(SYSCALL(fork));\n-#endif\n+#    endif\n }\n \n #if SANITIZER_FREEBSD\n@@ -1380,7 +1390,7 @@ uptr internal_clone(int (*fn)(void *), void *child_stack, int flags, void *arg,\n #elif defined(__aarch64__)\n uptr internal_clone(int (*fn)(void *), void *child_stack, int flags, void *arg,\n                     int *parent_tidptr, void *newtls, int *child_tidptr) {\n-  long long res;\n+  register long long res __asm__(\"x0\");\n   if (!fn || !child_stack)\n     return -EINVAL;\n   CHECK_EQ(0, (uptr)child_stack % 16);\n@@ -1760,6 +1770,8 @@ HandleSignalMode GetHandleSignalMode(int signum) {\n \n #if !SANITIZER_GO\n void *internal_start_thread(void *(*func)(void *arg), void *arg) {\n+  if (&real_pthread_create == 0)\n+    return nullptr;\n   // Start the thread with signals blocked, otherwise it can steal user signals.\n   ScopedBlockSignals block(nullptr);\n   void *th;\n@@ -1768,7 +1780,8 @@ void *internal_start_thread(void *(*func)(void *arg), void *arg) {\n }\n \n void internal_join_thread(void *th) {\n-  real_pthread_join(th, nullptr);\n+  if (&real_pthread_join)\n+    real_pthread_join(th, nullptr);\n }\n #else\n void *internal_start_thread(void *(*func)(void *), void *arg) { return 0; }\n@@ -1815,7 +1828,7 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n #else\n   uptr err = ucontext->uc_mcontext.gregs[REG_ERR];\n #endif // SANITIZER_FREEBSD\n-  return err & PF_WRITE ? WRITE : READ;\n+  return err & PF_WRITE ? Write : Read;\n #elif defined(__mips__)\n   uint32_t *exception_source;\n   uint32_t faulty_instruction;\n@@ -1838,7 +1851,7 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n     case 0x2a:  // swl\n     case 0x2e:  // swr\n #endif\n-      return SignalContext::WRITE;\n+      return SignalContext::Write;\n \n     case 0x20:  // lb\n     case 0x24:  // lbu\n@@ -1853,27 +1866,27 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n     case 0x22:  // lwl\n     case 0x26:  // lwr\n #endif\n-      return SignalContext::READ;\n+      return SignalContext::Read;\n #if __mips_isa_rev == 6\n     case 0x3b:  // pcrel\n       op_code = (faulty_instruction >> 19) & 0x3;\n       switch (op_code) {\n         case 0x1:  // lwpc\n         case 0x2:  // lwupc\n-          return SignalContext::READ;\n+          return SignalContext::Read;\n       }\n #endif\n   }\n-  return SignalContext::UNKNOWN;\n+  return SignalContext::Unknown;\n #elif defined(__arm__)\n   static const uptr FSR_WRITE = 1U << 11;\n   uptr fsr = ucontext->uc_mcontext.error_code;\n-  return fsr & FSR_WRITE ? WRITE : READ;\n+  return fsr & FSR_WRITE ? Write : Read;\n #elif defined(__aarch64__)\n   static const u64 ESR_ELx_WNR = 1U << 6;\n   u64 esr;\n-  if (!Aarch64GetESR(ucontext, &esr)) return UNKNOWN;\n-  return esr & ESR_ELx_WNR ? WRITE : READ;\n+  if (!Aarch64GetESR(ucontext, &esr)) return Unknown;\n+  return esr & ESR_ELx_WNR ? Write : Read;\n #elif defined(__sparc__)\n   // Decode the instruction to determine the access type.\n   // From OpenSolaris $SRC/uts/sun4/os/trap.c (get_accesstype).\n@@ -1889,7 +1902,7 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n #endif\n #endif\n   u32 instr = *(u32 *)pc;\n-  return (instr >> 21) & 1 ? WRITE: READ;\n+  return (instr >> 21) & 1 ? Write: Read;\n #elif defined(__riscv)\n #if SANITIZER_FREEBSD\n   unsigned long pc = ucontext->uc_mcontext.mc_gpregs.gp_sepc;\n@@ -1909,7 +1922,7 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n #if __riscv_xlen == 64\n       case 0b10'011:  // c.ldsp (rd != x0)\n #endif\n-        return rd ? SignalContext::READ : SignalContext::UNKNOWN;\n+        return rd ? SignalContext::Read : SignalContext::Unknown;\n       case 0b00'010:  // c.lw\n #if __riscv_flen >= 32 && __riscv_xlen == 32\n       case 0b10'011:  // c.flwsp\n@@ -1921,7 +1934,7 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n       case 0b00'001:  // c.fld\n       case 0b10'001:  // c.fldsp\n #endif\n-        return SignalContext::READ;\n+        return SignalContext::Read;\n       case 0b00'110:  // c.sw\n       case 0b10'110:  // c.swsp\n #if __riscv_flen >= 32 || __riscv_xlen == 64\n@@ -1932,9 +1945,9 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n       case 0b00'101:  // c.fsd\n       case 0b10'101:  // c.fsdsp\n #endif\n-        return SignalContext::WRITE;\n+        return SignalContext::Write;\n       default:\n-        return SignalContext::UNKNOWN;\n+        return SignalContext::Unknown;\n     }\n   }\n #endif\n@@ -1952,9 +1965,9 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n #endif\n         case 0b100:  // lbu\n         case 0b101:  // lhu\n-          return SignalContext::READ;\n+          return SignalContext::Read;\n         default:\n-          return SignalContext::UNKNOWN;\n+          return SignalContext::Unknown;\n       }\n     case 0b0100011:  // stores\n       switch (funct3) {\n@@ -1964,9 +1977,9 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n #if __riscv_xlen == 64\n         case 0b011:  // sd\n #endif\n-          return SignalContext::WRITE;\n+          return SignalContext::Write;\n         default:\n-          return SignalContext::UNKNOWN;\n+          return SignalContext::Unknown;\n       }\n #if __riscv_flen >= 32\n     case 0b0000111:  // floating-point loads\n@@ -1975,27 +1988,27 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n #if __riscv_flen == 64\n         case 0b011:  // fld\n #endif\n-          return SignalContext::READ;\n+          return SignalContext::Read;\n         default:\n-          return SignalContext::UNKNOWN;\n+          return SignalContext::Unknown;\n       }\n     case 0b0100111:  // floating-point stores\n       switch (funct3) {\n         case 0b010:  // fsw\n #if __riscv_flen == 64\n         case 0b011:  // fsd\n #endif\n-          return SignalContext::WRITE;\n+          return SignalContext::Write;\n         default:\n-          return SignalContext::UNKNOWN;\n+          return SignalContext::Unknown;\n       }\n #endif\n     default:\n-      return SignalContext::UNKNOWN;\n+      return SignalContext::Unknown;\n   }\n #else\n   (void)ucontext;\n-  return UNKNOWN;  // FIXME: Implement.\n+  return Unknown;  // FIXME: Implement.\n #endif\n }\n \n@@ -2070,12 +2083,19 @@ static void GetPcSpBp(void *context, uptr *pc, uptr *sp, uptr *bp) {\n   *sp = ucontext->uc_mcontext.gregs[REG_UESP];\n # endif\n #elif defined(__powerpc__) || defined(__powerpc64__)\n+#    if SANITIZER_FREEBSD\n+  ucontext_t *ucontext = (ucontext_t *)context;\n+  *pc = ucontext->uc_mcontext.mc_srr0;\n+  *sp = ucontext->uc_mcontext.mc_frame[1];\n+  *bp = ucontext->uc_mcontext.mc_frame[31];\n+#    else\n   ucontext_t *ucontext = (ucontext_t*)context;\n   *pc = ucontext->uc_mcontext.regs->nip;\n   *sp = ucontext->uc_mcontext.regs->gpr[PT_R1];\n   // The powerpc{,64}-linux ABIs do not specify r31 as the frame\n   // pointer, but GCC always uses r31 when we need a frame pointer.\n   *bp = ucontext->uc_mcontext.regs->gpr[PT_R31];\n+#    endif\n #elif defined(__sparc__)\n #if defined(__arch64__) || defined(__sparcv9)\n #define STACK_BIAS 2047\n@@ -2164,49 +2184,34 @@ void CheckASLR() {\n            GetArgv()[0]);\n     Die();\n   }\n-#elif SANITIZER_PPC64V2\n-  // Disable ASLR for Linux PPC64LE.\n-  int old_personality = personality(0xffffffff);\n-  if (old_personality != -1 && (old_personality & ADDR_NO_RANDOMIZE) == 0) {\n-    VReport(1, \"WARNING: Program is being run with address space layout \"\n-               \"randomization (ASLR) enabled which prevents the thread and \"\n-               \"memory sanitizers from working on powerpc64le.\\n\"\n-               \"ASLR will be disabled and the program re-executed.\\n\");\n-    CHECK_NE(personality(old_personality | ADDR_NO_RANDOMIZE), -1);\n-    ReExec();\n-  }\n #elif SANITIZER_FREEBSD\n-  int aslr_pie;\n-  uptr len = sizeof(aslr_pie);\n-#if SANITIZER_WORDSIZE == 64\n-  if (UNLIKELY(internal_sysctlbyname(\"kern.elf64.aslr.pie_enable\",\n-      &aslr_pie, &len, NULL, 0) == -1)) {\n+  int aslr_status;\n+  if (UNLIKELY(procctl(P_PID, 0, PROC_ASLR_STATUS, &aslr_status) == -1)) {\n     // We're making things less 'dramatic' here since\n-    // the OID is not necessarily guaranteed to be here\n+    // the cmd is not necessarily guaranteed to be here\n     // just yet regarding FreeBSD release\n     return;\n   }\n-\n-  if (aslr_pie > 0) {\n+  if ((aslr_status & PROC_ASLR_ACTIVE) != 0) {\n     Printf(\"This sanitizer is not compatible with enabled ASLR \"\n            \"and binaries compiled with PIE\\n\");\n     Die();\n   }\n-#endif\n-  // there might be 32 bits compat for 64 bits\n-  if (UNLIKELY(internal_sysctlbyname(\"kern.elf32.aslr.pie_enable\",\n-      &aslr_pie, &len, NULL, 0) == -1)) {\n-    return;\n-  }\n-\n-  if (aslr_pie > 0) {\n-    Printf(\"This sanitizer is not compatible with enabled ASLR \"\n-           \"and binaries compiled with PIE\\n\");\n-    Die();\n+#  elif SANITIZER_PPC64V2\n+  // Disable ASLR for Linux PPC64LE.\n+  int old_personality = personality(0xffffffff);\n+  if (old_personality != -1 && (old_personality & ADDR_NO_RANDOMIZE) == 0) {\n+    VReport(1,\n+            \"WARNING: Program is being run with address space layout \"\n+            \"randomization (ASLR) enabled which prevents the thread and \"\n+            \"memory sanitizers from working on powerpc64le.\\n\"\n+            \"ASLR will be disabled and the program re-executed.\\n\");\n+    CHECK_NE(personality(old_personality | ADDR_NO_RANDOMIZE), -1);\n+    ReExec();\n   }\n-#else\n+#  else\n   // Do nothing\n-#endif\n+#  endif\n }\n \n void CheckMPROTECT() {"}, {"sha": "ebd60e0b10f275d34b722dcb7372bc569bb9fa1c", "filename": "libsanitizer/sanitizer_common/sanitizer_linux.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -55,6 +55,9 @@ struct ScopedBlockSignals {\n   explicit ScopedBlockSignals(__sanitizer_sigset_t *copy);\n   ~ScopedBlockSignals();\n \n+  ScopedBlockSignals &operator=(const ScopedBlockSignals &) = delete;\n+  ScopedBlockSignals(const ScopedBlockSignals &) = delete;\n+\n  private:\n   __sanitizer_sigset_t saved_;\n };"}, {"sha": "25ad825f568bdc50a8ac61980697ef0014526c14", "filename": "libsanitizer/sanitizer_common/sanitizer_linux_libcdep.cpp", "status": "modified", "additions": 32, "deletions": 14, "changes": 46, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux_libcdep.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux_libcdep.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux_libcdep.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -216,7 +216,8 @@ void InitTlsSize() { }\n // On glibc x86_64, ThreadDescriptorSize() needs to be precise due to the usage\n // of g_tls_size. On other targets, ThreadDescriptorSize() is only used by lsan\n // to get the pointer to thread-specific data keys in the thread control block.\n-#if (SANITIZER_FREEBSD || SANITIZER_LINUX) && !SANITIZER_ANDROID\n+#if (SANITIZER_FREEBSD || SANITIZER_LINUX || SANITIZER_SOLARIS) && \\\n+    !SANITIZER_ANDROID && !SANITIZER_GO\n // sizeof(struct pthread) from glibc.\n static atomic_uintptr_t thread_descriptor_size;\n \n@@ -319,7 +320,6 @@ static uptr TlsPreTcbSize() {\n }\n #endif\n \n-#if !SANITIZER_GO\n namespace {\n struct TlsBlock {\n   uptr begin, end, align;\n@@ -407,9 +407,8 @@ __attribute__((unused)) static void GetStaticTlsBoundary(uptr *addr, uptr *size,\n   *addr = ranges[l].begin;\n   *size = ranges[r - 1].end - ranges[l].begin;\n }\n-#endif  // !SANITIZER_GO\n #endif  // (x86_64 || i386 || mips || ...) && (SANITIZER_FREEBSD ||\n-        // SANITIZER_LINUX) && !SANITIZER_ANDROID\n+        // SANITIZER_LINUX) && !SANITIZER_ANDROID && !SANITIZER_GO\n \n #if SANITIZER_NETBSD\n static struct tls_tcb * ThreadSelfTlsTcb() {\n@@ -478,7 +477,7 @@ static void GetTls(uptr *addr, uptr *size) {\n   const uptr pre_tcb_size = TlsPreTcbSize();\n   *addr = tp - pre_tcb_size;\n   *size = g_tls_size + pre_tcb_size;\n-#elif SANITIZER_FREEBSD || SANITIZER_LINUX\n+#elif SANITIZER_FREEBSD || SANITIZER_LINUX || SANITIZER_SOLARIS\n   uptr align;\n   GetStaticTlsBoundary(addr, size, &align);\n #if defined(__x86_64__) || defined(__i386__) || defined(__s390__) || \\\n@@ -539,11 +538,6 @@ static void GetTls(uptr *addr, uptr *size) {\n       *addr = (uptr)tcb->tcb_dtv[1];\n     }\n   }\n-#elif SANITIZER_SOLARIS\n-  // FIXME\n-  *addr = 0;\n-  *size = 0;\n-#else\n #error \"Unknown OS\"\n #endif\n }\n@@ -614,6 +608,34 @@ static int AddModuleSegments(const char *module_name, dl_phdr_info *info,\n       bool writable = phdr->p_flags & PF_W;\n       cur_module.addAddressRange(cur_beg, cur_end, executable,\n                                  writable);\n+    } else if (phdr->p_type == PT_NOTE) {\n+#  ifdef NT_GNU_BUILD_ID\n+      uptr off = 0;\n+      while (off + sizeof(ElfW(Nhdr)) < phdr->p_memsz) {\n+        auto *nhdr = reinterpret_cast<const ElfW(Nhdr) *>(info->dlpi_addr +\n+                                                          phdr->p_vaddr + off);\n+        constexpr auto kGnuNamesz = 4;  // \"GNU\" with NUL-byte.\n+        static_assert(kGnuNamesz % 4 == 0, \"kGnuNameSize is aligned to 4.\");\n+        if (nhdr->n_type == NT_GNU_BUILD_ID && nhdr->n_namesz == kGnuNamesz) {\n+          if (off + sizeof(ElfW(Nhdr)) + nhdr->n_namesz + nhdr->n_descsz >\n+              phdr->p_memsz) {\n+            // Something is very wrong, bail out instead of reading potentially\n+            // arbitrary memory.\n+            break;\n+          }\n+          const char *name =\n+              reinterpret_cast<const char *>(nhdr) + sizeof(*nhdr);\n+          if (internal_memcmp(name, \"GNU\", 3) == 0) {\n+            const char *value = reinterpret_cast<const char *>(nhdr) +\n+                                sizeof(*nhdr) + kGnuNamesz;\n+            cur_module.setUuid(value, nhdr->n_descsz);\n+            break;\n+          }\n+        }\n+        off += sizeof(*nhdr) + RoundUpTo(nhdr->n_namesz, 4) +\n+               RoundUpTo(nhdr->n_descsz, 4);\n+      }\n+#  endif\n     }\n   }\n   modules->push_back(cur_module);\n@@ -770,13 +792,9 @@ u32 GetNumberOfCPUs() {\n #elif SANITIZER_SOLARIS\n   return sysconf(_SC_NPROCESSORS_ONLN);\n #else\n-#if defined(CPU_COUNT)\n   cpu_set_t CPUs;\n   CHECK_EQ(sched_getaffinity(0, sizeof(cpu_set_t), &CPUs), 0);\n   return CPU_COUNT(&CPUs);\n-#else\n-  return 1;\n-#endif\n #endif\n }\n "}, {"sha": "74db831b0aaddf4a01345f7800d7df1953981444", "filename": "libsanitizer/sanitizer_common/sanitizer_linux_s390.cpp", "status": "modified", "additions": 10, "deletions": 4, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux_s390.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux_s390.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux_s390.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -57,8 +57,10 @@ uptr internal_mmap(void *addr, uptr length, int prot, int flags, int fd,\n \n uptr internal_clone(int (*fn)(void *), void *child_stack, int flags, void *arg,\n                     int *parent_tidptr, void *newtls, int *child_tidptr) {\n-  if (!fn || !child_stack)\n-    return -EINVAL;\n+  if (!fn || !child_stack) {\n+    errno = EINVAL;\n+    return -1;\n+  }\n   CHECK_EQ(0, (uptr)child_stack % 16);\n   // Minimum frame size.\n #ifdef __s390x__\n@@ -71,9 +73,9 @@ uptr internal_clone(int (*fn)(void *), void *child_stack, int flags, void *arg,\n   // And pass parameters.\n   ((unsigned long *)child_stack)[1] = (uptr)fn;\n   ((unsigned long *)child_stack)[2] = (uptr)arg;\n-  register long res __asm__(\"r2\");\n+  register uptr res __asm__(\"r2\");\n   register void *__cstack      __asm__(\"r2\") = child_stack;\n-  register int __flags         __asm__(\"r3\") = flags;\n+  register long __flags        __asm__(\"r3\") = flags;\n   register int * __ptidptr     __asm__(\"r4\") = parent_tidptr;\n   register int * __ctidptr     __asm__(\"r5\") = child_tidptr;\n   register void * __newtls     __asm__(\"r6\") = newtls;\n@@ -113,6 +115,10 @@ uptr internal_clone(int (*fn)(void *), void *child_stack, int flags, void *arg,\n                          \"r\"(__ctidptr),\n                          \"r\"(__newtls)\n                        : \"memory\", \"cc\");\n+  if (res >= (uptr)-4095) {\n+    errno = -res;\n+    return -1;\n+  }\n   return res;\n }\n "}, {"sha": "42acfbdcea09239fb2684836116362d2a9491bdf", "filename": "libsanitizer/sanitizer_common/sanitizer_lzw.h", "status": "added", "additions": 159, "deletions": 0, "changes": 159, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_lzw.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_lzw.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_lzw.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -0,0 +1,159 @@\n+//===-- sanitizer_lzw.h -----------------------------------------*- C++ -*-===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+//\n+// Lempel\u2013Ziv\u2013Welch encoding/decoding\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef SANITIZER_LZW_H\n+#define SANITIZER_LZW_H\n+\n+#include \"sanitizer_dense_map.h\"\n+\n+namespace __sanitizer {\n+\n+using LzwCodeType = u32;\n+\n+template <class T, class ItIn, class ItOut>\n+ItOut LzwEncode(ItIn begin, ItIn end, ItOut out) {\n+  using Substring =\n+      detail::DenseMapPair<LzwCodeType /* Prefix */, T /* Next input */>;\n+\n+  // Sentinel value for substrings of len 1.\n+  static constexpr LzwCodeType kNoPrefix =\n+      Min(DenseMapInfo<Substring>::getEmptyKey().first,\n+          DenseMapInfo<Substring>::getTombstoneKey().first) -\n+      1;\n+  DenseMap<Substring, LzwCodeType> prefix_to_code;\n+  {\n+    // Add all substring of len 1 as initial dictionary.\n+    InternalMmapVector<T> dict_len1;\n+    for (auto it = begin; it != end; ++it)\n+      if (prefix_to_code.try_emplace({kNoPrefix, *it}, 0).second)\n+        dict_len1.push_back(*it);\n+\n+    // Slightly helps with later delta encoding.\n+    Sort(dict_len1.data(), dict_len1.size());\n+\n+    // For large sizeof(T) we have to store dict_len1. Smaller types like u8 can\n+    // just generate them.\n+    *out = dict_len1.size();\n+    ++out;\n+\n+    for (uptr i = 0; i != dict_len1.size(); ++i) {\n+      // Remap after the Sort.\n+      prefix_to_code[{kNoPrefix, dict_len1[i]}] = i;\n+      *out = dict_len1[i];\n+      ++out;\n+    }\n+    CHECK_EQ(prefix_to_code.size(), dict_len1.size());\n+  }\n+\n+  if (begin == end)\n+    return out;\n+\n+  // Main LZW encoding loop.\n+  LzwCodeType match = prefix_to_code.find({kNoPrefix, *begin})->second;\n+  ++begin;\n+  for (auto it = begin; it != end; ++it) {\n+    // Extend match with the new item.\n+    auto ins = prefix_to_code.try_emplace({match, *it}, prefix_to_code.size());\n+    if (ins.second) {\n+      // This is a new substring, but emit the code for the current match\n+      // (before extend). This allows LZW decoder to recover the dictionary.\n+      *out = match;\n+      ++out;\n+      // Reset the match to a single item, which must be already in the map.\n+      match = prefix_to_code.find({kNoPrefix, *it})->second;\n+    } else {\n+      // Already known, use as the current match.\n+      match = ins.first->second;\n+    }\n+  }\n+\n+  *out = match;\n+  ++out;\n+\n+  return out;\n+}\n+\n+template <class T, class ItIn, class ItOut>\n+ItOut LzwDecode(ItIn begin, ItIn end, ItOut out) {\n+  if (begin == end)\n+    return out;\n+\n+  // Load dictionary of len 1 substrings. Theses correspont to lowest codes.\n+  InternalMmapVector<T> dict_len1(*begin);\n+  ++begin;\n+\n+  if (begin == end)\n+    return out;\n+\n+  for (auto& v : dict_len1) {\n+    v = *begin;\n+    ++begin;\n+  }\n+\n+  // Substrings of len 2 and up. Indexes are shifted because [0,\n+  // dict_len1.size()) stored in dict_len1. Substings get here after being\n+  // emitted to the output, so we can use output position.\n+  InternalMmapVector<detail::DenseMapPair<ItOut /* begin. */, ItOut /* end */>>\n+      code_to_substr;\n+\n+  // Copies already emitted substrings into the output again.\n+  auto copy = [&code_to_substr, &dict_len1](LzwCodeType code, ItOut out) {\n+    if (code < dict_len1.size()) {\n+      *out = dict_len1[code];\n+      ++out;\n+      return out;\n+    }\n+    const auto& s = code_to_substr[code - dict_len1.size()];\n+\n+    for (ItOut it = s.first; it != s.second; ++it, ++out) *out = *it;\n+    return out;\n+  };\n+\n+  // Returns lens of the substring with the given code.\n+  auto code_to_len = [&code_to_substr, &dict_len1](LzwCodeType code) -> uptr {\n+    if (code < dict_len1.size())\n+      return 1;\n+    const auto& s = code_to_substr[code - dict_len1.size()];\n+    return s.second - s.first;\n+  };\n+\n+  // Main LZW decoding loop.\n+  LzwCodeType prev_code = *begin;\n+  ++begin;\n+  out = copy(prev_code, out);\n+  for (auto it = begin; it != end; ++it) {\n+    LzwCodeType code = *it;\n+    auto start = out;\n+    if (code == dict_len1.size() + code_to_substr.size()) {\n+      // Special LZW case. The code is not in the dictionary yet. This is\n+      // possible only when the new substring is the same as previous one plus\n+      // the first item of the previous substring. We can emit that in two\n+      // steps.\n+      out = copy(prev_code, out);\n+      *out = *start;\n+      ++out;\n+    } else {\n+      out = copy(code, out);\n+    }\n+\n+    // Every time encoded emits the code, it also creates substing of len + 1\n+    // including the first item of the just emmited substring. Do the same here.\n+    uptr len = code_to_len(prev_code);\n+    code_to_substr.push_back({start - len, start + 1});\n+\n+    prev_code = code;\n+  }\n+  return out;\n+}\n+\n+}  // namespace __sanitizer\n+#endif"}, {"sha": "05512a576ad1321afb080322b2d881929d0c0e05", "filename": "libsanitizer/sanitizer_common/sanitizer_mac.cpp", "status": "modified", "additions": 47, "deletions": 48, "changes": 95, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -25,6 +25,7 @@\n #include \"sanitizer_common.h\"\n #include \"sanitizer_file.h\"\n #include \"sanitizer_flags.h\"\n+#include \"sanitizer_interface_internal.h\"\n #include \"sanitizer_internal_defs.h\"\n #include \"sanitizer_libc.h\"\n #include \"sanitizer_platform_limits_posix.h\"\n@@ -37,7 +38,7 @@\n extern char **environ;\n #endif\n \n-#if defined(__has_include) && __has_include(<os/trace.h>) && defined(__BLOCKS__)\n+#if defined(__has_include) && __has_include(<os/trace.h>)\n #define SANITIZER_OS_TRACE 1\n #include <os/trace.h>\n #else\n@@ -70,15 +71,7 @@ extern \"C\" {\n #include <mach/mach_time.h>\n #include <mach/vm_statistics.h>\n #include <malloc/malloc.h>\n-#if defined(__has_builtin) && __has_builtin(__builtin_os_log_format)\n-# include <os/log.h>\n-#else\n-   /* Without support for __builtin_os_log_format, fall back to the older\n-      method.  */\n-# define OS_LOG_DEFAULT 0\n-# define os_log_error(A,B,C) \\\n-  asl_log(nullptr, nullptr, ASL_LEVEL_ERR, \"%s\", (C));\n-#endif\n+#include <os/log.h>\n #include <pthread.h>\n #include <sched.h>\n #include <signal.h>\n@@ -273,30 +266,32 @@ int internal_sysctlbyname(const char *sname, void *oldp, uptr *oldlenp,\n \n static fd_t internal_spawn_impl(const char *argv[], const char *envp[],\n                                 pid_t *pid) {\n-  fd_t master_fd = kInvalidFd;\n-  fd_t slave_fd = kInvalidFd;\n+  fd_t primary_fd = kInvalidFd;\n+  fd_t secondary_fd = kInvalidFd;\n \n   auto fd_closer = at_scope_exit([&] {\n-    internal_close(master_fd);\n-    internal_close(slave_fd);\n+    internal_close(primary_fd);\n+    internal_close(secondary_fd);\n   });\n \n   // We need a new pseudoterminal to avoid buffering problems. The 'atos' tool\n   // in particular detects when it's talking to a pipe and forgets to flush the\n   // output stream after sending a response.\n-  master_fd = posix_openpt(O_RDWR);\n-  if (master_fd == kInvalidFd) return kInvalidFd;\n+  primary_fd = posix_openpt(O_RDWR);\n+  if (primary_fd == kInvalidFd)\n+    return kInvalidFd;\n \n-  int res = grantpt(master_fd) || unlockpt(master_fd);\n+  int res = grantpt(primary_fd) || unlockpt(primary_fd);\n   if (res != 0) return kInvalidFd;\n \n   // Use TIOCPTYGNAME instead of ptsname() to avoid threading problems.\n-  char slave_pty_name[128];\n-  res = ioctl(master_fd, TIOCPTYGNAME, slave_pty_name);\n+  char secondary_pty_name[128];\n+  res = ioctl(primary_fd, TIOCPTYGNAME, secondary_pty_name);\n   if (res == -1) return kInvalidFd;\n \n-  slave_fd = internal_open(slave_pty_name, O_RDWR);\n-  if (slave_fd == kInvalidFd) return kInvalidFd;\n+  secondary_fd = internal_open(secondary_pty_name, O_RDWR);\n+  if (secondary_fd == kInvalidFd)\n+    return kInvalidFd;\n \n   // File descriptor actions\n   posix_spawn_file_actions_t acts;\n@@ -307,9 +302,9 @@ static fd_t internal_spawn_impl(const char *argv[], const char *envp[],\n     posix_spawn_file_actions_destroy(&acts);\n   });\n \n-  res = posix_spawn_file_actions_adddup2(&acts, slave_fd, STDIN_FILENO) ||\n-        posix_spawn_file_actions_adddup2(&acts, slave_fd, STDOUT_FILENO) ||\n-        posix_spawn_file_actions_addclose(&acts, slave_fd);\n+  res = posix_spawn_file_actions_adddup2(&acts, secondary_fd, STDIN_FILENO) ||\n+        posix_spawn_file_actions_adddup2(&acts, secondary_fd, STDOUT_FILENO) ||\n+        posix_spawn_file_actions_addclose(&acts, secondary_fd);\n   if (res != 0) return kInvalidFd;\n \n   // Spawn attributes\n@@ -334,14 +329,14 @@ static fd_t internal_spawn_impl(const char *argv[], const char *envp[],\n \n   // Disable echo in the new terminal, disable CR.\n   struct termios termflags;\n-  tcgetattr(master_fd, &termflags);\n+  tcgetattr(primary_fd, &termflags);\n   termflags.c_oflag &= ~ONLCR;\n   termflags.c_lflag &= ~ECHO;\n-  tcsetattr(master_fd, TCSANOW, &termflags);\n+  tcsetattr(primary_fd, TCSANOW, &termflags);\n \n-  // On success, do not close master_fd on scope exit.\n-  fd_t fd = master_fd;\n-  master_fd = kInvalidFd;\n+  // On success, do not close primary_fd on scope exit.\n+  fd_t fd = primary_fd;\n+  primary_fd = kInvalidFd;\n \n   return fd;\n }\n@@ -398,6 +393,13 @@ bool FileExists(const char *filename) {\n   return S_ISREG(st.st_mode);\n }\n \n+bool DirExists(const char *path) {\n+  struct stat st;\n+  if (stat(path, &st))\n+    return false;\n+  return S_ISDIR(st.st_mode);\n+}\n+\n tid_t GetTid() {\n   tid_t tid;\n   pthread_threadid_np(nullptr, &tid);\n@@ -877,9 +879,9 @@ void LogFullErrorReport(const char *buffer) {\n SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n #if defined(__x86_64__) || defined(__i386__)\n   ucontext_t *ucontext = static_cast<ucontext_t*>(context);\n-  return ucontext->uc_mcontext->__es.__err & 2 /*T_PF_WRITE*/ ? WRITE : READ;\n+  return ucontext->uc_mcontext->__es.__err & 2 /*T_PF_WRITE*/ ? Write : Read;\n #else\n-  return UNKNOWN;\n+  return Unknown;\n #endif\n }\n \n@@ -894,18 +896,14 @@ bool SignalContext::IsTrueFaultingAddress() const {\n     (uptr)ptrauth_strip(     \\\n         (void *)arm_thread_state64_get_##r(ucontext->uc_mcontext->__ss), 0)\n #else\n-  #define AARCH64_GET_REG(r) ucontext->uc_mcontext->__ss.__##r\n+  #define AARCH64_GET_REG(r) (uptr)ucontext->uc_mcontext->__ss.__##r\n #endif\n \n static void GetPcSpBp(void *context, uptr *pc, uptr *sp, uptr *bp) {\n   ucontext_t *ucontext = (ucontext_t*)context;\n # if defined(__aarch64__)\n   *pc = AARCH64_GET_REG(pc);\n-#   if defined(__IPHONE_8_0) && __IPHONE_OS_VERSION_MAX_ALLOWED >= __IPHONE_8_0\n   *bp = AARCH64_GET_REG(fp);\n-#   else\n-  *bp = AARCH64_GET_REG(lr);\n-#   endif\n   *sp = AARCH64_GET_REG(sp);\n # elif defined(__x86_64__)\n   *pc = ucontext->uc_mcontext->__ss.__rip;\n@@ -1057,12 +1055,12 @@ void MaybeReexec() {\n   }\n \n   // Verify that interceptors really work.  We'll use dlsym to locate\n-  // \"pthread_create\", if interceptors are working, it should really point to\n-  // \"wrap_pthread_create\" within our own dylib.\n-  Dl_info info_pthread_create;\n-  void *dlopen_addr = dlsym(RTLD_DEFAULT, \"pthread_create\");\n-  RAW_CHECK(dladdr(dlopen_addr, &info_pthread_create));\n-  if (internal_strcmp(info.dli_fname, info_pthread_create.dli_fname) != 0) {\n+  // \"puts\", if interceptors are working, it should really point to\n+  // \"wrap_puts\" within our own dylib.\n+  Dl_info info_puts;\n+  void *dlopen_addr = dlsym(RTLD_DEFAULT, \"puts\");\n+  RAW_CHECK(dladdr(dlopen_addr, &info_puts));\n+  if (internal_strcmp(info.dli_fname, info_puts.dli_fname) != 0) {\n     Report(\n         \"ERROR: Interceptors are not working. This may be because %s is \"\n         \"loaded too late (e.g. via dlopen). Please launch the executable \"\n@@ -1229,7 +1227,7 @@ uptr MapDynamicShadow(uptr shadow_size_bytes, uptr shadow_scale,\n \n   uptr largest_gap_found = 0;\n   uptr max_occupied_addr = 0;\n-  VReport(2, \"FindDynamicShadowStart, space_size = %p\\n\", space_size);\n+  VReport(2, \"FindDynamicShadowStart, space_size = %p\\n\", (void *)space_size);\n   uptr shadow_start =\n       FindAvailableMemoryRange(space_size, alignment, granularity,\n                                &largest_gap_found, &max_occupied_addr);\n@@ -1238,20 +1236,21 @@ uptr MapDynamicShadow(uptr shadow_size_bytes, uptr shadow_scale,\n     VReport(\n         2,\n         \"Shadow doesn't fit, largest_gap_found = %p, max_occupied_addr = %p\\n\",\n-        largest_gap_found, max_occupied_addr);\n+        (void *)largest_gap_found, (void *)max_occupied_addr);\n     uptr new_max_vm = RoundDownTo(largest_gap_found << shadow_scale, alignment);\n     if (new_max_vm < max_occupied_addr) {\n       Report(\"Unable to find a memory range for dynamic shadow.\\n\");\n       Report(\n           \"space_size = %p, largest_gap_found = %p, max_occupied_addr = %p, \"\n           \"new_max_vm = %p\\n\",\n-          space_size, largest_gap_found, max_occupied_addr, new_max_vm);\n+          (void *)space_size, (void *)largest_gap_found,\n+          (void *)max_occupied_addr, (void *)new_max_vm);\n       CHECK(0 && \"cannot place shadow\");\n     }\n     RestrictMemoryToMaxAddress(new_max_vm);\n     high_mem_end = new_max_vm - 1;\n     space_size = (high_mem_end >> shadow_scale) + left_padding;\n-    VReport(2, \"FindDynamicShadowStart, space_size = %p\\n\", space_size);\n+    VReport(2, \"FindDynamicShadowStart, space_size = %p\\n\", (void *)space_size);\n     shadow_start = FindAvailableMemoryRange(space_size, alignment, granularity,\n                                             nullptr, nullptr);\n     if (shadow_start == 0) {\n@@ -1331,7 +1330,7 @@ void SignalContext::DumpAllRegisters(void *context) {\n # define DUMPREG64(r) \\\n     Printf(\"%s = 0x%016llx  \", #r, ucontext->uc_mcontext->__ss.__ ## r);\n # define DUMPREGA64(r) \\\n-    Printf(\"   %s = 0x%016llx  \", #r, AARCH64_GET_REG(r));\n+    Printf(\"   %s = 0x%016lx  \", #r, AARCH64_GET_REG(r));\n # define DUMPREG32(r) \\\n     Printf(\"%s = 0x%08x  \", #r, ucontext->uc_mcontext->__ss.__ ## r);\n # define DUMPREG_(r)   Printf(\" \"); DUMPREG(r);\n@@ -1401,7 +1400,7 @@ void DumpProcessMap() {\n     char uuid_str[128];\n     FormatUUID(uuid_str, sizeof(uuid_str), modules[i].uuid());\n     Printf(\"0x%zx-0x%zx %s (%s) %s\\n\", modules[i].base_address(),\n-           modules[i].max_executable_address(), modules[i].full_name(),\n+           modules[i].max_address(), modules[i].full_name(),\n            ModuleArchToString(modules[i].arch()), uuid_str);\n   }\n   Printf(\"End of module map.\\n\");"}, {"sha": "0b6af5a3c0edc649c4d65dfd1a6ca1bce13aa9b9", "filename": "libsanitizer/sanitizer_common/sanitizer_mac.h", "status": "modified", "additions": 0, "deletions": 20, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -14,26 +14,6 @@\n \n #include \"sanitizer_common.h\"\n #include \"sanitizer_platform.h\"\n-\n-/* TARGET_OS_OSX is not present in SDKs before Darwin16 (macOS 10.12) use\n-   TARGET_OS_MAC (we have no support for iOS in any form for these versions,\n-   so there's no ambiguity).  */\n-#if !defined(TARGET_OS_OSX) && TARGET_OS_MAC\n-# define TARGET_OS_OSX 1\n-#endif\n-\n-/* Other TARGET_OS_xxx are not present on earlier versions, define them to\n-   0 (we have no support for them; they are not valid targets anyway).  */\n-#ifndef TARGET_OS_IOS\n-#define TARGET_OS_IOS 0\n-#endif\n-#ifndef TARGET_OS_TV\n-#define TARGET_OS_TV 0\n-#endif\n-#ifndef TARGET_OS_WATCH\n-#define TARGET_OS_WATCH 0\n-#endif\n-\n #if SANITIZER_MAC\n #include \"sanitizer_posix.h\"\n "}, {"sha": "d2188a9e6d6239e7244fdabe2d2ce309de483483", "filename": "libsanitizer/sanitizer_common/sanitizer_mutex.h", "status": "modified", "additions": 40, "deletions": 23, "changes": 63, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_mutex.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_mutex.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_mutex.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -20,25 +20,27 @@\n \n namespace __sanitizer {\n \n-class MUTEX StaticSpinMutex {\n+class SANITIZER_MUTEX StaticSpinMutex {\n  public:\n   void Init() {\n     atomic_store(&state_, 0, memory_order_relaxed);\n   }\n \n-  void Lock() ACQUIRE() {\n+  void Lock() SANITIZER_ACQUIRE() {\n     if (LIKELY(TryLock()))\n       return;\n     LockSlow();\n   }\n \n-  bool TryLock() TRY_ACQUIRE(true) {\n+  bool TryLock() SANITIZER_TRY_ACQUIRE(true) {\n     return atomic_exchange(&state_, 1, memory_order_acquire) == 0;\n   }\n \n-  void Unlock() RELEASE() { atomic_store(&state_, 0, memory_order_release); }\n+  void Unlock() SANITIZER_RELEASE() {\n+    atomic_store(&state_, 0, memory_order_release);\n+  }\n \n-  void CheckLocked() const CHECK_LOCKED() {\n+  void CheckLocked() const SANITIZER_CHECK_LOCKED() {\n     CHECK_EQ(atomic_load(&state_, memory_order_relaxed), 1);\n   }\n \n@@ -48,7 +50,7 @@ class MUTEX StaticSpinMutex {\n   void LockSlow();\n };\n \n-class MUTEX SpinMutex : public StaticSpinMutex {\n+class SANITIZER_MUTEX SpinMutex : public StaticSpinMutex {\n  public:\n   SpinMutex() {\n     Init();\n@@ -156,12 +158,12 @@ class CheckedMutex {\n // Derive from CheckedMutex for the purposes of EBO.\n // We could make it a field marked with [[no_unique_address]],\n // but this attribute is not supported by some older compilers.\n-class MUTEX Mutex : CheckedMutex {\n+class SANITIZER_MUTEX Mutex : CheckedMutex {\n  public:\n   explicit constexpr Mutex(MutexType type = MutexUnchecked)\n       : CheckedMutex(type) {}\n \n-  void Lock() ACQUIRE() {\n+  void Lock() SANITIZER_ACQUIRE() {\n     CheckedMutex::Lock();\n     u64 reset_mask = ~0ull;\n     u64 state = atomic_load_relaxed(&state_);\n@@ -206,7 +208,21 @@ class MUTEX Mutex : CheckedMutex {\n     }\n   }\n \n-  void Unlock() RELEASE() {\n+  bool TryLock() SANITIZER_TRY_ACQUIRE(true) {\n+    u64 state = atomic_load_relaxed(&state_);\n+    for (;;) {\n+      if (UNLIKELY(state & (kWriterLock | kReaderLockMask)))\n+        return false;\n+      // The mutex is not read-/write-locked, try to lock.\n+      if (LIKELY(atomic_compare_exchange_weak(\n+              &state_, &state, state | kWriterLock, memory_order_acquire))) {\n+        CheckedMutex::Lock();\n+        return true;\n+      }\n+    }\n+  }\n+\n+  void Unlock() SANITIZER_RELEASE() {\n     CheckedMutex::Unlock();\n     bool wake_writer;\n     u64 wake_readers;\n@@ -234,7 +250,7 @@ class MUTEX Mutex : CheckedMutex {\n       readers_.Post(wake_readers);\n   }\n \n-  void ReadLock() ACQUIRE_SHARED() {\n+  void ReadLock() SANITIZER_ACQUIRE_SHARED() {\n     CheckedMutex::Lock();\n     u64 reset_mask = ~0ull;\n     u64 state = atomic_load_relaxed(&state_);\n@@ -271,7 +287,7 @@ class MUTEX Mutex : CheckedMutex {\n     }\n   }\n \n-  void ReadUnlock() RELEASE_SHARED() {\n+  void ReadUnlock() SANITIZER_RELEASE_SHARED() {\n     CheckedMutex::Unlock();\n     bool wake;\n     u64 new_state;\n@@ -297,13 +313,13 @@ class MUTEX Mutex : CheckedMutex {\n   // owns the mutex but a child checks that it is locked. Rather than\n   // maintaining complex state to work around those situations, the check only\n   // checks that the mutex is owned.\n-  void CheckWriteLocked() const CHECK_LOCKED() {\n+  void CheckWriteLocked() const SANITIZER_CHECK_LOCKED() {\n     CHECK(atomic_load(&state_, memory_order_relaxed) & kWriterLock);\n   }\n \n-  void CheckLocked() const CHECK_LOCKED() { CheckWriteLocked(); }\n+  void CheckLocked() const SANITIZER_CHECK_LOCKED() { CheckWriteLocked(); }\n \n-  void CheckReadLocked() const CHECK_LOCKED() {\n+  void CheckReadLocked() const SANITIZER_CHECK_LOCKED() {\n     CHECK(atomic_load(&state_, memory_order_relaxed) & kReaderLockMask);\n   }\n \n@@ -361,13 +377,13 @@ void FutexWait(atomic_uint32_t *p, u32 cmp);\n void FutexWake(atomic_uint32_t *p, u32 count);\n \n template <typename MutexType>\n-class SCOPED_LOCK GenericScopedLock {\n+class SANITIZER_SCOPED_LOCK GenericScopedLock {\n  public:\n-  explicit GenericScopedLock(MutexType *mu) ACQUIRE(mu) : mu_(mu) {\n+  explicit GenericScopedLock(MutexType *mu) SANITIZER_ACQUIRE(mu) : mu_(mu) {\n     mu_->Lock();\n   }\n \n-  ~GenericScopedLock() RELEASE() { mu_->Unlock(); }\n+  ~GenericScopedLock() SANITIZER_RELEASE() { mu_->Unlock(); }\n \n  private:\n   MutexType *mu_;\n@@ -377,13 +393,14 @@ class SCOPED_LOCK GenericScopedLock {\n };\n \n template <typename MutexType>\n-class SCOPED_LOCK GenericScopedReadLock {\n+class SANITIZER_SCOPED_LOCK GenericScopedReadLock {\n  public:\n-  explicit GenericScopedReadLock(MutexType *mu) ACQUIRE(mu) : mu_(mu) {\n+  explicit GenericScopedReadLock(MutexType *mu) SANITIZER_ACQUIRE(mu)\n+      : mu_(mu) {\n     mu_->ReadLock();\n   }\n \n-  ~GenericScopedReadLock() RELEASE() { mu_->ReadUnlock(); }\n+  ~GenericScopedReadLock() SANITIZER_RELEASE() { mu_->ReadUnlock(); }\n \n  private:\n   MutexType *mu_;\n@@ -393,18 +410,18 @@ class SCOPED_LOCK GenericScopedReadLock {\n };\n \n template <typename MutexType>\n-class SCOPED_LOCK GenericScopedRWLock {\n+class SANITIZER_SCOPED_LOCK GenericScopedRWLock {\n  public:\n   ALWAYS_INLINE explicit GenericScopedRWLock(MutexType *mu, bool write)\n-      ACQUIRE(mu)\n+      SANITIZER_ACQUIRE(mu)\n       : mu_(mu), write_(write) {\n     if (write_)\n       mu_->Lock();\n     else\n       mu_->ReadLock();\n   }\n \n-  ALWAYS_INLINE ~GenericScopedRWLock() RELEASE() {\n+  ALWAYS_INLINE ~GenericScopedRWLock() SANITIZER_RELEASE() {\n     if (write_)\n       mu_->Unlock();\n     else"}, {"sha": "e18b0030567f63334161791995583f1b6300c562", "filename": "libsanitizer/sanitizer_common/sanitizer_persistent_allocator.h", "status": "removed", "additions": 0, "deletions": 110, "changes": 110, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2285af309000b74da0f7dc756a0b55e5f0b1b56/libsanitizer%2Fsanitizer_common%2Fsanitizer_persistent_allocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2285af309000b74da0f7dc756a0b55e5f0b1b56/libsanitizer%2Fsanitizer_common%2Fsanitizer_persistent_allocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_persistent_allocator.h?ref=e2285af309000b74da0f7dc756a0b55e5f0b1b56", "patch": "@@ -1,110 +0,0 @@\n-//===-- sanitizer_persistent_allocator.h ------------------------*- C++ -*-===//\n-//\n-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n-// See https://llvm.org/LICENSE.txt for license information.\n-// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n-//\n-//===----------------------------------------------------------------------===//\n-//\n-// A fast memory allocator that does not support free() nor realloc().\n-// All allocations are forever.\n-//===----------------------------------------------------------------------===//\n-\n-#ifndef SANITIZER_PERSISTENT_ALLOCATOR_H\n-#define SANITIZER_PERSISTENT_ALLOCATOR_H\n-\n-#include \"sanitizer_internal_defs.h\"\n-#include \"sanitizer_mutex.h\"\n-#include \"sanitizer_atomic.h\"\n-#include \"sanitizer_common.h\"\n-\n-namespace __sanitizer {\n-\n-template <typename T>\n-class PersistentAllocator {\n- public:\n-  T *alloc(uptr count = 1);\n-  uptr allocated() const { return atomic_load_relaxed(&mapped_size); }\n-\n-  void TestOnlyUnmap();\n-\n- private:\n-  T *tryAlloc(uptr count);\n-  T *refillAndAlloc(uptr count);\n-  mutable StaticSpinMutex mtx;  // Protects alloc of new blocks.\n-  atomic_uintptr_t region_pos;  // Region allocator for Node's.\n-  atomic_uintptr_t region_end;\n-  atomic_uintptr_t mapped_size;\n-\n-  struct BlockInfo {\n-    const BlockInfo *next;\n-    uptr ptr;\n-    uptr size;\n-  };\n-  const BlockInfo *curr;\n-};\n-\n-template <typename T>\n-inline T *PersistentAllocator<T>::tryAlloc(uptr count) {\n-  // Optimisic lock-free allocation, essentially try to bump the region ptr.\n-  for (;;) {\n-    uptr cmp = atomic_load(&region_pos, memory_order_acquire);\n-    uptr end = atomic_load(&region_end, memory_order_acquire);\n-    uptr size = count * sizeof(T);\n-    if (cmp == 0 || cmp + size > end)\n-      return nullptr;\n-    if (atomic_compare_exchange_weak(&region_pos, &cmp, cmp + size,\n-                                     memory_order_acquire))\n-      return reinterpret_cast<T *>(cmp);\n-  }\n-}\n-\n-template <typename T>\n-inline T *PersistentAllocator<T>::alloc(uptr count) {\n-  // First, try to allocate optimisitically.\n-  T *s = tryAlloc(count);\n-  if (LIKELY(s))\n-    return s;\n-  return refillAndAlloc(count);\n-}\n-\n-template <typename T>\n-inline T *PersistentAllocator<T>::refillAndAlloc(uptr count) {\n-  // If failed, lock, retry and alloc new superblock.\n-  SpinMutexLock l(&mtx);\n-  for (;;) {\n-    T *s = tryAlloc(count);\n-    if (s)\n-      return s;\n-    atomic_store(&region_pos, 0, memory_order_relaxed);\n-    uptr size = count * sizeof(T) + sizeof(BlockInfo);\n-    uptr allocsz = RoundUpTo(Max<uptr>(size, 64u * 1024u), GetPageSizeCached());\n-    uptr mem = (uptr)MmapOrDie(allocsz, \"stack depot\");\n-    BlockInfo *new_block = (BlockInfo *)(mem + allocsz) - 1;\n-    new_block->next = curr;\n-    new_block->ptr = mem;\n-    new_block->size = allocsz;\n-    curr = new_block;\n-\n-    atomic_fetch_add(&mapped_size, allocsz, memory_order_relaxed);\n-\n-    allocsz -= sizeof(BlockInfo);\n-    atomic_store(&region_end, mem + allocsz, memory_order_release);\n-    atomic_store(&region_pos, mem, memory_order_release);\n-  }\n-}\n-\n-template <typename T>\n-void PersistentAllocator<T>::TestOnlyUnmap() {\n-  while (curr) {\n-    uptr mem = curr->ptr;\n-    uptr allocsz = curr->size;\n-    curr = curr->next;\n-    UnmapOrDie((void *)mem, allocsz);\n-  }\n-  internal_memset(this, 0, sizeof(*this));\n-}\n-\n-} // namespace __sanitizer\n-\n-#endif // SANITIZER_PERSISTENT_ALLOCATOR_H"}, {"sha": "8fe0d8314312c7101902e94c07676cb2cf91321f", "filename": "libsanitizer/sanitizer_common/sanitizer_platform.h", "status": "modified", "additions": 175, "deletions": 156, "changes": 331, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -22,103 +22,110 @@\n // function declarations into a .S file which doesn't compile.\n // https://crbug.com/1162741\n #if __has_include(<features.h>) && !defined(__ANDROID__)\n-#include <features.h>\n+#  include <features.h>\n #endif\n \n #if defined(__linux__)\n-# define SANITIZER_LINUX   1\n+#  define SANITIZER_LINUX 1\n #else\n-# define SANITIZER_LINUX   0\n+#  define SANITIZER_LINUX 0\n #endif\n \n #if defined(__GLIBC__)\n-# define SANITIZER_GLIBC   1\n+#  define SANITIZER_GLIBC 1\n #else\n-# define SANITIZER_GLIBC   0\n+#  define SANITIZER_GLIBC 0\n #endif\n \n #if defined(__FreeBSD__)\n-# define SANITIZER_FREEBSD 1\n+#  define SANITIZER_FREEBSD 1\n #else\n-# define SANITIZER_FREEBSD 0\n+#  define SANITIZER_FREEBSD 0\n #endif\n \n #if defined(__NetBSD__)\n-# define SANITIZER_NETBSD 1\n+#  define SANITIZER_NETBSD 1\n #else\n-# define SANITIZER_NETBSD 0\n+#  define SANITIZER_NETBSD 0\n #endif\n \n #if defined(__sun__) && defined(__svr4__)\n-# define SANITIZER_SOLARIS 1\n+#  define SANITIZER_SOLARIS 1\n #else\n-# define SANITIZER_SOLARIS 0\n+#  define SANITIZER_SOLARIS 0\n #endif\n \n #if defined(__APPLE__)\n-# define SANITIZER_MAC     1\n-# include <TargetConditionals.h>\n-# if TARGET_OS_OSX\n-#  define SANITIZER_OSX    1\n-# else\n-#  define SANITIZER_OSX    0\n-# endif\n-# if TARGET_OS_IPHONE\n-#  define SANITIZER_IOS    1\n-# else\n-#  define SANITIZER_IOS    0\n-# endif\n-# if TARGET_OS_SIMULATOR\n-#  define SANITIZER_IOSSIM 1\n-# else\n-#  define SANITIZER_IOSSIM 0\n-# endif\n+#  define SANITIZER_MAC 1\n+#  include <TargetConditionals.h>\n+#  if TARGET_OS_OSX\n+#    define SANITIZER_OSX 1\n+#  else\n+#    define SANITIZER_OSX 0\n+#  endif\n+#  if TARGET_OS_IPHONE\n+#    define SANITIZER_IOS 1\n+#  else\n+#    define SANITIZER_IOS 0\n+#  endif\n+#  if TARGET_OS_SIMULATOR\n+#    define SANITIZER_IOSSIM 1\n+#  else\n+#    define SANITIZER_IOSSIM 0\n+#  endif\n #else\n-# define SANITIZER_MAC     0\n-# define SANITIZER_IOS     0\n-# define SANITIZER_IOSSIM  0\n-# define SANITIZER_OSX     0\n+#  define SANITIZER_MAC 0\n+#  define SANITIZER_IOS 0\n+#  define SANITIZER_IOSSIM 0\n+#  define SANITIZER_OSX 0\n #endif\n \n #if defined(__APPLE__) && TARGET_OS_IPHONE && TARGET_OS_WATCH\n-# define SANITIZER_WATCHOS 1\n+#  define SANITIZER_WATCHOS 1\n #else\n-# define SANITIZER_WATCHOS 0\n+#  define SANITIZER_WATCHOS 0\n #endif\n \n #if defined(__APPLE__) && TARGET_OS_IPHONE && TARGET_OS_TV\n-# define SANITIZER_TVOS 1\n+#  define SANITIZER_TVOS 1\n #else\n-# define SANITIZER_TVOS 0\n+#  define SANITIZER_TVOS 0\n #endif\n \n #if defined(_WIN32)\n-# define SANITIZER_WINDOWS 1\n+#  define SANITIZER_WINDOWS 1\n #else\n-# define SANITIZER_WINDOWS 0\n+#  define SANITIZER_WINDOWS 0\n #endif\n \n #if defined(_WIN64)\n-# define SANITIZER_WINDOWS64 1\n+#  define SANITIZER_WINDOWS64 1\n #else\n-# define SANITIZER_WINDOWS64 0\n+#  define SANITIZER_WINDOWS64 0\n #endif\n \n #if defined(__ANDROID__)\n-# define SANITIZER_ANDROID 1\n+#  define SANITIZER_ANDROID 1\n #else\n-# define SANITIZER_ANDROID 0\n+#  define SANITIZER_ANDROID 0\n #endif\n \n #if defined(__Fuchsia__)\n-# define SANITIZER_FUCHSIA 1\n+#  define SANITIZER_FUCHSIA 1\n+#else\n+#  define SANITIZER_FUCHSIA 0\n+#endif\n+\n+// Assume linux that is not glibc or android is musl libc.\n+#if SANITIZER_LINUX && !SANITIZER_GLIBC && !SANITIZER_ANDROID\n+#  define SANITIZER_MUSL 1\n #else\n-# define SANITIZER_FUCHSIA 0\n+#  define SANITIZER_MUSL 0\n #endif\n \n-#define SANITIZER_POSIX \\\n+#define SANITIZER_POSIX                                     \\\n   (SANITIZER_FREEBSD || SANITIZER_LINUX || SANITIZER_MAC || \\\n-    SANITIZER_NETBSD || SANITIZER_SOLARIS)\n+   SANITIZER_NETBSD || SANITIZER_SOLARIS)\n \n #if __LP64__ || defined(_WIN64)\n #  define SANITIZER_WORDSIZE 64\n@@ -127,102 +134,114 @@\n #endif\n \n #if SANITIZER_WORDSIZE == 64\n-# define FIRST_32_SECOND_64(a, b) (b)\n+#  define FIRST_32_SECOND_64(a, b) (b)\n #else\n-# define FIRST_32_SECOND_64(a, b) (a)\n+#  define FIRST_32_SECOND_64(a, b) (a)\n #endif\n \n #if defined(__x86_64__) && !defined(_LP64)\n-# define SANITIZER_X32 1\n+#  define SANITIZER_X32 1\n #else\n-# define SANITIZER_X32 0\n+#  define SANITIZER_X32 0\n+#endif\n+\n+#if defined(__x86_64__) || defined(_M_X64)\n+#  define SANITIZER_X64 1\n+#else\n+#  define SANITIZER_X64 0\n #endif\n \n #if defined(__i386__) || defined(_M_IX86)\n-# define SANITIZER_I386 1\n+#  define SANITIZER_I386 1\n #else\n-# define SANITIZER_I386 0\n+#  define SANITIZER_I386 0\n #endif\n \n #if defined(__mips__)\n-# define SANITIZER_MIPS 1\n-# if defined(__mips64)\n+#  define SANITIZER_MIPS 1\n+#  if defined(__mips64)\n+#    define SANITIZER_MIPS32 0\n+#    define SANITIZER_MIPS64 1\n+#  else\n+#    define SANITIZER_MIPS32 1\n+#    define SANITIZER_MIPS64 0\n+#  endif\n+#else\n+#  define SANITIZER_MIPS 0\n #  define SANITIZER_MIPS32 0\n-#  define SANITIZER_MIPS64 1\n-# else\n-#  define SANITIZER_MIPS32 1\n #  define SANITIZER_MIPS64 0\n-# endif\n-#else\n-# define SANITIZER_MIPS 0\n-# define SANITIZER_MIPS32 0\n-# define SANITIZER_MIPS64 0\n #endif\n \n #if defined(__s390__)\n-# define SANITIZER_S390 1\n-# if defined(__s390x__)\n+#  define SANITIZER_S390 1\n+#  if defined(__s390x__)\n+#    define SANITIZER_S390_31 0\n+#    define SANITIZER_S390_64 1\n+#  else\n+#    define SANITIZER_S390_31 1\n+#    define SANITIZER_S390_64 0\n+#  endif\n+#else\n+#  define SANITIZER_S390 0\n #  define SANITIZER_S390_31 0\n-#  define SANITIZER_S390_64 1\n-# else\n-#  define SANITIZER_S390_31 1\n #  define SANITIZER_S390_64 0\n-# endif\n-#else\n-# define SANITIZER_S390 0\n-# define SANITIZER_S390_31 0\n-# define SANITIZER_S390_64 0\n #endif\n \n #if defined(__powerpc__)\n-# define SANITIZER_PPC 1\n-# if defined(__powerpc64__)\n-#  define SANITIZER_PPC32 0\n-#  define SANITIZER_PPC64 1\n+#  define SANITIZER_PPC 1\n+#  if defined(__powerpc64__)\n+#    define SANITIZER_PPC32 0\n+#    define SANITIZER_PPC64 1\n // 64-bit PPC has two ABIs (v1 and v2).  The old powerpc64 target is\n // big-endian, and uses v1 ABI (known for its function descriptors),\n // while the new powerpc64le target is little-endian and uses v2.\n // In theory, you could convince gcc to compile for their evil twins\n // (eg. big-endian v2), but you won't find such combinations in the wild\n // (it'd require bootstrapping a whole system, which would be quite painful\n // - there's no target triple for that).  LLVM doesn't support them either.\n-#  if _CALL_ELF == 2\n-#   define SANITIZER_PPC64V1 0\n-#   define SANITIZER_PPC64V2 1\n+#    if _CALL_ELF == 2\n+#      define SANITIZER_PPC64V1 0\n+#      define SANITIZER_PPC64V2 1\n+#    else\n+#      define SANITIZER_PPC64V1 1\n+#      define SANITIZER_PPC64V2 0\n+#    endif\n #  else\n-#   define SANITIZER_PPC64V1 1\n-#   define SANITIZER_PPC64V2 0\n+#    define SANITIZER_PPC32 1\n+#    define SANITIZER_PPC64 0\n+#    define SANITIZER_PPC64V1 0\n+#    define SANITIZER_PPC64V2 0\n #  endif\n-# else\n-#  define SANITIZER_PPC32 1\n+#else\n+#  define SANITIZER_PPC 0\n+#  define SANITIZER_PPC32 0\n #  define SANITIZER_PPC64 0\n #  define SANITIZER_PPC64V1 0\n #  define SANITIZER_PPC64V2 0\n-# endif\n+#endif\n+\n+#if defined(__arm__) || defined(_M_ARM)\n+#  define SANITIZER_ARM 1\n #else\n-# define SANITIZER_PPC 0\n-# define SANITIZER_PPC32 0\n-# define SANITIZER_PPC64 0\n-# define SANITIZER_PPC64V1 0\n-# define SANITIZER_PPC64V2 0\n+#  define SANITIZER_ARM 0\n #endif\n \n-#if defined(__arm__)\n-# define SANITIZER_ARM 1\n+#if defined(__aarch64__) || defined(_M_ARM64)\n+#  define SANITIZER_ARM64 1\n #else\n-# define SANITIZER_ARM 0\n+#  define SANITIZER_ARM64 0\n #endif\n \n #if SANITIZER_SOLARIS && SANITIZER_WORDSIZE == 32\n-# define SANITIZER_SOLARIS32 1\n+#  define SANITIZER_SOLARIS32 1\n #else\n-# define SANITIZER_SOLARIS32 0\n+#  define SANITIZER_SOLARIS32 0\n #endif\n \n #if defined(__riscv) && (__riscv_xlen == 64)\n-#define SANITIZER_RISCV64 1\n+#  define SANITIZER_RISCV64 1\n #else\n-#define SANITIZER_RISCV64 0\n+#  define SANITIZER_RISCV64 0\n #endif\n \n // By default we allow to use SizeClassAllocator64 on 64-bit platform.\n@@ -231,62 +250,52 @@\n // For such platforms build this code with -DSANITIZER_CAN_USE_ALLOCATOR64=0 or\n // change the definition of SANITIZER_CAN_USE_ALLOCATOR64 here.\n #ifndef SANITIZER_CAN_USE_ALLOCATOR64\n-# if (SANITIZER_ANDROID && defined(__aarch64__)) || SANITIZER_FUCHSIA\n-#  define SANITIZER_CAN_USE_ALLOCATOR64 1\n-# elif defined(__mips64) || defined(__aarch64__)\n-#  define SANITIZER_CAN_USE_ALLOCATOR64 0\n-# else\n-#  define SANITIZER_CAN_USE_ALLOCATOR64 (SANITIZER_WORDSIZE == 64)\n-# endif\n+#  if (SANITIZER_ANDROID && defined(__aarch64__)) || SANITIZER_FUCHSIA\n+#    define SANITIZER_CAN_USE_ALLOCATOR64 1\n+#  elif defined(__mips64) || defined(__aarch64__)\n+#    define SANITIZER_CAN_USE_ALLOCATOR64 0\n+#  else\n+#    define SANITIZER_CAN_USE_ALLOCATOR64 (SANITIZER_WORDSIZE == 64)\n+#  endif\n #endif\n \n // The range of addresses which can be returned my mmap.\n // FIXME: this value should be different on different platforms.  Larger values\n // will still work but will consume more memory for TwoLevelByteMap.\n #if defined(__mips__)\n-#if SANITIZER_GO && defined(__mips64)\n-#define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 47)\n-#else\n-# define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 40)\n-#endif\n+#  if SANITIZER_GO && defined(__mips64)\n+#    define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 47)\n+#  else\n+#    define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 40)\n+#  endif\n #elif SANITIZER_RISCV64\n-#define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 38)\n+#  define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 38)\n #elif defined(__aarch64__)\n-# if SANITIZER_MAC\n-#  if SANITIZER_OSX || SANITIZER_IOSSIM\n-#   define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 47)\n+#  if SANITIZER_MAC\n+#    if SANITIZER_OSX || SANITIZER_IOSSIM\n+#      define SANITIZER_MMAP_RANGE_SIZE \\\n+        FIRST_32_SECOND_64(1ULL << 32, 1ULL << 47)\n+#    else\n+// Darwin iOS/ARM64 has a 36-bit VMA, 64GiB VM\n+#      define SANITIZER_MMAP_RANGE_SIZE \\\n+        FIRST_32_SECOND_64(1ULL << 32, 1ULL << 36)\n+#    endif\n #  else\n-    // Darwin iOS/ARM64 has a 36-bit VMA, 64GiB VM\n-#   define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 36)\n+#    define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 48)\n #  endif\n-# else\n-#  define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 48)\n-# endif\n #elif defined(__sparc__)\n-#define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 52)\n+#  define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 52)\n #else\n-# define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 47)\n+#  define SANITIZER_MMAP_RANGE_SIZE FIRST_32_SECOND_64(1ULL << 32, 1ULL << 47)\n #endif\n \n // Whether the addresses are sign-extended from the VMA range to the word.\n // The SPARC64 Linux port implements this to split the VMA space into two\n // non-contiguous halves with a huge hole in the middle.\n #if defined(__sparc__) && SANITIZER_WORDSIZE == 64\n-#define SANITIZER_SIGN_EXTENDED_ADDRESSES 1\n+#  define SANITIZER_SIGN_EXTENDED_ADDRESSES 1\n #else\n-#define SANITIZER_SIGN_EXTENDED_ADDRESSES 0\n-#endif\n-\n-// The AArch64 and RISC-V linux ports use the canonical syscall set as\n-// mandated by the upstream linux community for all new ports. Other ports\n-// may still use legacy syscalls.\n-#ifndef SANITIZER_USES_CANONICAL_LINUX_SYSCALLS\n-#  if (defined(__aarch64__) || defined(__riscv) || defined(__hexagon__)) && \\\n-      SANITIZER_LINUX\n-#    define SANITIZER_USES_CANONICAL_LINUX_SYSCALLS 1\n-#  else\n-#    define SANITIZER_USES_CANONICAL_LINUX_SYSCALLS 0\n-#  endif\n+#  define SANITIZER_SIGN_EXTENDED_ADDRESSES 0\n #endif\n \n // udi16 syscalls can only be used when the following conditions are\n@@ -297,15 +306,15 @@\n // Since we don't want to include libc headers here, we check the\n // target only.\n #if defined(__arm__) || SANITIZER_X32 || defined(__sparc__)\n-#define SANITIZER_USES_UID16_SYSCALLS 1\n+#  define SANITIZER_USES_UID16_SYSCALLS 1\n #else\n-#define SANITIZER_USES_UID16_SYSCALLS 0\n+#  define SANITIZER_USES_UID16_SYSCALLS 0\n #endif\n \n #if defined(__mips__)\n-# define SANITIZER_POINTER_FORMAT_LENGTH FIRST_32_SECOND_64(8, 10)\n+#  define SANITIZER_POINTER_FORMAT_LENGTH FIRST_32_SECOND_64(8, 10)\n #else\n-# define SANITIZER_POINTER_FORMAT_LENGTH FIRST_32_SECOND_64(8, 12)\n+#  define SANITIZER_POINTER_FORMAT_LENGTH FIRST_32_SECOND_64(8, 12)\n #endif\n \n /// \\macro MSC_PREREQ\n@@ -314,68 +323,67 @@\n ///  * 1800: Microsoft Visual Studio 2013 / 12.0\n ///  * 1900: Microsoft Visual Studio 2015 / 14.0\n #ifdef _MSC_VER\n-# define MSC_PREREQ(version) (_MSC_VER >= (version))\n+#  define MSC_PREREQ(version) (_MSC_VER >= (version))\n #else\n-# define MSC_PREREQ(version) 0\n+#  define MSC_PREREQ(version) 0\n #endif\n \n-#if SANITIZER_MAC && !(defined(__arm64__) && SANITIZER_IOS)\n-# define SANITIZER_NON_UNIQUE_TYPEINFO 0\n+#if SANITIZER_MAC && defined(__x86_64__)\n+#  define SANITIZER_NON_UNIQUE_TYPEINFO 0\n #else\n-# define SANITIZER_NON_UNIQUE_TYPEINFO 1\n+#  define SANITIZER_NON_UNIQUE_TYPEINFO 1\n #endif\n \n // On linux, some architectures had an ABI transition from 64-bit long double\n // (ie. same as double) to 128-bit long double.  On those, glibc symbols\n // involving long doubles come in two versions, and we need to pass the\n // correct one to dlvsym when intercepting them.\n #if SANITIZER_LINUX && (SANITIZER_S390 || SANITIZER_PPC32 || SANITIZER_PPC64V1)\n-#define SANITIZER_NLDBL_VERSION \"GLIBC_2.4\"\n+#  define SANITIZER_NLDBL_VERSION \"GLIBC_2.4\"\n #endif\n \n #if SANITIZER_GO == 0\n-# define SANITIZER_GO 0\n+#  define SANITIZER_GO 0\n #endif\n \n // On PowerPC and ARM Thumb, calling pthread_exit() causes LSan to detect leaks.\n // pthread_exit() performs unwinding that leads to dlopen'ing libgcc_s.so.\n // dlopen mallocs \"libgcc_s.so\" string which confuses LSan, it fails to realize\n // that this allocation happens in dynamic linker and should be ignored.\n #if SANITIZER_PPC || defined(__thumb__)\n-# define SANITIZER_SUPPRESS_LEAK_ON_PTHREAD_EXIT 1\n+#  define SANITIZER_SUPPRESS_LEAK_ON_PTHREAD_EXIT 1\n #else\n-# define SANITIZER_SUPPRESS_LEAK_ON_PTHREAD_EXIT 0\n+#  define SANITIZER_SUPPRESS_LEAK_ON_PTHREAD_EXIT 0\n #endif\n \n-#if SANITIZER_FREEBSD || SANITIZER_MAC || SANITIZER_NETBSD || \\\n-  SANITIZER_SOLARIS\n-# define SANITIZER_MADVISE_DONTNEED MADV_FREE\n+#if SANITIZER_FREEBSD || SANITIZER_MAC || SANITIZER_NETBSD || SANITIZER_SOLARIS\n+#  define SANITIZER_MADVISE_DONTNEED MADV_FREE\n #else\n-# define SANITIZER_MADVISE_DONTNEED MADV_DONTNEED\n+#  define SANITIZER_MADVISE_DONTNEED MADV_DONTNEED\n #endif\n \n // Older gcc have issues aligning to a constexpr, and require an integer.\n // See https://gcc.gnu.org/bugzilla/show_bug.cgi?id=56859 among others.\n #if defined(__powerpc__) || defined(__powerpc64__)\n-# define SANITIZER_CACHE_LINE_SIZE 128\n+#  define SANITIZER_CACHE_LINE_SIZE 128\n #else\n-# define SANITIZER_CACHE_LINE_SIZE 64\n+#  define SANITIZER_CACHE_LINE_SIZE 64\n #endif\n \n // Enable offline markup symbolizer for Fuchsia.\n #if SANITIZER_FUCHSIA\n #  define SANITIZER_SYMBOLIZER_MARKUP 1\n #else\n-#define SANITIZER_SYMBOLIZER_MARKUP 0\n+#  define SANITIZER_SYMBOLIZER_MARKUP 0\n #endif\n \n // Enable ability to support sanitizer initialization that is\n // compatible with the sanitizer library being loaded via\n // `dlopen()`.\n #if SANITIZER_MAC\n-#define SANITIZER_SUPPORTS_INIT_FOR_DLOPEN 1\n+#  define SANITIZER_SUPPORTS_INIT_FOR_DLOPEN 1\n #else\n-#define SANITIZER_SUPPORTS_INIT_FOR_DLOPEN 0\n+#  define SANITIZER_SUPPORTS_INIT_FOR_DLOPEN 0\n #endif\n \n // SANITIZER_SUPPORTS_THREADLOCAL\n@@ -392,4 +400,15 @@\n #  endif\n #endif\n \n-#endif // SANITIZER_PLATFORM_H\n+#if defined(__thumb__) && defined(__linux__)\n+// Workaround for\n+// https://lab.llvm.org/buildbot/#/builders/clang-thumbv7-full-2stage\n+// or\n+// https://lab.llvm.org/staging/#/builders/clang-thumbv7-full-2stage\n+// It fails *rss_limit_mb_test* without meaningful errors.\n+#  define SANITIZER_START_BACKGROUND_THREAD_IN_ASAN_INTERNAL 1\n+#else\n+#  define SANITIZER_START_BACKGROUND_THREAD_IN_ASAN_INTERNAL 0\n+#endif\n+\n+#endif  // SANITIZER_PLATFORM_H"}, {"sha": "3cbbead4e98f7f485ca6ffab1fe29dedb90f265f", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_interceptors.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_interceptors.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_interceptors.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_interceptors.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -235,6 +235,7 @@\n #define SANITIZER_INTERCEPT_TIME SI_POSIX\n #define SANITIZER_INTERCEPT_GLOB (SI_GLIBC || SI_SOLARIS)\n #define SANITIZER_INTERCEPT_GLOB64 SI_GLIBC\n+#define SANITIZER_INTERCEPT___B64_TO SI_LINUX_NOT_ANDROID\n #define SANITIZER_INTERCEPT_POSIX_SPAWN SI_POSIX\n #define SANITIZER_INTERCEPT_WAIT SI_POSIX\n #define SANITIZER_INTERCEPT_INET SI_POSIX\n@@ -465,6 +466,7 @@\n #define SANITIZER_INTERCEPT_STAT                                        \\\n   (SI_FREEBSD || SI_MAC || SI_ANDROID || SI_NETBSD || SI_SOLARIS ||     \\\n    SI_STAT_LINUX)\n+#define SANITIZER_INTERCEPT_STAT64 SI_STAT_LINUX\n #define SANITIZER_INTERCEPT_LSTAT (SI_NETBSD || SI_FREEBSD || SI_STAT_LINUX)\n #define SANITIZER_INTERCEPT___XSTAT \\\n   ((!SANITIZER_INTERCEPT_STAT && SI_POSIX) || SI_STAT_LINUX)"}, {"sha": "0d25fa80e2ed8b9bb528226f1fd091f7b89070b6", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_freebsd.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_freebsd.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_freebsd.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_freebsd.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -130,7 +130,7 @@ unsigned struct_sigevent_sz = sizeof(struct sigevent);\n unsigned struct_sched_param_sz = sizeof(struct sched_param);\n unsigned struct_statfs_sz = sizeof(struct statfs);\n unsigned struct_sockaddr_sz = sizeof(struct sockaddr);\n-unsigned ucontext_t_sz = sizeof(ucontext_t);\n+unsigned ucontext_t_sz(void *ctx) { return sizeof(ucontext_t); }\n unsigned struct_rlimit_sz = sizeof(struct rlimit);\n unsigned struct_timespec_sz = sizeof(struct timespec);\n unsigned struct_utimbuf_sz = sizeof(struct utimbuf);"}, {"sha": "9859c52ec69f35512ea30a5618bb5f2f907aa88a", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_freebsd.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_freebsd.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_freebsd.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_freebsd.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -57,7 +57,7 @@ extern unsigned struct_sched_param_sz;\n extern unsigned struct_statfs64_sz;\n extern unsigned struct_statfs_sz;\n extern unsigned struct_sockaddr_sz;\n-extern unsigned ucontext_t_sz;\n+unsigned ucontext_t_sz(void *ctx);\n extern unsigned struct_rlimit_sz;\n extern unsigned struct_utimbuf_sz;\n extern unsigned struct_timespec_sz;"}, {"sha": "9d577570ea1e2e219b4a08c64f450794d6a145d7", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cpp", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_linux.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_linux.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_linux.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -26,10 +26,7 @@\n \n // With old kernels (and even new kernels on powerpc) asm/stat.h uses types that\n // are not defined anywhere in userspace headers. Fake them. This seems to work\n-// fine with newer headers, too.  Beware that with <sys/stat.h>, struct stat\n-// takes the form of struct stat64 on 32-bit platforms if _FILE_OFFSET_BITS=64.\n-// Also, for some platforms (e.g. mips) there are additional members in the\n-// <sys/stat.h> struct stat:s.\n+// fine with newer headers, too.\n #include <linux/posix_types.h>\n #  if defined(__x86_64__) || defined(__mips__) || defined(__hexagon__)\n #    include <sys/stat.h>"}, {"sha": "648e502b904a55d3b187476646d53b0f9e14f1e0", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_netbsd.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_netbsd.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_netbsd.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_netbsd.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -554,7 +554,7 @@ unsigned struct_tms_sz = sizeof(struct tms);\n unsigned struct_sigevent_sz = sizeof(struct sigevent);\n unsigned struct_sched_param_sz = sizeof(struct sched_param);\n unsigned struct_sockaddr_sz = sizeof(struct sockaddr);\n-unsigned ucontext_t_sz = sizeof(ucontext_t);\n+unsigned ucontext_t_sz(void *ctx) { return sizeof(ucontext_t); }\n unsigned struct_rlimit_sz = sizeof(struct rlimit);\n unsigned struct_timespec_sz = sizeof(struct timespec);\n unsigned struct_sembuf_sz = sizeof(struct sembuf);"}, {"sha": "dc6eb59b2800e14cf49371edbdd99c1ee7dd0243", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_netbsd.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_netbsd.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_netbsd.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_netbsd.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -45,7 +45,7 @@ extern unsigned struct_stack_t_sz;\n extern unsigned struct_sched_param_sz;\n extern unsigned struct_statfs_sz;\n extern unsigned struct_sockaddr_sz;\n-extern unsigned ucontext_t_sz;\n+unsigned ucontext_t_sz(void *ctx);\n \n extern unsigned struct_rlimit_sz;\n extern unsigned struct_utimbuf_sz;"}, {"sha": "e5cecaaaffc2ec04fef1b43750fcdf782006e79e", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_posix.cpp", "status": "modified", "additions": 28, "deletions": 5, "changes": 33, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_posix.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_posix.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_posix.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -170,8 +170,9 @@ typedef struct user_fpregs elf_fpregset_t;\n #endif\n \n // Include these after system headers to avoid name clashes and ambiguities.\n-#include \"sanitizer_internal_defs.h\"\n-#include \"sanitizer_platform_limits_posix.h\"\n+#  include \"sanitizer_common.h\"\n+#  include \"sanitizer_internal_defs.h\"\n+#  include \"sanitizer_platform_limits_posix.h\"\n \n namespace __sanitizer {\n   unsigned struct_utsname_sz = sizeof(struct utsname);\n@@ -214,10 +215,24 @@ namespace __sanitizer {\n #if !SANITIZER_ANDROID\n   unsigned struct_statfs_sz = sizeof(struct statfs);\n   unsigned struct_sockaddr_sz = sizeof(struct sockaddr);\n-  unsigned ucontext_t_sz = sizeof(ucontext_t);\n-#endif // !SANITIZER_ANDROID\n \n-#if SANITIZER_LINUX\n+  unsigned ucontext_t_sz(void *ctx) {\n+#    if SANITIZER_GLIBC && SANITIZER_X64\n+    // See kernel arch/x86/kernel/fpu/signal.c for details.\n+    const auto *fpregs = static_cast<ucontext_t *>(ctx)->uc_mcontext.fpregs;\n+    // The member names differ across header versions, but the actual layout\n+    // is always the same.  So avoid using members, just use arithmetic.\n+    const uint32_t *after_xmm =\n+        reinterpret_cast<const uint32_t *>(fpregs + 1) - 24;\n+    if (after_xmm[12] == FP_XSTATE_MAGIC1)\n+      return reinterpret_cast<const char *>(fpregs) + after_xmm[13] -\n+             static_cast<const char *>(ctx);\n+#    endif\n+    return sizeof(ucontext_t);\n+  }\n+#  endif  // !SANITIZER_ANDROID\n+\n+#  if SANITIZER_LINUX\n   unsigned struct_epoll_event_sz = sizeof(struct epoll_event);\n   unsigned struct_sysinfo_sz = sizeof(struct sysinfo);\n   unsigned __user_cap_header_struct_sz =\n@@ -575,6 +590,14 @@ unsigned struct_ElfW_Phdr_sz = sizeof(Elf_Phdr);\n   unsigned IOCTL_BLKROGET = BLKROGET;\n   unsigned IOCTL_BLKROSET = BLKROSET;\n   unsigned IOCTL_BLKRRPART = BLKRRPART;\n+  unsigned IOCTL_BLKFRASET = BLKFRASET;\n+  unsigned IOCTL_BLKFRAGET = BLKFRAGET;\n+  unsigned IOCTL_BLKSECTSET = BLKSECTSET;\n+  unsigned IOCTL_BLKSECTGET = BLKSECTGET;\n+  unsigned IOCTL_BLKSSZGET = BLKSSZGET;\n+  unsigned IOCTL_BLKBSZGET = BLKBSZGET;\n+  unsigned IOCTL_BLKBSZSET = BLKBSZSET;\n+  unsigned IOCTL_BLKGETSIZE64 = BLKGETSIZE64;\n   unsigned IOCTL_CDROMAUDIOBUFSIZ = CDROMAUDIOBUFSIZ;\n   unsigned IOCTL_CDROMEJECT = CDROMEJECT;\n   unsigned IOCTL_CDROMEJECT_SW = CDROMEJECT_SW;"}, {"sha": "62a99035db31b5aab83f125cfa9e64d05f29a0e0", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_posix.h", "status": "modified", "additions": 17, "deletions": 7, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_posix.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_posix.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_posix.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -57,12 +57,12 @@ extern unsigned struct_regmatch_sz;\n extern unsigned struct_fstab_sz;\n extern unsigned struct_statfs_sz;\n extern unsigned struct_sockaddr_sz;\n-extern unsigned ucontext_t_sz;\n-#endif // !SANITIZER_ANDROID\n+unsigned ucontext_t_sz(void *uctx);\n+#  endif  // !SANITIZER_ANDROID\n \n-#if SANITIZER_LINUX\n+#  if SANITIZER_LINUX\n \n-#if defined(__x86_64__)\n+#    if defined(__x86_64__)\n const unsigned struct_kernel_stat_sz = 144;\n const unsigned struct_kernel_stat64_sz = 0;\n #elif defined(__i386__)\n@@ -83,7 +83,7 @@ const unsigned struct_kernel_stat64_sz = 104;\n #elif defined(__mips__)\n const unsigned struct_kernel_stat_sz = SANITIZER_ANDROID\n                                            ? FIRST_32_SECOND_64(104, 128)\n-                                           : FIRST_32_SECOND_64(144, 216);\n+                                           : FIRST_32_SECOND_64(160, 216);\n const unsigned struct_kernel_stat64_sz = 104;\n #elif defined(__s390__) && !defined(__s390x__)\n const unsigned struct_kernel_stat_sz = 64;\n@@ -370,7 +370,8 @@ struct __sanitizer_group {\n   char **gr_mem;\n };\n \n-#  if (defined(__x86_64__) && !defined(_LP64)) || defined(__hexagon__)\n+#  if (SANITIZER_LINUX && !SANITIZER_GLIBC && !SANITIZER_ANDROID) || \\\n+      (defined(__x86_64__) && !defined(_LP64)) || defined(__hexagon__)\n typedef long long __sanitizer_time_t;\n #else\n typedef long __sanitizer_time_t;\n@@ -478,7 +479,8 @@ struct __sanitizer_dirent {\n   unsigned short d_reclen;\n   // more fields that we don't care about\n };\n-#  elif SANITIZER_ANDROID || defined(__x86_64__) || defined(__hexagon__)\n+#  elif (SANITIZER_LINUX && !SANITIZER_GLIBC) || defined(__x86_64__) || \\\n+      defined(__hexagon__)\n struct __sanitizer_dirent {\n   unsigned long long d_ino;\n   unsigned long long d_off;\n@@ -1108,6 +1110,14 @@ extern unsigned IOCTL_BLKRASET;\n extern unsigned IOCTL_BLKROGET;\n extern unsigned IOCTL_BLKROSET;\n extern unsigned IOCTL_BLKRRPART;\n+extern unsigned IOCTL_BLKFRASET;\n+extern unsigned IOCTL_BLKFRAGET;\n+extern unsigned IOCTL_BLKSECTSET;\n+extern unsigned IOCTL_BLKSECTGET;\n+extern unsigned IOCTL_BLKSSZGET;\n+extern unsigned IOCTL_BLKBSZGET;\n+extern unsigned IOCTL_BLKBSZSET;\n+extern unsigned IOCTL_BLKGETSIZE64;\n extern unsigned IOCTL_CDROMAUDIOBUFSIZ;\n extern unsigned IOCTL_CDROMEJECT;\n extern unsigned IOCTL_CDROMEJECT_SW;"}, {"sha": "dad7bde1498a7a969d6276903aa4428d425aaa75", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_solaris.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_solaris.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_solaris.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_solaris.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -89,7 +89,7 @@ namespace __sanitizer {\n   unsigned struct_sched_param_sz = sizeof(struct sched_param);\n   unsigned struct_statfs_sz = sizeof(struct statfs);\n   unsigned struct_sockaddr_sz = sizeof(struct sockaddr);\n-  unsigned ucontext_t_sz = sizeof(ucontext_t);\n+  unsigned ucontext_t_sz(void *ctx) { return sizeof(ucontext_t); }\n   unsigned struct_timespec_sz = sizeof(struct timespec);\n #if SANITIZER_SOLARIS32\n   unsigned struct_statvfs64_sz = sizeof(struct statvfs64);"}, {"sha": "84a81265162c65f990fc2f04d640a13be64e0115", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_solaris.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_solaris.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_solaris.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_solaris.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -43,7 +43,7 @@ extern unsigned struct_sched_param_sz;\n extern unsigned struct_statfs64_sz;\n extern unsigned struct_statfs_sz;\n extern unsigned struct_sockaddr_sz;\n-extern unsigned ucontext_t_sz;\n+unsigned ucontext_t_sz(void *ctx);\n \n extern unsigned struct_timespec_sz;\n extern unsigned struct_rlimit_sz;"}, {"sha": "3b330a3705e220507e6633d88f633c2066ecd81b", "filename": "libsanitizer/sanitizer_common/sanitizer_posix.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_posix.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_posix.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_posix.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -95,6 +95,7 @@ void *MmapAlignedOrDieOnFatalError(uptr size, uptr alignment,\n     UnmapOrDie((void*)map_res, res - map_res);\n   }\n   uptr end = res + size;\n+  end = RoundUpTo(end, GetPageSizeCached());\n   if (end != map_end)\n     UnmapOrDie((void*)end, map_end - end);\n   return (void*)res;"}, {"sha": "b6d8c7281bd43f35879fce20ff1fa0f01e3873ec", "filename": "libsanitizer/sanitizer_common/sanitizer_posix_libcdep.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_posix_libcdep.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_posix_libcdep.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_posix_libcdep.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -290,7 +290,7 @@ bool IsAccessibleMemoryRange(uptr beg, uptr size) {\n   return result;\n }\n \n-void PlatformPrepareForSandboxing(__sanitizer_sandbox_arguments *args) {\n+void PlatformPrepareForSandboxing(void *args) {\n   // Some kinds of sandboxes may forbid filesystem access, so we won't be able\n   // to read the file mappings from /proc/self/maps. Luckily, neither the\n   // process will be able to load additional libraries, so it's fine to use the"}, {"sha": "3a9e366d2df952a131634f79bea5e08841faa585", "filename": "libsanitizer/sanitizer_common/sanitizer_printf.cpp", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_printf.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_printf.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_printf.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -191,30 +191,30 @@ int VSNPrintf(char *buff, int buff_length,\n         break;\n       }\n       case 'p': {\n-        RAW_CHECK(!have_flags, kPrintfFormatsHelp, format);\n+        RAW_CHECK_VA(!have_flags, kPrintfFormatsHelp, format);\n         result += AppendPointer(&buff, buff_end, va_arg(args, uptr));\n         break;\n       }\n       case 's': {\n-        RAW_CHECK(!have_length, kPrintfFormatsHelp, format);\n+        RAW_CHECK_VA(!have_length, kPrintfFormatsHelp, format);\n         // Only left-justified width is supported.\n         CHECK(!have_width || left_justified);\n         result += AppendString(&buff, buff_end, left_justified ? -width : width,\n                                precision, va_arg(args, char*));\n         break;\n       }\n       case 'c': {\n-        RAW_CHECK(!have_flags, kPrintfFormatsHelp, format);\n+        RAW_CHECK_VA(!have_flags, kPrintfFormatsHelp, format);\n         result += AppendChar(&buff, buff_end, va_arg(args, int));\n         break;\n       }\n       case '%' : {\n-        RAW_CHECK(!have_flags, kPrintfFormatsHelp, format);\n+        RAW_CHECK_VA(!have_flags, kPrintfFormatsHelp, format);\n         result += AppendChar(&buff, buff_end, '%');\n         break;\n       }\n       default: {\n-        RAW_CHECK(false, kPrintfFormatsHelp, format);\n+        RAW_CHECK_VA(false, kPrintfFormatsHelp, format);\n       }\n     }\n   }"}, {"sha": "62b2e5e032166d0d4e93bd0ec79c4afdbd9498fe", "filename": "libsanitizer/sanitizer_common/sanitizer_procmaps_mac.cpp", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_procmaps_mac.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_procmaps_mac.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_procmaps_mac.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -143,16 +143,16 @@ void MemoryMappingLayout::LoadFromCache() {\n // early in the process, when dyld is one of the only images loaded,\n // so it will be hit after only a few iterations.\n static mach_header *get_dyld_image_header() {\n-  unsigned depth = 1;\n-  vm_size_t size = 0;\n   vm_address_t address = 0;\n-  kern_return_t err = KERN_SUCCESS;\n-  mach_msg_type_number_t count = VM_REGION_SUBMAP_INFO_COUNT_64;\n \n   while (true) {\n+    vm_size_t size = 0;\n+    unsigned depth = 1;\n     struct vm_region_submap_info_64 info;\n-    err = vm_region_recurse_64(mach_task_self(), &address, &size, &depth,\n-                               (vm_region_info_t)&info, &count);\n+    mach_msg_type_number_t count = VM_REGION_SUBMAP_INFO_COUNT_64;\n+    kern_return_t err =\n+        vm_region_recurse_64(mach_task_self(), &address, &size, &depth,\n+                             (vm_region_info_t)&info, &count);\n     if (err != KERN_SUCCESS) return nullptr;\n \n     if (size >= sizeof(mach_header) && info.protection & kProtectionRead) {"}, {"sha": "4aa60548516666fdd31f147fded71334e01e4c2f", "filename": "libsanitizer/sanitizer_common/sanitizer_quarantine.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_quarantine.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_quarantine.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_quarantine.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -149,8 +149,8 @@ class Quarantine {\n   Cache cache_;\n   char pad2_[kCacheLineSize];\n \n-  void NOINLINE Recycle(uptr min_size, Callback cb) REQUIRES(recycle_mutex_)\n-      RELEASE(recycle_mutex_) {\n+  void NOINLINE Recycle(uptr min_size, Callback cb)\n+      SANITIZER_REQUIRES(recycle_mutex_) SANITIZER_RELEASE(recycle_mutex_) {\n     Cache tmp;\n     {\n       SpinMutexLock l(&cache_mutex_);"}, {"sha": "f22e40cac28409512b8b1541afe6c75aa1353787", "filename": "libsanitizer/sanitizer_common/sanitizer_ring_buffer.h", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_ring_buffer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_ring_buffer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_ring_buffer.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -86,23 +86,28 @@ class CompactRingBuffer {\n   // Lower bytes store the address of the next buffer element.\n   static constexpr int kPageSizeBits = 12;\n   static constexpr int kSizeShift = 56;\n+  static constexpr int kSizeBits = 64 - kSizeShift;\n   static constexpr uptr kNextMask = (1ULL << kSizeShift) - 1;\n \n   uptr GetStorageSize() const { return (long_ >> kSizeShift) << kPageSizeBits; }\n \n+  static uptr SignExtend(uptr x) { return ((sptr)x) << kSizeBits >> kSizeBits; }\n+\n   void Init(void *storage, uptr size) {\n     CHECK_EQ(sizeof(CompactRingBuffer<T>), sizeof(void *));\n     CHECK(IsPowerOfTwo(size));\n     CHECK_GE(size, 1 << kPageSizeBits);\n     CHECK_LE(size, 128 << kPageSizeBits);\n     CHECK_EQ(size % 4096, 0);\n     CHECK_EQ(size % sizeof(T), 0);\n-    CHECK_EQ((uptr)storage % (size * 2), 0);\n-    long_ = (uptr)storage | ((size >> kPageSizeBits) << kSizeShift);\n+    uptr st = (uptr)storage;\n+    CHECK_EQ(st % (size * 2), 0);\n+    CHECK_EQ(st, SignExtend(st & kNextMask));\n+    long_ = (st & kNextMask) | ((size >> kPageSizeBits) << kSizeShift);\n   }\n \n   void SetNext(const T *next) {\n-    long_ = (long_ & ~kNextMask) | (uptr)next;\n+    long_ = (long_ & ~kNextMask) | ((uptr)next & kNextMask);\n   }\n \n  public:\n@@ -119,7 +124,7 @@ class CompactRingBuffer {\n     SetNext((const T *)storage + Idx);\n   }\n \n-  T *Next() const { return (T *)(long_ & kNextMask); }\n+  T *Next() const { return (T *)(SignExtend(long_ & kNextMask)); }\n \n   void *StartOfStorage() const {\n     return (void *)((uptr)Next() & ~(GetStorageSize() - 1));"}, {"sha": "148470943b47b3503c071d7342e8dbf531e5effb", "filename": "libsanitizer/sanitizer_common/sanitizer_stack_store.cpp", "status": "added", "additions": 379, "deletions": 0, "changes": 379, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stack_store.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stack_store.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stack_store.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -0,0 +1,379 @@\n+//===-- sanitizer_stack_store.cpp -------------------------------*- C++ -*-===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#include \"sanitizer_stack_store.h\"\n+\n+#include \"sanitizer_atomic.h\"\n+#include \"sanitizer_common.h\"\n+#include \"sanitizer_internal_defs.h\"\n+#include \"sanitizer_leb128.h\"\n+#include \"sanitizer_lzw.h\"\n+#include \"sanitizer_placement_new.h\"\n+#include \"sanitizer_stacktrace.h\"\n+\n+namespace __sanitizer {\n+\n+namespace {\n+struct StackTraceHeader {\n+  static constexpr u32 kStackSizeBits = 8;\n+\n+  u8 size;\n+  u8 tag;\n+  explicit StackTraceHeader(const StackTrace &trace)\n+      : size(Min<uptr>(trace.size, (1u << 8) - 1)), tag(trace.tag) {\n+    CHECK_EQ(trace.tag, static_cast<uptr>(tag));\n+  }\n+  explicit StackTraceHeader(uptr h)\n+      : size(h & ((1 << kStackSizeBits) - 1)), tag(h >> kStackSizeBits) {}\n+\n+  uptr ToUptr() const {\n+    return static_cast<uptr>(size) | (static_cast<uptr>(tag) << kStackSizeBits);\n+  }\n+};\n+}  // namespace\n+\n+StackStore::Id StackStore::Store(const StackTrace &trace, uptr *pack) {\n+  if (!trace.size && !trace.tag)\n+    return 0;\n+  StackTraceHeader h(trace);\n+  uptr idx = 0;\n+  *pack = 0;\n+  uptr *stack_trace = Alloc(h.size + 1, &idx, pack);\n+  *stack_trace = h.ToUptr();\n+  internal_memcpy(stack_trace + 1, trace.trace, h.size * sizeof(uptr));\n+  *pack += blocks_[GetBlockIdx(idx)].Stored(h.size + 1);\n+  return OffsetToId(idx);\n+}\n+\n+StackTrace StackStore::Load(Id id) {\n+  if (!id)\n+    return {};\n+  uptr idx = IdToOffset(id);\n+  uptr block_idx = GetBlockIdx(idx);\n+  CHECK_LT(block_idx, ARRAY_SIZE(blocks_));\n+  const uptr *stack_trace = blocks_[block_idx].GetOrUnpack(this);\n+  if (!stack_trace)\n+    return {};\n+  stack_trace += GetInBlockIdx(idx);\n+  StackTraceHeader h(*stack_trace);\n+  return StackTrace(stack_trace + 1, h.size, h.tag);\n+}\n+\n+uptr StackStore::Allocated() const {\n+  return atomic_load_relaxed(&allocated_) + sizeof(*this);\n+}\n+\n+uptr *StackStore::Alloc(uptr count, uptr *idx, uptr *pack) {\n+  for (;;) {\n+    // Optimisic lock-free allocation, essentially try to bump the\n+    // total_frames_.\n+    uptr start = atomic_fetch_add(&total_frames_, count, memory_order_relaxed);\n+    uptr block_idx = GetBlockIdx(start);\n+    uptr last_idx = GetBlockIdx(start + count - 1);\n+    if (LIKELY(block_idx == last_idx)) {\n+      // Fits into the a single block.\n+      CHECK_LT(block_idx, ARRAY_SIZE(blocks_));\n+      *idx = start;\n+      return blocks_[block_idx].GetOrCreate(this) + GetInBlockIdx(start);\n+    }\n+\n+    // Retry. We can't use range allocated in two different blocks.\n+    CHECK_LE(count, kBlockSizeFrames);\n+    uptr in_first = kBlockSizeFrames - GetInBlockIdx(start);\n+    // Mark tail/head of these blocks as \"stored\".to avoid waiting before we can\n+    // Pack().\n+    *pack += blocks_[block_idx].Stored(in_first);\n+    *pack += blocks_[last_idx].Stored(count - in_first);\n+  }\n+}\n+\n+void *StackStore::Map(uptr size, const char *mem_type) {\n+  atomic_fetch_add(&allocated_, size, memory_order_relaxed);\n+  return MmapNoReserveOrDie(size, mem_type);\n+}\n+\n+void StackStore::Unmap(void *addr, uptr size) {\n+  atomic_fetch_sub(&allocated_, size, memory_order_relaxed);\n+  UnmapOrDie(addr, size);\n+}\n+\n+uptr StackStore::Pack(Compression type) {\n+  uptr res = 0;\n+  for (BlockInfo &b : blocks_) res += b.Pack(type, this);\n+  return res;\n+}\n+\n+void StackStore::LockAll() {\n+  for (BlockInfo &b : blocks_) b.Lock();\n+}\n+\n+void StackStore::UnlockAll() {\n+  for (BlockInfo &b : blocks_) b.Unlock();\n+}\n+\n+void StackStore::TestOnlyUnmap() {\n+  for (BlockInfo &b : blocks_) b.TestOnlyUnmap(this);\n+  internal_memset(this, 0, sizeof(*this));\n+}\n+\n+uptr *StackStore::BlockInfo::Get() const {\n+  // Idiomatic double-checked locking uses memory_order_acquire here. But\n+  // relaxed is fine for us, justification is similar to\n+  // TwoLevelMap::GetOrCreate.\n+  return reinterpret_cast<uptr *>(atomic_load_relaxed(&data_));\n+}\n+\n+uptr *StackStore::BlockInfo::Create(StackStore *store) {\n+  SpinMutexLock l(&mtx_);\n+  uptr *ptr = Get();\n+  if (!ptr) {\n+    ptr = reinterpret_cast<uptr *>(store->Map(kBlockSizeBytes, \"StackStore\"));\n+    atomic_store(&data_, reinterpret_cast<uptr>(ptr), memory_order_release);\n+  }\n+  return ptr;\n+}\n+\n+uptr *StackStore::BlockInfo::GetOrCreate(StackStore *store) {\n+  uptr *ptr = Get();\n+  if (LIKELY(ptr))\n+    return ptr;\n+  return Create(store);\n+}\n+\n+class SLeb128Encoder {\n+ public:\n+  SLeb128Encoder(u8 *begin, u8 *end) : begin(begin), end(end) {}\n+\n+  bool operator==(const SLeb128Encoder &other) const {\n+    return begin == other.begin;\n+  }\n+\n+  bool operator!=(const SLeb128Encoder &other) const {\n+    return begin != other.begin;\n+  }\n+\n+  SLeb128Encoder &operator=(uptr v) {\n+    sptr diff = v - previous;\n+    begin = EncodeSLEB128(diff, begin, end);\n+    previous = v;\n+    return *this;\n+  }\n+  SLeb128Encoder &operator*() { return *this; }\n+  SLeb128Encoder &operator++() { return *this; }\n+\n+  u8 *base() const { return begin; }\n+\n+ private:\n+  u8 *begin;\n+  u8 *end;\n+  uptr previous = 0;\n+};\n+\n+class SLeb128Decoder {\n+ public:\n+  SLeb128Decoder(const u8 *begin, const u8 *end) : begin(begin), end(end) {}\n+\n+  bool operator==(const SLeb128Decoder &other) const {\n+    return begin == other.begin;\n+  }\n+\n+  bool operator!=(const SLeb128Decoder &other) const {\n+    return begin != other.begin;\n+  }\n+\n+  uptr operator*() {\n+    sptr diff;\n+    begin = DecodeSLEB128(begin, end, &diff);\n+    previous += diff;\n+    return previous;\n+  }\n+  SLeb128Decoder &operator++() { return *this; }\n+\n+  SLeb128Decoder operator++(int) { return *this; }\n+\n+ private:\n+  const u8 *begin;\n+  const u8 *end;\n+  uptr previous = 0;\n+};\n+\n+static u8 *CompressDelta(const uptr *from, const uptr *from_end, u8 *to,\n+                         u8 *to_end) {\n+  SLeb128Encoder encoder(to, to_end);\n+  for (; from != from_end; ++from, ++encoder) *encoder = *from;\n+  return encoder.base();\n+}\n+\n+static uptr *UncompressDelta(const u8 *from, const u8 *from_end, uptr *to,\n+                             uptr *to_end) {\n+  SLeb128Decoder decoder(from, from_end);\n+  SLeb128Decoder end(from_end, from_end);\n+  for (; decoder != end; ++to, ++decoder) *to = *decoder;\n+  CHECK_EQ(to, to_end);\n+  return to;\n+}\n+\n+static u8 *CompressLzw(const uptr *from, const uptr *from_end, u8 *to,\n+                       u8 *to_end) {\n+  SLeb128Encoder encoder(to, to_end);\n+  encoder = LzwEncode<uptr>(from, from_end, encoder);\n+  return encoder.base();\n+}\n+\n+static uptr *UncompressLzw(const u8 *from, const u8 *from_end, uptr *to,\n+                           uptr *to_end) {\n+  SLeb128Decoder decoder(from, from_end);\n+  SLeb128Decoder end(from_end, from_end);\n+  to = LzwDecode<uptr>(decoder, end, to);\n+  CHECK_EQ(to, to_end);\n+  return to;\n+}\n+\n+#if defined(_MSC_VER) && !defined(__clang__)\n+#  pragma warning(push)\n+// Disable 'nonstandard extension used: zero-sized array in struct/union'.\n+#  pragma warning(disable : 4200)\n+#endif\n+namespace {\n+struct PackedHeader {\n+  uptr size;\n+  StackStore::Compression type;\n+  u8 data[];\n+};\n+}  // namespace\n+#if defined(_MSC_VER) && !defined(__clang__)\n+#  pragma warning(pop)\n+#endif\n+\n+uptr *StackStore::BlockInfo::GetOrUnpack(StackStore *store) {\n+  SpinMutexLock l(&mtx_);\n+  switch (state) {\n+    case State::Storing:\n+      state = State::Unpacked;\n+      FALLTHROUGH;\n+    case State::Unpacked:\n+      return Get();\n+    case State::Packed:\n+      break;\n+  }\n+\n+  u8 *ptr = reinterpret_cast<u8 *>(Get());\n+  CHECK_NE(nullptr, ptr);\n+  const PackedHeader *header = reinterpret_cast<const PackedHeader *>(ptr);\n+  CHECK_LE(header->size, kBlockSizeBytes);\n+  CHECK_GE(header->size, sizeof(PackedHeader));\n+\n+  uptr packed_size_aligned = RoundUpTo(header->size, GetPageSizeCached());\n+\n+  uptr *unpacked =\n+      reinterpret_cast<uptr *>(store->Map(kBlockSizeBytes, \"StackStoreUnpack\"));\n+\n+  uptr *unpacked_end;\n+  switch (header->type) {\n+    case Compression::Delta:\n+      unpacked_end = UncompressDelta(header->data, ptr + header->size, unpacked,\n+                                     unpacked + kBlockSizeFrames);\n+      break;\n+    case Compression::LZW:\n+      unpacked_end = UncompressLzw(header->data, ptr + header->size, unpacked,\n+                                   unpacked + kBlockSizeFrames);\n+      break;\n+    default:\n+      UNREACHABLE(\"Unexpected type\");\n+      break;\n+  }\n+\n+  CHECK_EQ(kBlockSizeFrames, unpacked_end - unpacked);\n+\n+  MprotectReadOnly(reinterpret_cast<uptr>(unpacked), kBlockSizeBytes);\n+  atomic_store(&data_, reinterpret_cast<uptr>(unpacked), memory_order_release);\n+  store->Unmap(ptr, packed_size_aligned);\n+\n+  state = State::Unpacked;\n+  return Get();\n+}\n+\n+uptr StackStore::BlockInfo::Pack(Compression type, StackStore *store) {\n+  if (type == Compression::None)\n+    return 0;\n+\n+  SpinMutexLock l(&mtx_);\n+  switch (state) {\n+    case State::Unpacked:\n+    case State::Packed:\n+      return 0;\n+    case State::Storing:\n+      break;\n+  }\n+\n+  uptr *ptr = Get();\n+  if (!ptr || !Stored(0))\n+    return 0;\n+\n+  u8 *packed =\n+      reinterpret_cast<u8 *>(store->Map(kBlockSizeBytes, \"StackStorePack\"));\n+  PackedHeader *header = reinterpret_cast<PackedHeader *>(packed);\n+  u8 *alloc_end = packed + kBlockSizeBytes;\n+\n+  u8 *packed_end = nullptr;\n+  switch (type) {\n+    case Compression::Delta:\n+      packed_end =\n+          CompressDelta(ptr, ptr + kBlockSizeFrames, header->data, alloc_end);\n+      break;\n+    case Compression::LZW:\n+      packed_end =\n+          CompressLzw(ptr, ptr + kBlockSizeFrames, header->data, alloc_end);\n+      break;\n+    default:\n+      UNREACHABLE(\"Unexpected type\");\n+      break;\n+  }\n+\n+  header->type = type;\n+  header->size = packed_end - packed;\n+\n+  VPrintf(1, \"Packed block of %zu KiB to %zu KiB\\n\", kBlockSizeBytes >> 10,\n+          header->size >> 10);\n+\n+  if (kBlockSizeBytes - header->size < kBlockSizeBytes / 8) {\n+    VPrintf(1, \"Undo and keep block unpacked\\n\");\n+    MprotectReadOnly(reinterpret_cast<uptr>(ptr), kBlockSizeBytes);\n+    store->Unmap(packed, kBlockSizeBytes);\n+    state = State::Unpacked;\n+    return 0;\n+  }\n+\n+  uptr packed_size_aligned = RoundUpTo(header->size, GetPageSizeCached());\n+  store->Unmap(packed + packed_size_aligned,\n+               kBlockSizeBytes - packed_size_aligned);\n+  MprotectReadOnly(reinterpret_cast<uptr>(packed), packed_size_aligned);\n+\n+  atomic_store(&data_, reinterpret_cast<uptr>(packed), memory_order_release);\n+  store->Unmap(ptr, kBlockSizeBytes);\n+\n+  state = State::Packed;\n+  return kBlockSizeBytes - packed_size_aligned;\n+}\n+\n+void StackStore::BlockInfo::TestOnlyUnmap(StackStore *store) {\n+  if (uptr *ptr = Get())\n+    store->Unmap(ptr, kBlockSizeBytes);\n+}\n+\n+bool StackStore::BlockInfo::Stored(uptr n) {\n+  return n + atomic_fetch_add(&stored_, n, memory_order_release) ==\n+         kBlockSizeFrames;\n+}\n+\n+bool StackStore::BlockInfo::IsPacked() const {\n+  SpinMutexLock l(&mtx_);\n+  return state == State::Packed;\n+}\n+\n+}  // namespace __sanitizer"}, {"sha": "4f1a8caac6ed85c5aff8a5967c7e29df3b4d5ad3", "filename": "libsanitizer/sanitizer_common/sanitizer_stack_store.h", "status": "added", "additions": 121, "deletions": 0, "changes": 121, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stack_store.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stack_store.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stack_store.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -0,0 +1,121 @@\n+//===-- sanitizer_stack_store.h ---------------------------------*- C++ -*-===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef SANITIZER_STACK_STORE_H\n+#define SANITIZER_STACK_STORE_H\n+\n+#include \"sanitizer_atomic.h\"\n+#include \"sanitizer_common.h\"\n+#include \"sanitizer_internal_defs.h\"\n+#include \"sanitizer_mutex.h\"\n+#include \"sanitizer_stacktrace.h\"\n+\n+namespace __sanitizer {\n+\n+class StackStore {\n+  static constexpr uptr kBlockSizeFrames = 0x100000;\n+  static constexpr uptr kBlockCount = 0x1000;\n+  static constexpr uptr kBlockSizeBytes = kBlockSizeFrames * sizeof(uptr);\n+\n+ public:\n+  enum class Compression : u8 {\n+    None = 0,\n+    Delta,\n+    LZW,\n+  };\n+\n+  constexpr StackStore() = default;\n+\n+  using Id = u32;  // Enough for 2^32 * sizeof(uptr) bytes of traces.\n+  static_assert(u64(kBlockCount) * kBlockSizeFrames == 1ull << (sizeof(Id) * 8),\n+                \"\");\n+\n+  Id Store(const StackTrace &trace,\n+           uptr *pack /* number of blocks completed by this call */);\n+  StackTrace Load(Id id);\n+  uptr Allocated() const;\n+\n+  // Packs all blocks which don't expect any more writes. A block is going to be\n+  // packed once. As soon trace from that block was requested, it will unpack\n+  // and stay unpacked after that.\n+  // Returns the number of released bytes.\n+  uptr Pack(Compression type);\n+\n+  void LockAll();\n+  void UnlockAll();\n+\n+  void TestOnlyUnmap();\n+\n+ private:\n+  friend class StackStoreTest;\n+  static constexpr uptr GetBlockIdx(uptr frame_idx) {\n+    return frame_idx / kBlockSizeFrames;\n+  }\n+\n+  static constexpr uptr GetInBlockIdx(uptr frame_idx) {\n+    return frame_idx % kBlockSizeFrames;\n+  }\n+\n+  static constexpr uptr IdToOffset(Id id) {\n+    CHECK_NE(id, 0);\n+    return id - 1;  // Avoid zero as id.\n+  }\n+\n+  static constexpr uptr OffsetToId(Id id) {\n+    // This makes UINT32_MAX to 0 and it will be retrived as and empty stack.\n+    // But this is not a problem as we will not be able to store anything after\n+    // that anyway.\n+    return id + 1;  // Avoid zero as id.\n+  }\n+\n+  uptr *Alloc(uptr count, uptr *idx, uptr *pack);\n+\n+  void *Map(uptr size, const char *mem_type);\n+  void Unmap(void *addr, uptr size);\n+\n+  // Total number of allocated frames.\n+  atomic_uintptr_t total_frames_ = {};\n+\n+  // Tracks total allocated memory in bytes.\n+  atomic_uintptr_t allocated_ = {};\n+\n+  // Each block will hold pointer to exactly kBlockSizeFrames.\n+  class BlockInfo {\n+    atomic_uintptr_t data_;\n+    // Counter to track store progress to know when we can Pack() the block.\n+    atomic_uint32_t stored_;\n+    // Protects alloc of new blocks.\n+    mutable StaticSpinMutex mtx_;\n+\n+    enum class State : u8 {\n+      Storing = 0,\n+      Packed,\n+      Unpacked,\n+    };\n+    State state SANITIZER_GUARDED_BY(mtx_);\n+\n+    uptr *Create(StackStore *store);\n+\n+   public:\n+    uptr *Get() const;\n+    uptr *GetOrCreate(StackStore *store);\n+    uptr *GetOrUnpack(StackStore *store);\n+    uptr Pack(Compression type, StackStore *store);\n+    void TestOnlyUnmap(StackStore *store);\n+    bool Stored(uptr n);\n+    bool IsPacked() const;\n+    void Lock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS { mtx_.Lock(); }\n+    void Unlock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS { mtx_.Unlock(); }\n+  };\n+\n+  BlockInfo blocks_[kBlockCount] = {};\n+};\n+\n+}  // namespace __sanitizer\n+\n+#endif  // SANITIZER_STACK_STORE_H"}, {"sha": "a746d4621936c62555caa89efcf533804f4716d7", "filename": "libsanitizer/sanitizer_common/sanitizer_stackdepot.cpp", "status": "modified", "additions": 135, "deletions": 22, "changes": 157, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -12,22 +12,22 @@\n \n #include \"sanitizer_stackdepot.h\"\n \n+#include \"sanitizer_atomic.h\"\n #include \"sanitizer_common.h\"\n #include \"sanitizer_hash.h\"\n-#include \"sanitizer_persistent_allocator.h\"\n+#include \"sanitizer_mutex.h\"\n+#include \"sanitizer_stack_store.h\"\n #include \"sanitizer_stackdepotbase.h\"\n \n namespace __sanitizer {\n \n-static PersistentAllocator<uptr> traceAllocator;\n-\n struct StackDepotNode {\n   using hash_type = u64;\n   hash_type stack_hash;\n   u32 link;\n+  StackStore::Id store_id;\n \n   static const u32 kTabSizeLog = SANITIZER_ANDROID ? 16 : 20;\n-  static const u32 kStackSizeBits = 16;\n \n   typedef StackTrace args_type;\n   bool eq(hash_type hash, const args_type &args) const {\n@@ -50,14 +50,12 @@ struct StackDepotNode {\n   typedef StackDepotHandle handle_type;\n };\n \n+static StackStore stackStore;\n+\n // FIXME(dvyukov): this single reserved bit is used in TSan.\n typedef StackDepotBase<StackDepotNode, 1, StackDepotNode::kTabSizeLog>\n     StackDepot;\n static StackDepot theDepot;\n-// Keep rarely accessed stack traces out of frequently access nodes to improve\n-// caching efficiency.\n-static TwoLevelMap<uptr *, StackDepot::kNodesSize1, StackDepot::kNodesSize2>\n-    tracePtrs;\n // Keep mutable data out of frequently access nodes to improve caching\n // efficiency.\n static TwoLevelMap<atomic_uint32_t, StackDepot::kNodesSize1,\n@@ -73,26 +71,136 @@ void StackDepotHandle::inc_use_count_unsafe() {\n }\n \n uptr StackDepotNode::allocated() {\n-  return traceAllocator.allocated() + tracePtrs.MemoryUsage() +\n-         useCounts.MemoryUsage();\n+  return stackStore.Allocated() + useCounts.MemoryUsage();\n+}\n+\n+static void CompressStackStore() {\n+  u64 start = Verbosity() >= 1 ? MonotonicNanoTime() : 0;\n+  uptr diff = stackStore.Pack(static_cast<StackStore::Compression>(\n+      Abs(common_flags()->compress_stack_depot)));\n+  if (!diff)\n+    return;\n+  if (Verbosity() >= 1) {\n+    u64 finish = MonotonicNanoTime();\n+    uptr total_before = theDepot.GetStats().allocated + diff;\n+    VPrintf(1, \"%s: StackDepot released %zu KiB out of %zu KiB in %llu ms\\n\",\n+            SanitizerToolName, diff >> 10, total_before >> 10,\n+            (finish - start) / 1000000);\n+  }\n+}\n+\n+namespace {\n+\n+class CompressThread {\n+ public:\n+  constexpr CompressThread() = default;\n+  void NewWorkNotify();\n+  void Stop();\n+  void LockAndStop() SANITIZER_NO_THREAD_SAFETY_ANALYSIS;\n+  void Unlock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS;\n+\n+ private:\n+  enum class State {\n+    NotStarted = 0,\n+    Started,\n+    Failed,\n+    Stopped,\n+  };\n+\n+  void Run();\n+\n+  bool WaitForWork() {\n+    semaphore_.Wait();\n+    return atomic_load(&run_, memory_order_acquire);\n+  }\n+\n+  Semaphore semaphore_ = {};\n+  StaticSpinMutex mutex_ = {};\n+  State state_ SANITIZER_GUARDED_BY(mutex_) = State::NotStarted;\n+  void *thread_ SANITIZER_GUARDED_BY(mutex_) = nullptr;\n+  atomic_uint8_t run_ = {};\n+};\n+\n+static CompressThread compress_thread;\n+\n+void CompressThread::NewWorkNotify() {\n+  int compress = common_flags()->compress_stack_depot;\n+  if (!compress)\n+    return;\n+  if (compress > 0 /* for testing or debugging */) {\n+    SpinMutexLock l(&mutex_);\n+    if (state_ == State::NotStarted) {\n+      atomic_store(&run_, 1, memory_order_release);\n+      CHECK_EQ(nullptr, thread_);\n+      thread_ = internal_start_thread(\n+          [](void *arg) -> void * {\n+            reinterpret_cast<CompressThread *>(arg)->Run();\n+            return nullptr;\n+          },\n+          this);\n+      state_ = thread_ ? State::Started : State::Failed;\n+    }\n+    if (state_ == State::Started) {\n+      semaphore_.Post();\n+      return;\n+    }\n+  }\n+  CompressStackStore();\n+}\n+\n+void CompressThread::Run() {\n+  VPrintf(1, \"%s: StackDepot compression thread started\\n\", SanitizerToolName);\n+  while (WaitForWork()) CompressStackStore();\n+  VPrintf(1, \"%s: StackDepot compression thread stopped\\n\", SanitizerToolName);\n+}\n+\n+void CompressThread::Stop() {\n+  void *t = nullptr;\n+  {\n+    SpinMutexLock l(&mutex_);\n+    if (state_ != State::Started)\n+      return;\n+    state_ = State::Stopped;\n+    CHECK_NE(nullptr, thread_);\n+    t = thread_;\n+    thread_ = nullptr;\n+  }\n+  atomic_store(&run_, 0, memory_order_release);\n+  semaphore_.Post();\n+  internal_join_thread(t);\n }\n \n+void CompressThread::LockAndStop() {\n+  mutex_.Lock();\n+  if (state_ != State::Started)\n+    return;\n+  CHECK_NE(nullptr, thread_);\n+\n+  atomic_store(&run_, 0, memory_order_release);\n+  semaphore_.Post();\n+  internal_join_thread(thread_);\n+  // Allow to restart after Unlock() if needed.\n+  state_ = State::NotStarted;\n+  thread_ = nullptr;\n+}\n+\n+void CompressThread::Unlock() { mutex_.Unlock(); }\n+\n+}  // namespace\n+\n void StackDepotNode::store(u32 id, const args_type &args, hash_type hash) {\n   stack_hash = hash;\n-  uptr *stack_trace = traceAllocator.alloc(args.size + 1);\n-  CHECK_LT(args.size, 1 << kStackSizeBits);\n-  *stack_trace = args.size + (args.tag << kStackSizeBits);\n-  internal_memcpy(stack_trace + 1, args.trace, args.size * sizeof(uptr));\n-  tracePtrs[id] = stack_trace;\n+  uptr pack = 0;\n+  store_id = stackStore.Store(args, &pack);\n+  if (LIKELY(!pack))\n+    return;\n+  compress_thread.NewWorkNotify();\n }\n \n StackDepotNode::args_type StackDepotNode::load(u32 id) const {\n-  const uptr *stack_trace = tracePtrs[id];\n-  if (!stack_trace)\n+  if (!store_id)\n     return {};\n-  uptr size = *stack_trace & ((1 << kStackSizeBits) - 1);\n-  uptr tag = *stack_trace >> kStackSizeBits;\n-  return args_type(stack_trace + 1, size, tag);\n+  return stackStore.Load(store_id);\n }\n \n StackDepotStats StackDepotGetStats() { return theDepot.GetStats(); }\n@@ -109,9 +217,13 @@ StackTrace StackDepotGet(u32 id) {\n \n void StackDepotLockAll() {\n   theDepot.LockAll();\n+  compress_thread.LockAndStop();\n+  stackStore.LockAll();\n }\n \n void StackDepotUnlockAll() {\n+  stackStore.UnlockAll();\n+  compress_thread.Unlock();\n   theDepot.UnlockAll();\n }\n \n@@ -121,14 +233,15 @@ void StackDepotPrintAll() {\n #endif\n }\n \n+void StackDepotStopBackgroundThread() { compress_thread.Stop(); }\n+\n StackDepotHandle StackDepotNode::get_handle(u32 id) {\n   return StackDepotHandle(&theDepot.nodes[id], id);\n }\n \n void StackDepotTestOnlyUnmap() {\n   theDepot.TestOnlyUnmap();\n-  tracePtrs.TestOnlyUnmap();\n-  traceAllocator.TestOnlyUnmap();\n+  stackStore.TestOnlyUnmap();\n }\n \n } // namespace __sanitizer"}, {"sha": "cca6fd53468839c63d40e22e7b49ce182406c341", "filename": "libsanitizer/sanitizer_common/sanitizer_stackdepot.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -42,6 +42,7 @@ StackTrace StackDepotGet(u32 id);\n void StackDepotLockAll();\n void StackDepotUnlockAll();\n void StackDepotPrintAll();\n+void StackDepotStopBackgroundThread();\n \n void StackDepotTestOnlyUnmap();\n "}, {"sha": "3013a0c4abdf08872528d31b63ef13629cbd5492", "filename": "libsanitizer/sanitizer_common/sanitizer_stacktrace.cpp", "status": "modified", "additions": 11, "deletions": 17, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -20,11 +20,10 @@\n namespace __sanitizer {\n \n uptr StackTrace::GetNextInstructionPc(uptr pc) {\n-#if defined(__sparc__) || defined(__mips__)\n-  return pc + 8;\n-#elif defined(__powerpc__) || defined(__arm__) || defined(__aarch64__) || \\\n-    defined(__hexagon__)\n+#if defined(__aarch64__)\n   return STRIP_PAC_PC((void *)pc) + 4;\n+#elif defined(__sparc__) || defined(__mips__)\n+  return pc + 8;\n #elif SANITIZER_RISCV64\n   // Current check order is 4 -> 2 -> 6 -> 8\n   u8 InsnByte = *(u8 *)(pc);\n@@ -47,8 +46,10 @@ uptr StackTrace::GetNextInstructionPc(uptr pc) {\n   }\n   // bail-out if could not figure out the instruction size\n   return 0;\n-#else\n+#elif SANITIZER_S390 || SANITIZER_I386 || SANITIZER_X32 || SANITIZER_X64\n   return pc + 1;\n+#else\n+  return pc + 4;\n #endif\n }\n \n@@ -86,8 +87,8 @@ static inline uhwptr *GetCanonicFrame(uptr bp,\n   // Nope, this does not look right either. This means the frame after next does\n   // not have a valid frame pointer, but we can still extract the caller PC.\n   // Unfortunately, there is no way to decide between GCC and LLVM frame\n-  // layouts. Assume GCC.\n-  return bp_prev - 1;\n+  // layouts. Assume LLVM.\n+  return bp_prev;\n #else\n   return (uhwptr*)bp;\n #endif\n@@ -110,21 +111,14 @@ void BufferedStackTrace::UnwindFast(uptr pc, uptr bp, uptr stack_top,\n          IsAligned((uptr)frame, sizeof(*frame)) &&\n          size < max_depth) {\n #ifdef __powerpc__\n-    // PowerPC ABIs specify that the return address is saved on the\n-    // *caller's* stack frame.  Thus we must dereference the back chain\n-    // to find the caller frame before extracting it.\n+    // PowerPC ABIs specify that the return address is saved at offset\n+    // 16 of the *caller's* stack frame.  Thus we must dereference the\n+    // back chain to find the caller frame before extracting it.\n     uhwptr *caller_frame = (uhwptr*)frame[0];\n     if (!IsValidFrame((uptr)caller_frame, stack_top, bottom) ||\n         !IsAligned((uptr)caller_frame, sizeof(uhwptr)))\n       break;\n-    // For most ABIs the offset where the return address is saved is two\n-    // register sizes.  The exception is the SVR4 ABI, which uses an\n-    // offset of only one register size.\n-#ifdef _CALL_SYSV\n-    uhwptr pc1 = caller_frame[1];\n-#else\n     uhwptr pc1 = caller_frame[2];\n-#endif\n #elif defined(__s390__)\n     uhwptr pc1 = frame[14];\n #elif defined(__riscv)"}, {"sha": "9a5f8fb13a29d259e0e69468a5e8cd6040e51233", "filename": "libsanitizer/sanitizer_common/sanitizer_stacktrace.h", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -20,7 +20,7 @@ namespace __sanitizer {\n \n struct BufferedStackTrace;\n \n-static const u32 kStackTraceMax = 256;\n+static const u32 kStackTraceMax = 255;\n \n #if SANITIZER_LINUX && defined(__mips__)\n # define SANITIZER_CAN_FAST_UNWIND 0\n@@ -88,9 +88,6 @@ uptr StackTrace::GetPreviousInstructionPc(uptr pc) {\n   // so we return (pc-2) in that case in order to be safe.\n   // For A32 mode we return (pc-4) because all instructions are 32 bit long.\n   return (pc - 3) & (~1);\n-#elif defined(__powerpc__) || defined(__powerpc64__) || defined(__aarch64__)\n-  // PCs are always 4 byte aligned.\n-  return pc - 4;\n #elif defined(__sparc__) || defined(__mips__)\n   return pc - 8;\n #elif SANITIZER_RISCV64\n@@ -101,8 +98,10 @@ uptr StackTrace::GetPreviousInstructionPc(uptr pc) {\n   // It seems difficult to figure out the exact instruction length -\n   // pc - 2 seems like a safe option for the purposes of stack tracing\n   return pc - 2;\n-#else\n+#elif SANITIZER_S390 || SANITIZER_I386 || SANITIZER_X32 || SANITIZER_X64\n   return pc - 1;\n+#else\n+  return pc - 4;\n #endif\n }\n "}, {"sha": "47983ee7ec713f230d86f392a981aa51ec48d614", "filename": "libsanitizer/sanitizer_common/sanitizer_stacktrace_libcdep.cpp", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace_libcdep.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace_libcdep.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace_libcdep.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -166,8 +166,8 @@ void BufferedStackTrace::Unwind(u32 max_depth, uptr pc, uptr bp, void *context,\n   UnwindFast(pc, bp, stack_top, stack_bottom, max_depth);\n }\n \n-static int GetModuleAndOffsetForPc(uptr pc, char *module_name,\n-                                   uptr module_name_len, uptr *pc_offset) {\n+int GetModuleAndOffsetForPc(uptr pc, char *module_name, uptr module_name_len,\n+                            uptr *pc_offset) {\n   const char *found_module_name = nullptr;\n   bool ok = Symbolizer::GetOrInit()->GetModuleNameAndOffsetForPC(\n       pc, &found_module_name, pc_offset);\n@@ -216,10 +216,11 @@ void __sanitizer_symbolize_global(uptr data_addr, const char *fmt,\n }\n \n SANITIZER_INTERFACE_ATTRIBUTE\n-int __sanitizer_get_module_and_offset_for_pc(uptr pc, char *module_name,\n+int __sanitizer_get_module_and_offset_for_pc(void *pc, char *module_name,\n                                              uptr module_name_len,\n-                                             uptr *pc_offset) {\n-  return __sanitizer::GetModuleAndOffsetForPc(pc, module_name, module_name_len,\n-                                              pc_offset);\n+                                             void **pc_offset) {\n+  return __sanitizer::GetModuleAndOffsetForPc(\n+      reinterpret_cast<uptr>(pc), module_name, module_name_len,\n+      reinterpret_cast<uptr *>(pc_offset));\n }\n }  // extern \"C\""}, {"sha": "2d0eccc1602ab3f398fced45f1329515161c02dd", "filename": "libsanitizer/sanitizer_common/sanitizer_stacktrace_printer.cpp", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace_printer.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace_printer.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace_printer.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -104,6 +104,19 @@ static const char *DemangleFunctionName(const char *function) {\n   return function;\n }\n \n+static void MaybeBuildIdToBuffer(const AddressInfo &info, bool PrefixSpace,\n+                                 InternalScopedString *buffer) {\n+  if (info.uuid_size) {\n+    if (PrefixSpace)\n+      buffer->append(\" \");\n+    buffer->append(\"(BuildId: \");\n+    for (uptr i = 0; i < info.uuid_size; ++i) {\n+      buffer->append(\"%02x\", info.uuid[i]);\n+    }\n+    buffer->append(\")\");\n+  }\n+}\n+\n static const char kDefaultFormat[] = \"    #%n %p %F %L\";\n \n void RenderFrame(InternalScopedString *buffer, const char *format, int frame_no,\n@@ -140,6 +153,9 @@ void RenderFrame(InternalScopedString *buffer, const char *format, int frame_no,\n     case 'o':\n       buffer->append(\"0x%zx\", info->module_offset);\n       break;\n+    case 'b':\n+      MaybeBuildIdToBuffer(*info, /*PrefixSpace=*/false, buffer);\n+      break;\n     case 'f':\n       buffer->append(\"%s\", DemangleFunctionName(StripFunctionName(\n                                info->function, strip_func_prefix)));\n@@ -181,6 +197,8 @@ void RenderFrame(InternalScopedString *buffer, const char *format, int frame_no,\n       } else if (info->module) {\n         RenderModuleLocation(buffer, info->module, info->module_offset,\n                              info->module_arch, strip_path_prefix);\n+\n+        MaybeBuildIdToBuffer(*info, /*PrefixSpace=*/true, buffer);\n       } else {\n         buffer->append(\"(<unknown module>)\");\n       }\n@@ -193,6 +211,7 @@ void RenderFrame(InternalScopedString *buffer, const char *format, int frame_no,\n         // Always strip the module name for %M.\n         RenderModuleLocation(buffer, StripModuleName(info->module),\n                              info->module_offset, info->module_arch, \"\");\n+        MaybeBuildIdToBuffer(*info, /*PrefixSpace=*/true, buffer);\n       } else {\n         buffer->append(\"(%p)\", (void *)address);\n       }"}, {"sha": "f114acea79c9c6bac8e924ea7618623eea02c9a0", "filename": "libsanitizer/sanitizer_common/sanitizer_stoptheworld_win.cpp", "status": "added", "additions": 175, "deletions": 0, "changes": 175, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stoptheworld_win.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_stoptheworld_win.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stoptheworld_win.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -0,0 +1,175 @@\n+//===-- sanitizer_stoptheworld_win.cpp ------------------------------------===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+//\n+// See sanitizer_stoptheworld.h for details.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#include \"sanitizer_platform.h\"\n+\n+#if SANITIZER_WINDOWS\n+\n+#  define WIN32_LEAN_AND_MEAN\n+#  include <windows.h>\n+// windows.h needs to be included before tlhelp32.h\n+#  include <tlhelp32.h>\n+\n+#  include \"sanitizer_stoptheworld.h\"\n+\n+namespace __sanitizer {\n+\n+namespace {\n+\n+struct SuspendedThreadsListWindows final : public SuspendedThreadsList {\n+  InternalMmapVector<HANDLE> threadHandles;\n+  InternalMmapVector<DWORD> threadIds;\n+\n+  SuspendedThreadsListWindows() {\n+    threadIds.reserve(1024);\n+    threadHandles.reserve(1024);\n+  }\n+\n+  PtraceRegistersStatus GetRegistersAndSP(uptr index,\n+                                          InternalMmapVector<uptr> *buffer,\n+                                          uptr *sp) const override;\n+\n+  tid_t GetThreadID(uptr index) const override;\n+  uptr ThreadCount() const override;\n+};\n+\n+// Stack Pointer register names on different architectures\n+#  if SANITIZER_X64\n+#    define SP_REG Rsp\n+#  elif SANITIZER_I386\n+#    define SP_REG Esp\n+#  elif SANITIZER_ARM | SANITIZER_ARM64\n+#    define SP_REG Sp\n+#  else\n+#    error Architecture not supported!\n+#  endif\n+\n+PtraceRegistersStatus SuspendedThreadsListWindows::GetRegistersAndSP(\n+    uptr index, InternalMmapVector<uptr> *buffer, uptr *sp) const {\n+  CHECK_LT(index, threadHandles.size());\n+\n+  buffer->resize(RoundUpTo(sizeof(CONTEXT), sizeof(uptr)) / sizeof(uptr));\n+  CONTEXT *thread_context = reinterpret_cast<CONTEXT *>(buffer->data());\n+  thread_context->ContextFlags = CONTEXT_ALL;\n+  CHECK(GetThreadContext(threadHandles[index], thread_context));\n+  *sp = thread_context->SP_REG;\n+\n+  return REGISTERS_AVAILABLE;\n+}\n+\n+tid_t SuspendedThreadsListWindows::GetThreadID(uptr index) const {\n+  CHECK_LT(index, threadIds.size());\n+  return threadIds[index];\n+}\n+\n+uptr SuspendedThreadsListWindows::ThreadCount() const {\n+  return threadIds.size();\n+}\n+\n+struct RunThreadArgs {\n+  StopTheWorldCallback callback;\n+  void *argument;\n+};\n+\n+DWORD WINAPI RunThread(void *argument) {\n+  RunThreadArgs *run_args = (RunThreadArgs *)argument;\n+\n+  const DWORD this_thread = GetCurrentThreadId();\n+  const DWORD this_process = GetCurrentProcessId();\n+\n+  SuspendedThreadsListWindows suspended_threads_list;\n+  bool new_thread_found;\n+\n+  do {\n+    // Take a snapshot of all Threads\n+    const HANDLE threads = CreateToolhelp32Snapshot(TH32CS_SNAPTHREAD, 0);\n+    CHECK(threads != INVALID_HANDLE_VALUE);\n+\n+    THREADENTRY32 thread_entry;\n+    thread_entry.dwSize = sizeof(thread_entry);\n+    new_thread_found = false;\n+\n+    if (!Thread32First(threads, &thread_entry))\n+      break;\n+\n+    do {\n+      if (thread_entry.th32ThreadID == this_thread ||\n+          thread_entry.th32OwnerProcessID != this_process)\n+        continue;\n+\n+      bool suspended_thread = false;\n+      for (const auto thread_id : suspended_threads_list.threadIds) {\n+        if (thread_id == thread_entry.th32ThreadID) {\n+          suspended_thread = true;\n+          break;\n+        }\n+      }\n+\n+      // Skip the Thread if it was already suspended\n+      if (suspended_thread)\n+        continue;\n+\n+      const HANDLE thread =\n+          OpenThread(THREAD_ALL_ACCESS, FALSE, thread_entry.th32ThreadID);\n+      CHECK(thread);\n+\n+      if (SuspendThread(thread) == (DWORD)-1) {\n+        DWORD last_error = GetLastError();\n+\n+        VPrintf(1, \"Could not suspend thread %lu (error %lu)\",\n+                thread_entry.th32ThreadID, last_error);\n+        continue;\n+      }\n+\n+      suspended_threads_list.threadIds.push_back(thread_entry.th32ThreadID);\n+      suspended_threads_list.threadHandles.push_back(thread);\n+      new_thread_found = true;\n+    } while (Thread32Next(threads, &thread_entry));\n+\n+    CloseHandle(threads);\n+\n+    // Between the call to `CreateToolhelp32Snapshot` and suspending the\n+    // relevant Threads, new Threads could have potentially been created. So\n+    // continue to find and suspend new Threads until we don't find any.\n+  } while (new_thread_found);\n+\n+  // Now all Threads of this Process except of this Thread should be suspended.\n+  // Execute the callback function.\n+  run_args->callback(suspended_threads_list, run_args->argument);\n+\n+  // Resume all Threads\n+  for (const auto suspended_thread_handle :\n+       suspended_threads_list.threadHandles) {\n+    CHECK_NE(ResumeThread(suspended_thread_handle), -1);\n+    CloseHandle(suspended_thread_handle);\n+  }\n+\n+  return 0;\n+}\n+\n+}  // namespace\n+\n+void StopTheWorld(StopTheWorldCallback callback, void *argument) {\n+  struct RunThreadArgs arg = {callback, argument};\n+  DWORD trace_thread_id;\n+\n+  auto trace_thread =\n+      CreateThread(nullptr, 0, RunThread, &arg, 0, &trace_thread_id);\n+  CHECK(trace_thread);\n+\n+  WaitForSingleObject(trace_thread, INFINITE);\n+  CloseHandle(trace_thread);\n+}\n+\n+}  // namespace __sanitizer\n+\n+#endif  // SANITIZER_WINDOWS"}, {"sha": "d3cffaa6eeff66d1b2f61ea17ef22c3d0d731067", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer.cpp", "status": "modified", "additions": 13, "deletions": 7, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -11,10 +11,11 @@\n //===----------------------------------------------------------------------===//\n \n #include \"sanitizer_allocator_internal.h\"\n-#include \"sanitizer_platform.h\"\n+#include \"sanitizer_common.h\"\n #include \"sanitizer_internal_defs.h\"\n #include \"sanitizer_libc.h\"\n #include \"sanitizer_placement_new.h\"\n+#include \"sanitizer_platform.h\"\n #include \"sanitizer_symbolizer_internal.h\"\n \n namespace __sanitizer {\n@@ -30,13 +31,24 @@ void AddressInfo::Clear() {\n   InternalFree(file);\n   internal_memset(this, 0, sizeof(AddressInfo));\n   function_offset = kUnknown;\n+  uuid_size = 0;\n }\n \n void AddressInfo::FillModuleInfo(const char *mod_name, uptr mod_offset,\n                                  ModuleArch mod_arch) {\n   module = internal_strdup(mod_name);\n   module_offset = mod_offset;\n   module_arch = mod_arch;\n+  uuid_size = 0;\n+}\n+\n+void AddressInfo::FillModuleInfo(const LoadedModule &mod) {\n+  module = internal_strdup(mod.full_name());\n+  module_offset = address - mod.base_address();\n+  module_arch = mod.arch();\n+  if (mod.uuid_size())\n+    internal_memcpy(uuid, mod.uuid(), mod.uuid_size());\n+  uuid_size = mod.uuid_size();\n }\n \n SymbolizedStack::SymbolizedStack() : next(nullptr), info() {}\n@@ -126,10 +138,4 @@ Symbolizer::SymbolizerScope::~SymbolizerScope() {\n     sym_->end_hook_();\n }\n \n-void Symbolizer::LateInitializeTools() {\n-  for (auto &tool : tools_) {\n-    tool.LateInitialize();\n-  }\n-}\n-\n }  // namespace __sanitizer"}, {"sha": "bad4761e345fec07a338d17b3d383b6a68db0af4", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer.h", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -32,6 +32,8 @@ struct AddressInfo {\n   char *module;\n   uptr module_offset;\n   ModuleArch module_arch;\n+  u8 uuid[kModuleUUIDSize];\n+  uptr uuid_size;\n \n   static const uptr kUnknown = ~(uptr)0;\n   char *function;\n@@ -45,6 +47,8 @@ struct AddressInfo {\n   // Deletes all strings and resets all fields.\n   void Clear();\n   void FillModuleInfo(const char *mod_name, uptr mod_offset, ModuleArch arch);\n+  void FillModuleInfo(const LoadedModule &mod);\n+  uptr module_base() const { return address - module_offset; }\n };\n \n // Linked list of symbolized frames (each frame is described by AddressInfo).\n@@ -209,9 +213,6 @@ class Symbolizer final {\n    private:\n     const Symbolizer *sym_;\n   };\n-\n-  // Calls `LateInitialize()` on all items in `tools_`.\n-  void LateInitializeTools();\n };\n \n #ifdef SANITIZER_WINDOWS"}, {"sha": "df122ed3425c35f71d6e016c782f79b75ca91247", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer_internal.h", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_internal.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_internal.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_internal.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -70,11 +70,6 @@ class SymbolizerTool {\n     return nullptr;\n   }\n \n-  // Called during the LateInitialize phase of Sanitizer initialization.\n-  // Usually this is a safe place to call code that might need to use user\n-  // memory allocators.\n-  virtual void LateInitialize() {}\n-\n  protected:\n   ~SymbolizerTool() {}\n };\n@@ -91,7 +86,7 @@ class SymbolizerProcess {\n   ~SymbolizerProcess() {}\n \n   /// The maximum number of arguments required to invoke a tool process.\n-  static const unsigned kArgVMax = 6;\n+  static const unsigned kArgVMax = 16;\n \n   // Customizable by subclasses.\n   virtual bool StartSymbolizerSubprocess();"}, {"sha": "8bbd4af0c7c275499c45485b53758928aedcb734", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer_libcdep.cpp", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_libcdep.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_libcdep.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_libcdep.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -84,15 +84,12 @@ const char *ExtractTokenUpToDelimiter(const char *str, const char *delimiter,\n \n SymbolizedStack *Symbolizer::SymbolizePC(uptr addr) {\n   Lock l(&mu_);\n-  const char *module_name = nullptr;\n-  uptr module_offset;\n-  ModuleArch arch;\n   SymbolizedStack *res = SymbolizedStack::New(addr);\n-  if (!FindModuleNameAndOffsetForAddress(addr, &module_name, &module_offset,\n-                                         &arch))\n+  auto *mod = FindModuleForAddress(addr);\n+  if (!mod)\n     return res;\n   // Always fill data about module name and offset.\n-  res->info.FillModuleInfo(module_name, module_offset, arch);\n+  res->info.FillModuleInfo(*mod);\n   for (auto &tool : tools_) {\n     SymbolizerScope sym_scope(this);\n     if (tool.SymbolizePC(addr, res)) {\n@@ -277,14 +274,17 @@ class LLVMSymbolizerProcess final : public SymbolizerProcess {\n     const char* const kSymbolizerArch = \"--default-arch=unknown\";\n #endif\n \n-    const char *const inline_flag = common_flags()->symbolize_inline_frames\n-                                        ? \"--inlines\"\n-                                        : \"--no-inlines\";\n+    const char *const demangle_flag =\n+        common_flags()->demangle ? \"--demangle\" : \"--no-demangle\";\n+    const char *const inline_flag =\n+        common_flags()->symbolize_inline_frames ? \"--inlines\" : \"--no-inlines\";\n     int i = 0;\n     argv[i++] = path_to_binary;\n+    argv[i++] = demangle_flag;\n     argv[i++] = inline_flag;\n     argv[i++] = kSymbolizerArch;\n     argv[i++] = nullptr;\n+    CHECK_LE(i, kArgVMax);\n   }\n };\n "}, {"sha": "ac811c8a9136d04bbf675ce45ebf36181ffb96e9", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer_mac.cpp", "status": "modified", "additions": 3, "deletions": 54, "changes": 57, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_mac.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_mac.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_mac.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -20,7 +20,6 @@\n \n #include <dlfcn.h>\n #include <errno.h>\n-#include <mach/mach.h>\n #include <stdlib.h>\n #include <sys/wait.h>\n #include <unistd.h>\n@@ -58,65 +57,20 @@ bool DlAddrSymbolizer::SymbolizeData(uptr addr, DataInfo *datainfo) {\n   return true;\n }\n \n-#define K_ATOS_ENV_VAR \"__check_mach_ports_lookup\"\n-\n-// This cannot live in `AtosSymbolizerProcess` because instances of that object\n-// are allocated by the internal allocator which under ASan is poisoned with\n-// kAsanInternalHeapMagic.\n-static char kAtosMachPortEnvEntry[] = K_ATOS_ENV_VAR \"=000000000000000\";\n-\n class AtosSymbolizerProcess final : public SymbolizerProcess {\n  public:\n   explicit AtosSymbolizerProcess(const char *path)\n       : SymbolizerProcess(path, /*use_posix_spawn*/ true) {\n     pid_str_[0] = '\\0';\n   }\n \n-  void LateInitialize() {\n-    if (SANITIZER_IOSSIM) {\n-      // `putenv()` may call malloc/realloc so it is only safe to do this\n-      // during LateInitialize() or later (i.e. we can't do this in the\n-      // constructor).  We also can't do this in `StartSymbolizerSubprocess()`\n-      // because in TSan we switch allocators when we're symbolizing.\n-      // We use `putenv()` rather than `setenv()` so that we can later directly\n-      // write into the storage without LibC getting involved to change what the\n-      // variable is set to\n-      int result = putenv(kAtosMachPortEnvEntry);\n-      CHECK_EQ(result, 0);\n-    }\n-  }\n-\n  private:\n   bool StartSymbolizerSubprocess() override {\n-    // Configure sandbox before starting atos process.\n-\n     // Put the string command line argument in the object so that it outlives\n     // the call to GetArgV.\n-    internal_snprintf(pid_str_, sizeof(pid_str_), \"%d\", internal_getpid());\n-\n-    if (SANITIZER_IOSSIM) {\n-      // `atos` in the simulator is restricted in its ability to retrieve the\n-      // task port for the target process (us) so we need to do extra work\n-      // to pass our task port to it.\n-      mach_port_t ports[]{mach_task_self()};\n-      kern_return_t ret =\n-          mach_ports_register(mach_task_self(), ports, /*count=*/1);\n-      CHECK_EQ(ret, KERN_SUCCESS);\n-\n-      // Set environment variable that signals to `atos` that it should look\n-      // for our task port. We can't call `setenv()` here because it might call\n-      // malloc/realloc. To avoid that we instead update the\n-      // `mach_port_env_var_entry_` variable with our current PID.\n-      uptr count = internal_snprintf(kAtosMachPortEnvEntry,\n-                                     sizeof(kAtosMachPortEnvEntry),\n-                                     K_ATOS_ENV_VAR \"=%s\", pid_str_);\n-      CHECK_GE(count, sizeof(K_ATOS_ENV_VAR) + internal_strlen(pid_str_));\n-      // Document our assumption but without calling `getenv()` in normal\n-      // builds.\n-      DCHECK(getenv(K_ATOS_ENV_VAR));\n-      DCHECK_EQ(internal_strcmp(getenv(K_ATOS_ENV_VAR), pid_str_), 0);\n-    }\n+    internal_snprintf(pid_str_, sizeof(pid_str_), \"%d\", (int)internal_getpid());\n \n+    // Configure sandbox before starting atos process.\n     return SymbolizerProcess::StartSymbolizerSubprocess();\n   }\n \n@@ -137,13 +91,10 @@ class AtosSymbolizerProcess final : public SymbolizerProcess {\n       argv[i++] = \"-d\";\n     }\n     argv[i++] = nullptr;\n+    CHECK_LE(i, kArgVMax);\n   }\n \n   char pid_str_[16];\n-  // Space for `\\0` in `K_ATOS_ENV_VAR` is reused for `=`.\n-  static_assert(sizeof(kAtosMachPortEnvEntry) ==\n-                    (sizeof(K_ATOS_ENV_VAR) + sizeof(pid_str_)),\n-                \"sizes should match\");\n };\n \n #undef K_ATOS_ENV_VAR\n@@ -249,8 +200,6 @@ bool AtosSymbolizer::SymbolizeData(uptr addr, DataInfo *info) {\n   return true;\n }\n \n-void AtosSymbolizer::LateInitialize() { process_->LateInitialize(); }\n-\n }  // namespace __sanitizer\n \n #endif  // SANITIZER_MAC"}, {"sha": "d5abe9d98c1f9c93083d58ae44c6913d20540372", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer_mac.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_mac.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_mac.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_mac.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -35,7 +35,6 @@ class AtosSymbolizer final : public SymbolizerTool {\n \n   bool SymbolizePC(uptr addr, SymbolizedStack *stack) override;\n   bool SymbolizeData(uptr addr, DataInfo *info) override;\n-  void LateInitialize() override;\n \n  private:\n   AtosSymbolizerProcess *process_;"}, {"sha": "1ec0c5cad7a20c7e60062f5772fcc1064df6b904", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer_markup.cpp", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_markup.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_markup.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_markup.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -100,9 +100,7 @@ Symbolizer *Symbolizer::PlatformInit() {\n   return new (symbolizer_allocator_) Symbolizer({});\n }\n \n-void Symbolizer::LateInitialize() {\n-  Symbolizer::GetOrInit()->LateInitializeTools();\n-}\n+void Symbolizer::LateInitialize() { Symbolizer::GetOrInit(); }\n \n void StartReportDeadlySignal() {}\n void ReportDeadlySignal(const SignalContext &sig, u32 tid,"}, {"sha": "5f6e4cc3180e9cea8001aaf4338b8516542390c6", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer_posix_libcdep.cpp", "status": "modified", "additions": 34, "deletions": 24, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_posix_libcdep.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_posix_libcdep.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_posix_libcdep.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -213,9 +213,14 @@ class Addr2LineProcess final : public SymbolizerProcess {\n                const char *(&argv)[kArgVMax]) const override {\n     int i = 0;\n     argv[i++] = path_to_binary;\n-    argv[i++] = \"-iCfe\";\n+    if (common_flags()->demangle)\n+      argv[i++] = \"-C\";\n+    if (common_flags()->symbolize_inline_frames)\n+      argv[i++] = \"-i\";\n+    argv[i++] = \"-fe\";\n     argv[i++] = module_name_;\n     argv[i++] = nullptr;\n+    CHECK_LE(i, kArgVMax);\n   }\n \n   bool ReachedEndOfOutput(const char *buffer, uptr length) const override;\n@@ -312,37 +317,42 @@ class Addr2LinePool final : public SymbolizerTool {\n       FIRST_32_SECOND_64(UINT32_MAX, UINT64_MAX);\n };\n \n-#if SANITIZER_SUPPORTS_WEAK_HOOKS\n+#  if SANITIZER_SUPPORTS_WEAK_HOOKS\n extern \"C\" {\n SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE bool\n __sanitizer_symbolize_code(const char *ModuleName, u64 ModuleOffset,\n-                           char *Buffer, int MaxLength,\n-                           bool SymbolizeInlineFrames);\n-SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-bool __sanitizer_symbolize_data(const char *ModuleName, u64 ModuleOffset,\n-                                char *Buffer, int MaxLength);\n-SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-void __sanitizer_symbolize_flush();\n-SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE\n-int __sanitizer_symbolize_demangle(const char *Name, char *Buffer,\n-                                   int MaxLength);\n+                           char *Buffer, int MaxLength);\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE bool\n+__sanitizer_symbolize_data(const char *ModuleName, u64 ModuleOffset,\n+                           char *Buffer, int MaxLength);\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE void\n+__sanitizer_symbolize_flush();\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE int\n+__sanitizer_symbolize_demangle(const char *Name, char *Buffer, int MaxLength);\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE bool\n+__sanitizer_symbolize_set_demangle(bool Demangle);\n+SANITIZER_INTERFACE_ATTRIBUTE SANITIZER_WEAK_ATTRIBUTE bool\n+__sanitizer_symbolize_set_inline_frames(bool InlineFrames);\n }  // extern \"C\"\n \n class InternalSymbolizer final : public SymbolizerTool {\n  public:\n   static InternalSymbolizer *get(LowLevelAllocator *alloc) {\n-    if (__sanitizer_symbolize_code != 0 &&\n-        __sanitizer_symbolize_data != 0) {\n-      return new(*alloc) InternalSymbolizer();\n-    }\n+    if (__sanitizer_symbolize_set_demangle)\n+      CHECK(__sanitizer_symbolize_set_demangle(common_flags()->demangle));\n+    if (__sanitizer_symbolize_set_inline_frames)\n+      CHECK(__sanitizer_symbolize_set_inline_frames(\n+          common_flags()->symbolize_inline_frames));\n+    if (__sanitizer_symbolize_code && __sanitizer_symbolize_data)\n+      return new (*alloc) InternalSymbolizer();\n     return 0;\n   }\n \n   bool SymbolizePC(uptr addr, SymbolizedStack *stack) override {\n     bool result = __sanitizer_symbolize_code(\n-        stack->info.module, stack->info.module_offset, buffer_, kBufferSize,\n-        common_flags()->symbolize_inline_frames);\n-    if (result) ParseSymbolizePCOutput(buffer_, stack);\n+        stack->info.module, stack->info.module_offset, buffer_, kBufferSize);\n+    if (result)\n+      ParseSymbolizePCOutput(buffer_, stack);\n     return result;\n   }\n \n@@ -365,7 +375,7 @@ class InternalSymbolizer final : public SymbolizerTool {\n     if (__sanitizer_symbolize_demangle) {\n       for (uptr res_length = 1024;\n            res_length <= InternalSizeClassMap::kMaxSize;) {\n-        char *res_buff = static_cast<char*>(InternalAlloc(res_length));\n+        char *res_buff = static_cast<char *>(InternalAlloc(res_length));\n         uptr req_length =\n             __sanitizer_symbolize_demangle(name, res_buff, res_length);\n         if (req_length > res_length) {\n@@ -380,19 +390,19 @@ class InternalSymbolizer final : public SymbolizerTool {\n   }\n \n  private:\n-  InternalSymbolizer() { }\n+  InternalSymbolizer() {}\n \n   static const int kBufferSize = 16 * 1024;\n   char buffer_[kBufferSize];\n };\n-#else  // SANITIZER_SUPPORTS_WEAK_HOOKS\n+#  else  // SANITIZER_SUPPORTS_WEAK_HOOKS\n \n class InternalSymbolizer final : public SymbolizerTool {\n  public:\n   static InternalSymbolizer *get(LowLevelAllocator *alloc) { return 0; }\n };\n \n-#endif  // SANITIZER_SUPPORTS_WEAK_HOOKS\n+#  endif  // SANITIZER_SUPPORTS_WEAK_HOOKS\n \n const char *Symbolizer::PlatformDemangle(const char *name) {\n   return DemangleSwiftAndCXX(name);\n@@ -492,7 +502,7 @@ Symbolizer *Symbolizer::PlatformInit() {\n }\n \n void Symbolizer::LateInitialize() {\n-  Symbolizer::GetOrInit()->LateInitializeTools();\n+  Symbolizer::GetOrInit();\n   InitializeSwiftDemangler();\n }\n "}, {"sha": "ac855c8be1c869086463ed067456b3b7f777ef5d", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer_report.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_report.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_report.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_report.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -211,9 +211,9 @@ static void ReportDeadlySignalImpl(const SignalContext &sig, u32 tid,\n     Report(\"Hint: pc points to the zero page.\\n\");\n   if (sig.is_memory_access) {\n     const char *access_type =\n-        sig.write_flag == SignalContext::WRITE\n+        sig.write_flag == SignalContext::Write\n             ? \"WRITE\"\n-            : (sig.write_flag == SignalContext::READ ? \"READ\" : \"UNKNOWN\");\n+            : (sig.write_flag == SignalContext::Read ? \"READ\" : \"UNKNOWN\");\n     Report(\"The signal is caused by a %s memory access.\\n\", access_type);\n     if (!sig.is_true_faulting_addr)\n       Report(\"Hint: this fault was caused by a dereference of a high value \""}, {"sha": "c647ab107ec54c400e5214a1e1506e8d63f3c7e8", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer_win.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_win.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_win.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_win.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -318,7 +318,7 @@ Symbolizer *Symbolizer::PlatformInit() {\n }\n \n void Symbolizer::LateInitialize() {\n-  Symbolizer::GetOrInit()->LateInitializeTools();\n+  Symbolizer::GetOrInit();\n }\n \n }  // namespace __sanitizer"}, {"sha": "4ce5de0627561326e45a8212f7d819fa7c4c848a", "filename": "libsanitizer/sanitizer_common/sanitizer_syscalls_netbsd.inc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_syscalls_netbsd.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_syscalls_netbsd.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_syscalls_netbsd.inc?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -2255,13 +2255,13 @@ PRE_SYSCALL(getcontext)(void *ucp_) { /* Nothing to do */ }\n POST_SYSCALL(getcontext)(long long res, void *ucp_) { /* Nothing to do */ }\n PRE_SYSCALL(setcontext)(void *ucp_) {\n   if (ucp_) {\n-    PRE_READ(ucp_, ucontext_t_sz);\n+    PRE_READ(ucp_, ucontext_t_sz(ucp_));\n   }\n }\n POST_SYSCALL(setcontext)(long long res, void *ucp_) {}\n PRE_SYSCALL(_lwp_create)(void *ucp_, long long flags_, void *new_lwp_) {\n   if (ucp_) {\n-    PRE_READ(ucp_, ucontext_t_sz);\n+    PRE_READ(ucp_, ucontext_t_sz(ucp_));\n   }\n }\n POST_SYSCALL(_lwp_create)"}, {"sha": "278f6defca95fc1968481f493cacff9c659eee4f", "filename": "libsanitizer/sanitizer_common/sanitizer_thread_registry.cpp", "status": "modified", "additions": 47, "deletions": 9, "changes": 56, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_thread_registry.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_thread_registry.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_thread_registry.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -13,6 +13,8 @@\n \n #include \"sanitizer_thread_registry.h\"\n \n+#include \"sanitizer_placement_new.h\"\n+\n namespace __sanitizer {\n \n ThreadContextBase::ThreadContextBase(u32 tid)\n@@ -108,7 +110,7 @@ ThreadRegistry::ThreadRegistry(ThreadContextFactory factory, u32 max_threads,\n       max_threads_(max_threads),\n       thread_quarantine_size_(thread_quarantine_size),\n       max_reuse_(max_reuse),\n-      mtx_(),\n+      mtx_(MutexThreadRegistry),\n       total_threads_(0),\n       alive_threads_(0),\n       max_alive_threads_(0),\n@@ -162,6 +164,12 @@ u32 ThreadRegistry::CreateThread(uptr user_id, bool detached, u32 parent_tid,\n     max_alive_threads_++;\n     CHECK_EQ(alive_threads_, max_alive_threads_);\n   }\n+  if (user_id) {\n+    // Ensure that user_id is unique. If it's not the case we are screwed.\n+    // Ignoring this situation may lead to very hard to debug false\n+    // positives later (e.g. if we join a wrong thread).\n+    CHECK(live_.try_emplace(user_id, tid).second);\n+  }\n   tctx->SetCreated(user_id, total_threads_++, detached,\n                    parent_tid, arg);\n   return tid;\n@@ -221,14 +229,8 @@ void ThreadRegistry::SetThreadName(u32 tid, const char *name) {\n \n void ThreadRegistry::SetThreadNameByUserId(uptr user_id, const char *name) {\n   ThreadRegistryLock l(this);\n-  for (u32 tid = 0; tid < threads_.size(); tid++) {\n-    ThreadContextBase *tctx = threads_[tid];\n-    if (tctx != 0 && tctx->user_id == user_id &&\n-        tctx->status != ThreadStatusInvalid) {\n-      tctx->SetName(name);\n-      return;\n-    }\n-  }\n+  if (const auto *tid = live_.find(user_id))\n+    threads_[tid->second]->SetName(name);\n }\n \n void ThreadRegistry::DetachThread(u32 tid, void *arg) {\n@@ -241,6 +243,8 @@ void ThreadRegistry::DetachThread(u32 tid, void *arg) {\n   }\n   tctx->OnDetached(arg);\n   if (tctx->status == ThreadStatusFinished) {\n+    if (tctx->user_id)\n+      live_.erase(tctx->user_id);\n     tctx->SetDead();\n     QuarantinePush(tctx);\n   } else {\n@@ -260,6 +264,8 @@ void ThreadRegistry::JoinThread(u32 tid, void *arg) {\n         return;\n       }\n       if ((destroyed = tctx->GetDestroyed())) {\n+        if (tctx->user_id)\n+          live_.erase(tctx->user_id);\n         tctx->SetJoined(arg);\n         QuarantinePush(tctx);\n       }\n@@ -292,6 +298,8 @@ ThreadStatus ThreadRegistry::FinishThread(u32 tid) {\n   }\n   tctx->SetFinished();\n   if (dead) {\n+    if (tctx->user_id)\n+      live_.erase(tctx->user_id);\n     tctx->SetDead();\n     QuarantinePush(tctx);\n   }\n@@ -333,6 +341,19 @@ ThreadContextBase *ThreadRegistry::QuarantinePop() {\n   return tctx;\n }\n \n+u32 ThreadRegistry::ConsumeThreadUserId(uptr user_id) {\n+  ThreadRegistryLock l(this);\n+  u32 tid;\n+  auto *t = live_.find(user_id);\n+  CHECK(t);\n+  tid = t->second;\n+  live_.erase(t);\n+  auto *tctx = threads_[tid];\n+  CHECK_EQ(tctx->user_id, user_id);\n+  tctx->user_id = 0;\n+  return tid;\n+}\n+\n void ThreadRegistry::SetThreadUserId(u32 tid, uptr user_id) {\n   ThreadRegistryLock l(this);\n   ThreadContextBase *tctx = threads_[tid];\n@@ -341,6 +362,23 @@ void ThreadRegistry::SetThreadUserId(u32 tid, uptr user_id) {\n   CHECK_NE(tctx->status, ThreadStatusDead);\n   CHECK_EQ(tctx->user_id, 0);\n   tctx->user_id = user_id;\n+  CHECK(live_.try_emplace(user_id, tctx->tid).second);\n+}\n+\n+u32 ThreadRegistry::OnFork(u32 tid) {\n+  ThreadRegistryLock l(this);\n+  // We only purge user_id (pthread_t) of live threads because\n+  // they cause CHECK failures if new threads with matching pthread_t\n+  // created after fork.\n+  // Potentially we could purge more info (ThreadContextBase themselves),\n+  // but it's hard to test and easy to introduce new issues by doing this.\n+  for (auto *tctx : threads_) {\n+    if (tctx->tid == tid || !tctx->user_id)\n+      continue;\n+    CHECK(live_.erase(tctx->user_id));\n+    tctx->user_id = 0;\n+  }\n+  return alive_threads_;\n }\n \n }  // namespace __sanitizer"}, {"sha": "2c7e5c276fa1c798b6701d4558932b3a7c62278e", "filename": "libsanitizer/sanitizer_common/sanitizer_thread_registry.h", "status": "modified", "additions": 14, "deletions": 4, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_thread_registry.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_thread_registry.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_thread_registry.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -15,6 +15,7 @@\n #define SANITIZER_THREAD_REGISTRY_H\n \n #include \"sanitizer_common.h\"\n+#include \"sanitizer_dense_map.h\"\n #include \"sanitizer_list.h\"\n #include \"sanitizer_mutex.h\"\n \n@@ -85,7 +86,7 @@ class ThreadContextBase {\n \n typedef ThreadContextBase* (*ThreadContextFactory)(u32 tid);\n \n-class MUTEX ThreadRegistry {\n+class SANITIZER_MUTEX ThreadRegistry {\n  public:\n   ThreadRegistry(ThreadContextFactory factory);\n   ThreadRegistry(ThreadContextFactory factory, u32 max_threads,\n@@ -94,15 +95,17 @@ class MUTEX ThreadRegistry {\n                           uptr *alive = nullptr);\n   uptr GetMaxAliveThreads();\n \n-  void Lock() ACQUIRE() { mtx_.Lock(); }\n-  void CheckLocked() const CHECK_LOCKED() { mtx_.CheckLocked(); }\n-  void Unlock() RELEASE() { mtx_.Unlock(); }\n+  void Lock() SANITIZER_ACQUIRE() { mtx_.Lock(); }\n+  void CheckLocked() const SANITIZER_CHECK_LOCKED() { mtx_.CheckLocked(); }\n+  void Unlock() SANITIZER_RELEASE() { mtx_.Unlock(); }\n \n   // Should be guarded by ThreadRegistryLock.\n   ThreadContextBase *GetThreadLocked(u32 tid) {\n     return threads_.empty() ? nullptr : threads_[tid];\n   }\n \n+  u32 NumThreadsLocked() const { return threads_.size(); }\n+\n   u32 CreateThread(uptr user_id, bool detached, u32 parent_tid, void *arg);\n \n   typedef void (*ThreadCallback)(ThreadContextBase *tctx, void *arg);\n@@ -127,8 +130,14 @@ class MUTEX ThreadRegistry {\n   // Finishes thread and returns previous status.\n   ThreadStatus FinishThread(u32 tid);\n   void StartThread(u32 tid, tid_t os_id, ThreadType thread_type, void *arg);\n+  u32 ConsumeThreadUserId(uptr user_id);\n   void SetThreadUserId(u32 tid, uptr user_id);\n \n+  // OnFork must be called in the child process after fork to purge old\n+  // threads that don't exist anymore (except for the current thread tid).\n+  // Returns number of alive threads before fork.\n+  u32 OnFork(u32 tid);\n+\n  private:\n   const ThreadContextFactory context_factory_;\n   const u32 max_threads_;\n@@ -146,6 +155,7 @@ class MUTEX ThreadRegistry {\n   InternalMmapVector<ThreadContextBase *> threads_;\n   IntrusiveList<ThreadContextBase> dead_threads_;\n   IntrusiveList<ThreadContextBase> invalid_threads_;\n+  DenseMap<uptr, Tid> live_;\n \n   void QuarantinePush(ThreadContextBase *tctx);\n   ThreadContextBase *QuarantinePop();"}, {"sha": "c34ea804da201ba205c16ad82be99d8e629b4653", "filename": "libsanitizer/sanitizer_common/sanitizer_thread_safety.h", "status": "modified", "additions": 26, "deletions": 19, "changes": 45, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_thread_safety.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_thread_safety.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_thread_safety.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -16,27 +16,34 @@\n #define SANITIZER_THREAD_SAFETY_H\n \n #if defined(__clang__)\n-#  define THREAD_ANNOTATION(x) __attribute__((x))\n+#  define SANITIZER_THREAD_ANNOTATION(x) __attribute__((x))\n #else\n-#  define THREAD_ANNOTATION(x)\n+#  define SANITIZER_THREAD_ANNOTATION(x)\n #endif\n \n-#define MUTEX THREAD_ANNOTATION(capability(\"mutex\"))\n-#define SCOPED_LOCK THREAD_ANNOTATION(scoped_lockable)\n-#define GUARDED_BY(x) THREAD_ANNOTATION(guarded_by(x))\n-#define PT_GUARDED_BY(x) THREAD_ANNOTATION(pt_guarded_by(x))\n-#define REQUIRES(...) THREAD_ANNOTATION(requires_capability(__VA_ARGS__))\n-#define REQUIRES_SHARED(...) \\\n-  THREAD_ANNOTATION(requires_shared_capability(__VA_ARGS__))\n-#define ACQUIRE(...) THREAD_ANNOTATION(acquire_capability(__VA_ARGS__))\n-#define ACQUIRE_SHARED(...) \\\n-  THREAD_ANNOTATION(acquire_shared_capability(__VA_ARGS__))\n-#define TRY_ACQUIRE(...) THREAD_ANNOTATION(try_acquire_capability(__VA_ARGS__))\n-#define RELEASE(...) THREAD_ANNOTATION(release_capability(__VA_ARGS__))\n-#define RELEASE_SHARED(...) \\\n-  THREAD_ANNOTATION(release_shared_capability(__VA_ARGS__))\n-#define EXCLUDES(...) THREAD_ANNOTATION(locks_excluded(__VA_ARGS__))\n-#define CHECK_LOCKED(...) THREAD_ANNOTATION(assert_capability(__VA_ARGS__))\n-#define NO_THREAD_SAFETY_ANALYSIS THREAD_ANNOTATION(no_thread_safety_analysis)\n+#define SANITIZER_MUTEX SANITIZER_THREAD_ANNOTATION(capability(\"mutex\"))\n+#define SANITIZER_SCOPED_LOCK SANITIZER_THREAD_ANNOTATION(scoped_lockable)\n+#define SANITIZER_GUARDED_BY(x) SANITIZER_THREAD_ANNOTATION(guarded_by(x))\n+#define SANITIZER_PT_GUARDED_BY(x) SANITIZER_THREAD_ANNOTATION(pt_guarded_by(x))\n+#define SANITIZER_REQUIRES(...) \\\n+  SANITIZER_THREAD_ANNOTATION(requires_capability(__VA_ARGS__))\n+#define SANITIZER_REQUIRES_SHARED(...) \\\n+  SANITIZER_THREAD_ANNOTATION(requires_shared_capability(__VA_ARGS__))\n+#define SANITIZER_ACQUIRE(...) \\\n+  SANITIZER_THREAD_ANNOTATION(acquire_capability(__VA_ARGS__))\n+#define SANITIZER_ACQUIRE_SHARED(...) \\\n+  SANITIZER_THREAD_ANNOTATION(acquire_shared_capability(__VA_ARGS__))\n+#define SANITIZER_TRY_ACQUIRE(...) \\\n+  SANITIZER_THREAD_ANNOTATION(try_acquire_capability(__VA_ARGS__))\n+#define SANITIZER_RELEASE(...) \\\n+  SANITIZER_THREAD_ANNOTATION(release_capability(__VA_ARGS__))\n+#define SANITIZER_RELEASE_SHARED(...) \\\n+  SANITIZER_THREAD_ANNOTATION(release_shared_capability(__VA_ARGS__))\n+#define SANITIZER_EXCLUDES(...) \\\n+  SANITIZER_THREAD_ANNOTATION(locks_excluded(__VA_ARGS__))\n+#define SANITIZER_CHECK_LOCKED(...) \\\n+  SANITIZER_THREAD_ANNOTATION(assert_capability(__VA_ARGS__))\n+#define SANITIZER_NO_THREAD_SAFETY_ANALYSIS \\\n+  SANITIZER_THREAD_ANNOTATION(no_thread_safety_analysis)\n \n #endif"}, {"sha": "06a44d1b5c7af525861cfca276ae79bb8a88da84", "filename": "libsanitizer/sanitizer_common/sanitizer_type_traits.h", "status": "modified", "additions": 79, "deletions": 0, "changes": 79, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_type_traits.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_type_traits.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_type_traits.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -13,6 +13,8 @@\n #ifndef SANITIZER_TYPE_TRAITS_H\n #define SANITIZER_TYPE_TRAITS_H\n \n+#include \"sanitizer_common/sanitizer_internal_defs.h\"\n+\n namespace __sanitizer {\n \n struct true_type {\n@@ -57,6 +59,83 @@ struct conditional<false, T, F> {\n   using type = F;\n };\n \n+template <class T>\n+struct remove_reference {\n+  using type = T;\n+};\n+template <class T>\n+struct remove_reference<T&> {\n+  using type = T;\n+};\n+template <class T>\n+struct remove_reference<T&&> {\n+  using type = T;\n+};\n+\n+template <class T>\n+WARN_UNUSED_RESULT inline typename remove_reference<T>::type&& move(T&& t) {\n+  return static_cast<typename remove_reference<T>::type&&>(t);\n+}\n+\n+template <class T>\n+WARN_UNUSED_RESULT inline constexpr T&& forward(\n+    typename remove_reference<T>::type& t) {\n+  return static_cast<T&&>(t);\n+}\n+\n+template <class T>\n+WARN_UNUSED_RESULT inline constexpr T&& forward(\n+    typename remove_reference<T>::type&& t) {\n+  return static_cast<T&&>(t);\n+}\n+\n+template <class T, T v>\n+struct integral_constant {\n+  static constexpr const T value = v;\n+  typedef T value_type;\n+  typedef integral_constant type;\n+  constexpr operator value_type() const { return value; }\n+  constexpr value_type operator()() const { return value; }\n+};\n+\n+#ifndef __has_builtin\n+#  define __has_builtin(x) 0\n+#endif\n+\n+#if __has_builtin(__is_trivially_destructible)\n+\n+template <class T>\n+struct is_trivially_destructible\n+    : public integral_constant<bool, __is_trivially_destructible(T)> {};\n+\n+#elif __has_builtin(__has_trivial_destructor)\n+\n+template <class T>\n+struct is_trivially_destructible\n+    : public integral_constant<bool, __has_trivial_destructor(T)> {};\n+\n+#else\n+\n+template <class T>\n+struct is_trivially_destructible\n+    : public integral_constant<bool, /* less efficient fallback */ false> {};\n+\n+#endif\n+\n+#if __has_builtin(__is_trivially_copyable)\n+\n+template <class T>\n+struct is_trivially_copyable\n+    : public integral_constant<bool, __is_trivially_copyable(T)> {};\n+\n+#else\n+\n+template <class T>\n+struct is_trivially_copyable\n+    : public integral_constant<bool, /* less efficient fallback */ false> {};\n+\n+#endif\n+\n }  // namespace __sanitizer\n \n #endif"}, {"sha": "afcd01dae0b7a4e05138751190f15a82b94bef4f", "filename": "libsanitizer/sanitizer_common/sanitizer_unwind_win.cpp", "status": "modified", "additions": 17, "deletions": 10, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_unwind_win.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_unwind_win.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_unwind_win.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -57,30 +57,37 @@ void BufferedStackTrace::UnwindSlow(uptr pc, void *context, u32 max_depth) {\n   InitializeDbgHelpIfNeeded();\n \n   size = 0;\n-#if defined(_WIN64)\n+#    if SANITIZER_WINDOWS64\n+#      if SANITIZER_ARM64\n+  int machine_type = IMAGE_FILE_MACHINE_ARM64;\n+  stack_frame.AddrPC.Offset = ctx.Pc;\n+  stack_frame.AddrFrame.Offset = ctx.Fp;\n+  stack_frame.AddrStack.Offset = ctx.Sp;\n+#      else\n   int machine_type = IMAGE_FILE_MACHINE_AMD64;\n   stack_frame.AddrPC.Offset = ctx.Rip;\n   stack_frame.AddrFrame.Offset = ctx.Rbp;\n   stack_frame.AddrStack.Offset = ctx.Rsp;\n-#else\n+#      endif\n+#    else\n   int machine_type = IMAGE_FILE_MACHINE_I386;\n   stack_frame.AddrPC.Offset = ctx.Eip;\n   stack_frame.AddrFrame.Offset = ctx.Ebp;\n   stack_frame.AddrStack.Offset = ctx.Esp;\n-#endif\n+#    endif\n   stack_frame.AddrPC.Mode = AddrModeFlat;\n   stack_frame.AddrFrame.Mode = AddrModeFlat;\n   stack_frame.AddrStack.Mode = AddrModeFlat;\n   while (StackWalk64(machine_type, GetCurrentProcess(), GetCurrentThread(),\n-    &stack_frame, &ctx, NULL, SymFunctionTableAccess64,\n-    SymGetModuleBase64, NULL) &&\n-    size < Min(max_depth, kStackTraceMax)) {\n+                     &stack_frame, &ctx, NULL, SymFunctionTableAccess64,\n+                     SymGetModuleBase64, NULL) &&\n+         size < Min(max_depth, kStackTraceMax)) {\n     trace_buffer[size++] = (uptr)stack_frame.AddrPC.Offset;\n   }\n }\n-#ifdef __clang__\n-#pragma clang diagnostic pop\n-#endif\n-#endif  // #if !SANITIZER_GO\n+#    ifdef __clang__\n+#      pragma clang diagnostic pop\n+#    endif\n+#  endif  // #if !SANITIZER_GO\n \n #endif  // SANITIZER_WINDOWS"}, {"sha": "53770331199fdd4a0e144f6cff56602ae9f59e1c", "filename": "libsanitizer/sanitizer_common/sanitizer_win.cpp", "status": "modified", "additions": 27, "deletions": 11, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_win.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fsanitizer_common%2Fsanitizer_win.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_win.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -16,7 +16,6 @@\n \n #define WIN32_LEAN_AND_MEAN\n #define NOGDI\n-#include <direct.h>\n #include <windows.h>\n #include <io.h>\n #include <psapi.h>\n@@ -94,6 +93,11 @@ bool FileExists(const char *filename) {\n   return ::GetFileAttributesA(filename) != INVALID_FILE_ATTRIBUTES;\n }\n \n+bool DirExists(const char *path) {\n+  auto attr = ::GetFileAttributesA(path);\n+  return (attr != INVALID_FILE_ATTRIBUTES) && (attr & FILE_ATTRIBUTE_DIRECTORY);\n+}\n+\n uptr internal_getpid() {\n   return GetProcessId(GetCurrentProcess());\n }\n@@ -337,6 +341,11 @@ bool MprotectNoAccess(uptr addr, uptr size) {\n   return VirtualProtect((LPVOID)addr, size, PAGE_NOACCESS, &old_protection);\n }\n \n+bool MprotectReadOnly(uptr addr, uptr size) {\n+  DWORD old_protection;\n+  return VirtualProtect((LPVOID)addr, size, PAGE_READONLY, &old_protection);\n+}\n+\n void ReleaseMemoryPagesToOS(uptr beg, uptr end) {\n   uptr beg_aligned = RoundDownTo(beg, GetPageSizeCached()),\n        end_aligned = RoundDownTo(end, GetPageSizeCached());\n@@ -513,7 +522,7 @@ void ReExec() {\n   UNIMPLEMENTED();\n }\n \n-void PlatformPrepareForSandboxing(__sanitizer_sandbox_arguments *args) {}\n+void PlatformPrepareForSandboxing(void *args) {}\n \n bool StackSizeIsUnlimited() {\n   UNIMPLEMENTED();\n@@ -566,7 +575,9 @@ void Abort() {\n   internal__exit(3);\n }\n \n-bool CreateDir(const char *pathname) { return _mkdir(pathname) == 0; }\n+bool CreateDir(const char *pathname) {\n+  return CreateDirectoryA(pathname, nullptr) != 0;\n+}\n \n #if !SANITIZER_GO\n // Read the file to extract the ImageBase field from the PE header. If ASLR is\n@@ -944,13 +955,18 @@ void SignalContext::InitPcSpBp() {\n   CONTEXT *context_record = (CONTEXT *)context;\n \n   pc = (uptr)exception_record->ExceptionAddress;\n-#ifdef _WIN64\n+#  if SANITIZER_WINDOWS64\n+#    if SANITIZER_ARM64\n+  bp = (uptr)context_record->Fp;\n+  sp = (uptr)context_record->Sp;\n+#    else\n   bp = (uptr)context_record->Rbp;\n   sp = (uptr)context_record->Rsp;\n-#else\n+#    endif\n+#  else\n   bp = (uptr)context_record->Ebp;\n   sp = (uptr)context_record->Esp;\n-#endif\n+#  endif\n }\n \n uptr SignalContext::GetAddress() const {\n@@ -972,21 +988,21 @@ SignalContext::WriteFlag SignalContext::GetWriteFlag() const {\n \n   // The write flag is only available for access violation exceptions.\n   if (exception_record->ExceptionCode != EXCEPTION_ACCESS_VIOLATION)\n-    return SignalContext::UNKNOWN;\n+    return SignalContext::Unknown;\n \n   // The contents of this array are documented at\n   // https://docs.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-exception_record\n   // The first element indicates read as 0, write as 1, or execute as 8.  The\n   // second element is the faulting address.\n   switch (exception_record->ExceptionInformation[0]) {\n     case 0:\n-      return SignalContext::READ;\n+      return SignalContext::Read;\n     case 1:\n-      return SignalContext::WRITE;\n+      return SignalContext::Write;\n     case 8:\n-      return SignalContext::UNKNOWN;\n+      return SignalContext::Unknown;\n   }\n-  return SignalContext::UNKNOWN;\n+  return SignalContext::Unknown;\n }\n \n void SignalContext::DumpAllRegisters(void *context) {"}, {"sha": "d122b67c0aaa5fc9016d5d208120d38e75a543dc", "filename": "libsanitizer/tsan/tsan_clock.cpp", "status": "removed", "additions": 0, "deletions": 625, "changes": 625, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2285af309000b74da0f7dc756a0b55e5f0b1b56/libsanitizer%2Ftsan%2Ftsan_clock.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2285af309000b74da0f7dc756a0b55e5f0b1b56/libsanitizer%2Ftsan%2Ftsan_clock.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_clock.cpp?ref=e2285af309000b74da0f7dc756a0b55e5f0b1b56", "patch": "@@ -1,625 +0,0 @@\n-//===-- tsan_clock.cpp ----------------------------------------------------===//\n-//\n-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n-// See https://llvm.org/LICENSE.txt for license information.\n-// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n-//\n-//===----------------------------------------------------------------------===//\n-//\n-// This file is a part of ThreadSanitizer (TSan), a race detector.\n-//\n-//===----------------------------------------------------------------------===//\n-#include \"tsan_clock.h\"\n-#include \"tsan_rtl.h\"\n-#include \"sanitizer_common/sanitizer_placement_new.h\"\n-\n-// SyncClock and ThreadClock implement vector clocks for sync variables\n-// (mutexes, atomic variables, file descriptors, etc) and threads, respectively.\n-// ThreadClock contains fixed-size vector clock for maximum number of threads.\n-// SyncClock contains growable vector clock for currently necessary number of\n-// threads.\n-// Together they implement very simple model of operations, namely:\n-//\n-//   void ThreadClock::acquire(const SyncClock *src) {\n-//     for (int i = 0; i < kMaxThreads; i++)\n-//       clock[i] = max(clock[i], src->clock[i]);\n-//   }\n-//\n-//   void ThreadClock::release(SyncClock *dst) const {\n-//     for (int i = 0; i < kMaxThreads; i++)\n-//       dst->clock[i] = max(dst->clock[i], clock[i]);\n-//   }\n-//\n-//   void ThreadClock::releaseStoreAcquire(SyncClock *sc) const {\n-//     for (int i = 0; i < kMaxThreads; i++) {\n-//       tmp = clock[i];\n-//       clock[i] = max(clock[i], sc->clock[i]);\n-//       sc->clock[i] = tmp;\n-//     }\n-//   }\n-//\n-//   void ThreadClock::ReleaseStore(SyncClock *dst) const {\n-//     for (int i = 0; i < kMaxThreads; i++)\n-//       dst->clock[i] = clock[i];\n-//   }\n-//\n-//   void ThreadClock::acq_rel(SyncClock *dst) {\n-//     acquire(dst);\n-//     release(dst);\n-//   }\n-//\n-// Conformance to this model is extensively verified in tsan_clock_test.cpp.\n-// However, the implementation is significantly more complex. The complexity\n-// allows to implement important classes of use cases in O(1) instead of O(N).\n-//\n-// The use cases are:\n-// 1. Singleton/once atomic that has a single release-store operation followed\n-//    by zillions of acquire-loads (the acquire-load is O(1)).\n-// 2. Thread-local mutex (both lock and unlock can be O(1)).\n-// 3. Leaf mutex (unlock is O(1)).\n-// 4. A mutex shared by 2 threads (both lock and unlock can be O(1)).\n-// 5. An atomic with a single writer (writes can be O(1)).\n-// The implementation dynamically adopts to workload. So if an atomic is in\n-// read-only phase, these reads will be O(1); if it later switches to read/write\n-// phase, the implementation will correctly handle that by switching to O(N).\n-//\n-// Thread-safety note: all const operations on SyncClock's are conducted under\n-// a shared lock; all non-const operations on SyncClock's are conducted under\n-// an exclusive lock; ThreadClock's are private to respective threads and so\n-// do not need any protection.\n-//\n-// Description of SyncClock state:\n-// clk_ - variable size vector clock, low kClkBits hold timestamp,\n-//   the remaining bits hold \"acquired\" flag (the actual value is thread's\n-//   reused counter);\n-//   if acquired == thr->reused_, then the respective thread has already\n-//   acquired this clock (except possibly for dirty elements).\n-// dirty_ - holds up to two indices in the vector clock that other threads\n-//   need to acquire regardless of \"acquired\" flag value;\n-// release_store_tid_ - denotes that the clock state is a result of\n-//   release-store operation by the thread with release_store_tid_ index.\n-// release_store_reused_ - reuse count of release_store_tid_.\n-\n-namespace __tsan {\n-\n-static atomic_uint32_t *ref_ptr(ClockBlock *cb) {\n-  return reinterpret_cast<atomic_uint32_t *>(&cb->table[ClockBlock::kRefIdx]);\n-}\n-\n-// Drop reference to the first level block idx.\n-static void UnrefClockBlock(ClockCache *c, u32 idx, uptr blocks) {\n-  ClockBlock *cb = ctx->clock_alloc.Map(idx);\n-  atomic_uint32_t *ref = ref_ptr(cb);\n-  u32 v = atomic_load(ref, memory_order_acquire);\n-  for (;;) {\n-    CHECK_GT(v, 0);\n-    if (v == 1)\n-      break;\n-    if (atomic_compare_exchange_strong(ref, &v, v - 1, memory_order_acq_rel))\n-      return;\n-  }\n-  // First level block owns second level blocks, so them as well.\n-  for (uptr i = 0; i < blocks; i++)\n-    ctx->clock_alloc.Free(c, cb->table[ClockBlock::kBlockIdx - i]);\n-  ctx->clock_alloc.Free(c, idx);\n-}\n-\n-ThreadClock::ThreadClock(unsigned tid, unsigned reused)\n-    : tid_(tid)\n-    , reused_(reused + 1)  // 0 has special meaning\n-    , last_acquire_()\n-    , global_acquire_()\n-    , cached_idx_()\n-    , cached_size_()\n-    , cached_blocks_() {\n-  CHECK_LT(tid, kMaxTidInClock);\n-  CHECK_EQ(reused_, ((u64)reused_ << kClkBits) >> kClkBits);\n-  nclk_ = tid_ + 1;\n-  internal_memset(clk_, 0, sizeof(clk_));\n-}\n-\n-void ThreadClock::ResetCached(ClockCache *c) {\n-  if (cached_idx_) {\n-    UnrefClockBlock(c, cached_idx_, cached_blocks_);\n-    cached_idx_ = 0;\n-    cached_size_ = 0;\n-    cached_blocks_ = 0;\n-  }\n-}\n-\n-void ThreadClock::acquire(ClockCache *c, SyncClock *src) {\n-  DCHECK_LE(nclk_, kMaxTid);\n-  DCHECK_LE(src->size_, kMaxTid);\n-\n-  // Check if it's empty -> no need to do anything.\n-  const uptr nclk = src->size_;\n-  if (nclk == 0)\n-    return;\n-\n-  bool acquired = false;\n-  for (unsigned i = 0; i < kDirtyTids; i++) {\n-    SyncClock::Dirty dirty = src->dirty_[i];\n-    unsigned tid = dirty.tid();\n-    if (tid != kInvalidTid) {\n-      if (clk_[tid] < dirty.epoch) {\n-        clk_[tid] = dirty.epoch;\n-        acquired = true;\n-      }\n-    }\n-  }\n-\n-  // Check if we've already acquired src after the last release operation on src\n-  if (tid_ >= nclk || src->elem(tid_).reused != reused_) {\n-    // O(N) acquire.\n-    nclk_ = max(nclk_, nclk);\n-    u64 *dst_pos = &clk_[0];\n-    for (ClockElem &src_elem : *src) {\n-      u64 epoch = src_elem.epoch;\n-      if (*dst_pos < epoch) {\n-        *dst_pos = epoch;\n-        acquired = true;\n-      }\n-      dst_pos++;\n-    }\n-\n-    // Remember that this thread has acquired this clock.\n-    if (nclk > tid_)\n-      src->elem(tid_).reused = reused_;\n-  }\n-\n-  if (acquired) {\n-    last_acquire_ = clk_[tid_];\n-    ResetCached(c);\n-  }\n-}\n-\n-void ThreadClock::releaseStoreAcquire(ClockCache *c, SyncClock *sc) {\n-  DCHECK_LE(nclk_, kMaxTid);\n-  DCHECK_LE(sc->size_, kMaxTid);\n-\n-  if (sc->size_ == 0) {\n-    // ReleaseStore will correctly set release_store_tid_,\n-    // which can be important for future operations.\n-    ReleaseStore(c, sc);\n-    return;\n-  }\n-\n-  nclk_ = max(nclk_, (uptr) sc->size_);\n-\n-  // Check if we need to resize sc.\n-  if (sc->size_ < nclk_)\n-    sc->Resize(c, nclk_);\n-\n-  bool acquired = false;\n-\n-  sc->Unshare(c);\n-  // Update sc->clk_.\n-  sc->FlushDirty();\n-  uptr i = 0;\n-  for (ClockElem &ce : *sc) {\n-    u64 tmp = clk_[i];\n-    if (clk_[i] < ce.epoch) {\n-      clk_[i] = ce.epoch;\n-      acquired = true;\n-    }\n-    ce.epoch = tmp;\n-    ce.reused = 0;\n-    i++;\n-  }\n-  sc->release_store_tid_ = kInvalidTid;\n-  sc->release_store_reused_ = 0;\n-\n-  if (acquired) {\n-    last_acquire_ = clk_[tid_];\n-    ResetCached(c);\n-  }\n-}\n-\n-void ThreadClock::release(ClockCache *c, SyncClock *dst) {\n-  DCHECK_LE(nclk_, kMaxTid);\n-  DCHECK_LE(dst->size_, kMaxTid);\n-\n-  if (dst->size_ == 0) {\n-    // ReleaseStore will correctly set release_store_tid_,\n-    // which can be important for future operations.\n-    ReleaseStore(c, dst);\n-    return;\n-  }\n-\n-  // Check if we need to resize dst.\n-  if (dst->size_ < nclk_)\n-    dst->Resize(c, nclk_);\n-\n-  // Check if we had not acquired anything from other threads\n-  // since the last release on dst. If so, we need to update\n-  // only dst->elem(tid_).\n-  if (!HasAcquiredAfterRelease(dst)) {\n-    UpdateCurrentThread(c, dst);\n-    if (dst->release_store_tid_ != tid_ ||\n-        dst->release_store_reused_ != reused_)\n-      dst->release_store_tid_ = kInvalidTid;\n-    return;\n-  }\n-\n-  // O(N) release.\n-  dst->Unshare(c);\n-  // First, remember whether we've acquired dst.\n-  bool acquired = IsAlreadyAcquired(dst);\n-  // Update dst->clk_.\n-  dst->FlushDirty();\n-  uptr i = 0;\n-  for (ClockElem &ce : *dst) {\n-    ce.epoch = max(ce.epoch, clk_[i]);\n-    ce.reused = 0;\n-    i++;\n-  }\n-  // Clear 'acquired' flag in the remaining elements.\n-  dst->release_store_tid_ = kInvalidTid;\n-  dst->release_store_reused_ = 0;\n-  // If we've acquired dst, remember this fact,\n-  // so that we don't need to acquire it on next acquire.\n-  if (acquired)\n-    dst->elem(tid_).reused = reused_;\n-}\n-\n-void ThreadClock::ReleaseStore(ClockCache *c, SyncClock *dst) {\n-  DCHECK_LE(nclk_, kMaxTid);\n-  DCHECK_LE(dst->size_, kMaxTid);\n-\n-  if (dst->size_ == 0 && cached_idx_ != 0) {\n-    // Reuse the cached clock.\n-    // Note: we could reuse/cache the cached clock in more cases:\n-    // we could update the existing clock and cache it, or replace it with the\n-    // currently cached clock and release the old one. And for a shared\n-    // existing clock, we could replace it with the currently cached;\n-    // or unshare, update and cache. But, for simplicity, we currently reuse\n-    // cached clock only when the target clock is empty.\n-    dst->tab_ = ctx->clock_alloc.Map(cached_idx_);\n-    dst->tab_idx_ = cached_idx_;\n-    dst->size_ = cached_size_;\n-    dst->blocks_ = cached_blocks_;\n-    CHECK_EQ(dst->dirty_[0].tid(), kInvalidTid);\n-    // The cached clock is shared (immutable),\n-    // so this is where we store the current clock.\n-    dst->dirty_[0].set_tid(tid_);\n-    dst->dirty_[0].epoch = clk_[tid_];\n-    dst->release_store_tid_ = tid_;\n-    dst->release_store_reused_ = reused_;\n-    // Remember that we don't need to acquire it in future.\n-    dst->elem(tid_).reused = reused_;\n-    // Grab a reference.\n-    atomic_fetch_add(ref_ptr(dst->tab_), 1, memory_order_relaxed);\n-    return;\n-  }\n-\n-  // Check if we need to resize dst.\n-  if (dst->size_ < nclk_)\n-    dst->Resize(c, nclk_);\n-\n-  if (dst->release_store_tid_ == tid_ &&\n-      dst->release_store_reused_ == reused_ &&\n-      !HasAcquiredAfterRelease(dst)) {\n-    UpdateCurrentThread(c, dst);\n-    return;\n-  }\n-\n-  // O(N) release-store.\n-  dst->Unshare(c);\n-  // Note: dst can be larger than this ThreadClock.\n-  // This is fine since clk_ beyond size is all zeros.\n-  uptr i = 0;\n-  for (ClockElem &ce : *dst) {\n-    ce.epoch = clk_[i];\n-    ce.reused = 0;\n-    i++;\n-  }\n-  for (uptr i = 0; i < kDirtyTids; i++) dst->dirty_[i].set_tid(kInvalidTid);\n-  dst->release_store_tid_ = tid_;\n-  dst->release_store_reused_ = reused_;\n-  // Remember that we don't need to acquire it in future.\n-  dst->elem(tid_).reused = reused_;\n-\n-  // If the resulting clock is cachable, cache it for future release operations.\n-  // The clock is always cachable if we released to an empty sync object.\n-  if (cached_idx_ == 0 && dst->Cachable()) {\n-    // Grab a reference to the ClockBlock.\n-    atomic_uint32_t *ref = ref_ptr(dst->tab_);\n-    if (atomic_load(ref, memory_order_acquire) == 1)\n-      atomic_store_relaxed(ref, 2);\n-    else\n-      atomic_fetch_add(ref_ptr(dst->tab_), 1, memory_order_relaxed);\n-    cached_idx_ = dst->tab_idx_;\n-    cached_size_ = dst->size_;\n-    cached_blocks_ = dst->blocks_;\n-  }\n-}\n-\n-void ThreadClock::acq_rel(ClockCache *c, SyncClock *dst) {\n-  acquire(c, dst);\n-  ReleaseStore(c, dst);\n-}\n-\n-// Updates only single element related to the current thread in dst->clk_.\n-void ThreadClock::UpdateCurrentThread(ClockCache *c, SyncClock *dst) const {\n-  // Update the threads time, but preserve 'acquired' flag.\n-  for (unsigned i = 0; i < kDirtyTids; i++) {\n-    SyncClock::Dirty *dirty = &dst->dirty_[i];\n-    const unsigned tid = dirty->tid();\n-    if (tid == tid_ || tid == kInvalidTid) {\n-      dirty->set_tid(tid_);\n-      dirty->epoch = clk_[tid_];\n-      return;\n-    }\n-  }\n-  // Reset all 'acquired' flags, O(N).\n-  // We are going to touch dst elements, so we need to unshare it.\n-  dst->Unshare(c);\n-  dst->elem(tid_).epoch = clk_[tid_];\n-  for (uptr i = 0; i < dst->size_; i++)\n-    dst->elem(i).reused = 0;\n-  dst->FlushDirty();\n-}\n-\n-// Checks whether the current thread has already acquired src.\n-bool ThreadClock::IsAlreadyAcquired(const SyncClock *src) const {\n-  if (src->elem(tid_).reused != reused_)\n-    return false;\n-  for (unsigned i = 0; i < kDirtyTids; i++) {\n-    SyncClock::Dirty dirty = src->dirty_[i];\n-    if (dirty.tid() != kInvalidTid) {\n-      if (clk_[dirty.tid()] < dirty.epoch)\n-        return false;\n-    }\n-  }\n-  return true;\n-}\n-\n-// Checks whether the current thread has acquired anything\n-// from other clocks after releasing to dst (directly or indirectly).\n-bool ThreadClock::HasAcquiredAfterRelease(const SyncClock *dst) const {\n-  const u64 my_epoch = dst->elem(tid_).epoch;\n-  return my_epoch <= last_acquire_ ||\n-      my_epoch <= atomic_load_relaxed(&global_acquire_);\n-}\n-\n-// Sets a single element in the vector clock.\n-// This function is called only from weird places like AcquireGlobal.\n-void ThreadClock::set(ClockCache *c, unsigned tid, u64 v) {\n-  DCHECK_LT(tid, kMaxTid);\n-  DCHECK_GE(v, clk_[tid]);\n-  clk_[tid] = v;\n-  if (nclk_ <= tid)\n-    nclk_ = tid + 1;\n-  last_acquire_ = clk_[tid_];\n-  ResetCached(c);\n-}\n-\n-void ThreadClock::DebugDump(int(*printf)(const char *s, ...)) {\n-  printf(\"clock=[\");\n-  for (uptr i = 0; i < nclk_; i++)\n-    printf(\"%s%llu\", i == 0 ? \"\" : \",\", clk_[i]);\n-  printf(\"] tid=%u/%u last_acq=%llu\", tid_, reused_, last_acquire_);\n-}\n-\n-SyncClock::SyncClock() {\n-  ResetImpl();\n-}\n-\n-SyncClock::~SyncClock() {\n-  // Reset must be called before dtor.\n-  CHECK_EQ(size_, 0);\n-  CHECK_EQ(blocks_, 0);\n-  CHECK_EQ(tab_, 0);\n-  CHECK_EQ(tab_idx_, 0);\n-}\n-\n-void SyncClock::Reset(ClockCache *c) {\n-  if (size_)\n-    UnrefClockBlock(c, tab_idx_, blocks_);\n-  ResetImpl();\n-}\n-\n-void SyncClock::ResetImpl() {\n-  tab_ = 0;\n-  tab_idx_ = 0;\n-  size_ = 0;\n-  blocks_ = 0;\n-  release_store_tid_ = kInvalidTid;\n-  release_store_reused_ = 0;\n-  for (uptr i = 0; i < kDirtyTids; i++) dirty_[i].set_tid(kInvalidTid);\n-}\n-\n-void SyncClock::Resize(ClockCache *c, uptr nclk) {\n-  Unshare(c);\n-  if (nclk <= capacity()) {\n-    // Memory is already allocated, just increase the size.\n-    size_ = nclk;\n-    return;\n-  }\n-  if (size_ == 0) {\n-    // Grow from 0 to one-level table.\n-    CHECK_EQ(size_, 0);\n-    CHECK_EQ(blocks_, 0);\n-    CHECK_EQ(tab_, 0);\n-    CHECK_EQ(tab_idx_, 0);\n-    tab_idx_ = ctx->clock_alloc.Alloc(c);\n-    tab_ = ctx->clock_alloc.Map(tab_idx_);\n-    internal_memset(tab_, 0, sizeof(*tab_));\n-    atomic_store_relaxed(ref_ptr(tab_), 1);\n-    size_ = 1;\n-  } else if (size_ > blocks_ * ClockBlock::kClockCount) {\n-    u32 idx = ctx->clock_alloc.Alloc(c);\n-    ClockBlock *new_cb = ctx->clock_alloc.Map(idx);\n-    uptr top = size_ - blocks_ * ClockBlock::kClockCount;\n-    CHECK_LT(top, ClockBlock::kClockCount);\n-    const uptr move = top * sizeof(tab_->clock[0]);\n-    internal_memcpy(&new_cb->clock[0], tab_->clock, move);\n-    internal_memset(&new_cb->clock[top], 0, sizeof(*new_cb) - move);\n-    internal_memset(tab_->clock, 0, move);\n-    append_block(idx);\n-  }\n-  // At this point we have first level table allocated and all clock elements\n-  // are evacuated from it to a second level block.\n-  // Add second level tables as necessary.\n-  while (nclk > capacity()) {\n-    u32 idx = ctx->clock_alloc.Alloc(c);\n-    ClockBlock *cb = ctx->clock_alloc.Map(idx);\n-    internal_memset(cb, 0, sizeof(*cb));\n-    append_block(idx);\n-  }\n-  size_ = nclk;\n-}\n-\n-// Flushes all dirty elements into the main clock array.\n-void SyncClock::FlushDirty() {\n-  for (unsigned i = 0; i < kDirtyTids; i++) {\n-    Dirty *dirty = &dirty_[i];\n-    if (dirty->tid() != kInvalidTid) {\n-      CHECK_LT(dirty->tid(), size_);\n-      elem(dirty->tid()).epoch = dirty->epoch;\n-      dirty->set_tid(kInvalidTid);\n-    }\n-  }\n-}\n-\n-bool SyncClock::IsShared() const {\n-  if (size_ == 0)\n-    return false;\n-  atomic_uint32_t *ref = ref_ptr(tab_);\n-  u32 v = atomic_load(ref, memory_order_acquire);\n-  CHECK_GT(v, 0);\n-  return v > 1;\n-}\n-\n-// Unshares the current clock if it's shared.\n-// Shared clocks are immutable, so they need to be unshared before any updates.\n-// Note: this does not apply to dirty entries as they are not shared.\n-void SyncClock::Unshare(ClockCache *c) {\n-  if (!IsShared())\n-    return;\n-  // First, copy current state into old.\n-  SyncClock old;\n-  old.tab_ = tab_;\n-  old.tab_idx_ = tab_idx_;\n-  old.size_ = size_;\n-  old.blocks_ = blocks_;\n-  old.release_store_tid_ = release_store_tid_;\n-  old.release_store_reused_ = release_store_reused_;\n-  for (unsigned i = 0; i < kDirtyTids; i++)\n-    old.dirty_[i] = dirty_[i];\n-  // Then, clear current object.\n-  ResetImpl();\n-  // Allocate brand new clock in the current object.\n-  Resize(c, old.size_);\n-  // Now copy state back into this object.\n-  Iter old_iter(&old);\n-  for (ClockElem &ce : *this) {\n-    ce = *old_iter;\n-    ++old_iter;\n-  }\n-  release_store_tid_ = old.release_store_tid_;\n-  release_store_reused_ = old.release_store_reused_;\n-  for (unsigned i = 0; i < kDirtyTids; i++)\n-    dirty_[i] = old.dirty_[i];\n-  // Drop reference to old and delete if necessary.\n-  old.Reset(c);\n-}\n-\n-// Can we cache this clock for future release operations?\n-ALWAYS_INLINE bool SyncClock::Cachable() const {\n-  if (size_ == 0)\n-    return false;\n-  for (unsigned i = 0; i < kDirtyTids; i++) {\n-    if (dirty_[i].tid() != kInvalidTid)\n-      return false;\n-  }\n-  return atomic_load_relaxed(ref_ptr(tab_)) == 1;\n-}\n-\n-// elem linearizes the two-level structure into linear array.\n-// Note: this is used only for one time accesses, vector operations use\n-// the iterator as it is much faster.\n-ALWAYS_INLINE ClockElem &SyncClock::elem(unsigned tid) const {\n-  DCHECK_LT(tid, size_);\n-  const uptr block = tid / ClockBlock::kClockCount;\n-  DCHECK_LE(block, blocks_);\n-  tid %= ClockBlock::kClockCount;\n-  if (block == blocks_)\n-    return tab_->clock[tid];\n-  u32 idx = get_block(block);\n-  ClockBlock *cb = ctx->clock_alloc.Map(idx);\n-  return cb->clock[tid];\n-}\n-\n-ALWAYS_INLINE uptr SyncClock::capacity() const {\n-  if (size_ == 0)\n-    return 0;\n-  uptr ratio = sizeof(ClockBlock::clock[0]) / sizeof(ClockBlock::table[0]);\n-  // How many clock elements we can fit into the first level block.\n-  // +1 for ref counter.\n-  uptr top = ClockBlock::kClockCount - RoundUpTo(blocks_ + 1, ratio) / ratio;\n-  return blocks_ * ClockBlock::kClockCount + top;\n-}\n-\n-ALWAYS_INLINE u32 SyncClock::get_block(uptr bi) const {\n-  DCHECK(size_);\n-  DCHECK_LT(bi, blocks_);\n-  return tab_->table[ClockBlock::kBlockIdx - bi];\n-}\n-\n-ALWAYS_INLINE void SyncClock::append_block(u32 idx) {\n-  uptr bi = blocks_++;\n-  CHECK_EQ(get_block(bi), 0);\n-  tab_->table[ClockBlock::kBlockIdx - bi] = idx;\n-}\n-\n-// Used only by tests.\n-u64 SyncClock::get(unsigned tid) const {\n-  for (unsigned i = 0; i < kDirtyTids; i++) {\n-    Dirty dirty = dirty_[i];\n-    if (dirty.tid() == tid)\n-      return dirty.epoch;\n-  }\n-  return elem(tid).epoch;\n-}\n-\n-// Used only by Iter test.\n-u64 SyncClock::get_clean(unsigned tid) const {\n-  return elem(tid).epoch;\n-}\n-\n-void SyncClock::DebugDump(int(*printf)(const char *s, ...)) {\n-  printf(\"clock=[\");\n-  for (uptr i = 0; i < size_; i++)\n-    printf(\"%s%llu\", i == 0 ? \"\" : \",\", elem(i).epoch);\n-  printf(\"] reused=[\");\n-  for (uptr i = 0; i < size_; i++)\n-    printf(\"%s%llu\", i == 0 ? \"\" : \",\", elem(i).reused);\n-  printf(\"] release_store_tid=%d/%d dirty_tids=%d[%llu]/%d[%llu]\",\n-         release_store_tid_, release_store_reused_, dirty_[0].tid(),\n-         dirty_[0].epoch, dirty_[1].tid(), dirty_[1].epoch);\n-}\n-\n-void SyncClock::Iter::Next() {\n-  // Finished with the current block, move on to the next one.\n-  block_++;\n-  if (block_ < parent_->blocks_) {\n-    // Iterate over the next second level block.\n-    u32 idx = parent_->get_block(block_);\n-    ClockBlock *cb = ctx->clock_alloc.Map(idx);\n-    pos_ = &cb->clock[0];\n-    end_ = pos_ + min(parent_->size_ - block_ * ClockBlock::kClockCount,\n-        ClockBlock::kClockCount);\n-    return;\n-  }\n-  if (block_ == parent_->blocks_ &&\n-      parent_->size_ > parent_->blocks_ * ClockBlock::kClockCount) {\n-    // Iterate over elements in the first level block.\n-    pos_ = &parent_->tab_->clock[0];\n-    end_ = pos_ + min(parent_->size_ - block_ * ClockBlock::kClockCount,\n-        ClockBlock::kClockCount);\n-    return;\n-  }\n-  parent_ = nullptr;  // denotes end\n-}\n-}  // namespace __tsan"}, {"sha": "11cbc0c0b86b64c6a30486d1d59f1e1ff283fecb", "filename": "libsanitizer/tsan/tsan_clock.h", "status": "removed", "additions": 0, "deletions": 293, "changes": 293, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2285af309000b74da0f7dc756a0b55e5f0b1b56/libsanitizer%2Ftsan%2Ftsan_clock.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2285af309000b74da0f7dc756a0b55e5f0b1b56/libsanitizer%2Ftsan%2Ftsan_clock.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_clock.h?ref=e2285af309000b74da0f7dc756a0b55e5f0b1b56", "patch": "@@ -1,293 +0,0 @@\n-//===-- tsan_clock.h --------------------------------------------*- C++ -*-===//\n-//\n-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n-// See https://llvm.org/LICENSE.txt for license information.\n-// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n-//\n-//===----------------------------------------------------------------------===//\n-//\n-// This file is a part of ThreadSanitizer (TSan), a race detector.\n-//\n-//===----------------------------------------------------------------------===//\n-#ifndef TSAN_CLOCK_H\n-#define TSAN_CLOCK_H\n-\n-#include \"tsan_defs.h\"\n-#include \"tsan_dense_alloc.h\"\n-\n-namespace __tsan {\n-\n-typedef DenseSlabAlloc<ClockBlock, 1 << 22, 1 << 10> ClockAlloc;\n-typedef DenseSlabAllocCache ClockCache;\n-\n-// The clock that lives in sync variables (mutexes, atomics, etc).\n-class SyncClock {\n- public:\n-  SyncClock();\n-  ~SyncClock();\n-\n-  uptr size() const;\n-\n-  // These are used only in tests.\n-  u64 get(unsigned tid) const;\n-  u64 get_clean(unsigned tid) const;\n-\n-  void Resize(ClockCache *c, uptr nclk);\n-  void Reset(ClockCache *c);\n-\n-  void DebugDump(int(*printf)(const char *s, ...));\n-\n-  // Clock element iterator.\n-  // Note: it iterates only over the table without regard to dirty entries.\n-  class Iter {\n-   public:\n-    explicit Iter(SyncClock* parent);\n-    Iter& operator++();\n-    bool operator!=(const Iter& other);\n-    ClockElem &operator*();\n-\n-   private:\n-    SyncClock *parent_;\n-    // [pos_, end_) is the current continuous range of clock elements.\n-    ClockElem *pos_;\n-    ClockElem *end_;\n-    int block_;  // Current number of second level block.\n-\n-    NOINLINE void Next();\n-  };\n-\n-  Iter begin();\n-  Iter end();\n-\n- private:\n-  friend class ThreadClock;\n-  friend class Iter;\n-  static const uptr kDirtyTids = 2;\n-\n-  struct Dirty {\n-    u32 tid() const { return tid_ == kShortInvalidTid ? kInvalidTid : tid_; }\n-    void set_tid(u32 tid) {\n-      tid_ = tid == kInvalidTid ? kShortInvalidTid : tid;\n-    }\n-    u64 epoch : kClkBits;\n-\n-   private:\n-    // Full kInvalidTid won't fit into Dirty::tid.\n-    static const u64 kShortInvalidTid = (1ull << (64 - kClkBits)) - 1;\n-    u64 tid_ : 64 - kClkBits;  // kInvalidId if not active\n-  };\n-\n-  static_assert(sizeof(Dirty) == 8, \"Dirty is not 64bit\");\n-\n-  unsigned release_store_tid_;\n-  unsigned release_store_reused_;\n-  Dirty dirty_[kDirtyTids];\n-  // If size_ is 0, tab_ is nullptr.\n-  // If size <= 64 (kClockCount), tab_ contains pointer to an array with\n-  // 64 ClockElem's (ClockBlock::clock).\n-  // Otherwise, tab_ points to an array with up to 127 u32 elements,\n-  // each pointing to the second-level 512b block with 64 ClockElem's.\n-  // Unused space in the first level ClockBlock is used to store additional\n-  // clock elements.\n-  // The last u32 element in the first level ClockBlock is always used as\n-  // reference counter.\n-  //\n-  // See the following scheme for details.\n-  // All memory blocks are 512 bytes (allocated from ClockAlloc).\n-  // Clock (clk) elements are 64 bits.\n-  // Idx and ref are 32 bits.\n-  //\n-  // tab_\n-  //    |\n-  //    \\/\n-  //    +----------------------------------------------------+\n-  //    | clk128 | clk129 | ...unused... | idx1 | idx0 | ref |\n-  //    +----------------------------------------------------+\n-  //                                        |      |\n-  //                                        |      \\/\n-  //                                        |      +----------------+\n-  //                                        |      | clk0 ... clk63 |\n-  //                                        |      +----------------+\n-  //                                        \\/\n-  //                                        +------------------+\n-  //                                        | clk64 ... clk127 |\n-  //                                        +------------------+\n-  //\n-  // Note: dirty entries, if active, always override what's stored in the clock.\n-  ClockBlock *tab_;\n-  u32 tab_idx_;\n-  u16 size_;\n-  u16 blocks_;  // Number of second level blocks.\n-\n-  void Unshare(ClockCache *c);\n-  bool IsShared() const;\n-  bool Cachable() const;\n-  void ResetImpl();\n-  void FlushDirty();\n-  uptr capacity() const;\n-  u32 get_block(uptr bi) const;\n-  void append_block(u32 idx);\n-  ClockElem &elem(unsigned tid) const;\n-};\n-\n-// The clock that lives in threads.\n-class ThreadClock {\n- public:\n-  typedef DenseSlabAllocCache Cache;\n-\n-  explicit ThreadClock(unsigned tid, unsigned reused = 0);\n-\n-  u64 get(unsigned tid) const;\n-  void set(ClockCache *c, unsigned tid, u64 v);\n-  void set(u64 v);\n-  void tick();\n-  uptr size() const;\n-\n-  void acquire(ClockCache *c, SyncClock *src);\n-  void releaseStoreAcquire(ClockCache *c, SyncClock *src);\n-  void release(ClockCache *c, SyncClock *dst);\n-  void acq_rel(ClockCache *c, SyncClock *dst);\n-  void ReleaseStore(ClockCache *c, SyncClock *dst);\n-  void ResetCached(ClockCache *c);\n-  void NoteGlobalAcquire(u64 v);\n-\n-  void DebugReset();\n-  void DebugDump(int(*printf)(const char *s, ...));\n-\n- private:\n-  static const uptr kDirtyTids = SyncClock::kDirtyTids;\n-  // Index of the thread associated with he clock (\"current thread\").\n-  const unsigned tid_;\n-  const unsigned reused_;  // tid_ reuse count.\n-  // Current thread time when it acquired something from other threads.\n-  u64 last_acquire_;\n-\n-  // Last time another thread has done a global acquire of this thread's clock.\n-  // It helps to avoid problem described in:\n-  // https://github.com/golang/go/issues/39186\n-  // See test/tsan/java_finalizer2.cpp for a regression test.\n-  // Note the failuire is _extremely_ hard to hit, so if you are trying\n-  // to reproduce it, you may want to run something like:\n-  // $ go get golang.org/x/tools/cmd/stress\n-  // $ stress -p=64 ./a.out\n-  //\n-  // The crux of the problem is roughly as follows.\n-  // A number of O(1) optimizations in the clocks algorithm assume proper\n-  // transitive cumulative propagation of clock values. The AcquireGlobal\n-  // operation may produce an inconsistent non-linearazable view of\n-  // thread clocks. Namely, it may acquire a later value from a thread\n-  // with a higher ID, but fail to acquire an earlier value from a thread\n-  // with a lower ID. If a thread that executed AcquireGlobal then releases\n-  // to a sync clock, it will spoil the sync clock with the inconsistent\n-  // values. If another thread later releases to the sync clock, the optimized\n-  // algorithm may break.\n-  //\n-  // The exact sequence of events that leads to the failure.\n-  // - thread 1 executes AcquireGlobal\n-  // - thread 1 acquires value 1 for thread 2\n-  // - thread 2 increments clock to 2\n-  // - thread 2 releases to sync object 1\n-  // - thread 3 at time 1\n-  // - thread 3 acquires from sync object 1\n-  // - thread 3 increments clock to 2\n-  // - thread 1 acquires value 2 for thread 3\n-  // - thread 1 releases to sync object 2\n-  // - sync object 2 clock has 1 for thread 2 and 2 for thread 3\n-  // - thread 3 releases to sync object 2\n-  // - thread 3 sees value 2 in the clock for itself\n-  //   and decides that it has already released to the clock\n-  //   and did not acquire anything from other threads after that\n-  //   (the last_acquire_ check in release operation)\n-  // - thread 3 does not update the value for thread 2 in the clock from 1 to 2\n-  // - thread 4 acquires from sync object 2\n-  // - thread 4 detects a false race with thread 2\n-  //   as it should have been synchronized with thread 2 up to time 2,\n-  //   but because of the broken clock it is now synchronized only up to time 1\n-  //\n-  // The global_acquire_ value helps to prevent this scenario.\n-  // Namely, thread 3 will not trust any own clock values up to global_acquire_\n-  // for the purposes of the last_acquire_ optimization.\n-  atomic_uint64_t global_acquire_;\n-\n-  // Cached SyncClock (without dirty entries and release_store_tid_).\n-  // We reuse it for subsequent store-release operations without intervening\n-  // acquire operations. Since it is shared (and thus constant), clock value\n-  // for the current thread is then stored in dirty entries in the SyncClock.\n-  // We host a reference to the table while it is cached here.\n-  u32 cached_idx_;\n-  u16 cached_size_;\n-  u16 cached_blocks_;\n-\n-  // Number of active elements in the clk_ table (the rest is zeros).\n-  uptr nclk_;\n-  u64 clk_[kMaxTidInClock];  // Fixed size vector clock.\n-\n-  bool IsAlreadyAcquired(const SyncClock *src) const;\n-  bool HasAcquiredAfterRelease(const SyncClock *dst) const;\n-  void UpdateCurrentThread(ClockCache *c, SyncClock *dst) const;\n-};\n-\n-ALWAYS_INLINE u64 ThreadClock::get(unsigned tid) const {\n-  DCHECK_LT(tid, kMaxTidInClock);\n-  return clk_[tid];\n-}\n-\n-ALWAYS_INLINE void ThreadClock::set(u64 v) {\n-  DCHECK_GE(v, clk_[tid_]);\n-  clk_[tid_] = v;\n-}\n-\n-ALWAYS_INLINE void ThreadClock::tick() {\n-  clk_[tid_]++;\n-}\n-\n-ALWAYS_INLINE uptr ThreadClock::size() const {\n-  return nclk_;\n-}\n-\n-ALWAYS_INLINE void ThreadClock::NoteGlobalAcquire(u64 v) {\n-  // Here we rely on the fact that AcquireGlobal is protected by\n-  // ThreadRegistryLock, thus only one thread at a time executes it\n-  // and values passed to this function should not go backwards.\n-  CHECK_LE(atomic_load_relaxed(&global_acquire_), v);\n-  atomic_store_relaxed(&global_acquire_, v);\n-}\n-\n-ALWAYS_INLINE SyncClock::Iter SyncClock::begin() {\n-  return Iter(this);\n-}\n-\n-ALWAYS_INLINE SyncClock::Iter SyncClock::end() {\n-  return Iter(nullptr);\n-}\n-\n-ALWAYS_INLINE uptr SyncClock::size() const {\n-  return size_;\n-}\n-\n-ALWAYS_INLINE SyncClock::Iter::Iter(SyncClock* parent)\n-    : parent_(parent)\n-    , pos_(nullptr)\n-    , end_(nullptr)\n-    , block_(-1) {\n-  if (parent)\n-    Next();\n-}\n-\n-ALWAYS_INLINE SyncClock::Iter& SyncClock::Iter::operator++() {\n-  pos_++;\n-  if (UNLIKELY(pos_ >= end_))\n-    Next();\n-  return *this;\n-}\n-\n-ALWAYS_INLINE bool SyncClock::Iter::operator!=(const SyncClock::Iter& other) {\n-  return parent_ != other.parent_;\n-}\n-\n-ALWAYS_INLINE ClockElem &SyncClock::Iter::operator*() {\n-  return *pos_;\n-}\n-}  // namespace __tsan\n-\n-#endif  // TSAN_CLOCK_H"}, {"sha": "1e61c31c5a970cc2daee6bacfa6b6259328e389c", "filename": "libsanitizer/tsan/tsan_debugging.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_debugging.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_debugging.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_debugging.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -157,7 +157,7 @@ int __tsan_get_report_mutex(void *report, uptr idx, uptr *mutex_id, void **addr,\n   ReportMutex *mutex = rep->mutexes[idx];\n   *mutex_id = mutex->id;\n   *addr = (void *)mutex->addr;\n-  *destroyed = mutex->destroyed;\n+  *destroyed = false;\n   if (mutex->stack) CopyTrace(mutex->stack->frames, trace, trace_size);\n   return 1;\n }"}, {"sha": "1ffa3d6aec40bd894b1b896b3068b6b5b551a305", "filename": "libsanitizer/tsan/tsan_defs.h", "status": "modified", "additions": 18, "deletions": 36, "changes": 54, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_defs.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_defs.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_defs.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -63,41 +63,14 @@ enum class Epoch : u16 {};\n constexpr uptr kEpochBits = 14;\n constexpr Epoch kEpochZero = static_cast<Epoch>(0);\n constexpr Epoch kEpochOver = static_cast<Epoch>(1 << kEpochBits);\n+constexpr Epoch kEpochLast = static_cast<Epoch>((1 << kEpochBits) - 1);\n \n-const int kClkBits = 42;\n-const unsigned kMaxTidReuse = (1 << (64 - kClkBits)) - 1;\n-\n-struct ClockElem {\n-  u64 epoch  : kClkBits;\n-  u64 reused : 64 - kClkBits;  // tid reuse count\n-};\n-\n-struct ClockBlock {\n-  static const uptr kSize = 512;\n-  static const uptr kTableSize = kSize / sizeof(u32);\n-  static const uptr kClockCount = kSize / sizeof(ClockElem);\n-  static const uptr kRefIdx = kTableSize - 1;\n-  static const uptr kBlockIdx = kTableSize - 2;\n-\n-  union {\n-    u32       table[kTableSize];\n-    ClockElem clock[kClockCount];\n-  };\n+inline Epoch EpochInc(Epoch epoch) {\n+  return static_cast<Epoch>(static_cast<u16>(epoch) + 1);\n+}\n \n-  ClockBlock() {\n-  }\n-};\n+inline bool EpochOverflow(Epoch epoch) { return epoch == kEpochOver; }\n \n-const int kTidBits = 13;\n-// Reduce kMaxTid by kClockCount because one slot in ClockBlock table is\n-// occupied by reference counter, so total number of elements we can store\n-// in SyncClock is kClockCount * (kTableSize - 1).\n-const unsigned kMaxTid = (1 << kTidBits) - ClockBlock::kClockCount;\n-#if !SANITIZER_GO\n-const unsigned kMaxTidInClock = kMaxTid * 2;  // This includes msb 'freed' bit.\n-#else\n-const unsigned kMaxTidInClock = kMaxTid;  // Go does not track freed memory.\n-#endif\n const uptr kShadowStackSize = 64 * 1024;\n \n // Count of shadow values in a shadow cell.\n@@ -107,7 +80,7 @@ const uptr kShadowCnt = 4;\n const uptr kShadowCell = 8;\n \n // Single shadow value.\n-typedef u64 RawShadow;\n+enum class RawShadow : u32 {};\n const uptr kShadowSize = sizeof(RawShadow);\n \n // Shadow memory is kShadowMultiplier times larger than user memory.\n@@ -184,10 +157,13 @@ MD5Hash md5_hash(const void *data, uptr size);\n struct Processor;\n struct ThreadState;\n class ThreadContext;\n+struct TidSlot;\n struct Context;\n struct ReportStack;\n class ReportDesc;\n class RegionAlloc;\n+struct Trace;\n+struct TracePart;\n \n typedef uptr AccessType;\n \n@@ -198,6 +174,9 @@ enum : AccessType {\n   kAccessVptr = 1 << 2,  // read or write of an object virtual table pointer\n   kAccessFree = 1 << 3,  // synthetic memory access during memory freeing\n   kAccessExternalPC = 1 << 4,  // access PC can have kExternalPCBit set\n+  kAccessCheckOnly = 1 << 5,   // check for races, but don't store\n+  kAccessNoRodata = 1 << 6,    // don't check for .rodata marker\n+  kAccessSlotLocked = 1 << 7,  // memory access with TidSlot locked\n };\n \n // Descriptor of user's memory block.\n@@ -219,15 +198,18 @@ enum ExternalTag : uptr {\n   // as 16-bit values, see tsan_defs.h.\n };\n \n-enum MutexType {\n-  MutexTypeTrace = MutexLastCommon,\n-  MutexTypeReport,\n+enum {\n+  MutexTypeReport = MutexLastCommon,\n   MutexTypeSyncVar,\n   MutexTypeAnnotations,\n   MutexTypeAtExit,\n   MutexTypeFired,\n   MutexTypeRacy,\n   MutexTypeGlobalProc,\n+  MutexTypeInternalAlloc,\n+  MutexTypeTrace,\n+  MutexTypeSlot,\n+  MutexTypeSlots,\n };\n \n }  // namespace __tsan"}, {"sha": "7a39a39d51de452a984b7e35c95fb7f694515133", "filename": "libsanitizer/tsan/tsan_dense_alloc.h", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_dense_alloc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_dense_alloc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_dense_alloc.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -104,6 +104,15 @@ class DenseSlabAlloc {\n     return atomic_load_relaxed(&fillpos_) * kL2Size * sizeof(T);\n   }\n \n+  template <typename Func>\n+  void ForEach(Func func) {\n+    SpinMutexLock lock(&mtx_);\n+    uptr fillpos = atomic_load_relaxed(&fillpos_);\n+    for (uptr l1 = 0; l1 < fillpos; l1++) {\n+      for (IndexT l2 = l1 == 0 ? 1 : 0; l2 < kL2Size; l2++) func(&map_[l1][l2]);\n+    }\n+  }\n+\n  private:\n   T *map_[kL1Size];\n   SpinMutex mtx_;"}, {"sha": "cf8f491fdbf094cda1ce2f281b35951d3be36323", "filename": "libsanitizer/tsan/tsan_fd.cpp", "status": "modified", "additions": 73, "deletions": 18, "changes": 91, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_fd.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_fd.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_fd.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -11,9 +11,12 @@\n //===----------------------------------------------------------------------===//\n \n #include \"tsan_fd.h\"\n-#include \"tsan_rtl.h\"\n+\n #include <sanitizer_common/sanitizer_atomic.h>\n \n+#include \"tsan_interceptors.h\"\n+#include \"tsan_rtl.h\"\n+\n namespace __tsan {\n \n const int kTableSizeL1 = 1024;\n@@ -26,6 +29,9 @@ struct FdSync {\n \n struct FdDesc {\n   FdSync *sync;\n+  // This is used to establish write -> epoll_wait synchronization\n+  // where epoll_wait receives notification about the write.\n+  atomic_uintptr_t aux_sync;  // FdSync*\n   Tid creation_tid;\n   StackID creation_stack;\n };\n@@ -100,6 +106,10 @@ static void init(ThreadState *thr, uptr pc, int fd, FdSync *s,\n     unref(thr, pc, d->sync);\n     d->sync = 0;\n   }\n+  unref(thr, pc,\n+        reinterpret_cast<FdSync *>(\n+            atomic_load(&d->aux_sync, memory_order_relaxed)));\n+  atomic_store(&d->aux_sync, 0, memory_order_relaxed);\n   if (flags()->io_sync == 0) {\n     unref(thr, pc, s);\n   } else if (flags()->io_sync == 1) {\n@@ -110,12 +120,17 @@ static void init(ThreadState *thr, uptr pc, int fd, FdSync *s,\n   }\n   d->creation_tid = thr->tid;\n   d->creation_stack = CurrentStackId(thr, pc);\n+  // This prevents false positives on fd_close_norace3.cpp test.\n+  // The mechanics of the false positive are not completely clear,\n+  // but it happens only if global reset is enabled (flush_memory_ms=1)\n+  // and may be related to lost writes during asynchronous MADV_DONTNEED.\n+  SlotLocker locker(thr);\n   if (write) {\n     // To catch races between fd usage and open.\n     MemoryRangeImitateWrite(thr, pc, (uptr)d, 8);\n   } else {\n     // See the dup-related comment in FdClose.\n-    MemoryAccess(thr, pc, (uptr)d, 8, kAccessRead);\n+    MemoryAccess(thr, pc, (uptr)d, 8, kAccessRead | kAccessSlotLocked);\n   }\n }\n \n@@ -177,6 +192,8 @@ void FdRelease(ThreadState *thr, uptr pc, int fd) {\n   MemoryAccess(thr, pc, (uptr)d, 8, kAccessRead);\n   if (s)\n     Release(thr, pc, (uptr)s);\n+  if (uptr aux_sync = atomic_load(&d->aux_sync, memory_order_acquire))\n+    Release(thr, pc, aux_sync);\n }\n \n void FdAccess(ThreadState *thr, uptr pc, int fd) {\n@@ -192,25 +209,39 @@ void FdClose(ThreadState *thr, uptr pc, int fd, bool write) {\n   if (bogusfd(fd))\n     return;\n   FdDesc *d = fddesc(thr, pc, fd);\n-  if (write) {\n-    // To catch races between fd usage and close.\n-    MemoryAccess(thr, pc, (uptr)d, 8, kAccessWrite);\n-  } else {\n-    // This path is used only by dup2/dup3 calls.\n-    // We do read instead of write because there is a number of legitimate\n-    // cases where write would lead to false positives:\n-    // 1. Some software dups a closed pipe in place of a socket before closing\n-    //    the socket (to prevent races actually).\n-    // 2. Some daemons dup /dev/null in place of stdin/stdout.\n-    // On the other hand we have not seen cases when write here catches real\n-    // bugs.\n-    MemoryAccess(thr, pc, (uptr)d, 8, kAccessRead);\n+  {\n+    // Need to lock the slot to make MemoryAccess and MemoryResetRange atomic\n+    // with respect to global reset. See the comment in MemoryRangeFreed.\n+    SlotLocker locker(thr);\n+    if (!MustIgnoreInterceptor(thr)) {\n+      if (write) {\n+        // To catch races between fd usage and close.\n+        MemoryAccess(thr, pc, (uptr)d, 8,\n+                     kAccessWrite | kAccessCheckOnly | kAccessSlotLocked);\n+      } else {\n+        // This path is used only by dup2/dup3 calls.\n+        // We do read instead of write because there is a number of legitimate\n+        // cases where write would lead to false positives:\n+        // 1. Some software dups a closed pipe in place of a socket before\n+        // closing\n+        //    the socket (to prevent races actually).\n+        // 2. Some daemons dup /dev/null in place of stdin/stdout.\n+        // On the other hand we have not seen cases when write here catches real\n+        // bugs.\n+        MemoryAccess(thr, pc, (uptr)d, 8,\n+                     kAccessRead | kAccessCheckOnly | kAccessSlotLocked);\n+      }\n+    }\n+    // We need to clear it, because if we do not intercept any call out there\n+    // that creates fd, we will hit false postives.\n+    MemoryResetRange(thr, pc, (uptr)d, 8);\n   }\n-  // We need to clear it, because if we do not intercept any call out there\n-  // that creates fd, we will hit false postives.\n-  MemoryResetRange(thr, pc, (uptr)d, 8);\n   unref(thr, pc, d->sync);\n   d->sync = 0;\n+  unref(thr, pc,\n+        reinterpret_cast<FdSync *>(\n+            atomic_load(&d->aux_sync, memory_order_relaxed)));\n+  atomic_store(&d->aux_sync, 0, memory_order_relaxed);\n   d->creation_tid = kInvalidTid;\n   d->creation_stack = kInvalidStackID;\n }\n@@ -269,6 +300,30 @@ void FdPollCreate(ThreadState *thr, uptr pc, int fd) {\n   init(thr, pc, fd, allocsync(thr, pc));\n }\n \n+void FdPollAdd(ThreadState *thr, uptr pc, int epfd, int fd) {\n+  DPrintf(\"#%d: FdPollAdd(%d, %d)\\n\", thr->tid, epfd, fd);\n+  if (bogusfd(epfd) || bogusfd(fd))\n+    return;\n+  FdDesc *d = fddesc(thr, pc, fd);\n+  // Associate fd with epoll fd only once.\n+  // While an fd can be associated with multiple epolls at the same time,\n+  // or with different epolls during different phases of lifetime,\n+  // synchronization semantics (and examples) of this are unclear.\n+  // So we don't support this for now.\n+  // If we change the association, it will also create lifetime management\n+  // problem for FdRelease which accesses the aux_sync.\n+  if (atomic_load(&d->aux_sync, memory_order_relaxed))\n+    return;\n+  FdDesc *epd = fddesc(thr, pc, epfd);\n+  FdSync *s = epd->sync;\n+  if (!s)\n+    return;\n+  uptr cmp = 0;\n+  if (atomic_compare_exchange_strong(\n+          &d->aux_sync, &cmp, reinterpret_cast<uptr>(s), memory_order_release))\n+    ref(s);\n+}\n+\n void FdSocketCreate(ThreadState *thr, uptr pc, int fd) {\n   DPrintf(\"#%d: FdSocketCreate(%d)\\n\", thr->tid, fd);\n   if (bogusfd(fd))"}, {"sha": "92625dc4b4a19241f41cfc25e22f8371d4068cbe", "filename": "libsanitizer/tsan/tsan_fd.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_fd.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_fd.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_fd.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -49,6 +49,7 @@ void FdEventCreate(ThreadState *thr, uptr pc, int fd);\n void FdSignalCreate(ThreadState *thr, uptr pc, int fd);\n void FdInotifyCreate(ThreadState *thr, uptr pc, int fd);\n void FdPollCreate(ThreadState *thr, uptr pc, int fd);\n+void FdPollAdd(ThreadState *thr, uptr pc, int epfd, int fd);\n void FdSocketCreate(ThreadState *thr, uptr pc, int fd);\n void FdSocketAccept(ThreadState *thr, uptr pc, int fd, int newfd);\n void FdSocketConnecting(ThreadState *thr, uptr pc, int fd);"}, {"sha": "ee78f25cc65c4bbe60f13bc3552577aaf70cf7d0", "filename": "libsanitizer/tsan/tsan_flags.cpp", "status": "modified", "additions": 1, "deletions": 7, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_flags.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_flags.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_flags.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -97,7 +97,7 @@ void InitializeFlags(Flags *f, const char *env, const char *env_option_name) {\n   ubsan_parser.ParseStringFromEnv(\"UBSAN_OPTIONS\");\n #endif\n \n-  // Sanity check.\n+  // Check flags.\n   if (!f->report_bugs) {\n     f->report_thread_leaks = false;\n     f->report_destroy_locked = false;\n@@ -110,12 +110,6 @@ void InitializeFlags(Flags *f, const char *env, const char *env_option_name) {\n \n   if (common_flags()->help) parser.PrintFlagDescriptions();\n \n-  if (f->history_size < 0 || f->history_size > 7) {\n-    Printf(\"ThreadSanitizer: incorrect value for history_size\"\n-           \" (must be [0..7])\\n\");\n-    Die();\n-  }\n-\n   if (f->io_sync < 0 || f->io_sync > 2) {\n     Printf(\"ThreadSanitizer: incorrect value for io_sync\"\n            \" (must be [0..2])\\n\");"}, {"sha": "32cf3bbf152dde65ef4c5a4e2afa6b175c13b13a", "filename": "libsanitizer/tsan/tsan_flags.inc", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_flags.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_flags.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_flags.inc?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -43,6 +43,9 @@ TSAN_FLAG(\n     bool, force_seq_cst_atomics, false,\n     \"If set, all atomics are effectively sequentially consistent (seq_cst), \"\n     \"regardless of what user actually specified.\")\n+TSAN_FLAG(bool, force_background_thread, false,\n+          \"If set, eagerly launch a background thread for memory reclamation \"\n+          \"instead of waiting for a user call to pthread_create.\")\n TSAN_FLAG(bool, halt_on_error, false, \"Exit after first reported error.\")\n TSAN_FLAG(int, atexit_sleep_ms, 1000,\n           \"Sleep in main thread before exiting for that many ms \"\n@@ -59,14 +62,10 @@ TSAN_FLAG(bool, stop_on_start, false,\n           \"Stops on start until __tsan_resume() is called (for debugging).\")\n TSAN_FLAG(bool, running_on_valgrind, false,\n           \"Controls whether RunningOnValgrind() returns true or false.\")\n-// There are a lot of goroutines in Go, so we use smaller history.\n TSAN_FLAG(\n-    int, history_size, SANITIZER_GO ? 1 : 3,\n-    \"Per-thread history size, controls how many previous memory accesses \"\n-    \"are remembered per thread.  Possible values are [0..7]. \"\n-    \"history_size=0 amounts to 32K memory accesses.  Each next value doubles \"\n-    \"the amount of memory accesses, up to history_size=7 that amounts to \"\n-    \"4M memory accesses.  The default value is 2 (128K memory accesses).\")\n+    uptr, history_size, 0,\n+    \"Per-thread history size,\"\n+    \" controls how many extra previous memory accesses are remembered per thread.\")\n TSAN_FLAG(int, io_sync, 1,\n           \"Controls level of synchronization implied by IO operations. \"\n           \"0 - no synchronization \"\n@@ -82,3 +81,6 @@ TSAN_FLAG(bool, ignore_noninstrumented_modules, SANITIZER_MAC ? true : false,\n           \"modules.\")\n TSAN_FLAG(bool, shared_ptr_interceptor, true,\n           \"Track atomic reference counting in libc++ shared_ptr and weak_ptr.\")\n+TSAN_FLAG(bool, print_full_thread_history, false,\n+          \"If set, prints thread creation stacks for the threads involved in \"\n+          \"the report and their ancestors up to the main thread.\")"}, {"sha": "3091ad809c40fca8e07c6519c5ba2f4d11d47615", "filename": "libsanitizer/tsan/tsan_interceptors.h", "status": "modified", "additions": 16, "deletions": 4, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interceptors.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interceptors.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interceptors.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -36,6 +36,10 @@ inline bool in_symbolizer() {\n }\n #endif\n \n+inline bool MustIgnoreInterceptor(ThreadState *thr) {\n+  return !thr->is_inited || thr->ignore_interceptors || thr->in_ignored_lib;\n+}\n+\n }  // namespace __tsan\n \n #define SCOPED_INTERCEPTOR_RAW(func, ...)            \\\n@@ -60,10 +64,10 @@ inline bool in_symbolizer() {\n #  define CHECK_REAL_FUNC(func) DCHECK(REAL(func))\n #endif\n \n-#define SCOPED_TSAN_INTERCEPTOR(func, ...)                                \\\n-  SCOPED_INTERCEPTOR_RAW(func, __VA_ARGS__);                              \\\n-  CHECK_REAL_FUNC(func);                                                  \\\n-  if (!thr->is_inited || thr->ignore_interceptors || thr->in_ignored_lib) \\\n+#define SCOPED_TSAN_INTERCEPTOR(func, ...)   \\\n+  SCOPED_INTERCEPTOR_RAW(func, __VA_ARGS__); \\\n+  CHECK_REAL_FUNC(func);                     \\\n+  if (MustIgnoreInterceptor(thr))            \\\n     return REAL(func)(__VA_ARGS__);\n \n #define SCOPED_TSAN_INTERCEPTOR_USER_CALLBACK_START() \\\n@@ -74,6 +78,14 @@ inline bool in_symbolizer() {\n \n #define TSAN_INTERCEPTOR(ret, func, ...) INTERCEPTOR(ret, func, __VA_ARGS__)\n \n+#if SANITIZER_FREEBSD\n+#  define TSAN_INTERCEPTOR_FREEBSD_ALIAS(ret, func, ...) \\\n+    TSAN_INTERCEPTOR(ret, _pthread_##func, __VA_ARGS__)  \\\n+    ALIAS(WRAPPER_NAME(pthread_##func));\n+#else\n+#  define TSAN_INTERCEPTOR_FREEBSD_ALIAS(ret, func, ...)\n+#endif\n+\n #if SANITIZER_NETBSD\n # define TSAN_INTERCEPTOR_NETBSD_ALIAS(ret, func, ...) \\\n   TSAN_INTERCEPTOR(ret, __libc_##func, __VA_ARGS__) \\"}, {"sha": "60ca9633868eab6603905f611b1a780deab14110", "filename": "libsanitizer/tsan/tsan_interceptors_posix.cpp", "status": "modified", "additions": 137, "deletions": 63, "changes": 200, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interceptors_posix.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interceptors_posix.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interceptors_posix.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -90,6 +90,7 @@ DECLARE_REAL(int, pthread_mutexattr_gettype, void *, void *)\n DECLARE_REAL(int, fflush, __sanitizer_FILE *fp)\n DECLARE_REAL_AND_INTERCEPTOR(void *, malloc, uptr size)\n DECLARE_REAL_AND_INTERCEPTOR(void, free, void *ptr)\n+extern \"C\" int pthread_equal(void *t1, void *t2);\n extern \"C\" void *pthread_self();\n extern \"C\" void _exit(int status);\n #if !SANITIZER_NETBSD\n@@ -176,6 +177,7 @@ struct ThreadSignalContext {\n struct AtExitCtx {\n   void (*f)();\n   void *arg;\n+  uptr pc;\n };\n \n // InterceptorContext holds all global data required for interceptors.\n@@ -287,20 +289,25 @@ void ScopedInterceptor::DisableIgnoresImpl() {\n }\n \n #define TSAN_INTERCEPT(func) INTERCEPT_FUNCTION(func)\n+#if SANITIZER_FREEBSD || SANITIZER_NETBSD\n+#  define TSAN_INTERCEPT_VER(func, ver) INTERCEPT_FUNCTION(func)\n+#else\n+#  define TSAN_INTERCEPT_VER(func, ver) INTERCEPT_FUNCTION_VER(func, ver)\n+#endif\n #if SANITIZER_FREEBSD\n-# define TSAN_INTERCEPT_VER(func, ver) INTERCEPT_FUNCTION(func)\n-# define TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS(func)\n-# define TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS_THR(func)\n-#elif SANITIZER_NETBSD\n-# define TSAN_INTERCEPT_VER(func, ver) INTERCEPT_FUNCTION(func)\n-# define TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS(func) \\\n-         INTERCEPT_FUNCTION(__libc_##func)\n-# define TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS_THR(func) \\\n-         INTERCEPT_FUNCTION(__libc_thr_##func)\n+#  define TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(func) \\\n+    INTERCEPT_FUNCTION(_pthread_##func)\n #else\n-# define TSAN_INTERCEPT_VER(func, ver) INTERCEPT_FUNCTION_VER(func, ver)\n-# define TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS(func)\n-# define TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS_THR(func)\n+#  define TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(func)\n+#endif\n+#if SANITIZER_NETBSD\n+#  define TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS(func) \\\n+    INTERCEPT_FUNCTION(__libc_##func)\n+#  define TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS_THR(func) \\\n+    INTERCEPT_FUNCTION(__libc_thr_##func)\n+#else\n+#  define TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS(func)\n+#  define TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS_THR(func)\n #endif\n \n #define READ_STRING_OF_LEN(thr, pc, s, len, n)                 \\\n@@ -366,7 +373,10 @@ TSAN_INTERCEPTOR(int, pause, int fake) {\n   return BLOCK_REAL(pause)(fake);\n }\n \n-static void at_exit_wrapper() {\n+// Note: we specifically call the function in such strange way\n+// with \"installed_at\" because in reports it will appear between\n+// callback frames and the frame that installed the callback.\n+static void at_exit_callback_installed_at() {\n   AtExitCtx *ctx;\n   {\n     // Ensure thread-safety.\n@@ -378,15 +388,21 @@ static void at_exit_wrapper() {\n     interceptor_ctx()->AtExitStack.PopBack();\n   }\n \n-  Acquire(cur_thread(), (uptr)0, (uptr)ctx);\n+  ThreadState *thr = cur_thread();\n+  Acquire(thr, ctx->pc, (uptr)ctx);\n+  FuncEntry(thr, ctx->pc);\n   ((void(*)())ctx->f)();\n+  FuncExit(thr);\n   Free(ctx);\n }\n \n-static void cxa_at_exit_wrapper(void *arg) {\n-  Acquire(cur_thread(), 0, (uptr)arg);\n+static void cxa_at_exit_callback_installed_at(void *arg) {\n+  ThreadState *thr = cur_thread();\n   AtExitCtx *ctx = (AtExitCtx*)arg;\n+  Acquire(thr, ctx->pc, (uptr)arg);\n+  FuncEntry(thr, ctx->pc);\n   ((void(*)(void *arg))ctx->f)(ctx->arg);\n+  FuncExit(thr);\n   Free(ctx);\n }\n \n@@ -400,22 +416,23 @@ TSAN_INTERCEPTOR(int, atexit, void (*f)()) {\n   // We want to setup the atexit callback even if we are in ignored lib\n   // or after fork.\n   SCOPED_INTERCEPTOR_RAW(atexit, f);\n-  return setup_at_exit_wrapper(thr, pc, (void(*)())f, 0, 0);\n+  return setup_at_exit_wrapper(thr, GET_CALLER_PC(), (void (*)())f, 0, 0);\n }\n #endif\n \n TSAN_INTERCEPTOR(int, __cxa_atexit, void (*f)(void *a), void *arg, void *dso) {\n   if (in_symbolizer())\n     return 0;\n   SCOPED_TSAN_INTERCEPTOR(__cxa_atexit, f, arg, dso);\n-  return setup_at_exit_wrapper(thr, pc, (void(*)())f, arg, dso);\n+  return setup_at_exit_wrapper(thr, GET_CALLER_PC(), (void (*)())f, arg, dso);\n }\n \n static int setup_at_exit_wrapper(ThreadState *thr, uptr pc, void(*f)(),\n       void *arg, void *dso) {\n   auto *ctx = New<AtExitCtx>();\n   ctx->f = f;\n   ctx->arg = arg;\n+  ctx->pc = pc;\n   Release(thr, pc, (uptr)ctx);\n   // Memory allocation in __cxa_atexit will race with free during exit,\n   // because we do not see synchronization around atexit callback list.\n@@ -431,25 +448,27 @@ static int setup_at_exit_wrapper(ThreadState *thr, uptr pc, void(*f)(),\n     // due to atexit_mu held on exit from the calloc interceptor.\n     ScopedIgnoreInterceptors ignore;\n \n-    res = REAL(__cxa_atexit)((void (*)(void *a))at_exit_wrapper, 0, 0);\n+    res = REAL(__cxa_atexit)((void (*)(void *a))at_exit_callback_installed_at,\n+                             0, 0);\n     // Push AtExitCtx on the top of the stack of callback functions\n     if (!res) {\n       interceptor_ctx()->AtExitStack.PushBack(ctx);\n     }\n   } else {\n-    res = REAL(__cxa_atexit)(cxa_at_exit_wrapper, ctx, dso);\n+    res = REAL(__cxa_atexit)(cxa_at_exit_callback_installed_at, ctx, dso);\n   }\n   ThreadIgnoreEnd(thr);\n   return res;\n }\n \n #if !SANITIZER_MAC && !SANITIZER_NETBSD\n-static void on_exit_wrapper(int status, void *arg) {\n+static void on_exit_callback_installed_at(int status, void *arg) {\n   ThreadState *thr = cur_thread();\n-  uptr pc = 0;\n-  Acquire(thr, pc, (uptr)arg);\n   AtExitCtx *ctx = (AtExitCtx*)arg;\n+  Acquire(thr, ctx->pc, (uptr)arg);\n+  FuncEntry(thr, ctx->pc);\n   ((void(*)(int status, void *arg))ctx->f)(status, ctx->arg);\n+  FuncExit(thr);\n   Free(ctx);\n }\n \n@@ -460,11 +479,12 @@ TSAN_INTERCEPTOR(int, on_exit, void(*f)(int, void*), void *arg) {\n   auto *ctx = New<AtExitCtx>();\n   ctx->f = (void(*)())f;\n   ctx->arg = arg;\n+  ctx->pc = GET_CALLER_PC();\n   Release(thr, pc, (uptr)ctx);\n   // Memory allocation in __cxa_atexit will race with free during exit,\n   // because we do not see synchronization around atexit callback list.\n   ThreadIgnoreBegin(thr, pc);\n-  int res = REAL(on_exit)(on_exit_wrapper, ctx);\n+  int res = REAL(on_exit)(on_exit_callback_installed_at, ctx);\n   ThreadIgnoreEnd(thr);\n   return res;\n }\n@@ -880,10 +900,11 @@ static int guard_acquire(ThreadState *thr, uptr pc, atomic_uint32_t *g,\n   }\n }\n \n-static void guard_release(ThreadState *thr, uptr pc, atomic_uint32_t *g) {\n+static void guard_release(ThreadState *thr, uptr pc, atomic_uint32_t *g,\n+                          u32 v) {\n   if (!thr->in_ignored_lib)\n     Release(thr, pc, (uptr)g);\n-  u32 old = atomic_exchange(g, kGuardDone, memory_order_release);\n+  u32 old = atomic_exchange(g, v, memory_order_release);\n   if (old & kGuardWaiter)\n     FutexWake(g, 1 << 30);\n }\n@@ -913,12 +934,12 @@ STDCXX_INTERCEPTOR(int, __cxa_guard_acquire, atomic_uint32_t *g) {\n \n STDCXX_INTERCEPTOR(void, __cxa_guard_release, atomic_uint32_t *g) {\n   SCOPED_INTERCEPTOR_RAW(__cxa_guard_release, g);\n-  guard_release(thr, pc, g);\n+  guard_release(thr, pc, g, kGuardDone);\n }\n \n STDCXX_INTERCEPTOR(void, __cxa_guard_abort, atomic_uint32_t *g) {\n   SCOPED_INTERCEPTOR_RAW(__cxa_guard_abort, g);\n-  atomic_store(g, kGuardInit, memory_order_relaxed);\n+  guard_release(thr, pc, g, kGuardInit);\n }\n \n namespace __tsan {\n@@ -1515,12 +1536,12 @@ TSAN_INTERCEPTOR(int, pthread_once, void *o, void (*f)()) {\n   // result in crashes due to too little stack space.\n   if (guard_acquire(thr, pc, a, !SANITIZER_MAC)) {\n     (*f)();\n-    guard_release(thr, pc, a);\n+    guard_release(thr, pc, a, kGuardDone);\n   }\n   return 0;\n }\n \n-#if SANITIZER_LINUX && !SANITIZER_ANDROID\n+#if SANITIZER_GLIBC\n TSAN_INTERCEPTOR(int, __fxstat, int version, int fd, void *buf) {\n   SCOPED_TSAN_INTERCEPTOR(__fxstat, version, fd, buf);\n   if (fd > 0)\n@@ -1533,20 +1554,20 @@ TSAN_INTERCEPTOR(int, __fxstat, int version, int fd, void *buf) {\n #endif\n \n TSAN_INTERCEPTOR(int, fstat, int fd, void *buf) {\n-#if SANITIZER_FREEBSD || SANITIZER_MAC || SANITIZER_ANDROID || SANITIZER_NETBSD\n-  SCOPED_TSAN_INTERCEPTOR(fstat, fd, buf);\n+#if SANITIZER_GLIBC\n+  SCOPED_TSAN_INTERCEPTOR(__fxstat, 0, fd, buf);\n   if (fd > 0)\n     FdAccess(thr, pc, fd);\n-  return REAL(fstat)(fd, buf);\n+  return REAL(__fxstat)(0, fd, buf);\n #else\n-  SCOPED_TSAN_INTERCEPTOR(__fxstat, 0, fd, buf);\n+  SCOPED_TSAN_INTERCEPTOR(fstat, fd, buf);\n   if (fd > 0)\n     FdAccess(thr, pc, fd);\n-  return REAL(__fxstat)(0, fd, buf);\n+  return REAL(fstat)(fd, buf);\n #endif\n }\n \n-#if SANITIZER_LINUX && !SANITIZER_ANDROID\n+#if SANITIZER_GLIBC\n TSAN_INTERCEPTOR(int, __fxstat64, int version, int fd, void *buf) {\n   SCOPED_TSAN_INTERCEPTOR(__fxstat64, version, fd, buf);\n   if (fd > 0)\n@@ -1558,7 +1579,7 @@ TSAN_INTERCEPTOR(int, __fxstat64, int version, int fd, void *buf) {\n #define TSAN_MAYBE_INTERCEPT___FXSTAT64\n #endif\n \n-#if SANITIZER_LINUX && !SANITIZER_ANDROID\n+#if SANITIZER_GLIBC\n TSAN_INTERCEPTOR(int, fstat64, int fd, void *buf) {\n   SCOPED_TSAN_INTERCEPTOR(__fxstat64, 0, fd, buf);\n   if (fd > 0)\n@@ -1665,11 +1686,10 @@ TSAN_INTERCEPTOR(int, eventfd, unsigned initval, int flags) {\n \n #if SANITIZER_LINUX\n TSAN_INTERCEPTOR(int, signalfd, int fd, void *mask, int flags) {\n-  SCOPED_TSAN_INTERCEPTOR(signalfd, fd, mask, flags);\n-  if (fd >= 0)\n-    FdClose(thr, pc, fd);\n+  SCOPED_INTERCEPTOR_RAW(signalfd, fd, mask, flags);\n+  FdClose(thr, pc, fd);\n   fd = REAL(signalfd)(fd, mask, flags);\n-  if (fd >= 0)\n+  if (!MustIgnoreInterceptor(thr))\n     FdSignalCreate(thr, pc, fd);\n   return fd;\n }\n@@ -1746,17 +1766,16 @@ TSAN_INTERCEPTOR(int, listen, int fd, int backlog) {\n }\n \n TSAN_INTERCEPTOR(int, close, int fd) {\n-  SCOPED_TSAN_INTERCEPTOR(close, fd);\n-  if (fd >= 0)\n+  SCOPED_INTERCEPTOR_RAW(close, fd);\n+  if (!in_symbolizer())\n     FdClose(thr, pc, fd);\n   return REAL(close)(fd);\n }\n \n #if SANITIZER_LINUX\n TSAN_INTERCEPTOR(int, __close, int fd) {\n-  SCOPED_TSAN_INTERCEPTOR(__close, fd);\n-  if (fd >= 0)\n-    FdClose(thr, pc, fd);\n+  SCOPED_INTERCEPTOR_RAW(__close, fd);\n+  FdClose(thr, pc, fd);\n   return REAL(__close)(fd);\n }\n #define TSAN_MAYBE_INTERCEPT___CLOSE TSAN_INTERCEPT(__close)\n@@ -1767,13 +1786,10 @@ TSAN_INTERCEPTOR(int, __close, int fd) {\n // glibc guts\n #if SANITIZER_LINUX && !SANITIZER_ANDROID\n TSAN_INTERCEPTOR(void, __res_iclose, void *state, bool free_addr) {\n-  SCOPED_TSAN_INTERCEPTOR(__res_iclose, state, free_addr);\n+  SCOPED_INTERCEPTOR_RAW(__res_iclose, state, free_addr);\n   int fds[64];\n   int cnt = ExtractResolvFDs(state, fds, ARRAY_SIZE(fds));\n-  for (int i = 0; i < cnt; i++) {\n-    if (fds[i] > 0)\n-      FdClose(thr, pc, fds[i]);\n-  }\n+  for (int i = 0; i < cnt; i++) FdClose(thr, pc, fds[i]);\n   REAL(__res_iclose)(state, free_addr);\n }\n #define TSAN_MAYBE_INTERCEPT___RES_ICLOSE TSAN_INTERCEPT(__res_iclose)\n@@ -1854,7 +1870,7 @@ TSAN_INTERCEPTOR(int, rmdir, char *path) {\n }\n \n TSAN_INTERCEPTOR(int, closedir, void *dirp) {\n-  SCOPED_TSAN_INTERCEPTOR(closedir, dirp);\n+  SCOPED_INTERCEPTOR_RAW(closedir, dirp);\n   if (dirp) {\n     int fd = dirfd(dirp);\n     FdClose(thr, pc, fd);\n@@ -1885,8 +1901,10 @@ TSAN_INTERCEPTOR(int, epoll_ctl, int epfd, int op, int fd, void *ev) {\n     FdAccess(thr, pc, epfd);\n   if (epfd >= 0 && fd >= 0)\n     FdAccess(thr, pc, fd);\n-  if (op == EPOLL_CTL_ADD && epfd >= 0)\n+  if (op == EPOLL_CTL_ADD && epfd >= 0) {\n+    FdPollAdd(thr, pc, epfd, fd);\n     FdRelease(thr, pc, epfd);\n+  }\n   int res = REAL(epoll_ctl)(epfd, op, fd, ev);\n   return res;\n }\n@@ -1949,13 +1967,14 @@ TSAN_INTERCEPTOR(int, pthread_sigmask, int how, const __sanitizer_sigset_t *set,\n \n namespace __tsan {\n \n-static void ReportErrnoSpoiling(ThreadState *thr, uptr pc) {\n+static void ReportErrnoSpoiling(ThreadState *thr, uptr pc, int sig) {\n   VarSizeStackTrace stack;\n   // StackTrace::GetNestInstructionPc(pc) is used because return address is\n   // expected, OutputReport() will undo this.\n   ObtainCurrentStack(thr, StackTrace::GetNextInstructionPc(pc), &stack);\n   ThreadRegistryLock l(&ctx->thread_registry);\n   ScopedReport rep(ReportTypeErrnoInSignal);\n+  rep.SetSigNum(sig);\n   if (!IsFiredSuppression(ctx, ReportTypeErrnoInSignal, stack)) {\n     rep.AddStack(stack, true);\n     OutputReport(thr, rep);\n@@ -1965,6 +1984,7 @@ static void ReportErrnoSpoiling(ThreadState *thr, uptr pc) {\n static void CallUserSignalHandler(ThreadState *thr, bool sync, bool acquire,\n                                   int sig, __sanitizer_siginfo *info,\n                                   void *uctx) {\n+  CHECK(thr->slot);\n   __sanitizer_sigaction *sigactions = interceptor_ctx()->sigactions;\n   if (acquire)\n     Acquire(thr, 0, (uptr)&sigactions[sig]);\n@@ -2021,7 +2041,7 @@ static void CallUserSignalHandler(ThreadState *thr, bool sync, bool acquire,\n   // signal; and it looks too fragile to intercept all ways to reraise a signal.\n   if (ShouldReport(thr, ReportTypeErrnoInSignal) && !sync && sig != SIGTERM &&\n       errno != 99)\n-    ReportErrnoSpoiling(thr, pc);\n+    ReportErrnoSpoiling(thr, pc, sig);\n   errno = saved_errno;\n }\n \n@@ -2132,11 +2152,11 @@ TSAN_INTERCEPTOR(int, pthread_kill, void *tid, int sig) {\n   ThreadSignalContext *sctx = SigCtx(thr);\n   CHECK_NE(sctx, 0);\n   int prev = sctx->int_signal_send;\n-  if (tid == pthread_self()) {\n+  bool self = pthread_equal(tid, pthread_self());\n+  if (self)\n     sctx->int_signal_send = sig;\n-  }\n   int res = REAL(pthread_kill)(tid, sig);\n-  if (tid == pthread_self()) {\n+  if (self) {\n     CHECK_EQ(sctx->int_signal_send, sig);\n     sctx->int_signal_send = prev;\n   }\n@@ -2193,6 +2213,7 @@ void atfork_child() {\n   FdOnFork(thr, pc);\n }\n \n+#if !SANITIZER_IOS\n TSAN_INTERCEPTOR(int, vfork, int fake) {\n   // Some programs (e.g. openjdk) call close for all file descriptors\n   // in the child process. Under tsan it leads to false positives, because\n@@ -2209,6 +2230,7 @@ TSAN_INTERCEPTOR(int, vfork, int fake) {\n   // Instead we simply turn vfork into fork.\n   return WRAP(fork)(fake);\n }\n+#endif\n \n #if SANITIZER_LINUX\n TSAN_INTERCEPTOR(int, clone, int (*fn)(void *), void *stack, int flags,\n@@ -2252,7 +2274,7 @@ struct dl_iterate_phdr_data {\n };\n \n static bool IsAppNotRodata(uptr addr) {\n-  return IsAppMem(addr) && *MemToShadow(addr) != kShadowRodata;\n+  return IsAppMem(addr) && *MemToShadow(addr) != Shadow::kRodata;\n }\n \n static int dl_iterate_phdr_cb(__sanitizer_dl_phdr_info *info, SIZE_T size,\n@@ -2358,9 +2380,18 @@ static void HandleRecvmsg(ThreadState *thr, uptr pc,\n #define COMMON_INTERCEPTOR_FILE_CLOSE(ctx, file) \\\n   if (file) {                                    \\\n     int fd = fileno_unlocked(file);              \\\n-    if (fd >= 0) FdClose(thr, pc, fd);           \\\n+    FdClose(thr, pc, fd);                        \\\n   }\n \n+#define COMMON_INTERCEPTOR_DLOPEN(filename, flag) \\\n+  ({                                              \\\n+    CheckNoDeepBind(filename, flag);              \\\n+    ThreadIgnoreBegin(thr, 0);                    \\\n+    void *res = REAL(dlopen)(filename, flag);     \\\n+    ThreadIgnoreEnd(thr);                         \\\n+    res;                                          \\\n+  })\n+\n #define COMMON_INTERCEPTOR_LIBRARY_LOADED(filename, handle) \\\n   libignore()->OnLibraryLoaded(filename)\n \n@@ -2391,8 +2422,11 @@ static void HandleRecvmsg(ThreadState *thr, uptr pc,\n #define COMMON_INTERCEPTOR_SET_THREAD_NAME(ctx, name) \\\n   ThreadSetName(((TsanInterceptorContext *) ctx)->thr, name)\n \n-#define COMMON_INTERCEPTOR_SET_PTHREAD_NAME(ctx, thread, name) \\\n-  __tsan::ctx->thread_registry.SetThreadNameByUserId(thread, name)\n+#define COMMON_INTERCEPTOR_SET_PTHREAD_NAME(ctx, thread, name)         \\\n+  if (pthread_equal(pthread_self(), reinterpret_cast<void *>(thread))) \\\n+    COMMON_INTERCEPTOR_SET_THREAD_NAME(ctx, name);                     \\\n+  else                                                                 \\\n+    __tsan::ctx->thread_registry.SetThreadNameByUserId(thread, name)\n \n #define COMMON_INTERCEPTOR_BLOCK_REAL(name) BLOCK_REAL(name)\n \n@@ -2553,7 +2587,7 @@ static USED void syscall_release(uptr pc, uptr addr) {\n }\n \n static void syscall_fd_close(uptr pc, int fd) {\n-  TSAN_SYSCALL();\n+  auto *thr = cur_thread();\n   FdClose(thr, pc, fd);\n }\n \n@@ -2688,6 +2722,26 @@ TSAN_INTERCEPTOR(void, thr_exit, tid_t *state) {\n #define TSAN_MAYBE_INTERCEPT_THR_EXIT\n #endif\n \n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, cond_init, void *c, void *a)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, cond_destroy, void *c)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, cond_signal, void *c)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, cond_broadcast, void *c)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, cond_wait, void *c, void *m)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, mutex_init, void *m, void *a)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, mutex_destroy, void *m)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, mutex_lock, void *m)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, mutex_trylock, void *m)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, mutex_unlock, void *m)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, rwlock_init, void *l, void *a)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, rwlock_destroy, void *l)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, rwlock_rdlock, void *l)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, rwlock_tryrdlock, void *l)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, rwlock_wrlock, void *l)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, rwlock_trywrlock, void *l)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, rwlock_unlock, void *l)\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, once, void *o, void (*i)())\n+TSAN_INTERCEPTOR_FREEBSD_ALIAS(int, sigmask, int f, void *n, void *o)\n+\n TSAN_INTERCEPTOR_NETBSD_ALIAS(int, cond_init, void *c, void *a)\n TSAN_INTERCEPTOR_NETBSD_ALIAS(int, cond_signal, void *c)\n TSAN_INTERCEPTOR_NETBSD_ALIAS(int, cond_broadcast, void *c)\n@@ -2916,6 +2970,26 @@ void InitializeInterceptors() {\n   }\n #endif\n \n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(cond_init);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(cond_destroy);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(cond_signal);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(cond_broadcast);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(cond_wait);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(mutex_init);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(mutex_destroy);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(mutex_lock);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(mutex_trylock);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(mutex_unlock);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(rwlock_init);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(rwlock_destroy);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(rwlock_rdlock);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(rwlock_tryrdlock);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(rwlock_wrlock);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(rwlock_trywrlock);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(rwlock_unlock);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(once);\n+  TSAN_MAYBE_INTERCEPT_FREEBSD_ALIAS(sigmask);\n+\n   TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS(cond_init);\n   TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS(cond_signal);\n   TSAN_MAYBE_INTERCEPT_NETBSD_ALIAS(cond_broadcast);"}, {"sha": "e6c4bf2e60a7b531adfcf880bfd09e99d7d599e8", "filename": "libsanitizer/tsan/tsan_interface.cpp", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interface.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interface.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interface.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -26,20 +26,6 @@ void __tsan_flush_memory() {\n   FlushShadowMemory();\n }\n \n-void __tsan_read16(void *addr) {\n-  uptr pc = CALLERPC;\n-  ThreadState *thr = cur_thread();\n-  MemoryAccess(thr, pc, (uptr)addr, 8, kAccessRead);\n-  MemoryAccess(thr, pc, (uptr)addr + 8, 8, kAccessRead);\n-}\n-\n-void __tsan_write16(void *addr) {\n-  uptr pc = CALLERPC;\n-  ThreadState *thr = cur_thread();\n-  MemoryAccess(thr, pc, (uptr)addr, 8, kAccessWrite);\n-  MemoryAccess(thr, pc, (uptr)addr + 8, 8, kAccessWrite);\n-}\n-\n void __tsan_read16_pc(void *addr, void *pc) {\n   uptr pc_no_pac = STRIP_PAC_PC(pc);\n   ThreadState *thr = cur_thread();"}, {"sha": "b0a424ff9c2550c12c94d4c355ee36ed740131ee", "filename": "libsanitizer/tsan/tsan_interface.inc", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interface.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interface.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interface.inc?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -34,6 +34,10 @@ void __tsan_read8(void *addr) {\n   MemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 8, kAccessRead);\n }\n \n+void __tsan_read16(void *addr) {\n+  MemoryAccess16(cur_thread(), CALLERPC, (uptr)addr, kAccessRead);\n+}\n+\n void __tsan_write1(void *addr) {\n   MemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 1, kAccessWrite);\n }\n@@ -50,6 +54,10 @@ void __tsan_write8(void *addr) {\n   MemoryAccess(cur_thread(), CALLERPC, (uptr)addr, 8, kAccessWrite);\n }\n \n+void __tsan_write16(void *addr) {\n+  MemoryAccess16(cur_thread(), CALLERPC, (uptr)addr, kAccessWrite);\n+}\n+\n void __tsan_read1_pc(void *addr, void *pc) {\n   MemoryAccess(cur_thread(), STRIP_PAC_PC(pc), (uptr)addr, 1, kAccessRead | kAccessExternalPC);\n }"}, {"sha": "f794a2fcdd0df7fa372e017fa8a80fe333f10563", "filename": "libsanitizer/tsan/tsan_interface_atomic.cpp", "status": "modified", "additions": 46, "deletions": 41, "changes": 87, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interface_atomic.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interface_atomic.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interface_atomic.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -235,8 +235,9 @@ static T AtomicLoad(ThreadState *thr, uptr pc, const volatile T *a, morder mo) {\n   T v = NoTsanAtomicLoad(a, mo);\n   SyncVar *s = ctx->metamap.GetSyncIfExists((uptr)a);\n   if (s) {\n-    ReadLock l(&s->mtx);\n-    AcquireImpl(thr, pc, &s->clock);\n+    SlotLocker locker(thr);\n+    ReadLock lock(&s->mtx);\n+    thr->clock.Acquire(s->clock);\n     // Re-read under sync mutex because we need a consistent snapshot\n     // of the value and the clock we acquire.\n     v = NoTsanAtomicLoad(a, mo);\n@@ -270,33 +271,36 @@ static void AtomicStore(ThreadState *thr, uptr pc, volatile T *a, T v,\n     NoTsanAtomicStore(a, v, mo);\n     return;\n   }\n-  __sync_synchronize();\n-  SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, (uptr)a, false);\n-  Lock l(&s->mtx);\n-  thr->fast_state.IncrementEpoch();\n-  // Can't increment epoch w/o writing to the trace as well.\n-  TraceAddEvent(thr, thr->fast_state, EventTypeMop, 0);\n-  ReleaseStoreImpl(thr, pc, &s->clock);\n-  NoTsanAtomicStore(a, v, mo);\n+  SlotLocker locker(thr);\n+  {\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, (uptr)a, false);\n+    Lock lock(&s->mtx);\n+    thr->clock.ReleaseStore(&s->clock);\n+    NoTsanAtomicStore(a, v, mo);\n+  }\n+  IncrementEpoch(thr);\n }\n \n template <typename T, T (*F)(volatile T *v, T op)>\n static T AtomicRMW(ThreadState *thr, uptr pc, volatile T *a, T v, morder mo) {\n   MemoryAccess(thr, pc, (uptr)a, AccessSize<T>(), kAccessWrite | kAccessAtomic);\n   if (LIKELY(mo == mo_relaxed))\n     return F(a, v);\n-  SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, (uptr)a, false);\n-  Lock l(&s->mtx);\n-  thr->fast_state.IncrementEpoch();\n-  // Can't increment epoch w/o writing to the trace as well.\n-  TraceAddEvent(thr, thr->fast_state, EventTypeMop, 0);\n-  if (IsAcqRelOrder(mo))\n-    AcquireReleaseImpl(thr, pc, &s->clock);\n-  else if (IsReleaseOrder(mo))\n-    ReleaseImpl(thr, pc, &s->clock);\n-  else if (IsAcquireOrder(mo))\n-    AcquireImpl(thr, pc, &s->clock);\n-  return F(a, v);\n+  SlotLocker locker(thr);\n+  {\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, (uptr)a, false);\n+    RWLock lock(&s->mtx, IsReleaseOrder(mo));\n+    if (IsAcqRelOrder(mo))\n+      thr->clock.ReleaseAcquire(&s->clock);\n+    else if (IsReleaseOrder(mo))\n+      thr->clock.Release(&s->clock);\n+    else if (IsAcquireOrder(mo))\n+      thr->clock.Acquire(s->clock);\n+    v = F(a, v);\n+  }\n+  if (IsReleaseOrder(mo))\n+    IncrementEpoch(thr);\n+  return v;\n }\n \n template<typename T>\n@@ -416,27 +420,28 @@ static bool AtomicCAS(ThreadState *thr, uptr pc, volatile T *a, T *c, T v,\n     *c = pr;\n     return false;\n   }\n-\n+  SlotLocker locker(thr);\n   bool release = IsReleaseOrder(mo);\n-  SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, (uptr)a, false);\n-  RWLock l(&s->mtx, release);\n-  T cc = *c;\n-  T pr = func_cas(a, cc, v);\n-  bool success = pr == cc;\n-  if (!success) {\n-    *c = pr;\n-    mo = fmo;\n+  bool success;\n+  {\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, (uptr)a, false);\n+    RWLock lock(&s->mtx, release);\n+    T cc = *c;\n+    T pr = func_cas(a, cc, v);\n+    success = pr == cc;\n+    if (!success) {\n+      *c = pr;\n+      mo = fmo;\n+    }\n+    if (success && IsAcqRelOrder(mo))\n+      thr->clock.ReleaseAcquire(&s->clock);\n+    else if (success && IsReleaseOrder(mo))\n+      thr->clock.Release(&s->clock);\n+    else if (IsAcquireOrder(mo))\n+      thr->clock.Acquire(s->clock);\n   }\n-  thr->fast_state.IncrementEpoch();\n-  // Can't increment epoch w/o writing to the trace as well.\n-  TraceAddEvent(thr, thr->fast_state, EventTypeMop, 0);\n-\n-  if (success && IsAcqRelOrder(mo))\n-    AcquireReleaseImpl(thr, pc, &s->clock);\n-  else if (success && IsReleaseOrder(mo))\n-    ReleaseImpl(thr, pc, &s->clock);\n-  else if (IsAcquireOrder(mo))\n-    AcquireImpl(thr, pc, &s->clock);\n+  if (success && release)\n+    IncrementEpoch(thr);\n   return success;\n }\n "}, {"sha": "7c15a16388268cf9765c8b0a15ce8a200169d3ff", "filename": "libsanitizer/tsan/tsan_interface_java.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interface_java.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_interface_java.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interface_java.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -106,7 +106,7 @@ void __tsan_java_free(jptr ptr, jptr size) {\n   DCHECK_GE(ptr, jctx->heap_begin);\n   DCHECK_LE(ptr + size, jctx->heap_begin + jctx->heap_size);\n \n-  ctx->metamap.FreeRange(thr->proc(), ptr, size);\n+  ctx->metamap.FreeRange(thr->proc(), ptr, size, false);\n }\n \n void __tsan_java_move(jptr src, jptr dst, jptr size) {\n@@ -133,7 +133,7 @@ void __tsan_java_move(jptr src, jptr dst, jptr size) {\n   // support that anymore as it contains addresses of accesses.\n   RawShadow *d = MemToShadow(dst);\n   RawShadow *dend = MemToShadow(dst + size);\n-  internal_memset(d, 0, (dend - d) * sizeof(*d));\n+  ShadowSet(d, dend, Shadow::kEmpty);\n }\n \n jptr __tsan_java_find(jptr *from_ptr, jptr to) {"}, {"sha": "0937e521193f46f47e673c74651aa427221d3bb8", "filename": "libsanitizer/tsan/tsan_mman.cpp", "status": "modified", "additions": 61, "deletions": 21, "changes": 82, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_mman.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_mman.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_mman.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -20,18 +20,6 @@\n #include \"tsan_report.h\"\n #include \"tsan_flags.h\"\n \n-// May be overriden by front-end.\n-SANITIZER_WEAK_DEFAULT_IMPL\n-void __sanitizer_malloc_hook(void *ptr, uptr size) {\n-  (void)ptr;\n-  (void)size;\n-}\n-\n-SANITIZER_WEAK_DEFAULT_IMPL\n-void __sanitizer_free_hook(void *ptr) {\n-  (void)ptr;\n-}\n-\n namespace __tsan {\n \n struct MapUnmapCallback {\n@@ -69,15 +57,29 @@ Allocator *allocator() {\n struct GlobalProc {\n   Mutex mtx;\n   Processor *proc;\n-\n-  GlobalProc() : mtx(MutexTypeGlobalProc), proc(ProcCreate()) {}\n+  // This mutex represents the internal allocator combined for\n+  // the purposes of deadlock detection. The internal allocator\n+  // uses multiple mutexes, moreover they are locked only occasionally\n+  // and they are spin mutexes which don't support deadlock detection.\n+  // So we use this fake mutex to serve as a substitute for these mutexes.\n+  CheckedMutex internal_alloc_mtx;\n+\n+  GlobalProc()\n+      : mtx(MutexTypeGlobalProc),\n+        proc(ProcCreate()),\n+        internal_alloc_mtx(MutexTypeInternalAlloc) {}\n };\n \n static char global_proc_placeholder[sizeof(GlobalProc)] ALIGNED(64);\n GlobalProc *global_proc() {\n   return reinterpret_cast<GlobalProc*>(&global_proc_placeholder);\n }\n \n+static void InternalAllocAccess() {\n+  global_proc()->internal_alloc_mtx.Lock();\n+  global_proc()->internal_alloc_mtx.Unlock();\n+}\n+\n ScopedGlobalProcessor::ScopedGlobalProcessor() {\n   GlobalProc *gp = global_proc();\n   ThreadState *thr = cur_thread();\n@@ -110,6 +112,24 @@ ScopedGlobalProcessor::~ScopedGlobalProcessor() {\n   gp->mtx.Unlock();\n }\n \n+void AllocatorLock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  global_proc()->internal_alloc_mtx.Lock();\n+  InternalAllocatorLock();\n+}\n+\n+void AllocatorUnlock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  InternalAllocatorUnlock();\n+  global_proc()->internal_alloc_mtx.Unlock();\n+}\n+\n+void GlobalProcessorLock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  global_proc()->mtx.Lock();\n+}\n+\n+void GlobalProcessorUnlock() SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  global_proc()->mtx.Unlock();\n+}\n+\n static constexpr uptr kMaxAllowedMallocSize = 1ull << 40;\n static uptr max_user_defined_malloc_size;\n \n@@ -166,6 +186,12 @@ void *user_alloc_internal(ThreadState *thr, uptr pc, uptr sz, uptr align,\n     GET_STACK_TRACE_FATAL(thr, pc);\n     ReportAllocationSizeTooBig(sz, malloc_limit, &stack);\n   }\n+  if (UNLIKELY(IsRssLimitExceeded())) {\n+    if (AllocatorMayReturnNull())\n+      return nullptr;\n+    GET_STACK_TRACE_FATAL(thr, pc);\n+    ReportRssLimitExceeded(&stack);\n+  }\n   void *p = allocator()->Allocate(&thr->proc()->alloc_cache, sz, align);\n   if (UNLIKELY(!p)) {\n     SetAllocatorOutOfMemory();\n@@ -219,16 +245,32 @@ void *user_reallocarray(ThreadState *thr, uptr pc, void *p, uptr size, uptr n) {\n \n void OnUserAlloc(ThreadState *thr, uptr pc, uptr p, uptr sz, bool write) {\n   DPrintf(\"#%d: alloc(%zu) = 0x%zx\\n\", thr->tid, sz, p);\n+  // Note: this can run before thread initialization/after finalization.\n+  // As a result this is not necessarily synchronized with DoReset,\n+  // which iterates over and resets all sync objects,\n+  // but it is fine to create new MBlocks in this context.\n   ctx->metamap.AllocBlock(thr, pc, p, sz);\n-  if (write && thr->ignore_reads_and_writes == 0)\n+  // If this runs before thread initialization/after finalization\n+  // and we don't have trace initialized, we can't imitate writes.\n+  // In such case just reset the shadow range, it is fine since\n+  // it affects only a small fraction of special objects.\n+  if (write && thr->ignore_reads_and_writes == 0 &&\n+      atomic_load_relaxed(&thr->trace_pos))\n     MemoryRangeImitateWrite(thr, pc, (uptr)p, sz);\n   else\n     MemoryResetRange(thr, pc, (uptr)p, sz);\n }\n \n void OnUserFree(ThreadState *thr, uptr pc, uptr p, bool write) {\n   CHECK_NE(p, (void*)0);\n-  uptr sz = ctx->metamap.FreeBlock(thr->proc(), p);\n+  if (!thr->slot) {\n+    // Very early/late in thread lifetime, or during fork.\n+    UNUSED uptr sz = ctx->metamap.FreeBlock(thr->proc(), p, false);\n+    DPrintf(\"#%d: free(0x%zx, %zu) (no slot)\\n\", thr->tid, p, sz);\n+    return;\n+  }\n+  SlotLocker locker(thr);\n+  uptr sz = ctx->metamap.FreeBlock(thr->proc(), p, true);\n   DPrintf(\"#%d: free(0x%zx, %zu)\\n\", thr->tid, p, sz);\n   if (write && thr->ignore_reads_and_writes == 0)\n     MemoryRangeFreed(thr, pc, (uptr)p, sz);\n@@ -310,7 +352,7 @@ void *user_pvalloc(ThreadState *thr, uptr pc, uptr sz) {\n }\n \n uptr user_alloc_usable_size(const void *p) {\n-  if (p == 0)\n+  if (p == 0 || !IsAppMem((uptr)p))\n     return 0;\n   MBlock *b = ctx->metamap.GetBlock((uptr)p);\n   if (!b)\n@@ -324,15 +366,13 @@ void invoke_malloc_hook(void *ptr, uptr size) {\n   ThreadState *thr = cur_thread();\n   if (ctx == 0 || !ctx->initialized || thr->ignore_interceptors)\n     return;\n-  __sanitizer_malloc_hook(ptr, size);\n   RunMallocHooks(ptr, size);\n }\n \n void invoke_free_hook(void *ptr) {\n   ThreadState *thr = cur_thread();\n   if (ctx == 0 || !ctx->initialized || thr->ignore_interceptors)\n     return;\n-  __sanitizer_free_hook(ptr);\n   RunFreeHooks(ptr);\n }\n \n@@ -342,6 +382,7 @@ void *Alloc(uptr sz) {\n     thr->nomalloc = 0;  // CHECK calls internal_malloc().\n     CHECK(0);\n   }\n+  InternalAllocAccess();\n   return InternalAlloc(sz, &thr->proc()->internal_alloc_cache);\n }\n \n@@ -351,6 +392,7 @@ void FreeImpl(void *p) {\n     thr->nomalloc = 0;  // CHECK calls internal_malloc().\n     CHECK(0);\n   }\n+  InternalAllocAccess();\n   InternalFree(p, &thr->proc()->internal_alloc_cache);\n }\n \n@@ -393,8 +435,6 @@ uptr __sanitizer_get_allocated_size(const void *p) {\n \n void __tsan_on_thread_idle() {\n   ThreadState *thr = cur_thread();\n-  thr->clock.ResetCached(&thr->proc()->clock_cache);\n-  thr->last_sleep_clock.ResetCached(&thr->proc()->clock_cache);\n   allocator()->SwallowCache(&thr->proc()->alloc_cache);\n   internal_allocator()->SwallowCache(&thr->proc()->internal_alloc_cache);\n   ctx->metamap.OnProcIdle(thr->proc());"}, {"sha": "2095f28c0253e47a95aa1f8a2db9665b1d157cbc", "filename": "libsanitizer/tsan/tsan_mman.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_mman.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_mman.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_mman.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -24,6 +24,10 @@ void ReplaceSystemMalloc();\n void AllocatorProcStart(Processor *proc);\n void AllocatorProcFinish(Processor *proc);\n void AllocatorPrintStats();\n+void AllocatorLock();\n+void AllocatorUnlock();\n+void GlobalProcessorLock();\n+void GlobalProcessorUnlock();\n \n // For user allocations.\n void *user_alloc_internal(ThreadState *thr, uptr pc, uptr sz,"}, {"sha": "3a75b80ac30ffad9ed8f149ec77f05557c4f80ba", "filename": "libsanitizer/tsan/tsan_mutexset.cpp", "status": "modified", "additions": 1, "deletions": 53, "changes": 54, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_mutexset.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_mutexset.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_mutexset.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -19,57 +19,7 @@ namespace __tsan {\n MutexSet::MutexSet() {\n }\n \n-void MutexSet::Add(u64 id, bool write, u64 epoch) {\n-  // Look up existing mutex with the same id.\n-  for (uptr i = 0; i < size_; i++) {\n-    if (descs_[i].id == id) {\n-      descs_[i].count++;\n-      descs_[i].epoch = epoch;\n-      return;\n-    }\n-  }\n-  // On overflow, find the oldest mutex and drop it.\n-  if (size_ == kMaxSize) {\n-    u64 minepoch = (u64)-1;\n-    u64 mini = (u64)-1;\n-    for (uptr i = 0; i < size_; i++) {\n-      if (descs_[i].epoch < minepoch) {\n-        minepoch = descs_[i].epoch;\n-        mini = i;\n-      }\n-    }\n-    RemovePos(mini);\n-    CHECK_EQ(size_, kMaxSize - 1);\n-  }\n-  // Add new mutex descriptor.\n-  descs_[size_].addr = 0;\n-  descs_[size_].stack_id = kInvalidStackID;\n-  descs_[size_].id = id;\n-  descs_[size_].write = write;\n-  descs_[size_].epoch = epoch;\n-  descs_[size_].seq = seq_++;\n-  descs_[size_].count = 1;\n-  size_++;\n-}\n-\n-void MutexSet::Del(u64 id, bool write) {\n-  for (uptr i = 0; i < size_; i++) {\n-    if (descs_[i].id == id) {\n-      if (--descs_[i].count == 0)\n-        RemovePos(i);\n-      return;\n-    }\n-  }\n-}\n-\n-void MutexSet::Remove(u64 id) {\n-  for (uptr i = 0; i < size_; i++) {\n-    if (descs_[i].id == id) {\n-      RemovePos(i);\n-      return;\n-    }\n-  }\n-}\n+void MutexSet::Reset() { internal_memset(this, 0, sizeof(*this)); }\n \n void MutexSet::AddAddr(uptr addr, StackID stack_id, bool write) {\n   // Look up existing mutex with the same id.\n@@ -93,9 +43,7 @@ void MutexSet::AddAddr(uptr addr, StackID stack_id, bool write) {\n   // Add new mutex descriptor.\n   descs_[size_].addr = addr;\n   descs_[size_].stack_id = stack_id;\n-  descs_[size_].id = 0;\n   descs_[size_].write = write;\n-  descs_[size_].epoch = 0;\n   descs_[size_].seq = seq_++;\n   descs_[size_].count = 1;\n   size_++;"}, {"sha": "aabd361e6afd99c01d5103b898e3a5346a889d5f", "filename": "libsanitizer/tsan/tsan_mutexset.h", "status": "modified", "additions": 2, "deletions": 9, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_mutexset.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_mutexset.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_mutexset.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -25,8 +25,6 @@ class MutexSet {\n   struct Desc {\n     uptr addr;\n     StackID stack_id;\n-    u64 id;\n-    u64 epoch;\n     u32 seq;\n     u32 count;\n     bool write;\n@@ -40,10 +38,7 @@ class MutexSet {\n   };\n \n   MutexSet();\n-  // The 'id' is obtained from SyncVar::GetId().\n-  void Add(u64 id, bool write, u64 epoch);\n-  void Del(u64 id, bool write);\n-  void Remove(u64 id);  // Removes the mutex completely (if it's destroyed).\n+  void Reset();\n   void AddAddr(uptr addr, StackID stack_id, bool write);\n   void DelAddr(uptr addr, bool destroy = false);\n   uptr Size() const;\n@@ -82,9 +77,7 @@ class DynamicMutexSet {\n // in different goroutine).\n #if SANITIZER_GO\n MutexSet::MutexSet() {}\n-void MutexSet::Add(u64 id, bool write, u64 epoch) {}\n-void MutexSet::Del(u64 id, bool write) {}\n-void MutexSet::Remove(u64 id) {}\n+void MutexSet::Reset() {}\n void MutexSet::AddAddr(uptr addr, StackID stack_id, bool write) {}\n void MutexSet::DelAddr(uptr addr, bool destroy) {}\n uptr MutexSet::Size() const { return 0; }"}, {"sha": "233bf0a39df0781c96f0b3b2ce964f7b77d87743", "filename": "libsanitizer/tsan/tsan_platform.h", "status": "modified", "additions": 89, "deletions": 196, "changes": 285, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_platform.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_platform.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_platform.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -18,8 +18,8 @@\n # error \"Only 64-bit is supported\"\n #endif\n \n+#include \"sanitizer_common/sanitizer_common.h\"\n #include \"tsan_defs.h\"\n-#include \"tsan_trace.h\"\n \n namespace __tsan {\n \n@@ -40,14 +40,12 @@ enum {\n C/C++ on linux/x86_64 and freebsd/x86_64\n 0000 0000 1000 - 0080 0000 0000: main binary and/or MAP_32BIT mappings (512GB)\n 0040 0000 0000 - 0100 0000 0000: -\n-0100 0000 0000 - 2000 0000 0000: shadow\n-2000 0000 0000 - 3000 0000 0000: -\n+0100 0000 0000 - 1000 0000 0000: shadow\n+1000 0000 0000 - 3000 0000 0000: -\n 3000 0000 0000 - 4000 0000 0000: metainfo (memory blocks and sync objects)\n 4000 0000 0000 - 5500 0000 0000: -\n 5500 0000 0000 - 5680 0000 0000: pie binaries without ASLR or on 4.1+ kernels\n-5680 0000 0000 - 6000 0000 0000: -\n-6000 0000 0000 - 6200 0000 0000: traces\n-6200 0000 0000 - 7d00 0000 0000: -\n+5680 0000 0000 - 7d00 0000 0000: -\n 7b00 0000 0000 - 7c00 0000 0000: heap\n 7c00 0000 0000 - 7e80 0000 0000: -\n 7e80 0000 0000 - 8000 0000 0000: modules and main thread stack\n@@ -67,10 +65,8 @@ C/C++ on netbsd/amd64 can reuse the same mapping:\n struct Mapping48AddressSpace {\n   static const uptr kMetaShadowBeg = 0x300000000000ull;\n   static const uptr kMetaShadowEnd = 0x340000000000ull;\n-  static const uptr kTraceMemBeg   = 0x600000000000ull;\n-  static const uptr kTraceMemEnd   = 0x620000000000ull;\n   static const uptr kShadowBeg     = 0x010000000000ull;\n-  static const uptr kShadowEnd     = 0x200000000000ull;\n+  static const uptr kShadowEnd = 0x100000000000ull;\n   static const uptr kHeapMemBeg    = 0x7b0000000000ull;\n   static const uptr kHeapMemEnd    = 0x7c0000000000ull;\n   static const uptr kLoAppMemBeg   = 0x000000001000ull;\n@@ -89,25 +85,22 @@ struct Mapping48AddressSpace {\n C/C++ on linux/mips64 (40-bit VMA)\n 0000 0000 00 - 0100 0000 00: -                                           (4 GB)\n 0100 0000 00 - 0200 0000 00: main binary                                 (4 GB)\n-0200 0000 00 - 2000 0000 00: -                                         (120 GB)\n-2000 0000 00 - 4000 0000 00: shadow                                    (128 GB)\n+0200 0000 00 - 1200 0000 00: -                                          (64 GB)\n+1200 0000 00 - 2200 0000 00: shadow                                     (64 GB)\n+2200 0000 00 - 4000 0000 00: -                                         (120 GB)\n 4000 0000 00 - 5000 0000 00: metainfo (memory blocks and sync objects)  (64 GB)\n 5000 0000 00 - aa00 0000 00: -                                         (360 GB)\n aa00 0000 00 - ab00 0000 00: main binary (PIE)                           (4 GB)\n-ab00 0000 00 - b000 0000 00: -                                          (20 GB)\n-b000 0000 00 - b200 0000 00: traces                                      (8 GB)\n-b200 0000 00 - fe00 0000 00: -                                         (304 GB)\n+ab00 0000 00 - fe00 0000 00: -                                         (332 GB)\n fe00 0000 00 - ff00 0000 00: heap                                        (4 GB)\n ff00 0000 00 - ff80 0000 00: -                                           (2 GB)\n ff80 0000 00 - ffff ffff ff: modules and main thread stack              (<2 GB)\n */\n struct MappingMips64_40 {\n   static const uptr kMetaShadowBeg = 0x4000000000ull;\n   static const uptr kMetaShadowEnd = 0x5000000000ull;\n-  static const uptr kTraceMemBeg   = 0xb000000000ull;\n-  static const uptr kTraceMemEnd   = 0xb200000000ull;\n-  static const uptr kShadowBeg     = 0x2000000000ull;\n-  static const uptr kShadowEnd     = 0x4000000000ull;\n+  static const uptr kShadowBeg = 0x1200000000ull;\n+  static const uptr kShadowEnd = 0x2200000000ull;\n   static const uptr kHeapMemBeg    = 0xfe00000000ull;\n   static const uptr kHeapMemEnd    = 0xff00000000ull;\n   static const uptr kLoAppMemBeg   = 0x0100000000ull;\n@@ -128,29 +121,25 @@ C/C++ on Darwin/iOS/ARM64 (36-bit VMA, 64 GB VM)\n 0100 0000 00 - 0200 0000 00: main binary, modules, thread stacks  (4 GB)\n 0200 0000 00 - 0300 0000 00: heap                                 (4 GB)\n 0300 0000 00 - 0400 0000 00: -                                    (4 GB)\n-0400 0000 00 - 0c00 0000 00: shadow memory                       (32 GB)\n-0c00 0000 00 - 0d00 0000 00: -                                    (4 GB)\n+0400 0000 00 - 0800 0000 00: shadow memory                       (16 GB)\n+0800 0000 00 - 0d00 0000 00: -                                   (20 GB)\n 0d00 0000 00 - 0e00 0000 00: metainfo                             (4 GB)\n-0e00 0000 00 - 0f00 0000 00: -                                    (4 GB)\n-0f00 0000 00 - 0fc0 0000 00: traces                               (3 GB)\n-0fc0 0000 00 - 1000 0000 00: -\n+0e00 0000 00 - 1000 0000 00: -\n */\n struct MappingAppleAarch64 {\n   static const uptr kLoAppMemBeg   = 0x0100000000ull;\n   static const uptr kLoAppMemEnd   = 0x0200000000ull;\n   static const uptr kHeapMemBeg    = 0x0200000000ull;\n   static const uptr kHeapMemEnd    = 0x0300000000ull;\n   static const uptr kShadowBeg     = 0x0400000000ull;\n-  static const uptr kShadowEnd     = 0x0c00000000ull;\n+  static const uptr kShadowEnd = 0x0800000000ull;\n   static const uptr kMetaShadowBeg = 0x0d00000000ull;\n   static const uptr kMetaShadowEnd = 0x0e00000000ull;\n-  static const uptr kTraceMemBeg   = 0x0f00000000ull;\n-  static const uptr kTraceMemEnd   = 0x0fc0000000ull;\n   static const uptr kHiAppMemBeg   = 0x0fc0000000ull;\n   static const uptr kHiAppMemEnd   = 0x0fc0000000ull;\n   static const uptr kShadowMsk = 0x0ull;\n   static const uptr kShadowXor = 0x0ull;\n-  static const uptr kShadowAdd = 0x0ull;\n+  static const uptr kShadowAdd = 0x0200000000ull;\n   static const uptr kVdsoBeg       = 0x7000000000000000ull;\n   static const uptr kMidAppMemBeg = 0;\n   static const uptr kMidAppMemEnd = 0;\n@@ -159,29 +148,25 @@ struct MappingAppleAarch64 {\n /*\n C/C++ on linux/aarch64 (39-bit VMA)\n 0000 0010 00 - 0100 0000 00: main binary\n-0100 0000 00 - 0800 0000 00: -\n-0800 0000 00 - 2000 0000 00: shadow memory\n+0100 0000 00 - 0400 0000 00: -\n+0400 0000 00 - 1000 0000 00: shadow memory\n 2000 0000 00 - 3100 0000 00: -\n 3100 0000 00 - 3400 0000 00: metainfo\n 3400 0000 00 - 5500 0000 00: -\n 5500 0000 00 - 5600 0000 00: main binary (PIE)\n-5600 0000 00 - 6000 0000 00: -\n-6000 0000 00 - 6200 0000 00: traces\n-6200 0000 00 - 7d00 0000 00: -\n+5600 0000 00 - 7c00 0000 00: -\n 7c00 0000 00 - 7d00 0000 00: heap\n 7d00 0000 00 - 7fff ffff ff: modules and main thread stack\n */\n struct MappingAarch64_39 {\n   static const uptr kLoAppMemBeg   = 0x0000001000ull;\n   static const uptr kLoAppMemEnd   = 0x0100000000ull;\n-  static const uptr kShadowBeg     = 0x0800000000ull;\n-  static const uptr kShadowEnd     = 0x2000000000ull;\n+  static const uptr kShadowBeg = 0x0400000000ull;\n+  static const uptr kShadowEnd = 0x1000000000ull;\n   static const uptr kMetaShadowBeg = 0x3100000000ull;\n   static const uptr kMetaShadowEnd = 0x3400000000ull;\n   static const uptr kMidAppMemBeg  = 0x5500000000ull;\n-  static const uptr kMidAppMemEnd  = 0x5600000000ull;\n-  static const uptr kTraceMemBeg   = 0x6000000000ull;\n-  static const uptr kTraceMemEnd   = 0x6200000000ull;\n+  static const uptr kMidAppMemEnd = 0x5600000000ull;\n   static const uptr kHeapMemBeg    = 0x7c00000000ull;\n   static const uptr kHeapMemEnd    = 0x7d00000000ull;\n   static const uptr kHiAppMemBeg   = 0x7e00000000ull;\n@@ -195,30 +180,26 @@ struct MappingAarch64_39 {\n /*\n C/C++ on linux/aarch64 (42-bit VMA)\n 00000 0010 00 - 01000 0000 00: main binary\n-01000 0000 00 - 10000 0000 00: -\n-10000 0000 00 - 20000 0000 00: shadow memory\n-20000 0000 00 - 26000 0000 00: -\n+01000 0000 00 - 08000 0000 00: -\n+08000 0000 00 - 10000 0000 00: shadow memory\n+10000 0000 00 - 26000 0000 00: -\n 26000 0000 00 - 28000 0000 00: metainfo\n 28000 0000 00 - 2aa00 0000 00: -\n 2aa00 0000 00 - 2ab00 0000 00: main binary (PIE)\n-2ab00 0000 00 - 36200 0000 00: -\n-36200 0000 00 - 36240 0000 00: traces\n-36240 0000 00 - 3e000 0000 00: -\n+2ab00 0000 00 - 3e000 0000 00: -\n 3e000 0000 00 - 3f000 0000 00: heap\n 3f000 0000 00 - 3ffff ffff ff: modules and main thread stack\n */\n struct MappingAarch64_42 {\n   static const uptr kBroken = kBrokenReverseMapping;\n   static const uptr kLoAppMemBeg   = 0x00000001000ull;\n   static const uptr kLoAppMemEnd   = 0x01000000000ull;\n-  static const uptr kShadowBeg     = 0x10000000000ull;\n-  static const uptr kShadowEnd     = 0x20000000000ull;\n+  static const uptr kShadowBeg = 0x08000000000ull;\n+  static const uptr kShadowEnd = 0x10000000000ull;\n   static const uptr kMetaShadowBeg = 0x26000000000ull;\n   static const uptr kMetaShadowEnd = 0x28000000000ull;\n   static const uptr kMidAppMemBeg  = 0x2aa00000000ull;\n-  static const uptr kMidAppMemEnd  = 0x2ab00000000ull;\n-  static const uptr kTraceMemBeg   = 0x36200000000ull;\n-  static const uptr kTraceMemEnd   = 0x36400000000ull;\n+  static const uptr kMidAppMemEnd = 0x2ab00000000ull;\n   static const uptr kHeapMemBeg    = 0x3e000000000ull;\n   static const uptr kHeapMemEnd    = 0x3f000000000ull;\n   static const uptr kHiAppMemBeg   = 0x3f000000000ull;\n@@ -232,14 +213,12 @@ struct MappingAarch64_42 {\n struct MappingAarch64_48 {\n   static const uptr kLoAppMemBeg   = 0x0000000001000ull;\n   static const uptr kLoAppMemEnd   = 0x0000200000000ull;\n-  static const uptr kShadowBeg     = 0x0002000000000ull;\n-  static const uptr kShadowEnd     = 0x0004000000000ull;\n+  static const uptr kShadowBeg = 0x0001000000000ull;\n+  static const uptr kShadowEnd = 0x0002000000000ull;\n   static const uptr kMetaShadowBeg = 0x0005000000000ull;\n   static const uptr kMetaShadowEnd = 0x0006000000000ull;\n   static const uptr kMidAppMemBeg  = 0x0aaaa00000000ull;\n-  static const uptr kMidAppMemEnd  = 0x0aaaf00000000ull;\n-  static const uptr kTraceMemBeg   = 0x0f06000000000ull;\n-  static const uptr kTraceMemEnd   = 0x0f06200000000ull;\n+  static const uptr kMidAppMemEnd = 0x0aaaf00000000ull;\n   static const uptr kHeapMemBeg    = 0x0ffff00000000ull;\n   static const uptr kHeapMemEnd    = 0x0ffff00000000ull;\n   static const uptr kHiAppMemBeg   = 0x0ffff00000000ull;\n@@ -257,9 +236,7 @@ C/C++ on linux/powerpc64 (44-bit VMA)\n 0001 0000 0000 - 0b00 0000 0000: shadow\n 0b00 0000 0000 - 0b00 0000 0000: -\n 0b00 0000 0000 - 0d00 0000 0000: metainfo (memory blocks and sync objects)\n-0d00 0000 0000 - 0d00 0000 0000: -\n-0d00 0000 0000 - 0f00 0000 0000: traces\n-0f00 0000 0000 - 0f00 0000 0000: -\n+0d00 0000 0000 - 0f00 0000 0000: -\n 0f00 0000 0000 - 0f50 0000 0000: heap\n 0f50 0000 0000 - 0f60 0000 0000: -\n 0f60 0000 0000 - 1000 0000 0000: modules and main thread stack\n@@ -269,8 +246,6 @@ struct MappingPPC64_44 {\n       kBrokenMapping | kBrokenReverseMapping | kBrokenLinearity;\n   static const uptr kMetaShadowBeg = 0x0b0000000000ull;\n   static const uptr kMetaShadowEnd = 0x0d0000000000ull;\n-  static const uptr kTraceMemBeg   = 0x0d0000000000ull;\n-  static const uptr kTraceMemEnd   = 0x0f0000000000ull;\n   static const uptr kShadowBeg     = 0x000100000000ull;\n   static const uptr kShadowEnd     = 0x0b0000000000ull;\n   static const uptr kLoAppMemBeg   = 0x000000000100ull;\n@@ -291,23 +266,19 @@ struct MappingPPC64_44 {\n C/C++ on linux/powerpc64 (46-bit VMA)\n 0000 0000 1000 - 0100 0000 0000: main binary\n 0100 0000 0000 - 0200 0000 0000: -\n-0100 0000 0000 - 1000 0000 0000: shadow\n-1000 0000 0000 - 1000 0000 0000: -\n-1000 0000 0000 - 2000 0000 0000: metainfo (memory blocks and sync objects)\n-2000 0000 0000 - 2000 0000 0000: -\n-2000 0000 0000 - 2200 0000 0000: traces\n-2200 0000 0000 - 3d00 0000 0000: -\n+0100 0000 0000 - 0800 0000 0000: shadow\n+0800 0000 0000 - 1000 0000 0000: -\n+1000 0000 0000 - 1200 0000 0000: metainfo (memory blocks and sync objects)\n+1200 0000 0000 - 3d00 0000 0000: -\n 3d00 0000 0000 - 3e00 0000 0000: heap\n 3e00 0000 0000 - 3e80 0000 0000: -\n 3e80 0000 0000 - 4000 0000 0000: modules and main thread stack\n */\n struct MappingPPC64_46 {\n   static const uptr kMetaShadowBeg = 0x100000000000ull;\n-  static const uptr kMetaShadowEnd = 0x200000000000ull;\n-  static const uptr kTraceMemBeg   = 0x200000000000ull;\n-  static const uptr kTraceMemEnd   = 0x220000000000ull;\n+  static const uptr kMetaShadowEnd = 0x120000000000ull;\n   static const uptr kShadowBeg     = 0x010000000000ull;\n-  static const uptr kShadowEnd     = 0x100000000000ull;\n+  static const uptr kShadowEnd = 0x080000000000ull;\n   static const uptr kHeapMemBeg    = 0x3d0000000000ull;\n   static const uptr kHeapMemEnd    = 0x3e0000000000ull;\n   static const uptr kLoAppMemBeg   = 0x000000001000ull;\n@@ -326,23 +297,19 @@ struct MappingPPC64_46 {\n C/C++ on linux/powerpc64 (47-bit VMA)\n 0000 0000 1000 - 0100 0000 0000: main binary\n 0100 0000 0000 - 0200 0000 0000: -\n-0100 0000 0000 - 1000 0000 0000: shadow\n-1000 0000 0000 - 1000 0000 0000: -\n-1000 0000 0000 - 2000 0000 0000: metainfo (memory blocks and sync objects)\n-2000 0000 0000 - 2000 0000 0000: -\n-2000 0000 0000 - 2200 0000 0000: traces\n-2200 0000 0000 - 7d00 0000 0000: -\n+0100 0000 0000 - 0800 0000 0000: shadow\n+0800 0000 0000 - 1000 0000 0000: -\n+1000 0000 0000 - 1200 0000 0000: metainfo (memory blocks and sync objects)\n+1200 0000 0000 - 7d00 0000 0000: -\n 7d00 0000 0000 - 7e00 0000 0000: heap\n 7e00 0000 0000 - 7e80 0000 0000: -\n 7e80 0000 0000 - 8000 0000 0000: modules and main thread stack\n */\n struct MappingPPC64_47 {\n   static const uptr kMetaShadowBeg = 0x100000000000ull;\n-  static const uptr kMetaShadowEnd = 0x200000000000ull;\n-  static const uptr kTraceMemBeg   = 0x200000000000ull;\n-  static const uptr kTraceMemEnd   = 0x220000000000ull;\n+  static const uptr kMetaShadowEnd = 0x120000000000ull;\n   static const uptr kShadowBeg     = 0x010000000000ull;\n-  static const uptr kShadowEnd     = 0x100000000000ull;\n+  static const uptr kShadowEnd = 0x080000000000ull;\n   static const uptr kHeapMemBeg    = 0x7d0000000000ull;\n   static const uptr kHeapMemEnd    = 0x7e0000000000ull;\n   static const uptr kLoAppMemBeg   = 0x000000001000ull;\n@@ -362,22 +329,18 @@ C/C++ on linux/s390x\n While the kernel provides a 64-bit address space, we have to restrict ourselves\n to 48 bits due to how e.g. SyncVar::GetId() works.\n 0000 0000 1000 - 0e00 0000 0000: binary, modules, stacks - 14 TiB\n-0e00 0000 0000 - 4000 0000 0000: -\n-4000 0000 0000 - 8000 0000 0000: shadow - 64TiB (4 * app)\n-8000 0000 0000 - 9000 0000 0000: -\n+0e00 0000 0000 - 2000 0000 0000: -\n+2000 0000 0000 - 4000 0000 0000: shadow - 32TiB (2 * app)\n+4000 0000 0000 - 9000 0000 0000: -\n 9000 0000 0000 - 9800 0000 0000: metainfo - 8TiB (0.5 * app)\n-9800 0000 0000 - a000 0000 0000: -\n-a000 0000 0000 - b000 0000 0000: traces - 16TiB (max history * 128k threads)\n-b000 0000 0000 - be00 0000 0000: -\n+9800 0000 0000 - be00 0000 0000: -\n be00 0000 0000 - c000 0000 0000: heap - 2TiB (max supported by the allocator)\n */\n struct MappingS390x {\n   static const uptr kMetaShadowBeg = 0x900000000000ull;\n   static const uptr kMetaShadowEnd = 0x980000000000ull;\n-  static const uptr kTraceMemBeg   = 0xa00000000000ull;\n-  static const uptr kTraceMemEnd   = 0xb00000000000ull;\n-  static const uptr kShadowBeg     = 0x400000000000ull;\n-  static const uptr kShadowEnd     = 0x800000000000ull;\n+  static const uptr kShadowBeg = 0x200000000000ull;\n+  static const uptr kShadowEnd = 0x400000000000ull;\n   static const uptr kHeapMemBeg    = 0xbe0000000000ull;\n   static const uptr kHeapMemEnd    = 0xc00000000000ull;\n   static const uptr kLoAppMemBeg   = 0x000000001000ull;\n@@ -397,21 +360,17 @@ struct MappingS390x {\n 0000 1000 0000 - 00c0 0000 0000: -\n 00c0 0000 0000 - 00e0 0000 0000: heap\n 00e0 0000 0000 - 2000 0000 0000: -\n-2000 0000 0000 - 2380 0000 0000: shadow\n-2380 0000 0000 - 3000 0000 0000: -\n+2000 0000 0000 - 21c0 0000 0000: shadow\n+21c0 0000 0000 - 3000 0000 0000: -\n 3000 0000 0000 - 4000 0000 0000: metainfo (memory blocks and sync objects)\n-4000 0000 0000 - 6000 0000 0000: -\n-6000 0000 0000 - 6200 0000 0000: traces\n-6200 0000 0000 - 8000 0000 0000: -\n+4000 0000 0000 - 8000 0000 0000: -\n */\n \n struct MappingGo48 {\n   static const uptr kMetaShadowBeg = 0x300000000000ull;\n   static const uptr kMetaShadowEnd = 0x400000000000ull;\n-  static const uptr kTraceMemBeg   = 0x600000000000ull;\n-  static const uptr kTraceMemEnd   = 0x620000000000ull;\n   static const uptr kShadowBeg     = 0x200000000000ull;\n-  static const uptr kShadowEnd     = 0x238000000000ull;\n+  static const uptr kShadowEnd = 0x21c000000000ull;\n   static const uptr kLoAppMemBeg = 0x000000001000ull;\n   static const uptr kLoAppMemEnd = 0x00e000000000ull;\n   static const uptr kMidAppMemBeg = 0;\n@@ -431,19 +390,17 @@ struct MappingGo48 {\n 0000 1000 0000 - 00f8 0000 0000: -\n 00c0 0000 0000 - 00e0 0000 0000: heap\n 00e0 0000 0000 - 0100 0000 0000: -\n-0100 0000 0000 - 0500 0000 0000: shadow\n-0500 0000 0000 - 0700 0000 0000: traces\n+0100 0000 0000 - 0300 0000 0000: shadow\n+0300 0000 0000 - 0700 0000 0000: -\n 0700 0000 0000 - 0770 0000 0000: metainfo (memory blocks and sync objects)\n 07d0 0000 0000 - 8000 0000 0000: -\n */\n \n struct MappingGoWindows {\n   static const uptr kMetaShadowBeg = 0x070000000000ull;\n   static const uptr kMetaShadowEnd = 0x077000000000ull;\n-  static const uptr kTraceMemBeg = 0x050000000000ull;\n-  static const uptr kTraceMemEnd = 0x070000000000ull;\n   static const uptr kShadowBeg     = 0x010000000000ull;\n-  static const uptr kShadowEnd     = 0x050000000000ull;\n+  static const uptr kShadowEnd = 0x030000000000ull;\n   static const uptr kLoAppMemBeg = 0x000000001000ull;\n   static const uptr kLoAppMemEnd = 0x00e000000000ull;\n   static const uptr kMidAppMemBeg = 0;\n@@ -463,21 +420,17 @@ struct MappingGoWindows {\n 0000 1000 0000 - 00c0 0000 0000: -\n 00c0 0000 0000 - 00e0 0000 0000: heap\n 00e0 0000 0000 - 2000 0000 0000: -\n-2000 0000 0000 - 2380 0000 0000: shadow\n-2380 0000 0000 - 2400 0000 0000: -\n-2400 0000 0000 - 3400 0000 0000: metainfo (memory blocks and sync objects)\n-3400 0000 0000 - 3600 0000 0000: -\n-3600 0000 0000 - 3800 0000 0000: traces\n-3800 0000 0000 - 4000 0000 0000: -\n+2000 0000 0000 - 21c0 0000 0000: shadow\n+21c0 0000 0000 - 2400 0000 0000: -\n+2400 0000 0000 - 2470 0000 0000: metainfo (memory blocks and sync objects)\n+2470 0000 0000 - 4000 0000 0000: -\n */\n \n struct MappingGoPPC64_46 {\n   static const uptr kMetaShadowBeg = 0x240000000000ull;\n-  static const uptr kMetaShadowEnd = 0x340000000000ull;\n-  static const uptr kTraceMemBeg   = 0x360000000000ull;\n-  static const uptr kTraceMemEnd   = 0x380000000000ull;\n+  static const uptr kMetaShadowEnd = 0x247000000000ull;\n   static const uptr kShadowBeg     = 0x200000000000ull;\n-  static const uptr kShadowEnd     = 0x238000000000ull;\n+  static const uptr kShadowEnd = 0x21c000000000ull;\n   static const uptr kLoAppMemBeg = 0x000000001000ull;\n   static const uptr kLoAppMemEnd = 0x00e000000000ull;\n   static const uptr kMidAppMemBeg = 0;\n@@ -497,21 +450,17 @@ struct MappingGoPPC64_46 {\n 0000 1000 0000 - 00c0 0000 0000: -\n 00c0 0000 0000 - 00e0 0000 0000: heap\n 00e0 0000 0000 - 2000 0000 0000: -\n-2000 0000 0000 - 3000 0000 0000: shadow\n-3000 0000 0000 - 3000 0000 0000: -\n-3000 0000 0000 - 4000 0000 0000: metainfo (memory blocks and sync objects)\n-4000 0000 0000 - 6000 0000 0000: -\n-6000 0000 0000 - 6200 0000 0000: traces\n-6200 0000 0000 - 8000 0000 0000: -\n+2000 0000 0000 - 2800 0000 0000: shadow\n+2800 0000 0000 - 3000 0000 0000: -\n+3000 0000 0000 - 3200 0000 0000: metainfo (memory blocks and sync objects)\n+3200 0000 0000 - 8000 0000 0000: -\n */\n \n struct MappingGoPPC64_47 {\n   static const uptr kMetaShadowBeg = 0x300000000000ull;\n-  static const uptr kMetaShadowEnd = 0x400000000000ull;\n-  static const uptr kTraceMemBeg   = 0x600000000000ull;\n-  static const uptr kTraceMemEnd   = 0x620000000000ull;\n+  static const uptr kMetaShadowEnd = 0x320000000000ull;\n   static const uptr kShadowBeg     = 0x200000000000ull;\n-  static const uptr kShadowEnd     = 0x300000000000ull;\n+  static const uptr kShadowEnd = 0x280000000000ull;\n   static const uptr kLoAppMemBeg = 0x000000001000ull;\n   static const uptr kLoAppMemEnd = 0x00e000000000ull;\n   static const uptr kMidAppMemBeg = 0;\n@@ -531,20 +480,16 @@ struct MappingGoPPC64_47 {\n 0000 1000 0000 - 00c0 0000 0000: -\n 00c0 0000 0000 - 00e0 0000 0000: heap\n 00e0 0000 0000 - 2000 0000 0000: -\n-2000 0000 0000 - 3000 0000 0000: shadow\n-3000 0000 0000 - 3000 0000 0000: -\n-3000 0000 0000 - 4000 0000 0000: metainfo (memory blocks and sync objects)\n-4000 0000 0000 - 6000 0000 0000: -\n-6000 0000 0000 - 6200 0000 0000: traces\n-6200 0000 0000 - 8000 0000 0000: -\n+2000 0000 0000 - 2800 0000 0000: shadow\n+2800 0000 0000 - 3000 0000 0000: -\n+3000 0000 0000 - 3200 0000 0000: metainfo (memory blocks and sync objects)\n+3200 0000 0000 - 8000 0000 0000: -\n */\n struct MappingGoAarch64 {\n   static const uptr kMetaShadowBeg = 0x300000000000ull;\n-  static const uptr kMetaShadowEnd = 0x400000000000ull;\n-  static const uptr kTraceMemBeg   = 0x600000000000ull;\n-  static const uptr kTraceMemEnd   = 0x620000000000ull;\n+  static const uptr kMetaShadowEnd = 0x320000000000ull;\n   static const uptr kShadowBeg     = 0x200000000000ull;\n-  static const uptr kShadowEnd     = 0x300000000000ull;\n+  static const uptr kShadowEnd = 0x280000000000ull;\n   static const uptr kLoAppMemBeg = 0x000000001000ull;\n   static const uptr kLoAppMemEnd = 0x00e000000000ull;\n   static const uptr kMidAppMemBeg = 0;\n@@ -565,20 +510,16 @@ Go on linux/mips64 (47-bit VMA)\n 0000 1000 0000 - 00c0 0000 0000: -\n 00c0 0000 0000 - 00e0 0000 0000: heap\n 00e0 0000 0000 - 2000 0000 0000: -\n-2000 0000 0000 - 3000 0000 0000: shadow\n-3000 0000 0000 - 3000 0000 0000: -\n-3000 0000 0000 - 4000 0000 0000: metainfo (memory blocks and sync objects)\n-4000 0000 0000 - 6000 0000 0000: -\n-6000 0000 0000 - 6200 0000 0000: traces\n-6200 0000 0000 - 8000 0000 0000: -\n+2000 0000 0000 - 2800 0000 0000: shadow\n+2800 0000 0000 - 3000 0000 0000: -\n+3000 0000 0000 - 3200 0000 0000: metainfo (memory blocks and sync objects)\n+3200 0000 0000 - 8000 0000 0000: -\n */\n struct MappingGoMips64_47 {\n   static const uptr kMetaShadowBeg = 0x300000000000ull;\n-  static const uptr kMetaShadowEnd = 0x400000000000ull;\n-  static const uptr kTraceMemBeg = 0x600000000000ull;\n-  static const uptr kTraceMemEnd = 0x620000000000ull;\n+  static const uptr kMetaShadowEnd = 0x320000000000ull;\n   static const uptr kShadowBeg = 0x200000000000ull;\n-  static const uptr kShadowEnd = 0x300000000000ull;\n+  static const uptr kShadowEnd = 0x280000000000ull;\n   static const uptr kLoAppMemBeg = 0x000000001000ull;\n   static const uptr kLoAppMemEnd = 0x00e000000000ull;\n   static const uptr kMidAppMemBeg = 0;\n@@ -597,19 +538,15 @@ struct MappingGoMips64_47 {\n Go on linux/s390x\n 0000 0000 1000 - 1000 0000 0000: executable and heap - 16 TiB\n 1000 0000 0000 - 4000 0000 0000: -\n-4000 0000 0000 - 8000 0000 0000: shadow - 64TiB (4 * app)\n-8000 0000 0000 - 9000 0000 0000: -\n+4000 0000 0000 - 6000 0000 0000: shadow - 64TiB (4 * app)\n+6000 0000 0000 - 9000 0000 0000: -\n 9000 0000 0000 - 9800 0000 0000: metainfo - 8TiB (0.5 * app)\n-9800 0000 0000 - a000 0000 0000: -\n-a000 0000 0000 - b000 0000 0000: traces - 16TiB (max history * 128k threads)\n */\n struct MappingGoS390x {\n   static const uptr kMetaShadowBeg = 0x900000000000ull;\n   static const uptr kMetaShadowEnd = 0x980000000000ull;\n-  static const uptr kTraceMemBeg   = 0xa00000000000ull;\n-  static const uptr kTraceMemEnd   = 0xb00000000000ull;\n   static const uptr kShadowBeg     = 0x400000000000ull;\n-  static const uptr kShadowEnd     = 0x800000000000ull;\n+  static const uptr kShadowEnd = 0x600000000000ull;\n   static const uptr kLoAppMemBeg = 0x000000001000ull;\n   static const uptr kLoAppMemEnd = 0x100000000000ull;\n   static const uptr kMidAppMemBeg = 0;\n@@ -648,11 +585,11 @@ ALWAYS_INLINE auto SelectMapping(Arg arg) {\n   return Func::template Apply<MappingGo48>(arg);\n #  endif\n #else  // SANITIZER_GO\n-#  if defined(__x86_64__) || SANITIZER_IOSSIM || SANITIZER_MAC && !SANITIZER_IOS\n-  return Func::template Apply<Mapping48AddressSpace>(arg);\n-#  elif defined(__aarch64__) && defined(__APPLE__)\n+#  if SANITIZER_IOS && !SANITIZER_IOSSIM\n   return Func::template Apply<MappingAppleAarch64>(arg);\n-#  elif defined(__aarch64__) && !defined(__APPLE__)\n+#  elif defined(__x86_64__) || SANITIZER_MAC\n+  return Func::template Apply<Mapping48AddressSpace>(arg);\n+#  elif defined(__aarch64__)\n   switch (vmaSize) {\n     case 39:\n       return Func::template Apply<MappingAarch64_39>(arg);\n@@ -715,8 +652,6 @@ enum MappingType {\n   kShadowEnd,\n   kMetaShadowBeg,\n   kMetaShadowEnd,\n-  kTraceMemBeg,\n-  kTraceMemEnd,\n   kVdsoBeg,\n };\n \n@@ -750,10 +685,6 @@ struct MappingField {\n         return Mapping::kMetaShadowBeg;\n       case kMetaShadowEnd:\n         return Mapping::kMetaShadowEnd;\n-      case kTraceMemBeg:\n-        return Mapping::kTraceMemBeg;\n-      case kTraceMemEnd:\n-        return Mapping::kTraceMemEnd;\n     }\n     Die();\n   }\n@@ -792,11 +723,6 @@ uptr MetaShadowBeg(void) { return SelectMapping<MappingField>(kMetaShadowBeg); }\n ALWAYS_INLINE\n uptr MetaShadowEnd(void) { return SelectMapping<MappingField>(kMetaShadowEnd); }\n \n-ALWAYS_INLINE\n-uptr TraceMemBeg(void) { return SelectMapping<MappingField>(kTraceMemBeg); }\n-ALWAYS_INLINE\n-uptr TraceMemEnd(void) { return SelectMapping<MappingField>(kTraceMemEnd); }\n-\n struct IsAppMemImpl {\n   template <typename Mapping>\n   static bool Apply(uptr mem) {\n@@ -934,43 +860,10 @@ inline uptr RestoreAddr(uptr addr) {\n   return SelectMapping<RestoreAddrImpl>(addr);\n }\n \n-// The additional page is to catch shadow stack overflow as paging fault.\n-// Windows wants 64K alignment for mmaps.\n-const uptr kTotalTraceSize = (kTraceSize * sizeof(Event) + sizeof(Trace)\n-    + (64 << 10) + (64 << 10) - 1) & ~((64 << 10) - 1);\n-\n-struct GetThreadTraceImpl {\n-  template <typename Mapping>\n-  static uptr Apply(uptr tid) {\n-    uptr p = Mapping::kTraceMemBeg + tid * kTotalTraceSize;\n-    DCHECK_LT(p, Mapping::kTraceMemEnd);\n-    return p;\n-  }\n-};\n-\n-ALWAYS_INLINE\n-uptr GetThreadTrace(int tid) { return SelectMapping<GetThreadTraceImpl>(tid); }\n-\n-struct GetThreadTraceHeaderImpl {\n-  template <typename Mapping>\n-  static uptr Apply(uptr tid) {\n-    uptr p = Mapping::kTraceMemBeg + tid * kTotalTraceSize +\n-             kTraceSize * sizeof(Event);\n-    DCHECK_LT(p, Mapping::kTraceMemEnd);\n-    return p;\n-  }\n-};\n-\n-ALWAYS_INLINE\n-uptr GetThreadTraceHeader(int tid) {\n-  return SelectMapping<GetThreadTraceHeaderImpl>(tid);\n-}\n-\n void InitializePlatform();\n void InitializePlatformEarly();\n void CheckAndProtect();\n void InitializeShadowMemoryPlatform();\n-void FlushShadowMemory();\n void WriteMemoryProfile(char *buf, uptr buf_size, u64 uptime_ns);\n int ExtractResolvFDs(void *state, int *fds, int nfd);\n int ExtractRecvmsgFDs(void *msg, int *fds, int nfd);"}, {"sha": "17dbdff8a539358c7adf9dec4aacdd41f4e41d13", "filename": "libsanitizer/tsan/tsan_platform_linux.cpp", "status": "modified", "additions": 18, "deletions": 30, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_platform_linux.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_platform_linux.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_platform_linux.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -94,7 +94,6 @@ enum {\n   MemMeta,\n   MemFile,\n   MemMmap,\n-  MemTrace,\n   MemHeap,\n   MemOther,\n   MemCount,\n@@ -112,8 +111,6 @@ void FillProfileCallback(uptr p, uptr rss, bool file, uptr *mem) {\n     mem[file ? MemFile : MemMmap] += rss;\n   else if (p >= HeapMemBeg() && p < HeapMemEnd())\n     mem[MemHeap] += rss;\n-  else if (p >= TraceMemBeg() && p < TraceMemEnd())\n-    mem[MemTrace] += rss;\n   else\n     mem[MemOther] += rss;\n }\n@@ -126,42 +123,33 @@ void WriteMemoryProfile(char *buf, uptr buf_size, u64 uptime_ns) {\n   StackDepotStats stacks = StackDepotGetStats();\n   uptr nthread, nlive;\n   ctx->thread_registry.GetNumberOfThreads(&nthread, &nlive);\n+  uptr trace_mem;\n+  {\n+    Lock l(&ctx->slot_mtx);\n+    trace_mem = ctx->trace_part_total_allocated * sizeof(TracePart);\n+  }\n   uptr internal_stats[AllocatorStatCount];\n   internal_allocator()->GetStats(internal_stats);\n   // All these are allocated from the common mmap region.\n-  mem[MemMmap] -= meta.mem_block + meta.sync_obj + stacks.allocated +\n-                  internal_stats[AllocatorStatMapped];\n+  mem[MemMmap] -= meta.mem_block + meta.sync_obj + trace_mem +\n+                  stacks.allocated + internal_stats[AllocatorStatMapped];\n   if (s64(mem[MemMmap]) < 0)\n     mem[MemMmap] = 0;\n   internal_snprintf(\n       buf, buf_size,\n-      \"%llus: RSS %zd MB: shadow:%zd meta:%zd file:%zd mmap:%zd\"\n-      \" trace:%zd heap:%zd other:%zd intalloc:%zd memblocks:%zd syncobj:%zu\"\n-      \" stacks=%zd[%zd] nthr=%zd/%zd\\n\",\n-      uptime_ns / (1000 * 1000 * 1000), mem[MemTotal] >> 20,\n-      mem[MemShadow] >> 20, mem[MemMeta] >> 20, mem[MemFile] >> 20,\n-      mem[MemMmap] >> 20, mem[MemTrace] >> 20, mem[MemHeap] >> 20,\n+      \"==%zu== %llus [%zu]: RSS %zd MB: shadow:%zd meta:%zd file:%zd\"\n+      \" mmap:%zd heap:%zd other:%zd intalloc:%zd memblocks:%zd syncobj:%zu\"\n+      \" trace:%zu stacks=%zd threads=%zu/%zu\\n\",\n+      internal_getpid(), uptime_ns / (1000 * 1000 * 1000), ctx->global_epoch,\n+      mem[MemTotal] >> 20, mem[MemShadow] >> 20, mem[MemMeta] >> 20,\n+      mem[MemFile] >> 20, mem[MemMmap] >> 20, mem[MemHeap] >> 20,\n       mem[MemOther] >> 20, internal_stats[AllocatorStatMapped] >> 20,\n-      meta.mem_block >> 20, meta.sync_obj >> 20, stacks.allocated >> 20,\n-      stacks.n_uniq_ids, nlive, nthread);\n-}\n-\n-#  if SANITIZER_LINUX\n-void FlushShadowMemoryCallback(\n-    const SuspendedThreadsList &suspended_threads_list,\n-    void *argument) {\n-  ReleaseMemoryPagesToOS(ShadowBeg(), ShadowEnd());\n-}\n-#endif\n-\n-void FlushShadowMemory() {\n-#if SANITIZER_LINUX\n-  StopTheWorld(FlushShadowMemoryCallback, 0);\n-#endif\n+      meta.mem_block >> 20, meta.sync_obj >> 20, trace_mem >> 20,\n+      stacks.allocated >> 20, nlive, nthread);\n }\n \n #if !SANITIZER_GO\n-// Mark shadow for .rodata sections with the special kShadowRodata marker.\n+// Mark shadow for .rodata sections with the special Shadow::kRodata marker.\n // Accesses to .rodata can't race, so this saves time, memory and trace space.\n static void MapRodata() {\n   // First create temp file.\n@@ -182,13 +170,13 @@ static void MapRodata() {\n     return;\n   internal_unlink(name);  // Unlink it now, so that we can reuse the buffer.\n   fd_t fd = openrv;\n-  // Fill the file with kShadowRodata.\n+  // Fill the file with Shadow::kRodata.\n   const uptr kMarkerSize = 512 * 1024 / sizeof(RawShadow);\n   InternalMmapVector<RawShadow> marker(kMarkerSize);\n   // volatile to prevent insertion of memset\n   for (volatile RawShadow *p = marker.data(); p < marker.data() + kMarkerSize;\n        p++)\n-    *p = kShadowRodata;\n+    *p = Shadow::kRodata;\n   internal_write(fd, marker.data(), marker.size() * sizeof(RawShadow));\n   // Map the file into memory.\n   uptr page = internal_mmap(0, GetPageSizeCached(), PROT_READ | PROT_WRITE,"}, {"sha": "44b98d46cfbcd24a3155a5619a6c19c9345e7896", "filename": "libsanitizer/tsan/tsan_platform_mac.cpp", "status": "modified", "additions": 72, "deletions": 80, "changes": 152, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_platform_mac.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_platform_mac.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_platform_mac.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -25,6 +25,7 @@\n #include \"tsan_rtl.h\"\n #include \"tsan_flags.h\"\n \n+#include <limits.h>\n #include <mach/mach.h>\n #include <pthread.h>\n #include <signal.h>\n@@ -45,76 +46,86 @@\n namespace __tsan {\n \n #if !SANITIZER_GO\n-static void *SignalSafeGetOrAllocate(uptr *dst, uptr size) {\n-  atomic_uintptr_t *a = (atomic_uintptr_t *)dst;\n-  void *val = (void *)atomic_load_relaxed(a);\n-  atomic_signal_fence(memory_order_acquire);  // Turns the previous load into\n-                                              // acquire wrt signals.\n-  if (UNLIKELY(val == nullptr)) {\n-    val = (void *)internal_mmap(nullptr, size, PROT_READ | PROT_WRITE,\n-                                MAP_PRIVATE | MAP_ANON, -1, 0);\n-    CHECK(val);\n-    void *cmp = nullptr;\n-    if (!atomic_compare_exchange_strong(a, (uintptr_t *)&cmp, (uintptr_t)val,\n-                                        memory_order_acq_rel)) {\n-      internal_munmap(val, size);\n-      val = cmp;\n-    }\n-  }\n-  return val;\n+static char main_thread_state[sizeof(ThreadState)] ALIGNED(\n+    SANITIZER_CACHE_LINE_SIZE);\n+static ThreadState *dead_thread_state;\n+static pthread_key_t thread_state_key;\n+\n+// We rely on the following documented, but Darwin-specific behavior to keep the\n+// reference to the ThreadState object alive in TLS:\n+// pthread_key_create man page:\n+//   If, after all the destructors have been called for all non-NULL values with\n+//   associated destructors, there are still some non-NULL values with\n+//   associated destructors, then the process is repeated.  If, after at least\n+//   [PTHREAD_DESTRUCTOR_ITERATIONS] iterations of destructor calls for\n+//   outstanding non-NULL values, there are still some non-NULL values with\n+//   associated destructors, the implementation stops calling destructors.\n+static_assert(PTHREAD_DESTRUCTOR_ITERATIONS == 4, \"Small number of iterations\");\n+static void ThreadStateDestructor(void *thr) {\n+  int res = pthread_setspecific(thread_state_key, thr);\n+  CHECK_EQ(res, 0);\n }\n \n-// On OS X, accessing TLVs via __thread or manually by using pthread_key_* is\n-// problematic, because there are several places where interceptors are called\n-// when TLVs are not accessible (early process startup, thread cleanup, ...).\n-// The following provides a \"poor man's TLV\" implementation, where we use the\n-// shadow memory of the pointer returned by pthread_self() to store a pointer to\n-// the ThreadState object. The main thread's ThreadState is stored separately\n-// in a static variable, because we need to access it even before the\n-// shadow memory is set up.\n-static uptr main_thread_identity = 0;\n-ALIGNED(64) static char main_thread_state[sizeof(ThreadState)];\n-static ThreadState *main_thread_state_loc = (ThreadState *)main_thread_state;\n-\n-// We cannot use pthread_self() before libpthread has been initialized.  Our\n-// current heuristic for guarding this is checking `main_thread_identity` which\n-// is only assigned in `__tsan::InitializePlatform`.\n-static ThreadState **cur_thread_location() {\n-  if (main_thread_identity == 0)\n-    return &main_thread_state_loc;\n-  uptr thread_identity = (uptr)pthread_self();\n-  if (thread_identity == main_thread_identity)\n-    return &main_thread_state_loc;\n-  return (ThreadState **)MemToShadow(thread_identity);\n+static void InitializeThreadStateStorage() {\n+  int res;\n+  CHECK_EQ(thread_state_key, 0);\n+  res = pthread_key_create(&thread_state_key, ThreadStateDestructor);\n+  CHECK_EQ(res, 0);\n+  res = pthread_setspecific(thread_state_key, main_thread_state);\n+  CHECK_EQ(res, 0);\n+\n+  auto dts = (ThreadState *)MmapOrDie(sizeof(ThreadState), \"ThreadState\");\n+  dts->fast_state.SetIgnoreBit();\n+  dts->ignore_interceptors = 1;\n+  dts->is_dead = true;\n+  const_cast<Tid &>(dts->tid) = kInvalidTid;\n+  res = internal_mprotect(dts, sizeof(ThreadState), PROT_READ);  // immutable\n+  CHECK_EQ(res, 0);\n+  dead_thread_state = dts;\n }\n \n ThreadState *cur_thread() {\n-  return (ThreadState *)SignalSafeGetOrAllocate(\n-      (uptr *)cur_thread_location(), sizeof(ThreadState));\n+  // Some interceptors get called before libpthread has been initialized and in\n+  // these cases we must avoid calling any pthread APIs.\n+  if (UNLIKELY(!thread_state_key)) {\n+    return (ThreadState *)main_thread_state;\n+  }\n+\n+  // We only reach this line after InitializeThreadStateStorage() ran, i.e,\n+  // after TSan (and therefore libpthread) have been initialized.\n+  ThreadState *thr = (ThreadState *)pthread_getspecific(thread_state_key);\n+  if (UNLIKELY(!thr)) {\n+    thr = (ThreadState *)MmapOrDie(sizeof(ThreadState), \"ThreadState\");\n+    int res = pthread_setspecific(thread_state_key, thr);\n+    CHECK_EQ(res, 0);\n+  }\n+  return thr;\n }\n \n void set_cur_thread(ThreadState *thr) {\n-  *cur_thread_location() = thr;\n+  int res = pthread_setspecific(thread_state_key, thr);\n+  CHECK_EQ(res, 0);\n }\n \n-// TODO(kuba.brecka): This is not async-signal-safe. In particular, we call\n-// munmap first and then clear `fake_tls`; if we receive a signal in between,\n-// handler will try to access the unmapped ThreadState.\n void cur_thread_finalize() {\n-  ThreadState **thr_state_loc = cur_thread_location();\n-  if (thr_state_loc == &main_thread_state_loc) {\n+  ThreadState *thr = (ThreadState *)pthread_getspecific(thread_state_key);\n+  CHECK(thr);\n+  if (thr == (ThreadState *)main_thread_state) {\n     // Calling dispatch_main() or xpc_main() actually invokes pthread_exit to\n     // exit the main thread. Let's keep the main thread's ThreadState.\n     return;\n   }\n-  internal_munmap(*thr_state_loc, sizeof(ThreadState));\n-  *thr_state_loc = nullptr;\n+  // Intercepted functions can still get called after cur_thread_finalize()\n+  // (called from DestroyThreadState()), so put a fake thread state for \"dead\"\n+  // threads.  An alternative solution would be to release the ThreadState\n+  // object from THREAD_DESTROY (which is delivered later and on the parent\n+  // thread) instead of THREAD_TERMINATE.\n+  int res = pthread_setspecific(thread_state_key, dead_thread_state);\n+  CHECK_EQ(res, 0);\n+  UnmapOrDie(thr, sizeof(ThreadState));\n }\n #endif\n \n-void FlushShadowMemory() {\n-}\n-\n static void RegionMemUsage(uptr start, uptr end, uptr *res, uptr *dirty) {\n   vm_address_t address = start;\n   vm_address_t end_address = end;\n@@ -142,12 +153,10 @@ static void RegionMemUsage(uptr start, uptr end, uptr *res, uptr *dirty) {\n void WriteMemoryProfile(char *buf, uptr buf_size, u64 uptime_ns) {\n   uptr shadow_res, shadow_dirty;\n   uptr meta_res, meta_dirty;\n-  uptr trace_res, trace_dirty;\n   RegionMemUsage(ShadowBeg(), ShadowEnd(), &shadow_res, &shadow_dirty);\n   RegionMemUsage(MetaShadowBeg(), MetaShadowEnd(), &meta_res, &meta_dirty);\n-  RegionMemUsage(TraceMemBeg(), TraceMemEnd(), &trace_res, &trace_dirty);\n \n-#if !SANITIZER_GO\n+#  if !SANITIZER_GO\n   uptr low_res, low_dirty;\n   uptr high_res, high_dirty;\n   uptr heap_res, heap_dirty;\n@@ -166,7 +175,6 @@ void WriteMemoryProfile(char *buf, uptr buf_size, u64 uptime_ns) {\n       buf, buf_size,\n       \"shadow   (0x%016zx-0x%016zx): resident %zd kB, dirty %zd kB\\n\"\n       \"meta     (0x%016zx-0x%016zx): resident %zd kB, dirty %zd kB\\n\"\n-      \"traces   (0x%016zx-0x%016zx): resident %zd kB, dirty %zd kB\\n\"\n #  if !SANITIZER_GO\n       \"low app  (0x%016zx-0x%016zx): resident %zd kB, dirty %zd kB\\n\"\n       \"high app (0x%016zx-0x%016zx): resident %zd kB, dirty %zd kB\\n\"\n@@ -179,7 +187,6 @@ void WriteMemoryProfile(char *buf, uptr buf_size, u64 uptime_ns) {\n       \"------------------------------\\n\",\n       ShadowBeg(), ShadowEnd(), shadow_res / 1024, shadow_dirty / 1024,\n       MetaShadowBeg(), MetaShadowEnd(), meta_res / 1024, meta_dirty / 1024,\n-      TraceMemBeg(), TraceMemEnd(), trace_res / 1024, trace_dirty / 1024,\n #  if !SANITIZER_GO\n       LoAppMemBeg(), LoAppMemEnd(), low_res / 1024, low_dirty / 1024,\n       HiAppMemBeg(), HiAppMemEnd(), high_res / 1024, high_dirty / 1024,\n@@ -222,11 +229,10 @@ static void my_pthread_introspection_hook(unsigned int event, pthread_t thread,\n       ThreadStart(thr, tid, GetTid(), ThreadType::Worker);\n     }\n   } else if (event == PTHREAD_INTROSPECTION_THREAD_TERMINATE) {\n-    if (thread == pthread_self()) {\n-      ThreadState *thr = cur_thread();\n-      if (thr->tctx) {\n-        DestroyThreadState();\n-      }\n+    CHECK_EQ(thread, pthread_self());\n+    ThreadState *thr = cur_thread();\n+    if (thr->tctx) {\n+      DestroyThreadState();\n     }\n   }\n \n@@ -253,8 +259,7 @@ void InitializePlatform() {\n #if !SANITIZER_GO\n   CheckAndProtect();\n \n-  CHECK_EQ(main_thread_identity, 0);\n-  main_thread_identity = (uptr)pthread_self();\n+  InitializeThreadStateStorage();\n \n   prev_pthread_introspection_hook =\n       pthread_introspection_hook_install(&my_pthread_introspection_hook);\n@@ -286,24 +291,11 @@ uptr ExtractLongJmpSp(uptr *env) {\n extern \"C\" void __tsan_tls_initialization() {}\n \n void ImitateTlsWrite(ThreadState *thr, uptr tls_addr, uptr tls_size) {\n-  // The pointer to the ThreadState object is stored in the shadow memory\n-  // of the tls.\n-  uptr tls_end = tls_addr + tls_size;\n-  uptr thread_identity = (uptr)pthread_self();\n   const uptr pc = StackTrace::GetNextInstructionPc(\n       reinterpret_cast<uptr>(__tsan_tls_initialization));\n-  if (thread_identity == main_thread_identity) {\n-    MemoryRangeImitateWrite(thr, pc, tls_addr, tls_size);\n-  } else {\n-    uptr thr_state_start = thread_identity;\n-    uptr thr_state_end = thr_state_start + sizeof(uptr);\n-    CHECK_GE(thr_state_start, tls_addr);\n-    CHECK_LE(thr_state_start, tls_addr + tls_size);\n-    CHECK_GE(thr_state_end, tls_addr);\n-    CHECK_LE(thr_state_end, tls_addr + tls_size);\n-    MemoryRangeImitateWrite(thr, pc, tls_addr, thr_state_start - tls_addr);\n-    MemoryRangeImitateWrite(thr, pc, thr_state_end, tls_end - thr_state_end);\n-  }\n+  // Unlike Linux, we only store a pointer to the ThreadState object in TLS;\n+  // just mark the entire range as written to.\n+  MemoryRangeImitateWrite(thr, pc, tls_addr, tls_size);\n }\n #endif\n "}, {"sha": "71874aad8dc5e84a730390108eb2e627c96c99fb", "filename": "libsanitizer/tsan/tsan_platform_posix.cpp", "status": "modified", "additions": 7, "deletions": 11, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_platform_posix.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_platform_posix.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_platform_posix.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -110,27 +110,23 @@ void CheckAndProtect() {\n     Die();\n   }\n \n-#    if defined(__aarch64__) && defined(__APPLE__) && SANITIZER_IOS\n+#    if SANITIZER_IOS && !SANITIZER_IOSSIM\n   ProtectRange(HeapMemEnd(), ShadowBeg());\n   ProtectRange(ShadowEnd(), MetaShadowBeg());\n-  ProtectRange(MetaShadowEnd(), TraceMemBeg());\n-#else\n+  ProtectRange(MetaShadowEnd(), HiAppMemBeg());\n+#    else\n   ProtectRange(LoAppMemEnd(), ShadowBeg());\n   ProtectRange(ShadowEnd(), MetaShadowBeg());\n   if (MidAppMemBeg()) {\n     ProtectRange(MetaShadowEnd(), MidAppMemBeg());\n-    ProtectRange(MidAppMemEnd(), TraceMemBeg());\n+    ProtectRange(MidAppMemEnd(), HeapMemBeg());\n   } else {\n-    ProtectRange(MetaShadowEnd(), TraceMemBeg());\n+    ProtectRange(MetaShadowEnd(), HeapMemBeg());\n   }\n-  // Memory for traces is mapped lazily in MapThreadTrace.\n-  // Protect the whole range for now, so that user does not map something here.\n-  ProtectRange(TraceMemBeg(), TraceMemEnd());\n-  ProtectRange(TraceMemEnd(), HeapMemBeg());\n   ProtectRange(HeapEnd(), HiAppMemBeg());\n-#endif\n+#    endif\n \n-#if defined(__s390x__)\n+#    if defined(__s390x__)\n   // Protect the rest of the address space.\n   const uptr user_addr_max_l4 = 0x0020000000000000ull;\n   const uptr user_addr_max_l5 = 0xfffffffffffff000ull;"}, {"sha": "eb8f354742f4ac357c3b85742d972d310844c92e", "filename": "libsanitizer/tsan/tsan_platform_windows.cpp", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_platform_windows.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_platform_windows.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_platform_windows.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -20,9 +20,6 @@\n \n namespace __tsan {\n \n-void FlushShadowMemory() {\n-}\n-\n void WriteMemoryProfile(char *buf, uptr buf_size, u64 uptime_ns) {}\n \n void InitializePlatformEarly() {"}, {"sha": "9f151279b601ddd7bdd4009511a4f95b8ac0e55f", "filename": "libsanitizer/tsan/tsan_report.cpp", "status": "modified", "additions": 13, "deletions": 16, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_report.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_report.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_report.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -126,7 +126,7 @@ static void PrintMutexSet(Vector<ReportMopMutex> const& mset) {\n     if (i == 0)\n       Printf(\" (mutexes:\");\n     const ReportMopMutex m = mset[i];\n-    Printf(\" %s M%llu\", m.write ? \"write\" : \"read\", m.id);\n+    Printf(\" %s M%u\", m.write ? \"write\" : \"read\", m.id);\n     Printf(i == mset.Size() - 1 ? \")\" : \",\");\n   }\n }\n@@ -211,29 +211,23 @@ static void PrintLocation(const ReportLocation *loc) {\n \n static void PrintMutexShort(const ReportMutex *rm, const char *after) {\n   Decorator d;\n-  Printf(\"%sM%lld%s%s\", d.Mutex(), rm->id, d.Default(), after);\n+  Printf(\"%sM%d%s%s\", d.Mutex(), rm->id, d.Default(), after);\n }\n \n static void PrintMutexShortWithAddress(const ReportMutex *rm,\n                                        const char *after) {\n   Decorator d;\n-  Printf(\"%sM%lld (%p)%s%s\", d.Mutex(), rm->id,\n+  Printf(\"%sM%d (%p)%s%s\", d.Mutex(), rm->id,\n          reinterpret_cast<void *>(rm->addr), d.Default(), after);\n }\n \n static void PrintMutex(const ReportMutex *rm) {\n   Decorator d;\n-  if (rm->destroyed) {\n-    Printf(\"%s\", d.Mutex());\n-    Printf(\"  Mutex M%llu is already destroyed.\\n\\n\", rm->id);\n-    Printf(\"%s\", d.Default());\n-  } else {\n-    Printf(\"%s\", d.Mutex());\n-    Printf(\"  Mutex M%llu (%p) created at:\\n\", rm->id,\n-           reinterpret_cast<void *>(rm->addr));\n-    Printf(\"%s\", d.Default());\n-    PrintStack(rm->stack);\n-  }\n+  Printf(\"%s\", d.Mutex());\n+  Printf(\"  Mutex M%u (%p) created at:\\n\", rm->id,\n+         reinterpret_cast<void *>(rm->addr));\n+  Printf(\"%s\", d.Default());\n+  PrintStack(rm->stack);\n }\n \n static void PrintThread(const ReportThread *rt) {\n@@ -312,6 +306,9 @@ void PrintReport(const ReportDesc *rep) {\n          (int)internal_getpid());\n   Printf(\"%s\", d.Default());\n \n+  if (rep->typ == ReportTypeErrnoInSignal)\n+    Printf(\"  Signal %u handler invoked at:\\n\", rep->signum);\n+\n   if (rep->typ == ReportTypeDeadlock) {\n     char thrbuf[kThreadBufSize];\n     Printf(\"  Cycle in lock order graph: \");\n@@ -460,12 +457,12 @@ void PrintReport(const ReportDesc *rep) {\n   } else if (rep->typ == ReportTypeDeadlock) {\n     Printf(\"WARNING: DEADLOCK\\n\");\n     for (uptr i = 0; i < rep->mutexes.Size(); i++) {\n-      Printf(\"Goroutine %d lock mutex %llu while holding mutex %llu:\\n\", 999,\n+      Printf(\"Goroutine %d lock mutex %u while holding mutex %u:\\n\", 999,\n              rep->mutexes[i]->id,\n              rep->mutexes[(i + 1) % rep->mutexes.Size()]->id);\n       PrintStack(rep->stacks[2*i]);\n       Printf(\"\\n\");\n-      Printf(\"Mutex %llu was previously locked here:\\n\",\n+      Printf(\"Mutex %u was previously locked here:\\n\",\n              rep->mutexes[(i + 1) % rep->mutexes.Size()]->id);\n       PrintStack(rep->stacks[2*i + 1]);\n       Printf(\"\\n\");"}, {"sha": "718eacde7ec4c8efcd19357a1f865ca025437d8b", "filename": "libsanitizer/tsan/tsan_report.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_report.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_report.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_report.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -43,7 +43,7 @@ struct ReportStack {\n };\n \n struct ReportMopMutex {\n-  u64 id;\n+  int id;\n   bool write;\n };\n \n@@ -91,9 +91,8 @@ struct ReportThread {\n };\n \n struct ReportMutex {\n-  u64 id;\n+  int id;\n   uptr addr;\n-  bool destroyed;\n   ReportStack *stack;\n };\n \n@@ -109,6 +108,7 @@ class ReportDesc {\n   Vector<Tid> unique_tids;\n   ReportStack *sleep;\n   int count;\n+  int signum = 0;\n \n   ReportDesc();\n   ~ReportDesc();"}, {"sha": "1d6fc7252662669fbb14c0ddf2cea8777963078a", "filename": "libsanitizer/tsan/tsan_rtl.cpp", "status": "modified", "additions": 516, "deletions": 208, "changes": 724, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -16,6 +16,7 @@\n #include \"sanitizer_common/sanitizer_atomic.h\"\n #include \"sanitizer_common/sanitizer_common.h\"\n #include \"sanitizer_common/sanitizer_file.h\"\n+#include \"sanitizer_common/sanitizer_interface_internal.h\"\n #include \"sanitizer_common/sanitizer_libc.h\"\n #include \"sanitizer_common/sanitizer_placement_new.h\"\n #include \"sanitizer_common/sanitizer_stackdepot.h\"\n@@ -34,6 +35,9 @@ extern \"C\" void __tsan_resume() {\n   __tsan_resumed = 1;\n }\n \n+SANITIZER_WEAK_DEFAULT_IMPL\n+void __tsan_test_only_on_fork() {}\n+\n namespace __tsan {\n \n #if !SANITIZER_GO\n@@ -54,109 +58,355 @@ Context *ctx;\n bool OnFinalize(bool failed);\n void OnInitialize();\n #else\n-#include <dlfcn.h>\n SANITIZER_WEAK_CXX_DEFAULT_IMPL\n bool OnFinalize(bool failed) {\n-#if !SANITIZER_GO\n+#  if !SANITIZER_GO\n   if (on_finalize)\n     return on_finalize(failed);\n-#endif\n+#  endif\n   return failed;\n }\n+\n SANITIZER_WEAK_CXX_DEFAULT_IMPL\n void OnInitialize() {\n-#if !SANITIZER_GO\n+#  if !SANITIZER_GO\n   if (on_initialize)\n     on_initialize();\n-#endif\n+#  endif\n }\n #endif\n \n-static ThreadContextBase *CreateThreadContext(Tid tid) {\n-  // Map thread trace when context is created.\n-  char name[50];\n-  internal_snprintf(name, sizeof(name), \"trace %u\", tid);\n-  MapThreadTrace(GetThreadTrace(tid), TraceSize() * sizeof(Event), name);\n-  const uptr hdr = GetThreadTraceHeader(tid);\n-  internal_snprintf(name, sizeof(name), \"trace header %u\", tid);\n-  MapThreadTrace(hdr, sizeof(Trace), name);\n-  new((void*)hdr) Trace();\n-  // We are going to use only a small part of the trace with the default\n-  // value of history_size. However, the constructor writes to the whole trace.\n-  // Release the unused part.\n-  uptr hdr_end = hdr + sizeof(Trace);\n-  hdr_end -= sizeof(TraceHeader) * (kTraceParts - TraceParts());\n-  hdr_end = RoundUp(hdr_end, GetPageSizeCached());\n-  if (hdr_end < hdr + sizeof(Trace)) {\n-    ReleaseMemoryPagesToOS(hdr_end, hdr + sizeof(Trace));\n-    uptr unused = hdr + sizeof(Trace) - hdr_end;\n-    if (hdr_end != (uptr)MmapFixedNoAccess(hdr_end, unused)) {\n-      Report(\"ThreadSanitizer: failed to mprotect [0x%zx-0x%zx) \\n\", hdr_end,\n-             unused);\n-      CHECK(\"unable to mprotect\" && 0);\n+static TracePart* TracePartAlloc(ThreadState* thr) {\n+  TracePart* part = nullptr;\n+  {\n+    Lock lock(&ctx->slot_mtx);\n+    uptr max_parts = Trace::kMinParts + flags()->history_size;\n+    Trace* trace = &thr->tctx->trace;\n+    if (trace->parts_allocated == max_parts ||\n+        ctx->trace_part_finished_excess) {\n+      part = ctx->trace_part_recycle.PopFront();\n+      DPrintf(\"#%d: TracePartAlloc: part=%p\\n\", thr->tid, part);\n+      if (part && part->trace) {\n+        Trace* trace1 = part->trace;\n+        Lock trace_lock(&trace1->mtx);\n+        part->trace = nullptr;\n+        TracePart* part1 = trace1->parts.PopFront();\n+        CHECK_EQ(part, part1);\n+        if (trace1->parts_allocated > trace1->parts.Size()) {\n+          ctx->trace_part_finished_excess +=\n+              trace1->parts_allocated - trace1->parts.Size();\n+          trace1->parts_allocated = trace1->parts.Size();\n+        }\n+      }\n+    }\n+    if (trace->parts_allocated < max_parts) {\n+      trace->parts_allocated++;\n+      if (ctx->trace_part_finished_excess)\n+        ctx->trace_part_finished_excess--;\n     }\n+    if (!part)\n+      ctx->trace_part_total_allocated++;\n+    else if (ctx->trace_part_recycle_finished)\n+      ctx->trace_part_recycle_finished--;\n   }\n-  return New<ThreadContext>(tid);\n+  if (!part)\n+    part = new (MmapOrDie(sizeof(*part), \"TracePart\")) TracePart();\n+  return part;\n }\n \n+static void TracePartFree(TracePart* part) SANITIZER_REQUIRES(ctx->slot_mtx) {\n+  DCHECK(part->trace);\n+  part->trace = nullptr;\n+  ctx->trace_part_recycle.PushFront(part);\n+}\n+\n+void TraceResetForTesting() {\n+  Lock lock(&ctx->slot_mtx);\n+  while (auto* part = ctx->trace_part_recycle.PopFront()) {\n+    if (auto trace = part->trace)\n+      CHECK_EQ(trace->parts.PopFront(), part);\n+    UnmapOrDie(part, sizeof(*part));\n+  }\n+  ctx->trace_part_total_allocated = 0;\n+  ctx->trace_part_recycle_finished = 0;\n+  ctx->trace_part_finished_excess = 0;\n+}\n+\n+static void DoResetImpl(uptr epoch) {\n+  ThreadRegistryLock lock0(&ctx->thread_registry);\n+  Lock lock1(&ctx->slot_mtx);\n+  CHECK_EQ(ctx->global_epoch, epoch);\n+  ctx->global_epoch++;\n+  CHECK(!ctx->resetting);\n+  ctx->resetting = true;\n+  for (u32 i = ctx->thread_registry.NumThreadsLocked(); i--;) {\n+    ThreadContext* tctx = (ThreadContext*)ctx->thread_registry.GetThreadLocked(\n+        static_cast<Tid>(i));\n+    // Potentially we could purge all ThreadStatusDead threads from the\n+    // registry. Since we reset all shadow, they can't race with anything\n+    // anymore. However, their tid's can still be stored in some aux places\n+    // (e.g. tid of thread that created something).\n+    auto trace = &tctx->trace;\n+    Lock lock(&trace->mtx);\n+    bool attached = tctx->thr && tctx->thr->slot;\n+    auto parts = &trace->parts;\n+    bool local = false;\n+    while (!parts->Empty()) {\n+      auto part = parts->Front();\n+      local = local || part == trace->local_head;\n+      if (local)\n+        CHECK(!ctx->trace_part_recycle.Queued(part));\n+      else\n+        ctx->trace_part_recycle.Remove(part);\n+      if (attached && parts->Size() == 1) {\n+        // The thread is running and this is the last/current part.\n+        // Set the trace position to the end of the current part\n+        // to force the thread to call SwitchTracePart and re-attach\n+        // to a new slot and allocate a new trace part.\n+        // Note: the thread is concurrently modifying the position as well,\n+        // so this is only best-effort. The thread can only modify position\n+        // within this part, because switching parts is protected by\n+        // slot/trace mutexes that we hold here.\n+        atomic_store_relaxed(\n+            &tctx->thr->trace_pos,\n+            reinterpret_cast<uptr>(&part->events[TracePart::kSize]));\n+        break;\n+      }\n+      parts->Remove(part);\n+      TracePartFree(part);\n+    }\n+    CHECK_LE(parts->Size(), 1);\n+    trace->local_head = parts->Front();\n+    if (tctx->thr && !tctx->thr->slot) {\n+      atomic_store_relaxed(&tctx->thr->trace_pos, 0);\n+      tctx->thr->trace_prev_pc = 0;\n+    }\n+    if (trace->parts_allocated > trace->parts.Size()) {\n+      ctx->trace_part_finished_excess +=\n+          trace->parts_allocated - trace->parts.Size();\n+      trace->parts_allocated = trace->parts.Size();\n+    }\n+  }\n+  while (ctx->slot_queue.PopFront()) {\n+  }\n+  for (auto& slot : ctx->slots) {\n+    slot.SetEpoch(kEpochZero);\n+    slot.journal.Reset();\n+    slot.thr = nullptr;\n+    ctx->slot_queue.PushBack(&slot);\n+  }\n+\n+  DPrintf(\"Resetting shadow...\\n\");\n+  if (!MmapFixedSuperNoReserve(ShadowBeg(), ShadowEnd() - ShadowBeg(),\n+                               \"shadow\")) {\n+    Printf(\"failed to reset shadow memory\\n\");\n+    Die();\n+  }\n+  DPrintf(\"Resetting meta shadow...\\n\");\n+  ctx->metamap.ResetClocks();\n+  ctx->resetting = false;\n+}\n+\n+// Clang does not understand locking all slots in the loop:\n+// error: expecting mutex 'slot.mtx' to be held at start of each loop\n+void DoReset(ThreadState* thr, uptr epoch) SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  {\n+    for (auto& slot : ctx->slots) {\n+      slot.mtx.Lock();\n+      if (UNLIKELY(epoch == 0))\n+        epoch = ctx->global_epoch;\n+      if (UNLIKELY(epoch != ctx->global_epoch)) {\n+        // Epoch can't change once we've locked the first slot.\n+        CHECK_EQ(slot.sid, 0);\n+        slot.mtx.Unlock();\n+        return;\n+      }\n+    }\n+  }\n+  DPrintf(\"#%d: DoReset epoch=%lu\\n\", thr ? thr->tid : -1, epoch);\n+  DoResetImpl(epoch);\n+  for (auto& slot : ctx->slots) slot.mtx.Unlock();\n+}\n+\n+void FlushShadowMemory() { DoReset(nullptr, 0); }\n+\n+static TidSlot* FindSlotAndLock(ThreadState* thr)\n+    SANITIZER_ACQUIRE(thr->slot->mtx) SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  CHECK(!thr->slot);\n+  TidSlot* slot = nullptr;\n+  for (;;) {\n+    uptr epoch;\n+    {\n+      Lock lock(&ctx->slot_mtx);\n+      epoch = ctx->global_epoch;\n+      if (slot) {\n+        // This is an exhausted slot from the previous iteration.\n+        if (ctx->slot_queue.Queued(slot))\n+          ctx->slot_queue.Remove(slot);\n+        thr->slot_locked = false;\n+        slot->mtx.Unlock();\n+      }\n+      for (;;) {\n+        slot = ctx->slot_queue.PopFront();\n+        if (!slot)\n+          break;\n+        if (slot->epoch() != kEpochLast) {\n+          ctx->slot_queue.PushBack(slot);\n+          break;\n+        }\n+      }\n+    }\n+    if (!slot) {\n+      DoReset(thr, epoch);\n+      continue;\n+    }\n+    slot->mtx.Lock();\n+    CHECK(!thr->slot_locked);\n+    thr->slot_locked = true;\n+    if (slot->thr) {\n+      DPrintf(\"#%d: preempting sid=%d tid=%d\\n\", thr->tid, (u32)slot->sid,\n+              slot->thr->tid);\n+      slot->SetEpoch(slot->thr->fast_state.epoch());\n+      slot->thr = nullptr;\n+    }\n+    if (slot->epoch() != kEpochLast)\n+      return slot;\n+  }\n+}\n+\n+void SlotAttachAndLock(ThreadState* thr) {\n+  TidSlot* slot = FindSlotAndLock(thr);\n+  DPrintf(\"#%d: SlotAttach: slot=%u\\n\", thr->tid, static_cast<int>(slot->sid));\n+  CHECK(!slot->thr);\n+  CHECK(!thr->slot);\n+  slot->thr = thr;\n+  thr->slot = slot;\n+  Epoch epoch = EpochInc(slot->epoch());\n+  CHECK(!EpochOverflow(epoch));\n+  slot->SetEpoch(epoch);\n+  thr->fast_state.SetSid(slot->sid);\n+  thr->fast_state.SetEpoch(epoch);\n+  if (thr->slot_epoch != ctx->global_epoch) {\n+    thr->slot_epoch = ctx->global_epoch;\n+    thr->clock.Reset();\n #if !SANITIZER_GO\n-static const u32 kThreadQuarantineSize = 16;\n-#else\n-static const u32 kThreadQuarantineSize = 64;\n+    thr->last_sleep_stack_id = kInvalidStackID;\n+    thr->last_sleep_clock.Reset();\n+#endif\n+  }\n+  thr->clock.Set(slot->sid, epoch);\n+  slot->journal.PushBack({thr->tid, epoch});\n+}\n+\n+static void SlotDetachImpl(ThreadState* thr, bool exiting) {\n+  TidSlot* slot = thr->slot;\n+  thr->slot = nullptr;\n+  if (thr != slot->thr) {\n+    slot = nullptr;  // we don't own the slot anymore\n+    if (thr->slot_epoch != ctx->global_epoch) {\n+      TracePart* part = nullptr;\n+      auto* trace = &thr->tctx->trace;\n+      {\n+        Lock l(&trace->mtx);\n+        auto* parts = &trace->parts;\n+        // The trace can be completely empty in an unlikely event\n+        // the thread is preempted right after it acquired the slot\n+        // in ThreadStart and did not trace any events yet.\n+        CHECK_LE(parts->Size(), 1);\n+        part = parts->PopFront();\n+        thr->tctx->trace.local_head = nullptr;\n+        atomic_store_relaxed(&thr->trace_pos, 0);\n+        thr->trace_prev_pc = 0;\n+      }\n+      if (part) {\n+        Lock l(&ctx->slot_mtx);\n+        TracePartFree(part);\n+      }\n+    }\n+    return;\n+  }\n+  CHECK(exiting || thr->fast_state.epoch() == kEpochLast);\n+  slot->SetEpoch(thr->fast_state.epoch());\n+  slot->thr = nullptr;\n+}\n+\n+void SlotDetach(ThreadState* thr) {\n+  Lock lock(&thr->slot->mtx);\n+  SlotDetachImpl(thr, true);\n+}\n+\n+void SlotLock(ThreadState* thr) SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  DCHECK(!thr->slot_locked);\n+#if SANITIZER_DEBUG\n+  // Check these mutexes are not locked.\n+  // We can call DoReset from SlotAttachAndLock, which will lock\n+  // these mutexes, but it happens only every once in a while.\n+  { ThreadRegistryLock lock(&ctx->thread_registry); }\n+  { Lock lock(&ctx->slot_mtx); }\n #endif\n+  TidSlot* slot = thr->slot;\n+  slot->mtx.Lock();\n+  thr->slot_locked = true;\n+  if (LIKELY(thr == slot->thr && thr->fast_state.epoch() != kEpochLast))\n+    return;\n+  SlotDetachImpl(thr, false);\n+  thr->slot_locked = false;\n+  slot->mtx.Unlock();\n+  SlotAttachAndLock(thr);\n+}\n+\n+void SlotUnlock(ThreadState* thr) {\n+  DCHECK(thr->slot_locked);\n+  thr->slot_locked = false;\n+  thr->slot->mtx.Unlock();\n+}\n \n Context::Context()\n     : initialized(),\n       report_mtx(MutexTypeReport),\n       nreported(),\n-      thread_registry(CreateThreadContext, kMaxTid, kThreadQuarantineSize,\n-                      kMaxTidReuse),\n+      thread_registry([](Tid tid) -> ThreadContextBase* {\n+        return new (Alloc(sizeof(ThreadContext))) ThreadContext(tid);\n+      }),\n       racy_mtx(MutexTypeRacy),\n       racy_stacks(),\n       racy_addresses(),\n       fired_suppressions_mtx(MutexTypeFired),\n-      clock_alloc(LINKER_INITIALIZED, \"clock allocator\") {\n+      slot_mtx(MutexTypeSlots),\n+      resetting() {\n   fired_suppressions.reserve(8);\n+  for (uptr i = 0; i < ARRAY_SIZE(slots); i++) {\n+    TidSlot* slot = &slots[i];\n+    slot->sid = static_cast<Sid>(i);\n+    slot_queue.PushBack(slot);\n+  }\n+  global_epoch = 1;\n }\n \n+TidSlot::TidSlot() : mtx(MutexTypeSlot) {}\n+\n // The objects are allocated in TLS, so one may rely on zero-initialization.\n-ThreadState::ThreadState(Context *ctx, Tid tid, int unique_id, u64 epoch,\n-                         unsigned reuse_count, uptr stk_addr, uptr stk_size,\n-                         uptr tls_addr, uptr tls_size)\n-    : fast_state(tid, epoch)\n-      // Do not touch these, rely on zero initialization,\n-      // they may be accessed before the ctor.\n-      // , ignore_reads_and_writes()\n-      // , ignore_interceptors()\n-      ,\n-      clock(tid, reuse_count)\n-#if !SANITIZER_GO\n-      ,\n-      jmp_bufs()\n-#endif\n-      ,\n-      tid(tid),\n-      unique_id(unique_id),\n-      stk_addr(stk_addr),\n-      stk_size(stk_size),\n-      tls_addr(tls_addr),\n-      tls_size(tls_size)\n-#if !SANITIZER_GO\n-      ,\n-      last_sleep_clock(tid)\n-#endif\n-{\n+ThreadState::ThreadState(Tid tid)\n+    // Do not touch these, rely on zero initialization,\n+    // they may be accessed before the ctor.\n+    // ignore_reads_and_writes()\n+    // ignore_interceptors()\n+    : tid(tid) {\n   CHECK_EQ(reinterpret_cast<uptr>(this) % SANITIZER_CACHE_LINE_SIZE, 0);\n #if !SANITIZER_GO\n-  shadow_stack_pos = shadow_stack;\n-  shadow_stack_end = shadow_stack + kShadowStackSize;\n+  // C/C++ uses fixed size shadow stack.\n+  const int kInitStackSize = kShadowStackSize;\n+  shadow_stack = static_cast<uptr*>(\n+      MmapNoReserveOrDie(kInitStackSize * sizeof(uptr), \"shadow stack\"));\n+  SetShadowRegionHugePageMode(reinterpret_cast<uptr>(shadow_stack),\n+                              kInitStackSize * sizeof(uptr));\n #else\n-  // Setup dynamic shadow stack.\n+  // Go uses malloc-allocated shadow stack with dynamic size.\n   const int kInitStackSize = 8;\n-  shadow_stack = (uptr *)Alloc(kInitStackSize * sizeof(uptr));\n+  shadow_stack = static_cast<uptr*>(Alloc(kInitStackSize * sizeof(uptr)));\n+#endif\n   shadow_stack_pos = shadow_stack;\n   shadow_stack_end = shadow_stack + kInitStackSize;\n-#endif\n }\n \n #if !SANITIZER_GO\n@@ -168,11 +418,11 @@ void MemoryProfiler(u64 uptime) {\n   WriteToFile(ctx->memprof_fd, buf.data(), internal_strlen(buf.data()));\n }\n \n-void InitializeMemoryProfiler() {\n+static bool InitializeMemoryProfiler() {\n   ctx->memprof_fd = kInvalidFd;\n   const char *fname = flags()->profile_memory;\n   if (!fname || !fname[0])\n-    return;\n+    return false;\n   if (internal_strcmp(fname, \"stdout\") == 0) {\n     ctx->memprof_fd = 1;\n   } else if (internal_strcmp(fname, \"stderr\") == 0) {\n@@ -184,11 +434,11 @@ void InitializeMemoryProfiler() {\n     if (ctx->memprof_fd == kInvalidFd) {\n       Printf(\"ThreadSanitizer: failed to open memory profile file '%s'\\n\",\n              filename.data());\n-      return;\n+      return false;\n     }\n   }\n   MemoryProfiler(0);\n-  MaybeSpawnBackgroundThread();\n+  return true;\n }\n \n static void *BackgroundThread(void *arg) {\n@@ -200,33 +450,34 @@ static void *BackgroundThread(void *arg) {\n   const u64 kMs2Ns = 1000 * 1000;\n   const u64 start = NanoTime();\n \n-  u64 last_flush = NanoTime();\n+  u64 last_flush = start;\n   uptr last_rss = 0;\n-  for (int i = 0;\n-      atomic_load(&ctx->stop_background_thread, memory_order_relaxed) == 0;\n-      i++) {\n+  while (!atomic_load_relaxed(&ctx->stop_background_thread)) {\n     SleepForMillis(100);\n     u64 now = NanoTime();\n \n     // Flush memory if requested.\n     if (flags()->flush_memory_ms > 0) {\n       if (last_flush + flags()->flush_memory_ms * kMs2Ns < now) {\n-        VPrintf(1, \"ThreadSanitizer: periodic memory flush\\n\");\n+        VReport(1, \"ThreadSanitizer: periodic memory flush\\n\");\n         FlushShadowMemory();\n-        last_flush = NanoTime();\n+        now = last_flush = NanoTime();\n       }\n     }\n     if (flags()->memory_limit_mb > 0) {\n       uptr rss = GetRSS();\n       uptr limit = uptr(flags()->memory_limit_mb) << 20;\n-      VPrintf(1, \"ThreadSanitizer: memory flush check\"\n-                 \" RSS=%llu LAST=%llu LIMIT=%llu\\n\",\n+      VReport(1,\n+              \"ThreadSanitizer: memory flush check\"\n+              \" RSS=%llu LAST=%llu LIMIT=%llu\\n\",\n               (u64)rss >> 20, (u64)last_rss >> 20, (u64)limit >> 20);\n       if (2 * rss > limit + last_rss) {\n-        VPrintf(1, \"ThreadSanitizer: flushing memory due to RSS\\n\");\n+        VReport(1, \"ThreadSanitizer: flushing memory due to RSS\\n\");\n         FlushShadowMemory();\n         rss = GetRSS();\n-        VPrintf(1, \"ThreadSanitizer: memory flushed RSS=%llu\\n\", (u64)rss>>20);\n+        now = NanoTime();\n+        VReport(1, \"ThreadSanitizer: memory flushed RSS=%llu\\n\",\n+                (u64)rss >> 20);\n       }\n       last_rss = rss;\n     }\n@@ -267,11 +518,43 @@ void DontNeedShadowFor(uptr addr, uptr size) {\n }\n \n #if !SANITIZER_GO\n+// We call UnmapShadow before the actual munmap, at that point we don't yet\n+// know if the provided address/size are sane. We can't call UnmapShadow\n+// after the actual munmap becuase at that point the memory range can\n+// already be reused for something else, so we can't rely on the munmap\n+// return value to understand is the values are sane.\n+// While calling munmap with insane values (non-canonical address, negative\n+// size, etc) is an error, the kernel won't crash. We must also try to not\n+// crash as the failure mode is very confusing (paging fault inside of the\n+// runtime on some derived shadow address).\n+static bool IsValidMmapRange(uptr addr, uptr size) {\n+  if (size == 0)\n+    return true;\n+  if (static_cast<sptr>(size) < 0)\n+    return false;\n+  if (!IsAppMem(addr) || !IsAppMem(addr + size - 1))\n+    return false;\n+  // Check that if the start of the region belongs to one of app ranges,\n+  // end of the region belongs to the same region.\n+  const uptr ranges[][2] = {\n+      {LoAppMemBeg(), LoAppMemEnd()},\n+      {MidAppMemBeg(), MidAppMemEnd()},\n+      {HiAppMemBeg(), HiAppMemEnd()},\n+  };\n+  for (auto range : ranges) {\n+    if (addr >= range[0] && addr < range[1])\n+      return addr + size <= range[1];\n+  }\n+  return false;\n+}\n+\n void UnmapShadow(ThreadState *thr, uptr addr, uptr size) {\n-  if (size == 0) return;\n+  if (size == 0 || !IsValidMmapRange(addr, size))\n+    return;\n   DontNeedShadowFor(addr, size);\n   ScopedGlobalProcessor sgp;\n-  ctx->metamap.ResetRange(thr->proc(), addr, size);\n+  SlotLocker locker(thr, true);\n+  ctx->metamap.ResetRange(thr->proc(), addr, size, true);\n }\n #endif\n \n@@ -317,18 +600,6 @@ void MapShadow(uptr addr, uptr size) {\n           addr + size, meta_begin, meta_end);\n }\n \n-void MapThreadTrace(uptr addr, uptr size, const char *name) {\n-  DPrintf(\"#0: Mapping trace at 0x%zx-0x%zx(0x%zx)\\n\", addr, addr + size, size);\n-  CHECK_GE(addr, TraceMemBeg());\n-  CHECK_LE(addr + size, TraceMemEnd());\n-  CHECK_EQ(addr, addr & ~((64 << 10) - 1));  // windows wants 64K alignment\n-  if (!MmapFixedSuperNoReserve(addr, size, name)) {\n-    Printf(\"FATAL: ThreadSanitizer can not mmap thread trace (0x%zx/0x%zx)\\n\",\n-           addr, size);\n-    Die();\n-  }\n-}\n-\n #if !SANITIZER_GO\n static void OnStackUnwind(const SignalContext &sig, const void *,\n                           BufferedStackTrace *stack) {\n@@ -347,8 +618,11 @@ void CheckUnwind() {\n   // since we are going to die soon.\n   ScopedIgnoreInterceptors ignore;\n #if !SANITIZER_GO\n-  cur_thread()->ignore_sync++;\n-  cur_thread()->ignore_reads_and_writes++;\n+  ThreadState* thr = cur_thread();\n+  thr->nomalloc = false;\n+  thr->ignore_sync++;\n+  thr->ignore_reads_and_writes++;\n+  atomic_store_relaxed(&thr->in_signal_handler, 0);\n #endif\n   PrintCurrentStackSlow(StackTrace::GetCurrentPc());\n }\n@@ -403,22 +677,23 @@ void Initialize(ThreadState *thr) {\n   Symbolizer::GetOrInit()->AddHooks(EnterSymbolizer, ExitSymbolizer);\n #endif\n \n-  VPrintf(1, \"***** Running under ThreadSanitizer v2 (pid %d) *****\\n\",\n+  VPrintf(1, \"***** Running under ThreadSanitizer v3 (pid %d) *****\\n\",\n           (int)internal_getpid());\n \n   // Initialize thread 0.\n-  Tid tid = ThreadCreate(thr, 0, 0, true);\n+  Tid tid = ThreadCreate(nullptr, 0, 0, true);\n   CHECK_EQ(tid, kMainTid);\n   ThreadStart(thr, tid, GetTid(), ThreadType::Regular);\n #if TSAN_CONTAINS_UBSAN\n   __ubsan::InitAsPlugin();\n #endif\n-  ctx->initialized = true;\n \n #if !SANITIZER_GO\n   Symbolizer::LateInitialize();\n-  InitializeMemoryProfiler();\n+  if (InitializeMemoryProfiler() || flags()->force_background_thread)\n+    MaybeSpawnBackgroundThread();\n #endif\n+  ctx->initialized = true;\n \n   if (flags()->stop_on_start) {\n     Printf(\"ThreadSanitizer is suspended at startup (pid %d).\"\n@@ -444,20 +719,19 @@ void MaybeSpawnBackgroundThread() {\n #endif\n }\n \n-\n int Finalize(ThreadState *thr) {\n   bool failed = false;\n \n   if (common_flags()->print_module_map == 1)\n     DumpProcessMap();\n \n   if (flags()->atexit_sleep_ms > 0 && ThreadCount(thr) > 1)\n-    SleepForMillis(flags()->atexit_sleep_ms);\n+    internal_usleep(u64(flags()->atexit_sleep_ms) * 1000);\n \n-  // Wait for pending reports.\n-  ctx->report_mtx.Lock();\n-  { ScopedErrorReportLock l; }\n-  ctx->report_mtx.Unlock();\n+  {\n+    // Wait for pending reports.\n+    ScopedErrorReportLock lock;\n+  }\n \n #if !SANITIZER_GO\n   if (Verbosity()) AllocatorPrintStats();\n@@ -483,10 +757,16 @@ int Finalize(ThreadState *thr) {\n }\n \n #if !SANITIZER_GO\n-void ForkBefore(ThreadState *thr, uptr pc) NO_THREAD_SAFETY_ANALYSIS {\n+void ForkBefore(ThreadState* thr, uptr pc) SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  GlobalProcessorLock();\n+  // Detaching from the slot makes OnUserFree skip writing to the shadow.\n+  // The slot will be locked so any attempts to use it will deadlock anyway.\n+  SlotDetach(thr);\n+  for (auto& slot : ctx->slots) slot.mtx.Lock();\n   ctx->thread_registry.Lock();\n-  ctx->report_mtx.Lock();\n+  ctx->slot_mtx.Lock();\n   ScopedErrorReportLock::Lock();\n+  AllocatorLock();\n   // Suppress all reports in the pthread_atfork callbacks.\n   // Reports will deadlock on the report_mtx.\n   // We could ignore sync operations as well,\n@@ -495,29 +775,38 @@ void ForkBefore(ThreadState *thr, uptr pc) NO_THREAD_SAFETY_ANALYSIS {\n   thr->suppress_reports++;\n   // On OS X, REAL(fork) can call intercepted functions (OSSpinLockLock), and\n   // we'll assert in CheckNoLocks() unless we ignore interceptors.\n+  // On OS X libSystem_atfork_prepare/parent/child callbacks are called\n+  // after/before our callbacks and they call free.\n   thr->ignore_interceptors++;\n+  // Disables memory write in OnUserAlloc/Free.\n+  thr->ignore_reads_and_writes++;\n+\n+  __tsan_test_only_on_fork();\n }\n \n-void ForkParentAfter(ThreadState *thr, uptr pc) NO_THREAD_SAFETY_ANALYSIS {\n+static void ForkAfter(ThreadState* thr) SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n   thr->suppress_reports--;  // Enabled in ForkBefore.\n   thr->ignore_interceptors--;\n+  thr->ignore_reads_and_writes--;\n+  AllocatorUnlock();\n   ScopedErrorReportLock::Unlock();\n-  ctx->report_mtx.Unlock();\n+  ctx->slot_mtx.Unlock();\n   ctx->thread_registry.Unlock();\n+  for (auto& slot : ctx->slots) slot.mtx.Unlock();\n+  SlotAttachAndLock(thr);\n+  SlotUnlock(thr);\n+  GlobalProcessorUnlock();\n }\n \n-void ForkChildAfter(ThreadState *thr, uptr pc,\n-                    bool start_thread) NO_THREAD_SAFETY_ANALYSIS {\n-  thr->suppress_reports--;  // Enabled in ForkBefore.\n-  thr->ignore_interceptors--;\n-  ScopedErrorReportLock::Unlock();\n-  ctx->report_mtx.Unlock();\n-  ctx->thread_registry.Unlock();\n+void ForkParentAfter(ThreadState* thr, uptr pc) { ForkAfter(thr); }\n \n-  uptr nthread = 0;\n-  ctx->thread_registry.GetNumberOfThreads(0, 0, &nthread /* alive threads */);\n-  VPrintf(1, \"ThreadSanitizer: forked new process with pid %d,\"\n-      \" parent had %d threads\\n\", (int)internal_getpid(), (int)nthread);\n+void ForkChildAfter(ThreadState* thr, uptr pc, bool start_thread) {\n+  ForkAfter(thr);\n+  u32 nthread = ctx->thread_registry.OnFork(thr->tid);\n+  VPrintf(1,\n+          \"ThreadSanitizer: forked new process with pid %d,\"\n+          \" parent had %d threads\\n\",\n+          (int)internal_getpid(), (int)nthread);\n   if (nthread == 1) {\n     if (start_thread)\n       StartBackgroundThread();\n@@ -527,6 +816,7 @@ void ForkChildAfter(ThreadState *thr, uptr pc,\n     // ignores for everything in the hope that we will exec soon.\n     ctx->after_multithreaded_fork = true;\n     thr->ignore_interceptors++;\n+    thr->suppress_reports++;\n     ThreadIgnoreBegin(thr, pc);\n     ThreadIgnoreSyncBegin(thr, pc);\n   }\n@@ -548,8 +838,10 @@ void GrowShadowStack(ThreadState *thr) {\n #endif\n \n StackID CurrentStackId(ThreadState *thr, uptr pc) {\n+#if !SANITIZER_GO\n   if (!thr->is_inited)  // May happen during bootstrap.\n     return kInvalidStackID;\n+#endif\n   if (pc != 0) {\n #if !SANITIZER_GO\n     DCHECK_LT(thr->shadow_stack_pos, thr->shadow_stack_end);\n@@ -567,114 +859,122 @@ StackID CurrentStackId(ThreadState *thr, uptr pc) {\n   return id;\n }\n \n-namespace v3 {\n-\n-NOINLINE\n-void TraceSwitchPart(ThreadState *thr) {\n+static bool TraceSkipGap(ThreadState* thr) {\n   Trace *trace = &thr->tctx->trace;\n   Event *pos = reinterpret_cast<Event *>(atomic_load_relaxed(&thr->trace_pos));\n   DCHECK_EQ(reinterpret_cast<uptr>(pos + 1) & TracePart::kAlignment, 0);\n   auto *part = trace->parts.Back();\n-  DPrintf(\"TraceSwitchPart part=%p pos=%p\\n\", part, pos);\n-  if (part) {\n-    // We can get here when we still have space in the current trace part.\n-    // The fast-path check in TraceAcquire has false positives in the middle of\n-    // the part. Check if we are indeed at the end of the current part or not,\n-    // and fill any gaps with NopEvent's.\n-    Event *end = &part->events[TracePart::kSize];\n-    DCHECK_GE(pos, &part->events[0]);\n-    DCHECK_LE(pos, end);\n-    if (pos + 1 < end) {\n-      if ((reinterpret_cast<uptr>(pos) & TracePart::kAlignment) ==\n-          TracePart::kAlignment)\n-        *pos++ = NopEvent;\n+  DPrintf(\"#%d: TraceSwitchPart enter trace=%p parts=%p-%p pos=%p\\n\", thr->tid,\n+          trace, trace->parts.Front(), part, pos);\n+  if (!part)\n+    return false;\n+  // We can get here when we still have space in the current trace part.\n+  // The fast-path check in TraceAcquire has false positives in the middle of\n+  // the part. Check if we are indeed at the end of the current part or not,\n+  // and fill any gaps with NopEvent's.\n+  Event* end = &part->events[TracePart::kSize];\n+  DCHECK_GE(pos, &part->events[0]);\n+  DCHECK_LE(pos, end);\n+  if (pos + 1 < end) {\n+    if ((reinterpret_cast<uptr>(pos) & TracePart::kAlignment) ==\n+        TracePart::kAlignment)\n       *pos++ = NopEvent;\n-      DCHECK_LE(pos + 2, end);\n-      atomic_store_relaxed(&thr->trace_pos, reinterpret_cast<uptr>(pos));\n-      // Ensure we setup trace so that the next TraceAcquire\n-      // won't detect trace part end.\n-      Event *ev;\n-      CHECK(TraceAcquire(thr, &ev));\n-      return;\n-    }\n-    // We are indeed at the end.\n-    for (; pos < end; pos++) *pos = NopEvent;\n+    *pos++ = NopEvent;\n+    DCHECK_LE(pos + 2, end);\n+    atomic_store_relaxed(&thr->trace_pos, reinterpret_cast<uptr>(pos));\n+    return true;\n   }\n+  // We are indeed at the end.\n+  for (; pos < end; pos++) *pos = NopEvent;\n+  return false;\n+}\n+\n+NOINLINE\n+void TraceSwitchPart(ThreadState* thr) {\n+  if (TraceSkipGap(thr))\n+    return;\n #if !SANITIZER_GO\n   if (ctx->after_multithreaded_fork) {\n     // We just need to survive till exec.\n-    CHECK(part);\n-    atomic_store_relaxed(&thr->trace_pos,\n-                         reinterpret_cast<uptr>(&part->events[0]));\n-    return;\n+    TracePart* part = thr->tctx->trace.parts.Back();\n+    if (part) {\n+      atomic_store_relaxed(&thr->trace_pos,\n+                           reinterpret_cast<uptr>(&part->events[0]));\n+      return;\n+    }\n   }\n #endif\n-  part = new (MmapOrDie(sizeof(TracePart), \"TracePart\")) TracePart();\n+  TraceSwitchPartImpl(thr);\n+}\n+\n+void TraceSwitchPartImpl(ThreadState* thr) {\n+  SlotLocker locker(thr, true);\n+  Trace* trace = &thr->tctx->trace;\n+  TracePart* part = TracePartAlloc(thr);\n   part->trace = trace;\n   thr->trace_prev_pc = 0;\n+  TracePart* recycle = nullptr;\n+  // Keep roughly half of parts local to the thread\n+  // (not queued into the recycle queue).\n+  uptr local_parts = (Trace::kMinParts + flags()->history_size + 1) / 2;\n   {\n     Lock lock(&trace->mtx);\n+    if (trace->parts.Empty())\n+      trace->local_head = part;\n+    if (trace->parts.Size() >= local_parts) {\n+      recycle = trace->local_head;\n+      trace->local_head = trace->parts.Next(recycle);\n+    }\n     trace->parts.PushBack(part);\n     atomic_store_relaxed(&thr->trace_pos,\n                          reinterpret_cast<uptr>(&part->events[0]));\n   }\n   // Make this part self-sufficient by restoring the current stack\n   // and mutex set in the beginning of the trace.\n   TraceTime(thr);\n-  for (uptr *pos = &thr->shadow_stack[0]; pos < thr->shadow_stack_pos; pos++)\n-    CHECK(TryTraceFunc(thr, *pos));\n+  {\n+    // Pathologically large stacks may not fit into the part.\n+    // In these cases we log only fixed number of top frames.\n+    const uptr kMaxFrames = 1000;\n+    // Check that kMaxFrames won't consume the whole part.\n+    static_assert(kMaxFrames < TracePart::kSize / 2, \"kMaxFrames is too big\");\n+    uptr* pos = Max(&thr->shadow_stack[0], thr->shadow_stack_pos - kMaxFrames);\n+    for (; pos < thr->shadow_stack_pos; pos++) {\n+      if (TryTraceFunc(thr, *pos))\n+        continue;\n+      CHECK(TraceSkipGap(thr));\n+      CHECK(TryTraceFunc(thr, *pos));\n+    }\n+  }\n   for (uptr i = 0; i < thr->mset.Size(); i++) {\n     MutexSet::Desc d = thr->mset.Get(i);\n-    TraceMutexLock(thr, d.write ? EventType::kLock : EventType::kRLock, 0,\n-                   d.addr, d.stack_id);\n+    for (uptr i = 0; i < d.count; i++)\n+      TraceMutexLock(thr, d.write ? EventType::kLock : EventType::kRLock, 0,\n+                     d.addr, d.stack_id);\n   }\n+  {\n+    Lock lock(&ctx->slot_mtx);\n+    // There is a small chance that the slot may be not queued at this point.\n+    // This can happen if the slot has kEpochLast epoch and another thread\n+    // in FindSlotAndLock discovered that it's exhausted and removed it from\n+    // the slot queue. kEpochLast can happen in 2 cases: (1) if TraceSwitchPart\n+    // was called with the slot locked and epoch already at kEpochLast,\n+    // or (2) if we've acquired a new slot in SlotLock in the beginning\n+    // of the function and the slot was at kEpochLast - 1, so after increment\n+    // in SlotAttachAndLock it become kEpochLast.\n+    if (ctx->slot_queue.Queued(thr->slot)) {\n+      ctx->slot_queue.Remove(thr->slot);\n+      ctx->slot_queue.PushBack(thr->slot);\n+    }\n+    if (recycle)\n+      ctx->trace_part_recycle.PushBack(recycle);\n+  }\n+  DPrintf(\"#%d: TraceSwitchPart exit parts=%p-%p pos=0x%zx\\n\", thr->tid,\n+          trace->parts.Front(), trace->parts.Back(),\n+          atomic_load_relaxed(&thr->trace_pos));\n }\n \n-}  // namespace v3\n-\n-void TraceSwitch(ThreadState *thr) {\n-#if !SANITIZER_GO\n-  if (ctx->after_multithreaded_fork)\n-    return;\n-#endif\n-  thr->nomalloc++;\n-  Trace *thr_trace = ThreadTrace(thr->tid);\n-  Lock l(&thr_trace->mtx);\n-  unsigned trace = (thr->fast_state.epoch() / kTracePartSize) % TraceParts();\n-  TraceHeader *hdr = &thr_trace->headers[trace];\n-  hdr->epoch0 = thr->fast_state.epoch();\n-  ObtainCurrentStack(thr, 0, &hdr->stack0);\n-  hdr->mset0 = thr->mset;\n-  thr->nomalloc--;\n-}\n-\n-Trace *ThreadTrace(Tid tid) { return (Trace *)GetThreadTraceHeader(tid); }\n-\n-uptr TraceTopPC(ThreadState *thr) {\n-  Event *events = (Event*)GetThreadTrace(thr->tid);\n-  uptr pc = events[thr->fast_state.GetTracePos()];\n-  return pc;\n-}\n-\n-uptr TraceSize() {\n-  return (uptr)(1ull << (kTracePartSizeBits + flags()->history_size + 1));\n-}\n-\n-uptr TraceParts() {\n-  return TraceSize() / kTracePartSize;\n-}\n-\n-#if !SANITIZER_GO\n-extern \"C\" void __tsan_trace_switch() {\n-  TraceSwitch(cur_thread());\n-}\n-\n-extern \"C\" void __tsan_report_race() {\n-  ReportRace(cur_thread());\n-}\n-#endif\n-\n-void ThreadIgnoreBegin(ThreadState *thr, uptr pc) {\n+void ThreadIgnoreBegin(ThreadState* thr, uptr pc) {\n   DPrintf(\"#%d: ThreadIgnoreBegin\\n\", thr->tid);\n   thr->ignore_reads_and_writes++;\n   CHECK_GT(thr->ignore_reads_and_writes, 0);\n@@ -734,26 +1034,34 @@ void build_consistency_debug() {}\n #else\n void build_consistency_release() {}\n #endif\n-\n }  // namespace __tsan\n \n #if SANITIZER_CHECK_DEADLOCKS\n namespace __sanitizer {\n using namespace __tsan;\n MutexMeta mutex_meta[] = {\n     {MutexInvalid, \"Invalid\", {}},\n-    {MutexThreadRegistry, \"ThreadRegistry\", {}},\n-    {MutexTypeTrace, \"Trace\", {MutexLeaf}},\n-    {MutexTypeReport, \"Report\", {MutexTypeSyncVar}},\n-    {MutexTypeSyncVar, \"SyncVar\", {}},\n+    {MutexThreadRegistry,\n+     \"ThreadRegistry\",\n+     {MutexTypeSlots, MutexTypeTrace, MutexTypeReport}},\n+    {MutexTypeReport, \"Report\", {MutexTypeTrace}},\n+    {MutexTypeSyncVar, \"SyncVar\", {MutexTypeReport, MutexTypeTrace}},\n     {MutexTypeAnnotations, \"Annotations\", {}},\n-    {MutexTypeAtExit, \"AtExit\", {MutexTypeSyncVar}},\n+    {MutexTypeAtExit, \"AtExit\", {}},\n     {MutexTypeFired, \"Fired\", {MutexLeaf}},\n     {MutexTypeRacy, \"Racy\", {MutexLeaf}},\n-    {MutexTypeGlobalProc, \"GlobalProc\", {}},\n+    {MutexTypeGlobalProc, \"GlobalProc\", {MutexTypeSlot, MutexTypeSlots}},\n+    {MutexTypeInternalAlloc, \"InternalAlloc\", {MutexLeaf}},\n+    {MutexTypeTrace, \"Trace\", {}},\n+    {MutexTypeSlot,\n+     \"Slot\",\n+     {MutexMulti, MutexTypeTrace, MutexTypeSyncVar, MutexThreadRegistry,\n+      MutexTypeSlots}},\n+    {MutexTypeSlots, \"Slots\", {MutexTypeTrace, MutexTypeReport}},\n     {},\n };\n \n void PrintMutexPC(uptr pc) { StackTrace(&pc, 1).Print(); }\n+\n }  // namespace __sanitizer\n #endif"}, {"sha": "b472c0f77df18f7fb63f72c79af7b8627763ca2e", "filename": "libsanitizer/tsan/tsan_rtl.h", "status": "modified", "additions": 154, "deletions": 187, "changes": 341, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -34,10 +34,10 @@\n #include \"sanitizer_common/sanitizer_suppressions.h\"\n #include \"sanitizer_common/sanitizer_thread_registry.h\"\n #include \"sanitizer_common/sanitizer_vector.h\"\n-#include \"tsan_clock.h\"\n #include \"tsan_defs.h\"\n #include \"tsan_flags.h\"\n #include \"tsan_ignoreset.h\"\n+#include \"tsan_ilist.h\"\n #include \"tsan_mman.h\"\n #include \"tsan_mutexset.h\"\n #include \"tsan_platform.h\"\n@@ -46,6 +46,7 @@\n #include \"tsan_stack_trace.h\"\n #include \"tsan_sync.h\"\n #include \"tsan_trace.h\"\n+#include \"tsan_vector_clock.h\"\n \n #if SANITIZER_WORDSIZE != 64\n # error \"ThreadSanitizer is supported only on 64-bit platforms\"\n@@ -116,7 +117,6 @@ struct Processor {\n #endif\n   DenseSlabAllocCache block_cache;\n   DenseSlabAllocCache sync_cache;\n-  DenseSlabAllocCache clock_cache;\n   DDPhysicalThread *dd_pt;\n };\n \n@@ -130,67 +130,85 @@ struct ScopedGlobalProcessor {\n };\n #endif\n \n+struct TidEpoch {\n+  Tid tid;\n+  Epoch epoch;\n+};\n+\n+struct TidSlot {\n+  Mutex mtx;\n+  Sid sid;\n+  atomic_uint32_t raw_epoch;\n+  ThreadState *thr;\n+  Vector<TidEpoch> journal;\n+  INode node;\n+\n+  Epoch epoch() const {\n+    return static_cast<Epoch>(atomic_load(&raw_epoch, memory_order_relaxed));\n+  }\n+\n+  void SetEpoch(Epoch v) {\n+    atomic_store(&raw_epoch, static_cast<u32>(v), memory_order_relaxed);\n+  }\n+\n+  TidSlot();\n+} ALIGNED(SANITIZER_CACHE_LINE_SIZE);\n+\n // This struct is stored in TLS.\n struct ThreadState {\n   FastState fast_state;\n-  // Synch epoch represents the threads's epoch before the last synchronization\n-  // action. It allows to reduce number of shadow state updates.\n-  // For example, fast_synch_epoch=100, last write to addr X was at epoch=150,\n-  // if we are processing write to X from the same thread at epoch=200,\n-  // we do nothing, because both writes happen in the same 'synch epoch'.\n-  // That is, if another memory access does not race with the former write,\n-  // it does not race with the latter as well.\n-  // QUESTION: can we can squeeze this into ThreadState::Fast?\n-  // E.g. ThreadState::Fast is a 44-bit, 32 are taken by synch_epoch and 12 are\n-  // taken by epoch between synchs.\n-  // This way we can save one load from tls.\n-  u64 fast_synch_epoch;\n+  int ignore_sync;\n+#if !SANITIZER_GO\n+  int ignore_interceptors;\n+#endif\n+  uptr *shadow_stack_pos;\n+\n+  // Current position in tctx->trace.Back()->events (Event*).\n+  atomic_uintptr_t trace_pos;\n+  // PC of the last memory access, used to compute PC deltas in the trace.\n+  uptr trace_prev_pc;\n+\n   // Technically `current` should be a separate THREADLOCAL variable;\n   // but it is placed here in order to share cache line with previous fields.\n   ThreadState* current;\n+\n+  atomic_sint32_t pending_signals;\n+\n+  VectorClock clock;\n+\n   // This is a slow path flag. On fast path, fast_state.GetIgnoreBit() is read.\n   // We do not distinguish beteween ignoring reads and writes\n   // for better performance.\n   int ignore_reads_and_writes;\n-  atomic_sint32_t pending_signals;\n-  int ignore_sync;\n   int suppress_reports;\n   // Go does not support ignores.\n #if !SANITIZER_GO\n   IgnoreSet mop_ignore_set;\n   IgnoreSet sync_ignore_set;\n-  // C/C++ uses fixed size shadow stack.\n-  uptr shadow_stack[kShadowStackSize];\n-#else\n-  // Go uses malloc-allocated shadow stack with dynamic size.\n-  uptr *shadow_stack;\n #endif\n+  uptr *shadow_stack;\n   uptr *shadow_stack_end;\n-  uptr *shadow_stack_pos;\n-  RawShadow *racy_shadow_addr;\n-  RawShadow racy_state[2];\n-  MutexSet mset;\n-  ThreadClock clock;\n #if !SANITIZER_GO\n   Vector<JmpBuf> jmp_bufs;\n-  int ignore_interceptors;\n-#endif\n-  const Tid tid;\n-  const int unique_id;\n-  bool in_symbolizer;\n+  int in_symbolizer;\n   bool in_ignored_lib;\n   bool is_inited;\n+#endif\n+  MutexSet mset;\n   bool is_dead;\n-  bool is_freeing;\n-  bool is_vptr_access;\n-  const uptr stk_addr;\n-  const uptr stk_size;\n-  const uptr tls_addr;\n-  const uptr tls_size;\n+  const Tid tid;\n+  uptr stk_addr;\n+  uptr stk_size;\n+  uptr tls_addr;\n+  uptr tls_size;\n   ThreadContext *tctx;\n \n   DDLogicalThread *dd_lt;\n \n+  TidSlot *slot;\n+  uptr slot_epoch;\n+  bool slot_locked;\n+\n   // Current wired Processor, or nullptr. Required to handle any events.\n   Processor *proc1;\n #if !SANITIZER_GO\n@@ -204,7 +222,7 @@ struct ThreadState {\n \n #if !SANITIZER_GO\n   StackID last_sleep_stack_id;\n-  ThreadClock last_sleep_clock;\n+  VectorClock last_sleep_clock;\n #endif\n \n   // Set in regions of runtime that must be signal-safe and fork-safe.\n@@ -213,16 +231,7 @@ struct ThreadState {\n \n   const ReportDesc *current_report;\n \n-  // Current position in tctx->trace.Back()->events (Event*).\n-  atomic_uintptr_t trace_pos;\n-  // PC of the last memory access, used to compute PC deltas in the trace.\n-  uptr trace_prev_pc;\n-  Sid sid;\n-  Epoch epoch;\n-\n-  explicit ThreadState(Context *ctx, Tid tid, int unique_id, u64 epoch,\n-                       unsigned reuse_count, uptr stk_addr, uptr stk_size,\n-                       uptr tls_addr, uptr tls_size);\n+  explicit ThreadState(Tid tid);\n } ALIGNED(SANITIZER_CACHE_LINE_SIZE);\n \n #if !SANITIZER_GO\n@@ -256,14 +265,9 @@ class ThreadContext final : public ThreadContextBase {\n   ~ThreadContext();\n   ThreadState *thr;\n   StackID creation_stack_id;\n-  SyncClock sync;\n-  // Epoch at which the thread had started.\n-  // If we see an event from the thread stamped by an older epoch,\n-  // the event is from a dead thread that shared tid with this thread.\n-  u64 epoch0;\n-  u64 epoch1;\n-\n-  v3::Trace trace;\n+  VectorClock *sync;\n+  uptr sync_epoch;\n+  Trace trace;\n \n   // Override superclass callbacks.\n   void OnDead() override;\n@@ -318,12 +322,22 @@ struct Context {\n   InternalMmapVector<FiredSuppression> fired_suppressions;\n   DDetector *dd;\n \n-  ClockAlloc clock_alloc;\n-\n   Flags flags;\n   fd_t memprof_fd;\n \n+  // The last slot index (kFreeSid) is used to denote freed memory.\n+  TidSlot slots[kThreadSlotCount - 1];\n+\n+  // Protects global_epoch, slot_queue, trace_part_recycle.\n   Mutex slot_mtx;\n+  uptr global_epoch;  // guarded by slot_mtx and by all slot mutexes\n+  bool resetting;     // global reset is in progress\n+  IList<TidSlot, &TidSlot::node> slot_queue SANITIZER_GUARDED_BY(slot_mtx);\n+  IList<TraceHeader, &TraceHeader::global, TracePart> trace_part_recycle\n+      SANITIZER_GUARDED_BY(slot_mtx);\n+  uptr trace_part_total_allocated SANITIZER_GUARDED_BY(slot_mtx);\n+  uptr trace_part_recycle_finished SANITIZER_GUARDED_BY(slot_mtx);\n+  uptr trace_part_finished_excess SANITIZER_GUARDED_BY(slot_mtx);\n };\n \n extern Context *ctx;  // The one and the only global runtime context.\n@@ -352,17 +366,17 @@ uptr TagFromShadowStackFrame(uptr pc);\n \n class ScopedReportBase {\n  public:\n-  void AddMemoryAccess(uptr addr, uptr external_tag, Shadow s, StackTrace stack,\n-                       const MutexSet *mset);\n+  void AddMemoryAccess(uptr addr, uptr external_tag, Shadow s, Tid tid,\n+                       StackTrace stack, const MutexSet *mset);\n   void AddStack(StackTrace stack, bool suppressable = false);\n   void AddThread(const ThreadContext *tctx, bool suppressable = false);\n-  void AddThread(Tid unique_tid, bool suppressable = false);\n+  void AddThread(Tid tid, bool suppressable = false);\n   void AddUniqueTid(Tid unique_tid);\n-  void AddMutex(const SyncVar *s);\n-  u64 AddMutex(u64 id);\n+  int AddMutex(uptr addr, StackID creation_stack_id);\n   void AddLocation(uptr addr, uptr size);\n   void AddSleep(StackID stack_id);\n   void SetCount(int count);\n+  void SetSigNum(int sig);\n \n   const ReportDesc *GetReport() const;\n \n@@ -376,8 +390,6 @@ class ScopedReportBase {\n   // at best it will cause deadlocks on internal mutexes.\n   ScopedIgnoreInterceptors ignore_interceptors_;\n \n-  void AddDeadMutex(u64 id);\n-\n   ScopedReportBase(const ScopedReportBase &) = delete;\n   void operator=(const ScopedReportBase &) = delete;\n };\n@@ -393,8 +405,6 @@ class ScopedReport : public ScopedReportBase {\n \n bool ShouldReport(ThreadState *thr, ReportType typ);\n ThreadContext *IsThreadStackOrTls(uptr addr, bool *is_stack);\n-void RestoreStack(Tid tid, const u64 epoch, VarSizeStackTrace *stk,\n-                  MutexSet *mset, uptr *tag = nullptr);\n \n // The stack could look like:\n //   <start> | <main> | <foo> | tag | <bar>\n@@ -442,7 +452,8 @@ void ForkBefore(ThreadState *thr, uptr pc);\n void ForkParentAfter(ThreadState *thr, uptr pc);\n void ForkChildAfter(ThreadState *thr, uptr pc, bool start_thread);\n \n-void ReportRace(ThreadState *thr);\n+void ReportRace(ThreadState *thr, RawShadow *shadow_mem, Shadow cur, Shadow old,\n+                AccessType typ);\n bool OutputReport(ThreadState *thr, const ScopedReport &srep);\n bool IsFiredSuppression(Context *ctx, ReportType type, StackTrace trace);\n bool IsExpectedReport(uptr addr, uptr size);\n@@ -472,55 +483,28 @@ int Finalize(ThreadState *thr);\n void OnUserAlloc(ThreadState *thr, uptr pc, uptr p, uptr sz, bool write);\n void OnUserFree(ThreadState *thr, uptr pc, uptr p, bool write);\n \n-void MemoryAccess(ThreadState *thr, uptr pc, uptr addr,\n-    int kAccessSizeLog, bool kAccessIsWrite, bool kIsAtomic);\n-void MemoryAccessImpl(ThreadState *thr, uptr addr,\n-    int kAccessSizeLog, bool kAccessIsWrite, bool kIsAtomic,\n-    u64 *shadow_mem, Shadow cur);\n-void MemoryAccessRange(ThreadState *thr, uptr pc, uptr addr,\n-    uptr size, bool is_write);\n+void MemoryAccess(ThreadState *thr, uptr pc, uptr addr, uptr size,\n+                  AccessType typ);\n void UnalignedMemoryAccess(ThreadState *thr, uptr pc, uptr addr, uptr size,\n                            AccessType typ);\n-\n-const int kSizeLog1 = 0;\n-const int kSizeLog2 = 1;\n-const int kSizeLog4 = 2;\n-const int kSizeLog8 = 3;\n+// This creates 2 non-inlined specialized versions of MemoryAccessRange.\n+template <bool is_read>\n+void MemoryAccessRangeT(ThreadState *thr, uptr pc, uptr addr, uptr size);\n \n ALWAYS_INLINE\n-void MemoryAccess(ThreadState *thr, uptr pc, uptr addr, uptr size,\n-                  AccessType typ) {\n-  int size_log;\n-  switch (size) {\n-    case 1:\n-      size_log = kSizeLog1;\n-      break;\n-    case 2:\n-      size_log = kSizeLog2;\n-      break;\n-    case 4:\n-      size_log = kSizeLog4;\n-      break;\n-    default:\n-      DCHECK_EQ(size, 8);\n-      size_log = kSizeLog8;\n-      break;\n-  }\n-  bool is_write = !(typ & kAccessRead);\n-  bool is_atomic = typ & kAccessAtomic;\n-  if (typ & kAccessVptr)\n-    thr->is_vptr_access = true;\n-  if (typ & kAccessFree)\n-    thr->is_freeing = true;\n-  MemoryAccess(thr, pc, addr, size_log, is_write, is_atomic);\n-  if (typ & kAccessVptr)\n-    thr->is_vptr_access = false;\n-  if (typ & kAccessFree)\n-    thr->is_freeing = false;\n+void MemoryAccessRange(ThreadState *thr, uptr pc, uptr addr, uptr size,\n+                       bool is_write) {\n+  if (size == 0)\n+    return;\n+  if (is_write)\n+    MemoryAccessRangeT<false>(thr, pc, addr, size);\n+  else\n+    MemoryAccessRangeT<true>(thr, pc, addr, size);\n }\n \n-void MemoryResetRange(ThreadState *thr, uptr pc, uptr addr, uptr size);\n+void ShadowSet(RawShadow *p, RawShadow *end, RawShadow v);\n void MemoryRangeFreed(ThreadState *thr, uptr pc, uptr addr, uptr size);\n+void MemoryResetRange(ThreadState *thr, uptr pc, uptr addr, uptr size);\n void MemoryRangeImitateWrite(ThreadState *thr, uptr pc, uptr addr, uptr size);\n void MemoryRangeImitateWriteOrResetRange(ThreadState *thr, uptr pc, uptr addr,\n                                          uptr size);\n@@ -530,9 +514,6 @@ void ThreadIgnoreEnd(ThreadState *thr);\n void ThreadIgnoreSyncBegin(ThreadState *thr, uptr pc);\n void ThreadIgnoreSyncEnd(ThreadState *thr);\n \n-void FuncEntry(ThreadState *thr, uptr pc);\n-void FuncExit(ThreadState *thr);\n-\n Tid ThreadCreate(ThreadState *thr, uptr pc, uptr uid, bool detached);\n void ThreadStart(ThreadState *thr, Tid tid, tid_t os_id,\n                  ThreadType thread_type);\n@@ -578,67 +559,21 @@ void Release(ThreadState *thr, uptr pc, uptr addr);\n void ReleaseStoreAcquire(ThreadState *thr, uptr pc, uptr addr);\n void ReleaseStore(ThreadState *thr, uptr pc, uptr addr);\n void AfterSleep(ThreadState *thr, uptr pc);\n-void AcquireImpl(ThreadState *thr, uptr pc, SyncClock *c);\n-void ReleaseImpl(ThreadState *thr, uptr pc, SyncClock *c);\n-void ReleaseStoreAcquireImpl(ThreadState *thr, uptr pc, SyncClock *c);\n-void ReleaseStoreImpl(ThreadState *thr, uptr pc, SyncClock *c);\n-void AcquireReleaseImpl(ThreadState *thr, uptr pc, SyncClock *c);\n-\n-// The hacky call uses custom calling convention and an assembly thunk.\n-// It is considerably faster that a normal call for the caller\n-// if it is not executed (it is intended for slow paths from hot functions).\n-// The trick is that the call preserves all registers and the compiler\n-// does not treat it as a call.\n-// If it does not work for you, use normal call.\n-#if !SANITIZER_DEBUG && defined(__x86_64__) && !SANITIZER_MAC\n-// The caller may not create the stack frame for itself at all,\n-// so we create a reserve stack frame for it (1024b must be enough).\n-#define HACKY_CALL(f) \\\n-  __asm__ __volatile__(\"sub $1024, %%rsp;\" \\\n-                       CFI_INL_ADJUST_CFA_OFFSET(1024) \\\n-                       \".hidden \" #f \"_thunk;\" \\\n-                       \"call \" #f \"_thunk;\" \\\n-                       \"add $1024, %%rsp;\" \\\n-                       CFI_INL_ADJUST_CFA_OFFSET(-1024) \\\n-                       ::: \"memory\", \"cc\");\n-#else\n-#define HACKY_CALL(f) f()\n-#endif\n-\n-void TraceSwitch(ThreadState *thr);\n-uptr TraceTopPC(ThreadState *thr);\n-uptr TraceSize();\n-uptr TraceParts();\n-Trace *ThreadTrace(Tid tid);\n-\n-extern \"C\" void __tsan_trace_switch();\n-void ALWAYS_INLINE TraceAddEvent(ThreadState *thr, FastState fs,\n-                                        EventType typ, u64 addr) {\n-  if (!kCollectHistory)\n-    return;\n-  DCHECK_GE((int)typ, 0);\n-  DCHECK_LE((int)typ, 7);\n-  DCHECK_EQ(GetLsb(addr, kEventPCBits), addr);\n-  u64 pos = fs.GetTracePos();\n-  if (UNLIKELY((pos % kTracePartSize) == 0)) {\n-#if !SANITIZER_GO\n-    HACKY_CALL(__tsan_trace_switch);\n-#else\n-    TraceSwitch(thr);\n-#endif\n-  }\n-  Event *trace = (Event*)GetThreadTrace(fs.tid());\n-  Event *evp = &trace[pos];\n-  Event ev = (u64)addr | ((u64)typ << kEventPCBits);\n-  *evp = ev;\n-}\n+void IncrementEpoch(ThreadState *thr);\n \n #if !SANITIZER_GO\n uptr ALWAYS_INLINE HeapEnd() {\n   return HeapMemEnd() + PrimaryAllocator::AdditionalSize();\n }\n #endif\n \n+void SlotAttachAndLock(ThreadState *thr) SANITIZER_ACQUIRE(thr->slot->mtx);\n+void SlotDetach(ThreadState *thr);\n+void SlotLock(ThreadState *thr) SANITIZER_ACQUIRE(thr->slot->mtx);\n+void SlotUnlock(ThreadState *thr) SANITIZER_RELEASE(thr->slot->mtx);\n+void DoReset(ThreadState *thr, uptr epoch);\n+void FlushShadowMemory();\n+\n ThreadState *FiberCreate(ThreadState *thr, uptr pc, unsigned flags);\n void FiberDestroy(ThreadState *thr, uptr pc, ThreadState *fiber);\n void FiberSwitch(ThreadState *thr, uptr pc, ThreadState *fiber, unsigned flags);\n@@ -649,6 +584,43 @@ enum FiberSwitchFlags {\n   FiberSwitchFlagNoSync = 1 << 0, // __tsan_switch_to_fiber_no_sync\n };\n \n+class SlotLocker {\n+ public:\n+  ALWAYS_INLINE\n+  SlotLocker(ThreadState *thr, bool recursive = false)\n+      : thr_(thr), locked_(recursive ? thr->slot_locked : false) {\n+    if (!locked_)\n+      SlotLock(thr_);\n+  }\n+\n+  ALWAYS_INLINE\n+  ~SlotLocker() {\n+    if (!locked_)\n+      SlotUnlock(thr_);\n+  }\n+\n+ private:\n+  ThreadState *thr_;\n+  bool locked_;\n+};\n+\n+class SlotUnlocker {\n+ public:\n+  SlotUnlocker(ThreadState *thr) : thr_(thr), locked_(thr->slot_locked) {\n+    if (locked_)\n+      SlotUnlock(thr_);\n+  }\n+\n+  ~SlotUnlocker() {\n+    if (locked_)\n+      SlotLock(thr_);\n+  }\n+\n+ private:\n+  ThreadState *thr_;\n+  bool locked_;\n+};\n+\n ALWAYS_INLINE void ProcessPendingSignals(ThreadState *thr) {\n   if (UNLIKELY(atomic_load_relaxed(&thr->pending_signals)))\n     ProcessPendingSignalsImpl(thr);\n@@ -667,16 +639,19 @@ void LazyInitialize(ThreadState *thr) {\n #endif\n }\n \n-namespace v3 {\n-\n+void TraceResetForTesting();\n void TraceSwitchPart(ThreadState *thr);\n-bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n-                  uptr size, AccessType typ, VarSizeStackTrace *pstk,\n+void TraceSwitchPartImpl(ThreadState *thr);\n+bool RestoreStack(EventType type, Sid sid, Epoch epoch, uptr addr, uptr size,\n+                  AccessType typ, Tid *ptid, VarSizeStackTrace *pstk,\n                   MutexSet *pmset, uptr *ptag);\n \n template <typename EventT>\n ALWAYS_INLINE WARN_UNUSED_RESULT bool TraceAcquire(ThreadState *thr,\n                                                    EventT **ev) {\n+  // TraceSwitchPart accesses shadow_stack, but it's called infrequently,\n+  // so we check it here proactively.\n+  DCHECK(thr->shadow_stack);\n   Event *pos = reinterpret_cast<Event *>(atomic_load_relaxed(&thr->trace_pos));\n #if SANITIZER_DEBUG\n   // TraceSwitch acquires these mutexes,\n@@ -747,20 +722,16 @@ void TraceMutexLock(ThreadState *thr, EventType type, uptr pc, uptr addr,\n void TraceMutexUnlock(ThreadState *thr, uptr addr);\n void TraceTime(ThreadState *thr);\n \n-}  // namespace v3\n+void TraceRestartFuncExit(ThreadState *thr);\n+void TraceRestartFuncEntry(ThreadState *thr, uptr pc);\n \n void GrowShadowStack(ThreadState *thr);\n \n ALWAYS_INLINE\n void FuncEntry(ThreadState *thr, uptr pc) {\n-  DPrintf2(\"#%d: FuncEntry %p\\n\", (int)thr->fast_state.tid(), (void *)pc);\n-  if (kCollectHistory) {\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeFuncEnter, pc);\n-  }\n-\n-  // Shadow stack maintenance can be replaced with\n-  // stack unwinding during trace switch (which presumably must be faster).\n+  DPrintf2(\"#%d: FuncEntry %p\\n\", (int)thr->fast_state.sid(), (void *)pc);\n+  if (UNLIKELY(!TryTraceFunc(thr, pc)))\n+    return TraceRestartFuncEntry(thr, pc);\n   DCHECK_GE(thr->shadow_stack_pos, thr->shadow_stack);\n #if !SANITIZER_GO\n   DCHECK_LT(thr->shadow_stack_pos, thr->shadow_stack_end);\n@@ -774,12 +745,9 @@ void FuncEntry(ThreadState *thr, uptr pc) {\n \n ALWAYS_INLINE\n void FuncExit(ThreadState *thr) {\n-  DPrintf2(\"#%d: FuncExit\\n\", (int)thr->fast_state.tid());\n-  if (kCollectHistory) {\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeFuncExit, 0);\n-  }\n-\n+  DPrintf2(\"#%d: FuncExit\\n\", (int)thr->fast_state.sid());\n+  if (UNLIKELY(!TryTraceFunc(thr, 0)))\n+    return TraceRestartFuncExit(thr);\n   DCHECK_GT(thr->shadow_stack_pos, thr->shadow_stack);\n #if !SANITIZER_GO\n   DCHECK_LT(thr->shadow_stack_pos, thr->shadow_stack_end);\n@@ -791,7 +759,6 @@ void FuncExit(ThreadState *thr) {\n extern void (*on_initialize)(void);\n extern int (*on_finalize)(int);\n #endif\n-\n }  // namespace __tsan\n \n #endif  // TSAN_RTL_H"}, {"sha": "7d771bfaad7f71300d73686abdc562b0aa7a9a6f", "filename": "libsanitizer/tsan/tsan_rtl_access.cpp", "status": "modified", "additions": 522, "deletions": 373, "changes": 895, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_access.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_access.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_access.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -15,15 +15,13 @@\n \n namespace __tsan {\n \n-namespace v3 {\n-\n-ALWAYS_INLINE USED bool TryTraceMemoryAccess(ThreadState *thr, uptr pc,\n+ALWAYS_INLINE USED bool TryTraceMemoryAccess(ThreadState* thr, uptr pc,\n                                              uptr addr, uptr size,\n                                              AccessType typ) {\n   DCHECK(size == 1 || size == 2 || size == 4 || size == 8);\n   if (!kCollectHistory)\n     return true;\n-  EventAccess *ev;\n+  EventAccess* ev;\n   if (UNLIKELY(!TraceAcquire(thr, &ev)))\n     return false;\n   u64 size_log = size == 1 ? 0 : size == 2 ? 1 : size == 4 ? 2 : 3;\n@@ -40,25 +38,27 @@ ALWAYS_INLINE USED bool TryTraceMemoryAccess(ThreadState *thr, uptr pc,\n     TraceRelease(thr, ev);\n     return true;\n   }\n-  auto *evex = reinterpret_cast<EventAccessExt *>(ev);\n+  auto* evex = reinterpret_cast<EventAccessExt*>(ev);\n   evex->is_access = 0;\n   evex->is_func = 0;\n   evex->type = EventType::kAccessExt;\n   evex->is_read = !!(typ & kAccessRead);\n   evex->is_atomic = !!(typ & kAccessAtomic);\n   evex->size_log = size_log;\n+  // Note: this is important, see comment in EventAccessExt.\n+  evex->_ = 0;\n   evex->addr = CompressAddr(addr);\n   evex->pc = pc;\n   TraceRelease(thr, evex);\n   return true;\n }\n \n-ALWAYS_INLINE USED bool TryTraceMemoryAccessRange(ThreadState *thr, uptr pc,\n-                                                  uptr addr, uptr size,\n-                                                  AccessType typ) {\n+ALWAYS_INLINE\n+bool TryTraceMemoryAccessRange(ThreadState* thr, uptr pc, uptr addr, uptr size,\n+                               AccessType typ) {\n   if (!kCollectHistory)\n     return true;\n-  EventAccessRange *ev;\n+  EventAccessRange* ev;\n   if (UNLIKELY(!TraceAcquire(thr, &ev)))\n     return false;\n   thr->trace_prev_pc = pc;\n@@ -75,7 +75,7 @@ ALWAYS_INLINE USED bool TryTraceMemoryAccessRange(ThreadState *thr, uptr pc,\n   return true;\n }\n \n-void TraceMemoryAccessRange(ThreadState *thr, uptr pc, uptr addr, uptr size,\n+void TraceMemoryAccessRange(ThreadState* thr, uptr pc, uptr addr, uptr size,\n                             AccessType typ) {\n   if (LIKELY(TryTraceMemoryAccessRange(thr, pc, addr, size, typ)))\n     return;\n@@ -84,15 +84,25 @@ void TraceMemoryAccessRange(ThreadState *thr, uptr pc, uptr addr, uptr size,\n   DCHECK(res);\n }\n \n-void TraceFunc(ThreadState *thr, uptr pc) {\n+void TraceFunc(ThreadState* thr, uptr pc) {\n   if (LIKELY(TryTraceFunc(thr, pc)))\n     return;\n   TraceSwitchPart(thr);\n   UNUSED bool res = TryTraceFunc(thr, pc);\n   DCHECK(res);\n }\n \n-void TraceMutexLock(ThreadState *thr, EventType type, uptr pc, uptr addr,\n+NOINLINE void TraceRestartFuncEntry(ThreadState* thr, uptr pc) {\n+  TraceSwitchPart(thr);\n+  FuncEntry(thr, pc);\n+}\n+\n+NOINLINE void TraceRestartFuncExit(ThreadState* thr) {\n+  TraceSwitchPart(thr);\n+  FuncExit(thr);\n+}\n+\n+void TraceMutexLock(ThreadState* thr, EventType type, uptr pc, uptr addr,\n                     StackID stk) {\n   DCHECK(type == EventType::kLock || type == EventType::kRLock);\n   if (!kCollectHistory)\n@@ -109,7 +119,7 @@ void TraceMutexLock(ThreadState *thr, EventType type, uptr pc, uptr addr,\n   TraceEvent(thr, ev);\n }\n \n-void TraceMutexUnlock(ThreadState *thr, uptr addr) {\n+void TraceMutexUnlock(ThreadState* thr, uptr addr) {\n   if (!kCollectHistory)\n     return;\n   EventUnlock ev;\n@@ -121,411 +131,553 @@ void TraceMutexUnlock(ThreadState *thr, uptr addr) {\n   TraceEvent(thr, ev);\n }\n \n-void TraceTime(ThreadState *thr) {\n+void TraceTime(ThreadState* thr) {\n   if (!kCollectHistory)\n     return;\n+  FastState fast_state = thr->fast_state;\n   EventTime ev;\n   ev.is_access = 0;\n   ev.is_func = 0;\n   ev.type = EventType::kTime;\n-  ev.sid = static_cast<u64>(thr->sid);\n-  ev.epoch = static_cast<u64>(thr->epoch);\n+  ev.sid = static_cast<u64>(fast_state.sid());\n+  ev.epoch = static_cast<u64>(fast_state.epoch());\n   ev._ = 0;\n   TraceEvent(thr, ev);\n }\n \n-}  // namespace v3\n+ALWAYS_INLINE RawShadow LoadShadow(RawShadow* p) {\n+  return static_cast<RawShadow>(\n+      atomic_load((atomic_uint32_t*)p, memory_order_relaxed));\n+}\n \n-ALWAYS_INLINE\n-Shadow LoadShadow(u64 *p) {\n-  u64 raw = atomic_load((atomic_uint64_t *)p, memory_order_relaxed);\n-  return Shadow(raw);\n+ALWAYS_INLINE void StoreShadow(RawShadow* sp, RawShadow s) {\n+  atomic_store((atomic_uint32_t*)sp, static_cast<u32>(s), memory_order_relaxed);\n }\n \n-ALWAYS_INLINE\n-void StoreShadow(u64 *sp, u64 s) {\n-  atomic_store((atomic_uint64_t *)sp, s, memory_order_relaxed);\n+NOINLINE void DoReportRace(ThreadState* thr, RawShadow* shadow_mem, Shadow cur,\n+                           Shadow old,\n+                           AccessType typ) SANITIZER_NO_THREAD_SAFETY_ANALYSIS {\n+  // For the free shadow markers the first element (that contains kFreeSid)\n+  // triggers the race, but the second element contains info about the freeing\n+  // thread, take it.\n+  if (old.sid() == kFreeSid)\n+    old = Shadow(LoadShadow(&shadow_mem[1]));\n+  // This prevents trapping on this address in future.\n+  for (uptr i = 0; i < kShadowCnt; i++)\n+    StoreShadow(&shadow_mem[i], i == 0 ? Shadow::kRodata : Shadow::kEmpty);\n+  // See the comment in MemoryRangeFreed as to why the slot is locked\n+  // for free memory accesses. ReportRace must not be called with\n+  // the slot locked because of the fork. But MemoryRangeFreed is not\n+  // called during fork because fork sets ignore_reads_and_writes,\n+  // so simply unlocking the slot should be fine.\n+  if (typ & kAccessSlotLocked)\n+    SlotUnlock(thr);\n+  ReportRace(thr, shadow_mem, cur, Shadow(old), typ);\n+  if (typ & kAccessSlotLocked)\n+    SlotLock(thr);\n }\n \n+#if !TSAN_VECTORIZE\n ALWAYS_INLINE\n-void StoreIfNotYetStored(u64 *sp, u64 *s) {\n-  StoreShadow(sp, *s);\n-  *s = 0;\n+bool ContainsSameAccess(RawShadow* s, Shadow cur, int unused0, int unused1,\n+                        AccessType typ) {\n+  for (uptr i = 0; i < kShadowCnt; i++) {\n+    auto old = LoadShadow(&s[i]);\n+    if (!(typ & kAccessRead)) {\n+      if (old == cur.raw())\n+        return true;\n+      continue;\n+    }\n+    auto masked = static_cast<RawShadow>(static_cast<u32>(old) |\n+                                         static_cast<u32>(Shadow::kRodata));\n+    if (masked == cur.raw())\n+      return true;\n+    if (!(typ & kAccessNoRodata) && !SANITIZER_GO) {\n+      if (old == Shadow::kRodata)\n+        return true;\n+    }\n+  }\n+  return false;\n }\n \n-extern \"C\" void __tsan_report_race();\n-\n ALWAYS_INLINE\n-void HandleRace(ThreadState *thr, u64 *shadow_mem, Shadow cur, Shadow old) {\n-  thr->racy_state[0] = cur.raw();\n-  thr->racy_state[1] = old.raw();\n-  thr->racy_shadow_addr = shadow_mem;\n-#if !SANITIZER_GO\n-  HACKY_CALL(__tsan_report_race);\n-#else\n-  ReportRace(thr);\n-#endif\n+bool CheckRaces(ThreadState* thr, RawShadow* shadow_mem, Shadow cur,\n+                int unused0, int unused1, AccessType typ) {\n+  bool stored = false;\n+  for (uptr idx = 0; idx < kShadowCnt; idx++) {\n+    RawShadow* sp = &shadow_mem[idx];\n+    Shadow old(LoadShadow(sp));\n+    if (LIKELY(old.raw() == Shadow::kEmpty)) {\n+      if (!(typ & kAccessCheckOnly) && !stored)\n+        StoreShadow(sp, cur.raw());\n+      return false;\n+    }\n+    if (LIKELY(!(cur.access() & old.access())))\n+      continue;\n+    if (LIKELY(cur.sid() == old.sid())) {\n+      if (!(typ & kAccessCheckOnly) &&\n+          LIKELY(cur.access() == old.access() && old.IsRWWeakerOrEqual(typ))) {\n+        StoreShadow(sp, cur.raw());\n+        stored = true;\n+      }\n+      continue;\n+    }\n+    if (LIKELY(old.IsBothReadsOrAtomic(typ)))\n+      continue;\n+    if (LIKELY(thr->clock.Get(old.sid()) >= old.epoch()))\n+      continue;\n+    DoReportRace(thr, shadow_mem, cur, old, typ);\n+    return true;\n+  }\n+  // We did not find any races and had already stored\n+  // the current access info, so we are done.\n+  if (LIKELY(stored))\n+    return false;\n+  // Choose a random candidate slot and replace it.\n+  uptr index =\n+      atomic_load_relaxed(&thr->trace_pos) / sizeof(Event) % kShadowCnt;\n+  StoreShadow(&shadow_mem[index], cur.raw());\n+  return false;\n }\n \n-static inline bool HappensBefore(Shadow old, ThreadState *thr) {\n-  return thr->clock.get(old.TidWithIgnore()) >= old.epoch();\n-}\n+#  define LOAD_CURRENT_SHADOW(cur, shadow_mem) UNUSED int access = 0, shadow = 0\n \n-ALWAYS_INLINE\n-void MemoryAccessImpl1(ThreadState *thr, uptr addr, int kAccessSizeLog,\n-                       bool kAccessIsWrite, bool kIsAtomic, u64 *shadow_mem,\n-                       Shadow cur) {\n-  // This potentially can live in an MMX/SSE scratch register.\n-  // The required intrinsics are:\n-  // __m128i _mm_move_epi64(__m128i*);\n-  // _mm_storel_epi64(u64*, __m128i);\n-  u64 store_word = cur.raw();\n-  bool stored = false;\n+#else /* !TSAN_VECTORIZE */\n \n-  // scan all the shadow values and dispatch to 4 categories:\n-  // same, replace, candidate and race (see comments below).\n-  // we consider only 3 cases regarding access sizes:\n-  // equal, intersect and not intersect. initially I considered\n-  // larger and smaller as well, it allowed to replace some\n-  // 'candidates' with 'same' or 'replace', but I think\n-  // it's just not worth it (performance- and complexity-wise).\n-\n-  Shadow old(0);\n-\n-  // It release mode we manually unroll the loop,\n-  // because empirically gcc generates better code this way.\n-  // However, we can't afford unrolling in debug mode, because the function\n-  // consumes almost 4K of stack. Gtest gives only 4K of stack to death test\n-  // threads, which is not enough for the unrolled loop.\n-#if SANITIZER_DEBUG\n-  for (int idx = 0; idx < 4; idx++) {\n-#  include \"tsan_update_shadow_word.inc\"\n-  }\n-#else\n-  int idx = 0;\n-#  include \"tsan_update_shadow_word.inc\"\n-  idx = 1;\n-  if (stored) {\n-#  include \"tsan_update_shadow_word.inc\"\n-  } else {\n-#  include \"tsan_update_shadow_word.inc\"\n-  }\n-  idx = 2;\n-  if (stored) {\n-#  include \"tsan_update_shadow_word.inc\"\n-  } else {\n-#  include \"tsan_update_shadow_word.inc\"\n+ALWAYS_INLINE\n+bool ContainsSameAccess(RawShadow* unused0, Shadow unused1, m128 shadow,\n+                        m128 access, AccessType typ) {\n+  // Note: we could check if there is a larger access of the same type,\n+  // e.g. we just allocated/memset-ed a block (so it contains 8 byte writes)\n+  // and now do smaller reads/writes, these can also be considered as \"same\n+  // access\". However, it will make the check more expensive, so it's unclear\n+  // if it's worth it. But this would conserve trace space, so it's useful\n+  // besides potential speed up.\n+  if (!(typ & kAccessRead)) {\n+    const m128 same = _mm_cmpeq_epi32(shadow, access);\n+    return _mm_movemask_epi8(same);\n   }\n-  idx = 3;\n-  if (stored) {\n-#  include \"tsan_update_shadow_word.inc\"\n-  } else {\n-#  include \"tsan_update_shadow_word.inc\"\n+  // For reads we need to reset read bit in the shadow,\n+  // because we need to match read with both reads and writes.\n+  // Shadow::kRodata has only read bit set, so it does what we want.\n+  // We also abuse it for rodata check to save few cycles\n+  // since we already loaded Shadow::kRodata into a register.\n+  // Reads from rodata can't race.\n+  // Measurements show that they can be 10-20% of all memory accesses.\n+  // Shadow::kRodata has epoch 0 which cannot appear in shadow normally\n+  // (thread epochs start from 1). So the same read bit mask\n+  // serves as rodata indicator.\n+  const m128 read_mask = _mm_set1_epi32(static_cast<u32>(Shadow::kRodata));\n+  const m128 masked_shadow = _mm_or_si128(shadow, read_mask);\n+  m128 same = _mm_cmpeq_epi32(masked_shadow, access);\n+  // Range memory accesses check Shadow::kRodata before calling this,\n+  // Shadow::kRodatas is not possible for free memory access\n+  // and Go does not use Shadow::kRodata.\n+  if (!(typ & kAccessNoRodata) && !SANITIZER_GO) {\n+    const m128 ro = _mm_cmpeq_epi32(shadow, read_mask);\n+    same = _mm_or_si128(ro, same);\n   }\n-#endif\n-\n-  // we did not find any races and had already stored\n-  // the current access info, so we are done\n-  if (LIKELY(stored))\n-    return;\n-  // choose a random candidate slot and replace it\n-  StoreShadow(shadow_mem + (cur.epoch() % kShadowCnt), store_word);\n-  return;\n-RACE:\n-  HandleRace(thr, shadow_mem, cur, old);\n-  return;\n+  return _mm_movemask_epi8(same);\n }\n \n-void UnalignedMemoryAccess(ThreadState *thr, uptr pc, uptr addr, uptr size,\n-                           AccessType typ) {\n-  DCHECK(!(typ & kAccessAtomic));\n-  const bool kAccessIsWrite = !(typ & kAccessRead);\n-  const bool kIsAtomic = false;\n-  while (size) {\n-    int size1 = 1;\n-    int kAccessSizeLog = kSizeLog1;\n-    if (size >= 8 && (addr & ~7) == ((addr + 7) & ~7)) {\n-      size1 = 8;\n-      kAccessSizeLog = kSizeLog8;\n-    } else if (size >= 4 && (addr & ~7) == ((addr + 3) & ~7)) {\n-      size1 = 4;\n-      kAccessSizeLog = kSizeLog4;\n-    } else if (size >= 2 && (addr & ~7) == ((addr + 1) & ~7)) {\n-      size1 = 2;\n-      kAccessSizeLog = kSizeLog2;\n-    }\n-    MemoryAccess(thr, pc, addr, kAccessSizeLog, kAccessIsWrite, kIsAtomic);\n-    addr += size1;\n-    size -= size1;\n+NOINLINE void DoReportRaceV(ThreadState* thr, RawShadow* shadow_mem, Shadow cur,\n+                            u32 race_mask, m128 shadow, AccessType typ) {\n+  // race_mask points which of the shadow elements raced with the current\n+  // access. Extract that element.\n+  CHECK_NE(race_mask, 0);\n+  u32 old;\n+  // Note: _mm_extract_epi32 index must be a constant value.\n+  switch (__builtin_ffs(race_mask) / 4) {\n+    case 0:\n+      old = _mm_extract_epi32(shadow, 0);\n+      break;\n+    case 1:\n+      old = _mm_extract_epi32(shadow, 1);\n+      break;\n+    case 2:\n+      old = _mm_extract_epi32(shadow, 2);\n+      break;\n+    case 3:\n+      old = _mm_extract_epi32(shadow, 3);\n+      break;\n   }\n+  Shadow prev(static_cast<RawShadow>(old));\n+  // For the free shadow markers the first element (that contains kFreeSid)\n+  // triggers the race, but the second element contains info about the freeing\n+  // thread, take it.\n+  if (prev.sid() == kFreeSid)\n+    prev = Shadow(static_cast<RawShadow>(_mm_extract_epi32(shadow, 1)));\n+  DoReportRace(thr, shadow_mem, cur, prev, typ);\n }\n \n ALWAYS_INLINE\n-bool ContainsSameAccessSlow(u64 *s, u64 a, u64 sync_epoch, bool is_write) {\n-  Shadow cur(a);\n-  for (uptr i = 0; i < kShadowCnt; i++) {\n-    Shadow old(LoadShadow(&s[i]));\n-    if (Shadow::Addr0AndSizeAreEqual(cur, old) &&\n-        old.TidWithIgnore() == cur.TidWithIgnore() &&\n-        old.epoch() > sync_epoch && old.IsAtomic() == cur.IsAtomic() &&\n-        old.IsRead() <= cur.IsRead())\n-      return true;\n+bool CheckRaces(ThreadState* thr, RawShadow* shadow_mem, Shadow cur,\n+                m128 shadow, m128 access, AccessType typ) {\n+  // Note: empty/zero slots don't intersect with any access.\n+  const m128 zero = _mm_setzero_si128();\n+  const m128 mask_access = _mm_set1_epi32(0x000000ff);\n+  const m128 mask_sid = _mm_set1_epi32(0x0000ff00);\n+  const m128 mask_read_atomic = _mm_set1_epi32(0xc0000000);\n+  const m128 access_and = _mm_and_si128(access, shadow);\n+  const m128 access_xor = _mm_xor_si128(access, shadow);\n+  const m128 intersect = _mm_and_si128(access_and, mask_access);\n+  const m128 not_intersect = _mm_cmpeq_epi32(intersect, zero);\n+  const m128 not_same_sid = _mm_and_si128(access_xor, mask_sid);\n+  const m128 same_sid = _mm_cmpeq_epi32(not_same_sid, zero);\n+  const m128 both_read_or_atomic = _mm_and_si128(access_and, mask_read_atomic);\n+  const m128 no_race =\n+      _mm_or_si128(_mm_or_si128(not_intersect, same_sid), both_read_or_atomic);\n+  const int race_mask = _mm_movemask_epi8(_mm_cmpeq_epi32(no_race, zero));\n+  if (UNLIKELY(race_mask))\n+    goto SHARED;\n+\n+STORE : {\n+  if (typ & kAccessCheckOnly)\n+    return false;\n+  // We could also replace different sid's if access is the same,\n+  // rw weaker and happens before. However, just checking access below\n+  // is not enough because we also need to check that !both_read_or_atomic\n+  // (reads from different sids can be concurrent).\n+  // Theoretically we could replace smaller accesses with larger accesses,\n+  // but it's unclear if it's worth doing.\n+  const m128 mask_access_sid = _mm_set1_epi32(0x0000ffff);\n+  const m128 not_same_sid_access = _mm_and_si128(access_xor, mask_access_sid);\n+  const m128 same_sid_access = _mm_cmpeq_epi32(not_same_sid_access, zero);\n+  const m128 access_read_atomic =\n+      _mm_set1_epi32((typ & (kAccessRead | kAccessAtomic)) << 30);\n+  const m128 rw_weaker =\n+      _mm_cmpeq_epi32(_mm_max_epu32(shadow, access_read_atomic), shadow);\n+  const m128 rewrite = _mm_and_si128(same_sid_access, rw_weaker);\n+  const int rewrite_mask = _mm_movemask_epi8(rewrite);\n+  int index = __builtin_ffs(rewrite_mask);\n+  if (UNLIKELY(index == 0)) {\n+    const m128 empty = _mm_cmpeq_epi32(shadow, zero);\n+    const int empty_mask = _mm_movemask_epi8(empty);\n+    index = __builtin_ffs(empty_mask);\n+    if (UNLIKELY(index == 0))\n+      index = (atomic_load_relaxed(&thr->trace_pos) / 2) % 16;\n   }\n+  StoreShadow(&shadow_mem[index / 4], cur.raw());\n+  // We could zero other slots determined by rewrite_mask.\n+  // That would help other threads to evict better slots,\n+  // but it's unclear if it's worth it.\n   return false;\n }\n \n-#if TSAN_VECTORIZE\n-#  define SHUF(v0, v1, i0, i1, i2, i3)                    \\\n-    _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(v0), \\\n-                                    _mm_castsi128_ps(v1), \\\n-                                    (i0)*1 + (i1)*4 + (i2)*16 + (i3)*64))\n-ALWAYS_INLINE\n-bool ContainsSameAccessFast(u64 *s, u64 a, u64 sync_epoch, bool is_write) {\n-  // This is an optimized version of ContainsSameAccessSlow.\n-  // load current access into access[0:63]\n-  const m128 access = _mm_cvtsi64_si128(a);\n-  // duplicate high part of access in addr0:\n-  // addr0[0:31]        = access[32:63]\n-  // addr0[32:63]       = access[32:63]\n-  // addr0[64:95]       = access[32:63]\n-  // addr0[96:127]      = access[32:63]\n-  const m128 addr0 = SHUF(access, access, 1, 1, 1, 1);\n-  // load 4 shadow slots\n-  const m128 shadow0 = _mm_load_si128((__m128i *)s);\n-  const m128 shadow1 = _mm_load_si128((__m128i *)s + 1);\n-  // load high parts of 4 shadow slots into addr_vect:\n-  // addr_vect[0:31]    = shadow0[32:63]\n-  // addr_vect[32:63]   = shadow0[96:127]\n-  // addr_vect[64:95]   = shadow1[32:63]\n-  // addr_vect[96:127]  = shadow1[96:127]\n-  m128 addr_vect = SHUF(shadow0, shadow1, 1, 3, 1, 3);\n-  if (!is_write) {\n-    // set IsRead bit in addr_vect\n-    const m128 rw_mask1 = _mm_cvtsi64_si128(1 << 15);\n-    const m128 rw_mask = SHUF(rw_mask1, rw_mask1, 0, 0, 0, 0);\n-    addr_vect = _mm_or_si128(addr_vect, rw_mask);\n-  }\n-  // addr0 == addr_vect?\n-  const m128 addr_res = _mm_cmpeq_epi32(addr0, addr_vect);\n-  // epoch1[0:63]       = sync_epoch\n-  const m128 epoch1 = _mm_cvtsi64_si128(sync_epoch);\n-  // epoch[0:31]        = sync_epoch[0:31]\n-  // epoch[32:63]       = sync_epoch[0:31]\n-  // epoch[64:95]       = sync_epoch[0:31]\n-  // epoch[96:127]      = sync_epoch[0:31]\n-  const m128 epoch = SHUF(epoch1, epoch1, 0, 0, 0, 0);\n-  // load low parts of shadow cell epochs into epoch_vect:\n-  // epoch_vect[0:31]   = shadow0[0:31]\n-  // epoch_vect[32:63]  = shadow0[64:95]\n-  // epoch_vect[64:95]  = shadow1[0:31]\n-  // epoch_vect[96:127] = shadow1[64:95]\n-  const m128 epoch_vect = SHUF(shadow0, shadow1, 0, 2, 0, 2);\n-  // epoch_vect >= sync_epoch?\n-  const m128 epoch_res = _mm_cmpgt_epi32(epoch_vect, epoch);\n-  // addr_res & epoch_res\n-  const m128 res = _mm_and_si128(addr_res, epoch_res);\n-  // mask[0] = res[7]\n-  // mask[1] = res[15]\n-  // ...\n-  // mask[15] = res[127]\n-  const int mask = _mm_movemask_epi8(res);\n-  return mask != 0;\n+SHARED:\n+  m128 thread_epochs = _mm_set1_epi32(0x7fffffff);\n+  // Need to unwind this because _mm_extract_epi8/_mm_insert_epi32\n+  // indexes must be constants.\n+#  define LOAD_EPOCH(idx)                                                     \\\n+    if (LIKELY(race_mask & (1 << (idx * 4)))) {                               \\\n+      u8 sid = _mm_extract_epi8(shadow, idx * 4 + 1);                         \\\n+      u16 epoch = static_cast<u16>(thr->clock.Get(static_cast<Sid>(sid)));    \\\n+      thread_epochs = _mm_insert_epi32(thread_epochs, u32(epoch) << 16, idx); \\\n+    }\n+  LOAD_EPOCH(0);\n+  LOAD_EPOCH(1);\n+  LOAD_EPOCH(2);\n+  LOAD_EPOCH(3);\n+#  undef LOAD_EPOCH\n+  const m128 mask_epoch = _mm_set1_epi32(0x3fff0000);\n+  const m128 shadow_epochs = _mm_and_si128(shadow, mask_epoch);\n+  const m128 concurrent = _mm_cmplt_epi32(thread_epochs, shadow_epochs);\n+  const int concurrent_mask = _mm_movemask_epi8(concurrent);\n+  if (LIKELY(concurrent_mask == 0))\n+    goto STORE;\n+\n+  DoReportRaceV(thr, shadow_mem, cur, concurrent_mask, shadow, typ);\n+  return true;\n }\n-#endif\n \n-ALWAYS_INLINE\n-bool ContainsSameAccess(u64 *s, u64 a, u64 sync_epoch, bool is_write) {\n-#if TSAN_VECTORIZE\n-  bool res = ContainsSameAccessFast(s, a, sync_epoch, is_write);\n-  // NOTE: this check can fail if the shadow is concurrently mutated\n-  // by other threads. But it still can be useful if you modify\n-  // ContainsSameAccessFast and want to ensure that it's not completely broken.\n-  // DCHECK_EQ(res, ContainsSameAccessSlow(s, a, sync_epoch, is_write));\n-  return res;\n-#else\n-  return ContainsSameAccessSlow(s, a, sync_epoch, is_write);\n+#  define LOAD_CURRENT_SHADOW(cur, shadow_mem)                         \\\n+    const m128 access = _mm_set1_epi32(static_cast<u32>((cur).raw())); \\\n+    const m128 shadow = _mm_load_si128(reinterpret_cast<m128*>(shadow_mem))\n #endif\n-}\n \n-ALWAYS_INLINE USED void MemoryAccess(ThreadState *thr, uptr pc, uptr addr,\n-                                     int kAccessSizeLog, bool kAccessIsWrite,\n-                                     bool kIsAtomic) {\n-  RawShadow *shadow_mem = MemToShadow(addr);\n-  DPrintf2(\n-      \"#%d: MemoryAccess: @%p %p size=%d\"\n-      \" is_write=%d shadow_mem=%p {%zx, %zx, %zx, %zx}\\n\",\n-      (int)thr->fast_state.tid(), (void *)pc, (void *)addr,\n-      (int)(1 << kAccessSizeLog), kAccessIsWrite, shadow_mem,\n-      (uptr)shadow_mem[0], (uptr)shadow_mem[1], (uptr)shadow_mem[2],\n-      (uptr)shadow_mem[3]);\n-#if SANITIZER_DEBUG\n-  if (!IsAppMem(addr)) {\n-    Printf(\"Access to non app mem %zx\\n\", addr);\n-    DCHECK(IsAppMem(addr));\n+char* DumpShadow(char* buf, RawShadow raw) {\n+  if (raw == Shadow::kEmpty) {\n+    internal_snprintf(buf, 64, \"0\");\n+    return buf;\n   }\n-  if (!IsShadowMem(shadow_mem)) {\n-    Printf(\"Bad shadow addr %p (%zx)\\n\", shadow_mem, addr);\n-    DCHECK(IsShadowMem(shadow_mem));\n-  }\n-#endif\n+  Shadow s(raw);\n+  AccessType typ;\n+  s.GetAccess(nullptr, nullptr, &typ);\n+  internal_snprintf(buf, 64, \"{tid=%u@%u access=0x%x typ=%x}\",\n+                    static_cast<u32>(s.sid()), static_cast<u32>(s.epoch()),\n+                    s.access(), static_cast<u32>(typ));\n+  return buf;\n+}\n \n-  if (!SANITIZER_GO && !kAccessIsWrite && *shadow_mem == kShadowRodata) {\n-    // Access to .rodata section, no races here.\n-    // Measurements show that it can be 10-20% of all memory accesses.\n-    return;\n-  }\n+// TryTrace* and TraceRestart* functions allow to turn memory access and func\n+// entry/exit callbacks into leaf functions with all associated performance\n+// benefits. These hottest callbacks do only 2 slow path calls: report a race\n+// and trace part switching. Race reporting is easy to turn into a tail call, we\n+// just always return from the runtime after reporting a race. But trace part\n+// switching is harder because it needs to be in the middle of callbacks. To\n+// turn it into a tail call we immidiately return after TraceRestart* functions,\n+// but TraceRestart* functions themselves recurse into the callback after\n+// switching trace part. As the result the hottest callbacks contain only tail\n+// calls, which effectively makes them leaf functions (can use all registers,\n+// no frame setup, etc).\n+NOINLINE void TraceRestartMemoryAccess(ThreadState* thr, uptr pc, uptr addr,\n+                                       uptr size, AccessType typ) {\n+  TraceSwitchPart(thr);\n+  MemoryAccess(thr, pc, addr, size, typ);\n+}\n+\n+ALWAYS_INLINE USED void MemoryAccess(ThreadState* thr, uptr pc, uptr addr,\n+                                     uptr size, AccessType typ) {\n+  RawShadow* shadow_mem = MemToShadow(addr);\n+  UNUSED char memBuf[4][64];\n+  DPrintf2(\"#%d: Access: %d@%d %p/%zd typ=0x%x {%s, %s, %s, %s}\\n\", thr->tid,\n+           static_cast<int>(thr->fast_state.sid()),\n+           static_cast<int>(thr->fast_state.epoch()), (void*)addr, size,\n+           static_cast<int>(typ), DumpShadow(memBuf[0], shadow_mem[0]),\n+           DumpShadow(memBuf[1], shadow_mem[1]),\n+           DumpShadow(memBuf[2], shadow_mem[2]),\n+           DumpShadow(memBuf[3], shadow_mem[3]));\n \n   FastState fast_state = thr->fast_state;\n-  if (UNLIKELY(fast_state.GetIgnoreBit())) {\n+  Shadow cur(fast_state, addr, size, typ);\n+\n+  LOAD_CURRENT_SHADOW(cur, shadow_mem);\n+  if (LIKELY(ContainsSameAccess(shadow_mem, cur, shadow, access, typ)))\n     return;\n-  }\n+  if (UNLIKELY(fast_state.GetIgnoreBit()))\n+    return;\n+  if (!TryTraceMemoryAccess(thr, pc, addr, size, typ))\n+    return TraceRestartMemoryAccess(thr, pc, addr, size, typ);\n+  CheckRaces(thr, shadow_mem, cur, shadow, access, typ);\n+}\n \n-  Shadow cur(fast_state);\n-  cur.SetAddr0AndSizeLog(addr & 7, kAccessSizeLog);\n-  cur.SetWrite(kAccessIsWrite);\n-  cur.SetAtomic(kIsAtomic);\n+void MemoryAccess16(ThreadState* thr, uptr pc, uptr addr, AccessType typ);\n \n-  if (LIKELY(ContainsSameAccess(shadow_mem, cur.raw(), thr->fast_synch_epoch,\n-                                kAccessIsWrite))) {\n-    return;\n-  }\n+NOINLINE\n+void RestartMemoryAccess16(ThreadState* thr, uptr pc, uptr addr,\n+                           AccessType typ) {\n+  TraceSwitchPart(thr);\n+  MemoryAccess16(thr, pc, addr, typ);\n+}\n \n-  if (kCollectHistory) {\n-    fast_state.IncrementEpoch();\n-    thr->fast_state = fast_state;\n-    TraceAddEvent(thr, fast_state, EventTypeMop, pc);\n-    cur.IncrementEpoch();\n+ALWAYS_INLINE USED void MemoryAccess16(ThreadState* thr, uptr pc, uptr addr,\n+                                       AccessType typ) {\n+  const uptr size = 16;\n+  FastState fast_state = thr->fast_state;\n+  if (UNLIKELY(fast_state.GetIgnoreBit()))\n+    return;\n+  Shadow cur(fast_state, 0, 8, typ);\n+  RawShadow* shadow_mem = MemToShadow(addr);\n+  bool traced = false;\n+  {\n+    LOAD_CURRENT_SHADOW(cur, shadow_mem);\n+    if (LIKELY(ContainsSameAccess(shadow_mem, cur, shadow, access, typ)))\n+      goto SECOND;\n+    if (!TryTraceMemoryAccessRange(thr, pc, addr, size, typ))\n+      return RestartMemoryAccess16(thr, pc, addr, typ);\n+    traced = true;\n+    if (UNLIKELY(CheckRaces(thr, shadow_mem, cur, shadow, access, typ)))\n+      return;\n   }\n+SECOND:\n+  shadow_mem += kShadowCnt;\n+  LOAD_CURRENT_SHADOW(cur, shadow_mem);\n+  if (LIKELY(ContainsSameAccess(shadow_mem, cur, shadow, access, typ)))\n+    return;\n+  if (!traced && !TryTraceMemoryAccessRange(thr, pc, addr, size, typ))\n+    return RestartMemoryAccess16(thr, pc, addr, typ);\n+  CheckRaces(thr, shadow_mem, cur, shadow, access, typ);\n+}\n \n-  MemoryAccessImpl1(thr, addr, kAccessSizeLog, kAccessIsWrite, kIsAtomic,\n-                    shadow_mem, cur);\n+NOINLINE\n+void RestartUnalignedMemoryAccess(ThreadState* thr, uptr pc, uptr addr,\n+                                  uptr size, AccessType typ) {\n+  TraceSwitchPart(thr);\n+  UnalignedMemoryAccess(thr, pc, addr, size, typ);\n }\n \n-// Called by MemoryAccessRange in tsan_rtl_thread.cpp\n-ALWAYS_INLINE USED void MemoryAccessImpl(ThreadState *thr, uptr addr,\n-                                         int kAccessSizeLog,\n-                                         bool kAccessIsWrite, bool kIsAtomic,\n-                                         u64 *shadow_mem, Shadow cur) {\n-  if (LIKELY(ContainsSameAccess(shadow_mem, cur.raw(), thr->fast_synch_epoch,\n-                                kAccessIsWrite))) {\n+ALWAYS_INLINE USED void UnalignedMemoryAccess(ThreadState* thr, uptr pc,\n+                                              uptr addr, uptr size,\n+                                              AccessType typ) {\n+  DCHECK_LE(size, 8);\n+  FastState fast_state = thr->fast_state;\n+  if (UNLIKELY(fast_state.GetIgnoreBit()))\n     return;\n+  RawShadow* shadow_mem = MemToShadow(addr);\n+  bool traced = false;\n+  uptr size1 = Min<uptr>(size, RoundUp(addr + 1, kShadowCell) - addr);\n+  {\n+    Shadow cur(fast_state, addr, size1, typ);\n+    LOAD_CURRENT_SHADOW(cur, shadow_mem);\n+    if (LIKELY(ContainsSameAccess(shadow_mem, cur, shadow, access, typ)))\n+      goto SECOND;\n+    if (!TryTraceMemoryAccessRange(thr, pc, addr, size, typ))\n+      return RestartUnalignedMemoryAccess(thr, pc, addr, size, typ);\n+    traced = true;\n+    if (UNLIKELY(CheckRaces(thr, shadow_mem, cur, shadow, access, typ)))\n+      return;\n   }\n+SECOND:\n+  uptr size2 = size - size1;\n+  if (LIKELY(size2 == 0))\n+    return;\n+  shadow_mem += kShadowCnt;\n+  Shadow cur(fast_state, 0, size2, typ);\n+  LOAD_CURRENT_SHADOW(cur, shadow_mem);\n+  if (LIKELY(ContainsSameAccess(shadow_mem, cur, shadow, access, typ)))\n+    return;\n+  if (!traced && !TryTraceMemoryAccessRange(thr, pc, addr, size, typ))\n+    return RestartUnalignedMemoryAccess(thr, pc, addr, size, typ);\n+  CheckRaces(thr, shadow_mem, cur, shadow, access, typ);\n+}\n \n-  MemoryAccessImpl1(thr, addr, kAccessSizeLog, kAccessIsWrite, kIsAtomic,\n-                    shadow_mem, cur);\n+void ShadowSet(RawShadow* p, RawShadow* end, RawShadow v) {\n+  DCHECK_LE(p, end);\n+  DCHECK(IsShadowMem(p));\n+  DCHECK(IsShadowMem(end));\n+  UNUSED const uptr kAlign = kShadowCnt * kShadowSize;\n+  DCHECK_EQ(reinterpret_cast<uptr>(p) % kAlign, 0);\n+  DCHECK_EQ(reinterpret_cast<uptr>(end) % kAlign, 0);\n+#if !TSAN_VECTORIZE\n+  for (; p < end; p += kShadowCnt) {\n+    p[0] = v;\n+    for (uptr i = 1; i < kShadowCnt; i++) p[i] = Shadow::kEmpty;\n+  }\n+#else\n+  m128 vv = _mm_setr_epi32(\n+      static_cast<u32>(v), static_cast<u32>(Shadow::kEmpty),\n+      static_cast<u32>(Shadow::kEmpty), static_cast<u32>(Shadow::kEmpty));\n+  m128* vp = reinterpret_cast<m128*>(p);\n+  m128* vend = reinterpret_cast<m128*>(end);\n+  for (; vp < vend; vp++) _mm_store_si128(vp, vv);\n+#endif\n }\n \n-static void MemoryRangeSet(ThreadState *thr, uptr pc, uptr addr, uptr size,\n-                           u64 val) {\n-  (void)thr;\n-  (void)pc;\n+static void MemoryRangeSet(uptr addr, uptr size, RawShadow val) {\n   if (size == 0)\n     return;\n-  // FIXME: fix me.\n-  uptr offset = addr % kShadowCell;\n-  if (offset) {\n-    offset = kShadowCell - offset;\n-    if (size <= offset)\n-      return;\n-    addr += offset;\n-    size -= offset;\n-  }\n-  DCHECK_EQ(addr % 8, 0);\n+  DCHECK_EQ(addr % kShadowCell, 0);\n+  DCHECK_EQ(size % kShadowCell, 0);\n   // If a user passes some insane arguments (memset(0)),\n   // let it just crash as usual.\n   if (!IsAppMem(addr) || !IsAppMem(addr + size - 1))\n     return;\n+  RawShadow* begin = MemToShadow(addr);\n+  RawShadow* end = begin + size / kShadowCell * kShadowCnt;\n   // Don't want to touch lots of shadow memory.\n   // If a program maps 10MB stack, there is no need reset the whole range.\n-  size = (size + (kShadowCell - 1)) & ~(kShadowCell - 1);\n   // UnmapOrDie/MmapFixedNoReserve does not work on Windows.\n-  if (SANITIZER_WINDOWS || size < common_flags()->clear_shadow_mmap_threshold) {\n-    RawShadow *p = MemToShadow(addr);\n-    CHECK(IsShadowMem(p));\n-    CHECK(IsShadowMem(p + size * kShadowCnt / kShadowCell - 1));\n-    // FIXME: may overwrite a part outside the region\n-    for (uptr i = 0; i < size / kShadowCell * kShadowCnt;) {\n-      p[i++] = val;\n-      for (uptr j = 1; j < kShadowCnt; j++) p[i++] = 0;\n-    }\n-  } else {\n-    // The region is big, reset only beginning and end.\n-    const uptr kPageSize = GetPageSizeCached();\n-    RawShadow *begin = MemToShadow(addr);\n-    RawShadow *end = begin + size / kShadowCell * kShadowCnt;\n-    RawShadow *p = begin;\n-    // Set at least first kPageSize/2 to page boundary.\n-    while ((p < begin + kPageSize / kShadowSize / 2) || ((uptr)p % kPageSize)) {\n-      *p++ = val;\n-      for (uptr j = 1; j < kShadowCnt; j++) *p++ = 0;\n-    }\n-    // Reset middle part.\n-    RawShadow *p1 = p;\n-    p = RoundDown(end, kPageSize);\n-    if (!MmapFixedSuperNoReserve((uptr)p1, (uptr)p - (uptr)p1))\n+  if (SANITIZER_WINDOWS ||\n+      size <= common_flags()->clear_shadow_mmap_threshold) {\n+    ShadowSet(begin, end, val);\n+    return;\n+  }\n+  // The region is big, reset only beginning and end.\n+  const uptr kPageSize = GetPageSizeCached();\n+  // Set at least first kPageSize/2 to page boundary.\n+  RawShadow* mid1 =\n+      Min(end, reinterpret_cast<RawShadow*>(RoundUp(\n+                   reinterpret_cast<uptr>(begin) + kPageSize / 2, kPageSize)));\n+  ShadowSet(begin, mid1, val);\n+  // Reset middle part.\n+  RawShadow* mid2 = RoundDown(end, kPageSize);\n+  if (mid2 > mid1) {\n+    if (!MmapFixedSuperNoReserve((uptr)mid1, (uptr)mid2 - (uptr)mid1))\n       Die();\n-    // Set the ending.\n-    while (p < end) {\n-      *p++ = val;\n-      for (uptr j = 1; j < kShadowCnt; j++) *p++ = 0;\n-    }\n   }\n+  // Set the ending.\n+  ShadowSet(mid2, end, val);\n }\n \n-void MemoryResetRange(ThreadState *thr, uptr pc, uptr addr, uptr size) {\n-  MemoryRangeSet(thr, pc, addr, size, 0);\n+void MemoryResetRange(ThreadState* thr, uptr pc, uptr addr, uptr size) {\n+  uptr addr1 = RoundDown(addr, kShadowCell);\n+  uptr size1 = RoundUp(size + addr - addr1, kShadowCell);\n+  MemoryRangeSet(addr1, size1, Shadow::kEmpty);\n }\n \n-void MemoryRangeFreed(ThreadState *thr, uptr pc, uptr addr, uptr size) {\n-  // Processing more than 1k (4k of shadow) is expensive,\n+void MemoryRangeFreed(ThreadState* thr, uptr pc, uptr addr, uptr size) {\n+  // Callers must lock the slot to ensure synchronization with the reset.\n+  // The problem with \"freed\" memory is that it's not \"monotonic\"\n+  // with respect to bug detection: freed memory is bad to access,\n+  // but then if the heap block is reallocated later, it's good to access.\n+  // As the result a garbage \"freed\" shadow can lead to a false positive\n+  // if it happens to match a real free in the thread trace,\n+  // but the heap block was reallocated before the current memory access,\n+  // so it's still good to access. It's not the case with data races.\n+  DCHECK(thr->slot_locked);\n+  DCHECK_EQ(addr % kShadowCell, 0);\n+  size = RoundUp(size, kShadowCell);\n+  // Processing more than 1k (2k of shadow) is expensive,\n   // can cause excessive memory consumption (user does not necessary touch\n   // the whole range) and most likely unnecessary.\n-  if (size > 1024)\n-    size = 1024;\n-  CHECK_EQ(thr->is_freeing, false);\n-  thr->is_freeing = true;\n-  MemoryAccessRange(thr, pc, addr, size, true);\n-  thr->is_freeing = false;\n-  if (kCollectHistory) {\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeMop, pc);\n+  size = Min<uptr>(size, 1024);\n+  const AccessType typ = kAccessWrite | kAccessFree | kAccessSlotLocked |\n+                         kAccessCheckOnly | kAccessNoRodata;\n+  TraceMemoryAccessRange(thr, pc, addr, size, typ);\n+  RawShadow* shadow_mem = MemToShadow(addr);\n+  Shadow cur(thr->fast_state, 0, kShadowCell, typ);\n+#if TSAN_VECTORIZE\n+  const m128 access = _mm_set1_epi32(static_cast<u32>(cur.raw()));\n+  const m128 freed = _mm_setr_epi32(\n+      static_cast<u32>(Shadow::FreedMarker()),\n+      static_cast<u32>(Shadow::FreedInfo(cur.sid(), cur.epoch())), 0, 0);\n+  for (; size; size -= kShadowCell, shadow_mem += kShadowCnt) {\n+    const m128 shadow = _mm_load_si128((m128*)shadow_mem);\n+    if (UNLIKELY(CheckRaces(thr, shadow_mem, cur, shadow, access, typ)))\n+      return;\n+    _mm_store_si128((m128*)shadow_mem, freed);\n   }\n-  Shadow s(thr->fast_state);\n-  s.ClearIgnoreBit();\n-  s.MarkAsFreed();\n-  s.SetWrite(true);\n-  s.SetAddr0AndSizeLog(0, 3);\n-  MemoryRangeSet(thr, pc, addr, size, s.raw());\n-}\n-\n-void MemoryRangeImitateWrite(ThreadState *thr, uptr pc, uptr addr, uptr size) {\n-  if (kCollectHistory) {\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeMop, pc);\n+#else\n+  for (; size; size -= kShadowCell, shadow_mem += kShadowCnt) {\n+    if (UNLIKELY(CheckRaces(thr, shadow_mem, cur, 0, 0, typ)))\n+      return;\n+    StoreShadow(&shadow_mem[0], Shadow::FreedMarker());\n+    StoreShadow(&shadow_mem[1], Shadow::FreedInfo(cur.sid(), cur.epoch()));\n+    StoreShadow(&shadow_mem[2], Shadow::kEmpty);\n+    StoreShadow(&shadow_mem[3], Shadow::kEmpty);\n   }\n-  Shadow s(thr->fast_state);\n-  s.ClearIgnoreBit();\n-  s.SetWrite(true);\n-  s.SetAddr0AndSizeLog(0, 3);\n-  MemoryRangeSet(thr, pc, addr, size, s.raw());\n+#endif\n+}\n+\n+void MemoryRangeImitateWrite(ThreadState* thr, uptr pc, uptr addr, uptr size) {\n+  DCHECK_EQ(addr % kShadowCell, 0);\n+  size = RoundUp(size, kShadowCell);\n+  TraceMemoryAccessRange(thr, pc, addr, size, kAccessWrite);\n+  Shadow cur(thr->fast_state, 0, 8, kAccessWrite);\n+  MemoryRangeSet(addr, size, cur.raw());\n }\n \n-void MemoryRangeImitateWriteOrResetRange(ThreadState *thr, uptr pc, uptr addr,\n+void MemoryRangeImitateWriteOrResetRange(ThreadState* thr, uptr pc, uptr addr,\n                                          uptr size) {\n   if (thr->ignore_reads_and_writes == 0)\n     MemoryRangeImitateWrite(thr, pc, addr, size);\n   else\n     MemoryResetRange(thr, pc, addr, size);\n }\n \n-void MemoryAccessRange(ThreadState *thr, uptr pc, uptr addr, uptr size,\n-                       bool is_write) {\n-  if (size == 0)\n-    return;\n+ALWAYS_INLINE\n+bool MemoryAccessRangeOne(ThreadState* thr, RawShadow* shadow_mem, Shadow cur,\n+                          AccessType typ) {\n+  LOAD_CURRENT_SHADOW(cur, shadow_mem);\n+  if (LIKELY(ContainsSameAccess(shadow_mem, cur, shadow, access, typ)))\n+    return false;\n+  return CheckRaces(thr, shadow_mem, cur, shadow, access, typ);\n+}\n+\n+template <bool is_read>\n+NOINLINE void RestartMemoryAccessRange(ThreadState* thr, uptr pc, uptr addr,\n+                                       uptr size) {\n+  TraceSwitchPart(thr);\n+  MemoryAccessRangeT<is_read>(thr, pc, addr, size);\n+}\n \n-  RawShadow *shadow_mem = MemToShadow(addr);\n-  DPrintf2(\"#%d: MemoryAccessRange: @%p %p size=%d is_write=%d\\n\", thr->tid,\n-           (void *)pc, (void *)addr, (int)size, is_write);\n+template <bool is_read>\n+void MemoryAccessRangeT(ThreadState* thr, uptr pc, uptr addr, uptr size) {\n+  const AccessType typ =\n+      (is_read ? kAccessRead : kAccessWrite) | kAccessNoRodata;\n+  RawShadow* shadow_mem = MemToShadow(addr);\n+  DPrintf2(\"#%d: MemoryAccessRange: @%p %p size=%d is_read=%d\\n\", thr->tid,\n+           (void*)pc, (void*)addr, (int)size, is_read);\n \n #if SANITIZER_DEBUG\n   if (!IsAppMem(addr)) {\n@@ -537,65 +689,62 @@ void MemoryAccessRange(ThreadState *thr, uptr pc, uptr addr, uptr size,\n     DCHECK(IsAppMem(addr + size - 1));\n   }\n   if (!IsShadowMem(shadow_mem)) {\n-    Printf(\"Bad shadow addr %p (%zx)\\n\", shadow_mem, addr);\n+    Printf(\"Bad shadow addr %p (%zx)\\n\", static_cast<void*>(shadow_mem), addr);\n     DCHECK(IsShadowMem(shadow_mem));\n   }\n-  if (!IsShadowMem(shadow_mem + size * kShadowCnt / 8 - 1)) {\n-    Printf(\"Bad shadow addr %p (%zx)\\n\", shadow_mem + size * kShadowCnt / 8 - 1,\n+  if (!IsShadowMem(shadow_mem + size * kShadowCnt - 1)) {\n+    Printf(\"Bad shadow addr %p (%zx)\\n\",\n+           static_cast<void*>(shadow_mem + size * kShadowCnt - 1),\n            addr + size - 1);\n-    DCHECK(IsShadowMem(shadow_mem + size * kShadowCnt / 8 - 1));\n+    DCHECK(IsShadowMem(shadow_mem + size * kShadowCnt - 1));\n   }\n #endif\n \n-  if (*shadow_mem == kShadowRodata) {\n-    DCHECK(!is_write);\n-    // Access to .rodata section, no races here.\n-    // Measurements show that it can be 10-20% of all memory accesses.\n+  // Access to .rodata section, no races here.\n+  // Measurements show that it can be 10-20% of all memory accesses.\n+  // Check here once to not check for every access separately.\n+  // Note: we could (and should) do this only for the is_read case\n+  // (writes shouldn't go to .rodata). But it happens in Chromium tests:\n+  // https://bugs.chromium.org/p/chromium/issues/detail?id=1275581#c19\n+  // Details are unknown since it happens only on CI machines.\n+  if (*shadow_mem == Shadow::kRodata)\n     return;\n-  }\n \n   FastState fast_state = thr->fast_state;\n-  if (fast_state.GetIgnoreBit())\n+  if (UNLIKELY(fast_state.GetIgnoreBit()))\n     return;\n \n-  fast_state.IncrementEpoch();\n-  thr->fast_state = fast_state;\n-  TraceAddEvent(thr, fast_state, EventTypeMop, pc);\n+  if (!TryTraceMemoryAccessRange(thr, pc, addr, size, typ))\n+    return RestartMemoryAccessRange<is_read>(thr, pc, addr, size);\n \n-  bool unaligned = (addr % kShadowCell) != 0;\n-\n-  // Handle unaligned beginning, if any.\n-  for (; addr % kShadowCell && size; addr++, size--) {\n-    int const kAccessSizeLog = 0;\n-    Shadow cur(fast_state);\n-    cur.SetWrite(is_write);\n-    cur.SetAddr0AndSizeLog(addr & (kShadowCell - 1), kAccessSizeLog);\n-    MemoryAccessImpl(thr, addr, kAccessSizeLog, is_write, false, shadow_mem,\n-                     cur);\n-  }\n-  if (unaligned)\n+  if (UNLIKELY(addr % kShadowCell)) {\n+    // Handle unaligned beginning, if any.\n+    uptr size1 = Min(size, RoundUp(addr, kShadowCell) - addr);\n+    size -= size1;\n+    Shadow cur(fast_state, addr, size1, typ);\n+    if (UNLIKELY(MemoryAccessRangeOne(thr, shadow_mem, cur, typ)))\n+      return;\n     shadow_mem += kShadowCnt;\n+  }\n   // Handle middle part, if any.\n-  for (; size >= kShadowCell; addr += kShadowCell, size -= kShadowCell) {\n-    int const kAccessSizeLog = 3;\n-    Shadow cur(fast_state);\n-    cur.SetWrite(is_write);\n-    cur.SetAddr0AndSizeLog(0, kAccessSizeLog);\n-    MemoryAccessImpl(thr, addr, kAccessSizeLog, is_write, false, shadow_mem,\n-                     cur);\n-    shadow_mem += kShadowCnt;\n+  Shadow cur(fast_state, 0, kShadowCell, typ);\n+  for (; size >= kShadowCell; size -= kShadowCell, shadow_mem += kShadowCnt) {\n+    if (UNLIKELY(MemoryAccessRangeOne(thr, shadow_mem, cur, typ)))\n+      return;\n   }\n   // Handle ending, if any.\n-  for (; size; addr++, size--) {\n-    int const kAccessSizeLog = 0;\n-    Shadow cur(fast_state);\n-    cur.SetWrite(is_write);\n-    cur.SetAddr0AndSizeLog(addr & (kShadowCell - 1), kAccessSizeLog);\n-    MemoryAccessImpl(thr, addr, kAccessSizeLog, is_write, false, shadow_mem,\n-                     cur);\n+  if (UNLIKELY(size)) {\n+    Shadow cur(fast_state, 0, size, typ);\n+    if (UNLIKELY(MemoryAccessRangeOne(thr, shadow_mem, cur, typ)))\n+      return;\n   }\n }\n \n+template void MemoryAccessRangeT<true>(ThreadState* thr, uptr pc, uptr addr,\n+                                       uptr size);\n+template void MemoryAccessRangeT<false>(ThreadState* thr, uptr pc, uptr addr,\n+                                        uptr size);\n+\n }  // namespace __tsan\n \n #if !SANITIZER_GO"}, {"sha": "f848be9dd46c346e1befb269c4b6d070f64054e2", "filename": "libsanitizer/tsan/tsan_rtl_amd64.S", "status": "modified", "additions": 0, "deletions": 236, "changes": 236, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_amd64.S", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_amd64.S", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_amd64.S?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -9,242 +9,6 @@\n .section __TEXT,__text\n #endif\n \n-ASM_HIDDEN(__tsan_trace_switch)\n-.globl ASM_SYMBOL(__tsan_trace_switch_thunk)\n-ASM_SYMBOL(__tsan_trace_switch_thunk):\n-  CFI_STARTPROC\n-  _CET_ENDBR\n-  # Save scratch registers.\n-  push %rax\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rax, 0)\n-  push %rcx\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rcx, 0)\n-  push %rdx\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rdx, 0)\n-  push %rsi\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rsi, 0)\n-  push %rdi\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rdi, 0)\n-  push %r8\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%r8, 0)\n-  push %r9\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%r9, 0)\n-  push %r10\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%r10, 0)\n-  push %r11\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%r11, 0)\n-  # All XMM registers are caller-saved.\n-  sub $0x100, %rsp\n-  CFI_ADJUST_CFA_OFFSET(0x100)\n-  movdqu %xmm0, 0x0(%rsp)\n-  movdqu %xmm1, 0x10(%rsp)\n-  movdqu %xmm2, 0x20(%rsp)\n-  movdqu %xmm3, 0x30(%rsp)\n-  movdqu %xmm4, 0x40(%rsp)\n-  movdqu %xmm5, 0x50(%rsp)\n-  movdqu %xmm6, 0x60(%rsp)\n-  movdqu %xmm7, 0x70(%rsp)\n-  movdqu %xmm8, 0x80(%rsp)\n-  movdqu %xmm9, 0x90(%rsp)\n-  movdqu %xmm10, 0xa0(%rsp)\n-  movdqu %xmm11, 0xb0(%rsp)\n-  movdqu %xmm12, 0xc0(%rsp)\n-  movdqu %xmm13, 0xd0(%rsp)\n-  movdqu %xmm14, 0xe0(%rsp)\n-  movdqu %xmm15, 0xf0(%rsp)\n-  # Align stack frame.\n-  push %rbx  # non-scratch\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rbx, 0)\n-  mov %rsp, %rbx  # save current rsp\n-  CFI_DEF_CFA_REGISTER(%rbx)\n-  shr $4, %rsp  # clear 4 lsb, align to 16\n-  shl $4, %rsp\n-\n-  call ASM_SYMBOL(__tsan_trace_switch)\n-\n-  # Unalign stack frame back.\n-  mov %rbx, %rsp  # restore the original rsp\n-  CFI_DEF_CFA_REGISTER(%rsp)\n-  pop %rbx\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  # Restore scratch registers.\n-  movdqu 0x0(%rsp), %xmm0\n-  movdqu 0x10(%rsp), %xmm1\n-  movdqu 0x20(%rsp), %xmm2\n-  movdqu 0x30(%rsp), %xmm3\n-  movdqu 0x40(%rsp), %xmm4\n-  movdqu 0x50(%rsp), %xmm5\n-  movdqu 0x60(%rsp), %xmm6\n-  movdqu 0x70(%rsp), %xmm7\n-  movdqu 0x80(%rsp), %xmm8\n-  movdqu 0x90(%rsp), %xmm9\n-  movdqu 0xa0(%rsp), %xmm10\n-  movdqu 0xb0(%rsp), %xmm11\n-  movdqu 0xc0(%rsp), %xmm12\n-  movdqu 0xd0(%rsp), %xmm13\n-  movdqu 0xe0(%rsp), %xmm14\n-  movdqu 0xf0(%rsp), %xmm15\n-  add $0x100, %rsp\n-  CFI_ADJUST_CFA_OFFSET(-0x100)\n-  pop %r11\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %r10\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %r9\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %r8\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %rdi\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %rsi\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %rdx\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %rcx\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %rax\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  CFI_RESTORE(%rax)\n-  CFI_RESTORE(%rbx)\n-  CFI_RESTORE(%rcx)\n-  CFI_RESTORE(%rdx)\n-  CFI_RESTORE(%rsi)\n-  CFI_RESTORE(%rdi)\n-  CFI_RESTORE(%r8)\n-  CFI_RESTORE(%r9)\n-  CFI_RESTORE(%r10)\n-  CFI_RESTORE(%r11)\n-  ret\n-  CFI_ENDPROC\n-\n-ASM_HIDDEN(__tsan_report_race)\n-.globl ASM_SYMBOL(__tsan_report_race_thunk)\n-ASM_SYMBOL(__tsan_report_race_thunk):\n-  CFI_STARTPROC\n-  _CET_ENDBR\n-  # Save scratch registers.\n-  push %rax\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rax, 0)\n-  push %rcx\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rcx, 0)\n-  push %rdx\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rdx, 0)\n-  push %rsi\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rsi, 0)\n-  push %rdi\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rdi, 0)\n-  push %r8\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%r8, 0)\n-  push %r9\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%r9, 0)\n-  push %r10\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%r10, 0)\n-  push %r11\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%r11, 0)\n-  # All XMM registers are caller-saved.\n-  sub $0x100, %rsp\n-  CFI_ADJUST_CFA_OFFSET(0x100)\n-  movdqu %xmm0, 0x0(%rsp)\n-  movdqu %xmm1, 0x10(%rsp)\n-  movdqu %xmm2, 0x20(%rsp)\n-  movdqu %xmm3, 0x30(%rsp)\n-  movdqu %xmm4, 0x40(%rsp)\n-  movdqu %xmm5, 0x50(%rsp)\n-  movdqu %xmm6, 0x60(%rsp)\n-  movdqu %xmm7, 0x70(%rsp)\n-  movdqu %xmm8, 0x80(%rsp)\n-  movdqu %xmm9, 0x90(%rsp)\n-  movdqu %xmm10, 0xa0(%rsp)\n-  movdqu %xmm11, 0xb0(%rsp)\n-  movdqu %xmm12, 0xc0(%rsp)\n-  movdqu %xmm13, 0xd0(%rsp)\n-  movdqu %xmm14, 0xe0(%rsp)\n-  movdqu %xmm15, 0xf0(%rsp)\n-  # Align stack frame.\n-  push %rbx  # non-scratch\n-  CFI_ADJUST_CFA_OFFSET(8)\n-  CFI_REL_OFFSET(%rbx, 0)\n-  mov %rsp, %rbx  # save current rsp\n-  CFI_DEF_CFA_REGISTER(%rbx)\n-  shr $4, %rsp  # clear 4 lsb, align to 16\n-  shl $4, %rsp\n-\n-  call ASM_SYMBOL(__tsan_report_race)\n-\n-  # Unalign stack frame back.\n-  mov %rbx, %rsp  # restore the original rsp\n-  CFI_DEF_CFA_REGISTER(%rsp)\n-  pop %rbx\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  # Restore scratch registers.\n-  movdqu 0x0(%rsp), %xmm0\n-  movdqu 0x10(%rsp), %xmm1\n-  movdqu 0x20(%rsp), %xmm2\n-  movdqu 0x30(%rsp), %xmm3\n-  movdqu 0x40(%rsp), %xmm4\n-  movdqu 0x50(%rsp), %xmm5\n-  movdqu 0x60(%rsp), %xmm6\n-  movdqu 0x70(%rsp), %xmm7\n-  movdqu 0x80(%rsp), %xmm8\n-  movdqu 0x90(%rsp), %xmm9\n-  movdqu 0xa0(%rsp), %xmm10\n-  movdqu 0xb0(%rsp), %xmm11\n-  movdqu 0xc0(%rsp), %xmm12\n-  movdqu 0xd0(%rsp), %xmm13\n-  movdqu 0xe0(%rsp), %xmm14\n-  movdqu 0xf0(%rsp), %xmm15\n-  add $0x100, %rsp\n-  CFI_ADJUST_CFA_OFFSET(-0x100)\n-  pop %r11\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %r10\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %r9\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %r8\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %rdi\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %rsi\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %rdx\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %rcx\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  pop %rax\n-  CFI_ADJUST_CFA_OFFSET(-8)\n-  CFI_RESTORE(%rax)\n-  CFI_RESTORE(%rbx)\n-  CFI_RESTORE(%rcx)\n-  CFI_RESTORE(%rdx)\n-  CFI_RESTORE(%rsi)\n-  CFI_RESTORE(%rdi)\n-  CFI_RESTORE(%r8)\n-  CFI_RESTORE(%r9)\n-  CFI_RESTORE(%r10)\n-  CFI_RESTORE(%r11)\n-  ret\n-  CFI_ENDPROC\n-\n ASM_HIDDEN(__tsan_setjmp)\n #if defined(__NetBSD__)\n .comm _ZN14__interception15real___setjmp14E,8,8"}, {"sha": "2e978852ea7d379c4ec408f00cdfeb177144bbae", "filename": "libsanitizer/tsan/tsan_rtl_mutex.cpp", "status": "modified", "additions": 334, "deletions": 311, "changes": 645, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_mutex.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_mutex.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_mutex.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -23,6 +23,8 @@\n namespace __tsan {\n \n void ReportDeadlock(ThreadState *thr, uptr pc, DDReport *r);\n+void ReportDestroyLocked(ThreadState *thr, uptr pc, uptr addr,\n+                         FastState last_lock, StackID creation_stack_id);\n \n struct Callback final : public DDCallback {\n   ThreadState *thr;\n@@ -36,17 +38,17 @@ struct Callback final : public DDCallback {\n   }\n \n   StackID Unwind() override { return CurrentStackId(thr, pc); }\n-  int UniqueTid() override { return thr->unique_id; }\n+  int UniqueTid() override { return thr->tid; }\n };\n \n void DDMutexInit(ThreadState *thr, uptr pc, SyncVar *s) {\n   Callback cb(thr, pc);\n   ctx->dd->MutexInit(&cb, &s->dd);\n-  s->dd.ctx = s->GetId();\n+  s->dd.ctx = s->addr;\n }\n \n static void ReportMutexMisuse(ThreadState *thr, uptr pc, ReportType typ,\n-    uptr addr, u64 mid) {\n+                              uptr addr, StackID creation_stack_id) {\n   // In Go, these misuses are either impossible, or detected by std lib,\n   // or false positives (e.g. unlock in a different thread).\n   if (SANITIZER_GO)\n@@ -55,103 +57,102 @@ static void ReportMutexMisuse(ThreadState *thr, uptr pc, ReportType typ,\n     return;\n   ThreadRegistryLock l(&ctx->thread_registry);\n   ScopedReport rep(typ);\n-  rep.AddMutex(mid);\n+  rep.AddMutex(addr, creation_stack_id);\n   VarSizeStackTrace trace;\n   ObtainCurrentStack(thr, pc, &trace);\n   rep.AddStack(trace, true);\n   rep.AddLocation(addr, 1);\n   OutputReport(thr, rep);\n }\n \n+static void RecordMutexLock(ThreadState *thr, uptr pc, uptr addr,\n+                            StackID stack_id, bool write) {\n+  auto typ = write ? EventType::kLock : EventType::kRLock;\n+  // Note: it's important to trace before modifying mutex set\n+  // because tracing can switch trace part and we write the current\n+  // mutex set in the beginning of each part.\n+  // If we do it in the opposite order, we will write already reduced\n+  // mutex set in the beginning of the part and then trace unlock again.\n+  TraceMutexLock(thr, typ, pc, addr, stack_id);\n+  thr->mset.AddAddr(addr, stack_id, write);\n+}\n+\n+static void RecordMutexUnlock(ThreadState *thr, uptr addr) {\n+  // See the comment in RecordMutexLock re order of operations.\n+  TraceMutexUnlock(thr, addr);\n+  thr->mset.DelAddr(addr);\n+}\n+\n void MutexCreate(ThreadState *thr, uptr pc, uptr addr, u32 flagz) {\n   DPrintf(\"#%d: MutexCreate %zx flagz=0x%x\\n\", thr->tid, addr, flagz);\n-  if (!(flagz & MutexFlagLinkerInit) && IsAppMem(addr)) {\n-    CHECK(!thr->is_freeing);\n-    thr->is_freeing = true;\n+  if (!(flagz & MutexFlagLinkerInit) && pc && IsAppMem(addr))\n     MemoryAccess(thr, pc, addr, 1, kAccessWrite);\n-    thr->is_freeing = false;\n-  }\n-  SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n-  Lock l(&s->mtx);\n+  SlotLocker locker(thr);\n+  auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n   s->SetFlags(flagz & MutexCreationFlagMask);\n   // Save stack in the case the sync object was created before as atomic.\n-  if (!SANITIZER_GO && s->creation_stack_id == 0)\n+  if (!SANITIZER_GO && s->creation_stack_id == kInvalidStackID)\n     s->creation_stack_id = CurrentStackId(thr, pc);\n }\n \n void MutexDestroy(ThreadState *thr, uptr pc, uptr addr, u32 flagz) {\n   DPrintf(\"#%d: MutexDestroy %zx\\n\", thr->tid, addr);\n   bool unlock_locked = false;\n-  u64 mid = 0;\n-  u64 last_lock = 0;\n+  StackID creation_stack_id;\n+  FastState last_lock;\n   {\n-    SyncVar *s = ctx->metamap.GetSyncIfExists(addr);\n-    if (s == 0)\n+    auto s = ctx->metamap.GetSyncIfExists(addr);\n+    if (!s)\n       return;\n-    Lock l(&s->mtx);\n-    if ((flagz & MutexFlagLinkerInit) || s->IsFlagSet(MutexFlagLinkerInit) ||\n-        ((flagz & MutexFlagNotStatic) && !s->IsFlagSet(MutexFlagNotStatic))) {\n-      // Destroy is no-op for linker-initialized mutexes.\n-      return;\n-    }\n-    if (common_flags()->detect_deadlocks) {\n-      Callback cb(thr, pc);\n-      ctx->dd->MutexDestroy(&cb, &s->dd);\n-      ctx->dd->MutexInit(&cb, &s->dd);\n-    }\n-    if (flags()->report_destroy_locked && s->owner_tid != kInvalidTid &&\n-        !s->IsFlagSet(MutexFlagBroken)) {\n-      s->SetFlags(MutexFlagBroken);\n-      unlock_locked = true;\n-    }\n-    mid = s->GetId();\n-    last_lock = s->last_lock;\n-    if (!unlock_locked)\n-      s->Reset(thr->proc());  // must not reset it before the report is printed\n-  }\n-  if (unlock_locked && ShouldReport(thr, ReportTypeMutexDestroyLocked)) {\n-    ThreadRegistryLock l(&ctx->thread_registry);\n-    ScopedReport rep(ReportTypeMutexDestroyLocked);\n-    rep.AddMutex(mid);\n-    VarSizeStackTrace trace;\n-    ObtainCurrentStack(thr, pc, &trace);\n-    rep.AddStack(trace, true);\n-    FastState last(last_lock);\n-    RestoreStack(last.tid(), last.epoch(), &trace, 0);\n-    rep.AddStack(trace, true);\n-    rep.AddLocation(addr, 1);\n-    OutputReport(thr, rep);\n-\n-    SyncVar *s = ctx->metamap.GetSyncIfExists(addr);\n-    if (s != 0) {\n-      Lock l(&s->mtx);\n-      s->Reset(thr->proc());\n+    SlotLocker locker(thr);\n+    {\n+      Lock lock(&s->mtx);\n+      creation_stack_id = s->creation_stack_id;\n+      last_lock = s->last_lock;\n+      if ((flagz & MutexFlagLinkerInit) || s->IsFlagSet(MutexFlagLinkerInit) ||\n+          ((flagz & MutexFlagNotStatic) && !s->IsFlagSet(MutexFlagNotStatic))) {\n+        // Destroy is no-op for linker-initialized mutexes.\n+        return;\n+      }\n+      if (common_flags()->detect_deadlocks) {\n+        Callback cb(thr, pc);\n+        ctx->dd->MutexDestroy(&cb, &s->dd);\n+        ctx->dd->MutexInit(&cb, &s->dd);\n+      }\n+      if (flags()->report_destroy_locked && s->owner_tid != kInvalidTid &&\n+          !s->IsFlagSet(MutexFlagBroken)) {\n+        s->SetFlags(MutexFlagBroken);\n+        unlock_locked = true;\n+      }\n+      s->Reset();\n     }\n+    // Imitate a memory write to catch unlock-destroy races.\n+    if (pc && IsAppMem(addr))\n+      MemoryAccess(thr, pc, addr, 1,\n+                   kAccessWrite | kAccessFree | kAccessSlotLocked);\n   }\n-  thr->mset.Remove(mid);\n-  // Imitate a memory write to catch unlock-destroy races.\n-  // Do this outside of sync mutex, because it can report a race which locks\n-  // sync mutexes.\n-  if (IsAppMem(addr))\n-    MemoryAccess(thr, pc, addr, 1, kAccessWrite | kAccessFree);\n+  if (unlock_locked && ShouldReport(thr, ReportTypeMutexDestroyLocked))\n+    ReportDestroyLocked(thr, pc, addr, last_lock, creation_stack_id);\n+  thr->mset.DelAddr(addr, true);\n   // s will be destroyed and freed in MetaMap::FreeBlock.\n }\n \n void MutexPreLock(ThreadState *thr, uptr pc, uptr addr, u32 flagz) {\n   DPrintf(\"#%d: MutexPreLock %zx flagz=0x%x\\n\", thr->tid, addr, flagz);\n-  if (!(flagz & MutexFlagTryLock) && common_flags()->detect_deadlocks) {\n-    SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n-    {\n-      ReadLock l(&s->mtx);\n-      s->UpdateFlags(flagz);\n-      if (s->owner_tid != thr->tid) {\n-        Callback cb(thr, pc);\n-        ctx->dd->MutexBeforeLock(&cb, &s->dd, true);\n-      }\n-    }\n-    Callback cb(thr, pc);\n-    ReportDeadlock(thr, pc, ctx->dd->GetReport(&cb));\n+  if (flagz & MutexFlagTryLock)\n+    return;\n+  if (!common_flags()->detect_deadlocks)\n+    return;\n+  Callback cb(thr, pc);\n+  {\n+    SlotLocker locker(thr);\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n+    ReadLock lock(&s->mtx);\n+    s->UpdateFlags(flagz);\n+    if (s->owner_tid != thr->tid)\n+      ctx->dd->MutexBeforeLock(&cb, &s->dd, true);\n   }\n+  ReportDeadlock(thr, pc, ctx->dd->GetReport(&cb));\n }\n \n void MutexPostLock(ThreadState *thr, uptr pc, uptr addr, u32 flagz, int rec) {\n@@ -161,48 +162,51 @@ void MutexPostLock(ThreadState *thr, uptr pc, uptr addr, u32 flagz, int rec) {\n     CHECK_GT(rec, 0);\n   else\n     rec = 1;\n-  if (IsAppMem(addr))\n+  if (pc && IsAppMem(addr))\n     MemoryAccess(thr, pc, addr, 1, kAccessRead | kAccessAtomic);\n-  u64 mid = 0;\n+  bool report_double_lock = false;\n   bool pre_lock = false;\n   bool first = false;\n-  bool report_double_lock = false;\n+  StackID creation_stack_id = kInvalidStackID;\n   {\n-    SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n-    Lock l(&s->mtx);\n-    s->UpdateFlags(flagz);\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeLock, s->GetId());\n-    if (s->owner_tid == kInvalidTid) {\n-      CHECK_EQ(s->recursion, 0);\n-      s->owner_tid = thr->tid;\n-      s->last_lock = thr->fast_state.raw();\n-    } else if (s->owner_tid == thr->tid) {\n-      CHECK_GT(s->recursion, 0);\n-    } else if (flags()->report_mutex_bugs && !s->IsFlagSet(MutexFlagBroken)) {\n-      s->SetFlags(MutexFlagBroken);\n-      report_double_lock = true;\n-    }\n-    first = s->recursion == 0;\n-    s->recursion += rec;\n-    if (first) {\n-      AcquireImpl(thr, pc, &s->clock);\n-      AcquireImpl(thr, pc, &s->read_clock);\n-    } else if (!s->IsFlagSet(MutexFlagWriteReentrant)) {\n-    }\n-    thr->mset.Add(s->GetId(), true, thr->fast_state.epoch());\n-    if (first && common_flags()->detect_deadlocks) {\n-      pre_lock =\n-          (flagz & MutexFlagDoPreLockOnPostLock) && !(flagz & MutexFlagTryLock);\n-      Callback cb(thr, pc);\n-      if (pre_lock)\n-        ctx->dd->MutexBeforeLock(&cb, &s->dd, true);\n-      ctx->dd->MutexAfterLock(&cb, &s->dd, true, flagz & MutexFlagTryLock);\n+    SlotLocker locker(thr);\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n+    creation_stack_id = s->creation_stack_id;\n+    RecordMutexLock(thr, pc, addr, creation_stack_id, true);\n+    {\n+      Lock lock(&s->mtx);\n+      first = s->recursion == 0;\n+      s->UpdateFlags(flagz);\n+      if (s->owner_tid == kInvalidTid) {\n+        CHECK_EQ(s->recursion, 0);\n+        s->owner_tid = thr->tid;\n+        s->last_lock = thr->fast_state;\n+      } else if (s->owner_tid == thr->tid) {\n+        CHECK_GT(s->recursion, 0);\n+      } else if (flags()->report_mutex_bugs && !s->IsFlagSet(MutexFlagBroken)) {\n+        s->SetFlags(MutexFlagBroken);\n+        report_double_lock = true;\n+      }\n+      s->recursion += rec;\n+      if (first) {\n+        if (!thr->ignore_sync) {\n+          thr->clock.Acquire(s->clock);\n+          thr->clock.Acquire(s->read_clock);\n+        }\n+      }\n+      if (first && common_flags()->detect_deadlocks) {\n+        pre_lock = (flagz & MutexFlagDoPreLockOnPostLock) &&\n+                   !(flagz & MutexFlagTryLock);\n+        Callback cb(thr, pc);\n+        if (pre_lock)\n+          ctx->dd->MutexBeforeLock(&cb, &s->dd, true);\n+        ctx->dd->MutexAfterLock(&cb, &s->dd, true, flagz & MutexFlagTryLock);\n+      }\n     }\n-    mid = s->GetId();\n   }\n   if (report_double_lock)\n-    ReportMutexMisuse(thr, pc, ReportTypeMutexDoubleLock, addr, mid);\n+    ReportMutexMisuse(thr, pc, ReportTypeMutexDoubleLock, addr,\n+                      creation_stack_id);\n   if (first && pre_lock && common_flags()->detect_deadlocks) {\n     Callback cb(thr, pc);\n     ReportDeadlock(thr, pc, ctx->dd->GetReport(&cb));\n@@ -211,40 +215,47 @@ void MutexPostLock(ThreadState *thr, uptr pc, uptr addr, u32 flagz, int rec) {\n \n int MutexUnlock(ThreadState *thr, uptr pc, uptr addr, u32 flagz) {\n   DPrintf(\"#%d: MutexUnlock %zx flagz=0x%x\\n\", thr->tid, addr, flagz);\n-  if (IsAppMem(addr))\n+  if (pc && IsAppMem(addr))\n     MemoryAccess(thr, pc, addr, 1, kAccessRead | kAccessAtomic);\n-  u64 mid = 0;\n+  StackID creation_stack_id;\n+  RecordMutexUnlock(thr, addr);\n   bool report_bad_unlock = false;\n   int rec = 0;\n   {\n-    SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n-    Lock l(&s->mtx);\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeUnlock, s->GetId());\n-    if (!SANITIZER_GO && (s->recursion == 0 || s->owner_tid != thr->tid)) {\n-      if (flags()->report_mutex_bugs && !s->IsFlagSet(MutexFlagBroken)) {\n-        s->SetFlags(MutexFlagBroken);\n-        report_bad_unlock = true;\n-      }\n-    } else {\n-      rec = (flagz & MutexFlagRecursiveUnlock) ? s->recursion : 1;\n-      s->recursion -= rec;\n-      if (s->recursion == 0) {\n-        s->owner_tid = kInvalidTid;\n-        ReleaseStoreImpl(thr, pc, &s->clock);\n+    SlotLocker locker(thr);\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n+    bool released = false;\n+    {\n+      Lock lock(&s->mtx);\n+      creation_stack_id = s->creation_stack_id;\n+      if (!SANITIZER_GO && (s->recursion == 0 || s->owner_tid != thr->tid)) {\n+        if (flags()->report_mutex_bugs && !s->IsFlagSet(MutexFlagBroken)) {\n+          s->SetFlags(MutexFlagBroken);\n+          report_bad_unlock = true;\n+        }\n       } else {\n+        rec = (flagz & MutexFlagRecursiveUnlock) ? s->recursion : 1;\n+        s->recursion -= rec;\n+        if (s->recursion == 0) {\n+          s->owner_tid = kInvalidTid;\n+          if (!thr->ignore_sync) {\n+            thr->clock.ReleaseStore(&s->clock);\n+            released = true;\n+          }\n+        }\n+      }\n+      if (common_flags()->detect_deadlocks && s->recursion == 0 &&\n+          !report_bad_unlock) {\n+        Callback cb(thr, pc);\n+        ctx->dd->MutexBeforeUnlock(&cb, &s->dd, true);\n       }\n     }\n-    thr->mset.Del(s->GetId(), true);\n-    if (common_flags()->detect_deadlocks && s->recursion == 0 &&\n-        !report_bad_unlock) {\n-      Callback cb(thr, pc);\n-      ctx->dd->MutexBeforeUnlock(&cb, &s->dd, true);\n-    }\n-    mid = s->GetId();\n+    if (released)\n+      IncrementEpoch(thr);\n   }\n   if (report_bad_unlock)\n-    ReportMutexMisuse(thr, pc, ReportTypeMutexBadUnlock, addr, mid);\n+    ReportMutexMisuse(thr, pc, ReportTypeMutexBadUnlock, addr,\n+                      creation_stack_id);\n   if (common_flags()->detect_deadlocks && !report_bad_unlock) {\n     Callback cb(thr, pc);\n     ReportDeadlock(thr, pc, ctx->dd->GetReport(&cb));\n@@ -254,53 +265,56 @@ int MutexUnlock(ThreadState *thr, uptr pc, uptr addr, u32 flagz) {\n \n void MutexPreReadLock(ThreadState *thr, uptr pc, uptr addr, u32 flagz) {\n   DPrintf(\"#%d: MutexPreReadLock %zx flagz=0x%x\\n\", thr->tid, addr, flagz);\n-  if (!(flagz & MutexFlagTryLock) && common_flags()->detect_deadlocks) {\n-    {\n-      SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n-      ReadLock l(&s->mtx);\n-      s->UpdateFlags(flagz);\n-      Callback cb(thr, pc);\n-      ctx->dd->MutexBeforeLock(&cb, &s->dd, false);\n-    }\n-    Callback cb(thr, pc);\n-    ReportDeadlock(thr, pc, ctx->dd->GetReport(&cb));\n+  if ((flagz & MutexFlagTryLock) || !common_flags()->detect_deadlocks)\n+    return;\n+  Callback cb(thr, pc);\n+  {\n+    SlotLocker locker(thr);\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n+    ReadLock lock(&s->mtx);\n+    s->UpdateFlags(flagz);\n+    ctx->dd->MutexBeforeLock(&cb, &s->dd, false);\n   }\n+  ReportDeadlock(thr, pc, ctx->dd->GetReport(&cb));\n }\n \n void MutexPostReadLock(ThreadState *thr, uptr pc, uptr addr, u32 flagz) {\n   DPrintf(\"#%d: MutexPostReadLock %zx flagz=0x%x\\n\", thr->tid, addr, flagz);\n-  if (IsAppMem(addr))\n+  if (pc && IsAppMem(addr))\n     MemoryAccess(thr, pc, addr, 1, kAccessRead | kAccessAtomic);\n-  u64 mid = 0;\n   bool report_bad_lock = false;\n   bool pre_lock = false;\n+  StackID creation_stack_id = kInvalidStackID;\n   {\n-    SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n-    ReadLock l(&s->mtx);\n-    s->UpdateFlags(flagz);\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeRLock, s->GetId());\n-    if (s->owner_tid != kInvalidTid) {\n-      if (flags()->report_mutex_bugs && !s->IsFlagSet(MutexFlagBroken)) {\n-        s->SetFlags(MutexFlagBroken);\n-        report_bad_lock = true;\n+    SlotLocker locker(thr);\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n+    creation_stack_id = s->creation_stack_id;\n+    RecordMutexLock(thr, pc, addr, creation_stack_id, false);\n+    {\n+      ReadLock lock(&s->mtx);\n+      s->UpdateFlags(flagz);\n+      if (s->owner_tid != kInvalidTid) {\n+        if (flags()->report_mutex_bugs && !s->IsFlagSet(MutexFlagBroken)) {\n+          s->SetFlags(MutexFlagBroken);\n+          report_bad_lock = true;\n+        }\n+      }\n+      if (!thr->ignore_sync)\n+        thr->clock.Acquire(s->clock);\n+      s->last_lock = thr->fast_state;\n+      if (common_flags()->detect_deadlocks) {\n+        pre_lock = (flagz & MutexFlagDoPreLockOnPostLock) &&\n+                   !(flagz & MutexFlagTryLock);\n+        Callback cb(thr, pc);\n+        if (pre_lock)\n+          ctx->dd->MutexBeforeLock(&cb, &s->dd, false);\n+        ctx->dd->MutexAfterLock(&cb, &s->dd, false, flagz & MutexFlagTryLock);\n       }\n     }\n-    AcquireImpl(thr, pc, &s->clock);\n-    s->last_lock = thr->fast_state.raw();\n-    thr->mset.Add(s->GetId(), false, thr->fast_state.epoch());\n-    if (common_flags()->detect_deadlocks) {\n-      pre_lock =\n-          (flagz & MutexFlagDoPreLockOnPostLock) && !(flagz & MutexFlagTryLock);\n-      Callback cb(thr, pc);\n-      if (pre_lock)\n-        ctx->dd->MutexBeforeLock(&cb, &s->dd, false);\n-      ctx->dd->MutexAfterLock(&cb, &s->dd, false, flagz & MutexFlagTryLock);\n-    }\n-    mid = s->GetId();\n   }\n   if (report_bad_lock)\n-    ReportMutexMisuse(thr, pc, ReportTypeMutexBadReadLock, addr, mid);\n+    ReportMutexMisuse(thr, pc, ReportTypeMutexBadReadLock, addr,\n+                      creation_stack_id);\n   if (pre_lock  && common_flags()->detect_deadlocks) {\n     Callback cb(thr, pc);\n     ReportDeadlock(thr, pc, ctx->dd->GetReport(&cb));\n@@ -309,31 +323,39 @@ void MutexPostReadLock(ThreadState *thr, uptr pc, uptr addr, u32 flagz) {\n \n void MutexReadUnlock(ThreadState *thr, uptr pc, uptr addr) {\n   DPrintf(\"#%d: MutexReadUnlock %zx\\n\", thr->tid, addr);\n-  if (IsAppMem(addr))\n+  if (pc && IsAppMem(addr))\n     MemoryAccess(thr, pc, addr, 1, kAccessRead | kAccessAtomic);\n-  u64 mid = 0;\n+  RecordMutexUnlock(thr, addr);\n+  StackID creation_stack_id;\n   bool report_bad_unlock = false;\n   {\n-    SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n-    Lock l(&s->mtx);\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeRUnlock, s->GetId());\n-    if (s->owner_tid != kInvalidTid) {\n-      if (flags()->report_mutex_bugs && !s->IsFlagSet(MutexFlagBroken)) {\n-        s->SetFlags(MutexFlagBroken);\n-        report_bad_unlock = true;\n+    SlotLocker locker(thr);\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n+    bool released = false;\n+    {\n+      Lock lock(&s->mtx);\n+      creation_stack_id = s->creation_stack_id;\n+      if (s->owner_tid != kInvalidTid) {\n+        if (flags()->report_mutex_bugs && !s->IsFlagSet(MutexFlagBroken)) {\n+          s->SetFlags(MutexFlagBroken);\n+          report_bad_unlock = true;\n+        }\n+      }\n+      if (!thr->ignore_sync) {\n+        thr->clock.Release(&s->read_clock);\n+        released = true;\n+      }\n+      if (common_flags()->detect_deadlocks && s->recursion == 0) {\n+        Callback cb(thr, pc);\n+        ctx->dd->MutexBeforeUnlock(&cb, &s->dd, false);\n       }\n     }\n-    ReleaseImpl(thr, pc, &s->read_clock);\n-    if (common_flags()->detect_deadlocks && s->recursion == 0) {\n-      Callback cb(thr, pc);\n-      ctx->dd->MutexBeforeUnlock(&cb, &s->dd, false);\n-    }\n-    mid = s->GetId();\n+    if (released)\n+      IncrementEpoch(thr);\n   }\n-  thr->mset.Del(mid, false);\n   if (report_bad_unlock)\n-    ReportMutexMisuse(thr, pc, ReportTypeMutexBadReadUnlock, addr, mid);\n+    ReportMutexMisuse(thr, pc, ReportTypeMutexBadReadUnlock, addr,\n+                      creation_stack_id);\n   if (common_flags()->detect_deadlocks) {\n     Callback cb(thr, pc);\n     ReportDeadlock(thr, pc, ctx->dd->GetReport(&cb));\n@@ -342,44 +364,52 @@ void MutexReadUnlock(ThreadState *thr, uptr pc, uptr addr) {\n \n void MutexReadOrWriteUnlock(ThreadState *thr, uptr pc, uptr addr) {\n   DPrintf(\"#%d: MutexReadOrWriteUnlock %zx\\n\", thr->tid, addr);\n-  if (IsAppMem(addr))\n+  if (pc && IsAppMem(addr))\n     MemoryAccess(thr, pc, addr, 1, kAccessRead | kAccessAtomic);\n-  u64 mid = 0;\n+  RecordMutexUnlock(thr, addr);\n+  StackID creation_stack_id;\n   bool report_bad_unlock = false;\n+  bool write = true;\n   {\n-    SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n-    Lock l(&s->mtx);\n-    bool write = true;\n-    if (s->owner_tid == kInvalidTid) {\n-      // Seems to be read unlock.\n-      write = false;\n-      thr->fast_state.IncrementEpoch();\n-      TraceAddEvent(thr, thr->fast_state, EventTypeRUnlock, s->GetId());\n-      ReleaseImpl(thr, pc, &s->read_clock);\n-    } else if (s->owner_tid == thr->tid) {\n-      // Seems to be write unlock.\n-      thr->fast_state.IncrementEpoch();\n-      TraceAddEvent(thr, thr->fast_state, EventTypeUnlock, s->GetId());\n-      CHECK_GT(s->recursion, 0);\n-      s->recursion--;\n-      if (s->recursion == 0) {\n-        s->owner_tid = kInvalidTid;\n-        ReleaseStoreImpl(thr, pc, &s->clock);\n-      } else {\n+    SlotLocker locker(thr);\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n+    bool released = false;\n+    {\n+      Lock lock(&s->mtx);\n+      creation_stack_id = s->creation_stack_id;\n+      if (s->owner_tid == kInvalidTid) {\n+        // Seems to be read unlock.\n+        write = false;\n+        if (!thr->ignore_sync) {\n+          thr->clock.Release(&s->read_clock);\n+          released = true;\n+        }\n+      } else if (s->owner_tid == thr->tid) {\n+        // Seems to be write unlock.\n+        CHECK_GT(s->recursion, 0);\n+        s->recursion--;\n+        if (s->recursion == 0) {\n+          s->owner_tid = kInvalidTid;\n+          if (!thr->ignore_sync) {\n+            thr->clock.ReleaseStore(&s->clock);\n+            released = true;\n+          }\n+        }\n+      } else if (!s->IsFlagSet(MutexFlagBroken)) {\n+        s->SetFlags(MutexFlagBroken);\n+        report_bad_unlock = true;\n+      }\n+      if (common_flags()->detect_deadlocks && s->recursion == 0) {\n+        Callback cb(thr, pc);\n+        ctx->dd->MutexBeforeUnlock(&cb, &s->dd, write);\n       }\n-    } else if (!s->IsFlagSet(MutexFlagBroken)) {\n-      s->SetFlags(MutexFlagBroken);\n-      report_bad_unlock = true;\n-    }\n-    thr->mset.Del(s->GetId(), write);\n-    if (common_flags()->detect_deadlocks && s->recursion == 0) {\n-      Callback cb(thr, pc);\n-      ctx->dd->MutexBeforeUnlock(&cb, &s->dd, write);\n     }\n-    mid = s->GetId();\n+    if (released)\n+      IncrementEpoch(thr);\n   }\n   if (report_bad_unlock)\n-    ReportMutexMisuse(thr, pc, ReportTypeMutexBadUnlock, addr, mid);\n+    ReportMutexMisuse(thr, pc, ReportTypeMutexBadUnlock, addr,\n+                      creation_stack_id);\n   if (common_flags()->detect_deadlocks) {\n     Callback cb(thr, pc);\n     ReportDeadlock(thr, pc, ctx->dd->GetReport(&cb));\n@@ -388,159 +418,128 @@ void MutexReadOrWriteUnlock(ThreadState *thr, uptr pc, uptr addr) {\n \n void MutexRepair(ThreadState *thr, uptr pc, uptr addr) {\n   DPrintf(\"#%d: MutexRepair %zx\\n\", thr->tid, addr);\n-  SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n-  Lock l(&s->mtx);\n+  SlotLocker locker(thr);\n+  auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n+  Lock lock(&s->mtx);\n   s->owner_tid = kInvalidTid;\n   s->recursion = 0;\n }\n \n void MutexInvalidAccess(ThreadState *thr, uptr pc, uptr addr) {\n   DPrintf(\"#%d: MutexInvalidAccess %zx\\n\", thr->tid, addr);\n-  SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n-  ReportMutexMisuse(thr, pc, ReportTypeMutexInvalidAccess, addr, s->GetId());\n+  StackID creation_stack_id = kInvalidStackID;\n+  {\n+    SlotLocker locker(thr);\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, true);\n+    if (s)\n+      creation_stack_id = s->creation_stack_id;\n+  }\n+  ReportMutexMisuse(thr, pc, ReportTypeMutexInvalidAccess, addr,\n+                    creation_stack_id);\n }\n \n void Acquire(ThreadState *thr, uptr pc, uptr addr) {\n   DPrintf(\"#%d: Acquire %zx\\n\", thr->tid, addr);\n   if (thr->ignore_sync)\n     return;\n-  SyncVar *s = ctx->metamap.GetSyncIfExists(addr);\n+  auto s = ctx->metamap.GetSyncIfExists(addr);\n   if (!s)\n     return;\n-  ReadLock l(&s->mtx);\n-  AcquireImpl(thr, pc, &s->clock);\n-}\n-\n-static void UpdateClockCallback(ThreadContextBase *tctx_base, void *arg) {\n-  ThreadState *thr = reinterpret_cast<ThreadState*>(arg);\n-  ThreadContext *tctx = static_cast<ThreadContext*>(tctx_base);\n-  u64 epoch = tctx->epoch1;\n-  if (tctx->status == ThreadStatusRunning) {\n-    epoch = tctx->thr->fast_state.epoch();\n-    tctx->thr->clock.NoteGlobalAcquire(epoch);\n-  }\n-  thr->clock.set(&thr->proc()->clock_cache, tctx->tid, epoch);\n+  SlotLocker locker(thr);\n+  if (!s->clock)\n+    return;\n+  ReadLock lock(&s->mtx);\n+  thr->clock.Acquire(s->clock);\n }\n \n void AcquireGlobal(ThreadState *thr) {\n   DPrintf(\"#%d: AcquireGlobal\\n\", thr->tid);\n   if (thr->ignore_sync)\n     return;\n-  ThreadRegistryLock l(&ctx->thread_registry);\n-  ctx->thread_registry.RunCallbackForEachThreadLocked(UpdateClockCallback, thr);\n-}\n-\n-void ReleaseStoreAcquire(ThreadState *thr, uptr pc, uptr addr) {\n-  DPrintf(\"#%d: ReleaseStoreAcquire %zx\\n\", thr->tid, addr);\n-  if (thr->ignore_sync)\n-    return;\n-  SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, false);\n-  Lock l(&s->mtx);\n-  thr->fast_state.IncrementEpoch();\n-  // Can't increment epoch w/o writing to the trace as well.\n-  TraceAddEvent(thr, thr->fast_state, EventTypeMop, 0);\n-  ReleaseStoreAcquireImpl(thr, pc, &s->clock);\n+  SlotLocker locker(thr);\n+  for (auto &slot : ctx->slots) thr->clock.Set(slot.sid, slot.epoch());\n }\n \n void Release(ThreadState *thr, uptr pc, uptr addr) {\n   DPrintf(\"#%d: Release %zx\\n\", thr->tid, addr);\n   if (thr->ignore_sync)\n     return;\n-  SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, false);\n-  Lock l(&s->mtx);\n-  thr->fast_state.IncrementEpoch();\n-  // Can't increment epoch w/o writing to the trace as well.\n-  TraceAddEvent(thr, thr->fast_state, EventTypeMop, 0);\n-  ReleaseImpl(thr, pc, &s->clock);\n+  SlotLocker locker(thr);\n+  {\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, false);\n+    Lock lock(&s->mtx);\n+    thr->clock.Release(&s->clock);\n+  }\n+  IncrementEpoch(thr);\n }\n \n void ReleaseStore(ThreadState *thr, uptr pc, uptr addr) {\n   DPrintf(\"#%d: ReleaseStore %zx\\n\", thr->tid, addr);\n   if (thr->ignore_sync)\n     return;\n-  SyncVar *s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, false);\n-  Lock l(&s->mtx);\n-  thr->fast_state.IncrementEpoch();\n-  // Can't increment epoch w/o writing to the trace as well.\n-  TraceAddEvent(thr, thr->fast_state, EventTypeMop, 0);\n-  ReleaseStoreImpl(thr, pc, &s->clock);\n-}\n-\n-#if !SANITIZER_GO\n-static void UpdateSleepClockCallback(ThreadContextBase *tctx_base, void *arg) {\n-  ThreadState *thr = reinterpret_cast<ThreadState*>(arg);\n-  ThreadContext *tctx = static_cast<ThreadContext*>(tctx_base);\n-  u64 epoch = tctx->epoch1;\n-  if (tctx->status == ThreadStatusRunning)\n-    epoch = tctx->thr->fast_state.epoch();\n-  thr->last_sleep_clock.set(&thr->proc()->clock_cache, tctx->tid, epoch);\n-}\n-\n-void AfterSleep(ThreadState *thr, uptr pc) {\n-  DPrintf(\"#%d: AfterSleep\\n\", thr->tid);\n-  if (thr->ignore_sync)\n-    return;\n-  thr->last_sleep_stack_id = CurrentStackId(thr, pc);\n-  ThreadRegistryLock l(&ctx->thread_registry);\n-  ctx->thread_registry.RunCallbackForEachThreadLocked(UpdateSleepClockCallback,\n-                                                      thr);\n-}\n-#endif\n-\n-void AcquireImpl(ThreadState *thr, uptr pc, SyncClock *c) {\n-  if (thr->ignore_sync)\n-    return;\n-  thr->clock.set(thr->fast_state.epoch());\n-  thr->clock.acquire(&thr->proc()->clock_cache, c);\n-}\n-\n-void ReleaseStoreAcquireImpl(ThreadState *thr, uptr pc, SyncClock *c) {\n-  if (thr->ignore_sync)\n-    return;\n-  thr->clock.set(thr->fast_state.epoch());\n-  thr->fast_synch_epoch = thr->fast_state.epoch();\n-  thr->clock.releaseStoreAcquire(&thr->proc()->clock_cache, c);\n+  SlotLocker locker(thr);\n+  {\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, false);\n+    Lock lock(&s->mtx);\n+    thr->clock.ReleaseStore(&s->clock);\n+  }\n+  IncrementEpoch(thr);\n }\n \n-void ReleaseImpl(ThreadState *thr, uptr pc, SyncClock *c) {\n+void ReleaseStoreAcquire(ThreadState *thr, uptr pc, uptr addr) {\n+  DPrintf(\"#%d: ReleaseStoreAcquire %zx\\n\", thr->tid, addr);\n   if (thr->ignore_sync)\n     return;\n-  thr->clock.set(thr->fast_state.epoch());\n-  thr->fast_synch_epoch = thr->fast_state.epoch();\n-  thr->clock.release(&thr->proc()->clock_cache, c);\n+  SlotLocker locker(thr);\n+  {\n+    auto s = ctx->metamap.GetSyncOrCreate(thr, pc, addr, false);\n+    Lock lock(&s->mtx);\n+    thr->clock.ReleaseStoreAcquire(&s->clock);\n+  }\n+  IncrementEpoch(thr);\n }\n \n-void ReleaseStoreImpl(ThreadState *thr, uptr pc, SyncClock *c) {\n-  if (thr->ignore_sync)\n-    return;\n-  thr->clock.set(thr->fast_state.epoch());\n-  thr->fast_synch_epoch = thr->fast_state.epoch();\n-  thr->clock.ReleaseStore(&thr->proc()->clock_cache, c);\n+void IncrementEpoch(ThreadState *thr) {\n+  DCHECK(!thr->ignore_sync);\n+  DCHECK(thr->slot_locked);\n+  Epoch epoch = EpochInc(thr->fast_state.epoch());\n+  if (!EpochOverflow(epoch)) {\n+    Sid sid = thr->fast_state.sid();\n+    thr->clock.Set(sid, epoch);\n+    thr->fast_state.SetEpoch(epoch);\n+    thr->slot->SetEpoch(epoch);\n+    TraceTime(thr);\n+  }\n }\n \n-void AcquireReleaseImpl(ThreadState *thr, uptr pc, SyncClock *c) {\n+#if !SANITIZER_GO\n+void AfterSleep(ThreadState *thr, uptr pc) {\n+  DPrintf(\"#%d: AfterSleep\\n\", thr->tid);\n   if (thr->ignore_sync)\n     return;\n-  thr->clock.set(thr->fast_state.epoch());\n-  thr->fast_synch_epoch = thr->fast_state.epoch();\n-  thr->clock.acq_rel(&thr->proc()->clock_cache, c);\n+  thr->last_sleep_stack_id = CurrentStackId(thr, pc);\n+  thr->last_sleep_clock.Reset();\n+  SlotLocker locker(thr);\n+  for (auto &slot : ctx->slots)\n+    thr->last_sleep_clock.Set(slot.sid, slot.epoch());\n }\n+#endif\n \n void ReportDeadlock(ThreadState *thr, uptr pc, DDReport *r) {\n   if (r == 0 || !ShouldReport(thr, ReportTypeDeadlock))\n     return;\n   ThreadRegistryLock l(&ctx->thread_registry);\n   ScopedReport rep(ReportTypeDeadlock);\n   for (int i = 0; i < r->n; i++) {\n-    rep.AddMutex(r->loop[i].mtx_ctx0);\n+    rep.AddMutex(r->loop[i].mtx_ctx0, r->loop[i].stk[0]);\n     rep.AddUniqueTid((int)r->loop[i].thr_ctx);\n     rep.AddThread((int)r->loop[i].thr_ctx);\n   }\n   uptr dummy_pc = 0x42;\n   for (int i = 0; i < r->n; i++) {\n     for (int j = 0; j < (flags()->second_deadlock_stack ? 2 : 1); j++) {\n       u32 stk = r->loop[i].stk[j];\n-      if (stk && stk != 0xffffffff) {\n+      if (stk && stk != kInvalidStackID) {\n         rep.AddStack(StackDepotGet(stk), true);\n       } else {\n         // Sometimes we fail to extract the stack trace (FIXME: investigate),\n@@ -552,4 +551,28 @@ void ReportDeadlock(ThreadState *thr, uptr pc, DDReport *r) {\n   OutputReport(thr, rep);\n }\n \n+void ReportDestroyLocked(ThreadState *thr, uptr pc, uptr addr,\n+                         FastState last_lock, StackID creation_stack_id) {\n+  // We need to lock the slot during RestoreStack because it protects\n+  // the slot journal.\n+  Lock slot_lock(&ctx->slots[static_cast<uptr>(last_lock.sid())].mtx);\n+  ThreadRegistryLock l0(&ctx->thread_registry);\n+  Lock slots_lock(&ctx->slot_mtx);\n+  ScopedReport rep(ReportTypeMutexDestroyLocked);\n+  rep.AddMutex(addr, creation_stack_id);\n+  VarSizeStackTrace trace;\n+  ObtainCurrentStack(thr, pc, &trace);\n+  rep.AddStack(trace, true);\n+\n+  Tid tid;\n+  DynamicMutexSet mset;\n+  uptr tag;\n+  if (!RestoreStack(EventType::kLock, last_lock.sid(), last_lock.epoch(), addr,\n+                    0, kAccessWrite, &tid, &trace, mset, &tag))\n+    return;\n+  rep.AddStack(trace, true);\n+  rep.AddLocation(addr, 1);\n+  OutputReport(thr, rep);\n+}\n+\n }  // namespace __tsan"}, {"sha": "8285e21aa1ec7a797dfcf4840ee5a7851106b497", "filename": "libsanitizer/tsan/tsan_rtl_ppc64.S", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_ppc64.S", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_ppc64.S", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_ppc64.S?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -1,6 +1,5 @@\n #include \"tsan_ppc_regs.h\"\n \n-        .machine altivec\n         .section .text\n         .hidden __tsan_setjmp\n         .globl _setjmp"}, {"sha": "5acc3967208e38cf1000334cc7f1f3c03ecd6562", "filename": "libsanitizer/tsan/tsan_rtl_proc.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_proc.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_proc.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_proc.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -35,7 +35,6 @@ void ProcDestroy(Processor *proc) {\n #if !SANITIZER_GO\n   AllocatorProcFinish(proc);\n #endif\n-  ctx->clock_alloc.FlushCache(&proc->clock_cache);\n   ctx->metamap.OnProcIdle(proc);\n   if (common_flags()->detect_deadlocks)\n      ctx->dd->DestroyPhysicalThread(proc->dd_pt);"}, {"sha": "4cf8816489df44251738fae1c47016dcab4fc13d", "filename": "libsanitizer/tsan/tsan_rtl_report.cpp", "status": "modified", "additions": 134, "deletions": 235, "changes": 369, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_report.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_report.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_report.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -175,22 +175,26 @@ void ScopedReportBase::AddStack(StackTrace stack, bool suppressable) {\n }\n \n void ScopedReportBase::AddMemoryAccess(uptr addr, uptr external_tag, Shadow s,\n-                                       StackTrace stack, const MutexSet *mset) {\n+                                       Tid tid, StackTrace stack,\n+                                       const MutexSet *mset) {\n+  uptr addr0, size;\n+  AccessType typ;\n+  s.GetAccess(&addr0, &size, &typ);\n   auto *mop = New<ReportMop>();\n   rep_->mops.PushBack(mop);\n-  mop->tid = s.tid();\n-  mop->addr = addr + s.addr0();\n-  mop->size = s.size();\n-  mop->write = s.IsWrite();\n-  mop->atomic = s.IsAtomic();\n+  mop->tid = tid;\n+  mop->addr = addr + addr0;\n+  mop->size = size;\n+  mop->write = !(typ & kAccessRead);\n+  mop->atomic = typ & kAccessAtomic;\n   mop->stack = SymbolizeStack(stack);\n   mop->external_tag = external_tag;\n   if (mop->stack)\n     mop->stack->suppressable = true;\n   for (uptr i = 0; i < mset->Size(); i++) {\n     MutexSet::Desc d = mset->Get(i);\n-    u64 mid = this->AddMutex(d.id);\n-    ReportMopMutex mtx = {mid, d.write};\n+    int id = this->AddMutex(d.addr, d.stack_id);\n+    ReportMopMutex mtx = {id, d.write};\n     mop->mset.PushBack(mtx);\n   }\n }\n@@ -219,18 +223,6 @@ void ScopedReportBase::AddThread(const ThreadContext *tctx, bool suppressable) {\n }\n \n #if !SANITIZER_GO\n-static bool FindThreadByUidLockedCallback(ThreadContextBase *tctx, void *arg) {\n-  int unique_id = *(int *)arg;\n-  return tctx->unique_id == (u32)unique_id;\n-}\n-\n-static ThreadContext *FindThreadByUidLocked(Tid unique_id) {\n-  ctx->thread_registry.CheckLocked();\n-  return static_cast<ThreadContext *>(\n-      ctx->thread_registry.FindThreadContextLocked(\n-          FindThreadByUidLockedCallback, &unique_id));\n-}\n-\n static ThreadContext *FindThreadByTidLocked(Tid tid) {\n   ctx->thread_registry.CheckLocked();\n   return static_cast<ThreadContext *>(\n@@ -262,55 +254,24 @@ ThreadContext *IsThreadStackOrTls(uptr addr, bool *is_stack) {\n }\n #endif\n \n-void ScopedReportBase::AddThread(Tid unique_tid, bool suppressable) {\n+void ScopedReportBase::AddThread(Tid tid, bool suppressable) {\n #if !SANITIZER_GO\n-  if (const ThreadContext *tctx = FindThreadByUidLocked(unique_tid))\n+  if (const ThreadContext *tctx = FindThreadByTidLocked(tid))\n     AddThread(tctx, suppressable);\n #endif\n }\n \n-void ScopedReportBase::AddMutex(const SyncVar *s) {\n-  for (uptr i = 0; i < rep_->mutexes.Size(); i++) {\n-    if (rep_->mutexes[i]->id == s->uid)\n-      return;\n-  }\n-  auto *rm = New<ReportMutex>();\n-  rep_->mutexes.PushBack(rm);\n-  rm->id = s->uid;\n-  rm->addr = s->addr;\n-  rm->destroyed = false;\n-  rm->stack = SymbolizeStackId(s->creation_stack_id);\n-}\n-\n-u64 ScopedReportBase::AddMutex(u64 id) {\n-  u64 uid = 0;\n-  u64 mid = id;\n-  uptr addr = SyncVar::SplitId(id, &uid);\n-  SyncVar *s = ctx->metamap.GetSyncIfExists(addr);\n-  // Check that the mutex is still alive.\n-  // Another mutex can be created at the same address,\n-  // so check uid as well.\n-  if (s && s->CheckId(uid)) {\n-    Lock l(&s->mtx);\n-    mid = s->uid;\n-    AddMutex(s);\n-  } else {\n-    AddDeadMutex(id);\n-  }\n-  return mid;\n-}\n-\n-void ScopedReportBase::AddDeadMutex(u64 id) {\n+int ScopedReportBase::AddMutex(uptr addr, StackID creation_stack_id) {\n   for (uptr i = 0; i < rep_->mutexes.Size(); i++) {\n-    if (rep_->mutexes[i]->id == id)\n-      return;\n+    if (rep_->mutexes[i]->addr == addr)\n+      return rep_->mutexes[i]->id;\n   }\n   auto *rm = New<ReportMutex>();\n   rep_->mutexes.PushBack(rm);\n-  rm->id = id;\n-  rm->addr = 0;\n-  rm->destroyed = true;\n-  rm->stack = 0;\n+  rm->id = rep_->mutexes.Size() - 1;\n+  rm->addr = addr;\n+  rm->stack = SymbolizeStackId(creation_stack_id);\n+  return rm->id;\n }\n \n void ScopedReportBase::AddLocation(uptr addr, uptr size) {\n@@ -327,7 +288,7 @@ void ScopedReportBase::AddLocation(uptr addr, uptr size) {\n     loc->tid = creat_tid;\n     loc->stack = SymbolizeStackId(creat_stack);\n     rep_->locs.PushBack(loc);\n-    ThreadContext *tctx = FindThreadByUidLocked(creat_tid);\n+    ThreadContext *tctx = FindThreadByTidLocked(creat_tid);\n     if (tctx)\n       AddThread(tctx);\n     return;\n@@ -343,16 +304,15 @@ void ScopedReportBase::AddLocation(uptr addr, uptr size) {\n   if (!b)\n     b = JavaHeapBlock(addr, &block_begin);\n   if (b != 0) {\n-    ThreadContext *tctx = FindThreadByTidLocked(b->tid);\n     auto *loc = New<ReportLocation>();\n     loc->type = ReportLocationHeap;\n-    loc->heap_chunk_start = (uptr)allocator()->GetBlockBegin((void *)addr);\n+    loc->heap_chunk_start = block_begin;\n     loc->heap_chunk_size = b->siz;\n     loc->external_tag = b->tag;\n-    loc->tid = tctx ? tctx->tid : b->tid;\n+    loc->tid = b->tid;\n     loc->stack = SymbolizeStackId(b->stk);\n     rep_->locs.PushBack(loc);\n-    if (tctx)\n+    if (ThreadContext *tctx = FindThreadByTidLocked(b->tid))\n       AddThread(tctx);\n     return;\n   }\n@@ -380,78 +340,15 @@ void ScopedReportBase::AddSleep(StackID stack_id) {\n \n void ScopedReportBase::SetCount(int count) { rep_->count = count; }\n \n+void ScopedReportBase::SetSigNum(int sig) { rep_->signum = sig; }\n+\n const ReportDesc *ScopedReportBase::GetReport() const { return rep_; }\n \n ScopedReport::ScopedReport(ReportType typ, uptr tag)\n     : ScopedReportBase(typ, tag) {}\n \n ScopedReport::~ScopedReport() {}\n \n-void RestoreStack(Tid tid, const u64 epoch, VarSizeStackTrace *stk,\n-                  MutexSet *mset, uptr *tag) {\n-  // This function restores stack trace and mutex set for the thread/epoch.\n-  // It does so by getting stack trace and mutex set at the beginning of\n-  // trace part, and then replaying the trace till the given epoch.\n-  Trace* trace = ThreadTrace(tid);\n-  ReadLock l(&trace->mtx);\n-  const int partidx = (epoch / kTracePartSize) % TraceParts();\n-  TraceHeader* hdr = &trace->headers[partidx];\n-  if (epoch < hdr->epoch0 || epoch >= hdr->epoch0 + kTracePartSize)\n-    return;\n-  CHECK_EQ(RoundDown(epoch, kTracePartSize), hdr->epoch0);\n-  const u64 epoch0 = RoundDown(epoch, TraceSize());\n-  const u64 eend = epoch % TraceSize();\n-  const u64 ebegin = RoundDown(eend, kTracePartSize);\n-  DPrintf(\"#%d: RestoreStack epoch=%zu ebegin=%zu eend=%zu partidx=%d\\n\",\n-          tid, (uptr)epoch, (uptr)ebegin, (uptr)eend, partidx);\n-  Vector<uptr> stack;\n-  stack.Resize(hdr->stack0.size + 64);\n-  for (uptr i = 0; i < hdr->stack0.size; i++) {\n-    stack[i] = hdr->stack0.trace[i];\n-    DPrintf2(\"  #%02zu: pc=%zx\\n\", i, stack[i]);\n-  }\n-  if (mset)\n-    *mset = hdr->mset0;\n-  uptr pos = hdr->stack0.size;\n-  Event *events = (Event*)GetThreadTrace(tid);\n-  for (uptr i = ebegin; i <= eend; i++) {\n-    Event ev = events[i];\n-    EventType typ = (EventType)(ev >> kEventPCBits);\n-    uptr pc = (uptr)(ev & ((1ull << kEventPCBits) - 1));\n-    DPrintf2(\"  %zu typ=%d pc=%zx\\n\", i, typ, pc);\n-    if (typ == EventTypeMop) {\n-      stack[pos] = pc;\n-    } else if (typ == EventTypeFuncEnter) {\n-      if (stack.Size() < pos + 2)\n-        stack.Resize(pos + 2);\n-      stack[pos++] = pc;\n-    } else if (typ == EventTypeFuncExit) {\n-      if (pos > 0)\n-        pos--;\n-    }\n-    if (mset) {\n-      if (typ == EventTypeLock) {\n-        mset->Add(pc, true, epoch0 + i);\n-      } else if (typ == EventTypeUnlock) {\n-        mset->Del(pc, true);\n-      } else if (typ == EventTypeRLock) {\n-        mset->Add(pc, false, epoch0 + i);\n-      } else if (typ == EventTypeRUnlock) {\n-        mset->Del(pc, false);\n-      }\n-    }\n-    for (uptr j = 0; j <= pos; j++)\n-      DPrintf2(\"      #%zu: %zx\\n\", j, stack[j]);\n-  }\n-  if (pos == 0 && stack[0] == 0)\n-    return;\n-  pos++;\n-  stk->Init(&stack[0], pos);\n-  ExtractTagFromStack(stk, tag);\n-}\n-\n-namespace v3 {\n-\n // Replays the trace up to last_pos position in the last part\n // or up to the provided epoch/sid (whichever is earlier)\n // and calls the provided function f for each event.\n@@ -469,6 +366,7 @@ void TraceReplay(Trace *trace, TracePart *last, Event *last_pos, Sid sid,\n     Event *end = &part->events[TracePart::kSize - 1];\n     if (part == last)\n       end = last_pos;\n+    f(kFreeSid, kEpochOver, nullptr);  // notify about part start\n     for (Event *evp = &part->events[0]; evp < end; evp++) {\n       Event *evp0 = evp;\n       if (!evp->is_access && !evp->is_func) {\n@@ -528,21 +426,36 @@ static constexpr bool IsWithinAccess(uptr addr1, uptr size1, uptr addr2,\n   return addr1 >= addr2 && addr1 + size1 <= addr2 + size2;\n }\n \n-// Replays the trace of thread tid up to the target event identified\n-// by sid/epoch/addr/size/typ and restores and returns stack, mutex set\n+// Replays the trace of slot sid up to the target event identified\n+// by epoch/addr/size/typ and restores and returns tid, stack, mutex set\n // and tag for that event. If there are multiple such events, it returns\n // the last one. Returns false if the event is not present in the trace.\n-bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n-                  uptr size, AccessType typ, VarSizeStackTrace *pstk,\n+bool RestoreStack(EventType type, Sid sid, Epoch epoch, uptr addr, uptr size,\n+                  AccessType typ, Tid *ptid, VarSizeStackTrace *pstk,\n                   MutexSet *pmset, uptr *ptag) {\n   // This function restores stack trace and mutex set for the thread/epoch.\n   // It does so by getting stack trace and mutex set at the beginning of\n   // trace part, and then replaying the trace till the given epoch.\n-  DPrintf2(\"RestoreStack: tid=%u sid=%u@%u addr=0x%zx/%zu typ=%x\\n\", tid,\n+  DPrintf2(\"RestoreStack: sid=%u@%u addr=0x%zx/%zu typ=%x\\n\",\n            static_cast<int>(sid), static_cast<int>(epoch), addr, size,\n            static_cast<int>(typ));\n   ctx->slot_mtx.CheckLocked();  // needed to prevent trace part recycling\n   ctx->thread_registry.CheckLocked();\n+  TidSlot *slot = &ctx->slots[static_cast<uptr>(sid)];\n+  Tid tid = kInvalidTid;\n+  // Need to lock the slot mutex as it protects slot->journal.\n+  slot->mtx.CheckLocked();\n+  for (uptr i = 0; i < slot->journal.Size(); i++) {\n+    DPrintf2(\"  journal: epoch=%d tid=%d\\n\",\n+             static_cast<int>(slot->journal[i].epoch), slot->journal[i].tid);\n+    if (i == slot->journal.Size() - 1 || slot->journal[i + 1].epoch > epoch) {\n+      tid = slot->journal[i].tid;\n+      break;\n+    }\n+  }\n+  if (tid == kInvalidTid)\n+    return false;\n+  *ptid = tid;\n   ThreadContext *tctx =\n       static_cast<ThreadContext *>(ctx->thread_registry.GetThreadLocked(tid));\n   Trace *trace = &tctx->trace;\n@@ -553,8 +466,10 @@ bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n   {\n     Lock lock(&trace->mtx);\n     first_part = trace->parts.Front();\n-    if (!first_part)\n+    if (!first_part) {\n+      DPrintf2(\"RestoreStack: tid=%d trace=%p no trace parts\\n\", tid, trace);\n       return false;\n+    }\n     last_part = trace->parts.Back();\n     last_pos = trace->final_pos;\n     if (tctx->thr)\n@@ -567,9 +482,18 @@ bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n   bool is_read = typ & kAccessRead;\n   bool is_atomic = typ & kAccessAtomic;\n   bool is_free = typ & kAccessFree;\n+  DPrintf2(\"RestoreStack: tid=%d parts=[%p-%p] last_pos=%p\\n\", tid,\n+           trace->parts.Front(), last_part, last_pos);\n   TraceReplay(\n       trace, last_part, last_pos, sid, epoch,\n       [&](Sid ev_sid, Epoch ev_epoch, Event *evp) {\n+        if (evp == nullptr) {\n+          // Each trace part is self-consistent, so we reset state.\n+          stack.Resize(0);\n+          mset->Reset();\n+          prev_pc = 0;\n+          return;\n+        }\n         bool match = ev_sid == sid && ev_epoch == epoch;\n         if (evp->is_access) {\n           if (evp->is_func == 0 && evp->type == EventType::kAccessExt &&\n@@ -592,12 +516,15 @@ bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n         if (evp->is_func) {\n           auto *ev = reinterpret_cast<EventFunc *>(evp);\n           if (ev->pc) {\n-            DPrintf2(\"  FuncEnter: pc=0x%llx\\n\", ev->pc);\n+            DPrintf2(\" FuncEnter: pc=0x%llx\\n\", ev->pc);\n             stack.PushBack(ev->pc);\n           } else {\n-            DPrintf2(\"  FuncExit\\n\");\n-            CHECK(stack.Size());\n-            stack.PopBack();\n+            DPrintf2(\" FuncExit\\n\");\n+            // We don't log pathologically large stacks in each part,\n+            // if the stack was truncated we can have more func exits than\n+            // entries.\n+            if (stack.Size())\n+              stack.PopBack();\n           }\n           return;\n         }\n@@ -666,8 +593,6 @@ bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n   return found;\n }\n \n-}  // namespace v3\n-\n bool RacyStacks::operator==(const RacyStacks &other) const {\n   if (hash[0] == other.hash[0] && hash[1] == other.hash[1])\n     return true;\n@@ -758,10 +683,7 @@ bool OutputReport(ThreadState *thr, const ScopedReport &srep) {\n     ctx->fired_suppressions.push_back(s);\n   }\n   {\n-    bool old_is_freeing = thr->is_freeing;\n-    thr->is_freeing = false;\n     bool suppressed = OnReport(rep, pc_or_addr != 0);\n-    thr->is_freeing = old_is_freeing;\n     if (suppressed) {\n       thr->current_report = nullptr;\n       return false;\n@@ -808,97 +730,72 @@ static bool IsFiredSuppression(Context *ctx, ReportType type, uptr addr) {\n   return false;\n }\n \n-static bool RaceBetweenAtomicAndFree(ThreadState *thr) {\n-  Shadow s0(thr->racy_state[0]);\n-  Shadow s1(thr->racy_state[1]);\n-  CHECK(!(s0.IsAtomic() && s1.IsAtomic()));\n-  if (!s0.IsAtomic() && !s1.IsAtomic())\n-    return true;\n-  if (s0.IsAtomic() && s1.IsFreed())\n-    return true;\n-  if (s1.IsAtomic() && thr->is_freeing)\n-    return true;\n-  return false;\n-}\n-\n-void ReportRace(ThreadState *thr) {\n+void ReportRace(ThreadState *thr, RawShadow *shadow_mem, Shadow cur, Shadow old,\n+                AccessType typ0) {\n   CheckedMutex::CheckNoLocks();\n \n   // Symbolizer makes lots of intercepted calls. If we try to process them,\n   // at best it will cause deadlocks on internal mutexes.\n   ScopedIgnoreInterceptors ignore;\n \n+  uptr addr = ShadowToMem(shadow_mem);\n+  DPrintf(\"#%d: ReportRace %p\\n\", thr->tid, (void *)addr);\n   if (!ShouldReport(thr, ReportTypeRace))\n     return;\n-  if (!flags()->report_atomic_races && !RaceBetweenAtomicAndFree(thr))\n+  uptr addr_off0, size0;\n+  cur.GetAccess(&addr_off0, &size0, nullptr);\n+  uptr addr_off1, size1, typ1;\n+  old.GetAccess(&addr_off1, &size1, &typ1);\n+  if (!flags()->report_atomic_races &&\n+      ((typ0 & kAccessAtomic) || (typ1 & kAccessAtomic)) &&\n+      !(typ0 & kAccessFree) && !(typ1 & kAccessFree))\n     return;\n \n-  bool freed = false;\n-  {\n-    Shadow s(thr->racy_state[1]);\n-    freed = s.GetFreedAndReset();\n-    thr->racy_state[1] = s.raw();\n-  }\n-\n-  uptr addr = ShadowToMem(thr->racy_shadow_addr);\n-  uptr addr_min = 0;\n-  uptr addr_max = 0;\n-  {\n-    uptr a0 = addr + Shadow(thr->racy_state[0]).addr0();\n-    uptr a1 = addr + Shadow(thr->racy_state[1]).addr0();\n-    uptr e0 = a0 + Shadow(thr->racy_state[0]).size();\n-    uptr e1 = a1 + Shadow(thr->racy_state[1]).size();\n-    addr_min = min(a0, a1);\n-    addr_max = max(e0, e1);\n-    if (IsExpectedReport(addr_min, addr_max - addr_min))\n-      return;\n-  }\n+  const uptr kMop = 2;\n+  Shadow s[kMop] = {cur, old};\n+  uptr addr0 = addr + addr_off0;\n+  uptr addr1 = addr + addr_off1;\n+  uptr end0 = addr0 + size0;\n+  uptr end1 = addr1 + size1;\n+  uptr addr_min = min(addr0, addr1);\n+  uptr addr_max = max(end0, end1);\n+  if (IsExpectedReport(addr_min, addr_max - addr_min))\n+    return;\n   if (HandleRacyAddress(thr, addr_min, addr_max))\n     return;\n \n-  ReportType typ = ReportTypeRace;\n-  if (thr->is_vptr_access && freed)\n-    typ = ReportTypeVptrUseAfterFree;\n-  else if (thr->is_vptr_access)\n-    typ = ReportTypeVptrRace;\n-  else if (freed)\n-    typ = ReportTypeUseAfterFree;\n+  ReportType rep_typ = ReportTypeRace;\n+  if ((typ0 & kAccessVptr) && (typ1 & kAccessFree))\n+    rep_typ = ReportTypeVptrUseAfterFree;\n+  else if (typ0 & kAccessVptr)\n+    rep_typ = ReportTypeVptrRace;\n+  else if (typ1 & kAccessFree)\n+    rep_typ = ReportTypeUseAfterFree;\n \n-  if (IsFiredSuppression(ctx, typ, addr))\n+  if (IsFiredSuppression(ctx, rep_typ, addr))\n     return;\n \n-  const uptr kMop = 2;\n   VarSizeStackTrace traces[kMop];\n-  uptr tags[kMop] = {kExternalTagNone};\n-  uptr toppc = TraceTopPC(thr);\n-  if (toppc >> kEventPCBits) {\n-    // This is a work-around for a known issue.\n-    // The scenario where this happens is rather elaborate and requires\n-    // an instrumented __sanitizer_report_error_summary callback and\n-    // a __tsan_symbolize_external callback and a race during a range memory\n-    // access larger than 8 bytes. MemoryAccessRange adds the current PC to\n-    // the trace and starts processing memory accesses. A first memory access\n-    // triggers a race, we report it and call the instrumented\n-    // __sanitizer_report_error_summary, which adds more stuff to the trace\n-    // since it is intrumented. Then a second memory access in MemoryAccessRange\n-    // also triggers a race and we get here and call TraceTopPC to get the\n-    // current PC, however now it contains some unrelated events from the\n-    // callback. Most likely, TraceTopPC will now return a EventTypeFuncExit\n-    // event. Later we subtract -1 from it (in GetPreviousInstructionPc)\n-    // and the resulting PC has kExternalPCBit set, so we pass it to\n-    // __tsan_symbolize_external_ex. __tsan_symbolize_external_ex is within its\n-    // rights to crash since the PC is completely bogus.\n-    // test/tsan/double_race.cpp contains a test case for this.\n-    toppc = 0;\n-  }\n-  ObtainCurrentStack(thr, toppc, &traces[0], &tags[0]);\n-  if (IsFiredSuppression(ctx, typ, traces[0]))\n+  Tid tids[kMop] = {thr->tid, kInvalidTid};\n+  uptr tags[kMop] = {kExternalTagNone, kExternalTagNone};\n+\n+  ObtainCurrentStack(thr, thr->trace_prev_pc, &traces[0], &tags[0]);\n+  if (IsFiredSuppression(ctx, rep_typ, traces[0]))\n     return;\n \n-  DynamicMutexSet mset2;\n-  Shadow s2(thr->racy_state[1]);\n-  RestoreStack(s2.tid(), s2.epoch(), &traces[1], mset2, &tags[1]);\n-  if (IsFiredSuppression(ctx, typ, traces[1]))\n+  DynamicMutexSet mset1;\n+  MutexSet *mset[kMop] = {&thr->mset, mset1};\n+\n+  // We need to lock the slot during RestoreStack because it protects\n+  // the slot journal.\n+  Lock slot_lock(&ctx->slots[static_cast<uptr>(s[1].sid())].mtx);\n+  ThreadRegistryLock l0(&ctx->thread_registry);\n+  Lock slots_lock(&ctx->slot_mtx);\n+  if (!RestoreStack(EventType::kAccessExt, s[1].sid(), s[1].epoch(), addr1,\n+                    size1, typ1, &tids[1], &traces[1], mset[1], &tags[1]))\n+    return;\n+\n+  if (IsFiredSuppression(ctx, rep_typ, traces[1]))\n     return;\n \n   if (HandleRacyStacks(thr, traces))\n@@ -908,39 +805,41 @@ void ReportRace(ThreadState *thr) {\n   uptr tag = kExternalTagNone;\n   for (uptr i = 0; i < kMop; i++) {\n     if (tags[i] != kExternalTagNone) {\n-      typ = ReportTypeExternalRace;\n+      rep_typ = ReportTypeExternalRace;\n       tag = tags[i];\n       break;\n     }\n   }\n \n-  ThreadRegistryLock l0(&ctx->thread_registry);\n-  ScopedReport rep(typ, tag);\n-  for (uptr i = 0; i < kMop; i++) {\n-    Shadow s(thr->racy_state[i]);\n-    rep.AddMemoryAccess(addr, tags[i], s, traces[i],\n-                        i == 0 ? &thr->mset : mset2);\n-  }\n+  ScopedReport rep(rep_typ, tag);\n+  for (uptr i = 0; i < kMop; i++)\n+    rep.AddMemoryAccess(addr, tags[i], s[i], tids[i], traces[i], mset[i]);\n \n   for (uptr i = 0; i < kMop; i++) {\n-    FastState s(thr->racy_state[i]);\n     ThreadContext *tctx = static_cast<ThreadContext *>(\n-        ctx->thread_registry.GetThreadLocked(s.tid()));\n-    if (s.epoch() < tctx->epoch0 || s.epoch() > tctx->epoch1)\n-      continue;\n+        ctx->thread_registry.GetThreadLocked(tids[i]));\n     rep.AddThread(tctx);\n   }\n \n   rep.AddLocation(addr_min, addr_max - addr_min);\n \n-#if !SANITIZER_GO\n-  {\n-    Shadow s(thr->racy_state[1]);\n-    if (s.epoch() <= thr->last_sleep_clock.get(s.tid()))\n-      rep.AddSleep(thr->last_sleep_stack_id);\n+  if (flags()->print_full_thread_history) {\n+    const ReportDesc *rep_desc = rep.GetReport();\n+    for (uptr i = 0; i < rep_desc->threads.Size(); i++) {\n+      Tid parent_tid = rep_desc->threads[i]->parent_tid;\n+      if (parent_tid == kMainTid || parent_tid == kInvalidTid)\n+        continue;\n+      ThreadContext *parent_tctx = static_cast<ThreadContext *>(\n+          ctx->thread_registry.GetThreadLocked(parent_tid));\n+      rep.AddThread(parent_tctx);\n+    }\n   }\n-#endif\n \n+#if !SANITIZER_GO\n+  if (!((typ0 | typ1) & kAccessFree) &&\n+      s[1].epoch() <= thr->last_sleep_clock.Get(s[1].sid()))\n+    rep.AddSleep(thr->last_sleep_stack_id);\n+#endif\n   OutputReport(thr, rep);\n }\n "}, {"sha": "86c8b3764cc71934b86112d0e872667762bd04c4", "filename": "libsanitizer/tsan/tsan_rtl_thread.cpp", "status": "modified", "additions": 117, "deletions": 111, "changes": 228, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_thread.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_rtl_thread.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_thread.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -21,20 +21,14 @@ namespace __tsan {\n \n // ThreadContext implementation.\n \n-ThreadContext::ThreadContext(Tid tid)\n-    : ThreadContextBase(tid), thr(), sync(), epoch0(), epoch1() {}\n+ThreadContext::ThreadContext(Tid tid) : ThreadContextBase(tid), thr(), sync() {}\n \n #if !SANITIZER_GO\n ThreadContext::~ThreadContext() {\n }\n #endif\n \n-void ThreadContext::OnReset() {\n-  CHECK_EQ(sync.size(), 0);\n-  uptr trace_p = GetThreadTrace(tid);\n-  ReleaseMemoryPagesToOS(trace_p, trace_p + TraceSize() * sizeof(Event));\n-  //!!! ReleaseMemoryToOS(GetThreadTraceHeader(tid), sizeof(Trace));\n-}\n+void ThreadContext::OnReset() { CHECK(!sync); }\n \n #if !SANITIZER_GO\n struct ThreadLeak {\n@@ -57,7 +51,9 @@ static void CollectThreadLeaks(ThreadContextBase *tctx_base, void *arg) {\n }\n #endif\n \n-#if !SANITIZER_GO\n+// Disabled on Mac because lldb test TestTsanBasic fails:\n+// https://reviews.llvm.org/D112603#3163158\n+#if !SANITIZER_GO && !SANITIZER_MAC\n static void ReportIgnoresEnabled(ThreadContext *tctx, IgnoreSet *set) {\n   if (tctx->tid == kMainTid) {\n     Printf(\"ThreadSanitizer: main thread finished with ignores enabled\\n\");\n@@ -112,30 +108,35 @@ int ThreadCount(ThreadState *thr) {\n }\n \n struct OnCreatedArgs {\n-  ThreadState *thr;\n-  uptr pc;\n+  VectorClock *sync;\n+  uptr sync_epoch;\n+  StackID stack;\n };\n \n Tid ThreadCreate(ThreadState *thr, uptr pc, uptr uid, bool detached) {\n-  OnCreatedArgs args = { thr, pc };\n-  u32 parent_tid = thr ? thr->tid : kInvalidTid;  // No parent for GCD workers.\n-  Tid tid = ctx->thread_registry.CreateThread(uid, detached, parent_tid, &args);\n-  DPrintf(\"#%d: ThreadCreate tid=%d uid=%zu\\n\", parent_tid, tid, uid);\n+  // The main thread and GCD workers don't have a parent thread.\n+  Tid parent = kInvalidTid;\n+  OnCreatedArgs arg = {nullptr, 0, kInvalidStackID};\n+  if (thr) {\n+    parent = thr->tid;\n+    arg.stack = CurrentStackId(thr, pc);\n+    if (!thr->ignore_sync) {\n+      SlotLocker locker(thr);\n+      thr->clock.ReleaseStore(&arg.sync);\n+      arg.sync_epoch = ctx->global_epoch;\n+      IncrementEpoch(thr);\n+    }\n+  }\n+  Tid tid = ctx->thread_registry.CreateThread(uid, detached, parent, &arg);\n+  DPrintf(\"#%d: ThreadCreate tid=%d uid=%zu\\n\", parent, tid, uid);\n   return tid;\n }\n \n void ThreadContext::OnCreated(void *arg) {\n-  thr = 0;\n-  if (tid == kMainTid)\n-    return;\n   OnCreatedArgs *args = static_cast<OnCreatedArgs *>(arg);\n-  if (!args->thr)  // GCD workers don't have a parent thread.\n-    return;\n-  args->thr->fast_state.IncrementEpoch();\n-  // Can't increment epoch w/o writing to the trace as well.\n-  TraceAddEvent(args->thr, args->thr->fast_state, EventTypeMop, 0);\n-  ReleaseImpl(args->thr, 0, &sync);\n-  creation_stack_id = CurrentStackId(args->thr, args->pc);\n+  sync = args->sync;\n+  sync_epoch = args->sync_epoch;\n+  creation_stack_id = args->stack;\n }\n \n extern \"C\" void __tsan_stack_initialization() {}\n@@ -150,6 +151,15 @@ struct OnStartedArgs {\n \n void ThreadStart(ThreadState *thr, Tid tid, tid_t os_id,\n                  ThreadType thread_type) {\n+  ctx->thread_registry.StartThread(tid, os_id, thread_type, thr);\n+  if (!thr->ignore_sync) {\n+    SlotAttachAndLock(thr);\n+    if (thr->tctx->sync_epoch == ctx->global_epoch)\n+      thr->clock.Acquire(thr->tctx->sync);\n+    SlotUnlock(thr);\n+  }\n+  Free(thr->tctx->sync);\n+\n   uptr stk_addr = 0;\n   uptr stk_size = 0;\n   uptr tls_addr = 0;\n@@ -159,12 +169,10 @@ void ThreadStart(ThreadState *thr, Tid tid, tid_t os_id,\n     GetThreadStackAndTls(tid == kMainTid, &stk_addr, &stk_size, &tls_addr,\n                          &tls_size);\n #endif\n-\n-  ThreadRegistry *tr = &ctx->thread_registry;\n-  OnStartedArgs args = { thr, stk_addr, stk_size, tls_addr, tls_size };\n-  tr->StartThread(tid, os_id, thread_type, &args);\n-\n-  while (!thr->tctx->trace.parts.Empty()) thr->tctx->trace.parts.PopBack();\n+  thr->stk_addr = stk_addr;\n+  thr->stk_size = stk_size;\n+  thr->tls_addr = tls_addr;\n+  thr->tls_size = tls_size;\n \n #if !SANITIZER_GO\n   if (ctx->after_multithreaded_fork) {\n@@ -192,130 +200,128 @@ void ThreadStart(ThreadState *thr, Tid tid, tid_t os_id,\n }\n \n void ThreadContext::OnStarted(void *arg) {\n-  OnStartedArgs *args = static_cast<OnStartedArgs *>(arg);\n-  thr = args->thr;\n-  // RoundUp so that one trace part does not contain events\n-  // from different threads.\n-  epoch0 = RoundUp(epoch1 + 1, kTracePartSize);\n-  epoch1 = (u64)-1;\n-  new (thr)\n-      ThreadState(ctx, tid, unique_id, epoch0, reuse_count, args->stk_addr,\n-                  args->stk_size, args->tls_addr, args->tls_size);\n+  thr = static_cast<ThreadState *>(arg);\n+  DPrintf(\"#%d: ThreadStart\\n\", tid);\n+  new (thr) ThreadState(tid);\n   if (common_flags()->detect_deadlocks)\n-    thr->dd_lt = ctx->dd->CreateLogicalThread(unique_id);\n-  thr->fast_state.SetHistorySize(flags()->history_size);\n-  // Commit switch to the new part of the trace.\n-  // TraceAddEvent will reset stack0/mset0 in the new part for us.\n-  TraceAddEvent(thr, thr->fast_state, EventTypeMop, 0);\n-\n-  thr->fast_synch_epoch = epoch0;\n-  AcquireImpl(thr, 0, &sync);\n-  sync.Reset(&thr->proc()->clock_cache);\n+    thr->dd_lt = ctx->dd->CreateLogicalThread(tid);\n   thr->tctx = this;\n+#if !SANITIZER_GO\n   thr->is_inited = true;\n-  DPrintf(\n-      \"#%d: ThreadStart epoch=%zu stk_addr=%zx stk_size=%zx \"\n-      \"tls_addr=%zx tls_size=%zx\\n\",\n-      tid, (uptr)epoch0, args->stk_addr, args->stk_size, args->tls_addr,\n-      args->tls_size);\n+#endif\n }\n \n void ThreadFinish(ThreadState *thr) {\n+  DPrintf(\"#%d: ThreadFinish\\n\", thr->tid);\n   ThreadCheckIgnore(thr);\n   if (thr->stk_addr && thr->stk_size)\n     DontNeedShadowFor(thr->stk_addr, thr->stk_size);\n   if (thr->tls_addr && thr->tls_size)\n     DontNeedShadowFor(thr->tls_addr, thr->tls_size);\n   thr->is_dead = true;\n-  ctx->thread_registry.FinishThread(thr->tid);\n-}\n-\n-void ThreadContext::OnFinished() {\n-#if SANITIZER_GO\n+#if !SANITIZER_GO\n+  thr->is_inited = false;\n+  thr->ignore_interceptors++;\n+  PlatformCleanUpThreadState(thr);\n+#endif\n+  if (!thr->ignore_sync) {\n+    SlotLocker locker(thr);\n+    ThreadRegistryLock lock(&ctx->thread_registry);\n+    // Note: detached is protected by the thread registry mutex,\n+    // the thread may be detaching concurrently in another thread.\n+    if (!thr->tctx->detached) {\n+      thr->clock.ReleaseStore(&thr->tctx->sync);\n+      thr->tctx->sync_epoch = ctx->global_epoch;\n+      IncrementEpoch(thr);\n+    }\n+  }\n+#if !SANITIZER_GO\n+  UnmapOrDie(thr->shadow_stack, kShadowStackSize * sizeof(uptr));\n+#else\n   Free(thr->shadow_stack);\n+#endif\n+  thr->shadow_stack = nullptr;\n   thr->shadow_stack_pos = nullptr;\n   thr->shadow_stack_end = nullptr;\n-#endif\n-  if (!detached) {\n-    thr->fast_state.IncrementEpoch();\n-    // Can't increment epoch w/o writing to the trace as well.\n-    TraceAddEvent(thr, thr->fast_state, EventTypeMop, 0);\n-    ReleaseImpl(thr, 0, &sync);\n-  }\n-  epoch1 = thr->fast_state.epoch();\n-\n   if (common_flags()->detect_deadlocks)\n     ctx->dd->DestroyLogicalThread(thr->dd_lt);\n-  thr->clock.ResetCached(&thr->proc()->clock_cache);\n-#if !SANITIZER_GO\n-  thr->last_sleep_clock.ResetCached(&thr->proc()->clock_cache);\n-#endif\n-#if !SANITIZER_GO\n-  PlatformCleanUpThreadState(thr);\n-#endif\n+  SlotDetach(thr);\n+  ctx->thread_registry.FinishThread(thr->tid);\n   thr->~ThreadState();\n-  thr = 0;\n+}\n+\n+void ThreadContext::OnFinished() {\n+  Lock lock(&ctx->slot_mtx);\n+  Lock lock1(&trace.mtx);\n+  // Queue all trace parts into the global recycle queue.\n+  auto parts = &trace.parts;\n+  while (trace.local_head) {\n+    CHECK(parts->Queued(trace.local_head));\n+    ctx->trace_part_recycle.PushBack(trace.local_head);\n+    trace.local_head = parts->Next(trace.local_head);\n+  }\n+  ctx->trace_part_recycle_finished += parts->Size();\n+  if (ctx->trace_part_recycle_finished > Trace::kFinishedThreadHi) {\n+    ctx->trace_part_finished_excess += parts->Size();\n+    trace.parts_allocated = 0;\n+  } else if (ctx->trace_part_recycle_finished > Trace::kFinishedThreadLo &&\n+             parts->Size() > 1) {\n+    ctx->trace_part_finished_excess += parts->Size() - 1;\n+    trace.parts_allocated = 1;\n+  }\n+  // From now on replay will use trace->final_pos.\n+  trace.final_pos = (Event *)atomic_load_relaxed(&thr->trace_pos);\n+  atomic_store_relaxed(&thr->trace_pos, 0);\n+  thr->tctx = nullptr;\n+  thr = nullptr;\n }\n \n struct ConsumeThreadContext {\n   uptr uid;\n   ThreadContextBase *tctx;\n };\n \n-static bool ConsumeThreadByUid(ThreadContextBase *tctx, void *arg) {\n-  ConsumeThreadContext *findCtx = (ConsumeThreadContext *)arg;\n-  if (tctx->user_id == findCtx->uid && tctx->status != ThreadStatusInvalid) {\n-    if (findCtx->tctx) {\n-      // Ensure that user_id is unique. If it's not the case we are screwed.\n-      // Something went wrong before, but now there is no way to recover.\n-      // Returning a wrong thread is not an option, it may lead to very hard\n-      // to debug false positives (e.g. if we join a wrong thread).\n-      Report(\"ThreadSanitizer: dup thread with used id 0x%zx\\n\", findCtx->uid);\n-      Die();\n-    }\n-    findCtx->tctx = tctx;\n-    tctx->user_id = 0;\n-  }\n-  return false;\n-}\n-\n Tid ThreadConsumeTid(ThreadState *thr, uptr pc, uptr uid) {\n-  ConsumeThreadContext findCtx = {uid, nullptr};\n-  ctx->thread_registry.FindThread(ConsumeThreadByUid, &findCtx);\n-  Tid tid = findCtx.tctx ? findCtx.tctx->tid : kInvalidTid;\n-  DPrintf(\"#%d: ThreadTid uid=%zu tid=%d\\n\", thr->tid, uid, tid);\n-  return tid;\n+  return ctx->thread_registry.ConsumeThreadUserId(uid);\n }\n \n+struct JoinArg {\n+  VectorClock *sync;\n+  uptr sync_epoch;\n+};\n+\n void ThreadJoin(ThreadState *thr, uptr pc, Tid tid) {\n   CHECK_GT(tid, 0);\n-  CHECK_LT(tid, kMaxTid);\n   DPrintf(\"#%d: ThreadJoin tid=%d\\n\", thr->tid, tid);\n-  ctx->thread_registry.JoinThread(tid, thr);\n+  JoinArg arg = {};\n+  ctx->thread_registry.JoinThread(tid, &arg);\n+  if (!thr->ignore_sync) {\n+    SlotLocker locker(thr);\n+    if (arg.sync_epoch == ctx->global_epoch)\n+      thr->clock.Acquire(arg.sync);\n+  }\n+  Free(arg.sync);\n }\n \n-void ThreadContext::OnJoined(void *arg) {\n-  ThreadState *caller_thr = static_cast<ThreadState *>(arg);\n-  AcquireImpl(caller_thr, 0, &sync);\n-  sync.Reset(&caller_thr->proc()->clock_cache);\n+void ThreadContext::OnJoined(void *ptr) {\n+  auto arg = static_cast<JoinArg *>(ptr);\n+  arg->sync = sync;\n+  arg->sync_epoch = sync_epoch;\n+  sync = nullptr;\n+  sync_epoch = 0;\n }\n \n-void ThreadContext::OnDead() { CHECK_EQ(sync.size(), 0); }\n+void ThreadContext::OnDead() { CHECK_EQ(sync, nullptr); }\n \n void ThreadDetach(ThreadState *thr, uptr pc, Tid tid) {\n   CHECK_GT(tid, 0);\n-  CHECK_LT(tid, kMaxTid);\n   ctx->thread_registry.DetachThread(tid, thr);\n }\n \n-void ThreadContext::OnDetached(void *arg) {\n-  ThreadState *thr1 = static_cast<ThreadState *>(arg);\n-  sync.Reset(&thr1->proc()->clock_cache);\n-}\n+void ThreadContext::OnDetached(void *arg) { Free(sync); }\n \n void ThreadNotJoined(ThreadState *thr, uptr pc, Tid tid, uptr uid) {\n   CHECK_GT(tid, 0);\n-  CHECK_LT(tid, kMaxTid);\n   ctx->thread_registry.SetThreadUserId(tid, uid);\n }\n "}, {"sha": "843573ecf5d30f894b7913b16ceb762c45fce0ee", "filename": "libsanitizer/tsan/tsan_shadow.h", "status": "modified", "additions": 131, "deletions": 184, "changes": 315, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_shadow.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_shadow.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_shadow.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -10,223 +10,170 @@\n #define TSAN_SHADOW_H\n \n #include \"tsan_defs.h\"\n-#include \"tsan_trace.h\"\n \n namespace __tsan {\n \n-// FastState (from most significant bit):\n-//   ignore          : 1\n-//   tid             : kTidBits\n-//   unused          : -\n-//   history_size    : 3\n-//   epoch           : kClkBits\n class FastState {\n  public:\n-  FastState(u64 tid, u64 epoch) {\n-    x_ = tid << kTidShift;\n-    x_ |= epoch;\n-    DCHECK_EQ(tid, this->tid());\n-    DCHECK_EQ(epoch, this->epoch());\n-    DCHECK_EQ(GetIgnoreBit(), false);\n-  }\n-\n-  explicit FastState(u64 x) : x_(x) {}\n-\n-  u64 raw() const { return x_; }\n-\n-  u64 tid() const {\n-    u64 res = (x_ & ~kIgnoreBit) >> kTidShift;\n-    return res;\n-  }\n-\n-  u64 TidWithIgnore() const {\n-    u64 res = x_ >> kTidShift;\n-    return res;\n-  }\n-\n-  u64 epoch() const {\n-    u64 res = x_ & ((1ull << kClkBits) - 1);\n-    return res;\n-  }\n+  FastState() { Reset(); }\n \n-  void IncrementEpoch() {\n-    u64 old_epoch = epoch();\n-    x_ += 1;\n-    DCHECK_EQ(old_epoch + 1, epoch());\n-    (void)old_epoch;\n+  void Reset() {\n+    part_.unused0_ = 0;\n+    part_.sid_ = static_cast<u8>(kFreeSid);\n+    part_.epoch_ = static_cast<u16>(kEpochLast);\n+    part_.unused1_ = 0;\n+    part_.ignore_accesses_ = false;\n   }\n \n-  void SetIgnoreBit() { x_ |= kIgnoreBit; }\n-  void ClearIgnoreBit() { x_ &= ~kIgnoreBit; }\n-  bool GetIgnoreBit() const { return (s64)x_ < 0; }\n+  void SetSid(Sid sid) { part_.sid_ = static_cast<u8>(sid); }\n \n-  void SetHistorySize(int hs) {\n-    CHECK_GE(hs, 0);\n-    CHECK_LE(hs, 7);\n-    x_ = (x_ & ~(kHistoryMask << kHistoryShift)) | (u64(hs) << kHistoryShift);\n-  }\n+  Sid sid() const { return static_cast<Sid>(part_.sid_); }\n \n-  ALWAYS_INLINE\n-  int GetHistorySize() const {\n-    return (int)((x_ >> kHistoryShift) & kHistoryMask);\n-  }\n+  Epoch epoch() const { return static_cast<Epoch>(part_.epoch_); }\n \n-  void ClearHistorySize() { SetHistorySize(0); }\n+  void SetEpoch(Epoch epoch) { part_.epoch_ = static_cast<u16>(epoch); }\n \n-  ALWAYS_INLINE\n-  u64 GetTracePos() const {\n-    const int hs = GetHistorySize();\n-    // When hs == 0, the trace consists of 2 parts.\n-    const u64 mask = (1ull << (kTracePartSizeBits + hs + 1)) - 1;\n-    return epoch() & mask;\n-  }\n+  void SetIgnoreBit() { part_.ignore_accesses_ = 1; }\n+  void ClearIgnoreBit() { part_.ignore_accesses_ = 0; }\n+  bool GetIgnoreBit() const { return part_.ignore_accesses_; }\n \n  private:\n   friend class Shadow;\n-  static const int kTidShift = 64 - kTidBits - 1;\n-  static const u64 kIgnoreBit = 1ull << 63;\n-  static const u64 kFreedBit = 1ull << 63;\n-  static const u64 kHistoryShift = kClkBits;\n-  static const u64 kHistoryMask = 7;\n-  u64 x_;\n+  struct Parts {\n+    u32 unused0_ : 8;\n+    u32 sid_ : 8;\n+    u32 epoch_ : kEpochBits;\n+    u32 unused1_ : 1;\n+    u32 ignore_accesses_ : 1;\n+  };\n+  union {\n+    Parts part_;\n+    u32 raw_;\n+  };\n };\n \n-// Shadow (from most significant bit):\n-//   freed           : 1\n-//   tid             : kTidBits\n-//   is_atomic       : 1\n-//   is_read         : 1\n-//   size_log        : 2\n-//   addr0           : 3\n-//   epoch           : kClkBits\n-class Shadow : public FastState {\n- public:\n-  explicit Shadow(u64 x) : FastState(x) {}\n+static_assert(sizeof(FastState) == kShadowSize, \"bad FastState size\");\n \n-  explicit Shadow(const FastState &s) : FastState(s.x_) { ClearHistorySize(); }\n-\n-  void SetAddr0AndSizeLog(u64 addr0, unsigned kAccessSizeLog) {\n-    DCHECK_EQ((x_ >> kClkBits) & 31, 0);\n-    DCHECK_LE(addr0, 7);\n-    DCHECK_LE(kAccessSizeLog, 3);\n-    x_ |= ((kAccessSizeLog << 3) | addr0) << kClkBits;\n-    DCHECK_EQ(kAccessSizeLog, size_log());\n-    DCHECK_EQ(addr0, this->addr0());\n-  }\n-\n-  void SetWrite(unsigned kAccessIsWrite) {\n-    DCHECK_EQ(x_ & kReadBit, 0);\n-    if (!kAccessIsWrite)\n-      x_ |= kReadBit;\n-    DCHECK_EQ(kAccessIsWrite, IsWrite());\n-  }\n-\n-  void SetAtomic(bool kIsAtomic) {\n-    DCHECK(!IsAtomic());\n-    if (kIsAtomic)\n-      x_ |= kAtomicBit;\n-    DCHECK_EQ(IsAtomic(), kIsAtomic);\n-  }\n-\n-  bool IsAtomic() const { return x_ & kAtomicBit; }\n-\n-  bool IsZero() const { return x_ == 0; }\n-\n-  static inline bool TidsAreEqual(const Shadow s1, const Shadow s2) {\n-    u64 shifted_xor = (s1.x_ ^ s2.x_) >> kTidShift;\n-    DCHECK_EQ(shifted_xor == 0, s1.TidWithIgnore() == s2.TidWithIgnore());\n-    return shifted_xor == 0;\n-  }\n-\n-  static ALWAYS_INLINE bool Addr0AndSizeAreEqual(const Shadow s1,\n-                                                 const Shadow s2) {\n-    u64 masked_xor = ((s1.x_ ^ s2.x_) >> kClkBits) & 31;\n-    return masked_xor == 0;\n+class Shadow {\n+ public:\n+  static constexpr RawShadow kEmpty = static_cast<RawShadow>(0);\n+\n+  Shadow(FastState state, u32 addr, u32 size, AccessType typ) {\n+    raw_ = state.raw_;\n+    DCHECK_GT(size, 0);\n+    DCHECK_LE(size, 8);\n+    UNUSED Sid sid0 = part_.sid_;\n+    UNUSED u16 epoch0 = part_.epoch_;\n+    raw_ |= (!!(typ & kAccessAtomic) << kIsAtomicShift) |\n+            (!!(typ & kAccessRead) << kIsReadShift) |\n+            (((((1u << size) - 1) << (addr & 0x7)) & 0xff) << kAccessShift);\n+    // Note: we don't check kAccessAtomic because it overlaps with\n+    // FastState::ignore_accesses_ and it may be set spuriously.\n+    DCHECK_EQ(part_.is_read_, !!(typ & kAccessRead));\n+    DCHECK_EQ(sid(), sid0);\n+    DCHECK_EQ(epoch(), epoch0);\n+  }\n+\n+  explicit Shadow(RawShadow x = Shadow::kEmpty) { raw_ = static_cast<u32>(x); }\n+\n+  RawShadow raw() const { return static_cast<RawShadow>(raw_); }\n+  Sid sid() const { return part_.sid_; }\n+  Epoch epoch() const { return static_cast<Epoch>(part_.epoch_); }\n+  u8 access() const { return part_.access_; }\n+\n+  void GetAccess(uptr *addr, uptr *size, AccessType *typ) const {\n+    DCHECK(part_.access_ != 0 || raw_ == static_cast<u32>(Shadow::kRodata));\n+    if (addr)\n+      *addr = part_.access_ ? __builtin_ffs(part_.access_) - 1 : 0;\n+    if (size)\n+      *size = part_.access_ == kFreeAccess ? kShadowCell\n+                                           : __builtin_popcount(part_.access_);\n+    if (typ)\n+      *typ = (part_.is_read_ ? kAccessRead : kAccessWrite) |\n+             (part_.is_atomic_ ? kAccessAtomic : 0) |\n+             (part_.access_ == kFreeAccess ? kAccessFree : 0);\n   }\n \n-  static ALWAYS_INLINE bool TwoRangesIntersect(Shadow s1, Shadow s2,\n-                                               unsigned kS2AccessSize) {\n-    bool res = false;\n-    u64 diff = s1.addr0() - s2.addr0();\n-    if ((s64)diff < 0) {  // s1.addr0 < s2.addr0\n-      // if (s1.addr0() + size1) > s2.addr0()) return true;\n-      if (s1.size() > -diff)\n-        res = true;\n-    } else {\n-      // if (s2.addr0() + kS2AccessSize > s1.addr0()) return true;\n-      if (kS2AccessSize > diff)\n-        res = true;\n-    }\n-    DCHECK_EQ(res, TwoRangesIntersectSlow(s1, s2));\n-    DCHECK_EQ(res, TwoRangesIntersectSlow(s2, s1));\n+  ALWAYS_INLINE\n+  bool IsBothReadsOrAtomic(AccessType typ) const {\n+    u32 is_read = !!(typ & kAccessRead);\n+    u32 is_atomic = !!(typ & kAccessAtomic);\n+    bool res =\n+        raw_ & ((is_atomic << kIsAtomicShift) | (is_read << kIsReadShift));\n+    DCHECK_EQ(res,\n+              (part_.is_read_ && is_read) || (part_.is_atomic_ && is_atomic));\n     return res;\n   }\n \n-  u64 ALWAYS_INLINE addr0() const { return (x_ >> kClkBits) & 7; }\n-  u64 ALWAYS_INLINE size() const { return 1ull << size_log(); }\n-  bool ALWAYS_INLINE IsWrite() const { return !IsRead(); }\n-  bool ALWAYS_INLINE IsRead() const { return x_ & kReadBit; }\n-\n-  // The idea behind the freed bit is as follows.\n-  // When the memory is freed (or otherwise unaccessible) we write to the shadow\n-  // values with tid/epoch related to the free and the freed bit set.\n-  // During memory accesses processing the freed bit is considered\n-  // as msb of tid. So any access races with shadow with freed bit set\n-  // (it is as if write from a thread with which we never synchronized before).\n-  // This allows us to detect accesses to freed memory w/o additional\n-  // overheads in memory access processing and at the same time restore\n-  // tid/epoch of free.\n-  void MarkAsFreed() { x_ |= kFreedBit; }\n-\n-  bool IsFreed() const { return x_ & kFreedBit; }\n-\n-  bool GetFreedAndReset() {\n-    bool res = x_ & kFreedBit;\n-    x_ &= ~kFreedBit;\n+  ALWAYS_INLINE\n+  bool IsRWWeakerOrEqual(AccessType typ) const {\n+    u32 is_read = !!(typ & kAccessRead);\n+    u32 is_atomic = !!(typ & kAccessAtomic);\n+    UNUSED u32 res0 =\n+        (part_.is_atomic_ > is_atomic) ||\n+        (part_.is_atomic_ == is_atomic && part_.is_read_ >= is_read);\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+    const u32 kAtomicReadMask = (1 << kIsAtomicShift) | (1 << kIsReadShift);\n+    bool res = (raw_ & kAtomicReadMask) >=\n+               ((is_atomic << kIsAtomicShift) | (is_read << kIsReadShift));\n+\n+    DCHECK_EQ(res, res0);\n     return res;\n+#else\n+    return res0;\n+#endif\n   }\n \n-  bool ALWAYS_INLINE IsBothReadsOrAtomic(bool kIsWrite, bool kIsAtomic) const {\n-    bool v = x_ & ((u64(kIsWrite ^ 1) << kReadShift) |\n-                   (u64(kIsAtomic) << kAtomicShift));\n-    DCHECK_EQ(v, (!IsWrite() && !kIsWrite) || (IsAtomic() && kIsAtomic));\n-    return v;\n-  }\n-\n-  bool ALWAYS_INLINE IsRWNotWeaker(bool kIsWrite, bool kIsAtomic) const {\n-    bool v = ((x_ >> kReadShift) & 3) <= u64((kIsWrite ^ 1) | (kIsAtomic << 1));\n-    DCHECK_EQ(v, (IsAtomic() < kIsAtomic) ||\n-                     (IsAtomic() == kIsAtomic && !IsWrite() <= !kIsWrite));\n-    return v;\n+  // The FreedMarker must not pass \"the same access check\" so that we don't\n+  // return from the race detection algorithm early.\n+  static RawShadow FreedMarker() {\n+    FastState fs;\n+    fs.SetSid(kFreeSid);\n+    fs.SetEpoch(kEpochLast);\n+    Shadow s(fs, 0, 8, kAccessWrite);\n+    return s.raw();\n   }\n \n-  bool ALWAYS_INLINE IsRWWeakerOrEqual(bool kIsWrite, bool kIsAtomic) const {\n-    bool v = ((x_ >> kReadShift) & 3) >= u64((kIsWrite ^ 1) | (kIsAtomic << 1));\n-    DCHECK_EQ(v, (IsAtomic() > kIsAtomic) ||\n-                     (IsAtomic() == kIsAtomic && !IsWrite() >= !kIsWrite));\n-    return v;\n+  static RawShadow FreedInfo(Sid sid, Epoch epoch) {\n+    Shadow s;\n+    s.part_.sid_ = sid;\n+    s.part_.epoch_ = static_cast<u16>(epoch);\n+    s.part_.access_ = kFreeAccess;\n+    return s.raw();\n   }\n \n  private:\n-  static const u64 kReadShift = 5 + kClkBits;\n-  static const u64 kReadBit = 1ull << kReadShift;\n-  static const u64 kAtomicShift = 6 + kClkBits;\n-  static const u64 kAtomicBit = 1ull << kAtomicShift;\n-\n-  u64 size_log() const { return (x_ >> (3 + kClkBits)) & 3; }\n-\n-  static bool TwoRangesIntersectSlow(const Shadow s1, const Shadow s2) {\n-    if (s1.addr0() == s2.addr0())\n-      return true;\n-    if (s1.addr0() < s2.addr0() && s1.addr0() + s1.size() > s2.addr0())\n-      return true;\n-    if (s2.addr0() < s1.addr0() && s2.addr0() + s2.size() > s1.addr0())\n-      return true;\n-    return false;\n-  }\n+  struct Parts {\n+    u8 access_;\n+    Sid sid_;\n+    u16 epoch_ : kEpochBits;\n+    u16 is_read_ : 1;\n+    u16 is_atomic_ : 1;\n+  };\n+  union {\n+    Parts part_;\n+    u32 raw_;\n+  };\n+\n+  static constexpr u8 kFreeAccess = 0x81;\n+\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+  static constexpr uptr kAccessShift = 0;\n+  static constexpr uptr kIsReadShift = 30;\n+  static constexpr uptr kIsAtomicShift = 31;\n+#else\n+  static constexpr uptr kAccessShift = 24;\n+  static constexpr uptr kIsReadShift = 1;\n+  static constexpr uptr kIsAtomicShift = 0;\n+#endif\n+\n+ public:\n+  // .rodata shadow marker, see MapRodata and ContainsSameAccessFast.\n+  static constexpr RawShadow kRodata =\n+      static_cast<RawShadow>(1 << kIsReadShift);\n };\n \n-const RawShadow kShadowRodata = (RawShadow)-1;  // .rodata shadow marker\n+static_assert(sizeof(Shadow) == kShadowSize, \"bad Shadow size\");\n \n }  // namespace __tsan\n "}, {"sha": "09d41780d188a0c926dbbc1eaeb842a4d23522e3", "filename": "libsanitizer/tsan/tsan_sync.cpp", "status": "modified", "additions": 46, "deletions": 36, "changes": 82, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_sync.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_sync.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_sync.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -18,43 +18,31 @@ namespace __tsan {\n \n void DDMutexInit(ThreadState *thr, uptr pc, SyncVar *s);\n \n-SyncVar::SyncVar() : mtx(MutexTypeSyncVar) { Reset(0); }\n+SyncVar::SyncVar() : mtx(MutexTypeSyncVar) { Reset(); }\n \n-void SyncVar::Init(ThreadState *thr, uptr pc, uptr addr, u64 uid,\n-                   bool save_stack) {\n+void SyncVar::Init(ThreadState *thr, uptr pc, uptr addr, bool save_stack) {\n+  Reset();\n   this->addr = addr;\n-  this->uid = uid;\n-  this->next = 0;\n-\n-  creation_stack_id = kInvalidStackID;\n+  next = 0;\n   if (save_stack && !SANITIZER_GO)  // Go does not use them\n     creation_stack_id = CurrentStackId(thr, pc);\n   if (common_flags()->detect_deadlocks)\n     DDMutexInit(thr, pc, this);\n }\n \n-void SyncVar::Reset(Processor *proc) {\n-  uid = 0;\n+void SyncVar::Reset() {\n+  CHECK(!ctx->resetting);\n   creation_stack_id = kInvalidStackID;\n   owner_tid = kInvalidTid;\n-  last_lock = 0;\n+  last_lock.Reset();\n   recursion = 0;\n   atomic_store_relaxed(&flags, 0);\n-\n-  if (proc == 0) {\n-    CHECK_EQ(clock.size(), 0);\n-    CHECK_EQ(read_clock.size(), 0);\n-  } else {\n-    clock.Reset(&proc->clock_cache);\n-    read_clock.Reset(&proc->clock_cache);\n-  }\n+  Free(clock);\n+  Free(read_clock);\n }\n \n MetaMap::MetaMap()\n-    : block_alloc_(LINKER_INITIALIZED, \"heap block allocator\"),\n-      sync_alloc_(LINKER_INITIALIZED, \"sync allocator\") {\n-  atomic_store(&uid_gen_, 0, memory_order_relaxed);\n-}\n+    : block_alloc_(\"heap block allocator\"), sync_alloc_(\"sync allocator\") {}\n \n void MetaMap::AllocBlock(ThreadState *thr, uptr pc, uptr p, uptr sz) {\n   u32 idx = block_alloc_.Alloc(&thr->proc()->block_cache);\n@@ -68,16 +56,16 @@ void MetaMap::AllocBlock(ThreadState *thr, uptr pc, uptr p, uptr sz) {\n   *meta = idx | kFlagBlock;\n }\n \n-uptr MetaMap::FreeBlock(Processor *proc, uptr p) {\n+uptr MetaMap::FreeBlock(Processor *proc, uptr p, bool reset) {\n   MBlock* b = GetBlock(p);\n   if (b == 0)\n     return 0;\n   uptr sz = RoundUpTo(b->siz, kMetaShadowCell);\n-  FreeRange(proc, p, sz);\n+  FreeRange(proc, p, sz, reset);\n   return sz;\n }\n \n-bool MetaMap::FreeRange(Processor *proc, uptr p, uptr sz) {\n+bool MetaMap::FreeRange(Processor *proc, uptr p, uptr sz, bool reset) {\n   bool has_something = false;\n   u32 *meta = MemToMeta(p);\n   u32 *end = MemToMeta(p + sz);\n@@ -99,7 +87,8 @@ bool MetaMap::FreeRange(Processor *proc, uptr p, uptr sz) {\n         DCHECK(idx & kFlagSync);\n         SyncVar *s = sync_alloc_.Map(idx & ~kFlagMask);\n         u32 next = s->next;\n-        s->Reset(proc);\n+        if (reset)\n+          s->Reset();\n         sync_alloc_.Free(&proc->sync_cache, idx & ~kFlagMask);\n         idx = next;\n       } else {\n@@ -116,30 +105,30 @@ bool MetaMap::FreeRange(Processor *proc, uptr p, uptr sz) {\n // which can be huge. The function probes pages one-by-one until it finds a page\n // without meta objects, at this point it stops freeing meta objects. Because\n // thread stacks grow top-down, we do the same starting from end as well.\n-void MetaMap::ResetRange(Processor *proc, uptr p, uptr sz) {\n+void MetaMap::ResetRange(Processor *proc, uptr p, uptr sz, bool reset) {\n   if (SANITIZER_GO) {\n     // UnmapOrDie/MmapFixedNoReserve does not work on Windows,\n     // so we do the optimization only for C/C++.\n-    FreeRange(proc, p, sz);\n+    FreeRange(proc, p, sz, reset);\n     return;\n   }\n   const uptr kMetaRatio = kMetaShadowCell / kMetaShadowSize;\n   const uptr kPageSize = GetPageSizeCached() * kMetaRatio;\n   if (sz <= 4 * kPageSize) {\n     // If the range is small, just do the normal free procedure.\n-    FreeRange(proc, p, sz);\n+    FreeRange(proc, p, sz, reset);\n     return;\n   }\n   // First, round both ends of the range to page size.\n   uptr diff = RoundUp(p, kPageSize) - p;\n   if (diff != 0) {\n-    FreeRange(proc, p, diff);\n+    FreeRange(proc, p, diff, reset);\n     p += diff;\n     sz -= diff;\n   }\n   diff = p + sz - RoundDown(p + sz, kPageSize);\n   if (diff != 0) {\n-    FreeRange(proc, p + sz - diff, diff);\n+    FreeRange(proc, p + sz - diff, diff, reset);\n     sz -= diff;\n   }\n   // Now we must have a non-empty page-aligned range.\n@@ -150,15 +139,15 @@ void MetaMap::ResetRange(Processor *proc, uptr p, uptr sz) {\n   const uptr sz0 = sz;\n   // Probe start of the range.\n   for (uptr checked = 0; sz > 0; checked += kPageSize) {\n-    bool has_something = FreeRange(proc, p, kPageSize);\n+    bool has_something = FreeRange(proc, p, kPageSize, reset);\n     p += kPageSize;\n     sz -= kPageSize;\n     if (!has_something && checked > (128 << 10))\n       break;\n   }\n   // Probe end of the range.\n   for (uptr checked = 0; sz > 0; checked += kPageSize) {\n-    bool has_something = FreeRange(proc, p + sz - kPageSize, kPageSize);\n+    bool has_something = FreeRange(proc, p + sz - kPageSize, kPageSize, reset);\n     sz -= kPageSize;\n     // Stacks grow down, so sync object are most likely at the end of the region\n     // (if it is a stack). The very end of the stack is TLS and tsan increases\n@@ -177,6 +166,27 @@ void MetaMap::ResetRange(Processor *proc, uptr p, uptr sz) {\n     Die();\n }\n \n+void MetaMap::ResetClocks() {\n+  // This can be called from the background thread\n+  // which does not have proc/cache.\n+  // The cache is too large for stack.\n+  static InternalAllocatorCache cache;\n+  internal_memset(&cache, 0, sizeof(cache));\n+  internal_allocator()->InitCache(&cache);\n+  sync_alloc_.ForEach([&](SyncVar *s) {\n+    if (s->clock) {\n+      InternalFree(s->clock, &cache);\n+      s->clock = nullptr;\n+    }\n+    if (s->read_clock) {\n+      InternalFree(s->read_clock, &cache);\n+      s->read_clock = nullptr;\n+    }\n+    s->last_lock.Reset();\n+  });\n+  internal_allocator()->DestroyCache(&cache);\n+}\n+\n MBlock* MetaMap::GetBlock(uptr p) {\n   u32 *meta = MemToMeta(p);\n   u32 idx = *meta;\n@@ -193,6 +203,7 @@ MBlock* MetaMap::GetBlock(uptr p) {\n \n SyncVar *MetaMap::GetSync(ThreadState *thr, uptr pc, uptr addr, bool create,\n                           bool save_stack) {\n+  DCHECK(!create || thr->slot_locked);\n   u32 *meta = MemToMeta(addr);\n   u32 idx0 = *meta;\n   u32 myidx = 0;\n@@ -203,7 +214,7 @@ SyncVar *MetaMap::GetSync(ThreadState *thr, uptr pc, uptr addr, bool create,\n       SyncVar * s = sync_alloc_.Map(idx & ~kFlagMask);\n       if (LIKELY(s->addr == addr)) {\n         if (UNLIKELY(myidx != 0)) {\n-          mys->Reset(thr->proc());\n+          mys->Reset();\n           sync_alloc_.Free(&thr->proc()->sync_cache, myidx);\n         }\n         return s;\n@@ -218,10 +229,9 @@ SyncVar *MetaMap::GetSync(ThreadState *thr, uptr pc, uptr addr, bool create,\n     }\n \n     if (LIKELY(myidx == 0)) {\n-      const u64 uid = atomic_fetch_add(&uid_gen_, 1, memory_order_relaxed);\n       myidx = sync_alloc_.Alloc(&thr->proc()->sync_cache);\n       mys = sync_alloc_.Map(myidx);\n-      mys->Init(thr, pc, addr, uid, save_stack);\n+      mys->Init(thr, pc, addr, save_stack);\n     }\n     mys->next = idx0;\n     if (atomic_compare_exchange_strong((atomic_uint32_t*)meta, &idx0,"}, {"sha": "67d3c0b5e7dd7bd6fe73ad7bbec4e03eef043968", "filename": "libsanitizer/tsan/tsan_sync.h", "status": "modified", "additions": 21, "deletions": 26, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_sync.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_sync.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_sync.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -16,8 +16,9 @@\n #include \"sanitizer_common/sanitizer_common.h\"\n #include \"sanitizer_common/sanitizer_deadlock_detector_interface.h\"\n #include \"tsan_defs.h\"\n-#include \"tsan_clock.h\"\n #include \"tsan_dense_alloc.h\"\n+#include \"tsan_shadow.h\"\n+#include \"tsan_vector_clock.h\"\n \n namespace __tsan {\n \n@@ -53,34 +54,18 @@ struct SyncVar {\n \n   uptr addr;  // overwritten by DenseSlabAlloc freelist\n   Mutex mtx;\n-  u64 uid;  // Globally unique id.\n   StackID creation_stack_id;\n   Tid owner_tid;  // Set only by exclusive owners.\n-  u64 last_lock;\n+  FastState last_lock;\n   int recursion;\n   atomic_uint32_t flags;\n   u32 next;  // in MetaMap\n   DDMutex dd;\n-  SyncClock read_clock;  // Used for rw mutexes only.\n-  // The clock is placed last, so that it is situated on a different cache line\n-  // with the mtx. This reduces contention for hot sync objects.\n-  SyncClock clock;\n+  VectorClock *read_clock;  // Used for rw mutexes only.\n+  VectorClock *clock;\n \n-  void Init(ThreadState *thr, uptr pc, uptr addr, u64 uid, bool save_stack);\n-  void Reset(Processor *proc);\n-\n-  u64 GetId() const {\n-    // 48 lsb is addr, then 14 bits is low part of uid, then 2 zero bits.\n-    return GetLsb((u64)addr | (uid << 48), 60);\n-  }\n-  bool CheckId(u64 uid) const {\n-    CHECK_EQ(uid, GetLsb(uid, 14));\n-    return GetLsb(this->uid, 14) == uid;\n-  }\n-  static uptr SplitId(u64 id, u64 *uid) {\n-    *uid = id >> 48;\n-    return (uptr)GetLsb(id, 48);\n-  }\n+  void Init(ThreadState *thr, uptr pc, uptr addr, bool save_stack);\n+  void Reset();\n \n   bool IsFlagSet(u32 f) const {\n     return atomic_load_relaxed(&flags) & f;\n@@ -110,9 +95,20 @@ class MetaMap {\n   MetaMap();\n \n   void AllocBlock(ThreadState *thr, uptr pc, uptr p, uptr sz);\n-  uptr FreeBlock(Processor *proc, uptr p);\n-  bool FreeRange(Processor *proc, uptr p, uptr sz);\n-  void ResetRange(Processor *proc, uptr p, uptr sz);\n+\n+  // FreeBlock resets all sync objects in the range if reset=true and must not\n+  // run concurrently with ResetClocks which resets all sync objects\n+  // w/o any synchronization (as part of DoReset).\n+  // If we don't have a thread slot (very early/late in thread lifetime or\n+  // Go/Java callbacks) or the slot is not locked, then reset must be set to\n+  // false. In such case sync object clocks will be reset later (when it's\n+  // reused or during the next ResetClocks).\n+  uptr FreeBlock(Processor *proc, uptr p, bool reset);\n+  bool FreeRange(Processor *proc, uptr p, uptr sz, bool reset);\n+  void ResetRange(Processor *proc, uptr p, uptr sz, bool reset);\n+  // Reset vector clocks of all sync objects.\n+  // Must be called when no other threads access sync objects.\n+  void ResetClocks();\n   MBlock* GetBlock(uptr p);\n \n   SyncVar *GetSyncOrCreate(ThreadState *thr, uptr pc, uptr addr,\n@@ -142,7 +138,6 @@ class MetaMap {\n   typedef DenseSlabAlloc<SyncVar, 1 << 20, 1 << 10, kFlagMask> SyncAlloc;\n   BlockAlloc block_alloc_;\n   SyncAlloc sync_alloc_;\n-  atomic_uint64_t uid_gen_;\n \n   SyncVar *GetSync(ThreadState *thr, uptr pc, uptr addr, bool create,\n                    bool save_stack);"}, {"sha": "01bb7b34f43a2c5b6e3f1ee93a44db51033bf739", "filename": "libsanitizer/tsan/tsan_trace.h", "status": "modified", "additions": 53, "deletions": 60, "changes": 113, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_trace.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Ftsan%2Ftsan_trace.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_trace.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -19,57 +19,6 @@\n \n namespace __tsan {\n \n-const int kTracePartSizeBits = 13;\n-const int kTracePartSize = 1 << kTracePartSizeBits;\n-const int kTraceParts = 2 * 1024 * 1024 / kTracePartSize;\n-const int kTraceSize = kTracePartSize * kTraceParts;\n-\n-// Must fit into 3 bits.\n-enum EventType {\n-  EventTypeMop,\n-  EventTypeFuncEnter,\n-  EventTypeFuncExit,\n-  EventTypeLock,\n-  EventTypeUnlock,\n-  EventTypeRLock,\n-  EventTypeRUnlock\n-};\n-\n-// Represents a thread event (from most significant bit):\n-// u64 typ  : 3;   // EventType.\n-// u64 addr : 61;  // Associated pc.\n-typedef u64 Event;\n-\n-const uptr kEventPCBits = 61;\n-\n-struct TraceHeader {\n-#if !SANITIZER_GO\n-  BufferedStackTrace stack0;  // Start stack for the trace.\n-#else\n-  VarSizeStackTrace stack0;\n-#endif\n-  u64        epoch0;  // Start epoch for the trace.\n-  MutexSet   mset0;\n-\n-  TraceHeader() : stack0(), epoch0() {}\n-};\n-\n-struct Trace {\n-  Mutex mtx;\n-#if !SANITIZER_GO\n-  // Must be last to catch overflow as paging fault.\n-  // Go shadow stack is dynamically allocated.\n-  uptr shadow_stack[kShadowStackSize];\n-#endif\n-  // Must be the last field, because we unmap the unused part in\n-  // CreateThreadContext.\n-  TraceHeader headers[kTraceParts];\n-\n-  Trace() : mtx(MutexTypeTrace) {}\n-};\n-\n-namespace v3 {\n-\n enum class EventType : u64 {\n   kAccessExt,\n   kAccessRange,\n@@ -99,6 +48,8 @@ static constexpr Event NopEvent = {1, 0, EventType::kAccessExt, 0};\n // close enough to each other. Otherwise we fall back to EventAccessExt.\n struct EventAccess {\n   static constexpr uptr kPCBits = 15;\n+  static_assert(kPCBits + kCompressedAddrBits + 5 == 64,\n+                \"unused bits in EventAccess\");\n \n   u64 is_access : 1;  // = 1\n   u64 is_read : 1;\n@@ -119,13 +70,23 @@ static_assert(sizeof(EventFunc) == 8, \"bad EventFunc size\");\n \n // Extended memory access with full PC.\n struct EventAccessExt {\n+  // Note: precisely specifying the unused parts of the bitfield is critical for\n+  // performance. If we don't specify them, compiler will generate code to load\n+  // the old value and shuffle it to extract the unused bits to apply to the new\n+  // value. If we specify the unused part and store 0 in there, all that\n+  // unnecessary code goes away (store of the 0 const is combined with other\n+  // constant parts).\n+  static constexpr uptr kUnusedBits = 11;\n+  static_assert(kCompressedAddrBits + kUnusedBits + 9 == 64,\n+                \"unused bits in EventAccessExt\");\n+\n   u64 is_access : 1;   // = 0\n   u64 is_func : 1;     // = 0\n   EventType type : 3;  // = EventType::kAccessExt\n   u64 is_read : 1;\n   u64 is_atomic : 1;\n   u64 size_log : 2;\n-  u64 _ : 11;\n+  u64 _ : kUnusedBits;\n   u64 addr : kCompressedAddrBits;\n   u64 pc;\n };\n@@ -134,6 +95,8 @@ static_assert(sizeof(EventAccessExt) == 16, \"bad EventAccessExt size\");\n // Access to a memory range.\n struct EventAccessRange {\n   static constexpr uptr kSizeLoBits = 13;\n+  static_assert(kCompressedAddrBits + kSizeLoBits + 7 == 64,\n+                \"unused bits in EventAccessRange\");\n \n   u64 is_access : 1;   // = 0\n   u64 is_func : 1;     // = 0\n@@ -150,36 +113,51 @@ static_assert(sizeof(EventAccessRange) == 16, \"bad EventAccessRange size\");\n // Mutex lock.\n struct EventLock {\n   static constexpr uptr kStackIDLoBits = 15;\n+  static constexpr uptr kStackIDHiBits =\n+      sizeof(StackID) * kByteBits - kStackIDLoBits;\n+  static constexpr uptr kUnusedBits = 3;\n+  static_assert(kCompressedAddrBits + kStackIDLoBits + 5 == 64,\n+                \"unused bits in EventLock\");\n+  static_assert(kCompressedAddrBits + kStackIDHiBits + kUnusedBits == 64,\n+                \"unused bits in EventLock\");\n \n   u64 is_access : 1;   // = 0\n   u64 is_func : 1;     // = 0\n   EventType type : 3;  // = EventType::kLock or EventType::kRLock\n   u64 pc : kCompressedAddrBits;\n   u64 stack_lo : kStackIDLoBits;\n   u64 stack_hi : sizeof(StackID) * kByteBits - kStackIDLoBits;\n-  u64 _ : 3;\n+  u64 _ : kUnusedBits;\n   u64 addr : kCompressedAddrBits;\n };\n static_assert(sizeof(EventLock) == 16, \"bad EventLock size\");\n \n // Mutex unlock.\n struct EventUnlock {\n+  static constexpr uptr kUnusedBits = 15;\n+  static_assert(kCompressedAddrBits + kUnusedBits + 5 == 64,\n+                \"unused bits in EventUnlock\");\n+\n   u64 is_access : 1;   // = 0\n   u64 is_func : 1;     // = 0\n   EventType type : 3;  // = EventType::kUnlock\n-  u64 _ : 15;\n+  u64 _ : kUnusedBits;\n   u64 addr : kCompressedAddrBits;\n };\n static_assert(sizeof(EventUnlock) == 8, \"bad EventUnlock size\");\n \n // Time change event.\n struct EventTime {\n+  static constexpr uptr kUnusedBits = 37;\n+  static_assert(kUnusedBits + sizeof(Sid) * kByteBits + kEpochBits + 5 == 64,\n+                \"unused bits in EventTime\");\n+\n   u64 is_access : 1;   // = 0\n   u64 is_func : 1;     // = 0\n   EventType type : 3;  // = EventType::kTime\n   u64 sid : sizeof(Sid) * kByteBits;\n   u64 epoch : kEpochBits;\n-  u64 _ : 64 - 5 - sizeof(Sid) * kByteBits - kEpochBits;\n+  u64 _ : kUnusedBits;\n };\n static_assert(sizeof(EventTime) == 8, \"bad EventTime size\");\n \n@@ -188,10 +166,12 @@ struct Trace;\n struct TraceHeader {\n   Trace* trace = nullptr;  // back-pointer to Trace containing this part\n   INode trace_parts;       // in Trace::parts\n+  INode global;            // in Contex::trace_part_recycle\n };\n \n struct TracePart : TraceHeader {\n-  static constexpr uptr kByteSize = 256 << 10;\n+  // There are a lot of goroutines in Go, so we use smaller parts.\n+  static constexpr uptr kByteSize = (SANITIZER_GO ? 128 : 256) << 10;\n   static constexpr uptr kSize =\n       (kByteSize - sizeof(TraceHeader)) / sizeof(Event);\n   // TraceAcquire does a fast event pointer overflow check by comparing\n@@ -209,13 +189,26 @@ static_assert(sizeof(TracePart) == TracePart::kByteSize, \"bad TracePart size\");\n struct Trace {\n   Mutex mtx;\n   IList<TraceHeader, &TraceHeader::trace_parts, TracePart> parts;\n-  Event* final_pos =\n-      nullptr;  // final position in the last part for finished threads\n+  // First node non-queued into ctx->trace_part_recycle.\n+  TracePart* local_head;\n+  // Final position in the last part for finished threads.\n+  Event* final_pos = nullptr;\n+  // Number of trace parts allocated on behalf of this trace specifically.\n+  // Total number of parts in this trace can be larger if we retake some\n+  // parts from other traces.\n+  uptr parts_allocated = 0;\n \n   Trace() : mtx(MutexTypeTrace) {}\n-};\n \n-}  // namespace v3\n+  // We need at least 3 parts per thread, because we want to keep at last\n+  // 2 parts per thread that are not queued into ctx->trace_part_recycle\n+  // (the current one being filled and one full part that ensures that\n+  // we always have at least one part worth of previous memory accesses).\n+  static constexpr uptr kMinParts = 3;\n+\n+  static constexpr uptr kFinishedThreadLo = 16;\n+  static constexpr uptr kFinishedThreadHi = 64;\n+};\n \n }  // namespace __tsan\n "}, {"sha": "a58ef0f17efa197bc2e6d9f253c7d0b8d3a509ca", "filename": "libsanitizer/tsan/tsan_update_shadow_word.inc", "status": "removed", "additions": 0, "deletions": 59, "changes": 59, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2285af309000b74da0f7dc756a0b55e5f0b1b56/libsanitizer%2Ftsan%2Ftsan_update_shadow_word.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2285af309000b74da0f7dc756a0b55e5f0b1b56/libsanitizer%2Ftsan%2Ftsan_update_shadow_word.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_update_shadow_word.inc?ref=e2285af309000b74da0f7dc756a0b55e5f0b1b56", "patch": "@@ -1,59 +0,0 @@\n-//===-- tsan_update_shadow_word.inc -----------------------------*- C++ -*-===//\n-//\n-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n-// See https://llvm.org/LICENSE.txt for license information.\n-// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n-//\n-//===----------------------------------------------------------------------===//\n-//\n-// This file is a part of ThreadSanitizer (TSan), a race detector.\n-//\n-// Body of the hottest inner loop.\n-// If we wrap this body into a function, compilers (both gcc and clang)\n-// produce sligtly less efficient code.\n-//===----------------------------------------------------------------------===//\n-do {\n-  const unsigned kAccessSize = 1 << kAccessSizeLog;\n-  u64 *sp = &shadow_mem[idx];\n-  old = LoadShadow(sp);\n-  if (LIKELY(old.IsZero())) {\n-    if (!stored) {\n-      StoreIfNotYetStored(sp, &store_word);\n-      stored = true;\n-    }\n-    break;\n-  }\n-  // is the memory access equal to the previous?\n-  if (LIKELY(Shadow::Addr0AndSizeAreEqual(cur, old))) {\n-    // same thread?\n-    if (LIKELY(Shadow::TidsAreEqual(old, cur))) {\n-      if (LIKELY(old.IsRWWeakerOrEqual(kAccessIsWrite, kIsAtomic))) {\n-        StoreIfNotYetStored(sp, &store_word);\n-        stored = true;\n-      }\n-      break;\n-    }\n-    if (HappensBefore(old, thr)) {\n-      if (old.IsRWWeakerOrEqual(kAccessIsWrite, kIsAtomic)) {\n-        StoreIfNotYetStored(sp, &store_word);\n-        stored = true;\n-      }\n-      break;\n-    }\n-    if (LIKELY(old.IsBothReadsOrAtomic(kAccessIsWrite, kIsAtomic)))\n-      break;\n-    goto RACE;\n-  }\n-  // Do the memory access intersect?\n-  if (Shadow::TwoRangesIntersect(old, cur, kAccessSize)) {\n-    if (Shadow::TidsAreEqual(old, cur))\n-      break;\n-    if (old.IsBothReadsOrAtomic(kAccessIsWrite, kIsAtomic))\n-      break;\n-    if (LIKELY(HappensBefore(old, thr)))\n-      break;\n-    goto RACE;\n-  }\n-  // The accesses do not intersect.\n-  break;\n-} while (0);"}, {"sha": "3673e66539d0e103e9ed6bb59f331da97dbc1c57", "filename": "libsanitizer/ubsan/ubsan_diag.cpp", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_diag.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_diag.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_diag.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -32,15 +32,13 @@ using namespace __ubsan;\n // Windows.\n // TODO(yln): This is a temporary workaround. GetStackTrace functions will be\n // removed in the future.\n-void ubsan_GetStackTrace(BufferedStackTrace *stack, uptr max_depth,\n-                         uptr pc, uptr bp, void *context, bool fast) {\n+void ubsan_GetStackTrace(BufferedStackTrace *stack, uptr max_depth, uptr pc,\n+                         uptr bp, void *context, bool request_fast) {\n   uptr top = 0;\n   uptr bottom = 0;\n-  if (StackTrace::WillUseFastUnwind(fast)) {\n-    GetThreadStackTopAndBottom(false, &top, &bottom);\n-    stack->Unwind(max_depth, pc, bp, nullptr, top, bottom, true);\n-  } else\n-    stack->Unwind(max_depth, pc, bp, context, 0, 0, false);\n+  GetThreadStackTopAndBottom(false, &top, &bottom);\n+  bool fast = StackTrace::WillUseFastUnwind(request_fast);\n+  stack->Unwind(max_depth, pc, bp, context, top, bottom, fast);\n }\n \n static void MaybePrintStackTrace(uptr pc, uptr bp) {"}, {"sha": "25cefd46ce27ced7fb6092d8d04b5074c56ebe95", "filename": "libsanitizer/ubsan/ubsan_flags.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_flags.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_flags.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_flags.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -50,7 +50,6 @@ void InitializeFlags() {\n   {\n     CommonFlags cf;\n     cf.CopyFrom(*common_flags());\n-    cf.print_summary = false;\n     cf.external_symbolizer_path = GetFlag(\"UBSAN_SYMBOLIZER_PATH\");\n     OverrideCommonFlags(cf);\n   }"}, {"sha": "e201e6bba22078e3d873aeb5792b98ecd860cdaa", "filename": "libsanitizer/ubsan/ubsan_handlers.cpp", "status": "modified", "additions": 0, "deletions": 15, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_handlers.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_handlers.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_handlers.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -894,21 +894,6 @@ void __ubsan_handle_cfi_bad_type(CFICheckFailData *Data, ValueHandle Vtable,\n \n }  // namespace __ubsan\n \n-void __ubsan::__ubsan_handle_cfi_bad_icall(CFIBadIcallData *CallData,\n-                                           ValueHandle Function) {\n-  GET_REPORT_OPTIONS(false);\n-  CFICheckFailData Data = {CFITCK_ICall, CallData->Loc, CallData->Type};\n-  handleCFIBadIcall(&Data, Function, Opts);\n-}\n-\n-void __ubsan::__ubsan_handle_cfi_bad_icall_abort(CFIBadIcallData *CallData,\n-                                                 ValueHandle Function) {\n-  GET_REPORT_OPTIONS(true);\n-  CFICheckFailData Data = {CFITCK_ICall, CallData->Loc, CallData->Type};\n-  handleCFIBadIcall(&Data, Function, Opts);\n-  Die();\n-}\n-\n void __ubsan::__ubsan_handle_cfi_check_fail(CFICheckFailData *Data,\n                                             ValueHandle Value,\n                                             uptr ValidVtable) {"}, {"sha": "219fb15de55fe02a4544422095baa0d8532baaa0", "filename": "libsanitizer/ubsan/ubsan_handlers.h", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_handlers.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_handlers.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_handlers.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -215,20 +215,12 @@ enum CFITypeCheckKind : unsigned char {\n   CFITCK_VMFCall,\n };\n \n-struct CFIBadIcallData {\n-  SourceLocation Loc;\n-  const TypeDescriptor &Type;\n-};\n-\n struct CFICheckFailData {\n   CFITypeCheckKind CheckKind;\n   SourceLocation Loc;\n   const TypeDescriptor &Type;\n };\n \n-/// \\brief Handle control flow integrity failure for indirect function calls.\n-RECOVERABLE(cfi_bad_icall, CFIBadIcallData *Data, ValueHandle Function)\n-\n /// \\brief Handle control flow integrity failures.\n RECOVERABLE(cfi_check_fail, CFICheckFailData *Data, ValueHandle Function,\n             uptr VtableIsValid)"}, {"sha": "fd534c2573f6dcf74f8246eff85d11c0ad0d420a", "filename": "libsanitizer/ubsan/ubsan_handlers_cxx.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_handlers_cxx.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_handlers_cxx.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_handlers_cxx.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -51,4 +51,4 @@ __ubsan_handle_function_type_mismatch_v1_abort(FunctionTypeMismatchData *Data,\n                                                ValueHandle fnRTTI);\n }\n \n-#endif // UBSAN_HANDLERS_H\n+#endif // UBSAN_HANDLERS_CXX_H"}, {"sha": "5802d58896f0fe8066bd707b3e760d3dbd7a4e82", "filename": "libsanitizer/ubsan/ubsan_init.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_init.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_init.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_init.cpp?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -12,13 +12,14 @@\n \n #include \"ubsan_platform.h\"\n #if CAN_SANITIZE_UB\n-#include \"ubsan_diag.h\"\n-#include \"ubsan_init.h\"\n-#include \"ubsan_flags.h\"\n #include \"sanitizer_common/sanitizer_common.h\"\n+#include \"sanitizer_common/sanitizer_interface_internal.h\"\n #include \"sanitizer_common/sanitizer_libc.h\"\n #include \"sanitizer_common/sanitizer_mutex.h\"\n #include \"sanitizer_common/sanitizer_symbolizer.h\"\n+#include \"ubsan_diag.h\"\n+#include \"ubsan_flags.h\"\n+#include \"ubsan_init.h\"\n \n using namespace __ubsan;\n "}, {"sha": "d2cc2e10bd2f023b8d9aa1685a79a192a6d1e1e8", "filename": "libsanitizer/ubsan/ubsan_platform.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_platform.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f732bf6a603721f61102a08ad2d023c7c2670870/libsanitizer%2Fubsan%2Fubsan_platform.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_platform.h?ref=f732bf6a603721f61102a08ad2d023c7c2670870", "patch": "@@ -12,7 +12,6 @@\n #ifndef UBSAN_PLATFORM_H\n #define UBSAN_PLATFORM_H\n \n-#ifndef CAN_SANITIZE_UB\n // Other platforms should be easy to add, and probably work as-is.\n #if defined(__linux__) || defined(__FreeBSD__) || defined(__APPLE__) ||        \\\n     defined(__NetBSD__) || defined(__DragonFly__) ||                           \\\n@@ -22,6 +21,5 @@\n #else\n # define CAN_SANITIZE_UB 0\n #endif\n-#endif //CAN_SANITIZE_UB\n \n #endif"}]}