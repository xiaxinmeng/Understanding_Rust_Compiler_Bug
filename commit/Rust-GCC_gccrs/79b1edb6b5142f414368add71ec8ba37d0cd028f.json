{"sha": "79b1edb6b5142f414368add71ec8ba37d0cd028f", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NzliMWVkYjZiNTE0MmY0MTQzNjhhZGQ3MWVjOGJhMzdkMGNkMDI4Zg==", "commit": {"author": {"name": "Richard Henderson", "email": "rth@redhat.com", "date": "2011-11-09T22:54:55Z"}, "committer": {"name": "Richard Henderson", "email": "rth@gcc.gnu.org", "date": "2011-11-09T22:54:55Z"}, "message": "libitm: Remove unused code.\n\nIn particular, unused code that's presenting portability problems.\n\nFrom-SVN: r181241", "tree": {"sha": "b83ad479a56bee69877750fe78fab6e95bcec8c3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/b83ad479a56bee69877750fe78fab6e95bcec8c3"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/79b1edb6b5142f414368add71ec8ba37d0cd028f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/79b1edb6b5142f414368add71ec8ba37d0cd028f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/79b1edb6b5142f414368add71ec8ba37d0cd028f", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/79b1edb6b5142f414368add71ec8ba37d0cd028f/comments", "author": null, "committer": null, "parents": [{"sha": "cb8010f922d1d8f39a3035ffcce56dfca32fb822", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cb8010f922d1d8f39a3035ffcce56dfca32fb822", "html_url": "https://github.com/Rust-GCC/gccrs/commit/cb8010f922d1d8f39a3035ffcce56dfca32fb822"}], "stats": {"total": 2708, "additions": 40, "deletions": 2668}, "files": [{"sha": "b1629b1a89e32667bd9e38d3761aa97899aca6e1", "filename": "libitm/ChangeLog", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2FChangeLog?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -1,3 +1,26 @@\n+2011-11-09  Richard Henderson  <rth@redhat.com>\n+\n+\t* barrier.tpl, memcpy.cc, memset.cc, method-wbetl.cc: Remove file.\n+\t* config/alpha/unaligned.h: Remove file.\n+\t* config/generic/unaligned.h: Remove file.\n+\t* config/x86/unaligned.h: Remove file.\n+\t* config/generic/cachepage.h: Remove file.\n+\t* config/posix/cachepage.cc: Remove file.\n+\t* config/generic/cacheline.cc: Remove file.\n+\t* config/x86/cacheline.cc: Remove file.\n+\t* config/generic/cacheline.h (gtm_cacheline): Remove the\n+\tstore_mask, copy_mask, copy_mask_wb methods.\n+\t* config/x86/cacheline.h: Likewise.\n+\t* config/alpha/cacheline.h: Fall back to generic after setting size.\n+\t* config/generic/tls.cc (gtm_mask_stack): Remove.\n+\t* config/x86/x86_avx.cc (GTM_vpperm_shift): Remove.\n+\t(GTM_vpalignr_table): Remove.\n+\t* config/x86/x86_sse.cc (GTM_palignr_table): Remove.\n+\t(GTM_pshift_table): Remove.\n+\t* config/libitm_i.h: Don't include cachepage.h.\n+\t* Makefile.am (libitm_la_SOURCES): Remove cacheline.cc, cachepage.cc\n+\t* Makefile.in, testsuite/Makefile.in: Rebuild.\n+\n 2011-11-09  Richard Henderson  <rth@redhat.com>\n \n \t* config/x86/cacheline.h (gtm_cacheline::store_mask): Use .byte"}, {"sha": "45789866b4f216997e4860d97d430cbcb5dd4fd4", "filename": "libitm/Makefile.am", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2FMakefile.am", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2FMakefile.am", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2FMakefile.am?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -41,7 +41,7 @@ libitm_la_LDFLAGS = $(libitm_version_info) $(libitm_version_script) \\\n \n libitm_la_SOURCES = \\\n \taatree.cc alloc.cc alloc_c.cc alloc_cpp.cc barrier.cc beginend.cc \\\n-\tclone.cc cacheline.cc cachepage.cc eh_cpp.cc local.cc \\\n+\tclone.cc eh_cpp.cc local.cc \\\n \tquery.cc retry.cc rwlock.cc useraction.cc util.cc \\\n \tsjlj.S tls.cc method-serial.cc method-gl.cc\n "}, {"sha": "8816580fb65c8ea108cfb530a3d56151ff8faa98", "filename": "libitm/Makefile.in", "status": "modified", "additions": 12, "deletions": 17, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2FMakefile.in?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -48,6 +48,7 @@ DIST_COMMON = $(am__configure_deps) $(srcdir)/../config.guess \\\n \t$(top_srcdir)/configure ChangeLog\n ACLOCAL_M4 = $(top_srcdir)/aclocal.m4\n am__aclocal_m4_deps = $(top_srcdir)/../config/acx.m4 \\\n+\t$(top_srcdir)/../config/asmcfi.m4 \\\n \t$(top_srcdir)/../config/depstand.m4 \\\n \t$(top_srcdir)/../config/enable.m4 \\\n \t$(top_srcdir)/../config/futex.m4 \\\n@@ -94,17 +95,17 @@ am__installdirs = \"$(DESTDIR)$(toolexeclibdir)\" \"$(DESTDIR)$(infodir)\" \\\n LTLIBRARIES = $(toolexeclib_LTLIBRARIES)\n libitm_la_LIBADD =\n am__libitm_la_SOURCES_DIST = aatree.cc alloc.cc alloc_c.cc \\\n-\talloc_cpp.cc barrier.cc beginend.cc clone.cc cacheline.cc \\\n-\tcachepage.cc eh_cpp.cc local.cc query.cc retry.cc rwlock.cc \\\n-\tuseraction.cc util.cc sjlj.S tls.cc method-serial.cc \\\n-\tmethod-gl.cc x86_sse.cc x86_avx.cc futex.cc\n+\talloc_cpp.cc barrier.cc beginend.cc clone.cc eh_cpp.cc \\\n+\tlocal.cc query.cc retry.cc rwlock.cc useraction.cc util.cc \\\n+\tsjlj.S tls.cc method-serial.cc method-gl.cc x86_sse.cc \\\n+\tx86_avx.cc futex.cc\n @ARCH_X86_TRUE@am__objects_1 = x86_sse.lo x86_avx.lo\n @ARCH_FUTEX_TRUE@am__objects_2 = futex.lo\n am_libitm_la_OBJECTS = aatree.lo alloc.lo alloc_c.lo alloc_cpp.lo \\\n-\tbarrier.lo beginend.lo clone.lo cacheline.lo cachepage.lo \\\n-\teh_cpp.lo local.lo query.lo retry.lo rwlock.lo useraction.lo \\\n-\tutil.lo sjlj.lo tls.lo method-serial.lo method-gl.lo \\\n-\t$(am__objects_1) $(am__objects_2)\n+\tbarrier.lo beginend.lo clone.lo eh_cpp.lo local.lo query.lo \\\n+\tretry.lo rwlock.lo useraction.lo util.lo sjlj.lo tls.lo \\\n+\tmethod-serial.lo method-gl.lo $(am__objects_1) \\\n+\t$(am__objects_2)\n libitm_la_OBJECTS = $(am_libitm_la_OBJECTS)\n DEFAULT_INCLUDES = -I.@am__isrc@\n depcomp = $(SHELL) $(top_srcdir)/../depcomp\n@@ -234,8 +235,6 @@ ECHO_N = @ECHO_N@\n ECHO_T = @ECHO_T@\n EGREP = @EGREP@\n EXEEXT = @EXEEXT@\n-FC = @FC@\n-FCFLAGS = @FCFLAGS@\n FGREP = @FGREP@\n GREP = @GREP@\n INSTALL = @INSTALL@\n@@ -286,7 +285,6 @@ abs_top_srcdir = @abs_top_srcdir@\n ac_ct_CC = @ac_ct_CC@\n ac_ct_CXX = @ac_ct_CXX@\n ac_ct_DUMPBIN = @ac_ct_DUMPBIN@\n-ac_ct_FC = @ac_ct_FC@\n am__include = @am__include@\n am__leading_dot = @am__leading_dot@\n am__quote = @am__quote@\n@@ -371,10 +369,9 @@ libitm_la_LDFLAGS = $(libitm_version_info) $(libitm_version_script) \\\n         -no-undefined\n \n libitm_la_SOURCES = aatree.cc alloc.cc alloc_c.cc alloc_cpp.cc \\\n-\tbarrier.cc beginend.cc clone.cc cacheline.cc cachepage.cc \\\n-\teh_cpp.cc local.cc query.cc retry.cc rwlock.cc useraction.cc \\\n-\tutil.cc sjlj.S tls.cc method-serial.cc method-gl.cc \\\n-\t$(am__append_1) $(am__append_2)\n+\tbarrier.cc beginend.cc clone.cc eh_cpp.cc local.cc query.cc \\\n+\tretry.cc rwlock.cc useraction.cc util.cc sjlj.S tls.cc \\\n+\tmethod-serial.cc method-gl.cc $(am__append_1) $(am__append_2)\n \n # Automake Documentation:\n # If your package has Texinfo files in many directories, you can use the\n@@ -500,8 +497,6 @@ distclean-compile:\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/alloc_cpp.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/barrier.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/beginend.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/cacheline.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/cachepage.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/clone.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/eh_cpp.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/futex.Plo@am__quote@"}, {"sha": "dcf101356e673b208f302e03b87416e94483be00", "filename": "libitm/barrier.tpl", "status": "removed", "additions": 0, "deletions": 170, "changes": 170, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fbarrier.tpl", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fbarrier.tpl", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fbarrier.tpl?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,170 +0,0 @@\n-/* -*- c++ -*- */\n-/* Copyright (C) 2008, 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#include \"unaligned.h\"\n-\n-namespace {\n-\n-using namespace GTM;\n-\n-template<typename T>\n-T do_read (const T *ptr, abi_dispatch::lock_type lock)\n-{\n-  //\n-  // Find the cacheline that holds the current value of *PTR.\n-  //\n-  abi_dispatch *disp = abi_disp();\n-  uintptr_t iptr = reinterpret_cast<uintptr_t>(ptr);\n-  // Normalize PTR by chopping off the bottom bits so we can search\n-  // for PTR in the cacheline hash.\n-  uintptr_t iline = iptr & -CACHELINE_SIZE;\n-  // The position in the resulting cacheline where *PTR is actually stored.\n-  uintptr_t iofs = iptr & (CACHELINE_SIZE - 1);\n-  const gtm_cacheline *pline = reinterpret_cast<const gtm_cacheline *>(iline);\n-  // Search for the actual cacheline that holds the current value of *PTR.\n-  const gtm_cacheline *line = disp->read_lock(pline, lock);\n-\n-  // Point to the position in the cacheline where *PTR is stored.\n-  ptr = reinterpret_cast<const T *>(&line->b[iofs]);\n-\n-  // Straight up loads, because we're either aligned, or we don't care\n-  // about alignment.\n-  //\n-  // If we require alignment on type T, do a straight load if we're\n-  // aligned.  Otherwise do a straight load IFF the load fits entirely\n-  // in this cacheline.  That is, it won't span multiple cachelines.\n-  if (__builtin_expect (strict_alignment<T>::value\n-\t\t\t? (iofs & (sizeof (T) - 1)) == 0\n-\t\t\t: iofs + sizeof(T) <= CACHELINE_SIZE, 1))\n-    {\n-    do_normal_load:\n-      return *ptr;\n-    }\n-  // If alignment on T is necessary, but we're unaligned, yet we fit\n-  // entirely in this cacheline... do the unaligned load dance.\n-  else if (__builtin_expect (strict_alignment<T>::value\n-\t\t\t     && iofs + sizeof(T) <= CACHELINE_SIZE, 1))\n-    {\n-    do_unaligned_load:\n-      return unaligned_load<T>(ptr);\n-    }\n-  // Otherwise, this load will span multiple cachelines.\n-  else\n-    {\n-      // Get the following cacheline for the rest of the data.\n-      const gtm_cacheline *line2 = disp->read_lock(pline + 1, lock);\n-\n-      // If the two cachelines are adjacent, just load it all in one\n-      // swoop.\n-      if (line2 == line + 1)\n-\t{\n-\t  if (!strict_alignment<T>::value)\n-\t    goto do_normal_load;\n-\t  else\n-\t    goto do_unaligned_load;\n-\t}\n-      else\n-\t{\n-\t  // Otherwise, ask the backend to load from two different\n-\t  // cachelines.\n-\t  return unaligned_load2<T>(line, line2, iofs);\n-\t}\n-    }\n-}\n-\n-template<typename T>\n-void do_write (T *ptr, T val, abi_dispatch::lock_type lock)\n-{\n-  // Note: See comments for do_read() above for hints on this\n-  // function.  Ideally we should abstract out a lot out of these two\n-  // functions, and avoid all this duplication.\n-\n-  abi_dispatch *disp = abi_disp();\n-  uintptr_t iptr = reinterpret_cast<uintptr_t>(ptr);\n-  uintptr_t iline = iptr & -CACHELINE_SIZE;\n-  uintptr_t iofs = iptr & (CACHELINE_SIZE - 1);\n-  gtm_cacheline *pline = reinterpret_cast<gtm_cacheline *>(iline);\n-  gtm_cacheline_mask m = ((gtm_cacheline_mask)2 << (sizeof(T) - 1)) - 1;\n-  abi_dispatch::mask_pair pair = disp->write_lock(pline, lock);\n-\n-  ptr = reinterpret_cast<T *>(&pair.line->b[iofs]);\n-\n-  if (__builtin_expect (strict_alignment<T>::value\n-\t\t\t? (iofs & (sizeof (val) - 1)) == 0\n-\t\t\t: iofs + sizeof(val) <= CACHELINE_SIZE, 1))\n-    {\n-      *pair.mask |= m << iofs;\n-    do_normal_store:\n-      *ptr = val;\n-    }\n-  else if (__builtin_expect (strict_alignment<T>::value\n-\t\t\t     && iofs + sizeof(val) <= CACHELINE_SIZE, 1))\n-    {\n-      *pair.mask |= m << iofs;\n-    do_unaligned_store:\n-      unaligned_store<T>(ptr, val);\n-    }\n-  else\n-    {\n-      *pair.mask |= m << iofs;\n-      abi_dispatch::mask_pair pair2 = disp->write_lock(pline + 1, lock);\n-\n-      uintptr_t ileft = CACHELINE_SIZE - iofs;\n-      *pair2.mask |= m >> ileft;\n-\n-      if (pair2.line == pair.line + 1)\n-\t{\n-\t  if (!strict_alignment<T>::value)\n-\t    goto do_normal_store;\n-\t  else\n-\t    goto do_unaligned_store;\n-\t}\n-      else\n-\tunaligned_store2<T>(pair.line, pair2.line, iofs, val);\n-    }\n-}\n-\n-} /* anonymous namespace */\n-\n-#define ITM_READ(T, LOCK)\t\t\t\t\t\t\\\n-  _ITM_TYPE_##T ITM_REGPARM _ITM_##LOCK##T (const _ITM_TYPE_##T *ptr)\t\\\n-  {\t\t\t\t\t\t\t\t\t\\\n-    return do_read (ptr, abi_dispatch::LOCK);\t\t\t\t\\\n-  }\n-\n-#define ITM_WRITE(T, LOCK)\t\t\t\t\t\t\\\n-  void ITM_REGPARM _ITM_##LOCK##T (_ITM_TYPE_##T *ptr, _ITM_TYPE_##T val) \\\n-  {\t\t\t\t\t\t\t\t\t\\\n-    do_write (ptr, val, abi_dispatch::LOCK);\t\t\t\t\\\n-  }\n-\n-#define ITM_BARRIERS(T)\t\t\\\n-  ITM_READ(T, R)\t\t\\\n-  ITM_READ(T, RaR)\t\t\\\n-  ITM_READ(T, RaW)\t\t\\\n-  ITM_READ(T, RfW)\t\t\\\n-  ITM_WRITE(T, W)\t\t\\\n-  ITM_WRITE(T, WaR)\t\t\\\n-  ITM_WRITE(T, WaW)"}, {"sha": "611a1c9a26e4b338b8cb37cf66ef620a722bc9a4", "filename": "libitm/config/alpha/cacheline.h", "status": "modified", "additions": 1, "deletions": 85, "changes": 86, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Falpha%2Fcacheline.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Falpha%2Fcacheline.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Falpha%2Fcacheline.h?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -33,90 +33,6 @@\n // modification mask, below.\n #define CACHELINE_SIZE 64\n \n-#ifdef __alpha_bwx__\n-# include \"config/generic/cacheline.h\"\n-#else\n-// If we don't have byte-word stores, then we'll never be able to\n-// adjust *all* of the byte loads/stores to be truely atomic.  So\n-// only guarantee 4-byte aligned values atomicly stored, exactly\n-// like the native system.  Use byte zap instructions to accelerate\n-// sub-word masked stores.\n+#include \"config/generic/cacheline.h\"\n \n-namespace GTM HIDDEN {\n-\n-// A gtm_cacheline_mask stores a modified bit for every modified byte\n-// in the cacheline with which it is associated.\n-typedef sized_integral<CACHELINE_SIZE / 8>::type gtm_cacheline_mask;\n-\n-union gtm_cacheline\n-{\n-  // Byte access to the cacheline.\n-  unsigned char b[CACHELINE_SIZE] __attribute__((aligned(CACHELINE_SIZE)));\n-\n-  // Larger sized access to the cacheline.\n-  uint16_t u16[CACHELINE_SIZE / sizeof(uint16_t)];\n-  uint32_t u32[CACHELINE_SIZE / sizeof(uint32_t)];\n-  uint64_t u64[CACHELINE_SIZE / sizeof(uint64_t)];\n-  gtm_word w[CACHELINE_SIZE / sizeof(gtm_word)];\n-\n-  // Store S into D, but only the bytes specified by M.\n-  static void store_mask(uint32_t *d, uint32_t s, uint8_t m);\n-  static void store_mask(uint64_t *d, uint64_t s, uint8_t m);\n-\n-  // Copy S to D, but only the bytes specified by M.\n-  static void copy_mask (gtm_cacheline * __restrict d,\n-\t\t\t const gtm_cacheline * __restrict s,\n-\t\t\t gtm_cacheline_mask m);\n-\n-  // A write barrier to emit after (a series of) copy_mask.\n-  static void copy_mask_wb () { atomic_write_barrier(); }\n-};\n-\n-inline void ALWAYS_INLINE\n-gtm_cacheline::store_mask (uint32_t *d, uint32_t s, uint8_t m)\n-{\n-  const uint8_t tm = (1 << sizeof(uint32_t)) - 1;\n-\n-  m &= tm;\n-  if (__builtin_expect (m, tm))\n-    {\n-      if (__builtin_expect (m == tm, 1))\n-\t*d = s;\n-      else\n-\t*d = __builtin_alpha_zap (*d, m) | __builtin_alpha_zapnot (s, m);\n-    }\n-}\n-\n-inline void ALWAYS_INLINE\n-gtm_cacheline::store_mask (uint64_t *d, uint64_t s, uint8_t m)\n-{\n-  if (__builtin_expect (m, 0xff))\n-    {\n-      if (__builtin_expect (m == 0xff, 1))\n-\t*d = s;\n-      else\n-\t{\n-\t  typedef uint32_t *p32 __attribute__((may_alias));\n-\t  p32 d32 = reinterpret_cast<p32>(d);\n-\n-\t  if ((m & 0x0f) == 0x0f)\n-\t    {\n-\t      d32[0] = s;\n-\t      m &= 0xf0;\n-\t    }\n-\t  else if ((m & 0xf0) == 0xf0)\n-\t    {\n-\t      d32[1] = s >> 32;\n-\t      m &= 0x0f;\n-\t    }\n-\n-\t  if (m)\n-\t    *d = __builtin_alpha_zap (*d, m) | __builtin_alpha_zapnot (s, m);\n-\t}\n-    }\n-}\n-\n-} // namespace GTM\n-\n-#endif // __alpha_bwx__\n #endif // LIBITM_ALPHA_CACHELINE_H"}, {"sha": "3d091aee22838d178edbfba0fd0ca9c14ccf1cdb", "filename": "libitm/config/alpha/unaligned.h", "status": "removed", "additions": 0, "deletions": 118, "changes": 118, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Falpha%2Funaligned.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Falpha%2Funaligned.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Falpha%2Funaligned.h?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,118 +0,0 @@\n-/* Copyright (C) 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#ifndef LIBITM_ALPHA_UNALIGNED_H\n-#define LIBITM_ALPHA_UNALIGNED_H 1\n-\n-#define HAVE_ARCH_UNALIGNED_LOAD2_U2 1\n-#define HAVE_ARCH_UNALIGNED_LOAD2_U4 1\n-#define HAVE_ARCH_UNALIGNED_LOAD2_U8 1\n-\n-#ifndef __alpha_bwx__\n-#define HAVE_ARCH_UNALIGNED_STORE2_U2 1\n-#endif\n-#define HAVE_ARCH_UNALIGNED_STORE2_U4 1\n-#define HAVE_ARCH_UNALIGNED_STORE2_U8 1\n-\n-#include \"config/generic/unaligned.h\"\n-\n-namespace GTM HIDDEN {\n-\n-template<>\n-inline uint16_t ALWAYS_INLINE\n-unaligned_load2<uint16_t>(const gtm_cacheline *c1,\n-\t\t\t  const gtm_cacheline *c2, size_t ofs)\n-{\n-  uint64_t v1 = c1->u64[CACHELINE_SIZE / sizeof(uint64_t) - 1];\n-  uint64_t v2 = c2->u64[0];\n-\n-  return __builtin_alpha_extwl (v1, ofs) | __builtin_alpha_extwh (v2, ofs);\n-}\n-\n-template<>\n-inline uint32_t ALWAYS_INLINE\n-unaligned_load2<uint32_t>(const gtm_cacheline *c1,\n-\t\t\t  const gtm_cacheline *c2, size_t ofs)\n-{\n-  uint64_t v1 = c1->u64[CACHELINE_SIZE / sizeof(uint64_t) - 1];\n-  uint64_t v2 = c2->u64[0];\n-\n-  return __builtin_alpha_extll (v1, ofs) + __builtin_alpha_extlh (v2, ofs);\n-}\n-\n-template<>\n-inline uint64_t ALWAYS_INLINE\n-unaligned_load2<uint64_t>(const gtm_cacheline *c1,\n-\t\t\t  const gtm_cacheline *c2, size_t ofs)\n-{\n-  uint64_t v1 = c1->u64[CACHELINE_SIZE / sizeof(uint64_t) - 1];\n-  uint64_t v2 = c2->u64[0];\n-\n-  return __builtin_alpha_extql (v1, ofs) | __builtin_alpha_extqh (v2, ofs);\n-}\n-\n-#ifndef __alpha_bwx__\n-template<>\n-inline void\n-unaligned_store2<uint16_t>(gtm_cacheline *c1, gtm_cacheline *c2,\n-\t\t\t   size_t ofs, uint16_t val)\n-{\n-  uint32_t vl = (uint32_t)val << 24, vh = val >> 8;\n-\n-  gtm_cacheline::store_mask (&c1->u32[CACHELINE_SIZE / 4 - 1], vl, 4);\n-  gtm_cacheline::store_mask (&c2->u32[0], vh, 1);\n-}\n-#endif\n-\n-template<>\n-inline void\n-unaligned_store2<uint32_t>(gtm_cacheline *c1, gtm_cacheline *c2,\n-\t\t\t   size_t ofs, uint32_t val)\n-{\n-  uint64_t vl = __builtin_alpha_insll (val, ofs);\n-  uint64_t ml = __builtin_alpha_insll (~0u, ofs);\n-  uint64_t vh = __builtin_alpha_inslh (val, ofs);\n-  uint64_t mh = __builtin_alpha_inslh (~0u, ofs);\n-\n-  gtm_cacheline::store_mask (&c1->u64[CACHELINE_SIZE / 8 - 1], vl, ml);\n-  gtm_cacheline::store_mask (&c2->u64[0], vh, mh);\n-}\n-\n-template<>\n-inline void\n-unaligned_store2<uint64_t>(gtm_cacheline *c1, gtm_cacheline *c2,\n-\t\t\t   size_t ofs, uint64_t val)\n-{\n-  uint64_t vl = __builtin_alpha_insql (val, ofs);\n-  uint64_t ml = __builtin_alpha_insql (~0u, ofs);\n-  uint64_t vh = __builtin_alpha_insqh (val, ofs);\n-  uint64_t mh = __builtin_alpha_insqh (~0u, ofs);\n-\n-  gtm_cacheline::store_mask (&c1->u64[CACHELINE_SIZE / 8 - 1], vl, ml);\n-  gtm_cacheline::store_mask (&c2->u64[0], vh, mh);\n-}\n-\n-} // namespace GTM\n-\n-#endif // LIBITM_ALPHA_UNALIGNED_H"}, {"sha": "108ffba303785a24e018241c137b5855208a161b", "filename": "libitm/config/generic/cacheline.cc", "status": "removed", "additions": 0, "deletions": 49, "changes": 49, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fgeneric%2Fcacheline.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fgeneric%2Fcacheline.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fgeneric%2Fcacheline.cc?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,49 +0,0 @@\n-/* Copyright (C) 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#include \"libitm_i.h\"\n-\n-\n-namespace GTM HIDDEN {\n-\n-void\n-gtm_cacheline::copy_mask (gtm_cacheline * __restrict d,\n-\t\t\t  const gtm_cacheline * __restrict s,\n-\t\t\t  gtm_cacheline_mask m)\n-{\n-  const size_t n = sizeof (gtm_word);\n-\n-  if (m == (gtm_cacheline_mask) -1)\n-    {\n-      *d = *s;\n-      return;\n-    }\n-  if (__builtin_expect (m == 0, 0))\n-    return;\n-\n-  for (size_t i = 0; i < CACHELINE_SIZE / n; ++i, m >>= n)\n-    store_mask (&d->w[i], s->w[i], m);\n-}\n-\n-} // namespace GTM"}, {"sha": "dd7d877d1d16c6de46f9238bb0853536414aaa3f", "filename": "libitm/config/generic/cacheline.h", "status": "modified", "additions": 0, "deletions": 49, "changes": 49, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Fgeneric%2Fcacheline.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Fgeneric%2Fcacheline.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fgeneric%2Fcacheline.h?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -51,57 +51,8 @@ union gtm_cacheline\n   uint32_t u32[CACHELINE_SIZE / sizeof(uint32_t)];\n   uint64_t u64[CACHELINE_SIZE / sizeof(uint64_t)];\n   gtm_word w[CACHELINE_SIZE / sizeof(gtm_word)];\n-\n-  // Store S into D, but only the bytes specified by M.\n-  template<typename T> static void store_mask (T *d, T s, uint8_t m);\n-\n-  // Copy S to D, but only the bytes specified by M.\n-  static void copy_mask (gtm_cacheline * __restrict d,\n-\t\t\t const gtm_cacheline * __restrict s,\n-\t\t\t gtm_cacheline_mask m);\n-\n-  // A write barrier to emit after (a series of) copy_mask.\n-  // When we're emitting non-temporal stores, the normal strong\n-  // ordering of the machine doesn't apply.\n-  static void copy_mask_wb () { atomic_write_barrier(); }\n };\n \n-template<typename T>\n-inline void\n-gtm_cacheline::store_mask (T *d, T s, uint8_t m)\n-{\n-  const uint8_t tm = (1 << sizeof(T)) - 1;\n-\n-  if (__builtin_expect (m & tm, tm))\n-    {\n-      if (__builtin_expect ((m & tm) == tm, 1))\n-\t*d = s;\n-      else\n-\t{\n-\t  const int half = sizeof(T) / 2;\n-\t  typedef typename sized_integral<half>::type half_t;\n-\t  half_t *dhalf = reinterpret_cast<half_t *>(d);\n-\t  half_t s1, s2;\n-\n-\t  if (WORDS_BIGENDIAN)\n-\t    s1 = s >> half*8, s2 = s;\n-\t  else\n-\t    s1 = s, s2 = s >> half*8;\n-\n-\t  store_mask (dhalf, s1, m);\n-\t  store_mask (dhalf + 1, s2, m >> half);\n-\t}\n-    }\n-}\n-\n-template<>\n-inline void ALWAYS_INLINE\n-gtm_cacheline::store_mask<uint8_t> (uint8_t *d, uint8_t s, uint8_t m)\n-{\n-  if (m & 1)\n-    *d = s;\n-}\n-\n } // namespace GTM\n \n #endif // LIBITM_CACHELINE_H"}, {"sha": "a5472f3831b13feb67d4270f52188ff048391b0c", "filename": "libitm/config/generic/cachepage.h", "status": "removed", "additions": 0, "deletions": 77, "changes": 77, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fgeneric%2Fcachepage.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fgeneric%2Fcachepage.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fgeneric%2Fcachepage.h?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,77 +0,0 @@\n-/* Copyright (C) 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#ifndef LIBITM_CACHEPAGE_H\n-#define LIBITM_CACHEPAGE_H 1\n-\n-namespace GTM HIDDEN {\n-\n-// A \"page\" worth of saved cachelines plus modification masks.  This\n-// arrangement is intended to minimize the overhead of alignment.  The\n-// PAGE_SIZE defined by the target must be a constant for this to work,\n-// which means that this definition may not be the same as the real\n-// system page size.  An additional define of FIXED_PAGE_SIZE by the\n-// target indicates that PAGE_SIZE exactly matches the system page size.\n-\n-#ifndef PAGE_SIZE\n-#define PAGE_SIZE 4096\n-#endif\n-\n-struct gtm_cacheline_page\n-{\n-  static const size_t LINES\n-    = ((PAGE_SIZE - sizeof(gtm_cacheline_page *))\n-       / (CACHELINE_SIZE + sizeof(gtm_cacheline_mask)));\n-\n-  gtm_cacheline lines[LINES] __attribute__((aligned(PAGE_SIZE)));\n-  gtm_cacheline_mask masks[LINES];\n-  gtm_cacheline_page *prev;\n-\n-  static gtm_cacheline_page *\n-  page_for_line (gtm_cacheline *c)\n-  {\n-    return (gtm_cacheline_page *)((uintptr_t)c & -PAGE_SIZE);\n-  }\n-\n-  gtm_cacheline_mask *\n-  mask_for_line (gtm_cacheline *c)\n-  {\n-    size_t index = c - &this->lines[0];\n-    return &this->masks[index];\n-  }\n-\n-  static gtm_cacheline_mask *\n-  mask_for_page_line (gtm_cacheline *c)\n-  {\n-    gtm_cacheline_page *p = page_for_line (c);\n-    return p->mask_for_line (c);\n-  }\n-\n-  static void *operator new (size_t);\n-  static void operator delete (void *);\n-};\n-\n-} // namespace GTM\n-\n-#endif // LIBITM_CACHEPAGE_H"}, {"sha": "e502e50869b42fa60d44a29437f9af8d7a5d8c21", "filename": "libitm/config/generic/tls.cc", "status": "modified", "additions": 0, "deletions": 47, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Fgeneric%2Ftls.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Fgeneric%2Ftls.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fgeneric%2Ftls.cc?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -30,51 +30,4 @@ namespace GTM HIDDEN {\n __thread gtm_thread_tls _gtm_thr_tls;\n #endif\n \n-// Filter out any updates that overlap the libitm stack, as defined by\n-// TOP (entry point to library) and BOT (below current function).  This\n-// definition should be fine for all stack-grows-down architectures.\n-\n-gtm_cacheline_mask __attribute__((noinline))\n-gtm_mask_stack(gtm_cacheline *line, gtm_cacheline_mask mask)\n-{\n-  void *top = gtm_thr()->jb.cfa;\n-  void *bot = __builtin_dwarf_cfa();\n-\n-  // We must have come through an entry point that set TOP.\n-  assert (top != NULL);\n-\n-  if (line + 1 < bot)\n-    {\n-      // Since we don't have the REAL stack boundaries for this thread,\n-      // we cannot know if this is a dead write to a stack address below\n-      // the current function or if it is write to another VMA.  In either\n-      // case allowing the write should not affect correctness.\n-    }\n-  else if (line >= top)\n-    {\n-      // A valid write to an address in an outer stack frame, or a write\n-      // to another VMA.\n-    }\n-  else\n-    {\n-      uintptr_t diff = (uintptr_t)top - (uintptr_t)line;\n-      if (diff >= CACHELINE_SIZE)\n-\t{\n-\t  // The write is either fully within the proscribed area, or the tail\n-\t  // of the cacheline overlaps the proscribed area.  Assume that all\n-\t  // stacks are at least cacheline aligned and declare the head of the\n-\t  // cacheline dead.\n-\t  mask = 0;\n-\t}\n-      else\n-\t{\n-\t  // The head of the cacheline is within the proscribed area, but the\n-\t  // tail of the cacheline is live.  Eliminate the dead writes.\n-\t  mask &= (gtm_cacheline_mask)-1 << diff;\n-\t}\n-    }\n-\n-  return mask;\n-}\n-\n } // namespace GTM"}, {"sha": "50cb13bd277b49127554aca285b14775100e9e61", "filename": "libitm/config/generic/unaligned.h", "status": "removed", "additions": 0, "deletions": 228, "changes": 228, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fgeneric%2Funaligned.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fgeneric%2Funaligned.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fgeneric%2Funaligned.h?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,228 +0,0 @@\n-/* Copyright (C) 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#ifndef LIBITM_UNALIGNED_H\n-#define LIBITM_UNALIGNED_H 1\n-\n-namespace GTM HIDDEN {\n-\n-#ifndef STRICT_ALIGNMENT\n-#define STRICT_ALIGNMENT 1\n-#endif\n-\n-// A type trait for whether type T requires strict alignment.\n-// The generic types are assumed to all be the same; specializations\n-// for target-specific types should be done in config/cpu/unaligned.h.\n-template<typename T>\n-  struct strict_alignment\n-    : public std::integral_constant<bool, STRICT_ALIGNMENT>\n-  { };\n-\n-// A helper template for accessing an integral type the same size as T\n-template<typename T>\n-  struct make_integral\n-    : public sized_integral<sizeof(T)>\n-  { };\n-\n-// A helper class for accessing T as an unaligned value.\n-template<typename T>\n-struct __attribute__((packed)) unaligned_helper\n-  { T x; };\n-\n-// A helper class for view-converting T as an integer.\n-template<typename T>\n-union view_convert_helper\n-{\n-  typedef T type;\n-  typedef make_integral<T> itype;\n-\n-  type t;\n-  itype i;\n-};\n-\n-// Generate an unaligned load sequence.\n-// The compiler knows how to do this for any specific type.\n-template<typename T>\n-inline T ALWAYS_INLINE\n-unaligned_load(const void *t)\n-{\n-  typedef unaligned_helper<T> UT;\n-  const UT *ut = reinterpret_cast<const UT *>(t);\n-  return ut->x;\n-}\n-\n-// Generate an unaligned store sequence.\n-template<typename T>\n-inline void ALWAYS_INLINE\n-unaligned_store(void *t, T val)\n-{\n-  typedef unaligned_helper<T> UT;\n-  UT *ut = reinterpret_cast<UT *>(t);\n-  ut->x = val;\n-}\n-\n-// Generate an unaligned load from two different cachelines.\n-// It is known that OFS + SIZEOF(T) > CACHELINE_SIZE.\n-template<typename T>\n-inline T ALWAYS_INLINE\n-unaligned_load2(const gtm_cacheline *c1, const gtm_cacheline *c2, size_t ofs)\n-{\n-  size_t left = CACHELINE_SIZE - ofs;\n-  T ret;\n-\n-  memcpy (&ret, &c1->b[ofs], left);\n-  memcpy ((char *)&ret + ofs, c2, sizeof(T) - left);\n-\n-  return ret;\n-}\n-\n-// Generate an unaligned store into two different cachelines.\n-// It is known that OFS + SIZEOF(T) > CACHELINE_SIZE.\n-template<typename T>\n-inline void ALWAYS_INLINE\n-unaligned_store2(gtm_cacheline *c1, gtm_cacheline *c2, size_t ofs, T val)\n-{\n-  size_t left = CACHELINE_SIZE - ofs;\n-  memcpy (&c1->b[ofs], &val, left);\n-  memcpy (c2, (char *)&val + left, sizeof(T) - left);\n-}\n-\n-#ifndef HAVE_ARCH_UNALIGNED_LOAD2_U2\n-template<>\n-inline uint16_t ALWAYS_INLINE\n-unaligned_load2<uint16_t>(const gtm_cacheline *c1,\n-\t\t\t  const gtm_cacheline *c2, size_t ofs)\n-{\n-  uint16_t v1 = c1->b[CACHELINE_SIZE - 1];\n-  uint16_t v2 = c2->b[0];\n-\n-  if (WORDS_BIGENDIAN)\n-    return v1 << 8 | v2;\n-  else\n-    return v2 << 8 | v1;\n-}\n-#endif\n-\n-#ifndef HAVE_ARCH_UNALIGNED_LOAD2_U4\n-template<>\n-inline uint32_t ALWAYS_INLINE\n-unaligned_load2<uint32_t>(const gtm_cacheline *c1,\n-\t\t\t  const gtm_cacheline *c2, size_t ofs)\n-{\n-  uint32_t v1 = c1->u32[CACHELINE_SIZE / sizeof(uint32_t) - 1];\n-  uint32_t v2 = c2->u32[0];\n-  int s2 = (ofs & (sizeof(uint32_t) - 1)) * 8;\n-  int s1 = sizeof(uint32_t) * 8 - s2;\n-\n-  if (WORDS_BIGENDIAN)\n-    return v1 << s2 | v2 >> s1;\n-  else\n-    return v2 << s2 | v1 >> s1;\n-}\n-#endif\n-\n-#ifndef HAVE_ARCH_UNALIGNED_LOAD2_U8\n-template<>\n-inline uint64_t ALWAYS_INLINE\n-unaligned_load2<uint64_t>(const gtm_cacheline *c1,\n-\t\t\t  const gtm_cacheline *c2, size_t ofs)\n-{\n-  uint64_t v1 = c1->u64[CACHELINE_SIZE / sizeof(uint64_t) - 1];\n-  uint64_t v2 = c2->u64[0];\n-  int s2 = (ofs & (sizeof(uint64_t) - 1)) * 8;\n-  int s1 = sizeof(uint64_t) * 8 - s2;\n-\n-  if (WORDS_BIGENDIAN)\n-    return v1 << s2 | v2 >> s1;\n-  else\n-    return v2 << s2 | v1 >> s1;\n-}\n-#endif\n-\n-template<>\n-inline float ALWAYS_INLINE\n-unaligned_load2<float>(const gtm_cacheline *c1,\n-\t\t       const gtm_cacheline *c2, size_t ofs)\n-{\n-  typedef view_convert_helper<float> VC; VC vc;\n-  vc.i = unaligned_load2<VC::itype>(c1, c2, ofs);\n-  return vc.t;\n-}\n-\n-template<>\n-inline double ALWAYS_INLINE\n-unaligned_load2<double>(const gtm_cacheline *c1,\n-\t\t\tconst gtm_cacheline *c2, size_t ofs)\n-{\n-  typedef view_convert_helper<double> VC; VC vc;\n-  vc.i = unaligned_load2<VC::itype>(c1, c2, ofs);\n-  return vc.t;\n-}\n-\n-#ifndef HAVE_ARCH_UNALIGNED_STORE2_U2\n-template<>\n-inline void ALWAYS_INLINE\n-unaligned_store2<uint16_t>(gtm_cacheline *c1, gtm_cacheline *c2,\n-\t\t\t   size_t ofs, uint16_t val)\n-{\n-  uint8_t vl = val, vh = val >> 8;\n-\n-  if (WORDS_BIGENDIAN)\n-    {\n-      c1->b[CACHELINE_SIZE - 1] = vh;\n-      c2->b[0] = vl;\n-    }\n-  else\n-    {\n-      c1->b[CACHELINE_SIZE - 1] = vl;\n-      c2->b[0] = vh;\n-    }\n-}\n-#endif\n-\n-#if 0\n-#ifndef HAVE_ARCH_UNALIGNED_STORE2_U4\n-template<>\n-inline void ALWAYS_INLINE\n-unaligned_store2<uint32_t>(gtm_cacheline *c1, gtm_cacheline *c2,\n-\t\t\t   size_t ofs, uint32_t val)\n-{\n-  // ??? We could reuse the store_mask stuff here.\n-}\n-#endif\n-\n-template<>\n-inline void ALWAYS_INLINE\n-unaligned_store2<float>(gtm_cacheline *c1, gtm_cacheline *c2,\n-\t\t\tsize_t ofs, float val)\n-{\n-  typedef view_convert_helper<float> VC; VC vc;\n-  vc.t = val;\n-  unaligned_store2(c1, c2, ofs, vc.i);\n-}\n-#endif\n-\n-} // namespace GTM\n-\n-#endif // LIBITM_UNALIGNED_H"}, {"sha": "128cd5435aeb08647820d2137339b4124ab7bd47", "filename": "libitm/config/posix/cachepage.cc", "status": "removed", "additions": 0, "deletions": 183, "changes": 183, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fposix%2Fcachepage.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fposix%2Fcachepage.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fposix%2Fcachepage.cc?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,183 +0,0 @@\n-/* Copyright (C) 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#include \"libitm_i.h\"\n-#include <pthread.h>\n-\n-//\n-// We have three possibilities for alloction: mmap, memalign, posix_memalign\n-//\n-\n-#if defined(HAVE_MMAP_ANON) || defined(HAVE_MMAP_DEV_ZERO)\n-#include <sys/mman.h>\n-#include <fcntl.h>\n-#endif\n-#ifdef HAVE_MALLOC_H\n-#include <malloc.h>\n-#endif\n-\n-namespace GTM HIDDEN {\n-\n-#if defined(HAVE_MMAP_ANON)\n-# if !defined(MAP_ANONYMOUS) && defined(MAP_ANON)\n-#  define MAP_ANONYMOUS MAP_ANON\n-# endif\n-# define dev_zero -1\n-#elif defined(HAVE_MMAP_DEV_ZERO)\n-# ifndef MAP_ANONYMOUS\n-#  define MAP_ANONYMOUS 0\n-# endif\n-static int dev_zero = -1;\n-#endif\n-\n-#if defined(HAVE_MMAP_ANON) || defined(HAVE_MMAP_DEV_ZERO)\n-/* If we get here, we've already opened /dev/zero and verified that\n-   PAGE_SIZE is valid for the system.  */\n-static gtm_cacheline_page * alloc_mmap (void) UNUSED;\n-static gtm_cacheline_page *\n-alloc_mmap (void)\n-{\n-  gtm_cacheline_page *r;\n-  r = (gtm_cacheline_page *) mmap (NULL, PAGE_SIZE, PROT_READ | PROT_WRITE,\n-\t\t\t\t   MAP_PRIVATE | MAP_ANONYMOUS, dev_zero, 0);\n-  if (r == (gtm_cacheline_page *) MAP_FAILED)\n-    abort ();\n-  return r;\n-}\n-#endif /* MMAP_ANON | MMAP_DEV_ZERO */\n-\n-#ifdef HAVE_MEMALIGN\n-static gtm_cacheline_page * alloc_memalign (void) UNUSED;\n-static gtm_cacheline_page *\n-alloc_memalign (void)\n-{\n-  gtm_cacheline_page *r;\n-  r = (gtm_cacheline_page *) memalign (PAGE_SIZE, PAGE_SIZE);\n-  if (r == NULL)\n-    abort ();\n-  return r;\n-}\n-#endif /* MEMALIGN */\n-\n-#ifdef HAVE_POSIX_MEMALIGN\n-static gtm_cacheline_page *alloc_posix_memalign (void) UNUSED;\n-static gtm_cacheline_page *\n-alloc_posix_memalign (void)\n-{\n-  void *r;\n-  if (posix_memalign (&r, PAGE_SIZE, PAGE_SIZE))\n-    abort ();\n-  return (gtm_cacheline_page *) r;\n-}\n-#endif /* POSIX_MEMALIGN */\n-\n-#if defined(HAVE_MMAP_ANON) && defined(FIXED_PAGE_SIZE)\n-# define alloc_page  alloc_mmap\n-#elif defined(HAVE_MMAP_DEV_ZERO) && defined(FIXED_PAGE_SIZE)\n-static gtm_cacheline_page *\n-alloc_page (void)\n-{\n-  if (dev_zero < 0)\n-    {\n-      dev_zero = open (\"/dev/zero\", O_RDWR);\n-      assert (dev_zero >= 0);\n-    }\n-  return alloc_mmap ();\n-}\n-#elif defined(HAVE_MMAP_ANON) || defined(HAVE_MMAP_DEV_ZERO)\n-static gtm_cacheline_page * (*alloc_page) (void);\n-static void __attribute__((constructor))\n-init_alloc_page (void)\n-{\n-  size_t page_size = getpagesize ();\n-  if (page_size <= PAGE_SIZE && PAGE_SIZE % page_size == 0)\n-    {\n-# ifndef HAVE_MMAP_ANON\n-      dev_zero = open (\"/dev/zero\", O_RDWR);\n-      assert (dev_zero >= 0);\n-# endif\n-      alloc_page = alloc_mmap;\n-      return;\n-    }\n-# ifdef HAVE_MEMALIGN\n-  alloc_page = alloc_memalign;\n-# elif defined(HAVE_POSIX_MEMALIGN)\n-  alloc_page = alloc_posix_memalign;\n-# else\n-#  error \"No fallback aligned memory allocation method\"\n-# endif\n-}\n-#elif defined(HAVE_MEMALIGN)\n-# define alloc_page  alloc_memalign\n-#elif defined(HAVE_POSIX_MEMALIGN)\n-# define alloc_page  alloc_posix_memalign\n-#else\n-# error \"No aligned memory allocation method\"\n-#endif\n-\n-static gtm_cacheline_page *free_pages;\n-static pthread_mutex_t free_page_lock = PTHREAD_MUTEX_INITIALIZER;\n-\n-void *\n-gtm_cacheline_page::operator new (size_t size)\n-{\n-  assert (size == sizeof (gtm_cacheline_page));\n-  assert (size <= PAGE_SIZE);\n-\n-  pthread_mutex_lock(&free_page_lock);\n-\n-  gtm_cacheline_page *r = free_pages;\n-  free_pages = r ? r->prev : NULL;\n-\n-  pthread_mutex_unlock(&free_page_lock);\n-\n-  if (r == NULL)\n-    r = alloc_page ();\n-\n-  return r;\n-}\n-\n-void\n-gtm_cacheline_page::operator delete (void *xhead)\n-{\n-  gtm_cacheline_page *head = static_cast<gtm_cacheline_page *>(xhead);\n-  gtm_cacheline_page *tail;\n-\n-  if (head == 0)\n-    return;\n-\n-  /* ??? We should eventually really free some of these.  */\n-\n-  for (tail = head; tail->prev != 0; tail = tail->prev)\n-    continue;\n-\n-  pthread_mutex_lock(&free_page_lock);\n-\n-  tail->prev = free_pages;\n-  free_pages = head;\n-\n-  pthread_mutex_unlock(&free_page_lock);\n-}\n-\n-} // namespace GTM"}, {"sha": "2e49a35595322f022c92149486d7478c0157ff9a", "filename": "libitm/config/x86/cacheline.cc", "status": "removed", "additions": 0, "deletions": 73, "changes": 73, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fx86%2Fcacheline.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fx86%2Fcacheline.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fx86%2Fcacheline.cc?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,73 +0,0 @@\n-/* Copyright (C) 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#include \"libitm_i.h\"\n-\n-namespace GTM HIDDEN {\n-\n-uint32_t const gtm_bit_to_byte_mask[16] =\n-{\n-  0x00000000,\n-  0x000000ff,\n-  0x0000ff00,\n-  0x0000ffff,\n-  0x00ff0000,\n-  0x00ff00ff,\n-  0x00ffff00,\n-  0x00ffffff,\n-  0xff000000,\n-  0xff0000ff,\n-  0xff00ff00,\n-  0xff00ffff,\n-  0xffff0000,\n-  0xffff00ff,\n-  0xffffff00,\n-  0xffffffff\n-};\n-\n-#ifdef __SSE2__\n-# define MEMBER\tm128i\n-#else\n-# define MEMBER\tw\n-#endif\n-\n-void\n-gtm_cacheline::copy_mask (gtm_cacheline * __restrict d,\n-\t\t\t  const gtm_cacheline * __restrict s,\n-\t\t\t  gtm_cacheline_mask m)\n-{\n-  if (m == (gtm_cacheline_mask)-1)\n-    {\n-      *d = *s;\n-      return;\n-    }\n-  if (__builtin_expect (m == 0, 0))\n-    return;\n-\n-  size_t n = sizeof(d->MEMBER[0]);\n-  for (size_t i = 0; i < CACHELINE_SIZE / n; ++i, m >>= n)\n-    store_mask (&d->MEMBER[i], s->MEMBER[i], m);\n-}\n-\n-} // namespace GTM"}, {"sha": "337c9995c17838a064195c5d3372cee03312ad3e", "filename": "libitm/config/x86/cacheline.h", "status": "modified", "additions": 2, "deletions": 121, "changes": 123, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Fx86%2Fcacheline.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Fx86%2Fcacheline.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fx86%2Fcacheline.h?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -40,8 +40,6 @@ namespace GTM HIDDEN {\n // in the cacheline with which it is associated.\n typedef sized_integral<CACHELINE_SIZE / 8>::type gtm_cacheline_mask;\n \n-extern uint32_t const gtm_bit_to_byte_mask[16];\n-\n union gtm_cacheline\n {\n   // Byte access to the cacheline.\n@@ -67,38 +65,13 @@ union gtm_cacheline\n   __m256i m256i[CACHELINE_SIZE / sizeof(__m256i)];\n #endif\n \n-  // Store S into D, but only the bytes specified by M.\n-  static void store_mask (uint32_t *d, uint32_t s, uint8_t m);\n-  static void store_mask (uint64_t *d, uint64_t s, uint8_t m);\n-#ifdef __SSE2__\n-  static void store_mask (__m128i *d, __m128i s, uint16_t m);\n-#endif\n-\n-  // Copy S to D, but only the bytes specified by M.\n-  static void copy_mask (gtm_cacheline * __restrict d,\n-\t\t\t const gtm_cacheline * __restrict s,\n-\t\t\t gtm_cacheline_mask m);\n-\n-  // A write barrier to emit after (a series of) copy_mask.\n-  // When we're emitting non-temporal stores, the normal strong\n-  // ordering of the machine doesn't apply.\n-  static void copy_mask_wb ();\n-\n #if defined(__SSE__) || defined(__AVX__)\n   // Copy S to D; only bother defining if we can do this more efficiently\n   // than the compiler-generated default implementation.\n   gtm_cacheline& operator= (const gtm_cacheline &s);\n #endif // SSE, AVX\n };\n \n-inline void\n-gtm_cacheline::copy_mask_wb ()\n-{\n-#ifdef __SSE2__\n-  _mm_sfence ();\n-#endif\n-}\n-\n #if defined(__SSE__) || defined(__AVX__)\n inline gtm_cacheline& ALWAYS_INLINE\n gtm_cacheline::operator= (const gtm_cacheline & __restrict s)\n@@ -141,103 +114,11 @@ gtm_cacheline::operator= (const gtm_cacheline & __restrict s)\n     }\n \n   return *this;\n-}\n-#endif\n \n-// Support masked integer stores more efficiently with an unlocked cmpxchg\n-// insn.  My reasoning is that while we write to locations that we do not wish\n-// to modify, we do it in an uninterruptable insn, and so we either truely\n-// write back the original data or the insn fails -- unlike with a\n-// load/and/or/write sequence which can be interrupted either by a kernel\n-// task switch or an unlucky cacheline steal by another processor.  Avoiding\n-// the LOCK prefix improves performance by a factor of 10, and we don't need\n-// the memory barrier semantics implied by that prefix.\n-\n-inline void ALWAYS_INLINE\n-gtm_cacheline::store_mask (uint32_t *d, uint32_t s, uint8_t m)\n-{\n-  gtm_cacheline_mask tm = (1 << sizeof (s)) - 1;\n-  if (__builtin_expect (m & tm, tm))\n-    {\n-      if (__builtin_expect ((m & tm) == tm, 1))\n-\t*d = s;\n-      else\n-\t{\n-\t  gtm_cacheline_mask bm = gtm_bit_to_byte_mask[m & 15];\n-\t  gtm_word n, o = *d;\n-\n-\t  __asm(\"\\n0:\\t\"\n-\t\t\"mov\t%[o], %[n]\\n\\t\"\n-\t\t\"and\t%[m], %[n]\\n\\t\"\n-\t\t\"or\t%[s], %[n]\\n\\t\"\n-\t\t\"cmpxchg %[n], %[d]\\n\\t\"\n-\t\t\".byte\t0x2e\\n\\t\"\t// predict not-taken, aka jnz,pn\n-\t\t\"jnz\t0b\"\n-\t\t: [d] \"+m\"(*d), [n] \"=&r\" (n), [o] \"+a\"(o)\n-\t\t: [s] \"r\" (s & bm), [m] \"r\" (~bm));\n-\t}\n-    }\n+#undef CP\n+#undef TYPE\n }\n-\n-inline void ALWAYS_INLINE\n-gtm_cacheline::store_mask (uint64_t *d, uint64_t s, uint8_t m)\n-{\n-  gtm_cacheline_mask tm = (1 << sizeof (s)) - 1;\n-  if (__builtin_expect (m & tm, tm))\n-    {\n-      if (__builtin_expect ((m & tm) == tm, 1))\n-\t*d = s;\n-      else\n-\t{\n-#ifdef __x86_64__\n-\t  uint32_t bl = gtm_bit_to_byte_mask[m & 15];\n-\t  uint32_t bh = gtm_bit_to_byte_mask[(m >> 4) & 15];\n-\t  gtm_cacheline_mask bm = bl | ((gtm_cacheline_mask)bh << 31 << 1);\n-\t  uint64_t n, o = *d;\n-\t  __asm(\"\\n0:\\t\"\n-\t\t\"mov\t%[o], %[n]\\n\\t\"\n-\t\t\"and\t%[m], %[n]\\n\\t\"\n-\t\t\"or\t%[s], %[n]\\n\\t\"\n-\t\t\"cmpxchg %[n], %[d]\\n\\t\"\n-\t\t\".byte\t0x2e\\n\\t\"\t// predict not-taken, aka jnz,pn\n-\t\t\"jnz\t0b\"\n-\t\t: [d] \"+m\"(*d), [n] \"=&r\" (n), [o] \"+a\"(o)\n-\t\t: [s] \"r\" (s & bm), [m] \"r\" (~bm));\n-#else\n-\t  /* ??? While it's possible to perform this operation with\n-\t     cmpxchg8b, the sequence requires all 7 general registers\n-\t     and thus cannot be performed with -fPIC.  Don't even try.  */\n-\t  uint32_t *d32 = reinterpret_cast<uint32_t *>(d);\n-\t  store_mask (d32, s, m);\n-\t  store_mask (d32 + 1, s >> 32, m >> 4);\n #endif\n-\t}\n-    }\n-}\n-\n-#ifdef __SSE2__\n-inline void ALWAYS_INLINE\n-gtm_cacheline::store_mask (__m128i *d, __m128i s, uint16_t m)\n-{\n-  if (__builtin_expect (m == 0, 0))\n-    return;\n-  if (__builtin_expect (m == 0xffff, 1))\n-    *d = s;\n-  else\n-    {\n-      __m128i bm0, bm1, bm2, bm3;\n-      bm0 = _mm_set_epi32 (0, 0, 0, gtm_bit_to_byte_mask[m & 15]); m >>= 4;\n-      bm1 = _mm_set_epi32 (0, 0, 0, gtm_bit_to_byte_mask[m & 15]); m >>= 4;\n-      bm2 = _mm_set_epi32 (0, 0, 0, gtm_bit_to_byte_mask[m & 15]); m >>= 4;\n-      bm3 = _mm_set_epi32 (0, 0, 0, gtm_bit_to_byte_mask[m & 15]); m >>= 4;\n-      bm0 = _mm_unpacklo_epi32 (bm0, bm1);\n-      bm2 = _mm_unpacklo_epi32 (bm2, bm3);\n-      bm0 = _mm_unpacklo_epi64 (bm0, bm2);\n-\n-      _mm_maskmoveu_si128 (s, bm0, (char *)d);\n-    }\n-}\n-#endif // SSE2\n \n } // namespace GTM\n "}, {"sha": "01abc47dccb387cf0c9fb3f89e64a2119f91c038", "filename": "libitm/config/x86/unaligned.h", "status": "removed", "additions": 0, "deletions": 237, "changes": 237, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fx86%2Funaligned.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fconfig%2Fx86%2Funaligned.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fx86%2Funaligned.h?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,237 +0,0 @@\n-/* Copyright (C) 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#ifndef LIBITM_X86_UNALIGNED_H\n-#define LIBITM_X86_UNALIGNED_H 1\n-\n-#define HAVE_ARCH_UNALIGNED_LOAD2_U4 1\n-#define HAVE_ARCH_UNALIGNED_LOAD2_U8 1\n-\n-#include \"config/generic/unaligned.h\"\n-\n-namespace GTM HIDDEN {\n-\n-template<>\n-inline uint32_t\n-unaligned_load2<uint32_t>(const gtm_cacheline *c1,\n-\t\t\t  const gtm_cacheline *c2, size_t ofs)\n-{\n-  uint32_t r, lo, hi;\n-  lo = c1->u32[CACHELINE_SIZE / sizeof(uint32_t) - 1];\n-  hi = c2->u32[0];\n-  asm(\"shrd %b2, %1, %0\" : \"=r\"(r) : \"r\"(hi), \"c\"((ofs & 3) * 8), \"0\"(lo));\n-  return r;\n-}\n-\n-template<>\n-inline uint64_t\n-unaligned_load2<uint64_t>(const gtm_cacheline *c1,\n-\t\t\t  const gtm_cacheline *c2, size_t ofs)\n-{\n-#ifdef __x86_64__\n-  uint64_t r, lo, hi;\n-  lo = c1->u64[CACHELINE_SIZE / sizeof(uint64_t) - 1];\n-  hi = c2->u64[0];\n-  asm(\"shrd %b2, %1, %0\" : \"=r\"(r) : \"r\"(hi), \"c\"((ofs & 3) * 8), \"0\"(lo));\n-  return r;\n-#else\n-  uint32_t v0, v1, v2;\n-  uint64_t r;\n-\n-  if (ofs < CACHELINE_SIZE - 4)\n-    {\n-      v0 = c1->u32[CACHELINE_SIZE / sizeof(uint32_t) - 2];\n-      v1 = c1->u32[CACHELINE_SIZE / sizeof(uint32_t) - 1];\n-      v2 = c2->u32[0];\n-    }\n-  else\n-    {\n-      v0 = c1->u32[CACHELINE_SIZE / sizeof(uint32_t) - 1];\n-      v1 = c2->u32[0];\n-      v2 = c2->u32[1];\n-    }\n-  ofs = (ofs & 3) * 8;\n-  asm(\"shrd %%cl, %[v1], %[v0]; shrd %%cl, %[v2], %[v1]\"\n-      : \"=A\"(r) : \"c\"(ofs), [v0] \"a\"(v0), [v1] \"d\"(v1), [v2] \"r\"(v2));\n-\n-  return r;\n-#endif\n-}\n-\n-#if defined(__SSE2__) || defined(__MMX__)\n-template<>\n-inline _ITM_TYPE_M64\n-unaligned_load2<_ITM_TYPE_M64>(const gtm_cacheline *c1,\n-\t\t\t       const gtm_cacheline *c2, size_t ofs)\n-{\n-# ifdef __x86_64__\n-  __m128i lo = _mm_movpi64_epi64 (c1->m64[CACHELINE_SIZE / 8 - 1]);\n-  __m128i hi = _mm_movpi64_epi64 (c2->m64[0]);\n-\n-  ofs = (ofs & 7) * 8;\n-  lo = _mm_srli_epi64 (lo, ofs);\n-  hi = _mm_slli_epi64 (hi, 64 - ofs);\n-  lo = lo | hi;\n-  return _mm_movepi64_pi64 (lo);\n-# else\n-  // On 32-bit we're about to return the result in an MMX register, so go\n-  // ahead and do the computation in that unit, even if SSE2 is available.\n-  __m64 lo = c1->m64[CACHELINE_SIZE / 8 - 1];\n-  __m64 hi = c2->m64[0];\n-\n-  ofs = (ofs & 7) * 8;\n-  lo = _mm_srli_si64 (lo, ofs);\n-  hi = _mm_slli_si64 (hi, 64 - ofs);\n-  return lo | hi;\n-# endif\n-}\n-#endif // SSE2 or MMX\n-\n-// The SSE types are strictly aligned.\n-#ifdef __SSE__\n-template<>\n-  struct strict_alignment<_ITM_TYPE_M128>\n-    : public std::true_type\n-  { };\n-\n-// Expand the unaligned SSE move instructions.\n-template<>\n-inline _ITM_TYPE_M128\n-unaligned_load<_ITM_TYPE_M128>(const void *t)\n-{\n-  return _mm_loadu_ps (static_cast<const float *>(t));\n-}\n-\n-template<>\n-inline void\n-unaligned_store<_ITM_TYPE_M128>(void *t, _ITM_TYPE_M128 val)\n-{\n-  _mm_storeu_ps (static_cast<float *>(t), val);\n-}\n-#endif // SSE\n-\n-#ifdef __AVX__\n-// The AVX types are strictly aligned when it comes to vmovaps vs vmovups.\n-template<>\n-  struct strict_alignment<_ITM_TYPE_M256>\n-    : public std::true_type\n-  { };\n-\n-template<>\n-inline _ITM_TYPE_M256\n-unaligned_load<_ITM_TYPE_M256>(const void *t)\n-{\n-  return _mm256_loadu_ps (static_cast<const float *>(t));\n-}\n-\n-template<>\n-inline void\n-unaligned_store<_ITM_TYPE_M256>(void *t, _ITM_TYPE_M256 val)\n-{\n-  _mm256_storeu_ps (static_cast<float *>(t), val);\n-}\n-#endif // AVX\n-\n-#ifdef __XOP__\n-# define HAVE_ARCH_REALIGN_M128I 1\n-extern const __v16qi GTM_vpperm_shift[16];\n-inline __m128i\n-realign_m128i (__m128i lo, __m128i hi, unsigned byte_count)\n-{\n-  return _mm_perm_epi8 (lo, hi, GTM_vpperm_shift[byte_count]);\n-}\n-#elif defined(__AVX__)\n-# define HAVE_ARCH_REALIGN_M128I 1\n-extern \"C\" const uint64_t GTM_vpalignr_table[16];\n-inline __m128i\n-realign_m128i (__m128i lo, __m128i hi, unsigned byte_count)\n-{\n-  register __m128i xmm0 __asm__(\"xmm0\") = hi;\n-  register __m128i xmm1 __asm__(\"xmm1\") = lo;\n-  __asm(\"call *%2\" : \"+x\"(xmm0) : \"x\"(xmm1),\n-\t\"r\"(&GTM_vpalignr_table[byte_count]));\n-  return xmm0;\n-}\n-#elif defined(__SSSE3__)\n-# define HAVE_ARCH_REALIGN_M128I 1\n-extern \"C\" const uint64_t GTM_palignr_table[16];\n-inline __m128i\n-realign_m128i (__m128i lo, __m128i hi, unsigned byte_count)\n-{\n-  register __m128i xmm0 __asm__(\"xmm0\") = hi;\n-  register __m128i xmm1 __asm__(\"xmm1\") = lo;\n-  __asm(\"call *%2\" : \"+x\"(xmm0) : \"x\"(xmm1),\n-\t\"r\"(&GTM_palignr_table[byte_count]));\n-  return xmm0;\n-}\n-#elif defined(__SSE2__)\n-# define HAVE_ARCH_REALIGN_M128I 1\n-extern \"C\" const char GTM_pshift_table[16 * 16];\n-inline __m128i\n-realign_m128i (__m128i lo, __m128i hi, unsigned byte_count)\n-{\n-  register __m128i xmm0 __asm__(\"xmm0\") = lo;\n-  register __m128i xmm1 __asm__(\"xmm1\") = hi;\n-  __asm(\"call *%2\" : \"+x\"(xmm0), \"+x\"(xmm1)\n-\t: \"r\"(GTM_pshift_table + byte_count*16));\n-  return xmm0;\n-}\n-#endif // XOP, AVX, SSSE3, SSE2\n-\n-#ifdef HAVE_ARCH_REALIGN_M128I\n-template<>\n-inline _ITM_TYPE_M128\n-unaligned_load2<_ITM_TYPE_M128>(const gtm_cacheline *c1,\n-\t\t\t\tconst gtm_cacheline *c2, size_t ofs)\n-{\n-  return (_ITM_TYPE_M128)\n-    realign_m128i (c1->m128i[CACHELINE_SIZE / 16 - 1],\n-\t\t   c2->m128i[0], ofs & 15);\n-}\n-#endif // HAVE_ARCH_REALIGN_M128I\n-\n-#ifdef __AVX__\n-template<>\n-inline _ITM_TYPE_M256\n-unaligned_load2<_ITM_TYPE_M256>(const gtm_cacheline *c1,\n-\t\t\t\tconst gtm_cacheline *c2, size_t ofs)\n-{\n-  __m128i v0, v1;\n-  __m256i r;\n-\n-  v0 = (__m128i) unaligned_load2<_ITM_TYPE_M128>(c1, c2, ofs);\n-  if (ofs < CACHELINE_SIZE - 16)\n-    v1 = v0, v0 = _mm_loadu_si128 ((const __m128i *) &c1->b[ofs]);\n-  else\n-    v1 = _mm_loadu_si128((const __m128i *)&c2->b[ofs + 16 - CACHELINE_SIZE]);\n-\n-  r = _mm256_castsi128_si256 ((__m128i)v0);\n-  r = _mm256_insertf128_si256 (r, (__m128i)v1, 1);\n-  return (_ITM_TYPE_M256) r;\n-}\n-#endif // AVX\n-\n-} // namespace GTM\n-\n-#endif // LIBITM_X86_UNALIGNED_H"}, {"sha": "30420aa87f540fb9e35e0f0c326b6bef87fd9db8", "filename": "libitm/config/x86/x86_avx.cc", "status": "modified", "additions": 0, "deletions": 59, "changes": 59, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Fx86%2Fx86_avx.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Fx86%2Fx86_avx.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fx86%2Fx86_avx.cc?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -34,62 +34,3 @@ _ITM_LM256 (const _ITM_TYPE_M256 *ptr)\n {\n   GTM::GTM_LB (ptr, sizeof (*ptr));\n }\n-\n-// Helpers for re-aligning two 128-bit values.\n-#ifdef __XOP__\n-const __v16qi GTM::GTM_vpperm_shift[16] =\n-{\n-  {  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15 },\n-  {  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16 },\n-  {  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17 },\n-  {  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18 },\n-  {  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19 },\n-  {  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20 },\n-  {  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21 },\n-  {  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22 },\n-  {  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23 },\n-  {  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24 },\n-  { 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25 },\n-  { 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26 },\n-  { 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27 },\n-  { 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28 },\n-  { 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29 },\n-  { 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30 },\n-};\n-#else\n-# define INSN0\t\t\"movdqa  %xmm1, %xmm0\"\n-# define INSN(N)\t\"vpalignr $\" #N \", %xmm0, %xmm1, %xmm0\"\n-# define TABLE_ENT_0\tINSN0 \"\\n\\tret\\n\\t\"\n-# define TABLE_ENT(N)\t\".balign 8\\n\\t\" INSN(N) \"\\n\\tret\\n\\t\"\n-\n-asm(\".pushsection .text\\n\\\n-\t.balign 16\\n\\\n-\t.globl\tGTM_vpalignr_table\\n\\\n-\t.hidden\tGTM_vpalignr_table\\n\\\n-\t.type\tGTM_vpalignr_table, @function\\n\\\n-GTM_vpalignr_table:\\n\\t\"\n-\tTABLE_ENT_0\n-\tTABLE_ENT(1)\n-\tTABLE_ENT(2)\n-\tTABLE_ENT(3)\n-\tTABLE_ENT(4)\n-\tTABLE_ENT(5)\n-\tTABLE_ENT(6)\n-\tTABLE_ENT(7)\n-\tTABLE_ENT(8)\n-\tTABLE_ENT(9)\n-\tTABLE_ENT(10)\n-\tTABLE_ENT(11)\n-\tTABLE_ENT(12)\n-\tTABLE_ENT(13)\n-\tTABLE_ENT(14)\n-\tTABLE_ENT(15)\n-\t\".balign 8\\n\\\n-\t.size\tGTM_vpalignr_table, .-GTM_vpalignr_table\\n\\\n-\t.popsection\");\n-\n-# undef INSN0\n-# undef INSN\n-# undef TABLE_ENT_0\n-# undef TABLE_ENT\n-#endif"}, {"sha": "5a1c67ac8b16f3b388cba5979613b3091d9b4163", "filename": "libitm/config/x86/x86_sse.cc", "status": "modified", "additions": 0, "deletions": 79, "changes": 79, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Fx86%2Fx86_sse.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Fconfig%2Fx86%2Fx86_sse.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fx86%2Fx86_sse.cc?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -41,82 +41,3 @@ _ITM_LM128 (const _ITM_TYPE_M128 *ptr)\n {\n   GTM::GTM_LB (ptr, sizeof (*ptr));\n }\n-\n-// Helpers for re-aligning two 128-bit values.\n-#ifdef __SSSE3__\n-# define INSN0\t\t\"movdqa\t%xmm1, %xmm0\"\n-# define INSN(N)\t\"palignr $\" #N \", %xmm1, %xmm0\"\n-# define TABLE_ENT_0\tINSN0 \"\\n\\tret\\n\\t\"\n-# define TABLE_ENT(N)\t\".balign 8\\n\\t\" INSN(N) \"\\n\\tret\\n\\t\"\n-\n-asm(\".pushsection .text\\n\\\n-\t.balign 16\\n\\\n-\t.globl\tGTM_palignr_table\\n\\\n-\t.hidden\tGTM_palignr_table\\n\\\n-\t.type\tGTM_palignr_table, @function\\n\\\n-GTM_palignr_table:\\n\\t\"\n-\tTABLE_ENT_0\n-\tTABLE_ENT(1)\n-\tTABLE_ENT(2)\n-\tTABLE_ENT(3)\n-\tTABLE_ENT(4)\n-\tTABLE_ENT(5)\n-\tTABLE_ENT(6)\n-\tTABLE_ENT(7)\n-\tTABLE_ENT(8)\n-\tTABLE_ENT(9)\n-\tTABLE_ENT(10)\n-\tTABLE_ENT(11)\n-\tTABLE_ENT(12)\n-\tTABLE_ENT(13)\n-\tTABLE_ENT(14)\n-\tTABLE_ENT(15)\n-\t\".balign 8\\n\\\n-\t.size\tGTM_palignr_table, .-GTM_palignr_table\\n\\\n-\t.popsection\");\n-\n-# undef INSN0\n-# undef INSN\n-# undef TABLE_ENT_0\n-# undef TABLE_ENT\n-#elif defined(__SSE2__)\n-# define INSNS_8\t\"punpcklqdq %xmm1, %xmm0\"\n-# define INSNS(N)\t\"psrldq $\"#N\", %xmm0\\n\\t\" \\\n-\t\t\t\"pslldq $(16-\"#N\"), %xmm1\\n\\t\" \\\n-\t\t\t\"por %xmm1, %xmm0\"\n-# define TABLE_ENT_0\t\"ret\\n\\t\"\n-# define TABLE_ENT_8\t\".balign 16\\n\\t\" INSNS_8 \"\\n\\tret\\n\\t\"\n-# define TABLE_ENT(N)\t\".balign 16\\n\\t\" INSNS(N) \"\\n\\tret\\n\\t\"\n-\n-asm(\".pushsection .text\\n\\\n-\t.balign 16\\n\\\n-\t.globl\tGTM_pshift_table\\n\\\n-\t.hidden\tGTM_pshift_table\\n\\\n-\t.type\tGTM_pshift_table, @function\\n\\\n-GTM_pshift_table:\\n\\t\"\n-\tTABLE_ENT_0\n-\tTABLE_ENT(1)\n-\tTABLE_ENT(2)\n-\tTABLE_ENT(3)\n-\tTABLE_ENT(4)\n-\tTABLE_ENT(5)\n-\tTABLE_ENT(6)\n-\tTABLE_ENT(7)\n-\tTABLE_ENT_8\n-\tTABLE_ENT(9)\n-\tTABLE_ENT(10)\n-\tTABLE_ENT(11)\n-\tTABLE_ENT(12)\n-\tTABLE_ENT(13)\n-\tTABLE_ENT(14)\n-\tTABLE_ENT(15)\n-\t\".balign 8\\n\\\n-\t.size\tGTM_pshift_table, .-GTM_pshift_table\\n\\\n-\t.popsection\");\n-\n-# undef INSNS_8\n-# undef INSNS\n-# undef TABLE_ENT_0\n-# undef TABLE_ENT_8\n-# undef TABLE_ENT\n-#endif"}, {"sha": "58e43b01281b0de372d19cb7df051364b2576afb", "filename": "libitm/libitm_i.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Flibitm_i.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Flibitm_i.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Flibitm_i.h?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -78,7 +78,6 @@ enum gtm_restart_reason\n #include \"rwlock.h\"\n #include \"aatree.h\"\n #include \"cacheline.h\"\n-#include \"cachepage.h\"\n #include \"stmlock.h\"\n #include \"dispatch.h\"\n #include \"containers.h\""}, {"sha": "9919e6a9afaceb02de99742576fa01c9edff454f", "filename": "libitm/memcpy.cc", "status": "removed", "additions": 0, "deletions": 365, "changes": 365, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fmemcpy.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fmemcpy.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fmemcpy.cc?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,365 +0,0 @@\n-/* Copyright (C) 2008, 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#include \"libitm_i.h\"\n-\n-using namespace GTM;\n-\n-static void\n-do_memcpy (uintptr_t idst, uintptr_t isrc, size_t size,\n-\t   abi_dispatch::lock_type W, abi_dispatch::lock_type R)\n-{\n-  abi_dispatch *disp = abi_disp();\n-  // The position in the destination cacheline where *IDST starts.\n-  uintptr_t dofs = idst & (CACHELINE_SIZE - 1);\n-  // The position in the source cacheline where *ISRC starts.\n-  uintptr_t sofs = isrc & (CACHELINE_SIZE - 1);\n-  const gtm_cacheline *src\n-    = reinterpret_cast<const gtm_cacheline *>(isrc & -CACHELINE_SIZE);\n-  gtm_cacheline *dst\n-    = reinterpret_cast<gtm_cacheline *>(idst & -CACHELINE_SIZE);\n-  const gtm_cacheline *sline;\n-  abi_dispatch::mask_pair dpair;\n-\n-  if (size == 0)\n-    return;\n-\n-  // If both SRC and DST data start at the same position in the cachelines,\n-  // we can easily copy the data in tandem, cacheline by cacheline...\n-  if (dofs == sofs)\n-    {\n-      // We copy the data in three stages:\n-\n-      // (a) Copy stray bytes at the beginning that are smaller than a\n-      // cacheline.\n-      if (sofs != 0)\n-\t{\n-\t  size_t sleft = CACHELINE_SIZE - sofs;\n-\t  size_t min = (size <= sleft ? size : sleft);\n-\n-\t  dpair = disp->write_lock(dst, W);\n-\t  sline = disp->read_lock(src, R);\n-\t  *dpair.mask |= (((gtm_cacheline_mask)1 << min) - 1) << sofs;\n-\t  memcpy (&dpair.line->b[sofs], &sline->b[sofs], min);\n-\t  dst++;\n-\t  src++;\n-\t  size -= min;\n-\t}\n-\n-      // (b) Copy subsequent cacheline sized chunks.\n-      while (size >= CACHELINE_SIZE)\n-\t{\n-\t  dpair = disp->write_lock(dst, W);\n-\t  sline = disp->read_lock(src, R);\n-\t  *dpair.mask = -1;\n-\t  *dpair.line = *sline;\n-\t  dst++;\n-\t  src++;\n-\t  size -= CACHELINE_SIZE;\n-\t}\n-\n-      // (c) Copy anything left over.\n-      if (size != 0)\n-\t{\n-\t  dpair = disp->write_lock(dst, W);\n-\t  sline = disp->read_lock(src, R);\n-\t  *dpair.mask |= ((gtm_cacheline_mask)1 << size) - 1;\n-\t  memcpy (dpair.line, sline, size);\n-\t}\n-    }\n-  // ... otherwise, we must copy the data in disparate hunks using\n-  // temporary storage.\n-  else\n-    {\n-      gtm_cacheline c;\n-      size_t sleft = CACHELINE_SIZE - sofs;\n-\n-      sline = disp->read_lock(src, R);\n-\n-      // As above, we copy the data in three stages:\n-\n-      // (a) Copy stray bytes at the beginning that are smaller than a\n-      // cacheline.\n-      if (dofs != 0)\n-\t{\n-\t  size_t dleft = CACHELINE_SIZE - dofs;\n-\t  size_t min = (size <= dleft ? size : dleft);\n-\n-\t  dpair = disp->write_lock(dst, W);\n-\t  *dpair.mask |= (((gtm_cacheline_mask)1 << min) - 1) << dofs;\n-\n-\t  // If what's left in the source cacheline will fit in the\n-\t  // rest of the destination cacheline, straight up copy it.\n-\t  if (min <= sleft)\n-\t    {\n-\t      memcpy (&dpair.line->b[dofs], &sline->b[sofs], min);\n-\t      sofs += min;\n-\t    }\n-\t  // Otherwise, we need more bits from the source cacheline\n-\t  // that are available.  Piece together what we need from\n-\t  // contiguous (source) cachelines, into temp space, and copy\n-\t  // it over.\n-\t  else\n-\t    {\n-\t      memcpy (&c, &sline->b[sofs], sleft);\n-\t      sline = disp->read_lock(++src, R);\n-\t      sofs = min - sleft;\n-\t      memcpy (&c.b[sleft], sline, sofs);\n-\t      memcpy (&dpair.line->b[dofs], &c, min);\n-\t    }\n-\t  sleft = CACHELINE_SIZE - sofs;\n-\n-\t  dst++;\n-\t  size -= min;\n-\t}\n-\n-      // (b) Copy subsequent cacheline sized chunks.\n-      while (size >= CACHELINE_SIZE)\n-\t{\n-\t  // We have a full (destination) cacheline where to put the\n-\t  // data, but to get to the corresponding cacheline sized\n-\t  // chunk in the source, we have to piece together two\n-\t  // contiguous source cachelines.\n-\n-\t  memcpy (&c, &sline->b[sofs], sleft);\n-\t  sline = disp->read_lock(++src, R);\n-\t  memcpy (&c.b[sleft], sline, sofs);\n-\n-\t  dpair = disp->write_lock(dst, W);\n-\t  *dpair.mask = -1;\n-\t  *dpair.line = c;\n-\n-\t  dst++;\n-\t  size -= CACHELINE_SIZE;\n-\t}\n-\n-      // (c) Copy anything left over.\n-      if (size != 0)\n-\t{\n-\t  dpair = disp->write_lock(dst, W);\n-\t  *dpair.mask |= ((gtm_cacheline_mask)1 << size) - 1;\n-\t  // If what's left to copy is entirely in the remaining\n-\t  // source cacheline, do it.\n-\t  if (size <= sleft)\n-\t    memcpy (dpair.line, &sline->b[sofs], size);\n-\t  // Otherwise, piece together the remaining bits, and copy.\n-\t  else\n-\t    {\n-\t      memcpy (&c, &sline->b[sofs], sleft);\n-\t      sline = disp->read_lock(++src, R);\n-\t      memcpy (&c.b[sleft], sline, size - sleft);\n-\t      memcpy (dpair.line, &c, size);\n-\t    }\n-\t}\n-    }\n-}\n-\n-static void\n-do_memmove (uintptr_t idst, uintptr_t isrc, size_t size,\n-\t    abi_dispatch::lock_type W, abi_dispatch::lock_type R)\n-{\n-  abi_dispatch *disp = abi_disp();\n-  uintptr_t dleft, sleft, sofs, dofs;\n-  const gtm_cacheline *sline;\n-  abi_dispatch::mask_pair dpair;\n-\n-  if (size == 0)\n-    return;\n-\n-  /* The co-aligned memmove below doesn't work for DST == SRC, so filter\n-     that out.  It's tempting to just return here, as this is a no-op move.\n-     However, our caller has the right to expect the locks to be acquired\n-     as advertized.  */\n-  if (__builtin_expect (idst == isrc, 0))\n-    {\n-      /* If the write lock is already acquired, nothing to do.  */\n-      if (W == abi_dispatch::WaW)\n-\treturn;\n-      /* If the destination is protected, acquire a write lock.  */\n-      if (W != abi_dispatch::NOLOCK)\n-\tR = abi_dispatch::RfW;\n-      /* Notice serial mode, where we don't acquire locks at all.  */\n-      if (R == abi_dispatch::NOLOCK)\n-\treturn;\n-\n-      idst = isrc + size;\n-      for (isrc &= -CACHELINE_SIZE; isrc < idst; isrc += CACHELINE_SIZE)\n-\tdisp->read_lock(reinterpret_cast<const gtm_cacheline *>(isrc), R);\n-      return;\n-    }\n-\n-  /* Fall back to memcpy if the implementation above can handle it.  */\n-  if (idst < isrc || isrc + size <= idst)\n-    {\n-      do_memcpy (idst, isrc, size, W, R);\n-      return;\n-    }\n-\n-  /* What remains requires a backward copy from the end of the blocks.  */\n-  idst += size;\n-  isrc += size;\n-  dofs = idst & (CACHELINE_SIZE - 1);\n-  sofs = isrc & (CACHELINE_SIZE - 1);\n-  dleft = CACHELINE_SIZE - dofs;\n-  sleft = CACHELINE_SIZE - sofs;\n-\n-  gtm_cacheline *dst\n-    = reinterpret_cast<gtm_cacheline *>(idst & -CACHELINE_SIZE);\n-  const gtm_cacheline *src\n-    = reinterpret_cast<const gtm_cacheline *>(isrc & -CACHELINE_SIZE);\n-  if (dofs == 0)\n-    dst--;\n-  if (sofs == 0)\n-    src--;\n-\n-  if (dofs == sofs)\n-    {\n-      /* Since DST and SRC are co-aligned, and we didn't use the memcpy\n-\t optimization above, that implies that SIZE > CACHELINE_SIZE.  */\n-      if (sofs != 0)\n-\t{\n-\t  dpair = disp->write_lock(dst, W);\n-\t  sline = disp->read_lock(src, R);\n-\t  *dpair.mask |= ((gtm_cacheline_mask)1 << sleft) - 1;\n-\t  memcpy (dpair.line, sline, sleft);\n-\t  dst--;\n-\t  src--;\n-\t  size -= sleft;\n-\t}\n-\n-      while (size >= CACHELINE_SIZE)\n-\t{\n-\t  dpair = disp->write_lock(dst, W);\n-\t  sline = disp->read_lock(src, R);\n-\t  *dpair.mask = -1;\n-\t  *dpair.line = *sline;\n-\t  dst--;\n-\t  src--;\n-\t  size -= CACHELINE_SIZE;\n-\t}\n-\n-      if (size != 0)\n-\t{\n-\t  size_t ofs = CACHELINE_SIZE - size;\n-\t  dpair = disp->write_lock(dst, W);\n-\t  sline = disp->read_lock(src, R);\n-\t  *dpair.mask |= (((gtm_cacheline_mask)1 << size) - 1) << ofs;\n-\t  memcpy (&dpair.line->b[ofs], &sline->b[ofs], size);\n-\t}\n-    }\n-  else\n-    {\n-      gtm_cacheline c;\n-\n-      sline = disp->read_lock(src, R);\n-      if (dofs != 0)\n-\t{\n-\t  size_t min = (size <= dofs ? size : dofs);\n-\n-\t  if (min <= sofs)\n-\t    {\n-\t      sofs -= min;\n-\t      memcpy (&c, &sline->b[sofs], min);\n-\t    }\n-\t  else\n-\t    {\n-\t      size_t min_ofs = min - sofs;\n-\t      memcpy (&c.b[min_ofs], sline, sofs);\n-\t      sline = disp->read_lock(--src, R);\n-\t      sofs = CACHELINE_SIZE - min_ofs;\n-\t      memcpy (&c, &sline->b[sofs], min_ofs);\n-\t    }\n-\n-\t  dofs = dleft - min;\n-\t  dpair = disp->write_lock(dst, W);\n-\t  *dpair.mask |= (((gtm_cacheline_mask)1 << min) - 1) << dofs;\n-\t  memcpy (&dpair.line->b[dofs], &c, min);\n-\n-\t  sleft = CACHELINE_SIZE - sofs;\n-\t  dst--;\n-\t  size -= min;\n-\t}\n-\n-      while (size >= CACHELINE_SIZE)\n-\t{\n-\t  memcpy (&c.b[sleft], sline, sofs);\n-\t  sline = disp->read_lock(--src, R);\n-\t  memcpy (&c, &sline->b[sofs], sleft);\n-\n-\t  dpair = disp->write_lock(dst, W);\n-\t  *dpair.mask = -1;\n-\t  *dpair.line = c;\n-\n-\t  dst--;\n-\t  size -= CACHELINE_SIZE;\n-\t}\n-\n-      if (size != 0)\n-\t{\n-\t  dofs = CACHELINE_SIZE - size;\n-\n-\t  memcpy (&c.b[sleft], sline, sofs);\n-\t  if (sleft > dofs)\n-\t    {\n-\t      sline = disp->read_lock(--src, R);\n-\t      memcpy (&c, &sline->b[sofs], sleft);\n-\t    }\n-\n-\t  dpair = disp->write_lock(dst, W);\n-\t  *dpair.mask |= (gtm_cacheline_mask)-1 << dofs;\n-\t  memcpy (&dpair.line->b[dofs], &c.b[dofs], size);\n-\t}\n-    }\n-}\n-\n-#define ITM_MEM_DEF(NAME, READ, WRITE) \\\n-void ITM_REGPARM _ITM_memcpy##NAME(void *dst, const void *src, size_t size)  \\\n-{\t\t\t\t\t\t\t\t\t     \\\n-  do_memcpy ((uintptr_t)dst, (uintptr_t)src, size,\t\t\t     \\\n-\t     abi_dispatch::WRITE, abi_dispatch::READ);\t\t\t     \\\n-}\t\t\t\t\t\t\t\t\t     \\\n-void ITM_REGPARM _ITM_memmove##NAME(void *dst, const void *src, size_t size) \\\n-{\t\t\t\t\t\t\t\t\t     \\\n-  do_memmove ((uintptr_t)dst, (uintptr_t)src, size,\t\t\t     \\\n-\t      abi_dispatch::WRITE, abi_dispatch::READ);\t\t\t     \\\n-}\n-\n-ITM_MEM_DEF(RnWt,\tNOLOCK,\t\tW)\n-ITM_MEM_DEF(RnWtaR,\tNOLOCK,\t\tWaR)\n-ITM_MEM_DEF(RnWtaW,\tNOLOCK,\t\tWaW)\n-\n-ITM_MEM_DEF(RtWn,\tR,\t\tNOLOCK)\n-ITM_MEM_DEF(RtWt,\tR,\t\tW)\n-ITM_MEM_DEF(RtWtaR,\tR,\t\tWaR)\n-ITM_MEM_DEF(RtWtaW,\tR,\t\tWaW)\n-\n-ITM_MEM_DEF(RtaRWn,\tRaR,\t\tNOLOCK)\n-ITM_MEM_DEF(RtaRWt,\tRaR,\t\tW)\n-ITM_MEM_DEF(RtaRWtaR,\tRaR,\t\tWaR)\n-ITM_MEM_DEF(RtaRWtaW,\tRaR,\t\tWaW)\n-\n-ITM_MEM_DEF(RtaWWn,\tRaW,\t\tNOLOCK)\n-ITM_MEM_DEF(RtaWWt,\tRaW,\t\tW)\n-ITM_MEM_DEF(RtaWWtaR,\tRaW,\t\tWaR)\n-ITM_MEM_DEF(RtaWWtaW,\tRaW,\t\tWaW)"}, {"sha": "3a627dd6c7d17f7876135a2776777ff8cd365f62", "filename": "libitm/memset.cc", "status": "removed", "additions": 0, "deletions": 78, "changes": 78, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fmemset.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fmemset.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fmemset.cc?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,78 +0,0 @@\n-/* Copyright (C) 2008, 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#include \"libitm_i.h\"\n-\n-using namespace GTM;\n-\n-static void\n-do_memset(uintptr_t idst, int c, size_t size, abi_dispatch::lock_type W)\n-{\n-  abi_dispatch *disp = abi_disp();\n-  uintptr_t dofs = idst & (CACHELINE_SIZE - 1);\n-  abi_dispatch::mask_pair dpair;\n-  gtm_cacheline *dst\n-    = reinterpret_cast<gtm_cacheline *>(idst & -CACHELINE_SIZE);\n-\n-  if (size == 0)\n-    return;\n-\n-  if (dofs != 0)\n-    {\n-      size_t dleft = CACHELINE_SIZE - dofs;\n-      size_t min = (size <= dleft ? size : dleft);\n-\n-      dpair = disp->write_lock(dst, W);\n-      *dpair.mask |= (((gtm_cacheline_mask)1 << min) - 1) << dofs;\n-      memset (&dpair.line->b[dofs], c, min);\n-      dst++;\n-      size -= min;\n-    }\n-\n-  while (size >= CACHELINE_SIZE)\n-    {\n-      dpair = disp->write_lock(dst, W);\n-      *dpair.mask = -1;\n-      memset (dpair.line, c, CACHELINE_SIZE);\n-      dst++;\n-      size -= CACHELINE_SIZE;\n-    }\n-\n-  if (size != 0)\n-    {\n-      dpair = disp->write_lock(dst, W);\n-      *dpair.mask |= ((gtm_cacheline_mask)1 << size) - 1;\n-      memset (dpair.line, c, size);\n-    }\n-}\n-\n-#define ITM_MEM_DEF(WRITE) \\\n-void ITM_REGPARM _ITM_memset##WRITE(void *dst, int c, size_t size)\t\\\n-{\t\t\t\t\t\t\t\t\t\\\n-  do_memset ((uintptr_t)dst, c, size, abi_dispatch::WRITE);\t\t\\\n-}\n-\n-ITM_MEM_DEF(W)\n-ITM_MEM_DEF(WaR)\n-ITM_MEM_DEF(WaW)"}, {"sha": "093d1c769f199bf751c3e951afd622cf071f8efc", "filename": "libitm/method-wbetl.cc", "status": "removed", "additions": 0, "deletions": 628, "changes": 628, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fmethod-wbetl.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb8010f922d1d8f39a3035ffcce56dfca32fb822/libitm%2Fmethod-wbetl.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fmethod-wbetl.cc?ref=cb8010f922d1d8f39a3035ffcce56dfca32fb822", "patch": "@@ -1,628 +0,0 @@\n-/* Copyright (C) 2009, 2011 Free Software Foundation, Inc.\n-   Contributed by Richard Henderson <rth@redhat.com>.\n-\n-   This file is part of the GNU Transactional Memory Library (libitm).\n-\n-   Libitm is free software; you can redistribute it and/or modify it\n-   under the terms of the GNU General Public License as published by\n-   the Free Software Foundation; either version 3 of the License, or\n-   (at your option) any later version.\n-\n-   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n-   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n-   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n-   more details.\n-\n-   Under Section 7 of GPL version 3, you are granted additional\n-   permissions described in the GCC Runtime Library Exception, version\n-   3.1, as published by the Free Software Foundation.\n-\n-   You should have received a copy of the GNU General Public License and\n-   a copy of the GCC Runtime Library Exception along with this program;\n-   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n-   <http://www.gnu.org/licenses/>.  */\n-\n-#include \"libitm_i.h\"\n-\n-namespace {\n-\n-using namespace GTM;\n-\n-class wbetl_dispatch : public abi_dispatch\n-{\n- private:\n-  static const size_t RW_SET_SIZE = 4096;\n-\n-  struct r_entry\n-  {\n-    gtm_version version;\n-    gtm_stmlock *lock;\n-  };\n-\n-  r_entry *m_rset_entries;\n-  size_t m_rset_nb_entries;\n-  size_t m_rset_size;\n-\n-  struct w_entry\n-  {\n-    /* There's a hashtable where the locks are held, so multiple\n-       cachelines can hash to a given bucket.  This link points to the\n-       possible next cacheline that also hashes to this bucket.  */\n-    struct w_entry *next;\n-\n-    /* Every entry in this bucket (accessed by NEXT) has the same LOCK\n-       address below.  */\n-    gtm_stmlock *lock;\n-\n-    gtm_cacheline *addr;\n-    gtm_cacheline *value;\n-    gtm_version version;\n-  };\n-\n-  w_entry *m_wset_entries;\n-  size_t m_wset_nb_entries;\n-  size_t m_wset_size;\n-  bool m_wset_reallocate;\n-\n-  gtm_version m_start;\n-  gtm_version m_end;\n-\n-  gtm_cacheline_page *m_cache_page;\n-  unsigned m_n_cache_page;\n-\n- private:\n-  bool local_w_entry_p (w_entry *w);\n-  bool has_read (gtm_stmlock *lock);\n-  bool validate();\n-  bool extend();\n-\n-  gtm_cacheline *do_write_lock(gtm_cacheline *);\n-  gtm_cacheline *do_after_write_lock(gtm_cacheline *);\n-  const gtm_cacheline *do_read_lock(const gtm_cacheline *, bool);\n-\n- public:\n-  wbetl_dispatch();\n-\n-  virtual const gtm_cacheline *read_lock(const gtm_cacheline *, ls_modifier);\n-  virtual mask_pair write_lock(gtm_cacheline *, ls_modifier);\n-\n-  virtual bool trycommit();\n-  virtual void rollback();\n-  virtual void reinit();\n-  virtual void fini();\n-  virtual bool trydropreference (void *, size_t);\n-};\n-\n-/* Check if W is one of our write locks.  */\n-\n-inline bool\n-wbetl_dispatch::local_w_entry_p (w_entry *w)\n-{\n-  return (m_wset_entries <= w && w < m_wset_entries + m_wset_nb_entries);\n-}\n-\n-/* Check if stripe has been read previously.  */\n-\n-inline bool\n-wbetl_dispatch::has_read (gtm_stmlock *lock)\n-{\n-  // ??? Consider using an AA tree to lookup the r_set entries.\n-  size_t n = m_rset_nb_entries;\n-  for (size_t i = 0; i < n; ++i)\n-    if (m_rset_entries[i].lock == lock)\n-      return true;\n-\n-  return false;\n-}\n-\n-/* Validate read set, i.e. check if all read addresses are still valid now.  */\n-\n-bool\n-wbetl_dispatch::validate ()\n-{\n-  __sync_synchronize ();\n-\n-  size_t n = m_rset_nb_entries;\n-  for (size_t i = 0; i < n; ++i)\n-    {\n-      r_entry *r = &m_rset_entries[i];\n-      gtm_stmlock l = *r->lock;\n-\n-      if (gtm_stmlock_owned_p (l))\n-\t{\n-\t  w_entry *w = (w_entry *) gtm_stmlock_get_addr (l);\n-\n-\t  // If someone has locked us, it better be by someone in the\n-\t  // current thread.\n-\t  if (!local_w_entry_p (w))\n-\t    return false;\n-\t}\n-      else if (gtm_stmlock_get_version (l) != r->version)\n-\treturn false;\n-    }\n-\n-  return true;\n-}\n-\n-/* Extend the snapshot range.  */\n-\n-bool\n-wbetl_dispatch::extend ()\n-{\n-  gtm_version now = gtm_get_clock ();\n-\n-  if (validate ())\n-    {\n-      m_end = now;\n-      return true;\n-    }\n-  return false;\n-}\n-\n-/* Acquire a write lock on ADDR.  */\n-\n-gtm_cacheline *\n-wbetl_dispatch::do_write_lock(gtm_cacheline *addr)\n-{\n-  gtm_stmlock *lock;\n-  gtm_stmlock l, l2;\n-  gtm_version version;\n-  w_entry *w, *prev = NULL;\n-\n-  lock = gtm_get_stmlock (addr);\n-  l = *lock;\n-\n- restart_no_load:\n-  if (gtm_stmlock_owned_p (l))\n-    {\n-      w = (w_entry *) gtm_stmlock_get_addr (l);\n-\n-      /* Did we previously write the same address?  */\n-      if (local_w_entry_p (w))\n-\t{\n-\t  prev = w;\n-\t  while (1)\n-\t    {\n-\t      if (addr == prev->addr)\n-\t\treturn prev->value;\n-\t      if (prev->next == NULL)\n-\t\tbreak;\n-\t      prev = prev->next;\n-\t    }\n-\n-\t  /* Get version from previous entry write set.  */\n-\t  version = prev->version;\n-\n-\t  /* If there's not enough entries, we must reallocate the array,\n-\t     which invalidates all pointers to write set entries, which\n-\t     means we have to restart the transaction.  */\n-\t  if (m_wset_nb_entries == m_wset_size)\n-\t    {\n-\t      m_wset_size *= 2;\n-\t      m_wset_reallocate = true;\n-\t      gtm_tx()->restart (RESTART_REALLOCATE);\n-\t    }\n-\n-\t  w = &m_wset_entries[m_wset_nb_entries];\n-\t  goto do_write;\n-\t}\n-\n-      gtm_tx()->restart (RESTART_LOCKED_WRITE);\n-    }\n-  else\n-    {\n-      version = gtm_stmlock_get_version (l);\n-\n-      /* We might have read an older version previously.  */\n-      if (version > m_end)\n-\t{\n-\t  if (has_read (lock))\n-\t    gtm_tx()->restart (RESTART_VALIDATE_WRITE);\n-\t}\n-\n-      /* Extend write set, aborting to reallocate write set entries.  */\n-      if (m_wset_nb_entries == m_wset_size)\n-\t{\n-\t  m_wset_size *= 2;\n-\t  m_wset_reallocate = true;\n-\t  gtm_tx()->restart (RESTART_REALLOCATE);\n-\t}\n-\n-      /* Acquire the lock.  */\n-      w = &m_wset_entries[m_wset_nb_entries];\n-      l2 = gtm_stmlock_set_owned (w);\n-      l = __sync_val_compare_and_swap (lock, l, l2);\n-      if (l != l2)\n-\tgoto restart_no_load;\n-    }\n-\n- do_write:\n-  m_wset_nb_entries++;\n-  if (prev != NULL)\n-    prev->next = w;\n-  w->next = 0;\n-  w->lock = lock;\n-  w->addr = addr;\n-  w->version = version;\n-\n-  gtm_cacheline_page *page = m_cache_page;\n-  unsigned index = m_n_cache_page;\n-\n-  if (page == NULL || index == gtm_cacheline_page::LINES)\n-    {\n-      gtm_cacheline_page *npage = new gtm_cacheline_page;\n-      npage->prev = page;\n-      m_cache_page = page = npage;\n-      m_n_cache_page = 1;\n-      index = 0;\n-    }\n-  else\n-    m_n_cache_page = index + 1;\n-\n-  gtm_cacheline *line = &page->lines[index];\n-  w->value = line;\n-  page->masks[index] = 0;\n-  *line = *addr;\n-\n-  return line;\n-}\n-\n-gtm_cacheline *\n-wbetl_dispatch::do_after_write_lock (gtm_cacheline *addr)\n-{\n-  gtm_stmlock *lock;\n-  gtm_stmlock l;\n-  w_entry *w;\n-\n-  lock = gtm_get_stmlock (addr);\n-  l = *lock;\n-  assert (gtm_stmlock_owned_p (l));\n-\n-  w = (w_entry *) gtm_stmlock_get_addr (l);\n-  assert (local_w_entry_p (w));\n-\n-  while (1)\n-    {\n-      if (addr == w->addr)\n-\treturn w->value;\n-      w = w->next;\n-    }\n-}\n-\n-/* Acquire a read lock on ADDR.  */\n-\n-const gtm_cacheline *\n-wbetl_dispatch::do_read_lock (const gtm_cacheline *addr, bool after_read)\n-{\n-  gtm_stmlock *lock;\n-  gtm_stmlock l, l2;\n-  gtm_version version;\n-  w_entry *w;\n-\n-  lock = gtm_get_stmlock (addr);\n-  l = *lock;\n-\n- restart_no_load:\n-  if (gtm_stmlock_owned_p (l))\n-    {\n-      w = (w_entry *) gtm_stmlock_get_addr (l);\n-\n-      /* Did we previously write the same address?  */\n-      if (local_w_entry_p (w))\n-\t{\n-\t  while (1)\n-\t    {\n-\t      if (addr == w->addr)\n-\t\treturn w->value;\n-\t      if (w->next == NULL)\n-\t\treturn addr;\n-\t      w = w->next;\n-\t    }\n-\t}\n-\n-      gtm_tx()->restart (RESTART_LOCKED_READ);\n-    }\n-\n-  version = gtm_stmlock_get_version (l);\n-\n-  /* If version is no longer valid, re-validate the read set.  */\n-  if (version > m_end)\n-    {\n-      if (!extend ())\n-\tgtm_tx()->restart (RESTART_VALIDATE_READ);\n-\n-      if (!after_read)\n-\t{\n-\t  // Verify that the version has not yet been overwritten.  The read\n-\t  // value has not yet been added to read set and may not have been\n-\t  // checked during the extend.\n-\t  //\n-\t  // ??? This only makes sense if we're actually reading the value\n-\t  // and returning it now -- which I believe the original TinySTM\n-\t  // did.  This doesn't make a whole lot of sense when we're\n-\t  // manipulating cachelines as we are now.  Do we need some other\n-\t  // form of lock verification here, or is the validate call in\n-\t  // trycommit sufficient?\n-\n-\t  __sync_synchronize ();\n-\t  l2 = *lock;\n-\t  if (l != l2)\n-\t    {\n-\t      l = l2;\n-\t      goto restart_no_load;\n-\t    }\n-\t}\n-    }\n-\n-  if (!after_read)\n-    {\n-      r_entry *r;\n-\n-      /* Add the address and version to the read set.  */\n-      if (m_rset_nb_entries == m_rset_size)\n-\t{\n-\t  m_rset_size *= 2;\n-\n-\t  m_rset_entries = (r_entry *)\n-\t    xrealloc (m_rset_entries, m_rset_size * sizeof(r_entry));\n-\t}\n-      r = &m_rset_entries[m_rset_nb_entries++];\n-      r->version = version;\n-      r->lock = lock;\n-    }\n-\n-  return addr;\n-}\n-\n-const gtm_cacheline *\n-wbetl_dispatch::read_lock (const gtm_cacheline *addr, ls_modifier ltype)\n-{\n-  switch (ltype)\n-    {\n-    case NONTXNAL:\n-      return addr;\n-    case R:\n-      return do_read_lock (addr, false);\n-    case RaR:\n-      return do_read_lock (addr, true);\n-    case RaW:\n-      return do_after_write_lock (const_cast<gtm_cacheline *>(addr));\n-    case RfW:\n-      return do_write_lock (const_cast<gtm_cacheline *>(addr));\n-    default:\n-      abort ();\n-    }\n-}\n-\n-abi_dispatch::mask_pair\n-wbetl_dispatch::write_lock (gtm_cacheline *addr, ls_modifier ltype)\n-{\n-  gtm_cacheline *line;\n-\n-  switch (ltype)\n-    {\n-    case NONTXNAL:\n-      return mask_pair (addr, &mask_sink);\n-    case W:\n-    case WaR:\n-      line = do_write_lock (addr);\n-      break;\n-    case WaW:\n-      line = do_after_write_lock (addr);\n-      break;\n-    default:\n-      abort ();\n-    }\n-\n-  return mask_pair (line, gtm_cacheline_page::mask_for_page_line (line));\n-}\n-\n-/* Commit the transaction.  */\n-\n-bool\n-wbetl_dispatch::trycommit ()\n-{\n-  const size_t n = m_wset_nb_entries;\n-  if (n != 0)\n-    {\n-      /* Get commit timestamp.  */\n-      gtm_version t = gtm_inc_clock ();\n-\n-      /* Validate only if a concurrent transaction has started since.  */\n-      if (m_start != t - 1 && !validate ())\n-\treturn false;\n-\n-      /* Install new versions.  */\n-      for (size_t i = 0; i < n; ++i)\n-\t{\n-\t  w_entry *w = &m_wset_entries[i];\n-\t  gtm_cacheline_mask mask\n-\t    = *gtm_cacheline_page::mask_for_page_line (w->value);\n-\n-\t  /* Filter out any updates that overlap the libitm stack.  */\n-\t  mask = gtm_mask_stack (w->addr, mask);\n-\n-\t  gtm_cacheline::copy_mask (w->addr, w->value, mask);\n-\t}\n-\n-      /* Only emit barrier after all cachelines are copied.  */\n-      gtm_cacheline::copy_mask_wb ();\n-\n-      /* Drop locks.  */\n-      for (size_t i = 0; i < n; ++i)\n-\t{\n-\t  w_entry *w = &m_wset_entries[i];\n-\n-\t  /* Every link along the chain has the same lock, but only\n-\t     bother dropping the lock once per bucket (at the end).  */\n-\t  if (w->next == NULL)\n-\t    *w->lock = gtm_stmlock_set_version (t);\n-\t}\n-    }\n-\n-  __sync_synchronize ();\n-  return true;\n-}\n-\n-void\n-wbetl_dispatch::rollback ()\n-{\n-  /* Drop locks.  */\n-  const size_t n = m_wset_nb_entries;\n-  for (size_t i = 0; i < n; ++i)\n-    {\n-      w_entry *w = &m_wset_entries[i];\n-\n-      /* Every link along the chain has the same lock, but only\n-\t bother dropping the lock once per bucket (at the end).  */\n-      if (w->next == NULL)\n-\t*w->lock = gtm_stmlock_set_version (w->version);\n-    }\n-\n-  __sync_synchronize ();\n-}\n-\n-void\n-wbetl_dispatch::reinit ()\n-{\n-  gtm_cacheline_page *page;\n-\n-  m_rset_nb_entries = 0;\n-  m_wset_nb_entries = 0;\n-\n-  if (m_wset_reallocate)\n-    {\n-      m_wset_reallocate = 0;\n-      m_wset_entries = (w_entry *)\n-\txrealloc (m_wset_entries, m_wset_size * sizeof(w_entry));\n-    }\n-\n-  page = m_cache_page;\n-  if (page)\n-    {\n-      /* Release all but one of the pages of cachelines.  */\n-      gtm_cacheline_page *prev = page->prev;\n-      if (prev)\n-\t{\n-\t  page->prev = 0;\n-\t  delete prev;\n-\t}\n-\n-      /* Start the next cacheline allocation from the beginning.  */\n-      m_n_cache_page = 0;\n-    }\n-\n-  m_start = m_end = gtm_get_clock ();\n-}\n-\n-void\n-wbetl_dispatch::fini ()\n-{\n-  delete m_cache_page;\n-  free (m_rset_entries);\n-  free (m_wset_entries);\n-  delete this;\n-}\n-\n-/* Attempt to drop any internal references to PTR.  Return TRUE if successful.\n-\n-   This is an adaptation of the transactional memcpy function.\n-\n-   What we do here is flush out the current transactional content of\n-   PTR to real memory, and remove the write mask bits associated with\n-   it so future commits will ignore this piece of memory.  */\n-\n-bool\n-wbetl_dispatch::trydropreference (void *ptr, size_t size)\n-{\n-  if (size == 0)\n-    return true;\n-\n-  if (!validate ())\n-    return false;\n-\n-  uintptr_t isrc = (uintptr_t)ptr;\n-  // The position in the source cacheline where *PTR starts.\n-  uintptr_t sofs = isrc & (CACHELINE_SIZE - 1);\n-  gtm_cacheline *src\n-    = reinterpret_cast<gtm_cacheline *>(isrc & -CACHELINE_SIZE);\n-  unsigned char *dst = (unsigned char *)ptr;\n-  abi_dispatch::mask_pair pair;\n-\n-  // If we're trying to drop a reference, we should already have a\n-  // write lock on it.  If we don't have one, there's no work to do.\n-  if (!gtm_stmlock_owned_p (*gtm_get_stmlock (src)))\n-    return true;\n-\n-  // We copy the data in three stages:\n-\n-  // (a) Copy stray bytes at the beginning that are smaller than a\n-  // cacheline.\n-  if (sofs != 0)\n-    {\n-      size_t sleft = CACHELINE_SIZE - sofs;\n-      size_t min = (size <= sleft ? size : sleft);\n-\n-      // WaW will give us the current locked entry.\n-      pair = this->write_lock (src, WaW);\n-\n-      // *jedi mind wave*...these aren't the droids you're looking for.\n-      *pair.mask &= ~((((gtm_cacheline_mask)1 << min) - 1) << sofs);\n-\n-      memcpy (dst, &pair.line->b[sofs], min);\n-      dst += min;\n-      src++;\n-      size -= min;\n-    }\n-\n-  // (b) Copy subsequent cacheline sized chunks.\n-  while (size >= CACHELINE_SIZE)\n-    {\n-      pair = this->write_lock(src, WaW);\n-      *pair.mask = 0;\n-      memcpy (dst, pair.line, CACHELINE_SIZE);\n-      dst += CACHELINE_SIZE;\n-      src++;\n-      size -= CACHELINE_SIZE;\n-    }\n-\n-  // (c) Copy anything left over.\n-  if (size != 0)\n-    {\n-      pair = this->write_lock(src, WaW);\n-      *pair.mask &= ~(((gtm_cacheline_mask)1 << size) - 1);\n-      memcpy (dst, pair.line, size);\n-    }\n-\n-  // No need to drop locks, since we're going to abort the transaction\n-  // anyhow.\n-\n-  return true;\n-}\n-\n-\n-wbetl_dispatch::wbetl_dispatch ()\n-  : abi_dispatch (false, false)\n-{\n-  m_rset_entries = (r_entry *) xmalloc (RW_SET_SIZE * sizeof(r_entry));\n-  m_rset_nb_entries = 0;\n-  m_rset_size = RW_SET_SIZE;\n-\n-  m_wset_entries = (w_entry *) xmalloc (RW_SET_SIZE * sizeof(w_entry));\n-  m_wset_nb_entries = 0;\n-  m_wset_size = RW_SET_SIZE;\n-  m_wset_reallocate = false;\n-\n-  m_start = m_end = gtm_get_clock ();\n-\n-  m_cache_page = 0;\n-  m_n_cache_page = 0;\n-}\n-\n-} // anon namespace\n-\n-abi_dispatch *\n-GTM::dispatch_wbetl ()\n-{\n-  return new wbetl_dispatch ();\n-}"}, {"sha": "6990cfeb6812bec5a74dd9efa402b7e9715a88b3", "filename": "libitm/testsuite/Makefile.in", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Ftestsuite%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/79b1edb6b5142f414368add71ec8ba37d0cd028f/libitm%2Ftestsuite%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Ftestsuite%2FMakefile.in?ref=79b1edb6b5142f414368add71ec8ba37d0cd028f", "patch": "@@ -38,6 +38,7 @@ subdir = testsuite\n DIST_COMMON = $(srcdir)/Makefile.am $(srcdir)/Makefile.in\n ACLOCAL_M4 = $(top_srcdir)/aclocal.m4\n am__aclocal_m4_deps = $(top_srcdir)/../config/acx.m4 \\\n+\t$(top_srcdir)/../config/asmcfi.m4 \\\n \t$(top_srcdir)/../config/depstand.m4 \\\n \t$(top_srcdir)/../config/enable.m4 \\\n \t$(top_srcdir)/../config/futex.m4 \\\n@@ -90,8 +91,6 @@ ECHO_N = @ECHO_N@\n ECHO_T = @ECHO_T@\n EGREP = @EGREP@\n EXEEXT = @EXEEXT@\n-FC = @FC@\n-FCFLAGS = @FCFLAGS@\n FGREP = @FGREP@\n GREP = @GREP@\n INSTALL = @INSTALL@\n@@ -142,7 +141,6 @@ abs_top_srcdir = @abs_top_srcdir@\n ac_ct_CC = @ac_ct_CC@\n ac_ct_CXX = @ac_ct_CXX@\n ac_ct_DUMPBIN = @ac_ct_DUMPBIN@\n-ac_ct_FC = @ac_ct_FC@\n am__include = @am__include@\n am__leading_dot = @am__leading_dot@\n am__quote = @am__quote@"}]}