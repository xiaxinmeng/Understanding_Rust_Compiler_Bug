{"sha": "8845cb377abb4e2ede6cc5c1a6bc635d71063fd2", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODg0NWNiMzc3YWJiNGUyZWRlNmNjNWMxYTZiYzYzNWQ3MTA2M2ZkMg==", "commit": {"author": {"name": "Aaron Sawdey", "email": "acsawdey@linux.vnet.ibm.com", "date": "2017-06-23T19:59:42Z"}, "committer": {"name": "Aaron Sawdey", "email": "acsawdey@gcc.gnu.org", "date": "2017-06-23T19:59:42Z"}, "message": "rs6000-string.c: (expand_block_clear...\n\n2017-06-23  Aaron Sawdey  <acsawdey@linux.vnet.ibm.com>\n\n\t* config/rs6000/rs6000-string.c: (expand_block_clear,\n\tdo_load_for_compare, select_block_compare_mode,\n\tcompute_current_alignment, expand_block_compare,\n\texpand_strncmp_align_check, expand_strn_compare,\n\texpand_block_move, rs6000_output_load_multiple)\n\tMove functions related to string/block move/compare\n\tto a separate file.\n\t* config/rs6000/rs6000.c: Move above functions to rs6000-string.c.\n\t* config/rs6000/rs6000-protos.h (rs6000_emit_dot_insn): Add prototype\n\tfor this function which is now used in two files.\n\t* config/rs6000/t-rs6000: Add rule to compile rs6000-string.o.\n\t* config.gcc: Add rs6000-string.o to extra_objs for\n\ttargets powerpc*-*-* and rs6000*-*-*.\n\nFrom-SVN: r249608", "tree": {"sha": "6b52e92334dba52e520a19c81ecd7a045718e1ba", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6b52e92334dba52e520a19c81ecd7a045718e1ba"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/comments", "author": null, "committer": null, "parents": [{"sha": "37416b699f362c378a0351c3b2b2e32754a1cc76", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/37416b699f362c378a0351c3b2b2e32754a1cc76", "html_url": "https://github.com/Rust-GCC/gccrs/commit/37416b699f362c378a0351c3b2b2e32754a1cc76"}], "stats": {"total": 2921, "additions": 1489, "deletions": 1432}, "files": [{"sha": "5cb2af40718f2847745b437f89be01c70ffd7f3f", "filename": "gcc/ChangeLog", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=8845cb377abb4e2ede6cc5c1a6bc635d71063fd2", "patch": "@@ -1,3 +1,19 @@\n+2017-06-23  Aaron Sawdey  <acsawdey@linux.vnet.ibm.com>\n+\n+\t* config/rs6000/rs6000-string.c: (expand_block_clear,\n+\tdo_load_for_compare, select_block_compare_mode,\n+\tcompute_current_alignment, expand_block_compare,\n+\texpand_strncmp_align_check, expand_strn_compare,\n+\texpand_block_move, rs6000_output_load_multiple)\n+\tMove functions related to string/block move/compare\n+\tto a separate file.\n+\t* config/rs6000/rs6000.c: Move above functions to rs6000-string.c.\n+\t* config/rs6000/rs6000-protos.h (rs6000_emit_dot_insn): Add prototype\n+\tfor this function which is now used in two files.\n+\t* config/rs6000/t-rs6000: Add rule to compile rs6000-string.o.\n+\t* config.gcc: Add rs6000-string.o to extra_objs for\n+\ttargets powerpc*-*-* and rs6000*-*-*.\n+\n 2017-06-23  Michael Meissner  <meissner@linux.vnet.ibm.com>\n \n \tPR target/80510"}, {"sha": "d5609c024fdf2d7077563041e3cd203bc6441095", "filename": "gcc/config.gcc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=8845cb377abb4e2ede6cc5c1a6bc635d71063fd2", "patch": "@@ -454,6 +454,7 @@ powerpc*-*-*spe*)\n \t;;\n powerpc*-*-*)\n \tcpu_type=rs6000\n+\textra_objs=\"rs6000-string.o\"\n \textra_headers=\"ppc-asm.h altivec.h htmintrin.h htmxlintrin.h\"\n \textra_headers=\"${extra_headers} bmi2intrin.h bmiintrin.h x86intrin.h\"\n \textra_headers=\"${extra_headers} ppu_intrinsics.h spu2vmx.h vec_types.h si2vmx.h\"\n@@ -471,6 +472,7 @@ riscv*)\n \t;;\n rs6000*-*-*)\n \textra_options=\"${extra_options} g.opt fused-madd.opt rs6000/rs6000-tables.opt\"\n+\textra_objs=\"rs6000-string.o\"\n \t;;\n sparc*-*-*)\n \tcpu_type=sparc"}, {"sha": "e3132049270fbeab4fa5f017c1604e978a74e7c6", "filename": "gcc/config/rs6000/rs6000-protos.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2Fconfig%2Frs6000%2Frs6000-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2Fconfig%2Frs6000%2Frs6000-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000-protos.h?ref=8845cb377abb4e2ede6cc5c1a6bc635d71063fd2", "patch": "@@ -134,6 +134,7 @@ extern void rs6000_emit_sCOND (machine_mode, rtx[]);\n extern void rs6000_emit_cbranch (machine_mode, rtx[]);\n extern char * output_cbranch (rtx, const char *, int, rtx_insn *);\n extern const char * output_probe_stack_range (rtx, rtx);\n+extern void rs6000_emit_dot_insn (rtx dst, rtx src, int dot, rtx ccreg);\n extern bool rs6000_emit_set_const (rtx, rtx);\n extern int rs6000_emit_cmove (rtx, rtx, rtx, rtx);\n extern int rs6000_emit_vector_cond_expr (rtx, rtx, rtx, rtx, rtx, rtx);"}, {"sha": "c2fd056fb4670d3d9004f509cd2bdff43afd77de", "filename": "gcc/config/rs6000/rs6000-string.c", "status": "added", "additions": 1465, "deletions": 0, "changes": 1465, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2Fconfig%2Frs6000%2Frs6000-string.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2Fconfig%2Frs6000%2Frs6000-string.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000-string.c?ref=8845cb377abb4e2ede6cc5c1a6bc635d71063fd2", "patch": "@@ -0,0 +1,1465 @@\n+/* Subroutines used to expand string and block move, clear,\n+   compare and other operations for PowerPC.\n+   Copyright (C) 1991-2017 Free Software Foundation, Inc.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published\n+   by the Free Software Foundation; either version 3, or (at your\n+   option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT\n+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY\n+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public\n+   License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"backend.h\"\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"memmodel.h\"\n+#include \"tm_p.h\"\n+#include \"ira.h\"\n+#include \"print-tree.h\"\n+#include \"varasm.h\"\n+#include \"explow.h\"\n+#include \"expr.h\"\n+#include \"output.h\"\n+\n+/* Expand a block clear operation, and return 1 if successful.  Return 0\n+   if we should let the compiler generate normal code.\n+\n+   operands[0] is the destination\n+   operands[1] is the length\n+   operands[3] is the alignment */\n+\n+int\n+expand_block_clear (rtx operands[])\n+{\n+  rtx orig_dest = operands[0];\n+  rtx bytes_rtx\t= operands[1];\n+  rtx align_rtx = operands[3];\n+  bool constp\t= (GET_CODE (bytes_rtx) == CONST_INT);\n+  HOST_WIDE_INT align;\n+  HOST_WIDE_INT bytes;\n+  int offset;\n+  int clear_bytes;\n+  int clear_step;\n+\n+  /* If this is not a fixed size move, just call memcpy */\n+  if (! constp)\n+    return 0;\n+\n+  /* This must be a fixed size alignment  */\n+  gcc_assert (GET_CODE (align_rtx) == CONST_INT);\n+  align = INTVAL (align_rtx) * BITS_PER_UNIT;\n+\n+  /* Anything to clear? */\n+  bytes = INTVAL (bytes_rtx);\n+  if (bytes <= 0)\n+    return 1;\n+\n+  /* Use the builtin memset after a point, to avoid huge code bloat.\n+     When optimize_size, avoid any significant code bloat; calling\n+     memset is about 4 instructions, so allow for one instruction to\n+     load zero and three to do clearing.  */\n+  if (TARGET_ALTIVEC && align >= 128)\n+    clear_step = 16;\n+  else if (TARGET_POWERPC64 && (align >= 64 || !STRICT_ALIGNMENT))\n+    clear_step = 8;\n+  else\n+    clear_step = 4;\n+\n+  if (optimize_size && bytes > 3 * clear_step)\n+    return 0;\n+  if (! optimize_size && bytes > 8 * clear_step)\n+    return 0;\n+\n+  for (offset = 0; bytes > 0; offset += clear_bytes, bytes -= clear_bytes)\n+    {\n+      machine_mode mode = BLKmode;\n+      rtx dest;\n+\n+      if (bytes >= 16 && TARGET_ALTIVEC && align >= 128)\n+\t{\n+\t  clear_bytes = 16;\n+\t  mode = V4SImode;\n+\t}\n+      else if (bytes >= 8 && TARGET_POWERPC64\n+\t       && (align >= 64 || !STRICT_ALIGNMENT))\n+\t{\n+\t  clear_bytes = 8;\n+\t  mode = DImode;\n+\t  if (offset == 0 && align < 64)\n+\t    {\n+\t      rtx addr;\n+\n+\t      /* If the address form is reg+offset with offset not a\n+\t\t multiple of four, reload into reg indirect form here\n+\t\t rather than waiting for reload.  This way we get one\n+\t\t reload, not one per store.  */\n+\t      addr = XEXP (orig_dest, 0);\n+\t      if ((GET_CODE (addr) == PLUS || GET_CODE (addr) == LO_SUM)\n+\t\t  && GET_CODE (XEXP (addr, 1)) == CONST_INT\n+\t\t  && (INTVAL (XEXP (addr, 1)) & 3) != 0)\n+\t\t{\n+\t\t  addr = copy_addr_to_reg (addr);\n+\t\t  orig_dest = replace_equiv_address (orig_dest, addr);\n+\t\t}\n+\t    }\n+\t}\n+      else if (bytes >= 4 && (align >= 32 || !STRICT_ALIGNMENT))\n+\t{\t\t\t/* move 4 bytes */\n+\t  clear_bytes = 4;\n+\t  mode = SImode;\n+\t}\n+      else if (bytes >= 2 && (align >= 16 || !STRICT_ALIGNMENT))\n+\t{\t\t\t/* move 2 bytes */\n+\t  clear_bytes = 2;\n+\t  mode = HImode;\n+\t}\n+      else /* move 1 byte at a time */\n+\t{\n+\t  clear_bytes = 1;\n+\t  mode = QImode;\n+\t}\n+\n+      dest = adjust_address (orig_dest, mode, offset);\n+\n+      emit_move_insn (dest, CONST0_RTX (mode));\n+    }\n+\n+  return 1;\n+}\n+\n+/* Figure out the correct instructions to generate to load data for\n+   block compare.  MODE is used for the read from memory, and\n+   data is zero extended if REG is wider than MODE.  If LE code\n+   is being generated, bswap loads are used.\n+\n+   REG is the destination register to move the data into.\n+   MEM is the memory block being read.\n+   MODE is the mode of memory to use for the read.  */\n+static void\n+do_load_for_compare (rtx reg, rtx mem, machine_mode mode)\n+{\n+  switch (GET_MODE (reg))\n+    {\n+    case DImode:\n+      switch (mode)\n+\t{\n+\tcase QImode:\n+\t  emit_insn (gen_zero_extendqidi2 (reg, mem));\n+\t  break;\n+\tcase HImode:\n+\t  {\n+\t    rtx src = mem;\n+\t    if (!BYTES_BIG_ENDIAN)\n+\t      {\n+\t\tsrc = gen_reg_rtx (HImode);\n+\t\temit_insn (gen_bswaphi2 (src, mem));\n+\t      }\n+\t    emit_insn (gen_zero_extendhidi2 (reg, src));\n+\t    break;\n+\t  }\n+\tcase SImode:\n+\t  {\n+\t    rtx src = mem;\n+\t    if (!BYTES_BIG_ENDIAN)\n+\t      {\n+\t\tsrc = gen_reg_rtx (SImode);\n+\t\temit_insn (gen_bswapsi2 (src, mem));\n+\t      }\n+\t    emit_insn (gen_zero_extendsidi2 (reg, src));\n+\t  }\n+\t  break;\n+\tcase DImode:\n+\t  if (!BYTES_BIG_ENDIAN)\n+\t    emit_insn (gen_bswapdi2 (reg, mem));\n+\t  else\n+\t    emit_insn (gen_movdi (reg, mem));\n+\t  break;\n+\tdefault:\n+\t  gcc_unreachable ();\n+\t}\n+      break;\n+\n+    case SImode:\n+      switch (mode)\n+\t{\n+\tcase QImode:\n+\t  emit_insn (gen_zero_extendqisi2 (reg, mem));\n+\t  break;\n+\tcase HImode:\n+\t  {\n+\t    rtx src = mem;\n+\t    if (!BYTES_BIG_ENDIAN)\n+\t      {\n+\t\tsrc = gen_reg_rtx (HImode);\n+\t\temit_insn (gen_bswaphi2 (src, mem));\n+\t      }\n+\t    emit_insn (gen_zero_extendhisi2 (reg, src));\n+\t    break;\n+\t  }\n+\tcase SImode:\n+\t  if (!BYTES_BIG_ENDIAN)\n+\t    emit_insn (gen_bswapsi2 (reg, mem));\n+\t  else\n+\t    emit_insn (gen_movsi (reg, mem));\n+\t  break;\n+\tcase DImode:\n+\t  /* DImode is larger than the destination reg so is not expected.  */\n+\t  gcc_unreachable ();\n+\t  break;\n+\tdefault:\n+\t  gcc_unreachable ();\n+\t}\n+      break;\n+    default:\n+      gcc_unreachable ();\n+      break;\n+    }\n+}\n+\n+/* Select the mode to be used for reading the next chunk of bytes\n+   in the compare.\n+\n+   OFFSET is the current read offset from the beginning of the block.\n+   BYTES is the number of bytes remaining to be read.\n+   ALIGN is the minimum alignment of the memory blocks being compared in bytes.\n+   WORD_MODE_OK indicates using WORD_MODE is allowed, else SImode is\n+   the largest allowable mode.  */\n+static machine_mode\n+select_block_compare_mode (unsigned HOST_WIDE_INT offset,\n+\t\t\t   unsigned HOST_WIDE_INT bytes,\n+\t\t\t   unsigned HOST_WIDE_INT align, bool word_mode_ok)\n+{\n+  /* First see if we can do a whole load unit\n+     as that will be more efficient than a larger load + shift.  */\n+\n+  /* If big, use biggest chunk.\n+     If exactly chunk size, use that size.\n+     If remainder can be done in one piece with shifting, do that.\n+     Do largest chunk possible without violating alignment rules.  */\n+\n+  /* The most we can read without potential page crossing.  */\n+  unsigned HOST_WIDE_INT maxread = ROUND_UP (bytes, align);\n+\n+  if (word_mode_ok && bytes >= UNITS_PER_WORD)\n+    return word_mode;\n+  else if (bytes == GET_MODE_SIZE (SImode))\n+    return SImode;\n+  else if (bytes == GET_MODE_SIZE (HImode))\n+    return HImode;\n+  else if (bytes == GET_MODE_SIZE (QImode))\n+    return QImode;\n+  else if (bytes < GET_MODE_SIZE (SImode)\n+\t   && offset >= GET_MODE_SIZE (SImode) - bytes)\n+    /* This matches the case were we have SImode and 3 bytes\n+       and offset >= 1 and permits us to move back one and overlap\n+       with the previous read, thus avoiding having to shift\n+       unwanted bytes off of the input.  */\n+    return SImode;\n+  else if (word_mode_ok && bytes < UNITS_PER_WORD\n+\t   && offset >= UNITS_PER_WORD-bytes)\n+    /* Similarly, if we can use DImode it will get matched here and\n+       can do an overlapping read that ends at the end of the block.  */\n+    return word_mode;\n+  else if (word_mode_ok && maxread >= UNITS_PER_WORD)\n+    /* It is safe to do all remaining in one load of largest size,\n+       possibly with a shift to get rid of unwanted bytes.  */\n+    return word_mode;\n+  else if (maxread >= GET_MODE_SIZE (SImode))\n+    /* It is safe to do all remaining in one SImode load,\n+       possibly with a shift to get rid of unwanted bytes.  */\n+    return SImode;\n+  else if (bytes > GET_MODE_SIZE (SImode))\n+    return SImode;\n+  else if (bytes > GET_MODE_SIZE (HImode))\n+    return HImode;\n+\n+  /* final fallback is do one byte */\n+  return QImode;\n+}\n+\n+/* Compute the alignment of pointer+OFFSET where the original alignment\n+   of pointer was BASE_ALIGN.  */\n+static unsigned HOST_WIDE_INT\n+compute_current_alignment (unsigned HOST_WIDE_INT base_align,\n+\t\t\t   unsigned HOST_WIDE_INT offset)\n+{\n+  if (offset == 0)\n+    return base_align;\n+  return MIN (base_align, offset & -offset);\n+}\n+\n+/* Expand a block compare operation, and return true if successful.\n+   Return false if we should let the compiler generate normal code,\n+   probably a memcmp call.\n+\n+   OPERANDS[0] is the target (result).\n+   OPERANDS[1] is the first source.\n+   OPERANDS[2] is the second source.\n+   OPERANDS[3] is the length.\n+   OPERANDS[4] is the alignment.  */\n+bool\n+expand_block_compare (rtx operands[])\n+{\n+  rtx target = operands[0];\n+  rtx orig_src1 = operands[1];\n+  rtx orig_src2 = operands[2];\n+  rtx bytes_rtx = operands[3];\n+  rtx align_rtx = operands[4];\n+  HOST_WIDE_INT cmp_bytes = 0;\n+  rtx src1 = orig_src1;\n+  rtx src2 = orig_src2;\n+\n+  /* This case is complicated to handle because the subtract\n+     with carry instructions do not generate the 64-bit\n+     carry and so we must emit code to calculate it ourselves.\n+     We choose not to implement this yet.  */\n+  if (TARGET_32BIT && TARGET_POWERPC64)\n+    return false;\n+\n+  /* If this is not a fixed size compare, just call memcmp.  */\n+  if (!CONST_INT_P (bytes_rtx))\n+    return false;\n+\n+  /* This must be a fixed size alignment.  */\n+  if (!CONST_INT_P (align_rtx))\n+    return false;\n+\n+  unsigned int base_align = UINTVAL (align_rtx) / BITS_PER_UNIT;\n+\n+  /* SLOW_UNALIGNED_ACCESS -- don't do unaligned stuff.  */\n+  if (SLOW_UNALIGNED_ACCESS (word_mode, MEM_ALIGN (orig_src1))\n+      || SLOW_UNALIGNED_ACCESS (word_mode, MEM_ALIGN (orig_src2)))\n+    return false;\n+\n+  gcc_assert (GET_MODE (target) == SImode);\n+\n+  /* Anything to move?  */\n+  unsigned HOST_WIDE_INT bytes = UINTVAL (bytes_rtx);\n+  if (bytes == 0)\n+    return true;\n+\n+  /* The code generated for p7 and older is not faster than glibc\n+     memcmp if alignment is small and length is not short, so bail\n+     out to avoid those conditions.  */\n+  if (!TARGET_EFFICIENT_OVERLAPPING_UNALIGNED\n+      && ((base_align == 1 && bytes > 16)\n+\t  || (base_align == 2 && bytes > 32)))\n+    return false;\n+\n+  rtx tmp_reg_src1 = gen_reg_rtx (word_mode);\n+  rtx tmp_reg_src2 = gen_reg_rtx (word_mode);\n+  /* P7/P8 code uses cond for subfc. but P9 uses\n+     it for cmpld which needs CCUNSmode. */\n+  rtx cond;\n+  if (TARGET_P9_MISC)\n+    cond = gen_reg_rtx (CCUNSmode);\n+  else\n+    cond = gen_reg_rtx (CCmode);\n+\n+  /* If we have an LE target without ldbrx and word_mode is DImode,\n+     then we must avoid using word_mode.  */\n+  int word_mode_ok = !(!BYTES_BIG_ENDIAN && !TARGET_LDBRX\n+\t\t       && word_mode == DImode);\n+\n+  /* Strategy phase.  How many ops will this take and should we expand it?  */\n+\n+  unsigned HOST_WIDE_INT offset = 0;\n+  machine_mode load_mode =\n+    select_block_compare_mode (offset, bytes, base_align, word_mode_ok);\n+  unsigned int load_mode_size = GET_MODE_SIZE (load_mode);\n+\n+  /* We don't want to generate too much code.  */\n+  unsigned HOST_WIDE_INT max_bytes =\n+    load_mode_size * (unsigned HOST_WIDE_INT) rs6000_block_compare_inline_limit;\n+  if (!IN_RANGE (bytes, 1, max_bytes))\n+    return false;\n+\n+  bool generate_6432_conversion = false;\n+  rtx convert_label = NULL;\n+  rtx final_label = NULL;\n+\n+  /* Example of generated code for 18 bytes aligned 1 byte.\n+     Compiled with -fno-reorder-blocks for clarity.\n+             ldbrx 10,31,8\n+             ldbrx 9,7,8\n+             subfc. 9,9,10\n+             bne 0,.L6487\n+             addi 9,12,8\n+             addi 5,11,8\n+             ldbrx 10,0,9\n+             ldbrx 9,0,5\n+             subfc. 9,9,10\n+             bne 0,.L6487\n+             addi 9,12,16\n+             lhbrx 10,0,9\n+             addi 9,11,16\n+             lhbrx 9,0,9\n+             subf 9,9,10\n+             b .L6488\n+             .p2align 4,,15\n+     .L6487: #convert_label\n+             popcntd 9,9\n+             subfe 10,10,10\n+             or 9,9,10\n+     .L6488: #final_label\n+             extsw 10,9\n+\n+     We start off with DImode for two blocks that jump to the DI->SI conversion\n+     if the difference is found there, then a final block of HImode that skips\n+     the DI->SI conversion.  */\n+\n+  while (bytes > 0)\n+    {\n+      unsigned int align = compute_current_alignment (base_align, offset);\n+      if (TARGET_EFFICIENT_OVERLAPPING_UNALIGNED)\n+\tload_mode = select_block_compare_mode (offset, bytes, align,\n+\t\t\t\t\t       word_mode_ok);\n+      else\n+\tload_mode = select_block_compare_mode (0, bytes, align, word_mode_ok);\n+      load_mode_size = GET_MODE_SIZE (load_mode);\n+      if (bytes >= load_mode_size)\n+\tcmp_bytes = load_mode_size;\n+      else if (TARGET_EFFICIENT_OVERLAPPING_UNALIGNED)\n+\t{\n+\t  /* Move this load back so it doesn't go past the end.\n+\t     P8/P9 can do this efficiently.  */\n+\t  unsigned int extra_bytes = load_mode_size - bytes;\n+\t  cmp_bytes = bytes;\n+\t  if (extra_bytes < offset)\n+\t    {\n+\t      offset -= extra_bytes;\n+\t      cmp_bytes = load_mode_size;\n+\t      bytes = cmp_bytes;\n+\t    }\n+\t}\n+      else\n+\t/* P7 and earlier can't do the overlapping load trick fast,\n+\t   so this forces a non-overlapping load and a shift to get\n+\t   rid of the extra bytes.  */\n+\tcmp_bytes = bytes;\n+\n+      src1 = adjust_address (orig_src1, load_mode, offset);\n+      src2 = adjust_address (orig_src2, load_mode, offset);\n+\n+      if (!REG_P (XEXP (src1, 0)))\n+\t{\n+\t  rtx src1_reg = copy_addr_to_reg (XEXP (src1, 0));\n+\t  src1 = replace_equiv_address (src1, src1_reg);\n+\t}\n+      set_mem_size (src1, cmp_bytes);\n+\n+      if (!REG_P (XEXP (src2, 0)))\n+\t{\n+\t  rtx src2_reg = copy_addr_to_reg (XEXP (src2, 0));\n+\t  src2 = replace_equiv_address (src2, src2_reg);\n+\t}\n+      set_mem_size (src2, cmp_bytes);\n+\n+      do_load_for_compare (tmp_reg_src1, src1, load_mode);\n+      do_load_for_compare (tmp_reg_src2, src2, load_mode);\n+\n+      if (cmp_bytes < load_mode_size)\n+\t{\n+\t  /* Shift unneeded bytes off.  */\n+\t  rtx sh = GEN_INT (BITS_PER_UNIT * (load_mode_size - cmp_bytes));\n+\t  if (word_mode == DImode)\n+\t    {\n+\t      emit_insn (gen_lshrdi3 (tmp_reg_src1, tmp_reg_src1, sh));\n+\t      emit_insn (gen_lshrdi3 (tmp_reg_src2, tmp_reg_src2, sh));\n+\t    }\n+\t  else\n+\t    {\n+\t      emit_insn (gen_lshrsi3 (tmp_reg_src1, tmp_reg_src1, sh));\n+\t      emit_insn (gen_lshrsi3 (tmp_reg_src2, tmp_reg_src2, sh));\n+\t    }\n+\t}\n+\n+      int remain = bytes - cmp_bytes;\n+      if (GET_MODE_SIZE (GET_MODE (target)) > GET_MODE_SIZE (load_mode))\n+\t{\n+\t  /* Target is larger than load size so we don't need to\n+\t     reduce result size.  */\n+\n+\t  /* We previously did a block that need 64->32 conversion but\n+\t     the current block does not, so a label is needed to jump\n+\t     to the end.  */\n+\t  if (generate_6432_conversion && !final_label)\n+\t    final_label = gen_label_rtx ();\n+\n+\t  if (remain > 0)\n+\t    {\n+\t      /* This is not the last block, branch to the end if the result\n+\t\t of this subtract is not zero.  */\n+\t      if (!final_label)\n+\t\tfinal_label = gen_label_rtx ();\n+\t      rtx fin_ref = gen_rtx_LABEL_REF (VOIDmode, final_label);\n+\t      rtx tmp = gen_rtx_MINUS (word_mode, tmp_reg_src1, tmp_reg_src2);\n+\t      rtx cr = gen_reg_rtx (CCmode);\n+\t      rs6000_emit_dot_insn (tmp_reg_src2, tmp, 2, cr);\n+\t      emit_insn (gen_movsi (target,\n+\t\t\t\t    gen_lowpart (SImode, tmp_reg_src2)));\n+\t      rtx ne_rtx = gen_rtx_NE (VOIDmode, cr, const0_rtx);\n+\t      rtx ifelse = gen_rtx_IF_THEN_ELSE (VOIDmode, ne_rtx,\n+\t\t\t\t\t\t fin_ref, pc_rtx);\n+\t      rtx j = emit_jump_insn (gen_rtx_SET (pc_rtx, ifelse));\n+\t      JUMP_LABEL (j) = final_label;\n+\t      LABEL_NUSES (final_label) += 1;\n+\t    }\n+\t  else\n+\t    {\n+\t      if (word_mode == DImode)\n+\t\t{\n+\t\t  emit_insn (gen_subdi3 (tmp_reg_src2, tmp_reg_src1,\n+\t\t\t\t\t tmp_reg_src2));\n+\t\t  emit_insn (gen_movsi (target,\n+\t\t\t\t\tgen_lowpart (SImode, tmp_reg_src2)));\n+\t\t}\n+\t      else\n+\t\temit_insn (gen_subsi3 (target, tmp_reg_src1, tmp_reg_src2));\n+\n+\t      if (final_label)\n+\t\t{\n+\t\t  rtx fin_ref = gen_rtx_LABEL_REF (VOIDmode, final_label);\n+\t\t  rtx j = emit_jump_insn (gen_rtx_SET (pc_rtx, fin_ref));\n+\t\t  JUMP_LABEL(j) = final_label;\n+\t\t  LABEL_NUSES (final_label) += 1;\n+\t\t  emit_barrier ();\n+\t\t}\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  /* Do we need a 64->32 conversion block? We need the 64->32\n+\t     conversion even if target size == load_mode size because\n+\t     the subtract generates one extra bit.  */\n+\t  generate_6432_conversion = true;\n+\n+\t  if (remain > 0)\n+\t    {\n+\t      if (!convert_label)\n+\t\tconvert_label = gen_label_rtx ();\n+\n+\t      /* Compare to zero and branch to convert_label if not zero.  */\n+\t      rtx cvt_ref = gen_rtx_LABEL_REF (VOIDmode, convert_label);\n+\t      if (TARGET_P9_MISC)\n+\t\t{\n+\t\t/* Generate a compare, and convert with a setb later.  */\n+\t\t  rtx cmp = gen_rtx_COMPARE (CCUNSmode, tmp_reg_src1,\n+\t\t\t\t\t     tmp_reg_src2);\n+\t\t  emit_insn (gen_rtx_SET (cond, cmp));\n+\t\t}\n+\t      else\n+\t\t/* Generate a subfc. and use the longer\n+\t\t   sequence for conversion.  */\n+\t\tif (TARGET_64BIT)\n+\t\t  emit_insn (gen_subfdi3_carry_dot2 (tmp_reg_src2, tmp_reg_src2,\n+\t\t\t\t\t\t     tmp_reg_src1, cond));\n+\t\telse\n+\t\t  emit_insn (gen_subfsi3_carry_dot2 (tmp_reg_src2, tmp_reg_src2,\n+\t\t\t\t\t\t     tmp_reg_src1, cond));\n+\t      rtx ne_rtx = gen_rtx_NE (VOIDmode, cond, const0_rtx);\n+\t      rtx ifelse = gen_rtx_IF_THEN_ELSE (VOIDmode, ne_rtx,\n+\t\t\t\t\t\t cvt_ref, pc_rtx);\n+\t      rtx j = emit_jump_insn (gen_rtx_SET (pc_rtx, ifelse));\n+\t      JUMP_LABEL(j) = convert_label;\n+\t      LABEL_NUSES (convert_label) += 1;\n+\t    }\n+\t  else\n+\t    {\n+\t      /* Just do the subtract/compare.  Since this is the last block\n+\t\t the convert code will be generated immediately following.  */\n+\t      if (TARGET_P9_MISC)\n+\t\t{\n+\t\t  rtx cmp = gen_rtx_COMPARE (CCUNSmode, tmp_reg_src1,\n+\t\t\t\t\t     tmp_reg_src2);\n+\t\t  emit_insn (gen_rtx_SET (cond, cmp));\n+\t\t}\n+\t      else\n+\t\tif (TARGET_64BIT)\n+\t\t  emit_insn (gen_subfdi3_carry (tmp_reg_src2, tmp_reg_src2,\n+\t\t\t\t\t\ttmp_reg_src1));\n+\t\telse\n+\t\t  emit_insn (gen_subfsi3_carry (tmp_reg_src2, tmp_reg_src2,\n+\t\t\t\t\t\ttmp_reg_src1));\n+\t    }\n+\t}\n+\n+      offset += cmp_bytes;\n+      bytes -= cmp_bytes;\n+    }\n+\n+  if (generate_6432_conversion)\n+    {\n+      if (convert_label)\n+\temit_label (convert_label);\n+\n+      /* We need to produce DI result from sub, then convert to target SI\n+\t while maintaining <0 / ==0 / >0 properties. This sequence works:\n+\t subfc L,A,B\n+\t subfe H,H,H\n+\t popcntd L,L\n+\t rldimi L,H,6,0\n+\n+\t This is an alternate one Segher cooked up if somebody\n+\t wants to expand this for something that doesn't have popcntd:\n+\t subfc L,a,b\n+\t subfe H,x,x\n+\t addic t,L,-1\n+\t subfe v,t,L\n+\t or z,v,H\n+\n+\t And finally, p9 can just do this:\n+\t cmpld A,B\n+\t setb r */\n+\n+      if (TARGET_P9_MISC)\n+\t{\n+\t  emit_insn (gen_setb_unsigned (target, cond));\n+\t}\n+      else\n+\t{\n+\t  if (TARGET_64BIT)\n+\t    {\n+\t      rtx tmp_reg_ca = gen_reg_rtx (DImode);\n+\t      emit_insn (gen_subfdi3_carry_in_xx (tmp_reg_ca));\n+\t      emit_insn (gen_popcntddi2 (tmp_reg_src2, tmp_reg_src2));\n+\t      emit_insn (gen_iordi3 (tmp_reg_src2, tmp_reg_src2, tmp_reg_ca));\n+\t      emit_insn (gen_movsi (target, gen_lowpart (SImode, tmp_reg_src2)));\n+\t    }\n+\t  else\n+\t    {\n+\t      rtx tmp_reg_ca = gen_reg_rtx (SImode);\n+\t      emit_insn (gen_subfsi3_carry_in_xx (tmp_reg_ca));\n+\t      emit_insn (gen_popcntdsi2 (tmp_reg_src2, tmp_reg_src2));\n+\t      emit_insn (gen_iorsi3 (target, tmp_reg_src2, tmp_reg_ca));\n+\t    }\n+\t}\n+    }\n+\n+  if (final_label)\n+    emit_label (final_label);\n+\n+  gcc_assert (bytes == 0);\n+  return true;\n+}\n+\n+/* Generate alignment check and branch code to set up for\n+   strncmp when we don't have DI alignment.\n+   STRNCMP_LABEL is the label to branch if there is a page crossing.\n+   SRC is the string pointer to be examined.\n+   BYTES is the max number of bytes to compare.  */\n+static void\n+expand_strncmp_align_check (rtx strncmp_label, rtx src, HOST_WIDE_INT bytes)\n+{\n+  rtx lab_ref = gen_rtx_LABEL_REF (VOIDmode, strncmp_label);\n+  rtx src_check = copy_addr_to_reg (XEXP (src, 0));\n+  if (GET_MODE (src_check) == SImode)\n+    emit_insn (gen_andsi3 (src_check, src_check, GEN_INT (0xfff)));\n+  else\n+    emit_insn (gen_anddi3 (src_check, src_check, GEN_INT (0xfff)));\n+  rtx cond = gen_reg_rtx (CCmode);\n+  emit_move_insn (cond, gen_rtx_COMPARE (CCmode, src_check,\n+\t\t\t\t\t GEN_INT (4096 - bytes)));\n+\n+  rtx cmp_rtx = gen_rtx_LT (VOIDmode, cond, const0_rtx);\n+\n+  rtx ifelse = gen_rtx_IF_THEN_ELSE (VOIDmode, cmp_rtx,\n+\t\t\t\t     pc_rtx, lab_ref);\n+  rtx j = emit_jump_insn (gen_rtx_SET (pc_rtx, ifelse));\n+  JUMP_LABEL (j) = strncmp_label;\n+  LABEL_NUSES (strncmp_label) += 1;\n+}\n+\n+/* Expand a string compare operation with length, and return\n+   true if successful. Return false if we should let the\n+   compiler generate normal code, probably a strncmp call.\n+\n+   OPERANDS[0] is the target (result).\n+   OPERANDS[1] is the first source.\n+   OPERANDS[2] is the second source.\n+   If NO_LENGTH is zero, then:\n+   OPERANDS[3] is the length.\n+   OPERANDS[4] is the alignment in bytes.\n+   If NO_LENGTH is nonzero, then:\n+   OPERANDS[3] is the alignment in bytes.  */\n+bool\n+expand_strn_compare (rtx operands[], int no_length)\n+{\n+  rtx target = operands[0];\n+  rtx orig_src1 = operands[1];\n+  rtx orig_src2 = operands[2];\n+  rtx bytes_rtx, align_rtx;\n+  if (no_length)\n+    {\n+      bytes_rtx = NULL;\n+      align_rtx = operands[3];\n+    }\n+  else\n+    {\n+      bytes_rtx = operands[3];\n+      align_rtx = operands[4];\n+    }\n+  unsigned HOST_WIDE_INT cmp_bytes = 0;\n+  rtx src1 = orig_src1;\n+  rtx src2 = orig_src2;\n+\n+  /* If we have a length, it must be constant. This simplifies things\n+     a bit as we don't have to generate code to check if we've exceeded\n+     the length. Later this could be expanded to handle this case.  */\n+  if (!no_length && !CONST_INT_P (bytes_rtx))\n+    return false;\n+\n+  /* This must be a fixed size alignment.  */\n+  if (!CONST_INT_P (align_rtx))\n+    return false;\n+\n+  unsigned int base_align = UINTVAL (align_rtx);\n+  int align1 = MEM_ALIGN (orig_src1) / BITS_PER_UNIT;\n+  int align2 = MEM_ALIGN (orig_src2) / BITS_PER_UNIT;\n+\n+  /* SLOW_UNALIGNED_ACCESS -- don't do unaligned stuff.  */\n+  if (SLOW_UNALIGNED_ACCESS (word_mode, align1)\n+      || SLOW_UNALIGNED_ACCESS (word_mode, align2))\n+    return false;\n+\n+  gcc_assert (GET_MODE (target) == SImode);\n+\n+  /* If we have an LE target without ldbrx and word_mode is DImode,\n+     then we must avoid using word_mode.  */\n+  int word_mode_ok = !(!BYTES_BIG_ENDIAN && !TARGET_LDBRX\n+\t\t       && word_mode == DImode);\n+\n+  unsigned int word_mode_size = GET_MODE_SIZE (word_mode);\n+\n+  unsigned HOST_WIDE_INT offset = 0;\n+  unsigned HOST_WIDE_INT bytes; /* N from the strncmp args if available.  */\n+  unsigned HOST_WIDE_INT compare_length; /* How much to compare inline.  */\n+  if (no_length)\n+    /* Use this as a standin to determine the mode to use.  */\n+    bytes = rs6000_string_compare_inline_limit * word_mode_size;\n+  else\n+    bytes = UINTVAL (bytes_rtx);\n+\n+  machine_mode load_mode =\n+    select_block_compare_mode (offset, bytes, base_align, word_mode_ok);\n+  unsigned int load_mode_size = GET_MODE_SIZE (load_mode);\n+  compare_length = rs6000_string_compare_inline_limit * load_mode_size;\n+\n+  /* If we have equality at the end of the last compare and we have not\n+     found the end of the string, we need to call strcmp/strncmp to\n+     compare the remainder.  */\n+  bool equality_compare_rest = false;\n+\n+  if (no_length)\n+    {\n+      bytes = compare_length;\n+      equality_compare_rest = true;\n+    }\n+  else\n+    {\n+      if (bytes <= compare_length)\n+\tcompare_length = bytes;\n+      else\n+\tequality_compare_rest = true;\n+    }\n+\n+  rtx result_reg = gen_reg_rtx (word_mode);\n+  rtx final_move_label = gen_label_rtx ();\n+  rtx final_label = gen_label_rtx ();\n+  rtx begin_compare_label = NULL;\n+\n+  if (base_align < 8)\n+    {\n+      /* Generate code that checks distance to 4k boundary for this case.  */\n+      begin_compare_label = gen_label_rtx ();\n+      rtx strncmp_label = gen_label_rtx ();\n+      rtx jmp;\n+\n+      /* Strncmp for power8 in glibc does this:\n+\t rldicl\tr8,r3,0,52\n+\t cmpldi\tcr7,r8,4096-16\n+\t bgt\tcr7,L(pagecross) */\n+\n+      /* Make sure that the length we use for the alignment test and\n+         the subsequent code generation are in agreement so we do not\n+         go past the length we tested for a 4k boundary crossing.  */\n+      unsigned HOST_WIDE_INT align_test = compare_length;\n+      if (align_test < 8)\n+        {\n+          align_test = HOST_WIDE_INT_1U << ceil_log2 (align_test);\n+          base_align = align_test;\n+        }\n+      else\n+        {\n+          align_test = ROUND_UP (align_test, 8);\n+          base_align = 8;\n+        }\n+\n+      if (align1 < 8)\n+        expand_strncmp_align_check (strncmp_label, src1, align_test);\n+      if (align2 < 8)\n+        expand_strncmp_align_check (strncmp_label, src2, align_test);\n+\n+      /* Now generate the following sequence:\n+\t - branch to begin_compare\n+\t - strncmp_label\n+\t - call to strncmp\n+\t - branch to final_label\n+\t - begin_compare_label */\n+\n+      rtx cmp_ref = gen_rtx_LABEL_REF (VOIDmode, begin_compare_label);\n+      jmp = emit_jump_insn (gen_rtx_SET (pc_rtx, cmp_ref));\n+      JUMP_LABEL (jmp) = begin_compare_label;\n+      LABEL_NUSES (begin_compare_label) += 1;\n+      emit_barrier ();\n+\n+      emit_label (strncmp_label);\n+\n+      if (!REG_P (XEXP (src1, 0)))\n+\t{\n+\t  rtx src1_reg = copy_addr_to_reg (XEXP (src1, 0));\n+\t  src1 = replace_equiv_address (src1, src1_reg);\n+\t}\n+\n+      if (!REG_P (XEXP (src2, 0)))\n+\t{\n+\t  rtx src2_reg = copy_addr_to_reg (XEXP (src2, 0));\n+\t  src2 = replace_equiv_address (src2, src2_reg);\n+\t}\n+\n+      if (no_length)\n+\t{\n+\t  tree fun = builtin_decl_explicit (BUILT_IN_STRCMP);\n+\t  emit_library_call_value (XEXP (DECL_RTL (fun), 0),\n+\t\t\t\t   target, LCT_NORMAL, GET_MODE (target), 2,\n+\t\t\t\t   force_reg (Pmode, XEXP (src1, 0)), Pmode,\n+\t\t\t\t   force_reg (Pmode, XEXP (src2, 0)), Pmode);\n+\t}\n+      else\n+\t{\n+\t  /* -m32 -mpowerpc64 results in word_mode being DImode even\n+\t     though otherwise it is 32-bit. The length arg to strncmp\n+\t     is a size_t which will be the same size as pointers.  */\n+\t  rtx len_rtx;\n+\t  if (TARGET_64BIT)\n+\t    len_rtx = gen_reg_rtx (DImode);\n+\t  else\n+\t    len_rtx = gen_reg_rtx (SImode);\n+\n+\t  emit_move_insn (len_rtx, bytes_rtx);\n+\n+\t  tree fun = builtin_decl_explicit (BUILT_IN_STRNCMP);\n+\t  emit_library_call_value (XEXP (DECL_RTL (fun), 0),\n+\t\t\t\t   target, LCT_NORMAL, GET_MODE (target), 3,\n+\t\t\t\t   force_reg (Pmode, XEXP (src1, 0)), Pmode,\n+\t\t\t\t   force_reg (Pmode, XEXP (src2, 0)), Pmode,\n+\t\t\t\t   len_rtx, GET_MODE (len_rtx));\n+\t}\n+\n+      rtx fin_ref = gen_rtx_LABEL_REF (VOIDmode, final_label);\n+      jmp = emit_jump_insn (gen_rtx_SET (pc_rtx, fin_ref));\n+      JUMP_LABEL (jmp) = final_label;\n+      LABEL_NUSES (final_label) += 1;\n+      emit_barrier ();\n+      emit_label (begin_compare_label);\n+    }\n+\n+  rtx cleanup_label = NULL;\n+  rtx tmp_reg_src1 = gen_reg_rtx (word_mode);\n+  rtx tmp_reg_src2 = gen_reg_rtx (word_mode);\n+\n+  /* Generate sequence of ld/ldbrx, cmpb to compare out\n+     to the length specified.  */\n+  unsigned HOST_WIDE_INT bytes_to_compare = compare_length;\n+  while (bytes_to_compare > 0)\n+    {\n+      /* Compare sequence:\n+         check each 8B with: ld/ld cmpd bne\n+\t If equal, use rldicr/cmpb to check for zero byte.\n+         cleanup code at end:\n+         cmpb          get byte that differs\n+         cmpb          look for zero byte\n+         orc           combine\n+         cntlzd        get bit of first zero/diff byte\n+         subfic        convert for rldcl use\n+         rldcl rldcl   extract diff/zero byte\n+         subf          subtract for final result\n+\n+         The last compare can branch around the cleanup code if the\n+         result is zero because the strings are exactly equal.  */\n+      unsigned int align = compute_current_alignment (base_align, offset);\n+      if (TARGET_EFFICIENT_OVERLAPPING_UNALIGNED)\n+\tload_mode = select_block_compare_mode (offset, bytes_to_compare, align,\n+\t\t\t\t\t       word_mode_ok);\n+      else\n+\tload_mode = select_block_compare_mode (0, bytes_to_compare, align,\n+\t\t\t\t\t       word_mode_ok);\n+      load_mode_size = GET_MODE_SIZE (load_mode);\n+      if (bytes_to_compare >= load_mode_size)\n+\tcmp_bytes = load_mode_size;\n+      else if (TARGET_EFFICIENT_OVERLAPPING_UNALIGNED)\n+\t{\n+\t  /* Move this load back so it doesn't go past the end.\n+\t     P8/P9 can do this efficiently.  */\n+\t  unsigned int extra_bytes = load_mode_size - bytes_to_compare;\n+\t  cmp_bytes = bytes_to_compare;\n+\t  if (extra_bytes < offset)\n+\t    {\n+\t      offset -= extra_bytes;\n+\t      cmp_bytes = load_mode_size;\n+\t      bytes_to_compare = cmp_bytes;\n+\t    }\n+\t}\n+      else\n+\t/* P7 and earlier can't do the overlapping load trick fast,\n+\t   so this forces a non-overlapping load and a shift to get\n+\t   rid of the extra bytes.  */\n+\tcmp_bytes = bytes_to_compare;\n+\n+      src1 = adjust_address (orig_src1, load_mode, offset);\n+      src2 = adjust_address (orig_src2, load_mode, offset);\n+\n+      if (!REG_P (XEXP (src1, 0)))\n+\t{\n+\t  rtx src1_reg = copy_addr_to_reg (XEXP (src1, 0));\n+\t  src1 = replace_equiv_address (src1, src1_reg);\n+\t}\n+      set_mem_size (src1, cmp_bytes);\n+\n+      if (!REG_P (XEXP (src2, 0)))\n+\t{\n+\t  rtx src2_reg = copy_addr_to_reg (XEXP (src2, 0));\n+\t  src2 = replace_equiv_address (src2, src2_reg);\n+\t}\n+      set_mem_size (src2, cmp_bytes);\n+\n+      do_load_for_compare (tmp_reg_src1, src1, load_mode);\n+      do_load_for_compare (tmp_reg_src2, src2, load_mode);\n+\n+      /* We must always left-align the data we read, and\n+\t clear any bytes to the right that are beyond the string.\n+\t Otherwise the cmpb sequence won't produce the correct\n+\t results.  The beginning of the compare will be done\n+\t with word_mode so will not have any extra shifts or\n+\t clear rights.  */\n+\n+      if (load_mode_size < word_mode_size)\n+\t{\n+\t  /* Rotate left first. */\n+\t  rtx sh = GEN_INT (BITS_PER_UNIT * (word_mode_size - load_mode_size));\n+\t  if (word_mode == DImode)\n+\t    {\n+\t      emit_insn (gen_rotldi3 (tmp_reg_src1, tmp_reg_src1, sh));\n+\t      emit_insn (gen_rotldi3 (tmp_reg_src2, tmp_reg_src2, sh));\n+\t    }\n+\t  else\n+\t    {\n+\t      emit_insn (gen_rotlsi3 (tmp_reg_src1, tmp_reg_src1, sh));\n+\t      emit_insn (gen_rotlsi3 (tmp_reg_src2, tmp_reg_src2, sh));\n+\t    }\n+\t}\n+\n+      if (cmp_bytes < word_mode_size)\n+\t{\n+\t  /* Now clear right.  This plus the rotate can be\n+\t     turned into a rldicr instruction. */\n+\t  HOST_WIDE_INT mb = BITS_PER_UNIT * (word_mode_size - cmp_bytes);\n+\t  rtx mask = GEN_INT (HOST_WIDE_INT_M1U << mb);\n+\t  if (word_mode == DImode)\n+\t    {\n+\t      emit_insn (gen_anddi3_mask (tmp_reg_src1, tmp_reg_src1, mask));\n+\t      emit_insn (gen_anddi3_mask (tmp_reg_src2, tmp_reg_src2, mask));\n+\t    }\n+\t  else\n+\t    {\n+\t      emit_insn (gen_andsi3_mask (tmp_reg_src1, tmp_reg_src1, mask));\n+\t      emit_insn (gen_andsi3_mask (tmp_reg_src2, tmp_reg_src2, mask));\n+\t    }\n+\t}\n+\n+      /* Cases to handle.  A and B are chunks of the two strings.\n+\t 1: Not end of comparison:\n+\t A != B: branch to cleanup code to compute result.\n+\t A == B: check for 0 byte, next block if not found.\n+\t 2: End of the inline comparison:\n+\t A != B: branch to cleanup code to compute result.\n+\t A == B: check for 0 byte, call strcmp/strncmp\n+\t 3: compared requested N bytes:\n+\t A == B: branch to result 0.\n+\t A != B: cleanup code to compute result.  */\n+\n+      unsigned HOST_WIDE_INT remain = bytes_to_compare - cmp_bytes;\n+\n+      rtx dst_label;\n+      if (remain > 0 || equality_compare_rest)\n+\t{\n+\t  /* Branch to cleanup code, otherwise fall through to do\n+\t     more compares.  */\n+\t  if (!cleanup_label)\n+\t    cleanup_label = gen_label_rtx ();\n+\t  dst_label = cleanup_label;\n+\t}\n+      else\n+\t/* Branch to end and produce result of 0.  */\n+\tdst_label = final_move_label;\n+\n+      rtx lab_ref = gen_rtx_LABEL_REF (VOIDmode, dst_label);\n+      rtx cond = gen_reg_rtx (CCmode);\n+\n+      /* Always produce the 0 result, it is needed if\n+\t cmpb finds a 0 byte in this chunk.  */\n+      rtx tmp = gen_rtx_MINUS (word_mode, tmp_reg_src1, tmp_reg_src2);\n+      rs6000_emit_dot_insn (result_reg, tmp, 1, cond);\n+\n+      rtx cmp_rtx;\n+      if (remain == 0 && !equality_compare_rest)\n+\tcmp_rtx = gen_rtx_EQ (VOIDmode, cond, const0_rtx);\n+      else\n+\tcmp_rtx = gen_rtx_NE (VOIDmode, cond, const0_rtx);\n+\n+      rtx ifelse = gen_rtx_IF_THEN_ELSE (VOIDmode, cmp_rtx,\n+\t\t\t\t\t lab_ref, pc_rtx);\n+      rtx j = emit_jump_insn (gen_rtx_SET (pc_rtx, ifelse));\n+      JUMP_LABEL (j) = dst_label;\n+      LABEL_NUSES (dst_label) += 1;\n+\n+      if (remain > 0 || equality_compare_rest)\n+\t{\n+\t  /* Generate a cmpb to test for a 0 byte and branch\n+\t     to final result if found.  */\n+\t  rtx cmpb_zero = gen_reg_rtx (word_mode);\n+\t  rtx lab_ref_fin = gen_rtx_LABEL_REF (VOIDmode, final_move_label);\n+\t  rtx condz = gen_reg_rtx (CCmode);\n+\t  rtx zero_reg = gen_reg_rtx (word_mode);\n+\t  if (word_mode == SImode)\n+\t    {\n+\t      emit_insn (gen_movsi (zero_reg, GEN_INT (0)));\n+\t      emit_insn (gen_cmpbsi3 (cmpb_zero, tmp_reg_src1, zero_reg));\n+\t      if (cmp_bytes < word_mode_size)\n+\t\t{\n+\t\t  /* Don't want to look at zero bytes past end.  */\n+\t\t  HOST_WIDE_INT mb =\n+\t\t    BITS_PER_UNIT * (word_mode_size - cmp_bytes);\n+\t\t  rtx mask = GEN_INT (HOST_WIDE_INT_M1U << mb);\n+\t\t  emit_insn (gen_andsi3_mask (cmpb_zero, cmpb_zero, mask));\n+\t\t}\n+\t    }\n+\t  else\n+\t    {\n+\t      emit_insn (gen_movdi (zero_reg, GEN_INT (0)));\n+\t      emit_insn (gen_cmpbdi3 (cmpb_zero, tmp_reg_src1, zero_reg));\n+\t      if (cmp_bytes < word_mode_size)\n+\t\t{\n+\t\t  /* Don't want to look at zero bytes past end.  */\n+\t\t  HOST_WIDE_INT mb =\n+\t\t    BITS_PER_UNIT * (word_mode_size - cmp_bytes);\n+\t\t  rtx mask = GEN_INT (HOST_WIDE_INT_M1U << mb);\n+\t\t  emit_insn (gen_anddi3_mask (cmpb_zero, cmpb_zero, mask));\n+\t\t}\n+\t    }\n+\n+\t  emit_move_insn (condz, gen_rtx_COMPARE (CCmode, cmpb_zero, zero_reg));\n+\t  rtx cmpnz_rtx = gen_rtx_NE (VOIDmode, condz, const0_rtx);\n+\t  rtx ifelse = gen_rtx_IF_THEN_ELSE (VOIDmode, cmpnz_rtx,\n+\t\t\t\t\t     lab_ref_fin, pc_rtx);\n+\t  rtx j2 = emit_jump_insn (gen_rtx_SET (pc_rtx, ifelse));\n+\t  JUMP_LABEL (j2) = final_move_label;\n+\t  LABEL_NUSES (final_move_label) += 1;\n+\n+\t}\n+\n+      offset += cmp_bytes;\n+      bytes_to_compare -= cmp_bytes;\n+    }\n+\n+  if (equality_compare_rest)\n+    {\n+      /* Update pointers past what has been compared already.  */\n+      src1 = adjust_address (orig_src1, load_mode, offset);\n+      src2 = adjust_address (orig_src2, load_mode, offset);\n+\n+      if (!REG_P (XEXP (src1, 0)))\n+\t{\n+\t  rtx src1_reg = copy_addr_to_reg (XEXP (src1, 0));\n+\t  src1 = replace_equiv_address (src1, src1_reg);\n+\t}\n+      set_mem_size (src1, cmp_bytes);\n+\n+      if (!REG_P (XEXP (src2, 0)))\n+\t{\n+\t  rtx src2_reg = copy_addr_to_reg (XEXP (src2, 0));\n+\t  src2 = replace_equiv_address (src2, src2_reg);\n+\t}\n+      set_mem_size (src2, cmp_bytes);\n+\n+      /* Construct call to strcmp/strncmp to compare the rest of the string.  */\n+      if (no_length)\n+\t{\n+\t  tree fun = builtin_decl_explicit (BUILT_IN_STRCMP);\n+\t  emit_library_call_value (XEXP (DECL_RTL (fun), 0),\n+\t\t\t\t   target, LCT_NORMAL, GET_MODE (target), 2,\n+\t\t\t\t   force_reg (Pmode, XEXP (src1, 0)), Pmode,\n+\t\t\t\t   force_reg (Pmode, XEXP (src2, 0)), Pmode);\n+\t}\n+      else\n+\t{\n+\t  rtx len_rtx;\n+\t  if (TARGET_64BIT)\n+\t    len_rtx = gen_reg_rtx (DImode);\n+\t  else\n+\t    len_rtx = gen_reg_rtx (SImode);\n+\n+\t  emit_move_insn (len_rtx, GEN_INT (bytes - compare_length));\n+\t  tree fun = builtin_decl_explicit (BUILT_IN_STRNCMP);\n+\t  emit_library_call_value (XEXP (DECL_RTL (fun), 0),\n+\t\t\t\t   target, LCT_NORMAL, GET_MODE (target), 3,\n+\t\t\t\t   force_reg (Pmode, XEXP (src1, 0)), Pmode,\n+\t\t\t\t   force_reg (Pmode, XEXP (src2, 0)), Pmode,\n+\t\t\t\t   len_rtx, GET_MODE (len_rtx));\n+\t}\n+\n+      rtx fin_ref = gen_rtx_LABEL_REF (VOIDmode, final_label);\n+      rtx jmp = emit_jump_insn (gen_rtx_SET (pc_rtx, fin_ref));\n+      JUMP_LABEL (jmp) = final_label;\n+      LABEL_NUSES (final_label) += 1;\n+      emit_barrier ();\n+    }\n+\n+  if (cleanup_label)\n+    emit_label (cleanup_label);\n+\n+  /* Generate the final sequence that identifies the differing\n+     byte and generates the final result, taking into account\n+     zero bytes:\n+\n+     cmpb              cmpb_result1, src1, src2\n+     cmpb              cmpb_result2, src1, zero\n+     orc               cmpb_result1, cmp_result1, cmpb_result2\n+     cntlzd            get bit of first zero/diff byte\n+     addi              convert for rldcl use\n+     rldcl rldcl       extract diff/zero byte\n+     subf              subtract for final result\n+  */\n+\n+  rtx cmpb_diff = gen_reg_rtx (word_mode);\n+  rtx cmpb_zero = gen_reg_rtx (word_mode);\n+  rtx rot_amt = gen_reg_rtx (word_mode);\n+  rtx zero_reg = gen_reg_rtx (word_mode);\n+\n+  rtx rot1_1 = gen_reg_rtx (word_mode);\n+  rtx rot1_2 = gen_reg_rtx (word_mode);\n+  rtx rot2_1 = gen_reg_rtx (word_mode);\n+  rtx rot2_2 = gen_reg_rtx (word_mode);\n+\n+  if (word_mode == SImode)\n+    {\n+      emit_insn (gen_cmpbsi3 (cmpb_diff, tmp_reg_src1, tmp_reg_src2));\n+      emit_insn (gen_movsi (zero_reg, GEN_INT (0)));\n+      emit_insn (gen_cmpbsi3 (cmpb_zero, tmp_reg_src1, zero_reg));\n+      emit_insn (gen_one_cmplsi2 (cmpb_diff,cmpb_diff));\n+      emit_insn (gen_iorsi3 (cmpb_diff, cmpb_diff, cmpb_zero));\n+      emit_insn (gen_clzsi2 (rot_amt, cmpb_diff));\n+      emit_insn (gen_addsi3 (rot_amt, rot_amt, GEN_INT (8)));\n+      emit_insn (gen_rotlsi3 (rot1_1, tmp_reg_src1,\n+\t\t\t      gen_lowpart (SImode, rot_amt)));\n+      emit_insn (gen_andsi3_mask (rot1_2, rot1_1, GEN_INT (0xff)));\n+      emit_insn (gen_rotlsi3 (rot2_1, tmp_reg_src2,\n+\t\t\t      gen_lowpart (SImode, rot_amt)));\n+      emit_insn (gen_andsi3_mask (rot2_2, rot2_1, GEN_INT (0xff)));\n+      emit_insn (gen_subsi3 (result_reg, rot1_2, rot2_2));\n+    }\n+  else\n+    {\n+      emit_insn (gen_cmpbdi3 (cmpb_diff, tmp_reg_src1, tmp_reg_src2));\n+      emit_insn (gen_movdi (zero_reg, GEN_INT (0)));\n+      emit_insn (gen_cmpbdi3 (cmpb_zero, tmp_reg_src1, zero_reg));\n+      emit_insn (gen_one_cmpldi2 (cmpb_diff,cmpb_diff));\n+      emit_insn (gen_iordi3 (cmpb_diff, cmpb_diff, cmpb_zero));\n+      emit_insn (gen_clzdi2 (rot_amt, cmpb_diff));\n+      emit_insn (gen_adddi3 (rot_amt, rot_amt, GEN_INT (8)));\n+      emit_insn (gen_rotldi3 (rot1_1, tmp_reg_src1,\n+\t\t\t      gen_lowpart (SImode, rot_amt)));\n+      emit_insn (gen_anddi3_mask (rot1_2, rot1_1, GEN_INT (0xff)));\n+      emit_insn (gen_rotldi3 (rot2_1, tmp_reg_src2,\n+\t\t\t      gen_lowpart (SImode, rot_amt)));\n+      emit_insn (gen_anddi3_mask (rot2_2, rot2_1, GEN_INT (0xff)));\n+      emit_insn (gen_subdi3 (result_reg, rot1_2, rot2_2));\n+    }\n+\n+  emit_label (final_move_label);\n+  emit_insn (gen_movsi (target,\n+\t\t\tgen_lowpart (SImode, result_reg)));\n+  emit_label (final_label);\n+  return true;\n+}\n+\n+/* Expand a block move operation, and return 1 if successful.  Return 0\n+   if we should let the compiler generate normal code.\n+\n+   operands[0] is the destination\n+   operands[1] is the source\n+   operands[2] is the length\n+   operands[3] is the alignment */\n+\n+#define MAX_MOVE_REG 4\n+\n+int\n+expand_block_move (rtx operands[])\n+{\n+  rtx orig_dest = operands[0];\n+  rtx orig_src\t= operands[1];\n+  rtx bytes_rtx\t= operands[2];\n+  rtx align_rtx = operands[3];\n+  int constp\t= (GET_CODE (bytes_rtx) == CONST_INT);\n+  int align;\n+  int bytes;\n+  int offset;\n+  int move_bytes;\n+  rtx stores[MAX_MOVE_REG];\n+  int num_reg = 0;\n+\n+  /* If this is not a fixed size move, just call memcpy */\n+  if (! constp)\n+    return 0;\n+\n+  /* This must be a fixed size alignment */\n+  gcc_assert (GET_CODE (align_rtx) == CONST_INT);\n+  align = INTVAL (align_rtx) * BITS_PER_UNIT;\n+\n+  /* Anything to move? */\n+  bytes = INTVAL (bytes_rtx);\n+  if (bytes <= 0)\n+    return 1;\n+\n+  if (bytes > rs6000_block_move_inline_limit)\n+    return 0;\n+\n+  for (offset = 0; bytes > 0; offset += move_bytes, bytes -= move_bytes)\n+    {\n+      union {\n+\trtx (*movmemsi) (rtx, rtx, rtx, rtx);\n+\trtx (*mov) (rtx, rtx);\n+      } gen_func;\n+      machine_mode mode = BLKmode;\n+      rtx src, dest;\n+\n+      /* Altivec first, since it will be faster than a string move\n+\t when it applies, and usually not significantly larger.  */\n+      if (TARGET_ALTIVEC && bytes >= 16 && align >= 128)\n+\t{\n+\t  move_bytes = 16;\n+\t  mode = V4SImode;\n+\t  gen_func.mov = gen_movv4si;\n+\t}\n+      else if (TARGET_STRING\n+\t  && bytes > 24\t\t/* move up to 32 bytes at a time */\n+\t  && ! fixed_regs[5]\n+\t  && ! fixed_regs[6]\n+\t  && ! fixed_regs[7]\n+\t  && ! fixed_regs[8]\n+\t  && ! fixed_regs[9]\n+\t  && ! fixed_regs[10]\n+\t  && ! fixed_regs[11]\n+\t  && ! fixed_regs[12])\n+\t{\n+\t  move_bytes = (bytes > 32) ? 32 : bytes;\n+\t  gen_func.movmemsi = gen_movmemsi_8reg;\n+\t}\n+      else if (TARGET_STRING\n+\t       && bytes > 16\t/* move up to 24 bytes at a time */\n+\t       && ! fixed_regs[5]\n+\t       && ! fixed_regs[6]\n+\t       && ! fixed_regs[7]\n+\t       && ! fixed_regs[8]\n+\t       && ! fixed_regs[9]\n+\t       && ! fixed_regs[10])\n+\t{\n+\t  move_bytes = (bytes > 24) ? 24 : bytes;\n+\t  gen_func.movmemsi = gen_movmemsi_6reg;\n+\t}\n+      else if (TARGET_STRING\n+\t       && bytes > 8\t/* move up to 16 bytes at a time */\n+\t       && ! fixed_regs[5]\n+\t       && ! fixed_regs[6]\n+\t       && ! fixed_regs[7]\n+\t       && ! fixed_regs[8])\n+\t{\n+\t  move_bytes = (bytes > 16) ? 16 : bytes;\n+\t  gen_func.movmemsi = gen_movmemsi_4reg;\n+\t}\n+      else if (bytes >= 8 && TARGET_POWERPC64\n+\t       && (align >= 64 || !STRICT_ALIGNMENT))\n+\t{\n+\t  move_bytes = 8;\n+\t  mode = DImode;\n+\t  gen_func.mov = gen_movdi;\n+\t  if (offset == 0 && align < 64)\n+\t    {\n+\t      rtx addr;\n+\n+\t      /* If the address form is reg+offset with offset not a\n+\t\t multiple of four, reload into reg indirect form here\n+\t\t rather than waiting for reload.  This way we get one\n+\t\t reload, not one per load and/or store.  */\n+\t      addr = XEXP (orig_dest, 0);\n+\t      if ((GET_CODE (addr) == PLUS || GET_CODE (addr) == LO_SUM)\n+\t\t  && GET_CODE (XEXP (addr, 1)) == CONST_INT\n+\t\t  && (INTVAL (XEXP (addr, 1)) & 3) != 0)\n+\t\t{\n+\t\t  addr = copy_addr_to_reg (addr);\n+\t\t  orig_dest = replace_equiv_address (orig_dest, addr);\n+\t\t}\n+\t      addr = XEXP (orig_src, 0);\n+\t      if ((GET_CODE (addr) == PLUS || GET_CODE (addr) == LO_SUM)\n+\t\t  && GET_CODE (XEXP (addr, 1)) == CONST_INT\n+\t\t  && (INTVAL (XEXP (addr, 1)) & 3) != 0)\n+\t\t{\n+\t\t  addr = copy_addr_to_reg (addr);\n+\t\t  orig_src = replace_equiv_address (orig_src, addr);\n+\t\t}\n+\t    }\n+\t}\n+      else if (TARGET_STRING && bytes > 4 && !TARGET_POWERPC64)\n+\t{\t\t\t/* move up to 8 bytes at a time */\n+\t  move_bytes = (bytes > 8) ? 8 : bytes;\n+\t  gen_func.movmemsi = gen_movmemsi_2reg;\n+\t}\n+      else if (bytes >= 4 && (align >= 32 || !STRICT_ALIGNMENT))\n+\t{\t\t\t/* move 4 bytes */\n+\t  move_bytes = 4;\n+\t  mode = SImode;\n+\t  gen_func.mov = gen_movsi;\n+\t}\n+      else if (bytes >= 2 && (align >= 16 || !STRICT_ALIGNMENT))\n+\t{\t\t\t/* move 2 bytes */\n+\t  move_bytes = 2;\n+\t  mode = HImode;\n+\t  gen_func.mov = gen_movhi;\n+\t}\n+      else if (TARGET_STRING && bytes > 1)\n+\t{\t\t\t/* move up to 4 bytes at a time */\n+\t  move_bytes = (bytes > 4) ? 4 : bytes;\n+\t  gen_func.movmemsi = gen_movmemsi_1reg;\n+\t}\n+      else /* move 1 byte at a time */\n+\t{\n+\t  move_bytes = 1;\n+\t  mode = QImode;\n+\t  gen_func.mov = gen_movqi;\n+\t}\n+\n+      src = adjust_address (orig_src, mode, offset);\n+      dest = adjust_address (orig_dest, mode, offset);\n+\n+      if (mode != BLKmode)\n+\t{\n+\t  rtx tmp_reg = gen_reg_rtx (mode);\n+\n+\t  emit_insn ((*gen_func.mov) (tmp_reg, src));\n+\t  stores[num_reg++] = (*gen_func.mov) (dest, tmp_reg);\n+\t}\n+\n+      if (mode == BLKmode || num_reg >= MAX_MOVE_REG || bytes == move_bytes)\n+\t{\n+\t  int i;\n+\t  for (i = 0; i < num_reg; i++)\n+\t    emit_insn (stores[i]);\n+\t  num_reg = 0;\n+\t}\n+\n+      if (mode == BLKmode)\n+\t{\n+\t  /* Move the address into scratch registers.  The movmemsi\n+\t     patterns require zero offset.  */\n+\t  if (!REG_P (XEXP (src, 0)))\n+\t    {\n+\t      rtx src_reg = copy_addr_to_reg (XEXP (src, 0));\n+\t      src = replace_equiv_address (src, src_reg);\n+\t    }\n+\t  set_mem_size (src, move_bytes);\n+\n+\t  if (!REG_P (XEXP (dest, 0)))\n+\t    {\n+\t      rtx dest_reg = copy_addr_to_reg (XEXP (dest, 0));\n+\t      dest = replace_equiv_address (dest, dest_reg);\n+\t    }\n+\t  set_mem_size (dest, move_bytes);\n+\n+\t  emit_insn ((*gen_func.movmemsi) (dest, src,\n+\t\t\t\t\t   GEN_INT (move_bytes & 31),\n+\t\t\t\t\t   align_rtx));\n+\t}\n+    }\n+\n+  return 1;\n+}\n+\n+\f\n+/* Return a string to perform a load_multiple operation.\n+   operands[0] is the vector.\n+   operands[1] is the source address.\n+   operands[2] is the first destination register.  */\n+\n+const char *\n+rs6000_output_load_multiple (rtx operands[3])\n+{\n+  /* We have to handle the case where the pseudo used to contain the address\n+     is assigned to one of the output registers.  */\n+  int i, j;\n+  int words = XVECLEN (operands[0], 0);\n+  rtx xop[10];\n+\n+  if (XVECLEN (operands[0], 0) == 1)\n+    return \"lwz %2,0(%1)\";\n+\n+  for (i = 0; i < words; i++)\n+    if (refers_to_regno_p (REGNO (operands[2]) + i, operands[1]))\n+      {\n+\tif (i == words-1)\n+\t  {\n+\t    xop[0] = GEN_INT (4 * (words-1));\n+\t    xop[1] = operands[1];\n+\t    xop[2] = operands[2];\n+\t    output_asm_insn (\"lswi %2,%1,%0\\n\\tlwz %1,%0(%1)\", xop);\n+\t    return \"\";\n+\t  }\n+\telse if (i == 0)\n+\t  {\n+\t    xop[0] = GEN_INT (4 * (words-1));\n+\t    xop[1] = operands[1];\n+\t    xop[2] = gen_rtx_REG (SImode, REGNO (operands[2]) + 1);\n+\t    output_asm_insn (\"addi %1,%1,4\\n\\tlswi %2,%1,%0\\n\\tlwz %1,-4(%1)\", xop);\n+\t    return \"\";\n+\t  }\n+\telse\n+\t  {\n+\t    for (j = 0; j < words; j++)\n+\t      if (j != i)\n+\t\t{\n+\t\t  xop[0] = GEN_INT (j * 4);\n+\t\t  xop[1] = operands[1];\n+\t\t  xop[2] = gen_rtx_REG (SImode, REGNO (operands[2]) + j);\n+\t\t  output_asm_insn (\"lwz %2,%0(%1)\", xop);\n+\t\t}\n+\t    xop[0] = GEN_INT (i * 4);\n+\t    xop[1] = operands[1];\n+\t    output_asm_insn (\"lwz %1,%0(%1)\", xop);\n+\t    return \"\";\n+\t  }\n+      }\n+\n+  return \"lswi %2,%1,%N0\";\n+}\n+"}, {"sha": "79dccba1dcecb881f6ed058779bbd6f21166c5c1", "filename": "gcc/config/rs6000/rs6000.c", "status": "modified", "additions": 1, "deletions": 1432, "changes": 1433, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2Fconfig%2Frs6000%2Frs6000.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2Fconfig%2Frs6000%2Frs6000.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.c?ref=8845cb377abb4e2ede6cc5c1a6bc635d71063fd2", "patch": "@@ -18664,121 +18664,14 @@ rs6000_init_libfuncs (void)\n     }\n }\n \n-\f\n-/* Expand a block clear operation, and return 1 if successful.  Return 0\n-   if we should let the compiler generate normal code.\n-\n-   operands[0] is the destination\n-   operands[1] is the length\n-   operands[3] is the alignment */\n-\n-int\n-expand_block_clear (rtx operands[])\n-{\n-  rtx orig_dest = operands[0];\n-  rtx bytes_rtx\t= operands[1];\n-  rtx align_rtx = operands[3];\n-  bool constp\t= (GET_CODE (bytes_rtx) == CONST_INT);\n-  HOST_WIDE_INT align;\n-  HOST_WIDE_INT bytes;\n-  int offset;\n-  int clear_bytes;\n-  int clear_step;\n-\n-  /* If this is not a fixed size move, just call memcpy */\n-  if (! constp)\n-    return 0;\n-\n-  /* This must be a fixed size alignment  */\n-  gcc_assert (GET_CODE (align_rtx) == CONST_INT);\n-  align = INTVAL (align_rtx) * BITS_PER_UNIT;\n-\n-  /* Anything to clear? */\n-  bytes = INTVAL (bytes_rtx);\n-  if (bytes <= 0)\n-    return 1;\n-\n-  /* Use the builtin memset after a point, to avoid huge code bloat.\n-     When optimize_size, avoid any significant code bloat; calling\n-     memset is about 4 instructions, so allow for one instruction to\n-     load zero and three to do clearing.  */\n-  if (TARGET_ALTIVEC && align >= 128)\n-    clear_step = 16;\n-  else if (TARGET_POWERPC64 && (align >= 64 || !STRICT_ALIGNMENT))\n-    clear_step = 8;\n-  else\n-    clear_step = 4;\n-\n-  if (optimize_size && bytes > 3 * clear_step)\n-    return 0;\n-  if (! optimize_size && bytes > 8 * clear_step)\n-    return 0;\n-\n-  for (offset = 0; bytes > 0; offset += clear_bytes, bytes -= clear_bytes)\n-    {\n-      machine_mode mode = BLKmode;\n-      rtx dest;\n-\n-      if (bytes >= 16 && TARGET_ALTIVEC && align >= 128)\n-\t{\n-\t  clear_bytes = 16;\n-\t  mode = V4SImode;\n-\t}\n-      else if (bytes >= 8 && TARGET_POWERPC64\n-\t       && (align >= 64 || !STRICT_ALIGNMENT))\n-\t{\n-\t  clear_bytes = 8;\n-\t  mode = DImode;\n-\t  if (offset == 0 && align < 64)\n-\t    {\n-\t      rtx addr;\n-\n-\t      /* If the address form is reg+offset with offset not a\n-\t\t multiple of four, reload into reg indirect form here\n-\t\t rather than waiting for reload.  This way we get one\n-\t\t reload, not one per store.  */\n-\t      addr = XEXP (orig_dest, 0);\n-\t      if ((GET_CODE (addr) == PLUS || GET_CODE (addr) == LO_SUM)\n-\t\t  && GET_CODE (XEXP (addr, 1)) == CONST_INT\n-\t\t  && (INTVAL (XEXP (addr, 1)) & 3) != 0)\n-\t\t{\n-\t\t  addr = copy_addr_to_reg (addr);\n-\t\t  orig_dest = replace_equiv_address (orig_dest, addr);\n-\t\t}\n-\t    }\n-\t}\n-      else if (bytes >= 4 && (align >= 32 || !STRICT_ALIGNMENT))\n-\t{\t\t\t/* move 4 bytes */\n-\t  clear_bytes = 4;\n-\t  mode = SImode;\n-\t}\n-      else if (bytes >= 2 && (align >= 16 || !STRICT_ALIGNMENT))\n-\t{\t\t\t/* move 2 bytes */\n-\t  clear_bytes = 2;\n-\t  mode = HImode;\n-\t}\n-      else /* move 1 byte at a time */\n-\t{\n-\t  clear_bytes = 1;\n-\t  mode = QImode;\n-\t}\n-\n-      dest = adjust_address (orig_dest, mode, offset);\n-\n-      emit_move_insn (dest, CONST0_RTX (mode));\n-    }\n-\n-  return 1;\n-}\n-\n /* Emit a potentially record-form instruction, setting DST from SRC.\n    If DOT is 0, that is all; otherwise, set CCREG to the result of the\n    signed comparison of DST with zero.  If DOT is 1, the generated RTL\n    doesn't care about the DST result; if DOT is 2, it does.  If CCREG\n    is CR0 do a single dot insn (as a PARALLEL); otherwise, do a SET and\n    a separate COMPARE.  */\n \n-static void\n+void\n rs6000_emit_dot_insn (rtx dst, rtx src, int dot, rtx ccreg)\n {\n   if (dot == 0)\n@@ -18807,1330 +18700,6 @@ rs6000_emit_dot_insn (rtx dst, rtx src, int dot, rtx ccreg)\n     }\n }\n \n-/* Figure out the correct instructions to generate to load data for\n-   block compare.  MODE is used for the read from memory, and\n-   data is zero extended if REG is wider than MODE.  If LE code\n-   is being generated, bswap loads are used.\n-\n-   REG is the destination register to move the data into.\n-   MEM is the memory block being read.\n-   MODE is the mode of memory to use for the read.  */\n-static void\n-do_load_for_compare (rtx reg, rtx mem, machine_mode mode)\n-{\n-  switch (GET_MODE (reg))\n-    {\n-    case DImode:\n-      switch (mode)\n-\t{\n-\tcase QImode:\n-\t  emit_insn (gen_zero_extendqidi2 (reg, mem));\n-\t  break;\n-\tcase HImode:\n-\t  {\n-\t    rtx src = mem;\n-\t    if (!BYTES_BIG_ENDIAN)\n-\t      {\n-\t\tsrc = gen_reg_rtx (HImode);\n-\t\temit_insn (gen_bswaphi2 (src, mem));\n-\t      }\n-\t    emit_insn (gen_zero_extendhidi2 (reg, src));\n-\t    break;\n-\t  }\n-\tcase SImode:\n-\t  {\n-\t    rtx src = mem;\n-\t    if (!BYTES_BIG_ENDIAN)\n-\t      {\n-\t\tsrc = gen_reg_rtx (SImode);\n-\t\temit_insn (gen_bswapsi2 (src, mem));\n-\t      }\n-\t    emit_insn (gen_zero_extendsidi2 (reg, src));\n-\t  }\n-\t  break;\n-\tcase DImode:\n-\t  if (!BYTES_BIG_ENDIAN)\n-\t    emit_insn (gen_bswapdi2 (reg, mem));\n-\t  else\n-\t    emit_insn (gen_movdi (reg, mem));\n-\t  break;\n-\tdefault:\n-\t  gcc_unreachable ();\n-\t}\n-      break;\n-\n-    case SImode:\n-      switch (mode)\n-\t{\n-\tcase QImode:\n-\t  emit_insn (gen_zero_extendqisi2 (reg, mem));\n-\t  break;\n-\tcase HImode:\n-\t  {\n-\t    rtx src = mem;\n-\t    if (!BYTES_BIG_ENDIAN)\n-\t      {\n-\t\tsrc = gen_reg_rtx (HImode);\n-\t\temit_insn (gen_bswaphi2 (src, mem));\n-\t      }\n-\t    emit_insn (gen_zero_extendhisi2 (reg, src));\n-\t    break;\n-\t  }\n-\tcase SImode:\n-\t  if (!BYTES_BIG_ENDIAN)\n-\t    emit_insn (gen_bswapsi2 (reg, mem));\n-\t  else\n-\t    emit_insn (gen_movsi (reg, mem));\n-\t  break;\n-\tcase DImode:\n-\t  /* DImode is larger than the destination reg so is not expected.  */\n-\t  gcc_unreachable ();\n-\t  break;\n-\tdefault:\n-\t  gcc_unreachable ();\n-\t}\n-      break;\n-    default:\n-      gcc_unreachable ();\n-      break;\n-    }\n-}\n-\n-/* Select the mode to be used for reading the next chunk of bytes\n-   in the compare.\n-\n-   OFFSET is the current read offset from the beginning of the block.\n-   BYTES is the number of bytes remaining to be read.\n-   ALIGN is the minimum alignment of the memory blocks being compared in bytes.\n-   WORD_MODE_OK indicates using WORD_MODE is allowed, else SImode is\n-   the largest allowable mode.  */\n-static machine_mode\n-select_block_compare_mode (unsigned HOST_WIDE_INT offset,\n-\t\t\t   unsigned HOST_WIDE_INT bytes,\n-\t\t\t   unsigned HOST_WIDE_INT align, bool word_mode_ok)\n-{\n-  /* First see if we can do a whole load unit\n-     as that will be more efficient than a larger load + shift.  */\n-\n-  /* If big, use biggest chunk.\n-     If exactly chunk size, use that size.\n-     If remainder can be done in one piece with shifting, do that.\n-     Do largest chunk possible without violating alignment rules.  */\n-\n-  /* The most we can read without potential page crossing.  */\n-  unsigned HOST_WIDE_INT maxread = ROUND_UP (bytes, align);\n-\n-  if (word_mode_ok && bytes >= UNITS_PER_WORD)\n-    return word_mode;\n-  else if (bytes == GET_MODE_SIZE (SImode))\n-    return SImode;\n-  else if (bytes == GET_MODE_SIZE (HImode))\n-    return HImode;\n-  else if (bytes == GET_MODE_SIZE (QImode))\n-    return QImode;\n-  else if (bytes < GET_MODE_SIZE (SImode)\n-\t   && offset >= GET_MODE_SIZE (SImode) - bytes)\n-    /* This matches the case were we have SImode and 3 bytes\n-       and offset >= 1 and permits us to move back one and overlap\n-       with the previous read, thus avoiding having to shift\n-       unwanted bytes off of the input.  */\n-    return SImode;\n-  else if (word_mode_ok && bytes < UNITS_PER_WORD\n-\t   && offset >= UNITS_PER_WORD-bytes)\n-    /* Similarly, if we can use DImode it will get matched here and\n-       can do an overlapping read that ends at the end of the block.  */\n-    return word_mode;\n-  else if (word_mode_ok && maxread >= UNITS_PER_WORD)\n-    /* It is safe to do all remaining in one load of largest size,\n-       possibly with a shift to get rid of unwanted bytes.  */\n-    return word_mode;\n-  else if (maxread >= GET_MODE_SIZE (SImode))\n-    /* It is safe to do all remaining in one SImode load,\n-       possibly with a shift to get rid of unwanted bytes.  */\n-    return SImode;\n-  else if (bytes > GET_MODE_SIZE (SImode))\n-    return SImode;\n-  else if (bytes > GET_MODE_SIZE (HImode))\n-    return HImode;\n-\n-  /* final fallback is do one byte */\n-  return QImode;\n-}\n-\n-/* Compute the alignment of pointer+OFFSET where the original alignment\n-   of pointer was BASE_ALIGN.  */\n-static unsigned HOST_WIDE_INT\n-compute_current_alignment (unsigned HOST_WIDE_INT base_align,\n-\t\t\t   unsigned HOST_WIDE_INT offset)\n-{\n-  if (offset == 0)\n-    return base_align;\n-  return min (base_align, offset & -offset);\n-}\n-\n-/* Expand a block compare operation, and return true if successful.\n-   Return false if we should let the compiler generate normal code,\n-   probably a memcmp call.\n-\n-   OPERANDS[0] is the target (result).\n-   OPERANDS[1] is the first source.\n-   OPERANDS[2] is the second source.\n-   OPERANDS[3] is the length.\n-   OPERANDS[4] is the alignment.  */\n-bool\n-expand_block_compare (rtx operands[])\n-{\n-  rtx target = operands[0];\n-  rtx orig_src1 = operands[1];\n-  rtx orig_src2 = operands[2];\n-  rtx bytes_rtx = operands[3];\n-  rtx align_rtx = operands[4];\n-  HOST_WIDE_INT cmp_bytes = 0;\n-  rtx src1 = orig_src1;\n-  rtx src2 = orig_src2;\n-\n-  /* This case is complicated to handle because the subtract\n-     with carry instructions do not generate the 64-bit\n-     carry and so we must emit code to calculate it ourselves.\n-     We choose not to implement this yet.  */\n-  if (TARGET_32BIT && TARGET_POWERPC64)\n-    return false;\n-\n-  /* If this is not a fixed size compare, just call memcmp.  */\n-  if (!CONST_INT_P (bytes_rtx))\n-    return false;\n-\n-  /* This must be a fixed size alignment.  */\n-  if (!CONST_INT_P (align_rtx))\n-    return false;\n-\n-  unsigned int base_align = UINTVAL (align_rtx) / BITS_PER_UNIT;\n-\n-  /* SLOW_UNALIGNED_ACCESS -- don't do unaligned stuff.  */\n-  if (SLOW_UNALIGNED_ACCESS (word_mode, MEM_ALIGN (orig_src1))\n-      || SLOW_UNALIGNED_ACCESS (word_mode, MEM_ALIGN (orig_src2)))\n-    return false;\n-\n-  gcc_assert (GET_MODE (target) == SImode);\n-\n-  /* Anything to move?  */\n-  unsigned HOST_WIDE_INT bytes = UINTVAL (bytes_rtx);\n-  if (bytes == 0)\n-    return true;\n-\n-  /* The code generated for p7 and older is not faster than glibc\n-     memcmp if alignment is small and length is not short, so bail\n-     out to avoid those conditions.  */\n-  if (!TARGET_EFFICIENT_OVERLAPPING_UNALIGNED\n-      && ((base_align == 1 && bytes > 16)\n-\t  || (base_align == 2 && bytes > 32)))\n-    return false;\n-\n-  rtx tmp_reg_src1 = gen_reg_rtx (word_mode);\n-  rtx tmp_reg_src2 = gen_reg_rtx (word_mode);\n-  /* P7/P8 code uses cond for subfc. but P9 uses\n-     it for cmpld which needs CCUNSmode. */\n-  rtx cond;\n-  if (TARGET_P9_MISC)\n-    cond = gen_reg_rtx (CCUNSmode);\n-  else\n-    cond = gen_reg_rtx (CCmode);\n-\n-  /* If we have an LE target without ldbrx and word_mode is DImode,\n-     then we must avoid using word_mode.  */\n-  int word_mode_ok = !(!BYTES_BIG_ENDIAN && !TARGET_LDBRX\n-\t\t       && word_mode == DImode);\n-\n-  /* Strategy phase.  How many ops will this take and should we expand it?  */\n-\n-  unsigned HOST_WIDE_INT offset = 0;\n-  machine_mode load_mode =\n-    select_block_compare_mode (offset, bytes, base_align, word_mode_ok);\n-  unsigned int load_mode_size = GET_MODE_SIZE (load_mode);\n-\n-  /* We don't want to generate too much code.  */\n-  unsigned HOST_WIDE_INT max_bytes =\n-    load_mode_size * (unsigned HOST_WIDE_INT) rs6000_block_compare_inline_limit;\n-  if (!IN_RANGE (bytes, 1, max_bytes))\n-    return false;\n-\n-  bool generate_6432_conversion = false;\n-  rtx convert_label = NULL;\n-  rtx final_label = NULL;\n-\n-  /* Example of generated code for 18 bytes aligned 1 byte.\n-     Compiled with -fno-reorder-blocks for clarity.\n-             ldbrx 10,31,8\n-             ldbrx 9,7,8\n-             subfc. 9,9,10\n-             bne 0,.L6487\n-             addi 9,12,8\n-             addi 5,11,8\n-             ldbrx 10,0,9\n-             ldbrx 9,0,5\n-             subfc. 9,9,10\n-             bne 0,.L6487\n-             addi 9,12,16\n-             lhbrx 10,0,9\n-             addi 9,11,16\n-             lhbrx 9,0,9\n-             subf 9,9,10\n-             b .L6488\n-             .p2align 4,,15\n-     .L6487: #convert_label\n-             popcntd 9,9\n-             subfe 10,10,10\n-             or 9,9,10\n-     .L6488: #final_label\n-             extsw 10,9\n-\n-     We start off with DImode for two blocks that jump to the DI->SI conversion\n-     if the difference is found there, then a final block of HImode that skips\n-     the DI->SI conversion.  */\n-\n-  while (bytes > 0)\n-    {\n-      unsigned int align = compute_current_alignment (base_align, offset);\n-      if (TARGET_EFFICIENT_OVERLAPPING_UNALIGNED)\n-\tload_mode = select_block_compare_mode (offset, bytes, align,\n-\t\t\t\t\t       word_mode_ok);\n-      else\n-\tload_mode = select_block_compare_mode (0, bytes, align, word_mode_ok);\n-      load_mode_size = GET_MODE_SIZE (load_mode);\n-      if (bytes >= load_mode_size)\n-\tcmp_bytes = load_mode_size;\n-      else if (TARGET_EFFICIENT_OVERLAPPING_UNALIGNED)\n-\t{\n-\t  /* Move this load back so it doesn't go past the end.\n-\t     P8/P9 can do this efficiently.  */\n-\t  unsigned int extra_bytes = load_mode_size - bytes;\n-\t  cmp_bytes = bytes;\n-\t  if (extra_bytes < offset)\n-\t    {\n-\t      offset -= extra_bytes;\n-\t      cmp_bytes = load_mode_size;\n-\t      bytes = cmp_bytes;\n-\t    }\n-\t}\n-      else\n-\t/* P7 and earlier can't do the overlapping load trick fast,\n-\t   so this forces a non-overlapping load and a shift to get\n-\t   rid of the extra bytes.  */\n-\tcmp_bytes = bytes;\n-\n-      src1 = adjust_address (orig_src1, load_mode, offset);\n-      src2 = adjust_address (orig_src2, load_mode, offset);\n-\n-      if (!REG_P (XEXP (src1, 0)))\n-\t{\n-\t  rtx src1_reg = copy_addr_to_reg (XEXP (src1, 0));\n-\t  src1 = replace_equiv_address (src1, src1_reg);\n-\t}\n-      set_mem_size (src1, cmp_bytes);\n-\n-      if (!REG_P (XEXP (src2, 0)))\n-\t{\n-\t  rtx src2_reg = copy_addr_to_reg (XEXP (src2, 0));\n-\t  src2 = replace_equiv_address (src2, src2_reg);\n-\t}\n-      set_mem_size (src2, cmp_bytes);\n-\n-      do_load_for_compare (tmp_reg_src1, src1, load_mode);\n-      do_load_for_compare (tmp_reg_src2, src2, load_mode);\n-\n-      if (cmp_bytes < load_mode_size)\n-\t{\n-\t  /* Shift unneeded bytes off.  */\n-\t  rtx sh = GEN_INT (BITS_PER_UNIT * (load_mode_size - cmp_bytes));\n-\t  if (word_mode == DImode)\n-\t    {\n-\t      emit_insn (gen_lshrdi3 (tmp_reg_src1, tmp_reg_src1, sh));\n-\t      emit_insn (gen_lshrdi3 (tmp_reg_src2, tmp_reg_src2, sh));\n-\t    }\n-\t  else\n-\t    {\n-\t      emit_insn (gen_lshrsi3 (tmp_reg_src1, tmp_reg_src1, sh));\n-\t      emit_insn (gen_lshrsi3 (tmp_reg_src2, tmp_reg_src2, sh));\n-\t    }\n-\t}\n-\n-      int remain = bytes - cmp_bytes;\n-      if (GET_MODE_SIZE (GET_MODE (target)) > GET_MODE_SIZE (load_mode))\n-\t{\n-\t  /* Target is larger than load size so we don't need to\n-\t     reduce result size.  */\n-\n-\t  /* We previously did a block that need 64->32 conversion but\n-\t     the current block does not, so a label is needed to jump\n-\t     to the end.  */\n-\t  if (generate_6432_conversion && !final_label)\n-\t    final_label = gen_label_rtx ();\n-\n-\t  if (remain > 0)\n-\t    {\n-\t      /* This is not the last block, branch to the end if the result\n-\t\t of this subtract is not zero.  */\n-\t      if (!final_label)\n-\t\tfinal_label = gen_label_rtx ();\n-\t      rtx fin_ref = gen_rtx_LABEL_REF (VOIDmode, final_label);\n-\t      rtx tmp = gen_rtx_MINUS (word_mode, tmp_reg_src1, tmp_reg_src2);\n-\t      rtx cr = gen_reg_rtx (CCmode);\n-\t      rs6000_emit_dot_insn (tmp_reg_src2, tmp, 2, cr);\n-\t      emit_insn (gen_movsi (target,\n-\t\t\t\t    gen_lowpart (SImode, tmp_reg_src2)));\n-\t      rtx ne_rtx = gen_rtx_NE (VOIDmode, cr, const0_rtx);\n-\t      rtx ifelse = gen_rtx_IF_THEN_ELSE (VOIDmode, ne_rtx,\n-\t\t\t\t\t\t fin_ref, pc_rtx);\n-\t      rtx j = emit_jump_insn (gen_rtx_SET (pc_rtx, ifelse));\n-\t      JUMP_LABEL (j) = final_label;\n-\t      LABEL_NUSES (final_label) += 1;\n-\t    }\n-\t  else\n-\t    {\n-\t      if (word_mode == DImode)\n-\t\t{\n-\t\t  emit_insn (gen_subdi3 (tmp_reg_src2, tmp_reg_src1,\n-\t\t\t\t\t tmp_reg_src2));\n-\t\t  emit_insn (gen_movsi (target,\n-\t\t\t\t\tgen_lowpart (SImode, tmp_reg_src2)));\n-\t\t}\n-\t      else\n-\t\temit_insn (gen_subsi3 (target, tmp_reg_src1, tmp_reg_src2));\n-\n-\t      if (final_label)\n-\t\t{\n-\t\t  rtx fin_ref = gen_rtx_LABEL_REF (VOIDmode, final_label);\n-\t\t  rtx j = emit_jump_insn (gen_rtx_SET (pc_rtx, fin_ref));\n-\t\t  JUMP_LABEL(j) = final_label;\n-\t\t  LABEL_NUSES (final_label) += 1;\n-\t\t  emit_barrier ();\n-\t\t}\n-\t    }\n-\t}\n-      else\n-\t{\n-\t  /* Do we need a 64->32 conversion block? We need the 64->32\n-\t     conversion even if target size == load_mode size because\n-\t     the subtract generates one extra bit.  */\n-\t  generate_6432_conversion = true;\n-\n-\t  if (remain > 0)\n-\t    {\n-\t      if (!convert_label)\n-\t\tconvert_label = gen_label_rtx ();\n-\n-\t      /* Compare to zero and branch to convert_label if not zero.  */\n-\t      rtx cvt_ref = gen_rtx_LABEL_REF (VOIDmode, convert_label);\n-\t      if (TARGET_P9_MISC)\n-\t\t{\n-\t\t/* Generate a compare, and convert with a setb later.  */\n-\t\t  rtx cmp = gen_rtx_COMPARE (CCUNSmode, tmp_reg_src1,\n-\t\t\t\t\t     tmp_reg_src2);\n-\t\t  emit_insn (gen_rtx_SET (cond, cmp));\n-\t\t}\n-\t      else\n-\t\t/* Generate a subfc. and use the longer\n-\t\t   sequence for conversion.  */\n-\t\tif (TARGET_64BIT)\n-\t\t  emit_insn (gen_subfdi3_carry_dot2 (tmp_reg_src2, tmp_reg_src2,\n-\t\t\t\t\t\t     tmp_reg_src1, cond));\n-\t\telse\n-\t\t  emit_insn (gen_subfsi3_carry_dot2 (tmp_reg_src2, tmp_reg_src2,\n-\t\t\t\t\t\t     tmp_reg_src1, cond));\n-\t      rtx ne_rtx = gen_rtx_NE (VOIDmode, cond, const0_rtx);\n-\t      rtx ifelse = gen_rtx_IF_THEN_ELSE (VOIDmode, ne_rtx,\n-\t\t\t\t\t\t cvt_ref, pc_rtx);\n-\t      rtx j = emit_jump_insn (gen_rtx_SET (pc_rtx, ifelse));\n-\t      JUMP_LABEL(j) = convert_label;\n-\t      LABEL_NUSES (convert_label) += 1;\n-\t    }\n-\t  else\n-\t    {\n-\t      /* Just do the subtract/compare.  Since this is the last block\n-\t\t the convert code will be generated immediately following.  */\n-\t      if (TARGET_P9_MISC)\n-\t\t{\n-\t\t  rtx cmp = gen_rtx_COMPARE (CCUNSmode, tmp_reg_src1,\n-\t\t\t\t\t     tmp_reg_src2);\n-\t\t  emit_insn (gen_rtx_SET (cond, cmp));\n-\t\t}\n-\t      else\n-\t\tif (TARGET_64BIT)\n-\t\t  emit_insn (gen_subfdi3_carry (tmp_reg_src2, tmp_reg_src2,\n-\t\t\t\t\t\ttmp_reg_src1));\n-\t\telse\n-\t\t  emit_insn (gen_subfsi3_carry (tmp_reg_src2, tmp_reg_src2,\n-\t\t\t\t\t\ttmp_reg_src1));\n-\t    }\n-\t}\n-\n-      offset += cmp_bytes;\n-      bytes -= cmp_bytes;\n-    }\n-\n-  if (generate_6432_conversion)\n-    {\n-      if (convert_label)\n-\temit_label (convert_label);\n-\n-      /* We need to produce DI result from sub, then convert to target SI\n-\t while maintaining <0 / ==0 / >0 properties. This sequence works:\n-\t subfc L,A,B\n-\t subfe H,H,H\n-\t popcntd L,L\n-\t rldimi L,H,6,0\n-\n-\t This is an alternate one Segher cooked up if somebody\n-\t wants to expand this for something that doesn't have popcntd:\n-\t subfc L,a,b\n-\t subfe H,x,x\n-\t addic t,L,-1\n-\t subfe v,t,L\n-\t or z,v,H\n-\n-\t And finally, p9 can just do this:\n-\t cmpld A,B\n-\t setb r */\n-\n-      if (TARGET_P9_MISC)\n-\t{\n-\t  emit_insn (gen_setb_unsigned (target, cond));\n-\t}\n-      else\n-\t{\n-\t  if (TARGET_64BIT)\n-\t    {\n-\t      rtx tmp_reg_ca = gen_reg_rtx (DImode);\n-\t      emit_insn (gen_subfdi3_carry_in_xx (tmp_reg_ca));\n-\t      emit_insn (gen_popcntddi2 (tmp_reg_src2, tmp_reg_src2));\n-\t      emit_insn (gen_iordi3 (tmp_reg_src2, tmp_reg_src2, tmp_reg_ca));\n-\t      emit_insn (gen_movsi (target, gen_lowpart (SImode, tmp_reg_src2)));\n-\t    }\n-\t  else\n-\t    {\n-\t      rtx tmp_reg_ca = gen_reg_rtx (SImode);\n-\t      emit_insn (gen_subfsi3_carry_in_xx (tmp_reg_ca));\n-\t      emit_insn (gen_popcntdsi2 (tmp_reg_src2, tmp_reg_src2));\n-\t      emit_insn (gen_iorsi3 (target, tmp_reg_src2, tmp_reg_ca));\n-\t    }\n-\t}\n-    }\n-\n-  if (final_label)\n-    emit_label (final_label);\n-\n-  gcc_assert (bytes == 0);\n-  return true;\n-}\n-\n-/* Generate alignment check and branch code to set up for\n-   strncmp when we don't have DI alignment.\n-   STRNCMP_LABEL is the label to branch if there is a page crossing.\n-   SRC is the string pointer to be examined.\n-   BYTES is the max number of bytes to compare.  */\n-static void\n-expand_strncmp_align_check (rtx strncmp_label, rtx src, HOST_WIDE_INT bytes)\n-{\n-  rtx lab_ref = gen_rtx_LABEL_REF (VOIDmode, strncmp_label);\n-  rtx src_check = copy_addr_to_reg (XEXP (src, 0));\n-  if (GET_MODE (src_check) == SImode)\n-    emit_insn (gen_andsi3 (src_check, src_check, GEN_INT (0xfff)));\n-  else\n-    emit_insn (gen_anddi3 (src_check, src_check, GEN_INT (0xfff)));\n-  rtx cond = gen_reg_rtx (CCmode);\n-  emit_move_insn (cond, gen_rtx_COMPARE (CCmode, src_check,\n-\t\t\t\t\t GEN_INT (4096 - bytes)));\n-\n-  rtx cmp_rtx = gen_rtx_LT (VOIDmode, cond, const0_rtx);\n-\n-  rtx ifelse = gen_rtx_IF_THEN_ELSE (VOIDmode, cmp_rtx,\n-\t\t\t\t     pc_rtx, lab_ref);\n-  rtx j = emit_jump_insn (gen_rtx_SET (pc_rtx, ifelse));\n-  JUMP_LABEL (j) = strncmp_label;\n-  LABEL_NUSES (strncmp_label) += 1;\n-}\n-\n-/* Expand a string compare operation with length, and return\n-   true if successful. Return false if we should let the\n-   compiler generate normal code, probably a strncmp call.\n-\n-   OPERANDS[0] is the target (result).\n-   OPERANDS[1] is the first source.\n-   OPERANDS[2] is the second source.\n-   If NO_LENGTH is zero, then:\n-   OPERANDS[3] is the length.\n-   OPERANDS[4] is the alignment in bytes.\n-   If NO_LENGTH is nonzero, then:\n-   OPERANDS[3] is the alignment in bytes.  */\n-bool\n-expand_strn_compare (rtx operands[], int no_length)\n-{\n-  rtx target = operands[0];\n-  rtx orig_src1 = operands[1];\n-  rtx orig_src2 = operands[2];\n-  rtx bytes_rtx, align_rtx;\n-  if (no_length)\n-    {\n-      bytes_rtx = NULL;\n-      align_rtx = operands[3];\n-    }\n-  else\n-    {\n-      bytes_rtx = operands[3];\n-      align_rtx = operands[4];\n-    }\n-  unsigned HOST_WIDE_INT cmp_bytes = 0;\n-  rtx src1 = orig_src1;\n-  rtx src2 = orig_src2;\n-\n-  /* If we have a length, it must be constant. This simplifies things\n-     a bit as we don't have to generate code to check if we've exceeded\n-     the length. Later this could be expanded to handle this case.  */\n-  if (!no_length && !CONST_INT_P (bytes_rtx))\n-    return false;\n-\n-  /* This must be a fixed size alignment.  */\n-  if (!CONST_INT_P (align_rtx))\n-    return false;\n-\n-  unsigned int base_align = UINTVAL (align_rtx);\n-  int align1 = MEM_ALIGN (orig_src1) / BITS_PER_UNIT;\n-  int align2 = MEM_ALIGN (orig_src2) / BITS_PER_UNIT;\n-\n-  /* SLOW_UNALIGNED_ACCESS -- don't do unaligned stuff.  */\n-  if (SLOW_UNALIGNED_ACCESS (word_mode, align1)\n-      || SLOW_UNALIGNED_ACCESS (word_mode, align2))\n-    return false;\n-\n-  gcc_assert (GET_MODE (target) == SImode);\n-\n-  /* If we have an LE target without ldbrx and word_mode is DImode,\n-     then we must avoid using word_mode.  */\n-  int word_mode_ok = !(!BYTES_BIG_ENDIAN && !TARGET_LDBRX\n-\t\t       && word_mode == DImode);\n-\n-  unsigned int word_mode_size = GET_MODE_SIZE (word_mode);\n-\n-  unsigned HOST_WIDE_INT offset = 0;\n-  unsigned HOST_WIDE_INT bytes; /* N from the strncmp args if available.  */\n-  unsigned HOST_WIDE_INT compare_length; /* How much to compare inline.  */\n-  if (no_length)\n-    /* Use this as a standin to determine the mode to use.  */\n-    bytes = rs6000_string_compare_inline_limit * word_mode_size;\n-  else\n-    bytes = UINTVAL (bytes_rtx);\n-\n-  machine_mode load_mode =\n-    select_block_compare_mode (offset, bytes, base_align, word_mode_ok);\n-  unsigned int load_mode_size = GET_MODE_SIZE (load_mode);\n-  compare_length = rs6000_string_compare_inline_limit * load_mode_size;\n-\n-  /* If we have equality at the end of the last compare and we have not\n-     found the end of the string, we need to call strcmp/strncmp to\n-     compare the remainder.  */\n-  bool equality_compare_rest = false;\n-\n-  if (no_length)\n-    {\n-      bytes = compare_length;\n-      equality_compare_rest = true;\n-    }\n-  else\n-    {\n-      if (bytes <= compare_length)\n-\tcompare_length = bytes;\n-      else\n-\tequality_compare_rest = true;\n-    }\n-\n-  rtx result_reg = gen_reg_rtx (word_mode);\n-  rtx final_move_label = gen_label_rtx ();\n-  rtx final_label = gen_label_rtx ();\n-  rtx begin_compare_label = NULL;\n-\n-  if (base_align < 8)\n-    {\n-      /* Generate code that checks distance to 4k boundary for this case.  */\n-      begin_compare_label = gen_label_rtx ();\n-      rtx strncmp_label = gen_label_rtx ();\n-      rtx jmp;\n-\n-      /* Strncmp for power8 in glibc does this:\n-\t rldicl\tr8,r3,0,52\n-\t cmpldi\tcr7,r8,4096-16\n-\t bgt\tcr7,L(pagecross) */\n-\n-      /* Make sure that the length we use for the alignment test and\n-         the subsequent code generation are in agreement so we do not\n-         go past the length we tested for a 4k boundary crossing.  */\n-      unsigned HOST_WIDE_INT align_test = compare_length;\n-      if (align_test < 8)\n-        {\n-          align_test = HOST_WIDE_INT_1U << ceil_log2 (align_test);\n-          base_align = align_test;\n-        }\n-      else\n-        {\n-          align_test = ROUND_UP (align_test, 8);\n-          base_align = 8;\n-        }\n-\n-      if (align1 < 8)\n-        expand_strncmp_align_check (strncmp_label, src1, align_test);\n-      if (align2 < 8)\n-        expand_strncmp_align_check (strncmp_label, src2, align_test);\n-\n-      /* Now generate the following sequence:\n-\t - branch to begin_compare\n-\t - strncmp_label\n-\t - call to strncmp\n-\t - branch to final_label\n-\t - begin_compare_label */\n-\n-      rtx cmp_ref = gen_rtx_LABEL_REF (VOIDmode, begin_compare_label);\n-      jmp = emit_jump_insn (gen_rtx_SET (pc_rtx, cmp_ref));\n-      JUMP_LABEL (jmp) = begin_compare_label;\n-      LABEL_NUSES (begin_compare_label) += 1;\n-      emit_barrier ();\n-\n-      emit_label (strncmp_label);\n-\n-      if (!REG_P (XEXP (src1, 0)))\n-\t{\n-\t  rtx src1_reg = copy_addr_to_reg (XEXP (src1, 0));\n-\t  src1 = replace_equiv_address (src1, src1_reg);\n-\t}\n-\n-      if (!REG_P (XEXP (src2, 0)))\n-\t{\n-\t  rtx src2_reg = copy_addr_to_reg (XEXP (src2, 0));\n-\t  src2 = replace_equiv_address (src2, src2_reg);\n-\t}\n-\n-      if (no_length)\n-\t{\n-\t  tree fun = builtin_decl_explicit (BUILT_IN_STRCMP);\n-\t  emit_library_call_value (XEXP (DECL_RTL (fun), 0),\n-\t\t\t\t   target, LCT_NORMAL, GET_MODE (target), 2,\n-\t\t\t\t   force_reg (Pmode, XEXP (src1, 0)), Pmode,\n-\t\t\t\t   force_reg (Pmode, XEXP (src2, 0)), Pmode);\n-\t}\n-      else\n-\t{\n-\t  /* -m32 -mpowerpc64 results in word_mode being DImode even\n-\t     though otherwise it is 32-bit. The length arg to strncmp\n-\t     is a size_t which will be the same size as pointers.  */\n-\t  rtx len_rtx;\n-\t  if (TARGET_64BIT)\n-\t    len_rtx = gen_reg_rtx (DImode);\n-\t  else\n-\t    len_rtx = gen_reg_rtx (SImode);\n-\n-\t  emit_move_insn (len_rtx, bytes_rtx);\n-\n-\t  tree fun = builtin_decl_explicit (BUILT_IN_STRNCMP);\n-\t  emit_library_call_value (XEXP (DECL_RTL (fun), 0),\n-\t\t\t\t   target, LCT_NORMAL, GET_MODE (target), 3,\n-\t\t\t\t   force_reg (Pmode, XEXP (src1, 0)), Pmode,\n-\t\t\t\t   force_reg (Pmode, XEXP (src2, 0)), Pmode,\n-\t\t\t\t   len_rtx, GET_MODE (len_rtx));\n-\t}\n-\n-      rtx fin_ref = gen_rtx_LABEL_REF (VOIDmode, final_label);\n-      jmp = emit_jump_insn (gen_rtx_SET (pc_rtx, fin_ref));\n-      JUMP_LABEL (jmp) = final_label;\n-      LABEL_NUSES (final_label) += 1;\n-      emit_barrier ();\n-      emit_label (begin_compare_label);\n-    }\n-\n-  rtx cleanup_label = NULL;\n-  rtx tmp_reg_src1 = gen_reg_rtx (word_mode);\n-  rtx tmp_reg_src2 = gen_reg_rtx (word_mode);\n-\n-  /* Generate sequence of ld/ldbrx, cmpb to compare out\n-     to the length specified.  */\n-  unsigned HOST_WIDE_INT bytes_to_compare = compare_length;\n-  while (bytes_to_compare > 0)\n-    {\n-      /* Compare sequence:\n-         check each 8B with: ld/ld cmpd bne\n-\t If equal, use rldicr/cmpb to check for zero byte.\n-         cleanup code at end:\n-         cmpb          get byte that differs\n-         cmpb          look for zero byte\n-         orc           combine\n-         cntlzd        get bit of first zero/diff byte\n-         subfic        convert for rldcl use\n-         rldcl rldcl   extract diff/zero byte\n-         subf          subtract for final result\n-\n-         The last compare can branch around the cleanup code if the\n-         result is zero because the strings are exactly equal.  */\n-      unsigned int align = compute_current_alignment (base_align, offset);\n-      if (TARGET_EFFICIENT_OVERLAPPING_UNALIGNED)\n-\tload_mode = select_block_compare_mode (offset, bytes_to_compare, align,\n-\t\t\t\t\t       word_mode_ok);\n-      else\n-\tload_mode = select_block_compare_mode (0, bytes_to_compare, align,\n-\t\t\t\t\t       word_mode_ok);\n-      load_mode_size = GET_MODE_SIZE (load_mode);\n-      if (bytes_to_compare >= load_mode_size)\n-\tcmp_bytes = load_mode_size;\n-      else if (TARGET_EFFICIENT_OVERLAPPING_UNALIGNED)\n-\t{\n-\t  /* Move this load back so it doesn't go past the end.\n-\t     P8/P9 can do this efficiently.  */\n-\t  unsigned int extra_bytes = load_mode_size - bytes_to_compare;\n-\t  cmp_bytes = bytes_to_compare;\n-\t  if (extra_bytes < offset)\n-\t    {\n-\t      offset -= extra_bytes;\n-\t      cmp_bytes = load_mode_size;\n-\t      bytes_to_compare = cmp_bytes;\n-\t    }\n-\t}\n-      else\n-\t/* P7 and earlier can't do the overlapping load trick fast,\n-\t   so this forces a non-overlapping load and a shift to get\n-\t   rid of the extra bytes.  */\n-\tcmp_bytes = bytes_to_compare;\n-\n-      src1 = adjust_address (orig_src1, load_mode, offset);\n-      src2 = adjust_address (orig_src2, load_mode, offset);\n-\n-      if (!REG_P (XEXP (src1, 0)))\n-\t{\n-\t  rtx src1_reg = copy_addr_to_reg (XEXP (src1, 0));\n-\t  src1 = replace_equiv_address (src1, src1_reg);\n-\t}\n-      set_mem_size (src1, cmp_bytes);\n-\n-      if (!REG_P (XEXP (src2, 0)))\n-\t{\n-\t  rtx src2_reg = copy_addr_to_reg (XEXP (src2, 0));\n-\t  src2 = replace_equiv_address (src2, src2_reg);\n-\t}\n-      set_mem_size (src2, cmp_bytes);\n-\n-      do_load_for_compare (tmp_reg_src1, src1, load_mode);\n-      do_load_for_compare (tmp_reg_src2, src2, load_mode);\n-\n-      /* We must always left-align the data we read, and\n-\t clear any bytes to the right that are beyond the string.\n-\t Otherwise the cmpb sequence won't produce the correct\n-\t results.  The beginning of the compare will be done\n-\t with word_mode so will not have any extra shifts or\n-\t clear rights.  */\n-\n-      if (load_mode_size < word_mode_size)\n-\t{\n-\t  /* Rotate left first. */\n-\t  rtx sh = GEN_INT (BITS_PER_UNIT * (word_mode_size - load_mode_size));\n-\t  if (word_mode == DImode)\n-\t    {\n-\t      emit_insn (gen_rotldi3 (tmp_reg_src1, tmp_reg_src1, sh));\n-\t      emit_insn (gen_rotldi3 (tmp_reg_src2, tmp_reg_src2, sh));\n-\t    }\n-\t  else\n-\t    {\n-\t      emit_insn (gen_rotlsi3 (tmp_reg_src1, tmp_reg_src1, sh));\n-\t      emit_insn (gen_rotlsi3 (tmp_reg_src2, tmp_reg_src2, sh));\n-\t    }\n-\t}\n-\n-      if (cmp_bytes < word_mode_size)\n-\t{\n-\t  /* Now clear right.  This plus the rotate can be\n-\t     turned into a rldicr instruction. */\n-\t  HOST_WIDE_INT mb = BITS_PER_UNIT * (word_mode_size - cmp_bytes);\n-\t  rtx mask = GEN_INT (HOST_WIDE_INT_M1U << mb);\n-\t  if (word_mode == DImode)\n-\t    {\n-\t      emit_insn (gen_anddi3_mask (tmp_reg_src1, tmp_reg_src1, mask));\n-\t      emit_insn (gen_anddi3_mask (tmp_reg_src2, tmp_reg_src2, mask));\n-\t    }\n-\t  else\n-\t    {\n-\t      emit_insn (gen_andsi3_mask (tmp_reg_src1, tmp_reg_src1, mask));\n-\t      emit_insn (gen_andsi3_mask (tmp_reg_src2, tmp_reg_src2, mask));\n-\t    }\n-\t}\n-\n-      /* Cases to handle.  A and B are chunks of the two strings.\n-\t 1: Not end of comparison:\n-\t A != B: branch to cleanup code to compute result.\n-\t A == B: check for 0 byte, next block if not found.\n-\t 2: End of the inline comparison:\n-\t A != B: branch to cleanup code to compute result.\n-\t A == B: check for 0 byte, call strcmp/strncmp\n-\t 3: compared requested N bytes:\n-\t A == B: branch to result 0.\n-\t A != B: cleanup code to compute result.  */\n-\n-      unsigned HOST_WIDE_INT remain = bytes_to_compare - cmp_bytes;\n-\n-      rtx dst_label;\n-      if (remain > 0 || equality_compare_rest)\n-\t{\n-\t  /* Branch to cleanup code, otherwise fall through to do\n-\t     more compares.  */\n-\t  if (!cleanup_label)\n-\t    cleanup_label = gen_label_rtx ();\n-\t  dst_label = cleanup_label;\n-\t}\n-      else\n-\t/* Branch to end and produce result of 0.  */\n-\tdst_label = final_move_label;\n-\n-      rtx lab_ref = gen_rtx_LABEL_REF (VOIDmode, dst_label);\n-      rtx cond = gen_reg_rtx (CCmode);\n-\n-      /* Always produce the 0 result, it is needed if\n-\t cmpb finds a 0 byte in this chunk.  */\n-      rtx tmp = gen_rtx_MINUS (word_mode, tmp_reg_src1, tmp_reg_src2);\n-      rs6000_emit_dot_insn (result_reg, tmp, 1, cond);\n-\n-      rtx cmp_rtx;\n-      if (remain == 0 && !equality_compare_rest)\n-\tcmp_rtx = gen_rtx_EQ (VOIDmode, cond, const0_rtx);\n-      else\n-\tcmp_rtx = gen_rtx_NE (VOIDmode, cond, const0_rtx);\n-\n-      rtx ifelse = gen_rtx_IF_THEN_ELSE (VOIDmode, cmp_rtx,\n-\t\t\t\t\t lab_ref, pc_rtx);\n-      rtx j = emit_jump_insn (gen_rtx_SET (pc_rtx, ifelse));\n-      JUMP_LABEL (j) = dst_label;\n-      LABEL_NUSES (dst_label) += 1;\n-\n-      if (remain > 0 || equality_compare_rest)\n-\t{\n-\t  /* Generate a cmpb to test for a 0 byte and branch\n-\t     to final result if found.  */\n-\t  rtx cmpb_zero = gen_reg_rtx (word_mode);\n-\t  rtx lab_ref_fin = gen_rtx_LABEL_REF (VOIDmode, final_move_label);\n-\t  rtx condz = gen_reg_rtx (CCmode);\n-\t  rtx zero_reg = gen_reg_rtx (word_mode);\n-\t  if (word_mode == SImode)\n-\t    {\n-\t      emit_insn (gen_movsi (zero_reg, GEN_INT (0)));\n-\t      emit_insn (gen_cmpbsi3 (cmpb_zero, tmp_reg_src1, zero_reg));\n-\t      if (cmp_bytes < word_mode_size)\n-\t\t{\n-\t\t  /* Don't want to look at zero bytes past end.  */\n-\t\t  HOST_WIDE_INT mb =\n-\t\t    BITS_PER_UNIT * (word_mode_size - cmp_bytes);\n-\t\t  rtx mask = GEN_INT (HOST_WIDE_INT_M1U << mb);\n-\t\t  emit_insn (gen_andsi3_mask (cmpb_zero, cmpb_zero, mask));\n-\t\t}\n-\t    }\n-\t  else\n-\t    {\n-\t      emit_insn (gen_movdi (zero_reg, GEN_INT (0)));\n-\t      emit_insn (gen_cmpbdi3 (cmpb_zero, tmp_reg_src1, zero_reg));\n-\t      if (cmp_bytes < word_mode_size)\n-\t\t{\n-\t\t  /* Don't want to look at zero bytes past end.  */\n-\t\t  HOST_WIDE_INT mb =\n-\t\t    BITS_PER_UNIT * (word_mode_size - cmp_bytes);\n-\t\t  rtx mask = GEN_INT (HOST_WIDE_INT_M1U << mb);\n-\t\t  emit_insn (gen_anddi3_mask (cmpb_zero, cmpb_zero, mask));\n-\t\t}\n-\t    }\n-\n-\t  emit_move_insn (condz, gen_rtx_COMPARE (CCmode, cmpb_zero, zero_reg));\n-\t  rtx cmpnz_rtx = gen_rtx_NE (VOIDmode, condz, const0_rtx);\n-\t  rtx ifelse = gen_rtx_IF_THEN_ELSE (VOIDmode, cmpnz_rtx,\n-\t\t\t\t\t     lab_ref_fin, pc_rtx);\n-\t  rtx j2 = emit_jump_insn (gen_rtx_SET (pc_rtx, ifelse));\n-\t  JUMP_LABEL (j2) = final_move_label;\n-\t  LABEL_NUSES (final_move_label) += 1;\n-\n-\t}\n-\n-      offset += cmp_bytes;\n-      bytes_to_compare -= cmp_bytes;\n-    }\n-\n-  if (equality_compare_rest)\n-    {\n-      /* Update pointers past what has been compared already.  */\n-      src1 = adjust_address (orig_src1, load_mode, offset);\n-      src2 = adjust_address (orig_src2, load_mode, offset);\n-\n-      if (!REG_P (XEXP (src1, 0)))\n-\t{\n-\t  rtx src1_reg = copy_addr_to_reg (XEXP (src1, 0));\n-\t  src1 = replace_equiv_address (src1, src1_reg);\n-\t}\n-      set_mem_size (src1, cmp_bytes);\n-\n-      if (!REG_P (XEXP (src2, 0)))\n-\t{\n-\t  rtx src2_reg = copy_addr_to_reg (XEXP (src2, 0));\n-\t  src2 = replace_equiv_address (src2, src2_reg);\n-\t}\n-      set_mem_size (src2, cmp_bytes);\n-\n-      /* Construct call to strcmp/strncmp to compare the rest of the string.  */\n-      if (no_length)\n-\t{\n-\t  tree fun = builtin_decl_explicit (BUILT_IN_STRCMP);\n-\t  emit_library_call_value (XEXP (DECL_RTL (fun), 0),\n-\t\t\t\t   target, LCT_NORMAL, GET_MODE (target), 2,\n-\t\t\t\t   force_reg (Pmode, XEXP (src1, 0)), Pmode,\n-\t\t\t\t   force_reg (Pmode, XEXP (src2, 0)), Pmode);\n-\t}\n-      else\n-\t{\n-\t  rtx len_rtx;\n-\t  if (TARGET_64BIT)\n-\t    len_rtx = gen_reg_rtx (DImode);\n-\t  else\n-\t    len_rtx = gen_reg_rtx (SImode);\n-\n-\t  emit_move_insn (len_rtx, GEN_INT (bytes - compare_length));\n-\t  tree fun = builtin_decl_explicit (BUILT_IN_STRNCMP);\n-\t  emit_library_call_value (XEXP (DECL_RTL (fun), 0),\n-\t\t\t\t   target, LCT_NORMAL, GET_MODE (target), 3,\n-\t\t\t\t   force_reg (Pmode, XEXP (src1, 0)), Pmode,\n-\t\t\t\t   force_reg (Pmode, XEXP (src2, 0)), Pmode,\n-\t\t\t\t   len_rtx, GET_MODE (len_rtx));\n-\t}\n-\n-      rtx fin_ref = gen_rtx_LABEL_REF (VOIDmode, final_label);\n-      rtx jmp = emit_jump_insn (gen_rtx_SET (pc_rtx, fin_ref));\n-      JUMP_LABEL (jmp) = final_label;\n-      LABEL_NUSES (final_label) += 1;\n-      emit_barrier ();\n-    }\n-\n-  if (cleanup_label)\n-    emit_label (cleanup_label);\n-\n-  /* Generate the final sequence that identifies the differing\n-     byte and generates the final result, taking into account\n-     zero bytes:\n-\n-     cmpb              cmpb_result1, src1, src2\n-     cmpb              cmpb_result2, src1, zero\n-     orc               cmpb_result1, cmp_result1, cmpb_result2\n-     cntlzd            get bit of first zero/diff byte\n-     addi              convert for rldcl use\n-     rldcl rldcl       extract diff/zero byte\n-     subf              subtract for final result\n-  */\n-\n-  rtx cmpb_diff = gen_reg_rtx (word_mode);\n-  rtx cmpb_zero = gen_reg_rtx (word_mode);\n-  rtx rot_amt = gen_reg_rtx (word_mode);\n-  rtx zero_reg = gen_reg_rtx (word_mode);\n-\n-  rtx rot1_1 = gen_reg_rtx (word_mode);\n-  rtx rot1_2 = gen_reg_rtx (word_mode);\n-  rtx rot2_1 = gen_reg_rtx (word_mode);\n-  rtx rot2_2 = gen_reg_rtx (word_mode);\n-\n-  if (word_mode == SImode)\n-    {\n-      emit_insn (gen_cmpbsi3 (cmpb_diff, tmp_reg_src1, tmp_reg_src2));\n-      emit_insn (gen_movsi (zero_reg, GEN_INT (0)));\n-      emit_insn (gen_cmpbsi3 (cmpb_zero, tmp_reg_src1, zero_reg));\n-      emit_insn (gen_one_cmplsi2 (cmpb_diff,cmpb_diff));\n-      emit_insn (gen_iorsi3 (cmpb_diff, cmpb_diff, cmpb_zero));\n-      emit_insn (gen_clzsi2 (rot_amt, cmpb_diff));\n-      emit_insn (gen_addsi3 (rot_amt, rot_amt, GEN_INT (8)));\n-      emit_insn (gen_rotlsi3 (rot1_1, tmp_reg_src1,\n-\t\t\t      gen_lowpart (SImode, rot_amt)));\n-      emit_insn (gen_andsi3_mask (rot1_2, rot1_1, GEN_INT (0xff)));\n-      emit_insn (gen_rotlsi3 (rot2_1, tmp_reg_src2,\n-\t\t\t      gen_lowpart (SImode, rot_amt)));\n-      emit_insn (gen_andsi3_mask (rot2_2, rot2_1, GEN_INT (0xff)));\n-      emit_insn (gen_subsi3 (result_reg, rot1_2, rot2_2));\n-    }\n-  else\n-    {\n-      emit_insn (gen_cmpbdi3 (cmpb_diff, tmp_reg_src1, tmp_reg_src2));\n-      emit_insn (gen_movdi (zero_reg, GEN_INT (0)));\n-      emit_insn (gen_cmpbdi3 (cmpb_zero, tmp_reg_src1, zero_reg));\n-      emit_insn (gen_one_cmpldi2 (cmpb_diff,cmpb_diff));\n-      emit_insn (gen_iordi3 (cmpb_diff, cmpb_diff, cmpb_zero));\n-      emit_insn (gen_clzdi2 (rot_amt, cmpb_diff));\n-      emit_insn (gen_adddi3 (rot_amt, rot_amt, GEN_INT (8)));\n-      emit_insn (gen_rotldi3 (rot1_1, tmp_reg_src1,\n-\t\t\t      gen_lowpart (SImode, rot_amt)));\n-      emit_insn (gen_anddi3_mask (rot1_2, rot1_1, GEN_INT (0xff)));\n-      emit_insn (gen_rotldi3 (rot2_1, tmp_reg_src2,\n-\t\t\t      gen_lowpart (SImode, rot_amt)));\n-      emit_insn (gen_anddi3_mask (rot2_2, rot2_1, GEN_INT (0xff)));\n-      emit_insn (gen_subdi3 (result_reg, rot1_2, rot2_2));\n-    }\n-\n-  emit_label (final_move_label);\n-  emit_insn (gen_movsi (target,\n-\t\t\tgen_lowpart (SImode, result_reg)));\n-  emit_label (final_label);\n-  return true;\n-}\n-\n-/* Expand a block move operation, and return 1 if successful.  Return 0\n-   if we should let the compiler generate normal code.\n-\n-   operands[0] is the destination\n-   operands[1] is the source\n-   operands[2] is the length\n-   operands[3] is the alignment */\n-\n-#define MAX_MOVE_REG 4\n-\n-int\n-expand_block_move (rtx operands[])\n-{\n-  rtx orig_dest = operands[0];\n-  rtx orig_src\t= operands[1];\n-  rtx bytes_rtx\t= operands[2];\n-  rtx align_rtx = operands[3];\n-  int constp\t= (GET_CODE (bytes_rtx) == CONST_INT);\n-  int align;\n-  int bytes;\n-  int offset;\n-  int move_bytes;\n-  rtx stores[MAX_MOVE_REG];\n-  int num_reg = 0;\n-\n-  /* If this is not a fixed size move, just call memcpy */\n-  if (! constp)\n-    return 0;\n-\n-  /* This must be a fixed size alignment */\n-  gcc_assert (GET_CODE (align_rtx) == CONST_INT);\n-  align = INTVAL (align_rtx) * BITS_PER_UNIT;\n-\n-  /* Anything to move? */\n-  bytes = INTVAL (bytes_rtx);\n-  if (bytes <= 0)\n-    return 1;\n-\n-  if (bytes > rs6000_block_move_inline_limit)\n-    return 0;\n-\n-  for (offset = 0; bytes > 0; offset += move_bytes, bytes -= move_bytes)\n-    {\n-      union {\n-\trtx (*movmemsi) (rtx, rtx, rtx, rtx);\n-\trtx (*mov) (rtx, rtx);\n-      } gen_func;\n-      machine_mode mode = BLKmode;\n-      rtx src, dest;\n-\n-      /* Altivec first, since it will be faster than a string move\n-\t when it applies, and usually not significantly larger.  */\n-      if (TARGET_ALTIVEC && bytes >= 16 && align >= 128)\n-\t{\n-\t  move_bytes = 16;\n-\t  mode = V4SImode;\n-\t  gen_func.mov = gen_movv4si;\n-\t}\n-      else if (TARGET_STRING\n-\t  && bytes > 24\t\t/* move up to 32 bytes at a time */\n-\t  && ! fixed_regs[5]\n-\t  && ! fixed_regs[6]\n-\t  && ! fixed_regs[7]\n-\t  && ! fixed_regs[8]\n-\t  && ! fixed_regs[9]\n-\t  && ! fixed_regs[10]\n-\t  && ! fixed_regs[11]\n-\t  && ! fixed_regs[12])\n-\t{\n-\t  move_bytes = (bytes > 32) ? 32 : bytes;\n-\t  gen_func.movmemsi = gen_movmemsi_8reg;\n-\t}\n-      else if (TARGET_STRING\n-\t       && bytes > 16\t/* move up to 24 bytes at a time */\n-\t       && ! fixed_regs[5]\n-\t       && ! fixed_regs[6]\n-\t       && ! fixed_regs[7]\n-\t       && ! fixed_regs[8]\n-\t       && ! fixed_regs[9]\n-\t       && ! fixed_regs[10])\n-\t{\n-\t  move_bytes = (bytes > 24) ? 24 : bytes;\n-\t  gen_func.movmemsi = gen_movmemsi_6reg;\n-\t}\n-      else if (TARGET_STRING\n-\t       && bytes > 8\t/* move up to 16 bytes at a time */\n-\t       && ! fixed_regs[5]\n-\t       && ! fixed_regs[6]\n-\t       && ! fixed_regs[7]\n-\t       && ! fixed_regs[8])\n-\t{\n-\t  move_bytes = (bytes > 16) ? 16 : bytes;\n-\t  gen_func.movmemsi = gen_movmemsi_4reg;\n-\t}\n-      else if (bytes >= 8 && TARGET_POWERPC64\n-\t       && (align >= 64 || !STRICT_ALIGNMENT))\n-\t{\n-\t  move_bytes = 8;\n-\t  mode = DImode;\n-\t  gen_func.mov = gen_movdi;\n-\t  if (offset == 0 && align < 64)\n-\t    {\n-\t      rtx addr;\n-\n-\t      /* If the address form is reg+offset with offset not a\n-\t\t multiple of four, reload into reg indirect form here\n-\t\t rather than waiting for reload.  This way we get one\n-\t\t reload, not one per load and/or store.  */\n-\t      addr = XEXP (orig_dest, 0);\n-\t      if ((GET_CODE (addr) == PLUS || GET_CODE (addr) == LO_SUM)\n-\t\t  && GET_CODE (XEXP (addr, 1)) == CONST_INT\n-\t\t  && (INTVAL (XEXP (addr, 1)) & 3) != 0)\n-\t\t{\n-\t\t  addr = copy_addr_to_reg (addr);\n-\t\t  orig_dest = replace_equiv_address (orig_dest, addr);\n-\t\t}\n-\t      addr = XEXP (orig_src, 0);\n-\t      if ((GET_CODE (addr) == PLUS || GET_CODE (addr) == LO_SUM)\n-\t\t  && GET_CODE (XEXP (addr, 1)) == CONST_INT\n-\t\t  && (INTVAL (XEXP (addr, 1)) & 3) != 0)\n-\t\t{\n-\t\t  addr = copy_addr_to_reg (addr);\n-\t\t  orig_src = replace_equiv_address (orig_src, addr);\n-\t\t}\n-\t    }\n-\t}\n-      else if (TARGET_STRING && bytes > 4 && !TARGET_POWERPC64)\n-\t{\t\t\t/* move up to 8 bytes at a time */\n-\t  move_bytes = (bytes > 8) ? 8 : bytes;\n-\t  gen_func.movmemsi = gen_movmemsi_2reg;\n-\t}\n-      else if (bytes >= 4 && (align >= 32 || !STRICT_ALIGNMENT))\n-\t{\t\t\t/* move 4 bytes */\n-\t  move_bytes = 4;\n-\t  mode = SImode;\n-\t  gen_func.mov = gen_movsi;\n-\t}\n-      else if (bytes >= 2 && (align >= 16 || !STRICT_ALIGNMENT))\n-\t{\t\t\t/* move 2 bytes */\n-\t  move_bytes = 2;\n-\t  mode = HImode;\n-\t  gen_func.mov = gen_movhi;\n-\t}\n-      else if (TARGET_STRING && bytes > 1)\n-\t{\t\t\t/* move up to 4 bytes at a time */\n-\t  move_bytes = (bytes > 4) ? 4 : bytes;\n-\t  gen_func.movmemsi = gen_movmemsi_1reg;\n-\t}\n-      else /* move 1 byte at a time */\n-\t{\n-\t  move_bytes = 1;\n-\t  mode = QImode;\n-\t  gen_func.mov = gen_movqi;\n-\t}\n-\n-      src = adjust_address (orig_src, mode, offset);\n-      dest = adjust_address (orig_dest, mode, offset);\n-\n-      if (mode != BLKmode)\n-\t{\n-\t  rtx tmp_reg = gen_reg_rtx (mode);\n-\n-\t  emit_insn ((*gen_func.mov) (tmp_reg, src));\n-\t  stores[num_reg++] = (*gen_func.mov) (dest, tmp_reg);\n-\t}\n-\n-      if (mode == BLKmode || num_reg >= MAX_MOVE_REG || bytes == move_bytes)\n-\t{\n-\t  int i;\n-\t  for (i = 0; i < num_reg; i++)\n-\t    emit_insn (stores[i]);\n-\t  num_reg = 0;\n-\t}\n-\n-      if (mode == BLKmode)\n-\t{\n-\t  /* Move the address into scratch registers.  The movmemsi\n-\t     patterns require zero offset.  */\n-\t  if (!REG_P (XEXP (src, 0)))\n-\t    {\n-\t      rtx src_reg = copy_addr_to_reg (XEXP (src, 0));\n-\t      src = replace_equiv_address (src, src_reg);\n-\t    }\n-\t  set_mem_size (src, move_bytes);\n-\n-\t  if (!REG_P (XEXP (dest, 0)))\n-\t    {\n-\t      rtx dest_reg = copy_addr_to_reg (XEXP (dest, 0));\n-\t      dest = replace_equiv_address (dest, dest_reg);\n-\t    }\n-\t  set_mem_size (dest, move_bytes);\n-\n-\t  emit_insn ((*gen_func.movmemsi) (dest, src,\n-\t\t\t\t\t   GEN_INT (move_bytes & 31),\n-\t\t\t\t\t   align_rtx));\n-\t}\n-    }\n-\n-  return 1;\n-}\n-\n-\f\n-/* Return a string to perform a load_multiple operation.\n-   operands[0] is the vector.\n-   operands[1] is the source address.\n-   operands[2] is the first destination register.  */\n-\n-const char *\n-rs6000_output_load_multiple (rtx operands[3])\n-{\n-  /* We have to handle the case where the pseudo used to contain the address\n-     is assigned to one of the output registers.  */\n-  int i, j;\n-  int words = XVECLEN (operands[0], 0);\n-  rtx xop[10];\n-\n-  if (XVECLEN (operands[0], 0) == 1)\n-    return \"lwz %2,0(%1)\";\n-\n-  for (i = 0; i < words; i++)\n-    if (refers_to_regno_p (REGNO (operands[2]) + i, operands[1]))\n-      {\n-\tif (i == words-1)\n-\t  {\n-\t    xop[0] = GEN_INT (4 * (words-1));\n-\t    xop[1] = operands[1];\n-\t    xop[2] = operands[2];\n-\t    output_asm_insn (\"lswi %2,%1,%0\\n\\tlwz %1,%0(%1)\", xop);\n-\t    return \"\";\n-\t  }\n-\telse if (i == 0)\n-\t  {\n-\t    xop[0] = GEN_INT (4 * (words-1));\n-\t    xop[1] = operands[1];\n-\t    xop[2] = gen_rtx_REG (SImode, REGNO (operands[2]) + 1);\n-\t    output_asm_insn (\"addi %1,%1,4\\n\\tlswi %2,%1,%0\\n\\tlwz %1,-4(%1)\", xop);\n-\t    return \"\";\n-\t  }\n-\telse\n-\t  {\n-\t    for (j = 0; j < words; j++)\n-\t      if (j != i)\n-\t\t{\n-\t\t  xop[0] = GEN_INT (j * 4);\n-\t\t  xop[1] = operands[1];\n-\t\t  xop[2] = gen_rtx_REG (SImode, REGNO (operands[2]) + j);\n-\t\t  output_asm_insn (\"lwz %2,%0(%1)\", xop);\n-\t\t}\n-\t    xop[0] = GEN_INT (i * 4);\n-\t    xop[1] = operands[1];\n-\t    output_asm_insn (\"lwz %1,%0(%1)\", xop);\n-\t    return \"\";\n-\t  }\n-      }\n-\n-  return \"lswi %2,%1,%N0\";\n-}\n-\n \f\n /* A validation routine: say whether CODE, a condition code, and MODE\n    match.  The other alternatives either don't make sense or should"}, {"sha": "a47091ad1b42ba163ba4597fedf9be1eab3d1e35", "filename": "gcc/config/rs6000/t-rs6000", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2Fconfig%2Frs6000%2Ft-rs6000", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8845cb377abb4e2ede6cc5c1a6bc635d71063fd2/gcc%2Fconfig%2Frs6000%2Ft-rs6000", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Ft-rs6000?ref=8845cb377abb4e2ede6cc5c1a6bc635d71063fd2", "patch": "@@ -26,6 +26,10 @@ rs6000-c.o: $(srcdir)/config/rs6000/rs6000-c.c\n \t$(COMPILE) $<\n \t$(POSTCOMPILE)\n \n+rs6000-string.o: $(srcdir)/config/rs6000/rs6000-string.c\n+\t$(COMPILE) $<\n+\t$(POSTCOMPILE)\n+\n $(srcdir)/config/rs6000/rs6000-tables.opt: $(srcdir)/config/rs6000/genopt.sh \\\n   $(srcdir)/config/rs6000/rs6000-cpus.def\n \t$(SHELL) $(srcdir)/config/rs6000/genopt.sh $(srcdir)/config/rs6000 > \\"}]}