{"sha": "828e70c1d7bb5c849a2df44aa832793c71833058", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODI4ZTcwYzFkN2JiNWM4NDlhMmRmNDRhYTgzMjc5M2M3MTgzMzA1OA==", "commit": {"author": {"name": "James Greenhalgh", "email": "james.greenhalgh@arm.com", "date": "2013-09-16T09:53:11Z"}, "committer": {"name": "James Greenhalgh", "email": "jgreenhalgh@gcc.gnu.org", "date": "2013-09-16T09:53:11Z"}, "message": "[AArch64] Improve arm_neon.h vml<as>_lane handling.\n\ngcc/\n\t* config/aarch64/aarch64-simd-builtins.def (fma): New.\n\t* config/aarch64/aarch64-simd.md\n\t(aarch64_mla_elt<mode>): New.\n\t(aarch64_mla_elt_<vswap_width_name><mode>): Likewise.\n\t(aarch64_mls_elt<mode>): Likewise.\n\t(aarch64_mls_elt_<vswap_width_name><mode>): Likewise.\n\t(aarch64_fma4_elt<mode>): Likewise.\n\t(aarch64_fma4_elt_<vswap_width_name><mode>): Likewise.\n\t(aarch64_fma4_elt_to_128v2df): Likewise.\n\t(aarch64_fma4_elt_to_64df): Likewise.\n\t(fnma<mode>4): Likewise.\n\t(aarch64_fnma4_elt<mode>): Likewise.\n\t(aarch64_fnma4_elt_<vswap_width_name><mode>): Likewise.\n\t(aarch64_fnma4_elt_to_128v2df): Likewise.\n\t(aarch64_fnma4_elt_to_64df): Likewise.\n\t* config/aarch64/iterators.md (VDQSF): New.\n\t* config/aarch64/arm_neon.h\n\t(vfm<as><sdq>_lane<q>_f<32, 64>): Convert to C implementation.\n\t(vml<sa><q>_lane<q>_<fsu><16, 32, 64>): Likewise.\n\ngcc/testsuite/\n\t* gcc.target/aarch64/fmla-intrinsic.c: New.\n\t* gcc.target/aarch64/mla-intrinsic.c: Likewise.\n\t* gcc.target/aarch64/fmls-intrinsic.c: Likewise.\n\t* gcc.target/aarch64/mls-intrinsic.c: Likewise.\n\nFrom-SVN: r202625", "tree": {"sha": "8fa3db5b4d674a6afa031fe0d47a15cbe1e15109", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/8fa3db5b4d674a6afa031fe0d47a15cbe1e15109"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/828e70c1d7bb5c849a2df44aa832793c71833058", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/828e70c1d7bb5c849a2df44aa832793c71833058", "html_url": "https://github.com/Rust-GCC/gccrs/commit/828e70c1d7bb5c849a2df44aa832793c71833058", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/828e70c1d7bb5c849a2df44aa832793c71833058/comments", "author": {"login": "jgreenhalgh-arm", "id": 6104025, "node_id": "MDQ6VXNlcjYxMDQwMjU=", "avatar_url": "https://avatars.githubusercontent.com/u/6104025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgreenhalgh-arm", "html_url": "https://github.com/jgreenhalgh-arm", "followers_url": "https://api.github.com/users/jgreenhalgh-arm/followers", "following_url": "https://api.github.com/users/jgreenhalgh-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jgreenhalgh-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgreenhalgh-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgreenhalgh-arm/subscriptions", "organizations_url": "https://api.github.com/users/jgreenhalgh-arm/orgs", "repos_url": "https://api.github.com/users/jgreenhalgh-arm/repos", "events_url": "https://api.github.com/users/jgreenhalgh-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jgreenhalgh-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "779aea46cc3b6b1a62f989ee0c04e68803733ba0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/779aea46cc3b6b1a62f989ee0c04e68803733ba0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/779aea46cc3b6b1a62f989ee0c04e68803733ba0"}], "stats": {"total": 1886, "additions": 1231, "deletions": 655}, "files": [{"sha": "68091bbcbbd1c46c13bfeb5e0b54b3464a39c1d2", "filename": "gcc/ChangeLog", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=828e70c1d7bb5c849a2df44aa832793c71833058", "patch": "@@ -1,3 +1,25 @@\n+2013-09-16  James Greenhalgh  <james.greenhalgh@arm.com>\n+\n+\t* config/aarch64/aarch64-simd-builtins.def (fma): New.\n+\t* config/aarch64/aarch64-simd.md\n+\t(aarch64_mla_elt<mode>): New.\n+\t(aarch64_mla_elt_<vswap_width_name><mode>): Likewise.\n+\t(aarch64_mls_elt<mode>): Likewise.\n+\t(aarch64_mls_elt_<vswap_width_name><mode>): Likewise.\n+\t(aarch64_fma4_elt<mode>): Likewise.\n+\t(aarch64_fma4_elt_<vswap_width_name><mode>): Likewise.\n+\t(aarch64_fma4_elt_to_128v2df): Likewise.\n+\t(aarch64_fma4_elt_to_64df): Likewise.\n+\t(fnma<mode>4): Likewise.\n+\t(aarch64_fnma4_elt<mode>): Likewise.\n+\t(aarch64_fnma4_elt_<vswap_width_name><mode>): Likewise.\n+\t(aarch64_fnma4_elt_to_128v2df): Likewise.\n+\t(aarch64_fnma4_elt_to_64df): Likewise.\n+\t* config/aarch64/iterators.md (VDQSF): New.\n+\t* config/aarch64/arm_neon.h\n+\t(vfm<as><sdq>_lane<q>_f<32, 64>): Convert to C implementation.\n+\t(vml<sa><q>_lane<q>_<fsu><16, 32, 64>): Likewise.\n+\n 2013-09-16  James Greenhalgh  <james.greenhalgh@arm.com>\n \n \t* config/aarch64/aarch64-simd.md (aarch64_mul3_elt<mode>): New."}, {"sha": "35897f3939556d7bb804d4b4ae692a300b103681", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=828e70c1d7bb5c849a2df44aa832793c71833058", "patch": "@@ -359,3 +359,6 @@\n   /* Implemented by aarch64_st1<VALL:mode>.  */\n   BUILTIN_VALL (STORE1, st1, 0)\n \n+  /* Implemented by fma<mode>4.  */\n+  BUILTIN_VDQF (TERNOP, fma, 4)\n+"}, {"sha": "f13cd5b7cdbdff95bbc378a76a6dd05de031487d", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 195, "deletions": 0, "changes": 195, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=828e70c1d7bb5c849a2df44aa832793c71833058", "patch": "@@ -1070,6 +1070,38 @@\n    (set_attr \"simd_mode\" \"<MODE>\")]\n )\n \n+(define_insn \"*aarch64_mla_elt<mode>\"\n+ [(set (match_operand:VDQHS 0 \"register_operand\" \"=w\")\n+       (plus:VDQHS\n+\t (mult:VDQHS\n+\t   (vec_duplicate:VDQHS\n+\t      (vec_select:<VEL>\n+\t\t(match_operand:VDQHS 1 \"register_operand\" \"<h_con>\")\n+\t\t  (parallel [(match_operand:SI 2 \"immediate_operand\")])))\n+\t   (match_operand:VDQHS 3 \"register_operand\" \"w\"))\n+\t (match_operand:VDQHS 4 \"register_operand\" \"0\")))]\n+ \"TARGET_SIMD\"\n+ \"mla\\t%0.<Vtype>, %3.<Vtype>, %1.<Vtype>[%2]\"\n+  [(set_attr \"simd_type\" \"simd_mla\")\n+   (set_attr \"simd_mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*aarch64_mla_elt_<vswap_width_name><mode>\"\n+ [(set (match_operand:VDQHS 0 \"register_operand\" \"=w\")\n+       (plus:VDQHS\n+\t (mult:VDQHS\n+\t   (vec_duplicate:VDQHS\n+\t      (vec_select:<VEL>\n+\t\t(match_operand:<VSWAP_WIDTH> 1 \"register_operand\" \"<h_con>\")\n+\t\t  (parallel [(match_operand:SI 2 \"immediate_operand\")])))\n+\t   (match_operand:VDQHS 3 \"register_operand\" \"w\"))\n+\t (match_operand:VDQHS 4 \"register_operand\" \"0\")))]\n+ \"TARGET_SIMD\"\n+ \"mla\\t%0.<Vtype>, %3.<Vtype>, %1.<Vtype>[%2]\"\n+  [(set_attr \"simd_type\" \"simd_mla\")\n+   (set_attr \"simd_mode\" \"<MODE>\")]\n+)\n+\n (define_insn \"aarch64_mls<mode>\"\n  [(set (match_operand:VQ_S 0 \"register_operand\" \"=w\")\n        (minus:VQ_S (match_operand:VQ_S 1 \"register_operand\" \"0\")\n@@ -1081,6 +1113,38 @@\n    (set_attr \"simd_mode\" \"<MODE>\")]\n )\n \n+(define_insn \"*aarch64_mls_elt<mode>\"\n+ [(set (match_operand:VDQHS 0 \"register_operand\" \"=w\")\n+       (minus:VDQHS\n+\t (match_operand:VDQHS 4 \"register_operand\" \"0\")\n+\t (mult:VDQHS\n+\t   (vec_duplicate:VDQHS\n+\t      (vec_select:<VEL>\n+\t\t(match_operand:VDQHS 1 \"register_operand\" \"<h_con>\")\n+\t\t  (parallel [(match_operand:SI 2 \"immediate_operand\")])))\n+\t   (match_operand:VDQHS 3 \"register_operand\" \"w\"))))]\n+ \"TARGET_SIMD\"\n+ \"mls\\t%0.<Vtype>, %3.<Vtype>, %1.<Vtype>[%2]\"\n+  [(set_attr \"simd_type\" \"simd_mla\")\n+   (set_attr \"simd_mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*aarch64_mls_elt_<vswap_width_name><mode>\"\n+ [(set (match_operand:VDQHS 0 \"register_operand\" \"=w\")\n+       (minus:VDQHS\n+\t (match_operand:VDQHS 4 \"register_operand\" \"0\")\n+\t (mult:VDQHS\n+\t   (vec_duplicate:VDQHS\n+\t      (vec_select:<VEL>\n+\t\t(match_operand:<VSWAP_WIDTH> 1 \"register_operand\" \"<h_con>\")\n+\t\t  (parallel [(match_operand:SI 2 \"immediate_operand\")])))\n+\t   (match_operand:VDQHS 3 \"register_operand\" \"w\"))))]\n+ \"TARGET_SIMD\"\n+ \"mls\\t%0.<Vtype>, %3.<Vtype>, %1.<Vtype>[%2]\"\n+  [(set_attr \"simd_type\" \"simd_mla\")\n+   (set_attr \"simd_mode\" \"<MODE>\")]\n+)\n+\n ;; Max/Min operations.\n (define_insn \"<su><maxmin><mode>3\"\n  [(set (match_operand:VQ_S 0 \"register_operand\" \"=w\")\n@@ -1483,6 +1547,137 @@\n    (set_attr \"simd_mode\" \"<MODE>\")]\n )\n \n+(define_insn \"*aarch64_fma4_elt<mode>\"\n+  [(set (match_operand:VDQF 0 \"register_operand\" \"=w\")\n+    (fma:VDQF\n+      (vec_duplicate:VDQF\n+\t(vec_select:<VEL>\n+\t  (match_operand:VDQF 1 \"register_operand\" \"<h_con>\")\n+\t  (parallel [(match_operand:SI 2 \"immediate_operand\")])))\n+      (match_operand:VDQF 3 \"register_operand\" \"w\")\n+      (match_operand:VDQF 4 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+  \"fmla\\\\t%0.<Vtype>, %3.<Vtype>, %1.<Vtype>[%2]\"\n+  [(set_attr \"simd_type\" \"simd_fmla_elt\")\n+   (set_attr \"simd_mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*aarch64_fma4_elt_<vswap_width_name><mode>\"\n+  [(set (match_operand:VDQSF 0 \"register_operand\" \"=w\")\n+    (fma:VDQSF\n+      (vec_duplicate:VDQSF\n+\t(vec_select:<VEL>\n+\t  (match_operand:<VSWAP_WIDTH> 1 \"register_operand\" \"<h_con>\")\n+\t  (parallel [(match_operand:SI 2 \"immediate_operand\")])))\n+      (match_operand:VDQSF 3 \"register_operand\" \"w\")\n+      (match_operand:VDQSF 4 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+  \"fmla\\\\t%0.<Vtype>, %3.<Vtype>, %1.<Vtype>[%2]\"\n+  [(set_attr \"simd_type\" \"simd_fmla_elt\")\n+   (set_attr \"simd_mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*aarch64_fma4_elt_to_128df\"\n+  [(set (match_operand:V2DF 0 \"register_operand\" \"=w\")\n+    (fma:V2DF\n+      (vec_duplicate:V2DF\n+\t  (match_operand:DF 1 \"register_operand\" \"w\"))\n+      (match_operand:V2DF 2 \"register_operand\" \"w\")\n+      (match_operand:V2DF 3 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+  \"fmla\\\\t%0.2d, %2.2d, %1.2d[0]\"\n+  [(set_attr \"simd_type\" \"simd_fmla_elt\")\n+   (set_attr \"simd_mode\" \"V2DF\")]\n+)\n+\n+(define_insn \"*aarch64_fma4_elt_to_64v2df\"\n+  [(set (match_operand:DF 0 \"register_operand\" \"=w\")\n+    (fma:DF\n+\t(vec_select:DF\n+\t  (match_operand:V2DF 1 \"register_operand\" \"w\")\n+\t  (parallel [(match_operand:SI 2 \"immediate_operand\")]))\n+      (match_operand:DF 3 \"register_operand\" \"w\")\n+      (match_operand:DF 4 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+  \"fmla\\\\t%0.2d, %3.2d, %1.2d[%2]\"\n+  [(set_attr \"simd_type\" \"simd_fmla_elt\")\n+   (set_attr \"simd_mode\" \"V2DF\")]\n+)\n+\n+(define_insn \"fnma<mode>4\"\n+  [(set (match_operand:VDQF 0 \"register_operand\" \"=w\")\n+\t(fma:VDQF\n+\t  (match_operand:VDQF 1 \"register_operand\" \"w\")\n+          (neg:VDQF\n+\t    (match_operand:VDQF 2 \"register_operand\" \"w\"))\n+\t  (match_operand:VDQF 3 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+ \"fmls\\\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>\"\n+  [(set_attr \"simd_type\" \"simd_fmla\")\n+   (set_attr \"simd_mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*aarch64_fnma4_elt<mode>\"\n+  [(set (match_operand:VDQF 0 \"register_operand\" \"=w\")\n+    (fma:VDQF\n+      (neg:VDQF\n+        (match_operand:VDQF 3 \"register_operand\" \"w\"))\n+      (vec_duplicate:VDQF\n+\t(vec_select:<VEL>\n+\t  (match_operand:VDQF 1 \"register_operand\" \"<h_con>\")\n+\t  (parallel [(match_operand:SI 2 \"immediate_operand\")])))\n+      (match_operand:VDQF 4 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+  \"fmls\\\\t%0.<Vtype>, %3.<Vtype>, %1.<Vtype>[%2]\"\n+  [(set_attr \"simd_type\" \"simd_fmla_elt\")\n+   (set_attr \"simd_mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*aarch64_fnma4_elt_<vswap_width_name><mode>\"\n+  [(set (match_operand:VDQSF 0 \"register_operand\" \"=w\")\n+    (fma:VDQSF\n+      (neg:VDQSF\n+        (match_operand:VDQSF 3 \"register_operand\" \"w\"))\n+      (vec_duplicate:VDQSF\n+\t(vec_select:<VEL>\n+\t  (match_operand:<VSWAP_WIDTH> 1 \"register_operand\" \"<h_con>\")\n+\t  (parallel [(match_operand:SI 2 \"immediate_operand\")])))\n+      (match_operand:VDQSF 4 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+  \"fmls\\\\t%0.<Vtype>, %3.<Vtype>, %1.<Vtype>[%2]\"\n+  [(set_attr \"simd_type\" \"simd_fmla_elt\")\n+   (set_attr \"simd_mode\" \"<MODE>\")]\n+)\n+\n+(define_insn \"*aarch64_fnma4_elt_to_128df\"\n+  [(set (match_operand:V2DF 0 \"register_operand\" \"=w\")\n+    (fma:V2DF\n+      (neg:V2DF\n+        (match_operand:V2DF 2 \"register_operand\" \"w\"))\n+      (vec_duplicate:V2DF\n+\t(match_operand:DF 1 \"register_operand\" \"w\"))\n+      (match_operand:V2DF 3 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+  \"fmls\\\\t%0.2d, %2.2d, %1.2d[0]\"\n+  [(set_attr \"simd_type\" \"simd_fmla_elt\")\n+   (set_attr \"simd_mode\" \"V2DF\")]\n+)\n+\n+(define_insn \"*aarch64_fnma4_elt_to_64v2df\"\n+  [(set (match_operand:DF 0 \"register_operand\" \"=w\")\n+    (fma:DF\n+      (vec_select:DF\n+\t(match_operand:V2DF 1 \"register_operand\" \"w\")\n+\t(parallel [(match_operand:SI 2 \"immediate_operand\")]))\n+      (neg:DF\n+        (match_operand:DF 3 \"register_operand\" \"w\"))\n+      (match_operand:DF 4 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+  \"fmls\\\\t%0.2d, %3.2d, %1.2d[%2]\"\n+  [(set_attr \"simd_type\" \"simd_fmla_elt\")\n+   (set_attr \"simd_mode\" \"V2DF\")]\n+)\n+\n ;; Vector versions of the floating-point frint patterns.\n ;; Expands to btrunc, ceil, floor, nearbyint, rint, round.\n (define_insn \"<frint_pattern><mode>2\""}, {"sha": "cb5860206a1812f347a77d4a6e06519f8c3a696f", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 595, "deletions": 655, "changes": 1250, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=828e70c1d7bb5c849a2df44aa832793c71833058", "patch": "@@ -6100,33 +6100,6 @@ vfma_f32 (float32x2_t a, float32x2_t b, float32x2_t c)\n   return result;\n }\n \n-#define vfma_lane_f32(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float32x2_t c_ = (c);                                            \\\n-       float32x2_t b_ = (b);                                            \\\n-       float32x2_t a_ = (a);                                            \\\n-       float32x2_t result;                                              \\\n-       __asm__ (\"fmla %0.2s,%2.2s,%3.s[%4]\"                             \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vfmad_lane_f64(a, b, c)                                         \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float64x2_t b_ = (b);                                            \\\n-       float64_t a_ = (a);                                              \\\n-       float64_t result;                                                \\\n-       __asm__ (\"fmla %d0,%d1,%2.d[%3]\"                                 \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vfmaq_f32 (float32x4_t a, float32x4_t b, float32x4_t c)\n {\n@@ -6149,47 +6122,6 @@ vfmaq_f64 (float64x2_t a, float64x2_t b, float64x2_t c)\n   return result;\n }\n \n-#define vfmaq_lane_f32(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float32x4_t c_ = (c);                                            \\\n-       float32x4_t b_ = (b);                                            \\\n-       float32x4_t a_ = (a);                                            \\\n-       float32x4_t result;                                              \\\n-       __asm__ (\"fmla %0.4s,%2.4s,%3.s[%4]\"                             \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vfmaq_lane_f64(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float64x2_t c_ = (c);                                            \\\n-       float64x2_t b_ = (b);                                            \\\n-       float64x2_t a_ = (a);                                            \\\n-       float64x2_t result;                                              \\\n-       __asm__ (\"fmla %0.2d,%2.2d,%3.d[%4]\"                             \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vfmas_lane_f32(a, b, c)                                         \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float32x4_t b_ = (b);                                            \\\n-       float32_t a_ = (a);                                              \\\n-       float32_t result;                                                \\\n-       __asm__ (\"fmla %s0,%s1,%2.s[%3]\"                                 \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vfma_n_f32 (float32x2_t a, float32x2_t b, float32_t c)\n {\n@@ -6234,19 +6166,6 @@ vfms_f32 (float32x2_t a, float32x2_t b, float32x2_t c)\n   return result;\n }\n \n-#define vfmsd_lane_f64(a, b, c)                                         \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float64x2_t b_ = (b);                                            \\\n-       float64_t a_ = (a);                                              \\\n-       float64_t result;                                                \\\n-       __asm__ (\"fmls %d0,%d1,%2.d[%3]\"                                 \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vfmsq_f32 (float32x4_t a, float32x4_t b, float32x4_t c)\n {\n@@ -6269,19 +6188,6 @@ vfmsq_f64 (float64x2_t a, float64x2_t b, float64x2_t c)\n   return result;\n }\n \n-#define vfmss_lane_f32(a, b, c)                                         \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float32x4_t b_ = (b);                                            \\\n-       float32_t a_ = (a);                                              \\\n-       float32_t result;                                                \\\n-       __asm__ (\"fmls %s0,%s1,%2.s[%3]\"                                 \\\n-                : \"=w\"(result)                                          \\\n-                : \"w\"(a_), \"w\"(b_), \"i\"(c)                              \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vget_high_f32 (float32x4_t a)\n {\n@@ -7122,133 +7028,6 @@ vld1q_dup_u64 (const uint64_t * a)\n        result;                                                          \\\n      })\n \n-#define vmla_lane_f32(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float32x2_t c_ = (c);                                            \\\n-       float32x2_t b_ = (b);                                            \\\n-       float32x2_t a_ = (a);                                            \\\n-       float32x2_t result;                                              \\\n-       float32x2_t t1;                                                  \\\n-       __asm__ (\"fmul %1.2s, %3.2s, %4.s[%5]; fadd %0.2s, %0.2s, %1.2s\" \\\n-                : \"=w\"(result), \"=w\"(t1)                                \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmla_lane_s16(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x4_t c_ = (c);                                              \\\n-       int16x4_t b_ = (b);                                              \\\n-       int16x4_t a_ = (a);                                              \\\n-       int16x4_t result;                                                \\\n-       __asm__ (\"mla %0.4h, %2.4h, %3.h[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmla_lane_s32(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x2_t c_ = (c);                                              \\\n-       int32x2_t b_ = (b);                                              \\\n-       int32x2_t a_ = (a);                                              \\\n-       int32x2_t result;                                                \\\n-       __asm__ (\"mla %0.2s, %2.2s, %3.s[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmla_lane_u16(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x4_t c_ = (c);                                             \\\n-       uint16x4_t b_ = (b);                                             \\\n-       uint16x4_t a_ = (a);                                             \\\n-       uint16x4_t result;                                               \\\n-       __asm__ (\"mla %0.4h, %2.4h, %3.h[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmla_lane_u32(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x2_t c_ = (c);                                             \\\n-       uint32x2_t b_ = (b);                                             \\\n-       uint32x2_t a_ = (a);                                             \\\n-       uint32x2_t result;                                               \\\n-       __asm__ (\"mla %0.2s, %2.2s, %3.s[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmla_laneq_s16(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x8_t c_ = (c);                                              \\\n-       int16x4_t b_ = (b);                                              \\\n-       int16x4_t a_ = (a);                                              \\\n-       int16x4_t result;                                                \\\n-       __asm__ (\"mla %0.4h, %2.4h, %3.h[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmla_laneq_s32(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x4_t c_ = (c);                                              \\\n-       int32x2_t b_ = (b);                                              \\\n-       int32x2_t a_ = (a);                                              \\\n-       int32x2_t result;                                                \\\n-       __asm__ (\"mla %0.2s, %2.2s, %3.s[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmla_laneq_u16(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x8_t c_ = (c);                                             \\\n-       uint16x4_t b_ = (b);                                             \\\n-       uint16x4_t a_ = (a);                                             \\\n-       uint16x4_t result;                                               \\\n-       __asm__ (\"mla %0.4h, %2.4h, %3.h[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmla_laneq_u32(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x4_t c_ = (c);                                             \\\n-       uint32x2_t b_ = (b);                                             \\\n-       uint32x2_t a_ = (a);                                             \\\n-       uint32x2_t result;                                               \\\n-       __asm__ (\"mla %0.2s, %2.2s, %3.s[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vmla_n_f32 (float32x2_t a, float32x2_t b, float32_t c)\n {\n@@ -7815,133 +7594,6 @@ vmlal_u32 (uint64x2_t a, uint32x2_t b, uint32x2_t c)\n   return result;\n }\n \n-#define vmlaq_lane_f32(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float32x4_t c_ = (c);                                            \\\n-       float32x4_t b_ = (b);                                            \\\n-       float32x4_t a_ = (a);                                            \\\n-       float32x4_t result;                                              \\\n-       float32x4_t t1;                                                  \\\n-       __asm__ (\"fmul %1.4s, %3.4s, %4.s[%5]; fadd %0.4s, %0.4s, %1.4s\" \\\n-                : \"=w\"(result), \"=w\"(t1)                                \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlaq_lane_s16(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x8_t c_ = (c);                                              \\\n-       int16x8_t b_ = (b);                                              \\\n-       int16x8_t a_ = (a);                                              \\\n-       int16x8_t result;                                                \\\n-       __asm__ (\"mla %0.8h, %2.8h, %3.h[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlaq_lane_s32(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x4_t c_ = (c);                                              \\\n-       int32x4_t b_ = (b);                                              \\\n-       int32x4_t a_ = (a);                                              \\\n-       int32x4_t result;                                                \\\n-       __asm__ (\"mla %0.4s, %2.4s, %3.s[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlaq_lane_u16(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x8_t c_ = (c);                                             \\\n-       uint16x8_t b_ = (b);                                             \\\n-       uint16x8_t a_ = (a);                                             \\\n-       uint16x8_t result;                                               \\\n-       __asm__ (\"mla %0.8h, %2.8h, %3.h[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlaq_lane_u32(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x4_t c_ = (c);                                             \\\n-       uint32x4_t b_ = (b);                                             \\\n-       uint32x4_t a_ = (a);                                             \\\n-       uint32x4_t result;                                               \\\n-       __asm__ (\"mla %0.4s, %2.4s, %3.s[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlaq_laneq_s16(a, b, c, d)                                     \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x8_t c_ = (c);                                              \\\n-       int16x8_t b_ = (b);                                              \\\n-       int16x8_t a_ = (a);                                              \\\n-       int16x8_t result;                                                \\\n-       __asm__ (\"mla %0.8h, %2.8h, %3.h[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlaq_laneq_s32(a, b, c, d)                                     \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x4_t c_ = (c);                                              \\\n-       int32x4_t b_ = (b);                                              \\\n-       int32x4_t a_ = (a);                                              \\\n-       int32x4_t result;                                                \\\n-       __asm__ (\"mla %0.4s, %2.4s, %3.s[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlaq_laneq_u16(a, b, c, d)                                     \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x8_t c_ = (c);                                             \\\n-       uint16x8_t b_ = (b);                                             \\\n-       uint16x8_t a_ = (a);                                             \\\n-       uint16x8_t result;                                               \\\n-       __asm__ (\"mla %0.8h, %2.8h, %3.h[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlaq_laneq_u32(a, b, c, d)                                     \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x4_t c_ = (c);                                             \\\n-       uint32x4_t b_ = (b);                                             \\\n-       uint32x4_t a_ = (a);                                             \\\n-       uint32x4_t result;                                               \\\n-       __asm__ (\"mla %0.4s, %2.4s, %3.s[%4]\"                            \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vmlaq_n_f32 (float32x4_t a, float32x4_t b, float32_t c)\n {\n@@ -8046,106 +7698,35 @@ vmlaq_s32 (int32x4_t a, int32x4_t b, int32x4_t c)\n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n vmlaq_u8 (uint8x16_t a, uint8x16_t b, uint8x16_t c)\n {\n-  uint8x16_t result;\n-  __asm__ (\"mla %0.16b, %2.16b, %3.16b\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vmlaq_u16 (uint16x8_t a, uint16x8_t b, uint16x8_t c)\n-{\n-  uint16x8_t result;\n-  __asm__ (\"mla %0.8h, %2.8h, %3.8h\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vmlaq_u32 (uint32x4_t a, uint32x4_t b, uint32x4_t c)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"mla %0.4s, %2.4s, %3.4s\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-#define vmls_lane_f32(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float32x2_t c_ = (c);                                            \\\n-       float32x2_t b_ = (b);                                            \\\n-       float32x2_t a_ = (a);                                            \\\n-       float32x2_t result;                                              \\\n-       float32x2_t t1;                                                  \\\n-       __asm__ (\"fmul %1.2s, %3.2s, %4.s[%5]; fsub %0.2s, %0.2s, %1.2s\" \\\n-                : \"=w\"(result), \"=w\"(t1)                                \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmls_lane_s16(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x4_t c_ = (c);                                              \\\n-       int16x4_t b_ = (b);                                              \\\n-       int16x4_t a_ = (a);                                              \\\n-       int16x4_t result;                                                \\\n-       __asm__ (\"mls %0.4h,%2.4h,%3.h[%4]\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmls_lane_s32(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x2_t c_ = (c);                                              \\\n-       int32x2_t b_ = (b);                                              \\\n-       int32x2_t a_ = (a);                                              \\\n-       int32x2_t result;                                                \\\n-       __asm__ (\"mls %0.2s,%2.2s,%3.s[%4]\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmls_lane_u16(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x4_t c_ = (c);                                             \\\n-       uint16x4_t b_ = (b);                                             \\\n-       uint16x4_t a_ = (a);                                             \\\n-       uint16x4_t result;                                               \\\n-       __asm__ (\"mls %0.4h,%2.4h,%3.h[%4]\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+  uint8x16_t result;\n+  __asm__ (\"mla %0.16b, %2.16b, %3.16b\"\n+           : \"=w\"(result)\n+           : \"0\"(a), \"w\"(b), \"w\"(c)\n+           : /* No clobbers */);\n+  return result;\n+}\n \n-#define vmls_lane_u32(a, b, c, d)                                       \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x2_t c_ = (c);                                             \\\n-       uint32x2_t b_ = (b);                                             \\\n-       uint32x2_t a_ = (a);                                             \\\n-       uint32x2_t result;                                               \\\n-       __asm__ (\"mls %0.2s,%2.2s,%3.s[%4]\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vmlaq_u16 (uint16x8_t a, uint16x8_t b, uint16x8_t c)\n+{\n+  uint16x8_t result;\n+  __asm__ (\"mla %0.8h, %2.8h, %3.8h\"\n+           : \"=w\"(result)\n+           : \"0\"(a), \"w\"(b), \"w\"(c)\n+           : /* No clobbers */);\n+  return result;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vmlaq_u32 (uint32x4_t a, uint32x4_t b, uint32x4_t c)\n+{\n+  uint32x4_t result;\n+  __asm__ (\"mla %0.4s, %2.4s, %3.4s\"\n+           : \"=w\"(result)\n+           : \"0\"(a), \"w\"(b), \"w\"(c)\n+           : /* No clobbers */);\n+  return result;\n+}\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vmls_n_f32 (float32x2_t a, float32x2_t b, float32_t c)\n@@ -8713,148 +8294,6 @@ vmlsl_u32 (uint64x2_t a, uint32x2_t b, uint32x2_t c)\n   return result;\n }\n \n-#define vmlsq_lane_f32(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       float32x4_t c_ = (c);                                            \\\n-       float32x4_t b_ = (b);                                            \\\n-       float32x4_t a_ = (a);                                            \\\n-       float32x4_t result;                                              \\\n-       float32x4_t t1;                                                  \\\n-       __asm__ (\"fmul %1.4s, %3.4s, %4.s[%5]; fsub %0.4s, %0.4s, %1.4s\" \\\n-                : \"=w\"(result), \"=w\"(t1)                                \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlsq_lane_s16(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x8_t c_ = (c);                                              \\\n-       int16x8_t b_ = (b);                                              \\\n-       int16x8_t a_ = (a);                                              \\\n-       int16x8_t result;                                                \\\n-       __asm__ (\"mls %0.8h,%2.8h,%3.h[%4]\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlsq_lane_s32(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x4_t c_ = (c);                                              \\\n-       int32x4_t b_ = (b);                                              \\\n-       int32x4_t a_ = (a);                                              \\\n-       int32x4_t result;                                                \\\n-       __asm__ (\"mls %0.4s,%2.4s,%3.s[%4]\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlsq_lane_u16(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x8_t c_ = (c);                                             \\\n-       uint16x8_t b_ = (b);                                             \\\n-       uint16x8_t a_ = (a);                                             \\\n-       uint16x8_t result;                                               \\\n-       __asm__ (\"mls %0.8h,%2.8h,%3.h[%4]\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlsq_lane_u32(a, b, c, d)                                      \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x4_t c_ = (c);                                             \\\n-       uint32x4_t b_ = (b);                                             \\\n-       uint32x4_t a_ = (a);                                             \\\n-       uint32x4_t result;                                               \\\n-       __asm__ (\"mls %0.4s,%2.4s,%3.s[%4]\"                              \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-#define vmlsq_laneq_f32(__a, __b, __c, __d)\t\t\t\t\\\n-  __extension__\t\t\t\t\t\t\t\t\\\n-    ({\t\t\t\t\t\t\t\t\t\\\n-       float32x4_t __c_ = (__c);\t\t\t\t\t\\\n-       float32x4_t __b_ = (__b);\t\t\t\t\t\\\n-       float32x4_t __a_ = (__a);\t\t\t\t\t\\\n-       float32x4_t __result;\t\t\t\t\t\t\\\n-       float32x4_t __t1;\t\t\t\t\t\t\\\n-       __asm__ (\"fmul %1.4s, %3.4s, %4.s[%5]; fsub %0.4s, %0.4s, %1.4s\"\t\\\n-                : \"=w\"(__result), \"=w\"(__t1)\t\t\t\t\\\n-                : \"0\"(__a_), \"w\"(__b_), \"w\"(__c_), \"i\"(__d)\t\t\\\n-                : /* No clobbers */);\t\t\t\t\t\\\n-       __result;\t\t\t\t\t\t\t\\\n-     })\n-\n-#define vmlsq_laneq_s16(__a, __b, __c, __d)\t\t\t\t\\\n-  __extension__\t\t\t\t\t\t\t\t\\\n-    ({\t\t\t\t\t\t\t\t\t\\\n-       int16x8_t __c_ = (__c);\t\t\t\t\t\t\\\n-       int16x8_t __b_ = (__b);\t\t\t\t\t\t\\\n-       int16x8_t __a_ = (__a);\t\t\t\t\t\t\\\n-       int16x8_t __result;\t\t\t\t\t\t\\\n-       __asm__ (\"mls %0.8h, %2.8h, %3.h[%4]\"\t\t\t\t\\\n-                : \"=w\"(__result)\t\t\t\t\t\\\n-                : \"0\"(__a_), \"w\"(__b_), \"x\"(__c_), \"i\"(__d)\t\t\\\n-                : /* No clobbers */);\t\t\t\t\t\\\n-       __result;\t\t\t\t\t\t\t\\\n-     })\n-\n-#define vmlsq_laneq_s32(__a, __b, __c, __d)\t\t\t\t\\\n-  __extension__\t\t\t\t\t\t\t\t\\\n-    ({\t\t\t\t\t\t\t\t\t\\\n-       int32x4_t __c_ = (__c);\t\t\t\t\t\t\\\n-       int32x4_t __b_ = (__b);\t\t\t\t\t\t\\\n-       int32x4_t __a_ = (__a);\t\t\t\t\t\t\\\n-       int32x4_t __result;\t\t\t\t\t\t\\\n-       __asm__ (\"mls %0.4s, %2.4s, %3.s[%4]\"\t\t\t\t\\\n-                : \"=w\"(__result)\t\t\t\t\t\\\n-                : \"0\"(__a_), \"w\"(__b_), \"w\"(__c_), \"i\"(__d)\t\t\\\n-                : /* No clobbers */);\t\t\t\t\t\\\n-       __result;\t\t\t\t\t\t\t\\\n-     })\n-\n-#define vmlsq_laneq_u16(__a, __b, __c, __d)\t\t\t\t\\\n-  __extension__\t\t\t\t\t\t\t\t\\\n-    ({\t\t\t\t\t\t\t\t\t\\\n-       uint16x8_t __c_ = (__c);\t\t\t\t\t\t\\\n-       uint16x8_t __b_ = (__b);\t\t\t\t\t\t\\\n-       uint16x8_t __a_ = (__a);\t\t\t\t\t\t\\\n-       uint16x8_t __result;\t\t\t\t\t\t\\\n-       __asm__ (\"mls %0.8h, %2.8h, %3.h[%4]\"\t\t\t\t\\\n-                : \"=w\"(__result)\t\t\t\t\t\\\n-                : \"0\"(__a_), \"w\"(__b_), \"x\"(__c_), \"i\"(__d)\t\t\\\n-                : /* No clobbers */);\t\t\t\t\t\\\n-       __result;\t\t\t\t\t\t\t\\\n-     })\n-\n-#define vmlsq_laneq_u32(__a, __b, __c, __d)\t\t\t\t\\\n-  __extension__\t\t\t\t\t\t\t\t\\\n-    ({\t\t\t\t\t\t\t\t\t\\\n-       uint32x4_t __c_ = (__c);\t\t\t\t\t\t\\\n-       uint32x4_t __b_ = (__b);\t\t\t\t\t\t\\\n-       uint32x4_t __a_ = (__a);\t\t\t\t\t\t\\\n-       uint32x4_t __result;\t\t\t\t\t\t\\\n-       __asm__ (\"mls %0.4s, %2.4s, %3.s[%4]\"\t\t\t\t\\\n-                : \"=w\"(__result)\t\t\t\t\t\\\n-                : \"0\"(__a_), \"w\"(__b_), \"w\"(__c_), \"i\"(__d)\t\t\\\n-                : /* No clobbers */);\t\t\t\t\t\\\n-       __result;\t\t\t\t\t\t\t\\\n-     })\n-\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vmlsq_n_f32 (float32x4_t a, float32x4_t b, float32_t c)\n {\n@@ -19488,130 +18927,334 @@ vduph_lane_p16 (poly16x4_t __a, const int __b)\n   return __aarch64_vget_lane_p16 (__a, __b);\n }\n \n-__extension__ static __inline int16_t __attribute__ ((__always_inline__))\n-vduph_lane_s16 (int16x4_t __a, const int __b)\n+__extension__ static __inline int16_t __attribute__ ((__always_inline__))\n+vduph_lane_s16 (int16x4_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_s16 (__a, __b);\n+}\n+\n+__extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n+vduph_lane_u16 (uint16x4_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_u16 (__a, __b);\n+}\n+\n+/* vdups_lane  */\n+__extension__ static __inline float32_t __attribute__ ((__always_inline__))\n+vdups_lane_f32 (float32x2_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_f32 (__a, __b);\n+}\n+\n+__extension__ static __inline int32_t __attribute__ ((__always_inline__))\n+vdups_lane_s32 (int32x2_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_s32 (__a, __b);\n+}\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vdups_lane_u32 (uint32x2_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_u32 (__a, __b);\n+}\n+\n+/* vdupd_lane  */\n+__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n+vdupd_lane_f64 (float64x1_t __a, const int __attribute__ ((unused)) __b)\n+{\n+  return __a;\n+}\n+\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vdupd_lane_s64 (int64x1_t __a, const int __attribute__ ((unused)) __b)\n+{\n+  return __a;\n+}\n+\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vdupd_lane_u64 (uint64x1_t __a, const int __attribute__ ((unused)) __b)\n+{\n+  return __a;\n+}\n+\n+/* vdupb_laneq  */\n+__extension__ static __inline poly8_t __attribute__ ((__always_inline__))\n+vdupb_laneq_p8 (poly8x16_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_p8 (__a, __b);\n+}\n+\n+__extension__ static __inline int8_t __attribute__ ((__always_inline__))\n+vdupb_laneq_s8 (int8x16_t __a, const int __attribute__ ((unused)) __b)\n+{\n+  return __aarch64_vgetq_lane_s8 (__a, __b);\n+}\n+\n+__extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n+vdupb_laneq_u8 (uint8x16_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_u8 (__a, __b);\n+}\n+\n+/* vduph_laneq  */\n+__extension__ static __inline poly16_t __attribute__ ((__always_inline__))\n+vduph_laneq_p16 (poly16x8_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_p16 (__a, __b);\n+}\n+\n+__extension__ static __inline int16_t __attribute__ ((__always_inline__))\n+vduph_laneq_s16 (int16x8_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_s16 (__a, __b);\n+}\n+\n+__extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n+vduph_laneq_u16 (uint16x8_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_u16 (__a, __b);\n+}\n+\n+/* vdups_laneq  */\n+__extension__ static __inline float32_t __attribute__ ((__always_inline__))\n+vdups_laneq_f32 (float32x4_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_f32 (__a, __b);\n+}\n+\n+__extension__ static __inline int32_t __attribute__ ((__always_inline__))\n+vdups_laneq_s32 (int32x4_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_s32 (__a, __b);\n+}\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vdups_laneq_u32 (uint32x4_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_u32 (__a, __b);\n+}\n+\n+/* vdupd_laneq  */\n+__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n+vdupd_laneq_f64 (float64x2_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_f64 (__a, __b);\n+}\n+\n+__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n+vdupd_laneq_s64 (int64x2_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_s64 (__a, __b);\n+}\n+\n+__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n+vdupd_laneq_u64 (uint64x2_t __a, const int __b)\n+{\n+  return __aarch64_vgetq_lane_u64 (__a, __b);\n+}\n+\n+/* vfma_lane  */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vfma_lane_f32 (float32x2_t __a, float32x2_t __b,\n+\t       float32x2_t __c, const int __lane)\n+{\n+  return __builtin_aarch64_fmav2sf (__b,\n+\t\t\t\t    __aarch64_vdup_lane_f32 (__c, __lane),\n+\t\t\t\t    __a);\n+}\n+\n+__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n+vfma_lane_f64 (float64_t __a, float64_t __b,\n+\t       float64_t __c, const int __lane)\n+{\n+  return __builtin_fma (__b, __c, __a);\n+}\n+\n+__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n+vfmad_lane_f64 (float64_t __a, float64_t __b,\n+\t        float64_t __c, const int __lane)\n+{\n+  return __builtin_fma (__b, __c, __a);\n+}\n+\n+__extension__ static __inline float32_t __attribute__ ((__always_inline__))\n+vfmas_lane_f32 (float32_t __a, float32_t __b,\n+\t        float32x2_t __c, const int __lane)\n+{\n+  return __builtin_fmaf (__b, __aarch64_vget_lane_f32 (__c, __lane), __a);\n+}\n+\n+/* vfma_laneq  */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vfma_laneq_f32 (float32x2_t __a, float32x2_t __b,\n+\t        float32x4_t __c, const int __lane)\n+{\n+  return __builtin_aarch64_fmav2sf (__b,\n+\t\t\t\t    __aarch64_vdup_laneq_f32 (__c, __lane),\n+\t\t\t\t    __a);\n+}\n+\n+__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n+vfma_laneq_f64 (float64_t __a, float64_t __b,\n+\t        float64x2_t __c, const int __lane)\n {\n-  return __aarch64_vget_lane_s16 (__a, __b);\n+  return __builtin_fma (__b, __aarch64_vgetq_lane_f64 (__c, __lane), __a);\n }\n \n-__extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n-vduph_lane_u16 (uint16x4_t __a, const int __b)\n+__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n+vfmad_laneq_f64 (float64_t __a, float64_t __b,\n+\t         float64x2_t __c, const int __lane)\n {\n-  return __aarch64_vget_lane_u16 (__a, __b);\n+  return __builtin_fma (__b, __aarch64_vgetq_lane_f64 (__c, __lane), __a);\n }\n \n-/* vdups_lane  */\n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n-vdups_lane_f32 (float32x2_t __a, const int __b)\n+vfmas_laneq_f32 (float32_t __a, float32_t __b,\n+\t\t float32x4_t __c, const int __lane)\n {\n-  return __aarch64_vget_lane_f32 (__a, __b);\n+  return __builtin_fmaf (__b, __aarch64_vgetq_lane_f32 (__c, __lane), __a);\n }\n \n-__extension__ static __inline int32_t __attribute__ ((__always_inline__))\n-vdups_lane_s32 (int32x2_t __a, const int __b)\n+/* vfmaq_lane  */\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vfmaq_lane_f32 (float32x4_t __a, float32x4_t __b,\n+\t        float32x2_t __c, const int __lane)\n {\n-  return __aarch64_vget_lane_s32 (__a, __b);\n+  return __builtin_aarch64_fmav4sf (__b,\n+\t\t\t\t    __aarch64_vdupq_lane_f32 (__c, __lane),\n+\t\t\t\t    __a);\n }\n \n-__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n-vdups_lane_u32 (uint32x2_t __a, const int __b)\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vfmaq_lane_f64 (float64x2_t __a, float64x2_t __b,\n+\t        float64_t __c, const int __lane)\n {\n-  return __aarch64_vget_lane_u32 (__a, __b);\n+  return __builtin_aarch64_fmav2df (__b, vdupq_n_f64 (__c), __a);\n }\n \n-/* vdupd_lane  */\n-__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n-vdupd_lane_f64 (float64x1_t __a, const int __attribute__ ((unused)) __b)\n+/* vfmaq_laneq  */\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vfmaq_laneq_f32 (float32x4_t __a, float32x4_t __b,\n+\t         float32x4_t __c, const int __lane)\n {\n-  return __a;\n+  return __builtin_aarch64_fmav4sf (__b,\n+\t\t\t\t    __aarch64_vdupq_laneq_f32 (__c, __lane),\n+\t\t\t\t    __a);\n }\n \n-__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n-vdupd_lane_s64 (int64x1_t __a, const int __attribute__ ((unused)) __b)\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vfmaq_laneq_f64 (float64x2_t __a, float64x2_t __b,\n+\t         float64x2_t __c, const int __lane)\n {\n-  return __a;\n+  return __builtin_aarch64_fmav2df (__b,\n+\t\t\t\t    __aarch64_vdupq_laneq_f64 (__c, __lane),\n+\t\t\t\t    __a);\n }\n \n-__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n-vdupd_lane_u64 (uint64x1_t __a, const int __attribute__ ((unused)) __b)\n+/* vfms_lane  */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vfms_lane_f32 (float32x2_t __a, float32x2_t __b,\n+\t       float32x2_t __c, const int __lane)\n {\n-  return __a;\n+  return __builtin_aarch64_fmav2sf (-__b,\n+\t\t\t\t    __aarch64_vdup_lane_f32 (__c, __lane),\n+\t\t\t\t    __a);\n }\n \n-/* vdupb_laneq  */\n-__extension__ static __inline poly8_t __attribute__ ((__always_inline__))\n-vdupb_laneq_p8 (poly8x16_t __a, const int __b)\n+__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n+vfms_lane_f64 (float64_t __a, float64_t __b,\n+\t       float64_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_p8 (__a, __b);\n+  return __builtin_fma (-__b, __c, __a);\n }\n \n-__extension__ static __inline int8_t __attribute__ ((__always_inline__))\n-vdupb_laneq_s8 (int8x16_t __a, const int __attribute__ ((unused)) __b)\n+__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n+vfmsd_lane_f64 (float64_t __a, float64_t __b,\n+\t        float64_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_s8 (__a, __b);\n+  return __builtin_fma (-__b, __c, __a);\n }\n \n-__extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n-vdupb_laneq_u8 (uint8x16_t __a, const int __b)\n+__extension__ static __inline float32_t __attribute__ ((__always_inline__))\n+vfmss_lane_f32 (float32_t __a, float32_t __b,\n+\t        float32x2_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_u8 (__a, __b);\n+  return __builtin_fmaf (-__b, __aarch64_vget_lane_f32 (__c, __lane), __a);\n }\n \n-/* vduph_laneq  */\n-__extension__ static __inline poly16_t __attribute__ ((__always_inline__))\n-vduph_laneq_p16 (poly16x8_t __a, const int __b)\n+/* vfms_laneq  */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vfms_laneq_f32 (float32x2_t __a, float32x2_t __b,\n+\t        float32x4_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_p16 (__a, __b);\n+  return __builtin_aarch64_fmav2sf (-__b,\n+\t\t\t\t    __aarch64_vdup_laneq_f32 (__c, __lane),\n+\t\t\t\t    __a);\n }\n \n-__extension__ static __inline int16_t __attribute__ ((__always_inline__))\n-vduph_laneq_s16 (int16x8_t __a, const int __b)\n+__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n+vfms_laneq_f64 (float64_t __a, float64_t __b,\n+\t        float64x2_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_s16 (__a, __b);\n+  return __builtin_fma (-__b, __aarch64_vgetq_lane_f64 (__c, __lane), __a);\n }\n \n-__extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n-vduph_laneq_u16 (uint16x8_t __a, const int __b)\n+__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n+vfmsd_laneq_f64 (float64_t __a, float64_t __b,\n+\t         float64x2_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_u16 (__a, __b);\n+  return __builtin_fma (-__b, __aarch64_vgetq_lane_f64 (__c, __lane), __a);\n }\n \n-/* vdups_laneq  */\n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n-vdups_laneq_f32 (float32x4_t __a, const int __b)\n+vfmss_laneq_f32 (float32_t __a, float32_t __b,\n+\t\t float32x4_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_f32 (__a, __b);\n+  return __builtin_fmaf (-__b, __aarch64_vgetq_lane_f32 (__c, __lane), __a);\n }\n \n-__extension__ static __inline int32_t __attribute__ ((__always_inline__))\n-vdups_laneq_s32 (int32x4_t __a, const int __b)\n-{\n-  return __aarch64_vgetq_lane_s32 (__a, __b);\n-}\n+/* vfmsq_lane  */\n \n-__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n-vdups_laneq_u32 (uint32x4_t __a, const int __b)\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vfmsq_lane_f32 (float32x4_t __a, float32x4_t __b,\n+\t        float32x2_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_u32 (__a, __b);\n+  return __builtin_aarch64_fmav4sf (-__b,\n+\t\t\t\t    __aarch64_vdupq_lane_f32 (__c, __lane),\n+\t\t\t\t    __a);\n }\n \n-/* vdupd_laneq  */\n-__extension__ static __inline float64_t __attribute__ ((__always_inline__))\n-vdupd_laneq_f64 (float64x2_t __a, const int __b)\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vfmsq_lane_f64 (float64x2_t __a, float64x2_t __b,\n+\t        float64_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_f64 (__a, __b);\n+  return __builtin_aarch64_fmav2df (-__b, vdupq_n_f64 (__c), __a);\n }\n \n-__extension__ static __inline int64_t __attribute__ ((__always_inline__))\n-vdupd_laneq_s64 (int64x2_t __a, const int __b)\n+/* vfmsq_laneq  */\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vfmsq_laneq_f32 (float32x4_t __a, float32x4_t __b,\n+\t         float32x4_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_s64 (__a, __b);\n+  return __builtin_aarch64_fmav4sf (-__b,\n+\t\t\t\t    __aarch64_vdupq_laneq_f32 (__c, __lane),\n+\t\t\t\t    __a);\n }\n \n-__extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n-vdupd_laneq_u64 (uint64x2_t __a, const int __b)\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vfmsq_laneq_f64 (float64x2_t __a, float64x2_t __b,\n+\t         float64x2_t __c, const int __lane)\n {\n-  return __aarch64_vgetq_lane_u64 (__a, __b);\n+  return __builtin_aarch64_fmav2df (-__b,\n+\t\t\t\t    __aarch64_vdupq_laneq_f64 (__c, __lane),\n+\t\t\t\t    __a);\n }\n \n /* vld1 */\n@@ -21131,6 +20774,156 @@ vmlaq_f64 (float64x2_t a, float64x2_t b, float64x2_t c)\n   return a + b * c;\n }\n \n+/* vmla_lane  */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vmla_lane_f32 (float32x2_t __a, float32x2_t __b,\n+\t       float32x2_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vget_lane_f32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vmla_lane_s16 (int16x4_t __a, int16x4_t __b,\n+\t\tint16x4_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vget_lane_s16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vmla_lane_s32 (int32x2_t __a, int32x2_t __b,\n+\t\tint32x2_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vget_lane_s32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vmla_lane_u16 (uint16x4_t __a, uint16x4_t __b,\n+\t\tuint16x4_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vget_lane_u16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vmla_lane_u32 (uint32x2_t __a, uint32x2_t __b,\n+\t       uint32x2_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vget_lane_u32 (__c, __lane)));\n+}\n+\n+/* vmla_laneq  */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vmla_laneq_f32 (float32x2_t __a, float32x2_t __b,\n+\t        float32x4_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vgetq_lane_f32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vmla_laneq_s16 (int16x4_t __a, int16x4_t __b,\n+\t\tint16x8_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vgetq_lane_s16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vmla_laneq_s32 (int32x2_t __a, int32x2_t __b,\n+\t\tint32x4_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vgetq_lane_s32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vmla_laneq_u16 (uint16x4_t __a, uint16x4_t __b,\n+\t\tuint16x8_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vgetq_lane_u16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vmla_laneq_u32 (uint32x2_t __a, uint32x2_t __b,\n+\t\tuint32x4_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vgetq_lane_u32 (__c, __lane)));\n+}\n+\n+/* vmlaq_lane  */\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vmlaq_lane_f32 (float32x4_t __a, float32x4_t __b,\n+\t\tfloat32x2_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vget_lane_f32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vmlaq_lane_s16 (int16x8_t __a, int16x8_t __b,\n+\t\tint16x4_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vget_lane_s16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vmlaq_lane_s32 (int32x4_t __a, int32x4_t __b,\n+\t\tint32x2_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vget_lane_s32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vmlaq_lane_u16 (uint16x8_t __a, uint16x8_t __b,\n+\t\tuint16x4_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vget_lane_u16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vmlaq_lane_u32 (uint32x4_t __a, uint32x4_t __b,\n+\t\tuint32x2_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vget_lane_u32 (__c, __lane)));\n+}\n+\n+  /* vmlaq_laneq  */\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vmlaq_laneq_f32 (float32x4_t __a, float32x4_t __b,\n+\t\t float32x4_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vgetq_lane_f32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vmlaq_laneq_s16 (int16x8_t __a, int16x8_t __b,\n+\t\tint16x8_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vgetq_lane_s16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vmlaq_laneq_s32 (int32x4_t __a, int32x4_t __b,\n+\t\tint32x4_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vgetq_lane_s32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vmlaq_laneq_u16 (uint16x8_t __a, uint16x8_t __b,\n+\t\tuint16x8_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vgetq_lane_u16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vmlaq_laneq_u32 (uint32x4_t __a, uint32x4_t __b,\n+\t\tuint32x4_t __c, const int __lane)\n+{\n+  return (__a + (__b * __aarch64_vgetq_lane_u32 (__c, __lane)));\n+}\n+\n+/* vmls  */\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vmls_f32 (float32x2_t a, float32x2_t b, float32x2_t c)\n {\n@@ -21149,6 +20942,153 @@ vmlsq_f64 (float64x2_t a, float64x2_t b, float64x2_t c)\n   return a - b * c;\n }\n \n+/* vmls_lane  */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vmls_lane_f32 (float32x2_t __a, float32x2_t __b,\n+\t       float32x2_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vget_lane_f32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vmls_lane_s16 (int16x4_t __a, int16x4_t __b,\n+\t\tint16x4_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vget_lane_s16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vmls_lane_s32 (int32x2_t __a, int32x2_t __b,\n+\t\tint32x2_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vget_lane_s32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vmls_lane_u16 (uint16x4_t __a, uint16x4_t __b,\n+\t\tuint16x4_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vget_lane_u16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vmls_lane_u32 (uint32x2_t __a, uint32x2_t __b,\n+\t       uint32x2_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vget_lane_u32 (__c, __lane)));\n+}\n+\n+/* vmls_laneq  */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vmls_laneq_f32 (float32x2_t __a, float32x2_t __b,\n+\t       float32x4_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vgetq_lane_f32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vmls_laneq_s16 (int16x4_t __a, int16x4_t __b,\n+\t\tint16x8_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vgetq_lane_s16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vmls_laneq_s32 (int32x2_t __a, int32x2_t __b,\n+\t\tint32x4_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vgetq_lane_s32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vmls_laneq_u16 (uint16x4_t __a, uint16x4_t __b,\n+\t\tuint16x8_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vgetq_lane_u16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vmls_laneq_u32 (uint32x2_t __a, uint32x2_t __b,\n+\t\tuint32x4_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vgetq_lane_u32 (__c, __lane)));\n+}\n+\n+/* vmlsq_lane  */\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vmlsq_lane_f32 (float32x4_t __a, float32x4_t __b,\n+\t\tfloat32x2_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vget_lane_f32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vmlsq_lane_s16 (int16x8_t __a, int16x8_t __b,\n+\t\tint16x4_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vget_lane_s16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vmlsq_lane_s32 (int32x4_t __a, int32x4_t __b,\n+\t\tint32x2_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vget_lane_s32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vmlsq_lane_u16 (uint16x8_t __a, uint16x8_t __b,\n+\t\tuint16x4_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vget_lane_u16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vmlsq_lane_u32 (uint32x4_t __a, uint32x4_t __b,\n+\t\tuint32x2_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vget_lane_u32 (__c, __lane)));\n+}\n+\n+  /* vmlsq_laneq  */\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vmlsq_laneq_f32 (float32x4_t __a, float32x4_t __b,\n+\t\tfloat32x4_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vgetq_lane_f32 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vmlsq_laneq_s16 (int16x8_t __a, int16x8_t __b,\n+\t\tint16x8_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vgetq_lane_s16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vmlsq_laneq_s32 (int32x4_t __a, int32x4_t __b,\n+\t\tint32x4_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vgetq_lane_s32 (__c, __lane)));\n+}\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vmlsq_laneq_u16 (uint16x8_t __a, uint16x8_t __b,\n+\t\tuint16x8_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vgetq_lane_u16 (__c, __lane)));\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vmlsq_laneq_u32 (uint32x4_t __a, uint32x4_t __b,\n+\t\tuint32x4_t __c, const int __lane)\n+{\n+  return (__a - (__b * __aarch64_vgetq_lane_u32 (__c, __lane)));\n+}\n+\n /* vmul_lane  */\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))"}, {"sha": "ec8d813fa3f53ad822d94ccf42ac0619380d7e3b", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=828e70c1d7bb5c849a2df44aa832793c71833058", "patch": "@@ -89,6 +89,9 @@\n ;; Vector Float modes.\n (define_mode_iterator VDQF [V2SF V4SF V2DF])\n \n+;; Vector single Float modes.\n+(define_mode_iterator VDQSF [V2SF V4SF])\n+\n ;; Modes suitable to use as the return type of a vcond expression.\n (define_mode_iterator VDQF_COND [V2SF V2SI V4SF V4SI V2DF V2DI])\n "}, {"sha": "4f9b8a79191454b72cedff83ccaf328f6a855cce", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=828e70c1d7bb5c849a2df44aa832793c71833058", "patch": "@@ -1,3 +1,10 @@\n+2013-09-16  James Greenhalgh  <james.greenhalgh@arm.com>\n+\n+\t* gcc.target/aarch64/fmla-intrinsic.c: New.\n+\t* gcc.target/aarch64/mla-intrinsic.c: Likewise.\n+\t* gcc.target/aarch64/fmls-intrinsic.c: Likewise.\n+\t* gcc.target/aarch64/mls-intrinsic.c: Likewise.\n+\n 2013-09-16  James Greenhalgh  <james.greenhalgh@arm.com>\n \n \t* gcc.target/aarch64/mul_intrinsic_1.c: New."}, {"sha": "0bf1b86b79e655770e014920538cd31376136d0a", "filename": "gcc/testsuite/gcc.target/aarch64/fmla_intrinsic_1.c", "status": "added", "additions": 116, "deletions": 0, "changes": 116, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Ffmla_intrinsic_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Ffmla_intrinsic_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Ffmla_intrinsic_1.c?ref=828e70c1d7bb5c849a2df44aa832793c71833058", "patch": "@@ -0,0 +1,116 @@\n+/* { dg-do run } */\n+/* { dg-options \"-O3 --save-temps\" } */\n+\n+#include <arm_neon.h>\n+\n+#define DELTA 0.0001\n+\n+extern double fabs (double);\n+\n+extern void abort (void);\n+\n+#define TEST_VMLA(q1, q2, size, in1_lanes, in2_lanes)\t\t\t\\\n+static void\t\t\t\t\t\t\t\t\\\n+test_vfma##q1##_lane##q2##_f##size (float##size##_t * res,\t\t\\\n+\t\t\t\t   const float##size##_t *in1,\t\t\\\n+\t\t\t\t   const float##size##_t *in2)\t\t\\\n+{\t\t\t\t\t\t\t\t\t\\\n+  float##size##x##in1_lanes##_t a = vld1##q1##_f##size (res);\t\t\\\n+  float##size##x##in1_lanes##_t b = vld1##q1##_f##size (in1);\t\t\\\n+  float##size##x##in2_lanes##_t c;\t\t\t\t\t\\\n+  if (in2_lanes > 1)\t\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      c = vld1##q2##_f##size (in2);\t\t\t\t\t\\\n+      a = vfma##q1##_lane##q2##_f##size (a, b, c, 1);\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  else\t\t\t\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      c = vld1##q2##_f##size (in2 + 1);\t\t\t\t\t\\\n+      a = vfma##q1##_lane##q2##_f##size (a, b, c, 0);\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  vst1##q1##_f##size (res, a);\t\t\t\t\t\t\\\n+}\n+\n+#define BUILD_VARS(width, n_lanes, n_half_lanes)\t\t\\\n+TEST_VMLA ( ,  , width, n_half_lanes, n_half_lanes)\t\t\\\n+TEST_VMLA (q,  , width, n_lanes, n_half_lanes)\t\t\t\\\n+TEST_VMLA ( , q, width, n_half_lanes, n_lanes)\t\t\t\\\n+TEST_VMLA (q, q, width, n_lanes, n_lanes)\t\t\t\\\n+\n+BUILD_VARS (32, 4, 2)\n+BUILD_VARS (64, 2, 1)\n+\n+#define POOL2 {0.0, 1.0}\n+#define POOL4 {0.0, 1.0, 2.0, 3.0}\n+#define EMPTY2 {0.0, 0.0}\n+#define EMPTY4 {0.0, 0.0, 0.0, 0.0}\n+\n+#define BUILD_TEST(size, lanes)\t\t\t\t\t\\\n+static void\t\t\t\t\t\t\t\\\n+test_f##size (void)\t\t\t\t\t\t\\\n+{\t\t\t\t\t\t\t\t\\\n+  int i;\t\t\t\t\t\t\t\\\n+  float##size##_t pool[lanes] = POOL##lanes;\t\t\t\\\n+  float##size##_t res[lanes] = EMPTY##lanes;\t\t\t\\\n+  float##size##_t res2[lanes] = EMPTY##lanes;\t\t\t\\\n+  float##size##_t res3[lanes] = EMPTY##lanes;\t\t\t\\\n+  float##size##_t res4[lanes] = EMPTY##lanes;\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\t\t\\\n+  test_vfma_lane_f##size (res, pool, pool);\t\t\t\\\n+  for (i = 0; i < lanes / 2; i++)\t\t\t\t\\\n+    if (fabs (res[i] - pool[i]) > DELTA)\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\t\t\\\n+  test_vfmaq_lane_f##size (res2, pool, pool);\t\t\t\\\n+  for (i = 0; i < lanes; i++)\t\t\t\t\t\\\n+    if (fabs (res2[i] - pool[i]) > DELTA)\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\t\t\\\n+  test_vfma_laneq_f##size (res3, pool, pool);\t\t\t\\\n+  for (i = 0; i < lanes / 2; i++)\t\t\t\t\\\n+    if (fabs (res3[i] - pool[i]) > DELTA)\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\t\t\\\n+  test_vfmaq_laneq_f##size (res4, pool, pool);\t\t\t\\\n+  for (i = 0; i < lanes; i++)\t\t\t\t\t\\\n+    if (fabs (res4[i] - pool[i]) > DELTA)\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+}\n+\n+BUILD_TEST (32, 4)\n+BUILD_TEST (64, 2)\n+\n+int\n+main (int argc, char **argv)\n+{\n+  test_f32 ();\n+  test_f64 ();\n+  return 0;\n+}\n+\n+/* vfma_laneq_f32.\n+   vfma_lane_f32.  */\n+/* { dg-final { scan-assembler-times \"fmla\\\\tv\\[0-9\\]+\\.2s, v\\[0-9\\]+\\.2s, v\\[0-9\\]+\\.2s\\\\\\[\\[0-9\\]+\\\\\\]\" 2 } } */\n+\n+/* vfmaq_lane_f32.\n+   vfmaq_laneq_f32.  */\n+/* { dg-final { scan-assembler-times \"fmla\\\\tv\\[0-9\\]+\\.4s, v\\[0-9\\]+\\.4s, v\\[0-9\\]+\\.4s\\\\\\[\\[0-9\\]+\\\\\\]\" 2 } } */\n+\n+/* vfma_lane_f64.  */\n+/* { dg-final { scan-assembler-times \"fmadd\\\\td\\[0-9\\]+\\, d\\[0-9\\]+\\, d\\[0-9\\]+\\, d\\[0-9\\]+\" 1 } } */\n+\n+/* vfmaq_lane_f64.\n+   vfma_laneq_f64.\n+   vfmaq_laneq_f64.  */\n+/* { dg-final { scan-assembler-times \"fmla\\\\tv\\[0-9\\]+\\.2d, v\\[0-9\\]+\\.2d, v\\[0-9\\]+\\.2d\\\\\\[\\[0-9\\]+\\\\\\]\" 3 } } */\n+\n+/* { dg-final { cleanup-saved-temps } } */\n+"}, {"sha": "8cc2942f8f1970c181a85b29780610ae822d589b", "filename": "gcc/testsuite/gcc.target/aarch64/fmls_intrinsic_1.c", "status": "added", "additions": 117, "deletions": 0, "changes": 117, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Ffmls_intrinsic_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Ffmls_intrinsic_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Ffmls_intrinsic_1.c?ref=828e70c1d7bb5c849a2df44aa832793c71833058", "patch": "@@ -0,0 +1,117 @@\n+/* { dg-do run } */\n+/* { dg-options \"-O3 --save-temps\" } */\n+\n+#include <arm_neon.h>\n+\n+#define DELTA 0.0001\n+\n+extern double fabs (double);\n+\n+extern void abort (void);\n+\n+#define TEST_VMLS(q1, q2, size, in1_lanes, in2_lanes)\t\t\t\\\n+static void\t\t\t\t\t\t\t\t\\\n+test_vfms##q1##_lane##q2##_f##size (float##size##_t * res,\t\t\\\n+\t\t\t\t   const float##size##_t *in1,\t\t\\\n+\t\t\t\t   const float##size##_t *in2)\t\t\\\n+{\t\t\t\t\t\t\t\t\t\\\n+  float##size##x##in1_lanes##_t a = vld1##q1##_f##size (res);\t\t\\\n+  float##size##x##in1_lanes##_t b = vld1##q1##_f##size (in1);\t\t\\\n+  float##size##x##in2_lanes##_t c;\t\t\t\t\t\\\n+  if (in2_lanes > 1)\t\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      c = vld1##q2##_f##size (in2);\t\t\t\t\t\\\n+      a = vfms##q1##_lane##q2##_f##size (a, b, c, 1);\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  else\t\t\t\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      c = vld1##q2##_f##size (in2 + 1);\t\t\t\t\t\\\n+      a = vfms##q1##_lane##q2##_f##size (a, b, c, 0);\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+  vst1##q1##_f##size (res, a);\t\t\t\t\t\t\\\n+}\n+\n+#define BUILD_VARS(width, n_lanes, n_half_lanes)\t\t\\\n+TEST_VMLS ( ,  , width, n_half_lanes, n_half_lanes)\t\t\\\n+TEST_VMLS (q,  , width, n_lanes, n_half_lanes)\t\t\t\\\n+TEST_VMLS ( , q, width, n_half_lanes, n_lanes)\t\t\t\\\n+TEST_VMLS (q, q, width, n_lanes, n_lanes)\t\t\t\\\n+\n+BUILD_VARS (32, 4, 2)\n+BUILD_VARS (64, 2, 1)\n+\n+#define POOL2 {0.0, 1.0}\n+#define POOL4 {0.0, 1.0, 2.0, 3.0}\n+#define EMPTY2 {0.0, 0.0}\n+#define EMPTY4 {0.0, 0.0, 0.0, 0.0}\n+\n+#define BUILD_TEST(size, lanes)\t\t\t\t\t\\\n+static void\t\t\t\t\t\t\t\\\n+test_f##size (void)\t\t\t\t\t\t\\\n+{\t\t\t\t\t\t\t\t\\\n+  int i;\t\t\t\t\t\t\t\\\n+  float##size##_t pool[lanes] = POOL##lanes;\t\t\t\\\n+  float##size##_t res[lanes] = EMPTY##lanes;\t\t\t\\\n+  float##size##_t res2[lanes] = EMPTY##lanes;\t\t\t\\\n+  float##size##_t res3[lanes] = EMPTY##lanes;\t\t\t\\\n+  float##size##_t res4[lanes] = EMPTY##lanes;\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\t\t\\\n+  test_vfms_lane_f##size (res, pool, pool);\t\t\t\\\n+  asm volatile (\"\" : :\"Q\" (res) : \"memory\");\t\t\t\\\n+  for (i = 0; i < lanes / 2; i++)\t\t\t\t\\\n+    if (fabs (res[i] + pool[i]) > DELTA)\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  test_vfmsq_lane_f##size (res2, pool, pool);\t\t\t\\\n+  asm volatile (\"\" : :\"Q\" (res2) : \"memory\");\t\t\t\\\n+  for (i = 0; i < lanes; i++)\t\t\t\t\t\\\n+    if (fabs (res2[i] + pool[i]) > DELTA)\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  test_vfms_laneq_f##size (res3, pool, pool);\t\t\t\\\n+  asm volatile (\"\" : :\"Q\" (res3) : \"memory\");\t\t\t\\\n+  for (i = 0; i < lanes / 2; i++)\t\t\t\t\\\n+    if (fabs (res3[i] + pool[i]) > DELTA)\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  test_vfmsq_laneq_f##size (res4, pool, pool);\t\t\t\\\n+  asm volatile (\"\" : :\"Q\" (res4) : \"memory\");\t\t\t\\\n+  for (i = 0; i < lanes; i++)\t\t\t\t\t\\\n+    if (fabs (res4[i] + pool[i]) > DELTA)\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+}\n+\n+BUILD_TEST (32, 4)\n+BUILD_TEST (64, 2)\n+\n+int\n+main (int argc, char **argv)\n+{\n+  test_f32 ();\n+  test_f64 ();\n+  return 0;\n+}\n+\n+/* vfms_laneq_f32.\n+   vfms_lane_f32.  */\n+/* { dg-final { scan-assembler-times \"fmls\\\\tv\\[0-9\\]+\\.2s, v\\[0-9\\]+\\.2s, v\\[0-9\\]+\\.2s\\\\\\[\\[0-9\\]+\\\\\\]\" 2 } } */\n+\n+/* vfmsq_lane_f32.\n+   vfmsq_laneq_f32.  */\n+/* { dg-final { scan-assembler-times \"fmls\\\\tv\\[0-9\\]+\\.4s, v\\[0-9\\]+\\.4s, v\\[0-9\\]+\\.4s\\\\\\[\\[0-9\\]+\\\\\\]\" 2 } } */\n+\n+/* vfms_lane_f64.  */\n+/* { dg-final { scan-assembler-times \"fmsub\\\\td\\[0-9\\]+\\, d\\[0-9\\]+\\, d\\[0-9\\]+\\, d\\[0-9\\]+\" 1 } } */\n+\n+/* vfmsq_lane_f64.\n+   vfms_laneq_f64.\n+   vfmsq_laneq_f64.  */\n+/* { dg-final { scan-assembler-times \"fmls\\\\tv\\[0-9\\]+\\.2d, v\\[0-9\\]+\\.2d, v\\[0-9\\]+\\.2d\\\\\\[\\[0-9\\]+\\\\\\]\" 3 } } */\n+\n+/* { dg-final { cleanup-saved-temps } } */\n+"}, {"sha": "fce41387354e65dc8fb2896f4f3bbdbb6d3b54f2", "filename": "gcc/testsuite/gcc.target/aarch64/mla_intrinsic_1.c", "status": "added", "additions": 84, "deletions": 0, "changes": 84, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fmla_intrinsic_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fmla_intrinsic_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fmla_intrinsic_1.c?ref=828e70c1d7bb5c849a2df44aa832793c71833058", "patch": "@@ -0,0 +1,84 @@\n+/* { dg-do run } */\n+/* { dg-options \"-O3 --save-temps\" } */\n+\n+#include <arm_neon.h>\n+\n+extern void abort (void);\n+\n+#define MAPs(size, xx) int##size##xx##_t\n+#define MAPu(size, xx) uint##size##xx##_t\n+\n+\n+#define TEST_VMLA(q, su, size, in1_lanes, in2_lanes)\t\t\\\n+static void\t\t\t\t\t\t\t\\\n+test_vmlaq_lane##q##_##su##size (MAP##su (size, ) * res,\t\\\n+\t\t\t\t const MAP##su(size, ) *in1,\t\\\n+\t\t\t\t const MAP##su(size, ) *in2)\t\\\n+{\t\t\t\t\t\t\t\t\\\n+  MAP##su (size, x##in1_lanes) a = vld1q_##su##size (res);\t\\\n+  MAP##su (size, x##in1_lanes) b = vld1q_##su##size (in1);\t\\\n+  MAP##su (size, x##in2_lanes) c = vld1##q##_##su##size (in2);\t\\\n+  a = vmlaq_lane##q##_##su##size (a, b, c, 1);\t\t\t\\\n+  vst1q_##su##size (res, a);\t\t\t\t\t\\\n+}\n+\n+#define BUILD_VARS(width, n_lanes, n_half_lanes)\t\t\\\n+TEST_VMLA (, s, width, n_lanes, n_half_lanes)\t\t\t\\\n+TEST_VMLA (q, s, width, n_lanes, n_lanes)\t\t\t\\\n+TEST_VMLA (, u, width, n_lanes, n_half_lanes)\t\t\t\\\n+TEST_VMLA (q, u, width, n_lanes, n_lanes)\t\t\t\\\n+\n+BUILD_VARS (32, 4, 2)\n+BUILD_VARS (16, 8, 4)\n+\n+#define POOL4 {0, 1, 2, 3}\n+#define POOL8 {0, 1, 2, 3, 4, 5, 6, 7}\n+#define EMPTY4 {0, 0, 0, 0}\n+#define EMPTY8 {0, 0, 0, 0, 0, 0, 0, 0}\n+\n+#define BUILD_TEST(su, size, lanes)\t\t\t\t\\\n+static void\t\t\t\t\t\t\t\\\n+test_##su##size (void)\t\t\t\t\t\t\\\n+{\t\t\t\t\t\t\t\t\\\n+  int i;\t\t\t\t\t\t\t\\\n+  MAP##su (size,) pool[lanes] = POOL##lanes;\t\t\t\\\n+  MAP##su (size,) res[lanes] = EMPTY##lanes;\t\t\t\\\n+  MAP##su (size,) res2[lanes] = EMPTY##lanes;\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\t\t\\\n+  test_vmlaq_lane_##su##size (res, pool, pool);\t\t\t\\\n+  for (i = 0; i < lanes; i++)\t\t\t\t\t\\\n+    if (res[i] != pool[i])\t\t\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\t\t\\\n+  test_vmlaq_laneq_##su##size (res2, pool, pool);\t\t\\\n+  for (i = 0; i < lanes; i++)\t\t\t\t\t\\\n+    if (res2[i] != pool[i])\t\t\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+}\n+\n+#undef BUILD_VARS\n+#define BUILD_VARS(size, lanes)\t\t\t\t\t\\\n+BUILD_TEST (s, size, lanes)\t\t\t\t\t\\\n+BUILD_TEST (u, size, lanes)\n+\n+BUILD_VARS (32, 4)\n+BUILD_VARS (16, 8)\n+\n+int\n+main (int argc, char **argv)\n+{\n+  test_s32 ();\n+  test_u32 ();\n+  test_s16 ();\n+  test_u16 ();\n+  return 0;\n+}\n+\n+/* { dg-final { scan-assembler-times \"mla\\\\tv\\[0-9\\]+\\.4s, v\\[0-9\\]+\\.4s, v\\[0-9\\]+\\.4s\\\\\\[\\[0-9\\]+\\\\\\]\" 4 } } */\n+/* { dg-final { scan-assembler-times \"mla\\\\tv\\[0-9\\]+\\.8h, v\\[0-9\\]+\\.8h, v\\[0-9\\]+\\.8h\\\\\\[\\[0-9\\]+\\\\\\]\" 4 } } */\n+/* { dg-final { cleanup-saved-temps } } */\n+"}, {"sha": "8bf95b641c8a6e8e6f7cf636aa45a35a824b3b63", "filename": "gcc/testsuite/gcc.target/aarch64/mls_intrinsic_1.c", "status": "added", "additions": 89, "deletions": 0, "changes": 89, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fmls_intrinsic_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/828e70c1d7bb5c849a2df44aa832793c71833058/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fmls_intrinsic_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fmls_intrinsic_1.c?ref=828e70c1d7bb5c849a2df44aa832793c71833058", "patch": "@@ -0,0 +1,89 @@\n+/* { dg-do run } */\n+/* { dg-options \"-O3 --save-temps\" } */\n+\n+#include <arm_neon.h>\n+\n+extern void abort (void);\n+\n+#define MAPs(size, xx) int##size##xx##_t\n+#define MAPu(size, xx) uint##size##xx##_t\n+\n+\n+#define TEST_VMLS(q, su, size, in1_lanes, in2_lanes)\t\t\\\n+static void\t\t\t\t\t\t\t\\\n+test_vmlsq_lane##q##_##su##size (MAP##su (size, ) * res,\t\\\n+\t\t\t\t const MAP##su(size, ) *in1,\t\\\n+\t\t\t\t const MAP##su(size, ) *in2)\t\\\n+{\t\t\t\t\t\t\t\t\\\n+  MAP##su (size, x##in1_lanes) a = vld1q_##su##size (res);\t\\\n+  MAP##su (size, x##in1_lanes) b = vld1q_##su##size (in1);\t\\\n+  MAP##su (size, x##in2_lanes) c = vld1##q##_##su##size (in2);\t\\\n+  a = vmlsq_lane##q##_##su##size (a, b, c, 1);\t\t\t\\\n+  vst1q_##su##size (res, a);\t\t\t\t\t\\\n+}\n+\n+#define BUILD_VARS(width, n_lanes, n_half_lanes)\t\t\\\n+TEST_VMLS (, s, width, n_lanes, n_half_lanes)\t\t\t\\\n+TEST_VMLS (q, s, width, n_lanes, n_lanes)\t\t\t\\\n+TEST_VMLS (, u, width, n_lanes, n_half_lanes)\t\t\t\\\n+TEST_VMLS (q, u, width, n_lanes, n_lanes)\t\t\t\\\n+\n+BUILD_VARS (32, 4, 2)\n+BUILD_VARS (16, 8, 4)\n+\n+#define MAP_OPs +\n+#define MAP_OPu -\n+\n+#define POOL4 {0, 1, 2, 3}\n+#define POOL8 {0, 1, 2, 3, 4, 5, 6, 7}\n+#define EMPTY4s {0, 0, 0, 0}\n+#define EMPTY8s {0, 0, 0, 0, 0, 0, 0, 0}\n+#define EMPTY4u {0, 2, 4, 6}\n+#define EMPTY8u {0, 2, 4, 6, 8, 10, 12, 14}\n+\n+#define BUILD_TEST(su, size, lanes)\t\t\t\t\\\n+static void\t\t\t\t\t\t\t\\\n+test_##su##size (void)\t\t\t\t\t\t\\\n+{\t\t\t\t\t\t\t\t\\\n+  int i;\t\t\t\t\t\t\t\\\n+  MAP##su (size,) pool[lanes] = POOL##lanes;\t\t\t\\\n+  MAP##su (size,) res[lanes] = EMPTY##lanes##su;\t\t\\\n+  MAP##su (size,) res2[lanes] = EMPTY##lanes##su;\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\t\t\\\n+  test_vmlsq_lane_##su##size (res, pool, pool);\t\t\t\\\n+  for (i = 0; i < lanes; i++)\t\t\t\t\t\\\n+    if (res[i] MAP_OP##su pool[i] != 0)\t\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+  /* Forecfully avoid optimization.  */\t\t\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\t\t\\\n+  test_vmlsq_laneq_##su##size (res2, pool, pool);\t\t\\\n+  for (i = 0; i < lanes; i++)\t\t\t\t\t\\\n+    if (res2[i] MAP_OP##su pool[i] != 0)\t\t\t\\\n+      abort ();\t\t\t\t\t\t\t\\\n+}\n+\n+#undef BUILD_VARS\n+#define BUILD_VARS(size, lanes)\t\t\t\t\t\\\n+BUILD_TEST (s, size, lanes)\t\t\t\t\t\\\n+BUILD_TEST (u, size, lanes)\n+\n+BUILD_VARS (32, 4)\n+BUILD_VARS (16, 8)\n+\n+int\n+main (int argc, char **argv)\n+{\n+  test_s32 ();\n+  test_u32 ();\n+  test_s16 ();\n+  test_u16 ();\n+  return 0;\n+}\n+\n+/* { dg-final { scan-assembler-times \"mls\\\\tv\\[0-9\\]+\\.4s, v\\[0-9\\]+\\.4s, v\\[0-9\\]+\\.4s\\\\\\[\\[0-9\\]+\\\\\\]\" 4 } } */\n+/* { dg-final { scan-assembler-times \"mls\\\\tv\\[0-9\\]+\\.8h, v\\[0-9\\]+\\.8h, v\\[0-9\\]+\\.8h\\\\\\[\\[0-9\\]+\\\\\\]\" 4 } } */\n+/* { dg-final { cleanup-saved-temps } } */\n+"}]}