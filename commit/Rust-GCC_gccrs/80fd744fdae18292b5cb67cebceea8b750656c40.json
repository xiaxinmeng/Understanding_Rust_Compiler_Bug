{"sha": "80fd744fdae18292b5cb67cebceea8b750656c40", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODBmZDc0NGZkYWUxODI5MmI1Y2I2N2NlYmNlZWE4Yjc1MDY1NmM0MA==", "commit": {"author": {"name": "Richard Henderson", "email": "rth@redhat.com", "date": "2007-03-06T15:59:38Z"}, "committer": {"name": "Richard Henderson", "email": "rth@gcc.gnu.org", "date": "2007-03-06T15:59:38Z"}, "message": "i386.c (x86_use_leave, [...]): Merge into ...\n\n\t* config/i386/i386.c (x86_use_leave, x86_push_memory,\n\tx86_zero_extend_with_and, x86_movx, x86_double_with_add,\n\tx86_use_bit_test, x86_unroll_strlen, x86_deep_branch,\n\tx86_branch_hints, x86_use_sahf, x86_partial_reg_stall,\n\tx86_partial_flag_reg_stall, x86_use_himode_fiop, x86_use_simode_fiop,\n\tx86_use_mov0, x86_use_cltd, x86_read_modify_write, x86_read_modify,\n\tx86_split_long_moves, x86_promote_QImode, x86_fast_prefix, \n\tx86_single_stringop, x86_qimode_math, x86_promote_qi_regs,\n\tx86_himode_math, x86_promote_hi_regs, x86_sub_esp_4, x86_sub_esp_8,\n\tx86_add_esp_4, x86_add_esp_8, x86_integer_DFmode_moves, \n\tx86_partial_reg_dependency, x86_memory_mismatch_stall, \n\tx86_prologue_using_move, x86_epilogue_using_move, x86_shift1,\n\tx86_sse_partial_reg_dependency, x86_sse_split_regs, \n\tx86_sse_unaligned_move_optimal, x86_sse_typeless_stores,\n\tx86_sse_load0_by_pxor, x86_use_ffreep, x86_use_incdec,\n\tx86_inter_unit_moves, x86_ext_80387_constants, x86_four_jump_limit,\n\tx86_schedule, x86_use_bt, x86_pad_returns): Merge into ...\n\t(ix86_tune_features): ... here.  New array.\n\t(x86_cmove, x86_use_xchgb, x86_cmpxchg, x86_cmpxchg8b,\t\n\tx86_xadd, x86_bswap): Merge into ...\n\t(ix86_arch_features): ... here.  New array.\n\t(x86_3dnow_a): Remove.\n\t(x86_accumulate_outgoing_args): Make static.\n\t(x86_arch_always_fancy_math_387): Make static.\n\t(ix86_tune_mask, ix86_arch_mask): Move ...\n\t(override_options): ... to local variables here.  Apply the\n\tappropriate mask to each element of ix86_arch_features and\n\tix86_tune_features.  Adjust TARGET_CMOVE and TARGET_USE_SAHF\n\tas were done in the old macros.\n\t(standard_80387_constant_p): Use TARGET_EXT_80387_CONSTANTS.\n\t* config/i386/i386.h (x86_use_leave, x86_push_memory,\n\tx86_zero_extend_with_and, x86_use_bit_test, x86_cmove, x86_deep_branch,\n\tx86_branch_hints, x86_unroll_strlen, x86_double_with_add,\n\tx86_partial_reg_stall, x86_movx, x86_use_himode_fiop,\n\tx86_use_simode_fiop, x86_use_mov0, x86_use_cltd, x86_use_xchgb,\n\tx86_read_modify_write, x86_read_modify, x86_split_long_moves,\n\tx86_promote_QImode, x86_single_stringop, x86_fast_prefix,\n\tx86_himode_math, x86_qimode_math, x86_promote_qi_regs,\n\tx86_promote_hi_regs, x86_integer_DFmode_moves, x86_add_esp_4,\n\tx86_add_esp_8, x86_sub_esp_4, x86_sub_esp_8,\n\tx86_partial_reg_dependency, x86_memory_mismatch_stall,\n\tx86_accumulate_outgoing_args, x86_prologue_using_move,\n\tx86_epilogue_using_move, x86_decompose_lea,\n\tx86_arch_always_fancy_math_387, x86_shift1,\n\tx86_sse_partial_reg_dependency, x86_sse_split_regs,\n\tx86_sse_unaligned_move_optimal, x86_sse_typeless_stores,\t\n\tx86_sse_load0_by_pxor, x86_use_ffreep, x86_inter_unit_moves,\n\tx86_schedule, x86_use_bt, x86_cmpxchg, x86_cmpxchg8b, x86_xadd,\n\tx86_use_incdec, x86_pad_returns, x86_bswap,\n\tx86_partial_flag_reg_stall): Remove.\n\t(enum ix86_tune_indices): New.\n\t(ix86_tune_features): New.\n\t(TARGET_USE_LEAVE, TARGET_PUSH_MEMORY, TARGET_ZERO_EXTEND_WITH_AND,\n\tTARGET_USE_BIT_TEST, TARGET_UNROLL_STRLEN,\n\tTARGET_DEEP_BRANCH_PREDICTION, TARGET_BRANCH_PREDICTION_HINTS,\n\tTARGET_DOUBLE_WITH_ADD, TARGET_USE_SAHF, TARGET_MOVX,\n\tTARGET_PARTIAL_REG_STALL, TARGET_PARTIAL_FLAG_REG_STALL,\n\tTARGET_USE_HIMODE_FIOP, TARGET_USE_SIMODE_FIOP, TARGET_USE_MOV0,\n\tTARGET_USE_CLTD, TARGET_USE_XCHGB, TARGET_SPLIT_LONG_MOVES,\n\tTARGET_READ_MODIFY_WRITE, TARGET_READ_MODIFY, TARGET_PROMOTE_QImode,\n\tTARGET_FAST_PREFIX, TARGET_SINGLE_STRINGOP, TARGET_QIMODE_MATH,\n\tTARGET_HIMODE_MATH, TARGET_PROMOTE_QI_REGS, TARGET_PROMOTE_HI_REGS,\n\tTARGET_ADD_ESP_4, TARGET_ADD_ESP_8, TARGET_SUB_ESP_4,\n\tTARGET_SUB_ESP_8, TARGET_INTEGER_DFMODE_MOVES,\n\tTARGET_PARTIAL_REG_DEPENDENCY, TARGET_SSE_PARTIAL_REG_DEPENDENCY,\n\tTARGET_SSE_UNALIGNED_MOVE_OPTIMAL, TARGET_SSE_SPLIT_REGS,\n\tTARGET_SSE_TYPELESS_STORES, TARGET_SSE_LOAD0_BY_PXOR,\n\tTARGET_MEMORY_MISMATCH_STALL, TARGET_PROLOGUE_USING_MOVE,\n\tTARGET_EPILOGUE_USING_MOVE, TARGET_SHIFT1, TARGET_USE_FFREEP,\n\tTARGET_INTER_UNIT_MOVES, TARGET_FOUR_JUMP_LIMIT, TARGET_SCHEDULE,\n\tTARGET_USE_BT, TARGET_USE_INCDEC, TARGET_PAD_RETURNS,\n\tTARGET_EXT_80387_CONSTANTS): Use it.\n\t(enum ix86_arch_indices): New.\n\t(ix86_arch_features): New.\n\t(TARGET_CMOVE, TARGET_CMPXCHG, TARGET_CMPXCHG8B, TARGET_XADD,\n\tTARGET_BSWAP): Use it.\n\t(ix86_tune_mask, ix86_arch_mask): Remove.\n\nFrom-SVN: r122621", "tree": {"sha": "8dcd9af23e8d7fadc4bbdb37a6caff7a249aa7b0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/8dcd9af23e8d7fadc4bbdb37a6caff7a249aa7b0"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/80fd744fdae18292b5cb67cebceea8b750656c40", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/80fd744fdae18292b5cb67cebceea8b750656c40", "html_url": "https://github.com/Rust-GCC/gccrs/commit/80fd744fdae18292b5cb67cebceea8b750656c40", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/80fd744fdae18292b5cb67cebceea8b750656c40/comments", "author": null, "committer": null, "parents": [{"sha": "14da607343d7637050c36ee3d338156dcc431354", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/14da607343d7637050c36ee3d338156dcc431354", "html_url": "https://github.com/Rust-GCC/gccrs/commit/14da607343d7637050c36ee3d338156dcc431354"}], "stats": {"total": 807, "additions": 517, "deletions": 290}, "files": [{"sha": "bc27cf5a55a7fa7a35fc68c81c2781ddcabdeff2", "filename": "gcc/ChangeLog", "status": "modified", "additions": 80, "deletions": 0, "changes": 80, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/80fd744fdae18292b5cb67cebceea8b750656c40/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/80fd744fdae18292b5cb67cebceea8b750656c40/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=80fd744fdae18292b5cb67cebceea8b750656c40", "patch": "@@ -1,3 +1,83 @@\n+2007-03-06  Richard Henderson  <rth@redhat.com>\n+\n+\t* config/i386/i386.c (x86_use_leave, x86_push_memory,\n+\tx86_zero_extend_with_and, x86_movx, x86_double_with_add,\n+\tx86_use_bit_test, x86_unroll_strlen, x86_deep_branch,\n+\tx86_branch_hints, x86_use_sahf, x86_partial_reg_stall,\n+\tx86_partial_flag_reg_stall, x86_use_himode_fiop, x86_use_simode_fiop,\n+\tx86_use_mov0, x86_use_cltd, x86_read_modify_write, x86_read_modify,\n+\tx86_split_long_moves, x86_promote_QImode, x86_fast_prefix, \n+\tx86_single_stringop, x86_qimode_math, x86_promote_qi_regs,\n+\tx86_himode_math, x86_promote_hi_regs, x86_sub_esp_4, x86_sub_esp_8,\n+\tx86_add_esp_4, x86_add_esp_8, x86_integer_DFmode_moves, \n+\tx86_partial_reg_dependency, x86_memory_mismatch_stall, \n+\tx86_prologue_using_move, x86_epilogue_using_move, x86_shift1,\n+\tx86_sse_partial_reg_dependency, x86_sse_split_regs, \n+\tx86_sse_unaligned_move_optimal, x86_sse_typeless_stores,\n+\tx86_sse_load0_by_pxor, x86_use_ffreep, x86_use_incdec,\n+\tx86_inter_unit_moves, x86_ext_80387_constants, x86_four_jump_limit,\n+\tx86_schedule, x86_use_bt, x86_pad_returns): Merge into ...\n+\t(ix86_tune_features): ... here.  New array.\n+\t(x86_cmove, x86_use_xchgb, x86_cmpxchg, x86_cmpxchg8b,\t\n+\tx86_xadd, x86_bswap): Merge into ...\n+\t(ix86_arch_features): ... here.  New array.\n+\t(x86_3dnow_a): Remove.\n+\t(x86_accumulate_outgoing_args): Make static.\n+\t(x86_arch_always_fancy_math_387): Make static.\n+\t(ix86_tune_mask, ix86_arch_mask): Move ...\n+\t(override_options): ... to local variables here.  Apply the\n+\tappropriate mask to each element of ix86_arch_features and\n+\tix86_tune_features.  Adjust TARGET_CMOVE and TARGET_USE_SAHF\n+\tas were done in the old macros.\n+\t(standard_80387_constant_p): Use TARGET_EXT_80387_CONSTANTS.\n+\t* config/i386/i386.h (x86_use_leave, x86_push_memory,\n+\tx86_zero_extend_with_and, x86_use_bit_test, x86_cmove, x86_deep_branch,\n+\tx86_branch_hints, x86_unroll_strlen, x86_double_with_add,\n+\tx86_partial_reg_stall, x86_movx, x86_use_himode_fiop,\n+\tx86_use_simode_fiop, x86_use_mov0, x86_use_cltd, x86_use_xchgb,\n+\tx86_read_modify_write, x86_read_modify, x86_split_long_moves,\n+\tx86_promote_QImode, x86_single_stringop, x86_fast_prefix,\n+\tx86_himode_math, x86_qimode_math, x86_promote_qi_regs,\n+\tx86_promote_hi_regs, x86_integer_DFmode_moves, x86_add_esp_4,\n+\tx86_add_esp_8, x86_sub_esp_4, x86_sub_esp_8,\n+\tx86_partial_reg_dependency, x86_memory_mismatch_stall,\n+\tx86_accumulate_outgoing_args, x86_prologue_using_move,\n+\tx86_epilogue_using_move, x86_decompose_lea,\n+\tx86_arch_always_fancy_math_387, x86_shift1,\n+\tx86_sse_partial_reg_dependency, x86_sse_split_regs,\n+\tx86_sse_unaligned_move_optimal, x86_sse_typeless_stores,\t\n+\tx86_sse_load0_by_pxor, x86_use_ffreep, x86_inter_unit_moves,\n+\tx86_schedule, x86_use_bt, x86_cmpxchg, x86_cmpxchg8b, x86_xadd,\n+\tx86_use_incdec, x86_pad_returns, x86_bswap,\n+\tx86_partial_flag_reg_stall): Remove.\n+\t(enum ix86_tune_indices): New.\n+\t(ix86_tune_features): New.\n+\t(TARGET_USE_LEAVE, TARGET_PUSH_MEMORY, TARGET_ZERO_EXTEND_WITH_AND,\n+\tTARGET_USE_BIT_TEST, TARGET_UNROLL_STRLEN,\n+\tTARGET_DEEP_BRANCH_PREDICTION, TARGET_BRANCH_PREDICTION_HINTS,\n+\tTARGET_DOUBLE_WITH_ADD, TARGET_USE_SAHF, TARGET_MOVX,\n+\tTARGET_PARTIAL_REG_STALL, TARGET_PARTIAL_FLAG_REG_STALL,\n+\tTARGET_USE_HIMODE_FIOP, TARGET_USE_SIMODE_FIOP, TARGET_USE_MOV0,\n+\tTARGET_USE_CLTD, TARGET_USE_XCHGB, TARGET_SPLIT_LONG_MOVES,\n+\tTARGET_READ_MODIFY_WRITE, TARGET_READ_MODIFY, TARGET_PROMOTE_QImode,\n+\tTARGET_FAST_PREFIX, TARGET_SINGLE_STRINGOP, TARGET_QIMODE_MATH,\n+\tTARGET_HIMODE_MATH, TARGET_PROMOTE_QI_REGS, TARGET_PROMOTE_HI_REGS,\n+\tTARGET_ADD_ESP_4, TARGET_ADD_ESP_8, TARGET_SUB_ESP_4,\n+\tTARGET_SUB_ESP_8, TARGET_INTEGER_DFMODE_MOVES,\n+\tTARGET_PARTIAL_REG_DEPENDENCY, TARGET_SSE_PARTIAL_REG_DEPENDENCY,\n+\tTARGET_SSE_UNALIGNED_MOVE_OPTIMAL, TARGET_SSE_SPLIT_REGS,\n+\tTARGET_SSE_TYPELESS_STORES, TARGET_SSE_LOAD0_BY_PXOR,\n+\tTARGET_MEMORY_MISMATCH_STALL, TARGET_PROLOGUE_USING_MOVE,\n+\tTARGET_EPILOGUE_USING_MOVE, TARGET_SHIFT1, TARGET_USE_FFREEP,\n+\tTARGET_INTER_UNIT_MOVES, TARGET_FOUR_JUMP_LIMIT, TARGET_SCHEDULE,\n+\tTARGET_USE_BT, TARGET_USE_INCDEC, TARGET_PAD_RETURNS,\n+\tTARGET_EXT_80387_CONSTANTS): Use it.\n+\t(enum ix86_arch_indices): New.\n+\t(ix86_arch_features): New.\n+\t(TARGET_CMOVE, TARGET_CMPXCHG, TARGET_CMPXCHG8B, TARGET_XADD,\n+\tTARGET_BSWAP): Use it.\n+\t(ix86_tune_mask, ix86_arch_mask): Remove.\n+\n 2007-03-06  Joseph Myers  <joseph@codesourcery.com>\n \n \tPR bootstrap/31020"}, {"sha": "cf3b3ff319bd3a7c1976da070e0310aa9435aa11", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 278, "deletions": 183, "changes": 461, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/80fd744fdae18292b5cb67cebceea8b750656c40/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/80fd744fdae18292b5cb67cebceea8b750656c40/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=80fd744fdae18292b5cb67cebceea8b750656c40", "patch": "@@ -1004,187 +1004,221 @@ const struct processor_costs *ix86_cost = &pentium_cost;\n    (PPro/PENT4/NOCONA/CORE2/Athlon/K8).  */\n #define m_GENERIC (m_GENERIC32 | m_GENERIC64)\n \n-/* Leave is not affecting Nocona SPEC2000 results negatively, so enabling for\n-   Generic64 seems like good code size tradeoff.  We can't enable it for 32bit\n-   generic because it is not working well with PPro base chips.  */\n-const int x86_use_leave = m_386 | m_K6_GEODE | m_ATHLON_K8_AMDFAM10 | m_CORE2\n-                          | m_GENERIC64;\n-const int x86_push_memory = m_386 | m_K6_GEODE | m_ATHLON_K8_AMDFAM10 | m_PENT4\n-                            | m_NOCONA | m_CORE2 | m_GENERIC;\n-const int x86_zero_extend_with_and = m_486 | m_PENT;\n-/* Enable to zero extend integer registers to avoid partial dependencies */\n-const int x86_movx = m_ATHLON_K8_AMDFAM10 | m_PPRO | m_PENT4 | m_NOCONA\n-                     | m_CORE2 | m_GENERIC | m_GEODE /* m_386 | m_K6 */;\n-const int x86_double_with_add = ~m_386;\n-const int x86_use_bit_test = m_386;\n-const int x86_unroll_strlen = m_486 | m_PENT | m_PPRO | m_ATHLON_K8_AMDFAM10\n-                              | m_K6 | m_CORE2 | m_GENERIC;\n-const int x86_cmove = m_PPRO | m_GEODE | m_ATHLON_K8_AMDFAM10 | m_PENT4\n-                      | m_NOCONA;\n-const int x86_3dnow_a = m_ATHLON_K8_AMDFAM10;\n-const int x86_deep_branch = m_PPRO | m_K6_GEODE | m_ATHLON_K8_AMDFAM10\n-                            | m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC;\n-/* Branch hints were put in P4 based on simulation result. But\n-   after P4 was made, no performance benefit was observed with\n-   branch hints. It also increases the code size. As the result,\n-   icc never generates branch hints.  */\n-const int x86_branch_hints = 0;\n-const int x86_use_sahf = m_PPRO | m_K6_GEODE | m_PENT4 | m_NOCONA | m_GENERIC32;\n-                         /*m_GENERIC | m_ATHLON_K8 ? */\n-/* We probably ought to watch for partial register stalls on Generic32\n-   compilation setting as well.  However in current implementation the\n-   partial register stalls are not eliminated very well - they can\n-   be introduced via subregs synthesized by combine and can happen\n-   in caller/callee saving sequences.\n-   Because this option pays back little on PPro based chips and is in conflict\n-   with partial reg. dependencies used by Athlon/P4 based chips, it is better\n-   to leave it off for generic32 for now.  */\n-const int x86_partial_reg_stall = m_PPRO;\n-const int x86_partial_flag_reg_stall =  m_CORE2 | m_GENERIC;\n-const int x86_use_himode_fiop = m_386 | m_486 | m_K6_GEODE;\n-const int x86_use_simode_fiop = ~(m_PPRO | m_ATHLON_K8_AMDFAM10 | m_PENT\n-                                  | m_CORE2 | m_GENERIC);\n-const int x86_use_mov0 = m_K6;\n-const int x86_use_cltd = ~(m_PENT | m_K6 | m_CORE2 | m_GENERIC);\n-/* Use xchgb %rh,%rl instead of rolw/rorw $8,rx.  */\n-const int x86_use_xchgb = m_PENT4;\n-const int x86_read_modify_write = ~m_PENT;\n-const int x86_read_modify = ~(m_PENT | m_PPRO);\n-const int x86_split_long_moves = m_PPRO;\n-const int x86_promote_QImode = m_K6_GEODE | m_PENT | m_386 | m_486\n-                               | m_ATHLON_K8_AMDFAM10 | m_CORE2 | m_GENERIC;\n-                               /* m_PENT4 ? */\n-const int x86_fast_prefix = ~(m_PENT | m_486 | m_386);\n-const int x86_single_stringop = m_386 | m_PENT4 | m_NOCONA;\n-const int x86_qimode_math = ~(0);\n-const int x86_promote_qi_regs = 0;\n-/* On PPro this flag is meant to avoid partial register stalls.  Just like\n-   the x86_partial_reg_stall this option might be considered for Generic32\n-   if our scheme for avoiding partial stalls was more effective.  */\n-const int x86_himode_math = ~(m_PPRO);\n-const int x86_promote_hi_regs = m_PPRO;\n-/* Enable if add/sub rsp is preferred over 1 or 2 push/pop */\n-const int x86_sub_esp_4 = m_ATHLON_K8_AMDFAM10 | m_PPRO | m_PENT4 | m_NOCONA\n-                          | m_CORE2 | m_GENERIC;\n-const int x86_sub_esp_8 = m_ATHLON_K8_AMDFAM10 | m_PPRO | m_386 | m_486\n-                          | m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC;\n-const int x86_add_esp_4 = m_ATHLON_K8_AMDFAM10 | m_K6_GEODE | m_PENT4 | m_NOCONA\n-                          | m_CORE2 | m_GENERIC;\n-const int x86_add_esp_8 = m_ATHLON_K8_AMDFAM10 | m_PPRO | m_K6_GEODE | m_386\n-                          | m_486 | m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC;\n-/* Enable if integer moves are preferred for DFmode copies */\n-const int x86_integer_DFmode_moves = ~(m_ATHLON_K8_AMDFAM10 | m_PENT4 | m_NOCONA\n-                                       | m_PPRO | m_CORE2 | m_GENERIC | m_GEODE);\n-const int x86_partial_reg_dependency = m_ATHLON_K8_AMDFAM10 | m_PENT4 | m_NOCONA\n-                                       | m_CORE2 | m_GENERIC;\n-const int x86_memory_mismatch_stall = m_ATHLON_K8_AMDFAM10 | m_PENT4 | m_NOCONA \n-                                      | m_CORE2 | m_GENERIC;\n-/* If ACCUMULATE_OUTGOING_ARGS is enabled, the maximum amount of space required\n-   for outgoing arguments will be computed and placed into the variable\n-   `current_function_outgoing_args_size'. No space will be pushed onto the stack\n-   for each call; instead, the function prologue should increase the stack frame\n-   size by this amount. Setting both PUSH_ARGS and ACCUMULATE_OUTGOING_ARGS is\n-   not proper. */\n-const int x86_accumulate_outgoing_args = m_ATHLON_K8_AMDFAM10 | m_PENT4\n-                                         | m_NOCONA | m_PPRO | m_CORE2\n-                                         | m_GENERIC;\n-const int x86_prologue_using_move = m_ATHLON_K8 | m_PPRO | m_CORE2 | m_GENERIC;\n-const int x86_epilogue_using_move = m_ATHLON_K8 | m_PPRO | m_CORE2 | m_GENERIC;\n-const int x86_shift1 = ~m_486;\n-const int x86_arch_always_fancy_math_387 = m_PENT | m_PPRO\n-                                           | m_ATHLON_K8_AMDFAM10 | m_PENT4 \n-                                           | m_NOCONA | m_CORE2 | m_GENERIC;\n-/* In Generic model we have an conflict here in between PPro/Pentium4 based chips\n-   that thread 128bit SSE registers as single units versus K8 based chips that\n-   divide SSE registers to two 64bit halves.\n-   x86_sse_partial_reg_dependency promote all store destinations to be 128bit\n-   to allow register renaming on 128bit SSE units, but usually results in one\n-   extra microop on 64bit SSE units.  Experimental results shows that disabling\n-   this option on P4 brings over 20% SPECfp regression, while enabling it on\n-   K8 brings roughly 2.4% regression that can be partly masked by careful scheduling\n-   of moves.  */\n-const int x86_sse_partial_reg_dependency = m_PENT4 | m_NOCONA | m_PPRO | m_CORE2\n-                                           | m_GENERIC | m_AMDFAM10;\n-/* Set for machines where the type and dependencies are resolved on SSE\n-   register parts instead of whole registers, so we may maintain just\n-   lower part of scalar values in proper format leaving the upper part\n-   undefined.  */\n-const int x86_sse_split_regs = m_ATHLON_K8;\n-/* Code generation for scalar reg-reg moves of single and double precision data:\n-     if (x86_sse_partial_reg_dependency == true | x86_sse_split_regs == true)\n-       movaps reg, reg\n-     else\n-       movss reg, reg\n-     if (x86_sse_partial_reg_dependency == true)\n-       movapd reg, reg\n-     else\n-       movsd reg, reg\n+/* Feature tests against the various tunings.  */\n+unsigned int ix86_tune_features[X86_TUNE_LAST] = {\n+  /* X86_TUNE_USE_LEAVE: Leave does not affect Nocona SPEC2000 results\n+     negatively, so enabling for Generic64 seems like good code size\n+     tradeoff.  We can't enable it for 32bit generic because it does not\n+     work well with PPro base chips.  */\n+  m_386 | m_K6_GEODE | m_ATHLON_K8_AMDFAM10 | m_CORE2 | m_GENERIC64,\n+\n+  /* X86_TUNE_PUSH_MEMORY */\n+  m_386 | m_K6_GEODE | m_ATHLON_K8_AMDFAM10 | m_PENT4\n+  | m_NOCONA | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_ZERO_EXTEND_WITH_AND */\n+  m_486 | m_PENT,\n+\n+  /* X86_TUNE_USE_BIT_TEST */\n+  m_386,\n+\n+  /* X86_TUNE_UNROLL_STRLEN */\n+  m_486 | m_PENT | m_PPRO | m_ATHLON_K8_AMDFAM10 | m_K6 | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_DEEP_BRANCH_PREDICTION */\n+  m_PPRO | m_K6_GEODE | m_ATHLON_K8_AMDFAM10 | m_PENT4\n+  | m_NOCONA | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_BRANCH_PREDICTION_HINTS: Branch hints were put in P4 based\n+     on simulation result. But after P4 was made, no performance benefit\n+     was observed with branch hints.  It also increases the code size.\n+     As a result, icc never generates branch hints.  */\n+  0,\n+\n+  /* X86_TUNE_DOUBLE_WITH_ADD */\n+  ~m_386,\n+  \n+  /* X86_TUNE_USE_SAHF */\n+  m_PPRO | m_K6_GEODE | m_PENT4 | m_NOCONA | m_GENERIC32,\n+  /* | m_GENERIC | m_ATHLON_K8 ? */\n+\n+  /* X86_TUNE_MOVX: Enable to zero extend integer registers to avoid\n+     partial dependencies */\n+  m_ATHLON_K8_AMDFAM10 | m_PPRO | m_PENT4 | m_NOCONA\n+  | m_CORE2 | m_GENERIC | m_GEODE /* m_386 | m_K6 */,\n+\n+  /* X86_TUNE_PARTIAL_REG_STALL: We probably ought to watch for partial\n+     register stalls on Generic32 compilation setting as well.  However\n+     in current implementation the partial register stalls are not eliminated\n+     very well - they can be introduced via subregs synthesized by combine\n+     and can happen in caller/callee saving sequences.  Because this option\n+     pays back little on PPro based chips and is in conflict with partial reg\n+     dependencies used by Athlon/P4 based chips, it is better to leave it off\n+     for generic32 for now.  */\n+  m_PPRO,\n+\n+  /* X86_TUNE_PARTIAL_FLAG_REG_STALL */\n+  m_CORE2 | m_GENERIC,\n+  \n+  /* X86_TUNE_USE_HIMODE_FIOP */\n+  m_386 | m_486 | m_K6_GEODE,\n \n-   Code generation for scalar loads of double precision data:\n-     if (x86_sse_split_regs == true)\n-       movlpd mem, reg      (gas syntax)\n-     else\n-       movsd mem, reg\n- \n-   Code generation for unaligned packed loads of single precision data\n-   (x86_sse_unaligned_move_optimal overrides x86_sse_partial_reg_dependency):\n-     if (x86_sse_unaligned_move_optimal)\n-       movups mem, reg\n+  /* X86_TUNE_USE_SIMODE_FIOP */\n+  ~(m_PPRO | m_ATHLON_K8_AMDFAM10 | m_PENT | m_CORE2 | m_GENERIC),\n \n-     if (x86_sse_partial_reg_dependency == true)\n-       {\n-         xorps  reg, reg\n-         movlps mem, reg\n-         movhps mem+8, reg\n-       }\n-     else\n-       {\n-         movlps mem, reg\n-         movhps mem+8, reg\n-       }\n+  /* X86_TUNE_USE_MOV0 */\n+  m_K6,\n+  \n+  /* X86_TUNE_USE_CLTD */\n+  ~(m_PENT | m_K6 | m_CORE2 | m_GENERIC),\n \n-   Code generation for unaligned packed loads of double precision data\n-   (x86_sse_unaligned_move_optimal overrides x86_sse_split_regs):\n-     if (x86_sse_unaligned_move_optimal)\n-       movupd mem, reg\n+  /* X86_TUNE_USE_XCHGB: Use xchgb %rh,%rl instead of rolw/rorw $8,rx.  */\n+  m_PENT4,\n \n-     if (x86_sse_split_regs == true)\n-       {\n-         movlpd mem, reg\n-         movhpd mem+8, reg\n-       }\n-     else\n-       {\n-         movsd  mem, reg\n-         movhpd mem+8, reg\n-       }\n- */\n-const int x86_sse_unaligned_move_optimal = m_AMDFAM10;\n-const int x86_sse_typeless_stores = m_ATHLON_K8_AMDFAM10;\n-const int x86_sse_load0_by_pxor = m_PPRO | m_PENT4 | m_NOCONA;\n-const int x86_use_ffreep = m_ATHLON_K8_AMDFAM10;\n-const int x86_use_incdec = ~(m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC);\n-\n-const int x86_inter_unit_moves = ~(m_ATHLON_K8_AMDFAM10 | m_GENERIC);\n-\n-const int x86_ext_80387_constants = m_K6_GEODE | m_ATHLON_K8 | m_PENT4\n-                                    | m_NOCONA | m_PPRO | m_CORE2 | m_GENERIC;\n-/* Some CPU cores are not able to predict more than 4 branch instructions in\n-   the 16 byte window.  */\n-const int x86_four_jump_limit = m_PPRO | m_ATHLON_K8_AMDFAM10 | m_PENT4\n-                                | m_NOCONA | m_CORE2 | m_GENERIC;\n-const int x86_schedule = m_PPRO | m_ATHLON_K8_AMDFAM10 | m_K6_GEODE | m_PENT\n-                         | m_CORE2 | m_GENERIC;\n-const int x86_use_bt = m_ATHLON_K8_AMDFAM10;\n-/* Compare and exchange was added for 80486.  */\n-const int x86_cmpxchg = ~m_386;\n-/* Compare and exchange 8 bytes was added for pentium.  */\n-const int x86_cmpxchg8b = ~(m_386 | m_486);\n-/* Exchange and add was added for 80486.  */\n-const int x86_xadd = ~m_386;\n-/* Byteswap was added for 80486.  */\n-const int x86_bswap = ~m_386;\n-const int x86_pad_returns = m_ATHLON_K8_AMDFAM10 | m_CORE2 | m_GENERIC;\n+  /* X86_TUNE_SPLIT_LONG_MOVES */\n+  m_PPRO,\n+\n+  /* X86_TUNE_READ_MODIFY_WRITE */\n+  ~m_PENT,\n+\n+  /* X86_TUNE_READ_MODIFY */\n+  ~(m_PENT | m_PPRO),\n+\n+  /* X86_TUNE_PROMOTE_QIMODE */\n+  m_K6_GEODE | m_PENT | m_386 | m_486 | m_ATHLON_K8_AMDFAM10 | m_CORE2\n+  | m_GENERIC /* | m_PENT4 ? */,\n+\n+  /* X86_TUNE_FAST_PREFIX */\n+  ~(m_PENT | m_486 | m_386),\n+\n+  /* X86_TUNE_SINGLE_STRINGOP */\n+  m_386 | m_PENT4 | m_NOCONA,\n+  \n+  /* X86_TUNE_QIMODE_MATH */\n+  ~0,\n+  \n+  /* X86_TUNE_HIMODE_MATH: On PPro this flag is meant to avoid partial\n+     register stalls.  Just like X86_TUNE_PARTIAL_REG_STALL this option\n+     might be considered for Generic32 if our scheme for avoiding partial\n+     stalls was more effective.  */\n+  ~m_PPRO,\n+\n+  /* X86_TUNE_PROMOTE_QI_REGS */\n+  0,\n+\n+  /* X86_TUNE_PROMOTE_HI_REGS */\n+  m_PPRO,\n+\n+  /* X86_TUNE_ADD_ESP_4: Enable if add/sub is preferred over 1/2 push/pop.  */\n+  m_ATHLON_K8_AMDFAM10 | m_K6_GEODE | m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_ADD_ESP_8 */\n+  m_ATHLON_K8_AMDFAM10 | m_PPRO | m_K6_GEODE | m_386\n+  | m_486 | m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_SUB_ESP_4 */\n+  m_ATHLON_K8_AMDFAM10 | m_PPRO | m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_SUB_ESP_8 */\n+  m_ATHLON_K8_AMDFAM10 | m_PPRO | m_386 | m_486\n+  | m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_INTEGER_DFMODE_MOVES: Enable if integer moves are preferred\n+     for DFmode copies */\n+  ~(m_ATHLON_K8_AMDFAM10 | m_PENT4 | m_NOCONA | m_PPRO | m_CORE2\n+    | m_GENERIC | m_GEODE),\n+\n+  /* X86_TUNE_PARTIAL_REG_DEPENDENCY */\n+  m_ATHLON_K8_AMDFAM10 | m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY: In the Generic model we have a\n+     conflict here in between PPro/Pentium4 based chips that thread 128bit\n+     SSE registers as single units versus K8 based chips that divide SSE\n+     registers to two 64bit halves.  This knob promotes all store destinations\n+     to be 128bit to allow register renaming on 128bit SSE units, but usually\n+     results in one extra microop on 64bit SSE units.  Experimental results\n+     shows that disabling this option on P4 brings over 20% SPECfp regression,\n+     while enabling it on K8 brings roughly 2.4% regression that can be partly\n+     masked by careful scheduling of moves.  */\n+  m_PENT4 | m_NOCONA | m_PPRO | m_CORE2 | m_GENERIC | m_AMDFAM10,\n+\n+  /* X86_TUNE_SSE_UNALIGNED_MOVE_OPTIMAL */\n+  m_AMDFAM10,\n+\n+  /* X86_TUNE_SSE_SPLIT_REGS: Set for machines where the type and dependencies\n+     are resolved on SSE register parts instead of whole registers, so we may\n+     maintain just lower part of scalar values in proper format leaving the\n+     upper part undefined.  */\n+  m_ATHLON_K8,\n+\n+  /* X86_TUNE_SSE_TYPELESS_STORES */\n+  m_ATHLON_K8_AMDFAM10,\n+\n+  /* X86_TUNE_SSE_LOAD0_BY_PXOR */\n+  m_PPRO | m_PENT4 | m_NOCONA,\n+\n+  /* X86_TUNE_MEMORY_MISMATCH_STALL */\n+  m_ATHLON_K8_AMDFAM10 | m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_PROLOGUE_USING_MOVE */\n+  m_ATHLON_K8 | m_PPRO | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_EPILOGUE_USING_MOVE */\n+  m_ATHLON_K8 | m_PPRO | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_SHIFT1 */\n+  ~m_486,\n+\n+  /* X86_TUNE_USE_FFREEP */\n+  m_ATHLON_K8_AMDFAM10,\n+\n+  /* X86_TUNE_INTER_UNIT_MOVES */\n+  ~(m_ATHLON_K8_AMDFAM10 | m_GENERIC),\n+\n+  /* X86_TUNE_FOUR_JUMP_LIMIT: Some CPU cores are not able to predict more\n+     than 4 branch instructions in the 16 byte window.  */\n+  m_PPRO | m_ATHLON_K8_AMDFAM10 | m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_SCHEDULE */\n+  m_PPRO | m_ATHLON_K8_AMDFAM10 | m_K6_GEODE | m_PENT | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_USE_BT */\n+  m_ATHLON_K8_AMDFAM10,\n+\n+  /* X86_TUNE_USE_INCDEC */\n+  ~(m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC),\n+\n+  /* X86_TUNE_PAD_RETURNS */\n+  m_ATHLON_K8_AMDFAM10 | m_CORE2 | m_GENERIC,\n+\n+  /* X86_TUNE_EXT_80387_CONSTANTS */\n+  m_K6_GEODE | m_ATHLON_K8 | m_PENT4 | m_NOCONA | m_PPRO | m_CORE2 | m_GENERIC\n+};\n+\n+/* Feature tests against the various architecture variations.  */\n+unsigned int ix86_arch_features[X86_ARCH_LAST] = {\n+  /* X86_ARCH_CMOVE */\n+  m_PPRO | m_GEODE | m_ATHLON_K8_AMDFAM10 | m_PENT4 | m_NOCONA,\n+\n+  /* X86_ARCH_CMPXCHG: Compare and exchange was added for 80486.  */\n+  ~m_386,\n+\n+  /* X86_ARCH_CMPXCHG8B: Compare and exchange 8 bytes was added for pentium. */\n+  ~(m_386 | m_486),\n+\n+  /* X86_ARCH_XADD: Exchange and add was added for 80486.  */\n+  ~m_386,\n+\n+  /* X86_ARCH_BSWAP: Byteswap was added for 80486.  */\n+  ~m_386,\n+};\n+\n+static const unsigned int x86_accumulate_outgoing_args\n+  = m_ATHLON_K8_AMDFAM10 | m_PENT4 | m_NOCONA | m_PPRO | m_CORE2 | m_GENERIC;\n+\n+static const unsigned int x86_arch_always_fancy_math_387\n+  = m_PENT | m_PPRO | m_ATHLON_K8_AMDFAM10 | m_PENT4\n+    | m_NOCONA | m_CORE2 | m_GENERIC;\n \n static enum stringop_alg stringop_alg = no_stringop;\n \n@@ -1397,11 +1431,9 @@ enum fpmath_unit ix86_fpmath;\n \n /* Which cpu are we scheduling for.  */\n enum processor_type ix86_tune;\n-int ix86_tune_mask;\n \n /* Which instruction set architecture to use.  */\n enum processor_type ix86_arch;\n-int ix86_arch_mask;\n \n /* true if sse prefetch instruction is not NOOP.  */\n int x86_prefetch_sse;\n@@ -1811,6 +1843,7 @@ override_options (void)\n {\n   int i;\n   int ix86_tune_defaulted = 0;\n+  unsigned int ix86_arch_mask, ix86_tune_mask;\n \n   /* Comes from final.c -- no real reason to change it.  */\n #define MAX_CODE_ALIGN 16\n@@ -2124,6 +2157,10 @@ override_options (void)\n   if (i == pta_size)\n     error (\"bad value (%s) for -march= switch\", ix86_arch_string);\n \n+  ix86_arch_mask = 1u << ix86_arch;\n+  for (i = 0; i < X86_ARCH_LAST; ++i)\n+    ix86_arch_features[i] &= ix86_arch_mask;\n+\n   for (i = 0; i < pta_size; i++)\n     if (! strcmp (ix86_tune_string, processor_alias_table[i].name))\n       {\n@@ -2155,8 +2192,9 @@ override_options (void)\n   if (i == pta_size)\n     error (\"bad value (%s) for -mtune= switch\", ix86_tune_string);\n \n-  ix86_arch_mask = 1 << ix86_arch;\n-  ix86_tune_mask = 1 << ix86_tune;\n+  ix86_tune_mask = 1u << ix86_tune;\n+  for (i = 0; i < X86_TUNE_LAST; ++i)\n+    ix86_tune_features[i] &= ix86_tune_mask;\n \n   if (optimize_size)\n     ix86_cost = &size_cost;\n@@ -2366,7 +2404,6 @@ override_options (void)\n     error (\"-msseregparm used without SSE enabled\");\n \n   ix86_fpmath = TARGET_FPMATH_DEFAULT;\n-\n   if (ix86_fpmath_string != 0)\n     {\n       if (! strcmp (ix86_fpmath_string, \"387\"))\n@@ -2425,6 +2462,15 @@ override_options (void)\n       target_flags |= MASK_ACCUMULATE_OUTGOING_ARGS;\n     }\n \n+  /* For sane SSE instruction set generation we need fcomi instruction.\n+     It is safe to enable all CMOVE instructions.  */\n+  if (TARGET_SSE)\n+    TARGET_CMOVE = 1;\n+\n+  /* ??? Any idea why this is unconditionally disabled for 64-bit?  */\n+  if (TARGET_64BIT)\n+    TARGET_USE_SAHF = 0;\n+  \n   /* Figure out what ASM_GENERATE_INTERNAL_LABEL builds as a prefix.  */\n   {\n     char *p;\n@@ -4999,7 +5045,7 @@ standard_80387_constant_p (rtx x)\n   /* For XFmode constants, try to find a special 80387 instruction when\n      optimizing for size or on those CPUs that benefit from them.  */\n   if (GET_MODE (x) == XFmode\n-      && (optimize_size || x86_ext_80387_constants & ix86_tune_mask))\n+      && (optimize_size || TARGET_EXT_80387_CONSTANTS))\n     {\n       int i;\n \n@@ -9499,6 +9545,55 @@ ix86_expand_vector_move (enum machine_mode mode, rtx operands[])\n \n /* Implement the movmisalign patterns for SSE.  Non-SSE modes go\n    straight to ix86_expand_vector_move.  */\n+/* Code generation for scalar reg-reg moves of single and double precision data:\n+     if (x86_sse_partial_reg_dependency == true | x86_sse_split_regs == true)\n+       movaps reg, reg\n+     else\n+       movss reg, reg\n+     if (x86_sse_partial_reg_dependency == true)\n+       movapd reg, reg\n+     else\n+       movsd reg, reg\n+\n+   Code generation for scalar loads of double precision data:\n+     if (x86_sse_split_regs == true)\n+       movlpd mem, reg      (gas syntax)\n+     else\n+       movsd mem, reg\n+ \n+   Code generation for unaligned packed loads of single precision data\n+   (x86_sse_unaligned_move_optimal overrides x86_sse_partial_reg_dependency):\n+     if (x86_sse_unaligned_move_optimal)\n+       movups mem, reg\n+\n+     if (x86_sse_partial_reg_dependency == true)\n+       {\n+         xorps  reg, reg\n+         movlps mem, reg\n+         movhps mem+8, reg\n+       }\n+     else\n+       {\n+         movlps mem, reg\n+         movhps mem+8, reg\n+       }\n+\n+   Code generation for unaligned packed loads of double precision data\n+   (x86_sse_unaligned_move_optimal overrides x86_sse_split_regs):\n+     if (x86_sse_unaligned_move_optimal)\n+       movupd mem, reg\n+\n+     if (x86_sse_split_regs == true)\n+       {\n+         movlpd mem, reg\n+         movhpd mem+8, reg\n+       }\n+     else\n+       {\n+         movsd  mem, reg\n+         movhpd mem+8, reg\n+       }\n+ */\n \n void\n ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])"}, {"sha": "8e3032cf5154c76c4aa825e80af6254081a22231", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 159, "deletions": 107, "changes": 266, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/80fd744fdae18292b5cb67cebceea8b750656c40/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/80fd744fdae18292b5cb67cebceea8b750656c40/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=80fd744fdae18292b5cb67cebceea8b750656c40", "patch": "@@ -179,110 +179,165 @@ extern const struct processor_costs *ix86_cost;\n #define TARGET_GENERIC (TARGET_GENERIC32 || TARGET_GENERIC64)\n #define TARGET_AMDFAM10 (ix86_tune == PROCESSOR_AMDFAM10)\n \n-extern const int x86_use_leave, x86_push_memory, x86_zero_extend_with_and;\n-extern const int x86_use_bit_test, x86_cmove, x86_deep_branch;\n-extern const int x86_branch_hints, x86_unroll_strlen;\n-extern const int x86_double_with_add, x86_partial_reg_stall, x86_movx;\n-extern const int x86_use_himode_fiop, x86_use_simode_fiop;\n-extern const int x86_use_mov0, x86_use_cltd, x86_use_xchgb;\n-extern const int x86_read_modify_write, x86_read_modify, x86_split_long_moves;\n-extern const int x86_promote_QImode, x86_single_stringop, x86_fast_prefix;\n-extern const int x86_himode_math, x86_qimode_math, x86_promote_qi_regs;\n-extern const int x86_promote_hi_regs, x86_integer_DFmode_moves;\n-extern const int x86_add_esp_4, x86_add_esp_8, x86_sub_esp_4, x86_sub_esp_8;\n-extern const int x86_partial_reg_dependency, x86_memory_mismatch_stall;\n-extern const int x86_accumulate_outgoing_args, x86_prologue_using_move;\n-extern const int x86_epilogue_using_move, x86_decompose_lea;\n-extern const int x86_arch_always_fancy_math_387, x86_shift1;\n-extern const int x86_sse_partial_reg_dependency, x86_sse_split_regs;\n-extern const int x86_sse_unaligned_move_optimal;\n-extern const int x86_sse_typeless_stores, x86_sse_load0_by_pxor;\n-extern const int x86_use_ffreep;\n-extern const int x86_inter_unit_moves, x86_schedule;\n-extern const int x86_use_bt;\n-extern const int x86_cmpxchg, x86_cmpxchg8b, x86_xadd;\n-extern const int x86_use_incdec;\n-extern const int x86_pad_returns;\n-extern const int x86_bswap;\n-extern const int x86_partial_flag_reg_stall;\n-extern int x86_prefetch_sse, x86_cmpxchg16b;\n-\n-#define TARGET_USE_LEAVE (x86_use_leave & ix86_tune_mask)\n-#define TARGET_PUSH_MEMORY (x86_push_memory & ix86_tune_mask)\n-#define TARGET_ZERO_EXTEND_WITH_AND (x86_zero_extend_with_and & ix86_tune_mask)\n-#define TARGET_USE_BIT_TEST (x86_use_bit_test & ix86_tune_mask)\n-#define TARGET_UNROLL_STRLEN (x86_unroll_strlen & ix86_tune_mask)\n-/* For sane SSE instruction set generation we need fcomi instruction.  It is\n-   safe to enable all CMOVE instructions.  */\n-#define TARGET_CMOVE ((x86_cmove & ix86_arch_mask) || TARGET_SSE)\n-#define TARGET_FISTTP (TARGET_SSE3 && TARGET_80387)\n-#define TARGET_DEEP_BRANCH_PREDICTION (x86_deep_branch & ix86_tune_mask)\n-#define TARGET_BRANCH_PREDICTION_HINTS (x86_branch_hints & ix86_tune_mask)\n-#define TARGET_DOUBLE_WITH_ADD (x86_double_with_add & ix86_tune_mask)\n-#define TARGET_USE_SAHF ((x86_use_sahf & ix86_tune_mask) && !TARGET_64BIT)\n-#define TARGET_MOVX (x86_movx & ix86_tune_mask)\n-#define TARGET_PARTIAL_REG_STALL (x86_partial_reg_stall & ix86_tune_mask)\n-#define TARGET_PARTIAL_FLAG_REG_STALL\t\t\\\n-  (x86_partial_flag_reg_stall & ix86_tune_mask)\n-#define TARGET_USE_HIMODE_FIOP (x86_use_himode_fiop & ix86_tune_mask)\n-#define TARGET_USE_SIMODE_FIOP (x86_use_simode_fiop & ix86_tune_mask)\n-#define TARGET_USE_MOV0 (x86_use_mov0 & ix86_tune_mask)\n-#define TARGET_USE_CLTD (x86_use_cltd & ix86_tune_mask)\n-#define TARGET_USE_XCHGB (x86_use_xchgb & ix86_tune_mask)\n-#define TARGET_SPLIT_LONG_MOVES (x86_split_long_moves & ix86_tune_mask)\n-#define TARGET_READ_MODIFY_WRITE (x86_read_modify_write & ix86_tune_mask)\n-#define TARGET_READ_MODIFY (x86_read_modify & ix86_tune_mask)\n-#define TARGET_PROMOTE_QImode (x86_promote_QImode & ix86_tune_mask)\n-#define TARGET_FAST_PREFIX (x86_fast_prefix & ix86_tune_mask)\n-#define TARGET_SINGLE_STRINGOP (x86_single_stringop & ix86_tune_mask)\n-#define TARGET_QIMODE_MATH (x86_qimode_math & ix86_tune_mask)\n-#define TARGET_HIMODE_MATH (x86_himode_math & ix86_tune_mask)\n-#define TARGET_PROMOTE_QI_REGS (x86_promote_qi_regs & ix86_tune_mask)\n-#define TARGET_PROMOTE_HI_REGS (x86_promote_hi_regs & ix86_tune_mask)\n-#define TARGET_ADD_ESP_4 (x86_add_esp_4 & ix86_tune_mask)\n-#define TARGET_ADD_ESP_8 (x86_add_esp_8 & ix86_tune_mask)\n-#define TARGET_SUB_ESP_4 (x86_sub_esp_4 & ix86_tune_mask)\n-#define TARGET_SUB_ESP_8 (x86_sub_esp_8 & ix86_tune_mask)\n-#define TARGET_INTEGER_DFMODE_MOVES (x86_integer_DFmode_moves & ix86_tune_mask)\n-#define TARGET_PARTIAL_REG_DEPENDENCY\t\t\\\n-  (x86_partial_reg_dependency & ix86_tune_mask)\n-#define TARGET_SSE_PARTIAL_REG_DEPENDENCY\t\t\\\n-  (x86_sse_partial_reg_dependency & ix86_tune_mask)\n-#define TARGET_SSE_UNALIGNED_MOVE_OPTIMAL\t\t\\\n-  (x86_sse_unaligned_move_optimal & ix86_tune_mask)\n-#define TARGET_SSE_SPLIT_REGS (x86_sse_split_regs & ix86_tune_mask)\n-#define TARGET_SSE_TYPELESS_STORES (x86_sse_typeless_stores & ix86_tune_mask)\n-#define TARGET_SSE_LOAD0_BY_PXOR (x86_sse_load0_by_pxor & ix86_tune_mask)\n-#define TARGET_MEMORY_MISMATCH_STALL\t\t\\\n-  (x86_memory_mismatch_stall & ix86_tune_mask)\n-#define TARGET_PROLOGUE_USING_MOVE (x86_prologue_using_move & ix86_tune_mask)\n-#define TARGET_EPILOGUE_USING_MOVE (x86_epilogue_using_move & ix86_tune_mask)\n-#define TARGET_PREFETCH_SSE (x86_prefetch_sse)\n-#define TARGET_SHIFT1 (x86_shift1 & ix86_tune_mask)\n-#define TARGET_USE_FFREEP (x86_use_ffreep & ix86_tune_mask)\n-#define TARGET_INTER_UNIT_MOVES (x86_inter_unit_moves & ix86_tune_mask)\n-#define TARGET_FOUR_JUMP_LIMIT (x86_four_jump_limit & ix86_tune_mask)\n-#define TARGET_SCHEDULE (x86_schedule & ix86_tune_mask)\n-#define TARGET_USE_BT (x86_use_bt & ix86_tune_mask)\n-#define TARGET_USE_INCDEC (x86_use_incdec & ix86_tune_mask)\n-#define TARGET_PAD_RETURNS (x86_pad_returns & ix86_tune_mask)\n-\n-#define ASSEMBLER_DIALECT (ix86_asm_dialect)\n-\n-#define TARGET_SSE_MATH ((ix86_fpmath & FPMATH_SSE) != 0)\n-#define TARGET_MIX_SSE_I387 ((ix86_fpmath & FPMATH_SSE) \\\n-\t\t\t     && (ix86_fpmath & FPMATH_387))\n-\n-#define TARGET_GNU_TLS (ix86_tls_dialect == TLS_DIALECT_GNU)\n-#define TARGET_GNU2_TLS (ix86_tls_dialect == TLS_DIALECT_GNU2)\n-#define TARGET_ANY_GNU_TLS (TARGET_GNU_TLS || TARGET_GNU2_TLS)\n-#define TARGET_SUN_TLS (ix86_tls_dialect == TLS_DIALECT_SUN)\n-\n-#define TARGET_CMPXCHG (x86_cmpxchg & ix86_arch_mask)\n-#define TARGET_CMPXCHG8B (x86_cmpxchg8b & ix86_arch_mask)\n-#define TARGET_CMPXCHG16B (x86_cmpxchg16b)\n-#define TARGET_XADD (x86_xadd & ix86_arch_mask)\n-#define TARGET_BSWAP (x86_bswap & ix86_arch_mask)\n+/* Feature tests against the various tunings.  */\n+enum ix86_tune_indices {\n+  X86_TUNE_USE_LEAVE,\n+  X86_TUNE_PUSH_MEMORY,\n+  X86_TUNE_ZERO_EXTEND_WITH_AND,\n+  X86_TUNE_USE_BIT_TEST,\n+  X86_TUNE_UNROLL_STRLEN,\n+  X86_TUNE_DEEP_BRANCH_PREDICTION,\n+  X86_TUNE_BRANCH_PREDICTION_HINTS,\n+  X86_TUNE_DOUBLE_WITH_ADD,\n+  X86_TUNE_USE_SAHF,\t\t\t/* && !TARGET_64BIT */\n+  X86_TUNE_MOVX,\n+  X86_TUNE_PARTIAL_REG_STALL,\n+  X86_TUNE_PARTIAL_FLAG_REG_STALL,\n+  X86_TUNE_USE_HIMODE_FIOP,\n+  X86_TUNE_USE_SIMODE_FIOP,\n+  X86_TUNE_USE_MOV0,\n+  X86_TUNE_USE_CLTD,\n+  X86_TUNE_USE_XCHGB,\n+  X86_TUNE_SPLIT_LONG_MOVES,\n+  X86_TUNE_READ_MODIFY_WRITE,\n+  X86_TUNE_READ_MODIFY,\n+  X86_TUNE_PROMOTE_QIMODE,\n+  X86_TUNE_FAST_PREFIX,\n+  X86_TUNE_SINGLE_STRINGOP,\n+  X86_TUNE_QIMODE_MATH,\n+  X86_TUNE_HIMODE_MATH,\n+  X86_TUNE_PROMOTE_QI_REGS,\n+  X86_TUNE_PROMOTE_HI_REGS,\n+  X86_TUNE_ADD_ESP_4,\n+  X86_TUNE_ADD_ESP_8,\n+  X86_TUNE_SUB_ESP_4,\n+  X86_TUNE_SUB_ESP_8,\n+  X86_TUNE_INTEGER_DFMODE_MOVES,\n+  X86_TUNE_PARTIAL_REG_DEPENDENCY,\n+  X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY,\n+  X86_TUNE_SSE_UNALIGNED_MOVE_OPTIMAL,\n+  X86_TUNE_SSE_SPLIT_REGS,\n+  X86_TUNE_SSE_TYPELESS_STORES,\n+  X86_TUNE_SSE_LOAD0_BY_PXOR,\n+  X86_TUNE_MEMORY_MISMATCH_STALL,\n+  X86_TUNE_PROLOGUE_USING_MOVE,\n+  X86_TUNE_EPILOGUE_USING_MOVE,\n+  X86_TUNE_SHIFT1,\n+  X86_TUNE_USE_FFREEP,\n+  X86_TUNE_INTER_UNIT_MOVES,\n+  X86_TUNE_FOUR_JUMP_LIMIT,\n+  X86_TUNE_SCHEDULE,\n+  X86_TUNE_USE_BT,\n+  X86_TUNE_USE_INCDEC,\n+  X86_TUNE_PAD_RETURNS,\n+  X86_TUNE_EXT_80387_CONSTANTS,\n+\n+  X86_TUNE_LAST\n+};\n+\n+extern unsigned int ix86_tune_features[X86_TUNE_LAST];\n+\n+#define TARGET_USE_LEAVE\tix86_tune_features[X86_TUNE_USE_LEAVE]\n+#define TARGET_PUSH_MEMORY\tix86_tune_features[X86_TUNE_PUSH_MEMORY]\n+#define TARGET_ZERO_EXTEND_WITH_AND \\\n+\tix86_tune_features[X86_TUNE_ZERO_EXTEND_WITH_AND]\n+#define TARGET_USE_BIT_TEST\tix86_tune_features[X86_TUNE_USE_BIT_TEST]\n+#define TARGET_UNROLL_STRLEN\tix86_tune_features[X86_TUNE_UNROLL_STRLEN]\n+#define TARGET_DEEP_BRANCH_PREDICTION \\\n+\tix86_tune_features[X86_TUNE_DEEP_BRANCH_PREDICTION]\n+#define TARGET_BRANCH_PREDICTION_HINTS \\\n+\tix86_tune_features[X86_TUNE_BRANCH_PREDICTION_HINTS]\n+#define TARGET_DOUBLE_WITH_ADD\tix86_tune_features[X86_TUNE_DOUBLE_WITH_ADD]\n+#define TARGET_USE_SAHF\t\tix86_tune_features[X86_TUNE_USE_SAHF]\n+#define TARGET_MOVX\t\tix86_tune_features[X86_TUNE_MOVX]\n+#define TARGET_PARTIAL_REG_STALL ix86_tune_features[X86_TUNE_PARTIAL_REG_STALL]\n+#define TARGET_PARTIAL_FLAG_REG_STALL \\\n+\tix86_tune_features[X86_TUNE_PARTIAL_FLAG_REG_STALL]\n+#define TARGET_USE_HIMODE_FIOP\tix86_tune_features[X86_TUNE_USE_HIMODE_FIOP]\n+#define TARGET_USE_SIMODE_FIOP\tix86_tune_features[X86_TUNE_USE_SIMODE_FIOP]\n+#define TARGET_USE_MOV0\t\tix86_tune_features[X86_TUNE_USE_MOV0]\n+#define TARGET_USE_CLTD\t\tix86_tune_features[X86_TUNE_USE_CLTD]\n+#define TARGET_USE_XCHGB\tix86_tune_features[X86_TUNE_USE_XCHGB]\n+#define TARGET_SPLIT_LONG_MOVES\tix86_tune_features[X86_TUNE_SPLIT_LONG_MOVES]\n+#define TARGET_READ_MODIFY_WRITE ix86_tune_features[X86_TUNE_READ_MODIFY_WRITE]\n+#define TARGET_READ_MODIFY\tix86_tune_features[X86_TUNE_READ_MODIFY]\n+#define TARGET_PROMOTE_QImode\tix86_tune_features[X86_TUNE_PROMOTE_QIMODE]\n+#define TARGET_FAST_PREFIX\tix86_tune_features[X86_TUNE_FAST_PREFIX]\n+#define TARGET_SINGLE_STRINGOP\tix86_tune_features[X86_TUNE_SINGLE_STRINGOP]\n+#define TARGET_QIMODE_MATH\tix86_tune_features[X86_TUNE_QIMODE_MATH]\n+#define TARGET_HIMODE_MATH\tix86_tune_features[X86_TUNE_HIMODE_MATH]\n+#define TARGET_PROMOTE_QI_REGS\tix86_tune_features[X86_TUNE_PROMOTE_QI_REGS]\n+#define TARGET_PROMOTE_HI_REGS\tix86_tune_features[X86_TUNE_PROMOTE_HI_REGS]\n+#define TARGET_ADD_ESP_4\tix86_tune_features[X86_TUNE_ADD_ESP_4]\n+#define TARGET_ADD_ESP_8\tix86_tune_features[X86_TUNE_ADD_ESP_8]\n+#define TARGET_SUB_ESP_4\tix86_tune_features[X86_TUNE_SUB_ESP_4]\n+#define TARGET_SUB_ESP_8\tix86_tune_features[X86_TUNE_SUB_ESP_8]\n+#define TARGET_INTEGER_DFMODE_MOVES \\\n+\tix86_tune_features[X86_TUNE_INTEGER_DFMODE_MOVES]\n+#define TARGET_PARTIAL_REG_DEPENDENCY \\\n+\tix86_tune_features[X86_TUNE_PARTIAL_REG_DEPENDENCY]\n+#define TARGET_SSE_PARTIAL_REG_DEPENDENCY \\\n+\tix86_tune_features[X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY]\n+#define TARGET_SSE_UNALIGNED_MOVE_OPTIMAL \\\n+\tix86_tune_features[X86_TUNE_SSE_UNALIGNED_MOVE_OPTIMAL]\n+#define TARGET_SSE_SPLIT_REGS\tix86_tune_features[X86_TUNE_SSE_SPLIT_REGS]\n+#define TARGET_SSE_TYPELESS_STORES \\\n+\tix86_tune_features[X86_TUNE_SSE_TYPELESS_STORES]\n+#define TARGET_SSE_LOAD0_BY_PXOR ix86_tune_features[X86_TUNE_SSE_LOAD0_BY_PXOR]\n+#define TARGET_MEMORY_MISMATCH_STALL \\\n+\tix86_tune_features[X86_TUNE_MEMORY_MISMATCH_STALL]\n+#define TARGET_PROLOGUE_USING_MOVE \\\n+\tix86_tune_features[X86_TUNE_PROLOGUE_USING_MOVE]\n+#define TARGET_EPILOGUE_USING_MOVE \\\n+\tix86_tune_features[X86_TUNE_EPILOGUE_USING_MOVE]\n+#define TARGET_SHIFT1\t\tix86_tune_features[X86_TUNE_SHIFT1]\n+#define TARGET_USE_FFREEP\tix86_tune_features[X86_TUNE_USE_FFREEP]\n+#define TARGET_INTER_UNIT_MOVES\tix86_tune_features[X86_TUNE_INTER_UNIT_MOVES]\n+#define TARGET_FOUR_JUMP_LIMIT\tix86_tune_features[X86_TUNE_FOUR_JUMP_LIMIT]\n+#define TARGET_SCHEDULE\t\tix86_tune_features[X86_TUNE_SCHEDULE]\n+#define TARGET_USE_BT\t\tix86_tune_features[X86_TUNE_USE_BT]\n+#define TARGET_USE_INCDEC\tix86_tune_features[X86_TUNE_USE_INCDEC]\n+#define TARGET_PAD_RETURNS\tix86_tune_features[X86_TUNE_PAD_RETURNS]\n+#define TARGET_EXT_80387_CONSTANTS \\\n+\tix86_tune_features[X86_TUNE_EXT_80387_CONSTANTS]\n+\n+/* Feature tests against the various architecture variations.  */\n+enum ix86_arch_indices {\n+  X86_ARCH_CMOVE,\t\t/* || TARGET_SSE */\n+  X86_ARCH_CMPXCHG,\n+  X86_ARCH_CMPXCHG8B,\n+  X86_ARCH_XADD,\n+  X86_ARCH_BSWAP,\n+\n+  X86_ARCH_LAST\n+};\n+  \n+extern unsigned int ix86_arch_features[X86_ARCH_LAST];\n+\n+#define TARGET_CMOVE\t\tix86_arch_features[X86_ARCH_CMOVE]\n+#define TARGET_CMPXCHG\t\tix86_arch_features[X86_ARCH_CMPXCHG]\n+#define TARGET_CMPXCHG8B\tix86_arch_features[X86_ARCH_CMPXCHG8B]\n+#define TARGET_XADD\t\tix86_arch_features[X86_ARCH_XADD]\n+#define TARGET_BSWAP\t\tix86_arch_features[X86_ARCH_BSWAP]\n+\n+#define TARGET_FISTTP\t\t(TARGET_SSE3 && TARGET_80387)\n+\n+extern int x86_prefetch_sse;\n+#define TARGET_PREFETCH_SSE\tx86_prefetch_sse\n+\n+extern int x86_cmpxchg16b;\n+#define TARGET_CMPXCHG16B\tx86_cmpxchg16b\n+\n+#define ASSEMBLER_DIALECT\t(ix86_asm_dialect)\n+\n+#define TARGET_SSE_MATH\t\t((ix86_fpmath & FPMATH_SSE) != 0)\n+#define TARGET_MIX_SSE_I387 \\\n+ ((ix86_fpmath & (FPMATH_SSE | FPMATH_387)) == (FPMATH_SSE | FPMATH_387))\n+\n+#define TARGET_GNU_TLS\t\t(ix86_tls_dialect == TLS_DIALECT_GNU)\n+#define TARGET_GNU2_TLS\t\t(ix86_tls_dialect == TLS_DIALECT_GNU2)\n+#define TARGET_ANY_GNU_TLS\t(TARGET_GNU_TLS || TARGET_GNU2_TLS)\n+#define TARGET_SUN_TLS\t\t(ix86_tls_dialect == TLS_DIALECT_SUN)\n \n #ifndef TARGET_64BIT_DEFAULT\n #define TARGET_64BIT_DEFAULT 0\n@@ -2132,10 +2187,7 @@ enum processor_type\n };\n \n extern enum processor_type ix86_tune;\n-extern int ix86_tune_mask;\n-\n extern enum processor_type ix86_arch;\n-extern int ix86_arch_mask;\n \n enum fpmath_unit\n {"}]}