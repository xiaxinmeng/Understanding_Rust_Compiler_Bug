{"sha": "5bea0c6c8c3deb0b52aec49434484b35e75293f2", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NWJlYTBjNmM4YzNkZWIwYjUyYWVjNDk0MzQ0ODRiMzVlNzUyOTNmMg==", "commit": {"author": {"name": "Kyrylo Tkachov", "email": "kyrylo.tkachov@arm.com", "date": "2013-10-03T13:58:42Z"}, "committer": {"name": "Kyrylo Tkachov", "email": "ktkachov@gcc.gnu.org", "date": "2013-10-03T13:58:42Z"}, "message": "aarch-common-protos.h (struct alu_cost_table): New.\n\n[gcc/]\n2013-10-03  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>\n            Richard Earnshaw  <richard.earnshaw@arm.com>\n\n\t* config/arm/aarch-common-protos.h (struct alu_cost_table): New.\n\t(struct mult_cost_table): Likewise.\n\t(struct mem_cost_table): Likewise.\n\t(struct fp_cost_table): Likewise.\n\t(struct vector_cost_table): Likewise.\n\t(cpu_cost_table): Likewise.\n\t* config/arm/arm.opt (mold-rts-costs): New option.\n\t(mnew-generic-costs): Likewise.\n\t* config/arm/arm.c (generic_extra_costs): New table.\n\t(cortexa15_extra_costs): Likewise.\n\t(arm_slowmul_tune): Use NULL as new costs.\n\t(arm_fastmul_tune): Likewise.\n\t(arm_strongarm_tune): Likewise.\n\t(arm_xscale_tune): Likewise.\n\t(arm_9e_tune): Likewise.\n\t(arm_v6t2_tune): Likewise.\n\t(arm_cortex_a5_tune): Likewise.\n\t(arm_cortex_a9_tune): Likewise.\n\t(arm_v6m_tune): Likewise.\n\t(arm_fa726te_tune): Likewise.\n\t(arm_cortex_a15_tune): Use cortex15_extra_costs.\n\t(arm_cortex_tune): Use generict_extra_costs.\n\t(shifter_op_p): New function.\n\t(arm_unspec_cost): Likewise.\n\t(LIBCALL_COST): Define.\n\t(arm_new_rtx_costs): New function.\n\t(arm_rtx_costs): Use arm_new_rtx_costs when core-specific\n\ttable is available. Use old costs otherwise unless mnew-generic-costs\n\tis specified.\n\t* config/arm/arm-protos.h (tune_params): Add insn_extra_cost field.\n\t(cpu_cost_table): Declare.\n\nCo-Authored-By: Richard Earnshaw <rearnsha@arm.com>\n\nFrom-SVN: r203160", "tree": {"sha": "d5f7a0a26c792204a9e44ba13b0f5b1f3f440052", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d5f7a0a26c792204a9e44ba13b0f5b1f3f440052"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/5bea0c6c8c3deb0b52aec49434484b35e75293f2", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5bea0c6c8c3deb0b52aec49434484b35e75293f2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5bea0c6c8c3deb0b52aec49434484b35e75293f2", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5bea0c6c8c3deb0b52aec49434484b35e75293f2/comments", "author": {"login": "ktkachov-arm", "id": 74917949, "node_id": "MDQ6VXNlcjc0OTE3OTQ5", "avatar_url": "https://avatars.githubusercontent.com/u/74917949?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ktkachov-arm", "html_url": "https://github.com/ktkachov-arm", "followers_url": "https://api.github.com/users/ktkachov-arm/followers", "following_url": "https://api.github.com/users/ktkachov-arm/following{/other_user}", "gists_url": "https://api.github.com/users/ktkachov-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/ktkachov-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ktkachov-arm/subscriptions", "organizations_url": "https://api.github.com/users/ktkachov-arm/orgs", "repos_url": "https://api.github.com/users/ktkachov-arm/repos", "events_url": "https://api.github.com/users/ktkachov-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/ktkachov-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "24c569251d2a19a0a75af13d7fc85802ad574801", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/24c569251d2a19a0a75af13d7fc85802ad574801", "html_url": "https://github.com/Rust-GCC/gccrs/commit/24c569251d2a19a0a75af13d7fc85802ad574801"}], "stats": {"total": 2069, "additions": 2057, "deletions": 12}, "files": [{"sha": "915d76931857fcb2d1024b095eda22495c5b0e96", "filename": "gcc/ChangeLog", "status": "modified", "additions": 35, "deletions": 0, "changes": 35, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5bea0c6c8c3deb0b52aec49434484b35e75293f2/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5bea0c6c8c3deb0b52aec49434484b35e75293f2/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=5bea0c6c8c3deb0b52aec49434484b35e75293f2", "patch": "@@ -1,3 +1,38 @@\n+2013-10-03  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>\n+            Richard Earnshaw  <richard.earnshaw@arm.com>\n+\n+\t* config/arm/aarch-common-protos.h (struct alu_cost_table): New.\n+\t(struct mult_cost_table): Likewise.\n+\t(struct mem_cost_table): Likewise.\n+\t(struct fp_cost_table): Likewise.\n+\t(struct vector_cost_table): Likewise.\n+\t(cpu_cost_table): Likewise.\n+\t* config/arm/arm.opt (mold-rts-costs): New option.\n+\t(mnew-generic-costs): Likewise.\n+\t* config/arm/arm.c (generic_extra_costs): New table.\n+\t(cortexa15_extra_costs): Likewise.\n+\t(arm_slowmul_tune): Use NULL as new costs.\n+\t(arm_fastmul_tune): Likewise.\n+\t(arm_strongarm_tune): Likewise.\n+\t(arm_xscale_tune): Likewise.\n+\t(arm_9e_tune): Likewise.\n+\t(arm_v6t2_tune): Likewise.\n+\t(arm_cortex_a5_tune): Likewise.\n+\t(arm_cortex_a9_tune): Likewise.\n+\t(arm_v6m_tune): Likewise.\n+\t(arm_fa726te_tune): Likewise.\n+\t(arm_cortex_a15_tune): Use cortex15_extra_costs.\n+\t(arm_cortex_tune): Use generict_extra_costs.\n+\t(shifter_op_p): New function.\n+\t(arm_unspec_cost): Likewise.\n+\t(LIBCALL_COST): Define.\n+\t(arm_new_rtx_costs): New function.\n+\t(arm_rtx_costs): Use arm_new_rtx_costs when core-specific\n+\ttable is available. Use old costs otherwise unless mnew-generic-costs\n+\tis specified.\n+\t* config/arm/arm-protos.h (tune_params): Add insn_extra_cost field.\n+\t(cpu_cost_table): Declare.\n+\n 2013-10-03  Marcus Shawcroft  <marcus.shawcroft@arm.com>\n \n \tPR target/58460"}, {"sha": "841f544e83d0a995440c7ef2dabc1a7f37eaf07a", "filename": "gcc/config/arm/aarch-common-protos.h", "status": "modified", "additions": 99, "deletions": 2, "changes": 101, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5bea0c6c8c3deb0b52aec49434484b35e75293f2/gcc%2Fconfig%2Farm%2Faarch-common-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5bea0c6c8c3deb0b52aec49434484b35e75293f2/gcc%2Fconfig%2Farm%2Faarch-common-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Faarch-common-protos.h?ref=5bea0c6c8c3deb0b52aec49434484b35e75293f2", "patch": "@@ -1,5 +1,4 @@\n-/* Function prototypes for instruction scheduling dependeoncy routines,\n-   defined in aarch-common.c\n+/* Functions and structures shared between arm and aarch64.\n \n    Copyright (C) 1991-2013 Free Software Foundation, Inc.\n    Contributed by ARM Ltd.\n@@ -33,4 +32,102 @@ extern int arm_no_early_alu_shift_value_dep (rtx, rtx);\n extern int arm_no_early_mul_dep (rtx, rtx);\n extern int arm_no_early_store_addr_dep (rtx, rtx);\n \n+/* RTX cost table definitions.  These are used when tuning for speed rather\n+   than for size and should reflect the _additional_ cost over the cost\n+   of the fastest instruction in the machine, which is COSTS_N_INSNS (1).\n+   Therefore it's okay for some costs to be 0.\n+   Costs may not have a negative value.  */\n+struct alu_cost_table\n+{\n+  const int arith;\t\t/* ADD/SUB.  */\n+  const int logical;\t\t/* AND/ORR/EOR/BIC, etc.  */\n+  const int shift;\t\t/* Simple shift.  */\n+  const int shift_reg;\t\t/* Simple shift by reg.  */\n+  const int arith_shift;\t/* Additional when arith also shifts...  */\n+  const int arith_shift_reg;\t/* ... and when the shift is by a reg.  */\n+  const int log_shift;\t\t/* Additional when logic also shifts...  */\n+  const int log_shift_reg;\t/* ... and when the shift is by a reg.  */\n+  const int extnd;\t\t/* Zero/sign extension.  */\n+  const int extnd_arith;\t/* Extend and arith.  */\n+  const int bfi;\t\t/* Bit-field insert.  */\n+  const int bfx;\t\t/* Bit-field extraction.  */\n+  const int clz;\t\t/* Count Leading Zeros.  */\n+  const int non_exec;\t\t/* Extra cost when not executing insn.  */\n+  const bool non_exec_costs_exec; /* True if non-execution must add the exec\n+\t\t\t\t     cost.  */\n+};\n+\n+struct mult_cost_table\n+{\n+  const int simple;\n+  const int flag_setting;\t/* Additional cost if multiply sets flags. */\n+  const int extend;\n+  const int add;\n+  const int extend_add;\n+  const int idiv;\n+};\n+\n+/* Calculations of LDM costs are complex.  We assume an initial cost\n+   (ldm_1st) which will load the number of registers mentioned in\n+   ldm_regs_per_insn_1st registers; then each additional\n+   ldm_regs_per_insn_subsequent registers cost one more insn.\n+   Similarly for STM operations.\n+   Therefore the ldm_regs_per_insn_1st/stm_regs_per_insn_1st and\n+   ldm_regs_per_insn_subsequent/stm_regs_per_insn_subsequent fields indicate\n+   the number of registers loaded/stored and are expressed by a simple integer\n+   and not by a COSTS_N_INSNS (N) expression.\n+   */\n+struct mem_cost_table\n+{\n+  const int load;\n+  const int load_sign_extend;\t/* Additional to load cost.  */\n+  const int ldrd;\t\t/* Cost of LDRD.  */\n+  const int ldm_1st;\n+  const int ldm_regs_per_insn_1st;\n+  const int ldm_regs_per_insn_subsequent;\n+  const int loadf;\t\t/* SFmode.  */\n+  const int loadd;\t\t/* DFmode.  */\n+  const int load_unaligned;\t/* Extra for unaligned loads.  */\n+  const int store;\n+  const int strd;\n+  const int stm_1st;\n+  const int stm_regs_per_insn_1st;\n+  const int stm_regs_per_insn_subsequent;\n+  const int storef;\t\t/* SFmode.  */\n+  const int stored;\t\t/* DFmode.  */\n+  const int store_unaligned;\t/* Extra for unaligned stores.  */\n+};\n+\n+struct fp_cost_table\n+{\n+  const int div;\n+  const int mult;\n+  const int mult_addsub;\t/* Non-fused.  */\n+  const int fma;\t\t/* Fused.  */\n+  const int addsub;\n+  const int fpconst;\t\t/* Immediate.  */\n+  const int neg;\t\t/* NEG and ABS.  */\n+  const int compare;\n+  const int widen;\t\t/* Widen to this size.  */\n+  const int narrow;\t\t/* Narrow from this size.  */\n+  const int toint;\n+  const int fromint;\n+  const int roundint;\t\t/* V8 round to integral, remains FP format.  */\n+};\n+\n+struct vector_cost_table\n+{\n+  const int alu;\n+};\n+\n+struct cpu_cost_table\n+{\n+  const struct alu_cost_table alu;\n+  const struct mult_cost_table mult[2]; /* SImode and DImode.  */\n+  const struct mem_cost_table ldst;\n+  const struct fp_cost_table fp[2]; /* SFmode and DFmode.  */\n+  const struct vector_cost_table vect;\n+};\n+\n+\n #endif /* GCC_AARCH_COMMON_PROTOS_H */"}, {"sha": "944cf100d9a17c55b0934f0294b0d82ebeb12f82", "filename": "gcc/config/arm/arm-protos.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5bea0c6c8c3deb0b52aec49434484b35e75293f2/gcc%2Fconfig%2Farm%2Farm-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5bea0c6c8c3deb0b52aec49434484b35e75293f2/gcc%2Fconfig%2Farm%2Farm-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm-protos.h?ref=5bea0c6c8c3deb0b52aec49434484b35e75293f2", "patch": "@@ -246,9 +246,12 @@ struct cpu_vec_costs {\n #ifdef RTX_CODE\n /* This needs to be here because we need RTX_CODE and similar.  */\n \n+struct cpu_cost_table;\n+\n struct tune_params\n {\n   bool (*rtx_costs) (rtx, RTX_CODE, RTX_CODE, int *, bool);\n+  const struct cpu_cost_table *insn_extra_cost;\n   bool (*sched_adjust_cost) (rtx, rtx, rtx, int *);\n   int constant_limit;\n   /* Maximum number of instructions to conditionalise.  */"}, {"sha": "42f3f478308e3d421f5362bce1d463af9127f695", "filename": "gcc/config/arm/arm.c", "status": "modified", "additions": 1912, "deletions": 10, "changes": 1922, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5bea0c6c8c3deb0b52aec49434484b35e75293f2/gcc%2Fconfig%2Farm%2Farm.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5bea0c6c8c3deb0b52aec49434484b35e75293f2/gcc%2Fconfig%2Farm%2Farm.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.c?ref=5bea0c6c8c3deb0b52aec49434484b35e75293f2", "patch": "@@ -945,10 +945,212 @@ struct cpu_vec_costs arm_default_vec_cost = {\n   1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n };\n \n+\n+const struct cpu_cost_table generic_extra_costs =\n+{\n+  /* ALU */\n+  {\n+    0,\t\t\t/* Arith.  */\n+    0,\t\t\t/* Logical.  */\n+    0,\t\t\t/* Shift.  */\n+    COSTS_N_INSNS (1),\t/* Shift_reg.  */\n+    0,\t\t\t/* Arith_shift.  */\n+    COSTS_N_INSNS (1),\t/* Arith_shift_reg.  */\n+    0,\t\t\t/* Log_shift.  */\n+    COSTS_N_INSNS (1),\t/* Log_shift_reg.  */\n+    0,\t\t\t/* Extend.  */\n+    COSTS_N_INSNS (1),\t/* Extend_arith.  */\n+    0,\t\t\t/* Bfi.  */\n+    0,\t\t\t/* Bfx.  */\n+    0,\t\t\t/* Clz.  */\n+    COSTS_N_INSNS (1),\t/* non_exec.  */\n+    false\t\t/* non_exec_costs_exec.  */\n+  },\n+  /* MULT SImode */\n+  {\n+    {\n+      COSTS_N_INSNS (2),\t/* Simple.  */\n+      COSTS_N_INSNS (1),\t/* Flag_setting.  */\n+      COSTS_N_INSNS (2),\t/* Extend.  */\n+      COSTS_N_INSNS (3),\t/* Add.  */\n+      COSTS_N_INSNS (3),\t/* Extend_add.  */\n+      COSTS_N_INSNS (8)\t\t/* Idiv.  */\n+    },\n+    /* MULT DImode */\n+    {\n+      0,\t\t\t/* Simple (N/A).  */\n+      0,\t\t\t/* Flag_setting (N/A).  */\n+      COSTS_N_INSNS (2),\t/* Extend.  */\n+      0,\t\t\t/* Add (N/A).  */\n+      COSTS_N_INSNS (3),\t/* Extend_add.  */\n+      0\t\t\t\t/* Idiv (N/A).  */\n+    }\n+  },\n+  /* LD/ST */\n+  {\n+    COSTS_N_INSNS (2),\t/* Load.  */\n+    COSTS_N_INSNS (2),\t/* Load_sign_extend.  */\n+    COSTS_N_INSNS (3),\t/* Ldrd.  */\n+    COSTS_N_INSNS (2),\t/* Ldm_1st.  */\n+    1,\t\t\t/* Ldm_regs_per_insn_1st.  */\n+    1,\t\t\t/* Ldm_regs_per_insn_subsequent.  */\n+    COSTS_N_INSNS (2),\t/* Loadf.  */\n+    COSTS_N_INSNS (3),\t/* Loadd.  */\n+    COSTS_N_INSNS (1),  /* Load_unaligned.  */\n+    COSTS_N_INSNS (2),\t/* Store.  */\n+    COSTS_N_INSNS (3),\t/* Strd.  */\n+    COSTS_N_INSNS (2),\t/* Stm_1st.  */\n+    1,\t\t\t/* Stm_regs_per_insn_1st.  */\n+    1,\t\t\t/* Stm_regs_per_insn_subsequent.  */\n+    COSTS_N_INSNS (2),\t/* Storef.  */\n+    COSTS_N_INSNS (3),\t/* Stored.  */\n+    COSTS_N_INSNS (1)  /* Store_unaligned.  */\n+  },\n+  {\n+    /* FP SFmode */\n+    {\n+      COSTS_N_INSNS (7),\t/* Div.  */\n+      COSTS_N_INSNS (2),\t/* Mult.  */\n+      COSTS_N_INSNS (3),\t/* Mult_addsub.  */\n+      COSTS_N_INSNS (3),\t/* Fma.  */\n+      COSTS_N_INSNS (1),\t/* Addsub.  */\n+      0,\t\t\t/* Fpconst.  */\n+      0,\t\t\t/* Neg.  */\n+      0,\t\t\t/* Compare.  */\n+      0,\t\t\t/* Widen.  */\n+      0,\t\t\t/* Narrow.  */\n+      0,\t\t\t/* Toint.  */\n+      0,\t\t\t/* Fromint.  */\n+      0\t\t\t\t/* Roundint.  */\n+    },\n+    /* FP DFmode */\n+    {\n+      COSTS_N_INSNS (15),\t/* Div.  */\n+      COSTS_N_INSNS (5),\t/* Mult.  */\n+      COSTS_N_INSNS (7),\t/* Mult_addsub.  */\n+      COSTS_N_INSNS (7),\t/* Fma.  */\n+      COSTS_N_INSNS (3),\t/* Addsub.  */\n+      0,\t\t\t/* Fpconst.  */\n+      0,\t\t\t/* Neg.  */\n+      0,\t\t\t/* Compare.  */\n+      0,\t\t\t/* Widen.  */\n+      0,\t\t\t/* Narrow.  */\n+      0,\t\t\t/* Toint.  */\n+      0,\t\t\t/* Fromint.  */\n+      0\t\t\t\t/* Roundint.  */\n+    }\n+  },\n+  /* Vector */\n+  {\n+    COSTS_N_INSNS (1)\t/* Alu.  */\n+  }\n+};\n+\n+const struct cpu_cost_table cortexa15_extra_costs =\n+{\n+  /* ALU */\n+  {\n+    COSTS_N_INSNS (1),\t/* Arith.  */\n+    COSTS_N_INSNS (1),\t/* Logical.  */\n+    COSTS_N_INSNS (1),\t/* Shift.  */\n+    COSTS_N_INSNS (1),\t/* Shift_reg.  */\n+    COSTS_N_INSNS (1),\t/* Arith_shift.  */\n+    COSTS_N_INSNS (1),\t/* Arith_shift_reg.  */\n+    COSTS_N_INSNS (1),\t/* Log_shift.  */\n+    COSTS_N_INSNS (1),\t/* Log_shift_reg.  */\n+    COSTS_N_INSNS (1),\t/* Extend.  */\n+    COSTS_N_INSNS (2),\t/* Extend_arith.  */\n+    COSTS_N_INSNS (2),\t/* Bfi.  */\n+    COSTS_N_INSNS (1),\t/* Bfx.  */\n+    COSTS_N_INSNS (1),\t/* Clz.  */\n+    COSTS_N_INSNS (1),\t/* non_exec.  */\n+    true\t\t/* non_exec_costs_exec.  */\n+  },\n+  /* MULT SImode */\n+  {\n+    {\n+      COSTS_N_INSNS (3),\t/* Simple.  */\n+      COSTS_N_INSNS (4),\t/* Flag_setting.  */\n+      COSTS_N_INSNS (3),\t/* Extend.  */\n+      COSTS_N_INSNS (4),\t/* Add.  */\n+      COSTS_N_INSNS (4),\t/* Extend_add.  */\n+      COSTS_N_INSNS (19)\t/* Idiv.  */\n+    },\n+    /* MULT DImode */\n+    {\n+      0,\t\t\t/* Simple (N/A).  */\n+      0,\t\t\t/* Flag_setting (N/A).  */\n+      COSTS_N_INSNS (4),\t/* Extend.  */\n+      0,\t\t\t/* Add (N/A).  */\n+      COSTS_N_INSNS (6),\t/* Extend_add.  */\n+      0\t\t\t\t/* Idiv (N/A).  */\n+    }\n+  },\n+  /* LD/ST */\n+  {\n+    COSTS_N_INSNS (4),\t/* Load.  */\n+    COSTS_N_INSNS (4),\t/* Load_sign_extend.  */\n+    COSTS_N_INSNS (4),\t/* Ldrd.  */\n+    COSTS_N_INSNS (5),\t/* Ldm_1st.  */\n+    1,\t\t\t/* Ldm_regs_per_insn_1st.  */\n+    2,\t\t\t/* Ldm_regs_per_insn_subsequent.  */\n+    COSTS_N_INSNS (5),\t/* Loadf.  */\n+    COSTS_N_INSNS (5),\t/* Loadd.  */\n+    COSTS_N_INSNS (1),  /* Load_unaligned.  */\n+    COSTS_N_INSNS (1),\t/* Store.  */\n+    COSTS_N_INSNS (1),\t/* Strd.  */\n+    COSTS_N_INSNS (2),\t/* Stm_1st.  */\n+    1,\t\t\t/* Stm_regs_per_insn_1st.  */\n+    2,\t\t\t/* Stm_regs_per_insn_subsequent.  */\n+    COSTS_N_INSNS (1),\t/* Storef.  */\n+    COSTS_N_INSNS (1),\t/* Stored.  */\n+    COSTS_N_INSNS (1)\t/* Store_unaligned.  */\n+  },\n+  {\n+    /* FP SFmode */\n+    {\n+      COSTS_N_INSNS (18),\t/* Div.  */\n+      COSTS_N_INSNS (5),\t/* Mult.  */\n+      COSTS_N_INSNS (3),\t/* Mult_addsub. */\n+      COSTS_N_INSNS (13),\t/* Fma.  */\n+      COSTS_N_INSNS (5),\t/* Addsub.  */\n+      COSTS_N_INSNS (5),\t/* Fpconst. */\n+      COSTS_N_INSNS (3),\t/* Neg.  */\n+      COSTS_N_INSNS (3),\t/* Compare.  */\n+      COSTS_N_INSNS (3),\t/* Widen.  */\n+      COSTS_N_INSNS (3),\t/* Narrow.  */\n+      COSTS_N_INSNS (3),\t/* Toint.  */\n+      COSTS_N_INSNS (3),\t/* Fromint.  */\n+      COSTS_N_INSNS (3)\t\t/* Roundint.  */\n+    },\n+    /* FP DFmode */\n+    {\n+      COSTS_N_INSNS (32),\t/* Div.  */\n+      COSTS_N_INSNS (5),\t/* Mult.  */\n+      COSTS_N_INSNS (3),\t/* Mult_addsub.  */\n+      COSTS_N_INSNS (13),\t/* Fma.  */\n+      COSTS_N_INSNS (5),\t/* Addsub.  */\n+      COSTS_N_INSNS (3),\t/* Fpconst.  */\n+      COSTS_N_INSNS (3),\t/* Neg.  */\n+      COSTS_N_INSNS (3),\t/* Compare.  */\n+      COSTS_N_INSNS (3),\t/* Widen.  */\n+      COSTS_N_INSNS (3),\t/* Narrow.  */\n+      COSTS_N_INSNS (3),\t/* Toint.  */\n+      COSTS_N_INSNS (3),\t/* Fromint.  */\n+      COSTS_N_INSNS (3)\t\t/* Roundint.  */\n+    }\n+  },\n+  /* Vector */\n+  {\n+    COSTS_N_INSNS (1)\t/* Alu.  */\n+  }\n+};\n+\n const struct tune_params arm_slowmul_tune =\n {\n   arm_slowmul_rtx_costs,\n   NULL,\n+  NULL,\n   3,\t\t\t\t\t\t/* Constant limit.  */\n   5,\t\t\t\t\t\t/* Max cond insns.  */\n   ARM_PREFETCH_NOT_BENEFICIAL,\n@@ -964,6 +1166,7 @@ const struct tune_params arm_fastmul_tune =\n {\n   arm_fastmul_rtx_costs,\n   NULL,\n+  NULL,\n   1,\t\t\t\t\t\t/* Constant limit.  */\n   5,\t\t\t\t\t\t/* Max cond insns.  */\n   ARM_PREFETCH_NOT_BENEFICIAL,\n@@ -982,6 +1185,7 @@ const struct tune_params arm_strongarm_tune =\n {\n   arm_fastmul_rtx_costs,\n   NULL,\n+  NULL,\n   1,\t\t\t\t\t\t/* Constant limit.  */\n   3,\t\t\t\t\t\t/* Max cond insns.  */\n   ARM_PREFETCH_NOT_BENEFICIAL,\n@@ -996,6 +1200,7 @@ const struct tune_params arm_strongarm_tune =\n const struct tune_params arm_xscale_tune =\n {\n   arm_xscale_rtx_costs,\n+  NULL,\n   xscale_sched_adjust_cost,\n   2,\t\t\t\t\t\t/* Constant limit.  */\n   3,\t\t\t\t\t\t/* Max cond insns.  */\n@@ -1012,6 +1217,7 @@ const struct tune_params arm_9e_tune =\n {\n   arm_9e_rtx_costs,\n   NULL,\n+  NULL,\n   1,\t\t\t\t\t\t/* Constant limit.  */\n   5,\t\t\t\t\t\t/* Max cond insns.  */\n   ARM_PREFETCH_NOT_BENEFICIAL,\n@@ -1027,6 +1233,7 @@ const struct tune_params arm_v6t2_tune =\n {\n   arm_9e_rtx_costs,\n   NULL,\n+  NULL,\n   1,\t\t\t\t\t\t/* Constant limit.  */\n   5,\t\t\t\t\t\t/* Max cond insns.  */\n   ARM_PREFETCH_NOT_BENEFICIAL,\n@@ -1042,6 +1249,7 @@ const struct tune_params arm_v6t2_tune =\n const struct tune_params arm_cortex_tune =\n {\n   arm_9e_rtx_costs,\n+  &generic_extra_costs,\n   NULL,\n   1,\t\t\t\t\t\t/* Constant limit.  */\n   5,\t\t\t\t\t\t/* Max cond insns.  */\n@@ -1057,6 +1265,7 @@ const struct tune_params arm_cortex_tune =\n const struct tune_params arm_cortex_a15_tune =\n {\n   arm_9e_rtx_costs,\n+  &cortexa15_extra_costs,\n   NULL,\n   1,\t\t\t\t\t\t/* Constant limit.  */\n   2,\t\t\t\t\t\t/* Max cond insns.  */\n@@ -1076,6 +1285,7 @@ const struct tune_params arm_cortex_a5_tune =\n {\n   arm_9e_rtx_costs,\n   NULL,\n+  NULL,\n   1,\t\t\t\t\t\t/* Constant limit.  */\n   1,\t\t\t\t\t\t/* Max cond insns.  */\n   ARM_PREFETCH_NOT_BENEFICIAL,\n@@ -1090,6 +1300,7 @@ const struct tune_params arm_cortex_a5_tune =\n const struct tune_params arm_cortex_a9_tune =\n {\n   arm_9e_rtx_costs,\n+  NULL,\n   cortex_a9_sched_adjust_cost,\n   1,\t\t\t\t\t\t/* Constant limit.  */\n   5,\t\t\t\t\t\t/* Max cond insns.  */\n@@ -1108,6 +1319,7 @@ const struct tune_params arm_v6m_tune =\n {\n   arm_9e_rtx_costs,\n   NULL,\n+  NULL,\n   1,\t\t\t\t\t\t/* Constant limit.  */\n   5,\t\t\t\t\t\t/* Max cond insns.  */\n   ARM_PREFETCH_NOT_BENEFICIAL,\n@@ -1122,6 +1334,7 @@ const struct tune_params arm_v6m_tune =\n const struct tune_params arm_fa726te_tune =\n {\n   arm_9e_rtx_costs,\n+  NULL,\n   fa726te_sched_adjust_cost,\n   1,\t\t\t\t\t\t/* Constant limit.  */\n   5,\t\t\t\t\t\t/* Max cond insns.  */\n@@ -8291,18 +8504,1707 @@ arm_size_rtx_costs (rtx x, enum rtx_code code, enum rtx_code outer_code,\n     }\n }\n \n-/* RTX costs when optimizing for size.  */\n+/* Helper function for arm_rtx_costs.  If the operand is a valid shift\n+   operand, then return the operand that is being shifted.  If the shift\n+   is not by a constant, then set SHIFT_REG to point to the operand.\n+   Return NULL if OP is not a shifter operand.  */\n+static rtx\n+shifter_op_p (rtx op, rtx *shift_reg)\n+{\n+  enum rtx_code code = GET_CODE (op);\n+\n+  if (code == MULT && CONST_INT_P (XEXP (op, 1))\n+      && exact_log2 (INTVAL (XEXP (op, 1))) > 0)\n+    return XEXP (op, 0);\n+  else if (code == ROTATE && CONST_INT_P (XEXP (op, 1)))\n+    return XEXP (op, 0);\n+  else if (code == ROTATERT || code == ASHIFT || code == LSHIFTRT\n+\t   || code == ASHIFTRT)\n+    {\n+      if (!CONST_INT_P (XEXP (op, 1)))\n+\t*shift_reg = XEXP (op, 1);\n+      return XEXP (op, 0);\n+    }\n+\n+  return NULL;\n+}\n+\n static bool\n-arm_rtx_costs (rtx x, int code, int outer_code, int opno ATTRIBUTE_UNUSED,\n-\t       int *total, bool speed)\n+arm_unspec_cost (rtx x, enum rtx_code /* outer_code */, bool speed_p, int *cost)\n {\n-  if (!speed)\n-    return arm_size_rtx_costs (x, (enum rtx_code) code,\n-\t\t\t       (enum rtx_code) outer_code, total);\n-  else\n-    return current_tune->rtx_costs (x, (enum rtx_code) code,\n-\t\t\t\t    (enum rtx_code) outer_code,\n-\t\t\t\t    total, speed);\n+  const struct cpu_cost_table *extra_cost = current_tune->insn_extra_cost;\n+  gcc_assert (GET_CODE (x) == UNSPEC);\n+\n+  switch (XINT (x, 1))\n+    {\n+    case UNSPEC_UNALIGNED_LOAD:\n+      /* We can only do unaligned loads into the integer unit, and we can't\n+\t use LDM or LDRD.  */\n+      *cost = COSTS_N_INSNS (ARM_NUM_REGS (GET_MODE (x)));\n+      if (speed_p)\n+\t*cost += (ARM_NUM_REGS (GET_MODE (x)) * extra_cost->ldst.load\n+\t\t  + extra_cost->ldst.load_unaligned);\n+\n+#ifdef NOT_YET\n+      *cost += arm_address_cost (XEXP (XVECEXP (x, 0, 0), 0), GET_MODE (x),\n+\t\t\t\t ADDR_SPACE_GENERIC, speed_p);\n+#endif\n+      return true;\n+\n+    case UNSPEC_UNALIGNED_STORE:\n+      *cost = COSTS_N_INSNS (ARM_NUM_REGS (GET_MODE (x)));\n+      if (speed_p)\n+\t*cost += (ARM_NUM_REGS (GET_MODE (x)) * extra_cost->ldst.store\n+\t\t  + extra_cost->ldst.store_unaligned);\n+\n+      *cost += rtx_cost (XVECEXP (x, 0, 0), UNSPEC, 0, speed_p);\n+#ifdef NOT_YET\n+      *cost += arm_address_cost (XEXP (XVECEXP (x, 0, 0), 0), GET_MODE (x),\n+\t\t\t\t ADDR_SPACE_GENERIC, speed_p);\n+#endif\n+      return true;\n+\n+    case UNSPEC_VRINTZ:\n+    case UNSPEC_VRINTP:\n+    case UNSPEC_VRINTM:\n+    case UNSPEC_VRINTR:\n+    case UNSPEC_VRINTX:\n+    case UNSPEC_VRINTA:\n+      *cost = COSTS_N_INSNS (1);\n+      if (speed_p)\n+        *cost += extra_cost->fp[GET_MODE (x) == DFmode].roundint;\n+\n+      return true;\n+    default:\n+      *cost = COSTS_N_INSNS (2);\n+      break;\n+    }\n+  return false;\n+}\n+\n+/* Cost of a libcall.  We assume one insn per argument, an amount for the\n+   call (one insn for -Os) and then one for processing the result.  */\n+#define LIBCALL_COST(N) COSTS_N_INSNS (N + (speed_p ? 18 : 2))\n+\n+/* RTX costs.  Make an estimate of the cost of executing the operation\n+   X, which is contained with an operation with code OUTER_CODE.\n+   SPEED_P indicates whether the cost desired is the performance cost,\n+   or the size cost.  The estimate is stored in COST and the return\n+   value is TRUE if the cost calculation is final, or FALSE if the\n+   caller should recurse through the operands of X to add additional\n+   costs.\n+\n+   We currently make no attempt to model the size savings of Thumb-2\n+   16-bit instructions.  At the normal points in compilation where\n+   this code is called we have no measure of whether the condition\n+   flags are live or not, and thus no realistic way to determine what\n+   the size will eventually be.  */\n+static bool\n+arm_new_rtx_costs (rtx x, enum rtx_code code, enum rtx_code outer_code,\n+\t\t   const struct cpu_cost_table *extra_cost,\n+\t\t   int *cost, bool speed_p)\n+{\n+  enum machine_mode mode = GET_MODE (x);\n+\n+  if (TARGET_THUMB1)\n+    {\n+      if (speed_p)\n+\t*cost = thumb1_rtx_costs (x, code, outer_code);\n+      else\n+\t*cost = thumb1_size_rtx_costs (x, code, outer_code);\n+      return true;\n+    }\n+\n+  switch (code)\n+    {\n+    case SET:\n+      *cost = 0;\n+      if (REG_P (SET_SRC (x))\n+\t  && REG_P (SET_DEST (x)))\n+\t{\n+\t  /* Assume that most copies can be done with a single insn,\n+\t     unless we don't have HW FP, in which case everything\n+\t     larger than word mode will require two insns.  */\n+\t  *cost = COSTS_N_INSNS (((!TARGET_HARD_FLOAT\n+\t\t\t\t   && GET_MODE_SIZE (mode) > 4)\n+\t\t\t\t  || mode == DImode)\n+\t\t\t\t ? 2 : 1);\n+\t  /* Conditional register moves can be encoded\n+\t     in 16 bits in Thumb mode.  */\n+\t  if (!speed_p && TARGET_THUMB && outer_code == COND_EXEC)\n+\t    *cost >>= 1;\n+\t}\n+\n+      if (CONST_INT_P (SET_SRC (x)))\n+\t{\n+\t  /* Handle CONST_INT here, since the value doesn't have a mode\n+\t     and we would otherwise be unable to work out the true cost.  */\n+\t  *cost = rtx_cost (SET_DEST (x), SET, 0, speed_p);\n+\t  mode = GET_MODE (SET_DEST (x));\n+\t  outer_code = SET;\n+\t  /* Slightly lower the cost of setting a core reg to a constant.\n+\t     This helps break up chains and allows for better scheduling.  */\n+\t  if (REG_P (SET_DEST (x))\n+\t      && REGNO (SET_DEST (x)) <= LR_REGNUM)\n+\t    *cost -= 1;\n+\t  x = SET_SRC (x);\n+\t  /* Immediate moves with an immediate in the range [0, 255] can be\n+\t     encoded in 16 bits in Thumb mode.  */\n+\t  if (!speed_p && TARGET_THUMB && GET_MODE (x) == SImode\n+\t      && INTVAL (x) >= 0 && INTVAL (x) <=255)\n+\t    *cost >>= 1;\n+\t  goto const_int_cost;\n+\t}\n+\n+      return false;\n+\n+    case MEM:\n+      /* A memory access costs 1 insn if the mode is small, or the address is\n+\t a single register, otherwise it costs one insn per word.  */\n+      if (REG_P (XEXP (x, 0)))\n+\t*cost = COSTS_N_INSNS (1);\n+      else if (flag_pic\n+\t       && GET_CODE (XEXP (x, 0)) == PLUS\n+\t       && will_be_in_index_register (XEXP (XEXP (x, 0), 1)))\n+\t/* This will be split into two instructions.\n+\t   See arm.md:calculate_pic_address.  */\n+\t*cost = COSTS_N_INSNS (2);\n+      else\n+\t*cost = COSTS_N_INSNS (ARM_NUM_REGS (mode));\n+\n+      /* For speed optimizations, add the costs of the address and\n+\t accessing memory.  */\n+      if (speed_p)\n+#ifdef NOT_YET\n+\t*cost += (extra_cost->ldst.load\n+\t\t  + arm_address_cost (XEXP (x, 0), mode,\n+\t\t\t\t      ADDR_SPACE_GENERIC, speed_p));\n+#else\n+        *cost += extra_cost->ldst.load;\n+#endif\n+      return true;\n+\n+    case PARALLEL:\n+    {\n+   /* Calculations of LDM costs are complex.  We assume an initial cost\n+   (ldm_1st) which will load the number of registers mentioned in\n+   ldm_regs_per_insn_1st registers; then each additional\n+   ldm_regs_per_insn_subsequent registers cost one more insn.  The\n+   formula for N regs is thus:\n+\n+   ldm_1st + COSTS_N_INSNS ((max (N - ldm_regs_per_insn_1st, 0)\n+\t\t\t     + ldm_regs_per_insn_subsequent - 1)\n+\t\t\t    / ldm_regs_per_insn_subsequent).\n+\n+   Additional costs may also be added for addressing.  A similar\n+   formula is used for STM.  */\n+\n+      bool is_ldm = load_multiple_operation (x, SImode);\n+      bool is_stm = store_multiple_operation (x, SImode);\n+\n+      *cost = COSTS_N_INSNS (1);\n+\n+      if (is_ldm || is_stm)\n+        {\n+\t  if (speed_p)\n+\t    {\n+\t      HOST_WIDE_INT nregs = XVECLEN (x, 0);\n+\t      HOST_WIDE_INT regs_per_insn_1st = is_ldm\n+\t                              ? extra_cost->ldst.ldm_regs_per_insn_1st\n+\t                              : extra_cost->ldst.stm_regs_per_insn_1st;\n+\t      HOST_WIDE_INT regs_per_insn_sub = is_ldm\n+\t                       ? extra_cost->ldst.ldm_regs_per_insn_subsequent\n+\t                       : extra_cost->ldst.stm_regs_per_insn_subsequent;\n+\n+\t      *cost += regs_per_insn_1st\n+\t               + COSTS_N_INSNS (((MAX (nregs - regs_per_insn_1st, 0))\n+\t\t\t\t\t    + regs_per_insn_sub - 1)\n+\t\t\t\t\t  / regs_per_insn_sub);\n+\t      return true;\n+\t    }\n+\n+        }\n+      return false;\n+    }\n+    case DIV:\n+    case UDIV:\n+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT\n+\t  && (mode == SFmode || !TARGET_VFP_SINGLE))\n+\t*cost = COSTS_N_INSNS (speed_p\n+\t\t\t       ? extra_cost->fp[mode != SFmode].div : 1);\n+      else if (mode == SImode && TARGET_IDIV)\n+\t*cost = COSTS_N_INSNS (speed_p ? extra_cost->mult[0].idiv : 1);\n+      else\n+\t*cost = LIBCALL_COST (2);\n+      return false;\t/* All arguments must be in registers.  */\n+\n+    case MOD:\n+    case UMOD:\n+      *cost = LIBCALL_COST (2);\n+      return false;\t/* All arguments must be in registers.  */\n+\n+    case ROTATE:\n+      if (mode == SImode && REG_P (XEXP (x, 1)))\n+\t{\n+\t  *cost = (COSTS_N_INSNS (2)\n+\t\t   + rtx_cost (XEXP (x, 0), code, 0, speed_p));\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.shift_reg;\n+\t  return true;\n+\t}\n+      /* Fall through */\n+    case ROTATERT:\n+    case ASHIFT:\n+    case LSHIFTRT:\n+    case ASHIFTRT:\n+      if (mode == DImode && CONST_INT_P (XEXP (x, 1)))\n+\t{\n+\t  *cost = (COSTS_N_INSNS (3)\n+\t\t   + rtx_cost (XEXP (x, 0), code, 0, speed_p));\n+\t  if (speed_p)\n+\t    *cost += 2 * extra_cost->alu.shift;\n+\t  return true;\n+\t}\n+      else if (mode == SImode)\n+\t{\n+\t  *cost = (COSTS_N_INSNS (1)\n+\t\t   + rtx_cost (XEXP (x, 0), code, 0, speed_p));\n+\t  /* Slightly disparage register shifts at -Os, but not by much.  */\n+\t  if (!CONST_INT_P (XEXP (x, 1)))\n+\t    *cost += (speed_p ? extra_cost->alu.shift_reg : 1\n+\t\t      + rtx_cost (XEXP (x, 1), code, 1, speed_p));\n+\t  return true;\n+\t}\n+      else if (GET_MODE_CLASS (mode) == MODE_INT\n+\t       && GET_MODE_SIZE (mode) < 4)\n+\t{\n+\t  if (code == ASHIFT)\n+\t    {\n+\t      *cost = (COSTS_N_INSNS (1)\n+\t\t       + rtx_cost (XEXP (x, 0), code, 0, speed_p));\n+\t      /* Slightly disparage register shifts at -Os, but not by\n+\t         much.  */\n+\t      if (!CONST_INT_P (XEXP (x, 1)))\n+\t\t*cost += (speed_p ? extra_cost->alu.shift_reg : 1\n+\t\t\t  + rtx_cost (XEXP (x, 1), code, 1, speed_p));\n+\t    }\n+\t  else if (code == LSHIFTRT || code == ASHIFTRT)\n+\t    {\n+\t      if (arm_arch_thumb2 && CONST_INT_P (XEXP (x, 1)))\n+\t\t{\n+\t\t  /* Can use SBFX/UBFX.  */\n+\t\t  *cost = COSTS_N_INSNS (1);\n+\t\t  if (speed_p)\n+\t\t    *cost += extra_cost->alu.bfx;\n+\t\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  *cost = COSTS_N_INSNS (2);\n+\t\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t\t  if (speed_p)\n+\t\t    {\n+\t\t      if (CONST_INT_P (XEXP (x, 1)))\n+\t\t\t*cost += 2 * extra_cost->alu.shift;\n+\t\t      else\n+\t\t\t*cost += (extra_cost->alu.shift\n+\t\t\t\t  + extra_cost->alu.shift_reg);\n+\t\t    }\n+\t\t  else\n+\t\t    /* Slightly disparage register shifts.  */\n+\t\t    *cost += !CONST_INT_P (XEXP (x, 1));\n+\t\t}\n+\t    }\n+\t  else /* Rotates.  */\n+\t    {\n+\t      *cost = COSTS_N_INSNS (3 + !CONST_INT_P (XEXP (x, 1)));\n+\t      *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t      if (speed_p)\n+\t\t{\n+\t\t  if (CONST_INT_P (XEXP (x, 1)))\n+\t\t    *cost += (2 * extra_cost->alu.shift\n+\t\t\t      + extra_cost->alu.log_shift);\n+\t\t  else\n+\t\t    *cost += (extra_cost->alu.shift\n+\t\t\t      + extra_cost->alu.shift_reg\n+\t\t\t      + extra_cost->alu.log_shift_reg);\n+\t\t}\n+\t    }\n+\t  return true;\n+\t}\n+\n+      *cost = LIBCALL_COST (2);\n+      return false;\n+\n+    case MINUS:\n+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT\n+\t  && (mode == SFmode || !TARGET_VFP_SINGLE))\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (GET_CODE (XEXP (x, 0)) == MULT\n+\t      || GET_CODE (XEXP (x, 1)) == MULT)\n+\t    {\n+\t      rtx mul_op0, mul_op1, sub_op;\n+\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->fp[mode != SFmode].mult_addsub;\n+\n+\t      if (GET_CODE (XEXP (x, 0)) == MULT)\n+\t\t{\n+\t\t  mul_op0 = XEXP (XEXP (x, 0), 0);\n+\t\t  mul_op1 = XEXP (XEXP (x, 0), 1);\n+\t\t  sub_op = XEXP (x, 1);\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  mul_op0 = XEXP (XEXP (x, 1), 0);\n+\t\t  mul_op1 = XEXP (XEXP (x, 1), 1);\n+\t\t  sub_op = XEXP (x, 0);\n+\t\t}\n+\n+\t      /* The first operand of the multiply may be optionally\n+\t\t negated.  */\n+\t      if (GET_CODE (mul_op0) == NEG)\n+\t\tmul_op0 = XEXP (mul_op0, 0);\n+\n+\t      *cost += (rtx_cost (mul_op0, code, 0, speed_p)\n+\t\t\t+ rtx_cost (mul_op1, code, 0, speed_p)\n+\t\t\t+ rtx_cost (sub_op, code, 0, speed_p));\n+\n+\t      return true;\n+\t    }\n+\n+\t  if (speed_p)\n+\t    *cost += extra_cost->fp[mode != SFmode].addsub;\n+\t  return false;\n+\t}\n+\n+      if (mode == SImode)\n+\t{\n+\t  rtx shift_by_reg = NULL;\n+\t  rtx shift_op;\n+\t  rtx non_shift_op;\n+\n+\t  *cost = COSTS_N_INSNS (1);\n+\n+\t  shift_op = shifter_op_p (XEXP (x, 0), &shift_by_reg);\n+\t  if (shift_op == NULL)\n+\t    {\n+\t      shift_op = shifter_op_p (XEXP (x, 1), &shift_by_reg);\n+\t      non_shift_op = XEXP (x, 0);\n+\t    }\n+\t  else\n+\t    non_shift_op = XEXP (x, 1);\n+\n+\t  if (shift_op != NULL)\n+\t    {\n+\t      if (shift_by_reg != NULL)\n+\t\t{\n+\t\t  if (speed_p)\n+\t\t    *cost += extra_cost->alu.arith_shift_reg;\n+\t\t  *cost += rtx_cost (shift_by_reg, code, 0, speed_p);\n+\t\t}\n+\t      else if (speed_p)\n+\t\t*cost += extra_cost->alu.arith_shift;\n+\n+\t      *cost += (rtx_cost (shift_op, code, 0, speed_p)\n+\t\t\t+ rtx_cost (non_shift_op, code, 0, speed_p));\n+\t      return true;\n+\t    }\n+\n+\t  if (arm_arch_thumb2\n+\t      && GET_CODE (XEXP (x, 1)) == MULT)\n+\t    {\n+\t      /* MLS.  */\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->mult[0].add;\n+\t      *cost += (rtx_cost (XEXP (x, 0), MINUS, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (XEXP (x, 1), 0), MULT, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (XEXP (x, 1), 1), MULT, 1, speed_p));\n+\t      return true;\n+\t    }\n+\n+\t  if (CONST_INT_P (XEXP (x, 0)))\n+\t    {\n+\t      int insns = arm_gen_constant (MINUS, SImode, NULL_RTX,\n+\t\t\t\t\t    INTVAL (XEXP (x, 0)), NULL_RTX,\n+\t\t\t\t\t    NULL_RTX, 1, 0);\n+\t      *cost = COSTS_N_INSNS (insns);\n+\t      if (speed_p)\n+\t\t*cost += insns * extra_cost->alu.arith;\n+\t      *cost += rtx_cost (XEXP (x, 1), code, 1, speed_p);\n+\t      return true;\n+\t    }\n+\n+\t  return false;\n+\t}\n+\n+      if (GET_MODE_CLASS (mode) == MODE_INT\n+\t  && GET_MODE_SIZE (mode) < 4)\n+\t{\n+\t  /* Slightly disparage, as we might need to widen the result.  */\n+\t  *cost = 1 + COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.arith;\n+\n+\t  if (CONST_INT_P (XEXP (x, 0)))\n+\t    {\n+\t      *cost += rtx_cost (XEXP (x, 1), code, 1, speed_p);\n+\t      return true;\n+\t    }\n+\n+\t  return false;\n+\t}\n+\n+      if (mode == DImode)\n+\t{\n+\t  *cost = COSTS_N_INSNS (2);\n+\n+\t  if (GET_CODE (XEXP (x, 0)) == ZERO_EXTEND)\n+\t    {\n+\t      rtx op1 = XEXP (x, 1);\n+\n+\t      if (speed_p)\n+\t\t*cost += 2 * extra_cost->alu.arith;\n+\n+\t      if (GET_CODE (op1) == ZERO_EXTEND)\n+\t\t*cost += rtx_cost (XEXP (op1, 0), ZERO_EXTEND, 0, speed_p);\n+\t      else\n+\t\t*cost += rtx_cost (op1, MINUS, 1, speed_p);\n+\t      *cost += rtx_cost (XEXP (XEXP (x, 0), 0), ZERO_EXTEND,\n+\t\t\t\t 0, speed_p);\n+\t      return true;\n+\t    }\n+\t  else if (GET_CODE (XEXP (x, 0)) == SIGN_EXTEND)\n+\t    {\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->alu.arith + extra_cost->alu.arith_shift;\n+\t      *cost += (rtx_cost (XEXP (XEXP (x, 0), 0), SIGN_EXTEND,\n+\t\t\t\t  0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (x, 1), MINUS, 1, speed_p));\n+\t      return true;\n+\t    }\n+\t  else if (GET_CODE (XEXP (x, 1)) == ZERO_EXTEND\n+\t\t   || GET_CODE (XEXP (x, 1)) == SIGN_EXTEND)\n+\t    {\n+\t      if (speed_p)\n+\t\t*cost += (extra_cost->alu.arith\n+\t\t\t  + (GET_CODE (XEXP (x, 1)) == ZERO_EXTEND\n+\t\t\t     ? extra_cost->alu.arith\n+\t\t\t     : extra_cost->alu.arith_shift));\n+\t      *cost += (rtx_cost (XEXP (x, 0), MINUS, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (XEXP (x, 1), 0),\n+\t\t\t\t    GET_CODE (XEXP (x, 1)), 0, speed_p));\n+\t      return true;\n+\t    }\n+\n+\t  if (speed_p)\n+\t    *cost += 2 * extra_cost->alu.arith;\n+\t  return false;\n+\t}\n+\n+      /* Vector mode?  */\n+\n+      *cost = LIBCALL_COST (2);\n+      return false;\n+\n+    case PLUS:\n+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT\n+\t  && (mode == SFmode || !TARGET_VFP_SINGLE))\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (GET_CODE (XEXP (x, 0)) == MULT)\n+\t    {\n+\t      rtx mul_op0, mul_op1, add_op;\n+\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->fp[mode != SFmode].mult_addsub;\n+\n+\t      mul_op0 = XEXP (XEXP (x, 0), 0);\n+\t      mul_op1 = XEXP (XEXP (x, 0), 1);\n+\t      add_op = XEXP (x, 1);\n+\n+\t      *cost += (rtx_cost (mul_op0, code, 0, speed_p)\n+\t\t\t+ rtx_cost (mul_op1, code, 0, speed_p)\n+\t\t\t+ rtx_cost (add_op, code, 0, speed_p));\n+\n+\t      return true;\n+\t    }\n+\n+\t  if (speed_p)\n+\t    *cost += extra_cost->fp[mode != SFmode].addsub;\n+\t  return false;\n+\t}\n+      else if (GET_MODE_CLASS (mode) == MODE_FLOAT)\n+\t{\n+\t  *cost = LIBCALL_COST (2);\n+\t  return false;\n+\t}\n+\n+      if (GET_MODE_CLASS (mode) == MODE_INT\n+\t  && GET_MODE_SIZE (mode) < 4)\n+\t{\n+\t  /* Narrow modes can be synthesized in SImode, but the range\n+\t     of useful sub-operations is limited.  */\n+\t  if (CONST_INT_P (XEXP (x, 1)))\n+\t    {\n+\t      int insns = arm_gen_constant (PLUS, SImode, NULL_RTX,\n+\t\t\t\t\t    INTVAL (XEXP (x, 1)), NULL_RTX,\n+\t\t\t\t\t    NULL_RTX, 1, 0);\n+\t      *cost = COSTS_N_INSNS (insns);\n+\t      if (speed_p)\n+\t\t*cost += insns * extra_cost->alu.arith;\n+\t      /* Slightly penalize a narrow operation as the result may\n+\t\t need widening.  */\n+\t      *cost += 1 + rtx_cost (XEXP (x, 0), PLUS, 0, speed_p);\n+\t      return true;\n+\t    }\n+\n+\t  /* Slightly penalize a narrow operation as the result may\n+\t     need widening.  */\n+\t  *cost = 1 + COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.arith;\n+\n+\t  return false;\n+\t}\n+\n+      if (mode == SImode)\n+\t{\n+\t  rtx shift_op, shift_reg;\n+\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (TARGET_INT_SIMD\n+\t      && (GET_CODE (XEXP (x, 0)) == ZERO_EXTEND\n+\t\t  || GET_CODE (XEXP (x, 0)) == SIGN_EXTEND))\n+\t    {\n+\t      /* UXTA[BH] or SXTA[BH].  */\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->alu.extnd_arith;\n+\t      *cost += (rtx_cost (XEXP (XEXP (x, 0), 0), ZERO_EXTEND, 0,\n+\t\t\t\t  speed_p)\n+\t\t\t+ rtx_cost (XEXP (x, 1), PLUS, 0, speed_p));\n+\t      return true;\n+\t    }\n+\n+\t  shift_reg = NULL;\n+\t  shift_op = shifter_op_p (XEXP (x, 0), &shift_reg);\n+\t  if (shift_op != NULL)\n+\t    {\n+\t      if (shift_reg)\n+\t\t{\n+\t\t  if (speed_p)\n+\t\t    *cost += extra_cost->alu.arith_shift_reg;\n+\t\t  *cost += rtx_cost (shift_reg, ASHIFT, 1, speed_p);\n+\t\t}\n+\t      else if (speed_p)\n+\t\t*cost += extra_cost->alu.arith_shift;\n+\n+\t      *cost += (rtx_cost (shift_op, ASHIFT, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (x, 1), PLUS, 1, speed_p));\n+\t      return true;\n+\t    }\n+\t  if (GET_CODE (XEXP (x, 0)) == MULT)\n+\t    {\n+\t      rtx mul_op = XEXP (x, 0);\n+\n+\t      *cost = COSTS_N_INSNS (1);\n+\n+\t      if (TARGET_DSP_MULTIPLY\n+\t\t  && ((GET_CODE (XEXP (mul_op, 0)) == SIGN_EXTEND\n+\t\t       && (GET_CODE (XEXP (mul_op, 1)) == SIGN_EXTEND\n+\t\t\t   || (GET_CODE (XEXP (mul_op, 1)) == ASHIFTRT\n+\t\t\t       && CONST_INT_P (XEXP (XEXP (mul_op, 1), 1))\n+\t\t\t       && INTVAL (XEXP (XEXP (mul_op, 1), 1)) == 16)))\n+\t\t      || (GET_CODE (XEXP (mul_op, 0)) == ASHIFTRT\n+\t\t\t  && CONST_INT_P (XEXP (XEXP (mul_op, 0), 1))\n+\t\t\t  && INTVAL (XEXP (XEXP (mul_op, 0), 1)) == 16\n+\t\t\t  && (GET_CODE (XEXP (mul_op, 1)) == SIGN_EXTEND\n+\t\t\t      || (GET_CODE (XEXP (mul_op, 1)) == ASHIFTRT\n+\t\t\t\t  && CONST_INT_P (XEXP (XEXP (mul_op, 1), 1))\n+\t\t\t\t  && (INTVAL (XEXP (XEXP (mul_op, 1), 1))\n+\t\t\t\t      == 16))))))\n+\t\t{\n+\t\t  /* SMLA[BT][BT].  */\n+\t\t  if (speed_p)\n+\t\t    *cost += extra_cost->mult[0].extend_add;\n+\t\t  *cost += (rtx_cost (XEXP (XEXP (mul_op, 0), 0),\n+\t\t\t\t      SIGN_EXTEND, 0, speed_p)\n+\t\t\t    + rtx_cost (XEXP (XEXP (mul_op, 1), 0),\n+\t\t\t\t\tSIGN_EXTEND, 0, speed_p)\n+\t\t\t    + rtx_cost (XEXP (x, 1), PLUS, 1, speed_p));\n+\t\t  return true;\n+\t\t}\n+\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->mult[0].add;\n+\t      *cost += (rtx_cost (XEXP (mul_op, 0), MULT, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (mul_op, 1), MULT, 1, speed_p)\n+\t\t\t+ rtx_cost (XEXP (x, 1), PLUS, 1, speed_p));\n+\t      return true;\n+\t    }\n+\t  if (CONST_INT_P (XEXP (x, 1)))\n+\t    {\n+\t      int insns = arm_gen_constant (PLUS, SImode, NULL_RTX,\n+\t\t\t\t\t    INTVAL (XEXP (x, 1)), NULL_RTX,\n+\t\t\t\t\t    NULL_RTX, 1, 0);\n+\t      *cost = COSTS_N_INSNS (insns);\n+\t      if (speed_p)\n+\t\t*cost += insns * extra_cost->alu.arith;\n+\t      *cost += rtx_cost (XEXP (x, 0), PLUS, 0, speed_p);\n+\t      return true;\n+\t    }\n+\t  return false;\n+\t}\n+\n+      if (mode == DImode)\n+\t{\n+\t  if (arm_arch3m\n+\t      && GET_CODE (XEXP (x, 0)) == MULT\n+\t      && ((GET_CODE (XEXP (XEXP (x, 0), 0)) == ZERO_EXTEND\n+\t\t   && GET_CODE (XEXP (XEXP (x, 0), 1)) == ZERO_EXTEND)\n+\t\t  || (GET_CODE (XEXP (XEXP (x, 0), 0)) == SIGN_EXTEND\n+\t\t      && GET_CODE (XEXP (XEXP (x, 0), 1)) == SIGN_EXTEND)))\n+\t    {\n+\t      *cost = COSTS_N_INSNS (1);\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->mult[1].extend_add;\n+\t      *cost += (rtx_cost (XEXP (XEXP (XEXP (x, 0), 0), 0),\n+\t\t\t\t  ZERO_EXTEND, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (XEXP (XEXP (x, 0), 1), 0),\n+\t\t\t\t    ZERO_EXTEND, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (x, 1), PLUS, 1, speed_p));\n+\t      return true;\n+\t    }\n+\n+\t  *cost = COSTS_N_INSNS (2);\n+\n+\t  if (GET_CODE (XEXP (x, 0)) == ZERO_EXTEND\n+\t      || GET_CODE (XEXP (x, 0)) == SIGN_EXTEND)\n+\t    {\n+\t      if (speed_p)\n+\t\t*cost += (extra_cost->alu.arith\n+\t\t\t  + (GET_CODE (XEXP (x, 0)) == ZERO_EXTEND\n+\t\t\t     ? extra_cost->alu.arith\n+\t\t\t     : extra_cost->alu.arith_shift));\n+\n+\t      *cost += (rtx_cost (XEXP (XEXP (x, 0), 0), ZERO_EXTEND, 0,\n+\t\t\t\t  speed_p)\n+\t\t\t+ rtx_cost (XEXP (x, 1), PLUS, 1, speed_p));\n+\t      return true;\n+\t    }\n+\n+\t  if (speed_p)\n+\t    *cost += 2 * extra_cost->alu.arith;\n+\t  return false;\n+\t}\n+\n+      /* Vector mode?  */\n+      *cost = LIBCALL_COST (2);\n+      return false;\n+\n+    case AND: case XOR: case IOR:\n+      if (mode == SImode)\n+\t{\n+\t  enum rtx_code subcode = GET_CODE (XEXP (x, 0));\n+\t  rtx op0 = XEXP (x, 0);\n+\t  rtx shift_op, shift_reg;\n+\n+\t  *cost = COSTS_N_INSNS (1);\n+\n+\t  if (subcode == NOT\n+\t      && (code == AND\n+\t\t  || (code == IOR && TARGET_THUMB2)))\n+\t    op0 = XEXP (op0, 0);\n+\n+\t  shift_reg = NULL;\n+\t  shift_op = shifter_op_p (op0, &shift_reg);\n+\t  if (shift_op != NULL)\n+\t    {\n+\t      if (shift_reg)\n+\t\t{\n+\t\t  if (speed_p)\n+\t\t    *cost += extra_cost->alu.log_shift_reg;\n+\t\t  *cost += rtx_cost (shift_reg, ASHIFT, 1, speed_p);\n+\t\t}\n+\t      else if (speed_p)\n+\t\t*cost += extra_cost->alu.log_shift;\n+\n+\t      *cost += (rtx_cost (shift_op, ASHIFT, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (x, 1), code, 1, speed_p));\n+\t      return true;\n+\t    }\n+\n+\t  if (CONST_INT_P (XEXP (x, 1)))\n+\t    {\n+\t      int insns = arm_gen_constant (code, SImode, NULL_RTX,\n+\t\t\t\t\t    INTVAL (XEXP (x, 1)), NULL_RTX,\n+\t\t\t\t\t    NULL_RTX, 1, 0);\n+\n+\t      *cost = COSTS_N_INSNS (insns);\n+\t      if (speed_p)\n+\t\t*cost += insns * extra_cost->alu.logical;\n+\t      *cost += rtx_cost (op0, code, 0, speed_p);\n+\t      return true;\n+\t    }\n+\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.logical;\n+\t  *cost += (rtx_cost (op0, code, 0, speed_p)\n+\t\t    + rtx_cost (XEXP (x, 1), code, 1, speed_p));\n+\t  return true;\n+\t}\n+\n+      if (mode == DImode)\n+\t{\n+\t  rtx op0 = XEXP (x, 0);\n+\t  enum rtx_code subcode = GET_CODE (op0);\n+\n+\t  *cost = COSTS_N_INSNS (2);\n+\n+\t  if (subcode == NOT\n+\t      && (code == AND\n+\t\t  || (code == IOR && TARGET_THUMB2)))\n+\t    op0 = XEXP (op0, 0);\n+\n+\t  if (GET_CODE (op0) == ZERO_EXTEND)\n+\t    {\n+\t      if (speed_p)\n+\t\t*cost += 2 * extra_cost->alu.logical;\n+\n+\t      *cost += (rtx_cost (XEXP (op0, 0), ZERO_EXTEND, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (x, 1), code, 0, speed_p));\n+\t      return true;\n+\t    }\n+\t  else if (GET_CODE (op0) == SIGN_EXTEND)\n+\t    {\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->alu.logical + extra_cost->alu.log_shift;\n+\n+\t      *cost += (rtx_cost (XEXP (op0, 0), SIGN_EXTEND, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (x, 1), code, 0, speed_p));\n+\t      return true;\n+\t    }\n+\n+\t  if (speed_p)\n+\t    *cost += 2 * extra_cost->alu.logical;\n+\n+\t  return true;\n+\t}\n+      /* Vector mode?  */\n+\n+      *cost = LIBCALL_COST (2);\n+      return false;\n+\n+    case MULT:\n+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT\n+\t  && (mode == SFmode || !TARGET_VFP_SINGLE))\n+\t{\n+\t  rtx op0 = XEXP (x, 0);\n+\n+\t  *cost = COSTS_N_INSNS (1);\n+\n+\t  if (GET_CODE (op0) == NEG)\n+\t    op0 = XEXP (op0, 0);\n+\n+\t  if (speed_p)\n+\t    *cost += extra_cost->fp[mode != SFmode].mult;\n+\n+\t  *cost += (rtx_cost (op0, MULT, 0, speed_p)\n+\t\t    + rtx_cost (XEXP (x, 1), MULT, 1, speed_p));\n+\t  return true;\n+\t}\n+      else if (GET_MODE_CLASS (mode) == MODE_FLOAT)\n+\t{\n+\t  *cost = LIBCALL_COST (2);\n+\t  return false;\n+\t}\n+\n+      if (mode == SImode)\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (TARGET_DSP_MULTIPLY\n+\t      && ((GET_CODE (XEXP (x, 0)) == SIGN_EXTEND\n+\t\t   && (GET_CODE (XEXP (x, 1)) == SIGN_EXTEND\n+\t\t       || (GET_CODE (XEXP (x, 1)) == ASHIFTRT\n+\t\t\t   && CONST_INT_P (XEXP (XEXP (x, 1), 1))\n+\t\t\t   && INTVAL (XEXP (XEXP (x, 1), 1)) == 16)))\n+\t\t  || (GET_CODE (XEXP (x, 0)) == ASHIFTRT\n+\t\t      && CONST_INT_P (XEXP (XEXP (x, 0), 1))\n+\t\t      && INTVAL (XEXP (XEXP (x, 0), 1)) == 16\n+\t\t      && (GET_CODE (XEXP (x, 1)) == SIGN_EXTEND\n+\t\t\t  || (GET_CODE (XEXP (x, 1)) == ASHIFTRT\n+\t\t\t      && CONST_INT_P (XEXP (XEXP (x, 1), 1))\n+\t\t\t      && (INTVAL (XEXP (XEXP (x, 1), 1))\n+\t\t\t\t  == 16))))))\n+\t    {\n+\t      /* SMUL[TB][TB].  */\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->mult[0].extend;\n+\t      *cost += (rtx_cost (XEXP (x, 0), SIGN_EXTEND, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (x, 1), SIGN_EXTEND, 0, speed_p));\n+\t      return true;\n+\t    }\n+\t  if (speed_p)\n+\t    *cost += extra_cost->mult[0].simple;\n+\t  return false;\n+\t}\n+\n+      if (mode == DImode)\n+\t{\n+\t  if (arm_arch3m\n+\t      && ((GET_CODE (XEXP (x, 0)) == ZERO_EXTEND\n+\t\t   && GET_CODE (XEXP (x, 1)) == ZERO_EXTEND)\n+\t\t  || (GET_CODE (XEXP (x, 0)) == SIGN_EXTEND\n+\t\t      && GET_CODE (XEXP (x, 1)) == SIGN_EXTEND)))\n+\t    {\n+\t      *cost = COSTS_N_INSNS (1);\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->mult[1].extend;\n+\t      *cost += (rtx_cost (XEXP (XEXP (x, 0), 0),\n+\t\t\t\t  ZERO_EXTEND, 0, speed_p)\n+\t\t\t+ rtx_cost (XEXP (XEXP (x, 1), 0),\n+\t\t\t\t    ZERO_EXTEND, 0, speed_p));\n+\t      return true;\n+\t    }\n+\n+\t  *cost = LIBCALL_COST (2);\n+\t  return false;\n+\t}\n+\n+      /* Vector mode?  */\n+      *cost = LIBCALL_COST (2);\n+      return false;\n+\n+    case NEG:\n+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT\n+\t  && (mode == SFmode || !TARGET_VFP_SINGLE))\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->fp[mode != SFmode].neg;\n+\n+\t  return false;\n+\t}\n+      else if (GET_MODE_CLASS (mode) == MODE_FLOAT)\n+\t{\n+\t  *cost = LIBCALL_COST (1);\n+\t  return false;\n+\t}\n+\n+      if (mode == SImode)\n+\t{\n+\t  if (GET_CODE (XEXP (x, 0)) == ABS)\n+\t    {\n+\t      *cost = COSTS_N_INSNS (2);\n+\t      /* Assume the non-flag-changing variant.  */\n+\t      if (speed_p)\n+\t\t*cost += (extra_cost->alu.log_shift\n+\t\t\t  + extra_cost->alu.arith_shift);\n+\t      *cost += rtx_cost (XEXP (XEXP (x, 0), 0), ABS, 0, speed_p);\n+\t      return true;\n+\t    }\n+\n+\t  if (GET_RTX_CLASS (GET_CODE (XEXP (x, 0))) == RTX_COMPARE\n+\t      || GET_RTX_CLASS (GET_CODE (XEXP (x, 0))) == RTX_COMM_COMPARE)\n+\t    {\n+\t      *cost = COSTS_N_INSNS (2);\n+\t      /* No extra cost for MOV imm and MVN imm.  */\n+\t      /* If the comparison op is using the flags, there's no further\n+\t\t cost, otherwise we need to add the cost of the comparison.  */\n+\t      if (!(REG_P (XEXP (XEXP (x, 0), 0))\n+\t\t    && REGNO (XEXP (XEXP (x, 0), 0)) == CC_REGNUM\n+\t\t    && XEXP (XEXP (x, 0), 1) == const0_rtx))\n+\t\t{\n+\t\t  *cost += (COSTS_N_INSNS (1)\n+\t\t\t    + rtx_cost (XEXP (XEXP (x, 0), 0), COMPARE, 0,\n+\t\t\t\t\tspeed_p)\n+\t\t\t    + rtx_cost (XEXP (XEXP (x, 0), 1), COMPARE, 1,\n+\t\t\t\t\tspeed_p));\n+\t\t  if (speed_p)\n+\t\t    *cost += extra_cost->alu.arith;\n+\t\t}\n+\t      return true;\n+\t    }\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.arith;\n+\t  return false;\n+\t}\n+\n+      if (GET_MODE_CLASS (mode) == MODE_INT\n+\t  && GET_MODE_SIZE (mode) < 4)\n+\t{\n+\t  /* Slightly disparage, as we might need an extend operation.  */\n+\t  *cost = 1 + COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.arith;\n+\t  return false;\n+\t}\n+\n+      if (mode == DImode)\n+\t{\n+\t  *cost = COSTS_N_INSNS (2);\n+\t  if (speed_p)\n+\t    *cost += 2 * extra_cost->alu.arith;\n+\t  return false;\n+\t}\n+\n+      /* Vector mode?  */\n+      *cost = LIBCALL_COST (1);\n+      return false;\n+\n+    case NOT:\n+      if (mode == SImode)\n+\t{\n+\t  rtx shift_op;\n+\t  rtx shift_reg = NULL;\n+\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  shift_op = shifter_op_p (XEXP (x, 0), &shift_reg);\n+\n+\t  if (shift_op)\n+\t    {\n+\t      if (shift_reg != NULL)\n+\t\t{\n+\t\t  if (speed_p)\n+\t\t    *cost += extra_cost->alu.log_shift_reg;\n+\t\t  *cost += rtx_cost (shift_reg, ASHIFT, 1, speed_p);\n+\t\t}\n+\t      else if (speed_p)\n+\t\t*cost += extra_cost->alu.log_shift;\n+\t      *cost += rtx_cost (shift_op, ASHIFT, 0, speed_p);\n+\t      return true;\n+\t    }\n+\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.logical;\n+\t  return false;\n+\t}\n+      if (mode == DImode)\n+\t{\n+\t  *cost = COSTS_N_INSNS (2);\n+\t  return false;\n+\t}\n+\n+      /* Vector mode?  */\n+\n+      *cost += LIBCALL_COST (1);\n+      return false;\n+\n+    case IF_THEN_ELSE:\n+      {\n+        if (GET_CODE (XEXP (x, 1)) == PC || GET_CODE (XEXP (x, 2)) == PC)\n+\t  {\n+\t    *cost = COSTS_N_INSNS (4);\n+\t    return true;\n+\t  }\n+\tint op1cost = rtx_cost (XEXP (x, 1), SET, 1, speed_p);\n+\tint op2cost = rtx_cost (XEXP (x, 2), SET, 1, speed_p);\n+\n+\t*cost = rtx_cost (XEXP (x, 0), IF_THEN_ELSE, 0, speed_p);\n+\t/* Assume that if one arm of the if_then_else is a register,\n+\t   that it will be tied with the result and eliminate the\n+\t   conditional insn.  */\n+\tif (REG_P (XEXP (x, 1)))\n+\t  *cost += op2cost;\n+\telse if (REG_P (XEXP (x, 2)))\n+\t  *cost += op1cost;\n+\telse\n+\t  {\n+\t    if (speed_p)\n+\t      {\n+\t\tif (extra_cost->alu.non_exec_costs_exec)\n+\t\t  *cost += op1cost + op2cost + extra_cost->alu.non_exec;\n+\t\telse\n+\t\t  *cost += MAX (op1cost, op2cost) + extra_cost->alu.non_exec;\n+\t      }\n+\t    else\n+\t      *cost += op1cost + op2cost;\n+\t  }\n+      }\n+      return true;\n+\n+    case COMPARE:\n+      if (cc_register (XEXP (x, 0), VOIDmode) && XEXP (x, 1) == const0_rtx)\n+\t*cost = 0;\n+      else\n+\t{\n+\t  enum machine_mode op0mode;\n+\t  /* We'll mostly assume that the cost of a compare is the cost of the\n+\t     LHS.  However, there are some notable exceptions.  */\n+\n+\t  /* Floating point compares are never done as side-effects.  */\n+\t  op0mode = GET_MODE (XEXP (x, 0));\n+\t  if (TARGET_HARD_FLOAT && GET_MODE_CLASS (op0mode) == MODE_FLOAT\n+\t      && (op0mode == SFmode || !TARGET_VFP_SINGLE))\n+\t    {\n+\t      *cost = COSTS_N_INSNS (1);\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->fp[op0mode != SFmode].compare;\n+\n+\t      if (XEXP (x, 1) == CONST0_RTX (op0mode))\n+\t\t{\n+\t\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t\t  return true;\n+\t\t}\n+\n+\t      return false;\n+\t    }\n+\t  else if (GET_MODE_CLASS (op0mode) == MODE_FLOAT)\n+\t    {\n+\t      *cost = LIBCALL_COST (2);\n+\t      return false;\n+\t    }\n+\n+\t  /* DImode compares normally take two insns.  */\n+\t  if (op0mode == DImode)\n+\t    {\n+\t      *cost = COSTS_N_INSNS (2);\n+\t      if (speed_p)\n+\t\t*cost += 2 * extra_cost->alu.arith;\n+\t      return false;\n+\t    }\n+\n+\t  if (op0mode == SImode)\n+\t    {\n+\t      rtx shift_op;\n+\t      rtx shift_reg;\n+\n+\t      if (XEXP (x, 1) == const0_rtx\n+\t\t  && !(REG_P (XEXP (x, 0))\n+\t\t       || (GET_CODE (XEXP (x, 0)) == SUBREG\n+\t\t\t   && REG_P (SUBREG_REG (XEXP (x, 0))))))\n+\t\t{\n+\t\t  *cost = rtx_cost (XEXP (x, 0), COMPARE, 0, speed_p);\n+\n+\t\t  /* Multiply operations that set the flags are often\n+\t\t     significantly more expensive.  */\n+\t\t  if (speed_p\n+\t\t      && GET_CODE (XEXP (x, 0)) == MULT\n+\t\t      && !power_of_two_operand (XEXP (XEXP (x, 0), 1), mode))\n+\t\t    *cost += extra_cost->mult[0].flag_setting;\n+\n+\t\t  if (speed_p\n+\t\t      && GET_CODE (XEXP (x, 0)) == PLUS\n+\t\t      && GET_CODE (XEXP (XEXP (x, 0), 0)) == MULT\n+\t\t      && !power_of_two_operand (XEXP (XEXP (XEXP (x, 0),\n+\t\t\t\t\t\t\t    0), 1), mode))\n+\t\t    *cost += extra_cost->mult[0].flag_setting;\n+\t\t  return true;\n+\t\t}\n+\n+\t      shift_reg = NULL;\n+\t      shift_op = shifter_op_p (XEXP (x, 0), &shift_reg);\n+\t      if (shift_op != NULL)\n+\t\t{\n+\t\t  *cost = COSTS_N_INSNS (1);\n+\t\t  if (shift_reg != NULL)\n+\t\t    {\n+\t\t      *cost += rtx_cost (shift_reg, ASHIFT, 1, speed_p);\n+\t\t      if (speed_p)\n+\t\t\t*cost += extra_cost->alu.arith_shift_reg;\n+\t\t    }\n+\t\t  else if (speed_p)\n+\t\t    *cost += extra_cost->alu.arith_shift;\n+\t\t  *cost += (rtx_cost (shift_op, ASHIFT, 0, speed_p)\n+\t\t\t    + rtx_cost (XEXP (x, 1), COMPARE, 1, speed_p));\n+\t\t  return true;\n+\t\t}\n+\n+\t      *cost = COSTS_N_INSNS (1);\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->alu.arith;\n+\t      if (CONST_INT_P (XEXP (x, 1))\n+\t\t  && const_ok_for_op (INTVAL (XEXP (x, 1)), COMPARE))\n+\t\t{\n+\t\t  *cost += rtx_cost (XEXP (x, 0), COMPARE, 0, speed_p);\n+\t\t  return true;\n+\t\t}\n+\t      return false;\n+\t    }\n+\n+\t  /* Vector mode?  */\n+\n+\t  *cost = LIBCALL_COST (2);\n+\t  return false;\n+\t}\n+      return true;\n+\n+    case EQ:\n+    case NE:\n+    case LT:\n+    case LE:\n+    case GT:\n+    case GE:\n+    case LTU:\n+    case LEU:\n+    case GEU:\n+    case GTU:\n+    case ORDERED:\n+    case UNORDERED:\n+    case UNEQ:\n+    case UNLE:\n+    case UNLT:\n+    case UNGE:\n+    case UNGT:\n+    case LTGT:\n+      if (outer_code == SET)\n+\t{\n+\t  /* Is it a store-flag operation?  */\n+\t  if (REG_P (XEXP (x, 0)) && REGNO (XEXP (x, 0)) == CC_REGNUM\n+\t      && XEXP (x, 1) == const0_rtx)\n+\t    {\n+\t      /* Thumb also needs an IT insn.  */\n+\t      *cost = COSTS_N_INSNS (TARGET_THUMB ? 3 : 2);\n+\t      return true;\n+\t    }\n+\t  if (XEXP (x, 1) == const0_rtx)\n+\t    {\n+\t      switch (code)\n+\t\t{\n+\t\tcase LT:\n+\t\t  /* LSR Rd, Rn, #31.  */\n+\t\t  *cost = COSTS_N_INSNS (1);\n+\t\t  if (speed_p)\n+\t\t    *cost += extra_cost->alu.shift;\n+\t\t  break;\n+\n+\t\tcase EQ:\n+\t\t  /* RSBS T1, Rn, #0\n+\t\t     ADC  Rd, Rn, T1.  */\n+\n+\t\tcase NE:\n+\t\t  /* SUBS T1, Rn, #1\n+\t\t     SBC  Rd, Rn, T1.  */\n+\t\t  *cost = COSTS_N_INSNS (2);\n+\t\t  break;\n+\n+\t\tcase LE:\n+\t\t  /* RSBS T1, Rn, Rn, LSR #31\n+\t\t     ADC  Rd, Rn, T1. */\n+\t\t  *cost = COSTS_N_INSNS (2);\n+\t\t  if (speed_p)\n+\t\t    *cost += extra_cost->alu.arith_shift;\n+\t\t  break;\n+\n+\t\tcase GT:\n+\t\t  /* RSB  Rd, Rn, Rn, ASR #1\n+\t\t     LSR  Rd, Rd, #31.  */\n+\t\t  *cost = COSTS_N_INSNS (2);\n+\t\t  if (speed_p)\n+\t\t    *cost += (extra_cost->alu.arith_shift\n+\t\t\t      + extra_cost->alu.shift);\n+\t\t  break;\n+\n+\t\tcase GE:\n+\t\t  /* ASR  Rd, Rn, #31\n+\t\t     ADD  Rd, Rn, #1.  */\n+\t\t  *cost = COSTS_N_INSNS (2);\n+\t\t  if (speed_p)\n+\t\t    *cost += extra_cost->alu.shift;\n+\t\t  break;\n+\n+\t\tdefault:\n+\t\t  /* Remaining cases are either meaningless or would take\n+\t\t     three insns anyway.  */\n+\t\t  *cost = COSTS_N_INSNS (3);\n+\t\t  break;\n+\t\t}\n+\t      *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t      return true;\n+\t    }\n+\t  else\n+\t    {\n+\t      *cost = COSTS_N_INSNS (TARGET_THUMB ? 4 : 3);\n+\t      if (CONST_INT_P (XEXP (x, 1))\n+\t\t  && const_ok_for_op (INTVAL (XEXP (x, 1)), COMPARE))\n+\t\t{\n+\t\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t\t  return true;\n+\t\t}\n+\n+\t      return false;\n+\t    }\n+\t}\n+      /* Not directly inside a set.  If it involves the condition code\n+\t register it must be the condition for a branch, cond_exec or\n+\t I_T_E operation.  Since the comparison is performed elsewhere\n+\t this is just the control part which has no additional\n+\t cost.  */\n+      else if (REG_P (XEXP (x, 0)) && REGNO (XEXP (x, 0)) == CC_REGNUM\n+\t       && XEXP (x, 1) == const0_rtx)\n+\t{\n+\t  *cost = 0;\n+\t  return true;\n+\t}\n+\n+    case ABS:\n+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT\n+\t  && (mode == SFmode || !TARGET_VFP_SINGLE))\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->fp[mode != SFmode].neg;\n+\n+\t  return false;\n+\t}\n+      else if (GET_MODE_CLASS (mode) == MODE_FLOAT)\n+\t{\n+\t  *cost = LIBCALL_COST (1);\n+\t  return false;\n+\t}\n+\n+      if (mode == SImode)\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.log_shift + extra_cost->alu.arith_shift;\n+\t  return false;\n+\t}\n+      /* Vector mode?  */\n+      *cost = LIBCALL_COST (1);\n+      return false;\n+\n+    case SIGN_EXTEND:\n+      if ((arm_arch4 || GET_MODE (XEXP (x, 0)) == SImode)\n+\t  && MEM_P (XEXP (x, 0)))\n+\t{\n+\t  *cost = rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\n+\t  if (mode == DImode)\n+\t    *cost += COSTS_N_INSNS (1);\n+\n+\t  if (!speed_p)\n+\t    return true;\n+\n+\t  if (GET_MODE (XEXP (x, 0)) == SImode)\n+\t    *cost += extra_cost->ldst.load;\n+\t  else\n+\t    *cost += extra_cost->ldst.load_sign_extend;\n+\n+\t  if (mode == DImode)\n+\t    *cost += extra_cost->alu.shift;\n+\n+\t  return true;\n+\t}\n+\n+      /* Widening from less than 32-bits requires an extend operation.  */\n+      if (GET_MODE (XEXP (x, 0)) != SImode && arm_arch6)\n+\t{\n+\t  /* We have SXTB/SXTH.  */\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.extnd;\n+\t}\n+      else if (GET_MODE (XEXP (x, 0)) != SImode)\n+\t{\n+\t  /* Needs two shifts.  */\n+\t  *cost = COSTS_N_INSNS (2);\n+\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t  if (speed_p)\n+\t    *cost += 2 * extra_cost->alu.shift;\n+\t}\n+\n+      /* Widening beyond 32-bits requires one more insn.  */\n+      if (mode == DImode)\n+\t{\n+\t  *cost += COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.shift;\n+\t}\n+\n+      return true;\n+\n+    case ZERO_EXTEND:\n+      if ((arm_arch4\n+\t   || GET_MODE (XEXP (x, 0)) == SImode\n+\t   || GET_MODE (XEXP (x, 0)) == QImode)\n+\t  && MEM_P (XEXP (x, 0)))\n+\t{\n+\t  *cost = rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\n+\t  if (mode == DImode)\n+\t    *cost += COSTS_N_INSNS (1);  /* No speed penalty.  */\n+\n+\t  return true;\n+\t}\n+\n+      /* Widening from less than 32-bits requires an extend operation.  */\n+      if (GET_MODE (XEXP (x, 0)) == QImode)\n+\t{\n+\t  /* UXTB can be a shorter instruction in Thumb2, but it might\n+\t     be slower than the AND Rd, Rn, #255 alternative.  When\n+\t     optimizing for speed it should never be slower to use\n+\t     AND, and we don't really model 16-bit vs 32-bit insns\n+\t     here.  */\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.logical;\n+\t}\n+      else if (GET_MODE (XEXP (x, 0)) != SImode && arm_arch6)\n+\t{\n+\t  /* We have UXTB/UXTH.  */\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.extnd;\n+\t}\n+      else if (GET_MODE (XEXP (x, 0)) != SImode)\n+\t{\n+\t  /* Needs two shifts.  It's marginally preferable to use\n+\t     shifts rather than two BIC instructions as the second\n+\t     shift may merge with a subsequent insn as a shifter\n+\t     op.  */\n+\t  *cost = COSTS_N_INSNS (2);\n+\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t  if (speed_p)\n+\t    *cost += 2 * extra_cost->alu.shift;\n+\t}\n+\n+      /* Widening beyond 32-bits requires one more insn.  */\n+      if (mode == DImode)\n+\t{\n+\t  *cost += COSTS_N_INSNS (1);\t/* No speed penalty.  */\n+\t}\n+\n+      return true;\n+\n+    case CONST_INT:\n+      *cost = 0;\n+      /* CONST_INT has no mode, so we cannot tell for sure how many\n+\t insns are really going to be needed.  The best we can do is\n+\t look at the value passed.  If it fits in SImode, then assume\n+\t that's the mode it will be used for.  Otherwise assume it\n+\t will be used in DImode.  */\n+      if (INTVAL (x) == trunc_int_for_mode (INTVAL (x), SImode))\n+\tmode = SImode;\n+      else\n+\tmode = DImode;\n+\n+      /* Avoid blowing up in arm_gen_constant ().  */\n+      if (!(outer_code == PLUS\n+\t    || outer_code == AND\n+\t    || outer_code == IOR\n+\t    || outer_code == XOR\n+\t    || outer_code == MINUS))\n+\touter_code = SET;\n+\n+    const_int_cost:\n+      if (mode == SImode)\n+\t{\n+\t  *cost += 0;\n+\t  *cost += COSTS_N_INSNS (arm_gen_constant (outer_code, SImode, NULL,\n+\t\t\t\t\t\t    INTVAL (x), NULL, NULL,\n+\t\t\t\t\t\t    0, 0));\n+\t  /* Extra costs?  */\n+\t}\n+      else\n+\t{\n+\t  *cost += COSTS_N_INSNS (arm_gen_constant\n+\t\t\t\t  (outer_code, SImode, NULL,\n+\t\t\t\t   trunc_int_for_mode (INTVAL (x), SImode),\n+\t\t\t\t   NULL, NULL, 0, 0)\n+\t\t\t\t  + arm_gen_constant (outer_code, SImode, NULL,\n+\t\t\t\t\t\t      INTVAL (x) >> 32, NULL,\n+\t\t\t\t\t\t      NULL, 0, 0));\n+\t  /* Extra costs?  */\n+\t}\n+\n+      return true;\n+\n+    case CONST:\n+    case LABEL_REF:\n+    case SYMBOL_REF:\n+      if (speed_p)\n+\t{\n+\t  if (arm_arch_thumb2 && !flag_pic)\n+\t    *cost = COSTS_N_INSNS (2);\n+\t  else\n+\t    *cost = COSTS_N_INSNS (1) + extra_cost->ldst.load;\n+\t}\n+      else\n+\t*cost = COSTS_N_INSNS (2);\n+\n+      if (flag_pic)\n+\t{\n+\t  *cost += COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.arith;\n+\t}\n+\n+      return true;\n+\n+    case CONST_FIXED:\n+      *cost = COSTS_N_INSNS (4);\n+      /* Fixme.  */\n+      return true;\n+\n+    case CONST_DOUBLE:\n+      if (TARGET_HARD_FLOAT && GET_MODE_CLASS (mode) == MODE_FLOAT\n+\t  && (mode == SFmode || !TARGET_VFP_SINGLE))\n+\t{\n+\t  if (vfp3_const_double_rtx (x))\n+\t    {\n+\t      *cost = COSTS_N_INSNS (1);\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->fp[mode == DFmode].fpconst;\n+\t      return true;\n+\t    }\n+\n+\t  if (speed_p)\n+\t    {\n+\t      *cost = COSTS_N_INSNS (1);\n+\t      if (mode == DFmode)\n+\t\t*cost += extra_cost->ldst.loadd;\n+\t      else\n+\t\t*cost += extra_cost->ldst.loadf;\n+\t    }\n+\t  else\n+\t    *cost = COSTS_N_INSNS (2 + (mode == DFmode));\n+\n+\t  return true;\n+\t}\n+      *cost = COSTS_N_INSNS (4);\n+      return true;\n+\n+    case CONST_VECTOR:\n+      /* Fixme.  */\n+      if (TARGET_NEON\n+\t  && TARGET_HARD_FLOAT\n+\t  && (VALID_NEON_DREG_MODE (mode) || VALID_NEON_QREG_MODE (mode))\n+\t  && neon_immediate_valid_for_move (x, mode, NULL, NULL))\n+\t*cost = COSTS_N_INSNS (1);\n+      else\n+\t*cost = COSTS_N_INSNS (4);\n+      return true;\n+\n+    case HIGH:\n+    case LO_SUM:\n+      *cost = COSTS_N_INSNS (1);\n+      /* When optimizing for size, we prefer constant pool entries to\n+\t MOVW/MOVT pairs, so bump the cost of these slightly.  */\n+      if (!speed_p)\n+\t*cost += 1;\n+      return true;\n+\n+    case CLZ:\n+      *cost = COSTS_N_INSNS (1);\n+      if (speed_p)\n+\t*cost += extra_cost->alu.clz;\n+      return false;\n+\n+    case SMIN:\n+      if (XEXP (x, 1) == const0_rtx)\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.log_shift;\n+\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t  return true;\n+\t}\n+      /* Fall through.  */\n+    case SMAX:\n+    case UMIN:\n+    case UMAX:\n+      *cost = COSTS_N_INSNS (2);\n+      return false;\n+\n+    case TRUNCATE:\n+      if (GET_CODE (XEXP (x, 0)) == ASHIFTRT\n+\t  && CONST_INT_P (XEXP (XEXP (x, 0), 1))\n+\t  && INTVAL (XEXP (XEXP (x, 0), 1)) == 32\n+\t  && GET_CODE (XEXP (XEXP (x, 0), 0)) == MULT\n+\t  && ((GET_CODE (XEXP (XEXP (XEXP (x, 0), 0), 0)) == SIGN_EXTEND\n+\t       && GET_CODE (XEXP (XEXP (XEXP (x, 0), 0), 1)) == SIGN_EXTEND)\n+\t      || (GET_CODE (XEXP (XEXP (XEXP (x, 0), 0), 0)) == ZERO_EXTEND\n+\t\t  && (GET_CODE (XEXP (XEXP (XEXP (x, 0), 0), 1))\n+\t\t      == ZERO_EXTEND))))\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->mult[1].extend;\n+\t  *cost += (rtx_cost (XEXP (XEXP (XEXP (x, 0), 0), 0), ZERO_EXTEND, 0,\n+\t\t\t      speed_p)\n+\t\t    + rtx_cost (XEXP (XEXP (XEXP (x, 0), 0), 1), ZERO_EXTEND,\n+\t\t\t\t0, speed_p));\n+\t  return true;\n+\t}\n+      *cost = LIBCALL_COST (1);\n+      return false;\n+\n+    case UNSPEC:\n+      return arm_unspec_cost (x, outer_code, speed_p, cost);\n+\n+    case PC:\n+      /* Reading the PC is like reading any other register.  Writing it\n+\t is more expensive, but we take that into account elsewhere.  */\n+      *cost = 0;\n+      return true;\n+\n+    case ZERO_EXTRACT:\n+      /* TODO: Simple zero_extract of bottom bits using AND.  */\n+      /* Fall through.  */\n+    case SIGN_EXTRACT:\n+      if (arm_arch6\n+\t  && mode == SImode\n+\t  && CONST_INT_P (XEXP (x, 1))\n+\t  && CONST_INT_P (XEXP (x, 2)))\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->alu.bfx;\n+\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t  return true;\n+\t}\n+      /* Without UBFX/SBFX, need to resort to shift operations.  */\n+      *cost = COSTS_N_INSNS (2);\n+      if (speed_p)\n+\t*cost += 2 * extra_cost->alu.shift;\n+      *cost += rtx_cost (XEXP (x, 0), ASHIFT, 0, speed_p);\n+      return true;\n+\n+    case FLOAT_EXTEND:\n+      if (TARGET_HARD_FLOAT)\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->fp[mode == DFmode].widen;\n+\t  if (!TARGET_FPU_ARMV8\n+\t      && GET_MODE (XEXP (x, 0)) == HFmode)\n+\t    {\n+\t      /* Pre v8, widening HF->DF is a two-step process, first\n+\t         widening to SFmode.  */\n+\t      *cost += COSTS_N_INSNS (1);\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->fp[0].widen;\n+\t    }\n+\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t  return true;\n+\t}\n+\n+      *cost = LIBCALL_COST (1);\n+      return false;\n+\n+    case FLOAT_TRUNCATE:\n+      if (TARGET_HARD_FLOAT)\n+\t{\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->fp[mode == DFmode].narrow;\n+\t  *cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t  return true;\n+\t  /* Vector modes?  */\n+\t}\n+      *cost = LIBCALL_COST (1);\n+      return false;\n+\n+    case FIX:\n+    case UNSIGNED_FIX:\n+      if (TARGET_HARD_FLOAT)\n+\t{\n+\t  if (GET_MODE_CLASS (mode) == MODE_INT)\n+\t    {\n+\t      *cost = COSTS_N_INSNS (1);\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->fp[GET_MODE (XEXP (x, 0)) == DFmode].toint;\n+\t      /* Strip of the 'cost' of rounding towards zero.  */\n+\t      if (GET_CODE (XEXP (x, 0)) == FIX)\n+\t\t*cost += rtx_cost (XEXP (XEXP (x, 0), 0), code, 0, speed_p);\n+\t      else\n+\t\t*cost += rtx_cost (XEXP (x, 0), code, 0, speed_p);\n+\t      /* ??? Increase the cost to deal with transferring from\n+\t\t FP -> CORE registers?  */\n+\t      return true;\n+\t    }\n+\t  else if (GET_MODE_CLASS (mode) == MODE_FLOAT\n+\t\t   && TARGET_FPU_ARMV8)\n+\t    {\n+\t      *cost = COSTS_N_INSNS (1);\n+\t      if (speed_p)\n+\t\t*cost += extra_cost->fp[mode == DFmode].roundint;\n+\t      return false;\n+\t    }\n+\t  /* Vector costs? */\n+\t}\n+      *cost = LIBCALL_COST (1);\n+      return false;\n+\n+    case FLOAT:\n+    case UNSIGNED_FLOAT:\n+      if (TARGET_HARD_FLOAT)\n+\t{\n+\t  /* ??? Increase the cost to deal with transferring from CORE\n+\t     -> FP registers?  */\n+\t  *cost = COSTS_N_INSNS (1);\n+\t  if (speed_p)\n+\t    *cost += extra_cost->fp[mode == DFmode].fromint;\n+\t  return false;\n+\t}\n+      *cost = LIBCALL_COST (1);\n+      return false;\n+\n+    case CALL:\n+      *cost = COSTS_N_INSNS (1);\n+      return true;\n+\n+    case ASM_OPERANDS:\n+      /* Just a guess.  Cost one insn per input.  */\n+      *cost = COSTS_N_INSNS (ASM_OPERANDS_INPUT_LENGTH (x));\n+      return true;\n+\n+    default:\n+      if (mode != VOIDmode)\n+\t*cost = COSTS_N_INSNS (ARM_NUM_REGS (mode));\n+      else\n+\t*cost = COSTS_N_INSNS (4); /* Who knows?  */\n+      return false;\n+    }\n+}\n+\n+/* RTX costs when optimizing for size.  */\n+static bool\n+arm_rtx_costs (rtx x, int code, int outer_code, int opno ATTRIBUTE_UNUSED,\n+\t       int *total, bool speed)\n+{\n+  bool result;\n+\n+  if (TARGET_OLD_RTX_COSTS\n+      || (!current_tune->insn_extra_cost && !TARGET_NEW_GENERIC_COSTS))\n+    {\n+      /* Old way.  (Deprecated.)  */\n+      if (!speed)\n+\tresult = arm_size_rtx_costs (x, (enum rtx_code) code,\n+\t\t\t\t     (enum rtx_code) outer_code, total);\n+      else\n+\tresult = current_tune->rtx_costs (x,  (enum rtx_code) code,\n+\t\t\t\t\t  (enum rtx_code) outer_code, total,\n+\t\t\t\t\t  speed);\n+    }\n+  else\n+    {\n+    /* New way.  */\n+      if (current_tune->insn_extra_cost)\n+        result =  arm_new_rtx_costs (x, (enum rtx_code) code,\n+\t\t\t\t     (enum rtx_code) outer_code,\n+\t\t\t\t     current_tune->insn_extra_cost,\n+\t\t\t\t     total, speed);\n+    /* TARGET_NEW_GENERIC_COSTS && !TARGET_OLD_RTX_COSTS\n+       && current_tune->insn_extra_cost != NULL  */\n+      else\n+        result =  arm_new_rtx_costs (x, (enum rtx_code) code,\n+\t\t\t\t    (enum rtx_code) outer_code,\n+\t\t\t\t    &generic_extra_costs, total, speed);\n+    }\n+\n+  if (dump_file && (dump_flags & TDF_DETAILS))\n+    {\n+      print_rtl_single (dump_file, x);\n+      fprintf (dump_file, \"\\n%s cost: %d (%s)\\n\", speed ? \"Hot\" : \"Cold\",\n+\t       *total, result ? \"final\" : \"partial\");\n+    }\n+  return result;\n }\n \n /* RTX costs for cores with a slow MUL implementation.  Thumb-2 is not"}, {"sha": "66e128ecbd45a61f97d7ad521594248c9316d319", "filename": "gcc/config/arm/arm.opt", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5bea0c6c8c3deb0b52aec49434484b35e75293f2/gcc%2Fconfig%2Farm%2Farm.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5bea0c6c8c3deb0b52aec49434484b35e75293f2/gcc%2Fconfig%2Farm%2Farm.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.opt?ref=5bea0c6c8c3deb0b52aec49434484b35e75293f2", "patch": "@@ -243,6 +243,14 @@ mrestrict-it\n Target Report Var(arm_restrict_it) Init(2)\n Generate IT blocks appropriate for ARMv8.\n \n+mold-rtx-costs\n+Target Report Mask(OLD_RTX_COSTS)\n+Use the old RTX costing tables (transitional).\n+\n+mnew-generic-costs\n+Target Report Mask(NEW_GENERIC_COSTS)\n+Use the new generic RTX cost tables if new core-specific cost table not available (transitional).\n+\n mfix-cortex-m3-ldrd\n Target Report Var(fix_cm3_ldrd) Init(2)\n Avoid overlapping destination and address registers on LDRD instructions"}]}