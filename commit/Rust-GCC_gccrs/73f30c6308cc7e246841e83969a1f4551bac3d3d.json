{"sha": "73f30c6308cc7e246841e83969a1f4551bac3d3d", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NzNmMzBjNjMwOGNjN2UyNDY4NDFlODM5NjlhMWY0NTUxYmFjM2QzZA==", "commit": {"author": {"name": "Zdenek Dvorak", "email": "dvorakz@suse.cz", "date": "2006-12-14T02:05:20Z"}, "committer": {"name": "Zdenek Dvorak", "email": "rakdver@gcc.gnu.org", "date": "2006-12-14T02:05:20Z"}, "message": "tree-ssa-loop-ivopts.c: Include tree-affine.h.\n\n\t* tree-ssa-loop-ivopts.c: Include tree-affine.h.\n\t(divide): Removed.\n\t(constant_multiple_of): Fix order of operators for division.\n\t(aff_combination_const, aff_combination_elt, aff_combination_scale,\n\taff_combination_add_elt, aff_combination_add, aff_combination_convert,\n\ttree_to_aff_combination, add_elt_to_tree, unshare_aff_combination,\n\taff_combination_to_tree): Moved to tree-affine.c and made to work with\n\tdouble_int coefficients.\n\t(get_computation_aff, get_computation_at): Work with double_int\n\tcoefficients.\n\t(get_computation_cost_at): Do not use divide.\n\t(rewrite_use_nonlinear_expr, rewrite_use_address, rewrite_use_compare):\n\tAssert that expressing the computation did not fail.\n\t* tree-ssa-address.c: Include tree-affine.h.\n\t(add_to_parts, most_expensive_mult_to_index, addr_to_parts,\n\tcreate_mem_ref): Work with double_int coefficients.\n\t* tree-affine.c: New file.\n\t* tree-affine.h: New file.\n\t* tree-flow.h (struct affine_tree_combination): Removed.\n\t* Makefile.in (tree-affine.o): Add.\n\t(tree-ssa-address.o, tree-ssa-loop-ivopts.o): Add tree-affine.h\n\tdependency.\n\nFrom-SVN: r119854", "tree": {"sha": "33e4fecfec131da30950f9a5e548ab0f2c428746", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/33e4fecfec131da30950f9a5e548ab0f2c428746"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/73f30c6308cc7e246841e83969a1f4551bac3d3d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/73f30c6308cc7e246841e83969a1f4551bac3d3d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/73f30c6308cc7e246841e83969a1f4551bac3d3d", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/73f30c6308cc7e246841e83969a1f4551bac3d3d/comments", "author": null, "committer": null, "parents": [{"sha": "904e0e974d06c1cfae8941447d0dd207b9b87fd2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/904e0e974d06c1cfae8941447d0dd207b9b87fd2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/904e0e974d06c1cfae8941447d0dd207b9b87fd2"}], "stats": {"total": 1237, "additions": 623, "deletions": 614}, "files": [{"sha": "d7b85b7e72c30ddf36cb560354b9cc6f91411dc5", "filename": "gcc/ChangeLog", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=73f30c6308cc7e246841e83969a1f4551bac3d3d", "patch": "@@ -1,3 +1,28 @@\n+2006-12-13  Zdenek Dvorak <dvorakz@suse.cz>\n+\n+\t* tree-ssa-loop-ivopts.c: Include tree-affine.h.\n+\t(divide): Removed.\n+\t(constant_multiple_of): Fix order of operators for division.\n+\t(aff_combination_const, aff_combination_elt, aff_combination_scale,\n+\taff_combination_add_elt, aff_combination_add, aff_combination_convert,\n+\ttree_to_aff_combination, add_elt_to_tree, unshare_aff_combination,\n+\taff_combination_to_tree): Moved to tree-affine.c and made to work with\n+\tdouble_int coefficients.\n+\t(get_computation_aff, get_computation_at): Work with double_int\n+\tcoefficients.\n+\t(get_computation_cost_at): Do not use divide.\n+\t(rewrite_use_nonlinear_expr, rewrite_use_address, rewrite_use_compare):\n+\tAssert that expressing the computation did not fail.\n+\t* tree-ssa-address.c: Include tree-affine.h.\n+\t(add_to_parts, most_expensive_mult_to_index, addr_to_parts,\n+\tcreate_mem_ref): Work with double_int coefficients.\n+\t* tree-affine.c: New file.\n+\t* tree-affine.h: New file.\n+\t* tree-flow.h (struct affine_tree_combination): Removed.\n+\t* Makefile.in (tree-affine.o): Add.\n+\t(tree-ssa-address.o, tree-ssa-loop-ivopts.o): Add tree-affine.h\n+\tdependency.\n+\n 2006-12-13  Peter Bergner  <bergner@vnet.ibm.com>\n \n \tPR middle-end/30191"}, {"sha": "7ff6a4086cbcd62eafdaa2beb525f84fb2560795", "filename": "gcc/Makefile.in", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=73f30c6308cc7e246841e83969a1f4551bac3d3d", "patch": "@@ -988,7 +988,7 @@ OBJS-common = \\\n  tree-vectorizer.o tree-vect-analyze.o tree-vect-transform.o\t\t   \\\n  tree-vect-patterns.o tree-ssa-loop-prefetch.o tree-ssa-coalesce.o\t   \\\n  tree-ssa-loop-ivcanon.o tree-ssa-propagate.o tree-ssa-address.o\t   \\\n- tree-ssa-math-opts.o\t\t\t\t\t\t\t   \\\n+ tree-ssa-math-opts.o tree-affine.o\t\t\t\t\t   \\\n  tree-ssa-loop-ivopts.o tree-if-conv.o tree-ssa-loop-unswitch.o\t\t   \\\n  alias.o bb-reorder.o bitmap.o builtins.o caller-save.o calls.o\t  \t   \\\n  cfg.o cfganal.o cfgbuild.o cfgcleanup.o cfglayout.o cfgloop.o\t\t   \\\n@@ -1992,7 +1992,7 @@ tree-ssa-address.o : tree-ssa-address.c $(TREE_FLOW_H) $(CONFIG_H) \\\n    $(SYSTEM_H) $(RTL_H) $(TREE_H) $(TM_P_H) \\\n    output.h $(DIAGNOSTIC_H) $(TIMEVAR_H) $(TM_H) coretypes.h $(TREE_DUMP_H) \\\n    tree-pass.h $(FLAGS_H) $(TREE_INLINE_H) $(RECOG_H) insn-config.h $(EXPR_H) \\\n-   gt-tree-ssa-address.h $(GGC_H)\n+   gt-tree-ssa-address.h $(GGC_H) tree-affine.h\n tree-ssa-loop-niter.o : tree-ssa-loop-niter.c $(TREE_FLOW_H) $(CONFIG_H) \\\n    $(SYSTEM_H) $(RTL_H) $(TREE_H) $(TM_P_H) $(CFGLOOP_H) $(PARAMS_H) \\\n    $(TREE_INLINE_H) output.h $(DIAGNOSTIC_H) $(TM_H) coretypes.h $(TREE_DUMP_H) \\\n@@ -2018,7 +2018,10 @@ tree-ssa-loop-ivopts.o : tree-ssa-loop-ivopts.c $(TREE_FLOW_H) $(CONFIG_H) \\\n    output.h $(DIAGNOSTIC_H) $(TIMEVAR_H) $(TM_H) coretypes.h $(TREE_DUMP_H) \\\n    tree-pass.h $(GGC_H) $(RECOG_H) insn-config.h $(HASHTAB_H) $(SCEV_H) \\\n    $(CFGLOOP_H) $(PARAMS_H) langhooks.h $(BASIC_BLOCK_H) hard-reg-set.h \\\n-   tree-chrec.h $(VARRAY_H)\n+   tree-chrec.h $(VARRAY_H) tree-affine.h\n+tree-affine.o : tree-affine.c tree-affine.h $(CONFIG_H) \\\n+   $(SYSTEM_H) $(RTL_H) $(TREE_H) $(TM_P_H) \\\n+   output.h $(DIAGNOSTIC_H) $(TM_H) coretypes.h $(TREE_DUMP_H)\n tree-ssa-loop-manip.o : tree-ssa-loop-manip.c $(TREE_FLOW_H) $(CONFIG_H) \\\n    $(SYSTEM_H) $(RTL_H) $(TREE_H) $(TM_P_H) $(CFGLOOP_H) \\\n    output.h $(DIAGNOSTIC_H) $(TIMEVAR_H) $(TM_H) coretypes.h $(TREE_DUMP_H) \\"}, {"sha": "762e82e0b4a54df1abad369eb4171dfda2af363f", "filename": "gcc/tree-affine.c", "status": "added", "additions": 414, "deletions": 0, "changes": 414, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2Ftree-affine.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2Ftree-affine.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-affine.c?ref=73f30c6308cc7e246841e83969a1f4551bac3d3d", "patch": "@@ -0,0 +1,414 @@\n+/* Operations with affine combinations of trees.\n+   Copyright (C) 2005 Free Software Foundation, Inc.\n+   \n+This file is part of GCC.\n+   \n+GCC is free software; you can redistribute it and/or modify it\n+under the terms of the GNU General Public License as published by the\n+Free Software Foundation; either version 2, or (at your option) any\n+later version.\n+   \n+GCC is distributed in the hope that it will be useful, but WITHOUT\n+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+   \n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA\n+02110-1301, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"tree.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"hard-reg-set.h\"\n+#include \"output.h\"\n+#include \"diagnostic.h\"\n+#include \"tree-dump.h\"\n+#include \"tree-affine.h\"\n+\n+/* Extends CST as appropriate for the affine combinations COMB.  */\n+\n+double_int\n+double_int_ext_for_comb (double_int cst, aff_tree *comb)\n+{\n+  return double_int_sext (cst, TYPE_PRECISION (comb->type));\n+}\n+\n+/* Initializes affine combination COMB so that its value is zero in TYPE.  */\n+\n+static void\n+aff_combination_zero (aff_tree *comb, tree type)\n+{\n+  comb->type = type;\n+  comb->offset = double_int_zero;\n+  comb->n = 0;\n+  comb->rest = NULL_TREE;\n+}\n+\n+/* Sets COMB to CST.  */\n+\n+void\n+aff_combination_const (aff_tree *comb, tree type, double_int cst)\n+{\n+  aff_combination_zero (comb, type);\n+  comb->offset = double_int_ext_for_comb (cst, comb);\n+}\n+\n+/* Sets COMB to single element ELT.  */\n+\n+void\n+aff_combination_elt (aff_tree *comb, tree type, tree elt)\n+{\n+  aff_combination_zero (comb, type);\n+\n+  comb->n = 1;\n+  comb->elts[0].val = elt;\n+  comb->elts[0].coef = double_int_one;\n+}\n+\n+/* Scales COMB by SCALE.  */\n+\n+void\n+aff_combination_scale (aff_tree *comb, double_int scale)\n+{\n+  unsigned i, j;\n+\n+  scale = double_int_ext_for_comb (scale, comb);\n+  if (double_int_one_p (scale))\n+    return;\n+\n+  if (double_int_zero_p (scale))\n+    {\n+      aff_combination_zero (comb, comb->type);\n+      return;\n+    }\n+\n+  comb->offset\n+    = double_int_ext_for_comb (double_int_mul (scale, comb->offset), comb);\n+  for (i = 0, j = 0; i < comb->n; i++)\n+    {\n+      double_int new_coef;\n+\n+      new_coef\n+\t= double_int_ext_for_comb (double_int_mul (scale, comb->elts[i].coef),\n+\t\t\t\t   comb);\n+      /* A coefficient may become zero due to overflow.  Remove the zero\n+\t elements.  */\n+      if (double_int_zero_p (new_coef))\n+\tcontinue;\n+      comb->elts[j].coef = new_coef;\n+      comb->elts[j].val = comb->elts[i].val;\n+      j++;\n+    }\n+  comb->n = j;\n+\n+  if (comb->rest)\n+    {\n+      if (comb->n < MAX_AFF_ELTS)\n+\t{\n+\t  comb->elts[comb->n].coef = scale;\n+\t  comb->elts[comb->n].val = comb->rest;\n+\t  comb->rest = NULL_TREE;\n+\t  comb->n++;\n+\t}\n+      else\n+\tcomb->rest = fold_build2 (MULT_EXPR, comb->type, comb->rest, \n+\t\t\t\t  double_int_to_tree (comb->type, scale));\n+    }\n+}\n+\n+/* Adds ELT * SCALE to COMB.  */\n+\n+void\n+aff_combination_add_elt (aff_tree *comb, tree elt, double_int scale)\n+{\n+  unsigned i;\n+\n+  scale = double_int_ext_for_comb (scale, comb);\n+  if (double_int_zero_p (scale))\n+    return;\n+\n+  for (i = 0; i < comb->n; i++)\n+    if (operand_equal_p (comb->elts[i].val, elt, 0))\n+      {\n+\tdouble_int new_coef;\n+\n+\tnew_coef = double_int_add (comb->elts[i].coef, scale);\n+\tnew_coef = double_int_ext_for_comb (new_coef, comb);\n+\tif (!double_int_zero_p (new_coef))\n+\t  {\n+\t    comb->elts[i].coef = new_coef;\n+\t    return;\n+\t  }\n+\n+\tcomb->n--;\n+\tcomb->elts[i] = comb->elts[comb->n];\n+\n+\tif (comb->rest)\n+\t  {\n+\t    gcc_assert (comb->n == MAX_AFF_ELTS - 1);\n+\t    comb->elts[comb->n].coef = double_int_one;\n+\t    comb->elts[comb->n].val = comb->rest;\n+\t    comb->rest = NULL_TREE;\n+\t    comb->n++;\n+\t  }\n+\treturn;\n+      }\n+  if (comb->n < MAX_AFF_ELTS)\n+    {\n+      comb->elts[comb->n].coef = scale;\n+      comb->elts[comb->n].val = elt;\n+      comb->n++;\n+      return;\n+    }\n+\n+  if (double_int_one_p (scale))\n+    elt = fold_convert (comb->type, elt);\n+  else\n+    elt = fold_build2 (MULT_EXPR, comb->type,\n+\t\t       fold_convert (comb->type, elt),\n+\t\t       double_int_to_tree (comb->type, scale)); \n+\n+  if (comb->rest)\n+    comb->rest = fold_build2 (PLUS_EXPR, comb->type, comb->rest, elt);\n+  else\n+    comb->rest = elt;\n+}\n+\n+/* Adds COMB2 to COMB1.  */\n+\n+void\n+aff_combination_add (aff_tree *comb1, aff_tree *comb2)\n+{\n+  unsigned i;\n+\n+  comb1->offset\n+    = double_int_ext_for_comb (double_int_add (comb1->offset, comb2->offset),\n+\t\t\t       comb1);\n+  for (i = 0; i < comb2->n; i++)\n+    aff_combination_add_elt (comb1, comb2->elts[i].val, comb2->elts[i].coef);\n+  if (comb2->rest)\n+    aff_combination_add_elt (comb1, comb2->rest, double_int_one);\n+}\n+\n+/* Converts affine combination COMB to TYPE.  */\n+\n+void\n+aff_combination_convert (aff_tree *comb, tree type)\n+{\n+  unsigned i, j;\n+  tree comb_type = comb->type;\n+\n+  gcc_assert (TYPE_PRECISION (type) <= TYPE_PRECISION (comb_type));\n+  comb->type = type;\n+  if (comb->rest)\n+    comb->rest = fold_convert (type, comb->rest);\n+\n+  if (TYPE_PRECISION (type) == TYPE_PRECISION (comb_type))\n+    return;\n+\n+  comb->offset = double_int_ext_for_comb (comb->offset, comb);\n+  for (i = j = 0; i < comb->n; i++)\n+    {\n+      double_int new_coef = double_int_ext_for_comb (comb->elts[i].coef, comb);\n+      if (double_int_zero_p (new_coef))\n+\tcontinue;\n+      comb->elts[j].coef = new_coef;\n+      comb->elts[j].val = fold_convert (type, comb->elts[i].val);\n+      j++;\n+    }\n+\n+  comb->n = j;\n+  if (comb->n < MAX_AFF_ELTS && comb->rest)\n+    {\n+      comb->elts[comb->n].coef = double_int_one;\n+      comb->elts[comb->n].val = comb->rest;\n+      comb->rest = NULL_TREE;\n+      comb->n++;\n+    }\n+}\n+\n+/* Splits EXPR into an affine combination of parts.  */\n+\n+void\n+tree_to_aff_combination (tree expr, tree type, aff_tree *comb)\n+{\n+  aff_tree tmp;\n+  enum tree_code code;\n+  tree cst, core, toffset;\n+  HOST_WIDE_INT bitpos, bitsize;\n+  enum machine_mode mode;\n+  int unsignedp, volatilep;\n+\n+  STRIP_NOPS (expr);\n+\n+  code = TREE_CODE (expr);\n+  switch (code)\n+    {\n+    case INTEGER_CST:\n+      aff_combination_const (comb, type, tree_to_double_int (expr));\n+      return;\n+\n+    case PLUS_EXPR:\n+    case MINUS_EXPR:\n+      tree_to_aff_combination (TREE_OPERAND (expr, 0), type, comb);\n+      tree_to_aff_combination (TREE_OPERAND (expr, 1), type, &tmp);\n+      if (code == MINUS_EXPR)\n+\taff_combination_scale (&tmp, double_int_minus_one);\n+      aff_combination_add (comb, &tmp);\n+      return;\n+\n+    case MULT_EXPR:\n+      cst = TREE_OPERAND (expr, 1);\n+      if (TREE_CODE (cst) != INTEGER_CST)\n+\tbreak;\n+      tree_to_aff_combination (TREE_OPERAND (expr, 0), type, comb);\n+      aff_combination_scale (comb, tree_to_double_int (cst));\n+      return;\n+\n+    case NEGATE_EXPR:\n+      tree_to_aff_combination (TREE_OPERAND (expr, 0), type, comb);\n+      aff_combination_scale (comb, double_int_minus_one);\n+      return;\n+\n+    case ADDR_EXPR:\n+      core = get_inner_reference (TREE_OPERAND (expr, 0), &bitsize, &bitpos,\n+\t\t\t\t  &toffset, &mode, &unsignedp, &volatilep,\n+\t\t\t\t  false);\n+      if (bitpos % BITS_PER_UNIT != 0)\n+\tbreak;\n+      aff_combination_const (comb, type,\n+\t\t\t     uhwi_to_double_int (bitpos / BITS_PER_UNIT));\n+      core = build_fold_addr_expr (core);\n+      if (TREE_CODE (core) == ADDR_EXPR)\n+\taff_combination_add_elt (comb, core, double_int_one);\n+      else\n+\t{\n+\t  tree_to_aff_combination (core, type, &tmp);\n+\t  aff_combination_add (comb, &tmp);\n+\t}\n+      if (toffset)\n+\t{\n+\t  tree_to_aff_combination (toffset, type, &tmp);\n+\t  aff_combination_add (comb, &tmp);\n+\t}\n+      return;\n+\n+    default:\n+      break;\n+    }\n+\n+  aff_combination_elt (comb, type, expr);\n+}\n+\n+/* Creates EXPR + ELT * SCALE in TYPE.  EXPR is taken from affine\n+   combination COMB.  */\n+\n+static tree\n+add_elt_to_tree (tree expr, tree type, tree elt, double_int scale,\n+\t\t aff_tree *comb)\n+{\n+  enum tree_code code;\n+\n+  scale = double_int_ext_for_comb (scale, comb);\n+  elt = fold_convert (type, elt);\n+\n+  if (double_int_one_p (scale))\n+    {\n+      if (!expr)\n+\treturn elt;\n+\n+      return fold_build2 (PLUS_EXPR, type, expr, elt);\n+    }\n+\n+  if (double_int_minus_one_p (scale))\n+    {\n+      if (!expr)\n+\treturn fold_build1 (NEGATE_EXPR, type, elt);\n+\n+      return fold_build2 (MINUS_EXPR, type, expr, elt);\n+    }\n+\n+  if (!expr)\n+    return fold_build2 (MULT_EXPR, type, elt,\n+\t\t\tdouble_int_to_tree (type, scale));\n+\n+  if (double_int_negative_p (scale))\n+    {\n+      code = MINUS_EXPR;\n+      scale = double_int_neg (scale);\n+    }\n+  else\n+    code = PLUS_EXPR;\n+\n+  elt = fold_build2 (MULT_EXPR, type, elt,\n+\t\t     double_int_to_tree (type, scale));\n+  return fold_build2 (code, type, expr, elt);\n+}\n+\n+/* Makes tree from the affine combination COMB.  */\n+\n+tree\n+aff_combination_to_tree (aff_tree *comb)\n+{\n+  tree type = comb->type;\n+  tree expr = comb->rest;\n+  unsigned i;\n+  double_int off, sgn;\n+\n+  gcc_assert (comb->n == MAX_AFF_ELTS || comb->rest == NULL_TREE);\n+\n+  for (i = 0; i < comb->n; i++)\n+    expr = add_elt_to_tree (expr, type, comb->elts[i].val, comb->elts[i].coef,\n+\t\t\t    comb);\n+\n+  /* Ensure that we get x - 1, not x + (-1) or x + 0xff..f if x is\n+     unsigned.  */\n+  if (double_int_negative_p (comb->offset))\n+    {\n+      off = double_int_neg (comb->offset);\n+      sgn = double_int_minus_one;\n+    }\n+  else\n+    {\n+      off = comb->offset;\n+      sgn = double_int_one;\n+    }\n+  return add_elt_to_tree (expr, type, double_int_to_tree (type, off), sgn,\n+\t\t\t  comb);\n+}\n+\n+/* Copies the tree elements of COMB to ensure that they are not shared.  */\n+\n+void\n+unshare_aff_combination (aff_tree *comb)\n+{\n+  unsigned i;\n+\n+  for (i = 0; i < comb->n; i++)\n+    comb->elts[i].val = unshare_expr (comb->elts[i].val);\n+  if (comb->rest)\n+    comb->rest = unshare_expr (comb->rest);\n+}\n+\n+/* Remove M-th element from COMB.  */\n+\n+void\n+aff_combination_remove_elt (aff_tree *comb, unsigned m)\n+{\n+  comb->n--;\n+  if (m <= comb->n)\n+    comb->elts[m] = comb->elts[comb->n];\n+  if (comb->rest)\n+    {\n+      comb->elts[comb->n].coef = double_int_one;\n+      comb->elts[comb->n].val = comb->rest;\n+      comb->rest = NULL_TREE;\n+      comb->n++;\n+    }\n+}"}, {"sha": "010f4a76d9b411b8475df56c27b28b9a1ed4ad92", "filename": "gcc/tree-affine.h", "status": "added", "additions": 71, "deletions": 0, "changes": 71, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2Ftree-affine.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2Ftree-affine.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-affine.h?ref=73f30c6308cc7e246841e83969a1f4551bac3d3d", "patch": "@@ -0,0 +1,71 @@\n+/* Operations with affine combinations of trees.\n+   Copyright (C) 2005 Free Software Foundation, Inc.\n+   \n+This file is part of GCC.\n+   \n+GCC is free software; you can redistribute it and/or modify it\n+under the terms of the GNU General Public License as published by the\n+Free Software Foundation; either version 2, or (at your option) any\n+later version.\n+   \n+GCC is distributed in the hope that it will be useful, but WITHOUT\n+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+   \n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA\n+02110-1301, USA.  */\n+\n+/* Affine combination of trees.  We keep track of at most MAX_AFF_ELTS elements\n+   to make things simpler; this is sufficient in most cases.  */\n+\n+#define MAX_AFF_ELTS 8\n+\n+/* Element of an affine combination.  */\n+\n+struct aff_comb_elt\n+{\n+  /* The value of the element.  */\n+  tree val;\n+  \n+  /* Its coefficient in the combination.  */\n+  double_int coef;\n+};\n+\n+typedef struct affine_tree_combination\n+{\n+  /* Type of the result of the combination.  */\n+  tree type;\n+\n+  /* Constant offset.  */\n+  double_int offset;\n+\n+  /* Number of elements of the combination.  */\n+  unsigned n;\n+\n+  /* Elements and their coefficients.  Type of elements may be different from\n+     TYPE, but their sizes must be the same (STRIP_NOPS is applied to the\n+     elements).\n+     \n+     The coefficients are always sign extened from the precision of TYPE\n+     (regardless of signedness of TYPE).  */\n+  struct aff_comb_elt elts[MAX_AFF_ELTS];\n+\n+  /* Remainder of the expression.  Usually NULL, used only if there are more\n+     than MAX_AFF_ELTS elements.  Type of REST must be TYPE.  */\n+  tree rest;\n+} aff_tree;\n+\n+double_int double_int_ext_for_comb (double_int, aff_tree *);\n+void aff_combination_const (aff_tree *, tree, double_int);\n+void aff_combination_elt (aff_tree *, tree, tree);\n+void aff_combination_scale (aff_tree *, double_int);\n+void aff_combination_add (aff_tree *, aff_tree *);\n+void aff_combination_add_elt (aff_tree *, tree, double_int);\n+void aff_combination_remove_elt (aff_tree *, unsigned);\n+void aff_combination_convert (aff_tree *, tree);\n+void tree_to_aff_combination (tree, tree, aff_tree *);\n+tree aff_combination_to_tree (aff_tree *);\n+void unshare_aff_combination (aff_tree *);"}, {"sha": "f4337d2a1e01d2cf8abf425111dfa3480538387a", "filename": "gcc/tree-flow.h", "status": "modified", "additions": 1, "deletions": 27, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2Ftree-flow.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2Ftree-flow.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-flow.h?ref=73f30c6308cc7e246841e83969a1f4551bac3d3d", "patch": "@@ -1002,40 +1002,14 @@ extern void remove_unused_locals (void);\n \n /* In tree-ssa-address.c  */\n \n-/* Affine combination of trees.  We keep track of at most MAX_AFF_ELTS elements\n-   to make things simpler; this is sufficient in most cases.  */\n-\n-#define MAX_AFF_ELTS 8\n-\n-struct affine_tree_combination\n-{\n-  /* Type of the result of the combination.  */\n-  tree type;\n-\n-  /* Mask modulo that the operations are performed.  */\n-  unsigned HOST_WIDE_INT mask;\n-\n-  /* Constant offset.  */\n-  unsigned HOST_WIDE_INT offset;\n-\n-  /* Number of elements of the combination.  */\n-  unsigned n;\n-\n-  /* Elements and their coefficients.  */\n-  tree elts[MAX_AFF_ELTS];\n-  unsigned HOST_WIDE_INT coefs[MAX_AFF_ELTS];\n-\n-  /* Remainder of the expression.  */\n-  tree rest;\n-};\n-\n /* Description of a memory address.  */\n \n struct mem_address\n {\n   tree symbol, base, index, step, offset;\n };\n \n+struct affine_tree_combination;\n tree create_mem_ref (block_stmt_iterator *, tree, \n \t\t     struct affine_tree_combination *);\n rtx addr_for_mem_ref (struct mem_address *, bool);"}, {"sha": "eb39370f0d89b869f0846c2763dc6e4b3a85c384", "filename": "gcc/tree-ssa-address.c", "status": "modified", "additions": 56, "deletions": 38, "changes": 94, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2Ftree-ssa-address.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2Ftree-ssa-address.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-address.c?ref=73f30c6308cc7e246841e83969a1f4551bac3d3d", "patch": "@@ -42,6 +42,7 @@ Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA\n #include \"recog.h\"\n #include \"expr.h\"\n #include \"ggc.h\"\n+#include \"tree-affine.h\"\n \n /* TODO -- handling of symbols (according to Richard Hendersons\n    comments, http://gcc.gnu.org/ml/gcc-patches/2005-04/msg00949.html):\n@@ -346,25 +347,20 @@ fixed_address_object_p (tree obj)\n    construct.  */\n \n static void\n-add_to_parts (struct mem_address *parts, tree type, tree elt,\n-\t      unsigned HOST_WIDE_INT coef)\n+add_to_parts (struct mem_address *parts, tree type, tree elt)\n {\n+  tree elt_core = elt;\n+  STRIP_NOPS (elt_core);\n+\n   /* Check if this is a symbol.  */\n   if (!parts->symbol\n-      && coef == 1\n-      && TREE_CODE (elt) == ADDR_EXPR\n-      && fixed_address_object_p (TREE_OPERAND (elt, 0)))\n+      && TREE_CODE (elt_core) == ADDR_EXPR\n+      && fixed_address_object_p (TREE_OPERAND (elt_core, 0)))\n     {\n-      parts->symbol = TREE_OPERAND (elt, 0);\n+      parts->symbol = TREE_OPERAND (elt_core, 0);\n       return;\n     }\n \n-  if (coef != 1)\n-    elt = fold_build2 (MULT_EXPR, type, fold_convert (type, elt),\n-\t\t       build_int_cst_type (type, coef));\n-  else\n-    elt = fold_convert (type, elt);\n-\n   if (!parts->base)\n     {\n       parts->base = elt;\n@@ -388,52 +384,69 @@ add_to_parts (struct mem_address *parts, tree type, tree elt,\n \n static void\n most_expensive_mult_to_index (struct mem_address *parts, tree type,\n-\t\t\t      struct affine_tree_combination *addr)\n+\t\t\t      aff_tree *addr)\n {\n-  unsigned HOST_WIDE_INT best_mult = 0;\n+  HOST_WIDE_INT coef;\n+  double_int best_mult, amult, amult_neg;\n   unsigned best_mult_cost = 0, acost;\n   tree mult_elt = NULL_TREE, elt;\n   unsigned i, j;\n+  enum tree_code op_code;\n \n+  best_mult = double_int_zero;\n   for (i = 0; i < addr->n; i++)\n     {\n+      if (!double_int_fits_in_shwi_p (addr->elts[i].coef))\n+\tcontinue;\n+\n       /* FIXME: Should use the correct memory mode rather than Pmode.  */\n-      if (addr->coefs[i] == 1\n-\t  || !multiplier_allowed_in_address_p (addr->coefs[i], Pmode))\n+\n+      coef = double_int_to_shwi (addr->elts[i].coef);\n+      if (coef == 1\n+\t  || !multiplier_allowed_in_address_p (coef, Pmode))\n \tcontinue;\n-      \n-      acost = multiply_by_cost (addr->coefs[i], Pmode);\n+\n+      acost = multiply_by_cost (coef, Pmode);\n \n       if (acost > best_mult_cost)\n \t{\n \t  best_mult_cost = acost;\n-\t  best_mult = addr->coefs[i];\n+\t  best_mult = addr->elts[i].coef;\n \t}\n     }\n \n-  if (!best_mult)\n+  if (!best_mult_cost)\n     return;\n \n+  /* Collect elements multiplied by best_mult.  */\n   for (i = j = 0; i < addr->n; i++)\n     {\n-      if (addr->coefs[i] != best_mult)\n+      amult = addr->elts[i].coef;\n+      amult_neg = double_int_ext_for_comb (double_int_neg (amult), addr);\n+ \n+      if (double_int_equal_p (amult, best_mult))\n+\top_code = PLUS_EXPR;\n+      else if (double_int_equal_p (amult_neg, best_mult))\n+\top_code = MINUS_EXPR;\n+      else\n \t{\n-\t  addr->coefs[j] = addr->coefs[i];\n \t  addr->elts[j] = addr->elts[i];\n \t  j++;\n \t  continue;\n \t}\n-\n-      elt = fold_convert (type, addr->elts[i]);\n-      if (!mult_elt)\n+  \n+      elt = fold_convert (type, addr->elts[i].val);\n+      if (mult_elt)\n+\tmult_elt = fold_build2 (op_code, type, mult_elt, elt);\n+      else if (op_code == PLUS_EXPR)\n \tmult_elt = elt;\n       else\n-\tmult_elt = fold_build2 (PLUS_EXPR, type, mult_elt, elt);\n+\tmult_elt = fold_build1 (NEGATE_EXPR, type, elt);\n     }\n   addr->n = j;\n-\n+  \n   parts->index = mult_elt;\n-  parts->step = build_int_cst_type (type, best_mult);\n+  parts->step = double_int_to_tree (type, best_mult);\n }\n \n /* Splits address ADDR into PARTS.\n@@ -442,22 +455,22 @@ most_expensive_mult_to_index (struct mem_address *parts, tree type,\n    to PARTS.  Some architectures do not support anything but single\n    register in address, possibly with a small integer offset; while\n    create_mem_ref will simplify the address to an acceptable shape\n-   later, it would be a small bit more efficient to know that asking\n-   for complicated addressing modes is useless.  */\n+   later, it would be more efficient to know that asking for complicated\n+   addressing modes is useless.  */\n \n static void\n-addr_to_parts (struct affine_tree_combination *addr, tree type,\n-\t       struct mem_address *parts)\n+addr_to_parts (aff_tree *addr, tree type, struct mem_address *parts)\n {\n+  tree part;\n   unsigned i;\n \n   parts->symbol = NULL_TREE;\n   parts->base = NULL_TREE;\n   parts->index = NULL_TREE;\n   parts->step = NULL_TREE;\n \n-  if (addr->offset)\n-    parts->offset = build_int_cst_type (type, addr->offset);\n+  if (!double_int_zero_p (addr->offset))\n+    parts->offset = double_int_to_tree (type, addr->offset);\n   else\n     parts->offset = NULL_TREE;\n \n@@ -467,9 +480,15 @@ addr_to_parts (struct affine_tree_combination *addr, tree type,\n \n   /* Then try to process the remaining elements.  */\n   for (i = 0; i < addr->n; i++)\n-    add_to_parts (parts, type, addr->elts[i], addr->coefs[i]);\n+    {\n+      part = fold_convert (type, addr->elts[i].val);\n+      if (!double_int_one_p (addr->elts[i].coef))\n+\tpart = fold_build2 (MULT_EXPR, type, part,\n+\t\t\t    double_int_to_tree (type, addr->elts[i].coef));\n+      add_to_parts (parts, type, part);\n+    }\n   if (addr->rest)\n-    add_to_parts (parts, type, addr->rest, 1);\n+    add_to_parts (parts, type, addr->rest);\n }\n \n /* Force the PARTS to register.  */\n@@ -490,8 +509,7 @@ gimplify_mem_ref_parts (block_stmt_iterator *bsi, struct mem_address *parts)\n    of created memory reference.  */\n \n tree\n-create_mem_ref (block_stmt_iterator *bsi, tree type,\n-\t\tstruct affine_tree_combination *addr)\n+create_mem_ref (block_stmt_iterator *bsi, tree type, aff_tree *addr)\n {\n   tree mem_ref, tmp;\n   tree addr_type = build_pointer_type (type);"}, {"sha": "1efaf994bab3c3fe823d0c9be814e7534c029cd6", "filename": "gcc/tree-ssa-loop-ivopts.c", "status": "modified", "additions": 50, "deletions": 546, "changes": 596, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2Ftree-ssa-loop-ivopts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/73f30c6308cc7e246841e83969a1f4551bac3d3d/gcc%2Ftree-ssa-loop-ivopts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-loop-ivopts.c?ref=73f30c6308cc7e246841e83969a1f4551bac3d3d", "patch": "@@ -89,6 +89,7 @@ Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA\n #include \"cfgloop.h\"\n #include \"params.h\"\n #include \"langhooks.h\"\n+#include \"tree-affine.h\"\n \n /* The infinite cost.  */\n #define INFTY 10000000\n@@ -524,57 +525,6 @@ name_info (struct ivopts_data *data, tree name)\n   return ver_info (data, SSA_NAME_VERSION (name));\n }\n \n-/* Checks whether there exists number X such that X * B = A, counting modulo\n-   2^BITS.  */\n-\n-static bool\n-divide (unsigned bits, unsigned HOST_WIDE_INT a, unsigned HOST_WIDE_INT b,\n-\tHOST_WIDE_INT *x)\n-{\n-  unsigned HOST_WIDE_INT mask = ~(~(unsigned HOST_WIDE_INT) 0 << (bits - 1) << 1);\n-  unsigned HOST_WIDE_INT inv, ex, val;\n-  unsigned i;\n-\n-  a &= mask;\n-  b &= mask;\n-\n-  /* First divide the whole equation by 2 as long as possible.  */\n-  while (!(a & 1) && !(b & 1))\n-    {\n-      a >>= 1;\n-      b >>= 1;\n-      bits--;\n-      mask >>= 1;\n-    }\n-\n-  if (!(b & 1))\n-    {\n-      /* If b is still even, a is odd and there is no such x.  */\n-      return false;\n-    }\n-\n-  /* Find the inverse of b.  We compute it as\n-     b^(2^(bits - 1) - 1) (mod 2^bits).  */\n-  inv = 1;\n-  ex = b;\n-  for (i = 0; i < bits - 1; i++)\n-    {\n-      inv = (inv * ex) & mask;\n-      ex = (ex * ex) & mask;\n-    }\n-\n-  val = (a * inv) & mask;\n-\n-  gcc_assert (((val * b) & mask) == a);\n-\n-  if ((val >> (bits - 1)) & 1)\n-    val |= ~mask;\n-\n-  *x = val;\n-\n-  return true;\n-}\n-\n /* Returns true if STMT is after the place where the IP_NORMAL ivs will be\n    emitted in LOOP.  */\n \n@@ -2613,8 +2563,8 @@ constant_multiple_of (tree top, tree bot, double_int *mul)\n       if (TREE_CODE (bot) != INTEGER_CST)\n \treturn false;\n \n-      p0 = double_int_sext (tree_to_double_int (bot), precision);\n-      p1 = double_int_sext (tree_to_double_int (top), precision);\n+      p0 = double_int_sext (tree_to_double_int (top), precision);\n+      p1 = double_int_sext (tree_to_double_int (bot), precision);\n       if (double_int_zero_p (p1))\n \treturn false;\n       *mul = double_int_sext (double_int_sdivmod (p0, p1, FLOOR_DIV_EXPR, &res),\n@@ -2626,354 +2576,6 @@ constant_multiple_of (tree top, tree bot, double_int *mul)\n     }\n }\n \n-/* Sets COMB to CST.  */\n-\n-static void\n-aff_combination_const (struct affine_tree_combination *comb, tree type,\n-\t\t       unsigned HOST_WIDE_INT cst)\n-{\n-  unsigned prec = TYPE_PRECISION (type);\n-\n-  comb->type = type;\n-  comb->mask = (((unsigned HOST_WIDE_INT) 2 << (prec - 1)) - 1);\n-\n-  comb->n = 0;\n-  comb->rest = NULL_TREE;\n-  comb->offset = cst & comb->mask;\n-}\n-\n-/* Sets COMB to single element ELT.  */\n-\n-static void\n-aff_combination_elt (struct affine_tree_combination *comb, tree type, tree elt)\n-{\n-  unsigned prec = TYPE_PRECISION (type);\n-\n-  comb->type = type;\n-  comb->mask = (((unsigned HOST_WIDE_INT) 2 << (prec - 1)) - 1);\n-\n-  comb->n = 1;\n-  comb->elts[0] = elt;\n-  comb->coefs[0] = 1;\n-  comb->rest = NULL_TREE;\n-  comb->offset = 0;\n-}\n-\n-/* Scales COMB by SCALE.  */\n-\n-static void\n-aff_combination_scale (struct affine_tree_combination *comb,\n-\t\t       unsigned HOST_WIDE_INT scale)\n-{\n-  unsigned i, j;\n-\n-  if (scale == 1)\n-    return;\n-\n-  if (scale == 0)\n-    {\n-      aff_combination_const (comb, comb->type, 0);\n-      return;\n-    }\n-\n-  comb->offset = (scale * comb->offset) & comb->mask;\n-  for (i = 0, j = 0; i < comb->n; i++)\n-    {\n-      comb->coefs[j] = (scale * comb->coefs[i]) & comb->mask;\n-      comb->elts[j] = comb->elts[i];\n-      if (comb->coefs[j] != 0)\n-\tj++;\n-    }\n-  comb->n = j;\n-\n-  if (comb->rest)\n-    {\n-      if (comb->n < MAX_AFF_ELTS)\n-\t{\n-\t  comb->coefs[comb->n] = scale;\n-\t  comb->elts[comb->n] = comb->rest;\n-\t  comb->rest = NULL_TREE;\n-\t  comb->n++;\n-\t}\n-      else\n-\tcomb->rest = fold_build2 (MULT_EXPR, comb->type, comb->rest,\n-\t\t\t\t  build_int_cst_type (comb->type, scale));\n-    }\n-}\n-\n-/* Adds ELT * SCALE to COMB.  */\n-\n-static void\n-aff_combination_add_elt (struct affine_tree_combination *comb, tree elt,\n-\t\t\t unsigned HOST_WIDE_INT scale)\n-{\n-  unsigned i;\n-\n-  if (scale == 0)\n-    return;\n-\n-  for (i = 0; i < comb->n; i++)\n-    if (operand_equal_p (comb->elts[i], elt, 0))\n-      {\n-\tcomb->coefs[i] = (comb->coefs[i] + scale) & comb->mask;\n-\tif (comb->coefs[i])\n-\t  return;\n-\n-\tcomb->n--;\n-\tcomb->coefs[i] = comb->coefs[comb->n];\n-\tcomb->elts[i] = comb->elts[comb->n];\n-\n-\tif (comb->rest)\n-\t  {\n-\t    gcc_assert (comb->n == MAX_AFF_ELTS - 1);\n-\t    comb->coefs[comb->n] = 1;\n-\t    comb->elts[comb->n] = comb->rest;\n-\t    comb->rest = NULL_TREE;\n-\t    comb->n++;\n-\t  }\n-\treturn;\n-      }\n-  if (comb->n < MAX_AFF_ELTS)\n-    {\n-      comb->coefs[comb->n] = scale;\n-      comb->elts[comb->n] = elt;\n-      comb->n++;\n-      return;\n-    }\n-\n-  if (scale == 1)\n-    elt = fold_convert (comb->type, elt);\n-  else\n-    elt = fold_build2 (MULT_EXPR, comb->type,\n-\t\t       fold_convert (comb->type, elt),\n-\t\t       build_int_cst_type (comb->type, scale)); \n-\n-  if (comb->rest)\n-    comb->rest = fold_build2 (PLUS_EXPR, comb->type, comb->rest, elt);\n-  else\n-    comb->rest = elt;\n-}\n-\n-/* Adds COMB2 to COMB1.  */\n-\n-static void\n-aff_combination_add (struct affine_tree_combination *comb1,\n-\t\t     struct affine_tree_combination *comb2)\n-{\n-  unsigned i;\n-\n-  comb1->offset = (comb1->offset + comb2->offset) & comb1->mask;\n-  for (i = 0; i < comb2->n; i++)\n-    aff_combination_add_elt (comb1, comb2->elts[i], comb2->coefs[i]);\n-  if (comb2->rest)\n-    aff_combination_add_elt (comb1, comb2->rest, 1);\n-}\n-\n-/* Convert COMB to TYPE.  */\n-\n-static void\n-aff_combination_convert (tree type, struct affine_tree_combination *comb)\n-{\n-  unsigned prec = TYPE_PRECISION (type);\n-  unsigned i;\n-\n-  /* If the precision of both types is the same, it suffices to change the type\n-     of the whole combination -- the elements are allowed to have another type\n-     equivalent wrto STRIP_NOPS.  */\n-  if (prec == TYPE_PRECISION (comb->type))\n-    {\n-      comb->type = type;\n-      return;\n-    }\n-\n-  comb->mask = (((unsigned HOST_WIDE_INT) 2 << (prec - 1)) - 1);\n-  comb->offset = comb->offset & comb->mask;\n-\n-  /* The type of the elements can be different from comb->type only as\n-     much as what STRIP_NOPS would remove.  We can just directly cast\n-     to TYPE.  */\n-  for (i = 0; i < comb->n; i++)\n-    comb->elts[i] = fold_convert (type, comb->elts[i]);\n-  if (comb->rest)\n-    comb->rest = fold_convert (type, comb->rest);\n-\n-  comb->type = type;\n-}\n-\n-/* Splits EXPR into an affine combination of parts.  */\n-\n-static void\n-tree_to_aff_combination (tree expr, tree type,\n-\t\t\t struct affine_tree_combination *comb)\n-{\n-  struct affine_tree_combination tmp;\n-  enum tree_code code;\n-  tree cst, core, toffset;\n-  HOST_WIDE_INT bitpos, bitsize;\n-  enum machine_mode mode;\n-  int unsignedp, volatilep;\n-\n-  STRIP_NOPS (expr);\n-\n-  code = TREE_CODE (expr);\n-  switch (code)\n-    {\n-    case INTEGER_CST:\n-      aff_combination_const (comb, type, int_cst_value (expr));\n-      return;\n-\n-    case PLUS_EXPR:\n-    case MINUS_EXPR:\n-      tree_to_aff_combination (TREE_OPERAND (expr, 0), type, comb);\n-      tree_to_aff_combination (TREE_OPERAND (expr, 1), type, &tmp);\n-      if (code == MINUS_EXPR)\n-\taff_combination_scale (&tmp, -1);\n-      aff_combination_add (comb, &tmp);\n-      return;\n-\n-    case MULT_EXPR:\n-      cst = TREE_OPERAND (expr, 1);\n-      if (TREE_CODE (cst) != INTEGER_CST)\n-\tbreak;\n-      tree_to_aff_combination (TREE_OPERAND (expr, 0), type, comb);\n-      aff_combination_scale (comb, int_cst_value (cst));\n-      return;\n-\n-    case NEGATE_EXPR:\n-      tree_to_aff_combination (TREE_OPERAND (expr, 0), type, comb);\n-      aff_combination_scale (comb, -1);\n-      return;\n-\n-    case ADDR_EXPR:\n-      core = get_inner_reference (TREE_OPERAND (expr, 0), &bitsize, &bitpos,\n-\t\t\t\t  &toffset, &mode, &unsignedp, &volatilep,\n-\t\t\t\t  false);\n-      if (bitpos % BITS_PER_UNIT != 0)\n-\tbreak;\n-      aff_combination_const (comb, type, bitpos / BITS_PER_UNIT);\n-      core = build_fold_addr_expr (core);\n-      if (TREE_CODE (core) == ADDR_EXPR)\n-\taff_combination_add_elt (comb, core, 1);\n-      else\n-\t{\n-\t  tree_to_aff_combination (core, type, &tmp);\n-\t  aff_combination_add (comb, &tmp);\n-\t}\n-      if (toffset)\n-\t{\n-\t  tree_to_aff_combination (toffset, type, &tmp);\n-\t  aff_combination_add (comb, &tmp);\n-\t}\n-      return;\n-\n-    default:\n-      break;\n-    }\n-\n-  aff_combination_elt (comb, type, expr);\n-}\n-\n-/* Creates EXPR + ELT * SCALE in TYPE.  MASK is the mask for width of TYPE.  */\n-\n-static tree\n-add_elt_to_tree (tree expr, tree type, tree elt, unsigned HOST_WIDE_INT scale,\n-\t\t unsigned HOST_WIDE_INT mask)\n-{\n-  enum tree_code code;\n-\n-  scale &= mask;\n-  elt = fold_convert (type, elt);\n-\n-  if (scale == 1)\n-    {\n-      if (!expr)\n-\treturn elt;\n-\n-      return fold_build2 (PLUS_EXPR, type, expr, elt);\n-    }\n-\n-  if (scale == mask)\n-    {\n-      if (!expr)\n-\treturn fold_build1 (NEGATE_EXPR, type, elt);\n-\n-      return fold_build2 (MINUS_EXPR, type, expr, elt);\n-    }\n-\n-  if (!expr)\n-    return fold_build2 (MULT_EXPR, type, elt,\n-\t\t\tbuild_int_cst_type (type, scale));\n-\n-  if ((scale | (mask >> 1)) == mask)\n-    {\n-      /* Scale is negative.  */\n-      code = MINUS_EXPR;\n-      scale = (-scale) & mask;\n-    }\n-  else\n-    code = PLUS_EXPR;\n-\n-  elt = fold_build2 (MULT_EXPR, type, elt,\n-\t\t     build_int_cst_type (type, scale));\n-  return fold_build2 (code, type, expr, elt);\n-}\n-\n-/* Copies the tree elements of COMB to ensure that they are not shared.  */\n-\n-static void\n-unshare_aff_combination (struct affine_tree_combination *comb)\n-{\n-  unsigned i;\n-\n-  for (i = 0; i < comb->n; i++)\n-    comb->elts[i] = unshare_expr (comb->elts[i]);\n-  if (comb->rest)\n-    comb->rest = unshare_expr (comb->rest);\n-}\n-\n-/* Makes tree from the affine combination COMB.  */\n-\n-static tree\n-aff_combination_to_tree (struct affine_tree_combination *comb)\n-{\n-  tree type = comb->type;\n-  tree expr = comb->rest;\n-  unsigned i;\n-  unsigned HOST_WIDE_INT off, sgn;\n-\n-  if (comb->n == 0 && comb->offset == 0)\n-    {\n-      if (expr)\n-\t{\n-\t  /* Handle the special case produced by get_computation_aff when\n-\t     the type does not fit in HOST_WIDE_INT.  */\n-\t  return fold_convert (type, expr);\n-\t}\n-      else\n-\treturn build_int_cst (type, 0);\n-    }\n-\n-  gcc_assert (comb->n == MAX_AFF_ELTS || comb->rest == NULL_TREE);\n-\n-  for (i = 0; i < comb->n; i++)\n-    expr = add_elt_to_tree (expr, type, comb->elts[i], comb->coefs[i],\n-\t\t\t    comb->mask);\n-\n-  if ((comb->offset | (comb->mask >> 1)) == comb->mask)\n-    {\n-      /* Offset is negative.  */\n-      off = (-comb->offset) & comb->mask;\n-      sgn = comb->mask;\n-    }\n-  else\n-    {\n-      off = comb->offset;\n-      sgn = 1;\n-    }\n-  return add_elt_to_tree (expr, type, build_int_cst_type (type, off), sgn,\n-\t\t\t  comb->mask);\n-}\n-\n /* Folds EXPR using the affine expressions framework.  */\n \n static tree\n@@ -3039,16 +2641,11 @@ get_computation_aff (struct loop *loop,\n   tree ubase = use->iv->base;\n   tree ustep = use->iv->step;\n   tree cbase = cand->iv->base;\n-  tree cstep = cand->iv->step;\n+  tree cstep = cand->iv->step, cstep_common;\n   tree utype = TREE_TYPE (ubase), ctype = TREE_TYPE (cbase);\n-  tree common_type;\n+  tree common_type, var;\n   tree uutype;\n-  tree expr, delta;\n-  tree ratio;\n-  unsigned HOST_WIDE_INT ustepi, cstepi;\n-  HOST_WIDE_INT ratioi;\n-  struct affine_tree_combination cbase_aff, expr_aff;\n-  tree cstep_orig = cstep, ustep_orig = ustep;\n+  aff_tree cbase_aff, var_aff;\n   double_int rat;\n \n   if (TYPE_PRECISION (utype) > TYPE_PRECISION (ctype))\n@@ -3057,65 +2654,19 @@ get_computation_aff (struct loop *loop,\n       return false;\n     }\n \n-  expr = var_at_stmt (loop, cand, at);\n-\n-  if (TREE_TYPE (expr) != ctype)\n-    {\n-      /* This may happen with the original ivs.  */\n-      expr = fold_convert (ctype, expr);\n-    }\n-\n-  if (TYPE_UNSIGNED (utype))\n-    uutype = utype;\n-  else\n-    {\n-      uutype = unsigned_type_for (utype);\n-      ubase = fold_convert (uutype, ubase);\n-      ustep = fold_convert (uutype, ustep);\n-    }\n+  var = var_at_stmt (loop, cand, at);\n+  uutype = unsigned_type_for (utype);\n \n-  if (uutype != ctype)\n+  /* If the conversion is not noop, perform it.  */\n+  if (TYPE_PRECISION (utype) < TYPE_PRECISION (ctype))\n     {\n-      expr = fold_convert (uutype, expr);\n-      cbase = fold_convert (uutype, cbase);\n       cstep = fold_convert (uutype, cstep);\n-\n-      /* If the conversion is not noop, we must take it into account when\n-\t considering the value of the step.  */\n-      if (TYPE_PRECISION (utype) < TYPE_PRECISION (ctype))\n-\tcstep_orig = cstep;\n-    }\n-\n-  if (cst_and_fits_in_hwi (cstep_orig)\n-      && cst_and_fits_in_hwi (ustep_orig))\n-    {\n-      ustepi = int_cst_value (ustep_orig);\n-      cstepi = int_cst_value (cstep_orig);\n-\n-      if (!divide (TYPE_PRECISION (uutype), ustepi, cstepi, &ratioi))\n-\t{\n-\t  /* TODO maybe consider case when ustep divides cstep and the ratio is\n-\t     a power of 2 (so that the division is fast to execute)?  We would\n-\t     need to be much more careful with overflows etc. then.  */\n-\t  return false;\n-\t}\n-\n-      ratio = build_int_cst_type (uutype, ratioi);\n+      cbase = fold_convert (uutype, cbase);\n+      var = fold_convert (uutype, var);\n     }\n-  else\n-    {\n-      if (!constant_multiple_of (ustep_orig, cstep_orig, &rat))\n-\treturn false;\n-      ratio = double_int_to_tree (uutype, rat);\n \n-      /* Ratioi is only used to detect special cases when the multiplicative\n-\t factor is 1 or -1, so if rat does not fit to HOST_WIDE_INT, we may\n-\t set it to 0.  */\n-      if (double_int_fits_in_shwi_p (rat))\n-\tratioi = double_int_to_shwi (rat);\n-      else\n-\tratioi = 0;\n-    }\n+  if (!constant_multiple_of (ustep, cstep, &rat))\n+    return false;\n \n   /* In case both UBASE and CBASE are shortened to UUTYPE from some common\n      type, we achieve better folding by computing their difference in this\n@@ -3124,73 +2675,32 @@ get_computation_aff (struct loop *loop,\n      anyway.  */\n   common_type = determine_common_wider_type (&ubase, &cbase);\n \n-  /* We may need to shift the value if we are after the increment.  */\n-  if (stmt_after_increment (loop, cand, at))\n-    {\n-      if (uutype != common_type)\n-\tcstep = fold_convert (common_type, cstep);\n-      cbase = fold_build2 (PLUS_EXPR, common_type, cbase, cstep);\n-    }\n-\n-  /* use = ubase - ratio * cbase + ratio * var.\n-\n-     In general case ubase + ratio * (var - cbase) could be better (one less\n-     multiplication), but often it is possible to eliminate redundant parts\n-     of computations from (ubase - ratio * cbase) term, and if it does not\n-     happen, fold is able to apply the distributive law to obtain this form\n-     anyway.  */\n+  /* use = ubase - ratio * cbase + ratio * var.  */\n+  tree_to_aff_combination (ubase, common_type, aff);\n+  tree_to_aff_combination (cbase, common_type, &cbase_aff);\n+  tree_to_aff_combination (var, uutype, &var_aff);\n \n-  if (TYPE_PRECISION (common_type) > HOST_BITS_PER_WIDE_INT)\n+  /* We need to shift the value if we are after the increment.  */\n+  if (stmt_after_increment (loop, cand, at))\n     {\n-      /* Let's compute in trees and just return the result in AFF.  This case\n-\t should not be very common, and fold itself is not that bad either,\n-\t so making the aff. functions more complicated to handle this case\n-\t is not that urgent.  */\n-      if (ratioi == 1)\n-\t{\n-\t  delta = fold_build2 (MINUS_EXPR, common_type, ubase, cbase);\n-\t  if (uutype != common_type)\n-\t    delta = fold_convert (uutype, delta);\n-\t  expr = fold_build2 (PLUS_EXPR, uutype, expr, delta);\n-\t}\n-      else if (ratioi == -1)\n-\t{\n-\t  delta = fold_build2 (PLUS_EXPR, common_type, ubase, cbase);\n-\t  if (uutype != common_type)\n-\t    delta = fold_convert (uutype, delta);\n-\t  expr = fold_build2 (MINUS_EXPR, uutype, delta, expr);\n-\t}\n+      aff_tree cstep_aff;\n+  \n+      if (common_type != uutype)\n+\tcstep_common = fold_convert (common_type, cstep);\n       else\n-\t{\n-\t  delta = fold_build2 (MULT_EXPR, common_type, cbase, ratio);\n-\t  delta = fold_build2 (MINUS_EXPR, common_type, ubase, delta);\n-\t  if (uutype != common_type)\n-\t    delta = fold_convert (uutype, delta);\n-\t  expr = fold_build2 (MULT_EXPR, uutype, ratio, expr);\n-\t  expr = fold_build2 (PLUS_EXPR, uutype, delta, expr);\n-\t}\n+\tcstep_common = cstep;\n \n-      aff->type = uutype;\n-      aff->n = 0;\n-      aff->offset = 0;\n-      aff->mask = 0;\n-      aff->rest = expr;\n-      return true;\n+      tree_to_aff_combination (cstep_common, common_type, &cstep_aff);\n+      aff_combination_add (&cbase_aff, &cstep_aff);\n     }\n \n-  /* If we got here, the types fits in HOST_WIDE_INT, thus it must be\n-     possible to compute ratioi.  */\n-  gcc_assert (ratioi);\n-\n-  tree_to_aff_combination (ubase, common_type, aff);\n-  tree_to_aff_combination (cbase, common_type, &cbase_aff);\n-  tree_to_aff_combination (expr, uutype, &expr_aff);\n-  aff_combination_scale (&cbase_aff, -ratioi);\n-  aff_combination_scale (&expr_aff, ratioi);\n+  aff_combination_scale (&cbase_aff, double_int_neg (rat));\n   aff_combination_add (aff, &cbase_aff);\n   if (common_type != uutype)\n-    aff_combination_convert (uutype, aff);\n-  aff_combination_add (aff, &expr_aff);\n+    aff_combination_convert (aff, uutype);\n+\n+  aff_combination_scale (&var_aff, rat);\n+  aff_combination_add (aff, &var_aff);\n \n   return true;\n }\n@@ -3202,7 +2712,7 @@ static tree\n get_computation_at (struct loop *loop,\n \t\t    struct iv_use *use, struct iv_cand *cand, tree at)\n {\n-  struct affine_tree_combination aff;\n+  aff_tree aff;\n   tree type = TREE_TYPE (use->iv->base);\n \n   if (!get_computation_aff (loop, use, cand, at, &aff))\n@@ -3862,10 +3372,11 @@ get_computation_cost_at (struct ivopts_data *data,\n   tree ubase = use->iv->base, ustep = use->iv->step;\n   tree cbase, cstep;\n   tree utype = TREE_TYPE (ubase), ctype;\n-  unsigned HOST_WIDE_INT ustepi, cstepi, offset = 0;\n+  unsigned HOST_WIDE_INT cstepi, offset = 0;\n   HOST_WIDE_INT ratio, aratio;\n   bool var_present, symbol_present;\n   unsigned cost = 0, n_sums;\n+  double_int rat;\n \n   *depends_on = NULL;\n \n@@ -3913,26 +3424,13 @@ get_computation_cost_at (struct ivopts_data *data,\n   else\n     cstepi = 0;\n \n-  if (cst_and_fits_in_hwi (ustep)\n-      && cst_and_fits_in_hwi (cstep))\n-    {\n-      ustepi = int_cst_value (ustep);\n-\n-      if (!divide (TYPE_PRECISION (utype), ustepi, cstepi, &ratio))\n-\treturn INFTY;\n-    }\n-  else\n-    {\n-      double_int rat;\n-      \n-      if (!constant_multiple_of (ustep, cstep, &rat))\n-\treturn INFTY;\n+  if (!constant_multiple_of (ustep, cstep, &rat))\n+    return INFTY;\n     \n-      if (double_int_fits_in_shwi_p (rat))\n-\tratio = double_int_to_shwi (rat);\n-      else\n-\treturn INFTY;\n-    }\n+  if (double_int_fits_in_shwi_p (rat))\n+    ratio = double_int_to_shwi (rat);\n+  else\n+    return INFTY;\n \n   /* use = ubase + ratio * (var - cbase).  If either cbase is a constant\n      or ratio == 1, it is better to handle this like\n@@ -5427,7 +4925,10 @@ rewrite_use_nonlinear_expr (struct ivopts_data *data,\n \t\t\t\t   unshare_expr (step)));\n     }\n   else\n-    comp = get_computation (data->current_loop, use, cand);\n+    {\n+      comp = get_computation (data->current_loop, use, cand);\n+      gcc_assert (comp != NULL_TREE);\n+    }\n \n   switch (TREE_CODE (use->stmt))\n     {\n@@ -5593,11 +5094,13 @@ static void\n rewrite_use_address (struct ivopts_data *data,\n \t\t     struct iv_use *use, struct iv_cand *cand)\n {\n-  struct affine_tree_combination aff;\n+  aff_tree aff;\n   block_stmt_iterator bsi = bsi_for_stmt (use->stmt);\n   tree ref;\n+  bool ok;\n \n-  get_computation_aff (data->current_loop, use, cand, use->stmt, &aff);\n+  ok = get_computation_aff (data->current_loop, use, cand, use->stmt, &aff);\n+  gcc_assert (ok);\n   unshare_aff_combination (&aff);\n \n   ref = create_mem_ref (&bsi, TREE_TYPE (*use->op_p), &aff);\n@@ -5640,6 +5143,7 @@ rewrite_use_compare (struct ivopts_data *data,\n   /* The induction variable elimination failed; just express the original\n      giv.  */\n   comp = get_computation (data->current_loop, use, cand);\n+  gcc_assert (comp != NULL_TREE);\n \n   cond = *use->op_p;\n   op_p = &TREE_OPERAND (cond, 0);"}]}