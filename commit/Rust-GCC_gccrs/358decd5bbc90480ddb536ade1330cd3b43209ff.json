{"sha": "358decd5bbc90480ddb536ade1330cd3b43209ff", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MzU4ZGVjZDViYmM5MDQ4MGRkYjUzNmFkZTEzMzBjZDNiNDMyMDlmZg==", "commit": {"author": {"name": "Jiong Wang", "email": "jiong.wang@arm.com", "date": "2016-07-25T14:02:42Z"}, "committer": {"name": "Jiong Wang", "email": "jiwang@gcc.gnu.org", "date": "2016-07-25T14:02:42Z"}, "message": "[AArch64][1/10] ARMv8.2-A FP16 data processing intrinsics\n\ngcc/\n\t* config/aarch64/aarch64-simd.md\n\t(aarch64_<PERMUTE:perm_insn><PERMUTE:perm_hilo><mode>): Use VALL_F16.\n\t(aarch64_ext<mode>): Likewise.\n\t(aarch64_rev<REVERSE:rev_op><mode>): Likewise.\n\t* config/aarch64/aarch64.c (aarch64_evpc_trn): Support V4HFmode and\n\tV8HFmode.\n\t(aarch64_evpc_uzp): Likewise.\n\t(aarch64_evpc_zip): Likewise.\n\t(aarch64_evpc_ext): Likewise.\n\t(aarch64_evpc_rev): Likewise.\n\t* config/aarch64/arm_neon.h (__aarch64_vdup_lane_f16): New.\n\t(__aarch64_vdup_laneq_f16): New..\n\t(__aarch64_vdupq_lane_f16): New.\n\t(__aarch64_vdupq_laneq_f16): New.\n\t(vbsl_f16): New.\n\t(vbslq_f16): New.\n\t(vdup_n_f16): New.\n\t(vdupq_n_f16): New.\n\t(vdup_lane_f16): New.\n\t(vdup_laneq_f16): New.\n\t(vdupq_lane_f16): New.\n\t(vdupq_laneq_f16): New.\n\t(vduph_lane_f16): New.\n\t(vduph_laneq_f16): New.\n\t(vext_f16): New.\n\t(vextq_f16): New.\n\t(vmov_n_f16): New.\n\t(vmovq_n_f16): New.\n\t(vrev64_f16): New.\n\t(vrev64q_f16): New.\n\t(vtrn1_f16): New.\n\t(vtrn1q_f16): New.\n\t(vtrn2_f16): New.\n\t(vtrn2q_f16): New.\n\t(vtrn_f16): New.\n\t(vtrnq_f16): New.\n\t(__INTERLEAVE_LIST): Support float16x4_t, float16x8_t.\n\t(vuzp1_f16): New.\n\t(vuzp1q_f16): New.\n\t(vuzp2_f16): New.\n\t(vuzp2q_f16): New.\n\t(vzip1_f16): New.\n\t(vzip2q_f16): New.\n\t(vmov_n_f16): Reimplement using vdup_n_f16.\n\t(vmovq_n_f16): Reimplement using vdupq_n_f16..\n\nFrom-SVN: r238715", "tree": {"sha": "3d977a902c3044d838c2880fb4e372df30690d91", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/3d977a902c3044d838c2880fb4e372df30690d91"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/358decd5bbc90480ddb536ade1330cd3b43209ff", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/358decd5bbc90480ddb536ade1330cd3b43209ff", "html_url": "https://github.com/Rust-GCC/gccrs/commit/358decd5bbc90480ddb536ade1330cd3b43209ff", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/358decd5bbc90480ddb536ade1330cd3b43209ff/comments", "author": null, "committer": null, "parents": [{"sha": "37d6a4b7799e83ffa638091ae78f7e5e3133263f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/37d6a4b7799e83ffa638091ae78f7e5e3133263f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/37d6a4b7799e83ffa638091ae78f7e5e3133263f"}], "stats": {"total": 361, "additions": 346, "deletions": 15}, "files": [{"sha": "1e3f304a79d8a498737b317c96fc89301bb76ef3", "filename": "gcc/ChangeLog", "status": "modified", "additions": 48, "deletions": 0, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/358decd5bbc90480ddb536ade1330cd3b43209ff/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/358decd5bbc90480ddb536ade1330cd3b43209ff/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=358decd5bbc90480ddb536ade1330cd3b43209ff", "patch": "@@ -1,3 +1,51 @@\n+2016-07-25  Jiong Wang  <jiong.wang@arm.com>\n+\n+\t* config/aarch64/aarch64-simd.md\n+\t(aarch64_<PERMUTE:perm_insn><PERMUTE:perm_hilo><mode>): Use VALL_F16.\n+\t(aarch64_ext<mode>): Likewise.\n+\t(aarch64_rev<REVERSE:rev_op><mode>): Likewise.\n+\t* config/aarch64/aarch64.c (aarch64_evpc_trn): Support V4HFmode and\n+\tV8HFmode.\n+\t(aarch64_evpc_uzp): Likewise.\n+\t(aarch64_evpc_zip): Likewise.\n+\t(aarch64_evpc_ext): Likewise.\n+\t(aarch64_evpc_rev): Likewise.\n+\t* config/aarch64/arm_neon.h (__aarch64_vdup_lane_f16): New.\n+\t(__aarch64_vdup_laneq_f16): New..\n+\t(__aarch64_vdupq_lane_f16): New.\n+\t(__aarch64_vdupq_laneq_f16): New.\n+\t(vbsl_f16): New.\n+\t(vbslq_f16): New.\n+\t(vdup_n_f16): New.\n+\t(vdupq_n_f16): New.\n+\t(vdup_lane_f16): New.\n+\t(vdup_laneq_f16): New.\n+\t(vdupq_lane_f16): New.\n+\t(vdupq_laneq_f16): New.\n+\t(vduph_lane_f16): New.\n+\t(vduph_laneq_f16): New.\n+\t(vext_f16): New.\n+\t(vextq_f16): New.\n+\t(vmov_n_f16): New.\n+\t(vmovq_n_f16): New.\n+\t(vrev64_f16): New.\n+\t(vrev64q_f16): New.\n+\t(vtrn1_f16): New.\n+\t(vtrn1q_f16): New.\n+\t(vtrn2_f16): New.\n+\t(vtrn2q_f16): New.\n+\t(vtrn_f16): New.\n+\t(vtrnq_f16): New.\n+\t(__INTERLEAVE_LIST): Support float16x4_t, float16x8_t.\n+\t(vuzp1_f16): New.\n+\t(vuzp1q_f16): New.\n+\t(vuzp2_f16): New.\n+\t(vuzp2q_f16): New.\n+\t(vzip1_f16): New.\n+\t(vzip2q_f16): New.\n+\t(vmov_n_f16): Reimplement using vdup_n_f16.\n+\t(vmovq_n_f16): Reimplement using vdupq_n_f16..\n+\n 2016-07-25  Jiong Wang  <jiong.wang@arm.com>\n \n \t* config/aarch64/aarch64.c (aarch64_add_constant): New parameter"}, {"sha": "251ad972a4bed027f8c77946fb21ce8d94dc3035", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/358decd5bbc90480ddb536ade1330cd3b43209ff/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/358decd5bbc90480ddb536ade1330cd3b43209ff/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=358decd5bbc90480ddb536ade1330cd3b43209ff", "patch": "@@ -5219,22 +5219,22 @@\n )\n \n (define_insn \"aarch64_<PERMUTE:perm_insn><PERMUTE:perm_hilo><mode>\"\n-  [(set (match_operand:VALL 0 \"register_operand\" \"=w\")\n-\t(unspec:VALL [(match_operand:VALL 1 \"register_operand\" \"w\")\n-\t\t      (match_operand:VALL 2 \"register_operand\" \"w\")]\n-\t\t       PERMUTE))]\n+  [(set (match_operand:VALL_F16 0 \"register_operand\" \"=w\")\n+\t(unspec:VALL_F16 [(match_operand:VALL_F16 1 \"register_operand\" \"w\")\n+\t\t\t  (match_operand:VALL_F16 2 \"register_operand\" \"w\")]\n+\t PERMUTE))]\n   \"TARGET_SIMD\"\n   \"<PERMUTE:perm_insn><PERMUTE:perm_hilo>\\\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>\"\n   [(set_attr \"type\" \"neon_permute<q>\")]\n )\n \n ;; Note immediate (third) operand is lane index not byte index.\n (define_insn \"aarch64_ext<mode>\"\n-  [(set (match_operand:VALL 0 \"register_operand\" \"=w\")\n-        (unspec:VALL [(match_operand:VALL 1 \"register_operand\" \"w\")\n-                      (match_operand:VALL 2 \"register_operand\" \"w\")\n-                      (match_operand:SI 3 \"immediate_operand\" \"i\")]\n-                     UNSPEC_EXT))]\n+  [(set (match_operand:VALL_F16 0 \"register_operand\" \"=w\")\n+        (unspec:VALL_F16 [(match_operand:VALL_F16 1 \"register_operand\" \"w\")\n+\t\t\t  (match_operand:VALL_F16 2 \"register_operand\" \"w\")\n+\t\t\t  (match_operand:SI 3 \"immediate_operand\" \"i\")]\n+\t UNSPEC_EXT))]\n   \"TARGET_SIMD\"\n {\n   operands[3] = GEN_INT (INTVAL (operands[3])\n@@ -5245,8 +5245,8 @@\n )\n \n (define_insn \"aarch64_rev<REVERSE:rev_op><mode>\"\n-  [(set (match_operand:VALL 0 \"register_operand\" \"=w\")\n-\t(unspec:VALL [(match_operand:VALL 1 \"register_operand\" \"w\")]\n+  [(set (match_operand:VALL_F16 0 \"register_operand\" \"=w\")\n+\t(unspec:VALL_F16 [(match_operand:VALL_F16 1 \"register_operand\" \"w\")]\n                     REVERSE))]\n   \"TARGET_SIMD\"\n   \"rev<REVERSE:rev_op>\\\\t%0.<Vtype>, %1.<Vtype>\""}, {"sha": "381cf7d3b8509ff5c0d51294ef555c4ea4d6eedd", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/358decd5bbc90480ddb536ade1330cd3b43209ff/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/358decd5bbc90480ddb536ade1330cd3b43209ff/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=358decd5bbc90480ddb536ade1330cd3b43209ff", "patch": "@@ -12286,6 +12286,8 @@ aarch64_evpc_trn (struct expand_vec_perm_d *d)\n \tcase V4SImode: gen = gen_aarch64_trn2v4si; break;\n \tcase V2SImode: gen = gen_aarch64_trn2v2si; break;\n \tcase V2DImode: gen = gen_aarch64_trn2v2di; break;\n+\tcase V4HFmode: gen = gen_aarch64_trn2v4hf; break;\n+\tcase V8HFmode: gen = gen_aarch64_trn2v8hf; break;\n \tcase V4SFmode: gen = gen_aarch64_trn2v4sf; break;\n \tcase V2SFmode: gen = gen_aarch64_trn2v2sf; break;\n \tcase V2DFmode: gen = gen_aarch64_trn2v2df; break;\n@@ -12304,6 +12306,8 @@ aarch64_evpc_trn (struct expand_vec_perm_d *d)\n \tcase V4SImode: gen = gen_aarch64_trn1v4si; break;\n \tcase V2SImode: gen = gen_aarch64_trn1v2si; break;\n \tcase V2DImode: gen = gen_aarch64_trn1v2di; break;\n+\tcase V4HFmode: gen = gen_aarch64_trn1v4hf; break;\n+\tcase V8HFmode: gen = gen_aarch64_trn1v8hf; break;\n \tcase V4SFmode: gen = gen_aarch64_trn1v4sf; break;\n \tcase V2SFmode: gen = gen_aarch64_trn1v2sf; break;\n \tcase V2DFmode: gen = gen_aarch64_trn1v2df; break;\n@@ -12369,6 +12373,8 @@ aarch64_evpc_uzp (struct expand_vec_perm_d *d)\n \tcase V4SImode: gen = gen_aarch64_uzp2v4si; break;\n \tcase V2SImode: gen = gen_aarch64_uzp2v2si; break;\n \tcase V2DImode: gen = gen_aarch64_uzp2v2di; break;\n+\tcase V4HFmode: gen = gen_aarch64_uzp2v4hf; break;\n+\tcase V8HFmode: gen = gen_aarch64_uzp2v8hf; break;\n \tcase V4SFmode: gen = gen_aarch64_uzp2v4sf; break;\n \tcase V2SFmode: gen = gen_aarch64_uzp2v2sf; break;\n \tcase V2DFmode: gen = gen_aarch64_uzp2v2df; break;\n@@ -12387,6 +12393,8 @@ aarch64_evpc_uzp (struct expand_vec_perm_d *d)\n \tcase V4SImode: gen = gen_aarch64_uzp1v4si; break;\n \tcase V2SImode: gen = gen_aarch64_uzp1v2si; break;\n \tcase V2DImode: gen = gen_aarch64_uzp1v2di; break;\n+\tcase V4HFmode: gen = gen_aarch64_uzp1v4hf; break;\n+\tcase V8HFmode: gen = gen_aarch64_uzp1v8hf; break;\n \tcase V4SFmode: gen = gen_aarch64_uzp1v4sf; break;\n \tcase V2SFmode: gen = gen_aarch64_uzp1v2sf; break;\n \tcase V2DFmode: gen = gen_aarch64_uzp1v2df; break;\n@@ -12457,6 +12465,8 @@ aarch64_evpc_zip (struct expand_vec_perm_d *d)\n \tcase V4SImode: gen = gen_aarch64_zip2v4si; break;\n \tcase V2SImode: gen = gen_aarch64_zip2v2si; break;\n \tcase V2DImode: gen = gen_aarch64_zip2v2di; break;\n+\tcase V4HFmode: gen = gen_aarch64_zip2v4hf; break;\n+\tcase V8HFmode: gen = gen_aarch64_zip2v8hf; break;\n \tcase V4SFmode: gen = gen_aarch64_zip2v4sf; break;\n \tcase V2SFmode: gen = gen_aarch64_zip2v2sf; break;\n \tcase V2DFmode: gen = gen_aarch64_zip2v2df; break;\n@@ -12475,6 +12485,8 @@ aarch64_evpc_zip (struct expand_vec_perm_d *d)\n \tcase V4SImode: gen = gen_aarch64_zip1v4si; break;\n \tcase V2SImode: gen = gen_aarch64_zip1v2si; break;\n \tcase V2DImode: gen = gen_aarch64_zip1v2di; break;\n+\tcase V4HFmode: gen = gen_aarch64_zip1v4hf; break;\n+\tcase V8HFmode: gen = gen_aarch64_zip1v8hf; break;\n \tcase V4SFmode: gen = gen_aarch64_zip1v4sf; break;\n \tcase V2SFmode: gen = gen_aarch64_zip1v2sf; break;\n \tcase V2DFmode: gen = gen_aarch64_zip1v2df; break;\n@@ -12519,6 +12531,8 @@ aarch64_evpc_ext (struct expand_vec_perm_d *d)\n     case V8HImode: gen = gen_aarch64_extv8hi; break;\n     case V2SImode: gen = gen_aarch64_extv2si; break;\n     case V4SImode: gen = gen_aarch64_extv4si; break;\n+    case V4HFmode: gen = gen_aarch64_extv4hf; break;\n+    case V8HFmode: gen = gen_aarch64_extv8hf; break;\n     case V2SFmode: gen = gen_aarch64_extv2sf; break;\n     case V4SFmode: gen = gen_aarch64_extv4sf; break;\n     case V2DImode: gen = gen_aarch64_extv2di; break;\n@@ -12594,6 +12608,8 @@ aarch64_evpc_rev (struct expand_vec_perm_d *d)\n \tcase V2SImode: gen = gen_aarch64_rev64v2si;  break;\n \tcase V4SFmode: gen = gen_aarch64_rev64v4sf;  break;\n \tcase V2SFmode: gen = gen_aarch64_rev64v2sf;  break;\n+\tcase V8HFmode: gen = gen_aarch64_rev64v8hf;  break;\n+\tcase V4HFmode: gen = gen_aarch64_rev64v4hf;  break;\n \tdefault:\n \t  return false;\n \t}"}, {"sha": "fd5f094de6a058065e2b1377f5ffc4c1aba01f97", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 271, "deletions": 4, "changes": 275, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/358decd5bbc90480ddb536ade1330cd3b43209ff/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/358decd5bbc90480ddb536ade1330cd3b43209ff/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=358decd5bbc90480ddb536ade1330cd3b43209ff", "patch": "@@ -466,6 +466,8 @@ typedef struct poly16x8x4_t\n #define __aarch64_vdup_lane_any(__size, __q, __a, __b) \\\n   vdup##__q##_n_##__size (__aarch64_vget_lane_any (__a, __b))\n \n+#define __aarch64_vdup_lane_f16(__a, __b) \\\n+   __aarch64_vdup_lane_any (f16, , __a, __b)\n #define __aarch64_vdup_lane_f32(__a, __b) \\\n    __aarch64_vdup_lane_any (f32, , __a, __b)\n #define __aarch64_vdup_lane_f64(__a, __b) \\\n@@ -492,6 +494,8 @@ typedef struct poly16x8x4_t\n    __aarch64_vdup_lane_any (u64, , __a, __b)\n \n /* __aarch64_vdup_laneq internal macros.  */\n+#define __aarch64_vdup_laneq_f16(__a, __b) \\\n+   __aarch64_vdup_lane_any (f16, , __a, __b)\n #define __aarch64_vdup_laneq_f32(__a, __b) \\\n    __aarch64_vdup_lane_any (f32, , __a, __b)\n #define __aarch64_vdup_laneq_f64(__a, __b) \\\n@@ -518,6 +522,8 @@ typedef struct poly16x8x4_t\n    __aarch64_vdup_lane_any (u64, , __a, __b)\n \n /* __aarch64_vdupq_lane internal macros.  */\n+#define __aarch64_vdupq_lane_f16(__a, __b) \\\n+   __aarch64_vdup_lane_any (f16, q, __a, __b)\n #define __aarch64_vdupq_lane_f32(__a, __b) \\\n    __aarch64_vdup_lane_any (f32, q, __a, __b)\n #define __aarch64_vdupq_lane_f64(__a, __b) \\\n@@ -544,6 +550,8 @@ typedef struct poly16x8x4_t\n    __aarch64_vdup_lane_any (u64, q, __a, __b)\n \n /* __aarch64_vdupq_laneq internal macros.  */\n+#define __aarch64_vdupq_laneq_f16(__a, __b) \\\n+   __aarch64_vdup_lane_any (f16, q, __a, __b)\n #define __aarch64_vdupq_laneq_f32(__a, __b) \\\n    __aarch64_vdup_lane_any (f32, q, __a, __b)\n #define __aarch64_vdupq_laneq_f64(__a, __b) \\\n@@ -10213,6 +10221,12 @@ vaddvq_f64 (float64x2_t __a)\n \n /* vbsl  */\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vbsl_f16 (uint16x4_t __a, float16x4_t __b, float16x4_t __c)\n+{\n+  return __builtin_aarch64_simd_bslv4hf_suss (__a, __b, __c);\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vbsl_f32 (uint32x2_t __a, float32x2_t __b, float32x2_t __c)\n {\n@@ -10288,6 +10302,12 @@ vbsl_u64 (uint64x1_t __a, uint64x1_t __b, uint64x1_t __c)\n       {__builtin_aarch64_simd_bsldi_uuuu (__a[0], __b[0], __c[0])};\n }\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vbslq_f16 (uint16x8_t __a, float16x8_t __b, float16x8_t __c)\n+{\n+  return __builtin_aarch64_simd_bslv8hf_suss (__a, __b, __c);\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vbslq_f32 (uint32x4_t __a, float32x4_t __b, float32x4_t __c)\n {\n@@ -13243,6 +13263,12 @@ vcvtpq_u64_f64 (float64x2_t __a)\n \n /* vdup_n  */\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vdup_n_f16 (float16_t __a)\n+{\n+  return (float16x4_t) {__a, __a, __a, __a};\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vdup_n_f32 (float32_t __a)\n {\n@@ -13317,6 +13343,12 @@ vdup_n_u64 (uint64_t __a)\n \n /* vdupq_n  */\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vdupq_n_f16 (float16_t __a)\n+{\n+  return (float16x8_t) {__a, __a, __a, __a, __a, __a, __a, __a};\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vdupq_n_f32 (float32_t __a)\n {\n@@ -13394,6 +13426,12 @@ vdupq_n_u64 (uint64_t __a)\n \n /* vdup_lane  */\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vdup_lane_f16 (float16x4_t __a, const int __b)\n+{\n+  return __aarch64_vdup_lane_f16 (__a, __b);\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vdup_lane_f32 (float32x2_t __a, const int __b)\n {\n@@ -13468,6 +13506,12 @@ vdup_lane_u64 (uint64x1_t __a, const int __b)\n \n /* vdup_laneq  */\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vdup_laneq_f16 (float16x8_t __a, const int __b)\n+{\n+  return __aarch64_vdup_laneq_f16 (__a, __b);\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vdup_laneq_f32 (float32x4_t __a, const int __b)\n {\n@@ -13541,6 +13585,13 @@ vdup_laneq_u64 (uint64x2_t __a, const int __b)\n }\n \n /* vdupq_lane  */\n+\n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vdupq_lane_f16 (float16x4_t __a, const int __b)\n+{\n+  return __aarch64_vdupq_lane_f16 (__a, __b);\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vdupq_lane_f32 (float32x2_t __a, const int __b)\n {\n@@ -13614,6 +13665,13 @@ vdupq_lane_u64 (uint64x1_t __a, const int __b)\n }\n \n /* vdupq_laneq  */\n+\n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vdupq_laneq_f16 (float16x8_t __a, const int __b)\n+{\n+  return __aarch64_vdupq_laneq_f16 (__a, __b);\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vdupq_laneq_f32 (float32x4_t __a, const int __b)\n {\n@@ -13706,6 +13764,13 @@ vdupb_lane_u8 (uint8x8_t __a, const int __b)\n }\n \n /* vduph_lane  */\n+\n+__extension__ static __inline float16_t __attribute__ ((__always_inline__))\n+vduph_lane_f16 (float16x4_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_any (__a, __b);\n+}\n+\n __extension__ static __inline poly16_t __attribute__ ((__always_inline__))\n vduph_lane_p16 (poly16x4_t __a, const int __b)\n {\n@@ -13725,6 +13790,7 @@ vduph_lane_u16 (uint16x4_t __a, const int __b)\n }\n \n /* vdups_lane  */\n+\n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vdups_lane_f32 (float32x2_t __a, const int __b)\n {\n@@ -13785,6 +13851,13 @@ vdupb_laneq_u8 (uint8x16_t __a, const int __b)\n }\n \n /* vduph_laneq  */\n+\n+__extension__ static __inline float16_t __attribute__ ((__always_inline__))\n+vduph_laneq_f16 (float16x8_t __a, const int __b)\n+{\n+  return __aarch64_vget_lane_any (__a, __b);\n+}\n+\n __extension__ static __inline poly16_t __attribute__ ((__always_inline__))\n vduph_laneq_p16 (poly16x8_t __a, const int __b)\n {\n@@ -13804,6 +13877,7 @@ vduph_laneq_u16 (uint16x8_t __a, const int __b)\n }\n \n /* vdups_laneq  */\n+\n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vdups_laneq_f32 (float32x4_t __a, const int __b)\n {\n@@ -13843,6 +13917,19 @@ vdupd_laneq_u64 (uint64x2_t __a, const int __b)\n \n /* vext  */\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vext_f16 (float16x4_t __a, float16x4_t __b, __const int __c)\n+{\n+  __AARCH64_LANE_CHECK (__a, __c);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a,\n+\t\t\t    (uint16x4_t) {4 - __c, 5 - __c, 6 - __c, 7 - __c});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+\t\t\t    (uint16x4_t) {__c, __c + 1, __c + 2, __c + 3});\n+#endif\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vext_f32 (float32x2_t __a, float32x2_t __b, __const int __c)\n {\n@@ -13974,6 +14061,22 @@ vext_u64 (uint64x1_t __a, uint64x1_t __b, __const int __c)\n   return __a;\n }\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vextq_f16 (float16x8_t __a, float16x8_t __b, __const int __c)\n+{\n+  __AARCH64_LANE_CHECK (__a, __c);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__b, __a,\n+\t\t\t    (uint16x8_t) {8 - __c, 9 - __c, 10 - __c, 11 - __c,\n+\t\t\t\t\t  12 - __c, 13 - __c, 14 - __c,\n+\t\t\t\t\t  15 - __c});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+\t\t\t    (uint16x8_t) {__c, __c + 1, __c + 2, __c + 3,\n+\t\t\t\t\t  __c + 4, __c + 5, __c + 6, __c + 7});\n+#endif\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vextq_f32 (float32x4_t __a, float32x4_t __b, __const int __c)\n {\n@@ -14609,8 +14712,7 @@ vld1q_u64 (const uint64_t *a)\n __extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n vld1_dup_f16 (const float16_t* __a)\n {\n-  float16_t __f = *__a;\n-  return (float16x4_t) { __f, __f, __f, __f };\n+  return vdup_n_f16 (*__a);\n }\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n@@ -14690,8 +14792,7 @@ vld1_dup_u64 (const uint64_t* __a)\n __extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n vld1q_dup_f16 (const float16_t* __a)\n {\n-  float16_t __f = *__a;\n-  return (float16x8_t) { __f, __f, __f, __f, __f, __f, __f, __f };\n+  return vdupq_n_f16 (*__a);\n }\n \n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n@@ -18294,6 +18395,12 @@ vmlsq_laneq_u32 (uint32x4_t __a, uint32x4_t __b,\n \n /* vmov_n_  */\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vmov_n_f16 (float16_t __a)\n+{\n+  return vdup_n_f16 (__a);\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vmov_n_f32 (float32_t __a)\n {\n@@ -18366,6 +18473,12 @@ vmov_n_u64 (uint64_t __a)\n   return (uint64x1_t) {__a};\n }\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vmovq_n_f16 (float16_t __a)\n+{\n+  return vdupq_n_f16 (__a);\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vmovq_n_f32 (float32_t __a)\n {\n@@ -21123,6 +21236,12 @@ vrev32q_u16 (uint16x8_t a)\n   return __builtin_shuffle (a, (uint16x8_t) { 1, 0, 3, 2, 5, 4, 7, 6 });\n }\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vrev64_f16 (float16x4_t __a)\n+{\n+  return __builtin_shuffle (__a, (uint16x4_t) { 3, 2, 1, 0 });\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vrev64_f32 (float32x2_t a)\n {\n@@ -21177,6 +21296,12 @@ vrev64_u32 (uint32x2_t a)\n   return __builtin_shuffle (a, (uint32x2_t) { 1, 0 });\n }\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vrev64q_f16 (float16x8_t __a)\n+{\n+  return __builtin_shuffle (__a, (uint16x8_t) { 3, 2, 1, 0, 7, 6, 5, 4 });\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vrev64q_f32 (float32x4_t a)\n {\n@@ -24129,6 +24254,16 @@ vtbx4_p8 (poly8x8_t __r, poly8x8x4_t __tab, uint8x8_t __idx)\n \n /* vtrn */\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vtrn1_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {5, 1, 7, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {0, 4, 2, 6});\n+#endif\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vtrn1_f32 (float32x2_t __a, float32x2_t __b)\n {\n@@ -24219,6 +24354,16 @@ vtrn1_u32 (uint32x2_t __a, uint32x2_t __b)\n #endif\n }\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vtrn1q_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {9, 1, 11, 3, 13, 5, 15, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {0, 8, 2, 10, 4, 12, 6, 14});\n+#endif\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vtrn1q_f32 (float32x4_t __a, float32x4_t __b)\n {\n@@ -24345,6 +24490,16 @@ vtrn1q_u64 (uint64x2_t __a, uint64x2_t __b)\n #endif\n }\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vtrn2_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {4, 0, 6, 2});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {1, 5, 3, 7});\n+#endif\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vtrn2_f32 (float32x2_t __a, float32x2_t __b)\n {\n@@ -24435,6 +24590,16 @@ vtrn2_u32 (uint32x2_t __a, uint32x2_t __b)\n #endif\n }\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vtrn2q_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {8, 0, 10, 2, 12, 4, 14, 6});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {1, 9, 3, 11, 5, 13, 7, 15});\n+#endif\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vtrn2q_f32 (float32x4_t __a, float32x4_t __b)\n {\n@@ -24561,6 +24726,12 @@ vtrn2q_u64 (uint64x2_t __a, uint64x2_t __b)\n #endif\n }\n \n+__extension__ static __inline float16x4x2_t __attribute__ ((__always_inline__))\n+vtrn_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+  return (float16x4x2_t) {vtrn1_f16 (__a, __b), vtrn2_f16 (__a, __b)};\n+}\n+\n __extension__ static __inline float32x2x2_t __attribute__ ((__always_inline__))\n vtrn_f32 (float32x2_t a, float32x2_t b)\n {\n@@ -24615,6 +24786,12 @@ vtrn_u32 (uint32x2_t a, uint32x2_t b)\n   return (uint32x2x2_t) {vtrn1_u32 (a, b), vtrn2_u32 (a, b)};\n }\n \n+__extension__ static __inline float16x8x2_t __attribute__ ((__always_inline__))\n+vtrnq_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+  return (float16x8x2_t) {vtrn1q_f16 (__a, __b), vtrn2q_f16 (__a, __b)};\n+}\n+\n __extension__ static __inline float32x4x2_t __attribute__ ((__always_inline__))\n vtrnq_f32 (float32x4_t a, float32x4_t b)\n {\n@@ -24863,6 +25040,7 @@ vuqaddd_s64 (int64_t __a, uint64_t __b)\n   }\n \n #define __INTERLEAVE_LIST(op)\t\t\t\t\t\\\n+  __DEFINTERLEAVE (op, float16x4x2_t, float16x4_t, f16,)\t\\\n   __DEFINTERLEAVE (op, float32x2x2_t, float32x2_t, f32,)\t\\\n   __DEFINTERLEAVE (op, poly8x8x2_t, poly8x8_t, p8,)\t\t\\\n   __DEFINTERLEAVE (op, poly16x4x2_t, poly16x4_t, p16,)\t\t\\\n@@ -24872,6 +25050,7 @@ vuqaddd_s64 (int64_t __a, uint64_t __b)\n   __DEFINTERLEAVE (op, uint8x8x2_t, uint8x8_t, u8,)\t\t\\\n   __DEFINTERLEAVE (op, uint16x4x2_t, uint16x4_t, u16,)\t\t\\\n   __DEFINTERLEAVE (op, uint32x2x2_t, uint32x2_t, u32,)\t\t\\\n+  __DEFINTERLEAVE (op, float16x8x2_t, float16x8_t, f16, q)\t\\\n   __DEFINTERLEAVE (op, float32x4x2_t, float32x4_t, f32, q)\t\\\n   __DEFINTERLEAVE (op, poly8x16x2_t, poly8x16_t, p8, q)\t\t\\\n   __DEFINTERLEAVE (op, poly16x8x2_t, poly16x8_t, p16, q)\t\\\n@@ -24884,6 +25063,16 @@ vuqaddd_s64 (int64_t __a, uint64_t __b)\n \n /* vuzp */\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vuzp1_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {5, 7, 1, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {0, 2, 4, 6});\n+#endif\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vuzp1_f32 (float32x2_t __a, float32x2_t __b)\n {\n@@ -24974,6 +25163,16 @@ vuzp1_u32 (uint32x2_t __a, uint32x2_t __b)\n #endif\n }\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vuzp1q_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {9, 11, 13, 15, 1, 3, 5, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {0, 2, 4, 6, 8, 10, 12, 14});\n+#endif\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vuzp1q_f32 (float32x4_t __a, float32x4_t __b)\n {\n@@ -25100,6 +25299,16 @@ vuzp1q_u64 (uint64x2_t __a, uint64x2_t __b)\n #endif\n }\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vuzp2_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {4, 6, 0, 2});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {1, 3, 5, 7});\n+#endif\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vuzp2_f32 (float32x2_t __a, float32x2_t __b)\n {\n@@ -25190,6 +25399,16 @@ vuzp2_u32 (uint32x2_t __a, uint32x2_t __b)\n #endif\n }\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vuzp2q_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {8, 10, 12, 14, 0, 2, 4, 6});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {1, 3, 5, 7, 9, 11, 13, 15});\n+#endif\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vuzp2q_f32 (float32x4_t __a, float32x4_t __b)\n {\n@@ -25320,6 +25539,16 @@ __INTERLEAVE_LIST (uzp)\n \n /* vzip */\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vzip1_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {6, 2, 7, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {0, 4, 1, 5});\n+#endif\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vzip1_f32 (float32x2_t __a, float32x2_t __b)\n {\n@@ -25410,6 +25639,18 @@ vzip1_u32 (uint32x2_t __a, uint32x2_t __b)\n #endif\n }\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vzip1q_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b,\n+\t\t\t    (uint16x8_t) {12, 4, 13, 5, 14, 6, 15, 7});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+\t\t\t    (uint16x8_t) {0, 8, 1, 9, 2, 10, 3, 11});\n+#endif\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vzip1q_f32 (float32x4_t __a, float32x4_t __b)\n {\n@@ -25539,6 +25780,16 @@ vzip1q_u64 (uint64x2_t __a, uint64x2_t __b)\n #endif\n }\n \n+__extension__ static __inline float16x4_t __attribute__ ((__always_inline__))\n+vzip2_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {4, 0, 5, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {2, 6, 3, 7});\n+#endif\n+}\n+\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vzip2_f32 (float32x2_t __a, float32x2_t __b)\n {\n@@ -25629,6 +25880,18 @@ vzip2_u32 (uint32x2_t __a, uint32x2_t __b)\n #endif\n }\n \n+__extension__ static __inline float16x8_t __attribute__ ((__always_inline__))\n+vzip2q_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b,\n+\t\t\t    (uint16x8_t) {8, 0, 9, 1, 10, 2, 11, 3});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+\t\t\t    (uint16x8_t) {4, 12, 5, 13, 6, 14, 7, 15});\n+#endif\n+}\n+\n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vzip2q_f32 (float32x4_t __a, float32x4_t __b)\n {\n@@ -25768,6 +26031,7 @@ __INTERLEAVE_LIST (zip)\n #undef __aarch64_vget_lane_any\n \n #undef __aarch64_vdup_lane_any\n+#undef __aarch64_vdup_lane_f16\n #undef __aarch64_vdup_lane_f32\n #undef __aarch64_vdup_lane_f64\n #undef __aarch64_vdup_lane_p8\n@@ -25780,6 +26044,7 @@ __INTERLEAVE_LIST (zip)\n #undef __aarch64_vdup_lane_u16\n #undef __aarch64_vdup_lane_u32\n #undef __aarch64_vdup_lane_u64\n+#undef __aarch64_vdup_laneq_f16\n #undef __aarch64_vdup_laneq_f32\n #undef __aarch64_vdup_laneq_f64\n #undef __aarch64_vdup_laneq_p8\n@@ -25792,6 +26057,7 @@ __INTERLEAVE_LIST (zip)\n #undef __aarch64_vdup_laneq_u16\n #undef __aarch64_vdup_laneq_u32\n #undef __aarch64_vdup_laneq_u64\n+#undef __aarch64_vdupq_lane_f16\n #undef __aarch64_vdupq_lane_f32\n #undef __aarch64_vdupq_lane_f64\n #undef __aarch64_vdupq_lane_p8\n@@ -25804,6 +26070,7 @@ __INTERLEAVE_LIST (zip)\n #undef __aarch64_vdupq_lane_u16\n #undef __aarch64_vdupq_lane_u32\n #undef __aarch64_vdupq_lane_u64\n+#undef __aarch64_vdupq_laneq_f16\n #undef __aarch64_vdupq_laneq_f32\n #undef __aarch64_vdupq_laneq_f64\n #undef __aarch64_vdupq_laneq_p8"}]}