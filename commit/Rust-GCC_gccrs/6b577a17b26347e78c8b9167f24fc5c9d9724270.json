{"sha": "6b577a17b26347e78c8b9167f24fc5c9d9724270", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NmI1NzdhMTdiMjYzNDdlNzhjOGI5MTY3ZjI0ZmM1YzlkOTcyNDI3MA==", "commit": {"author": {"name": "Julian Brown", "email": "julian@codesourcery.com", "date": "2020-10-21T17:00:19Z"}, "committer": {"name": "Julian Brown", "email": "julian@codesourcery.com", "date": "2021-01-05T17:56:36Z"}, "message": "nvptx: Cache stacks block for OpenMP kernel launch\n\n2021-01-05  Julian Brown  <julian@codesourcery.com>\n\nlibgomp/\n\t* plugin/plugin-nvptx.c (SOFTSTACK_CACHE_LIMIT): New define.\n\t(struct ptx_device): Add omp_stacks struct.\n\t(nvptx_open_device): Initialise cached-stacks housekeeping info.\n\t(nvptx_close_device): Free cached stacks block and mutex.\n\t(nvptx_stacks_free): New function.\n\t(nvptx_alloc): Add SUPPRESS_ERRORS parameter.\n\t(GOMP_OFFLOAD_alloc): Add strategies for freeing soft-stacks block.\n\t(nvptx_stacks_alloc): Rename to...\n\t(nvptx_stacks_acquire): This.  Cache stacks block between runs if same\n\tsize or smaller is required.\n\t(nvptx_stacks_free): Remove.\n\t(GOMP_OFFLOAD_run): Call nvptx_stacks_acquire and lock stacks block\n\tduring kernel execution.", "tree": {"sha": "6b81c6a7319ca62e499d75538f4d3c57dab4dea1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6b81c6a7319ca62e499d75538f4d3c57dab4dea1"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6b577a17b26347e78c8b9167f24fc5c9d9724270", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6b577a17b26347e78c8b9167f24fc5c9d9724270", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6b577a17b26347e78c8b9167f24fc5c9d9724270", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6b577a17b26347e78c8b9167f24fc5c9d9724270/comments", "author": {"login": "jtb20", "id": 6094880, "node_id": "MDQ6VXNlcjYwOTQ4ODA=", "avatar_url": "https://avatars.githubusercontent.com/u/6094880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jtb20", "html_url": "https://github.com/jtb20", "followers_url": "https://api.github.com/users/jtb20/followers", "following_url": "https://api.github.com/users/jtb20/following{/other_user}", "gists_url": "https://api.github.com/users/jtb20/gists{/gist_id}", "starred_url": "https://api.github.com/users/jtb20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jtb20/subscriptions", "organizations_url": "https://api.github.com/users/jtb20/orgs", "repos_url": "https://api.github.com/users/jtb20/repos", "events_url": "https://api.github.com/users/jtb20/events{/privacy}", "received_events_url": "https://api.github.com/users/jtb20/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jtb20", "id": 6094880, "node_id": "MDQ6VXNlcjYwOTQ4ODA=", "avatar_url": "https://avatars.githubusercontent.com/u/6094880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jtb20", "html_url": "https://github.com/jtb20", "followers_url": "https://api.github.com/users/jtb20/followers", "following_url": "https://api.github.com/users/jtb20/following{/other_user}", "gists_url": "https://api.github.com/users/jtb20/gists{/gist_id}", "starred_url": "https://api.github.com/users/jtb20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jtb20/subscriptions", "organizations_url": "https://api.github.com/users/jtb20/orgs", "repos_url": "https://api.github.com/users/jtb20/repos", "events_url": "https://api.github.com/users/jtb20/events{/privacy}", "received_events_url": "https://api.github.com/users/jtb20/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "407bcf8e28f417a2be94d9a0a38a0b7effcf2b94", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/407bcf8e28f417a2be94d9a0a38a0b7effcf2b94", "html_url": "https://github.com/Rust-GCC/gccrs/commit/407bcf8e28f417a2be94d9a0a38a0b7effcf2b94"}], "stats": {"total": 114, "additions": 96, "deletions": 18}, "files": [{"sha": "681c344b9c2cd94f70367c70c009c111e87a149f", "filename": "libgomp/plugin/plugin-nvptx.c", "status": "modified", "additions": 96, "deletions": 18, "changes": 114, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6b577a17b26347e78c8b9167f24fc5c9d9724270/libgomp%2Fplugin%2Fplugin-nvptx.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6b577a17b26347e78c8b9167f24fc5c9d9724270/libgomp%2Fplugin%2Fplugin-nvptx.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Fplugin%2Fplugin-nvptx.c?ref=6b577a17b26347e78c8b9167f24fc5c9d9724270", "patch": "@@ -49,6 +49,15 @@\n #include <assert.h>\n #include <errno.h>\n \n+/* An arbitrary fixed limit (128MB) for the size of the OpenMP soft stacks\n+   block to cache between kernel invocations.  For soft-stacks blocks bigger\n+   than this, we will free the block before attempting another GPU memory\n+   allocation (i.e. in GOMP_OFFLOAD_alloc).  Otherwise, if an allocation fails,\n+   we will free the cached soft-stacks block anyway then retry the\n+   allocation.  If that fails too, we lose.  */\n+\n+#define SOFTSTACK_CACHE_LIMIT 134217728\n+\n #if CUDA_VERSION < 6000\n extern CUresult cuGetErrorString (CUresult, const char **);\n #define CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_MULTIPROCESSOR 82\n@@ -307,6 +316,14 @@ struct ptx_device\n   struct ptx_free_block *free_blocks;\n   pthread_mutex_t free_blocks_lock;\n \n+  /* OpenMP stacks, cached between kernel invocations.  */\n+  struct\n+    {\n+      CUdeviceptr ptr;\n+      size_t size;\n+      pthread_mutex_t lock;\n+    } omp_stacks;\n+\n   struct ptx_device *next;\n };\n \n@@ -514,6 +531,10 @@ nvptx_open_device (int n)\n   ptx_dev->free_blocks = NULL;\n   pthread_mutex_init (&ptx_dev->free_blocks_lock, NULL);\n \n+  ptx_dev->omp_stacks.ptr = 0;\n+  ptx_dev->omp_stacks.size = 0;\n+  pthread_mutex_init (&ptx_dev->omp_stacks.lock, NULL);\n+\n   return ptx_dev;\n }\n \n@@ -534,6 +555,11 @@ nvptx_close_device (struct ptx_device *ptx_dev)\n   pthread_mutex_destroy (&ptx_dev->free_blocks_lock);\n   pthread_mutex_destroy (&ptx_dev->image_lock);\n \n+  pthread_mutex_destroy (&ptx_dev->omp_stacks.lock);\n+\n+  if (ptx_dev->omp_stacks.ptr)\n+    CUDA_CALL (cuMemFree, ptx_dev->omp_stacks.ptr);\n+\n   if (!ptx_dev->ctx_shared)\n     CUDA_CALL (cuCtxDestroy, ptx_dev->ctx);\n \n@@ -999,12 +1025,40 @@ goacc_profiling_acc_ev_alloc (struct goacc_thread *thr, void *dp, size_t s)\n   GOMP_PLUGIN_goacc_profiling_dispatch (prof_info, &data_event_info, api_info);\n }\n \n+/* Free the cached soft-stacks block if it is above the SOFTSTACK_CACHE_LIMIT\n+   size threshold, or if FORCE is true.  */\n+\n+static void\n+nvptx_stacks_free (struct ptx_device *ptx_dev, bool force)\n+{\n+  pthread_mutex_lock (&ptx_dev->omp_stacks.lock);\n+  if (ptx_dev->omp_stacks.ptr\n+      && (force || ptx_dev->omp_stacks.size > SOFTSTACK_CACHE_LIMIT))\n+    {\n+      CUresult r = CUDA_CALL_NOCHECK (cuMemFree, ptx_dev->omp_stacks.ptr);\n+      if (r != CUDA_SUCCESS)\n+\tGOMP_PLUGIN_fatal (\"cuMemFree error: %s\", cuda_error (r));\n+      ptx_dev->omp_stacks.ptr = 0;\n+      ptx_dev->omp_stacks.size = 0;\n+    }\n+  pthread_mutex_unlock (&ptx_dev->omp_stacks.lock);\n+}\n+\n static void *\n-nvptx_alloc (size_t s)\n+nvptx_alloc (size_t s, bool suppress_errors)\n {\n   CUdeviceptr d;\n \n-  CUDA_CALL_ERET (NULL, cuMemAlloc, &d, s);\n+  CUresult r = CUDA_CALL_NOCHECK (cuMemAlloc, &d, s);\n+  if (suppress_errors && r == CUDA_ERROR_OUT_OF_MEMORY)\n+    return NULL;\n+  else if (r != CUDA_SUCCESS)\n+    {\n+      GOMP_PLUGIN_error (\"nvptx_alloc error: %s\", cuda_error (r));\n+      return NULL;\n+    }\n+\n+  /* NOTE: We only do profiling stuff if the memory allocation succeeds.  */\n   struct goacc_thread *thr = GOMP_PLUGIN_goacc_thread ();\n   bool profiling_p\n     = __builtin_expect (thr != NULL && thr->prof_info != NULL, false);\n@@ -1352,6 +1406,8 @@ GOMP_OFFLOAD_alloc (int ord, size_t size)\n   ptx_dev->free_blocks = NULL;\n   pthread_mutex_unlock (&ptx_dev->free_blocks_lock);\n \n+  nvptx_stacks_free (ptx_dev, false);\n+\n   while (blocks)\n     {\n       tmp = blocks->next;\n@@ -1360,7 +1416,16 @@ GOMP_OFFLOAD_alloc (int ord, size_t size)\n       blocks = tmp;\n     }\n \n-  return nvptx_alloc (size);\n+  void *d = nvptx_alloc (size, true);\n+  if (d)\n+    return d;\n+  else\n+    {\n+      /* Memory allocation failed.  Try freeing the stacks block, and\n+\t retrying.  */\n+      nvptx_stacks_free (ptx_dev, true);\n+      return nvptx_alloc (size, false);\n+    }\n }\n \n bool\n@@ -1866,26 +1931,36 @@ nvptx_stacks_size ()\n   return 128 * 1024;\n }\n \n-/* Return contiguous storage for NUM stacks, each SIZE bytes.  */\n+/* Return contiguous storage for NUM stacks, each SIZE bytes.  The lock for\n+   the storage should be held on entry, and remains held on exit.  */\n \n static void *\n-nvptx_stacks_alloc (size_t size, int num)\n+nvptx_stacks_acquire (struct ptx_device *ptx_dev, size_t size, int num)\n {\n-  CUdeviceptr stacks;\n-  CUresult r = CUDA_CALL_NOCHECK (cuMemAlloc, &stacks, size * num);\n+  if (ptx_dev->omp_stacks.ptr && ptx_dev->omp_stacks.size >= size * num)\n+    return (void *) ptx_dev->omp_stacks.ptr;\n+\n+  /* Free the old, too-small stacks.  */\n+  if (ptx_dev->omp_stacks.ptr)\n+    {\n+      CUresult r = CUDA_CALL_NOCHECK (cuCtxSynchronize, );\n+      if (r != CUDA_SUCCESS)\n+\tGOMP_PLUGIN_fatal (\"cuCtxSynchronize error: %s\\n\", cuda_error (r));\n+      r = CUDA_CALL_NOCHECK (cuMemFree, ptx_dev->omp_stacks.ptr);\n+      if (r != CUDA_SUCCESS)\n+\tGOMP_PLUGIN_fatal (\"cuMemFree error: %s\", cuda_error (r));\n+    }\n+\n+  /* Make new and bigger stacks, and remember where we put them and how big\n+     they are.  */\n+  CUresult r = CUDA_CALL_NOCHECK (cuMemAlloc, &ptx_dev->omp_stacks.ptr,\n+\t\t\t\t  size * num);\n   if (r != CUDA_SUCCESS)\n     GOMP_PLUGIN_fatal (\"cuMemAlloc error: %s\", cuda_error (r));\n-  return (void *) stacks;\n-}\n \n-/* Release storage previously allocated by nvptx_stacks_alloc.  */\n+  ptx_dev->omp_stacks.size = size * num;\n \n-static void\n-nvptx_stacks_free (void *p, int num)\n-{\n-  CUresult r = CUDA_CALL_NOCHECK (cuMemFree, (CUdeviceptr) p);\n-  if (r != CUDA_SUCCESS)\n-    GOMP_PLUGIN_fatal (\"cuMemFree error: %s\", cuda_error (r));\n+  return (void *) ptx_dev->omp_stacks.ptr;\n }\n \n void\n@@ -1922,7 +1997,9 @@ GOMP_OFFLOAD_run (int ord, void *tgt_fn, void *tgt_vars, void **args)\n   nvptx_adjust_launch_bounds (tgt_fn, ptx_dev, &teams, &threads);\n \n   size_t stack_size = nvptx_stacks_size ();\n-  void *stacks = nvptx_stacks_alloc (stack_size, teams * threads);\n+\n+  pthread_mutex_lock (&ptx_dev->omp_stacks.lock);\n+  void *stacks = nvptx_stacks_acquire (ptx_dev, stack_size, teams * threads);\n   void *fn_args[] = {tgt_vars, stacks, (void *) stack_size};\n   size_t fn_args_size = sizeof fn_args;\n   void *config[] = {\n@@ -1944,7 +2021,8 @@ GOMP_OFFLOAD_run (int ord, void *tgt_fn, void *tgt_vars, void **args)\n \t\t       maybe_abort_msg);\n   else if (r != CUDA_SUCCESS)\n     GOMP_PLUGIN_fatal (\"cuCtxSynchronize error: %s\", cuda_error (r));\n-  nvptx_stacks_free (stacks, teams * threads);\n+\n+  pthread_mutex_unlock (&ptx_dev->omp_stacks.lock);\n }\n \n /* TODO: Implement GOMP_OFFLOAD_async_run. */"}]}