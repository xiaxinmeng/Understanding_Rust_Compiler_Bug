{"sha": "c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YzYyOWMyNDYwNDIyMmQ3YjNhOGU2YzhhNTAxMGY0ZjNkMmExYjFhNg==", "commit": {"author": {"name": "Gleb Fotengauer-Malinovskiy", "email": "glebfm@altlinux.org", "date": "2015-08-20T17:55:24Z"}, "committer": {"name": "Torvald Riegel", "email": "torvald@gcc.gnu.org", "date": "2015-08-20T17:55:24Z"}, "message": "libitm: Don't redefine __always_inline in local_atomic.\n\nFrom-SVN: r227040", "tree": {"sha": "3a721ae3e193b41bd8571aecd2c916c5c8eea2f6", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/3a721ae3e193b41bd8571aecd2c916c5c8eea2f6"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6/comments", "author": {"login": "glebfm", "id": 1343999, "node_id": "MDQ6VXNlcjEzNDM5OTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1343999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glebfm", "html_url": "https://github.com/glebfm", "followers_url": "https://api.github.com/users/glebfm/followers", "following_url": "https://api.github.com/users/glebfm/following{/other_user}", "gists_url": "https://api.github.com/users/glebfm/gists{/gist_id}", "starred_url": "https://api.github.com/users/glebfm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glebfm/subscriptions", "organizations_url": "https://api.github.com/users/glebfm/orgs", "repos_url": "https://api.github.com/users/glebfm/repos", "events_url": "https://api.github.com/users/glebfm/events{/privacy}", "received_events_url": "https://api.github.com/users/glebfm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "f4cd6a8d567ba14f7c936e53bf6c65724c6d42b7", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f4cd6a8d567ba14f7c936e53bf6c65724c6d42b7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f4cd6a8d567ba14f7c936e53bf6c65724c6d42b7"}], "stats": {"total": 305, "additions": 155, "deletions": 150}, "files": [{"sha": "6285c85fd44aff37722e6e04c00c2898102f9aaa", "filename": "libitm/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6/libitm%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6/libitm%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2FChangeLog?ref=c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6", "patch": "@@ -1,3 +1,9 @@\n+2015-08-20  Gleb Fotengauer-Malinovskiy  <glebfm@altlinux.org>  (tiny change)\n+\n+\tPR libitm/61164\n+\t* local_atomic (__always_inline): Rename to...\n+\t(__libitm_always_inline): ... this.\n+\n 2015-07-03  Carlos S\u00e1nchez de La Lama  <csanchezdll@gmail.com>\n \n \tPR target/52482"}, {"sha": "e536275dc9fab1249d8f9d067def512c427bbc4d", "filename": "libitm/local_atomic", "status": "modified", "additions": 149, "deletions": 150, "changes": 299, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6/libitm%2Flocal_atomic", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6/libitm%2Flocal_atomic", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Flocal_atomic?ref=c629c24604222d7b3a8e6c8a5010f4f3d2a1b1a6", "patch": "@@ -41,8 +41,7 @@\n #ifndef _GLIBCXX_ATOMIC\n #define _GLIBCXX_ATOMIC 1\n \n-#undef  __always_inline\n-#define __always_inline __attribute__((always_inline))\n+#define __libitm_always_inline __attribute__((always_inline))\n \n // #pragma GCC system_header\n \n@@ -74,7 +73,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       memory_order_seq_cst\n     } memory_order;\n \n-  inline __always_inline memory_order\n+  inline __libitm_always_inline memory_order\n   __calculate_memory_order(memory_order __m) noexcept\n   {\n     const bool __cond1 = __m == memory_order_release;\n@@ -84,13 +83,13 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     return __mo2;\n   }\n \n-  inline __always_inline void\n+  inline __libitm_always_inline void\n   atomic_thread_fence(memory_order __m) noexcept\n   {\n     __atomic_thread_fence (__m);\n   }\n \n-  inline __always_inline void\n+  inline __libitm_always_inline void\n   atomic_signal_fence(memory_order __m) noexcept\n   {\n     __atomic_thread_fence (__m);\n@@ -280,19 +279,19 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     // Conversion to ATOMIC_FLAG_INIT.\n     atomic_flag(bool __i) noexcept : __atomic_flag_base({ __i }) { }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     test_and_set(memory_order __m = memory_order_seq_cst) noexcept\n     {\n       return __atomic_test_and_set (&_M_i, __m);\n     }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept\n     {\n       return __atomic_test_and_set (&_M_i, __m);\n     }\n \n-    __always_inline void\n+    __libitm_always_inline void\n     clear(memory_order __m = memory_order_seq_cst) noexcept\n     {\n       // __glibcxx_assert(__m != memory_order_consume);\n@@ -302,7 +301,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       __atomic_clear (&_M_i, __m);\n     }\n \n-    __always_inline void\n+    __libitm_always_inline void\n     clear(memory_order __m = memory_order_seq_cst) volatile noexcept\n     {\n       // __glibcxx_assert(__m != memory_order_consume);\n@@ -455,7 +454,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       is_lock_free() const volatile noexcept\n       { return __atomic_is_lock_free (sizeof (_M_i), &_M_i); }\n \n-      __always_inline void\n+      __libitm_always_inline void\n       store(__int_type __i, memory_order __m = memory_order_seq_cst) noexcept\n       {\n \t// __glibcxx_assert(__m != memory_order_acquire);\n@@ -465,7 +464,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \t__atomic_store_n(&_M_i, __i, __m);\n       }\n \n-      __always_inline void\n+      __libitm_always_inline void\n       store(__int_type __i,\n \t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n@@ -476,7 +475,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \t__atomic_store_n(&_M_i, __i, __m);\n       }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       load(memory_order __m = memory_order_seq_cst) const noexcept\n       {\n \t// __glibcxx_assert(__m != memory_order_release);\n@@ -485,7 +484,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_load_n(&_M_i, __m);\n       }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       {\n \t// __glibcxx_assert(__m != memory_order_release);\n@@ -494,21 +493,21 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_load_n(&_M_i, __m);\n       }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       exchange(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn __atomic_exchange_n(&_M_i, __i, __m);\n       }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       exchange(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn __atomic_exchange_n(&_M_i, __i, __m);\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t\t    memory_order __m1, memory_order __m2) noexcept\n       {\n@@ -519,7 +518,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 1, __m1, __m2);\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t\t    memory_order __m1,\n \t\t\t    memory_order __m2) volatile noexcept\n@@ -531,23 +530,23 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 1, __m1, __m2);\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_weak(__i1, __i2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t   memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_weak(__i1, __i2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t\t      memory_order __m1, memory_order __m2) noexcept\n       {\n@@ -558,7 +557,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0, __m1, __m2);\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) volatile noexcept\n@@ -570,68 +569,68 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0, __m1, __m2);\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t\t      memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_strong(__i1, __i2, __m,\n \t\t\t\t       __calculate_memory_order(__m));\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_strong(__i1, __i2, __m,\n \t\t\t\t       __calculate_memory_order(__m));\n       }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       fetch_add(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_add(&_M_i, __i, __m); }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       fetch_add(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_add(&_M_i, __i, __m); }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       fetch_sub(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_sub(&_M_i, __i, __m); }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       fetch_sub(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_sub(&_M_i, __i, __m); }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       fetch_and(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_and(&_M_i, __i, __m); }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       fetch_and(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_and(&_M_i, __i, __m); }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       fetch_or(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_or(&_M_i, __i, __m); }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       fetch_or(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_or(&_M_i, __i, __m); }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       fetch_xor(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_xor(&_M_i, __i, __m); }\n \n-      __always_inline __int_type\n+      __libitm_always_inline __int_type\n       fetch_xor(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_xor(&_M_i, __i, __m); }\n@@ -733,7 +732,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       is_lock_free() const volatile noexcept\n       { return __atomic_is_lock_free (sizeof (_M_p), &_M_p); }\n \n-      __always_inline void\n+      __libitm_always_inline void\n       store(__pointer_type __p,\n \t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n@@ -744,7 +743,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \t__atomic_store_n(&_M_p, __p, __m);\n       }\n \n-      __always_inline void\n+      __libitm_always_inline void\n       store(__pointer_type __p,\n \t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n@@ -755,7 +754,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \t__atomic_store_n(&_M_p, __p, __m);\n       }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       load(memory_order __m = memory_order_seq_cst) const noexcept\n       {\n \t// __glibcxx_assert(__m != memory_order_release);\n@@ -764,7 +763,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_load_n(&_M_p, __m);\n       }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       {\n \t// __glibcxx_assert(__m != memory_order_release);\n@@ -773,21 +772,21 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_load_n(&_M_p, __m);\n       }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       exchange(__pointer_type __p,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn __atomic_exchange_n(&_M_p, __p, __m);\n       }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       exchange(__pointer_type __p,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn __atomic_exchange_n(&_M_p, __p, __m);\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) noexcept\n@@ -799,7 +798,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0, __m1, __m2);\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) volatile noexcept\n@@ -811,22 +810,22 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0, __m1, __m2);\n       }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       fetch_add(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_add(&_M_p, __d, __m); }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       fetch_add(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_add(&_M_p, __d, __m); }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       fetch_sub(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_sub(&_M_p, __d, __m); }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       fetch_sub(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_sub(&_M_p, __d, __m); }\n@@ -870,67 +869,67 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     bool\n     is_lock_free() const volatile noexcept { return _M_base.is_lock_free(); }\n \n-    __always_inline void\n+    __libitm_always_inline void\n     store(bool __i, memory_order __m = memory_order_seq_cst) noexcept\n     { _M_base.store(__i, __m); }\n \n-    __always_inline void\n+    __libitm_always_inline void\n     store(bool __i, memory_order __m = memory_order_seq_cst) volatile noexcept\n     { _M_base.store(__i, __m); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     load(memory_order __m = memory_order_seq_cst) const noexcept\n     { return _M_base.load(__m); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n     { return _M_base.load(__m); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     exchange(bool __i, memory_order __m = memory_order_seq_cst) noexcept\n     { return _M_base.exchange(__i, __m); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     exchange(bool __i,\n \t     memory_order __m = memory_order_seq_cst) volatile noexcept\n     { return _M_base.exchange(__i, __m); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     compare_exchange_weak(bool& __i1, bool __i2, memory_order __m1,\n \t\t\t  memory_order __m2) noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m1, __m2); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     compare_exchange_weak(bool& __i1, bool __i2, memory_order __m1,\n \t\t\t  memory_order __m2) volatile noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m1, __m2); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     compare_exchange_weak(bool& __i1, bool __i2,\n \t\t\t  memory_order __m = memory_order_seq_cst) noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     compare_exchange_weak(bool& __i1, bool __i2,\n \t\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     compare_exchange_strong(bool& __i1, bool __i2, memory_order __m1,\n \t\t\t    memory_order __m2) noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m1, __m2); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     compare_exchange_strong(bool& __i1, bool __i2, memory_order __m1,\n \t\t\t    memory_order __m2) volatile noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m1, __m2); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     compare_exchange_strong(bool& __i1, bool __i2,\n \t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m); }\n \n-    __always_inline bool\n+    __libitm_always_inline bool\n     compare_exchange_strong(bool& __i1, bool __i2,\n \t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m); }\n@@ -980,35 +979,35 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       store(_Tp __i, memory_order _m = memory_order_seq_cst) noexcept\n       { __atomic_store(&_M_i, &__i, _m); }\n \n-      __always_inline void\n+      __libitm_always_inline void\n       store(_Tp __i, memory_order _m = memory_order_seq_cst) volatile noexcept\n       { __atomic_store(&_M_i, &__i, _m); }\n \n-      __always_inline _Tp\n+      __libitm_always_inline _Tp\n       load(memory_order _m = memory_order_seq_cst) const noexcept\n       { \n         _Tp tmp;\n \t__atomic_load(&_M_i, &tmp, _m); \n \treturn tmp;\n       }\n \n-      __always_inline _Tp\n+      __libitm_always_inline _Tp\n       load(memory_order _m = memory_order_seq_cst) const volatile noexcept\n       { \n         _Tp tmp;\n \t__atomic_load(&_M_i, &tmp, _m); \n \treturn tmp;\n       }\n \n-      __always_inline _Tp\n+      __libitm_always_inline _Tp\n       exchange(_Tp __i, memory_order _m = memory_order_seq_cst) noexcept\n       { \n         _Tp tmp;\n \t__atomic_exchange(&_M_i, &__i, &tmp, _m); \n \treturn tmp;\n       }\n \n-      __always_inline _Tp\n+      __libitm_always_inline _Tp\n       exchange(_Tp __i, \n \t       memory_order _m = memory_order_seq_cst) volatile noexcept\n       { \n@@ -1017,50 +1016,50 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn tmp;\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(_Tp& __e, _Tp __i, memory_order __s, \n \t\t\t    memory_order __f) noexcept\n       {\n \treturn __atomic_compare_exchange(&_M_i, &__e, &__i, true, __s, __f); \n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(_Tp& __e, _Tp __i, memory_order __s, \n \t\t\t    memory_order __f) volatile noexcept\n       {\n \treturn __atomic_compare_exchange(&_M_i, &__e, &__i, true, __s, __f); \n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(_Tp& __e, _Tp __i,\n \t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n       { return compare_exchange_weak(__e, __i, __m, __m); }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(_Tp& __e, _Tp __i,\n \t\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return compare_exchange_weak(__e, __i, __m, __m); }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(_Tp& __e, _Tp __i, memory_order __s, \n \t\t\t      memory_order __f) noexcept\n       {\n \treturn __atomic_compare_exchange(&_M_i, &__e, &__i, false, __s, __f); \n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(_Tp& __e, _Tp __i, memory_order __s, \n \t\t\t      memory_order __f) volatile noexcept\n       {\n \treturn __atomic_compare_exchange(&_M_i, &__e, &__i, false, __s, __f); \n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(_Tp& __e, _Tp __i,\n \t\t\t       memory_order __m = memory_order_seq_cst) noexcept\n       { return compare_exchange_strong(__e, __i, __m, __m); }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(_Tp& __e, _Tp __i,\n \t\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return compare_exchange_strong(__e, __i, __m, __m); }\n@@ -1153,104 +1152,104 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       is_lock_free() const volatile noexcept\n       { return _M_b.is_lock_free(); }\n \n-      __always_inline void\n+      __libitm_always_inline void\n       store(__pointer_type __p,\n \t    memory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.store(__p, __m); }\n \n-      __always_inline void\n+      __libitm_always_inline void\n       store(__pointer_type __p,\n \t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.store(__p, __m); }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       load(memory_order __m = memory_order_seq_cst) const noexcept\n       { return _M_b.load(__m); }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       { return _M_b.load(__m); }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       exchange(__pointer_type __p,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.exchange(__p, __m); }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       exchange(__pointer_type __p,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.exchange(__p, __m); }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t    memory_order __m1, memory_order __m2) noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t    memory_order __m1,\n \t\t\t    memory_order __m2) volatile noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_weak(__p1, __p2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n \t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_weak(__p1, __p2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m1, memory_order __m2) noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) volatile noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn _M_b.compare_exchange_strong(__p1, __p2, __m,\n \t\t\t\t\t    __calculate_memory_order(__m));\n       }\n \n-      __always_inline bool\n+      __libitm_always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn _M_b.compare_exchange_strong(__p1, __p2, __m,\n \t\t\t\t\t    __calculate_memory_order(__m));\n       }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       fetch_add(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.fetch_add(__d, __m); }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       fetch_add(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.fetch_add(__d, __m); }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       fetch_sub(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.fetch_sub(__d, __m); }\n \n-      __always_inline __pointer_type\n+      __libitm_always_inline __pointer_type\n       fetch_sub(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.fetch_sub(__d, __m); }\n@@ -1544,122 +1543,122 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \n \n   // Function definitions, atomic_flag operations.\n-  inline __always_inline bool\n+  inline __libitm_always_inline bool\n   atomic_flag_test_and_set_explicit(atomic_flag* __a,\n \t\t\t\t    memory_order __m) noexcept\n   { return __a->test_and_set(__m); }\n \n-  inline __always_inline bool\n+  inline __libitm_always_inline bool\n   atomic_flag_test_and_set_explicit(volatile atomic_flag* __a,\n \t\t\t\t    memory_order __m) noexcept\n   { return __a->test_and_set(__m); }\n \n-  inline __always_inline void\n+  inline __libitm_always_inline void\n   atomic_flag_clear_explicit(atomic_flag* __a, memory_order __m) noexcept\n   { __a->clear(__m); }\n \n-  inline __always_inline void\n+  inline __libitm_always_inline void\n   atomic_flag_clear_explicit(volatile atomic_flag* __a,\n \t\t\t     memory_order __m) noexcept\n   { __a->clear(__m); }\n \n-  inline __always_inline bool\n+  inline __libitm_always_inline bool\n   atomic_flag_test_and_set(atomic_flag* __a) noexcept\n   { return atomic_flag_test_and_set_explicit(__a, memory_order_seq_cst); }\n \n-  inline __always_inline bool\n+  inline __libitm_always_inline bool\n   atomic_flag_test_and_set(volatile atomic_flag* __a) noexcept\n   { return atomic_flag_test_and_set_explicit(__a, memory_order_seq_cst); }\n \n-  inline __always_inline void\n+  inline __libitm_always_inline void\n   atomic_flag_clear(atomic_flag* __a) noexcept\n   { atomic_flag_clear_explicit(__a, memory_order_seq_cst); }\n \n-  inline __always_inline void\n+  inline __libitm_always_inline void\n   atomic_flag_clear(volatile atomic_flag* __a) noexcept\n   { atomic_flag_clear_explicit(__a, memory_order_seq_cst); }\n \n \n   // Function templates generally applicable to atomic types.\n   template<typename _ITp>\n-    __always_inline bool\n+    __libitm_always_inline bool\n     atomic_is_lock_free(const atomic<_ITp>* __a) noexcept\n     { return __a->is_lock_free(); }\n \n   template<typename _ITp>\n-    __always_inline bool\n+    __libitm_always_inline bool\n     atomic_is_lock_free(const volatile atomic<_ITp>* __a) noexcept\n     { return __a->is_lock_free(); }\n \n   template<typename _ITp>\n-    __always_inline void\n+    __libitm_always_inline void\n     atomic_init(atomic<_ITp>* __a, _ITp __i) noexcept;\n \n   template<typename _ITp>\n-    __always_inline void\n+    __libitm_always_inline void\n     atomic_init(volatile atomic<_ITp>* __a, _ITp __i) noexcept;\n \n   template<typename _ITp>\n-    __always_inline void\n+    __libitm_always_inline void\n     atomic_store_explicit(atomic<_ITp>* __a, _ITp __i,\n \t\t\t  memory_order __m) noexcept\n     { __a->store(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline void\n+    __libitm_always_inline void\n     atomic_store_explicit(volatile atomic<_ITp>* __a, _ITp __i,\n \t\t\t  memory_order __m) noexcept\n     { __a->store(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_load_explicit(const atomic<_ITp>* __a, memory_order __m) noexcept\n     { return __a->load(__m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_load_explicit(const volatile atomic<_ITp>* __a,\n \t\t\t memory_order __m) noexcept\n     { return __a->load(__m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_exchange_explicit(atomic<_ITp>* __a, _ITp __i,\n \t\t\t     memory_order __m) noexcept\n     { return __a->exchange(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_exchange_explicit(volatile atomic<_ITp>* __a, _ITp __i,\n \t\t\t     memory_order __m) noexcept\n     { return __a->exchange(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline bool\n+    __libitm_always_inline bool\n     atomic_compare_exchange_weak_explicit(atomic<_ITp>* __a,\n \t\t\t\t\t  _ITp* __i1, _ITp __i2,\n \t\t\t\t\t  memory_order __m1,\n \t\t\t\t\t  memory_order __m2) noexcept\n     { return __a->compare_exchange_weak(*__i1, __i2, __m1, __m2); }\n \n   template<typename _ITp>\n-    __always_inline bool\n+    __libitm_always_inline bool\n     atomic_compare_exchange_weak_explicit(volatile atomic<_ITp>* __a,\n \t\t\t\t\t  _ITp* __i1, _ITp __i2,\n \t\t\t\t\t  memory_order __m1,\n \t\t\t\t\t  memory_order __m2) noexcept\n     { return __a->compare_exchange_weak(*__i1, __i2, __m1, __m2); }\n \n   template<typename _ITp>\n-    __always_inline bool\n+    __libitm_always_inline bool\n     atomic_compare_exchange_strong_explicit(atomic<_ITp>* __a,\n \t\t\t\t\t    _ITp* __i1, _ITp __i2,\n \t\t\t\t\t    memory_order __m1,\n \t\t\t\t\t    memory_order __m2) noexcept\n     { return __a->compare_exchange_strong(*__i1, __i2, __m1, __m2); }\n \n   template<typename _ITp>\n-    __always_inline bool\n+    __libitm_always_inline bool\n     atomic_compare_exchange_strong_explicit(volatile atomic<_ITp>* __a,\n \t\t\t\t\t    _ITp* __i1, _ITp __i2,\n \t\t\t\t\t    memory_order __m1,\n@@ -1668,37 +1667,37 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \n \n   template<typename _ITp>\n-    __always_inline void\n+    __libitm_always_inline void\n     atomic_store(atomic<_ITp>* __a, _ITp __i) noexcept\n     { atomic_store_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline void\n+    __libitm_always_inline void\n     atomic_store(volatile atomic<_ITp>* __a, _ITp __i) noexcept\n     { atomic_store_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_load(const atomic<_ITp>* __a) noexcept\n     { return atomic_load_explicit(__a, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_load(const volatile atomic<_ITp>* __a) noexcept\n     { return atomic_load_explicit(__a, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_exchange(atomic<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_exchange_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_exchange(volatile atomic<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_exchange_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline bool\n+    __libitm_always_inline bool\n     atomic_compare_exchange_weak(atomic<_ITp>* __a,\n \t\t\t\t _ITp* __i1, _ITp __i2) noexcept\n     {\n@@ -1708,7 +1707,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     }\n \n   template<typename _ITp>\n-    __always_inline bool\n+    __libitm_always_inline bool\n     atomic_compare_exchange_weak(volatile atomic<_ITp>* __a,\n \t\t\t\t _ITp* __i1, _ITp __i2) noexcept\n     {\n@@ -1718,7 +1717,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     }\n \n   template<typename _ITp>\n-    __always_inline bool\n+    __libitm_always_inline bool\n     atomic_compare_exchange_strong(atomic<_ITp>* __a,\n \t\t\t\t   _ITp* __i1, _ITp __i2) noexcept\n     {\n@@ -1728,7 +1727,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     }\n \n   template<typename _ITp>\n-    __always_inline bool\n+    __libitm_always_inline bool\n     atomic_compare_exchange_strong(volatile atomic<_ITp>* __a,\n \t\t\t\t   _ITp* __i1, _ITp __i2) noexcept\n     {\n@@ -1742,158 +1741,158 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n   // intergral types as specified in the standard, excluding address\n   // types.\n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_add_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_add_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_sub_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_sub(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_sub_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_sub(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_and_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_and(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_and_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_and(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_or_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t     memory_order __m) noexcept\n     { return __a->fetch_or(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_or_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t     memory_order __m) noexcept\n     { return __a->fetch_or(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_xor_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_xor(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_xor_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_xor(__i, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_add(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_add_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_add(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_add_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_sub(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_sub_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_sub(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_sub_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_and(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_and_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_and(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_and_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_or(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_or_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_or(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_or_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_xor(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_xor_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    __always_inline _ITp\n+    __libitm_always_inline _ITp\n     atomic_fetch_xor(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_xor_explicit(__a, __i, memory_order_seq_cst); }\n \n \n   // Partial specializations for pointers.\n   template<typename _ITp>\n-    __always_inline _ITp*\n+    __libitm_always_inline _ITp*\n     atomic_fetch_add_explicit(atomic<_ITp*>* __a, ptrdiff_t __d,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__d, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp*\n+    __libitm_always_inline _ITp*\n     atomic_fetch_add_explicit(volatile atomic<_ITp*>* __a, ptrdiff_t __d,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__d, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp*\n+    __libitm_always_inline _ITp*\n     atomic_fetch_add(volatile atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_add(__d); }\n \n   template<typename _ITp>\n-    __always_inline _ITp*\n+    __libitm_always_inline _ITp*\n     atomic_fetch_add(atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_add(__d); }\n \n   template<typename _ITp>\n-    __always_inline _ITp*\n+    __libitm_always_inline _ITp*\n     atomic_fetch_sub_explicit(volatile atomic<_ITp*>* __a,\n \t\t\t      ptrdiff_t __d, memory_order __m) noexcept\n     { return __a->fetch_sub(__d, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp*\n+    __libitm_always_inline _ITp*\n     atomic_fetch_sub_explicit(atomic<_ITp*>* __a, ptrdiff_t __d,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_sub(__d, __m); }\n \n   template<typename _ITp>\n-    __always_inline _ITp*\n+    __libitm_always_inline _ITp*\n     atomic_fetch_sub(volatile atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_sub(__d); }\n \n   template<typename _ITp>\n-    __always_inline _ITp*\n+    __libitm_always_inline _ITp*\n     atomic_fetch_sub(atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_sub(__d); }\n   // @} group atomics"}]}