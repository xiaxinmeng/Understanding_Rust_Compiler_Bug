{"sha": "45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07", "node_id": "C_kwDOANBUbNoAKDQ1MzgxZDZmOWY0ZTdiNWM3YjA2MmY1YWQ4Y2M5Nzg4MDkxYzJkMDc", "commit": {"author": {"name": "Andrew Stubbs", "email": "ams@codesourcery.com", "date": "2020-08-03T20:09:36Z"}, "committer": {"name": "Andrew Stubbs", "email": "ams@codesourcery.com", "date": "2022-10-11T10:37:10Z"}, "message": "amdgcn: add multiple vector sizes\n\nThe vectors sizes are simulated using implicit masking, but they make life\neasier for the autovectorizer and SLP passes.\n\ngcc/ChangeLog:\n\n\t* config/gcn/gcn-modes.def (VECTOR_MODE): Add new modes\n\tV32QI, V32HI, V32SI, V32DI, V32TI, V32HF, V32SF, V32DF,\n\tV16QI, V16HI, V16SI, V16DI, V16TI, V16HF, V16SF, V16DF,\n\tV8QI, V8HI, V8SI, V8DI, V8TI, V8HF, V8SF, V8DF,\n\tV4QI, V4HI, V4SI, V4DI, V4TI, V4HF, V4SF, V4DF,\n\tV2QI, V2HI, V2SI, V2DI, V2TI, V2HF, V2SF, V2DF.\n\t(ADJUST_ALIGNMENT): Likewise.\n\t* config/gcn/gcn-protos.h (gcn_full_exec): Delete.\n\t(gcn_full_exec_reg): Delete.\n\t(gcn_scalar_exec): Delete.\n\t(gcn_scalar_exec_reg): Delete.\n\t(vgpr_1reg_mode_p): Use inner mode to identify vector registers.\n\t(vgpr_2reg_mode_p): Likewise.\n\t(vgpr_vector_mode_p): Use VECTOR_MODE_P.\n\t* config/gcn/gcn-valu.md (V_QI, V_HI, V_HF, V_SI, V_SF, V_DI, V_DF,\n\tV_QIHI, V_1REG, V_INT_1REG, V_INT_1REG_ALT, V_FP_1REG, V_2REG, V_noQI,\n\tV_noHI, V_INT_noQI, V_INT_noHI, V_ALL, V_ALL_ALT, V_INT, V_FP):\n\tAdd additional vector modes.\n\t(V64_SI, V64_DI, V64_ALL, V64_FP): New iterators.\n\t(scalar_mode, SCALAR_MODE, vnsi, VnSI, vndi, VnDI, sdwa):\n\tAdd additional vector mode mappings.\n\t(mov<mode>): Implement vector length conversions.\n\t(ldexp<mode>3<exec>): Use VnSI.\n\t(frexp<mode>_exp2<exec>): Likewise.\n\t(VCVT_MODE, VCVT_FMODE, VCVT_IMODE): Add additional vector modes.\n\t(reduc_<reduc_op>_scal_<mode>): Use V64_ALL.\n\t(fold_left_plus_<mode>): Use V64_FP.\n\t(*<reduc_op>_dpp_shr_<mode>): Use V64_1REG.\n\t(*<reduc_op>_dpp_shr_<mode>): Use V64_DI.\n\t(*plus_carry_dpp_shr_<mode>): Use V64_INT_1REG.\n\t(*plus_carry_in_dpp_shr_<mode>): Use V64_SI.\n\t(*plus_carry_dpp_shr_<mode>): Use V64_DI.\n\t(mov_from_lane63_<mode>): Use V64_2REG.\n\t* config/gcn/gcn.cc (VnMODE): New function.\n\t(gcn_can_change_mode_class): Support multiple vector sizes.\n\t(gcn_modes_tieable_p): Likewise.\n\t(gcn_operand_part): Likewise.\n\t(gcn_scalar_exec): Delete function.\n\t(gcn_scalar_exec_reg): Delete function.\n\t(gcn_full_exec): Delete function.\n\t(gcn_full_exec_reg): Delete function.\n\t(gcn_inline_fp_constant_p): Support multiple vector sizes.\n\t(gcn_fp_constant_p): Likewise.\n\t(A): New macro.\n\t(GEN_VN_NOEXEC): New macro.\n\t(GEN_VNM_NOEXEC): New macro.\n\t(GEN_VN): New macro.\n\t(GEN_VNM): New macro.\n\t(GET_VN_FN): New macro.\n\t(CODE_FOR): New macro.\n\t(CODE_FOR_OP): New macro.\n\t(gen_mov_with_exec): Delete function.\n\t(gen_duplicate_load): Delete function.\n\t(gcn_expand_vector_init): Support multiple vector sizes.\n\t(strided_constant): Likewise.\n\t(gcn_addr_space_legitimize_address): Likewise.\n\t(gcn_expand_scalar_to_vector_address): Likewise.\n\t(gcn_expand_scaled_offsets): Likewise.\n\t(gcn_secondary_reload): Likewise.\n\t(gcn_valid_cvt_p): Likewise.\n\t(gcn_expand_builtin_1): Likewise.\n\t(gcn_make_vec_perm_address): Likewise.\n\t(gcn_vectorize_vec_perm_const): Likewise.\n\t(gcn_vector_mode_supported_p): Likewise.\n\t(gcn_autovectorize_vector_modes): New hook.\n\t(gcn_related_vector_mode): Support multiple vector sizes.\n\t(gcn_expand_dpp_shr_insn): Add FIXME comment.\n\t(gcn_md_reorg): Support multiple vector sizes.\n\t(print_reg): Likewise.\n\t(print_operand): Likewise.\n\t(TARGET_VECTORIZE_AUTOVECTORIZE_VECTOR_MODES): New hook.", "tree": {"sha": "365283eb6e89b425f507bfc58cf2b7d6be7f3789", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/365283eb6e89b425f507bfc58cf2b7d6be7f3789"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07", "html_url": "https://github.com/Rust-GCC/gccrs/commit/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07/comments", "author": {"login": "ams-cs", "id": 2235130, "node_id": "MDQ6VXNlcjIyMzUxMzA=", "avatar_url": "https://avatars.githubusercontent.com/u/2235130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ams-cs", "html_url": "https://github.com/ams-cs", "followers_url": "https://api.github.com/users/ams-cs/followers", "following_url": "https://api.github.com/users/ams-cs/following{/other_user}", "gists_url": "https://api.github.com/users/ams-cs/gists{/gist_id}", "starred_url": "https://api.github.com/users/ams-cs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ams-cs/subscriptions", "organizations_url": "https://api.github.com/users/ams-cs/orgs", "repos_url": "https://api.github.com/users/ams-cs/repos", "events_url": "https://api.github.com/users/ams-cs/events{/privacy}", "received_events_url": "https://api.github.com/users/ams-cs/received_events", "type": "User", "site_admin": false}, "committer": {"login": "ams-cs", "id": 2235130, "node_id": "MDQ6VXNlcjIyMzUxMzA=", "avatar_url": "https://avatars.githubusercontent.com/u/2235130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ams-cs", "html_url": "https://github.com/ams-cs", "followers_url": "https://api.github.com/users/ams-cs/followers", "following_url": "https://api.github.com/users/ams-cs/following{/other_user}", "gists_url": "https://api.github.com/users/ams-cs/gists{/gist_id}", "starred_url": "https://api.github.com/users/ams-cs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ams-cs/subscriptions", "organizations_url": "https://api.github.com/users/ams-cs/orgs", "repos_url": "https://api.github.com/users/ams-cs/repos", "events_url": "https://api.github.com/users/ams-cs/events{/privacy}", "received_events_url": "https://api.github.com/users/ams-cs/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "25413fdb2ac24933214123e24ba165026452a6f2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/25413fdb2ac24933214123e24ba165026452a6f2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/25413fdb2ac24933214123e24ba165026452a6f2"}], "stats": {"total": 1363, "additions": 938, "deletions": 425}, "files": [{"sha": "1b8a3203463d85fa64d9220d6d3f9ca4c15c0958", "filename": "gcc/config/gcn/gcn-modes.def", "status": "modified", "additions": 82, "deletions": 0, "changes": 82, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07/gcc%2Fconfig%2Fgcn%2Fgcn-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07/gcc%2Fconfig%2Fgcn%2Fgcn-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fgcn%2Fgcn-modes.def?ref=45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07", "patch": "@@ -29,6 +29,48 @@ VECTOR_MODE (FLOAT, HF, 64);    /*\t\t  V64HF */\n VECTOR_MODE (FLOAT, SF, 64);    /*\t\t  V64SF */\n VECTOR_MODE (FLOAT, DF, 64);    /*\t\t  V64DF */\n \n+/* Artificial vector modes, for when vector masking doesn't work (yet).  */\n+VECTOR_MODE (INT, QI, 32);      /*\t\t  V32QI */\n+VECTOR_MODE (INT, HI, 32);      /*\t\t  V32HI */\n+VECTOR_MODE (INT, SI, 32);      /*\t\t  V32SI */\n+VECTOR_MODE (INT, DI, 32);      /*\t\t  V32DI */\n+VECTOR_MODE (INT, TI, 32);      /*\t\t  V32TI */\n+VECTOR_MODE (FLOAT, HF, 32);    /*\t\t  V32HF */\n+VECTOR_MODE (FLOAT, SF, 32);    /*\t\t  V32SF */\n+VECTOR_MODE (FLOAT, DF, 32);    /*\t\t  V32DF */\n+VECTOR_MODE (INT, QI, 16);      /*\t\t  V16QI */\n+VECTOR_MODE (INT, HI, 16);      /*\t\t  V16HI */\n+VECTOR_MODE (INT, SI, 16);      /*\t\t  V16SI */\n+VECTOR_MODE (INT, DI, 16);      /*\t\t  V16DI */\n+VECTOR_MODE (INT, TI, 16);      /*\t\t  V16TI */\n+VECTOR_MODE (FLOAT, HF, 16);    /*\t\t  V16HF */\n+VECTOR_MODE (FLOAT, SF, 16);    /*\t\t  V16SF */\n+VECTOR_MODE (FLOAT, DF, 16);    /*\t\t  V16DF */\n+VECTOR_MODE (INT, QI, 8);      /*\t\t  V8QI */\n+VECTOR_MODE (INT, HI, 8);      /*\t\t  V8HI */\n+VECTOR_MODE (INT, SI, 8);      /*\t\t  V8SI */\n+VECTOR_MODE (INT, DI, 8);      /*\t\t  V8DI */\n+VECTOR_MODE (INT, TI, 8);      /*\t\t  V8TI */\n+VECTOR_MODE (FLOAT, HF, 8);    /*\t\t  V8HF */\n+VECTOR_MODE (FLOAT, SF, 8);    /*\t\t  V8SF */\n+VECTOR_MODE (FLOAT, DF, 8);    /*\t\t  V8DF */\n+VECTOR_MODE (INT, QI, 4);      /*\t\t  V4QI */\n+VECTOR_MODE (INT, HI, 4);      /*\t\t  V4HI */\n+VECTOR_MODE (INT, SI, 4);      /*\t\t  V4SI */\n+VECTOR_MODE (INT, DI, 4);      /*\t\t  V4DI */\n+VECTOR_MODE (INT, TI, 4);      /*\t\t  V4TI */\n+VECTOR_MODE (FLOAT, HF, 4);    /*\t\t  V4HF */\n+VECTOR_MODE (FLOAT, SF, 4);    /*\t\t  V4SF */\n+VECTOR_MODE (FLOAT, DF, 4);    /*\t\t  V4DF */\n+VECTOR_MODE (INT, QI, 2);      /*\t\t  V2QI */\n+VECTOR_MODE (INT, HI, 2);      /*\t\t  V2HI */\n+VECTOR_MODE (INT, SI, 2);      /*\t\t  V2SI */\n+VECTOR_MODE (INT, DI, 2);      /*\t\t  V2DI */\n+VECTOR_MODE (INT, TI, 2);      /*\t\t  V2TI */\n+VECTOR_MODE (FLOAT, HF, 2);    /*\t\t  V2HF */\n+VECTOR_MODE (FLOAT, SF, 2);    /*\t\t  V2SF */\n+VECTOR_MODE (FLOAT, DF, 2);    /*\t\t  V2DF */\n+\n /* Vector units handle reads independently and thus no large alignment\n    needed.  */\n ADJUST_ALIGNMENT (V64QI, 1);\n@@ -39,3 +81,43 @@ ADJUST_ALIGNMENT (V64TI, 16);\n ADJUST_ALIGNMENT (V64HF, 2);\n ADJUST_ALIGNMENT (V64SF, 4);\n ADJUST_ALIGNMENT (V64DF, 8);\n+ADJUST_ALIGNMENT (V32QI, 1);\n+ADJUST_ALIGNMENT (V32HI, 2);\n+ADJUST_ALIGNMENT (V32SI, 4);\n+ADJUST_ALIGNMENT (V32DI, 8);\n+ADJUST_ALIGNMENT (V32TI, 16);\n+ADJUST_ALIGNMENT (V32HF, 2);\n+ADJUST_ALIGNMENT (V32SF, 4);\n+ADJUST_ALIGNMENT (V32DF, 8);\n+ADJUST_ALIGNMENT (V16QI, 1);\n+ADJUST_ALIGNMENT (V16HI, 2);\n+ADJUST_ALIGNMENT (V16SI, 4);\n+ADJUST_ALIGNMENT (V16DI, 8);\n+ADJUST_ALIGNMENT (V16TI, 16);\n+ADJUST_ALIGNMENT (V16HF, 2);\n+ADJUST_ALIGNMENT (V16SF, 4);\n+ADJUST_ALIGNMENT (V16DF, 8);\n+ADJUST_ALIGNMENT (V8QI, 1);\n+ADJUST_ALIGNMENT (V8HI, 2);\n+ADJUST_ALIGNMENT (V8SI, 4);\n+ADJUST_ALIGNMENT (V8DI, 8);\n+ADJUST_ALIGNMENT (V8TI, 16);\n+ADJUST_ALIGNMENT (V8HF, 2);\n+ADJUST_ALIGNMENT (V8SF, 4);\n+ADJUST_ALIGNMENT (V8DF, 8);\n+ADJUST_ALIGNMENT (V4QI, 1);\n+ADJUST_ALIGNMENT (V4HI, 2);\n+ADJUST_ALIGNMENT (V4SI, 4);\n+ADJUST_ALIGNMENT (V4DI, 8);\n+ADJUST_ALIGNMENT (V4TI, 16);\n+ADJUST_ALIGNMENT (V4HF, 2);\n+ADJUST_ALIGNMENT (V4SF, 4);\n+ADJUST_ALIGNMENT (V4DF, 8);\n+ADJUST_ALIGNMENT (V2QI, 1);\n+ADJUST_ALIGNMENT (V2HI, 2);\n+ADJUST_ALIGNMENT (V2SI, 4);\n+ADJUST_ALIGNMENT (V2DI, 8);\n+ADJUST_ALIGNMENT (V2TI, 16);\n+ADJUST_ALIGNMENT (V2HF, 2);\n+ADJUST_ALIGNMENT (V2SF, 4);\n+ADJUST_ALIGNMENT (V2DF, 8);"}, {"sha": "6300c1cbd3662d0e38d28a9f3065ab8472b4a4ad", "filename": "gcc/config/gcn/gcn-protos.h", "status": "modified", "additions": 10, "deletions": 12, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07/gcc%2Fconfig%2Fgcn%2Fgcn-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07/gcc%2Fconfig%2Fgcn%2Fgcn-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fgcn%2Fgcn-protos.h?ref=45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07", "patch": "@@ -34,8 +34,6 @@ extern rtx gcn_expand_scalar_to_vector_address (machine_mode, rtx, rtx, rtx);\n extern void gcn_expand_vector_init (rtx, rtx);\n extern bool gcn_flat_address_p (rtx, machine_mode);\n extern bool gcn_fp_constant_p (rtx, bool);\n-extern rtx gcn_full_exec ();\n-extern rtx gcn_full_exec_reg ();\n extern rtx gcn_gen_undef (machine_mode);\n extern bool gcn_global_address_p (rtx);\n extern tree gcn_goacc_adjust_private_decl (location_t, tree var, int level);\n@@ -67,8 +65,6 @@ extern rtx gcn_operand_part (machine_mode, rtx, int);\n extern bool gcn_regno_mode_code_ok_for_base_p (int, machine_mode,\n \t\t\t\t\t       addr_space_t, int, int);\n extern reg_class gcn_regno_reg_class (int regno);\n-extern rtx gcn_scalar_exec ();\n-extern rtx gcn_scalar_exec_reg ();\n extern bool gcn_scalar_flat_address_p (rtx);\n extern bool gcn_scalar_flat_mem_p (rtx);\n extern bool gcn_sgpr_move_p (rtx, rtx);\n@@ -105,9 +101,11 @@ extern gimple_opt_pass *make_pass_omp_gcn (gcc::context *ctxt);\n inline bool\n vgpr_1reg_mode_p (machine_mode mode)\n {\n-  return (mode == SImode || mode == SFmode || mode == HImode || mode == QImode\n-\t  || mode == V64QImode || mode == V64HImode || mode == V64SImode\n-\t  || mode == V64HFmode || mode == V64SFmode || mode == BImode);\n+  if (VECTOR_MODE_P (mode))\n+    mode = GET_MODE_INNER (mode);\n+\n+  return (mode == SImode || mode == SFmode || mode == HImode || mode == HFmode\n+\t  || mode == QImode || mode == BImode);\n }\n \n /* Return true if MODE is valid for 1 SGPR register.  */\n@@ -124,18 +122,18 @@ sgpr_1reg_mode_p (machine_mode mode)\n inline bool\n vgpr_2reg_mode_p (machine_mode mode)\n {\n-  return (mode == DImode || mode == DFmode\n-\t  || mode == V64DImode || mode == V64DFmode);\n+  if (VECTOR_MODE_P (mode))\n+    mode = GET_MODE_INNER (mode);\n+\n+  return (mode == DImode || mode == DFmode);\n }\n \n /* Return true if MODE can be handled directly by VGPR operations.  */\n \n inline bool\n vgpr_vector_mode_p (machine_mode mode)\n {\n-  return (mode == V64QImode || mode == V64HImode\n-\t  || mode == V64SImode || mode == V64DImode\n-\t  || mode == V64HFmode || mode == V64SFmode || mode == V64DFmode);\n+  return VECTOR_MODE_P (mode);\n }\n \n "}, {"sha": "52d2fcb880a5899a3cebf99924e24610c13814d5", "filename": "gcc/config/gcn/gcn-valu.md", "status": "modified", "additions": 268, "deletions": 64, "changes": 332, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07/gcc%2Fconfig%2Fgcn%2Fgcn-valu.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07/gcc%2Fconfig%2Fgcn%2Fgcn-valu.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fgcn%2Fgcn-valu.md?ref=45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07", "patch": "@@ -17,88 +17,243 @@\n ;; {{{ Vector iterators\n \n ; Vector modes for specific types\n-; (This will make more sense when there are multiple vector sizes)\n (define_mode_iterator V_QI\n-\t\t      [V64QI])\n+\t\t      [V2QI V4QI V8QI V16QI V32QI V64QI])\n (define_mode_iterator V_HI\n-\t\t      [V64HI])\n+\t\t      [V2HI V4HI V8HI V16HI V32HI V64HI])\n (define_mode_iterator V_HF\n-\t\t      [V64HF])\n+\t\t      [V2HF V4HF V8HF V16HF V32HF V64HF])\n (define_mode_iterator V_SI\n-\t\t      [V64SI])\n+\t\t      [V2SI V4SI V8SI V16SI V32SI V64SI])\n (define_mode_iterator V_SF\n-\t\t      [V64SF])\n+\t\t      [V2SF V4SF V8SF V16SF V32SF V64SF])\n (define_mode_iterator V_DI\n-\t\t      [V64DI])\n+\t\t      [V2DI V4DI V8DI V16DI V32DI V64DI])\n (define_mode_iterator V_DF\n-\t\t      [V64DF])\n+\t\t      [V2DF V4DF V8DF V16DF V32DF V64DF])\n+\n+(define_mode_iterator V64_SI\n+\t\t      [V64SI])\n+(define_mode_iterator V64_DI\n+\t\t      [V64DI])\n \n ; Vector modes for sub-dword modes\n (define_mode_iterator V_QIHI\n-\t\t      [V64QI V64HI])\n+\t\t      [V2QI V2HI\n+\t\t       V4QI V4HI\n+\t\t       V8QI V8HI\n+\t\t       V16QI V16HI\n+\t\t       V32QI V32HI\n+\t\t       V64QI V64HI])\n \n ; Vector modes for one vector register\n (define_mode_iterator V_1REG\n-\t\t      [V64QI V64HI V64SI V64HF V64SF])\n+\t\t      [V2QI V2HI V2SI V2HF V2SF\n+\t\t       V4QI V4HI V4SI V4HF V4SF\n+\t\t       V8QI V8HI V8SI V8HF V8SF\n+\t\t       V16QI V16HI V16SI V16HF V16SF\n+\t\t       V32QI V32HI V32SI V32HF V32SF\n+\t\t       V64QI V64HI V64SI V64HF V64SF])\n \n (define_mode_iterator V_INT_1REG\n-\t\t      [V64QI V64HI V64SI])\n+\t\t      [V2QI V2HI V2SI\n+\t\t       V4QI V4HI V4SI\n+\t\t       V8QI V8HI V8SI\n+\t\t       V16QI V16HI V16SI\n+\t\t       V32QI V32HI V32SI\n+\t\t       V64QI V64HI V64SI])\n (define_mode_iterator V_INT_1REG_ALT\n-\t\t      [V64QI V64HI V64SI])\n+\t\t      [V2QI V2HI V2SI\n+\t\t       V4QI V4HI V4SI\n+\t\t       V8QI V8HI V8SI\n+\t\t       V16QI V16HI V16SI\n+\t\t       V32QI V32HI V32SI\n+\t\t       V64QI V64HI V64SI])\n (define_mode_iterator V_FP_1REG\n-\t\t      [V64HF V64SF])\n+\t\t      [V2HF V2SF\n+\t\t       V4HF V4SF\n+\t\t       V8HF V8SF\n+\t\t       V16HF V16SF\n+\t\t       V32HF V32SF\n+\t\t       V64HF V64SF])\n+\n+; V64_* modes are for where more general support is unimplemented\n+; (e.g. reductions)\n+(define_mode_iterator V64_1REG\n+\t\t      [V64QI V64HI V64SI V64HF V64SF])\n+(define_mode_iterator V64_INT_1REG\n+\t\t      [V64QI V64HI V64SI])\n \n ; Vector modes for two vector registers\n (define_mode_iterator V_2REG\n+\t\t      [V2DI V2DF\n+\t\t       V4DI V4DF\n+\t\t       V8DI V8DF\n+\t\t       V16DI V16DF\n+\t\t       V32DI V32DF\n+\t\t       V64DI V64DF])\n+\n+(define_mode_iterator V64_2REG\n \t\t      [V64DI V64DF])\n \n ; Vector modes with native support\n (define_mode_iterator V_noQI\n-\t\t      [V64HI V64HF V64SI V64SF V64DI V64DF])\n+\t\t      [V2HI V2HF V2SI V2SF V2DI V2DF\n+\t\t       V4HI V4HF V4SI V4SF V4DI V4DF\n+\t\t       V8HI V8HF V8SI V8SF V8DI V8DF\n+\t\t       V16HI V16HF V16SI V16SF V16DI V16DF\n+\t\t       V32HI V32HF V32SI V32SF V32DI V32DF\n+\t\t       V64HI V64HF V64SI V64SF V64DI V64DF])\n (define_mode_iterator V_noHI\n-\t\t      [V64HF V64SI V64SF V64DI V64DF])\n+\t\t      [V2HF V2SI V2SF V2DI V2DF\n+\t\t       V4HF V4SI V4SF V4DI V4DF\n+\t\t       V8HF V8SI V8SF V8DI V8DF\n+\t\t       V16HF V16SI V16SF V16DI V16DF\n+\t\t       V32HF V32SI V32SF V32DI V32DF\n+\t\t       V64HF V64SI V64SF V64DI V64DF])\n \n (define_mode_iterator V_INT_noQI\n-\t\t      [V64HI V64SI V64DI])\n+\t\t      [V2HI V2SI V2DI\n+\t\t       V4HI V4SI V4DI\n+\t\t       V8HI V8SI V8DI\n+\t\t       V16HI V16SI V16DI\n+\t\t       V32HI V32SI V32DI\n+\t\t       V64HI V64SI V64DI])\n (define_mode_iterator V_INT_noHI\n-\t\t      [V64SI V64DI])\n+\t\t      [V2SI V2DI\n+\t\t       V4SI V4DI\n+\t\t       V8SI V8DI\n+\t\t       V16SI V16DI\n+\t\t       V32SI V32DI\n+\t\t       V64SI V64DI])\n \n ; All of above\n (define_mode_iterator V_ALL\n-\t\t      [V64QI V64HI V64HF V64SI V64SF V64DI V64DF])\n+\t\t      [V2QI V2HI V2HF V2SI V2SF V2DI V2DF\n+\t\t       V4QI V4HI V4HF V4SI V4SF V4DI V4DF\n+\t\t       V8QI V8HI V8HF V8SI V8SF V8DI V8DF\n+\t\t       V16QI V16HI V16HF V16SI V16SF V16DI V16DF\n+\t\t       V32QI V32HI V32HF V32SI V32SF V32DI V32DF\n+\t\t       V64QI V64HI V64HF V64SI V64SF V64DI V64DF])\n (define_mode_iterator V_ALL_ALT\n-\t\t      [V64QI V64HI V64HF V64SI V64SF V64DI V64DF])\n+\t\t      [V2QI V2HI V2HF V2SI V2SF V2DI V2DF\n+\t\t       V4QI V4HI V4HF V4SI V4SF V4DI V4DF\n+\t\t       V8QI V8HI V8HF V8SI V8SF V8DI V8DF\n+\t\t       V16QI V16HI V16HF V16SI V16SF V16DI V16DF\n+\t\t       V32QI V32HI V32HF V32SI V32SF V32DI V32DF\n+\t\t       V64QI V64HI V64HF V64SI V64SF V64DI V64DF])\n \n (define_mode_iterator V_INT\n-\t\t      [V64QI V64HI V64SI V64DI])\n+\t\t      [V2QI V2HI V2SI V2DI\n+\t\t       V4QI V4HI V4SI V4DI\n+\t\t       V8QI V8HI V8SI V8DI\n+\t\t       V16QI V16HI V16SI V16DI\n+\t\t       V32QI V32HI V32SI V32DI\n+\t\t       V64QI V64HI V64SI V64DI])\n (define_mode_iterator V_FP\n+\t\t      [V2HF V2SF V2DF\n+\t\t       V4HF V4SF V4DF\n+\t\t       V8HF V8SF V8DF\n+\t\t       V16HF V16SF V16DF\n+\t\t       V32HF V32SF V32DF\n+\t\t       V64HF V64SF V64DF])\n+\n+(define_mode_iterator V64_ALL\n+\t\t      [V64QI V64HI V64HF V64SI V64SF V64DI V64DF])\n+(define_mode_iterator V64_FP\n \t\t      [V64HF V64SF V64DF])\n \n (define_mode_attr scalar_mode\n-  [(V64QI \"qi\") (V64HI \"hi\") (V64SI \"si\")\n+  [(V2QI \"qi\") (V2HI \"hi\") (V2SI \"si\")\n+   (V2HF \"hf\") (V2SF \"sf\") (V2DI \"di\") (V2DF \"df\")\n+   (V4QI \"qi\") (V4HI \"hi\") (V4SI \"si\")\n+   (V4HF \"hf\") (V4SF \"sf\") (V4DI \"di\") (V4DF \"df\")\n+   (V8QI \"qi\") (V8HI \"hi\") (V8SI \"si\")\n+   (V8HF \"hf\") (V8SF \"sf\") (V8DI \"di\") (V8DF \"df\")\n+   (V16QI \"qi\") (V16HI \"hi\") (V16SI \"si\")\n+   (V16HF \"hf\") (V16SF \"sf\") (V16DI \"di\") (V16DF \"df\")\n+   (V32QI \"qi\") (V32HI \"hi\") (V32SI \"si\")\n+   (V32HF \"hf\") (V32SF \"sf\") (V32DI \"di\") (V32DF \"df\")\n+   (V64QI \"qi\") (V64HI \"hi\") (V64SI \"si\")\n    (V64HF \"hf\") (V64SF \"sf\") (V64DI \"di\") (V64DF \"df\")])\n \n (define_mode_attr SCALAR_MODE\n-  [(V64QI \"QI\") (V64HI \"HI\") (V64SI \"SI\")\n+  [(V2QI \"QI\") (V2HI \"HI\") (V2SI \"SI\")\n+   (V2HF \"HF\") (V2SF \"SF\") (V2DI \"DI\") (V2DF \"DF\")\n+   (V4QI \"QI\") (V4HI \"HI\") (V4SI \"SI\")\n+   (V4HF \"HF\") (V4SF \"SF\") (V4DI \"DI\") (V4DF \"DF\")\n+   (V8QI \"QI\") (V8HI \"HI\") (V8SI \"SI\")\n+   (V8HF \"HF\") (V8SF \"SF\") (V8DI \"DI\") (V8DF \"DF\")\n+   (V16QI \"QI\") (V16HI \"HI\") (V16SI \"SI\")\n+   (V16HF \"HF\") (V16SF \"SF\") (V16DI \"DI\") (V16DF \"DF\")\n+   (V32QI \"QI\") (V32HI \"HI\") (V32SI \"SI\")\n+   (V32HF \"HF\") (V32SF \"SF\") (V32DI \"DI\") (V32DF \"DF\")\n+   (V64QI \"QI\") (V64HI \"HI\") (V64SI \"SI\")\n    (V64HF \"HF\") (V64SF \"SF\") (V64DI \"DI\") (V64DF \"DF\")])\n \n (define_mode_attr vnsi\n-  [(V64QI \"v64si\") (V64HI \"v64si\") (V64HF \"v64si\") (V64SI \"v64si\")\n+  [(V2QI \"v2si\") (V2HI \"v2si\") (V2HF \"v2si\") (V2SI \"v2si\")\n+   (V2SF \"v2si\") (V2DI \"v2si\") (V2DF \"v2si\")\n+   (V4QI \"v4si\") (V4HI \"v4si\") (V4HF \"v4si\") (V4SI \"v4si\")\n+   (V4SF \"v4si\") (V4DI \"v4si\") (V4DF \"v4si\")\n+   (V8QI \"v8si\") (V8HI \"v8si\") (V8HF \"v8si\") (V8SI \"v8si\")\n+   (V8SF \"v8si\") (V8DI \"v8si\") (V8DF \"v8si\")\n+   (V16QI \"v16si\") (V16HI \"v16si\") (V16HF \"v16si\") (V16SI \"v16si\")\n+   (V16SF \"v16si\") (V16DI \"v16si\") (V16DF \"v16si\")\n+   (V32QI \"v32si\") (V32HI \"v32si\") (V32HF \"v32si\") (V32SI \"v32si\")\n+   (V32SF \"v32si\") (V32DI \"v32si\") (V32DF \"v32si\")\n+   (V64QI \"v64si\") (V64HI \"v64si\") (V64HF \"v64si\") (V64SI \"v64si\")\n    (V64SF \"v64si\") (V64DI \"v64si\") (V64DF \"v64si\")])\n \n (define_mode_attr VnSI\n-  [(V64QI \"V64SI\") (V64HI \"V64SI\") (V64HF \"V64SI\") (V64SI \"V64SI\")\n+  [(V2QI \"V2SI\") (V2HI \"V2SI\") (V2HF \"V2SI\") (V2SI \"V2SI\")\n+   (V2SF \"V2SI\") (V2DI \"V2SI\") (V2DF \"V2SI\")\n+   (V4QI \"V4SI\") (V4HI \"V4SI\") (V4HF \"V4SI\") (V4SI \"V4SI\")\n+   (V4SF \"V4SI\") (V4DI \"V4SI\") (V4DF \"V4SI\")\n+   (V8QI \"V8SI\") (V8HI \"V8SI\") (V8HF \"V8SI\") (V8SI \"V8SI\")\n+   (V8SF \"V8SI\") (V8DI \"V8SI\") (V8DF \"V8SI\")\n+   (V16QI \"V16SI\") (V16HI \"V16SI\") (V16HF \"V16SI\") (V16SI \"V16SI\")\n+   (V16SF \"V16SI\") (V16DI \"V16SI\") (V16DF \"V16SI\")\n+   (V32QI \"V32SI\") (V32HI \"V32SI\") (V32HF \"V32SI\") (V32SI \"V32SI\")\n+   (V32SF \"V32SI\") (V32DI \"V32SI\") (V32DF \"V32SI\")\n+   (V64QI \"V64SI\") (V64HI \"V64SI\") (V64HF \"V64SI\") (V64SI \"V64SI\")\n    (V64SF \"V64SI\") (V64DI \"V64SI\") (V64DF \"V64SI\")])\n \n (define_mode_attr vndi\n-  [(V64QI \"v64di\") (V64HI \"v64di\") (V64HF \"v64di\") (V64SI \"v64di\")\n+  [(V2QI \"v2di\") (V2HI \"v2di\") (V2HF \"v2di\") (V2SI \"v2di\")\n+   (V2SF \"v2di\") (V2DI \"v2di\") (V2DF \"v2di\")\n+   (V4QI \"v4di\") (V4HI \"v4di\") (V4HF \"v4di\") (V4SI \"v4di\")\n+   (V4SF \"v4di\") (V4DI \"v4di\") (V4DF \"v4di\")\n+   (V8QI \"v8di\") (V8HI \"v8di\") (V8HF \"v8di\") (V8SI \"v8di\")\n+   (V8SF \"v8di\") (V8DI \"v8di\") (V8DF \"v8di\")\n+   (V16QI \"v16di\") (V16HI \"v16di\") (V16HF \"v16di\") (V16SI \"v16di\")\n+   (V16SF \"v16di\") (V16DI \"v16di\") (V16DF \"v16di\")\n+   (V32QI \"v32di\") (V32HI \"v32di\") (V32HF \"v32di\") (V32SI \"v32di\")\n+   (V32SF \"v32di\") (V32DI \"v32di\") (V32DF \"v32di\")\n+   (V64QI \"v64di\") (V64HI \"v64di\") (V64HF \"v64di\") (V64SI \"v64di\")\n    (V64SF \"v64di\") (V64DI \"v64di\") (V64DF \"v64di\")])\n \n (define_mode_attr VnDI\n-  [(V64QI \"V64DI\") (V64HI \"V64DI\") (V64HF \"V64DI\") (V64SI \"V64DI\")\n+  [(V2QI \"V2DI\") (V2HI \"V2DI\") (V2HF \"V2DI\") (V2SI \"V2DI\")\n+   (V2SF \"V2DI\") (V2DI \"V2DI\") (V2DF \"V2DI\")\n+   (V4QI \"V4DI\") (V4HI \"V4DI\") (V4HF \"V4DI\") (V4SI \"V4DI\")\n+   (V4SF \"V4DI\") (V4DI \"V4DI\") (V4DF \"V4DI\")\n+   (V8QI \"V8DI\") (V8HI \"V8DI\") (V8HF \"V8DI\") (V8SI \"V8DI\")\n+   (V8SF \"V8DI\") (V8DI \"V8DI\") (V8DF \"V8DI\")\n+   (V16QI \"V16DI\") (V16HI \"V16DI\") (V16HF \"V16DI\") (V16SI \"V16DI\")\n+   (V16SF \"V16DI\") (V16DI \"V16DI\") (V16DF \"V16DI\")\n+   (V32QI \"V32DI\") (V32HI \"V32DI\") (V32HF \"V32DI\") (V32SI \"V32DI\")\n+   (V32SF \"V32DI\") (V32DI \"V32DI\") (V32DF \"V32DI\")\n+   (V64QI \"V64DI\") (V64HI \"V64DI\") (V64HF \"V64DI\") (V64SI \"V64DI\")\n    (V64SF \"V64DI\") (V64DI \"V64DI\") (V64DF \"V64DI\")])\n \n-(define_mode_attr sdwa [(V64QI \"BYTE_0\") (V64HI \"WORD_0\") (V64SI \"DWORD\")])\n+(define_mode_attr sdwa\n+  [(V2QI \"BYTE_0\") (V2HI \"WORD_0\") (V2SI \"DWORD\")\n+   (V4QI \"BYTE_0\") (V4HI \"WORD_0\") (V4SI \"DWORD\")\n+   (V8QI \"BYTE_0\") (V8HI \"WORD_0\") (V8SI \"DWORD\")\n+   (V16QI \"BYTE_0\") (V16HI \"WORD_0\") (V16SI \"DWORD\")\n+   (V32QI \"BYTE_0\") (V32HI \"WORD_0\") (V32SI \"DWORD\")\n+   (V64QI \"BYTE_0\") (V64HI \"WORD_0\") (V64SI \"DWORD\")])\n \n ;; }}}\n ;; {{{ Substitutions\n@@ -180,6 +335,37 @@\n \t(match_operand:V_ALL 1 \"general_operand\"))]\n   \"\"\n   {\n+    /* Bitwise reinterpret casts via SUBREG don't work with GCN vector\n+       registers, but we can convert the MEM to a mode that does work.  */\n+    if (MEM_P (operands[0]) && !SUBREG_P (operands[0])\n+\t&& SUBREG_P (operands[1])\n+\t&& GET_MODE_SIZE (GET_MODE (operands[1]))\n+\t   == GET_MODE_SIZE (GET_MODE (SUBREG_REG (operands[1]))))\n+      {\n+        rtx src = SUBREG_REG (operands[1]);\n+        rtx mem = copy_rtx (operands[0]);\n+\tPUT_MODE_RAW (mem, GET_MODE (src));\n+\temit_move_insn (mem, src);\n+\tDONE;\n+      }\n+    if (MEM_P (operands[1]) && !SUBREG_P (operands[1])\n+\t&& SUBREG_P (operands[0])\n+\t&& GET_MODE_SIZE (GET_MODE (operands[0]))\n+\t   == GET_MODE_SIZE (GET_MODE (SUBREG_REG (operands[0]))))\n+      {\n+        rtx dest = SUBREG_REG (operands[0]);\n+        rtx mem = copy_rtx (operands[1]);\n+\tPUT_MODE_RAW (mem, GET_MODE (dest));\n+\temit_move_insn (dest, mem);\n+\tDONE;\n+      }\n+\n+    /* SUBREG of MEM is not supported.  */\n+    gcc_assert ((!SUBREG_P (operands[0])\n+\t\t || !MEM_P (SUBREG_REG (operands[0])))\n+\t\t&& (!SUBREG_P (operands[1])\n+\t\t    || !MEM_P (SUBREG_REG (operands[1]))));\n+\n     if (MEM_P (operands[0]) && !lra_in_progress && !reload_completed)\n       {\n \toperands[1] = force_reg (<MODE>mode, operands[1]);\n@@ -2419,10 +2605,10 @@\n    (set_attr \"length\" \"8\")])\n \n (define_insn \"ldexp<mode>3<exec>\"\n-  [(set (match_operand:V_FP 0 \"register_operand\"  \"=v\")\n+  [(set (match_operand:V_FP 0 \"register_operand\"     \"=  v\")\n \t(unspec:V_FP\n-\t  [(match_operand:V_FP 1 \"gcn_alu_operand\" \"vB\")\n-\t   (match_operand:V64SI 2 \"gcn_alu_operand\" \"vSvA\")]\n+\t  [(match_operand:V_FP 1 \"gcn_alu_operand\"   \"  vB\")\n+\t   (match_operand:<VnSI> 2 \"gcn_alu_operand\" \"vSvA\")]\n \t  UNSPEC_LDEXP))]\n   \"\"\n   \"v_ldexp%i0\\t%0, %1, %2\"\n@@ -2452,8 +2638,8 @@\n    (set_attr \"length\" \"8\")])\n \n (define_insn \"frexp<mode>_exp2<exec>\"\n-  [(set (match_operand:V64SI 0 \"register_operand\" \"=v\")\n-\t(unspec:V64SI\n+  [(set (match_operand:<VnSI> 0 \"register_operand\" \"=v\")\n+\t(unspec:<VnSI>\n \t  [(match_operand:V_FP 1 \"gcn_alu_operand\" \"vB\")]\n \t  UNSPEC_FREXP_EXP))]\n   \"\"\n@@ -2640,9 +2826,27 @@\n (define_mode_iterator CVT_FROM_MODE [HI SI HF SF DF])\n (define_mode_iterator CVT_TO_MODE [HI SI HF SF DF])\n \n-(define_mode_iterator VCVT_MODE [V64HI V64SI V64HF V64SF V64DF])\n-(define_mode_iterator VCVT_FMODE [V64HF V64SF V64DF])\n-(define_mode_iterator VCVT_IMODE [V64HI V64SI])\n+(define_mode_iterator VCVT_MODE\n+\t\t      [V2HI V2SI V2HF V2SF V2DF\n+\t\t       V4HI V4SI V4HF V4SF V4DF\n+\t\t       V8HI V8SI V8HF V8SF V8DF\n+\t\t       V16HI V16SI V16HF V16SF V16DF\n+\t\t       V32HI V32SI V32HF V32SF V32DF\n+\t\t       V64HI V64SI V64HF V64SF V64DF])\n+(define_mode_iterator VCVT_FMODE\n+\t\t      [V2HF V2SF V2DF\n+\t\t       V4HF V4SF V4DF\n+\t\t       V8HF V8SF V8DF\n+\t\t       V16HF V16SF V16DF\n+\t\t       V32HF V32SF V32DF\n+\t\t       V64HF V64SF V64DF])\n+(define_mode_iterator VCVT_IMODE\n+\t\t      [V2HI V2SI\n+\t\t       V4HI V4SI\n+\t\t       V8HI V8SI\n+\t\t       V16HI V16SI\n+\t\t       V32HI V32SI\n+\t\t       V64HI V64SI])\n \n (define_code_iterator cvt_op [fix unsigned_fix\n \t\t\t      float unsigned_float\n@@ -3265,7 +3469,7 @@\n (define_expand \"reduc_<reduc_op>_scal_<mode>\"\n   [(set (match_operand:<SCALAR_MODE> 0 \"register_operand\")\n \t(unspec:<SCALAR_MODE>\n-\t  [(match_operand:V_ALL 1 \"register_operand\")]\n+\t  [(match_operand:V64_ALL 1 \"register_operand\")]\n \t  REDUC_UNSPEC))]\n   \"\"\n   {\n@@ -3284,7 +3488,7 @@\n (define_expand \"fold_left_plus_<mode>\"\n  [(match_operand:<SCALAR_MODE> 0 \"register_operand\")\n   (match_operand:<SCALAR_MODE> 1 \"gcn_alu_operand\")\n-  (match_operand:V_FP 2 \"gcn_alu_operand\")]\n+  (match_operand:V64_FP 2 \"gcn_alu_operand\")]\n   \"can_create_pseudo_p ()\n    && (flag_openacc || flag_openmp\n        || flag_associative_math)\"\n@@ -3300,11 +3504,11 @@\n    })\n \n (define_insn \"*<reduc_op>_dpp_shr_<mode>\"\n-  [(set (match_operand:V_1REG 0 \"register_operand\"   \"=v\")\n-\t(unspec:V_1REG\n-\t  [(match_operand:V_1REG 1 \"register_operand\" \"v\")\n-\t   (match_operand:V_1REG 2 \"register_operand\" \"v\")\n-\t   (match_operand:SI 3 \"const_int_operand\"    \"n\")]\n+  [(set (match_operand:V64_1REG 0 \"register_operand\"   \"=v\")\n+\t(unspec:V64_1REG\n+\t  [(match_operand:V64_1REG 1 \"register_operand\" \"v\")\n+\t   (match_operand:V64_1REG 2 \"register_operand\" \"v\")\n+\t   (match_operand:SI 3 \"const_int_operand\"      \"n\")]\n \t  REDUC_UNSPEC))]\n   ; GCN3 requires a carry out, GCN5 not\n   \"!(TARGET_GCN3 && SCALAR_INT_MODE_P (<SCALAR_MODE>mode)\n@@ -3317,11 +3521,11 @@\n    (set_attr \"length\" \"8\")])\n \n (define_insn_and_split \"*<reduc_op>_dpp_shr_<mode>\"\n-  [(set (match_operand:V_DI 0 \"register_operand\"    \"=v\")\n-\t(unspec:V_DI\n-\t  [(match_operand:V_DI 1 \"register_operand\" \"v\")\n-\t   (match_operand:V_DI 2 \"register_operand\" \"v\")\n-\t   (match_operand:SI 3 \"const_int_operand\"  \"n\")]\n+  [(set (match_operand:V64_DI 0 \"register_operand\"    \"=v\")\n+\t(unspec:V64_DI\n+\t  [(match_operand:V64_DI 1 \"register_operand\" \"v\")\n+\t   (match_operand:V64_DI 2 \"register_operand\" \"v\")\n+\t   (match_operand:SI 3 \"const_int_operand\"    \"n\")]\n \t  REDUC_2REG_UNSPEC))]\n   \"\"\n   \"#\"\n@@ -3346,10 +3550,10 @@\n ; Special cases for addition.\n \n (define_insn \"*plus_carry_dpp_shr_<mode>\"\n-  [(set (match_operand:V_INT_1REG 0 \"register_operand\"   \"=v\")\n-\t(unspec:V_INT_1REG\n-\t  [(match_operand:V_INT_1REG 1 \"register_operand\" \"v\")\n-\t   (match_operand:V_INT_1REG 2 \"register_operand\" \"v\")\n+  [(set (match_operand:V64_INT_1REG 0 \"register_operand\"   \"=v\")\n+\t(unspec:V64_INT_1REG\n+\t  [(match_operand:V64_INT_1REG 1 \"register_operand\" \"v\")\n+\t   (match_operand:V64_INT_1REG 2 \"register_operand\" \"v\")\n \t   (match_operand:SI 3 \"const_int_operand\"\t  \"n\")]\n \t  UNSPEC_PLUS_CARRY_DPP_SHR))\n    (clobber (reg:DI VCC_REG))]\n@@ -3363,12 +3567,12 @@\n    (set_attr \"length\" \"8\")])\n \n (define_insn \"*plus_carry_in_dpp_shr_<mode>\"\n-  [(set (match_operand:V_SI 0 \"register_operand\"    \"=v\")\n-\t(unspec:V_SI\n-\t  [(match_operand:V_SI 1 \"register_operand\" \"v\")\n-\t   (match_operand:V_SI 2 \"register_operand\" \"v\")\n-\t   (match_operand:SI 3 \"const_int_operand\"  \"n\")\n-\t   (match_operand:DI 4 \"register_operand\"   \"cV\")]\n+  [(set (match_operand:V64_SI 0 \"register_operand\"    \"=v\")\n+\t(unspec:V64_SI\n+\t  [(match_operand:V64_SI 1 \"register_operand\" \"v\")\n+\t   (match_operand:V64_SI 2 \"register_operand\" \"v\")\n+\t   (match_operand:SI 3 \"const_int_operand\"    \"n\")\n+\t   (match_operand:DI 4 \"register_operand\"     \"cV\")]\n \t  UNSPEC_PLUS_CARRY_IN_DPP_SHR))\n    (clobber (reg:DI VCC_REG))]\n   \"\"\n@@ -3381,11 +3585,11 @@\n    (set_attr \"length\" \"8\")])\n \n (define_insn_and_split \"*plus_carry_dpp_shr_<mode>\"\n-  [(set (match_operand:V_DI 0 \"register_operand\"    \"=v\")\n-\t(unspec:V_DI\n-\t  [(match_operand:V_DI 1 \"register_operand\" \"v\")\n-\t   (match_operand:V_DI 2 \"register_operand\" \"v\")\n-\t   (match_operand:SI 3 \"const_int_operand\"  \"n\")]\n+  [(set (match_operand:V64_DI 0 \"register_operand\"    \"=v\")\n+\t(unspec:V64_DI\n+\t  [(match_operand:V64_DI 1 \"register_operand\" \"v\")\n+\t   (match_operand:V64_DI 2 \"register_operand\" \"v\")\n+\t   (match_operand:SI 3 \"const_int_operand\"    \"n\")]\n \t  UNSPEC_PLUS_CARRY_DPP_SHR))\n    (clobber (reg:DI VCC_REG))]\n   \"\"\n@@ -3416,7 +3620,7 @@\n (define_insn \"mov_from_lane63_<mode>\"\n   [(set (match_operand:<SCALAR_MODE> 0 \"register_operand\" \"=Sg,v\")\n \t(unspec:<SCALAR_MODE>\n-\t  [(match_operand:V_1REG 1 \"register_operand\"\t  \"  v,v\")]\n+\t  [(match_operand:V64_1REG 1 \"register_operand\"\t  \"  v,v\")]\n \t  UNSPEC_MOV_FROM_LANE63))]\n   \"\"\n   \"@\n@@ -3429,7 +3633,7 @@\n (define_insn \"mov_from_lane63_<mode>\"\n   [(set (match_operand:<SCALAR_MODE> 0 \"register_operand\" \"=Sg,v\")\n \t(unspec:<SCALAR_MODE>\n-\t  [(match_operand:V_2REG 1 \"register_operand\"\t  \"  v,v\")]\n+\t  [(match_operand:V64_2REG 1 \"register_operand\"\t  \"  v,v\")]\n \t  UNSPEC_MOV_FROM_LANE63))]\n   \"\"\n   \"@"}, {"sha": "e1636f6ddd6f98b310a1dc7a38b3cd983d1d34d7", "filename": "gcc/config/gcn/gcn.cc", "status": "modified", "additions": 578, "deletions": 349, "changes": 927, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07/gcc%2Fconfig%2Fgcn%2Fgcn.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07/gcc%2Fconfig%2Fgcn%2Fgcn.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fgcn%2Fgcn.cc?ref=45381d6f9f4e7b5c7b062f5ad8cc9788091c2d07", "patch": "@@ -395,6 +395,97 @@ gcn_scalar_mode_supported_p (scalar_mode mode)\n \t  || mode == TImode);\n }\n \n+/* Return a vector mode with N lanes of MODE.  */\n+\n+static machine_mode\n+VnMODE (int n, machine_mode mode)\n+{\n+  switch (mode)\n+    {\n+    case QImode:\n+      switch (n)\n+\t{\n+\tcase 2: return V2QImode;\n+\tcase 4: return V4QImode;\n+\tcase 8: return V8QImode;\n+\tcase 16: return V16QImode;\n+\tcase 32: return V32QImode;\n+\tcase 64: return V64QImode;\n+\t}\n+      break;\n+    case HImode:\n+      switch (n)\n+\t{\n+\tcase 2: return V2HImode;\n+\tcase 4: return V4HImode;\n+\tcase 8: return V8HImode;\n+\tcase 16: return V16HImode;\n+\tcase 32: return V32HImode;\n+\tcase 64: return V64HImode;\n+\t}\n+      break;\n+    case HFmode:\n+      switch (n)\n+\t{\n+\tcase 2: return V2HFmode;\n+\tcase 4: return V4HFmode;\n+\tcase 8: return V8HFmode;\n+\tcase 16: return V16HFmode;\n+\tcase 32: return V32HFmode;\n+\tcase 64: return V64HFmode;\n+\t}\n+      break;\n+    case SImode:\n+      switch (n)\n+\t{\n+\tcase 2: return V2SImode;\n+\tcase 4: return V4SImode;\n+\tcase 8: return V8SImode;\n+\tcase 16: return V16SImode;\n+\tcase 32: return V32SImode;\n+\tcase 64: return V64SImode;\n+\t}\n+      break;\n+    case SFmode:\n+      switch (n)\n+\t{\n+\tcase 2: return V2SFmode;\n+\tcase 4: return V4SFmode;\n+\tcase 8: return V8SFmode;\n+\tcase 16: return V16SFmode;\n+\tcase 32: return V32SFmode;\n+\tcase 64: return V64SFmode;\n+\t}\n+      break;\n+    case DImode:\n+      switch (n)\n+\t{\n+\tcase 2: return V2DImode;\n+\tcase 4: return V4DImode;\n+\tcase 8: return V8DImode;\n+\tcase 16: return V16DImode;\n+\tcase 32: return V32DImode;\n+\tcase 64: return V64DImode;\n+\t}\n+      break;\n+    case DFmode:\n+      switch (n)\n+\t{\n+\tcase 2: return V2DFmode;\n+\tcase 4: return V4DFmode;\n+\tcase 8: return V8DFmode;\n+\tcase 16: return V16DFmode;\n+\tcase 32: return V32DFmode;\n+\tcase 64: return V64DFmode;\n+\t}\n+      break;\n+    default:\n+      break;\n+    }\n+\n+  return VOIDmode;\n+}\n+\n /* Implement TARGET_CLASS_MAX_NREGS.\n  \n    Return the number of hard registers needed to hold a value of MODE in\n@@ -556,6 +647,23 @@ gcn_can_change_mode_class (machine_mode from, machine_mode to,\n {\n   if (!vgpr_vector_mode_p (from) && !vgpr_vector_mode_p (to))\n     return true;\n+\n+  /* Vector conversions are only valid when changing mode with a fixed number\n+     of lanes, or changing number of lanes with a fixed mode.  Anything else\n+     would require actual data movement.  */\n+  if (VECTOR_MODE_P (from) && VECTOR_MODE_P (to)\n+      && GET_MODE_NUNITS (from) != GET_MODE_NUNITS (to)\n+      && GET_MODE_INNER (from) != GET_MODE_INNER (to))\n+    return false;\n+\n+  /* Vector/scalar conversions are only permitted when the scalar mode\n+     is the same or smaller than the inner vector mode.  */\n+  if ((VECTOR_MODE_P (from) && !VECTOR_MODE_P (to)\n+       && GET_MODE_SIZE (to) >= GET_MODE_SIZE (GET_MODE_INNER (from)))\n+      || (VECTOR_MODE_P (to) && !VECTOR_MODE_P (from)\n+\t  && GET_MODE_SIZE (from) >= GET_MODE_SIZE (GET_MODE_INNER (to))))\n+    return false;\n+\n   return (gcn_class_max_nregs (regclass, from)\n \t  == gcn_class_max_nregs (regclass, to));\n }\n@@ -595,6 +703,16 @@ gcn_class_likely_spilled_p (reg_class_t rclass)\n bool\n gcn_modes_tieable_p (machine_mode mode1, machine_mode mode2)\n {\n+  if (VECTOR_MODE_P (mode1) || VECTOR_MODE_P (mode2))\n+    {\n+      int vf1 = (VECTOR_MODE_P (mode1) ? GET_MODE_NUNITS (mode1) : 1);\n+      int vf2 = (VECTOR_MODE_P (mode2) ? GET_MODE_NUNITS (mode2) : 1);\n+      machine_mode inner1 = (vf1 > 1 ? GET_MODE_INNER (mode1) : mode1);\n+      machine_mode inner2 = (vf2 > 1 ? GET_MODE_INNER (mode2) : mode2);\n+\n+      return (vf1 == vf2 || (inner1 == inner2 && vf2 <= vf1));\n+    }\n+\n   return (GET_MODE_BITSIZE (mode1) <= MAX_FIXED_MODE_SIZE\n \t  && GET_MODE_BITSIZE (mode2) <= MAX_FIXED_MODE_SIZE);\n }\n@@ -616,14 +734,16 @@ gcn_truly_noop_truncation (poly_uint64 outprec, poly_uint64 inprec)\n rtx\n gcn_operand_part (machine_mode mode, rtx op, int n)\n {\n-  if (GET_MODE_SIZE (mode) >= 256)\n+  int vf = VECTOR_MODE_P (mode) ? GET_MODE_NUNITS (mode) : 1;\n+\n+  if (vf > 1)\n     {\n-      /*gcc_assert (GET_MODE_SIZE (mode) == 256 || n == 0);  */\n+      machine_mode vsimode = VnMODE (vf, SImode);\n \n       if (REG_P (op))\n \t{\n \t  gcc_assert (REGNO (op) + n < FIRST_PSEUDO_REGISTER);\n-\t  return gen_rtx_REG (V64SImode, REGNO (op) + n);\n+\t  return gen_rtx_REG (vsimode, REGNO (op) + n);\n \t}\n       if (GET_CODE (op) == CONST_VECTOR)\n \t{\n@@ -634,10 +754,10 @@ gcn_operand_part (machine_mode mode, rtx op, int n)\n \t    RTVEC_ELT (v, i) = gcn_operand_part (GET_MODE_INNER (mode),\n \t\t\t\t\t\t CONST_VECTOR_ELT (op, i), n);\n \n-\t  return gen_rtx_CONST_VECTOR (V64SImode, v);\n+\t  return gen_rtx_CONST_VECTOR (vsimode, v);\n \t}\n       if (GET_CODE (op) == UNSPEC && XINT (op, 1) == UNSPEC_VECTOR)\n-\treturn gcn_gen_undef (V64SImode);\n+\treturn gcn_gen_undef (vsimode);\n       gcc_unreachable ();\n     }\n   else if (GET_MODE_SIZE (mode) == 8 && REG_P (op))\n@@ -734,38 +854,6 @@ get_exec (int64_t val)\n   return reg;\n }\n \n-/* Return value of scalar exec register.  */\n-\n-rtx\n-gcn_scalar_exec ()\n-{\n-  return const1_rtx;\n-}\n-\n-/* Return pseudo holding scalar exec register.  */\n-\n-rtx\n-gcn_scalar_exec_reg ()\n-{\n-  return get_exec (1);\n-}\n-\n-/* Return value of full exec register.  */\n-\n-rtx\n-gcn_full_exec ()\n-{\n-  return constm1_rtx;\n-}\n-\n-/* Return pseudo holding full exec register.  */\n-\n-rtx\n-gcn_full_exec_reg ()\n-{\n-  return get_exec (-1);\n-}\n-\n /* }}}  */\n /* {{{ Immediate constants.  */\n \n@@ -802,8 +890,13 @@ int\n gcn_inline_fp_constant_p (rtx x, bool allow_vector)\n {\n   machine_mode mode = GET_MODE (x);\n+  int vf = VECTOR_MODE_P (mode) ? GET_MODE_NUNITS (mode) : 1;\n \n-  if ((mode == V64HFmode || mode == V64SFmode || mode == V64DFmode)\n+  if (vf > 1)\n+    mode = GET_MODE_INNER (mode);\n+\n+  if (vf > 1\n+      && (mode == HFmode || mode == SFmode || mode == DFmode)\n       && allow_vector)\n     {\n       int n;\n@@ -812,7 +905,7 @@ gcn_inline_fp_constant_p (rtx x, bool allow_vector)\n       n = gcn_inline_fp_constant_p (CONST_VECTOR_ELT (x, 0), false);\n       if (!n)\n \treturn 0;\n-      for (int i = 1; i < 64; i++)\n+      for (int i = 1; i < vf; i++)\n \tif (CONST_VECTOR_ELT (x, i) != CONST_VECTOR_ELT (x, 0))\n \t  return 0;\n       return 1;\n@@ -867,8 +960,13 @@ bool\n gcn_fp_constant_p (rtx x, bool allow_vector)\n {\n   machine_mode mode = GET_MODE (x);\n+  int vf = VECTOR_MODE_P (mode) ? GET_MODE_NUNITS (mode) : 1;\n \n-  if ((mode == V64HFmode || mode == V64SFmode || mode == V64DFmode)\n+  if (vf > 1)\n+    mode = GET_MODE_INNER (mode);\n+\n+  if (vf > 1\n+      && (mode == HFmode || mode == SFmode || mode == DFmode)\n       && allow_vector)\n     {\n       int n;\n@@ -877,7 +975,7 @@ gcn_fp_constant_p (rtx x, bool allow_vector)\n       n = gcn_fp_constant_p (CONST_VECTOR_ELT (x, 0), false);\n       if (!n)\n \treturn false;\n-      for (int i = 1; i < 64; i++)\n+      for (int i = 1; i < vf; i++)\n \tif (CONST_VECTOR_ELT (x, i) != CONST_VECTOR_ELT (x, 0))\n \t  return false;\n       return true;\n@@ -1090,6 +1188,244 @@ gcn_gen_undef (machine_mode mode)\n   return gen_rtx_UNSPEC (mode, gen_rtvec (1, const0_rtx), UNSPEC_VECTOR);\n }\n \n+/* }}}  */\n+/* {{{ Utility functions.  */\n+\n+/*  Generalised accessor functions for instruction patterns.\n+    The machine desription '@' prefix does something similar, but as of\n+    GCC 10 is incompatible with define_subst, and anyway it doesn't\n+    auto-handle the exec feature.\n+\n+    Four macros are provided; each function only needs one:\n+\n+    GEN_VN         - create accessor functions for all sizes of one mode\n+    GEN_VNM        - create accessor functions for all sizes of all modes\n+    GEN_VN_NOEXEC  - for insns without \"_exec\" variants\n+    GEN_VNM_NOEXEC - likewise\n+ \n+    E.g.  add<mode>3\n+      GEN_VNM (add, 3, A(rtx dest, rtx s1, rtx s2), A(dest, s1, s2)\n+\n+      gen_addvNsi3 (dst, a, b)\n+        -> calls gen_addv64si3, or gen_addv32si3, etc.\n+\n+      gen_addvNm3 (dst, a, b)\n+        -> calls gen_addv64qi3, or gen_addv2di3, etc.\n+\n+    The mode is determined from the first parameter, which must be called\n+    \"dest\" (or else the macro doesn't work).\n+\n+    Each function has two optional parameters at the end: merge_src and exec.\n+    If exec is non-null, the function will call the \"_exec\" variant of the\n+    insn.  If exec is non-null but merge_src is null then an undef unspec\n+    will be created.\n+\n+    E.g. cont.\n+      gen_addvNsi3 (v64sidst, a, b, oldval, exec)\n+        -> calls gen_addv64si3_exec (v64sidst, a, b, oldval, exec)\n+\n+      gen_addvNm3 (v2qidst, a, b, NULL, exec)\n+        -> calls gen_addv2qi3_exec (v2qidst, a, b,\n+                                    gcn_gen_undef (V2QImode), exec)\n+   */\n+\n+#define A(...) __VA_ARGS__\n+#define GEN_VN_NOEXEC(PREFIX, SUFFIX, PARAMS, ARGS) \\\n+static rtx \\\n+gen_##PREFIX##vN##SUFFIX (PARAMS) \\\n+{ \\\n+  machine_mode mode = GET_MODE (dest); \\\n+  int n = GET_MODE_NUNITS (mode); \\\n+  \\\n+  switch (n) \\\n+    { \\\n+    case 2: return gen_##PREFIX##v2##SUFFIX (ARGS); \\\n+    case 4: return gen_##PREFIX##v4##SUFFIX (ARGS); \\\n+    case 8: return gen_##PREFIX##v8##SUFFIX (ARGS); \\\n+    case 16: return gen_##PREFIX##v16##SUFFIX (ARGS); \\\n+    case 32: return gen_##PREFIX##v32##SUFFIX (ARGS); \\\n+    case 64: return gen_##PREFIX##v64##SUFFIX (ARGS); \\\n+    } \\\n+  \\\n+  gcc_unreachable (); \\\n+  return NULL_RTX; \\\n+}\n+\n+#define GEN_VNM_NOEXEC(PREFIX, SUFFIX, PARAMS, ARGS) \\\n+GEN_VN_NOEXEC (PREFIX, qi##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN_NOEXEC (PREFIX, hi##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN_NOEXEC (PREFIX, hf##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN_NOEXEC (PREFIX, si##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN_NOEXEC (PREFIX, sf##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN_NOEXEC (PREFIX, di##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN_NOEXEC (PREFIX, df##SUFFIX, A(PARAMS), A(ARGS)) \\\n+static rtx \\\n+gen_##PREFIX##vNm##SUFFIX (PARAMS) \\\n+{ \\\n+  machine_mode mode = GET_MODE_INNER (GET_MODE (dest)); \\\n+  \\\n+  switch (mode) \\\n+    { \\\n+    case E_QImode: return gen_##PREFIX##vNqi##SUFFIX (ARGS); \\\n+    case E_HImode: return gen_##PREFIX##vNhi##SUFFIX (ARGS); \\\n+    case E_HFmode: return gen_##PREFIX##vNhf##SUFFIX (ARGS); \\\n+    case E_SImode: return gen_##PREFIX##vNsi##SUFFIX (ARGS); \\\n+    case E_SFmode: return gen_##PREFIX##vNsf##SUFFIX (ARGS); \\\n+    case E_DImode: return gen_##PREFIX##vNdi##SUFFIX (ARGS); \\\n+    case E_DFmode: return gen_##PREFIX##vNdf##SUFFIX (ARGS); \\\n+    default: \\\n+      break; \\\n+    } \\\n+  \\\n+  gcc_unreachable (); \\\n+  return NULL_RTX; \\\n+}\n+\n+#define GEN_VN(PREFIX, SUFFIX, PARAMS, ARGS) \\\n+static rtx \\\n+gen_##PREFIX##vN##SUFFIX (PARAMS, rtx merge_src=NULL, rtx exec=NULL) \\\n+{ \\\n+  machine_mode mode = GET_MODE (dest); \\\n+  int n = GET_MODE_NUNITS (mode); \\\n+  \\\n+  if (exec && !merge_src) \\\n+\tmerge_src = gcn_gen_undef (mode); \\\n+      \\\n+  if (exec) \\\n+    switch (n) \\\n+      { \\\n+      case 2: return gen_##PREFIX##v2##SUFFIX##_exec (ARGS, merge_src, exec); \\\n+      case 4: return gen_##PREFIX##v4##SUFFIX##_exec (ARGS, merge_src, exec); \\\n+      case 8: return gen_##PREFIX##v8##SUFFIX##_exec (ARGS, merge_src, exec); \\\n+      case 16: return gen_##PREFIX##v16##SUFFIX##_exec (ARGS, merge_src, exec); \\\n+      case 32: return gen_##PREFIX##v32##SUFFIX##_exec (ARGS, merge_src, exec); \\\n+      case 64: return gen_##PREFIX##v64##SUFFIX##_exec (ARGS, merge_src, exec); \\\n+      } \\\n+  else \\\n+    switch (n) \\\n+      { \\\n+      case 2: return gen_##PREFIX##v2##SUFFIX (ARGS); \\\n+      case 4: return gen_##PREFIX##v4##SUFFIX (ARGS); \\\n+      case 8: return gen_##PREFIX##v8##SUFFIX (ARGS); \\\n+      case 16: return gen_##PREFIX##v16##SUFFIX (ARGS); \\\n+      case 32: return gen_##PREFIX##v32##SUFFIX (ARGS); \\\n+      case 64: return gen_##PREFIX##v64##SUFFIX (ARGS); \\\n+      } \\\n+  \\\n+  gcc_unreachable (); \\\n+  return NULL_RTX; \\\n+}\n+\n+#define GEN_VNM(PREFIX, SUFFIX, PARAMS, ARGS) \\\n+GEN_VN (PREFIX, qi##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN (PREFIX, hi##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN (PREFIX, hf##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN (PREFIX, si##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN (PREFIX, sf##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN (PREFIX, di##SUFFIX, A(PARAMS), A(ARGS)) \\\n+GEN_VN (PREFIX, df##SUFFIX, A(PARAMS), A(ARGS)) \\\n+static rtx \\\n+gen_##PREFIX##vNm##SUFFIX (PARAMS, rtx merge_src=NULL, rtx exec=NULL) \\\n+{ \\\n+  machine_mode mode = GET_MODE_INNER (GET_MODE (dest)); \\\n+  \\\n+  switch (mode) \\\n+    { \\\n+    case E_QImode: return gen_##PREFIX##vNqi##SUFFIX (ARGS, merge_src, exec); \\\n+    case E_HImode: return gen_##PREFIX##vNhi##SUFFIX (ARGS, merge_src, exec); \\\n+    case E_HFmode: return gen_##PREFIX##vNhf##SUFFIX (ARGS, merge_src, exec); \\\n+    case E_SImode: return gen_##PREFIX##vNsi##SUFFIX (ARGS, merge_src, exec); \\\n+    case E_SFmode: return gen_##PREFIX##vNsf##SUFFIX (ARGS, merge_src, exec); \\\n+    case E_DImode: return gen_##PREFIX##vNdi##SUFFIX (ARGS, merge_src, exec); \\\n+    case E_DFmode: return gen_##PREFIX##vNdf##SUFFIX (ARGS, merge_src, exec); \\\n+    default: \\\n+      break; \\\n+    } \\\n+  \\\n+  gcc_unreachable (); \\\n+  return NULL_RTX; \\\n+}\n+\n+GEN_VNM (add,3, A(rtx dest, rtx src1, rtx src2), A(dest, src1, src2))\n+GEN_VN (add,si3_dup, A(rtx dest, rtx src1, rtx src2), A(dest, src1, src2))\n+GEN_VN (add,si3_vcc_dup, A(rtx dest, rtx src1, rtx src2, rtx vcc),\n+\tA(dest, src1, src2, vcc))\n+GEN_VN (add,di3_sext_dup2, A(rtx dest, rtx src1, rtx src2), A(dest, src1, src2))\n+GEN_VN (add,di3_vcc_zext_dup, A(rtx dest, rtx src1, rtx src2, rtx vcc),\n+\tA(dest, src1, src2, vcc))\n+GEN_VN (add,di3_zext_dup2, A(rtx dest, rtx src1, rtx src2), A(dest, src1, src2))\n+GEN_VN (add,di3_vcc_zext_dup2, A(rtx dest, rtx src1, rtx src2, rtx vcc),\n+\tA(dest, src1, src2, vcc))\n+GEN_VN (addc,si3, A(rtx dest, rtx src1, rtx src2, rtx vccout, rtx vccin),\n+\tA(dest, src1, src2, vccout, vccin))\n+GEN_VN (ashl,si3, A(rtx dest, rtx src, rtx shift), A(dest, src, shift))\n+GEN_VNM_NOEXEC (ds_bpermute,, A(rtx dest, rtx addr, rtx src, rtx exec),\n+\t\tA(dest, addr, src, exec))\n+GEN_VNM (mov,, A(rtx dest, rtx src), A(dest, src))\n+GEN_VN (mul,si3_dup, A(rtx dest, rtx src1, rtx src2), A(dest, src1, src2))\n+GEN_VNM (vec_duplicate,, A(rtx dest, rtx src), A(dest, src))\n+\n+#undef GEN_VNM\n+#undef GEN_VN\n+#undef GET_VN_FN\n+#undef A\n+\n+/* Get icode for vector instructions without an optab.  */\n+\n+#define CODE_FOR(PREFIX, SUFFIX) \\\n+static int \\\n+get_code_for_##PREFIX##vN##SUFFIX (int nunits) \\\n+{ \\\n+  switch (nunits) \\\n+    { \\\n+    case 2: return CODE_FOR_##PREFIX##v2##SUFFIX; \\\n+    case 4: return CODE_FOR_##PREFIX##v4##SUFFIX; \\\n+    case 8: return CODE_FOR_##PREFIX##v8##SUFFIX; \\\n+    case 16: return CODE_FOR_##PREFIX##v16##SUFFIX; \\\n+    case 32: return CODE_FOR_##PREFIX##v32##SUFFIX; \\\n+    case 64: return CODE_FOR_##PREFIX##v64##SUFFIX; \\\n+    } \\\n+  \\\n+  gcc_unreachable (); \\\n+  return CODE_FOR_nothing; \\\n+}\n+\n+#define CODE_FOR_OP(PREFIX) \\\n+\t  CODE_FOR (PREFIX, qi) \\\n+\tCODE_FOR (PREFIX, hi) \\\n+\tCODE_FOR (PREFIX, hf) \\\n+\tCODE_FOR (PREFIX, si) \\\n+\tCODE_FOR (PREFIX, sf) \\\n+\tCODE_FOR (PREFIX, di) \\\n+\tCODE_FOR (PREFIX, df) \\\n+static int \\\n+get_code_for_##PREFIX (machine_mode mode) \\\n+{ \\\n+  int vf = GET_MODE_NUNITS (mode); \\\n+  machine_mode smode = GET_MODE_INNER (mode); \\\n+  \\\n+  switch (smode) \\\n+    { \\\n+    case E_QImode: return get_code_for_##PREFIX##vNqi (vf); \\\n+    case E_HImode: return get_code_for_##PREFIX##vNhi (vf); \\\n+    case E_HFmode: return get_code_for_##PREFIX##vNhf (vf); \\\n+    case E_SImode: return get_code_for_##PREFIX##vNsi (vf); \\\n+    case E_SFmode: return get_code_for_##PREFIX##vNsf (vf); \\\n+    case E_DImode: return get_code_for_##PREFIX##vNdi (vf); \\\n+    case E_DFmode: return get_code_for_##PREFIX##vNdf (vf); \\\n+    default: break; \\\n+    } \\\n+  \\\n+  gcc_unreachable (); \\\n+  return CODE_FOR_nothing; \\\n+}\n+\n+CODE_FOR_OP (reload_in)\n+CODE_FOR_OP (reload_out)\n+\n+#undef CODE_FOR_OP\n+#undef CODE_FOR\n+\n /* }}}  */\n /* {{{ Addresses, pointers and moves.  */\n \n@@ -1644,60 +1980,6 @@ regno_ok_for_index_p (int regno)\n   return regno == M0_REG || VGPR_REGNO_P (regno);\n }\n \n-/* Generate move which uses the exec flags.  If EXEC is NULL, then it is\n-   assumed that all lanes normally relevant to the mode of the move are\n-   affected.  If PREV is NULL, then a sensible default is supplied for\n-   the inactive lanes.  */\n-\n-static rtx\n-gen_mov_with_exec (rtx op0, rtx op1, rtx exec = NULL, rtx prev = NULL)\n-{\n-  machine_mode mode = GET_MODE (op0);\n-\n-  if (vgpr_vector_mode_p (mode))\n-    {\n-      if (exec && exec != CONSTM1_RTX (DImode))\n-\t{\n-\t  if (!prev)\n-\t    prev = op0;\n-\t}\n-      else\n-\t{\n-\t  if (!prev)\n-\t    prev = gcn_gen_undef (mode);\n-\t  exec = gcn_full_exec_reg ();\n-\t}\n-\n-      rtx set = gen_rtx_SET (op0, gen_rtx_VEC_MERGE (mode, op1, prev, exec));\n-\n-      return gen_rtx_PARALLEL (VOIDmode,\n-\t       gen_rtvec (2, set,\n-\t\t\t gen_rtx_CLOBBER (VOIDmode,\n-\t\t\t\t\t  gen_rtx_SCRATCH (V64DImode))));\n-    }\n-\n-  return (gen_rtx_PARALLEL\n-\t  (VOIDmode,\n-\t   gen_rtvec (2, gen_rtx_SET (op0, op1),\n-\t\t      gen_rtx_USE (VOIDmode,\n-\t\t\t\t   exec ? exec : gcn_scalar_exec ()))));\n-}\n-\n-/* Generate masked move.  */\n-\n-static rtx\n-gen_duplicate_load (rtx op0, rtx op1, rtx op2 = NULL, rtx exec = NULL)\n-{\n-  if (exec)\n-    return (gen_rtx_SET (op0,\n-\t\t\t gen_rtx_VEC_MERGE (GET_MODE (op0),\n-\t\t\t\t\t    gen_rtx_VEC_DUPLICATE (GET_MODE\n-\t\t\t\t\t\t\t\t   (op0), op1),\n-\t\t\t\t\t    op2, exec)));\n-  else\n-    return (gen_rtx_SET (op0, gen_rtx_VEC_DUPLICATE (GET_MODE (op0), op1)));\n-}\n-\n /* Expand vector init of OP0 by VEC.\n    Implements vec_init instruction pattern.  */\n \n@@ -1707,10 +1989,11 @@ gcn_expand_vector_init (rtx op0, rtx vec)\n   int64_t initialized_mask = 0;\n   int64_t curr_mask = 1;\n   machine_mode mode = GET_MODE (op0);\n+  int vf = GET_MODE_NUNITS (mode);\n \n   rtx val = XVECEXP (vec, 0, 0);\n \n-  for (int i = 1; i < 64; i++)\n+  for (int i = 1; i < vf; i++)\n     if (rtx_equal_p (val, XVECEXP (vec, 0, i)))\n       curr_mask |= (int64_t) 1 << i;\n \n@@ -1719,26 +2002,26 @@ gcn_expand_vector_init (rtx op0, rtx vec)\n   else\n     {\n       val = force_reg (GET_MODE_INNER (mode), val);\n-      emit_insn (gen_duplicate_load (op0, val));\n+      emit_insn (gen_vec_duplicatevNm (op0, val));\n     }\n   initialized_mask |= curr_mask;\n-  for (int i = 1; i < 64; i++)\n+  for (int i = 1; i < vf; i++)\n     if (!(initialized_mask & ((int64_t) 1 << i)))\n       {\n \tcurr_mask = (int64_t) 1 << i;\n \trtx val = XVECEXP (vec, 0, i);\n \n-\tfor (int j = i + 1; j < 64; j++)\n+\tfor (int j = i + 1; j < vf; j++)\n \t  if (rtx_equal_p (val, XVECEXP (vec, 0, j)))\n \t    curr_mask |= (int64_t) 1 << j;\n \tif (gcn_constant_p (val))\n-\t  emit_insn (gen_mov_with_exec (op0, gcn_vec_constant (mode, val),\n-\t\t\t\t\tget_exec (curr_mask)));\n+\t  emit_insn (gen_movvNm (op0, gcn_vec_constant (mode, val), op0,\n+\t\t\t\t get_exec (curr_mask)));\n \telse\n \t  {\n \t    val = force_reg (GET_MODE_INNER (mode), val);\n-\t    emit_insn (gen_duplicate_load (op0, val, op0,\n-\t\t\t\t\t   get_exec (curr_mask)));\n+\t    emit_insn (gen_vec_duplicatevNm (op0, val, op0,\n+\t\t\t\t\t     get_exec (curr_mask)));\n \t  }\n \tinitialized_mask |= curr_mask;\n       }\n@@ -1751,18 +2034,18 @@ strided_constant (machine_mode mode, int base, int val)\n {\n   rtx x = gen_reg_rtx (mode);\n   emit_move_insn (x, gcn_vec_constant (mode, base));\n-  emit_insn (gen_addv64si3_exec (x, x, gcn_vec_constant (mode, val * 32),\n-\t\t\t\t x, get_exec (0xffffffff00000000)));\n-  emit_insn (gen_addv64si3_exec (x, x, gcn_vec_constant (mode, val * 16),\n-\t\t\t\t x, get_exec (0xffff0000ffff0000)));\n-  emit_insn (gen_addv64si3_exec (x, x, gcn_vec_constant (mode, val * 8),\n-\t\t\t\t x, get_exec (0xff00ff00ff00ff00)));\n-  emit_insn (gen_addv64si3_exec (x, x, gcn_vec_constant (mode, val * 4),\n-\t\t\t\t x, get_exec (0xf0f0f0f0f0f0f0f0)));\n-  emit_insn (gen_addv64si3_exec (x, x, gcn_vec_constant (mode, val * 2),\n-\t\t\t\t x, get_exec (0xcccccccccccccccc)));\n-  emit_insn (gen_addv64si3_exec (x, x, gcn_vec_constant (mode, val * 1),\n-\t\t\t\t x, get_exec (0xaaaaaaaaaaaaaaaa)));\n+  emit_insn (gen_addvNm3 (x, x, gcn_vec_constant (mode, val * 32),\n+\t\t\t  x, get_exec (0xffffffff00000000)));\n+  emit_insn (gen_addvNm3 (x, x, gcn_vec_constant (mode, val * 16),\n+\t\t\t  x, get_exec (0xffff0000ffff0000)));\n+  emit_insn (gen_addvNm3 (x, x, gcn_vec_constant (mode, val * 8),\n+\t\t\t  x, get_exec (0xff00ff00ff00ff00)));\n+  emit_insn (gen_addvNm3 (x, x, gcn_vec_constant (mode, val * 4),\n+\t\t\t  x, get_exec (0xf0f0f0f0f0f0f0f0)));\n+  emit_insn (gen_addvNm3 (x, x, gcn_vec_constant (mode, val * 2),\n+\t\t\t  x, get_exec (0xcccccccccccccccc)));\n+  emit_insn (gen_addvNm3 (x, x, gcn_vec_constant (mode, val * 1),\n+\t\t\t  x, get_exec (0xaaaaaaaaaaaaaaaa)));\n   return x;\n }\n \n@@ -1792,32 +2075,36 @@ gcn_addr_space_legitimize_address (rtx x, rtx old, machine_mode mode,\n     case ADDR_SPACE_LDS:\n     case ADDR_SPACE_GDS:\n       /* FIXME: LDS support offsets, handle them!.  */\n-      if (vgpr_vector_mode_p (mode) && GET_MODE (x) != V64SImode)\n+      if (vgpr_vector_mode_p (mode)\n+\t  && GET_MODE_INNER (GET_MODE (x)) != SImode)\n \t{\n-\t  rtx addrs = gen_reg_rtx (V64SImode);\n+\t  machine_mode simode = VnMODE (GET_MODE_NUNITS (mode), SImode);\n+\t  rtx addrs = gen_reg_rtx (simode);\n \t  rtx base = force_reg (SImode, x);\n-\t  rtx offsets = strided_constant (V64SImode, 0,\n+\t  rtx offsets = strided_constant (simode, 0,\n \t\t\t\t\t  GET_MODE_UNIT_SIZE (mode));\n \n-\t  emit_insn (gen_vec_duplicatev64si (addrs, base));\n-\t  emit_insn (gen_addv64si3 (addrs, offsets, addrs));\n+\t  emit_insn (gen_vec_duplicatevNsi (addrs, base));\n+\t  emit_insn (gen_addvNsi3 (addrs, offsets, addrs));\n \t  return addrs;\n \t}\n       return x;\n     }\n   gcc_unreachable ();\n }\n \n-/* Convert a (mem:<MODE> (reg:DI)) to (mem:<MODE> (reg:V64DI)) with the\n+/* Convert a (mem:<MODE> (reg:DI)) to (mem:<MODE> (reg:VnDI)) with the\n    proper vector of stepped addresses.\n \n    MEM will be a DImode address of a vector in an SGPR.\n-   TMP will be a V64DImode VGPR pair or (scratch:V64DI).  */\n+   TMP will be a VnDImode VGPR pair or (scratch:VnDI).  */\n \n rtx\n gcn_expand_scalar_to_vector_address (machine_mode mode, rtx exec, rtx mem,\n \t\t\t\t     rtx tmp)\n {\n+  machine_mode pmode = VnMODE (GET_MODE_NUNITS (mode), DImode);\n+  machine_mode offmode = VnMODE (GET_MODE_NUNITS (mode), SImode);\n   gcc_assert (MEM_P (mem));\n   rtx mem_base = XEXP (mem, 0);\n   rtx mem_index = NULL_RTX;\n@@ -1841,22 +2128,18 @@ gcn_expand_scalar_to_vector_address (machine_mode mode, rtx exec, rtx mem,\n \n   machine_mode inner = GET_MODE_INNER (mode);\n   int shift = exact_log2 (GET_MODE_SIZE (inner));\n-  rtx ramp = gen_rtx_REG (V64SImode, VGPR_REGNO (1));\n-  rtx undef_v64si = gcn_gen_undef (V64SImode);\n+  rtx ramp = gen_rtx_REG (offmode, VGPR_REGNO (1));\n   rtx new_base = NULL_RTX;\n   addr_space_t as = MEM_ADDR_SPACE (mem);\n \n   rtx tmplo = (REG_P (tmp)\n-\t       ? gcn_operand_part (V64DImode, tmp, 0)\n-\t       : gen_reg_rtx (V64SImode));\n+\t       ? gcn_operand_part (pmode, tmp, 0)\n+\t       : gen_reg_rtx (offmode));\n \n   /* tmplo[:] = ramp[:] << shift  */\n-  if (exec)\n-    emit_insn (gen_ashlv64si3_exec (tmplo, ramp,\n-\t\t\t\t    gen_int_mode (shift, SImode),\n-\t\t\t\t    undef_v64si, exec));\n-  else\n-    emit_insn (gen_ashlv64si3 (tmplo, ramp, gen_int_mode (shift, SImode)));\n+  emit_insn (gen_ashlvNsi3 (tmplo, ramp,\n+\t\t\t    gen_int_mode (shift, SImode),\n+\t\t\t    NULL, exec));\n \n   if (AS_FLAT_P (as))\n     {\n@@ -1866,53 +2149,41 @@ gcn_expand_scalar_to_vector_address (machine_mode mode, rtx exec, rtx mem,\n \t{\n \t  rtx mem_base_lo = gcn_operand_part (DImode, mem_base, 0);\n \t  rtx mem_base_hi = gcn_operand_part (DImode, mem_base, 1);\n-\t  rtx tmphi = gcn_operand_part (V64DImode, tmp, 1);\n+\t  rtx tmphi = gcn_operand_part (pmode, tmp, 1);\n \n \t  /* tmphi[:] = mem_base_hi  */\n-\t  if (exec)\n-\t    emit_insn (gen_vec_duplicatev64si_exec (tmphi, mem_base_hi,\n-\t\t\t\t\t\t    undef_v64si, exec));\n-\t  else\n-\t    emit_insn (gen_vec_duplicatev64si (tmphi, mem_base_hi));\n+\t  emit_insn (gen_vec_duplicatevNsi (tmphi, mem_base_hi, NULL, exec));\n \n \t  /* tmp[:] += zext (mem_base)  */\n \t  if (exec)\n \t    {\n-\t      emit_insn (gen_addv64si3_vcc_dup_exec (tmplo, mem_base_lo, tmplo,\n-\t\t\t\t\t\t     vcc, undef_v64si, exec));\n-\t      emit_insn (gen_addcv64si3_exec (tmphi, tmphi, const0_rtx,\n-\t\t\t\t\t      vcc, vcc, undef_v64si, exec));\n+\t      emit_insn (gen_addvNsi3_vcc_dup (tmplo, mem_base_lo, tmplo,\n+\t\t\t\t\t       vcc, NULL, exec));\n+\t      emit_insn (gen_addcvNsi3 (tmphi, tmphi, const0_rtx,\n+\t\t\t\t        vcc, vcc, NULL, exec));\n \t    }\n \t  else\n-\t    emit_insn (gen_addv64di3_vcc_zext_dup (tmp, mem_base_lo, tmp, vcc));\n+\t    emit_insn (gen_addvNdi3_vcc_zext_dup (tmp, mem_base_lo, tmp, vcc));\n \t}\n       else\n \t{\n-\t  tmp = gen_reg_rtx (V64DImode);\n-\t  if (exec)\n-\t    emit_insn (gen_addv64di3_vcc_zext_dup2_exec\n-\t\t       (tmp, tmplo, mem_base, vcc, gcn_gen_undef (V64DImode),\n-\t\t\texec));\n-\t  else\n-\t    emit_insn (gen_addv64di3_vcc_zext_dup2 (tmp, tmplo, mem_base, vcc));\n+\t  tmp = gen_reg_rtx (pmode);\n+\t  emit_insn (gen_addvNdi3_vcc_zext_dup2 (tmp, tmplo, mem_base, vcc,\n+\t\t\t\t\t\t NULL, exec));\n \t}\n \n       new_base = tmp;\n     }\n   else if (AS_ANY_DS_P (as))\n     {\n-      if (!exec)\n-\temit_insn (gen_addv64si3_dup (tmplo, tmplo, mem_base));\n-      else\n-        emit_insn (gen_addv64si3_dup_exec (tmplo, tmplo, mem_base,\n-\t\t\t\t\t   gcn_gen_undef (V64SImode), exec));\n+      emit_insn (gen_addvNsi3_dup (tmplo, tmplo, mem_base, NULL, exec));\n       new_base = tmplo;\n     }\n   else\n     {\n-      mem_base = gen_rtx_VEC_DUPLICATE (V64DImode, mem_base);\n-      new_base = gen_rtx_PLUS (V64DImode, mem_base,\n-\t\t\t       gen_rtx_SIGN_EXTEND (V64DImode, tmplo));\n+      mem_base = gen_rtx_VEC_DUPLICATE (pmode, mem_base);\n+      new_base = gen_rtx_PLUS (pmode, mem_base,\n+\t\t\t       gen_rtx_SIGN_EXTEND (pmode, tmplo));\n     }\n \n   return gen_rtx_PLUS (GET_MODE (new_base), new_base,\n@@ -1929,42 +2200,33 @@ gcn_expand_scalar_to_vector_address (machine_mode mode, rtx exec, rtx mem,\n    If EXEC is set then _exec patterns will be used, otherwise plain.\n \n    Return values.\n-     ADDR_SPACE_FLAT   - return V64DImode vector of absolute addresses.\n-     ADDR_SPACE_GLOBAL - return V64SImode vector of offsets.  */\n+     ADDR_SPACE_FLAT   - return VnDImode vector of absolute addresses.\n+     ADDR_SPACE_GLOBAL - return VnSImode vector of offsets.  */\n \n rtx\n gcn_expand_scaled_offsets (addr_space_t as, rtx base, rtx offsets, rtx scale,\n \t\t\t   bool unsigned_p, rtx exec)\n {\n-  rtx tmpsi = gen_reg_rtx (V64SImode);\n-  rtx tmpdi = gen_reg_rtx (V64DImode);\n-  rtx undefsi = exec ? gcn_gen_undef (V64SImode) : NULL;\n-  rtx undefdi = exec ? gcn_gen_undef (V64DImode) : NULL;\n+  int vf = GET_MODE_NUNITS (GET_MODE (offsets));\n+  rtx tmpsi = gen_reg_rtx (VnMODE (vf, SImode));\n+  rtx tmpdi = gen_reg_rtx (VnMODE (vf, DImode));\n \n   if (CONST_INT_P (scale)\n       && INTVAL (scale) > 0\n       && exact_log2 (INTVAL (scale)) >= 0)\n-    emit_insn (gen_ashlv64si3 (tmpsi, offsets,\n-\t\t\t       GEN_INT (exact_log2 (INTVAL (scale)))));\n+    emit_insn (gen_ashlvNsi3 (tmpsi, offsets,\n+\t\t\t      GEN_INT (exact_log2 (INTVAL (scale))),\n+\t\t\t      NULL, exec));\n   else\n-    (exec\n-     ? emit_insn (gen_mulv64si3_dup_exec (tmpsi, offsets, scale, undefsi,\n-\t\t\t\t\t  exec))\n-     : emit_insn (gen_mulv64si3_dup (tmpsi, offsets, scale)));\n+     emit_insn (gen_mulvNsi3_dup (tmpsi, offsets, scale, NULL, exec));\n \n   /* \"Global\" instructions do not support negative register offsets.  */\n   if (as == ADDR_SPACE_FLAT || !unsigned_p)\n     {\n       if (unsigned_p)\n-\t(exec\n-\t ?  emit_insn (gen_addv64di3_zext_dup2_exec (tmpdi, tmpsi, base,\n-\t\t\t\t\t\t    undefdi, exec))\n-\t :  emit_insn (gen_addv64di3_zext_dup2 (tmpdi, tmpsi, base)));\n+\t emit_insn (gen_addvNdi3_zext_dup2 (tmpdi, tmpsi, base, NULL, exec));\n       else\n-\t(exec\n-\t ?  emit_insn (gen_addv64di3_sext_dup2_exec (tmpdi, tmpsi, base,\n-\t\t\t\t\t\t     undefdi, exec))\n-\t :  emit_insn (gen_addv64di3_sext_dup2 (tmpdi, tmpsi, base)));\n+\t emit_insn (gen_addvNdi3_sext_dup2 (tmpdi, tmpsi, base, NULL, exec));\n       return tmpdi;\n     }\n   else if (as == ADDR_SPACE_GLOBAL)\n@@ -2065,59 +2327,9 @@ gcn_secondary_reload (bool in_p, rtx x, reg_class_t rclass,\n \t      || GET_MODE_CLASS (reload_mode) == MODE_VECTOR_FLOAT)\n \t    {\n \t      if (in_p)\n-\t\tswitch (reload_mode)\n-\t\t  {\n-\t\t  case E_V64SImode:\n-\t\t    sri->icode = CODE_FOR_reload_inv64si;\n-\t\t    break;\n-\t\t  case E_V64SFmode:\n-\t\t    sri->icode = CODE_FOR_reload_inv64sf;\n-\t\t    break;\n-\t\t  case E_V64HImode:\n-\t\t    sri->icode = CODE_FOR_reload_inv64hi;\n-\t\t    break;\n-\t\t  case E_V64HFmode:\n-\t\t    sri->icode = CODE_FOR_reload_inv64hf;\n-\t\t    break;\n-\t\t  case E_V64QImode:\n-\t\t    sri->icode = CODE_FOR_reload_inv64qi;\n-\t\t    break;\n-\t\t  case E_V64DImode:\n-\t\t    sri->icode = CODE_FOR_reload_inv64di;\n-\t\t    break;\n-\t\t  case E_V64DFmode:\n-\t\t    sri->icode = CODE_FOR_reload_inv64df;\n-\t\t    break;\n-\t\t  default:\n-\t\t    gcc_unreachable ();\n-\t\t  }\n+\t\tsri->icode = get_code_for_reload_in (reload_mode);\n \t      else\n-\t\tswitch (reload_mode)\n-\t\t  {\n-\t\t  case E_V64SImode:\n-\t\t    sri->icode = CODE_FOR_reload_outv64si;\n-\t\t    break;\n-\t\t  case E_V64SFmode:\n-\t\t    sri->icode = CODE_FOR_reload_outv64sf;\n-\t\t    break;\n-\t\t  case E_V64HImode:\n-\t\t    sri->icode = CODE_FOR_reload_outv64hi;\n-\t\t    break;\n-\t\t  case E_V64HFmode:\n-\t\t    sri->icode = CODE_FOR_reload_outv64hf;\n-\t\t    break;\n-\t\t  case E_V64QImode:\n-\t\t    sri->icode = CODE_FOR_reload_outv64qi;\n-\t\t    break;\n-\t\t  case E_V64DImode:\n-\t\t    sri->icode = CODE_FOR_reload_outv64di;\n-\t\t    break;\n-\t\t  case E_V64DFmode:\n-\t\t    sri->icode = CODE_FOR_reload_outv64df;\n-\t\t    break;\n-\t\t  default:\n-\t\t    gcc_unreachable ();\n-\t\t  }\n+\t\tsri->icode = get_code_for_reload_out (reload_mode);\n \t      break;\n \t    }\n \t  /* Fallthrough.  */\n@@ -3428,6 +3640,9 @@ gcn_valid_cvt_p (machine_mode from, machine_mode to, enum gcn_cvt_t op)\n \n   if (VECTOR_MODE_P (from))\n     {\n+      if (GET_MODE_NUNITS (from) != GET_MODE_NUNITS (to))\n+\treturn false;\n+\n       from = GET_MODE_INNER (from);\n       to = GET_MODE_INNER (to);\n     }\n@@ -3926,7 +4141,7 @@ gcn_expand_builtin_1 (tree exp, rtx target, rtx /*subtarget */ ,\n \trtx mem = gen_rtx_MEM (GET_MODE (target), addrs);\n \t/*set_mem_addr_space (mem, ADDR_SPACE_FLAT); */\n \t/* FIXME: set attributes.  */\n-\temit_insn (gen_mov_with_exec (target, mem, exec));\n+\temit_insn (gen_movvNm (target, mem, NULL, exec));\n \treturn target;\n       }\n     case GCN_BUILTIN_FLAT_STORE_PTR_INT32:\n@@ -3961,20 +4176,18 @@ gcn_expand_builtin_1 (tree exp, rtx target, rtx /*subtarget */ ,\n \trtx mem = gen_rtx_MEM (vmode, addrs);\n \t/*set_mem_addr_space (mem, ADDR_SPACE_FLAT); */\n \t/* FIXME: set attributes.  */\n-\temit_insn (gen_mov_with_exec (mem, val, exec));\n+\temit_insn (gen_movvNm (mem, val, NULL, exec));\n \treturn target;\n       }\n     case GCN_BUILTIN_SQRTVF:\n       {\n \tif (ignore)\n \t  return target;\n-\trtx exec = gcn_full_exec_reg ();\n \trtx arg = force_reg (V64SFmode,\n \t\t\t     expand_expr (CALL_EXPR_ARG (exp, 0), NULL_RTX,\n \t\t\t\t\t  V64SFmode,\n \t\t\t\t\t  EXPAND_NORMAL));\n-\temit_insn (gen_sqrtv64sf2_exec\n-\t\t   (target, arg, gcn_gen_undef (V64SFmode), exec));\n+\temit_insn (gen_sqrtv64sf2 (target, arg));\n \treturn target;\n       }\n     case GCN_BUILTIN_SQRTF:\n@@ -3992,20 +4205,17 @@ gcn_expand_builtin_1 (tree exp, rtx target, rtx /*subtarget */ ,\n       {\n \tif (ignore)\n \t  return target;\n-\trtx exec = gcn_full_exec_reg ();\n \trtx arg = force_reg (V64SFmode,\n \t\t\t     expand_expr (CALL_EXPR_ARG (exp, 0), NULL_RTX,\n \t\t\t\t\t  V64SFmode,\n \t\t\t\t\t  EXPAND_NORMAL));\n-\temit_insn (gen_absv64sf2_exec\n-\t\t   (target, arg, gcn_gen_undef (V64SFmode), exec));\n+\temit_insn (gen_absv64sf2 (target, arg));\n \treturn target;\n       }\n     case GCN_BUILTIN_LDEXPVF:\n       {\n \tif (ignore)\n \t  return target;\n-\trtx exec = gcn_full_exec_reg ();\n \trtx arg1 = force_reg (V64SFmode,\n \t\t\t      expand_expr (CALL_EXPR_ARG (exp, 0), NULL_RTX,\n \t\t\t\t\t   V64SFmode,\n@@ -4014,15 +4224,13 @@ gcn_expand_builtin_1 (tree exp, rtx target, rtx /*subtarget */ ,\n \t\t\t      expand_expr (CALL_EXPR_ARG (exp, 1), NULL_RTX,\n \t\t\t\t\t   V64SImode,\n \t\t\t\t\t   EXPAND_NORMAL));\n-\temit_insn (gen_ldexpv64sf3_exec\n-\t\t   (target, arg1, arg2, gcn_gen_undef (V64SFmode), exec));\n+\temit_insn (gen_ldexpv64sf3 (target, arg1, arg2));\n \treturn target;\n       }\n     case GCN_BUILTIN_LDEXPV:\n       {\n \tif (ignore)\n \t  return target;\n-\trtx exec = gcn_full_exec_reg ();\n \trtx arg1 = force_reg (V64DFmode,\n \t\t\t      expand_expr (CALL_EXPR_ARG (exp, 0), NULL_RTX,\n \t\t\t\t\t   V64SFmode,\n@@ -4031,60 +4239,51 @@ gcn_expand_builtin_1 (tree exp, rtx target, rtx /*subtarget */ ,\n \t\t\t      expand_expr (CALL_EXPR_ARG (exp, 1), NULL_RTX,\n \t\t\t\t\t   V64SImode,\n \t\t\t\t\t   EXPAND_NORMAL));\n-\temit_insn (gen_ldexpv64df3_exec\n-\t\t   (target, arg1, arg2, gcn_gen_undef (V64DFmode), exec));\n+\temit_insn (gen_ldexpv64df3 (target, arg1, arg2));\n \treturn target;\n       }\n     case GCN_BUILTIN_FREXPVF_EXP:\n       {\n \tif (ignore)\n \t  return target;\n-\trtx exec = gcn_full_exec_reg ();\n \trtx arg = force_reg (V64SFmode,\n \t\t\t     expand_expr (CALL_EXPR_ARG (exp, 0), NULL_RTX,\n \t\t\t\t\t  V64SFmode,\n \t\t\t\t\t  EXPAND_NORMAL));\n-\temit_insn (gen_frexpv64sf_exp2_exec\n-\t\t   (target, arg, gcn_gen_undef (V64SImode), exec));\n+\temit_insn (gen_frexpv64sf_exp2 (target, arg));\n \treturn target;\n       }\n     case GCN_BUILTIN_FREXPVF_MANT:\n       {\n \tif (ignore)\n \t  return target;\n-\trtx exec = gcn_full_exec_reg ();\n \trtx arg = force_reg (V64SFmode,\n \t\t\t     expand_expr (CALL_EXPR_ARG (exp, 0), NULL_RTX,\n \t\t\t\t\t  V64SFmode,\n \t\t\t\t\t  EXPAND_NORMAL));\n-\temit_insn (gen_frexpv64sf_mant2_exec\n-\t\t   (target, arg, gcn_gen_undef (V64SFmode), exec));\n+\temit_insn (gen_frexpv64sf_mant2 (target, arg));\n \treturn target;\n       }\n     case GCN_BUILTIN_FREXPV_EXP:\n       {\n \tif (ignore)\n \t  return target;\n-\trtx exec = gcn_full_exec_reg ();\n \trtx arg = force_reg (V64DFmode,\n \t\t\t     expand_expr (CALL_EXPR_ARG (exp, 0), NULL_RTX,\n \t\t\t\t\t  V64DFmode,\n \t\t\t\t\t  EXPAND_NORMAL));\n-\temit_insn (gen_frexpv64df_exp2_exec\n-\t\t   (target, arg, gcn_gen_undef (V64SImode), exec));\n+\temit_insn (gen_frexpv64df_exp2 (target, arg));\n \treturn target;\n       }\n     case GCN_BUILTIN_FREXPV_MANT:\n       {\n \tif (ignore)\n \t  return target;\n-\trtx exec = gcn_full_exec_reg ();\n \trtx arg = force_reg (V64DFmode,\n \t\t\t     expand_expr (CALL_EXPR_ARG (exp, 0), NULL_RTX,\n \t\t\t\t\t  V64DFmode,\n \t\t\t\t\t  EXPAND_NORMAL));\n-\temit_insn (gen_frexpv64df_mant2_exec\n-\t\t   (target, arg, gcn_gen_undef (V64DFmode), exec));\n+\temit_insn (gen_frexpv64df_mant2 (target, arg));\n \treturn target;\n       }\n     case GCN_BUILTIN_OMP_DIM_SIZE:\n@@ -4239,10 +4438,11 @@ gcn_vectorize_get_mask_mode (machine_mode)\n    Helper function for gcn_vectorize_vec_perm_const.  */\n \n static rtx\n-gcn_make_vec_perm_address (unsigned int *perm)\n+gcn_make_vec_perm_address (unsigned int *perm, int nelt)\n {\n-  rtx x = gen_reg_rtx (V64SImode);\n-  emit_move_insn (x, gcn_vec_constant (V64SImode, 0));\n+  machine_mode mode = VnMODE (nelt, SImode);\n+  rtx x = gen_reg_rtx (mode);\n+  emit_move_insn (x, gcn_vec_constant (mode, 0));\n \n   /* Permutation addresses use byte addressing.  With each vector lane being\n      4 bytes wide, and with 64 lanes in total, only bits 2..7 are significant,\n@@ -4258,15 +4458,13 @@ gcn_make_vec_perm_address (unsigned int *perm)\n     {\n       uint64_t exec_mask = 0;\n       uint64_t lane_mask = 1;\n-      for (int j = 0; j < 64; j++, lane_mask <<= 1)\n-\tif ((perm[j] * 4) & bit_mask)\n+      for (int j = 0; j < nelt; j++, lane_mask <<= 1)\n+\tif (((perm[j] % nelt) * 4) & bit_mask)\n \t  exec_mask |= lane_mask;\n \n       if (exec_mask)\n-\temit_insn (gen_addv64si3_exec (x, x,\n-\t\t\t\t       gcn_vec_constant (V64SImode,\n-\t\t\t\t\t\t\t bit_mask),\n-\t\t\t\t       x, get_exec (exec_mask)));\n+\temit_insn (gen_addvNsi3 (x, x, gcn_vec_constant (mode, bit_mask),\n+\t\t\t\t x, get_exec (exec_mask)));\n     }\n \n   return x;\n@@ -4336,39 +4534,11 @@ gcn_vectorize_vec_perm_const (machine_mode vmode, machine_mode op_mode,\n \tsrc1_lanes |= lane_bit;\n     }\n \n-  rtx addr = gcn_make_vec_perm_address (perm);\n-  rtx (*ds_bpermute) (rtx, rtx, rtx, rtx);\n-\n-  switch (vmode)\n-    {\n-    case E_V64QImode:\n-      ds_bpermute = gen_ds_bpermutev64qi;\n-      break;\n-    case E_V64HImode:\n-      ds_bpermute = gen_ds_bpermutev64hi;\n-      break;\n-    case E_V64SImode:\n-      ds_bpermute = gen_ds_bpermutev64si;\n-      break;\n-    case E_V64HFmode:\n-      ds_bpermute = gen_ds_bpermutev64hf;\n-      break;\n-    case E_V64SFmode:\n-      ds_bpermute = gen_ds_bpermutev64sf;\n-      break;\n-    case E_V64DImode:\n-      ds_bpermute = gen_ds_bpermutev64di;\n-      break;\n-    case E_V64DFmode:\n-      ds_bpermute = gen_ds_bpermutev64df;\n-      break;\n-    default:\n-      gcc_assert (false);\n-    }\n+  rtx addr = gcn_make_vec_perm_address (perm, nelt);\n \n   /* Load elements from src0 to dst.  */\n-  gcc_assert (~src1_lanes);\n-  emit_insn (ds_bpermute (dst, addr, src0, gcn_full_exec_reg ()));\n+  gcc_assert ((~src1_lanes) & (0xffffffffffffffffUL > (64-nelt)));\n+  emit_insn (gen_ds_bpermutevNm (dst, addr, src0, get_exec (vmode)));\n \n   /* Load elements from src1 to dst.  */\n   if (src1_lanes)\n@@ -4379,8 +4549,8 @@ gcn_vectorize_vec_perm_const (machine_mode vmode, machine_mode op_mode,\n          the two source vectors together.\n        */\n       rtx tmp = gen_reg_rtx (vmode);\n-      emit_insn (ds_bpermute (tmp, addr, src1, gcn_full_exec_reg ()));\n-      emit_insn (gen_mov_with_exec (dst, tmp, get_exec (src1_lanes)));\n+      emit_insn (gen_ds_bpermutevNm (tmp, addr, src1, get_exec (vmode)));\n+      emit_insn (gen_movvNm (dst, tmp, dst, get_exec (src1_lanes)));\n     }\n \n   return true;\n@@ -4396,7 +4566,22 @@ gcn_vector_mode_supported_p (machine_mode mode)\n {\n   return (mode == V64QImode || mode == V64HImode\n \t  || mode == V64SImode || mode == V64DImode\n-\t  || mode == V64SFmode || mode == V64DFmode);\n+\t  || mode == V64SFmode || mode == V64DFmode\n+\t  || mode == V32QImode || mode == V32HImode\n+\t  || mode == V32SImode || mode == V32DImode\n+\t  || mode == V32SFmode || mode == V32DFmode\n+\t  || mode == V16QImode || mode == V16HImode\n+\t  || mode == V16SImode || mode == V16DImode\n+\t  || mode == V16SFmode || mode == V16DFmode\n+\t  || mode == V8QImode || mode == V8HImode\n+\t  || mode == V8SImode || mode == V8DImode\n+\t  || mode == V8SFmode || mode == V8DFmode\n+\t  || mode == V4QImode || mode == V4HImode\n+\t  || mode == V4SImode || mode == V4DImode\n+\t  || mode == V4SFmode || mode == V4DFmode\n+\t  || mode == V2QImode || mode == V2HImode\n+\t  || mode == V2SImode || mode == V2DImode\n+\t  || mode == V2SFmode || mode == V2DFmode);\n }\n \n /* Implement TARGET_VECTORIZE_PREFERRED_SIMD_MODE.\n@@ -4425,23 +4610,74 @@ gcn_vectorize_preferred_simd_mode (scalar_mode mode)\n     }\n }\n \n+/* Implement TARGET_VECTORIZE_AUTOVECTORIZE_VECTOR_MODES.\n+\n+   Try all the vector modes.  */\n+\n+unsigned int gcn_autovectorize_vector_modes (vector_modes *modes,\n+\t\t\t\t\t     bool ARG_UNUSED (all))\n+{\n+  modes->safe_push (V64QImode);\n+  modes->safe_push (V64HImode);\n+  modes->safe_push (V64SImode);\n+  modes->safe_push (V64SFmode);\n+  modes->safe_push (V64DImode);\n+  modes->safe_push (V64DFmode);\n+\n+  modes->safe_push (V32QImode);\n+  modes->safe_push (V32HImode);\n+  modes->safe_push (V32SImode);\n+  modes->safe_push (V32SFmode);\n+  modes->safe_push (V32DImode);\n+  modes->safe_push (V32DFmode);\n+\n+  modes->safe_push (V16QImode);\n+  modes->safe_push (V16HImode);\n+  modes->safe_push (V16SImode);\n+  modes->safe_push (V16SFmode);\n+  modes->safe_push (V16DImode);\n+  modes->safe_push (V16DFmode);\n+\n+  modes->safe_push (V8QImode);\n+  modes->safe_push (V8HImode);\n+  modes->safe_push (V8SImode);\n+  modes->safe_push (V8SFmode);\n+  modes->safe_push (V8DImode);\n+  modes->safe_push (V8DFmode);\n+\n+  modes->safe_push (V4QImode);\n+  modes->safe_push (V4HImode);\n+  modes->safe_push (V4SImode);\n+  modes->safe_push (V4SFmode);\n+  modes->safe_push (V4DImode);\n+  modes->safe_push (V4DFmode);\n+\n+  modes->safe_push (V2QImode);\n+  modes->safe_push (V2HImode);\n+  modes->safe_push (V2SImode);\n+  modes->safe_push (V2SFmode);\n+  modes->safe_push (V2DImode);\n+  modes->safe_push (V2DFmode);\n+\n+  /* We shouldn't need VECT_COMPARE_COSTS as they should all cost the same.  */\n+  return 0;\n+}\n+\n /* Implement TARGET_VECTORIZE_RELATED_MODE.\n \n    All GCN vectors are 64-lane, so this is simpler than other architectures.\n    In particular, we do *not* want to match vector bit-size.  */\n \n static opt_machine_mode\n-gcn_related_vector_mode (machine_mode ARG_UNUSED (vector_mode),\n+gcn_related_vector_mode (machine_mode vector_mode,\n \t\t\t scalar_mode element_mode, poly_uint64 nunits)\n {\n-  if (known_ne (nunits, 0U) && known_ne (nunits, 64U))\n-    return VOIDmode;\n+  int n = nunits.to_constant ();\n \n-  machine_mode pref_mode = gcn_vectorize_preferred_simd_mode (element_mode);\n-  if (!VECTOR_MODE_P (pref_mode))\n-    return VOIDmode;\n+  if (n == 0)\n+    n = GET_MODE_NUNITS (vector_mode);\n \n-  return pref_mode;\n+  return VnMODE (n, element_mode);\n }\n \n /* Implement TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT.\n@@ -4566,6 +4802,8 @@ gcn_expand_dpp_shr_insn (machine_mode mode, const char *insn,\n    The vector register SRC of mode MODE is reduced using the operation given\n    by UNSPEC, and the scalar result is returned in lane 63 of a vector\n    register.  */\n+/* FIXME: Implement reductions for sizes other than V64.\n+          (They're currently disabled in the machine description.)  */\n \n rtx\n gcn_expand_reduc_scalar (machine_mode mode, rtx src, int unspec)\n@@ -4975,10 +5213,11 @@ gcn_md_reorg (void)\n \t\t      {\n \t\t\tif (VECTOR_MODE_P (GET_MODE (x)))\n \t\t\t  {\n-\t\t\t    new_exec = -1;\n-\t\t\t    break;\n+\t\t\t    int vf = GET_MODE_NUNITS (GET_MODE (x));\n+\t\t\t    new_exec = MAX ((uint64_t)new_exec,\n+\t\t\t\t\t    0xffffffffffffffffUL >> (64-vf));\n \t\t\t  }\n-\t\t\telse\n+\t\t\telse if (new_exec == 0)\n \t\t\t  new_exec = 1;\n \t\t      }\n \t\t  }\n@@ -5693,13 +5932,12 @@ static void\n print_reg (FILE *file, rtx x)\n {\n   machine_mode mode = GET_MODE (x);\n+  if (VECTOR_MODE_P (mode))\n+    mode = GET_MODE_INNER (mode);\n   if (mode == BImode || mode == QImode || mode == HImode || mode == SImode\n-      || mode == HFmode || mode == SFmode\n-      || mode == V64SFmode || mode == V64SImode\n-      || mode == V64QImode || mode == V64HImode)\n+      || mode == HFmode || mode == SFmode)\n     fprintf (file, \"%s\", reg_names[REGNO (x)]);\n-  else if (mode == DImode || mode == V64DImode\n-\t   || mode == DFmode || mode == V64DFmode)\n+  else if (mode == DImode || mode == DFmode)\n     {\n       if (SGPR_REGNO_P (REGNO (x)))\n \tfprintf (file, \"s[%i:%i]\", REGNO (x) - FIRST_SGPR_REG,\n@@ -6146,20 +6384,20 @@ print_operand (FILE *file, rtx x, int code)\n     case 'o':\n       {\n \tconst char *s = 0;\n-\tswitch (GET_MODE_SIZE (GET_MODE (x)))\n+\tmachine_mode mode = GET_MODE (x);\n+\tif (VECTOR_MODE_P (mode))\n+\t  mode = GET_MODE_INNER (mode);\n+\n+\tswitch (mode)\n \t  {\n-\t  case 1:\n+\t  case E_QImode:\n \t    s = \"_ubyte\";\n \t    break;\n-\t  case 2:\n+\t  case E_HImode:\n+\t  case E_HFmode:\n \t    s = \"_ushort\";\n \t    break;\n-\t  /* The following are full-vector variants.  */\n-\t  case 64:\n-\t    s = \"_ubyte\";\n-\t    break;\n-\t  case 128:\n-\t    s = \"_ushort\";\n+\t  default:\n \t    break;\n \t  }\n \n@@ -6174,43 +6412,31 @@ print_operand (FILE *file, rtx x, int code)\n       }\n     case 's':\n       {\n-\tconst char *s = \"\";\n-\tswitch (GET_MODE_SIZE (GET_MODE (x)))\n+\tconst char *s;\n+\tmachine_mode mode = GET_MODE (x);\n+\tif (VECTOR_MODE_P (mode))\n+\t  mode = GET_MODE_INNER (mode);\n+\n+\tswitch (mode)\n \t  {\n-\t  case 1:\n+\t  case E_QImode:\n \t    s = \"_byte\";\n \t    break;\n-\t  case 2:\n+\t  case E_HImode:\n+\t  case E_HFmode:\n \t    s = \"_short\";\n \t    break;\n-\t  case 4:\n+\t  case E_SImode:\n+\t  case E_SFmode:\n \t    s = \"_dword\";\n \t    break;\n-\t  case 8:\n+\t  case E_DImode:\n+\t  case E_DFmode:\n \t    s = \"_dwordx2\";\n \t    break;\n-\t  case 12:\n-\t    s = \"_dwordx3\";\n-\t    break;\n-\t  case 16:\n+\t  case E_TImode:\n \t    s = \"_dwordx4\";\n \t    break;\n-\t  case 32:\n-\t    s = \"_dwordx8\";\n-\t    break;\n-\t  case 64:\n-\t    s = VECTOR_MODE_P (GET_MODE (x)) ? \"_byte\" : \"_dwordx16\";\n-\t    break;\n-\t  /* The following are full-vector variants.  */\n-\t  case 128:\n-\t    s = \"_short\";\n-\t    break;\n-\t  case 256:\n-\t    s = \"_dword\";\n-\t    break;\n-\t  case 512:\n-\t    s = \"_dwordx2\";\n-\t    break;\n \t  default:\n \t    output_operand_lossage (\"invalid operand %%xn code\");\n \t    return;\n@@ -6714,6 +6940,9 @@ gcn_dwarf_register_span (rtx rtl)\n #define TARGET_ASM_TRAMPOLINE_TEMPLATE gcn_asm_trampoline_template\n #undef  TARGET_ATTRIBUTE_TABLE\n #define TARGET_ATTRIBUTE_TABLE gcn_attribute_table\n+#undef  TARGET_VECTORIZE_AUTOVECTORIZE_VECTOR_MODES\n+#define TARGET_VECTORIZE_AUTOVECTORIZE_VECTOR_MODES \\\n+  gcn_autovectorize_vector_modes\n #undef  TARGET_BUILTIN_DECL\n #define TARGET_BUILTIN_DECL gcn_builtin_decl\n #undef  TARGET_CAN_CHANGE_MODE_CLASS"}]}