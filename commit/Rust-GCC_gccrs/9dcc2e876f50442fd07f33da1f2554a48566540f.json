{"sha": "9dcc2e876f50442fd07f33da1f2554a48566540f", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OWRjYzJlODc2ZjUwNDQyZmQwN2YzM2RhMWYyNTU0YTQ4NTY2NTQwZg==", "commit": {"author": {"name": "Trevor Smigiel", "email": "Trevor_Smigiel@playstation.sony.com", "date": "2008-09-05T22:12:24Z"}, "committer": {"name": "Trevor Smigiel", "email": "tsmigiel@gcc.gnu.org", "date": "2008-09-05T22:12:24Z"}, "message": "Improved branch hints, safe hints, and scheduling.\n\n\t* haifa-sched.c (sched_emit_insn) : Define.\n\t* sched-int.h (sched_emit_insn) : Add prototype.\n\t* doc/invoke.texi (-mdual-nops, -mhint-max-nops,\n\t-mhint-max-distance -msafe-hints) : Document.\n\t* config/spu/spu.c (spu_flag_var_tracking): New.\n\t(TARGET_SCHED_INIT_GLOBAL, TARGET_SCHED_INIT,\n\tTARGET_SCHED_REORDER, TARGET_SCHED_REORDER2,\n\tTARGET_ASM_FILE_START): Define.\n\t(TARGET_SCHED_ADJUST_PRIORITY): Remove.\n\t(STOP_HINT_P, HINTED_P, SCHED_ON_EVEN_P): Define.\n\t(spu_emit_branch_hint): Add blocks argument.\n\t(insert_branch_hints, insert_nops): Remove.\n\t(pad_bb, insert_hbrp_for_ilb_runout, insert_hbrp, in_spu_reorg,\n\tuses_ls_unit, spu_sched_init_global, spu_sched_init,\n\tspu_sched_reorder, asm_file_start): New functions.\n\t(clock_var, spu_sched_length, pipe0_clock,\n\tpipe1_clock, prev_clock_var, prev_priority,\n\tspu_ls_first, prev_ls_clock): New static variables.\n\t* config/spu/spu.h (TARGET_DEFAULT): Add MASK_SAFE_HINTS.\n\t* config/spu.md (iprefetch): Add operand, make it clobber MEM.\n\t(nopn_nv): Add a non-volatile version of nop.\n\t* config/spu/spu.opt (-mdual-nops, -mhint-max-nops,\n\t-mhint-max-distance, -msafe-hints): New options.\n\nFrom-SVN: r140047", "tree": {"sha": "c37a03103b3833c1eaba6a9dccfbcb1a8045587e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/c37a03103b3833c1eaba6a9dccfbcb1a8045587e"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/9dcc2e876f50442fd07f33da1f2554a48566540f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9dcc2e876f50442fd07f33da1f2554a48566540f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9dcc2e876f50442fd07f33da1f2554a48566540f", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9dcc2e876f50442fd07f33da1f2554a48566540f/comments", "author": null, "committer": null, "parents": [{"sha": "a82f1f2efa3d87858bb4450212246447c6e0a5fb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a82f1f2efa3d87858bb4450212246447c6e0a5fb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a82f1f2efa3d87858bb4450212246447c6e0a5fb"}], "stats": {"total": 1270, "additions": 989, "deletions": 281}, "files": [{"sha": "e1e7df6801be15939f4f4d6b5b223ad6749b9824", "filename": "gcc/ChangeLog", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=9dcc2e876f50442fd07f33da1f2554a48566540f", "patch": "@@ -1,3 +1,31 @@\n+2008-09-05  Trevor Smigiel <Trevor_Smigiel@playstation.sony.com>\n+\n+\tImproved branch hints, safe hints, and scheduling.\n+\n+\t* haifa-sched.c (sched_emit_insn) : Define.\n+\t* sched-int.h (sched_emit_insn) : Add prototype.\n+\t* doc/invoke.texi (-mdual-nops, -mhint-max-nops,\n+\t-mhint-max-distance -msafe-hints) : Document.\n+\t* config/spu/spu.c (spu_flag_var_tracking): New.\n+\t(TARGET_SCHED_INIT_GLOBAL, TARGET_SCHED_INIT,\n+\tTARGET_SCHED_REORDER, TARGET_SCHED_REORDER2,\n+\tTARGET_ASM_FILE_START): Define.\n+\t(TARGET_SCHED_ADJUST_PRIORITY): Remove.\n+\t(STOP_HINT_P, HINTED_P, SCHED_ON_EVEN_P): Define.\n+\t(spu_emit_branch_hint): Add blocks argument.\n+\t(insert_branch_hints, insert_nops): Remove.\n+\t(pad_bb, insert_hbrp_for_ilb_runout, insert_hbrp, in_spu_reorg,\n+\tuses_ls_unit, spu_sched_init_global, spu_sched_init,\n+\tspu_sched_reorder, asm_file_start): New functions.\n+\t(clock_var, spu_sched_length, pipe0_clock,\n+\tpipe1_clock, prev_clock_var, prev_priority,\n+\tspu_ls_first, prev_ls_clock): New static variables.\n+\t* config/spu/spu.h (TARGET_DEFAULT): Add MASK_SAFE_HINTS.\n+\t* config/spu.md (iprefetch): Add operand, make it clobber MEM.\n+\t(nopn_nv): Add a non-volatile version of nop.\n+\t* config/spu/spu.opt (-mdual-nops, -mhint-max-nops,\n+\t-mhint-max-distance, -msafe-hints): New options.\n+\n 2008-09-05  Janis Johnson  <janis187@us.ibm.com>\n \t    Samuel Tardieu  <sam@rfc1149.net>\n "}, {"sha": "35a04a7a820dc44e941425f9a378931628ccc777", "filename": "gcc/config/spu/spu.c", "status": "modified", "additions": 887, "deletions": 278, "changes": 1165, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fconfig%2Fspu%2Fspu.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fconfig%2Fspu%2Fspu.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu.c?ref=9dcc2e876f50442fd07f33da1f2554a48566540f", "patch": "@@ -54,6 +54,9 @@\n #include \"tm-constrs.h\"\n #include \"spu-builtins.h\"\n #include \"ddg.h\"\n+#include \"sbitmap.h\"\n+#include \"timevar.h\"\n+#include \"df.h\"\n \n /* Builtin types, data and prototypes. */\n struct spu_builtin_range\n@@ -94,19 +97,19 @@ static rtx frame_emit_add_imm (rtx dst, rtx src, HOST_WIDE_INT imm,\n static void emit_nop_for_insn (rtx insn);\n static bool insn_clobbers_hbr (rtx insn);\n static void spu_emit_branch_hint (rtx before, rtx branch, rtx target,\n-\t\t\t\t  int distance);\n+\t\t\t\t  int distance, sbitmap blocks);\n static rtx spu_emit_vector_compare (enum rtx_code rcode, rtx op0, rtx op1,\n \t                            enum machine_mode dmode);\n static rtx get_branch_target (rtx branch);\n-static void insert_branch_hints (void);\n-static void insert_nops (void);\n static void spu_machine_dependent_reorg (void);\n static int spu_sched_issue_rate (void);\n static int spu_sched_variable_issue (FILE * dump, int verbose, rtx insn,\n \t\t\t\t     int can_issue_more);\n static int get_pipe (rtx insn);\n-static int spu_sched_adjust_priority (rtx insn, int pri);\n static int spu_sched_adjust_cost (rtx insn, rtx link, rtx dep_insn, int cost);\n+static void spu_sched_init_global (FILE *, int, int);\n+static void spu_sched_init (FILE *, int, int);\n+static int spu_sched_reorder (FILE *, int, rtx *, int *, int);\n static tree spu_handle_fndecl_attribute (tree * node, tree name, tree args,\n \t\t\t\t\t int flags,\n \t\t\t\t\t unsigned char *no_add_attrs);\n@@ -139,6 +142,7 @@ static int spu_builtin_vectorization_cost (bool);\n static bool spu_vector_alignment_reachable (const_tree, bool);\n static tree spu_builtin_vec_perm (tree, tree *);\n static int spu_sms_res_mii (struct ddg *g);\n+static void asm_file_start (void);\n \n extern const char *reg_names[];\n rtx spu_compare_op0, spu_compare_op1;\n@@ -148,6 +152,18 @@ int spu_arch;\n /* Which cpu are we tuning for.  */\n int spu_tune;\n \n+/* The hardware requires 8 insns between a hint and the branch it\n+   effects.  This variable describes how many rtl instructions the\n+   compiler needs to see before inserting a hint, and then the compiler\n+   will insert enough nops to make it at least 8 insns.  The default is\n+   for the compiler to allow up to 2 nops be emitted.  The nops are\n+   inserted in pairs, so we round down. */\n+int spu_hint_dist = (8*4) - (2*4);\n+\n+/* Determines whether we run variable tracking in machine dependent\n+   reorganization.  */\n+static int spu_flag_var_tracking;\n+\n enum spu_immediate {\n   SPU_NONE,\n   SPU_IL,\n@@ -213,11 +229,20 @@ tree spu_builtin_types[SPU_BTI_MAX];\n #undef TARGET_SCHED_ISSUE_RATE\n #define TARGET_SCHED_ISSUE_RATE spu_sched_issue_rate\n \n+#undef TARGET_SCHED_INIT_GLOBAL\n+#define TARGET_SCHED_INIT_GLOBAL spu_sched_init_global\n+\n+#undef TARGET_SCHED_INIT\n+#define TARGET_SCHED_INIT spu_sched_init\n+\n #undef TARGET_SCHED_VARIABLE_ISSUE\n #define TARGET_SCHED_VARIABLE_ISSUE spu_sched_variable_issue\n \n-#undef TARGET_SCHED_ADJUST_PRIORITY\n-#define TARGET_SCHED_ADJUST_PRIORITY spu_sched_adjust_priority\n+#undef TARGET_SCHED_REORDER\n+#define TARGET_SCHED_REORDER spu_sched_reorder\n+\n+#undef TARGET_SCHED_REORDER2\n+#define TARGET_SCHED_REORDER2 spu_sched_reorder\n \n #undef TARGET_SCHED_ADJUST_COST\n #define TARGET_SCHED_ADJUST_COST spu_sched_adjust_cost\n@@ -301,6 +326,9 @@ const struct attribute_spec spu_attribute_table[];\n #undef TARGET_SCHED_SMS_RES_MII\n #define TARGET_SCHED_SMS_RES_MII spu_sms_res_mii\n \n+#undef TARGET_ASM_FILE_START\n+#define TARGET_ASM_FILE_START asm_file_start\n+\n struct gcc_target targetm = TARGET_INITIALIZER;\n \n void\n@@ -329,9 +357,14 @@ spu_override_options (void)\n \n   flag_omit_frame_pointer = 1;\n \n+  /* Functions must be 8 byte aligned so we correctly handle dual issue */\n   if (align_functions < 8)\n     align_functions = 8;\n \n+  spu_hint_dist = 8*4 - spu_max_nops*4;\n+  if (spu_hint_dist < 0) \n+    spu_hint_dist = 0;\n+\n   if (spu_fixed_range_string)\n     fix_range (spu_fixed_range_string);\n \n@@ -1983,16 +2016,6 @@ spu_const (enum machine_mode mode, HOST_WIDE_INT val)\n \n   return gen_rtx_CONST_VECTOR (mode, v);\n }\n-\f\n-/* branch hint stuff */\n-\n-/* The hardware requires 8 insns between a hint and the branch it\n-   effects.  This variable describes how many rtl instructions the\n-   compiler needs to see before inserting a hint.  (FIXME: We should\n-   accept less and insert nops to enforce it because hinting is always\n-   profitable for performance, but we do need to be careful of code\n-   size.) */\n-int spu_hint_dist = (8 * 4);\n \n /* Create a MODE vector constant from 4 ints. */\n rtx\n@@ -2017,75 +2040,200 @@ spu_const_from_ints(enum machine_mode mode, int a, int b, int c, int d)\n   arr[15] = (d >> 0) & 0xff;\n   return array_to_constant(mode, arr);\n }\n+\f\n+/* branch hint stuff */\n \n /* An array of these is used to propagate hints to predecessor blocks. */\n struct spu_bb_info\n {\n-  rtx prop_jump;\t\t/* propagated from another block */\n-  basic_block bb;\t\t/* the original block. */\n+  rtx prop_jump; /* propagated from another block */\n+  int bb_index;  /* the original block. */\n };\n+static struct spu_bb_info *spu_bb_info;\n \n-/* The special $hbr register is used to prevent the insn scheduler from\n-   moving hbr insns across instructions which invalidate them.  It\n-   should only be used in a clobber, and this function searches for\n-   insns which clobber it.  */\n-static bool\n-insn_clobbers_hbr (rtx insn)\n+#define STOP_HINT_P(INSN) \\\n+\t\t(GET_CODE(INSN) == CALL_INSN \\\n+\t\t || INSN_CODE(INSN) == CODE_FOR_divmodsi4 \\\n+\t\t || INSN_CODE(INSN) == CODE_FOR_udivmodsi4)\n+\n+/* 1 when RTX is a hinted branch or its target.  We keep track of\n+   what has been hinted so the safe-hint code can test it easily.  */\n+#define HINTED_P(RTX)\t\t\t\t\t\t\\\n+  (RTL_FLAG_CHECK3(\"HINTED_P\", (RTX), CODE_LABEL, JUMP_INSN, CALL_INSN)->unchanging)\n+\n+/* 1 when RTX is an insn that must be scheduled on an even boundary. */\n+#define SCHED_ON_EVEN_P(RTX)\t\t\t\t\t\t\\\n+  (RTL_FLAG_CHECK2(\"SCHED_ON_EVEN_P\", (RTX), JUMP_INSN, CALL_INSN)->in_struct)\n+\n+/* Emit a nop for INSN such that the two will dual issue.  This assumes\n+   INSN is 8-byte aligned.  When INSN is inline asm we emit an lnop.\n+   We check for TImode to handle a MULTI1 insn which has dual issued its\n+   first instruction.  get_pipe returns -1 for MULTI0, inline asm, or\n+   ADDR_VEC insns. */\n+static void\n+emit_nop_for_insn (rtx insn)\n {\n-  if (INSN_P (insn) && GET_CODE (PATTERN (insn)) == PARALLEL)\n+  int p;\n+  rtx new_insn;\n+  p = get_pipe (insn);\n+  if ((CALL_P (insn) || JUMP_P (insn)) && SCHED_ON_EVEN_P (insn))\n+    new_insn = emit_insn_after (gen_lnop (), insn);\n+  else if (p == 1 && GET_MODE (insn) == TImode)\n     {\n-      rtx parallel = PATTERN (insn);\n-      rtx clobber;\n-      int j;\n-      for (j = XVECLEN (parallel, 0) - 1; j >= 0; j--)\n+      new_insn = emit_insn_before (gen_nopn (GEN_INT (127)), insn);\n+      PUT_MODE (new_insn, TImode);\n+      PUT_MODE (insn, VOIDmode);\n+    }\n+  else\n+    new_insn = emit_insn_after (gen_lnop (), insn);\n+  recog_memoized (new_insn);\n+}\n+\n+/* Insert nops in basic blocks to meet dual issue alignment\n+   requirements.  Also make sure hbrp and hint instructions are at least\n+   one cycle apart, possibly inserting a nop.  */\n+static void\n+pad_bb(void)\n+{\n+  rtx insn, next_insn, prev_insn, hbr_insn = 0;\n+  int length;\n+  int addr;\n+\n+  /* This sets up INSN_ADDRESSES. */\n+  shorten_branches (get_insns ());\n+\n+  /* Keep track of length added by nops. */\n+  length = 0;\n+\n+  prev_insn = 0;\n+  insn = get_insns ();\n+  if (!active_insn_p (insn))\n+    insn = next_active_insn (insn);\n+  for (; insn; insn = next_insn)\n+    {\n+      next_insn = next_active_insn (insn);\n+      if (INSN_CODE (insn) == CODE_FOR_iprefetch\n+\t  || INSN_CODE (insn) == CODE_FOR_hbr)\n \t{\n-\t  clobber = XVECEXP (parallel, 0, j);\n-\t  if (GET_CODE (clobber) == CLOBBER\n-\t      && GET_CODE (XEXP (clobber, 0)) == REG\n-\t      && REGNO (XEXP (clobber, 0)) == HBR_REGNUM)\n-\t    return 1;\n+\t  if (hbr_insn)\n+\t    {\n+\t      int a0 = INSN_ADDRESSES (INSN_UID (hbr_insn));\n+\t      int a1 = INSN_ADDRESSES (INSN_UID (insn));\n+\t      if ((a1 - a0 == 8 && GET_MODE (insn) != TImode)\n+\t\t  || (a1 - a0 == 4))\n+\t\t{\n+\t\t  prev_insn = emit_insn_before (gen_lnop (), insn);\n+\t\t  PUT_MODE (prev_insn, GET_MODE (insn));\n+\t\t  PUT_MODE (insn, TImode);\n+\t\t  length += 4;\n+\t\t}\n+\t    }\n+\t  hbr_insn = insn;\n+\t}\n+      if (INSN_CODE (insn) == CODE_FOR_blockage)\n+\t{\n+\t  if (GET_MODE (insn) == TImode)\n+\t    PUT_MODE (next_insn, TImode);\n+\t  insn = next_insn;\n+\t  next_insn = next_active_insn (insn);\n+\t}\n+      addr = INSN_ADDRESSES (INSN_UID (insn));\n+      if ((CALL_P (insn) || JUMP_P (insn)) && SCHED_ON_EVEN_P (insn))\n+\t{\n+\t  if (((addr + length) & 7) != 0)\n+\t    {\n+\t      emit_nop_for_insn (prev_insn);\n+\t      length += 4;\n+\t    }\n \t}\n+      else if (GET_MODE (insn) == TImode\n+\t       && ((next_insn && GET_MODE (next_insn) != TImode)\n+\t\t   || get_attr_type (insn) == TYPE_MULTI0)\n+\t       && ((addr + length) & 7) != 0)\n+\t{\n+\t  /* prev_insn will always be set because the first insn is\n+\t     always 8-byte aligned. */\n+\t  emit_nop_for_insn (prev_insn);\n+\t  length += 4;\n+\t}\n+      prev_insn = insn;\n     }\n-  return 0;\n }\n \n+\f\n+/* Routines for branch hints. */\n+\n static void\n-spu_emit_branch_hint (rtx before, rtx branch, rtx target, int distance)\n+spu_emit_branch_hint (rtx before, rtx branch, rtx target,\n+\t\t      int distance, sbitmap blocks)\n {\n-  rtx branch_label;\n-  rtx hint, insn, prev, next;\n+  rtx branch_label = 0;\n+  rtx hint;\n+  rtx insn;\n+  rtx table;\n \n   if (before == 0 || branch == 0 || target == 0)\n     return;\n \n+  /* While scheduling we require hints to be no further than 600, so\n+     we need to enforce that here too */\n   if (distance > 600)\n     return;\n \n+  /* If we have a Basic block note, emit it after the basic block note.  */\n+  if (NOTE_KIND (before) == NOTE_INSN_BASIC_BLOCK)\n+    before = NEXT_INSN (before);\n \n   branch_label = gen_label_rtx ();\n   LABEL_NUSES (branch_label)++;\n   LABEL_PRESERVE_P (branch_label) = 1;\n   insn = emit_label_before (branch_label, branch);\n   branch_label = gen_rtx_LABEL_REF (VOIDmode, branch_label);\n+  SET_BIT (blocks, BLOCK_FOR_INSN (branch)->index);\n+\n+  hint = emit_insn_before (gen_hbr (branch_label, target), before);\n+  recog_memoized (hint);\n+  HINTED_P (branch) = 1;\n \n-  /* If the previous insn is pipe0, make the hbr dual issue with it.  If\n-     the current insn is pipe0, dual issue with it. */\n-  prev = prev_active_insn (before);\n-  if (prev && get_pipe (prev) == 0)\n-    hint = emit_insn_before (gen_hbr (branch_label, target), before);\n-  else if (get_pipe (before) == 0 && distance > spu_hint_dist)\n+  if (GET_CODE (target) == LABEL_REF)\n+    HINTED_P (XEXP (target, 0)) = 1;\n+  else if (tablejump_p (branch, 0, &table))\n     {\n-      next = next_active_insn (before);\n-      hint = emit_insn_after (gen_hbr (branch_label, target), before);\n-      if (next)\n-\tPUT_MODE (next, TImode);\n+      rtvec vec;\n+      int j;\n+      if (GET_CODE (PATTERN (table)) == ADDR_VEC)\n+\tvec = XVEC (PATTERN (table), 0);\n+      else\n+\tvec = XVEC (PATTERN (table), 1);\n+      for (j = GET_NUM_ELEM (vec) - 1; j >= 0; --j)\n+\tHINTED_P (XEXP (RTVEC_ELT (vec, j), 0)) = 1;\n     }\n-  else\n+\n+  if (distance >= 588)\n     {\n-      hint = emit_insn_before (gen_hbr (branch_label, target), before);\n-      PUT_MODE (hint, TImode);\n+      /* Make sure the hint isn't scheduled any earlier than this point,\n+         which could make it too far for the branch offest to fit */\n+      recog_memoized (emit_insn_before (gen_blockage (), hint));\n+    }\n+  else if (distance <= 8 * 4)\n+    {\n+      /* To guarantee at least 8 insns between the hint and branch we\n+         insert nops. */\n+      int d;\n+      for (d = distance; d < 8 * 4; d += 4)\n+\t{\n+\t  insn =\n+\t    emit_insn_after (gen_nopn_nv (gen_rtx_REG (SImode, 127)), hint);\n+\t  recog_memoized (insn);\n+\t}\n+\n+      /* Make sure any nops inserted aren't scheduled before the hint. */\n+      recog_memoized (emit_insn_after (gen_blockage (), hint));\n+\n+      /* Make sure any nops inserted aren't scheduled after the call. */\n+      if (CALL_P (branch) && distance < 8 * 4)\n+\trecog_memoized (emit_insn_before (gen_blockage (), branch));\n     }\n-  recog_memoized (hint);\n }\n \n /* Returns 0 if we don't want a hint for this branch.  Otherwise return\n@@ -2155,245 +2303,403 @@ get_branch_target (rtx branch)\n   return 0;\n }\n \n+/* The special $hbr register is used to prevent the insn scheduler from\n+   moving hbr insns across instructions which invalidate them.  It\n+   should only be used in a clobber, and this function searches for\n+   insns which clobber it.  */\n+static bool\n+insn_clobbers_hbr (rtx insn)\n+{\n+  if (INSN_P (insn)\n+      && GET_CODE (PATTERN (insn)) == PARALLEL)\n+    {\n+      rtx parallel = PATTERN (insn);\n+      rtx clobber;\n+      int j;\n+      for (j = XVECLEN (parallel, 0) - 1; j >= 0; j--)\n+\t{\n+\t  clobber = XVECEXP (parallel, 0, j);\n+\t  if (GET_CODE (clobber) == CLOBBER\n+\t      && GET_CODE (XEXP (clobber, 0)) == REG\n+\t      && REGNO (XEXP (clobber, 0)) == HBR_REGNUM)\n+\t    return 1;\n+\t}\n+    }\n+  return 0;\n+}\n+\n+/* Search up to 32 insns starting at FIRST:\n+   - at any kind of hinted branch, just return\n+   - at any unconditional branch in the first 15 insns, just return\n+   - at a call or indirect branch, after the first 15 insns, force it to\n+     an even address and return\n+   - at any unconditional branch, after the first 15 insns, force it to\n+     an even address. \n+   At then end of the search, insert an hbrp within 4 insns of FIRST,\n+   and an hbrp within 16 instructions of FIRST.\n+ */\n static void\n-insert_branch_hints (void)\n+insert_hbrp_for_ilb_runout (rtx first)\n {\n-  struct spu_bb_info *spu_bb_info;\n-  rtx branch, insn, next;\n-  rtx branch_target = 0;\n-  int branch_addr = 0, insn_addr, head_addr;\n-  basic_block bb;\n-  unsigned int j;\n+  rtx insn, before_4 = 0, before_16 = 0;\n+  int addr = 0, length, first_addr = -1;\n+  int hbrp_addr0 = 128 * 4, hbrp_addr1 = 128 * 4;\n+  int insert_lnop_after = 0;\n+  for (insn = first; insn; insn = NEXT_INSN (insn))\n+    if (INSN_P (insn))\n+      {\n+\tif (first_addr == -1)\n+\t  first_addr = INSN_ADDRESSES (INSN_UID (insn));\n+\taddr = INSN_ADDRESSES (INSN_UID (insn)) - first_addr;\n+\tlength = get_attr_length (insn);\n+\n+\tif (before_4 == 0 && addr + length >= 4 * 4)\n+\t  before_4 = insn;\n+\t/* We test for 14 instructions because the first hbrp will add\n+\t   up to 2 instructions. */\n+\tif (before_16 == 0 && addr + length >= 14 * 4)\n+\t  before_16 = insn;\n+\n+\tif (INSN_CODE (insn) == CODE_FOR_hbr)\n+\t  {\n+\t    /* Make sure an hbrp is at least 2 cycles away from a hint. \n+\t       Insert an lnop after the hbrp when necessary. */\n+\t    if (before_4 == 0 && addr > 0)\n+\t      {\n+\t\tbefore_4 = insn;\n+\t\tinsert_lnop_after |= 1;\n+\t      }\n+\t    else if (before_4 && addr <= 4 * 4)\n+\t      insert_lnop_after |= 1;\n+\t    if (before_16 == 0 && addr > 10 * 4)\n+\t      {\n+\t\tbefore_16 = insn;\n+\t\tinsert_lnop_after |= 2;\n+\t      }\n+\t    else if (before_16 && addr <= 14 * 4)\n+\t      insert_lnop_after |= 2;\n+\t  }\n \n-  spu_bb_info =\n-    (struct spu_bb_info *) xcalloc (last_basic_block + 1,\n-\t\t\t\t    sizeof (struct spu_bb_info));\n+\tif (INSN_CODE (insn) == CODE_FOR_iprefetch)\n+\t  {\n+\t    if (addr < hbrp_addr0)\n+\t      hbrp_addr0 = addr;\n+\t    else if (addr < hbrp_addr1)\n+\t      hbrp_addr1 = addr;\n+\t  }\n \n-  /* We need exact insn addresses and lengths.  */\n-  shorten_branches (get_insns ());\n+\tif (CALL_P (insn) || JUMP_P (insn))\n+\t  {\n+\t    if (HINTED_P (insn))\n+\t      return;\n+\n+\t    /* Any branch after the first 15 insns should be on an even\n+\t       address to avoid a special case branch.  There might be\n+\t       some nops and/or hbrps inserted, so we test after 10\n+\t       insns. */\n+\t    if (addr > 10 * 4)\n+\t      SCHED_ON_EVEN_P (insn) = 1;\n+\t  }\n \n-  FOR_EACH_BB_REVERSE (bb)\n-  {\n-    head_addr = INSN_ADDRESSES (INSN_UID (BB_HEAD (bb)));\n-    branch = 0;\n-    if (spu_bb_info[bb->index].prop_jump)\n-      {\n-\tbranch = spu_bb_info[bb->index].prop_jump;\n-\tbranch_target = get_branch_target (branch);\n-\tbranch_addr = INSN_ADDRESSES (INSN_UID (branch));\n-      }\n-    /* Search from end of a block to beginning.   In this loop, find\n-       jumps which need a branch and emit them only when:\n-       - it's an indirect branch and we're at the insn which sets\n-       the register  \n-       - we're at an insn that will invalidate the hint. e.g., a\n-       call, another hint insn, inline asm that clobbers $hbr, and\n-       some inlined operations (divmodsi4).  Don't consider jumps\n-       because they are only at the end of a block and are\n-       considered when we are deciding whether to propagate\n-       - we're getting too far away from the branch.  The hbr insns\n-       only have a signed 10-bit offset\n-       We go back as far as possible so the branch will be considered\n-       for propagation when we get to the beginning of the block.  */\n-    next = 0;\n-    for (insn = BB_END (bb); insn; insn = PREV_INSN (insn))\n-      {\n-\tif (INSN_P (insn))\n+\tif (CALL_P (insn) || tablejump_p (insn, 0, 0))\n+\t  return;\n+\n+\n+\tif (addr + length >= 32 * 4)\n \t  {\n-\t    insn_addr = INSN_ADDRESSES (INSN_UID (insn));\n-\t    if (branch && next\n-\t\t&& ((GET_CODE (branch_target) == REG\n-\t\t     && set_of (branch_target, insn) != NULL_RTX)\n-\t\t    || insn_clobbers_hbr (insn)\n-\t\t    || branch_addr - insn_addr > 600))\n+\t    gcc_assert (before_4 && before_16);\n+\t    if (hbrp_addr0 > 4 * 4)\n \t      {\n-\t\tint next_addr = INSN_ADDRESSES (INSN_UID (next));\n-\t\tif (insn != BB_END (bb)\n-\t\t    && branch_addr - next_addr >= spu_hint_dist)\n+\t\tinsn =\n+\t\t  emit_insn_before (gen_iprefetch (GEN_INT (1)), before_4);\n+\t\trecog_memoized (insn);\n+\t\tINSN_ADDRESSES_NEW (insn,\n+\t\t\t\t    INSN_ADDRESSES (INSN_UID (before_4)));\n+\t\tPUT_MODE (insn, GET_MODE (before_4));\n+\t\tPUT_MODE (before_4, TImode);\n+\t\tif (insert_lnop_after & 1)\n \t\t  {\n-\t\t    if (dump_file)\n-\t\t      fprintf (dump_file,\n-\t\t\t       \"hint for %i in block %i before %i\\n\",\n-\t\t\t       INSN_UID (branch), bb->index, INSN_UID (next));\n-\t\t    spu_emit_branch_hint (next, branch, branch_target,\n-\t\t\t\t\t  branch_addr - next_addr);\n+\t\t    insn = emit_insn_before (gen_lnop (), before_4);\n+\t\t    recog_memoized (insn);\n+\t\t    INSN_ADDRESSES_NEW (insn,\n+\t\t\t\t\tINSN_ADDRESSES (INSN_UID (before_4)));\n+\t\t    PUT_MODE (insn, TImode);\n \t\t  }\n-\t\tbranch = 0;\n \t      }\n-\n-\t    /* JUMP_P will only be true at the end of a block.  When\n-\t       branch is already set it means we've previously decided\n-\t       to propagate a hint for that branch into this block. */\n-\t    if (CALL_P (insn) || (JUMP_P (insn) && !branch))\n+\t    if ((hbrp_addr0 <= 4 * 4 || hbrp_addr0 > 16 * 4)\n+\t\t&& hbrp_addr1 > 16 * 4)\n \t      {\n-\t\tbranch = 0;\n-\t\tif ((branch_target = get_branch_target (insn)))\n+\t\tinsn =\n+\t\t  emit_insn_before (gen_iprefetch (GEN_INT (2)), before_16);\n+\t\trecog_memoized (insn);\n+\t\tINSN_ADDRESSES_NEW (insn,\n+\t\t\t\t    INSN_ADDRESSES (INSN_UID (before_16)));\n+\t\tPUT_MODE (insn, GET_MODE (before_16));\n+\t\tPUT_MODE (before_16, TImode);\n+\t\tif (insert_lnop_after & 2)\n \t\t  {\n-\t\t    branch = insn;\n-\t\t    branch_addr = insn_addr;\n+\t\t    insn = emit_insn_before (gen_lnop (), before_16);\n+\t\t    recog_memoized (insn);\n+\t\t    INSN_ADDRESSES_NEW (insn,\n+\t\t\t\t\tINSN_ADDRESSES (INSN_UID\n+\t\t\t\t\t\t\t(before_16)));\n+\t\t    PUT_MODE (insn, TImode);\n \t\t  }\n \t      }\n-\n-\t    /* When a branch hint is emitted it will be inserted\n-\t       before \"next\".  Make sure next is the beginning of a\n-\t       cycle to minimize impact on the scheduled insns. */\n-\t    if (GET_MODE (insn) == TImode)\n-\t      next = insn;\n+\t    return;\n \t  }\n-\tif (insn == BB_HEAD (bb))\n-\t  break;\n       }\n+    else if (BARRIER_P (insn))\n+      return;\n \n-    if (branch)\n-      {\n-\t/* If we haven't emitted a hint for this branch yet, it might\n-\t   be profitable to emit it in one of the predecessor blocks,\n-\t   especially for loops.  */\n-\trtx bbend;\n-\tbasic_block prev = 0, prop = 0, prev2 = 0;\n-\tint loop_exit = 0, simple_loop = 0;\n-\tint next_addr = 0;\n-\tif (next)\n-\t  next_addr = INSN_ADDRESSES (INSN_UID (next));\n-\n-\tfor (j = 0; j < EDGE_COUNT (bb->preds); j++)\n-\t  if (EDGE_PRED (bb, j)->flags & EDGE_FALLTHRU)\n-\t    prev = EDGE_PRED (bb, j)->src;\n-\t  else\n-\t    prev2 = EDGE_PRED (bb, j)->src;\n-\n-\tfor (j = 0; j < EDGE_COUNT (bb->succs); j++)\n-\t  if (EDGE_SUCC (bb, j)->flags & EDGE_LOOP_EXIT)\n-\t    loop_exit = 1;\n-\t  else if (EDGE_SUCC (bb, j)->dest == bb)\n-\t    simple_loop = 1;\n-\n-\t/* If this branch is a loop exit then propagate to previous\n-\t   fallthru block. This catches the cases when it is a simple\n-\t   loop or when there is an initial branch into the loop. */\n-\tif (prev && loop_exit && prev->loop_depth <= bb->loop_depth)\n-\t  prop = prev;\n-\n-\t/* If there is only one adjacent predecessor.  Don't propagate\n-\t   outside this loop.  This loop_depth test isn't perfect, but\n-\t   I'm not sure the loop_father member is valid at this point.  */\n-\telse if (prev && single_pred_p (bb)\n-\t\t && prev->loop_depth == bb->loop_depth)\n-\t  prop = prev;\n-\n-\t/* If this is the JOIN block of a simple IF-THEN then\n-\t   propagate the hint to the HEADER block. */\n-\telse if (prev && prev2\n-\t\t && EDGE_COUNT (bb->preds) == 2\n-\t\t && EDGE_COUNT (prev->preds) == 1\n-\t\t && EDGE_PRED (prev, 0)->src == prev2\n-\t\t && prev2->loop_depth == bb->loop_depth\n-\t\t && GET_CODE (branch_target) != REG)\n-\t  prop = prev;\n-\n-\t/* Don't propagate when:\n-\t   - this is a simple loop and the hint would be too far\n-\t   - this is not a simple loop and there are 16 insns in\n-\t   this block already\n-\t   - the predecessor block ends in a branch that will be\n-\t   hinted\n-\t   - the predecessor block ends in an insn that invalidates\n-\t   the hint */\n-\tif (prop\n-\t    && prop->index >= 0\n-\t    && (bbend = BB_END (prop))\n-\t    && branch_addr - INSN_ADDRESSES (INSN_UID (bbend)) <\n-\t    (simple_loop ? 600 : 16 * 4) && get_branch_target (bbend) == 0\n-\t    && (JUMP_P (bbend) || !insn_clobbers_hbr (bbend)))\n-\t  {\n-\t    if (dump_file)\n-\t      fprintf (dump_file, \"propagate from %i to %i (loop depth %i) \"\n-\t\t       \"for %i (loop_exit %i simple_loop %i dist %i)\\n\",\n-\t\t       bb->index, prop->index, bb->loop_depth,\n-\t\t       INSN_UID (branch), loop_exit, simple_loop,\n-\t\t       branch_addr - INSN_ADDRESSES (INSN_UID (bbend)));\n-\n-\t    spu_bb_info[prop->index].prop_jump = branch;\n-\t    spu_bb_info[prop->index].bb = bb;\n-\t  }\n-\telse if (next && branch_addr - next_addr >= spu_hint_dist)\n-\t  {\n-\t    if (dump_file)\n-\t      fprintf (dump_file, \"hint for %i in block %i before %i\\n\",\n-\t\t       INSN_UID (branch), bb->index, INSN_UID (next));\n-\t    spu_emit_branch_hint (next, branch, branch_target,\n-\t\t\t\t  branch_addr - next_addr);\n-\t  }\n-\tbranch = 0;\n-      }\n-  }\n-  free (spu_bb_info);\n }\n-\f\n-/* Emit a nop for INSN such that the two will dual issue.  This assumes\n-   INSN is 8-byte aligned.  When INSN is inline asm we emit an lnop.\n-   We check for TImode to handle a MULTI1 insn which has dual issued its\n-   first instruction.  get_pipe returns -1 for MULTI0, inline asm, or\n-   ADDR_VEC insns. */\n+\n+/* The SPU might hang when it executes 48 inline instructions after a\n+   hinted branch jumps to its hinted target.  The beginning of a\n+   function and the return from a call might have been hinted, and must\n+   be handled as well.  To prevent a hang we insert 2 hbrps.  The first\n+   should be within 6 insns of the branch target.  The second should be\n+   within 22 insns of the branch target.  When determining if hbrps are\n+   necessary, we look for only 32 inline instructions, because up to to\n+   12 nops and 4 hbrps could be inserted.  Similarily, when inserting\n+   new hbrps, we insert them within 4 and 16 insns of the target.  */\n static void\n-emit_nop_for_insn (rtx insn)\n+insert_hbrp (void)\n {\n-  int p;\n-  rtx new_insn;\n-  p = get_pipe (insn);\n-  if (p == 1 && GET_MODE (insn) == TImode)\n+  rtx insn;\n+  if (TARGET_SAFE_HINTS)\n     {\n-      new_insn = emit_insn_before (gen_nopn (GEN_INT (127)), insn);\n-      PUT_MODE (new_insn, TImode);\n-      PUT_MODE (insn, VOIDmode);\n+      shorten_branches (get_insns ());\n+      /* Insert hbrp at beginning of function */\n+      insn = next_active_insn (get_insns ());\n+      if (insn)\n+\tinsert_hbrp_for_ilb_runout (insn);\n+      /* Insert hbrp after hinted targets. */\n+      for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n+\tif ((LABEL_P (insn) && HINTED_P (insn)) || CALL_P (insn))\n+\t  insert_hbrp_for_ilb_runout (next_active_insn (insn));\n     }\n-  else\n-    new_insn = emit_insn_after (gen_lnop (), insn);\n }\n \n-/* Insert nops in basic blocks to meet dual issue alignment\n-   requirements. */\n+static int in_spu_reorg;\n+\n+/* Insert branch hints.  There are no branch optimizations after this\n+   pass, so it's safe to set our branch hints now. */\n static void\n-insert_nops (void)\n+spu_machine_dependent_reorg (void)\n {\n-  rtx insn, next_insn, prev_insn;\n-  int length;\n-  int addr;\n+  sbitmap blocks;\n+  basic_block bb;\n+  rtx branch, insn;\n+  rtx branch_target = 0;\n+  int branch_addr = 0, insn_addr, required_dist = 0;\n+  int i;\n+  unsigned int j;\n \n-  /* This sets up INSN_ADDRESSES. */\n-  shorten_branches (get_insns ());\n+  if (!TARGET_BRANCH_HINTS || optimize == 0)\n+    {\n+      /* We still do it for unoptimized code because an external\n+         function might have hinted a call or return. */\n+      insert_hbrp ();\n+      pad_bb ();\n+      return;\n+    }\n \n-  /* Keep track of length added by nops. */\n-  length = 0;\n+  blocks = sbitmap_alloc (last_basic_block);\n+  sbitmap_zero (blocks);\n \n-  prev_insn = 0;\n-  for (insn = get_insns (); insn; insn = next_insn)\n+  in_spu_reorg = 1;\n+  compute_bb_for_insn ();\n+\n+  compact_blocks ();\n+\n+  spu_bb_info =\n+    (struct spu_bb_info *) xcalloc (n_basic_blocks,\n+\t\t\t\t    sizeof (struct spu_bb_info));\n+\n+  /* We need exact insn addresses and lengths.  */\n+  shorten_branches (get_insns ());\n+\n+  for (i = n_basic_blocks - 1; i >= 0; i--)\n     {\n-      next_insn = next_active_insn (insn);\n-      addr = INSN_ADDRESSES (INSN_UID (insn));\n-      if (GET_MODE (insn) == TImode\n-\t  && next_insn\n-\t  && GET_MODE (next_insn) != TImode\n-\t  && ((addr + length) & 7) != 0)\n+      bb = BASIC_BLOCK (i);\n+      branch = 0;\n+      if (spu_bb_info[i].prop_jump)\n \t{\n-\t  /* prev_insn will always be set because the first insn is\n-\t     always 8-byte aligned. */\n-\t  emit_nop_for_insn (prev_insn);\n-\t  length += 4;\n+\t  branch = spu_bb_info[i].prop_jump;\n+\t  branch_target = get_branch_target (branch);\n+\t  branch_addr = INSN_ADDRESSES (INSN_UID (branch));\n+\t  required_dist = spu_hint_dist;\n+\t}\n+      /* Search from end of a block to beginning.   In this loop, find\n+         jumps which need a branch and emit them only when:\n+         - it's an indirect branch and we're at the insn which sets\n+         the register  \n+         - we're at an insn that will invalidate the hint. e.g., a\n+         call, another hint insn, inline asm that clobbers $hbr, and\n+         some inlined operations (divmodsi4).  Don't consider jumps\n+         because they are only at the end of a block and are\n+         considered when we are deciding whether to propagate\n+         - we're getting too far away from the branch.  The hbr insns\n+         only have a signed 10 bit offset\n+         We go back as far as possible so the branch will be considered\n+         for propagation when we get to the beginning of the block.  */\n+      for (insn = BB_END (bb); insn; insn = PREV_INSN (insn))\n+\t{\n+\t  if (INSN_P (insn))\n+\t    {\n+\t      insn_addr = INSN_ADDRESSES (INSN_UID (insn));\n+\t      if (branch\n+\t\t  && ((GET_CODE (branch_target) == REG\n+\t\t       && set_of (branch_target, insn) != NULL_RTX)\n+\t\t      || insn_clobbers_hbr (insn)\n+\t\t      || branch_addr - insn_addr > 600))\n+\t\t{\n+\t\t  rtx next = NEXT_INSN (insn);\n+\t\t  int next_addr = INSN_ADDRESSES (INSN_UID (next));\n+\t\t  if (insn != BB_END (bb)\n+\t\t      && branch_addr - next_addr >= required_dist)\n+\t\t    {\n+\t\t      if (dump_file)\n+\t\t\tfprintf (dump_file,\n+\t\t\t\t \"hint for %i in block %i before %i\\n\",\n+\t\t\t\t INSN_UID (branch), bb->index,\n+\t\t\t\t INSN_UID (next));\n+\t\t      spu_emit_branch_hint (next, branch, branch_target,\n+\t\t\t\t\t    branch_addr - next_addr, blocks);\n+\t\t    }\n+\t\t  branch = 0;\n+\t\t}\n+\n+\t      /* JUMP_P will only be true at the end of a block.  When\n+\t         branch is already set it means we've previously decided\n+\t         to propagate a hint for that branch into this block. */\n+\t      if (CALL_P (insn) || (JUMP_P (insn) && !branch))\n+\t\t{\n+\t\t  branch = 0;\n+\t\t  if ((branch_target = get_branch_target (insn)))\n+\t\t    {\n+\t\t      branch = insn;\n+\t\t      branch_addr = insn_addr;\n+\t\t      required_dist = spu_hint_dist;\n+\t\t    }\n+\t\t}\n+\t    }\n+\t  if (insn == BB_HEAD (bb))\n+\t    break;\n+\t}\n+\n+      if (branch)\n+\t{\n+\t  /* If we haven't emitted a hint for this branch yet, it might\n+\t     be profitable to emit it in one of the predecessor blocks,\n+\t     especially for loops.  */\n+\t  rtx bbend;\n+\t  basic_block prev = 0, prop = 0, prev2 = 0;\n+\t  int loop_exit = 0, simple_loop = 0;\n+\t  int next_addr = INSN_ADDRESSES (INSN_UID (NEXT_INSN (insn)));\n+\n+\t  for (j = 0; j < EDGE_COUNT (bb->preds); j++)\n+\t    if (EDGE_PRED (bb, j)->flags & EDGE_FALLTHRU)\n+\t      prev = EDGE_PRED (bb, j)->src;\n+\t    else\n+\t      prev2 = EDGE_PRED (bb, j)->src;\n+\n+\t  for (j = 0; j < EDGE_COUNT (bb->succs); j++)\n+\t    if (EDGE_SUCC (bb, j)->flags & EDGE_LOOP_EXIT)\n+\t      loop_exit = 1;\n+\t    else if (EDGE_SUCC (bb, j)->dest == bb)\n+\t      simple_loop = 1;\n+\n+\t  /* If this branch is a loop exit then propagate to previous\n+\t     fallthru block. This catches the cases when it is a simple\n+\t     loop or when there is an initial branch into the loop. */\n+\t  if (prev && (loop_exit || simple_loop)\n+\t      && prev->loop_depth <= bb->loop_depth)\n+\t    prop = prev;\n+\n+\t  /* If there is only one adjacent predecessor.  Don't propagate\n+\t     outside this loop.  This loop_depth test isn't perfect, but\n+\t     I'm not sure the loop_father member is valid at this point.  */\n+\t  else if (prev && single_pred_p (bb)\n+\t\t   && prev->loop_depth == bb->loop_depth)\n+\t    prop = prev;\n+\n+\t  /* If this is the JOIN block of a simple IF-THEN then\n+\t     propogate the hint to the HEADER block. */\n+\t  else if (prev && prev2\n+\t\t   && EDGE_COUNT (bb->preds) == 2\n+\t\t   && EDGE_COUNT (prev->preds) == 1\n+\t\t   && EDGE_PRED (prev, 0)->src == prev2\n+\t\t   && prev2->loop_depth == bb->loop_depth\n+\t\t   && GET_CODE (branch_target) != REG)\n+\t    prop = prev;\n+\n+\t  /* Don't propagate when:\n+\t     - this is a simple loop and the hint would be too far\n+\t     - this is not a simple loop and there are 16 insns in\n+\t     this block already\n+\t     - the predecessor block ends in a branch that will be\n+\t     hinted\n+\t     - the predecessor block ends in an insn that invalidates\n+\t     the hint */\n+\t  if (prop\n+\t      && prop->index >= 0\n+\t      && (bbend = BB_END (prop))\n+\t      && branch_addr - INSN_ADDRESSES (INSN_UID (bbend)) <\n+\t      (simple_loop ? 600 : 16 * 4) && get_branch_target (bbend) == 0\n+\t      && (JUMP_P (bbend) || !insn_clobbers_hbr (bbend)))\n+\t    {\n+\t      if (dump_file)\n+\t\tfprintf (dump_file, \"propagate from %i to %i (loop depth %i) \"\n+\t\t\t \"for %i (loop_exit %i simple_loop %i dist %i)\\n\",\n+\t\t\t bb->index, prop->index, bb->loop_depth,\n+\t\t\t INSN_UID (branch), loop_exit, simple_loop,\n+\t\t\t branch_addr - INSN_ADDRESSES (INSN_UID (bbend)));\n+\n+\t      spu_bb_info[prop->index].prop_jump = branch;\n+\t      spu_bb_info[prop->index].bb_index = i;\n+\t    }\n+\t  else if (branch_addr - next_addr >= required_dist)\n+\t    {\n+\t      if (dump_file)\n+\t\tfprintf (dump_file, \"hint for %i in block %i before %i\\n\",\n+\t\t\t INSN_UID (branch), bb->index,\n+\t\t\t INSN_UID (NEXT_INSN (insn)));\n+\t      spu_emit_branch_hint (NEXT_INSN (insn), branch, branch_target,\n+\t\t\t\t    branch_addr - next_addr, blocks);\n+\t    }\n+\t  branch = 0;\n \t}\n-      prev_insn = insn;\n     }\n-}\n+  free (spu_bb_info);\n \n-static void\n-spu_machine_dependent_reorg (void)\n-{\n-  if (optimize > 0)\n+  if (!sbitmap_empty_p (blocks))\n+    find_many_sub_basic_blocks (blocks);\n+\n+  /* We have to schedule to make sure alignment is ok. */\n+  FOR_EACH_BB (bb) bb->flags &= ~BB_DISABLE_SCHEDULE;\n+\n+  /* The hints need to be scheduled, so call it again. */\n+  schedule_insns ();\n+\n+  insert_hbrp ();\n+\n+  pad_bb ();\n+\n+\n+  if (spu_flag_var_tracking)\n     {\n-      if (TARGET_BRANCH_HINTS)\n-\tinsert_branch_hints ();\n-      insert_nops ();\n+      df_analyze ();\n+      timevar_push (TV_VAR_TRACKING);\n+      variable_tracking_main ();\n+      timevar_pop (TV_VAR_TRACKING);\n+      df_finish_pass (false);\n     }\n+\n+  free_bb_for_insn ();\n+\n+  in_spu_reorg = 0;\n }\n \f\n \n@@ -2405,15 +2711,14 @@ spu_sched_issue_rate (void)\n }\n \n static int\n-spu_sched_variable_issue (FILE * dump ATTRIBUTE_UNUSED,\n-\t\t\t  int verbose ATTRIBUTE_UNUSED, rtx insn,\n-\t\t\t  int can_issue_more)\n+uses_ls_unit(rtx insn)\n {\n-  if (GET_CODE (PATTERN (insn)) != USE\n-      && GET_CODE (PATTERN (insn)) != CLOBBER\n-      && get_pipe (insn) != -2)\n-    can_issue_more--;\n-  return can_issue_more;\n+  rtx set = single_set (insn);\n+  if (set != 0\n+      && (GET_CODE (SET_DEST (set)) == MEM\n+\t  || GET_CODE (SET_SRC (set)) == MEM))\n+    return 1;\n+  return 0;\n }\n \n static int\n@@ -2439,7 +2744,6 @@ get_pipe (rtx insn)\n     case TYPE_FPD:\n     case TYPE_FP6:\n     case TYPE_FP7:\n-    case TYPE_IPREFETCH:\n       return 0;\n \n     case TYPE_LNOP:\n@@ -2449,41 +2753,332 @@ get_pipe (rtx insn)\n     case TYPE_BR:\n     case TYPE_MULTI1:\n     case TYPE_HBR:\n+    case TYPE_IPREFETCH:\n       return 1;\n     default:\n       abort ();\n     }\n }\n \n+\n+/* haifa-sched.c has a static variable that keeps track of the current\n+   cycle.  It is passed to spu_sched_reorder, and we record it here for\n+   use by spu_sched_variable_issue.  It won't be accurate if the\n+   scheduler updates it's clock_var between the two calls. */\n+static int clock_var;\n+\n+/* This is used to keep track of insn alignment.  Set to 0 at the\n+   beginning of each block and increased by the \"length\" attr of each\n+   insn scheduled. */\n+static int spu_sched_length;\n+\n+/* Record when we've issued pipe0 and pipe1 insns so we can reorder the\n+   ready list appropriately in spu_sched_reorder(). */\n+static int pipe0_clock;\n+static int pipe1_clock;\n+\n+static int prev_clock_var;\n+\n+static int prev_priority;\n+\n+/* The SPU needs to load the next ilb sometime during the execution of\n+   the previous ilb.  There is a potential conflict if every cycle has a\n+   load or store.  To avoid the conflict we make sure the load/store\n+   unit is free for at least one cycle during the execution of insns in\n+   the previous ilb. */\n+static int spu_ls_first;\n+static int prev_ls_clock;\n+\n+static void\n+spu_sched_init_global (FILE *file ATTRIBUTE_UNUSED, int verbose ATTRIBUTE_UNUSED,\n+\t\t       int max_ready ATTRIBUTE_UNUSED)\n+{\n+  spu_sched_length = 0;\n+}\n+\n+static void\n+spu_sched_init (FILE *file ATTRIBUTE_UNUSED, int verbose ATTRIBUTE_UNUSED,\n+\t\tint max_ready ATTRIBUTE_UNUSED)\n+{\n+  if (align_labels > 4 || align_loops > 4 || align_jumps > 4)\n+    {\n+      /* When any block might be at least 8-byte aligned, assume they\n+         will all be at least 8-byte aligned to make sure dual issue\n+         works out correctly. */\n+      spu_sched_length = 0;\n+    }\n+  spu_ls_first = INT_MAX;\n+  clock_var = -1;\n+  prev_ls_clock = -1;\n+  pipe0_clock = -1;\n+  pipe1_clock = -1;\n+  prev_clock_var = -1;\n+  prev_priority = -1;\n+}\n+\n static int\n-spu_sched_adjust_priority (rtx insn, int pri)\n+spu_sched_variable_issue (FILE *file ATTRIBUTE_UNUSED,\n+\t\t\t  int verbose ATTRIBUTE_UNUSED, rtx insn, int more)\n {\n-  int p = get_pipe (insn);\n-  /* Schedule UNSPEC_CONVERT's early so they have less effect on\n-   * scheduling.  */\n+  int len;\n+  int p;\n   if (GET_CODE (PATTERN (insn)) == USE\n       || GET_CODE (PATTERN (insn)) == CLOBBER\n-      || p == -2)\n-    return pri + 100; \n-  /* Schedule pipe0 insns early for greedier dual issue. */\n-  if (p != 1)\n-    return pri + 50;\n-  return pri;\n+      || (len = get_attr_length (insn)) == 0)\n+    return more;\n+\n+  spu_sched_length += len;\n+\n+  /* Reset on inline asm */\n+  if (INSN_CODE (insn) == -1)\n+    {\n+      spu_ls_first = INT_MAX;\n+      pipe0_clock = -1;\n+      pipe1_clock = -1;\n+      return 0;\n+    }\n+  p = get_pipe (insn);\n+  if (p == 0)\n+    pipe0_clock = clock_var;\n+  else\n+    pipe1_clock = clock_var;\n+\n+  if (in_spu_reorg)\n+    {\n+      if (clock_var - prev_ls_clock > 1\n+\t  || INSN_CODE (insn) == CODE_FOR_iprefetch)\n+\tspu_ls_first = INT_MAX;\n+      if (uses_ls_unit (insn))\n+\t{\n+\t  if (spu_ls_first == INT_MAX)\n+\t    spu_ls_first = spu_sched_length;\n+\t  prev_ls_clock = clock_var;\n+\t}\n+\n+      /* The scheduler hasn't inserted the nop, but we will later on.\n+         Include those nops in spu_sched_length. */\n+      if (prev_clock_var == clock_var && (spu_sched_length & 7))\n+\tspu_sched_length += 4;\n+      prev_clock_var = clock_var;\n+\n+      /* more is -1 when called from spu_sched_reorder for new insns\n+         that don't have INSN_PRIORITY */\n+      if (more >= 0)\n+\tprev_priority = INSN_PRIORITY (insn);\n+    }\n+\n+  /* Always try issueing more insns.  spu_sched_reorder will decide \n+     when the cycle should be advanced. */\n+  return 1;\n+}\n+\n+/* This function is called for both TARGET_SCHED_REORDER and\n+   TARGET_SCHED_REORDER2.  */\n+static int\n+spu_sched_reorder (FILE *file ATTRIBUTE_UNUSED, int verbose ATTRIBUTE_UNUSED,\n+\t\t   rtx *ready, int *nreadyp, int clock)\n+{\n+  int i, nready = *nreadyp;\n+  int pipe_0, pipe_1, pipe_hbrp, pipe_ls, schedule_i;\n+  rtx insn;\n+\n+  clock_var = clock;\n+\n+  if (nready <= 0 || pipe1_clock >= clock)\n+    return 0;\n+\n+  /* Find any rtl insns that don't generate assembly insns and schedule\n+     them first. */\n+  for (i = nready - 1; i >= 0; i--)\n+    {\n+      insn = ready[i];\n+      if (INSN_CODE (insn) == -1\n+\t  || INSN_CODE (insn) == CODE_FOR_blockage\n+\t  || INSN_CODE (insn) == CODE_FOR__spu_convert)\n+\t{\n+\t  ready[i] = ready[nready - 1];\n+\t  ready[nready - 1] = insn;\n+\t  return 1;\n+\t}\n+    }\n+\n+  pipe_0 = pipe_1 = pipe_hbrp = pipe_ls = schedule_i = -1;\n+  for (i = 0; i < nready; i++)\n+    if (INSN_CODE (ready[i]) != -1)\n+      {\n+\tinsn = ready[i];\n+\tswitch (get_attr_type (insn))\n+\t  {\n+\t  default:\n+\t  case TYPE_MULTI0:\n+\t  case TYPE_CONVERT:\n+\t  case TYPE_FX2:\n+\t  case TYPE_FX3:\n+\t  case TYPE_SPR:\n+\t  case TYPE_NOP:\n+\t  case TYPE_FXB:\n+\t  case TYPE_FPD:\n+\t  case TYPE_FP6:\n+\t  case TYPE_FP7:\n+\t    pipe_0 = i;\n+\t    break;\n+\t  case TYPE_LOAD:\n+\t  case TYPE_STORE:\n+\t    pipe_ls = i;\n+\t  case TYPE_LNOP:\n+\t  case TYPE_SHUF:\n+\t  case TYPE_BR:\n+\t  case TYPE_MULTI1:\n+\t  case TYPE_HBR:\n+\t    pipe_1 = i;\n+\t    break;\n+\t  case TYPE_IPREFETCH:\n+\t    pipe_hbrp = i;\n+\t    break;\n+\t  }\n+      }\n+\n+  /* In the first scheduling phase, schedule loads and stores together\n+     to increase the chance they will get merged during postreload CSE. */\n+  if (!reload_completed && pipe_ls >= 0)\n+    {\n+      insn = ready[pipe_ls];\n+      ready[pipe_ls] = ready[nready - 1];\n+      ready[nready - 1] = insn;\n+      return 1;\n+    }\n+\n+  /* If there is an hbrp ready, prefer it over other pipe 1 insns. */\n+  if (pipe_hbrp >= 0)\n+    pipe_1 = pipe_hbrp;\n+\n+  /* When we have loads/stores in every cycle of the last 15 insns and\n+     we are about to schedule another load/store, emit an hbrp insn\n+     instead. */\n+  if (in_spu_reorg\n+      && spu_sched_length - spu_ls_first >= 4 * 15\n+      && !(pipe0_clock < clock && pipe_0 >= 0) && pipe_1 == pipe_ls)\n+    {\n+      insn = sched_emit_insn (gen_iprefetch (GEN_INT (3)));\n+      recog_memoized (insn);\n+      if (pipe0_clock < clock)\n+\tPUT_MODE (insn, TImode);\n+      spu_sched_variable_issue (file, verbose, insn, -1);\n+      return 0;\n+    }\n+\n+  /* In general, we want to emit nops to increase dual issue, but dual\n+     issue isn't faster when one of the insns could be scheduled later\n+     without effecting the critical path.  We look at INSN_PRIORITY to\n+     make a good guess, but it isn't perfect so -mdual-nops=n can be\n+     used to effect it. */\n+  if (in_spu_reorg && spu_dual_nops < 10)\n+    {\n+      /* When we are at an even address and we are not issueing nops to\n+         improve scheduling then we need to advance the cycle.  */\n+      if ((spu_sched_length & 7) == 0 && prev_clock_var == clock\n+\t  && (spu_dual_nops == 0\n+\t      || (pipe_1 != -1\n+\t\t  && prev_priority >\n+\t\t  INSN_PRIORITY (ready[pipe_1]) + spu_dual_nops)))\n+\treturn 0;\n+\n+      /* When at an odd address, schedule the highest priority insn\n+         without considering pipeline. */\n+      if ((spu_sched_length & 7) == 4 && prev_clock_var != clock\n+\t  && (spu_dual_nops == 0\n+\t      || (prev_priority >\n+\t\t  INSN_PRIORITY (ready[nready - 1]) + spu_dual_nops)))\n+\treturn 1;\n+    }\n+\n+\n+  /* We haven't issued a pipe0 insn yet this cycle, if there is a\n+     pipe0 insn in the ready list, schedule it. */\n+  if (pipe0_clock < clock && pipe_0 >= 0)\n+    schedule_i = pipe_0;\n+\n+  /* Either we've scheduled a pipe0 insn already or there is no pipe0\n+     insn to schedule.  Put a pipe1 insn at the front of the ready list. */\n+  else\n+    schedule_i = pipe_1;\n+\n+  if (schedule_i > -1)\n+    {\n+      insn = ready[schedule_i];\n+      ready[schedule_i] = ready[nready - 1];\n+      ready[nready - 1] = insn;\n+      return 1;\n+    }\n+  return 0;\n }\n \n /* INSN is dependent on DEP_INSN. */\n static int\n-spu_sched_adjust_cost (rtx insn, rtx link ATTRIBUTE_UNUSED,\n-\t\t       rtx dep_insn ATTRIBUTE_UNUSED, int cost)\n+spu_sched_adjust_cost (rtx insn, rtx link, rtx dep_insn, int cost)\n {\n-  if (GET_CODE (insn) == CALL_INSN)\n+  rtx set;\n+\n+  /* The blockage pattern is used to prevent instructions from being\n+     moved across it and has no cost. */\n+  if (INSN_CODE (insn) == CODE_FOR_blockage\n+      || INSN_CODE (dep_insn) == CODE_FOR_blockage)\n+    return 0;\n+\n+  if (INSN_CODE (insn) == CODE_FOR__spu_convert\n+      || INSN_CODE (dep_insn) == CODE_FOR__spu_convert)\n+    return 0;\n+\n+  /* Make sure hbrps are spread out. */\n+  if (INSN_CODE (insn) == CODE_FOR_iprefetch\n+      && INSN_CODE (dep_insn) == CODE_FOR_iprefetch)\n+    return 8;\n+\n+  /* Make sure hints and hbrps are 2 cycles apart. */\n+  if ((INSN_CODE (insn) == CODE_FOR_iprefetch\n+       || INSN_CODE (insn) == CODE_FOR_hbr)\n+       && (INSN_CODE (dep_insn) == CODE_FOR_iprefetch\n+\t   || INSN_CODE (dep_insn) == CODE_FOR_hbr))\n+    return 2;\n+\n+  /* An hbrp has no real dependency on other insns. */\n+  if (INSN_CODE (insn) == CODE_FOR_iprefetch\n+      || INSN_CODE (dep_insn) == CODE_FOR_iprefetch)\n+    return 0;\n+\n+  /* Assuming that it is unlikely an argument register will be used in\n+     the first cycle of the called function, we reduce the cost for\n+     slightly better scheduling of dep_insn.  When not hinted, the\n+     mispredicted branch would hide the cost as well.  */\n+  if (CALL_P (insn))\n+  {\n+    rtx target = get_branch_target (insn);\n+    if (GET_CODE (target) != REG || !set_of (target, insn))\n+      return cost - 2;\n+    return cost;\n+  }\n+\n+  /* And when returning from a function, let's assume the return values\n+     are completed sooner too. */\n+  if (CALL_P (dep_insn))\n     return cost - 2;\n+\n+  /* Make sure an instruction that loads from the back chain is schedule\n+     away from the return instruction so a hint is more likely to get\n+     issued. */\n+  if (INSN_CODE (insn) == CODE_FOR__return\n+      && (set = single_set (dep_insn))\n+      && GET_CODE (SET_DEST (set)) == REG\n+      && REGNO (SET_DEST (set)) == LINK_REGISTER_REGNUM)\n+    return 20;\n+\n   /* The dfa scheduler sets cost to 0 for all anti-dependencies and the\n      scheduler makes every insn in a block anti-dependent on the final\n      jump_insn.  We adjust here so higher cost insns will get scheduled\n      earlier. */\n-  if (GET_CODE (insn) == JUMP_INSN && REG_NOTE_KIND (link) == REG_DEP_ANTI)\n+  if (JUMP_P (insn) && REG_NOTE_KIND (link) == REG_DEP_ANTI)\n     return insn_cost (dep_insn) - 3;\n+\n   return cost;\n }\n \f\n@@ -5660,3 +6255,17 @@ spu_libgcc_shift_count_mode (void)\n    for shift counts.  */\n   return SImode;\n }\n+\n+/* An early place to adjust some flags after GCC has finished processing\n+ * them. */\n+static void\n+asm_file_start (void)\n+{\n+  /* Variable tracking should be run after all optimizations which\n+     change order of insns.  It also needs a valid CFG. */\n+  spu_flag_var_tracking = flag_var_tracking;\n+  flag_var_tracking = 0;\n+\n+  default_file_start ();\n+}\n+"}, {"sha": "578c7427cf4e657291ab3665b15c31df6e3f7ec6", "filename": "gcc/config/spu/spu.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fconfig%2Fspu%2Fspu.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fconfig%2Fspu%2Fspu.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu.h?ref=9dcc2e876f50442fd07f33da1f2554a48566540f", "patch": "@@ -50,7 +50,8 @@ extern GTY(()) int spu_tune;\n \n /* Default target_flags if no switches specified.  */\n #ifndef TARGET_DEFAULT\n-#define TARGET_DEFAULT (MASK_ERROR_RELOC | MASK_SAFE_DMA | MASK_BRANCH_HINTS)\n+#define TARGET_DEFAULT (MASK_ERROR_RELOC | MASK_SAFE_DMA | MASK_BRANCH_HINTS \\\n+\t\t\t| MASK_SAFE_HINTS)\n #endif\n \n \f"}, {"sha": "d1fa6f0350807d85f0b6b835d83ebae48ac9c100", "filename": "gcc/config/spu/spu.md", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fconfig%2Fspu%2Fspu.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fconfig%2Fspu%2Fspu.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu.md?ref=9dcc2e876f50442fd07f33da1f2554a48566540f", "patch": "@@ -4200,12 +4200,23 @@ selb\\t%0,%4,%0,%3\"\n   \"lnop\"\n   [(set_attr \"type\" \"lnop\")])\n \n+;; The operand is so we know why we generated this hbrp.\n+;; We clobber mem to make sure it isn't moved over any\n+;; loads, stores or calls while scheduling.\n (define_insn \"iprefetch\"\n-  [(unspec [(const_int 0)] UNSPEC_IPREFETCH)]\n+  [(unspec [(match_operand:SI 0 \"const_int_operand\" \"n\")] UNSPEC_IPREFETCH)\n+   (clobber (mem:BLK (scratch)))]\n   \"\"\n-  \"hbrp\"\n+  \"hbrp\\t# %0\"\n   [(set_attr \"type\" \"iprefetch\")])\n \n+;; A non-volatile version so it gets scheduled\n+(define_insn \"nopn_nv\"\n+  [(unspec [(match_operand:SI 0 \"register_operand\" \"r\")] UNSPEC_NOP)]\n+  \"\"\n+  \"nop\\t%0\"\n+  [(set_attr \"type\" \"nop\")])\n+\n (define_insn \"hbr\"\n   [(set (reg:SI 130)\n \t(unspec:SI [(match_operand:SI 0 \"immediate_operand\" \"i,i,i\")"}, {"sha": "9cd63a14aea19bddc1444c9d660ee210ccd1a752", "filename": "gcc/config/spu/spu.opt", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fconfig%2Fspu%2Fspu.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fconfig%2Fspu%2Fspu.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu.opt?ref=9dcc2e876f50442fd07f33da1f2554a48566540f", "patch": "@@ -35,6 +35,14 @@ munsafe-dma\n Target Report RejectNegative InverseMask(SAFE_DMA)\n volatile must be specified on any memory that is effected by DMA\n \n+mdual-nops\n+Target Report Var(spu_dual_nops,10) Init(10)\n+Insert nops when it might improve performance by allowing dual issue (default)\n+\n+mdual-nops=\n+Target RejectNegative Joined UInteger Var(spu_dual_nops)\n+Insert nops when it might improve performance by allowing dual issue (default)\n+\n mstdmain\n Target Report Mask(STD_MAIN)\n Use standard main function as entry for startup\n@@ -43,6 +51,14 @@ mbranch-hints\n Target Report Mask(BRANCH_HINTS)\n Generate branch hints for branches\n \n+mhint-max-nops=\n+Target RejectNegative Joined UInteger Var(spu_max_nops) Init(2)\n+Maximum number of nops to insert for a hint (Default 2)\n+\n+mhint-max-distance=\n+Target RejectNegative Joined Var(spu_max_distance_str)\n+Approximate maximum number of instructions to allow between a hint and its branch [125]\n+\n msmall-mem\n Target Report RejectNegative InverseMask(LARGE_MEM)\n Generate code for 18 bit addressing\n@@ -55,6 +71,10 @@ mfixed-range=\n Target RejectNegative Joined Var(spu_fixed_range_string)\n Specify range of registers to make fixed\n \n+msafe-hints\n+Target Report Mask(SAFE_HINTS)\n+Insert hbrp instructions after hinted branch targets to avoid the SPU hang issue\n+\n march=\n Target RejectNegative Joined Var(spu_arch_string)\n Generate code for given CPU"}, {"sha": "b5bd719d659c930d9389dcc906eabb330095c9e1", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=9dcc2e876f50442fd07f33da1f2554a48566540f", "patch": "@@ -14797,6 +14797,33 @@ useful when compiling kernel code.  A register range is specified as\n two registers separated by a dash.  Multiple register ranges can be\n specified separated by a comma.\n \n+@item -mdual-nops\n+@itemx -mdual-nops=@var{n}\n+@opindex mdual-nops\n+By default, GCC will insert nops to increase dual issue when it expects\n+it to increase performance.  @var{n} can be a value from 0 to 10.  A\n+smaller @var{n} will insert fewer nops.  10 is the default, 0 is the\n+same as @option{-mno-dual-nops}.  Disabled with @option{-Os}.\n+\n+@item -mhint-max-nops=@var{n}\n+@opindex mhint-max-nops\n+Maximum number of nops to insert for a branch hint.  A branch hint must\n+be at least 8 instructions away from the branch it is effecting.  GCC\n+will insert up to @var{n} nops to enforce this, otherwise it will not\n+generate the branch hint.\n+\n+@item -mhint-max-distance=@var{n}\n+@opindex mhint-max-distance\n+The encoding of the branch hint instruction limits the hint to be within\n+256 instructions of the branch it is effecting.  By default, GCC makes\n+sure it is within 125. \n+\n+@item -msafe-hints\n+@opindex msafe-hints\n+Work around a hardware bug which causes the SPU to stall indefinitely.\n+By default, GCC will insert the @code{hbrp} instruction to make sure\n+this stall won't happen.\n+\n @end table\n \n @node System V Options"}, {"sha": "833e1552b31697d8c55e88426c086a8e3146d6bc", "filename": "gcc/haifa-sched.c", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fhaifa-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fhaifa-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fhaifa-sched.c?ref=9dcc2e876f50442fd07f33da1f2554a48566540f", "patch": "@@ -4979,4 +4979,15 @@ sched_create_empty_bb_1 (basic_block after)\n   return create_empty_bb (after);\n }\n \n+/* Insert PAT as an INSN into the schedule and update the necessary data\n+   structures to account for it. */\n+rtx\n+sched_emit_insn (rtx pat)\n+{\n+  rtx insn = emit_insn_after (pat, last_scheduled_insn);\n+  last_scheduled_insn = insn;\n+  haifa_init_insn (insn);\n+  return insn;\n+}\n+\n #endif /* INSN_SCHEDULING */"}, {"sha": "7fd3b5526a22de3667c128fdc15941e77c42fb1d", "filename": "gcc/sched-int.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fsched-int.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9dcc2e876f50442fd07f33da1f2554a48566540f/gcc%2Fsched-int.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-int.h?ref=9dcc2e876f50442fd07f33da1f2554a48566540f", "patch": "@@ -1147,6 +1147,7 @@ extern void unlink_bb_notes (basic_block, basic_block);\n extern void add_block (basic_block, basic_block);\n extern rtx bb_note (basic_block);\n extern void concat_note_lists (rtx, rtx *);\n+extern rtx sched_emit_insn (rtx);\n \f\n \n /* Types and functions in sched-rgn.c.  */"}]}