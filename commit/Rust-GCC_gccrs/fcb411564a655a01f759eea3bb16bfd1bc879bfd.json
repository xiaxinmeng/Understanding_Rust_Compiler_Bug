{"sha": "fcb411564a655a01f759eea3bb16bfd1bc879bfd", "node_id": "C_kwDOANBUbNoAKGZjYjQxMTU2NGE2NTVhMDFmNzU5ZWVhM2JiMTZiZmQxYmM4NzliZmQ", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2023-03-28T11:34:51Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2023-03-28T11:34:51Z"}, "message": "aarch64: Restore vectorisation of vld1 inputs [PR109072]\n\nBefore GCC 12, we would vectorize:\n\n  int32_t arr[] = { x, x, x, x };\n\nat -O3.  Vectorizing the store on its own is often a loss, particularly\nfor integers, so g:4963079769c99c4073adfd799885410ad484cbbe suppressed it.\nThis was necessary to fix regressions from enabling vectorisation at -O2,\n\nHowever, the vectorisation is important if the code subsequently loads\nfrom the array using vld1:\n\n  return vld1q_s32 (arr);\n\nThis approach of initialising an array and loading from it is the\nrecommend endian-agnostic way of constructing an ACLE vector.\n\nAs discussed in the PR notes, the general fix would be to fold the\nstore and load-back to a constructor (preferably before vectorisation).\nBut that's clearly not stage 4 material.\n\nThis patch instead delays folding vld1 until after inlining and\nrecords which decls a vld1 loads from.  It then treats vector\nstores to those decls as free, on the optimistic assumption that\nthey will be removed later.  The patch also brute-forces\nvectorization of plain constructor+store sequences, since some\nof the CPU costs make that (dubiously) expensive even when the\nstore is discounted.\n\nDelaying folding showed that we were failing to update the vops.\nThe patch fixes that too.\n\nThanks to Tamar for discussion & help with testing.\n\ngcc/\n\tPR target/109072\n\t* config/aarch64/aarch64-protos.h (aarch64_vector_load_decl): Declare.\n\t* config/aarch64/aarch64.h (machine_function::vector_load_decls): New\n\tvariable.\n\t* config/aarch64/aarch64-builtins.cc (aarch64_record_vector_load_arg):\n\tNew function.\n\t(aarch64_general_gimple_fold_builtin): Delay folding of vld1 until\n\tafter inlining.  Record which decls are loaded from.  Fix handling\n\tof vops for loads and stores.\n\t* config/aarch64/aarch64.cc (aarch64_vector_load_decl): New function.\n\t(aarch64_accesses_vector_load_decl_p): Likewise.\n\t(aarch64_vector_costs::m_stores_to_vector_load_decl): New member\n\tvariable.\n\t(aarch64_vector_costs::add_stmt_cost): If the function has a vld1\n\tthat loads from a decl, treat vector stores to those decls as\n\tzero cost.\n\t(aarch64_vector_costs::finish_cost): ...and in that case,\n\tif the vector code does nothing more than a store, give the\n\tprologue a zero cost as well.\n\ngcc/testsuite/\n\tPR target/109072\n\t* gcc.target/aarch64/pr109072_1.c: New test.\n\t* gcc.target/aarch64/pr109072_2.c: Likewise.", "tree": {"sha": "1d25efc3e95e69b3965bfec817b53102c7743b0e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1d25efc3e95e69b3965bfec817b53102c7743b0e"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/fcb411564a655a01f759eea3bb16bfd1bc879bfd", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fcb411564a655a01f759eea3bb16bfd1bc879bfd", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fcb411564a655a01f759eea3bb16bfd1bc879bfd", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fcb411564a655a01f759eea3bb16bfd1bc879bfd/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "75cda3be0232f745cda4e177d514f6900390af0b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/75cda3be0232f745cda4e177d514f6900390af0b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/75cda3be0232f745cda4e177d514f6900390af0b"}], "stats": {"total": 439, "additions": 435, "deletions": 4}, "files": [{"sha": "cc6b7c01fd1d9a3d2ddd1d73326332ed13a25f54", "filename": "gcc/config/aarch64/aarch64-builtins.cc", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.cc?ref=fcb411564a655a01f759eea3bb16bfd1bc879bfd", "patch": "@@ -2994,6 +2994,19 @@ get_mem_type_for_load_store (unsigned int fcode)\n   }\n }\n \n+/* We've seen a vector load from address ADDR.  Record it in\n+   vector_load_decls, if appropriate.  */\n+static void\n+aarch64_record_vector_load_arg (tree addr)\n+{\n+  tree decl = aarch64_vector_load_decl (addr);\n+  if (!decl)\n+    return;\n+  if (!cfun->machine->vector_load_decls)\n+    cfun->machine->vector_load_decls = hash_set<tree>::create_ggc (31);\n+  cfun->machine->vector_load_decls->add (decl);\n+}\n+\n /* Try to fold STMT, given that it's a call to the built-in function with\n    subcode FCODE.  Return the new statement on success and null on\n    failure.  */\n@@ -3051,6 +3064,11 @@ aarch64_general_gimple_fold_builtin (unsigned int fcode, gcall *stmt,\n      BUILTIN_VALL_F16 (LOAD1, ld1, 0, LOAD)\n      BUILTIN_VDQ_I (LOAD1_U, ld1, 0, LOAD)\n      BUILTIN_VALLP_NO_DI (LOAD1_P, ld1, 0, LOAD)\n+\t/* Punt until after inlining, so that we stand more chance of\n+\t   recording something meaningful in vector_load_decls.  */\n+\tif (!cfun->after_inlining)\n+\t  break;\n+\taarch64_record_vector_load_arg (args[0]);\n \tif (!BYTES_BIG_ENDIAN)\n \t  {\n \t    enum aarch64_simd_type mem_type\n@@ -3069,6 +3087,8 @@ aarch64_general_gimple_fold_builtin (unsigned int fcode, gcall *stmt,\n \t\t\t\t     fold_build2 (MEM_REF,\n \t\t\t\t\t\t  access_type,\n \t\t\t\t\t\t  args[0], zero));\n+\t    gimple_set_vuse (new_stmt, gimple_vuse (stmt));\n+\t    gimple_set_vdef (new_stmt, gimple_vdef (stmt));\n \t  }\n \tbreak;\n \n@@ -3092,6 +3112,8 @@ aarch64_general_gimple_fold_builtin (unsigned int fcode, gcall *stmt,\n \t      = gimple_build_assign (fold_build2 (MEM_REF, access_type,\n \t\t\t\t\t\t  args[0], zero),\n \t\t\t\t     args[1]);\n+\t    gimple_set_vuse (new_stmt, gimple_vuse (stmt));\n+\t    gimple_set_vdef (new_stmt, gimple_vdef (stmt));\n \t  }\n \tbreak;\n "}, {"sha": "63339fa47df983fac97ed8b1cf965852a2aa8f42", "filename": "gcc/config/aarch64/aarch64-protos.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h?ref=fcb411564a655a01f759eea3bb16bfd1bc879bfd", "patch": "@@ -761,6 +761,7 @@ bool aarch64_const_vec_all_same_in_range_p (rtx, HOST_WIDE_INT,\n bool aarch64_constant_address_p (rtx);\n bool aarch64_emit_approx_div (rtx, rtx, rtx);\n bool aarch64_emit_approx_sqrt (rtx, rtx, bool);\n+tree aarch64_vector_load_decl (tree);\n void aarch64_expand_call (rtx, rtx, rtx, bool);\n bool aarch64_expand_cpymem (rtx *);\n bool aarch64_expand_setmem (rtx *);"}, {"sha": "cc119d0acdd7cbc8ec743a508e3019decc5ee147", "filename": "gcc/config/aarch64/aarch64.cc", "status": "modified", "additions": 66, "deletions": 4, "changes": 70, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.cc?ref=fcb411564a655a01f759eea3bb16bfd1bc879bfd", "patch": "@@ -15661,6 +15661,33 @@ aarch64_first_cycle_multipass_dfa_lookahead_guard (rtx_insn *insn,\n \n /* Vectorizer cost model target hooks.  */\n \n+/* If a vld1 from address ADDR should be recorded in vector_load_decls,\n+   return the decl that should be recorded.  Return null otherwise.  */\n+tree\n+aarch64_vector_load_decl (tree addr)\n+{\n+  if (TREE_CODE (addr) != ADDR_EXPR)\n+    return NULL_TREE;\n+  tree base = get_base_address (TREE_OPERAND (addr, 0));\n+  if (TREE_CODE (base) != VAR_DECL)\n+    return NULL_TREE;\n+  return base;\n+}\n+\n+/* Return true if STMT_INFO accesses a decl that is known to be the\n+   argument to a vld1 in the same function.  */\n+static bool\n+aarch64_accesses_vector_load_decl_p (stmt_vec_info stmt_info)\n+{\n+  if (!cfun->machine->vector_load_decls)\n+    return false;\n+  auto dr = STMT_VINFO_DATA_REF (stmt_info);\n+  if (!dr)\n+    return false;\n+  tree decl = aarch64_vector_load_decl (DR_BASE_ADDRESS (dr));\n+  return decl && cfun->machine->vector_load_decls->contains (decl);\n+}\n+\n /* Information about how the CPU would issue the scalar, Advanced SIMD\n    or SVE version of a vector loop, using the scheme defined by the\n    aarch64_base_vec_issue_info hierarchy of structures.  */\n@@ -15891,6 +15918,20 @@ class aarch64_vector_costs : public vector_costs\n      supported by Advanced SIMD and SVE2.  */\n   bool m_has_avg = false;\n \n+  /* True if the vector body contains a store to a decl and if the\n+     function is known to have a vld1 from the same decl.\n+\n+     In the Advanced SIMD ACLE, the recommended endian-agnostic way of\n+     initializing a vector is:\n+\n+       float f[4] = { elts };\n+       float32x4_t x = vld1q_f32(f);\n+\n+     We should strongly prefer vectorization of the initialization of f,\n+     so that the store to f and the load back can be optimized away,\n+     leaving a vectorization of { elts }.  */\n+  bool m_stores_to_vector_load_decl = false;\n+\n   /* - If M_VEC_FLAGS is zero then we're costing the original scalar code.\n      - If M_VEC_FLAGS & VEC_ADVSIMD is nonzero then we're costing Advanced\n        SIMD code.\n@@ -16907,6 +16948,18 @@ aarch64_vector_costs::add_stmt_cost (int count, vect_cost_for_stmt kind,\n \t    }\n \t}\n     }\n+\n+  /* If the statement stores to a decl that is known to be the argument\n+     to a vld1 in the same function, ignore the store for costing purposes.\n+     See the comment above m_stores_to_vector_load_decl for more details.  */\n+  if (stmt_info\n+      && (kind == vector_store || kind == unaligned_store)\n+      && aarch64_accesses_vector_load_decl_p (stmt_info))\n+    {\n+      stmt_cost = 0;\n+      m_stores_to_vector_load_decl = true;\n+    }\n+\n   return record_stmt_cost (stmt_info, where, (count * stmt_cost).ceil ());\n }\n \n@@ -17196,12 +17249,21 @@ aarch64_vector_costs::finish_cost (const vector_costs *uncast_scalar_costs)\n \n   /* Apply the heuristic described above m_stp_sequence_cost.  Prefer\n      the scalar code in the event of a tie, since there is more chance\n-     of scalar code being optimized with surrounding operations.  */\n+     of scalar code being optimized with surrounding operations.\n+\n+     In addition, if the vector body is a simple store to a decl that\n+     is elsewhere loaded using vld1, strongly prefer the vector form,\n+     to the extent of giving the prologue a zero cost.  See the comment\n+     above m_stores_to_vector_load_decl for details.  */\n   if (!loop_vinfo\n       && scalar_costs\n-      && m_stp_sequence_cost != ~0U\n-      && m_stp_sequence_cost >= scalar_costs->m_stp_sequence_cost)\n-    m_costs[vect_body] = 2 * scalar_costs->total_cost ();\n+      && m_stp_sequence_cost != ~0U)\n+    {\n+      if (m_stores_to_vector_load_decl)\n+\tm_costs[vect_prologue] = 0;\n+      else if (m_stp_sequence_cost >= scalar_costs->m_stp_sequence_cost)\n+\tm_costs[vect_body] = 2 * scalar_costs->total_cost ();\n+    }\n \n   vector_costs::finish_cost (scalar_costs);\n }"}, {"sha": "155cace6afea4b1b3828146e22f8a8214deaf76b", "filename": "gcc/config/aarch64/aarch64.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Fconfig%2Faarch64%2Faarch64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Fconfig%2Faarch64%2Faarch64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.h?ref=fcb411564a655a01f759eea3bb16bfd1bc879bfd", "patch": "@@ -860,6 +860,7 @@ struct GTY (()) aarch64_frame\n   bool is_scs_enabled;\n };\n \n+#ifdef hash_set_h\n typedef struct GTY (()) machine_function\n {\n   struct aarch64_frame frame;\n@@ -868,8 +869,12 @@ typedef struct GTY (()) machine_function\n   /* One entry for each general purpose register.  */\n   rtx call_via[SP_REGNUM];\n   bool label_is_assembled;\n+  /* A set of all decls that have been passed to a vld1 intrinsic in the\n+     current function.  This is used to help guide the vector cost model.  */\n+  hash_set<tree> *vector_load_decls;\n } machine_function;\n #endif\n+#endif\n \n /* Which ABI to use.  */\n enum aarch64_abi_type"}, {"sha": "6c1d2b0bdccfb74b80d938a0d94413f0f9dda5ab", "filename": "gcc/testsuite/gcc.target/aarch64/pr109072_1.c", "status": "added", "additions": 281, "deletions": 0, "changes": 281, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fpr109072_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fpr109072_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fpr109072_1.c?ref=fcb411564a655a01f759eea3bb16bfd1bc879bfd", "patch": "@@ -0,0 +1,281 @@\n+/* { dg-options \"-O2 -fno-schedule-insns -fno-schedule-insns2\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" \"\" { target aarch64_little_endian } } } */\n+\n+#include <arm_neon.h>\n+\n+/*\n+** s32x2_1:\n+**\tdup\tv0\\.2s, w0\n+**\tret\n+*/\n+int32x2_t\n+s32x2_1 (int32_t x)\n+{\n+  int32_t arr[] = { x, x };\n+  return vld1_s32 (arr);\n+}\n+\n+/*\n+** s32x2_2:\n+**\tfmov\ts0, w0\n+**\tret\n+*/\n+int32x2_t\n+s32x2_2 (int32_t x)\n+{\n+  int32_t arr[] = { x, 0 };\n+  return vld1_s32 (arr);\n+}\n+\n+/*\n+** s32x2_3:\n+**\tfmov\ts0, w0\n+**\tins\tv0\\.s\\[1\\], w1\n+**\tret\n+*/\n+int32x2_t\n+s32x2_3 (int32_t x, int32_t y)\n+{\n+  int32_t arr[] = { x, y };\n+  return vld1_s32 (arr);\n+}\n+\n+/*\n+** f32x2_1:\n+**\tdup\tv0\\.2s, v0.s\\[0\\]\n+**\tret\n+*/\n+float32x2_t\n+f32x2_1 (float32_t x)\n+{\n+  float32_t arr[] = { x, x };\n+  return vld1_f32 (arr);\n+}\n+\n+/*\n+** f32x2_2:\n+**\tins\tv0\\.s\\[1\\], v1.s\\[0\\]\n+**\tret\n+*/\n+float32x2_t\n+f32x2_2 (float32_t x, float32_t y)\n+{\n+  float32_t arr[] = { x, y };\n+  return vld1_f32 (arr);\n+}\n+\n+/*\n+** s16x4_1:\n+**\tdup\tv0\\.4h, w0\n+**\tret\n+*/\n+int16x4_t\n+s16x4_1 (int16_t x)\n+{\n+  int16_t arr[] = { x, x, x, x };\n+  return vld1_s16 (arr);\n+}\n+\n+/*\n+** s16x4_2:\n+**\t...\n+**\tfmov\t[dsh]0, [wx][0-9]+\n+**\tret\n+*/\n+int16x4_t\n+s16x4_2 (int16_t x)\n+{\n+  int16_t arr[] = { x, 0, 0, 0 };\n+  return vld1_s16 (arr);\n+}\n+\n+/*\n+** s16x4_3:\n+**\tdup\tv0\\.4h, w1\n+**\tins\tv0.h\\[0\\], w0\n+**\tret\n+*/\n+int16x4_t\n+s16x4_3 (int16_t x, int16_t y)\n+{\n+  int16_t arr[] = { x, y, y, y };\n+  return vld1_s16 (arr);\n+}\n+\n+/*\n+** f16x4_1:\n+**\tdup\tv0\\.4h, v0.h\\[0\\]\n+**\tret\n+*/\n+float16x4_t\n+f16x4_1 (float16_t x)\n+{\n+  float16_t arr[] = { x, x, x, x };\n+  return vld1_f16 (arr);\n+}\n+\n+/*\n+** s64x2_1:\n+**\tdup\tv0\\.2d, x0\n+**\tret\n+*/\n+int64x2_t\n+s64x2_1 (int64_t x)\n+{\n+  int64_t arr[] = { x, x };\n+  return vld1q_s64 (arr);\n+}\n+\n+/*\n+** s64x2_2: { xfail *-*-* }\n+**\tfmov\td0, x0\n+**\tret\n+*/\n+int64x2_t\n+s64x2_2 (int64_t x)\n+{\n+  int64_t arr[] = { x, 0 };\n+  return vld1q_s64 (arr);\n+}\n+\n+/*\n+** s64x2_3:\n+**\tfmov\td0, x0\n+**\tins\tv0\\.d\\[1\\], x1\n+**\tret\n+*/\n+int64x2_t\n+s64x2_3 (int64_t x, int64_t y)\n+{\n+  int64_t arr[] = { x, y };\n+  return vld1q_s64 (arr);\n+}\n+\n+/*\n+** f64x2_1:\n+**\tdup\tv0\\.2d, v0.d\\[0\\]\n+**\tret\n+*/\n+float64x2_t\n+f64x2_1 (float64_t x)\n+{\n+  float64_t arr[] = { x, x };\n+  return vld1q_f64 (arr);\n+}\n+\n+/*\n+** f64x2_2:\n+**\tins\tv0\\.d\\[1\\], v1.d\\[0\\]\n+**\tret\n+*/\n+float64x2_t\n+f64x2_2 (float64_t x, float64_t y)\n+{\n+  float64_t arr[] = { x, y };\n+  return vld1q_f64 (arr);\n+}\n+\n+/*\n+** s32x4_1:\n+**\tdup\tv0\\.4s, w0\n+**\tret\n+*/\n+int32x4_t\n+s32x4_1 (int32_t x)\n+{\n+  int32_t arr[] = { x, x, x, x };\n+  return vld1q_s32 (arr);\n+}\n+\n+/*\n+** s32x4_2: { xfail *-*-* }\n+**\tfmov\ts0, w0\n+**\tret\n+*/\n+int32x4_t\n+s32x4_2 (int32_t x)\n+{\n+  int32_t arr[] = { x, 0, 0, 0 };\n+  return vld1q_s32 (arr);\n+}\n+\n+/*\n+** s32x4_3:\n+**\tdup\tv0\\.4s, w1\n+**\tins\tv0.s\\[0\\], w0\n+**\tret\n+*/\n+int32x4_t\n+s32x4_3 (int32_t x, int32_t y)\n+{\n+  int32_t arr[] = { x, y, y, y };\n+  return vld1q_s32 (arr);\n+}\n+\n+/*\n+** f32x4_1:\n+**\tdup\tv0\\.4s, v0.s\\[0\\]\n+**\tret\n+*/\n+float32x4_t\n+f32x4_1 (float32_t x)\n+{\n+  float32_t arr[] = { x, x, x, x };\n+  return vld1q_f32 (arr);\n+}\n+\n+void consume (float32x4_t, float32x4_t, float32x4_t, float32x4_t);\n+\n+/*\n+** produce_1:\n+** (\n+**\tdup\tv0\\.4s, v0\\.s\\[0\\]\n+**\tdup\tv1\\.4s, v1\\.s\\[0\\]\n+**\tdup\tv2\\.4s, v2\\.s\\[0\\]\n+**\tdup\tv3\\.4s, v3\\.s\\[0\\]\n+** |\n+**\tdup\tv3\\.4s, v3\\.s\\[0\\]\n+**\tdup\tv2\\.4s, v2\\.s\\[0\\]\n+**\tdup\tv1\\.4s, v1\\.s\\[0\\]\n+**\tdup\tv0\\.4s, v0\\.s\\[0\\]\n+** )\n+**\tb\tconsume\n+*/\n+void\n+produce_1 (float32_t a, float32_t b, float32_t c, float32_t d)\n+{\n+  float arr[4][4] = {\n+    { a, a, a, a },\n+    { b, b, b, b },\n+    { c, c, c, c },\n+    { d, d, d, d }\n+  };\n+  consume (vld1q_f32 (arr[0]), vld1q_f32 (arr[1]),\n+\t   vld1q_f32 (arr[2]), vld1q_f32 (arr[3]));\n+}\n+\n+/*\n+** produce_2:\n+** (\n+**\tdup\tv0\\.4s, v0\\.s\\[0\\]\n+**\tdup\tv1\\.4s, v1\\.s\\[0\\]\n+**\tdup\tv2\\.4s, v2\\.s\\[0\\]\n+**\tdup\tv3\\.4s, v3\\.s\\[0\\]\n+** |\n+**\tdup\tv3\\.4s, v3\\.s\\[0\\]\n+**\tdup\tv2\\.4s, v2\\.s\\[0\\]\n+**\tdup\tv1\\.4s, v1\\.s\\[0\\]\n+**\tdup\tv0\\.4s, v0\\.s\\[0\\]\n+** )\n+**\tb\tconsume\n+*/\n+void\n+produce_2 (float32_t a, float32_t b, float32_t c, float32_t d)\n+{\n+  float arr0[] = { a, a, a, a };\n+  float arr1[] = { b, b, b, b };\n+  float arr2[] = { c, c, c, c };\n+  float arr3[] = { d, d, d, d };\n+  consume (vld1q_f32 (arr0), vld1q_f32 (arr1),\n+\t   vld1q_f32 (arr2), vld1q_f32 (arr3));\n+}"}, {"sha": "d532f08aa0c9bbd5b929c611225ebd9e2874b54c", "filename": "gcc/testsuite/gcc.target/aarch64/pr109072_2.c", "status": "added", "additions": 60, "deletions": 0, "changes": 60, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fpr109072_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fcb411564a655a01f759eea3bb16bfd1bc879bfd/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fpr109072_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fpr109072_2.c?ref=fcb411564a655a01f759eea3bb16bfd1bc879bfd", "patch": "@@ -0,0 +1,60 @@\n+/* { dg-options \"-O\" } */\n+\n+#pragma GCC target \"arch=armv8.2-a+dotprod\"\n+\n+#include <arm_neon.h>\n+\n+static inline uint32_t horizontal_add_uint32x4(const uint32x4_t a) {\n+  return vaddvq_u32(a);\n+}\n+\n+static inline unsigned int sadwxh_avg_neon(const uint8_t *src_ptr,\n+                                           int src_stride,\n+                                           const uint8_t *ref_ptr,\n+                                           int ref_stride, int w, int h,\n+                                           const uint8_t *second_pred) {\n+\n+\n+  uint32x4_t sum[2] = { vdupq_n_u32(0), vdupq_n_u32(0) };\n+\n+  int i = h;\n+  do {\n+    int j = 0;\n+    do {\n+      uint8x16_t s0, s1, r0, r1, p0, p1, avg0, avg1, diff0, diff1;\n+\n+      s0 = vld1q_u8(src_ptr + j);\n+      r0 = vld1q_u8(ref_ptr + j);\n+      p0 = vld1q_u8(second_pred);\n+      avg0 = vrhaddq_u8(r0, p0);\n+      diff0 = vabdq_u8(s0, avg0);\n+      sum[0] = vdotq_u32(sum[0], diff0, vdupq_n_u8(1));\n+\n+      s1 = vld1q_u8(src_ptr + j + 16);\n+      r1 = vld1q_u8(ref_ptr + j + 16);\n+      p1 = vld1q_u8(second_pred + 16);\n+      avg1 = vrhaddq_u8(r1, p1);\n+      diff1 = vabdq_u8(s1, avg1);\n+      sum[1] = vdotq_u32(sum[1], diff1, vdupq_n_u8(1));\n+\n+      j += 32;\n+      second_pred += 32;\n+    } while (j < w);\n+\n+    src_ptr += src_stride;\n+    ref_ptr += ref_stride;\n+  } while (--i != 0);\n+\n+  return horizontal_add_uint32x4(vaddq_u32(sum[0], sum[1]));\n+}\n+\n+static inline unsigned int sad32xh_avg_neon(const uint8_t *src_ptr,\n+                                            int src_stride,\n+                                            const uint8_t *ref_ptr,\n+                                            int ref_stride, int h,\n+                                            const uint8_t *second_pred) {\n+  return sadwxh_avg_neon(src_ptr, src_stride, ref_ptr, ref_stride, 32, h,\n+                         second_pred);\n+}\n+\n+uint32_t vpx_sad32x16_avg_neon(const uint8_t *src, int src_stride, const uint8_t *ref, int ref_stride, const uint8_t *second_pred) { return sad32xh_avg_neon(src, src_stride, ref, ref_stride, (16), second_pred); }"}]}