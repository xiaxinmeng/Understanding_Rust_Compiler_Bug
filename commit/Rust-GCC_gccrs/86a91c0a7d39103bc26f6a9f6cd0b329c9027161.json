{"sha": "86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODZhOTFjMGE3ZDM5MTAzYmMyNmY2YTlmNmNkMGIzMjljOTAyNzE2MQ==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2018-07-31T14:24:27Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2018-07-31T14:24:27Z"}, "message": "[28/46] Use stmt_vec_info instead of gimple stmts internally (part 1)\n\nThis first part makes functions use stmt_vec_infos instead of\ngimple stmts in cases where the stmt_vec_info was already available\nand where the change is mechanical.  Most of it is just replacing\n\"stmt\" with \"stmt_info\".\n\n2018-07-31  Richard Sandiford  <richard.sandiford@arm.com>\n\ngcc/\n\t* tree-vect-data-refs.c (vect_slp_analyze_node_dependences):\n\t(vect_check_gather_scatter, vect_create_data_ref_ptr, bump_vector_ptr)\n\t(vect_permute_store_chain, vect_setup_realignment)\n\t(vect_permute_load_chain, vect_shift_permute_load_chain)\n\t(vect_transform_grouped_load): Use stmt_vec_info rather than gimple\n\tstmts internally, and when passing values to other vectorizer routines.\n\t* tree-vect-loop-manip.c (vect_can_advance_ivs_p): Likewise.\n\t* tree-vect-loop.c (vect_analyze_scalar_cycles_1)\n\t(vect_analyze_loop_operations, get_initial_def_for_reduction)\n\t(vect_create_epilog_for_reduction, vectorize_fold_left_reduction)\n\t(vectorizable_reduction, vectorizable_induction)\n\t(vectorizable_live_operation, vect_transform_loop_stmt)\n\t(vect_transform_loop): Likewise.\n\t* tree-vect-patterns.c (vect_reassociating_reduction_p)\n\t(vect_recog_widen_op_pattern, vect_recog_mixed_size_cond_pattern)\n\t(vect_recog_bool_pattern, vect_recog_gather_scatter_pattern): Likewise.\n\t* tree-vect-slp.c (vect_analyze_slp_instance): Likewise.\n\t(vect_slp_analyze_node_operations_1): Likewise.\n\t* tree-vect-stmts.c (vect_mark_relevant, process_use)\n\t(exist_non_indexing_operands_for_use_p, vect_init_vector_1)\n\t(vect_mark_stmts_to_be_vectorized, vect_get_vec_def_for_operand)\n\t(vect_finish_stmt_generation_1, get_group_load_store_type)\n\t(get_load_store_type, vect_build_gather_load_calls)\n\t(vectorizable_bswap, vectorizable_call, vectorizable_simd_clone_call)\n\t(vect_create_vectorized_demotion_stmts, vectorizable_conversion)\n\t(vectorizable_assignment, vectorizable_shift, vectorizable_operation)\n\t(vectorizable_store, vectorizable_load, vectorizable_condition)\n\t(vectorizable_comparison, vect_analyze_stmt, vect_transform_stmt)\n\t(supportable_widening_operation): Likewise.\n\t(vect_get_vector_types_for_stmt): Likewise.\n\t* tree-vectorizer.h (vect_dr_behavior): Likewise.\n\nFrom-SVN: r263143", "tree": {"sha": "ef335fde6747ef17bc19a8761f0a2b7a77801ade", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ef335fde6747ef17bc19a8761f0a2b7a77801ade"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "html_url": "https://github.com/Rust-GCC/gccrs/commit/86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "91987857e694109fa312c9d2d83785aee5fb9c28", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/91987857e694109fa312c9d2d83785aee5fb9c28", "html_url": "https://github.com/Rust-GCC/gccrs/commit/91987857e694109fa312c9d2d83785aee5fb9c28"}], "stats": {"total": 837, "additions": 459, "deletions": 378}, "files": [{"sha": "01573895c575d5cc14c61431eaf717ed009b98ca", "filename": "gcc/ChangeLog", "status": "modified", "additions": 34, "deletions": 0, "changes": 34, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "patch": "@@ -1,3 +1,37 @@\n+2018-07-31  Richard Sandiford  <richard.sandiford@arm.com>\n+\n+\t* tree-vect-data-refs.c (vect_slp_analyze_node_dependences):\n+\t(vect_check_gather_scatter, vect_create_data_ref_ptr, bump_vector_ptr)\n+\t(vect_permute_store_chain, vect_setup_realignment)\n+\t(vect_permute_load_chain, vect_shift_permute_load_chain)\n+\t(vect_transform_grouped_load): Use stmt_vec_info rather than gimple\n+\tstmts internally, and when passing values to other vectorizer routines.\n+\t* tree-vect-loop-manip.c (vect_can_advance_ivs_p): Likewise.\n+\t* tree-vect-loop.c (vect_analyze_scalar_cycles_1)\n+\t(vect_analyze_loop_operations, get_initial_def_for_reduction)\n+\t(vect_create_epilog_for_reduction, vectorize_fold_left_reduction)\n+\t(vectorizable_reduction, vectorizable_induction)\n+\t(vectorizable_live_operation, vect_transform_loop_stmt)\n+\t(vect_transform_loop): Likewise.\n+\t* tree-vect-patterns.c (vect_reassociating_reduction_p)\n+\t(vect_recog_widen_op_pattern, vect_recog_mixed_size_cond_pattern)\n+\t(vect_recog_bool_pattern, vect_recog_gather_scatter_pattern): Likewise.\n+\t* tree-vect-slp.c (vect_analyze_slp_instance): Likewise.\n+\t(vect_slp_analyze_node_operations_1): Likewise.\n+\t* tree-vect-stmts.c (vect_mark_relevant, process_use)\n+\t(exist_non_indexing_operands_for_use_p, vect_init_vector_1)\n+\t(vect_mark_stmts_to_be_vectorized, vect_get_vec_def_for_operand)\n+\t(vect_finish_stmt_generation_1, get_group_load_store_type)\n+\t(get_load_store_type, vect_build_gather_load_calls)\n+\t(vectorizable_bswap, vectorizable_call, vectorizable_simd_clone_call)\n+\t(vect_create_vectorized_demotion_stmts, vectorizable_conversion)\n+\t(vectorizable_assignment, vectorizable_shift, vectorizable_operation)\n+\t(vectorizable_store, vectorizable_load, vectorizable_condition)\n+\t(vectorizable_comparison, vect_analyze_stmt, vect_transform_stmt)\n+\t(supportable_widening_operation): Likewise.\n+\t(vect_get_vector_types_for_stmt): Likewise.\n+\t* tree-vectorizer.h (vect_dr_behavior): Likewise.\n+\n 2018-07-31  Richard Sandiford  <richard.sandiford@arm.com>\n \n \t* tree-vect-data-refs.c (vect_analyze_data_ref_dependence)"}, {"sha": "94584745167d18287b3fa1758fcd88f53433312c", "filename": "gcc/tree-vect-data-refs.c", "status": "modified", "additions": 33, "deletions": 33, "changes": 66, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-data-refs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-data-refs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-data-refs.c?ref=86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "patch": "@@ -712,7 +712,7 @@ vect_slp_analyze_node_dependences (slp_instance instance, slp_tree node,\n \t     been sunk to (and we verify if we can do that as well).  */\n \t  if (gimple_visited_p (stmt))\n \t    {\n-\t      if (stmt != last_store)\n+\t      if (stmt_info != last_store)\n \t\tcontinue;\n \t      unsigned i;\n \t      stmt_vec_info store_info;\n@@ -3666,7 +3666,7 @@ vect_check_gather_scatter (gimple *stmt, loop_vec_info loop_vinfo,\n \n   /* See whether this is already a call to a gather/scatter internal function.\n      If not, see whether it's a masked load or store.  */\n-  gcall *call = dyn_cast <gcall *> (stmt);\n+  gcall *call = dyn_cast <gcall *> (stmt_info->stmt);\n   if (call && gimple_call_internal_p (call))\n     {\n       ifn = gimple_call_internal_fn (call);\n@@ -4677,8 +4677,8 @@ vect_create_data_ref_ptr (gimple *stmt, tree aggr_type, struct loop *at_loop,\n   if (loop_vinfo)\n     {\n       loop = LOOP_VINFO_LOOP (loop_vinfo);\n-      nested_in_vect_loop = nested_in_vect_loop_p (loop, stmt);\n-      containing_loop = (gimple_bb (stmt))->loop_father;\n+      nested_in_vect_loop = nested_in_vect_loop_p (loop, stmt_info);\n+      containing_loop = (gimple_bb (stmt_info->stmt))->loop_father;\n       pe = loop_preheader_edge (loop);\n     }\n   else\n@@ -4786,7 +4786,7 @@ vect_create_data_ref_ptr (gimple *stmt, tree aggr_type, struct loop *at_loop,\n \n   /* Create: (&(base[init_val+offset]+byte_offset) in the loop preheader.  */\n \n-  new_temp = vect_create_addr_base_for_vector_ref (stmt, &new_stmt_list,\n+  new_temp = vect_create_addr_base_for_vector_ref (stmt_info, &new_stmt_list,\n \t\t\t\t\t\t   offset, byte_offset);\n   if (new_stmt_list)\n     {\n@@ -4934,7 +4934,7 @@ bump_vector_ptr (tree dataref_ptr, gimple *ptr_incr, gimple_stmt_iterator *gsi,\n     new_dataref_ptr = make_ssa_name (TREE_TYPE (dataref_ptr));\n   incr_stmt = gimple_build_assign (new_dataref_ptr, POINTER_PLUS_EXPR,\n \t\t\t\t   dataref_ptr, update);\n-  vect_finish_stmt_generation (stmt, incr_stmt, gsi);\n+  vect_finish_stmt_generation (stmt_info, incr_stmt, gsi);\n \n   /* Copy the points-to information if it exists. */\n   if (DR_PTR_INFO (dr))\n@@ -5282,7 +5282,7 @@ vect_permute_store_chain (vec<tree> dr_chain,\n \t  data_ref = make_temp_ssa_name (vectype, NULL, \"vect_shuffle3_low\");\n \t  perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR, vect1,\n \t\t\t\t\t   vect2, perm3_mask_low);\n-\t  vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t  vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \n \t  vect1 = data_ref;\n \t  vect2 = dr_chain[2];\n@@ -5293,7 +5293,7 @@ vect_permute_store_chain (vec<tree> dr_chain,\n \t  data_ref = make_temp_ssa_name (vectype, NULL, \"vect_shuffle3_high\");\n \t  perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR, vect1,\n \t\t\t\t\t   vect2, perm3_mask_high);\n-\t  vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t  vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t  (*result_chain)[j] = data_ref;\n \t}\n     }\n@@ -5332,7 +5332,7 @@ vect_permute_store_chain (vec<tree> dr_chain,\n \t\thigh = make_temp_ssa_name (vectype, NULL, \"vect_inter_high\");\n \t\tperm_stmt = gimple_build_assign (high, VEC_PERM_EXPR, vect1,\n \t\t\t\t\t\t vect2, perm_mask_high);\n-\t\tvect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t\tvect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t\t(*result_chain)[2*j] = high;\n \n \t\t/* Create interleaving stmt:\n@@ -5342,7 +5342,7 @@ vect_permute_store_chain (vec<tree> dr_chain,\n \t\tlow = make_temp_ssa_name (vectype, NULL, \"vect_inter_low\");\n \t\tperm_stmt = gimple_build_assign (low, VEC_PERM_EXPR, vect1,\n \t\t\t\t\t\t vect2, perm_mask_low);\n-\t\tvect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t\tvect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t\t(*result_chain)[2*j+1] = low;\n \t      }\n \t    memcpy (dr_chain.address (), result_chain->address (),\n@@ -5415,7 +5415,7 @@ vect_setup_realignment (gimple *stmt, gimple_stmt_iterator *gsi,\n   struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n   struct loop *loop = NULL;\n   edge pe = NULL;\n-  tree scalar_dest = gimple_assign_lhs (stmt);\n+  tree scalar_dest = gimple_assign_lhs (stmt_info->stmt);\n   tree vec_dest;\n   gimple *inc;\n   tree ptr;\n@@ -5429,13 +5429,13 @@ vect_setup_realignment (gimple *stmt, gimple_stmt_iterator *gsi,\n   bool inv_p;\n   bool compute_in_loop = false;\n   bool nested_in_vect_loop = false;\n-  struct loop *containing_loop = (gimple_bb (stmt))->loop_father;\n+  struct loop *containing_loop = (gimple_bb (stmt_info->stmt))->loop_father;\n   struct loop *loop_for_initial_load = NULL;\n \n   if (loop_vinfo)\n     {\n       loop = LOOP_VINFO_LOOP (loop_vinfo);\n-      nested_in_vect_loop = nested_in_vect_loop_p (loop, stmt);\n+      nested_in_vect_loop = nested_in_vect_loop_p (loop, stmt_info);\n     }\n \n   gcc_assert (alignment_support_scheme == dr_explicit_realign\n@@ -5518,9 +5518,9 @@ vect_setup_realignment (gimple *stmt, gimple_stmt_iterator *gsi,\n \n       gcc_assert (!compute_in_loop);\n       vec_dest = vect_create_destination_var (scalar_dest, vectype);\n-      ptr = vect_create_data_ref_ptr (stmt, vectype, loop_for_initial_load,\n-\t\t\t\t      NULL_TREE, &init_addr, NULL, &inc,\n-\t\t\t\t      true, &inv_p);\n+      ptr = vect_create_data_ref_ptr (stmt_info, vectype,\n+\t\t\t\t      loop_for_initial_load, NULL_TREE,\n+\t\t\t\t      &init_addr, NULL, &inc, true, &inv_p);\n       if (TREE_CODE (ptr) == SSA_NAME)\n \tnew_temp = copy_ssa_name (ptr);\n       else\n@@ -5562,7 +5562,7 @@ vect_setup_realignment (gimple *stmt, gimple_stmt_iterator *gsi,\n       if (!init_addr)\n \t{\n \t  /* Generate the INIT_ADDR computation outside LOOP.  */\n-\t  init_addr = vect_create_addr_base_for_vector_ref (stmt, &stmts,\n+\t  init_addr = vect_create_addr_base_for_vector_ref (stmt_info, &stmts,\n \t\t\t\t\t\t\t    NULL_TREE);\n           if (loop)\n             {\n@@ -5890,7 +5890,7 @@ vect_permute_load_chain (vec<tree> dr_chain,\n \t  data_ref = make_temp_ssa_name (vectype, NULL, \"vect_shuffle3_low\");\n \t  perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR, first_vect,\n \t\t\t\t\t   second_vect, perm3_mask_low);\n-\t  vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t  vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \n \t  /* Create interleaving stmt (high part of):\n \t     high = VEC_PERM_EXPR <first_vect, second_vect2, {k, 3 + k, 6 + k,\n@@ -5900,7 +5900,7 @@ vect_permute_load_chain (vec<tree> dr_chain,\n \t  data_ref = make_temp_ssa_name (vectype, NULL, \"vect_shuffle3_high\");\n \t  perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR, first_vect,\n \t\t\t\t\t   second_vect, perm3_mask_high);\n-\t  vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t  vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t  (*result_chain)[k] = data_ref;\n \t}\n     }\n@@ -5935,15 +5935,15 @@ vect_permute_load_chain (vec<tree> dr_chain,\n \t      perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR,\n \t\t\t\t\t       first_vect, second_vect,\n \t\t\t\t\t       perm_mask_even);\n-\t      vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t      vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t      (*result_chain)[j/2] = data_ref;\n \n \t      /* data_ref = permute_odd (first_data_ref, second_data_ref);  */\n \t      data_ref = make_temp_ssa_name (vectype, NULL, \"vect_perm_odd\");\n \t      perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR,\n \t\t\t\t\t       first_vect, second_vect,\n \t\t\t\t\t       perm_mask_odd);\n-\t      vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t      vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t      (*result_chain)[j/2+length/2] = data_ref;\n \t    }\n \t  memcpy (dr_chain.address (), result_chain->address (),\n@@ -6143,26 +6143,26 @@ vect_shift_permute_load_chain (vec<tree> dr_chain,\n \t      perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR,\n \t\t\t\t\t       first_vect, first_vect,\n \t\t\t\t\t       perm2_mask1);\n-\t      vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t      vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t      vect[0] = data_ref;\n \n \t      data_ref = make_temp_ssa_name (vectype, NULL, \"vect_shuffle2\");\n \t      perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR,\n \t\t\t\t\t       second_vect, second_vect,\n \t\t\t\t\t       perm2_mask2);\n-\t      vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t      vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t      vect[1] = data_ref;\n \n \t      data_ref = make_temp_ssa_name (vectype, NULL, \"vect_shift\");\n \t      perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR,\n \t\t\t\t\t       vect[0], vect[1], shift1_mask);\n-\t      vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t      vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t      (*result_chain)[j/2 + length/2] = data_ref;\n \n \t      data_ref = make_temp_ssa_name (vectype, NULL, \"vect_select\");\n \t      perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR,\n \t\t\t\t\t       vect[0], vect[1], select_mask);\n-\t      vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t      vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t      (*result_chain)[j/2] = data_ref;\n \t    }\n \t  memcpy (dr_chain.address (), result_chain->address (),\n@@ -6259,7 +6259,7 @@ vect_shift_permute_load_chain (vec<tree> dr_chain,\n \t  perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR,\n \t\t\t\t\t   dr_chain[k], dr_chain[k],\n \t\t\t\t\t   perm3_mask);\n-\t  vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t  vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t  vect[k] = data_ref;\n \t}\n \n@@ -6269,7 +6269,7 @@ vect_shift_permute_load_chain (vec<tree> dr_chain,\n \t  perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR,\n \t\t\t\t\t   vect[k % 3], vect[(k + 1) % 3],\n \t\t\t\t\t   shift1_mask);\n-\t  vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t  vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t  vect_shift[k] = data_ref;\n \t}\n \n@@ -6280,7 +6280,7 @@ vect_shift_permute_load_chain (vec<tree> dr_chain,\n \t\t\t\t\t   vect_shift[(4 - k) % 3],\n \t\t\t\t\t   vect_shift[(3 - k) % 3],\n \t\t\t\t\t   shift2_mask);\n-\t  vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t  vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \t  vect[k] = data_ref;\n \t}\n \n@@ -6289,13 +6289,13 @@ vect_shift_permute_load_chain (vec<tree> dr_chain,\n       data_ref = make_temp_ssa_name (vectype, NULL, \"vect_shift3\");\n       perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR, vect[0],\n \t\t\t\t       vect[0], shift3_mask);\n-      vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+      vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n       (*result_chain)[nelt % 3] = data_ref;\n \n       data_ref = make_temp_ssa_name (vectype, NULL, \"vect_shift4\");\n       perm_stmt = gimple_build_assign (data_ref, VEC_PERM_EXPR, vect[1],\n \t\t\t\t       vect[1], shift4_mask);\n-      vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+      vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n       (*result_chain)[0] = data_ref;\n       return true;\n     }\n@@ -6328,10 +6328,10 @@ vect_transform_grouped_load (gimple *stmt, vec<tree> dr_chain, int size,\n   mode = TYPE_MODE (STMT_VINFO_VECTYPE (stmt_info));\n   if (targetm.sched.reassociation_width (VEC_PERM_EXPR, mode) > 1\n       || pow2p_hwi (size)\n-      || !vect_shift_permute_load_chain (dr_chain, size, stmt,\n+      || !vect_shift_permute_load_chain (dr_chain, size, stmt_info,\n \t\t\t\t\t gsi, &result_chain))\n-    vect_permute_load_chain (dr_chain, size, stmt, gsi, &result_chain);\n-  vect_record_grouped_load_vectors (stmt, result_chain);\n+    vect_permute_load_chain (dr_chain, size, stmt_info, gsi, &result_chain);\n+  vect_record_grouped_load_vectors (stmt_info, result_chain);\n   result_chain.release ();\n }\n "}, {"sha": "f065854b7fdd8f67f2b04121ae8c5c600f270757", "filename": "gcc/tree-vect-loop-manip.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-loop-manip.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-loop-manip.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop-manip.c?ref=86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "patch": "@@ -1380,8 +1380,8 @@ vect_can_advance_ivs_p (loop_vec_info loop_vinfo)\n       stmt_vec_info phi_info = loop_vinfo->lookup_stmt (phi);\n       if (dump_enabled_p ())\n \t{\n-          dump_printf_loc (MSG_NOTE, vect_location, \"Analyze phi: \");\n-          dump_gimple_stmt (MSG_NOTE, TDF_SLIM, phi, 0);\n+\t  dump_printf_loc (MSG_NOTE, vect_location, \"Analyze phi: \");\n+\t  dump_gimple_stmt (MSG_NOTE, TDF_SLIM, phi_info->stmt, 0);\n \t}\n \n       /* Skip virtual phi's. The data dependences that are associated with"}, {"sha": "18201f572345a4919ec364d7611c0f00a3f75de1", "filename": "gcc/tree-vect-loop.c", "status": "modified", "additions": 51, "deletions": 46, "changes": 97, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-loop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-loop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop.c?ref=86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "patch": "@@ -526,7 +526,7 @@ vect_analyze_scalar_cycles_1 (loop_vec_info loop_vinfo, struct loop *loop)\n \t  || (LOOP_VINFO_LOOP (loop_vinfo) != loop\n \t      && TREE_CODE (step) != INTEGER_CST))\n \t{\n-\t  worklist.safe_push (phi);\n+\t  worklist.safe_push (stmt_vinfo);\n \t  continue;\n \t}\n \n@@ -1595,19 +1595,20 @@ vect_analyze_loop_operations (loop_vec_info loop_vinfo)\n               need_to_vectorize = true;\n               if (STMT_VINFO_DEF_TYPE (stmt_info) == vect_induction_def\n \t\t  && ! PURE_SLP_STMT (stmt_info))\n-                ok = vectorizable_induction (phi, NULL, NULL, NULL, &cost_vec);\n+\t\tok = vectorizable_induction (stmt_info, NULL, NULL, NULL,\n+\t\t\t\t\t     &cost_vec);\n \t      else if ((STMT_VINFO_DEF_TYPE (stmt_info) == vect_reduction_def\n \t\t\t|| STMT_VINFO_DEF_TYPE (stmt_info) == vect_nested_cycle)\n \t\t       && ! PURE_SLP_STMT (stmt_info))\n-\t\tok = vectorizable_reduction (phi, NULL, NULL, NULL, NULL,\n+\t\tok = vectorizable_reduction (stmt_info, NULL, NULL, NULL, NULL,\n \t\t\t\t\t     &cost_vec);\n             }\n \n \t  /* SLP PHIs are tested by vect_slp_analyze_node_operations.  */\n \t  if (ok\n \t      && STMT_VINFO_LIVE_P (stmt_info)\n \t      && !PURE_SLP_STMT (stmt_info))\n-\t    ok = vectorizable_live_operation (phi, NULL, NULL, -1, NULL,\n+\t    ok = vectorizable_live_operation (stmt_info, NULL, NULL, -1, NULL,\n \t\t\t\t\t      &cost_vec);\n \n           if (!ok)\n@@ -4045,7 +4046,7 @@ get_initial_def_for_reduction (gimple *stmt, tree init_val,\n   struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n   tree scalar_type = TREE_TYPE (init_val);\n   tree vectype = get_vectype_for_scalar_type (scalar_type);\n-  enum tree_code code = gimple_assign_rhs_code (stmt);\n+  enum tree_code code = gimple_assign_rhs_code (stmt_vinfo->stmt);\n   tree def_for_init;\n   tree init_def;\n   REAL_VALUE_TYPE real_init_val = dconst0;\n@@ -4057,8 +4058,8 @@ get_initial_def_for_reduction (gimple *stmt, tree init_val,\n   gcc_assert (POINTER_TYPE_P (scalar_type) || INTEGRAL_TYPE_P (scalar_type)\n \t      || SCALAR_FLOAT_TYPE_P (scalar_type));\n \n-  gcc_assert (nested_in_vect_loop_p (loop, stmt)\n-\t      || loop == (gimple_bb (stmt))->loop_father);\n+  gcc_assert (nested_in_vect_loop_p (loop, stmt_vinfo)\n+\t      || loop == (gimple_bb (stmt_vinfo->stmt))->loop_father);\n \n   vect_reduction_type reduction_type\n     = STMT_VINFO_VEC_REDUCTION_TYPE (stmt_vinfo);\n@@ -4127,7 +4128,7 @@ get_initial_def_for_reduction (gimple *stmt, tree init_val,\n \t    if (reduction_type != COND_REDUCTION\n \t\t&& reduction_type != EXTRACT_LAST_REDUCTION)\n \t      {\n-\t\tinit_def = vect_get_vec_def_for_operand (init_val, stmt);\n+\t\tinit_def = vect_get_vec_def_for_operand (init_val, stmt_vinfo);\n \t\tbreak;\n \t      }\n \t  }\n@@ -4406,7 +4407,7 @@ vect_create_epilog_for_reduction (vec<tree> vect_defs, gimple *stmt,\n   tree vec_dest;\n   tree new_temp = NULL_TREE, new_dest, new_name, new_scalar_dest;\n   gimple *epilog_stmt = NULL;\n-  enum tree_code code = gimple_assign_rhs_code (stmt);\n+  enum tree_code code = gimple_assign_rhs_code (stmt_info->stmt);\n   gimple *exit_phi;\n   tree bitsize;\n   tree adjustment_def = NULL;\n@@ -4435,7 +4436,7 @@ vect_create_epilog_for_reduction (vec<tree> vect_defs, gimple *stmt,\n   if (slp_node)\n     group_size = SLP_TREE_SCALAR_STMTS (slp_node).length (); \n \n-  if (nested_in_vect_loop_p (loop, stmt))\n+  if (nested_in_vect_loop_p (loop, stmt_info))\n     {\n       outer_loop = loop;\n       loop = loop->inner;\n@@ -4504,11 +4505,13 @@ vect_create_epilog_for_reduction (vec<tree> vect_defs, gimple *stmt,\n \t  /* Do not use an adjustment def as that case is not supported\n \t     correctly if ncopies is not one.  */\n \t  vect_is_simple_use (initial_def, loop_vinfo, &initial_def_dt);\n-\t  vec_initial_def = vect_get_vec_def_for_operand (initial_def, stmt);\n+\t  vec_initial_def = vect_get_vec_def_for_operand (initial_def,\n+\t\t\t\t\t\t\t  stmt_info);\n \t}\n       else\n-\tvec_initial_def = get_initial_def_for_reduction (stmt, initial_def,\n-\t\t\t\t\t\t\t &adjustment_def);\n+\tvec_initial_def\n+\t  = get_initial_def_for_reduction (stmt_info, initial_def,\n+\t\t\t\t\t   &adjustment_def);\n       vec_initial_defs.create (1);\n       vec_initial_defs.quick_push (vec_initial_def);\n     }\n@@ -5676,7 +5679,7 @@ vect_create_epilog_for_reduction (vec<tree> vect_defs, gimple *stmt,\n                   preheader_arg = PHI_ARG_DEF_FROM_EDGE (use_stmt,\n                                              loop_preheader_edge (outer_loop));\n                   vect_phi_init = get_initial_def_for_reduction\n-\t\t    (stmt, preheader_arg, NULL);\n+\t\t    (stmt_info, preheader_arg, NULL);\n \n                   /* Update phi node arguments with vs0 and vs2.  */\n                   add_phi_arg (vect_phi, vect_phi_init,\n@@ -5841,7 +5844,7 @@ vectorize_fold_left_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n   else\n     ncopies = vect_get_num_copies (loop_vinfo, vectype_in);\n \n-  gcc_assert (!nested_in_vect_loop_p (loop, stmt));\n+  gcc_assert (!nested_in_vect_loop_p (loop, stmt_info));\n   gcc_assert (ncopies == 1);\n   gcc_assert (TREE_CODE_LENGTH (code) == binary_op);\n   gcc_assert (reduc_index == (code == MINUS_EXPR ? 0 : 1));\n@@ -5859,13 +5862,14 @@ vectorize_fold_left_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n   auto_vec<tree> vec_oprnds0;\n   if (slp_node)\n     {\n-      vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL, slp_node);\n+      vect_get_vec_defs (op0, NULL_TREE, stmt_info, &vec_oprnds0, NULL,\n+\t\t\t slp_node);\n       group_size = SLP_TREE_SCALAR_STMTS (slp_node).length ();\n       scalar_dest_def_info = SLP_TREE_SCALAR_STMTS (slp_node)[group_size - 1];\n     }\n   else\n     {\n-      tree loop_vec_def0 = vect_get_vec_def_for_operand (op0, stmt);\n+      tree loop_vec_def0 = vect_get_vec_def_for_operand (op0, stmt_info);\n       vec_oprnds0.create (1);\n       vec_oprnds0.quick_push (loop_vec_def0);\n       scalar_dest_def_info = stmt_info;\n@@ -6099,7 +6103,7 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n       && STMT_VINFO_DEF_TYPE (stmt_info) != vect_nested_cycle)\n     return false;\n \n-  if (nested_in_vect_loop_p (loop, stmt))\n+  if (nested_in_vect_loop_p (loop, stmt_info))\n     {\n       loop = loop->inner;\n       nested_cycle = true;\n@@ -6109,7 +6113,7 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n     gcc_assert (slp_node\n \t\t&& REDUC_GROUP_FIRST_ELEMENT (stmt_info) == stmt_info);\n \n-  if (gphi *phi = dyn_cast <gphi *> (stmt))\n+  if (gphi *phi = dyn_cast <gphi *> (stmt_info->stmt))\n     {\n       tree phi_result = gimple_phi_result (phi);\n       /* Analysis is fully done on the reduction stmt invocation.  */\n@@ -6164,7 +6168,7 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n \t  && STMT_VINFO_RELEVANT (reduc_stmt_info) <= vect_used_only_live\n \t  && (use_stmt_info = loop_vinfo->lookup_single_use (phi_result))\n \t  && (use_stmt_info == reduc_stmt_info\n-\t      || STMT_VINFO_RELATED_STMT (use_stmt_info) == reduc_stmt))\n+\t      || STMT_VINFO_RELATED_STMT (use_stmt_info) == reduc_stmt_info))\n \tsingle_defuse_cycle = true;\n \n       /* Create the destination vector  */\n@@ -6548,7 +6552,7 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n     {\n       /* Only call during the analysis stage, otherwise we'll lose\n \t STMT_VINFO_TYPE.  */\n-      if (!vec_stmt && !vectorizable_condition (stmt, gsi, NULL,\n+      if (!vec_stmt && !vectorizable_condition (stmt_info, gsi, NULL,\n \t\t\t\t\t\tops[reduc_index], 0, NULL,\n \t\t\t\t\t\tcost_vec))\n         {\n@@ -6935,7 +6939,7 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n       && (STMT_VINFO_RELEVANT (stmt_info) <= vect_used_only_live)\n       && (use_stmt_info = loop_vinfo->lookup_single_use (reduc_phi_result))\n       && (use_stmt_info == stmt_info\n-\t  || STMT_VINFO_RELATED_STMT (use_stmt_info) == stmt))\n+\t  || STMT_VINFO_RELATED_STMT (use_stmt_info) == stmt_info))\n     {\n       single_defuse_cycle = true;\n       epilog_copies = 1;\n@@ -7015,13 +7019,13 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n \n   if (reduction_type == FOLD_LEFT_REDUCTION)\n     return vectorize_fold_left_reduction\n-      (stmt, gsi, vec_stmt, slp_node, reduc_def_phi, code,\n+      (stmt_info, gsi, vec_stmt, slp_node, reduc_def_phi, code,\n        reduc_fn, ops, vectype_in, reduc_index, masks);\n \n   if (reduction_type == EXTRACT_LAST_REDUCTION)\n     {\n       gcc_assert (!slp_node);\n-      return vectorizable_condition (stmt, gsi, vec_stmt,\n+      return vectorizable_condition (stmt_info, gsi, vec_stmt,\n \t\t\t\t     NULL, reduc_index, NULL, NULL);\n     }\n \n@@ -7053,7 +7057,7 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n       if (code == COND_EXPR)\n         {\n           gcc_assert (!slp_node);\n-\t  vectorizable_condition (stmt, gsi, vec_stmt,\n+\t  vectorizable_condition (stmt_info, gsi, vec_stmt,\n \t\t\t\t  PHI_RESULT (phis[0]->stmt),\n \t\t\t\t  reduc_index, NULL, NULL);\n           /* Multiple types are not supported for condition.  */\n@@ -7090,12 +7094,12 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n           else\n \t    {\n               vec_oprnds0.quick_push\n-\t\t(vect_get_vec_def_for_operand (ops[0], stmt));\n+\t\t(vect_get_vec_def_for_operand (ops[0], stmt_info));\n               vec_oprnds1.quick_push\n-\t\t(vect_get_vec_def_for_operand (ops[1], stmt));\n+\t\t(vect_get_vec_def_for_operand (ops[1], stmt_info));\n               if (op_type == ternary_op)\n \t\tvec_oprnds2.quick_push \n-\t\t  (vect_get_vec_def_for_operand (ops[2], stmt));\n+\t\t  (vect_get_vec_def_for_operand (ops[2], stmt_info));\n \t    }\n         }\n       else\n@@ -7144,7 +7148,8 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      new_temp = make_ssa_name (vec_dest, call);\n \t      gimple_call_set_lhs (call, new_temp);\n \t      gimple_call_set_nothrow (call, true);\n-\t      new_stmt_info = vect_finish_stmt_generation (stmt, call, gsi);\n+\t      new_stmt_info\n+\t\t= vect_finish_stmt_generation (stmt_info, call, gsi);\n \t    }\n \t  else\n \t    {\n@@ -7156,7 +7161,7 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      new_temp = make_ssa_name (vec_dest, new_stmt);\n \t      gimple_assign_set_lhs (new_stmt, new_temp);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t    }\n \n           if (slp_node)\n@@ -7184,7 +7189,7 @@ vectorizable_reduction (gimple *stmt, gimple_stmt_iterator *gsi,\n   if ((!single_defuse_cycle || code == COND_EXPR) && !slp_node)\n     vect_defs[0] = gimple_get_lhs ((*vec_stmt)->stmt);\n \n-  vect_create_epilog_for_reduction (vect_defs, stmt, reduc_def_phi,\n+  vect_create_epilog_for_reduction (vect_defs, stmt_info, reduc_def_phi,\n \t\t\t\t    epilog_copies, reduc_fn, phis,\n \t\t\t\t    double_reduc, slp_node, slp_node_instance,\n \t\t\t\t    cond_reduc_val, cond_reduc_op_code,\n@@ -7293,7 +7298,7 @@ vectorizable_induction (gimple *phi,\n   gcc_assert (ncopies >= 1);\n \n   /* FORNOW. These restrictions should be relaxed.  */\n-  if (nested_in_vect_loop_p (loop, phi))\n+  if (nested_in_vect_loop_p (loop, stmt_info))\n     {\n       imm_use_iterator imm_iter;\n       use_operand_p use_p;\n@@ -7443,10 +7448,10 @@ vectorizable_induction (gimple *phi,\n       new_name = fold_build2 (MULT_EXPR, TREE_TYPE (step_expr),\n \t\t\t      expr, step_expr);\n       if (! CONSTANT_CLASS_P (new_name))\n-\tnew_name = vect_init_vector (phi, new_name,\n+\tnew_name = vect_init_vector (stmt_info, new_name,\n \t\t\t\t     TREE_TYPE (step_expr), NULL);\n       new_vec = build_vector_from_val (vectype, new_name);\n-      vec_step = vect_init_vector (phi, new_vec, vectype, NULL);\n+      vec_step = vect_init_vector (stmt_info, new_vec, vectype, NULL);\n \n       /* Now generate the IVs.  */\n       unsigned group_size = SLP_TREE_SCALAR_STMTS (slp_node).length ();\n@@ -7513,10 +7518,10 @@ vectorizable_induction (gimple *phi,\n \t  new_name = fold_build2 (MULT_EXPR, TREE_TYPE (step_expr),\n \t\t\t\t  expr, step_expr);\n \t  if (! CONSTANT_CLASS_P (new_name))\n-\t    new_name = vect_init_vector (phi, new_name,\n+\t    new_name = vect_init_vector (stmt_info, new_name,\n \t\t\t\t\t TREE_TYPE (step_expr), NULL);\n \t  new_vec = build_vector_from_val (vectype, new_name);\n-\t  vec_step = vect_init_vector (phi, new_vec, vectype, NULL);\n+\t  vec_step = vect_init_vector (stmt_info, new_vec, vectype, NULL);\n \t  for (; ivn < nvects; ++ivn)\n \t    {\n \t      gimple *iv = SLP_TREE_VEC_STMTS (slp_node)[ivn - nivs]->stmt;\n@@ -7549,7 +7554,7 @@ vectorizable_induction (gimple *phi,\n       /* iv_loop is nested in the loop to be vectorized.  init_expr had already\n \t been created during vectorization of previous stmts.  We obtain it\n \t from the STMT_VINFO_VEC_STMT of the defining stmt.  */\n-      vec_init = vect_get_vec_def_for_operand (init_expr, phi);\n+      vec_init = vect_get_vec_def_for_operand (init_expr, stmt_info);\n       /* If the initial value is not of proper type, convert it.  */\n       if (!useless_type_conversion_p (vectype, TREE_TYPE (vec_init)))\n \t{\n@@ -7651,7 +7656,7 @@ vectorizable_induction (gimple *phi,\n   gcc_assert (CONSTANT_CLASS_P (new_name)\n \t      || TREE_CODE (new_name) == SSA_NAME);\n   new_vec = build_vector_from_val (vectype, t);\n-  vec_step = vect_init_vector (phi, new_vec, vectype, NULL);\n+  vec_step = vect_init_vector (stmt_info, new_vec, vectype, NULL);\n \n \n   /* Create the following def-use cycle:\n@@ -7717,7 +7722,7 @@ vectorizable_induction (gimple *phi,\n       gcc_assert (CONSTANT_CLASS_P (new_name)\n \t\t  || TREE_CODE (new_name) == SSA_NAME);\n       new_vec = build_vector_from_val (vectype, t);\n-      vec_step = vect_init_vector (phi, new_vec, vectype, NULL);\n+      vec_step = vect_init_vector (stmt_info, new_vec, vectype, NULL);\n \n       vec_def = induc_def;\n       prev_stmt_vinfo = induction_phi_info;\n@@ -7815,15 +7820,15 @@ vectorizable_live_operation (gimple *stmt,\n     return false;\n \n   /* FORNOW.  CHECKME.  */\n-  if (nested_in_vect_loop_p (loop, stmt))\n+  if (nested_in_vect_loop_p (loop, stmt_info))\n     return false;\n \n   /* If STMT is not relevant and it is a simple assignment and its inputs are\n      invariant then it can remain in place, unvectorized.  The original last\n      scalar value that it computes will be used.  */\n   if (!STMT_VINFO_RELEVANT_P (stmt_info))\n     {\n-      gcc_assert (is_simple_and_all_uses_invariant (stmt, loop_vinfo));\n+      gcc_assert (is_simple_and_all_uses_invariant (stmt_info, loop_vinfo));\n       if (dump_enabled_p ())\n \tdump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t \"statement is simple and uses invariant.  Leaving in \"\n@@ -8222,11 +8227,11 @@ vect_transform_loop_stmt (loop_vec_info loop_vinfo, gimple *stmt,\n     {\n       dump_printf_loc (MSG_NOTE, vect_location,\n \t\t       \"------>vectorizing statement: \");\n-      dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt, 0);\n+      dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt_info->stmt, 0);\n     }\n \n   if (MAY_HAVE_DEBUG_BIND_STMTS && !STMT_VINFO_LIVE_P (stmt_info))\n-    vect_loop_kill_debug_uses (loop, stmt);\n+    vect_loop_kill_debug_uses (loop, stmt_info);\n \n   if (!STMT_VINFO_RELEVANT_P (stmt_info)\n       && !STMT_VINFO_LIVE_P (stmt_info))\n@@ -8267,7 +8272,7 @@ vect_transform_loop_stmt (loop_vec_info loop_vinfo, gimple *stmt,\n     dump_printf_loc (MSG_NOTE, vect_location, \"transform statement.\\n\");\n \n   bool grouped_store = false;\n-  if (vect_transform_stmt (stmt, gsi, &grouped_store, NULL, NULL))\n+  if (vect_transform_stmt (stmt_info, gsi, &grouped_store, NULL, NULL))\n     *seen_store = stmt_info;\n }\n \n@@ -8422,7 +8427,7 @@ vect_transform_loop (loop_vec_info loop_vinfo)\n \t    continue;\n \n \t  if (MAY_HAVE_DEBUG_BIND_STMTS && !STMT_VINFO_LIVE_P (stmt_info))\n-\t    vect_loop_kill_debug_uses (loop, phi);\n+\t    vect_loop_kill_debug_uses (loop, stmt_info);\n \n \t  if (!STMT_VINFO_RELEVANT_P (stmt_info)\n \t      && !STMT_VINFO_LIVE_P (stmt_info))\n@@ -8441,7 +8446,7 @@ vect_transform_loop (loop_vec_info loop_vinfo)\n \t    {\n \t      if (dump_enabled_p ())\n \t\tdump_printf_loc (MSG_NOTE, vect_location, \"transform phi.\\n\");\n-\t      vect_transform_stmt (phi, NULL, NULL, NULL, NULL);\n+\t      vect_transform_stmt (stmt_info, NULL, NULL, NULL, NULL);\n \t    }\n \t}\n "}, {"sha": "fa11c6ec684198ef751e85bc34b35b07e755b3ca", "filename": "gcc/tree-vect-patterns.c", "status": "modified", "additions": 13, "deletions": 14, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-patterns.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-patterns.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-patterns.c?ref=86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "patch": "@@ -842,7 +842,7 @@ vect_reassociating_reduction_p (stmt_vec_info stmt_info, tree_code code,\n   /* We don't allow changing the order of the computation in the inner-loop\n      when doing outer-loop vectorization.  */\n   struct loop *loop = LOOP_VINFO_LOOP (loop_info);\n-  if (loop && nested_in_vect_loop_p (loop, assign))\n+  if (loop && nested_in_vect_loop_p (loop, stmt_info))\n     return false;\n \n   if (!vect_reassociating_reduction_p (stmt_info))\n@@ -1196,7 +1196,7 @@ vect_recog_widen_op_pattern (stmt_vec_info last_stmt_info, tree *type_out,\n   auto_vec<tree> dummy_vec;\n   if (!vectype\n       || !vecitype\n-      || !supportable_widening_operation (wide_code, last_stmt,\n+      || !supportable_widening_operation (wide_code, last_stmt_info,\n \t\t\t\t\t  vecitype, vectype,\n \t\t\t\t\t  &dummy_code, &dummy_code,\n \t\t\t\t\t  &dummy_int, &dummy_vec))\n@@ -3118,11 +3118,11 @@ vect_recog_mixed_size_cond_pattern (stmt_vec_info stmt_vinfo, tree *type_out)\n     return NULL;\n \n   if ((TREE_CODE (then_clause) != INTEGER_CST\n-       && !type_conversion_p (then_clause, last_stmt, false, &orig_type0,\n-                              &def_stmt0, &promotion))\n+       && !type_conversion_p (then_clause, stmt_vinfo, false, &orig_type0,\n+\t\t\t      &def_stmt0, &promotion))\n       || (TREE_CODE (else_clause) != INTEGER_CST\n-          && !type_conversion_p (else_clause, last_stmt, false, &orig_type1,\n-                                 &def_stmt1, &promotion)))\n+\t  && !type_conversion_p (else_clause, stmt_vinfo, false, &orig_type1,\n+\t\t\t\t &def_stmt1, &promotion)))\n     return NULL;\n \n   if (orig_type0 && orig_type1\n@@ -3709,7 +3709,7 @@ vect_recog_bool_pattern (stmt_vec_info stmt_vinfo, tree *type_out)\n \n       if (check_bool_pattern (var, vinfo, bool_stmts))\n \t{\n-\t  rhs = adjust_bool_stmts (bool_stmts, TREE_TYPE (lhs), last_stmt);\n+\t  rhs = adjust_bool_stmts (bool_stmts, TREE_TYPE (lhs), stmt_vinfo);\n \t  lhs = vect_recog_temp_ssa_var (TREE_TYPE (lhs), NULL);\n \t  if (useless_type_conversion_p (TREE_TYPE (lhs), TREE_TYPE (rhs)))\n \t    pattern_stmt = gimple_build_assign (lhs, SSA_NAME, rhs);\n@@ -3776,7 +3776,7 @@ vect_recog_bool_pattern (stmt_vec_info stmt_vinfo, tree *type_out)\n       if (!check_bool_pattern (var, vinfo, bool_stmts))\n \treturn NULL;\n \n-      rhs = adjust_bool_stmts (bool_stmts, type, last_stmt);\n+      rhs = adjust_bool_stmts (bool_stmts, type, stmt_vinfo);\n \n       lhs = vect_recog_temp_ssa_var (TREE_TYPE (lhs), NULL);\n       pattern_stmt \n@@ -3800,7 +3800,7 @@ vect_recog_bool_pattern (stmt_vec_info stmt_vinfo, tree *type_out)\n \treturn NULL;\n \n       if (check_bool_pattern (var, vinfo, bool_stmts))\n-\trhs = adjust_bool_stmts (bool_stmts, TREE_TYPE (vectype), last_stmt);\n+\trhs = adjust_bool_stmts (bool_stmts, TREE_TYPE (vectype), stmt_vinfo);\n       else\n \t{\n \t  tree type = search_type_for_mask (var, vinfo);\n@@ -4234,13 +4234,12 @@ vect_recog_gather_scatter_pattern (stmt_vec_info stmt_info, tree *type_out)\n \n   /* Get the boolean that controls whether the load or store happens.\n      This is null if the operation is unconditional.  */\n-  gimple *stmt = stmt_info->stmt;\n-  tree mask = vect_get_load_store_mask (stmt);\n+  tree mask = vect_get_load_store_mask (stmt_info);\n \n   /* Make sure that the target supports an appropriate internal\n      function for the gather/scatter operation.  */\n   gather_scatter_info gs_info;\n-  if (!vect_check_gather_scatter (stmt, loop_vinfo, &gs_info)\n+  if (!vect_check_gather_scatter (stmt_info, loop_vinfo, &gs_info)\n       || gs_info.decl)\n     return NULL;\n \n@@ -4273,7 +4272,7 @@ vect_recog_gather_scatter_pattern (stmt_vec_info stmt_info, tree *type_out)\n     }\n   else\n     {\n-      tree rhs = vect_get_store_rhs (stmt);\n+      tree rhs = vect_get_store_rhs (stmt_info);\n       if (mask != NULL)\n \tpattern_stmt = gimple_build_call_internal (IFN_MASK_SCATTER_STORE, 5,\n \t\t\t\t\t\t   base, offset, scale, rhs,\n@@ -4295,7 +4294,7 @@ vect_recog_gather_scatter_pattern (stmt_vec_info stmt_info, tree *type_out)\n \n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n   *type_out = vectype;\n-  vect_pattern_detected (\"gather/scatter pattern\", stmt);\n+  vect_pattern_detected (\"gather/scatter pattern\", stmt_info->stmt);\n \n   return pattern_stmt;\n }"}, {"sha": "0d7069aa2ce979c11c268e64943ebdc82846e814", "filename": "gcc/tree-vect-slp.c", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-slp.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-slp.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-slp.c?ref=86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "patch": "@@ -2096,8 +2096,8 @@ vect_analyze_slp_instance (vec_info *vinfo,\n                   dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n \t\t\t\t   \"Build SLP failed: unsupported load \"\n \t\t\t\t   \"permutation \");\n-\t\t      dump_gimple_stmt (MSG_MISSED_OPTIMIZATION,\n-\t\t\t\t\tTDF_SLIM, stmt, 0);\n+\t\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION,\n+\t\t\t\t    TDF_SLIM, stmt_info->stmt, 0);\n                 }\n \t      vect_free_slp_instance (new_instance, false);\n               return false;\n@@ -2172,8 +2172,9 @@ vect_analyze_slp_instance (vec_info *vinfo,\n \t  gcc_assert ((const_nunits & (const_nunits - 1)) == 0);\n \t  unsigned group1_size = i & ~(const_nunits - 1);\n \n-\t  gimple *rest = vect_split_slp_store_group (stmt, group1_size);\n-\t  bool res = vect_analyze_slp_instance (vinfo, stmt, max_tree_size);\n+\t  gimple *rest = vect_split_slp_store_group (stmt_info, group1_size);\n+\t  bool res = vect_analyze_slp_instance (vinfo, stmt_info,\n+\t\t\t\t\t\tmax_tree_size);\n \t  /* If the first non-match was in the middle of a vector,\n \t     skip the rest of that vector.  */\n \t  if (group1_size < i)\n@@ -2513,7 +2514,6 @@ vect_slp_analyze_node_operations_1 (vec_info *vinfo, slp_tree node,\n \t\t\t\t    stmt_vector_for_cost *cost_vec)\n {\n   stmt_vec_info stmt_info = SLP_TREE_SCALAR_STMTS (node)[0];\n-  gimple *stmt = stmt_info->stmt;\n   gcc_assert (STMT_SLP_TYPE (stmt_info) != loop_vect);\n \n   /* For BB vectorization vector types are assigned here.\n@@ -2567,7 +2567,7 @@ vect_slp_analyze_node_operations_1 (vec_info *vinfo, slp_tree node,\n     }\n \n   bool dummy;\n-  return vect_analyze_stmt (stmt, &dummy, node, node_instance, cost_vec);\n+  return vect_analyze_stmt (stmt_info, &dummy, node, node_instance, cost_vec);\n }\n \n /* Analyze statements contained in SLP tree NODE after recursively analyzing"}, {"sha": "ed8444b9ce3656b3c56fd57ce6485699148d7538", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 319, "deletions": 276, "changes": 595, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "patch": "@@ -205,7 +205,7 @@ vect_mark_relevant (vec<gimple *> *worklist, gimple *stmt,\n     {\n       dump_printf_loc (MSG_NOTE, vect_location,\n \t\t       \"mark relevant %d, live %d: \", relevant, live_p);\n-      dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt, 0);\n+      dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt_info->stmt, 0);\n     }\n \n   /* If this stmt is an original stmt in a pattern, we might need to mark its\n@@ -244,7 +244,7 @@ vect_mark_relevant (vec<gimple *> *worklist, gimple *stmt,\n       return;\n     }\n \n-  worklist->safe_push (stmt);\n+  worklist->safe_push (stmt_info);\n }\n \n \n@@ -389,10 +389,10 @@ exist_non_indexing_operands_for_use_p (tree use, gimple *stmt)\n      Therefore, all we need to check is if STMT falls into the\n      first case, and whether var corresponds to USE.  */\n \n-  gassign *assign = dyn_cast <gassign *> (stmt);\n+  gassign *assign = dyn_cast <gassign *> (stmt_info->stmt);\n   if (!assign || !gimple_assign_copy_p (assign))\n     {\n-      gcall *call = dyn_cast <gcall *> (stmt);\n+      gcall *call = dyn_cast <gcall *> (stmt_info->stmt);\n       if (call && gimple_call_internal_p (call))\n \t{\n \t  internal_fn ifn = gimple_call_internal_fn (call);\n@@ -463,7 +463,7 @@ process_use (gimple *stmt, tree use, loop_vec_info loop_vinfo,\n \n   /* case 1: we are only interested in uses that need to be vectorized.  Uses\n      that are used for address computation are not considered relevant.  */\n-  if (!force && !exist_non_indexing_operands_for_use_p (use, stmt))\n+  if (!force && !exist_non_indexing_operands_for_use_p (use, stmt_vinfo))\n      return true;\n \n   if (!vect_is_simple_use (use, loop_vinfo, &dt, &dstmt_vinfo))\n@@ -484,8 +484,8 @@ process_use (gimple *stmt, tree use, loop_vec_info loop_vinfo,\n      only way that STMT, which is a reduction-phi, was put in the worklist,\n      as there should be no other uses for DSTMT_VINFO in the loop.  So we just\n      check that everything is as expected, and we are done.  */\n-  bb = gimple_bb (stmt);\n-  if (gimple_code (stmt) == GIMPLE_PHI\n+  bb = gimple_bb (stmt_vinfo->stmt);\n+  if (gimple_code (stmt_vinfo->stmt) == GIMPLE_PHI\n       && STMT_VINFO_DEF_TYPE (stmt_vinfo) == vect_reduction_def\n       && gimple_code (dstmt_vinfo->stmt) != GIMPLE_PHI\n       && STMT_VINFO_DEF_TYPE (dstmt_vinfo) == vect_reduction_def\n@@ -576,10 +576,11 @@ process_use (gimple *stmt, tree use, loop_vec_info loop_vinfo,\n      inductions.  Otherwise we'll needlessly vectorize the IV increment\n      and cause hybrid SLP for SLP inductions.  Unless the PHI is live\n      of course.  */\n-  else if (gimple_code (stmt) == GIMPLE_PHI\n+  else if (gimple_code (stmt_vinfo->stmt) == GIMPLE_PHI\n \t   && STMT_VINFO_DEF_TYPE (stmt_vinfo) == vect_induction_def\n \t   && ! STMT_VINFO_LIVE_P (stmt_vinfo)\n-\t   && (PHI_ARG_DEF_FROM_EDGE (stmt, loop_latch_edge (bb->loop_father))\n+\t   && (PHI_ARG_DEF_FROM_EDGE (stmt_vinfo->stmt,\n+\t\t\t\t      loop_latch_edge (bb->loop_father))\n \t       == use))\n     {\n       if (dump_enabled_p ())\n@@ -740,56 +741,56 @@ vect_mark_stmts_to_be_vectorized (loop_vec_info loop_vinfo)\n           /* Pattern statements are not inserted into the code, so\n              FOR_EACH_PHI_OR_STMT_USE optimizes their operands out, and we\n              have to scan the RHS or function arguments instead.  */\n-\t  if (gassign *assign = dyn_cast <gassign *> (stmt))\n+\t  if (gassign *assign = dyn_cast <gassign *> (stmt_vinfo->stmt))\n \t    {\n \t      enum tree_code rhs_code = gimple_assign_rhs_code (assign);\n \t      tree op = gimple_assign_rhs1 (assign);\n \n \t      i = 1;\n \t      if (rhs_code == COND_EXPR && COMPARISON_CLASS_P (op))\n \t\t{\n-\t\t  if (!process_use (stmt, TREE_OPERAND (op, 0), loop_vinfo,\n-\t\t\t\t    relevant, &worklist, false)\n-\t\t      || !process_use (stmt, TREE_OPERAND (op, 1), loop_vinfo,\n-\t\t\t\t       relevant, &worklist, false))\n+\t\t  if (!process_use (stmt_vinfo, TREE_OPERAND (op, 0),\n+\t\t\t\t    loop_vinfo, relevant, &worklist, false)\n+\t\t      || !process_use (stmt_vinfo, TREE_OPERAND (op, 1),\n+\t\t\t\t       loop_vinfo, relevant, &worklist, false))\n \t\t    return false;\n \t\t  i = 2;\n \t\t}\n \t      for (; i < gimple_num_ops (assign); i++)\n \t\t{\n \t\t  op = gimple_op (assign, i);\n                   if (TREE_CODE (op) == SSA_NAME\n-\t\t      && !process_use (stmt, op, loop_vinfo, relevant,\n+\t\t      && !process_use (stmt_vinfo, op, loop_vinfo, relevant,\n \t\t\t\t       &worklist, false))\n                     return false;\n                  }\n             }\n-\t  else if (gcall *call = dyn_cast <gcall *> (stmt))\n+\t  else if (gcall *call = dyn_cast <gcall *> (stmt_vinfo->stmt))\n \t    {\n \t      for (i = 0; i < gimple_call_num_args (call); i++)\n \t\t{\n \t\t  tree arg = gimple_call_arg (call, i);\n-\t\t  if (!process_use (stmt, arg, loop_vinfo, relevant,\n+\t\t  if (!process_use (stmt_vinfo, arg, loop_vinfo, relevant,\n \t\t\t\t    &worklist, false))\n                     return false;\n \t\t}\n \t    }\n         }\n       else\n-        FOR_EACH_PHI_OR_STMT_USE (use_p, stmt, iter, SSA_OP_USE)\n+\tFOR_EACH_PHI_OR_STMT_USE (use_p, stmt_vinfo->stmt, iter, SSA_OP_USE)\n           {\n             tree op = USE_FROM_PTR (use_p);\n-\t    if (!process_use (stmt, op, loop_vinfo, relevant,\n+\t    if (!process_use (stmt_vinfo, op, loop_vinfo, relevant,\n \t\t\t      &worklist, false))\n               return false;\n           }\n \n       if (STMT_VINFO_GATHER_SCATTER_P (stmt_vinfo))\n \t{\n \t  gather_scatter_info gs_info;\n-\t  if (!vect_check_gather_scatter (stmt, loop_vinfo, &gs_info))\n+\t  if (!vect_check_gather_scatter (stmt_vinfo, loop_vinfo, &gs_info))\n \t    gcc_unreachable ();\n-\t  if (!process_use (stmt, gs_info.offset, loop_vinfo, relevant,\n+\t  if (!process_use (stmt_vinfo, gs_info.offset, loop_vinfo, relevant,\n \t\t\t    &worklist, true))\n \t    return false;\n \t}\n@@ -1362,8 +1363,8 @@ vect_init_vector_1 (gimple *stmt, gimple *new_stmt, gimple_stmt_iterator *gsi)\n \t  basic_block new_bb;\n \t  edge pe;\n \n-          if (nested_in_vect_loop_p (loop, stmt))\n-            loop = loop->inner;\n+\t  if (nested_in_vect_loop_p (loop, stmt_vinfo))\n+\t    loop = loop->inner;\n \n \t  pe = loop_preheader_edge (loop);\n           new_bb = gsi_insert_on_edge_immediate (pe, new_stmt);\n@@ -1573,7 +1574,7 @@ vect_get_vec_def_for_operand (tree op, gimple *stmt, tree vectype)\n \tvector_type = get_vectype_for_scalar_type (TREE_TYPE (op));\n \n       gcc_assert (vector_type);\n-      return vect_init_vector (stmt, op, vector_type, NULL);\n+      return vect_init_vector (stmt_vinfo, op, vector_type, NULL);\n     }\n   else\n     return vect_get_vec_def_for_operand_1 (def_stmt_info, dt);\n@@ -1740,12 +1741,12 @@ vect_finish_stmt_generation_1 (gimple *stmt, gimple *vec_stmt)\n       dump_gimple_stmt (MSG_NOTE, TDF_SLIM, vec_stmt, 0);\n     }\n \n-  gimple_set_location (vec_stmt, gimple_location (stmt));\n+  gimple_set_location (vec_stmt, gimple_location (stmt_info->stmt));\n \n   /* While EH edges will generally prevent vectorization, stmt might\n      e.g. be in a must-not-throw region.  Ensure newly created stmts\n      that could throw are part of the same region.  */\n-  int lp_nr = lookup_stmt_eh_lp (stmt);\n+  int lp_nr = lookup_stmt_eh_lp (stmt_info->stmt);\n   if (lp_nr != 0 && stmt_could_throw_p (vec_stmt))\n     add_stmt_to_eh_lp (vec_stmt, lp_nr);\n \n@@ -2269,7 +2270,7 @@ get_group_load_store_type (gimple *stmt, tree vectype, bool slp,\n \n       if (!STMT_VINFO_STRIDED_P (stmt_info)\n \t  && (can_overrun_p || !would_overrun_p)\n-\t  && compare_step_with_zero (stmt) > 0)\n+\t  && compare_step_with_zero (stmt_info) > 0)\n \t{\n \t  /* First cope with the degenerate case of a single-element\n \t     vector.  */\n@@ -2309,7 +2310,7 @@ get_group_load_store_type (gimple *stmt, tree vectype, bool slp,\n       if (*memory_access_type == VMAT_ELEMENTWISE\n \t  && single_element_p\n \t  && loop_vinfo\n-\t  && vect_use_strided_gather_scatters_p (stmt, loop_vinfo,\n+\t  && vect_use_strided_gather_scatters_p (stmt_info, loop_vinfo,\n \t\t\t\t\t\t masked_p, gs_info))\n \t*memory_access_type = VMAT_GATHER_SCATTER;\n     }\n@@ -2421,7 +2422,7 @@ get_load_store_type (gimple *stmt, tree vectype, bool slp, bool masked_p,\n   if (STMT_VINFO_GATHER_SCATTER_P (stmt_info))\n     {\n       *memory_access_type = VMAT_GATHER_SCATTER;\n-      if (!vect_check_gather_scatter (stmt, loop_vinfo, gs_info))\n+      if (!vect_check_gather_scatter (stmt_info, loop_vinfo, gs_info))\n \tgcc_unreachable ();\n       else if (!vect_is_simple_use (gs_info->offset, vinfo,\n \t\t\t\t    &gs_info->offset_dt,\n@@ -2436,26 +2437,26 @@ get_load_store_type (gimple *stmt, tree vectype, bool slp, bool masked_p,\n     }\n   else if (STMT_VINFO_GROUPED_ACCESS (stmt_info))\n     {\n-      if (!get_group_load_store_type (stmt, vectype, slp, masked_p, vls_type,\n-\t\t\t\t      memory_access_type, gs_info))\n+      if (!get_group_load_store_type (stmt_info, vectype, slp, masked_p,\n+\t\t\t\t      vls_type, memory_access_type, gs_info))\n \treturn false;\n     }\n   else if (STMT_VINFO_STRIDED_P (stmt_info))\n     {\n       gcc_assert (!slp);\n       if (loop_vinfo\n-\t  && vect_use_strided_gather_scatters_p (stmt, loop_vinfo,\n+\t  && vect_use_strided_gather_scatters_p (stmt_info, loop_vinfo,\n \t\t\t\t\t\t masked_p, gs_info))\n \t*memory_access_type = VMAT_GATHER_SCATTER;\n       else\n \t*memory_access_type = VMAT_ELEMENTWISE;\n     }\n   else\n     {\n-      int cmp = compare_step_with_zero (stmt);\n+      int cmp = compare_step_with_zero (stmt_info);\n       if (cmp < 0)\n \t*memory_access_type = get_negative_load_store_type\n-\t  (stmt, vectype, vls_type, ncopies);\n+\t  (stmt_info, vectype, vls_type, ncopies);\n       else if (cmp == 0)\n \t{\n \t  gcc_assert (vls_type == VLS_LOAD);\n@@ -2742,8 +2743,8 @@ vect_build_gather_load_calls (gimple *stmt, gimple_stmt_iterator *gsi,\n   else\n     gcc_unreachable ();\n \n-  tree vec_dest = vect_create_destination_var (gimple_get_lhs (stmt),\n-\t\t\t\t\t       vectype);\n+  tree scalar_dest = gimple_get_lhs (stmt_info->stmt);\n+  tree vec_dest = vect_create_destination_var (scalar_dest, vectype);\n \n   tree ptr = fold_convert (ptrtype, gs_info->base);\n   if (!is_gimple_min_invariant (ptr))\n@@ -2765,19 +2766,19 @@ vect_build_gather_load_calls (gimple *stmt, gimple_stmt_iterator *gsi,\n \n   if (!mask)\n     {\n-      src_op = vect_build_zero_merge_argument (stmt, rettype);\n-      mask_op = vect_build_all_ones_mask (stmt, masktype);\n+      src_op = vect_build_zero_merge_argument (stmt_info, rettype);\n+      mask_op = vect_build_all_ones_mask (stmt_info, masktype);\n     }\n \n   for (int j = 0; j < ncopies; ++j)\n     {\n       tree op, var;\n       if (modifier == WIDEN && (j & 1))\n \top = permute_vec_elements (vec_oprnd0, vec_oprnd0,\n-\t\t\t\t   perm_mask, stmt, gsi);\n+\t\t\t\t   perm_mask, stmt_info, gsi);\n       else if (j == 0)\n \top = vec_oprnd0\n-\t  = vect_get_vec_def_for_operand (gs_info->offset, stmt);\n+\t  = vect_get_vec_def_for_operand (gs_info->offset, stmt_info);\n       else\n \top = vec_oprnd0\n \t  = vect_get_vec_def_for_stmt_copy (gs_info->offset_dt, vec_oprnd0);\n@@ -2789,19 +2790,19 @@ vect_build_gather_load_calls (gimple *stmt, gimple_stmt_iterator *gsi,\n \t  var = vect_get_new_ssa_name (idxtype, vect_simple_var);\n \t  op = build1 (VIEW_CONVERT_EXPR, idxtype, op);\n \t  gassign *new_stmt = gimple_build_assign (var, VIEW_CONVERT_EXPR, op);\n-\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t  vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t  op = var;\n \t}\n \n       if (mask)\n \t{\n \t  if (mask_perm_mask && (j & 1))\n \t    mask_op = permute_vec_elements (mask_op, mask_op,\n-\t\t\t\t\t    mask_perm_mask, stmt, gsi);\n+\t\t\t\t\t    mask_perm_mask, stmt_info, gsi);\n \t  else\n \t    {\n \t      if (j == 0)\n-\t\tvec_mask = vect_get_vec_def_for_operand (mask, stmt);\n+\t\tvec_mask = vect_get_vec_def_for_operand (mask, stmt_info);\n \t      else\n \t\tvec_mask = vect_get_vec_def_for_stmt_copy (mask_dt, vec_mask);\n \n@@ -2815,7 +2816,7 @@ vect_build_gather_load_calls (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  mask_op = build1 (VIEW_CONVERT_EXPR, masktype, mask_op);\n \t\t  gassign *new_stmt\n \t\t    = gimple_build_assign (var, VIEW_CONVERT_EXPR, mask_op);\n-\t\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t  vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t  mask_op = var;\n \t\t}\n \t    }\n@@ -2832,17 +2833,19 @@ vect_build_gather_load_calls (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t\tTYPE_VECTOR_SUBPARTS (rettype)));\n \t  op = vect_get_new_ssa_name (rettype, vect_simple_var);\n \t  gimple_call_set_lhs (new_call, op);\n-\t  vect_finish_stmt_generation (stmt, new_call, gsi);\n+\t  vect_finish_stmt_generation (stmt_info, new_call, gsi);\n \t  var = make_ssa_name (vec_dest);\n \t  op = build1 (VIEW_CONVERT_EXPR, vectype, op);\n \t  gassign *new_stmt = gimple_build_assign (var, VIEW_CONVERT_EXPR, op);\n-\t  new_stmt_info = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t  new_stmt_info\n+\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t}\n       else\n \t{\n \t  var = make_ssa_name (vec_dest, new_call);\n \t  gimple_call_set_lhs (new_call, var);\n-\t  new_stmt_info = vect_finish_stmt_generation (stmt, new_call, gsi);\n+\t  new_stmt_info\n+\t    = vect_finish_stmt_generation (stmt_info, new_call, gsi);\n \t}\n \n       if (modifier == NARROW)\n@@ -2852,7 +2855,8 @@ vect_build_gather_load_calls (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      prev_res = var;\n \t      continue;\n \t    }\n-\t  var = permute_vec_elements (prev_res, var, perm_mask, stmt, gsi);\n+\t  var = permute_vec_elements (prev_res, var, perm_mask,\n+\t\t\t\t      stmt_info, gsi);\n \t  new_stmt_info = loop_vinfo->lookup_def (var);\n \t}\n \n@@ -3027,7 +3031,7 @@ vectorizable_bswap (gimple *stmt, gimple_stmt_iterator *gsi,\n     {\n       /* Handle uses.  */\n       if (j == 0)\n-        vect_get_vec_defs (op, NULL, stmt, &vec_oprnds, NULL, slp_node);\n+\tvect_get_vec_defs (op, NULL, stmt_info, &vec_oprnds, NULL, slp_node);\n       else\n         vect_get_vec_defs_for_stmt_copy (dt, &vec_oprnds, NULL);\n \n@@ -3040,15 +3044,16 @@ vectorizable_bswap (gimple *stmt, gimple_stmt_iterator *gsi,\n \t tree tem = make_ssa_name (char_vectype);\n \t new_stmt = gimple_build_assign (tem, build1 (VIEW_CONVERT_EXPR,\n \t\t\t\t\t\t      char_vectype, vop));\n-\t vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t tree tem2 = make_ssa_name (char_vectype);\n \t new_stmt = gimple_build_assign (tem2, VEC_PERM_EXPR,\n \t\t\t\t\t tem, tem, bswap_vconst);\n-\t vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t tem = make_ssa_name (vectype);\n \t new_stmt = gimple_build_assign (tem, build1 (VIEW_CONVERT_EXPR,\n \t\t\t\t\t\t      vectype, tem2));\n-\t new_stmt_info = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t new_stmt_info\n+\t   = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n          if (slp_node)\n \t   SLP_TREE_VEC_STMTS (slp_node).quick_push (new_stmt_info);\n        }\n@@ -3137,8 +3142,8 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n       && ! vec_stmt)\n     return false;\n \n-  /* Is GS a vectorizable call?   */\n-  stmt = dyn_cast <gcall *> (gs);\n+  /* Is STMT_INFO a vectorizable call?   */\n+  stmt = dyn_cast <gcall *> (stmt_info->stmt);\n   if (!stmt)\n     return false;\n \n@@ -3307,7 +3312,7 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t       && (gimple_call_builtin_p (stmt, BUILT_IN_BSWAP16)\n \t\t   || gimple_call_builtin_p (stmt, BUILT_IN_BSWAP32)\n \t\t   || gimple_call_builtin_p (stmt, BUILT_IN_BSWAP64)))\n-\treturn vectorizable_bswap (stmt, gsi, vec_stmt, slp_node,\n+\treturn vectorizable_bswap (stmt_info, gsi, vec_stmt, slp_node,\n \t\t\t\t   vectype_in, dt, cost_vec);\n       else\n \t{\n@@ -3400,7 +3405,7 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t\t      gimple_call_set_lhs (call, half_res);\n \t\t      gimple_call_set_nothrow (call, true);\n \t\t      new_stmt_info\n-\t\t\t= vect_finish_stmt_generation (stmt, call, gsi);\n+\t\t\t= vect_finish_stmt_generation (stmt_info, call, gsi);\n \t\t      if ((i & 1) == 0)\n \t\t\t{\n \t\t\t  prev_res = half_res;\n@@ -3411,7 +3416,8 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t\t\t= gimple_build_assign (new_temp, convert_code,\n \t\t\t\t\t       prev_res, half_res);\n \t\t      new_stmt_info\n-\t\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t\t= vect_finish_stmt_generation (stmt_info, new_stmt,\n+\t\t\t\t\t\t       gsi);\n \t\t    }\n \t\t  else\n \t\t    {\n@@ -3435,7 +3441,7 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t\t      gimple_call_set_lhs (call, new_temp);\n \t\t      gimple_call_set_nothrow (call, true);\n \t\t      new_stmt_info\n-\t\t\t= vect_finish_stmt_generation (stmt, call, gsi);\n+\t\t\t= vect_finish_stmt_generation (stmt_info, call, gsi);\n \t\t    }\n \t\t  SLP_TREE_VEC_STMTS (slp_node).quick_push (new_stmt_info);\n \t\t}\n@@ -3453,7 +3459,7 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t      op = gimple_call_arg (stmt, i);\n \t      if (j == 0)\n \t\tvec_oprnd0\n-\t\t  = vect_get_vec_def_for_operand (op, stmt);\n+\t\t  = vect_get_vec_def_for_operand (op, stmt_info);\n \t      else\n \t\tvec_oprnd0\n \t\t  = vect_get_vec_def_for_stmt_copy (dt[i], orig_vargs[i]);\n@@ -3476,11 +3482,11 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t      tree new_var\n \t\t= vect_get_new_ssa_name (vectype_out, vect_simple_var, \"cst_\");\n \t      gimple *init_stmt = gimple_build_assign (new_var, cst);\n-\t      vect_init_vector_1 (stmt, init_stmt, NULL);\n+\t      vect_init_vector_1 (stmt_info, init_stmt, NULL);\n \t      new_temp = make_ssa_name (vec_dest);\n \t      gimple *new_stmt = gimple_build_assign (new_temp, new_var);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t    }\n \t  else if (modifier == NARROW)\n \t    {\n@@ -3491,7 +3497,8 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t      gcall *call = gimple_build_call_internal_vec (ifn, vargs);\n \t      gimple_call_set_lhs (call, half_res);\n \t      gimple_call_set_nothrow (call, true);\n-\t      new_stmt_info = vect_finish_stmt_generation (stmt, call, gsi);\n+\t      new_stmt_info\n+\t\t= vect_finish_stmt_generation (stmt_info, call, gsi);\n \t      if ((j & 1) == 0)\n \t\t{\n \t\t  prev_res = half_res;\n@@ -3501,7 +3508,7 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t      gassign *new_stmt = gimple_build_assign (new_temp, convert_code,\n \t\t\t\t\t\t       prev_res, half_res);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t    }\n \t  else\n \t    {\n@@ -3513,7 +3520,8 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t      new_temp = make_ssa_name (vec_dest, call);\n \t      gimple_call_set_lhs (call, new_temp);\n \t      gimple_call_set_nothrow (call, true);\n-\t      new_stmt_info = vect_finish_stmt_generation (stmt, call, gsi);\n+\t      new_stmt_info\n+\t\t= vect_finish_stmt_generation (stmt_info, call, gsi);\n \t    }\n \n \t  if (j == (modifier == NARROW ? 1 : 0))\n@@ -3566,7 +3574,7 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t\t  gimple_call_set_lhs (call, new_temp);\n \t\t  gimple_call_set_nothrow (call, true);\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, call, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, call, gsi);\n \t\t  SLP_TREE_VEC_STMTS (slp_node).quick_push (new_stmt_info);\n \t\t}\n \n@@ -3584,7 +3592,7 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t      if (j == 0)\n \t\t{\n \t\t  vec_oprnd0\n-\t\t    = vect_get_vec_def_for_operand (op, stmt);\n+\t\t    = vect_get_vec_def_for_operand (op, stmt_info);\n \t\t  vec_oprnd1\n \t\t    = vect_get_vec_def_for_stmt_copy (dt[i], vec_oprnd0);\n \t\t}\n@@ -3605,7 +3613,8 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi,\n \t  gcall *new_stmt = gimple_build_call_vec (fndecl, vargs);\n \t  new_temp = make_ssa_name (vec_dest, new_stmt);\n \t  gimple_call_set_lhs (new_stmt, new_temp);\n-\t  new_stmt_info = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t  new_stmt_info\n+\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \n \t  if (j == 0)\n \t    STMT_VINFO_VEC_STMT (stmt_info) = new_stmt_info;\n@@ -3793,7 +3802,7 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \n   vectype = STMT_VINFO_VECTYPE (stmt_info);\n \n-  if (loop_vinfo && nested_in_vect_loop_p (loop, stmt))\n+  if (loop_vinfo && nested_in_vect_loop_p (loop, stmt_info))\n     return false;\n \n   /* FORNOW */\n@@ -4098,7 +4107,7 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t      gcc_assert ((k & (k - 1)) == 0);\n \t\t      if (m == 0)\n \t\t\tvec_oprnd0\n-\t\t\t  = vect_get_vec_def_for_operand (op, stmt);\n+\t\t\t  = vect_get_vec_def_for_operand (op, stmt_info);\n \t\t      else\n \t\t\t{\n \t\t\t  vec_oprnd0 = arginfo[i].op;\n@@ -4115,7 +4124,7 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t      gassign *new_stmt\n \t\t\t= gimple_build_assign (make_ssa_name (atype),\n \t\t\t\t\t       vec_oprnd0);\n-\t\t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t      vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t      vargs.safe_push (gimple_assign_lhs (new_stmt));\n \t\t    }\n \t\t  else\n@@ -4132,7 +4141,7 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t{\n \t\t\t  if (m == 0 && l == 0)\n \t\t\t    vec_oprnd0\n-\t\t\t      = vect_get_vec_def_for_operand (op, stmt);\n+\t\t\t      = vect_get_vec_def_for_operand (op, stmt_info);\n \t\t\t  else\n \t\t\t    vec_oprnd0\n \t\t\t      = vect_get_vec_def_for_stmt_copy (arginfo[i].dt,\n@@ -4151,7 +4160,8 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t  gassign *new_stmt\n \t\t\t    = gimple_build_assign (make_ssa_name (atype),\n \t\t\t\t\t\t   vec_oprnd0);\n-\t\t\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t\t  vect_finish_stmt_generation (stmt_info, new_stmt,\n+\t\t\t\t\t\t       gsi);\n \t\t\t  vargs.safe_push (gimple_assign_lhs (new_stmt));\n \t\t\t}\n \t\t    }\n@@ -4220,7 +4230,7 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  gassign *new_stmt\n \t\t    = gimple_build_assign (new_temp, code,\n \t\t\t\t\t   arginfo[i].op, tcst);\n-\t\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t  vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t  vargs.safe_push (new_temp);\n \t\t}\n \t      break;\n@@ -4249,7 +4259,7 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t  gimple_call_set_lhs (new_call, new_temp);\n \t}\n       stmt_vec_info new_stmt_info\n-\t= vect_finish_stmt_generation (stmt, new_call, gsi);\n+\t= vect_finish_stmt_generation (stmt_info, new_call, gsi);\n \n       if (vec_dest)\n \t{\n@@ -4275,7 +4285,7 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  gimple *new_stmt\n \t\t    = gimple_build_assign (make_ssa_name (vectype), t);\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \n \t\t  if (j == 0 && l == 0)\n \t\t    STMT_VINFO_VEC_STMT (stmt_info)\n@@ -4287,7 +4297,7 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t}\n \n \t      if (ratype)\n-\t\tvect_clobber_variable (stmt, gsi, new_temp);\n+\t\tvect_clobber_variable (stmt_info, gsi, new_temp);\n \t      continue;\n \t    }\n \t  else if (simd_clone_subparts (vectype) > nunits)\n@@ -4307,11 +4317,12 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t      gimple *new_stmt\n \t\t\t= gimple_build_assign (make_ssa_name (rtype), tem);\n \t\t      new_stmt_info\n-\t\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t\t= vect_finish_stmt_generation (stmt_info, new_stmt,\n+\t\t\t\t\t\t       gsi);\n \t\t      CONSTRUCTOR_APPEND_ELT (ret_ctor_elts, NULL_TREE,\n \t\t\t\t\t      gimple_assign_lhs (new_stmt));\n \t\t    }\n-\t\t  vect_clobber_variable (stmt, gsi, new_temp);\n+\t\t  vect_clobber_variable (stmt_info, gsi, new_temp);\n \t\t}\n \t      else\n \t\tCONSTRUCTOR_APPEND_ELT (ret_ctor_elts, NULL_TREE, new_temp);\n@@ -4321,7 +4332,7 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      gimple *new_stmt\n \t\t= gimple_build_assign (make_ssa_name (vec_dest), vec_oprnd0);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \n \t      if ((unsigned) j == k - 1)\n \t\tSTMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = new_stmt_info;\n@@ -4339,8 +4350,8 @@ vectorizable_simd_clone_call (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      gimple *new_stmt\n \t\t= gimple_build_assign (make_ssa_name (vec_dest), t);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n-\t      vect_clobber_variable (stmt, gsi, new_temp);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n+\t      vect_clobber_variable (stmt_info, gsi, new_temp);\n \t    }\n \t}\n \n@@ -4493,7 +4504,7 @@ vect_create_vectorized_demotion_stmts (vec<tree> *vec_oprnds,\n       new_tmp = make_ssa_name (vec_dest, new_stmt);\n       gimple_assign_set_lhs (new_stmt, new_tmp);\n       stmt_vec_info new_stmt_info\n-\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \n       if (multi_step_cvt)\n \t/* Store the resulting vector for next recursive call.  */\n@@ -4527,8 +4538,8 @@ vect_create_vectorized_demotion_stmts (vec<tree> *vec_oprnds,\n \t previous level.  */\n       vec_oprnds->truncate ((i+1)/2);\n       vect_create_vectorized_demotion_stmts (vec_oprnds, multi_step_cvt - 1,\n-\t\t\t\t\t     stmt, vec_dsts, gsi, slp_node,\n-\t\t\t\t\t     VEC_PACK_TRUNC_EXPR,\n+\t\t\t\t\t     stmt_info, vec_dsts, gsi,\n+\t\t\t\t\t     slp_node, VEC_PACK_TRUNC_EXPR,\n \t\t\t\t\t     prev_stmt_info);\n     }\n \n@@ -4793,9 +4804,9 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n       return false;\n \n     case WIDEN:\n-      if (supportable_widening_operation (code, stmt, vectype_out, vectype_in,\n-\t\t\t\t\t  &code1, &code2, &multi_step_cvt,\n-\t\t\t\t\t  &interm_types))\n+      if (supportable_widening_operation (code, stmt_info, vectype_out,\n+\t\t\t\t\t  vectype_in, &code1, &code2,\n+\t\t\t\t\t  &multi_step_cvt, &interm_types))\n \t{\n \t  /* Binary widening operation can only be supported directly by the\n \t     architecture.  */\n@@ -4826,15 +4837,16 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t\t\t\t  cvt_type, &decl1, &codecvt1))\n \t\tgoto unsupported;\n \t    }\n-\t  else if (!supportable_widening_operation (code, stmt, vectype_out,\n-\t\t\t\t\t\t    cvt_type, &codecvt1,\n-\t\t\t\t\t\t    &codecvt2, &multi_step_cvt,\n+\t  else if (!supportable_widening_operation (code, stmt_info,\n+\t\t\t\t\t\t    vectype_out, cvt_type,\n+\t\t\t\t\t\t    &codecvt1, &codecvt2,\n+\t\t\t\t\t\t    &multi_step_cvt,\n \t\t\t\t\t\t    &interm_types))\n \t    continue;\n \t  else\n \t    gcc_assert (multi_step_cvt == 0);\n \n-\t  if (supportable_widening_operation (NOP_EXPR, stmt, cvt_type,\n+\t  if (supportable_widening_operation (NOP_EXPR, stmt_info, cvt_type,\n \t\t\t\t\t      vectype_in, &code1, &code2,\n \t\t\t\t\t      &multi_step_cvt, &interm_types))\n \t    {\n@@ -4973,7 +4985,8 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n       for (j = 0; j < ncopies; j++)\n \t{\n \t  if (j == 0)\n-\t    vect_get_vec_defs (op0, NULL, stmt, &vec_oprnds0, NULL, slp_node);\n+\t    vect_get_vec_defs (op0, NULL, stmt_info, &vec_oprnds0,\n+\t\t\t       NULL, slp_node);\n \t  else\n \t    vect_get_vec_defs_for_stmt_copy (dt, &vec_oprnds0, NULL);\n \n@@ -4987,7 +5000,7 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  new_temp = make_ssa_name (vec_dest, new_stmt);\n \t\t  gimple_call_set_lhs (new_stmt, new_temp);\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t}\n \t      else\n \t\t{\n@@ -4997,7 +5010,7 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  new_temp = make_ssa_name (vec_dest, new_stmt);\n \t\t  gimple_assign_set_lhs (new_stmt, new_temp);\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t}\n \n \t      if (slp_node)\n@@ -5038,23 +5051,24 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t      for (k = 0; k < slp_node->vec_stmts_size - 1; k++)\n \t\t\tvec_oprnds1.quick_push (vec_oprnd1);\n \n-\t\t      vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL,\n-\t\t\t\t\t slp_node);\n+\t\t      vect_get_vec_defs (op0, NULL_TREE, stmt_info,\n+\t\t\t\t\t &vec_oprnds0, NULL, slp_node);\n \t\t    }\n \t\t  else\n-\t\t    vect_get_vec_defs (op0, op1, stmt, &vec_oprnds0,\n+\t\t    vect_get_vec_defs (op0, op1, stmt_info, &vec_oprnds0,\n \t\t\t\t       &vec_oprnds1, slp_node);\n \t\t}\n \t      else\n \t\t{\n-\t\t  vec_oprnd0 = vect_get_vec_def_for_operand (op0, stmt);\n+\t\t  vec_oprnd0 = vect_get_vec_def_for_operand (op0, stmt_info);\n \t\t  vec_oprnds0.quick_push (vec_oprnd0);\n \t\t  if (op_type == binary_op)\n \t\t    {\n \t\t      if (code == WIDEN_LSHIFT_EXPR)\n \t\t\tvec_oprnd1 = op1;\n \t\t      else\n-\t\t\tvec_oprnd1 = vect_get_vec_def_for_operand (op1, stmt);\n+\t\t\tvec_oprnd1\n+\t\t\t  = vect_get_vec_def_for_operand (op1, stmt_info);\n \t\t      vec_oprnds1.quick_push (vec_oprnd1);\n \t\t    }\n \t\t}\n@@ -5087,8 +5101,8 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  c2 = codecvt2;\n \t\t}\n \t      vect_create_vectorized_promotion_stmts (&vec_oprnds0,\n-\t\t\t\t\t\t      &vec_oprnds1,\n-\t\t\t\t\t\t      stmt, this_dest, gsi,\n+\t\t\t\t\t\t      &vec_oprnds1, stmt_info,\n+\t\t\t\t\t\t      this_dest, gsi,\n \t\t\t\t\t\t      c1, c2, decl1, decl2,\n \t\t\t\t\t\t      op_type);\n \t    }\n@@ -5104,7 +5118,8 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t      new_temp = make_ssa_name (vec_dest, new_stmt);\n \t\t      gimple_call_set_lhs (new_stmt, new_temp);\n \t\t      new_stmt_info\n-\t\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t\t= vect_finish_stmt_generation (stmt_info, new_stmt,\n+\t\t\t\t\t\t       gsi);\n \t\t    }\n \t\t  else\n \t\t    {\n@@ -5113,7 +5128,8 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t      gassign *new_stmt\n \t\t\t= gimple_build_assign (new_temp, codecvt1, vop0);\n \t\t      new_stmt_info\n-\t\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t\t= vect_finish_stmt_generation (stmt_info, new_stmt,\n+\t\t\t\t\t\t       gsi);\n \t\t    }\n \t\t}\n \t      else\n@@ -5144,12 +5160,13 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n \t{\n \t  /* Handle uses.  */\n \t  if (slp_node)\n-\t    vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL,\n+\t    vect_get_vec_defs (op0, NULL_TREE, stmt_info, &vec_oprnds0, NULL,\n \t\t\t       slp_node);\n \t  else\n \t    {\n \t      vec_oprnds0.truncate (0);\n-\t      vect_get_loop_based_defs (&last_oprnd, stmt, dt[0], &vec_oprnds0,\n+\t      vect_get_loop_based_defs (&last_oprnd, stmt_info, dt[0],\n+\t\t\t\t\t&vec_oprnds0,\n \t\t\t\t\tvect_pow2 (multi_step_cvt) - 1);\n \t    }\n \n@@ -5162,22 +5179,22 @@ vectorizable_conversion (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t    gcall *new_stmt = gimple_build_call (decl1, 1, vop0);\n \t\t    new_temp = make_ssa_name (vec_dest, new_stmt);\n \t\t    gimple_call_set_lhs (new_stmt, new_temp);\n-\t\t    vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t  }\n \t\telse\n \t\t  {\n \t\t    gcc_assert (TREE_CODE_LENGTH (codecvt1) == unary_op);\n \t\t    new_temp = make_ssa_name (vec_dest);\n \t\t    gassign *new_stmt\n \t\t      = gimple_build_assign (new_temp, codecvt1, vop0);\n-\t\t    vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t  }\n \n \t\tvec_oprnds0[i] = new_temp;\n \t      }\n \n \t  vect_create_vectorized_demotion_stmts (&vec_oprnds0, multi_step_cvt,\n-\t\t\t\t\t\t stmt, vec_dsts, gsi,\n+\t\t\t\t\t\t stmt_info, vec_dsts, gsi,\n \t\t\t\t\t\t slp_node, code1,\n \t\t\t\t\t\t &prev_stmt_info);\n \t}\n@@ -5324,7 +5341,7 @@ vectorizable_assignment (gimple *stmt, gimple_stmt_iterator *gsi,\n     {\n       /* Handle uses.  */\n       if (j == 0)\n-        vect_get_vec_defs (op, NULL, stmt, &vec_oprnds, NULL, slp_node);\n+\tvect_get_vec_defs (op, NULL, stmt_info, &vec_oprnds, NULL, slp_node);\n       else\n         vect_get_vec_defs_for_stmt_copy (dt, &vec_oprnds, NULL);\n \n@@ -5338,7 +5355,8 @@ vectorizable_assignment (gimple *stmt, gimple_stmt_iterator *gsi,\n \t gassign *new_stmt = gimple_build_assign (vec_dest, vop);\n          new_temp = make_ssa_name (vec_dest, new_stmt);\n          gimple_assign_set_lhs (new_stmt, new_temp);\n-\t new_stmt_info = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t new_stmt_info\n+\t   = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n          if (slp_node)\n \t   SLP_TREE_VEC_STMTS (slp_node).quick_push (new_stmt_info);\n        }\n@@ -5623,7 +5641,7 @@ vectorizable_shift (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  if (vec_stmt && !slp_node)\n \t\t    {\n \t\t      op1 = fold_convert (TREE_TYPE (vectype), op1);\n-\t\t      op1 = vect_init_vector (stmt, op1,\n+\t\t      op1 = vect_init_vector (stmt_info, op1,\n \t\t\t\t\t      TREE_TYPE (vectype), NULL);\n \t\t    }\n \t\t}\n@@ -5722,11 +5740,11 @@ vectorizable_shift (gimple *stmt, gimple_stmt_iterator *gsi,\n              (a special case for certain kind of vector shifts); otherwise,\n              operand 1 should be of a vector type (the usual case).  */\n           if (vec_oprnd1)\n-            vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL,\n-                               slp_node);\n+\t    vect_get_vec_defs (op0, NULL_TREE, stmt_info, &vec_oprnds0, NULL,\n+\t\t\t       slp_node);\n           else\n-            vect_get_vec_defs (op0, op1, stmt, &vec_oprnds0, &vec_oprnds1,\n-                               slp_node);\n+\t    vect_get_vec_defs (op0, op1, stmt_info, &vec_oprnds0, &vec_oprnds1,\n+\t\t\t       slp_node);\n         }\n       else\n         vect_get_vec_defs_for_stmt_copy (dt, &vec_oprnds0, &vec_oprnds1);\n@@ -5739,7 +5757,8 @@ vectorizable_shift (gimple *stmt, gimple_stmt_iterator *gsi,\n \t  gassign *new_stmt = gimple_build_assign (vec_dest, code, vop0, vop1);\n           new_temp = make_ssa_name (vec_dest, new_stmt);\n           gimple_assign_set_lhs (new_stmt, new_temp);\n-\t  new_stmt_info = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t  new_stmt_info\n+\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n           if (slp_node)\n \t    SLP_TREE_VEC_STMTS (slp_node).quick_push (new_stmt_info);\n         }\n@@ -6076,7 +6095,7 @@ vectorizable_operation (gimple *stmt, gimple_stmt_iterator *gsi,\n       if (j == 0)\n \t{\n \t  if (op_type == binary_op)\n-\t    vect_get_vec_defs (op0, op1, stmt, &vec_oprnds0, &vec_oprnds1,\n+\t    vect_get_vec_defs (op0, op1, stmt_info, &vec_oprnds0, &vec_oprnds1,\n \t\t\t       slp_node);\n \t  else if (op_type == ternary_op)\n \t    {\n@@ -6094,14 +6113,14 @@ vectorizable_operation (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t}\n \t      else\n \t\t{\n-\t\t  vect_get_vec_defs (op0, op1, stmt, &vec_oprnds0, &vec_oprnds1,\n-\t\t\t\t     NULL);\n-\t\t  vect_get_vec_defs (op2, NULL_TREE, stmt, &vec_oprnds2, NULL,\n-\t\t\t\t     NULL);\n+\t\t  vect_get_vec_defs (op0, op1, stmt_info, &vec_oprnds0,\n+\t\t\t\t     &vec_oprnds1, NULL);\n+\t\t  vect_get_vec_defs (op2, NULL_TREE, stmt_info, &vec_oprnds2,\n+\t\t\t\t     NULL, NULL);\n \t\t}\n \t    }\n \t  else\n-\t    vect_get_vec_defs (op0, NULL_TREE, stmt, &vec_oprnds0, NULL,\n+\t    vect_get_vec_defs (op0, NULL_TREE, stmt_info, &vec_oprnds0, NULL,\n \t\t\t       slp_node);\n \t}\n       else\n@@ -6127,7 +6146,8 @@ vectorizable_operation (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t\t\t\t   vop0, vop1, vop2);\n \t  new_temp = make_ssa_name (vec_dest, new_stmt);\n \t  gimple_assign_set_lhs (new_stmt, new_temp);\n-\t  new_stmt_info = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t  new_stmt_info\n+\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t  if (vec_cvt_dest)\n \t    {\n \t      new_temp = build1 (VIEW_CONVERT_EXPR, vectype_out, new_temp);\n@@ -6137,7 +6157,7 @@ vectorizable_operation (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      new_temp = make_ssa_name (vec_cvt_dest, new_stmt);\n \t      gimple_assign_set_lhs (new_stmt, new_temp);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t    }\n           if (slp_node)\n \t    SLP_TREE_VEC_STMTS (slp_node).quick_push (new_stmt_info);\n@@ -6275,7 +6295,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n   /* Is vectorizable store? */\n \n   tree mask = NULL_TREE, mask_vectype = NULL_TREE;\n-  if (gassign *assign = dyn_cast <gassign *> (stmt))\n+  if (gassign *assign = dyn_cast <gassign *> (stmt_info->stmt))\n     {\n       tree scalar_dest = gimple_assign_lhs (assign);\n       if (TREE_CODE (scalar_dest) == VIEW_CONVERT_EXPR\n@@ -6292,7 +6312,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n     }\n   else\n     {\n-      gcall *call = dyn_cast <gcall *> (stmt);\n+      gcall *call = dyn_cast <gcall *> (stmt_info->stmt);\n       if (!call || !gimple_call_internal_p (call))\n \treturn false;\n \n@@ -6312,13 +6332,13 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n       if (mask_index >= 0)\n \t{\n \t  mask = gimple_call_arg (call, mask_index);\n-\t  if (!vect_check_load_store_mask (stmt, mask, &mask_dt,\n+\t  if (!vect_check_load_store_mask (stmt_info, mask, &mask_dt,\n \t\t\t\t\t   &mask_vectype))\n \t    return false;\n \t}\n     }\n \n-  op = vect_get_store_rhs (stmt);\n+  op = vect_get_store_rhs (stmt_info);\n \n   /* Cannot have hybrid store SLP -- that would mean storing to the\n      same location twice.  */\n@@ -6346,15 +6366,15 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n   gcc_assert (ncopies >= 1);\n \n   /* FORNOW.  This restriction should be relaxed.  */\n-  if (loop && nested_in_vect_loop_p (loop, stmt) && ncopies > 1)\n+  if (loop && nested_in_vect_loop_p (loop, stmt_info) && ncopies > 1)\n     {\n       if (dump_enabled_p ())\n \tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n \t\t\t \"multiple types in nested loop.\\n\");\n       return false;\n     }\n \n-  if (!vect_check_store_rhs (stmt, op, &rhs_dt, &rhs_vectype, &vls_type))\n+  if (!vect_check_store_rhs (stmt_info, op, &rhs_dt, &rhs_vectype, &vls_type))\n     return false;\n \n   elem_type = TREE_TYPE (vectype);\n@@ -6364,7 +6384,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n     return false;\n \n   vect_memory_access_type memory_access_type;\n-  if (!get_load_store_type (stmt, vectype, slp, mask, vls_type, ncopies,\n+  if (!get_load_store_type (stmt_info, vectype, slp, mask, vls_type, ncopies,\n \t\t\t    &memory_access_type, &gs_info))\n     return false;\n \n@@ -6501,7 +6521,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n       /* Currently we support only unconditional scatter stores,\n \t so mask should be all ones.  */\n       mask = build_int_cst (masktype, -1);\n-      mask = vect_init_vector (stmt, mask, masktype, NULL);\n+      mask = vect_init_vector (stmt_info, mask, masktype, NULL);\n \n       scale = build_int_cst (scaletype, gs_info.scale);\n \n@@ -6511,9 +6531,9 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t  if (j == 0)\n \t    {\n \t      src = vec_oprnd1\n-\t\t= vect_get_vec_def_for_operand (op, stmt);\n+\t\t= vect_get_vec_def_for_operand (op, stmt_info);\n \t      op = vec_oprnd0\n-\t\t= vect_get_vec_def_for_operand (gs_info.offset, stmt);\n+\t\t= vect_get_vec_def_for_operand (gs_info.offset, stmt_info);\n \t    }\n \t  else if (modifier != NONE && (j & 1))\n \t    {\n@@ -6522,12 +6542,12 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  src = vec_oprnd1\n \t\t    = vect_get_vec_def_for_stmt_copy (rhs_dt, vec_oprnd1);\n \t\t  op = permute_vec_elements (vec_oprnd0, vec_oprnd0, perm_mask,\n-\t\t\t\t\t     stmt, gsi);\n+\t\t\t\t\t     stmt_info, gsi);\n \t\t}\n \t      else if (modifier == NARROW)\n \t\t{\n \t\t  src = permute_vec_elements (vec_oprnd1, vec_oprnd1, perm_mask,\n-\t\t\t\t\t      stmt, gsi);\n+\t\t\t\t\t      stmt_info, gsi);\n \t\t  op = vec_oprnd0\n \t\t    = vect_get_vec_def_for_stmt_copy (gs_info.offset_dt,\n \t\t\t\t\t\t      vec_oprnd0);\n@@ -6552,7 +6572,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      src = build1 (VIEW_CONVERT_EXPR, srctype, src);\n \t      gassign *new_stmt\n \t\t= gimple_build_assign (var, VIEW_CONVERT_EXPR, src);\n-\t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t      vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t      src = var;\n \t    }\n \n@@ -6564,14 +6584,14 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      op = build1 (VIEW_CONVERT_EXPR, idxtype, op);\n \t      gassign *new_stmt\n \t\t= gimple_build_assign (var, VIEW_CONVERT_EXPR, op);\n-\t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t      vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t      op = var;\n \t    }\n \n \t  gcall *new_stmt\n \t    = gimple_build_call (gs_info.decl, 5, ptr, mask, op, src, scale);\n \t  stmt_vec_info new_stmt_info\n-\t    = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \n \t  if (prev_stmt_info == NULL_STMT_VEC_INFO)\n \t    STMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = new_stmt_info;\n@@ -6588,7 +6608,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n   if (grouped_store)\n     {\n       /* FORNOW */\n-      gcc_assert (!loop || !nested_in_vect_loop_p (loop, stmt));\n+      gcc_assert (!loop || !nested_in_vect_loop_p (loop, stmt_info));\n \n       /* We vectorize all the stmts of the interleaving group when we\n \t reach the last stmt in the group.  */\n@@ -6642,7 +6662,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n       unsigned int const_nunits = nunits.to_constant ();\n \n       gcc_assert (!LOOP_VINFO_FULLY_MASKED_P (loop_vinfo));\n-      gcc_assert (!nested_in_vect_loop_p (loop, stmt));\n+      gcc_assert (!nested_in_vect_loop_p (loop, stmt_info));\n \n       stride_base\n \t= fold_build_pointer_plus\n@@ -6768,7 +6788,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      tree newoff = copy_ssa_name (running_off, NULL);\n \t      incr = gimple_build_assign (newoff, POINTER_PLUS_EXPR,\n \t\t\t\t\t  running_off, pos);\n-\t      vect_finish_stmt_generation (stmt, incr, gsi);\n+\t      vect_finish_stmt_generation (stmt_info, incr, gsi);\n \t      running_off = newoff;\n \t    }\n \t  unsigned int group_el = 0;\n@@ -6782,8 +6802,8 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t{\n \t\t  if (slp)\n \t\t    {\n-\t\t      vect_get_vec_defs (op, NULL_TREE, stmt, &vec_oprnds, NULL,\n-\t\t\t\t\t slp_node);\n+\t\t      vect_get_vec_defs (op, NULL_TREE, stmt_info,\n+\t\t\t\t\t &vec_oprnds, NULL, slp_node);\n \t\t      vec_oprnd = vec_oprnds[0];\n \t\t    }\n \t\t  else\n@@ -6811,7 +6831,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  gimple *pun\n \t\t    = gimple_build_assign (tem, build1 (VIEW_CONVERT_EXPR,\n \t\t\t\t\t\t\tlvectype, vec_oprnd));\n-\t\t  vect_finish_stmt_generation (stmt, pun, gsi);\n+\t\t  vect_finish_stmt_generation (stmt_info, pun, gsi);\n \t\t  vec_oprnd = tem;\n \t\t}\n \t      for (i = 0; i < nstores; i++)\n@@ -6838,7 +6858,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  /* And store it to *running_off.  */\n \t\t  assign = gimple_build_assign (newref, elem);\n \t\t  stmt_vec_info assign_info\n-\t\t    = vect_finish_stmt_generation (stmt, assign, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, assign, gsi);\n \n \t\t  group_el += lnel;\n \t\t  if (! slp\n@@ -6847,7 +6867,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t      newoff = copy_ssa_name (running_off, NULL);\n \t\t      incr = gimple_build_assign (newoff, POINTER_PLUS_EXPR,\n \t\t\t\t\t\t  running_off, stride_step);\n-\t\t      vect_finish_stmt_generation (stmt, incr, gsi);\n+\t\t      vect_finish_stmt_generation (stmt_info, incr, gsi);\n \n \t\t      running_off = newoff;\n \t\t      group_el = 0;\n@@ -6905,7 +6925,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n   else if (memory_access_type == VMAT_GATHER_SCATTER)\n     {\n       aggr_type = elem_type;\n-      vect_get_strided_load_store_ops (stmt, loop_vinfo, &gs_info,\n+      vect_get_strided_load_store_ops (stmt_info, loop_vinfo, &gs_info,\n \t\t\t\t       &bump, &vec_offset);\n     }\n   else\n@@ -6969,8 +6989,8 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n           if (slp)\n             {\n \t      /* Get vectorized arguments for SLP_NODE.  */\n-              vect_get_vec_defs (op, NULL_TREE, stmt, &vec_oprnds,\n-                                 NULL, slp_node);\n+\t      vect_get_vec_defs (op, NULL_TREE, stmt_info, &vec_oprnds,\n+\t\t\t\t NULL, slp_node);\n \n               vec_oprnd = vec_oprnds[0];\n             }\n@@ -6999,7 +7019,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  next_stmt_info = DR_GROUP_NEXT_ELEMENT (next_stmt_info);\n \t\t}\n \t      if (mask)\n-\t\tvec_mask = vect_get_vec_def_for_operand (mask, stmt,\n+\t\tvec_mask = vect_get_vec_def_for_operand (mask, stmt_info,\n \t\t\t\t\t\t\t mask_vectype);\n \t    }\n \n@@ -7022,7 +7042,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t    }\n \t  else if (STMT_VINFO_GATHER_SCATTER_P (stmt_info))\n \t    {\n-\t      vect_get_gather_scatter_ops (loop, stmt, &gs_info,\n+\t      vect_get_gather_scatter_ops (loop, stmt_info, &gs_info,\n \t\t\t\t\t   &dataref_ptr, &vec_offset);\n \t      inv_p = false;\n \t    }\n@@ -7061,8 +7081,8 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t    vec_offset = vect_get_vec_def_for_stmt_copy (gs_info.offset_dt,\n \t\t\t\t\t\t\t vec_offset);\n \t  else\n-\t    dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi, stmt,\n-\t\t\t\t\t   bump);\n+\t    dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi,\n+\t\t\t\t\t   stmt_info, bump);\n \t}\n \n       if (memory_access_type == VMAT_LOAD_STORE_LANES)\n@@ -7075,13 +7095,13 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t  /* Invalidate the current contents of VEC_ARRAY.  This should\n \t     become an RTL clobber too, which prevents the vector registers\n \t     from being upward-exposed.  */\n-\t  vect_clobber_variable (stmt, gsi, vec_array);\n+\t  vect_clobber_variable (stmt_info, gsi, vec_array);\n \n \t  /* Store the individual vectors into the array.  */\n \t  for (i = 0; i < vec_num; i++)\n \t    {\n \t      vec_oprnd = dr_chain[i];\n-\t      write_vector_array (stmt, gsi, vec_oprnd, vec_array, i);\n+\t      write_vector_array (stmt_info, gsi, vec_oprnd, vec_array, i);\n \t    }\n \n \t  tree final_mask = NULL;\n@@ -7114,10 +7134,10 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      gimple_call_set_lhs (call, data_ref);\n \t    }\n \t  gimple_call_set_nothrow (call, true);\n-\t  new_stmt_info = vect_finish_stmt_generation (stmt, call, gsi);\n+\t  new_stmt_info = vect_finish_stmt_generation (stmt_info, call, gsi);\n \n \t  /* Record that VEC_ARRAY is now dead.  */\n-\t  vect_clobber_variable (stmt, gsi, vec_array);\n+\t  vect_clobber_variable (stmt_info, gsi, vec_array);\n \t}\n       else\n \t{\n@@ -7127,7 +7147,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      if (j == 0)\n \t\tresult_chain.create (group_size);\n \t      /* Permute.  */\n-\t      vect_permute_store_chain (dr_chain, group_size, stmt, gsi,\n+\t      vect_permute_store_chain (dr_chain, group_size, stmt_info, gsi,\n \t\t\t\t\t&result_chain);\n \t    }\n \n@@ -7159,14 +7179,14 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t       scale, vec_oprnd);\n \t\t  gimple_call_set_nothrow (call, true);\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, call, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, call, gsi);\n \t\t  break;\n \t\t}\n \n \t      if (i > 0)\n \t\t/* Bump the vector pointer.  */\n \t\tdataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi,\n-\t\t\t\t\t       stmt, bump);\n+\t\t\t\t\t       stmt_info, bump);\n \n \t      if (slp)\n \t\tvec_oprnd = vec_oprnds[i];\n@@ -7193,16 +7213,15 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      if (memory_access_type == VMAT_CONTIGUOUS_REVERSE)\n \t\t{\n \t\t  tree perm_mask = perm_mask_for_reverse (vectype);\n-\t\t  tree perm_dest \n-\t\t    = vect_create_destination_var (vect_get_store_rhs (stmt),\n-\t\t\t\t\t\t   vectype);\n+\t\t  tree perm_dest = vect_create_destination_var\n+\t\t    (vect_get_store_rhs (stmt_info), vectype);\n \t\t  tree new_temp = make_ssa_name (perm_dest);\n \n \t\t  /* Generate the permute statement.  */\n \t\t  gimple *perm_stmt \n \t\t    = gimple_build_assign (new_temp, VEC_PERM_EXPR, vec_oprnd,\n \t\t\t\t\t   vec_oprnd, perm_mask);\n-\t\t  vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+\t\t  vect_finish_stmt_generation (stmt_info, perm_stmt, gsi);\n \n \t\t  perm_stmt = SSA_NAME_DEF_STMT (new_temp);\n \t\t  vec_oprnd = new_temp;\n@@ -7219,7 +7238,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t\t\t\t  final_mask, vec_oprnd);\n \t\t  gimple_call_set_nothrow (call, true);\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, call, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, call, gsi);\n \t\t}\n \t      else\n \t\t{\n@@ -7242,7 +7261,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  gassign *new_stmt\n \t\t    = gimple_build_assign (data_ref, vec_oprnd);\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t}\n \n \t      if (slp)\n@@ -7446,7 +7465,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n     return false;\n \n   tree mask = NULL_TREE, mask_vectype = NULL_TREE;\n-  if (gassign *assign = dyn_cast <gassign *> (stmt))\n+  if (gassign *assign = dyn_cast <gassign *> (stmt_info->stmt))\n     {\n       scalar_dest = gimple_assign_lhs (assign);\n       if (TREE_CODE (scalar_dest) != SSA_NAME)\n@@ -7465,7 +7484,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n     }\n   else\n     {\n-      gcall *call = dyn_cast <gcall *> (stmt);\n+      gcall *call = dyn_cast <gcall *> (stmt_info->stmt);\n       if (!call || !gimple_call_internal_p (call))\n \treturn false;\n \n@@ -7489,7 +7508,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n       if (mask_index >= 0)\n \t{\n \t  mask = gimple_call_arg (call, mask_index);\n-\t  if (!vect_check_load_store_mask (stmt, mask, &mask_dt,\n+\t  if (!vect_check_load_store_mask (stmt_info, mask, &mask_dt,\n \t\t\t\t\t   &mask_vectype))\n \t    return false;\n \t}\n@@ -7504,7 +7523,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n   if (loop_vinfo)\n     {\n       loop = LOOP_VINFO_LOOP (loop_vinfo);\n-      nested_in_vect_loop = nested_in_vect_loop_p (loop, stmt);\n+      nested_in_vect_loop = nested_in_vect_loop_p (loop, stmt_info);\n       vf = LOOP_VINFO_VECT_FACTOR (loop_vinfo);\n     }\n   else\n@@ -7601,7 +7620,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n     group_size = 1;\n \n   vect_memory_access_type memory_access_type;\n-  if (!get_load_store_type (stmt, vectype, slp, mask, VLS_LOAD, ncopies,\n+  if (!get_load_store_type (stmt_info, vectype, slp, mask, VLS_LOAD, ncopies,\n \t\t\t    &memory_access_type, &gs_info))\n     return false;\n \n@@ -7669,7 +7688,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \n   if (memory_access_type == VMAT_GATHER_SCATTER && gs_info.decl)\n     {\n-      vect_build_gather_load_calls (stmt, gsi, vec_stmt, &gs_info, mask,\n+      vect_build_gather_load_calls (stmt_info, gsi, vec_stmt, &gs_info, mask,\n \t\t\t\t    mask_dt);\n       return true;\n     }\n@@ -7712,7 +7731,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t  if (grouped_load)\n \t    cst_offset\n \t      = (tree_to_uhwi (TYPE_SIZE_UNIT (TREE_TYPE (vectype)))\n-\t\t * vect_get_place_in_interleaving_chain (stmt,\n+\t\t * vect_get_place_in_interleaving_chain (stmt_info,\n \t\t\t\t\t\t\t first_stmt_info));\n \t  group_size = 1;\n \t  ref_type = reference_alias_ptr_type (DR_REF (dr));\n@@ -7857,7 +7876,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      gassign *new_stmt\n \t\t= gimple_build_assign (make_ssa_name (ltype), data_ref);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t      if (nloads > 1)\n \t\tCONSTRUCTOR_APPEND_ELT (v, NULL_TREE,\n \t\t\t\t\tgimple_assign_lhs (new_stmt));\n@@ -7869,7 +7888,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  tree newoff = copy_ssa_name (running_off);\n \t\t  gimple *incr = gimple_build_assign (newoff, POINTER_PLUS_EXPR,\n \t\t\t\t\t\t      running_off, stride_step);\n-\t\t  vect_finish_stmt_generation (stmt, incr, gsi);\n+\t\t  vect_finish_stmt_generation (stmt_info, incr, gsi);\n \n \t\t  running_off = newoff;\n \t\t  group_el = 0;\n@@ -7878,7 +7897,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t  if (nloads > 1)\n \t    {\n \t      tree vec_inv = build_constructor (lvectype, v);\n-\t      new_temp = vect_init_vector (stmt, vec_inv, lvectype, gsi);\n+\t      new_temp = vect_init_vector (stmt_info, vec_inv, lvectype, gsi);\n \t      new_stmt_info = vinfo->lookup_def (new_temp);\n \t      if (lvectype != vectype)\n \t\t{\n@@ -7888,7 +7907,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t\t\t   build1 (VIEW_CONVERT_EXPR,\n \t\t\t\t\t\t   vectype, new_temp));\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t}\n \t    }\n \n@@ -8145,7 +8164,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n   else if (memory_access_type == VMAT_GATHER_SCATTER)\n     {\n       aggr_type = elem_type;\n-      vect_get_strided_load_store_ops (stmt, loop_vinfo, &gs_info,\n+      vect_get_strided_load_store_ops (stmt_info, loop_vinfo, &gs_info,\n \t\t\t\t       &bump, &vec_offset);\n     }\n   else\n@@ -8198,11 +8217,11 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t\t\t\t    DR_INIT (first_dr),\n \t\t\t\t\t\t    DR_INIT (ptrdr)));\n \t      dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi,\n-\t\t\t\t\t     stmt, diff);\n+\t\t\t\t\t     stmt_info, diff);\n \t    }\n \t  else if (STMT_VINFO_GATHER_SCATTER_P (stmt_info))\n \t    {\n-\t      vect_get_gather_scatter_ops (loop, stmt, &gs_info,\n+\t      vect_get_gather_scatter_ops (loop, stmt_info, &gs_info,\n \t\t\t\t\t   &dataref_ptr, &vec_offset);\n \t      inv_p = false;\n \t    }\n@@ -8213,7 +8232,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t\t\t  simd_lane_access_p, &inv_p,\n \t\t\t\t\t  byte_offset, bump);\n \t  if (mask)\n-\t    vec_mask = vect_get_vec_def_for_operand (mask, stmt,\n+\t    vec_mask = vect_get_vec_def_for_operand (mask, stmt_info,\n \t\t\t\t\t\t     mask_vectype);\n \t}\n       else\n@@ -8226,7 +8245,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t\t\t\t\t vec_offset);\n \t  else\n \t    dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi,\n-\t\t\t\t\t   stmt, bump);\n+\t\t\t\t\t   stmt_info, bump);\n \t  if (mask)\n \t    vec_mask = vect_get_vec_def_for_stmt_copy (mask_dt, vec_mask);\n \t}\n@@ -8269,21 +8288,21 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t    }\n \t  gimple_call_set_lhs (call, vec_array);\n \t  gimple_call_set_nothrow (call, true);\n-\t  new_stmt_info = vect_finish_stmt_generation (stmt, call, gsi);\n+\t  new_stmt_info = vect_finish_stmt_generation (stmt_info, call, gsi);\n \n \t  /* Extract each vector into an SSA_NAME.  */\n \t  for (i = 0; i < vec_num; i++)\n \t    {\n-\t      new_temp = read_vector_array (stmt, gsi, scalar_dest,\n+\t      new_temp = read_vector_array (stmt_info, gsi, scalar_dest,\n \t\t\t\t\t    vec_array, i);\n \t      dr_chain.quick_push (new_temp);\n \t    }\n \n \t  /* Record the mapping between SSA_NAMEs and statements.  */\n-\t  vect_record_grouped_load_vectors (stmt, dr_chain);\n+\t  vect_record_grouped_load_vectors (stmt_info, dr_chain);\n \n \t  /* Record that VEC_ARRAY is now dead.  */\n-\t  vect_clobber_variable (stmt, gsi, vec_array);\n+\t  vect_clobber_variable (stmt_info, gsi, vec_array);\n \t}\n       else\n \t{\n@@ -8301,7 +8320,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \n \t      if (i > 0)\n \t\tdataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi,\n-\t\t\t\t\t       stmt, bump);\n+\t\t\t\t\t       stmt_info, bump);\n \n \t      /* 2. Create the vector-load in the loop.  */\n \t      gimple *new_stmt = NULL;\n@@ -8402,7 +8421,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t\t  build_int_cst\n \t\t\t\t  (TREE_TYPE (dataref_ptr),\n \t\t\t\t   -(HOST_WIDE_INT) align));\n-\t\t    vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t    data_ref\n \t\t      = build2 (MEM_REF, vectype, ptr,\n \t\t\t\tbuild_int_cst (ref_type, 0));\n@@ -8412,22 +8431,23 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t    new_stmt = gimple_build_assign (vec_dest, data_ref);\n \t\t    new_temp = make_ssa_name (vec_dest, new_stmt);\n \t\t    gimple_assign_set_lhs (new_stmt, new_temp);\n-\t\t    gimple_set_vdef (new_stmt, gimple_vdef (stmt));\n-\t\t    gimple_set_vuse (new_stmt, gimple_vuse (stmt));\n-\t\t    vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    gimple_set_vdef (new_stmt, gimple_vdef (stmt_info->stmt));\n+\t\t    gimple_set_vuse (new_stmt, gimple_vuse (stmt_info->stmt));\n+\t\t    vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t    msq = new_temp;\n \n \t\t    bump = size_binop (MULT_EXPR, vs,\n \t\t\t\t       TYPE_SIZE_UNIT (elem_type));\n \t\t    bump = size_binop (MINUS_EXPR, bump, size_one_node);\n-\t\t    ptr = bump_vector_ptr (dataref_ptr, NULL, gsi, stmt, bump);\n+\t\t    ptr = bump_vector_ptr (dataref_ptr, NULL, gsi,\n+\t\t\t\t\t   stmt_info, bump);\n \t\t    new_stmt = gimple_build_assign\n \t\t\t\t (NULL_TREE, BIT_AND_EXPR, ptr,\n \t\t\t\t  build_int_cst\n \t\t\t\t  (TREE_TYPE (ptr), -(HOST_WIDE_INT) align));\n \t\t    ptr = copy_ssa_name (ptr, new_stmt);\n \t\t    gimple_assign_set_lhs (new_stmt, ptr);\n-\t\t    vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t    data_ref\n \t\t      = build2 (MEM_REF, vectype, ptr,\n \t\t\t\tbuild_int_cst (ref_type, 0));\n@@ -8444,7 +8464,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t      (new_temp, BIT_AND_EXPR, dataref_ptr,\n \t\t       build_int_cst (TREE_TYPE (dataref_ptr),\n \t\t\t\t     -(HOST_WIDE_INT) align));\n-\t\t    vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t    data_ref\n \t\t      = build2 (MEM_REF, vectype, new_temp,\n \t\t\t\tbuild_int_cst (ref_type, 0));\n@@ -8463,7 +8483,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      new_temp = make_ssa_name (vec_dest, new_stmt);\n \t      gimple_set_lhs (new_stmt, new_temp);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \n \t      /* 3. Handle explicit realignment if necessary/supported.\n \t\t Create in loop:\n@@ -8480,7 +8500,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  new_temp = make_ssa_name (vec_dest, new_stmt);\n \t\t  gimple_assign_set_lhs (new_stmt, new_temp);\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \n \t\t  if (alignment_support_scheme == dr_explicit_realign_optimized)\n \t\t    {\n@@ -8503,7 +8523,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t     thus we can insert it on the preheader edge.  */\n \t\t  if (LOOP_VINFO_NO_DATA_DEPENDENCIES (loop_vinfo)\n \t\t      && !nested_in_vect_loop\n-\t\t      && hoist_defs_of_uses (stmt, loop))\n+\t\t      && hoist_defs_of_uses (stmt_info, loop))\n \t\t    {\n \t\t      if (dump_enabled_p ())\n \t\t\t{\n@@ -8518,15 +8538,16 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t gimple_build_assign (tem,\n \t\t\t\t\t      unshare_expr\n \t\t\t\t\t        (gimple_assign_rhs1 (stmt))));\n-\t\t      new_temp = vect_init_vector (stmt, tem, vectype, NULL);\n+\t\t      new_temp = vect_init_vector (stmt_info, tem,\n+\t\t\t\t\t\t   vectype, NULL);\n \t\t      new_stmt = SSA_NAME_DEF_STMT (new_temp);\n \t\t      new_stmt_info = vinfo->add_stmt (new_stmt);\n \t\t    }\n \t\t  else\n \t\t    {\n \t\t      gimple_stmt_iterator gsi2 = *gsi;\n \t\t      gsi_next (&gsi2);\n-\t\t      new_temp = vect_init_vector (stmt, scalar_dest,\n+\t\t      new_temp = vect_init_vector (stmt_info, scalar_dest,\n \t\t\t\t\t\t   vectype, &gsi2);\n \t\t      new_stmt_info = vinfo->lookup_def (new_temp);\n \t\t    }\n@@ -8536,7 +8557,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t{\n \t\t  tree perm_mask = perm_mask_for_reverse (vectype);\n \t\t  new_temp = permute_vec_elements (new_temp, new_temp,\n-\t\t\t\t\t\t   perm_mask, stmt, gsi);\n+\t\t\t\t\t\t   perm_mask, stmt_info, gsi);\n \t\t  new_stmt_info = vinfo->lookup_def (new_temp);\n \t\t}\n \n@@ -8562,7 +8583,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t       * group_gap_adj);\n \t\t  tree bump = wide_int_to_tree (sizetype, bump_val);\n \t\t  dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi,\n-\t\t\t\t\t\t stmt, bump);\n+\t\t\t\t\t\t stmt_info, bump);\n \t\t  group_elt = 0;\n \t\t}\n \t    }\n@@ -8575,7 +8596,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t   * group_gap_adj);\n \t      tree bump = wide_int_to_tree (sizetype, bump_val);\n \t      dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi,\n-\t\t\t\t\t     stmt, bump);\n+\t\t\t\t\t     stmt_info, bump);\n \t    }\n \t}\n \n@@ -8598,7 +8619,8 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi,\n           if (grouped_load)\n   \t    {\n \t      if (memory_access_type != VMAT_LOAD_STORE_LANES)\n-\t\tvect_transform_grouped_load (stmt, dr_chain, group_size, gsi);\n+\t\tvect_transform_grouped_load (stmt_info, dr_chain,\n+\t\t\t\t\t     group_size, gsi);\n \t      *vec_stmt = STMT_VINFO_VEC_STMT (stmt_info);\n \t    }\n           else\n@@ -8942,36 +8964,36 @@ vectorizable_condition (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      if (masked)\n \t\t{\n \t\t  vec_cond_lhs\n-\t\t    = vect_get_vec_def_for_operand (cond_expr, stmt,\n+\t\t    = vect_get_vec_def_for_operand (cond_expr, stmt_info,\n \t\t\t\t\t\t    comp_vectype);\n \t\t  vect_is_simple_use (cond_expr, stmt_info->vinfo, &dts[0]);\n \t\t}\n \t      else\n \t\t{\n \t\t  vec_cond_lhs\n \t\t    = vect_get_vec_def_for_operand (cond_expr0,\n-\t\t\t\t\t\t    stmt, comp_vectype);\n+\t\t\t\t\t\t    stmt_info, comp_vectype);\n \t\t  vect_is_simple_use (cond_expr0, loop_vinfo, &dts[0]);\n \n \t\t  vec_cond_rhs\n \t\t    = vect_get_vec_def_for_operand (cond_expr1,\n-\t\t\t\t\t\t    stmt, comp_vectype);\n+\t\t\t\t\t\t    stmt_info, comp_vectype);\n \t\t  vect_is_simple_use (cond_expr1, loop_vinfo, &dts[1]);\n \t\t}\n \t      if (reduc_index == 1)\n \t\tvec_then_clause = reduc_def;\n \t      else\n \t\t{\n \t\t  vec_then_clause = vect_get_vec_def_for_operand (then_clause,\n-\t\t\t\t\t\t\t\t  stmt);\n+\t\t\t\t\t\t\t\t  stmt_info);\n \t\t  vect_is_simple_use (then_clause, loop_vinfo, &dts[2]);\n \t\t}\n \t      if (reduc_index == 2)\n \t\tvec_else_clause = reduc_def;\n \t      else\n \t\t{\n \t\t  vec_else_clause = vect_get_vec_def_for_operand (else_clause,\n-\t\t\t\t\t\t\t\t  stmt);\n+\t\t\t\t\t\t\t\t  stmt_info);\n \t\t  vect_is_simple_use (else_clause, loop_vinfo, &dts[3]);\n \t\t}\n \t    }\n@@ -9026,7 +9048,7 @@ vectorizable_condition (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t    new_stmt\n \t\t      = gimple_build_assign (new_temp, bitop1, vec_cond_lhs,\n \t\t\t\t\t     vec_cond_rhs);\n-\t\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t  vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t  if (bitop2 == NOP_EXPR)\n \t\t    vec_compare = new_temp;\n \t\t  else if (bitop2 == BIT_NOT_EXPR)\n@@ -9041,7 +9063,7 @@ vectorizable_condition (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t      new_stmt\n \t\t\t= gimple_build_assign (vec_compare, bitop2,\n \t\t\t\t\t       vec_cond_lhs, new_temp);\n-\t\t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t      vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t    }\n \t\t}\n \t    }\n@@ -9052,7 +9074,7 @@ vectorizable_condition (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t  tree vec_compare_name = make_ssa_name (vec_cmp_type);\n \t\t  gassign *new_stmt = gimple_build_assign (vec_compare_name,\n \t\t\t\t\t\t\t   vec_compare);\n-\t\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t  vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t  vec_compare = vec_compare_name;\n \t\t}\n \t      gcc_assert (reduc_index == 2);\n@@ -9061,17 +9083,18 @@ vectorizable_condition (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t vec_then_clause);\n \t      gimple_call_set_lhs (new_stmt, scalar_dest);\n \t      SSA_NAME_DEF_STMT (scalar_dest) = new_stmt;\n-\t      if (stmt == gsi_stmt (*gsi))\n-\t\tnew_stmt_info = vect_finish_replace_stmt (stmt, new_stmt);\n+\t      if (stmt_info->stmt == gsi_stmt (*gsi))\n+\t\tnew_stmt_info = vect_finish_replace_stmt (stmt_info, new_stmt);\n \t      else\n \t\t{\n \t\t  /* In this case we're moving the definition to later in the\n \t\t     block.  That doesn't matter because the only uses of the\n \t\t     lhs are in phi statements.  */\n-\t\t  gimple_stmt_iterator old_gsi = gsi_for_stmt (stmt);\n+\t\t  gimple_stmt_iterator old_gsi\n+\t\t    = gsi_for_stmt (stmt_info->stmt);\n \t\t  gsi_remove (&old_gsi, true);\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t}\n \t    }\n \t  else\n@@ -9081,7 +9104,7 @@ vectorizable_condition (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t= gimple_build_assign (new_temp, VEC_COND_EXPR, vec_compare,\n \t\t\t\t       vec_then_clause, vec_else_clause);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t    }\n           if (slp_node)\n \t    SLP_TREE_VEC_STMTS (slp_node).quick_push (new_stmt_info);\n@@ -9307,8 +9330,10 @@ vectorizable_comparison (gimple *stmt, gimple_stmt_iterator *gsi,\n \t    }\n \t  else\n \t    {\n-\t      vec_rhs1 = vect_get_vec_def_for_operand (rhs1, stmt, vectype);\n-\t      vec_rhs2 = vect_get_vec_def_for_operand (rhs2, stmt, vectype);\n+\t      vec_rhs1 = vect_get_vec_def_for_operand (rhs1, stmt_info,\n+\t\t\t\t\t\t       vectype);\n+\t      vec_rhs2 = vect_get_vec_def_for_operand (rhs2, stmt_info,\n+\t\t\t\t\t\t       vectype);\n \t    }\n \t}\n       else\n@@ -9336,7 +9361,7 @@ vectorizable_comparison (gimple *stmt, gimple_stmt_iterator *gsi,\n \t      gassign *new_stmt = gimple_build_assign (new_temp, code,\n \t\t\t\t\t\t       vec_rhs1, vec_rhs2);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t    }\n \t  else\n \t    {\n@@ -9347,7 +9372,7 @@ vectorizable_comparison (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\tnew_stmt = gimple_build_assign (new_temp, bitop1, vec_rhs1,\n \t\t\t\t\t\tvec_rhs2);\n \t      new_stmt_info\n-\t\t= vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t= vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t      if (bitop2 != NOP_EXPR)\n \t\t{\n \t\t  tree res = make_ssa_name (mask);\n@@ -9357,7 +9382,7 @@ vectorizable_comparison (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t    new_stmt = gimple_build_assign (res, bitop2, vec_rhs1,\n \t\t\t\t\t\t    new_temp);\n \t\t  new_stmt_info\n-\t\t    = vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t\t    = vect_finish_stmt_generation (stmt_info, new_stmt, gsi);\n \t\t}\n \t    }\n \t  if (slp_node)\n@@ -9427,10 +9452,10 @@ vect_analyze_stmt (gimple *stmt, bool *need_to_vectorize, slp_tree node,\n   if (dump_enabled_p ())\n     {\n       dump_printf_loc (MSG_NOTE, vect_location, \"==> examining statement: \");\n-      dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt, 0);\n+      dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt_info->stmt, 0);\n     }\n \n-  if (gimple_has_volatile_ops (stmt))\n+  if (gimple_has_volatile_ops (stmt_info->stmt))\n     {\n       if (dump_enabled_p ())\n         dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n@@ -9447,7 +9472,6 @@ vect_analyze_stmt (gimple *stmt, bool *need_to_vectorize, slp_tree node,\n \n       for (si = gsi_start (pattern_def_seq); !gsi_end_p (si); gsi_next (&si))\n \t{\n-\t  gimple *pattern_def_stmt = gsi_stmt (si);\n \t  stmt_vec_info pattern_def_stmt_info\n \t    = vinfo->lookup_stmt (gsi_stmt (si));\n \t  if (STMT_VINFO_RELEVANT_P (pattern_def_stmt_info)\n@@ -9458,10 +9482,11 @@ vect_analyze_stmt (gimple *stmt, bool *need_to_vectorize, slp_tree node,\n \t\t{\n \t\t  dump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t\t   \"==> examining pattern def statement: \");\n-\t\t  dump_gimple_stmt (MSG_NOTE, TDF_SLIM, pattern_def_stmt, 0);\n+\t\t  dump_gimple_stmt (MSG_NOTE, TDF_SLIM,\n+\t\t\t\t    pattern_def_stmt_info->stmt, 0);\n \t\t}\n \n-\t      if (!vect_analyze_stmt (pattern_def_stmt,\n+\t      if (!vect_analyze_stmt (pattern_def_stmt_info,\n \t\t\t\t      need_to_vectorize, node, node_instance,\n \t\t\t\t      cost_vec))\n \t\treturn false;\n@@ -9499,7 +9524,7 @@ vect_analyze_stmt (gimple *stmt, bool *need_to_vectorize, slp_tree node,\n             {\n               dump_printf_loc (MSG_NOTE, vect_location,\n                                \"==> examining pattern statement: \");\n-              dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt, 0);\n+\t      dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt_info->stmt, 0);\n             }\n         }\n       else\n@@ -9521,7 +9546,7 @@ vect_analyze_stmt (gimple *stmt, bool *need_to_vectorize, slp_tree node,\n         {\n           dump_printf_loc (MSG_NOTE, vect_location,\n                            \"==> examining pattern statement: \");\n-          dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt, 0);\n+\t  dump_gimple_stmt (MSG_NOTE, TDF_SLIM, pattern_stmt_info->stmt, 0);\n         }\n \n       if (!vect_analyze_stmt (pattern_stmt_info, need_to_vectorize, node,\n@@ -9557,8 +9582,9 @@ vect_analyze_stmt (gimple *stmt, bool *need_to_vectorize, slp_tree node,\n \n   if (STMT_VINFO_RELEVANT_P (stmt_info))\n     {\n-      gcc_assert (!VECTOR_MODE_P (TYPE_MODE (gimple_expr_type (stmt))));\n-      gcall *call = dyn_cast <gcall *> (stmt);\n+      tree type = gimple_expr_type (stmt_info->stmt);\n+      gcc_assert (!VECTOR_MODE_P (TYPE_MODE (type)));\n+      gcall *call = dyn_cast <gcall *> (stmt_info->stmt);\n       gcc_assert (STMT_VINFO_VECTYPE (stmt_info)\n \t\t  || (call && gimple_call_lhs (call) == NULL_TREE));\n       *need_to_vectorize = true;\n@@ -9575,34 +9601,40 @@ vect_analyze_stmt (gimple *stmt, bool *need_to_vectorize, slp_tree node,\n   if (!bb_vinfo\n       && (STMT_VINFO_RELEVANT_P (stmt_info)\n \t  || STMT_VINFO_DEF_TYPE (stmt_info) == vect_reduction_def))\n-    ok = (vectorizable_simd_clone_call (stmt, NULL, NULL, node, cost_vec)\n-\t  || vectorizable_conversion (stmt, NULL, NULL, node, cost_vec)\n-\t  || vectorizable_shift (stmt, NULL, NULL, node, cost_vec)\n-\t  || vectorizable_operation (stmt, NULL, NULL, node, cost_vec)\n-\t  || vectorizable_assignment (stmt, NULL, NULL, node, cost_vec)\n-\t  || vectorizable_load (stmt, NULL, NULL, node, node_instance, cost_vec)\n-\t  || vectorizable_call (stmt, NULL, NULL, node, cost_vec)\n-\t  || vectorizable_store (stmt, NULL, NULL, node, cost_vec)\n-\t  || vectorizable_reduction (stmt, NULL, NULL, node, node_instance,\n+    ok = (vectorizable_simd_clone_call (stmt_info, NULL, NULL, node, cost_vec)\n+\t  || vectorizable_conversion (stmt_info, NULL, NULL, node, cost_vec)\n+\t  || vectorizable_shift (stmt_info, NULL, NULL, node, cost_vec)\n+\t  || vectorizable_operation (stmt_info, NULL, NULL, node, cost_vec)\n+\t  || vectorizable_assignment (stmt_info, NULL, NULL, node, cost_vec)\n+\t  || vectorizable_load (stmt_info, NULL, NULL, node, node_instance,\n+\t\t\t\tcost_vec)\n+\t  || vectorizable_call (stmt_info, NULL, NULL, node, cost_vec)\n+\t  || vectorizable_store (stmt_info, NULL, NULL, node, cost_vec)\n+\t  || vectorizable_reduction (stmt_info, NULL, NULL, node,\n+\t\t\t\t     node_instance, cost_vec)\n+\t  || vectorizable_induction (stmt_info, NULL, NULL, node, cost_vec)\n+\t  || vectorizable_condition (stmt_info, NULL, NULL, NULL, 0, node,\n \t\t\t\t     cost_vec)\n-\t  || vectorizable_induction (stmt, NULL, NULL, node, cost_vec)\n-\t  || vectorizable_condition (stmt, NULL, NULL, NULL, 0, node, cost_vec)\n-\t  || vectorizable_comparison (stmt, NULL, NULL, NULL, node, cost_vec));\n+\t  || vectorizable_comparison (stmt_info, NULL, NULL, NULL, node,\n+\t\t\t\t      cost_vec));\n   else\n     {\n       if (bb_vinfo)\n-\tok = (vectorizable_simd_clone_call (stmt, NULL, NULL, node, cost_vec)\n-\t      || vectorizable_conversion (stmt, NULL, NULL, node, cost_vec)\n-\t      || vectorizable_shift (stmt, NULL, NULL, node, cost_vec)\n-\t      || vectorizable_operation (stmt, NULL, NULL, node, cost_vec)\n-\t      || vectorizable_assignment (stmt, NULL, NULL, node, cost_vec)\n-\t      || vectorizable_load (stmt, NULL, NULL, node, node_instance,\n+\tok = (vectorizable_simd_clone_call (stmt_info, NULL, NULL, node,\n+\t\t\t\t\t    cost_vec)\n+\t      || vectorizable_conversion (stmt_info, NULL, NULL, node,\n+\t\t\t\t\t  cost_vec)\n+\t      || vectorizable_shift (stmt_info, NULL, NULL, node, cost_vec)\n+\t      || vectorizable_operation (stmt_info, NULL, NULL, node, cost_vec)\n+\t      || vectorizable_assignment (stmt_info, NULL, NULL, node,\n+\t\t\t\t\t  cost_vec)\n+\t      || vectorizable_load (stmt_info, NULL, NULL, node, node_instance,\n \t\t\t\t    cost_vec)\n-\t      || vectorizable_call (stmt, NULL, NULL, node, cost_vec)\n-\t      || vectorizable_store (stmt, NULL, NULL, node, cost_vec)\n-\t      || vectorizable_condition (stmt, NULL, NULL, NULL, 0, node,\n+\t      || vectorizable_call (stmt_info, NULL, NULL, node, cost_vec)\n+\t      || vectorizable_store (stmt_info, NULL, NULL, node, cost_vec)\n+\t      || vectorizable_condition (stmt_info, NULL, NULL, NULL, 0, node,\n \t\t\t\t\t cost_vec)\n-\t      || vectorizable_comparison (stmt, NULL, NULL, NULL, node,\n+\t      || vectorizable_comparison (stmt_info, NULL, NULL, NULL, node,\n \t\t\t\t\t  cost_vec));\n     }\n \n@@ -9613,7 +9645,8 @@ vect_analyze_stmt (gimple *stmt, bool *need_to_vectorize, slp_tree node,\n           dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n                            \"not vectorized: relevant stmt not \");\n           dump_printf (MSG_MISSED_OPTIMIZATION, \"supported: \");\n-          dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM, stmt, 0);\n+\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n+\t\t\t    stmt_info->stmt, 0);\n         }\n \n       return false;\n@@ -9623,13 +9656,14 @@ vect_analyze_stmt (gimple *stmt, bool *need_to_vectorize, slp_tree node,\n       need extra handling, except for vectorizable reductions.  */\n   if (!bb_vinfo\n       && STMT_VINFO_TYPE (stmt_info) != reduc_vec_info_type\n-      && !can_vectorize_live_stmts (stmt, NULL, node, NULL, cost_vec))\n+      && !can_vectorize_live_stmts (stmt_info, NULL, node, NULL, cost_vec))\n     {\n       if (dump_enabled_p ())\n         {\n           dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n                            \"not vectorized: live stmt not supported: \");\n-          dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM, stmt, 0);\n+\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n+\t\t\t    stmt_info->stmt, 0);\n         }\n \n        return false;\n@@ -9660,45 +9694,49 @@ vect_transform_stmt (gimple *stmt, gimple_stmt_iterator *gsi,\n   bool nested_p = (STMT_VINFO_LOOP_VINFO (stmt_info)\n \t\t   && nested_in_vect_loop_p\n \t\t        (LOOP_VINFO_LOOP (STMT_VINFO_LOOP_VINFO (stmt_info)),\n-\t\t\t stmt));\n+\t\t\t stmt_info));\n \n   switch (STMT_VINFO_TYPE (stmt_info))\n     {\n     case type_demotion_vec_info_type:\n     case type_promotion_vec_info_type:\n     case type_conversion_vec_info_type:\n-      done = vectorizable_conversion (stmt, gsi, &vec_stmt, slp_node, NULL);\n+      done = vectorizable_conversion (stmt_info, gsi, &vec_stmt, slp_node,\n+\t\t\t\t      NULL);\n       gcc_assert (done);\n       break;\n \n     case induc_vec_info_type:\n-      done = vectorizable_induction (stmt, gsi, &vec_stmt, slp_node, NULL);\n+      done = vectorizable_induction (stmt_info, gsi, &vec_stmt, slp_node,\n+\t\t\t\t     NULL);\n       gcc_assert (done);\n       break;\n \n     case shift_vec_info_type:\n-      done = vectorizable_shift (stmt, gsi, &vec_stmt, slp_node, NULL);\n+      done = vectorizable_shift (stmt_info, gsi, &vec_stmt, slp_node, NULL);\n       gcc_assert (done);\n       break;\n \n     case op_vec_info_type:\n-      done = vectorizable_operation (stmt, gsi, &vec_stmt, slp_node, NULL);\n+      done = vectorizable_operation (stmt_info, gsi, &vec_stmt, slp_node,\n+\t\t\t\t     NULL);\n       gcc_assert (done);\n       break;\n \n     case assignment_vec_info_type:\n-      done = vectorizable_assignment (stmt, gsi, &vec_stmt, slp_node, NULL);\n+      done = vectorizable_assignment (stmt_info, gsi, &vec_stmt, slp_node,\n+\t\t\t\t      NULL);\n       gcc_assert (done);\n       break;\n \n     case load_vec_info_type:\n-      done = vectorizable_load (stmt, gsi, &vec_stmt, slp_node,\n+      done = vectorizable_load (stmt_info, gsi, &vec_stmt, slp_node,\n                                 slp_node_instance, NULL);\n       gcc_assert (done);\n       break;\n \n     case store_vec_info_type:\n-      done = vectorizable_store (stmt, gsi, &vec_stmt, slp_node, NULL);\n+      done = vectorizable_store (stmt_info, gsi, &vec_stmt, slp_node, NULL);\n       gcc_assert (done);\n       if (STMT_VINFO_GROUPED_ACCESS (stmt_info) && !slp_node)\n \t{\n@@ -9716,27 +9754,30 @@ vect_transform_stmt (gimple *stmt, gimple_stmt_iterator *gsi,\n       break;\n \n     case condition_vec_info_type:\n-      done = vectorizable_condition (stmt, gsi, &vec_stmt, NULL, 0, slp_node, NULL);\n+      done = vectorizable_condition (stmt_info, gsi, &vec_stmt, NULL, 0,\n+\t\t\t\t     slp_node, NULL);\n       gcc_assert (done);\n       break;\n \n     case comparison_vec_info_type:\n-      done = vectorizable_comparison (stmt, gsi, &vec_stmt, NULL, slp_node, NULL);\n+      done = vectorizable_comparison (stmt_info, gsi, &vec_stmt, NULL,\n+\t\t\t\t      slp_node, NULL);\n       gcc_assert (done);\n       break;\n \n     case call_vec_info_type:\n-      done = vectorizable_call (stmt, gsi, &vec_stmt, slp_node, NULL);\n+      done = vectorizable_call (stmt_info, gsi, &vec_stmt, slp_node, NULL);\n       stmt = gsi_stmt (*gsi);\n       break;\n \n     case call_simd_clone_vec_info_type:\n-      done = vectorizable_simd_clone_call (stmt, gsi, &vec_stmt, slp_node, NULL);\n+      done = vectorizable_simd_clone_call (stmt_info, gsi, &vec_stmt,\n+\t\t\t\t\t   slp_node, NULL);\n       stmt = gsi_stmt (*gsi);\n       break;\n \n     case reduc_vec_info_type:\n-      done = vectorizable_reduction (stmt, gsi, &vec_stmt, slp_node,\n+      done = vectorizable_reduction (stmt_info, gsi, &vec_stmt, slp_node,\n \t\t\t\t     slp_node_instance, NULL);\n       gcc_assert (done);\n       break;\n@@ -9797,7 +9838,8 @@ vect_transform_stmt (gimple *stmt, gimple_stmt_iterator *gsi,\n      being vectorized.  */\n   if (STMT_VINFO_TYPE (stmt_info) != reduc_vec_info_type)\n     {\n-      done = can_vectorize_live_stmts (stmt, gsi, slp_node, &vec_stmt, NULL);\n+      done = can_vectorize_live_stmts (stmt_info, gsi, slp_node, &vec_stmt,\n+\t\t\t\t       NULL);\n       gcc_assert (done);\n     }\n \n@@ -10344,18 +10386,18 @@ supportable_widening_operation (enum tree_code code, gimple *stmt,\n \t a VEC_WIDEN_MULT_LO/HI_EXPR check.  */\n       if (vect_loop\n \t  && STMT_VINFO_RELEVANT (stmt_info) == vect_used_by_reduction\n-\t  && !nested_in_vect_loop_p (vect_loop, stmt)\n+\t  && !nested_in_vect_loop_p (vect_loop, stmt_info)\n \t  && supportable_widening_operation (VEC_WIDEN_MULT_EVEN_EXPR,\n-\t\t\t\t\t     stmt, vectype_out, vectype_in,\n-\t\t\t\t\t     code1, code2, multi_step_cvt,\n-\t\t\t\t\t     interm_types))\n+\t\t\t\t\t     stmt_info, vectype_out,\n+\t\t\t\t\t     vectype_in, code1, code2,\n+\t\t\t\t\t     multi_step_cvt, interm_types))\n         {\n           /* Elements in a vector with vect_used_by_reduction property cannot\n              be reordered if the use chain with this property does not have the\n              same operation.  One such an example is s += a * b, where elements\n              in a and b cannot be reordered.  Here we check if the vector defined\n              by STMT is only directly used in the reduction statement.  */\n-\t  tree lhs = gimple_assign_lhs (stmt);\n+\t  tree lhs = gimple_assign_lhs (stmt_info->stmt);\n \t  stmt_vec_info use_stmt_info = loop_info->lookup_single_use (lhs);\n \t  if (use_stmt_info\n \t      && STMT_VINFO_DEF_TYPE (use_stmt_info) == vect_reduction_def)\n@@ -10827,7 +10869,8 @@ vect_get_vector_types_for_stmt (stmt_vec_info stmt_info,\n       if (*stmt_vectype_out != boolean_type_node)\n \t{\n \t  HOST_WIDE_INT dummy;\n-\t  scalar_type = vect_get_smallest_scalar_type (stmt, &dummy, &dummy);\n+\t  scalar_type = vect_get_smallest_scalar_type (stmt_info,\n+\t\t\t\t\t\t       &dummy, &dummy);\n \t}\n       if (dump_enabled_p ())\n \t{"}, {"sha": "69c7f6831b7664f6fddc0cf4528c03e6dfd425e5", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86a91c0a7d39103bc26f6a9f6cd0b329c9027161/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=86a91c0a7d39103bc26f6a9f6cd0b329c9027161", "patch": "@@ -1325,7 +1325,7 @@ vect_dr_behavior (data_reference *dr)\n   stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   if (loop_vinfo == NULL\n-      || !nested_in_vect_loop_p (LOOP_VINFO_LOOP (loop_vinfo), stmt))\n+      || !nested_in_vect_loop_p (LOOP_VINFO_LOOP (loop_vinfo), stmt_info))\n     return &DR_INNERMOST (dr);\n   else\n     return &STMT_VINFO_DR_WRT_VEC_LOOP (stmt_info);"}]}