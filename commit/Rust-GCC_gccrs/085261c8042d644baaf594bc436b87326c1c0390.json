{"sha": "085261c8042d644baaf594bc436b87326c1c0390", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDg1MjYxYzgwNDJkNjQ0YmFhZjU5NGJjNDM2Yjg3MzI2YzFjMDM5MA==", "commit": {"author": {"name": "Andreas Krebbel", "email": "krebbel@linux.vnet.ibm.com", "date": "2015-05-19T17:26:35Z"}, "committer": {"name": "Andreas Krebbel", "email": "krebbel@gcc.gnu.org", "date": "2015-05-19T17:26:35Z"}, "message": "S/390 Vector base support.\n\ngcc/\n\t* config/s390/constraints.md (j00, jm1, jxx, jyy, v): New\n\tconstraints.\n\t* config/s390/predicates.md (const0_operand, constm1_operand)\n\t(constable_operand): Accept vector operands.\n\t* config/s390/s390-modes.def: Add supported vector modes.\n\t* config/s390/s390-protos.h (s390_cannot_change_mode_class)\n\t(s390_function_arg_vector, s390_contiguous_bitmask_vector_p)\n\t(s390_bytemask_vector_p, s390_expand_vec_strlen)\n\t(s390_expand_vec_compare, s390_expand_vcond)\n\t(s390_expand_vec_init): Add prototypes.\n\t* config/s390/s390.c (VEC_ARG_NUM_REG): New macro.\n\t(s390_vector_mode_supported_p): New function.\n\t(s390_contiguous_bitmask_p): Mask out the irrelevant bits.\n\t(s390_contiguous_bitmask_vector_p): New function.\n\t(s390_bytemask_vector_p): New function.\n\t(s390_split_ok_p): Vector regs don't work either.\n\t(regclass_map): Add VEC_REGS.\n\t(s390_legitimate_constant_p): Handle vector constants.\n\t(s390_cannot_force_const_mem): Handle CONST_VECTOR.\n\t(legitimate_reload_vector_constant_p): New function.\n\t(s390_preferred_reload_class): Handle CONST_VECTOR.\n\t(s390_reload_symref_address):  Likewise.\n\t(s390_secondary_reload): Vector memory instructions only support\n\tshort displacements.  Rename reload*_nonoffmem* to reload*_la*.\n\t(s390_emit_ccraw_jump): New function.\n\t(s390_expand_vec_strlen): New function.\n\t(s390_expand_vec_compare): New function.\n\t(s390_expand_vcond): New function.\n\t(s390_expand_vec_init): New function.\n\t(s390_dwarf_frame_reg_mode): New function.\n\t(print_operand): Handle addresses with 'O' and 'R' constraints.\n\t(NR_C_MODES, constant_modes): Add vector modes.\n\t(s390_output_pool_entry): Handle vector constants.\n\t(s390_hard_regno_mode_ok): Handle vector registers.\n\t(s390_class_max_nregs): Likewise.\n\t(s390_cannot_change_mode_class): New function.\n\t(s390_invalid_arg_for_unprototyped_fn): New function.\n\t(s390_function_arg_vector): New function.\n\t(s390_function_arg_float): Remove size variable.\n\t(s390_pass_by_reference): Handle vector arguments.\n\t(s390_function_arg_advance): Likewise.\n\t(s390_function_arg): Likewise.\n\t(s390_return_in_memory): Vector values are returned in a VR if\n\tpossible.\n\t(s390_function_and_libcall_value): Handle vector arguments.\n\t(s390_gimplify_va_arg): Likewise.\n\t(s390_call_saved_register_used): Consider the arguments named.\n\t(s390_conditional_register_usage): Disable v16-v31 for non-vec\n\ttargets.\n\t(s390_preferred_simd_mode): New function.\n\t(s390_support_vector_misalignment): New function.\n\t(s390_vector_alignment): New function.\n\t(TARGET_STRICT_ARGUMENT_NAMING, TARGET_DWARF_FRAME_REG_MODE)\n\t(TARGET_VECTOR_MODE_SUPPORTED_P)\n\t(TARGET_INVALID_ARG_FOR_UNPROTOTYPED_FN)\n\t(TARGET_VECTORIZE_PREFERRED_SIMD_MODE)\n\t(TARGET_VECTORIZE_SUPPORT_VECTOR_MISALIGNMENT)\n\t(TARGET_VECTOR_ALIGNMENT): Define target macro.\n\t* config/s390/s390.h (FUNCTION_ARG_PADDING): Define macro.\n\t(FIRST_PSEUDO_REGISTER): Increase value.\n\t(VECTOR_NOFP_REGNO_P, VECTOR_REGNO_P, VECTOR_NOFP_REG_P)\n\t(VECTOR_REG_P): Define macros.\n\t(FIXED_REGISTERS, CALL_USED_REGISTERS)\n\t(CALL_REALLY_USED_REGISTERS, REG_ALLOC_ORDER)\n\t(HARD_REGNO_CALL_PART_CLOBBERED, REG_CLASS_NAMES)\n\t(FUNCTION_ARG_REGNO_P, FUNCTION_VALUE_REGNO_P, REGISTER_NAMES):\n\tAdd vector registers.\n\t(CANNOT_CHANGE_MODE_CLASS): Call C function.\n\t(enum reg_class): Add VEC_REGS, ADDR_VEC_REGS, GENERAL_VEC_REGS.\n\t(SECONDARY_MEMORY_NEEDED): Allow SF<->SI mode moves without\n\tmemory.\n\t(DBX_REGISTER_NUMBER, FIRST_VEC_ARG_REGNO, LAST_VEC_ARG_REGNO)\n\t(SHORT_DISP_IN_RANGE, VECTOR_STORE_FLAG_VALUE): Define macro.\n\t* config/s390/s390.md (UNSPEC_VEC_*): New constants.\n\t(VR*_REGNUM): New constants.\n\t(ALL): New mode iterator.\n\t(INTALL): Remove mode iterator.\n\tInclude vector.md.\n\t(movti): Implement TImode moves for VRs.\n\tDisable TImode splitter for VR targets.\n\tImplement splitting TImode GPR<->VR moves.\n\t(reload*_tomem_z10, reload*_toreg_z10): Replace INTALL with ALL.\n\t(reload<mode>_nonoffmem_in, reload<mode>_nonoffmem_out): Rename to\n\treload<mode>_la_in, reload<mode>_la_out.\n\t(*movdi_64, *movsi_zarch, *movhi, *movqi, *mov<mode>_64dfp)\n\t(*mov<mode>_64, *mov<mode>_31): Add vector instructions.\n\t(TD/TF mode splitter): Enable for GPRs only (formerly !FP).\n\t(mov<mode> SF SD): Prefer lder, lde for loading.\n\tAdd lrl and strl instructions.\n\tAdd vector instructions.\n\t(strlen<mode>): Rename old strlen<mode> to strlen_srst<mode>.\n\tCall s390_expand_vec_strlen on z13.\n\t(*cc_to_int): Change predicate to nonimmediate_operand.\n\t(addti3): Rename to *addti3.  New expander.\n\t(subti3): Rename to *subti3.  New expander.\n\t* config/s390/vector.md: New file.\n\nFrom-SVN: r223395", "tree": {"sha": "f2088fab028533e22683011130369154e968243d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/f2088fab028533e22683011130369154e968243d"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/085261c8042d644baaf594bc436b87326c1c0390", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/085261c8042d644baaf594bc436b87326c1c0390", "html_url": "https://github.com/Rust-GCC/gccrs/commit/085261c8042d644baaf594bc436b87326c1c0390", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/085261c8042d644baaf594bc436b87326c1c0390/comments", "author": null, "committer": null, "parents": [{"sha": "55ac540cd6ec0bbdf76ba5fd57ddd67f17112609", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/55ac540cd6ec0bbdf76ba5fd57ddd67f17112609", "html_url": "https://github.com/Rust-GCC/gccrs/commit/55ac540cd6ec0bbdf76ba5fd57ddd67f17112609"}], "stats": {"total": 3072, "additions": 2812, "deletions": 260}, "files": [{"sha": "bf7fe8fd8b68e15d5e816fa4a14a68b44f236656", "filename": "gcc/ChangeLog", "status": "modified", "additions": 99, "deletions": 0, "changes": 99, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/085261c8042d644baaf594bc436b87326c1c0390/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/085261c8042d644baaf594bc436b87326c1c0390/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=085261c8042d644baaf594bc436b87326c1c0390", "patch": "@@ -1,3 +1,102 @@\n+2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>\n+\n+\t* config/s390/constraints.md (j00, jm1, jxx, jyy, v): New\n+\tconstraints.\n+\t* config/s390/predicates.md (const0_operand, constm1_operand)\n+\t(constable_operand): Accept vector operands.\n+\t* config/s390/s390-modes.def: Add supported vector modes.\n+\t* config/s390/s390-protos.h (s390_cannot_change_mode_class)\n+\t(s390_function_arg_vector, s390_contiguous_bitmask_vector_p)\n+\t(s390_bytemask_vector_p, s390_expand_vec_strlen)\n+\t(s390_expand_vec_compare, s390_expand_vcond)\n+\t(s390_expand_vec_init): Add prototypes.\n+\t* config/s390/s390.c (VEC_ARG_NUM_REG): New macro.\n+\t(s390_vector_mode_supported_p): New function.\n+\t(s390_contiguous_bitmask_p): Mask out the irrelevant bits.\n+\t(s390_contiguous_bitmask_vector_p): New function.\n+\t(s390_bytemask_vector_p): New function.\n+\t(s390_split_ok_p): Vector regs don't work either.\n+\t(regclass_map): Add VEC_REGS.\n+\t(s390_legitimate_constant_p): Handle vector constants.\n+\t(s390_cannot_force_const_mem): Handle CONST_VECTOR.\n+\t(legitimate_reload_vector_constant_p): New function.\n+\t(s390_preferred_reload_class): Handle CONST_VECTOR.\n+\t(s390_reload_symref_address):  Likewise.\n+\t(s390_secondary_reload): Vector memory instructions only support\n+\tshort displacements.  Rename reload*_nonoffmem* to reload*_la*.\n+\t(s390_emit_ccraw_jump): New function.\n+\t(s390_expand_vec_strlen): New function.\n+\t(s390_expand_vec_compare): New function.\n+\t(s390_expand_vcond): New function.\n+\t(s390_expand_vec_init): New function.\n+\t(s390_dwarf_frame_reg_mode): New function.\n+\t(print_operand): Handle addresses with 'O' and 'R' constraints.\n+\t(NR_C_MODES, constant_modes): Add vector modes.\n+\t(s390_output_pool_entry): Handle vector constants.\n+\t(s390_hard_regno_mode_ok): Handle vector registers.\n+\t(s390_class_max_nregs): Likewise.\n+\t(s390_cannot_change_mode_class): New function.\n+\t(s390_invalid_arg_for_unprototyped_fn): New function.\n+\t(s390_function_arg_vector): New function.\n+\t(s390_function_arg_float): Remove size variable.\n+\t(s390_pass_by_reference): Handle vector arguments.\n+\t(s390_function_arg_advance): Likewise.\n+\t(s390_function_arg): Likewise.\n+\t(s390_return_in_memory): Vector values are returned in a VR if\n+\tpossible.\n+\t(s390_function_and_libcall_value): Handle vector arguments.\n+\t(s390_gimplify_va_arg): Likewise.\n+\t(s390_call_saved_register_used): Consider the arguments named.\n+\t(s390_conditional_register_usage): Disable v16-v31 for non-vec\n+\ttargets.\n+\t(s390_preferred_simd_mode): New function.\n+\t(s390_support_vector_misalignment): New function.\n+\t(s390_vector_alignment): New function.\n+\t(TARGET_STRICT_ARGUMENT_NAMING, TARGET_DWARF_FRAME_REG_MODE)\n+\t(TARGET_VECTOR_MODE_SUPPORTED_P)\n+\t(TARGET_INVALID_ARG_FOR_UNPROTOTYPED_FN)\n+\t(TARGET_VECTORIZE_PREFERRED_SIMD_MODE)\n+\t(TARGET_VECTORIZE_SUPPORT_VECTOR_MISALIGNMENT)\n+\t(TARGET_VECTOR_ALIGNMENT): Define target macro.\n+\t* config/s390/s390.h (FUNCTION_ARG_PADDING): Define macro.\n+\t(FIRST_PSEUDO_REGISTER): Increase value.\n+\t(VECTOR_NOFP_REGNO_P, VECTOR_REGNO_P, VECTOR_NOFP_REG_P)\n+\t(VECTOR_REG_P): Define macros.\n+\t(FIXED_REGISTERS, CALL_USED_REGISTERS)\n+\t(CALL_REALLY_USED_REGISTERS, REG_ALLOC_ORDER)\n+\t(HARD_REGNO_CALL_PART_CLOBBERED, REG_CLASS_NAMES)\n+\t(FUNCTION_ARG_REGNO_P, FUNCTION_VALUE_REGNO_P, REGISTER_NAMES):\n+\tAdd vector registers.\n+\t(CANNOT_CHANGE_MODE_CLASS): Call C function.\n+\t(enum reg_class): Add VEC_REGS, ADDR_VEC_REGS, GENERAL_VEC_REGS.\n+\t(SECONDARY_MEMORY_NEEDED): Allow SF<->SI mode moves without\n+\tmemory.\n+\t(DBX_REGISTER_NUMBER, FIRST_VEC_ARG_REGNO, LAST_VEC_ARG_REGNO)\n+\t(SHORT_DISP_IN_RANGE, VECTOR_STORE_FLAG_VALUE): Define macro.\n+\t* config/s390/s390.md (UNSPEC_VEC_*): New constants.\n+\t(VR*_REGNUM): New constants.\n+\t(ALL): New mode iterator.\n+\t(INTALL): Remove mode iterator.\n+\tInclude vector.md.\n+\t(movti): Implement TImode moves for VRs.\n+\tDisable TImode splitter for VR targets.\n+\tImplement splitting TImode GPR<->VR moves.\n+\t(reload*_tomem_z10, reload*_toreg_z10): Replace INTALL with ALL.\n+\t(reload<mode>_nonoffmem_in, reload<mode>_nonoffmem_out): Rename to\n+\treload<mode>_la_in, reload<mode>_la_out.\n+\t(*movdi_64, *movsi_zarch, *movhi, *movqi, *mov<mode>_64dfp)\n+\t(*mov<mode>_64, *mov<mode>_31): Add vector instructions.\n+\t(TD/TF mode splitter): Enable for GPRs only (formerly !FP).\n+\t(mov<mode> SF SD): Prefer lder, lde for loading.\n+\tAdd lrl and strl instructions.\n+\tAdd vector instructions.\n+\t(strlen<mode>): Rename old strlen<mode> to strlen_srst<mode>.\n+\tCall s390_expand_vec_strlen on z13.\n+\t(*cc_to_int): Change predicate to nonimmediate_operand.\n+\t(addti3): Rename to *addti3.  New expander.\n+\t(subti3): Rename to *subti3.  New expander.\n+\t* config/s390/vector.md: New file.\n+\n 2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>\n \n \t* common/config/s390/s390-common.c (processor_flags_table): Add"}, {"sha": "66d4acecdb3524f21993a59381519a4565f470a6", "filename": "gcc/config/s390/constraints.md", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fconstraints.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fconstraints.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fconstraints.md?ref=085261c8042d644baaf594bc436b87326c1c0390", "patch": "@@ -29,7 +29,13 @@\n ;;    c -- Condition code register 33.\n ;;    d -- Any register from 0 to 15.\n ;;    f -- Floating point registers.\n+;;    j -- Multiple letter constraint for constant scalar and vector values\n+;;         j00: constant zero scalar or vector\n+;;         jm1: constant scalar or vector with all bits set\n+;;         jxx: contiguous bitmask of 0 or 1 in all vector elements\n+;;         jyy: constant consisting of byte chunks being either 0 or 0xff\n ;;    t -- Access registers 36 and 37.\n+;;    v -- Vector registers v0-v31.\n ;;    C -- A signed 8-bit constant (-128..127)\n ;;    D -- An unsigned 16-bit constant (0..65535)\n ;;    G -- Const double zero operand\n@@ -102,13 +108,35 @@\n   \"FP_REGS\"\n   \"Floating point registers\")\n \n+(define_constraint \"j00\"\n+  \"Zero scalar or vector constant\"\n+  (match_test \"op == CONST0_RTX (GET_MODE (op))\"))\n+\n+(define_constraint \"jm1\"\n+  \"All one bit scalar or vector constant\"\n+  (match_test \"op == CONSTM1_RTX (GET_MODE (op))\"))\n+\n+(define_constraint \"jxx\"\n+  \"@internal\"\n+  (and (match_code \"const_vector\")\n+       (match_test \"s390_contiguous_bitmask_vector_p (op, NULL, NULL)\")))\n+\n+(define_constraint \"jyy\"\n+  \"@internal\"\n+  (and (match_code \"const_vector\")\n+       (match_test \"s390_bytemask_vector_p (op, NULL)\")))\n \n (define_register_constraint \"t\"\n   \"ACCESS_REGS\"\n   \"@internal\n    Access registers 36 and 37\")\n \n \n+(define_register_constraint \"v\"\n+  \"VEC_REGS\"\n+  \"Vector registers v0-v31\")\n+\n+\n ;;\n ;;  General constraints for constants.\n ;;"}, {"sha": "eeaf1aef9f80e00c35422e2c2d3716abdc90d37c", "filename": "gcc/config/s390/predicates.md", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fpredicates.md?ref=085261c8042d644baaf594bc436b87326c1c0390", "patch": "@@ -24,16 +24,20 @@\n \n ;; operands --------------------------------------------------------------\n \n-;; Return true if OP a (const_int 0) operand.\n-\n+;; Return true if OP a const 0 operand (int/float/vector).\n (define_predicate \"const0_operand\"\n-  (and (match_code \"const_int, const_double\")\n+  (and (match_code \"const_int,const_double,const_vector\")\n        (match_test \"op == CONST0_RTX (mode)\")))\n \n+;; Return true if OP an all ones operand (int/float/vector).\n+(define_predicate \"constm1_operand\"\n+  (and (match_code \"const_int, const_double,const_vector\")\n+       (match_test \"op == CONSTM1_RTX (mode)\")))\n+\n ;; Return true if OP is constant.\n \n (define_special_predicate \"consttable_operand\"\n-  (and (match_code \"symbol_ref, label_ref, const, const_int, const_double\")\n+  (and (match_code \"symbol_ref, label_ref, const, const_int, const_double, const_vector\")\n        (match_test \"CONSTANT_P (op)\")))\n \n ;; Return true if OP is a valid S-type operand."}, {"sha": "a40559effe9b63c18032b686105898b2e8c2dd48", "filename": "gcc/config/s390/s390-modes.def", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fs390-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fs390-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fs390-modes.def?ref=085261c8042d644baaf594bc436b87326c1c0390", "patch": "@@ -181,3 +181,24 @@ CC_MODE (CCT1);\n CC_MODE (CCT2);\n CC_MODE (CCT3);\n CC_MODE (CCRAW);\n+\n+/* Vector modes.  */\n+\n+VECTOR_MODES (INT, 2);        /*                 V2QI */\n+VECTOR_MODES (INT, 4);        /*            V4QI V2HI */\n+VECTOR_MODES (INT, 8);        /*       V8QI V4HI V2SI */\n+VECTOR_MODES (INT, 16);       /* V16QI V8HI V4SI V2DI */\n+\n+VECTOR_MODE (FLOAT, SF, 2);   /* V2SF */\n+VECTOR_MODE (FLOAT, SF, 4);   /* V4SF */\n+VECTOR_MODE (FLOAT, DF, 2);   /* V2DF */\n+\n+VECTOR_MODE (INT, QI, 1);     /* V1QI */\n+VECTOR_MODE (INT, HI, 1);     /* V1HI */\n+VECTOR_MODE (INT, SI, 1);     /* V1SI */\n+VECTOR_MODE (INT, DI, 1);     /* V1DI */\n+VECTOR_MODE (INT, TI, 1);     /* V1TI */\n+\n+VECTOR_MODE (FLOAT, SF, 1);   /* V1SF */\n+VECTOR_MODE (FLOAT, DF, 1);   /* V1DF */\n+VECTOR_MODE (FLOAT, TF, 1);   /* V1TF */"}, {"sha": "b23806f8669c6aff321187d09d3fbf8133928158", "filename": "gcc/config/s390/s390-protos.h", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fs390-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fs390-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fs390-protos.h?ref=085261c8042d644baaf594bc436b87326c1c0390", "patch": "@@ -43,6 +43,9 @@ extern void s390_set_has_landing_pad_p (bool);\n extern bool s390_hard_regno_mode_ok (unsigned int, machine_mode);\n extern bool s390_hard_regno_rename_ok (unsigned int, unsigned int);\n extern int s390_class_max_nregs (enum reg_class, machine_mode);\n+extern int s390_cannot_change_mode_class (machine_mode, machine_mode,\n+\t\t\t\t\t  enum reg_class);\n+extern bool s390_function_arg_vector (machine_mode, const_tree);\n \n #ifdef RTX_CODE\n extern int s390_extra_constraint_str (rtx, int, const char *);\n@@ -51,6 +54,8 @@ extern int s390_const_double_ok_for_constraint_p (rtx, int, const char *);\n extern int s390_single_part (rtx, machine_mode, machine_mode, int);\n extern unsigned HOST_WIDE_INT s390_extract_part (rtx, machine_mode, int);\n extern bool s390_contiguous_bitmask_p (unsigned HOST_WIDE_INT, int, int *, int *);\n+extern bool s390_contiguous_bitmask_vector_p (rtx, int *, int *);\n+extern bool s390_bytemask_vector_p (rtx, unsigned *);\n extern bool s390_split_ok_p (rtx, rtx, machine_mode, int);\n extern bool s390_overlap_p (rtx, rtx, HOST_WIDE_INT);\n extern bool s390_offset_p (rtx, rtx, rtx);\n@@ -83,13 +88,17 @@ extern void s390_load_address (rtx, rtx);\n extern bool s390_expand_movmem (rtx, rtx, rtx);\n extern void s390_expand_setmem (rtx, rtx, rtx);\n extern bool s390_expand_cmpmem (rtx, rtx, rtx, rtx);\n+extern void s390_expand_vec_strlen (rtx, rtx, rtx);\n extern bool s390_expand_addcc (enum rtx_code, rtx, rtx, rtx, rtx, rtx);\n extern bool s390_expand_insv (rtx, rtx, rtx, rtx);\n extern void s390_expand_cs_hqi (machine_mode, rtx, rtx, rtx,\n \t\t\t\trtx, rtx, bool);\n extern void s390_expand_atomic (machine_mode, enum rtx_code,\n \t\t\t\trtx, rtx, rtx, bool);\n extern void s390_expand_tbegin (rtx, rtx, rtx, bool);\n+extern void s390_expand_vec_compare (rtx, enum rtx_code, rtx, rtx);\n+extern void s390_expand_vcond (rtx, rtx, rtx, enum rtx_code, rtx, rtx);\n+extern void s390_expand_vec_init (rtx, rtx);\n extern rtx s390_return_addr_rtx (int, rtx);\n extern rtx s390_back_chain_rtx (void);\n extern rtx_insn *s390_emit_call (rtx, rtx, rtx, rtx);"}, {"sha": "3ff9526ebcd3d9fb856d7f0abae945b17d7fc60e", "filename": "gcc/config/s390/s390.c", "status": "modified", "additions": 1031, "deletions": 88, "changes": 1119, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fs390.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fs390.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fs390.c?ref=085261c8042d644baaf594bc436b87326c1c0390", "patch": "@@ -97,6 +97,10 @@ along with GCC; see the file COPYING3.  If not see\n #include \"context.h\"\n #include \"builtins.h\"\n #include \"rtl-iter.h\"\n+#include \"intl.h\"\n+#include \"plugin-api.h\"\n+#include \"ipa-ref.h\"\n+#include \"cgraph.h\"\n \n /* Define the specific costs for a given cpu.  */\n \n@@ -440,6 +444,7 @@ struct GTY(()) machine_function\n /* Number of GPRs and FPRs used for argument passing.  */\n #define GP_ARG_NUM_REG 5\n #define FP_ARG_NUM_REG (TARGET_64BIT? 4 : 2)\n+#define VEC_ARG_NUM_REG 8\n \n /* A couple of shortcuts.  */\n #define CONST_OK_FOR_J(x) \\\n@@ -576,6 +581,35 @@ s390_scalar_mode_supported_p (machine_mode mode)\n   return default_scalar_mode_supported_p (mode);\n }\n \n+/* Return true if the back end supports vector mode MODE.  */\n+static bool\n+s390_vector_mode_supported_p (machine_mode mode)\n+{\n+  machine_mode inner;\n+\n+  if (!VECTOR_MODE_P (mode)\n+      || !TARGET_VX\n+      || GET_MODE_SIZE (mode) > 16)\n+    return false;\n+\n+  inner = GET_MODE_INNER (mode);\n+\n+  switch (inner)\n+    {\n+    case QImode:\n+    case HImode:\n+    case SImode:\n+    case DImode:\n+    case TImode:\n+    case SFmode:\n+    case DFmode:\n+    case TFmode:\n+      return true;\n+    default:\n+      return false;\n+    }\n+}\n+\n /* Set the has_landing_pad_p flag in struct machine_function to VALUE.  */\n \n void\n@@ -1473,6 +1507,9 @@ s390_contiguous_bitmask_p (unsigned HOST_WIDE_INT in, int size,\n   /* Calculate a mask for all bits beyond the contiguous bits.  */\n   mask = (-1LL & ~(((1ULL << (tmp_length + tmp_pos - 1)) << 1) - 1));\n \n+  if ((unsigned)size < sizeof (HOST_WIDE_INT) * BITS_PER_UNIT)\n+    mask &= (HOST_WIDE_INT_1U << size) - 1;\n+\n   if (mask & in)\n     return false;\n \n@@ -1488,6 +1525,101 @@ s390_contiguous_bitmask_p (unsigned HOST_WIDE_INT in, int size,\n   return true;\n }\n \n+/* Return true if OP contains the same contiguous bitfield in *all*\n+   its elements.  START and END can be used to obtain the start and\n+   end position of the bitfield.\n+\n+   START/STOP give the position of the first/last bit of the bitfield\n+   counting from the lowest order bit starting with zero.  In order to\n+   use these values for S/390 instructions this has to be converted to\n+   \"bits big endian\" style.  */\n+\n+bool\n+s390_contiguous_bitmask_vector_p (rtx op, int *start, int *end)\n+{\n+  unsigned HOST_WIDE_INT mask;\n+  int length, size;\n+\n+  if (!VECTOR_MODE_P (GET_MODE (op))\n+      || GET_CODE (op) != CONST_VECTOR\n+      || !CONST_INT_P (XVECEXP (op, 0, 0)))\n+    return false;\n+\n+  if (GET_MODE_NUNITS (GET_MODE (op)) > 1)\n+    {\n+      int i;\n+\n+      for (i = 1; i < GET_MODE_NUNITS (GET_MODE (op)); ++i)\n+\tif (!rtx_equal_p (XVECEXP (op, 0, i), XVECEXP (op, 0, 0)))\n+\t  return false;\n+    }\n+\n+  size = GET_MODE_UNIT_BITSIZE (GET_MODE (op));\n+  mask = UINTVAL (XVECEXP (op, 0, 0));\n+  if (s390_contiguous_bitmask_p (mask, size, start,\n+\t\t\t\t end != NULL ? &length : NULL))\n+    {\n+      if (end != NULL)\n+\t*end = *start + length - 1;\n+      return true;\n+    }\n+  /* 0xff00000f style immediates can be covered by swapping start and\n+     end indices in vgm.  */\n+  if (s390_contiguous_bitmask_p (~mask, size, start,\n+\t\t\t\t end != NULL ? &length : NULL))\n+    {\n+      if (end != NULL)\n+\t*end = *start - 1;\n+      if (start != NULL)\n+\t*start = *start + length;\n+      return true;\n+    }\n+  return false;\n+}\n+\n+/* Return true if C consists only of byte chunks being either 0 or\n+   0xff.  If MASK is !=NULL a byte mask is generated which is\n+   appropriate for the vector generate byte mask instruction.  */\n+\n+bool\n+s390_bytemask_vector_p (rtx op, unsigned *mask)\n+{\n+  int i;\n+  unsigned tmp_mask = 0;\n+  int nunit, unit_size;\n+\n+  if (!VECTOR_MODE_P (GET_MODE (op))\n+      || GET_CODE (op) != CONST_VECTOR\n+      || !CONST_INT_P (XVECEXP (op, 0, 0)))\n+    return false;\n+\n+  nunit = GET_MODE_NUNITS (GET_MODE (op));\n+  unit_size = GET_MODE_UNIT_SIZE (GET_MODE (op));\n+\n+  for (i = 0; i < nunit; i++)\n+    {\n+      unsigned HOST_WIDE_INT c;\n+      int j;\n+\n+      if (!CONST_INT_P (XVECEXP (op, 0, i)))\n+\treturn false;\n+\n+      c = UINTVAL (XVECEXP (op, 0, i));\n+      for (j = 0; j < unit_size; j++)\n+\t{\n+\t  if ((c & 0xff) != 0 && (c & 0xff) != 0xff)\n+\t    return false;\n+\t  tmp_mask |= (c & 1) << ((nunit - 1 - i) * unit_size + j);\n+\t  c = c >> BITS_PER_UNIT;\n+\t}\n+    }\n+\n+  if (mask != NULL)\n+    *mask = tmp_mask;\n+\n+  return true;\n+}\n+\n /* Check whether a rotate of ROTL followed by an AND of CONTIG is\n    equivalent to a shift followed by the AND.  In particular, CONTIG\n    should not overlap the (rotated) bit 0/bit 63 gap.  Negative values\n@@ -1513,8 +1645,8 @@ s390_extzv_shift_ok (int bitsize, int rotl, unsigned HOST_WIDE_INT contig)\n bool\n s390_split_ok_p (rtx dst, rtx src, machine_mode mode, int first_subword)\n {\n-  /* Floating point registers cannot be split.  */\n-  if (FP_REG_P (src) || FP_REG_P (dst))\n+  /* Floating point and vector registers cannot be split.  */\n+  if (FP_REG_P (src) || FP_REG_P (dst) || VECTOR_REG_P (src) || VECTOR_REG_P (dst))\n     return false;\n \n   /* We don't need to split if operands are directly accessible.  */\n@@ -1705,16 +1837,20 @@ s390_init_machine_status (void)\n /* Map for smallest class containing reg regno.  */\n \n const enum reg_class regclass_map[FIRST_PSEUDO_REGISTER] =\n-{ GENERAL_REGS, ADDR_REGS, ADDR_REGS, ADDR_REGS,\n-  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,\n-  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,\n-  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,\n-  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,\n-  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,\n-  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,\n-  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,\n-  ADDR_REGS,    CC_REGS,   ADDR_REGS, ADDR_REGS,\n-  ACCESS_REGS,\tACCESS_REGS\n+{ GENERAL_REGS, ADDR_REGS, ADDR_REGS, ADDR_REGS,  /*  0 */\n+  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,  /*  4 */\n+  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,  /*  8 */\n+  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,  /* 12 */\n+  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,    /* 16 */\n+  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,    /* 20 */\n+  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,    /* 24 */\n+  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,    /* 28 */\n+  ADDR_REGS,    CC_REGS,   ADDR_REGS, ADDR_REGS,  /* 32 */\n+  ACCESS_REGS,\tACCESS_REGS, VEC_REGS, VEC_REGS,  /* 36 */\n+  VEC_REGS, VEC_REGS, VEC_REGS, VEC_REGS,         /* 40 */\n+  VEC_REGS, VEC_REGS, VEC_REGS, VEC_REGS,         /* 44 */\n+  VEC_REGS, VEC_REGS, VEC_REGS, VEC_REGS,         /* 48 */\n+  VEC_REGS, VEC_REGS                              /* 52 */\n };\n \n /* Return attribute type of insn.  */\n@@ -2775,6 +2911,17 @@ legitimate_pic_operand_p (rtx op)\n static bool\n s390_legitimate_constant_p (machine_mode mode, rtx op)\n {\n+  if (VECTOR_MODE_P (mode) && GET_CODE (op) == CONST_VECTOR)\n+    {\n+      if (GET_MODE_SIZE (mode) != 16)\n+\treturn 0;\n+\n+      if (!const0_operand (op, mode)\n+\t  && !s390_contiguous_bitmask_vector_p (op, NULL, NULL)\n+\t  && !s390_bytemask_vector_p (op, NULL))\n+\treturn 0;\n+    }\n+\n   /* Accept all non-symbolic constants.  */\n   if (!SYMBOLIC_CONST (op))\n     return 1;\n@@ -2811,6 +2958,7 @@ s390_cannot_force_const_mem (machine_mode mode, rtx x)\n     {\n     case CONST_INT:\n     case CONST_DOUBLE:\n+    case CONST_VECTOR:\n       /* Accept all non-symbolic constants.  */\n       return false;\n \n@@ -2943,6 +3091,27 @@ legitimate_reload_fp_constant_p (rtx op)\n   return false;\n }\n \n+/* Returns true if the constant value OP is a legitimate vector operand\n+   during and after reload.\n+   This function accepts all constants which can be loaded directly\n+   into an VR.  */\n+\n+static bool\n+legitimate_reload_vector_constant_p (rtx op)\n+{\n+  /* FIXME: Support constant vectors with all the same 16 bit unsigned\n+     operands.  These can be loaded with vrepi.  */\n+\n+  if (TARGET_VX && GET_MODE_SIZE (GET_MODE (op)) == 16\n+      && (const0_operand (op, GET_MODE (op))\n+\t  || constm1_operand (op, GET_MODE (op))\n+\t  || s390_contiguous_bitmask_vector_p (op, NULL, NULL)\n+\t  || s390_bytemask_vector_p (op, NULL)))\n+    return true;\n+\n+  return false;\n+}\n+\n /* Given an rtx OP being reloaded into a reg required to be in class RCLASS,\n    return the class of reg to actually use.  */\n \n@@ -2953,6 +3122,7 @@ s390_preferred_reload_class (rtx op, reg_class_t rclass)\n     {\n       /* Constants we cannot reload into general registers\n \t must be forced into the literal pool.  */\n+      case CONST_VECTOR:\n       case CONST_DOUBLE:\n       case CONST_INT:\n \tif (reg_class_subset_p (GENERAL_REGS, rclass)\n@@ -2964,6 +3134,10 @@ s390_preferred_reload_class (rtx op, reg_class_t rclass)\n \telse if (reg_class_subset_p (FP_REGS, rclass)\n \t\t && legitimate_reload_fp_constant_p (op))\n \t  return FP_REGS;\n+\telse if (reg_class_subset_p (VEC_REGS, rclass)\n+\t\t && legitimate_reload_vector_constant_p (op))\n+\t  return VEC_REGS;\n+\n \treturn NO_REGS;\n \n       /* If a symbolic constant or a PLUS is reloaded,\n@@ -3087,6 +3261,7 @@ s390_reload_symref_address (rtx reg, rtx mem, rtx scratch, bool tomem)\n   /* Reload might have pulled a constant out of the literal pool.\n      Force it back in.  */\n   if (CONST_INT_P (mem) || GET_CODE (mem) == CONST_DOUBLE\n+      || GET_CODE (mem) == CONST_VECTOR\n       || GET_CODE (mem) == CONST)\n     mem = force_const_mem (GET_MODE (reg), mem);\n \n@@ -3126,6 +3301,30 @@ s390_secondary_reload (bool in_p, rtx x, reg_class_t rclass_i,\n   if (reg_classes_intersect_p (CC_REGS, rclass))\n     return GENERAL_REGS;\n \n+  if (TARGET_VX)\n+    {\n+      /* The vst/vl vector move instructions allow only for short\n+\t displacements.  */\n+      if (MEM_P (x)\n+\t  && GET_CODE (XEXP (x, 0)) == PLUS\n+\t  && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT\n+\t  && !SHORT_DISP_IN_RANGE(INTVAL (XEXP (XEXP (x, 0), 1)))\n+\t  && reg_class_subset_p (rclass, VEC_REGS)\n+\t  && (!reg_class_subset_p (rclass, FP_REGS)\n+\t      || (GET_MODE_SIZE (mode) > 8\n+\t\t  && s390_class_max_nregs (FP_REGS, mode) == 1)))\n+\t{\n+\t  if (in_p)\n+\t    sri->icode = (TARGET_64BIT ?\n+\t\t\t  CODE_FOR_reloaddi_la_in :\n+\t\t\t  CODE_FOR_reloadsi_la_in);\n+\t  else\n+\t    sri->icode = (TARGET_64BIT ?\n+\t\t\t  CODE_FOR_reloaddi_la_out :\n+\t\t\t  CODE_FOR_reloadsi_la_out);\n+\t}\n+    }\n+\n   if (TARGET_Z10)\n     {\n       HOST_WIDE_INT offset;\n@@ -3174,7 +3373,27 @@ s390_secondary_reload (bool in_p, rtx x, reg_class_t rclass_i,\n \t      __SECONDARY_RELOAD_CASE (SD, sd);\n \t      __SECONDARY_RELOAD_CASE (DD, dd);\n \t      __SECONDARY_RELOAD_CASE (TD, td);\n-\n+\t      __SECONDARY_RELOAD_CASE (V1QI, v1qi);\n+\t      __SECONDARY_RELOAD_CASE (V2QI, v2qi);\n+\t      __SECONDARY_RELOAD_CASE (V4QI, v4qi);\n+\t      __SECONDARY_RELOAD_CASE (V8QI, v8qi);\n+\t      __SECONDARY_RELOAD_CASE (V16QI, v16qi);\n+\t      __SECONDARY_RELOAD_CASE (V1HI, v1hi);\n+\t      __SECONDARY_RELOAD_CASE (V2HI, v2hi);\n+\t      __SECONDARY_RELOAD_CASE (V4HI, v4hi);\n+\t      __SECONDARY_RELOAD_CASE (V8HI, v8hi);\n+\t      __SECONDARY_RELOAD_CASE (V1SI, v1si);\n+\t      __SECONDARY_RELOAD_CASE (V2SI, v2si);\n+\t      __SECONDARY_RELOAD_CASE (V4SI, v4si);\n+\t      __SECONDARY_RELOAD_CASE (V1DI, v1di);\n+\t      __SECONDARY_RELOAD_CASE (V2DI, v2di);\n+\t      __SECONDARY_RELOAD_CASE (V1TI, v1ti);\n+\t      __SECONDARY_RELOAD_CASE (V1SF, v1sf);\n+\t      __SECONDARY_RELOAD_CASE (V2SF, v2sf);\n+\t      __SECONDARY_RELOAD_CASE (V4SF, v4sf);\n+\t      __SECONDARY_RELOAD_CASE (V1DF, v1df);\n+\t      __SECONDARY_RELOAD_CASE (V2DF, v2df);\n+\t      __SECONDARY_RELOAD_CASE (V1TF, v1tf);\n \t    default:\n \t      gcc_unreachable ();\n \t    }\n@@ -3213,12 +3432,12 @@ s390_secondary_reload (bool in_p, rtx x, reg_class_t rclass_i,\n \t{\n \t  if (in_p)\n \t    sri->icode = (TARGET_64BIT ?\n-\t\t\t  CODE_FOR_reloaddi_nonoffmem_in :\n-\t\t\t  CODE_FOR_reloadsi_nonoffmem_in);\n+\t\t\t  CODE_FOR_reloaddi_la_in :\n+\t\t\t  CODE_FOR_reloadsi_la_in);\n \t  else\n \t    sri->icode = (TARGET_64BIT ?\n-\t\t\t  CODE_FOR_reloaddi_nonoffmem_out :\n-\t\t\t  CODE_FOR_reloadsi_nonoffmem_out);\n+\t\t\t  CODE_FOR_reloaddi_la_out :\n+\t\t\t  CODE_FOR_reloadsi_la_out);\n \t}\n     }\n \n@@ -4452,6 +4671,138 @@ s390_expand_cmpmem (rtx target, rtx op0, rtx op1, rtx len)\n   return true;\n }\n \n+/* Emit a conditional jump to LABEL for condition code mask MASK using\n+   comparsion operator COMPARISON.  Return the emitted jump insn.  */\n+\n+static rtx\n+s390_emit_ccraw_jump (HOST_WIDE_INT mask, enum rtx_code comparison, rtx label)\n+{\n+  rtx temp;\n+\n+  gcc_assert (comparison == EQ || comparison == NE);\n+  gcc_assert (mask > 0 && mask < 15);\n+\n+  temp = gen_rtx_fmt_ee (comparison, VOIDmode,\n+\t\t\t gen_rtx_REG (CCRAWmode, CC_REGNUM), GEN_INT (mask));\n+  temp = gen_rtx_IF_THEN_ELSE (VOIDmode, temp,\n+\t\t\t       gen_rtx_LABEL_REF (VOIDmode, label), pc_rtx);\n+  temp = gen_rtx_SET (pc_rtx, temp);\n+  return emit_jump_insn (temp);\n+}\n+\n+/* Emit the instructions to implement strlen of STRING and store the\n+   result in TARGET.  The string has the known ALIGNMENT.  This\n+   version uses vector instructions and is therefore not appropriate\n+   for targets prior to z13.  */\n+\n+void\n+s390_expand_vec_strlen (rtx target, rtx string, rtx alignment)\n+{\n+  int very_unlikely = REG_BR_PROB_BASE / 100 - 1;\n+  int very_likely = REG_BR_PROB_BASE - 1;\n+  rtx highest_index_to_load_reg = gen_reg_rtx (Pmode);\n+  rtx str_reg = gen_reg_rtx (V16QImode);\n+  rtx str_addr_base_reg = gen_reg_rtx (Pmode);\n+  rtx str_idx_reg = gen_reg_rtx (Pmode);\n+  rtx result_reg = gen_reg_rtx (V16QImode);\n+  rtx is_aligned_label = gen_label_rtx ();\n+  rtx into_loop_label = NULL_RTX;\n+  rtx loop_start_label = gen_label_rtx ();\n+  rtx temp;\n+  rtx len = gen_reg_rtx (QImode);\n+  rtx cond;\n+\n+  s390_load_address (str_addr_base_reg, XEXP (string, 0));\n+  emit_move_insn (str_idx_reg, const0_rtx);\n+\n+  if (INTVAL (alignment) < 16)\n+    {\n+      /* Check whether the address happens to be aligned properly so\n+\t jump directly to the aligned loop.  */\n+      emit_cmp_and_jump_insns (gen_rtx_AND (Pmode,\n+\t\t\t\t\t    str_addr_base_reg, GEN_INT (15)),\n+\t\t\t       const0_rtx, EQ, NULL_RTX,\n+\t\t\t       Pmode, 1, is_aligned_label);\n+\n+      temp = gen_reg_rtx (Pmode);\n+      temp = expand_binop (Pmode, and_optab, str_addr_base_reg,\n+\t\t\t   GEN_INT (15), temp, 1, OPTAB_DIRECT);\n+      gcc_assert (REG_P (temp));\n+      highest_index_to_load_reg =\n+\texpand_binop (Pmode, sub_optab, GEN_INT (15), temp,\n+\t\t      highest_index_to_load_reg, 1, OPTAB_DIRECT);\n+      gcc_assert (REG_P (highest_index_to_load_reg));\n+      emit_insn (gen_vllv16qi (str_reg,\n+\t\t   convert_to_mode (SImode, highest_index_to_load_reg, 1),\n+\t\t   gen_rtx_MEM (BLKmode, str_addr_base_reg)));\n+\n+      into_loop_label = gen_label_rtx ();\n+      s390_emit_jump (into_loop_label, NULL_RTX);\n+      emit_barrier ();\n+    }\n+\n+  emit_label (is_aligned_label);\n+  LABEL_NUSES (is_aligned_label) = INTVAL (alignment) < 16 ? 2 : 1;\n+\n+  /* Reaching this point we are only performing 16 bytes aligned\n+     loads.  */\n+  emit_move_insn (highest_index_to_load_reg, GEN_INT (15));\n+\n+  emit_label (loop_start_label);\n+  LABEL_NUSES (loop_start_label) = 1;\n+\n+  /* Load 16 bytes of the string into VR.  */\n+  emit_move_insn (str_reg,\n+\t\t  gen_rtx_MEM (V16QImode,\n+\t\t\t       gen_rtx_PLUS (Pmode, str_idx_reg,\n+\t\t\t\t\t     str_addr_base_reg)));\n+  if (into_loop_label != NULL_RTX)\n+    {\n+      emit_label (into_loop_label);\n+      LABEL_NUSES (into_loop_label) = 1;\n+    }\n+\n+  /* Increment string index by 16 bytes.  */\n+  expand_binop (Pmode, add_optab, str_idx_reg, GEN_INT (16),\n+\t\tstr_idx_reg, 1, OPTAB_DIRECT);\n+\n+  emit_insn (gen_vec_vfenesv16qi (result_reg, str_reg, str_reg,\n+\t\t\t\t  GEN_INT (VSTRING_FLAG_ZS | VSTRING_FLAG_CS)));\n+\n+  add_int_reg_note (s390_emit_ccraw_jump (8, NE, loop_start_label),\n+\t\t    REG_BR_PROB, very_likely);\n+  emit_insn (gen_vec_extractv16qi (len, result_reg, GEN_INT (7)));\n+\n+  /* If the string pointer wasn't aligned we have loaded less then 16\n+     bytes and the remaining bytes got filled with zeros (by vll).\n+     Now we have to check whether the resulting index lies within the\n+     bytes actually part of the string.  */\n+\n+  cond = s390_emit_compare (GT, convert_to_mode (Pmode, len, 1),\n+\t\t\t    highest_index_to_load_reg);\n+  s390_load_address (highest_index_to_load_reg,\n+\t\t     gen_rtx_PLUS (Pmode, highest_index_to_load_reg,\n+\t\t\t\t   const1_rtx));\n+  if (TARGET_64BIT)\n+    emit_insn (gen_movdicc (str_idx_reg, cond,\n+\t\t\t    highest_index_to_load_reg, str_idx_reg));\n+  else\n+    emit_insn (gen_movsicc (str_idx_reg, cond,\n+\t\t\t    highest_index_to_load_reg, str_idx_reg));\n+\n+  add_int_reg_note (s390_emit_jump (is_aligned_label, cond), REG_BR_PROB,\n+\t\t    very_unlikely);\n+\n+  expand_binop (Pmode, add_optab, str_idx_reg,\n+\t\tGEN_INT (-16), str_idx_reg, 1, OPTAB_DIRECT);\n+  /* FIXME: len is already zero extended - so avoid the llgcr emitted\n+     here.  */\n+  temp = expand_binop (Pmode, add_optab, str_idx_reg,\n+\t\t       convert_to_mode (Pmode, len, 1),\n+\t\t       target, 1, OPTAB_DIRECT);\n+  if (temp != target)\n+    emit_move_insn (target, temp);\n+}\n \n /* Expand conditional increment or decrement using alc/slb instructions.\n    Should generate code setting DST to either SRC or SRC + INCREMENT,\n@@ -4806,6 +5157,215 @@ s390_expand_mask_and_shift (rtx val, machine_mode mode, rtx count)\n \t\t\t      NULL_RTX, 1, OPTAB_DIRECT);\n }\n \n+/* Generate a vector comparison COND of CMP_OP1 and CMP_OP2 and store\n+   the result in TARGET.  */\n+\n+void\n+s390_expand_vec_compare (rtx target, enum rtx_code cond,\n+\t\t\t rtx cmp_op1, rtx cmp_op2)\n+{\n+  machine_mode mode = GET_MODE (target);\n+  bool neg_p = false, swap_p = false;\n+  rtx tmp;\n+\n+  if (GET_MODE (cmp_op1) == V2DFmode)\n+    {\n+      switch (cond)\n+\t{\n+\t  /* NE a != b -> !(a == b) */\n+\tcase NE:   cond = EQ; neg_p = true;                break;\n+\t  /* UNGT a u> b -> !(b >= a) */\n+\tcase UNGT: cond = GE; neg_p = true; swap_p = true; break;\n+\t  /* UNGE a u>= b -> !(b > a) */\n+\tcase UNGE: cond = GT; neg_p = true; swap_p = true; break;\n+\t  /* LE: a <= b -> b >= a */\n+\tcase LE:   cond = GE;               swap_p = true; break;\n+\t  /* UNLE: a u<= b -> !(a > b) */\n+\tcase UNLE: cond = GT; neg_p = true;                break;\n+\t  /* LT: a < b -> b > a */\n+\tcase LT:   cond = GT;               swap_p = true; break;\n+\t  /* UNLT: a u< b -> !(a >= b) */\n+\tcase UNLT: cond = GE; neg_p = true;                break;\n+\tcase UNEQ:\n+\t  emit_insn (gen_vec_cmpuneqv2df (target, cmp_op1, cmp_op2));\n+\t  return;\n+\tcase LTGT:\n+\t  emit_insn (gen_vec_cmpltgtv2df (target, cmp_op1, cmp_op2));\n+\t  return;\n+\tcase ORDERED:\n+\t  emit_insn (gen_vec_orderedv2df (target, cmp_op1, cmp_op2));\n+\t  return;\n+\tcase UNORDERED:\n+\t  emit_insn (gen_vec_unorderedv2df (target, cmp_op1, cmp_op2));\n+\t  return;\n+\tdefault: break;\n+\t}\n+    }\n+  else\n+    {\n+      switch (cond)\n+\t{\n+\t  /* NE: a != b -> !(a == b) */\n+\tcase NE:  cond = EQ;  neg_p = true;                break;\n+\t  /* GE: a >= b -> !(b > a) */\n+\tcase GE:  cond = GT;  neg_p = true; swap_p = true; break;\n+\t  /* GEU: a >= b -> !(b > a) */\n+\tcase GEU: cond = GTU; neg_p = true; swap_p = true; break;\n+\t  /* LE: a <= b -> !(a > b) */\n+\tcase LE:  cond = GT;  neg_p = true;                break;\n+\t  /* LEU: a <= b -> !(a > b) */\n+\tcase LEU: cond = GTU; neg_p = true;                break;\n+\t  /* LT: a < b -> b > a */\n+\tcase LT:  cond = GT;                swap_p = true; break;\n+\t  /* LTU: a < b -> b > a */\n+\tcase LTU: cond = GTU;               swap_p = true; break;\n+\tdefault: break;\n+\t}\n+    }\n+\n+  if (swap_p)\n+    {\n+      tmp = cmp_op1; cmp_op1 = cmp_op2; cmp_op2 = tmp;\n+    }\n+\n+  emit_insn (gen_rtx_SET (target, gen_rtx_fmt_ee (cond,\n+\t\t\t\t\t\t  mode,\n+\t\t\t\t\t\t  cmp_op1, cmp_op2)));\n+  if (neg_p)\n+    emit_insn (gen_rtx_SET (target, gen_rtx_NOT (mode, target)));\n+}\n+\n+/* Generate a vector comparison expression loading either elements of\n+   THEN or ELS into TARGET depending on the comparison COND of CMP_OP1\n+   and CMP_OP2.  */\n+\n+void\n+s390_expand_vcond (rtx target, rtx then, rtx els,\n+\t\t   enum rtx_code cond, rtx cmp_op1, rtx cmp_op2)\n+{\n+  rtx tmp;\n+  machine_mode result_mode;\n+  rtx result_target;\n+\n+  /* We always use an integral type vector to hold the comparison\n+     result.  */\n+  result_mode = GET_MODE (cmp_op1) == V2DFmode ? V2DImode : GET_MODE (cmp_op1);\n+  result_target = gen_reg_rtx (result_mode);\n+\n+  /* Alternatively this could be done by reload by lowering the cmp*\n+     predicates.  But it appears to be better for scheduling etc. to\n+     have that in early.  */\n+  if (!REG_P (cmp_op1))\n+    cmp_op1 = force_reg (GET_MODE (target), cmp_op1);\n+\n+  if (!REG_P (cmp_op2))\n+    cmp_op2 = force_reg (GET_MODE (target), cmp_op2);\n+\n+  s390_expand_vec_compare (result_target, cond,\n+\t\t\t   cmp_op1, cmp_op2);\n+\n+  /* If the results are supposed to be either -1 or 0 we are done\n+     since this is what our compare instructions generate anyway.  */\n+  if (constm1_operand (then, GET_MODE (then))\n+      && const0_operand (els, GET_MODE (els)))\n+    {\n+      emit_move_insn (target, gen_rtx_SUBREG (GET_MODE (target),\n+\t\t\t\t\t      result_target, 0));\n+      return;\n+    }\n+\n+  /* Otherwise we will do a vsel afterwards.  */\n+  /* This gets triggered e.g.\n+     with gcc.c-torture/compile/pr53410-1.c */\n+  if (!REG_P (then))\n+    then = force_reg (GET_MODE (target), then);\n+\n+  if (!REG_P (els))\n+    els = force_reg (GET_MODE (target), els);\n+\n+  tmp = gen_rtx_fmt_ee (EQ, VOIDmode,\n+\t\t\tresult_target,\n+\t\t\tCONST0_RTX (result_mode));\n+\n+  /* We compared the result against zero above so we have to swap then\n+     and els here.  */\n+  tmp = gen_rtx_IF_THEN_ELSE (GET_MODE (target), tmp, els, then);\n+\n+  gcc_assert (GET_MODE (target) == GET_MODE (then));\n+  emit_insn (gen_rtx_SET (target, tmp));\n+}\n+\n+/* Emit the RTX necessary to initialize the vector TARGET with values\n+   in VALS.  */\n+void\n+s390_expand_vec_init (rtx target, rtx vals)\n+{\n+  machine_mode mode = GET_MODE (target);\n+  machine_mode inner_mode = GET_MODE_INNER (mode);\n+  int n_elts = GET_MODE_NUNITS (mode);\n+  bool all_same = true, all_regs = true, all_const_int = true;\n+  rtx x;\n+  int i;\n+\n+  for (i = 0; i < n_elts; ++i)\n+    {\n+      x = XVECEXP (vals, 0, i);\n+\n+      if (!CONST_INT_P (x))\n+\tall_const_int = false;\n+\n+      if (i > 0 && !rtx_equal_p (x, XVECEXP (vals, 0, 0)))\n+\tall_same = false;\n+\n+      if (!REG_P (x))\n+\tall_regs = false;\n+    }\n+\n+  /* Use vector gen mask or vector gen byte mask if possible.  */\n+  if (all_same && all_const_int\n+      && (XVECEXP (vals, 0, 0) == const0_rtx\n+\t  || s390_contiguous_bitmask_vector_p (XVECEXP (vals, 0, 0),\n+\t\t\t\t\t       NULL, NULL)\n+\t  || s390_bytemask_vector_p (XVECEXP (vals, 0, 0), NULL)))\n+    {\n+      emit_insn (gen_rtx_SET (target,\n+\t\t\t      gen_rtx_CONST_VECTOR (mode, XVEC (vals, 0))));\n+      return;\n+    }\n+\n+  if (all_same)\n+    {\n+      emit_insn (gen_rtx_SET (target,\n+\t\t\t      gen_rtx_VEC_DUPLICATE (mode,\n+\t\t\t\t\t\t     XVECEXP (vals, 0, 0))));\n+      return;\n+    }\n+\n+  if (all_regs && REG_P (target) && n_elts == 2 && inner_mode == DImode)\n+    {\n+      /* Use vector load pair.  */\n+      emit_insn (gen_rtx_SET (target,\n+\t\t\t      gen_rtx_VEC_CONCAT (mode,\n+\t\t\t\t\t\t  XVECEXP (vals, 0, 0),\n+\t\t\t\t\t\t  XVECEXP (vals, 0, 1))));\n+      return;\n+    }\n+\n+  /* We are about to set the vector elements one by one.  Zero out the\n+     full register first in order to help the data flow framework to\n+     detect it as full VR set.  */\n+  emit_insn (gen_rtx_SET (target, CONST0_RTX (mode)));\n+\n+  /* Unfortunately the vec_init expander is not allowed to fail.  So\n+     we have to implement the fallback ourselves.  */\n+  for (i = 0; i < n_elts; i++)\n+    emit_insn (gen_rtx_SET (target,\n+\t\t\t    gen_rtx_UNSPEC (mode,\n+\t\t\t\t\t    gen_rtvec (3, XVECEXP (vals, 0, i),\n+\t\t\t\t\t\t       GEN_INT (i), target),\n+\t\t\t\t\t    UNSPEC_VEC_SET)));\n+}\n+\n /* Structure to hold the initial parameters for a compare_and_swap operation\n    in HImode and QImode.  */\n \n@@ -5101,6 +5661,20 @@ s390_output_dwarf_dtprel (FILE *file, int size, rtx x)\n   fputs (\"@DTPOFF\", file);\n }\n \n+/* Return the proper mode for REGNO being represented in the dwarf\n+   unwind table.  */\n+machine_mode\n+s390_dwarf_frame_reg_mode (int regno)\n+{\n+  machine_mode save_mode = default_dwarf_frame_reg_mode (regno);\n+\n+  /* The rightmost 64 bits of vector registers are call-clobbered.  */\n+  if (GET_MODE_SIZE (save_mode) > 8)\n+    save_mode = DImode;\n+\n+  return save_mode;\n+}\n+\n #ifdef TARGET_ALTERNATE_LONG_DOUBLE_MANGLING\n /* Implement TARGET_MANGLE_TYPE.  */\n \n@@ -5427,24 +6001,26 @@ print_operand_address (FILE *file, rtx addr)\n     'J': print tls_load/tls_gdcall/tls_ldcall suffix\n     'M': print the second word of a TImode operand.\n     'N': print the second word of a DImode operand.\n-    'O': print only the displacement of a memory reference.\n-    'R': print only the base register of a memory reference.\n+    'O': print only the displacement of a memory reference or address.\n+    'R': print only the base register of a memory reference or address.\n     'S': print S-type memory reference (base+displacement).\n     'Y': print shift count operand.\n \n     'b': print integer X as if it's an unsigned byte.\n     'c': print integer X as if it's an signed byte.\n-    'e': \"end\" of DImode contiguous bitmask X.\n-    'f': \"end\" of SImode contiguous bitmask X.\n+    'e': \"end\" contiguous bitmask X in either DImode or vector inner mode.\n+    'f': \"end\" contiguous bitmask X in SImode.\n     'h': print integer X as if it's a signed halfword.\n     'i': print the first nonzero HImode part of X.\n     'j': print the first HImode part unequal to -1 of X.\n     'k': print the first nonzero SImode part of X.\n     'm': print the first SImode part unequal to -1 of X.\n     'o': print integer X as if it's an unsigned 32bit word.\n-    's': \"start\" of DImode contiguous bitmask X.\n-    't': \"start\" of SImode contiguous bitmask X.\n+    's': \"start\" of contiguous bitmask X in either DImode or vector inner mode.\n+    't': CONST_INT: \"start\" of contiguous bitmask X in SImode.\n+         CONST_VECTOR: Generate a bitmask for vgbm instruction.\n     'x': print integer X as if it's an unsigned halfword.\n+    'v': print register number as vector register (v1 instead of f1).\n */\n \n void\n@@ -5503,14 +6079,7 @@ print_operand (FILE *file, rtx x, int code)\n         struct s390_address ad;\n \tint ret;\n \n-\tif (!MEM_P (x))\n-\t  {\n-\t    output_operand_lossage (\"memory reference expected for \"\n-\t\t\t\t    \"'O' output modifier\");\n-\t    return;\n-\t  }\n-\n-\tret = s390_decompose_address (XEXP (x, 0), &ad);\n+\tret = s390_decompose_address (MEM_P (x) ? XEXP (x, 0) : x, &ad);\n \n \tif (!ret\n \t    || (ad.base && !REGNO_OK_FOR_BASE_P (REGNO (ad.base)))\n@@ -5532,14 +6101,7 @@ print_operand (FILE *file, rtx x, int code)\n         struct s390_address ad;\n \tint ret;\n \n-\tif (!MEM_P (x))\n-\t  {\n-\t    output_operand_lossage (\"memory reference expected for \"\n-\t\t\t\t    \"'R' output modifier\");\n-\t    return;\n-\t  }\n-\n-\tret = s390_decompose_address (XEXP (x, 0), &ad);\n+\tret = s390_decompose_address (MEM_P (x) ? XEXP (x, 0) : x, &ad);\n \n \tif (!ret\n \t    || (ad.base && !REGNO_OK_FOR_BASE_P (REGNO (ad.base)))\n@@ -5617,7 +6179,17 @@ print_operand (FILE *file, rtx x, int code)\n   switch (GET_CODE (x))\n     {\n     case REG:\n-      fprintf (file, \"%s\", reg_names[REGNO (x)]);\n+      /* Print FP regs as fx instead of vx when they are accessed\n+\t through non-vector mode.  */\n+      if (code == 'v'\n+\t  || VECTOR_NOFP_REG_P (x)\n+\t  || (FP_REG_P (x) && VECTOR_MODE_P (GET_MODE (x)))\n+\t  || (VECTOR_REG_P (x)\n+\t      && (GET_MODE_SIZE (GET_MODE (x)) /\n+\t\t  s390_class_max_nregs (FP_REGS, GET_MODE (x))) > 8))\n+\tfprintf (file, \"%%v%s\", reg_names[REGNO (x)] + 2);\n+      else\n+\tfprintf (file, \"%s\", reg_names[REGNO (x)]);\n       break;\n \n     case MEM:\n@@ -5704,6 +6276,39 @@ print_operand (FILE *file, rtx x, int code)\n \t\t\t\t    code);\n \t}\n       break;\n+    case CONST_VECTOR:\n+      switch (code)\n+\t{\n+\tcase 'e':\n+\tcase 's':\n+\t  {\n+\t    int start, stop, inner_len;\n+\t    bool ok;\n+\n+\t    inner_len = GET_MODE_UNIT_BITSIZE (GET_MODE (x));\n+\t    ok = s390_contiguous_bitmask_vector_p (x, &start, &stop);\n+\t    gcc_assert (ok);\n+\t    if (code == 's' || code == 't')\n+\t      ival = inner_len - stop - 1;\n+\t    else\n+\t      ival = inner_len - start - 1;\n+\t    fprintf (file, HOST_WIDE_INT_PRINT_DEC, ival);\n+\t  }\n+\t  break;\n+\tcase 't':\n+\t  {\n+\t    unsigned mask;\n+\t    bool ok = s390_bytemask_vector_p (x, &mask);\n+\t    gcc_assert (ok);\n+\t    fprintf (file, \"%u\", mask);\n+\t  }\n+\t  break;\n+\n+\tdefault:\n+\t  output_operand_lossage (\"invalid constant vector for output \"\n+\t\t\t\t  \"modifier '%c'\", code);\n+\t}\n+      break;\n \n     default:\n       if (code == 0)\n@@ -6263,14 +6868,19 @@ replace_ltrel_base (rtx *x)\n /* We keep a list of constants which we have to add to internal\n    constant tables in the middle of large functions.  */\n \n-#define NR_C_MODES 11\n+#define NR_C_MODES 31\n machine_mode constant_modes[NR_C_MODES] =\n {\n   TFmode, TImode, TDmode,\n+  V16QImode, V8HImode, V4SImode, V2DImode, V4SFmode, V2DFmode, V1TFmode,\n   DFmode, DImode, DDmode,\n+  V8QImode, V4HImode, V2SImode, V1DImode, V2SFmode, V1DFmode,\n   SFmode, SImode, SDmode,\n+  V4QImode, V2HImode, V1SImode,  V1SFmode,\n   HImode,\n-  QImode\n+  V2QImode, V1HImode,\n+  QImode,\n+  V1QImode\n };\n \n struct constant\n@@ -7285,6 +7895,23 @@ s390_output_pool_entry (rtx exp, machine_mode mode, unsigned int align)\n       mark_symbol_refs_as_used (exp);\n       break;\n \n+    case MODE_VECTOR_INT:\n+    case MODE_VECTOR_FLOAT:\n+      {\n+\tint i;\n+\tmachine_mode inner_mode;\n+\tgcc_assert (GET_CODE (exp) == CONST_VECTOR);\n+\n+\tinner_mode = GET_MODE_INNER (GET_MODE (exp));\n+\tfor (i = 0; i < XVECLEN (exp, 0); i++)\n+\t  s390_output_pool_entry (XVECEXP (exp, 0, i),\n+\t\t\t\t  inner_mode,\n+\t\t\t\t  i == 0\n+\t\t\t\t  ? align\n+\t\t\t\t  : GET_MODE_BITSIZE (inner_mode));\n+      }\n+      break;\n+\n     default:\n       gcc_unreachable ();\n     }\n@@ -8096,9 +8723,25 @@ s390_optimize_nonescaping_tx (void)\n bool\n s390_hard_regno_mode_ok (unsigned int regno, machine_mode mode)\n {\n+  if (!TARGET_VX && VECTOR_NOFP_REGNO_P (regno))\n+    return false;\n+\n   switch (REGNO_REG_CLASS (regno))\n     {\n+    case VEC_REGS:\n+      return ((GET_MODE_CLASS (mode) == MODE_INT\n+\t       && s390_class_max_nregs (VEC_REGS, mode) == 1)\n+\t      || mode == DFmode\n+\t      || s390_vector_mode_supported_p (mode));\n+      break;\n     case FP_REGS:\n+      if (TARGET_VX\n+\t  && ((GET_MODE_CLASS (mode) == MODE_INT\n+\t       && s390_class_max_nregs (FP_REGS, mode) == 1)\n+\t      || mode == DFmode\n+\t      || s390_vector_mode_supported_p (mode)))\n+\treturn true;\n+\n       if (REGNO_PAIR_OK (regno, mode))\n \t{\n \t  if (mode == SImode || mode == DImode)\n@@ -8185,19 +8828,86 @@ s390_hard_regno_scratch_ok (unsigned int regno)\n int\n s390_class_max_nregs (enum reg_class rclass, machine_mode mode)\n {\n+  int reg_size;\n+  bool reg_pair_required_p = false;\n+\n   switch (rclass)\n     {\n     case FP_REGS:\n+    case VEC_REGS:\n+      reg_size = TARGET_VX ? 16 : 8;\n+\n+      /* TF and TD modes would fit into a VR but we put them into a\n+\t register pair since we do not have 128bit FP instructions on\n+\t full VRs.  */\n+      if (TARGET_VX\n+\t  && SCALAR_FLOAT_MODE_P (mode)\n+\t  && GET_MODE_SIZE (mode) >= 16)\n+\treg_pair_required_p = true;\n+\n+      /* Even if complex types would fit into a single FPR/VR we force\n+\t them into a register pair to deal with the parts more easily.\n+\t (FIXME: What about complex ints?)  */\n       if (GET_MODE_CLASS (mode) == MODE_COMPLEX_FLOAT)\n-\treturn 2 * ((GET_MODE_SIZE (mode) / 2 + 8 - 1) / 8);\n-      else\n-\treturn (GET_MODE_SIZE (mode) + 8 - 1) / 8;\n+\treg_pair_required_p = true;\n+      break;\n     case ACCESS_REGS:\n-      return (GET_MODE_SIZE (mode) + 4 - 1) / 4;\n+      reg_size = 4;\n+      break;\n     default:\n+      reg_size = UNITS_PER_WORD;\n       break;\n     }\n-  return (GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1) / UNITS_PER_WORD;\n+\n+  if (reg_pair_required_p)\n+    return 2 * ((GET_MODE_SIZE (mode) / 2 + reg_size - 1) / reg_size);\n+\n+  return (GET_MODE_SIZE (mode) + reg_size - 1) / reg_size;\n+}\n+\n+/* Return TRUE if changing mode from FROM to TO should not be allowed\n+   for register class CLASS.  */\n+\n+int\n+s390_cannot_change_mode_class (machine_mode from_mode,\n+\t\t\t       machine_mode to_mode,\n+\t\t\t       enum reg_class rclass)\n+{\n+  machine_mode small_mode;\n+  machine_mode big_mode;\n+\n+  if (GET_MODE_SIZE (from_mode) == GET_MODE_SIZE (to_mode))\n+    return 0;\n+\n+  if (GET_MODE_SIZE (from_mode) < GET_MODE_SIZE (to_mode))\n+    {\n+      small_mode = from_mode;\n+      big_mode = to_mode;\n+    }\n+  else\n+    {\n+      small_mode = to_mode;\n+      big_mode = from_mode;\n+    }\n+\n+  /* Values residing in VRs are little-endian style.  All modes are\n+     placed left-aligned in an VR.  This means that we cannot allow\n+     switching between modes with differing sizes.  Also if the vector\n+     facility is available we still place TFmode values in VR register\n+     pairs, since the only instructions we have operating on TFmodes\n+     only deal with register pairs.  Therefore we have to allow DFmode\n+     subregs of TFmodes to enable the TFmode splitters.  */\n+  if (reg_classes_intersect_p (VEC_REGS, rclass)\n+      && (GET_MODE_SIZE (small_mode) < 8\n+\t  || s390_class_max_nregs (VEC_REGS, big_mode) == 1))\n+    return 1;\n+\n+  /* Likewise for access registers, since they have only half the\n+     word size on 64-bit.  */\n+  if (reg_classes_intersect_p (ACCESS_REGS, rclass))\n+    return 1;\n+\n+  return 0;\n }\n \n /* Return true if we use LRA instead of reload pass.  */\n@@ -9228,6 +9938,23 @@ s390_can_use_return_insn (void)\n   return cfun_frame_layout.frame_size == 0;\n }\n \n+/* The VX ABI differs for vararg functions.  Therefore we need the\n+   prototype of the callee to be available when passing vector type\n+   values.  */\n+static const char *\n+s390_invalid_arg_for_unprototyped_fn (const_tree typelist, const_tree funcdecl, const_tree val)\n+{\n+  return ((TARGET_VX_ABI\n+\t   && typelist == 0\n+\t   && VECTOR_TYPE_P (TREE_TYPE (val))\n+\t   && (funcdecl == NULL_TREE\n+\t       || (TREE_CODE (funcdecl) == FUNCTION_DECL\n+\t\t   && DECL_BUILT_IN_CLASS (funcdecl) != BUILT_IN_MD)))\n+\t  ? N_(\"Vector argument passed to unprototyped function\")\n+\t  : NULL);\n+}\n+\n+\n /* Return the size in bytes of a function argument of\n    type TYPE and/or mode MODE.  At least one of TYPE or\n    MODE must be specified.  */\n@@ -9246,14 +9973,62 @@ s390_function_arg_size (machine_mode mode, const_tree type)\n   gcc_unreachable ();\n }\n \n+/* Return true if a function argument of type TYPE and mode MODE\n+   is to be passed in a vector register, if available.  */\n+\n+bool\n+s390_function_arg_vector (machine_mode mode, const_tree type)\n+{\n+  if (!TARGET_VX_ABI)\n+    return false;\n+\n+  if (s390_function_arg_size (mode, type) > 16)\n+    return false;\n+\n+  /* No type info available for some library calls ...  */\n+  if (!type)\n+    return VECTOR_MODE_P (mode);\n+\n+  /* The ABI says that record types with a single member are treated\n+     just like that member would be.  */\n+  while (TREE_CODE (type) == RECORD_TYPE)\n+    {\n+      tree field, single = NULL_TREE;\n+\n+      for (field = TYPE_FIELDS (type); field; field = DECL_CHAIN (field))\n+\t{\n+\t  if (TREE_CODE (field) != FIELD_DECL)\n+\t    continue;\n+\n+\t  if (single == NULL_TREE)\n+\t    single = TREE_TYPE (field);\n+\t  else\n+\t    return false;\n+\t}\n+\n+      if (single == NULL_TREE)\n+\treturn false;\n+      else\n+\t{\n+\t  /* If the field declaration adds extra byte due to\n+\t     e.g. padding this is not accepted as vector type.  */\n+\t  if (int_size_in_bytes (single) <= 0\n+\t      || int_size_in_bytes (single) != int_size_in_bytes (type))\n+\t    return false;\n+\t  type = single;\n+\t}\n+    }\n+\n+  return VECTOR_TYPE_P (type);\n+}\n+\n /* Return true if a function argument of type TYPE and mode MODE\n    is to be passed in a floating-point register, if available.  */\n \n static bool\n s390_function_arg_float (machine_mode mode, const_tree type)\n {\n-  int size = s390_function_arg_size (mode, type);\n-  if (size > 8)\n+  if (s390_function_arg_size (mode, type) > 8)\n     return false;\n \n   /* Soft-float changes the ABI: no floating-point registers are used.  */\n@@ -9336,20 +10111,24 @@ s390_pass_by_reference (cumulative_args_t ca ATTRIBUTE_UNUSED,\n \t\t\tbool named ATTRIBUTE_UNUSED)\n {\n   int size = s390_function_arg_size (mode, type);\n+\n+  if (s390_function_arg_vector (mode, type))\n+    return false;\n+\n   if (size > 8)\n     return true;\n \n   if (type)\n     {\n       if (AGGREGATE_TYPE_P (type) && exact_log2 (size) < 0)\n-        return 1;\n+        return true;\n \n       if (TREE_CODE (type) == COMPLEX_TYPE\n \t  || TREE_CODE (type) == VECTOR_TYPE)\n-        return 1;\n+\treturn true;\n     }\n \n-  return 0;\n+  return false;\n }\n \n /* Update the data in CUM to advance over an argument of mode MODE and\n@@ -9360,11 +10139,21 @@ s390_pass_by_reference (cumulative_args_t ca ATTRIBUTE_UNUSED,\n \n static void\n s390_function_arg_advance (cumulative_args_t cum_v, machine_mode mode,\n-\t\t\t   const_tree type, bool named ATTRIBUTE_UNUSED)\n+\t\t\t   const_tree type, bool named)\n {\n   CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);\n \n-  if (s390_function_arg_float (mode, type))\n+  if (s390_function_arg_vector (mode, type))\n+    {\n+      /* We are called for unnamed vector stdarg arguments which are\n+\t passed on the stack.  In this case this hook does not have to\n+\t do anything since stack arguments are tracked by common\n+\t code.  */\n+      if (!named)\n+\treturn;\n+      cum->vrs += 1;\n+    }\n+  else if (s390_function_arg_float (mode, type))\n     {\n       cum->fprs += 1;\n     }\n@@ -9398,14 +10187,24 @@ s390_function_arg_advance (cumulative_args_t cum_v, machine_mode mode,\n \n static rtx\n s390_function_arg (cumulative_args_t cum_v, machine_mode mode,\n-\t\t   const_tree type, bool named ATTRIBUTE_UNUSED)\n+\t\t   const_tree type, bool named)\n {\n   CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);\n \n-  if (s390_function_arg_float (mode, type))\n+\n+  if (s390_function_arg_vector (mode, type))\n+    {\n+      /* Vector arguments being part of the ellipsis are passed on the\n+\t stack.  */\n+      if (!named || (cum->vrs + 1 > VEC_ARG_NUM_REG))\n+\treturn NULL_RTX;\n+\n+      return gen_rtx_REG (mode, cum->vrs + FIRST_VEC_ARG_REGNO);\n+    }\n+  else if (s390_function_arg_float (mode, type))\n     {\n       if (cum->fprs + 1 > FP_ARG_NUM_REG)\n-\treturn 0;\n+\treturn NULL_RTX;\n       else\n \treturn gen_rtx_REG (mode, cum->fprs + 16);\n     }\n@@ -9415,7 +10214,7 @@ s390_function_arg (cumulative_args_t cum_v, machine_mode mode,\n       int n_gprs = (size + UNITS_PER_LONG - 1) / UNITS_PER_LONG;\n \n       if (cum->gprs + n_gprs > GP_ARG_NUM_REG)\n-\treturn 0;\n+\treturn NULL_RTX;\n       else if (n_gprs == 1 || UNITS_PER_WORD == UNITS_PER_LONG)\n \treturn gen_rtx_REG (mode, cum->gprs + 2);\n       else if (n_gprs == 2)\n@@ -9458,11 +10257,17 @@ s390_return_in_memory (const_tree type, const_tree fundecl ATTRIBUTE_UNUSED)\n       || TREE_CODE (type) == REAL_TYPE)\n     return int_size_in_bytes (type) > 8;\n \n+  /* vector types which fit into a VR.  */\n+  if (TARGET_VX_ABI\n+      && VECTOR_TYPE_P (type)\n+      && int_size_in_bytes (type) <= 16)\n+    return false;\n+\n   /* Aggregates and similar constructs are always returned\n      in memory.  */\n   if (AGGREGATE_TYPE_P (type)\n       || TREE_CODE (type) == COMPLEX_TYPE\n-      || TREE_CODE (type) == VECTOR_TYPE)\n+      || VECTOR_TYPE_P (type))\n     return true;\n \n   /* ??? We get called on all sorts of random stuff from\n@@ -9500,6 +10305,12 @@ s390_function_and_libcall_value (machine_mode mode,\n \t\t\t\t const_tree fntype_or_decl,\n \t\t\t\t bool outgoing ATTRIBUTE_UNUSED)\n {\n+  /* For vector return types it is important to use the RET_TYPE\n+     argument whenever available since the middle-end might have\n+     changed the mode to a scalar mode.  */\n+  bool vector_ret_type_p = ((ret_type && VECTOR_TYPE_P (ret_type))\n+\t\t\t    || (!ret_type && VECTOR_MODE_P (mode)));\n+\n   /* For normal functions perform the promotion as\n      promote_function_mode would do.  */\n   if (ret_type)\n@@ -9509,10 +10320,14 @@ s390_function_and_libcall_value (machine_mode mode,\n \t\t\t\t    fntype_or_decl, 1);\n     }\n \n-  gcc_assert (GET_MODE_CLASS (mode) == MODE_INT || SCALAR_FLOAT_MODE_P (mode));\n-  gcc_assert (GET_MODE_SIZE (mode) <= 8);\n+  gcc_assert (GET_MODE_CLASS (mode) == MODE_INT\n+\t      || SCALAR_FLOAT_MODE_P (mode)\n+\t      || (TARGET_VX_ABI && vector_ret_type_p));\n+  gcc_assert (GET_MODE_SIZE (mode) <= (TARGET_VX_ABI ? 16 : 8));\n \n-  if (TARGET_HARD_FLOAT && SCALAR_FLOAT_MODE_P (mode))\n+  if (TARGET_VX_ABI && vector_ret_type_p)\n+    return gen_rtx_REG (mode, FIRST_VEC_ARG_REGNO);\n+  else if (TARGET_HARD_FLOAT && SCALAR_FLOAT_MODE_P (mode))\n     return gen_rtx_REG (mode, 16);\n   else if (GET_MODE_SIZE (mode) <= UNITS_PER_LONG\n \t   || UNITS_PER_LONG == UNITS_PER_WORD)\n@@ -9676,9 +10491,13 @@ s390_va_start (tree valist, rtx nextarg ATTRIBUTE_UNUSED)\n       expand_expr (t, const0_rtx, VOIDmode, EXPAND_NORMAL);\n     }\n \n-  /* Find the overflow area.  */\n+  /* Find the overflow area.\n+     FIXME: This currently is too pessimistic when the vector ABI is\n+     enabled.  In that case we *always* set up the overflow area\n+     pointer.  */\n   if (n_gpr + cfun->va_list_gpr_size > GP_ARG_NUM_REG\n-      || n_fpr + cfun->va_list_fpr_size > FP_ARG_NUM_REG)\n+      || n_fpr + cfun->va_list_fpr_size > FP_ARG_NUM_REG\n+      || TARGET_VX_ABI)\n     {\n       t = make_tree (TREE_TYPE (ovf), virtual_incoming_args_rtx);\n \n@@ -9720,6 +10539,9 @@ s390_va_start (tree valist, rtx nextarg ATTRIBUTE_UNUSED)\n        ret = args.reg_save_area[args.gpr+8]\n      else\n        ret = *args.overflow_arg_area++;\n+   } else if (vector value) {\n+       ret = *args.overflow_arg_area;\n+       args.overflow_arg_area += size / 8;\n    } else if (float value) {\n      if (args.fgpr < 2)\n        ret = args.reg_save_area[args.fpr+64]\n@@ -9739,7 +10561,10 @@ s390_gimplify_va_arg (tree valist, tree type, gimple_seq *pre_p,\n   tree f_gpr, f_fpr, f_ovf, f_sav;\n   tree gpr, fpr, ovf, sav, reg, t, u;\n   int indirect_p, size, n_reg, sav_ofs, sav_scale, max_reg;\n-  tree lab_false, lab_over, addr;\n+  tree lab_false, lab_over;\n+  tree addr = create_tmp_var (ptr_type_node, \"addr\");\n+  bool left_align_p; /* How a value < UNITS_PER_LONG is aligned within\n+\t\t\ta stack slot.  */\n \n   f_gpr = TYPE_FIELDS (TREE_TYPE (va_list_type_node));\n   f_fpr = DECL_CHAIN (f_gpr);\n@@ -9777,6 +10602,23 @@ s390_gimplify_va_arg (tree valist, tree type, gimple_seq *pre_p,\n       sav_scale = UNITS_PER_LONG;\n       size = UNITS_PER_LONG;\n       max_reg = GP_ARG_NUM_REG - n_reg;\n+      left_align_p = false;\n+    }\n+  else if (s390_function_arg_vector (TYPE_MODE (type), type))\n+    {\n+      if (TARGET_DEBUG_ARG)\n+\t{\n+\t  fprintf (stderr, \"va_arg: vector type\");\n+\t  debug_tree (type);\n+\t}\n+\n+      indirect_p = 0;\n+      reg = NULL_TREE;\n+      n_reg = 0;\n+      sav_ofs = 0;\n+      sav_scale = 8;\n+      max_reg = 0;\n+      left_align_p = true;\n     }\n   else if (s390_function_arg_float (TYPE_MODE (type), type))\n     {\n@@ -9793,6 +10635,7 @@ s390_gimplify_va_arg (tree valist, tree type, gimple_seq *pre_p,\n       sav_ofs = 16 * UNITS_PER_LONG;\n       sav_scale = 8;\n       max_reg = FP_ARG_NUM_REG - n_reg;\n+      left_align_p = false;\n     }\n   else\n     {\n@@ -9817,53 +10660,74 @@ s390_gimplify_va_arg (tree valist, tree type, gimple_seq *pre_p,\n \n       sav_scale = UNITS_PER_LONG;\n       max_reg = GP_ARG_NUM_REG - n_reg;\n+      left_align_p = false;\n     }\n \n   /* Pull the value out of the saved registers ...  */\n \n-  lab_false = create_artificial_label (UNKNOWN_LOCATION);\n-  lab_over = create_artificial_label (UNKNOWN_LOCATION);\n-  addr = create_tmp_var (ptr_type_node, \"addr\");\n+  if (reg != NULL_TREE)\n+    {\n+      /*\n+\tif (reg > ((typeof (reg))max_reg))\n+          goto lab_false;\n \n-  t = fold_convert (TREE_TYPE (reg), size_int (max_reg));\n-  t = build2 (GT_EXPR, boolean_type_node, reg, t);\n-  u = build1 (GOTO_EXPR, void_type_node, lab_false);\n-  t = build3 (COND_EXPR, void_type_node, t, u, NULL_TREE);\n-  gimplify_and_add (t, pre_p);\n+        addr = sav + sav_ofs + reg * save_scale;\n \n-  t = fold_build_pointer_plus_hwi (sav, sav_ofs);\n-  u = build2 (MULT_EXPR, TREE_TYPE (reg), reg,\n-\t      fold_convert (TREE_TYPE (reg), size_int (sav_scale)));\n-  t = fold_build_pointer_plus (t, u);\n+\tgoto lab_over;\n \n-  gimplify_assign (addr, t, pre_p);\n+        lab_false:\n+      */\n+\n+      lab_false = create_artificial_label (UNKNOWN_LOCATION);\n+      lab_over = create_artificial_label (UNKNOWN_LOCATION);\n+\n+      t = fold_convert (TREE_TYPE (reg), size_int (max_reg));\n+      t = build2 (GT_EXPR, boolean_type_node, reg, t);\n+      u = build1 (GOTO_EXPR, void_type_node, lab_false);\n+      t = build3 (COND_EXPR, void_type_node, t, u, NULL_TREE);\n+      gimplify_and_add (t, pre_p);\n \n-  gimple_seq_add_stmt (pre_p, gimple_build_goto (lab_over));\n+      t = fold_build_pointer_plus_hwi (sav, sav_ofs);\n+      u = build2 (MULT_EXPR, TREE_TYPE (reg), reg,\n+\t\t  fold_convert (TREE_TYPE (reg), size_int (sav_scale)));\n+      t = fold_build_pointer_plus (t, u);\n \n-  gimple_seq_add_stmt (pre_p, gimple_build_label (lab_false));\n+      gimplify_assign (addr, t, pre_p);\n \n+      gimple_seq_add_stmt (pre_p, gimple_build_goto (lab_over));\n+\n+      gimple_seq_add_stmt (pre_p, gimple_build_label (lab_false));\n+    }\n \n   /* ... Otherwise out of the overflow area.  */\n \n   t = ovf;\n-  if (size < UNITS_PER_LONG)\n+  if (size < UNITS_PER_LONG && !left_align_p)\n     t = fold_build_pointer_plus_hwi (t, UNITS_PER_LONG - size);\n \n   gimplify_expr (&t, pre_p, NULL, is_gimple_val, fb_rvalue);\n \n   gimplify_assign (addr, t, pre_p);\n \n-  t = fold_build_pointer_plus_hwi (t, size);\n+  if (size < UNITS_PER_LONG && left_align_p)\n+    t = fold_build_pointer_plus_hwi (t, UNITS_PER_LONG);\n+  else\n+    t = fold_build_pointer_plus_hwi (t, size);\n+\n   gimplify_assign (ovf, t, pre_p);\n \n-  gimple_seq_add_stmt (pre_p, gimple_build_label (lab_over));\n+  if (reg != NULL_TREE)\n+    gimple_seq_add_stmt (pre_p, gimple_build_label (lab_over));\n \n \n   /* Increment register save count.  */\n \n-  u = build2 (PREINCREMENT_EXPR, TREE_TYPE (reg), reg,\n-\t      fold_convert (TREE_TYPE (reg), size_int (n_reg)));\n-  gimplify_and_add (u, pre_p);\n+  if (n_reg > 0)\n+    {\n+      u = build2 (PREINCREMENT_EXPR, TREE_TYPE (reg), reg,\n+\t\t  fold_convert (TREE_TYPE (reg), size_int (n_reg)));\n+      gimplify_and_add (u, pre_p);\n+    }\n \n   if (indirect_p)\n     {\n@@ -10664,15 +11528,18 @@ s390_call_saved_register_used (tree call_expr)\n       mode = TYPE_MODE (type);\n       gcc_assert (mode);\n \n+      /* We assume that in the target function all parameters are\n+\t named.  This only has an impact on vector argument register\n+\t usage none of which is call-saved.  */\n       if (pass_by_reference (&cum_v, mode, type, true))\n  \t{\n  \t  mode = Pmode;\n  \t  type = build_pointer_type (type);\n  \t}\n \n-       parm_rtx = s390_function_arg (cum, mode, type, 0);\n+       parm_rtx = s390_function_arg (cum, mode, type, true);\n \n-       s390_function_arg_advance (cum, mode, type, 0);\n+       s390_function_arg_advance (cum, mode, type, true);\n \n        if (!parm_rtx)\n \t continue;\n@@ -10879,6 +11746,13 @@ s390_conditional_register_usage (void)\n       for (i = FPR0_REGNUM; i <= FPR15_REGNUM; i++)\n \tcall_used_regs[i] = fixed_regs[i] = 1;\n     }\n+\n+  /* Disable v16 - v31 for non-vector target.  */\n+  if (!TARGET_VX)\n+    {\n+      for (i = VR16_REGNUM; i <= VR31_REGNUM; i++)\n+\tfixed_regs[i] = call_used_regs[i] = call_really_used_regs[i] = 1;\n+    }\n }\n \n /* Corresponding function to eh_return expander.  */\n@@ -12249,6 +13123,55 @@ s390_atomic_assign_expand_fenv (tree *hold, tree *clear, tree *update)\n #undef FPC_DXC_SHIFT\n }\n \n+/* Return the vector mode to be used for inner mode MODE when doing\n+   vectorization.  */\n+static machine_mode\n+s390_preferred_simd_mode (machine_mode mode)\n+{\n+  if (TARGET_VX)\n+    switch (mode)\n+      {\n+      case DFmode:\n+\treturn V2DFmode;\n+      case DImode:\n+\treturn V2DImode;\n+      case SImode:\n+\treturn V4SImode;\n+      case HImode:\n+\treturn V8HImode;\n+      case QImode:\n+\treturn V16QImode;\n+      default:;\n+      }\n+  return word_mode;\n+}\n+\n+/* Our hardware does not require vectors to be strictly aligned.  */\n+static bool\n+s390_support_vector_misalignment (machine_mode mode ATTRIBUTE_UNUSED,\n+\t\t\t\t  const_tree type ATTRIBUTE_UNUSED,\n+\t\t\t\t  int misalignment ATTRIBUTE_UNUSED,\n+\t\t\t\t  bool is_packed ATTRIBUTE_UNUSED)\n+{\n+  return true;\n+}\n+\n+/* The vector ABI requires vector types to be aligned on an 8 byte\n+   boundary (our stack alignment).  However, we allow this to be\n+   overriden by the user, while this definitely breaks the ABI.  */\n+static HOST_WIDE_INT\n+s390_vector_alignment (const_tree type)\n+{\n+  if (!TARGET_VX_ABI)\n+    return default_vector_alignment (type);\n+\n+  if (TYPE_USER_ALIGN (type))\n+    return TYPE_ALIGN (type);\n+\n+  return MIN (64, tree_to_shwi (TYPE_SIZE (type)));\n+}\n+\n+\n /* Initialize GCC target structure.  */\n \n #undef  TARGET_ASM_ALIGNED_HI_OP\n@@ -12357,6 +13280,8 @@ s390_atomic_assign_expand_fenv (tree *hold, tree *clear, tree *update)\n #define TARGET_FUNCTION_VALUE s390_function_value\n #undef TARGET_LIBCALL_VALUE\n #define TARGET_LIBCALL_VALUE s390_libcall_value\n+#undef TARGET_STRICT_ARGUMENT_NAMING\n+#define TARGET_STRICT_ARGUMENT_NAMING hook_bool_CUMULATIVE_ARGS_true\n \n #undef TARGET_KEEP_LEAF_WHEN_PROFILED\n #define TARGET_KEEP_LEAF_WHEN_PROFILED s390_keep_leaf_when_profiled\n@@ -12375,6 +13300,9 @@ s390_atomic_assign_expand_fenv (tree *hold, tree *clear, tree *update)\n #define TARGET_ASM_OUTPUT_DWARF_DTPREL s390_output_dwarf_dtprel\n #endif\n \n+#undef TARGET_DWARF_FRAME_REG_MODE\n+#define TARGET_DWARF_FRAME_REG_MODE s390_dwarf_frame_reg_mode\n+\n #ifdef TARGET_ALTERNATE_LONG_DOUBLE_MANGLING\n #undef TARGET_MANGLE_TYPE\n #define TARGET_MANGLE_TYPE s390_mangle_type\n@@ -12383,6 +13311,9 @@ s390_atomic_assign_expand_fenv (tree *hold, tree *clear, tree *update)\n #undef TARGET_SCALAR_MODE_SUPPORTED_P\n #define TARGET_SCALAR_MODE_SUPPORTED_P s390_scalar_mode_supported_p\n \n+#undef TARGET_VECTOR_MODE_SUPPORTED_P\n+#define TARGET_VECTOR_MODE_SUPPORTED_P s390_vector_mode_supported_p\n+\n #undef  TARGET_PREFERRED_RELOAD_CLASS\n #define TARGET_PREFERRED_RELOAD_CLASS s390_preferred_reload_class\n \n@@ -12443,6 +13374,18 @@ s390_atomic_assign_expand_fenv (tree *hold, tree *clear, tree *update)\n #undef TARGET_ATOMIC_ASSIGN_EXPAND_FENV\n #define TARGET_ATOMIC_ASSIGN_EXPAND_FENV s390_atomic_assign_expand_fenv\n \n+#undef TARGET_INVALID_ARG_FOR_UNPROTOTYPED_FN\n+#define TARGET_INVALID_ARG_FOR_UNPROTOTYPED_FN s390_invalid_arg_for_unprototyped_fn\n+\n+#undef TARGET_VECTORIZE_PREFERRED_SIMD_MODE\n+#define TARGET_VECTORIZE_PREFERRED_SIMD_MODE s390_preferred_simd_mode\n+\n+#undef TARGET_VECTORIZE_SUPPORT_VECTOR_MISALIGNMENT\n+#define TARGET_VECTORIZE_SUPPORT_VECTOR_MISALIGNMENT s390_support_vector_misalignment\n+\n+#undef TARGET_VECTOR_ALIGNMENT\n+#define TARGET_VECTOR_ALIGNMENT s390_vector_alignment\n+\n struct gcc_target targetm = TARGET_INITIALIZER;\n \n #include \"gt-s390.h\""}, {"sha": "9d151e6806c26e4b8378e4d2d84e81b4d14f622d", "filename": "gcc/config/s390/s390.h", "status": "modified", "additions": 123, "deletions": 51, "changes": 174, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fs390.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fs390.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fs390.h?ref=085261c8042d644baaf594bc436b87326c1c0390", "patch": "@@ -199,6 +199,13 @@ enum processor_flags\n \n #define STACK_SIZE_MODE (Pmode)\n \n+/* Vector arguments are left-justified when placed on the stack during\n+   parameter passing.  */\n+#define FUNCTION_ARG_PADDING(MODE, TYPE)\t\t\t\\\n+  (s390_function_arg_vector ((MODE), (TYPE))\t\t\t\\\n+   ? upward\t\t\t\t\t\t\t\\\n+   : DEFAULT_FUNCTION_ARG_PADDING ((MODE), (TYPE)))\n+\n #ifndef IN_LIBGCC2\n \n /* Width of a word, in units (bytes).  */\n@@ -296,9 +303,11 @@ enum processor_flags\n    Reg 35: Return address pointer\n \n    Registers 36 and 37 are mapped to access registers\n-   0 and 1, used to implement thread-local storage.  */\n+   0 and 1, used to implement thread-local storage.\n+\n+   Reg 38-53: Vector registers v16-v31  */\n \n-#define FIRST_PSEUDO_REGISTER 38\n+#define FIRST_PSEUDO_REGISTER 54\n \n /* Standard register usage.  */\n #define GENERAL_REGNO_P(N)\t((int)(N) >= 0 && (N) < 16)\n@@ -307,13 +316,17 @@ enum processor_flags\n #define CC_REGNO_P(N)\t\t((N) == 33)\n #define FRAME_REGNO_P(N)\t((N) == 32 || (N) == 34 || (N) == 35)\n #define ACCESS_REGNO_P(N)\t((N) == 36 || (N) == 37)\n+#define VECTOR_NOFP_REGNO_P(N)  ((N) >= 38 && (N) <= 53)\n+#define VECTOR_REGNO_P(N)       (FP_REGNO_P (N) || VECTOR_NOFP_REGNO_P (N))\n \n #define GENERAL_REG_P(X)\t(REG_P (X) && GENERAL_REGNO_P (REGNO (X)))\n #define ADDR_REG_P(X)\t\t(REG_P (X) && ADDR_REGNO_P (REGNO (X)))\n #define FP_REG_P(X)\t\t(REG_P (X) && FP_REGNO_P (REGNO (X)))\n #define CC_REG_P(X)\t\t(REG_P (X) && CC_REGNO_P (REGNO (X)))\n #define FRAME_REG_P(X)\t\t(REG_P (X) && FRAME_REGNO_P (REGNO (X)))\n #define ACCESS_REG_P(X)\t\t(REG_P (X) && ACCESS_REGNO_P (REGNO (X)))\n+#define VECTOR_NOFP_REG_P(X)    (REG_P (X) && VECTOR_NOFP_REGNO_P (REGNO (X)))\n+#define VECTOR_REG_P(X)         (REG_P (X) && VECTOR_REGNO_P (REGNO (X)))\n \n /* Set up fixed registers and calling convention:\n \n@@ -328,7 +341,9 @@ enum processor_flags\n \n    On 31-bit, FPRs 18-19 are call-clobbered;\n    on 64-bit, FPRs 24-31 are call-clobbered.\n-   The remaining FPRs are call-saved.  */\n+   The remaining FPRs are call-saved.\n+\n+   All non-FP vector registers are call-clobbered v16-v31.  */\n \n #define FIXED_REGISTERS\t\t\t\t\\\n { 0, 0, 0, 0, \t\t\t\t\t\\\n@@ -340,7 +355,11 @@ enum processor_flags\n   0, 0, 0, 0, \t\t\t\t\t\\\n   0, 0, 0, 0, \t\t\t\t\t\\\n   1, 1, 1, 1,\t\t\t\t\t\\\n-  1, 1 }\n+  1, 1,\t\t\t\t\t\t\\\n+  0, 0, 0, 0, \t\t\t\t\t\\\n+  0, 0, 0, 0, \t\t\t\t\t\\\n+  0, 0, 0, 0, \t\t\t\t\t\\\n+  0, 0, 0, 0 }\n \n #define CALL_USED_REGISTERS\t\t\t\\\n { 1, 1, 1, 1, \t\t\t\t\t\\\n@@ -352,26 +371,35 @@ enum processor_flags\n   1, 1, 1, 1, \t\t\t\t\t\\\n   1, 1, 1, 1, \t\t\t\t\t\\\n   1, 1, 1, 1,\t\t\t\t\t\\\n-  1, 1 }\n+  1, 1,\t\t\t\t\t        \\\n+  1, 1, 1, 1, \t\t\t\t\t\\\n+  1, 1, 1, 1,\t\t\t\t\t\\\n+  1, 1, 1, 1, \t\t\t\t\t\\\n+  1, 1, 1, 1 }\n \n #define CALL_REALLY_USED_REGISTERS\t\t\\\n-{ 1, 1, 1, 1, \t\t\t\t\t\\\n+{ 1, 1, 1, 1, \t/* r0 - r15 */\t\t\t\\\n   1, 1, 0, 0, \t\t\t\t\t\\\n   0, 0, 0, 0, \t\t\t\t\t\\\n   0, 0, 0, 0,\t\t\t\t\t\\\n+  1, 1, 1, 1, \t/* f0 (16) - f15 (31) */\t\\\n   1, 1, 1, 1, \t\t\t\t\t\\\n   1, 1, 1, 1, \t\t\t\t\t\\\n   1, 1, 1, 1, \t\t\t\t\t\\\n-  1, 1, 1, 1, \t\t\t\t\t\\\n+  1, 1, 1, 1,\t/* arg, cc, fp, ret addr */\t\\\n+  0, 0,\t\t/* a0 (36), a1 (37) */\t        \\\n+  1, 1, 1, 1, \t/* v16 (38) - v23 (45) */\t\\\n   1, 1, 1, 1,\t\t\t\t\t\\\n-  0, 0 }\n+  1, 1, 1, 1, \t/* v24 (46) - v31 (53) */\t\\\n+  1, 1, 1, 1 }\n \n /* Preferred register allocation order.  */\n-#define REG_ALLOC_ORDER                                         \\\n-{  1, 2, 3, 4, 5, 0, 12, 11, 10, 9, 8, 7, 6, 14, 13,            \\\n-   16, 17, 18, 19, 20, 21, 22, 23,                              \\\n-   24, 25, 26, 27, 28, 29, 30, 31,                              \\\n-   15, 32, 33, 34, 35, 36, 37 }\n+#define REG_ALLOC_ORDER\t\t\t\t\t\t\t\\\n+  {  1, 2, 3, 4, 5, 0, 12, 11, 10, 9, 8, 7, 6, 14, 13,\t\t\t\\\n+     16, 17, 18, 19, 20, 21, 22, 23,\t\t\t\t\t\\\n+     24, 25, 26, 27, 28, 29, 30, 31,\t\t\t\t\t\\\n+     38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, \t\\\n+     15, 32, 33, 34, 35, 36, 37 }\n \n \n /* Fitting values into registers.  */\n@@ -411,26 +439,22 @@ enum processor_flags\n    but conforms to the 31-bit ABI, GPRs can hold 8 bytes;\n    the ABI guarantees only that the lower 4 bytes are\n    saved across calls, however.  */\n-#define HARD_REGNO_CALL_PART_CLOBBERED(REGNO, MODE)\t\t\\\n-  (!TARGET_64BIT && TARGET_ZARCH\t\t\t\t\\\n-   && GET_MODE_SIZE (MODE) > 4\t\t\t\t\t\\\n-   && (((REGNO) >= 6 && (REGNO) <= 15) || (REGNO) == 32))\n+#define HARD_REGNO_CALL_PART_CLOBBERED(REGNO, MODE)\t\t\t\\\n+  ((!TARGET_64BIT && TARGET_ZARCH\t\t\t\t\t\\\n+    && GET_MODE_SIZE (MODE) > 4\t\t\t\t\t\t\\\n+    && (((REGNO) >= 6 && (REGNO) <= 15) || (REGNO) == 32))\t\t\\\n+   || (TARGET_VX\t\t\t\t\t\t\t\\\n+       && GET_MODE_SIZE (MODE) > 8\t\t\t\t\t\\\n+       && (((TARGET_64BIT && (REGNO) >= 24 && (REGNO) <= 31))\t\t\\\n+\t   || (!TARGET_64BIT && ((REGNO) == 18 || (REGNO) == 19)))))\n \n /* Maximum number of registers to represent a value of mode MODE\n    in a register of class CLASS.  */\n #define CLASS_MAX_NREGS(CLASS, MODE)   \t\t\t\t\t\\\n   s390_class_max_nregs ((CLASS), (MODE))\n \n-/* If a 4-byte value is loaded into a FPR, it is placed into the\n-   *upper* half of the register, not the lower.  Therefore, we\n-   cannot use SUBREGs to switch between modes in FP registers.\n-   Likewise for access registers, since they have only half the\n-   word size on 64-bit.  */\n #define CANNOT_CHANGE_MODE_CLASS(FROM, TO, CLASS)\t\t        \\\n-  (GET_MODE_SIZE (FROM) != GET_MODE_SIZE (TO)\t\t\t        \\\n-   ? ((reg_classes_intersect_p (FP_REGS, CLASS)\t\t\t\t\\\n-       && (GET_MODE_SIZE (FROM) < 8 || GET_MODE_SIZE (TO) < 8))\t\t\\\n-      || reg_classes_intersect_p (ACCESS_REGS, CLASS)) : 0)\n+  s390_cannot_change_mode_class ((FROM), (TO), (CLASS))\n \n /* Register classes.  */\n \n@@ -458,18 +482,21 @@ enum reg_class\n   NO_REGS, CC_REGS, ADDR_REGS, GENERAL_REGS, ACCESS_REGS,\n   ADDR_CC_REGS, GENERAL_CC_REGS,\n   FP_REGS, ADDR_FP_REGS, GENERAL_FP_REGS,\n+  VEC_REGS, ADDR_VEC_REGS, GENERAL_VEC_REGS,\n   ALL_REGS, LIM_REG_CLASSES\n };\n #define N_REG_CLASSES (int) LIM_REG_CLASSES\n \n #define REG_CLASS_NAMES\t\t\t\t\t\t\t\\\n { \"NO_REGS\", \"CC_REGS\", \"ADDR_REGS\", \"GENERAL_REGS\", \"ACCESS_REGS\",\t\\\n   \"ADDR_CC_REGS\", \"GENERAL_CC_REGS\",\t\t\t\t\t\\\n-  \"FP_REGS\", \"ADDR_FP_REGS\", \"GENERAL_FP_REGS\", \"ALL_REGS\" }\n+  \"FP_REGS\", \"ADDR_FP_REGS\", \"GENERAL_FP_REGS\",\t\t\t\t\\\n+  \"VEC_REGS\", \"ADDR_VEC_REGS\", \"GENERAL_VEC_REGS\",\t\t\t\\\n+  \"ALL_REGS\" }\n \n /* Class -> register mapping.  */\n-#define REG_CLASS_CONTENTS \\\n-{\t\t\t\t       \t\t\t\\\n+#define REG_CLASS_CONTENTS\t\t\t\t\\\n+{\t\t\t\t\t\t\t\\\n   { 0x00000000, 0x00000000 },\t/* NO_REGS */\t\t\\\n   { 0x00000000, 0x00000002 },\t/* CC_REGS */\t\t\\\n   { 0x0000fffe, 0x0000000d },\t/* ADDR_REGS */\t\t\\\n@@ -480,7 +507,10 @@ enum reg_class\n   { 0xffff0000, 0x00000000 },\t/* FP_REGS */\t\t\\\n   { 0xfffffffe, 0x0000000d },\t/* ADDR_FP_REGS */\t\\\n   { 0xffffffff, 0x0000000d },\t/* GENERAL_FP_REGS */\t\\\n-  { 0xffffffff, 0x0000003f },\t/* ALL_REGS */\t\t\\\n+  { 0xffff0000, 0x003fffc0 },\t/* VEC_REGS */\t\t\\\n+  { 0xfffffffe, 0x003fffcd },\t/* ADDR_VEC_REGS */\t\\\n+  { 0xffffffff, 0x003fffcd },\t/* GENERAL_VEC_REGS */\t\\\n+  { 0xffffffff, 0x003fffff },\t/* ALL_REGS */\t\t\\\n }\n \n /* In some case register allocation order is not enough for IRA to\n@@ -511,14 +541,27 @@ extern const enum reg_class regclass_map[FIRST_PSEUDO_REGISTER];\n #define REGNO_OK_FOR_BASE_P(REGNO) REGNO_OK_FOR_INDEX_P (REGNO)\n \n \n-/* We need secondary memory to move data between GPRs and FPRs.  With\n-   DFP the ldgr lgdr instructions are available.  But these\n-   instructions do not handle GPR pairs so it is not possible for 31\n-   bit.  */\n-#define SECONDARY_MEMORY_NEEDED(CLASS1, CLASS2, MODE) \\\n- ((CLASS1) != (CLASS2)                                \\\n-  && ((CLASS1) == FP_REGS || (CLASS2) == FP_REGS)     \\\n-  && (!TARGET_DFP || !TARGET_64BIT || GET_MODE_SIZE (MODE) != 8))\n+/* We need secondary memory to move data between GPRs and FPRs.\n+\n+   - With DFP the ldgr lgdr instructions are available.  Due to the\n+     different alignment we cannot use them for SFmode.  For 31 bit a\n+     64 bit value in GPR would be a register pair so here we still\n+     need to go via memory.\n+\n+   - With z13 we can do the SF/SImode moves with vlgvf.  Due to the\n+     overlapping of FPRs and VRs we still disallow TF/TD modes to be\n+     in full VRs so as before also on z13 we do these moves via\n+     memory.\n+\n+     FIXME: Should we try splitting it into two vlgvg's/vlvg's instead?  */\n+#define SECONDARY_MEMORY_NEEDED(CLASS1, CLASS2, MODE)\t\t\t\\\n+  (((reg_classes_intersect_p (CLASS1, VEC_REGS)\t\t\t\t\\\n+     && reg_classes_intersect_p (CLASS2, GENERAL_REGS))\t\t\t\\\n+    || (reg_classes_intersect_p (CLASS1, GENERAL_REGS)\t\t\t\\\n+\t&& reg_classes_intersect_p (CLASS2, VEC_REGS)))\t\t\t\\\n+   && (!TARGET_DFP || !TARGET_64BIT || GET_MODE_SIZE (MODE) != 8)\t\\\n+   && (!TARGET_VX || (SCALAR_FLOAT_MODE_P (MODE)\t\t\t\\\n+\t\t\t  && GET_MODE_SIZE (MODE) > 8)))\n \n /* Get_secondary_mem widens its argument to BITS_PER_WORD which loses on 64bit\n    because the movsi and movsf patterns don't handle r/f moves.  */\n@@ -612,6 +655,11 @@ extern const enum reg_class regclass_map[FIRST_PSEUDO_REGISTER];\n /* Let the assembler generate debug line info.  */\n #define DWARF2_ASM_LINE_DEBUG_INFO 1\n \n+/* Define the dwarf register mapping.\n+   v16-v31 -> 68-83\n+   rX      -> X      otherwise  */\n+#define DBX_REGISTER_NUMBER(regno)\t\t\t\\\n+  ((regno >= 38 && regno <= 53) ? regno + 30 : regno)\n \n /* Frame registers.  */\n \n@@ -659,21 +707,29 @@ typedef struct s390_arg_structure\n {\n   int gprs;\t\t\t/* gpr so far */\n   int fprs;\t\t\t/* fpr so far */\n+  int vrs;                      /* vr so far */\n }\n CUMULATIVE_ARGS;\n \n #define INIT_CUMULATIVE_ARGS(CUM, FNTYPE, LIBNAME, NN, N_NAMED_ARGS) \\\n-  ((CUM).gprs=0, (CUM).fprs=0)\n+  ((CUM).gprs=0, (CUM).fprs=0, (CUM).vrs=0)\n+\n+#define FIRST_VEC_ARG_REGNO 46\n+#define LAST_VEC_ARG_REGNO 53\n \n /* Arguments can be placed in general registers 2 to 6, or in floating\n    point registers 0 and 2 for 31 bit and fprs 0, 2, 4 and 6 for 64\n    bit.  */\n-#define FUNCTION_ARG_REGNO_P(N) (((N) >=2 && (N) <7) || \\\n-  (N) == 16 || (N) == 17 || (TARGET_64BIT && ((N) == 18 || (N) == 19)))\n+#define FUNCTION_ARG_REGNO_P(N)\t\t\t\t\t\t\\\n+  (((N) >=2 && (N) < 7) || (N) == 16 || (N) == 17\t\t\t\\\n+   || (TARGET_64BIT && ((N) == 18 || (N) == 19))\t\t\t\\\n+   || (TARGET_VX && ((N) >= FIRST_VEC_ARG_REGNO && (N) <= LAST_VEC_ARG_REGNO)))\n \n \n-/* Only gpr 2 and fpr 0 are ever used as return registers.  */\n-#define FUNCTION_VALUE_REGNO_P(N) ((N) == 2 || (N) == 16)\n+/* Only gpr 2, fpr 0, and v24 are ever used as return registers.  */\n+#define FUNCTION_VALUE_REGNO_P(N)\t\t\\\n+  ((N) == 2 || (N) == 16\t\t\t\\\n+   || (TARGET_VX && (N) == FIRST_VEC_ARG_REGNO))\n \n \n /* Function entry and exit.  */\n@@ -833,12 +889,20 @@ do {\t\t\t\t\t\t\t\t\t\\\n /* How to refer to registers in assembler output.  This sequence is\n    indexed by compiler's hard-register-number (see above).  */\n #define REGISTER_NAMES\t\t\t\t\t\t\t\\\n-{ \"%r0\",  \"%r1\",  \"%r2\",  \"%r3\",  \"%r4\",  \"%r5\",  \"%r6\",  \"%r7\",\t\\\n-  \"%r8\",  \"%r9\",  \"%r10\", \"%r11\", \"%r12\", \"%r13\", \"%r14\", \"%r15\",\t\\\n-  \"%f0\",  \"%f2\",  \"%f4\",  \"%f6\",  \"%f1\",  \"%f3\",  \"%f5\",  \"%f7\",\t\\\n-  \"%f8\",  \"%f10\", \"%f12\", \"%f14\", \"%f9\",  \"%f11\", \"%f13\", \"%f15\",\t\\\n-  \"%ap\",  \"%cc\",  \"%fp\",  \"%rp\",  \"%a0\",  \"%a1\"\t\t\t\t\\\n-}\n+  { \"%r0\",  \"%r1\",  \"%r2\",  \"%r3\",  \"%r4\",  \"%r5\",  \"%r6\",  \"%r7\",\t\\\n+    \"%r8\",  \"%r9\",  \"%r10\", \"%r11\", \"%r12\", \"%r13\", \"%r14\", \"%r15\",\t\\\n+    \"%f0\",  \"%f2\",  \"%f4\",  \"%f6\",  \"%f1\",  \"%f3\",  \"%f5\",  \"%f7\",\t\\\n+    \"%f8\",  \"%f10\", \"%f12\", \"%f14\", \"%f9\",  \"%f11\", \"%f13\", \"%f15\",\t\\\n+    \"%ap\",  \"%cc\",  \"%fp\",  \"%rp\",  \"%a0\",  \"%a1\",\t\t\t\\\n+    \"%v16\", \"%v18\", \"%v20\", \"%v22\", \"%v17\", \"%v19\", \"%v21\", \"%v23\",\t\\\n+    \"%v24\", \"%v26\", \"%v28\", \"%v30\", \"%v25\", \"%v27\", \"%v29\", \"%v31\"\t\\\n+  }\n+\n+#define ADDITIONAL_REGISTER_NAMES\t\t\t\t\t\\\n+  { { \"v0\", 16 }, { \"v2\",  17 }, { \"v4\",  18 }, { \"v6\",  19 },\t\t\\\n+    { \"v1\", 20 }, { \"v3\",  21 }, { \"v5\",  22 }, { \"v7\",  23 },          \\\n+    { \"v8\", 24 }, { \"v10\", 25 }, { \"v12\", 26 }, { \"v14\", 27 },          \\\n+    { \"v9\", 28 }, { \"v11\", 29 }, { \"v13\", 30 }, { \"v15\", 31 } };\n \n /* Print operand X (an rtx) in assembler syntax to file FILE.  */\n #define PRINT_OPERAND(FILE, X, CODE) print_operand (FILE, X, CODE)\n@@ -908,13 +972,21 @@ do {\t\t\t\t\t\t\t\t\t\\\n #define SYMBOL_REF_NOT_NATURALLY_ALIGNED_P(X) \\\n   ((SYMBOL_REF_FLAGS (X) & SYMBOL_FLAG_NOT_NATURALLY_ALIGNED))\n \n+/* Check whether integer displacement is in range for a short displacement.  */\n+#define SHORT_DISP_IN_RANGE(d) ((d) >= 0 && (d) <= 4095)\n+\n /* Check whether integer displacement is in range.  */\n #define DISP_IN_RANGE(d) \\\n   (TARGET_LONG_DISPLACEMENT? ((d) >= -524288 && (d) <= 524287) \\\n-                           : ((d) >= 0 && (d) <= 4095))\n+                           : SHORT_DISP_IN_RANGE(d))\n \n /* Reads can reuse write prefetches, used by tree-ssa-prefetch-loops.c.  */\n #define READ_CAN_USE_WRITE_PREFETCH 1\n \n extern const int processor_flags_table[];\n-#endif\n+\n+/* The truth element value for vector comparisons.  Our instructions\n+   always generate -1 in that case.  */\n+#define VECTOR_STORE_FLAG_VALUE(MODE) CONSTM1_RTX (GET_MODE_INNER (MODE))\n+\n+#endif /* S390_H */"}, {"sha": "ef087c2e73053ab4a84b4fb94967fa57a0114845", "filename": "gcc/config/s390/s390.md", "status": "modified", "additions": 267, "deletions": 117, "changes": 384, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fs390.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fs390.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fs390.md?ref=085261c8042d644baaf594bc436b87326c1c0390", "patch": "@@ -125,7 +125,23 @@\n    UNSPEC_FPINT_CEIL\n    UNSPEC_FPINT_NEARBYINT\n    UNSPEC_FPINT_RINT\n- ])\n+\n+   ; Vector\n+   UNSPEC_VEC_EXTRACT\n+   UNSPEC_VEC_SET\n+   UNSPEC_VEC_PERM\n+   UNSPEC_VEC_SRLB\n+   UNSPEC_VEC_GENBYTEMASK\n+   UNSPEC_VEC_VSUM\n+   UNSPEC_VEC_VSUMG\n+   UNSPEC_VEC_SMULT_EVEN\n+   UNSPEC_VEC_UMULT_EVEN\n+   UNSPEC_VEC_SMULT_ODD\n+   UNSPEC_VEC_UMULT_ODD\n+   UNSPEC_VEC_LOAD_LEN\n+   UNSPEC_VEC_VFENE\n+   UNSPEC_VEC_VFENECC\n+])\n \n ;;\n ;; UNSPEC_VOLATILE usage\n@@ -216,6 +232,11 @@\n    (FPR13_REGNUM                30)\n    (FPR14_REGNUM                27)\n    (FPR15_REGNUM                31)\n+   (VR0_REGNUM                  16)\n+   (VR16_REGNUM                 38)\n+   (VR23_REGNUM                 45)\n+   (VR24_REGNUM                 46)\n+   (VR31_REGNUM                 53)\n   ])\n \n ;;\n@@ -246,7 +267,7 @@\n ;; Used to determine defaults for length and other attribute values.\n \n (define_attr \"op_type\"\n-  \"NN,E,RR,RRE,RX,RS,RSI,RI,SI,S,SS,SSE,RXE,RSE,RIL,RIE,RXY,RSY,SIY,RRF,RRR,SIL,RRS,RIS\"\n+  \"NN,E,RR,RRE,RX,RS,RSI,RI,SI,S,SS,SSE,RXE,RSE,RIL,RIE,RXY,RSY,SIY,RRF,RRR,SIL,RRS,RIS,VRI,VRR,VRS,VRV,VRX\"\n   (const_string \"NN\"))\n \n ;; Instruction type attribute used for scheduling.\n@@ -403,12 +424,13 @@\n \n ;; Iterators\n \n+(define_mode_iterator ALL [TI DI SI HI QI TF DF SF TD DD SD V1QI V2QI V4QI V8QI V16QI V1HI V2HI V4HI V8HI V1SI V2SI V4SI V1DI V2DI V1SF V2SF V4SF V1TI V1DF V2DF V1TF])\n+\n ;; These mode iterators allow floating point patterns to be generated from the\n ;; same template.\n (define_mode_iterator FP_ALL [TF DF SF (TD \"TARGET_HARD_DFP\") (DD \"TARGET_HARD_DFP\")\n                               (SD \"TARGET_HARD_DFP\")])\n (define_mode_iterator FP [TF DF SF (TD \"TARGET_HARD_DFP\") (DD \"TARGET_HARD_DFP\")])\n-(define_mode_iterator FPALL [TF DF SF TD DD SD])\n (define_mode_iterator BFP [TF DF SF])\n (define_mode_iterator DFP [TD DD])\n (define_mode_iterator DFP_ALL [TD DD SD])\n@@ -444,7 +466,6 @@\n ;; This mode iterator allows the integer patterns to be defined from the\n ;; same template.\n (define_mode_iterator INT [(DI \"TARGET_ZARCH\") SI HI QI])\n-(define_mode_iterator INTALL [TI DI SI HI QI])\n (define_mode_iterator DINT [(TI \"TARGET_ZARCH\") DI SI HI QI])\n \n ;; This iterator allows some 'ashift' and 'lshiftrt' pattern to be defined from\n@@ -614,6 +635,8 @@\n ;; Allow return and simple_return to be defined from a single template.\n (define_code_iterator ANY_RETURN [return simple_return])\n \n+(include \"vector.md\")\n+\n ;;\n ;;- Compare instructions.\n ;;\n@@ -1246,17 +1269,27 @@\n ; movti instruction pattern(s).\n ;\n \n+; FIXME: More constants are possible by enabling jxx, jyy constraints\n+; for TImode (use double-int for the calculations)\n (define_insn \"movti\"\n-  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"=d,QS,d,o\")\n-        (match_operand:TI 1 \"general_operand\" \"QS,d,dPRT,d\"))]\n+  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"=d,QS,v,  v,  v,v,d, v,QR,   d,o\")\n+        (match_operand:TI 1 \"general_operand\"      \"QS, d,v,j00,jm1,d,v,QR, v,dPRT,d\"))]\n   \"TARGET_ZARCH\"\n   \"@\n    lmg\\t%0,%N0,%S1\n    stmg\\t%1,%N1,%S0\n+   vlr\\t%v0,%v1\n+   vzero\\t%v0\n+   vone\\t%v0\n+   vlvgp\\t%v0,%1,%N1\n+   #\n+   vl\\t%v0,%1\n+   vst\\t%v1,%0\n    #\n    #\"\n-  [(set_attr \"op_type\" \"RSY,RSY,*,*\")\n-   (set_attr \"type\" \"lm,stm,*,*\")])\n+  [(set_attr \"op_type\" \"RSY,RSY,VRR,VRI,VRI,VRR,*,VRX,VRX,*,*\")\n+   (set_attr \"type\" \"lm,stm,*,*,*,*,*,*,*,*,*\")\n+   (set_attr \"cpu_facility\" \"*,*,vec,vec,vec,vec,vec,vec,vec,*,*\")])\n \n (define_split\n   [(set (match_operand:TI 0 \"nonimmediate_operand\" \"\")\n@@ -1286,10 +1319,14 @@\n   operands[5] = operand_subword (operands[1], 0, 0, TImode);\n })\n \n+; Use part of the TImode target reg to perform the address\n+; calculation.  If the TImode value is supposed to be copied into a VR\n+; this splitter is not necessary.\n (define_split\n   [(set (match_operand:TI 0 \"register_operand\" \"\")\n         (match_operand:TI 1 \"memory_operand\" \"\"))]\n   \"TARGET_ZARCH && reload_completed\n+   && !VECTOR_REG_P (operands[0])\n    && !s_operand (operands[1], VOIDmode)\"\n   [(set (match_dup 0) (match_dup 1))]\n {\n@@ -1300,6 +1337,25 @@\n })\n \n \n+; Split a VR -> GPR TImode move into 2 vector load GR from VR element.\n+; For the higher order bits we do simply a DImode move while the\n+; second part is done via vec extract.  Both will end up as vlgvg.\n+(define_split\n+  [(set (match_operand:TI 0 \"register_operand\" \"\")\n+        (match_operand:TI 1 \"register_operand\" \"\"))]\n+  \"TARGET_VX && reload_completed\n+   && GENERAL_REG_P (operands[0])\n+   && VECTOR_REG_P (operands[1])\"\n+  [(set (match_dup 2) (match_dup 4))\n+   (set (match_dup 3) (unspec:DI [(match_dup 5) (const_int 1)]\n+\t\t\t\t UNSPEC_VEC_EXTRACT))]\n+{\n+  operands[2] = operand_subword (operands[0], 0, 0, TImode);\n+  operands[3] = operand_subword (operands[0], 1, 0, TImode);\n+  operands[4] = gen_rtx_REG (DImode, REGNO (operands[1]));\n+  operands[5] = gen_rtx_REG (V2DImode, REGNO (operands[1]));\n+})\n+\n ;\n ; Patterns used for secondary reloads\n ;\n@@ -1308,40 +1364,20 @@\n ; Unfortunately there is no such variant for QI, TI and FP mode moves.\n ; These patterns are also used for unaligned SI and DI accesses.\n \n-(define_expand \"reload<INTALL:mode><P:mode>_tomem_z10\"\n-  [(parallel [(match_operand:INTALL 0 \"memory_operand\"   \"\")\n-\t      (match_operand:INTALL 1 \"register_operand\" \"=d\")\n-\t      (match_operand:P 2 \"register_operand\" \"=&a\")])]\n+(define_expand \"reload<ALL:mode><P:mode>_tomem_z10\"\n+  [(parallel [(match_operand:ALL 0 \"memory_operand\"   \"\")\n+\t      (match_operand:ALL 1 \"register_operand\" \"=d\")\n+\t      (match_operand:P   2 \"register_operand\" \"=&a\")])]\n   \"TARGET_Z10\"\n {\n   s390_reload_symref_address (operands[1], operands[0], operands[2], 1);\n   DONE;\n })\n \n-(define_expand \"reload<INTALL:mode><P:mode>_toreg_z10\"\n-  [(parallel [(match_operand:INTALL 0 \"register_operand\" \"=d\")\n-\t      (match_operand:INTALL 1 \"memory_operand\"   \"\")\n-\t      (match_operand:P 2 \"register_operand\" \"=a\")])]\n-  \"TARGET_Z10\"\n-{\n-  s390_reload_symref_address (operands[0], operands[1], operands[2], 0);\n-  DONE;\n-})\n-\n-(define_expand \"reload<FPALL:mode><P:mode>_tomem_z10\"\n-  [(parallel [(match_operand:FPALL 0 \"memory_operand\"   \"\")\n-\t      (match_operand:FPALL 1 \"register_operand\" \"=d\")\n-\t      (match_operand:P 2 \"register_operand\" \"=&a\")])]\n-  \"TARGET_Z10\"\n-{\n-  s390_reload_symref_address (operands[1], operands[0], operands[2], 1);\n-  DONE;\n-})\n-\n-(define_expand \"reload<FPALL:mode><P:mode>_toreg_z10\"\n-  [(parallel [(match_operand:FPALL 0 \"register_operand\" \"=d\")\n-\t      (match_operand:FPALL 1 \"memory_operand\"   \"\")\n-\t      (match_operand:P 2 \"register_operand\" \"=a\")])]\n+(define_expand \"reload<ALL:mode><P:mode>_toreg_z10\"\n+  [(parallel [(match_operand:ALL 0 \"register_operand\" \"=d\")\n+\t      (match_operand:ALL 1 \"memory_operand\"   \"\")\n+\t      (match_operand:P   2 \"register_operand\" \"=a\")])]\n   \"TARGET_Z10\"\n {\n   s390_reload_symref_address (operands[0], operands[1], operands[2], 0);\n@@ -1370,9 +1406,16 @@\n   DONE;\n })\n \n-; Handles assessing a non-offsetable memory address\n+; Not all the indirect memory access instructions support the full\n+; format (long disp + index + base).  So whenever a move from/to such\n+; an address is required and the instruction cannot deal with it we do\n+; a load address into a scratch register first and use this as the new\n+; base register.\n+; This in particular is used for:\n+; - non-offsetable memory accesses for multiword moves\n+; - full vector reg moves with long displacements\n \n-(define_expand \"reload<mode>_nonoffmem_in\"\n+(define_expand \"reload<mode>_la_in\"\n   [(parallel [(match_operand 0   \"register_operand\" \"\")\n               (match_operand 1   \"\" \"\")\n               (match_operand:P 2 \"register_operand\" \"=&a\")])]\n@@ -1385,7 +1428,7 @@\n   DONE;\n })\n \n-(define_expand \"reload<mode>_nonoffmem_out\"\n+(define_expand \"reload<mode>_la_out\"\n   [(parallel [(match_operand   0 \"\" \"\")\n               (match_operand   1 \"register_operand\" \"\")\n               (match_operand:P 2 \"register_operand\" \"=&a\")])]\n@@ -1438,11 +1481,9 @@\n \n (define_insn \"*movdi_64\"\n   [(set (match_operand:DI 0 \"nonimmediate_operand\"\n-                            \"=d,d,d,d,d,d,d,d,f,d,d,d,d,d,\n-                             RT,!*f,!*f,!*f,!R,!T,b,Q,d,t,Q,t\")\n+         \"=d,    d,    d,    d,    d, d,    d,    d,f,d,d,d,d, d,RT,!*f,!*f,!*f,!R,!T,b,Q,d,t,Q,t,v,v,v,d, v,QR\")\n         (match_operand:DI 1 \"general_operand\"\n-                            \"K,N0HD0,N1HD0,N2HD0,N3HD0,Os,N0SD0,N1SD0,d,f,L,b,d,RT,\n-                             d,*f,R,T,*f,*f,d,K,t,d,t,Q\"))]\n+         \" K,N0HD0,N1HD0,N2HD0,N3HD0,Os,N0SD0,N1SD0,d,f,L,b,d,RT, d, *f,  R,  T,*f,*f,d,K,t,d,t,Q,K,v,d,v,QR, v\"))]\n   \"TARGET_ZARCH\"\n   \"@\n    lghi\\t%0,%h1\n@@ -1470,15 +1511,21 @@\n    #\n    #\n    stam\\t%1,%N1,%S0\n-   lam\\t%0,%N0,%S1\"\n+   lam\\t%0,%N0,%S1\n+   vleig\\t%v0,%h1,0\n+   vlr\\t%v0,%v1\n+   vlvgg\\t%v0,%1,0\n+   vlgvg\\t%0,%v1,0\n+   vleg\\t%v0,%1,0\n+   vsteg\\t%v1,%0,0\"\n   [(set_attr \"op_type\" \"RI,RI,RI,RI,RI,RIL,RIL,RIL,RRE,RRE,RXY,RIL,RRE,RXY,\n-                        RXY,RR,RX,RXY,RX,RXY,RIL,SIL,*,*,RS,RS\")\n+                        RXY,RR,RX,RXY,RX,RXY,RIL,SIL,*,*,RS,RS,VRI,VRR,VRS,VRS,VRX,VRX\")\n    (set_attr \"type\" \"*,*,*,*,*,*,*,*,floaddf,floaddf,la,larl,lr,load,store,\n-                     floaddf,floaddf,floaddf,fstoredf,fstoredf,larl,*,*,*,\n-                     *,*\")\n+                     floaddf,floaddf,floaddf,fstoredf,fstoredf,larl,*,*,*,*,\n+                     *,*,*,*,*,*,*\")\n    (set_attr \"cpu_facility\" \"*,*,*,*,*,extimm,extimm,extimm,dfp,dfp,longdisp,\n                              z10,*,*,*,*,*,longdisp,*,longdisp,\n-                             z10,z10,*,*,*,*\")\n+                             z10,z10,*,*,*,*,vec,vec,vec,vec,vec,vec\")\n    (set_attr \"z10prop\" \"z10_fwd_A1,\n                         z10_fwd_E1,\n                         z10_fwd_E1,\n@@ -1504,7 +1551,7 @@\n                         *,\n                         *,\n                         *,\n-                        *\")\n+                        *,*,*,*,*,*,*\")\n ])\n \n (define_split\n@@ -1696,9 +1743,9 @@\n \n (define_insn \"*movsi_zarch\"\n   [(set (match_operand:SI 0 \"nonimmediate_operand\"\n-\t\t\t    \"=d,d,d,d,d,d,d,d,d,R,T,!*f,!*f,!*f,!R,!T,d,t,Q,b,Q,t\")\n+\t \"=d,    d,    d, d,d,d,d,d,d,R,T,!*f,!*f,!*f,!*f,!*f,!R,!T,d,t,Q,b,Q,t,v,v,v,d, v,QR\")\n         (match_operand:SI 1 \"general_operand\"\n-\t\t\t    \"K,N0HS0,N1HS0,Os,L,b,d,R,T,d,d,*f,R,T,*f,*f,t,d,t,d,K,Q\"))]\n+\t \" K,N0HS0,N1HS0,Os,L,b,d,R,T,d,d, *f, *f,  R,  R,  T,*f,*f,t,d,t,d,K,Q,K,v,d,v,QR, v\"))]\n   \"TARGET_ZARCH\"\n   \"@\n    lhi\\t%0,%h1\n@@ -1712,7 +1759,9 @@\n    ly\\t%0,%1\n    st\\t%1,%0\n    sty\\t%1,%0\n+   lder\\t%0,%1\n    ler\\t%0,%1\n+   lde\\t%0,%1\n    le\\t%0,%1\n    ley\\t%0,%1\n    ste\\t%1,%0\n@@ -1722,9 +1771,15 @@\n    stam\\t%1,%1,%S0\n    strl\\t%1,%0\n    mvhi\\t%0,%1\n-   lam\\t%0,%0,%S1\"\n+   lam\\t%0,%0,%S1\n+   vleif\\t%v0,%h1,0\n+   vlr\\t%v0,%v1\n+   vlvgf\\t%v0,%1,0\n+   vlgvf\\t%0,%v1,0\n+   vlef\\t%v0,%1,0\n+   vstef\\t%v1,%0,0\"\n   [(set_attr \"op_type\" \"RI,RI,RI,RIL,RXY,RIL,RR,RX,RXY,RX,RXY,\n-                        RR,RX,RXY,RX,RXY,RRE,RRE,RS,RIL,SIL,RS\")\n+                        RRE,RR,RXE,RX,RXY,RX,RXY,RRE,RRE,RS,RIL,SIL,RS,VRI,VRR,VRS,VRS,VRX,VRX\")\n    (set_attr \"type\" \"*,\n                      *,\n                      *,\n@@ -1739,16 +1794,18 @@\n                      floadsf,\n                      floadsf,\n                      floadsf,\n+                     floadsf,\n+                     floadsf,\n                      fstoresf,\n                      fstoresf,\n                      *,\n                      *,\n                      *,\n                      larl,\n                      *,\n-                     *\")\n+                     *,*,*,*,*,*,*\")\n    (set_attr \"cpu_facility\" \"*,*,*,extimm,longdisp,z10,*,*,longdisp,*,longdisp,\n-                             *,*,longdisp,*,longdisp,*,*,*,z10,z10,*\")\n+                             vec,*,vec,*,longdisp,*,longdisp,*,*,*,z10,z10,*,vec,vec,vec,vec,vec,vec\")\n    (set_attr \"z10prop\" \"z10_fwd_A1,\n                         z10_fwd_E1,\n                         z10_fwd_E1,\n@@ -1765,42 +1822,38 @@\n                         *,\n                         *,\n                         *,\n+                        *,\n+                        *,\n                         z10_super_E1,\n                         z10_super,\n                         *,\n                         z10_rec,\n                         z10_super,\n-                        *\")])\n+                        *,*,*,*,*,*,*\")])\n \n (define_insn \"*movsi_esa\"\n-  [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=d,d,d,R,!*f,!*f,!R,d,t,Q,t\")\n-        (match_operand:SI 1 \"general_operand\" \"K,d,R,d,*f,R,*f,t,d,t,Q\"))]\n+  [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=d,d,d,R,!*f,!*f,!*f,!*f,!R,d,t,Q,t\")\n+        (match_operand:SI 1 \"general_operand\"       \"K,d,R,d, *f, *f,  R,  R,*f,t,d,t,Q\"))]\n   \"!TARGET_ZARCH\"\n   \"@\n    lhi\\t%0,%h1\n    lr\\t%0,%1\n    l\\t%0,%1\n    st\\t%1,%0\n+   lder\\t%0,%1\n    ler\\t%0,%1\n+   lde\\t%0,%1\n    le\\t%0,%1\n    ste\\t%1,%0\n    ear\\t%0,%1\n    sar\\t%0,%1\n    stam\\t%1,%1,%S0\n    lam\\t%0,%0,%S1\"\n-  [(set_attr \"op_type\" \"RI,RR,RX,RX,RR,RX,RX,RRE,RRE,RS,RS\")\n-   (set_attr \"type\" \"*,lr,load,store,floadsf,floadsf,fstoresf,*,*,*,*\")\n-   (set_attr \"z10prop\" \"z10_fwd_A1,\n-                        z10_fr_E1,\n-                        z10_fwd_A3,\n-                        z10_rec,\n-                        *,\n-                        *,\n-                        *,\n-                        z10_super_E1,\n-                        z10_super,\n-                        *,\n-                        *\")\n+  [(set_attr \"op_type\" \"RI,RR,RX,RX,RRE,RR,RXE,RX,RX,RRE,RRE,RS,RS\")\n+   (set_attr \"type\" \"*,lr,load,store,floadsf,floadsf,floadsf,floadsf,fstoresf,*,*,*,*\")\n+   (set_attr \"z10prop\" \"z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_rec,*,*,*,*,*,z10_super_E1,\n+                        z10_super,*,*\")\n+   (set_attr \"cpu_facility\" \"*,*,*,*,vec,*,vec,*,*,*,*,*,*\")\n ])\n \n (define_peephole2\n@@ -1910,8 +1963,8 @@\n })\n \n (define_insn \"*movhi\"\n-  [(set (match_operand:HI 0 \"nonimmediate_operand\" \"=d,d,d,d,d,R,T,b,Q\")\n-        (match_operand:HI 1 \"general_operand\"      \" d,n,R,T,b,d,d,d,K\"))]\n+  [(set (match_operand:HI 0 \"nonimmediate_operand\" \"=d,d,d,d,d,R,T,b,Q,v,v,v,d, v,QR\")\n+        (match_operand:HI 1 \"general_operand\"      \" d,n,R,T,b,d,d,d,K,K,v,d,v,QR, v\"))]\n   \"\"\n   \"@\n    lr\\t%0,%1\n@@ -1922,10 +1975,16 @@\n    sth\\t%1,%0\n    sthy\\t%1,%0\n    sthrl\\t%1,%0\n-   mvhhi\\t%0,%1\"\n-  [(set_attr \"op_type\"      \"RR,RI,RX,RXY,RIL,RX,RXY,RIL,SIL\")\n-   (set_attr \"type\"         \"lr,*,*,*,larl,store,store,store,*\")\n-   (set_attr \"cpu_facility\" \"*,*,*,*,z10,*,*,z10,z10\")\n+   mvhhi\\t%0,%1\n+   vleih\\t%v0,%h1,0\n+   vlr\\t%v0,%v1\n+   vlvgh\\t%v0,%1,0\n+   vlgvh\\t%0,%v1,0\n+   vleh\\t%v0,%1,0\n+   vsteh\\t%v1,%0,0\"\n+  [(set_attr \"op_type\"      \"RR,RI,RX,RXY,RIL,RX,RXY,RIL,SIL,VRI,VRR,VRS,VRS,VRX,VRX\")\n+   (set_attr \"type\"         \"lr,*,*,*,larl,store,store,store,*,*,*,*,*,*,*\")\n+   (set_attr \"cpu_facility\" \"*,*,*,*,z10,*,*,z10,z10,vec,vec,vec,vec,vec,vec\")\n    (set_attr \"z10prop\" \"z10_fr_E1,\n                        z10_fwd_A1,\n                        z10_super_E1,\n@@ -1934,7 +1993,7 @@\n                        z10_rec,\n                        z10_rec,\n                        z10_rec,\n-                       z10_super\")])\n+                       z10_super,*,*,*,*,*,*\")])\n \n (define_peephole2\n   [(set (match_operand:HI 0 \"register_operand\" \"\")\n@@ -1969,8 +2028,8 @@\n })\n \n (define_insn \"*movqi\"\n-  [(set (match_operand:QI 0 \"nonimmediate_operand\" \"=d,d,d,d,R,T,Q,S,?Q\")\n-        (match_operand:QI 1 \"general_operand\"      \" d,n,R,T,d,d,n,n,?Q\"))]\n+  [(set (match_operand:QI 0 \"nonimmediate_operand\" \"=d,d,d,d,R,T,Q,S,?Q,v,v,v,d, v,QR\")\n+        (match_operand:QI 1 \"general_operand\"      \" d,n,R,T,d,d,n,n,?Q,K,v,d,v,QR, v\"))]\n   \"\"\n   \"@\n    lr\\t%0,%1\n@@ -1981,9 +2040,16 @@\n    stcy\\t%1,%0\n    mvi\\t%S0,%b1\n    mviy\\t%S0,%b1\n-   #\"\n-  [(set_attr \"op_type\" \"RR,RI,RX,RXY,RX,RXY,SI,SIY,SS\")\n-   (set_attr \"type\" \"lr,*,*,*,store,store,store,store,*\")\n+   #\n+   vleib\\t%v0,%b1,0\n+   vlr\\t%v0,%v1\n+   vlvgb\\t%v0,%1,0\n+   vlgvb\\t%0,%v1,0\n+   vleb\\t%v0,%1,0\n+   vsteb\\t%v1,%0,0\"\n+  [(set_attr \"op_type\" \"RR,RI,RX,RXY,RX,RXY,SI,SIY,SS,VRI,VRR,VRS,VRS,VRX,VRX\")\n+   (set_attr \"type\" \"lr,*,*,*,store,store,store,store,*,*,*,*,*,*,*\")\n+   (set_attr \"cpu_facility\" \"*,*,*,*,*,*,*,*,*,vec,vec,vec,vec,vec,vec\")\n    (set_attr \"z10prop\" \"z10_fr_E1,\n                         z10_fwd_A1,\n                         z10_super_E1,\n@@ -1992,7 +2058,7 @@\n                         z10_rec,\n                         z10_super,\n                         z10_super,\n-                        *\")])\n+                        *,*,*,*,*,*,*\")])\n \n (define_peephole2\n   [(set (match_operand:QI 0 \"nonimmediate_operand\" \"\")\n@@ -2124,7 +2190,7 @@\n   [(set (match_operand:TD_TF 0 \"register_operand\" \"\")\n         (match_operand:TD_TF 1 \"memory_operand\"   \"\"))]\n   \"TARGET_ZARCH && reload_completed\n-   && !FP_REG_P (operands[0])\n+   && GENERAL_REG_P (operands[0])\n    && !s_operand (operands[1], VOIDmode)\"\n   [(set (match_dup 0) (match_dup 1))]\n {\n@@ -2180,9 +2246,9 @@\n \n (define_insn \"*mov<mode>_64dfp\"\n   [(set (match_operand:DD_DF 0 \"nonimmediate_operand\"\n-\t\t\t       \"=f,f,f,d,f,f,R,T,d,d, d,RT\")\n+\t\t\t       \"=f,f,f,d,f,f,R,T,d,d,d, d,b,RT,v,v,d,v,QR\")\n         (match_operand:DD_DF 1 \"general_operand\"\n-\t\t\t       \" G,f,d,f,R,T,f,f,G,d,RT, d\"))]\n+\t\t\t       \" G,f,d,f,R,T,f,f,G,d,b,RT,d, d,v,d,v,QR,v\"))]\n   \"TARGET_DFP\"\n   \"@\n    lzdr\\t%0\n@@ -2195,17 +2261,24 @@\n    stdy\\t%1,%0\n    lghi\\t%0,0\n    lgr\\t%0,%1\n+   lgrl\\t%0,%1\n    lg\\t%0,%1\n-   stg\\t%1,%0\"\n-  [(set_attr \"op_type\" \"RRE,RR,RRE,RRE,RX,RXY,RX,RXY,RI,RRE,RXY,RXY\")\n+   stgrl\\t%1,%0\n+   stg\\t%1,%0\n+   vlr\\t%v0,%v1\n+   vlvgg\\t%v0,%1,0\n+   vlgvg\\t%0,%v1,0\n+   vleg\\t%0,%1,0\n+   vsteg\\t%1,%0,0\"\n+  [(set_attr \"op_type\" \"RRE,RR,RRE,RRE,RX,RXY,RX,RXY,RI,RRE,RIL,RXY,RIL,RXY,VRR,VRS,VRS,VRX,VRX\")\n    (set_attr \"type\" \"fsimpdf,floaddf,floaddf,floaddf,floaddf,floaddf,\n-                     fstoredf,fstoredf,*,lr,load,store\")\n-   (set_attr \"z10prop\" \"*,*,*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_rec\")\n-   (set_attr \"cpu_facility\" \"z196,*,*,*,*,*,*,*,*,*,*,*\")])\n+                     fstoredf,fstoredf,*,lr,load,load,store,store,*,*,*,load,store\")\n+   (set_attr \"z10prop\" \"*,*,*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_fwd_A3,z10_rec,z10_rec,*,*,*,*,*\")\n+   (set_attr \"cpu_facility\" \"z196,*,*,*,*,*,*,*,*,*,z10,*,z10,*,vec,vec,vec,vec,vec\")])\n \n (define_insn \"*mov<mode>_64\"\n-  [(set (match_operand:DD_DF 0 \"nonimmediate_operand\" \"=f,f,f,f,R,T,d,d, d,RT\")\n-        (match_operand:DD_DF 1 \"general_operand\"      \" G,f,R,T,f,f,G,d,RT, d\"))]\n+  [(set (match_operand:DD_DF 0 \"nonimmediate_operand\" \"=f,f,f,f,R,T,d,d,d, d,b,RT,v,v,QR\")\n+        (match_operand:DD_DF 1 \"general_operand\"      \" G,f,R,T,f,f,G,d,b,RT,d, d,v,QR,v\"))]\n   \"TARGET_ZARCH\"\n   \"@\n    lzdr\\t%0\n@@ -2216,13 +2289,18 @@\n    stdy\\t%1,%0\n    lghi\\t%0,0\n    lgr\\t%0,%1\n+   lgrl\\t%0,%1\n    lg\\t%0,%1\n-   stg\\t%1,%0\"\n-  [(set_attr \"op_type\" \"RRE,RR,RX,RXY,RX,RXY,RI,RRE,RXY,RXY\")\n+   stgrl\\t%1,%0\n+   stg\\t%1,%0\n+   vlr\\t%v0,%v1\n+   vleg\\t%v0,%1,0\n+   vsteg\\t%v1,%0,0\"\n+  [(set_attr \"op_type\" \"RRE,RR,RX,RXY,RX,RXY,RI,RRE,RIL,RXY,RIL,RXY,VRR,VRX,VRX\")\n    (set_attr \"type\"    \"fsimpdf,fload<mode>,fload<mode>,fload<mode>,\n-                        fstore<mode>,fstore<mode>,*,lr,load,store\")\n-   (set_attr \"z10prop\" \"*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_rec\")\n-   (set_attr \"cpu_facility\" \"z196,*,*,*,*,*,*,*,*,*\")])\n+                        fstore<mode>,fstore<mode>,*,lr,load,load,store,store,*,load,store\")\n+   (set_attr \"z10prop\" \"*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_fwd_A3,z10_rec,z10_rec,*,*,*\")\n+   (set_attr \"cpu_facility\" \"z196,*,*,*,*,*,*,*,z10,*,z10,*,vec,vec,vec\")])\n \n (define_insn \"*mov<mode>_31\"\n   [(set (match_operand:DD_DF 0 \"nonimmediate_operand\"\n@@ -2295,28 +2373,38 @@\n \n (define_insn \"mov<mode>\"\n   [(set (match_operand:SD_SF 0 \"nonimmediate_operand\"\n-\t\t\t       \"=f,f,f,f,R,T,d,d,d,d,R,T\")\n+\t\t\t       \"=f,f,f,f,f,f,R,T,d,d,d,d,d,b,R,T,v,v,v,d,v,QR\")\n         (match_operand:SD_SF 1 \"general_operand\"\n-\t\t\t       \" G,f,R,T,f,f,G,d,R,T,d,d\"))]\n+\t\t\t       \" G,f,f,R,R,T,f,f,G,d,b,R,T,d,d,d,v,G,d,v,QR,v\"))]\n   \"\"\n   \"@\n    lzer\\t%0\n+   lder\\t%0,%1\n    ler\\t%0,%1\n+   lde\\t%0,%1\n    le\\t%0,%1\n    ley\\t%0,%1\n    ste\\t%1,%0\n    stey\\t%1,%0\n    lhi\\t%0,0\n    lr\\t%0,%1\n+   lrl\\t%0,%1\n    l\\t%0,%1\n    ly\\t%0,%1\n+   strl\\t%1,%0\n    st\\t%1,%0\n-   sty\\t%1,%0\"\n-  [(set_attr \"op_type\" \"RRE,RR,RX,RXY,RX,RXY,RI,RR,RX,RXY,RX,RXY\")\n-   (set_attr \"type\"    \"fsimpsf,fload<mode>,fload<mode>,fload<mode>,\n-                        fstore<mode>,fstore<mode>,*,lr,load,load,store,store\")\n-   (set_attr \"z10prop\" \"*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_fwd_A3,z10_rec,z10_rec\")\n-   (set_attr \"cpu_facility\" \"z196,*,*,*,*,*,*,*,*,*,*,*\")])\n+   sty\\t%1,%0\n+   vlr\\t%v0,%v1\n+   vleif\\t%v0,0\n+   vlvgf\\t%v0,%1,0\n+   vlgvf\\t%0,%v1,0\n+   vleg\\t%0,%1,0\n+   vsteg\\t%1,%0,0\"\n+  [(set_attr \"op_type\" \"RRE,RRE,RR,RXE,RX,RXY,RX,RXY,RI,RR,RIL,RX,RXY,RIL,RX,RXY,VRR,VRI,VRS,VRS,VRX,VRX\")\n+   (set_attr \"type\"    \"fsimpsf,fsimpsf,fload<mode>,fload<mode>,fload<mode>,fload<mode>,\n+                        fstore<mode>,fstore<mode>,*,lr,load,load,load,store,store,store,*,*,*,*,load,store\")\n+   (set_attr \"z10prop\" \"*,*,*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fr_E1,z10_fwd_A3,z10_fwd_A3,z10_rec,z10_rec,z10_rec,*,*,*,*,*,*\")\n+   (set_attr \"cpu_facility\" \"z196,vec,*,vec,*,*,*,*,*,*,z10,*,*,z10,*,*,vec,vec,vec,vec,vec,vec\")])\n \n ;\n ; movcc instruction pattern\n@@ -2606,6 +2694,22 @@\n ;\n \n (define_expand \"strlen<mode>\"\n+  [(match_operand:P   0 \"register_operand\" \"\")  ; result\n+   (match_operand:BLK 1 \"memory_operand\" \"\")    ; input string\n+   (match_operand:SI  2 \"immediate_operand\" \"\") ; search character\n+   (match_operand:SI  3 \"immediate_operand\" \"\")] ; known alignment\n+  \"\"\n+{\n+  if (!TARGET_VX || operands[2] != const0_rtx)\n+    emit_insn (gen_strlen_srst<mode> (operands[0], operands[1],\n+\t\t\t\t      operands[2], operands[3]));\n+  else\n+    s390_expand_vec_strlen (operands[0], operands[1], operands[3]);\n+\n+  DONE;\n+})\n+\n+(define_expand \"strlen_srst<mode>\"\n   [(set (reg:SI 0) (match_operand:SI 2 \"immediate_operand\" \"\"))\n    (parallel\n     [(set (match_dup 4)\n@@ -2915,8 +3019,12 @@\n   operands[2] = GEN_INT (S390_TDC_INFINITY);\n })\n \n+; This extracts CC into a GPR properly shifted.  The actual IPM\n+; instruction will be issued by reload.  The constraint of operand 1\n+; forces reload to use a GPR.  So reload will issue a movcc insn for\n+; copying CC into a GPR first.\n (define_insn_and_split \"*cc_to_int\"\n-  [(set (match_operand:SI 0 \"register_operand\" \"=d\")\n+  [(set (match_operand:SI 0 \"nonimmediate_operand\"     \"=d\")\n         (unspec:SI [(match_operand 1 \"register_operand\" \"0\")]\n                    UNSPEC_CC_TO_INT))]\n   \"operands != NULL\"\n@@ -4667,10 +4775,29 @@\n ; addti3 instruction pattern(s).\n ;\n \n-(define_insn_and_split \"addti3\"\n-  [(set (match_operand:TI 0 \"register_operand\" \"=&d\")\n+(define_expand \"addti3\"\n+  [(parallel\n+    [(set (match_operand:TI          0 \"register_operand\"     \"\")\n+\t  (plus:TI (match_operand:TI 1 \"nonimmediate_operand\" \"\")\n+\t\t   (match_operand:TI 2 \"general_operand\"      \"\") ) )\n+     (clobber (reg:CC CC_REGNUM))])]\n+  \"TARGET_ZARCH\"\n+{\n+  /* For z13 we have vaq which doesn't set CC.  */\n+  if (TARGET_VX)\n+    {\n+      emit_insn (gen_rtx_SET (operands[0],\n+\t\t\t      gen_rtx_PLUS (TImode,\n+                                            copy_to_mode_reg (TImode, operands[1]),\n+                                            copy_to_mode_reg (TImode, operands[2]))));\n+      DONE;\n+    }\n+})\n+\n+(define_insn_and_split \"*addti3\"\n+  [(set (match_operand:TI          0 \"register_operand\"    \"=&d\")\n         (plus:TI (match_operand:TI 1 \"nonimmediate_operand\" \"%0\")\n-                 (match_operand:TI 2 \"general_operand\" \"do\") ) )\n+                 (match_operand:TI 2 \"general_operand\"      \"do\") ) )\n    (clobber (reg:CC CC_REGNUM))]\n   \"TARGET_ZARCH\"\n   \"#\"\n@@ -4690,7 +4817,9 @@\n    operands[5] = operand_subword (operands[2], 0, 0, TImode);\n    operands[6] = operand_subword (operands[0], 1, 0, TImode);\n    operands[7] = operand_subword (operands[1], 1, 0, TImode);\n-   operands[8] = operand_subword (operands[2], 1, 0, TImode);\")\n+   operands[8] = operand_subword (operands[2], 1, 0, TImode);\"\n+  [(set_attr \"op_type\"  \"*\")\n+   (set_attr \"cpu_facility\" \"*\")])\n \n ;\n ; adddi3 instruction pattern(s).\n@@ -5128,10 +5257,29 @@\n ; subti3 instruction pattern(s).\n ;\n \n-(define_insn_and_split \"subti3\"\n-  [(set (match_operand:TI 0 \"register_operand\" \"=&d\")\n-        (minus:TI (match_operand:TI 1 \"register_operand\" \"0\")\n-                  (match_operand:TI 2 \"general_operand\" \"do\") ) )\n+(define_expand \"subti3\"\n+  [(parallel\n+    [(set (match_operand:TI           0 \"register_operand\" \"\")\n+\t  (minus:TI (match_operand:TI 1 \"register_operand\" \"\")\n+\t\t    (match_operand:TI 2 \"general_operand\"  \"\") ) )\n+     (clobber (reg:CC CC_REGNUM))])]\n+  \"TARGET_ZARCH\"\n+{\n+  /* For z13 we have vaq which doesn't set CC.  */\n+  if (TARGET_VX)\n+    {\n+      emit_insn (gen_rtx_SET (operands[0],\n+\t\t\t      gen_rtx_MINUS (TImode,\n+                                            operands[1],\n+                                            copy_to_mode_reg (TImode, operands[2]))));\n+      DONE;\n+    }\n+})\n+\n+(define_insn_and_split \"*subti3\"\n+  [(set (match_operand:TI           0 \"register_operand\" \"=&d\")\n+        (minus:TI (match_operand:TI 1 \"register_operand\"   \"0\")\n+                  (match_operand:TI 2 \"general_operand\"   \"do\") ) )\n    (clobber (reg:CC CC_REGNUM))]\n   \"TARGET_ZARCH\"\n   \"#\"\n@@ -5150,7 +5298,9 @@\n    operands[5] = operand_subword (operands[2], 0, 0, TImode);\n    operands[6] = operand_subword (operands[0], 1, 0, TImode);\n    operands[7] = operand_subword (operands[1], 1, 0, TImode);\n-   operands[8] = operand_subword (operands[2], 1, 0, TImode);\")\n+   operands[8] = operand_subword (operands[2], 1, 0, TImode);\"\n+  [(set_attr \"op_type\"      \"*\")\n+   (set_attr \"cpu_facility\" \"*\")])\n \n ;\n ; subdi3 instruction pattern(s)."}, {"sha": "f07f5a70f18c5dd8ea63ab5dd88d67ec48cd23f3", "filename": "gcc/config/s390/vector.md", "status": "added", "additions": 1226, "deletions": 0, "changes": 1226, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fvector.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/085261c8042d644baaf594bc436b87326c1c0390/gcc%2Fconfig%2Fs390%2Fvector.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fvector.md?ref=085261c8042d644baaf594bc436b87326c1c0390", "patch": "@@ -0,0 +1,1226 @@\n+;;- Instruction patterns for the System z vector facility\n+;;  Copyright (C) 2015 Free Software Foundation, Inc.\n+;;  Contributed by Andreas Krebbel (Andreas.Krebbel@de.ibm.com)\n+\n+;; This file is part of GCC.\n+\n+;; GCC is free software; you can redistribute it and/or modify it under\n+;; the terms of the GNU General Public License as published by the Free\n+;; Software Foundation; either version 3, or (at your option) any later\n+;; version.\n+\n+;; GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+;; WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+;; FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+;; for more details.\n+\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+; All vector modes supported in a vector register\n+(define_mode_iterator V\n+  [V1QI V2QI V4QI V8QI V16QI V1HI V2HI V4HI V8HI V1SI V2SI V4SI V1DI V2DI V1SF\n+   V2SF V4SF V1DF V2DF])\n+(define_mode_iterator VT\n+  [V1QI V2QI V4QI V8QI V16QI V1HI V2HI V4HI V8HI V1SI V2SI V4SI V1DI V2DI V1SF\n+   V2SF V4SF V1DF V2DF V1TF V1TI TI])\n+\n+; All vector modes directly supported by the hardware having full vector reg size\n+; V_HW2 is duplicate of V_HW for having two iterators expanding\n+; independently e.g. vcond\n+(define_mode_iterator V_HW  [V16QI V8HI V4SI V2DI V2DF])\n+(define_mode_iterator V_HW2 [V16QI V8HI V4SI V2DI V2DF])\n+; Including TI for instructions that support it (va, vn, ...)\n+(define_mode_iterator VT_HW [V16QI V8HI V4SI V2DI V2DF V1TI TI])\n+\n+; All full size integer vector modes supported in a vector register + TImode\n+(define_mode_iterator VIT_HW    [V16QI V8HI V4SI V2DI V1TI TI])\n+(define_mode_iterator VI_HW     [V16QI V8HI V4SI V2DI])\n+(define_mode_iterator VI_HW_QHS [V16QI V8HI V4SI])\n+(define_mode_iterator VI_HW_HS  [V8HI V4SI])\n+(define_mode_iterator VI_HW_QH  [V16QI V8HI])\n+\n+; All integer vector modes supported in a vector register + TImode\n+(define_mode_iterator VIT [V1QI V2QI V4QI V8QI V16QI V1HI V2HI V4HI V8HI V1SI V2SI V4SI V1DI V2DI V1TI TI])\n+(define_mode_iterator VI  [V2QI V4QI V8QI V16QI V2HI V4HI V8HI V2SI V4SI V2DI])\n+(define_mode_iterator VI_QHS [V4QI V8QI V16QI V4HI V8HI V4SI])\n+\n+(define_mode_iterator V_8   [V1QI])\n+(define_mode_iterator V_16  [V2QI  V1HI])\n+(define_mode_iterator V_32  [V4QI  V2HI V1SI V1SF])\n+(define_mode_iterator V_64  [V8QI  V4HI V2SI V2SF V1DI V1DF])\n+(define_mode_iterator V_128 [V16QI V8HI V4SI V4SF V2DI V2DF V1TI V1TF])\n+\n+; A blank for vector modes and a * for TImode.  This is used to hide\n+; the TImode expander name in case it is defined already.  See addti3\n+; for an example.\n+(define_mode_attr ti* [(V1QI \"\") (V2QI \"\") (V4QI \"\") (V8QI \"\") (V16QI \"\")\n+\t\t       (V1HI \"\") (V2HI \"\") (V4HI \"\") (V8HI \"\")\n+\t\t       (V1SI \"\") (V2SI \"\") (V4SI \"\")\n+\t\t       (V1DI \"\") (V2DI \"\")\n+\t\t       (V1TI \"*\") (TI \"*\")])\n+\n+; The element type of the vector.\n+(define_mode_attr non_vec[(V1QI \"QI\") (V2QI \"QI\") (V4QI \"QI\") (V8QI \"QI\") (V16QI \"QI\")\n+\t\t\t  (V1HI \"HI\") (V2HI \"HI\") (V4HI \"HI\") (V8HI \"HI\")\n+\t\t\t  (V1SI \"SI\") (V2SI \"SI\") (V4SI \"SI\")\n+\t\t\t  (V1DI \"DI\") (V2DI \"DI\")\n+\t\t\t  (V1TI \"TI\")\n+\t\t\t  (V1SF \"SF\") (V2SF \"SF\") (V4SF \"SF\")\n+\t\t\t  (V1DF \"DF\") (V2DF \"DF\")\n+\t\t\t  (V1TF \"TF\")])\n+\n+; The instruction suffix\n+(define_mode_attr bhfgq[(V1QI \"b\") (V2QI \"b\") (V4QI \"b\") (V8QI \"b\") (V16QI \"b\")\n+\t\t\t(V1HI \"h\") (V2HI \"h\") (V4HI \"h\") (V8HI \"h\")\n+\t\t\t(V1SI \"f\") (V2SI \"f\") (V4SI \"f\")\n+\t\t\t(V1DI \"g\") (V2DI \"g\")\n+\t\t\t(V1TI \"q\") (TI \"q\")\n+\t\t\t(V1SF \"f\") (V2SF \"f\") (V4SF \"f\")\n+\t\t\t(V1DF \"g\") (V2DF \"g\")\n+\t\t\t(V1TF \"q\")])\n+\n+; This is for vmalhw. It gets an 'w' attached to avoid confusion with\n+; multiply and add logical high vmalh.\n+(define_mode_attr w [(V1QI \"\")  (V2QI \"\")  (V4QI \"\")  (V8QI \"\") (V16QI \"\")\n+\t\t     (V1HI \"w\") (V2HI \"w\") (V4HI \"w\") (V8HI \"w\")\n+\t\t     (V1SI \"\")  (V2SI \"\")  (V4SI \"\")\n+\t\t     (V1DI \"\")  (V2DI \"\")])\n+\n+; Resulting mode of a vector comparison.  For floating point modes an\n+; integer vector mode with the same element size is picked.\n+(define_mode_attr tointvec [(V1QI \"V1QI\") (V2QI \"V2QI\") (V4QI \"V4QI\") (V8QI \"V8QI\") (V16QI \"V16QI\")\n+\t\t\t    (V1HI \"V1HI\") (V2HI \"V2HI\") (V4HI \"V4HI\") (V8HI \"V8HI\")\n+\t\t\t    (V1SI \"V1SI\") (V2SI \"V2SI\") (V4SI \"V4SI\")\n+\t\t\t    (V1DI \"V1DI\") (V2DI \"V2DI\")\n+\t\t\t    (V1TI \"V1TI\")\n+\t\t\t    (V1SF \"V1SI\") (V2SF \"V2SI\") (V4SF \"V4SI\")\n+\t\t\t    (V1DF \"V1DI\") (V2DF \"V2DI\")\n+\t\t\t    (V1TF \"V1TI\")])\n+\n+; Vector with doubled element size.\n+(define_mode_attr vec_double [(V2QI \"V1HI\") (V4QI \"V2HI\") (V8QI \"V4HI\") (V16QI \"V8HI\")\n+\t\t\t      (V2HI \"V1SI\") (V4HI \"V2SI\") (V8HI \"V4SI\")\n+\t\t\t      (V2SI \"V1DI\") (V4SI \"V2DI\")\n+\t\t\t      (V2DI \"V1TI\")\n+\t\t\t      (V2SF \"V1DF\") (V4SF \"V2DF\")])\n+\n+; Vector with half the element size.\n+(define_mode_attr vec_half [(V1HI \"V2QI\") (V2HI \"V4QI\") (V4HI \"V8QI\") (V8HI \"V16QI\")\n+\t\t\t    (V1SI \"V2HI\") (V2SI \"V4HI\") (V4SI \"V8HI\")\n+\t\t\t    (V1DI \"V2SI\") (V2DI \"V4SI\")\n+\t\t\t    (V1TI \"V2DI\")\n+\t\t\t    (V1DF \"V2SF\") (V2DF \"V4SF\")\n+\t\t\t    (V1TF \"V1DF\")])\n+\n+; The comparisons not setting CC iterate over the rtx code.\n+(define_code_iterator VFCMP_HW_OP [eq gt ge])\n+(define_code_attr asm_fcmp_op [(eq \"e\") (gt \"h\") (ge \"he\")])\n+\n+\n+\n+; Comparison operators on int and fp compares which are directly\n+; supported by the HW.\n+(define_code_iterator VICMP_HW_OP [eq gt gtu])\n+; For int insn_cmp_op can be used in the insn name as well as in the asm output.\n+(define_code_attr insn_cmp_op [(eq \"eq\") (gt \"h\") (gtu \"hl\") (ge \"he\")])\n+\n+; Flags for vector string instructions (vfae all 4, vfee only ZS and CS, vstrc all 4)\n+(define_constants\n+  [(VSTRING_FLAG_IN         8)   ; invert result\n+   (VSTRING_FLAG_RT         4)   ; result type\n+   (VSTRING_FLAG_ZS         2)   ; zero search\n+   (VSTRING_FLAG_CS         1)]) ; condition code set\n+\n+; Full HW vector size moves\n+(define_insn \"mov<mode>\"\n+  [(set (match_operand:V_128 0 \"nonimmediate_operand\" \"=v, v,QR,  v,  v,  v,  v,v,d\")\n+\t(match_operand:V_128 1 \"general_operand\"      \" v,QR, v,j00,jm1,jyy,jxx,d,v\"))]\n+  \"TARGET_VX\"\n+  \"@\n+   vlr\\t%v0,%v1\n+   vl\\t%v0,%1\n+   vst\\t%v1,%0\n+   vzero\\t%v0\n+   vone\\t%v0\n+   vgbm\\t%v0,%t1\n+   vgm<bhfgq>\\t%v0,%s1,%e1\n+   vlvgp\\t%v0,%1,%N1\n+   #\"\n+  [(set_attr \"op_type\" \"VRR,VRX,VRX,VRI,VRI,VRI,VRI,VRR,*\")])\n+\n+(define_split\n+  [(set (match_operand:V_128 0 \"register_operand\" \"\")\n+\t(match_operand:V_128 1 \"register_operand\" \"\"))]\n+  \"TARGET_VX && GENERAL_REG_P (operands[0]) && VECTOR_REG_P (operands[1])\"\n+  [(set (match_dup 2)\n+\t(unspec:DI [(subreg:V2DI (match_dup 1) 0)\n+\t\t    (const_int 0)] UNSPEC_VEC_EXTRACT))\n+   (set (match_dup 3)\n+\t(unspec:DI [(subreg:V2DI (match_dup 1) 0)\n+\t\t    (const_int 1)] UNSPEC_VEC_EXTRACT))]\n+{\n+  operands[2] = operand_subword (operands[0], 0, 0, <MODE>mode);\n+  operands[3] = operand_subword (operands[0], 1, 0, <MODE>mode);\n+})\n+\n+; Moves for smaller vector modes.\n+\n+; In these patterns only the vlr, vone, and vzero instructions write\n+; VR bytes outside the mode.  This should be ok since we disallow\n+; formerly bigger modes being accessed with smaller modes via\n+; subreg. Note: The vone, vzero instructions could easily be replaced\n+; with vlei which would only access the bytes belonging to the mode.\n+; However, this would probably be slower.\n+\n+(define_insn \"mov<mode>\"\n+  [(set (match_operand:V_8 0 \"nonimmediate_operand\" \"=v,v,d, v,QR,  v,  v,  v,  v,d,  Q,  S,  Q,  S,  d,  d,d,d,d,R,T\")\n+        (match_operand:V_8 1 \"general_operand\"      \" v,d,v,QR, v,j00,jm1,jyy,jxx,d,j00,j00,jm1,jm1,j00,jm1,R,T,b,d,d\"))]\n+  \"\"\n+  \"@\n+   vlr\\t%v0,%v1\n+   vlvgb\\t%v0,%1,0\n+   vlgvb\\t%0,%v1,0\n+   vleb\\t%v0,%1,0\n+   vsteb\\t%v1,%0,0\n+   vzero\\t%v0\n+   vone\\t%v0\n+   vgbm\\t%v0,%t1\n+   vgm\\t%v0,%s1,%e1\n+   lr\\t%0,%1\n+   mvi\\t%0,0\n+   mviy\\t%0,0\n+   mvi\\t%0,-1\n+   mviy\\t%0,-1\n+   lhi\\t%0,0\n+   lhi\\t%0,-1\n+   lh\\t%0,%1\n+   lhy\\t%0,%1\n+   lhrl\\t%0,%1\n+   stc\\t%1,%0\n+   stcy\\t%1,%0\"\n+  [(set_attr \"op_type\"      \"VRR,VRS,VRS,VRX,VRX,VRI,VRI,VRI,VRI,RR,SI,SIY,SI,SIY,RI,RI,RX,RXY,RIL,RX,RXY\")])\n+\n+(define_insn \"mov<mode>\"\n+  [(set (match_operand:V_16 0 \"nonimmediate_operand\" \"=v,v,d, v,QR,  v,  v,  v,  v,d,  Q,  Q,  d,  d,d,d,d,R,T,b\")\n+        (match_operand:V_16 1 \"general_operand\"      \" v,d,v,QR, v,j00,jm1,jyy,jxx,d,j00,jm1,j00,jm1,R,T,b,d,d,d\"))]\n+  \"\"\n+  \"@\n+   vlr\\t%v0,%v1\n+   vlvgh\\t%v0,%1,0\n+   vlgvh\\t%0,%v1,0\n+   vleh\\t%v0,%1,0\n+   vsteh\\t%v1,%0,0\n+   vzero\\t%v0\n+   vone\\t%v0\n+   vgbm\\t%v0,%t1\n+   vgm\\t%v0,%s1,%e1\n+   lr\\t%0,%1\n+   mvhhi\\t%0,0\n+   mvhhi\\t%0,-1\n+   lhi\\t%0,0\n+   lhi\\t%0,-1\n+   lh\\t%0,%1\n+   lhy\\t%0,%1\n+   lhrl\\t%0,%1\n+   sth\\t%1,%0\n+   sthy\\t%1,%0\n+   sthrl\\t%1,%0\"\n+  [(set_attr \"op_type\"      \"VRR,VRS,VRS,VRX,VRX,VRI,VRI,VRI,VRI,RR,SIL,SIL,RI,RI,RX,RXY,RIL,RX,RXY,RIL\")])\n+\n+(define_insn \"mov<mode>\"\n+  [(set (match_operand:V_32 0 \"nonimmediate_operand\" \"=f,f,f,R,T,v,v,d, v,QR,  f,  v,  v,  v,  v,  Q,  Q,  d,  d,d,d,d,d,R,T,b\")\n+\t(match_operand:V_32 1 \"general_operand\"      \" f,R,T,f,f,v,d,v,QR, v,j00,j00,jm1,jyy,jxx,j00,jm1,j00,jm1,b,d,R,T,d,d,d\"))]\n+  \"TARGET_VX\"\n+  \"@\n+   lder\\t%v0,%v1\n+   lde\\t%0,%1\n+   ley\\t%0,%1\n+   ste\\t%1,%0\n+   stey\\t%1,%0\n+   vlr\\t%v0,%v1\n+   vlvgf\\t%v0,%1,0\n+   vlgvf\\t%0,%v1,0\n+   vlef\\t%v0,%1,0\n+   vstef\\t%1,%0,0\n+   lzer\\t%v0\n+   vzero\\t%v0\n+   vone\\t%v0\n+   vgbm\\t%v0,%t1\n+   vgm\\t%v0,%s1,%e1\n+   mvhi\\t%0,0\n+   mvhi\\t%0,-1\n+   lhi\\t%0,0\n+   lhi\\t%0,-1\n+   lrl\\t%0,%1\n+   lr\\t%0,%1\n+   l\\t%0,%1\n+   ly\\t%0,%1\n+   st\\t%1,%0\n+   sty\\t%1,%0\n+   strl\\t%1,%0\"\n+  [(set_attr \"op_type\" \"RRE,RXE,RXY,RX,RXY,VRR,VRS,VRS,VRX,VRX,RRE,VRI,VRI,VRI,VRI,SIL,SIL,RI,RI,\n+                        RIL,RR,RX,RXY,RX,RXY,RIL\")])\n+\n+(define_insn \"mov<mode>\"\n+  [(set (match_operand:V_64 0 \"nonimmediate_operand\"\n+         \"=f,f,f,R,T,v,v,d, v,QR,  f,  v,  v,  v,  v,  Q,  Q,  d,  d,f,d,d,d, d,RT,b\")\n+        (match_operand:V_64 1 \"general_operand\"\n+         \" f,R,T,f,f,v,d,v,QR, v,j00,j00,jm1,jyy,jxx,j00,jm1,j00,jm1,d,f,b,d,RT, d,d\"))]\n+  \"TARGET_ZARCH\"\n+  \"@\n+   ldr\\t%0,%1\n+   ld\\t%0,%1\n+   ldy\\t%0,%1\n+   std\\t%1,%0\n+   stdy\\t%1,%0\n+   vlr\\t%v0,%v1\n+   vlvgg\\t%v0,%1,0\n+   vlgvg\\t%0,%v1,0\n+   vleg\\t%v0,%1,0\n+   vsteg\\t%v1,%0,0\n+   lzdr\\t%0\n+   vzero\\t%v0\n+   vone\\t%v0\n+   vgbm\\t%v0,%t1\n+   vgm\\t%v0,%s1,%e1\n+   mvghi\\t%0,0\n+   mvghi\\t%0,-1\n+   lghi\\t%0,0\n+   lghi\\t%0,-1\n+   ldgr\\t%0,%1\n+   lgdr\\t%0,%1\n+   lgrl\\t%0,%1\n+   lgr\\t%0,%1\n+   lg\\t%0,%1\n+   stg\\t%1,%0\n+   stgrl\\t%1,%0\"\n+  [(set_attr \"op_type\" \"RRE,RX,RXY,RX,RXY,VRR,VRS,VRS,VRX,VRX,RRE,VRI,VRI,VRI,VRI,\n+                        SIL,SIL,RI,RI,RRE,RRE,RIL,RR,RXY,RXY,RIL\")])\n+\n+\n+; vec_load_lanes?\n+\n+; vec_store_lanes?\n+\n+; FIXME: Support also vector mode operands for 1\n+; FIXME: A target memory operand seems to be useful otherwise we end\n+; up with vl vlvgg vst.  Shouldn't the middle-end be able to handle\n+; that itself?\n+(define_insn \"*vec_set<mode>\"\n+  [(set (match_operand:V                    0 \"register_operand\"             \"=v, v,v\")\n+\t(unspec:V [(match_operand:<non_vec> 1 \"general_operand\"               \"d,QR,K\")\n+\t\t   (match_operand:DI        2 \"shift_count_or_setmem_operand\" \"Y, I,I\")\n+\t\t   (match_operand:V         3 \"register_operand\"              \"0, 0,0\")]\n+\t\t  UNSPEC_VEC_SET))]\n+  \"TARGET_VX\"\n+  \"@\n+   vlvg<bhfgq>\\t%v0,%1,%Y2\n+   vle<bhfgq>\\t%v0,%1,%2\n+   vlei<bhfgq>\\t%v0,%1,%2\"\n+  [(set_attr \"op_type\" \"VRS,VRX,VRI\")])\n+\n+; vec_set is supposed to *modify* an existing vector so operand 0 is\n+; duplicated as input operand.\n+(define_expand \"vec_set<mode>\"\n+  [(set (match_operand:V                    0 \"register_operand\"              \"\")\n+\t(unspec:V [(match_operand:<non_vec> 1 \"general_operand\"               \"\")\n+\t\t   (match_operand:SI        2 \"shift_count_or_setmem_operand\" \"\")\n+\t\t   (match_dup 0)]\n+\t\t   UNSPEC_VEC_SET))]\n+  \"TARGET_VX\")\n+\n+; FIXME: Support also vector mode operands for 0\n+; FIXME: This should be (vec_select ..) or something but it does only allow constant selectors :(\n+; This is used via RTL standard name as well as for expanding the builtin\n+(define_insn \"vec_extract<mode>\"\n+  [(set (match_operand:<non_vec> 0 \"nonimmediate_operand\"                        \"=d,QR\")\n+\t(unspec:<non_vec> [(match_operand:V  1 \"register_operand\"                \" v, v\")\n+\t\t\t   (match_operand:SI 2 \"shift_count_or_setmem_operand\"   \" Y, I\")]\n+\t\t\t  UNSPEC_VEC_EXTRACT))]\n+  \"TARGET_VX\"\n+  \"@\n+   vlgv<bhfgq>\\t%0,%v1,%Y2\n+   vste<bhfgq>\\t%v1,%0,%2\"\n+  [(set_attr \"op_type\" \"VRS,VRX\")])\n+\n+(define_expand \"vec_init<V_HW:mode>\"\n+  [(match_operand:V_HW 0 \"register_operand\" \"\")\n+   (match_operand:V_HW 1 \"nonmemory_operand\" \"\")]\n+  \"TARGET_VX\"\n+{\n+  s390_expand_vec_init (operands[0], operands[1]);\n+  DONE;\n+})\n+\n+; Replicate from vector element\n+(define_insn \"*vec_splat<mode>\"\n+  [(set (match_operand:V_HW   0 \"register_operand\" \"=v\")\n+\t(vec_duplicate:V_HW\n+\t (vec_select:<non_vec>\n+\t  (match_operand:V_HW 1 \"register_operand\"  \"v\")\n+\t  (parallel\n+\t   [(match_operand:QI 2 \"immediate_operand\" \"C\")]))))]\n+  \"TARGET_VX\"\n+  \"vrep<bhfgq>\\t%v0,%v1,%2\"\n+  [(set_attr \"op_type\" \"VRI\")])\n+\n+(define_insn \"*vec_splats<mode>\"\n+  [(set (match_operand:V_HW                          0 \"register_operand\" \"=v,v,v,v\")\n+\t(vec_duplicate:V_HW (match_operand:<non_vec> 1 \"general_operand\"  \"QR,I,v,d\")))]\n+  \"TARGET_VX\"\n+  \"@\n+   vlrep<bhfgq>\\t%v0,%1\n+   vrepi<bhfgq>\\t%v0,%1\n+   vrep<bhfgq>\\t%v0,%v1,0\n+   #\"\n+  [(set_attr \"op_type\" \"VRX,VRI,VRI,*\")])\n+\n+; vec_splats is supposed to replicate op1 into all elements of op0\n+; This splitter first sets the rightmost element of op0 to op1 and\n+; then does a vec_splat to replicate that element into all other\n+; elements.\n+(define_split\n+  [(set (match_operand:V_HW                          0 \"register_operand\" \"\")\n+\t(vec_duplicate:V_HW (match_operand:<non_vec> 1 \"register_operand\" \"\")))]\n+  \"TARGET_VX && GENERAL_REG_P (operands[1])\"\n+  [(set (match_dup 0)\n+\t(unspec:V_HW [(match_dup 1) (match_dup 2) (match_dup 0)] UNSPEC_VEC_SET))\n+   (set (match_dup 0)\n+\t(vec_duplicate:V_HW\n+\t (vec_select:<non_vec>\n+\t  (match_dup 0) (parallel [(match_dup 2)]))))]\n+{\n+  operands[2] = GEN_INT (GET_MODE_NUNITS (<MODE>mode) - 1);\n+})\n+\n+(define_expand \"vcond<V_HW:mode><V_HW2:mode>\"\n+  [(set (match_operand:V_HW 0 \"register_operand\" \"\")\n+\t(if_then_else:V_HW\n+\t (match_operator 3 \"comparison_operator\"\n+\t\t\t [(match_operand:V_HW2 4 \"register_operand\" \"\")\n+\t\t\t  (match_operand:V_HW2 5 \"register_operand\" \"\")])\n+\t (match_operand:V_HW 1 \"nonmemory_operand\" \"\")\n+\t (match_operand:V_HW 2 \"nonmemory_operand\" \"\")))]\n+  \"TARGET_VX && GET_MODE_NUNITS (<V_HW:MODE>mode) == GET_MODE_NUNITS (<V_HW2:MODE>mode)\"\n+{\n+  s390_expand_vcond (operands[0], operands[1], operands[2],\n+\t\t     GET_CODE (operands[3]), operands[4], operands[5]);\n+  DONE;\n+})\n+\n+(define_expand \"vcondu<V_HW:mode><V_HW2:mode>\"\n+  [(set (match_operand:V_HW 0 \"register_operand\" \"\")\n+\t(if_then_else:V_HW\n+\t (match_operator 3 \"comparison_operator\"\n+\t\t\t [(match_operand:V_HW2 4 \"register_operand\" \"\")\n+\t\t\t  (match_operand:V_HW2 5 \"register_operand\" \"\")])\n+\t (match_operand:V_HW 1 \"nonmemory_operand\" \"\")\n+\t (match_operand:V_HW 2 \"nonmemory_operand\" \"\")))]\n+  \"TARGET_VX && GET_MODE_NUNITS (<V_HW:MODE>mode) == GET_MODE_NUNITS (<V_HW2:MODE>mode)\"\n+{\n+  s390_expand_vcond (operands[0], operands[1], operands[2],\n+\t\t     GET_CODE (operands[3]), operands[4], operands[5]);\n+  DONE;\n+})\n+\n+; We only have HW support for byte vectors.  The middle-end is\n+; supposed to lower the mode if required.\n+(define_insn \"vec_permv16qi\"\n+  [(set (match_operand:V16QI 0 \"register_operand\"               \"=v\")\n+\t(unspec:V16QI [(match_operand:V16QI 1 \"register_operand\" \"v\")\n+\t\t       (match_operand:V16QI 2 \"register_operand\" \"v\")\n+\t\t       (match_operand:V16QI 3 \"register_operand\" \"v\")]\n+\t\t      UNSPEC_VEC_PERM))]\n+  \"TARGET_VX\"\n+  \"vperm\\t%v0,%v1,%v2,%v3\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vec_perm_const for V2DI using vpdi?\n+\n+;;\n+;; Vector integer arithmetic instructions\n+;;\n+\n+; vab, vah, vaf, vag, vaq\n+\n+; We use nonimmediate_operand instead of register_operand since it is\n+; better to have the reloads into VRs instead of splitting the\n+; operation into two DImode ADDs.\n+(define_insn \"<ti*>add<mode>3\"\n+  [(set (match_operand:VIT           0 \"nonimmediate_operand\" \"=v\")\n+\t(plus:VIT (match_operand:VIT 1 \"nonimmediate_operand\"  \"v\")\n+\t\t  (match_operand:VIT 2 \"nonimmediate_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"va<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vsb, vsh, vsf, vsg, vsq\n+(define_insn \"<ti*>sub<mode>3\"\n+  [(set (match_operand:VIT            0 \"nonimmediate_operand\" \"=v\")\n+\t(minus:VIT (match_operand:VIT 1 \"nonimmediate_operand\"  \"v\")\n+\t\t   (match_operand:VIT 2 \"nonimmediate_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vs<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vmlb, vmlhw, vmlf\n+(define_insn \"mul<mode>3\"\n+  [(set (match_operand:VI_QHS              0 \"register_operand\" \"=v\")\n+\t(mult:VI_QHS (match_operand:VI_QHS 1 \"register_operand\"  \"v\")\n+\t\t     (match_operand:VI_QHS 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vml<bhfgq><w>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vlcb, vlch, vlcf, vlcg\n+(define_insn \"neg<mode>2\"\n+  [(set (match_operand:VI         0 \"register_operand\" \"=v\")\n+\t(neg:VI (match_operand:VI 1 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vlc<bhfgq>\\t%v0,%v1\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vlpb, vlph, vlpf, vlpg\n+(define_insn \"abs<mode>2\"\n+  [(set (match_operand:VI         0 \"register_operand\" \"=v\")\n+\t(abs:VI (match_operand:VI 1 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vlp<bhfgq>\\t%v0,%v1\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+\n+; Vector sum across\n+\n+; Sum across DImode parts of the 1st operand and add the rightmost\n+; element of 2nd operand\n+; vsumgh, vsumgf\n+(define_insn \"*vec_sum2<mode>\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=v\")\n+\t(unspec:V2DI [(match_operand:VI_HW_HS 1 \"register_operand\" \"v\")\n+\t\t      (match_operand:VI_HW_HS 2 \"register_operand\" \"v\")]\n+\t\t     UNSPEC_VEC_VSUMG))]\n+  \"TARGET_VX\"\n+  \"vsumg<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vsumb, vsumh\n+(define_insn \"*vec_sum4<mode>\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=v\")\n+\t(unspec:V4SI [(match_operand:VI_HW_QH 1 \"register_operand\" \"v\")\n+\t\t      (match_operand:VI_HW_QH 2 \"register_operand\" \"v\")]\n+\t\t     UNSPEC_VEC_VSUM))]\n+  \"TARGET_VX\"\n+  \"vsum<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+;;\n+;; Vector bit instructions (int + fp)\n+;;\n+\n+; Vector and\n+\n+(define_insn \"and<mode>3\"\n+  [(set (match_operand:VT         0 \"register_operand\" \"=v\")\n+\t(and:VT (match_operand:VT 1 \"register_operand\"  \"v\")\n+\t\t(match_operand:VT 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vn\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+\n+; Vector or\n+\n+(define_insn \"ior<mode>3\"\n+  [(set (match_operand:VT         0 \"register_operand\" \"=v\")\n+\t(ior:VT (match_operand:VT 1 \"register_operand\"  \"v\")\n+\t\t(match_operand:VT 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vo\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+\n+; Vector xor\n+\n+(define_insn \"xor<mode>3\"\n+  [(set (match_operand:VT         0 \"register_operand\" \"=v\")\n+\t(xor:VT (match_operand:VT 1 \"register_operand\"  \"v\")\n+\t\t(match_operand:VT 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vx\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+\n+; Bitwise inversion of a vector - used for vec_cmpne\n+(define_insn \"*not<mode>\"\n+  [(set (match_operand:VT         0 \"register_operand\" \"=v\")\n+\t(not:VT (match_operand:VT 1 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vnot\\t%v0,%v1\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; Vector population count\n+\n+(define_insn \"popcountv16qi2\"\n+  [(set (match_operand:V16QI                0 \"register_operand\" \"=v\")\n+\t(unspec:V16QI [(match_operand:V16QI 1 \"register_operand\"  \"v\")]\n+\t\t      UNSPEC_POPCNT))]\n+  \"TARGET_VX\"\n+  \"vpopct\\t%v0,%v1,0\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vpopct only counts bits in byte elements.  Bigger element sizes need\n+; to be emulated.  Word and doubleword elements can use the sum across\n+; instructions.  For halfword sized elements we do a shift of a copy\n+; of the result, add it to the result and extend it to halfword\n+; element size (unpack).\n+\n+(define_expand \"popcountv8hi2\"\n+  [(set (match_dup 2)\n+\t(unspec:V16QI [(subreg:V16QI (match_operand:V8HI 1 \"register_operand\" \"v\") 0)]\n+\t\t      UNSPEC_POPCNT))\n+   ; Make a copy of the result\n+   (set (match_dup 3) (match_dup 2))\n+   ; Generate the shift count operand in a VR (8->byte 7)\n+   (set (match_dup 4) (match_dup 5))\n+   (set (match_dup 4) (unspec:V16QI [(const_int 8)\n+\t\t\t\t     (const_int 7)\n+\t\t\t\t     (match_dup 4)] UNSPEC_VEC_SET))\n+   ; Vector shift right logical by one byte\n+   (set (match_dup 3)\n+\t(unspec:V16QI [(match_dup 3) (match_dup 4)] UNSPEC_VEC_SRLB))\n+   ; Add the shifted and the original result\n+   (set (match_dup 2)\n+\t(plus:V16QI (match_dup 2) (match_dup 3)))\n+   ; Generate mask for the odd numbered byte elements\n+   (set (match_dup 3)\n+\t(const_vector:V16QI [(const_int 0) (const_int 255)\n+\t\t\t     (const_int 0) (const_int 255)\n+\t\t\t     (const_int 0) (const_int 255)\n+\t\t\t     (const_int 0) (const_int 255)\n+\t\t\t     (const_int 0) (const_int 255)\n+\t\t\t     (const_int 0) (const_int 255)\n+\t\t\t     (const_int 0) (const_int 255)\n+\t\t\t     (const_int 0) (const_int 255)]))\n+   ; Zero out the even indexed bytes\n+   (set (match_operand:V8HI 0 \"register_operand\" \"=v\")\n+\t(and:V8HI (subreg:V8HI (match_dup 2) 0)\n+\t\t  (subreg:V8HI (match_dup 3) 0)))\n+]\n+  \"TARGET_VX\"\n+{\n+  operands[2] = gen_reg_rtx (V16QImode);\n+  operands[3] = gen_reg_rtx (V16QImode);\n+  operands[4] = gen_reg_rtx (V16QImode);\n+  operands[5] = CONST0_RTX (V16QImode);\n+})\n+\n+(define_expand \"popcountv4si2\"\n+  [(set (match_dup 2)\n+\t(unspec:V16QI [(subreg:V16QI (match_operand:V4SI 1 \"register_operand\" \"v\") 0)]\n+\t\t      UNSPEC_POPCNT))\n+   (set (match_operand:V4SI 0 \"register_operand\" \"=v\")\n+\t(unspec:V4SI [(match_dup 2) (match_dup 3)]\n+\t\t     UNSPEC_VEC_VSUM))]\n+  \"TARGET_VX\"\n+{\n+  operands[2] = gen_reg_rtx (V16QImode);\n+  operands[3] = force_reg (V16QImode, CONST0_RTX (V16QImode));\n+})\n+\n+(define_expand \"popcountv2di2\"\n+  [(set (match_dup 2)\n+\t(unspec:V16QI [(subreg:V16QI (match_operand:V2DI 1 \"register_operand\" \"v\") 0)]\n+\t\t      UNSPEC_POPCNT))\n+   (set (match_dup 3)\n+\t(unspec:V4SI [(match_dup 2) (match_dup 4)]\n+\t\t     UNSPEC_VEC_VSUM))\n+   (set (match_operand:V2DI 0 \"register_operand\" \"=v\")\n+\t(unspec:V2DI [(match_dup 3) (match_dup 5)]\n+\t\t     UNSPEC_VEC_VSUMG))]\n+  \"TARGET_VX\"\n+{\n+  operands[2] = gen_reg_rtx (V16QImode);\n+  operands[3] = gen_reg_rtx (V4SImode);\n+  operands[4] = force_reg (V16QImode, CONST0_RTX (V16QImode));\n+  operands[5] = force_reg (V4SImode, CONST0_RTX (V4SImode));\n+})\n+\n+; Count leading zeros\n+(define_insn \"clz<mode>2\"\n+  [(set (match_operand:V        0 \"register_operand\" \"=v\")\n+\t(clz:V (match_operand:V 1 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vclz<bhfgq>\\t%v0,%v1\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; Count trailing zeros\n+(define_insn \"ctz<mode>2\"\n+  [(set (match_operand:V        0 \"register_operand\" \"=v\")\n+\t(ctz:V (match_operand:V 1 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vctz<bhfgq>\\t%v0,%v1\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+\n+; Vector rotate instructions\n+\n+; Each vector element rotated by a scalar\n+; verllb, verllh, verllf, verllg\n+(define_insn \"rotl<mode>3\"\n+  [(set (match_operand:VI            0 \"register_operand\"             \"=v\")\n+\t(rotate:VI (match_operand:VI 1 \"register_operand\"              \"v\")\n+\t\t   (match_operand:SI 2 \"shift_count_or_setmem_operand\" \"Y\")))]\n+  \"TARGET_VX\"\n+  \"verll<bhfgq>\\t%v0,%v1,%Y2\"\n+  [(set_attr \"op_type\" \"VRS\")])\n+\n+; Each vector element rotated by the corresponding vector element\n+; verllvb, verllvh, verllvf, verllvg\n+(define_insn \"vrotl<mode>3\"\n+  [(set (match_operand:VI            0 \"register_operand\" \"=v\")\n+\t(rotate:VI (match_operand:VI 1 \"register_operand\"  \"v\")\n+\t\t   (match_operand:VI 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"verllv<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+\n+; Shift each element by scalar value\n+\n+; veslb, veslh, veslf, veslg\n+(define_insn \"ashl<mode>3\"\n+  [(set (match_operand:VI            0 \"register_operand\"             \"=v\")\n+\t(ashift:VI (match_operand:VI 1 \"register_operand\"              \"v\")\n+\t\t   (match_operand:SI 2 \"shift_count_or_setmem_operand\" \"Y\")))]\n+  \"TARGET_VX\"\n+  \"vesl<bhfgq>\\t%v0,%v1,%Y2\"\n+  [(set_attr \"op_type\" \"VRS\")])\n+\n+; vesrab, vesrah, vesraf, vesrag\n+(define_insn \"ashr<mode>3\"\n+  [(set (match_operand:VI              0 \"register_operand\"             \"=v\")\n+\t(ashiftrt:VI (match_operand:VI 1 \"register_operand\"              \"v\")\n+\t\t     (match_operand:SI 2 \"shift_count_or_setmem_operand\" \"Y\")))]\n+  \"TARGET_VX\"\n+  \"vesra<bhfgq>\\t%v0,%v1,%Y2\"\n+  [(set_attr \"op_type\" \"VRS\")])\n+\n+; vesrlb, vesrlh, vesrlf, vesrlg\n+(define_insn \"lshr<mode>3\"\n+  [(set (match_operand:VI              0 \"register_operand\"             \"=v\")\n+\t(lshiftrt:VI (match_operand:VI 1 \"register_operand\"              \"v\")\n+\t\t     (match_operand:SI 2 \"shift_count_or_setmem_operand\" \"Y\")))]\n+  \"TARGET_VX\"\n+  \"vesrl<bhfgq>\\t%v0,%v1,%Y2\"\n+  [(set_attr \"op_type\" \"VRS\")])\n+\n+\n+; Shift each element by corresponding vector element\n+\n+; veslvb, veslvh, veslvf, veslvg\n+(define_insn \"vashl<mode>3\"\n+  [(set (match_operand:VI            0 \"register_operand\" \"=v\")\n+\t(ashift:VI (match_operand:VI 1 \"register_operand\"  \"v\")\n+\t\t   (match_operand:VI 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"veslv<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vesravb, vesravh, vesravf, vesravg\n+(define_insn \"vashr<mode>3\"\n+  [(set (match_operand:VI              0 \"register_operand\" \"=v\")\n+\t(ashiftrt:VI (match_operand:VI 1 \"register_operand\"  \"v\")\n+\t\t     (match_operand:VI 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vesrav<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vesrlvb, vesrlvh, vesrlvf, vesrlvg\n+(define_insn \"vlshr<mode>3\"\n+  [(set (match_operand:VI              0 \"register_operand\" \"=v\")\n+\t(lshiftrt:VI (match_operand:VI 1 \"register_operand\"  \"v\")\n+\t\t     (match_operand:VI 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vesrlv<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; Vector shift right logical by byte\n+\n+; Pattern used by e.g. popcount\n+(define_insn \"*vec_srb<mode>\"\n+  [(set (match_operand:V_HW 0 \"register_operand\"                    \"=v\")\n+\t(unspec:V_HW [(match_operand:V_HW 1 \"register_operand\"       \"v\")\n+\t\t      (match_operand:<tointvec> 2 \"register_operand\" \"v\")]\n+\t\t     UNSPEC_VEC_SRLB))]\n+  \"TARGET_VX\"\n+  \"vsrlb\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+\n+; vmnb, vmnh, vmnf, vmng\n+(define_insn \"smin<mode>3\"\n+  [(set (match_operand:VI          0 \"register_operand\" \"=v\")\n+\t(smin:VI (match_operand:VI 1 \"register_operand\"  \"v\")\n+\t\t (match_operand:VI 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vmn<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vmxb, vmxh, vmxf, vmxg\n+(define_insn \"smax<mode>3\"\n+  [(set (match_operand:VI          0 \"register_operand\" \"=v\")\n+\t(smax:VI (match_operand:VI 1 \"register_operand\"  \"v\")\n+\t\t (match_operand:VI 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vmx<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vmnlb, vmnlh, vmnlf, vmnlg\n+(define_insn \"umin<mode>3\"\n+  [(set (match_operand:VI          0 \"register_operand\" \"=v\")\n+\t(umin:VI (match_operand:VI 1 \"register_operand\"  \"v\")\n+\t\t (match_operand:VI 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vmnl<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vmxlb, vmxlh, vmxlf, vmxlg\n+(define_insn \"umax<mode>3\"\n+  [(set (match_operand:VI          0 \"register_operand\" \"=v\")\n+\t(umax:VI (match_operand:VI 1 \"register_operand\"  \"v\")\n+\t\t (match_operand:VI 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vmxl<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vmeb, vmeh, vmef\n+(define_insn \"vec_widen_smult_even_<mode>\"\n+  [(set (match_operand:<vec_double>                    0 \"register_operand\" \"=v\")\n+\t(unspec:<vec_double> [(match_operand:VI_QHS 1 \"register_operand\"  \"v\")\n+\t\t\t      (match_operand:VI_QHS 2 \"register_operand\"  \"v\")]\n+\t\t\t     UNSPEC_VEC_SMULT_EVEN))]\n+  \"TARGET_VX\"\n+  \"vme<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vmleb, vmleh, vmlef\n+(define_insn \"vec_widen_umult_even_<mode>\"\n+  [(set (match_operand:<vec_double>                 0 \"register_operand\" \"=v\")\n+\t(unspec:<vec_double> [(match_operand:VI_QHS 1 \"register_operand\"  \"v\")\n+\t\t\t      (match_operand:VI_QHS 2 \"register_operand\"  \"v\")]\n+\t\t\t     UNSPEC_VEC_UMULT_EVEN))]\n+  \"TARGET_VX\"\n+  \"vmle<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vmob, vmoh, vmof\n+(define_insn \"vec_widen_smult_odd_<mode>\"\n+  [(set (match_operand:<vec_double>                 0 \"register_operand\" \"=v\")\n+\t(unspec:<vec_double> [(match_operand:VI_QHS 1 \"register_operand\"  \"v\")\n+\t\t\t      (match_operand:VI_QHS 2 \"register_operand\"  \"v\")]\n+\t\t\t     UNSPEC_VEC_SMULT_ODD))]\n+  \"TARGET_VX\"\n+  \"vmo<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vmlob, vmloh, vmlof\n+(define_insn \"vec_widen_umult_odd_<mode>\"\n+  [(set (match_operand:<vec_double>                 0 \"register_operand\" \"=v\")\n+\t(unspec:<vec_double> [(match_operand:VI_QHS 1 \"register_operand\"  \"v\")\n+\t\t\t      (match_operand:VI_QHS 2 \"register_operand\"  \"v\")]\n+\t\t\t     UNSPEC_VEC_UMULT_ODD))]\n+  \"TARGET_VX\"\n+  \"vmlo<bhfgq>\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; vec_widen_umult_hi\n+; vec_widen_umult_lo\n+; vec_widen_smult_hi\n+; vec_widen_smult_lo\n+\n+; vec_widen_ushiftl_hi\n+; vec_widen_ushiftl_lo\n+; vec_widen_sshiftl_hi\n+; vec_widen_sshiftl_lo\n+\n+;;\n+;; Vector floating point arithmetic instructions\n+;;\n+\n+(define_insn \"addv2df3\"\n+  [(set (match_operand:V2DF            0 \"register_operand\" \"=v\")\n+\t(plus:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t   (match_operand:V2DF 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vfadb\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+(define_insn \"subv2df3\"\n+  [(set (match_operand:V2DF             0 \"register_operand\" \"=v\")\n+\t(minus:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t    (match_operand:V2DF 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vfsdb\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+(define_insn \"mulv2df3\"\n+  [(set (match_operand:V2DF            0 \"register_operand\" \"=v\")\n+\t(mult:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t   (match_operand:V2DF 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vfmdb\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+(define_insn \"divv2df3\"\n+  [(set (match_operand:V2DF           0 \"register_operand\" \"=v\")\n+\t(div:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t  (match_operand:V2DF 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vfddb\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+(define_insn \"sqrtv2df2\"\n+  [(set (match_operand:V2DF            0 \"register_operand\" \"=v\")\n+\t(sqrt:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vfsqdb\\t%v0,%v1\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+(define_insn \"fmav2df4\"\n+  [(set (match_operand:V2DF           0 \"register_operand\" \"=v\")\n+\t(fma:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t  (match_operand:V2DF 2 \"register_operand\"  \"v\")\n+\t\t  (match_operand:V2DF 3 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vfmadb\\t%v0,%v1,%v2,%v3\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+(define_insn \"fmsv2df4\"\n+  [(set (match_operand:V2DF                     0 \"register_operand\" \"=v\")\n+\t(fma:V2DF (match_operand:V2DF           1 \"register_operand\"  \"v\")\n+\t\t  (match_operand:V2DF           2 \"register_operand\"  \"v\")\n+\t\t  (neg:V2DF (match_operand:V2DF 3 \"register_operand\"  \"v\"))))]\n+  \"TARGET_VX\"\n+  \"vfmsdb\\t%v0,%v1,%v2,%v3\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+(define_insn \"negv2df2\"\n+  [(set (match_operand:V2DF           0 \"register_operand\" \"=v\")\n+\t(neg:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vflcdb\\t%v0,%v1\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+(define_insn \"absv2df2\"\n+  [(set (match_operand:V2DF           0 \"register_operand\" \"=v\")\n+\t(abs:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vflpdb\\t%v0,%v1\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+(define_insn \"*negabsv2df2\"\n+  [(set (match_operand:V2DF                     0 \"register_operand\" \"=v\")\n+\t(neg:V2DF (abs:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\"))))]\n+  \"TARGET_VX\"\n+  \"vflndb\\t%v0,%v1\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; Emulate with compare + select\n+(define_insn_and_split \"smaxv2df3\"\n+  [(set (match_operand:V2DF            0 \"register_operand\" \"=v\")\n+\t(smax:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t   (match_operand:V2DF 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"#\"\n+  \"\"\n+  [(set (match_dup 3)\n+\t(gt:V2DI (match_dup 1) (match_dup 2)))\n+   (set (match_dup 0)\n+\t(if_then_else:V2DF\n+\t (eq (match_dup 3) (match_dup 4))\n+\t (match_dup 2)\n+\t (match_dup 1)))]\n+{\n+  operands[3] = gen_reg_rtx (V2DImode);\n+  operands[4] = CONST0_RTX (V2DImode);\n+})\n+\n+; Emulate with compare + select\n+(define_insn_and_split \"sminv2df3\"\n+  [(set (match_operand:V2DF            0 \"register_operand\" \"=v\")\n+\t(smin:V2DF (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t   (match_operand:V2DF 2 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"#\"\n+  \"\"\n+  [(set (match_dup 3)\n+\t(gt:V2DI (match_dup 1) (match_dup 2)))\n+   (set (match_dup 0)\n+\t(if_then_else:V2DF\n+\t (eq (match_dup 3) (match_dup 4))\n+\t (match_dup 1)\n+\t (match_dup 2)))]\n+{\n+  operands[3] = gen_reg_rtx (V2DImode);\n+  operands[4] = CONST0_RTX (V2DImode);\n+})\n+\n+\n+;;\n+;; Integer compares\n+;;\n+\n+(define_insn \"*vec_cmp<VICMP_HW_OP:code><VI:mode>_nocc\"\n+  [(set (match_operand:VI                 2 \"register_operand\" \"=v\")\n+\t(VICMP_HW_OP:VI (match_operand:VI 0 \"register_operand\"  \"v\")\n+\t\t\t(match_operand:VI 1 \"register_operand\"  \"v\")))]\n+  \"TARGET_VX\"\n+  \"vc<VICMP_HW_OP:insn_cmp_op><VI:bhfgq>\\t%v2,%v0,%v1\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+\n+;;\n+;; Floating point compares\n+;;\n+\n+; EQ, GT, GE\n+(define_insn \"*vec_cmp<VFCMP_HW_OP:code>v2df_nocc\"\n+  [(set (match_operand:V2DI                   0 \"register_operand\" \"=v\")\n+\t(VFCMP_HW_OP:V2DI (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t\t  (match_operand:V2DF 2 \"register_operand\"  \"v\")))]\n+   \"TARGET_VX\"\n+   \"vfc<VFCMP_HW_OP:asm_fcmp_op>db\\t%v0,%v1,%v2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; Expanders for not directly supported comparisons\n+\n+; UNEQ a u== b -> !(a > b | b > a)\n+(define_expand \"vec_cmpuneqv2df\"\n+  [(set (match_operand:V2DI          0 \"register_operand\" \"=v\")\n+\t(gt:V2DI (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t (match_operand:V2DF 2 \"register_operand\"  \"v\")))\n+   (set (match_dup 3)\n+\t(gt:V2DI (match_dup 2) (match_dup 1)))\n+   (set (match_dup 0) (ior:V2DI (match_dup 0) (match_dup 3)))\n+   (set (match_dup 0) (not:V2DI (match_dup 0)))]\n+  \"TARGET_VX\"\n+{\n+  operands[3] = gen_reg_rtx (V2DImode);\n+})\n+\n+; LTGT a <> b -> a > b | b > a\n+(define_expand \"vec_cmpltgtv2df\"\n+  [(set (match_operand:V2DI          0 \"register_operand\" \"=v\")\n+\t(gt:V2DI (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t (match_operand:V2DF 2 \"register_operand\"  \"v\")))\n+   (set (match_dup 3) (gt:V2DI (match_dup 2) (match_dup 1)))\n+   (set (match_dup 0) (ior:V2DI (match_dup 0) (match_dup 3)))]\n+  \"TARGET_VX\"\n+{\n+  operands[3] = gen_reg_rtx (V2DImode);\n+})\n+\n+; ORDERED (a, b): a >= b | b > a\n+(define_expand \"vec_orderedv2df\"\n+  [(set (match_operand:V2DI          0 \"register_operand\" \"=v\")\n+\t(ge:V2DI (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t (match_operand:V2DF 2 \"register_operand\"  \"v\")))\n+   (set (match_dup 3) (gt:V2DI (match_dup 2) (match_dup 1)))\n+   (set (match_dup 0) (ior:V2DI (match_dup 0) (match_dup 3)))]\n+  \"TARGET_VX\"\n+{\n+  operands[3] = gen_reg_rtx (V2DImode);\n+})\n+\n+; UNORDERED (a, b): !ORDERED (a, b)\n+(define_expand \"vec_unorderedv2df\"\n+  [(set (match_operand:V2DI          0 \"register_operand\" \"=v\")\n+\t(ge:V2DI (match_operand:V2DF 1 \"register_operand\"  \"v\")\n+\t\t (match_operand:V2DF 2 \"register_operand\"  \"v\")))\n+   (set (match_dup 3) (gt:V2DI (match_dup 2) (match_dup 1)))\n+   (set (match_dup 0) (ior:V2DI (match_dup 0) (match_dup 3)))\n+   (set (match_dup 0) (not:V2DI (match_dup 0)))]\n+  \"TARGET_VX\"\n+{\n+  operands[3] = gen_reg_rtx (V2DImode);\n+})\n+\n+(define_insn \"*vec_load_pairv2di\"\n+  [(set (match_operand:V2DI                0 \"register_operand\" \"=v\")\n+\t(vec_concat:V2DI (match_operand:DI 1 \"register_operand\"  \"d\")\n+\t\t\t (match_operand:DI 2 \"register_operand\"  \"d\")))]\n+  \"TARGET_VX\"\n+  \"vlvgp\\t%v0,%1,%2\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+(define_insn \"vllv16qi\"\n+  [(set (match_operand:V16QI              0 \"register_operand\" \"=v\")\n+\t(unspec:V16QI [(match_operand:SI  1 \"register_operand\"  \"d\")\n+\t\t       (match_operand:BLK 2 \"memory_operand\"    \"Q\")]\n+\t\t      UNSPEC_VEC_LOAD_LEN))]\n+  \"TARGET_VX\"\n+  \"vll\\t%v0,%1,%2\"\n+  [(set_attr \"op_type\" \"VRS\")])\n+\n+; vfenebs, vfenehs, vfenefs\n+; vfenezbs, vfenezhs, vfenezfs\n+(define_insn \"vec_vfenes<mode>\"\n+  [(set (match_operand:VI_HW_QHS 0 \"register_operand\" \"=v\")\n+\t(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 \"register_operand\" \"v\")\n+\t\t\t   (match_operand:VI_HW_QHS 2 \"register_operand\" \"v\")\n+\t\t\t   (match_operand:QI 3 \"immediate_operand\" \"C\")]\n+\t\t\t  UNSPEC_VEC_VFENE))\n+   (set (reg:CCRAW CC_REGNUM)\n+\t(unspec:CCRAW [(match_dup 1)\n+\t\t       (match_dup 2)\n+\t\t       (match_dup 3)]\n+\t\t      UNSPEC_VEC_VFENECC))]\n+  \"TARGET_VX\"\n+{\n+  unsigned HOST_WIDE_INT flags = INTVAL (operands[3]);\n+\n+  gcc_assert (!(flags & ~(VSTRING_FLAG_ZS | VSTRING_FLAG_CS)));\n+  flags &= ~VSTRING_FLAG_CS;\n+\n+  if (flags == VSTRING_FLAG_ZS)\n+    return \"vfenez<bhfgq>s\\t%v0,%v1,%v2\";\n+  return \"vfene<bhfgq>s\\t%v0,%v1,%v2\";\n+}\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+\n+; Vector select\n+\n+; The following splitters simplify vec_sel for constant 0 or -1\n+; selection sources.  This is required to generate efficient code for\n+; vcond.\n+\n+; a = b == c;\n+(define_split\n+  [(set (match_operand:V 0 \"register_operand\" \"\")\n+\t(if_then_else:V\n+\t (eq (match_operand:<tointvec> 3 \"register_operand\" \"\")\n+\t     (match_operand:V 4 \"const0_operand\" \"\"))\n+\t (match_operand:V 1 \"const0_operand\" \"\")\n+\t (match_operand:V 2 \"constm1_operand\" \"\")))]\n+  \"TARGET_VX\"\n+  [(set (match_dup 0) (match_dup 3))]\n+{\n+  PUT_MODE (operands[3], <V:MODE>mode);\n+})\n+\n+; a = ~(b == c)\n+(define_split\n+  [(set (match_operand:V 0 \"register_operand\" \"\")\n+\t(if_then_else:V\n+\t (eq (match_operand:<tointvec> 3 \"register_operand\" \"\")\n+\t     (match_operand:V 4 \"const0_operand\" \"\"))\n+\t (match_operand:V 1 \"constm1_operand\" \"\")\n+\t (match_operand:V 2 \"const0_operand\" \"\")))]\n+  \"TARGET_VX\"\n+  [(set (match_dup 0) (not:V (match_dup 3)))]\n+{\n+  PUT_MODE (operands[3], <V:MODE>mode);\n+})\n+\n+; a = b != c\n+(define_split\n+  [(set (match_operand:V 0 \"register_operand\" \"\")\n+\t(if_then_else:V\n+\t (ne (match_operand:<tointvec> 3 \"register_operand\" \"\")\n+\t     (match_operand:V 4 \"const0_operand\" \"\"))\n+\t (match_operand:V 1 \"constm1_operand\" \"\")\n+\t (match_operand:V 2 \"const0_operand\" \"\")))]\n+  \"TARGET_VX\"\n+  [(set (match_dup 0) (match_dup 3))]\n+{\n+  PUT_MODE (operands[3], <V:MODE>mode);\n+})\n+\n+; a = ~(b != c)\n+(define_split\n+  [(set (match_operand:V 0 \"register_operand\" \"\")\n+\t(if_then_else:V\n+\t (ne (match_operand:<tointvec> 3 \"register_operand\" \"\")\n+\t     (match_operand:V 4 \"const0_operand\" \"\"))\n+\t (match_operand:V 1 \"const0_operand\" \"\")\n+\t (match_operand:V 2 \"constm1_operand\" \"\")))]\n+  \"TARGET_VX\"\n+  [(set (match_dup 0) (not:V (match_dup 3)))]\n+{\n+  PUT_MODE (operands[3], <V:MODE>mode);\n+})\n+\n+; op0 = op3 == 0 ? op1 : op2\n+(define_insn \"*vec_sel0<mode>\"\n+  [(set (match_operand:V 0 \"register_operand\" \"=v\")\n+\t(if_then_else:V\n+\t (eq (match_operand:<tointvec> 3 \"register_operand\" \"v\")\n+\t     (match_operand:<tointvec> 4 \"const0_operand\" \"\"))\n+\t (match_operand:V 1 \"register_operand\" \"v\")\n+\t (match_operand:V 2 \"register_operand\" \"v\")))]\n+  \"TARGET_VX\"\n+  \"vsel\\t%v0,%2,%1,%3\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; op0 = !op3 == 0 ? op1 : op2\n+(define_insn \"*vec_sel0<mode>\"\n+  [(set (match_operand:V 0 \"register_operand\" \"=v\")\n+\t(if_then_else:V\n+\t (eq (not:<tointvec> (match_operand:<tointvec> 3 \"register_operand\" \"v\"))\n+\t     (match_operand:<tointvec> 4 \"const0_operand\" \"\"))\n+\t (match_operand:V 1 \"register_operand\" \"v\")\n+\t (match_operand:V 2 \"register_operand\" \"v\")))]\n+  \"TARGET_VX\"\n+  \"vsel\\t%v0,%1,%2,%3\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; op0 = op3 == -1 ? op1 : op2\n+(define_insn \"*vec_sel1<mode>\"\n+  [(set (match_operand:V 0 \"register_operand\" \"=v\")\n+\t(if_then_else:V\n+\t (eq (match_operand:<tointvec> 3 \"register_operand\" \"v\")\n+\t     (match_operand:<tointvec> 4 \"constm1_operand\" \"\"))\n+\t (match_operand:V 1 \"register_operand\" \"v\")\n+\t (match_operand:V 2 \"register_operand\" \"v\")))]\n+  \"TARGET_VX\"\n+  \"vsel\\t%v0,%1,%2,%3\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+; op0 = !op3 == -1 ? op1 : op2\n+(define_insn \"*vec_sel1<mode>\"\n+  [(set (match_operand:V 0 \"register_operand\" \"=v\")\n+\t(if_then_else:V\n+\t (eq (not:<tointvec> (match_operand:<tointvec> 3 \"register_operand\" \"v\"))\n+\t     (match_operand:<tointvec> 4 \"constm1_operand\" \"\"))\n+\t (match_operand:V 1 \"register_operand\" \"v\")\n+\t (match_operand:V 2 \"register_operand\" \"v\")))]\n+  \"TARGET_VX\"\n+  \"vsel\\t%v0,%2,%1,%3\"\n+  [(set_attr \"op_type\" \"VRR\")])\n+\n+\n+\n+; reduc_smin\n+; reduc_smax\n+; reduc_umin\n+; reduc_umax\n+\n+; vec_shl vrep + vsl\n+; vec_shr\n+\n+; vec_pack_trunc\n+; vec_pack_ssat\n+; vec_pack_usat\n+; vec_pack_sfix_trunc\n+; vec_pack_ufix_trunc\n+; vec_unpacks_hi\n+; vec_unpacks_low\n+; vec_unpacku_hi\n+; vec_unpacku_low\n+; vec_unpacks_float_hi\n+; vec_unpacks_float_lo\n+; vec_unpacku_float_hi\n+; vec_unpacku_float_lo"}]}