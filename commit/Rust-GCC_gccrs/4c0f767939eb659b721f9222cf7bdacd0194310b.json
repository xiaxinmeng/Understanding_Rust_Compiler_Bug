{"sha": "4c0f767939eb659b721f9222cf7bdacd0194310b", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NGMwZjc2NzkzOWViNjU5YjcyMWY5MjIyY2Y3YmRhY2QwMTk0MzEwYg==", "commit": {"author": {"name": "Jan Hubicka", "email": "jh@suse.cz", "date": "2011-04-17T14:22:20Z"}, "committer": {"name": "Jan Hubicka", "email": "hubicka@gcc.gnu.org", "date": "2011-04-17T14:22:20Z"}, "message": "lto-symtab.c (lto_cgraph_replace_node): When call statement is present, also set gimple_call_set_cannot_inline.\n\n\n\t* lto-symtab.c (lto_cgraph_replace_node): When call statement is\n\tpresent, also set gimple_call_set_cannot_inline.\n\t* ipa-inline.c: Update toplevel comment.\n\t(MAX_TIME): Remove.\n\t(cgraph_clone_inlined_nodes): Fix linebreaks.\n\t(cgraph_check_inline_limits): Restructure to ...\n\t(caller_growth_limits): ... this one; be more tolerant\n\ton growth in nested inline chains; add explanatory comment;\n\tfix stack accounting thinko introduced by previous patch.\n\t(cgraph_default_inline_p): Remove.\n\t(report_inline_failed_reason): New function.\n\t(can_inline_edge_p): New function.\n\t(can_early_inline_edge_p): New function.\n\t(leaf_node_p): Move upwards in file.\n\t(want_early_inline_function_p): New function.\n\t(want_inline_small_function_p): New function.\n\t(want_inline_self_recursive_call_p): New function.\n\t(cgraph_edge_badness): Rename to ...\n\t(edge_badness) ... this one; fix linebreaks.\n\t(update_edge_key): Update call of edge_baddness; add\n\tdetailed dump about queue updates.\n\t(update_caller_keys): Use can_inline_edge_p and\n\twant_inline_small_function_p.\n\t(cgraph_decide_recursive_inlining): Rename to...\n\t(recursive_inlining): Use can_inline_edge_p and\n\twant_inline_self_recursive_call_p; simplify and\n\tremove no longer valid FIXME.\n\t(cgraph_set_inline_failed): Remove.\n\t(add_new_edges_to_heap): Use can_inline_edge_p and\n\twant_inline_small_function_p.\n\t(cgraph_decide_inlining_of_small_functions): Rename to ...\n\t(inline_small_functions): ... this one; cleanup; use\n\tcan/want predicates; cleanup debug ouput; work edges\n\ttill fibheap is exhausted and do not stop once unit\n\tgrowth is reached; remove later loop processing remaining\n\tedges.\n\t(cgraph_flatten): Rename to ...\n\t(flatten_function): ... this one; use can_inline_edge_p\n\tand can_early_inline_edge_p predicates.\n\t(cgraph_decide_inlining): Rename to ...\n\t(ipa_inline): ... this one; remove unreachable nodes before\n\tinlining functions called once; simplify the pass.\n\t(cgraph_perform_always_inlining): Rename to ...\n\t(inline_always_inline_functions): ... this one; use\n\tDECL_DISREGARD_INLINE_LIMITS; use can_inline_edge_p\n\tpredicate\n\t(cgraph_decide_inlining_incrementally): Rename to ...\n\t(early_inline_small_functions): ... this one; simplify\n\tusing new predicates; cleanup; make dumps prettier.\n\t(cgraph_early_inlining): Rename to ...\n\t(early_inliner): newer inline regular functions into always-inlines;\n\tfix updating of call stmt summaries.\n\t(pass_early_inline): Update for new names.\n\t(inline_transform): Fix formating.\n\t(gate_cgraph_decide_inlining): Rename to ...\n\t(pass_ipa_inline): ... this one.\n\t* ipa-inline.h (inline_summary): Remove disregard_inline_limits.\n\t* ipa-inline-analysis.c (dump_inline_summary): Update.\n\t(compute_inline_parameters): Do not compute disregard_inline_limits;\n\tlook for mismatching arguments.\n\t(estimate_growth): Fix handlig of non-trivial self recursion.\n\t(inline_read_summary): Do not read info->disregard_inline_limits.\n\t(inline_write_summary): Do not write info->disregard_inline_limits.\n\t* tree-inline.c (inline_forbidden_into_p, tree_can_inline_p): Remove and\n\tmove all checks into can_inline_edge_p predicate; re-enable code comparing\n\toptimization levels.\n\t(expand_call_inline): Do not test inline_forbidden_into_p.\n\t* Makefile.in (ipa-inline.o): Update arguments.\n\n\t* gcc.dg/winline-5.c: Update testcase.\n\nFrom-SVN: r172609", "tree": {"sha": "437b0b8c6f18f8d396674a0c5a7fe26c430fa602", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/437b0b8c6f18f8d396674a0c5a7fe26c430fa602"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/4c0f767939eb659b721f9222cf7bdacd0194310b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4c0f767939eb659b721f9222cf7bdacd0194310b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4c0f767939eb659b721f9222cf7bdacd0194310b", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4c0f767939eb659b721f9222cf7bdacd0194310b/comments", "author": null, "committer": null, "parents": [{"sha": "51c5169c60be01c229b8637d11283d843bc6e126", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/51c5169c60be01c229b8637d11283d843bc6e126", "html_url": "https://github.com/Rust-GCC/gccrs/commit/51c5169c60be01c229b8637d11283d843bc6e126"}], "stats": {"total": 1492, "additions": 825, "deletions": 667}, "files": [{"sha": "23199ca80b30aa99c74bebce34b5ca90974dc7f9", "filename": "gcc/ChangeLog", "status": "modified", "additions": 71, "deletions": 0, "changes": 71, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=4c0f767939eb659b721f9222cf7bdacd0194310b", "patch": "@@ -1,3 +1,74 @@\n+2011-04-17  Jan Hubicka  <jh@suse.cz>\n+\n+\t* lto-symtab.c (lto_cgraph_replace_node): When call statement is\n+\tpresent, also set gimple_call_set_cannot_inline.\n+\t* ipa-inline.c: Update toplevel comment.\n+\t(MAX_TIME): Remove.\n+\t(cgraph_clone_inlined_nodes): Fix linebreaks.\n+\t(cgraph_check_inline_limits): Restructure to ...\n+\t(caller_growth_limits): ... this one; be more tolerant\n+\ton growth in nested inline chains; add explanatory comment;\n+\tfix stack accounting thinko introduced by previous patch.\n+\t(cgraph_default_inline_p): Remove.\n+\t(report_inline_failed_reason): New function.\n+\t(can_inline_edge_p): New function.\n+\t(can_early_inline_edge_p): New function.\n+\t(leaf_node_p): Move upwards in file.\n+\t(want_early_inline_function_p): New function.\n+\t(want_inline_small_function_p): New function.\n+\t(want_inline_self_recursive_call_p): New function.\n+\t(cgraph_edge_badness): Rename to ...\n+\t(edge_badness) ... this one; fix linebreaks.\n+\t(update_edge_key): Update call of edge_baddness; add\n+\tdetailed dump about queue updates.\n+\t(update_caller_keys): Use can_inline_edge_p and\n+\twant_inline_small_function_p.\n+\t(cgraph_decide_recursive_inlining): Rename to...\n+\t(recursive_inlining): Use can_inline_edge_p and\n+\twant_inline_self_recursive_call_p; simplify and\n+\tremove no longer valid FIXME.\n+\t(cgraph_set_inline_failed): Remove.\n+\t(add_new_edges_to_heap): Use can_inline_edge_p and\n+\twant_inline_small_function_p.\n+\t(cgraph_decide_inlining_of_small_functions): Rename to ...\n+\t(inline_small_functions): ... this one; cleanup; use\n+\tcan/want predicates; cleanup debug ouput; work edges\n+\ttill fibheap is exhausted and do not stop once unit\n+\tgrowth is reached; remove later loop processing remaining\n+\tedges.\n+\t(cgraph_flatten): Rename to ...\n+\t(flatten_function): ... this one; use can_inline_edge_p\n+\tand can_early_inline_edge_p predicates.\n+\t(cgraph_decide_inlining): Rename to ...\n+\t(ipa_inline): ... this one; remove unreachable nodes before\n+\tinlining functions called once; simplify the pass.\n+\t(cgraph_perform_always_inlining): Rename to ...\n+\t(inline_always_inline_functions): ... this one; use\n+\tDECL_DISREGARD_INLINE_LIMITS; use can_inline_edge_p\n+\tpredicate\n+\t(cgraph_decide_inlining_incrementally): Rename to ...\n+\t(early_inline_small_functions): ... this one; simplify\n+\tusing new predicates; cleanup; make dumps prettier.\n+\t(cgraph_early_inlining): Rename to ...\n+\t(early_inliner): newer inline regular functions into always-inlines;\n+\tfix updating of call stmt summaries.\n+\t(pass_early_inline): Update for new names.\n+\t(inline_transform): Fix formating.\n+\t(gate_cgraph_decide_inlining): Rename to ...\n+\t(pass_ipa_inline): ... this one.\n+\t* ipa-inline.h (inline_summary): Remove disregard_inline_limits.\n+\t* ipa-inline-analysis.c (dump_inline_summary): Update.\n+\t(compute_inline_parameters): Do not compute disregard_inline_limits;\n+\tlook for mismatching arguments.\n+\t(estimate_growth): Fix handlig of non-trivial self recursion.\n+\t(inline_read_summary): Do not read info->disregard_inline_limits.\n+\t(inline_write_summary): Do not write info->disregard_inline_limits.\n+\t* tree-inline.c (inline_forbidden_into_p, tree_can_inline_p): Remove and\n+\tmove all checks into can_inline_edge_p predicate; re-enable code comparing\n+\toptimization levels.\n+\t(expand_call_inline): Do not test inline_forbidden_into_p.\n+\t* Makefile.in (ipa-inline.o): Update arguments.\n+\n 2011-04-17  Revital Eres  <revital.eres@linaro.org>\n \n \t* ddg.c (free_ddg_all_sccs): Free sccs field in struct ddg_all_sccs."}, {"sha": "a1b78596bc784ba713b61fca67a131c813616cb5", "filename": "gcc/Makefile.in", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=4c0f767939eb659b721f9222cf7bdacd0194310b", "patch": "@@ -3028,8 +3028,8 @@ matrix-reorg.o : matrix-reorg.c $(CONFIG_H) $(SYSTEM_H) coretypes.h  \\\n ipa-inline.o : ipa-inline.c gt-ipa-inline.h $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    $(TREE_H) langhooks.h $(TREE_INLINE_H) $(FLAGS_H) $(CGRAPH_H) intl.h \\\n    $(DIAGNOSTIC_H) $(FIBHEAP_H) $(PARAMS_H) $(TIMEVAR_H) $(TREE_PASS_H) \\\n-   $(HASHTAB_H) $(COVERAGE_H) $(GGC_H) $(TREE_FLOW_H) $(RTL_H) $(IPA_PROP_H) \\\n-   $(EXCEPT_H) gimple-pretty-print.h ipa-inline.h\n+   $(COVERAGE_H) $(GGC_H) $(TREE_FLOW_H) $(RTL_H) $(IPA_PROP_H) \\\n+   $(EXCEPT_H) gimple-pretty-print.h ipa-inline.h $(TARGET_H)\n ipa-inline-analysis.o : ipa-inline-analysis.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    $(TREE_H) langhooks.h $(TREE_INLINE_H) $(FLAGS_H) $(CGRAPH_H) intl.h \\\n    $(DIAGNOSTIC_H) $(PARAMS_H) $(TIMEVAR_H) $(TREE_PASS_H) \\"}, {"sha": "d10efdfeb98f9986cfbfe079e9377dfcb980b040", "filename": "gcc/cif-code.def", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Fcif-code.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Fcif-code.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcif-code.def?ref=4c0f767939eb659b721f9222cf7bdacd0194310b", "patch": "@@ -79,6 +79,8 @@ DEFCIFCODE(OPTIMIZING_FOR_SIZE,\n \n /* Inlining failed because of mismatched options or arguments.  */\n DEFCIFCODE(TARGET_OPTION_MISMATCH, N_(\"target specific option mismatch\"))\n+DEFCIFCODE(TARGET_OPTIMIZATION_MISMATCH,\n+\t   N_(\"optimization level attribute mismatch\"))\n DEFCIFCODE(MISMATCHED_ARGUMENTS, N_(\"mismatched arguments\"))\n \n /* Call was originally indirect.  */\n@@ -89,4 +91,18 @@ DEFCIFCODE(ORIGINALLY_INDIRECT_CALL,\n DEFCIFCODE(INDIRECT_UNKNOWN_CALL,\n \t   N_(\"indirect function call with a yet undetermined callee\"))\n \n+/* We can't inline different EH personalities together.  */\n+DEFCIFCODE(EH_PERSONALITY,\n+\t   N_(\"excepion handling personality mismatch\"))\n+\n+/* Don't inline if the callee can throw non-call exceptions but the\n+   caller cannot.  */\n+DEFCIFCODE(NON_CALL_EXCEPTIONS,\n+\t   N_(\"excepion handling personality mismatch\"))\n+\n+/* Don't inline if the callee can throw non-call exceptions but the\n+   caller cannot.  */\n+DEFCIFCODE(OPTIMIZATION_MISMATCH,\n+\t   N_(\"optimization mode mismatch\"))\n+\n DEFCIFCODE(OVERWRITABLE, N_(\"function body can be overwriten at linktime\"))"}, {"sha": "30fbcbc4f07cf3ba22720a6771f1674679f00cfe", "filename": "gcc/ipa-inline-analysis.c", "status": "modified", "additions": 39, "deletions": 25, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Fipa-inline-analysis.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Fipa-inline-analysis.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fipa-inline-analysis.c?ref=4c0f767939eb659b721f9222cf7bdacd0194310b", "patch": "@@ -131,7 +131,7 @@ dump_inline_summary (FILE *f, struct cgraph_node *node)\n       struct inline_summary *s = inline_summary (node);\n       fprintf (f, \"Inline summary for %s/%i\", cgraph_node_name (node),\n \t       node->uid);\n-      if (s->disregard_inline_limits)\n+      if (DECL_DISREGARD_INLINE_LIMITS (node->decl))\n \tfprintf (f, \" always_inline\");\n       if (s->inlinable)\n \tfprintf (f, \" inlinable\");\n@@ -142,7 +142,7 @@ dump_inline_summary (FILE *f, struct cgraph_node *node)\n       fprintf (f, \"  global time:     %i\\n\", s->time);\n       fprintf (f, \"  self size:       %i, benefit: %i\\n\",\n \t       s->self_size, s->size_inlining_benefit);\n-      fprintf (f, \"  global size:     %i\", s->size);\n+      fprintf (f, \"  global size:     %i\\n\", s->size);\n       fprintf (f, \"  self stack:      %i\\n\",\n \t       (int)s->estimated_self_stack_size);\n       fprintf (f, \"  global stack:    %i\\n\\n\",\n@@ -303,6 +303,17 @@ estimate_function_body_sizes (struct cgraph_node *node)\n \t      struct cgraph_edge *edge = cgraph_edge (node, stmt);\n \t      edge->call_stmt_size = this_size;\n \t      edge->call_stmt_time = this_time;\n+\n+\t      /* Do not inline calls where we cannot triviall work around mismatches\n+\t\t in argument or return types.  */\n+\t      if (edge->callee\n+\t\t  && !gimple_check_call_matching_types (stmt, edge->callee->decl))\n+\t\t{\n+\t\t  edge->call_stmt_cannot_inline_p = true;\n+\t\t  gimple_call_set_cannot_inline (stmt, true);\n+\t\t}\n+\t      else\n+\t\tgcc_assert (!gimple_call_cannot_inline_p (stmt));\n \t    }\n \n \t  this_time *= freq;\n@@ -364,8 +375,6 @@ compute_inline_parameters (struct cgraph_node *node)\n \n   /* Can this function be inlined at all?  */\n   info->inlinable = tree_inlinable_function_p (node->decl);\n-  if (!info->inlinable)\n-    info->disregard_inline_limits = 0;\n \n   /* Inlinable functions always can change signature.  */\n   if (info->inlinable)\n@@ -388,8 +397,6 @@ compute_inline_parameters (struct cgraph_node *node)\n   info->estimated_growth = INT_MIN;\n   info->stack_frame_offset = 0;\n   info->estimated_stack_size = info->estimated_self_stack_size;\n-  info->disregard_inline_limits\n-    = DECL_DISREGARD_INLINE_LIMITS (node->decl);\n }\n \n \n@@ -483,25 +490,34 @@ estimate_growth (struct cgraph_node *node)\n \n   for (e = node->callers; e; e = e->next_caller)\n     {\n-      if (e->caller == node)\n+      gcc_checking_assert (e->inline_failed);\n+\n+      if (e->caller == node\n+\t  || (e->caller->global.inlined_to\n+\t      && e->caller->global.inlined_to == node))\n         self_recursive = true;\n-      if (e->inline_failed)\n-\tgrowth += estimate_edge_growth (e);\n+      growth += estimate_edge_growth (e);\n+    }\n+     \n+\n+  /* For self recursive functions the growth estimation really should be\n+     infinity.  We don't want to return very large values because the growth\n+     plays various roles in badness computation fractions.  Be sure to not\n+     return zero or negative growths. */\n+  if (self_recursive)\n+    growth = growth < info->size ? info->size : growth;\n+  else\n+    {\n+      if (cgraph_will_be_removed_from_program_if_no_direct_calls (node)\n+\t  && !DECL_EXTERNAL (node->decl))\n+\tgrowth -= info->size;\n+      /* COMDAT functions are very often not shared across multiple units since they\n+\t come from various template instantiations.  Take this into account.  */\n+      else  if (DECL_COMDAT (node->decl)\n+\t\t&& cgraph_can_remove_if_no_direct_calls_p (node))\n+\tgrowth -= (info->size\n+\t\t   * (100 - PARAM_VALUE (PARAM_COMDAT_SHARING_PROBABILITY)) + 50) / 100;\n     }\n-\n-  /* ??? Wrong for non-trivially self recursive functions or cases where\n-     we decide to not inline for different reasons, but it is not big deal\n-     as in that case we will keep the body around, but we will also avoid\n-     some inlining.  */\n-  if (cgraph_will_be_removed_from_program_if_no_direct_calls (node)\n-      && !DECL_EXTERNAL (node->decl) && !self_recursive)\n-    growth -= info->size;\n-  /* COMDAT functions are very often not shared across multiple units since they\n-     come from various template instantiations.  Take this into account.  */\n-  else  if (DECL_COMDAT (node->decl) && !self_recursive\n-\t    && cgraph_can_remove_if_no_direct_calls_p (node))\n-    growth -= (info->size\n-\t       * (100 - PARAM_VALUE (PARAM_COMDAT_SHARING_PROBABILITY)) + 50) / 100;\n \n   info->estimated_growth = growth;\n   return growth;\n@@ -621,7 +637,6 @@ inline_read_summary (void)\n \t      bp = lto_input_bitpack (ib);\n \t      info->inlinable = bp_unpack_value (&bp, 1);\n \t      info->versionable = bp_unpack_value (&bp, 1);\n-\t      info->disregard_inline_limits = bp_unpack_value (&bp, 1);\n \t    }\n \n \t  lto_destroy_simple_input_block (file_data,\n@@ -688,7 +703,6 @@ inline_write_summary (cgraph_node_set set,\n \t  bp = bitpack_create (ob->main_stream);\n \t  bp_pack_value (&bp, info->inlinable, 1);\n \t  bp_pack_value (&bp, info->versionable, 1);\n-\t  bp_pack_value (&bp, info->disregard_inline_limits, 1);\n \t  lto_output_bitpack (&bp);\n \t}\n     }"}, {"sha": "ecad8fa1bd01786b1c372042240c7f91567e7078", "filename": "gcc/ipa-inline.c", "status": "modified", "additions": 693, "deletions": 530, "changes": 1223, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Fipa-inline.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Fipa-inline.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fipa-inline.c?ref=4c0f767939eb659b721f9222cf7bdacd0194310b", "patch": "@@ -21,73 +21,86 @@ along with GCC; see the file COPYING3.  If not see\n \n /*  Inlining decision heuristics\n \n-    We separate inlining decisions from the inliner itself and store it\n-    inside callgraph as so called inline plan.  Refer to cgraph.c\n-    documentation about particular representation of inline plans in the\n-    callgraph.\n+    The implementation of inliner is organized as follows:\n \n-    There are three major parts of this file:\n+    Transformation of callgraph to represent inlining decisions.\n \n-    cgraph_mark_inline_edge implementation\n+      The inline decisions are stored in callgraph in \"inline plan\" and\n+      all applied later.\n \n-      This function allows to mark given call inline and performs necessary\n-      modifications of cgraph (production of the clones and updating overall\n-      statistics)\n+      To mark given call inline, use cgraph_mark_inline function.\n+      The function marks the edge inlinable and, if neccesary, produces\n+      virtual clone in the callgraph representing the new copy of callee's\n+      function body.\n+\n+      The inline plan is applied on given function body by inline_transform. \n \n     inlining heuristics limits\n \n-      These functions allow to check that particular inlining is allowed\n-      by the limits specified by user (allowed function growth, overall unit\n-      growth and so on).\n+      can_inline_edge_p allow to check that particular inlining is allowed\n+      by the limits specified by user (allowed function growth, growth and so\n+      on).\n+\n+      Functions are inlined when it is obvious the result is profitable (such\n+      as functions called once or when inlining reduce code size).\n+      In addition to that we perform inlining of small functions and recursive\n+      inlining.\n \n     inlining heuristics\n \n-      This is implementation of IPA pass aiming to get as much of benefit\n-      from inlining obeying the limits checked above.\n+       The inliner itself is split into two passes:\n+\n+       pass_early_inlining\n \n-      The implementation of particular heuristics is separated from\n-      the rest of code to make it easier to replace it with more complicated\n-      implementation in the future.  The rest of inlining code acts as a\n-      library aimed to modify the callgraph and verify that the parameters\n-      on code size growth fits.\n+\t Simple local inlining pass inlining callees into current function.\n+\t This pass makes no use of whole unit analysis and thus it can do only\n+\t very simple decisions based on local properties.\n \n-      To mark given call inline, use cgraph_mark_inline function, the\n-      verification is performed by cgraph_default_inline_p and\n-      cgraph_check_inline_limits.\n+\t The strength of the pass is that it is run in topological order\n+\t (reverse postorder) on the callgraph. Functions are converted into SSA\n+\t form just before this pass and optimized subsequently. As a result, the\n+\t callees of the function seen by the early inliner was already optimized\n+\t and results of early inlining adds a lot of optimization oppurtunities\n+\t for the local optimization.\n \n-      The heuristics implements simple knapsack style algorithm ordering\n-      all functions by their \"profitability\" (estimated by code size growth)\n-      and inlining them in priority order.\n+\t The pass handle the obvious inlining decisions within the copmilation\n+\t unit - inlining auto inline functions, inlining for size and\n+\t flattening.\n \n-      cgraph_decide_inlining implements heuristics taking whole callgraph\n-      into account, while cgraph_decide_inlining_incrementally considers\n-      only one function at a time and is used by early inliner.\n+\t main strength of the pass is the ability to eliminate abstraction\n+\t penalty in C++ code (via combination of inlining and early\n+\t optimization) and thus improve quality of analysis done by real IPA\n+\t optimizers.\n \n-   The inliner itself is split into two passes:\n+\t Because of lack of whole unit knowledge, the pass can not really make\n+\t good code size/performance tradeoffs.  It however does very simple\n+\t speculative inlining allowing code size to grow by\n+\t EARLY_INLINING_INSNS when calee is leaf function.  In this case the\n+\t optimizations perfomed later are very likely to eliminate the cost.\n \n-   pass_early_inlining\n+       pass_ipa_inline\n \n-     Simple local inlining pass inlining callees into current function.  This\n-     pass makes no global whole compilation unit analysis and this when allowed\n-     to do inlining expanding code size it might result in unbounded growth of\n-     whole unit.\n+\t This is the real inliner able to handle inlining with whole program\n+\t knowledge. It performs following steps:\n \n-     The pass is run during conversion into SSA form.  Only functions already\n-     converted into SSA form are inlined, so the conversion must happen in\n-     topological order on the callgraph (that is maintained by pass manager).\n-     The functions after inlining are early optimized so the early inliner sees\n-     unoptimized function itself, but all considered callees are already\n-     optimized allowing it to unfold abstraction penalty on C++ effectively and\n-     cheaply.\n+\t 1) inlining of small functions.  This is implemented by greedy\n+\t algorithm ordering all inlinable cgraph edges by their badness and\n+\t inlining them in this order as long as inline limits allows doing so.\n \n-   pass_ipa_inline\n+\t This heuristics is not very good on inlining recursive calls. Recursive\n+\t calls can be inlined with results similar to loop unrolling. To do so,\n+\t special purpose recursive inliner is executed on function when\n+\t recursive edge is met as viable candidate.\n \n-     This is the main pass implementing simple greedy algorithm to do inlining\n-     of small functions that results in overall growth of compilation unit and\n-     inlining of functions called once.  The pass compute just so called inline\n-     plan (representation of inlining to be done in callgraph) and unlike early\n-     inlining it is not performing the inlining itself.\n- */\n+\t 2) Unreachable functions are removed from callgraph.  Inlining leads\n+\t to devirtualization and other modification of callgraph so functions\n+\t may become unreachable during the process. Also functions declared as\n+\t extern inline or virtual functions are removed, since after inlining\n+\t we no longer need the offline bodies.\n+\n+\t 3) Functions called once and not exported from the unit are inlined.\n+\t This should almost always lead to reduction of code size by eliminating\n+\t the need for offline copy of the function.  */\n \n #include \"config.h\"\n #include \"system.h\"\n@@ -105,18 +118,15 @@ along with GCC; see the file COPYING3.  If not see\n #include \"fibheap.h\"\n #include \"intl.h\"\n #include \"tree-pass.h\"\n-#include \"hashtab.h\"\n #include \"coverage.h\"\n #include \"ggc.h\"\n-#include \"tree-flow.h\"\n #include \"rtl.h\"\n+#include \"tree-flow.h\"\n #include \"ipa-prop.h\"\n #include \"except.h\"\n+#include \"target.h\"\n #include \"ipa-inline.h\"\n \n-#define MAX_TIME 1000000000\n-\n-\n /* Statistics we collect about inlining algorithm.  */\n static int ncalls_inlined;\n static int nfunctions_inlined;\n@@ -163,19 +173,21 @@ cgraph_clone_inlined_nodes (struct cgraph_edge *e, bool duplicate,\n       /* We may eliminate the need for out-of-line copy to be output.\n \t In that case just go ahead and re-use it.  */\n       if (!e->callee->callers->next_caller\n-\t  /* Recursive inlining never wants the master clone to be overwritten.  */\n+\t  /* Recursive inlining never wants the master clone to\n+\t     be overwritten.  */\n \t  && update_original\n-\t  /* FIXME: When address is taken of DECL_EXTERNAL function we still can remove its\n-\t     offline copy, but we would need to keep unanalyzed node in the callgraph so\n-\t     references can point to it.  */\n+\t  /* FIXME: When address is taken of DECL_EXTERNAL function we still\n+\t     can remove its offline copy, but we would need to keep unanalyzed\n+\t     node in the callgraph so references can point to it.  */\n \t  && !e->callee->address_taken\n \t  && cgraph_can_remove_if_no_direct_calls_p (e->callee)\n \t  /* Inlining might enable more devirtualizing, so we want to remove\n \t     those only after all devirtualizable virtual calls are processed.\n \t     Lacking may edges in callgraph we just preserve them post\n \t     inlining.  */\n \t  && (!DECL_VIRTUAL_P (e->callee->decl)\n-\t      || (!DECL_COMDAT (e->callee->decl) && !DECL_EXTERNAL (e->callee->decl)))\n+\t      || (!DECL_COMDAT (e->callee->decl)\n+\t\t  && !DECL_EXTERNAL (e->callee->decl)))\n \t  /* Don't reuse if more than one function shares a comdat group.\n \t     If the other function(s) are needed, we need to emit even\n \t     this function out of line.  */\n@@ -214,7 +226,8 @@ cgraph_clone_inlined_nodes (struct cgraph_edge *e, bool duplicate,\n       + caller_info->estimated_self_stack_size;\n   peak = callee_info->stack_frame_offset\n       + callee_info->estimated_self_stack_size;\n-  if (inline_summary (e->callee->global.inlined_to)->estimated_stack_size < peak)\n+  if (inline_summary (e->callee->global.inlined_to)->estimated_stack_size\n+      < peak)\n     inline_summary (e->callee->global.inlined_to)->estimated_stack_size = peak;\n   cgraph_propagate_frequency (e->callee);\n \n@@ -272,33 +285,52 @@ cgraph_mark_inline_edge (struct cgraph_edge *e, bool update_original,\n     return false;\n }\n \n-/* Return false when inlining edge E is not good idea\n-   as it would cause too large growth of the callers function body\n-   or stack frame size.  *REASON if non-NULL is updated if the\n-   inlining is not a good idea.  */\n+/* Return false when inlining edge E would lead to violating\n+   limits on function unit growth or stack usage growth.  \n+\n+   The relative function body growth limit is present generally\n+   to avoid problems with non-linear behaviour of the compiler.\n+   To allow inlining huge functions into tiny wrapper, the limit\n+   is always based on the bigger of the two functions considered.\n+\n+   For stack growth limits we always base the growth in stack usage\n+   of the callers.  We want to prevent applications from segfaulting\n+   on stack overflow when functions with huge stack frames gets\n+   inlined. */\n \n static bool\n-cgraph_check_inline_limits (struct cgraph_edge *e,\n-\t\t\t    cgraph_inline_failed_t *reason)\n+caller_growth_limits (struct cgraph_edge *e)\n {\n   struct cgraph_node *to = e->caller;\n   struct cgraph_node *what = e->callee;\n   int newsize;\n-  int limit;\n-  HOST_WIDE_INT stack_size_limit, inlined_stack;\n-  struct inline_summary *info, *what_info;\n-\n-  if (to->global.inlined_to)\n-    to = to->global.inlined_to;\n+  int limit = 0;\n+  HOST_WIDE_INT stack_size_limit = 0, inlined_stack;\n+  struct inline_summary *info, *what_info, *outer_info = inline_summary (to);\n+\n+  /* Look for function e->caller is inlined to.  While doing\n+     so work out the largest function body on the way.  As\n+     described above, we want to base our function growth\n+     limits based on that.  Not on the self size of the\n+     outer function, not on the self size of inline code\n+     we immediately inline to.  This is the most relaxed\n+     interpretation of the rule \"do not grow large functions\n+     too much in order to prevent compiler from exploding\".  */\n+  do\n+    {\n+      info = inline_summary (to);\n+      if (limit < info->self_size)\n+\tlimit = info->self_size;\n+      if (stack_size_limit < info->estimated_self_stack_size)\n+\tstack_size_limit = info->estimated_self_stack_size;\n+      if (to->global.inlined_to)\n+        to = to->callers->caller;\n+    }\n+  while (to->global.inlined_to);\n \n-  info = inline_summary (to);\n   what_info = inline_summary (what);\n \n-  /* When inlining large function body called once into small function,\n-     take the inlined function as base for limiting the growth.  */\n-  if (info->self_size > what_info->self_size)\n-    limit = info->self_size;\n-  else\n+  if (limit < what_info->self_size)\n     limit = what_info->self_size;\n \n   limit += limit * PARAM_VALUE (PARAM_LARGE_FUNCTION_GROWTH) / 100;\n@@ -310,79 +342,421 @@ cgraph_check_inline_limits (struct cgraph_edge *e,\n       && newsize > PARAM_VALUE (PARAM_LARGE_FUNCTION_INSNS)\n       && newsize > limit)\n     {\n-      if (reason)\n-        *reason = CIF_LARGE_FUNCTION_GROWTH_LIMIT;\n+      e->inline_failed = CIF_LARGE_FUNCTION_GROWTH_LIMIT;\n       return false;\n     }\n \n-  stack_size_limit = info->estimated_self_stack_size;\n+  /* FIXME: Stack size limit often prevents inlining in fortran programs\n+     due to large i/o datastructures used by the fortran frontend.\n+     We ought to ignore this limit when we know that the edge is executed\n+     on every invocation of the caller (i.e. its call statement dominates\n+     exit block).  We do not track this information, yet.  */\n+  stack_size_limit += (stack_size_limit\n+\t\t       * PARAM_VALUE (PARAM_STACK_FRAME_GROWTH) / 100);\n \n-  stack_size_limit += stack_size_limit * PARAM_VALUE (PARAM_STACK_FRAME_GROWTH) / 100;\n-\n-  inlined_stack = (info->stack_frame_offset\n-\t\t   + info->estimated_self_stack_size\n+  inlined_stack = (outer_info->stack_frame_offset\n+\t\t   + outer_info->estimated_self_stack_size\n \t\t   + what_info->estimated_stack_size);\n-  if (inlined_stack  > stack_size_limit\n+  /* Check new stack consumption with stack consumption at the place\n+     stack is used.  */\n+  if (inlined_stack > stack_size_limit\n+      /* If function already has large stack usage from sibbling\n+\t inline call, we can inline, too.\n+\t This bit overoptimistically assume that we are good at stack\n+\t packing.  */\n+      && inlined_stack > info->estimated_stack_size\n       && inlined_stack > PARAM_VALUE (PARAM_LARGE_STACK_FRAME))\n     {\n-      if (reason)\n-        *reason = CIF_LARGE_STACK_FRAME_GROWTH_LIMIT;\n+      e->inline_failed = CIF_LARGE_STACK_FRAME_GROWTH_LIMIT;\n       return false;\n     }\n   return true;\n }\n \n-/* Return true when function N is small enough to be inlined.  */\n+/* Dump info about why inlining has failed.  */\n+\n+static void\n+report_inline_failed_reason (struct cgraph_edge *e)\n+{\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"  not inlinable: %s/%i -> %s/%i, %s\\n\",\n+\t       cgraph_node_name (e->caller), e->caller->uid,\n+\t       cgraph_node_name (e->callee), e->callee->uid,\n+\t       cgraph_inline_failed_string (e->inline_failed));\n+    }\n+}\n+\n+/* Decide if we can inline the edge and possibly update\n+   inline_failed reason.  \n+   We check whether inlining is possible at all and whether\n+   caller growth limits allow doing so.  \n+\n+   if REPORT is true, output reason to the dump file.  */\n \n static bool\n-cgraph_default_inline_p (struct cgraph_node *n, cgraph_inline_failed_t *reason)\n+can_inline_edge_p (struct cgraph_edge *e, bool report)\n {\n-  tree decl = n->decl;\n-  struct inline_summary *info = inline_summary (n);\n+  bool inlinable = true;\n+  tree caller_tree = DECL_FUNCTION_SPECIFIC_OPTIMIZATION (e->caller->decl);\n+  tree callee_tree = DECL_FUNCTION_SPECIFIC_OPTIMIZATION (e->callee->decl);\n \n-  if (info->disregard_inline_limits)\n-    return true;\n+  gcc_assert (e->inline_failed);\n \n-  if (!flag_inline_small_functions && !DECL_DECLARED_INLINE_P (decl))\n+  if (!e->callee->analyzed)\n+    {\n+      e->inline_failed = CIF_BODY_NOT_AVAILABLE;\n+      inlinable = false;\n+    }\n+  else if (!inline_summary (e->callee)->inlinable)\n+    {\n+      e->inline_failed = CIF_FUNCTION_NOT_INLINABLE;\n+      inlinable = false;\n+    }\n+  else if (cgraph_function_body_availability (e->callee) <= AVAIL_OVERWRITABLE)\n     {\n-      if (reason)\n-\t*reason = CIF_FUNCTION_NOT_INLINE_CANDIDATE;\n+      e->inline_failed = CIF_OVERWRITABLE;\n       return false;\n     }\n-  if (!n->analyzed)\n+  else if (e->call_stmt_cannot_inline_p)\n+    {\n+      e->inline_failed = CIF_MISMATCHED_ARGUMENTS;\n+      inlinable = false;\n+    }\n+  /* Don't inline if the functions have different EH personalities.  */\n+  else if (DECL_FUNCTION_PERSONALITY (e->caller->decl)\n+\t   && DECL_FUNCTION_PERSONALITY (e->callee->decl)\n+\t   && (DECL_FUNCTION_PERSONALITY (e->caller->decl)\n+\t       != DECL_FUNCTION_PERSONALITY (e->callee->decl)))\n+    {\n+      e->inline_failed = CIF_EH_PERSONALITY;\n+      inlinable = false;\n+    }\n+  /* Don't inline if the callee can throw non-call exceptions but the\n+     caller cannot.\n+     FIXME: this is obviously wrong for LTO where STRUCT_FUNCTION is missing.\n+     Move the flag into cgraph node or mirror it in the inline summary.  */\n+  else if (DECL_STRUCT_FUNCTION (e->callee->decl)\n+\t   && DECL_STRUCT_FUNCTION (e->callee->decl)->can_throw_non_call_exceptions\n+\t   && !(DECL_STRUCT_FUNCTION (e->caller->decl)\n+\t        && DECL_STRUCT_FUNCTION (e->caller->decl)->can_throw_non_call_exceptions))\n+    {\n+      e->inline_failed = CIF_NON_CALL_EXCEPTIONS;\n+      inlinable = false;\n+    }\n+  /* Check compatibility of target optimizatio noptions.  */\n+  else if (!targetm.target_option.can_inline_p (e->caller->decl,\n+\t\t\t\t\t\te->callee->decl))\n+    {\n+      e->inline_failed = CIF_TARGET_OPTION_MISMATCH;\n+      inlinable = false;\n+    }\n+  /* Check if caller growth allows the inlining.  */\n+  else if (!DECL_DISREGARD_INLINE_LIMITS (e->callee->decl)\n+           && !caller_growth_limits (e))\n+    inlinable = false;\n+  /* Don't inline a function with a higher optimization level than the\n+     caller.  FIXME: this is really just tip of iceberg of handling\n+     optimization attribute.  */\n+  else if (caller_tree != callee_tree)\n     {\n-      if (reason)\n-\t*reason = CIF_BODY_NOT_AVAILABLE;\n+      struct cl_optimization *caller_opt\n+\t= TREE_OPTIMIZATION ((caller_tree)\n+\t\t\t     ? caller_tree\n+\t\t\t     : optimization_default_node);\n+\n+      struct cl_optimization *callee_opt\n+\t= TREE_OPTIMIZATION ((callee_tree)\n+\t\t\t     ? callee_tree\n+\t\t\t     : optimization_default_node);\n+\n+      if ((caller_opt->x_optimize > callee_opt->x_optimize)\n+\t  || (caller_opt->x_optimize_size != callee_opt->x_optimize_size))\n+\t{\n+          e->inline_failed = CIF_TARGET_OPTIMIZATION_MISMATCH;\n+\t  inlinable = false;\n+\t}\n+    }\n+\n+  /* Be sure that the cannot_inline_p flag is up to date.  */\n+  gcc_checking_assert (!e->call_stmt\n+\t\t       || (gimple_call_cannot_inline_p (e->call_stmt)\n+\t\t           == e->call_stmt_cannot_inline_p)\n+\t\t       /* In -flto-partition=none mode we really keep things out of\n+\t\t\t  sync because call_stmt_cannot_inline_p is set at cgraph\n+\t\t\t  merging when function bodies are not there yet.  */\n+\t\t       || (in_lto_p && !gimple_call_cannot_inline_p (e->call_stmt)));\n+  if (!inlinable && report)\n+    report_inline_failed_reason (e);\n+  return inlinable;\n+}\n+\n+\n+/* Return true if the edge E is inlinable during early inlining.  */\n+\n+static bool\n+can_early_inline_edge_p (struct cgraph_edge *e)\n+{\n+  /* Early inliner might get called at WPA stage when IPA pass adds new\n+     function.  In this case we can not really do any of early inlining\n+     because function bodies are missing.  */\n+  if (!gimple_has_body_p (e->callee->decl))\n+    {\n+      e->inline_failed = CIF_BODY_NOT_AVAILABLE;\n       return false;\n     }\n-  if (cgraph_function_body_availability (n) <= AVAIL_OVERWRITABLE)\n+  /* In early inliner some of callees may not be in SSA form yet\n+     (i.e. the callgraph is cyclic and we did not process\n+     the callee by early inliner, yet).  We don't have CIF code for this\n+     case; later we will re-do the decision in the real inliner.  */\n+  if (!gimple_in_ssa_p (DECL_STRUCT_FUNCTION (e->caller->decl))\n+      || !gimple_in_ssa_p (DECL_STRUCT_FUNCTION (e->callee->decl)))\n     {\n-      if (reason)\n-\t*reason = CIF_OVERWRITABLE;\n+      if (dump_file)\n+\tfprintf (dump_file, \"  edge not inlinable: not in SSA form\\n\");\n       return false;\n     }\n+  if (!can_inline_edge_p (e, true))\n+    return false;\n+  return true;\n+}\n+\n+\n+/* Return true when N is leaf function.  Accept cheap builtins\n+   in leaf functions.  */\n+\n+static bool\n+leaf_node_p (struct cgraph_node *n)\n+{\n+  struct cgraph_edge *e;\n+  for (e = n->callees; e; e = e->next_callee)\n+    if (!is_inexpensive_builtin (e->callee->decl))\n+      return false;\n+  return true;\n+}\n+\n \n+/* Return true if we are interested in inlining small function.  */\n \n-  if (DECL_DECLARED_INLINE_P (decl))\n+static bool\n+want_early_inline_function_p (struct cgraph_edge *e)\n+{\n+  bool want_inline = true;\n+\n+  if (DECL_DISREGARD_INLINE_LIMITS (e->callee->decl))\n+    ;\n+  else if (!DECL_DECLARED_INLINE_P (e->callee->decl)\n+\t   && !flag_inline_small_functions)\n+    {\n+      e->inline_failed = CIF_FUNCTION_NOT_INLINE_CANDIDATE;\n+      report_inline_failed_reason (e);\n+      want_inline = false;\n+    }\n+  else\n     {\n-      if (info->size >= MAX_INLINE_INSNS_SINGLE)\n+      int growth = estimate_edge_growth (e);\n+      if (growth <= 0)\n+\t;\n+      else if (!cgraph_maybe_hot_edge_p (e)\n+\t       && growth > 0)\n+\t{\n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"  will not early inline: %s/%i->%s/%i, \"\n+\t\t     \"call is cold and code would grow by %i\\n\",\n+\t\t     cgraph_node_name (e->caller), e->caller->uid,\n+\t\t     cgraph_node_name (e->callee), e->callee->uid,\n+\t\t     growth);\n+\t  want_inline = false;\n+\t}\n+      else if (!leaf_node_p (e->callee)\n+\t       && growth > 0)\n \t{\n-\t  if (reason)\n-\t    *reason = CIF_MAX_INLINE_INSNS_SINGLE_LIMIT;\n-\t  return false;\n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"  will not early inline: %s/%i->%s/%i, \"\n+\t\t     \"callee is not leaf and code would grow by %i\\n\",\n+\t\t     cgraph_node_name (e->caller), e->caller->uid,\n+\t\t     cgraph_node_name (e->callee), e->callee->uid,\n+\t\t     growth);\n+\t  want_inline = false;\n \t}\n+      else if (growth > PARAM_VALUE (PARAM_EARLY_INLINING_INSNS))\n+\t{\n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"  will not early inline: %s/%i->%s/%i, \"\n+\t\t     \"growth %i exceeds --param early-inlining-insns\\n\",\n+\t\t     cgraph_node_name (e->caller), e->caller->uid,\n+\t\t     cgraph_node_name (e->callee), e->callee->uid,\n+\t\t     growth);\n+\t  want_inline = false;\n+\t}\n+    }\n+  return want_inline;\n+}\n+\n+/* Return true if we are interested in inlining small function.\n+   When REPORT is true, report reason to dump file.  */\n+\n+static bool\n+want_inline_small_function_p (struct cgraph_edge *e, bool report)\n+{\n+  bool want_inline = true;\n+\n+  if (DECL_DISREGARD_INLINE_LIMITS (e->callee->decl))\n+    ;\n+  else if (!DECL_DECLARED_INLINE_P (e->callee->decl)\n+\t   && !flag_inline_small_functions)\n+    {\n+      e->inline_failed = CIF_FUNCTION_NOT_INLINE_CANDIDATE;\n+      want_inline = false;\n     }\n   else\n     {\n-      if (info->size >= MAX_INLINE_INSNS_AUTO)\n+      int growth = estimate_edge_growth (e);\n+\n+      if (growth <= 0)\n+\t;\n+      else if (DECL_DECLARED_INLINE_P (e->callee->decl)\n+\t       && growth >= MAX_INLINE_INSNS_SINGLE)\n+\t{\n+          e->inline_failed = CIF_MAX_INLINE_INSNS_SINGLE_LIMIT;\n+\t  want_inline = false;\n+\t}\n+      else if (!DECL_DECLARED_INLINE_P (e->callee->decl)\n+\t       && !flag_inline_functions)\n+\t{\n+          e->inline_failed = CIF_NOT_DECLARED_INLINED;\n+\t  want_inline = false;\n+\t}\n+      else if (!DECL_DECLARED_INLINE_P (e->callee->decl)\n+\t       && growth >= MAX_INLINE_INSNS_AUTO)\n+\t{\n+          e->inline_failed = CIF_MAX_INLINE_INSNS_AUTO_LIMIT;\n+\t  want_inline = false;\n+\t}\n+      else if (!cgraph_maybe_hot_edge_p (e)\n+\t       && estimate_growth (e->callee) > 0)\n \t{\n-\t  if (reason)\n-\t    *reason = CIF_MAX_INLINE_INSNS_AUTO_LIMIT;\n-\t  return false;\n+          e->inline_failed = CIF_UNLIKELY_CALL;\n+\t  want_inline = false;\n \t}\n     }\n+  if (!want_inline && report)\n+    report_inline_failed_reason (e);\n+  return want_inline;\n+}\n \n-  return true;\n+/* EDGE is self recursive edge.\n+   We hand two cases - when function A is inlining into itself\n+   or when function A is being inlined into another inliner copy of function\n+   A within function B.  \n+\n+   In first case OUTER_NODE points to the toplevel copy of A, while\n+   in the second case OUTER_NODE points to the outermost copy of A in B.\n+\n+   In both cases we want to be extra selective since\n+   inlining the call will just introduce new recursive calls to appear.  */\n+static bool\n+want_inline_self_recursive_call_p (struct cgraph_edge *edge,\n+\t\t\t\t   struct cgraph_node *outer_node,\n+\t\t\t\t   bool peeling,\n+\t\t\t\t   int depth)\n+{\n+  char const *reason = NULL;\n+  bool want_inline = true;\n+  int caller_freq = CGRAPH_FREQ_BASE;\n+  int max_depth = PARAM_VALUE (PARAM_MAX_INLINE_RECURSIVE_DEPTH_AUTO);\n+\n+  if (DECL_DECLARED_INLINE_P (edge->callee->decl))\n+    max_depth = PARAM_VALUE (PARAM_MAX_INLINE_RECURSIVE_DEPTH);\n+\n+  if (!cgraph_maybe_hot_edge_p (edge))\n+    {\n+      reason = \"recursive call is cold\";\n+      want_inline = false;\n+    }\n+  else if (max_count && !outer_node->count)\n+    {\n+      reason = \"not executed in profile\";\n+      want_inline = false;\n+    }\n+  else if (depth > max_depth)\n+    {\n+      reason = \"--param max-inline-recursive-depth exceeded.\";\n+      want_inline = false;\n+    }\n+\n+  if (outer_node->global.inlined_to)\n+    caller_freq = outer_node->callers->frequency;\n+\n+  if (!want_inline)\n+    ;\n+  /* Inlining of self recursive function into copy of itself within other function\n+     is transformation similar to loop peeling.\n+\n+     Peeling is profitable if we can inline enough copies to make probablility\n+     of actual call to the self recursive function very small.  Be sure that\n+     the probability of recursion is small.\n+\n+     We ensure that the frequency of recusing is at most 1 - (1/max_depth).\n+     This way the expected number of recusion is at most max_depth.  */\n+  else if (peeling)\n+    {\n+      int max_prob = CGRAPH_FREQ_BASE - ((CGRAPH_FREQ_BASE + max_depth - 1)\n+\t\t\t\t\t / max_depth);\n+      int i;\n+      for (i = 1; i < depth; i++)\n+\tmax_prob = max_prob * max_prob / CGRAPH_FREQ_BASE;\n+      if (max_count\n+\t  && (edge->count * CGRAPH_FREQ_BASE / outer_node->count\n+\t      >= max_prob))\n+\t{\n+\t  reason = \"profile of recursive call is too large\";\n+\t  want_inline = false;\n+\t}\n+      if (!max_count\n+\t  && (edge->frequency * CGRAPH_FREQ_BASE / caller_freq\n+\t      >= max_prob))\n+\t{\n+\t  reason = \"frequency of recursive call is too large\";\n+\t  want_inline = false;\n+\t}\n+    }\n+  /* Recusive inlining, i.e. equivalent of unrolling, is profitable if recusion\n+     depth is large.  We reduce function call overhead and increase chances that\n+     things fit in hardware return predictor.\n+\n+     Recursive inlining might however increase cost of stack frame setup\n+     actually slowing down functions whose recursion tree is wide rather than\n+     deep.\n+\n+     Deciding reliably on when to do recursive inlining withthout profile feedback\n+     is tricky.  For now we disable recursive inlining when probability of self\n+     recursion is low. \n+\n+     Recursive inlining of self recursive call within loop also results in large loop\n+     depths that generally optimize badly.  We may want to throttle down inlining\n+     in those cases.  In particular this seems to happen in one of libstdc++ rb tree\n+     methods.  */\n+  else\n+    {\n+      if (max_count\n+\t  && (edge->count * 100 / outer_node->count\n+\t      <= PARAM_VALUE (PARAM_MIN_INLINE_RECURSIVE_PROBABILITY)))\n+\t{\n+\t  reason = \"profile of recursive call is too small\";\n+\t  want_inline = false;\n+\t}\n+      else if (!max_count\n+\t       && (edge->frequency * 100 / caller_freq\n+\t           <= PARAM_VALUE (PARAM_MIN_INLINE_RECURSIVE_PROBABILITY)))\n+\t{\n+\t  reason = \"frequency of recursive call is too small\";\n+\t  want_inline = false;\n+\t}\n+    }\n+  if (!want_inline && dump_file)\n+    fprintf (dump_file, \"   not inlining recursively: %s\\n\", reason);\n+  return want_inline;\n }\n \n /* A cost model driving the inlining heuristics in a way so the edges with\n@@ -392,13 +766,13 @@ cgraph_default_inline_p (struct cgraph_node *n, cgraph_inline_failed_t *reason)\n    of the function or function body size.  */\n \n static int\n-cgraph_edge_badness (struct cgraph_edge *edge, bool dump)\n+edge_badness (struct cgraph_edge *edge, bool dump)\n {\n   gcov_type badness;\n   int growth;\n   struct inline_summary *callee_info = inline_summary (edge->callee);\n \n-  if (callee_info->disregard_inline_limits)\n+  if (DECL_DISREGARD_INLINE_LIMITS (edge->callee->decl))\n     return INT_MIN;\n \n   growth = estimate_edge_growth (edge);\n@@ -488,7 +862,8 @@ cgraph_edge_badness (struct cgraph_edge *edge, bool dump)\n \t  fprintf (dump_file,\n \t\t   \"      %i: guessed profile. frequency %i, overall growth %i,\"\n \t\t   \" benefit %i%%, divisor %i\\n\",\n-\t\t   (int) badness, edge->frequency, growth_for_all, benefitperc, div);\n+\t\t   (int) badness, edge->frequency, growth_for_all,\n+\t\t   benefitperc, div);\n \t}\n     }\n   /* When function local profile is not available or it does not give\n@@ -523,10 +898,10 @@ cgraph_edge_badness (struct cgraph_edge *edge, bool dump)\n }\n \n /* Recompute badness of EDGE and update its key in HEAP if needed.  */\n-static void\n+static inline void\n update_edge_key (fibheap_t heap, struct cgraph_edge *edge)\n {\n-  int badness = cgraph_edge_badness (edge, false);\n+  int badness = edge_badness (edge, false);\n   if (edge->aux)\n     {\n       fibnode_t n = (fibnode_t) edge->aux;\n@@ -539,11 +914,30 @@ update_edge_key (fibheap_t heap, struct cgraph_edge *edge)\n       if (badness < n->key)\n \t{\n \t  fibheap_replace_key (heap, n, badness);\n+\t  if (dump_file && (dump_flags & TDF_DETAILS))\n+\t    {\n+\t      fprintf (dump_file,\n+\t\t       \"  decreasing badness %s/%i -> %s/%i, %i to %i\\n\",\n+\t\t       cgraph_node_name (edge->caller), edge->caller->uid,\n+\t\t       cgraph_node_name (edge->callee), edge->callee->uid,\n+\t\t       (int)n->key,\n+\t\t       badness);\n+\t    }\n \t  gcc_checking_assert (n->key == badness);\n \t}\n     }\n   else\n-    edge->aux = fibheap_insert (heap, badness, edge);\n+    {\n+       if (dump_file && (dump_flags & TDF_DETAILS))\n+\t {\n+\t   fprintf (dump_file,\n+\t\t    \"  enqueuing call %s/%i -> %s/%i, badness %i\\n\",\n+\t\t    cgraph_node_name (edge->caller), edge->caller->uid,\n+\t\t    cgraph_node_name (edge->callee), edge->callee->uid,\n+\t\t    badness);\n+\t }\n+      edge->aux = fibheap_insert (heap, badness, edge);\n+    }\n }\n \n /* Recompute heap nodes for each of caller edge.  */\n@@ -553,7 +947,6 @@ update_caller_keys (fibheap_t heap, struct cgraph_node *node,\n \t\t    bitmap updated_nodes)\n {\n   struct cgraph_edge *edge;\n-  cgraph_inline_failed_t failed_reason;\n \n   if (!inline_summary (node)->inlinable\n       || cgraph_function_body_availability (node) <= AVAIL_OVERWRITABLE\n@@ -569,23 +962,20 @@ update_caller_keys (fibheap_t heap, struct cgraph_node *node,\n       break;\n   if (!edge)\n     return;\n-  /* Prune out edges we won't inline into anymore.  */\n-  if (!cgraph_default_inline_p (node, &failed_reason))\n-    {\n-      for (; edge; edge = edge->next_caller)\n-\tif (edge->aux)\n+\n+  for (; edge; edge = edge->next_caller)\n+    if (edge->inline_failed)\n+      {\n+\tif (can_inline_edge_p (edge, false)\n+\t    && want_inline_small_function_p (edge, false))\n+          update_edge_key (heap, edge);\n+\telse if (edge->aux)\n \t  {\n+\t    report_inline_failed_reason (edge);\n \t    fibheap_delete_node (heap, (fibnode_t) edge->aux);\n \t    edge->aux = NULL;\n-\t    if (edge->inline_failed)\n-\t      edge->inline_failed = failed_reason;\n \t  }\n-      return;\n-    }\n-\n-  for (; edge; edge = edge->next_caller)\n-    if (edge->inline_failed)\n-      update_edge_key (heap, edge);\n+      }\n }\n \n /* Recompute heap nodes for each uninlined call.\n@@ -613,12 +1003,7 @@ update_callee_keys (fibheap_t heap, struct cgraph_node *node,\n \t    && !bitmap_bit_p (updated_nodes, e->callee->uid))\n \t  {\n \t    inline_summary (node)->estimated_growth = INT_MIN;\n-\t    /* If function becomes uninlinable, we need to remove it from the heap.  */\n-\t    if (!cgraph_default_inline_p (e->callee, &e->inline_failed))\n-\t      update_caller_keys (heap, e->callee, updated_nodes);\n-\t    else\n-\t    /* Otherwise update just edge E.  */\n-\t      update_edge_key (heap, e);\n+\t    update_edge_key (heap, e);\n \t  }\n \tif (e->next_callee)\n \t  e = e->next_callee;\n@@ -702,42 +1087,26 @@ lookup_recursive_calls (struct cgraph_node *node, struct cgraph_node *where,\n    is NULL.  */\n \n static bool\n-cgraph_decide_recursive_inlining (struct cgraph_edge *edge,\n-\t\t\t\t  VEC (cgraph_edge_p, heap) **new_edges)\n+recursive_inlining (struct cgraph_edge *edge,\n+\t\t    VEC (cgraph_edge_p, heap) **new_edges)\n {\n   int limit = PARAM_VALUE (PARAM_MAX_INLINE_INSNS_RECURSIVE_AUTO);\n-  int max_depth = PARAM_VALUE (PARAM_MAX_INLINE_RECURSIVE_DEPTH_AUTO);\n-  int probability = PARAM_VALUE (PARAM_MIN_INLINE_RECURSIVE_PROBABILITY);\n   fibheap_t heap;\n   struct cgraph_node *node;\n   struct cgraph_edge *e;\n-  struct cgraph_node *master_clone, *next;\n+  struct cgraph_node *master_clone = NULL, *next;\n   int depth = 0;\n   int n = 0;\n \n   node = edge->caller;\n   if (node->global.inlined_to)\n     node = node->global.inlined_to;\n \n-  /* It does not make sense to recursively inline always-inline functions\n-     as we are going to sorry() on the remaining calls anyway.  */\n-  if (inline_summary (node)->disregard_inline_limits\n-      && lookup_attribute (\"always_inline\", DECL_ATTRIBUTES (node->decl)))\n-    return false;\n-\n-  if (optimize_function_for_size_p (DECL_STRUCT_FUNCTION (node->decl))\n-      || (!flag_inline_functions && !DECL_DECLARED_INLINE_P (node->decl)))\n-    return false;\n-\n   if (DECL_DECLARED_INLINE_P (node->decl))\n-    {\n-      limit = PARAM_VALUE (PARAM_MAX_INLINE_INSNS_RECURSIVE);\n-      max_depth = PARAM_VALUE (PARAM_MAX_INLINE_RECURSIVE_DEPTH);\n-    }\n+    limit = PARAM_VALUE (PARAM_MAX_INLINE_INSNS_RECURSIVE);\n \n   /* Make sure that function is small enough to be considered for inlining.  */\n-  if (!max_depth\n-      || estimate_size_after_inlining (node, edge)  >= limit)\n+  if (estimate_size_after_inlining (node, edge)  >= limit)\n     return false;\n   heap = fibheap_new ();\n   lookup_recursive_calls (node, node, heap);\n@@ -752,14 +1121,6 @@ cgraph_decide_recursive_inlining (struct cgraph_edge *edge,\n \t     \"  Performing recursive inlining on %s\\n\",\n \t     cgraph_node_name (node));\n \n-  /* We need original clone to copy around.  */\n-  master_clone = cgraph_clone_node (node, node->decl,\n-\t\t\t\t    node->count, CGRAPH_FREQ_BASE, 1,\n-  \t\t\t\t    false, NULL);\n-  for (e = master_clone->callees; e; e = e->next_callee)\n-    if (!e->inline_failed)\n-      cgraph_clone_inlined_nodes (e, true, false);\n-\n   /* Do the inlining and update list of recursive call during process.  */\n   while (!fibheap_empty (heap))\n     {\n@@ -770,35 +1131,17 @@ cgraph_decide_recursive_inlining (struct cgraph_edge *edge,\n       if (estimate_size_after_inlining (node, curr) > limit)\n \tbreak;\n \n+      if (!can_inline_edge_p (curr, true))\n+\tcontinue;\n+\n       depth = 1;\n       for (cnode = curr->caller;\n \t   cnode->global.inlined_to; cnode = cnode->callers->caller)\n \tif (node->decl == curr->callee->decl)\n \t  depth++;\n-      if (depth > max_depth)\n-\t{\n-          if (dump_file)\n-\t    fprintf (dump_file,\n-\t\t     \"   maximal depth reached\\n\");\n-\t  continue;\n-\t}\n \n-      if (max_count)\n-\t{\n-          if (!cgraph_maybe_hot_edge_p (curr))\n-\t    {\n-\t      if (dump_file)\n-\t\tfprintf (dump_file, \"   Not inlining cold call\\n\");\n-\t      continue;\n-\t    }\n-          if (curr->count * 100 / node->count < probability)\n-\t    {\n-\t      if (dump_file)\n-\t\tfprintf (dump_file,\n-\t\t\t \"   Probability of edge is too small\\n\");\n-\t      continue;\n-\t    }\n-\t}\n+      if (!want_inline_self_recursive_call_p (curr, node, false, depth))\n+\tcontinue;\n \n       if (dump_file)\n \t{\n@@ -811,18 +1154,34 @@ cgraph_decide_recursive_inlining (struct cgraph_edge *edge,\n \t    }\n \t  fprintf (dump_file, \"\\n\");\n \t}\n+      if (!master_clone)\n+\t{\n+\t  /* We need original clone to copy around.  */\n+\t  master_clone = cgraph_clone_node (node, node->decl,\n+\t\t\t\t\t    node->count, CGRAPH_FREQ_BASE, 1,\n+\t\t\t\t\t    false, NULL);\n+\t  for (e = master_clone->callees; e; e = e->next_callee)\n+\t    if (!e->inline_failed)\n+\t      cgraph_clone_inlined_nodes (e, true, false);\n+\t}\n+\n       cgraph_redirect_edge_callee (curr, master_clone);\n       cgraph_mark_inline_edge (curr, false, new_edges);\n       lookup_recursive_calls (node, curr->callee, heap);\n       n++;\n     }\n+\n   if (!fibheap_empty (heap) && dump_file)\n     fprintf (dump_file, \"    Recursive inlining growth limit met.\\n\");\n-\n   fibheap_delete (heap);\n+\n+  if (!master_clone)\n+    return false;\n+\n   if (dump_file)\n     fprintf (dump_file,\n-\t     \"\\n   Inlined %i times, body grown from size %i to %i, time %i to %i\\n\", n,\n+\t     \"\\n   Inlined %i times, \"\n+\t     \"body grown from size %i to %i, time %i to %i\\n\", n,\n \t     inline_summary (master_clone)->size, inline_summary (node)->size,\n \t     inline_summary (master_clone)->time, inline_summary (node)->time);\n \n@@ -837,27 +1196,7 @@ cgraph_decide_recursive_inlining (struct cgraph_edge *edge,\n \tcgraph_remove_node (node);\n     }\n   cgraph_remove_node (master_clone);\n-  /* FIXME: Recursive inlining actually reduces number of calls of the\n-     function.  At this place we should probably walk the function and\n-     inline clones and compensate the counts accordingly.  This probably\n-     doesn't matter much in practice.  */\n-  return n > 0;\n-}\n-\n-/* Set inline_failed for all callers of given function to REASON.  */\n-\n-static void\n-cgraph_set_inline_failed (struct cgraph_node *node,\n-\t\t\t  cgraph_inline_failed_t reason)\n-{\n-  struct cgraph_edge *e;\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Inlining failed: %s\\n\",\n-\t     cgraph_inline_failed_string (reason));\n-  for (e = node->callers; e; e = e->next_caller)\n-    if (e->inline_failed)\n-      e->inline_failed = reason;\n+  return true;\n }\n \n /* Given whole compilation unit estimate of INSNS, compute how large we can\n@@ -884,8 +1223,9 @@ add_new_edges_to_heap (fibheap_t heap, VEC (cgraph_edge_p, heap) *new_edges)\n       gcc_assert (!edge->aux);\n       if (inline_summary (edge->callee)->inlinable\n \t  && edge->inline_failed\n-\t  && cgraph_default_inline_p (edge->callee, &edge->inline_failed))\n-        edge->aux = fibheap_insert (heap, cgraph_edge_badness (edge, false), edge);\n+\t  && can_inline_edge_p (edge, true)\n+\t  && want_inline_small_function_p (edge, true))\n+        edge->aux = fibheap_insert (heap, edge_badness (edge, false), edge);\n     }\n }\n \n@@ -898,11 +1238,10 @@ add_new_edges_to_heap (fibheap_t heap, VEC (cgraph_edge_p, heap) *new_edges)\n    to be passed to cgraph_inlined_into and cgraph_inlined_callees.  */\n \n static void\n-cgraph_decide_inlining_of_small_functions (void)\n+inline_small_functions (void)\n {\n   struct cgraph_node *node;\n   struct cgraph_edge *edge;\n-  cgraph_inline_failed_t failed_reason;\n   fibheap_t heap = fibheap_new ();\n   bitmap updated_nodes = BITMAP_ALLOC (NULL);\n   int min_size, max_size;\n@@ -921,31 +1260,20 @@ cgraph_decide_inlining_of_small_functions (void)\n       {\n \tstruct inline_summary *info = inline_summary (node);\n \n-\tif (!info->inlinable || !node->callers)\n-\t  {\n-\t    struct cgraph_edge *e;\n-\t    for (e = node->callers; e; e = e->next_caller)\n-\t      {\n-\t\tgcc_assert (e->inline_failed);\n-\t\te->inline_failed = CIF_FUNCTION_NOT_INLINABLE;\n-\t      }\n-\t    continue;\n-\t  }\n \tif (dump_file)\n-\t  fprintf (dump_file, \"Considering inline candidate %s.\\n\", cgraph_node_name (node));\n+\t  fprintf (dump_file, \"Enqueueing calls of %s/%i.\\n\",\n+\t\t   cgraph_node_name (node), node->uid);\n \n \tinfo->estimated_growth = INT_MIN;\n-\tif (!cgraph_default_inline_p (node, &failed_reason))\n-\t  {\n-\t    cgraph_set_inline_failed (node, failed_reason);\n-\t    continue;\n-\t  }\n \n \tfor (edge = node->callers; edge; edge = edge->next_caller)\n-\t  if (edge->inline_failed)\n+\t  if (edge->inline_failed\n+\t      && can_inline_edge_p (edge, true)\n+\t      && want_inline_small_function_p (edge, true)\n+\t      && edge->inline_failed)\n \t    {\n \t      gcc_assert (!edge->aux);\n-\t      edge->aux = fibheap_insert (heap, cgraph_edge_badness (edge, false), edge);\n+\t      update_edge_key (heap, edge);\n \t    }\n       }\n \n@@ -960,7 +1288,6 @@ cgraph_decide_inlining_of_small_functions (void)\n       int badness = fibheap_min_key (heap);\n       int current_badness;\n       int growth;\n-      cgraph_inline_failed_t not_good = CIF_OK;\n \n       edge = (struct cgraph_edge *) fibheap_extract_min (heap);\n       gcc_assert (edge->aux);\n@@ -971,13 +1298,16 @@ cgraph_decide_inlining_of_small_functions (void)\n       /* When updating the edge costs, we only decrease badness in the keys.\n \t When the badness increase, we keep the heap as it is and re-insert\n \t key now.  */\n-      current_badness = cgraph_edge_badness (edge, false);\n+      current_badness = edge_badness (edge, false);\n       gcc_assert (current_badness >= badness);\n       if (current_badness != badness)\n \t{\n \t  edge->aux = fibheap_insert (heap, current_badness, edge);\n \t  continue;\n \t}\n+\n+      if (!can_inline_edge_p (edge, true))\n+\tcontinue;\n       \n       callee = edge->callee;\n       growth = estimate_edge_growth (edge);\n@@ -989,81 +1319,32 @@ cgraph_decide_inlining_of_small_functions (void)\n \t\t   inline_summary (edge->callee)->size);\n \t  fprintf (dump_file,\n \t\t   \" to be inlined into %s in %s:%i\\n\"\n-\t\t   \" Estimated growth after inlined into all callees is %+i insns.\\n\"\n+\t\t   \" Estimated growth after inlined into all is %+i insns.\\n\"\n \t\t   \" Estimated badness is %i, frequency %.2f.\\n\",\n \t\t   cgraph_node_name (edge->caller),\n \t\t   flag_wpa ? \"unknown\"\n \t\t   : gimple_filename ((const_gimple) edge->call_stmt),\n-\t\t   flag_wpa ? -1 : gimple_lineno ((const_gimple) edge->call_stmt),\n+\t\t   flag_wpa ? -1\n+\t\t   : gimple_lineno ((const_gimple) edge->call_stmt),\n \t\t   estimate_growth (edge->callee),\n \t\t   badness,\n \t\t   edge->frequency / (double)CGRAPH_FREQ_BASE);\n \t  if (edge->count)\n-\t    fprintf (dump_file,\" Called \"HOST_WIDEST_INT_PRINT_DEC\"x\\n\", edge->count);\n+\t    fprintf (dump_file,\" Called \"HOST_WIDEST_INT_PRINT_DEC\"x\\n\",\n+\t\t     edge->count);\n \t  if (dump_flags & TDF_DETAILS)\n-\t    cgraph_edge_badness (edge, true);\n-\t}\n-\n-      /* When not having profile info ready we don't weight by any way the\n-         position of call in procedure itself.  This means if call of\n-\t function A from function B seems profitable to inline, the recursive\n-\t call of function A in inline copy of A in B will look profitable too\n-\t and we end up inlining until reaching maximal function growth.  This\n-\t is not good idea so prohibit the recursive inlining.\n-\n-\t ??? When the frequencies are taken into account we might not need this\n-\t restriction.\n-\n-\t We need to be careful here, in some testcases, e.g. directives.c in\n-\t libcpp, we can estimate self recursive function to have negative growth\n-\t for inlining completely.\n-\t */\n-      if (!edge->count)\n-\t{\n-\t  where = edge->caller;\n-\t  while (where->global.inlined_to)\n-\t    {\n-\t      if (where->decl == edge->callee->decl)\n-\t\tbreak;\n-\t      where = where->callers->caller;\n-\t    }\n-\t  if (where->global.inlined_to)\n-\t    {\n-\t      edge->inline_failed\n-\t\t= (inline_summary (edge->callee)->disregard_inline_limits\n-\t\t   ? CIF_RECURSIVE_INLINING : CIF_UNSPECIFIED);\n-\t      if (dump_file)\n-\t\tfprintf (dump_file, \" inline_failed:Recursive inlining performed only for function itself.\\n\");\n-\t      continue;\n-\t    }\n+\t    edge_badness (edge, true);\n \t}\n \n-      if (inline_summary (edge->callee)->disregard_inline_limits)\n-\t;\n-      else if (!cgraph_maybe_hot_edge_p (edge))\n- \tnot_good = CIF_UNLIKELY_CALL;\n-      else if (!flag_inline_functions\n-\t  && !DECL_DECLARED_INLINE_P (edge->callee->decl))\n- \tnot_good = CIF_NOT_DECLARED_INLINED;\n-      else if (optimize_function_for_size_p (DECL_STRUCT_FUNCTION(edge->caller->decl)))\n- \tnot_good = CIF_OPTIMIZING_FOR_SIZE;\n-      if (not_good && growth > 0 && estimate_growth (edge->callee) > 0)\n-\t{\n-\t  edge->inline_failed = not_good;\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \" inline_failed:%s.\\n\",\n-\t\t     cgraph_inline_failed_string (edge->inline_failed));\n-\t  continue;\n-\t}\n-      if (!cgraph_default_inline_p (edge->callee, &edge->inline_failed))\n+      if (overall_size + growth > max_size\n+\t  && !DECL_DISREGARD_INLINE_LIMITS (edge->callee->decl))\n \t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \" inline_failed:%s.\\n\",\n-\t\t     cgraph_inline_failed_string (edge->inline_failed));\n+\t  edge->inline_failed = CIF_INLINE_UNIT_GROWTH_LIMIT;\n+\t  report_inline_failed_reason (edge);\n \t  continue;\n \t}\n-      if (!tree_can_inline_p (edge)\n-\t  || edge->call_stmt_cannot_inline_p)\n+\n+      if (!want_inline_small_function_p (edge, true))\n \t{\n \t  if (dump_file)\n \t    fprintf (dump_file, \" inline_failed:%s.\\n\",\n@@ -1075,9 +1356,9 @@ cgraph_decide_inlining_of_small_functions (void)\n \t  where = edge->caller;\n \t  if (where->global.inlined_to)\n \t    where = where->global.inlined_to;\n-\t  if (!cgraph_decide_recursive_inlining (edge,\n-\t\t\t\t\t\t flag_indirect_inlining\n-\t\t\t\t\t\t ? &new_indirect_edges : NULL))\n+\t  if (!recursive_inlining (edge,\n+\t\t\t\t   flag_indirect_inlining\n+\t\t\t\t   ? &new_indirect_edges : NULL))\n \t    {\n \t      edge->inline_failed = CIF_RECURSIVE_INLINING;\n \t      continue;\n@@ -1089,14 +1370,33 @@ cgraph_decide_inlining_of_small_functions (void)\n       else\n \t{\n \t  struct cgraph_node *callee;\n-\t  if (!cgraph_check_inline_limits (edge, &edge->inline_failed))\n+\t  struct cgraph_node *outer_node = NULL;\n+\t  int depth = 0;\n+\n+\t  /* Consider the case where self recursive function A is inlined into B.\n+\t     This is desired optimization in some cases, since it leads to effect\n+\t     similar of loop peeling and we might completely optimize out the\n+\t     recursive call.  However we must be extra selective.  */\n+\n+\t  where = edge->caller;\n+\t  while (where->global.inlined_to)\n \t    {\n-\t      if (dump_file)\n-\t\tfprintf (dump_file, \" Not inlining into %s:%s.\\n\",\n-\t\t\t cgraph_node_name (edge->caller),\n-\t\t\t cgraph_inline_failed_string (edge->inline_failed));\n+\t      if (where->decl == edge->callee->decl)\n+\t\touter_node = where, depth++;\n+\t      where = where->callers->caller;\n+\t    }\n+\t  if (outer_node\n+\t      && !want_inline_self_recursive_call_p (edge, outer_node,\n+\t\t\t\t\t\t     true, depth))\n+\t    {\n+\t      edge->inline_failed\n+\t\t= (DECL_DISREGARD_INLINE_LIMITS (edge->callee->decl)\n+\t\t   ? CIF_RECURSIVE_INLINING : CIF_UNSPECIFIED);\n \t      continue;\n \t    }\n+\t  else if (depth && dump_file)\n+\t    fprintf (dump_file, \" Peeling recursion with depth %i\\n\", depth);\n+\n \t  callee = edge->callee;\n \t  gcc_checking_assert (!callee->global.inlined_to);\n \t  cgraph_mark_inline_edge (edge, true, &new_indirect_edges);\n@@ -1148,43 +1448,6 @@ cgraph_decide_inlining_of_small_functions (void)\n \t    fprintf (dump_file, \"New minimal size reached: %i\\n\", min_size);\n \t}\n     }\n-  while (!fibheap_empty (heap))\n-    {\n-      int badness = fibheap_min_key (heap);\n-\n-      edge = (struct cgraph_edge *) fibheap_extract_min (heap);\n-      gcc_assert (edge->aux);\n-      edge->aux = NULL;\n-      if (!edge->inline_failed)\n-\tcontinue;\n-#ifdef ENABLE_CHECKING\n-      gcc_assert (cgraph_edge_badness (edge, false) >= badness);\n-#endif\n-      if (dump_file)\n-\t{\n-\t  fprintf (dump_file,\n-\t\t   \"\\nSkipping %s with %i size\\n\",\n-\t\t   cgraph_node_name (edge->callee),\n-\t\t   inline_summary (edge->callee)->size);\n-\t  fprintf (dump_file,\n-\t\t   \" called by %s in %s:%i\\n\"\n-\t\t   \" Estimated growth after inlined into all callees is %+i insns.\\n\"\n-\t\t   \" Estimated badness is %i, frequency %.2f.\\n\",\n-\t\t   cgraph_node_name (edge->caller),\n-\t\t   flag_wpa ? \"unknown\"\n-\t\t   : gimple_filename ((const_gimple) edge->call_stmt),\n-\t\t   flag_wpa ? -1 : gimple_lineno ((const_gimple) edge->call_stmt),\n-\t\t   estimate_growth (edge->callee),\n-\t\t   badness,\n-\t\t   edge->frequency / (double)CGRAPH_FREQ_BASE);\n-\t  if (edge->count)\n-\t    fprintf (dump_file,\" Called \"HOST_WIDEST_INT_PRINT_DEC\"x\\n\", edge->count);\n-\t  if (dump_flags & TDF_DETAILS)\n-\t    cgraph_edge_badness (edge, true);\n-\t}\n-      if (!inline_summary (edge->callee)->disregard_inline_limits && edge->inline_failed)\n-\tedge->inline_failed = CIF_INLINE_UNIT_GROWTH_LIMIT;\n-    }\n \n   if (new_indirect_edges)\n     VEC_free (cgraph_edge_p, heap, new_indirect_edges);\n@@ -1195,7 +1458,7 @@ cgraph_decide_inlining_of_small_functions (void)\n /* Flatten NODE from the IPA inliner.  */\n \n static void\n-cgraph_flatten (struct cgraph_node *node)\n+flatten_function (struct cgraph_node *node)\n {\n   struct cgraph_edge *e;\n \n@@ -1208,22 +1471,6 @@ cgraph_flatten (struct cgraph_node *node)\n     {\n       struct cgraph_node *orig_callee;\n \n-      if (e->call_stmt_cannot_inline_p)\n-\t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \"Not inlining: %s\",\n-\t\t     cgraph_inline_failed_string (e->inline_failed));\n-\t  continue;\n-\t}\n-\n-      if (!e->callee->analyzed)\n-\t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file,\n-\t\t     \"Not inlining: Function body not available.\\n\");\n-\t  continue;\n-\t}\n-\n       /* We've hit cycle?  It is time to give up.  */\n       if (e->callee->aux)\n \t{\n@@ -1240,25 +1487,25 @@ cgraph_flatten (struct cgraph_node *node)\n \t it in order to fully flatten the leaves.  */\n       if (!e->inline_failed)\n \t{\n-\t  cgraph_flatten (e->callee);\n+\t  flatten_function (e->callee);\n \t  continue;\n \t}\n \n+      /* Flatten attribute needs to be processed during late inlining. For\n+\t extra code quality we however do flattening during early optimization,\n+\t too.  */\n+      if (cgraph_state != CGRAPH_STATE_IPA_SSA\n+\t  ? !can_inline_edge_p (e, true)\n+\t  : !can_early_inline_edge_p (e))\n+\tcontinue;\n+\n       if (cgraph_edge_recursive_p (e))\n \t{\n \t  if (dump_file)\n \t    fprintf (dump_file, \"Not inlining: recursive call.\\n\");\n \t  continue;\n \t}\n \n-      if (!tree_can_inline_p (e))\n-\t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \"Not inlining: %s\",\n-\t\t     cgraph_inline_failed_string (e->inline_failed));\n-\t  continue;\n-\t}\n-\n       if (gimple_in_ssa_p (DECL_STRUCT_FUNCTION (node->decl))\n \t  != gimple_in_ssa_p (DECL_STRUCT_FUNCTION (e->callee->decl)))\n \t{\n@@ -1277,7 +1524,7 @@ cgraph_flatten (struct cgraph_node *node)\n       cgraph_mark_inline_edge (e, true, NULL);\n       if (e->callee != orig_callee)\n \torig_callee->aux = (void *) node;\n-      cgraph_flatten (e->callee);\n+      flatten_function (e->callee);\n       if (e->callee != orig_callee)\n \torig_callee->aux = NULL;\n     }\n@@ -1289,7 +1536,7 @@ cgraph_flatten (struct cgraph_node *node)\n    expenses on updating data structures.  */\n \n static unsigned int\n-cgraph_decide_inlining (void)\n+ipa_inline (void)\n {\n   struct cgraph_node *node;\n   int nnodes;\n@@ -1366,67 +1613,59 @@ cgraph_decide_inlining (void)\n \t  if (dump_file)\n \t    fprintf (dump_file,\n \t\t     \"Flattening %s\\n\", cgraph_node_name (node));\n-\t  cgraph_flatten (node);\n+\t  flatten_function (node);\n \t}\n     }\n \n-  cgraph_decide_inlining_of_small_functions ();\n+  inline_small_functions ();\n+  cgraph_remove_unreachable_nodes (true, dump_file);\n+  free (order);\n \n+  /* We already perform some inlining of functions called once during\n+     inlining small functions above.  After unreachable nodes are removed,\n+     we still might do a quick check that nothing new is found.  */\n   if (flag_inline_functions_called_once)\n     {\n       if (dump_file)\n \tfprintf (dump_file, \"\\nDeciding on functions called once:\\n\");\n \n       /* And finally decide what functions are called once.  */\n-      for (i = nnodes - 1; i >= 0; i--)\n+      for (node = cgraph_nodes; node; node = node->next)\n \t{\n-\t  node = order[i];\n-\n \t  if (node->callers\n \t      && !node->callers->next_caller\n \t      && !node->global.inlined_to\n-\t      && cgraph_will_be_removed_from_program_if_no_direct_calls (node)\n-\t      && inline_summary (node)->inlinable\n-\t      && cgraph_function_body_availability (node) >= AVAIL_AVAILABLE\n \t      && node->callers->inline_failed\n \t      && node->callers->caller != node\n \t      && node->callers->caller->global.inlined_to != node\n-\t      && !node->callers->call_stmt_cannot_inline_p\n-\t      && tree_can_inline_p (node->callers)\n-\t      && !DECL_EXTERNAL (node->decl))\n+\t      && cgraph_will_be_removed_from_program_if_no_direct_calls (node)\n+\t      && inline_summary (node)->inlinable\n+\t      && cgraph_function_body_availability (node) >= AVAIL_AVAILABLE\n+\t      && !DECL_EXTERNAL (node->decl)\n+\t      && can_inline_edge_p (node->callers, true))\n \t    {\n-\t      cgraph_inline_failed_t reason;\n+\t      struct cgraph_node *caller = node->callers->caller;\n+\n \t      old_size = overall_size;\n \t      if (dump_file)\n \t\t{\n \t\t  fprintf (dump_file,\n-\t\t\t   \"\\nConsidering %s size %i.\\n\",\n+\t\t\t   \"\\nInlining %s size %i.\\n\",\n \t\t\t   cgraph_node_name (node), inline_summary (node)->size);\n \t\t  fprintf (dump_file,\n \t\t\t   \" Called once from %s %i insns.\\n\",\n \t\t\t   cgraph_node_name (node->callers->caller),\n \t\t\t   inline_summary (node->callers->caller)->size);\n \t\t}\n \n-\t      if (cgraph_check_inline_limits (node->callers, &reason))\n-\t\t{\n-\t\t  struct cgraph_node *caller = node->callers->caller;\n-\t\t  cgraph_mark_inline_edge (node->callers, true, NULL);\n-\t\t  if (dump_file)\n-\t\t    fprintf (dump_file,\n-\t\t\t     \" Inlined into %s which now has %i size\"\n-\t\t\t     \" for a net change of %+i size.\\n\",\n-\t\t\t     cgraph_node_name (caller),\n-\t\t\t     inline_summary (caller)->size,\n-\t\t\t     overall_size - old_size);\n-\t\t}\n-\t      else\n-\t\t{\n-\t\t  if (dump_file)\n-\t\t    fprintf (dump_file,\n-\t\t\t     \" Not inlining: %s.\\n\",\n-                             cgraph_inline_failed_string (reason));\n-\t\t}\n+\t      cgraph_mark_inline_edge (node->callers, true, NULL);\n+\t      if (dump_file)\n+\t\tfprintf (dump_file,\n+\t\t\t \" Inlined into %s which now has %i size\"\n+\t\t\t \" for a net change of %+i size.\\n\",\n+\t\t\t cgraph_node_name (caller),\n+\t\t\t inline_summary (caller)->size,\n+\t\t\t overall_size - old_size);\n \t    }\n \t}\n     }\n@@ -1441,92 +1680,39 @@ cgraph_decide_inlining (void)\n \t     \"size %i turned to %i size.\\n\\n\",\n \t     ncalls_inlined, nfunctions_inlined, initial_size,\n \t     overall_size);\n-  free (order);\n   /* In WPA we use inline summaries for partitioning process.  */\n   if (!flag_wpa)\n     inline_free_summary ();\n   return 0;\n }\n \n-/* Return true when N is leaf function.  Accept cheap builtins\n-   in leaf functions.  */\n-\n-static bool\n-leaf_node_p (struct cgraph_node *n)\n-{\n-  struct cgraph_edge *e;\n-  for (e = n->callees; e; e = e->next_callee)\n-    if (!is_inexpensive_builtin (e->callee->decl))\n-      return false;\n-  return true;\n-}\n-\n-/* Return true if the edge E is inlinable during early inlining.  */\n-\n-static bool\n-cgraph_edge_early_inlinable_p (struct cgraph_edge *e, FILE *file)\n-{\n-  if (!inline_summary (e->callee)->inlinable)\n-    {\n-      if (file)\n-\tfprintf (file, \"Not inlining: Function not inlinable.\\n\");\n-      return false;\n-    }\n-  if (!e->callee->analyzed)\n-    {\n-      if (file)\n-\tfprintf (file, \"Not inlining: Function body not available.\\n\");\n-      return false;\n-    }\n-  if (!tree_can_inline_p (e)\n-      || e->call_stmt_cannot_inline_p)\n-    {\n-      if (file)\n-\tfprintf (file, \"Not inlining: %s.\\n\",\n-\t\t cgraph_inline_failed_string (e->inline_failed));\n-      return false;\n-    }\n-  if (!gimple_in_ssa_p (DECL_STRUCT_FUNCTION (e->caller->decl))\n-      || !gimple_in_ssa_p (DECL_STRUCT_FUNCTION (e->callee->decl)))\n-    {\n-      if (file)\n-\tfprintf (file, \"Not inlining: not in SSA form.\\n\");\n-      return false;\n-    }\n-  return true;\n-}\n-\n /* Inline always-inline function calls in NODE.  */\n \n static bool\n-cgraph_perform_always_inlining (struct cgraph_node *node)\n+inline_always_inline_functions (struct cgraph_node *node)\n {\n   struct cgraph_edge *e;\n   bool inlined = false;\n \n   for (e = node->callees; e; e = e->next_callee)\n     {\n-      if (!inline_summary (e->callee)->disregard_inline_limits)\n+      if (!DECL_DISREGARD_INLINE_LIMITS (e->callee->decl))\n \tcontinue;\n \n-      if (dump_file)\n-\tfprintf (dump_file,\n-\t\t \"Considering always-inline candidate %s.\\n\",\n-\t\t cgraph_node_name (e->callee));\n-\n       if (cgraph_edge_recursive_p (e))\n \t{\n \t  if (dump_file)\n-\t    fprintf (dump_file, \"Not inlining: recursive call.\\n\");\n+\t    fprintf (dump_file, \"  Not inlining recursive call to %s.\\n\",\n+\t\t     cgraph_node_name (e->callee));\n \t  e->inline_failed = CIF_RECURSIVE_INLINING;\n \t  continue;\n \t}\n \n-      if (!cgraph_edge_early_inlinable_p (e, dump_file))\n+      if (!can_early_inline_edge_p (e))\n \tcontinue;\n \n       if (dump_file)\n-\tfprintf (dump_file, \" Inlining %s into %s.\\n\",\n+\tfprintf (dump_file, \"  Inlining %s into %s (always_inline).\\n\",\n \t\t cgraph_node_name (e->callee),\n \t\t cgraph_node_name (e->caller));\n       cgraph_mark_inline_edge (e, true, NULL);\n@@ -1540,24 +1726,15 @@ cgraph_perform_always_inlining (struct cgraph_node *node)\n    expenses on updating data structures.  */\n \n static bool\n-cgraph_decide_inlining_incrementally (struct cgraph_node *node)\n+early_inline_small_functions (struct cgraph_node *node)\n {\n   struct cgraph_edge *e;\n   bool inlined = false;\n-  cgraph_inline_failed_t failed_reason;\n-\n-  /* Never inline regular functions into always-inline functions\n-     during incremental inlining.  */\n-  if (inline_summary (node)->disregard_inline_limits)\n-    return false;\n \n   for (e = node->callees; e; e = e->next_callee)\n     {\n-      int allowed_growth = 0;\n-\n       if (!inline_summary (e->callee)->inlinable\n-\t  || !e->inline_failed\n-\t  || inline_summary (e->callee)->disregard_inline_limits)\n+\t  || !e->inline_failed)\n \tcontinue;\n \n       /* Do not consider functions not declared inline.  */\n@@ -1570,47 +1747,25 @@ cgraph_decide_inlining_incrementally (struct cgraph_node *node)\n \tfprintf (dump_file, \"Considering inline candidate %s.\\n\",\n \t\t cgraph_node_name (e->callee));\n \n+      if (!can_early_inline_edge_p (e))\n+\tcontinue;\n+\n       if (cgraph_edge_recursive_p (e))\n \t{\n \t  if (dump_file)\n-\t    fprintf (dump_file, \"Not inlining: recursive call.\\n\");\n+\t    fprintf (dump_file, \"  Not inlining: recursive call.\\n\");\n \t  continue;\n \t}\n \n-      if (!cgraph_edge_early_inlinable_p (e, dump_file))\n+      if (!want_early_inline_function_p (e))\n \tcontinue;\n \n-      if (cgraph_maybe_hot_edge_p (e) && leaf_node_p (e->callee)\n-\t  && optimize_function_for_speed_p (cfun))\n-\tallowed_growth = PARAM_VALUE (PARAM_EARLY_INLINING_INSNS);\n-\n-      /* When the function body would grow and inlining the function\n-\t won't eliminate the need for offline copy of the function,\n-\t don't inline.  */\n-      if (estimate_edge_growth (e) > allowed_growth)\n-\t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file,\n-\t\t     \"Not inlining: code size would grow by %i.\\n\",\n-\t\t     estimate_edge_growth (e));\n-\t  continue;\n-\t}\n-      if (!cgraph_check_inline_limits (e, &e->inline_failed))\n-\t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \"Not inlining: %s.\\n\",\n-\t\t     cgraph_inline_failed_string (e->inline_failed));\n-\t  continue;\n-\t}\n-      if (cgraph_default_inline_p (e->callee, &failed_reason))\n-\t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \" Inlining %s into %s.\\n\",\n-\t\t     cgraph_node_name (e->callee),\n-\t\t     cgraph_node_name (e->caller));\n-\t  cgraph_mark_inline_edge (e, true, NULL);\n-\t  inlined = true;\n-\t}\n+      if (dump_file)\n+\tfprintf (dump_file, \" Inlining %s into %s.\\n\",\n+\t\t cgraph_node_name (e->callee),\n+\t\t cgraph_node_name (e->caller));\n+      cgraph_mark_inline_edge (e, true, NULL);\n+      inlined = true;\n     }\n \n   return inlined;\n@@ -1626,7 +1781,7 @@ static GTY ((length (\"nnodes\"))) struct cgraph_node **order;\n    passes to be somewhat more effective and avoids some code duplication in\n    later real inlining pass for testcases with very many function calls.  */\n static unsigned int\n-cgraph_early_inlining (void)\n+early_inliner (void)\n {\n   struct cgraph_node *node = cgraph_get_node (current_function_decl);\n   struct cgraph_edge *edge;\n@@ -1643,11 +1798,20 @@ cgraph_early_inlining (void)\n \n   /* Even when not optimizing or not inlining inline always-inline\n      functions.  */\n-  inlined = cgraph_perform_always_inlining (node);\n+  inlined = inline_always_inline_functions (node);\n \n   if (!optimize\n       || flag_no_inline\n-      || !flag_early_inlining)\n+      || !flag_early_inlining\n+      /* Never inline regular functions into always-inline functions\n+\t during incremental inlining.  This sucks as functions calling\n+\t always inline functions will get less optimized, but at the\n+\t same time inlining of functions calling always inline\n+\t functoin into an always inline function might introduce\n+\t cycles of edges to be always inlined in the callgraph.\n+\n+\t We might want to be smarter and just avoid this type of inlining.  */\n+      || DECL_DISREGARD_INLINE_LIMITS (node->decl))\n     ;\n   else if (lookup_attribute (\"flatten\",\n \t\t\t     DECL_ATTRIBUTES (node->decl)) != NULL)\n@@ -1657,18 +1821,30 @@ cgraph_early_inlining (void)\n       if (dump_file)\n \tfprintf (dump_file,\n \t\t \"Flattening %s\\n\", cgraph_node_name (node));\n-      cgraph_flatten (node);\n+      flatten_function (node);\n       inlined = true;\n     }\n   else\n     {\n       /* We iterate incremental inlining to get trivial cases of indirect\n \t inlining.  */\n       while (iterations < PARAM_VALUE (PARAM_EARLY_INLINER_MAX_ITERATIONS)\n-\t     && cgraph_decide_inlining_incrementally (node))\n+\t     && early_inline_small_functions (node))\n \t{\n \t  timevar_push (TV_INTEGRATION);\n \t  todo |= optimize_inline_calls (current_function_decl);\n+\n+\t  /* Technically we ought to recompute inline parameters so the new\n+ \t     iteration of early inliner works as expected.  We however have\n+\t     values approximately right and thus we only need to update edge\n+\t     info that might be cleared out for newly discovered edges.  */\n+\t  for (edge = node->callees; edge; edge = edge->next_callee)\n+\t    {\n+\t      edge->call_stmt_size\n+\t\t= estimate_num_insns (edge->call_stmt, &eni_size_weights);\n+\t      edge->call_stmt_time\n+\t\t= estimate_num_insns (edge->call_stmt, &eni_time_weights);\n+\t    }\n \t  timevar_pop (TV_INTEGRATION);\n \t  iterations++;\n \t  inlined = false;\n@@ -1681,19 +1857,6 @@ cgraph_early_inlining (void)\n     {\n       timevar_push (TV_INTEGRATION);\n       todo |= optimize_inline_calls (current_function_decl);\n-\n-      /* Technically we ought to recompute inline parameters so the new iteration of\n-\t early inliner works as expected.  We however have values approximately right\n-\t and thus we only need to update edge info that might be cleared out for\n-\t newly discovered edges.  */\n-      for (edge = node->callees; edge; edge = edge->next_callee)\n-\t{\n-\t  edge->call_stmt_size\n-\t    = estimate_num_insns (edge->call_stmt, &eni_size_weights);\n-\t  edge->call_stmt_time\n-\t    = estimate_num_insns (edge->call_stmt, &eni_time_weights);\n-\t}\n-\n       timevar_pop (TV_INTEGRATION);\n     }\n \n@@ -1708,7 +1871,7 @@ struct gimple_opt_pass pass_early_inline =\n   GIMPLE_PASS,\n   \"einline\",\t \t\t\t/* name */\n   NULL,\t\t\t\t\t/* gate */\n-  cgraph_early_inlining,\t\t/* execute */\n+  early_inliner,\t\t\t/* execute */\n   NULL,\t\t\t\t\t/* sub */\n   NULL,\t\t\t\t\t/* next */\n   0,\t\t\t\t\t/* static_pass_number */\n@@ -1730,8 +1893,8 @@ inline_transform (struct cgraph_node *node)\n   struct cgraph_edge *e;\n   bool inline_p = false;\n \n-  /* FIXME: Currently the pass manager is adding inline transform more than once to some\n-     clones.  This needs revisiting after WPA cleanups.  */\n+  /* FIXME: Currently the pass manager is adding inline transform more than\n+     once to some clones.  This needs revisiting after WPA cleanups.  */\n   if (cfun->after_inlining)\n     return 0;\n \n@@ -1762,7 +1925,7 @@ inline_transform (struct cgraph_node *node)\n    happens during early inlining.  */\n \n static bool\n-gate_cgraph_decide_inlining (void)\n+gate_ipa_inline (void)\n {\n   /* ???  We'd like to skip this if not optimizing or not inlining as\n      all always-inline functions have been processed by early\n@@ -1777,8 +1940,8 @@ struct ipa_opt_pass_d pass_ipa_inline =\n  {\n   IPA_PASS,\n   \"inline\",\t\t\t\t/* name */\n-  gate_cgraph_decide_inlining,\t\t/* gate */\n-  cgraph_decide_inlining,\t\t/* execute */\n+  gate_ipa_inline,\t\t\t/* gate */\n+  ipa_inline,\t\t\t\t/* execute */\n   NULL,\t\t\t\t\t/* sub */\n   NULL,\t\t\t\t\t/* next */\n   0,\t\t\t\t\t/* static_pass_number */"}, {"sha": "1fd1f5c9ff560eb1f171e08da27f6326e2806cb3", "filename": "gcc/ipa-inline.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Fipa-inline.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Fipa-inline.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fipa-inline.h?ref=4c0f767939eb659b721f9222cf7bdacd0194310b", "patch": "@@ -41,8 +41,6 @@ struct inline_summary\n   /* False when there something makes versioning impossible.\n      Currently computed and used only by ipa-cp.  */\n   unsigned versionable : 1;\n-  /* True when function should be inlined independently on its size.  */\n-  unsigned disregard_inline_limits : 1;\n \n   /* Information about function that will result after applying all the\n      inline decisions present in the callgraph.  Generally kept up to"}, {"sha": "2cf1dc6b00a3edba291aa48fe23179728a753ebb", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=4c0f767939eb659b721f9222cf7bdacd0194310b", "patch": "@@ -1,3 +1,7 @@\n+2011-04-17  Jan Hubicka  <jh@suse.cz>\n+\n+\t* gcc.dg/winline-5.c: Update testcase.\n+\n 2011-04-17  Eric Botcazou  <ebotcazou@adacore.com>\n \n \t* gnat.dg/discr27.ad[sb]: Move dg directive."}, {"sha": "a42ae943fe0b18b5161788b35004dde10c4f1d8f", "filename": "gcc/testsuite/gcc.dg/winline-5.c", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Ftestsuite%2Fgcc.dg%2Fwinline-5.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Ftestsuite%2Fgcc.dg%2Fwinline-5.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fwinline-5.c?ref=4c0f767939eb659b721f9222cf7bdacd0194310b", "patch": "@@ -15,15 +15,7 @@ inline int q(void) /* { dg-warning \"inline-unit-growth\" } */\n \tbig();\n \tbig();\n }\n-inline int q1(void)\n-{\n-\tbig();\n-\tbig();\n-\tbig();\n-}\n int t (void)\n {\n- /* We allow one inlining over limit.  */\n-\tq1();\n \treturn q ();\t\t /* { dg-warning \"called from here\" } */\n }"}, {"sha": "ea4baac1f4e9e4d5b3546ccecaf7a14500c8fe2d", "filename": "gcc/tree-inline.c", "status": "modified", "additions": 0, "deletions": 99, "changes": 99, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Ftree-inline.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Ftree-inline.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-inline.c?ref=4c0f767939eb659b721f9222cf7bdacd0194310b", "patch": "@@ -3138,29 +3138,6 @@ inline_forbidden_p (tree fndecl)\n   return forbidden_p;\n }\n \n-/* Return true if CALLEE cannot be inlined into CALLER.  */\n-\n-static bool\n-inline_forbidden_into_p (tree caller, tree callee)\n-{\n-  /* Don't inline if the functions have different EH personalities.  */\n-  if (DECL_FUNCTION_PERSONALITY (caller)\n-      && DECL_FUNCTION_PERSONALITY (callee)\n-      && (DECL_FUNCTION_PERSONALITY (caller)\n-\t  != DECL_FUNCTION_PERSONALITY (callee)))\n-    return true;\n-\n-  /* Don't inline if the callee can throw non-call exceptions but the\n-     caller cannot.  */\n-  if (DECL_STRUCT_FUNCTION (callee)\n-      && DECL_STRUCT_FUNCTION (callee)->can_throw_non_call_exceptions\n-      && !(DECL_STRUCT_FUNCTION (caller)\n-\t   && DECL_STRUCT_FUNCTION (caller)->can_throw_non_call_exceptions))\n-    return true;\n-\n-  return false;\n-}\n-\n /* Returns nonzero if FN is a function that does not have any\n    fundamental inline blocking properties.  */\n \n@@ -3750,10 +3727,6 @@ expand_call_inline (basic_block bb, gimple stmt, copy_body_data *id)\n       && gimple_has_body_p (DECL_ABSTRACT_ORIGIN (fn)))\n     fn = DECL_ABSTRACT_ORIGIN (fn);\n \n-  /* First check that inlining isn't simply forbidden in this case.  */\n-  if (inline_forbidden_into_p (cg_edge->caller->decl, cg_edge->callee->decl))\n-    goto egress;\n-\n   /* Don't try to inline functions that are not well-suited to inlining.  */\n   if (!cgraph_inline_p (cg_edge, &reason))\n     {\n@@ -5298,75 +5271,3 @@ build_duplicate_type (tree type)\n \n   return type;\n }\n-\n-/* Return whether it is safe to inline a function because it used different\n-   target specific options or call site actual types mismatch parameter types.\n-   E is the call edge to be checked.  */\n-bool\n-tree_can_inline_p (struct cgraph_edge *e)\n-{\n-#if 0\n-  /* This causes a regression in SPEC in that it prevents a cold function from\n-     inlining a hot function.  Perhaps this should only apply to functions\n-     that the user declares hot/cold/optimize explicitly.  */\n-\n-  /* Don't inline a function with a higher optimization level than the\n-     caller, or with different space constraints (hot/cold functions).  */\n-  tree caller_tree = DECL_FUNCTION_SPECIFIC_OPTIMIZATION (caller);\n-  tree callee_tree = DECL_FUNCTION_SPECIFIC_OPTIMIZATION (callee);\n-\n-  if (caller_tree != callee_tree)\n-    {\n-      struct cl_optimization *caller_opt\n-\t= TREE_OPTIMIZATION ((caller_tree)\n-\t\t\t     ? caller_tree\n-\t\t\t     : optimization_default_node);\n-\n-      struct cl_optimization *callee_opt\n-\t= TREE_OPTIMIZATION ((callee_tree)\n-\t\t\t     ? callee_tree\n-\t\t\t     : optimization_default_node);\n-\n-      if ((caller_opt->optimize > callee_opt->optimize)\n-\t  || (caller_opt->optimize_size != callee_opt->optimize_size))\n-\treturn false;\n-    }\n-#endif\n-  tree caller, callee;\n-\n-  caller = e->caller->decl;\n-  callee = e->callee->decl;\n-\n-  /* First check that inlining isn't simply forbidden in this case.  */\n-  if (inline_forbidden_into_p (caller, callee))\n-    {\n-      e->inline_failed = CIF_UNSPECIFIED;\n-      if (e->call_stmt)\n-\tgimple_call_set_cannot_inline (e->call_stmt, true);\n-      return false;\n-    }\n-\n-  /* Allow the backend to decide if inlining is ok.  */\n-  if (!targetm.target_option.can_inline_p (caller, callee))\n-    {\n-      e->inline_failed = CIF_TARGET_OPTION_MISMATCH;\n-      if (e->call_stmt)\n-\tgimple_call_set_cannot_inline (e->call_stmt, true);\n-      e->call_stmt_cannot_inline_p = true;\n-      return false;\n-    }\n-\n-  /* Do not inline calls where we cannot triviall work around mismatches\n-     in argument or return types.  */\n-  if (e->call_stmt\n-      && !gimple_check_call_matching_types (e->call_stmt, callee))\n-    {\n-      e->inline_failed = CIF_MISMATCHED_ARGUMENTS;\n-      if (e->call_stmt)\n-\tgimple_call_set_cannot_inline (e->call_stmt, true);\n-      e->call_stmt_cannot_inline_p = true;\n-      return false;\n-    }\n-\n-  return true;\n-}"}, {"sha": "fb039e3194ca513997b3cdc84cd31cdc7f31b214", "filename": "gcc/tree-inline.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Ftree-inline.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4c0f767939eb659b721f9222cf7bdacd0194310b/gcc%2Ftree-inline.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-inline.h?ref=4c0f767939eb659b721f9222cf7bdacd0194310b", "patch": "@@ -182,7 +182,6 @@ int estimate_num_insns (gimple, eni_weights *);\n int estimate_num_insns_fn (tree, eni_weights *);\n int count_insns_seq (gimple_seq, eni_weights *);\n bool tree_versionable_function_p (tree);\n-bool tree_can_inline_p (struct cgraph_edge *e);\n \n extern tree remap_decl (tree decl, copy_body_data *id);\n extern tree remap_type (tree type, copy_body_data *id);"}]}