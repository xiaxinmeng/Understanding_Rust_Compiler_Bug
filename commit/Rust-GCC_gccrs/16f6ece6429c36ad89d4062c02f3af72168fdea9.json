{"sha": "16f6ece6429c36ad89d4062c02f3af72168fdea9", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MTZmNmVjZTY0MjljMzZhZDg5ZDQwNjJjMDJmM2FmNzIxNjhmZGVhOQ==", "commit": {"author": {"name": "Bernd Schmidt", "email": "bernds@redhat.co.uk", "date": "2000-12-03T16:11:45Z"}, "committer": {"name": "Bernd Schmidt", "email": "bernds@gcc.gnu.org", "date": "2000-12-03T16:11:45Z"}, "message": "Move dependency code out of haifa-sched.c\n\nFrom-SVN: r37975", "tree": {"sha": "ba0266e2403479ce69ad386bc15adc36568cbd8b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ba0266e2403479ce69ad386bc15adc36568cbd8b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/16f6ece6429c36ad89d4062c02f3af72168fdea9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/16f6ece6429c36ad89d4062c02f3af72168fdea9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/16f6ece6429c36ad89d4062c02f3af72168fdea9", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/16f6ece6429c36ad89d4062c02f3af72168fdea9/comments", "author": null, "committer": null, "parents": [{"sha": "c62c265908370049de8565fe666386a33e35e73a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c62c265908370049de8565fe666386a33e35e73a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c62c265908370049de8565fe666386a33e35e73a"}], "stats": {"total": 3070, "additions": 1599, "deletions": 1471}, "files": [{"sha": "d38b6e8bf03baf8066a4ffacfa317bc65d2ce422", "filename": "gcc/ChangeLog", "status": "modified", "additions": 32, "deletions": 0, "changes": 32, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/16f6ece6429c36ad89d4062c02f3af72168fdea9/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/16f6ece6429c36ad89d4062c02f3af72168fdea9/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=16f6ece6429c36ad89d4062c02f3af72168fdea9", "patch": "@@ -1,5 +1,32 @@\n 2000-12-03  Bernd Schmidt  <bernds@redhat.co.uk>\n \n+\t* Makefile.in (OBJS): Add sched-deps.o.\n+\t(sched-deps.o): New rule.\n+\t* haifa-sched.c (struct deps, struct haifa_insn_data): Moved to\n+\tsched-int.h.\n+\t(INSN_DEPEND, INSN_LUID, CANT_MOVE, INSN_DEP_COUNT): Macros moved to\n+\tsched-int.h.\n+\t(SIZE_FOR_MODE): Delete unused macro.\n+\t(reg_known_equiv_p, reg_known_value, reg_pending_clobbers,\n+\treg_pending_sets, reg_pending_sets_all, true_dependency_cache,\n+\tanti_dependency_cache, output_dependency_cache,\n+\tforward_dependency_cache): Variables moved to sched-deps.c.\n+\t(add_dependence, remove_dependence, find_insn_list,\n+\tfind_insn_mem_list, add_insn_mem_dependence, flush_pending_lists,\n+\tsched_analyze_insn, sched_analyze_1, sched_analyze_2,\n+\tsched_analyze, group_leader, compute_forward_dependences,\n+\tinit_deps, free_deps, init_dependency_caches, free_dependency_caches):\n+\tFunctions moved to sched-deps.c.\n+\t(schedule_region): Call init_deps_global and finish_deps_global\n+\tinstead of directly manipulating dependency data structures.\n+\t* sched-deps.c: New file.\n+\t(init_deps_global, finish_deps_global): New functions.\n+\t* sched-int.h (struct haifa_insn_data, struct deps): Moved here from\n+\thaifa-sched.c.\n+\t(h_i_d): Declare.\n+\t(INSN_DEPEND, INSN_LUID, CANT_MOVE, INSN_DEP_COUNT): Macros moved here\n+\tfrom haifa-sched.c.\n+\n \t* Makefile.in (OBJS): Add sched-vis.o.\n \t(sched-vis.o): New rule.\n \t* haifa-sched.c (get_unit_last_insn): New function.\n@@ -20,6 +47,11 @@\n \tprint_block_visualization): Lose basic block argument.  All callers\n \tchanged.\n \t(visualize_scheduled_insns): Use new function get_unit_last_insn.\n+\t* sched-int.h (current_sched_info, sched_dump): Declare.\n+\t(init_target_units, insn_print_units, init_block_visualization,\n+\tprint_block_visualization, visualize_scheduled_inns,\n+\tvisualize_no_unit, visualize_stall_cycles, visualize_alloc,\n+\tvisualize_free): Declare functions.\n \n \t* sched-int.h: New file.\n \t* Makefile.in (haifa-sched.o): Depend on it."}, {"sha": "62f5ae07edec19ef8499125dd716238b518b06c7", "filename": "gcc/Makefile.in", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/16f6ece6429c36ad89d4062c02f3af72168fdea9/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/16f6ece6429c36ad89d4062c02f3af72168fdea9/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=16f6ece6429c36ad89d4062c02f3af72168fdea9", "patch": "@@ -737,7 +737,7 @@ OBJS = diagnostic.o version.o tree.o print-tree.o stor-layout.o fold-const.o  \\\n  mbchar.o splay-tree.o graph.o sbitmap.o resource.o hash.o predict.o\t      \\\n  lists.o ggc-common.o $(GGC) stringpool.o simplify-rtx.o ssa.o bb-reorder.o   \\\n  sibcall.o conflict.o timevar.o ifcvt.o dominance.o dependence.o dce.o \\\n- sched-vis.o hashtab.o\n+ sched-vis.o sched-deps.o hashtab.o\n \n BACKEND = toplev.o libbackend.a\n \n@@ -1455,6 +1455,9 @@ regmove.o : regmove.c $(CONFIG_H) system.h $(RTL_H) insn-config.h \\\n haifa-sched.o : haifa-sched.c $(CONFIG_H) system.h $(RTL_H) sched-int.h \\\n    $(BASIC_BLOCK_H) $(REGS_H) hard-reg-set.h flags.h insn-config.h function.h \\\n    $(INSN_ATTR_H) toplev.h $(RECOG_H) except.h\n+sched-deps.o : haifa-sched.c $(CONFIG_H) system.h $(RTL_H) sched-int.h \\\n+   $(BASIC_BLOCK_H) $(REGS_H) hard-reg-set.h flags.h insn-config.h function.h \\\n+   $(INSN_ATTR_H) toplev.h $(RECOG_H) except.h\n sched-vis.o : sched-vis.c $(CONFIG_H) system.h $(RTL_H) sched-int.h \\\n    $(INSN_ATTR_H) $(REGS_H)\n final.o : final.c $(CONFIG_H) system.h $(RTL_H) $(TREE_H) flags.h intl.h \\"}, {"sha": "d768d85e963f043ed33432ac662359c947187c9c", "filename": "gcc/haifa-sched.c", "status": "modified", "additions": 3, "deletions": 1470, "changes": 1473, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/16f6ece6429c36ad89d4062c02f3af72168fdea9/gcc%2Fhaifa-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/16f6ece6429c36ad89d4062c02f3af72168fdea9/gcc%2Fhaifa-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fhaifa-sched.c?ref=16f6ece6429c36ad89d4062c02f3af72168fdea9", "patch": "@@ -172,9 +172,6 @@ the Free Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n #include \"recog.h\"\n #include \"sched-int.h\"\n \n-extern char *reg_known_equiv_p;\n-extern rtx *reg_known_value;\n-\n #ifdef INSN_SCHEDULING\n \n /* issue_rate is the number of insns that can be scheduled in the same\n@@ -225,158 +222,9 @@ fix_sched_param (param, val)\n     warning (\"fix_sched_param: unknown param: %s\", param);\n }\n \n-/* Describe state of dependencies used during sched_analyze phase.  */\n-struct deps\n-{\n-  /* The *_insns and *_mems are paired lists.  Each pending memory operation\n-     will have a pointer to the MEM rtx on one list and a pointer to the\n-     containing insn on the other list in the same place in the list.  */\n-\n-  /* We can't use add_dependence like the old code did, because a single insn\n-     may have multiple memory accesses, and hence needs to be on the list\n-     once for each memory access.  Add_dependence won't let you add an insn\n-     to a list more than once.  */\n-\n-  /* An INSN_LIST containing all insns with pending read operations.  */\n-  rtx pending_read_insns;\n-\n-  /* An EXPR_LIST containing all MEM rtx's which are pending reads.  */\n-  rtx pending_read_mems;\n-\n-  /* An INSN_LIST containing all insns with pending write operations.  */\n-  rtx pending_write_insns;\n-\n-  /* An EXPR_LIST containing all MEM rtx's which are pending writes.  */\n-  rtx pending_write_mems;\n-\n-  /* Indicates the combined length of the two pending lists.  We must prevent\n-     these lists from ever growing too large since the number of dependencies\n-     produced is at least O(N*N), and execution time is at least O(4*N*N), as\n-     a function of the length of these pending lists.  */\n-  int pending_lists_length;\n-\n-  /* The last insn upon which all memory references must depend.\n-     This is an insn which flushed the pending lists, creating a dependency\n-     between it and all previously pending memory references.  This creates\n-     a barrier (or a checkpoint) which no memory reference is allowed to cross.\n-\n-     This includes all non constant CALL_INSNs.  When we do interprocedural\n-     alias analysis, this restriction can be relaxed.\n-     This may also be an INSN that writes memory if the pending lists grow\n-     too large.  */\n-  rtx last_pending_memory_flush;\n-\n-  /* The last function call we have seen.  All hard regs, and, of course,\n-     the last function call, must depend on this.  */\n-  rtx last_function_call;\n-\n-  /* Used to keep post-call psuedo/hard reg movements together with\n-     the call.  */\n-  int in_post_call_group_p;\n-\n-  /* The LOG_LINKS field of this is a list of insns which use a pseudo\n-     register that does not already cross a call.  We create\n-     dependencies between each of those insn and the next call insn,\n-     to ensure that they won't cross a call after scheduling is done.  */\n-  rtx sched_before_next_call;\n-\n-  /* Element N is the next insn that sets (hard or pseudo) register\n-     N within the current basic block; or zero, if there is no\n-     such insn.  Needed for new registers which may be introduced\n-     by splitting insns.  */\n-  rtx *reg_last_uses;\n-  rtx *reg_last_sets;\n-  rtx *reg_last_clobbers;\n-};\n-\n-static regset reg_pending_sets;\n-static regset reg_pending_clobbers;\n-static int reg_pending_sets_all;\n-\n-/* To speed up the test for duplicate dependency links we keep a\n-   record of dependencies created by add_dependence when the average\n-   number of instructions in a basic block is very large.\n-\n-   Studies have shown that there is typically around 5 instructions between\n-   branches for typical C code.  So we can make a guess that the average\n-   basic block is approximately 5 instructions long; we will choose 100X\n-   the average size as a very large basic block.\n-\n-   Each insn has associated bitmaps for its dependencies.  Each bitmap\n-   has enough entries to represent a dependency on any other insn in\n-   the insn chain.  All bitmap for true dependencies cache is\n-   allocated then the rest two ones are also allocated. */\n-static sbitmap *true_dependency_cache;\n-static sbitmap *anti_dependency_cache;\n-static sbitmap *output_dependency_cache;\n-\n-/* To speed up checking consistency of formed forward insn\n-   dependencies we use the following cache.  Another possible solution\n-   could be switching off checking duplication of insns in forward\n-   dependencies. */\n-#ifdef ENABLE_CHECKING\n-static sbitmap *forward_dependency_cache;\n-#endif\n-\n-/* Indexed by INSN_UID, the collection of all data associated with\n-   a single instruction.  */\n-\n-struct haifa_insn_data\n-{\n-  /* A list of insns which depend on the instruction.  Unlike LOG_LINKS,\n-     it represents forward dependancies.  */\n-  rtx depend;\n-\n-  /* The line number note in effect for each insn.  For line number\n-     notes, this indicates whether the note may be reused.  */\n-  rtx line_note;\n-\n-  /* Logical uid gives the original ordering of the insns.  */\n-  int luid;\n-\n-  /* A priority for each insn.  */\n-  int priority;\n-\n-  /* The number of incoming edges in the forward dependency graph.\n-     As scheduling proceds, counts are decreased.  An insn moves to\n-     the ready queue when its counter reaches zero.  */\n-  int dep_count;\n-\n-  /* An encoding of the blockage range function.  Both unit and range\n-     are coded.  */\n-  unsigned int blockage;\n-\n-  /* Number of instructions referring to this insn.  */\n-  int ref_count;\n-\n-  /* The minimum clock tick at which the insn becomes ready.  This is\n-     used to note timing constraints for the insns in the pending list.  */\n-  int tick;\n+struct haifa_insn_data *h_i_d;\n \n-  short cost;\n-\n-  /* An encoding of the function units used.  */\n-  short units;\n-\n-  /* This weight is an estimation of the insn's contribution to\n-     register pressure.  */\n-  short reg_weight;\n-\n-  /* Some insns (e.g. call) are not allowed to move across blocks.  */\n-  unsigned int cant_move : 1;\n-\n-  /* Set if there's DEF-USE dependance between some speculatively\n-     moved load insn and this one.  */\n-  unsigned int fed_by_spec_load : 1;\n-  unsigned int is_load_insn : 1;\n-};\n-\n-static struct haifa_insn_data *h_i_d;\n-\n-#define INSN_DEPEND(INSN)\t(h_i_d[INSN_UID (INSN)].depend)\n-#define INSN_LUID(INSN)\t\t(h_i_d[INSN_UID (INSN)].luid)\n #define INSN_PRIORITY(INSN)\t(h_i_d[INSN_UID (INSN)].priority)\n-#define INSN_DEP_COUNT(INSN)\t(h_i_d[INSN_UID (INSN)].dep_count)\n #define INSN_COST(INSN)\t\t(h_i_d[INSN_UID (INSN)].cost)\n #define INSN_UNIT(INSN)\t\t(h_i_d[INSN_UID (INSN)].units)\n #define INSN_REG_WEIGHT(INSN)\t(h_i_d[INSN_UID (INSN)].reg_weight)\n@@ -407,7 +255,6 @@ static struct haifa_insn_data *h_i_d;\n #define INSN_REF_COUNT(INSN)\t(h_i_d[INSN_UID (INSN)].ref_count)\n #define LINE_NOTE(INSN)\t\t(h_i_d[INSN_UID (INSN)].line_note)\n #define INSN_TICK(INSN)\t\t(h_i_d[INSN_UID (INSN)].tick)\n-#define CANT_MOVE(insn)\t\t(h_i_d[INSN_UID (insn)].cant_move)\n #define FED_BY_SPEC_LOAD(insn)\t(h_i_d[INSN_UID (insn)].fed_by_spec_load)\n #define IS_LOAD_INSN(insn)\t(h_i_d[INSN_UID (insn)].is_load_insn)\n \n@@ -489,10 +336,6 @@ struct ready_list\n };\n \n /* Forward declarations.  */\n-static void add_dependence PARAMS ((rtx, rtx, enum reg_note));\n-static void remove_dependence PARAMS ((rtx, rtx));\n-static rtx find_insn_list PARAMS ((rtx, rtx));\n-static void set_sched_group_p PARAMS ((rtx));\n static unsigned int blockage_range PARAMS ((int, rtx));\n static void clear_units PARAMS ((void));\n static void schedule_unit PARAMS ((int, rtx, int));\n@@ -501,13 +344,6 @@ static int potential_hazard PARAMS ((int, rtx, int));\n static int insn_cost PARAMS ((rtx, rtx, rtx));\n static int priority PARAMS ((rtx));\n static void free_pending_lists PARAMS ((void));\n-static void add_insn_mem_dependence PARAMS ((struct deps *, rtx *, rtx *, rtx,\n-\t\t\t\t\t     rtx));\n-static void flush_pending_lists PARAMS ((struct deps *, rtx, int));\n-static void sched_analyze_1 PARAMS ((struct deps *, rtx, rtx));\n-static void sched_analyze_2 PARAMS ((struct deps *, rtx, rtx));\n-static void sched_analyze_insn PARAMS ((struct deps *, rtx, rtx, rtx));\n-static void sched_analyze PARAMS ((struct deps *, rtx, rtx));\n static int rank_for_schedule PARAMS ((const PTR, const PTR));\n static void swap_sort PARAMS ((rtx *, int));\n static void queue_insn PARAMS ((rtx, int));\n@@ -740,8 +576,6 @@ static int haifa_classify_insn PARAMS ((rtx));\n static int is_prisky PARAMS ((rtx, int, int));\n static int is_exception_free PARAMS ((rtx, int, int));\n \n-static char find_insn_mem_list PARAMS ((rtx, rtx, rtx, rtx));\n-static void compute_forward_dependences PARAMS ((rtx, rtx));\n static void add_branch_dependences PARAMS ((rtx, rtx));\n static void compute_block_backward_dependences PARAMS ((int));\n void debug_dependencies PARAMS ((void));\n@@ -794,12 +628,7 @@ void debug_reg_vector PARAMS ((regset));\n \n static rtx move_insn1 PARAMS ((rtx, rtx));\n static rtx move_insn PARAMS ((rtx, rtx));\n-static rtx group_leader PARAMS ((rtx));\n static int set_priorities PARAMS ((int));\n-static void init_deps PARAMS ((struct deps *));\n-static void free_deps PARAMS ((struct deps *));\n-static void init_dependency_caches PARAMS ((int));\n-static void free_dependency_caches PARAMS ((void));\n static void init_regions PARAMS ((void));\n static void sched_init PARAMS ((FILE *));\n static void schedule_region PARAMS ((int));\n@@ -809,307 +638,6 @@ static void propagate_deps PARAMS ((int, struct deps *, int));\n \f\n /* Point to state used for the current scheduling pass.  */\n struct sched_info *current_sched_info;\n-\n-#define SIZE_FOR_MODE(X) (GET_MODE_SIZE (GET_MODE (X)))\n-\n-/* Add ELEM wrapped in an INSN_LIST with reg note kind DEP_TYPE to the\n-   LOG_LINKS of INSN, if not already there.  DEP_TYPE indicates the type\n-   of dependence that this link represents.  */\n-\n-static void\n-add_dependence (insn, elem, dep_type)\n-     rtx insn;\n-     rtx elem;\n-     enum reg_note dep_type;\n-{\n-  rtx link, next;\n-  int present_p;\n-  enum reg_note present_dep_type;\n-\n-  /* Don't depend an insn on itself.  */\n-  if (insn == elem)\n-    return;\n-\n-  /* We can get a dependency on deleted insns due to optimizations in\n-     the register allocation and reloading or due to splitting.  Any\n-     such dependency is useless and can be ignored.  */\n-  if (GET_CODE (elem) == NOTE)\n-    return;\n-\n-  /* If elem is part of a sequence that must be scheduled together, then\n-     make the dependence point to the last insn of the sequence.\n-     When HAVE_cc0, it is possible for NOTEs to exist between users and\n-     setters of the condition codes, so we must skip past notes here.\n-     Otherwise, NOTEs are impossible here.  */\n-  next = next_nonnote_insn (elem);\n-  if (next && SCHED_GROUP_P (next)\n-      && GET_CODE (next) != CODE_LABEL)\n-    {\n-      /* Notes will never intervene here though, so don't bother checking\n-         for them.  */\n-      /* Hah!  Wrong.  */\n-      /* We must reject CODE_LABELs, so that we don't get confused by one\n-         that has LABEL_PRESERVE_P set, which is represented by the same\n-         bit in the rtl as SCHED_GROUP_P.  A CODE_LABEL can never be\n-         SCHED_GROUP_P.  */\n-\n-      rtx nnext;\n-      while ((nnext = next_nonnote_insn (next)) != NULL\n-\t     && SCHED_GROUP_P (nnext)\n-\t     && GET_CODE (nnext) != CODE_LABEL)\n-\tnext = nnext;\n-\n-      /* Again, don't depend an insn on itself.  */\n-      if (insn == next)\n-\treturn;\n-\n-      /* Make the dependence to NEXT, the last insn of the group, instead\n-         of the original ELEM.  */\n-      elem = next;\n-    }\n-\n-  present_p = 1;\n-#ifdef INSN_SCHEDULING\n-  /* (This code is guarded by INSN_SCHEDULING, otherwise INSN_BB is undefined.)\n-     No need for interblock dependences with calls, since\n-     calls are not moved between blocks.   Note: the edge where\n-     elem is a CALL is still required.  */\n-  if (GET_CODE (insn) == CALL_INSN\n-      && (INSN_BB (elem) != INSN_BB (insn)))\n-    return;\n-\n-  /* If we already have a dependency for ELEM, then we do not need to\n-     do anything.  Avoiding the list walk below can cut compile times\n-     dramatically for some code.  */\n-  if (true_dependency_cache != NULL)\n-    {\n-      if (anti_dependency_cache == NULL || output_dependency_cache == NULL)\n-\tabort ();\n-      if (TEST_BIT (true_dependency_cache[INSN_LUID (insn)], INSN_LUID (elem)))\n-\tpresent_dep_type = 0;\n-      else if (TEST_BIT (anti_dependency_cache[INSN_LUID (insn)],\n-\t\t\t INSN_LUID (elem)))\n-\tpresent_dep_type = REG_DEP_ANTI;\n-      else if (TEST_BIT (output_dependency_cache[INSN_LUID (insn)],\n-\t\t\t INSN_LUID (elem)))\n-\tpresent_dep_type = REG_DEP_OUTPUT;\n-      else \n-\tpresent_p = 0;\n-      if (present_p && (int) dep_type >= (int) present_dep_type)\n-\treturn;\n-    }\n-#endif\n-\n-  /* Check that we don't already have this dependence.  */\n-  if (present_p)\n-    for (link = LOG_LINKS (insn); link; link = XEXP (link, 1))\n-      if (XEXP (link, 0) == elem)\n-\t{\n-#ifdef INSN_SCHEDULING\n-\t  /* Clear corresponding cache entry because type of the link\n-             may be changed. */\n-\t  if (true_dependency_cache != NULL)\n-\t    {\n-\t      if (REG_NOTE_KIND (link) == REG_DEP_ANTI)\n-\t\tRESET_BIT (anti_dependency_cache[INSN_LUID (insn)],\n-\t\t\t   INSN_LUID (elem));\n-\t      else if (REG_NOTE_KIND (link) == REG_DEP_OUTPUT\n-\t\t       && output_dependency_cache)\n-\t\tRESET_BIT (output_dependency_cache[INSN_LUID (insn)],\n-\t\t\t   INSN_LUID (elem));\n-\t      else\n-\t\tabort ();\n-\t    }\n-#endif\n-\n-\t  /* If this is a more restrictive type of dependence than the existing\n-\t     one, then change the existing dependence to this type.  */\n-\t  if ((int) dep_type < (int) REG_NOTE_KIND (link))\n-\t    PUT_REG_NOTE_KIND (link, dep_type);\n-\t  \n-#ifdef INSN_SCHEDULING\n-\t  /* If we are adding a dependency to INSN's LOG_LINKs, then\n-\t     note that in the bitmap caches of dependency information. */\n-\t  if (true_dependency_cache != NULL)\n-\t    {\n-\t      if ((int)REG_NOTE_KIND (link) == 0)\n-\t\tSET_BIT (true_dependency_cache[INSN_LUID (insn)],\n-\t\t\t INSN_LUID (elem));\n-\t      else if (REG_NOTE_KIND (link) == REG_DEP_ANTI)\n-\t\tSET_BIT (anti_dependency_cache[INSN_LUID (insn)],\n-\t\t\t INSN_LUID (elem));\n-\t      else if (REG_NOTE_KIND (link) == REG_DEP_OUTPUT)\n-\t\tSET_BIT (output_dependency_cache[INSN_LUID (insn)],\n-\t\t\t INSN_LUID (elem));\n-\t    }\n-#endif\n-\t  return;\n-      }\n-  /* Might want to check one level of transitivity to save conses.  */\n-\n-  link = alloc_INSN_LIST (elem, LOG_LINKS (insn));\n-  LOG_LINKS (insn) = link;\n-\n-  /* Insn dependency, not data dependency.  */\n-  PUT_REG_NOTE_KIND (link, dep_type);\n-\n-#ifdef INSN_SCHEDULING\n-  /* If we are adding a dependency to INSN's LOG_LINKs, then note that\n-     in the bitmap caches of dependency information. */\n-  if (true_dependency_cache != NULL)\n-    {\n-      if ((int)dep_type == 0)\n-\tSET_BIT (true_dependency_cache[INSN_LUID (insn)], INSN_LUID (elem));\n-      else if (dep_type == REG_DEP_ANTI)\n-\tSET_BIT (anti_dependency_cache[INSN_LUID (insn)], INSN_LUID (elem));\n-      else if (dep_type == REG_DEP_OUTPUT)\n-\tSET_BIT (output_dependency_cache[INSN_LUID (insn)], INSN_LUID (elem));\n-    }\n-#endif\n-}\n-\n-/* Remove ELEM wrapped in an INSN_LIST from the LOG_LINKS\n-   of INSN.  Abort if not found.  */\n-\n-static void\n-remove_dependence (insn, elem)\n-     rtx insn;\n-     rtx elem;\n-{\n-  rtx prev, link, next;\n-  int found = 0;\n-\n-  for (prev = 0, link = LOG_LINKS (insn); link; link = next)\n-    {\n-      next = XEXP (link, 1);\n-      if (XEXP (link, 0) == elem)\n-\t{\n-\t  if (prev)\n-\t    XEXP (prev, 1) = next;\n-\t  else\n-\t    LOG_LINKS (insn) = next;\n-\n-#ifdef INSN_SCHEDULING\n-\t  /* If we are removing a dependency from the LOG_LINKS list,\n-\t     make sure to remove it from the cache too.  */\n-\t  if (true_dependency_cache != NULL)\n-\t    {\n-\t      if (REG_NOTE_KIND (link) == 0)\n-\t\tRESET_BIT (true_dependency_cache[INSN_LUID (insn)],\n-\t\t\t   INSN_LUID (elem));\n-\t      else if (REG_NOTE_KIND (link) == REG_DEP_ANTI)\n-\t\tRESET_BIT (anti_dependency_cache[INSN_LUID (insn)],\n-\t\t\t   INSN_LUID (elem));\n-\t      else if (REG_NOTE_KIND (link) == REG_DEP_OUTPUT)\n-\t\tRESET_BIT (output_dependency_cache[INSN_LUID (insn)],\n-\t\t\t   INSN_LUID (elem));\n-\t    }\n-#endif\n-\n-\t  free_INSN_LIST_node (link);\n-\n-\t  found = 1;\n-\t}\n-      else\n-\tprev = link;\n-    }\n-\n-  if (!found)\n-    abort ();\n-  return;\n-}\n-\n-/* Return the INSN_LIST containing INSN in LIST, or NULL\n-   if LIST does not contain INSN.  */\n-\n-static inline rtx\n-find_insn_list (insn, list)\n-     rtx insn;\n-     rtx list;\n-{\n-  while (list)\n-    {\n-      if (XEXP (list, 0) == insn)\n-\treturn list;\n-      list = XEXP (list, 1);\n-    }\n-  return 0;\n-}\n-\n-/* Set SCHED_GROUP_P and care for the rest of the bookkeeping that\n-   goes along with that.  */\n-\n-static void\n-set_sched_group_p (insn)\n-     rtx insn;\n-{\n-  rtx link, prev;\n-\n-  SCHED_GROUP_P (insn) = 1;\n-\n-  /* There may be a note before this insn now, but all notes will\n-     be removed before we actually try to schedule the insns, so\n-     it won't cause a problem later.  We must avoid it here though.  */\n-  prev = prev_nonnote_insn (insn);\n-\n-  /* Make a copy of all dependencies on the immediately previous insn,\n-     and add to this insn.  This is so that all the dependencies will\n-     apply to the group.  Remove an explicit dependence on this insn\n-     as SCHED_GROUP_P now represents it.  */\n-\n-  if (find_insn_list (prev, LOG_LINKS (insn)))\n-    remove_dependence (insn, prev);\n-\n-  for (link = LOG_LINKS (prev); link; link = XEXP (link, 1))\n-    add_dependence (insn, XEXP (link, 0), REG_NOTE_KIND (link));\n-}\n-\n-/* If it is profitable to use them, initialize caches for tracking\n-   dependency informatino.  LUID is the number of insns to be scheduled,\n-   it is used in the estimate of profitability.  */\n-static void\n-init_dependency_caches (luid)\n-     int luid;\n-{\n-  /* ?!? We could save some memory by computing a per-region luid mapping\n-     which could reduce both the number of vectors in the cache and the size\n-     of each vector.  Instead we just avoid the cache entirely unless the\n-     average number of instructions in a basic block is very high.  See\n-     the comment before the declaration of true_dependency_cache for\n-     what we consider \"very high\".  */\n-  if (luid / n_basic_blocks > 100 * 5)\n-    {\n-      true_dependency_cache = sbitmap_vector_alloc (luid, luid);\n-      sbitmap_vector_zero (true_dependency_cache, luid);\n-      anti_dependency_cache = sbitmap_vector_alloc (luid, luid);\n-      sbitmap_vector_zero (anti_dependency_cache, luid);\n-      output_dependency_cache = sbitmap_vector_alloc (luid, luid);\n-      sbitmap_vector_zero (output_dependency_cache, luid);\n-#ifdef ENABLE_CHECKING\n-      forward_dependency_cache = sbitmap_vector_alloc (luid, luid);\n-      sbitmap_vector_zero (forward_dependency_cache, luid);\n-#endif\n-    }\n-}\n-\n-/* Free the caches allocated in init_dependency_caches.  */\n-static void\n-free_dependency_caches ()\n-{\n-  if (true_dependency_cache)\n-    {\n-      free (true_dependency_cache);\n-      true_dependency_cache = NULL;\n-      free (anti_dependency_cache);\n-      anti_dependency_cache = NULL;\n-      free (output_dependency_cache);\n-      output_dependency_cache = NULL;\n-#ifdef ENABLE_CHECKING\n-      free (forward_dependency_cache);\n-      forward_dependency_cache = NULL;\n-#endif\n-    }\n-}\n \f\n #ifndef INSN_SCHEDULING\n void\n@@ -2853,37 +2381,7 @@ is_exception_free (insn, bb_src, bb_trg)\n \n   return flag_schedule_speculative_load_dangerous;\n }\n-\n-/* Process an insn's memory dependencies.  There are four kinds of\n-   dependencies:\n-\n-   (0) read dependence: read follows read\n-   (1) true dependence: read follows write\n-   (2) anti dependence: write follows read\n-   (3) output dependence: write follows write\n-\n-   We are careful to build only dependencies which actually exist, and\n-   use transitivity to avoid building too many links.  */\n \f\n-/* Return 1 if the pair (insn, x) is found in (LIST, LIST1), or 0\n-   otherwise.  */\n-\n-HAIFA_INLINE static char\n-find_insn_mem_list (insn, x, list, list1)\n-     rtx insn, x;\n-     rtx list, list1;\n-{\n-  while (list)\n-    {\n-      if (XEXP (list, 0) == insn\n-\t  && XEXP (list1, 0) == x)\n-\treturn 1;\n-      list = XEXP (list, 1);\n-      list1 = XEXP (list1, 1);\n-    }\n-  return 0;\n-}\n-\n /* Compute the function units used by INSN.  This caches the value\n    returned by function_units_used.  A function unit is encoded as the\n    unit number if the value is non-negative and the compliment of a\n@@ -3299,836 +2797,6 @@ free_pending_lists ()\n       free_EXPR_LIST_list (&bb_deps[bb].pending_write_mems);\n     }\n }\n-\n-/* Add an INSN and MEM reference pair to a pending INSN_LIST and MEM_LIST.\n-   The MEM is a memory reference contained within INSN, which we are saving\n-   so that we can do memory aliasing on it.  */\n-\n-static void\n-add_insn_mem_dependence (deps, insn_list, mem_list, insn, mem)\n-     struct deps *deps;\n-     rtx *insn_list, *mem_list, insn, mem;\n-{\n-  register rtx link;\n-\n-  link = alloc_INSN_LIST (insn, *insn_list);\n-  *insn_list = link;\n-\n-  link = alloc_EXPR_LIST (VOIDmode, mem, *mem_list);\n-  *mem_list = link;\n-\n-  deps->pending_lists_length++;\n-}\n-\f\n-/* Make a dependency between every memory reference on the pending lists\n-   and INSN, thus flushing the pending lists.  If ONLY_WRITE, don't flush\n-   the read list.  */\n-\n-static void\n-flush_pending_lists (deps, insn, only_write)\n-     struct deps *deps;\n-     rtx insn;\n-     int only_write;\n-{\n-  rtx u;\n-  rtx link;\n-\n-  while (deps->pending_read_insns && ! only_write)\n-    {\n-      add_dependence (insn, XEXP (deps->pending_read_insns, 0),\n-\t\t      REG_DEP_ANTI);\n-\n-      link = deps->pending_read_insns;\n-      deps->pending_read_insns = XEXP (deps->pending_read_insns, 1);\n-      free_INSN_LIST_node (link);\n-\n-      link = deps->pending_read_mems;\n-      deps->pending_read_mems = XEXP (deps->pending_read_mems, 1);\n-      free_EXPR_LIST_node (link);\n-    }\n-  while (deps->pending_write_insns)\n-    {\n-      add_dependence (insn, XEXP (deps->pending_write_insns, 0),\n-\t\t      REG_DEP_ANTI);\n-\n-      link = deps->pending_write_insns;\n-      deps->pending_write_insns = XEXP (deps->pending_write_insns, 1);\n-      free_INSN_LIST_node (link);\n-\n-      link = deps->pending_write_mems;\n-      deps->pending_write_mems = XEXP (deps->pending_write_mems, 1);\n-      free_EXPR_LIST_node (link);\n-    }\n-  deps->pending_lists_length = 0;\n-\n-  /* last_pending_memory_flush is now a list of insns.  */\n-  for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n-    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-  free_INSN_LIST_list (&deps->last_pending_memory_flush);\n-  deps->last_pending_memory_flush = alloc_INSN_LIST (insn, NULL_RTX);\n-}\n-\n-/* Analyze a single SET, CLOBBER, PRE_DEC, POST_DEC, PRE_INC or POST_INC\n-   rtx, X, creating all dependencies generated by the write to the\n-   destination of X, and reads of everything mentioned.  */\n-\n-static void\n-sched_analyze_1 (deps, x, insn)\n-     struct deps *deps;\n-     rtx x;\n-     rtx insn;\n-{\n-  register int regno;\n-  register rtx dest = XEXP (x, 0);\n-  enum rtx_code code = GET_CODE (x);\n-\n-  if (dest == 0)\n-    return;\n-\n-  if (GET_CODE (dest) == PARALLEL\n-      && GET_MODE (dest) == BLKmode)\n-    {\n-      register int i;\n-      for (i = XVECLEN (dest, 0) - 1; i >= 0; i--)\n-\tsched_analyze_1 (deps, XVECEXP (dest, 0, i), insn);\n-      if (GET_CODE (x) == SET)\n-\tsched_analyze_2 (deps, SET_SRC (x), insn);\n-      return;\n-    }\n-\n-  while (GET_CODE (dest) == STRICT_LOW_PART || GET_CODE (dest) == SUBREG\n-\t || GET_CODE (dest) == ZERO_EXTRACT || GET_CODE (dest) == SIGN_EXTRACT)\n-    {\n-      if (GET_CODE (dest) == ZERO_EXTRACT || GET_CODE (dest) == SIGN_EXTRACT)\n-\t{\n-\t  /* The second and third arguments are values read by this insn.  */\n-\t  sched_analyze_2 (deps, XEXP (dest, 1), insn);\n-\t  sched_analyze_2 (deps, XEXP (dest, 2), insn);\n-\t}\n-      dest = XEXP (dest, 0);\n-    }\n-\n-  if (GET_CODE (dest) == REG)\n-    {\n-      register int i;\n-\n-      regno = REGNO (dest);\n-\n-      /* A hard reg in a wide mode may really be multiple registers.\n-         If so, mark all of them just like the first.  */\n-      if (regno < FIRST_PSEUDO_REGISTER)\n-\t{\n-\t  i = HARD_REGNO_NREGS (regno, GET_MODE (dest));\n-\t  while (--i >= 0)\n-\t    {\n-\t      int r = regno + i;\n-\t      rtx u;\n-\n-\t      for (u = deps->reg_last_uses[r]; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-\t      for (u = deps->reg_last_sets[r]; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n-\n-\t      /* Clobbers need not be ordered with respect to one\n-\t\t another, but sets must be ordered with respect to a\n-\t\t pending clobber.  */\n-\t      if (code == SET)\n-\t\t{\n-\t\t  free_INSN_LIST_list (&deps->reg_last_uses[r]);\n-\t\t  for (u = deps->reg_last_clobbers[r]; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n-\t\t  SET_REGNO_REG_SET (reg_pending_sets, r);\n-\t\t}\n-\t      else\n-\t\tSET_REGNO_REG_SET (reg_pending_clobbers, r);\n-\n-\t      /* Function calls clobber all call_used regs.  */\n-\t      if (global_regs[r] || (code == SET && call_used_regs[r]))\n-\t\tfor (u = deps->last_function_call; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t    }\n-\t}\n-      else\n-\t{\n-\t  rtx u;\n-\n-\t  for (u = deps->reg_last_uses[regno]; u; u = XEXP (u, 1))\n-\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-\t  for (u = deps->reg_last_sets[regno]; u; u = XEXP (u, 1))\n-\t    add_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n-\n-\t  if (code == SET)\n-\t    {\n-\t      free_INSN_LIST_list (&deps->reg_last_uses[regno]);\n-\t      for (u = deps->reg_last_clobbers[regno]; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n-\t      SET_REGNO_REG_SET (reg_pending_sets, regno);\n-\t    }\n-\t  else\n-\t    SET_REGNO_REG_SET (reg_pending_clobbers, regno);\n-\n-\t  /* Pseudos that are REG_EQUIV to something may be replaced\n-\t     by that during reloading.  We need only add dependencies for\n-\t     the address in the REG_EQUIV note.  */\n-\t  if (!reload_completed\n-\t      && reg_known_equiv_p[regno]\n-\t      && GET_CODE (reg_known_value[regno]) == MEM)\n-\t    sched_analyze_2 (deps, XEXP (reg_known_value[regno], 0), insn);\n-\n-\t  /* Don't let it cross a call after scheduling if it doesn't\n-\t     already cross one.  */\n-\n-\t  if (REG_N_CALLS_CROSSED (regno) == 0)\n-\t    for (u = deps->last_function_call; u; u = XEXP (u, 1))\n-\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t}\n-    }\n-  else if (GET_CODE (dest) == MEM)\n-    {\n-      /* Writing memory.  */\n-\n-      if (deps->pending_lists_length > 32)\n-\t{\n-\t  /* Flush all pending reads and writes to prevent the pending lists\n-\t     from getting any larger.  Insn scheduling runs too slowly when\n-\t     these lists get long.  The number 32 was chosen because it\n-\t     seems like a reasonable number.  When compiling GCC with itself,\n-\t     this flush occurs 8 times for sparc, and 10 times for m88k using\n-\t     the number 32.  */\n-\t  flush_pending_lists (deps, insn, 0);\n-\t}\n-      else\n-\t{\n-\t  rtx u;\n-\t  rtx pending, pending_mem;\n-\n-\t  pending = deps->pending_read_insns;\n-\t  pending_mem = deps->pending_read_mems;\n-\t  while (pending)\n-\t    {\n-\t      if (anti_dependence (XEXP (pending_mem, 0), dest))\n-\t\tadd_dependence (insn, XEXP (pending, 0), REG_DEP_ANTI);\n-\n-\t      pending = XEXP (pending, 1);\n-\t      pending_mem = XEXP (pending_mem, 1);\n-\t    }\n-\n-\t  pending = deps->pending_write_insns;\n-\t  pending_mem = deps->pending_write_mems;\n-\t  while (pending)\n-\t    {\n-\t      if (output_dependence (XEXP (pending_mem, 0), dest))\n-\t\tadd_dependence (insn, XEXP (pending, 0), REG_DEP_OUTPUT);\n-\n-\t      pending = XEXP (pending, 1);\n-\t      pending_mem = XEXP (pending_mem, 1);\n-\t    }\n-\n-\t  for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n-\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-\t  add_insn_mem_dependence (deps, &deps->pending_write_insns,\n-\t\t\t\t   &deps->pending_write_mems, insn, dest);\n-\t}\n-      sched_analyze_2 (deps, XEXP (dest, 0), insn);\n-    }\n-\n-  /* Analyze reads.  */\n-  if (GET_CODE (x) == SET)\n-    sched_analyze_2 (deps, SET_SRC (x), insn);\n-}\n-\n-/* Analyze the uses of memory and registers in rtx X in INSN.  */\n-\n-static void\n-sched_analyze_2 (deps, x, insn)\n-     struct deps *deps;\n-     rtx x;\n-     rtx insn;\n-{\n-  register int i;\n-  register int j;\n-  register enum rtx_code code;\n-  register const char *fmt;\n-\n-  if (x == 0)\n-    return;\n-\n-  code = GET_CODE (x);\n-\n-  switch (code)\n-    {\n-    case CONST_INT:\n-    case CONST_DOUBLE:\n-    case SYMBOL_REF:\n-    case CONST:\n-    case LABEL_REF:\n-      /* Ignore constants.  Note that we must handle CONST_DOUBLE here\n-         because it may have a cc0_rtx in its CONST_DOUBLE_CHAIN field, but\n-         this does not mean that this insn is using cc0.  */\n-      return;\n-\n-#ifdef HAVE_cc0\n-    case CC0:\n-      /* User of CC0 depends on immediately preceding insn.  */\n-      set_sched_group_p (insn);\n-      return;\n-#endif\n-\n-    case REG:\n-      {\n-\trtx u;\n-\tint regno = REGNO (x);\n-\tif (regno < FIRST_PSEUDO_REGISTER)\n-\t  {\n-\t    int i;\n-\n-\t    i = HARD_REGNO_NREGS (regno, GET_MODE (x));\n-\t    while (--i >= 0)\n-\t      {\n-\t\tint r = regno + i;\n-\t\tdeps->reg_last_uses[r]\n-\t\t  = alloc_INSN_LIST (insn, deps->reg_last_uses[r]);\n-\n-\t\tfor (u = deps->reg_last_sets[r]; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t\t/* ??? This should never happen.  */\n-\t\tfor (u = deps->reg_last_clobbers[r]; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t\tif (call_used_regs[r] || global_regs[r])\n-\t\t  /* Function calls clobber all call_used regs.  */\n-\t\t  for (u = deps->last_function_call; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t      }\n-\t  }\n-\telse\n-\t  {\n-\t    deps->reg_last_uses[regno]\n-\t      = alloc_INSN_LIST (insn, deps->reg_last_uses[regno]);\n-\n-\t    for (u = deps->reg_last_sets[regno]; u; u = XEXP (u, 1))\n-\t      add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t    /* ??? This should never happen.  */\n-\t    for (u = deps->reg_last_clobbers[regno]; u; u = XEXP (u, 1))\n-\t      add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t    /* Pseudos that are REG_EQUIV to something may be replaced\n-\t       by that during reloading.  We need only add dependencies for\n-\t       the address in the REG_EQUIV note.  */\n-\t    if (!reload_completed\n-\t\t&& reg_known_equiv_p[regno]\n-\t\t&& GET_CODE (reg_known_value[regno]) == MEM)\n-\t      sched_analyze_2 (deps, XEXP (reg_known_value[regno], 0), insn);\n-\n-\t    /* If the register does not already cross any calls, then add this\n-\t       insn to the sched_before_next_call list so that it will still\n-\t       not cross calls after scheduling.  */\n-\t    if (REG_N_CALLS_CROSSED (regno) == 0)\n-\t      add_dependence (deps->sched_before_next_call, insn,\n-\t\t\t      REG_DEP_ANTI);\n-\t  }\n-\treturn;\n-      }\n-\n-    case MEM:\n-      {\n-\t/* Reading memory.  */\n-\trtx u;\n-\trtx pending, pending_mem;\n-\n-\tpending = deps->pending_read_insns;\n-\tpending_mem = deps->pending_read_mems;\n-\twhile (pending)\n-\t  {\n-\t    if (read_dependence (XEXP (pending_mem, 0), x))\n-\t      add_dependence (insn, XEXP (pending, 0), REG_DEP_ANTI);\n-\n-\t    pending = XEXP (pending, 1);\n-\t    pending_mem = XEXP (pending_mem, 1);\n-\t  }\n-\n-\tpending = deps->pending_write_insns;\n-\tpending_mem = deps->pending_write_mems;\n-\twhile (pending)\n-\t  {\n-\t    if (true_dependence (XEXP (pending_mem, 0), VOIDmode,\n-\t\t\t\t x, rtx_varies_p))\n-\t      add_dependence (insn, XEXP (pending, 0), 0);\n-\n-\t    pending = XEXP (pending, 1);\n-\t    pending_mem = XEXP (pending_mem, 1);\n-\t  }\n-\n-\tfor (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n-\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-\t/* Always add these dependencies to pending_reads, since\n-\t   this insn may be followed by a write.  */\n-\tadd_insn_mem_dependence (deps, &deps->pending_read_insns,\n-\t\t\t\t &deps->pending_read_mems, insn, x);\n-\n-\t/* Take advantage of tail recursion here.  */\n-\tsched_analyze_2 (deps, XEXP (x, 0), insn);\n-\treturn;\n-      }\n-\n-    /* Force pending stores to memory in case a trap handler needs them.  */\n-    case TRAP_IF:\n-      flush_pending_lists (deps, insn, 1);\n-      break;\n-\n-    case ASM_OPERANDS:\n-    case ASM_INPUT:\n-    case UNSPEC_VOLATILE:\n-      {\n-\trtx u;\n-\n-\t/* Traditional and volatile asm instructions must be considered to use\n-\t   and clobber all hard registers, all pseudo-registers and all of\n-\t   memory.  So must TRAP_IF and UNSPEC_VOLATILE operations.\n-\n-\t   Consider for instance a volatile asm that changes the fpu rounding\n-\t   mode.  An insn should not be moved across this even if it only uses\n-\t   pseudo-regs because it might give an incorrectly rounded result.  */\n-\tif (code != ASM_OPERANDS || MEM_VOLATILE_P (x))\n-\t  {\n-\t    int max_reg = max_reg_num ();\n-\t    for (i = 0; i < max_reg; i++)\n-\t      {\n-\t\tfor (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t\tfree_INSN_LIST_list (&deps->reg_last_uses[i]);\n-\n-\t\tfor (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t\tfor (u = deps->reg_last_clobbers[i]; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), 0);\n-\t      }\n-\t    reg_pending_sets_all = 1;\n-\n-\t    flush_pending_lists (deps, insn, 0);\n-\t  }\n-\n-\t/* For all ASM_OPERANDS, we must traverse the vector of input operands.\n-\t   We can not just fall through here since then we would be confused\n-\t   by the ASM_INPUT rtx inside ASM_OPERANDS, which do not indicate\n-\t   traditional asms unlike their normal usage.  */\n-\n-\tif (code == ASM_OPERANDS)\n-\t  {\n-\t    for (j = 0; j < ASM_OPERANDS_INPUT_LENGTH (x); j++)\n-\t      sched_analyze_2 (deps, ASM_OPERANDS_INPUT (x, j), insn);\n-\t    return;\n-\t  }\n-\tbreak;\n-      }\n-\n-    case PRE_DEC:\n-    case POST_DEC:\n-    case PRE_INC:\n-    case POST_INC:\n-      /* These both read and modify the result.  We must handle them as writes\n-         to get proper dependencies for following instructions.  We must handle\n-         them as reads to get proper dependencies from this to previous\n-         instructions.  Thus we need to pass them to both sched_analyze_1\n-         and sched_analyze_2.  We must call sched_analyze_2 first in order\n-         to get the proper antecedent for the read.  */\n-      sched_analyze_2 (deps, XEXP (x, 0), insn);\n-      sched_analyze_1 (deps, x, insn);\n-      return;\n-\n-    case POST_MODIFY:\n-    case PRE_MODIFY:\n-      /* op0 = op0 + op1 */\n-      sched_analyze_2 (deps, XEXP (x, 0), insn);\n-      sched_analyze_2 (deps, XEXP (x, 1), insn);\n-      sched_analyze_1 (deps, x, insn);\n-      return;\n-\n-    default:\n-      break;\n-    }\n-\n-  /* Other cases: walk the insn.  */\n-  fmt = GET_RTX_FORMAT (code);\n-  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n-    {\n-      if (fmt[i] == 'e')\n-\tsched_analyze_2 (deps, XEXP (x, i), insn);\n-      else if (fmt[i] == 'E')\n-\tfor (j = 0; j < XVECLEN (x, i); j++)\n-\t  sched_analyze_2 (deps, XVECEXP (x, i, j), insn);\n-    }\n-}\n-\n-/* Analyze an INSN with pattern X to find all dependencies.  */\n-\n-static void\n-sched_analyze_insn (deps, x, insn, loop_notes)\n-     struct deps *deps;\n-     rtx x, insn;\n-     rtx loop_notes;\n-{\n-  register RTX_CODE code = GET_CODE (x);\n-  rtx link;\n-  int maxreg = max_reg_num ();\n-  int i;\n-\n-  if (code == COND_EXEC)\n-    {\n-      sched_analyze_2 (deps, COND_EXEC_TEST (x), insn);\n-\n-      /* ??? Should be recording conditions so we reduce the number of\n-\t false dependancies.  */\n-      x = COND_EXEC_CODE (x);\n-      code = GET_CODE (x);\n-    }\n-  if (code == SET || code == CLOBBER)\n-    sched_analyze_1 (deps, x, insn);\n-  else if (code == PARALLEL)\n-    {\n-      register int i;\n-      for (i = XVECLEN (x, 0) - 1; i >= 0; i--)\n-\t{\n-\t  rtx sub = XVECEXP (x, 0, i);\n-\t  code = GET_CODE (sub);\n-\n-\t  if (code == COND_EXEC)\n-\t    {\n-\t      sched_analyze_2 (deps, COND_EXEC_TEST (sub), insn);\n-\t      sub = COND_EXEC_CODE (sub);\n-\t      code = GET_CODE (sub);\n-\t    }\n-\t  if (code == SET || code == CLOBBER)\n-\t    sched_analyze_1 (deps, sub, insn);\n-\t  else\n-\t    sched_analyze_2 (deps, sub, insn);\n-\t}\n-    }\n-  else\n-    sched_analyze_2 (deps, x, insn);\n-\n-  /* Mark registers CLOBBERED or used by called function.  */\n-  if (GET_CODE (insn) == CALL_INSN)\n-    for (link = CALL_INSN_FUNCTION_USAGE (insn); link; link = XEXP (link, 1))\n-      {\n-\tif (GET_CODE (XEXP (link, 0)) == CLOBBER)\n-\t  sched_analyze_1 (deps, XEXP (link, 0), insn);\n-\telse\n-\t  sched_analyze_2 (deps, XEXP (link, 0), insn);\n-      }\n-\n-  /* If there is a {LOOP,EHREGION}_{BEG,END} note in the middle of a basic\n-     block, then we must be sure that no instructions are scheduled across it.\n-     Otherwise, the reg_n_refs info (which depends on loop_depth) would\n-     become incorrect.  */\n-\n-  if (loop_notes)\n-    {\n-      int max_reg = max_reg_num ();\n-      int schedule_barrier_found = 0;\n-      rtx link;\n-\n-      /* Update loop_notes with any notes from this insn.  Also determine\n-\t if any of the notes on the list correspond to instruction scheduling\n-\t barriers (loop, eh & setjmp notes, but not range notes.  */\n-      link = loop_notes;\n-      while (XEXP (link, 1))\n-\t{\n-\t  if (INTVAL (XEXP (link, 0)) == NOTE_INSN_LOOP_BEG\n-\t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_LOOP_END\n-\t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_EH_REGION_BEG\n-\t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_EH_REGION_END\n-\t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_SETJMP)\n-\t    schedule_barrier_found = 1;\n-\n-\t  link = XEXP (link, 1);\n-\t}\n-      XEXP (link, 1) = REG_NOTES (insn);\n-      REG_NOTES (insn) = loop_notes;\n-\n-      /* Add dependencies if a scheduling barrier was found.  */\n-      if (schedule_barrier_found)\n-\t{\n-\t  for (i = 0; i < max_reg; i++)\n-\t    {\n-\t      rtx u;\n-\t      for (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t      free_INSN_LIST_list (&deps->reg_last_uses[i]);\n-\n-\t      for (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), 0);\n-\n-\t      for (u = deps->reg_last_clobbers[i]; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), 0);\n-\t    }\n-\t  reg_pending_sets_all = 1;\n-\n-\t  flush_pending_lists (deps, insn, 0);\n-\t}\n-\n-    }\n-\n-  /* Accumulate clobbers until the next set so that it will be output dependent\n-     on all of them.  At the next set we can clear the clobber list, since\n-     subsequent sets will be output dependent on it.  */\n-  EXECUTE_IF_SET_IN_REG_SET\n-    (reg_pending_sets, 0, i,\n-     {\n-       free_INSN_LIST_list (&deps->reg_last_sets[i]);\n-       free_INSN_LIST_list (&deps->reg_last_clobbers[i]);\n-       deps->reg_last_sets[i] = alloc_INSN_LIST (insn, NULL_RTX);\n-     });\n-  EXECUTE_IF_SET_IN_REG_SET\n-    (reg_pending_clobbers, 0, i,\n-     {\n-       deps->reg_last_clobbers[i]\n-\t = alloc_INSN_LIST (insn, deps->reg_last_clobbers[i]);\n-     });\n-  CLEAR_REG_SET (reg_pending_sets);\n-  CLEAR_REG_SET (reg_pending_clobbers);\n-\n-  if (reg_pending_sets_all)\n-    {\n-      for (i = 0; i < maxreg; i++)\n-\t{\n-\t  free_INSN_LIST_list (&deps->reg_last_sets[i]);\n-\t  free_INSN_LIST_list (&deps->reg_last_clobbers[i]);\n-\t  deps->reg_last_sets[i] = alloc_INSN_LIST (insn, NULL_RTX);\n-\t}\n-\n-      reg_pending_sets_all = 0;\n-    }\n-\n-  /* If a post-call group is still open, see if it should remain so.\n-     This insn must be a simple move of a hard reg to a pseudo or\n-     vice-versa.\n-\n-     We must avoid moving these insns for correctness on\n-     SMALL_REGISTER_CLASS machines, and for special registers like\n-     PIC_OFFSET_TABLE_REGNUM.  For simplicity, extend this to all\n-     hard regs for all targets.  */\n-\n-  if (deps->in_post_call_group_p)\n-    {\n-      rtx tmp, set = single_set (insn);\n-      int src_regno, dest_regno;\n-\n-      if (set == NULL)\n-\tgoto end_call_group;\n-\n-      tmp = SET_DEST (set);\n-      if (GET_CODE (tmp) == SUBREG)\n-\ttmp = SUBREG_REG (tmp);\n-      if (GET_CODE (tmp) == REG)\n-\tdest_regno = REGNO (tmp);\n-      else\n-\tgoto end_call_group;\n-\n-      tmp = SET_SRC (set);\n-      if (GET_CODE (tmp) == SUBREG)\n-\ttmp = SUBREG_REG (tmp);\n-      if (GET_CODE (tmp) == REG)\n-\tsrc_regno = REGNO (tmp);\n-      else\n-\tgoto end_call_group;\n-\n-      if (src_regno < FIRST_PSEUDO_REGISTER\n-\t  || dest_regno < FIRST_PSEUDO_REGISTER)\n-\t{\n-\t  set_sched_group_p (insn);\n-\t  CANT_MOVE (insn) = 1;\n-\t}\n-      else\n-\t{\n-\tend_call_group:\n-\t  deps->in_post_call_group_p = 0;\n-\t}\n-    }\n-}\n-\n-/* Analyze every insn between HEAD and TAIL inclusive, creating LOG_LINKS\n-   for every dependency.  */\n-\n-static void\n-sched_analyze (deps, head, tail)\n-     struct deps *deps;\n-     rtx head, tail;\n-{\n-  register rtx insn;\n-  register rtx u;\n-  rtx loop_notes = 0;\n-\n-  for (insn = head;; insn = NEXT_INSN (insn))\n-    {\n-      if (GET_CODE (insn) == INSN || GET_CODE (insn) == JUMP_INSN)\n-\t{\n-\t  /* Clear out the stale LOG_LINKS from flow.  */\n-\t  free_INSN_LIST_list (&LOG_LINKS (insn));\n-\n-\t  /* Clear out stale SCHED_GROUP_P.  */\n-\t  SCHED_GROUP_P (insn) = 0;\n-\n-\t  /* Make each JUMP_INSN a scheduling barrier for memory\n-             references.  */\n-\t  if (GET_CODE (insn) == JUMP_INSN)\n-\t    deps->last_pending_memory_flush\n-\t      = alloc_INSN_LIST (insn, deps->last_pending_memory_flush);\n-\t  sched_analyze_insn (deps, PATTERN (insn), insn, loop_notes);\n-\t  loop_notes = 0;\n-\t}\n-      else if (GET_CODE (insn) == CALL_INSN)\n-\t{\n-\t  rtx x;\n-\t  register int i;\n-\n-\t  /* Clear out stale SCHED_GROUP_P.  */\n-\t  SCHED_GROUP_P (insn) = 0;\n-\n-\t  CANT_MOVE (insn) = 1;\n-\n-\t  /* Clear out the stale LOG_LINKS from flow.  */\n-\t  free_INSN_LIST_list (&LOG_LINKS (insn));\n-\n-\t  /* Any instruction using a hard register which may get clobbered\n-\t     by a call needs to be marked as dependent on this call.\n-\t     This prevents a use of a hard return reg from being moved\n-\t     past a void call (i.e. it does not explicitly set the hard\n-\t     return reg).  */\n-\n-\t  /* If this call is followed by a NOTE_INSN_SETJMP, then assume that\n-\t     all registers, not just hard registers, may be clobbered by this\n-\t     call.  */\n-\n-\t  /* Insn, being a CALL_INSN, magically depends on\n-\t     `last_function_call' already.  */\n-\n-\t  if (NEXT_INSN (insn) && GET_CODE (NEXT_INSN (insn)) == NOTE\n-\t      && NOTE_LINE_NUMBER (NEXT_INSN (insn)) == NOTE_INSN_SETJMP)\n-\t    {\n-\t      int max_reg = max_reg_num ();\n-\t      for (i = 0; i < max_reg; i++)\n-\t\t{\n-\t\t  for (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t\t  free_INSN_LIST_list (&deps->reg_last_uses[i]);\n-\n-\t\t  for (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t\t  for (u = deps->reg_last_clobbers[i]; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), 0);\n-\t\t}\n-\t      reg_pending_sets_all = 1;\n-\n-\t      /* Add a pair of REG_SAVE_NOTEs which we will later\n-\t\t convert back into a NOTE_INSN_SETJMP note.  See\n-\t\t reemit_notes for why we use a pair of NOTEs.  */\n-\t      REG_NOTES (insn) = alloc_EXPR_LIST (REG_SAVE_NOTE,\n-\t\t\t\t\t\t  GEN_INT (0),\n-\t\t\t\t\t\t  REG_NOTES (insn));\n-\t      REG_NOTES (insn) = alloc_EXPR_LIST (REG_SAVE_NOTE,\n-\t\t\t\t\t\t  GEN_INT (NOTE_INSN_SETJMP),\n-\t\t\t\t\t\t  REG_NOTES (insn));\n-\t    }\n-\t  else\n-\t    {\n-\t      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n-\t\tif (call_used_regs[i] || global_regs[i])\n-\t\t  {\n-\t\t    for (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n-\t\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-\t\t    for (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n-\t\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-\t\t    SET_REGNO_REG_SET (reg_pending_clobbers, i);\n-\t\t  }\n-\t    }\n-\n-\t  /* For each insn which shouldn't cross a call, add a dependence\n-\t     between that insn and this call insn.  */\n-\t  x = LOG_LINKS (deps->sched_before_next_call);\n-\t  while (x)\n-\t    {\n-\t      add_dependence (insn, XEXP (x, 0), REG_DEP_ANTI);\n-\t      x = XEXP (x, 1);\n-\t    }\n-\t  free_INSN_LIST_list (&LOG_LINKS (deps->sched_before_next_call));\n-\n-\t  sched_analyze_insn (deps, PATTERN (insn), insn, loop_notes);\n-\t  loop_notes = 0;\n-\n-\t  /* In the absence of interprocedural alias analysis, we must flush\n-\t     all pending reads and writes, and start new dependencies starting\n-\t     from here.  But only flush writes for constant calls (which may\n-\t     be passed a pointer to something we haven't written yet).  */\n-\t  flush_pending_lists (deps, insn, CONST_CALL_P (insn));\n-\n-\t  /* Depend this function call (actually, the user of this\n-\t     function call) on all hard register clobberage.  */\n-\n-\t  /* last_function_call is now a list of insns.  */\n-\t  free_INSN_LIST_list (&deps->last_function_call);\n-\t  deps->last_function_call = alloc_INSN_LIST (insn, NULL_RTX);\n-\n-\t  /* Before reload, begin a post-call group, so as to keep the\n-\t     lifetimes of hard registers correct.  */\n-\t  if (! reload_completed)\n-\t    deps->in_post_call_group_p = 1;\n-\t}\n-\n-      /* See comments on reemit_notes as to why we do this.\n-\t ??? Actually, the reemit_notes just say what is done, not why.  */\n-\n-      else if (GET_CODE (insn) == NOTE\n-\t       && (NOTE_LINE_NUMBER (insn) == NOTE_INSN_RANGE_BEG\n-\t\t   || NOTE_LINE_NUMBER (insn) == NOTE_INSN_RANGE_END))\n-\t{\n-\t  loop_notes = alloc_EXPR_LIST (REG_SAVE_NOTE, NOTE_RANGE_INFO (insn),\n-\t\t\t\t\tloop_notes);\n-\t  loop_notes = alloc_EXPR_LIST (REG_SAVE_NOTE,\n-\t\t\t\t\tGEN_INT (NOTE_LINE_NUMBER (insn)),\n-\t\t\t\t\tloop_notes);\n-\t}\n-      else if (GET_CODE (insn) == NOTE\n-\t       && (NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_BEG\n-\t\t   || NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_END\n-\t\t   || NOTE_LINE_NUMBER (insn) == NOTE_INSN_EH_REGION_BEG\n-\t\t   || NOTE_LINE_NUMBER (insn) == NOTE_INSN_EH_REGION_END\n-\t\t   || (NOTE_LINE_NUMBER (insn) == NOTE_INSN_SETJMP\n-\t\t       && GET_CODE (PREV_INSN (insn)) != CALL_INSN)))\n-\t{\n-\t  rtx rtx_region;\n-\n-\t  if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_EH_REGION_BEG\n-\t      || NOTE_LINE_NUMBER (insn) == NOTE_INSN_EH_REGION_END)\n-\t    rtx_region = GEN_INT (NOTE_EH_HANDLER (insn));\n-\t  else\n-\t    rtx_region = GEN_INT (0);\n-\n-\t  loop_notes = alloc_EXPR_LIST (REG_SAVE_NOTE,\n-\t\t\t\t\trtx_region,\n-\t\t\t\t\tloop_notes);\n-\t  loop_notes = alloc_EXPR_LIST (REG_SAVE_NOTE,\n-\t\t\t\t\tGEN_INT (NOTE_LINE_NUMBER (insn)),\n-\t\t\t\t\tloop_notes);\n-\t  CONST_CALL_P (loop_notes) = CONST_CALL_P (insn);\n-\t}\n-\n-      if (insn == tail)\n-\treturn;\n-    }\n-  abort ();\n-}\n \f\n /* Macros and functions for keeping the priority queue sorted, and\n    dealing with queueing and dequeueing of instructions.  */\n@@ -5305,25 +3973,6 @@ move_insn (insn, last)\n   return retval;\n }\n \n-/* Return an insn which represents a SCHED_GROUP, which is\n-   the last insn in the group.  */\n-\n-static rtx\n-group_leader (insn)\n-     rtx insn;\n-{\n-  rtx prev;\n-\n-  do\n-    {\n-      prev = insn;\n-      insn = next_nonnote_insn (insn);\n-    }\n-  while (insn && SCHED_GROUP_P (insn) && (GET_CODE (insn) != CODE_LABEL));\n-\n-  return prev;\n-}\n-\n /* Use forward list scheduling to rearrange insns of block BB in region RGN,\n    possibly bringing insns from subsequent blocks in the same region.  */\n \n@@ -5545,117 +4194,6 @@ debug_reg_vector (s)\n   fprintf (sched_dump, \"\\n\");\n }\n \n-/* Examine insns in the range [ HEAD, TAIL ] and Use the backward\n-   dependences from LOG_LINKS to build forward dependences in\n-   INSN_DEPEND.  */\n-\n-static void\n-compute_forward_dependences (head, tail)\n-     rtx head, tail;\n-{\n-  rtx insn, link;\n-  rtx next_tail;\n-  enum reg_note dep_type;\n-\n-  next_tail = NEXT_INSN (tail);\n-  for (insn = head; insn != next_tail; insn = NEXT_INSN (insn))\n-    {\n-      if (! INSN_P (insn))\n-\tcontinue;\n-\n-      insn = group_leader (insn);\n-\n-      for (link = LOG_LINKS (insn); link; link = XEXP (link, 1))\n-\t{\n-\t  rtx x = group_leader (XEXP (link, 0));\n-\t  rtx new_link;\n-\n-\t  if (x != XEXP (link, 0))\n-\t    continue;\n-\n-#ifdef ENABLE_CHECKING\n-\t  /* If add_dependence is working properly there should never\n-\t     be notes, deleted insns or duplicates in the backward\n-\t     links.  Thus we need not check for them here.\n-\n-\t     However, if we have enabled checking we might as well go\n-\t     ahead and verify that add_dependence worked properly.  */\n-\t  if (GET_CODE (x) == NOTE\n-\t      || INSN_DELETED_P (x)\n-\t      || (forward_dependency_cache != NULL\n-\t\t  && TEST_BIT (forward_dependency_cache[INSN_LUID (x)],\n-\t\t\t       INSN_LUID (insn)))\n-\t      || (forward_dependency_cache == NULL\n-\t\t  && find_insn_list (insn, INSN_DEPEND (x))))\n-\t    abort ();\n-\t  if (forward_dependency_cache != NULL)\n-\t    SET_BIT (forward_dependency_cache[INSN_LUID (x)],\n-\t\t     INSN_LUID (insn));\n-#endif\n-\n-\t  new_link = alloc_INSN_LIST (insn, INSN_DEPEND (x));\n-\n-\t  dep_type = REG_NOTE_KIND (link);\n-\t  PUT_REG_NOTE_KIND (new_link, dep_type);\n-\n-\t  INSN_DEPEND (x) = new_link;\n-\t  INSN_DEP_COUNT (insn) += 1;\n-\t}\n-    }\n-}\n-\n-/* Initialize variables for region data dependence analysis.\n-   n_bbs is the number of region blocks.  */\n-\n-static void\n-init_deps (deps)\n-     struct deps *deps;\n-{\n-  int maxreg = max_reg_num ();\n-  deps->reg_last_uses = (rtx *) xcalloc (maxreg, sizeof (rtx));\n-  deps->reg_last_sets = (rtx *) xcalloc (maxreg, sizeof (rtx));\n-  deps->reg_last_clobbers = (rtx *) xcalloc (maxreg, sizeof (rtx));\n-\n-  deps->pending_read_insns = 0;\n-  deps->pending_read_mems = 0;\n-  deps->pending_write_insns = 0;\n-  deps->pending_write_mems = 0;\n-  deps->pending_lists_length = 0;\n-  deps->last_pending_memory_flush = 0;\n-  deps->last_function_call = 0;\n-  deps->in_post_call_group_p = 0;\n-\n-  deps->sched_before_next_call\n-    = gen_rtx_INSN (VOIDmode, 0, NULL_RTX, NULL_RTX,\n-\t\t    NULL_RTX, 0, NULL_RTX, NULL_RTX);\n-  LOG_LINKS (deps->sched_before_next_call) = 0;\n-}\n-\n-/* Free insn lists found in DEPS.  */\n-\n-static void\n-free_deps (deps)\n-     struct deps *deps;\n-{\n-  int max_reg = max_reg_num ();\n-  int i;\n-\n-  /* Note this loop is executed max_reg * nr_regions times.  It's first\n-     implementation accounted for over 90% of the calls to free_INSN_LIST_list.\n-     The list was empty for the vast majority of those calls.  On the PA, not\n-     calling free_INSN_LIST_list in those cases improves -O2 compile times by\n-     3-5% on average.  */\n-  for (i = 0; i < max_reg; ++i)\n-    {\n-      if (deps->reg_last_clobbers[i])\n-\tfree_INSN_LIST_list (&deps->reg_last_clobbers[i]);\n-      if (deps->reg_last_sets[i])\n-\tfree_INSN_LIST_list (&deps->reg_last_sets[i]);\n-      if (deps->reg_last_uses[i])\n-\tfree_INSN_LIST_list (&deps->reg_last_uses[i]);\n-    }\n-}\n-\n /* Add dependences so that branches are scheduled to run last in their\n    block.  */\n \n@@ -6051,16 +4589,12 @@ schedule_region (rgn)\n   int bb;\n   int rgn_n_insns = 0;\n   int sched_rgn_n_insns = 0;\n-  regset_head reg_pending_sets_head;\n-  regset_head reg_pending_clobbers_head;\n \n   /* Set variables for the current region.  */\n   current_nr_blocks = RGN_NR_BLOCKS (rgn);\n   current_blocks = RGN_BLOCKS (rgn);\n \n-  reg_pending_sets = INITIALIZE_REG_SET (reg_pending_sets_head);\n-  reg_pending_clobbers = INITIALIZE_REG_SET (reg_pending_clobbers_head);\n-  reg_pending_sets_all = 0;\n+  init_deps_global ();\n \n   /* Initializations for region data dependence analyisis.  */\n   bb_deps = (struct deps *) xmalloc (sizeof (struct deps) * current_nr_blocks);\n@@ -6218,8 +4752,7 @@ schedule_region (rgn)\n   /* Done with this region.  */\n   free_pending_lists ();\n \n-  FREE_REG_SET (reg_pending_sets);\n-  FREE_REG_SET (reg_pending_clobbers);\n+  finish_deps_global ();\n \n   free (bb_deps);\n "}, {"sha": "7f9914cdd82be5225faba2779fed1c3fc155d1e1", "filename": "gcc/sched-deps.c", "status": "added", "additions": 1399, "deletions": 0, "changes": 1399, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/16f6ece6429c36ad89d4062c02f3af72168fdea9/gcc%2Fsched-deps.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/16f6ece6429c36ad89d4062c02f3af72168fdea9/gcc%2Fsched-deps.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-deps.c?ref=16f6ece6429c36ad89d4062c02f3af72168fdea9", "patch": "@@ -0,0 +1,1399 @@\n+/* Instruction scheduling pass.  This file computes dependencies between\n+   instructions.\n+   Copyright (C) 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n+   1999, 2000 Free Software Foundation, Inc.\n+   Contributed by Michael Tiemann (tiemann@cygnus.com) Enhanced by,\n+   and currently maintained by, Jim Wilson (wilson@cygnus.com)\n+\n+This file is part of GNU CC.\n+\n+GNU CC is free software; you can redistribute it and/or modify it\n+under the terms of the GNU General Public License as published by the\n+Free Software Foundation; either version 2, or (at your option) any\n+later version.\n+\n+GNU CC is distributed in the hope that it will be useful, but WITHOUT\n+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GNU CC; see the file COPYING.  If not, write to the Free\n+the Free Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\f\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"toplev.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"regs.h\"\n+#include \"function.h\"\n+#include \"flags.h\"\n+#include \"insn-config.h\"\n+#include \"insn-attr.h\"\n+#include \"except.h\"\n+#include \"toplev.h\"\n+#include \"recog.h\"\n+#include \"sched-int.h\"\n+\n+extern char *reg_known_equiv_p;\n+extern rtx *reg_known_value;\n+\n+static regset_head reg_pending_sets_head;\n+static regset_head reg_pending_clobbers_head;\n+\n+static regset reg_pending_sets;\n+static regset reg_pending_clobbers;\n+static int reg_pending_sets_all;\n+\n+/* To speed up the test for duplicate dependency links we keep a\n+   record of dependencies created by add_dependence when the average\n+   number of instructions in a basic block is very large.\n+\n+   Studies have shown that there is typically around 5 instructions between\n+   branches for typical C code.  So we can make a guess that the average\n+   basic block is approximately 5 instructions long; we will choose 100X\n+   the average size as a very large basic block.\n+\n+   Each insn has associated bitmaps for its dependencies.  Each bitmap\n+   has enough entries to represent a dependency on any other insn in\n+   the insn chain.  All bitmap for true dependencies cache is\n+   allocated then the rest two ones are also allocated. */\n+static sbitmap *true_dependency_cache;\n+static sbitmap *anti_dependency_cache;\n+static sbitmap *output_dependency_cache;\n+\n+/* To speed up checking consistency of formed forward insn\n+   dependencies we use the following cache.  Another possible solution\n+   could be switching off checking duplication of insns in forward\n+   dependencies. */\n+#ifdef ENABLE_CHECKING\n+static sbitmap *forward_dependency_cache;\n+#endif\n+\n+static void remove_dependence PARAMS ((rtx, rtx));\n+static void set_sched_group_p PARAMS ((rtx));\n+\n+static void flush_pending_lists PARAMS ((struct deps *, rtx, int));\n+static void sched_analyze_1 PARAMS ((struct deps *, rtx, rtx));\n+static void sched_analyze_2 PARAMS ((struct deps *, rtx, rtx));\n+static void sched_analyze_insn PARAMS ((struct deps *, rtx, rtx, rtx));\n+static rtx group_leader PARAMS ((rtx));\n+\f\n+/* Return the INSN_LIST containing INSN in LIST, or NULL\n+   if LIST does not contain INSN.  */\n+\n+HAIFA_INLINE rtx\n+find_insn_list (insn, list)\n+     rtx insn;\n+     rtx list;\n+{\n+  while (list)\n+    {\n+      if (XEXP (list, 0) == insn)\n+\treturn list;\n+      list = XEXP (list, 1);\n+    }\n+  return 0;\n+}\n+\n+/* Return 1 if the pair (insn, x) is found in (LIST, LIST1), or 0\n+   otherwise.  */\n+\n+HAIFA_INLINE int\n+find_insn_mem_list (insn, x, list, list1)\n+     rtx insn, x;\n+     rtx list, list1;\n+{\n+  while (list)\n+    {\n+      if (XEXP (list, 0) == insn\n+\t  && XEXP (list1, 0) == x)\n+\treturn 1;\n+      list = XEXP (list, 1);\n+      list1 = XEXP (list1, 1);\n+    }\n+  return 0;\n+}\n+\f\n+/* Add ELEM wrapped in an INSN_LIST with reg note kind DEP_TYPE to the\n+   LOG_LINKS of INSN, if not already there.  DEP_TYPE indicates the type\n+   of dependence that this link represents.  */\n+\n+void\n+add_dependence (insn, elem, dep_type)\n+     rtx insn;\n+     rtx elem;\n+     enum reg_note dep_type;\n+{\n+  rtx link, next;\n+  int present_p;\n+  enum reg_note present_dep_type;\n+\n+  /* Don't depend an insn on itself.  */\n+  if (insn == elem)\n+    return;\n+\n+  /* We can get a dependency on deleted insns due to optimizations in\n+     the register allocation and reloading or due to splitting.  Any\n+     such dependency is useless and can be ignored.  */\n+  if (GET_CODE (elem) == NOTE)\n+    return;\n+\n+  /* If elem is part of a sequence that must be scheduled together, then\n+     make the dependence point to the last insn of the sequence.\n+     When HAVE_cc0, it is possible for NOTEs to exist between users and\n+     setters of the condition codes, so we must skip past notes here.\n+     Otherwise, NOTEs are impossible here.  */\n+  next = next_nonnote_insn (elem);\n+  if (next && SCHED_GROUP_P (next)\n+      && GET_CODE (next) != CODE_LABEL)\n+    {\n+      /* Notes will never intervene here though, so don't bother checking\n+         for them.  */\n+      /* Hah!  Wrong.  */\n+      /* We must reject CODE_LABELs, so that we don't get confused by one\n+         that has LABEL_PRESERVE_P set, which is represented by the same\n+         bit in the rtl as SCHED_GROUP_P.  A CODE_LABEL can never be\n+         SCHED_GROUP_P.  */\n+\n+      rtx nnext;\n+      while ((nnext = next_nonnote_insn (next)) != NULL\n+\t     && SCHED_GROUP_P (nnext)\n+\t     && GET_CODE (nnext) != CODE_LABEL)\n+\tnext = nnext;\n+\n+      /* Again, don't depend an insn on itself.  */\n+      if (insn == next)\n+\treturn;\n+\n+      /* Make the dependence to NEXT, the last insn of the group, instead\n+         of the original ELEM.  */\n+      elem = next;\n+    }\n+\n+  present_p = 1;\n+#ifdef INSN_SCHEDULING\n+  /* ??? No good way to tell from here whether we're doing interblock\n+     scheduling.  Possibly add another callback.  */\n+#if 0\n+  /* (This code is guarded by INSN_SCHEDULING, otherwise INSN_BB is undefined.)\n+     No need for interblock dependences with calls, since\n+     calls are not moved between blocks.   Note: the edge where\n+     elem is a CALL is still required.  */\n+  if (GET_CODE (insn) == CALL_INSN\n+      && (INSN_BB (elem) != INSN_BB (insn)))\n+    return;\n+#endif\n+\n+  /* If we already have a dependency for ELEM, then we do not need to\n+     do anything.  Avoiding the list walk below can cut compile times\n+     dramatically for some code.  */\n+  if (true_dependency_cache != NULL)\n+    {\n+      if (anti_dependency_cache == NULL || output_dependency_cache == NULL)\n+\tabort ();\n+      if (TEST_BIT (true_dependency_cache[INSN_LUID (insn)], INSN_LUID (elem)))\n+\tpresent_dep_type = 0;\n+      else if (TEST_BIT (anti_dependency_cache[INSN_LUID (insn)],\n+\t\t\t INSN_LUID (elem)))\n+\tpresent_dep_type = REG_DEP_ANTI;\n+      else if (TEST_BIT (output_dependency_cache[INSN_LUID (insn)],\n+\t\t\t INSN_LUID (elem)))\n+\tpresent_dep_type = REG_DEP_OUTPUT;\n+      else \n+\tpresent_p = 0;\n+      if (present_p && (int) dep_type >= (int) present_dep_type)\n+\treturn;\n+    }\n+#endif\n+\n+  /* Check that we don't already have this dependence.  */\n+  if (present_p)\n+    for (link = LOG_LINKS (insn); link; link = XEXP (link, 1))\n+      if (XEXP (link, 0) == elem)\n+\t{\n+#ifdef INSN_SCHEDULING\n+\t  /* Clear corresponding cache entry because type of the link\n+             may be changed. */\n+\t  if (true_dependency_cache != NULL)\n+\t    {\n+\t      if (REG_NOTE_KIND (link) == REG_DEP_ANTI)\n+\t\tRESET_BIT (anti_dependency_cache[INSN_LUID (insn)],\n+\t\t\t   INSN_LUID (elem));\n+\t      else if (REG_NOTE_KIND (link) == REG_DEP_OUTPUT\n+\t\t       && output_dependency_cache)\n+\t\tRESET_BIT (output_dependency_cache[INSN_LUID (insn)],\n+\t\t\t   INSN_LUID (elem));\n+\t      else\n+\t\tabort ();\n+\t    }\n+#endif\n+\n+\t  /* If this is a more restrictive type of dependence than the existing\n+\t     one, then change the existing dependence to this type.  */\n+\t  if ((int) dep_type < (int) REG_NOTE_KIND (link))\n+\t    PUT_REG_NOTE_KIND (link, dep_type);\n+\t  \n+#ifdef INSN_SCHEDULING\n+\t  /* If we are adding a dependency to INSN's LOG_LINKs, then\n+\t     note that in the bitmap caches of dependency information. */\n+\t  if (true_dependency_cache != NULL)\n+\t    {\n+\t      if ((int)REG_NOTE_KIND (link) == 0)\n+\t\tSET_BIT (true_dependency_cache[INSN_LUID (insn)],\n+\t\t\t INSN_LUID (elem));\n+\t      else if (REG_NOTE_KIND (link) == REG_DEP_ANTI)\n+\t\tSET_BIT (anti_dependency_cache[INSN_LUID (insn)],\n+\t\t\t INSN_LUID (elem));\n+\t      else if (REG_NOTE_KIND (link) == REG_DEP_OUTPUT)\n+\t\tSET_BIT (output_dependency_cache[INSN_LUID (insn)],\n+\t\t\t INSN_LUID (elem));\n+\t    }\n+#endif\n+\t  return;\n+      }\n+  /* Might want to check one level of transitivity to save conses.  */\n+\n+  link = alloc_INSN_LIST (elem, LOG_LINKS (insn));\n+  LOG_LINKS (insn) = link;\n+\n+  /* Insn dependency, not data dependency.  */\n+  PUT_REG_NOTE_KIND (link, dep_type);\n+\n+#ifdef INSN_SCHEDULING\n+  /* If we are adding a dependency to INSN's LOG_LINKs, then note that\n+     in the bitmap caches of dependency information. */\n+  if (true_dependency_cache != NULL)\n+    {\n+      if ((int)dep_type == 0)\n+\tSET_BIT (true_dependency_cache[INSN_LUID (insn)], INSN_LUID (elem));\n+      else if (dep_type == REG_DEP_ANTI)\n+\tSET_BIT (anti_dependency_cache[INSN_LUID (insn)], INSN_LUID (elem));\n+      else if (dep_type == REG_DEP_OUTPUT)\n+\tSET_BIT (output_dependency_cache[INSN_LUID (insn)], INSN_LUID (elem));\n+    }\n+#endif\n+}\n+\n+/* Remove ELEM wrapped in an INSN_LIST from the LOG_LINKS\n+   of INSN.  Abort if not found.  */\n+\n+static void\n+remove_dependence (insn, elem)\n+     rtx insn;\n+     rtx elem;\n+{\n+  rtx prev, link, next;\n+  int found = 0;\n+\n+  for (prev = 0, link = LOG_LINKS (insn); link; link = next)\n+    {\n+      next = XEXP (link, 1);\n+      if (XEXP (link, 0) == elem)\n+\t{\n+\t  if (prev)\n+\t    XEXP (prev, 1) = next;\n+\t  else\n+\t    LOG_LINKS (insn) = next;\n+\n+#ifdef INSN_SCHEDULING\n+\t  /* If we are removing a dependency from the LOG_LINKS list,\n+\t     make sure to remove it from the cache too.  */\n+\t  if (true_dependency_cache != NULL)\n+\t    {\n+\t      if (REG_NOTE_KIND (link) == 0)\n+\t\tRESET_BIT (true_dependency_cache[INSN_LUID (insn)],\n+\t\t\t   INSN_LUID (elem));\n+\t      else if (REG_NOTE_KIND (link) == REG_DEP_ANTI)\n+\t\tRESET_BIT (anti_dependency_cache[INSN_LUID (insn)],\n+\t\t\t   INSN_LUID (elem));\n+\t      else if (REG_NOTE_KIND (link) == REG_DEP_OUTPUT)\n+\t\tRESET_BIT (output_dependency_cache[INSN_LUID (insn)],\n+\t\t\t   INSN_LUID (elem));\n+\t    }\n+#endif\n+\n+\t  free_INSN_LIST_node (link);\n+\n+\t  found = 1;\n+\t}\n+      else\n+\tprev = link;\n+    }\n+\n+  if (!found)\n+    abort ();\n+  return;\n+}\n+\n+/* Return an insn which represents a SCHED_GROUP, which is\n+   the last insn in the group.  */\n+\n+static rtx\n+group_leader (insn)\n+     rtx insn;\n+{\n+  rtx prev;\n+\n+  do\n+    {\n+      prev = insn;\n+      insn = next_nonnote_insn (insn);\n+    }\n+  while (insn && SCHED_GROUP_P (insn) && (GET_CODE (insn) != CODE_LABEL));\n+\n+  return prev;\n+}\n+\n+/* Set SCHED_GROUP_P and care for the rest of the bookkeeping that\n+   goes along with that.  */\n+\n+static void\n+set_sched_group_p (insn)\n+     rtx insn;\n+{\n+  rtx link, prev;\n+\n+  SCHED_GROUP_P (insn) = 1;\n+\n+  /* There may be a note before this insn now, but all notes will\n+     be removed before we actually try to schedule the insns, so\n+     it won't cause a problem later.  We must avoid it here though.  */\n+  prev = prev_nonnote_insn (insn);\n+\n+  /* Make a copy of all dependencies on the immediately previous insn,\n+     and add to this insn.  This is so that all the dependencies will\n+     apply to the group.  Remove an explicit dependence on this insn\n+     as SCHED_GROUP_P now represents it.  */\n+\n+  if (find_insn_list (prev, LOG_LINKS (insn)))\n+    remove_dependence (insn, prev);\n+\n+  for (link = LOG_LINKS (prev); link; link = XEXP (link, 1))\n+    add_dependence (insn, XEXP (link, 0), REG_NOTE_KIND (link));\n+}\n+\f\n+/* Process an insn's memory dependencies.  There are four kinds of\n+   dependencies:\n+\n+   (0) read dependence: read follows read\n+   (1) true dependence: read follows write\n+   (2) anti dependence: write follows read\n+   (3) output dependence: write follows write\n+\n+   We are careful to build only dependencies which actually exist, and\n+   use transitivity to avoid building too many links.  */\n+\n+/* Add an INSN and MEM reference pair to a pending INSN_LIST and MEM_LIST.\n+   The MEM is a memory reference contained within INSN, which we are saving\n+   so that we can do memory aliasing on it.  */\n+\n+void\n+add_insn_mem_dependence (deps, insn_list, mem_list, insn, mem)\n+     struct deps *deps;\n+     rtx *insn_list, *mem_list, insn, mem;\n+{\n+  register rtx link;\n+\n+  link = alloc_INSN_LIST (insn, *insn_list);\n+  *insn_list = link;\n+\n+  link = alloc_EXPR_LIST (VOIDmode, mem, *mem_list);\n+  *mem_list = link;\n+\n+  deps->pending_lists_length++;\n+}\n+\n+/* Make a dependency between every memory reference on the pending lists\n+   and INSN, thus flushing the pending lists.  If ONLY_WRITE, don't flush\n+   the read list.  */\n+\n+static void\n+flush_pending_lists (deps, insn, only_write)\n+     struct deps *deps;\n+     rtx insn;\n+     int only_write;\n+{\n+  rtx u;\n+  rtx link;\n+\n+  while (deps->pending_read_insns && ! only_write)\n+    {\n+      add_dependence (insn, XEXP (deps->pending_read_insns, 0),\n+\t\t      REG_DEP_ANTI);\n+\n+      link = deps->pending_read_insns;\n+      deps->pending_read_insns = XEXP (deps->pending_read_insns, 1);\n+      free_INSN_LIST_node (link);\n+\n+      link = deps->pending_read_mems;\n+      deps->pending_read_mems = XEXP (deps->pending_read_mems, 1);\n+      free_EXPR_LIST_node (link);\n+    }\n+  while (deps->pending_write_insns)\n+    {\n+      add_dependence (insn, XEXP (deps->pending_write_insns, 0),\n+\t\t      REG_DEP_ANTI);\n+\n+      link = deps->pending_write_insns;\n+      deps->pending_write_insns = XEXP (deps->pending_write_insns, 1);\n+      free_INSN_LIST_node (link);\n+\n+      link = deps->pending_write_mems;\n+      deps->pending_write_mems = XEXP (deps->pending_write_mems, 1);\n+      free_EXPR_LIST_node (link);\n+    }\n+  deps->pending_lists_length = 0;\n+\n+  /* last_pending_memory_flush is now a list of insns.  */\n+  for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n+    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\n+  free_INSN_LIST_list (&deps->last_pending_memory_flush);\n+  deps->last_pending_memory_flush = alloc_INSN_LIST (insn, NULL_RTX);\n+}\n+\f\n+/* Analyze a single SET, CLOBBER, PRE_DEC, POST_DEC, PRE_INC or POST_INC\n+   rtx, X, creating all dependencies generated by the write to the\n+   destination of X, and reads of everything mentioned.  */\n+\n+static void\n+sched_analyze_1 (deps, x, insn)\n+     struct deps *deps;\n+     rtx x;\n+     rtx insn;\n+{\n+  register int regno;\n+  register rtx dest = XEXP (x, 0);\n+  enum rtx_code code = GET_CODE (x);\n+\n+  if (dest == 0)\n+    return;\n+\n+  if (GET_CODE (dest) == PARALLEL\n+      && GET_MODE (dest) == BLKmode)\n+    {\n+      register int i;\n+      for (i = XVECLEN (dest, 0) - 1; i >= 0; i--)\n+\tsched_analyze_1 (deps, XVECEXP (dest, 0, i), insn);\n+      if (GET_CODE (x) == SET)\n+\tsched_analyze_2 (deps, SET_SRC (x), insn);\n+      return;\n+    }\n+\n+  while (GET_CODE (dest) == STRICT_LOW_PART || GET_CODE (dest) == SUBREG\n+\t || GET_CODE (dest) == ZERO_EXTRACT || GET_CODE (dest) == SIGN_EXTRACT)\n+    {\n+      if (GET_CODE (dest) == ZERO_EXTRACT || GET_CODE (dest) == SIGN_EXTRACT)\n+\t{\n+\t  /* The second and third arguments are values read by this insn.  */\n+\t  sched_analyze_2 (deps, XEXP (dest, 1), insn);\n+\t  sched_analyze_2 (deps, XEXP (dest, 2), insn);\n+\t}\n+      dest = XEXP (dest, 0);\n+    }\n+\n+  if (GET_CODE (dest) == REG)\n+    {\n+      register int i;\n+\n+      regno = REGNO (dest);\n+\n+      /* A hard reg in a wide mode may really be multiple registers.\n+         If so, mark all of them just like the first.  */\n+      if (regno < FIRST_PSEUDO_REGISTER)\n+\t{\n+\t  i = HARD_REGNO_NREGS (regno, GET_MODE (dest));\n+\t  while (--i >= 0)\n+\t    {\n+\t      int r = regno + i;\n+\t      rtx u;\n+\n+\t      for (u = deps->reg_last_uses[r]; u; u = XEXP (u, 1))\n+\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\n+\t      for (u = deps->reg_last_sets[r]; u; u = XEXP (u, 1))\n+\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n+\n+\t      /* Clobbers need not be ordered with respect to one\n+\t\t another, but sets must be ordered with respect to a\n+\t\t pending clobber.  */\n+\t      if (code == SET)\n+\t\t{\n+\t\t  free_INSN_LIST_list (&deps->reg_last_uses[r]);\n+\t\t  for (u = deps->reg_last_clobbers[r]; u; u = XEXP (u, 1))\n+\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n+\t\t  SET_REGNO_REG_SET (reg_pending_sets, r);\n+\t\t}\n+\t      else\n+\t\tSET_REGNO_REG_SET (reg_pending_clobbers, r);\n+\n+\t      /* Function calls clobber all call_used regs.  */\n+\t      if (global_regs[r] || (code == SET && call_used_regs[r]))\n+\t\tfor (u = deps->last_function_call; u; u = XEXP (u, 1))\n+\t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  rtx u;\n+\n+\t  for (u = deps->reg_last_uses[regno]; u; u = XEXP (u, 1))\n+\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\n+\t  for (u = deps->reg_last_sets[regno]; u; u = XEXP (u, 1))\n+\t    add_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n+\n+\t  if (code == SET)\n+\t    {\n+\t      free_INSN_LIST_list (&deps->reg_last_uses[regno]);\n+\t      for (u = deps->reg_last_clobbers[regno]; u; u = XEXP (u, 1))\n+\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n+\t      SET_REGNO_REG_SET (reg_pending_sets, regno);\n+\t    }\n+\t  else\n+\t    SET_REGNO_REG_SET (reg_pending_clobbers, regno);\n+\n+\t  /* Pseudos that are REG_EQUIV to something may be replaced\n+\t     by that during reloading.  We need only add dependencies for\n+\t     the address in the REG_EQUIV note.  */\n+\t  if (!reload_completed\n+\t      && reg_known_equiv_p[regno]\n+\t      && GET_CODE (reg_known_value[regno]) == MEM)\n+\t    sched_analyze_2 (deps, XEXP (reg_known_value[regno], 0), insn);\n+\n+\t  /* Don't let it cross a call after scheduling if it doesn't\n+\t     already cross one.  */\n+\n+\t  if (REG_N_CALLS_CROSSED (regno) == 0)\n+\t    for (u = deps->last_function_call; u; u = XEXP (u, 1))\n+\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t}\n+    }\n+  else if (GET_CODE (dest) == MEM)\n+    {\n+      /* Writing memory.  */\n+\n+      if (deps->pending_lists_length > 32)\n+\t{\n+\t  /* Flush all pending reads and writes to prevent the pending lists\n+\t     from getting any larger.  Insn scheduling runs too slowly when\n+\t     these lists get long.  The number 32 was chosen because it\n+\t     seems like a reasonable number.  When compiling GCC with itself,\n+\t     this flush occurs 8 times for sparc, and 10 times for m88k using\n+\t     the number 32.  */\n+\t  flush_pending_lists (deps, insn, 0);\n+\t}\n+      else\n+\t{\n+\t  rtx u;\n+\t  rtx pending, pending_mem;\n+\n+\t  pending = deps->pending_read_insns;\n+\t  pending_mem = deps->pending_read_mems;\n+\t  while (pending)\n+\t    {\n+\t      if (anti_dependence (XEXP (pending_mem, 0), dest))\n+\t\tadd_dependence (insn, XEXP (pending, 0), REG_DEP_ANTI);\n+\n+\t      pending = XEXP (pending, 1);\n+\t      pending_mem = XEXP (pending_mem, 1);\n+\t    }\n+\n+\t  pending = deps->pending_write_insns;\n+\t  pending_mem = deps->pending_write_mems;\n+\t  while (pending)\n+\t    {\n+\t      if (output_dependence (XEXP (pending_mem, 0), dest))\n+\t\tadd_dependence (insn, XEXP (pending, 0), REG_DEP_OUTPUT);\n+\n+\t      pending = XEXP (pending, 1);\n+\t      pending_mem = XEXP (pending_mem, 1);\n+\t    }\n+\n+\t  for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n+\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\n+\t  add_insn_mem_dependence (deps, &deps->pending_write_insns,\n+\t\t\t\t   &deps->pending_write_mems, insn, dest);\n+\t}\n+      sched_analyze_2 (deps, XEXP (dest, 0), insn);\n+    }\n+\n+  /* Analyze reads.  */\n+  if (GET_CODE (x) == SET)\n+    sched_analyze_2 (deps, SET_SRC (x), insn);\n+}\n+\n+/* Analyze the uses of memory and registers in rtx X in INSN.  */\n+\n+static void\n+sched_analyze_2 (deps, x, insn)\n+     struct deps *deps;\n+     rtx x;\n+     rtx insn;\n+{\n+  register int i;\n+  register int j;\n+  register enum rtx_code code;\n+  register const char *fmt;\n+\n+  if (x == 0)\n+    return;\n+\n+  code = GET_CODE (x);\n+\n+  switch (code)\n+    {\n+    case CONST_INT:\n+    case CONST_DOUBLE:\n+    case SYMBOL_REF:\n+    case CONST:\n+    case LABEL_REF:\n+      /* Ignore constants.  Note that we must handle CONST_DOUBLE here\n+         because it may have a cc0_rtx in its CONST_DOUBLE_CHAIN field, but\n+         this does not mean that this insn is using cc0.  */\n+      return;\n+\n+#ifdef HAVE_cc0\n+    case CC0:\n+      /* User of CC0 depends on immediately preceding insn.  */\n+      set_sched_group_p (insn);\n+      return;\n+#endif\n+\n+    case REG:\n+      {\n+\trtx u;\n+\tint regno = REGNO (x);\n+\tif (regno < FIRST_PSEUDO_REGISTER)\n+\t  {\n+\t    int i;\n+\n+\t    i = HARD_REGNO_NREGS (regno, GET_MODE (x));\n+\t    while (--i >= 0)\n+\t      {\n+\t\tint r = regno + i;\n+\t\tdeps->reg_last_uses[r]\n+\t\t  = alloc_INSN_LIST (insn, deps->reg_last_uses[r]);\n+\n+\t\tfor (u = deps->reg_last_sets[r]; u; u = XEXP (u, 1))\n+\t\t  add_dependence (insn, XEXP (u, 0), 0);\n+\n+\t\t/* ??? This should never happen.  */\n+\t\tfor (u = deps->reg_last_clobbers[r]; u; u = XEXP (u, 1))\n+\t\t  add_dependence (insn, XEXP (u, 0), 0);\n+\n+\t\tif (call_used_regs[r] || global_regs[r])\n+\t\t  /* Function calls clobber all call_used regs.  */\n+\t\t  for (u = deps->last_function_call; u; u = XEXP (u, 1))\n+\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t      }\n+\t  }\n+\telse\n+\t  {\n+\t    deps->reg_last_uses[regno]\n+\t      = alloc_INSN_LIST (insn, deps->reg_last_uses[regno]);\n+\n+\t    for (u = deps->reg_last_sets[regno]; u; u = XEXP (u, 1))\n+\t      add_dependence (insn, XEXP (u, 0), 0);\n+\n+\t    /* ??? This should never happen.  */\n+\t    for (u = deps->reg_last_clobbers[regno]; u; u = XEXP (u, 1))\n+\t      add_dependence (insn, XEXP (u, 0), 0);\n+\n+\t    /* Pseudos that are REG_EQUIV to something may be replaced\n+\t       by that during reloading.  We need only add dependencies for\n+\t       the address in the REG_EQUIV note.  */\n+\t    if (!reload_completed\n+\t\t&& reg_known_equiv_p[regno]\n+\t\t&& GET_CODE (reg_known_value[regno]) == MEM)\n+\t      sched_analyze_2 (deps, XEXP (reg_known_value[regno], 0), insn);\n+\n+\t    /* If the register does not already cross any calls, then add this\n+\t       insn to the sched_before_next_call list so that it will still\n+\t       not cross calls after scheduling.  */\n+\t    if (REG_N_CALLS_CROSSED (regno) == 0)\n+\t      add_dependence (deps->sched_before_next_call, insn,\n+\t\t\t      REG_DEP_ANTI);\n+\t  }\n+\treturn;\n+      }\n+\n+    case MEM:\n+      {\n+\t/* Reading memory.  */\n+\trtx u;\n+\trtx pending, pending_mem;\n+\n+\tpending = deps->pending_read_insns;\n+\tpending_mem = deps->pending_read_mems;\n+\twhile (pending)\n+\t  {\n+\t    if (read_dependence (XEXP (pending_mem, 0), x))\n+\t      add_dependence (insn, XEXP (pending, 0), REG_DEP_ANTI);\n+\n+\t    pending = XEXP (pending, 1);\n+\t    pending_mem = XEXP (pending_mem, 1);\n+\t  }\n+\n+\tpending = deps->pending_write_insns;\n+\tpending_mem = deps->pending_write_mems;\n+\twhile (pending)\n+\t  {\n+\t    if (true_dependence (XEXP (pending_mem, 0), VOIDmode,\n+\t\t\t\t x, rtx_varies_p))\n+\t      add_dependence (insn, XEXP (pending, 0), 0);\n+\n+\t    pending = XEXP (pending, 1);\n+\t    pending_mem = XEXP (pending_mem, 1);\n+\t  }\n+\n+\tfor (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n+\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\n+\t/* Always add these dependencies to pending_reads, since\n+\t   this insn may be followed by a write.  */\n+\tadd_insn_mem_dependence (deps, &deps->pending_read_insns,\n+\t\t\t\t &deps->pending_read_mems, insn, x);\n+\n+\t/* Take advantage of tail recursion here.  */\n+\tsched_analyze_2 (deps, XEXP (x, 0), insn);\n+\treturn;\n+      }\n+\n+    /* Force pending stores to memory in case a trap handler needs them.  */\n+    case TRAP_IF:\n+      flush_pending_lists (deps, insn, 1);\n+      break;\n+\n+    case ASM_OPERANDS:\n+    case ASM_INPUT:\n+    case UNSPEC_VOLATILE:\n+      {\n+\trtx u;\n+\n+\t/* Traditional and volatile asm instructions must be considered to use\n+\t   and clobber all hard registers, all pseudo-registers and all of\n+\t   memory.  So must TRAP_IF and UNSPEC_VOLATILE operations.\n+\n+\t   Consider for instance a volatile asm that changes the fpu rounding\n+\t   mode.  An insn should not be moved across this even if it only uses\n+\t   pseudo-regs because it might give an incorrectly rounded result.  */\n+\tif (code != ASM_OPERANDS || MEM_VOLATILE_P (x))\n+\t  {\n+\t    int max_reg = max_reg_num ();\n+\t    for (i = 0; i < max_reg; i++)\n+\t      {\n+\t\tfor (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n+\t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t\tfree_INSN_LIST_list (&deps->reg_last_uses[i]);\n+\n+\t\tfor (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n+\t\t  add_dependence (insn, XEXP (u, 0), 0);\n+\n+\t\tfor (u = deps->reg_last_clobbers[i]; u; u = XEXP (u, 1))\n+\t\t  add_dependence (insn, XEXP (u, 0), 0);\n+\t      }\n+\t    reg_pending_sets_all = 1;\n+\n+\t    flush_pending_lists (deps, insn, 0);\n+\t  }\n+\n+\t/* For all ASM_OPERANDS, we must traverse the vector of input operands.\n+\t   We can not just fall through here since then we would be confused\n+\t   by the ASM_INPUT rtx inside ASM_OPERANDS, which do not indicate\n+\t   traditional asms unlike their normal usage.  */\n+\n+\tif (code == ASM_OPERANDS)\n+\t  {\n+\t    for (j = 0; j < ASM_OPERANDS_INPUT_LENGTH (x); j++)\n+\t      sched_analyze_2 (deps, ASM_OPERANDS_INPUT (x, j), insn);\n+\t    return;\n+\t  }\n+\tbreak;\n+      }\n+\n+    case PRE_DEC:\n+    case POST_DEC:\n+    case PRE_INC:\n+    case POST_INC:\n+      /* These both read and modify the result.  We must handle them as writes\n+         to get proper dependencies for following instructions.  We must handle\n+         them as reads to get proper dependencies from this to previous\n+         instructions.  Thus we need to pass them to both sched_analyze_1\n+         and sched_analyze_2.  We must call sched_analyze_2 first in order\n+         to get the proper antecedent for the read.  */\n+      sched_analyze_2 (deps, XEXP (x, 0), insn);\n+      sched_analyze_1 (deps, x, insn);\n+      return;\n+\n+    case POST_MODIFY:\n+    case PRE_MODIFY:\n+      /* op0 = op0 + op1 */\n+      sched_analyze_2 (deps, XEXP (x, 0), insn);\n+      sched_analyze_2 (deps, XEXP (x, 1), insn);\n+      sched_analyze_1 (deps, x, insn);\n+      return;\n+\n+    default:\n+      break;\n+    }\n+\n+  /* Other cases: walk the insn.  */\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\tsched_analyze_2 (deps, XEXP (x, i), insn);\n+      else if (fmt[i] == 'E')\n+\tfor (j = 0; j < XVECLEN (x, i); j++)\n+\t  sched_analyze_2 (deps, XVECEXP (x, i, j), insn);\n+    }\n+}\n+\n+/* Analyze an INSN with pattern X to find all dependencies.  */\n+\n+static void\n+sched_analyze_insn (deps, x, insn, loop_notes)\n+     struct deps *deps;\n+     rtx x, insn;\n+     rtx loop_notes;\n+{\n+  register RTX_CODE code = GET_CODE (x);\n+  rtx link;\n+  int maxreg = max_reg_num ();\n+  int i;\n+\n+  if (code == COND_EXEC)\n+    {\n+      sched_analyze_2 (deps, COND_EXEC_TEST (x), insn);\n+\n+      /* ??? Should be recording conditions so we reduce the number of\n+\t false dependancies.  */\n+      x = COND_EXEC_CODE (x);\n+      code = GET_CODE (x);\n+    }\n+  if (code == SET || code == CLOBBER)\n+    sched_analyze_1 (deps, x, insn);\n+  else if (code == PARALLEL)\n+    {\n+      register int i;\n+      for (i = XVECLEN (x, 0) - 1; i >= 0; i--)\n+\t{\n+\t  rtx sub = XVECEXP (x, 0, i);\n+\t  code = GET_CODE (sub);\n+\n+\t  if (code == COND_EXEC)\n+\t    {\n+\t      sched_analyze_2 (deps, COND_EXEC_TEST (sub), insn);\n+\t      sub = COND_EXEC_CODE (sub);\n+\t      code = GET_CODE (sub);\n+\t    }\n+\t  if (code == SET || code == CLOBBER)\n+\t    sched_analyze_1 (deps, sub, insn);\n+\t  else\n+\t    sched_analyze_2 (deps, sub, insn);\n+\t}\n+    }\n+  else\n+    sched_analyze_2 (deps, x, insn);\n+\n+  /* Mark registers CLOBBERED or used by called function.  */\n+  if (GET_CODE (insn) == CALL_INSN)\n+    for (link = CALL_INSN_FUNCTION_USAGE (insn); link; link = XEXP (link, 1))\n+      {\n+\tif (GET_CODE (XEXP (link, 0)) == CLOBBER)\n+\t  sched_analyze_1 (deps, XEXP (link, 0), insn);\n+\telse\n+\t  sched_analyze_2 (deps, XEXP (link, 0), insn);\n+      }\n+\n+  /* If there is a {LOOP,EHREGION}_{BEG,END} note in the middle of a basic\n+     block, then we must be sure that no instructions are scheduled across it.\n+     Otherwise, the reg_n_refs info (which depends on loop_depth) would\n+     become incorrect.  */\n+\n+  if (loop_notes)\n+    {\n+      int max_reg = max_reg_num ();\n+      int schedule_barrier_found = 0;\n+      rtx link;\n+\n+      /* Update loop_notes with any notes from this insn.  Also determine\n+\t if any of the notes on the list correspond to instruction scheduling\n+\t barriers (loop, eh & setjmp notes, but not range notes.  */\n+      link = loop_notes;\n+      while (XEXP (link, 1))\n+\t{\n+\t  if (INTVAL (XEXP (link, 0)) == NOTE_INSN_LOOP_BEG\n+\t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_LOOP_END\n+\t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_EH_REGION_BEG\n+\t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_EH_REGION_END\n+\t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_SETJMP)\n+\t    schedule_barrier_found = 1;\n+\n+\t  link = XEXP (link, 1);\n+\t}\n+      XEXP (link, 1) = REG_NOTES (insn);\n+      REG_NOTES (insn) = loop_notes;\n+\n+      /* Add dependencies if a scheduling barrier was found.  */\n+      if (schedule_barrier_found)\n+\t{\n+\t  for (i = 0; i < max_reg; i++)\n+\t    {\n+\t      rtx u;\n+\t      for (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n+\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t      free_INSN_LIST_list (&deps->reg_last_uses[i]);\n+\n+\t      for (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n+\t\tadd_dependence (insn, XEXP (u, 0), 0);\n+\n+\t      for (u = deps->reg_last_clobbers[i]; u; u = XEXP (u, 1))\n+\t\tadd_dependence (insn, XEXP (u, 0), 0);\n+\t    }\n+\t  reg_pending_sets_all = 1;\n+\n+\t  flush_pending_lists (deps, insn, 0);\n+\t}\n+\n+    }\n+\n+  /* Accumulate clobbers until the next set so that it will be output dependent\n+     on all of them.  At the next set we can clear the clobber list, since\n+     subsequent sets will be output dependent on it.  */\n+  EXECUTE_IF_SET_IN_REG_SET\n+    (reg_pending_sets, 0, i,\n+     {\n+       free_INSN_LIST_list (&deps->reg_last_sets[i]);\n+       free_INSN_LIST_list (&deps->reg_last_clobbers[i]);\n+       deps->reg_last_sets[i] = alloc_INSN_LIST (insn, NULL_RTX);\n+     });\n+  EXECUTE_IF_SET_IN_REG_SET\n+    (reg_pending_clobbers, 0, i,\n+     {\n+       deps->reg_last_clobbers[i]\n+\t = alloc_INSN_LIST (insn, deps->reg_last_clobbers[i]);\n+     });\n+  CLEAR_REG_SET (reg_pending_sets);\n+  CLEAR_REG_SET (reg_pending_clobbers);\n+\n+  if (reg_pending_sets_all)\n+    {\n+      for (i = 0; i < maxreg; i++)\n+\t{\n+\t  free_INSN_LIST_list (&deps->reg_last_sets[i]);\n+\t  free_INSN_LIST_list (&deps->reg_last_clobbers[i]);\n+\t  deps->reg_last_sets[i] = alloc_INSN_LIST (insn, NULL_RTX);\n+\t}\n+\n+      reg_pending_sets_all = 0;\n+    }\n+\n+  /* If a post-call group is still open, see if it should remain so.\n+     This insn must be a simple move of a hard reg to a pseudo or\n+     vice-versa.\n+\n+     We must avoid moving these insns for correctness on\n+     SMALL_REGISTER_CLASS machines, and for special registers like\n+     PIC_OFFSET_TABLE_REGNUM.  For simplicity, extend this to all\n+     hard regs for all targets.  */\n+\n+  if (deps->in_post_call_group_p)\n+    {\n+      rtx tmp, set = single_set (insn);\n+      int src_regno, dest_regno;\n+\n+      if (set == NULL)\n+\tgoto end_call_group;\n+\n+      tmp = SET_DEST (set);\n+      if (GET_CODE (tmp) == SUBREG)\n+\ttmp = SUBREG_REG (tmp);\n+      if (GET_CODE (tmp) == REG)\n+\tdest_regno = REGNO (tmp);\n+      else\n+\tgoto end_call_group;\n+\n+      tmp = SET_SRC (set);\n+      if (GET_CODE (tmp) == SUBREG)\n+\ttmp = SUBREG_REG (tmp);\n+      if (GET_CODE (tmp) == REG)\n+\tsrc_regno = REGNO (tmp);\n+      else\n+\tgoto end_call_group;\n+\n+      if (src_regno < FIRST_PSEUDO_REGISTER\n+\t  || dest_regno < FIRST_PSEUDO_REGISTER)\n+\t{\n+\t  set_sched_group_p (insn);\n+\t  CANT_MOVE (insn) = 1;\n+\t}\n+      else\n+\t{\n+\tend_call_group:\n+\t  deps->in_post_call_group_p = 0;\n+\t}\n+    }\n+}\n+\n+/* Analyze every insn between HEAD and TAIL inclusive, creating LOG_LINKS\n+   for every dependency.  */\n+\n+void\n+sched_analyze (deps, head, tail)\n+     struct deps *deps;\n+     rtx head, tail;\n+{\n+  register rtx insn;\n+  register rtx u;\n+  rtx loop_notes = 0;\n+\n+  for (insn = head;; insn = NEXT_INSN (insn))\n+    {\n+      if (GET_CODE (insn) == INSN || GET_CODE (insn) == JUMP_INSN)\n+\t{\n+\t  /* Clear out the stale LOG_LINKS from flow.  */\n+\t  free_INSN_LIST_list (&LOG_LINKS (insn));\n+\n+\t  /* Clear out stale SCHED_GROUP_P.  */\n+\t  SCHED_GROUP_P (insn) = 0;\n+\n+\t  /* Make each JUMP_INSN a scheduling barrier for memory\n+             references.  */\n+\t  if (GET_CODE (insn) == JUMP_INSN)\n+\t    deps->last_pending_memory_flush\n+\t      = alloc_INSN_LIST (insn, deps->last_pending_memory_flush);\n+\t  sched_analyze_insn (deps, PATTERN (insn), insn, loop_notes);\n+\t  loop_notes = 0;\n+\t}\n+      else if (GET_CODE (insn) == CALL_INSN)\n+\t{\n+\t  rtx x;\n+\t  register int i;\n+\n+\t  /* Clear out stale SCHED_GROUP_P.  */\n+\t  SCHED_GROUP_P (insn) = 0;\n+\n+\t  CANT_MOVE (insn) = 1;\n+\n+\t  /* Clear out the stale LOG_LINKS from flow.  */\n+\t  free_INSN_LIST_list (&LOG_LINKS (insn));\n+\n+\t  /* Any instruction using a hard register which may get clobbered\n+\t     by a call needs to be marked as dependent on this call.\n+\t     This prevents a use of a hard return reg from being moved\n+\t     past a void call (i.e. it does not explicitly set the hard\n+\t     return reg).  */\n+\n+\t  /* If this call is followed by a NOTE_INSN_SETJMP, then assume that\n+\t     all registers, not just hard registers, may be clobbered by this\n+\t     call.  */\n+\n+\t  /* Insn, being a CALL_INSN, magically depends on\n+\t     `last_function_call' already.  */\n+\n+\t  if (NEXT_INSN (insn) && GET_CODE (NEXT_INSN (insn)) == NOTE\n+\t      && NOTE_LINE_NUMBER (NEXT_INSN (insn)) == NOTE_INSN_SETJMP)\n+\t    {\n+\t      int max_reg = max_reg_num ();\n+\t      for (i = 0; i < max_reg; i++)\n+\t\t{\n+\t\t  for (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n+\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t\t  free_INSN_LIST_list (&deps->reg_last_uses[i]);\n+\n+\t\t  for (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n+\t\t    add_dependence (insn, XEXP (u, 0), 0);\n+\n+\t\t  for (u = deps->reg_last_clobbers[i]; u; u = XEXP (u, 1))\n+\t\t    add_dependence (insn, XEXP (u, 0), 0);\n+\t\t}\n+\t      reg_pending_sets_all = 1;\n+\n+\t      /* Add a pair of REG_SAVE_NOTEs which we will later\n+\t\t convert back into a NOTE_INSN_SETJMP note.  See\n+\t\t reemit_notes for why we use a pair of NOTEs.  */\n+\t      REG_NOTES (insn) = alloc_EXPR_LIST (REG_SAVE_NOTE,\n+\t\t\t\t\t\t  GEN_INT (0),\n+\t\t\t\t\t\t  REG_NOTES (insn));\n+\t      REG_NOTES (insn) = alloc_EXPR_LIST (REG_SAVE_NOTE,\n+\t\t\t\t\t\t  GEN_INT (NOTE_INSN_SETJMP),\n+\t\t\t\t\t\t  REG_NOTES (insn));\n+\t    }\n+\t  else\n+\t    {\n+\t      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+\t\tif (call_used_regs[i] || global_regs[i])\n+\t\t  {\n+\t\t    for (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n+\t\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\n+\t\t    for (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n+\t\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\n+\t\t    SET_REGNO_REG_SET (reg_pending_clobbers, i);\n+\t\t  }\n+\t    }\n+\n+\t  /* For each insn which shouldn't cross a call, add a dependence\n+\t     between that insn and this call insn.  */\n+\t  x = LOG_LINKS (deps->sched_before_next_call);\n+\t  while (x)\n+\t    {\n+\t      add_dependence (insn, XEXP (x, 0), REG_DEP_ANTI);\n+\t      x = XEXP (x, 1);\n+\t    }\n+\t  free_INSN_LIST_list (&LOG_LINKS (deps->sched_before_next_call));\n+\n+\t  sched_analyze_insn (deps, PATTERN (insn), insn, loop_notes);\n+\t  loop_notes = 0;\n+\n+\t  /* In the absence of interprocedural alias analysis, we must flush\n+\t     all pending reads and writes, and start new dependencies starting\n+\t     from here.  But only flush writes for constant calls (which may\n+\t     be passed a pointer to something we haven't written yet).  */\n+\t  flush_pending_lists (deps, insn, CONST_CALL_P (insn));\n+\n+\t  /* Depend this function call (actually, the user of this\n+\t     function call) on all hard register clobberage.  */\n+\n+\t  /* last_function_call is now a list of insns.  */\n+\t  free_INSN_LIST_list (&deps->last_function_call);\n+\t  deps->last_function_call = alloc_INSN_LIST (insn, NULL_RTX);\n+\n+\t  /* Before reload, begin a post-call group, so as to keep the\n+\t     lifetimes of hard registers correct.  */\n+\t  if (! reload_completed)\n+\t    deps->in_post_call_group_p = 1;\n+\t}\n+\n+      /* See comments on reemit_notes as to why we do this.\n+\t ??? Actually, the reemit_notes just say what is done, not why.  */\n+\n+      else if (GET_CODE (insn) == NOTE\n+\t       && (NOTE_LINE_NUMBER (insn) == NOTE_INSN_RANGE_BEG\n+\t\t   || NOTE_LINE_NUMBER (insn) == NOTE_INSN_RANGE_END))\n+\t{\n+\t  loop_notes = alloc_EXPR_LIST (REG_SAVE_NOTE, NOTE_RANGE_INFO (insn),\n+\t\t\t\t\tloop_notes);\n+\t  loop_notes = alloc_EXPR_LIST (REG_SAVE_NOTE,\n+\t\t\t\t\tGEN_INT (NOTE_LINE_NUMBER (insn)),\n+\t\t\t\t\tloop_notes);\n+\t}\n+      else if (GET_CODE (insn) == NOTE\n+\t       && (NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_BEG\n+\t\t   || NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_END\n+\t\t   || NOTE_LINE_NUMBER (insn) == NOTE_INSN_EH_REGION_BEG\n+\t\t   || NOTE_LINE_NUMBER (insn) == NOTE_INSN_EH_REGION_END\n+\t\t   || (NOTE_LINE_NUMBER (insn) == NOTE_INSN_SETJMP\n+\t\t       && GET_CODE (PREV_INSN (insn)) != CALL_INSN)))\n+\t{\n+\t  rtx rtx_region;\n+\n+\t  if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_EH_REGION_BEG\n+\t      || NOTE_LINE_NUMBER (insn) == NOTE_INSN_EH_REGION_END)\n+\t    rtx_region = GEN_INT (NOTE_EH_HANDLER (insn));\n+\t  else\n+\t    rtx_region = GEN_INT (0);\n+\n+\t  loop_notes = alloc_EXPR_LIST (REG_SAVE_NOTE,\n+\t\t\t\t\trtx_region,\n+\t\t\t\t\tloop_notes);\n+\t  loop_notes = alloc_EXPR_LIST (REG_SAVE_NOTE,\n+\t\t\t\t\tGEN_INT (NOTE_LINE_NUMBER (insn)),\n+\t\t\t\t\tloop_notes);\n+\t  CONST_CALL_P (loop_notes) = CONST_CALL_P (insn);\n+\t}\n+\n+      if (insn == tail)\n+\treturn;\n+    }\n+  abort ();\n+}\n+\f\n+/* Examine insns in the range [ HEAD, TAIL ] and Use the backward\n+   dependences from LOG_LINKS to build forward dependences in\n+   INSN_DEPEND.  */\n+\n+void\n+compute_forward_dependences (head, tail)\n+     rtx head, tail;\n+{\n+  rtx insn, link;\n+  rtx next_tail;\n+  enum reg_note dep_type;\n+\n+  next_tail = NEXT_INSN (tail);\n+  for (insn = head; insn != next_tail; insn = NEXT_INSN (insn))\n+    {\n+      if (! INSN_P (insn))\n+\tcontinue;\n+\n+      insn = group_leader (insn);\n+\n+      for (link = LOG_LINKS (insn); link; link = XEXP (link, 1))\n+\t{\n+\t  rtx x = group_leader (XEXP (link, 0));\n+\t  rtx new_link;\n+\n+\t  if (x != XEXP (link, 0))\n+\t    continue;\n+\n+#ifdef ENABLE_CHECKING\n+\t  /* If add_dependence is working properly there should never\n+\t     be notes, deleted insns or duplicates in the backward\n+\t     links.  Thus we need not check for them here.\n+\n+\t     However, if we have enabled checking we might as well go\n+\t     ahead and verify that add_dependence worked properly.  */\n+\t  if (GET_CODE (x) == NOTE\n+\t      || INSN_DELETED_P (x)\n+\t      || (forward_dependency_cache != NULL\n+\t\t  && TEST_BIT (forward_dependency_cache[INSN_LUID (x)],\n+\t\t\t       INSN_LUID (insn)))\n+\t      || (forward_dependency_cache == NULL\n+\t\t  && find_insn_list (insn, INSN_DEPEND (x))))\n+\t    abort ();\n+\t  if (forward_dependency_cache != NULL)\n+\t    SET_BIT (forward_dependency_cache[INSN_LUID (x)],\n+\t\t     INSN_LUID (insn));\n+#endif\n+\n+\t  new_link = alloc_INSN_LIST (insn, INSN_DEPEND (x));\n+\n+\t  dep_type = REG_NOTE_KIND (link);\n+\t  PUT_REG_NOTE_KIND (new_link, dep_type);\n+\n+\t  INSN_DEPEND (x) = new_link;\n+\t  INSN_DEP_COUNT (insn) += 1;\n+\t}\n+    }\n+}\n+\f\n+/* Initialize variables for region data dependence analysis.\n+   n_bbs is the number of region blocks.  */\n+\n+void\n+init_deps (deps)\n+     struct deps *deps;\n+{\n+  int maxreg = max_reg_num ();\n+  deps->reg_last_uses = (rtx *) xcalloc (maxreg, sizeof (rtx));\n+  deps->reg_last_sets = (rtx *) xcalloc (maxreg, sizeof (rtx));\n+  deps->reg_last_clobbers = (rtx *) xcalloc (maxreg, sizeof (rtx));\n+\n+  deps->pending_read_insns = 0;\n+  deps->pending_read_mems = 0;\n+  deps->pending_write_insns = 0;\n+  deps->pending_write_mems = 0;\n+  deps->pending_lists_length = 0;\n+  deps->last_pending_memory_flush = 0;\n+  deps->last_function_call = 0;\n+  deps->in_post_call_group_p = 0;\n+\n+  deps->sched_before_next_call\n+    = gen_rtx_INSN (VOIDmode, 0, NULL_RTX, NULL_RTX,\n+\t\t    NULL_RTX, 0, NULL_RTX, NULL_RTX);\n+  LOG_LINKS (deps->sched_before_next_call) = 0;\n+}\n+\n+/* Free insn lists found in DEPS.  */\n+\n+void\n+free_deps (deps)\n+     struct deps *deps;\n+{\n+  int max_reg = max_reg_num ();\n+  int i;\n+\n+  /* Note this loop is executed max_reg * nr_regions times.  It's first\n+     implementation accounted for over 90% of the calls to free_INSN_LIST_list.\n+     The list was empty for the vast majority of those calls.  On the PA, not\n+     calling free_INSN_LIST_list in those cases improves -O2 compile times by\n+     3-5% on average.  */\n+  for (i = 0; i < max_reg; ++i)\n+    {\n+      if (deps->reg_last_clobbers[i])\n+\tfree_INSN_LIST_list (&deps->reg_last_clobbers[i]);\n+      if (deps->reg_last_sets[i])\n+\tfree_INSN_LIST_list (&deps->reg_last_sets[i]);\n+      if (deps->reg_last_uses[i])\n+\tfree_INSN_LIST_list (&deps->reg_last_uses[i]);\n+    }\n+}\n+\n+/* If it is profitable to use them, initialize caches for tracking\n+   dependency informatino.  LUID is the number of insns to be scheduled,\n+   it is used in the estimate of profitability.  */\n+\n+void\n+init_dependency_caches (luid)\n+     int luid;\n+{\n+  /* ?!? We could save some memory by computing a per-region luid mapping\n+     which could reduce both the number of vectors in the cache and the size\n+     of each vector.  Instead we just avoid the cache entirely unless the\n+     average number of instructions in a basic block is very high.  See\n+     the comment before the declaration of true_dependency_cache for\n+     what we consider \"very high\".  */\n+  if (luid / n_basic_blocks > 100 * 5)\n+    {\n+      true_dependency_cache = sbitmap_vector_alloc (luid, luid);\n+      sbitmap_vector_zero (true_dependency_cache, luid);\n+      anti_dependency_cache = sbitmap_vector_alloc (luid, luid);\n+      sbitmap_vector_zero (anti_dependency_cache, luid);\n+      output_dependency_cache = sbitmap_vector_alloc (luid, luid);\n+      sbitmap_vector_zero (output_dependency_cache, luid);\n+#ifdef ENABLE_CHECKING\n+      forward_dependency_cache = sbitmap_vector_alloc (luid, luid);\n+      sbitmap_vector_zero (forward_dependency_cache, luid);\n+#endif\n+    }\n+}\n+\n+/* Free the caches allocated in init_dependency_caches.  */\n+\n+void\n+free_dependency_caches ()\n+{\n+  if (true_dependency_cache)\n+    {\n+      free (true_dependency_cache);\n+      true_dependency_cache = NULL;\n+      free (anti_dependency_cache);\n+      anti_dependency_cache = NULL;\n+      free (output_dependency_cache);\n+      output_dependency_cache = NULL;\n+#ifdef ENABLE_CHECKING\n+      free (forward_dependency_cache);\n+      forward_dependency_cache = NULL;\n+#endif\n+    }\n+}\n+\n+/* Initialize some global variables needed by the dependency analysis\n+   code.  */\n+\n+void\n+init_deps_global ()\n+{\n+  reg_pending_sets = INITIALIZE_REG_SET (reg_pending_sets_head);\n+  reg_pending_clobbers = INITIALIZE_REG_SET (reg_pending_clobbers_head);\n+  reg_pending_sets_all = 0;\n+}\n+\n+/* Free everything used by the dependency analysis code.  */\n+\n+void\n+finish_deps_global ()\n+{\n+  FREE_REG_SET (reg_pending_sets);\n+  FREE_REG_SET (reg_pending_clobbers);\n+}"}, {"sha": "faa4a8b788a25fd184ffb36cfccec3a6a350d96f", "filename": "gcc/sched-int.h", "status": "modified", "additions": 161, "deletions": 0, "changes": 161, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/16f6ece6429c36ad89d4062c02f3af72168fdea9/gcc%2Fsched-int.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/16f6ece6429c36ad89d4062c02f3af72168fdea9/gcc%2Fsched-int.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-int.h?ref=16f6ece6429c36ad89d4062c02f3af72168fdea9", "patch": "@@ -23,6 +23,70 @@ the Free Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n /* Forward declaration.  */\n struct ready_list;\n \n+/* Describe state of dependencies used during sched_analyze phase.  */\n+struct deps\n+{\n+  /* The *_insns and *_mems are paired lists.  Each pending memory operation\n+     will have a pointer to the MEM rtx on one list and a pointer to the\n+     containing insn on the other list in the same place in the list.  */\n+\n+  /* We can't use add_dependence like the old code did, because a single insn\n+     may have multiple memory accesses, and hence needs to be on the list\n+     once for each memory access.  Add_dependence won't let you add an insn\n+     to a list more than once.  */\n+\n+  /* An INSN_LIST containing all insns with pending read operations.  */\n+  rtx pending_read_insns;\n+\n+  /* An EXPR_LIST containing all MEM rtx's which are pending reads.  */\n+  rtx pending_read_mems;\n+\n+  /* An INSN_LIST containing all insns with pending write operations.  */\n+  rtx pending_write_insns;\n+\n+  /* An EXPR_LIST containing all MEM rtx's which are pending writes.  */\n+  rtx pending_write_mems;\n+\n+  /* Indicates the combined length of the two pending lists.  We must prevent\n+     these lists from ever growing too large since the number of dependencies\n+     produced is at least O(N*N), and execution time is at least O(4*N*N), as\n+     a function of the length of these pending lists.  */\n+  int pending_lists_length;\n+\n+  /* The last insn upon which all memory references must depend.\n+     This is an insn which flushed the pending lists, creating a dependency\n+     between it and all previously pending memory references.  This creates\n+     a barrier (or a checkpoint) which no memory reference is allowed to cross.\n+\n+     This includes all non constant CALL_INSNs.  When we do interprocedural\n+     alias analysis, this restriction can be relaxed.\n+     This may also be an INSN that writes memory if the pending lists grow\n+     too large.  */\n+  rtx last_pending_memory_flush;\n+\n+  /* The last function call we have seen.  All hard regs, and, of course,\n+     the last function call, must depend on this.  */\n+  rtx last_function_call;\n+\n+  /* Used to keep post-call psuedo/hard reg movements together with\n+     the call.  */\n+  int in_post_call_group_p;\n+\n+  /* The LOG_LINKS field of this is a list of insns which use a pseudo\n+     register that does not already cross a call.  We create\n+     dependencies between each of those insn and the next call insn,\n+     to ensure that they won't cross a call after scheduling is done.  */\n+  rtx sched_before_next_call;\n+\n+  /* Element N is the next insn that sets (hard or pseudo) register\n+     N within the current basic block; or zero, if there is no\n+     such insn.  Needed for new registers which may be introduced\n+     by splitting insns.  */\n+  rtx *reg_last_uses;\n+  rtx *reg_last_sets;\n+  rtx *reg_last_clobbers;\n+};\n+\n /* This structure holds some state of the current scheduling pass, and\n    contains some function pointers that abstract out some of the non-generic\n    functionality from functions such as schedule_block or schedule_insn.\n@@ -63,10 +127,107 @@ struct sched_info\n   int queue_must_finish_empty;\n };\n \n+extern struct sched_info *current_sched_info;\n+\n+/* Indexed by INSN_UID, the collection of all data associated with\n+   a single instruction.  */\n+\n+struct haifa_insn_data\n+{\n+  /* A list of insns which depend on the instruction.  Unlike LOG_LINKS,\n+     it represents forward dependancies.  */\n+  rtx depend;\n+\n+  /* The line number note in effect for each insn.  For line number\n+     notes, this indicates whether the note may be reused.  */\n+  rtx line_note;\n+\n+  /* Logical uid gives the original ordering of the insns.  */\n+  int luid;\n+\n+  /* A priority for each insn.  */\n+  int priority;\n+\n+  /* The number of incoming edges in the forward dependency graph.\n+     As scheduling proceds, counts are decreased.  An insn moves to\n+     the ready queue when its counter reaches zero.  */\n+  int dep_count;\n+\n+  /* An encoding of the blockage range function.  Both unit and range\n+     are coded.  */\n+  unsigned int blockage;\n+\n+  /* Number of instructions referring to this insn.  */\n+  int ref_count;\n+\n+  /* The minimum clock tick at which the insn becomes ready.  This is\n+     used to note timing constraints for the insns in the pending list.  */\n+  int tick;\n+\n+  short cost;\n+\n+  /* An encoding of the function units used.  */\n+  short units;\n+\n+  /* This weight is an estimation of the insn's contribution to\n+     register pressure.  */\n+  short reg_weight;\n+\n+  /* Some insns (e.g. call) are not allowed to move across blocks.  */\n+  unsigned int cant_move : 1;\n+\n+  /* Set if there's DEF-USE dependance between some speculatively\n+     moved load insn and this one.  */\n+  unsigned int fed_by_spec_load : 1;\n+  unsigned int is_load_insn : 1;\n+};\n+\n+extern struct haifa_insn_data *h_i_d;\n+\n+/* Accessor macros for h_i_d.  There are more in haifa-sched.c.  */\n+#define INSN_DEPEND(INSN)\t(h_i_d[INSN_UID (INSN)].depend)\n+#define INSN_LUID(INSN)\t\t(h_i_d[INSN_UID (INSN)].luid)\n+#define CANT_MOVE(insn)\t\t(h_i_d[INSN_UID (insn)].cant_move)\n+#define INSN_DEP_COUNT(INSN)\t(h_i_d[INSN_UID (INSN)].dep_count)\n+\n+extern FILE *sched_dump;\n+\n #ifndef __GNUC__\n #define __inline\n #endif\n \n #ifndef HAIFA_INLINE\n #define HAIFA_INLINE __inline\n #endif\n+\n+/* Functions in sched-vis.c.  */\n+extern void init_target_units PARAMS ((void));\n+extern void insn_print_units PARAMS ((rtx));\n+extern void init_block_visualization PARAMS ((void));\n+extern void print_block_visualization PARAMS ((const char *));\n+extern void visualize_scheduled_insns PARAMS ((int));\n+extern void visualize_no_unit PARAMS ((rtx));\n+extern void visualize_stall_cycles PARAMS ((int));\n+extern void visualize_alloc PARAMS ((void));\n+extern void visualize_free PARAMS ((void));\n+\n+/* Functions in sched-deps.c.  */\n+extern void add_dependence PARAMS ((rtx, rtx, enum reg_note));\n+extern void add_insn_mem_dependence PARAMS ((struct deps *, rtx *, rtx *, rtx,\n+\t\t\t\t\t     rtx));\n+extern void sched_analyze PARAMS ((struct deps *, rtx, rtx));\n+extern void init_deps PARAMS ((struct deps *));\n+extern void free_deps PARAMS ((struct deps *));\n+extern void init_deps_global PARAMS ((void));\n+extern void finish_deps_global PARAMS ((void));\n+extern void compute_forward_dependences PARAMS ((rtx, rtx));\n+extern int find_insn_mem_list PARAMS ((rtx, rtx, rtx, rtx));\n+extern rtx find_insn_list PARAMS ((rtx, rtx));\n+extern void init_dependency_caches PARAMS ((int));\n+extern void free_dependency_caches PARAMS ((void));\n+\n+/* Functions in haifa-sched.c.  */\n+extern int insn_unit PARAMS ((rtx));\n+extern rtx get_unit_last_insn PARAMS ((int));\n+extern int actual_hazard_this_instance PARAMS ((int, int, rtx, int, int));\n+"}]}