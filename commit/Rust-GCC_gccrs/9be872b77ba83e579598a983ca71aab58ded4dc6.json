{"sha": "9be872b77ba83e579598a983ca71aab58ded4dc6", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OWJlODcyYjc3YmE4M2U1Nzk1OThhOTgzY2E3MWFhYjU4ZGVkNGRjNg==", "commit": {"author": {"name": "Zdenek Dvorak", "email": "dvorakz@suse.cz", "date": "2005-05-01T08:08:14Z"}, "committer": {"name": "Zdenek Dvorak", "email": "rakdver@gcc.gnu.org", "date": "2005-05-01T08:08:14Z"}, "message": "re PR tree-optimization/18316 (Missed IV optimization)\n\n\tPR tree-optimization/18316\n\tPR tree-optimization/19126\n\t* tree.c (build_int_cst_type): Avoid shift by size of type.\n\t* tree-scalar-evolution.c (simple_iv): Add allow_nonconstant_step\n\targument.\n\t* tree-scalar-evolution.h (simple_iv): Declaration changed.\n\t* tree-ssa-loop-ivopts.c (struct iv_cand): Add depends_on\n\tfield.\n\t(dump_cand): Dump depends_on information.\n\t(determine_biv_step): Add argument to simple_iv call.\n\t(contains_abnormal_ssa_name_p): Handle case expr == NULL.\n\t(find_bivs, find_givs_in_stmt_scev): Do not require step to be a\n\tconstant.\n\t(add_candidate_1): Record depends_on for candidates.\n\t(tree_int_cst_sign_bit, constant_multiple_of): New functions.\n\t(get_computation_at, get_computation_cost_at, may_eliminate_iv):\n\tHandle ivs with nonconstant step.\n\t(iv_ca_set_remove_invariants, iv_ca_set_add_invariants): New functions.\n\t(iv_ca_set_no_cp, iv_ca_set_cp): Handle cand->depends_on.\n\t(create_new_iv): Unshare the step before passing it to create_iv.\n\t(free_loop_data): Free cand->depends_on.\n\t(build_addr_strip_iref): New function.\n\t(find_interesting_uses_address): Use build_addr_strip_iref.\n\t(strip_offset_1): Split the recursive part from strip_offset.\n\tStrip constant offset component_refs and array_refs.\n\t(strip_offset): Split the recursive part to strip_offset_1.\n\t(add_address_candidates): Removed.\n\t(add_derived_ivs_candidates): Do not use add_address_candidates.\n\t(add_iv_value_candidates): Add candidates with stripped constant\n\toffset.  Consider all candidates with initial value 0 important.\n\t(struct affine_tree_combination): New.\n\t(aff_combination_const, aff_combination_elt, aff_combination_scale,\n\taff_combination_add_elt, aff_combination_add,\n\ttree_to_aff_combination, add_elt_to_tree, aff_combination_to_tree,\n\tfold_affine_sum): New functions.\n\t(get_computation_at): Use fold_affine_sum.\n\t* tree-ssa-loop-manip.c (create_iv): Handle ivs with nonconstant step.\n\t* tree-ssa-loop-niter.c (number_of_iterations_exit): Add argument\n\tto simple_iv call.\n\n\t* gcc.dg/tree-ssa/loop-8.c: New test.\n\nFrom-SVN: r99059", "tree": {"sha": "359e730fe382da42688f9c59944e546219c7b446", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/359e730fe382da42688f9c59944e546219c7b446"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/9be872b77ba83e579598a983ca71aab58ded4dc6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9be872b77ba83e579598a983ca71aab58ded4dc6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9be872b77ba83e579598a983ca71aab58ded4dc6", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9be872b77ba83e579598a983ca71aab58ded4dc6/comments", "author": null, "committer": null, "parents": [{"sha": "600f3598c72114182a6e2647bda341492734a4d3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/600f3598c72114182a6e2647bda341492734a4d3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/600f3598c72114182a6e2647bda341492734a4d3"}], "stats": {"total": 1031, "additions": 834, "deletions": 197}, "files": [{"sha": "af36b4e073de1c70d48c4f46e8880bc8adc0e6a0", "filename": "gcc/ChangeLog", "status": "modified", "additions": 42, "deletions": 0, "changes": 42, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=9be872b77ba83e579598a983ca71aab58ded4dc6", "patch": "@@ -1,3 +1,45 @@\n+2005-05-01  Zdenek Dvorak  <dvorakz@suse.cz>\n+\n+\tPR tree-optimization/18316\n+\tPR tree-optimization/19126\n+\t* tree.c (build_int_cst_type): Avoid shift by size of type.\n+\t* tree-scalar-evolution.c (simple_iv): Add allow_nonconstant_step\n+\targument.\n+\t* tree-scalar-evolution.h (simple_iv): Declaration changed.\n+\t* tree-ssa-loop-ivopts.c (struct iv_cand): Add depends_on\n+\tfield.\n+\t(dump_cand): Dump depends_on information.\n+\t(determine_biv_step): Add argument to simple_iv call.\n+\t(contains_abnormal_ssa_name_p): Handle case expr == NULL.\n+\t(find_bivs, find_givs_in_stmt_scev): Do not require step to be a\n+\tconstant.\n+\t(add_candidate_1): Record depends_on for candidates.\n+\t(tree_int_cst_sign_bit, constant_multiple_of): New functions.\n+\t(get_computation_at, get_computation_cost_at, may_eliminate_iv):\n+\tHandle ivs with nonconstant step.\n+\t(iv_ca_set_remove_invariants, iv_ca_set_add_invariants): New functions.\n+\t(iv_ca_set_no_cp, iv_ca_set_cp): Handle cand->depends_on.\n+\t(create_new_iv): Unshare the step before passing it to create_iv.\n+\t(free_loop_data): Free cand->depends_on.\n+\t(build_addr_strip_iref): New function.\n+\t(find_interesting_uses_address): Use build_addr_strip_iref.\n+\t(strip_offset_1): Split the recursive part from strip_offset.\n+\tStrip constant offset component_refs and array_refs.\n+\t(strip_offset): Split the recursive part to strip_offset_1.\n+\t(add_address_candidates): Removed.\n+\t(add_derived_ivs_candidates): Do not use add_address_candidates.\n+\t(add_iv_value_candidates): Add candidates with stripped constant\n+\toffset.  Consider all candidates with initial value 0 important.\n+\t(struct affine_tree_combination): New.\n+\t(aff_combination_const, aff_combination_elt, aff_combination_scale,\n+\taff_combination_add_elt, aff_combination_add,\n+\ttree_to_aff_combination, add_elt_to_tree, aff_combination_to_tree,\n+\tfold_affine_sum): New functions.\n+\t(get_computation_at): Use fold_affine_sum.\n+\t* tree-ssa-loop-manip.c (create_iv): Handle ivs with nonconstant step.\n+\t* tree-ssa-loop-niter.c (number_of_iterations_exit): Add argument\n+\tto simple_iv call.\n+\n 2005-04-30  Michael Matz  <matz@suse.de>\n \n \t* config/i386/i386.md (movmemsi): Also active when"}, {"sha": "f1c04ecedd5784fd1f80970fff12a15d1b0b5500", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=9be872b77ba83e579598a983ca71aab58ded4dc6", "patch": "@@ -1,3 +1,7 @@\n+2005-05-01  Zdenek Dvorak  <dvorakz@suse.cz>\n+\n+\t* gcc.dg/tree-ssa/loop-8.c: New test.\n+\n 2005-04-30  Michael Maty  <matz@suse.de>\n \n \t* gcc.dg/inline-mcpy.c: New test."}, {"sha": "9718aad33cf7398745a8a3381d38d115ef579e97", "filename": "gcc/testsuite/gcc.dg/tree-ssa/loop-8.c", "status": "added", "additions": 25, "deletions": 0, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Floop-8.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Floop-8.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Floop-8.c?ref=9be872b77ba83e579598a983ca71aab58ded4dc6", "patch": "@@ -0,0 +1,25 @@\n+/* A test for strength reduction of ivs with nonconstant step.  */\n+\n+/* { dg-do compile } */\n+/* { dg-options \"-O1 -fdump-tree-vars\" } */\n+\n+int bar (void);\n+\n+int a[100];\n+\n+void xxx (void)\n+{\n+  int iter, step = bar ();\n+\n+  for (iter = 0; iter < 10; iter++)\n+    a[iter * step] = bar ();\n+}\n+\n+/* The array access should be strength reduced.  But to determine the value of\n+   the step, we need to calculate step * sizeof (int), thus we need to be\n+   a bit careful about which multiplications we disallow.  */\n+\n+/* { dg-final { scan-tree-dump-times \"step \\\\* \\[^0-9\\]\" 0 \"vars\" } } */\n+/* { dg-final { scan-tree-dump-times \"\\[^0-9\\] \\\\* step\" 0 \"vars\" } } */\n+\n+/* { dg-final { cleanup-tree-dump \"vars\" } } */"}, {"sha": "c923409d6ed3565430bc82b17ccef2877722d754", "filename": "gcc/tree-scalar-evolution.c", "status": "modified", "additions": 13, "deletions": 3, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree-scalar-evolution.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree-scalar-evolution.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-scalar-evolution.c?ref=9be872b77ba83e579598a983ca71aab58ded4dc6", "patch": "@@ -2547,10 +2547,13 @@ scev_reset (void)\n }\n \n /* Checks whether OP behaves as a simple affine iv of LOOP in STMT and returns\n-   its BASE and STEP if possible.  */\n+   its BASE and STEP if possible.  If ALLOW_NONCONSTANT_STEP is true, we\n+   want STEP to be invariant in LOOP.  Otherwise we require it to be an\n+   integer constant.  */\n \n bool\n-simple_iv (struct loop *loop, tree stmt, tree op, tree *base, tree *step)\n+simple_iv (struct loop *loop, tree stmt, tree op, tree *base, tree *step,\n+\t   bool allow_nonconstant_step)\n {\n   basic_block bb = bb_for_stmt (stmt);\n   tree type, ev;\n@@ -2579,8 +2582,15 @@ simple_iv (struct loop *loop, tree stmt, tree op, tree *base, tree *step)\n     return false;\n \n   *step = CHREC_RIGHT (ev);\n-  if (TREE_CODE (*step) != INTEGER_CST)\n+  if (allow_nonconstant_step)\n+    {\n+      if (tree_contains_chrecs (*step, NULL)\n+\t  || chrec_contains_symbols_defined_in_loop (*step, loop->num))\n+\treturn false;\n+    }\n+  else if (TREE_CODE (*step) != INTEGER_CST)\n     return false;\n+\n   *base = CHREC_LEFT (ev);\n   if (tree_contains_chrecs (*base, NULL)\n       || chrec_contains_symbols_defined_in_loop (*base, loop->num))"}, {"sha": "def30b4e77ad37f468cf49fb07a2f647af2b53e0", "filename": "gcc/tree-scalar-evolution.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree-scalar-evolution.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree-scalar-evolution.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-scalar-evolution.h?ref=9be872b77ba83e579598a983ca71aab58ded4dc6", "patch": "@@ -32,6 +32,6 @@ extern tree analyze_scalar_evolution (struct loop *, tree);\n extern tree instantiate_parameters (struct loop *, tree);\n extern void gather_stats_on_scev_database (void);\n extern void scev_analysis (void);\n-extern bool simple_iv (struct loop *, tree, tree, tree *, tree *);\n+extern bool simple_iv (struct loop *, tree, tree, tree *, tree *, bool);\n \n #endif  /* GCC_TREE_SCALAR_EVOLUTION_H  */"}, {"sha": "1d8e308bfe033de112b0cec403004f9acd833cb1", "filename": "gcc/tree-ssa-loop-ivopts.c", "status": "modified", "additions": 732, "deletions": 182, "changes": 914, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree-ssa-loop-ivopts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree-ssa-loop-ivopts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-loop-ivopts.c?ref=9be872b77ba83e579598a983ca71aab58ded4dc6", "patch": "@@ -191,6 +191,8 @@ struct iv_cand\n \t\t\t   to replace the final value of an iv by direct\n \t\t\t   computation of the value.  */\n   unsigned cost;\t/* Cost of the candidate.  */\n+  bitmap depends_on;\t/* The list of invariants that are used in step of the\n+\t\t\t   biv.  */\n };\n \n /* The data used by the induction variable optimizations.  */\n@@ -492,6 +494,12 @@ dump_cand (FILE *file, struct iv_cand *cand)\n   fprintf (file, \"candidate %d%s\\n\",\n \t   cand->id, cand->important ? \" (important)\" : \"\");\n \n+  if (cand->depends_on)\n+    {\n+      fprintf (file, \"  depends on \");\n+      dump_bitmap (file, cand->depends_on);\n+    }\n+\n   if (!iv)\n     {\n       fprintf (file, \"  final value replacement\\n\");\n@@ -864,23 +872,23 @@ get_iv (struct ivopts_data *data, tree var)\n   return name_info (data, var)->iv;\n }\n \n-/* Determines the step of a biv defined in PHI.  */\n+/* Determines the step of a biv defined in PHI.  Returns NULL if PHI does\n+   not define a simple affine biv with nonzero step.  */\n \n static tree\n determine_biv_step (tree phi)\n {\n   struct loop *loop = bb_for_stmt (phi)->loop_father;\n   tree name = PHI_RESULT (phi), base, step;\n-  tree type = TREE_TYPE (name);\n \n   if (!is_gimple_reg (name))\n     return NULL_TREE;\n \n-  if (!simple_iv (loop, phi, name, &base, &step))\n+  if (!simple_iv (loop, phi, name, &base, &step, true))\n     return NULL_TREE;\n \n-  if (!step)\n-    return build_int_cst (type, 0);\n+  if (zero_p (step))\n+    return NULL_TREE;\n \n   return step;\n }\n@@ -923,9 +931,15 @@ idx_contains_abnormal_ssa_name_p (tree base, tree *index,\n static bool\n contains_abnormal_ssa_name_p (tree expr)\n {\n-  enum tree_code code = TREE_CODE (expr);\n-  enum tree_code_class class = TREE_CODE_CLASS (code);\n-    \n+  enum tree_code code;\n+  enum tree_code_class class;\n+\n+  if (!expr)\n+    return false;\n+\n+  code = TREE_CODE (expr);\n+  class = TREE_CODE_CLASS (code);\n+\n   if (code == SSA_NAME)\n     return SSA_NAME_OCCURS_IN_ABNORMAL_PHI (expr) != 0;\n \n@@ -974,25 +988,18 @@ find_bivs (struct ivopts_data *data)\n \tcontinue;\n \n       step = determine_biv_step (phi);\n-\n       if (!step)\n \tcontinue;\n-      if (cst_and_fits_in_hwi (step)\n-\t  && int_cst_value (step) == 0)\n-\tcontinue;\n \n       base = PHI_ARG_DEF_FROM_EDGE (phi, loop_preheader_edge (loop));\n-      if (contains_abnormal_ssa_name_p (base))\n+      if (contains_abnormal_ssa_name_p (base)\n+\t  || contains_abnormal_ssa_name_p (step))\n \tcontinue;\n \n       type = TREE_TYPE (PHI_RESULT (phi));\n       base = fold_convert (type, base);\n-      step = fold_convert (type, step);\n-\n-      /* FIXME: We do not handle induction variables whose step does\n-\t not satisfy cst_and_fits_in_hwi.  */\n-      if (!cst_and_fits_in_hwi (step))\n-\tcontinue;\n+      if (step)\n+\tstep = fold_convert (type, step);\n \n       set_iv (data, PHI_RESULT (phi), base, step);\n       found = true;\n@@ -1053,16 +1060,11 @@ find_givs_in_stmt_scev (struct ivopts_data *data, tree stmt,\n   if (TREE_CODE (lhs) != SSA_NAME)\n     return false;\n \n-  if (!simple_iv (loop, stmt, TREE_OPERAND (stmt, 1), base, step))\n+  if (!simple_iv (loop, stmt, TREE_OPERAND (stmt, 1), base, step, true))\n     return false;\n \n-  /* FIXME: We do not handle induction variables whose step does\n-     not satisfy cst_and_fits_in_hwi.  */\n-  if (!zero_p (*step)\n-      && !cst_and_fits_in_hwi (*step))\n-    return false;\n-\n-  if (contains_abnormal_ssa_name_p (*base))\n+  if (contains_abnormal_ssa_name_p (*base)\n+      || contains_abnormal_ssa_name_p (*step))\n     return false;\n \n   return true;\n@@ -1451,13 +1453,12 @@ idx_find_step (tree base, tree *idx, void *data)\n       return false;\n     }\n \n-  step = fold_binary_to_constant (MULT_EXPR, type, step, iv_step);\n+  step = fold_build2 (MULT_EXPR, type, step, iv_step);\n \n   if (!*dta->step_p)\n     *dta->step_p = step;\n   else\n-    *dta->step_p = fold_binary_to_constant (PLUS_EXPR, type,\n-\t\t\t\t\t    *dta->step_p, step);\n+    *dta->step_p = fold_build2 (PLUS_EXPR, type, *dta->step_p, step);\n \n   return true;\n }\n@@ -1509,6 +1510,25 @@ may_be_unaligned_p (tree ref)\n   return false;\n }\n \n+/* Builds ADDR_EXPR of object OBJ.  If OBJ is an INDIRECT_REF, the indirect_ref\n+   is stripped instead.  */\n+\n+static tree\n+build_addr_strip_iref (tree obj)\n+{\n+  tree type;\n+\n+  if (TREE_CODE (obj) == INDIRECT_REF)\n+    {\n+      type = build_pointer_type (TREE_TYPE (obj));\n+      obj = fold_convert (type, TREE_OPERAND (obj, 0));\n+    }\n+  else\n+    obj = build_addr (obj);\n+\n+  return obj;\n+}\n+\n /* Finds addresses in *OP_P inside STMT.  */\n \n static void\n@@ -1543,10 +1563,7 @@ find_interesting_uses_address (struct ivopts_data *data, tree stmt, tree *op_p)\n   gcc_assert (TREE_CODE (base) != ALIGN_INDIRECT_REF);\n   gcc_assert (TREE_CODE (base) != MISALIGNED_INDIRECT_REF);\n \n-  if (TREE_CODE (base) == INDIRECT_REF)\n-    base = TREE_OPERAND (base, 0);\n-  else\n-    base = build_addr (base);\n+  base = build_addr_strip_iref (base);\n \n   civ = alloc_iv (base, step);\n   record_use (data, op_p, civ, stmt, USE_ADDRESS);\n@@ -1758,18 +1775,21 @@ find_interesting_uses (struct ivopts_data *data)\n }\n \n /* Strips constant offsets from EXPR and stores them to OFFSET.  If INSIDE_ADDR\n-   is true, assume we are inside an address.  */\n+   is true, assume we are inside an address.  If TOP_COMPREF is true, assume\n+   we are at the top-level of the processed address.  */\n \n static tree\n-strip_offset (tree expr, bool inside_addr, unsigned HOST_WIDE_INT *offset)\n+strip_offset_1 (tree expr, bool inside_addr, bool top_compref,\n+\t\tunsigned HOST_WIDE_INT *offset)\n {\n-  tree op0 = NULL_TREE, op1 = NULL_TREE, step;\n+  tree op0 = NULL_TREE, op1 = NULL_TREE, tmp, step;\n   enum tree_code code;\n   tree type, orig_type = TREE_TYPE (expr);\n   unsigned HOST_WIDE_INT off0, off1, st;\n   tree orig_expr = expr;\n \n   STRIP_NOPS (expr);\n+\n   type = TREE_TYPE (expr);\n   code = TREE_CODE (expr);\n   *offset = 0;\n@@ -1789,8 +1809,8 @@ strip_offset (tree expr, bool inside_addr, unsigned HOST_WIDE_INT *offset)\n       op0 = TREE_OPERAND (expr, 0);\n       op1 = TREE_OPERAND (expr, 1);\n \n-      op0 = strip_offset (op0, false, &off0);\n-      op1 = strip_offset (op1, false, &off1);\n+      op0 = strip_offset_1 (op0, false, false, &off0);\n+      op1 = strip_offset_1 (op1, false, false, &off1);\n \n       *offset = (code == PLUS_EXPR ? off0 + off1 : off0 - off1);\n       if (op0 == TREE_OPERAND (expr, 0)\n@@ -1804,10 +1824,10 @@ strip_offset (tree expr, bool inside_addr, unsigned HOST_WIDE_INT *offset)\n \t  if (code == PLUS_EXPR)\n \t    expr = op1;\n \t  else\n-\t    expr = build1 (NEGATE_EXPR, type, op1);\n+\t    expr = fold_build1 (NEGATE_EXPR, type, op1);\n \t}\n       else\n-\texpr = build2 (code, type, op0, op1);\n+\texpr = fold_build2 (code, type, op0, op1);\n \n       return fold_convert (orig_type, expr);\n \n@@ -1821,17 +1841,49 @@ strip_offset (tree expr, bool inside_addr, unsigned HOST_WIDE_INT *offset)\n \n       st = int_cst_value (step);\n       op1 = TREE_OPERAND (expr, 1);\n-      op1 = strip_offset (op1, false, &off1);\n+      op1 = strip_offset_1 (op1, false, false, &off1);\n       *offset = off1 * st;\n+\n+      if (top_compref\n+\t  && zero_p (op1))\n+\t{\n+\t  /* Strip the component reference completely.  */\n+\t  op0 = TREE_OPERAND (expr, 0);\n+\t  op0 = strip_offset_1 (op0, inside_addr, top_compref, &off0);\n+\t  *offset += off0;\n+\t  return op0;\n+\t}\n       break;\n \n     case COMPONENT_REF:\n       if (!inside_addr)\n \treturn orig_expr;\n+\n+      tmp = component_ref_field_offset (expr);\n+      if (top_compref\n+\t  && cst_and_fits_in_hwi (tmp))\n+\t{\n+\t  /* Strip the component reference completely.  */\n+\t  op0 = TREE_OPERAND (expr, 0);\n+\t  op0 = strip_offset_1 (op0, inside_addr, top_compref, &off0);\n+\t  *offset = off0 + int_cst_value (tmp);\n+\t  return op0;\n+\t}\n       break;\n \n     case ADDR_EXPR:\n-      inside_addr = true;\n+      op0 = TREE_OPERAND (expr, 0);\n+      op0 = strip_offset_1 (op0, true, true, &off0);\n+      *offset += off0;\n+\n+      if (op0 == TREE_OPERAND (expr, 0))\n+\treturn orig_expr;\n+\n+      expr = build_addr_strip_iref (op0);\n+      return fold_convert (orig_type, expr);\n+\n+    case INDIRECT_REF:\n+      inside_addr = false;\n       break;\n \n     default:\n@@ -1841,7 +1893,7 @@ strip_offset (tree expr, bool inside_addr, unsigned HOST_WIDE_INT *offset)\n   /* Default handling of expressions for that we want to recurse into\n      the first operand.  */\n   op0 = TREE_OPERAND (expr, 0);\n-  op0 = strip_offset (op0, inside_addr, &off0);\n+  op0 = strip_offset_1 (op0, inside_addr, false, &off0);\n   *offset += off0;\n \n   if (op0 == TREE_OPERAND (expr, 0)\n@@ -1853,7 +1905,20 @@ strip_offset (tree expr, bool inside_addr, unsigned HOST_WIDE_INT *offset)\n   if (op1)\n     TREE_OPERAND (expr, 1) = op1;\n \n-  return fold_convert (orig_type, expr);\n+  /* Inside address, we might strip the top level component references,\n+     thus changing type of the expresion.  Handling of ADDR_EXPR\n+     will fix that.  */\n+  expr = fold_convert (orig_type, expr);\n+\n+  return expr;\n+}\n+\n+/* Strips constant offsets from EXPR and stores them to OFFSET.  */\n+\n+static tree\n+strip_offset (tree expr, unsigned HOST_WIDE_INT *offset)\n+{\n+  return strip_offset_1 (expr, false, false, offset);\n }\n \n /* Returns variant of TYPE that can be used as base for different uses.\n@@ -1872,6 +1937,30 @@ generic_type_for (tree type)\n   return unsigned_type_for (type);\n }\n \n+/* Records invariants in *EXPR_P.  Callback for walk_tree.  DATA contains\n+   the bitmap to that we should store it.  */\n+\n+static struct ivopts_data *fd_ivopts_data;\n+static tree\n+find_depends (tree *expr_p, int *ws ATTRIBUTE_UNUSED, void *data)\n+{\n+  bitmap *depends_on = data;\n+  struct version_info *info;\n+\n+  if (TREE_CODE (*expr_p) != SSA_NAME)\n+    return NULL_TREE;\n+  info = name_info (fd_ivopts_data, *expr_p);\n+\n+  if (!info->inv_id || info->has_nonlin_use)\n+    return NULL_TREE;\n+\n+  if (!*depends_on)\n+    *depends_on = BITMAP_ALLOC (NULL);\n+  bitmap_set_bit (*depends_on, info->inv_id);\n+\n+  return NULL_TREE;\n+}\n+\n /* Adds a candidate BASE + STEP * i.  Important field is set to IMPORTANT and\n    position to POS.  If USE is not NULL, the candidate is set as related to\n    it.  If both BASE and STEP are NULL, we add a pseudocandidate for the\n@@ -1954,6 +2043,13 @@ add_candidate_1 (struct ivopts_data *data,\n       cand->incremented_at = incremented_at;\n       VEC_safe_push (iv_cand_p, heap, data->iv_candidates, cand);\n \n+      if (step\n+\t  && TREE_CODE (step) != INTEGER_CST)\n+\t{\n+\t  fd_ivopts_data = data;\n+\t  walk_tree (&step, find_depends, &cand->depends_on, NULL);\n+\t}\n+\n       if (dump_file && (dump_flags & TDF_DETAILS))\n \tdump_cand (dump_file, cand);\n     }\n@@ -2089,50 +2185,19 @@ static void\n add_iv_value_candidates (struct ivopts_data *data,\n \t\t\t struct iv *iv, struct iv_use *use)\n {\n-  add_candidate (data, iv->base, iv->step, false, use);\n-\n-  /* The same, but with initial value zero.  */\n-  add_candidate (data, build_int_cst (TREE_TYPE (iv->base), 0),\n-\t\t iv->step, false, use);\n-}\n-\n-/* Adds candidates based on the address IV and USE.  */\n-\n-static void\n-add_address_candidates (struct ivopts_data *data,\n-\t\t\tstruct iv *iv, struct iv_use *use)\n-{\n-  tree base, abase;\n   unsigned HOST_WIDE_INT offset;\n+  tree base;\n \n-  /* First, the trivial choices.  */\n-  add_iv_value_candidates (data, iv, use);\n-\n-  /* Second, try removing the COMPONENT_REFs.  */\n-  if (TREE_CODE (iv->base) == ADDR_EXPR)\n-    {\n-      base = TREE_OPERAND (iv->base, 0);\n-      while (TREE_CODE (base) == COMPONENT_REF\n-\t     || (TREE_CODE (base) == ARRAY_REF\n-\t\t && TREE_CODE (TREE_OPERAND (base, 1)) == INTEGER_CST))\n-\tbase = TREE_OPERAND (base, 0);\n-\n-      if (base != TREE_OPERAND (iv->base, 0))\n-\t{ \n-\t  gcc_assert (TREE_CODE (base) != ALIGN_INDIRECT_REF);\n-\t  gcc_assert (TREE_CODE (base) != MISALIGNED_INDIRECT_REF);\n+  add_candidate (data, iv->base, iv->step, false, use);\n \n-\t  if (TREE_CODE (base) == INDIRECT_REF)\n-\t    base = TREE_OPERAND (base, 0);\n-\t  else\n-\t    base = build_addr (base);\n-\t  add_candidate (data, base, iv->step, false, use);\n-\t}\n-    }\n+  /* The same, but with initial value zero.  Make such variable important,\n+     since it is generic enough so that possibly many uses may be based\n+     on it.  */\n+  add_candidate (data, build_int_cst (TREE_TYPE (iv->base), 0),\n+\t\t iv->step, true, use);\n \n   /* Third, try removing the constant offset.  */\n-  abase = iv->base;\n-  base = strip_offset (abase, false, &offset);\n+  base = strip_offset (iv->base, &offset);\n   if (offset)\n     add_candidate (data, base, iv->step, false, use);\n }\n@@ -2172,6 +2237,7 @@ add_derived_ivs_candidates (struct ivopts_data *data)\n \t{\n \tcase USE_NONLINEAR_EXPR:\n \tcase USE_COMPARE:\n+\tcase USE_ADDRESS:\n \t  /* Just add the ivs based on the value of the iv used here.  */\n \t  add_iv_value_candidates (data, use->iv, use);\n \t  break;\n@@ -2184,10 +2250,6 @@ add_derived_ivs_candidates (struct ivopts_data *data)\n \t  add_iv_outer_candidates (data, use);\n \t  break;\n \n-\tcase USE_ADDRESS:\n-\t  add_address_candidates (data, use->iv, use);\n-\t  break;\n-\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -2494,6 +2556,452 @@ var_at_stmt (struct loop *loop, struct iv_cand *cand, tree stmt)\n     return cand->var_before;\n }\n \n+/* Return the most significant (sign) bit of T.  Similar to tree_int_cst_msb,\n+   but the bit is determined from TYPE_PRECISION, not MODE_BITSIZE.  */\n+\n+static int\n+tree_int_cst_sign_bit (tree t)\n+{\n+  unsigned bitno = TYPE_PRECISION (TREE_TYPE (t)) - 1;\n+  unsigned HOST_WIDE_INT w;\n+\n+  if (bitno < HOST_BITS_PER_WIDE_INT)\n+    w = TREE_INT_CST_LOW (t);\n+  else\n+    {\n+      w = TREE_INT_CST_HIGH (t);\n+      bitno -= HOST_BITS_PER_WIDE_INT;\n+    }\n+\n+  return (w >> bitno) & 1;\n+}\n+\n+/* If we can prove that TOP = cst * BOT for some constant cst in TYPE,\n+   return cst.  Otherwise return NULL_TREE.  */\n+\n+static tree\n+constant_multiple_of (tree type, tree top, tree bot)\n+{\n+  tree res, mby, p0, p1;\n+  enum tree_code code;\n+  bool negate;\n+\n+  STRIP_NOPS (top);\n+  STRIP_NOPS (bot);\n+\n+  if (operand_equal_p (top, bot, 0))\n+    return build_int_cst (type, 1);\n+\n+  code = TREE_CODE (top);\n+  switch (code)\n+    {\n+    case MULT_EXPR:\n+      mby = TREE_OPERAND (top, 1);\n+      if (TREE_CODE (mby) != INTEGER_CST)\n+\treturn NULL_TREE;\n+\n+      res = constant_multiple_of (type, TREE_OPERAND (top, 0), bot);\n+      if (!res)\n+\treturn NULL_TREE;\n+\n+      return fold_binary_to_constant (MULT_EXPR, type, res,\n+\t\t\t\t      fold_convert (type, mby));\n+\n+    case PLUS_EXPR:\n+    case MINUS_EXPR:\n+      p0 = constant_multiple_of (type, TREE_OPERAND (top, 0), bot);\n+      if (!p0)\n+\treturn NULL_TREE;\n+      p1 = constant_multiple_of (type, TREE_OPERAND (top, 1), bot);\n+      if (!p1)\n+\treturn NULL_TREE;\n+\n+      return fold_binary_to_constant (code, type, p0, p1);\n+\n+    case INTEGER_CST:\n+      if (TREE_CODE (bot) != INTEGER_CST)\n+\treturn NULL_TREE;\n+\n+      bot = fold_convert (type, bot);\n+      top = fold_convert (type, top);\n+\n+      /* If BOT seems to be negative, try dividing by -BOT instead, and negate\n+\t the result afterwards.  */\n+      if (tree_int_cst_sign_bit (bot))\n+\t{\n+\t  negate = true;\n+\t  bot = fold_unary_to_constant (NEGATE_EXPR, type, bot);\n+\t}\n+      else\n+\tnegate = false;\n+\n+      /* Ditto for TOP.  */\n+      if (tree_int_cst_sign_bit (top))\n+\t{\n+\t  negate = !negate;\n+\t  top = fold_unary_to_constant (NEGATE_EXPR, type, top);\n+\t}\n+\n+      if (!zero_p (fold_binary_to_constant (TRUNC_MOD_EXPR, type, top, bot)))\n+\treturn NULL_TREE;\n+\n+      res = fold_binary_to_constant (EXACT_DIV_EXPR, type, top, bot);\n+      if (negate)\n+\tres = fold_unary_to_constant (NEGATE_EXPR, type, res);\n+      return res;\n+\n+    default:\n+      return NULL_TREE;\n+    }\n+}\n+\n+/* Affine combination of trees.  We keep track of at most MAX_AFF_ELTS elements\n+   to make things simpler; this is sufficient in most cases.  */\n+\n+#define MAX_AFF_ELTS 8\n+\n+struct affine_tree_combination\n+{\n+  /* Type of the result of the combination.  */\n+  tree type;\n+\n+  /* Mask modulo that the operations are performed.  */\n+  unsigned HOST_WIDE_INT mask;\n+\n+  /* Constant offset.  */\n+  unsigned HOST_WIDE_INT offset;\n+\n+  /* Number of elements of the combination.  */\n+  unsigned n;\n+\n+  /* Elements and their coefficients.  */\n+  tree elts[MAX_AFF_ELTS];\n+  unsigned HOST_WIDE_INT coefs[MAX_AFF_ELTS];\n+\n+  /* Remainder of the expression.  */\n+  tree rest;\n+};\n+\n+/* Sets COMB to CST.  */\n+\n+static void\n+aff_combination_const (struct affine_tree_combination *comb, tree type,\n+\t\t       unsigned HOST_WIDE_INT cst)\n+{\n+  unsigned prec = TYPE_PRECISION (type);\n+\n+  comb->type = type;\n+  comb->mask = (((unsigned HOST_WIDE_INT) 2 << (prec - 1)) - 1);\n+\n+  comb->n = 0;\n+  comb->rest = NULL_TREE;\n+  comb->offset = cst & comb->mask;\n+}\n+\n+/* Sets COMB to single element ELT.  */\n+\n+static void\n+aff_combination_elt (struct affine_tree_combination *comb, tree type, tree elt)\n+{\n+  unsigned prec = TYPE_PRECISION (type);\n+\n+  comb->type = type;\n+  comb->mask = (((unsigned HOST_WIDE_INT) 2 << (prec - 1)) - 1);\n+\n+  comb->n = 1;\n+  comb->elts[0] = elt;\n+  comb->coefs[0] = 1;\n+  comb->rest = NULL_TREE;\n+  comb->offset = 0;\n+}\n+\n+/* Scales COMB by SCALE.  */\n+\n+static void\n+aff_combination_scale (struct affine_tree_combination *comb,\n+\t\t       unsigned HOST_WIDE_INT scale)\n+{\n+  unsigned i, j;\n+\n+  if (scale == 1)\n+    return;\n+\n+  if (scale == 0)\n+    {\n+      aff_combination_const (comb, comb->type, 0);\n+      return;\n+    }\n+\n+  comb->offset = (scale * comb->offset) & comb->mask;\n+  for (i = 0, j = 0; i < comb->n; i++)\n+    {\n+      comb->coefs[j] = (scale * comb->coefs[i]) & comb->mask;\n+      comb->elts[j] = comb->elts[i];\n+      if (comb->coefs[j] != 0)\n+\tj++;\n+    }\n+  comb->n = j;\n+\n+  if (comb->rest)\n+    {\n+      if (comb->n < MAX_AFF_ELTS)\n+\t{\n+\t  comb->coefs[comb->n] = scale;\n+\t  comb->elts[comb->n] = comb->rest;\n+\t  comb->rest = NULL_TREE;\n+\t  comb->n++;\n+\t}\n+      else\n+\tcomb->rest = fold_build2 (MULT_EXPR, comb->type, comb->rest,\n+\t\t\t\t  build_int_cst_type (comb->type, scale));\n+    }\n+}\n+\n+/* Adds ELT * SCALE to COMB.  */\n+\n+static void\n+aff_combination_add_elt (struct affine_tree_combination *comb, tree elt,\n+\t\t\t unsigned HOST_WIDE_INT scale)\n+{\n+  unsigned i;\n+\n+  if (scale == 0)\n+    return;\n+\n+  for (i = 0; i < comb->n; i++)\n+    if (operand_equal_p (comb->elts[i], elt, 0))\n+      {\n+\tcomb->coefs[i] = (comb->coefs[i] + scale) & comb->mask;\n+\tif (comb->coefs[i])\n+\t  return;\n+\n+\tcomb->n--;\n+\tcomb->coefs[i] = comb->coefs[comb->n];\n+\tcomb->elts[i] = comb->elts[comb->n];\n+\treturn;\n+      }\n+  if (comb->n < MAX_AFF_ELTS)\n+    {\n+      comb->coefs[comb->n] = scale;\n+      comb->elts[comb->n] = elt;\n+      comb->n++;\n+      return;\n+    }\n+\n+  if (scale == 1)\n+    elt = fold_convert (comb->type, elt);\n+  else\n+    elt = fold_build2 (MULT_EXPR, comb->type,\n+\t\t       fold_convert (comb->type, elt),\n+\t\t       build_int_cst_type (comb->type, scale)); \n+\n+  if (comb->rest)\n+    comb->rest = fold_build2 (PLUS_EXPR, comb->type, comb->rest, elt);\n+  else\n+    comb->rest = elt;\n+}\n+\n+/* Adds COMB2 to COMB1.  */\n+\n+static void\n+aff_combination_add (struct affine_tree_combination *comb1,\n+\t\t     struct affine_tree_combination *comb2)\n+{\n+  unsigned i;\n+\n+  comb1->offset = (comb1->offset + comb2->offset) & comb1->mask;\n+  for (i = 0; i < comb2-> n; i++)\n+    aff_combination_add_elt (comb1, comb2->elts[i], comb2->coefs[i]);\n+  if (comb2->rest)\n+    aff_combination_add_elt (comb1, comb2->rest, 1);\n+}\n+\n+/* Splits EXPR into an affine combination of parts.  */\n+\n+static void\n+tree_to_aff_combination (tree expr, tree type,\n+\t\t\t struct affine_tree_combination *comb)\n+{\n+  struct affine_tree_combination tmp;\n+  enum tree_code code;\n+  tree cst, core, toffset;\n+  HOST_WIDE_INT bitpos, bitsize;\n+  enum machine_mode mode;\n+  int unsignedp, volatilep;\n+\n+  STRIP_NOPS (expr);\n+\n+  code = TREE_CODE (expr);\n+  switch (code)\n+    {\n+    case INTEGER_CST:\n+      aff_combination_const (comb, type, int_cst_value (expr));\n+      return;\n+\n+    case PLUS_EXPR:\n+    case MINUS_EXPR:\n+      tree_to_aff_combination (TREE_OPERAND (expr, 0), type, comb);\n+      tree_to_aff_combination (TREE_OPERAND (expr, 1), type, &tmp);\n+      if (code == MINUS_EXPR)\n+\taff_combination_scale (&tmp, -1);\n+      aff_combination_add (comb, &tmp);\n+      return;\n+\n+    case MULT_EXPR:\n+      cst = TREE_OPERAND (expr, 1);\n+      if (TREE_CODE (cst) != INTEGER_CST)\n+\tbreak;\n+      tree_to_aff_combination (TREE_OPERAND (expr, 0), type, comb);\n+      aff_combination_scale (comb, int_cst_value (cst));\n+      return;\n+\n+    case NEGATE_EXPR:\n+      tree_to_aff_combination (TREE_OPERAND (expr, 0), type, comb);\n+      aff_combination_scale (comb, -1);\n+      return;\n+\n+    case ADDR_EXPR:\n+      core = get_inner_reference (TREE_OPERAND (expr, 0), &bitsize, &bitpos,\n+\t\t\t\t  &toffset, &mode, &unsignedp, &volatilep,\n+\t\t\t\t  false);\n+      if (bitpos % BITS_PER_UNIT != 0)\n+\tbreak;\n+      aff_combination_const (comb, type, bitpos / BITS_PER_UNIT);\n+      core = build_addr_strip_iref (core);\n+      if (TREE_CODE (core) == ADDR_EXPR)\n+\taff_combination_add_elt (comb, core, 1);\n+      else\n+\t{\n+\t  tree_to_aff_combination (core, type, &tmp);\n+\t  aff_combination_add (comb, &tmp);\n+\t}\n+      if (toffset)\n+\t{\n+\t  tree_to_aff_combination (toffset, type, &tmp);\n+\t  aff_combination_add (comb, &tmp);\n+\t}\n+      return;\n+\n+    default:\n+      break;\n+    }\n+\n+  aff_combination_elt (comb, type, expr);\n+}\n+\n+/* Creates EXPR + ELT * SCALE in TYPE.  MASK is the mask for width of TYPE.  */\n+\n+static tree\n+add_elt_to_tree (tree expr, tree type, tree elt, unsigned HOST_WIDE_INT scale,\n+\t\t unsigned HOST_WIDE_INT mask)\n+{\n+  enum tree_code code;\n+\n+  scale &= mask;\n+  elt = fold_convert (type, elt);\n+\n+  if (scale == 1)\n+    {\n+      if (!expr)\n+\treturn elt;\n+\n+      return fold_build2 (PLUS_EXPR, type, expr, elt);\n+    }\n+\n+  if (scale == mask)\n+    {\n+      if (!expr)\n+\treturn fold_build1 (NEGATE_EXPR, type, elt);\n+\n+      return fold_build2 (MINUS_EXPR, type, expr, elt);\n+    }\n+\n+  if (!expr)\n+    return fold_build2 (MULT_EXPR, type, elt,\n+\t\t\tbuild_int_cst_type (type, scale));\n+\n+  if ((scale | (mask >> 1)) == mask)\n+    {\n+      /* Scale is negative.  */\n+      code = MINUS_EXPR;\n+      scale = (-scale) & mask;\n+    }\n+  else\n+    code = PLUS_EXPR;\n+\n+  elt = fold_build2 (MULT_EXPR, type, elt,\n+\t\t     build_int_cst_type (type, scale));\n+  return fold_build2 (code, type, expr, elt);\n+}\n+\n+/* Makes tree from the affine combination COMB.  */\n+\n+static tree\n+aff_combination_to_tree (struct affine_tree_combination *comb)\n+{\n+  tree type = comb->type;\n+  tree expr = comb->rest;\n+  unsigned i;\n+  unsigned HOST_WIDE_INT off, sgn;\n+\n+  gcc_assert (comb->n == MAX_AFF_ELTS || comb->rest == NULL_TREE);\n+\n+  for (i = 0; i < comb->n; i++)\n+    expr = add_elt_to_tree (expr, type, comb->elts[i], comb->coefs[i],\n+\t\t\t    comb->mask);\n+\n+  if ((comb->offset | (comb->mask >> 1)) == comb->mask)\n+    {\n+      /* Offset is negative.  */\n+      off = (-comb->offset) & comb->mask;\n+      sgn = comb->mask;\n+    }\n+  else\n+    {\n+      off = comb->offset;\n+      sgn = 1;\n+    }\n+  return add_elt_to_tree (expr, type, build_int_cst_type (type, off), sgn,\n+\t\t\t  comb->mask);\n+}\n+\n+/* Folds X + RATIO * Y in TYPE.  */\n+\n+static tree\n+fold_affine_sum (tree type, tree x, tree y, HOST_WIDE_INT ratio)\n+{\n+  enum tree_code code;\n+  tree cst;\n+  struct affine_tree_combination cx, cy;\n+\n+  if (TYPE_PRECISION (type) > HOST_BITS_PER_WIDE_INT)\n+    {\n+      if (ratio == 1)\n+\treturn fold_build2 (PLUS_EXPR, type, x, y);\n+      if (ratio == -1)\n+\treturn fold_build2 (MINUS_EXPR, type, x, y);\n+\n+      if (ratio < 0)\n+\t{\n+\t  code = MINUS_EXPR;\n+\t  ratio = -ratio;\n+\t}\n+      else\n+\tcode = PLUS_EXPR;\n+\n+      cst = build_int_cst_type (type, ratio);\n+      y = fold_build2 (MULT_EXPR, type, y, cst);\n+      return fold_build2 (code, type, x, y);\n+    }\n+\n+  tree_to_aff_combination (x, type, &cx);\n+  tree_to_aff_combination (y, type, &cy);\n+  aff_combination_scale (&cy, ratio);\n+  aff_combination_add (&cx, &cy);\n+\n+  return aff_combination_to_tree (&cx);\n+}\n+\n /* Determines the expression by that USE is expressed from induction variable\n    CAND at statement AT in LOOP.  */\n \n@@ -2542,19 +3050,41 @@ get_computation_at (struct loop *loop,\n       cstep = fold_convert (uutype, cstep);\n     }\n \n-  if (!cst_and_fits_in_hwi (cstep)\n-      || !cst_and_fits_in_hwi (ustep))\n-    return NULL_TREE;\n+  if (cst_and_fits_in_hwi (cstep)\n+      && cst_and_fits_in_hwi (ustep))\n+    {\n+      ustepi = int_cst_value (ustep);\n+      cstepi = int_cst_value (cstep);\n \n-  ustepi = int_cst_value (ustep);\n-  cstepi = int_cst_value (cstep);\n+      if (!divide (TYPE_PRECISION (uutype), ustepi, cstepi, &ratioi))\n+\t{\n+\t  /* TODO maybe consider case when ustep divides cstep and the ratio is\n+\t     a power of 2 (so that the division is fast to execute)?  We would\n+\t     need to be much more careful with overflows etc. then.  */\n+\t  return NULL_TREE;\n+\t}\n \n-  if (!divide (TYPE_PRECISION (uutype), ustepi, cstepi, &ratioi))\n+      ratio = build_int_cst_type (uutype, ratioi);\n+    }\n+  else\n     {\n-      /* TODO maybe consider case when ustep divides cstep and the ratio is\n-\t a power of 2 (so that the division is fast to execute)?  We would\n-\t need to be much more careful with overflows etc. then.  */\n-      return NULL_TREE;\n+      ratio = constant_multiple_of (uutype, ustep, cstep);\n+      if (!ratio)\n+\treturn NULL_TREE;\n+\n+      /* Ratioi is only used to detect special cases when the multiplicative\n+\t factor is 1 or -1, so if we cannot convert ratio to HOST_WIDE_INT,\n+\t we may set it to 0.  We prefer cst_and_fits_in_hwi/int_cst_value\n+\t to integer_onep/integer_all_onesp, since the former ignores\n+\t TREE_OVERFLOW.  */\n+      if (cst_and_fits_in_hwi (ratio))\n+\tratioi = int_cst_value (ratio);\n+      else if (integer_onep (ratio))\n+\tratioi = 1;\n+      else if (integer_all_onesp (ratio))\n+\tratioi = -1;\n+      else\n+\tratioi = 0;\n     }\n \n   /* We may need to shift the value if we are after the increment.  */\n@@ -2571,21 +3101,25 @@ get_computation_at (struct loop *loop,\n \n   if (ratioi == 1)\n     {\n-      delta = fold (build2 (MINUS_EXPR, uutype, ubase, cbase));\n-      expr = fold (build2 (PLUS_EXPR, uutype, expr, delta));\n+      delta = fold_affine_sum (uutype, ubase, cbase, -1);\n+      expr = fold_build2 (PLUS_EXPR, uutype, expr, delta);\n     }\n   else if (ratioi == -1)\n     {\n-      delta = fold (build2 (PLUS_EXPR, uutype, ubase, cbase));\n-      expr = fold (build2 (MINUS_EXPR, uutype, delta, expr));\n+      delta = fold_affine_sum (uutype, ubase, cbase, 1);\n+      expr = fold_build2 (MINUS_EXPR, uutype, delta, expr);\n     }\n   else\n     {\n-      ratio = build_int_cst_type (uutype, ratioi);\n-      delta = fold (build2 (MULT_EXPR, uutype, ratio, cbase));\n-      delta = fold (build2 (MINUS_EXPR, uutype, ubase, delta));\n-      expr = fold (build2 (MULT_EXPR, uutype, ratio, expr));\n-      expr = fold (build2 (PLUS_EXPR, uutype, delta, expr));\n+      if (ratioi)\n+\tdelta = fold_affine_sum (uutype, ubase, cbase, -ratioi);\n+      else\n+\t{\n+\t  delta = fold_build2 (MULT_EXPR, uutype, ratio, cbase);\n+\t  delta = fold_affine_sum (uutype, ubase, delta, -1);\n+\t}\n+      expr = fold_build2 (MULT_EXPR, uutype, ratio, expr);\n+      expr = fold_build2 (PLUS_EXPR, uutype, delta, expr);\n     }\n \n   return fold_convert (utype, expr);\n@@ -2849,31 +3383,6 @@ get_address_cost (bool symbol_present, bool var_present,\n \n   return cost + acost;\n }\n-\n-/* Records invariants in *EXPR_P.  Callback for walk_tree.  DATA contains\n-   the bitmap to that we should store it.  */\n-\n-static struct ivopts_data *fd_ivopts_data;\n-static tree\n-find_depends (tree *expr_p, int *ws ATTRIBUTE_UNUSED, void *data)\n-{\n-  bitmap *depends_on = data;\n-  struct version_info *info;\n-\n-  if (TREE_CODE (*expr_p) != SSA_NAME)\n-    return NULL_TREE;\n-  info = name_info (fd_ivopts_data, *expr_p);\n-\n-  if (!info->inv_id || info->has_nonlin_use)\n-    return NULL_TREE;\n-\n-  if (!*depends_on)\n-    *depends_on = BITMAP_ALLOC (NULL);\n-  bitmap_set_bit (*depends_on, info->inv_id);\n-\n-  return NULL_TREE;\n-}\n-\n /* Estimates cost of forcing EXPR into a variable.  DEPENDS_ON is a set of the\n    invariants the computation depends on.  */\n \n@@ -3107,8 +3616,8 @@ difference_cost (struct ivopts_data *data,\n   enum machine_mode mode = TYPE_MODE (TREE_TYPE (e1));\n   unsigned HOST_WIDE_INT off1, off2;\n \n-  e1 = strip_offset (e1, false, &off1);\n-  e2 = strip_offset (e2, false, &off2);\n+  e1 = strip_offset (e1, &off1);\n+  e2 = strip_offset (e2, &off2);\n   *offset += off1 - off2;\n \n   STRIP_NOPS (e1);\n@@ -3191,29 +3700,49 @@ get_computation_cost_at (struct ivopts_data *data,\n \treturn INFTY;\n     }\n \n-  if (!cst_and_fits_in_hwi (ustep)\n-      || !cst_and_fits_in_hwi (cstep))\n-    return INFTY;\n-\n-  if (TREE_CODE (ubase) == INTEGER_CST\n-      && !cst_and_fits_in_hwi (ubase))\n-    goto fallback;\n-\n-  if (TREE_CODE (cbase) == INTEGER_CST\n-      && !cst_and_fits_in_hwi (cbase))\n-    goto fallback;\n-    \n-  ustepi = int_cst_value (ustep);\n-  cstepi = int_cst_value (cstep);\n-\n   if (TYPE_PRECISION (utype) != TYPE_PRECISION (ctype))\n     {\n       /* TODO -- add direct handling of this case.  */\n       goto fallback;\n     }\n \n-  if (!divide (TYPE_PRECISION (utype), ustepi, cstepi, &ratio))\n-    return INFTY;\n+  /* CSTEPI is removed from the offset in case statement is after the\n+     increment.  If the step is not constant, we use zero instead.\n+     This is a bit inprecise (there is the extra addition), but\n+     redundancy elimination is likely to transform the code so that\n+     it uses value of the variable before increment anyway,\n+     so it is not that much unrealistic.  */\n+  if (cst_and_fits_in_hwi (cstep))\n+    cstepi = int_cst_value (cstep);\n+  else\n+    cstepi = 0;\n+\n+  if (cst_and_fits_in_hwi (ustep)\n+      && cst_and_fits_in_hwi (cstep))\n+    {\n+      ustepi = int_cst_value (ustep);\n+\n+      if (!divide (TYPE_PRECISION (utype), ustepi, cstepi, &ratio))\n+\treturn INFTY;\n+    }\n+  else\n+    {\n+      tree rat;\n+      \n+      rat = constant_multiple_of (utype, ustep, cstep);\n+    \n+      if (!rat)\n+\treturn INFTY;\n+\n+      if (cst_and_fits_in_hwi (rat))\n+\tratio = int_cst_value (rat);\n+      else if (integer_onep (rat))\n+\tratio = 1;\n+      else if (integer_all_onesp (rat))\n+\tratio = -1;\n+      else\n+\treturn INFTY;\n+    }\n \n   /* use = ubase + ratio * (var - cbase).  If either cbase is a constant\n      or ratio == 1, it is better to handle this like\n@@ -3222,7 +3751,7 @@ get_computation_cost_at (struct ivopts_data *data,\n      \n      (also holds in the case ratio == -1, TODO.  */\n \n-  if (TREE_CODE (cbase) == INTEGER_CST)\n+  if (cst_and_fits_in_hwi (cbase))\n     {\n       offset = - ratio * int_cst_value (cbase); \n       cost += difference_cost (data,\n@@ -3432,6 +3961,9 @@ may_eliminate_iv (struct ivopts_data *data,\n   tree wider_type, period, per_type;\n   struct loop *loop = data->current_loop;\n   \n+  if (TREE_CODE (cand->iv->step) != INTEGER_CST)\n+    return false;\n+\n   /* For now works only for exits that dominate the loop latch.  TODO -- extend\n      for other conditions inside loop body.  */\n   ex_bb = bb_for_stmt (use->stmt);\n@@ -3905,16 +4437,33 @@ iv_ca_recount_cost (struct ivopts_data *data, struct iv_ca *ivs)\n   ivs->cost = cost;\n }\n \n+/* Remove invariants in set INVS to set IVS.  */\n+\n+static void\n+iv_ca_set_remove_invariants (struct iv_ca *ivs, bitmap invs)\n+{\n+  bitmap_iterator bi;\n+  unsigned iid;\n+\n+  if (!invs)\n+    return;\n+\n+  EXECUTE_IF_SET_IN_BITMAP (invs, 0, iid, bi)\n+    {\n+      ivs->n_invariant_uses[iid]--;\n+      if (ivs->n_invariant_uses[iid] == 0)\n+\tivs->n_regs--;\n+    }\n+}\n+\n /* Set USE not to be expressed by any candidate in IVS.  */\n \n static void\n iv_ca_set_no_cp (struct ivopts_data *data, struct iv_ca *ivs,\n \t\t struct iv_use *use)\n {\n-  unsigned uid = use->id, cid, iid;\n-  bitmap deps;\n+  unsigned uid = use->id, cid;\n   struct cost_pair *cp;\n-  bitmap_iterator bi;\n \n   cp = ivs->cand_for_use[uid];\n   if (!cp)\n@@ -3933,23 +4482,33 @@ iv_ca_set_no_cp (struct ivopts_data *data, struct iv_ca *ivs,\n \tivs->n_regs--;\n       ivs->n_cands--;\n       ivs->cand_cost -= cp->cand->cost;\n+\n+      iv_ca_set_remove_invariants (ivs, cp->cand->depends_on);\n     }\n \n   ivs->cand_use_cost -= cp->cost;\n \n-  deps = cp->depends_on;\n+  iv_ca_set_remove_invariants (ivs, cp->depends_on);\n+  iv_ca_recount_cost (data, ivs);\n+}\n+\n+/* Add invariants in set INVS to set IVS.  */\n \n-  if (deps)\n+static void\n+iv_ca_set_add_invariants (struct iv_ca *ivs, bitmap invs)\n+{\n+  bitmap_iterator bi;\n+  unsigned iid;\n+\n+  if (!invs)\n+    return;\n+\n+  EXECUTE_IF_SET_IN_BITMAP (invs, 0, iid, bi)\n     {\n-      EXECUTE_IF_SET_IN_BITMAP (deps, 0, iid, bi)\n-\t{\n-\t  ivs->n_invariant_uses[iid]--;\n-\t  if (ivs->n_invariant_uses[iid] == 0)\n-\t    ivs->n_regs--;\n-\t}\n+      ivs->n_invariant_uses[iid]++;\n+      if (ivs->n_invariant_uses[iid] == 1)\n+\tivs->n_regs++;\n     }\n-\n-  iv_ca_recount_cost (data, ivs);\n }\n \n /* Set cost pair for USE in set IVS to CP.  */\n@@ -3958,9 +4517,7 @@ static void\n iv_ca_set_cp (struct ivopts_data *data, struct iv_ca *ivs,\n \t      struct iv_use *use, struct cost_pair *cp)\n {\n-  unsigned uid = use->id, cid, iid;\n-  bitmap deps;\n-  bitmap_iterator bi;\n+  unsigned uid = use->id, cid;\n \n   if (ivs->cand_for_use[uid] == cp)\n     return;\n@@ -3983,22 +4540,12 @@ iv_ca_set_cp (struct ivopts_data *data, struct iv_ca *ivs,\n \t    ivs->n_regs++;\n \t  ivs->n_cands++;\n \t  ivs->cand_cost += cp->cand->cost;\n-\t}\n-\n-      ivs->cand_use_cost += cp->cost;\n-\n-      deps = cp->depends_on;\n \n-      if (deps)\n-\t{\n-\t  EXECUTE_IF_SET_IN_BITMAP (deps, 0, iid, bi)\n-\t    {\n-\t      ivs->n_invariant_uses[iid]++;\n-\t      if (ivs->n_invariant_uses[iid] == 1)\n-\t\tivs->n_regs++;\n-\t    }\n+\t  iv_ca_set_add_invariants (ivs, cp->cand->depends_on);\n \t}\n \n+      ivs->cand_use_cost += cp->cost;\n+      iv_ca_set_add_invariants (ivs, cp->depends_on);\n       iv_ca_recount_cost (data, ivs);\n     }\n }\n@@ -4681,7 +5228,8 @@ create_new_iv (struct ivopts_data *data, struct iv_cand *cand)\n \n   base = unshare_expr (cand->iv->base);\n \n-  create_iv (base, cand->iv->step, cand->var_before, data->current_loop,\n+  create_iv (base, unshare_expr (cand->iv->step),\n+\t     cand->var_before, data->current_loop,\n \t     &incr_pos, after, &cand->var_before, &cand->var_after);\n }\n \n@@ -5287,6 +5835,8 @@ free_loop_data (struct ivopts_data *data)\n \n       if (cand->iv)\n \tfree (cand->iv);\n+      if (cand->depends_on)\n+\tBITMAP_FREE (cand->depends_on);\n       free (cand);\n     }\n   VEC_truncate (iv_cand_p, data->iv_candidates, 0);"}, {"sha": "ff8f89986049068063c05e6941373afe18d2e9b2", "filename": "gcc/tree-ssa-loop-manip.c", "status": "modified", "additions": 8, "deletions": 5, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree-ssa-loop-manip.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree-ssa-loop-manip.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-loop-manip.c?ref=9be872b77ba83e579598a983ca71aab58ded4dc6", "patch": "@@ -54,6 +54,7 @@ create_iv (tree base, tree step, tree var, struct loop *loop,\n   tree stmt, initial, step1, stmts;\n   tree vb, va;\n   enum tree_code incr_op = PLUS_EXPR;\n+  edge pe = loop_preheader_edge (loop);\n \n   if (!var)\n     {\n@@ -92,6 +93,12 @@ create_iv (tree base, tree step, tree var, struct loop *loop,\n \t}\n     }\n \n+  /* Gimplify the step if necessary.  We put the computations in front of the\n+     loop (i.e. the step should be loop invariant).  */\n+  step = force_gimple_operand (step, &stmts, true, var);\n+  if (stmts)\n+    bsi_insert_on_edge_immediate_loop (pe, stmts);\n+\n   stmt = build2 (MODIFY_EXPR, void_type_node, va,\n \t\t build2 (incr_op, TREE_TYPE (base),\n \t\t\t vb, step));\n@@ -103,11 +110,7 @@ create_iv (tree base, tree step, tree var, struct loop *loop,\n \n   initial = force_gimple_operand (base, &stmts, true, var);\n   if (stmts)\n-    {\n-      edge pe = loop_preheader_edge (loop);\n-\n-      bsi_insert_on_edge_immediate_loop (pe, stmts);\n-    }\n+    bsi_insert_on_edge_immediate_loop (pe, stmts);\n \n   stmt = create_phi_node (vb, loop->header);\n   SSA_NAME_DEF_STMT (vb) = stmt;"}, {"sha": "5d0148713184d33063519d8fff80fdab1c74af41", "filename": "gcc/tree-ssa-loop-niter.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree-ssa-loop-niter.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree-ssa-loop-niter.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-loop-niter.c?ref=9be872b77ba83e579598a983ca71aab58ded4dc6", "patch": "@@ -952,9 +952,9 @@ number_of_iterations_exit (struct loop *loop, edge exit,\n       && !POINTER_TYPE_P (type))\n     return false;\n      \n-  if (!simple_iv (loop, stmt, op0, &base0, &step0))\n+  if (!simple_iv (loop, stmt, op0, &base0, &step0, false))\n     return false;\n-  if (!simple_iv (loop, stmt, op1, &base1, &step1))\n+  if (!simple_iv (loop, stmt, op1, &base1, &step1, false))\n     return false;\n \n   niter->niter = NULL_TREE;"}, {"sha": "48efe09ccdda7bacb8b40fc8a8943c832bf31b87", "filename": "gcc/tree.c", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9be872b77ba83e579598a983ca71aab58ded4dc6/gcc%2Ftree.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree.c?ref=9be872b77ba83e579598a983ca71aab58ded4dc6", "patch": "@@ -518,7 +518,7 @@ tree\n build_int_cst_type (tree type, HOST_WIDE_INT low)\n {\n   unsigned HOST_WIDE_INT val = (unsigned HOST_WIDE_INT) low;\n-  unsigned HOST_WIDE_INT hi;\n+  unsigned HOST_WIDE_INT hi, mask;\n   unsigned bits;\n   bool signed_p;\n   bool negative;\n@@ -538,10 +538,12 @@ build_int_cst_type (tree type, HOST_WIDE_INT low)\n       negative = ((val >> (bits - 1)) & 1) != 0;\n \n       /* Mask out the bits outside of the precision of the constant.  */\n+      mask = (((unsigned HOST_WIDE_INT) 2) << (bits - 1)) - 1;\n+\n       if (signed_p && negative)\n-\tval = val | ((~(unsigned HOST_WIDE_INT) 0) << bits);\n+\tval |= ~mask;\n       else\n-\tval = val & ~((~(unsigned HOST_WIDE_INT) 0) << bits);\n+\tval &= mask;\n     }\n \n   /* Determine the high bits.  */\n@@ -556,7 +558,8 @@ build_int_cst_type (tree type, HOST_WIDE_INT low)\n       else\n \t{\n \t  bits -= HOST_BITS_PER_WIDE_INT;\n-\t  hi = hi & ~((~(unsigned HOST_WIDE_INT) 0) << bits);\n+\t  mask = (((unsigned HOST_WIDE_INT) 2) << (bits - 1)) - 1;\n+\t  hi &= mask;\n \t}\n     }\n "}]}