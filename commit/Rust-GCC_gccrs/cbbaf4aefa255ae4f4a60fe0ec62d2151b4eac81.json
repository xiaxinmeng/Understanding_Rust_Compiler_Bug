{"sha": "cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6Y2JiYWY0YWVmYTI1NWFlNGY0YTYwZmUwZWM2MmQyMTUxYjRlYWM4MQ==", "commit": {"author": {"name": "J\"orn Rennecke", "email": "joern.rennecke@st.com", "date": "2005-05-11T12:24:43Z"}, "committer": {"name": "Joern Rennecke", "email": "amylaar@gcc.gnu.org", "date": "2005-05-11T12:24:43Z"}, "message": "re PR middle-end/20371 (Some corner cases of MS bitfields don't work)\n\n\tPR middle-end/20371:\n\t* tree.h (record_layout_info_s): New member prev_packed.\n\t* stor-layout.c (update_alignment_for_field): Fix comment about\n\tKNOWN_ALIGN.  For MS bitfields, if we start a new run, make sure\n\twe start it properly aligned.\n\t(place_field): At the beginning of a record, pass 0 as KNOWN_ALIGN\n\tto update_alignment_for_field, and recompute it afterwards using\n\tthe alignment of the record.\n\tWhen a packed bitfield precedes an MS bitfield, don't add padding\n\tat the end of the packed bitfield on behalf of the base type of\n\tthe packed bit field.\n\tDon't adjust rli->bitpos at the end\n\tof an MS bitfield run if we already adjusted bitpos/offset for an\n\talignment as large or larger than the bitfield type size.\n\tTake possible record alignment > BIGGEST_ALIGNMENT into account\n\twhen calculating actual_align.\n\tOnly put packed buit fields into rli->prev_field if they end up\n\tsuitably aligned.\n\tAlso set rli->remaining_in_alignment when we re-set rli->prev_field.\n\tUpdate rli->remaining_in_alignment when we have already started a\n\trun of bit fields and we process a packed bit field.\n\nFrom-SVN: r99574", "tree": {"sha": "0c50bf11c578486bacdaffc10fe47cf62d14a852", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/0c50bf11c578486bacdaffc10fe47cf62d14a852"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81", "html_url": "https://github.com/Rust-GCC/gccrs/commit/cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81/comments", "author": null, "committer": null, "parents": [{"sha": "4aad410db7d0ecc97870e0861f8ccda16fca2506", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4aad410db7d0ecc97870e0861f8ccda16fca2506", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4aad410db7d0ecc97870e0861f8ccda16fca2506"}], "stats": {"total": 140, "additions": 119, "deletions": 21}, "files": [{"sha": "145839cb65427d957d4bed6f3ba333c1dca669af", "filename": "gcc/ChangeLog", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81", "patch": "@@ -1,3 +1,27 @@\n+2005-05-11  J\"orn Rennecke <joern.rennecke@st.com>\n+\n+\tPR middle-end/20371:\n+\t* tree.h (record_layout_info_s): New member prev_packed.\n+\t* stor-layout.c (update_alignment_for_field): Fix comment about\n+\tKNOWN_ALIGN.  For MS bitfields, if we start a new run, make sure\n+\twe start it properly aligned.\n+\t(place_field): At the beginning of a record, pass 0 as KNOWN_ALIGN\n+\tto update_alignment_for_field, and recompute it afterwards using\n+\tthe alignment of the record.\n+\tWhen a packed bitfield precedes an MS bitfield, don't add padding\n+\tat the end of the packed bitfield on behalf of the base type of\n+\tthe packed bit field.\n+\tDon't adjust rli->bitpos at the end\n+\tof an MS bitfield run if we already adjusted bitpos/offset for an\n+\talignment as large or larger than the bitfield type size.\n+\tTake possible record alignment > BIGGEST_ALIGNMENT into account\n+\twhen calculating actual_align.\n+\tOnly put packed buit fields into rli->prev_field if they end up\n+\tsuitably aligned.\n+\tAlso set rli->remaining_in_alignment when we re-set rli->prev_field.\n+\tUpdate rli->remaining_in_alignment when we have already started a\n+\trun of bit fields and we process a packed bit field.\n+\n 2005-05-11  Sebastian Pop  <pop@cri.ensmp.fr>\n \n \t* tree-data-ref.c (find_data_references_in_loop): Give up when"}, {"sha": "cffb81c7f6277705e60a32deb95190d0369b0fc2", "filename": "gcc/stor-layout.c", "status": "modified", "additions": 92, "deletions": 21, "changes": 113, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81/gcc%2Fstor-layout.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81/gcc%2Fstor-layout.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fstor-layout.c?ref=cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81", "patch": "@@ -634,9 +634,9 @@ rli_size_so_far (record_layout_info rli)\n }\n \n /* FIELD is about to be added to RLI->T.  The alignment (in bits) of\n-   the next available location is given by KNOWN_ALIGN.  Update the\n-   variable alignment fields in RLI, and return the alignment to give\n-   the FIELD.  */\n+   the next available location within the record is given by KNOWN_ALIGN.\n+   Update the variable alignment fields in RLI, and return the alignment\n+   to give the FIELD.  */\n \n unsigned int\n update_alignment_for_field (record_layout_info rli, tree field,\n@@ -682,6 +682,18 @@ update_alignment_for_field (record_layout_info rli, tree field,\n \t    type_align = MIN (type_align, maximum_field_alignment);\n \t  rli->record_align = MAX (rli->record_align, type_align);\n \t  rli->unpacked_align = MAX (rli->unpacked_align, TYPE_ALIGN (type));\n+\t  /* If we start a new run, make sure we start it properly aligned.  */\n+\t  if ((!rli->prev_field\n+\t       || integer_zerop (DECL_SIZE (field))\n+\t       || integer_zerop (DECL_SIZE (rli->prev_field))\n+\t       || !host_integerp (DECL_SIZE (rli->prev_field), 0)\n+\t       || !host_integerp (TYPE_SIZE (type), 0)\n+\t       || !simple_cst_equal (TYPE_SIZE (type),\n+\t\t\t\t     TYPE_SIZE (TREE_TYPE (rli->prev_field)))\n+\t       || (rli->remaining_in_alignment\n+\t\t   < tree_low_cst (DECL_SIZE (field), 0)))\n+\t      && desired_align < type_align)\n+\t    desired_align = type_align;\n \t}\n     }\n #ifdef PCC_BITFIELD_TYPE_MATTERS\n@@ -820,7 +832,7 @@ place_field (record_layout_info rli, tree field)\n     known_align = (tree_low_cst (rli->bitpos, 1)\n \t\t   & - tree_low_cst (rli->bitpos, 1));\n   else if (integer_zerop (rli->offset))\n-    known_align = BIGGEST_ALIGNMENT;\n+    known_align = 0;\n   else if (host_integerp (rli->offset, 1))\n     known_align = (BITS_PER_UNIT\n \t\t   * (tree_low_cst (rli->offset, 1)\n@@ -829,6 +841,8 @@ place_field (record_layout_info rli, tree field)\n     known_align = rli->offset_align;\n \n   desired_align = update_alignment_for_field (rli, field, known_align);\n+  if (known_align == 0)\n+    known_align = MAX (BIGGEST_ALIGNMENT, rli->record_align);\n \n   if (warn_packed && DECL_PACKED (field))\n     {\n@@ -1001,18 +1015,30 @@ place_field (record_layout_info rli, tree field)\n \n \t      if (rli->remaining_in_alignment < bitsize)\n \t\t{\n-\t\t  /* out of bits; bump up to next 'word'.  */\n-\t\t  rli->offset = DECL_FIELD_OFFSET (rli->prev_field);\n-\t\t  rli->bitpos\n-\t\t    = size_binop (PLUS_EXPR, TYPE_SIZE (type),\n-\t\t\t\t  DECL_FIELD_BIT_OFFSET (rli->prev_field));\n-\t\t  rli->prev_field = field;\n-\t\t  rli->remaining_in_alignment\n-\t\t    = tree_low_cst (TYPE_SIZE (type), 0);\n+\t\t  /* If PREV_FIELD is packed, and we haven't lumped\n+\t\t     non-packed bitfields with it, treat this as if PREV_FIELD\n+\t\t     was not a bitfield.  This avoids anomalies where a packed\n+\t\t     bitfield with long long base type can take up more\n+\t\t     space than a same-size bitfield with base type short.  */\n+\t\t  if (rli->prev_packed)\n+\t\t    rli->prev_field = prev_saved = NULL;\n+\t\t  else\n+\t\t    {\n+\t\t      /* out of bits; bump up to next 'word'.  */\n+\t\t      rli->offset = DECL_FIELD_OFFSET (rli->prev_field);\n+\t\t      rli->bitpos\n+\t\t\t= size_binop (PLUS_EXPR, TYPE_SIZE (type),\n+\t\t\t\t      DECL_FIELD_BIT_OFFSET (rli->prev_field));\n+\t\t      rli->prev_field = field;\n+\t\t      rli->remaining_in_alignment\n+\t\t\t= tree_low_cst (TYPE_SIZE (type), 0) - bitsize;\n+\t\t    }\n \t\t}\n-\n-\t      rli->remaining_in_alignment -= bitsize;\n+\t      else\n+\t\trli->remaining_in_alignment -= bitsize;\n \t    }\n+\t  else if (rli->prev_packed)\n+\t    rli->prev_field = prev_saved = NULL;\n \t  else\n \t    {\n \t      /* End of a run: if leaving a run of bitfields of the same type\n@@ -1028,9 +1054,14 @@ place_field (record_layout_info rli, tree field)\n \t\t{\n \t\t  tree type_size = TYPE_SIZE (TREE_TYPE (rli->prev_field));\n \n-\t\t  rli->bitpos\n-\t\t    = size_binop (PLUS_EXPR, type_size,\n-\t\t\t\t  DECL_FIELD_BIT_OFFSET (rli->prev_field));\n+\t\t  /* If the desired alignment is greater or equal to TYPE_SIZE,\n+\t\t     we have already adjusted rli->bitpos / rli->offset above.\n+\t\t   */\n+\t\t  if ((unsigned HOST_WIDE_INT) tree_low_cst (type_size, 0)\n+\t\t      > desired_align)\n+\t\t    rli->bitpos\n+\t\t      = size_binop (PLUS_EXPR, type_size,\n+\t\t\t\t    DECL_FIELD_BIT_OFFSET (rli->prev_field));\n \t\t}\n \t      else\n \t\t/* We \"use up\" size zero fields; the code below should behave\n@@ -1044,6 +1075,7 @@ place_field (record_layout_info rli, tree field)\n \t\trli->prev_field = NULL;\n \t    }\n \n+\t  rli->prev_packed = 0;\n \t  normalize_rli (rli);\n         }\n \n@@ -1116,20 +1148,59 @@ place_field (record_layout_info rli, tree field)\n     actual_align = (tree_low_cst (DECL_FIELD_BIT_OFFSET (field), 1)\n \t\t    & - tree_low_cst (DECL_FIELD_BIT_OFFSET (field), 1));\n   else if (integer_zerop (DECL_FIELD_OFFSET (field)))\n-    actual_align = BIGGEST_ALIGNMENT;\n+    actual_align = MAX (BIGGEST_ALIGNMENT, rli->record_align);\n   else if (host_integerp (DECL_FIELD_OFFSET (field), 1))\n     actual_align = (BITS_PER_UNIT\n \t\t   * (tree_low_cst (DECL_FIELD_OFFSET (field), 1)\n \t\t      & - tree_low_cst (DECL_FIELD_OFFSET (field), 1)));\n   else\n     actual_align = DECL_OFFSET_ALIGN (field);\n+  /* ACTUAL_ALIGN is still the actual alignment *within the record* .\n+     store / extract bit field operations will check the alignment of the\n+     record against the mode of bit fields.  */\n \n   if (known_align != actual_align)\n     layout_decl (field, actual_align);\n \n-  /* Only the MS bitfields use this.  */\n-  if (rli->prev_field == NULL && DECL_BIT_FIELD_TYPE(field))\n-      rli->prev_field = field;\n+  if (DECL_BIT_FIELD_TYPE (field))\n+    {\n+      unsigned int type_align = TYPE_ALIGN (type);\n+\n+      /* Only the MS bitfields use this.  We used to also put any kind of\n+\t packed bit fields into prev_field, but that makes no sense, because\n+\t an 8 bit packed bit field shouldn't impose more restriction on\n+\t following fields than a char field, and the alignment requirements\n+\t are also not fulfilled.\n+\t There is no sane value to set rli->remaining_in_alignment to when\n+\t a packed bitfield in prev_field is unaligned.  */\n+      if (maximum_field_alignment != 0)\n+\ttype_align = MIN (type_align, maximum_field_alignment);\n+      gcc_assert (rli->prev_field\n+\t\t  || actual_align >= type_align || DECL_PACKED (field)\n+\t\t  || integer_zerop (DECL_SIZE (field))\n+\t\t  || !targetm.ms_bitfield_layout_p (rli->t));\n+      if (rli->prev_field == NULL && actual_align >= type_align\n+\t  && !integer_zerop (DECL_SIZE (field)))\n+\t{\n+\t  rli->prev_field = field;\n+\t  /* rli->remaining_in_alignment has not been set if the bitfield\n+\t     has size zero, or if it is a packed bitfield.  */\n+\t  rli->remaining_in_alignment\n+\t    = (tree_low_cst (TYPE_SIZE (TREE_TYPE (field)), 0)\n+\t       - tree_low_cst (DECL_SIZE (field), 0));\n+\t  rli->prev_packed = DECL_PACKED (field);\n+\n+\t}\n+      else if (rli->prev_field && DECL_PACKED (field))\n+\t{\n+\t  HOST_WIDE_INT bitsize = tree_low_cst (DECL_SIZE (field), 0);\n+\n+\t  if (rli->remaining_in_alignment < bitsize)\n+\t    rli->prev_field = NULL;\n+\t  else\n+\t    rli->remaining_in_alignment -= bitsize;\n+\t}\n+    }\n \n   /* Now add size of this field to the size of the record.  If the size is\n      not constant, treat the field as being a multiple of bytes and just"}, {"sha": "44a07afb676185727e7a3fda2999fc528dba6b3e", "filename": "gcc/tree.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81/gcc%2Ftree.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81/gcc%2Ftree.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree.h?ref=cbbaf4aefa255ae4f4a60fe0ec62d2151b4eac81", "patch": "@@ -3142,6 +3142,9 @@ typedef struct record_layout_info_s\n   tree pending_statics;\n   /* Bits remaining in the current alignment group */\n   int remaining_in_alignment;\n+  /* True if prev_field was packed and we haven't found any non-packed\n+     fields that we have put in the same alignment group.  */\n+  int prev_packed;\n   /* True if we've seen a packed field that didn't have normal\n      alignment anyway.  */\n   int packed_maybe_necessary;"}]}