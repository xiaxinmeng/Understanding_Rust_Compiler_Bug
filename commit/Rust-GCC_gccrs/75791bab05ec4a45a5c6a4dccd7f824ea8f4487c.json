{"sha": "75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NzU3OTFiYWIwNWVjNGE0NWE1YzZhNGRjY2Q3ZjgyNGVhOGY0NDg3Yw==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-08-30T21:07:47Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-08-30T21:07:47Z"}, "message": "runtime: use -fgo-c-header to build C header file\n    \n    Use the new -fgo-c-header option to build a header file for the Go\n    runtime code in libgo/go/runtime, and use the new header file in the C\n    runtime code in libgo/runtime.  This will ensure that the Go code and C\n    code share the same data structures as we convert the runtime from C to\n    Go.\n    \n    The new file libgo/go/runtime/runtime2.go is copied from the Go 1.7\n    release, and then edited to remove unnecessary data structures and\n    modify others for use with libgo.\n    \n    The new file libgo/go/runtime/mcache.go is an initial version of the\n    same files in the Go 1.7 release, and will be replaced by the Go 1.7\n    file when we convert to the new memory allocator.\n    \n    The new file libgo/go/runtime/type.go describes the gccgo version of the\n    reflection data structures, and replaces the Go 1.7 runtime file which\n    describes the gc version of those structures.\n    \n    Using the new header file means changing a number of struct fields to\n    use Go naming conventions (that is, no underscores) and to rename\n    constants to have a leading underscore so that they are not exported\n    from the Go package.  These names were updated in the C code.\n    \n    The C code was also changed to drop the thread-local variable m, as was\n    done some time ago in the gc sources.  Now the m field is always\n    accessed using g->m, where g is the single remaining thread-local\n    variable.  This in turn required some adjustments to set g->m correctly\n    in all cases.\n    \n    Also pass the new -fgo-compiling-runtime option when compiling the\n    runtime package, although that option doesn't do anything yet.\n    \n    Reviewed-on: https://go-review.googlesource.com/28051\n\nFrom-SVN: r239872", "tree": {"sha": "9c8130f09f3b1f343a58af6dd0f97f3a14f617c9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9c8130f09f3b1f343a58af6dd0f97f3a14f617c9"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/comments", "author": null, "committer": null, "parents": [{"sha": "7875b41f1d0137005d87cc5dd12b2a7df2f30c5e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7875b41f1d0137005d87cc5dd12b2a7df2f30c5e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7875b41f1d0137005d87cc5dd12b2a7df2f30c5e"}], "stats": {"total": 2757, "additions": 1757, "deletions": 1000}, "files": [{"sha": "633dfcfdd8aae3f3bc1073147b11ad4ae98a9dc6", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -1,4 +1,4 @@\n-9c91e7eeb404b5b639cd6e80e2a38da948bb35ec\n+394486a1cec9bbb81216311ed153179d9fe1c2c5\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "0a54a8972c5bcdeab201c4555ba3073ffd6fb2c5", "filename": "libgo/Makefile.am", "status": "modified", "additions": 34, "deletions": 12, "changes": 46, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2FMakefile.am", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2FMakefile.am", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2FMakefile.am?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -595,6 +595,16 @@ s-version: Makefile\n \t$(SHELL) $(srcdir)/mvifdiff.sh version.go.tmp version.go\n \t$(STAMP) $@\n \n+runtime_sysinfo.go: s-runtime_sysinfo; @true\n+s-runtime_sysinfo: sysinfo.go\n+\trm -f tmp-runtime_sysinfo.go\n+\techo 'package runtime' > tmp-runtime_sysinfo.go\n+\techo >> tmp-runtime_sysinfo.go\n+\tgrep 'const _sizeof_ucontext_t ' sysinfo.go >> tmp-runtime_sysinfo.go\n+\tgrep 'type _sigset_t ' sysinfo.go >> tmp-runtime_sysinfo.go\n+\t$(SHELL) $(srcdir)/mvifdiff.sh tmp-runtime_sysinfo.go runtime_sysinfo.go\n+\t$(STAMP) $@\n+\n noinst_DATA = zstdpkglist.go\n \n # Generate the list of go std packages that were included in libgo\n@@ -877,6 +887,13 @@ libgolibbegin_a_CFLAGS = $(AM_CFLAGS) -fPIC\n libnetgo_a_SOURCES =\n libnetgo_a_LIBADD = netgo.o\n \n+# Make sure runtime.inc is built before compiling any .c file.\n+$(libgo_la_OBJECTS): runtime.inc\n+$(libgo_llgo_la_OBJECTS): runtime.inc\n+$(libgobegin_a_OBJECTS): runtime.inc\n+$(libgobegin_llgo_a_OBJECTS): runtime.inc\n+$(libgolibbegin_a_OBJECTS): runtime.inc\n+\n LTLDFLAGS = $(shell $(SHELL) $(top_srcdir)/../libtool-ldflags $(LDFLAGS))\n \n GOCFLAGS = $(CFLAGS)\n@@ -904,7 +921,7 @@ BUILDDEPS = \\\n BUILDPACKAGE = \\\n \t$(MKDIR_P) $(@D); \\\n \tfiles=`echo $^ | sed -e 's/[^ ]*\\.gox//g' -e 's/[^ ]*\\.dep//'`; \\\n-\t$(LTGOCOMPILE) -I . -c -fgo-pkgpath=`echo $@ | sed -e 's/.lo$$//' -e 's/-go$$//'` -o $@ $$files\n+\t$(LTGOCOMPILE) -I . -c -fgo-pkgpath=`echo $@ | sed -e 's/.lo$$//' -e 's/-go$$//'` $($(subst -,_,$(subst .,_,$(subst /,_,$@)))_GOCFLAGS) -o $@ $$files\n \n # Build deps for netgo.o.\n BUILDNETGODEPS = \\\n@@ -1006,7 +1023,7 @@ bytes.lo.dep: $(srcdir)/go/bytes/*.go\n \t$(BUILDDEPS)\n bytes.lo:\n \t$(BUILDPACKAGE)\n-bytes/index.lo: go/bytes/indexbyte.c\n+bytes/index.lo: go/bytes/indexbyte.c runtime.inc\n \t@$(MKDIR_P) bytes\n \t$(LTCOMPILE) -c -o bytes/index.lo $(srcdir)/go/bytes/indexbyte.c\n bytes/check: $(CHECK_DEPS)\n@@ -1191,7 +1208,7 @@ reflect-go.lo:\n \t$(BUILDPACKAGE)\n reflect/check: $(CHECK_DEPS)\n \t@$(CHECK)\n-reflect/makefunc_ffi_c.lo: go/reflect/makefunc_ffi_c.c\n+reflect/makefunc_ffi_c.lo: go/reflect/makefunc_ffi_c.c runtime.inc\n \t@$(MKDIR_P) reflect\n \t$(LTCOMPILE) -c -o $@ $<\n .PHONY: reflect/check\n@@ -1205,13 +1222,18 @@ regexp/check: $(CHECK_DEPS)\n \t@$(CHECK)\n .PHONY: regexp/check\n \n-extra_go_files_runtime = version.go\n+extra_go_files_runtime = runtime_sysinfo.go version.go\n \n @go_include@ runtime-go.lo.dep\n-runtime-go.lo.dep: $(srcdir)/go/runtime/*.go\n+runtime-go.lo.dep: $(srcdir)/go/runtime/*.go $(extra_go_files_runtime)\n \t$(BUILDDEPS)\n+runtime_go_lo_GOCFLAGS = -fgo-c-header=runtime.inc.tmp -fgo-compiling-runtime\n runtime-go.lo:\n \t$(BUILDPACKAGE)\n+runtime.inc: s-runtime-inc; @true\n+s-runtime-inc: runtime-go.lo\n+\t$(SHELL) $(srcdir)/mvifdiff.sh runtime.inc.tmp runtime.inc\n+\t$(STAMP) $@\n runtime/check: $(CHECK_DEPS)\n \t@$(CHECK)\n .PHONY: runtime/check\n@@ -1239,7 +1261,7 @@ strings.lo.dep: $(srcdir)/go/strings/*.go\n \t$(BUILDDEPS)\n strings.lo:\n \t$(BUILDPACKAGE)\n-strings/index.lo: go/strings/indexbyte.c\n+strings/index.lo: go/strings/indexbyte.c runtime.inc\n \t@$(MKDIR_P) strings\n \t$(LTCOMPILE) -c -o strings/index.lo $(srcdir)/go/strings/indexbyte.c\n strings/check: $(CHECK_DEPS)\n@@ -2062,7 +2084,7 @@ log/syslog.lo.dep: $(srcdir)/go/log/syslog/*.go\n \t$(BUILDDEPS)\n log/syslog.lo:\n \t$(BUILDPACKAGE)\n-log/syslog/syslog_c.lo: go/log/syslog/syslog_c.c log/syslog.lo\n+log/syslog/syslog_c.lo: go/log/syslog/syslog_c.c runtime.inc log/syslog.lo\n \t@$(MKDIR_P) log/syslog\n \t$(LTCOMPILE) -c -o $@ $(srcdir)/go/log/syslog/syslog_c.c\n log/syslog/check: $(CHECK_DEPS)\n@@ -2348,7 +2370,7 @@ sync/atomic.lo.dep: $(srcdir)/go/sync/atomic/*.go\n \t$(BUILDDEPS)\n sync/atomic.lo:\n \t$(BUILDPACKAGE)\n-sync/atomic_c.lo: go/sync/atomic/atomic.c sync/atomic.lo\n+sync/atomic_c.lo: go/sync/atomic/atomic.c runtime.inc sync/atomic.lo\n \t$(LTCOMPILE) -c -o $@ $(srcdir)/go/sync/atomic/atomic.c\n sync/atomic/check: $(CHECK_DEPS)\n \t@$(CHECK)\n@@ -2427,17 +2449,17 @@ unicode/utf8/check: $(CHECK_DEPS)\n .PHONY: unicode/utf8/check\n \n @go_include@ syscall.lo.dep\n-syscall.lo.dep: $(srcdir)/go/syscall/*.go\n+syscall.lo.dep: $(srcdir)/go/syscall/*.go $(extra_go_files_syscall)\n \t$(BUILDDEPS)\n syscall.lo:\n \t$(BUILDPACKAGE)\n-syscall/errno.lo: go/syscall/errno.c\n+syscall/errno.lo: go/syscall/errno.c runtime.inc\n \t@$(MKDIR_P) syscall\n \t$(LTCOMPILE) -c -o $@ $<\n-syscall/signame.lo: go/syscall/signame.c\n+syscall/signame.lo: go/syscall/signame.c runtime.inc\n \t@$(MKDIR_P) syscall\n \t$(LTCOMPILE) -c -o $@ $<\n-syscall/wait.lo: go/syscall/wait.c\n+syscall/wait.lo: go/syscall/wait.c runtime.inc\n \t@$(MKDIR_P) syscall\n \t$(LTCOMPILE) -c -o $@ $<\n syscall/check: $(CHECK_DEPS)"}, {"sha": "d9c510028b681cf5ca11896b390f699dc56a7ca2", "filename": "libgo/Makefile.in", "status": "modified", "additions": 34, "deletions": 12, "changes": 46, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2FMakefile.in?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -1162,7 +1162,7 @@ BUILDDEPS = \\\n BUILDPACKAGE = \\\n \t$(MKDIR_P) $(@D); \\\n \tfiles=`echo $^ | sed -e 's/[^ ]*\\.gox//g' -e 's/[^ ]*\\.dep//'`; \\\n-\t$(LTGOCOMPILE) -I . -c -fgo-pkgpath=`echo $@ | sed -e 's/.lo$$//' -e 's/-go$$//'` -o $@ $$files\n+\t$(LTGOCOMPILE) -I . -c -fgo-pkgpath=`echo $@ | sed -e 's/.lo$$//' -e 's/-go$$//'` $($(subst -,_,$(subst .,_,$(subst /,_,$@)))_GOCFLAGS) -o $@ $$files\n \n \n # Build deps for netgo.o.\n@@ -1235,7 +1235,8 @@ CHECK_DEPS = $(toolexeclibgo_DATA) $(toolexeclibgoarchive_DATA) \\\n @HAVE_STAT_TIMESPEC_FALSE@@LIBGO_IS_SOLARIS_TRUE@matchargs_os = \n @HAVE_STAT_TIMESPEC_TRUE@@LIBGO_IS_SOLARIS_TRUE@matchargs_os = --tag=solaristag\n @LIBGO_IS_SOLARIS_FALSE@matchargs_os = \n-extra_go_files_runtime = version.go\n+extra_go_files_runtime = runtime_sysinfo.go version.go\n+runtime_go_lo_GOCFLAGS = -fgo-c-header=runtime.inc.tmp -fgo-compiling-runtime\n @LIBGO_IS_BSD_TRUE@golang_org_x_net_route_lo = \\\n @LIBGO_IS_BSD_TRUE@\tgolang_org/x/net/route/route.lo\n \n@@ -3570,6 +3571,16 @@ s-version: Makefile\n \t$(SHELL) $(srcdir)/mvifdiff.sh version.go.tmp version.go\n \t$(STAMP) $@\n \n+runtime_sysinfo.go: s-runtime_sysinfo; @true\n+s-runtime_sysinfo: sysinfo.go\n+\trm -f tmp-runtime_sysinfo.go\n+\techo 'package runtime' > tmp-runtime_sysinfo.go\n+\techo >> tmp-runtime_sysinfo.go\n+\tgrep 'const _sizeof_ucontext_t ' sysinfo.go >> tmp-runtime_sysinfo.go\n+\tgrep 'type _sigset_t ' sysinfo.go >> tmp-runtime_sysinfo.go\n+\t$(SHELL) $(srcdir)/mvifdiff.sh tmp-runtime_sysinfo.go runtime_sysinfo.go\n+\t$(STAMP) $@\n+\n # Generate the list of go std packages that were included in libgo\n zstdpkglist.go: s-zstdpkglist; @true\n s-zstdpkglist: Makefile\n@@ -3639,6 +3650,13 @@ s-epoll: Makefile\n \t$(SHELL) $(srcdir)/mvifdiff.sh epoll.go.tmp epoll.go\n \t$(STAMP) $@\n \n+# Make sure runtime.inc is built before compiling any .c file.\n+$(libgo_la_OBJECTS): runtime.inc\n+$(libgo_llgo_la_OBJECTS): runtime.inc\n+$(libgobegin_a_OBJECTS): runtime.inc\n+$(libgobegin_llgo_a_OBJECTS): runtime.inc\n+$(libgolibbegin_a_OBJECTS): runtime.inc\n+\n @go_include@ bufio.lo.dep\n bufio.lo.dep: $(srcdir)/go/bufio/*.go\n \t$(BUILDDEPS)\n@@ -3653,7 +3671,7 @@ bytes.lo.dep: $(srcdir)/go/bytes/*.go\n \t$(BUILDDEPS)\n bytes.lo:\n \t$(BUILDPACKAGE)\n-bytes/index.lo: go/bytes/indexbyte.c\n+bytes/index.lo: go/bytes/indexbyte.c runtime.inc\n \t@$(MKDIR_P) bytes\n \t$(LTCOMPILE) -c -o bytes/index.lo $(srcdir)/go/bytes/indexbyte.c\n bytes/check: $(CHECK_DEPS)\n@@ -3828,7 +3846,7 @@ reflect-go.lo:\n \t$(BUILDPACKAGE)\n reflect/check: $(CHECK_DEPS)\n \t@$(CHECK)\n-reflect/makefunc_ffi_c.lo: go/reflect/makefunc_ffi_c.c\n+reflect/makefunc_ffi_c.lo: go/reflect/makefunc_ffi_c.c runtime.inc\n \t@$(MKDIR_P) reflect\n \t$(LTCOMPILE) -c -o $@ $<\n .PHONY: reflect/check\n@@ -3843,10 +3861,14 @@ regexp/check: $(CHECK_DEPS)\n .PHONY: regexp/check\n \n @go_include@ runtime-go.lo.dep\n-runtime-go.lo.dep: $(srcdir)/go/runtime/*.go\n+runtime-go.lo.dep: $(srcdir)/go/runtime/*.go $(extra_go_files_runtime)\n \t$(BUILDDEPS)\n runtime-go.lo:\n \t$(BUILDPACKAGE)\n+runtime.inc: s-runtime-inc; @true\n+s-runtime-inc: runtime-go.lo\n+\t$(SHELL) $(srcdir)/mvifdiff.sh runtime.inc.tmp runtime.inc\n+\t$(STAMP) $@\n runtime/check: $(CHECK_DEPS)\n \t@$(CHECK)\n .PHONY: runtime/check\n@@ -3874,7 +3896,7 @@ strings.lo.dep: $(srcdir)/go/strings/*.go\n \t$(BUILDDEPS)\n strings.lo:\n \t$(BUILDPACKAGE)\n-strings/index.lo: go/strings/indexbyte.c\n+strings/index.lo: go/strings/indexbyte.c runtime.inc\n \t@$(MKDIR_P) strings\n \t$(LTCOMPILE) -c -o strings/index.lo $(srcdir)/go/strings/indexbyte.c\n strings/check: $(CHECK_DEPS)\n@@ -4688,7 +4710,7 @@ log/syslog.lo.dep: $(srcdir)/go/log/syslog/*.go\n \t$(BUILDDEPS)\n log/syslog.lo:\n \t$(BUILDPACKAGE)\n-log/syslog/syslog_c.lo: go/log/syslog/syslog_c.c log/syslog.lo\n+log/syslog/syslog_c.lo: go/log/syslog/syslog_c.c runtime.inc log/syslog.lo\n \t@$(MKDIR_P) log/syslog\n \t$(LTCOMPILE) -c -o $@ $(srcdir)/go/log/syslog/syslog_c.c\n log/syslog/check: $(CHECK_DEPS)\n@@ -4970,7 +4992,7 @@ sync/atomic.lo.dep: $(srcdir)/go/sync/atomic/*.go\n \t$(BUILDDEPS)\n sync/atomic.lo:\n \t$(BUILDPACKAGE)\n-sync/atomic_c.lo: go/sync/atomic/atomic.c sync/atomic.lo\n+sync/atomic_c.lo: go/sync/atomic/atomic.c runtime.inc sync/atomic.lo\n \t$(LTCOMPILE) -c -o $@ $(srcdir)/go/sync/atomic/atomic.c\n sync/atomic/check: $(CHECK_DEPS)\n \t@$(CHECK)\n@@ -5049,17 +5071,17 @@ unicode/utf8/check: $(CHECK_DEPS)\n .PHONY: unicode/utf8/check\n \n @go_include@ syscall.lo.dep\n-syscall.lo.dep: $(srcdir)/go/syscall/*.go\n+syscall.lo.dep: $(srcdir)/go/syscall/*.go $(extra_go_files_syscall)\n \t$(BUILDDEPS)\n syscall.lo:\n \t$(BUILDPACKAGE)\n-syscall/errno.lo: go/syscall/errno.c\n+syscall/errno.lo: go/syscall/errno.c runtime.inc\n \t@$(MKDIR_P) syscall\n \t$(LTCOMPILE) -c -o $@ $<\n-syscall/signame.lo: go/syscall/signame.c\n+syscall/signame.lo: go/syscall/signame.c runtime.inc\n \t@$(MKDIR_P) syscall\n \t$(LTCOMPILE) -c -o $@ $<\n-syscall/wait.lo: go/syscall/wait.c\n+syscall/wait.lo: go/syscall/wait.c runtime.inc\n \t@$(MKDIR_P) syscall\n \t$(LTCOMPILE) -c -o $@ $<\n syscall/check: $(CHECK_DEPS)"}, {"sha": "e383e0d57d0452dfb8bf29d29a90d58fe8e2c459", "filename": "libgo/go/runtime/mcache.go", "status": "added", "additions": 102, "deletions": 0, "changes": 102, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fgo%2Fruntime%2Fmcache.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fgo%2Fruntime%2Fmcache.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmcache.go?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -0,0 +1,102 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime\n+\n+// This is a temporary mcache.go for gccgo.\n+// At some point it will be replaced by the one in the gc runtime package.\n+\n+import \"unsafe\"\n+\n+const (\n+\t// Computed constant. The definition of MaxSmallSize and the\n+\t// algorithm in msize.go produces some number of different allocation\n+\t// size classes. NumSizeClasses is that number. It's needed here\n+\t// because there are static arrays of this length; when msize runs its\n+\t// size choosing algorithm it double-checks that NumSizeClasses agrees.\n+\t_NumSizeClasses = 67\n+)\n+\n+type mcachelist struct {\n+\tlist  *mlink\n+\tnlist uint32\n+}\n+\n+// Per-thread (in Go, per-P) cache for small objects.\n+// No locking needed because it is per-thread (per-P).\n+//\n+// mcaches are allocated from non-GC'd memory, so any heap pointers\n+// must be specially handled.\n+type mcache struct {\n+\t// The following members are accessed on every malloc,\n+\t// so they are grouped here for better caching.\n+\tnext_sample      int32   // trigger heap sample after allocating this many bytes\n+\tlocal_cachealloc uintptr // bytes allocated (or freed) from cache since last lock of heap\n+\n+\t// Allocator cache for tiny objects w/o pointers.\n+\t// See \"Tiny allocator\" comment in malloc.go.\n+\n+\t// tiny points to the beginning of the current tiny block, or\n+\t// nil if there is no current tiny block.\n+\t//\n+\t// tiny is a heap pointer. Since mcache is in non-GC'd memory,\n+\t// we handle it by clearing it in releaseAll during mark\n+\t// termination.\n+\ttiny     unsafe.Pointer\n+\ttinysize uintptr\n+\n+\t// The rest is not accessed on every malloc.\n+\talloc [_NumSizeClasses]*mspan     // spans to allocate from\n+\tfree  [_NumSizeClasses]mcachelist // lists of explicitly freed objects\n+\n+\t// Local allocator stats, flushed during GC.\n+\tlocal_nlookup    uintptr                  // number of pointer lookups\n+\tlocal_largefree  uintptr                  // bytes freed for large objects (>maxsmallsize)\n+\tlocal_nlargefree uintptr                  // number of frees for large objects (>maxsmallsize)\n+\tlocal_nsmallfree [_NumSizeClasses]uintptr // number of frees for small objects (<=maxsmallsize)\n+}\n+\n+type mtypes struct {\n+\tcompression byte\n+\tdata        uintptr\n+}\n+\n+type special struct {\n+\tnext   *special\n+\toffset uint16\n+\tkind   byte\n+}\n+\n+type mspan struct {\n+\tnext     *mspan // next span in list, or nil if none\n+\tprev     *mspan // previous span's next field, or list head's first field if none\n+\tstart    uintptr\n+\tnpages   uintptr // number of pages in span\n+\tfreelist *mlink\n+\n+\t// sweep generation:\n+\t// if sweepgen == h->sweepgen - 2, the span needs sweeping\n+\t// if sweepgen == h->sweepgen - 1, the span is currently being swept\n+\t// if sweepgen == h->sweepgen, the span is swept and ready to use\n+\t// h->sweepgen is incremented by 2 after every GC\n+\n+\tsweepgen    uint32\n+\tref         uint16\n+\tsizeclass   uint8   // size class\n+\tincache     bool    // being used by an mcache\n+\tstate       uint8   // mspaninuse etc\n+\tneedzero    uint8   // needs to be zeroed before allocation\n+\telemsize    uintptr // computed from sizeclass or from npages\n+\tunusedsince int64   // first time spotted by gc in mspanfree state\n+\tnpreleased  uintptr // number of pages released to the os\n+\tlimit       uintptr // end of data in span\n+\ttypes       mtypes\n+\tspeciallock mutex    // guards specials list\n+\tspecials    *special // linked list of special records sorted by offset.\n+\tfreebuf     *mlink\n+}\n+\n+type mlink struct {\n+\tnext *mlink\n+}"}, {"sha": "05ce513aa9a7da65b22369bba29ee89a6e092c9b", "filename": "libgo/go/runtime/runtime2.go", "status": "added", "additions": 835, "deletions": 0, "changes": 835, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fgo%2Fruntime%2Fruntime2.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fgo%2Fruntime%2Fruntime2.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime2.go?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -0,0 +1,835 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime\n+\n+import (\n+\t\"unsafe\"\n+)\n+\n+// defined constants\n+const (\n+\t// G status\n+\t//\n+\t// Beyond indicating the general state of a G, the G status\n+\t// acts like a lock on the goroutine's stack (and hence its\n+\t// ability to execute user code).\n+\t//\n+\t// If you add to this list, add to the list\n+\t// of \"okay during garbage collection\" status\n+\t// in mgcmark.go too.\n+\n+\t// _Gidle means this goroutine was just allocated and has not\n+\t// yet been initialized.\n+\t_Gidle = iota // 0\n+\n+\t// _Grunnable means this goroutine is on a run queue. It is\n+\t// not currently executing user code. The stack is not owned.\n+\t_Grunnable // 1\n+\n+\t// _Grunning means this goroutine may execute user code. The\n+\t// stack is owned by this goroutine. It is not on a run queue.\n+\t// It is assigned an M and a P.\n+\t_Grunning // 2\n+\n+\t// _Gsyscall means this goroutine is executing a system call.\n+\t// It is not executing user code. The stack is owned by this\n+\t// goroutine. It is not on a run queue. It is assigned an M.\n+\t_Gsyscall // 3\n+\n+\t// _Gwaiting means this goroutine is blocked in the runtime.\n+\t// It is not executing user code. It is not on a run queue,\n+\t// but should be recorded somewhere (e.g., a channel wait\n+\t// queue) so it can be ready()d when necessary. The stack is\n+\t// not owned *except* that a channel operation may read or\n+\t// write parts of the stack under the appropriate channel\n+\t// lock. Otherwise, it is not safe to access the stack after a\n+\t// goroutine enters _Gwaiting (e.g., it may get moved).\n+\t_Gwaiting // 4\n+\n+\t// _Gmoribund_unused is currently unused, but hardcoded in gdb\n+\t// scripts.\n+\t_Gmoribund_unused // 5\n+\n+\t// _Gdead means this goroutine is currently unused. It may be\n+\t// just exited, on a free list, or just being initialized. It\n+\t// is not executing user code. It may or may not have a stack\n+\t// allocated. The G and its stack (if any) are owned by the M\n+\t// that is exiting the G or that obtained the G from the free\n+\t// list.\n+\t_Gdead // 6\n+\n+\t// _Genqueue_unused is currently unused.\n+\t_Genqueue_unused // 7\n+\n+\t// _Gcopystack means this goroutine's stack is being moved. It\n+\t// is not executing user code and is not on a run queue. The\n+\t// stack is owned by the goroutine that put it in _Gcopystack.\n+\t_Gcopystack // 8\n+\n+\t// _Gscan combined with one of the above states other than\n+\t// _Grunning indicates that GC is scanning the stack. The\n+\t// goroutine is not executing user code and the stack is owned\n+\t// by the goroutine that set the _Gscan bit.\n+\t//\n+\t// _Gscanrunning is different: it is used to briefly block\n+\t// state transitions while GC signals the G to scan its own\n+\t// stack. This is otherwise like _Grunning.\n+\t//\n+\t// atomicstatus&~Gscan gives the state the goroutine will\n+\t// return to when the scan completes.\n+\t_Gscan         = 0x1000\n+\t_Gscanrunnable = _Gscan + _Grunnable // 0x1001\n+\t_Gscanrunning  = _Gscan + _Grunning  // 0x1002\n+\t_Gscansyscall  = _Gscan + _Gsyscall  // 0x1003\n+\t_Gscanwaiting  = _Gscan + _Gwaiting  // 0x1004\n+)\n+\n+const (\n+\t// P status\n+\t_Pidle    = iota\n+\t_Prunning // Only this P is allowed to change from _Prunning.\n+\t_Psyscall\n+\t_Pgcstop\n+\t_Pdead\n+)\n+\n+// Mutual exclusion locks.  In the uncontended case,\n+// as fast as spin locks (just a few user-level instructions),\n+// but on the contention path they sleep in the kernel.\n+// A zeroed Mutex is unlocked (no need to initialize each lock).\n+type mutex struct {\n+\t// Futex-based impl treats it as uint32 key,\n+\t// while sema-based impl as M* waitm.\n+\t// Used to be a union, but unions break precise GC.\n+\tkey uintptr\n+}\n+\n+// sleep and wakeup on one-time events.\n+// before any calls to notesleep or notewakeup,\n+// must call noteclear to initialize the Note.\n+// then, exactly one thread can call notesleep\n+// and exactly one thread can call notewakeup (once).\n+// once notewakeup has been called, the notesleep\n+// will return.  future notesleep will return immediately.\n+// subsequent noteclear must be called only after\n+// previous notesleep has returned, e.g. it's disallowed\n+// to call noteclear straight after notewakeup.\n+//\n+// notetsleep is like notesleep but wakes up after\n+// a given number of nanoseconds even if the event\n+// has not yet happened.  if a goroutine uses notetsleep to\n+// wake up early, it must wait to call noteclear until it\n+// can be sure that no other goroutine is calling\n+// notewakeup.\n+//\n+// notesleep/notetsleep are generally called on g0,\n+// notetsleepg is similar to notetsleep but is called on user g.\n+type note struct {\n+\t// Futex-based impl treats it as uint32 key,\n+\t// while sema-based impl as M* waitm.\n+\t// Used to be a union, but unions break precise GC.\n+\tkey uintptr\n+}\n+\n+type funcval struct {\n+\tfn uintptr\n+\t// variable-size, fn-specific data here\n+}\n+\n+type iface struct {\n+\ttab  unsafe.Pointer\n+\tdata unsafe.Pointer\n+}\n+\n+type eface struct {\n+\t_type *_type\n+\tdata  unsafe.Pointer\n+}\n+\n+func efaceOf(ep *interface{}) *eface {\n+\treturn (*eface)(unsafe.Pointer(ep))\n+}\n+\n+// The guintptr, muintptr, and puintptr are all used to bypass write barriers.\n+// It is particularly important to avoid write barriers when the current P has\n+// been released, because the GC thinks the world is stopped, and an\n+// unexpected write barrier would not be synchronized with the GC,\n+// which can lead to a half-executed write barrier that has marked the object\n+// but not queued it. If the GC skips the object and completes before the\n+// queuing can occur, it will incorrectly free the object.\n+//\n+// We tried using special assignment functions invoked only when not\n+// holding a running P, but then some updates to a particular memory\n+// word went through write barriers and some did not. This breaks the\n+// write barrier shadow checking mode, and it is also scary: better to have\n+// a word that is completely ignored by the GC than to have one for which\n+// only a few updates are ignored.\n+//\n+// Gs, Ms, and Ps are always reachable via true pointers in the\n+// allgs, allm, and allp lists or (during allocation before they reach those lists)\n+// from stack variables.\n+\n+// A guintptr holds a goroutine pointer, but typed as a uintptr\n+// to bypass write barriers. It is used in the Gobuf goroutine state\n+// and in scheduling lists that are manipulated without a P.\n+//\n+// The Gobuf.g goroutine pointer is almost always updated by assembly code.\n+// In one of the few places it is updated by Go code - func save - it must be\n+// treated as a uintptr to avoid a write barrier being emitted at a bad time.\n+// Instead of figuring out how to emit the write barriers missing in the\n+// assembly manipulation, we change the type of the field to uintptr,\n+// so that it does not require write barriers at all.\n+//\n+// Goroutine structs are published in the allg list and never freed.\n+// That will keep the goroutine structs from being collected.\n+// There is never a time that Gobuf.g's contain the only references\n+// to a goroutine: the publishing of the goroutine in allg comes first.\n+// Goroutine pointers are also kept in non-GC-visible places like TLS,\n+// so I can't see them ever moving. If we did want to start moving data\n+// in the GC, we'd need to allocate the goroutine structs from an\n+// alternate arena. Using guintptr doesn't make that problem any worse.\n+type guintptr uintptr\n+\n+//go:nosplit\n+func (gp guintptr) ptr() *g { return (*g)(unsafe.Pointer(gp)) }\n+\n+//go:nosplit\n+func (gp *guintptr) set(g *g) { *gp = guintptr(unsafe.Pointer(g)) }\n+\n+/*\n+//go:nosplit\n+func (gp *guintptr) cas(old, new guintptr) bool {\n+\treturn atomic.Casuintptr((*uintptr)(unsafe.Pointer(gp)), uintptr(old), uintptr(new))\n+}\n+*/\n+\n+type puintptr uintptr\n+\n+//go:nosplit\n+func (pp puintptr) ptr() *p { return (*p)(unsafe.Pointer(pp)) }\n+\n+//go:nosplit\n+func (pp *puintptr) set(p *p) { *pp = puintptr(unsafe.Pointer(p)) }\n+\n+type muintptr uintptr\n+\n+//go:nosplit\n+func (mp muintptr) ptr() *m { return (*m)(unsafe.Pointer(mp)) }\n+\n+//go:nosplit\n+func (mp *muintptr) set(m *m) { *mp = muintptr(unsafe.Pointer(m)) }\n+\n+// sudog represents a g in a wait list, such as for sending/receiving\n+// on a channel.\n+//\n+// sudog is necessary because the g \u2194 synchronization object relation\n+// is many-to-many. A g can be on many wait lists, so there may be\n+// many sudogs for one g; and many gs may be waiting on the same\n+// synchronization object, so there may be many sudogs for one object.\n+//\n+// sudogs are allocated from a special pool. Use acquireSudog and\n+// releaseSudog to allocate and free them.\n+/*\n+Commented out for gccgo for now.\n+\n+type sudog struct {\n+\t// The following fields are protected by the hchan.lock of the\n+\t// channel this sudog is blocking on. shrinkstack depends on\n+\t// this.\n+\n+\tg          *g\n+\tselectdone *uint32 // CAS to 1 to win select race (may point to stack)\n+\tnext       *sudog\n+\tprev       *sudog\n+\telem       unsafe.Pointer // data element (may point to stack)\n+\n+\t// The following fields are never accessed concurrently.\n+\t// waitlink is only accessed by g.\n+\n+\treleasetime int64\n+\tticket      uint32\n+\twaitlink    *sudog // g.waiting list\n+\tc           *hchan // channel\n+}\n+*/\n+\n+type gcstats struct {\n+\t// the struct must consist of only uint64's,\n+\t// because it is casted to uint64[].\n+\tnhandoff    uint64\n+\tnhandoffcnt uint64\n+\tnprocyield  uint64\n+\tnosyield    uint64\n+\tnsleep      uint64\n+}\n+\n+/*\n+Not used by gccgo.\n+\n+type libcall struct {\n+\tfn   uintptr\n+\tn    uintptr // number of parameters\n+\targs uintptr // parameters\n+\tr1   uintptr // return values\n+\tr2   uintptr\n+\terr  uintptr // error number\n+}\n+\n+*/\n+\n+/*\n+Not used by gccgo.\n+\n+// describes how to handle callback\n+type wincallbackcontext struct {\n+\tgobody       unsafe.Pointer // go function to call\n+\targsize      uintptr        // callback arguments size (in bytes)\n+\trestorestack uintptr        // adjust stack on return by (in bytes) (386 only)\n+\tcleanstack   bool\n+}\n+*/\n+\n+/*\n+Not used by gccgo.\n+\n+// Stack describes a Go execution stack.\n+// The bounds of the stack are exactly [lo, hi),\n+// with no implicit data structures on either side.\n+type stack struct {\n+\tlo uintptr\n+\thi uintptr\n+}\n+\n+// stkbar records the state of a G's stack barrier.\n+type stkbar struct {\n+\tsavedLRPtr uintptr // location overwritten by stack barrier PC\n+\tsavedLRVal uintptr // value overwritten at savedLRPtr\n+}\n+*/\n+\n+type g struct {\n+\t// Stack parameters.\n+\t// stack describes the actual stack memory: [stack.lo, stack.hi).\n+\t// stackguard0 is the stack pointer compared in the Go stack growth prologue.\n+\t// It is stack.lo+StackGuard normally, but can be StackPreempt to trigger a preemption.\n+\t// stackguard1 is the stack pointer compared in the C stack growth prologue.\n+\t// It is stack.lo+StackGuard on g0 and gsignal stacks.\n+\t// It is ~0 on other goroutine stacks, to trigger a call to morestackc (and crash).\n+\t// Not for gccgo: stack       stack   // offset known to runtime/cgo\n+\t// Not for gccgo: stackguard0 uintptr // offset known to liblink\n+\t// Not for gccgo: stackguard1 uintptr // offset known to liblink\n+\n+\t_panic *_panic // innermost panic - offset known to liblink\n+\t_defer *_defer // innermost defer\n+\tm      *m      // current m; offset known to arm liblink\n+\t// Not for gccgo: stackAlloc     uintptr // stack allocation is [stack.lo,stack.lo+stackAlloc)\n+\t// Not for gccgo: sched          gobuf\n+\t// Not for gccgo: syscallsp      uintptr        // if status==Gsyscall, syscallsp = sched.sp to use during gc\n+\t// Not for gccgo: syscallpc      uintptr        // if status==Gsyscall, syscallpc = sched.pc to use during gc\n+\t// Not for gccgo: stkbar         []stkbar       // stack barriers, from low to high (see top of mstkbar.go)\n+\t// Not for gccgo: stkbarPos      uintptr        // index of lowest stack barrier not hit\n+\t// Not for gccgo: stktopsp       uintptr        // expected sp at top of stack, to check in traceback\n+\tparam        unsafe.Pointer // passed parameter on wakeup\n+\tatomicstatus uint32\n+\t// Not for gccgo: stackLock      uint32 // sigprof/scang lock; TODO: fold in to atomicstatus\n+\tgoid           int64\n+\twaitsince      int64  // approx time when the g become blocked\n+\twaitreason     string // if status==Gwaiting\n+\tschedlink      guintptr\n+\tpreempt        bool     // preemption signal, duplicates stackguard0 = stackpreempt\n+\tpaniconfault   bool     // panic (instead of crash) on unexpected fault address\n+\tpreemptscan    bool     // preempted g does scan for gc\n+\tgcscandone     bool     // g has scanned stack; protected by _Gscan bit in status\n+\tgcscanvalid    bool     // false at start of gc cycle, true if G has not run since last scan; transition from true to false by calling queueRescan and false to true by calling dequeueRescan\n+\tthrowsplit     bool     // must not split stack\n+\traceignore     int8     // ignore race detection events\n+\tsysblocktraced bool     // StartTrace has emitted EvGoInSyscall about this goroutine\n+\tsysexitticks   int64    // cputicks when syscall has returned (for tracing)\n+\ttraceseq       uint64   // trace event sequencer\n+\ttracelastp     puintptr // last P emitted an event for this goroutine\n+\tlockedm        *m\n+\tsig            uint32\n+\n+\t// Temporary gccgo field.\n+\twritenbuf int32\n+\t// Not for gccgo yet: writebuf       []byte\n+\t// Temporary different type for gccgo.\n+\twritebuf *byte\n+\n+\tsigcode0 uintptr\n+\tsigcode1 uintptr\n+\tsigpc    uintptr\n+\tgopc     uintptr // pc of go statement that created this goroutine\n+\tstartpc  uintptr // pc of goroutine function\n+\tracectx  uintptr\n+\t// Not for gccgo for now: waiting        *sudog    // sudog structures this g is waiting on (that have a valid elem ptr); in lock order\n+\t// Not for gccgo: cgoCtxt        []uintptr // cgo traceback context\n+\n+\t// Per-G GC state\n+\n+\t// gcRescan is this G's index in work.rescan.list. If this is\n+\t// -1, this G is not on the rescan list.\n+\t//\n+\t// If gcphase != _GCoff and this G is visible to the garbage\n+\t// collector, writes to this are protected by work.rescan.lock.\n+\tgcRescan int32\n+\n+\t// gcAssistBytes is this G's GC assist credit in terms of\n+\t// bytes allocated. If this is positive, then the G has credit\n+\t// to allocate gcAssistBytes bytes without assisting. If this\n+\t// is negative, then the G must correct this by performing\n+\t// scan work. We track this in bytes to make it fast to update\n+\t// and check for debt in the malloc hot path. The assist ratio\n+\t// determines how this corresponds to scan work debt.\n+\tgcAssistBytes int64\n+\n+\t// Remaining fields are specific to gccgo.\n+\n+\texception unsafe.Pointer // current exception being thrown\n+\tisforeign bool           // whether current exception is not from Go\n+\n+\t// Fields that hold stack and context information if status is Gsyscall\n+\tgcstack       unsafe.Pointer\n+\tgcstacksize   uintptr\n+\tgcnextsegment unsafe.Pointer\n+\tgcnextsp      unsafe.Pointer\n+\tgcinitialsp   unsafe.Pointer\n+\tgcregs        _ucontext_t\n+\n+\tentry    unsafe.Pointer // goroutine entry point\n+\tfromgogo bool           // whether entered from gogo function\n+\n+\tissystem     bool // do not output in stack dump\n+\tisbackground bool // ignore in deadlock detector\n+\n+\ttraceback *traceback // stack traceback buffer\n+\n+\tcontext      _ucontext_t        // saved context for setcontext\n+\tstackcontext [10]unsafe.Pointer // split-stack context\n+}\n+\n+type m struct {\n+\tg0 *g // goroutine with scheduling stack\n+\t// Not for gccgo: morebuf gobuf  // gobuf arg to morestack\n+\t// Not for gccgo: divmod  uint32 // div/mod denominator for arm - known to liblink\n+\n+\t// Fields not known to debuggers.\n+\tprocid  uint64 // for debuggers, but offset not hard-coded\n+\tgsignal *g     // signal-handling g\n+\tsigmask sigset // storage for saved signal mask\n+\t// Not for gccgo: tls           [6]uintptr // thread-local storage (for x86 extern register)\n+\tmstartfn    uintptr\n+\tcurg        *g       // current running goroutine\n+\tcaughtsig   guintptr // goroutine running during fatal signal\n+\tp           puintptr // attached p for executing go code (nil if not executing go code)\n+\tnextp       puintptr\n+\tid          int32\n+\tmallocing   int32\n+\tthrowing    int32\n+\tpreemptoff  string // if != \"\", keep curg running on this m\n+\tlocks       int32\n+\tsoftfloat   int32\n+\tdying       int32\n+\tprofilehz   int32\n+\thelpgc      int32\n+\tspinning    bool // m is out of work and is actively looking for work\n+\tblocked     bool // m is blocked on a note\n+\tinwb        bool // m is executing a write barrier\n+\tnewSigstack bool // minit on C thread called sigaltstack\n+\tprintlock   int8\n+\tfastrand    uint32\n+\tncgocall    uint64 // number of cgo calls in total\n+\tncgo        int32  // number of cgo calls currently in progress\n+\t// Not for gccgo: cgoCallersUse uint32      // if non-zero, cgoCallers in use temporarily\n+\t// Not for gccgo: cgoCallers    *cgoCallers // cgo traceback if crashing in cgo call\n+\tpark        note\n+\talllink     *m // on allm\n+\tschedlink   muintptr\n+\tmcache      *mcache\n+\tlockedg     *g\n+\tcreatestack [32]location // stack that created this thread.\n+\t// Not for gccgo: freglo        [16]uint32  // d[i] lsb and f[i]\n+\t// Not for gccgo: freghi        [16]uint32  // d[i] msb and f[i+16]\n+\t// Not for gccgo: fflag         uint32      // floating point compare flags\n+\tlocked        uint32  // tracking for lockosthread\n+\tnextwaitm     uintptr // next m waiting for lock\n+\tgcstats       gcstats\n+\tneedextram    bool\n+\ttraceback     uint8\n+\twaitunlockf   unsafe.Pointer // todo go func(*g, unsafe.pointer) bool\n+\twaitlock      unsafe.Pointer\n+\twaittraceev   byte\n+\twaittraceskip int\n+\tstartingtrace bool\n+\tsyscalltick   uint32\n+\t// Not for gccgo: thread        uintptr // thread handle\n+\n+\t// these are here because they are too large to be on the stack\n+\t// of low-level NOSPLIT functions.\n+\t// Not for gccgo: libcall   libcall\n+\t// Not for gccgo: libcallpc uintptr // for cpu profiler\n+\t// Not for gccgo: libcallsp uintptr\n+\t// Not for gccgo: libcallg  guintptr\n+\t// Not for gccgo: syscall   libcall // stores syscall parameters on windows\n+\n+\t// Not for gccgo: mOS\n+\n+\t// Remaining fields are specific to gccgo.\n+\n+\tgsignalstack     unsafe.Pointer // stack for gsignal\n+\tgsignalstacksize uintptr\n+\n+\tdropextram bool // drop after call is done\n+\n+\tgcing int32\n+\n+\twaitsema uintptr // semaphore on systems that don't use futexes\n+\n+\tcgomal *cgoMal // allocations via _cgo_allocate\n+}\n+\n+type p struct {\n+\tlock mutex\n+\n+\tid          int32\n+\tstatus      uint32 // one of pidle/prunning/...\n+\tlink        puintptr\n+\tschedtick   uint32   // incremented on every scheduler call\n+\tsyscalltick uint32   // incremented on every system call\n+\tm           muintptr // back-link to associated m (nil if idle)\n+\tmcache      *mcache\n+\t// Not for gccgo: racectx     uintptr\n+\n+\t// Not for gccgo yet: deferpool    [5][]*_defer // pool of available defer structs of different sizes (see panic.go)\n+\t// Not for gccgo yet: deferpoolbuf [5][32]*_defer\n+\t// Temporary gccgo type for deferpool field.\n+\tdeferpool *_defer\n+\n+\t// Cache of goroutine ids, amortizes accesses to runtime\u00b7sched.goidgen.\n+\tgoidcache    uint64\n+\tgoidcacheend uint64\n+\n+\t// Queue of runnable goroutines. Accessed without lock.\n+\trunqhead uint32\n+\trunqtail uint32\n+\trunq     [256]guintptr\n+\t// runnext, if non-nil, is a runnable G that was ready'd by\n+\t// the current G and should be run next instead of what's in\n+\t// runq if there's time remaining in the running G's time\n+\t// slice. It will inherit the time left in the current time\n+\t// slice. If a set of goroutines is locked in a\n+\t// communicate-and-wait pattern, this schedules that set as a\n+\t// unit and eliminates the (potentially large) scheduling\n+\t// latency that otherwise arises from adding the ready'd\n+\t// goroutines to the end of the run queue.\n+\trunnext guintptr\n+\n+\t// Available G's (status == Gdead)\n+\tgfree    *g\n+\tgfreecnt int32\n+\n+\t// Not for gccgo for now: sudogcache []*sudog\n+\t// Not for gccgo for now: sudogbuf   [128]*sudog\n+\n+\t// Not for gccgo for now: tracebuf traceBufPtr\n+\n+\t// Not for gccgo for now: palloc persistentAlloc // per-P to avoid mutex\n+\n+\t// Per-P GC state\n+\t// Not for gccgo for now: gcAssistTime     int64 // Nanoseconds in assistAlloc\n+\t// Not for gccgo for now: gcBgMarkWorker   guintptr\n+\t// Not for gccgo for now: gcMarkWorkerMode gcMarkWorkerMode\n+\n+\t// gcw is this P's GC work buffer cache. The work buffer is\n+\t// filled by write barriers, drained by mutator assists, and\n+\t// disposed on certain GC state transitions.\n+\t// Not for gccgo for now: gcw gcWork\n+\n+\trunSafePointFn uint32 // if 1, run sched.safePointFn at next safe point\n+\n+\tpad [64]byte\n+}\n+\n+const (\n+\t// The max value of GOMAXPROCS.\n+\t// There are no fundamental restrictions on the value.\n+\t_MaxGomaxprocs = 1 << 8\n+)\n+\n+/*\n+Commented out for gccgo for now.\n+\n+type schedt struct {\n+\t// accessed atomically. keep at top to ensure alignment on 32-bit systems.\n+\tgoidgen  uint64\n+\tlastpoll uint64\n+\n+\tlock mutex\n+\n+\tmidle        muintptr // idle m's waiting for work\n+\tnmidle       int32    // number of idle m's waiting for work\n+\tnmidlelocked int32    // number of locked m's waiting for work\n+\tmcount       int32    // number of m's that have been created\n+\tmaxmcount    int32    // maximum number of m's allowed (or die)\n+\n+\tngsys uint32 // number of system goroutines; updated atomically\n+\n+\tpidle      puintptr // idle p's\n+\tnpidle     uint32\n+\tnmspinning uint32 // See \"Worker thread parking/unparking\" comment in proc.go.\n+\n+\t// Global runnable queue.\n+\trunqhead guintptr\n+\trunqtail guintptr\n+\trunqsize int32\n+\n+\t// Global cache of dead G's.\n+\tgflock       mutex\n+\tgfreeStack   *g\n+\tgfreeNoStack *g\n+\tngfree       int32\n+\n+\t// Central cache of sudog structs.\n+\tsudoglock  mutex\n+\tsudogcache *sudog\n+\n+\t// Central pool of available defer structs of different sizes.\n+\tdeferlock mutex\n+\tdeferpool [5]*_defer\n+\n+\tgcwaiting  uint32 // gc is waiting to run\n+\tstopwait   int32\n+\tstopnote   note\n+\tsysmonwait uint32\n+\tsysmonnote note\n+\n+\t// safepointFn should be called on each P at the next GC\n+\t// safepoint if p.runSafePointFn is set.\n+\tsafePointFn   func(*p)\n+\tsafePointWait int32\n+\tsafePointNote note\n+\n+\tprofilehz int32 // cpu profiling rate\n+\n+\tprocresizetime int64 // nanotime() of last change to gomaxprocs\n+\ttotaltime      int64 // \u222bgomaxprocs dt up to procresizetime\n+}\n+*/\n+\n+// The m.locked word holds two pieces of state counting active calls to LockOSThread/lockOSThread.\n+// The low bit (LockExternal) is a boolean reporting whether any LockOSThread call is active.\n+// External locks are not recursive; a second lock is silently ignored.\n+// The upper bits of m.locked record the nesting depth of calls to lockOSThread\n+// (counting up by LockInternal), popped by unlockOSThread (counting down by LockInternal).\n+// Internal locks can be recursive. For instance, a lock for cgo can occur while the main\n+// goroutine is holding the lock during the initialization phase.\n+const (\n+\t_LockExternal = 1\n+\t_LockInternal = 2\n+)\n+\n+const (\n+\t_SigNotify   = 1 << iota // let signal.Notify have signal, even if from kernel\n+\t_SigKill                 // if signal.Notify doesn't take it, exit quietly\n+\t_SigThrow                // if signal.Notify doesn't take it, exit loudly\n+\t_SigPanic                // if the signal is from the kernel, panic\n+\t_SigDefault              // if the signal isn't explicitly requested, don't monitor it\n+\t_SigHandling             // our signal handler is registered\n+\t_SigGoExit               // cause all runtime procs to exit (only used on Plan 9).\n+\t_SigSetStack             // add SA_ONSTACK to libc handler\n+\t_SigUnblock              // unblocked in minit\n+)\n+\n+/*\n+gccgo does not use this.\n+\n+// Layout of in-memory per-function information prepared by linker\n+// See https://golang.org/s/go12symtab.\n+// Keep in sync with linker\n+// and with package debug/gosym and with symtab.go in package runtime.\n+type _func struct {\n+\tentry   uintptr // start pc\n+\tnameoff int32   // function name\n+\n+\targs int32 // in/out args size\n+\t_    int32 // previously legacy frame size; kept for layout compatibility\n+\n+\tpcsp      int32\n+\tpcfile    int32\n+\tpcln      int32\n+\tnpcdata   int32\n+\tnfuncdata int32\n+}\n+\n+*/\n+\n+// Lock-free stack node.\n+// // Also known to export_test.go.\n+type lfnode struct {\n+\tnext    uint64\n+\tpushcnt uintptr\n+}\n+\n+type forcegcstate struct {\n+\tlock mutex\n+\tg    *g\n+\tidle uint32\n+}\n+\n+/*\n+// startup_random_data holds random bytes initialized at startup. These come from\n+// the ELF AT_RANDOM auxiliary vector (vdso_linux_amd64.go or os_linux_386.go).\n+var startupRandomData []byte\n+\n+// extendRandom extends the random numbers in r[:n] to the whole slice r.\n+// Treats n<0 as n==0.\n+func extendRandom(r []byte, n int) {\n+\tif n < 0 {\n+\t\tn = 0\n+\t}\n+\tfor n < len(r) {\n+\t\t// Extend random bits using hash function & time seed\n+\t\tw := n\n+\t\tif w > 16 {\n+\t\t\tw = 16\n+\t\t}\n+\t\th := memhash(unsafe.Pointer(&r[n-w]), uintptr(nanotime()), uintptr(w))\n+\t\tfor i := 0; i < sys.PtrSize && n < len(r); i++ {\n+\t\t\tr[n] = byte(h)\n+\t\t\tn++\n+\t\t\th >>= 8\n+\t\t}\n+\t}\n+}\n+*/\n+\n+// deferred subroutine calls\n+// This is the gccgo version.\n+type _defer struct {\n+\t// The next entry in the stack.\n+\tnext *_defer\n+\n+\t// The stack variable for the function which called this defer\n+\t// statement.  This is set to true if we are returning from\n+\t// that function, false if we are panicing through it.\n+\tframe *bool\n+\n+\t// The value of the panic stack when this function is\n+\t// deferred.  This function can not recover this value from\n+\t// the panic stack.  This can happen if a deferred function\n+\t// has a defer statement itself.\n+\t_panic *_panic\n+\n+\t// The function to call.\n+\tpfn uintptr\n+\n+\t// The argument to pass to the function.\n+\targ unsafe.Pointer\n+\n+\t// The return address that a recover thunk matches against.\n+\t// This is set by __go_set_defer_retaddr which is called by\n+\t// the thunks created by defer statements.\n+\tretaddr uintptr\n+\n+\t// Set to true if a function created by reflect.MakeFunc is\n+\t// permitted to recover.  The return address of such a\n+\t// function function will be somewhere in libffi, so __retaddr\n+\t// is not useful.\n+\tmakefunccanrecover bool\n+\n+\t// Set to true if this defer stack entry is not part of the\n+\t// defer pool.\n+\tspecial bool\n+}\n+\n+// panics\n+// This is the gccgo version.\n+type _panic struct {\n+\t// The next entry in the stack.\n+\tnext *_panic\n+\n+\t// The value associated with this panic.\n+\targ interface{}\n+\n+\t// Whether this panic has been recovered.\n+\trecovered bool\n+\n+\t// Whether this panic was pushed on the stack because of an\n+\t// exception thrown in some other language.\n+\tisforeign bool\n+}\n+\n+const (\n+\t_TraceRuntimeFrames = 1 << iota // include frames for internal runtime functions.\n+\t_TraceTrap                      // the initial PC, SP are from a trap, not a return PC from a call\n+\t_TraceJumpStack                 // if traceback is on a systemstack, resume trace at g that called into it\n+)\n+\n+// The maximum number of frames we print for a traceback\n+const _TracebackMaxFrames = 100\n+\n+var (\n+//\temptystring string\n+//\tallglen     uintptr\n+//\tallm        *m\n+//\tallp        [_MaxGomaxprocs + 1]*p\n+//\tgomaxprocs  int32\n+//\tpanicking   uint32\n+//\tncpu        int32\n+//\tforcegc     forcegcstate\n+//\tsched       schedt\n+//\tnewprocs    int32\n+\n+// Information about what cpu features are available.\n+// Set on startup in asm_{x86,amd64}.s.\n+//\tcpuid_ecx         uint32\n+//\tcpuid_edx         uint32\n+//\tcpuid_ebx7        uint32\n+//\tlfenceBeforeRdtsc bool\n+//\tsupport_avx       bool\n+//\tsupport_avx2      bool\n+\n+//\tgoarm                uint8 // set by cmd/link on arm systems\n+//\tframepointer_enabled bool  // set by cmd/link\n+)\n+\n+// Set by the linker so the runtime can determine the buildmode.\n+var (\n+//\tislibrary bool // -buildmode=c-shared\n+//\tisarchive bool // -buildmode=c-archive\n+)\n+\n+// Types that are only used by gccgo.\n+\n+// _ucontext_t is a Go version of the C ucontext_t type, used by getcontext.\n+// _sizeof_ucontext_t is defined by the Makefile from <ucontext.h>.\n+type _ucontext_t [_sizeof_ucontext_t / unsafe.Sizeof(uintptr(0))]unsafe.Pointer\n+\n+// traceback is used to collect stack traces from other goroutines.\n+type traceback struct {\n+\tgp     *g\n+\tlocbuf [_TracebackMaxFrames]location\n+\tc      int\n+}\n+\n+// location is a location in the program, used for backtraces.\n+type location struct {\n+\tpc       uintptr\n+\tfilename string\n+\tfunction string\n+\tlineno   int\n+}\n+\n+// cgoMal tracks allocations made by _cgo_allocate\n+// FIXME: _cgo_allocate has been removed from gc and can probably be\n+// removed from gccgo too.\n+type cgoMal struct {\n+\tnext  *cgoMal\n+\talloc unsafe.Pointer\n+}\n+\n+// sigset is the Go version of the C type sigset_t.\n+// _sigset_t is defined by the Makefile from <signal.h>.\n+type sigset _sigset_t"}, {"sha": "fb5f034dd68d49e7a8e4e301f9d97705b86a3432", "filename": "libgo/go/runtime/type.go", "status": "added", "additions": 100, "deletions": 0, "changes": 100, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fgo%2Fruntime%2Ftype.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fgo%2Fruntime%2Ftype.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ftype.go?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -0,0 +1,100 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Runtime type representation.\n+\n+package runtime\n+\n+import \"unsafe\"\n+\n+type _type struct {\n+\tkind       uint8\n+\talign      int8\n+\tfieldAlign uint8\n+\t_          uint8\n+\tsize       uintptr\n+\thash       uint32\n+\n+\thashfn  func(unsafe.Pointer, uintptr) uintptr\n+\tequalfn func(unsafe.Pointer, unsafe.Pointer, uintptr) bool\n+\n+\tgc     unsafe.Pointer\n+\tstring *string\n+\t*uncommonType\n+\tptrToThis *_type\n+}\n+\n+type method struct {\n+\tname    *string\n+\tpkgPath *string\n+\tmtyp    *_type\n+\ttyp     *_type\n+\ttfn     unsafe.Pointer\n+}\n+\n+type uncommonType struct {\n+\tname    *string\n+\tpkgPath *string\n+\tmethods []method\n+}\n+\n+type imethod struct {\n+\tname    *string\n+\tpkgPath *string\n+\ttyp     *_type\n+}\n+\n+type interfaceType struct {\n+\ttyp     _type\n+\tmethods []imethod\n+}\n+\n+type mapType struct {\n+\ttyp  _type\n+\tkey  *_type\n+\telem *_type\n+}\n+\n+type arrayType struct {\n+\ttyp   _type\n+\telem  *_type\n+\tslice *_type\n+\tlen   uintptr\n+}\n+\n+type chanType struct {\n+\ttyp  _type\n+\telem *_type\n+\tdir  uintptr\n+}\n+\n+type slicetype struct {\n+\ttyp  _type\n+\telem *_type\n+}\n+\n+type functype struct {\n+\ttyp       _type\n+\tdotdotdot bool\n+\tin        []*_type\n+\tout       []*_type\n+}\n+\n+type ptrtype struct {\n+\ttyp  _type\n+\telem *_type\n+}\n+\n+type structfield struct {\n+\tname    *string // nil for embedded fields\n+\tpkgPath *string // nil for exported Names; otherwise import path\n+\ttyp     *_type  // type of field\n+\ttag     *string // nil if no tag\n+\toffset  uintptr // byte offset of field within struct\n+}\n+\n+type structtype struct {\n+\ttyp    _type\n+\tfields []structfield\n+}"}, {"sha": "10e38036c60ba1030e8cd62749e60f299c33106f", "filename": "libgo/mksysinfo.sh", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fmksysinfo.sh", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fmksysinfo.sh", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fmksysinfo.sh?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -29,6 +29,7 @@ cat > sysinfo.c <<EOF\n #include <dirent.h>\n #include <errno.h>\n #include <fcntl.h>\n+#include <ucontext.h>\n #include <netinet/in.h>\n /* <netinet/tcp.h> needs u_char/u_short, but <sys/bsd_types> is only\n    included by <netinet/in.h> if _SGIAPI (i.e. _SGI_SOURCE"}, {"sha": "598c2617840f33c231dcb50d68cb728b77c1d6ca", "filename": "libgo/runtime/go-cgo.c", "status": "modified", "additions": 11, "deletions": 13, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-cgo.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-cgo.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-cgo.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -36,7 +36,6 @@ void\n syscall_cgocall ()\n {\n   M* m;\n-  G* g;\n \n   if (runtime_needextram && runtime_cas (&runtime_needextram, 1, 0))\n     runtime_newextram ();\n@@ -45,8 +44,7 @@ syscall_cgocall ()\n \n   m = runtime_m ();\n   ++m->ncgocall;\n-  g = runtime_g ();\n-  ++g->ncgo;\n+  ++m->ncgo;\n   runtime_entersyscall ();\n }\n \n@@ -59,18 +57,18 @@ syscall_cgocalldone ()\n \n   g = runtime_g ();\n   __go_assert (g != NULL);\n-  --g->ncgo;\n-  if (g->ncgo == 0)\n+  --g->m->ncgo;\n+  if (g->m->ncgo == 0)\n     {\n       /* We are going back to Go, and we are not in a recursive call.\n \t Let the garbage collector clean up any unreferenced\n \t memory.  */\n-      g->cgomal = NULL;\n+      g->m->cgomal = NULL;\n     }\n \n   /* If we are invoked because the C function called _cgo_panic, then\n      _cgo_panic will already have exited syscall mode.  */\n-  if (g->status == Gsyscall)\n+  if (g->atomicstatus == _Gsyscall)\n     runtime_exitsyscall ();\n \n   runtime_unlockOSThread();\n@@ -93,7 +91,7 @@ syscall_cgocallback ()\n \n   runtime_exitsyscall ();\n \n-  if (runtime_g ()->ncgo == 0)\n+  if (runtime_m ()->ncgo == 0)\n     {\n       /* The C call to Go came from a thread not currently running any\n \t Go.  In the case of -buildmode=c-archive or c-shared, this\n@@ -119,7 +117,7 @@ syscall_cgocallbackdone ()\n \n   runtime_entersyscall ();\n   mp = runtime_m ();\n-  if (mp->dropextram && runtime_g ()->ncgo == 0)\n+  if (mp->dropextram && mp->ncgo == 0)\n     {\n       mp->dropextram = false;\n       runtime_dropm ();\n@@ -133,16 +131,16 @@ void *\n alloc_saved (size_t n)\n {\n   void *ret;\n-  G *g;\n+  M *m;\n   CgoMal *c;\n \n   ret = __go_alloc (n);\n \n-  g = runtime_g ();\n+  m = runtime_m ();\n   c = (CgoMal *) __go_alloc (sizeof (CgoMal));\n-  c->next = g->cgomal;\n+  c->next = m->cgomal;\n   c->alloc = ret;\n-  g->cgomal = c;\n+  m->cgomal = c;\n \n   return ret;\n }"}, {"sha": "f3e14bd0b966e4695ee77b4aa9fab2e85dc615ef", "filename": "libgo/runtime/go-defer.c", "status": "modified", "additions": 19, "deletions": 20, "changes": 39, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-defer.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-defer.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-defer.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -9,27 +9,26 @@\n #include \"runtime.h\"\n #include \"go-alloc.h\"\n #include \"go-panic.h\"\n-#include \"go-defer.h\"\n \n /* This function is called each time we need to defer a call.  */\n \n void\n __go_defer (_Bool *frame, void (*pfn) (void *), void *arg)\n {\n   G *g;\n-  struct __go_defer_stack *n;\n+  Defer *n;\n \n   g = runtime_g ();\n   n = runtime_newdefer ();\n-  n->__next = g->defer;\n-  n->__frame = frame;\n-  n->__panic = g->panic;\n-  n->__pfn = pfn;\n-  n->__arg = arg;\n-  n->__retaddr = NULL;\n-  n->__makefunc_can_recover = 0;\n-  n->__special = 0;\n-  g->defer = n;\n+  n->next = g->_defer;\n+  n->frame = frame;\n+  n->_panic = g->_panic;\n+  n->pfn = (uintptr) pfn;\n+  n->arg = arg;\n+  n->retaddr = 0;\n+  n->makefunccanrecover = 0;\n+  n->special = 0;\n+  g->_defer = n;\n }\n \n /* This function is called when we want to undefer the stack.  */\n@@ -40,19 +39,19 @@ __go_undefer (_Bool *frame)\n   G *g;\n \n   g = runtime_g ();\n-  while (g->defer != NULL && g->defer->__frame == frame)\n+  while (g->_defer != NULL && g->_defer->frame == frame)\n     {\n-      struct __go_defer_stack *d;\n+      Defer *d;\n       void (*pfn) (void *);\n \n-      d = g->defer;\n-      pfn = d->__pfn;\n-      d->__pfn = NULL;\n+      d = g->_defer;\n+      pfn = (void (*) (void *)) d->pfn;\n+      d->pfn = 0;\n \n       if (pfn != NULL)\n-\t(*pfn) (d->__arg);\n+\t(*pfn) (d->arg);\n \n-      g->defer = d->__next;\n+      g->_defer = d->next;\n \n       /* This may be called by a cgo callback routine to defer the\n \t call to syscall.CgocallBackDone, in which case we will not\n@@ -79,7 +78,7 @@ __go_set_defer_retaddr (void *retaddr)\n   G *g;\n \n   g = runtime_g ();\n-  if (g->defer != NULL)\n-    g->defer->__retaddr = __builtin_extract_return_addr (retaddr);\n+  if (g->_defer != NULL)\n+    g->_defer->retaddr = (uintptr) __builtin_extract_return_addr (retaddr);\n   return 0;\n }"}, {"sha": "acf2d40c69c2c16564d8d6b869b99b0b6fc7873b", "filename": "libgo/runtime/go-defer.h", "status": "removed", "additions": 0, "deletions": 47, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7875b41f1d0137005d87cc5dd12b2a7df2f30c5e/libgo%2Fruntime%2Fgo-defer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7875b41f1d0137005d87cc5dd12b2a7df2f30c5e/libgo%2Fruntime%2Fgo-defer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-defer.h?ref=7875b41f1d0137005d87cc5dd12b2a7df2f30c5e", "patch": "@@ -1,47 +0,0 @@\n-/* go-defer.h -- the defer stack.\n-\n-   Copyright 2010 The Go Authors. All rights reserved.\n-   Use of this source code is governed by a BSD-style\n-   license that can be found in the LICENSE file.  */\n-\n-struct __go_panic_stack;\n-\n-/* The defer stack is a list of these structures.  */\n-\n-struct __go_defer_stack\n-{\n-  /* The next entry in the stack.  */\n-  struct __go_defer_stack *__next;\n-\n-  /* The stack variable for the function which called this defer\n-     statement.  This is set to 1 if we are returning from that\n-     function, 0 if we are panicing through it.  */\n-  _Bool *__frame;\n-\n-  /* The value of the panic stack when this function is deferred.\n-     This function can not recover this value from the panic stack.\n-     This can happen if a deferred function has a defer statement\n-     itself.  */\n-  struct __go_panic_stack *__panic;\n-\n-  /* The function to call.  */\n-  void (*__pfn) (void *);\n-\n-  /* The argument to pass to the function.  */\n-  void *__arg;\n-\n-  /* The return address that a recover thunk matches against.  This is\n-     set by __go_set_defer_retaddr which is called by the thunks\n-     created by defer statements.  */\n-  const void *__retaddr;\n-\n-  /* Set to true if a function created by reflect.MakeFunc is\n-     permitted to recover.  The return address of such a function\n-     function will be somewhere in libffi, so __retaddr is not\n-     useful.  */\n-  _Bool __makefunc_can_recover;\n-\n-  /* Set to true if this defer stack entry is not part of the defer\n-     pool.  */\n-  _Bool __special;\n-};"}, {"sha": "408ef2a75f5d8bd386aa37a7d7df8eeb4e4891bb", "filename": "libgo/runtime/go-deferred-recover.c", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-deferred-recover.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-deferred-recover.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-deferred-recover.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -8,7 +8,6 @@\n \n #include \"runtime.h\"\n #include \"go-panic.h\"\n-#include \"go-defer.h\"\n \n /* This is called when a call to recover is deferred.  That is,\n    something like\n@@ -82,7 +81,7 @@ __go_deferred_recover ()\n   G *g;\n \n   g = runtime_g ();\n-  if (g->defer == NULL || g->defer->__panic != g->panic)\n+  if (g->_defer == NULL || g->_defer->_panic != g->_panic)\n     {\n       struct __go_empty_interface ret;\n "}, {"sha": "436a96626b8900b0afa6ff78d68bcd73eb87e273", "filename": "libgo/runtime/go-panic.c", "status": "modified", "additions": 20, "deletions": 21, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-panic.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-panic.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-panic.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -11,23 +11,22 @@\n #include \"arch.h\"\n #include \"malloc.h\"\n #include \"go-alloc.h\"\n-#include \"go-defer.h\"\n #include \"go-panic.h\"\n #include \"interface.h\"\n \n /* Print the panic stack.  This is used when there is no recover.  */\n \n static void\n-__printpanics (struct __go_panic_stack *p)\n+__printpanics (Panic *p)\n {\n-  if (p->__next != NULL)\n+  if (p->next != NULL)\n     {\n-      __printpanics (p->__next);\n+      __printpanics (p->next);\n       runtime_printf (\"\\t\");\n     }\n   runtime_printf (\"panic: \");\n-  runtime_printany (p->__arg);\n-  if (p->__was_recovered)\n+  runtime_printany (p->arg);\n+  if (p->recovered)\n     runtime_printf (\" [recovered]\");\n   runtime_printf (\"\\n\");\n }\n@@ -39,39 +38,39 @@ void\n __go_panic (struct __go_empty_interface arg)\n {\n   G *g;\n-  struct __go_panic_stack *n;\n+  Panic *n;\n \n   g = runtime_g ();\n \n-  n = (struct __go_panic_stack *) __go_alloc (sizeof (struct __go_panic_stack));\n-  n->__arg = arg;\n-  n->__next = g->panic;\n-  g->panic = n;\n+  n = (Panic *) __go_alloc (sizeof (Panic));\n+  n->arg = arg;\n+  n->next = g->_panic;\n+  g->_panic = n;\n \n   /* Run all the defer functions.  */\n \n   while (1)\n     {\n-      struct __go_defer_stack *d;\n+      Defer *d;\n       void (*pfn) (void *);\n \n-      d = g->defer;\n+      d = g->_defer;\n       if (d == NULL)\n \tbreak;\n \n-      pfn = d->__pfn;\n-      d->__pfn = NULL;\n+      pfn = (void (*) (void *)) d->pfn;\n+      d->pfn = 0;\n \n       if (pfn != NULL)\n \t{\n-\t  (*pfn) (d->__arg);\n+\t  (*pfn) (d->arg);\n \n-\t  if (n->__was_recovered)\n+\t  if (n->recovered)\n \t    {\n \t      /* Some defer function called recover.  That means that\n \t\t we should stop running this panic.  */\n \n-\t      g->panic = n->__next;\n+\t      g->_panic = n->next;\n \t      __go_free (n);\n \n \t      /* Now unwind the stack by throwing an exception.  The\n@@ -91,10 +90,10 @@ __go_panic (struct __go_empty_interface arg)\n \t     it did not call recover, we know that we are not\n \t     returning from the calling function--we are panicing\n \t     through it.  */\n-\t  *d->__frame = 0;\n+\t  *d->frame = 0;\n \t}\n \n-      g->defer = d->__next;\n+      g->_defer = d->next;\n \n       /* This may be called by a cgo callback routine to defer the\n \t call to syscall.CgocallBackDone, in which case we will not\n@@ -107,6 +106,6 @@ __go_panic (struct __go_empty_interface arg)\n   /* The panic was not recovered.  */\n \n   runtime_startpanic ();\n-  __printpanics (g->panic);\n+  __printpanics (g->_panic);\n   runtime_dopanic (0);\n }"}, {"sha": "1b172d92db9cf2910d2c1cf0cf68f8b1ca86e113", "filename": "libgo/runtime/go-panic.h", "status": "modified", "additions": 2, "deletions": 21, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-panic.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-panic.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-panic.h?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -11,25 +11,6 @@\n \n struct String;\n struct __go_type_descriptor;\n-struct __go_defer_stack;\n-\n-/* The stack of panic calls.  */\n-\n-struct __go_panic_stack\n-{\n-  /* The next entry in the stack.  */\n-  struct __go_panic_stack *__next;\n-\n-  /* The value associated with this panic.  */\n-  struct __go_empty_interface __arg;\n-\n-  /* Whether this panic has been recovered.  */\n-  _Bool __was_recovered;\n-\n-  /* Whether this panic was pushed on the stack because of an\n-     exception thrown in some other language.  */\n-  _Bool __is_foreign;\n-};\n \n extern void __go_panic (struct __go_empty_interface)\n   __attribute__ ((noreturn));\n@@ -42,8 +23,8 @@ extern _Bool __go_can_recover (void *);\n \n extern void __go_makefunc_can_recover (void *retaddr);\n \n-struct Location;\n-extern void __go_makefunc_ffi_can_recover (struct Location *, int);\n+struct location;\n+extern void __go_makefunc_ffi_can_recover (struct location *, int);\n \n extern void __go_makefunc_returning (void);\n "}, {"sha": "4d2f0f9fc39b084bb9b9deba55656fe7c3706eb0", "filename": "libgo/runtime/go-recover.c", "status": "modified", "additions": 30, "deletions": 31, "changes": 61, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-recover.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-recover.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-recover.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -7,33 +7,32 @@\n #include \"runtime.h\"\n #include \"interface.h\"\n #include \"go-panic.h\"\n-#include \"go-defer.h\"\n \n /* If the top of the defer stack can be recovered, then return it.\n    Otherwise return NULL.  */\n \n-static struct __go_defer_stack *\n+static Defer *\n current_defer ()\n {\n   G *g;\n-  struct __go_defer_stack *d;\n+  Defer *d;\n \n   g = runtime_g ();\n \n-  d = g->defer;\n+  d = g->_defer;\n   if (d == NULL)\n     return NULL;\n \n   /* The panic which would be recovered is the one on the top of the\n      panic stack.  We do not want to recover it if that panic was on\n      the top of the panic stack when this function was deferred.  */\n-  if (d->__panic == g->panic)\n+  if (d->_panic == g->_panic)\n     return NULL;\n \n   /* The deferred thunk will call _go_set_defer_retaddr.  If this has\n      not happened, then we have not been called via defer, and we can\n      not recover.  */\n-  if (d->__retaddr == NULL)\n+  if (d->retaddr == 0)\n     return NULL;\n \n   return d;\n@@ -48,7 +47,7 @@ current_defer ()\n _Bool\n __go_can_recover (void *retaddr)\n {\n-  struct __go_defer_stack *d;\n+  Defer *d;\n   const char* ret;\n   const char* dret;\n   Location locs[16];\n@@ -64,7 +63,7 @@ __go_can_recover (void *retaddr)\n \n   ret = (const char *) __builtin_extract_return_addr (retaddr);\n \n-  dret = (const char *) d->__retaddr;\n+  dret = (const char *) (uintptr) d->retaddr;\n   if (ret <= dret && ret + 16 >= dret)\n     return 1;\n \n@@ -111,7 +110,7 @@ __go_can_recover (void *retaddr)\n   /* If the function calling recover was created by reflect.MakeFunc,\n      then __go_makefunc_can_recover or __go_makefunc_ffi_can_recover\n      will have set the __makefunc_can_recover field.  */\n-  if (!d->__makefunc_can_recover)\n+  if (!d->makefunccanrecover)\n     return 0;\n \n   /* We look up the stack, ignoring libffi functions and functions in\n@@ -178,32 +177,32 @@ __go_can_recover (void *retaddr)\n void\n __go_makefunc_can_recover (void *retaddr)\n {\n-  struct __go_defer_stack *d;\n+  Defer *d;\n \n   d = current_defer ();\n   if (d == NULL)\n     return;\n \n   /* If we are already in a call stack of MakeFunc functions, there is\n      nothing we can usefully check here.  */\n-  if (d->__makefunc_can_recover)\n+  if (d->makefunccanrecover)\n     return;\n \n   if (__go_can_recover (retaddr))\n-    d->__makefunc_can_recover = 1;\n+    d->makefunccanrecover = 1;\n }\n \n /* This function is called when code is about to enter a function\n    created by the libffi version of reflect.MakeFunc.  This function\n    is passed the names of the callers of the libffi code that called\n    the stub.  It uses to decide whether it is permitted to call\n-   recover, and sets d->__makefunc_can_recover so that __go_recover\n-   can make the same decision.  */\n+   recover, and sets d->makefunccanrecover so that __go_recover can\n+   make the same decision.  */\n \n void\n-__go_makefunc_ffi_can_recover (struct Location *loc, int n)\n+__go_makefunc_ffi_can_recover (struct location *loc, int n)\n {\n-  struct __go_defer_stack *d;\n+  Defer *d;\n   const byte *name;\n   intgo len;\n \n@@ -213,7 +212,7 @@ __go_makefunc_ffi_can_recover (struct Location *loc, int n)\n \n   /* If we are already in a call stack of MakeFunc functions, there is\n      nothing we can usefully check here.  */\n-  if (d->__makefunc_can_recover)\n+  if (d->makefunccanrecover)\n     return;\n \n   /* LOC points to the caller of our caller.  That will be a thunk.\n@@ -228,26 +227,26 @@ __go_makefunc_ffi_can_recover (struct Location *loc, int n)\n   if (len > 4\n       && __builtin_strchr ((const char *) name, '.') == NULL\n       && __builtin_strncmp ((const char *) name, \"__go_\", 4) == 0)\n-    d->__makefunc_can_recover = 1;\n+    d->makefunccanrecover = 1;\n }\n \n /* This function is called when code is about to exit a function\n    created by reflect.MakeFunc.  It is called by the function stub\n-   used by MakeFunc.  It clears the __makefunc_can_recover field.\n-   It's OK to always clear this field, because __go_can_recover will\n-   only be called by a stub created for a function that calls recover.\n-   That stub will not call a function created by reflect.MakeFunc, so\n-   by the time we get here any caller higher up on the call stack no\n+   used by MakeFunc.  It clears the makefunccanrecover field.  It's OK\n+   to always clear this field, because __go_can_recover will only be\n+   called by a stub created for a function that calls recover.  That\n+   stub will not call a function created by reflect.MakeFunc, so by\n+   the time we get here any caller higher up on the call stack no\n    longer needs the information.  */\n \n void\n __go_makefunc_returning (void)\n {\n-  struct __go_defer_stack *d;\n+  Defer *d;\n \n-  d = runtime_g ()->defer;\n+  d = runtime_g ()->_defer;\n   if (d != NULL)\n-    d->__makefunc_can_recover = 0;\n+    d->makefunccanrecover = 0;\n }\n \n /* This is only called when it is valid for the caller to recover the\n@@ -257,19 +256,19 @@ struct __go_empty_interface\n __go_recover ()\n {\n   G *g;\n-  struct __go_panic_stack *p;\n+  Panic *p;\n \n   g = runtime_g ();\n \n-  if (g->panic == NULL || g->panic->__was_recovered)\n+  if (g->_panic == NULL || g->_panic->recovered)\n     {\n       struct __go_empty_interface ret;\n \n       ret.__type_descriptor = NULL;\n       ret.__object = NULL;\n       return ret;\n     }\n-  p = g->panic;\n-  p->__was_recovered = 1;\n-  return p->__arg;\n+  p = g->_panic;\n+  p->recovered = 1;\n+  return p->arg;\n }"}, {"sha": "0aef2fc9b08782f8d46744981cc6a191f2aeb78a", "filename": "libgo/runtime/go-signal.c", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-signal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-signal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-signal.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -26,11 +26,11 @@ extern void __splitstack_setcontext(void *context[10]);\n \n #endif\n \n-#define N SigNotify\n-#define K SigKill\n-#define T SigThrow\n-#define P SigPanic\n-#define D SigDefault\n+#define N _SigNotify\n+#define K _SigKill\n+#define T _SigThrow\n+#define P _SigPanic\n+#define D _SigDefault\n \n /* Signal actions.  This collects the sigtab tables for several\n    different targets from the master library.  SIGKILL and SIGSTOP are\n@@ -182,14 +182,14 @@ runtime_sighandler (int sig, Siginfo *info,\n #ifdef SA_SIGINFO\n       notify = info != NULL && info->si_code == SI_USER;\n #endif\n-      if (notify || (t->flags & SigNotify) != 0)\n+      if (notify || (t->flags & _SigNotify) != 0)\n \t{\n \t  if (__go_sigsend (sig))\n \t    return;\n \t}\n-      if ((t->flags & SigKill) != 0)\n+      if ((t->flags & _SigKill) != 0)\n \truntime_exit (2);\n-      if ((t->flags & SigThrow) == 0)\n+      if ((t->flags & _SigThrow) == 0)\n \treturn;\n \n       runtime_startpanic ();\n@@ -320,7 +320,7 @@ sig_panic_info_handler (int sig, Siginfo *info, void *context)\n #endif\n     }\n \n-  /* All signals with SigPanic should be in cases above, and this\n+  /* All signals with _SigPanic should be in cases above, and this\n      handler should only be invoked for those signals.  */\n   __builtin_unreachable ();\n }\n@@ -365,7 +365,7 @@ sig_panic_handler (int sig)\n #endif\n     }\n \n-  /* All signals with SigPanic should be in cases above, and this\n+  /* All signals with _SigPanic should be in cases above, and this\n      handler should only be invoked for those signals.  */\n   __builtin_unreachable ();\n }\n@@ -406,7 +406,7 @@ sig_tramp_info (int sig, Siginfo *info, void *context)\n       /* We are running on the signal stack.  Set the split stack\n \t context so that the stack guards are checked correctly.  */\n #ifdef USING_SPLIT_STACK\n-      __splitstack_setcontext (&mp->gsignal->stack_context[0]);\n+      __splitstack_setcontext (&mp->gsignal->stackcontext[0]);\n #endif\n     }\n \n@@ -451,7 +451,7 @@ runtime_setsig (int32 i, GoSighandler *fn, bool restart)\n \n   t = &runtime_sigtab[i];\n \n-  if ((t->flags & SigPanic) == 0)\n+  if ((t->flags & _SigPanic) == 0)\n     {\n #ifdef SA_SIGINFO\n       sa.sa_flags = SA_ONSTACK | SA_SIGINFO;"}, {"sha": "ea11e4e3b1d937a556076cffc42073ba480bfd47", "filename": "libgo/runtime/go-unwind.c", "status": "modified", "additions": 27, "deletions": 29, "changes": 56, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-unwind.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fgo-unwind.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-unwind.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -15,7 +15,6 @@\n \n #include \"runtime.h\"\n #include \"go-alloc.h\"\n-#include \"go-defer.h\"\n #include \"go-panic.h\"\n \n /* The code for a Go exception.  */\n@@ -57,55 +56,54 @@ __go_check_defer (_Bool *frame)\n       /* Some other language has thrown an exception.  We know there\n \t are no defer handlers, so there is nothing to do.  */\n     }\n-  else if (g->is_foreign)\n+  else if (g->isforeign)\n     {\n-      struct __go_panic_stack *n;\n-      _Bool was_recovered;\n+      Panic *n;\n+      _Bool recovered;\n \n       /* Some other language has thrown an exception.  We need to run\n \t the local defer handlers.  If they call recover, we stop\n \t unwinding the stack here.  */\n \n-      n = ((struct __go_panic_stack *)\n-\t   __go_alloc (sizeof (struct __go_panic_stack)));\n+      n = (Panic *) __go_alloc (sizeof (Panic));\n \n-      n->__arg.__type_descriptor = NULL;\n-      n->__arg.__object = NULL;\n-      n->__was_recovered = 0;\n-      n->__is_foreign = 1;\n-      n->__next = g->panic;\n-      g->panic = n;\n+      n->arg.__type_descriptor = NULL;\n+      n->arg.__object = NULL;\n+      n->recovered = 0;\n+      n->isforeign = 1;\n+      n->next = g->_panic;\n+      g->_panic = n;\n \n       while (1)\n \t{\n-\t  struct __go_defer_stack *d;\n+\t  Defer *d;\n \t  void (*pfn) (void *);\n \n-\t  d = g->defer;\n-\t  if (d == NULL || d->__frame != frame || d->__pfn == NULL)\n+\t  d = g->_defer;\n+\t  if (d == NULL || d->frame != frame || d->pfn == 0)\n \t    break;\n \n-\t  pfn = d->__pfn;\n-\t  g->defer = d->__next;\n+\t  pfn = (void (*) (void *)) d->pfn;\n+\t  g->_defer = d->next;\n \n-\t  (*pfn) (d->__arg);\n+\t  (*pfn) (d->arg);\n \n \t  if (runtime_m () != NULL)\n \t    runtime_freedefer (d);\n \n-\t  if (n->__was_recovered)\n+\t  if (n->recovered)\n \t    {\n \t      /* The recover function caught the panic thrown by some\n \t\t other language.  */\n \t      break;\n \t    }\n \t}\n \n-      was_recovered = n->__was_recovered;\n-      g->panic = n->__next;\n+      recovered = n->recovered;\n+      g->_panic = n->next;\n       __go_free (n);\n \n-      if (was_recovered)\n+      if (recovered)\n \t{\n \t  /* Just return and continue executing Go code.  */\n \t  *frame = 1;\n@@ -115,17 +113,17 @@ __go_check_defer (_Bool *frame)\n       /* We are panicing through this function.  */\n       *frame = 0;\n     }\n-  else if (g->defer != NULL\n-\t   && g->defer->__pfn == NULL\n-\t   && g->defer->__frame == frame)\n+  else if (g->_defer != NULL\n+\t   && g->_defer->pfn == 0\n+\t   && g->_defer->frame == frame)\n     {\n-      struct __go_defer_stack *d;\n+      Defer *d;\n \n       /* This is the defer function which called recover.  Simply\n \t return to stop the stack unwind, and let the Go code continue\n \t to execute.  */\n-      d = g->defer;\n-      g->defer = d->__next;\n+      d = g->_defer;\n+      g->_defer = d->next;\n \n       if (runtime_m () != NULL)\n \truntime_freedefer (d);\n@@ -432,7 +430,7 @@ PERSONALITY_FUNCTION (int version,\n   else\n     {\n       g->exception = ue_header;\n-      g->is_foreign = is_foreign;\n+      g->isforeign = is_foreign;\n     }\n \n   _Unwind_SetGR (context, __builtin_eh_return_data_regno (0),"}, {"sha": "18fe913c4eb1b3759fe43f552e0a99842b1ebca9", "filename": "libgo/runtime/heapdump.c", "status": "modified", "additions": 22, "deletions": 23, "changes": 45, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fheapdump.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fheapdump.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fheapdump.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -14,7 +14,6 @@\n #include \"malloc.h\"\n #include \"mgc0.h\"\n #include \"go-type.h\"\n-#include \"go-defer.h\"\n #include \"go-panic.h\"\n \n #define hash __hash\n@@ -265,15 +264,15 @@ dumpgoroutine(G *gp)\n \tdumpint((uintptr)0);\n \tdumpint(gp->goid);\n \tdumpint(gp->gopc);\n-\tdumpint(gp->status);\n+\tdumpint(gp->atomicstatus);\n \tdumpbool(gp->issystem);\n \tdumpbool(gp->isbackground);\n \tdumpint(gp->waitsince);\n-\tdumpcstr((const int8 *)gp->waitreason);\n+\tdumpstr(gp->waitreason);\n \tdumpint((uintptr)0);\n \tdumpint((uintptr)gp->m);\n-\tdumpint((uintptr)gp->defer);\n-\tdumpint((uintptr)gp->panic);\n+\tdumpint((uintptr)gp->_defer);\n+\tdumpint((uintptr)gp->_panic);\n \n \t// dump stack\n \t// child.args.n = -1;\n@@ -285,24 +284,24 @@ dumpgoroutine(G *gp)\n \t// runtime_gentraceback(pc, sp, lr, gp, 0, nil, 0x7fffffff, dumpframe, &child, false);\n \n \t// dump defer & panic records\n-\tfor(d = gp->defer; d != nil; d = d->__next) {\n+\tfor(d = gp->_defer; d != nil; d = d->next) {\n \t\tdumpint(TagDefer);\n \t\tdumpint((uintptr)d);\n \t\tdumpint((uintptr)gp);\n-\t\tdumpint((uintptr)d->__arg);\n-\t\tdumpint((uintptr)d->__frame);\n-\t\tdumpint((uintptr)d->__pfn);\n+\t\tdumpint((uintptr)d->arg);\n+\t\tdumpint((uintptr)d->frame);\n+\t\tdumpint((uintptr)d->pfn);\n \t\tdumpint((uintptr)0);\n-\t\tdumpint((uintptr)d->__next);\n+\t\tdumpint((uintptr)d->next);\n \t}\n-\tfor (p = gp->panic; p != nil; p = p->__next) {\n+\tfor (p = gp->_panic; p != nil; p = p->next) {\n \t\tdumpint(TagPanic);\n \t\tdumpint((uintptr)p);\n \t\tdumpint((uintptr)gp);\n-\t\tdumpint((uintptr)p->__arg.__type_descriptor);\n-\t\tdumpint((uintptr)p->__arg.__object);\n+\t\tdumpint((uintptr)p->arg.__type_descriptor);\n+\t\tdumpint((uintptr)p->arg.__object);\n \t\tdumpint((uintptr)0);\n-\t\tdumpint((uintptr)p->__next);\n+\t\tdumpint((uintptr)p->next);\n \t}\n }\n \n@@ -315,15 +314,15 @@ dumpgs(void)\n \t// goroutines & stacks\n \tfor(i = 0; i < runtime_allglen; i++) {\n \t\tgp = runtime_allg[i];\n-\t\tswitch(gp->status){\n+\t\tswitch(gp->atomicstatus){\n \t\tdefault:\n-\t\t\truntime_printf(\"unexpected G.status %d\\n\", gp->status);\n+\t\t\truntime_printf(\"unexpected G.status %d\\n\", gp->atomicstatus);\n \t\t\truntime_throw(\"mark - bad status\");\n-\t\tcase Gdead:\n+\t\tcase _Gdead:\n \t\t\tbreak;\n-\t\tcase Grunnable:\n-\t\tcase Gsyscall:\n-\t\tcase Gwaiting:\n+\t\tcase _Grunnable:\n+\t\tcase _Gsyscall:\n+\t\tcase _Gwaiting:\n \t\t\tdumpgoroutine(gp);\n \t\t\tbreak;\n \t\t}\n@@ -602,7 +601,7 @@ mdump(G *gp)\n \tflush();\n \n \tgp->param = nil;\n-\tgp->status = Grunning;\n+\tgp->atomicstatus = _Grunning;\n \truntime_gogo(gp);\n }\n \n@@ -632,8 +631,8 @@ runtime_debug_WriteHeapDump(uintptr fd)\n \n \t// Call dump routine on M stack.\n \tg = runtime_g();\n-\tg->status = Gwaiting;\n-\tg->waitreason = \"dumping heap\";\n+\tg->atomicstatus = _Gwaiting;\n+\tg->waitreason = runtime_gostringnocopy((const byte*)\"dumping heap\");\n \truntime_mcall(mdump);\n \n \t// Reset dump file."}, {"sha": "06ac6e7f90d265392093d43f89b745f41c8754c0", "filename": "libgo/runtime/lock_sema.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Flock_sema.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Flock_sema.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Flock_sema.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -73,7 +73,7 @@ runtime_lock(Lock *l)\n \t\t\t// for this lock, chained through m->nextwaitm.\n \t\t\t// Queue this M.\n \t\t\tfor(;;) {\n-\t\t\t\tm->nextwaitm = (void*)(v&~LOCKED);\n+\t\t\t\tm->nextwaitm = v&~LOCKED;\n \t\t\t\tif(runtime_casp((void**)&l->key, (void*)v, (void*)((uintptr)m|LOCKED)))\n \t\t\t\t\tbreak;\n \t\t\t\tv = (uintptr)runtime_atomicloadp((void**)&l->key);\n@@ -104,7 +104,7 @@ runtime_unlock(Lock *l)\n \t\t\t// Other M's are waiting for the lock.\n \t\t\t// Dequeue an M.\n \t\t\tmp = (void*)(v&~LOCKED);\n-\t\t\tif(runtime_casp((void**)&l->key, (void*)v, mp->nextwaitm)) {\n+\t\t\tif(runtime_cas(&l->key, v, mp->nextwaitm)) {\n \t\t\t\t// Dequeued an M.  Wake it.\n \t\t\t\truntime_semawakeup(mp);\n \t\t\t\tbreak;"}, {"sha": "fbb7b744eeb18bef1e4596072f72bd60f0032dcc", "filename": "libgo/runtime/malloc.goc", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmalloc.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmalloc.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmalloc.goc?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -92,11 +92,11 @@ runtime_mallocgc(uintptr size, uintptr typ, uint32 flag)\n \t\treturn &runtime_zerobase;\n \t}\n \n-\tm = runtime_m();\n \tg = runtime_g();\n+\tm = g->m;\n \n \tincallback = false;\n-\tif(m->mcache == nil && g->ncgo > 0) {\n+\tif(m->mcache == nil && m->ncgo > 0) {\n \t\t// For gccgo this case can occur when a cgo or SWIG function\n \t\t// has an interface return type and the function\n \t\t// returns a non-pointer, so memory allocation occurs\n@@ -165,11 +165,11 @@ runtime_mallocgc(uintptr size, uintptr typ, uint32 flag)\n \t\t\t\t\ttiny = (byte*)ROUND((uintptr)tiny, 4);\n \t\t\t\telse if((size&1) == 0)\n \t\t\t\t\ttiny = (byte*)ROUND((uintptr)tiny, 2);\n-\t\t\t\tsize1 = size + (tiny - c->tiny);\n+\t\t\t\tsize1 = size + (tiny - (byte*)c->tiny);\n \t\t\t\tif(size1 <= tinysize) {\n \t\t\t\t\t// The object fits into existing tiny block.\n \t\t\t\t\tv = (MLink*)tiny;\n-\t\t\t\t\tc->tiny += size1;\n+\t\t\t\t\tc->tiny = (byte*)c->tiny + size1;\n \t\t\t\t\tc->tinysize -= size1;\n \t\t\t\t\tm->mallocing = 0;\n \t\t\t\t\tm->locks--;\n@@ -281,7 +281,7 @@ largealloc(uint32 flag, uintptr *sizep)\n \ts = runtime_MHeap_Alloc(&runtime_mheap, npages, 0, 1, !(flag & FlagNoZero));\n \tif(s == nil)\n \t\truntime_throw(\"out of memory\");\n-\ts->limit = (byte*)(s->start<<PageShift) + size;\n+\ts->limit = (uintptr)((byte*)(s->start<<PageShift) + size);\n \t*sizep = npages<<PageShift;\n \tv = (void*)(s->start << PageShift);\n \t// setup for mark sweep\n@@ -475,7 +475,7 @@ runtime_purgecachedstats(MCache *c)\n \n \t// Protected by either heap or GC lock.\n \th = &runtime_mheap;\n-\tmstats.heap_alloc += c->local_cachealloc;\n+\tmstats.heap_alloc += (intptr)c->local_cachealloc;\n \tc->local_cachealloc = 0;\n \tmstats.nlookup += c->local_nlookup;\n \tc->local_nlookup = 0;\n@@ -493,7 +493,7 @@ extern uintptr runtime_sizeof_C_MStats\n   __asm__ (GOSYM_PREFIX \"runtime.Sizeof_C_MStats\");\n \n // Size of the trailing by_size array differs between Go and C,\n-// NumSizeClasses was changed, but we can not change Go struct because of backward compatibility.\n+// _NumSizeClasses was changed, but we can not change Go struct because of backward compatibility.\n // sizeof_C_MStats is what C thinks about size of Go struct.\n \n // Initialized in mallocinit because it's defined in go/runtime/mem.go.\n@@ -511,7 +511,7 @@ runtime_mallocinit(void)\n \tuint64 i;\n \tbool reserved;\n \n-\truntime_sizeof_C_MStats = sizeof(MStats) - (NumSizeClasses - 61) * sizeof(mstats.by_size[0]);\n+\truntime_sizeof_C_MStats = sizeof(MStats) - (_NumSizeClasses - 61) * sizeof(mstats.by_size[0]);\n \n \tp = nil;\n \tp_size = 0;"}, {"sha": "acd919f7abefd1aa1ad2cea1f9c2d52b5b4d21d7", "filename": "libgo/runtime/malloc.h", "status": "modified", "additions": 14, "deletions": 86, "changes": 100, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmalloc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmalloc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmalloc.h?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -82,11 +82,11 @@\n \n typedef struct MCentral\tMCentral;\n typedef struct MHeap\tMHeap;\n-typedef struct MSpan\tMSpan;\n+typedef struct mspan\tMSpan;\n typedef struct MStats\tMStats;\n-typedef struct MLink\tMLink;\n-typedef struct MTypes\tMTypes;\n-typedef struct GCStats\tGCStats;\n+typedef struct mlink\tMLink;\n+typedef struct mtypes\tMTypes;\n+typedef struct gcstats\tGCStats;\n \n enum\n {\n@@ -100,10 +100,10 @@ enum\n {\n \t// Computed constant.  The definition of MaxSmallSize and the\n \t// algorithm in msize.c produce some number of different allocation\n-\t// size classes.  NumSizeClasses is that number.  It's needed here\n+\t// size classes.  _NumSizeClasses is that number.  It's needed here\n \t// because there are static arrays of this length; when msize runs its\n \t// size choosing algorithm it double-checks that NumSizeClasses agrees.\n-\tNumSizeClasses = 67,\n+\t// _NumSizeClasses is defined in runtime2.go as 67.\n \n \t// Tunable constants.\n \tMaxSmallSize = 32<<10,\n@@ -148,13 +148,6 @@ enum\n #else\n #define\tMaxMem\t((uintptr)-1)\n #endif\n-\n-// A generic linked list of blocks.  (Typically the block is bigger than sizeof(MLink).)\n-struct MLink\n-{\n-\tMLink *next;\n-};\n-\n // SysAlloc obtains a large chunk of zeroed memory from the\n // operating system, typically on the order of a hundred kilobytes\n // or a megabyte.\n@@ -274,7 +267,7 @@ struct MStats\n \t\tuint32 size;\n \t\tuint64 nmalloc;\n \t\tuint64 nfree;\n-\t} by_size[NumSizeClasses];\n+\t} by_size[_NumSizeClasses];\n };\n \n extern MStats mstats\n@@ -284,7 +277,7 @@ void\truntime_updatememstats(GCStats *stats);\n // Size classes.  Computed and initialized by InitSizes.\n //\n // SizeToClass(0 <= n <= MaxSmallSize) returns the size class,\n-//\t1 <= sizeclass < NumSizeClasses, for n.\n+//\t1 <= sizeclass < _NumSizeClasses, for n.\n //\tSize class 0 is reserved to mean \"not small\".\n //\n // class_to_size[i] = largest size in class i\n@@ -293,41 +286,14 @@ void\truntime_updatememstats(GCStats *stats);\n \n int32\truntime_SizeToClass(int32);\n uintptr\truntime_roundupsize(uintptr);\n-extern\tint32\truntime_class_to_size[NumSizeClasses];\n-extern\tint32\truntime_class_to_allocnpages[NumSizeClasses];\n+extern\tint32\truntime_class_to_size[_NumSizeClasses];\n+extern\tint32\truntime_class_to_allocnpages[_NumSizeClasses];\n extern\tint8\truntime_size_to_class8[1024/8 + 1];\n extern\tint8\truntime_size_to_class128[(MaxSmallSize-1024)/128 + 1];\n extern\tvoid\truntime_InitSizes(void);\n \n \n-typedef struct MCacheList MCacheList;\n-struct MCacheList\n-{\n-\tMLink *list;\n-\tuint32 nlist;\n-};\n-\n-// Per-thread (in Go, per-P) cache for small objects.\n-// No locking needed because it is per-thread (per-P).\n-struct MCache\n-{\n-\t// The following members are accessed on every malloc,\n-\t// so they are grouped here for better caching.\n-\tint32 next_sample;\t\t// trigger heap sample after allocating this many bytes\n-\tintptr local_cachealloc;\t// bytes allocated (or freed) from cache since last lock of heap\n-\t// Allocator cache for tiny objects w/o pointers.\n-\t// See \"Tiny allocator\" comment in malloc.goc.\n-\tbyte*\ttiny;\n-\tuintptr\ttinysize;\n-\t// The rest is not accessed on every malloc.\n-\tMSpan*\talloc[NumSizeClasses];\t// spans to allocate from\n-\tMCacheList free[NumSizeClasses];// lists of explicitly freed objects\n-\t// Local allocator stats, flushed during GC.\n-\tuintptr local_nlookup;\t\t// number of pointer lookups\n-\tuintptr local_largefree;\t// bytes freed for large objects (>MaxSmallSize)\n-\tuintptr local_nlargefree;\t// number of frees for large objects (>MaxSmallSize)\n-\tuintptr local_nsmallfree[NumSizeClasses];\t// number of frees for small objects (<=MaxSmallSize)\n-};\n+typedef struct mcachelist MCacheList;\n \n MSpan*\truntime_MCache_Refill(MCache *c, int32 sizeclass);\n void\truntime_MCache_Free(MCache *c, MLink *p, int32 sizeclass, uintptr size);\n@@ -364,11 +330,6 @@ enum\n \tMTypes_Words = 2,\n \tMTypes_Bytes = 3,\n };\n-struct MTypes\n-{\n-\tbyte\tcompression;\t// one of MTypes_*\n-\tuintptr\tdata;\n-};\n \n enum\n {\n@@ -380,13 +341,7 @@ enum\n \t// if that happens.\n };\n \n-typedef struct Special Special;\n-struct Special\n-{\n-\tSpecial*\tnext;\t// linked list in span\n-\tuint16\t\toffset;\t// span offset of object\n-\tbyte\t\tkind;\t// kind of Special\n-};\n+typedef struct special Special;\n \n // The described object has a finalizer set for it.\n typedef struct SpecialFinalizer SpecialFinalizer;\n@@ -415,33 +370,6 @@ enum\n \tMSpanListHead,\n \tMSpanDead,\n };\n-struct MSpan\n-{\n-\tMSpan\t*next;\t\t// in a span linked list\n-\tMSpan\t*prev;\t\t// in a span linked list\n-\tPageID\tstart;\t\t// starting page number\n-\tuintptr\tnpages;\t\t// number of pages in span\n-\tMLink\t*freelist;\t// list of free objects\n-\t// sweep generation:\n-\t// if sweepgen == h->sweepgen - 2, the span needs sweeping\n-\t// if sweepgen == h->sweepgen - 1, the span is currently being swept\n-\t// if sweepgen == h->sweepgen, the span is swept and ready to use\n-\t// h->sweepgen is incremented by 2 after every GC\n-\tuint32\tsweepgen;\n-\tuint16\tref;\t\t// capacity - number of objects in freelist\n-\tuint8\tsizeclass;\t// size class\n-\tbool\tincache;\t// being used by an MCache\n-\tuint8\tstate;\t\t// MSpanInUse etc\n-\tuint8\tneedzero;\t// needs to be zeroed before allocation\n-\tuintptr\telemsize;\t// computed from sizeclass or from npages\n-\tint64   unusedsince;\t// First time spotted by GC in MSpanFree state\n-\tuintptr npreleased;\t// number of pages released to the OS\n-\tbyte\t*limit;\t\t// end of data in span\n-\tMTypes\ttypes;\t\t// types of allocated objects in this span\n-\tLock\tspecialLock;\t// guards specials list\n-\tSpecial\t*specials;\t// linked list of special records sorted by offset.\n-\tMLink\t*freebuf;\t// objects freed explicitly, not incorporated into freelist yet\n-};\n \n void\truntime_MSpan_Init(MSpan *span, PageID start, uintptr npages);\n void\truntime_MSpan_EnsureSwept(MSpan *span);\n@@ -509,7 +437,7 @@ struct MHeap\n \tstruct {\n \t\tMCentral;\n \t\tbyte pad[64];\n-\t} central[NumSizeClasses];\n+\t} central[_NumSizeClasses];\n \n \tFixAlloc spanalloc;\t// allocator for Span*\n \tFixAlloc cachealloc;\t// allocator for MCache*\n@@ -520,7 +448,7 @@ struct MHeap\n \t// Malloc stats.\n \tuint64 largefree;\t// bytes freed for large objects (>MaxSmallSize)\n \tuint64 nlargefree;\t// number of frees for large objects (>MaxSmallSize)\n-\tuint64 nsmallfree[NumSizeClasses];\t// number of frees for small objects (<=MaxSmallSize)\n+\tuint64 nsmallfree[_NumSizeClasses];\t// number of frees for small objects (<=MaxSmallSize)\n };\n extern MHeap runtime_mheap;\n "}, {"sha": "439627146951a47a0d3682edb73feef8c1cf965a", "filename": "libgo/runtime/mcache.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmcache.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmcache.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmcache.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -27,7 +27,7 @@ runtime_allocmcache(void)\n \tc = runtime_FixAlloc_Alloc(&runtime_mheap.cachealloc);\n \truntime_unlock(&runtime_mheap);\n \truntime_memclr((byte*)c, sizeof(*c));\n-\tfor(i = 0; i < NumSizeClasses; i++)\n+\tfor(i = 0; i < _NumSizeClasses; i++)\n \t\tc->alloc[i] = &emptymspan;\n \n \t// Set first allocation sample size.\n@@ -115,7 +115,7 @@ runtime_MCache_ReleaseAll(MCache *c)\n \tMSpan *s;\n \tMCacheList *l;\n \n-\tfor(i=0; i<NumSizeClasses; i++) {\n+\tfor(i=0; i<_NumSizeClasses; i++) {\n \t\ts = c->alloc[i];\n \t\tif(s != &emptymspan) {\n \t\t\truntime_MCentral_UncacheSpan(&runtime_mheap.central[i], s);"}, {"sha": "62e2c2d7dfb5d57f022c6f41eddc62bf392eb9db", "filename": "libgo/runtime/mcentral.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmcentral.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmcentral.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmcentral.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -272,7 +272,7 @@ MCentral_Grow(MCentral *c)\n \t// Carve span into sequence of blocks.\n \ttailp = &s->freelist;\n \tp = (byte*)(s->start << PageShift);\n-\ts->limit = p + size*n;\n+\ts->limit = (uintptr)(p + size*n);\n \tfor(i=0; i<n; i++) {\n \t\tv = (MLink*)p;\n \t\t*tailp = v;"}, {"sha": "1f6a40cd630724404717438df4c91cf242eb6789", "filename": "libgo/runtime/mgc0.c", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmgc0.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmgc0.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmgc0.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -321,7 +321,7 @@ markonly(const void *obj)\n \tx = k;\n \tx -= (uintptr)runtime_mheap.arena_start>>PageShift;\n \ts = runtime_mheap.spans[x];\n-\tif(s == nil || k < s->start || (const byte*)obj >= s->limit || s->state != MSpanInUse)\n+\tif(s == nil || k < s->start || (uintptr)obj >= s->limit || s->state != MSpanInUse)\n \t\treturn false;\n \tp = (byte*)((uintptr)s->start<<PageShift);\n \tif(s->sizeclass == 0) {\n@@ -517,7 +517,7 @@ flushptrbuf(Scanbuf *sbuf)\n \t\tx = k;\n \t\tx -= (uintptr)arena_start>>PageShift;\n \t\ts = runtime_mheap.spans[x];\n-\t\tif(s == nil || k < s->start || obj >= s->limit || s->state != MSpanInUse)\n+\t\tif(s == nil || k < s->start || (uintptr)obj >= s->limit || s->state != MSpanInUse)\n \t\t\tcontinue;\n \t\tp = (byte*)((uintptr)s->start<<PageShift);\n \t\tif(s->sizeclass == 0) {\n@@ -651,8 +651,8 @@ static uintptr defaultProg[2] = {PtrSize, GC_DEFAULT_PTR};\n static uintptr chanProg[2] = {0, GC_CHAN};\n \n // Local variables of a program fragment or loop\n-typedef struct Frame Frame;\n-struct Frame {\n+typedef struct GCFrame GCFrame;\n+struct GCFrame {\n \tuintptr count, elemsize, b;\n \tconst uintptr *loop_or_ret;\n };\n@@ -731,7 +731,7 @@ scanblock(Workbuf *wbuf, bool keepworking)\n \tconst Type *t, *et;\n \tSlice *sliceptr;\n \tString *stringptr;\n-\tFrame *stack_ptr, stack_top, stack[GC_STACK_CAPACITY+4];\n+\tGCFrame *stack_ptr, stack_top, stack[GC_STACK_CAPACITY+4];\n \tBufferList *scanbuffers;\n \tScanbuf sbuf;\n \tEface *eface;\n@@ -1057,7 +1057,7 @@ scanblock(Workbuf *wbuf, bool keepworking)\n \n \t\t\t// Stack push.\n \t\t\t*stack_ptr-- = stack_top;\n-\t\t\tstack_top = (Frame){count, elemsize, i, pc};\n+\t\t\tstack_top = (GCFrame){count, elemsize, i, pc};\n \t\t\tcontinue;\n \n \t\tcase GC_ARRAY_NEXT:\n@@ -1074,7 +1074,7 @@ scanblock(Workbuf *wbuf, bool keepworking)\n \t\tcase GC_CALL:\n \t\t\t// Stack push.\n \t\t\t*stack_ptr-- = stack_top;\n-\t\t\tstack_top = (Frame){1, 0, stack_top.b + pc[1], pc+3 /*return address*/};\n+\t\t\tstack_top = (GCFrame){1, 0, stack_top.b + pc[1], pc+3 /*return address*/};\n \t\t\tpc = (const uintptr*)((const byte*)pc + *(const int32*)(pc+2));  // target of the CALL instruction\n \t\t\tcontinue;\n \n@@ -1357,7 +1357,7 @@ markroot(ParFor *desc, uint32 i)\n \t\tgp = runtime_allg[i - RootCount];\n \t\t// remember when we've first observed the G blocked\n \t\t// needed only to output in traceback\n-\t\tif((gp->status == Gwaiting || gp->status == Gsyscall) && gp->waitsince == 0)\n+\t\tif((gp->atomicstatus == _Gwaiting || gp->atomicstatus == _Gsyscall) && gp->waitsince == 0)\n \t\t\tgp->waitsince = work.tstart;\n \t\taddstackroots(gp, &wbuf);\n \t\tbreak;\n@@ -1472,17 +1472,17 @@ handoff(Workbuf *b)\n static void\n addstackroots(G *gp, Workbuf **wbufp)\n {\n-\tswitch(gp->status){\n+\tswitch(gp->atomicstatus){\n \tdefault:\n-\t\truntime_printf(\"unexpected G.status %d (goroutine %p %D)\\n\", gp->status, gp, gp->goid);\n+\t\truntime_printf(\"unexpected G.status %d (goroutine %p %D)\\n\", gp->atomicstatus, gp, gp->goid);\n \t\truntime_throw(\"mark - bad status\");\n-\tcase Gdead:\n+\tcase _Gdead:\n \t\treturn;\n-\tcase Grunning:\n+\tcase _Grunning:\n \t\truntime_throw(\"mark - world not stopped\");\n-\tcase Grunnable:\n-\tcase Gsyscall:\n-\tcase Gwaiting:\n+\tcase _Grunnable:\n+\tcase _Gsyscall:\n+\tcase _Gwaiting:\n \t\tbreak;\n \t}\n \n@@ -1512,12 +1512,12 @@ addstackroots(G *gp, Workbuf **wbufp)\n \t\t// the system call instead, since that won't change underfoot.\n \t\tif(gp->gcstack != nil) {\n \t\t\tsp = gp->gcstack;\n-\t\t\tspsize = gp->gcstack_size;\n-\t\t\tnext_segment = gp->gcnext_segment;\n-\t\t\tnext_sp = gp->gcnext_sp;\n-\t\t\tinitial_sp = gp->gcinitial_sp;\n+\t\t\tspsize = gp->gcstacksize;\n+\t\t\tnext_segment = gp->gcnextsegment;\n+\t\t\tnext_sp = gp->gcnextsp;\n+\t\t\tinitial_sp = gp->gcinitialsp;\n \t\t} else {\n-\t\t\tsp = __splitstack_find_context(&gp->stack_context[0],\n+\t\t\tsp = __splitstack_find_context(&gp->stackcontext[0],\n \t\t\t\t\t\t       &spsize, &next_segment,\n \t\t\t\t\t\t       &next_sp, &initial_sp);\n \t\t}\n@@ -1543,11 +1543,11 @@ addstackroots(G *gp, Workbuf **wbufp)\n \t} else {\n \t\t// Scanning another goroutine's stack.\n \t\t// The goroutine is usually asleep (the world is stopped).\n-\t\tbottom = (byte*)gp->gcnext_sp;\n+\t\tbottom = (byte*)gp->gcnextsp;\n \t\tif(bottom == nil)\n \t\t\treturn;\n \t}\n-\ttop = (byte*)gp->gcinitial_sp + gp->gcstack_size;\n+\ttop = (byte*)gp->gcinitialsp + gp->gcstacksize;\n \tif(top > bottom)\n \t\tenqueue1(wbufp, (Obj){bottom, top - bottom, 0});\n \telse\n@@ -2186,8 +2186,8 @@ runtime_gc(int32 force)\n \t\t// switch to g0, call gc(&a), then switch back\n \t\tg = runtime_g();\n \t\tg->param = &a;\n-\t\tg->status = Gwaiting;\n-\t\tg->waitreason = \"garbage collection\";\n+\t\tg->atomicstatus = _Gwaiting;\n+\t\tg->waitreason = runtime_gostringnocopy((const byte*)\"garbage collection\");\n \t\truntime_mcall(mgc);\n \t\tm = runtime_m();\n \t}\n@@ -2214,7 +2214,7 @@ mgc(G *gp)\n {\n \tgc(gp->param);\n \tgp->param = nil;\n-\tgp->status = Grunning;\n+\tgp->atomicstatus = _Grunning;\n \truntime_gogo(gp);\n }\n \n@@ -2404,7 +2404,7 @@ runtime_ReadMemStats(MStats *stats)\n \truntime_stoptheworld();\n \truntime_updatememstats(nil);\n \t// Size of the trailing by_size array differs between Go and C,\n-\t// NumSizeClasses was changed, but we can not change Go struct because of backward compatibility.\n+\t// _NumSizeClasses was changed, but we can not change Go struct because of backward compatibility.\n \truntime_memmove(stats, &mstats, runtime_sizeof_C_MStats);\n \tm->gcing = 0;\n \tm->locks++;"}, {"sha": "04dc971d688489a7cdcaf58847a215905e9b3595", "filename": "libgo/runtime/mheap.c", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmheap.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmheap.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmheap.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -176,7 +176,7 @@ runtime_MHeap_Alloc(MHeap *h, uintptr npage, int32 sizeclass, bool large, bool n\n \tMSpan *s;\n \n \truntime_lock(h);\n-\tmstats.heap_alloc += runtime_m()->mcache->local_cachealloc;\n+\tmstats.heap_alloc += (intptr)runtime_m()->mcache->local_cachealloc;\n \truntime_m()->mcache->local_cachealloc = 0;\n \ts = MHeap_AllocLocked(h, npage, sizeclass);\n \tif(s != nil) {\n@@ -377,7 +377,7 @@ runtime_MHeap_LookupMaybe(MHeap *h, void *v)\n \tq = p;\n \tq -= (uintptr)h->arena_start >> PageShift;\n \ts = h->spans[q];\n-\tif(s == nil || p < s->start || (byte*)v >= s->limit || s->state != MSpanInUse)\n+\tif(s == nil || p < s->start || (uintptr)v >= s->limit || s->state != MSpanInUse)\n \t\treturn nil;\n \treturn s;\n }\n@@ -387,7 +387,7 @@ void\n runtime_MHeap_Free(MHeap *h, MSpan *s, int32 acct)\n {\n \truntime_lock(h);\n-\tmstats.heap_alloc += runtime_m()->mcache->local_cachealloc;\n+\tmstats.heap_alloc += (intptr)runtime_m()->mcache->local_cachealloc;\n \truntime_m()->mcache->local_cachealloc = 0;\n \tmstats.heap_inuse -= s->npages<<PageShift;\n \tif(acct) {\n@@ -597,7 +597,7 @@ runtime_MSpan_Init(MSpan *span, PageID start, uintptr npages)\n \tspan->unusedsince = 0;\n \tspan->npreleased = 0;\n \tspan->types.compression = MTypes_Empty;\n-\tspan->specialLock.key = 0;\n+\tspan->speciallock.key = 0;\n \tspan->specials = nil;\n \tspan->needzero = 0;\n \tspan->freebuf = nil;\n@@ -681,13 +681,13 @@ addspecial(void *p, Special *s)\n \toffset = (uintptr)p - (span->start << PageShift);\n \tkind = s->kind;\n \n-\truntime_lock(&span->specialLock);\n+\truntime_lock(&span->speciallock);\n \n \t// Find splice point, check for existing record.\n \tt = &span->specials;\n \twhile((x = *t) != nil) {\n \t\tif(offset == x->offset && kind == x->kind) {\n-\t\t\truntime_unlock(&span->specialLock);\n+\t\t\truntime_unlock(&span->speciallock);\n \t\t\truntime_m()->locks--;\n \t\t\treturn false; // already exists\n \t\t}\n@@ -699,7 +699,7 @@ addspecial(void *p, Special *s)\n \ts->offset = offset;\n \ts->next = x;\n \t*t = s;\n-\truntime_unlock(&span->specialLock);\n+\truntime_unlock(&span->speciallock);\n \truntime_m()->locks--;\n \treturn true;\n }\n@@ -725,20 +725,20 @@ removespecial(void *p, byte kind)\n \n \toffset = (uintptr)p - (span->start << PageShift);\n \n-\truntime_lock(&span->specialLock);\n+\truntime_lock(&span->speciallock);\n \tt = &span->specials;\n \twhile((s = *t) != nil) {\n \t\t// This function is used for finalizers only, so we don't check for\n \t\t// \"interior\" specials (p must be exactly equal to s->offset).\n \t\tif(offset == s->offset && kind == s->kind) {\n \t\t\t*t = s->next;\n-\t\t\truntime_unlock(&span->specialLock);\n+\t\t\truntime_unlock(&span->speciallock);\n \t\t\truntime_m()->locks--;\n \t\t\treturn s;\n \t\t}\n \t\tt = &s->next;\n \t}\n-\truntime_unlock(&span->specialLock);\n+\truntime_unlock(&span->speciallock);\n \truntime_m()->locks--;\n \treturn nil;\n }\n@@ -838,7 +838,7 @@ runtime_freeallspecials(MSpan *span, void *p, uintptr size)\n \t// this is required to not cause deadlock between span->specialLock and proflock\n \tlist = nil;\n \toffset = (uintptr)p - (span->start << PageShift);\n-\truntime_lock(&span->specialLock);\n+\truntime_lock(&span->speciallock);\n \tt = &span->specials;\n \twhile((s = *t) != nil) {\n \t\tif(offset + size <= s->offset)\n@@ -850,7 +850,7 @@ runtime_freeallspecials(MSpan *span, void *p, uintptr size)\n \t\t} else\n \t\t\tt = &s->next;\n \t}\n-\truntime_unlock(&span->specialLock);\n+\truntime_unlock(&span->speciallock);\n \n \twhile(list != nil) {\n \t\ts = list;\n@@ -908,7 +908,7 @@ runtime_MHeap_SplitSpan(MHeap *h, MSpan *s)\n \t\t// Allocate a new span for the first half.\n \t\tt = runtime_FixAlloc_Alloc(&h->spanalloc);\n \t\truntime_MSpan_Init(t, s->start, npages/2);\n-\t\tt->limit = (byte*)((t->start + npages/2) << PageShift);\n+\t\tt->limit = (uintptr)((t->start + npages/2) << PageShift);\n \t\tt->state = MSpanInUse;\n \t\tt->elemsize = npages << (PageShift - 1);\n \t\tt->sweepgen = s->sweepgen;"}, {"sha": "bb3f9e8c8e0abb06f1349ee22c22f3cdd8bbb1e2", "filename": "libgo/runtime/mprof.goc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmprof.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmprof.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmprof.goc?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -479,7 +479,7 @@ func GoroutineProfile(b Slice) (n int, ok bool) {\n \t\t\tsaveg(g, r++);\n \t\t\tfor(i = 0; i < runtime_allglen; i++) {\n \t\t\t\tgp = runtime_allg[i];\n-\t\t\t\tif(gp == g || gp->status == Gdead)\n+\t\t\t\tif(gp == g || gp->atomicstatus == _Gdead)\n \t\t\t\t\tcontinue;\n \t\t\t\tsaveg(gp, r++);\n \t\t\t}"}, {"sha": "1bafc82bd618c82882a96c16109ffa5928b42027", "filename": "libgo/runtime/msize.c", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmsize.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fmsize.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmsize.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -29,8 +29,8 @@\n #include \"arch.h\"\n #include \"malloc.h\"\n \n-int32 runtime_class_to_size[NumSizeClasses];\n-int32 runtime_class_to_allocnpages[NumSizeClasses];\n+int32 runtime_class_to_size[_NumSizeClasses];\n+int32 runtime_class_to_allocnpages[_NumSizeClasses];\n \n // The SizeToClass lookup is implemented using two arrays,\n // one mapping sizes <= 1024 to their class and one mapping\n@@ -101,14 +101,14 @@ runtime_InitSizes(void)\n \t\truntime_class_to_size[sizeclass] = size;\n \t\tsizeclass++;\n \t}\n-\tif(sizeclass != NumSizeClasses) {\n-\t\truntime_printf(\"sizeclass=%d NumSizeClasses=%d\\n\", sizeclass, NumSizeClasses);\n-\t\truntime_throw(\"InitSizes - bad NumSizeClasses\");\n+\tif(sizeclass != _NumSizeClasses) {\n+\t\truntime_printf(\"sizeclass=%d _NumSizeClasses=%d\\n\", sizeclass, _NumSizeClasses);\n+\t\truntime_throw(\"InitSizes - bad _NumSizeClasses\");\n \t}\n \n \t// Initialize the size_to_class tables.\n \tnextsize = 0;\n-\tfor (sizeclass = 1; sizeclass < NumSizeClasses; sizeclass++) {\n+\tfor (sizeclass = 1; sizeclass < _NumSizeClasses; sizeclass++) {\n \t\tfor(; nextsize < 1024 && nextsize <= runtime_class_to_size[sizeclass]; nextsize+=8)\n \t\t\truntime_size_to_class8[nextsize/8] = sizeclass;\n \t\tif(nextsize >= 1024)\n@@ -120,7 +120,7 @@ runtime_InitSizes(void)\n \tif(0) {\n \t\tfor(n=0; n < MaxSmallSize; n++) {\n \t\t\tsizeclass = runtime_SizeToClass(n);\n-\t\t\tif(sizeclass < 1 || sizeclass >= NumSizeClasses || runtime_class_to_size[sizeclass] < n) {\n+\t\t\tif(sizeclass < 1 || sizeclass >= _NumSizeClasses || runtime_class_to_size[sizeclass] < n) {\n \t\t\t\truntime_printf(\"size=%d sizeclass=%d runtime_class_to_size=%d\\n\", n, sizeclass, runtime_class_to_size[sizeclass]);\n \t\t\t\truntime_printf(\"incorrect SizeToClass\");\n \t\t\t\tgoto dump;\n@@ -140,9 +140,9 @@ runtime_InitSizes(void)\n \n dump:\n \tif(1){\n-\t\truntime_printf(\"NumSizeClasses=%d\\n\", NumSizeClasses);\n+\t\truntime_printf(\"NumSizeClasses=%d\\n\", _NumSizeClasses);\n \t\truntime_printf(\"runtime_class_to_size:\");\n-\t\tfor(sizeclass=0; sizeclass<NumSizeClasses; sizeclass++)\n+\t\tfor(sizeclass=0; sizeclass<_NumSizeClasses; sizeclass++)\n \t\t\truntime_printf(\" %d\", runtime_class_to_size[sizeclass]);\n \t\truntime_printf(\"\\n\\n\");\n \t\truntime_printf(\"size_to_class8:\");"}, {"sha": "2744ec5989f65db3af0455f86d2dc9790d8f6b6f", "filename": "libgo/runtime/netpoll.goc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fnetpoll.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fnetpoll.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fnetpoll.goc?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -301,11 +301,11 @@ runtime_netpollready(G **gpp, PollDesc *pd, int32 mode)\n \tif(mode == 'w' || mode == 'r'+'w')\n \t\twg = netpollunblock(pd, 'w', true);\n \tif(rg) {\n-\t\trg->schedlink = *gpp;\n+\t\trg->schedlink = (uintptr)*gpp;\n \t\t*gpp = rg;\n \t}\n \tif(wg) {\n-\t\twg->schedlink = *gpp;\n+\t\twg->schedlink = (uintptr)*gpp;\n \t\t*gpp = wg;\n \t}\n }"}, {"sha": "3fb3bde32232408b1e1a8d5c8983a10db93189fd", "filename": "libgo/runtime/panic.c", "status": "modified", "additions": 11, "deletions": 12, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fpanic.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fpanic.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fpanic.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -4,7 +4,6 @@\n \n #include \"runtime.h\"\n #include \"malloc.h\"\n-#include \"go-defer.h\"\n #include \"go-panic.h\"\n \n // Code related to defer, panic and recover.\n@@ -21,10 +20,10 @@ runtime_newdefer()\n \tP *p;\n \n \td = nil;\n-\tp = runtime_m()->p;\n+\tp = (P*)runtime_m()->p;\n \td = p->deferpool;\n \tif(d)\n-\t\tp->deferpool = d->__next;\n+\t\tp->deferpool = d->next;\n \tif(d == nil) {\n \t\t// deferpool is empty\n \t\td = runtime_malloc(sizeof(Defer));\n@@ -39,10 +38,10 @@ runtime_freedefer(Defer *d)\n {\n \tP *p;\n \n-\tif(d->__special)\n+\tif(d->special)\n \t\treturn;\n-\tp = runtime_m()->p;\n-\td->__next = p->deferpool;\n+\tp = (P*)runtime_m()->p;\n+\td->next = p->deferpool;\n \tp->deferpool = d;\n \t// No need to wipe out pointers in argp/pc/fn/args,\n \t// because we empty the pool before GC.\n@@ -58,14 +57,14 @@ __go_rundefer(void)\n \tDefer *d;\n \n \tg = runtime_g();\n-\twhile((d = g->defer) != nil) {\n+\twhile((d = g->_defer) != nil) {\n \t\tvoid (*pfn)(void*);\n \n-\t\tg->defer = d->__next;\n-\t\tpfn = d->__pfn;\n-\t\td->__pfn = nil;\n+\t\tg->_defer = d->next;\n+\t\tpfn = (void (*) (void *))d->pfn;\n+\t\td->pfn = 0;\n \t\tif (pfn != nil)\n-\t\t\t(*pfn)(d->__arg);\n+\t\t\t(*pfn)(d->arg);\n \t\truntime_freedefer(d);\n \t}\n }\n@@ -171,7 +170,7 @@ runtime_canpanic(G *gp)\n \t\treturn false;\n \tif(m->locks-m->softfloat != 0 || m->mallocing != 0 || m->throwing != 0 || m->gcing != 0 || m->dying != 0)\n \t\treturn false;\n-\tif(gp->status != Grunning)\n+\tif(gp->atomicstatus != _Grunning)\n \t\treturn false;\n #ifdef GOOS_windows\n \tif(m->libcallsp != 0)"}, {"sha": "1ac03a42372790b36ec031e0fac2dc91ea428add", "filename": "libgo/runtime/proc.c", "status": "modified", "additions": 374, "deletions": 346, "changes": 720, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fproc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fproc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fproc.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -19,7 +19,6 @@\n #include \"defs.h\"\n #include \"malloc.h\"\n #include \"go-type.h\"\n-#include \"go-defer.h\"\n \n #ifdef USING_SPLIT_STACK\n \n@@ -62,7 +61,6 @@ static void gtraceback(G*);\n #endif\n \n static __thread G *g;\n-static __thread M *m;\n \n #ifndef SETCONTEXT_CLOBBERS_TLS\n \n@@ -179,14 +177,15 @@ M* runtime_m(void) __attribute__ ((noinline, no_split_stack));\n M*\n runtime_m(void)\n {\n-\treturn m;\n+\tif(g == nil)\n+\t\treturn nil;\n+\treturn g->m;\n }\n \n-// Set m and g.\n+// Set g.\n void\n-runtime_setmg(M* mp, G* gp)\n+runtime_setg(G* gp)\n {\n-\tm = mp;\n \tg = gp;\n }\n \n@@ -242,12 +241,12 @@ void\n runtime_gogo(G* newg)\n {\n #ifdef USING_SPLIT_STACK\n-\t__splitstack_setcontext(&newg->stack_context[0]);\n+\t__splitstack_setcontext(&newg->stackcontext[0]);\n #endif\n \tg = newg;\n \tnewg->fromgogo = true;\n-\tfixcontext(&newg->context);\n-\tsetcontext(&newg->context);\n+\tfixcontext((ucontext_t*)&newg->context[0]);\n+\tsetcontext((ucontext_t*)&newg->context[0]);\n \truntime_throw(\"gogo setcontext returned\");\n }\n \n@@ -266,37 +265,37 @@ runtime_mcall(void (*pfn)(G*))\n \t// collector.\n \t__builtin_unwind_init();\n \n-\tmp = m;\n \tgp = g;\n+\tmp = gp->m;\n \tif(gp == mp->g0)\n \t\truntime_throw(\"runtime: mcall called on m->g0 stack\");\n \n \tif(gp != nil) {\n \n #ifdef USING_SPLIT_STACK\n-\t\t__splitstack_getcontext(&g->stack_context[0]);\n+\t\t__splitstack_getcontext(&g->stackcontext[0]);\n #else\n-\t\tgp->gcnext_sp = &pfn;\n+\t\tgp->gcnextsp = &pfn;\n #endif\n \t\tgp->fromgogo = false;\n-\t\tgetcontext(&gp->context);\n+\t\tgetcontext((ucontext_t*)&gp->context[0]);\n \n \t\t// When we return from getcontext, we may be running\n-\t\t// in a new thread.  That means that m and g may have\n-\t\t// changed.  They are global variables so we will\n-\t\t// reload them, but the addresses of m and g may be\n-\t\t// cached in our local stack frame, and those\n-\t\t// addresses may be wrong.  Call functions to reload\n-\t\t// the values for this thread.\n-\t\tmp = runtime_m();\n+\t\t// in a new thread.  That means that g may have\n+\t\t// changed.  It is a global variables so we will\n+\t\t// reload it, but the address of g may be cached in\n+\t\t// our local stack frame, and that address may be\n+\t\t// wrong.  Call the function to reload the value for\n+\t\t// this thread.\n \t\tgp = runtime_g();\n+\t\tmp = gp->m;\n \n \t\tif(gp->traceback != nil)\n \t\t\tgtraceback(gp);\n \t}\n \tif (gp == nil || !gp->fromgogo) {\n #ifdef USING_SPLIT_STACK\n-\t\t__splitstack_setcontext(&mp->g0->stack_context[0]);\n+\t\t__splitstack_setcontext(&mp->g0->stackcontext[0]);\n #endif\n \t\tmp->g0->entry = (byte*)pfn;\n \t\tmp->g0->param = gp;\n@@ -306,8 +305,8 @@ runtime_mcall(void (*pfn)(G*))\n \t\t// the getcontext call just above.\n \t\tg = mp->g0;\n \n-\t\tfixcontext(&mp->g0->context);\n-\t\tsetcontext(&mp->g0->context);\n+\t\tfixcontext((ucontext_t*)&mp->g0->context[0]);\n+\t\tsetcontext((ucontext_t*)&mp->g0->context[0]);\n \t\truntime_throw(\"runtime: mcall function returned\");\n \t}\n }\n@@ -360,10 +359,6 @@ struct Sched {\n \n enum\n {\n-\t// The max value of GOMAXPROCS.\n-\t// There are no fundamental restrictions on the value.\n-\tMaxGomaxprocs = 1<<8,\n-\n \t// Number of goroutine ids to grab from runtime_sched.goidgen to local per-P cache at once.\n \t// 16 seems to provide enough amortization, but other than that it's mostly arbitrary number.\n \tGoidCacheBatch = 16,\n@@ -442,6 +437,7 @@ bool runtime_isstarted;\n void\n runtime_schedinit(void)\n {\n+\tM *m;\n \tint32 n, procs;\n \tString s;\n \tconst byte *p;\n@@ -481,11 +477,11 @@ runtime_schedinit(void)\n \ts = runtime_getenv(\"GOMAXPROCS\");\n \tp = s.str;\n \tif(p != nil && (n = runtime_atoi(p, s.len)) > 0) {\n-\t\tif(n > MaxGomaxprocs)\n-\t\t\tn = MaxGomaxprocs;\n+\t\tif(n > _MaxGomaxprocs)\n+\t\t\tn = _MaxGomaxprocs;\n \t\tprocs = n;\n \t}\n-\truntime_allp = runtime_malloc((MaxGomaxprocs+1)*sizeof(runtime_allp[0]));\n+\truntime_allp = runtime_malloc((_MaxGomaxprocs+1)*sizeof(runtime_allp[0]));\n \tprocresize(procs);\n \n \t// Can not enable GC until all roots are registered.\n@@ -583,17 +579,17 @@ runtime_main(void* dummy __attribute__((unused)))\n \truntime_lockOSThread();\n \t\n \t// Defer unlock so that runtime.Goexit during init does the unlock too.\n-\td.__pfn = initDone;\n-\td.__next = g->defer;\n-\td.__arg = (void*)-1;\n-\td.__panic = g->panic;\n-\td.__retaddr = nil;\n-\td.__makefunc_can_recover = 0;\n-\td.__frame = &frame;\n-\td.__special = true;\n-\tg->defer = &d;\n-\n-\tif(m != &runtime_m0)\n+\td.pfn = (uintptr)(void*)initDone;\n+\td.next = g->_defer;\n+\td.arg = (void*)-1;\n+\td._panic = g->_panic;\n+\td.retaddr = 0;\n+\td.makefunccanrecover = 0;\n+\td.frame = &frame;\n+\td.special = true;\n+\tg->_defer = &d;\n+\n+\tif(g->m != &runtime_m0)\n \t\truntime_throw(\"runtime_main not on m0\");\n \t__go_go(runtime_MHeap_Scavenger, nil);\n \n@@ -605,9 +601,9 @@ runtime_main(void* dummy __attribute__((unused)))\n \n \tclosechan(runtime_main_init_done);\n \n-\tif(g->defer != &d || d.__pfn != initDone)\n+\tif(g->_defer != &d || (void*)d.pfn != initDone)\n \t\truntime_throw(\"runtime: bad defer entry after init\");\n-\tg->defer = d.__next;\n+\tg->_defer = d.next;\n \truntime_unlockOSThread();\n \n \t// For gccgo we have to wait until after main is initialized\n@@ -640,42 +636,42 @@ runtime_main(void* dummy __attribute__((unused)))\n void\n runtime_goroutineheader(G *gp)\n {\n-\tconst char *status;\n+\tString status;\n \tint64 waitfor;\n \n-\tswitch(gp->status) {\n-\tcase Gidle:\n-\t\tstatus = \"idle\";\n+\tswitch(gp->atomicstatus) {\n+\tcase _Gidle:\n+\t\tstatus = runtime_gostringnocopy((const byte*)\"idle\");\n \t\tbreak;\n-\tcase Grunnable:\n-\t\tstatus = \"runnable\";\n+\tcase _Grunnable:\n+\t\tstatus = runtime_gostringnocopy((const byte*)\"runnable\");\n \t\tbreak;\n-\tcase Grunning:\n-\t\tstatus = \"running\";\n+\tcase _Grunning:\n+\t\tstatus = runtime_gostringnocopy((const byte*)\"running\");\n \t\tbreak;\n-\tcase Gsyscall:\n-\t\tstatus = \"syscall\";\n+\tcase _Gsyscall:\n+\t\tstatus = runtime_gostringnocopy((const byte*)\"syscall\");\n \t\tbreak;\n-\tcase Gwaiting:\n-\t\tif(gp->waitreason)\n+\tcase _Gwaiting:\n+\t\tif(gp->waitreason.len > 0)\n \t\t\tstatus = gp->waitreason;\n \t\telse\n-\t\t\tstatus = \"waiting\";\n+\t\t\tstatus = runtime_gostringnocopy((const byte*)\"waiting\");\n \t\tbreak;\n \tdefault:\n-\t\tstatus = \"???\";\n+\t\tstatus = runtime_gostringnocopy((const byte*)\"???\");\n \t\tbreak;\n \t}\n \n \t// approx time the G is blocked, in minutes\n \twaitfor = 0;\n-\tif((gp->status == Gwaiting || gp->status == Gsyscall) && gp->waitsince != 0)\n+\tif((gp->atomicstatus == _Gwaiting || gp->atomicstatus == _Gsyscall) && gp->waitsince != 0)\n \t\twaitfor = (runtime_nanotime() - gp->waitsince) / (60LL*1000*1000*1000);\n \n \tif(waitfor < 1)\n-\t\truntime_printf(\"goroutine %D [%s]:\\n\", gp->goid, status);\n+\t\truntime_printf(\"goroutine %D [%S]:\\n\", gp->goid, status);\n \telse\n-\t\truntime_printf(\"goroutine %D [%s, %D minutes]:\\n\", gp->goid, status, waitfor);\n+\t\truntime_printf(\"goroutine %D [%S, %D minutes]:\\n\", gp->goid, status, waitfor);\n }\n \n void\n@@ -693,13 +689,6 @@ runtime_printcreatedby(G *g)\n \t}\n }\n \n-struct Traceback\n-{\n-\tG* gp;\n-\tLocation locbuf[TracebackMaxFrames];\n-\tint32 c;\n-};\n-\n void\n runtime_tracebackothers(G * volatile me)\n {\n@@ -712,15 +701,15 @@ runtime_tracebackothers(G * volatile me)\n \ttraceback = runtime_gotraceback(nil);\n \t\n \t// Show the current goroutine first, if we haven't already.\n-\tif((gp = m->curg) != nil && gp != me) {\n+\tif((gp = g->m->curg) != nil && gp != me) {\n \t\truntime_printf(\"\\n\");\n \t\truntime_goroutineheader(gp);\n \t\tgp->traceback = &tb;\n \n #ifdef USING_SPLIT_STACK\n-\t\t__splitstack_getcontext(&me->stack_context[0]);\n+\t\t__splitstack_getcontext(&me->stackcontext[0]);\n #endif\n-\t\tgetcontext(&me->context);\n+\t\tgetcontext((ucontext_t*)&me->context[0]);\n \n \t\tif(gp->traceback != nil) {\n \t\t  runtime_gogo(gp);\n@@ -733,7 +722,7 @@ runtime_tracebackothers(G * volatile me)\n \truntime_lock(&allglock);\n \tfor(i = 0; i < runtime_allglen; i++) {\n \t\tgp = runtime_allg[i];\n-\t\tif(gp == me || gp == m->curg || gp->status == Gdead)\n+\t\tif(gp == me || gp == g->m->curg || gp->atomicstatus == _Gdead)\n \t\t\tcontinue;\n \t\tif(gp->issystem && traceback < 2)\n \t\t\tcontinue;\n@@ -749,19 +738,19 @@ runtime_tracebackothers(G * volatile me)\n \t\t// This means that if g is running or in a syscall, we\n \t\t// can't reliably print a stack trace.  FIXME.\n \n-\t\tif(gp->status == Grunning) {\n+\t\tif(gp->atomicstatus == _Grunning) {\n \t\t\truntime_printf(\"\\tgoroutine running on other thread; stack unavailable\\n\");\n \t\t\truntime_printcreatedby(gp);\n-\t\t} else if(gp->status == Gsyscall) {\n+\t\t} else if(gp->atomicstatus == _Gsyscall) {\n \t\t\truntime_printf(\"\\tgoroutine in C code; stack unavailable\\n\");\n \t\t\truntime_printcreatedby(gp);\n \t\t} else {\n \t\t\tgp->traceback = &tb;\n \n #ifdef USING_SPLIT_STACK\n-\t\t\t__splitstack_getcontext(&me->stack_context[0]);\n+\t\t\t__splitstack_getcontext(&me->stackcontext[0]);\n #endif\n-\t\t\tgetcontext(&me->context);\n+\t\t\tgetcontext((ucontext_t*)&me->context[0]);\n \n \t\t\tif(gp->traceback != nil) {\n \t\t\t\truntime_gogo(gp);\n@@ -794,8 +783,12 @@ gtraceback(G* gp)\n \n \ttraceback = gp->traceback;\n \tgp->traceback = nil;\n+\tif(gp->m != nil)\n+\t\truntime_throw(\"gtraceback: m is not nil\");\n+\tgp->m = traceback->gp->m;\n \ttraceback->c = runtime_callers(1, traceback->locbuf,\n \t\tsizeof traceback->locbuf / sizeof traceback->locbuf[0], false);\n+\tgp->m = nil;\n \truntime_gogo(traceback->gp);\n }\n \n@@ -804,7 +797,7 @@ mcommoninit(M *mp)\n {\n \t// If there is no mcache runtime_callers() will crash,\n \t// and we are most likely in sysmon thread so the stack is senseless anyway.\n-\tif(m->mcache)\n+\tif(g->m->mcache)\n \t\truntime_callers(1, mp->createstack, nelem(mp->createstack), false);\n \n \tmp->fastrand = 0x49f6428aUL + mp->id + runtime_cputicks();\n@@ -828,16 +821,16 @@ void\n runtime_ready(G *gp)\n {\n \t// Mark runnable.\n-\tm->locks++;  // disable preemption because it can be holding p in a local var\n-\tif(gp->status != Gwaiting) {\n-\t\truntime_printf(\"goroutine %D has status %d\\n\", gp->goid, gp->status);\n-\t\truntime_throw(\"bad g->status in ready\");\n+\tg->m->locks++;  // disable preemption because it can be holding p in a local var\n+\tif(gp->atomicstatus != _Gwaiting) {\n+\t\truntime_printf(\"goroutine %D has status %d\\n\", gp->goid, gp->atomicstatus);\n+\t\truntime_throw(\"bad g->atomicstatus in ready\");\n \t}\n-\tgp->status = Grunnable;\n-\trunqput(m->p, gp);\n+\tgp->atomicstatus = _Grunnable;\n+\trunqput((P*)g->m->p, gp);\n \tif(runtime_atomicload(&runtime_sched.npidle) != 0 && runtime_atomicload(&runtime_sched.nmspinning) == 0)  // TODO: fast atomic\n \t\twakep();\n-\tm->locks--;\n+\tg->m->locks--;\n }\n \n int32\n@@ -884,7 +877,7 @@ runtime_helpgc(int32 nproc)\n \truntime_lock(&runtime_sched);\n \tpos = 0;\n \tfor(n = 1; n < nproc; n++) {  // one M is currently running\n-\t\tif(runtime_allp[pos]->mcache == m->mcache)\n+\t\tif(runtime_allp[pos]->mcache == g->m->mcache)\n \t\t\tpos++;\n \t\tmp = mget();\n \t\tif(mp == nil)\n@@ -938,18 +931,18 @@ runtime_stoptheworld(void)\n \truntime_atomicstore((uint32*)&runtime_sched.gcwaiting, 1);\n \tpreemptall();\n \t// stop current P\n-\tm->p->status = Pgcstop;\n+\t((P*)g->m->p)->status = _Pgcstop;\n \truntime_sched.stopwait--;\n-\t// try to retake all P's in Psyscall status\n+\t// try to retake all P's in _Psyscall status\n \tfor(i = 0; i < runtime_gomaxprocs; i++) {\n \t\tp = runtime_allp[i];\n \t\ts = p->status;\n-\t\tif(s == Psyscall && runtime_cas(&p->status, s, Pgcstop))\n+\t\tif(s == _Psyscall && runtime_cas(&p->status, s, _Pgcstop))\n \t\t\truntime_sched.stopwait--;\n \t}\n \t// stop idle P's\n \twhile((p = pidleget()) != nil) {\n-\t\tp->status = Pgcstop;\n+\t\tp->status = _Pgcstop;\n \t\truntime_sched.stopwait--;\n \t}\n \twait = runtime_sched.stopwait > 0;\n@@ -964,15 +957,15 @@ runtime_stoptheworld(void)\n \t\truntime_throw(\"stoptheworld: not stopped\");\n \tfor(i = 0; i < runtime_gomaxprocs; i++) {\n \t\tp = runtime_allp[i];\n-\t\tif(p->status != Pgcstop)\n+\t\tif(p->status != _Pgcstop)\n \t\t\truntime_throw(\"stoptheworld: not stopped\");\n \t}\n }\n \n static void\n mhelpgc(void)\n {\n-\tm->helpgc = -1;\n+\tg->m->helpgc = -1;\n }\n \n void\n@@ -983,7 +976,7 @@ runtime_starttheworld(void)\n \tG *gp;\n \tbool add;\n \n-\tm->locks++;  // disable preemption because it can be holding p in a local var\n+\tg->m->locks++;  // disable preemption because it can be holding p in a local var\n \tgp = runtime_netpoll(false);  // non-blocking\n \tinjectglist(gp);\n \tadd = needaddgcproc();\n@@ -1003,8 +996,8 @@ runtime_starttheworld(void)\n \t\t\tpidleput(p);\n \t\t\tbreak;\n \t\t}\n-\t\tp->m = mget();\n-\t\tp->link = p1;\n+\t\tp->m = (uintptr)mget();\n+\t\tp->link = (uintptr)p1;\n \t\tp1 = p;\n \t}\n \tif(runtime_sched.sysmonwait) {\n@@ -1015,13 +1008,13 @@ runtime_starttheworld(void)\n \n \twhile(p1) {\n \t\tp = p1;\n-\t\tp1 = p1->link;\n+\t\tp1 = (P*)p1->link;\n \t\tif(p->m) {\n-\t\t\tmp = p->m;\n-\t\t\tp->m = nil;\n+\t\t\tmp = (M*)p->m;\n+\t\t\tp->m = 0;\n \t\t\tif(mp->nextp)\n \t\t\t\truntime_throw(\"starttheworld: inconsistent mp->nextp\");\n-\t\t\tmp->nextp = p;\n+\t\t\tmp->nextp = (uintptr)p;\n \t\t\truntime_notewakeup(&mp->park);\n \t\t} else {\n \t\t\t// Start M to run P.  Do not start another M below.\n@@ -1040,15 +1033,18 @@ runtime_starttheworld(void)\n \t\t// the maximum number of procs.\n \t\tnewm(mhelpgc, nil);\n \t}\n-\tm->locks--;\n+\tg->m->locks--;\n }\n \n // Called to start an M.\n void*\n runtime_mstart(void* mp)\n {\n+\tM *m;\n+\n \tm = (M*)mp;\n \tg = m->g0;\n+\tg->m = m;\n \n \tinitcontext();\n \n@@ -1059,15 +1055,15 @@ runtime_mstart(void* mp)\n \t// Once we call schedule we're never coming back,\n \t// so other calls can reuse this stack space.\n #ifdef USING_SPLIT_STACK\n-\t__splitstack_getcontext(&g->stack_context[0]);\n+\t__splitstack_getcontext(&g->stackcontext[0]);\n #else\n-\tg->gcinitial_sp = &mp;\n-\t// Setting gcstack_size to 0 is a marker meaning that gcinitial_sp\n+\tg->gcinitialsp = &mp;\n+\t// Setting gcstacksize to 0 is a marker meaning that gcinitialsp\n \t// is the top of the stack, not the bottom.\n-\tg->gcstack_size = 0;\n-\tg->gcnext_sp = &mp;\n+\tg->gcstacksize = 0;\n+\tg->gcnextsp = &mp;\n #endif\n-\tgetcontext(&g->context);\n+\tgetcontext((ucontext_t*)&g->context[0]);\n \n \tif(g->entry != nil) {\n \t\t// Got here from mcall.\n@@ -1097,14 +1093,14 @@ runtime_mstart(void* mp)\n \t}\n \t\n \tif(m->mstartfn)\n-\t\tm->mstartfn();\n+\t\t((void (*)(void))m->mstartfn)();\n \n \tif(m->helpgc) {\n \t\tm->helpgc = 0;\n \t\tstopm();\n \t} else if(m != &runtime_m0) {\n-\t\tacquirep(m->nextp);\n-\t\tm->nextp = nil;\n+\t\tacquirep((P*)m->nextp);\n+\t\tm->nextp = 0;\n \t}\n \tschedule();\n \n@@ -1127,12 +1123,12 @@ struct CgoThreadStart\n // Allocate a new m unassociated with any thread.\n // Can use p for allocation context if needed.\n M*\n-runtime_allocm(P *p, int32 stacksize, byte** ret_g0_stack, size_t* ret_g0_stacksize)\n+runtime_allocm(P *p, int32 stacksize, byte** ret_g0_stack, uintptr* ret_g0_stacksize)\n {\n \tM *mp;\n \n-\tm->locks++;  // disable GC because it can be called from sysmon\n-\tif(m->p == nil)\n+\tg->m->locks++;  // disable GC because it can be called from sysmon\n+\tif(g->m->p == 0)\n \t\tacquirep(p);  // temporarily borrow p for mallocs in this function\n #if 0\n \tif(mtype == nil) {\n@@ -1145,10 +1141,11 @@ runtime_allocm(P *p, int32 stacksize, byte** ret_g0_stack, size_t* ret_g0_stacks\n \tmp = runtime_mal(sizeof *mp);\n \tmcommoninit(mp);\n \tmp->g0 = runtime_malg(stacksize, ret_g0_stack, ret_g0_stacksize);\n+\tmp->g0->m = mp;\n \n-\tif(p == m->p)\n+\tif(p == (P*)g->m->p)\n \t\treleasep();\n-\tm->locks--;\n+\tg->m->locks--;\n \n \treturn mp;\n }\n@@ -1235,26 +1232,26 @@ runtime_needm(void)\n \t// after exitsyscall makes sure it is okay to be\n \t// running at all (that is, there's no garbage collection\n \t// running right now).\n-\tmp->needextram = mp->schedlink == nil;\n-\tunlockextra(mp->schedlink);\n+\tmp->needextram = mp->schedlink == 0;\n+\tunlockextra((M*)mp->schedlink);\n \n-\t// Install m and g (= m->curg).\n-\truntime_setmg(mp, mp->curg);\n+\t// Install g (= m->curg).\n+\truntime_setg(mp->curg);\n \n \t// Initialize g's context as in mstart.\n \tinitcontext();\n-\tg->status = Gsyscall;\n+\tg->atomicstatus = _Gsyscall;\n \tg->entry = nil;\n \tg->param = nil;\n #ifdef USING_SPLIT_STACK\n-\t__splitstack_getcontext(&g->stack_context[0]);\n+\t__splitstack_getcontext(&g->stackcontext[0]);\n #else\n-\tg->gcinitial_sp = &mp;\n+\tg->gcinitialsp = &mp;\n \tg->gcstack = nil;\n-\tg->gcstack_size = 0;\n-\tg->gcnext_sp = &mp;\n+\tg->gcstacksize = 0;\n+\tg->gcnextsp = &mp;\n #endif\n-\tgetcontext(&g->context);\n+\tgetcontext((ucontext_t*)&g->context[0]);\n \n \tif(g->entry != nil) {\n \t\t// Got here from mcall.\n@@ -1284,7 +1281,7 @@ runtime_newextram(void)\n \tM *mp, *mnext;\n \tG *gp;\n \tbyte *g0_sp, *sp;\n-\tsize_t g0_spsize, spsize;\n+\tuintptr g0_spsize, spsize;\n \n \t// Create extra goroutine locked to extra m.\n \t// The goroutine is the context in which the cgo callback will run.\n@@ -1293,9 +1290,10 @@ runtime_newextram(void)\n \t// the goroutine stack ends.\n \tmp = runtime_allocm(nil, StackMin, &g0_sp, &g0_spsize);\n \tgp = runtime_malg(StackMin, &sp, &spsize);\n-\tgp->status = Gdead;\n+\tgp->atomicstatus = _Gdead;\n+\tgp->m = mp;\n \tmp->curg = gp;\n-\tmp->locked = LockInternal;\n+\tmp->locked = _LockInternal;\n \tmp->lockedg = gp;\n \tgp->lockedm = mp;\n \tgp->goid = runtime_xadd64(&runtime_sched.goidgen, 1);\n@@ -1304,14 +1302,14 @@ runtime_newextram(void)\n \n \t// The context for gp will be set up in runtime_needm.  But\n \t// here we need to set up the context for g0.\n-\tgetcontext(&mp->g0->context);\n-\tmp->g0->context.uc_stack.ss_sp = g0_sp;\n-\tmp->g0->context.uc_stack.ss_size = g0_spsize;\n-\tmakecontext(&mp->g0->context, kickoff, 0);\n+\tgetcontext((ucontext_t*)&mp->g0->context[0]);\n+\t((ucontext_t*)&mp->g0->context[0])->uc_stack.ss_sp = g0_sp;\n+\t((ucontext_t*)&mp->g0->context[0])->uc_stack.ss_size = (size_t)g0_spsize;\n+\tmakecontext((ucontext_t*)&mp->g0->context[0], kickoff, 0);\n \n \t// Add m to the extra list.\n \tmnext = lockextra(true);\n-\tmp->schedlink = mnext;\n+\tmp->schedlink = (uintptr)mnext;\n \tunlockextra(mp);\n }\n \n@@ -1347,16 +1345,16 @@ runtime_dropm(void)\n \truntime_unminit();\n \n \t// Clear m and g, and return m to the extra list.\n-\t// After the call to setmg we can only call nosplit functions.\n-\tmp = m;\n-\truntime_setmg(nil, nil);\n+\t// After the call to setg we can only call nosplit functions.\n+\tmp = g->m;\n+\truntime_setg(nil);\n \n-\tmp->curg->status = Gdead;\n+\tmp->curg->atomicstatus = _Gdead;\n \tmp->curg->gcstack = nil;\n-\tmp->curg->gcnext_sp = nil;\n+\tmp->curg->gcnextsp = nil;\n \n \tmnext = lockextra(true);\n-\tmp->schedlink = mnext;\n+\tmp->schedlink = (uintptr)mnext;\n \tunlockextra(mp);\n }\n \n@@ -1417,7 +1415,7 @@ countextra()\n \t\t\tcontinue;\n \t\t}\n \t\tc = 0;\n-\t\tfor(mc = mp; mc != nil; mc = mc->schedlink)\n+\t\tfor(mc = mp; mc != nil; mc = (M*)mc->schedlink)\n \t\t\tc++;\n \t\truntime_atomicstorep(&runtime_extram, mp);\n \t\treturn c;\n@@ -1431,8 +1429,8 @@ newm(void(*fn)(void), P *p)\n \tM *mp;\n \n \tmp = runtime_allocm(p, -1, nil, nil);\n-\tmp->nextp = p;\n-\tmp->mstartfn = fn;\n+\tmp->nextp = (uintptr)p;\n+\tmp->mstartfn = (uintptr)(void*)fn;\n \n \truntime_newosproc(mp);\n }\n@@ -1442,6 +1440,9 @@ newm(void(*fn)(void), P *p)\n static void\n stopm(void)\n {\n+\tM* m;\n+\n+\tm = g->m;\n \tif(m->locks)\n \t\truntime_throw(\"stopm holding locks\");\n \tif(m->p)\n@@ -1456,21 +1457,22 @@ stopm(void)\n \tmput(m);\n \truntime_unlock(&runtime_sched);\n \truntime_notesleep(&m->park);\n+\tm = g->m;\n \truntime_noteclear(&m->park);\n \tif(m->helpgc) {\n \t\truntime_gchelper();\n \t\tm->helpgc = 0;\n \t\tm->mcache = nil;\n \t\tgoto retry;\n \t}\n-\tacquirep(m->nextp);\n-\tm->nextp = nil;\n+\tacquirep((P*)m->nextp);\n+\tm->nextp = 0;\n }\n \n static void\n mspinning(void)\n {\n-\tm->spinning = true;\n+\tg->m->spinning = true;\n }\n \n // Schedules some M to run the p (creates an M if necessary).\n@@ -1505,7 +1507,7 @@ startm(P *p, bool spinning)\n \tif(mp->nextp)\n \t\truntime_throw(\"startm: m has p\");\n \tmp->spinning = spinning;\n-\tmp->nextp = p;\n+\tmp->nextp = (uintptr)p;\n \truntime_notewakeup(&mp->park);\n }\n \n@@ -1527,7 +1529,7 @@ handoffp(P *p)\n \t}\n \truntime_lock(&runtime_sched);\n \tif(runtime_sched.gcwaiting) {\n-\t\tp->status = Pgcstop;\n+\t\tp->status = _Pgcstop;\n \t\tif(--runtime_sched.stopwait == 0)\n \t\t\truntime_notewakeup(&runtime_sched.stopnote);\n \t\truntime_unlock(&runtime_sched);\n@@ -1565,8 +1567,10 @@ wakep(void)\n static void\n stoplockedm(void)\n {\n+\tM *m;\n \tP *p;\n \n+\tm = g->m;\n \tif(m->lockedg == nil || m->lockedg->lockedm != m)\n \t\truntime_throw(\"stoplockedm: inconsistent locking\");\n \tif(m->p) {\n@@ -1577,11 +1581,12 @@ stoplockedm(void)\n \tincidlelocked(1);\n \t// Wait until another thread schedules lockedg again.\n \truntime_notesleep(&m->park);\n+\tm = g->m;\n \truntime_noteclear(&m->park);\n-\tif(m->lockedg->status != Grunnable)\n+\tif(m->lockedg->atomicstatus != _Grunnable)\n \t\truntime_throw(\"stoplockedm: not runnable\");\n-\tacquirep(m->nextp);\n-\tm->nextp = nil;\n+\tacquirep((P*)m->nextp);\n+\tm->nextp = 0;\n }\n \n // Schedules the locked m to run the locked gp.\n@@ -1592,14 +1597,14 @@ startlockedm(G *gp)\n \tP *p;\n \n \tmp = gp->lockedm;\n-\tif(mp == m)\n+\tif(mp == g->m)\n \t\truntime_throw(\"startlockedm: locked to me\");\n \tif(mp->nextp)\n \t\truntime_throw(\"startlockedm: m has p\");\n \t// directly handoff current P to the locked m\n \tincidlelocked(-1);\n \tp = releasep();\n-\tmp->nextp = p;\n+\tmp->nextp = (uintptr)p;\n \truntime_notewakeup(&mp->park);\n \tstopm();\n }\n@@ -1613,13 +1618,13 @@ gcstopm(void)\n \n \tif(!runtime_sched.gcwaiting)\n \t\truntime_throw(\"gcstopm: not waiting for gc\");\n-\tif(m->spinning) {\n-\t\tm->spinning = false;\n+\tif(g->m->spinning) {\n+\t\tg->m->spinning = false;\n \t\truntime_xadd(&runtime_sched.nmspinning, -1);\n \t}\n \tp = releasep();\n \truntime_lock(&runtime_sched);\n-\tp->status = Pgcstop;\n+\tp->status = _Pgcstop;\n \tif(--runtime_sched.stopwait == 0)\n \t\truntime_notewakeup(&runtime_sched.stopnote);\n \truntime_unlock(&runtime_sched);\n@@ -1633,19 +1638,19 @@ execute(G *gp)\n {\n \tint32 hz;\n \n-\tif(gp->status != Grunnable) {\n-\t\truntime_printf(\"execute: bad g status %d\\n\", gp->status);\n+\tif(gp->atomicstatus != _Grunnable) {\n+\t\truntime_printf(\"execute: bad g status %d\\n\", gp->atomicstatus);\n \t\truntime_throw(\"execute: bad g status\");\n \t}\n-\tgp->status = Grunning;\n+\tgp->atomicstatus = _Grunning;\n \tgp->waitsince = 0;\n-\tm->p->schedtick++;\n-\tm->curg = gp;\n-\tgp->m = m;\n+\t((P*)g->m->p)->schedtick++;\n+\tg->m->curg = gp;\n+\tgp->m = g->m;\n \n \t// Check whether the profiler needs to be turned on or off.\n \thz = runtime_sched.profilehz;\n-\tif(m->profilehz != hz)\n+\tif(g->m->profilehz != hz)\n \t\truntime_resetcpuprofiler(hz);\n \n \truntime_gogo(gp);\n@@ -1668,42 +1673,42 @@ findrunnable(void)\n \tif(runtime_fingwait && runtime_fingwake && (gp = runtime_wakefing()) != nil)\n \t\truntime_ready(gp);\n \t// local runq\n-\tgp = runqget(m->p);\n+\tgp = runqget((P*)g->m->p);\n \tif(gp)\n \t\treturn gp;\n \t// global runq\n \tif(runtime_sched.runqsize) {\n \t\truntime_lock(&runtime_sched);\n-\t\tgp = globrunqget(m->p, 0);\n+\t\tgp = globrunqget((P*)g->m->p, 0);\n \t\truntime_unlock(&runtime_sched);\n \t\tif(gp)\n \t\t\treturn gp;\n \t}\n \t// poll network\n \tgp = runtime_netpoll(false);  // non-blocking\n \tif(gp) {\n-\t\tinjectglist(gp->schedlink);\n-\t\tgp->status = Grunnable;\n+\t\tinjectglist((G*)gp->schedlink);\n+\t\tgp->atomicstatus = _Grunnable;\n \t\treturn gp;\n \t}\n \t// If number of spinning M's >= number of busy P's, block.\n \t// This is necessary to prevent excessive CPU consumption\n \t// when GOMAXPROCS>>1 but the program parallelism is low.\n-\tif(!m->spinning && 2 * runtime_atomicload(&runtime_sched.nmspinning) >= runtime_gomaxprocs - runtime_atomicload(&runtime_sched.npidle))  // TODO: fast atomic\n+\tif(!g->m->spinning && 2 * runtime_atomicload(&runtime_sched.nmspinning) >= runtime_gomaxprocs - runtime_atomicload(&runtime_sched.npidle))  // TODO: fast atomic\n \t\tgoto stop;\n-\tif(!m->spinning) {\n-\t\tm->spinning = true;\n+\tif(!g->m->spinning) {\n+\t\tg->m->spinning = true;\n \t\truntime_xadd(&runtime_sched.nmspinning, 1);\n \t}\n \t// random steal from other P's\n \tfor(i = 0; i < 2*runtime_gomaxprocs; i++) {\n \t\tif(runtime_sched.gcwaiting)\n \t\t\tgoto top;\n \t\tp = runtime_allp[runtime_fastrand1()%runtime_gomaxprocs];\n-\t\tif(p == m->p)\n+\t\tif(p == (P*)g->m->p)\n \t\t\tgp = runqget(p);\n \t\telse\n-\t\t\tgp = runqsteal(m->p, p);\n+\t\t\tgp = runqsteal((P*)g->m->p, p);\n \t\tif(gp)\n \t\t\treturn gp;\n \t}\n@@ -1715,15 +1720,15 @@ findrunnable(void)\n \t\tgoto top;\n \t}\n \tif(runtime_sched.runqsize) {\n-\t\tgp = globrunqget(m->p, 0);\n+\t\tgp = globrunqget((P*)g->m->p, 0);\n \t\truntime_unlock(&runtime_sched);\n \t\treturn gp;\n \t}\n \tp = releasep();\n \tpidleput(p);\n \truntime_unlock(&runtime_sched);\n-\tif(m->spinning) {\n-\t\tm->spinning = false;\n+\tif(g->m->spinning) {\n+\t\tg->m->spinning = false;\n \t\truntime_xadd(&runtime_sched.nmspinning, -1);\n \t}\n \t// check all runqueues once again\n@@ -1742,9 +1747,9 @@ findrunnable(void)\n \t}\n \t// poll network\n \tif(runtime_xchg64(&runtime_sched.lastpoll, 0) != 0) {\n-\t\tif(m->p)\n+\t\tif(g->m->p)\n \t\t\truntime_throw(\"findrunnable: netpoll with p\");\n-\t\tif(m->spinning)\n+\t\tif(g->m->spinning)\n \t\t\truntime_throw(\"findrunnable: netpoll with spinning\");\n \t\tgp = runtime_netpoll(true);  // block until new work is available\n \t\truntime_atomicstore64(&runtime_sched.lastpoll, runtime_nanotime());\n@@ -1754,8 +1759,8 @@ findrunnable(void)\n \t\t\truntime_unlock(&runtime_sched);\n \t\t\tif(p) {\n \t\t\t\tacquirep(p);\n-\t\t\t\tinjectglist(gp->schedlink);\n-\t\t\t\tgp->status = Grunnable;\n+\t\t\t\tinjectglist((G*)gp->schedlink);\n+\t\t\t\tgp->atomicstatus = _Grunnable;\n \t\t\t\treturn gp;\n \t\t\t}\n \t\t\tinjectglist(gp);\n@@ -1770,8 +1775,8 @@ resetspinning(void)\n {\n \tint32 nmspinning;\n \n-\tif(m->spinning) {\n-\t\tm->spinning = false;\n+\tif(g->m->spinning) {\n+\t\tg->m->spinning = false;\n \t\tnmspinning = runtime_xadd(&runtime_sched.nmspinning, -1);\n \t\tif(nmspinning < 0)\n \t\t\truntime_throw(\"findrunnable: negative nmspinning\");\n@@ -1797,8 +1802,8 @@ injectglist(G *glist)\n \truntime_lock(&runtime_sched);\n \tfor(n = 0; glist; n++) {\n \t\tgp = glist;\n-\t\tglist = gp->schedlink;\n-\t\tgp->status = Grunnable;\n+\t\tglist = (G*)gp->schedlink;\n+\t\tgp->atomicstatus = _Grunnable;\n \t\tglobrunqput(gp);\n \t}\n \truntime_unlock(&runtime_sched);\n@@ -1815,7 +1820,7 @@ schedule(void)\n \tG *gp;\n \tuint32 tick;\n \n-\tif(m->locks)\n+\tif(g->m->locks)\n \t\truntime_throw(\"schedule: holding locks\");\n \n top:\n@@ -1828,19 +1833,19 @@ schedule(void)\n \t// Check the global runnable queue once in a while to ensure fairness.\n \t// Otherwise two goroutines can completely occupy the local runqueue\n \t// by constantly respawning each other.\n-\ttick = m->p->schedtick;\n+\ttick = ((P*)g->m->p)->schedtick;\n \t// This is a fancy way to say tick%61==0,\n \t// it uses 2 MUL instructions instead of a single DIV and so is faster on modern processors.\n \tif(tick - (((uint64)tick*0x4325c53fu)>>36)*61 == 0 && runtime_sched.runqsize > 0) {\n \t\truntime_lock(&runtime_sched);\n-\t\tgp = globrunqget(m->p, 1);\n+\t\tgp = globrunqget((P*)g->m->p, 1);\n \t\truntime_unlock(&runtime_sched);\n \t\tif(gp)\n \t\t\tresetspinning();\n \t}\n \tif(gp == nil) {\n-\t\tgp = runqget(m->p);\n-\t\tif(gp && m->spinning)\n+\t\tgp = runqget((P*)g->m->p);\n+\t\tif(gp && g->m->spinning)\n \t\t\truntime_throw(\"schedule: spinning with local work\");\n \t}\n \tif(gp == nil) {\n@@ -1863,11 +1868,11 @@ schedule(void)\n void\n runtime_park(bool(*unlockf)(G*, void*), void *lock, const char *reason)\n {\n-\tif(g->status != Grunning)\n+\tif(g->atomicstatus != _Grunning)\n \t\truntime_throw(\"bad g status\");\n-\tm->waitlock = lock;\n-\tm->waitunlockf = unlockf;\n-\tg->waitreason = reason;\n+\tg->m->waitlock = lock;\n+\tg->m->waitunlockf = unlockf;\n+\tg->waitreason = runtime_gostringnocopy((const byte*)reason);\n \truntime_mcall(park0);\n }\n \n@@ -1891,17 +1896,19 @@ runtime_parkunlock(Lock *lock, const char *reason)\n static void\n park0(G *gp)\n {\n+\tM *m;\n \tbool ok;\n \n-\tgp->status = Gwaiting;\n+\tm = g->m;\n+\tgp->atomicstatus = _Gwaiting;\n \tgp->m = nil;\n \tm->curg = nil;\n \tif(m->waitunlockf) {\n-\t\tok = m->waitunlockf(gp, m->waitlock);\n+\t\tok = ((bool (*)(G*, void*))m->waitunlockf)(gp, m->waitlock);\n \t\tm->waitunlockf = nil;\n \t\tm->waitlock = nil;\n \t\tif(!ok) {\n-\t\t\tgp->status = Grunnable;\n+\t\t\tgp->atomicstatus = _Grunnable;\n \t\t\texecute(gp);  // Schedule it back, never returns.\n \t\t}\n \t}\n@@ -1916,7 +1923,7 @@ park0(G *gp)\n void\n runtime_gosched(void)\n {\n-\tif(g->status != Grunning)\n+\tif(g->atomicstatus != _Grunning)\n \t\truntime_throw(\"bad g status\");\n \truntime_mcall(runtime_gosched0);\n }\n@@ -1925,7 +1932,10 @@ runtime_gosched(void)\n void\n runtime_gosched0(G *gp)\n {\n-\tgp->status = Grunnable;\n+\tM *m;\n+\n+\tm = g->m;\n+\tgp->atomicstatus = _Grunnable;\n \tgp->m = nil;\n \tm->curg = nil;\n \truntime_lock(&runtime_sched);\n@@ -1946,7 +1956,7 @@ void runtime_goexit(void) __attribute__ ((noinline));\n void\n runtime_goexit(void)\n {\n-\tif(g->status != Grunning)\n+\tif(g->atomicstatus != _Grunning)\n \t\truntime_throw(\"bad g status\");\n \truntime_mcall(goexit0);\n }\n@@ -1955,25 +1965,28 @@ runtime_goexit(void)\n static void\n goexit0(G *gp)\n {\n-\tgp->status = Gdead;\n+\tM *m;\n+\n+\tm = g->m;\n+\tgp->atomicstatus = _Gdead;\n \tgp->entry = nil;\n \tgp->m = nil;\n \tgp->lockedm = nil;\n \tgp->paniconfault = 0;\n-\tgp->defer = nil; // should be true already but just in case.\n-\tgp->panic = nil; // non-nil for Goexit during panic. points at stack-allocated data.\n+\tgp->_defer = nil; // should be true already but just in case.\n+\tgp->_panic = nil; // non-nil for Goexit during panic. points at stack-allocated data.\n \tgp->writenbuf = 0;\n \tgp->writebuf = nil;\n-\tgp->waitreason = nil;\n+\tgp->waitreason = runtime_gostringnocopy(nil);\n \tgp->param = nil;\n \tm->curg = nil;\n \tm->lockedg = nil;\n-\tif(m->locked & ~LockExternal) {\n+\tif(m->locked & ~_LockExternal) {\n \t\truntime_printf(\"invalid m->locked = %d\\n\", m->locked);\n \t\truntime_throw(\"internal lockOSThread error\");\n \t}\t\n \tm->locked = 0;\n-\tgfput(m->p, gp);\n+\tgfput((P*)m->p, gp);\n \tschedule();\n }\n \n@@ -1994,7 +2007,7 @@ runtime_entersyscall()\n {\n \t// Save the registers in the g structure so that any pointers\n \t// held in registers will be seen by the garbage collector.\n-\tgetcontext(&g->gcregs);\n+\tgetcontext((ucontext_t*)&g->gcregs[0]);\n \n \t// Do the work in a separate function, so that this function\n \t// doesn't save any registers on its own stack.  If this\n@@ -2011,24 +2024,24 @@ runtime_entersyscall()\n static void\n doentersyscall()\n {\n-\t// Disable preemption because during this function g is in Gsyscall status,\n+\t// Disable preemption because during this function g is in _Gsyscall status,\n \t// but can have inconsistent g->sched, do not let GC observe it.\n-\tm->locks++;\n+\tg->m->locks++;\n \n \t// Leave SP around for GC and traceback.\n #ifdef USING_SPLIT_STACK\n-\tg->gcstack = __splitstack_find(nil, nil, &g->gcstack_size,\n-\t\t\t\t       &g->gcnext_segment, &g->gcnext_sp,\n-\t\t\t\t       &g->gcinitial_sp);\n+\tg->gcstack = __splitstack_find(nil, nil, &g->gcstacksize,\n+\t\t\t\t       &g->gcnextsegment, &g->gcnextsp,\n+\t\t\t\t       &g->gcinitialsp);\n #else\n \t{\n \t\tvoid *v;\n \n-\t\tg->gcnext_sp = (byte *) &v;\n+\t\tg->gcnextsp = (byte *) &v;\n \t}\n #endif\n \n-\tg->status = Gsyscall;\n+\tg->atomicstatus = _Gsyscall;\n \n \tif(runtime_atomicload(&runtime_sched.sysmonwait)) {  // TODO: fast atomic\n \t\truntime_lock(&runtime_sched);\n@@ -2039,19 +2052,19 @@ doentersyscall()\n \t\truntime_unlock(&runtime_sched);\n \t}\n \n-\tm->mcache = nil;\n-\tm->p->m = nil;\n-\truntime_atomicstore(&m->p->status, Psyscall);\n+\tg->m->mcache = nil;\n+\t((P*)(g->m->p))->m = 0;\n+\truntime_atomicstore(&((P*)g->m->p)->status, _Psyscall);\n \tif(runtime_atomicload(&runtime_sched.gcwaiting)) {\n \t\truntime_lock(&runtime_sched);\n-\t\tif (runtime_sched.stopwait > 0 && runtime_cas(&m->p->status, Psyscall, Pgcstop)) {\n+\t\tif (runtime_sched.stopwait > 0 && runtime_cas(&((P*)g->m->p)->status, _Psyscall, _Pgcstop)) {\n \t\t\tif(--runtime_sched.stopwait == 0)\n \t\t\t\truntime_notewakeup(&runtime_sched.stopnote);\n \t\t}\n \t\truntime_unlock(&runtime_sched);\n \t}\n \n-\tm->locks--;\n+\tg->m->locks--;\n }\n \n // The same as runtime_entersyscall(), but with a hint that the syscall is blocking.\n@@ -2060,29 +2073,29 @@ runtime_entersyscallblock(void)\n {\n \tP *p;\n \n-\tm->locks++;  // see comment in entersyscall\n+\tg->m->locks++;  // see comment in entersyscall\n \n \t// Leave SP around for GC and traceback.\n #ifdef USING_SPLIT_STACK\n-\tg->gcstack = __splitstack_find(nil, nil, &g->gcstack_size,\n-\t\t\t\t       &g->gcnext_segment, &g->gcnext_sp,\n-\t\t\t\t       &g->gcinitial_sp);\n+\tg->gcstack = __splitstack_find(nil, nil, &g->gcstacksize,\n+\t\t\t\t       &g->gcnextsegment, &g->gcnextsp,\n+\t\t\t\t       &g->gcinitialsp);\n #else\n-\tg->gcnext_sp = (byte *) &p;\n+\tg->gcnextsp = (byte *) &p;\n #endif\n \n \t// Save the registers in the g structure so that any pointers\n \t// held in registers will be seen by the garbage collector.\n-\tgetcontext(&g->gcregs);\n+\tgetcontext((ucontext_t*)&g->gcregs[0]);\n \n-\tg->status = Gsyscall;\n+\tg->atomicstatus = _Gsyscall;\n \n \tp = releasep();\n \thandoffp(p);\n \tif(g->isbackground)  // do not consider blocked scavenger for deadlock detection\n \t\tincidlelocked(1);\n \n-\tm->locks--;\n+\tg->m->locks--;\n }\n \n // The goroutine g exited its system call.\n@@ -2094,29 +2107,29 @@ runtime_exitsyscall(void)\n {\n \tG *gp;\n \n-\tm->locks++;  // see comment in entersyscall\n-\n \tgp = g;\n+\tgp->m->locks++;  // see comment in entersyscall\n+\n \tif(gp->isbackground)  // do not consider blocked scavenger for deadlock detection\n \t\tincidlelocked(-1);\n \n-\tg->waitsince = 0;\n+\tgp->waitsince = 0;\n \tif(exitsyscallfast()) {\n \t\t// There's a cpu for us, so we can run.\n-\t\tm->p->syscalltick++;\n-\t\tgp->status = Grunning;\n+\t\t((P*)gp->m->p)->syscalltick++;\n+\t\tgp->atomicstatus = _Grunning;\n \t\t// Garbage collector isn't running (since we are),\n \t\t// so okay to clear gcstack and gcsp.\n #ifdef USING_SPLIT_STACK\n \t\tgp->gcstack = nil;\n #endif\n-\t\tgp->gcnext_sp = nil;\n-\t\truntime_memclr(&gp->gcregs, sizeof gp->gcregs);\n-\t\tm->locks--;\n+\t\tgp->gcnextsp = nil;\n+\t\truntime_memclr(&gp->gcregs[0], sizeof gp->gcregs);\n+\t\tgp->m->locks--;\n \t\treturn;\n \t}\n \n-\tm->locks--;\n+\tgp->m->locks--;\n \n \t// Call the scheduler.\n \truntime_mcall(exitsyscall0);\n@@ -2130,34 +2143,37 @@ runtime_exitsyscall(void)\n #ifdef USING_SPLIT_STACK\n \tgp->gcstack = nil;\n #endif\n-\tgp->gcnext_sp = nil;\n-\truntime_memclr(&gp->gcregs, sizeof gp->gcregs);\n+\tgp->gcnextsp = nil;\n+\truntime_memclr(&gp->gcregs[0], sizeof gp->gcregs);\n \n-\t// Don't refer to m again, we might be running on a different\n-\t// thread after returning from runtime_mcall.\n-\truntime_m()->p->syscalltick++;\n+\t// Note that this gp->m might be different than the earlier\n+\t// gp->m after returning from runtime_mcall.\n+\t((P*)gp->m->p)->syscalltick++;\n }\n \n static bool\n exitsyscallfast(void)\n {\n+\tG *gp;\n \tP *p;\n \n+\tgp = g;\n+\n \t// Freezetheworld sets stopwait but does not retake P's.\n \tif(runtime_sched.stopwait) {\n-\t\tm->p = nil;\n+\t\tgp->m->p = 0;\n \t\treturn false;\n \t}\n \n \t// Try to re-acquire the last P.\n-\tif(m->p && m->p->status == Psyscall && runtime_cas(&m->p->status, Psyscall, Prunning)) {\n+\tif(gp->m->p && ((P*)gp->m->p)->status == _Psyscall && runtime_cas(&((P*)gp->m->p)->status, _Psyscall, _Prunning)) {\n \t\t// There's a cpu for us, so we can run.\n-\t\tm->mcache = m->p->mcache;\n-\t\tm->p->m = m;\n+\t\tgp->m->mcache = ((P*)gp->m->p)->mcache;\n+\t\t((P*)gp->m->p)->m = (uintptr)gp->m;\n \t\treturn true;\n \t}\n \t// Try to get any other idle P.\n-\tm->p = nil;\n+\tgp->m->p = 0;\n \tif(runtime_sched.pidle) {\n \t\truntime_lock(&runtime_sched);\n \t\tp = pidleget();\n@@ -2179,9 +2195,11 @@ exitsyscallfast(void)\n static void\n exitsyscall0(G *gp)\n {\n+\tM *m;\n \tP *p;\n \n-\tgp->status = Grunnable;\n+\tm = g->m;\n+\tgp->atomicstatus = _Grunnable;\n \tgp->m = nil;\n \tm->curg = nil;\n \truntime_lock(&runtime_sched);\n@@ -2235,19 +2253,21 @@ syscall_runtime_AfterFork(void)\n \n // Allocate a new g, with a stack big enough for stacksize bytes.\n G*\n-runtime_malg(int32 stacksize, byte** ret_stack, size_t* ret_stacksize)\n+runtime_malg(int32 stacksize, byte** ret_stack, uintptr* ret_stacksize)\n {\n \tG *newg;\n \n \tnewg = allocg();\n \tif(stacksize >= 0) {\n #if USING_SPLIT_STACK\n \t\tint dont_block_signals = 0;\n+\t\tsize_t ss_stacksize;\n \n \t\t*ret_stack = __splitstack_makecontext(stacksize,\n-\t\t\t\t\t\t      &newg->stack_context[0],\n-\t\t\t\t\t\t      ret_stacksize);\n-\t\t__splitstack_block_signals_context(&newg->stack_context[0],\n+\t\t\t\t\t\t      &newg->stackcontext[0],\n+\t\t\t\t\t\t      &ss_stacksize);\n+\t\t*ret_stacksize = (uintptr)ss_stacksize;\n+\t\t__splitstack_block_signals_context(&newg->stackcontext[0],\n \t\t\t\t\t\t   &dont_block_signals, nil);\n #else\n                 // In 64-bit mode, the maximum Go allocation space is\n@@ -2265,9 +2285,9 @@ runtime_malg(int32 stacksize, byte** ret_stack, size_t* ret_stacksize)\n \t\t\t*ret_stack = runtime_mallocgc(stacksize, 0, FlagNoProfiling|FlagNoGC);\n \t\t\truntime_xadd(&runtime_stacks_sys, stacksize);\n \t\t}\n-\t\t*ret_stacksize = stacksize;\n-\t\tnewg->gcinitial_sp = *ret_stack;\n-\t\tnewg->gcstack_size = (size_t)stacksize;\n+\t\t*ret_stacksize = (uintptr)stacksize;\n+\t\tnewg->gcinitialsp = *ret_stack;\n+\t\tnewg->gcstacksize = (uintptr)stacksize;\n #endif\n \t}\n \treturn newg;\n@@ -2310,36 +2330,39 @@ __go_go(void (*fn)(void*), void* arg)\n \n //runtime_printf(\"newproc1 %p %p narg=%d nret=%d\\n\", fn->fn, argp, narg, nret);\n \tif(fn == nil) {\n-\t\tm->throwing = -1;  // do not dump full stacks\n+\t\tg->m->throwing = -1;  // do not dump full stacks\n \t\truntime_throw(\"go of nil func value\");\n \t}\n-\tm->locks++;  // disable preemption because it can be holding p in a local var\n+\tg->m->locks++;  // disable preemption because it can be holding p in a local var\n \n-\tp = m->p;\n+\tp = (P*)g->m->p;\n \tif((newg = gfget(p)) != nil) {\n #ifdef USING_SPLIT_STACK\n \t\tint dont_block_signals = 0;\n \n-\t\tsp = __splitstack_resetcontext(&newg->stack_context[0],\n+\t\tsp = __splitstack_resetcontext(&newg->stackcontext[0],\n \t\t\t\t\t       &spsize);\n-\t\t__splitstack_block_signals_context(&newg->stack_context[0],\n+\t\t__splitstack_block_signals_context(&newg->stackcontext[0],\n \t\t\t\t\t\t   &dont_block_signals, nil);\n #else\n-\t\tsp = newg->gcinitial_sp;\n-\t\tspsize = newg->gcstack_size;\n+\t\tsp = newg->gcinitialsp;\n+\t\tspsize = newg->gcstacksize;\n \t\tif(spsize == 0)\n \t\t\truntime_throw(\"bad spsize in __go_go\");\n-\t\tnewg->gcnext_sp = sp;\n+\t\tnewg->gcnextsp = sp;\n #endif\n \t} else {\n-\t\tnewg = runtime_malg(StackMin, &sp, &spsize);\n+\t\tuintptr malsize;\n+\n+\t\tnewg = runtime_malg(StackMin, &sp, &malsize);\n+\t\tspsize = (size_t)malsize;\n \t\tallgadd(newg);\n \t}\n \n \tnewg->entry = (byte*)fn;\n \tnewg->param = arg;\n \tnewg->gopc = (uintptr)__builtin_return_address(0);\n-\tnewg->status = Grunnable;\n+\tnewg->atomicstatus = _Grunnable;\n \tif(p->goidcache == p->goidcacheend) {\n \t\tp->goidcache = runtime_xadd64(&runtime_sched.goidgen, GoidCacheBatch);\n \t\tp->goidcacheend = p->goidcache + GoidCacheBatch;\n@@ -2353,19 +2376,19 @@ __go_go(void (*fn)(void*), void* arg)\n \t\tsize_t volatile vspsize = spsize;\n \t\tG * volatile vnewg = newg;\n \n-\t\tgetcontext(&vnewg->context);\n-\t\tvnewg->context.uc_stack.ss_sp = vsp;\n+\t\tgetcontext((ucontext_t*)&vnewg->context[0]);\n+\t\t((ucontext_t*)&vnewg->context[0])->uc_stack.ss_sp = vsp;\n #ifdef MAKECONTEXT_STACK_TOP\n-\t\tvnewg->context.uc_stack.ss_sp += vspsize;\n+\t\t((ucontext_t*)&vnewg->context[0])->uc_stack.ss_sp += vspsize;\n #endif\n-\t\tvnewg->context.uc_stack.ss_size = vspsize;\n-\t\tmakecontext(&vnewg->context, kickoff, 0);\n+\t\t((ucontext_t*)&vnewg->context[0])->uc_stack.ss_size = vspsize;\n+\t\tmakecontext((ucontext_t*)&vnewg->context[0], kickoff, 0);\n \n \t\trunqput(p, vnewg);\n \n \t\tif(runtime_atomicload(&runtime_sched.npidle) != 0 && runtime_atomicload(&runtime_sched.nmspinning) == 0 && fn != runtime_main)  // TODO: fast atomic\n \t\t\twakep();\n-\t\tm->locks--;\n+\t\tg->m->locks--;\n \t\treturn vnewg;\n \t}\n }\n@@ -2400,16 +2423,16 @@ allgadd(G *gp)\n static void\n gfput(P *p, G *gp)\n {\n-\tgp->schedlink = p->gfree;\n+\tgp->schedlink = (uintptr)p->gfree;\n \tp->gfree = gp;\n \tp->gfreecnt++;\n \tif(p->gfreecnt >= 64) {\n \t\truntime_lock(&runtime_sched.gflock);\n \t\twhile(p->gfreecnt >= 32) {\n \t\t\tp->gfreecnt--;\n \t\t\tgp = p->gfree;\n-\t\t\tp->gfree = gp->schedlink;\n-\t\t\tgp->schedlink = runtime_sched.gfree;\n+\t\t\tp->gfree = (G*)gp->schedlink;\n+\t\t\tgp->schedlink = (uintptr)runtime_sched.gfree;\n \t\t\truntime_sched.gfree = gp;\n \t\t}\n \t\truntime_unlock(&runtime_sched.gflock);\n@@ -2430,15 +2453,15 @@ gfget(P *p)\n \t\twhile(p->gfreecnt < 32 && runtime_sched.gfree) {\n \t\t\tp->gfreecnt++;\n \t\t\tgp = runtime_sched.gfree;\n-\t\t\truntime_sched.gfree = gp->schedlink;\n-\t\t\tgp->schedlink = p->gfree;\n+\t\t\truntime_sched.gfree = (G*)gp->schedlink;\n+\t\t\tgp->schedlink = (uintptr)p->gfree;\n \t\t\tp->gfree = gp;\n \t\t}\n \t\truntime_unlock(&runtime_sched.gflock);\n \t\tgoto retry;\n \t}\n \tif(gp) {\n-\t\tp->gfree = gp->schedlink;\n+\t\tp->gfree = (G*)gp->schedlink;\n \t\tp->gfreecnt--;\n \t}\n \treturn gp;\n@@ -2454,8 +2477,8 @@ gfpurge(P *p)\n \twhile(p->gfreecnt) {\n \t\tp->gfreecnt--;\n \t\tgp = p->gfree;\n-\t\tp->gfree = gp->schedlink;\n-\t\tgp->schedlink = runtime_sched.gfree;\n+\t\tp->gfree = (G*)gp->schedlink;\n+\t\tgp->schedlink = (uintptr)runtime_sched.gfree;\n \t\truntime_sched.gfree = gp;\n \t}\n \truntime_unlock(&runtime_sched.gflock);\n@@ -2482,8 +2505,8 @@ runtime_gomaxprocsfunc(int32 n)\n {\n \tint32 ret;\n \n-\tif(n > MaxGomaxprocs)\n-\t\tn = MaxGomaxprocs;\n+\tif(n > _MaxGomaxprocs)\n+\t\tn = _MaxGomaxprocs;\n \truntime_lock(&runtime_sched);\n \tret = runtime_gomaxprocs;\n \tif(n <= 0 || n == ret) {\n@@ -2493,10 +2516,10 @@ runtime_gomaxprocsfunc(int32 n)\n \truntime_unlock(&runtime_sched);\n \n \truntime_semacquire(&runtime_worldsema, false);\n-\tm->gcing = 1;\n+\tg->m->gcing = 1;\n \truntime_stoptheworld();\n \tnewprocs = n;\n-\tm->gcing = 0;\n+\tg->m->gcing = 0;\n \truntime_semrelease(&runtime_worldsema);\n \truntime_starttheworld();\n \n@@ -2509,22 +2532,22 @@ runtime_gomaxprocsfunc(int32 n)\n static void\n lockOSThread(void)\n {\n-\tm->lockedg = g;\n-\tg->lockedm = m;\n+\tg->m->lockedg = g;\n+\tg->lockedm = g->m;\n }\n \n void\truntime_LockOSThread(void) __asm__ (GOSYM_PREFIX \"runtime.LockOSThread\");\n void\n runtime_LockOSThread(void)\n {\n-\tm->locked |= LockExternal;\n+\tg->m->locked |= _LockExternal;\n \tlockOSThread();\n }\n \n void\n runtime_lockOSThread(void)\n {\n-\tm->locked += LockInternal;\n+\tg->m->locked += _LockInternal;\n \tlockOSThread();\n }\n \n@@ -2535,9 +2558,9 @@ runtime_lockOSThread(void)\n static void\n unlockOSThread(void)\n {\n-\tif(m->locked != 0)\n+\tif(g->m->locked != 0)\n \t\treturn;\n-\tm->lockedg = nil;\n+\tg->m->lockedg = nil;\n \tg->lockedm = nil;\n }\n \n@@ -2546,23 +2569,23 @@ void\truntime_UnlockOSThread(void) __asm__ (GOSYM_PREFIX \"runtime.UnlockOSThread\"\n void\n runtime_UnlockOSThread(void)\n {\n-\tm->locked &= ~LockExternal;\n+\tg->m->locked &= ~_LockExternal;\n \tunlockOSThread();\n }\n \n void\n runtime_unlockOSThread(void)\n {\n-\tif(m->locked < LockInternal)\n+\tif(g->m->locked < _LockInternal)\n \t\truntime_throw(\"runtime: internal error: misuse of lockOSThread/unlockOSThread\");\n-\tm->locked -= LockInternal;\n+\tg->m->locked -= _LockInternal;\n \tunlockOSThread();\n }\n \n bool\n runtime_lockedOSThread(void)\n {\n-\treturn g->lockedm != nil && m->lockedg != nil;\n+\treturn g->lockedm != nil && g->m->lockedg != nil;\n }\n \n int32\n@@ -2580,8 +2603,8 @@ runtime_gcount(void)\n \t// Compromise solution is to introduce per-P counters of active goroutines.\n \tfor(i = 0; i < runtime_allglen; i++) {\n \t\tgp = runtime_allg[i];\n-\t\ts = gp->status;\n-\t\tif(s == Grunnable || s == Grunning || s == Gsyscall || s == Gwaiting)\n+\t\ts = gp->atomicstatus;\n+\t\tif(s == _Grunnable || s == _Grunning || s == _Gsyscall || s == _Gwaiting)\n \t\t\tn++;\n \t}\n \truntime_unlock(&allglock);\n@@ -2609,7 +2632,7 @@ static void GC(void) {}\n void\n runtime_sigprof()\n {\n-\tM *mp = m;\n+\tM *mp = g->m;\n \tint32 n, i;\n \tbool traceback;\n \n@@ -2675,7 +2698,7 @@ runtime_setcpuprofilerate(void (*fn)(uintptr*, int32), int32 hz)\n \n \t// Disable preemption, otherwise we can be rescheduled to another thread\n \t// that has profiling enabled.\n-\tm->locks++;\n+\tg->m->locks++;\n \n \t// Stop profiler on this thread so that it is safe to lock prof.\n \t// if a profiling signal came in while we had prof locked,\n@@ -2693,7 +2716,7 @@ runtime_setcpuprofilerate(void (*fn)(uintptr*, int32), int32 hz)\n \tif(hz != 0)\n \t\truntime_resetcpuprofiler(hz);\n \n-\tm->locks--;\n+\tg->m->locks--;\n }\n \n // Change number of processors.  The world is stopped, sched is locked.\n@@ -2706,20 +2729,20 @@ procresize(int32 new)\n \tP *p;\n \n \told = runtime_gomaxprocs;\n-\tif(old < 0 || old > MaxGomaxprocs || new <= 0 || new >MaxGomaxprocs)\n+\tif(old < 0 || old > _MaxGomaxprocs || new <= 0 || new >_MaxGomaxprocs)\n \t\truntime_throw(\"procresize: invalid arg\");\n \t// initialize new P's\n \tfor(i = 0; i < new; i++) {\n \t\tp = runtime_allp[i];\n \t\tif(p == nil) {\n \t\t\tp = (P*)runtime_mallocgc(sizeof(*p), 0, FlagNoInvokeGC);\n \t\t\tp->id = i;\n-\t\t\tp->status = Pgcstop;\n+\t\t\tp->status = _Pgcstop;\n \t\t\truntime_atomicstorep(&runtime_allp[i], p);\n \t\t}\n \t\tif(p->mcache == nil) {\n \t\t\tif(old==0 && i==0)\n-\t\t\t\tp->mcache = m->mcache;  // bootstrap\n+\t\t\t\tp->mcache = g->m->mcache;  // bootstrap\n \t\t\telse\n \t\t\t\tp->mcache = runtime_allocmcache();\n \t\t}\n@@ -2739,9 +2762,9 @@ procresize(int32 new)\n \t\t\tempty = false;\n \t\t\t// pop from tail of local queue\n \t\t\tp->runqtail--;\n-\t\t\tgp = p->runq[p->runqtail%nelem(p->runq)];\n+\t\t\tgp = (G*)p->runq[p->runqtail%nelem(p->runq)];\n \t\t\t// push onto head of global queue\n-\t\t\tgp->schedlink = runtime_sched.runqhead;\n+\t\t\tgp->schedlink = (uintptr)runtime_sched.runqhead;\n \t\t\truntime_sched.runqhead = gp;\n \t\t\tif(runtime_sched.runqtail == nil)\n \t\t\t\truntime_sched.runqtail = gp;\n@@ -2753,7 +2776,7 @@ procresize(int32 new)\n \t// so if we have a spare G we want to put it into allp[1].\n \tfor(i = 1; (uint32)i < (uint32)new * nelem(p->runq)/2 && runtime_sched.runqsize > 0; i++) {\n \t\tgp = runtime_sched.runqhead;\n-\t\truntime_sched.runqhead = gp->schedlink;\n+\t\truntime_sched.runqhead = (G*)gp->schedlink;\n \t\tif(runtime_sched.runqhead == nil)\n \t\t\truntime_sched.runqtail = nil;\n \t\truntime_sched.runqsize--;\n@@ -2766,21 +2789,21 @@ procresize(int32 new)\n \t\truntime_freemcache(p->mcache);\n \t\tp->mcache = nil;\n \t\tgfpurge(p);\n-\t\tp->status = Pdead;\n+\t\tp->status = _Pdead;\n \t\t// can't free P itself because it can be referenced by an M in syscall\n \t}\n \n-\tif(m->p)\n-\t\tm->p->m = nil;\n-\tm->p = nil;\n-\tm->mcache = nil;\n+\tif(g->m->p)\n+\t\t((P*)g->m->p)->m = 0;\n+\tg->m->p = 0;\n+\tg->m->mcache = nil;\n \tp = runtime_allp[0];\n-\tp->m = nil;\n-\tp->status = Pidle;\n+\tp->m = 0;\n+\tp->status = _Pidle;\n \tacquirep(p);\n \tfor(i = new-1; i > 0; i--) {\n \t\tp = runtime_allp[i];\n-\t\tp->status = Pidle;\n+\t\tp->status = _Pidle;\n \t\tpidleput(p);\n \t}\n \truntime_atomicstore((uint32*)&runtime_gomaxprocs, new);\n@@ -2790,36 +2813,41 @@ procresize(int32 new)\n static void\n acquirep(P *p)\n {\n+\tM *m;\n+\n+\tm = g->m;\n \tif(m->p || m->mcache)\n \t\truntime_throw(\"acquirep: already in go\");\n-\tif(p->m || p->status != Pidle) {\n-\t\truntime_printf(\"acquirep: p->m=%p(%d) p->status=%d\\n\", p->m, p->m ? p->m->id : 0, p->status);\n+\tif(p->m || p->status != _Pidle) {\n+\t\truntime_printf(\"acquirep: p->m=%p(%d) p->status=%d\\n\", p->m, p->m ? ((M*)p->m)->id : 0, p->status);\n \t\truntime_throw(\"acquirep: invalid p state\");\n \t}\n \tm->mcache = p->mcache;\n-\tm->p = p;\n-\tp->m = m;\n-\tp->status = Prunning;\n+\tm->p = (uintptr)p;\n+\tp->m = (uintptr)m;\n+\tp->status = _Prunning;\n }\n \n // Disassociate p and the current m.\n static P*\n releasep(void)\n {\n+\tM *m;\n \tP *p;\n \n-\tif(m->p == nil || m->mcache == nil)\n+\tm = g->m;\n+\tif(m->p == 0 || m->mcache == nil)\n \t\truntime_throw(\"releasep: invalid arg\");\n-\tp = m->p;\n-\tif(p->m != m || p->mcache != m->mcache || p->status != Prunning) {\n+\tp = (P*)m->p;\n+\tif((M*)p->m != m || p->mcache != m->mcache || p->status != _Prunning) {\n \t\truntime_printf(\"releasep: m=%p m->p=%p p->m=%p m->mcache=%p p->mcache=%p p->status=%d\\n\",\n \t\t\tm, m->p, p->m, m->mcache, p->mcache, p->status);\n \t\truntime_throw(\"releasep: invalid p state\");\n \t}\n-\tm->p = nil;\n+\tm->p = 0;\n \tm->mcache = nil;\n-\tp->m = nil;\n-\tp->status = Pidle;\n+\tp->m = 0;\n+\tp->status = _Pidle;\n \treturn p;\n }\n \n@@ -2870,10 +2898,10 @@ checkdead(void)\n \t\tgp = runtime_allg[i];\n \t\tif(gp->isbackground)\n \t\t\tcontinue;\n-\t\ts = gp->status;\n-\t\tif(s == Gwaiting)\n+\t\ts = gp->atomicstatus;\n+\t\tif(s == _Gwaiting)\n \t\t\tgrunning++;\n-\t\telse if(s == Grunnable || s == Grunning || s == Gsyscall) {\n+\t\telse if(s == _Grunnable || s == _Grunning || s == _Gsyscall) {\n \t\t\truntime_unlock(&allglock);\n \t\t\truntime_printf(\"runtime: checkdead: find g %D in status %d\\n\", gp->goid, s);\n \t\t\truntime_throw(\"checkdead: runnable g\");\n@@ -2882,7 +2910,7 @@ checkdead(void)\n \truntime_unlock(&allglock);\n \tif(grunning == 0)  // possible if main goroutine calls runtime_Goexit()\n \t\truntime_throw(\"no goroutines (main called runtime.Goexit) - deadlock!\");\n-\tm->throwing = -1;  // do not dump full stacks\n+\tg->m->throwing = -1;  // do not dump full stacks\n \truntime_throw(\"all goroutines are asleep - deadlock!\");\n }\n \n@@ -2958,7 +2986,7 @@ struct Pdesc\n \tuint32\tsyscalltick;\n \tint64\tsyscallwhen;\n };\n-static Pdesc pdesc[MaxGomaxprocs];\n+static Pdesc pdesc[_MaxGomaxprocs];\n \n static uint32\n retake(int64 now)\n@@ -2975,7 +3003,7 @@ retake(int64 now)\n \t\t\tcontinue;\n \t\tpd = &pdesc[i];\n \t\ts = p->status;\n-\t\tif(s == Psyscall) {\n+\t\tif(s == _Psyscall) {\n \t\t\t// Retake P from syscall if it's there for more than 1 sysmon tick (at least 20us).\n \t\t\tt = p->syscalltick;\n \t\t\tif(pd->syscalltick != t) {\n@@ -2995,12 +3023,12 @@ retake(int64 now)\n \t\t\t// Otherwise the M from which we retake can exit the syscall,\n \t\t\t// increment nmidle and report deadlock.\n \t\t\tincidlelocked(-1);\n-\t\t\tif(runtime_cas(&p->status, s, Pidle)) {\n+\t\t\tif(runtime_cas(&p->status, s, _Pidle)) {\n \t\t\t\tn++;\n \t\t\t\thandoffp(p);\n \t\t\t}\n \t\t\tincidlelocked(1);\n-\t\t} else if(s == Prunning) {\n+\t\t} else if(s == _Prunning) {\n \t\t\t// Preempt G if it's running for more than 10ms.\n \t\t\tt = p->schedtick;\n \t\t\tif(pd->schedtick != t) {\n@@ -3060,7 +3088,7 @@ runtime_schedtrace(bool detailed)\n \t\tp = runtime_allp[i];\n \t\tif(p == nil)\n \t\t\tcontinue;\n-\t\tmp = p->m;\n+\t\tmp = (M*)p->m;\n \t\th = runtime_atomicload(&p->runqhead);\n \t\tt = runtime_atomicload(&p->runqtail);\n \t\tif(detailed)\n@@ -3084,7 +3112,7 @@ runtime_schedtrace(bool detailed)\n \t\treturn;\n \t}\n \tfor(mp = runtime_allm; mp; mp = mp->alllink) {\n-\t\tp = mp->p;\n+\t\tp = (P*)mp->p;\n \t\tgp = mp->curg;\n \t\tlockedg = mp->lockedg;\n \t\tid1 = -1;\n@@ -3100,15 +3128,15 @@ runtime_schedtrace(bool detailed)\n \t\t\t\" locks=%d dying=%d helpgc=%d spinning=%d blocked=%d lockedg=%D\\n\",\n \t\t\tmp->id, id1, id2,\n \t\t\tmp->mallocing, mp->throwing, mp->gcing, mp->locks, mp->dying, mp->helpgc,\n-\t\t\tmp->spinning, m->blocked, id3);\n+\t\t\tmp->spinning, mp->blocked, id3);\n \t}\n \truntime_lock(&allglock);\n \tfor(gi = 0; gi < runtime_allglen; gi++) {\n \t\tgp = runtime_allg[gi];\n \t\tmp = gp->m;\n \t\tlockedm = gp->lockedm;\n-\t\truntime_printf(\"  G%D: status=%d(%s) m=%d lockedm=%d\\n\",\n-\t\t\tgp->goid, gp->status, gp->waitreason, mp ? mp->id : -1,\n+\t\truntime_printf(\"  G%D: status=%d(%S) m=%d lockedm=%d\\n\",\n+\t\t\tgp->goid, gp->atomicstatus, gp->waitreason, mp ? mp->id : -1,\n \t\t\tlockedm ? lockedm->id : -1);\n \t}\n \truntime_unlock(&allglock);\n@@ -3120,7 +3148,7 @@ runtime_schedtrace(bool detailed)\n static void\n mput(M *mp)\n {\n-\tmp->schedlink = runtime_sched.midle;\n+\tmp->schedlink = (uintptr)runtime_sched.midle;\n \truntime_sched.midle = mp;\n \truntime_sched.nmidle++;\n \tcheckdead();\n@@ -3134,7 +3162,7 @@ mget(void)\n \tM *mp;\n \n \tif((mp = runtime_sched.midle) != nil){\n-\t\truntime_sched.midle = mp->schedlink;\n+\t\truntime_sched.midle = (M*)mp->schedlink;\n \t\truntime_sched.nmidle--;\n \t}\n \treturn mp;\n@@ -3145,9 +3173,9 @@ mget(void)\n static void\n globrunqput(G *gp)\n {\n-\tgp->schedlink = nil;\n+\tgp->schedlink = 0;\n \tif(runtime_sched.runqtail)\n-\t\truntime_sched.runqtail->schedlink = gp;\n+\t\truntime_sched.runqtail->schedlink = (uintptr)gp;\n \telse\n \t\truntime_sched.runqhead = gp;\n \truntime_sched.runqtail = gp;\n@@ -3159,9 +3187,9 @@ globrunqput(G *gp)\n static void\n globrunqputbatch(G *ghead, G *gtail, int32 n)\n {\n-\tgtail->schedlink = nil;\n+\tgtail->schedlink = 0;\n \tif(runtime_sched.runqtail)\n-\t\truntime_sched.runqtail->schedlink = ghead;\n+\t\truntime_sched.runqtail->schedlink = (uintptr)ghead;\n \telse\n \t\truntime_sched.runqhead = ghead;\n \truntime_sched.runqtail = gtail;\n@@ -3189,11 +3217,11 @@ globrunqget(P *p, int32 max)\n \tif(runtime_sched.runqsize == 0)\n \t\truntime_sched.runqtail = nil;\n \tgp = runtime_sched.runqhead;\n-\truntime_sched.runqhead = gp->schedlink;\n+\truntime_sched.runqhead = (G*)gp->schedlink;\n \tn--;\n \twhile(n--) {\n \t\tgp1 = runtime_sched.runqhead;\n-\t\truntime_sched.runqhead = gp1->schedlink;\n+\t\truntime_sched.runqhead = (G*)gp1->schedlink;\n \t\trunqput(p, gp1);\n \t}\n \treturn gp;\n@@ -3204,7 +3232,7 @@ globrunqget(P *p, int32 max)\n static void\n pidleput(P *p)\n {\n-\tp->link = runtime_sched.pidle;\n+\tp->link = (uintptr)runtime_sched.pidle;\n \truntime_sched.pidle = p;\n \truntime_xadd(&runtime_sched.npidle, 1);  // TODO: fast atomic\n }\n@@ -3218,7 +3246,7 @@ pidleget(void)\n \n \tp = runtime_sched.pidle;\n \tif(p) {\n-\t\truntime_sched.pidle = p->link;\n+\t\truntime_sched.pidle = (P*)p->link;\n \t\truntime_xadd(&runtime_sched.npidle, -1);  // TODO: fast atomic\n \t}\n \treturn p;\n@@ -3236,7 +3264,7 @@ runqput(P *p, G *gp)\n \th = runtime_atomicload(&p->runqhead);  // load-acquire, synchronize with consumers\n \tt = p->runqtail;\n \tif(t - h < nelem(p->runq)) {\n-\t\tp->runq[t%nelem(p->runq)] = gp;\n+\t\tp->runq[t%nelem(p->runq)] = (uintptr)gp;\n \t\truntime_atomicstore(&p->runqtail, t+1);  // store-release, makes the item available for consumption\n \t\treturn;\n \t}\n@@ -3260,13 +3288,13 @@ runqputslow(P *p, G *gp, uint32 h, uint32 t)\n \tif(n != nelem(p->runq)/2)\n \t\truntime_throw(\"runqputslow: queue is not full\");\n \tfor(i=0; i<n; i++)\n-\t\tbatch[i] = p->runq[(h+i)%nelem(p->runq)];\n+\t\tbatch[i] = (G*)p->runq[(h+i)%nelem(p->runq)];\n \tif(!runtime_cas(&p->runqhead, h, h+n))  // cas-release, commits consume\n \t\treturn false;\n \tbatch[n] = gp;\n \t// Link the goroutines.\n \tfor(i=0; i<n; i++)\n-\t\tbatch[i]->schedlink = batch[i+1];\n+\t\tbatch[i]->schedlink = (uintptr)batch[i+1];\n \t// Now put the batch on global queue.\n \truntime_lock(&runtime_sched);\n \tglobrunqputbatch(batch[0], batch[n], n+1);\n@@ -3287,7 +3315,7 @@ runqget(P *p)\n \t\tt = p->runqtail;\n \t\tif(t == h)\n \t\t\treturn nil;\n-\t\tgp = p->runq[h%nelem(p->runq)];\n+\t\tgp = (G*)p->runq[h%nelem(p->runq)];\n \t\tif(runtime_cas(&p->runqhead, h, h+1))  // cas-release, commits consume\n \t\t\treturn gp;\n \t}\n@@ -3311,7 +3339,7 @@ runqgrab(P *p, G **batch)\n \t\tif(n > nelem(p->runq)/2)  // read inconsistent h and t\n \t\t\tcontinue;\n \t\tfor(i=0; i<n; i++)\n-\t\t\tbatch[i] = p->runq[(h+i)%nelem(p->runq)];\n+\t\t\tbatch[i] = (G*)p->runq[(h+i)%nelem(p->runq)];\n \t\tif(runtime_cas(&p->runqhead, h, h+n))  // cas-release, commits consume\n \t\t\tbreak;\n \t}\n@@ -3340,7 +3368,7 @@ runqsteal(P *p, P *p2)\n \tif(t - h + n >= nelem(p->runq))\n \t\truntime_throw(\"runqsteal: runq overflow\");\n \tfor(i=0; i<n; i++, t++)\n-\t\tp->runq[t%nelem(p->runq)] = batch[i];\n+\t\tp->runq[t%nelem(p->runq)] = (uintptr)batch[i];\n \truntime_atomicstore(&p->runqtail, t);  // store-release, makes the item available for consumption\n \treturn gp;\n }\n@@ -3480,7 +3508,7 @@ sync_runtime_canSpin(intgo i)\n \tif (i >= ACTIVE_SPIN || runtime_ncpu <= 1 || runtime_gomaxprocs <= (int32)(runtime_sched.npidle+runtime_sched.nmspinning)+1) {\n \t\treturn false;\n \t}\n-\tp = m->p;\n+\tp = (P*)g->m->p;\n \treturn p != nil && p->runqhead == p->runqtail;\n }\n "}, {"sha": "c7d33bcef4c65a170c343ebfc2bb13b52327c37b", "filename": "libgo/runtime/runtime.c", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fruntime.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fruntime.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -272,7 +272,8 @@ runtime_tickspersecond(void)\n void\n runtime_mpreinit(M *mp)\n {\n-\tmp->gsignal = runtime_malg(32*1024, &mp->gsignalstack, &mp->gsignalstacksize);\t// OS X wants >=8K, Linux >=2K\n+\tmp->gsignal = runtime_malg(32*1024, (byte**)&mp->gsignalstack, &mp->gsignalstacksize);\t// OS X wants >=8K, Linux >=2K\n+\tmp->gsignal->m = mp;\n }\n \n // Called to initialize a new m (including the bootstrap m)."}, {"sha": "617766b8a990f749147172f729cbfca743852332", "filename": "libgo/runtime/runtime.h", "status": "modified", "additions": 31, "deletions": 237, "changes": 268, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fruntime.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fruntime.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.h?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -56,43 +56,60 @@ typedef uintptr\t\tuintreg;\n typedef\tuint8\t\t\tbool;\n typedef\tuint8\t\t\tbyte;\n typedef\tstruct\tFunc\t\tFunc;\n-typedef\tstruct\tG\t\tG;\n-typedef\tstruct\tLock\t\tLock;\n-typedef\tstruct\tM\t\tM;\n-typedef\tstruct\tP\t\tP;\n-typedef\tstruct\tNote\t\tNote;\n+typedef\tstruct\tg\t\tG;\n+typedef\tstruct\tmutex\t\tLock;\n+typedef\tstruct\tm\t\tM;\n+typedef\tstruct\tp\t\tP;\n+typedef\tstruct\tnote\t\tNote;\n typedef\tstruct\tString\t\tString;\n typedef\tstruct\tFuncVal\t\tFuncVal;\n typedef\tstruct\tSigTab\t\tSigTab;\n-typedef\tstruct\tMCache\t\tMCache;\n+typedef\tstruct\tmcache\t\tMCache;\n typedef struct\tFixAlloc\tFixAlloc;\n typedef\tstruct\tHchan\t\tHchan;\n typedef\tstruct\tTimers\t\tTimers;\n typedef\tstruct\tTimer\t\tTimer;\n-typedef\tstruct\tGCStats\t\tGCStats;\n+typedef\tstruct\tgcstats\t\tGCStats;\n typedef\tstruct\tLFNode\t\tLFNode;\n typedef\tstruct\tParFor\t\tParFor;\n typedef\tstruct\tParForThread\tParForThread;\n-typedef\tstruct\tCgoMal\t\tCgoMal;\n+typedef\tstruct\tcgoMal\t\tCgoMal;\n typedef\tstruct\tPollDesc\tPollDesc;\n typedef\tstruct\tDebugVars\tDebugVars;\n \n typedef\tstruct\t__go_open_array\t\tSlice;\n typedef struct\t__go_interface\t\tIface;\n typedef\tstruct\t__go_empty_interface\tEface;\n typedef\tstruct\t__go_type_descriptor\tType;\n-typedef\tstruct\t__go_defer_stack\tDefer;\n-typedef\tstruct\t__go_panic_stack\tPanic;\n+typedef\tstruct\t_defer\t\t\tDefer;\n+typedef\tstruct\t_panic\t\t\tPanic;\n \n typedef struct\t__go_ptr_type\t\tPtrType;\n typedef struct\t__go_func_type\t\tFuncType;\n typedef struct\t__go_interface_type\tInterfaceType;\n typedef struct\t__go_map_type\t\tMapType;\n typedef struct\t__go_channel_type\tChanType;\n \n-typedef struct  Traceback\tTraceback;\n+typedef struct  traceback\tTraceback;\n \n-typedef struct\tLocation\tLocation;\n+typedef struct\tlocation\tLocation;\n+\n+struct String\n+{\n+\tconst byte*\tstr;\n+\tintgo\t\tlen;\n+};\n+\n+struct FuncVal\n+{\n+\tvoid\t(*fn)(void);\n+\t// variable-size, fn-specific data here\n+};\n+\n+#include \"array.h\"\n+#include \"interface.h\"\n+\n+#include \"runtime.inc\"\n \n /*\n  * Per-CPU declaration.\n@@ -103,33 +120,6 @@ extern G*\truntime_g(void);\n extern M\truntime_m0;\n extern G\truntime_g0;\n \n-/*\n- * defined constants\n- */\n-enum\n-{\n-\t// G status\n-\t//\n-\t// If you add to this list, add to the list\n-\t// of \"okay during garbage collection\" status\n-\t// in mgc0.c too.\n-\tGidle,\n-\tGrunnable,\n-\tGrunning,\n-\tGsyscall,\n-\tGwaiting,\n-\tGmoribund_unused,  // currently unused, but hardcoded in gdb scripts\n-\tGdead,\n-};\n-enum\n-{\n-\t// P status\n-\tPidle,\n-\tPrunning,\n-\tPsyscall,\n-\tPgcstop,\n-\tPdead,\n-};\n enum\n {\n \ttrue\t= 1,\n@@ -146,201 +136,13 @@ enum\n \t// Global <-> per-M stack segment cache transfer batch size.\n \tStackCacheBatch = 16,\n };\n-/*\n- * structures\n- */\n-struct\tLock\n-{\n-\t// Futex-based impl treats it as uint32 key,\n-\t// while sema-based impl as M* waitm.\n-\t// Used to be a union, but unions break precise GC.\n-\tuintptr\tkey;\n-};\n-struct\tNote\n-{\n-\t// Futex-based impl treats it as uint32 key,\n-\t// while sema-based impl as M* waitm.\n-\t// Used to be a union, but unions break precise GC.\n-\tuintptr\tkey;\n-};\n-struct String\n-{\n-\tconst byte*\tstr;\n-\tintgo\t\tlen;\n-};\n-struct FuncVal\n-{\n-\tvoid\t(*fn)(void);\n-\t// variable-size, fn-specific data here\n-};\n-struct\tGCStats\n-{\n-\t// the struct must consist of only uint64's,\n-\t// because it is casted to uint64[].\n-\tuint64\tnhandoff;\n-\tuint64\tnhandoffcnt;\n-\tuint64\tnprocyield;\n-\tuint64\tnosyield;\n-\tuint64\tnsleep;\n-};\n-\n-// A location in the program, used for backtraces.\n-struct\tLocation\n-{\n-\tuintptr\tpc;\n-\tString\tfilename;\n-\tString\tfunction;\n-\tintgo\tlineno;\n-};\n-\n-struct\tG\n-{\n-\tDefer*\tdefer;\n-\tPanic*\tpanic;\n-\tvoid*\texception;\t// current exception being thrown\n-\tbool\tis_foreign;\t// whether current exception from other language\n-\tvoid\t*gcstack;\t// if status==Gsyscall, gcstack = stackbase to use during gc\n-\tsize_t\tgcstack_size;\n-\tvoid*\tgcnext_segment;\n-\tvoid*\tgcnext_sp;\n-\tvoid*\tgcinitial_sp;\n-\tucontext_t gcregs;\n-\tbyte*\tentry;\t\t// initial function\n-\tvoid*\tparam;\t\t// passed parameter on wakeup\n-\tbool\tfromgogo;\t// reached from gogo\n-\tint16\tstatus;\n-\tuint32\tselgen;\t\t// valid sudog pointer\n-\tint64\tgoid;\n-\tint64\twaitsince;\t// approx time when the G become blocked\n-\tconst char*\twaitreason;\t// if status==Gwaiting\n-\tG*\tschedlink;\n-\tbool\tispanic;\n-\tbool\tissystem;\t// do not output in stack dump\n-\tbool\tisbackground;\t// ignore in deadlock detector\n-\tbool\tpaniconfault;\t// panic (instead of crash) on unexpected fault address\n-\tM*\tm;\t\t// for debuggers, but offset not hard-coded\n-\tM*\tlockedm;\n-\tint32\tsig;\n-\tint32\twritenbuf;\n-\tbyte*\twritebuf;\n-\tuintptr\tsigcode0;\n-\tuintptr\tsigcode1;\n-\t// uintptr\tsigpc;\n-\tuintptr\tgopc;\t// pc of go statement that created this goroutine\n-\n-\tint32\tncgo;\n-\tCgoMal*\tcgomal;\n-\n-\tTraceback* traceback;\n-\n-\tucontext_t\tcontext;\n-\tvoid*\t\tstack_context[10];\n-};\n-\n-struct\tM\n-{\n-\tG*\tg0;\t\t// goroutine with scheduling stack\n-\tG*\tgsignal;\t// signal-handling G\n-\tbyte*\tgsignalstack;\n-\tsize_t\tgsignalstacksize;\n-\tvoid\t(*mstartfn)(void);\n-\tG*\tcurg;\t\t// current running goroutine\n-\tG*\tcaughtsig;\t// goroutine running during fatal signal\n-\tP*\tp;\t\t// attached P for executing Go code (nil if not executing Go code)\n-\tP*\tnextp;\n-\tint32\tid;\n-\tint32\tmallocing;\n-\tint32\tthrowing;\n-\tint32\tgcing;\n-\tint32\tlocks;\n-\tint32\tsoftfloat;\n-\tint32\tdying;\n-\tint32\tprofilehz;\n-\tint32\thelpgc;\n-\tbool\tspinning;\t// M is out of work and is actively looking for work\n-\tbool\tblocked;\t// M is blocked on a Note\n-\tuint32\tfastrand;\n-\tuint64\tncgocall;\t// number of cgo calls in total\n-\tint32\tncgo;\t\t// number of cgo calls currently in progress\n-\tCgoMal*\tcgomal;\n-\tNote\tpark;\n-\tM*\talllink;\t// on allm\n-\tM*\tschedlink;\n-\tMCache\t*mcache;\n-\tG*\tlockedg;\n-\tLocation createstack[32];\t// Stack that created this thread.\n-\tuint32\tlocked;\t// tracking for LockOSThread\n-\tM*\tnextwaitm;\t// next M waiting for lock\n-\tuintptr\twaitsema;\t// semaphore for parking on locks\n-\tuint32\twaitsemacount;\n-\tuint32\twaitsemalock;\n-\tGCStats\tgcstats;\n-\tbool\tneedextram;\n-\tbool\tdropextram;\t// for gccgo: drop after call is done.\n-\tuint8\ttraceback;\n-\tbool\t(*waitunlockf)(G*, void*);\n-\tvoid*\twaitlock;\n-\tuintptr\tend[];\n-};\n-\n-struct P\n-{\n-\tLock;\n-\n-\tint32\tid;\n-\tuint32\tstatus;\t\t// one of Pidle/Prunning/...\n-\tP*\tlink;\n-\tuint32\tschedtick;\t// incremented on every scheduler call\n-\tuint32\tsyscalltick;\t// incremented on every system call\n-\tM*\tm;\t\t// back-link to associated M (nil if idle)\n-\tMCache*\tmcache;\n-\tDefer*\tdeferpool;\t// pool of available Defer structs (see panic.c)\n-\n-\t// Cache of goroutine ids, amortizes accesses to runtime_sched.goidgen.\n-\tuint64\tgoidcache;\n-\tuint64\tgoidcacheend;\n-\n-\t// Queue of runnable goroutines.\n-\tuint32\trunqhead;\n-\tuint32\trunqtail;\n-\tG*\trunq[256];\n-\n-\t// Available G's (status == Gdead)\n-\tG*\tgfree;\n-\tint32\tgfreecnt;\n-\n-\tbyte\tpad[64];\n-};\n-\n-// The m->locked word holds two pieces of state counting active calls to LockOSThread/lockOSThread.\n-// The low bit (LockExternal) is a boolean reporting whether any LockOSThread call is active.\n-// External locks are not recursive; a second lock is silently ignored.\n-// The upper bits of m->lockedcount record the nesting depth of calls to lockOSThread\n-// (counting up by LockInternal), popped by unlockOSThread (counting down by LockInternal).\n-// Internal locks can be recursive. For instance, a lock for cgo can occur while the main\n-// goroutine is holding the lock during the initialization phase.\n-enum\n-{\n-\tLockExternal = 1,\n-\tLockInternal = 2,\n-};\n \n struct\tSigTab\n {\n \tint32\tsig;\n \tint32\tflags;\n \tvoid*   fwdsig;\n };\n-enum\n-{\n-\tSigNotify = 1<<0,\t// let signal.Notify have signal, even if from kernel\n-\tSigKill = 1<<1,\t\t// if signal.Notify doesn't take it, exit quietly\n-\tSigThrow = 1<<2,\t// if signal.Notify doesn't take it, exit loudly\n-\tSigPanic = 1<<3,\t// if the signal is from the kernel, panic\n-\tSigDefault = 1<<4,\t// if the signal isn't explicitly requested, don't monitor it\n-\tSigHandling = 1<<5,\t// our signal handler is registered\n-\tSigGoExit = 1<<6,\t// cause all runtime procs to exit (only used on Plan 9).\n-};\n \n // Layout of in-memory per-function information prepared by linker\n // See http://golang.org/s/go12symtab.\n@@ -438,14 +240,6 @@ struct ParFor\n \tuint64 nsleep;\n };\n \n-// Track memory allocated by code not written in Go during a cgo call,\n-// so that the garbage collector can see them.\n-struct CgoMal\n-{\n-\tCgoMal\t*next;\n-\tvoid\t*alloc;\n-};\n-\n // Holds variables parsed from GODEBUG env var.\n struct DebugVars\n {\n@@ -565,7 +359,7 @@ void\truntime_ready(G*);\n String\truntime_getenv(const char*);\n int32\truntime_atoi(const byte*, intgo);\n void*\truntime_mstart(void*);\n-G*\truntime_malg(int32, byte**, size_t*);\n+G*\truntime_malg(int32, byte**, uintptr*);\n void\truntime_mpreinit(M*);\n void\truntime_minit(void);\n void\truntime_unminit(void);\n@@ -604,7 +398,7 @@ int32\truntime_round2(int32 x); // round x up to a power of 2.\n #define runtime_atomicloadp(p) __atomic_load_n (p, __ATOMIC_SEQ_CST)\n #define runtime_atomicstorep(p, v) __atomic_store_n (p, v, __ATOMIC_SEQ_CST)\n \n-void runtime_setmg(M*, G*);\n+void runtime_setg(G*);\n void runtime_newextram(void);\n #define runtime_exit(s) exit(s)\n #define runtime_breakpoint() __builtin_trap()"}, {"sha": "26b5566017d00761ec902d3d05ee56d79d5a7b9d", "filename": "libgo/runtime/runtime1.goc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fruntime1.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fruntime1.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime1.goc?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -65,7 +65,7 @@ func sync.runtime_procPin() (p int) {\n \tmp = runtime_m();\n \t// Disable preemption.\n \tmp->locks++;\n-\tp = mp->p->id;\n+\tp = ((P*)mp->p)->id;\n }\n \n func sync.runtime_procUnpin() {\n@@ -78,7 +78,7 @@ func sync_atomic.runtime_procPin() (p int) {\n \tmp = runtime_m();\n \t// Disable preemption.\n \tmp->locks++;\n-\tp = mp->p->id;\n+\tp = ((P*)mp->p)->id;\n }\n \n func sync_atomic.runtime_procUnpin() {"}, {"sha": "2cca08cf47bed7fed9335b531d8a595f93f1683e", "filename": "libgo/runtime/signal_unix.c", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fsignal_unix.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/75791bab05ec4a45a5c6a4dccd7f824ea8f4487c/libgo%2Fruntime%2Fsignal_unix.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fsignal_unix.c?ref=75791bab05ec4a45a5c6a4dccd7f824ea8f4487c", "patch": "@@ -26,7 +26,7 @@ runtime_initsig(bool preinit)\n \t// First call: basic setup.\n \tfor(i = 0; runtime_sigtab[i].sig != -1; i++) {\n \t\tt = &runtime_sigtab[i];\n-\t\tif((t->flags == 0) || (t->flags & SigDefault))\n+\t\tif((t->flags == 0) || (t->flags & _SigDefault))\n \t\t\tcontinue;\n \n \t\tt->fwdsig = runtime_getsig(i);\n@@ -42,10 +42,10 @@ runtime_initsig(bool preinit)\n \t\t\t}\n \t\t}\n \n-\t\tif(runtime_isarchive && (t->flags&SigPanic) == 0)\n+\t\tif(runtime_isarchive && (t->flags&_SigPanic) == 0)\n \t\t\tcontinue;\n \n-\t\tt->flags |= SigHandling;\n+\t\tt->flags |= _SigHandling;\n \t\truntime_setsig(i, runtime_sighandler, true);\n \t}\n }\n@@ -67,8 +67,8 @@ runtime_sigenable(uint32 sig)\n \tif(t == nil)\n \t\treturn;\n \n-\tif((t->flags & SigNotify) && !(t->flags & SigHandling)) {\n-\t\tt->flags |= SigHandling;\n+\tif((t->flags & _SigNotify) && !(t->flags & _SigHandling)) {\n+\t\tt->flags |= _SigHandling;\n \t\tt->fwdsig = runtime_getsig(i);\n \t\truntime_setsig(i, runtime_sighandler, true);\n \t}\n@@ -92,7 +92,7 @@ runtime_sigdisable(uint32 sig)\n \t\treturn;\n \n \tif((sig == SIGHUP || sig == SIGINT) && t->fwdsig == GO_SIG_IGN) {\n-\t\tt->flags &= ~SigHandling;\n+\t\tt->flags &= ~_SigHandling;\n \t\truntime_setsig(i, t->fwdsig, true);\n \t}\n }\n@@ -114,8 +114,8 @@ runtime_sigignore(uint32 sig)\n \tif(t == nil)\n \t\treturn;\n \n-\tif((t->flags & SigNotify) != 0) {\n-\t\tt->flags &= ~SigHandling;\n+\tif((t->flags & _SigNotify) != 0) {\n+\t\tt->flags &= ~_SigHandling;\n \t\truntime_setsig(i, GO_SIG_IGN, true);\n \t}\n }"}]}