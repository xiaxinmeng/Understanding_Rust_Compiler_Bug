{"sha": "8b7d5dab44547924887d15e00823fac44df2aa61", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OGI3ZDVkYWI0NDU0NzkyNDg4N2QxNWUwMDgyM2ZhYzQ0ZGYyYWE2MQ==", "commit": {"author": {"name": "Richard Sandiford", "email": "rdsandiford@googlemail.com", "date": "2012-11-18T17:33:15Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2012-11-18T17:33:15Z"}, "message": "stor-layout.c (bit_field_mode_iterator::bit_field_mode_iterator): Set up a default value of bitregion_end_.\n\ngcc/\n\t* stor-layout.c (bit_field_mode_iterator::bit_field_mode_iterator):\n\tSet up a default value of bitregion_end_.\n\t(bit_field_mode_iterator::next_mode): Always apply bitregion_end_\n\tcheck.  Include SLOW_UNALIGNED_ACCESS in the alignment check.\n\t(get_best_mode): Ignore modes that are wider than the alignment.\n\nFrom-SVN: r193604", "tree": {"sha": "b9aa47c05ac831e27c812e6887cc6829f0cdfba3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/b9aa47c05ac831e27c812e6887cc6829f0cdfba3"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/8b7d5dab44547924887d15e00823fac44df2aa61", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8b7d5dab44547924887d15e00823fac44df2aa61", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8b7d5dab44547924887d15e00823fac44df2aa61", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8b7d5dab44547924887d15e00823fac44df2aa61/comments", "author": {"login": "rsandifo", "id": 4235983, "node_id": "MDQ6VXNlcjQyMzU5ODM=", "avatar_url": "https://avatars.githubusercontent.com/u/4235983?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo", "html_url": "https://github.com/rsandifo", "followers_url": "https://api.github.com/users/rsandifo/followers", "following_url": "https://api.github.com/users/rsandifo/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo/orgs", "repos_url": "https://api.github.com/users/rsandifo/repos", "events_url": "https://api.github.com/users/rsandifo/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "073a544dd99d1526bbf8906e8ac13d42a752d200", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/073a544dd99d1526bbf8906e8ac13d42a752d200", "html_url": "https://github.com/Rust-GCC/gccrs/commit/073a544dd99d1526bbf8906e8ac13d42a752d200"}], "stats": {"total": 89, "additions": 76, "deletions": 13}, "files": [{"sha": "c153f7320156a39f8eeac81653099634d372d465", "filename": "gcc/ChangeLog", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8b7d5dab44547924887d15e00823fac44df2aa61/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8b7d5dab44547924887d15e00823fac44df2aa61/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=8b7d5dab44547924887d15e00823fac44df2aa61", "patch": "@@ -1,3 +1,11 @@\n+2012-11-18  Richard Sandiford  <rdsandiford@googlemail.com>\n+\n+\t* stor-layout.c (bit_field_mode_iterator::bit_field_mode_iterator):\n+\tSet up a default value of bitregion_end_.\n+\t(bit_field_mode_iterator::next_mode): Always apply bitregion_end_\n+\tcheck.  Include SLOW_UNALIGNED_ACCESS in the alignment check.\n+\t(get_best_mode): Ignore modes that are wider than the alignment.\n+\n 2012-11-18  Richard Sandiford  <rdsandiford@googlemail.com>\n \n \t* machmode.h (bit_field_mode_iterator): New class."}, {"sha": "4735724fa0c6a56f08df64ee663eff5718efd5ad", "filename": "gcc/stor-layout.c", "status": "modified", "additions": 68, "deletions": 13, "changes": 81, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8b7d5dab44547924887d15e00823fac44df2aa61/gcc%2Fstor-layout.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8b7d5dab44547924887d15e00823fac44df2aa61/gcc%2Fstor-layout.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fstor-layout.c?ref=8b7d5dab44547924887d15e00823fac44df2aa61", "patch": "@@ -2646,6 +2646,13 @@ ::bit_field_mode_iterator (HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n   bitregion_end_ (bitregion_end), align_ (MIN (align, BIGGEST_ALIGNMENT)),\n   volatilep_ (volatilep), count_ (0)\n {\n+  if (!bitregion_end_)\n+    {\n+      /* We can assume that any aligned chunk of ALIGN_ bits that overlaps\n+\t the bitfield is mapped and won't trap.  */\n+      bitregion_end_ = bitpos + bitsize + align_ - 1;\n+      bitregion_end_ -= bitregion_end_ % align_ + 1;\n+    }\n }\n \n /* Calls to this function return successively larger modes that can be used\n@@ -2676,23 +2683,15 @@ bit_field_mode_iterator::next_mode (enum machine_mode *out_mode)\n       if (count_ > 0 && unit > BITS_PER_WORD)\n \tbreak;\n \n-      /* Stop if the mode is wider than the alignment of the containing\n-\t object.\n-\n-\t It is tempting to omit the following line unless STRICT_ALIGNMENT\n-\t is true.  But that is incorrect, since if the bitfield uses part\n-\t of 3 bytes and we use a 4-byte mode, we could get a spurious segv\n-\t if the extra 4th byte is past the end of memory.\n-\t (Though at least one Unix compiler ignores this problem:\n-\t that on the Sequent 386 machine.  */\n-      if (unit > align_)\n-\tbreak;\n-\n       /* Stop if the mode goes outside the bitregion.  */\n       HOST_WIDE_INT start = bitpos_ - (bitpos_ % unit);\n       if (bitregion_start_ && start < bitregion_start_)\n \tbreak;\n-      if (bitregion_end_ && start + unit > bitregion_end_ + 1)\n+      if (start + unit > bitregion_end_ + 1)\n+\tbreak;\n+\n+      /* Stop if the mode requires too much alignment.  */\n+      if (unit > align_ && SLOW_UNALIGNED_ACCESS (mode_, align_))\n \tbreak;\n \n       *out_mode = mode_;\n@@ -2751,6 +2750,62 @@ get_best_mode (int bitsize, int bitpos,\n   enum machine_mode widest_mode = VOIDmode;\n   enum machine_mode mode;\n   while (iter.next_mode (&mode)\n+\t /* ??? For historical reasons, reject modes that are wider than\n+\t    the alignment.  This has both advantages and disadvantages.\n+\t    Removing this check means that something like:\n+\n+\t       struct s { unsigned int x; unsigned int y; };\n+\t       int f (struct s *s) { return s->x == 0 && s->y == 0; }\n+\n+\t    can be implemented using a single load and compare on\n+\t    64-bit machines that have no alignment restrictions.\n+\t    For example, on powerpc64-linux-gnu, we would generate:\n+\n+\t\t    ld 3,0(3)\n+\t\t    cntlzd 3,3\n+\t\t    srdi 3,3,6\n+\t\t    blr\n+\n+\t    rather than:\n+\n+\t\t    lwz 9,0(3)\n+\t\t    cmpwi 7,9,0\n+\t\t    bne 7,.L3\n+\t\t    lwz 3,4(3)\n+\t\t    cntlzw 3,3\n+\t\t    srwi 3,3,5\n+\t\t    extsw 3,3\n+\t\t    blr\n+\t\t    .p2align 4,,15\n+\t    .L3:\n+\t\t    li 3,0\n+\t\t    blr\n+\n+\t    However, accessing more than one field can make life harder\n+\t    for the gimple optimizers.  For example, gcc.dg/vect/bb-slp-5.c\n+\t    has a series of unsigned short copies followed by a series of\n+\t    unsigned short comparisons.  With this check, both the copies\n+\t    and comparisons remain 16-bit accesses and FRE is able\n+\t    to eliminate the latter.  Without the check, the comparisons\n+\t    can be done using 2 64-bit operations, which FRE isn't able\n+\t    to handle in the same way.\n+\n+\t    Either way, it would probably be worth disabling this check\n+\t    during expand.  One particular example where removing the\n+\t    check would help is the get_best_mode call in store_bit_field.\n+\t    If we are given a memory bitregion of 128 bits that is aligned\n+\t    to a 64-bit boundary, and the bitfield we want to modify is\n+\t    in the second half of the bitregion, this check causes\n+\t    store_bitfield to turn the memory into a 64-bit reference\n+\t    to the _first_ half of the region.  We later use\n+\t    adjust_bitfield_address to get a reference to the correct half,\n+\t    but doing so looks to adjust_bitfield_address as though we are\n+\t    moving past the end of the original object, so it drops the\n+\t    associated MEM_EXPR and MEM_OFFSET.  Removing the check\n+\t    causes store_bit_field to keep a 128-bit memory reference,\n+\t    so that the final bitfield reference still has a MEM_EXPR\n+\t    and MEM_OFFSET.  */\n+\t && GET_MODE_BITSIZE (mode) <= align\n \t && (largest_mode == VOIDmode\n \t     || GET_MODE_SIZE (mode) <= GET_MODE_SIZE (largest_mode)))\n     {"}]}