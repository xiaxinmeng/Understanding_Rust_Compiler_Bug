{"sha": "a28cfe49203705ff9675b79fce88d6087b11d098", "node_id": "C_kwDOANBUbNoAKGEyOGNmZTQ5MjAzNzA1ZmY5Njc1Yjc5ZmNlODhkNjA4N2IxMWQwOTg", "commit": {"author": {"name": "Bill Schmidt", "email": "wschmidt@linux.ibm.com", "date": "2021-11-07T13:56:07Z"}, "committer": {"name": "Bill Schmidt", "email": "wschmidt@linux.ibm.com", "date": "2021-11-07T15:23:55Z"}, "message": "rs6000: Replace the builtin expansion machinery\n\nThis patch forms the meat of the improvements for this patch series.\nWe develop a replacement for rs6000_expand_builtin and its supporting\nfunctions, which are inefficient and difficult to maintain.\n\nDifferences between the old and new support in this patch include:\n - Make use of the new builtin data structures, directly looking up\n   a function's information rather than searching for the function\n   multiple times;\n - Test for enablement of builtins at expand time, to support #pragma\n   target changes within a compilation unit;\n - Use the builtin function attributes (e.g., bif_is_cpu) to control\n   special handling;\n - Refactor common code into one place; and\n - Provide common error handling in one place for operands that are\n   restricted to specific values or ranges.\n\n2021-11-07  Bill Schmidt  <wschmidt@linux.ibm.com>\n\ngcc/\n\t* config/rs6000/rs6000-call.c (rs6000_expand_new_builtin): New\n\tforward decl.\n\t(rs6000_invalid_new_builtin): New function.\n\t(rs6000_expand_builtin): Call rs6000_expand_new_builtin.\n\t(rs6000_expand_ldst_mask): New function.\n\t(new_cpu_expand_builtin): Likewise.\n\t(elemrev_icode): Likewise.\n\t(ldv_expand_builtin): Likewise.\n\t(lxvrse_expand_builtin): Likewise.\n\t(lxvrze_expand_builtin): Likewise.\n\t(stv_expand_builtin): Likewise.\n\t(new_mma_expand_builtin): Likewise.\n\t(new_htm_spr_num): Likewise.\n\t(new_htm_expand_builtin): Likewise.\n\t(rs6000_expand_new_builtin): Likewise.\n\t(rs6000_init_builtins): Initialize altivec_builtin_mask_for_load.", "tree": {"sha": "ec4c143e0a14235f3a972653c99810986829a305", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ec4c143e0a14235f3a972653c99810986829a305"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a28cfe49203705ff9675b79fce88d6087b11d098", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a28cfe49203705ff9675b79fce88d6087b11d098", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a28cfe49203705ff9675b79fce88d6087b11d098", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a28cfe49203705ff9675b79fce88d6087b11d098/comments", "author": null, "committer": null, "parents": [{"sha": "4898e958a92d45dbf23c0f28bc7552689ba16ecc", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4898e958a92d45dbf23c0f28bc7552689ba16ecc", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4898e958a92d45dbf23c0f28bc7552689ba16ecc"}], "stats": {"total": 1448, "additions": 1332, "deletions": 116}, "files": [{"sha": "7121e50e6b7cf682e8a3d12b1a0466cbe9339770", "filename": "gcc/config/rs6000/rs6000-call.c", "status": "modified", "additions": 1332, "deletions": 116, "changes": 1448, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a28cfe49203705ff9675b79fce88d6087b11d098/gcc%2Fconfig%2Frs6000%2Frs6000-call.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a28cfe49203705ff9675b79fce88d6087b11d098/gcc%2Fconfig%2Frs6000%2Frs6000-call.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000-call.c?ref=a28cfe49203705ff9675b79fce88d6087b11d098", "patch": "@@ -190,6 +190,7 @@ static tree builtin_function_type (machine_mode, machine_mode,\n static void rs6000_common_init_builtins (void);\n static void htm_init_builtins (void);\n static void mma_init_builtins (void);\n+static rtx rs6000_expand_new_builtin (tree, rtx, rtx, machine_mode, int);\n static bool rs6000_gimple_fold_new_builtin (gimple_stmt_iterator *gsi);\n \n \n@@ -11789,6 +11790,83 @@ rs6000_invalid_builtin (enum rs6000_builtins fncode)\n     error (\"%qs is not supported with the current options\", name);\n }\n \n+/* Raise an error message for a builtin function that is called without the\n+   appropriate target options being set.  */\n+\n+static void\n+rs6000_invalid_new_builtin (enum rs6000_gen_builtins fncode)\n+{\n+  size_t j = (size_t) fncode;\n+  const char *name = rs6000_builtin_info_x[j].bifname;\n+\n+  switch (rs6000_builtin_info_x[j].enable)\n+    {\n+    case ENB_P5:\n+      error (\"%qs requires the %qs option\", name, \"-mcpu=power5\");\n+      break;\n+    case ENB_P6:\n+      error (\"%qs requires the %qs option\", name, \"-mcpu=power6\");\n+      break;\n+    case ENB_ALTIVEC:\n+      error (\"%qs requires the %qs option\", name, \"-maltivec\");\n+      break;\n+    case ENB_CELL:\n+      error (\"%qs requires the %qs option\", name, \"-mcpu=cell\");\n+      break;\n+    case ENB_VSX:\n+      error (\"%qs requires the %qs option\", name, \"-mvsx\");\n+      break;\n+    case ENB_P7:\n+      error (\"%qs requires the %qs option\", name, \"-mcpu=power7\");\n+      break;\n+    case ENB_P7_64:\n+      error (\"%qs requires the %qs option and either the %qs or %qs option\",\n+\t     name, \"-mcpu=power7\", \"-m64\", \"-mpowerpc64\");\n+      break;\n+    case ENB_P8:\n+      error (\"%qs requires the %qs option\", name, \"-mcpu=power8\");\n+      break;\n+    case ENB_P8V:\n+      error (\"%qs requires the %qs option\", name, \"-mpower8-vector\");\n+      break;\n+    case ENB_P9:\n+      error (\"%qs requires the %qs option\", name, \"-mcpu=power9\");\n+      break;\n+    case ENB_P9_64:\n+      error (\"%qs requires the %qs option and either the %qs or %qs option\",\n+\t     name, \"-mcpu=power9\", \"-m64\", \"-mpowerpc64\");\n+      break;\n+    case ENB_P9V:\n+      error (\"%qs requires the %qs option\", name, \"-mpower9-vector\");\n+      break;\n+    case ENB_IEEE128_HW:\n+      error (\"%qs requires ISA 3.0 IEEE 128-bit floating point\", name);\n+      break;\n+    case ENB_DFP:\n+      error (\"%qs requires the %qs option\", name, \"-mhard-dfp\");\n+      break;\n+    case ENB_CRYPTO:\n+      error (\"%qs requires the %qs option\", name, \"-mcrypto\");\n+      break;\n+    case ENB_HTM:\n+      error (\"%qs requires the %qs option\", name, \"-mhtm\");\n+      break;\n+    case ENB_P10:\n+      error (\"%qs requires the %qs option\", name, \"-mcpu=power10\");\n+      break;\n+    case ENB_P10_64:\n+      error (\"%qs requires the %qs option and either the %qs or %qs option\",\n+\t     name, \"-mcpu=power10\", \"-m64\", \"-mpowerpc64\");\n+      break;\n+    case ENB_MMA:\n+      error (\"%qs requires the %qs option\", name, \"-mmma\");\n+      break;\n+    default:\n+    case ENB_ALWAYS:\n+      gcc_unreachable ();\n+    }\n+}\n+\n /* Target hook for early folding of built-ins, shamelessly stolen\n    from ia64.c.  */\n \n@@ -14362,6 +14440,9 @@ rs6000_expand_builtin (tree exp, rtx target, rtx subtarget ATTRIBUTE_UNUSED,\n \t\t       machine_mode mode ATTRIBUTE_UNUSED,\n \t\t       int ignore ATTRIBUTE_UNUSED)\n {\n+  if (new_builtins_are_live)\n+    return rs6000_expand_new_builtin (exp, target, subtarget, mode, ignore);\n+\n   tree fndecl = TREE_OPERAND (CALL_EXPR_FN (exp), 0);\n   enum rs6000_builtins fcode\n     = (enum rs6000_builtins) DECL_MD_FUNCTION_CODE (fndecl);\n@@ -14654,149 +14735,1281 @@ rs6000_expand_builtin (tree exp, rtx target, rtx subtarget ATTRIBUTE_UNUSED,\n   gcc_unreachable ();\n }\n \n-/* Create a builtin vector type with a name.  Taking care not to give\n-   the canonical type a name.  */\n-\n-static tree\n-rs6000_vector_type (const char *name, tree elt_type, unsigned num_elts)\n+/* Expand ALTIVEC_BUILTIN_MASK_FOR_LOAD.  */\n+rtx\n+rs6000_expand_ldst_mask (rtx target, tree arg0)\n {\n-  tree result = build_vector_type (elt_type, num_elts);\n+  int icode2 = BYTES_BIG_ENDIAN ? (int) CODE_FOR_altivec_lvsr_direct\n+\t\t\t\t: (int) CODE_FOR_altivec_lvsl_direct;\n+  machine_mode tmode = insn_data[icode2].operand[0].mode;\n+  machine_mode mode = insn_data[icode2].operand[1].mode;\n \n-  /* Copy so we don't give the canonical type a name.  */\n-  result = build_variant_type_copy (result);\n+  gcc_assert (TARGET_ALTIVEC);\n \n-  add_builtin_type (name, result);\n+  gcc_assert (POINTER_TYPE_P (TREE_TYPE (arg0)));\n+  rtx op = expand_expr (arg0, NULL_RTX, Pmode, EXPAND_NORMAL);\n+  rtx addr = memory_address (mode, op);\n+  /* We need to negate the address.  */\n+  op = gen_reg_rtx (GET_MODE (addr));\n+  emit_insn (gen_rtx_SET (op, gen_rtx_NEG (GET_MODE (addr), addr)));\n+  op = gen_rtx_MEM (mode, op);\n \n-  return result;\n+  if (target == 0\n+      || GET_MODE (target) != tmode\n+      || !insn_data[icode2].operand[0].predicate (target, tmode))\n+    target = gen_reg_rtx (tmode);\n+\n+  rtx pat = GEN_FCN (icode2) (target, op);\n+  if (!pat)\n+    return 0;\n+  emit_insn (pat);\n+\n+  return target;\n }\n \n-void\n-rs6000_init_builtins (void)\n+/* Expand the CPU builtin in FCODE and store the result in TARGET.  */\n+static rtx\n+new_cpu_expand_builtin (enum rs6000_gen_builtins fcode,\n+\t\t\ttree exp ATTRIBUTE_UNUSED, rtx target)\n {\n-  tree tdecl;\n-  tree ftype;\n-  tree t;\n-  machine_mode mode;\n-  const char *str;\n+  /* __builtin_cpu_init () is a nop, so expand to nothing.  */\n+  if (fcode == RS6000_BIF_CPU_INIT)\n+    return const0_rtx;\n \n-  if (TARGET_DEBUG_BUILTIN)\n-    fprintf (stderr, \"rs6000_init_builtins%s%s\\n\",\n-\t     (TARGET_ALTIVEC)\t   ? \", altivec\" : \"\",\n-\t     (TARGET_VSX)\t   ? \", vsx\"\t : \"\");\n+  if (target == 0 || GET_MODE (target) != SImode)\n+    target = gen_reg_rtx (SImode);\n \n-  if (new_builtins_are_live)\n-    V2DI_type_node = rs6000_vector_type (\"__vector long long\",\n-\t\t\t\t\t long_long_integer_type_node, 2);\n-  else\n+  /* TODO: Factor the #ifdef'd code into a separate function.  */\n+#ifdef TARGET_LIBC_PROVIDES_HWCAP_IN_TCB\n+  tree arg = TREE_OPERAND (CALL_EXPR_ARG (exp, 0), 0);\n+  /* Target clones creates an ARRAY_REF instead of STRING_CST, convert it back\n+     to a STRING_CST.  */\n+  if (TREE_CODE (arg) == ARRAY_REF\n+      && TREE_CODE (TREE_OPERAND (arg, 0)) == STRING_CST\n+      && TREE_CODE (TREE_OPERAND (arg, 1)) == INTEGER_CST\n+      && compare_tree_int (TREE_OPERAND (arg, 1), 0) == 0)\n+    arg = TREE_OPERAND (arg, 0);\n+\n+  if (TREE_CODE (arg) != STRING_CST)\n     {\n-      str = TARGET_POWERPC64 ? \"__vector long\" : \"__vector long long\";\n-      V2DI_type_node = rs6000_vector_type (str,\n-\t\t\t\t\t   long_long_integer_type_node,\n-\t\t\t\t\t   2);\n+      error (\"builtin %qs only accepts a string argument\",\n+\t     rs6000_builtin_info_x[(size_t) fcode].bifname);\n+      return const0_rtx;\n     }\n-  ptr_V2DI_type_node\n-    = build_pointer_type (build_qualified_type (V2DI_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n-\n-  V2DF_type_node = rs6000_vector_type (\"__vector double\", double_type_node, 2);\n-  ptr_V2DF_type_node\n-    = build_pointer_type (build_qualified_type (V2DF_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n \n-  V4SI_type_node = rs6000_vector_type (\"__vector signed int\",\n-\t\t\t\t       intSI_type_node, 4);\n-  ptr_V4SI_type_node\n-    = build_pointer_type (build_qualified_type (V4SI_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n+  if (fcode == RS6000_BIF_CPU_IS)\n+    {\n+      const char *cpu = TREE_STRING_POINTER (arg);\n+      rtx cpuid = NULL_RTX;\n+      for (size_t i = 0; i < ARRAY_SIZE (cpu_is_info); i++)\n+\tif (strcmp (cpu, cpu_is_info[i].cpu) == 0)\n+\t  {\n+\t    /* The CPUID value in the TCB is offset by _DL_FIRST_PLATFORM.  */\n+\t    cpuid = GEN_INT (cpu_is_info[i].cpuid + _DL_FIRST_PLATFORM);\n+\t    break;\n+\t  }\n+      if (cpuid == NULL_RTX)\n+\t{\n+\t  /* Invalid CPU argument.  */\n+\t  error (\"cpu %qs is an invalid argument to builtin %qs\",\n+\t\t cpu, rs6000_builtin_info_x[(size_t) fcode].bifname);\n+\t  return const0_rtx;\n+\t}\n \n-  V4SF_type_node = rs6000_vector_type (\"__vector float\", float_type_node, 4);\n-  ptr_V4SF_type_node\n-    = build_pointer_type (build_qualified_type (V4SF_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n+      rtx platform = gen_reg_rtx (SImode);\n+      rtx address = gen_rtx_PLUS (Pmode,\n+\t\t\t\t  gen_rtx_REG (Pmode, TLS_REGNUM),\n+\t\t\t\t  GEN_INT (TCB_PLATFORM_OFFSET));\n+      rtx tcbmem = gen_const_mem (SImode, address);\n+      emit_move_insn (platform, tcbmem);\n+      emit_insn (gen_eqsi3 (target, platform, cpuid));\n+    }\n+  else if (fcode == RS6000_BIF_CPU_SUPPORTS)\n+    {\n+      const char *hwcap = TREE_STRING_POINTER (arg);\n+      rtx mask = NULL_RTX;\n+      int hwcap_offset;\n+      for (size_t i = 0; i < ARRAY_SIZE (cpu_supports_info); i++)\n+\tif (strcmp (hwcap, cpu_supports_info[i].hwcap) == 0)\n+\t  {\n+\t    mask = GEN_INT (cpu_supports_info[i].mask);\n+\t    hwcap_offset = TCB_HWCAP_OFFSET (cpu_supports_info[i].id);\n+\t    break;\n+\t  }\n+      if (mask == NULL_RTX)\n+\t{\n+\t  /* Invalid HWCAP argument.  */\n+\t  error (\"%s %qs is an invalid argument to builtin %qs\",\n+\t\t \"hwcap\", hwcap,\n+\t\t rs6000_builtin_info_x[(size_t) fcode].bifname);\n+\t  return const0_rtx;\n+\t}\n \n-  V8HI_type_node = rs6000_vector_type (\"__vector signed short\",\n-\t\t\t\t       intHI_type_node, 8);\n-  ptr_V8HI_type_node\n-    = build_pointer_type (build_qualified_type (V8HI_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n+      rtx tcb_hwcap = gen_reg_rtx (SImode);\n+      rtx address = gen_rtx_PLUS (Pmode,\n+\t\t\t\t  gen_rtx_REG (Pmode, TLS_REGNUM),\n+\t\t\t\t  GEN_INT (hwcap_offset));\n+      rtx tcbmem = gen_const_mem (SImode, address);\n+      emit_move_insn (tcb_hwcap, tcbmem);\n+      rtx scratch1 = gen_reg_rtx (SImode);\n+      emit_insn (gen_rtx_SET (scratch1,\n+\t\t\t      gen_rtx_AND (SImode, tcb_hwcap, mask)));\n+      rtx scratch2 = gen_reg_rtx (SImode);\n+      emit_insn (gen_eqsi3 (scratch2, scratch1, const0_rtx));\n+      emit_insn (gen_rtx_SET (target,\n+\t\t\t      gen_rtx_XOR (SImode, scratch2, const1_rtx)));\n+    }\n+  else\n+    gcc_unreachable ();\n \n-  V16QI_type_node = rs6000_vector_type (\"__vector signed char\",\n-\t\t\t\t\tintQI_type_node, 16);\n-  ptr_V16QI_type_node\n-    = build_pointer_type (build_qualified_type (V16QI_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n+  /* Record that we have expanded a CPU builtin, so that we can later\n+     emit a reference to the special symbol exported by LIBC to ensure we\n+     do not link against an old LIBC that doesn't support this feature.  */\n+  cpu_builtin_p = true;\n \n-  unsigned_V16QI_type_node = rs6000_vector_type (\"__vector unsigned char\",\n-\t\t\t\t\tunsigned_intQI_type_node, 16);\n-  ptr_unsigned_V16QI_type_node\n-    = build_pointer_type (build_qualified_type (unsigned_V16QI_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n+#else\n+  warning (0, \"builtin %qs needs GLIBC (2.23 and newer) that exports hardware \"\n+\t   \"capability bits\", rs6000_builtin_info_x[(size_t) fcode].bifname);\n \n-  unsigned_V8HI_type_node = rs6000_vector_type (\"__vector unsigned short\",\n-\t\t\t\t       unsigned_intHI_type_node, 8);\n-  ptr_unsigned_V8HI_type_node\n-    = build_pointer_type (build_qualified_type (unsigned_V8HI_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n+  /* For old LIBCs, always return FALSE.  */\n+  emit_move_insn (target, GEN_INT (0));\n+#endif /* TARGET_LIBC_PROVIDES_HWCAP_IN_TCB */\n \n-  unsigned_V4SI_type_node = rs6000_vector_type (\"__vector unsigned int\",\n-\t\t\t\t       unsigned_intSI_type_node, 4);\n-  ptr_unsigned_V4SI_type_node\n-    = build_pointer_type (build_qualified_type (unsigned_V4SI_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n+  return target;\n+}\n \n-  if (new_builtins_are_live)\n-    unsigned_V2DI_type_node\n-      = rs6000_vector_type (\"__vector unsigned long long\",\n-\t\t\t    long_long_unsigned_type_node, 2);\n-  else\n+/* For the element-reversing load/store built-ins, produce the correct\n+   insn_code depending on the target endianness.  */\n+static insn_code\n+elemrev_icode (rs6000_gen_builtins fcode)\n+{\n+  switch (fcode)\n     {\n-      str = TARGET_POWERPC64\n-\t? \"__vector unsigned long\"\n-\t: \"__vector unsigned long long\";\n-      unsigned_V2DI_type_node\n-\t= rs6000_vector_type (str, long_long_unsigned_type_node, 2);\n-    }\n+    case RS6000_BIF_ST_ELEMREV_V1TI:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_store_v1ti\n+\t\t\t      : CODE_FOR_vsx_st_elemrev_v1ti;\n \n-  ptr_unsigned_V2DI_type_node\n-    = build_pointer_type (build_qualified_type (unsigned_V2DI_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n+    case RS6000_BIF_ST_ELEMREV_V2DF:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_store_v2df\n+\t\t\t      : CODE_FOR_vsx_st_elemrev_v2df;\n \n-  opaque_V4SI_type_node = build_opaque_vector_type (intSI_type_node, 4);\n+    case RS6000_BIF_ST_ELEMREV_V2DI:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_store_v2di\n+\t\t\t      : CODE_FOR_vsx_st_elemrev_v2di;\n \n-  const_str_type_node\n-    = build_pointer_type (build_qualified_type (char_type_node,\n-\t\t\t\t\t\tTYPE_QUAL_CONST));\n+    case RS6000_BIF_ST_ELEMREV_V4SF:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_store_v4sf\n+\t\t\t      : CODE_FOR_vsx_st_elemrev_v4sf;\n \n-  /* We use V1TI mode as a special container to hold __int128_t items that\n-     must live in VSX registers.  */\n-  if (intTI_type_node)\n-    {\n-      V1TI_type_node = rs6000_vector_type (\"__vector __int128\",\n-\t\t\t\t\t   intTI_type_node, 1);\n-      ptr_V1TI_type_node\n-\t= build_pointer_type (build_qualified_type (V1TI_type_node,\n-\t\t\t\t\t\t    TYPE_QUAL_CONST));\n-      unsigned_V1TI_type_node\n-\t= rs6000_vector_type (\"__vector unsigned __int128\",\n-\t\t\t      unsigned_intTI_type_node, 1);\n-      ptr_unsigned_V1TI_type_node\n-\t= build_pointer_type (build_qualified_type (unsigned_V1TI_type_node,\n-\t\t\t\t\t\t    TYPE_QUAL_CONST));\n-    }\n+    case RS6000_BIF_ST_ELEMREV_V4SI:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_store_v4si\n+\t\t\t      : CODE_FOR_vsx_st_elemrev_v4si;\n \n-  /* The 'vector bool ...' types must be kept distinct from 'vector unsigned ...'\n-     types, especially in C++ land.  Similarly, 'vector pixel' is distinct from\n-     'vector unsigned short'.  */\n+    case RS6000_BIF_ST_ELEMREV_V8HI:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_store_v8hi\n+\t\t\t      : CODE_FOR_vsx_st_elemrev_v8hi;\n \n-  bool_char_type_node = build_distinct_type_copy (unsigned_intQI_type_node);\n-  bool_short_type_node = build_distinct_type_copy (unsigned_intHI_type_node);\n-  bool_int_type_node = build_distinct_type_copy (unsigned_intSI_type_node);\n-  bool_long_long_type_node = build_distinct_type_copy (unsigned_intDI_type_node);\n-  pixel_type_node = build_distinct_type_copy (unsigned_intHI_type_node);\n+    case RS6000_BIF_ST_ELEMREV_V16QI:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_store_v16qi\n+\t\t\t      : CODE_FOR_vsx_st_elemrev_v16qi;\n \n-  long_integer_type_internal_node = long_integer_type_node;\n-  long_unsigned_type_internal_node = long_unsigned_type_node;\n-  long_long_integer_type_internal_node = long_long_integer_type_node;\n+    case RS6000_BIF_LD_ELEMREV_V2DF:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_load_v2df\n+\t\t\t      : CODE_FOR_vsx_ld_elemrev_v2df;\n+\n+    case RS6000_BIF_LD_ELEMREV_V1TI:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_load_v1ti\n+\t\t\t      : CODE_FOR_vsx_ld_elemrev_v1ti;\n+\n+    case RS6000_BIF_LD_ELEMREV_V2DI:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_load_v2di\n+\t\t\t      : CODE_FOR_vsx_ld_elemrev_v2di;\n+\n+    case RS6000_BIF_LD_ELEMREV_V4SF:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_load_v4sf\n+\t\t\t      : CODE_FOR_vsx_ld_elemrev_v4sf;\n+\n+    case RS6000_BIF_LD_ELEMREV_V4SI:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_load_v4si\n+\t\t\t      : CODE_FOR_vsx_ld_elemrev_v4si;\n+\n+    case RS6000_BIF_LD_ELEMREV_V8HI:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_load_v8hi\n+\t\t\t      : CODE_FOR_vsx_ld_elemrev_v8hi;\n+\n+    case RS6000_BIF_LD_ELEMREV_V16QI:\n+      return BYTES_BIG_ENDIAN ? CODE_FOR_vsx_load_v16qi\n+\t\t\t      : CODE_FOR_vsx_ld_elemrev_v16qi;\n+    default:\n+      ;\n+    }\n+\n+  gcc_unreachable ();\n+}\n+\n+/* Expand an AltiVec vector load builtin, and return the expanded rtx.  */\n+static rtx\n+ldv_expand_builtin (rtx target, insn_code icode, rtx *op, machine_mode tmode)\n+{\n+  if (target == 0\n+      || GET_MODE (target) != tmode\n+      || !insn_data[icode].operand[0].predicate (target, tmode))\n+    target = gen_reg_rtx (tmode);\n+\n+  op[1] = copy_to_mode_reg (Pmode, op[1]);\n+\n+  /* These CELL built-ins use BLKmode instead of tmode for historical\n+     (i.e., unknown) reasons.  TODO: Is this necessary?  */\n+  bool blk = (icode == CODE_FOR_altivec_lvlx\n+\t      || icode == CODE_FOR_altivec_lvlxl\n+\t      || icode == CODE_FOR_altivec_lvrx\n+\t      || icode == CODE_FOR_altivec_lvrxl);\n+\n+  /* For LVX, express the RTL accurately by ANDing the address with -16.\n+     LVXL and LVE*X expand to use UNSPECs to hide their special behavior,\n+     so the raw address is fine.  */\n+  /* TODO: That statement seems wrong, as the UNSPECs don't surround the\n+     memory expression, so a latent bug may lie here.  The &-16 is likely\n+     needed for all VMX-style loads.  */\n+  if (icode == CODE_FOR_altivec_lvx_v1ti\n+      || icode == CODE_FOR_altivec_lvx_v2df\n+      || icode == CODE_FOR_altivec_lvx_v2di\n+      || icode == CODE_FOR_altivec_lvx_v4sf\n+      || icode == CODE_FOR_altivec_lvx_v4si\n+      || icode == CODE_FOR_altivec_lvx_v8hi\n+      || icode == CODE_FOR_altivec_lvx_v16qi)\n+    {\n+      rtx rawaddr;\n+      if (op[0] == const0_rtx)\n+\trawaddr = op[1];\n+      else\n+\t{\n+\t  op[0] = copy_to_mode_reg (Pmode, op[0]);\n+\t  rawaddr = gen_rtx_PLUS (Pmode, op[1], op[0]);\n+\t}\n+      rtx addr = gen_rtx_AND (Pmode, rawaddr, gen_rtx_CONST_INT (Pmode, -16));\n+      addr = gen_rtx_MEM (blk ? BLKmode : tmode, addr);\n+\n+      emit_insn (gen_rtx_SET (target, addr));\n+    }\n+  else\n+    {\n+      rtx addr;\n+      if (op[0] == const0_rtx)\n+\taddr = gen_rtx_MEM (blk ? BLKmode : tmode, op[1]);\n+      else\n+\t{\n+\t  op[0] = copy_to_mode_reg (Pmode, op[0]);\n+\t  addr = gen_rtx_MEM (blk ? BLKmode : tmode,\n+\t\t\t      gen_rtx_PLUS (Pmode, op[1], op[0]));\n+\t}\n+\n+      rtx pat = GEN_FCN (icode) (target, addr);\n+      if (!pat)\n+\treturn 0;\n+      emit_insn (pat);\n+    }\n+\n+  return target;\n+}\n+\n+/* Expand a builtin function that loads a scalar into a vector register\n+   with sign extension, and return the expanded rtx.  */\n+static rtx\n+lxvrse_expand_builtin (rtx target, insn_code icode, rtx *op,\n+\t\t       machine_mode tmode, machine_mode smode)\n+{\n+  rtx pat, addr;\n+  op[1] = copy_to_mode_reg (Pmode, op[1]);\n+\n+  if (op[0] == const0_rtx)\n+    addr = gen_rtx_MEM (tmode, op[1]);\n+  else\n+    {\n+      op[0] = copy_to_mode_reg (Pmode, op[0]);\n+      addr = gen_rtx_MEM (smode,\n+\t\t\t  gen_rtx_PLUS (Pmode, op[1], op[0]));\n+    }\n+\n+  rtx discratch = gen_reg_rtx (DImode);\n+  rtx tiscratch = gen_reg_rtx (TImode);\n+\n+  /* Emit the lxvr*x insn.  */\n+  pat = GEN_FCN (icode) (tiscratch, addr);\n+  if (!pat)\n+    return 0;\n+  emit_insn (pat);\n+\n+  /* Emit a sign extension from QI,HI,WI to double (DI).  */\n+  rtx scratch = gen_lowpart (smode, tiscratch);\n+  if (icode == CODE_FOR_vsx_lxvrbx)\n+    emit_insn (gen_extendqidi2 (discratch, scratch));\n+  else if (icode == CODE_FOR_vsx_lxvrhx)\n+    emit_insn (gen_extendhidi2 (discratch, scratch));\n+  else if (icode == CODE_FOR_vsx_lxvrwx)\n+    emit_insn (gen_extendsidi2 (discratch, scratch));\n+  /*  Assign discratch directly if scratch is already DI.  */\n+  if (icode == CODE_FOR_vsx_lxvrdx)\n+    discratch = scratch;\n+\n+  /* Emit the sign extension from DI (double) to TI (quad).  */\n+  emit_insn (gen_extendditi2 (target, discratch));\n+\n+  return target;\n+}\n+\n+/* Expand a builtin function that loads a scalar into a vector register\n+   with zero extension, and return the expanded rtx.  */\n+static rtx\n+lxvrze_expand_builtin (rtx target, insn_code icode, rtx *op,\n+\t\t       machine_mode tmode, machine_mode smode)\n+{\n+  rtx pat, addr;\n+  op[1] = copy_to_mode_reg (Pmode, op[1]);\n+\n+  if (op[0] == const0_rtx)\n+    addr = gen_rtx_MEM (tmode, op[1]);\n+  else\n+    {\n+      op[0] = copy_to_mode_reg (Pmode, op[0]);\n+      addr = gen_rtx_MEM (smode,\n+\t\t\t  gen_rtx_PLUS (Pmode, op[1], op[0]));\n+    }\n+\n+  pat = GEN_FCN (icode) (target, addr);\n+  if (!pat)\n+    return 0;\n+  emit_insn (pat);\n+  return target;\n+}\n+\n+/* Expand an AltiVec vector store builtin, and return the expanded rtx.  */\n+static rtx\n+stv_expand_builtin (insn_code icode, rtx *op,\n+\t\t    machine_mode tmode, machine_mode smode)\n+{\n+  op[2] = copy_to_mode_reg (Pmode, op[2]);\n+\n+  /* For STVX, express the RTL accurately by ANDing the address with -16.\n+     STVXL and STVE*X expand to use UNSPECs to hide their special behavior,\n+     so the raw address is fine.  */\n+  /* TODO: That statement seems wrong, as the UNSPECs don't surround the\n+     memory expression, so a latent bug may lie here.  The &-16 is likely\n+     needed for all VMX-style stores.  */\n+  if (icode == CODE_FOR_altivec_stvx_v2df\n+      || icode == CODE_FOR_altivec_stvx_v2di\n+      || icode == CODE_FOR_altivec_stvx_v4sf\n+      || icode == CODE_FOR_altivec_stvx_v4si\n+      || icode == CODE_FOR_altivec_stvx_v8hi\n+      || icode == CODE_FOR_altivec_stvx_v16qi)\n+    {\n+      rtx rawaddr;\n+      if (op[1] == const0_rtx)\n+\trawaddr = op[2];\n+      else\n+\t{\n+\t  op[1] = copy_to_mode_reg (Pmode, op[1]);\n+\t  rawaddr = gen_rtx_PLUS (Pmode, op[2], op[1]);\n+\t}\n+\n+      rtx addr = gen_rtx_AND (Pmode, rawaddr, gen_rtx_CONST_INT (Pmode, -16));\n+      addr = gen_rtx_MEM (tmode, addr);\n+      op[0] = copy_to_mode_reg (tmode, op[0]);\n+      emit_insn (gen_rtx_SET (addr, op[0]));\n+    }\n+  else if (icode == CODE_FOR_vsx_stxvrbx\n+\t   || icode == CODE_FOR_vsx_stxvrhx\n+\t   || icode == CODE_FOR_vsx_stxvrwx\n+\t   || icode == CODE_FOR_vsx_stxvrdx)\n+    {\n+      rtx truncrtx = gen_rtx_TRUNCATE (tmode, op[0]);\n+      op[0] = copy_to_mode_reg (E_TImode, truncrtx);\n+\n+      rtx addr;\n+      if (op[1] == const0_rtx)\n+\taddr = gen_rtx_MEM (Pmode, op[2]);\n+      else\n+\t{\n+\t  op[1] = copy_to_mode_reg (Pmode, op[1]);\n+\t  addr = gen_rtx_MEM (tmode, gen_rtx_PLUS (Pmode, op[2], op[1]));\n+\t}\n+      rtx pat = GEN_FCN (icode) (addr, op[0]);\n+      if (pat)\n+\temit_insn (pat);\n+    }\n+  else\n+    {\n+      if (!insn_data[icode].operand[1].predicate (op[0], smode))\n+\top[0] = copy_to_mode_reg (smode, op[0]);\n+\n+      rtx addr;\n+      if (op[1] == const0_rtx)\n+\taddr = gen_rtx_MEM (tmode, op[2]);\n+      else\n+\t{\n+\t  op[1] = copy_to_mode_reg (Pmode, op[1]);\n+\t  addr = gen_rtx_MEM (tmode, gen_rtx_PLUS (Pmode, op[2], op[1]));\n+\t}\n+\n+      rtx pat = GEN_FCN (icode) (addr, op[0]);\n+      if (pat)\n+\temit_insn (pat);\n+    }\n+\n+  return NULL_RTX;\n+}\n+\n+/* Expand the MMA built-in in EXP, and return it.  */\n+static rtx\n+new_mma_expand_builtin (tree exp, rtx target, insn_code icode,\n+\t\t\trs6000_gen_builtins fcode)\n+{\n+  tree fndecl = TREE_OPERAND (CALL_EXPR_FN (exp), 0);\n+  bool void_func = TREE_TYPE (TREE_TYPE (fndecl)) == void_type_node;\n+  machine_mode tmode = VOIDmode;\n+  rtx op[MAX_MMA_OPERANDS];\n+  unsigned nopnds = 0;\n+\n+  if (!void_func)\n+    {\n+      tmode = insn_data[icode].operand[0].mode;\n+      if (!(target\n+\t    && GET_MODE (target) == tmode\n+\t    && insn_data[icode].operand[0].predicate (target, tmode)))\n+\ttarget = gen_reg_rtx (tmode);\n+      op[nopnds++] = target;\n+    }\n+  else\n+    target = const0_rtx;\n+\n+  call_expr_arg_iterator iter;\n+  tree arg;\n+  FOR_EACH_CALL_EXPR_ARG (arg, iter, exp)\n+    {\n+      if (arg == error_mark_node)\n+\treturn const0_rtx;\n+\n+      rtx opnd;\n+      const struct insn_operand_data *insn_op;\n+      insn_op = &insn_data[icode].operand[nopnds];\n+      if (TREE_CODE (arg) == ADDR_EXPR\n+\t  && MEM_P (DECL_RTL (TREE_OPERAND (arg, 0))))\n+\topnd = DECL_RTL (TREE_OPERAND (arg, 0));\n+      else\n+\topnd = expand_normal (arg);\n+\n+      if (!insn_op->predicate (opnd, insn_op->mode))\n+\t{\n+\t  /* TODO: This use of constraints needs explanation.  */\n+\t  if (!strcmp (insn_op->constraint, \"n\"))\n+\t    {\n+\t      if (!CONST_INT_P (opnd))\n+\t\terror (\"argument %d must be an unsigned literal\", nopnds);\n+\t      else\n+\t\terror (\"argument %d is an unsigned literal that is \"\n+\t\t       \"out of range\", nopnds);\n+\t      return const0_rtx;\n+\t    }\n+\t  opnd = copy_to_mode_reg (insn_op->mode, opnd);\n+\t}\n+\n+      /* Some MMA instructions have INOUT accumulator operands, so force\n+\t their target register to be the same as their input register.  */\n+      if (!void_func\n+\t  && nopnds == 1\n+\t  && !strcmp (insn_op->constraint, \"0\")\n+\t  && insn_op->mode == tmode\n+\t  && REG_P (opnd)\n+\t  && insn_data[icode].operand[0].predicate (opnd, tmode))\n+\ttarget = op[0] = opnd;\n+\n+      op[nopnds++] = opnd;\n+    }\n+\n+  rtx pat;\n+  switch (nopnds)\n+    {\n+    case 1:\n+      pat = GEN_FCN (icode) (op[0]);\n+      break;\n+    case 2:\n+      pat = GEN_FCN (icode) (op[0], op[1]);\n+      break;\n+    case 3:\n+      /* The ASSEMBLE builtin source operands are reversed in little-endian\n+\t mode, so reorder them.  */\n+      if (fcode == RS6000_BIF_ASSEMBLE_PAIR_V_INTERNAL && !WORDS_BIG_ENDIAN)\n+\tstd::swap (op[1], op[2]);\n+      pat = GEN_FCN (icode) (op[0], op[1], op[2]);\n+      break;\n+    case 4:\n+      pat = GEN_FCN (icode) (op[0], op[1], op[2], op[3]);\n+      break;\n+    case 5:\n+      /* The ASSEMBLE builtin source operands are reversed in little-endian\n+\t mode, so reorder them.  */\n+      if (fcode == RS6000_BIF_ASSEMBLE_ACC_INTERNAL && !WORDS_BIG_ENDIAN)\n+\t{\n+\t  std::swap (op[1], op[4]);\n+\t  std::swap (op[2], op[3]);\n+\t}\n+      pat = GEN_FCN (icode) (op[0], op[1], op[2], op[3], op[4]);\n+      break;\n+    case 6:\n+      pat = GEN_FCN (icode) (op[0], op[1], op[2], op[3], op[4], op[5]);\n+      break;\n+    case 7:\n+      pat = GEN_FCN (icode) (op[0], op[1], op[2], op[3], op[4], op[5], op[6]);\n+      break;\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+  if (!pat)\n+    return NULL_RTX;\n+\n+  emit_insn (pat);\n+  return target;\n+}\n+\n+/* Return the appropriate SPR number associated with the given builtin.  */\n+static inline HOST_WIDE_INT\n+new_htm_spr_num (enum rs6000_gen_builtins code)\n+{\n+  if (code == RS6000_BIF_GET_TFHAR\n+      || code == RS6000_BIF_SET_TFHAR)\n+    return TFHAR_SPR;\n+  else if (code == RS6000_BIF_GET_TFIAR\n+\t   || code == RS6000_BIF_SET_TFIAR)\n+    return TFIAR_SPR;\n+  else if (code == RS6000_BIF_GET_TEXASR\n+\t   || code == RS6000_BIF_SET_TEXASR)\n+    return TEXASR_SPR;\n+  gcc_assert (code == RS6000_BIF_GET_TEXASRU\n+\t      || code == RS6000_BIF_SET_TEXASRU);\n+  return TEXASRU_SPR;\n+}\n+\n+/* Expand the HTM builtin in EXP and store the result in TARGET.\n+   Return the expanded rtx.  */\n+static rtx\n+new_htm_expand_builtin (bifdata *bifaddr, rs6000_gen_builtins fcode,\n+\t\t\ttree exp, rtx target)\n+{\n+  if (!TARGET_POWERPC64\n+      && (fcode == RS6000_BIF_TABORTDC\n+\t  || fcode == RS6000_BIF_TABORTDCI))\n+    {\n+      error (\"builtin %qs is only valid in 64-bit mode\", bifaddr->bifname);\n+      return const0_rtx;\n+    }\n+\n+  tree fndecl = TREE_OPERAND (CALL_EXPR_FN (exp), 0);\n+  bool nonvoid = TREE_TYPE (TREE_TYPE (fndecl)) != void_type_node;\n+  bool uses_spr = bif_is_htmspr (*bifaddr);\n+  insn_code icode = bifaddr->icode;\n+\n+  if (uses_spr)\n+    icode = rs6000_htm_spr_icode (nonvoid);\n+\n+  rtx op[MAX_HTM_OPERANDS];\n+  int nopnds = 0;\n+  const insn_operand_data *insn_op = &insn_data[icode].operand[0];\n+\n+  if (nonvoid)\n+    {\n+      machine_mode tmode = (uses_spr) ? insn_op->mode : E_SImode;\n+      if (!target\n+\t  || GET_MODE (target) != tmode\n+\t  || (uses_spr && !insn_op->predicate (target, tmode)))\n+\ttarget = gen_reg_rtx (tmode);\n+      if (uses_spr)\n+\top[nopnds++] = target;\n+    }\n+\n+  tree arg;\n+  call_expr_arg_iterator iter;\n+\n+  FOR_EACH_CALL_EXPR_ARG (arg, iter, exp)\n+    {\n+      if (arg == error_mark_node || nopnds >= MAX_HTM_OPERANDS)\n+\treturn const0_rtx;\n+\n+      insn_op = &insn_data[icode].operand[nopnds];\n+      op[nopnds] = expand_normal (arg);\n+\n+      if (!insn_op->predicate (op[nopnds], insn_op->mode))\n+\t{\n+\t  /* TODO: This use of constraints could use explanation.\n+\t     This happens a couple of places, perhaps make that a\n+\t     function to document what's happening.  */\n+\t  if (!strcmp (insn_op->constraint, \"n\"))\n+\t    {\n+\t      int arg_num = nonvoid ? nopnds : nopnds + 1;\n+\t      if (!CONST_INT_P (op[nopnds]))\n+\t\terror (\"argument %d must be an unsigned literal\", arg_num);\n+\t      else\n+\t\terror (\"argument %d is an unsigned literal that is \"\n+\t\t       \"out of range\", arg_num);\n+\t      return const0_rtx;\n+\t    }\n+\t  op[nopnds] = copy_to_mode_reg (insn_op->mode, op[nopnds]);\n+\t}\n+\n+      nopnds++;\n+    }\n+\n+  /* Handle the builtins for extended mnemonics.  These accept\n+     no arguments, but map to builtins that take arguments.  */\n+  switch (fcode)\n+    {\n+    case RS6000_BIF_TENDALL:  /* Alias for: tend. 1  */\n+    case RS6000_BIF_TRESUME:  /* Alias for: tsr. 1  */\n+      op[nopnds++] = GEN_INT (1);\n+      break;\n+    case RS6000_BIF_TSUSPEND: /* Alias for: tsr. 0  */\n+      op[nopnds++] = GEN_INT (0);\n+      break;\n+    default:\n+      break;\n+    }\n+\n+  /* If this builtin accesses SPRs, then pass in the appropriate\n+     SPR number and SPR regno as the last two operands.  */\n+  rtx cr = NULL_RTX;\n+  if (uses_spr)\n+    {\n+      machine_mode mode = TARGET_POWERPC64 ? DImode : SImode;\n+      op[nopnds++] = gen_rtx_CONST_INT (mode, new_htm_spr_num (fcode));\n+    }\n+  /* If this builtin accesses a CR field, then pass in a scratch\n+     CR field as the last operand.  */\n+  else if (bif_is_htmcr (*bifaddr))\n+    {\n+      cr = gen_reg_rtx (CCmode);\n+      op[nopnds++] = cr;\n+    }\n+\n+  rtx pat;\n+  switch (nopnds)\n+    {\n+    case 1:\n+      pat = GEN_FCN (icode) (op[0]);\n+      break;\n+    case 2:\n+      pat = GEN_FCN (icode) (op[0], op[1]);\n+      break;\n+    case 3:\n+      pat = GEN_FCN (icode) (op[0], op[1], op[2]);\n+      break;\n+    case 4:\n+      pat = GEN_FCN (icode) (op[0], op[1], op[2], op[3]);\n+      break;\n+    default:\n+      gcc_unreachable ();\n+    }\n+  if (!pat)\n+    return NULL_RTX;\n+  emit_insn (pat);\n+\n+  if (bif_is_htmcr (*bifaddr))\n+    {\n+      if (fcode == RS6000_BIF_TBEGIN)\n+\t{\n+\t  /* Emit code to set TARGET to true or false depending on\n+\t     whether the tbegin. instruction succeeded or failed\n+\t     to start a transaction.  We do this by placing the 1's\n+\t     complement of CR's EQ bit into TARGET.  */\n+\t  rtx scratch = gen_reg_rtx (SImode);\n+\t  emit_insn (gen_rtx_SET (scratch,\n+\t\t\t\t  gen_rtx_EQ (SImode, cr,\n+\t\t\t\t\t      const0_rtx)));\n+\t  emit_insn (gen_rtx_SET (target,\n+\t\t\t\t  gen_rtx_XOR (SImode, scratch,\n+\t\t\t\t\t       GEN_INT (1))));\n+\t}\n+      else\n+\t{\n+\t  /* Emit code to copy the 4-bit condition register field\n+\t     CR into the least significant end of register TARGET.  */\n+\t  rtx scratch1 = gen_reg_rtx (SImode);\n+\t  rtx scratch2 = gen_reg_rtx (SImode);\n+\t  rtx subreg = simplify_gen_subreg (CCmode, scratch1, SImode, 0);\n+\t  emit_insn (gen_movcc (subreg, cr));\n+\t  emit_insn (gen_lshrsi3 (scratch2, scratch1, GEN_INT (28)));\n+\t  emit_insn (gen_andsi3 (target, scratch2, GEN_INT (0xf)));\n+\t}\n+    }\n+\n+  if (nonvoid)\n+    return target;\n+  return const0_rtx;\n+}\n+\n+/* Expand an expression EXP that calls a built-in function,\n+   with result going to TARGET if that's convenient\n+   (and in mode MODE if that's convenient).\n+   SUBTARGET may be used as the target for computing one of EXP's operands.\n+   IGNORE is nonzero if the value is to be ignored.\n+   Use the new builtin infrastructure.  */\n+static rtx\n+rs6000_expand_new_builtin (tree exp, rtx target,\n+\t\t\t   rtx /* subtarget */,\n+\t\t\t   machine_mode /* mode */,\n+\t\t\t   int ignore)\n+{\n+  tree fndecl = TREE_OPERAND (CALL_EXPR_FN (exp), 0);\n+  enum rs6000_gen_builtins fcode\n+    = (enum rs6000_gen_builtins) DECL_MD_FUNCTION_CODE (fndecl);\n+  size_t uns_fcode = (size_t)fcode;\n+  enum insn_code icode = rs6000_builtin_info_x[uns_fcode].icode;\n+\n+  /* TODO: The following commentary and code is inherited from the original\n+     builtin processing code.  The commentary is a bit confusing, with the\n+     intent being that KFmode is always IEEE-128, IFmode is always IBM\n+     double-double, and TFmode is the current long double.  The code is\n+     confusing in that it converts from KFmode to TFmode pattern names,\n+     when the other direction is more intuitive.  Try to address this.  */\n+\n+  /* We have two different modes (KFmode, TFmode) that are the IEEE\n+     128-bit floating point type, depending on whether long double is the\n+     IBM extended double (KFmode) or long double is IEEE 128-bit (TFmode).\n+     It is simpler if we only define one variant of the built-in function,\n+     and switch the code when defining it, rather than defining two built-\n+     ins and using the overload table in rs6000-c.c to switch between the\n+     two.  If we don't have the proper assembler, don't do this switch\n+     because CODE_FOR_*kf* and CODE_FOR_*tf* will be CODE_FOR_nothing.  */\n+  if (FLOAT128_IEEE_P (TFmode))\n+    switch (icode)\n+      {\n+      case CODE_FOR_sqrtkf2_odd:\n+\ticode = CODE_FOR_sqrttf2_odd;\n+\tbreak;\n+      case CODE_FOR_trunckfdf2_odd:\n+\ticode = CODE_FOR_trunctfdf2_odd;\n+\tbreak;\n+      case CODE_FOR_addkf3_odd:\n+\ticode = CODE_FOR_addtf3_odd;\n+\tbreak;\n+      case CODE_FOR_subkf3_odd:\n+\ticode = CODE_FOR_subtf3_odd;\n+\tbreak;\n+      case CODE_FOR_mulkf3_odd:\n+\ticode = CODE_FOR_multf3_odd;\n+\tbreak;\n+      case CODE_FOR_divkf3_odd:\n+\ticode = CODE_FOR_divtf3_odd;\n+\tbreak;\n+      case CODE_FOR_fmakf4_odd:\n+\ticode = CODE_FOR_fmatf4_odd;\n+\tbreak;\n+      case CODE_FOR_xsxexpqp_kf:\n+\ticode = CODE_FOR_xsxexpqp_tf;\n+\tbreak;\n+      case CODE_FOR_xsxsigqp_kf:\n+\ticode = CODE_FOR_xsxsigqp_tf;\n+\tbreak;\n+      case CODE_FOR_xststdcnegqp_kf:\n+\ticode = CODE_FOR_xststdcnegqp_tf;\n+\tbreak;\n+      case CODE_FOR_xsiexpqp_kf:\n+\ticode = CODE_FOR_xsiexpqp_tf;\n+\tbreak;\n+      case CODE_FOR_xsiexpqpf_kf:\n+\ticode = CODE_FOR_xsiexpqpf_tf;\n+\tbreak;\n+      case CODE_FOR_xststdcqp_kf:\n+\ticode = CODE_FOR_xststdcqp_tf;\n+\tbreak;\n+      case CODE_FOR_xscmpexpqp_eq_kf:\n+\ticode = CODE_FOR_xscmpexpqp_eq_tf;\n+\tbreak;\n+      case CODE_FOR_xscmpexpqp_lt_kf:\n+\ticode = CODE_FOR_xscmpexpqp_lt_tf;\n+\tbreak;\n+      case CODE_FOR_xscmpexpqp_gt_kf:\n+\ticode = CODE_FOR_xscmpexpqp_gt_tf;\n+\tbreak;\n+      case CODE_FOR_xscmpexpqp_unordered_kf:\n+\ticode = CODE_FOR_xscmpexpqp_unordered_tf;\n+\tbreak;\n+      default:\n+\tbreak;\n+      }\n+\n+  /* In case of \"#pragma target\" changes, we initialize all builtins\n+     but check for actual availability now, during expand time.  For\n+     invalid builtins, generate a normal call.  */\n+  bifdata *bifaddr = &rs6000_builtin_info_x[uns_fcode];\n+  bif_enable e = bifaddr->enable;\n+\n+  if (!(e == ENB_ALWAYS\n+\t|| (e == ENB_P5         && TARGET_POPCNTB)\n+\t|| (e == ENB_P6         && TARGET_CMPB)\n+\t|| (e == ENB_ALTIVEC    && TARGET_ALTIVEC)\n+\t|| (e == ENB_CELL       && TARGET_ALTIVEC\n+\t\t\t\t&& rs6000_cpu == PROCESSOR_CELL)\n+\t|| (e == ENB_VSX        && TARGET_VSX)\n+\t|| (e == ENB_P7         && TARGET_POPCNTD)\n+\t|| (e == ENB_P7_64      && TARGET_POPCNTD\n+\t\t\t\t&& TARGET_POWERPC64)\n+\t|| (e == ENB_P8         && TARGET_DIRECT_MOVE)\n+\t|| (e == ENB_P8V        && TARGET_P8_VECTOR)\n+\t|| (e == ENB_P9         && TARGET_MODULO)\n+\t|| (e == ENB_P9_64      && TARGET_MODULO\n+\t\t\t\t&& TARGET_POWERPC64)\n+\t|| (e == ENB_P9V        && TARGET_P9_VECTOR)\n+\t|| (e == ENB_IEEE128_HW && TARGET_FLOAT128_HW)\n+\t|| (e == ENB_DFP        && TARGET_DFP)\n+\t|| (e == ENB_CRYPTO     && TARGET_CRYPTO)\n+\t|| (e == ENB_HTM        && TARGET_HTM)\n+\t|| (e == ENB_P10        && TARGET_POWER10)\n+\t|| (e == ENB_P10_64     && TARGET_POWER10\n+\t\t\t\t&& TARGET_POWERPC64)\n+\t|| (e == ENB_MMA        && TARGET_MMA)))\n+    {\n+      rs6000_invalid_new_builtin (fcode);\n+      return expand_call (exp, target, ignore);\n+    }\n+\n+  if (bif_is_nosoft (*bifaddr)\n+      && rs6000_isa_flags & OPTION_MASK_SOFT_FLOAT)\n+    {\n+      error (\"%<%s%> not supported with %<-msoft-float%>\",\n+\t     bifaddr->bifname);\n+      return const0_rtx;\n+    }\n+\n+  if (bif_is_no32bit (*bifaddr) && TARGET_32BIT)\n+    fatal_error (input_location,\n+\t\t \"%<%s%> is not supported in 32-bit mode\",\n+\t\t bifaddr->bifname);\n+\n+  if (bif_is_cpu (*bifaddr))\n+    return new_cpu_expand_builtin (fcode, exp, target);\n+\n+  if (bif_is_init (*bifaddr))\n+    return altivec_expand_vec_init_builtin (TREE_TYPE (exp), exp, target);\n+\n+  if (bif_is_set (*bifaddr))\n+    return altivec_expand_vec_set_builtin (exp);\n+\n+  if (bif_is_extract (*bifaddr))\n+    return altivec_expand_vec_ext_builtin (exp, target);\n+\n+  if (bif_is_predicate (*bifaddr))\n+    return altivec_expand_predicate_builtin (icode, exp, target);\n+\n+  if (bif_is_htm (*bifaddr))\n+    return new_htm_expand_builtin (bifaddr, fcode, exp, target);\n+\n+  if (bif_is_32bit (*bifaddr) && TARGET_32BIT)\n+    {\n+      if (fcode == RS6000_BIF_MFTB)\n+\ticode = CODE_FOR_rs6000_mftb_si;\n+      else\n+\tgcc_unreachable ();\n+    }\n+\n+  if (bif_is_endian (*bifaddr) && BYTES_BIG_ENDIAN)\n+    {\n+      if (fcode == RS6000_BIF_LD_ELEMREV_V1TI)\n+\ticode = CODE_FOR_vsx_load_v1ti;\n+      else if (fcode == RS6000_BIF_LD_ELEMREV_V2DF)\n+\ticode = CODE_FOR_vsx_load_v2df;\n+      else if (fcode == RS6000_BIF_LD_ELEMREV_V2DI)\n+\ticode = CODE_FOR_vsx_load_v2di;\n+      else if (fcode == RS6000_BIF_LD_ELEMREV_V4SF)\n+\ticode = CODE_FOR_vsx_load_v4sf;\n+      else if (fcode == RS6000_BIF_LD_ELEMREV_V4SI)\n+\ticode = CODE_FOR_vsx_load_v4si;\n+      else if (fcode == RS6000_BIF_LD_ELEMREV_V8HI)\n+\ticode = CODE_FOR_vsx_load_v8hi;\n+      else if (fcode == RS6000_BIF_LD_ELEMREV_V16QI)\n+\ticode = CODE_FOR_vsx_load_v16qi;\n+      else if (fcode == RS6000_BIF_ST_ELEMREV_V1TI)\n+\ticode = CODE_FOR_vsx_store_v1ti;\n+      else if (fcode == RS6000_BIF_ST_ELEMREV_V2DF)\n+\ticode = CODE_FOR_vsx_store_v2df;\n+      else if (fcode == RS6000_BIF_ST_ELEMREV_V2DI)\n+\ticode = CODE_FOR_vsx_store_v2di;\n+      else if (fcode == RS6000_BIF_ST_ELEMREV_V4SF)\n+\ticode = CODE_FOR_vsx_store_v4sf;\n+      else if (fcode == RS6000_BIF_ST_ELEMREV_V4SI)\n+\ticode = CODE_FOR_vsx_store_v4si;\n+      else if (fcode == RS6000_BIF_ST_ELEMREV_V8HI)\n+\ticode = CODE_FOR_vsx_store_v8hi;\n+      else if (fcode == RS6000_BIF_ST_ELEMREV_V16QI)\n+\ticode = CODE_FOR_vsx_store_v16qi;\n+      else\n+\tgcc_unreachable ();\n+    }\n+\n+\n+  /* TRUE iff the built-in function returns void.  */\n+  bool void_func = TREE_TYPE (TREE_TYPE (fndecl)) == void_type_node;\n+  /* Position of first argument (0 for void-returning functions, else 1).  */\n+  int k;\n+  /* Modes for the return value, if any, and arguments.  */\n+  const int MAX_BUILTIN_ARGS = 6;\n+  machine_mode mode[MAX_BUILTIN_ARGS + 1];\n+\n+  if (void_func)\n+    k = 0;\n+  else\n+    {\n+      k = 1;\n+      mode[0] = insn_data[icode].operand[0].mode;\n+    }\n+\n+  /* Tree expressions for each argument.  */\n+  tree arg[MAX_BUILTIN_ARGS];\n+  /* RTL expressions for each argument.  */\n+  rtx op[MAX_BUILTIN_ARGS];\n+\n+  int nargs = bifaddr->nargs;\n+  gcc_assert (nargs <= MAX_BUILTIN_ARGS);\n+\n+\n+  for (int i = 0; i < nargs; i++)\n+    {\n+      arg[i] = CALL_EXPR_ARG (exp, i);\n+      if (arg[i] == error_mark_node)\n+\treturn const0_rtx;\n+      STRIP_NOPS (arg[i]);\n+      op[i] = expand_normal (arg[i]);\n+      /* We have a couple of pesky patterns that don't specify the mode...  */\n+      mode[i+k] = insn_data[icode].operand[i+k].mode;\n+      if (!mode[i+k])\n+\tmode[i+k] = Pmode;\n+    }\n+\n+  /* Check for restricted constant arguments.  */\n+  for (int i = 0; i < 2; i++)\n+    {\n+      switch (bifaddr->restr[i])\n+\t{\n+\tcase RES_BITS:\n+\t  {\n+\t    size_t mask = 1;\n+\t    mask <<= bifaddr->restr_val1[i];\n+\t    mask--;\n+\t    tree restr_arg = arg[bifaddr->restr_opnd[i] - 1];\n+\t    STRIP_NOPS (restr_arg);\n+\t    if (!(TREE_CODE (restr_arg) == INTEGER_CST\n+\t\t  && (TREE_INT_CST_LOW (restr_arg) & ~mask) == 0))\n+\t      {\n+\t\terror (\"argument %d must be a %d-bit unsigned literal\",\n+\t\t       bifaddr->restr_opnd[i], bifaddr->restr_val1[i]);\n+\t\treturn CONST0_RTX (mode[0]);\n+\t      }\n+\t    break;\n+\t  }\n+\tcase RES_RANGE:\n+\t  {\n+\t    tree restr_arg = arg[bifaddr->restr_opnd[i] - 1];\n+\t    STRIP_NOPS (restr_arg);\n+\t    if (!(TREE_CODE (restr_arg) == INTEGER_CST\n+\t\t  && IN_RANGE (tree_to_shwi (restr_arg),\n+\t\t\t       bifaddr->restr_val1[i],\n+\t\t\t       bifaddr->restr_val2[i])))\n+\t      {\n+\t\terror (\"argument %d must be a literal between %d and %d,\"\n+\t\t       \" inclusive\",\n+\t\t       bifaddr->restr_opnd[i], bifaddr->restr_val1[i],\n+\t\t       bifaddr->restr_val2[i]);\n+\t\treturn CONST0_RTX (mode[0]);\n+\t      }\n+\t    break;\n+\t  }\n+\tcase RES_VAR_RANGE:\n+\t  {\n+\t    tree restr_arg = arg[bifaddr->restr_opnd[i] - 1];\n+\t    STRIP_NOPS (restr_arg);\n+\t    if (TREE_CODE (restr_arg) == INTEGER_CST\n+\t\t&& !IN_RANGE (tree_to_shwi (restr_arg),\n+\t\t\t      bifaddr->restr_val1[i],\n+\t\t\t      bifaddr->restr_val2[i]))\n+\t      {\n+\t\terror (\"argument %d must be a variable or a literal \"\n+\t\t       \"between %d and %d, inclusive\",\n+\t\t       bifaddr->restr_opnd[i], bifaddr->restr_val1[i],\n+\t\t       bifaddr->restr_val2[i]);\n+\t\treturn CONST0_RTX (mode[0]);\n+\t      }\n+\t    break;\n+\t  }\n+\tcase RES_VALUES:\n+\t  {\n+\t    tree restr_arg = arg[bifaddr->restr_opnd[i] - 1];\n+\t    STRIP_NOPS (restr_arg);\n+\t    if (!(TREE_CODE (restr_arg) == INTEGER_CST\n+\t\t  && (tree_to_shwi (restr_arg) == bifaddr->restr_val1[i]\n+\t\t      || tree_to_shwi (restr_arg) == bifaddr->restr_val2[i])))\n+\t      {\n+\t\terror (\"argument %d must be either a literal %d or a \"\n+\t\t       \"literal %d\",\n+\t\t       bifaddr->restr_opnd[i], bifaddr->restr_val1[i],\n+\t\t       bifaddr->restr_val2[i]);\n+\t\treturn CONST0_RTX (mode[0]);\n+\t      }\n+\t    break;\n+\t  }\n+\tdefault:\n+\tcase RES_NONE:\n+\t  break;\n+\t}\n+    }\n+\n+  if (bif_is_ldstmask (*bifaddr))\n+    return rs6000_expand_ldst_mask (target, arg[0]);\n+\n+  if (bif_is_stvec (*bifaddr))\n+    {\n+      if (bif_is_reve (*bifaddr))\n+\ticode = elemrev_icode (fcode);\n+      return stv_expand_builtin (icode, op, mode[0], mode[1]);\n+    }\n+\n+  if (bif_is_ldvec (*bifaddr))\n+    {\n+      if (bif_is_reve (*bifaddr))\n+\ticode = elemrev_icode (fcode);\n+      return ldv_expand_builtin (target, icode, op, mode[0]);\n+    }\n+\n+  if (bif_is_lxvrse (*bifaddr))\n+    return lxvrse_expand_builtin (target, icode, op, mode[0], mode[1]);\n+\n+  if (bif_is_lxvrze (*bifaddr))\n+    return lxvrze_expand_builtin (target, icode, op, mode[0], mode[1]);\n+\n+  if (bif_is_mma (*bifaddr))\n+    return new_mma_expand_builtin (exp, target, icode, fcode);\n+\n+  if (fcode == RS6000_BIF_PACK_IF\n+      && TARGET_LONG_DOUBLE_128\n+      && !TARGET_IEEEQUAD)\n+    {\n+      icode = CODE_FOR_packtf;\n+      fcode = RS6000_BIF_PACK_TF;\n+      uns_fcode = (size_t) fcode;\n+    }\n+  else if (fcode == RS6000_BIF_UNPACK_IF\n+\t   && TARGET_LONG_DOUBLE_128\n+\t   && !TARGET_IEEEQUAD)\n+    {\n+      icode = CODE_FOR_unpacktf;\n+      fcode = RS6000_BIF_UNPACK_TF;\n+      uns_fcode = (size_t) fcode;\n+    }\n+\n+  if (TREE_TYPE (TREE_TYPE (fndecl)) == void_type_node)\n+    target = NULL_RTX;\n+  else if (target == 0\n+\t   || GET_MODE (target) != mode[0]\n+\t   || !insn_data[icode].operand[0].predicate (target, mode[0]))\n+    target = gen_reg_rtx (mode[0]);\n+\n+  for (int i = 0; i < nargs; i++)\n+    if (!insn_data[icode].operand[i+k].predicate (op[i], mode[i+k]))\n+      op[i] = copy_to_mode_reg (mode[i+k], op[i]);\n+\n+  rtx pat;\n+\n+  switch (nargs)\n+    {\n+    case 0:\n+      pat = (void_func\n+\t     ? GEN_FCN (icode) ()\n+\t     : GEN_FCN (icode) (target));\n+      break;\n+    case 1:\n+      pat = (void_func\n+\t     ? GEN_FCN (icode) (op[0])\n+\t     : GEN_FCN (icode) (target, op[0]));\n+      break;\n+    case 2:\n+      pat = (void_func\n+\t     ? GEN_FCN (icode) (op[0], op[1])\n+\t     : GEN_FCN (icode) (target, op[0], op[1]));\n+      break;\n+    case 3:\n+      pat = (void_func\n+\t     ? GEN_FCN (icode) (op[0], op[1], op[2])\n+\t     : GEN_FCN (icode) (target, op[0], op[1], op[2]));\n+      break;\n+    case 4:\n+      pat = (void_func\n+\t     ? GEN_FCN (icode) (op[0], op[1], op[2], op[3])\n+\t     : GEN_FCN (icode) (target, op[0], op[1], op[2], op[3]));\n+      break;\n+    case 5:\n+      pat = (void_func\n+\t     ? GEN_FCN (icode) (op[0], op[1], op[2], op[3], op[4])\n+\t     : GEN_FCN (icode) (target, op[0], op[1], op[2], op[3], op[4]));\n+      break;\n+    case 6:\n+      pat = (void_func\n+\t     ? GEN_FCN (icode) (op[0], op[1], op[2], op[3], op[4], op[5])\n+\t     : GEN_FCN (icode) (target, op[0], op[1],\n+\t\t\t\top[2], op[3], op[4], op[5]));\n+      break;\n+    default:\n+      gcc_assert (MAX_BUILTIN_ARGS == 6);\n+      gcc_unreachable ();\n+    }\n+\n+  if (!pat)\n+    return 0;\n+\n+  emit_insn (pat);\n+  return target;\n+}\n+\n+/* Create a builtin vector type with a name.  Taking care not to give\n+   the canonical type a name.  */\n+\n+static tree\n+rs6000_vector_type (const char *name, tree elt_type, unsigned num_elts)\n+{\n+  tree result = build_vector_type (elt_type, num_elts);\n+\n+  /* Copy so we don't give the canonical type a name.  */\n+  result = build_variant_type_copy (result);\n+\n+  add_builtin_type (name, result);\n+\n+  return result;\n+}\n+\n+void\n+rs6000_init_builtins (void)\n+{\n+  tree tdecl;\n+  tree ftype;\n+  tree t;\n+  machine_mode mode;\n+  const char *str;\n+\n+  if (TARGET_DEBUG_BUILTIN)\n+    fprintf (stderr, \"rs6000_init_builtins%s%s\\n\",\n+\t     (TARGET_ALTIVEC)\t   ? \", altivec\" : \"\",\n+\t     (TARGET_VSX)\t   ? \", vsx\"\t : \"\");\n+\n+  if (new_builtins_are_live)\n+    V2DI_type_node = rs6000_vector_type (\"__vector long long\",\n+\t\t\t\t\t long_long_integer_type_node, 2);\n+  else\n+    {\n+      str = TARGET_POWERPC64 ? \"__vector long\" : \"__vector long long\";\n+      V2DI_type_node = rs6000_vector_type (str,\n+\t\t\t\t\t   long_long_integer_type_node,\n+\t\t\t\t\t   2);\n+    }\n+  ptr_V2DI_type_node\n+    = build_pointer_type (build_qualified_type (V2DI_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  V2DF_type_node = rs6000_vector_type (\"__vector double\", double_type_node, 2);\n+  ptr_V2DF_type_node\n+    = build_pointer_type (build_qualified_type (V2DF_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  V4SI_type_node = rs6000_vector_type (\"__vector signed int\",\n+\t\t\t\t       intSI_type_node, 4);\n+  ptr_V4SI_type_node\n+    = build_pointer_type (build_qualified_type (V4SI_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  V4SF_type_node = rs6000_vector_type (\"__vector float\", float_type_node, 4);\n+  ptr_V4SF_type_node\n+    = build_pointer_type (build_qualified_type (V4SF_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  V8HI_type_node = rs6000_vector_type (\"__vector signed short\",\n+\t\t\t\t       intHI_type_node, 8);\n+  ptr_V8HI_type_node\n+    = build_pointer_type (build_qualified_type (V8HI_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  V16QI_type_node = rs6000_vector_type (\"__vector signed char\",\n+\t\t\t\t\tintQI_type_node, 16);\n+  ptr_V16QI_type_node\n+    = build_pointer_type (build_qualified_type (V16QI_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  unsigned_V16QI_type_node = rs6000_vector_type (\"__vector unsigned char\",\n+\t\t\t\t\tunsigned_intQI_type_node, 16);\n+  ptr_unsigned_V16QI_type_node\n+    = build_pointer_type (build_qualified_type (unsigned_V16QI_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  unsigned_V8HI_type_node = rs6000_vector_type (\"__vector unsigned short\",\n+\t\t\t\t       unsigned_intHI_type_node, 8);\n+  ptr_unsigned_V8HI_type_node\n+    = build_pointer_type (build_qualified_type (unsigned_V8HI_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  unsigned_V4SI_type_node = rs6000_vector_type (\"__vector unsigned int\",\n+\t\t\t\t       unsigned_intSI_type_node, 4);\n+  ptr_unsigned_V4SI_type_node\n+    = build_pointer_type (build_qualified_type (unsigned_V4SI_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  if (new_builtins_are_live)\n+    unsigned_V2DI_type_node\n+      = rs6000_vector_type (\"__vector unsigned long long\",\n+\t\t\t    long_long_unsigned_type_node, 2);\n+  else\n+    {\n+      str = TARGET_POWERPC64\n+\t? \"__vector unsigned long\"\n+\t: \"__vector unsigned long long\";\n+      unsigned_V2DI_type_node\n+\t= rs6000_vector_type (str, long_long_unsigned_type_node, 2);\n+    }\n+\n+  ptr_unsigned_V2DI_type_node\n+    = build_pointer_type (build_qualified_type (unsigned_V2DI_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  opaque_V4SI_type_node = build_opaque_vector_type (intSI_type_node, 4);\n+\n+  const_str_type_node\n+    = build_pointer_type (build_qualified_type (char_type_node,\n+\t\t\t\t\t\tTYPE_QUAL_CONST));\n+\n+  /* We use V1TI mode as a special container to hold __int128_t items that\n+     must live in VSX registers.  */\n+  if (intTI_type_node)\n+    {\n+      V1TI_type_node = rs6000_vector_type (\"__vector __int128\",\n+\t\t\t\t\t   intTI_type_node, 1);\n+      ptr_V1TI_type_node\n+\t= build_pointer_type (build_qualified_type (V1TI_type_node,\n+\t\t\t\t\t\t    TYPE_QUAL_CONST));\n+      unsigned_V1TI_type_node\n+\t= rs6000_vector_type (\"__vector unsigned __int128\",\n+\t\t\t      unsigned_intTI_type_node, 1);\n+      ptr_unsigned_V1TI_type_node\n+\t= build_pointer_type (build_qualified_type (unsigned_V1TI_type_node,\n+\t\t\t\t\t\t    TYPE_QUAL_CONST));\n+    }\n+\n+  /* The 'vector bool ...' types must be kept distinct from 'vector unsigned ...'\n+     types, especially in C++ land.  Similarly, 'vector pixel' is distinct from\n+     'vector unsigned short'.  */\n+\n+  bool_char_type_node = build_distinct_type_copy (unsigned_intQI_type_node);\n+  bool_short_type_node = build_distinct_type_copy (unsigned_intHI_type_node);\n+  bool_int_type_node = build_distinct_type_copy (unsigned_intSI_type_node);\n+  bool_long_long_type_node = build_distinct_type_copy (unsigned_intDI_type_node);\n+  pixel_type_node = build_distinct_type_copy (unsigned_intHI_type_node);\n+\n+  long_integer_type_internal_node = long_integer_type_node;\n+  long_unsigned_type_internal_node = long_unsigned_type_node;\n+  long_long_integer_type_internal_node = long_long_integer_type_node;\n   long_long_unsigned_type_internal_node = long_long_unsigned_type_node;\n   intQI_type_internal_node = intQI_type_node;\n   uintQI_type_internal_node = unsigned_intQI_type_node;\n@@ -15047,6 +16260,9 @@ rs6000_init_builtins (void)\n \n   if (new_builtins_are_live)\n     {\n+      altivec_builtin_mask_for_load\n+\t= rs6000_builtin_decls_x[RS6000_BIF_MASK_FOR_LOAD];\n+\n #ifdef SUBTARGET_INIT_BUILTINS\n       SUBTARGET_INIT_BUILTINS;\n #endif"}]}