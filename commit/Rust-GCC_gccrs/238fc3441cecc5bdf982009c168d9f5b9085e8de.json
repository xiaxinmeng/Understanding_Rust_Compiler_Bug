{"sha": "238fc3441cecc5bdf982009c168d9f5b9085e8de", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjM4ZmMzNDQxY2VjYzViZGY5ODIwMDljMTY4ZDlmNWI5MDg1ZThkZQ==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-10-14T13:36:35Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-10-14T13:36:35Z"}, "message": "runtime: copy cpuprof code from Go 1.7 runtime\n    \n    This replaces runtime/cpuprof.goc with go/runtime/cpuprof.go and adjusts\n    the supporting code in runtime/proc.c.\n    \n    This adds another case where the compiler needs to avoid heap allocation\n    in the runtime package: when evaluating a method expression into a\n    closure.  Implementing this required moving the relevant code from\n    do_get_backend to do_flatten, so that I could easily add a temporary\n    variable.  Doing that let me get rid of Bound_method_expression::do_lower.\n    \n    Reviewed-on: https://go-review.googlesource.com/31050\n\nFrom-SVN: r241163", "tree": {"sha": "319b4c046950b68f79d46161f30121384b85c6be", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/319b4c046950b68f79d46161f30121384b85c6be"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/238fc3441cecc5bdf982009c168d9f5b9085e8de", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/238fc3441cecc5bdf982009c168d9f5b9085e8de", "html_url": "https://github.com/Rust-GCC/gccrs/commit/238fc3441cecc5bdf982009c168d9f5b9085e8de", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/238fc3441cecc5bdf982009c168d9f5b9085e8de/comments", "author": null, "committer": null, "parents": [{"sha": "6d59425df7e3d4e9b49c6521288a048bb35ec70c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6d59425df7e3d4e9b49c6521288a048bb35ec70c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6d59425df7e3d4e9b49c6521288a048bb35ec70c"}], "stats": {"total": 1109, "additions": 568, "deletions": 541}, "files": [{"sha": "769defcc22100f1f8cd42348b73427dd7ee6802a", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -1,4 +1,4 @@\n-e3913d96fb024b916c87a4dc01f413523467ead9\n+5f043fc2bf0f92a84a1f7da57acd79a61c9d2592\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "daa1c92cce6b4167f2b4cccd9a8ef7e974d612db", "filename": "gcc/go/gofrontend/expressions.cc", "status": "modified", "additions": 55, "deletions": 58, "changes": 113, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -3623,6 +3623,8 @@ Unsafe_type_conversion_expression::do_get_backend(Translate_context* context)\n               || et->map_type() != NULL\n               || et->channel_type() != NULL\n \t      || et->is_nil_type());\n+  else if (t->function_type() != NULL)\n+    go_assert(et->points_to() != NULL);\n   else\n     go_unreachable();\n \n@@ -6482,34 +6484,6 @@ Bound_method_expression::do_traverse(Traverse* traverse)\n   return Expression::traverse(&this->expr_, traverse);\n }\n \n-// Lower the expression.  If this is a method value rather than being\n-// called, and the method is accessed via a pointer, we may need to\n-// add nil checks.  Introduce a temporary variable so that those nil\n-// checks do not cause multiple evaluation.\n-\n-Expression*\n-Bound_method_expression::do_lower(Gogo*, Named_object*,\n-\t\t\t\t  Statement_inserter* inserter, int)\n-{\n-  // For simplicity we use a temporary for every call to an embedded\n-  // method, even though some of them might be pure value methods and\n-  // not require a temporary.\n-  if (this->expr_->var_expression() == NULL\n-      && this->expr_->temporary_reference_expression() == NULL\n-      && this->expr_->set_and_use_temporary_expression() == NULL\n-      && (this->method_->field_indexes() != NULL\n-\t  || (this->method_->is_value_method()\n-\t      && this->expr_->type()->points_to() != NULL)))\n-    {\n-      Temporary_statement* temp =\n-\tStatement::make_temporary(this->expr_->type(), NULL, this->location());\n-      inserter->insert(temp);\n-      this->expr_ = Expression::make_set_and_use_temporary(temp, this->expr_,\n-\t\t\t\t\t\t\t   this->location());\n-    }\n-  return this;\n-}\n-\n // Return the type of a bound method expression.  The type of this\n // object is simply the type of the method with no receiver.\n \n@@ -6724,40 +6698,51 @@ bme_check_nil(const Method::Field_indexes* field_indexes, Location loc,\n   return cond;\n }\n \n-// Get the backend representation for a method value.\n+// Flatten a method value into a struct with nil checks.  We can't do\n+// this in the lowering phase, because if the method value is called\n+// directly we don't need a thunk.  That case will have been handled\n+// by Call_expression::do_lower, so if we get here then we do need a\n+// thunk.\n \n-Bexpression*\n-Bound_method_expression::do_get_backend(Translate_context* context)\n+Expression*\n+Bound_method_expression::do_flatten(Gogo* gogo, Named_object*,\n+\t\t\t\t    Statement_inserter* inserter)\n {\n-  Named_object* thunk = Bound_method_expression::create_thunk(context->gogo(),\n+  Location loc = this->location();\n+\n+  Named_object* thunk = Bound_method_expression::create_thunk(gogo,\n \t\t\t\t\t\t\t      this->method_,\n \t\t\t\t\t\t\t      this->function_);\n   if (thunk->is_erroneous())\n     {\n       go_assert(saw_errors());\n-      return context->backend()->error_expression();\n+      return Expression::make_error(loc);\n     }\n \n-  // FIXME: We should lower this earlier, but we can't lower it in the\n-  // lowering pass because at that point we don't know whether we need\n-  // to create the thunk or not.  If the expression is called, we\n-  // don't need the thunk.\n-\n-  Location loc = this->location();\n+  // Force the expression into a variable.  This is only necessary if\n+  // we are going to do nil checks below, but it's easy enough to\n+  // always do it.\n+  Expression* expr = this->expr_;\n+  if (!expr->is_variable())\n+    {\n+      Temporary_statement* etemp = Statement::make_temporary(NULL, expr, loc);\n+      inserter->insert(etemp);\n+      expr = Expression::make_temporary_reference(etemp, loc);\n+    }\n \n   // If the method expects a value, and we have a pointer, we need to\n   // dereference the pointer.\n \n   Named_object* fn = this->method_->named_object();\n-  Function_type* fntype;\n+  Function_type *fntype;\n   if (fn->is_function())\n     fntype = fn->func_value()->type();\n   else if (fn->is_function_declaration())\n     fntype = fn->func_declaration_value()->type();\n   else\n     go_unreachable();\n \n-  Expression* val = this->expr_;\n+  Expression* val = expr;\n   if (fntype->receiver()->type()->points_to() == NULL\n       && val->type()->points_to() != NULL)\n     val = Expression::make_unary(OPERATOR_MULT, val, loc);\n@@ -6781,17 +6766,28 @@ Bound_method_expression::do_get_backend(Translate_context* context)\n   vals->push_back(val);\n \n   Expression* ret = Expression::make_struct_composite_literal(st, vals, loc);\n-  ret = Expression::make_heap_expression(ret, loc);\n \n-  // See whether the expression or any embedded pointers are nil.\n+  if (!gogo->compiling_runtime() || gogo->package_name() != \"runtime\")\n+    ret = Expression::make_heap_expression(ret, loc);\n+  else\n+    {\n+      // When compiling the runtime, method closures do not escape.\n+      // When escape analysis becomes the default, and applies to\n+      // method closures, this should be changed to make it an error\n+      // if a method closure escapes.\n+      Temporary_statement* ctemp = Statement::make_temporary(st, ret, loc);\n+      inserter->insert(ctemp);\n+      ret = Expression::make_temporary_reference(ctemp, loc);\n+      ret = Expression::make_unary(OPERATOR_AND, ret, loc);\n+      ret->unary_expression()->set_does_not_escape();\n+    }\n+\n+  // If necessary, check whether the expression or any embedded\n+  // pointers are nil.\n \n   Expression* nil_check = NULL;\n-  Expression* expr = this->expr_;\n   if (this->method_->field_indexes() != NULL)\n     {\n-      // Note that we are evaluating this->expr_ twice, but that is OK\n-      // because in the lowering pass we forced it into a temporary\n-      // variable.\n       Expression* ref = expr;\n       nil_check = bme_check_nil(this->method_->field_indexes(), loc, &ref);\n       expr = ref;\n@@ -6808,19 +6804,20 @@ Bound_method_expression::do_get_backend(Translate_context* context)\n \tnil_check = Expression::make_binary(OPERATOR_OROR, nil_check, n, loc);\n     }\n \n-  Bexpression* bme = ret->get_backend(context);\n   if (nil_check != NULL)\n     {\n-      Gogo* gogo = context->gogo();\n-      Bexpression* crash =\n-\tgogo->runtime_error(RUNTIME_ERROR_NIL_DEREFERENCE,\n-\t\t\t    loc)->get_backend(context);\n-      Btype* btype = ret->type()->get_backend(gogo);\n-      Bexpression* bcheck = nil_check->get_backend(context);\n-      bme = gogo->backend()->conditional_expression(btype, bcheck, crash,\n-\t\t\t\t\t\t    bme, loc);\n-    }\n-  return bme;\n+      Expression* crash = gogo->runtime_error(RUNTIME_ERROR_NIL_DEREFERENCE,\n+\t\t\t\t\t      loc);\n+      // Fix the type of the conditional expression by pretending to\n+      // evaluate to RET either way through the conditional.\n+      crash = Expression::make_compound(crash, ret, loc);\n+      ret = Expression::make_conditional(nil_check, crash, ret, loc);\n+    }\n+\n+  // RET is a pointer to a struct, but we want a function type.\n+  ret = Expression::make_unsafe_cast(this->type(), ret, loc);\n+\n+  return ret;\n }\n \n // Dump ast representation of a bound method expression."}, {"sha": "0d00f458c38d32ffefa9bea5f4a71d39d8e04b9c", "filename": "gcc/go/gofrontend/expressions.h", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/gcc%2Fgo%2Fgofrontend%2Fexpressions.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/gcc%2Fgo%2Fgofrontend%2Fexpressions.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fexpressions.h?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -2888,7 +2888,7 @@ class Bound_method_expression : public Expression\n   do_traverse(Traverse*);\n \n   Expression*\n-  do_lower(Gogo*, Named_object*, Statement_inserter*, int);\n+  do_flatten(Gogo*, Named_object*, Statement_inserter*);\n \n   Type*\n   do_type();\n@@ -2907,7 +2907,8 @@ class Bound_method_expression : public Expression\n   }\n \n   Bexpression*\n-  do_get_backend(Translate_context*);\n+  do_get_backend(Translate_context*)\n+  { go_unreachable(); }\n \n   void\n   do_dump_expression(Ast_dump_context*) const;"}, {"sha": "dee6fbcc25cba7509e8e4c10d23ea532c0709854", "filename": "libgo/Makefile.am", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2FMakefile.am", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2FMakefile.am", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2FMakefile.am?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -512,7 +512,6 @@ runtime_files = \\\n \t$(runtime_thread_files) \\\n \truntime/yield.c \\\n \t$(rtems_task_variable_add_file) \\\n-\tcpuprof.c \\\n \tgo-iface.c \\\n \tlfstack.c \\\n \tmalloc.c \\"}, {"sha": "c811312f21d69c5da9d2bfe13ee81cff4695eedb", "filename": "libgo/Makefile.in", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2FMakefile.in?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -261,9 +261,9 @@ am__objects_6 = go-append.lo go-assert.lo go-assert-interface.lo \\\n \tmcentral.lo $(am__objects_1) mfixalloc.lo mgc0.lo mheap.lo \\\n \tmsize.lo $(am__objects_2) panic.lo parfor.lo print.lo proc.lo \\\n \truntime.lo signal_unix.lo thread.lo $(am__objects_3) yield.lo \\\n-\t$(am__objects_4) cpuprof.lo go-iface.lo lfstack.lo malloc.lo \\\n-\tmprof.lo netpoll.lo rdebug.lo reflect.lo runtime1.lo \\\n-\tsigqueue.lo time.lo $(am__objects_5)\n+\t$(am__objects_4) go-iface.lo lfstack.lo malloc.lo mprof.lo \\\n+\tnetpoll.lo rdebug.lo reflect.lo runtime1.lo sigqueue.lo \\\n+\ttime.lo $(am__objects_5)\n am_libgo_llgo_la_OBJECTS = $(am__objects_6)\n libgo_llgo_la_OBJECTS = $(am_libgo_llgo_la_OBJECTS)\n libgo_llgo_la_LINK = $(LIBTOOL) --tag=CC $(AM_LIBTOOLFLAGS) \\\n@@ -911,7 +911,6 @@ runtime_files = \\\n \t$(runtime_thread_files) \\\n \truntime/yield.c \\\n \t$(rtems_task_variable_add_file) \\\n-\tcpuprof.c \\\n \tgo-iface.c \\\n \tlfstack.c \\\n \tmalloc.c \\\n@@ -1547,7 +1546,6 @@ mostlyclean-compile:\n distclean-compile:\n \t-rm -f *.tab.c\n \n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/cpuprof.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/env_posix.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/getncpu-bsd.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/getncpu-irix.Plo@am__quote@"}, {"sha": "873276f3639be8355ec16a0b8591c96b381d1141", "filename": "libgo/go/runtime/cpuprof.go", "status": "added", "additions": 453, "deletions": 0, "changes": 453, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fgo%2Fruntime%2Fcpuprof.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fgo%2Fruntime%2Fcpuprof.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fcpuprof.go?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -0,0 +1,453 @@\n+// Copyright 2011 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// CPU profiling.\n+// Based on algorithms and data structures used in\n+// http://code.google.com/p/google-perftools/.\n+//\n+// The main difference between this code and the google-perftools\n+// code is that this code is written to allow copying the profile data\n+// to an arbitrary io.Writer, while the google-perftools code always\n+// writes to an operating system file.\n+//\n+// The signal handler for the profiling clock tick adds a new stack trace\n+// to a hash table tracking counts for recent traces. Most clock ticks\n+// hit in the cache. In the event of a cache miss, an entry must be\n+// evicted from the hash table, copied to a log that will eventually be\n+// written as profile data. The google-perftools code flushed the\n+// log itself during the signal handler. This code cannot do that, because\n+// the io.Writer might block or need system calls or locks that are not\n+// safe to use from within the signal handler. Instead, we split the log\n+// into two halves and let the signal handler fill one half while a goroutine\n+// is writing out the other half. When the signal handler fills its half, it\n+// offers to swap with the goroutine. If the writer is not done with its half,\n+// we lose the stack trace for this clock tick (and record that loss).\n+// The goroutine interacts with the signal handler by calling getprofile() to\n+// get the next log piece to write, implicitly handing back the last log\n+// piece it obtained.\n+//\n+// The state of this dance between the signal handler and the goroutine\n+// is encoded in the Profile.handoff field. If handoff == 0, then the goroutine\n+// is not using either log half and is waiting (or will soon be waiting) for\n+// a new piece by calling notesleep(&p.wait).  If the signal handler\n+// changes handoff from 0 to non-zero, it must call notewakeup(&p.wait)\n+// to wake the goroutine. The value indicates the number of entries in the\n+// log half being handed off. The goroutine leaves the non-zero value in\n+// place until it has finished processing the log half and then flips the number\n+// back to zero. Setting the high bit in handoff means that the profiling is over,\n+// and the goroutine is now in charge of flushing the data left in the hash table\n+// to the log and returning that data.\n+//\n+// The handoff field is manipulated using atomic operations.\n+// For the most part, the manipulation of handoff is orderly: if handoff == 0\n+// then the signal handler owns it and can change it to non-zero.\n+// If handoff != 0 then the goroutine owns it and can change it to zero.\n+// If that were the end of the story then we would not need to manipulate\n+// handoff using atomic operations. The operations are needed, however,\n+// in order to let the log closer set the high bit to indicate \"EOF\" safely\n+// in the situation when normally the goroutine \"owns\" handoff.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"unsafe\"\n+)\n+\n+const (\n+\tnumBuckets      = 1 << 10\n+\tlogSize         = 1 << 17\n+\tassoc           = 4\n+\tmaxCPUProfStack = 64\n+)\n+\n+type cpuprofEntry struct {\n+\tcount uintptr\n+\tdepth int\n+\tstack [maxCPUProfStack]uintptr\n+}\n+\n+type cpuProfile struct {\n+\ton     bool    // profiling is on\n+\twait   note    // goroutine waits here\n+\tcount  uintptr // tick count\n+\tevicts uintptr // eviction count\n+\tlost   uintptr // lost ticks that need to be logged\n+\n+\t// Active recent stack traces.\n+\thash [numBuckets]struct {\n+\t\tentry [assoc]cpuprofEntry\n+\t}\n+\n+\t// Log of traces evicted from hash.\n+\t// Signal handler has filled log[toggle][:nlog].\n+\t// Goroutine is writing log[1-toggle][:handoff].\n+\tlog     [2][logSize / 2]uintptr\n+\tnlog    int\n+\ttoggle  int32\n+\thandoff uint32\n+\n+\t// Writer state.\n+\t// Writer maintains its own toggle to avoid races\n+\t// looking at signal handler's toggle.\n+\twtoggle  uint32\n+\twholding bool // holding & need to release a log half\n+\tflushing bool // flushing hash table - profile is over\n+\teodSent  bool // special end-of-data record sent; => flushing\n+}\n+\n+var (\n+\tcpuprofLock mutex\n+\tcpuprof     *cpuProfile\n+\n+\teod = [3]uintptr{0, 1, 0}\n+)\n+\n+func setcpuprofilerate(hz int32) {\n+\tsystemstack(func() {\n+\t\tsetcpuprofilerate_m(hz)\n+\t})\n+}\n+\n+// lostProfileData is a no-op function used in profiles\n+// to mark the number of profiling stack traces that were\n+// discarded due to slow data writers.\n+func lostProfileData() {}\n+\n+// SetCPUProfileRate sets the CPU profiling rate to hz samples per second.\n+// If hz <= 0, SetCPUProfileRate turns off profiling.\n+// If the profiler is on, the rate cannot be changed without first turning it off.\n+//\n+// Most clients should use the runtime/pprof package or\n+// the testing package's -test.cpuprofile flag instead of calling\n+// SetCPUProfileRate directly.\n+func SetCPUProfileRate(hz int) {\n+\t// Clamp hz to something reasonable.\n+\tif hz < 0 {\n+\t\thz = 0\n+\t}\n+\tif hz > 1000000 {\n+\t\thz = 1000000\n+\t}\n+\n+\tlock(&cpuprofLock)\n+\tif hz > 0 {\n+\t\tif cpuprof == nil {\n+\t\t\tcpuprof = (*cpuProfile)(sysAlloc(unsafe.Sizeof(cpuProfile{}), &memstats.other_sys))\n+\t\t\tif cpuprof == nil {\n+\t\t\t\tprint(\"runtime: cpu profiling cannot allocate memory\\n\")\n+\t\t\t\tunlock(&cpuprofLock)\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t}\n+\t\tif cpuprof.on || cpuprof.handoff != 0 {\n+\t\t\tprint(\"runtime: cannot set cpu profile rate until previous profile has finished.\\n\")\n+\t\t\tunlock(&cpuprofLock)\n+\t\t\treturn\n+\t\t}\n+\n+\t\tcpuprof.on = true\n+\t\t// pprof binary header format.\n+\t\t// https://github.com/gperftools/gperftools/blob/master/src/profiledata.cc#L119\n+\t\tp := &cpuprof.log[0]\n+\t\tp[0] = 0                 // count for header\n+\t\tp[1] = 3                 // depth for header\n+\t\tp[2] = 0                 // version number\n+\t\tp[3] = uintptr(1e6 / hz) // period (microseconds)\n+\t\tp[4] = 0\n+\t\tcpuprof.nlog = 5\n+\t\tcpuprof.toggle = 0\n+\t\tcpuprof.wholding = false\n+\t\tcpuprof.wtoggle = 0\n+\t\tcpuprof.flushing = false\n+\t\tcpuprof.eodSent = false\n+\t\tnoteclear(&cpuprof.wait)\n+\n+\t\tsetcpuprofilerate(int32(hz))\n+\t} else if cpuprof != nil && cpuprof.on {\n+\t\tsetcpuprofilerate(0)\n+\t\tcpuprof.on = false\n+\n+\t\t// Now add is not running anymore, and getprofile owns the entire log.\n+\t\t// Set the high bit in cpuprof.handoff to tell getprofile.\n+\t\tfor {\n+\t\t\tn := cpuprof.handoff\n+\t\t\tif n&0x80000000 != 0 {\n+\t\t\t\tprint(\"runtime: setcpuprofile(off) twice\\n\")\n+\t\t\t}\n+\t\t\tif atomic.Cas(&cpuprof.handoff, n, n|0x80000000) {\n+\t\t\t\tif n == 0 {\n+\t\t\t\t\t// we did the transition from 0 -> nonzero so we wake getprofile\n+\t\t\t\t\tnotewakeup(&cpuprof.wait)\n+\t\t\t\t}\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t}\n+\tunlock(&cpuprofLock)\n+}\n+\n+// add adds the stack trace to the profile.\n+// It is called from signal handlers and other limited environments\n+// and cannot allocate memory or acquire locks that might be\n+// held at the time of the signal, nor can it use substantial amounts\n+// of stack. It is allowed to call evict.\n+//go:nowritebarrierrec\n+func (p *cpuProfile) add(pc []uintptr) {\n+\tp.addWithFlushlog(pc, p.flushlog)\n+}\n+\n+// addWithFlushlog implements add and addNonGo.\n+// It is called from signal handlers and other limited environments\n+// and cannot allocate memory or acquire locks that might be\n+// held at the time of the signal, nor can it use substantial amounts\n+// of stack. It may be called by a signal handler with no g or m.\n+// It is allowed to call evict, passing the flushlog parameter.\n+//go:nosplit\n+//go:nowritebarrierrec\n+func (p *cpuProfile) addWithFlushlog(pc []uintptr, flushlog func() bool) {\n+\tif len(pc) > maxCPUProfStack {\n+\t\tpc = pc[:maxCPUProfStack]\n+\t}\n+\n+\t// Compute hash.\n+\th := uintptr(0)\n+\tfor _, x := range pc {\n+\t\th = h<<8 | (h >> (8 * (unsafe.Sizeof(h) - 1)))\n+\t\th += x * 41\n+\t}\n+\tp.count++\n+\n+\t// Add to entry count if already present in table.\n+\tb := &p.hash[h%numBuckets]\n+Assoc:\n+\tfor i := range b.entry {\n+\t\te := &b.entry[i]\n+\t\tif e.depth != len(pc) {\n+\t\t\tcontinue\n+\t\t}\n+\t\tfor j := range pc {\n+\t\t\tif e.stack[j] != pc[j] {\n+\t\t\t\tcontinue Assoc\n+\t\t\t}\n+\t\t}\n+\t\te.count++\n+\t\treturn\n+\t}\n+\n+\t// Evict entry with smallest count.\n+\tvar e *cpuprofEntry\n+\tfor i := range b.entry {\n+\t\tif e == nil || b.entry[i].count < e.count {\n+\t\t\te = &b.entry[i]\n+\t\t}\n+\t}\n+\tif e.count > 0 {\n+\t\tif !p.evict(e, flushlog) {\n+\t\t\t// Could not evict entry. Record lost stack.\n+\t\t\tp.lost++\n+\t\t\treturn\n+\t\t}\n+\t\tp.evicts++\n+\t}\n+\n+\t// Reuse the newly evicted entry.\n+\te.depth = len(pc)\n+\te.count = 1\n+\tcopy(e.stack[:], pc)\n+}\n+\n+// evict copies the given entry's data into the log, so that\n+// the entry can be reused.  evict is called from add, which\n+// is called from the profiling signal handler, so it must not\n+// allocate memory or block, and it may be called with no g or m.\n+// It is safe to call flushlog. evict returns true if the entry was\n+// copied to the log, false if there was no room available.\n+//go:nosplit\n+//go:nowritebarrierrec\n+func (p *cpuProfile) evict(e *cpuprofEntry, flushlog func() bool) bool {\n+\td := e.depth\n+\tnslot := d + 2\n+\tlog := &p.log[p.toggle]\n+\tif p.nlog+nslot > len(log) {\n+\t\tif !flushlog() {\n+\t\t\treturn false\n+\t\t}\n+\t\tlog = &p.log[p.toggle]\n+\t}\n+\n+\tq := p.nlog\n+\tlog[q] = e.count\n+\tq++\n+\tlog[q] = uintptr(d)\n+\tq++\n+\tcopy(log[q:], e.stack[:d])\n+\tq += d\n+\tp.nlog = q\n+\te.count = 0\n+\treturn true\n+}\n+\n+// flushlog tries to flush the current log and switch to the other one.\n+// flushlog is called from evict, called from add, called from the signal handler,\n+// so it cannot allocate memory or block. It can try to swap logs with\n+// the writing goroutine, as explained in the comment at the top of this file.\n+//go:nowritebarrierrec\n+func (p *cpuProfile) flushlog() bool {\n+\tif !atomic.Cas(&p.handoff, 0, uint32(p.nlog)) {\n+\t\treturn false\n+\t}\n+\tnotewakeup(&p.wait)\n+\n+\tp.toggle = 1 - p.toggle\n+\tlog := &p.log[p.toggle]\n+\tq := 0\n+\tif p.lost > 0 {\n+\t\tlostPC := funcPC(lostProfileData)\n+\t\tlog[0] = p.lost\n+\t\tlog[1] = 1\n+\t\tlog[2] = lostPC\n+\t\tq = 3\n+\t\tp.lost = 0\n+\t}\n+\tp.nlog = q\n+\treturn true\n+}\n+\n+// addNonGo is like add, but runs on a non-Go thread.\n+// It can't do anything that might need a g or an m.\n+// With this entry point, we don't try to flush the log when evicting an\n+// old entry. Instead, we just drop the stack trace if we're out of space.\n+//go:nosplit\n+//go:nowritebarrierrec\n+func (p *cpuProfile) addNonGo(pc []uintptr) {\n+\tp.addWithFlushlog(pc, func() bool { return false })\n+}\n+\n+// getprofile blocks until the next block of profiling data is available\n+// and returns it as a []byte. It is called from the writing goroutine.\n+func (p *cpuProfile) getprofile() []byte {\n+\tif p == nil {\n+\t\treturn nil\n+\t}\n+\n+\tif p.wholding {\n+\t\t// Release previous log to signal handling side.\n+\t\t// Loop because we are racing against SetCPUProfileRate(0).\n+\t\tfor {\n+\t\t\tn := p.handoff\n+\t\t\tif n == 0 {\n+\t\t\t\tprint(\"runtime: phase error during cpu profile handoff\\n\")\n+\t\t\t\treturn nil\n+\t\t\t}\n+\t\t\tif n&0x80000000 != 0 {\n+\t\t\t\tp.wtoggle = 1 - p.wtoggle\n+\t\t\t\tp.wholding = false\n+\t\t\t\tp.flushing = true\n+\t\t\t\tgoto Flush\n+\t\t\t}\n+\t\t\tif atomic.Cas(&p.handoff, n, 0) {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t\tp.wtoggle = 1 - p.wtoggle\n+\t\tp.wholding = false\n+\t}\n+\n+\tif p.flushing {\n+\t\tgoto Flush\n+\t}\n+\n+\tif !p.on && p.handoff == 0 {\n+\t\treturn nil\n+\t}\n+\n+\t// Wait for new log.\n+\tnotetsleepg(&p.wait, -1)\n+\tnoteclear(&p.wait)\n+\n+\tswitch n := p.handoff; {\n+\tcase n == 0:\n+\t\tprint(\"runtime: phase error during cpu profile wait\\n\")\n+\t\treturn nil\n+\tcase n == 0x80000000:\n+\t\tp.flushing = true\n+\t\tgoto Flush\n+\tdefault:\n+\t\tn &^= 0x80000000\n+\n+\t\t// Return new log to caller.\n+\t\tp.wholding = true\n+\n+\t\treturn uintptrBytes(p.log[p.wtoggle][:n])\n+\t}\n+\n+\t// In flush mode.\n+\t// Add is no longer being called. We own the log.\n+\t// Also, p.handoff is non-zero, so flushlog will return false.\n+\t// Evict the hash table into the log and return it.\n+Flush:\n+\tfor i := range p.hash {\n+\t\tb := &p.hash[i]\n+\t\tfor j := range b.entry {\n+\t\t\te := &b.entry[j]\n+\t\t\tif e.count > 0 && !p.evict(e, p.flushlog) {\n+\t\t\t\t// Filled the log. Stop the loop and return what we've got.\n+\t\t\t\tbreak Flush\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Return pending log data.\n+\tif p.nlog > 0 {\n+\t\t// Note that we're using toggle now, not wtoggle,\n+\t\t// because we're working on the log directly.\n+\t\tn := p.nlog\n+\t\tp.nlog = 0\n+\t\treturn uintptrBytes(p.log[p.toggle][:n])\n+\t}\n+\n+\t// Made it through the table without finding anything to log.\n+\tif !p.eodSent {\n+\t\t// We may not have space to append this to the partial log buf,\n+\t\t// so we always return a new slice for the end-of-data marker.\n+\t\tp.eodSent = true\n+\t\treturn uintptrBytes(eod[:])\n+\t}\n+\n+\t// Finally done. Clean up and return nil.\n+\tp.flushing = false\n+\tif !atomic.Cas(&p.handoff, p.handoff, 0) {\n+\t\tprint(\"runtime: profile flush racing with something\\n\")\n+\t}\n+\treturn nil\n+}\n+\n+func uintptrBytes(p []uintptr) (ret []byte) {\n+\tpp := (*slice)(unsafe.Pointer(&p))\n+\trp := (*slice)(unsafe.Pointer(&ret))\n+\n+\trp.array = pp.array\n+\trp.len = pp.len * int(unsafe.Sizeof(p[0]))\n+\trp.cap = rp.len\n+\n+\treturn\n+}\n+\n+// CPUProfile returns the next chunk of binary CPU profiling stack trace data,\n+// blocking until data is available. If profiling is turned off and all the profile\n+// data accumulated while it was on has been returned, CPUProfile returns nil.\n+// The caller must save the returned data before calling CPUProfile again.\n+//\n+// Most clients should use the runtime/pprof package or\n+// the testing package's -test.cpuprofile flag instead of calling\n+// CPUProfile directly.\n+func CPUProfile() []byte {\n+\treturn cpuprof.getprofile()\n+}\n+\n+//go:linkname runtime_pprof_runtime_cyclesPerSecond runtime_pprof.runtime_cyclesPerSecond\n+func runtime_pprof_runtime_cyclesPerSecond() int64 {\n+\treturn tickspersecond()\n+}"}, {"sha": "30a0f559a929f98d64796943baa9295338ef3c04", "filename": "libgo/go/runtime/stubs.go", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fgo%2Fruntime%2Fstubs.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fgo%2Fruntime%2Fstubs.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstubs.go?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -415,3 +415,16 @@ func startTheWorld() {\n func getMstats() *mstats {\n \treturn &memstats\n }\n+\n+// Temporary for gccgo until we port proc.go.\n+func setcpuprofilerate_m(hz int32)\n+\n+// Temporary for gccgo until we port mem_GOOS.go.\n+func sysAlloc(n uintptr, sysStat *uint64) unsafe.Pointer\n+\n+// Temporary for gccgo until we port proc.go, so that the C signal\n+// handler can call into cpuprof.\n+//go:linkname cpuprofAdd runtime.cpuprofAdd\n+func cpuprofAdd(stk []uintptr) {\n+\tcpuprof.add(stk)\n+}"}, {"sha": "123e074666db6e5b190f93af733d6170dc1772c1", "filename": "libgo/runtime/cpuprof.goc", "status": "removed", "additions": 0, "deletions": 442, "changes": 442, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6d59425df7e3d4e9b49c6521288a048bb35ec70c/libgo%2Fruntime%2Fcpuprof.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6d59425df7e3d4e9b49c6521288a048bb35ec70c/libgo%2Fruntime%2Fcpuprof.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fcpuprof.goc?ref=6d59425df7e3d4e9b49c6521288a048bb35ec70c", "patch": "@@ -1,442 +0,0 @@\n-// Copyright 2011 The Go Authors.  All rights reserved.\n-// Use of this source code is governed by a BSD-style\n-// license that can be found in the LICENSE file.\n-\n-// CPU profiling.\n-// Based on algorithms and data structures used in\n-// http://code.google.com/p/google-perftools/.\n-//\n-// The main difference between this code and the google-perftools\n-// code is that this code is written to allow copying the profile data\n-// to an arbitrary io.Writer, while the google-perftools code always\n-// writes to an operating system file.\n-//\n-// The signal handler for the profiling clock tick adds a new stack trace\n-// to a hash table tracking counts for recent traces.  Most clock ticks\n-// hit in the cache.  In the event of a cache miss, an entry must be \n-// evicted from the hash table, copied to a log that will eventually be\n-// written as profile data.  The google-perftools code flushed the\n-// log itself during the signal handler.  This code cannot do that, because\n-// the io.Writer might block or need system calls or locks that are not\n-// safe to use from within the signal handler.  Instead, we split the log\n-// into two halves and let the signal handler fill one half while a goroutine\n-// is writing out the other half.  When the signal handler fills its half, it\n-// offers to swap with the goroutine.  If the writer is not done with its half,\n-// we lose the stack trace for this clock tick (and record that loss).\n-// The goroutine interacts with the signal handler by calling getprofile() to\n-// get the next log piece to write, implicitly handing back the last log\n-// piece it obtained.\n-//\n-// The state of this dance between the signal handler and the goroutine\n-// is encoded in the Profile.handoff field.  If handoff == 0, then the goroutine\n-// is not using either log half and is waiting (or will soon be waiting) for\n-// a new piece by calling notesleep(&p->wait).  If the signal handler\n-// changes handoff from 0 to non-zero, it must call notewakeup(&p->wait)\n-// to wake the goroutine.  The value indicates the number of entries in the\n-// log half being handed off.  The goroutine leaves the non-zero value in\n-// place until it has finished processing the log half and then flips the number\n-// back to zero.  Setting the high bit in handoff means that the profiling is over, \n-// and the goroutine is now in charge of flushing the data left in the hash table\n-// to the log and returning that data.  \n-//\n-// The handoff field is manipulated using atomic operations.\n-// For the most part, the manipulation of handoff is orderly: if handoff == 0\n-// then the signal handler owns it and can change it to non-zero.  \n-// If handoff != 0 then the goroutine owns it and can change it to zero.\n-// If that were the end of the story then we would not need to manipulate\n-// handoff using atomic operations.  The operations are needed, however,\n-// in order to let the log closer set the high bit to indicate \"EOF\" safely\n-// in the situation when normally the goroutine \"owns\" handoff.\n-\n-package runtime\n-#include \"runtime.h\"\n-#include \"arch.h\"\n-#include \"malloc.h\"\n-\n-#include \"array.h\"\n-typedef struct __go_open_array Slice;\n-#define array __values\n-#define len __count\n-#define cap __capacity\n-\n-enum\n-{\n-\tHashSize = 1<<10,\n-\tLogSize = 1<<17,\n-\tAssoc = 4,\n-\tMaxStack = 64,\n-};\n-\n-typedef struct Profile Profile;\n-typedef struct Bucket Bucket;\n-typedef struct Entry Entry;\n-\n-struct Entry {\n-\tuintptr count;\n-\tuintptr depth;\n-\tuintptr stack[MaxStack];\n-};\n-\n-struct Bucket {\n-\tEntry entry[Assoc];\n-};\n-\n-struct Profile {\n-\tbool on;\t\t// profiling is on\n-\tNote wait;\t\t// goroutine waits here\n-\tuintptr count;\t\t// tick count\n-\tuintptr evicts;\t\t// eviction count\n-\tuintptr lost;\t\t// lost ticks that need to be logged\n-\n-\t// Active recent stack traces.\n-\tBucket hash[HashSize];\n-\n-\t// Log of traces evicted from hash.\n-\t// Signal handler has filled log[toggle][:nlog].\n-\t// Goroutine is writing log[1-toggle][:handoff].\n-\tuintptr log[2][LogSize/2];\n-\tuintptr nlog;\n-\tint32 toggle;\n-\tuint32 handoff;\n-\t\n-\t// Writer state.\n-\t// Writer maintains its own toggle to avoid races\n-\t// looking at signal handler's toggle.\n-\tuint32 wtoggle;\n-\tbool wholding;\t// holding & need to release a log half\n-\tbool flushing;\t// flushing hash table - profile is over\n-\tbool eod_sent;  // special end-of-data record sent; => flushing\n-};\n-\n-static Lock lk;\n-static Profile *prof;\n-\n-static void tick(uintptr*, int32);\n-static void add(Profile*, uintptr*, int32);\n-static bool evict(Profile*, Entry*);\n-static bool flushlog(Profile*);\n-\n-static uintptr eod[3] = {0, 1, 0};\n-\n-// LostProfileData is a no-op function used in profiles\n-// to mark the number of profiling stack traces that were\n-// discarded due to slow data writers.\n-static void\n-LostProfileData(void)\n-{\n-}\n-\n-extern void runtime_SetCPUProfileRate(intgo)\n-     __asm__ (GOSYM_PREFIX \"runtime.SetCPUProfileRate\");\n-\n-// SetCPUProfileRate sets the CPU profiling rate.\n-// The user documentation is in debug.go.\n-void\n-runtime_SetCPUProfileRate(intgo hz)\n-{\n-\tuintptr *p;\n-\tuintptr n;\n-\n-\t// Clamp hz to something reasonable.\n-\tif(hz < 0)\n-\t\thz = 0;\n-\tif(hz > 1000000)\n-\t\thz = 1000000;\n-\n-\truntime_lock(&lk);\n-\tif(hz > 0) {\n-\t\tif(prof == nil) {\n-\t\t\tprof = runtime_SysAlloc(sizeof *prof, &mstats()->other_sys);\n-\t\t\tif(prof == nil) {\n-\t\t\t\truntime_printf(\"runtime: cpu profiling cannot allocate memory\\n\");\n-\t\t\t\truntime_unlock(&lk);\n-\t\t\t\treturn;\n-\t\t\t}\n-\t\t}\n-\t\tif(prof->on || prof->handoff != 0) {\n-\t\t\truntime_printf(\"runtime: cannot set cpu profile rate until previous profile has finished.\\n\");\n-\t\t\truntime_unlock(&lk);\n-\t\t\treturn;\n-\t\t}\n-\n-\t\tprof->on = true;\n-\t\tp = prof->log[0];\n-\t\t// pprof binary header format.\n-\t\t// http://code.google.com/p/google-perftools/source/browse/trunk/src/profiledata.cc#117\n-\t\t*p++ = 0;  // count for header\n-\t\t*p++ = 3;  // depth for header\n-\t\t*p++ = 0;  // version number\n-\t\t*p++ = 1000000 / hz;  // period (microseconds)\n-\t\t*p++ = 0;\n-\t\tprof->nlog = p - prof->log[0];\n-\t\tprof->toggle = 0;\n-\t\tprof->wholding = false;\n-\t\tprof->wtoggle = 0;\n-\t\tprof->flushing = false;\n-\t\tprof->eod_sent = false;\n-\t\truntime_noteclear(&prof->wait);\n-\n-\t\truntime_setcpuprofilerate(tick, hz);\n-\t} else if(prof != nil && prof->on) {\n-\t\truntime_setcpuprofilerate(nil, 0);\n-\t\tprof->on = false;\n-\n-\t\t// Now add is not running anymore, and getprofile owns the entire log.\n-\t\t// Set the high bit in prof->handoff to tell getprofile.\n-\t\tfor(;;) {\n-\t\t\tn = prof->handoff;\n-\t\t\tif(n&0x80000000)\n-\t\t\t\truntime_printf(\"runtime: setcpuprofile(off) twice\");\n-\t\t\tif(runtime_cas(&prof->handoff, n, n|0x80000000))\n-\t\t\t\tbreak;\n-\t\t}\n-\t\tif(n == 0) {\n-\t\t\t// we did the transition from 0 -> nonzero so we wake getprofile\n-\t\t\truntime_notewakeup(&prof->wait);\n-\t\t}\n-\t}\n-\truntime_unlock(&lk);\n-}\n-\n-static void\n-tick(uintptr *pc, int32 n)\n-{\n-\tadd(prof, pc, n);\n-}\n-\n-// add adds the stack trace to the profile.\n-// It is called from signal handlers and other limited environments\n-// and cannot allocate memory or acquire locks that might be\n-// held at the time of the signal, nor can it use substantial amounts\n-// of stack.  It is allowed to call evict.\n-static void\n-add(Profile *p, uintptr *pc, int32 n)\n-{\n-\tint32 i, j;\n-\tuintptr h, x;\n-\tBucket *b;\n-\tEntry *e;\n-\n-\tif(n > MaxStack)\n-\t\tn = MaxStack;\n-\t\n-\t// Compute hash.\n-\th = 0;\n-\tfor(i=0; i<n; i++) {\n-\t\th = h<<8 | (h>>(8*(sizeof(h)-1)));\n-\t\tx = pc[i];\n-\t\th += x*31 + x*7 + x*3;\n-\t}\n-\tp->count++;\n-\n-\t// Add to entry count if already present in table.\n-\tb = &p->hash[h%HashSize];\n-\tfor(i=0; i<Assoc; i++) {\n-\t\te = &b->entry[i];\n-\t\tif(e->depth != (uintptr)n)\t\n-\t\t\tcontinue;\n-\t\tfor(j=0; j<n; j++)\n-\t\t\tif(e->stack[j] != pc[j])\n-\t\t\t\tgoto ContinueAssoc;\n-\t\te->count++;\n-\t\treturn;\n-\tContinueAssoc:;\n-\t}\n-\n-\t// Evict entry with smallest count.\n-\te = &b->entry[0];\n-\tfor(i=1; i<Assoc; i++)\n-\t\tif(b->entry[i].count < e->count)\n-\t\t\te = &b->entry[i];\n-\tif(e->count > 0) {\n-\t\tif(!evict(p, e)) {\n-\t\t\t// Could not evict entry.  Record lost stack.\n-\t\t\tp->lost++;\n-\t\t\treturn;\n-\t\t}\n-\t\tp->evicts++;\n-\t}\n-\t\n-\t// Reuse the newly evicted entry.\n-\te->depth = n;\n-\te->count = 1;\n-\tfor(i=0; i<n; i++)\n-\t\te->stack[i] = pc[i];\n-}\n-\n-// evict copies the given entry's data into the log, so that\n-// the entry can be reused.  evict is called from add, which\n-// is called from the profiling signal handler, so it must not\n-// allocate memory or block.  It is safe to call flushLog.\n-// evict returns true if the entry was copied to the log,\n-// false if there was no room available.\n-static bool\n-evict(Profile *p, Entry *e)\n-{\n-\tint32 i, d, nslot;\n-\tuintptr *log, *q;\n-\t\n-\td = e->depth;\n-\tnslot = d+2;\n-\tlog = p->log[p->toggle];\n-\tif(p->nlog+nslot > nelem(p->log[0])) {\n-\t\tif(!flushlog(p))\n-\t\t\treturn false;\n-\t\tlog = p->log[p->toggle];\n-\t}\n-\t\n-\tq = log+p->nlog;\n-\t*q++ = e->count;\n-\t*q++ = d;\n-\tfor(i=0; i<d; i++)\n-\t\t*q++ = e->stack[i];\n-\tp->nlog = q - log;\n-\te->count = 0;\n-\treturn true;\n-}\n-\n-// flushlog tries to flush the current log and switch to the other one.\n-// flushlog is called from evict, called from add, called from the signal handler,\n-// so it cannot allocate memory or block.  It can try to swap logs with\n-// the writing goroutine, as explained in the comment at the top of this file.\n-static bool\n-flushlog(Profile *p)\n-{\n-\tuintptr *log, *q;\n-\n-\tif(!runtime_cas(&p->handoff, 0, p->nlog))\n-\t\treturn false;\n-\truntime_notewakeup(&p->wait);\n-\n-\tp->toggle = 1 - p->toggle;\n-\tlog = p->log[p->toggle];\n-\tq = log;\n-\tif(p->lost > 0) {\n-\t\t*q++ = p->lost;\n-\t\t*q++ = 1;\n-\t\t*q++ = (uintptr)LostProfileData;\n-\t\tp->lost = 0;\n-\t}\n-\tp->nlog = q - log;\n-\treturn true;\n-}\n-\n-// getprofile blocks until the next block of profiling data is available\n-// and returns it as a []byte.  It is called from the writing goroutine.\n-Slice\n-getprofile(Profile *p)\n-{\n-\tuint32 i, j, n;\n-\tSlice ret;\n-\tBucket *b;\n-\tEntry *e;\n-\n-\tret.array = nil;\n-\tret.len = 0;\n-\tret.cap = 0;\n-\t\n-\tif(p == nil)\t\n-\t\treturn ret;\n-\n-\tif(p->wholding) {\n-\t\t// Release previous log to signal handling side.\n-\t\t// Loop because we are racing against SetCPUProfileRate(0).\n-\t\tfor(;;) {\n-\t\t\tn = p->handoff;\n-\t\t\tif(n == 0) {\n-\t\t\t\truntime_printf(\"runtime: phase error during cpu profile handoff\\n\");\n-\t\t\t\treturn ret;\n-\t\t\t}\n-\t\t\tif(n & 0x80000000) {\n-\t\t\t\tp->wtoggle = 1 - p->wtoggle;\n-\t\t\t\tp->wholding = false;\n-\t\t\t\tp->flushing = true;\n-\t\t\t\tgoto flush;\n-\t\t\t}\n-\t\t\tif(runtime_cas(&p->handoff, n, 0))\n-\t\t\t\tbreak;\n-\t\t}\n-\t\tp->wtoggle = 1 - p->wtoggle;\n-\t\tp->wholding = false;\n-\t}\n-\t\n-\tif(p->flushing)\n-\t\tgoto flush;\n-\t\n-\tif(!p->on && p->handoff == 0)\n-\t\treturn ret;\n-\n-\t// Wait for new log.\n-\truntime_notetsleepg(&p->wait, -1);\n-\truntime_noteclear(&p->wait);\n-\n-\tn = p->handoff;\n-\tif(n == 0) {\n-\t\truntime_printf(\"runtime: phase error during cpu profile wait\\n\");\n-\t\treturn ret;\n-\t}\n-\tif(n == 0x80000000) {\n-\t\tp->flushing = true;\n-\t\tgoto flush;\n-\t}\n-\tn &= ~0x80000000;\n-\n-\t// Return new log to caller.\n-\tp->wholding = true;\n-\n-\tret.array = (byte*)p->log[p->wtoggle];\n-\tret.len = n*sizeof(uintptr);\n-\tret.cap = ret.len;\n-\treturn ret;\n-\n-flush:\n-\t// In flush mode.\n-\t// Add is no longer being called.  We own the log.\n-\t// Also, p->handoff is non-zero, so flushlog will return false.\n-\t// Evict the hash table into the log and return it.\n-\tfor(i=0; i<HashSize; i++) {\n-\t\tb = &p->hash[i];\n-\t\tfor(j=0; j<Assoc; j++) {\n-\t\t\te = &b->entry[j];\n-\t\t\tif(e->count > 0 && !evict(p, e)) {\n-\t\t\t\t// Filled the log.  Stop the loop and return what we've got.\n-\t\t\t\tgoto breakflush;\n-\t\t\t}\n-\t\t}\n-\t}\n-breakflush:\n-\n-\t// Return pending log data.\n-\tif(p->nlog > 0) {\n-\t\t// Note that we're using toggle now, not wtoggle,\n-\t\t// because we're working on the log directly.\n-\t\tret.array = (byte*)p->log[p->toggle];\n-\t\tret.len = p->nlog*sizeof(uintptr);\n-\t\tret.cap = ret.len;\n-\t\tp->nlog = 0;\n-\t\treturn ret;\n-\t}\n-\n-\t// Made it through the table without finding anything to log.\n-\tif(!p->eod_sent) {\n-\t\t// We may not have space to append this to the partial log buf,\n-\t\t// so we always return a new slice for the end-of-data marker.\n-\t\tp->eod_sent = true;\n-\t\tret.array = (byte*)eod;\n-\t\tret.len = sizeof eod;\n-\t\tret.cap = ret.len;\n-\t\treturn ret;\n-\t}\n-\n-\t// Finally done.  Clean up and return nil.\n-\tp->flushing = false;\n-\tif(!runtime_cas(&p->handoff, p->handoff, 0))\n-\t\truntime_printf(\"runtime: profile flush racing with something\\n\");\n-\treturn ret;  // set to nil at top of function\n-}\n-\n-// CPUProfile returns the next cpu profile block as a []byte.\n-// The user documentation is in debug.go.\n-func CPUProfile() (ret Slice) {\n-\tret = getprofile(prof);\n-}"}, {"sha": "99829eb6385a855134b1e5983d3a1bf4d63fd6b7", "filename": "libgo/runtime/go-signal.c", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fruntime%2Fgo-signal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fruntime%2Fgo-signal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-signal.c?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -156,6 +156,8 @@ runtime_sighandler (int sig, Siginfo *info,\n #ifdef SIGPROF\n   if (sig == SIGPROF)\n     {\n+      /* FIXME: Handle m == NULL by calling something like gc's\n+\t sigprofNonGo.  */\n       if (m != NULL && gp != m->g0 && gp != m->gsignal)\n \truntime_sigprof ();\n       return;"}, {"sha": "12a25b57677707a65abd064e0a5bfff63726563f", "filename": "libgo/runtime/malloc.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fruntime%2Fmalloc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fruntime%2Fmalloc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmalloc.h?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -184,7 +184,8 @@ enum\n // SysFault marks a (already SysAlloc'd) region to fault\n // if accessed.  Used only for debugging the runtime.\n \n-void*\truntime_SysAlloc(uintptr nbytes, uint64 *stat);\n+void*\truntime_SysAlloc(uintptr nbytes, uint64 *stat)\n+  __asm__ (GOSYM_PREFIX \"runtime.sysAlloc\");\n void\truntime_SysFree(void *v, uintptr nbytes, uint64 *stat);\n void\truntime_SysUnused(void *v, uintptr nbytes);\n void\truntime_SysUsed(void *v, uintptr nbytes);"}, {"sha": "246ab7d1b022b89b325c743aa6f2194cc287986c", "filename": "libgo/runtime/proc.c", "status": "modified", "additions": 32, "deletions": 26, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fruntime%2Fproc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fruntime%2Fproc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fproc.c?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -2686,11 +2686,8 @@ runtime_mcount(void)\n }\n \n static struct {\n-\tLock;\n-\tvoid (*fn)(uintptr*, int32);\n+\tuint32 lock;\n \tint32 hz;\n-\tuintptr pcbuf[TracebackMaxFrames];\n-\tLocation locbuf[TracebackMaxFrames];\n } prof;\n \n static void System(void) {}\n@@ -2703,8 +2700,11 @@ runtime_sigprof()\n \tM *mp = g->m;\n \tint32 n, i;\n \tbool traceback;\n+\tuintptr pcbuf[TracebackMaxFrames];\n+\tLocation locbuf[TracebackMaxFrames];\n+\tSlice stk;\n \n-\tif(prof.fn == nil || prof.hz == 0)\n+\tif(prof.hz == 0)\n \t\treturn;\n \n \tif(mp == nil)\n@@ -2718,12 +2718,6 @@ runtime_sigprof()\n \tif(mp->mcache == nil)\n \t\ttraceback = false;\n \n-\truntime_lock(&prof);\n-\tif(prof.fn == nil) {\n-\t\truntime_unlock(&prof);\n-\t\tmp->mallocing--;\n-\t\treturn;\n-\t}\n \tn = 0;\n \n \tif(runtime_atomicload(&runtime_in_callers) > 0) {\n@@ -2735,34 +2729,44 @@ runtime_sigprof()\n \t}\n \n \tif(traceback) {\n-\t\tn = runtime_callers(0, prof.locbuf, nelem(prof.locbuf), false);\n+\t\tn = runtime_callers(0, locbuf, nelem(locbuf), false);\n \t\tfor(i = 0; i < n; i++)\n-\t\t\tprof.pcbuf[i] = prof.locbuf[i].pc;\n+\t\t\tpcbuf[i] = locbuf[i].pc;\n \t}\n \tif(!traceback || n <= 0) {\n \t\tn = 2;\n-\t\tprof.pcbuf[0] = (uintptr)runtime_getcallerpc(&n);\n+\t\tpcbuf[0] = (uintptr)runtime_getcallerpc(&n);\n \t\tif(mp->gcing || mp->helpgc)\n-\t\t\tprof.pcbuf[1] = (uintptr)GC;\n+\t\t\tpcbuf[1] = (uintptr)GC;\n \t\telse\n-\t\t\tprof.pcbuf[1] = (uintptr)System;\n+\t\t\tpcbuf[1] = (uintptr)System;\n+\t}\n+\n+\tif (prof.hz != 0) {\n+\t\tstk.__values = &pcbuf[0];\n+\t\tstk.__count = n;\n+\t\tstk.__capacity = n;\n+\n+\t\t// Simple cas-lock to coordinate with setcpuprofilerate.\n+\t\twhile (!runtime_cas(&prof.lock, 0, 1)) {\n+\t\t\truntime_osyield();\n+\t\t}\n+\t\tif (prof.hz != 0) {\n+\t\t\truntime_cpuprofAdd(stk);\n+\t\t}\n+\t\truntime_atomicstore(&prof.lock, 0);\n \t}\n-\tprof.fn(prof.pcbuf, n);\n-\truntime_unlock(&prof);\n+\n \tmp->mallocing--;\n }\n \n // Arrange to call fn with a traceback hz times a second.\n void\n-runtime_setcpuprofilerate(void (*fn)(uintptr*, int32), int32 hz)\n+runtime_setcpuprofilerate_m(int32 hz)\n {\n \t// Force sane arguments.\n \tif(hz < 0)\n \t\thz = 0;\n-\tif(hz == 0)\n-\t\tfn = nil;\n-\tif(fn == nil)\n-\t\thz = 0;\n \n \t// Disable preemption, otherwise we can be rescheduled to another thread\n \t// that has profiling enabled.\n@@ -2773,10 +2777,12 @@ runtime_setcpuprofilerate(void (*fn)(uintptr*, int32), int32 hz)\n \t// it would deadlock.\n \truntime_resetcpuprofiler(0);\n \n-\truntime_lock(&prof);\n-\tprof.fn = fn;\n+\twhile (!runtime_cas(&prof.lock, 0, 1)) {\n+\t\truntime_osyield();\n+\t}\n \tprof.hz = hz;\n-\truntime_unlock(&prof);\n+\truntime_atomicstore(&prof.lock, 0);\n+\n \truntime_lock(&runtime_sched);\n \truntime_sched.profilehz = hz;\n \truntime_unlock(&runtime_sched);"}, {"sha": "96f550ced0b140c29af66a9e250dd610b82576fd", "filename": "libgo/runtime/runtime.h", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fruntime%2Fruntime.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fruntime%2Fruntime.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.h?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -417,7 +417,10 @@ void\truntime_freezetheworld(void);\n void\truntime_unwindstack(G*, byte*);\n void\truntime_sigprof();\n void\truntime_resetcpuprofiler(int32);\n-void\truntime_setcpuprofilerate(void(*)(uintptr*, int32), int32);\n+void\truntime_setcpuprofilerate_m(int32)\n+     __asm__ (GOSYM_PREFIX \"runtime.setcpuprofilerate_m\");\n+void\truntime_cpuprofAdd(Slice)\n+     __asm__ (GOSYM_PREFIX \"runtime.cpuprofAdd\");\n void\truntime_usleep(uint32)\n      __asm__ (GOSYM_PREFIX \"runtime.usleep\");\n int64\truntime_cputicks(void)"}, {"sha": "bc5ba4ad5394fd02e988fc9354278b4c2b0dbe50", "filename": "libgo/runtime/runtime1.goc", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fruntime%2Fruntime1.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/238fc3441cecc5bdf982009c168d9f5b9085e8de/libgo%2Fruntime%2Fruntime1.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime1.goc?ref=238fc3441cecc5bdf982009c168d9f5b9085e8de", "patch": "@@ -55,10 +55,6 @@ func getgoroot() (out String) {\n \tout = runtime_getenv(\"GOROOT\");\n }\n \n-func runtime_pprof.runtime_cyclesPerSecond() (res int64) {\n-\tres = runtime_tickspersecond();\n-}\n-\n func sync.runtime_procPin() (p int) {\n \tM *mp;\n "}]}