{"sha": "2724d1bba6b36451404811fba3244f8897717ef3", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjcyNGQxYmJhNmIzNjQ1MTQwNDgxMWZiYTMyNDRmODg5NzcxN2VmMw==", "commit": {"author": {"name": "Richard Biener", "email": "rguenther@suse.de", "date": "2021-07-30T09:06:50Z"}, "committer": {"name": "Richard Biener", "email": "rguenther@suse.de", "date": "2021-08-04T10:38:03Z"}, "message": "Rewrite more vector loads to scalar loads\n\nThis teaches forwprop to rewrite more vector loads that are only\nused in BIT_FIELD_REFs as scalar loads.  This provides the\nremaining uplift to SPEC CPU 2017 510.parest_r on Zen 2 which\nhas CPU gathers disabled.\n\nIn particular vector load + vec_unpack + bit-field-ref is turned\ninto (extending) scalar loads which avoids costly XMM/GPR\ntransitions.  To not conflict with vector load + bit-field-ref\n+ vector constructor matching to vector load + shuffle the\nextended transform is only done after vector lowering.\n\n2021-07-30  Richard Biener  <rguenther@suse.de>\n\n\t* tree-ssa-forwprop.c (pass_forwprop::execute): Split\n\tout code to decompose vector loads ...\n\t(optimize_vector_load): ... here.  Generalize it to\n\thandle intermediate widening and TARGET_MEM_REF loads\n\tand apply it to loads with a supported vector mode as well.", "tree": {"sha": "82eaab401387f80c8bc760e0c1bf01d0b57102c1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/82eaab401387f80c8bc760e0c1bf01d0b57102c1"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/2724d1bba6b36451404811fba3244f8897717ef3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2724d1bba6b36451404811fba3244f8897717ef3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2724d1bba6b36451404811fba3244f8897717ef3", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2724d1bba6b36451404811fba3244f8897717ef3/comments", "author": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "87a0b607e40f8122c7fc45d496ef48799fe11550", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/87a0b607e40f8122c7fc45d496ef48799fe11550", "html_url": "https://github.com/Rust-GCC/gccrs/commit/87a0b607e40f8122c7fc45d496ef48799fe11550"}], "stats": {"total": 244, "additions": 182, "deletions": 62}, "files": [{"sha": "bd64b8e46bc8e7f523d442a568292d8e4d11ce23", "filename": "gcc/tree-ssa-forwprop.c", "status": "modified", "additions": 182, "deletions": 62, "changes": 244, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2724d1bba6b36451404811fba3244f8897717ef3/gcc%2Ftree-ssa-forwprop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2724d1bba6b36451404811fba3244f8897717ef3/gcc%2Ftree-ssa-forwprop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-forwprop.c?ref=2724d1bba6b36451404811fba3244f8897717ef3", "patch": "@@ -2757,6 +2757,182 @@ simplify_vector_constructor (gimple_stmt_iterator *gsi)\n }\n \n \n+/* Rewrite the vector load at *GSI to component-wise loads if the load\n+   is only used in BIT_FIELD_REF extractions with eventual intermediate\n+   widening.  */\n+\n+static void\n+optimize_vector_load (gimple_stmt_iterator *gsi)\n+{\n+  gimple *stmt = gsi_stmt (*gsi);\n+  tree lhs = gimple_assign_lhs (stmt);\n+  tree rhs = gimple_assign_rhs1 (stmt);\n+\n+  /* Gather BIT_FIELD_REFs to rewrite, looking through\n+     VEC_UNPACK_{LO,HI}_EXPR.  */\n+  use_operand_p use_p;\n+  imm_use_iterator iter;\n+  bool rewrite = true;\n+  auto_vec<gimple *, 8> bf_stmts;\n+  auto_vec<tree, 8> worklist;\n+  worklist.quick_push (lhs);\n+  do\n+    {\n+      tree def = worklist.pop ();\n+      unsigned HOST_WIDE_INT def_eltsize\n+\t= TREE_INT_CST_LOW (TYPE_SIZE (TREE_TYPE (TREE_TYPE (def))));\n+      FOR_EACH_IMM_USE_FAST (use_p, iter, def)\n+\t{\n+\t  gimple *use_stmt = USE_STMT (use_p);\n+\t  if (is_gimple_debug (use_stmt))\n+\t    continue;\n+\t  if (!is_gimple_assign (use_stmt))\n+\t    {\n+\t      rewrite = false;\n+\t      break;\n+\t    }\n+\t  enum tree_code use_code = gimple_assign_rhs_code (use_stmt);\n+\t  tree use_rhs = gimple_assign_rhs1 (use_stmt);\n+\t  if (use_code == BIT_FIELD_REF\n+\t      && TREE_OPERAND (use_rhs, 0) == def\n+\t      /* If its on the VEC_UNPACK_{HI,LO}_EXPR\n+\t\t def need to verify it is element aligned.  */\n+\t      && (def == lhs\n+\t\t  || (known_eq (bit_field_size (use_rhs), def_eltsize)\n+\t\t      && constant_multiple_p (bit_field_offset (use_rhs),\n+\t\t\t\t\t      def_eltsize))))\n+\t    {\n+\t      bf_stmts.safe_push (use_stmt);\n+\t      continue;\n+\t    }\n+\t  /* Walk through one level of VEC_UNPACK_{LO,HI}_EXPR.  */\n+\t  if (def == lhs\n+\t      && (use_code == VEC_UNPACK_HI_EXPR\n+\t\t  || use_code == VEC_UNPACK_LO_EXPR)\n+\t      && use_rhs == lhs)\n+\t    {\n+\t      worklist.safe_push (gimple_assign_lhs (use_stmt));\n+\t      continue;\n+\t    }\n+\t  rewrite = false;\n+\t  break;\n+\t}\n+      if (!rewrite)\n+\tbreak;\n+    }\n+  while (!worklist.is_empty ());\n+\n+  if (!rewrite)\n+    {\n+      gsi_next (gsi);\n+      return;\n+    }\n+  /* We now have all ultimate uses of the load to rewrite in bf_stmts.  */\n+\n+  /* Prepare the original ref to be wrapped in adjusted BIT_FIELD_REFs.\n+     For TARGET_MEM_REFs we have to separate the LEA from the reference.  */\n+  tree load_rhs = rhs;\n+  if (TREE_CODE (load_rhs) == TARGET_MEM_REF)\n+    {\n+      if (TREE_CODE (TREE_OPERAND (load_rhs, 0)) == ADDR_EXPR)\n+\tmark_addressable (TREE_OPERAND (TREE_OPERAND (load_rhs, 0), 0));\n+      tree tem = make_ssa_name (TREE_TYPE (TREE_OPERAND (load_rhs, 0)));\n+      gimple *new_stmt\n+\t= gimple_build_assign (tem, build1 (ADDR_EXPR, TREE_TYPE (tem),\n+\t\t\t\t\t    unshare_expr (load_rhs)));\n+      gsi_insert_before (gsi, new_stmt, GSI_SAME_STMT);\n+      load_rhs = build2_loc (EXPR_LOCATION (load_rhs),\n+\t\t\t     MEM_REF, TREE_TYPE (load_rhs), tem,\n+\t\t\t     build_int_cst\n+\t\t\t       (TREE_TYPE (TREE_OPERAND (load_rhs, 1)), 0));\n+    }\n+\n+  /* Rewrite the BIT_FIELD_REFs to be actual loads, re-emitting them at\n+     the place of the original load.  */\n+  for (gimple *use_stmt : bf_stmts)\n+    {\n+      tree bfr = gimple_assign_rhs1 (use_stmt);\n+      tree new_rhs = unshare_expr (load_rhs);\n+      if (TREE_OPERAND (bfr, 0) != lhs)\n+\t{\n+\t  /* When the BIT_FIELD_REF is on the promoted vector we have to\n+\t     adjust it and emit a conversion afterwards.  */\n+\t  gimple *def_stmt\n+\t      = SSA_NAME_DEF_STMT (TREE_OPERAND (bfr, 0));\n+\t  enum tree_code def_code\n+\t      = gimple_assign_rhs_code (def_stmt);\n+\n+\t  /* The adjusted BIT_FIELD_REF is of the promotion source\n+\t     vector size and at half of the offset...  */\n+\t  new_rhs = fold_build3 (BIT_FIELD_REF,\n+\t\t\t\t TREE_TYPE (TREE_TYPE (lhs)),\n+\t\t\t\t new_rhs,\n+\t\t\t\t TYPE_SIZE (TREE_TYPE (TREE_TYPE (lhs))),\n+\t\t\t\t size_binop (EXACT_DIV_EXPR,\n+\t\t\t\t\t     TREE_OPERAND (bfr, 2),\n+\t\t\t\t\t     bitsize_int (2)));\n+\t  /* ... and offsetted by half of the vector if VEC_UNPACK_HI_EXPR.  */\n+\t  if (def_code == (!BYTES_BIG_ENDIAN\n+\t\t\t   ? VEC_UNPACK_HI_EXPR : VEC_UNPACK_LO_EXPR))\n+\t    TREE_OPERAND (new_rhs, 2)\n+\t      = size_binop (PLUS_EXPR, TREE_OPERAND (new_rhs, 2),\n+\t\t\t    size_binop (EXACT_DIV_EXPR,\n+\t\t\t\t\tTYPE_SIZE (TREE_TYPE (lhs)),\n+\t\t\t\t\tbitsize_int (2)));\n+\t  tree tem = make_ssa_name (TREE_TYPE (TREE_TYPE (lhs)));\n+\t  gimple *new_stmt = gimple_build_assign (tem, new_rhs);\n+\t  location_t loc = gimple_location (use_stmt);\n+\t  gimple_set_location (new_stmt, loc);\n+\t  gsi_insert_before (gsi, new_stmt, GSI_SAME_STMT);\n+\t  /* Perform scalar promotion.  */\n+\t  new_stmt = gimple_build_assign (gimple_assign_lhs (use_stmt),\n+\t\t\t\t\t  NOP_EXPR, tem);\n+\t  gimple_set_location (new_stmt, loc);\n+\t  gsi_insert_before (gsi, new_stmt, GSI_SAME_STMT);\n+\t}\n+      else\n+\t{\n+\t  /* When the BIT_FIELD_REF is on the original load result\n+\t     we can just wrap that.  */\n+\t  tree new_rhs = fold_build3 (BIT_FIELD_REF, TREE_TYPE (bfr),\n+\t\t\t\t      unshare_expr (load_rhs),\n+\t\t\t\t      TREE_OPERAND (bfr, 1),\n+\t\t\t\t      TREE_OPERAND (bfr, 2));\n+\t  gimple *new_stmt = gimple_build_assign (gimple_assign_lhs (use_stmt),\n+\t\t\t\t\t\t  new_rhs);\n+\t  location_t loc = gimple_location (use_stmt);\n+\t  gimple_set_location (new_stmt, loc);\n+\t  gsi_insert_before (gsi, new_stmt, GSI_SAME_STMT);\n+\t}\n+      gimple_stmt_iterator gsi2 = gsi_for_stmt (use_stmt);\n+      unlink_stmt_vdef (use_stmt);\n+      gsi_remove (&gsi2, true);\n+    }\n+\n+  /* Finally get rid of the intermediate stmts.  */\n+  gimple *use_stmt;\n+  FOR_EACH_IMM_USE_STMT (use_stmt, iter, lhs)\n+    {\n+      if (is_gimple_debug (use_stmt))\n+\t{\n+\t  if (gimple_debug_bind_p (use_stmt))\n+\t    {\n+\t      gimple_debug_bind_reset_value (use_stmt);\n+\t      update_stmt (use_stmt);\n+\t    }\n+\t  continue;\n+\t}\n+      gimple_stmt_iterator gsi2 = gsi_for_stmt (use_stmt);\n+      unlink_stmt_vdef (use_stmt);\n+      release_defs (use_stmt);\n+      gsi_remove (&gsi2, true);\n+    }\n+  /* And the original load.  */\n+  release_defs (stmt);\n+  gsi_remove (gsi, true);\n+}\n+\n+\n /* Primitive \"lattice\" function for gimple_simplify.  */\n \n static tree\n@@ -3007,71 +3183,15 @@ pass_forwprop::execute (function *fun)\n \t\tgsi_next (&gsi);\n \t    }\n \t  else if (TREE_CODE (TREE_TYPE (lhs)) == VECTOR_TYPE\n-\t\t   && TYPE_MODE (TREE_TYPE (lhs)) == BLKmode\n+\t\t   && (TYPE_MODE (TREE_TYPE (lhs)) == BLKmode\n+\t\t       /* After vector lowering rewrite all loads, but\n+\t\t\t  initially do not since this conflicts with\n+\t\t\t  vector CONSTRUCTOR to shuffle optimization.  */\n+\t\t       || (fun->curr_properties & PROP_gimple_lvec))\n \t\t   && gimple_assign_load_p (stmt)\n \t\t   && !gimple_has_volatile_ops (stmt)\n-\t\t   && (TREE_CODE (gimple_assign_rhs1 (stmt))\n-\t\t       != TARGET_MEM_REF)\n \t\t   && !stmt_can_throw_internal (cfun, stmt))\n-\t    {\n-\t      /* Rewrite loads used only in BIT_FIELD_REF extractions to\n-\t         component-wise loads.  */\n-\t      use_operand_p use_p;\n-\t      imm_use_iterator iter;\n-\t      bool rewrite = true;\n-\t      FOR_EACH_IMM_USE_FAST (use_p, iter, lhs)\n-\t\t{\n-\t\t  gimple *use_stmt = USE_STMT (use_p);\n-\t\t  if (is_gimple_debug (use_stmt))\n-\t\t    continue;\n-\t\t  if (!is_gimple_assign (use_stmt)\n-\t\t      || gimple_assign_rhs_code (use_stmt) != BIT_FIELD_REF\n-\t\t      || TREE_OPERAND (gimple_assign_rhs1 (use_stmt), 0) != lhs)\n-\t\t    {\n-\t\t      rewrite = false;\n-\t\t      break;\n-\t\t    }\n-\t\t}\n-\t      if (rewrite)\n-\t\t{\n-\t\t  gimple *use_stmt;\n-\t\t  FOR_EACH_IMM_USE_STMT (use_stmt, iter, lhs)\n-\t\t    {\n-\t\t      if (is_gimple_debug (use_stmt))\n-\t\t\t{\n-\t\t\t  if (gimple_debug_bind_p (use_stmt))\n-\t\t\t    {\n-\t\t\t      gimple_debug_bind_reset_value (use_stmt);\n-\t\t\t      update_stmt (use_stmt);\n-\t\t\t    }\n-\t\t\t  continue;\n-\t\t\t}\n-\n-\t\t      tree bfr = gimple_assign_rhs1 (use_stmt);\n-\t\t      tree new_rhs = fold_build3 (BIT_FIELD_REF,\n-\t\t\t\t\t\t  TREE_TYPE (bfr),\n-\t\t\t\t\t\t  unshare_expr (rhs),\n-\t\t\t\t\t\t  TREE_OPERAND (bfr, 1),\n-\t\t\t\t\t\t  TREE_OPERAND (bfr, 2));\n-\t\t      gimple *new_stmt\n-\t\t\t= gimple_build_assign (gimple_assign_lhs (use_stmt),\n-\t\t\t\t\t       new_rhs);\n-\n-\t\t      location_t loc = gimple_location (use_stmt);\n-\t\t      gimple_set_location (new_stmt, loc);\n-\t\t      gimple_stmt_iterator gsi2 = gsi_for_stmt (use_stmt);\n-\t\t      unlink_stmt_vdef (use_stmt);\n-\t\t      gsi_remove (&gsi2, true);\n-\n-\t\t      gsi_insert_before (&gsi, new_stmt, GSI_SAME_STMT);\n-\t\t    }\n-\n-\t\t  release_defs (stmt);\n-\t\t  gsi_remove (&gsi, true);\n-\t\t}\n-\t      else\n-\t\tgsi_next (&gsi);\n-\t    }\n+\t    optimize_vector_load (&gsi);\n \n \t  else if (code == COMPLEX_EXPR)\n \t    {"}]}