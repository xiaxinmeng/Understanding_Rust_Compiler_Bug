{"sha": "e1306f499ca8cd493a69b96646c4241bdc3f99be", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZTEzMDZmNDk5Y2E4Y2Q0OTNhNjliOTY2NDZjNDI0MWJkYzNmOTliZQ==", "commit": {"author": {"name": "Bernd Schmidt", "email": "bernds@cygnus.co.uk", "date": "1999-11-29T11:44:57Z"}, "committer": {"name": "Bernd Schmidt", "email": "crux@gcc.gnu.org", "date": "1999-11-29T11:44:57Z"}, "message": "Clean up parts of the scheduler\n\nFrom-SVN: r30700", "tree": {"sha": "432a4fd59af987b1ac758344d1d7ba92894d94a4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/432a4fd59af987b1ac758344d1d7ba92894d94a4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/e1306f499ca8cd493a69b96646c4241bdc3f99be", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e1306f499ca8cd493a69b96646c4241bdc3f99be", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e1306f499ca8cd493a69b96646c4241bdc3f99be", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e1306f499ca8cd493a69b96646c4241bdc3f99be/comments", "author": null, "committer": null, "parents": [{"sha": "d94d6abfeb4e190a7e327877c2a6cf9d2c373c53", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d94d6abfeb4e190a7e327877c2a6cf9d2c373c53", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d94d6abfeb4e190a7e327877c2a6cf9d2c373c53"}], "stats": {"total": 965, "additions": 431, "deletions": 534}, "files": [{"sha": "5263d3ce4ab35f6424c272254be67f7192892068", "filename": "gcc/ChangeLog", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e1306f499ca8cd493a69b96646c4241bdc3f99be/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e1306f499ca8cd493a69b96646c4241bdc3f99be/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=e1306f499ca8cd493a69b96646c4241bdc3f99be", "patch": "@@ -1,5 +1,29 @@\n 1999-11-29  Bernd Schmidt  <bernds@cygnus.co.uk>\n \n+\t* haifa-sched.c (reg_last_uses, reg_last_sets, reg_last_clobbers,\n+\tpending_read_insns, pending_write_insns, pending_read_mems,\n+\tpending_write_mems, pending_list_length, last_pending_memory_flush,\n+\tlast_function_call, sched_before_next_call): Move static variables\n+\tinto a structure.\n+\t(bb_ prefixed versions): Replace with single array bb_deps.\n+\t(struct deps): New structure.\n+\t(add_insn_mem_dependence, flush_pending_lists, sched_analyze_1,\n+\tsched_analyze_2, sched_analyze_insn, sched_analyze): Accept new\n+\targument of type \"struct deps *\"; use that instead of global\n+\tvariables.  All callers changed.\n+\t(init_rgn_data_dependencies): Delete function.\n+\t(init_rtx_vector): Delete function.\n+\t(init_deps): New function.\n+\n+\t(free_pending_lists): Simplify, we always use the bb_deps array even\n+\tif only one basic block.\n+\t(compute_block_backward_dependences): Likewise.\n+\t(schedule_region): Likewise.\n+\n+\t(propagate_deps): New function, broken out of\n+\tcompute_block_backward_dependences.\n+\t(compute_block_backward_dependences): Use it.\n+\n \t* alpha.md: Delete useless patterns that tried to work around\n \tregister elimination problems.\n "}, {"sha": "e3eedbb6793a69b88a5b9afbc28ce6ae74c8b396", "filename": "gcc/haifa-sched.c", "status": "modified", "additions": 407, "deletions": 534, "changes": 941, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e1306f499ca8cd493a69b96646c4241bdc3f99be/gcc%2Fhaifa-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e1306f499ca8cd493a69b96646c4241bdc3f99be/gcc%2Fhaifa-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fhaifa-sched.c?ref=e1306f499ca8cd493a69b96646c4241bdc3f99be", "patch": "@@ -231,14 +231,66 @@ fix_sched_param (param, val)\n     warning (\"fix_sched_param: unknown param: %s\", param);\n }\n \n+/* Describe state of dependencies used during sched_analyze phase.  */\n+struct deps\n+{\n+  /* The *_insns and *_mems are paired lists.  Each pending memory operation\n+     will have a pointer to the MEM rtx on one list and a pointer to the\n+     containing insn on the other list in the same place in the list.  */\n+\n+  /* We can't use add_dependence like the old code did, because a single insn\n+     may have multiple memory accesses, and hence needs to be on the list\n+     once for each memory access.  Add_dependence won't let you add an insn\n+     to a list more than once.  */\n+\n+  /* An INSN_LIST containing all insns with pending read operations.  */\n+  rtx pending_read_insns;\n+\n+  /* An EXPR_LIST containing all MEM rtx's which are pending reads.  */\n+  rtx pending_read_mems;\n+\n+  /* An INSN_LIST containing all insns with pending write operations.  */\n+  rtx pending_write_insns;\n+\n+  /* An EXPR_LIST containing all MEM rtx's which are pending writes.  */\n+  rtx pending_write_mems;\n+\n+  /* Indicates the combined length of the two pending lists.  We must prevent\n+     these lists from ever growing too large since the number of dependencies\n+     produced is at least O(N*N), and execution time is at least O(4*N*N), as\n+     a function of the length of these pending lists.  */\n+  int pending_lists_length;\n+\n+  /* The last insn upon which all memory references must depend.\n+     This is an insn which flushed the pending lists, creating a dependency\n+     between it and all previously pending memory references.  This creates\n+     a barrier (or a checkpoint) which no memory reference is allowed to cross.\n+\n+     This includes all non constant CALL_INSNs.  When we do interprocedural\n+     alias analysis, this restriction can be relaxed.\n+     This may also be an INSN that writes memory if the pending lists grow\n+     too large.  */\n+  rtx last_pending_memory_flush;\n+\n+  /* The last function call we have seen.  All hard regs, and, of course,\n+     the last function call, must depend on this.  */\n+  rtx last_function_call;\n+\n+  /* The LOG_LINKS field of this is a list of insns which use a pseudo register\n+     that does not already cross a call.  We create dependencies between each\n+     of those insn and the next call insn, to ensure that they won't cross a call\n+     after scheduling is done.  */\n+  rtx sched_before_next_call;\n+\n+  /* Element N is the next insn that sets (hard or pseudo) register\n+     N within the current basic block; or zero, if there is no\n+     such insn.  Needed for new registers which may be introduced\n+     by splitting insns.  */\n+  rtx *reg_last_uses;\n+  rtx *reg_last_sets;\n+  rtx *reg_last_clobbers;\n+};\n \n-/* Element N is the next insn that sets (hard or pseudo) register\n-   N within the current basic block; or zero, if there is no\n-   such insn.  Needed for new registers which may be introduced\n-   by splitting insns.  */\n-static rtx *reg_last_uses;\n-static rtx *reg_last_sets;\n-static rtx *reg_last_clobbers;\n static regset reg_pending_sets;\n static regset reg_pending_clobbers;\n static int reg_pending_sets_all;\n@@ -427,12 +479,13 @@ static int potential_hazard PROTO ((int, rtx, int));\n static int insn_cost PROTO ((rtx, rtx, rtx));\n static int priority PROTO ((rtx));\n static void free_pending_lists PROTO ((void));\n-static void add_insn_mem_dependence PROTO ((rtx *, rtx *, rtx, rtx));\n-static void flush_pending_lists PROTO ((rtx, int));\n-static void sched_analyze_1 PROTO ((rtx, rtx));\n-static void sched_analyze_2 PROTO ((rtx, rtx));\n-static void sched_analyze_insn PROTO ((rtx, rtx, rtx));\n-static void sched_analyze PROTO ((rtx, rtx));\n+static void add_insn_mem_dependence PROTO ((struct deps *, rtx *, rtx *, rtx,\n+\t\t\t\t\t    rtx));\n+static void flush_pending_lists PROTO ((struct deps *, rtx, int));\n+static void sched_analyze_1 PROTO ((struct deps *, rtx, rtx));\n+static void sched_analyze_2 PROTO ((struct deps *, rtx, rtx));\n+static void sched_analyze_insn PROTO ((struct deps *, rtx, rtx, rtx));\n+static void sched_analyze PROTO ((struct deps *, rtx, rtx));\n static int rank_for_schedule PROTO ((const PTR, const PTR));\n static void swap_sort PROTO ((rtx *, int));\n static void queue_insn PROTO ((rtx, int));\n@@ -670,7 +723,6 @@ static int is_exception_free PROTO ((rtx, int, int));\n \n static char find_insn_mem_list PROTO ((rtx, rtx, rtx, rtx));\n static void compute_block_forward_dependences PROTO ((int));\n-static void init_rgn_data_dependences PROTO ((int));\n static void add_branch_dependences PROTO ((rtx, rtx));\n static void compute_block_backward_dependences PROTO ((int));\n void debug_dependencies PROTO ((void));\n@@ -731,7 +783,7 @@ static rtx move_insn1 PROTO ((rtx, rtx));\n static rtx move_insn PROTO ((rtx, rtx));\n static rtx group_leader PROTO ((rtx));\n static int set_priorities PROTO ((int));\n-static void init_rtx_vector PROTO ((rtx **, rtx *, int, int));\n+static void init_deps PROTO ((struct deps *));\n static void schedule_region PROTO ((int));\n \n #endif /* INSN_SCHEDULING */\n@@ -907,89 +959,20 @@ schedule_insns (dump_file)\n \n /* Computation of memory dependencies.  */\n \n-/* The *_insns and *_mems are paired lists.  Each pending memory operation\n-   will have a pointer to the MEM rtx on one list and a pointer to the\n-   containing insn on the other list in the same place in the list.  */\n-\n-/* We can't use add_dependence like the old code did, because a single insn\n-   may have multiple memory accesses, and hence needs to be on the list\n-   once for each memory access.  Add_dependence won't let you add an insn\n-   to a list more than once.  */\n-\n-/* An INSN_LIST containing all insns with pending read operations.  */\n-static rtx pending_read_insns;\n-\n-/* An EXPR_LIST containing all MEM rtx's which are pending reads.  */\n-static rtx pending_read_mems;\n-\n-/* An INSN_LIST containing all insns with pending write operations.  */\n-static rtx pending_write_insns;\n-\n-/* An EXPR_LIST containing all MEM rtx's which are pending writes.  */\n-static rtx pending_write_mems;\n-\n-/* Indicates the combined length of the two pending lists.  We must prevent\n-   these lists from ever growing too large since the number of dependencies\n-   produced is at least O(N*N), and execution time is at least O(4*N*N), as\n-   a function of the length of these pending lists.  */\n-\n-static int pending_lists_length;\n-\n-/* The last insn upon which all memory references must depend.\n-   This is an insn which flushed the pending lists, creating a dependency\n-   between it and all previously pending memory references.  This creates\n-   a barrier (or a checkpoint) which no memory reference is allowed to cross.\n-\n-   This includes all non constant CALL_INSNs.  When we do interprocedural\n-   alias analysis, this restriction can be relaxed.\n-   This may also be an INSN that writes memory if the pending lists grow\n-   too large.  */\n-\n-static rtx last_pending_memory_flush;\n-\n-/* The last function call we have seen.  All hard regs, and, of course,\n-   the last function call, must depend on this.  */\n-\n-static rtx last_function_call;\n-\n-/* The LOG_LINKS field of this is a list of insns which use a pseudo register\n-   that does not already cross a call.  We create dependencies between each\n-   of those insn and the next call insn, to ensure that they won't cross a call\n-   after scheduling is done.  */\n+/* Data structures for the computation of data dependences in a regions.  We\n+   keep one mem_deps structure for every basic block.  Before analyzing the\n+   data dependences for a bb, its variables are initialized as a function of\n+   the variables of its predecessors.  When the analysis for a bb completes,\n+   we save the contents to the corresponding bb_mem_deps[bb] variable.  */\n \n-static rtx sched_before_next_call;\n+static struct deps *bb_deps;\n \n /* Pointer to the last instruction scheduled.  Used by rank_for_schedule,\n    so that insns independent of the last scheduled insn will be preferred\n    over dependent instructions.  */\n \n static rtx last_scheduled_insn;\n \n-/* Data structures for the computation of data dependences in a regions.  We\n-   keep one copy of each of the declared above variables for each bb in the\n-   region.  Before analyzing the data dependences for a bb, its variables\n-   are initialized as a function of the variables of its predecessors.  When\n-   the analysis for a bb completes, we save the contents of each variable X\n-   to a corresponding bb_X[bb] variable.  For example, pending_read_insns is\n-   copied to bb_pending_read_insns[bb].  Another change is that few\n-   variables are now a list of insns rather than a single insn:\n-   last_pending_memory_flash, last_function_call, reg_last_sets.  The\n-   manipulation of these variables was changed appropriately.  */\n-\n-static rtx **bb_reg_last_uses;\n-static rtx **bb_reg_last_sets;\n-static rtx **bb_reg_last_clobbers;\n-\n-static rtx *bb_pending_read_insns;\n-static rtx *bb_pending_read_mems;\n-static rtx *bb_pending_write_insns;\n-static rtx *bb_pending_write_mems;\n-static int *bb_pending_lists_length;\n-\n-static rtx *bb_last_pending_memory_flush;\n-static rtx *bb_last_function_call;\n-static rtx *bb_sched_before_next_call;\n-\n /* Functions for construction of the control flow graph.  */\n \n /* Return 1 if control flow graph should not be constructed, 0 otherwise.\n@@ -3149,25 +3132,14 @@ priority (insn)\n static void\n free_pending_lists ()\n {\n-  if (current_nr_blocks <= 1)\n-    {\n-      free_INSN_LIST_list (&pending_read_insns);\n-      free_INSN_LIST_list (&pending_write_insns);\n-      free_EXPR_LIST_list (&pending_read_mems);\n-      free_EXPR_LIST_list (&pending_write_mems);\n-    }\n-  else\n-    {\n-      /* Interblock scheduling.  */\n-      int bb;\n+  int bb;\n \n-      for (bb = 0; bb < current_nr_blocks; bb++)\n-\t{\n-\t  free_INSN_LIST_list (&bb_pending_read_insns[bb]);\n-\t  free_INSN_LIST_list (&bb_pending_write_insns[bb]);\n-\t  free_EXPR_LIST_list (&bb_pending_read_mems[bb]);\n-\t  free_EXPR_LIST_list (&bb_pending_write_mems[bb]);\n-\t}\n+  for (bb = 0; bb < current_nr_blocks; bb++)\n+    {\n+      free_INSN_LIST_list (&bb_deps[bb].pending_read_insns);\n+      free_INSN_LIST_list (&bb_deps[bb].pending_write_insns);\n+      free_EXPR_LIST_list (&bb_deps[bb].pending_read_mems);\n+      free_EXPR_LIST_list (&bb_deps[bb].pending_write_mems);\n     }\n }\n \n@@ -3176,7 +3148,8 @@ free_pending_lists ()\n    so that we can do memory aliasing on it.  */\n \n static void\n-add_insn_mem_dependence (insn_list, mem_list, insn, mem)\n+add_insn_mem_dependence (deps, insn_list, mem_list, insn, mem)\n+     struct deps *deps;\n      rtx *insn_list, *mem_list, insn, mem;\n {\n   register rtx link;\n@@ -3187,62 +3160,65 @@ add_insn_mem_dependence (insn_list, mem_list, insn, mem)\n   link = alloc_EXPR_LIST (VOIDmode, mem, *mem_list);\n   *mem_list = link;\n \n-  pending_lists_length++;\n+  deps->pending_lists_length++;\n }\n \f\n-\n /* Make a dependency between every memory reference on the pending lists\n    and INSN, thus flushing the pending lists.  If ONLY_WRITE, don't flush\n    the read list.  */\n \n static void\n-flush_pending_lists (insn, only_write)\n+flush_pending_lists (deps, insn, only_write)\n+     struct deps *deps;\n      rtx insn;\n      int only_write;\n {\n   rtx u;\n   rtx link;\n \n-  while (pending_read_insns && ! only_write)\n+  while (deps->pending_read_insns && ! only_write)\n     {\n-      add_dependence (insn, XEXP (pending_read_insns, 0), REG_DEP_ANTI);\n+      add_dependence (insn, XEXP (deps->pending_read_insns, 0),\n+\t\t      REG_DEP_ANTI);\n \n-      link = pending_read_insns;\n-      pending_read_insns = XEXP (pending_read_insns, 1);\n+      link = deps->pending_read_insns;\n+      deps->pending_read_insns = XEXP (deps->pending_read_insns, 1);\n       free_INSN_LIST_node (link);\n \n-      link = pending_read_mems;\n-      pending_read_mems = XEXP (pending_read_mems, 1);\n+      link = deps->pending_read_mems;\n+      deps->pending_read_mems = XEXP (deps->pending_read_mems, 1);\n       free_EXPR_LIST_node (link);\n     }\n-  while (pending_write_insns)\n+  while (deps->pending_write_insns)\n     {\n-      add_dependence (insn, XEXP (pending_write_insns, 0), REG_DEP_ANTI);\n+      add_dependence (insn, XEXP (deps->pending_write_insns, 0),\n+\t\t      REG_DEP_ANTI);\n \n-      link = pending_write_insns;\n-      pending_write_insns = XEXP (pending_write_insns, 1);\n+      link = deps->pending_write_insns;\n+      deps->pending_write_insns = XEXP (deps->pending_write_insns, 1);\n       free_INSN_LIST_node (link);\n \n-      link = pending_write_mems;\n-      pending_write_mems = XEXP (pending_write_mems, 1);\n+      link = deps->pending_write_mems;\n+      deps->pending_write_mems = XEXP (deps->pending_write_mems, 1);\n       free_EXPR_LIST_node (link);\n     }\n-  pending_lists_length = 0;\n+  deps->pending_lists_length = 0;\n \n   /* last_pending_memory_flush is now a list of insns.  */\n-  for (u = last_pending_memory_flush; u; u = XEXP (u, 1))\n+  for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n     add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \n-  free_INSN_LIST_list (&last_pending_memory_flush);\n-  last_pending_memory_flush = alloc_INSN_LIST (insn, NULL_RTX);\n+  free_INSN_LIST_list (&deps->last_pending_memory_flush);\n+  deps->last_pending_memory_flush = alloc_INSN_LIST (insn, NULL_RTX);\n }\n \n /* Analyze a single SET, CLOBBER, PRE_DEC, POST_DEC, PRE_INC or POST_INC\n    rtx, X, creating all dependencies generated by the write to the\n    destination of X, and reads of everything mentioned.  */\n \n static void\n-sched_analyze_1 (x, insn)\n+sched_analyze_1 (deps, x, insn)\n+     struct deps *deps;\n      rtx x;\n      rtx insn;\n {\n@@ -3258,9 +3234,9 @@ sched_analyze_1 (x, insn)\n     {\n       register int i;\n       for (i = XVECLEN (dest, 0) - 1; i >= 0; i--)\n-\tsched_analyze_1 (XVECEXP (dest, 0, i), insn);\n+\tsched_analyze_1 (deps, XVECEXP (dest, 0, i), insn);\n       if (GET_CODE (x) == SET)\n-\tsched_analyze_2 (SET_SRC (x), insn);\n+\tsched_analyze_2 (deps, SET_SRC (x), insn);\n       return;\n     }\n \n@@ -3270,8 +3246,8 @@ sched_analyze_1 (x, insn)\n       if (GET_CODE (dest) == ZERO_EXTRACT || GET_CODE (dest) == SIGN_EXTRACT)\n \t{\n \t  /* The second and third arguments are values read by this insn.  */\n-\t  sched_analyze_2 (XEXP (dest, 1), insn);\n-\t  sched_analyze_2 (XEXP (dest, 2), insn);\n+\t  sched_analyze_2 (deps, XEXP (dest, 1), insn);\n+\t  sched_analyze_2 (deps, XEXP (dest, 2), insn);\n \t}\n       dest = XEXP (dest, 0);\n     }\n@@ -3289,48 +3265,48 @@ sched_analyze_1 (x, insn)\n \t  i = HARD_REGNO_NREGS (regno, GET_MODE (dest));\n \t  while (--i >= 0)\n \t    {\n+\t      int r = regno + i;\n \t      rtx u;\n \n-\t      for (u = reg_last_uses[regno + i]; u; u = XEXP (u, 1))\n+\t      for (u = deps->reg_last_uses[r]; u; u = XEXP (u, 1))\n \t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \n-\t      for (u = reg_last_sets[regno + i]; u; u = XEXP (u, 1))\n+\t      for (u = deps->reg_last_sets[r]; u; u = XEXP (u, 1))\n \t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n \n \t      /* Clobbers need not be ordered with respect to one\n \t\t another, but sets must be ordered with respect to a\n \t\t pending clobber.  */\n \t      if (code == SET)\n \t\t{\n-\t\t  free_INSN_LIST_list (&reg_last_uses[regno + i]);\n-\t          for (u = reg_last_clobbers[regno + i]; u; u = XEXP (u, 1))\n+\t\t  free_INSN_LIST_list (&deps->reg_last_uses[r]);\n+\t          for (u = deps->reg_last_clobbers[r]; u; u = XEXP (u, 1))\n \t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n-\t          SET_REGNO_REG_SET (reg_pending_sets, regno + i);\n+\t          SET_REGNO_REG_SET (reg_pending_sets, r);\n \t\t}\n \t      else\n-\t\tSET_REGNO_REG_SET (reg_pending_clobbers, regno + i);\n+\t\tSET_REGNO_REG_SET (reg_pending_clobbers, r);\n \n \t      /* Function calls clobber all call_used regs.  */\n-\t      if (global_regs[regno + i]\n-\t\t  || (code == SET && call_used_regs[regno + i]))\n-\t\tfor (u = last_function_call; u; u = XEXP (u, 1))\n+\t      if (global_regs[r] || (code == SET && call_used_regs[r]))\n+\t\tfor (u = deps->last_function_call; u; u = XEXP (u, 1))\n \t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \t    }\n \t}\n       else\n \t{\n \t  rtx u;\n \n-\t  for (u = reg_last_uses[regno]; u; u = XEXP (u, 1))\n+\t  for (u = deps->reg_last_uses[regno]; u; u = XEXP (u, 1))\n \t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \n-\t  for (u = reg_last_sets[regno]; u; u = XEXP (u, 1))\n+\t  for (u = deps->reg_last_sets[regno]; u; u = XEXP (u, 1))\n \t    add_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n \n \t  if (code == SET)\n \t    {\n-\t      free_INSN_LIST_list (&reg_last_uses[regno]);\n-\t      for (u = reg_last_clobbers[regno]; u; u = XEXP (u, 1))\n+\t      free_INSN_LIST_list (&deps->reg_last_uses[regno]);\n+\t      for (u = deps->reg_last_clobbers[regno]; u; u = XEXP (u, 1))\n \t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n \t      SET_REGNO_REG_SET (reg_pending_sets, regno);\n \t    }\n@@ -3343,37 +3319,37 @@ sched_analyze_1 (x, insn)\n \t  if (!reload_completed\n \t      && reg_known_equiv_p[regno]\n \t      && GET_CODE (reg_known_value[regno]) == MEM)\n-\t    sched_analyze_2 (XEXP (reg_known_value[regno], 0), insn);\n+\t    sched_analyze_2 (deps, XEXP (reg_known_value[regno], 0), insn);\n \n \t  /* Don't let it cross a call after scheduling if it doesn't\n \t     already cross one.  */\n \n \t  if (REG_N_CALLS_CROSSED (regno) == 0)\n-\t    for (u = last_function_call; u; u = XEXP (u, 1))\n+\t    for (u = deps->last_function_call; u; u = XEXP (u, 1))\n \t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \t}\n     }\n   else if (GET_CODE (dest) == MEM)\n     {\n       /* Writing memory.  */\n \n-      if (pending_lists_length > 32)\n+      if (deps->pending_lists_length > 32)\n \t{\n \t  /* Flush all pending reads and writes to prevent the pending lists\n \t     from getting any larger.  Insn scheduling runs too slowly when\n \t     these lists get long.  The number 32 was chosen because it\n \t     seems like a reasonable number.  When compiling GCC with itself,\n \t     this flush occurs 8 times for sparc, and 10 times for m88k using\n \t     the number 32.  */\n-\t  flush_pending_lists (insn, 0);\n+\t  flush_pending_lists (deps, insn, 0);\n \t}\n       else\n \t{\n \t  rtx u;\n \t  rtx pending, pending_mem;\n \n-\t  pending = pending_read_insns;\n-\t  pending_mem = pending_read_mems;\n+\t  pending = deps->pending_read_insns;\n+\t  pending_mem = deps->pending_read_mems;\n \t  while (pending)\n \t    {\n \t      if (anti_dependence (XEXP (pending_mem, 0), dest))\n@@ -3383,8 +3359,8 @@ sched_analyze_1 (x, insn)\n \t      pending_mem = XEXP (pending_mem, 1);\n \t    }\n \n-\t  pending = pending_write_insns;\n-\t  pending_mem = pending_write_mems;\n+\t  pending = deps->pending_write_insns;\n+\t  pending_mem = deps->pending_write_mems;\n \t  while (pending)\n \t    {\n \t      if (output_dependence (XEXP (pending_mem, 0), dest))\n@@ -3394,24 +3370,25 @@ sched_analyze_1 (x, insn)\n \t      pending_mem = XEXP (pending_mem, 1);\n \t    }\n \n-\t  for (u = last_pending_memory_flush; u; u = XEXP (u, 1))\n+\t  for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n \t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \n-\t  add_insn_mem_dependence (&pending_write_insns, &pending_write_mems,\n-\t\t\t\t   insn, dest);\n+\t  add_insn_mem_dependence (deps, &deps->pending_write_insns,\n+\t\t\t\t   &deps->pending_write_mems, insn, dest);\n \t}\n-      sched_analyze_2 (XEXP (dest, 0), insn);\n+      sched_analyze_2 (deps, XEXP (dest, 0), insn);\n     }\n \n   /* Analyze reads.  */\n   if (GET_CODE (x) == SET)\n-    sched_analyze_2 (SET_SRC (x), insn);\n+    sched_analyze_2 (deps, SET_SRC (x), insn);\n }\n \n /* Analyze the uses of memory and registers in rtx X in INSN.  */\n \n static void\n-sched_analyze_2 (x, insn)\n+sched_analyze_2 (deps, x, insn)\n+     struct deps *deps;\n      rtx x;\n      rtx insn;\n {\n@@ -3476,32 +3453,33 @@ sched_analyze_2 (x, insn)\n \t    i = HARD_REGNO_NREGS (regno, GET_MODE (x));\n \t    while (--i >= 0)\n \t      {\n-\t\treg_last_uses[regno + i]\n-\t\t  = alloc_INSN_LIST (insn, reg_last_uses[regno + i]);\n+\t\tint r = regno + i;\n+\t\tdeps->reg_last_uses[r]\n+\t\t  = alloc_INSN_LIST (insn, deps->reg_last_uses[r]);\n \n-\t\tfor (u = reg_last_sets[regno + i]; u; u = XEXP (u, 1))\n+\t\tfor (u = deps->reg_last_sets[r]; u; u = XEXP (u, 1))\n \t\t  add_dependence (insn, XEXP (u, 0), 0);\n \n \t\t/* ??? This should never happen.  */\n-\t\tfor (u = reg_last_clobbers[regno + i]; u; u = XEXP (u, 1))\n+\t\tfor (u = deps->reg_last_clobbers[r]; u; u = XEXP (u, 1))\n \t\t  add_dependence (insn, XEXP (u, 0), 0);\n \n-\t\tif ((call_used_regs[regno + i] || global_regs[regno + i]))\n+\t\tif (call_used_regs[r] || global_regs[r])\n \t\t  /* Function calls clobber all call_used regs.  */\n-\t\t  for (u = last_function_call; u; u = XEXP (u, 1))\n+\t\t  for (u = deps->last_function_call; u; u = XEXP (u, 1))\n \t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \t      }\n \t  }\n \telse\n \t  {\n-\t    reg_last_uses[regno] = alloc_INSN_LIST (insn,\n-\t\t\t\t\t\t    reg_last_uses[regno]);\n+\t    deps->reg_last_uses[regno]\n+\t      = alloc_INSN_LIST (insn, deps->reg_last_uses[regno]);\n \n-\t    for (u = reg_last_sets[regno]; u; u = XEXP (u, 1))\n+\t    for (u = deps->reg_last_sets[regno]; u; u = XEXP (u, 1))\n \t      add_dependence (insn, XEXP (u, 0), 0);\n \n \t    /* ??? This should never happen.  */\n-\t    for (u = reg_last_clobbers[regno]; u; u = XEXP (u, 1))\n+\t    for (u = deps->reg_last_clobbers[regno]; u; u = XEXP (u, 1))\n \t      add_dependence (insn, XEXP (u, 0), 0);\n \n \t    /* Pseudos that are REG_EQUIV to something may be replaced\n@@ -3510,13 +3488,14 @@ sched_analyze_2 (x, insn)\n \t    if (!reload_completed\n \t\t&& reg_known_equiv_p[regno]\n \t\t&& GET_CODE (reg_known_value[regno]) == MEM)\n-\t      sched_analyze_2 (XEXP (reg_known_value[regno], 0), insn);\n+\t      sched_analyze_2 (deps, XEXP (reg_known_value[regno], 0), insn);\n \n \t    /* If the register does not already cross any calls, then add this\n \t       insn to the sched_before_next_call list so that it will still\n \t       not cross calls after scheduling.  */\n \t    if (REG_N_CALLS_CROSSED (regno) == 0)\n-\t      add_dependence (sched_before_next_call, insn, REG_DEP_ANTI);\n+\t      add_dependence (deps->sched_before_next_call, insn,\n+\t\t\t      REG_DEP_ANTI);\n \t  }\n \treturn;\n       }\n@@ -3527,8 +3506,8 @@ sched_analyze_2 (x, insn)\n \trtx u;\n \trtx pending, pending_mem;\n \n-\tpending = pending_read_insns;\n-\tpending_mem = pending_read_mems;\n+\tpending = deps->pending_read_insns;\n+\tpending_mem = deps->pending_read_mems;\n \twhile (pending)\n \t  {\n \t    if (read_dependence (XEXP (pending_mem, 0), x))\n@@ -3538,8 +3517,8 @@ sched_analyze_2 (x, insn)\n \t    pending_mem = XEXP (pending_mem, 1);\n \t  }\n \n-\tpending = pending_write_insns;\n-\tpending_mem = pending_write_mems;\n+\tpending = deps->pending_write_insns;\n+\tpending_mem = deps->pending_write_mems;\n \twhile (pending)\n \t  {\n \t    if (true_dependence (XEXP (pending_mem, 0), VOIDmode,\n@@ -3550,22 +3529,22 @@ sched_analyze_2 (x, insn)\n \t    pending_mem = XEXP (pending_mem, 1);\n \t  }\n \n-\tfor (u = last_pending_memory_flush; u; u = XEXP (u, 1))\n+\tfor (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n \t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \n \t/* Always add these dependencies to pending_reads, since\n \t   this insn may be followed by a write.  */\n-\tadd_insn_mem_dependence (&pending_read_insns, &pending_read_mems,\n-\t\t\t\t insn, x);\n+\tadd_insn_mem_dependence (deps, &deps->pending_read_insns,\n+\t\t\t\t &deps->pending_read_mems, insn, x);\n \n \t/* Take advantage of tail recursion here.  */\n-\tsched_analyze_2 (XEXP (x, 0), insn);\n+\tsched_analyze_2 (deps, XEXP (x, 0), insn);\n \treturn;\n       }\n \n     /* Force pending stores to memory in case a trap handler needs them.  */\n     case TRAP_IF:\n-      flush_pending_lists (insn, 1);\n+      flush_pending_lists (deps, insn, 1);\n       break;\n \n     case ASM_OPERANDS:\n@@ -3586,19 +3565,19 @@ sched_analyze_2 (x, insn)\n \t    int max_reg = max_reg_num ();\n \t    for (i = 0; i < max_reg; i++)\n \t      {\n-\t\tfor (u = reg_last_uses[i]; u; u = XEXP (u, 1))\n+\t\tfor (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n \t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t\tfree_INSN_LIST_list (&reg_last_uses[i]);\n+\t\tfree_INSN_LIST_list (&deps->reg_last_uses[i]);\n \n-\t\tfor (u = reg_last_sets[i]; u; u = XEXP (u, 1))\n+\t\tfor (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n \t\t  add_dependence (insn, XEXP (u, 0), 0);\n \n-\t\tfor (u = reg_last_clobbers[i]; u; u = XEXP (u, 1))\n+\t\tfor (u = deps->reg_last_clobbers[i]; u; u = XEXP (u, 1))\n \t\t  add_dependence (insn, XEXP (u, 0), 0);\n \t      }\n \t    reg_pending_sets_all = 1;\n \n-\t    flush_pending_lists (insn, 0);\n+\t    flush_pending_lists (deps, insn, 0);\n \t  }\n \n \t/* For all ASM_OPERANDS, we must traverse the vector of input operands.\n@@ -3609,7 +3588,7 @@ sched_analyze_2 (x, insn)\n \tif (code == ASM_OPERANDS)\n \t  {\n \t    for (j = 0; j < ASM_OPERANDS_INPUT_LENGTH (x); j++)\n-\t      sched_analyze_2 (ASM_OPERANDS_INPUT (x, j), insn);\n+\t      sched_analyze_2 (deps, ASM_OPERANDS_INPUT (x, j), insn);\n \t    return;\n \t  }\n \tbreak;\n@@ -3625,8 +3604,8 @@ sched_analyze_2 (x, insn)\n          instructions.  Thus we need to pass them to both sched_analyze_1\n          and sched_analyze_2.  We must call sched_analyze_2 first in order\n          to get the proper antecedent for the read.  */\n-      sched_analyze_2 (XEXP (x, 0), insn);\n-      sched_analyze_1 (x, insn);\n+      sched_analyze_2 (deps, XEXP (x, 0), insn);\n+      sched_analyze_1 (deps, x, insn);\n       return;\n \n     default:\n@@ -3638,17 +3617,18 @@ sched_analyze_2 (x, insn)\n   for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n     {\n       if (fmt[i] == 'e')\n-\tsched_analyze_2 (XEXP (x, i), insn);\n+\tsched_analyze_2 (deps, XEXP (x, i), insn);\n       else if (fmt[i] == 'E')\n \tfor (j = 0; j < XVECLEN (x, i); j++)\n-\t  sched_analyze_2 (XVECEXP (x, i, j), insn);\n+\t  sched_analyze_2 (deps, XVECEXP (x, i, j), insn);\n     }\n }\n \n /* Analyze an INSN with pattern X to find all dependencies.  */\n \n static void\n-sched_analyze_insn (x, insn, loop_notes)\n+sched_analyze_insn (deps, x, insn, loop_notes)\n+     struct deps *deps;\n      rtx x, insn;\n      rtx loop_notes;\n {\n@@ -3658,30 +3638,30 @@ sched_analyze_insn (x, insn, loop_notes)\n   int i;\n \n   if (code == SET || code == CLOBBER)\n-    sched_analyze_1 (x, insn);\n+    sched_analyze_1 (deps, x, insn);\n   else if (code == PARALLEL)\n     {\n       register int i;\n       for (i = XVECLEN (x, 0) - 1; i >= 0; i--)\n \t{\n \t  code = GET_CODE (XVECEXP (x, 0, i));\n \t  if (code == SET || code == CLOBBER)\n-\t    sched_analyze_1 (XVECEXP (x, 0, i), insn);\n+\t    sched_analyze_1 (deps, XVECEXP (x, 0, i), insn);\n \t  else\n-\t    sched_analyze_2 (XVECEXP (x, 0, i), insn);\n+\t    sched_analyze_2 (deps, XVECEXP (x, 0, i), insn);\n \t}\n     }\n   else\n-    sched_analyze_2 (x, insn);\n+    sched_analyze_2 (deps, x, insn);\n \n   /* Mark registers CLOBBERED or used by called function.  */\n   if (GET_CODE (insn) == CALL_INSN)\n     for (link = CALL_INSN_FUNCTION_USAGE (insn); link; link = XEXP (link, 1))\n       {\n \tif (GET_CODE (XEXP (link, 0)) == CLOBBER)\n-\t  sched_analyze_1 (XEXP (link, 0), insn);\n+\t  sched_analyze_1 (deps, XEXP (link, 0), insn);\n \telse\n-\t  sched_analyze_2 (XEXP (link, 0), insn);\n+\t  sched_analyze_2 (deps, XEXP (link, 0), insn);\n       }\n \n   /* If there is a {LOOP,EHREGION}_{BEG,END} note in the middle of a basic\n@@ -3719,49 +3699,49 @@ sched_analyze_insn (x, insn, loop_notes)\n \t  for (i = 0; i < max_reg; i++)\n \t    {\n \t      rtx u;\n-\t      for (u = reg_last_uses[i]; u; u = XEXP (u, 1))\n+\t      for (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n \t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t      free_INSN_LIST_list (&reg_last_uses[i]);\n+\t      free_INSN_LIST_list (&deps->reg_last_uses[i]);\n \n-\t      for (u = reg_last_sets[i]; u; u = XEXP (u, 1))\n+\t      for (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n \t\tadd_dependence (insn, XEXP (u, 0), 0);\n \n-\t      for (u = reg_last_clobbers[i]; u; u = XEXP (u, 1))\n+\t      for (u = deps->reg_last_clobbers[i]; u; u = XEXP (u, 1))\n \t\tadd_dependence (insn, XEXP (u, 0), 0);\n \t    }\n \t  reg_pending_sets_all = 1;\n \n-\t  flush_pending_lists (insn, 0);\n+\t  flush_pending_lists (deps, insn, 0);\n \t}\n \n     }\n \n   /* Accumulate clobbers until the next set so that it will be output dependent\n      on all of them.  At the next set we can clear the clobber list, since\n      subsequent sets will be output dependent on it.  */\n-  EXECUTE_IF_SET_IN_REG_SET (reg_pending_sets, 0, i,\n-\t\t\t     {\n-\t\t\t       free_INSN_LIST_list (&reg_last_sets[i]);\n-\t\t\t       free_INSN_LIST_list (&reg_last_clobbers[i]);\n-\t\t\t       reg_last_sets[i]\n-\t\t\t\t = alloc_INSN_LIST (insn, NULL_RTX);\n-\t\t\t     });\n-  EXECUTE_IF_SET_IN_REG_SET (reg_pending_clobbers, 0, i,\n-\t\t\t     {\n-\t\t\t       reg_last_clobbers[i]\n-\t\t\t\t = alloc_INSN_LIST (insn, \n-\t\t\t\t\t\t    reg_last_clobbers[i]);\n-\t\t\t     });\n+  EXECUTE_IF_SET_IN_REG_SET\n+    (reg_pending_sets, 0, i,\n+     {\n+       free_INSN_LIST_list (&deps->reg_last_sets[i]);\n+       free_INSN_LIST_list (&deps->reg_last_clobbers[i]);\n+       deps->reg_last_sets[i] = alloc_INSN_LIST (insn, NULL_RTX);\n+     });\n+  EXECUTE_IF_SET_IN_REG_SET\n+    (reg_pending_clobbers, 0, i,\n+     {\n+       deps->reg_last_clobbers[i]\n+\t = alloc_INSN_LIST (insn, deps->reg_last_clobbers[i]);\n+     });\n   CLEAR_REG_SET (reg_pending_sets);\n   CLEAR_REG_SET (reg_pending_clobbers);\n \n   if (reg_pending_sets_all)\n     {\n       for (i = 0; i < maxreg; i++)\n \t{\n-\t  free_INSN_LIST_list (&reg_last_sets[i]);\n-\t  free_INSN_LIST_list (&reg_last_clobbers[i]);\n-\t  reg_last_sets[i] = alloc_INSN_LIST (insn, NULL_RTX);\n+\t  free_INSN_LIST_list (&deps->reg_last_sets[i]);\n+\t  free_INSN_LIST_list (&deps->reg_last_clobbers[i]);\n+\t  deps->reg_last_sets[i] = alloc_INSN_LIST (insn, NULL_RTX);\n \t}\n \n       reg_pending_sets_all = 0;\n@@ -3808,7 +3788,8 @@ sched_analyze_insn (x, insn, loop_notes)\n    for every dependency.  */\n \n static void\n-sched_analyze (head, tail)\n+sched_analyze (deps, head, tail)\n+     struct deps *deps;\n      rtx head, tail;\n {\n   register rtx insn;\n@@ -3825,9 +3806,9 @@ sched_analyze (head, tail)\n \t  /* Make each JUMP_INSN a scheduling barrier for memory\n              references.  */\n \t  if (GET_CODE (insn) == JUMP_INSN)\n-\t    last_pending_memory_flush\n-\t      = alloc_INSN_LIST (insn, last_pending_memory_flush);\n-\t  sched_analyze_insn (PATTERN (insn), insn, loop_notes);\n+\t    deps->last_pending_memory_flush\n+\t      = alloc_INSN_LIST (insn, deps->last_pending_memory_flush);\n+\t  sched_analyze_insn (deps, PATTERN (insn), insn, loop_notes);\n \t  loop_notes = 0;\n \t}\n       else if (GET_CODE (insn) == CALL_INSN)\n@@ -3859,14 +3840,14 @@ sched_analyze (head, tail)\n \t      int max_reg = max_reg_num ();\n \t      for (i = 0; i < max_reg; i++)\n \t\t{\n-\t\t  for (u = reg_last_uses[i]; u; u = XEXP (u, 1))\n+\t\t  for (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n \t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t\t  free_INSN_LIST_list (&reg_last_uses[i]);\n+\t\t  free_INSN_LIST_list (&deps->reg_last_uses[i]);\n \n-\t\t  for (u = reg_last_sets[i]; u; u = XEXP (u, 1))\n+\t\t  for (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n \t\t    add_dependence (insn, XEXP (u, 0), 0);\n \n-\t\t  for (u = reg_last_clobbers[i]; u; u = XEXP (u, 1))\n+\t\t  for (u = deps->reg_last_clobbers[i]; u; u = XEXP (u, 1))\n \t\t    add_dependence (insn, XEXP (u, 0), 0);\n \t\t}\n \t      reg_pending_sets_all = 1;\n@@ -3886,10 +3867,10 @@ sched_analyze (head, tail)\n \t      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n \t\tif (call_used_regs[i] || global_regs[i])\n \t\t  {\n-\t\t    for (u = reg_last_uses[i]; u; u = XEXP (u, 1))\n+\t\t    for (u = deps->reg_last_uses[i]; u; u = XEXP (u, 1))\n \t\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \n-\t\t    for (u = reg_last_sets[i]; u; u = XEXP (u, 1))\n+\t\t    for (u = deps->reg_last_sets[i]; u; u = XEXP (u, 1))\n \t\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \n \t\t    SET_REGNO_REG_SET (reg_pending_clobbers, i);\n@@ -3898,29 +3879,29 @@ sched_analyze (head, tail)\n \n \t  /* For each insn which shouldn't cross a call, add a dependence\n \t     between that insn and this call insn.  */\n-\t  x = LOG_LINKS (sched_before_next_call);\n+\t  x = LOG_LINKS (deps->sched_before_next_call);\n \t  while (x)\n \t    {\n \t      add_dependence (insn, XEXP (x, 0), REG_DEP_ANTI);\n \t      x = XEXP (x, 1);\n \t    }\n-\t  free_INSN_LIST_list (&LOG_LINKS (sched_before_next_call));\n+\t  free_INSN_LIST_list (&LOG_LINKS (deps->sched_before_next_call));\n \n-\t  sched_analyze_insn (PATTERN (insn), insn, loop_notes);\n+\t  sched_analyze_insn (deps, PATTERN (insn), insn, loop_notes);\n \t  loop_notes = 0;\n \n \t  /* In the absence of interprocedural alias analysis, we must flush\n \t     all pending reads and writes, and start new dependencies starting\n \t     from here.  But only flush writes for constant calls (which may\n \t     be passed a pointer to something we haven't written yet).  */\n-\t  flush_pending_lists (insn, CONST_CALL_P (insn));\n+\t  flush_pending_lists (deps, insn, CONST_CALL_P (insn));\n \n \t  /* Depend this function call (actually, the user of this\n \t     function call) on all hard register clobberage.  */\n \n \t  /* last_function_call is now a list of insns.  */\n-\t  free_INSN_LIST_list(&last_function_call);\n-\t  last_function_call = alloc_INSN_LIST (insn, NULL_RTX);\n+\t  free_INSN_LIST_list (&deps->last_function_call);\n+\t  deps->last_function_call = alloc_INSN_LIST (insn, NULL_RTX);\n \t}\n \n       /* See comments on reemit_notes as to why we do this.  \n@@ -6194,30 +6175,27 @@ compute_block_forward_dependences (bb)\n /* Initialize variables for region data dependence analysis.\n    n_bbs is the number of region blocks.  */\n \n-__inline static void\n-init_rgn_data_dependences (n_bbs)\n-     int n_bbs;\n+static void\n+init_deps (deps)\n+     struct deps *deps;\n {\n-  int bb;\n-\n-  /* Variables for which one copy exists for each block.  */\n-  bzero ((char *) bb_pending_read_insns, n_bbs * sizeof (rtx));\n-  bzero ((char *) bb_pending_read_mems, n_bbs * sizeof (rtx));\n-  bzero ((char *) bb_pending_write_insns, n_bbs * sizeof (rtx));\n-  bzero ((char *) bb_pending_write_mems, n_bbs * sizeof (rtx));\n-  bzero ((char *) bb_pending_lists_length, n_bbs * sizeof (int));\n-  bzero ((char *) bb_last_pending_memory_flush, n_bbs * sizeof (rtx));\n-  bzero ((char *) bb_last_function_call, n_bbs * sizeof (rtx));\n-  bzero ((char *) bb_sched_before_next_call, n_bbs * sizeof (rtx));\n-\n-  /* Create an insn here so that we can hang dependencies off of it later.  */\n-  for (bb = 0; bb < n_bbs; bb++)\n-    {\n-      bb_sched_before_next_call[bb] =\n-\tgen_rtx_INSN (VOIDmode, 0, NULL_RTX, NULL_RTX,\n-\t\t      NULL_RTX, 0, NULL_RTX, NULL_RTX);\n-      LOG_LINKS (bb_sched_before_next_call[bb]) = 0;\n-    }\n+  int maxreg = max_reg_num ();\n+  deps->reg_last_uses = (rtx *) xcalloc (maxreg, sizeof (rtx));\n+  deps->reg_last_sets = (rtx *) xcalloc (maxreg, sizeof (rtx));\n+  deps->reg_last_clobbers = (rtx *) xcalloc (maxreg, sizeof (rtx));\n+\n+  deps->pending_read_insns = 0;\n+  deps->pending_read_mems = 0;\n+  deps->pending_write_insns = 0;\n+  deps->pending_write_mems = 0;\n+  deps->pending_lists_length = 0;\n+  deps->last_pending_memory_flush = 0;\n+  deps->last_function_call = 0;\n+\n+  deps->sched_before_next_call\n+    = gen_rtx_INSN (VOIDmode, 0, NULL_RTX, NULL_RTX,\n+\t\t    NULL_RTX, 0, NULL_RTX, NULL_RTX);\n+  LOG_LINKS (deps->sched_before_next_call) = 0;\n }\n \n /* Add dependences so that branches are scheduled to run last in their\n@@ -6227,7 +6205,6 @@ static void\n add_branch_dependences (head, tail)\n      rtx head, tail;\n {\n-\n   rtx insn, last;\n \n   /* For all branches, calls, uses, clobbers, and cc0 setters, force them\n@@ -6303,6 +6280,154 @@ add_branch_dependences (head, tail)\n       }\n }\n \n+/* After computing the dependencies for block BB, propagate the dependencies\n+   found in TMP_DEPS to the successors of the block.  MAX_REG is the number\n+   of registers.  */\n+static void\n+propagate_deps (bb, tmp_deps, max_reg)\n+     int bb;\n+     struct deps *tmp_deps;\n+     int max_reg;\n+{\n+  int b = BB_TO_BLOCK (bb);\n+  int e, first_edge;\n+  int reg;\n+  rtx link_insn, link_mem;\n+  rtx u;\n+\n+  /* These lists should point to the right place, for correct\n+     freeing later.  */\n+  bb_deps[bb].pending_read_insns = tmp_deps->pending_read_insns;\n+  bb_deps[bb].pending_read_mems = tmp_deps->pending_read_mems;\n+  bb_deps[bb].pending_write_insns = tmp_deps->pending_write_insns;\n+  bb_deps[bb].pending_write_mems = tmp_deps->pending_write_mems;\n+\n+  /* bb's structures are inherited by its successors.  */\n+  first_edge = e = OUT_EDGES (b);\n+  if (e <= 0)\n+    return;\n+\n+  do\n+    {\n+      rtx x;\n+      int b_succ = TO_BLOCK (e);\n+      int bb_succ = BLOCK_TO_BB (b_succ);\n+      struct deps *succ_deps = bb_deps + bb_succ;\n+\n+      /* Only bbs \"below\" bb, in the same region, are interesting.  */\n+      if (CONTAINING_RGN (b) != CONTAINING_RGN (b_succ)\n+\t  || bb_succ <= bb)\n+\t{\n+\t  e = NEXT_OUT (e);\n+\t  continue;\n+\t}\n+\n+      for (reg = 0; reg < max_reg; reg++)\n+\t{\n+\t  /* reg-last-uses lists are inherited by bb_succ.  */\n+\t  for (u = tmp_deps->reg_last_uses[reg]; u; u = XEXP (u, 1))\n+\t    {\n+\t      if (find_insn_list (XEXP (u, 0),\n+\t\t\t\t  succ_deps->reg_last_uses[reg]))\n+\t\tcontinue;\n+\n+\t      succ_deps->reg_last_uses[reg]\n+\t\t= alloc_INSN_LIST (XEXP (u, 0),\n+\t\t\t\t   succ_deps->reg_last_uses[reg]);\n+\t    }\n+\n+\t  /* reg-last-defs lists are inherited by bb_succ.  */\n+\t  for (u = tmp_deps->reg_last_sets[reg]; u; u = XEXP (u, 1))\n+\t    {\n+\t      if (find_insn_list (XEXP (u, 0),\n+\t\t\t\t  succ_deps->reg_last_sets[reg]))\n+\t\tcontinue;\n+\n+\t      succ_deps->reg_last_sets[reg]\n+\t\t= alloc_INSN_LIST (XEXP (u, 0),\n+\t\t\t\t   succ_deps->reg_last_sets[reg]);\n+\t    }\n+\n+\t  for (u = tmp_deps->reg_last_clobbers[reg]; u; u = XEXP (u, 1))\n+\t    {\n+\t      if (find_insn_list (XEXP (u, 0),\n+\t\t\t\t  succ_deps->reg_last_clobbers[reg]))\n+\t\tcontinue;\n+\n+\t      succ_deps->reg_last_clobbers[reg]\n+\t\t= alloc_INSN_LIST (XEXP (u, 0),\n+\t\t\t\t   succ_deps->reg_last_clobbers[reg]);\n+\t    }\n+\t}\n+\n+      /* Mem read/write lists are inherited by bb_succ.  */\n+      link_insn = tmp_deps->pending_read_insns;\n+      link_mem = tmp_deps->pending_read_mems;\n+      while (link_insn)\n+\t{\n+\t  if (!(find_insn_mem_list (XEXP (link_insn, 0),\n+\t\t\t\t    XEXP (link_mem, 0),\n+\t\t\t\t    succ_deps->pending_read_insns,\n+\t\t\t\t    succ_deps->pending_read_mems)))\n+\t    add_insn_mem_dependence (succ_deps, &succ_deps->pending_read_insns,\n+\t\t\t\t     &succ_deps->pending_read_mems,\n+\t\t\t\t     XEXP (link_insn, 0), XEXP (link_mem, 0));\n+\t  link_insn = XEXP (link_insn, 1);\n+\t  link_mem = XEXP (link_mem, 1);\n+\t}\n+\n+      link_insn = tmp_deps->pending_write_insns;\n+      link_mem = tmp_deps->pending_write_mems;\n+      while (link_insn)\n+\t{\n+\t  if (!(find_insn_mem_list (XEXP (link_insn, 0),\n+\t\t\t\t    XEXP (link_mem, 0),\n+\t\t\t\t    succ_deps->pending_write_insns,\n+\t\t\t\t    succ_deps->pending_write_mems)))\n+\t    add_insn_mem_dependence (succ_deps,\n+\t\t\t\t     &succ_deps->pending_write_insns,\n+\t\t\t\t     &succ_deps->pending_write_mems,\n+\t\t\t\t     XEXP (link_insn, 0), XEXP (link_mem, 0));\n+\n+\t  link_insn = XEXP (link_insn, 1);\n+\t  link_mem = XEXP (link_mem, 1);\n+\t}\n+\n+      /* last_function_call is inherited by bb_succ.  */\n+      for (u = tmp_deps->last_function_call; u; u = XEXP (u, 1))\n+\t{\n+\t  if (find_insn_list (XEXP (u, 0),\n+\t\t\t      succ_deps->last_function_call))\n+\t    continue;\n+\n+\t  succ_deps->last_function_call\n+\t    = alloc_INSN_LIST (XEXP (u, 0),\n+\t\t\t       succ_deps->last_function_call);\n+\t}\n+\n+      /* last_pending_memory_flush is inherited by bb_succ.  */\n+      for (u = tmp_deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n+\t{\n+\t  if (find_insn_list (XEXP (u, 0), \n+\t\t\t      succ_deps->last_pending_memory_flush))\n+\t    continue;\n+\n+\t  succ_deps->last_pending_memory_flush\n+\t    = alloc_INSN_LIST (XEXP (u, 0),\n+\t\t\t       succ_deps->last_pending_memory_flush);\n+\t}\n+\n+      /* sched_before_next_call is inherited by bb_succ.  */\n+      x = LOG_LINKS (tmp_deps->sched_before_next_call);\n+      for (; x; x = XEXP (x, 1))\n+\tadd_dependence (succ_deps->sched_before_next_call,\n+\t\t\tXEXP (x, 0), REG_DEP_ANTI);\n+\n+      e = NEXT_OUT (e);\n+    }\n+  while (e != first_edge);\n+}\n+\n /* Compute backward dependences inside bb.  In a multiple blocks region:\n    (1) a bb is analyzed after its predecessors, and (2) the lists in\n    effect at the end of bb (after analyzing for bb) are inherited by\n@@ -6324,189 +6449,20 @@ static void\n compute_block_backward_dependences (bb)\n      int bb;\n {\n-  int b;\n-  rtx x;\n+  int i;\n   rtx head, tail;\n   int max_reg = max_reg_num ();\n+  struct deps tmp_deps;\n \n-  b = BB_TO_BLOCK (bb);\n-\n-  if (current_nr_blocks == 1)\n-    {\n-      reg_last_uses = (rtx *) xcalloc (max_reg, sizeof (rtx));\n-      reg_last_sets = (rtx *) xcalloc (max_reg, sizeof (rtx));\n-      reg_last_clobbers = (rtx *) xcalloc (max_reg, sizeof (rtx));\n-\n-      pending_read_insns = 0;\n-      pending_read_mems = 0;\n-      pending_write_insns = 0;\n-      pending_write_mems = 0;\n-      pending_lists_length = 0;\n-      last_function_call = 0;\n-      last_pending_memory_flush = 0;\n-      sched_before_next_call\n-\t= gen_rtx_INSN (VOIDmode, 0, NULL_RTX, NULL_RTX,\n-\t\t\tNULL_RTX, 0, NULL_RTX, NULL_RTX);\n-      LOG_LINKS (sched_before_next_call) = 0;\n-    }\n-  else\n-    {\n-      reg_last_uses = bb_reg_last_uses[bb];\n-      reg_last_sets = bb_reg_last_sets[bb];\n-      reg_last_clobbers = bb_reg_last_clobbers[bb];\n-\n-      pending_read_insns = bb_pending_read_insns[bb];\n-      pending_read_mems = bb_pending_read_mems[bb];\n-      pending_write_insns = bb_pending_write_insns[bb];\n-      pending_write_mems = bb_pending_write_mems[bb];\n-      pending_lists_length = bb_pending_lists_length[bb];\n-      last_function_call = bb_last_function_call[bb];\n-      last_pending_memory_flush = bb_last_pending_memory_flush[bb];\n-\n-      sched_before_next_call = bb_sched_before_next_call[bb];\n-    }\n+  tmp_deps = bb_deps[bb];\n \n   /* Do the analysis for this block.  */\n   get_bb_head_tail (bb, &head, &tail);\n-  sched_analyze (head, tail);\n+  sched_analyze (&tmp_deps, head, tail);\n   add_branch_dependences (head, tail);\n \n   if (current_nr_blocks > 1)\n-    {\n-      int e, first_edge;\n-      int b_succ, bb_succ;\n-      int reg;\n-      rtx link_insn, link_mem;\n-      rtx u;\n-\n-      /* These lists should point to the right place, for correct\n-         freeing later.  */\n-      bb_pending_read_insns[bb] = pending_read_insns;\n-      bb_pending_read_mems[bb] = pending_read_mems;\n-      bb_pending_write_insns[bb] = pending_write_insns;\n-      bb_pending_write_mems[bb] = pending_write_mems;\n-\n-      /* bb's structures are inherited by it's successors.  */\n-      first_edge = e = OUT_EDGES (b);\n-      if (e > 0)\n-\tdo\n-\t  {\n-\t    b_succ = TO_BLOCK (e);\n-\t    bb_succ = BLOCK_TO_BB (b_succ);\n-\n-\t    /* Only bbs \"below\" bb, in the same region, are interesting.  */\n-\t    if (CONTAINING_RGN (b) != CONTAINING_RGN (b_succ)\n-\t\t|| bb_succ <= bb)\n-\t      {\n-\t\te = NEXT_OUT (e);\n-\t\tcontinue;\n-\t      }\n-\n-\t    for (reg = 0; reg < max_reg; reg++)\n-\t      {\n-\n-\t\t/* reg-last-uses lists are inherited by bb_succ.  */\n-\t\tfor (u = reg_last_uses[reg]; u; u = XEXP (u, 1))\n-\t\t  {\n-\t\t    if (find_insn_list (XEXP (u, 0),\n-\t\t\t\t\t(bb_reg_last_uses[bb_succ])[reg]))\n-\t\t      continue;\n-\n-\t\t    (bb_reg_last_uses[bb_succ])[reg]\n-\t\t      = alloc_INSN_LIST (XEXP (u, 0),\n-\t\t\t\t\t (bb_reg_last_uses[bb_succ])[reg]);\n-\t\t  }\n-\n-\t\t/* reg-last-defs lists are inherited by bb_succ.  */\n-\t\tfor (u = reg_last_sets[reg]; u; u = XEXP (u, 1))\n-\t\t  {\n-\t\t    if (find_insn_list (XEXP (u, 0),\n-\t\t\t\t\t(bb_reg_last_sets[bb_succ])[reg]))\n-\t\t      continue;\n-\n-\t\t    (bb_reg_last_sets[bb_succ])[reg]\n-\t\t      = alloc_INSN_LIST (XEXP (u, 0),\n-\t\t\t\t\t (bb_reg_last_sets[bb_succ])[reg]);\n-\t\t  }\n-\n-\t\tfor (u = reg_last_clobbers[reg]; u; u = XEXP (u, 1))\n-\t\t  {\n-\t\t    if (find_insn_list (XEXP (u, 0),\n-\t\t\t\t\t(bb_reg_last_clobbers[bb_succ])[reg]))\n-\t\t      continue;\n-\n-\t\t    (bb_reg_last_clobbers[bb_succ])[reg]\n-\t\t      = alloc_INSN_LIST (XEXP (u, 0),\n-\t\t\t\t\t (bb_reg_last_clobbers[bb_succ])[reg]);\n-\t\t  }\n-\t      }\n-\n-\t    /* Mem read/write lists are inherited by bb_succ.  */\n-\t    link_insn = pending_read_insns;\n-\t    link_mem = pending_read_mems;\n-\t    while (link_insn)\n-\t      {\n-\t\tif (!(find_insn_mem_list (XEXP (link_insn, 0),\n-\t\t\t\t\t  XEXP (link_mem, 0),\n-\t\t\t\t\t  bb_pending_read_insns[bb_succ],\n-\t\t\t\t\t  bb_pending_read_mems[bb_succ])))\n-\t\t  add_insn_mem_dependence (&bb_pending_read_insns[bb_succ],\n-\t\t\t\t\t   &bb_pending_read_mems[bb_succ],\n-\t\t\t\t   XEXP (link_insn, 0), XEXP (link_mem, 0));\n-\t\tlink_insn = XEXP (link_insn, 1);\n-\t\tlink_mem = XEXP (link_mem, 1);\n-\t      }\n-\n-\t    link_insn = pending_write_insns;\n-\t    link_mem = pending_write_mems;\n-\t    while (link_insn)\n-\t      {\n-\t\tif (!(find_insn_mem_list (XEXP (link_insn, 0),\n-\t\t\t\t\t  XEXP (link_mem, 0),\n-\t\t\t\t\t  bb_pending_write_insns[bb_succ],\n-\t\t\t\t\t  bb_pending_write_mems[bb_succ])))\n-\t\t  add_insn_mem_dependence (&bb_pending_write_insns[bb_succ],\n-\t\t\t\t\t   &bb_pending_write_mems[bb_succ],\n-\t\t\t\t   XEXP (link_insn, 0), XEXP (link_mem, 0));\n-\n-\t\tlink_insn = XEXP (link_insn, 1);\n-\t\tlink_mem = XEXP (link_mem, 1);\n-\t      }\n-\n-\t    /* last_function_call is inherited by bb_succ.  */\n-\t    for (u = last_function_call; u; u = XEXP (u, 1))\n-\t      {\n-\t\tif (find_insn_list (XEXP (u, 0),\n-\t\t\t\t    bb_last_function_call[bb_succ]))\n-\t\t  continue;\n-\n-\t\tbb_last_function_call[bb_succ]\n-\t\t  = alloc_INSN_LIST (XEXP (u, 0),\n-\t\t\t\t     bb_last_function_call[bb_succ]);\n-\t      }\n-\n-\t    /* last_pending_memory_flush is inherited by bb_succ.  */\n-\t    for (u = last_pending_memory_flush; u; u = XEXP (u, 1))\n-\t      {\n-\t\tif (find_insn_list (XEXP (u, 0), \n-\t\t\t\t    bb_last_pending_memory_flush[bb_succ]))\n-\t\t  continue;\n-\n-\t\tbb_last_pending_memory_flush[bb_succ]\n-\t\t  = alloc_INSN_LIST (XEXP (u, 0),\n-\t\t\t\t     bb_last_pending_memory_flush[bb_succ]);\n-\t      }\n-\n-\t    /* sched_before_next_call is inherited by bb_succ.  */\n-\t    x = LOG_LINKS (sched_before_next_call);\n-\t    for (; x; x = XEXP (x, 1))\n-\t      add_dependence (bb_sched_before_next_call[bb_succ],\n-\t\t\t      XEXP (x, 0), REG_DEP_ANTI);\n-\n-\t    e = NEXT_OUT (e);\n-\t  }\n-\twhile (e != first_edge);\n-    }\n+    propagate_deps (bb, &tmp_deps, max_reg);\n \n   /* Free up the INSN_LISTs.\n \n@@ -6515,29 +6471,23 @@ compute_block_backward_dependences (bb)\n      The list was empty for the vast majority of those calls.  On the PA, not \n      calling free_INSN_LIST_list in those cases improves -O2 compile times by\n      3-5% on average.  */\n-  for (b = 0; b < max_reg; ++b)\n+  for (i = 0; i < max_reg; ++i)\n     {\n-      if (reg_last_clobbers[b])\n-\tfree_INSN_LIST_list (&reg_last_clobbers[b]);\n-      if (reg_last_sets[b])\n-\tfree_INSN_LIST_list (&reg_last_sets[b]);\n-      if (reg_last_uses[b])\n-\tfree_INSN_LIST_list (&reg_last_uses[b]);\n+      if (tmp_deps.reg_last_clobbers[i])\n+\tfree_INSN_LIST_list (&tmp_deps.reg_last_clobbers[i]);\n+      if (tmp_deps.reg_last_sets[i])\n+\tfree_INSN_LIST_list (&tmp_deps.reg_last_sets[i]);\n+      if (tmp_deps.reg_last_uses[i])\n+\tfree_INSN_LIST_list (&tmp_deps.reg_last_uses[i]);\n     }\n \n   /* Assert that we won't need bb_reg_last_* for this block anymore.  */\n-  if (current_nr_blocks > 1)\n-    {\n-      bb_reg_last_uses[bb] = (rtx *) NULL_RTX;\n-      bb_reg_last_sets[bb] = (rtx *) NULL_RTX;\n-      bb_reg_last_clobbers[bb] = (rtx *) NULL_RTX;\n-    }\n-  else if (current_nr_blocks == 1)\n-    {\n-      free (reg_last_uses);\n-      free (reg_last_sets);\n-      free (reg_last_clobbers);\n-    }\n+  free (bb_deps[bb].reg_last_uses);\n+  free (bb_deps[bb].reg_last_sets);\n+  free (bb_deps[bb].reg_last_clobbers);\n+  bb_deps[bb].reg_last_uses = 0;\n+  bb_deps[bb].reg_last_sets = 0;\n+  bb_deps[bb].reg_last_clobbers = 0;\n }\n \n /* Print dependences for debugging, callable from debugger.  */\n@@ -6649,29 +6599,6 @@ set_priorities (bb)\n   return n_insn;\n }\n \n-/* Make each element of VECTOR point at an rtx-vector,\n-   taking the space for all those rtx-vectors from SPACE.\n-   SPACE is of type (rtx *), but it is really as long as NELTS rtx-vectors.\n-   BYTES_PER_ELT is the number of bytes in one rtx-vector.\n-   (this is the same as init_regset_vector () in flow.c)  */\n-\n-static void\n-init_rtx_vector (vector, space, nelts, bytes_per_elt)\n-     rtx **vector;\n-     rtx *space;\n-     int nelts;\n-     int bytes_per_elt;\n-{\n-  register int i;\n-  register rtx *p = space;\n-\n-  for (i = 0; i < nelts; i++)\n-    {\n-      vector[i] = p;\n-      p += bytes_per_elt / sizeof (*p);\n-    }\n-}\n-\n /* Schedule a region.  A region is either an inner loop, a loop-free\n    subroutine, or a single basic block.  Each bb in the region is\n    scheduled after its flow predecessors.  */\n@@ -6683,9 +6610,6 @@ schedule_region (rgn)\n   int bb;\n   int rgn_n_insns = 0;\n   int sched_rgn_n_insns = 0;\n-  rtx *bb_reg_last_uses_space = NULL;\n-  rtx *bb_reg_last_sets_space = NULL;\n-  rtx *bb_reg_last_clobbers_space = NULL;\n \n   /* Set variables for the current region.  */\n   current_nr_blocks = RGN_NR_BLOCKS (rgn);\n@@ -6696,48 +6620,9 @@ schedule_region (rgn)\n   reg_pending_sets_all = 0;\n \n   /* Initializations for region data dependence analyisis.  */\n-  if (current_nr_blocks > 1)\n-    {\n-      int maxreg = max_reg_num ();\n-\n-      bb_reg_last_uses = (rtx **) xmalloc (current_nr_blocks * sizeof (rtx *));\n-      bb_reg_last_uses_space \n-\t= (rtx *) xcalloc (current_nr_blocks * maxreg, sizeof (rtx));\n-      init_rtx_vector (bb_reg_last_uses, bb_reg_last_uses_space, \n-\t\t       current_nr_blocks, maxreg * sizeof (rtx *));\n-\n-      bb_reg_last_sets = (rtx **) xmalloc (current_nr_blocks * sizeof (rtx *));\n-      bb_reg_last_sets_space \n-\t= (rtx *) xcalloc (current_nr_blocks * maxreg, sizeof (rtx));\n-      init_rtx_vector (bb_reg_last_sets, bb_reg_last_sets_space, \n-\t\t       current_nr_blocks, maxreg * sizeof (rtx *));\n-\n-      bb_reg_last_clobbers =\n-\t(rtx **) xmalloc (current_nr_blocks * sizeof (rtx *));\n-      bb_reg_last_clobbers_space \n-\t= (rtx *) xcalloc (current_nr_blocks * maxreg, sizeof (rtx));\n-      init_rtx_vector (bb_reg_last_clobbers, bb_reg_last_clobbers_space, \n-\t\t       current_nr_blocks, maxreg * sizeof (rtx *));\n-\n-      bb_pending_read_insns \n-\t= (rtx *) xmalloc (current_nr_blocks * sizeof (rtx));\n-      bb_pending_read_mems \n-\t= (rtx *) xmalloc (current_nr_blocks * sizeof (rtx));\n-      bb_pending_write_insns =\n-\t(rtx *) xmalloc (current_nr_blocks * sizeof (rtx));\n-      bb_pending_write_mems \n-\t= (rtx *) xmalloc (current_nr_blocks * sizeof (rtx));\n-      bb_pending_lists_length =\n-\t(int *) xmalloc (current_nr_blocks * sizeof (int));\n-      bb_last_pending_memory_flush =\n-\t(rtx *) xmalloc (current_nr_blocks * sizeof (rtx));\n-      bb_last_function_call \n-\t= (rtx *) xmalloc (current_nr_blocks * sizeof (rtx));\n-      bb_sched_before_next_call =\n-\t(rtx *) xmalloc (current_nr_blocks * sizeof (rtx));\n-\n-      init_rgn_data_dependences (current_nr_blocks);\n-    }\n+  bb_deps = (struct deps *) xmalloc (sizeof (struct deps) * current_nr_blocks);\n+  for (bb = 0; bb < current_nr_blocks; bb++)\n+    init_deps (bb_deps + bb);\n \n   /* Compute LOG_LINKS.  */\n   for (bb = 0; bb < current_nr_blocks; bb++)\n@@ -6823,24 +6708,12 @@ schedule_region (rgn)\n   FREE_REG_SET (reg_pending_sets);\n   FREE_REG_SET (reg_pending_clobbers);\n \n+  free (bb_deps);\n+\n   if (current_nr_blocks > 1)\n     {\n       int i;\n \n-      free (bb_reg_last_uses_space);\n-      free (bb_reg_last_uses);\n-      free (bb_reg_last_sets_space);\n-      free (bb_reg_last_sets);\n-      free (bb_reg_last_clobbers_space);\n-      free (bb_reg_last_clobbers);\n-      free (bb_pending_read_insns);\n-      free (bb_pending_read_mems);\n-      free (bb_pending_write_insns);\n-      free (bb_pending_write_mems);\n-      free (bb_pending_lists_length);\n-      free (bb_last_pending_memory_flush);\n-      free (bb_last_function_call);\n-      free (bb_sched_before_next_call);\n       free (prob);\n       for (i = 0; i < current_nr_blocks; ++i)\n \t{"}]}