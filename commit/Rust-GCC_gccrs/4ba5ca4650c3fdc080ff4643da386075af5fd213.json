{"sha": "4ba5ca4650c3fdc080ff4643da386075af5fd213", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NGJhNWNhNDY1MGMzZmRjMDgwZmY0NjQzZGEzODYwNzVhZjVmZDIxMw==", "commit": {"author": {"name": "Kostya Serebryany", "email": "kcc@google.com", "date": "2012-11-27T14:01:46Z"}, "committer": {"name": "Kostya Serebryany", "email": "kcc@gcc.gnu.org", "date": "2012-11-27T14:01:46Z"}, "message": "[libsanitizer] merge from upstream r168699\n\nFrom-SVN: r193849", "tree": {"sha": "689d358a17fc5081b93d1dae5d2cd4e1ee168c01", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/689d358a17fc5081b93d1dae5d2cd4e1ee168c01"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/4ba5ca4650c3fdc080ff4643da386075af5fd213", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4ba5ca4650c3fdc080ff4643da386075af5fd213", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4ba5ca4650c3fdc080ff4643da386075af5fd213", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4ba5ca4650c3fdc080ff4643da386075af5fd213/comments", "author": {"login": "kcc", "id": 1789297, "node_id": "MDQ6VXNlcjE3ODkyOTc=", "avatar_url": "https://avatars.githubusercontent.com/u/1789297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kcc", "html_url": "https://github.com/kcc", "followers_url": "https://api.github.com/users/kcc/followers", "following_url": "https://api.github.com/users/kcc/following{/other_user}", "gists_url": "https://api.github.com/users/kcc/gists{/gist_id}", "starred_url": "https://api.github.com/users/kcc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kcc/subscriptions", "organizations_url": "https://api.github.com/users/kcc/orgs", "repos_url": "https://api.github.com/users/kcc/repos", "events_url": "https://api.github.com/users/kcc/events{/privacy}", "received_events_url": "https://api.github.com/users/kcc/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "169d8507caebe73a977bcdb9f16cbb6254e7efd8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/169d8507caebe73a977bcdb9f16cbb6254e7efd8", "html_url": "https://github.com/Rust-GCC/gccrs/commit/169d8507caebe73a977bcdb9f16cbb6254e7efd8"}], "stats": {"total": 573, "additions": 415, "deletions": 158}, "files": [{"sha": "a98290da718392b99dd4b8bd6728aea4fac5fd76", "filename": "libsanitizer/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2FChangeLog?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -1,3 +1,7 @@\n+2012-11-27  Kostya Serebryany  <kcc@google.com>\n+\n+\t* All files: Merge from upstream r168699.\n+\n 2012-11-24  Kostya Serebryany kcc@google.com\n \t    Jack Howarth <howarth@bromo.med.uc.edu>\n "}, {"sha": "00ef0478dcd60cf04b650b6e728d9a276587341b", "filename": "libsanitizer/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2FMERGE?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -1,4 +1,4 @@\n-168514\n+168699\n \n The first line of this file holds the svn revision number of the\n last merge done from the master library sources."}, {"sha": "03d5bbd3eb5b66bb4621724673eceea520861d31", "filename": "libsanitizer/asan/asan_allocator.cc", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_allocator.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_allocator.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_allocator.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -130,7 +130,7 @@ static void PoisonHeapPartialRightRedzone(uptr mem, uptr size) {\n }\n \n static u8 *MmapNewPagesAndPoisonShadow(uptr size) {\n-  CHECK(IsAligned(size, kPageSize));\n+  CHECK(IsAligned(size, GetPageSizeCached()));\n   u8 *res = (u8*)MmapOrDie(size, __FUNCTION__);\n   PoisonShadow((uptr)res, size, kAsanHeapLeftRedzoneMagic);\n   if (flags()->debug) {\n@@ -532,12 +532,13 @@ class MallocInfo {\n     uptr mmap_size = Max(size, kMinMmapSize);\n     uptr n_chunks = mmap_size / size;\n     CHECK(n_chunks * size == mmap_size);\n-    if (size < kPageSize) {\n+    uptr PageSize = GetPageSizeCached();\n+    if (size < PageSize) {\n       // Size is small, just poison the last chunk.\n       n_chunks--;\n     } else {\n       // Size is large, allocate an extra page at right and poison it.\n-      mmap_size += kPageSize;\n+      mmap_size += PageSize;\n     }\n     CHECK(n_chunks > 0);\n     u8 *mem = MmapNewPagesAndPoisonShadow(mmap_size);\n@@ -811,18 +812,19 @@ void *asan_realloc(void *p, uptr size, StackTrace *stack) {\n }\n \n void *asan_valloc(uptr size, StackTrace *stack) {\n-  void *ptr = (void*)Allocate(kPageSize, size, stack);\n+  void *ptr = (void*)Allocate(GetPageSizeCached(), size, stack);\n   __asan_malloc_hook(ptr, size);\n   return ptr;\n }\n \n void *asan_pvalloc(uptr size, StackTrace *stack) {\n-  size = RoundUpTo(size, kPageSize);\n+  uptr PageSize = GetPageSizeCached();\n+  size = RoundUpTo(size, PageSize);\n   if (size == 0) {\n     // pvalloc(0) should allocate one page.\n-    size = kPageSize;\n+    size = PageSize;\n   }\n-  void *ptr = (void*)Allocate(kPageSize, size, stack);\n+  void *ptr = (void*)Allocate(PageSize, size, stack);\n   __asan_malloc_hook(ptr, size);\n   return ptr;\n }\n@@ -941,7 +943,7 @@ uptr FakeStack::ClassMmapSize(uptr size_class) {\n }\n \n void FakeStack::AllocateOneSizeClass(uptr size_class) {\n-  CHECK(ClassMmapSize(size_class) >= kPageSize);\n+  CHECK(ClassMmapSize(size_class) >= GetPageSizeCached());\n   uptr new_mem = (uptr)MmapOrDie(\n       ClassMmapSize(size_class), __FUNCTION__);\n   // Printf(\"T%d new_mem[%zu]: %p-%p mmap %zu\\n\","}, {"sha": "2e1fe5100ddc4ff024fcec1368e240464fc5ffc1", "filename": "libsanitizer/asan/asan_linux.cc", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_linux.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_linux.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_linux.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -174,9 +174,10 @@ void ClearShadowMemoryForContext(void *context) {\n   uptr sp = (uptr)ucp->uc_stack.ss_sp;\n   uptr size = ucp->uc_stack.ss_size;\n   // Align to page size.\n-  uptr bottom = sp & ~(kPageSize - 1);\n+  uptr PageSize = GetPageSizeCached();\n+  uptr bottom = sp & ~(PageSize - 1);\n   size += sp - bottom;\n-  size = RoundUpTo(size, kPageSize);\n+  size = RoundUpTo(size, PageSize);\n   PoisonShadow(bottom, size, 0);\n }\n #else"}, {"sha": "5caf76ba1ff87f8a966fbf25caba3ebd60656950", "filename": "libsanitizer/asan/asan_mac.cc", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_mac.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_mac.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_mac.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -182,11 +182,11 @@ void ClearShadowMemoryForContext(void *context) {\n static void *island_allocator_pos = 0;\n \n #if SANITIZER_WORDSIZE == 32\n-# define kIslandEnd (0xffdf0000 - kPageSize)\n-# define kIslandBeg (kIslandEnd - 256 * kPageSize)\n+# define kIslandEnd (0xffdf0000 - GetPageSizeCached())\n+# define kIslandBeg (kIslandEnd - 256 * GetPageSizeCached())\n #else\n-# define kIslandEnd (0x7fffffdf0000 - kPageSize)\n-# define kIslandBeg (kIslandEnd - 256 * kPageSize)\n+# define kIslandEnd (0x7fffffdf0000 - GetPageSizeCached())\n+# define kIslandBeg (kIslandEnd - 256 * GetPageSizeCached())\n #endif\n \n extern \"C\"\n@@ -210,7 +210,7 @@ mach_error_t __interception_allocate_island(void **ptr,\n     internal_memset(island_allocator_pos, 0xCC, kIslandEnd - kIslandBeg);\n   };\n   *ptr = island_allocator_pos;\n-  island_allocator_pos = (char*)island_allocator_pos + kPageSize;\n+  island_allocator_pos = (char*)island_allocator_pos + GetPageSizeCached();\n   if (flags()->verbosity) {\n     Report(\"Branch island allocated at %p\\n\", *ptr);\n   }"}, {"sha": "b56b620a1e3d4833818571516ba0e61fca87d17f", "filename": "libsanitizer/asan/asan_malloc_mac.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_malloc_mac.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_malloc_mac.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_malloc_mac.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -163,7 +163,7 @@ void *mz_valloc(malloc_zone_t *zone, size_t size) {\n     return malloc_zone_valloc(system_malloc_zone, size);\n   }\n   GET_STACK_TRACE_HERE_FOR_MALLOC;\n-  return asan_memalign(kPageSize, size, &stack);\n+  return asan_memalign(GetPageSizeCached(), size, &stack);\n }\n \n #define GET_ZONE_FOR_PTR(ptr) \\"}, {"sha": "85d1129dea8d1cc7843fa85c2c53eca6618166d6", "filename": "libsanitizer/asan/asan_mapping.h", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_mapping.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_mapping.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_mapping.h?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -66,7 +66,12 @@ extern __attribute__((visibility(\"default\"))) uptr __asan_mapping_offset;\n #define kHighShadowBeg  MEM_TO_SHADOW(kHighMemBeg)\n #define kHighShadowEnd  MEM_TO_SHADOW(kHighMemEnd)\n \n-#define kShadowGapBeg   (kLowShadowEnd ? kLowShadowEnd + 1 : 16 * kPageSize)\n+// With the zero shadow base we can not actually map pages starting from 0.\n+// This constant is somewhat arbitrary.\n+#define kZeroBaseShadowStart (1 << 18)\n+\n+#define kShadowGapBeg   (kLowShadowEnd ? kLowShadowEnd + 1 \\\n+                                       : kZeroBaseShadowStart)\n #define kShadowGapEnd   (kHighShadowBeg - 1)\n \n #define kGlobalAndStackRedzone \\"}, {"sha": "3a581cd25f93a4b56dd56d0b5334dc46639808c3", "filename": "libsanitizer/asan/asan_rtl.cc", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_rtl.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_rtl.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_rtl.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -163,8 +163,8 @@ void ShowStatsAndAbort() {\n // ---------------------- mmap -------------------- {{{1\n // Reserve memory range [beg, end].\n static void ReserveShadowMemoryRange(uptr beg, uptr end) {\n-  CHECK((beg % kPageSize) == 0);\n-  CHECK(((end + 1) % kPageSize) == 0);\n+  CHECK((beg % GetPageSizeCached()) == 0);\n+  CHECK(((end + 1) % GetPageSizeCached()) == 0);\n   uptr size = end - beg + 1;\n   void *res = MmapFixedNoReserve(beg, size);\n   if (res != (void*)beg) {\n@@ -269,8 +269,9 @@ void NOINLINE __asan_handle_no_return() {\n   int local_stack;\n   AsanThread *curr_thread = asanThreadRegistry().GetCurrent();\n   CHECK(curr_thread);\n+  uptr PageSize = GetPageSizeCached();\n   uptr top = curr_thread->stack_top();\n-  uptr bottom = ((uptr)&local_stack - kPageSize) & ~(kPageSize-1);\n+  uptr bottom = ((uptr)&local_stack - PageSize) & ~(PageSize-1);\n   PoisonShadow(bottom, top - bottom, 0);\n }\n \n@@ -347,12 +348,13 @@ void __asan_init() {\n   }\n \n   uptr shadow_start = kLowShadowBeg;\n-  if (kLowShadowBeg > 0) shadow_start -= kMmapGranularity;\n+  if (kLowShadowBeg > 0) shadow_start -= GetMmapGranularity();\n   uptr shadow_end = kHighShadowEnd;\n   if (MemoryRangeIsAvailable(shadow_start, shadow_end)) {\n     if (kLowShadowBeg != kLowShadowEnd) {\n       // mmap the low shadow plus at least one page.\n-      ReserveShadowMemoryRange(kLowShadowBeg - kMmapGranularity, kLowShadowEnd);\n+      ReserveShadowMemoryRange(kLowShadowBeg - GetMmapGranularity(),\n+                               kLowShadowEnd);\n     }\n     // mmap the high shadow.\n     ReserveShadowMemoryRange(kHighShadowBeg, kHighShadowEnd);"}, {"sha": "566e0103e4188d26160014aa4220999ddab42b96", "filename": "libsanitizer/asan/asan_stats.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_stats.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_stats.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_stats.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -41,7 +41,7 @@ void AsanStats::Print() {\n   Printf(\"Stats: %zuM really freed by %zu calls\\n\",\n              really_freed>>20, real_frees);\n   Printf(\"Stats: %zuM (%zu full pages) mmaped in %zu calls\\n\",\n-             mmaped>>20, mmaped / kPageSize, mmaps);\n+             mmaped>>20, mmaped / GetPageSizeCached(), mmaps);\n \n   PrintMallocStatsArray(\"  mmaps   by size class: \", mmaped_by_size);\n   PrintMallocStatsArray(\"  mallocs by size class: \", malloced_by_size);"}, {"sha": "cc2e777a977814414e56f1c81df79ad79ab56829", "filename": "libsanitizer/asan/asan_thread.cc", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_thread.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fasan%2Fasan_thread.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_thread.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -26,15 +26,16 @@ AsanThread::AsanThread(LinkerInitialized x)\n \n AsanThread *AsanThread::Create(u32 parent_tid, thread_callback_t start_routine,\n                                void *arg, StackTrace *stack) {\n-  uptr size = RoundUpTo(sizeof(AsanThread), kPageSize);\n+  uptr PageSize = GetPageSizeCached();\n+  uptr size = RoundUpTo(sizeof(AsanThread), PageSize);\n   AsanThread *thread = (AsanThread*)MmapOrDie(size, __FUNCTION__);\n   thread->start_routine_ = start_routine;\n   thread->arg_ = arg;\n \n-  const uptr kSummaryAllocSize = kPageSize;\n+  const uptr kSummaryAllocSize = PageSize;\n   CHECK_LE(sizeof(AsanThreadSummary), kSummaryAllocSize);\n   AsanThreadSummary *summary =\n-      (AsanThreadSummary*)MmapOrDie(kPageSize, \"AsanThreadSummary\");\n+      (AsanThreadSummary*)MmapOrDie(PageSize, \"AsanThreadSummary\");\n   summary->Init(parent_tid, stack);\n   summary->set_thread(thread);\n   thread->set_summary(summary);\n@@ -64,7 +65,7 @@ void AsanThread::Destroy() {\n   // and we don't want it to have any poisoned stack.\n   ClearShadowForThreadStack();\n   fake_stack().Cleanup();\n-  uptr size = RoundUpTo(sizeof(AsanThread), kPageSize);\n+  uptr size = RoundUpTo(sizeof(AsanThread), GetPageSizeCached());\n   UnmapOrDie(this, size);\n }\n "}, {"sha": "88905b7beb1f2f28f0c9ae6933bdef9761e592fc", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -61,7 +61,7 @@ void *LowLevelAllocator::Allocate(uptr size) {\n   // Align allocation size.\n   size = RoundUpTo(size, 8);\n   if (allocated_end_ - allocated_current_ < (sptr)size) {\n-    uptr size_to_allocate = Max(size, kPageSize);\n+    uptr size_to_allocate = Max(size, GetPageSizeCached());\n     allocated_current_ =\n         (char*)MmapOrDie(size_to_allocate, __FUNCTION__);\n     allocated_end_ = allocated_current_ + size_to_allocate;"}, {"sha": "222e3adf212912326564e2b97577546aeb4cab53", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator64.h", "status": "modified", "additions": 13, "deletions": 10, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator64.h?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -215,7 +215,6 @@ class SizeClassAllocator64 {\n   }\n \n   static uptr AllocBeg()  { return kSpaceBeg; }\n-  static uptr AllocEnd()  { return kSpaceBeg  + kSpaceSize + AdditionalSize(); }\n   static uptr AllocSize() { return kSpaceSize + AdditionalSize(); }\n \n   static const uptr kNumClasses = 256;  // Power of two <= 256\n@@ -241,7 +240,7 @@ class SizeClassAllocator64 {\n \n   static uptr AdditionalSize() {\n     uptr res = sizeof(RegionInfo) * kNumClasses;\n-    CHECK_EQ(res % kPageSize, 0);\n+    CHECK_EQ(res % GetPageSizeCached(), 0);\n     return res;\n   }\n \n@@ -364,17 +363,18 @@ class LargeMmapAllocator {\n  public:\n   void Init() {\n     internal_memset(this, 0, sizeof(*this));\n+    page_size_ = GetPageSizeCached();\n   }\n   void *Allocate(uptr size, uptr alignment) {\n     CHECK(IsPowerOfTwo(alignment));\n     uptr map_size = RoundUpMapSize(size);\n-    if (alignment > kPageSize)\n+    if (alignment > page_size_)\n       map_size += alignment;\n     if (map_size < size) return 0;  // Overflow.\n     uptr map_beg = reinterpret_cast<uptr>(\n         MmapOrDie(map_size, \"LargeMmapAllocator\"));\n     uptr map_end = map_beg + map_size;\n-    uptr res = map_beg + kPageSize;\n+    uptr res = map_beg + page_size_;\n     if (res & (alignment - 1))  // Align.\n       res += alignment - (res & (alignment - 1));\n     CHECK_EQ(0, res & (alignment - 1));\n@@ -421,7 +421,7 @@ class LargeMmapAllocator {\n \n   bool PointerIsMine(void *p) {\n     // Fast check.\n-    if ((reinterpret_cast<uptr>(p) % kPageSize) != 0) return false;\n+    if ((reinterpret_cast<uptr>(p) & (page_size_ - 1))) return false;\n     SpinMutexLock l(&mutex_);\n     for (Header *l = list_; l; l = l->next) {\n       if (GetUser(l) == p) return true;\n@@ -430,10 +430,10 @@ class LargeMmapAllocator {\n   }\n \n   uptr GetActuallyAllocatedSize(void *p) {\n-    return RoundUpMapSize(GetHeader(p)->size) - kPageSize;\n+    return RoundUpMapSize(GetHeader(p)->size) - page_size_;\n   }\n \n-  // At least kPageSize/2 metadata bytes is available.\n+  // At least page_size_/2 metadata bytes is available.\n   void *GetMetaData(void *p) {\n     return GetHeader(p) + 1;\n   }\n@@ -457,17 +457,20 @@ class LargeMmapAllocator {\n     Header *prev;\n   };\n \n-  Header *GetHeader(uptr p) { return reinterpret_cast<Header*>(p - kPageSize); }\n+  Header *GetHeader(uptr p) {\n+    return reinterpret_cast<Header*>(p - page_size_);\n+  }\n   Header *GetHeader(void *p) { return GetHeader(reinterpret_cast<uptr>(p)); }\n \n   void *GetUser(Header *h) {\n-    return reinterpret_cast<void*>(reinterpret_cast<uptr>(h) + kPageSize);\n+    return reinterpret_cast<void*>(reinterpret_cast<uptr>(h) + page_size_);\n   }\n \n   uptr RoundUpMapSize(uptr size) {\n-    return RoundUpTo(size, kPageSize) + kPageSize;\n+    return RoundUpTo(size, page_size_) + page_size_;\n   }\n \n+  uptr page_size_;\n   Header *list_;\n   SpinMutex mutex_;\n };"}, {"sha": "76a55c0f8b42b8d5b15956729b1c19a96481fe00", "filename": "libsanitizer/sanitizer_common/sanitizer_common.cc", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -14,6 +14,13 @@\n \n namespace __sanitizer {\n \n+uptr GetPageSizeCached() {\n+  static uptr PageSize;\n+  if (!PageSize)\n+    PageSize = GetPageSize();\n+  return PageSize;\n+}\n+\n // By default, dump to stderr. If report_fd is kInvalidFd, try to obtain file\n // descriptor by opening file in report_path.\n static fd_t report_fd = kStderrFd;\n@@ -75,7 +82,8 @@ void RawWrite(const char *buffer) {\n \n uptr ReadFileToBuffer(const char *file_name, char **buff,\n                       uptr *buff_size, uptr max_len) {\n-  const uptr kMinFileLen = kPageSize;\n+  uptr PageSize = GetPageSizeCached();\n+  uptr kMinFileLen = PageSize;\n   uptr read_len = 0;\n   *buff = 0;\n   *buff_size = 0;\n@@ -89,8 +97,8 @@ uptr ReadFileToBuffer(const char *file_name, char **buff,\n     // Read up to one page at a time.\n     read_len = 0;\n     bool reached_eof = false;\n-    while (read_len + kPageSize <= size) {\n-      uptr just_read = internal_read(fd, *buff + read_len, kPageSize);\n+    while (read_len + PageSize <= size) {\n+      uptr just_read = internal_read(fd, *buff + read_len, PageSize);\n       if (just_read == 0) {\n         reached_eof = true;\n         break;"}, {"sha": "18b1e1a65a4f5f774a8a0aef4e8d4d0ee67050a3", "filename": "libsanitizer/sanitizer_common/sanitizer_common.h", "status": "modified", "additions": 4, "deletions": 13, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.h?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -21,25 +21,16 @@ namespace __sanitizer {\n // Constants.\n const uptr kWordSize = SANITIZER_WORDSIZE / 8;\n const uptr kWordSizeInBits = 8 * kWordSize;\n+\n #if defined(__powerpc__) || defined(__powerpc64__)\n-// Current PPC64 kernels use 64K pages sizes, but they can be\n-// configured with 4K or even other sizes.\n-// We may want to use getpagesize() or sysconf(_SC_PAGESIZE) here rather than\n-// hardcoding the values, but today these values need to be compile-time\n-// constants.\n-const uptr kPageSize = 1UL << 16;\n const uptr kCacheLineSize = 128;\n-const uptr kMmapGranularity = kPageSize;\n-#elif !defined(_WIN32)\n-const uptr kPageSize = 1UL << 12;\n-const uptr kCacheLineSize = 64;\n-const uptr kMmapGranularity = kPageSize;\n #else\n-const uptr kPageSize = 1UL << 12;\n const uptr kCacheLineSize = 64;\n-const uptr kMmapGranularity = 1UL << 16;\n #endif\n \n+uptr GetPageSize();\n+uptr GetPageSizeCached();\n+uptr GetMmapGranularity();\n // Threads\n int GetPid();\n uptr GetTid();"}, {"sha": "75d114751750c3fd16851c49f773898279e604c6", "filename": "libsanitizer/sanitizer_common/sanitizer_posix.cc", "status": "modified", "additions": 12, "deletions": 4, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_posix.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_posix.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_posix.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -30,6 +30,13 @@\n namespace __sanitizer {\n \n // ------------- sanitizer_common.h\n+uptr GetPageSize() {\n+  return sysconf(_SC_PAGESIZE);\n+}\n+\n+uptr GetMmapGranularity() {\n+  return GetPageSize();\n+}\n \n int GetPid() {\n   return getpid();\n@@ -40,7 +47,7 @@ uptr GetThreadSelf() {\n }\n \n void *MmapOrDie(uptr size, const char *mem_type) {\n-  size = RoundUpTo(size, kPageSize);\n+  size = RoundUpTo(size, GetPageSizeCached());\n   void *res = internal_mmap(0, size,\n                             PROT_READ | PROT_WRITE,\n                             MAP_PRIVATE | MAP_ANON, -1, 0);\n@@ -72,8 +79,9 @@ void UnmapOrDie(void *addr, uptr size) {\n }\n \n void *MmapFixedNoReserve(uptr fixed_addr, uptr size) {\n-  void *p = internal_mmap((void*)(fixed_addr & ~(kPageSize - 1)),\n-      RoundUpTo(size, kPageSize),\n+  uptr PageSize = GetPageSizeCached();\n+  void *p = internal_mmap((void*)(fixed_addr & ~(PageSize - 1)),\n+      RoundUpTo(size, PageSize),\n       PROT_READ | PROT_WRITE,\n       MAP_PRIVATE | MAP_ANON | MAP_FIXED | MAP_NORESERVE,\n       -1, 0);\n@@ -96,7 +104,7 @@ void *MapFileToMemory(const char *file_name, uptr *buff_size) {\n   uptr fsize = internal_filesize(fd);\n   CHECK_NE(fsize, (uptr)-1);\n   CHECK_GT(fsize, 0);\n-  *buff_size = RoundUpTo(fsize, kPageSize);\n+  *buff_size = RoundUpTo(fsize, GetPageSizeCached());\n   void *map = internal_mmap(0, *buff_size, PROT_READ, MAP_PRIVATE, fd, 0);\n   return (map == MAP_FAILED) ? 0 : map;\n }"}, {"sha": "368d05d8f4aa23ac63ce35a0a4f05c0f39d23918", "filename": "libsanitizer/sanitizer_common/sanitizer_stacktrace.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -63,7 +63,7 @@ void StackTrace::PrintStack(const uptr *addr, uptr size,\n                             bool symbolize, const char *strip_file_prefix,\n                             SymbolizeCallback symbolize_callback ) {\n   MemoryMappingLayout proc_maps;\n-  InternalScopedBuffer<char> buff(kPageSize * 2);\n+  InternalScopedBuffer<char> buff(GetPageSizeCached() * 2);\n   InternalScopedBuffer<AddressInfo> addr_frames(64);\n   uptr frame_num = 0;\n   for (uptr i = 0; i < size && addr[i]; i++) {"}, {"sha": "15ef7d96826dbd2ecdacff1df6260691b45876aa", "filename": "libsanitizer/sanitizer_common/sanitizer_win.cc", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_win.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Fsanitizer_common%2Fsanitizer_win.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_win.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -21,6 +21,14 @@\n namespace __sanitizer {\n \n // --------------------- sanitizer_common.h\n+uptr GetPageSize() {\n+  return 1U << 14;  // FIXME: is this configurable?\n+}\n+\n+uptr GetMmapGranularity() {\n+  return 1U << 16;  // FIXME: is this configurable?\n+}\n+\n bool FileExists(const char *filename) {\n   UNIMPLEMENTED();\n }"}, {"sha": "7ceb153c42bf0a16d81be72954ec03cd4b9f1656", "filename": "libsanitizer/tsan/tsan_interceptors.cc", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_interceptors.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_interceptors.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interceptors.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -564,13 +564,13 @@ TSAN_INTERCEPTOR(void*, memalign, uptr align, uptr sz) {\n \n TSAN_INTERCEPTOR(void*, valloc, uptr sz) {\n   SCOPED_TSAN_INTERCEPTOR(valloc, sz);\n-  return user_alloc(thr, pc, sz, kPageSize);\n+  return user_alloc(thr, pc, sz, GetPageSizeCached());\n }\n \n TSAN_INTERCEPTOR(void*, pvalloc, uptr sz) {\n   SCOPED_TSAN_INTERCEPTOR(pvalloc, sz);\n-  sz = RoundUp(sz, kPageSize);\n-  return user_alloc(thr, pc, sz, kPageSize);\n+  sz = RoundUp(sz, GetPageSizeCached());\n+  return user_alloc(thr, pc, sz, GetPageSizeCached());\n }\n \n TSAN_INTERCEPTOR(int, posix_memalign, void **memptr, uptr align, uptr sz) {"}, {"sha": "e3c89714d092b6d784d6c89813e53b50fdbbb44e", "filename": "libsanitizer/tsan/tsan_interface.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_interface.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_interface.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interface.h?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -42,6 +42,9 @@ void __tsan_vptr_update(void **vptr_p, void *new_val);\n void __tsan_func_entry(void *call_pc);\n void __tsan_func_exit();\n \n+void __tsan_read_range(void *addr, unsigned long size);  // NOLINT\n+void __tsan_write_range(void *addr, unsigned long size);  // NOLINT\n+\n #ifdef __cplusplus\n }  // extern \"C\"\n #endif"}, {"sha": "7193e7f52f4945e175b5ea9a51e8c2c067939d30", "filename": "libsanitizer/tsan/tsan_interface_atomic.cc", "status": "modified", "additions": 229, "deletions": 78, "changes": 307, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_interface_atomic.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_interface_atomic.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interface_atomic.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -9,6 +9,14 @@\n //\n //===----------------------------------------------------------------------===//\n \n+// ThreadSanitizer atomic operations are based on C++11/C1x standards.\n+// For background see C++11 standard.  A slightly older, publically\n+// available draft of the standard (not entirely up-to-date, but close enough\n+// for casual browsing) is available here:\n+// http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2011/n3242.pdf\n+// The following page contains more background information:\n+// http://www.hpl.hp.com/personal/Hans_Boehm/c++mm/\n+\n #include \"sanitizer_common/sanitizer_placement_new.h\"\n #include \"tsan_interface_atomic.h\"\n #include \"tsan_flags.h\"\n@@ -37,6 +45,7 @@ typedef __tsan_atomic8 a8;\n typedef __tsan_atomic16 a16;\n typedef __tsan_atomic32 a32;\n typedef __tsan_atomic64 a64;\n+typedef __tsan_atomic128 a128;\n const morder mo_relaxed = __tsan_memory_order_relaxed;\n const morder mo_consume = __tsan_memory_order_consume;\n const morder mo_acquire = __tsan_memory_order_acquire;\n@@ -50,7 +59,8 @@ static void AtomicStatInc(ThreadState *thr, uptr size, morder mo, StatType t) {\n   StatInc(thr, size == 1 ? StatAtomic1\n              : size == 2 ? StatAtomic2\n              : size == 4 ? StatAtomic4\n-             :             StatAtomic8);\n+             : size == 8 ? StatAtomic8\n+             :             StatAtomic16);\n   StatInc(thr, mo == mo_relaxed ? StatAtomicRelaxed\n              : mo == mo_consume ? StatAtomicConsume\n              : mo == mo_acquire ? StatAtomicAcquire\n@@ -77,6 +87,10 @@ static bool IsAcquireOrder(morder mo) {\n       || mo == mo_acq_rel || mo == mo_seq_cst;\n }\n \n+static bool IsAcqRelOrder(morder mo) {\n+  return mo == mo_acq_rel || mo == mo_seq_cst;\n+}\n+\n static morder ConvertOrder(morder mo) {\n   if (mo > (morder)100500) {\n     mo = morder(mo - 100500);\n@@ -98,6 +112,34 @@ static morder ConvertOrder(morder mo) {\n   return mo;\n }\n \n+template<typename T> T func_xchg(T v, T op) {\n+  return op;\n+}\n+\n+template<typename T> T func_add(T v, T op) {\n+  return v + op;\n+}\n+\n+template<typename T> T func_sub(T v, T op) {\n+  return v - op;\n+}\n+\n+template<typename T> T func_and(T v, T op) {\n+  return v & op;\n+}\n+\n+template<typename T> T func_or(T v, T op) {\n+  return v | op;\n+}\n+\n+template<typename T> T func_xor(T v, T op) {\n+  return v ^ op;\n+}\n+\n+template<typename T> T func_nand(T v, T op) {\n+  return ~v & op;\n+}\n+\n #define SCOPED_ATOMIC(func, ...) \\\n     mo = ConvertOrder(mo); \\\n     mo = flags()->force_seq_cst_atomics ? (morder)mo_seq_cst : mo; \\\n@@ -113,110 +155,128 @@ template<typename T>\n static T AtomicLoad(ThreadState *thr, uptr pc, const volatile T *a,\n     morder mo) {\n   CHECK(IsLoadOrder(mo));\n+  // This fast-path is critical for performance.\n+  // Assume the access is atomic.\n+  if (!IsAcquireOrder(mo) && sizeof(T) <= sizeof(a))\n+    return *a;\n+  SyncVar *s = CTX()->synctab.GetAndLock(thr, pc, (uptr)a, false);\n+  thr->clock.set(thr->tid, thr->fast_state.epoch());\n+  thr->clock.acquire(&s->clock);\n   T v = *a;\n-  if (IsAcquireOrder(mo))\n-    Acquire(thr, pc, (uptr)a);\n+  s->mtx.ReadUnlock();\n   return v;\n }\n \n template<typename T>\n static void AtomicStore(ThreadState *thr, uptr pc, volatile T *a, T v,\n     morder mo) {\n   CHECK(IsStoreOrder(mo));\n-  if (IsReleaseOrder(mo))\n-    ReleaseStore(thr, pc, (uptr)a);\n+  // This fast-path is critical for performance.\n+  // Assume the access is atomic.\n+  // Strictly saying even relaxed store cuts off release sequence,\n+  // so must reset the clock.\n+  if (!IsReleaseOrder(mo) && sizeof(T) <= sizeof(a)) {\n+    *a = v;\n+    return;\n+  }\n+  SyncVar *s = CTX()->synctab.GetAndLock(thr, pc, (uptr)a, true);\n+  thr->clock.set(thr->tid, thr->fast_state.epoch());\n+  thr->clock.ReleaseStore(&s->clock);\n   *a = v;\n+  s->mtx.Unlock();\n+}\n+\n+template<typename T, T (*F)(T v, T op)>\n+static T AtomicRMW(ThreadState *thr, uptr pc, volatile T *a, T v, morder mo) {\n+  SyncVar *s = CTX()->synctab.GetAndLock(thr, pc, (uptr)a, true);\n+  thr->clock.set(thr->tid, thr->fast_state.epoch());\n+  if (IsAcqRelOrder(mo))\n+    thr->clock.acq_rel(&s->clock);\n+  else if (IsReleaseOrder(mo))\n+    thr->clock.release(&s->clock);\n+  else if (IsAcquireOrder(mo))\n+    thr->clock.acquire(&s->clock);\n+  T c = *a;\n+  *a = F(c, v);\n+  s->mtx.Unlock();\n+  return c;\n }\n \n template<typename T>\n static T AtomicExchange(ThreadState *thr, uptr pc, volatile T *a, T v,\n     morder mo) {\n-  if (IsReleaseOrder(mo))\n-    Release(thr, pc, (uptr)a);\n-  v = __sync_lock_test_and_set(a, v);\n-  if (IsAcquireOrder(mo))\n-    Acquire(thr, pc, (uptr)a);\n-  return v;\n+  return AtomicRMW<T, func_xchg>(thr, pc, a, v, mo);\n }\n \n template<typename T>\n static T AtomicFetchAdd(ThreadState *thr, uptr pc, volatile T *a, T v,\n     morder mo) {\n-  if (IsReleaseOrder(mo))\n-    Release(thr, pc, (uptr)a);\n-  v = __sync_fetch_and_add(a, v);\n-  if (IsAcquireOrder(mo))\n-    Acquire(thr, pc, (uptr)a);\n-  return v;\n+  return AtomicRMW<T, func_add>(thr, pc, a, v, mo);\n }\n \n template<typename T>\n static T AtomicFetchSub(ThreadState *thr, uptr pc, volatile T *a, T v,\n     morder mo) {\n-  if (IsReleaseOrder(mo))\n-    Release(thr, pc, (uptr)a);\n-  v = __sync_fetch_and_sub(a, v);\n-  if (IsAcquireOrder(mo))\n-    Acquire(thr, pc, (uptr)a);\n-  return v;\n+  return AtomicRMW<T, func_sub>(thr, pc, a, v, mo);\n }\n \n template<typename T>\n static T AtomicFetchAnd(ThreadState *thr, uptr pc, volatile T *a, T v,\n     morder mo) {\n-  if (IsReleaseOrder(mo))\n-    Release(thr, pc, (uptr)a);\n-  v = __sync_fetch_and_and(a, v);\n-  if (IsAcquireOrder(mo))\n-    Acquire(thr, pc, (uptr)a);\n-  return v;\n+  return AtomicRMW<T, func_and>(thr, pc, a, v, mo);\n }\n \n template<typename T>\n static T AtomicFetchOr(ThreadState *thr, uptr pc, volatile T *a, T v,\n     morder mo) {\n-  if (IsReleaseOrder(mo))\n-    Release(thr, pc, (uptr)a);\n-  v = __sync_fetch_and_or(a, v);\n-  if (IsAcquireOrder(mo))\n-    Acquire(thr, pc, (uptr)a);\n-  return v;\n+  return AtomicRMW<T, func_or>(thr, pc, a, v, mo);\n }\n \n template<typename T>\n static T AtomicFetchXor(ThreadState *thr, uptr pc, volatile T *a, T v,\n     morder mo) {\n-  if (IsReleaseOrder(mo))\n-    Release(thr, pc, (uptr)a);\n-  v = __sync_fetch_and_xor(a, v);\n-  if (IsAcquireOrder(mo))\n-    Acquire(thr, pc, (uptr)a);\n-  return v;\n+  return AtomicRMW<T, func_xor>(thr, pc, a, v, mo);\n+}\n+\n+template<typename T>\n+static T AtomicFetchNand(ThreadState *thr, uptr pc, volatile T *a, T v,\n+    morder mo) {\n+  return AtomicRMW<T, func_nand>(thr, pc, a, v, mo);\n }\n \n template<typename T>\n static bool AtomicCAS(ThreadState *thr, uptr pc,\n-    volatile T *a, T *c, T v, morder mo) {\n-  if (IsReleaseOrder(mo))\n-    Release(thr, pc, (uptr)a);\n-  T cc = *c;\n-  T pr = __sync_val_compare_and_swap(a, cc, v);\n-  if (IsAcquireOrder(mo))\n-    Acquire(thr, pc, (uptr)a);\n-  if (pr == cc)\n-    return true;\n-  *c = pr;\n-  return false;\n+    volatile T *a, T *c, T v, morder mo, morder fmo) {\n+  (void)fmo;  // Unused because llvm does not pass it yet.\n+  SyncVar *s = CTX()->synctab.GetAndLock(thr, pc, (uptr)a, true);\n+  thr->clock.set(thr->tid, thr->fast_state.epoch());\n+  if (IsAcqRelOrder(mo))\n+    thr->clock.acq_rel(&s->clock);\n+  else if (IsReleaseOrder(mo))\n+    thr->clock.release(&s->clock);\n+  else if (IsAcquireOrder(mo))\n+    thr->clock.acquire(&s->clock);\n+  T cur = *a;\n+  bool res = false;\n+  if (cur == *c) {\n+    *a = v;\n+    res = true;\n+  } else {\n+    *c = cur;\n+  }\n+  s->mtx.Unlock();\n+  return res;\n }\n \n template<typename T>\n static T AtomicCAS(ThreadState *thr, uptr pc,\n-    volatile T *a, T c, T v, morder mo) {\n-  AtomicCAS(thr, pc, a, &c, v, mo);\n+    volatile T *a, T c, T v, morder mo, morder fmo) {\n+  AtomicCAS(thr, pc, a, &c, v, mo, fmo);\n   return c;\n }\n \n static void AtomicFence(ThreadState *thr, uptr pc, morder mo) {\n+  // FIXME(dvyukov): not implemented.\n   __sync_synchronize();\n }\n \n@@ -236,6 +296,12 @@ a64 __tsan_atomic64_load(const volatile a64 *a, morder mo) {\n   SCOPED_ATOMIC(Load, a, mo);\n }\n \n+#if __TSAN_HAS_INT128\n+a128 __tsan_atomic128_load(const volatile a128 *a, morder mo) {\n+  SCOPED_ATOMIC(Load, a, mo);\n+}\n+#endif\n+\n void __tsan_atomic8_store(volatile a8 *a, a8 v, morder mo) {\n   SCOPED_ATOMIC(Store, a, v, mo);\n }\n@@ -252,6 +318,12 @@ void __tsan_atomic64_store(volatile a64 *a, a64 v, morder mo) {\n   SCOPED_ATOMIC(Store, a, v, mo);\n }\n \n+#if __TSAN_HAS_INT128\n+void __tsan_atomic128_store(volatile a128 *a, a128 v, morder mo) {\n+  SCOPED_ATOMIC(Store, a, v, mo);\n+}\n+#endif\n+\n a8 __tsan_atomic8_exchange(volatile a8 *a, a8 v, morder mo) {\n   SCOPED_ATOMIC(Exchange, a, v, mo);\n }\n@@ -268,6 +340,12 @@ a64 __tsan_atomic64_exchange(volatile a64 *a, a64 v, morder mo) {\n   SCOPED_ATOMIC(Exchange, a, v, mo);\n }\n \n+#if __TSAN_HAS_INT128\n+a128 __tsan_atomic128_exchange(volatile a128 *a, a128 v, morder mo) {\n+  SCOPED_ATOMIC(Exchange, a, v, mo);\n+}\n+#endif\n+\n a8 __tsan_atomic8_fetch_add(volatile a8 *a, a8 v, morder mo) {\n   SCOPED_ATOMIC(FetchAdd, a, v, mo);\n }\n@@ -284,6 +362,12 @@ a64 __tsan_atomic64_fetch_add(volatile a64 *a, a64 v, morder mo) {\n   SCOPED_ATOMIC(FetchAdd, a, v, mo);\n }\n \n+#if __TSAN_HAS_INT128\n+a128 __tsan_atomic128_fetch_add(volatile a128 *a, a128 v, morder mo) {\n+  SCOPED_ATOMIC(FetchAdd, a, v, mo);\n+}\n+#endif\n+\n a8 __tsan_atomic8_fetch_sub(volatile a8 *a, a8 v, morder mo) {\n   SCOPED_ATOMIC(FetchSub, a, v, mo);\n }\n@@ -300,6 +384,12 @@ a64 __tsan_atomic64_fetch_sub(volatile a64 *a, a64 v, morder mo) {\n   SCOPED_ATOMIC(FetchSub, a, v, mo);\n }\n \n+#if __TSAN_HAS_INT128\n+a128 __tsan_atomic128_fetch_sub(volatile a128 *a, a128 v, morder mo) {\n+  SCOPED_ATOMIC(FetchSub, a, v, mo);\n+}\n+#endif\n+\n a8 __tsan_atomic8_fetch_and(volatile a8 *a, a8 v, morder mo) {\n   SCOPED_ATOMIC(FetchAnd, a, v, mo);\n }\n@@ -316,6 +406,12 @@ a64 __tsan_atomic64_fetch_and(volatile a64 *a, a64 v, morder mo) {\n   SCOPED_ATOMIC(FetchAnd, a, v, mo);\n }\n \n+#if __TSAN_HAS_INT128\n+a128 __tsan_atomic128_fetch_and(volatile a128 *a, a128 v, morder mo) {\n+  SCOPED_ATOMIC(FetchAnd, a, v, mo);\n+}\n+#endif\n+\n a8 __tsan_atomic8_fetch_or(volatile a8 *a, a8 v, morder mo) {\n   SCOPED_ATOMIC(FetchOr, a, v, mo);\n }\n@@ -332,6 +428,12 @@ a64 __tsan_atomic64_fetch_or(volatile a64 *a, a64 v, morder mo) {\n   SCOPED_ATOMIC(FetchOr, a, v, mo);\n }\n \n+#if __TSAN_HAS_INT128\n+a128 __tsan_atomic128_fetch_or(volatile a128 *a, a128 v, morder mo) {\n+  SCOPED_ATOMIC(FetchOr, a, v, mo);\n+}\n+#endif\n+\n a8 __tsan_atomic8_fetch_xor(volatile a8 *a, a8 v, morder mo) {\n   SCOPED_ATOMIC(FetchXor, a, v, mo);\n }\n@@ -348,64 +450,113 @@ a64 __tsan_atomic64_fetch_xor(volatile a64 *a, a64 v, morder mo) {\n   SCOPED_ATOMIC(FetchXor, a, v, mo);\n }\n \n+#if __TSAN_HAS_INT128\n+a128 __tsan_atomic128_fetch_xor(volatile a128 *a, a128 v, morder mo) {\n+  SCOPED_ATOMIC(FetchXor, a, v, mo);\n+}\n+#endif\n+\n+a8 __tsan_atomic8_fetch_nand(volatile a8 *a, a8 v, morder mo) {\n+  SCOPED_ATOMIC(FetchNand, a, v, mo);\n+}\n+\n+a16 __tsan_atomic16_fetch_nand(volatile a16 *a, a16 v, morder mo) {\n+  SCOPED_ATOMIC(FetchNand, a, v, mo);\n+}\n+\n+a32 __tsan_atomic32_fetch_nand(volatile a32 *a, a32 v, morder mo) {\n+  SCOPED_ATOMIC(FetchNand, a, v, mo);\n+}\n+\n+a64 __tsan_atomic64_fetch_nand(volatile a64 *a, a64 v, morder mo) {\n+  SCOPED_ATOMIC(FetchNand, a, v, mo);\n+}\n+\n+#if __TSAN_HAS_INT128\n+a128 __tsan_atomic128_fetch_nand(volatile a128 *a, a128 v, morder mo) {\n+  SCOPED_ATOMIC(FetchNand, a, v, mo);\n+}\n+#endif\n+\n int __tsan_atomic8_compare_exchange_strong(volatile a8 *a, a8 *c, a8 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n \n int __tsan_atomic16_compare_exchange_strong(volatile a16 *a, a16 *c, a16 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n \n int __tsan_atomic32_compare_exchange_strong(volatile a32 *a, a32 *c, a32 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n \n int __tsan_atomic64_compare_exchange_strong(volatile a64 *a, a64 *c, a64 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n+}\n+\n+#if __TSAN_HAS_INT128\n+int __tsan_atomic128_compare_exchange_strong(volatile a128 *a, a128 *c, a128 v,\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n+#endif\n \n int __tsan_atomic8_compare_exchange_weak(volatile a8 *a, a8 *c, a8 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n \n int __tsan_atomic16_compare_exchange_weak(volatile a16 *a, a16 *c, a16 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n \n int __tsan_atomic32_compare_exchange_weak(volatile a32 *a, a32 *c, a32 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n \n int __tsan_atomic64_compare_exchange_weak(volatile a64 *a, a64 *c, a64 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n \n+#if __TSAN_HAS_INT128\n+int __tsan_atomic128_compare_exchange_weak(volatile a128 *a, a128 *c, a128 v,\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n+}\n+#endif\n+\n a8 __tsan_atomic8_compare_exchange_val(volatile a8 *a, a8 c, a8 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n a16 __tsan_atomic16_compare_exchange_val(volatile a16 *a, a16 c, a16 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n \n a32 __tsan_atomic32_compare_exchange_val(volatile a32 *a, a32 c, a32 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n \n a64 __tsan_atomic64_compare_exchange_val(volatile a64 *a, a64 c, a64 v,\n-    morder mo) {\n-  SCOPED_ATOMIC(CAS, a, c, v, mo);\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n+}\n+\n+#if __TSAN_HAS_INT128\n+a128 __tsan_atomic64_compare_exchange_val(volatile a128 *a, a128 c, a128 v,\n+    morder mo, morder fmo) {\n+  SCOPED_ATOMIC(CAS, a, c, v, mo, fmo);\n }\n+#endif\n \n void __tsan_atomic_thread_fence(morder mo) {\n   char* a;"}, {"sha": "15a0cc6594f8927268ac1de02ccef80d6085194e", "filename": "libsanitizer/tsan/tsan_interface_atomic.h", "status": "modified", "additions": 69, "deletions": 16, "changes": 85, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_interface_atomic.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_interface_atomic.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interface_atomic.h?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -15,10 +15,19 @@\n extern \"C\" {\n #endif\n \n-typedef char  __tsan_atomic8;\n-typedef short __tsan_atomic16;  // NOLINT\n-typedef int   __tsan_atomic32;\n-typedef long  __tsan_atomic64;  // NOLINT\n+typedef char     __tsan_atomic8;\n+typedef short    __tsan_atomic16;  // NOLINT\n+typedef int      __tsan_atomic32;\n+typedef long     __tsan_atomic64;  // NOLINT\n+\n+#if defined(__SIZEOF_INT128__) \\\n+    || (__clang_major__ * 100 + __clang_minor__ >= 302)\n+typedef __int128 __tsan_atomic128;\n+#define __TSAN_HAS_INT128 1\n+#else\n+typedef char     __tsan_atomic128;\n+#define __TSAN_HAS_INT128 0\n+#endif\n \n // Part of ABI, do not change.\n // http://llvm.org/viewvc/llvm-project/libcxx/trunk/include/atomic?view=markup\n@@ -39,6 +48,8 @@ __tsan_atomic32 __tsan_atomic32_load(const volatile __tsan_atomic32 *a,\n     __tsan_memory_order mo);\n __tsan_atomic64 __tsan_atomic64_load(const volatile __tsan_atomic64 *a,\n     __tsan_memory_order mo);\n+__tsan_atomic128 __tsan_atomic128_load(const volatile __tsan_atomic128 *a,\n+    __tsan_memory_order mo);\n \n void __tsan_atomic8_store(volatile __tsan_atomic8 *a, __tsan_atomic8 v,\n     __tsan_memory_order mo);\n@@ -48,6 +59,8 @@ void __tsan_atomic32_store(volatile __tsan_atomic32 *a, __tsan_atomic32 v,\n     __tsan_memory_order mo);\n void __tsan_atomic64_store(volatile __tsan_atomic64 *a, __tsan_atomic64 v,\n     __tsan_memory_order mo);\n+void __tsan_atomic128_store(volatile __tsan_atomic128 *a, __tsan_atomic128 v,\n+    __tsan_memory_order mo);\n \n __tsan_atomic8 __tsan_atomic8_exchange(volatile __tsan_atomic8 *a,\n     __tsan_atomic8 v, __tsan_memory_order mo);\n@@ -57,6 +70,8 @@ __tsan_atomic32 __tsan_atomic32_exchange(volatile __tsan_atomic32 *a,\n     __tsan_atomic32 v, __tsan_memory_order mo);\n __tsan_atomic64 __tsan_atomic64_exchange(volatile __tsan_atomic64 *a,\n     __tsan_atomic64 v, __tsan_memory_order mo);\n+__tsan_atomic128 __tsan_atomic128_exchange(volatile __tsan_atomic128 *a,\n+    __tsan_atomic128 v, __tsan_memory_order mo);\n \n __tsan_atomic8 __tsan_atomic8_fetch_add(volatile __tsan_atomic8 *a,\n     __tsan_atomic8 v, __tsan_memory_order mo);\n@@ -66,6 +81,8 @@ __tsan_atomic32 __tsan_atomic32_fetch_add(volatile __tsan_atomic32 *a,\n     __tsan_atomic32 v, __tsan_memory_order mo);\n __tsan_atomic64 __tsan_atomic64_fetch_add(volatile __tsan_atomic64 *a,\n     __tsan_atomic64 v, __tsan_memory_order mo);\n+__tsan_atomic128 __tsan_atomic128_fetch_add(volatile __tsan_atomic128 *a,\n+    __tsan_atomic128 v, __tsan_memory_order mo);\n \n __tsan_atomic8 __tsan_atomic8_fetch_sub(volatile __tsan_atomic8 *a,\n     __tsan_atomic8 v, __tsan_memory_order mo);\n@@ -75,6 +92,8 @@ __tsan_atomic32 __tsan_atomic32_fetch_sub(volatile __tsan_atomic32 *a,\n     __tsan_atomic32 v, __tsan_memory_order mo);\n __tsan_atomic64 __tsan_atomic64_fetch_sub(volatile __tsan_atomic64 *a,\n     __tsan_atomic64 v, __tsan_memory_order mo);\n+__tsan_atomic128 __tsan_atomic128_fetch_sub(volatile __tsan_atomic128 *a,\n+    __tsan_atomic128 v, __tsan_memory_order mo);\n \n __tsan_atomic8 __tsan_atomic8_fetch_and(volatile __tsan_atomic8 *a,\n     __tsan_atomic8 v, __tsan_memory_order mo);\n@@ -84,6 +103,8 @@ __tsan_atomic32 __tsan_atomic32_fetch_and(volatile __tsan_atomic32 *a,\n     __tsan_atomic32 v, __tsan_memory_order mo);\n __tsan_atomic64 __tsan_atomic64_fetch_and(volatile __tsan_atomic64 *a,\n     __tsan_atomic64 v, __tsan_memory_order mo);\n+__tsan_atomic128 __tsan_atomic128_fetch_and(volatile __tsan_atomic128 *a,\n+    __tsan_atomic128 v, __tsan_memory_order mo);\n \n __tsan_atomic8 __tsan_atomic8_fetch_or(volatile __tsan_atomic8 *a,\n     __tsan_atomic8 v, __tsan_memory_order mo);\n@@ -93,6 +114,8 @@ __tsan_atomic32 __tsan_atomic32_fetch_or(volatile __tsan_atomic32 *a,\n     __tsan_atomic32 v, __tsan_memory_order mo);\n __tsan_atomic64 __tsan_atomic64_fetch_or(volatile __tsan_atomic64 *a,\n     __tsan_atomic64 v, __tsan_memory_order mo);\n+__tsan_atomic128 __tsan_atomic128_fetch_or(volatile __tsan_atomic128 *a,\n+    __tsan_atomic128 v, __tsan_memory_order mo);\n \n __tsan_atomic8 __tsan_atomic8_fetch_xor(volatile __tsan_atomic8 *a,\n     __tsan_atomic8 v, __tsan_memory_order mo);\n@@ -102,37 +125,67 @@ __tsan_atomic32 __tsan_atomic32_fetch_xor(volatile __tsan_atomic32 *a,\n     __tsan_atomic32 v, __tsan_memory_order mo);\n __tsan_atomic64 __tsan_atomic64_fetch_xor(volatile __tsan_atomic64 *a,\n     __tsan_atomic64 v, __tsan_memory_order mo);\n+__tsan_atomic128 __tsan_atomic128_fetch_xor(volatile __tsan_atomic128 *a,\n+    __tsan_atomic128 v, __tsan_memory_order mo);\n+\n+__tsan_atomic8 __tsan_atomic8_fetch_nand(volatile __tsan_atomic8 *a,\n+    __tsan_atomic8 v, __tsan_memory_order mo);\n+__tsan_atomic16 __tsan_atomic16_fetch_nand(volatile __tsan_atomic16 *a,\n+    __tsan_atomic16 v, __tsan_memory_order mo);\n+__tsan_atomic32 __tsan_atomic32_fetch_nand(volatile __tsan_atomic32 *a,\n+    __tsan_atomic32 v, __tsan_memory_order mo);\n+__tsan_atomic64 __tsan_atomic64_fetch_nand(volatile __tsan_atomic64 *a,\n+    __tsan_atomic64 v, __tsan_memory_order mo);\n+__tsan_atomic128 __tsan_atomic128_fetch_nand(volatile __tsan_atomic128 *a,\n+    __tsan_atomic128 v, __tsan_memory_order mo);\n \n int __tsan_atomic8_compare_exchange_weak(volatile __tsan_atomic8 *a,\n-    __tsan_atomic8 *c, __tsan_atomic8 v, __tsan_memory_order mo);\n+    __tsan_atomic8 *c, __tsan_atomic8 v, __tsan_memory_order mo,\n+    __tsan_memory_order fail_mo);\n int __tsan_atomic16_compare_exchange_weak(volatile __tsan_atomic16 *a,\n-    __tsan_atomic16 *c, __tsan_atomic16 v, __tsan_memory_order mo);\n+    __tsan_atomic16 *c, __tsan_atomic16 v, __tsan_memory_order mo,\n+    __tsan_memory_order fail_mo);\n int __tsan_atomic32_compare_exchange_weak(volatile __tsan_atomic32 *a,\n-    __tsan_atomic32 *c, __tsan_atomic32 v, __tsan_memory_order mo);\n+    __tsan_atomic32 *c, __tsan_atomic32 v, __tsan_memory_order mo,\n+    __tsan_memory_order fail_mo);\n int __tsan_atomic64_compare_exchange_weak(volatile __tsan_atomic64 *a,\n-    __tsan_atomic64 *c, __tsan_atomic64 v, __tsan_memory_order mo);\n+    __tsan_atomic64 *c, __tsan_atomic64 v, __tsan_memory_order mo,\n+    __tsan_memory_order fail_mo);\n+int __tsan_atomic128_compare_exchange_weak(volatile __tsan_atomic128 *a,\n+    __tsan_atomic128 *c, __tsan_atomic128 v, __tsan_memory_order mo,\n+    __tsan_memory_order fail_mo);\n \n int __tsan_atomic8_compare_exchange_strong(volatile __tsan_atomic8 *a,\n-    __tsan_atomic8 *c, __tsan_atomic8 v, __tsan_memory_order mo);\n+    __tsan_atomic8 *c, __tsan_atomic8 v, __tsan_memory_order mo,\n+    __tsan_memory_order fail_mo);\n int __tsan_atomic16_compare_exchange_strong(volatile __tsan_atomic16 *a,\n-    __tsan_atomic16 *c, __tsan_atomic16 v, __tsan_memory_order mo);\n+    __tsan_atomic16 *c, __tsan_atomic16 v, __tsan_memory_order mo,\n+    __tsan_memory_order fail_mo);\n int __tsan_atomic32_compare_exchange_strong(volatile __tsan_atomic32 *a,\n-    __tsan_atomic32 *c, __tsan_atomic32 v, __tsan_memory_order mo);\n+    __tsan_atomic32 *c, __tsan_atomic32 v, __tsan_memory_order mo,\n+    __tsan_memory_order fail_mo);\n int __tsan_atomic64_compare_exchange_strong(volatile __tsan_atomic64 *a,\n-    __tsan_atomic64 *c, __tsan_atomic64 v, __tsan_memory_order mo);\n+    __tsan_atomic64 *c, __tsan_atomic64 v, __tsan_memory_order mo,\n+    __tsan_memory_order fail_mo);\n+int __tsan_atomic128_compare_exchange_strong(volatile __tsan_atomic128 *a,\n+    __tsan_atomic128 *c, __tsan_atomic128 v, __tsan_memory_order mo,\n+    __tsan_memory_order fail_mo);\n \n __tsan_atomic8 __tsan_atomic8_compare_exchange_val(\n     volatile __tsan_atomic8 *a, __tsan_atomic8 c, __tsan_atomic8 v,\n-    __tsan_memory_order mo);\n+    __tsan_memory_order mo, __tsan_memory_order fail_mo);\n __tsan_atomic16 __tsan_atomic16_compare_exchange_val(\n     volatile __tsan_atomic16 *a, __tsan_atomic16 c, __tsan_atomic16 v,\n-    __tsan_memory_order mo);\n+    __tsan_memory_order mo, __tsan_memory_order fail_mo);\n __tsan_atomic32 __tsan_atomic32_compare_exchange_val(\n     volatile __tsan_atomic32 *a, __tsan_atomic32 c, __tsan_atomic32 v,\n-    __tsan_memory_order mo);\n+    __tsan_memory_order mo, __tsan_memory_order fail_mo);\n __tsan_atomic64 __tsan_atomic64_compare_exchange_val(\n     volatile __tsan_atomic64 *a, __tsan_atomic64 c, __tsan_atomic64 v,\n-    __tsan_memory_order mo);\n+    __tsan_memory_order mo, __tsan_memory_order fail_mo);\n+__tsan_atomic128 __tsan_atomic128_compare_exchange_val(\n+    volatile __tsan_atomic128 *a, __tsan_atomic128 c, __tsan_atomic128 v,\n+    __tsan_memory_order mo, __tsan_memory_order fail_mo);\n \n void __tsan_atomic_thread_fence(__tsan_memory_order mo);\n void __tsan_atomic_signal_fence(__tsan_memory_order mo);"}, {"sha": "133348a942caa906150c374187075e5cd7d8ada2", "filename": "libsanitizer/tsan/tsan_interface_inl.h", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_interface_inl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_interface_inl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interface_inl.h?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -61,3 +61,11 @@ void __tsan_func_entry(void *pc) {\n void __tsan_func_exit() {\n   FuncExit(cur_thread());\n }\n+\n+void __tsan_read_range(void *addr, uptr size) {\n+  MemoryAccessRange(cur_thread(), CALLERPC, (uptr)addr, size, false);\n+}\n+\n+void __tsan_write_range(void *addr, uptr size) {\n+  MemoryAccessRange(cur_thread(), CALLERPC, (uptr)addr, size, true);\n+}"}, {"sha": "67af1b25dd8a6178f4f5b63bed87d3b7f302cff5", "filename": "libsanitizer/tsan/tsan_platform.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_platform.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_platform.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_platform.h?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -50,7 +50,7 @@ static const uptr kLinuxAppMemMsk = 0x7c0000000000ULL;\n \n static const uptr kLinuxShadowBeg = MemToShadow(kLinuxAppMemBeg);\n static const uptr kLinuxShadowEnd =\n-  MemToShadow(kLinuxAppMemEnd) | (kPageSize - 1);\n+    MemToShadow(kLinuxAppMemEnd) | 0xff;\n \n static inline bool IsAppMem(uptr mem) {\n   return mem >= kLinuxAppMemBeg && mem <= kLinuxAppMemEnd;"}, {"sha": "ee60f532e4c7c350989521a599f18fbcee629c0a", "filename": "libsanitizer/tsan/tsan_rtl.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_rtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_rtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl.h?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -521,6 +521,7 @@ void AfterSleep(ThreadState *thr, uptr pc);\n #define HACKY_CALL(f) \\\n   __asm__ __volatile__(\"sub $1024, %%rsp;\" \\\n                        \"/*.cfi_adjust_cfa_offset 1024;*/\" \\\n+                       \".hidden \" #f \"_thunk;\" \\\n                        \"call \" #f \"_thunk;\" \\\n                        \"add $1024, %%rsp;\" \\\n                        \"/*.cfi_adjust_cfa_offset -1024;*/\" \\"}, {"sha": "8c6c9511581f5ab506b3a34bc5b3bcd4e66f9fa2", "filename": "libsanitizer/tsan/tsan_stat.cc", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_stat.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_stat.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_stat.cc?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -75,6 +75,11 @@ void StatOutput(u64 *stat) {\n   name[StatAtomicStore]                  = \"            store                 \";\n   name[StatAtomicExchange]               = \"            exchange              \";\n   name[StatAtomicFetchAdd]               = \"            fetch_add             \";\n+  name[StatAtomicFetchSub]               = \"            fetch_sub             \";\n+  name[StatAtomicFetchAnd]               = \"            fetch_and             \";\n+  name[StatAtomicFetchOr]                = \"            fetch_or              \";\n+  name[StatAtomicFetchXor]               = \"            fetch_xor             \";\n+  name[StatAtomicFetchNand]              = \"            fetch_nand            \";\n   name[StatAtomicCAS]                    = \"            compare_exchange      \";\n   name[StatAtomicFence]                  = \"            fence                 \";\n   name[StatAtomicRelaxed]                = \"  Including relaxed               \";\n@@ -87,6 +92,7 @@ void StatOutput(u64 *stat) {\n   name[StatAtomic2]                      = \"            size 2                \";\n   name[StatAtomic4]                      = \"            size 4                \";\n   name[StatAtomic8]                      = \"            size 8                \";\n+  name[StatAtomic16]                     = \"            size 16               \";\n \n   name[StatInterceptor]                  = \"Interceptors                      \";\n   name[StatInt_longjmp]                  = \"  longjmp                         \";"}, {"sha": "73d8c046e9a34c932df7cbbb3dd87e331fbd800f", "filename": "libsanitizer/tsan/tsan_stat.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_stat.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4ba5ca4650c3fdc080ff4643da386075af5fd213/libsanitizer%2Ftsan%2Ftsan_stat.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_stat.h?ref=4ba5ca4650c3fdc080ff4643da386075af5fd213", "patch": "@@ -75,6 +75,7 @@ enum StatType {\n   StatAtomicFetchAnd,\n   StatAtomicFetchOr,\n   StatAtomicFetchXor,\n+  StatAtomicFetchNand,\n   StatAtomicCAS,\n   StatAtomicFence,\n   StatAtomicRelaxed,\n@@ -87,6 +88,7 @@ enum StatType {\n   StatAtomic2,\n   StatAtomic4,\n   StatAtomic8,\n+  StatAtomic16,\n \n   // Interceptors.\n   StatInterceptor,"}]}