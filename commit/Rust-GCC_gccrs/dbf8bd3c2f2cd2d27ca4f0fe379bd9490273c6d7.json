{"sha": "dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7", "node_id": "C_kwDOANBUbNoAKGRiZjhiZDNjMmYyY2QyZDI3Y2E0ZjBmZTM3OWJkOTQ5MDI3M2M2ZDc", "commit": {"author": {"name": "Maged Michael", "email": "maged.michael@gmail.com", "date": "2021-12-07T15:20:58Z"}, "committer": {"name": "Jonathan Wakely", "email": "jwakely@redhat.com", "date": "2021-12-08T11:39:34Z"}, "message": "libstdc++: Skip atomic instructions in shared_ptr when both counts are 1\n\nThis rewrites _Sp_counted_base::_M_release to skip the two atomic\ninstructions that decrement each of the use count and the weak count\nwhen both are 1.\n\nBenefits: Save the cost of the last atomic decrements of each of the use\ncount and the weak count in _Sp_counted_base. Atomic instructions are\nsignificantly slower than regular loads and stores across major\narchitectures.\n\nHow current code works: _M_release() atomically decrements the use\ncount, checks if it was 1, if so calls _M_dispose(), atomically\ndecrements the weak count, checks if it was 1, and if so calls\n_M_destroy().\n\nHow the proposed algorithm works: _M_release() loads both use count and\nweak count together atomically (assuming suitable alignment, discussed\nlater), checks if the value corresponds to a 0x1 value in the individual\ncount members, and if so calls _M_dispose() and _M_destroy().\nOtherwise, it follows the original algorithm.\n\nWhy it works: When the current thread executing _M_release() finds each\nof the counts is equal to 1, then no other threads could possibly hold\nuse or weak references to this control block. That is, no other threads\ncould possibly access the counts or the protected object.\n\nThere are two crucial high-level issues that I'd like to point out first:\n- Atomicity of access to the counts together\n- Proper alignment of the counts together\n\nThe patch is intended to apply the proposed algorithm only to the case of\n64-bit mode, 4-byte counts, and 8-byte aligned _Sp_counted_base.\n\n** Atomicity **\n- The proposed algorithm depends on the mutual atomicity among 8-byte\natomic operations and 4-byte atomic operations on each of the 4-byte halves\nof the 8-byte aligned 8-byte block.\n- The standard does not guarantee atomicity of 8-byte operations on a pair\nof 8-byte aligned 4-byte objects.\n- To my knowledge this works in practice on systems that guarantee native\nimplementation of 4-byte and 8-byte atomic operations.\n- __atomic_always_lock_free is used to check for native atomic operations.\n\n** Alignment **\n- _Sp_counted_base is an internal base class, with a virtual destructor,\nso it has a vptr at the beginning of the class, and will be aligned to\nalignof(void*) i.e. 8 bytes.\n- The first members of the class are the 4-byte use count and 4-byte\nweak count, which will occupy 8 contiguous bytes immediately after the\nvptr, i.e. they form an 8-byte aligned 8 byte range.\n\nOther points:\n- The proposed algorithm can interact correctly with the current algorithm.\nThat is, multiple threads using different versions of the code with and\nwithout the patch operating on the same objects should always interact\ncorrectly. The intent for the patch is to be ABI compatible with the\ncurrent implementation.\n- The proposed patch involves a performance trade-off between saving the\ncosts of atomic instructions when the counts are both 1 vs adding the cost\nof loading the 8-byte combined counts and comparison with {0x1, 0x1}.\n- I noticed a big difference between the code generated by GCC vs LLVM. GCC\nseems to generate noticeably more code and what seems to be redundant null\nchecks and branches.\n- The patch has been in use (built using LLVM) in a large environment for\nmany months. The performance gains outweigh the losses (roughly 10 to 1)\nacross a large variety of workloads.\n\nSigned-off-by: Jonathan Wakely <jwakely@redhat.com>\n\nCo-authored-by: Jonathan Wakely <jwakely@redhat.com>\n\nlibstdc++-v3/ChangeLog:\n\n\t* include/bits/c++config (_GLIBCXX_TSAN): Define macro\n\tindicating that TSan is in use.\n\t* include/bits/shared_ptr_base.h (_Sp_counted_base::_M_release):\n\tReplace definition in primary template with explicit\n\tspecializations for _S_mutex and _S_atomic policies.\n\t(_Sp_counted_base<_S_mutex>::_M_release): New specialization.\n\t(_Sp_counted_base<_S_atomic>::_M_release): New specialization,\n\tusing a single atomic load to access both reference counts at\n\tonce.\n\t(_Sp_counted_base::_M_release_last_use): New member function.", "tree": {"sha": "120b28ba39e0d38d7a77bca0bbee9e7ca31fa1a2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/120b28ba39e0d38d7a77bca0bbee9e7ca31fa1a2"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7/comments", "author": {"login": "magedm", "id": 15930371, "node_id": "MDQ6VXNlcjE1OTMwMzcx", "avatar_url": "https://avatars.githubusercontent.com/u/15930371?v=4", "gravatar_id": "", "url": "https://api.github.com/users/magedm", "html_url": "https://github.com/magedm", "followers_url": "https://api.github.com/users/magedm/followers", "following_url": "https://api.github.com/users/magedm/following{/other_user}", "gists_url": "https://api.github.com/users/magedm/gists{/gist_id}", "starred_url": "https://api.github.com/users/magedm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/magedm/subscriptions", "organizations_url": "https://api.github.com/users/magedm/orgs", "repos_url": "https://api.github.com/users/magedm/repos", "events_url": "https://api.github.com/users/magedm/events{/privacy}", "received_events_url": "https://api.github.com/users/magedm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jwakely", "id": 1254480, "node_id": "MDQ6VXNlcjEyNTQ0ODA=", "avatar_url": "https://avatars.githubusercontent.com/u/1254480?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwakely", "html_url": "https://github.com/jwakely", "followers_url": "https://api.github.com/users/jwakely/followers", "following_url": "https://api.github.com/users/jwakely/following{/other_user}", "gists_url": "https://api.github.com/users/jwakely/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwakely/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwakely/subscriptions", "organizations_url": "https://api.github.com/users/jwakely/orgs", "repos_url": "https://api.github.com/users/jwakely/repos", "events_url": "https://api.github.com/users/jwakely/events{/privacy}", "received_events_url": "https://api.github.com/users/jwakely/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "13b6c7639cfdca892a3f02b63596b097e1839f38", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/13b6c7639cfdca892a3f02b63596b097e1839f38", "html_url": "https://github.com/Rust-GCC/gccrs/commit/13b6c7639cfdca892a3f02b63596b097e1839f38"}], "stats": {"total": 125, "additions": 103, "deletions": 22}, "files": [{"sha": "f2d704f57eb19dde85633344ab36a086bd63f8a7", "filename": "libstdc++-v3/include/bits/c++config", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fc%2B%2Bconfig", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fc%2B%2Bconfig", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fc%2B%2Bconfig?ref=dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7", "patch": "@@ -577,6 +577,15 @@ namespace std\n   do { __glibcxx_constexpr_assert(cond); } while (false)\n #endif\n \n+// Macro indicating that TSAN is in use.\n+#if __SANITIZE_THREAD__\n+#  define _GLIBCXX_TSAN 1\n+#elif defined __has_feature\n+# if __has_feature(thread_sanitizer)\n+#  define _GLIBCXX_TSAN 1\n+# endif\n+#endif\n+\n // Macros for race detectors.\n // _GLIBCXX_SYNCHRONIZATION_HAPPENS_BEFORE(A) and\n // _GLIBCXX_SYNCHRONIZATION_HAPPENS_AFTER(A) should be used to explain"}, {"sha": "90ad30947b0499952ef9eb4006c2e1e596266816", "filename": "libstdc++-v3/include/bits/shared_ptr_base.h", "status": "modified", "additions": 94, "deletions": 22, "changes": 116, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fshared_ptr_base.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fshared_ptr_base.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fshared_ptr_base.h?ref=dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7", "patch": "@@ -143,53 +143,64 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       virtual void*\n       _M_get_deleter(const std::type_info&) noexcept = 0;\n \n+      // Increment the use count (used when the count is greater than zero).\n       void\n       _M_add_ref_copy()\n       { __gnu_cxx::__atomic_add_dispatch(&_M_use_count, 1); }\n \n+      // Increment the use count if it is non-zero, throw otherwise.\n       void\n       _M_add_ref_lock()\n       {\n \tif (!_M_add_ref_lock_nothrow())\n \t  __throw_bad_weak_ptr();\n       }\n \n+      // Increment the use count if it is non-zero.\n       bool\n       _M_add_ref_lock_nothrow() noexcept;\n \n+      // Decrement the use count.\n       void\n-      _M_release() noexcept\n+      _M_release() noexcept;\n+\n+      // Called by _M_release() when the use count reaches zero.\n+      void\n+      _M_release_last_use() noexcept\n       {\n-        // Be race-detector-friendly.  For more info see bits/c++config.\n-        _GLIBCXX_SYNCHRONIZATION_HAPPENS_BEFORE(&_M_use_count);\n-\tif (__gnu_cxx::__exchange_and_add_dispatch(&_M_use_count, -1) == 1)\n+\t_GLIBCXX_SYNCHRONIZATION_HAPPENS_AFTER(&_M_use_count);\n+\t_M_dispose();\n+\t// There must be a memory barrier between dispose() and destroy()\n+\t// to ensure that the effects of dispose() are observed in the\n+\t// thread that runs destroy().\n+\t// See http://gcc.gnu.org/ml/libstdc++/2005-11/msg00136.html\n+\tif (_Mutex_base<_Lp>::_S_need_barriers)\n \t  {\n-            _GLIBCXX_SYNCHRONIZATION_HAPPENS_AFTER(&_M_use_count);\n-\t    _M_dispose();\n-\t    // There must be a memory barrier between dispose() and destroy()\n-\t    // to ensure that the effects of dispose() are observed in the\n-\t    // thread that runs destroy().\n-\t    // See http://gcc.gnu.org/ml/libstdc++/2005-11/msg00136.html\n-\t    if (_Mutex_base<_Lp>::_S_need_barriers)\n-\t      {\n-\t\t__atomic_thread_fence (__ATOMIC_ACQ_REL);\n-\t      }\n+\t    __atomic_thread_fence (__ATOMIC_ACQ_REL);\n+\t  }\n \n-            // Be race-detector-friendly.  For more info see bits/c++config.\n-            _GLIBCXX_SYNCHRONIZATION_HAPPENS_BEFORE(&_M_weak_count);\n-\t    if (__gnu_cxx::__exchange_and_add_dispatch(&_M_weak_count,\n-\t\t\t\t\t\t       -1) == 1)\n-              {\n-                _GLIBCXX_SYNCHRONIZATION_HAPPENS_AFTER(&_M_weak_count);\n-\t        _M_destroy();\n-              }\n+\t// Be race-detector-friendly.  For more info see bits/c++config.\n+\t_GLIBCXX_SYNCHRONIZATION_HAPPENS_BEFORE(&_M_weak_count);\n+\tif (__gnu_cxx::__exchange_and_add_dispatch(&_M_weak_count,\n+\t\t\t\t\t\t   -1) == 1)\n+\t  {\n+\t    _GLIBCXX_SYNCHRONIZATION_HAPPENS_AFTER(&_M_weak_count);\n+\t    _M_destroy();\n \t  }\n       }\n \n+      // As above, but 'noinline' to reduce code size on the cold path.\n+      __attribute__((__noinline__))\n+      void\n+      _M_release_last_use_cold() noexcept\n+      { _M_release_last_use(); }\n+\n+      // Increment the weak count.\n       void\n       _M_weak_add_ref() noexcept\n       { __gnu_cxx::__atomic_add_dispatch(&_M_weak_count, 1); }\n \n+      // Decrement the weak count.\n       void\n       _M_weak_release() noexcept\n       {\n@@ -286,6 +297,67 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n         }\n     }\n \n+  template<>\n+    inline void\n+    _Sp_counted_base<_S_mutex>::_M_release() noexcept\n+    {\n+      // Be race-detector-friendly.  For more info see bits/c++config.\n+      _GLIBCXX_SYNCHRONIZATION_HAPPENS_BEFORE(&_M_use_count);\n+      if (__gnu_cxx::__exchange_and_add_dispatch(&_M_use_count, -1) == 1)\n+\t{\n+\t  _M_release_last_use();\n+\t}\n+    }\n+\n+  template<>\n+    inline void\n+    _Sp_counted_base<_S_atomic>::_M_release() noexcept\n+    {\n+      _GLIBCXX_SYNCHRONIZATION_HAPPENS_BEFORE(&_M_use_count);\n+#if ! _GLIBCXX_TSAN\n+      constexpr bool __lock_free\n+\t= __atomic_always_lock_free(sizeof(long long), 0)\n+\t&& __atomic_always_lock_free(sizeof(_Atomic_word), 0);\n+      constexpr bool __double_word\n+\t= sizeof(long long) == 2 * sizeof(_Atomic_word);\n+      // The ref-count members follow the vptr, so are aligned to\n+      // alignof(void*).\n+      constexpr bool __aligned = __alignof(long long) <= alignof(void*);\n+      if _GLIBCXX17_CONSTEXPR (__lock_free && __double_word && __aligned)\n+\t{\n+\t  constexpr long long __unique_ref\n+\t    = 1LL + (1LL << (__CHAR_BIT__ * sizeof(_Atomic_word)));\n+\t  auto __both_counts = reinterpret_cast<long long*>(&_M_use_count);\n+\n+\t  _GLIBCXX_SYNCHRONIZATION_HAPPENS_BEFORE(&_M_weak_count);\n+\t  if (__atomic_load_n(__both_counts, __ATOMIC_ACQUIRE) == __unique_ref)\n+\t    {\n+\t      // Both counts are 1, so there are no weak references and\n+\t      // we are releasing the last strong reference. No other\n+\t      // threads can observe the effects of this _M_release()\n+\t      // call (e.g. calling use_count()) without a data race.\n+\t      *(long long*)(&_M_use_count) = 0;\n+\t      _GLIBCXX_SYNCHRONIZATION_HAPPENS_AFTER(&_M_use_count);\n+\t      _GLIBCXX_SYNCHRONIZATION_HAPPENS_AFTER(&_M_weak_count);\n+\t      _M_dispose();\n+\t      _M_destroy();\n+\t      return;\n+\t    }\n+\t  if (__gnu_cxx::__exchange_and_add_dispatch(&_M_use_count, -1) == 1)\n+\t    [[__unlikely__]]\n+\t    {\n+\t      _M_release_last_use_cold();\n+\t      return;\n+\t    }\n+\t}\n+      else\n+#endif\n+      if (__gnu_cxx::__exchange_and_add_dispatch(&_M_use_count, -1) == 1)\n+\t{\n+\t  _M_release_last_use();\n+\t}\n+    }\n+\n   template<>\n     inline void\n     _Sp_counted_base<_S_single>::_M_weak_add_ref() noexcept"}]}