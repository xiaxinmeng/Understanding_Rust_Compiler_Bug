{"sha": "6a0d3939018de736da03cb54a86fa2395b5bc464", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NmEwZDM5MzkwMThkZTczNmRhMDNjYjU0YTg2ZmEyMzk1YjViYzQ2NA==", "commit": {"author": {"name": "Steve Ellcey", "email": "sellcey@marvell.com", "date": "2019-04-10T20:28:19Z"}, "committer": {"name": "Steve Ellcey", "email": "sje@gcc.gnu.org", "date": "2019-04-10T20:28:19Z"}, "message": "re PR rtl-optimization/87763 (aarch64 target testcases fail after r265398)\n\n2018-04-10  Steve Ellcey  <sellcey@marvell.com>\n\n\tPR rtl-optimization/87763\n\t* config/aarch64/aarch64-protos.h (aarch64_masks_and_shift_for_bfi_p):\n\tNew prototype.\n\t* config/aarch64/aarch64.c (aarch64_masks_and_shift_for_bfi_p):\n\tNew function.\n\t* config/aarch64/aarch64.md (*aarch64_bfi<GPI:mode>5_shift):\n\tNew instruction.\n\t(*aarch64_bfi<GPI:mode>5_shift_alt): Ditto.\n\t(*aarch64_bfi<GPI:mode>4_noand): Ditto.\n\t(*aarch64_bfi<GPI:mode>4_noand_alt): Ditto.\n\t(*aarch64_bfi<GPI:mode>4_noshift): Ditto.\n\nFrom-SVN: r270266", "tree": {"sha": "b8ea778e3ebffd080f9834e9bba4e051daaf8619", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/b8ea778e3ebffd080f9834e9bba4e051daaf8619"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6a0d3939018de736da03cb54a86fa2395b5bc464", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6a0d3939018de736da03cb54a86fa2395b5bc464", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6a0d3939018de736da03cb54a86fa2395b5bc464", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6a0d3939018de736da03cb54a86fa2395b5bc464/comments", "author": null, "committer": null, "parents": [{"sha": "51d3c11a7c79294599c1e9088c80f7417566a75e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/51d3c11a7c79294599c1e9088c80f7417566a75e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/51d3c11a7c79294599c1e9088c80f7417566a75e"}], "stats": {"total": 134, "additions": 134, "deletions": 0}, "files": [{"sha": "a9db4b424d418b648c367df9a9fe6db233937d46", "filename": "gcc/ChangeLog", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a0d3939018de736da03cb54a86fa2395b5bc464/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a0d3939018de736da03cb54a86fa2395b5bc464/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=6a0d3939018de736da03cb54a86fa2395b5bc464", "patch": "@@ -1,3 +1,17 @@\n+2018-04-10  Steve Ellcey  <sellcey@marvell.com>\n+\n+\tPR rtl-optimization/87763\n+\t* config/aarch64/aarch64-protos.h (aarch64_masks_and_shift_for_bfi_p):\n+\tNew prototype.\n+\t* config/aarch64/aarch64.c (aarch64_masks_and_shift_for_bfi_p):\n+\tNew function.\n+\t* config/aarch64/aarch64.md (*aarch64_bfi<GPI:mode>5_shift):\n+\tNew instruction.\n+\t(*aarch64_bfi<GPI:mode>5_shift_alt): Ditto.\n+\t(*aarch64_bfi<GPI:mode>4_noand): Ditto.\n+\t(*aarch64_bfi<GPI:mode>4_noand_alt): Ditto.\n+\t(*aarch64_bfi<GPI:mode>4_noshift): Ditto.\n+\n 2019-04-10  Jonathan Wakely  <jwakely@redhat.com>\n \n \t* doc/invoke.texi (Optimize Options): Change \"Nevertheless\" to"}, {"sha": "b6c0d0a8eb6c1bbc6d0524fbf98686fa9ef9613d", "filename": "gcc/config/aarch64/aarch64-protos.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a0d3939018de736da03cb54a86fa2395b5bc464/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a0d3939018de736da03cb54a86fa2395b5bc464/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h?ref=6a0d3939018de736da03cb54a86fa2395b5bc464", "patch": "@@ -429,6 +429,9 @@ bool aarch64_label_mentioned_p (rtx);\n void aarch64_declare_function_name (FILE *, const char*, tree);\n bool aarch64_legitimate_pic_operand_p (rtx);\n bool aarch64_mask_and_shift_for_ubfiz_p (scalar_int_mode, rtx, rtx);\n+bool aarch64_masks_and_shift_for_bfi_p (scalar_int_mode, unsigned HOST_WIDE_INT,\n+\t\t\t\t\tunsigned HOST_WIDE_INT,\n+\t\t\t\t\tunsigned HOST_WIDE_INT);\n bool aarch64_zero_extend_const_eq (machine_mode, rtx, machine_mode, rtx);\n bool aarch64_move_imm (HOST_WIDE_INT, machine_mode);\n opt_machine_mode aarch64_sve_pred_mode (unsigned int);"}, {"sha": "9be75485ce3a97c34792d1ca13385cf384f6c043", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 29, "deletions": 0, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a0d3939018de736da03cb54a86fa2395b5bc464/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a0d3939018de736da03cb54a86fa2395b5bc464/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=6a0d3939018de736da03cb54a86fa2395b5bc464", "patch": "@@ -9336,6 +9336,35 @@ aarch64_mask_and_shift_for_ubfiz_p (scalar_int_mode mode, rtx mask,\n \t     & ((HOST_WIDE_INT_1U << INTVAL (shft_amnt)) - 1)) == 0;\n }\n \n+/* Return true if the masks and a shift amount from an RTX of the form\n+   ((x & MASK1) | ((y << SHIFT_AMNT) & MASK2)) are valid to combine into\n+   a BFI instruction of mode MODE.  See *arch64_bfi patterns.  */\n+\n+bool\n+aarch64_masks_and_shift_for_bfi_p (scalar_int_mode mode,\n+\t\t\t\t   unsigned HOST_WIDE_INT mask1,\n+\t\t\t\t   unsigned HOST_WIDE_INT shft_amnt,\n+\t\t\t\t   unsigned HOST_WIDE_INT mask2)\n+{\n+  unsigned HOST_WIDE_INT t;\n+\n+  /* Verify that there is no overlap in what bits are set in the two masks.  */\n+  if (mask1 != ~mask2)\n+    return false;\n+\n+  /* Verify that mask2 is not all zeros or ones.  */\n+  if (mask2 == 0 || mask2 == HOST_WIDE_INT_M1U)\n+    return false;\n+\n+  /* The shift amount should always be less than the mode size.  */\n+  gcc_assert (shft_amnt < GET_MODE_BITSIZE (mode));\n+\n+  /* Verify that the mask being shifted is contiguous and would be in the\n+     least significant bits after shifting by shft_amnt.  */\n+  t = mask2 + (HOST_WIDE_INT_1U << shft_amnt);\n+  return (t == (t & -t));\n+}\n+\n /* Calculate the cost of calculating X, storing it in *COST.  Result\n    is true if the total cost of the operation has now been calculated.  */\n static bool"}, {"sha": "e0df975a03223b3549e63f5744ba727193585961", "filename": "gcc/config/aarch64/aarch64.md", "status": "modified", "additions": 88, "deletions": 0, "changes": 88, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a0d3939018de736da03cb54a86fa2395b5bc464/gcc%2Fconfig%2Faarch64%2Faarch64.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a0d3939018de736da03cb54a86fa2395b5bc464/gcc%2Fconfig%2Faarch64%2Faarch64.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.md?ref=6a0d3939018de736da03cb54a86fa2395b5bc464", "patch": "@@ -5491,6 +5491,94 @@\n   [(set_attr \"type\" \"bfm\")]\n )\n \n+;;  Match a bfi instruction where the shift of OP3 means that we are\n+;;  actually copying the least significant bits of OP3 into OP0 by way\n+;;  of the AND masks and the IOR instruction.  A similar instruction\n+;;  with the two parts of the IOR swapped around was never triggered\n+;;  in a bootstrap build and test of GCC so it was not included.\n+\n+(define_insn \"*aarch64_bfi<GPI:mode>5_shift\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (ior:GPI (and:GPI (match_operand:GPI 1 \"register_operand\" \"0\")\n+                          (match_operand:GPI 2 \"const_int_operand\" \"n\"))\n+                 (and:GPI (ashift:GPI\n+                           (match_operand:GPI 3 \"register_operand\" \"r\")\n+                           (match_operand:GPI 4 \"aarch64_simd_shift_imm_<mode>\" \"n\"))\n+                          (match_operand:GPI 5 \"const_int_operand\" \"n\"))))]\n+  \"aarch64_masks_and_shift_for_bfi_p (<MODE>mode, UINTVAL (operands[2]),\n+\t\t\t\t      UINTVAL (operands[4]),\n+\t\t\t\t      UINTVAL(operands[5]))\"\n+  \"bfi\\t%<GPI:w>0, %<GPI:w>3, %4, %P5\"\n+  [(set_attr \"type\" \"bfm\")]\n+)\n+\n+(define_insn \"*aarch64_bfi<GPI:mode>5_shift_alt\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (ior:GPI (and:GPI (ashift:GPI\n+                           (match_operand:GPI 1 \"register_operand\" \"r\")\n+                           (match_operand:GPI 2 \"aarch64_simd_shift_imm_<mode>\" \"n\"))\n+                          (match_operand:GPI 3 \"const_int_operand\" \"n\"))\n+\t\t (and:GPI (match_operand:GPI 4 \"register_operand\" \"0\")\n+                          (match_operand:GPI 5 \"const_int_operand\" \"n\"))))]\n+  \"aarch64_masks_and_shift_for_bfi_p (<MODE>mode, UINTVAL (operands[5]),\n+\t\t\t\t      UINTVAL (operands[2]),\n+\t\t\t\t      UINTVAL(operands[3]))\"\n+  \"bfi\\t%<GPI:w>0, %<GPI:w>1, %2, %P3\"\n+  [(set_attr \"type\" \"bfm\")]\n+)\n+\n+;; Like *aarch64_bfi<GPI:mode>5_shift but with no and of the ashift because\n+;; the shift is large enough to remove the need for an AND instruction.\n+\n+(define_insn \"*aarch64_bfi<GPI:mode>4_noand\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (ior:GPI (and:GPI (match_operand:GPI 1 \"register_operand\" \"0\")\n+                          (match_operand:GPI 2 \"const_int_operand\" \"n\"))\n+                 (ashift:GPI\n+                          (match_operand:GPI 3 \"register_operand\" \"r\")\n+                          (match_operand:GPI 4 \"aarch64_simd_shift_imm_<mode>\" \"n\"))))]\n+  \"aarch64_masks_and_shift_for_bfi_p (<MODE>mode, UINTVAL (operands[2]),\n+\t\t\t\t      UINTVAL (operands[4]),\n+\t\t\t\t      HOST_WIDE_INT_M1U << UINTVAL (operands[4]) )\"\n+{\n+  operands[5] = GEN_INT (GET_MODE_BITSIZE (<MODE>mode) - UINTVAL (operands[4]));\n+  return \"bfi\\t%<GPI:w>0, %<GPI:w>3, %4, %5\";\n+}\n+  [(set_attr \"type\" \"bfm\")]\n+)\n+\n+(define_insn \"*aarch64_bfi<GPI:mode>4_noand_alt\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (ior:GPI (ashift:GPI\n+                          (match_operand:GPI 1 \"register_operand\" \"r\")\n+                          (match_operand:GPI 2 \"aarch64_simd_shift_imm_<mode>\" \"n\"))\n+\t\t (and:GPI (match_operand:GPI 3 \"register_operand\" \"0\")\n+                          (match_operand:GPI 4 \"const_int_operand\" \"n\"))))]\n+  \"aarch64_masks_and_shift_for_bfi_p (<MODE>mode, UINTVAL (operands[4]),\n+\t\t\t\t      UINTVAL (operands[2]),\n+\t\t\t\t      HOST_WIDE_INT_M1U << UINTVAL (operands[2]) )\"\n+{\n+  operands[5] = GEN_INT (GET_MODE_BITSIZE (<MODE>mode) - UINTVAL (operands[2]));\n+  return \"bfi\\t%<GPI:w>0, %<GPI:w>1, %2, %5\";\n+}\n+  [(set_attr \"type\" \"bfm\")]\n+)\n+\n+;; Like *aarch64_bfi<GPI:mode>5_shift but with no shifting, we are just\n+;; copying the least significant bits of OP3 to OP0.\n+\n+(define_insn \"*aarch64_bfi<GPI:mode>4_noshift\"\n+  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n+        (ior:GPI (and:GPI (match_operand:GPI 1 \"register_operand\" \"0\")\n+                          (match_operand:GPI 2 \"const_int_operand\" \"n\"))\n+                 (and:GPI (match_operand:GPI 3 \"register_operand\" \"r\")\n+                          (match_operand:GPI 4 \"const_int_operand\" \"n\"))))]\n+  \"aarch64_masks_and_shift_for_bfi_p (<MODE>mode, UINTVAL (operands[2]), 0,\n+\t\t\t\t      UINTVAL (operands[4]))\"\n+  \"bfi\\t%<GPI:w>0, %<GPI:w>3, 0, %P4\"\n+  [(set_attr \"type\" \"bfm\")]\n+)\n+\n (define_insn \"*extr_insv_lower_reg<mode>\"\n   [(set (zero_extract:GPI (match_operand:GPI 0 \"register_operand\" \"+r\")\n \t\t\t  (match_operand 1 \"const_int_operand\" \"n\")"}]}