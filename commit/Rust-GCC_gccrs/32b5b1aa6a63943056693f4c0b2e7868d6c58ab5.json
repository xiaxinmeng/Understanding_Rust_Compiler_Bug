{"sha": "32b5b1aa6a63943056693f4c0b2e7868d6c58ab5", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MzJiNWIxYWE2YTYzOTQzMDU2NjkzZjRjMGIyZTc4NjhkNmM1OGFiNQ==", "commit": {"author": {"name": "Stan Cox", "email": "coxs@gnu.org", "date": "1996-02-09T22:44:47Z"}, "committer": {"name": "Stan Cox", "email": "coxs@gnu.org", "date": "1996-02-09T22:44:47Z"}, "message": "(processor_costs): New variable.\n\nFrom-SVN: r11188", "tree": {"sha": "ed2851076f17cc5ddefb79af70bc323cc60a490c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ed2851076f17cc5ddefb79af70bc323cc60a490c"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/32b5b1aa6a63943056693f4c0b2e7868d6c58ab5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/32b5b1aa6a63943056693f4c0b2e7868d6c58ab5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/32b5b1aa6a63943056693f4c0b2e7868d6c58ab5", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/32b5b1aa6a63943056693f4c0b2e7868d6c58ab5/comments", "author": null, "committer": null, "parents": [{"sha": "2ee887f2725c1f074859ae85735f08bffca23dfb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2ee887f2725c1f074859ae85735f08bffca23dfb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2ee887f2725c1f074859ae85735f08bffca23dfb"}], "stats": {"total": 1049, "additions": 991, "deletions": 58}, "files": [{"sha": "aef0c8ec4796d4408f52a519fba6da27153caa5a", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 991, "deletions": 58, "changes": 1049, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/32b5b1aa6a63943056693f4c0b2e7868d6c58ab5/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/32b5b1aa6a63943056693f4c0b2e7868d6c58ab5/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=32b5b1aa6a63943056693f4c0b2e7868d6c58ab5", "patch": "@@ -1,5 +1,5 @@\n /* Subroutines for insn-output.c for Intel X86.\n-   Copyright (C) 1988, 1992, 1994, 1995 Free Software Foundation, Inc.\n+   Copyright (C) 1988, 1992, 1994, 1995, 1996 Free Software Foundation, Inc.\n \n This file is part of GNU CC.\n \n@@ -16,7 +16,7 @@ GNU General Public License for more details.\n You should have received a copy of the GNU General Public License\n along with GNU CC; see the file COPYING.  If not, write to\n the Free Software Foundation, 59 Temple Place - Suite 330,\n-Boston, MA 02111-1307, USA.  */\n+Boston, MA 02111-1307, USA. */\n \n #include <stdio.h>\n #include <setjmp.h>\n@@ -44,6 +44,46 @@ Boston, MA 02111-1307, USA.  */\n    even if the conditional was untrue.  */\n #endif\n \n+enum reg_mem\t\t\t/* Type of an operand for ix86_{binary,unary}_operator_ok */\n+{\n+  reg_p,\n+  mem_p,\n+  imm_p\n+};\n+\n+/* Processor costs (relative to an add) */\n+struct processor_costs i386_cost = {\t/* 386 specific costs */\n+  1,\t\t\t\t\t/* cost of an add instruction (2 cycles) */\n+  1,\t\t\t\t\t/* cost of a lea instruction */\n+  3,\t\t\t\t\t/* variable shift costs */\n+  2,\t\t\t\t\t/* constant shift costs */\n+  6,\t\t\t\t\t/* cost of starting a multiply */\n+  1,\t\t\t\t\t/* cost of multiply per each bit set */\n+  23\t\t\t\t\t/* cost of a divide/mod */\n+};\n+\n+struct processor_costs i486_cost = {\t/* 486 specific costs */\n+  1,\t\t\t\t\t/* cost of an add instruction */\n+  1,\t\t\t\t\t/* cost of a lea instruction */\n+  3,\t\t\t\t\t/* variable shift costs */\n+  2,\t\t\t\t\t/* constant shift costs */\n+  12,\t\t\t\t\t/* cost of starting a multiply */\n+  1,\t\t\t\t\t/* cost of multiply per each bit set */\n+  40\t\t\t\t\t/* cost of a divide/mod */\n+};\n+\n+struct processor_costs pentium_cost = {\t/* 486 specific costs */\n+  1,\t\t\t\t\t/* cost of an add instruction */\n+  1,\t\t\t\t\t/* cost of a lea instruction */\n+  3,\t\t\t\t\t/* variable shift costs */\n+  2,\t\t\t\t\t/* constant shift costs */\n+  12,\t\t\t\t\t/* cost of starting a multiply */\n+  1,\t\t\t\t\t/* cost of multiply per each bit set */\n+  40\t\t\t\t\t/* cost of a divide/mod */\n+};\n+\n+struct processor_costs *ix86_cost = &pentium_cost;\n+\n #define AT_BP(mode) (gen_rtx (MEM, (mode), frame_pointer_rtx))\n \n extern FILE *asm_out_file;\n@@ -128,16 +168,17 @@ override_options ()\n     {\n       char *name;\t\t/* Canonical processor name.  */\n       enum processor_type processor; /* Processor type enum value.  */\n+      struct processor_costs *cost; /* Processor costs */\n       int target_enable;\t/* Target flags to enable.  */\n       int target_disable;\t/* Target flags to disable.  */\n     } processor_target_table[]\n-      = {{PROCESSOR_COMMON_STRING, PROCESSOR_COMMON, 0, 0},\n-\t   {PROCESSOR_I386_STRING, PROCESSOR_I386, 0, 0},\n-\t   {PROCESSOR_I486_STRING, PROCESSOR_I486, 0, 0},\n-\t   {PROCESSOR_I586_STRING, PROCESSOR_PENTIUM, 0, 0},\n-\t   {PROCESSOR_PENTIUM_STRING, PROCESSOR_PENTIUM, 0, 0},\n-\t   {PROCESSOR_I686_STRING, PROCESSOR_PENTIUMPRO, 0, 0},\n-\t   {PROCESSOR_PENTIUMPRO_STRING, PROCESSOR_PENTIUMPRO, 0, 0}};\n+      = {{PROCESSOR_COMMON_STRING, PROCESSOR_COMMON, &i486_cost, 0, 0},\n+\t   {PROCESSOR_I386_STRING, PROCESSOR_I386, &i386_cost, 0, 0},\n+\t   {PROCESSOR_I486_STRING, PROCESSOR_I486, &i486_cost, 0, 0},\n+\t   {PROCESSOR_I586_STRING, PROCESSOR_PENTIUM, &pentium_cost, 0, 0},\n+\t   {PROCESSOR_PENTIUM_STRING, PROCESSOR_PENTIUM, &pentium_cost, 0, 0},\n+\t   {PROCESSOR_I686_STRING, PROCESSOR_PENTIUMPRO, &pentium_cost, 0, 0},\n+\t   {PROCESSOR_PENTIUMPRO_STRING, PROCESSOR_PENTIUMPRO, &pentium_cost, 0, 0}};\n \n   int ptt_size = sizeof (processor_target_table) / sizeof (struct ptt);\n \n@@ -298,54 +339,26 @@ order_regs_for_local_alloc ()\n \t}\n     }\n \n-  /* If users did not specify a register allocation order, favor eax\n-     normally except if DImode variables are used, in which case\n-     favor edx before eax, which seems to cause less spill register\n-     not found messages.  */\n+  /* If users did not specify a register allocation order, use natural order */\n   else\n     {\n-      rtx insn;\n-\n       for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n \treg_alloc_order[i] = i;\n-\n-      if (optimize)\n-\t{\n-\t  int use_dca = FALSE;\n-\n-\t  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n-\t    {\n-\t      if (GET_CODE (insn) == INSN)\n-\t\t{\n-\t\t  rtx set = NULL_RTX;\n-\t\t  rtx pattern = PATTERN (insn);\n-\n-\t\t  if (GET_CODE (pattern) == SET)\n-\t\t    set = pattern;\n-\n-\t\t  else if ((GET_CODE (pattern) == PARALLEL\n-\t\t\t    || GET_CODE (pattern) == SEQUENCE)\n-\t\t\t   && GET_CODE (XVECEXP (pattern, 0, 0)) == SET)\n-\t\t    set = XVECEXP (pattern, 0, 0);\n-\n-\t\t  if (set && GET_MODE (SET_SRC (set)) == DImode)\n-\t\t    {\n-\t\t      use_dca = TRUE;\n-\t\t      break;\n-\t\t    }\n-\t\t}\n-\t    }\n-\n-\t  if (use_dca)\n-\t    {\n-\t      reg_alloc_order[0] = 1;\t/* edx */\n-\t      reg_alloc_order[1] = 2;\t/* ecx */\n-\t      reg_alloc_order[2] = 0;\t/* eax */\n-\t    }\n-\t}\n     }\n }\n \n+\f\n+void\n+optimization_options (level)\n+     int level;\n+{\n+  /* For -O2, and beyond, turn off -fschedule-insns by default.  It tends to\n+     make the problem with not enough registers even worse */\n+#ifdef INSN_SCHEDULING\n+  if (level > 1)\n+    flag_schedule_insns = 0;\n+#endif\n+}\n \f\n /* Return nonzero if IDENTIFIER with arguments ARGS is a valid machine specific\n    attribute for DECL.  The attributes in ATTRIBUTES have previously been\n@@ -1513,6 +1526,137 @@ symbolic_reference_mentioned_p (op)\n \n   return 0;\n }\n+\f\n+/* Attempt to expand a binary operator.  Make the expansion closer to the\n+   actual machine, then just general_operand, which will allow 3 separate\n+   memory references (one output, two input) in a single insn.  Return\n+   whether the insn fails, or succeeds.  */\n+\n+int\n+ix86_expand_binary_operator (code, mode, operands)\n+     enum rtx_code code;\n+     enum machine_mode mode;\n+     rtx operands[];\n+{\n+  rtx insn;\n+  int i;\n+  int modified;\n+\n+  /* Recognize <var1> = <value> <op> <var1> for commutative operators */\n+  if (GET_RTX_CLASS (code) == 'c'\n+      && (rtx_equal_p (operands[0], operands[2])\n+\t  || immediate_operand (operands[1], mode)))\n+    {\n+      rtx temp = operands[1];\n+      operands[1] = operands[2];\n+      operands[2] = temp;\n+    }\n+\n+  /* If optimizing, copy to regs to improve CSE */\n+  if (TARGET_PSEUDO && optimize && ((reload_in_progress | reload_completed) == 0))\n+    {\n+      if (GET_CODE (operands[1]) == MEM && !rtx_equal_p (operands[0], operands[1]))\n+\toperands[1] = force_reg (GET_MODE (operands[1]), operands[1]);\n+\n+      if (GET_CODE (operands[2]) == MEM)\n+\toperands[2] = force_reg (GET_MODE (operands[2]), operands[2]);\n+    }\n+\n+  if (!ix86_binary_operator_ok (code, mode, operands))\n+    {\n+      /* If not optimizing, try to make a valid insn (optimize code previously did\n+\t this above to improve chances of CSE) */\n+\n+      if ((!TARGET_PSEUDO || !optimize)\n+\t  && ((reload_in_progress | reload_completed) == 0)\n+\t  && (GET_CODE (operands[1]) == MEM || GET_CODE (operands[2]) == MEM))\n+\t{\n+\t  modified = FALSE;\n+\t  if (GET_CODE (operands[1]) == MEM && !rtx_equal_p (operands[0], operands[1]))\n+\t    {\n+\t      operands[1] = force_reg (GET_MODE (operands[1]), operands[1]);\n+\t      modified = TRUE;\n+\t    }\n+\n+\t  if (GET_CODE (operands[2]) == MEM)\n+\t    {\n+\t      operands[2] = force_reg (GET_MODE (operands[2]), operands[2]);\n+\t      modified = TRUE;\n+\t    }\n+\n+\t  if (modified && !ix86_binary_operator_ok (code, mode, operands))\n+\t    return FALSE;\n+\t}\n+      else\n+\treturn FALSE;\n+    }\n+\n+  return TRUE;\n+}\n+\f\n+/* Return TRUE or FALSE depending on whether the binary operator meets the\n+   appropriate constraints.  */\n+\n+int\n+ix86_binary_operator_ok (code, mode, operands)\n+     enum rtx_code code;\n+     enum machine_mode mode;\n+     rtx operands[3];\n+{\n+  return TRUE;\n+}\n+\f\n+/* Attempt to expand a unary operator.  Make the expansion closer to the\n+   actual machine, then just general_operand, which will allow 2 separate\n+   memory references (one output, one input) in a single insn.  Return\n+   whether the insn fails, or succeeds.  */\n+\n+int\n+ix86_expand_unary_operator (code, mode, operands)\n+     enum rtx_code code;\n+     enum machine_mode mode;\n+     rtx operands[];\n+{\n+  rtx insn;\n+\n+  /* If optimizing, copy to regs to improve CSE */\n+  if (TARGET_PSEUDO\n+      && optimize\n+      && ((reload_in_progress | reload_completed) == 0)\n+      && GET_CODE (operands[1]) == MEM)\n+    {\n+      operands[1] = force_reg (GET_MODE (operands[1]), operands[1]);\n+    }\n+\n+  if (!ix86_unary_operator_ok (code, mode, operands))\n+    {\n+      if ((!TARGET_PSEUDO || !optimize)\n+\t  && ((reload_in_progress | reload_completed) == 0)\n+\t  && GET_CODE (operands[1]) == MEM)\n+\t{\n+\t  operands[1] = force_reg (GET_MODE (operands[1]), operands[1]);\n+\t  if (!ix86_unary_operator_ok (code, mode, operands))\n+\t    return FALSE;\n+\t}\n+      else\n+\treturn FALSE;\n+    }\n+\n+  return TRUE;\n+}\n+\f\n+/* Return TRUE or FALSE depending on whether the unary operator meets the\n+   appropriate constraints.  */\n+\n+int\n+ix86_unary_operator_ok (code, mode, operands)\n+     enum rtx_code code;\n+     enum machine_mode mode;\n+     rtx operands[2];\n+{\n+  return TRUE;\n+}\n+\n \f\n /* This function generates the assembly code for function entry.\n    FILE is an stdio stream to output the code to.\n@@ -1921,30 +2065,41 @@ legitimate_address_p (mode, addr, strict)\n \t}\n     }\n \n-  /* Validate displacement */\n+  /* Validate displacement\n+     Constant pool addresses must be handled special.  They are\n+     considered legitimate addresses, but only if not used with regs.\n+     When printed, the output routines know to print the reference with the\n+     PIC reg, even though the PIC reg doesn't appear in the RTL. */\n   if (disp)\n     {\n-      if (!CONSTANT_ADDRESS_P (disp))\n+      if (GET_CODE (disp) == SYMBOL_REF\n+\t  && CONSTANT_POOL_ADDRESS_P (disp)\n+\t  && !base\n+\t  && !indx)\n+\t;\n+\n+      else if (!CONSTANT_ADDRESS_P (disp))\n \t{\n \t  ADDR_INVALID (\"Displacement is not valid.\\n\", disp);\n \t  return FALSE;\n \t}\n \n-      if (GET_CODE (disp) == CONST_DOUBLE)\n+      else if (GET_CODE (disp) == CONST_DOUBLE)\n \t{\n \t  ADDR_INVALID (\"Displacement is a const_double.\\n\", disp);\n \t  return FALSE;\n \t}\n \n-      if (flag_pic && SYMBOLIC_CONST (disp) && base != pic_offset_table_rtx\n-\t  && (indx != pic_offset_table_rtx || scale != NULL_RTX))\n+      else if (flag_pic && SYMBOLIC_CONST (disp)\n+\t       && base != pic_offset_table_rtx\n+\t       && (indx != pic_offset_table_rtx || scale != NULL_RTX))\n \t{\n \t  ADDR_INVALID (\"Displacement is an invalid pic reference.\\n\", disp);\n \t  return FALSE;\n \t}\n \n-      if (HALF_PIC_P () && HALF_PIC_ADDRESS_P (disp)\n-\t  && (base != NULL_RTX || indx != NULL_RTX))\n+      else if (HALF_PIC_P () && HALF_PIC_ADDRESS_P (disp)\n+\t       && (base != NULL_RTX || indx != NULL_RTX))\n \t{\n \t  ADDR_INVALID (\"Displacement is an invalid half-pic reference.\\n\", disp);\n \t  return FALSE;\n@@ -2719,6 +2874,16 @@ notice_update_cc (exp)\n       /* Jumps do not alter the cc's.  */\n       if (SET_DEST (exp) == pc_rtx)\n \treturn;\n+#ifdef IS_STACK_MODE\n+      /* Moving into a memory of stack_mode may have been moved\n+         in between the use and set of cc0 by loop_spl(). So\n+         old value of cc.status must be retained */\n+      if(GET_CODE(SET_DEST(exp))==MEM \n+         && IS_STACK_MODE(GET_MODE(SET_DEST(exp))))\n+        {\n+          return;\n+        }\n+#endif\n       /* Moving register or memory into a register:\n \t it doesn't alter the cc's, but it might invalidate\n \t the RTX's which we remember the cc's came from.\n@@ -3099,6 +3264,15 @@ output_float_compare (insn, operands)\n   rtx body = XVECEXP (PATTERN (insn), 0, 0);\n   int unordered_compare = GET_MODE (SET_SRC (body)) == CCFPEQmode;\n \n+  rtx tmp;\n+  if (! STACK_TOP_P (operands[0]))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[1];\n+      operands[1] = tmp;\n+      cc_status.flags |= CC_REVERSED;\n+    }\n+    \n   if (! STACK_TOP_P (operands[0]))\n     abort ();\n \n@@ -3166,7 +3340,35 @@ output_fp_cc0_set (insn)\n   output_asm_insn (AS1 (fnsts%W0,%0), xops);\n \n   if (! TARGET_IEEE_FP)\n-    return \"sahf\";\n+    {\n+      if (!(cc_status.flags & CC_REVERSED))\n+        {\n+          next = next_cc0_user (insn);\n+        \n+          if (GET_CODE (next) == JUMP_INSN\n+              && GET_CODE (PATTERN (next)) == SET\n+              && SET_DEST (PATTERN (next)) == pc_rtx\n+              && GET_CODE (SET_SRC (PATTERN (next))) == IF_THEN_ELSE)\n+            {\n+              code = GET_CODE (XEXP (SET_SRC (PATTERN (next)), 0));\n+            }\n+          else if (GET_CODE (PATTERN (next)) == SET)\n+            {\n+              code = GET_CODE (SET_SRC (PATTERN (next)));\n+            }\n+          else\n+            {\n+              return \"sahf\";\n+            }\n+          if (code == GT || code == LT || code == EQ || code == NE\n+              || code == LE || code == GE)\n+            { /* We will test eax directly */\n+              cc_status.flags |= CC_TEST_AX;\n+              RET;\n+            }\n+        }\n+      return \"sahf\";\n+    }\n \n   next = next_cc0_user (insn);\n   if (next == NULL_RTX)\n@@ -3317,6 +3519,737 @@ assign_386_stack_local (mode, n)\n \n   return i386_stack_locals[(int) mode][n];\n }\n+\n+\f\n+int is_mul(op,mode)\n+    register rtx op;\n+    enum machine_mode mode;\n+{\n+  return (GET_CODE (op) == MULT);\n+}\n+\n+int is_div(op,mode)\n+    register rtx op;\n+    enum machine_mode mode;\n+{\n+  return (GET_CODE (op) == DIV);\n+}\n+\n+\f\n+#ifdef NOTYET\n+/* Create a new copy of an rtx.\n+   Recursively copies the operands of the rtx,\n+   except for those few rtx codes that are sharable.\n+   Doesn't share CONST  */\n+\n+rtx\n+copy_all_rtx (orig)\n+     register rtx orig;\n+{\n+  register rtx copy;\n+  register int i, j;\n+  register RTX_CODE code;\n+  register char *format_ptr;\n+\n+  code = GET_CODE (orig);\n+\n+  switch (code)\n+    {\n+    case REG:\n+    case QUEUED:\n+    case CONST_INT:\n+    case CONST_DOUBLE:\n+    case SYMBOL_REF:\n+    case CODE_LABEL:\n+    case PC:\n+    case CC0:\n+    case SCRATCH:\n+      /* SCRATCH must be shared because they represent distinct values. */\n+      return orig;\n+\n+#if 0\n+    case CONST:\n+      /* CONST can be shared if it contains a SYMBOL_REF.  If it contains\n+\t a LABEL_REF, it isn't sharable.  */\n+      if (GET_CODE (XEXP (orig, 0)) == PLUS\n+\t  && GET_CODE (XEXP (XEXP (orig, 0), 0)) == SYMBOL_REF\n+\t  && GET_CODE (XEXP (XEXP (orig, 0), 1)) == CONST_INT)\n+\treturn orig;\n+      break;\n+#endif\n+      /* A MEM with a constant address is not sharable.  The problem is that\n+\t the constant address may need to be reloaded.  If the mem is shared,\n+\t then reloading one copy of this mem will cause all copies to appear\n+\t to have been reloaded.  */\n+    }\n+\n+  copy = rtx_alloc (code);\n+  PUT_MODE (copy, GET_MODE (orig));\n+  copy->in_struct = orig->in_struct;\n+  copy->volatil = orig->volatil;\n+  copy->unchanging = orig->unchanging;\n+  copy->integrated = orig->integrated;\n+  /* intel1 */\n+  copy->is_spill_rtx = orig->is_spill_rtx;\n+  \n+  format_ptr = GET_RTX_FORMAT (GET_CODE (copy));\n+\n+  for (i = 0; i < GET_RTX_LENGTH (GET_CODE (copy)); i++)\n+    {\n+      switch (*format_ptr++)\n+\t{\n+\tcase 'e':\n+\t  XEXP (copy, i) = XEXP (orig, i);\n+\t  if (XEXP (orig, i) != NULL)\n+\t    XEXP (copy, i) = copy_rtx (XEXP (orig, i));\n+\t  break;\n+\n+\tcase '0':\n+\tcase 'u':\n+\t  XEXP (copy, i) = XEXP (orig, i);\n+\t  break;\n+\n+\tcase 'E':\n+\tcase 'V':\n+\t  XVEC (copy, i) = XVEC (orig, i);\n+\t  if (XVEC (orig, i) != NULL)\n+\t    {\n+\t      XVEC (copy, i) = rtvec_alloc (XVECLEN (orig, i));\n+\t      for (j = 0; j < XVECLEN (copy, i); j++)\n+\t\tXVECEXP (copy, i, j) = copy_rtx (XVECEXP (orig, i, j));\n+\t    }\n+\t  break;\n+\n+\tcase 'w':\n+\t  XWINT (copy, i) = XWINT (orig, i);\n+\t  break;\n+\n+\tcase 'i':\n+\t  XINT (copy, i) = XINT (orig, i);\n+\t  break;\n+\n+\tcase 's':\n+\tcase 'S':\n+\t  XSTR (copy, i) = XSTR (orig, i);\n+\t  break;\n+\n+\tdefault:\n+\t  abort ();\n+\t}\n+    }\n+  return copy;\n+}\n+\n+\f\n+/* try to rewrite a memory address to make it valid */\n+void \n+rewrite_address (mem_rtx)\n+     rtx mem_rtx;\n+{\n+  rtx index_rtx, base_rtx, offset_rtx, scale_rtx, ret_rtx;\n+  int scale = 1;\n+  int offset_adjust = 0;\n+  int was_only_offset = 0;\n+  rtx mem_addr = XEXP (mem_rtx, 0);\n+  char *storage = (char *) oballoc (0);\n+  int in_struct = 0;\n+  int is_spill_rtx = 0;\n+\n+  in_struct = MEM_IN_STRUCT_P (mem_rtx);\n+  is_spill_rtx = RTX_IS_SPILL_P (mem_rtx);\n+\n+  if (GET_CODE (mem_addr) == PLUS &&\n+      GET_CODE (XEXP (mem_addr, 1)) == PLUS &&\n+      GET_CODE (XEXP (XEXP (mem_addr, 1), 0)) == REG)\n+    {\t\t\t\t/* this part is utilized by the combiner */\n+      ret_rtx =\n+\tgen_rtx (PLUS, GET_MODE (mem_addr),\n+\t\t gen_rtx (PLUS, GET_MODE (XEXP (mem_addr, 1)),\n+\t\t\t  XEXP (mem_addr, 0),\n+\t\t\t  XEXP (XEXP (mem_addr, 1), 0)),\n+\t\t XEXP (XEXP (mem_addr, 1), 1));\n+      if (memory_address_p (GET_MODE (mem_rtx), ret_rtx))\n+\t{\n+\t  XEXP (mem_rtx, 0) = ret_rtx;\n+\t  RTX_IS_SPILL_P (ret_rtx) = is_spill_rtx;\n+\t  return;\n+\t}\n+      obfree (storage);\n+    }\n+\n+  /* this part is utilized by loop.c */\n+  /* If the address contains PLUS (reg,const) and this pattern is invalid\n+     in this case - try to rewrite the address to make it valid  intel1\n+  */\n+  storage = (char *) oballoc (0);\n+  index_rtx = base_rtx = offset_rtx = NULL;\n+  /* find the base index and offset elements of the memory address */\n+  if (GET_CODE (mem_addr) == PLUS)\n+    {\n+      if (GET_CODE (XEXP (mem_addr, 0)) == REG)\n+\t{\n+\t  if (GET_CODE (XEXP (mem_addr, 1)) == REG)\n+\t    {\n+\t      base_rtx = XEXP (mem_addr, 1);\n+\t      index_rtx = XEXP (mem_addr, 0);\n+\t    }\n+\t  else\n+\t    {\n+\t      base_rtx = XEXP (mem_addr, 0);\n+\t      offset_rtx = XEXP (mem_addr, 1);\n+\t    }\n+\t}\n+      else if (GET_CODE (XEXP (mem_addr, 0)) == MULT)\n+\t{\n+\t  index_rtx = XEXP (mem_addr, 0);\n+\t  if (GET_CODE (XEXP (mem_addr, 1)) == REG)\n+\t    {\n+\t      base_rtx = XEXP (mem_addr, 1);\n+\t    }\n+\t  else\n+\t    {\n+\t      offset_rtx = XEXP (mem_addr, 1);\n+\t    }\n+\t}\n+      else if (GET_CODE (XEXP (mem_addr, 0)) == PLUS)\n+\t{\n+\t  /* intel1 */\n+\t  if (GET_CODE (XEXP (XEXP (mem_addr, 0), 0)) == PLUS &&\n+\t      GET_CODE (XEXP (XEXP (XEXP (mem_addr, 0), 0), 0)) == MULT &&\n+\t      GET_CODE (XEXP (XEXP (XEXP (XEXP (mem_addr, 0), 0), 0), 0)) == REG &&\n+\t      GET_CODE (XEXP (XEXP (XEXP (XEXP (mem_addr, 0), 0), 0), 1)) == CONST_INT &&\n+\t      GET_CODE (XEXP (XEXP (XEXP (mem_addr, 0), 0), 1)) == CONST_INT &&\n+\t      GET_CODE (XEXP (XEXP (mem_addr, 0), 1)) == REG &&\n+\t      GET_CODE (XEXP (mem_addr, 1)) == SYMBOL_REF)\n+\t    {\n+\t      index_rtx = XEXP (XEXP (XEXP (mem_addr, 0), 0), 0);\n+\t      offset_rtx = XEXP (mem_addr, 1);\n+\t      base_rtx = XEXP (XEXP (mem_addr, 0), 1);\n+\t      offset_adjust = INTVAL (XEXP (XEXP (XEXP (mem_addr, 0), 0), 1));\n+\t    }\n+\t  else\n+\t    {\n+\t      offset_rtx = XEXP (mem_addr, 1);\n+\t      index_rtx = XEXP (XEXP (mem_addr, 0), 0);\n+\t      base_rtx = XEXP (XEXP (mem_addr, 0), 1);\n+\t    }\n+\t}\n+      else if (GET_CODE (XEXP (mem_addr, 0)) == CONST_INT)\n+\t{\n+\t  was_only_offset = 1;\n+\t  index_rtx = NULL;\n+\t  base_rtx = NULL;\n+\t  offset_rtx = XEXP (mem_addr, 1);\n+\t  offset_adjust = INTVAL (XEXP (mem_addr, 0));\n+\t  if (offset_adjust == 0)\n+\t    {\n+\t      XEXP (mem_rtx, 0) = offset_rtx;\n+\t      RTX_IS_SPILL_P (XEXP (mem_rtx, 0)) = is_spill_rtx;\n+\t      return;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  obfree (storage);\n+\t  return;\n+\t}\n+    }\n+  else if (GET_CODE (mem_addr) == MULT)\n+    {\n+      index_rtx = mem_addr;\n+    }\n+  else\n+    {\n+      obfree (storage);\n+      return;\n+    }\n+  if (index_rtx && GET_CODE (index_rtx) == MULT)\n+    {\n+      if (GET_CODE (XEXP (index_rtx, 1)) != CONST_INT)\n+\t{\n+\t  obfree (storage);\n+\t  return;\n+\t}\n+      scale_rtx = XEXP (index_rtx, 1);\n+      scale = INTVAL (scale_rtx);\n+      index_rtx = copy_all_rtx (XEXP (index_rtx, 0));\n+    }\n+  /* now find which of the elements are invalid and try to fix them */\n+  if (index_rtx && GET_CODE (index_rtx) == CONST_INT && base_rtx == NULL)\n+    {\n+      offset_adjust = INTVAL (index_rtx) * scale;\n+      if (offset_rtx && GET_CODE (offset_rtx) == CONST &&\n+\t  GET_CODE (XEXP (offset_rtx, 0)) == PLUS)\n+\t{\n+\t  if (GET_CODE (XEXP (XEXP (offset_rtx, 0), 0)) == SYMBOL_REF &&\n+\t      GET_CODE (XEXP (XEXP (offset_rtx, 0), 1)) == CONST_INT)\n+\t    {\n+\t      offset_rtx = copy_all_rtx (offset_rtx);\n+\t      XEXP (XEXP (offset_rtx, 0), 1) =\n+\t\tgen_rtx (CONST_INT, 0, INTVAL (XEXP (XEXP (offset_rtx, 0), 1)) + offset_adjust);\n+\t      if (!CONSTANT_P (offset_rtx))\n+\t\t{\n+\t\t  obfree (storage);\n+\t\t  return;\n+\t\t}\n+\t    }\n+\t}\n+      else if (offset_rtx && GET_CODE (offset_rtx) == SYMBOL_REF)\n+\t{\n+\t  offset_rtx =\n+\t    gen_rtx (CONST, GET_MODE (offset_rtx),\n+\t\t     gen_rtx (PLUS, GET_MODE (offset_rtx),\n+\t\t\t      offset_rtx,\n+\t\t\t      gen_rtx (CONST_INT, 0, offset_adjust)));\n+\t  if (!CONSTANT_P (offset_rtx))\n+\t    {\n+\t      obfree (storage);\n+\t      return;\n+\t    }\n+\t}\n+      else if (offset_rtx && GET_CODE (offset_rtx) == CONST_INT)\n+\t{\n+\t  offset_rtx = gen_rtx (CONST_INT, 0, INTVAL (offset_rtx) + offset_adjust);\n+\t}\n+      else if (!offset_rtx)\n+\t{\n+\t  offset_rtx = gen_rtx (CONST_INT, 0, 0);\n+\t}\n+      RTX_IS_SPILL_P (XEXP (mem_rtx, 0)) = is_spill_rtx;\n+      XEXP (mem_rtx, 0) = offset_rtx;\n+      return;\n+    }\n+  if (base_rtx && GET_CODE (base_rtx) == PLUS &&\n+      GET_CODE (XEXP (base_rtx, 0)) == REG &&\n+      GET_CODE (XEXP (base_rtx, 1)) == CONST_INT)\n+    {\n+      offset_adjust += INTVAL (XEXP (base_rtx, 1));\n+      base_rtx = copy_all_rtx (XEXP (base_rtx, 0));\n+    }\n+  else if (base_rtx && GET_CODE (base_rtx) == CONST_INT)\n+    {\n+      offset_adjust += INTVAL (base_rtx);\n+      base_rtx = NULL;\n+    }\n+  if (index_rtx && GET_CODE (index_rtx) == PLUS &&\n+      GET_CODE (XEXP (index_rtx, 0)) == REG &&\n+      GET_CODE (XEXP (index_rtx, 1)) == CONST_INT)\n+    {\n+      offset_adjust += INTVAL (XEXP (index_rtx, 1)) * scale;\n+      index_rtx = copy_all_rtx (XEXP (index_rtx, 0));\n+    }\n+  if (index_rtx)\n+    {\n+      if (!LEGITIMATE_INDEX_P (index_rtx)\n+      && !(index_rtx == stack_pointer_rtx && scale == 1 && base_rtx == NULL))\n+\t{\n+\t  obfree (storage);\n+\t  return;\n+\t}\n+    }\n+  if (base_rtx)\n+    {\n+      if (!LEGITIMATE_INDEX_P (base_rtx) && GET_CODE (base_rtx) != REG)\n+\t{\n+\t  obfree (storage);\n+\t  return;\n+\t}\n+    }\n+  if (offset_adjust != 0)\n+    {\n+      if (offset_rtx)\n+\t{\n+\t  if (GET_CODE (offset_rtx) == CONST &&\n+\t      GET_CODE (XEXP (offset_rtx, 0)) == PLUS)\n+\t    {\n+\t      if (GET_CODE (XEXP (XEXP (offset_rtx, 0), 0)) == SYMBOL_REF &&\n+\t\t  GET_CODE (XEXP (XEXP (offset_rtx, 0), 1)) == CONST_INT)\n+\t\t{\n+\t\t  offset_rtx = copy_all_rtx (offset_rtx);\n+\t\t  XEXP (XEXP (offset_rtx, 0), 1) =\n+\t\t    gen_rtx (CONST_INT, 0, INTVAL (XEXP (XEXP (offset_rtx, 0), 1)) + offset_adjust);\n+\t\t  if (!CONSTANT_P (offset_rtx))\n+\t\t    {\n+\t\t      obfree (storage);\n+\t\t      return;\n+\t\t    }\n+\t\t}\n+\t    }\n+\t  else if (GET_CODE (offset_rtx) == SYMBOL_REF)\n+\t    {\n+\t      offset_rtx =\n+\t\tgen_rtx (CONST, GET_MODE (offset_rtx),\n+\t\t\t gen_rtx (PLUS, GET_MODE (offset_rtx),\n+\t\t\t\t  offset_rtx,\n+\t\t\t\t  gen_rtx (CONST_INT, 0, offset_adjust)));\n+\t      if (!CONSTANT_P (offset_rtx))\n+\t\t{\n+\t\t  obfree (storage);\n+\t\t  return;\n+\t\t}\n+\t    }\n+\t  else if (GET_CODE (offset_rtx) == CONST_INT)\n+\t    {\n+\t      offset_rtx = gen_rtx (CONST_INT, 0, INTVAL (offset_rtx) + offset_adjust);\n+\t    }\n+\t  else\n+\t    {\n+\t      obfree (storage);\n+\t      return;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  offset_rtx = gen_rtx (CONST_INT, 0, offset_adjust);\n+\t}\n+      if (index_rtx)\n+\t{\n+\t  if (base_rtx)\n+\t    {\n+\t      if (scale != 1)\n+\t\t{\n+\t\t  if (GET_CODE (offset_rtx) == CONST_INT &&\n+\t\t      INTVAL (offset_rtx) == 0)\n+\t\t    {\n+\t\t      ret_rtx = gen_rtx (PLUS, GET_MODE (base_rtx),\n+\t\t\t     gen_rtx (MULT, GET_MODE (index_rtx), index_rtx,\n+\t\t\t\t      scale_rtx),\n+\t\t\t\t\t base_rtx);\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      ret_rtx = gen_rtx (PLUS, GET_MODE (offset_rtx),\n+\t\t\t\t\t gen_rtx (PLUS, GET_MODE (base_rtx),\n+\t\t\t     gen_rtx (MULT, GET_MODE (index_rtx), index_rtx,\n+\t\t\t\t      scale_rtx),\n+\t\t\t\t\t\t  base_rtx),\n+\t\t\t\t\t offset_rtx);\n+\t\t    }\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  if (GET_CODE (offset_rtx) == CONST_INT &&\n+\t\t      INTVAL (offset_rtx) == 0)\n+\t\t    {\n+\t\t      ret_rtx = gen_rtx (PLUS, GET_MODE (index_rtx), index_rtx, base_rtx);\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      ret_rtx = gen_rtx (PLUS, GET_MODE (offset_rtx),\n+\t\t\t     gen_rtx (PLUS, GET_MODE (index_rtx), index_rtx,\n+\t\t\t\t      base_rtx),\n+\t\t\t\t\t offset_rtx);\n+\t\t    }\n+\t\t}\n+\t    }\n+\t  else\n+\t    {\n+\t      if (scale != 1)\n+\t\t{\n+\t\t  if (GET_CODE (offset_rtx) == CONST_INT &&\n+\t\t      INTVAL (offset_rtx) == 0)\n+\t\t    {\n+\t\t      ret_rtx = gen_rtx (MULT, GET_MODE (index_rtx), index_rtx, scale_rtx);\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      ret_rtx =\n+\t\t\tgen_rtx (PLUS, GET_MODE (offset_rtx),\n+\t\t\t     gen_rtx (MULT, GET_MODE (index_rtx), index_rtx,\n+\t\t\t\t      scale_rtx),\n+\t\t\t\t offset_rtx);\n+\t\t    }\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  if (GET_CODE (offset_rtx) == CONST_INT &&\n+\t\t      INTVAL (offset_rtx) == 0)\n+\t\t    {\n+\t\t      ret_rtx = index_rtx;\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      ret_rtx = gen_rtx (PLUS, GET_MODE (index_rtx), index_rtx, offset_rtx);\n+\t\t    }\n+\t\t}\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  if (base_rtx)\n+\t    {\n+\t      if (GET_CODE (offset_rtx) == CONST_INT &&\n+\t\t  INTVAL (offset_rtx) == 0)\n+\t\t{\n+\t\t  ret_rtx = base_rtx;\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  ret_rtx = gen_rtx (PLUS, GET_MODE (base_rtx), base_rtx, offset_rtx);\n+\t\t}\n+\t    }\n+\t  else if (was_only_offset)\n+\t    {\n+\t      ret_rtx = offset_rtx;\n+\t    }\n+\t  else\n+\t    {\n+\t      obfree (storage);\n+\t      return;\n+\t    }\n+\t}\n+      XEXP (mem_rtx, 0) = ret_rtx;\n+      RTX_IS_SPILL_P (XEXP (mem_rtx, 0)) = is_spill_rtx;\n+      return;\n+    }\n+  else\n+    {\n+      obfree (storage);\n+      return;\n+    }\n+}\n+#endif /* NOTYET */\n+\n+\f\n+/* return 1 if the first insn to set cc before insn also sets the register\n+   reg_rtx - otherwise return 0 */\n+int\n+last_to_set_cc (reg_rtx, insn)\n+     rtx reg_rtx, insn;\n+{\n+  rtx prev_insn = PREV_INSN (insn);\n+\n+  while (prev_insn)\n+    {\n+      if (GET_CODE (prev_insn) == NOTE)\n+\t;\n+\n+      else if (GET_CODE (prev_insn) == INSN)\n+\t{\n+\t  if (GET_CODE (PATTERN (prev_insn)) != SET)\n+\t    return (0);\n+\n+\t  if (rtx_equal_p (SET_DEST (PATTERN (prev_insn)), reg_rtx))\n+\t    {\n+\t      if (sets_condition_code (SET_SRC (PATTERN (prev_insn))))\n+\t\treturn (1);\n+\n+\t      return (0);\n+\t    }\n+\n+\t  else if (!doesnt_set_condition_code (SET_SRC (PATTERN (prev_insn))))\n+\t    return (0);\n+\t}\n+\n+      else\n+\treturn (0);\n+\n+      prev_insn = PREV_INSN (prev_insn);\n+    }\n+\n+  return (0);\n+}\n+\n+\f\n+int\n+doesnt_set_condition_code (pat)\n+     rtx pat;\n+{\n+  switch (GET_CODE (pat))\n+    {\n+    case MEM:\n+    case REG:\n+      return (1);\n+\n+    default:\n+      return (0);\n+\n+    }\n+}\n+\n+\f\n+int\n+sets_condition_code (pat)\n+     rtx pat;\n+{\n+  switch (GET_CODE (pat))\n+    {\n+    case PLUS:\n+    case MINUS:\n+    case AND:\n+    case IOR:\n+    case XOR:\n+    case NOT:\n+    case NEG:\n+    case MULT:\n+    case DIV:\n+    case MOD:\n+    case UDIV:\n+    case UMOD:\n+      return (1);\n+\n+    default:\n+      return (0);\n+\n+    }\n+}\n+\n+\f\n+int\n+str_immediate_operand (op, mode)\n+     register rtx op;\n+     enum machine_mode mode;\n+{\n+  if (GET_CODE (op) == CONST_INT && INTVAL (op) <= 32 && INTVAL (op) >= 0)\n+    {\n+      return (1);\n+    }\n+  return (0);\n+}\n+\n+\f\n+int\n+is_fp_insn (insn)\n+     rtx insn;\n+{\n+  if (GET_CODE (insn) == INSN && GET_CODE (PATTERN (insn)) == SET\n+      && (GET_MODE (SET_DEST (PATTERN (insn))) == DFmode\n+\t  || GET_MODE (SET_DEST (PATTERN (insn))) == SFmode\n+\t  || GET_MODE (SET_DEST (PATTERN (insn))) == XFmode))\n+    {\n+      return (1);\n+    }\n+\n+  return (0);\n+}\n+\n+/*\n+  Return 1 if the mode of the SET_DEST of insn is floating point\n+  and it is not an fld or a move from memory to memory.\n+  Otherwise return 0 */\n+int\n+is_fp_dest (insn)\n+     rtx insn;\n+{\n+  if (GET_CODE (insn) == INSN && GET_CODE (PATTERN (insn)) == SET\n+      && (GET_MODE (SET_DEST (PATTERN (insn))) == DFmode\n+\t  || GET_MODE (SET_DEST (PATTERN (insn))) == SFmode\n+\t  || GET_MODE (SET_DEST (PATTERN (insn))) == XFmode)\n+      && GET_CODE (SET_DEST (PATTERN (insn))) == REG\n+      && REGNO (SET_DEST (PATTERN (insn))) >= FIRST_FLOAT_REG\n+      && GET_CODE (SET_SRC (insn)) != MEM)\n+    {\n+      return (1);\n+    }\n+\n+  return (0);\n+}\n+\n+/*\n+  Return 1 if the mode of the SET_DEST floating point and is memory\n+  and the source is a register.  \n+*/\n+int\n+is_fp_store (insn)\n+     rtx insn;\n+{\n+  if (GET_CODE (insn) == INSN && GET_CODE (PATTERN (insn)) == SET\n+      && (GET_MODE (SET_DEST (PATTERN (insn))) == DFmode\n+\t  || GET_MODE (SET_DEST (PATTERN (insn))) == SFmode\n+\t  || GET_MODE (SET_DEST (PATTERN (insn))) == XFmode)\n+      && GET_CODE (SET_DEST (PATTERN (insn))) == MEM\n+      && GET_CODE (SET_SRC (PATTERN (insn))) == REG)\n+    {\n+      return (1);\n+    }\n+\n+  return (0);\n+}\n+\n+\f\n+/*\n+  Return 1 if dep_insn sets a register which insn uses as a base\n+  or index to reference memory.\n+  otherwise return 0 */\n+\n+int\n+agi_dependent (insn, dep_insn)\n+     rtx insn, dep_insn;\n+{\n+  if (GET_CODE (dep_insn) == INSN\n+      && GET_CODE (PATTERN (dep_insn)) == SET\n+      && GET_CODE (SET_DEST (PATTERN (dep_insn))) == REG)\n+    {\n+      return (reg_mentioned_in_mem (SET_DEST (PATTERN (dep_insn)), insn));\n+    }\n+\n+  if (GET_CODE (dep_insn) == INSN && GET_CODE (PATTERN (dep_insn)) == SET\n+      && GET_CODE (SET_DEST (PATTERN (dep_insn))) == MEM\n+      && push_operand (SET_DEST (PATTERN (dep_insn)),\n+                       GET_MODE (SET_DEST (PATTERN (dep_insn)))))\n+    {\n+      return (reg_mentioned_in_mem (stack_pointer_rtx, insn));\n+    }\n+  \n+  return (0);\n+}\n+\n+\f\n+/*\n+  Return 1 if reg is used in rtl as a base or index for a memory ref\n+  otherwise return 0. */\n+\n+int\n+reg_mentioned_in_mem (reg, rtl)\n+     rtx reg, rtl;\n+{\n+  register char *fmt;\n+  register int i;\n+  register enum rtx_code code;\n+\n+  if (rtl == NULL)\n+    return (0);\n+\n+  code = GET_CODE (rtl);\n+\n+  switch (code)\n+    {\n+    case HIGH:\n+    case CONST_INT:\n+    case CONST:\n+    case CONST_DOUBLE:\n+    case SYMBOL_REF:\n+    case LABEL_REF:\n+    case PC:\n+    case CC0:\n+    case SUBREG:\n+      return (0);\n+\n+\n+    }\n+\n+  if (code == MEM && reg_mentioned_p (reg, rtl))\n+    return (1);\n+\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'E')\n+\t{\n+\t  register int j;\n+\t  for (j = XVECLEN (rtl, i) - 1; j >= 0; j--)\n+\t    {\n+\t      if (reg_mentioned_in_mem (reg, XVECEXP (rtl, i, j)))\n+\t\treturn 1;\n+\t    }\n+\t}\n+\n+      else if (fmt[i] == 'e' && reg_mentioned_in_mem (reg, XEXP (rtl, i)))\n+\treturn 1;\n+    }\n+\n+  return (0);\n+}\n \f\n /* Output the approprate insns for doing strlen if not just doing repnz; scasb\n "}]}