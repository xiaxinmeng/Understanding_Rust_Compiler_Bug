{"sha": "9a5cee0228a8b8d639e29682c800fe251175ce62", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OWE1Y2VlMDIyOGE4YjhkNjM5ZTI5NjgyYzgwMGZlMjUxMTc1Y2U2Mg==", "commit": {"author": {"name": "H.J. Lu", "email": "hongjiu.lu@intel.com", "date": "2007-05-22T14:37:19Z"}, "committer": {"name": "H.J. Lu", "email": "hjl@gcc.gnu.org", "date": "2007-05-22T14:37:19Z"}, "message": "config.gcc (i[34567]86-*-*): Add smmintrin.h to extra_headers.\n\n2007-05-22  H.J. Lu  <hongjiu.lu@intel.com>\n\t    Richard Henderson  <rth@redhat.com>\n\n\t* config.gcc (i[34567]86-*-*): Add smmintrin.h to\n\textra_headers.\n\t(x86_64-*-*): Likewise.\n\n\t* i386/i386-modes.def (V2QI): New.\n\n\t* config/i386/i386.c (ix86_handle_option): Handle SSE4.1 and\n\tSSE4A.\n\t(override_options): Support SSE4.1.\n\t(IX86_BUILTIN_BLENDPD): New for SSE4.1.\n\t(IX86_BUILTIN_BLENDPS): Likewise.\n\t(IX86_BUILTIN_BLENDVPD): Likewise.\n\t(IX86_BUILTIN_BLENDVPS): Likewise.\n\t(IX86_BUILTIN_PBLENDVB128): Likewise.\n\t(IX86_BUILTIN_PBLENDW128): Likewise.\n\t(IX86_BUILTIN_DPPD): Likewise.\n\t(IX86_BUILTIN_DPPS): Likewise.\n\t(IX86_BUILTIN_INSERTPS128): Likewise.\n\t(IX86_BUILTIN_MOVNTDQA): Likewise.\n\t(IX86_BUILTIN_MPSADBW128): Likewise.\n\t(IX86_BUILTIN_PACKUSDW128): Likewise.\n\t(IX86_BUILTIN_PCMPEQQ): Likewise.\n\t(IX86_BUILTIN_PHMINPOSUW128): Likewise.\n\t(IX86_BUILTIN_PMAXSB128): Likewise.\n\t(IX86_BUILTIN_PMAXSD128): Likewise.\n\t(IX86_BUILTIN_PMAXUD128): Likewise.\n\t(IX86_BUILTIN_PMAXUW128): Likewise.\n\t(IX86_BUILTIN_PMINSB128): Likewise.\n\t(IX86_BUILTIN_PMINSD128): Likewise.\n\t(IX86_BUILTIN_PMINUD128): Likewise.\n\t(IX86_BUILTIN_PMINUW128): Likewise.\n\t(IX86_BUILTIN_PMOVSXBW128): Likewise.\n\t(IX86_BUILTIN_PMOVSXBD128): Likewise.\n\t(IX86_BUILTIN_PMOVSXBQ128): Likewise.\n\t(IX86_BUILTIN_PMOVSXWD128): Likewise.\n\t(IX86_BUILTIN_PMOVSXWQ128): Likewise.\n\t(IX86_BUILTIN_PMOVSXDQ128): Likewise.\n\t(IX86_BUILTIN_PMOVZXBW128): Likewise.\n\t(IX86_BUILTIN_PMOVZXBD128): Likewise.\n\t(IX86_BUILTIN_PMOVZXBQ128): Likewise.\n\t(IX86_BUILTIN_PMOVZXWD128): Likewise.\n\t(IX86_BUILTIN_PMOVZXWQ128): Likewise.\n\t(IX86_BUILTIN_PMOVZXDQ128): Likewise.\n\t(IX86_BUILTIN_PMULDQ128): Likewise.\n\t(IX86_BUILTIN_PMULLD128): Likewise.\n\t(IX86_BUILTIN_ROUNDPD): Likewise.\n\t(IX86_BUILTIN_ROUNDPS): Likewise.\n\t(IX86_BUILTIN_ROUNDSD): Likewise.\n\t(IX86_BUILTIN_ROUNDSS): Likewise.\n\t(IX86_BUILTIN_PTESTZ): Likewise.\n\t(IX86_BUILTIN_PTESTC): Likewise.\n\t(IX86_BUILTIN_PTESTNZC): Likewise.\n\t(IX86_BUILTIN_VEC_EXT_V16QI): Likewise.\n\t(IX86_BUILTIN_VEC_SET_V2DI): Likewise.\n\t(IX86_BUILTIN_VEC_SET_V4SF): Likewise.\n\t(IX86_BUILTIN_VEC_SET_V4SI): Likewise.\n\t(IX86_BUILTIN_VEC_SET_V16QI): Likewise.\n\t(bdesc_ptest): New.\n\t(bdesc_sse_3arg): Likewise.\n\t(bdesc_2arg): Likewise.\n\t(bdesc_1arg): Likewise.\n\t(ix86_init_mmx_sse_builtins): Support SSE4.1.  Handle SSE builtins\n\twith 3 args.\n\t(ix86_expand_sse_4_operands_builtin): New.\n\t(ix86_expand_unop_builtin): Support 2 arg builtins with a constant\n\tsmaller than 8 bits as the 2nd arg.\n\t(ix86_expand_sse_ptest): New.\n\t(ix86_expand_builtin): Support SSE4.1. Support 3 arg SSE builtins.\n\t(ix86_expand_vector_set): Support SSE4.1.\n\t(ix86_expand_vector_extract): Likewise.\n\n\t* config/i386/i386.h (TARGET_CPU_CPP_BUILTINS): Define\n\t__SSE4_1__ for -msse4.1.\n\n\t* config/i386/i386.md (UNSPEC_BLENDV): New for SSE4.1.\n\t(UNSPEC_INSERTPS): Likewise.\n\t(UNSPEC_DP): Likewise.\n\t(UNSPEC_MOVNTDQA): Likewise.\n\t(UNSPEC_MPSADBW): Likewise.\n\t(UNSPEC_PHMINPOSUW): Likewise.\n\t(UNSPEC_PTEST): Likewise.\n\t(UNSPEC_ROUNDP): Likewise.\n\t(UNSPEC_ROUNDS): Likewise.\n\n\t* config/i386/i386.opt (msse4.1): New for SSE4.1.\n\n\t* config/i386/predicates.md (const_pow2_1_to_2_operand): New.\n\t(const_pow2_1_to_32768_operand): Likewise.\n\n\t* config/i386/smmintrin.h: New. The SSE4.1 intrinsic header\n\tfile.\n\n\t* config/i386/sse.md (*vec_setv4sf_sse4_1): New pattern for\n\tSSE4.1.\n\t(sse4_1_insertps): Likewise.\n\t(*sse4_1_extractps): Likewise.\n\t(sse4_1_ptest): Likewise.\n\t(sse4_1_mulv2siv2di3): Likewise.\n\t(*sse4_1_mulv4si3): Likewise.\n\t(*sse4_1_smax<mode>3): Likewise.\n\t(*sse4_1_umax<mode>3): Likewise.\n\t(*sse4_1_smin<mode>3): Likewise.\n\t(*sse4_1_umin<mode>3): Likewise.\n\t(sse4_1_eqv2di3): Likewise.\n\t(*sse4_1_pinsrb): Likewise.\n\t(*sse4_1_pinsrd): Likewise.\n\t(*sse4_1_pinsrq): Likewise.\n\t(*sse4_1_pextrb): Likewise.\n\t(*sse4_1_pextrb_memory): Likewise.\n\t(*sse4_1_pextrw_memory): Likewise.\n\t(*sse4_1_pextrq): Likewise.\n\t(sse4_1_blendpd): Likewise.\n\t(sse4_1_blendps): Likewise.\n\t(sse4_1_blendvpd): Likewise.\n\t(sse4_1_blendvps): Likewise.\n\t(sse4_1_dppd): Likewise.\n\t(sse4_1_dpps): Likewise.\n\t(sse4_1_movntdqa): Likewise.\n\t(sse4_1_mpsadbw): Likewise.\n\t(sse4_1_packusdw): Likewise.\n\t(sse4_1_pblendvb): Likewise.\n\t(sse4_1_pblendw): Likewise.\n\t(sse4_1_phminposuw): Likewise.\n\t(sse4_1_extendv8qiv8hi2): Likewise.\n\t(*sse4_1_extendv8qiv8hi2): Likewise.\n\t(sse4_1_extendv4qiv4si2): Likewise.\n\t(*sse4_1_extendv4qiv4si2): Likewise.\n\t(sse4_1_extendv2qiv2di2): Likewise.\n\t(*sse4_1_extendv2qiv2di2): Likewise.\n\t(sse4_1_extendv4hiv4si2): Likewise.\n\t(*sse4_1_extendv4hiv4si2): Likewise.\n\t(sse4_1_extendv2hiv2di2): Likewise.\n\t(*sse4_1_extendv2hiv2di2): Likewise.\n\t(sse4_1_extendv2siv2di2): Likewise.\n\t(*sse4_1_extendv2siv2di2): Likewise.\n\t(sse4_1_zero_extendv8qiv8hi2): Likewise.\n\t(*sse4_1_zero_extendv8qiv8hi2): Likewise.\n\t(sse4_1_zero_extendv4qiv4si2): Likewise.\n\t(*sse4_1_zero_extendv4qiv4si2): Likewise.\n\t(sse4_1_zero_extendv2qiv2di2): Likewise.\n\t(*sse4_1_zero_extendv2qiv2di2): Likewise.\n\t(sse4_1_zero_extendv4hiv4si2): Likewise.\n\t(*sse4_1_zero_extendv4hiv4si2): Likewise.\n\t(sse4_1_zero_extendv2hiv2di2): Likewise.\n\t(*sse4_1_zero_extendv2hiv2di2): Likewise.\n\t(sse4_1_zero_extendv2siv2di2): Likewise.\n\t(*sse4_1_zero_extendv2siv2di2): Likewise.\n\t(sse4_1_roundpd): Likewise.\n\t(sse4_1_roundps): Likewise.\n\t(sse4_1_roundsd): Likewise.\n\t(sse4_1_roundss): Likewise.\n\t(mulv4si3): Don't expand for SSE4.1.\n\t(smax<mode>3): Likewise.\n\t(umaxv4si3): Likewise.\n\t(uminv16qi3): Likewise.\n\t(umin<mode>3): Likewise.\n\t(umaxv8hi3): Rewrite.  Only enabled for SSE4.1.\n\n\t* doc/extend.texi: Document SSE4.1 built-in functions.\n\n\t* doc/invoke.texi: Document -msse4.1.\n\nCo-Authored-By: Richard Henderson <rth@redhat.com>\n\nFrom-SVN: r124945", "tree": {"sha": "2c9aaab1ef7877eb59417c50602fc058c38c90d6", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/2c9aaab1ef7877eb59417c50602fc058c38c90d6"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/9a5cee0228a8b8d639e29682c800fe251175ce62", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9a5cee0228a8b8d639e29682c800fe251175ce62", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9a5cee0228a8b8d639e29682c800fe251175ce62", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9a5cee0228a8b8d639e29682c800fe251175ce62/comments", "author": {"login": "hjl-tools", "id": 1072356, "node_id": "MDQ6VXNlcjEwNzIzNTY=", "avatar_url": "https://avatars.githubusercontent.com/u/1072356?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hjl-tools", "html_url": "https://github.com/hjl-tools", "followers_url": "https://api.github.com/users/hjl-tools/followers", "following_url": "https://api.github.com/users/hjl-tools/following{/other_user}", "gists_url": "https://api.github.com/users/hjl-tools/gists{/gist_id}", "starred_url": "https://api.github.com/users/hjl-tools/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hjl-tools/subscriptions", "organizations_url": "https://api.github.com/users/hjl-tools/orgs", "repos_url": "https://api.github.com/users/hjl-tools/repos", "events_url": "https://api.github.com/users/hjl-tools/events{/privacy}", "received_events_url": "https://api.github.com/users/hjl-tools/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "c099916d6224e9775b4a43969901ed8688f32e5b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c099916d6224e9775b4a43969901ed8688f32e5b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c099916d6224e9775b4a43969901ed8688f32e5b"}], "stats": {"total": 2396, "additions": 2295, "deletions": 101}, "files": [{"sha": "160684d6423b5ac993545069ab8e61448553380b", "filename": "gcc/ChangeLog", "status": "modified", "additions": 165, "deletions": 0, "changes": 165, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -1,3 +1,168 @@\n+2007-05-22  H.J. Lu  <hongjiu.lu@intel.com>\n+\t    Richard Henderson  <rth@redhat.com>\n+\n+\t* config.gcc (i[34567]86-*-*): Add smmintrin.h to\n+\textra_headers.\n+\t(x86_64-*-*): Likewise.\n+\n+\t* i386/i386-modes.def (V2QI): New.\n+\n+\t* config/i386/i386.c (ix86_handle_option): Handle SSE4.1 and\n+\tSSE4A.\n+\t(override_options): Support SSE4.1.\n+\t(IX86_BUILTIN_BLENDPD): New for SSE4.1.\n+\t(IX86_BUILTIN_BLENDPS): Likewise.\n+\t(IX86_BUILTIN_BLENDVPD): Likewise.\n+\t(IX86_BUILTIN_BLENDVPS): Likewise.\n+\t(IX86_BUILTIN_PBLENDVB128): Likewise.\n+\t(IX86_BUILTIN_PBLENDW128): Likewise.\n+\t(IX86_BUILTIN_DPPD): Likewise.\n+\t(IX86_BUILTIN_DPPS): Likewise.\n+\t(IX86_BUILTIN_INSERTPS128): Likewise.\n+\t(IX86_BUILTIN_MOVNTDQA): Likewise.\n+\t(IX86_BUILTIN_MPSADBW128): Likewise.\n+\t(IX86_BUILTIN_PACKUSDW128): Likewise.\n+\t(IX86_BUILTIN_PCMPEQQ): Likewise.\n+\t(IX86_BUILTIN_PHMINPOSUW128): Likewise.\n+\t(IX86_BUILTIN_PMAXSB128): Likewise.\n+\t(IX86_BUILTIN_PMAXSD128): Likewise.\n+\t(IX86_BUILTIN_PMAXUD128): Likewise.\n+\t(IX86_BUILTIN_PMAXUW128): Likewise.\n+\t(IX86_BUILTIN_PMINSB128): Likewise.\n+\t(IX86_BUILTIN_PMINSD128): Likewise.\n+\t(IX86_BUILTIN_PMINUD128): Likewise.\n+\t(IX86_BUILTIN_PMINUW128): Likewise.\n+\t(IX86_BUILTIN_PMOVSXBW128): Likewise.\n+\t(IX86_BUILTIN_PMOVSXBD128): Likewise.\n+\t(IX86_BUILTIN_PMOVSXBQ128): Likewise.\n+\t(IX86_BUILTIN_PMOVSXWD128): Likewise.\n+\t(IX86_BUILTIN_PMOVSXWQ128): Likewise.\n+\t(IX86_BUILTIN_PMOVSXDQ128): Likewise.\n+\t(IX86_BUILTIN_PMOVZXBW128): Likewise.\n+\t(IX86_BUILTIN_PMOVZXBD128): Likewise.\n+\t(IX86_BUILTIN_PMOVZXBQ128): Likewise.\n+\t(IX86_BUILTIN_PMOVZXWD128): Likewise.\n+\t(IX86_BUILTIN_PMOVZXWQ128): Likewise.\n+\t(IX86_BUILTIN_PMOVZXDQ128): Likewise.\n+\t(IX86_BUILTIN_PMULDQ128): Likewise.\n+\t(IX86_BUILTIN_PMULLD128): Likewise.\n+\t(IX86_BUILTIN_ROUNDPD): Likewise.\n+\t(IX86_BUILTIN_ROUNDPS): Likewise.\n+\t(IX86_BUILTIN_ROUNDSD): Likewise.\n+\t(IX86_BUILTIN_ROUNDSS): Likewise.\n+\t(IX86_BUILTIN_PTESTZ): Likewise.\n+\t(IX86_BUILTIN_PTESTC): Likewise.\n+\t(IX86_BUILTIN_PTESTNZC): Likewise.\n+\t(IX86_BUILTIN_VEC_EXT_V16QI): Likewise.\n+\t(IX86_BUILTIN_VEC_SET_V2DI): Likewise.\n+\t(IX86_BUILTIN_VEC_SET_V4SF): Likewise.\n+\t(IX86_BUILTIN_VEC_SET_V4SI): Likewise.\n+\t(IX86_BUILTIN_VEC_SET_V16QI): Likewise.\n+\t(bdesc_ptest): New.\n+\t(bdesc_sse_3arg): Likewise.\n+\t(bdesc_2arg): Likewise.\n+\t(bdesc_1arg): Likewise.\n+\t(ix86_init_mmx_sse_builtins): Support SSE4.1.  Handle SSE builtins\n+\twith 3 args.\n+\t(ix86_expand_sse_4_operands_builtin): New.\n+\t(ix86_expand_unop_builtin): Support 2 arg builtins with a constant\n+\tsmaller than 8 bits as the 2nd arg.\n+\t(ix86_expand_sse_ptest): New.\n+\t(ix86_expand_builtin): Support SSE4.1. Support 3 arg SSE builtins.\n+\t(ix86_expand_vector_set): Support SSE4.1.\n+\t(ix86_expand_vector_extract): Likewise.\n+\n+\t* config/i386/i386.h (TARGET_CPU_CPP_BUILTINS): Define\n+\t__SSE4_1__ for -msse4.1.\n+\n+\t* config/i386/i386.md (UNSPEC_BLENDV): New for SSE4.1.\n+\t(UNSPEC_INSERTPS): Likewise.\n+\t(UNSPEC_DP): Likewise.\n+\t(UNSPEC_MOVNTDQA): Likewise.\n+\t(UNSPEC_MPSADBW): Likewise.\n+\t(UNSPEC_PHMINPOSUW): Likewise.\n+\t(UNSPEC_PTEST): Likewise.\n+\t(UNSPEC_ROUNDP): Likewise.\n+\t(UNSPEC_ROUNDS): Likewise.\n+\n+\t* config/i386/i386.opt (msse4.1): New for SSE4.1.\n+\n+\t* config/i386/predicates.md (const_pow2_1_to_2_operand): New.\n+\t(const_pow2_1_to_32768_operand): Likewise.\n+\n+\t* config/i386/smmintrin.h: New. The SSE4.1 intrinsic header\n+\tfile.\n+\n+\t* config/i386/sse.md (*vec_setv4sf_sse4_1): New pattern for\n+\tSSE4.1.\n+\t(sse4_1_insertps): Likewise.\n+\t(*sse4_1_extractps): Likewise.\n+\t(sse4_1_ptest): Likewise.\n+\t(sse4_1_mulv2siv2di3): Likewise.\n+\t(*sse4_1_mulv4si3): Likewise.\n+\t(*sse4_1_smax<mode>3): Likewise.\n+\t(*sse4_1_umax<mode>3): Likewise.\n+\t(*sse4_1_smin<mode>3): Likewise.\n+\t(*sse4_1_umin<mode>3): Likewise.\n+\t(sse4_1_eqv2di3): Likewise.\n+\t(*sse4_1_pinsrb): Likewise.\n+\t(*sse4_1_pinsrd): Likewise.\n+\t(*sse4_1_pinsrq): Likewise.\n+\t(*sse4_1_pextrb): Likewise.\n+\t(*sse4_1_pextrb_memory): Likewise.\n+\t(*sse4_1_pextrw_memory): Likewise.\n+\t(*sse4_1_pextrq): Likewise.\n+\t(sse4_1_blendpd): Likewise.\n+\t(sse4_1_blendps): Likewise.\n+\t(sse4_1_blendvpd): Likewise.\n+\t(sse4_1_blendvps): Likewise.\n+\t(sse4_1_dppd): Likewise.\n+\t(sse4_1_dpps): Likewise.\n+\t(sse4_1_movntdqa): Likewise.\n+\t(sse4_1_mpsadbw): Likewise.\n+\t(sse4_1_packusdw): Likewise.\n+\t(sse4_1_pblendvb): Likewise.\n+\t(sse4_1_pblendw): Likewise.\n+\t(sse4_1_phminposuw): Likewise.\n+\t(sse4_1_extendv8qiv8hi2): Likewise.\n+\t(*sse4_1_extendv8qiv8hi2): Likewise.\n+\t(sse4_1_extendv4qiv4si2): Likewise.\n+\t(*sse4_1_extendv4qiv4si2): Likewise.\n+\t(sse4_1_extendv2qiv2di2): Likewise.\n+\t(*sse4_1_extendv2qiv2di2): Likewise.\n+\t(sse4_1_extendv4hiv4si2): Likewise.\n+\t(*sse4_1_extendv4hiv4si2): Likewise.\n+\t(sse4_1_extendv2hiv2di2): Likewise.\n+\t(*sse4_1_extendv2hiv2di2): Likewise.\n+\t(sse4_1_extendv2siv2di2): Likewise.\n+\t(*sse4_1_extendv2siv2di2): Likewise.\n+\t(sse4_1_zero_extendv8qiv8hi2): Likewise.\n+\t(*sse4_1_zero_extendv8qiv8hi2): Likewise.\n+\t(sse4_1_zero_extendv4qiv4si2): Likewise.\n+\t(*sse4_1_zero_extendv4qiv4si2): Likewise.\n+\t(sse4_1_zero_extendv2qiv2di2): Likewise.\n+\t(*sse4_1_zero_extendv2qiv2di2): Likewise.\n+\t(sse4_1_zero_extendv4hiv4si2): Likewise.\n+\t(*sse4_1_zero_extendv4hiv4si2): Likewise.\n+\t(sse4_1_zero_extendv2hiv2di2): Likewise.\n+\t(*sse4_1_zero_extendv2hiv2di2): Likewise.\n+\t(sse4_1_zero_extendv2siv2di2): Likewise.\n+\t(*sse4_1_zero_extendv2siv2di2): Likewise.\n+\t(sse4_1_roundpd): Likewise.\n+\t(sse4_1_roundps): Likewise.\n+\t(sse4_1_roundsd): Likewise.\n+\t(sse4_1_roundss): Likewise.\n+\t(mulv4si3): Don't expand for SSE4.1.\n+\t(smax<mode>3): Likewise.\n+\t(umaxv4si3): Likewise.\n+\t(uminv16qi3): Likewise.\n+\t(umin<mode>3): Likewise.\n+\t(umaxv8hi3): Rewrite.  Only enabled for SSE4.1.\n+\n+\t* doc/extend.texi: Document SSE4.1 built-in functions.\n+\n+\t* doc/invoke.texi: Document -msse4.1.\n+\n 2007-05-22  Nathan Sidwell  <nathan@codesourcery.com>\n \n \t* config/m68k/linux.h (ASM_SPEC): Add asm_pcrel_spec."}, {"sha": "fb346a0252ad2953d00a6cb0b0ff849b3edafff7", "filename": "gcc/config.gcc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -276,12 +276,12 @@ xscale-*-*)\n i[34567]86-*-*)\n \tcpu_type=i386\n \textra_headers=\"mmintrin.h mm3dnow.h xmmintrin.h emmintrin.h\n-\t\t       pmmintrin.h tmmintrin.h ammintrin.h\"\n+\t\t       pmmintrin.h tmmintrin.h ammintrin.h smmintrin.h\"\n \t;;\n x86_64-*-*)\n \tcpu_type=i386\n \textra_headers=\"mmintrin.h mm3dnow.h xmmintrin.h emmintrin.h\n-\t\t       pmmintrin.h tmmintrin.h ammintrin.h\"\n+\t\t       pmmintrin.h tmmintrin.h ammintrin.h smmintrin.h\"\n \tneed_64bit_hwint=yes\n \t;;\n ia64-*-*)"}, {"sha": "2efccda0aece2e73d52a5b95fe43a43bf975c040", "filename": "gcc/config/i386/i386-modes.def", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fi386-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fi386-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-modes.def?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -68,6 +68,7 @@ VECTOR_MODES (INT, 8);        /*       V8QI V4HI V2SI */\n VECTOR_MODES (INT, 16);       /* V16QI V8HI V4SI V2DI */\n VECTOR_MODES (FLOAT, 8);      /*            V4HF V2SF */\n VECTOR_MODES (FLOAT, 16);     /*       V8HF V4SF V2DF */\n+VECTOR_MODE (INT, QI, 2);     /*                 V2QI */\n VECTOR_MODE (INT, DI, 4);     /*                 V4DI */\n VECTOR_MODE (INT, SI, 8);     /*                 V8SI */\n VECTOR_MODE (INT, HI, 16);    /*                V16HI */"}, {"sha": "99491b29029acc3923af363e6e8f4225209ada26", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 507, "deletions": 5, "changes": 512, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -1594,13 +1594,29 @@ ix86_handle_option (size_t code, const char *arg ATTRIBUTE_UNUSED, int value)\n       return true;\n \n     case OPT_mssse3:\n+      if (!value)\n+\t{\n+\t  target_flags &= ~(MASK_SSE4_1 | MASK_SSE4A);\n+\t  target_flags_explicit |= MASK_SSE4_1 | MASK_SSE4A;\n+\t}\n+      return true;\n+\n+    case OPT_msse4_1:\n       if (!value)\n \t{\n \t  target_flags &= ~MASK_SSE4A;\n \t  target_flags_explicit |= MASK_SSE4A;\n \t}\n       return true;\n \n+    case OPT_msse4a:\n+      if (!value)\n+\t{\n+\t  target_flags &= ~MASK_SSE4_1;\n+\t  target_flags_explicit |= MASK_SSE4_1;\n+\t}\n+      return true;\n+\n     default:\n       return true;\n     }\n@@ -1674,7 +1690,8 @@ override_options (void)\n \t  PTA_POPCNT = 1 << 10,\n \t  PTA_ABM = 1 << 11,\n  \t  PTA_SSE4A = 1 << 12,\n-\t  PTA_NO_SAHF = 1 << 13\n+\t  PTA_NO_SAHF = 1 << 13,\n+ \t  PTA_SSE4_1 = 1 << 14\n \t} flags;\n     }\n   const processor_alias_table[] =\n@@ -1936,6 +1953,9 @@ override_options (void)\n \tif (processor_alias_table[i].flags & PTA_SSSE3\n \t    && !(target_flags_explicit & MASK_SSSE3))\n \t  target_flags |= MASK_SSSE3;\n+\tif (processor_alias_table[i].flags & PTA_SSE4_1\n+\t    && !(target_flags_explicit & MASK_SSE4_1))\n+\t  target_flags |= MASK_SSE4_1;\n \tif (processor_alias_table[i].flags & PTA_PREFETCH_SSE)\n \t  x86_prefetch_sse = true;\n \tif (processor_alias_table[i].flags & PTA_CX16)\n@@ -2141,6 +2161,10 @@ override_options (void)\n   if (!TARGET_80387)\n     target_flags |= MASK_NO_FANCY_MATH_387;\n \n+  /* Turn on SSSE3 builtins for -msse4.1.  */\n+  if (TARGET_SSE4_1)\n+    target_flags |= MASK_SSSE3;\n+\n   /* Turn on SSE3 builtins for -mssse3.  */\n   if (TARGET_SSSE3)\n     target_flags |= MASK_SSE3;\n@@ -16412,6 +16436,61 @@ enum ix86_builtins\n   IX86_BUILTIN_INSERTQI,\n   IX86_BUILTIN_INSERTQ,\n \n+  /* SSE4.1.  */\n+  IX86_BUILTIN_BLENDPD,\n+  IX86_BUILTIN_BLENDPS,\n+  IX86_BUILTIN_BLENDVPD,\n+  IX86_BUILTIN_BLENDVPS,\n+  IX86_BUILTIN_PBLENDVB128,\n+  IX86_BUILTIN_PBLENDW128,\n+\n+  IX86_BUILTIN_DPPD,\n+  IX86_BUILTIN_DPPS,\n+\n+  IX86_BUILTIN_INSERTPS128,\n+\n+  IX86_BUILTIN_MOVNTDQA,\n+  IX86_BUILTIN_MPSADBW128,\n+  IX86_BUILTIN_PACKUSDW128,\n+  IX86_BUILTIN_PCMPEQQ,\n+  IX86_BUILTIN_PHMINPOSUW128,\n+\n+  IX86_BUILTIN_PMAXSB128,\n+  IX86_BUILTIN_PMAXSD128,\n+  IX86_BUILTIN_PMAXUD128,\n+  IX86_BUILTIN_PMAXUW128,\n+\n+  IX86_BUILTIN_PMINSB128,\n+  IX86_BUILTIN_PMINSD128,\n+  IX86_BUILTIN_PMINUD128,\n+  IX86_BUILTIN_PMINUW128,\n+\n+  IX86_BUILTIN_PMOVSXBW128,\n+  IX86_BUILTIN_PMOVSXBD128,\n+  IX86_BUILTIN_PMOVSXBQ128,\n+  IX86_BUILTIN_PMOVSXWD128,\n+  IX86_BUILTIN_PMOVSXWQ128,\n+  IX86_BUILTIN_PMOVSXDQ128,\n+\n+  IX86_BUILTIN_PMOVZXBW128,\n+  IX86_BUILTIN_PMOVZXBD128,\n+  IX86_BUILTIN_PMOVZXBQ128,\n+  IX86_BUILTIN_PMOVZXWD128,\n+  IX86_BUILTIN_PMOVZXWQ128,\n+  IX86_BUILTIN_PMOVZXDQ128,\n+\n+  IX86_BUILTIN_PMULDQ128,\n+  IX86_BUILTIN_PMULLD128,\n+\n+  IX86_BUILTIN_ROUNDPD,\n+  IX86_BUILTIN_ROUNDPS,\n+  IX86_BUILTIN_ROUNDSD,\n+  IX86_BUILTIN_ROUNDSS,\n+\n+  IX86_BUILTIN_PTESTZ,\n+  IX86_BUILTIN_PTESTC,\n+  IX86_BUILTIN_PTESTNZC,\n+\n   IX86_BUILTIN_VEC_INIT_V2SI,\n   IX86_BUILTIN_VEC_INIT_V4HI,\n   IX86_BUILTIN_VEC_INIT_V8QI,\n@@ -16422,8 +16501,13 @@ enum ix86_builtins\n   IX86_BUILTIN_VEC_EXT_V8HI,\n   IX86_BUILTIN_VEC_EXT_V2SI,\n   IX86_BUILTIN_VEC_EXT_V4HI,\n+  IX86_BUILTIN_VEC_EXT_V16QI,\n+  IX86_BUILTIN_VEC_SET_V2DI,\n+  IX86_BUILTIN_VEC_SET_V4SF,\n+  IX86_BUILTIN_VEC_SET_V4SI,\n   IX86_BUILTIN_VEC_SET_V8HI,\n   IX86_BUILTIN_VEC_SET_V4HI,\n+  IX86_BUILTIN_VEC_SET_V16QI,\n \n   IX86_BUILTIN_MAX\n };\n@@ -16508,6 +16592,33 @@ static const struct builtin_description bdesc_comi[] =\n   { MASK_SSE2, CODE_FOR_sse2_ucomi, \"__builtin_ia32_ucomisdneq\", IX86_BUILTIN_UCOMINEQSD, LTGT, 0 },\n };\n \n+static const struct builtin_description bdesc_ptest[] =\n+{\n+  /* SSE4.1 */\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_ptest, \"__builtin_ia32_ptestz128\", IX86_BUILTIN_PTESTZ, EQ, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_ptest, \"__builtin_ia32_ptestc128\", IX86_BUILTIN_PTESTC, LTU, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_ptest, \"__builtin_ia32_ptestnzc128\", IX86_BUILTIN_PTESTNZC, GTU, 0 },\n+};\n+\n+/* SSE builtins with 3 arguments and the last argument must be a 8 bit\n+   constant or xmm0.  */\n+static const struct builtin_description bdesc_sse_3arg[] =\n+{\n+  /* SSE4.1 */\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_blendpd, \"__builtin_ia32_blendpd\", IX86_BUILTIN_BLENDPD, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_blendps, \"__builtin_ia32_blendps\", IX86_BUILTIN_BLENDPS, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_blendvpd, \"__builtin_ia32_blendvpd\", IX86_BUILTIN_BLENDVPD, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_blendvps, \"__builtin_ia32_blendvps\", IX86_BUILTIN_BLENDVPS, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_dppd, \"__builtin_ia32_dppd\", IX86_BUILTIN_DPPD, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_dpps, \"__builtin_ia32_dpps\", IX86_BUILTIN_DPPS, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_insertps, \"__builtin_ia32_insertps128\", IX86_BUILTIN_INSERTPS128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_mpsadbw, \"__builtin_ia32_mpsadbw128\", IX86_BUILTIN_MPSADBW128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_pblendvb, \"__builtin_ia32_pblendvb128\", IX86_BUILTIN_PBLENDVB128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_pblendw, \"__builtin_ia32_pblendw128\", IX86_BUILTIN_PBLENDW128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_roundsd, 0, IX86_BUILTIN_ROUNDSD, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_roundss, 0, IX86_BUILTIN_ROUNDSS, 0, 0 },\n+};\n+\n static const struct builtin_description bdesc_2arg[] =\n {\n   /* SSE */\n@@ -16806,7 +16917,21 @@ static const struct builtin_description bdesc_2arg[] =\n   { MASK_SSSE3, CODE_FOR_ssse3_psignv8hi3, \"__builtin_ia32_psignw128\", IX86_BUILTIN_PSIGNW128, 0, 0 },\n   { MASK_SSSE3, CODE_FOR_ssse3_psignv4hi3, \"__builtin_ia32_psignw\", IX86_BUILTIN_PSIGNW, 0, 0 },\n   { MASK_SSSE3, CODE_FOR_ssse3_psignv4si3, \"__builtin_ia32_psignd128\", IX86_BUILTIN_PSIGND128, 0, 0 },\n-  { MASK_SSSE3, CODE_FOR_ssse3_psignv2si3, \"__builtin_ia32_psignd\", IX86_BUILTIN_PSIGND, 0, 0 }\n+  { MASK_SSSE3, CODE_FOR_ssse3_psignv2si3, \"__builtin_ia32_psignd\", IX86_BUILTIN_PSIGND, 0, 0 },\n+\n+  /* SSE4.1 */\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_packusdw, \"__builtin_ia32_packusdw128\", IX86_BUILTIN_PACKUSDW128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_eqv2di3, \"__builtin_ia32_pcmpeqq\", IX86_BUILTIN_PCMPEQQ, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_smaxv16qi3, \"__builtin_ia32_pmaxsb128\", IX86_BUILTIN_PMAXSB128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_smaxv4si3, \"__builtin_ia32_pmaxsd128\", IX86_BUILTIN_PMAXSD128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_umaxv4si3, \"__builtin_ia32_pmaxud128\", IX86_BUILTIN_PMAXUD128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_umaxv8hi3, \"__builtin_ia32_pmaxuw128\", IX86_BUILTIN_PMAXUW128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sminv16qi3, \"__builtin_ia32_pminsb128\", IX86_BUILTIN_PMINSB128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sminv4si3, \"__builtin_ia32_pminsd128\", IX86_BUILTIN_PMINSD128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_uminv4si3, \"__builtin_ia32_pminud128\", IX86_BUILTIN_PMINUD128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_uminv8hi3, \"__builtin_ia32_pminuw128\", IX86_BUILTIN_PMINUW128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_mulv2siv2di3, 0, IX86_BUILTIN_PMULDQ128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_mulv4si3, \"__builtin_ia32_pmulld128\", IX86_BUILTIN_PMULLD128, 0, 0 },\n };\n \n static const struct builtin_description bdesc_1arg[] =\n@@ -16861,6 +16986,26 @@ static const struct builtin_description bdesc_1arg[] =\n   { MASK_SSSE3, CODE_FOR_absv4hi2, \"__builtin_ia32_pabsw\", IX86_BUILTIN_PABSW, 0, 0 },\n   { MASK_SSSE3, CODE_FOR_absv4si2, \"__builtin_ia32_pabsd128\", IX86_BUILTIN_PABSD128, 0, 0 },\n   { MASK_SSSE3, CODE_FOR_absv2si2, \"__builtin_ia32_pabsd\", IX86_BUILTIN_PABSD, 0, 0 },\n+\n+  /* SSE4.1 */\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_extendv8qiv8hi2, 0, IX86_BUILTIN_PMOVSXBW128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_extendv4qiv4si2, 0, IX86_BUILTIN_PMOVSXBD128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_extendv2qiv2di2, 0, IX86_BUILTIN_PMOVSXBQ128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_extendv4hiv4si2, 0, IX86_BUILTIN_PMOVSXWD128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_extendv2hiv2di2, 0, IX86_BUILTIN_PMOVSXWQ128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_extendv2siv2di2, 0, IX86_BUILTIN_PMOVSXDQ128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_zero_extendv8qiv8hi2, 0, IX86_BUILTIN_PMOVZXBW128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_zero_extendv4qiv4si2, 0, IX86_BUILTIN_PMOVZXBD128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_zero_extendv2qiv2di2, 0, IX86_BUILTIN_PMOVZXBQ128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_zero_extendv4hiv4si2, 0, IX86_BUILTIN_PMOVZXWD128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_zero_extendv2hiv2di2, 0, IX86_BUILTIN_PMOVZXWQ128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_zero_extendv2siv2di2, 0, IX86_BUILTIN_PMOVZXDQ128, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_phminposuw, \"__builtin_ia32_phminposuw128\", IX86_BUILTIN_PHMINPOSUW128, 0, 0 },\n+\n+  /* Fake 1 arg builtins with a constant smaller than 8 bits as the\n+     2nd arg.  */\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_roundpd, 0, IX86_BUILTIN_ROUNDPD, 0, 0 },\n+  { MASK_SSE4_1, CODE_FOR_sse4_1_roundps, 0, IX86_BUILTIN_ROUNDPS, 0, 0 },\n };\n \n /* Set up all the MMX/SSE builtins.  This is not called if TARGET_MMX\n@@ -17167,6 +17312,55 @@ ix86_init_mmx_sse_builtins (void)\n   tree v2di_ftype_v2di_v16qi\n     = build_function_type_list (V2DI_type_node, V2DI_type_node, V16QI_type_node,\n                                 NULL_TREE);\n+  tree v2df_ftype_v2df_v2df_v2df\n+    = build_function_type_list (V2DF_type_node,\n+\t\t\t\tV2DF_type_node, V2DF_type_node,\n+\t\t\t\tV2DF_type_node, NULL_TREE);\n+  tree v4sf_ftype_v4sf_v4sf_v4sf\n+    = build_function_type_list (V4SF_type_node,\n+\t\t\t\tV4SF_type_node, V4SF_type_node,\n+\t\t\t\tV4SF_type_node, NULL_TREE);\n+  tree v8hi_ftype_v16qi\n+    = build_function_type_list (V8HI_type_node, V16QI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4si_ftype_v16qi\n+    = build_function_type_list (V4SI_type_node, V16QI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v2di_ftype_v16qi\n+    = build_function_type_list (V2DI_type_node, V16QI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4si_ftype_v8hi\n+    = build_function_type_list (V4SI_type_node, V8HI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v2di_ftype_v8hi\n+    = build_function_type_list (V2DI_type_node, V8HI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v2di_ftype_v4si\n+    = build_function_type_list (V2DI_type_node, V4SI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v2di_ftype_pv2di\n+    = build_function_type_list (V2DI_type_node, pv2di_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v16qi_ftype_v16qi_v16qi_int\n+    = build_function_type_list (V16QI_type_node, V16QI_type_node,\n+\t\t\t\tV16QI_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v16qi_ftype_v16qi_v16qi_v16qi\n+    = build_function_type_list (V16QI_type_node, V16QI_type_node,\n+\t\t\t\tV16QI_type_node, V16QI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8hi_ftype_v8hi_v8hi_int\n+    = build_function_type_list (V8HI_type_node, V8HI_type_node,\n+\t\t\t\tV8HI_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4si_ftype_v4si_v4si_int\n+    = build_function_type_list (V4SI_type_node, V4SI_type_node,\n+\t\t\t\tV4SI_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree int_ftype_v2di_v2di\n+    = build_function_type_list (integer_type_node,\n+\t\t\t\tV2DI_type_node, V2DI_type_node,\n+\t\t\t\tNULL_TREE);\n \n   tree float80_type;\n   tree float128_type;\n@@ -17193,6 +17387,64 @@ ix86_init_mmx_sse_builtins (void)\n       (*lang_hooks.types.register_builtin_type) (float128_type, \"__float128\");\n     }\n \n+  /* Add all SSE builtins that are more or less simple operations on\n+     three operands.  */\n+  for (i = 0, d = bdesc_sse_3arg;\n+       i < ARRAY_SIZE (bdesc_sse_3arg);\n+       i++, d++)\n+    {\n+      /* Use one of the operands; the target can have a different mode for\n+\t mask-generating compares.  */\n+      enum machine_mode mode;\n+      tree type;\n+\n+      if (d->name == 0)\n+\tcontinue;\n+      mode = insn_data[d->icode].operand[1].mode;\n+\n+      switch (mode)\n+\t{\n+\tcase V16QImode:\n+\t  type = v16qi_ftype_v16qi_v16qi_int;\n+\t  break;\n+\tcase V8HImode:\n+\t  type = v8hi_ftype_v8hi_v8hi_int;\n+\t  break;\n+\tcase V4SImode:\n+\t  type = v4si_ftype_v4si_v4si_int;\n+\t  break;\n+\tcase V2DImode:\n+\t  type = v2di_ftype_v2di_v2di_int;\n+\t  break;\n+\tcase V2DFmode:\n+\t  type = v2df_ftype_v2df_v2df_int;\n+\t  break;\n+\tcase V4SFmode:\n+\t  type = v4sf_ftype_v4sf_v4sf_int;\n+\t  break;\n+\tdefault:\n+\t  gcc_unreachable ();\n+\t}\n+\n+      /* Override for variable blends.  */\n+      switch (d->icode)\n+\t{\n+\tcase CODE_FOR_sse4_1_blendvpd:\n+\t  type = v2df_ftype_v2df_v2df_v2df;\n+\t  break;\n+\tcase CODE_FOR_sse4_1_blendvps:\n+\t  type = v4sf_ftype_v4sf_v4sf_v4sf;\n+\t  break;\n+\tcase CODE_FOR_sse4_1_pblendvb:\n+\t  type = v16qi_ftype_v16qi_v16qi_v16qi;\n+\t  break;\n+\tdefault:\n+\t  break;\n+\t}\n+\n+      def_builtin (d->mask, d->name, type, d->code);\n+    }\n+\n   /* Add all builtins that are more or less simple operations on two\n      operands.  */\n   for (i = 0, d = bdesc_2arg; i < ARRAY_SIZE (bdesc_2arg); i++, d++)\n@@ -17322,6 +17574,10 @@ ix86_init_mmx_sse_builtins (void)\n     else\n       def_builtin (d->mask, d->name, int_ftype_v4sf_v4sf, d->code);\n \n+  /* ptest insns.  */\n+  for (i = 0, d = bdesc_ptest; i < ARRAY_SIZE (bdesc_ptest); i++, d++)\n+    def_builtin (d->mask, d->name, int_ftype_v2di_v2di, d->code);\n+\n   def_builtin (MASK_MMX, \"__builtin_ia32_packsswb\", v8qi_ftype_v4hi_v4hi, IX86_BUILTIN_PACKSSWB);\n   def_builtin (MASK_MMX, \"__builtin_ia32_packssdw\", v4hi_ftype_v2si_v2si, IX86_BUILTIN_PACKSSDW);\n   def_builtin (MASK_MMX, \"__builtin_ia32_packuswb\", v8qi_ftype_v4hi_v4hi, IX86_BUILTIN_PACKUSWB);\n@@ -17495,6 +17751,44 @@ ix86_init_mmx_sse_builtins (void)\n   def_builtin (MASK_SSSE3, \"__builtin_ia32_palignr\", di_ftype_di_di_int,\n \t       IX86_BUILTIN_PALIGNR);\n \n+  /* SSE4.1. */\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_movntdqa\",\n+\t       v2di_ftype_pv2di, IX86_BUILTIN_MOVNTDQA);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovsxbw128\",\n+\t       v8hi_ftype_v16qi, IX86_BUILTIN_PMOVSXBW128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovsxbd128\",\n+\t       v4si_ftype_v16qi, IX86_BUILTIN_PMOVSXBD128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovsxbq128\",\n+\t       v2di_ftype_v16qi, IX86_BUILTIN_PMOVSXBQ128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovsxwd128\",\n+\t       v4si_ftype_v8hi, IX86_BUILTIN_PMOVSXWD128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovsxwq128\",\n+\t       v2di_ftype_v8hi, IX86_BUILTIN_PMOVSXWQ128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovsxdq128\",\n+\t       v2di_ftype_v4si, IX86_BUILTIN_PMOVSXDQ128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovzxbw128\",\n+\t       v8hi_ftype_v16qi, IX86_BUILTIN_PMOVZXBW128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovzxbd128\",\n+\t       v4si_ftype_v16qi, IX86_BUILTIN_PMOVZXBD128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovzxbq128\",\n+\t       v2di_ftype_v16qi, IX86_BUILTIN_PMOVZXBQ128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovzxwd128\",\n+\t       v4si_ftype_v8hi, IX86_BUILTIN_PMOVZXWD128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovzxwq128\",\n+\t       v2di_ftype_v8hi, IX86_BUILTIN_PMOVZXWQ128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmovzxdq128\",\n+\t       v2di_ftype_v4si, IX86_BUILTIN_PMOVZXDQ128);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_pmuldq128\",\n+\t       v2di_ftype_v4si_v4si, IX86_BUILTIN_PMULDQ128);\n+  def_builtin_const (MASK_SSE4_1, \"__builtin_ia32_roundpd\",\n+\t\t     v2df_ftype_v2df_int, IX86_BUILTIN_ROUNDPD);\n+  def_builtin_const (MASK_SSE4_1, \"__builtin_ia32_roundps\",\n+\t\t     v4sf_ftype_v4sf_int, IX86_BUILTIN_ROUNDPS);\n+  def_builtin_const (MASK_SSE4_1, \"__builtin_ia32_roundsd\",\n+\t\t     v2df_ftype_v2df_v2df_int, IX86_BUILTIN_ROUNDSD);\n+  def_builtin_const (MASK_SSE4_1, \"__builtin_ia32_roundss\",\n+\t\t     v4sf_ftype_v4sf_v4sf_int, IX86_BUILTIN_ROUNDSS);\n+\n   /* AMDFAM10 SSE4A New built-ins  */\n   def_builtin (MASK_SSE4A, \"__builtin_ia32_movntsd\",\n                void_ftype_pdouble_v2df, IX86_BUILTIN_MOVNTSD);\n@@ -17567,7 +17861,30 @@ ix86_init_mmx_sse_builtins (void)\n   def_builtin (MASK_MMX, \"__builtin_ia32_vec_ext_v2si\",\n \t       ftype, IX86_BUILTIN_VEC_EXT_V2SI);\n \n+  ftype = build_function_type_list (intQI_type_node, V16QI_type_node,\n+\t\t\t\t    integer_type_node, NULL_TREE);\n+  def_builtin (MASK_SSE, \"__builtin_ia32_vec_ext_v16qi\",\n+\t       ftype, IX86_BUILTIN_VEC_EXT_V16QI);\n+\n   /* Access to the vec_set patterns.  */\n+  ftype = build_function_type_list (V2DI_type_node, V2DI_type_node,\n+\t\t\t\t    intDI_type_node,\n+\t\t\t\t    integer_type_node, NULL_TREE);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_vec_set_v2di\",\n+\t       ftype, IX86_BUILTIN_VEC_SET_V2DI);\n+\n+  ftype = build_function_type_list (V4SF_type_node, V4SF_type_node,\n+\t\t\t\t    float_type_node,\n+\t\t\t\t    integer_type_node, NULL_TREE);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_vec_set_v4sf\",\n+\t       ftype, IX86_BUILTIN_VEC_SET_V4SF);\n+\n+  ftype = build_function_type_list (V4SI_type_node, V4SI_type_node,\n+\t\t\t\t    intSI_type_node,\n+\t\t\t\t    integer_type_node, NULL_TREE);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_vec_set_v4si\",\n+\t       ftype, IX86_BUILTIN_VEC_SET_V4SI);\n+\n   ftype = build_function_type_list (V8HI_type_node, V8HI_type_node,\n \t\t\t\t    intHI_type_node,\n \t\t\t\t    integer_type_node, NULL_TREE);\n@@ -17579,6 +17896,12 @@ ix86_init_mmx_sse_builtins (void)\n \t\t\t\t    integer_type_node, NULL_TREE);\n   def_builtin (MASK_SSE | MASK_3DNOW_A, \"__builtin_ia32_vec_set_v4hi\",\n \t       ftype, IX86_BUILTIN_VEC_SET_V4HI);\n+\n+  ftype = build_function_type_list (V16QI_type_node, V16QI_type_node,\n+\t\t\t\t    intQI_type_node,\n+\t\t\t\t    integer_type_node, NULL_TREE);\n+  def_builtin (MASK_SSE4_1, \"__builtin_ia32_vec_set_v16qi\",\n+\t       ftype, IX86_BUILTIN_VEC_SET_V16QI);\n }\n \n static void\n@@ -17599,6 +17922,74 @@ safe_vector_operand (rtx x, enum machine_mode mode)\n   return x;\n }\n \n+/* Subroutine of ix86_expand_builtin to take care of SSE insns with\n+   4 operands. The third argument must be a constant smaller than 8\n+   bits or xmm0.  */\n+\n+static rtx\n+ix86_expand_sse_4_operands_builtin (enum insn_code icode, tree exp,\n+\t\t\t\t    rtx target)\n+{\n+  rtx pat;\n+  tree arg0 = CALL_EXPR_ARG (exp, 0);\n+  tree arg1 = CALL_EXPR_ARG (exp, 1);\n+  tree arg2 = CALL_EXPR_ARG (exp, 2);\n+  rtx op0 = expand_normal (arg0);\n+  rtx op1 = expand_normal (arg1);\n+  rtx op2 = expand_normal (arg2);\n+  enum machine_mode tmode = insn_data[icode].operand[0].mode;\n+  enum machine_mode mode0 = insn_data[icode].operand[1].mode;\n+  enum machine_mode mode1 = insn_data[icode].operand[2].mode;\n+  enum machine_mode mode2;\n+  rtx xmm0;\n+\n+  if (! (*insn_data[icode].operand[1].predicate) (op0, mode0))\n+    op0 = copy_to_mode_reg (mode0, op0);\n+  if ((optimize && !register_operand (op1, mode1))\n+      || !(*insn_data[icode].operand[2].predicate) (op1, mode1))\n+    op1 = copy_to_mode_reg (mode1, op1);\n+\n+  switch (icode)\n+    {\n+    case CODE_FOR_sse4_1_blendvpd:\n+    case CODE_FOR_sse4_1_blendvps:\n+    case CODE_FOR_sse4_1_pblendvb:\n+      /* The third argument of variable blends must be xmm0.  */\n+      xmm0 = gen_rtx_REG (tmode, FIRST_SSE_REG);\n+      emit_move_insn (xmm0, op2);\n+      op2 = xmm0;\n+      break;\n+    default:\n+      mode2 = insn_data[icode].operand[2].mode;\n+      if (! (*insn_data[icode].operand[3].predicate) (op2, mode2))\n+\t{\n+\t  switch (icode)\n+\t    {\n+\t    case CODE_FOR_sse4_1_roundsd:\n+\t    case CODE_FOR_sse4_1_roundss:\n+\t      error (\"the third argument must be a 4-bit immediate\");\n+\t      break;\n+\t    default:\n+\t      error (\"the third argument must be a 8-bit immediate\");\n+\t      break;\n+\t    }\n+\t  return const0_rtx;\n+\t}\n+      break;\n+    }\n+\n+  if (optimize\n+      || target == 0\n+      || GET_MODE (target) != tmode\n+      || ! (*insn_data[icode].operand[0].predicate) (target, tmode))\n+    target = gen_reg_rtx (tmode);\n+  pat = GEN_FCN (icode) (target, op0, op1, op2);\n+  if (! pat)\n+    return 0;\n+  emit_insn (pat);\n+  return target;\n+}\n+\n /* Subroutine of ix86_expand_builtin to take care of binop insns.  */\n \n static rtx\n@@ -17720,7 +18111,28 @@ ix86_expand_unop_builtin (enum insn_code icode, tree exp,\n \top0 = copy_to_mode_reg (mode0, op0);\n     }\n \n-  pat = GEN_FCN (icode) (target, op0);\n+  switch (icode)\n+    {\n+    case CODE_FOR_sse4_1_roundpd:\n+    case CODE_FOR_sse4_1_roundps:\n+\t{\n+\t  tree arg1 = CALL_EXPR_ARG (exp, 1);\n+\t  rtx op1 = expand_normal (arg1);\n+\t  enum machine_mode mode1 = insn_data[icode].operand[2].mode;\n+\n+\t  if (! (*insn_data[icode].operand[2].predicate) (op1, mode1))\n+\t    {\n+\t      error (\"the second argument must be a 4-bit immediate\");\n+\t      return const0_rtx;\n+\t    }\n+\t  pat = GEN_FCN (icode) (target, op0, op1);\n+\t}\n+      break;\n+    default:\n+      pat = GEN_FCN (icode) (target, op0);\n+      break;\n+    }\n+\n   if (! pat)\n     return 0;\n   emit_insn (pat);\n@@ -17867,6 +18279,50 @@ ix86_expand_sse_comi (const struct builtin_description *d, tree exp,\n   return SUBREG_REG (target);\n }\n \n+/* Subroutine of ix86_expand_builtin to take care of ptest insns.  */\n+\n+static rtx\n+ix86_expand_sse_ptest (const struct builtin_description *d, tree exp,\n+\t\t       rtx target)\n+{\n+  rtx pat;\n+  tree arg0 = CALL_EXPR_ARG (exp, 0);\n+  tree arg1 = CALL_EXPR_ARG (exp, 1);\n+  rtx op0 = expand_normal (arg0);\n+  rtx op1 = expand_normal (arg1);\n+  enum machine_mode mode0 = insn_data[d->icode].operand[0].mode;\n+  enum machine_mode mode1 = insn_data[d->icode].operand[1].mode;\n+  enum rtx_code comparison = d->comparison;\n+\n+  if (VECTOR_MODE_P (mode0))\n+    op0 = safe_vector_operand (op0, mode0);\n+  if (VECTOR_MODE_P (mode1))\n+    op1 = safe_vector_operand (op1, mode1);\n+\n+  target = gen_reg_rtx (SImode);\n+  emit_move_insn (target, const0_rtx);\n+  target = gen_rtx_SUBREG (QImode, target, 0);\n+\n+  if ((optimize && !register_operand (op0, mode0))\n+      || !(*insn_data[d->icode].operand[0].predicate) (op0, mode0))\n+    op0 = copy_to_mode_reg (mode0, op0);\n+  if ((optimize && !register_operand (op1, mode1))\n+      || !(*insn_data[d->icode].operand[1].predicate) (op1, mode1))\n+    op1 = copy_to_mode_reg (mode1, op1);\n+\n+  pat = GEN_FCN (d->icode) (op0, op1);\n+  if (! pat)\n+    return 0;\n+  emit_insn (pat);\n+  emit_insn (gen_rtx_SET (VOIDmode,\n+\t\t\t  gen_rtx_STRICT_LOW_PART (VOIDmode, target),\n+\t\t\t  gen_rtx_fmt_ee (comparison, QImode,\n+\t\t\t\t\t  SET_DEST (pat),\n+\t\t\t\t\t  const0_rtx)));\n+\n+  return SUBREG_REG (target);\n+}\n+\n /* Return the integer constant in ARG.  Constrain it to be in the range\n    of the subparts of VEC_TYPE; issue an error if not.  */\n \n@@ -18522,6 +18978,10 @@ ix86_expand_builtin (tree exp, rtx target, rtx subtarget ATTRIBUTE_UNUSED,\n       emit_insn (pat);\n       return target;\n \n+    case IX86_BUILTIN_MOVNTDQA:\n+      return ix86_expand_unop_builtin (CODE_FOR_sse4_1_movntdqa, exp,\n+\t\t\t\t       target, 1);\n+\n     case IX86_BUILTIN_MOVNTSD:\n       return ix86_expand_store_builtin (CODE_FOR_sse4a_vmmovntv2df, exp);\n \n@@ -18642,16 +19102,28 @@ ix86_expand_builtin (tree exp, rtx target, rtx subtarget ATTRIBUTE_UNUSED,\n     case IX86_BUILTIN_VEC_EXT_V8HI:\n     case IX86_BUILTIN_VEC_EXT_V2SI:\n     case IX86_BUILTIN_VEC_EXT_V4HI:\n+    case IX86_BUILTIN_VEC_EXT_V16QI:\n       return ix86_expand_vec_ext_builtin (exp, target);\n \n+    case IX86_BUILTIN_VEC_SET_V2DI:\n+    case IX86_BUILTIN_VEC_SET_V4SF:\n+    case IX86_BUILTIN_VEC_SET_V4SI:\n     case IX86_BUILTIN_VEC_SET_V8HI:\n     case IX86_BUILTIN_VEC_SET_V4HI:\n+    case IX86_BUILTIN_VEC_SET_V16QI:\n       return ix86_expand_vec_set_builtin (exp);\n \n     default:\n       break;\n     }\n \n+  for (i = 0, d = bdesc_sse_3arg;\n+       i < ARRAY_SIZE (bdesc_sse_3arg);\n+       i++, d++)\n+    if (d->code == fcode)\n+      return ix86_expand_sse_4_operands_builtin (d->icode, exp,\n+\t\t\t\t\t\t target);\n+\n   for (i = 0, d = bdesc_2arg; i < ARRAY_SIZE (bdesc_2arg); i++, d++)\n     if (d->code == fcode)\n       {\n@@ -18673,6 +19145,10 @@ ix86_expand_builtin (tree exp, rtx target, rtx subtarget ATTRIBUTE_UNUSED,\n     if (d->code == fcode)\n       return ix86_expand_sse_comi (d, exp, target);\n \n+  for (i = 0, d = bdesc_ptest; i < ARRAY_SIZE (bdesc_ptest); i++, d++)\n+    if (d->code == fcode)\n+      return ix86_expand_sse_ptest (d, exp, target);\n+\n   gcc_unreachable ();\n }\n \n@@ -20877,8 +21353,12 @@ ix86_expand_vector_set (bool mmx_ok, rtx target, rtx val, int elt)\n \t}\n       break;\n \n-    case V2DFmode:\n     case V2DImode:\n+      use_vec_merge = TARGET_SSE4_1;\n+      if (use_vec_merge)\n+\tbreak;\n+\n+    case V2DFmode:\n       {\n \trtx op0, op1;\n \n@@ -20899,6 +21379,10 @@ ix86_expand_vector_set (bool mmx_ok, rtx target, rtx val, int elt)\n       return;\n \n     case V4SFmode:\n+      use_vec_merge = TARGET_SSE4_1;\n+      if (use_vec_merge)\n+\tbreak;\n+\n       switch (elt)\n \t{\n \tcase 0:\n@@ -20946,6 +21430,10 @@ ix86_expand_vector_set (bool mmx_ok, rtx target, rtx val, int elt)\n       break;\n \n     case V4SImode:\n+      use_vec_merge = TARGET_SSE4_1;\n+      if (use_vec_merge)\n+\tbreak;\n+\n       /* Element 0 handled by vec_merge below.  */\n       if (elt == 0)\n \t{\n@@ -20990,6 +21478,9 @@ ix86_expand_vector_set (bool mmx_ok, rtx target, rtx val, int elt)\n       break;\n \n     case V16QImode:\n+      use_vec_merge = TARGET_SSE4_1;\n+      break;\n+\n     case V8QImode:\n     default:\n       break;\n@@ -21036,6 +21527,10 @@ ix86_expand_vector_extract (bool mmx_ok, rtx target, rtx vec, int elt)\n       break;\n \n     case V4SFmode:\n+      use_vec_extr = TARGET_SSE4_1;\n+      if (use_vec_extr)\n+\tbreak;\n+\n       switch (elt)\n \t{\n \tcase 0:\n@@ -21064,6 +21559,10 @@ ix86_expand_vector_extract (bool mmx_ok, rtx target, rtx vec, int elt)\n       break;\n \n     case V4SImode:\n+      use_vec_extr = TARGET_SSE4_1;\n+      if (use_vec_extr)\n+\tbreak;\n+\n       if (TARGET_SSE2)\n \t{\n \t  switch (elt)\n@@ -21109,6 +21608,9 @@ ix86_expand_vector_extract (bool mmx_ok, rtx target, rtx vec, int elt)\n       break;\n \n     case V16QImode:\n+      use_vec_extr = TARGET_SSE4_1;\n+      break;\n+\n     case V8QImode:\n       /* ??? Could extract the appropriate HImode element and shift.  */\n     default:\n@@ -21121,7 +21623,7 @@ ix86_expand_vector_extract (bool mmx_ok, rtx target, rtx vec, int elt)\n       tmp = gen_rtx_VEC_SELECT (inner_mode, vec, tmp);\n \n       /* Let the rtl optimizers know about the zero extension performed.  */\n-      if (inner_mode == HImode)\n+      if (inner_mode == QImode || inner_mode == HImode)\n \t{\n \t  tmp = gen_rtx_ZERO_EXTEND (SImode, tmp);\n \t  target = gen_lowpart (SImode, target);"}, {"sha": "e4cf24981ec79ed293905422505a5f2072c08a0e", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -540,6 +540,8 @@ extern const char *host_detect_local_cpu (int argc, const char **argv);\n \tbuiltin_define (\"__SSE3__\");\t\t\t\t\\\n       if (TARGET_SSSE3)\t\t\t\t\t\t\\\n \tbuiltin_define (\"__SSSE3__\");\t\t\t\t\\\n+      if (TARGET_SSE4_1)\t\t\t\t\t\\\n+\tbuiltin_define (\"__SSE4_1__\");\t\t\t\t\\\n       if (TARGET_SSE4A)\t\t\t\t\t\t\\\n  \tbuiltin_define (\"__SSE4A__\");\t\t                \\\n       if (TARGET_SSE_MATH && TARGET_SSE)\t\t\t\\"}, {"sha": "e4b2c86f13bd06fcc5648f1f5f7540e7cdf9f86c", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -162,6 +162,17 @@\n    (UNSPEC_EXTRQ                131)   \n    (UNSPEC_INSERTQI             132)\n    (UNSPEC_INSERTQ              133)\n+\n+   ; For SSE4.1 support\n+   (UNSPEC_BLENDV\t\t134)\n+   (UNSPEC_INSERTPS\t\t135)\n+   (UNSPEC_DP\t\t\t136)\n+   (UNSPEC_MOVNTDQA\t\t137)\n+   (UNSPEC_MPSADBW\t\t138)\n+   (UNSPEC_PHMINPOSUW\t\t139)\n+   (UNSPEC_PTEST\t\t140)\n+   (UNSPEC_ROUNDP\t\t141)\n+   (UNSPEC_ROUNDS\t\t142)\n   ])\n \n (define_constants"}, {"sha": "ac60526bbf65daa779aec3208350a8a7ed8bdb3e", "filename": "gcc/config/i386/i386.opt", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fi386.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fi386.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.opt?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -187,6 +187,10 @@ mssse3\n Target Report Mask(SSSE3)\n Support MMX, SSE, SSE2, SSE3 and SSSE3 built-in functions and code generation\n \n+msse4.1\n+Target Report Mask(SSE4_1)\n+Support MMX, SSE, SSE2, SSE3, SSSE3 and SSE4.1 built-in functions and code generation\n+\n msse4a\n Target Report Mask(SSE4A)\n Support MMX, SSE, SSE2, SSE3 and SSE4A built-in functions and code generation"}, {"sha": "5dcc24b68d20f138d8eb212d480f8b1779450173", "filename": "gcc/config/i386/predicates.md", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fpredicates.md?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -623,6 +623,11 @@\n   (and (match_code \"const_int\")\n        (match_test \"IN_RANGE (INTVAL (op), 4, 7)\")))\n \n+;; Match exactly one bit in 2-bit mask.\n+(define_predicate \"const_pow2_1_to_2_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"INTVAL (op) == 1 || INTVAL (op) == 2\")))\n+\n ;; Match exactly one bit in 4-bit mask.\n (define_predicate \"const_pow2_1_to_8_operand\"\n   (match_code \"const_int\")\n@@ -639,6 +644,14 @@\n   return log <= 7;\n })\n \n+;; Match exactly one bit in 16-bit mask.\n+(define_predicate \"const_pow2_1_to_32768_operand\"\n+  (match_code \"const_int\")\n+{\n+  unsigned int log = exact_log2 (INTVAL (op));\n+  return log <= 15;\n+})\n+\n ;; True if this is a constant appropriate for an increment or decrement.\n (define_predicate \"incdec_operand\"\n   (match_code \"const_int\")"}, {"sha": "d57e2e6640d698047d9ad6790213a61439a279f2", "filename": "gcc/config/i386/smmintrin.h", "status": "added", "additions": 578, "deletions": 0, "changes": 578, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fsmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fsmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsmmintrin.h?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -0,0 +1,578 @@\n+/* Copyright (C) 2007 Free Software Foundation, Inc.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify\n+   it under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 2, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful,\n+   but WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+   GNU General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING.  If not, write to\n+   the Free Software Foundation, 59 Temple Place - Suite 330,\n+   Boston, MA 02111-1307, USA.  */\n+\n+/* As a special exception, if you include this header file into source\n+   files compiled by GCC, this header file does not by itself cause\n+   the resulting executable to be covered by the GNU General Public\n+   License.  This exception does not however invalidate any other\n+   reasons why the executable file might be covered by the GNU General\n+   Public License.  */\n+\n+/* Implemented from the specification included in the Intel C++ Compiler\n+   User Guide and Reference, version 10.0.  */\n+\n+#ifndef _SMMINTRIN_H_INCLUDED\n+#define _SMMINTRIN_H_INCLUDED\n+\n+#ifndef __SSE4_1__\n+# error \"SSE4.1 instruction set not enabled\"\n+#else\n+\n+/* We need definitions from the SSSE3, SSE3, SSE2 and SSE header\n+   files.  */\n+#include <tmmintrin.h>\n+\n+/* SSE4.1 */\n+\n+/* Rounding mode macros. */\n+#define _MM_FROUND_TO_NEAREST_INT\t0x00\n+#define _MM_FROUND_TO_NEG_INF\t\t0x01\n+#define _MM_FROUND_TO_POS_INF\t\t0x02\n+#define _MM_FROUND_TO_ZERO\t\t0x03\n+#define _MM_FROUND_CUR_DIRECTION\t0x04\n+\n+#define _MM_FROUND_RAISE_EXC\t\t0x00\n+#define _MM_FROUND_NO_EXC\t\t0x08\n+\n+#define _MM_FROUND_NINT\t\t\\\n+  (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_RAISE_EXC)\n+#define _MM_FROUND_FLOOR\t\\\n+  (_MM_FROUND_TO_NEG_INF | _MM_FROUND_RAISE_EXC)\n+#define _MM_FROUND_CEIL\t\t\\\n+  (_MM_FROUND_TO_POS_INF | _MM_FROUND_RAISE_EXC)\n+#define _MM_FROUND_TRUNC\t\\\n+  (_MM_FROUND_TO_ZERO | _MM_FROUND_RAISE_EXC)\n+#define _MM_FROUND_RINT\t\t\\\n+  (_MM_FROUND_CUR_DIRECTION | _MM_FROUND_RAISE_EXC)\n+#define _MM_FROUND_NEARBYINT\t\\\n+  (_MM_FROUND_CUR_DIRECTION | _MM_FROUND_NO_EXC)\n+\n+/* Integer blend instructions - select data from 2 sources using\n+   constant/variable mask.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_blend_epi16 (__m128i __X, __m128i __Y, const int __M)\n+{\n+  return (__m128i) __builtin_ia32_pblendw128 ((__v8hi)__X,\n+\t\t\t\t\t      (__v8hi)__Y,\n+\t\t\t\t\t      __M);\n+}\n+#else\n+#define _mm_blend_epi16(X, Y, M) \\\n+  ((__m128i) __builtin_ia32_pblendw128 ((__v8hi)(X), (__v8hi)(Y), (M)))\n+#endif\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_blendv_epi8 (__m128i __X, __m128i __Y, __m128i __M)\n+{\n+  return (__m128i) __builtin_ia32_pblendvb128 ((__v16qi)__X,\n+\t\t\t\t\t       (__v16qi)__Y,\n+\t\t\t\t\t       (__v16qi)__M);\n+}\n+\n+/* Single precision floating point blend instructions - select data\n+   from 2 sources using constant/variable mask.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline __m128 __attribute__((__always_inline__))\n+_mm_blend_ps (__m128 __X, __m128 __Y, const int __M)\n+{\n+  return (__m128) __builtin_ia32_blendps ((__v4sf)__X,\n+\t\t\t\t\t  (__v4sf)__Y,\n+\t\t\t\t\t  __M);\n+}\n+#else\n+#define _mm_blend_ps(X, Y, M) \\\n+  ((__m128) __builtin_ia32_blendps ((__v4sf)(X), (__v4sf)(Y), (M)))\n+#endif\n+\n+static __inline __m128 __attribute__((__always_inline__))\n+_mm_blendv_ps (__m128 __X, __m128 __Y, __m128 __M)\n+{\n+  return (__m128) __builtin_ia32_blendvps ((__v4sf)__X,\n+\t\t\t\t\t   (__v4sf)__Y,\n+\t\t\t\t\t   (__v4sf)__M);\n+}\n+\n+/* Double precision floating point blend instructions - select data\n+   from 2 sources using constant/variable mask.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline __m128d __attribute__((__always_inline__))\n+_mm_blend_pd (__m128d __X, __m128d __Y, const int __M)\n+{\n+  return (__m128d) __builtin_ia32_blendpd ((__v2df)__X,\n+\t\t\t\t\t   (__v2df)__Y,\n+\t\t\t\t\t   __M);\n+}\n+#else\n+#define _mm_blend_pd(X, Y, M) \\\n+  ((__m128d) __builtin_ia32_blendpd ((__v2df)(X), (__v2df)(Y), (M)))\n+#endif\n+\n+static __inline __m128d __attribute__((__always_inline__))\n+_mm_blendv_pd (__m128d __X, __m128d __Y, __m128d __M)\n+{\n+  return (__m128d) __builtin_ia32_blendvpd ((__v2df)__X,\n+\t\t\t\t\t    (__v2df)__Y,\n+\t\t\t\t\t    (__v2df)__M);\n+}\n+\n+/* Dot product instructions with mask-defined summing and zeroing parts\n+   of result.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline __m128 __attribute__((__always_inline__))\n+_mm_dp_ps (__m128 __X, __m128 __Y, const int __M)\n+{\n+  return (__m128) __builtin_ia32_dpps ((__v4sf)__X,\n+\t\t\t\t       (__v4sf)__Y,\n+\t\t\t\t       __M);\n+}\n+\n+static __inline __m128d __attribute__((__always_inline__))\n+_mm_dp_pd (__m128d __X, __m128d __Y, const int __M)\n+{\n+  return (__m128d) __builtin_ia32_dppd ((__v2df)__X,\n+\t\t\t\t\t(__v2df)__Y,\n+\t\t\t\t\t__M);\n+}\n+#else\n+#define _mm_dp_ps(X, Y, M) \\\n+  ((__m128) __builtin_ia32_dpps ((__v4sf)(X), (__v4sf)(Y), (M)))\n+\n+#define _mm_dp_pd(X, Y, M) \\\n+  ((__m128d) __builtin_ia32_dppd ((__v2df)(X), (__v2df)(Y), (M)))\n+#endif\n+\n+/* Packed integer 64-bit comparison, zeroing or filling with ones\n+   corresponding parts of result.  */\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cmpeq_epi64 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pcmpeqq ((__v2di)__X, (__v2di)__Y);\n+}\n+\n+/*  Min/max packed integer instructions.  */\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_min_epi8 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pminsb128 ((__v16qi)__X, (__v16qi)__Y);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_max_epi8 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pmaxsb128 ((__v16qi)__X, (__v16qi)__Y);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_min_epu16 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pminuw128 ((__v8hi)__X, (__v8hi)__Y);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_max_epu16 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pmaxuw128 ((__v8hi)__X, (__v8hi)__Y);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_min_epi32 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pminsd128 ((__v4si)__X, (__v4si)__Y);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_max_epi32 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pmaxsd128 ((__v4si)__X, (__v4si)__Y);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_min_epu32 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pminud128 ((__v4si)__X, (__v4si)__Y);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_max_epu32 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pmaxud128 ((__v4si)__X, (__v4si)__Y);\n+}\n+\n+/* Packed integer 32-bit multiplication with truncation of upper\n+   halves of results.  */\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_mullo_epi32 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pmulld128 ((__v4si)__X, (__v4si)__Y);\n+}\n+\n+/* Packed integer 32-bit multiplication of 2 pairs of operands\n+   with two 64-bit results.  */\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_mul_epi32 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_pmuldq128 ((__v4si)__X, (__v4si)__Y);\n+}\n+\n+/* Packed integer 128-bit bitwise comparison. Return 1 if\n+   (__V & __M) == 0.  */\n+static __inline int __attribute__((__always_inline__))\n+_mm_testz_si128 (__m128i __M, __m128i __V)\n+{\n+  return __builtin_ia32_ptestz128 ((__v2di)__M, (__v2di)__V);\n+}\n+\n+/* Packed integer 128-bit bitwise comparison. Return 1 if\n+   (__V & ~__M) == 0.  */\n+static __inline int __attribute__((__always_inline__))\n+_mm_testc_si128 (__m128i __M, __m128i __V)\n+{\n+  return __builtin_ia32_ptestc128 ((__v2di)__M, (__v2di)__V);\n+}\n+\n+/* Packed integer 128-bit bitwise comparison. Return 1 if\n+   (__V & __M) != 0 && (__V & ~__M) != 0.  */\n+static __inline int __attribute__((__always_inline__))\n+_mm_testnzc_si128 (__m128i __M, __m128i __V)\n+{\n+  return __builtin_ia32_ptestnzc128 ((__v2di)__M, (__v2di)__V);\n+}\n+\n+/* Macros for packed integer 128-bit comparison intrinsics.  */\n+#define _mm_test_all_zeros(M, V) _mm_testz_si128 ((M), (V))\n+\n+#define _mm_test_all_ones(V) \\\n+  _mm_testc_si128 ((V), _mm_cmpeq_epi32 ((V), (V)))\n+\n+#define _mm_test_mix_ones_zeros(M, V) _mm_testnzc_si128 ((M), (V))\n+\n+/* Insert single precision float into packed single precision array\n+   element selected by index N.  The bits [7-6] of N define S\n+   index, the bits [5-4] define D index, and bits [3-0] define\n+   zeroing mask for D.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline __m128 __attribute__((__always_inline__))\n+_mm_insert_ps (__m128 __D, __m128 __S, const int __N)\n+{\n+  return (__m128) __builtin_ia32_insertps128 ((__v4sf)__D,\n+\t\t\t\t\t      (__v4sf)__S,\n+\t\t\t\t\t      __N);\n+}\n+#else\n+#define _mm_insert_ps(D, S, N) \\\n+  ((__m128) __builtin_ia32_insertps128 ((__v4sf)(D), (__v4sf)(S), (N)))\n+#endif\n+\n+/* Helper macro to create the N value for _mm_insert_ps.  */\n+#define _MM_MK_INSERTPS_NDX(S, D, M) (((S) << 6) | ((D) << 4) | (M))\n+\n+/* Extract binary representation of single precision float from packed\n+   single precision array element of X selected by index N.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline int __attribute__((__always_inline__))\n+_mm_extract_ps (__m128 __X, const int __N)\n+{\n+  union { int i; float f; } __tmp;\n+  __tmp.f = __builtin_ia32_vec_ext_v4sf ((__v4sf)__X, __N);\n+  return __tmp.i;\n+}\n+#else\n+#define _mm_extract_ps(X, N) \\\n+  (__extension__ \t\t\t\t\t\t\\\n+   ({\t\t\t\t\t\t\t\t\\\n+      union { int i; float f; } __tmp;\t\t\t\t\\\n+      __tmp.f = __builtin_ia32_vec_ext_v4sf ((__v4sf)(X), (N));\t\\\n+      __tmp.i;\t\t\t\t\t\t\t\\\n+    })\t\t\t\t\t\t\t\t\\\n+   )\n+#endif\n+\n+/* Extract binary representation of single precision float into\n+   D from packed single precision array element of S selected\n+   by index N.  */\n+#define _MM_EXTRACT_FLOAT(D, S, N) \\\n+  { (D) = __builtin_ia32_vec_ext_v4sf ((__v4sf)(S), (N)); }\n+  \n+/* Extract specified single precision float element into the lower\n+   part of __m128.  */\n+#define _MM_PICK_OUT_PS(X, N)\t\t\t\t\\\n+  _mm_insert_ps (_mm_setzero_ps (), (X), \t\t\\\n+\t\t _MM_MK_INSERTPS_NDX ((N), 0, 0x0e))\n+\n+/* Insert integer, S, into packed integer array element of D\n+   selected by index N.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_insert_epi8 (__m128i __D, int __S, const int __N)\n+{\n+  return (__m128i) __builtin_ia32_vec_set_v16qi ((__v16qi)__D,\n+\t\t\t\t\t\t __S, __N);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_insert_epi32 (__m128i __D, int __S, const int __N)\n+{\n+  return (__m128i) __builtin_ia32_vec_set_v4si ((__v4si)__D,\n+\t\t\t\t\t\t __S, __N);\n+}\n+\n+#ifdef __x86_64__\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_insert_epi64 (__m128i __D, long long __S, const int __N)\n+{\n+  return (__m128i) __builtin_ia32_vec_set_v2di ((__v2di)__D,\n+\t\t\t\t\t\t __S, __N);\n+}\n+#endif\n+#else\n+#define _mm_insert_epi8(D, S, N) \\\n+  ((__m128i) __builtin_ia32_vec_set_v16qi ((__v16qi)(D), (S), (N)))\n+\n+#define _mm_insert_epi32(D, S, N) \\\n+  ((__m128i) __builtin_ia32_vec_set_v4si ((__v4si)(D), (S), (N)))\n+\n+#ifdef __x86_64__\n+#define _mm_insert_epi64(D, S, N) \\\n+  ((__m128i) __builtin_ia32_vec_set_v2di ((__v2di)(D), (S), (N)))\n+#endif\n+#endif\n+\n+/* Extract integer from packed integer array element of X selected by\n+   index N.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline int __attribute__((__always_inline__))\n+_mm_extract_epi8 (__m128i __X, const int __N)\n+{\n+   return __builtin_ia32_vec_ext_v16qi ((__v16qi)__X, __N);\n+}\n+\n+static __inline int __attribute__((__always_inline__))\n+_mm_extract_epi32 (__m128i __X, const int __N)\n+{\n+   return __builtin_ia32_vec_ext_v4si ((__v4si)__X, __N);\n+}\n+\n+#ifdef __x86_64__\n+static __inline long long  __attribute__((__always_inline__))\n+_mm_extract_epi64 (__m128i __X, const int __N)\n+{\n+  return __builtin_ia32_vec_ext_v2di ((__v2di)__X, __N);\n+}\n+#endif\n+#else\n+#define _mm_extract_epi8(X, N) \\\n+  __builtin_ia32_vec_ext_v16qi ((__v16qi) X, (N))\n+#define _mm_extract_epi32(X, N) \\\n+  __builtin_ia32_vec_ext_v4si ((__v4si) X, (N))\n+\n+#ifdef __x86_64__\n+#define _mm_extract_epi64(X, N) \\\n+  ((long long) __builtin_ia32_vec_ext_v2di ((__v2di)(X), (N)))\n+#endif\n+#endif\n+\n+/* Return horizontal packed word minimum and its index in bits [15:0]\n+   and bits [18:16] respectively.  */\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_minpos_epu16 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_phminposuw128 ((__v8hi)__X);\n+}\n+\n+/* Packed/scalar double precision floating point rounding.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline __m128d __attribute__((__always_inline__))\n+_mm_round_pd (__m128d __V, const int __M)\n+{\n+  return (__m128d) __builtin_ia32_roundpd ((__v2df)__V, __M);\n+}\n+\n+static __inline __m128d __attribute__((__always_inline__))\n+_mm_round_sd(__m128d __D, __m128d __V, const int __M)\n+{\n+  return (__m128d) __builtin_ia32_roundsd ((__v2df)__D,\n+\t\t\t\t\t   (__v2df)__V,\n+\t\t\t\t\t   __M);\n+}\n+#else\n+#define _mm_round_pd(V, M) \\\n+  ((__m128d) __builtin_ia32_roundpd ((__v2df)(V), (M)))\n+\n+#define _mm_round_sd(D, V, M) \\\n+  ((__m128d) __builtin_ia32_roundsd ((__v2df)(D), (__v2df)(V), (M)))\n+#endif\n+\n+/* Packed/scalar single precision floating point rounding.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline __m128 __attribute__((__always_inline__))\n+_mm_round_ps (__m128 __V, const int __M)\n+{\n+  return (__m128) __builtin_ia32_roundps ((__v4sf)__V, __M);\n+}\n+\n+static __inline __m128 __attribute__((__always_inline__))\n+_mm_round_ss (__m128 __D, __m128 __V, const int __M)\n+{\n+  return (__m128) __builtin_ia32_roundss ((__v4sf)__D,\n+\t\t\t\t\t  (__v4sf)__V,\n+\t\t\t\t\t  __M);\n+}\n+#else\n+#define _mm_round_ps(V, M) \\\n+  ((__m128) __builtin_ia32_roundps ((__v4sf)(V), (M)))\n+\n+#define _mm_round_ss(D, V, M) \\\n+  ((__m128) __builtin_ia32_roundss ((__v4sf)(D), (__v4sf)(V), (M)))\n+#endif\n+\n+/* Macros for ceil/floor intrinsics.  */\n+#define _mm_ceil_pd(V)\t   _mm_round_pd ((V), _MM_FROUND_CEIL)\n+#define _mm_ceil_sd(D, V)  _mm_round_sd ((D), (V), _MM_FROUND_CEIL)\n+\n+#define _mm_floor_pd(V)\t   _mm_round_pd((V), _MM_FROUND_FLOOR)\n+#define _mm_floor_sd(D, V) _mm_round_sd ((D), (V), _MM_FROUND_FLOOR)\n+\n+#define _mm_ceil_ps(V)\t   _mm_round_ps ((V), _MM_FROUND_CEIL)\n+#define _mm_ceil_ss(D, V)  _mm_round_ss ((D), (V), _MM_FROUND_CEIL)\n+\n+#define _mm_floor_ps(V)\t   _mm_round_ps ((V), _MM_FROUND_FLOOR)\n+#define _mm_floor_ss(D, V) _mm_round_ss ((D), (V), _MM_FROUND_FLOOR)\n+\n+/* Packed integer sign-extension.  */\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepi8_epi32 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovsxbd128 ((__v16qi)__X);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepi16_epi32 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovsxwd128 ((__v8hi)__X);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepi8_epi64 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovsxbq128 ((__v16qi)__X);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepi32_epi64 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovsxdq128 ((__v4si)__X);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepi16_epi64 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovsxwq128 ((__v8hi)__X);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepi8_epi16 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovsxbw128 ((__v16qi)__X);\n+}\n+\n+/* Packed integer zero-extension. */\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepu8_epi32 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovzxbd128 ((__v16qi)__X);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepu16_epi32 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovzxwd128 ((__v8hi)__X);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepu8_epi64 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovzxbq128 ((__v16qi)__X);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepu32_epi64 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovzxdq128 ((__v4si)__X);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepu16_epi64 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovzxwq128 ((__v8hi)__X);\n+}\n+\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_cvtepu8_epi16 (__m128i __X)\n+{\n+  return (__m128i) __builtin_ia32_pmovzxbw128 ((__v16qi)__X);\n+}\n+\n+/* Pack 8 double words from 2 operands into 8 words of result with\n+   unsigned saturation. */\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_packus_epi32 (__m128i __X, __m128i __Y)\n+{\n+  return (__m128i) __builtin_ia32_packusdw128 ((__v4si)__X, (__v4si)__Y);\n+}\n+\n+/* Sum absolute 8-bit integer difference of adjacent groups of 4\n+   byte integers in the first 2 operands.  Starting offsets within\n+   operands are determined by the 3rd mask operand.  */\n+\n+#ifdef __OPTIMIZE__\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_mpsadbw_epu8 (__m128i __X, __m128i __Y, const int __M)\n+{\n+  return (__m128i) __builtin_ia32_mpsadbw128 ((__v16qi)__X,\n+\t\t\t\t\t      (__v16qi)__Y, __M);\n+}\n+#else\n+#define _mm_mpsadbw_epu8(X, Y, M) \\\n+  ((__m128i) __builtin_ia32_mpsadbw128 ((__v16qi)(X), (__v16qi)(Y), (M)))\n+#endif\n+\n+/* Load double quadword using non-temporal aligned hint.  */\n+static __inline __m128i __attribute__((__always_inline__))\n+_mm_stream_load_si128 (__m128i *__X)\n+{\n+  return (__m128i) __builtin_ia32_movntdqa ((__v2di *) __X);\n+}\n+\n+#endif /* __SSE4_1__ */\n+\n+#endif /* _SMMINTRIN_H_INCLUDED */"}, {"sha": "661f5bb5c8db3045e3f71591e7ad90f7e0e05a5c", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 928, "deletions": 92, "changes": 1020, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -1379,6 +1379,35 @@\n   [(set_attr \"type\" \"ssemov\")\n    (set_attr \"mode\" \"SF\")])\n \n+;; A subset is vec_setv4sf.\n+(define_insn \"*vec_setv4sf_sse4_1\"\n+  [(set (match_operand:V4SF 0 \"register_operand\" \"=x\")\n+\t(vec_merge:V4SF\n+\t  (vec_duplicate:V4SF\n+\t    (match_operand:SF 2 \"nonimmediate_operand\" \"xm\"))\n+\t  (match_operand:V4SF 1 \"register_operand\" \"0\")\n+\t  (match_operand:SI 3 \"const_pow2_1_to_8_operand\" \"n\")))]\n+  \"TARGET_SSE4_1\"\n+{\n+  operands[3] = GEN_INT (exact_log2 (INTVAL (operands[3])) << 4);\n+  return \"insertps\\t{%3, %2, %0|%0, %2, %3}\";\n+}\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V4SF\")])\n+\n+(define_insn \"sse4_1_insertps\"\n+  [(set (match_operand:V4SF 0 \"register_operand\" \"=x\")\n+\t(unspec:V4SF [(match_operand:V4SF 2 \"register_operand\" \"x\")\n+\t\t      (match_operand:V4SF 1 \"register_operand\" \"0\")\n+\t\t      (match_operand:SI 3 \"const_0_to_255_operand\" \"n\")]\n+\t\t     UNSPEC_INSERTPS))]\n+  \"TARGET_SSE4_1\"\n+  \"insertps\\t{%3, %2, %0|%0, %2, %3}\";\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V4SF\")])\n+\n (define_split\n   [(set (match_operand:V4SF 0 \"memory_operand\" \"\")\n \t(vec_merge:V4SF\n@@ -1423,6 +1452,17 @@\n   DONE;\n })\n \n+(define_insn \"*sse4_1_extractps\"\n+  [(set (match_operand:SF 0 \"register_operand\" \"=rm\")\n+\t(vec_select:SF\n+\t  (match_operand:V4SF 1 \"register_operand\" \"x\")\n+\t  (parallel [(match_operand:SI 2 \"const_0_to_3_operand\" \"n\")])))]\n+  \"TARGET_SSE4_1\"\n+  \"extractps\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V4SF\")])\n+\n (define_expand \"vec_extractv4sf\"\n   [(match_operand:SF 0 \"register_operand\" \"\")\n    (match_operand:V4SF 1 \"register_operand\" \"\")\n@@ -2880,6 +2920,23 @@\n    (set_attr \"prefix_data16\" \"1\")\n    (set_attr \"mode\" \"TI\")])\n \n+(define_insn \"sse4_1_mulv2siv2di3\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(mult:V2DI\n+\t  (sign_extend:V2DI\n+\t    (vec_select:V2SI\n+\t      (match_operand:V4SI 1 \"nonimmediate_operand\" \"%0\")\n+\t      (parallel [(const_int 0) (const_int 2)])))\n+\t  (sign_extend:V2DI\n+\t    (vec_select:V2SI\n+\t      (match_operand:V4SI 2 \"nonimmediate_operand\" \"xm\")\n+\t      (parallel [(const_int 0) (const_int 2)])))))]\n+  \"TARGET_SSE4_1 && ix86_binary_operator_ok (MULT, V4SImode, operands)\"\n+  \"pmuldq\\t{%2, %0|%0, %2}\"\n+  [(set_attr \"type\" \"sseimul\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_insn \"sse2_pmaddwd\"\n   [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n \t(plus:V4SI\n@@ -2923,46 +2980,64 @@\n \t\t   (match_operand:V4SI 2 \"register_operand\" \"\")))]\n   \"TARGET_SSE2\"\n {\n-  rtx t1, t2, t3, t4, t5, t6, thirtytwo;\n-  rtx op0, op1, op2;\n-\n-  op0 = operands[0];\n-  op1 = operands[1];\n-  op2 = operands[2];\n-  t1 = gen_reg_rtx (V4SImode);\n-  t2 = gen_reg_rtx (V4SImode);\n-  t3 = gen_reg_rtx (V4SImode);\n-  t4 = gen_reg_rtx (V4SImode);\n-  t5 = gen_reg_rtx (V4SImode);\n-  t6 = gen_reg_rtx (V4SImode);\n-  thirtytwo = GEN_INT (32);\n-\n-  /* Multiply elements 2 and 0.  */\n-  emit_insn (gen_sse2_umulv2siv2di3 (gen_lowpart (V2DImode, t1), op1, op2));\n-\n-  /* Shift both input vectors down one element, so that elements 3 and 1\n-     are now in the slots for elements 2 and 0.  For K8, at least, this is\n-     faster than using a shuffle.  */\n-  emit_insn (gen_sse2_lshrti3 (gen_lowpart (TImode, t2),\n-\t\t\t       gen_lowpart (TImode, op1), thirtytwo));\n-  emit_insn (gen_sse2_lshrti3 (gen_lowpart (TImode, t3),\n-\t\t\t       gen_lowpart (TImode, op2), thirtytwo));\n-\n-  /* Multiply elements 3 and 1.  */\n-  emit_insn (gen_sse2_umulv2siv2di3 (gen_lowpart (V2DImode, t4), t2, t3));\n-\n-  /* Move the results in element 2 down to element 1; we don't care what\n-     goes in elements 2 and 3.  */\n-  emit_insn (gen_sse2_pshufd_1 (t5, t1, const0_rtx, const2_rtx,\n-\t\t\t\tconst0_rtx, const0_rtx));\n-  emit_insn (gen_sse2_pshufd_1 (t6, t4, const0_rtx, const2_rtx,\n+  if (TARGET_SSE4_1)\n+    ix86_fixup_binary_operands_no_copy (MULT, V4SImode, operands);\n+ else\n+   {\n+     rtx t1, t2, t3, t4, t5, t6, thirtytwo;\n+     rtx op0, op1, op2;\n+\n+     op0 = operands[0];\n+     op1 = operands[1];\n+     op2 = operands[2];\n+     t1 = gen_reg_rtx (V4SImode);\n+     t2 = gen_reg_rtx (V4SImode);\n+     t3 = gen_reg_rtx (V4SImode);\n+     t4 = gen_reg_rtx (V4SImode);\n+     t5 = gen_reg_rtx (V4SImode);\n+     t6 = gen_reg_rtx (V4SImode);\n+     thirtytwo = GEN_INT (32);\n+\n+     /* Multiply elements 2 and 0.  */\n+     emit_insn (gen_sse2_umulv2siv2di3 (gen_lowpart (V2DImode, t1),\n+\t\t\t\t\top1, op2));\n+\n+     /* Shift both input vectors down one element, so that elements 3\n+\tand 1 are now in the slots for elements 2 and 0.  For K8, at\n+\tleast, this is faster than using a shuffle.  */\n+     emit_insn (gen_sse2_lshrti3 (gen_lowpart (TImode, t2),\n+\t\t\t\t  gen_lowpart (TImode, op1),\n+\t\t\t\t  thirtytwo));\n+     emit_insn (gen_sse2_lshrti3 (gen_lowpart (TImode, t3),\n+\t\t\t\t  gen_lowpart (TImode, op2),\n+\t\t\t\t  thirtytwo)); \n+     /* Multiply elements 3 and 1.  */\n+     emit_insn (gen_sse2_umulv2siv2di3 (gen_lowpart (V2DImode, t4),\n+\t\t\t\t\tt2, t3));\n+\n+     /* Move the results in element 2 down to element 1; we don't care\n+\twhat goes in elements 2 and 3.  */\n+     emit_insn (gen_sse2_pshufd_1 (t5, t1, const0_rtx, const2_rtx,\n \t\t\t\tconst0_rtx, const0_rtx));\n+     emit_insn (gen_sse2_pshufd_1 (t6, t4, const0_rtx, const2_rtx,\n+\t\t\t\t   const0_rtx, const0_rtx));\n \n-  /* Merge the parts back together.  */\n-  emit_insn (gen_sse2_punpckldq (op0, t5, t6));\n-  DONE;\n+    /* Merge the parts back together.  */\n+     emit_insn (gen_sse2_punpckldq (op0, t5, t6));\n+     DONE;\n+   }\n })\n \n+(define_insn \"*sse4_1_mulv4si3\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n+\t(mult:V4SI (match_operand:V4SI 1 \"nonimmediate_operand\" \"%0\")\n+\t\t   (match_operand:V4SI 2 \"nonimmediate_operand\" \"xm\")))]\n+  \"TARGET_SSE4_1 && ix86_binary_operator_ok (MULT, V4SImode, operands)\"\n+  \"pmulld\\t{%2, %0|%0, %2}\"\n+  [(set_attr \"type\" \"sseimul\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_expand \"mulv2di3\"\n   [(set (match_operand:V2DI 0 \"register_operand\" \"\")\n \t(mult:V2DI (match_operand:V2DI 1 \"register_operand\" \"\")\n@@ -3323,16 +3398,22 @@\n    (set_attr \"mode\" \"TI\")])\n \n (define_expand \"umaxv8hi3\"\n-  [(set (match_operand:V8HI 0 \"register_operand\" \"=x\")\n-\t(us_minus:V8HI (match_operand:V8HI 1 \"register_operand\" \"0\")\n-\t\t       (match_operand:V8HI 2 \"nonimmediate_operand\" \"xm\")))\n-   (set (match_dup 3)\n-\t(plus:V8HI (match_dup 0) (match_dup 2)))]\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"\")\n+\t(umax:V8HI (match_operand:V8HI 1 \"register_operand\" \"\")\n+\t\t   (match_operand:V8HI 2 \"nonimmediate_operand\" \"\")))]\n   \"TARGET_SSE2\"\n {\n-  operands[3] = operands[0];\n-  if (rtx_equal_p (operands[0], operands[2]))\n-    operands[0] = gen_reg_rtx (V8HImode);\n+  if (TARGET_SSE4_1)\n+    ix86_fixup_binary_operands_no_copy (UMAX, V8HImode, operands);\n+  else\n+    {\n+      rtx op0 = operands[0], op2 = operands[2], op3 = op0;\n+      if (rtx_equal_p (op3, op2))\n+\top3 = gen_reg_rtx (V8HImode);\n+      emit_insn (gen_sse2_ussubv8hi3 (op3, operands[1], op2));\n+      emit_insn (gen_addv8hi3 (op0, op3, op2));\n+      DONE;\n+    }\n })\n \n (define_expand \"smax<mode>3\"\n@@ -3341,40 +3422,72 @@\n \t\t\t(match_operand:SSEMODE14 2 \"register_operand\" \"\")))]\n   \"TARGET_SSE2\"\n {\n-  rtx xops[6];\n-  bool ok;\n-\n-  xops[0] = operands[0];\n-  xops[1] = operands[1];\n-  xops[2] = operands[2];\n-  xops[3] = gen_rtx_GT (VOIDmode, operands[1], operands[2]);\n-  xops[4] = operands[1];\n-  xops[5] = operands[2];\n-  ok = ix86_expand_int_vcond (xops);\n-  gcc_assert (ok);\n-  DONE;\n+  if (TARGET_SSE4_1)\n+    ix86_fixup_binary_operands_no_copy (SMAX, <MODE>mode, operands);\n+  else\n+  {\n+    rtx xops[6];\n+    bool ok;\n+\n+    xops[0] = operands[0];\n+    xops[1] = operands[1];\n+    xops[2] = operands[2];\n+    xops[3] = gen_rtx_GT (VOIDmode, operands[1], operands[2]);\n+    xops[4] = operands[1];\n+    xops[5] = operands[2];\n+    ok = ix86_expand_int_vcond (xops);\n+    gcc_assert (ok);\n+    DONE;\n+  }\n })\n \n+(define_insn \"*sse4_1_smax<mode>3\"\n+  [(set (match_operand:SSEMODE14 0 \"register_operand\" \"=x\")\n+\t(smax:SSEMODE14\n+\t  (match_operand:SSEMODE14 1 \"nonimmediate_operand\" \"%0\")\n+\t  (match_operand:SSEMODE14 2 \"nonimmediate_operand\" \"xm\")))]\n+  \"TARGET_SSE4_1 && ix86_binary_operator_ok (SMAX, <MODE>mode, operands)\"\n+  \"pmaxs<ssevecsize>\\t{%2, %0|%0, %2}\"\n+  [(set_attr \"type\" \"sseiadd\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_expand \"umaxv4si3\"\n   [(set (match_operand:V4SI 0 \"register_operand\" \"\")\n \t(umax:V4SI (match_operand:V4SI 1 \"register_operand\" \"\")\n \t\t   (match_operand:V4SI 2 \"register_operand\" \"\")))]\n   \"TARGET_SSE2\"\n {\n-  rtx xops[6];\n-  bool ok;\n-\n-  xops[0] = operands[0];\n-  xops[1] = operands[1];\n-  xops[2] = operands[2];\n-  xops[3] = gen_rtx_GTU (VOIDmode, operands[1], operands[2]);\n-  xops[4] = operands[1];\n-  xops[5] = operands[2];\n-  ok = ix86_expand_int_vcond (xops);\n-  gcc_assert (ok);\n-  DONE;\n+  if (TARGET_SSE4_1)\n+    ix86_fixup_binary_operands_no_copy (UMAX, V4SImode, operands);\n+  else\n+  {\n+    rtx xops[6];\n+    bool ok;\n+\n+    xops[0] = operands[0];\n+    xops[1] = operands[1];\n+    xops[2] = operands[2];\n+    xops[3] = gen_rtx_GTU (VOIDmode, operands[1], operands[2]);\n+    xops[4] = operands[1];\n+    xops[5] = operands[2];\n+    ok = ix86_expand_int_vcond (xops);\n+    gcc_assert (ok);\n+    DONE;\n+  }\n })\n \n+(define_insn \"*sse4_1_umax<mode>3\"\n+  [(set (match_operand:SSEMODE24 0 \"register_operand\" \"=x\")\n+\t(umax:SSEMODE24\n+\t  (match_operand:SSEMODE24 1 \"nonimmediate_operand\" \"%0\")\n+\t  (match_operand:SSEMODE24 2 \"nonimmediate_operand\" \"xm\")))]\n+  \"TARGET_SSE4_1 && ix86_binary_operator_ok (UMAX, <MODE>mode, operands)\"\n+  \"pmaxu<ssevecsize>\\t{%2, %0|%0, %2}\"\n+  [(set_attr \"type\" \"sseiadd\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_expand \"uminv16qi3\"\n   [(set (match_operand:V16QI 0 \"register_operand\" \"\")\n \t(umin:V16QI (match_operand:V16QI 1 \"nonimmediate_operand\" \"\")\n@@ -3415,40 +3528,72 @@\n \t\t\t(match_operand:SSEMODE14 2 \"register_operand\" \"\")))]\n   \"TARGET_SSE2\"\n {\n-  rtx xops[6];\n-  bool ok;\n-\n-  xops[0] = operands[0];\n-  xops[1] = operands[2];\n-  xops[2] = operands[1];\n-  xops[3] = gen_rtx_GT (VOIDmode, operands[1], operands[2]);\n-  xops[4] = operands[1];\n-  xops[5] = operands[2];\n-  ok = ix86_expand_int_vcond (xops);\n-  gcc_assert (ok);\n-  DONE;\n+  if (TARGET_SSE4_1)\n+    ix86_fixup_binary_operands_no_copy (SMIN, <MODE>mode, operands);\n+  else\n+    {\n+      rtx xops[6];\n+      bool ok;\n+\n+      xops[0] = operands[0];\n+      xops[1] = operands[2];\n+      xops[2] = operands[1];\n+      xops[3] = gen_rtx_GT (VOIDmode, operands[1], operands[2]);\n+      xops[4] = operands[1];\n+      xops[5] = operands[2];\n+      ok = ix86_expand_int_vcond (xops);\n+      gcc_assert (ok);\n+      DONE;\n+    }\n })\n \n+(define_insn \"*sse4_1_smin<mode>3\"\n+  [(set (match_operand:SSEMODE14 0 \"register_operand\" \"=x\")\n+\t(smin:SSEMODE14\n+\t  (match_operand:SSEMODE14 1 \"nonimmediate_operand\" \"%0\")\n+\t  (match_operand:SSEMODE14 2 \"nonimmediate_operand\" \"xm\")))]\n+  \"TARGET_SSE4_1 && ix86_binary_operator_ok (SMIN, <MODE>mode, operands)\"\n+  \"pmins<ssevecsize>\\t{%2, %0|%0, %2}\"\n+  [(set_attr \"type\" \"sseiadd\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_expand \"umin<mode>3\"\n   [(set (match_operand:SSEMODE24 0 \"register_operand\" \"\")\n \t(umin:SSEMODE24 (match_operand:SSEMODE24 1 \"register_operand\" \"\")\n \t\t\t(match_operand:SSEMODE24 2 \"register_operand\" \"\")))]\n   \"TARGET_SSE2\"\n {\n-  rtx xops[6];\n-  bool ok;\n-\n-  xops[0] = operands[0];\n-  xops[1] = operands[2];\n-  xops[2] = operands[1];\n-  xops[3] = gen_rtx_GTU (VOIDmode, operands[1], operands[2]);\n-  xops[4] = operands[1];\n-  xops[5] = operands[2];\n-  ok = ix86_expand_int_vcond (xops);\n-  gcc_assert (ok);\n-  DONE;\n+  if (TARGET_SSE4_1)\n+    ix86_fixup_binary_operands_no_copy (UMIN, <MODE>mode, operands);\n+  else\n+    {\n+      rtx xops[6];\n+      bool ok;\n+\n+      xops[0] = operands[0];\n+      xops[1] = operands[2];\n+      xops[2] = operands[1];\n+      xops[3] = gen_rtx_GTU (VOIDmode, operands[1], operands[2]);\n+      xops[4] = operands[1];\n+      xops[5] = operands[2];\n+      ok = ix86_expand_int_vcond (xops);\n+      gcc_assert (ok);\n+      DONE;\n+    }\n })\n \n+(define_insn \"*sse4_1_umin<mode>3\"\n+  [(set (match_operand:SSEMODE24 0 \"register_operand\" \"=x\")\n+\t(umin:SSEMODE24\n+\t  (match_operand:SSEMODE24 1 \"nonimmediate_operand\" \"%0\")\n+\t  (match_operand:SSEMODE24 2 \"nonimmediate_operand\" \"xm\")))]\n+  \"TARGET_SSE4_1 && ix86_binary_operator_ok (UMIN, <MODE>mode, operands)\"\n+  \"pminu<ssevecsize>\\t{%2, %0|%0, %2}\"\n+  [(set_attr \"type\" \"sseiadd\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n ;;\n ;; Parallel integral comparisons\n@@ -3466,6 +3611,17 @@\n    (set_attr \"prefix_data16\" \"1\")\n    (set_attr \"mode\" \"TI\")])\n \n+(define_insn \"sse4_1_eqv2di3\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(eq:V2DI\n+\t  (match_operand:V2DI 1 \"nonimmediate_operand\" \"%0\")\n+\t  (match_operand:V2DI 2 \"nonimmediate_operand\" \"xm\")))]\n+  \"TARGET_SSE4_1 && ix86_binary_operator_ok (EQ, V2DImode, operands)\"\n+  \"pcmpeqq\\t{%2, %0|%0, %2}\"\n+  [(set_attr \"type\" \"ssecmp\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_insn \"sse2_gt<mode>3\"\n   [(set (match_operand:SSEMODE124 0 \"register_operand\" \"=x\")\n \t(gt:SSEMODE124\n@@ -3989,6 +4145,22 @@\n    (set_attr \"prefix_data16\" \"1\")\n    (set_attr \"mode\" \"TI\")])\n \n+(define_insn \"*sse4_1_pinsrb\"\n+  [(set (match_operand:V16QI 0 \"register_operand\" \"=x\")\n+\t(vec_merge:V16QI\n+\t  (vec_duplicate:V16QI\n+\t    (match_operand:QI 2 \"nonimmediate_operand\" \"rm\"))\n+\t  (match_operand:V16QI 1 \"register_operand\" \"0\")\n+\t  (match_operand:SI 3 \"const_pow2_1_to_32768_operand\" \"n\")))]\n+  \"TARGET_SSE4_1\"\n+{\n+  operands[3] = GEN_INT (exact_log2 (INTVAL (operands[3])));\n+  return \"pinsrb\\t{%3, %k2, %0|%0, %k2, %3}\";\n+}\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_insn \"*sse2_pinsrw\"\n   [(set (match_operand:V8HI 0 \"register_operand\" \"=x\")\n \t(vec_merge:V8HI\n@@ -4005,6 +4177,62 @@\n    (set_attr \"prefix_data16\" \"1\")\n    (set_attr \"mode\" \"TI\")])\n \n+;; It must come before sse2_loadld since it is preferred.\n+(define_insn \"*sse4_1_pinsrd\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n+\t(vec_merge:V4SI\n+\t  (vec_duplicate:V4SI\n+\t    (match_operand:SI 2 \"nonimmediate_operand\" \"rm\"))\n+\t  (match_operand:V4SI 1 \"register_operand\" \"0\")\n+\t  (match_operand:SI 3 \"const_pow2_1_to_8_operand\" \"n\")))]\n+  \"TARGET_SSE4_1\"\n+{\n+  operands[3] = GEN_INT (exact_log2 (INTVAL (operands[3])));\n+  return \"pinsrd\\t{%3, %2, %0|%0, %2, %3}\";\n+}\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_pinsrq\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(vec_merge:V2DI\n+\t  (vec_duplicate:V2DI\n+\t    (match_operand:DI 2 \"nonimmediate_operand\" \"rm\"))\n+\t  (match_operand:V2DI 1 \"register_operand\" \"0\")\n+\t  (match_operand:SI 3 \"const_pow2_1_to_2_operand\" \"n\")))]\n+  \"TARGET_SSE4_1\"\n+{\n+  operands[3] = GEN_INT (exact_log2 (INTVAL (operands[3])));\n+  return \"pinsrq\\t{%3, %2, %0|%0, %2, %3}\";\n+}\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_pextrb\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=r\")\n+\t(zero_extend:SI\n+\t  (vec_select:QI\n+\t    (match_operand:V16QI 1 \"register_operand\" \"x\")\n+\t    (parallel [(match_operand:SI 2 \"const_0_to_15_operand\" \"n\")]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pextrb\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_pextrb_memory\"\n+  [(set (match_operand:QI 0 \"memory_operand\" \"=m\")\n+\t(vec_select:QI\n+\t  (match_operand:V16QI 1 \"register_operand\" \"x\")\n+\t  (parallel [(match_operand:SI 2 \"const_0_to_15_operand\" \"n\")])))]\n+  \"TARGET_SSE4_1\"\n+  \"pextrb\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_insn \"*sse2_pextrw\"\n   [(set (match_operand:SI 0 \"register_operand\" \"=r\")\n \t(zero_extend:SI\n@@ -4017,6 +4245,40 @@\n    (set_attr \"prefix_data16\" \"1\")\n    (set_attr \"mode\" \"TI\")])\n \n+(define_insn \"*sse4_1_pextrw_memory\"\n+  [(set (match_operand:HI 0 \"memory_operand\" \"=m\")\n+\t(vec_select:HI\n+\t  (match_operand:V8HI 1 \"register_operand\" \"x\")\n+\t  (parallel [(match_operand:SI 2 \"const_0_to_7_operand\" \"n\")])))]\n+  \"TARGET_SSE4_1\"\n+  \"pextrw\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_pextrd\"\n+  [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=rm\")\n+\t(vec_select:SI\n+\t  (match_operand:V4SI 1 \"register_operand\" \"x\")\n+\t  (parallel [(match_operand:SI 2 \"const_0_to_3_operand\" \"n\")])))]\n+  \"TARGET_SSE4_1\"\n+  \"pextrd\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+;; It must come before *vec_extractv2di_1_sse since it is preferred.\n+(define_insn \"*sse4_1_pextrq\"\n+  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=rm\")\n+\t(vec_select:DI\n+\t  (match_operand:V2DI 1 \"register_operand\" \"x\")\n+\t  (parallel [(match_operand:SI 2 \"const_0_to_1_operand\" \"n\")])))]\n+  \"TARGET_SSE4_1 && TARGET_64BIT\"\n+  \"pextrq\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_expand \"sse2_pshufd\"\n   [(match_operand:V4SI 0 \"register_operand\" \"\")\n    (match_operand:V4SI 1 \"nonimmediate_operand\" \"\")\n@@ -5500,3 +5762,577 @@\n   [(set_attr \"type\" \"sseins\")\n    (set_attr \"prefix_rep\" \"1\")\n    (set_attr \"mode\" \"TI\")])\n+\n+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n+;;\n+;; Intel SSE4.1 instructions\n+;;\n+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n+\n+(define_insn \"sse4_1_blendpd\"\n+  [(set (match_operand:V2DF 0 \"register_operand\" \"=x\")\n+\t(vec_merge:V2DF\n+\t  (match_operand:V2DF 2 \"nonimmediate_operand\" \"xm\")\n+\t  (match_operand:V2DF 1 \"register_operand\" \"0\")\n+\t  (match_operand:SI 3 \"const_0_to_3_operand\" \"n\")))]\n+  \"TARGET_SSE4_1\"\n+  \"blendpd\\t{%3, %2, %0|%0, %2, %3}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V2DF\")])\n+\n+(define_insn \"sse4_1_blendps\"\n+  [(set (match_operand:V4SF 0 \"register_operand\" \"=x\")\n+\t(vec_merge:V4SF\n+\t  (match_operand:V4SF 2 \"nonimmediate_operand\" \"xm\")\n+\t  (match_operand:V4SF 1 \"register_operand\" \"0\")\n+\t  (match_operand:SI 3 \"const_0_to_15_operand\" \"n\")))]\n+  \"TARGET_SSE4_1\"\n+  \"blendps\\t{%3, %2, %0|%0, %2, %3}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V4SF\")])\n+\n+(define_insn \"sse4_1_blendvpd\"\n+  [(set (match_operand:V2DF 0 \"register_operand\" \"=x\")\n+\t(unspec:V2DF [(match_operand:V2DF 1 \"register_operand\"  \"0\")\n+\t\t      (match_operand:V2DF 2 \"nonimmediate_operand\" \"xm\")\n+\t\t      (reg:V2DF 21)]\n+\t\t     UNSPEC_BLENDV))]\n+  \"TARGET_SSE4_1\"\n+  \"blendvpd\\t{%%xmm0, %2, %0|%0, %2, %%xmm0}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V2DF\")])\n+\n+(define_insn \"sse4_1_blendvps\"\n+  [(set (match_operand:V4SF 0 \"register_operand\" \"=x\")\n+\t(unspec:V4SF [(match_operand:V4SF 1 \"register_operand\" \"0\")\n+\t\t      (match_operand:V4SF 2 \"nonimmediate_operand\" \"xm\")\n+\t\t      (reg:V4SF 21)]\n+\t\t     UNSPEC_BLENDV))]\n+  \"TARGET_SSE4_1\"\n+  \"blendvps\\t{%%xmm0, %2, %0|%0, %2, %%xmm0}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V4SF\")])\n+\n+(define_insn \"sse4_1_dppd\"\n+  [(set (match_operand:V2DF 0 \"register_operand\" \"=x\")\n+\t(unspec:V2DF [(match_operand:V2DF 1 \"nonimmediate_operand\" \"%0\")\n+\t\t      (match_operand:V2DF 2 \"nonimmediate_operand\" \"xm\")\n+\t\t      (match_operand:SI 3 \"const_0_to_255_operand\" \"n\")]\n+\t\t      UNSPEC_DP))]\n+  \"TARGET_SSE4_1\"\n+  \"dppd\\t{%3, %2, %0|%0, %2, %3}\"\n+  [(set_attr \"type\" \"ssemul\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V2DF\")])\n+\n+(define_insn \"sse4_1_dpps\"\n+  [(set (match_operand:V4SF 0 \"register_operand\" \"=x\")\n+\t(unspec:V4SF [(match_operand:V4SF 1 \"nonimmediate_operand\" \"%0\")\n+\t\t      (match_operand:V4SF 2 \"nonimmediate_operand\" \"xm\")\n+\t\t      (match_operand:SI 3 \"const_0_to_255_operand\" \"n\")]\n+\t\t     UNSPEC_DP))]\n+  \"TARGET_SSE4_1\"\n+  \"dpps\\t{%3, %2, %0|%0, %2, %3}\"\n+  [(set_attr \"type\" \"ssemul\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V4SF\")])\n+\n+(define_insn \"sse4_1_movntdqa\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(unspec:V2DI [(match_operand:V2DI 1 \"memory_operand\" \"m\")]\n+\t\t     UNSPEC_MOVNTDQA))]\n+  \"TARGET_SSE4_1\"\n+  \"movntdqa\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssecvt\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_mpsadbw\"\n+  [(set (match_operand:V16QI 0 \"register_operand\" \"=x\")\n+\t(unspec:V16QI [(match_operand:V16QI 1 \"register_operand\" \"0\")\n+\t\t       (match_operand:V16QI 2 \"nonimmediate_operand\" \"xm\")\n+\t\t       (match_operand:SI 3 \"const_0_to_255_operand\" \"n\")]\n+\t\t      UNSPEC_MPSADBW))]\n+  \"TARGET_SSE4_1\"\n+  \"mpsadbw\\t{%3, %2, %0|%0, %2, %3}\"\n+  [(set_attr \"type\" \"sselog1\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_packusdw\"\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x\")\n+\t(vec_concat:V8HI\n+\t  (us_truncate:V4HI\n+\t    (match_operand:V4SI 1 \"register_operand\" \"0\"))\n+\t  (us_truncate:V4HI\n+\t    (match_operand:V4SI 2 \"nonimmediate_operand\" \"xm\"))))]\n+  \"TARGET_SSE4_1\"\n+  \"packusdw\\t{%2, %0|%0, %2}\"\n+  [(set_attr \"type\" \"sselog\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_pblendvb\"\n+  [(set (match_operand:V16QI 0 \"register_operand\" \"=x\")\n+\t(unspec:V16QI [(match_operand:V16QI 1 \"register_operand\"  \"0\")\n+\t\t       (match_operand:V16QI 2 \"nonimmediate_operand\" \"xm\")\n+\t\t       (reg:V16QI 21)]\n+\t\t      UNSPEC_BLENDV))]\n+  \"TARGET_SSE4_1\"\n+  \"pblendvb\\t{%%xmm0, %2, %0|%0, %2, %%xmm0}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_pblendw\"\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x\")\n+\t(vec_merge:V8HI\n+\t  (match_operand:V8HI 2 \"nonimmediate_operand\" \"xm\")\n+\t  (match_operand:V8HI 1 \"register_operand\" \"0\")\n+\t  (match_operand:SI 3 \"const_0_to_255_operand\" \"n\")))]\n+  \"TARGET_SSE4_1\"\n+  \"pblendw\\t{%3, %2, %0|%0, %2, %3}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_phminposuw\"\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x\")\n+\t(unspec:V8HI [(match_operand:V8HI 1 \"nonimmediate_operand\" \"xm\")]\n+\t\t     UNSPEC_PHMINPOSUW))]\n+  \"TARGET_SSE4_1\"\n+  \"phminposuw\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"sselog1\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_extendv8qiv8hi2\"\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V8HI\n+\t  (vec_select:V8QI\n+\t    (match_operand:V16QI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)\n+\t\t       (const_int 4)\n+\t\t       (const_int 5)\n+\t\t       (const_int 6)\n+\t\t       (const_int 7)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxbw\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_extendv8qiv8hi2\"\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V8HI\n+\t  (vec_select:V8QI\n+\t    (vec_duplicate:V16QI\n+\t      (match_operand:V8QI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)\n+\t\t       (const_int 4)\n+\t\t       (const_int 5)\n+\t\t       (const_int 6)\n+\t\t       (const_int 7)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxbw\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_extendv4qiv4si2\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V4SI\n+\t  (vec_select:V4QI\n+\t    (match_operand:V16QI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxbd\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_extendv4qiv4si2\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V4SI\n+\t  (vec_select:V4QI\n+\t    (vec_duplicate:V16QI\n+\t      (match_operand:V4QI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxbd\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_extendv2qiv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V2DI\n+\t  (vec_select:V2QI\n+\t    (match_operand:V16QI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxbq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_extendv2qiv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V2DI\n+\t  (vec_select:V2QI\n+\t    (vec_duplicate:V16QI\n+\t      (match_operand:V2QI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxbq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_extendv4hiv4si2\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V4SI\n+\t  (vec_select:V4HI\n+\t    (match_operand:V8HI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxwd\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_extendv4hiv4si2\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V4SI\n+\t  (vec_select:V4HI\n+\t    (vec_duplicate:V8HI\n+\t      (match_operand:V2HI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxwd\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_extendv2hiv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V2DI\n+\t  (vec_select:V2HI\n+\t    (match_operand:V8HI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxwq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_extendv2hiv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V2DI\n+\t  (vec_select:V2HI\n+\t    (vec_duplicate:V8HI\n+\t      (match_operand:V8HI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxwq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_extendv2siv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V2DI\n+\t  (vec_select:V2SI\n+\t    (match_operand:V4SI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxdq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_extendv2siv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(sign_extend:V2DI\n+\t  (vec_select:V2SI\n+\t    (vec_duplicate:V4SI\n+\t      (match_operand:V2SI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovsxdq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_zero_extendv8qiv8hi2\"\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V8HI\n+\t  (vec_select:V8QI\n+\t    (match_operand:V16QI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)\n+\t\t       (const_int 4)\n+\t\t       (const_int 5)\n+\t\t       (const_int 6)\n+\t\t       (const_int 7)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxbw\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_zero_extendv8qiv8hi2\"\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V8HI\n+\t  (vec_select:V8QI\n+\t    (vec_duplicate:V16QI\n+\t      (match_operand:V8QI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)\n+\t\t       (const_int 4)\n+\t\t       (const_int 5)\n+\t\t       (const_int 6)\n+\t\t       (const_int 7)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxbw\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_zero_extendv4qiv4si2\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V4SI\n+\t  (vec_select:V4QI\n+\t    (match_operand:V16QI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxbd\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_zero_extendv4qiv4si2\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V4SI\n+\t  (vec_select:V4QI\n+\t    (vec_duplicate:V16QI\n+\t      (match_operand:V4QI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxbd\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_zero_extendv2qiv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V2DI\n+\t  (vec_select:V2QI\n+\t    (match_operand:V16QI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxbq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_zero_extendv2qiv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V2DI\n+\t  (vec_select:V2QI\n+\t    (vec_duplicate:V16QI\n+\t      (match_operand:V2QI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxbq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_zero_extendv4hiv4si2\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V4SI\n+\t  (vec_select:V4HI\n+\t    (match_operand:V8HI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxwd\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_zero_extendv4hiv4si2\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V4SI\n+\t  (vec_select:V4HI\n+\t    (vec_duplicate:V8HI\n+\t      (match_operand:V4HI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)\n+\t\t       (const_int 2)\n+\t\t       (const_int 3)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxwd\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_zero_extendv2hiv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V2DI\n+\t  (vec_select:V2HI\n+\t    (match_operand:V8HI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxwq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_zero_extendv2hiv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V2DI\n+\t  (vec_select:V2HI\n+\t    (vec_duplicate:V8HI\n+\t      (match_operand:V2HI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxwq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_zero_extendv2siv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V2DI\n+\t  (vec_select:V2SI\n+\t    (match_operand:V4SI 1 \"register_operand\" \"x\")\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxdq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"*sse4_1_zero_extendv2siv2di2\"\n+  [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n+\t(zero_extend:V2DI\n+\t  (vec_select:V2SI\n+\t    (vec_duplicate:V4SI\n+\t      (match_operand:V2SI 1 \"nonimmediate_operand\" \"xm\"))\n+\t    (parallel [(const_int 0)\n+\t\t       (const_int 1)]))))]\n+  \"TARGET_SSE4_1\"\n+  \"pmovzxdq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssemov\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+;; ptest is very similar to comiss and ucomiss when setting FLAGS_REG.\n+;; But it is not a really compare instruction.\n+(define_insn \"sse4_1_ptest\"\n+  [(set (reg:CC FLAGS_REG)\n+\t(unspec:CC [(match_operand:V2DI 0 \"register_operand\" \"x\")\n+\t\t    (match_operand:V2DI 1 \"nonimmediate_operand\" \"xm\")]\n+\t\t   UNSPEC_PTEST))]\n+  \"TARGET_SSE4_1\"\n+  \"ptest\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"ssecomi\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"TI\")])\n+\n+(define_insn \"sse4_1_roundpd\"\n+  [(set (match_operand:V2DF 0 \"register_operand\" \"=x\")\n+\t(unspec:V2DF [(match_operand:V2DF 1 \"nonimmediate_operand\" \"xm\")\n+\t\t      (match_operand:SI 2 \"const_0_to_15_operand\" \"n\")]\n+\t\t     UNSPEC_ROUNDP))]\n+  \"TARGET_SSE4_1\"\n+  \"roundpd\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"ssecvt\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V2DF\")])\n+\n+(define_insn \"sse4_1_roundps\"\n+  [(set (match_operand:V4SF 0 \"register_operand\" \"=x\")\n+\t(unspec:V4SF [(match_operand:V4SF 1 \"nonimmediate_operand\" \"xm\")\n+\t\t      (match_operand:SI 2 \"const_0_to_15_operand\" \"n\")]\n+\t\t     UNSPEC_ROUNDP))]\n+  \"TARGET_SSE4_1\"\n+  \"roundps\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"ssecvt\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V4SF\")])\n+\n+(define_insn \"sse4_1_roundsd\"\n+  [(set (match_operand:V2DF 0 \"register_operand\" \"=x\")\n+\t(vec_merge:V2DF\n+\t  (unspec:V2DF [(match_operand:V2DF 2 \"register_operand\" \"x\")\n+\t\t\t(match_operand:SI 3 \"const_0_to_15_operand\" \"n\")]\n+\t\t       UNSPEC_ROUNDS)\n+\t  (match_operand:V2DF 1 \"register_operand\" \"0\")\n+\t  (const_int 1)))]\n+  \"TARGET_SSE4_1\"\n+  \"roundsd\\t{%3, %2, %0|%0, %2, %3}\"\n+  [(set_attr \"type\" \"ssecvt\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V2DF\")])\n+\n+(define_insn \"sse4_1_roundss\"\n+  [(set (match_operand:V4SF 0 \"register_operand\" \"=x\")\n+\t(vec_merge:V4SF\n+\t  (unspec:V4SF [(match_operand:V4SF 2 \"register_operand\" \"x\")\n+\t\t\t(match_operand:SI 3 \"const_0_to_15_operand\" \"n\")]\n+\t\t       UNSPEC_ROUNDS)\n+\t  (match_operand:V4SF 1 \"register_operand\" \"0\")\n+\t  (const_int 1)))]\n+  \"TARGET_SSE4_1\"\n+  \"roundss\\t{%3, %2, %0|%0, %2, %3}\"\n+  [(set_attr \"type\" \"ssecvt\")\n+   (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"mode\" \"V4SF\")])"}, {"sha": "a09e4530977f61cb79e9ac6761016eee437f9c23", "filename": "gcc/doc/extend.texi", "status": "modified", "additions": 78, "deletions": 0, "changes": 78, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fdoc%2Fextend.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fdoc%2Fextend.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Fextend.texi?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -7396,6 +7396,84 @@ v4si __builtin_ia32_pabsd128 (v4si)\n v8hi __builtin_ia32_pabsw128 (v8hi)\n @end smallexample\n \n+The following built-in functions are available when @option{-msse4.1} is\n+used.  All of them generate the machine instruction that is part of the\n+name.\n+\n+@smallexample\n+v2df __builtin_ia32_blendpd (v2df, v2df, const int)\n+v4sf __builtin_ia32_blendps (v4sf, v4sf, const int)\n+v2df __builtin_ia32_blendvpd (v2df, v2df, v2df)\n+v4sf __builtin_ia32_blendvps (v4sf, v4sf, v4sf)\n+v2df __builtin_ia32_dppd (__v2df, __v2df, const int)\n+v4sf __builtin_ia32_dpps (v4sf, v4sf, const int)\n+v4sf __builtin_ia32_insertps128 (v4sf, v4sf, const int)\n+v2di __builtin_ia32_movntdqa (v2di *);\n+v16qi __builtin_ia32_mpsadbw128 (v16qi, v16qi, const int)\n+v8hi __builtin_ia32_packusdw128 (v4si, v4si)\n+v16qi __builtin_ia32_pblendvb128 (v16qi, v16qi, v16qi)\n+v8hi __builtin_ia32_pblendw128 (v8hi, v8hi, const int)\n+v2di __builtin_ia32_pcmpeqq (v2di, v2di)\n+v8hi __builtin_ia32_phminposuw128 (v8hi)\n+v16qi __builtin_ia32_pmaxsb128 (v16qi, v16qi)\n+v4si __builtin_ia32_pmaxsd128 (v4si, v4si)\n+v4si __builtin_ia32_pmaxud128 (v4si, v4si)\n+v8hi __builtin_ia32_pmaxuw128 (v8hi, v8hi)\n+v16qi __builtin_ia32_pminsb128 (v16qi, v16qi)\n+v4si __builtin_ia32_pminsd128 (v4si, v4si)\n+v4si __builtin_ia32_pminud128 (v4si, v4si)\n+v8hi __builtin_ia32_pminuw128 (v8hi, v8hi)\n+v4si __builtin_ia32_pmovsxbd128 (v16qi)\n+v2di __builtin_ia32_pmovsxbq128 (v16qi)\n+v8hi __builtin_ia32_pmovsxbw128 (v16qi)\n+v2di __builtin_ia32_pmovsxdq128 (v4si)\n+v4si __builtin_ia32_pmovsxwd128 (v8hi)\n+v2di __builtin_ia32_pmovsxwq128 (v8hi)\n+v4si __builtin_ia32_pmovzxbd128 (v16qi)\n+v2di __builtin_ia32_pmovzxbq128 (v16qi)\n+v8hi __builtin_ia32_pmovzxbw128 (v16qi)\n+v2di __builtin_ia32_pmovzxdq128 (v4si)\n+v4si __builtin_ia32_pmovzxwd128 (v8hi)\n+v2di __builtin_ia32_pmovzxwq128 (v8hi)\n+v2di __builtin_ia32_pmuldq128 (v4si, v4si)\n+v4si __builtin_ia32_pmulld128 (v4si, v4si)\n+int __builtin_ia32_ptestc128 (v2di, v2di)\n+int __builtin_ia32_ptestnzc128 (v2di, v2di)\n+int __builtin_ia32_ptestz128 (v2di, v2di)\n+v2df __builtin_ia32_roundpd (v2df, const int)\n+v4sf __builtin_ia32_roundps (v4sf, const int)\n+v2df __builtin_ia32_roundsd (v2df, v2df, const int)\n+v4sf __builtin_ia32_roundss (v4sf, v4sf, const int)\n+@end smallexample\n+\n+The following built-in functions are available when @option{-msse4.1} is\n+used.\n+\n+@table @code\n+@item v4sf __builtin_ia32_vec_set_v4sf (v4sf, float, const int)\n+Generates the @code{insertps} machine instruction.\n+@item int __builtin_ia32_vec_ext_v16qi (v16qi, const int)\n+Generates the @code{pextrb} machine instruction.\n+@item v16qi __builtin_ia32_vec_set_v16qi (v16qi, int, const int)\n+Generates the @code{pinsrb} machine instruction.\n+@item v4si __builtin_ia32_vec_set_v4si (v4si, int, const int)\n+Generates the @code{pinsrd} machine instruction.\n+@item v2di __builtin_ia32_vec_set_v2di (v2di, long long, const int)\n+Generates the @code{pinsrq} machine instruction in 64bit mode.\n+@end table\n+\n+The following built-in functions are changed to generate new SSE4.1\n+instructions when @option{-msse4.1} is used.\n+\n+@table @code\n+@item float __builtin_ia32_vec_ext_v4sf (v4sf, const int)\n+Generates the @code{extractps} machine instruction.\n+@item int __builtin_ia32_vec_ext_v4si (v4si, const int)\n+Generates the @code{pextrd} machine instruction.\n+@item long long __builtin_ia32_vec_ext_v2di (v2di, const int)\n+Generates the @code{pextrq} machine instruction in 64bit mode.\n+@end table\n+\n The following built-in functions are available when @option{-msse4a} is used.\n \n @smallexample"}, {"sha": "21ef96cae7c35e633d90580d82c0a2a7a7582400", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a5cee0228a8b8d639e29682c800fe251175ce62/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=9a5cee0228a8b8d639e29682c800fe251175ce62", "patch": "@@ -547,7 +547,8 @@ Objective-C and Objective-C++ Dialects}.\n -mno-fp-ret-in-387  -msoft-float @gol\n -mno-wide-multiply  -mrtd  -malign-double @gol\n -mpreferred-stack-boundary=@var{num} -mcx16 -msahf @gol\n--mmmx  -msse  -msse2 -msse3 -mssse3 -msse4a -m3dnow -mpopcnt -mabm @gol\n+-mmmx  -msse  -msse2 -msse3 -mssse3 -msse4.1 @gol\n+-msse4a -m3dnow -mpopcnt -mabm @gol\n -mthreads  -mno-align-stringops  -minline-all-stringops @gol\n -mpush-args  -maccumulate-outgoing-args  -m128bit-long-double @gol\n -m96bit-long-double  -mregparm=@var{num}  -msseregparm @gol\n@@ -10260,6 +10261,8 @@ preferred alignment to @option{-mpreferred-stack-boundary=2}.\n @itemx -mno-sse3\n @item -mssse3\n @itemx -mno-ssse3\n+@item -msse4.1\n+@itemx -mno-sse4.1\n @item -msse4a\n @item -mno-sse4a\n @item -m3dnow\n@@ -10275,7 +10278,8 @@ preferred alignment to @option{-mpreferred-stack-boundary=2}.\n @opindex m3dnow\n @opindex mno-3dnow\n These switches enable or disable the use of instructions in the MMX,\n-SSE, SSE2, SSE3, SSSE3, SSE4A, ABM or 3DNow! extended instruction sets.\n+SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4A, ABM or 3DNow! extended\n+instruction sets.\n These extensions are also available as built-in functions: see\n @ref{X86 Built-in Functions}, for details of the functions enabled and\n disabled by these switches."}]}