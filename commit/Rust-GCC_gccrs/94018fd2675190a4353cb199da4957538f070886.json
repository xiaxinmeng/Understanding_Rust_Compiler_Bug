{"sha": "94018fd2675190a4353cb199da4957538f070886", "node_id": "C_kwDOANBUbNoAKDk0MDE4ZmQyNjc1MTkwYTQzNTNjYjE5OWRhNDk1NzUzOGYwNzA4ODY", "commit": {"author": {"name": "Richard Earnshaw", "email": "rearnsha@arm.com", "date": "2022-06-17T09:30:57Z"}, "committer": {"name": "Richard Earnshaw", "email": "rearnsha@arm.com", "date": "2022-06-17T09:33:15Z"}, "message": "arm: mve: Don't force trivial vector literals to the pool\n\nA bug in the ordering of the operands in the mve_mov<mode> pattern\nmeant that all literal values were being pushed to the literal pool.\nThis patch fixes that and simplifies some of the logic slightly so\nthat we can use as simple switch statement.\n\nFor example:\nvoid f (uint32_t *a)\n{\n  int i;\n  for (i = 0; i < 100; i++)\n    a[i] += 1;\n}\n\nNow compiles to:\n        push    {lr}\n        mov     lr, #25\n        vmov.i32        q2, #0x1  @ v4si\n        ...\n\ninstead of\n\n        push    {lr}\n        mov     lr, #25\n        vldr.64 d4, .L6\n        vldr.64 d5, .L6+8\n\t...\n.L7:\n        .align  3\n.L6:\n        .word   1\n        .word   1\n        .word   1\n        .word   1\n\ngcc/ChangeLog:\n\t* config/arm/mve.md (*mve_mov<mode>): Re-order constraints\n\tto avoid spilling trivial literals to the constant pool.\n\ngcc/testsuite/ChangeLog:\n\t* gcc.target/arm/acle/cde-mve-full-assembly.c: Adjust expected\n\toutput.", "tree": {"sha": "372eceff777e3b38d9d8f480b4c164d0ff941a53", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/372eceff777e3b38d9d8f480b4c164d0ff941a53"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/94018fd2675190a4353cb199da4957538f070886", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/94018fd2675190a4353cb199da4957538f070886", "html_url": "https://github.com/Rust-GCC/gccrs/commit/94018fd2675190a4353cb199da4957538f070886", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/94018fd2675190a4353cb199da4957538f070886/comments", "author": null, "committer": null, "parents": [{"sha": "bc7e9f76756f2f164bb5dc70b59bc0d838f9fa96", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bc7e9f76756f2f164bb5dc70b59bc0d838f9fa96", "html_url": "https://github.com/Rust-GCC/gccrs/commit/bc7e9f76756f2f164bb5dc70b59bc0d838f9fa96"}], "stats": {"total": 648, "additions": 311, "deletions": 337}, "files": [{"sha": "c4dec01baaca7f2245abc40f0c192aac0fde3bc3", "filename": "gcc/config/arm/mve.md", "status": "modified", "additions": 53, "deletions": 46, "changes": 99, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/94018fd2675190a4353cb199da4957538f070886/gcc%2Fconfig%2Farm%2Fmve.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/94018fd2675190a4353cb199da4957538f070886/gcc%2Fconfig%2Farm%2Fmve.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Fmve.md?ref=94018fd2675190a4353cb199da4957538f070886", "patch": "@@ -18,66 +18,73 @@\n ;; <http://www.gnu.org/licenses/>.\n \n (define_insn \"*mve_mov<mode>\"\n-  [(set (match_operand:MVE_types 0 \"nonimmediate_operand\" \"=w,w,r,w,w,r,w,Ux,w\")\n-\t(match_operand:MVE_types 1 \"general_operand\" \"w,r,w,Dn,UxUi,r,Dm,w,Ul\"))]\n+  [(set (match_operand:MVE_types 0 \"nonimmediate_operand\" \"=w,w,r,w   , w,   r,Ux,w\")\n+\t(match_operand:MVE_types 1 \"general_operand\"      \" w,r,w,DnDm,UxUi,r,w, Ul\"))]\n   \"TARGET_HAVE_MVE || TARGET_HAVE_MVE_FLOAT\"\n {\n-  if (which_alternative == 3 || which_alternative == 6)\n+  switch (which_alternative)\n     {\n-      int width, is_valid;\n-      static char templ[40];\n+    case 0:  /* [w,w].  */\n+      return \"vmov\\t%q0, %q1\";\n \n-      is_valid = simd_immediate_valid_for_move (operands[1], <MODE>mode,\n-\t&operands[1], &width);\n+    case 1:  /* [w,r].  */\n+      return \"vmov\\t%e0, %Q1, %R1  %@ <mode>\\;vmov\\t%f0, %J1, %K1\";\n+\n+    case 2:  /* [r,w].  */\n+      return \"vmov\\t%Q0, %R0, %e1  %@ <mode>\\;vmov\\t%J0, %K0, %f1\";\n+\n+    case 3:  /* [w,DnDm].  */\n+      {\n+\tint width, is_valid;\n+\n+\tis_valid = simd_immediate_valid_for_move (operands[1], <MODE>mode,\n+\t\t\t\t\t\t  &operands[1], &width);\n+\n+\tgcc_assert (is_valid);\n+\n+\tif (width == 0)\n+\t  return \"vmov.f32\\t%q0, %1  %@ <mode>\";\n+\telse\n+\t  {\n+\t    const int templ_size = 40;\n+\t    static char templ[templ_size];\n+\t    if (snprintf (templ, templ_size,\n+\t\t\t  \"vmov.i%d\\t%%q0, %%x1  %%@ <mode>\", width)\n+\t\t> templ_size)\n+\t      abort ();\n+\t    return templ;\n+\t  }\n+      }\n+\n+    case 4:  /* [w,UxUi].  */\n+      if (<MODE>mode == V2DFmode || <MODE>mode == V2DImode\n+\t  || <MODE>mode == TImode)\n+\treturn \"vldrw.u32\\t%q0, %E1\";\n+      else\n+\treturn \"vldr<V_sz_elem1>.<V_sz_elem>\\t%q0, %E1\";\n \n-      gcc_assert (is_valid != 0);\n+    case 5:  /* [r,r].  */\n+      return output_move_quad (operands);\n \n-      if (width == 0)\n-\treturn \"vmov.f32\\t%q0, %1  @ <mode>\";\n+    case 6:  /* [Ux,w].  */\n+      if (<MODE>mode == V2DFmode || <MODE>mode == V2DImode\n+\t  || <MODE>mode == TImode)\n+\treturn \"vstrw.32\\t%q1, %E0\";\n       else\n-\tsprintf (templ, \"vmov.i%d\\t%%q0, %%x1  @ <mode>\", width);\n-      return templ;\n-    }\n+\treturn \"vstr<V_sz_elem1>.<V_sz_elem>\\t%q1, %E0\";\n \n-  if (which_alternative == 4 || which_alternative == 7)\n-    {\n-      if (<MODE>mode == V2DFmode || <MODE>mode == V2DImode || <MODE>mode == TImode)\n-\t{\n-\t  if (which_alternative == 7)\n-\t    output_asm_insn (\"vstrw.32\\t%q1, %E0\", operands);\n-\t  else\n-\t    output_asm_insn (\"vldrw.u32\\t%q0, %E1\",operands);\n-\t}\n-      else\n-\t{\n-\t  if (which_alternative == 7)\n-\t    output_asm_insn (\"vstr<V_sz_elem1>.<V_sz_elem>\\t%q1, %E0\", operands);\n-\t  else\n-\t    output_asm_insn (\"vldr<V_sz_elem1>.<V_sz_elem>\\t%q0, %E1\", operands);\n-\t}\n-      return \"\";\n-    }\n-  switch (which_alternative)\n-    {\n-    case 0:\n-      return \"vmov\\t%q0, %q1\";\n-    case 1:\n-      return \"vmov\\t%e0, %Q1, %R1  @ <mode>\\;vmov\\t%f0, %J1, %K1\";\n-    case 2:\n-      return \"vmov\\t%Q0, %R0, %e1  @ <mode>\\;vmov\\t%J0, %K0, %f1\";\n-    case 5:\n-      return output_move_quad (operands);\n-    case 8:\n+    case 7:  /* [w,Ul].  */\n \treturn output_move_neon (operands);\n+\n     default:\n       gcc_unreachable ();\n       return \"\";\n     }\n }\n-  [(set_attr \"type\" \"mve_move,mve_move,mve_move,mve_move,mve_load,multiple,mve_move,mve_store,mve_load\")\n-   (set_attr \"length\" \"4,8,8,4,8,8,4,4,4\")\n-   (set_attr \"thumb2_pool_range\" \"*,*,*,*,1018,*,*,*,*\")\n-   (set_attr \"neg_pool_range\" \"*,*,*,*,996,*,*,*,*\")])\n+  [(set_attr \"type\" \"mve_move,mve_move,mve_move,mve_move,mve_load,multiple,mve_store,mve_load\")\n+   (set_attr \"length\" \"4,8,8,4,4,8,4,8\")\n+   (set_attr \"thumb2_pool_range\" \"*,*,*,*,1018,*,*,*\")\n+   (set_attr \"neg_pool_range\" \"*,*,*,*,996,*,*,*\")])\n \n (define_insn \"*mve_vdup<mode>\"\n   [(set (match_operand:MVE_vecs 0 \"s_register_operand\" \"=w\")"}, {"sha": "d025c3391fbe5241972cfcdda372150bea621602", "filename": "gcc/testsuite/gcc.target/arm/acle/cde-mve-full-assembly.c", "status": "modified", "additions": 258, "deletions": 291, "changes": 549, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/94018fd2675190a4353cb199da4957538f070886/gcc%2Ftestsuite%2Fgcc.target%2Farm%2Facle%2Fcde-mve-full-assembly.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/94018fd2675190a4353cb199da4957538f070886/gcc%2Ftestsuite%2Fgcc.target%2Farm%2Facle%2Fcde-mve-full-assembly.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Farm%2Facle%2Fcde-mve-full-assembly.c?ref=94018fd2675190a4353cb199da4957538f070886", "patch": "@@ -73,71 +73,61 @@\n */\n /*\n ** test_cde_vcx1qafloat16x8_tintint:\n-** \tvldr\\.64\td0, \\.L([0-9]*)\n-** \tvldr\\.64\td1, \\.L\\1\\+8\n+** \tvmov\\.i32\tq0, #0  @ v16qi\n ** \tvcx1a\tp0, q0, #33\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qafloat32x4_tintint:\n-** \tvldr\\.64\td0, \\.L([0-9]*)\n-** \tvldr\\.64\td1, \\.L\\1\\+8\n+** \tvmov\\.i32\tq0, #0  @ v16qi\n ** \tvcx1a\tp0, q0, #33\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qauint8x16_tintint:\n-** \tvldr\\.64\td0, \\.L([0-9]*)\n-** \tvldr\\.64\td1, \\.L\\1\\+8\n+** \tvmov\\.i32\tq0, #0  @ v16qi\n ** \tvcx1a\tp0, q0, #33\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qauint16x8_tintint:\n-** \tvldr\\.64\td0, \\.L([0-9]*)\n-** \tvldr\\.64\td1, \\.L\\1\\+8\n+** \tvmov\\.i32\tq0, #0  @ v16qi\n ** \tvcx1a\tp0, q0, #33\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qauint32x4_tintint:\n-** \tvldr\\.64\td0, \\.L([0-9]*)\n-** \tvldr\\.64\td1, \\.L\\1\\+8\n+** \tvmov\\.i32\tq0, #0  @ v16qi\n ** \tvcx1a\tp0, q0, #33\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qauint64x2_tintint:\n-** \tvldr\\.64\td0, \\.L([0-9]*)\n-** \tvldr\\.64\td1, \\.L\\1\\+8\n+** \tvmov\\.i32\tq0, #0  @ v16qi\n ** \tvcx1a\tp0, q0, #33\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qaint8x16_tintint:\n-** \tvldr\\.64\td0, \\.L([0-9]*)\n-** \tvldr\\.64\td1, \\.L\\1\\+8\n+** \tvmov\\.i32\tq0, #0  @ v16qi\n ** \tvcx1a\tp0, q0, #33\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qaint16x8_tintint:\n-** \tvldr\\.64\td0, \\.L([0-9]*)\n-** \tvldr\\.64\td1, \\.L\\1\\+8\n+** \tvmov\\.i32\tq0, #0  @ v16qi\n ** \tvcx1a\tp0, q0, #33\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qaint32x4_tintint:\n-** \tvldr\\.64\td0, \\.L([0-9]*)\n-** \tvldr\\.64\td1, \\.L\\1\\+8\n+** \tvmov\\.i32\tq0, #0  @ v16qi\n ** \tvcx1a\tp0, q0, #33\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qaint64x2_tintint:\n-** \tvldr\\.64\td0, \\.L([0-9]*)\n-** \tvldr\\.64\td1, \\.L\\1\\+8\n+** \tvmov\\.i32\tq0, #0  @ v16qi\n ** \tvcx1a\tp0, q0, #33\n ** \tbx\tlr\n */\n@@ -243,82 +233,72 @@\n */\n /*\n ** test_cde_vcx2qafloat16x8_tuint16x8_tint:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx2a\tp0, (q[0-7]), q0, #33\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[1-7]), #0  @ v16qi\n+** \tvcx2a\tp0, \\1, q0, #33\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qafloat16x8_tfloat32x4_tint:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx2a\tp0, (q[0-7]), q0, #33\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[1-7]), #0  @ v16qi\n+** \tvcx2a\tp0, \\1, q0, #33\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qafloat32x4_tuint8x16_tint:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx2a\tp0, (q[0-7]), q0, #33\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[1-7]), #0  @ v16qi\n+** \tvcx2a\tp0, \\1, q0, #33\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qaint64x2_tuint8x16_tint:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx2a\tp0, (q[0-7]), q0, #33\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[1-7]), #0  @ v16qi\n+** \tvcx2a\tp0, \\1, q0, #33\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qaint8x16_tuint8x16_tint:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx2a\tp0, (q[0-7]), q0, #33\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[1-7]), #0  @ v16qi\n+** \tvcx2a\tp0, \\1, q0, #33\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qauint16x8_tuint8x16_tint:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx2a\tp0, (q[0-7]), q0, #33\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[1-7]), #0  @ v16qi\n+** \tvcx2a\tp0, \\1, q0, #33\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qauint8x16_tint64x2_tint:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx2a\tp0, (q[0-7]), q0, #33\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[1-7]), #0  @ v16qi\n+** \tvcx2a\tp0, \\1, q0, #33\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qauint8x16_tint8x16_tint:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx2a\tp0, (q[0-7]), q0, #33\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[1-7]), #0  @ v16qi\n+** \tvcx2a\tp0, \\1, q0, #33\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qauint8x16_tuint16x8_tint:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx2a\tp0, (q[0-7]), q0, #33\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[1-7]), #0  @ v16qi\n+** \tvcx2a\tp0, \\1, q0, #33\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qauint8x16_tuint8x16_tint:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx2a\tp0, (q[0-7]), q0, #33\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[1-7]), #0  @ v16qi\n+** \tvcx2a\tp0, \\1, q0, #33\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n@@ -453,112 +433,99 @@\n */\n /*\n ** test_cde_vcx3qauint8x16_tuint8x16_tuint8x16_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qafloat16x8_tfloat16x8_tfloat16x8_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qafloat32x4_tuint64x2_tfloat16x8_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qauint16x8_tuint8x16_tuint8x16_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qauint8x16_tuint16x8_tuint8x16_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qauint8x16_tuint8x16_tuint16x8_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qaint8x16_tuint8x16_tuint8x16_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qauint8x16_tint8x16_tuint8x16_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qauint8x16_tuint8x16_tint8x16_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qaint64x2_tuint8x16_tuint8x16_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qauint8x16_tint64x2_tuint8x16_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qauint8x16_tuint8x16_tint64x2_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qauint8x16_tint64x2_tint64x2_t:\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L([0-9]*)\n-** \tvldr\\.64\td(?:[01][0-4]|[0-9]), \\.L\\1\\+8\n-** \tvcx3a\tp0, (q[0-7]), q0, q1, #12\n-** \tvmov\tq0, \\2\n+** \tvmov\\.i32\t(q[2-7]), #0  @ v16qi\n+** \tvcx3a\tp0, \\1, q0, q1, #12\n+** \tvmov\tq0, \\1\n ** \tbx\tlr\n */\n \n /* Predicated MVE intrinsics.  */\n /* Merging lane predication types.\n-   NOTE: Depending on the target, the setup instructions (vldr's and vmsr) can\n+   NOTE: Depending on the target, the setup instructions (vmov's and vmsr) can\n    be in a different order.  Here we just check that all the expected setup\n    instructions are there.  We don't check that the setup instructions are\n    different since the likelyhood of the compiler generating repeated versions\n@@ -567,80 +534,80 @@\n    contain back references).  */\n /*\n ** test_cde_vcx1q_mfloat16x8_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1t\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1q_mfloat32x4_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1t\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1q_muint8x16_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1t\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1q_muint16x8_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1t\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1q_muint32x4_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1t\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1q_muint64x2_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1t\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1q_mint8x16_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1t\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1q_mint16x8_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1t\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1q_mint32x4_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1t\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1q_mint64x2_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1t\tp0, q0, #32\n ** \tbx\tlr\n@@ -649,80 +616,80 @@\n \n /*\n ** test_cde_vcx1qa_mfloat16x8_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1at\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qa_mfloat32x4_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1at\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qa_muint8x16_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1at\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qa_muint16x8_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1at\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qa_muint32x4_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1at\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qa_muint64x2_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1at\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qa_mint8x16_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1at\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qa_mint16x8_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1at\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qa_mint32x4_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1at\tp0, q0, #32\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx1qa_mint64x2_tintint:\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n-** \t(?:vldr\\.64\td0, \\.L[0-9]*\\n\\tvldr\\.64\td1, \\.L[0-9]*\\+8|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n+** \t(?:vmov\\.i32\tq0, #0  @ v16qi|vmsr\t P0, r2\t@ movhi)\n ** \tvpst\n ** \tvcx1at\tp0, q0, #32\n ** \tbx\tlr\n@@ -731,421 +698,421 @@\n \n /*\n ** test_cde_vcx2q_mfloat16x8_tuint16x8_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2t\tp0, (q[0-7]), q0, #32\n+** \tvcx2t\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2q_mfloat16x8_tfloat32x4_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2t\tp0, (q[0-7]), q0, #32\n+** \tvcx2t\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2q_mfloat32x4_tuint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2t\tp0, (q[0-7]), q0, #32\n+** \tvcx2t\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2q_mint64x2_tuint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2t\tp0, (q[0-7]), q0, #32\n+** \tvcx2t\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2q_mint8x16_tuint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2t\tp0, (q[0-7]), q0, #32\n+** \tvcx2t\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2q_muint16x8_tuint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2t\tp0, (q[0-7]), q0, #32\n+** \tvcx2t\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2q_muint8x16_tint64x2_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2t\tp0, (q[0-7]), q0, #32\n+** \tvcx2t\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2q_muint8x16_tint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2t\tp0, (q[0-7]), q0, #32\n+** \tvcx2t\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2q_muint8x16_tuint16x8_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2t\tp0, (q[0-7]), q0, #32\n+** \tvcx2t\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2q_muint8x16_tuint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2t\tp0, (q[0-7]), q0, #32\n+** \tvcx2t\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n \n \n /*\n ** test_cde_vcx2qa_mfloat16x8_tuint16x8_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2at\tp0, (q[0-7]), q0, #32\n+** \tvcx2at\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qa_mfloat16x8_tfloat32x4_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2at\tp0, (q[0-7]), q0, #32\n+** \tvcx2at\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qa_mfloat32x4_tuint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2at\tp0, (q[0-7]), q0, #32\n+** \tvcx2at\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qa_mint64x2_tuint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2at\tp0, (q[0-7]), q0, #32\n+** \tvcx2at\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qa_mint8x16_tuint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2at\tp0, (q[0-7]), q0, #32\n+** \tvcx2at\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qa_muint16x8_tuint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2at\tp0, (q[0-7]), q0, #32\n+** \tvcx2at\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qa_muint8x16_tint64x2_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2at\tp0, (q[0-7]), q0, #32\n+** \tvcx2at\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qa_muint8x16_tint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2at\tp0, (q[0-7]), q0, #32\n+** \tvcx2at\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qa_muint8x16_tuint16x8_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2at\tp0, (q[0-7]), q0, #32\n+** \tvcx2at\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx2qa_muint8x16_tuint8x16_tint:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n+** \t(?:vmov\\.i32\tq[1-7], #0  @ v16qi|vmsr\t P0, r1\t@ movhi)\n ** \tvpst\n-** \tvcx2at\tp0, (q[0-7]), q0, #32\n+** \tvcx2at\tp0, (q[1-7]), q0, #32\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n \n \n /*\n ** test_cde_vcx3q_muint8x16_tuint8x16_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_mfloat16x8_tfloat16x8_tfloat16x8_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_mfloat32x4_tuint64x2_tfloat16x8_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_muint16x8_tuint8x16_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_muint8x16_tuint16x8_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_muint8x16_tuint8x16_tuint16x8_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_mint8x16_tuint8x16_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_muint8x16_tint8x16_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_muint8x16_tuint8x16_tint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_mint64x2_tuint8x16_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_muint8x16_tint64x2_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_muint8x16_tuint8x16_tint64x2_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3q_muint8x16_tint64x2_tint64x2_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3t\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3t\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n \n \n /*\n ** test_cde_vcx3qa_muint8x16_tuint8x16_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_mfloat16x8_tfloat16x8_tfloat16x8_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_mfloat32x4_tuint64x2_tfloat16x8_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_muint16x8_tuint8x16_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_muint8x16_tuint16x8_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_muint8x16_tuint8x16_tuint16x8_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_mint8x16_tuint8x16_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_muint8x16_tint8x16_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_muint8x16_tuint8x16_tint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_mint64x2_tuint8x16_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_muint8x16_tint64x2_tuint8x16_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_muint8x16_tuint8x16_tint64x2_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */\n /*\n ** test_cde_vcx3qa_muint8x16_tint64x2_tint64x2_t:\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n-** \t(?:vldr\\.64\td(?:[02468]|1[024]), \\.L[0-9]*\\n\\tvldr\\.64\td(?:[13579]|1[135]), \\.L[0-9]*\\+8|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n+** \t(?:vmov\\.i32\tq[2-7], #0  @ v16qi|vmsr\t P0, r0\t@ movhi)\n ** \tvpst\n-** \tvcx3at\tp0, (q[0-7]), q0, q1, #15\n+** \tvcx3at\tp0, (q[2-7]), q0, q1, #15\n ** \tvmov\tq0, \\1([[:space:]]+@ [^\\n]*)?\n ** \tbx\tlr\n */"}]}