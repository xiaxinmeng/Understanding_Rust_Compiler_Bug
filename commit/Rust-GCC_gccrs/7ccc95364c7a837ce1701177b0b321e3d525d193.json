{"sha": "7ccc95364c7a837ce1701177b0b321e3d525d193", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6N2NjYzk1MzY0YzdhODM3Y2UxNzAxMTc3YjBiMzIxZTNkNTI1ZDE5Mw==", "commit": {"author": {"name": "H.J. Lu", "email": "hongjiu.lu@intel.com", "date": "2016-04-20T13:39:28Z"}, "committer": {"name": "H.J. Lu", "email": "hjl@gcc.gnu.org", "date": "2016-04-20T13:39:28Z"}, "message": "Simplify ix86_expand_vector_move_misalign\n\nSince mov<mode>_internal patterns handle both aligned/unaligned load\nand store, we can simplify ix86_avx256_split_vector_move_misalign and\nix86_expand_vector_move_misalign.\n\n\t* config/i386/i386.c (ix86_avx256_split_vector_move_misalign):\n\tShort-cut unaligned load and store cases.  Handle all integer\n\tvector modes.\n\t(ix86_expand_vector_move_misalign): Short-cut unaligned load\n\tand store cases.  Call ix86_avx256_split_vector_move_misalign\n\tdirectly without checking mode class.\n\nFrom-SVN: r235283", "tree": {"sha": "fbce01cec197a6bbb225fc2cd1367e1f3d8a5d83", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/fbce01cec197a6bbb225fc2cd1367e1f3d8a5d83"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/7ccc95364c7a837ce1701177b0b321e3d525d193", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7ccc95364c7a837ce1701177b0b321e3d525d193", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7ccc95364c7a837ce1701177b0b321e3d525d193", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7ccc95364c7a837ce1701177b0b321e3d525d193/comments", "author": {"login": "hjl-tools", "id": 1072356, "node_id": "MDQ6VXNlcjEwNzIzNTY=", "avatar_url": "https://avatars.githubusercontent.com/u/1072356?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hjl-tools", "html_url": "https://github.com/hjl-tools", "followers_url": "https://api.github.com/users/hjl-tools/followers", "following_url": "https://api.github.com/users/hjl-tools/following{/other_user}", "gists_url": "https://api.github.com/users/hjl-tools/gists{/gist_id}", "starred_url": "https://api.github.com/users/hjl-tools/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hjl-tools/subscriptions", "organizations_url": "https://api.github.com/users/hjl-tools/orgs", "repos_url": "https://api.github.com/users/hjl-tools/repos", "events_url": "https://api.github.com/users/hjl-tools/events{/privacy}", "received_events_url": "https://api.github.com/users/hjl-tools/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "9e3e4fab8456108a000c66ac084d08f878a42cee", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9e3e4fab8456108a000c66ac084d08f878a42cee", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9e3e4fab8456108a000c66ac084d08f878a42cee"}], "stats": {"total": 261, "additions": 90, "deletions": 171}, "files": [{"sha": "e391a2145ee83829cd032222196ff60264e00c47", "filename": "gcc/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ccc95364c7a837ce1701177b0b321e3d525d193/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ccc95364c7a837ce1701177b0b321e3d525d193/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=7ccc95364c7a837ce1701177b0b321e3d525d193", "patch": "@@ -1,3 +1,12 @@\n+2016-04-20  H.J. Lu  <hongjiu.lu@intel.com>\n+\n+\t* config/i386/i386.c (ix86_avx256_split_vector_move_misalign):\n+\tShort-cut unaligned load and store cases.  Handle all integer\n+\tvector modes.\n+\t(ix86_expand_vector_move_misalign): Short-cut unaligned load\n+\tand store cases.  Call ix86_avx256_split_vector_move_misalign\n+\tdirectly without checking mode class.\n+\n 2016-04-20  Andrew Pinski  <apinski@cavium.com>\n             Kyrylo Tkachov  <kyrylo.tkachov@arm.com>\n "}, {"sha": "6379313ef38ae9944d72e7fa3c4335900eebed3c", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 81, "deletions": 171, "changes": 252, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ccc95364c7a837ce1701177b0b321e3d525d193/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ccc95364c7a837ce1701177b0b321e3d525d193/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=7ccc95364c7a837ce1701177b0b321e3d525d193", "patch": "@@ -18807,7 +18807,39 @@ ix86_avx256_split_vector_move_misalign (rtx op0, rtx op1)\n   rtx (*extract) (rtx, rtx, rtx);\n   machine_mode mode;\n \n-  switch (GET_MODE (op0))\n+  if ((MEM_P (op1) && !TARGET_AVX256_SPLIT_UNALIGNED_LOAD)\n+      || (MEM_P (op0) && !TARGET_AVX256_SPLIT_UNALIGNED_STORE))\n+    {\n+      emit_insn (gen_rtx_SET (op0, op1));\n+      return;\n+    }\n+\n+  rtx orig_op0 = NULL_RTX;\n+  mode = GET_MODE (op0);\n+  switch (GET_MODE_CLASS (mode))\n+    {\n+    case MODE_VECTOR_INT:\n+    case MODE_INT:\n+      if (mode != V32QImode)\n+\t{\n+\t  if (!MEM_P (op0))\n+\t    {\n+\t      orig_op0 = op0;\n+\t      op0 = gen_reg_rtx (V32QImode);\n+\t    }\n+\t  else\n+\t    op0 = gen_lowpart (V32QImode, op0);\n+\t  op1 = gen_lowpart (V32QImode, op1);\n+\t  mode = V32QImode;\n+\t}\n+      break;\n+    case MODE_VECTOR_FLOAT:\n+      break;\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+  switch (mode)\n     {\n     default:\n       gcc_unreachable ();\n@@ -18827,34 +18859,25 @@ ix86_avx256_split_vector_move_misalign (rtx op0, rtx op1)\n \n   if (MEM_P (op1))\n     {\n-      if (TARGET_AVX256_SPLIT_UNALIGNED_LOAD\n-\t  && optimize_insn_for_speed_p ())\n-\t{\n-\t  rtx r = gen_reg_rtx (mode);\n-\t  m = adjust_address (op1, mode, 0);\n-\t  emit_move_insn (r, m);\n-\t  m = adjust_address (op1, mode, 16);\n-\t  r = gen_rtx_VEC_CONCAT (GET_MODE (op0), r, m);\n-\t  emit_move_insn (op0, r);\n-\t}\n-      else\n-\temit_insn (gen_rtx_SET (op0, op1));\n+      rtx r = gen_reg_rtx (mode);\n+      m = adjust_address (op1, mode, 0);\n+      emit_move_insn (r, m);\n+      m = adjust_address (op1, mode, 16);\n+      r = gen_rtx_VEC_CONCAT (GET_MODE (op0), r, m);\n+      emit_move_insn (op0, r);\n     }\n   else if (MEM_P (op0))\n     {\n-      if (TARGET_AVX256_SPLIT_UNALIGNED_STORE\n-\t  && optimize_insn_for_speed_p ())\n-\t{\n-\t  m = adjust_address (op0, mode, 0);\n-\t  emit_insn (extract (m, op1, const0_rtx));\n-\t  m = adjust_address (op0, mode, 16);\n-\t  emit_insn (extract (m, op1, const1_rtx));\n-\t}\n-      else\n-\temit_insn (gen_rtx_SET (op0, op1));\n+      m = adjust_address (op0, mode, 0);\n+      emit_insn (extract (m, op1, const0_rtx));\n+      m = adjust_address (op0, mode, 16);\n+      emit_insn (extract (m, op1, const1_rtx));\n     }\n   else\n     gcc_unreachable ();\n+\n+  if (orig_op0)\n+    emit_move_insn (orig_op0, gen_lowpart (GET_MODE (orig_op0), op0));\n }\n \n /* Implement the movmisalign patterns for SSE.  Non-SSE modes go\n@@ -18912,118 +18935,50 @@ ix86_avx256_split_vector_move_misalign (rtx op0, rtx op1)\n void\n ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n {\n-  rtx op0, op1, orig_op0 = NULL_RTX, m;\n+  rtx op0, op1, m;\n \n   op0 = operands[0];\n   op1 = operands[1];\n \n-  if (GET_MODE_SIZE (mode) == 64)\n+  /* Use unaligned load/store for AVX512 or when optimizing for size.  */\n+  if (GET_MODE_SIZE (mode) == 64 || optimize_insn_for_size_p ())\n     {\n-      switch (GET_MODE_CLASS (mode))\n-\t{\n-\tcase MODE_VECTOR_INT:\n-\tcase MODE_INT:\n-\t  if (GET_MODE (op0) != V16SImode)\n-\t    {\n-\t      if (!MEM_P (op0))\n-\t\t{\n-\t\t  orig_op0 = op0;\n-\t\t  op0 = gen_reg_rtx (V16SImode);\n-\t\t}\n-\t      else\n-\t\top0 = gen_lowpart (V16SImode, op0);\n-\t    }\n-\t  op1 = gen_lowpart (V16SImode, op1);\n-\t  /* FALLTHRU */\n-\n-\tcase MODE_VECTOR_FLOAT:\n-\n-\t  emit_insn (gen_rtx_SET (op0, op1));\n-\t  if (orig_op0)\n-\t    emit_move_insn (orig_op0, gen_lowpart (GET_MODE (orig_op0), op0));\n-\t  break;\n-\n-\tdefault:\n-\t  gcc_unreachable ();\n-\t}\n-\n+      emit_insn (gen_rtx_SET (op0, op1));\n       return;\n     }\n \n-  if (TARGET_AVX\n-      && GET_MODE_SIZE (mode) == 32)\n+  if (TARGET_AVX)\n     {\n-      switch (GET_MODE_CLASS (mode))\n-\t{\n-\tcase MODE_VECTOR_INT:\n-\tcase MODE_INT:\n-\t  if (GET_MODE (op0) != V32QImode)\n-\t    {\n-\t      if (!MEM_P (op0))\n-\t\t{\n-\t\t  orig_op0 = op0;\n-\t\t  op0 = gen_reg_rtx (V32QImode);\n-\t\t}\n-\t      else\n-\t\top0 = gen_lowpart (V32QImode, op0);\n-\t    }\n-\t  op1 = gen_lowpart (V32QImode, op1);\n-\t  /* FALLTHRU */\n-\n-\tcase MODE_VECTOR_FLOAT:\n-\t  ix86_avx256_split_vector_move_misalign (op0, op1);\n-\t  if (orig_op0)\n-\t    emit_move_insn (orig_op0, gen_lowpart (GET_MODE (orig_op0), op0));\n-\t  break;\n+      if (GET_MODE_SIZE (mode) == 32)\n+\tix86_avx256_split_vector_move_misalign (op0, op1);\n+      else\n+\t/* Always use 128-bit mov<mode>_internal pattern for AVX.  */\n+\temit_insn (gen_rtx_SET (op0, op1));\n+      return;\n+    }\n \n-\tdefault:\n-\t  gcc_unreachable ();\n-\t}\n+  if (TARGET_SSE_UNALIGNED_LOAD_OPTIMAL\n+      || TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+    {\n+      emit_insn (gen_rtx_SET (op0, op1));\n+      return;\n+    }\n \n+  /* ??? If we have typed data, then it would appear that using\n+     movdqu is the only way to get unaligned data loaded with\n+     integer type.  */\n+  if (TARGET_SSE2 && GET_MODE_CLASS (mode) == MODE_VECTOR_INT)\n+    {\n+      emit_insn (gen_rtx_SET (op0, op1));\n       return;\n     }\n \n   if (MEM_P (op1))\n     {\n-      /* Normal *mov<mode>_internal pattern will handle\n-\t unaligned loads just fine if misaligned_operand\n-\t is true, and without the UNSPEC it can be combined\n-\t with arithmetic instructions.  */\n-      if (TARGET_AVX\n-\t  && (GET_MODE_CLASS (mode) == MODE_VECTOR_INT\n-\t      || GET_MODE_CLASS (mode) == MODE_VECTOR_FLOAT)\n-\t  && misaligned_operand (op1, GET_MODE (op1)))\n-\temit_insn (gen_rtx_SET (op0, op1));\n-      /* ??? If we have typed data, then it would appear that using\n-\t movdqu is the only way to get unaligned data loaded with\n-\t integer type.  */\n-      else if (TARGET_SSE2 && GET_MODE_CLASS (mode) == MODE_VECTOR_INT)\n-\t{\n-\t  if (GET_MODE (op0) != V16QImode)\n-\t    {\n-\t      orig_op0 = op0;\n-\t      op0 = gen_reg_rtx (V16QImode);\n-\t    }\n-\t  op1 = gen_lowpart (V16QImode, op1);\n-\t  /* We will eventually emit movups based on insn attributes.  */\n-\t  emit_insn (gen_rtx_SET (op0, op1));\n-\t  if (orig_op0)\n-\t    emit_move_insn (orig_op0, gen_lowpart (GET_MODE (orig_op0), op0));\n-\t}\n-      else if (TARGET_SSE2 && mode == V2DFmode)\n+      if (TARGET_SSE2 && mode == V2DFmode)\n         {\n           rtx zero;\n \n-\t  if (TARGET_AVX\n-\t      || TARGET_SSE_UNALIGNED_LOAD_OPTIMAL\n-\t      || TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\n-\t      || optimize_insn_for_size_p ())\n-\t    {\n-\t      /* We will eventually emit movups based on insn attributes.  */\n-\t      emit_insn (gen_rtx_SET (op0, op1));\n-\t      return;\n-\t    }\n-\n \t  /* When SSE registers are split into halves, we can avoid\n \t     writing to the top half twice.  */\n \t  if (TARGET_SSE_SPLIT_REGS)\n@@ -19053,24 +19008,6 @@ ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n         {\n \t  rtx t;\n \n-\t  if (TARGET_AVX\n-\t      || TARGET_SSE_UNALIGNED_LOAD_OPTIMAL\n-\t      || TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\n-\t      || optimize_insn_for_size_p ())\n-\t    {\n-\t      if (GET_MODE (op0) != V4SFmode)\n-\t\t{\n-\t\t  orig_op0 = op0;\n-\t\t  op0 = gen_reg_rtx (V4SFmode);\n-\t\t}\n-\t      op1 = gen_lowpart (V4SFmode, op1);\n-\t      emit_insn (gen_rtx_SET (op0, op1));\n-\t      if (orig_op0)\n-\t\temit_move_insn (orig_op0,\n-\t\t\t\tgen_lowpart (GET_MODE (orig_op0), op0));\n-\t      return;\n-            }\n-\n \t  if (mode != V4SFmode)\n \t    t = gen_reg_rtx (V4SFmode);\n \t  else\n@@ -19091,49 +19028,22 @@ ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n     }\n   else if (MEM_P (op0))\n     {\n-      if (TARGET_SSE2 && GET_MODE_CLASS (mode) == MODE_VECTOR_INT)\n-        {\n-\t  op0 = gen_lowpart (V16QImode, op0);\n-\t  op1 = gen_lowpart (V16QImode, op1);\n-\t  /* We will eventually emit movups based on insn attributes.  */\n-\t  emit_insn (gen_rtx_SET (op0, op1));\n-\t}\n-      else if (TARGET_SSE2 && mode == V2DFmode)\n-\t{\n-\t  if (TARGET_AVX\n-\t      || TARGET_SSE_UNALIGNED_STORE_OPTIMAL\n-\t      || TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\n-\t      || optimize_insn_for_size_p ())\n-\t    /* We will eventually emit movups based on insn attributes.  */\n-\t    emit_insn (gen_rtx_SET (op0, op1));\n-\t  else\n-\t    {\n-\t      m = adjust_address (op0, DFmode, 0);\n-\t      emit_insn (gen_sse2_storelpd (m, op1));\n-\t      m = adjust_address (op0, DFmode, 8);\n-\t      emit_insn (gen_sse2_storehpd (m, op1));\n-\t    }\n+      if (TARGET_SSE2 && mode == V2DFmode)\n+\t{\n+\t  m = adjust_address (op0, DFmode, 0);\n+\t  emit_insn (gen_sse2_storelpd (m, op1));\n+\t  m = adjust_address (op0, DFmode, 8);\n+\t  emit_insn (gen_sse2_storehpd (m, op1));\n \t}\n       else\n \t{\n \t  if (mode != V4SFmode)\n \t    op1 = gen_lowpart (V4SFmode, op1);\n \n-\t  if (TARGET_AVX\n-\t      || TARGET_SSE_UNALIGNED_STORE_OPTIMAL\n-\t      || TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\n-\t      || optimize_insn_for_size_p ())\n-\t    {\n-\t      op0 = gen_lowpart (V4SFmode, op0);\n-\t      emit_insn (gen_rtx_SET (op0, op1));\n-\t    }\n-\t  else\n-\t    {\n-\t      m = adjust_address (op0, V2SFmode, 0);\n-\t      emit_insn (gen_sse_storelps (m, op1));\n-\t      m = adjust_address (op0, V2SFmode, 8);\n-\t      emit_insn (gen_sse_storehps (m, op1));\n-\t    }\n+\t  m = adjust_address (op0, V2SFmode, 0);\n+\t  emit_insn (gen_sse_storelps (m, op1));\n+\t  m = adjust_address (op0, V2SFmode, 8);\n+\t  emit_insn (gen_sse_storehps (m, op1));\n \t}\n     }\n   else"}]}