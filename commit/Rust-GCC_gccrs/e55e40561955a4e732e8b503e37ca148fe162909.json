{"sha": "e55e40561955a4e732e8b503e37ca148fe162909", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZTU1ZTQwNTYxOTU1YTRlNzMyZThiNTAzZTM3Y2ExNDhmZTE2MjkwOQ==", "commit": {"author": {"name": "Georg-Johann Lay", "email": "avr@gjlay.de", "date": "2012-08-24T12:42:48Z"}, "committer": {"name": "Georg-Johann Lay", "email": "gjl@gcc.gnu.org", "date": "2012-08-24T12:42:48Z"}, "message": "re PR target/54222 ([avr] Implement fixed-point support)\n\nlibgcc/\n\tPR target/54222\n\t* config/avr/lib1funcs-fixed.S: New file.\n\t* config/avr/lib1funcs.S: Include it.  Undefine some divmodsi\n\tafter they are used.\n\t(neg2, neg4): New macros.\n\t(__mulqihi3,__umulqihi3,__mulhi3): Rewrite non-MUL variants.\n\t(__mulhisi3,__umulhisi3,__mulsi3): Rewrite non-MUL variants.\n\t(__umulhisi3): Speed up MUL variant if there is enough flash.\n\t* config/avr/avr-lib.h (TA, UTA): Adjust according to gcc's\n\tavr-modes.def.\n\t* config/avr/t-avr (LIB1ASMFUNCS): Add: _fractqqsf, _fractuqqsf,\n\t_fracthqsf, _fractuhqsf, _fracthasf, _fractuhasf, _fractsasf,\n\t_fractusasf, _fractsfqq, _fractsfuqq, _fractsfhq, _fractsfuhq,\n\t_fractsfha, _fractsfsa, _mulqq3, _muluqq3, _mulhq3, _muluhq3,\n\t_mulha3, _muluha3, _mulsa3, _mulusa3, _divqq3, _udivuqq3, _divhq3,\n\t_udivuhq3, _divha3, _udivuha3, _divsa3, _udivusa3.\n\t(LIB2FUNCS_EXCLUDE): Add supported functions.\n\ngcc/\n\tPR target/54222\n\t* avr-modes.def (HA, SA, DA, TA, UTA): Adjust modes.\n\t* avr/avr-fixed.md: New file.\n\t* avr/avr.md: Include it.\n\t(cc): Add: minus.\n\t(adjust_len): Add: minus, minus64, ufract, sfract.\n\t(ALL1, ALL2, ALL4, ORDERED234): New mode iterators.\n\t(MOVMODE): Add: QQ, UQQ, HQ, UHQ, HA, UHA, SQ, USQ, SA, USA.\n\t(MPUSH): Add: HQ, UHQ, HA, UHA, SQ, USQ, SA, USA.\n\t(pushqi1, xload8_A, xload_8, movqi_insn, *reload_inqi, addqi3,\n\tsubqi3, ashlqi3, *ashlqi3, ashrqi3, lshrqi3, *lshrqi3, *cmpqi, \n\tcbranchqi4, *cpse.eq): Generalize to handle all 8-bit modes in ALL1.\n\t(*movhi, reload_inhi, addhi3, *addhi3, addhi3_clobber, subhi3,\n\tashlhi3, *ashlhi3_const, ashrhi3, *ashirhi3_const, lshrhi3,\n\t*lshrhi3_const, *cmphi, cbranchhi4): Generalize to handle all\n\t16-bit modes in ALL2.\n\t(subhi3, casesi, strlenhi): Add clobber when expanding minus:HI.\n\t(*movsi, *reload_insi, addsi3, subsi3, ashlsi3, *ashlsi3_const,\n\tashrsi3, *ashrhi3_const, *ashrsi3_const, lshrsi3, *lshrsi3_const,\n\t*reversed_tstsi, *cmpsi, cbranchsi4): Generalize to handle all\n\t32-bit modes in ALL4.\n\t* avr-dimode.md (ALL8): New mode iterator.\n\t(adddi3, adddi3_insn, adddi3_const_insn, subdi3, subdi3_insn,\n\tsubdi3_const_insn, cbranchdi4, compare_di2,\n\tcompare_const_di2, ashrdi3, lshrdi3, rotldi3, ashldi3_insn,\n\tashrdi3_insn, lshrdi3_insn, rotldi3_insn): Generalize to handle\n\tall 64-bit modes in ALL8.\n\t* config/avr/avr-protos.h (avr_to_int_mode): New prototype.\n\t(avr_out_fract, avr_out_minus, avr_out_minus64): New prototypes.\n\t* config/avr/avr.c (TARGET_FIXED_POINT_SUPPORTED_P): Define to...\n\t(avr_fixed_point_supported_p): ...this new static function.\n\t(TARGET_BUILD_BUILTIN_VA_LIST): Define to...\n\t(avr_build_builtin_va_list): ...this new static function.\n\t(avr_adjust_type_node): New static function.\n\t(avr_scalar_mode_supported_p): Allow if ALL_FIXED_POINT_MODE_P.\n\t(avr_builtin_setjmp_frame_value): Use gen_subhi3 and return new\n\tpseudo instead of gen_rtx_MINUS.\n\t(avr_print_operand, avr_operand_rtx_cost): Handle: CONST_FIXED.\n\t(notice_update_cc): Handle: CC_MINUS.\n\t(output_movqi): Generalize to handle respective fixed-point modes.\n\t(output_movhi, output_movsisf, avr_2word_insn_p): Ditto.\n\t(avr_out_compare, avr_out_plus_1): Also handle fixed-point modes.\n\t(avr_assemble_integer): Ditto.\n\t(output_reload_in_const, output_reload_insisf): Ditto.\n\t(avr_compare_pattern): Skip all modes > 4 bytes.\n\t(avr_2word_insn_p): Skip movuqq_insn, movqq_insn.\n\t(avr_out_fract, avr_out_minus, avr_out_minus64): New functions.\n\t(avr_to_int_mode): New function.\n\t(adjust_insn_length): Handle: ADJUST_LEN_SFRACT,\n\tADJUST_LEN_UFRACT, ADJUST_LEN_MINUS, ADJUST_LEN_MINUS64.\n\t* config/avr/predicates.md (const0_operand): Allow const_fixed.\n\t(const_operand, const_or_immediate_operand): New.\n\t(nonmemory_or_const_operand): New.\n\t* config/avr/constraints.md (Ynn, Y00, Y01, Y02, Ym1, Ym2, YIJ):\n\tNew constraints.\n\t* config/avr/avr.h (LONG_LONG_ACCUM_TYPE_SIZE): Define.\n\nFrom-SVN: r190644", "tree": {"sha": "9180eac44e1ace2c0794f565cff49b3847b05c2b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9180eac44e1ace2c0794f565cff49b3847b05c2b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/e55e40561955a4e732e8b503e37ca148fe162909", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e55e40561955a4e732e8b503e37ca148fe162909", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e55e40561955a4e732e8b503e37ca148fe162909", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e55e40561955a4e732e8b503e37ca148fe162909/comments", "author": null, "committer": {"login": "sprintersb", "id": 8905355, "node_id": "MDQ6VXNlcjg5MDUzNTU=", "avatar_url": "https://avatars.githubusercontent.com/u/8905355?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sprintersb", "html_url": "https://github.com/sprintersb", "followers_url": "https://api.github.com/users/sprintersb/followers", "following_url": "https://api.github.com/users/sprintersb/following{/other_user}", "gists_url": "https://api.github.com/users/sprintersb/gists{/gist_id}", "starred_url": "https://api.github.com/users/sprintersb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sprintersb/subscriptions", "organizations_url": "https://api.github.com/users/sprintersb/orgs", "repos_url": "https://api.github.com/users/sprintersb/repos", "events_url": "https://api.github.com/users/sprintersb/events{/privacy}", "received_events_url": "https://api.github.com/users/sprintersb/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "2960a3685367ff2a1da3dfa428c200e07d97fe6e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2960a3685367ff2a1da3dfa428c200e07d97fe6e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2960a3685367ff2a1da3dfa428c200e07d97fe6e"}], "stats": {"total": 3729, "additions": 3049, "deletions": 680}, "files": [{"sha": "04d08a4e5219b02c802377f72a1335d961739590", "filename": "gcc/ChangeLog", "status": "modified", "additions": 59, "deletions": 0, "changes": 59, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -1,3 +1,62 @@\n+2012-08-24  Georg-Johann Lay  <avr@gjlay.de>\n+\n+\tPR target/54222\n+\t* avr-modes.def (HA, SA, DA, TA, UTA): Adjust modes.\n+\t* avr/avr-fixed.md: New file.\n+\t* avr/avr.md: Include it.\n+\t(cc): Add: minus.\n+\t(adjust_len): Add: minus, minus64, ufract, sfract.\n+\t(ALL1, ALL2, ALL4, ORDERED234): New mode iterators.\n+\t(MOVMODE): Add: QQ, UQQ, HQ, UHQ, HA, UHA, SQ, USQ, SA, USA.\n+\t(MPUSH): Add: HQ, UHQ, HA, UHA, SQ, USQ, SA, USA.\n+\t(pushqi1, xload8_A, xload_8, movqi_insn, *reload_inqi, addqi3,\n+\tsubqi3, ashlqi3, *ashlqi3, ashrqi3, lshrqi3, *lshrqi3, *cmpqi, \n+\tcbranchqi4, *cpse.eq): Generalize to handle all 8-bit modes in ALL1.\n+\t(*movhi, reload_inhi, addhi3, *addhi3, addhi3_clobber, subhi3,\n+\tashlhi3, *ashlhi3_const, ashrhi3, *ashirhi3_const, lshrhi3,\n+\t*lshrhi3_const, *cmphi, cbranchhi4): Generalize to handle all\n+\t16-bit modes in ALL2.\n+\t(subhi3, casesi, strlenhi): Add clobber when expanding minus:HI.\n+\t(*movsi, *reload_insi, addsi3, subsi3, ashlsi3, *ashlsi3_const,\n+\tashrsi3, *ashrhi3_const, *ashrsi3_const, lshrsi3, *lshrsi3_const,\n+\t*reversed_tstsi, *cmpsi, cbranchsi4): Generalize to handle all\n+\t32-bit modes in ALL4.\n+\t* avr-dimode.md (ALL8): New mode iterator.\n+\t(adddi3, adddi3_insn, adddi3_const_insn, subdi3, subdi3_insn,\n+\tsubdi3_const_insn, cbranchdi4, compare_di2,\n+\tcompare_const_di2, ashrdi3, lshrdi3, rotldi3, ashldi3_insn,\n+\tashrdi3_insn, lshrdi3_insn, rotldi3_insn): Generalize to handle\n+\tall 64-bit modes in ALL8.\n+\t* config/avr/avr-protos.h (avr_to_int_mode): New prototype.\n+\t(avr_out_fract, avr_out_minus, avr_out_minus64): New prototypes.\n+\t* config/avr/avr.c (TARGET_FIXED_POINT_SUPPORTED_P): Define to...\n+\t(avr_fixed_point_supported_p): ...this new static function.\n+\t(TARGET_BUILD_BUILTIN_VA_LIST): Define to...\n+\t(avr_build_builtin_va_list): ...this new static function.\n+\t(avr_adjust_type_node): New static function.\n+\t(avr_scalar_mode_supported_p): Allow if ALL_FIXED_POINT_MODE_P.\n+\t(avr_builtin_setjmp_frame_value): Use gen_subhi3 and return new\n+\tpseudo instead of gen_rtx_MINUS.\n+\t(avr_print_operand, avr_operand_rtx_cost): Handle: CONST_FIXED.\n+\t(notice_update_cc): Handle: CC_MINUS.\n+\t(output_movqi): Generalize to handle respective fixed-point modes.\n+\t(output_movhi, output_movsisf, avr_2word_insn_p): Ditto.\n+\t(avr_out_compare, avr_out_plus_1): Also handle fixed-point modes.\n+\t(avr_assemble_integer): Ditto.\n+\t(output_reload_in_const, output_reload_insisf): Ditto.\n+\t(avr_compare_pattern): Skip all modes > 4 bytes.\n+\t(avr_2word_insn_p): Skip movuqq_insn, movqq_insn.\n+\t(avr_out_fract, avr_out_minus, avr_out_minus64): New functions.\n+\t(avr_to_int_mode): New function.\n+\t(adjust_insn_length): Handle: ADJUST_LEN_SFRACT,\n+\tADJUST_LEN_UFRACT, ADJUST_LEN_MINUS, ADJUST_LEN_MINUS64.\n+\t* config/avr/predicates.md (const0_operand): Allow const_fixed.\n+\t(const_operand, const_or_immediate_operand): New.\n+\t(nonmemory_or_const_operand): New.\n+\t* config/avr/constraints.md (Ynn, Y00, Y01, Y02, Ym1, Ym2, YIJ):\n+\tNew constraints.\n+\t* config/avr/avr.h (LONG_LONG_ACCUM_TYPE_SIZE): Define.\n+\n 2012-08-23  Kenneth Zadeck <zadeck@naturalbridge.com>\n \n \t* alias.c (rtx_equal_for_memref_p): Convert constant cases."}, {"sha": "ed5752319eb26e3f8aa26ad2aca38624ac5c9025", "filename": "gcc/config/avr/avr-dimode.md", "status": "modified", "additions": 130, "deletions": 59, "changes": 189, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr-dimode.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr-dimode.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Favr-dimode.md?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -47,44 +47,58 @@\n   [(ACC_A\t18)\n    (ACC_B\t10)])\n \n+;; Supported modes that are 8 bytes wide\n+(define_mode_iterator ALL8 [(DI \"\")\n+                            (DQ \"\") (UDQ \"\")\n+                            (DA \"\") (UDA \"\")\n+                            (TA \"\") (UTA \"\")])\n+\n ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n ;; Addition\n ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n \n-(define_expand \"adddi3\"\n-  [(parallel [(match_operand:DI 0 \"general_operand\" \"\")\n-              (match_operand:DI 1 \"general_operand\" \"\")\n-              (match_operand:DI 2 \"general_operand\" \"\")])]\n+;; \"adddi3\"\n+;; \"adddq3\" \"addudq3\"\n+;; \"addda3\" \"adduda3\"\n+;; \"addta3\" \"adduta3\"\n+(define_expand \"add<mode>3\"\n+  [(parallel [(match_operand:ALL8 0 \"general_operand\" \"\")\n+              (match_operand:ALL8 1 \"general_operand\" \"\")\n+              (match_operand:ALL8 2 \"general_operand\" \"\")])]\n   \"avr_have_dimode\"\n   {\n-    rtx acc_a = gen_rtx_REG (DImode, ACC_A);\n+    rtx acc_a = gen_rtx_REG (<MODE>mode, ACC_A);\n \n     emit_move_insn (acc_a, operands[1]);\n \n-    if (s8_operand (operands[2], VOIDmode))\n+    if (DImode == <MODE>mode\n+        && s8_operand (operands[2], VOIDmode))\n       {\n         emit_move_insn (gen_rtx_REG (QImode, REG_X), operands[2]);\n         emit_insn (gen_adddi3_const8_insn ());\n       }        \n-    else if (CONST_INT_P (operands[2])\n-             || CONST_DOUBLE_P (operands[2]))\n+    else if (const_operand (operands[2], GET_MODE (operands[2])))\n       {\n-        emit_insn (gen_adddi3_const_insn (operands[2]));\n+        emit_insn (gen_add<mode>3_const_insn (operands[2]));\n       }\n     else\n       {\n-        emit_move_insn (gen_rtx_REG (DImode, ACC_B), operands[2]);\n-        emit_insn (gen_adddi3_insn ());\n+        emit_move_insn (gen_rtx_REG (<MODE>mode, ACC_B), operands[2]);\n+        emit_insn (gen_add<mode>3_insn ());\n       }\n \n     emit_move_insn (operands[0], acc_a);\n     DONE;\n   })\n \n-(define_insn \"adddi3_insn\"\n-  [(set (reg:DI ACC_A)\n-        (plus:DI (reg:DI ACC_A)\n-                 (reg:DI ACC_B)))]\n+;; \"adddi3_insn\"\n+;; \"adddq3_insn\" \"addudq3_insn\"\n+;; \"addda3_insn\" \"adduda3_insn\"\n+;; \"addta3_insn\" \"adduta3_insn\"\n+(define_insn \"add<mode>3_insn\"\n+  [(set (reg:ALL8 ACC_A)\n+        (plus:ALL8 (reg:ALL8 ACC_A)\n+                   (reg:ALL8 ACC_B)))]\n   \"avr_have_dimode\"\n   \"%~call __adddi3\"\n   [(set_attr \"adjust_len\" \"call\")\n@@ -99,10 +113,14 @@\n   [(set_attr \"adjust_len\" \"call\")\n    (set_attr \"cc\" \"clobber\")])\n \n-(define_insn \"adddi3_const_insn\"\n-  [(set (reg:DI ACC_A)\n-        (plus:DI (reg:DI ACC_A)\n-                 (match_operand:DI 0 \"const_double_operand\" \"n\")))]\n+;; \"adddi3_const_insn\"\n+;; \"adddq3_const_insn\" \"addudq3_const_insn\"\n+;; \"addda3_const_insn\" \"adduda3_const_insn\"\n+;; \"addta3_const_insn\" \"adduta3_const_insn\"\n+(define_insn \"add<mode>3_const_insn\"\n+  [(set (reg:ALL8 ACC_A)\n+        (plus:ALL8 (reg:ALL8 ACC_A)\n+                   (match_operand:ALL8 0 \"const_operand\" \"n Ynn\")))]\n   \"avr_have_dimode\n    && !s8_operand (operands[0], VOIDmode)\"\n   {\n@@ -116,30 +134,62 @@\n ;; Subtraction\n ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n \n-(define_expand \"subdi3\"\n-  [(parallel [(match_operand:DI 0 \"general_operand\" \"\")\n-              (match_operand:DI 1 \"general_operand\" \"\")\n-              (match_operand:DI 2 \"general_operand\" \"\")])]\n+;; \"subdi3\"\n+;; \"subdq3\" \"subudq3\"\n+;; \"subda3\" \"subuda3\"\n+;; \"subta3\" \"subuta3\"\n+(define_expand \"sub<mode>3\"\n+  [(parallel [(match_operand:ALL8 0 \"general_operand\" \"\")\n+              (match_operand:ALL8 1 \"general_operand\" \"\")\n+              (match_operand:ALL8 2 \"general_operand\" \"\")])]\n   \"avr_have_dimode\"\n   {\n-    rtx acc_a = gen_rtx_REG (DImode, ACC_A);\n+    rtx acc_a = gen_rtx_REG (<MODE>mode, ACC_A);\n \n     emit_move_insn (acc_a, operands[1]);\n-    emit_move_insn (gen_rtx_REG (DImode, ACC_B), operands[2]);\n-    emit_insn (gen_subdi3_insn ());\n+\n+    if (const_operand (operands[2], GET_MODE (operands[2])))\n+      {\n+        emit_insn (gen_sub<mode>3_const_insn (operands[2]));\n+      }\n+    else\n+     {\n+       emit_move_insn (gen_rtx_REG (<MODE>mode, ACC_B), operands[2]);\n+       emit_insn (gen_sub<mode>3_insn ());\n+     }\n+\n     emit_move_insn (operands[0], acc_a);\n     DONE;\n   })\n \n-(define_insn \"subdi3_insn\"\n-  [(set (reg:DI ACC_A)\n-        (minus:DI (reg:DI ACC_A)\n-                  (reg:DI ACC_B)))]\n+;; \"subdi3_insn\"\n+;; \"subdq3_insn\" \"subudq3_insn\"\n+;; \"subda3_insn\" \"subuda3_insn\"\n+;; \"subta3_insn\" \"subuta3_insn\"\n+(define_insn \"sub<mode>3_insn\"\n+  [(set (reg:ALL8 ACC_A)\n+        (minus:ALL8 (reg:ALL8 ACC_A)\n+                    (reg:ALL8 ACC_B)))]\n   \"avr_have_dimode\"\n   \"%~call __subdi3\"\n   [(set_attr \"adjust_len\" \"call\")\n    (set_attr \"cc\" \"set_czn\")])\n \n+;; \"subdi3_const_insn\"\n+;; \"subdq3_const_insn\" \"subudq3_const_insn\"\n+;; \"subda3_const_insn\" \"subuda3_const_insn\"\n+;; \"subta3_const_insn\" \"subuta3_const_insn\"\n+(define_insn \"sub<mode>3_const_insn\"\n+  [(set (reg:ALL8 ACC_A)\n+        (minus:ALL8 (reg:ALL8 ACC_A)\n+                    (match_operand:ALL8 0 \"const_operand\" \"n Ynn\")))]\n+  \"avr_have_dimode\"\n+  {\n+    return avr_out_minus64 (operands[0], NULL);\n+  }\n+  [(set_attr \"adjust_len\" \"minus64\")\n+   (set_attr \"cc\" \"clobber\")])\n+\n \n ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n ;; Negation\n@@ -180,15 +230,19 @@\n          (pc)))]\n   \"avr_have_dimode\")\n \n-(define_expand \"cbranchdi4\"\n-  [(parallel [(match_operand:DI 1 \"register_operand\" \"\")\n-              (match_operand:DI 2 \"nonmemory_operand\" \"\")\n+;; \"cbranchdi4\"\n+;; \"cbranchdq4\" \"cbranchudq4\"\n+;; \"cbranchda4\" \"cbranchuda4\"\n+;; \"cbranchta4\" \"cbranchuta4\"\n+(define_expand \"cbranch<mode>4\"\n+  [(parallel [(match_operand:ALL8 1 \"register_operand\" \"\")\n+              (match_operand:ALL8 2 \"nonmemory_operand\" \"\")\n               (match_operator 0 \"ordered_comparison_operator\" [(cc0)\n                                                                (const_int 0)])\n               (label_ref (match_operand 3 \"\" \"\"))])]\n   \"avr_have_dimode\"\n   {\n-    rtx acc_a = gen_rtx_REG (DImode, ACC_A);\n+    rtx acc_a = gen_rtx_REG (<MODE>mode, ACC_A);\n \n     emit_move_insn (acc_a, operands[1]);\n \n@@ -197,25 +251,28 @@\n         emit_move_insn (gen_rtx_REG (QImode, REG_X), operands[2]);\n         emit_insn (gen_compare_const8_di2 ());\n       }        \n-    else if (CONST_INT_P (operands[2])\n-             || CONST_DOUBLE_P (operands[2]))\n+    else if (const_operand (operands[2], GET_MODE (operands[2])))\n       {\n-        emit_insn (gen_compare_const_di2 (operands[2]));\n+        emit_insn (gen_compare_const_<mode>2 (operands[2]));\n       }\n     else\n       {\n-        emit_move_insn (gen_rtx_REG (DImode, ACC_B), operands[2]);\n-        emit_insn (gen_compare_di2 ());\n+        emit_move_insn (gen_rtx_REG (<MODE>mode, ACC_B), operands[2]);\n+        emit_insn (gen_compare_<mode>2 ());\n       }\n \n     emit_jump_insn (gen_conditional_jump (operands[0], operands[3]));\n     DONE;\n   })\n \n-(define_insn \"compare_di2\"\n+;; \"compare_di2\"\n+;; \"compare_dq2\" \"compare_udq2\"\n+;; \"compare_da2\" \"compare_uda2\"\n+;; \"compare_ta2\" \"compare_uta2\"\n+(define_insn \"compare_<mode>2\"\n   [(set (cc0)\n-        (compare (reg:DI ACC_A)\n-                 (reg:DI ACC_B)))]\n+        (compare (reg:ALL8 ACC_A)\n+                 (reg:ALL8 ACC_B)))]\n   \"avr_have_dimode\"\n   \"%~call __cmpdi2\"\n   [(set_attr \"adjust_len\" \"call\")\n@@ -230,10 +287,14 @@\n   [(set_attr \"adjust_len\" \"call\")\n    (set_attr \"cc\" \"compare\")])\n \n-(define_insn \"compare_const_di2\"\n+;; \"compare_const_di2\"\n+;; \"compare_const_dq2\" \"compare_const_udq2\"\n+;; \"compare_const_da2\" \"compare_const_uda2\"\n+;; \"compare_const_ta2\" \"compare_const_uta2\"\n+(define_insn \"compare_const_<mode>2\"\n   [(set (cc0)\n-        (compare (reg:DI ACC_A)\n-                 (match_operand:DI 0 \"const_double_operand\" \"n\")))\n+        (compare (reg:ALL8 ACC_A)\n+                 (match_operand:ALL8 0 \"const_operand\" \"n Ynn\")))\n    (clobber (match_scratch:QI 1 \"=&d\"))]\n   \"avr_have_dimode\n    && !s8_operand (operands[0], VOIDmode)\"\n@@ -254,29 +315,39 @@\n ;; Shift functions from libgcc are called without defining these insns,\n ;; but with them we can describe their reduced register footprint.\n \n-;; \"ashldi3\"\n-;; \"ashrdi3\"\n-;; \"lshrdi3\"\n-;; \"rotldi3\"\n-(define_expand \"<code_stdname>di3\"\n-  [(parallel [(match_operand:DI 0 \"general_operand\" \"\")\n-              (di_shifts:DI (match_operand:DI 1 \"general_operand\" \"\")\n-                            (match_operand:QI 2 \"general_operand\" \"\"))])]\n+;; \"ashldi3\"   \"ashrdi3\"   \"lshrdi3\"   \"rotldi3\"\n+;; \"ashldq3\"   \"ashrdq3\"   \"lshrdq3\"   \"rotldq3\"\n+;; \"ashlda3\"   \"ashrda3\"   \"lshrda3\"   \"rotlda3\"\n+;; \"ashlta3\"   \"ashrta3\"   \"lshrta3\"   \"rotlta3\"\n+;; \"ashludq3\"  \"ashrudq3\"  \"lshrudq3\"  \"rotludq3\"\n+;; \"ashluda3\"  \"ashruda3\"  \"lshruda3\"  \"rotluda3\"\n+;; \"ashluta3\"  \"ashruta3\"  \"lshruta3\"  \"rotluta3\"\n+(define_expand \"<code_stdname><mode>3\"\n+  [(parallel [(match_operand:ALL8 0 \"general_operand\" \"\")\n+              (di_shifts:ALL8 (match_operand:ALL8 1 \"general_operand\" \"\")\n+                              (match_operand:QI 2 \"general_operand\" \"\"))])]\n   \"avr_have_dimode\"\n   {\n-    rtx acc_a = gen_rtx_REG (DImode, ACC_A);\n+    rtx acc_a = gen_rtx_REG (<MODE>mode, ACC_A);\n \n     emit_move_insn (acc_a, operands[1]);\n     emit_move_insn (gen_rtx_REG (QImode, 16), operands[2]);\n-    emit_insn (gen_<code_stdname>di3_insn ());\n+    emit_insn (gen_<code_stdname><mode>3_insn ());\n     emit_move_insn (operands[0], acc_a);\n     DONE;\n   })\n \n-(define_insn \"<code_stdname>di3_insn\"\n-  [(set (reg:DI ACC_A)\n-        (di_shifts:DI (reg:DI ACC_A)\n-                      (reg:QI 16)))]\n+;; \"ashldi3_insn\"   \"ashrdi3_insn\"   \"lshrdi3_insn\"   \"rotldi3_insn\"\n+;; \"ashldq3_insn\"   \"ashrdq3_insn\"   \"lshrdq3_insn\"   \"rotldq3_insn\"\n+;; \"ashlda3_insn\"   \"ashrda3_insn\"   \"lshrda3_insn\"   \"rotlda3_insn\"\n+;; \"ashlta3_insn\"   \"ashrta3_insn\"   \"lshrta3_insn\"   \"rotlta3_insn\"\n+;; \"ashludq3_insn\"  \"ashrudq3_insn\"  \"lshrudq3_insn\"  \"rotludq3_insn\"\n+;; \"ashluda3_insn\"  \"ashruda3_insn\"  \"lshruda3_insn\"  \"rotluda3_insn\"\n+;; \"ashluta3_insn\"  \"ashruta3_insn\"  \"lshruta3_insn\"  \"rotluta3_insn\"\n+(define_insn \"<code_stdname><mode>3_insn\"\n+  [(set (reg:ALL8 ACC_A)\n+        (di_shifts:ALL8 (reg:ALL8 ACC_A)\n+                        (reg:QI 16)))]\n   \"avr_have_dimode\"\n   \"%~call __<code_stdname>di3\"\n   [(set_attr \"adjust_len\" \"call\")"}, {"sha": "bfbdaecf2157aaed1f24e38ae9397bb14ffad17f", "filename": "gcc/config/avr/avr-fixed.md", "status": "added", "additions": 287, "deletions": 0, "changes": 287, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr-fixed.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr-fixed.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Favr-fixed.md?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -0,0 +1,287 @@\n+;;   This file contains instructions that support fixed-point operations\n+;;   for Atmel AVR micro controllers.\n+;;   Copyright (C) 2012\n+;;   Free Software Foundation, Inc.\n+;;\n+;;   Contributed by Sean D'Epagnier  (sean@depagnier.com)\n+;;                  Georg-Johann Lay (avr@gjlay.de)\n+\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify\n+;; it under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful,\n+;; but WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+;; GNU General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+(define_mode_iterator ALL1Q [(QQ \"\") (UQQ \"\")])\n+(define_mode_iterator ALL2Q [(HQ \"\") (UHQ \"\")])\n+(define_mode_iterator ALL2A [(HA \"\") (UHA \"\")])\n+(define_mode_iterator ALL2QA [(HQ \"\") (UHQ \"\")\n+                              (HA \"\") (UHA \"\")])\n+(define_mode_iterator ALL4A [(SA \"\") (USA \"\")])\n+\n+;;; Conversions\n+\n+(define_mode_iterator FIXED_A\n+  [(QQ \"\") (UQQ \"\")\n+   (HQ \"\") (UHQ \"\") (HA \"\") (UHA \"\")\n+   (SQ \"\") (USQ \"\") (SA \"\") (USA \"\")\n+   (DQ \"\") (UDQ \"\") (DA \"\") (UDA \"\")\n+   (TA \"\") (UTA \"\")\n+   (QI \"\") (HI \"\") (SI \"\") (DI \"\")])\n+\n+;; Same so that be can build cross products\n+\n+(define_mode_iterator FIXED_B\n+  [(QQ \"\") (UQQ \"\")\n+   (HQ \"\") (UHQ \"\") (HA \"\") (UHA \"\")\n+   (SQ \"\") (USQ \"\") (SA \"\") (USA \"\")\n+   (DQ \"\") (UDQ \"\") (DA \"\") (UDA \"\")\n+   (TA \"\") (UTA \"\")\n+   (QI \"\") (HI \"\") (SI \"\") (DI \"\")])\n+\n+(define_insn \"fract<FIXED_B:mode><FIXED_A:mode>2\"\n+  [(set (match_operand:FIXED_A 0 \"register_operand\" \"=r\")\n+        (fract_convert:FIXED_A\n+         (match_operand:FIXED_B 1 \"register_operand\" \"r\")))]\n+  \"<FIXED_B:MODE>mode != <FIXED_A:MODE>mode\"\n+  {\n+    return avr_out_fract (insn, operands, true, NULL);\n+  }\n+  [(set_attr \"cc\" \"clobber\")\n+   (set_attr \"adjust_len\" \"sfract\")])\n+\n+(define_insn \"fractuns<FIXED_B:mode><FIXED_A:mode>2\"\n+  [(set (match_operand:FIXED_A 0 \"register_operand\" \"=r\")\n+        (unsigned_fract_convert:FIXED_A\n+         (match_operand:FIXED_B 1 \"register_operand\" \"r\")))]\n+  \"<FIXED_B:MODE>mode != <FIXED_A:MODE>mode\"\n+  {\n+    return avr_out_fract (insn, operands, false, NULL);\n+  }\n+  [(set_attr \"cc\" \"clobber\")\n+   (set_attr \"adjust_len\" \"ufract\")])\n+\n+;******************************************************************************\n+; mul\n+\n+;; \"mulqq3\" \"muluqq3\"\n+(define_expand \"mul<mode>3\"\n+  [(parallel [(match_operand:ALL1Q 0 \"register_operand\" \"\")\n+              (match_operand:ALL1Q 1 \"register_operand\" \"\")\n+              (match_operand:ALL1Q 2 \"register_operand\" \"\")])]\n+  \"\"\n+  {\n+    emit_insn (AVR_HAVE_MUL\n+      ? gen_mul<mode>3_enh (operands[0], operands[1], operands[2])\n+      : gen_mul<mode>3_nomul (operands[0], operands[1], operands[2]));\n+    DONE;\n+  })\n+\n+(define_insn \"mulqq3_enh\"\n+  [(set (match_operand:QQ 0 \"register_operand\"         \"=r\")\n+        (mult:QQ (match_operand:QQ 1 \"register_operand\" \"a\")\n+                 (match_operand:QQ 2 \"register_operand\" \"a\")))]\n+  \"AVR_HAVE_MUL\"\n+  \"fmuls %1,%2\\;dec r1\\;brvs 0f\\;inc r1\\;0:\\;mov %0,r1\\;clr __zero_reg__\"\n+  [(set_attr \"length\" \"6\")\n+   (set_attr \"cc\" \"clobber\")])\n+\n+(define_insn \"muluqq3_enh\"\n+  [(set (match_operand:UQQ 0 \"register_operand\"          \"=r\")\n+        (mult:UQQ (match_operand:UQQ 1 \"register_operand\" \"r\")\n+                  (match_operand:UQQ 2 \"register_operand\" \"r\")))]\n+  \"AVR_HAVE_MUL\"\n+  \"mul %1,%2\\;mov %0,r1\\;clr __zero_reg__\"\n+  [(set_attr \"length\" \"3\")\n+   (set_attr \"cc\" \"clobber\")])\n+\n+(define_expand \"mulqq3_nomul\"\n+  [(set (reg:QQ 24)\n+        (match_operand:QQ 1 \"register_operand\" \"\"))\n+   (set (reg:QQ 25)\n+        (match_operand:QQ 2 \"register_operand\" \"\"))\n+   ;; \"*mulqq3.call\"\n+   (parallel [(set (reg:QQ 23)\n+                   (mult:QQ (reg:QQ 24)\n+                            (reg:QQ 25)))\n+              (clobber (reg:QI 22))\n+              (clobber (reg:HI 24))])\n+   (set (match_operand:QQ 0 \"register_operand\" \"\")\n+        (reg:QQ 23))]\n+  \"!AVR_HAVE_MUL\")\n+\n+(define_expand \"muluqq3_nomul\"\n+  [(set (reg:UQQ 22)\n+        (match_operand:UQQ 1 \"register_operand\" \"\"))\n+   (set (reg:UQQ 24)\n+        (match_operand:UQQ 2 \"register_operand\" \"\"))\n+   ;; \"*umulqihi3.call\"\n+   (parallel [(set (reg:HI 24)\n+                   (mult:HI (zero_extend:HI (reg:QI 22))\n+                            (zero_extend:HI (reg:QI 24))))\n+              (clobber (reg:QI 21))\n+              (clobber (reg:HI 22))])\n+   (set (match_operand:UQQ 0 \"register_operand\" \"\")\n+        (reg:UQQ 25))]\n+  \"!AVR_HAVE_MUL\")\n+\n+(define_insn \"*mulqq3.call\"\n+  [(set (reg:QQ 23)\n+        (mult:QQ (reg:QQ 24)\n+                 (reg:QQ 25)))\n+   (clobber (reg:QI 22))\n+   (clobber (reg:HI 24))]\n+  \"!AVR_HAVE_MUL\"\n+  \"%~call __mulqq3\"\n+  [(set_attr \"type\" \"xcall\")\n+   (set_attr \"cc\" \"clobber\")])\n+\n+\n+;; \"mulhq3\" \"muluhq3\"\n+;; \"mulha3\" \"muluha3\"\n+(define_expand \"mul<mode>3\"\n+  [(set (reg:ALL2QA 18)\n+        (match_operand:ALL2QA 1 \"register_operand\" \"\"))\n+   (set (reg:ALL2QA 26)\n+        (match_operand:ALL2QA 2 \"register_operand\" \"\"))\n+   ;; \"*mulhq3.call.enh\"\n+   (parallel [(set (reg:ALL2QA 24)\n+                   (mult:ALL2QA (reg:ALL2QA 18)\n+                                (reg:ALL2QA 26)))\n+              (clobber (reg:HI 22))])\n+   (set (match_operand:ALL2QA 0 \"register_operand\" \"\")\n+        (reg:ALL2QA 24))]\n+  \"AVR_HAVE_MUL\")\n+\n+;; \"*mulhq3.call\"  \"*muluhq3.call\"\n+;; \"*mulha3.call\"  \"*muluha3.call\"\n+(define_insn \"*mul<mode>3.call\"\n+  [(set (reg:ALL2QA 24)\n+        (mult:ALL2QA (reg:ALL2QA 18)\n+                     (reg:ALL2QA 26)))\n+   (clobber (reg:HI 22))]\n+  \"AVR_HAVE_MUL\"\n+  \"%~call __mul<mode>3\"\n+  [(set_attr \"type\" \"xcall\")\n+   (set_attr \"cc\" \"clobber\")])\n+\n+\n+;; On the enhanced core, don't clobber either input and use a separate output\n+\n+;; \"mulsa3\" \"mulusa3\"\n+(define_expand \"mul<mode>3\"\n+  [(set (reg:ALL4A 16)\n+        (match_operand:ALL4A 1 \"register_operand\" \"\"))\n+   (set (reg:ALL4A 20)\n+        (match_operand:ALL4A 2 \"register_operand\" \"\"))\n+   (set (reg:ALL4A 24)\n+        (mult:ALL4A (reg:ALL4A 16)\n+                    (reg:ALL4A 20)))\n+   (set (match_operand:ALL4A 0 \"register_operand\" \"\")\n+        (reg:ALL4A 24))]\n+  \"AVR_HAVE_MUL\")\n+\n+;; \"*mulsa3.call\" \"*mulusa3.call\"\n+(define_insn \"*mul<mode>3.call\"\n+  [(set (reg:ALL4A 24)\n+        (mult:ALL4A (reg:ALL4A 16)\n+                    (reg:ALL4A 20)))]\n+  \"AVR_HAVE_MUL\"\n+  \"%~call __mul<mode>3\"\n+  [(set_attr \"type\" \"xcall\")\n+   (set_attr \"cc\" \"clobber\")])\n+\n+; / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /\n+; div\n+\n+(define_code_iterator usdiv [udiv div])\n+\n+;; \"divqq3\" \"udivuqq3\"\n+(define_expand \"<code><mode>3\"\n+  [(set (reg:ALL1Q 25)\n+        (match_operand:ALL1Q 1 \"register_operand\" \"\"))\n+   (set (reg:ALL1Q 22)\n+        (match_operand:ALL1Q 2 \"register_operand\" \"\"))\n+   (parallel [(set (reg:ALL1Q 24)\n+                   (usdiv:ALL1Q (reg:ALL1Q 25)\n+                                (reg:ALL1Q 22)))\n+              (clobber (reg:QI 25))])\n+   (set (match_operand:ALL1Q 0 \"register_operand\" \"\")\n+        (reg:ALL1Q 24))])\n+\n+;; \"*divqq3.call\" \"*udivuqq3.call\"\n+(define_insn \"*<code><mode>3.call\"\n+  [(set (reg:ALL1Q 24)\n+        (usdiv:ALL1Q (reg:ALL1Q 25)\n+                     (reg:ALL1Q 22)))\n+   (clobber (reg:QI 25))]\n+  \"\"\n+  \"%~call __<code><mode>3\"\n+  [(set_attr \"type\" \"xcall\")\n+   (set_attr \"cc\" \"clobber\")])\n+\n+;; \"divhq3\" \"udivuhq3\"\n+;; \"divha3\" \"udivuha3\"\n+(define_expand \"<code><mode>3\"\n+  [(set (reg:ALL2QA 26)\n+        (match_operand:ALL2QA 1 \"register_operand\" \"\"))\n+   (set (reg:ALL2QA 22)\n+        (match_operand:ALL2QA 2 \"register_operand\" \"\"))\n+   (parallel [(set (reg:ALL2QA 24)\n+                   (usdiv:ALL2QA (reg:ALL2QA 26)\n+                                 (reg:ALL2QA 22)))\n+              (clobber (reg:HI 26))\n+              (clobber (reg:QI 21))])\n+   (set (match_operand:ALL2QA 0 \"register_operand\" \"\")\n+        (reg:ALL2QA 24))])\n+\n+;; \"*divhq3.call\" \"*udivuhq3.call\"\n+;; \"*divha3.call\" \"*udivuha3.call\"\n+(define_insn \"*<code><mode>3.call\"\n+  [(set (reg:ALL2QA 24)\n+        (usdiv:ALL2QA (reg:ALL2QA 26)\n+                      (reg:ALL2QA 22)))\n+   (clobber (reg:HI 26))\n+   (clobber (reg:QI 21))]\n+  \"\"\n+  \"%~call __<code><mode>3\"\n+  [(set_attr \"type\" \"xcall\")\n+   (set_attr \"cc\" \"clobber\")])\n+\n+;; Note the first parameter gets passed in already offset by 2 bytes\n+\n+;; \"divsa3\" \"udivusa3\"\n+(define_expand \"<code><mode>3\"\n+  [(set (reg:ALL4A 24)\n+        (match_operand:ALL4A 1 \"register_operand\" \"\"))\n+   (set (reg:ALL4A 18)\n+        (match_operand:ALL4A 2 \"register_operand\" \"\"))\n+   (parallel [(set (reg:ALL4A 22)\n+                   (usdiv:ALL4A (reg:ALL4A 24)\n+                                (reg:ALL4A 18)))\n+              (clobber (reg:HI 26))\n+              (clobber (reg:HI 30))])\n+   (set (match_operand:ALL4A 0 \"register_operand\" \"\")\n+        (reg:ALL4A 22))])\n+\n+;; \"*divsa3.call\" \"*udivusa3.call\"\n+(define_insn \"*<code><mode>3.call\"\n+  [(set (reg:ALL4A 22)\n+        (usdiv:ALL4A (reg:ALL4A 24)\n+                     (reg:ALL4A 18)))\n+   (clobber (reg:HI 26))\n+   (clobber (reg:HI 30))]\n+  \"\"\n+  \"%~call __<code><mode>3\"\n+  [(set_attr \"type\" \"xcall\")\n+   (set_attr \"cc\" \"clobber\")])"}, {"sha": "09e6b4983f0a6f7ea3e73f9b00c4946c665693ed", "filename": "gcc/config/avr/avr-modes.def", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Favr-modes.def?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -1 +1,28 @@\n FRACTIONAL_INT_MODE (PSI, 24, 3);\n+\n+/* On 8 bit machines it requires fewer instructions for fixed point\n+   routines if the decimal place is on a byte boundary which is not\n+   the default for signed accum types.  */\n+\n+ADJUST_IBIT (HA, 7);\n+ADJUST_FBIT (HA, 8);\n+\n+ADJUST_IBIT (SA, 15);\n+ADJUST_FBIT (SA, 16);\n+\n+ADJUST_IBIT (DA, 31);\n+ADJUST_FBIT (DA, 32);\n+\n+/* Make TA and UTA 64 bits wide.\n+   128 bit wide modes would be insane on a 8-bit machine.\n+   This needs special treatment in avr.c and avr-lib.h.  */\n+\n+ADJUST_BYTESIZE  (TA, 8);\n+ADJUST_ALIGNMENT (TA, 1);\n+ADJUST_IBIT (TA, 15);\n+ADJUST_FBIT (TA, 48);\n+\n+ADJUST_BYTESIZE  (UTA, 8);\n+ADJUST_ALIGNMENT (UTA, 1);\n+ADJUST_IBIT (UTA, 16);\n+ADJUST_FBIT (UTA, 48);"}, {"sha": "5d6fabb6b6dbf83686d95896e17c98509c0d236f", "filename": "gcc/config/avr/avr-protos.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Favr-protos.h?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -79,6 +79,9 @@ extern const char* avr_load_lpm (rtx, rtx*, int*);\n \n extern bool avr_rotate_bytes (rtx operands[]);\n \n+extern const char* avr_out_fract (rtx, rtx[], bool, int*);\n+extern rtx avr_to_int_mode (rtx);\n+\n extern void expand_prologue (void);\n extern void expand_epilogue (bool);\n extern bool avr_emit_movmemhi (rtx*);\n@@ -92,6 +95,8 @@ extern const char* avr_out_plus (rtx*, int*, int*);\n extern const char* avr_out_plus_noclobber (rtx*, int*, int*);\n extern const char* avr_out_plus64 (rtx, int*);\n extern const char* avr_out_addto_sp (rtx*, int*);\n+extern const char* avr_out_minus (rtx*, int*, int*);\n+extern const char* avr_out_minus64 (rtx, int*);\n extern const char* avr_out_xload (rtx, rtx*, int*);\n extern const char* avr_out_movmem (rtx, rtx*, int*);\n extern const char* avr_out_insert_bits (rtx*, int*);"}, {"sha": "c17533000c78729e111fe353e15502576dfa1521", "filename": "gcc/config/avr/avr.c", "status": "modified", "additions": 568, "deletions": 24, "changes": 592, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Favr.c?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -49,6 +49,10 @@\n #include \"params.h\"\n #include \"df.h\"\n \n+#ifndef CONST_FIXED_P\n+#define CONST_FIXED_P(X) (CONST_FIXED == GET_CODE (X))\n+#endif\n+\n /* Maximal allowed offset for an address in the LD command */\n #define MAX_LD_OFFSET(MODE) (64 - (signed)GET_MODE_SIZE (MODE))\n \n@@ -264,6 +268,23 @@ avr_popcount_each_byte (rtx xval, int n_bytes, int pop_mask)\n   return true;\n }\n \n+\n+/* Access some RTX as INT_MODE.  If X is a CONST_FIXED we can get\n+   the bit representation of X by \"casting\" it to CONST_INT.  */\n+\n+rtx\n+avr_to_int_mode (rtx x)\n+{\n+  enum machine_mode mode = GET_MODE (x);\n+\n+  return VOIDmode == mode\n+    ? x\n+    : simplify_gen_subreg (int_mode_for_mode (mode), x, mode, 0);\n+}\n+\n+\n+/* Implement `TARGET_OPTION_OVERRIDE'.  */\n+\n static void\n avr_option_override (void)\n {\n@@ -389,9 +410,14 @@ avr_regno_reg_class (int r)\n }\n \n \n+/* Implement `TARGET_SCALAR_MODE_SUPPORTED_P'.  */\n+\n static bool\n avr_scalar_mode_supported_p (enum machine_mode mode)\n {\n+  if (ALL_FIXED_POINT_MODE_P (mode))\n+    return true;\n+\n   if (PSImode == mode)\n     return true;\n \n@@ -715,6 +741,58 @@ avr_initial_elimination_offset (int from, int to)\n     }\n }\n \n+\n+/* Helper for the function below.  */\n+\n+static void\n+avr_adjust_type_node (tree *node, enum machine_mode mode, int sat_p)\n+{\n+  *node = make_node (FIXED_POINT_TYPE);\n+  TYPE_SATURATING (*node) = sat_p;\n+  TYPE_UNSIGNED (*node) = UNSIGNED_FIXED_POINT_MODE_P (mode);\n+  TYPE_IBIT (*node) = GET_MODE_IBIT (mode);\n+  TYPE_FBIT (*node) = GET_MODE_FBIT (mode);\n+  TYPE_PRECISION (*node) = GET_MODE_BITSIZE (mode);\n+  TYPE_ALIGN (*node) = 8;\n+  SET_TYPE_MODE (*node, mode);\n+\n+  layout_type (*node);\n+}\n+\n+\n+/* Implement `TARGET_BUILD_BUILTIN_VA_LIST'.  */\n+\n+static tree\n+avr_build_builtin_va_list (void)\n+{\n+  /* avr-modes.def adjusts [U]TA to be 64-bit modes with 48 fractional bits.\n+     This is more appropriate for the 8-bit machine AVR than 128-bit modes.\n+     The ADJUST_IBIT/FBIT are handled in toplev:init_adjust_machine_modes()\n+     which is auto-generated by genmodes, but the compiler assigns [U]DAmode\n+     to the long long accum modes instead of the desired [U]TAmode.\n+\n+     Fix this now, right after node setup in tree.c:build_common_tree_nodes().\n+     This must run before c-cppbuiltin.c:builtin_define_fixed_point_constants()\n+     which built-in defines macros like __ULLACCUM_FBIT__ that are used by\n+     libgcc to detect IBIT and FBIT.  */\n+\n+  avr_adjust_type_node (&ta_type_node, TAmode, 0);\n+  avr_adjust_type_node (&uta_type_node, UTAmode, 0);\n+  avr_adjust_type_node (&sat_ta_type_node, TAmode, 1);\n+  avr_adjust_type_node (&sat_uta_type_node, UTAmode, 1);\n+\n+  unsigned_long_long_accum_type_node = uta_type_node;\n+  long_long_accum_type_node = ta_type_node;\n+  sat_unsigned_long_long_accum_type_node = sat_uta_type_node;\n+  sat_long_long_accum_type_node = sat_ta_type_node;\n+\n+  /* Dispatch to the default handler.  */\n+  \n+  return std_build_builtin_va_list ();\n+}\n+\n+\n+/* Implement `TARGET_BUILTIN_SETJMP_FRAME_VALUE'.  */\n /* Actual start of frame is virtual_stack_vars_rtx this is offset from \n    frame pointer by +STARTING_FRAME_OFFSET.\n    Using saved frame = virtual_stack_vars_rtx - STARTING_FRAME_OFFSET\n@@ -723,10 +801,13 @@ avr_initial_elimination_offset (int from, int to)\n static rtx\n avr_builtin_setjmp_frame_value (void)\n {\n-  return gen_rtx_MINUS (Pmode, virtual_stack_vars_rtx, \n-                        gen_int_mode (STARTING_FRAME_OFFSET, Pmode));\n+  rtx xval = gen_reg_rtx (Pmode);\n+  emit_insn (gen_subhi3 (xval, virtual_stack_vars_rtx,\n+                         gen_int_mode (STARTING_FRAME_OFFSET, Pmode)));\n+  return xval;\n }\n \n+\n /* Return contents of MEM at frame pointer + stack size + 1 (+2 if 3 byte PC).\n    This is return address of function.  */\n rtx \n@@ -1580,7 +1661,7 @@ avr_legitimate_address_p (enum machine_mode mode, rtx x, bool strict)\n                                   MEM, strict);\n \n       if (strict\n-          && DImode == mode\n+          && GET_MODE_SIZE (mode) > 4\n           && REG_X == REGNO (x))\n         {\n           ok = false;\n@@ -2081,6 +2162,14 @@ avr_print_operand (FILE *file, rtx x, int code)\n       /* Use normal symbol for direct address no linker trampoline needed */\n       output_addr_const (file, x);\n     }\n+  else if (GET_CODE (x) == CONST_FIXED)\n+    {\n+      HOST_WIDE_INT ival = INTVAL (avr_to_int_mode (x));\n+      if (code != 0)\n+        output_operand_lossage (\"Unsupported code '%c'for fixed-point:\",\n+                                code);\n+      fprintf (file, HOST_WIDE_INT_PRINT_DEC, ival);\n+    }\n   else if (GET_CODE (x) == CONST_DOUBLE)\n     {\n       long val;\n@@ -2116,6 +2205,7 @@ notice_update_cc (rtx body ATTRIBUTE_UNUSED, rtx insn)\n \n     case CC_OUT_PLUS:\n     case CC_OUT_PLUS_NOCLOBBER:\n+    case CC_MINUS:\n     case CC_LDI:\n       {\n         rtx *op = recog_data.operand;\n@@ -2139,6 +2229,11 @@ notice_update_cc (rtx body ATTRIBUTE_UNUSED, rtx insn)\n             cc = (enum attr_cc) icc;\n             break;\n \n+          case CC_MINUS:\n+            avr_out_minus (op, &len_dummy, &icc);\n+            cc = (enum attr_cc) icc;\n+            break;\n+\n           case CC_LDI:\n \n             cc = (op[1] == CONST0_RTX (GET_MODE (op[0]))\n@@ -2779,9 +2874,11 @@ output_movqi (rtx insn, rtx operands[], int *real_l)\n   if (real_l)\n     *real_l = 1;\n   \n-  if (register_operand (dest, QImode))\n+  gcc_assert (1 == GET_MODE_SIZE (GET_MODE (dest)));\n+\n+  if (REG_P (dest))\n     {\n-      if (register_operand (src, QImode)) /* mov r,r */\n+      if (REG_P (src)) /* mov r,r */\n \t{\n \t  if (test_hard_reg_class (STACK_REG, dest))\n \t    return \"out %0,%1\";\n@@ -2803,7 +2900,7 @@ output_movqi (rtx insn, rtx operands[], int *real_l)\n       rtx xop[2];\n \n       xop[0] = dest;\n-      xop[1] = src == const0_rtx ? zero_reg_rtx : src;\n+      xop[1] = src == CONST0_RTX (GET_MODE (dest)) ? zero_reg_rtx : src;\n \n       return out_movqi_mr_r (insn, xop, real_l);\n     }\n@@ -2825,6 +2922,8 @@ output_movhi (rtx insn, rtx xop[], int *plen)\n       return avr_out_lpm (insn, xop, plen);\n     }\n \n+  gcc_assert (2 == GET_MODE_SIZE (GET_MODE (dest)));\n+\n   if (REG_P (dest))\n     {\n       if (REG_P (src)) /* mov r,r */\n@@ -2843,7 +2942,6 @@ output_movhi (rtx insn, rtx xop[], int *plen)\n               return TARGET_NO_INTERRUPTS\n                 ? avr_asm_len (\"out __SP_H__,%B1\" CR_TAB\n                                \"out __SP_L__,%A1\", xop, plen, -2)\n-\n                 : avr_asm_len (\"in __tmp_reg__,__SREG__\"  CR_TAB\n                                \"cli\"                      CR_TAB\n                                \"out __SP_H__,%B1\"         CR_TAB\n@@ -2880,7 +2978,7 @@ output_movhi (rtx insn, rtx xop[], int *plen)\n       rtx xop[2];\n \n       xop[0] = dest;\n-      xop[1] = src == const0_rtx ? zero_reg_rtx : src;\n+      xop[1] = src == CONST0_RTX (GET_MODE (dest)) ? zero_reg_rtx : src;\n \n       return out_movhi_mr_r (insn, xop, plen);\n     }\n@@ -3403,9 +3501,10 @@ output_movsisf (rtx insn, rtx operands[], int *l)\n   if (!l)\n     l = &dummy;\n   \n-  if (register_operand (dest, VOIDmode))\n+  gcc_assert (4 == GET_MODE_SIZE (GET_MODE (dest)));\n+  if (REG_P (dest))\n     {\n-      if (register_operand (src, VOIDmode)) /* mov r,r */\n+      if (REG_P (src)) /* mov r,r */\n \t{\n \t  if (true_regnum (dest) > true_regnum (src))\n \t    {\n@@ -3440,10 +3539,10 @@ output_movsisf (rtx insn, rtx operands[], int *l)\n \t{\n           return output_reload_insisf (operands, NULL_RTX, real_l);\n         }\n-      else if (GET_CODE (src) == MEM)\n+      else if (MEM_P (src))\n \treturn out_movsi_r_mr (insn, operands, real_l); /* mov r,m */\n     }\n-  else if (GET_CODE (dest) == MEM)\n+  else if (MEM_P (dest))\n     {\n       const char *templ;\n \n@@ -4126,14 +4225,25 @@ avr_out_compare (rtx insn, rtx *xop, int *plen)\n   rtx xval = xop[1];\n   \n   /* MODE of the comparison.  */\n-  enum machine_mode mode = GET_MODE (xreg);\n+  enum machine_mode mode;\n \n   /* Number of bytes to operate on.  */\n-  int i, n_bytes = GET_MODE_SIZE (mode);\n+  int i, n_bytes = GET_MODE_SIZE (GET_MODE (xreg));\n \n   /* Value (0..0xff) held in clobber register xop[2] or -1 if unknown.  */\n   int clobber_val = -1;\n \n+  /* Map fixed mode operands to integer operands with the same binary\n+     representation.  They are easier to handle in the remainder.  */\n+\n+  if (CONST_FIXED == GET_CODE (xval))\n+    {\n+      xreg = avr_to_int_mode (xop[0]);\n+      xval = avr_to_int_mode (xop[1]);\n+    }\n+  \n+  mode = GET_MODE (xreg);\n+\n   gcc_assert (REG_P (xreg));\n   gcc_assert ((CONST_INT_P (xval) && n_bytes <= 4)\n               || (const_double_operand (xval, VOIDmode) && n_bytes == 8));\n@@ -4143,7 +4253,7 @@ avr_out_compare (rtx insn, rtx *xop, int *plen)\n \n   /* Comparisons == +/-1 and != +/-1 can be done similar to camparing\n      against 0 by ORing the bytes.  This is one instruction shorter.\n-     Notice that DImode comparisons are always against reg:DI 18\n+     Notice that 64-bit comparisons are always against reg:ALL8 18 (ACC_A)\n      and therefore don't use this.  */\n \n   if (!test_hard_reg_class (LD_REGS, xreg)\n@@ -5884,6 +5994,9 @@ avr_out_plus_1 (rtx *xop, int *plen, enum rtx_code code, int *pcc)\n   /* MODE of the operation.  */\n   enum machine_mode mode = GET_MODE (xop[0]);\n \n+  /* INT_MODE of the same size.  */\n+  enum machine_mode imode = int_mode_for_mode (mode);\n+\n   /* Number of bytes to operate on.  */\n   int i, n_bytes = GET_MODE_SIZE (mode);\n \n@@ -5908,8 +6021,11 @@ avr_out_plus_1 (rtx *xop, int *plen, enum rtx_code code, int *pcc)\n   \n   *pcc = (MINUS == code) ? CC_SET_CZN : CC_CLOBBER;\n \n+  if (CONST_FIXED_P (xval))\n+    xval = avr_to_int_mode (xval);\n+\n   if (MINUS == code)\n-    xval = simplify_unary_operation (NEG, mode, xval, mode);\n+    xval = simplify_unary_operation (NEG, imode, xval, imode);\n \n   op[2] = xop[3];\n \n@@ -5920,7 +6036,7 @@ avr_out_plus_1 (rtx *xop, int *plen, enum rtx_code code, int *pcc)\n     {\n       /* We operate byte-wise on the destination.  */\n       rtx reg8 = simplify_gen_subreg (QImode, xop[0], mode, i);\n-      rtx xval8 = simplify_gen_subreg (QImode, xval, mode, i);\n+      rtx xval8 = simplify_gen_subreg (QImode, xval, imode, i);\n \n       /* 8-bit value to operate with this byte. */\n       unsigned int val8 = UINTVAL (xval8) & GET_MODE_MASK (QImode);\n@@ -5941,7 +6057,7 @@ avr_out_plus_1 (rtx *xop, int *plen, enum rtx_code code, int *pcc)\n           && i + 2 <= n_bytes\n           && test_hard_reg_class (ADDW_REGS, reg8))\n         {\n-          rtx xval16 = simplify_gen_subreg (HImode, xval, mode, i);\n+          rtx xval16 = simplify_gen_subreg (HImode, xval, imode, i);\n           unsigned int val16 = UINTVAL (xval16) & GET_MODE_MASK (HImode);\n \n           /* Registers R24, X, Y, Z can use ADIW/SBIW with constants < 64\n@@ -6085,6 +6201,41 @@ avr_out_plus_noclobber (rtx *xop, int *plen, int *pcc)\n }\n \n \n+/* Output subtraction of register XOP[0] and compile time constant XOP[2]:\n+\n+      XOP[0] = XOP[0] - XOP[2]\n+\n+   This is basically the same as `avr_out_plus' except that we subtract.\n+   It's needed because (minus x const) is not mapped to (plus x -const)\n+   for the fixed point modes.  */\n+\n+const char*\n+avr_out_minus (rtx *xop, int *plen, int *pcc)\n+{\n+  rtx op[4];\n+\n+  if (pcc)\n+    *pcc = (int) CC_SET_CZN;\n+\n+  if (REG_P (xop[2]))\n+    return avr_asm_len (\"sub %A0,%A2\" CR_TAB\n+                        \"sbc %B0,%B2\", xop, plen, -2);\n+\n+  if (!CONST_INT_P (xop[2])\n+      && !CONST_FIXED_P (xop[2]))\n+    return avr_asm_len (\"subi %A0,lo8(%2)\" CR_TAB\n+                        \"sbci %B0,hi8(%2)\", xop, plen, -2);\n+  \n+  op[0] = avr_to_int_mode (xop[0]);\n+  op[1] = avr_to_int_mode (xop[1]);\n+  op[2] = gen_int_mode (-INTVAL (avr_to_int_mode (xop[2])),\n+                        GET_MODE (op[0]));\n+  op[3] = xop[3];\n+\n+  return avr_out_plus (op, plen, pcc);\n+}\n+\n+\n /* Prepare operands of adddi3_const_insn to be used with avr_out_plus_1.  */\n \n const char*\n@@ -6103,6 +6254,19 @@ avr_out_plus64 (rtx addend, int *plen)\n   return \"\";\n }\n \n+\n+/* Prepare operands of subdi3_const_insn to be used with avr_out_plus64.  */\n+\n+const char*\n+avr_out_minus64 (rtx subtrahend, int *plen)\n+{\n+  rtx xneg = avr_to_int_mode (subtrahend);\n+  xneg = simplify_unary_operation (NEG, DImode, xneg, DImode);\n+\n+  return avr_out_plus64 (xneg, plen);\n+}\n+\n+\n /* Output bit operation (IOR, AND, XOR) with register XOP[0] and compile\n    time constant XOP[2]:\n \n@@ -6442,6 +6606,349 @@ avr_rotate_bytes (rtx operands[])\n     return true;\n }\n \n+\n+/* Outputs instructions needed for fixed point type conversion.\n+   This includes converting between any fixed point type, as well\n+   as converting to any integer type.  Conversion between integer\n+   types is not supported.\n+\n+   The number of instructions generated depends on the types\n+   being converted and the registers assigned to them.\n+\n+   The number of instructions required to complete the conversion\n+   is least if the registers for source and destination are overlapping\n+   and are aligned at the decimal place as actual movement of data is\n+   completely avoided.  In some cases, the conversion may already be\n+   complete without any instructions needed.\n+\n+   When converting to signed types from signed types, sign extension\n+   is implemented.\n+\n+   Converting signed fractional types requires a bit shift if converting\n+   to or from any unsigned fractional type because the decimal place is\n+   shifted by 1 bit.  When the destination is a signed fractional, the sign\n+   is stored in either the carry or T bit.  */\n+\n+const char*\n+avr_out_fract (rtx insn, rtx operands[], bool intsigned, int *plen)\n+{\n+  int i;\n+  bool sbit[2];\n+  /* ilen: Length of integral part (in bytes)\n+     flen: Length of fractional part (in bytes)\n+     tlen: Length of operand (in bytes)\n+     blen: Length of operand (in bits) */\n+  int ilen[2], flen[2], tlen[2], blen[2];\n+  int rdest, rsource, offset;\n+  int start, end, dir;\n+  bool sign_in_T = false, sign_in_Carry = false, sign_done = false;\n+  bool widening_sign_extend = false;\n+  int clrword = -1, lastclr = 0, clr = 0;\n+  rtx xop[6];\n+\n+  const int dest = 0;\n+  const int src = 1;\n+\n+  xop[dest] = operands[dest];\n+  xop[src] = operands[src];\n+\n+  if (plen)\n+    *plen = 0;\n+\n+  /* Determine format (integer and fractional parts)\n+     of types needing conversion.  */\n+\n+  for (i = 0; i < 2; i++)\n+    {\n+      enum machine_mode mode = GET_MODE (xop[i]);\n+\n+      tlen[i] = GET_MODE_SIZE (mode);\n+      blen[i] = GET_MODE_BITSIZE (mode);\n+\n+      if (SCALAR_INT_MODE_P (mode))\n+        {\n+          sbit[i] = intsigned;\n+          ilen[i] = GET_MODE_SIZE (mode);\n+          flen[i] = 0;\n+        }\n+      else if (ALL_SCALAR_FIXED_POINT_MODE_P (mode))\n+        {\n+          sbit[i] = SIGNED_SCALAR_FIXED_POINT_MODE_P (mode);\n+          ilen[i] = (GET_MODE_IBIT (mode) + 1) / 8;\n+          flen[i] = (GET_MODE_FBIT (mode) + 1) / 8;\n+        }\n+      else\n+        fatal_insn (\"unsupported fixed-point conversion\", insn);\n+    }\n+\n+  /* Perform sign extension if source and dest are both signed,\n+     and there are more integer parts in dest than in source.  */\n+\n+  widening_sign_extend = sbit[dest] && sbit[src] && ilen[dest] > ilen[src];\n+\n+  rdest = REGNO (xop[dest]);\n+  rsource = REGNO (xop[src]);\n+  offset = flen[src] - flen[dest];\n+\n+  /* Position of MSB resp. sign bit.  */\n+\n+  xop[2] = GEN_INT (blen[dest] - 1);\n+  xop[3] = GEN_INT (blen[src] - 1);\n+\n+  /* Store the sign bit if the destination is a signed fract and the source\n+     has a sign in the integer part.  */\n+\n+  if (sbit[dest] && ilen[dest] == 0 && sbit[src] && ilen[src] > 0)\n+    {\n+      /* To avoid using BST and BLD if the source and destination registers\n+         overlap or the source is unused after, we can use LSL to store the\n+         sign bit in carry since we don't need the integral part of the source.\n+         Restoring the sign from carry saves one BLD instruction below.  */\n+\n+      if (reg_unused_after (insn, xop[src])\n+          || (rdest < rsource + tlen[src]\n+              && rdest + tlen[dest] > rsource))\n+        {\n+          avr_asm_len (\"lsl %T1%t3\", xop, plen, 1);\n+          sign_in_Carry = true;\n+        }\n+      else\n+        {\n+          avr_asm_len (\"bst %T1%T3\", xop, plen, 1);\n+          sign_in_T = true;\n+        }\n+    }\n+\n+  /* Pick the correct direction to shift bytes.  */\n+\n+  if (rdest < rsource + offset)\n+    {\n+      dir = 1;\n+      start = 0;\n+      end = tlen[dest];\n+    }\n+  else\n+    {\n+      dir = -1;\n+      start = tlen[dest] - 1;\n+      end = -1;\n+    }\n+\n+  /* Perform conversion by moving registers into place, clearing\n+     destination registers that do not overlap with any source.  */\n+\n+  for (i = start; i != end; i += dir)\n+    {\n+      int destloc = rdest + i;\n+      int sourceloc = rsource + i + offset;\n+\n+      /* Source register location is outside range of source register,\n+         so clear this byte in the dest.  */\n+\n+      if (sourceloc < rsource\n+          || sourceloc >= rsource + tlen[src])\n+        {\n+          if (AVR_HAVE_MOVW\n+              && i + dir != end\n+              && (sourceloc + dir < rsource\n+                  || sourceloc + dir >= rsource + tlen[src])\n+              && ((dir == 1 && !(destloc % 2) && !(sourceloc % 2))\n+                  || (dir == -1 && (destloc % 2) && (sourceloc % 2)))\n+              && clrword != -1)\n+            {\n+              /* Use already cleared word to clear two bytes at a time.  */\n+\n+              int even_i = i & ~1;\n+              int even_clrword = clrword & ~1;\n+\n+              xop[4] = GEN_INT (8 * even_i);\n+              xop[5] = GEN_INT (8 * even_clrword);\n+              avr_asm_len (\"movw %T0%t4,%T0%t5\", xop, plen, 1);\n+              i += dir;\n+            }\n+          else\n+            {\n+              if (i == tlen[dest] - 1\n+                  && widening_sign_extend\n+                  && blen[src] - 1 - 8 * offset < 0)\n+                {\n+                  /* The SBRC below that sign-extends would come\n+                     up with a negative bit number because the sign\n+                     bit is out of reach.  ALso avoid some early-clobber\n+                     situations because of premature CLR.  */\n+\n+                  if (reg_unused_after (insn, xop[src]))\n+                    avr_asm_len (\"lsl %T1%t3\" CR_TAB\n+                                 \"sbc %T0%t2,%T0%t2\", xop, plen, 2);\n+                  else\n+                    avr_asm_len (\"mov __tmp_reg__,%T1%t3\"  CR_TAB\n+                                 \"lsl __tmp_reg__\"         CR_TAB\n+                                 \"sbc %T0%t2,%T0%t2\", xop, plen, 3);\n+                  sign_done = true;\n+\n+                  continue;\n+                }\n+              \n+              /* Do not clear the register if it is going to get\n+                 sign extended with a MOV later.  */\n+\n+              if (sbit[dest] && sbit[src]\n+                  && i != tlen[dest] - 1\n+                  && i >= flen[dest])\n+                {\n+                  continue;\n+                }\n+\n+              xop[4] = GEN_INT (8 * i);\n+              avr_asm_len (\"clr %T0%t4\", xop, plen, 1);\n+\n+              /* If the last byte was cleared too, we have a cleared\n+                 word we can MOVW to clear two bytes at a time.  */\n+\n+              if (lastclr) \n+                clrword = i;\n+\n+              clr = 1;\n+            }\n+        }\n+      else if (destloc == sourceloc)\n+        {\n+          /* Source byte is already in destination:  Nothing needed.  */\n+\n+          continue;\n+        }\n+      else\n+        {\n+          /* Registers do not line up and source register location\n+             is within range:  Perform move, shifting with MOV or MOVW.  */\n+\n+          if (AVR_HAVE_MOVW\n+              && i + dir != end\n+              && sourceloc + dir >= rsource\n+              && sourceloc + dir < rsource + tlen[src]\n+              && ((dir == 1 && !(destloc % 2) && !(sourceloc % 2))\n+                  || (dir == -1 && (destloc % 2) && (sourceloc % 2))))\n+            {\n+              int even_i = i & ~1;\n+              int even_i_plus_offset = (i + offset) & ~1;\n+\n+              xop[4] = GEN_INT (8 * even_i);\n+              xop[5] = GEN_INT (8 * even_i_plus_offset);\n+              avr_asm_len (\"movw %T0%t4,%T1%t5\", xop, plen, 1);\n+              i += dir;\n+            }\n+          else\n+            {\n+              xop[4] = GEN_INT (8 * i);\n+              xop[5] = GEN_INT (8 * (i + offset));\n+              avr_asm_len (\"mov %T0%t4,%T1%t5\", xop, plen, 1);\n+            }\n+        }\n+\n+      lastclr = clr;\n+      clr = 0;\n+    }\n+      \n+  /* Perform sign extension if source and dest are both signed,\n+     and there are more integer parts in dest than in source.  */\n+\n+  if (widening_sign_extend)\n+    {\n+      if (!sign_done)\n+        {\n+          xop[4] = GEN_INT (blen[src] - 1 - 8 * offset);\n+\n+          /* Register was cleared above, so can become 0xff and extended.\n+             Note:  Instead of the CLR/SBRC/COM the sign extension could\n+             be performed after the LSL below by means of a SBC if only\n+             one byte has to be shifted left.  */\n+\n+          avr_asm_len (\"sbrc %T0%T4\" CR_TAB\n+                       \"com %T0%t2\", xop, plen, 2);\n+        }\n+\n+      /* Sign extend additional bytes by MOV and MOVW.  */\n+\n+      start = tlen[dest] - 2;\n+      end = flen[dest] + ilen[src] - 1;\n+\n+      for (i = start; i != end; i--)\n+        {\n+          if (AVR_HAVE_MOVW && i != start && i-1 != end)\n+            {\n+              i--;\n+              xop[4] = GEN_INT (8 * i);\n+              xop[5] = GEN_INT (8 * (tlen[dest] - 2));\n+              avr_asm_len (\"movw %T0%t4,%T0%t5\", xop, plen, 1);\n+            }\n+          else\n+            {\n+              xop[4] = GEN_INT (8 * i);\n+              xop[5] = GEN_INT (8 * (tlen[dest] - 1));\n+              avr_asm_len (\"mov %T0%t4,%T0%t5\", xop, plen, 1);\n+            }\n+        }\n+    }\n+\n+  /* If destination is a signed fract, and the source was not, a shift\n+     by 1 bit is needed.  Also restore sign from carry or T.  */\n+\n+  if (sbit[dest] && !ilen[dest] && (!sbit[src] || ilen[src]))\n+    {\n+      /* We have flen[src] non-zero fractional bytes to shift.\n+         Because of the right shift, handle one byte more so that the\n+         LSB won't be lost.  */\n+\n+      int nonzero = flen[src] + 1;\n+\n+      /* If the LSB is in the T flag and there are no fractional\n+         bits, the high byte is zero and no shift needed.  */\n+      \n+      if (flen[src] == 0 && sign_in_T)\n+        nonzero = 0;\n+\n+      start = flen[dest] - 1;\n+      end = start - nonzero;\n+\n+      for (i = start; i > end && i >= 0; i--)\n+        {\n+          xop[4] = GEN_INT (8 * i);\n+          if (i == start && !sign_in_Carry)\n+            avr_asm_len (\"lsr %T0%t4\", xop, plen, 1);\n+          else\n+            avr_asm_len (\"ror %T0%t4\", xop, plen, 1);\n+        }\n+\n+      if (sign_in_T)\n+        {\n+          avr_asm_len (\"bld %T0%T2\", xop, plen, 1);\n+        }\n+    }\n+  else if (sbit[src] && !ilen[src] && (!sbit[dest] || ilen[dest]))\n+    {\n+      /* If source was a signed fract and dest was not, shift 1 bit\n+         other way.  */\n+\n+      start = flen[dest] - flen[src];\n+\n+      if (start < 0)\n+        start = 0;\n+\n+      for (i = start; i < flen[dest]; i++)\n+        {\n+          xop[4] = GEN_INT (8 * i);\n+\n+          if (i == start)\n+            avr_asm_len (\"lsl %T0%t4\", xop, plen, 1);\n+          else\n+            avr_asm_len (\"rol %T0%t4\", xop, plen, 1);\n+        }\n+    }\n+\n+  return \"\";\n+}\n+\n+\n /* Modifies the length assigned to instruction INSN\n    LEN is the initially computed length of the insn.  */\n \n@@ -6489,6 +6996,8 @@ adjust_insn_length (rtx insn, int len)\n       \n     case ADJUST_LEN_OUT_PLUS: avr_out_plus (op, &len, NULL); break;\n     case ADJUST_LEN_PLUS64: avr_out_plus64 (op[0], &len); break;\n+    case ADJUST_LEN_MINUS: avr_out_minus (op, &len, NULL); break;\n+    case ADJUST_LEN_MINUS64: avr_out_minus64 (op[0], &len); break;\n     case ADJUST_LEN_OUT_PLUS_NOCLOBBER:\n       avr_out_plus_noclobber (op, &len, NULL); break;\n \n@@ -6502,6 +7011,9 @@ adjust_insn_length (rtx insn, int len)\n     case ADJUST_LEN_XLOAD: avr_out_xload (insn, op, &len); break;\n     case ADJUST_LEN_LOAD_LPM: avr_load_lpm (insn, op, &len); break;\n \n+    case ADJUST_LEN_SFRACT: avr_out_fract (insn, op, true, &len); break;\n+    case ADJUST_LEN_UFRACT: avr_out_fract (insn, op, false, &len); break;\n+\n     case ADJUST_LEN_TSTHI: avr_out_tsthi (insn, op, &len); break;\n     case ADJUST_LEN_TSTPSI: avr_out_tstpsi (insn, op, &len); break;\n     case ADJUST_LEN_TSTSI: avr_out_tstsi (insn, op, &len); break;\n@@ -6683,6 +7195,20 @@ avr_assemble_integer (rtx x, unsigned int size, int aligned_p)\n       \n       return true;\n     }\n+  else if (CONST_FIXED_P (x))\n+    {\n+      unsigned n;\n+\n+      /* varasm fails to handle big fixed modes that don't fit in hwi.  */\n+\n+      for (n = 0; n < size; n++)\n+        {\n+          rtx xn = simplify_gen_subreg (QImode, x, GET_MODE (x), n);\n+          default_assemble_integer (xn, 1, aligned_p);\n+        }\n+\n+      return true;\n+    }\n   \n   return default_assemble_integer (x, size, aligned_p);\n }\n@@ -7489,6 +8015,7 @@ avr_operand_rtx_cost (rtx x, enum machine_mode mode, enum rtx_code outer,\n       return 0;\n \n     case CONST_INT:\n+    case CONST_FIXED:\n     case CONST_DOUBLE:\n       return COSTS_N_INSNS (GET_MODE_SIZE (mode));\n \n@@ -7518,6 +8045,7 @@ avr_rtx_costs_1 (rtx x, int codearg, int outer_code ATTRIBUTE_UNUSED,\n   switch (code)\n     {\n     case CONST_INT:\n+    case CONST_FIXED:\n     case CONST_DOUBLE:\n     case SYMBOL_REF:\n     case CONST:\n@@ -8446,11 +8974,17 @@ avr_compare_pattern (rtx insn)\n   if (pattern\n       && NONJUMP_INSN_P (insn)\n       && SET_DEST (pattern) == cc0_rtx\n-      && GET_CODE (SET_SRC (pattern)) == COMPARE\n-      && DImode != GET_MODE (XEXP (SET_SRC (pattern), 0))\n-      && DImode != GET_MODE (XEXP (SET_SRC (pattern), 1)))\n+      && GET_CODE (SET_SRC (pattern)) == COMPARE)\n     {\n-      return pattern;\n+      enum machine_mode mode0 = GET_MODE (XEXP (SET_SRC (pattern), 0));\n+      enum machine_mode mode1 = GET_MODE (XEXP (SET_SRC (pattern), 1));\n+\n+      /* The 64-bit comparisons have fixed operands ACC_A and ACC_B.\n+         They must not be swapped, thus skip them.  */\n+\n+      if ((mode0 == VOIDmode || GET_MODE_SIZE (mode0) <= 4)\n+          && (mode1 == VOIDmode || GET_MODE_SIZE (mode1) <= 4))\n+        return pattern;\n     }\n \n   return NULL_RTX;\n@@ -8788,6 +9322,8 @@ avr_2word_insn_p (rtx insn)\n       return false;\n       \n     case CODE_FOR_movqi_insn:\n+    case CODE_FOR_movuqq_insn:\n+    case CODE_FOR_movqq_insn:\n       {\n         rtx set  = single_set (insn);\n         rtx src  = SET_SRC (set);\n@@ -8796,7 +9332,7 @@ avr_2word_insn_p (rtx insn)\n         /* Factor out LDS and STS from movqi_insn.  */\n         \n         if (MEM_P (dest)\n-            && (REG_P (src) || src == const0_rtx))\n+            && (REG_P (src) || src == CONST0_RTX (GET_MODE (dest))))\n           {\n             return CONSTANT_ADDRESS_P (XEXP (dest, 0));\n           }\n@@ -9021,7 +9557,7 @@ output_reload_in_const (rtx *op, rtx clobber_reg, int *len, bool clear_p)\n   \n   if (NULL_RTX == clobber_reg\n       && !test_hard_reg_class (LD_REGS, dest)\n-      && (! (CONST_INT_P (src) || CONST_DOUBLE_P (src))\n+      && (! (CONST_INT_P (src) || CONST_FIXED_P (src) || CONST_DOUBLE_P (src))\n           || !avr_popcount_each_byte (src, n_bytes,\n                                       (1 << 0) | (1 << 1) | (1 << 8))))\n     {\n@@ -9048,6 +9584,7 @@ output_reload_in_const (rtx *op, rtx clobber_reg, int *len, bool clear_p)\n       ldreg_p = test_hard_reg_class (LD_REGS, xdest[n]);\n \n       if (!CONST_INT_P (src)\n+          && !CONST_FIXED_P (src)\n           && !CONST_DOUBLE_P (src))\n         {\n           static const char* const asm_code[][2] =\n@@ -9239,6 +9776,7 @@ output_reload_insisf (rtx *op, rtx clobber_reg, int *len)\n   if (AVR_HAVE_MOVW\n       && !test_hard_reg_class (LD_REGS, op[0])\n       && (CONST_INT_P (op[1])\n+          || CONST_FIXED_P (op[1])\n           || CONST_DOUBLE_P (op[1])))\n     {\n       int len_clr, len_noclr;\n@@ -10834,6 +11372,12 @@ avr_fold_builtin (tree fndecl, int n_args ATTRIBUTE_UNUSED, tree *arg,\n #undef  TARGET_SCALAR_MODE_SUPPORTED_P\n #define TARGET_SCALAR_MODE_SUPPORTED_P avr_scalar_mode_supported_p\n \n+#undef  TARGET_BUILD_BUILTIN_VA_LIST\n+#define TARGET_BUILD_BUILTIN_VA_LIST avr_build_builtin_va_list\n+\n+#undef  TARGET_FIXED_POINT_SUPPORTED_P\n+#define TARGET_FIXED_POINT_SUPPORTED_P hook_bool_void_true\n+\n #undef  TARGET_ADDR_SPACE_SUBSET_P\n #define TARGET_ADDR_SPACE_SUBSET_P avr_addr_space_subset_p\n "}, {"sha": "f8686685b2f2ad5cc957fd5e1d2f7338a62232da", "filename": "gcc/config/avr/avr.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Favr.h?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -261,6 +261,7 @@ enum\n #define FLOAT_TYPE_SIZE 32\n #define DOUBLE_TYPE_SIZE 32\n #define LONG_DOUBLE_TYPE_SIZE 32\n+#define LONG_LONG_ACCUM_TYPE_SIZE 64\n \n #define DEFAULT_SIGNED_CHAR 1\n "}, {"sha": "6146fe6a013543c4609d5f0b3c2f5606ecd801a7", "filename": "gcc/config/avr/avr.md", "status": "modified", "additions": 601, "deletions": 446, "changes": 1047, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Favr.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Favr.md?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -88,10 +88,10 @@\n \n (include \"predicates.md\")\n (include \"constraints.md\")\n-  \n+\n ;; Condition code settings.\n (define_attr \"cc\" \"none,set_czn,set_zn,set_n,compare,clobber,\n-                   out_plus, out_plus_noclobber,ldi\"\n+                   out_plus, out_plus_noclobber,ldi,minus\"\n   (const_string \"none\"))\n \n (define_attr \"type\" \"branch,branch1,arith,xcall\"\n@@ -139,8 +139,10 @@\n \n (define_attr \"adjust_len\"\n   \"out_bitop, out_plus, out_plus_noclobber, plus64, addto_sp,\n+   minus, minus64,\n    tsthi, tstpsi, tstsi, compare, compare64, call,\n    mov8, mov16, mov24, mov32, reload_in16, reload_in24, reload_in32,\n+   ufract, sfract,\n    xload, movmem, load_lpm,\n    ashlqi, ashrqi, lshrqi,\n    ashlhi, ashrhi, lshrhi,\n@@ -225,8 +227,20 @@\n (define_mode_iterator QIDI [(QI \"\") (HI \"\") (PSI \"\") (SI \"\") (DI \"\")])\n (define_mode_iterator HISI [(HI \"\") (PSI \"\") (SI \"\")])\n \n+(define_mode_iterator ALL1 [(QI \"\") (QQ \"\") (UQQ \"\")])\n+(define_mode_iterator ALL2 [(HI \"\") (HQ \"\") (UHQ \"\") (HA \"\") (UHA \"\")])\n+(define_mode_iterator ALL4 [(SI \"\") (SQ \"\") (USQ \"\") (SA \"\") (USA \"\")])\n+\n ;; All supported move-modes\n-(define_mode_iterator MOVMODE [(QI \"\") (HI \"\") (SI \"\") (SF \"\") (PSI \"\")])\n+(define_mode_iterator MOVMODE [(QI \"\") (HI \"\") (SI \"\") (SF \"\") (PSI \"\")\n+                               (QQ \"\") (UQQ \"\")\n+                               (HQ \"\") (UHQ \"\") (HA \"\") (UHA \"\")\n+                               (SQ \"\") (USQ \"\") (SA \"\") (USA \"\")])\n+\n+;; Supported ordered modes that are 2, 3, 4 bytes wide\n+(define_mode_iterator ORDERED234 [(HI \"\") (SI \"\") (PSI \"\")\n+                                  (HQ \"\") (UHQ \"\") (HA \"\") (UHA \"\")\n+                                  (SQ \"\") (USQ \"\") (SA \"\") (USA \"\")])\n \n ;; Define code iterators\n ;; Define two incarnations so that we can build the cross product.\n@@ -317,9 +331,11 @@\n   DONE;\n })\n \n-(define_insn \"pushqi1\"\n-  [(set (mem:QI (post_dec:HI (reg:HI REG_SP)))\n-        (match_operand:QI 0 \"reg_or_0_operand\" \"r,L\"))]\n+;; \"pushqi1\"\n+;; \"pushqq1\"  \"pushuqq1\"\n+(define_insn \"push<mode>1\"\n+  [(set (mem:ALL1 (post_dec:HI (reg:HI REG_SP)))\n+        (match_operand:ALL1 0 \"reg_or_0_operand\" \"r,Y00\"))]\n   \"\"\n   \"@\n \tpush %0\n@@ -334,7 +350,11 @@\n    (PSI \"\")\n    (SI \"\") (CSI \"\")\n    (DI \"\") (CDI \"\")\n-   (SF \"\") (SC \"\")])\n+   (SF \"\") (SC \"\")\n+   (HA \"\") (UHA \"\") (HQ \"\") (UHQ \"\")\n+   (SA \"\") (USA \"\") (SQ \"\") (USQ \"\")\n+   (DA \"\") (UDA \"\") (DQ \"\") (UDQ \"\")\n+   (TA \"\") (UTA \"\")])\n \n (define_expand \"push<mode>1\"\n   [(match_operand:MPUSH 0 \"\" \"\")]\n@@ -422,12 +442,14 @@\n    (set_attr \"cc\" \"clobber\")])\n \n \n-(define_insn_and_split \"xload8_A\"\n-  [(set (match_operand:QI 0 \"register_operand\" \"=r\")\n-        (match_operand:QI 1 \"memory_operand\"    \"m\"))\n+;; \"xload8qi_A\"\n+;; \"xload8qq_A\" \"xload8uqq_A\"\n+(define_insn_and_split \"xload8<mode>_A\"\n+  [(set (match_operand:ALL1 0 \"register_operand\" \"=r\")\n+        (match_operand:ALL1 1 \"memory_operand\"    \"m\"))\n    (clobber (reg:HI REG_Z))]\n   \"can_create_pseudo_p()\n-   && !avr_xload_libgcc_p (QImode)\n+   && !avr_xload_libgcc_p (<MODE>mode)\n    && avr_mem_memx_p (operands[1])\n    && REG_P (XEXP (operands[1], 0))\"\n   { gcc_unreachable(); }\n@@ -441,16 +463,16 @@\n     emit_move_insn (reg_z, simplify_gen_subreg (HImode, addr, PSImode, 0));\n     emit_move_insn (hi8, simplify_gen_subreg (QImode, addr, PSImode, 2));\n \n-    insn = emit_insn (gen_xload_8 (operands[0], hi8));\n+    insn = emit_insn (gen_xload<mode>_8 (operands[0], hi8));\n     set_mem_addr_space (SET_SRC (single_set (insn)),\n                                  MEM_ADDR_SPACE (operands[1]));\n     DONE;\n   })\n \n-;; \"xloadqi_A\"\n-;; \"xloadhi_A\"\n+;; \"xloadqi_A\" \"xloadqq_A\" \"xloaduqq_A\"\n+;; \"xloadhi_A\" \"xloadhq_A\" \"xloaduhq_A\" \"xloadha_A\" \"xloaduha_A\"\n+;; \"xloadsi_A\" \"xloadsq_A\" \"xloadusq_A\" \"xloadsa_A\" \"xloadusa_A\"\n ;; \"xloadpsi_A\"\n-;; \"xloadsi_A\"\n ;; \"xloadsf_A\"\n (define_insn_and_split \"xload<mode>_A\"\n   [(set (match_operand:MOVMODE 0 \"register_operand\" \"=r\")\n@@ -488,11 +510,13 @@\n ;; Move value from address space memx to a register\n ;; These insns must be prior to respective generic move insn.\n \n-(define_insn \"xload_8\"\n-  [(set (match_operand:QI 0 \"register_operand\"                   \"=&r,r\")\n-        (mem:QI (lo_sum:PSI (match_operand:QI 1 \"register_operand\" \"r,r\")\n-                            (reg:HI REG_Z))))]\n-  \"!avr_xload_libgcc_p (QImode)\"\n+;; \"xloadqi_8\"\n+;; \"xloadqq_8\" \"xloaduqq_8\"\n+(define_insn \"xload<mode>_8\"\n+  [(set (match_operand:ALL1 0 \"register_operand\"                   \"=&r,r\")\n+        (mem:ALL1 (lo_sum:PSI (match_operand:QI 1 \"register_operand\" \"r,r\")\n+                              (reg:HI REG_Z))))]\n+  \"!avr_xload_libgcc_p (<MODE>mode)\"\n   {\n     return avr_out_xload (insn, operands, NULL);\n   }\n@@ -504,11 +528,11 @@\n ;; R21:Z : 24-bit source address\n ;; R22   : 1-4 byte output\n \n-;; \"xload_qi_libgcc\"\n-;; \"xload_hi_libgcc\"\n-;; \"xload_psi_libgcc\"\n-;; \"xload_si_libgcc\"\n+;; \"xload_qi_libgcc\" \"xload_qq_libgcc\" \"xload_uqq_libgcc\"\n+;; \"xload_hi_libgcc\" \"xload_hq_libgcc\" \"xload_uhq_libgcc\" \"xload_ha_libgcc\" \"xload_uha_libgcc\"\n+;; \"xload_si_libgcc\" \"xload_sq_libgcc\" \"xload_usq_libgcc\" \"xload_sa_libgcc\" \"xload_usa_libgcc\"\n ;; \"xload_sf_libgcc\"\n+;; \"xload_psi_libgcc\"\n (define_insn \"xload_<mode>_libgcc\"\n   [(set (reg:MOVMODE 22)\n         (mem:MOVMODE (lo_sum:PSI (reg:QI 21)\n@@ -528,9 +552,9 @@\n \n ;; General move expanders\n \n-;; \"movqi\"\n-;; \"movhi\"\n-;; \"movsi\"\n+;; \"movqi\" \"movqq\" \"movuqq\"\n+;; \"movhi\" \"movhq\" \"movuhq\" \"movha\" \"movuha\"\n+;; \"movsi\" \"movsq\" \"movusq\" \"movsa\" \"movusa\"\n ;; \"movsf\"\n ;; \"movpsi\"\n (define_expand \"mov<mode>\"\n@@ -546,8 +570,7 @@\n   \n     /* One of the operands has to be in a register.  */\n     if (!register_operand (dest, <MODE>mode)\n-        && !(register_operand (src, <MODE>mode)\n-             || src == CONST0_RTX (<MODE>mode)))\n+        && !reg_or_0_operand (src, <MODE>mode))\n       {\n         operands[1] = src = copy_to_mode_reg (<MODE>mode, src);\n       }\n@@ -560,7 +583,9 @@\n         src = replace_equiv_address (src, copy_to_mode_reg (PSImode, addr));\n \n       if (!avr_xload_libgcc_p (<MODE>mode))\n-        emit_insn (gen_xload8_A (dest, src));\n+        /* ; No <mode> here because gen_xload8<mode>_A only iterates over ALL1.\n+           ; insn-emit does not depend on the mode, it' all about operands.  */\n+        emit_insn (gen_xload8qi_A (dest, src));\n       else\n         emit_insn (gen_xload<mode>_A (dest, src));\n \n@@ -627,12 +652,13 @@\n ;; are call-saved registers, and most of LD_REGS are call-used registers,\n ;; so this may still be a win for registers live across function calls.\n \n-(define_insn \"movqi_insn\"\n-  [(set (match_operand:QI 0 \"nonimmediate_operand\" \"=r ,d,Qm,r ,q,r,*r\")\n-        (match_operand:QI 1 \"nox_general_operand\"   \"rL,i,rL,Qm,r,q,i\"))]\n-  \"register_operand (operands[0], QImode)\n-   || register_operand (operands[1], QImode)\n-   || const0_rtx == operands[1]\"\n+;; \"movqi_insn\"\n+;; \"movqq_insn\" \"movuqq_insn\"\n+(define_insn \"mov<mode>_insn\"\n+  [(set (match_operand:ALL1 0 \"nonimmediate_operand\" \"=r    ,d    ,Qm   ,r ,q,r,*r\")\n+        (match_operand:ALL1 1 \"nox_general_operand\"   \"r Y00,n Ynn,r Y00,Qm,r,q,i\"))]\n+  \"register_operand (operands[0], <MODE>mode)\n+   || reg_or_0_operand (operands[1], <MODE>mode)\"\n   {\n     return output_movqi (insn, operands, NULL);\n   }\n@@ -643,9 +669,11 @@\n ;; This is used in peephole2 to optimize loading immediate constants\n ;; if a scratch register from LD_REGS happens to be available.\n \n-(define_insn \"*reload_inqi\"\n-  [(set (match_operand:QI 0 \"register_operand\" \"=l\")\n-        (match_operand:QI 1 \"immediate_operand\" \"i\"))\n+;; \"*reload_inqi\"\n+;; \"*reload_inqq\" \"*reload_inuqq\"\n+(define_insn \"*reload_in<mode>\"\n+  [(set (match_operand:ALL1 0 \"register_operand\"    \"=l\")\n+        (match_operand:ALL1 1 \"const_operand\"        \"i\"))\n    (clobber (match_operand:QI 2 \"register_operand\" \"=&d\"))]\n   \"reload_completed\"\n   \"ldi %2,lo8(%1)\n@@ -655,14 +683,15 @@\n \n (define_peephole2\n   [(match_scratch:QI 2 \"d\")\n-   (set (match_operand:QI 0 \"l_register_operand\" \"\")\n-        (match_operand:QI 1 \"immediate_operand\" \"\"))]\n-  \"(operands[1] != const0_rtx\n-    && operands[1] != const1_rtx\n-    && operands[1] != constm1_rtx)\"\n-  [(parallel [(set (match_dup 0) (match_dup 1))\n-              (clobber (match_dup 2))])]\n-  \"\")\n+   (set (match_operand:ALL1 0 \"l_register_operand\" \"\")\n+        (match_operand:ALL1 1 \"const_operand\" \"\"))]\n+  ; No need for a clobber reg for 0x0, 0x01 or 0xff\n+  \"!satisfies_constraint_Y00 (operands[1])\n+   && !satisfies_constraint_Y01 (operands[1])\n+   && !satisfies_constraint_Ym1 (operands[1])\"\n+  [(parallel [(set (match_dup 0)\n+                   (match_dup 1))\n+              (clobber (match_dup 2))])])\n \n ;;============================================================================\n ;; move word (16 bit)\n@@ -693,33 +722,37 @@\n \n (define_peephole2\n   [(match_scratch:QI 2 \"d\")\n-   (set (match_operand:HI 0 \"l_register_operand\" \"\")\n-        (match_operand:HI 1 \"immediate_operand\" \"\"))]\n-  \"(operands[1] != const0_rtx \n-    && operands[1] != constm1_rtx)\"\n-  [(parallel [(set (match_dup 0) (match_dup 1))\n-              (clobber (match_dup 2))])]\n-  \"\")\n+   (set (match_operand:ALL2 0 \"l_register_operand\" \"\")\n+        (match_operand:ALL2 1 \"const_or_immediate_operand\" \"\"))]\n+  \"operands[1] != CONST0_RTX (<MODE>mode)\"\n+  [(parallel [(set (match_dup 0)\n+                   (match_dup 1))\n+              (clobber (match_dup 2))])])\n \n ;; '*' because it is not used in rtl generation, only in above peephole\n-(define_insn \"*reload_inhi\"\n-  [(set (match_operand:HI 0 \"register_operand\" \"=r\")\n-        (match_operand:HI 1 \"immediate_operand\" \"i\"))\n+;; \"*reload_inhi\"\n+;; \"*reload_inhq\" \"*reload_inuhq\"\n+;; \"*reload_inha\" \"*reload_inuha\"\n+(define_insn \"*reload_in<mode>\"\n+  [(set (match_operand:ALL2 0 \"l_register_operand\"  \"=l\")\n+        (match_operand:ALL2 1 \"immediate_operand\"    \"i\"))\n    (clobber (match_operand:QI 2 \"register_operand\" \"=&d\"))]\n   \"reload_completed\"\n   {\n     return output_reload_inhi (operands, operands[2], NULL);\n   }\n   [(set_attr \"length\" \"4\")\n    (set_attr \"adjust_len\" \"reload_in16\")\n-   (set_attr \"cc\" \"none\")])\n+   (set_attr \"cc\" \"clobber\")])\n \n-(define_insn \"*movhi\"\n-  [(set (match_operand:HI 0 \"nonimmediate_operand\" \"=r,r,r,m ,d,*r,q,r\")\n-        (match_operand:HI 1 \"nox_general_operand\"   \"r,L,m,rL,i,i ,r,q\"))]\n-  \"register_operand (operands[0], HImode)\n-   || register_operand (operands[1], HImode)\n-   || const0_rtx == operands[1]\"\n+;; \"*movhi\"\n+;; \"*movhq\" \"*movuhq\"\n+;; \"*movha\" \"*movuha\"\n+(define_insn \"*mov<mode>\"\n+  [(set (match_operand:ALL2 0 \"nonimmediate_operand\" \"=r,r  ,r,m    ,d,*r,q,r\")\n+        (match_operand:ALL2 1 \"nox_general_operand\"   \"r,Y00,m,r Y00,i,i ,r,q\"))]\n+  \"register_operand (operands[0], <MODE>mode)\n+   || reg_or_0_operand (operands[1], <MODE>mode)\"\n   {\n     return output_movhi (insn, operands, NULL);\n   }\n@@ -728,28 +761,30 @@\n    (set_attr \"cc\" \"none,none,clobber,clobber,none,clobber,none,none\")])\n \n (define_peephole2 ; movw\n-  [(set (match_operand:QI 0 \"even_register_operand\" \"\")\n-        (match_operand:QI 1 \"even_register_operand\" \"\"))\n-   (set (match_operand:QI 2 \"odd_register_operand\" \"\")\n-        (match_operand:QI 3 \"odd_register_operand\" \"\"))]\n+  [(set (match_operand:ALL1 0 \"even_register_operand\" \"\")\n+        (match_operand:ALL1 1 \"even_register_operand\" \"\"))\n+   (set (match_operand:ALL1 2 \"odd_register_operand\" \"\")\n+        (match_operand:ALL1 3 \"odd_register_operand\" \"\"))]\n   \"(AVR_HAVE_MOVW\n     && REGNO (operands[0]) == REGNO (operands[2]) - 1\n     && REGNO (operands[1]) == REGNO (operands[3]) - 1)\"\n-  [(set (match_dup 4) (match_dup 5))]\n+  [(set (match_dup 4)\n+        (match_dup 5))]\n   {\n     operands[4] = gen_rtx_REG (HImode, REGNO (operands[0]));\n     operands[5] = gen_rtx_REG (HImode, REGNO (operands[1]));\n   })\n \n (define_peephole2 ; movw_r\n-  [(set (match_operand:QI 0 \"odd_register_operand\" \"\")\n-        (match_operand:QI 1 \"odd_register_operand\" \"\"))\n-   (set (match_operand:QI 2 \"even_register_operand\" \"\")\n-        (match_operand:QI 3 \"even_register_operand\" \"\"))]\n+  [(set (match_operand:ALL1 0 \"odd_register_operand\" \"\")\n+        (match_operand:ALL1 1 \"odd_register_operand\" \"\"))\n+   (set (match_operand:ALL1 2 \"even_register_operand\" \"\")\n+        (match_operand:ALL1 3 \"even_register_operand\" \"\"))]\n   \"(AVR_HAVE_MOVW\n     && REGNO (operands[2]) == REGNO (operands[0]) - 1\n     && REGNO (operands[3]) == REGNO (operands[1]) - 1)\"\n-  [(set (match_dup 4) (match_dup 5))]\n+  [(set (match_dup 4)\n+        (match_dup 5))]\n   {\n     operands[4] = gen_rtx_REG (HImode, REGNO (operands[2]));\n     operands[5] = gen_rtx_REG (HImode, REGNO (operands[3]));\n@@ -801,19 +836,21 @@\n \n (define_peephole2 ; *reload_insi\n   [(match_scratch:QI 2 \"d\")\n-   (set (match_operand:SI 0 \"l_register_operand\" \"\")\n-        (match_operand:SI 1 \"const_int_operand\" \"\"))\n+   (set (match_operand:ALL4 0 \"l_register_operand\" \"\")\n+        (match_operand:ALL4 1 \"immediate_operand\" \"\"))\n    (match_dup 2)]\n-  \"(operands[1] != const0_rtx\n-    && operands[1] != constm1_rtx)\"\n-  [(parallel [(set (match_dup 0) (match_dup 1))\n-              (clobber (match_dup 2))])]\n-  \"\")\n+  \"operands[1] != CONST0_RTX (<MODE>mode)\"\n+  [(parallel [(set (match_dup 0)\n+                   (match_dup 1))\n+              (clobber (match_dup 2))])])\n \n ;; '*' because it is not used in rtl generation.\n+;; \"*reload_insi\"\n+;; \"*reload_insq\" \"*reload_inusq\"\n+;; \"*reload_insa\" \"*reload_inusa\"\n (define_insn \"*reload_insi\"\n-  [(set (match_operand:SI 0 \"register_operand\" \"=r\")\n-        (match_operand:SI 1 \"const_int_operand\" \"n\"))\n+  [(set (match_operand:ALL4 0 \"register_operand\"   \"=r\")\n+        (match_operand:ALL4 1 \"immediate_operand\"   \"n Ynn\"))\n    (clobber (match_operand:QI 2 \"register_operand\" \"=&d\"))]\n   \"reload_completed\"\n   {\n@@ -824,12 +861,14 @@\n    (set_attr \"cc\" \"clobber\")])\n \n \n-(define_insn \"*movsi\"\n-  [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=r,r,r ,Qm,!d,r\")\n-        (match_operand:SI 1 \"nox_general_operand\"   \"r,L,Qm,rL,i ,i\"))]\n-  \"register_operand (operands[0], SImode)\n-   || register_operand (operands[1], SImode)\n-   || const0_rtx == operands[1]\"\n+;; \"*movsi\"\n+;; \"*movsq\" \"*movusq\"\n+;; \"*movsa\" \"*movusa\"\n+(define_insn \"*mov<mode>\"\n+  [(set (match_operand:ALL4 0 \"nonimmediate_operand\" \"=r,r  ,r ,Qm   ,!d,r\")\n+        (match_operand:ALL4 1 \"nox_general_operand\"   \"r,Y00,Qm,r Y00,i ,i\"))]\n+  \"register_operand (operands[0], <MODE>mode)\n+   || reg_or_0_operand (operands[1], <MODE>mode)\"\n   {\n     return output_movsisf (insn, operands, NULL);\n   }\n@@ -844,8 +883,7 @@\n   [(set (match_operand:SF 0 \"nonimmediate_operand\" \"=r,r,r ,Qm,!d,r\")\n         (match_operand:SF 1 \"nox_general_operand\"   \"r,G,Qm,rG,F ,F\"))]\n   \"register_operand (operands[0], SFmode)\n-   || register_operand (operands[1], SFmode)\n-   || operands[1] == CONST0_RTX (SFmode)\"\n+   || reg_or_0_operand (operands[1], SFmode)\"\n   {\n     return output_movsisf (insn, operands, NULL);\n   }\n@@ -861,8 +899,7 @@\n   \"operands[1] != CONST0_RTX (SFmode)\"\n   [(parallel [(set (match_dup 0) \n                    (match_dup 1))\n-              (clobber (match_dup 2))])]\n-  \"\")\n+              (clobber (match_dup 2))])])\n \n ;; '*' because it is not used in rtl generation.\n (define_insn \"*reload_insf\"\n@@ -1015,9 +1052,10 @@\n    (set (match_dup 4)\n         (plus:HI (match_dup 4)\n                  (const_int -1)))\n-   (set (match_operand:HI 0 \"register_operand\" \"\")\n-        (minus:HI (match_dup 4)\n-                  (match_dup 5)))]\n+   (parallel [(set (match_operand:HI 0 \"register_operand\" \"\")\n+                   (minus:HI (match_dup 4)\n+                             (match_dup 5)))\n+              (clobber (scratch:QI))])]\n   \"\"\n   {\n     rtx addr;\n@@ -1043,10 +1081,12 @@\n ;+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n ; add bytes\n \n-(define_insn \"addqi3\"\n-  [(set (match_operand:QI 0 \"register_operand\"          \"=r,d,r,r,r,r\")\n-        (plus:QI (match_operand:QI 1 \"register_operand\" \"%0,0,0,0,0,0\")\n-                 (match_operand:QI 2 \"nonmemory_operand\" \"r,i,P,N,K,Cm2\")))]\n+;; \"addqi3\"\n+;; \"addqq3\" \"adduqq3\"\n+(define_insn \"add<mode>3\"\n+  [(set (match_operand:ALL1 0 \"register_operand\"            \"=r,d    ,r  ,r  ,r  ,r\")\n+        (plus:ALL1 (match_operand:ALL1 1 \"register_operand\" \"%0,0    ,0  ,0  ,0  ,0\")\n+                   (match_operand:ALL1 2 \"nonmemory_operand\" \"r,n Ynn,Y01,Ym1,Y02,Ym2\")))]\n   \"\"\n   \"@\n \tadd %0,%2\n@@ -1058,11 +1098,13 @@\n   [(set_attr \"length\" \"1,1,1,1,2,2\")\n    (set_attr \"cc\" \"set_czn,set_czn,set_zn,set_zn,set_zn,set_zn\")])\n \n-\n-(define_expand \"addhi3\"\n-  [(set (match_operand:HI 0 \"register_operand\" \"\")\n-        (plus:HI (match_operand:HI 1 \"register_operand\" \"\")\n-                 (match_operand:HI 2 \"nonmemory_operand\" \"\")))]\n+;; \"addhi3\"\n+;; \"addhq3\" \"adduhq3\"\n+;; \"addha3\" \"adduha3\"\n+(define_expand \"add<mode>3\"\n+  [(set (match_operand:ALL2 0 \"register_operand\" \"\")\n+        (plus:ALL2 (match_operand:ALL2 1 \"register_operand\" \"\")\n+                   (match_operand:ALL2 2 \"nonmemory_or_const_operand\" \"\")))]\n   \"\"\n   {\n     if (CONST_INT_P (operands[2]))\n@@ -1079,6 +1121,12 @@\n             DONE;\n           }\n       }\n+\n+    if (CONST_FIXED == GET_CODE (operands[2]))\n+      {\n+        emit_insn (gen_add<mode>3_clobber (operands[0], operands[1], operands[2]));\n+        DONE;\n+      }\n   })\n \n \n@@ -1124,24 +1172,22 @@\n   [(set_attr \"length\" \"6\")\n    (set_attr \"adjust_len\" \"addto_sp\")])\n \n-(define_insn \"*addhi3\"\n-  [(set (match_operand:HI 0 \"register_operand\"          \"=r,d,!w,d\")\n-        (plus:HI (match_operand:HI 1 \"register_operand\" \"%0,0,0 ,0\")\n-                 (match_operand:HI 2 \"nonmemory_operand\" \"r,s,IJ,n\")))]\n+;; \"*addhi3\"\n+;; \"*addhq3\" \"*adduhq3\"\n+;; \"*addha3\" \"*adduha3\"\n+(define_insn \"*add<mode>3\"\n+  [(set (match_operand:ALL2 0 \"register_operand\"                     \"=r,d,!w    ,d\")\n+        (plus:ALL2 (match_operand:ALL2 1 \"register_operand\"          \"%0,0,0     ,0\")\n+                   (match_operand:ALL2 2 \"nonmemory_or_const_operand\" \"r,s,IJ YIJ,n Ynn\")))]\n   \"\"\n   {\n-    static const char * const asm_code[] =\n-      {\n-        \"add %A0,%A2\\;adc %B0,%B2\",\n-        \"subi %A0,lo8(-(%2))\\;sbci %B0,hi8(-(%2))\",\n-        \"\",\n-        \"\"\n-      };\n-\n-    if (*asm_code[which_alternative])\n-      return asm_code[which_alternative];\n-\n-    return avr_out_plus_noclobber (operands, NULL, NULL);\n+    if (REG_P (operands[2]))\n+      return \"add %A0,%A2\\;adc %B0,%B2\";\n+    else if (CONST_INT_P (operands[2])\n+             || CONST_FIXED == GET_CODE (operands[2]))\n+      return avr_out_plus_noclobber (operands, NULL, NULL);\n+    else\n+      return \"subi %A0,lo8(-(%2))\\;sbci %B0,hi8(-(%2))\";\n   }\n   [(set_attr \"length\" \"2,2,2,2\")\n    (set_attr \"adjust_len\" \"*,*,out_plus_noclobber,out_plus_noclobber\")\n@@ -1152,41 +1198,44 @@\n ;; itself because that insn is special to reload.\n \n (define_peephole2 ; addhi3_clobber\n-  [(set (match_operand:HI 0 \"d_register_operand\" \"\")\n-        (match_operand:HI 1 \"const_int_operand\" \"\"))\n-   (set (match_operand:HI 2 \"l_register_operand\" \"\")\n-        (plus:HI (match_dup 2)\n-                 (match_dup 0)))]\n+  [(set (match_operand:ALL2 0 \"d_register_operand\" \"\")\n+        (match_operand:ALL2 1 \"const_operand\" \"\"))\n+   (set (match_operand:ALL2 2 \"l_register_operand\" \"\")\n+        (plus:ALL2 (match_dup 2)\n+                   (match_dup 0)))]\n   \"peep2_reg_dead_p (2, operands[0])\"\n   [(parallel [(set (match_dup 2)\n-                   (plus:HI (match_dup 2)\n-                            (match_dup 1)))\n+                   (plus:ALL2 (match_dup 2)\n+                              (match_dup 1)))\n               (clobber (match_dup 3))])]\n   {\n-    operands[3] = simplify_gen_subreg (QImode, operands[0], HImode, 0);\n+    operands[3] = simplify_gen_subreg (QImode, operands[0], <MODE>mode, 0);\n   })\n \n ;; Same, but with reload to NO_LD_REGS\n ;; Combine *reload_inhi with *addhi3\n \n (define_peephole2 ; addhi3_clobber\n-  [(parallel [(set (match_operand:HI 0 \"l_register_operand\" \"\")\n-                   (match_operand:HI 1 \"const_int_operand\" \"\"))\n+  [(parallel [(set (match_operand:ALL2 0 \"l_register_operand\" \"\")\n+                   (match_operand:ALL2 1 \"const_operand\" \"\"))\n               (clobber (match_operand:QI 2 \"d_register_operand\" \"\"))])\n-   (set (match_operand:HI 3 \"l_register_operand\" \"\")\n-        (plus:HI (match_dup 3)\n-                 (match_dup 0)))]\n+   (set (match_operand:ALL2 3 \"l_register_operand\" \"\")\n+        (plus:ALL2 (match_dup 3)\n+                   (match_dup 0)))]\n   \"peep2_reg_dead_p (2, operands[0])\"\n   [(parallel [(set (match_dup 3)\n-                   (plus:HI (match_dup 3)\n-                            (match_dup 1)))\n+                   (plus:ALL2 (match_dup 3)\n+                              (match_dup 1)))\n               (clobber (match_dup 2))])])\n \n-(define_insn \"addhi3_clobber\"\n-  [(set (match_operand:HI 0 \"register_operand\"           \"=!w,d,r\")\n-        (plus:HI (match_operand:HI 1 \"register_operand\"   \"%0,0,0\")\n-                 (match_operand:HI 2 \"const_int_operand\"  \"IJ,n,n\")))\n-   (clobber (match_scratch:QI 3                           \"=X,X,&d\"))]\n+;; \"addhi3_clobber\"\n+;; \"addhq3_clobber\" \"adduhq3_clobber\"\n+;; \"addha3_clobber\" \"adduha3_clobber\"\n+(define_insn \"add<mode>3_clobber\"\n+  [(set (match_operand:ALL2 0 \"register_operand\"            \"=!w    ,d    ,r\")\n+        (plus:ALL2 (match_operand:ALL2 1 \"register_operand\"  \"%0    ,0    ,0\")\n+                   (match_operand:ALL2 2 \"const_operand\"     \"IJ YIJ,n Ynn,n Ynn\")))\n+   (clobber (match_scratch:QI 3                             \"=X     ,X    ,&d\"))]\n   \"\"\n   {\n     gcc_assert (REGNO (operands[0]) == REGNO (operands[1]));\n@@ -1198,29 +1247,24 @@\n    (set_attr \"cc\" \"out_plus\")])\n \n \n-(define_insn \"addsi3\"\n-  [(set (match_operand:SI 0 \"register_operand\"          \"=r,d ,d,r\")\n-        (plus:SI (match_operand:SI 1 \"register_operand\" \"%0,0 ,0,0\")\n-                 (match_operand:SI 2 \"nonmemory_operand\" \"r,s ,n,n\")))\n-   (clobber (match_scratch:QI 3                         \"=X,X ,X,&d\"))]\n+;; \"addsi3\"\n+;; \"addsq3\" \"addusq3\"\n+;; \"addsa3\" \"addusa3\"\n+(define_insn \"add<mode>3\"\n+  [(set (match_operand:ALL4 0 \"register_operand\"            \"=r,d ,r\")\n+        (plus:ALL4 (match_operand:ALL4 1 \"register_operand\" \"%0,0 ,0\")\n+                   (match_operand:ALL4 2 \"nonmemory_operand\" \"r,i ,n Ynn\")))\n+   (clobber (match_scratch:QI 3                             \"=X,X ,&d\"))]\n   \"\"\n   {\n-    static const char * const asm_code[] =\n-      {\n-        \"add %A0,%A2\\;adc %B0,%B2\\;adc %C0,%C2\\;adc %D0,%D2\",\n-        \"subi %0,lo8(-(%2))\\;sbci %B0,hi8(-(%2))\\;sbci %C0,hlo8(-(%2))\\;sbci %D0,hhi8(-(%2))\",\n-        \"\",\n-        \"\"\n-      };\n-\n-    if (*asm_code[which_alternative])\n-      return asm_code[which_alternative];\n+    if (REG_P (operands[2]))\n+      return \"add %A0,%A2\\;adc %B0,%B2\\;adc %C0,%C2\\;adc %D0,%D2\";\n \n     return avr_out_plus (operands, NULL, NULL);\n   }\n-  [(set_attr \"length\" \"4,4,4,8\")\n-   (set_attr \"adjust_len\" \"*,*,out_plus,out_plus\")\n-   (set_attr \"cc\" \"set_n,set_czn,out_plus,out_plus\")])\n+  [(set_attr \"length\" \"4,4,8\")\n+   (set_attr \"adjust_len\" \"*,out_plus,out_plus\")\n+   (set_attr \"cc\" \"set_n,out_plus,out_plus\")])\n \n (define_insn \"*addpsi3_zero_extend.qi\"\n   [(set (match_operand:PSI 0 \"register_operand\"                          \"=r\")\n@@ -1329,27 +1373,38 @@\n \n ;-----------------------------------------------------------------------------\n ; sub bytes\n-(define_insn \"subqi3\"\n-  [(set (match_operand:QI 0 \"register_operand\" \"=r,d\")\n-        (minus:QI (match_operand:QI 1 \"register_operand\" \"0,0\")\n-                  (match_operand:QI 2 \"nonmemory_operand\" \"r,i\")))]\n+\n+;; \"subqi3\"\n+;; \"subqq3\" \"subuqq3\"\n+(define_insn \"sub<mode>3\"\n+  [(set (match_operand:ALL1 0 \"register_operand\"                      \"=r,d    ,r  ,r  ,r  ,r\")\n+        (minus:ALL1 (match_operand:ALL1 1 \"register_operand\"           \"0,0    ,0  ,0  ,0  ,0\")\n+                    (match_operand:ALL1 2 \"nonmemory_or_const_operand\" \"r,n Ynn,Y01,Ym1,Y02,Ym2\")))]\n   \"\"\n   \"@\n \tsub %0,%2\n-\tsubi %0,lo8(%2)\"\n-  [(set_attr \"length\" \"1,1\")\n-   (set_attr \"cc\" \"set_czn,set_czn\")])\n+\tsubi %0,lo8(%2)\n+\tdec %0\n+\tinc %0\n+\tdec %0\\;dec %0\n+\tinc %0\\;inc %0\"\n+  [(set_attr \"length\" \"1,1,1,1,2,2\")\n+   (set_attr \"cc\" \"set_czn,set_czn,set_zn,set_zn,set_zn,set_zn\")])\n \n-(define_insn \"subhi3\"\n-  [(set (match_operand:HI 0 \"register_operand\" \"=r,d\")\n-        (minus:HI (match_operand:HI 1 \"register_operand\" \"0,0\")\n-                  (match_operand:HI 2 \"nonmemory_operand\" \"r,i\")))]\n+;; \"subhi3\"\n+;; \"subhq3\" \"subuhq3\"\n+;; \"subha3\" \"subuha3\"\n+(define_insn \"sub<mode>3\"\n+  [(set (match_operand:ALL2 0 \"register_operand\"                      \"=r,d    ,*r\")\n+        (minus:ALL2 (match_operand:ALL2 1 \"register_operand\"           \"0,0    ,0\")\n+                    (match_operand:ALL2 2 \"nonmemory_or_const_operand\" \"r,i Ynn,Ynn\")))\n+   (clobber (match_scratch:QI 3                                       \"=X,X    ,&d\"))]\n   \"\"\n-  \"@\n-\tsub %A0,%A2\\;sbc %B0,%B2\n-\tsubi %A0,lo8(%2)\\;sbci %B0,hi8(%2)\"\n-  [(set_attr \"length\" \"2,2\")\n-   (set_attr \"cc\" \"set_czn,set_czn\")])\n+  {\n+    return avr_out_minus (operands, NULL, NULL);\n+  }\n+  [(set_attr \"adjust_len\" \"minus\")\n+   (set_attr \"cc\" \"minus\")])\n \n (define_insn \"*subhi3_zero_extend1\"\n   [(set (match_operand:HI 0 \"register_operand\"                          \"=r\")\n@@ -1373,13 +1428,23 @@\n   [(set_attr \"length\" \"5\")\n    (set_attr \"cc\" \"clobber\")])\n \n-(define_insn \"subsi3\"\n-  [(set (match_operand:SI 0 \"register_operand\"          \"=r\")\n-        (minus:SI (match_operand:SI 1 \"register_operand\" \"0\")\n-                  (match_operand:SI 2 \"register_operand\" \"r\")))]\n+;; \"subsi3\"\n+;; \"subsq3\" \"subusq3\"\n+;; \"subsa3\" \"subusa3\"\n+(define_insn \"sub<mode>3\"\n+  [(set (match_operand:ALL4 0 \"register_operand\"                      \"=r,d    ,r\")\n+        (minus:ALL4 (match_operand:ALL4 1 \"register_operand\"           \"0,0    ,0\")\n+                    (match_operand:ALL4 2 \"nonmemory_or_const_operand\" \"r,n Ynn,Ynn\")))\n+   (clobber (match_scratch:QI 3                                       \"=X,X    ,&d\"))]\n   \"\"\n-  \"sub %0,%2\\;sbc %B0,%B2\\;sbc %C0,%C2\\;sbc %D0,%D2\"\n+  {\n+    if (REG_P (operands[2]))\n+      return \"sub %0,%2\\;sbc %B0,%B2\\;sbc %C0,%C2\\;sbc %D0,%D2\";\n+    \n+    return avr_out_minus (operands, NULL, NULL);\n+  }\n   [(set_attr \"length\" \"4\")\n+   (set_attr \"adjust_len\" \"*,minus,minus\")\n    (set_attr \"cc\" \"set_czn\")])\n \n (define_insn \"*subsi3_zero_extend\"\n@@ -1515,8 +1580,18 @@\n \tadc %A0,__zero_reg__\\;adc %B0,__zero_reg__\\;adc %C0,__zero_reg__\\;adc %D0,__zero_reg__\"\n   [(set_attr \"length\" \"6\")\n    (set_attr \"cc\" \"clobber\")])\n-  \n \n+(define_insn \"*umulqihi3.call\"\n+  [(set (reg:HI 24)\n+        (mult:HI (zero_extend:HI (reg:QI 22))\n+                 (zero_extend:HI (reg:QI 24))))\n+   (clobber (reg:QI 21))\n+   (clobber (reg:HI 22))]\n+  \"!AVR_HAVE_MUL\"\n+  \"%~call __umulqihi3\"\n+  [(set_attr \"type\" \"xcall\")\n+   (set_attr \"cc\" \"clobber\")])\n+  \n ;; \"umulqihi3\"\n ;; \"mulqihi3\"\n (define_insn \"<extend_u>mulqihi3\"\n@@ -3303,44 +3378,58 @@\n ;;<< << << << << << << << << << << << << << << << << << << << << << << << << <<\n ;; arithmetic shift left\n \n-(define_expand \"ashlqi3\"\n-  [(set (match_operand:QI 0 \"register_operand\"            \"\")\n-        (ashift:QI (match_operand:QI 1 \"register_operand\" \"\")\n-                   (match_operand:QI 2 \"nop_general_operand\" \"\")))])\n+;; \"ashlqi3\"\n+;; \"ashlqq3\"  \"ashluqq3\"\n+(define_expand \"ashl<mode>3\"\n+  [(set (match_operand:ALL1 0 \"register_operand\" \"\")\n+        (ashift:ALL1 (match_operand:ALL1 1 \"register_operand\" \"\")\n+                     (match_operand:QI 2 \"nop_general_operand\" \"\")))])\n \n (define_split ; ashlqi3_const4\n-  [(set (match_operand:QI 0 \"d_register_operand\" \"\")\n-        (ashift:QI (match_dup 0)\n-                   (const_int 4)))]\n+  [(set (match_operand:ALL1 0 \"d_register_operand\" \"\")\n+        (ashift:ALL1 (match_dup 0)\n+                     (const_int 4)))]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n-   (set (match_dup 0) (and:QI (match_dup 0) (const_int -16)))]\n-  \"\")\n+  [(set (match_dup 1)\n+        (rotate:QI (match_dup 1)\n+                   (const_int 4)))\n+   (set (match_dup 1)\n+        (and:QI (match_dup 1)\n+                (const_int -16)))]\n+  {\n+    operands[1] = avr_to_int_mode (operands[0]);\n+  })\n \n (define_split ; ashlqi3_const5\n-  [(set (match_operand:QI 0 \"d_register_operand\" \"\")\n-        (ashift:QI (match_dup 0)\n-                   (const_int 5)))]\n+  [(set (match_operand:ALL1 0 \"d_register_operand\" \"\")\n+        (ashift:ALL1 (match_dup 0)\n+                     (const_int 5)))]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n-   (set (match_dup 0) (ashift:QI (match_dup 0) (const_int 1)))\n-   (set (match_dup 0) (and:QI (match_dup 0) (const_int -32)))]\n-  \"\")\n+  [(set (match_dup 1) (rotate:QI (match_dup 1) (const_int 4)))\n+   (set (match_dup 1) (ashift:QI (match_dup 1) (const_int 1)))\n+   (set (match_dup 1) (and:QI (match_dup 1) (const_int -32)))]\n+  {\n+    operands[1] = avr_to_int_mode (operands[0]);\n+  })\n \n (define_split ; ashlqi3_const6\n-  [(set (match_operand:QI 0 \"d_register_operand\" \"\")\n-        (ashift:QI (match_dup 0)\n-                   (const_int 6)))]\n+  [(set (match_operand:ALL1 0 \"d_register_operand\" \"\")\n+        (ashift:ALL1 (match_dup 0)\n+                     (const_int 6)))]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n-   (set (match_dup 0) (ashift:QI (match_dup 0) (const_int 2)))\n-   (set (match_dup 0) (and:QI (match_dup 0) (const_int -64)))]\n-  \"\")\n+  [(set (match_dup 1) (rotate:QI (match_dup 1) (const_int 4)))\n+   (set (match_dup 1) (ashift:QI (match_dup 1) (const_int 2)))\n+   (set (match_dup 1) (and:QI (match_dup 1) (const_int -64)))]\n+  {\n+    operands[1] = avr_to_int_mode (operands[0]);\n+  })\n \n-(define_insn \"*ashlqi3\"\n-  [(set (match_operand:QI 0 \"register_operand\"              \"=r,r,r,r,!d,r,r\")\n-        (ashift:QI (match_operand:QI 1 \"register_operand\"    \"0,0,0,0,0 ,0,0\")\n-                   (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,K,n ,n,Qm\")))]\n+;; \"*ashlqi3\"\n+;; \"*ashlqq3\"  \"*ashluqq3\"\n+(define_insn \"*ashl<mode>3\"\n+  [(set (match_operand:ALL1 0 \"register_operand\"              \"=r,r,r,r,!d,r,r\")\n+        (ashift:ALL1 (match_operand:ALL1 1 \"register_operand\"  \"0,0,0,0,0 ,0,0\")\n+                     (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,K,n ,n,Qm\")))]\n   \"\"\n   {\n     return ashlqi3_out (insn, operands, NULL);\n@@ -3349,10 +3438,10 @@\n    (set_attr \"adjust_len\" \"ashlqi\")\n    (set_attr \"cc\" \"clobber,none,set_czn,set_czn,set_czn,set_czn,clobber\")])\n \n-(define_insn \"ashlhi3\"\n-  [(set (match_operand:HI 0 \"register_operand\"              \"=r,r,r,r,r,r,r\")\n-        (ashift:HI (match_operand:HI 1 \"register_operand\"    \"0,0,0,r,0,0,0\")\n-                   (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,O,K,n,Qm\")))]\n+(define_insn \"ashl<mode>3\"\n+  [(set (match_operand:ALL2 0 \"register_operand\"              \"=r,r,r,r,r,r,r\")\n+        (ashift:ALL2 (match_operand:ALL2 1 \"register_operand\"  \"0,0,0,r,0,0,0\")\n+                     (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,O,K,n,Qm\")))]\n   \"\"\n   {\n     return ashlhi3_out (insn, operands, NULL);\n@@ -3377,8 +3466,7 @@\n   \"\"\n   [(set (match_dup 0)\n         (ashift:QI (match_dup 1)\n-                   (match_dup 2)))]\n-  \"\")\n+                   (match_dup 2)))])\n \n ;; ??? Combiner does not recognize that it could split the following insn;\n ;;     presumably because he has no register handy?\n@@ -3443,10 +3531,13 @@\n   })\n \n \n-(define_insn \"ashlsi3\"\n-  [(set (match_operand:SI 0 \"register_operand\"              \"=r,r,r,r,r,r,r\")\n-        (ashift:SI (match_operand:SI 1 \"register_operand\"    \"0,0,0,r,0,0,0\")\n-                   (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,O,K,n,Qm\")))]\n+;; \"ashlsi3\"\n+;; \"ashlsq3\"  \"ashlusq3\"\n+;; \"ashlsa3\"  \"ashlusa3\"\n+(define_insn \"ashl<mode>3\"\n+  [(set (match_operand:ALL4 0 \"register_operand\"                \"=r,r,r,r,r,r,r\")\n+        (ashift:ALL4 (match_operand:ALL4 1 \"register_operand\"    \"0,0,0,r,0,0,0\")\n+                     (match_operand:QI 2 \"nop_general_operand\"   \"r,L,P,O,K,n,Qm\")))]\n   \"\"\n   {\n     return ashlsi3_out (insn, operands, NULL);\n@@ -3458,55 +3549,65 @@\n ;; Optimize if a scratch register from LD_REGS happens to be available.\n \n (define_peephole2 ; ashlqi3_l_const4\n-  [(set (match_operand:QI 0 \"l_register_operand\" \"\")\n-        (ashift:QI (match_dup 0)\n-                   (const_int 4)))\n+  [(set (match_operand:ALL1 0 \"l_register_operand\" \"\")\n+        (ashift:ALL1 (match_dup 0)\n+                     (const_int 4)))\n    (match_scratch:QI 1 \"d\")]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n+  [(set (match_dup 2) (rotate:QI (match_dup 2) (const_int 4)))\n    (set (match_dup 1) (const_int -16))\n-   (set (match_dup 0) (and:QI (match_dup 0) (match_dup 1)))]\n-  \"\")\n+   (set (match_dup 2) (and:QI (match_dup 2) (match_dup 1)))]\n+  {\n+    operands[2] = avr_to_int_mode (operands[0]);\n+  })\n \n (define_peephole2 ; ashlqi3_l_const5\n-  [(set (match_operand:QI 0 \"l_register_operand\" \"\")\n-        (ashift:QI (match_dup 0)\n-                   (const_int 5)))\n+  [(set (match_operand:ALL1 0 \"l_register_operand\" \"\")\n+        (ashift:ALL1 (match_dup 0)\n+                     (const_int 5)))\n    (match_scratch:QI 1 \"d\")]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n-   (set (match_dup 0) (ashift:QI (match_dup 0) (const_int 1)))\n+  [(set (match_dup 2) (rotate:QI (match_dup 2) (const_int 4)))\n+   (set (match_dup 2) (ashift:QI (match_dup 2) (const_int 1)))\n    (set (match_dup 1) (const_int -32))\n-   (set (match_dup 0) (and:QI (match_dup 0) (match_dup 1)))]\n-  \"\")\n+   (set (match_dup 2) (and:QI (match_dup 2) (match_dup 1)))]\n+  {\n+    operands[2] = avr_to_int_mode (operands[0]);\n+  })\n \n (define_peephole2 ; ashlqi3_l_const6\n-  [(set (match_operand:QI 0 \"l_register_operand\" \"\")\n-        (ashift:QI (match_dup 0)\n-                   (const_int 6)))\n+  [(set (match_operand:ALL1 0 \"l_register_operand\" \"\")\n+        (ashift:ALL1 (match_dup 0)\n+                     (const_int 6)))\n    (match_scratch:QI 1 \"d\")]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n-   (set (match_dup 0) (ashift:QI (match_dup 0) (const_int 2)))\n+  [(set (match_dup 2) (rotate:QI (match_dup 2) (const_int 4)))\n+   (set (match_dup 2) (ashift:QI (match_dup 2) (const_int 2)))\n    (set (match_dup 1) (const_int -64))\n-   (set (match_dup 0) (and:QI (match_dup 0) (match_dup 1)))]\n-  \"\")\n+   (set (match_dup 2) (and:QI (match_dup 2) (match_dup 1)))]\n+  {\n+    operands[2] = avr_to_int_mode (operands[0]);\n+  })\n \n (define_peephole2\n   [(match_scratch:QI 3 \"d\")\n-   (set (match_operand:HI 0 \"register_operand\" \"\")\n-        (ashift:HI (match_operand:HI 1 \"register_operand\" \"\")\n-                   (match_operand:QI 2 \"const_int_operand\" \"\")))]\n+   (set (match_operand:ALL2 0 \"register_operand\" \"\")\n+        (ashift:ALL2 (match_operand:ALL2 1 \"register_operand\" \"\")\n+                     (match_operand:QI 2 \"const_int_operand\" \"\")))]\n   \"\"\n-  [(parallel [(set (match_dup 0) (ashift:HI (match_dup 1) (match_dup 2)))\n-              (clobber (match_dup 3))])]\n-  \"\")\n-\n-(define_insn \"*ashlhi3_const\"\n-  [(set (match_operand:HI 0 \"register_operand\"            \"=r,r,r,r,r\")\n-        (ashift:HI (match_operand:HI 1 \"register_operand\"  \"0,0,r,0,0\")\n-                   (match_operand:QI 2 \"const_int_operand\" \"L,P,O,K,n\")))\n-   (clobber (match_scratch:QI 3                           \"=X,X,X,X,&d\"))]\n+  [(parallel [(set (match_dup 0)\n+                   (ashift:ALL2 (match_dup 1)\n+                                (match_dup 2)))\n+              (clobber (match_dup 3))])])\n+\n+;; \"*ashlhi3_const\"\n+;; \"*ashlhq3_const\"  \"*ashluhq3_const\"\n+;; \"*ashlha3_const\"  \"*ashluha3_const\"\n+(define_insn \"*ashl<mode>3_const\"\n+  [(set (match_operand:ALL2 0 \"register_operand\"              \"=r,r,r,r,r\")\n+        (ashift:ALL2 (match_operand:ALL2 1 \"register_operand\"  \"0,0,r,0,0\")\n+                     (match_operand:QI 2 \"const_int_operand\"   \"L,P,O,K,n\")))\n+   (clobber (match_scratch:QI 3                               \"=X,X,X,X,&d\"))]\n   \"reload_completed\"\n   {\n     return ashlhi3_out (insn, operands, NULL);\n@@ -3517,19 +3618,24 @@\n \n (define_peephole2\n   [(match_scratch:QI 3 \"d\")\n-   (set (match_operand:SI 0 \"register_operand\" \"\")\n-        (ashift:SI (match_operand:SI 1 \"register_operand\" \"\")\n-                   (match_operand:QI 2 \"const_int_operand\" \"\")))]\n+   (set (match_operand:ALL4 0 \"register_operand\" \"\")\n+        (ashift:ALL4 (match_operand:ALL4 1 \"register_operand\" \"\")\n+                     (match_operand:QI 2 \"const_int_operand\" \"\")))]\n   \"\"\n-  [(parallel [(set (match_dup 0) (ashift:SI (match_dup 1) (match_dup 2)))\n+  [(parallel [(set (match_dup 0)\n+                   (ashift:ALL4 (match_dup 1)\n+                                (match_dup 2)))\n               (clobber (match_dup 3))])]\n   \"\")\n \n-(define_insn \"*ashlsi3_const\"\n-  [(set (match_operand:SI 0 \"register_operand\"            \"=r,r,r,r\")\n-        (ashift:SI (match_operand:SI 1 \"register_operand\"  \"0,0,r,0\")\n-                   (match_operand:QI 2 \"const_int_operand\" \"L,P,O,n\")))\n-   (clobber (match_scratch:QI 3                           \"=X,X,X,&d\"))]\n+;; \"*ashlsi3_const\"\n+;; \"*ashlsq3_const\"  \"*ashlusq3_const\"\n+;; \"*ashlsa3_const\"  \"*ashlusa3_const\"\n+(define_insn \"*ashl<mode>3_const\"\n+  [(set (match_operand:ALL4 0 \"register_operand\"              \"=r,r,r,r\")\n+        (ashift:ALL4 (match_operand:ALL4 1 \"register_operand\"  \"0,0,r,0\")\n+                     (match_operand:QI 2 \"const_int_operand\"   \"L,P,O,n\")))\n+   (clobber (match_scratch:QI 3                               \"=X,X,X,&d\"))]\n   \"reload_completed\"\n   {\n     return ashlsi3_out (insn, operands, NULL);\n@@ -3580,10 +3686,12 @@\n ;; >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >>\n ;; arithmetic shift right\n \n-(define_insn \"ashrqi3\"\n-  [(set (match_operand:QI 0 \"register_operand\"                \"=r,r,r,r,r          ,r      ,r\")\n-        (ashiftrt:QI (match_operand:QI 1 \"register_operand\"    \"0,0,0,0,0          ,0      ,0\")\n-                     (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,K,C03 C04 C05,C06 C07,Qm\")))]\n+;; \"ashrqi3\"\n+;; \"ashrqq3\"  \"ashruqq3\"\n+(define_insn \"ashr<mode>3\"\n+  [(set (match_operand:ALL1 0 \"register_operand\"                  \"=r,r,r,r,r          ,r      ,r\")\n+        (ashiftrt:ALL1 (match_operand:ALL1 1 \"register_operand\"    \"0,0,0,0,0          ,0      ,0\")\n+                       (match_operand:QI 2 \"nop_general_operand\"   \"r,L,P,K,C03 C04 C05,C06 C07,Qm\")))]\n   \"\"\n   {\n     return ashrqi3_out (insn, operands, NULL);\n@@ -3592,10 +3700,13 @@\n    (set_attr \"adjust_len\" \"ashrqi\")\n    (set_attr \"cc\" \"clobber,none,set_czn,set_czn,set_czn,clobber,clobber\")])\n \n-(define_insn \"ashrhi3\"\n-  [(set (match_operand:HI 0 \"register_operand\"                \"=r,r,r,r,r,r,r\")\n-        (ashiftrt:HI (match_operand:HI 1 \"register_operand\"    \"0,0,0,r,0,0,0\")\n-                     (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,O,K,n,Qm\")))]\n+;; \"ashrhi3\"\n+;; \"ashrhq3\"  \"ashruhq3\"\n+;; \"ashrha3\"  \"ashruha3\"\n+(define_insn \"ashr<mode>3\"\n+  [(set (match_operand:ALL2 0 \"register_operand\"                \"=r,r,r,r,r,r,r\")\n+        (ashiftrt:ALL2 (match_operand:ALL2 1 \"register_operand\"  \"0,0,0,r,0,0,0\")\n+                       (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,O,K,n,Qm\")))]\n   \"\"\n   {\n     return ashrhi3_out (insn, operands, NULL);\n@@ -3616,10 +3727,13 @@\n   [(set_attr \"adjust_len\" \"ashrpsi\")\n    (set_attr \"cc\" \"clobber\")])\n \n-(define_insn \"ashrsi3\"\n-  [(set (match_operand:SI 0 \"register_operand\"                \"=r,r,r,r,r,r,r\")\n-        (ashiftrt:SI (match_operand:SI 1 \"register_operand\"    \"0,0,0,r,0,0,0\")\n-                     (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,O,K,n,Qm\")))]\n+;; \"ashrsi3\"\n+;; \"ashrsq3\"  \"ashrusq3\"\n+;; \"ashrsa3\"  \"ashrusa3\"\n+(define_insn \"ashr<mode>3\"\n+  [(set (match_operand:ALL4 0 \"register_operand\"                  \"=r,r,r,r,r,r,r\")\n+        (ashiftrt:ALL4 (match_operand:ALL4 1 \"register_operand\"    \"0,0,0,r,0,0,0\")\n+                       (match_operand:QI 2 \"nop_general_operand\"   \"r,L,P,O,K,n,Qm\")))]\n   \"\"\n   {\n     return ashrsi3_out (insn, operands, NULL);\n@@ -3632,19 +3746,23 @@\n \n (define_peephole2\n   [(match_scratch:QI 3 \"d\")\n-   (set (match_operand:HI 0 \"register_operand\" \"\")\n-        (ashiftrt:HI (match_operand:HI 1 \"register_operand\" \"\")\n-                     (match_operand:QI 2 \"const_int_operand\" \"\")))]\n+   (set (match_operand:ALL2 0 \"register_operand\" \"\")\n+        (ashiftrt:ALL2 (match_operand:ALL2 1 \"register_operand\" \"\")\n+                       (match_operand:QI 2 \"const_int_operand\" \"\")))]\n   \"\"\n-  [(parallel [(set (match_dup 0) (ashiftrt:HI (match_dup 1) (match_dup 2)))\n-              (clobber (match_dup 3))])]\n-  \"\")\n-\n-(define_insn \"*ashrhi3_const\"\n-  [(set (match_operand:HI 0 \"register_operand\"              \"=r,r,r,r,r\")\n-        (ashiftrt:HI (match_operand:HI 1 \"register_operand\"  \"0,0,r,0,0\")\n-                     (match_operand:QI 2 \"const_int_operand\" \"L,P,O,K,n\")))\n-   (clobber (match_scratch:QI 3                             \"=X,X,X,X,&d\"))]\n+  [(parallel [(set (match_dup 0)\n+                   (ashiftrt:ALL2 (match_dup 1)\n+                                  (match_dup 2)))\n+              (clobber (match_dup 3))])])\n+\n+;; \"*ashrhi3_const\"\n+;; \"*ashrhq3_const\"  \"*ashruhq3_const\"\n+;; \"*ashrha3_const\"  \"*ashruha3_const\"\n+(define_insn \"*ashr<mode>3_const\"\n+  [(set (match_operand:ALL2 0 \"register_operand\"                \"=r,r,r,r,r\")\n+        (ashiftrt:ALL2 (match_operand:ALL2 1 \"register_operand\"  \"0,0,r,0,0\")\n+                       (match_operand:QI 2 \"const_int_operand\"   \"L,P,O,K,n\")))\n+   (clobber (match_scratch:QI 3                                 \"=X,X,X,X,&d\"))]\n   \"reload_completed\"\n   {\n     return ashrhi3_out (insn, operands, NULL);\n@@ -3655,19 +3773,23 @@\n \n (define_peephole2\n   [(match_scratch:QI 3 \"d\")\n-   (set (match_operand:SI 0 \"register_operand\" \"\")\n-        (ashiftrt:SI (match_operand:SI 1 \"register_operand\" \"\")\n-                     (match_operand:QI 2 \"const_int_operand\" \"\")))]\n+   (set (match_operand:ALL4 0 \"register_operand\" \"\")\n+        (ashiftrt:ALL4 (match_operand:ALL4 1 \"register_operand\" \"\")\n+                       (match_operand:QI 2 \"const_int_operand\" \"\")))]\n   \"\"\n-  [(parallel [(set (match_dup 0) (ashiftrt:SI (match_dup 1) (match_dup 2)))\n-              (clobber (match_dup 3))])]\n-  \"\")\n-\n-(define_insn \"*ashrsi3_const\"\n-  [(set (match_operand:SI 0 \"register_operand\"              \"=r,r,r,r\")\n-        (ashiftrt:SI (match_operand:SI 1 \"register_operand\"  \"0,0,r,0\")\n-                     (match_operand:QI 2 \"const_int_operand\" \"L,P,O,n\")))\n-   (clobber (match_scratch:QI 3                             \"=X,X,X,&d\"))]\n+  [(parallel [(set (match_dup 0)\n+                   (ashiftrt:ALL4 (match_dup 1)\n+                                  (match_dup 2)))\n+              (clobber (match_dup 3))])])\n+\n+;; \"*ashrsi3_const\"\n+;; \"*ashrsq3_const\"  \"*ashrusq3_const\"\n+;; \"*ashrsa3_const\"  \"*ashrusa3_const\"\n+(define_insn \"*ashr<mode>3_const\"\n+  [(set (match_operand:ALL4 0 \"register_operand\"                \"=r,r,r,r\")\n+        (ashiftrt:ALL4 (match_operand:ALL4 1 \"register_operand\"  \"0,0,r,0\")\n+                       (match_operand:QI 2 \"const_int_operand\"   \"L,P,O,n\")))\n+   (clobber (match_scratch:QI 3                                 \"=X,X,X,&d\"))]\n   \"reload_completed\"\n   {\n     return ashrsi3_out (insn, operands, NULL);\n@@ -3679,44 +3801,59 @@\n ;; >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >> >>\n ;; logical shift right\n \n-(define_expand \"lshrqi3\"\n-  [(set (match_operand:QI 0 \"register_operand\" \"\")\n-        (lshiftrt:QI (match_operand:QI 1 \"register_operand\" \"\")\n-                     (match_operand:QI 2 \"nop_general_operand\" \"\")))])\n+;; \"lshrqi3\"\n+;; \"lshrqq3 \"lshruqq3\"\n+(define_expand \"lshr<mode>3\"\n+  [(set (match_operand:ALL1 0 \"register_operand\" \"\")\n+        (lshiftrt:ALL1 (match_operand:ALL1 1 \"register_operand\" \"\")\n+                       (match_operand:QI 2 \"nop_general_operand\" \"\")))])\n \n (define_split\t; lshrqi3_const4\n-  [(set (match_operand:QI 0 \"d_register_operand\" \"\")\n-        (lshiftrt:QI (match_dup 0)\n-                     (const_int 4)))]\n+  [(set (match_operand:ALL1 0 \"d_register_operand\" \"\")\n+        (lshiftrt:ALL1 (match_dup 0)\n+                       (const_int 4)))]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n-   (set (match_dup 0) (and:QI (match_dup 0) (const_int 15)))]\n-  \"\")\n+  [(set (match_dup 1)\n+        (rotate:QI (match_dup 1)\n+                   (const_int 4)))\n+   (set (match_dup 1)\n+        (and:QI (match_dup 1)\n+                (const_int 15)))]\n+  {\n+    operands[1] = avr_to_int_mode (operands[0]);\n+  })\n \n (define_split\t; lshrqi3_const5\n-  [(set (match_operand:QI 0 \"d_register_operand\" \"\")\n-        (lshiftrt:QI (match_dup 0)\n-                     (const_int 5)))]\n+  [(set (match_operand:ALL1 0 \"d_register_operand\" \"\")\n+        (lshiftrt:ALL1 (match_dup 0)\n+                       (const_int 5)))]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n-   (set (match_dup 0) (lshiftrt:QI (match_dup 0) (const_int 1)))\n-   (set (match_dup 0) (and:QI (match_dup 0) (const_int 7)))]\n-  \"\")\n+  [(set (match_dup 1) (rotate:QI (match_dup 1) (const_int 4)))\n+   (set (match_dup 1) (lshiftrt:QI (match_dup 1) (const_int 1)))\n+   (set (match_dup 1) (and:QI (match_dup 1) (const_int 7)))]\n+  {\n+    operands[1] = avr_to_int_mode (operands[0]);\n+  })\n \n (define_split\t; lshrqi3_const6\n   [(set (match_operand:QI 0 \"d_register_operand\" \"\")\n         (lshiftrt:QI (match_dup 0)\n                      (const_int 6)))]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n-   (set (match_dup 0) (lshiftrt:QI (match_dup 0) (const_int 2)))\n-   (set (match_dup 0) (and:QI (match_dup 0) (const_int 3)))]\n-  \"\")\n+  [(set (match_dup 1) (rotate:QI (match_dup 1) (const_int 4)))\n+   (set (match_dup 1) (lshiftrt:QI (match_dup 1) (const_int 2)))\n+   (set (match_dup 1) (and:QI (match_dup 1) (const_int 3)))]\n+  {\n+    operands[1] = avr_to_int_mode (operands[0]);\n+  })\n \n-(define_insn \"*lshrqi3\"\n-  [(set (match_operand:QI 0 \"register_operand\"                \"=r,r,r,r,!d,r,r\")\n-        (lshiftrt:QI (match_operand:QI 1 \"register_operand\"    \"0,0,0,0,0 ,0,0\")\n-                     (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,K,n ,n,Qm\")))]\n+;; \"*lshrqi3\"\n+;; \"*lshrqq3\"\n+;; \"*lshruqq3\"\n+(define_insn \"*lshr<mode>3\"\n+  [(set (match_operand:ALL1 0 \"register_operand\"                  \"=r,r,r,r,!d,r,r\")\n+        (lshiftrt:ALL1 (match_operand:ALL1 1 \"register_operand\"    \"0,0,0,0,0 ,0,0\")\n+                       (match_operand:QI 2 \"nop_general_operand\"   \"r,L,P,K,n ,n,Qm\")))]\n   \"\"\n   {\n     return lshrqi3_out (insn, operands, NULL);\n@@ -3725,10 +3862,13 @@\n    (set_attr \"adjust_len\" \"lshrqi\")\n    (set_attr \"cc\" \"clobber,none,set_czn,set_czn,set_czn,set_czn,clobber\")])\n \n-(define_insn \"lshrhi3\"\n-  [(set (match_operand:HI 0 \"register_operand\"                \"=r,r,r,r,r,r,r\")\n-        (lshiftrt:HI (match_operand:HI 1 \"register_operand\"    \"0,0,0,r,0,0,0\")\n-                     (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,O,K,n,Qm\")))]\n+;; \"lshrhi3\"\n+;; \"lshrhq3\"  \"lshruhq3\"\n+;; \"lshrha3\"  \"lshruha3\"\n+(define_insn \"lshr<mode>3\"\n+  [(set (match_operand:ALL2 0 \"register_operand\"                \"=r,r,r,r,r,r,r\")\n+        (lshiftrt:ALL2 (match_operand:ALL2 1 \"register_operand\"    \"0,0,0,r,0,0,0\")\n+                       (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,O,K,n,Qm\")))]\n   \"\"\n   {\n     return lshrhi3_out (insn, operands, NULL);\n@@ -3749,10 +3889,13 @@\n   [(set_attr \"adjust_len\" \"lshrpsi\")\n    (set_attr \"cc\" \"clobber\")])\n \n-(define_insn \"lshrsi3\"\n-  [(set (match_operand:SI 0 \"register_operand\"                \"=r,r,r,r,r,r,r\")\n-        (lshiftrt:SI (match_operand:SI 1 \"register_operand\"    \"0,0,0,r,0,0,0\")\n-                     (match_operand:QI 2 \"nop_general_operand\" \"r,L,P,O,K,n,Qm\")))]\n+;; \"lshrsi3\"\n+;; \"lshrsq3\"  \"lshrusq3\"\n+;; \"lshrsa3\"  \"lshrusa3\"\n+(define_insn \"lshr<mode>3\"\n+  [(set (match_operand:ALL4 0 \"register_operand\"                  \"=r,r,r,r,r,r,r\")\n+        (lshiftrt:ALL4 (match_operand:ALL4 1 \"register_operand\"    \"0,0,0,r,0,0,0\")\n+                       (match_operand:QI 2 \"nop_general_operand\"   \"r,L,P,O,K,n,Qm\")))]\n   \"\"\n   {\n     return lshrsi3_out (insn, operands, NULL);\n@@ -3764,55 +3907,65 @@\n ;; Optimize if a scratch register from LD_REGS happens to be available.\n \n (define_peephole2 ; lshrqi3_l_const4\n-  [(set (match_operand:QI 0 \"l_register_operand\" \"\")\n-        (lshiftrt:QI (match_dup 0)\n-                     (const_int 4)))\n+  [(set (match_operand:ALL1 0 \"l_register_operand\" \"\")\n+        (lshiftrt:ALL1 (match_dup 0)\n+                       (const_int 4)))\n    (match_scratch:QI 1 \"d\")]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n+  [(set (match_dup 2) (rotate:QI (match_dup 2) (const_int 4)))\n    (set (match_dup 1) (const_int 15))\n-   (set (match_dup 0) (and:QI (match_dup 0) (match_dup 1)))]\n-  \"\")\n+   (set (match_dup 2) (and:QI (match_dup 2) (match_dup 1)))]\n+  {\n+    operands[2] = avr_to_int_mode (operands[0]);\n+  })\n \n (define_peephole2 ; lshrqi3_l_const5\n-  [(set (match_operand:QI 0 \"l_register_operand\" \"\")\n-        (lshiftrt:QI (match_dup 0)\n-                     (const_int 5)))\n+  [(set (match_operand:ALL1 0 \"l_register_operand\" \"\")\n+        (lshiftrt:ALL1 (match_dup 0)\n+                       (const_int 5)))\n    (match_scratch:QI 1 \"d\")]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n-   (set (match_dup 0) (lshiftrt:QI (match_dup 0) (const_int 1)))\n+  [(set (match_dup 2) (rotate:QI (match_dup 2) (const_int 4)))\n+   (set (match_dup 2) (lshiftrt:QI (match_dup 2) (const_int 1)))\n    (set (match_dup 1) (const_int 7))\n-   (set (match_dup 0) (and:QI (match_dup 0) (match_dup 1)))]\n-  \"\")\n+   (set (match_dup 2) (and:QI (match_dup 2) (match_dup 1)))]\n+  {\n+    operands[2] = avr_to_int_mode (operands[0]);\n+  })\n \n (define_peephole2 ; lshrqi3_l_const6\n-  [(set (match_operand:QI 0 \"l_register_operand\" \"\")\n-        (lshiftrt:QI (match_dup 0)\n-                     (const_int 6)))\n+  [(set (match_operand:ALL1 0 \"l_register_operand\" \"\")\n+        (lshiftrt:ALL1 (match_dup 0)\n+                       (const_int 6)))\n    (match_scratch:QI 1 \"d\")]\n   \"\"\n-  [(set (match_dup 0) (rotate:QI (match_dup 0) (const_int 4)))\n-   (set (match_dup 0) (lshiftrt:QI (match_dup 0) (const_int 2)))\n+  [(set (match_dup 2) (rotate:QI (match_dup 2) (const_int 4)))\n+   (set (match_dup 2) (lshiftrt:QI (match_dup 2) (const_int 2)))\n    (set (match_dup 1) (const_int 3))\n-   (set (match_dup 0) (and:QI (match_dup 0) (match_dup 1)))]\n-  \"\")\n+   (set (match_dup 2) (and:QI (match_dup 2) (match_dup 1)))]\n+  {\n+    operands[2] = avr_to_int_mode (operands[0]);\n+  })\n \n (define_peephole2\n   [(match_scratch:QI 3 \"d\")\n-   (set (match_operand:HI 0 \"register_operand\" \"\")\n-        (lshiftrt:HI (match_operand:HI 1 \"register_operand\" \"\")\n-                     (match_operand:QI 2 \"const_int_operand\" \"\")))]\n+   (set (match_operand:ALL2 0 \"register_operand\" \"\")\n+        (lshiftrt:ALL2 (match_operand:ALL2 1 \"register_operand\" \"\")\n+                       (match_operand:QI 2 \"const_int_operand\" \"\")))]\n   \"\"\n-  [(parallel [(set (match_dup 0) (lshiftrt:HI (match_dup 1) (match_dup 2)))\n-              (clobber (match_dup 3))])]\n-  \"\")\n-\n-(define_insn \"*lshrhi3_const\"\n-  [(set (match_operand:HI 0 \"register_operand\"              \"=r,r,r,r,r\")\n-        (lshiftrt:HI (match_operand:HI 1 \"register_operand\"  \"0,0,r,0,0\")\n-                     (match_operand:QI 2 \"const_int_operand\" \"L,P,O,K,n\")))\n-   (clobber (match_scratch:QI 3                             \"=X,X,X,X,&d\"))]\n+  [(parallel [(set (match_dup 0)\n+                   (lshiftrt:ALL2 (match_dup 1)\n+                                  (match_dup 2)))\n+              (clobber (match_dup 3))])])\n+\n+;; \"*lshrhi3_const\"\n+;; \"*lshrhq3_const\"  \"*lshruhq3_const\"\n+;; \"*lshrha3_const\"  \"*lshruha3_const\"\n+(define_insn \"*lshr<mode>3_const\"\n+  [(set (match_operand:ALL2 0 \"register_operand\"                \"=r,r,r,r,r\")\n+        (lshiftrt:ALL2 (match_operand:ALL2 1 \"register_operand\"  \"0,0,r,0,0\")\n+                       (match_operand:QI 2 \"const_int_operand\"   \"L,P,O,K,n\")))\n+   (clobber (match_scratch:QI 3                                 \"=X,X,X,X,&d\"))]\n   \"reload_completed\"\n   {\n     return lshrhi3_out (insn, operands, NULL);\n@@ -3823,19 +3976,23 @@\n \n (define_peephole2\n   [(match_scratch:QI 3 \"d\")\n-   (set (match_operand:SI 0 \"register_operand\" \"\")\n-        (lshiftrt:SI (match_operand:SI 1 \"register_operand\" \"\")\n-                     (match_operand:QI 2 \"const_int_operand\" \"\")))]\n+   (set (match_operand:ALL4 0 \"register_operand\" \"\")\n+        (lshiftrt:ALL4 (match_operand:ALL4 1 \"register_operand\" \"\")\n+                       (match_operand:QI 2 \"const_int_operand\" \"\")))]\n   \"\"\n-  [(parallel [(set (match_dup 0) (lshiftrt:SI (match_dup 1) (match_dup 2)))\n-              (clobber (match_dup 3))])]\n-  \"\")\n-\n-(define_insn \"*lshrsi3_const\"\n-  [(set (match_operand:SI 0 \"register_operand\"              \"=r,r,r,r\")\n-        (lshiftrt:SI (match_operand:SI 1 \"register_operand\"  \"0,0,r,0\")\n-                     (match_operand:QI 2 \"const_int_operand\" \"L,P,O,n\")))\n-   (clobber (match_scratch:QI 3                             \"=X,X,X,&d\"))]\n+  [(parallel [(set (match_dup 0)\n+                   (lshiftrt:ALL4 (match_dup 1)\n+                                  (match_dup 2)))\n+              (clobber (match_dup 3))])])\n+\n+;; \"*lshrsi3_const\"\n+;; \"*lshrsq3_const\"  \"*lshrusq3_const\"\n+;; \"*lshrsa3_const\"  \"*lshrusa3_const\"\n+(define_insn \"*lshr<mode>3_const\"\n+  [(set (match_operand:ALL4 0 \"register_operand\"               \"=r,r,r,r\")\n+        (lshiftrt:ALL4 (match_operand:ALL4 1 \"register_operand\" \"0,0,r,0\")\n+                       (match_operand:QI 2 \"const_int_operand\"  \"L,P,O,n\")))\n+   (clobber (match_scratch:QI 3                                \"=X,X,X,&d\"))]\n   \"reload_completed\"\n   {\n     return lshrsi3_out (insn, operands, NULL);\n@@ -4278,24 +4435,29 @@\n   [(set_attr \"cc\" \"compare\")\n    (set_attr \"length\" \"4\")])\n \n-(define_insn \"*reversed_tstsi\"\n+;; \"*reversed_tstsi\"\n+;; \"*reversed_tstsq\" \"*reversed_tstusq\"\n+;; \"*reversed_tstsa\" \"*reversed_tstusa\"\n+(define_insn \"*reversed_tst<mode>\"\n   [(set (cc0)\n-        (compare (const_int 0)\n-                 (match_operand:SI 0 \"register_operand\" \"r\")))\n-   (clobber (match_scratch:QI 1 \"=X\"))]\n-  \"\"\n-  \"cp __zero_reg__,%A0\n-\tcpc __zero_reg__,%B0\n-\tcpc __zero_reg__,%C0\n-\tcpc __zero_reg__,%D0\"\n+        (compare (match_operand:ALL4 0 \"const0_operand\"   \"Y00\")\n+                 (match_operand:ALL4 1 \"register_operand\" \"r\")))\n+   (clobber (match_scratch:QI 2 \"=X\"))]\n+  \"\"\n+  \"cp __zero_reg__,%A1\n+\tcpc __zero_reg__,%B1\n+\tcpc __zero_reg__,%C1\n+\tcpc __zero_reg__,%D1\"\n   [(set_attr \"cc\" \"compare\")\n    (set_attr \"length\" \"4\")])\n \n \n-(define_insn \"*cmpqi\"\n+;; \"*cmpqi\"\n+;; \"*cmpqq\" \"*cmpuqq\"\n+(define_insn \"*cmp<mode>\"\n   [(set (cc0)\n-        (compare (match_operand:QI 0 \"register_operand\"  \"r,r,d\")\n-                 (match_operand:QI 1 \"nonmemory_operand\" \"L,r,i\")))]\n+        (compare (match_operand:ALL1 0 \"register_operand\"  \"r  ,r,d\")\n+                 (match_operand:ALL1 1 \"nonmemory_operand\" \"Y00,r,i\")))]\n   \"\"\n   \"@\n \ttst %0\n@@ -4313,11 +4475,14 @@\n   [(set_attr \"cc\" \"compare\")\n    (set_attr \"length\" \"1\")])\n \n-(define_insn \"*cmphi\"\n+;; \"*cmphi\"\n+;; \"*cmphq\" \"*cmpuhq\"\n+;; \"*cmpha\" \"*cmpuha\"\n+(define_insn \"*cmp<mode>\"\n   [(set (cc0)\n-        (compare (match_operand:HI 0 \"register_operand\"  \"!w,r,r,d ,r  ,d,r\")\n-                 (match_operand:HI 1 \"nonmemory_operand\" \"L ,L,r,s ,s  ,M,n\")))\n-   (clobber (match_scratch:QI 2                         \"=X ,X,X,&d,&d ,X,&d\"))]\n+        (compare (match_operand:ALL2 0 \"register_operand\"  \"!w  ,r  ,r,d ,r  ,d,r\")\n+                 (match_operand:ALL2 1 \"nonmemory_operand\"  \"Y00,Y00,r,s ,s  ,M,n Ynn\")))\n+   (clobber (match_scratch:QI 2                            \"=X  ,X  ,X,&d,&d ,X,&d\"))]\n   \"\"\n   {\n     switch (which_alternative)\n@@ -4330,11 +4495,15 @@\n         return \"cp %A0,%A1\\;cpc %B0,%B1\";\n \n       case 3:\n+        if (<MODE>mode != HImode)\n+          break;\n         return reg_unused_after (insn, operands[0])\n                ? \"subi %A0,lo8(%1)\\;sbci %B0,hi8(%1)\"\n                : \"ldi %2,hi8(%1)\\;cpi %A0,lo8(%1)\\;cpc %B0,%2\";\n                \n       case 4:\n+        if (<MODE>mode != HImode)\n+          break;\n         return \"ldi %2,lo8(%1)\\;cp %A0,%2\\;ldi %2,hi8(%1)\\;cpc %B0,%2\";\n       }\n       \n@@ -4374,11 +4543,14 @@\n    (set_attr \"length\" \"3,3,5,6,3,7\")\n    (set_attr \"adjust_len\" \"tstpsi,*,*,*,compare,compare\")])\n \n-(define_insn \"*cmpsi\"\n+;; \"*cmpsi\"\n+;; \"*cmpsq\" \"*cmpusq\"\n+;; \"*cmpsa\" \"*cmpusa\"\n+(define_insn \"*cmp<mode>\"\n   [(set (cc0)\n-        (compare (match_operand:SI 0 \"register_operand\"  \"r,r ,d,r ,r\")\n-                 (match_operand:SI 1 \"nonmemory_operand\" \"L,r ,M,M ,n\")))\n-   (clobber (match_scratch:QI 2                         \"=X,X ,X,&d,&d\"))]\n+        (compare (match_operand:ALL4 0 \"register_operand\"  \"r  ,r ,d,r ,r\")\n+                 (match_operand:ALL4 1 \"nonmemory_operand\" \"Y00,r ,M,M ,n Ynn\")))\n+   (clobber (match_scratch:QI 2                           \"=X  ,X ,X,&d,&d\"))]\n   \"\"\n   {\n     if (0 == which_alternative)\n@@ -4398,55 +4570,33 @@\n ;; ----------------------------------------------------------------------\n ;; Conditional jump instructions\n \n-(define_expand \"cbranchsi4\"\n-  [(parallel [(set (cc0)\n-                   (compare (match_operand:SI 1 \"register_operand\" \"\")\n-                            (match_operand:SI 2 \"nonmemory_operand\" \"\")))\n-              (clobber (match_scratch:QI 4 \"\"))])\n+;; \"cbranchqi4\"\n+;; \"cbranchqq4\"  \"cbranchuqq4\"\n+(define_expand \"cbranch<mode>4\"\n+  [(set (cc0)\n+        (compare (match_operand:ALL1 1 \"register_operand\" \"\")\n+                 (match_operand:ALL1 2 \"nonmemory_operand\" \"\")))\n    (set (pc)\n         (if_then_else\n-              (match_operator 0 \"ordered_comparison_operator\" [(cc0)\n-                                                               (const_int 0)])\n-              (label_ref (match_operand 3 \"\" \"\"))\n-              (pc)))]\n- \"\")\n-\n-(define_expand \"cbranchpsi4\"\n-  [(parallel [(set (cc0)\n-                   (compare (match_operand:PSI 1 \"register_operand\" \"\")\n-                            (match_operand:PSI 2 \"nonmemory_operand\" \"\")))\n-              (clobber (match_scratch:QI 4 \"\"))])\n-   (set (pc)\n-        (if_then_else (match_operator 0 \"ordered_comparison_operator\" [(cc0)\n-                                                                       (const_int 0)])\n-                      (label_ref (match_operand 3 \"\" \"\"))\n-                      (pc)))]\n- \"\")\n+         (match_operator 0 \"ordered_comparison_operator\" [(cc0)\n+                                                          (const_int 0)])\n+         (label_ref (match_operand 3 \"\" \"\"))\n+         (pc)))])\n \n-(define_expand \"cbranchhi4\"\n+;; \"cbranchhi4\"  \"cbranchhq4\"  \"cbranchuhq4\"  \"cbranchha4\"  \"cbranchuha4\"\n+;; \"cbranchsi4\"  \"cbranchsq4\"  \"cbranchusq4\"  \"cbranchsa4\"  \"cbranchusa4\"\n+;; \"cbranchpsi4\"\n+(define_expand \"cbranch<mode>4\"\n   [(parallel [(set (cc0)\n-                   (compare (match_operand:HI 1 \"register_operand\" \"\")\n-                            (match_operand:HI 2 \"nonmemory_operand\" \"\")))\n+                   (compare (match_operand:ORDERED234 1 \"register_operand\" \"\")\n+                            (match_operand:ORDERED234 2 \"nonmemory_operand\" \"\")))\n               (clobber (match_scratch:QI 4 \"\"))])\n    (set (pc)\n         (if_then_else\n-              (match_operator 0 \"ordered_comparison_operator\" [(cc0)\n-                                                               (const_int 0)])\n-              (label_ref (match_operand 3 \"\" \"\"))\n-              (pc)))]\n- \"\")\n-\n-(define_expand \"cbranchqi4\"\n-  [(set (cc0)\n-        (compare (match_operand:QI 1 \"register_operand\" \"\")\n-                 (match_operand:QI 2 \"nonmemory_operand\" \"\")))\n-   (set (pc)\n-        (if_then_else\n-              (match_operator 0 \"ordered_comparison_operator\" [(cc0)\n-                                                               (const_int 0)])\n-              (label_ref (match_operand 3 \"\" \"\"))\n-              (pc)))]\n- \"\")\n+         (match_operator 0 \"ordered_comparison_operator\" [(cc0)\n+                                                          (const_int 0)])\n+         (label_ref (match_operand 3 \"\" \"\"))\n+         (pc)))])\n \n \n ;; Test a single bit in a QI/HI/SImode register.\n@@ -4477,7 +4627,7 @@\n                                     (const_int 4))))\n    (set_attr \"cc\" \"clobber\")])\n \n-;; Same test based on Bitwise AND RTL. Keep this incase gcc changes patterns.\n+;; Same test based on bitwise AND.  Keep this in case gcc changes patterns.\n ;; or for old peepholes.\n ;; Fixme - bitwise Mask will not work for DImode\n \n@@ -4492,12 +4642,12 @@\n          (label_ref (match_operand 3 \"\" \"\"))\n          (pc)))]\n   \"\"\n-{\n+  {\n     HOST_WIDE_INT bitnumber;\n     bitnumber = exact_log2 (GET_MODE_MASK (<MODE>mode) & INTVAL (operands[2]));\n     operands[2] = GEN_INT (bitnumber);\n     return avr_out_sbxx_branch (insn, operands);\n-}\n+  }\n   [(set (attr \"length\")\n         (if_then_else (and (ge (minus (pc) (match_dup 3)) (const_int -2046))\n                            (le (minus (pc) (match_dup 3)) (const_int 2046)))\n@@ -4837,9 +4987,10 @@\n \n \n (define_expand \"casesi\"\n-  [(set (match_dup 6)\n-        (minus:HI (subreg:HI (match_operand:SI 0 \"register_operand\" \"\") 0)\n-                  (match_operand:HI 1 \"register_operand\" \"\")))\n+  [(parallel [(set (match_dup 6)\n+                   (minus:HI (subreg:HI (match_operand:SI 0 \"register_operand\" \"\") 0)\n+                             (match_operand:HI 1 \"register_operand\" \"\")))\n+              (clobber (scratch:QI))])\n    (parallel [(set (cc0)\n                    (compare (match_dup 6)\n                             (match_operand:HI 2 \"register_operand\" \"\")))\n@@ -5201,8 +5352,8 @@\n \n (define_peephole ; \"*cpse.eq\"\n   [(set (cc0)\n-        (compare (match_operand:QI 1 \"register_operand\" \"r,r\")\n-                 (match_operand:QI 2 \"reg_or_0_operand\" \"r,L\")))\n+        (compare (match_operand:ALL1 1 \"register_operand\" \"r,r\")\n+                 (match_operand:ALL1 2 \"reg_or_0_operand\" \"r,Y00\")))\n    (set (pc)\n         (if_then_else (eq (cc0)\n                           (const_int 0))\n@@ -5236,8 +5387,8 @@\n \n (define_peephole ; \"*cpse.ne\"\n   [(set (cc0)\n-        (compare (match_operand:QI 1 \"register_operand\" \"\")\n-                 (match_operand:QI 2 \"reg_or_0_operand\" \"\")))\n+        (compare (match_operand:ALL1 1 \"register_operand\" \"\")\n+                 (match_operand:ALL1 2 \"reg_or_0_operand\" \"\")))\n    (set (pc)\n         (if_then_else (ne (cc0)\n                           (const_int 0))\n@@ -5246,7 +5397,7 @@\n   \"!AVR_HAVE_JMP_CALL\n    || !avr_current_device->errata_skip\"\n   {\n-    if (operands[2] == const0_rtx)\n+    if (operands[2] == CONST0_RTX (<MODE>mode))\n       operands[2] = zero_reg_rtx;\n \n     return 3 == avr_jump_mode (operands[0], insn)\n@@ -6265,4 +6416,8 @@\n   })\n \n \f\n+;; Fixed-point instructions\n+(include \"avr-fixed.md\")\n+\n+;; Operations on 64-bit registers\n (include \"avr-dimode.md\")"}, {"sha": "5a1c9f1aef128bf5b03df66e52e459c34ba15e75", "filename": "gcc/config/avr/constraints.md", "status": "modified", "additions": 44, "deletions": 0, "changes": 44, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Fconstraints.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Fconstraints.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Fconstraints.md?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -192,3 +192,47 @@\n   \"32-bit integer constant where no nibble equals 0xf.\"\n   (and (match_code \"const_int\")\n        (match_test \"!avr_has_nibble_0xf (op)\")))\n+\n+;; CONST_FIXED is no element of 'n' so cook our own.\n+;; \"i\" or \"s\" would match but because the insn uses iterators that cover\n+;; INT_MODE, \"i\" or \"s\" is not always possible.\n+\n+(define_constraint \"Ynn\"\n+  \"Fixed-point constant known at compile time.\"\n+  (match_code \"const_fixed\"))\n+\n+(define_constraint \"Y00\"\n+  \"Fixed-point or integer constant with bit representation 0x0\"\n+  (and (match_code \"const_fixed,const_int\")\n+       (match_test \"op == CONST0_RTX (GET_MODE (op))\")))\n+\n+(define_constraint \"Y01\"\n+  \"Fixed-point or integer constant with bit representation 0x1\"\n+  (ior (and (match_code \"const_fixed\")\n+            (match_test \"1 == INTVAL (avr_to_int_mode (op))\"))\n+       (match_test \"satisfies_constraint_P (op)\")))\n+\n+(define_constraint \"Ym1\"\n+  \"Fixed-point or integer constant with bit representation -0x1\"\n+  (ior (and (match_code \"const_fixed\")\n+            (match_test \"-1 == INTVAL (avr_to_int_mode (op))\"))\n+       (match_test \"satisfies_constraint_N (op)\")))\n+\n+(define_constraint \"Y02\"\n+  \"Fixed-point or integer constant with bit representation 0x2\"\n+  (ior (and (match_code \"const_fixed\")\n+            (match_test \"2 == INTVAL (avr_to_int_mode (op))\"))\n+       (match_test \"satisfies_constraint_K (op)\")))\n+\n+(define_constraint \"Ym2\"\n+  \"Fixed-point or integer constant with bit representation -0x2\"\n+  (ior (and (match_code \"const_fixed\")\n+            (match_test \"-2 == INTVAL (avr_to_int_mode (op))\"))\n+       (match_test \"satisfies_constraint_Cm2 (op)\")))\n+\n+;; Similar to \"IJ\" used with ADIW/SBIW, but for CONST_FIXED.\n+\n+(define_constraint \"YIJ\"\n+  \"Fixed-point constant from @minus{}0x003f to 0x003f.\"\n+  (and (match_code \"const_fixed\")\n+       (match_test \"IN_RANGE (INTVAL (avr_to_int_mode (op)), -63, 63)\")))"}, {"sha": "04587ae491f2528f17055e21b2ac2a6a41670cf2", "filename": "gcc/config/avr/predicates.md", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/gcc%2Fconfig%2Favr%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Fpredicates.md?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -74,7 +74,7 @@\n \n ;; Return 1 if OP is the zero constant for MODE.\n (define_predicate \"const0_operand\"\n-  (and (match_code \"const_int,const_double\")\n+  (and (match_code \"const_int,const_fixed,const_double\")\n        (match_test \"op == CONST0_RTX (mode)\")))\n \n ;; Return 1 if OP is the one constant integer for MODE.\n@@ -248,3 +248,21 @@\n (define_predicate \"o16_operand\"\n   (and (match_code \"const_int\")\n        (match_test \"IN_RANGE (INTVAL (op), -(1<<16), -1)\")))\n+\n+;; Const int, fixed, or double operand\n+(define_predicate \"const_operand\"\n+  (ior (match_code \"const_fixed\")\n+       (match_code \"const_double\")\n+       (match_operand 0 \"const_int_operand\")))\n+\n+;; Const int, const fixed, or const double operand\n+(define_predicate \"nonmemory_or_const_operand\"\n+  (ior (match_code \"const_fixed\")\n+       (match_code \"const_double\")\n+       (match_operand 0 \"nonmemory_operand\")))\n+\n+;; Immediate, const fixed, or const double operand\n+(define_predicate \"const_or_immediate_operand\"\n+  (ior (match_code \"const_fixed\")\n+       (match_code \"const_double\")\n+       (match_operand 0 \"immediate_operand\")))"}, {"sha": "60f19491d761597f8f09412fbd3ed26e0a62ec9b", "filename": "libgcc/ChangeLog", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/libgcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/libgcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgcc%2FChangeLog?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -1,3 +1,23 @@\n+2012-08-24  Georg-Johann Lay  <avr@gjlay.de>\n+\n+\tPR target/54222\n+\t* config/avr/lib1funcs-fixed.S: New file.\n+\t* config/avr/lib1funcs.S: Include it.  Undefine some divmodsi\n+\tafter they are used.\n+\t(neg2, neg4): New macros.\n+\t(__mulqihi3,__umulqihi3,__mulhi3): Rewrite non-MUL variants.\n+\t(__mulhisi3,__umulhisi3,__mulsi3): Rewrite non-MUL variants.\n+\t(__umulhisi3): Speed up MUL variant if there is enough flash.\n+\t* config/avr/avr-lib.h (TA, UTA): Adjust according to gcc's\n+\tavr-modes.def.\n+\t* config/avr/t-avr (LIB1ASMFUNCS): Add: _fractqqsf, _fractuqqsf,\n+\t_fracthqsf, _fractuhqsf, _fracthasf, _fractuhasf, _fractsasf,\n+\t_fractusasf, _fractsfqq, _fractsfuqq, _fractsfhq, _fractsfuhq,\n+\t_fractsfha, _fractsfsa, _mulqq3, _muluqq3, _mulhq3, _muluhq3,\n+\t_mulha3, _muluha3, _mulsa3, _mulusa3, _divqq3, _udivuqq3, _divhq3,\n+\t_udivuhq3, _divha3, _udivuha3, _divsa3, _udivusa3.\n+\t(LIB2FUNCS_EXCLUDE): Add supported functions.\n+\n 2012-08-22  Georg-Johann Lay  <avr@gjlay.de>\n \n \t* Makefile.in (fixed-funcs,fixed-conv-funcs): filter-out"}, {"sha": "66082eb8a486fed62ee08c1d74c6c126d29b6721", "filename": "libgcc/config/avr/avr-lib.h", "status": "modified", "additions": 76, "deletions": 0, "changes": 76, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/libgcc%2Fconfig%2Favr%2Favr-lib.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/libgcc%2Fconfig%2Favr%2Favr-lib.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgcc%2Fconfig%2Favr%2Favr-lib.h?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -4,3 +4,79 @@\n #define DI SI\n typedef int QItype __attribute__ ((mode (QI)));\n #endif\n+\n+/* fixed-bit.h does not define functions for TA and UTA because\n+   that part is wrapped in #if MIN_UNITS_PER_WORD > 4.\n+   This would lead to empty functions for TA and UTA.\n+   Thus, supply appropriate defines as if HAVE_[U]TA == 1.\n+   #define HAVE_[U]TA 1 won't work because avr-modes.def\n+   uses ADJUST_BYTESIZE(TA,8) and fixed-bit.h is not generic enough\n+   to arrange for such changes of the mode size.  */\n+\n+typedef unsigned _Fract UTAtype __attribute__ ((mode (UTA)));\n+\n+#if defined (UTA_MODE)\n+#define FIXED_SIZE      8       /* in bytes */\n+#define INT_C_TYPE      UDItype\n+#define UINT_C_TYPE     UDItype\n+#define HINT_C_TYPE     USItype\n+#define HUINT_C_TYPE    USItype\n+#define MODE_NAME       UTA\n+#define MODE_NAME_S     uta\n+#define MODE_UNSIGNED   1\n+#endif\n+\n+#if defined (FROM_UTA)\n+#define FROM_TYPE               4       /* ID for fixed-point */\n+#define FROM_MODE_NAME          UTA\n+#define FROM_MODE_NAME_S        uta\n+#define FROM_INT_C_TYPE         UDItype\n+#define FROM_SINT_C_TYPE        DItype\n+#define FROM_UINT_C_TYPE        UDItype\n+#define FROM_MODE_UNSIGNED      1\n+#define FROM_FIXED_SIZE         8       /* in bytes */\n+#elif defined (TO_UTA)\n+#define TO_TYPE                 4       /* ID for fixed-point */\n+#define TO_MODE_NAME            UTA\n+#define TO_MODE_NAME_S          uta\n+#define TO_INT_C_TYPE           UDItype\n+#define TO_SINT_C_TYPE          DItype\n+#define TO_UINT_C_TYPE          UDItype\n+#define TO_MODE_UNSIGNED        1\n+#define TO_FIXED_SIZE           8       /* in bytes */\n+#endif\n+\n+/* Same for TAmode */\n+\n+typedef _Fract TAtype  __attribute__ ((mode (TA)));\n+\n+#if defined (TA_MODE)\n+#define FIXED_SIZE      8       /* in bytes */\n+#define INT_C_TYPE      DItype\n+#define UINT_C_TYPE     UDItype\n+#define HINT_C_TYPE     SItype\n+#define HUINT_C_TYPE    USItype\n+#define MODE_NAME       TA\n+#define MODE_NAME_S     ta\n+#define MODE_UNSIGNED   0\n+#endif\n+\n+#if defined (FROM_TA)\n+#define FROM_TYPE               4       /* ID for fixed-point */\n+#define FROM_MODE_NAME          TA\n+#define FROM_MODE_NAME_S        ta\n+#define FROM_INT_C_TYPE         DItype\n+#define FROM_SINT_C_TYPE        DItype\n+#define FROM_UINT_C_TYPE        UDItype\n+#define FROM_MODE_UNSIGNED      0\n+#define FROM_FIXED_SIZE         8       /* in bytes */\n+#elif defined (TO_TA)\n+#define TO_TYPE                 4       /* ID for fixed-point */\n+#define TO_MODE_NAME            TA\n+#define TO_MODE_NAME_S          ta\n+#define TO_INT_C_TYPE           DItype\n+#define TO_SINT_C_TYPE          DItype\n+#define TO_UINT_C_TYPE          UDItype\n+#define TO_MODE_UNSIGNED        0\n+#define TO_FIXED_SIZE           8       /* in bytes */\n+#endif"}, {"sha": "c1aff53d5fd02117d6e829479ccebb846838fbbc", "filename": "libgcc/config/avr/lib1funcs-fixed.S", "status": "added", "additions": 874, "deletions": 0, "changes": 874, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/libgcc%2Fconfig%2Favr%2Flib1funcs-fixed.S", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/libgcc%2Fconfig%2Favr%2Flib1funcs-fixed.S", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgcc%2Fconfig%2Favr%2Flib1funcs-fixed.S?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -0,0 +1,874 @@\n+/*  -*- Mode: Asm -*-  */\n+;;    Copyright (C) 2012\n+;;    Free Software Foundation, Inc.\n+;;    Contributed by Sean D'Epagnier  (sean@depagnier.com)\n+;;                   Georg-Johann Lay (avr@gjlay.de)\n+\n+;; This file is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by the\n+;; Free Software Foundation; either version 3, or (at your option) any\n+;; later version.\n+\n+;; In addition to the permissions in the GNU General Public License, the\n+;; Free Software Foundation gives you unlimited permission to link the\n+;; compiled version of this file into combinations with other programs,\n+;; and to distribute those combinations without any restriction coming\n+;; from the use of this file.  (The General Public License restrictions\n+;; do apply in other respects; for example, they cover modification of\n+;; the file, and distribution when not linked into a combine\n+;; executable.)\n+\n+;; This file is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+\n+;; You should have received a copy of the GNU General Public License\n+;; along with this program; see the file COPYING.  If not, write to\n+;; the Free Software Foundation, 51 Franklin Street, Fifth Floor,\n+;; Boston, MA 02110-1301, USA.\n+\n+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n+;; Fixed point library routines for AVR\n+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n+\n+.section .text.libgcc.fixed, \"ax\", @progbits\n+\n+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n+;; Conversions to float\n+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n+\n+#if defined (L_fractqqsf)\n+DEFUN __fractqqsf\n+    ;; Move in place for SA -> SF conversion\n+    clr     r22\n+    mov     r23, r24\n+    lsl     r23\n+    ;; Sign-extend\n+    sbc     r24, r24\n+    mov     r25, r24\n+    XJMP    __fractsasf\n+ENDF __fractqqsf\n+#endif  /* L_fractqqsf */\n+\n+#if defined (L_fractuqqsf)\n+DEFUN __fractuqqsf\n+    ;; Move in place for USA -> SF conversion\n+    clr     r22\n+    mov     r23, r24\n+    ;; Zero-extend\n+    clr     r24\n+    clr     r25\n+    XJMP    __fractusasf\n+ENDF __fractuqqsf\n+#endif  /* L_fractuqqsf */\n+\n+#if defined (L_fracthqsf)\n+DEFUN __fracthqsf\n+    ;; Move in place for SA -> SF conversion\n+    wmov    22, 24\n+    lsl     r22\n+    rol     r23\n+    ;; Sign-extend\n+    sbc     r24, r24\n+    mov     r25, r24\n+    XJMP    __fractsasf\n+ENDF __fracthqsf\n+#endif  /* L_fracthqsf */\n+\n+#if defined (L_fractuhqsf)\n+DEFUN __fractuhqsf\n+    ;; Move in place for USA -> SF conversion\n+    wmov    22, 24\n+    ;; Zero-extend\n+    clr     r24\n+    clr     r25\n+    XJMP    __fractusasf\n+ENDF __fractuhqsf\n+#endif  /* L_fractuhqsf */\n+\n+#if defined (L_fracthasf)\n+DEFUN __fracthasf\n+    ;; Move in place for SA -> SF conversion\n+    clr     r22\n+    mov     r23, r24\n+    mov     r24, r25\n+    ;; Sign-extend\n+    lsl     r25\n+    sbc     r25, r25\n+    XJMP    __fractsasf\n+ENDF __fracthasf\n+#endif  /* L_fracthasf */\n+\n+#if defined (L_fractuhasf)\n+DEFUN __fractuhasf\n+    ;; Move in place for USA -> SF conversion\n+    clr     r22\n+    mov     r23, r24\n+    mov     r24, r25\n+    ;; Zero-extend\n+    clr     r25\n+    XJMP    __fractusasf\n+ENDF __fractuhasf\n+#endif  /* L_fractuhasf */\n+\n+\n+#if defined (L_fractsqsf)\n+DEFUN __fractsqsf\n+    XCALL   __floatsisf\n+    ;; Divide non-zero results by 2^31 to move the\n+    ;; decimal point into place\n+    tst     r25\n+    breq    0f\n+    subi    r24, exp_lo (31)\n+    sbci    r25, exp_hi (31)\n+0:  ret\n+ENDF __fractsqsf\n+#endif  /* L_fractsqsf */\n+\n+#if defined (L_fractusqsf)\n+DEFUN __fractusqsf\n+    XCALL   __floatunsisf\n+    ;; Divide non-zero results by 2^32 to move the\n+    ;; decimal point into place\n+    cpse    r25, __zero_reg__\n+    subi    r25, exp_hi (32)\n+    ret\n+ENDF __fractusqsf\n+#endif  /* L_fractusqsf */\n+\n+#if defined (L_fractsasf)\n+DEFUN __fractsasf\n+    XCALL   __floatsisf\n+    ;; Divide non-zero results by 2^16 to move the\n+    ;; decimal point into place\n+    cpse    r25, __zero_reg__\n+    subi    r25, exp_hi (16)\n+    ret\n+ENDF __fractsasf\n+#endif  /* L_fractsasf */\n+\n+#if defined (L_fractusasf)\n+DEFUN __fractusasf\n+    XCALL   __floatunsisf\n+    ;; Divide non-zero results by 2^16 to move the\n+    ;; decimal point into place\n+    cpse    r25, __zero_reg__\n+    subi    r25, exp_hi (16)\n+    ret\n+ENDF __fractusasf\n+#endif  /* L_fractusasf */\n+\n+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n+;; Conversions from float\n+;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n+       \n+#if defined (L_fractsfqq)\n+DEFUN __fractsfqq\n+    ;; Multiply with 2^{24+7} to get a QQ result in r25\n+    subi    r24, exp_lo (-31)\n+    sbci    r25, exp_hi (-31)\n+    XCALL   __fixsfsi\n+    mov     r24, r25\n+    ret\n+ENDF __fractsfqq\n+#endif  /* L_fractsfqq */\n+\n+#if defined (L_fractsfuqq)\n+DEFUN __fractsfuqq\n+    ;; Multiply with 2^{24+8} to get a UQQ result in r25\n+    subi    r25, exp_hi (-32)\n+    XCALL   __fixunssfsi\n+    mov     r24, r25\n+    ret\n+ENDF __fractsfuqq\n+#endif  /* L_fractsfuqq */\n+\n+#if defined (L_fractsfha)\n+DEFUN __fractsfha\n+    ;; Multiply with 2^24 to get a HA result in r25:r24\n+    subi    r25, exp_hi (-24)\n+    XJMP    __fixsfsi\n+ENDF __fractsfha\n+#endif  /* L_fractsfha */\n+\n+#if defined (L_fractsfuha)\n+DEFUN __fractsfuha\n+    ;; Multiply with 2^24 to get a UHA result in r25:r24\n+    subi    r25, exp_hi (-24)\n+    XJMP    __fixunssfsi\n+ENDF __fractsfuha\n+#endif  /* L_fractsfuha */\n+\n+#if defined (L_fractsfhq)\n+DEFUN __fractsfsq\n+ENDF  __fractsfsq\n+\n+DEFUN __fractsfhq\n+    ;; Multiply with 2^{16+15} to get a HQ result in r25:r24\n+    ;; resp. with 2^31 to get a SQ result in r25:r22\n+    subi    r24, exp_lo (-31)\n+    sbci    r25, exp_hi (-31)\n+    XJMP    __fixsfsi\n+ENDF __fractsfhq\n+#endif  /* L_fractsfhq */\n+\n+#if defined (L_fractsfuhq)\n+DEFUN __fractsfusq\n+ENDF  __fractsfusq\n+\n+DEFUN __fractsfuhq\n+    ;; Multiply with 2^{16+16} to get a UHQ result in r25:r24\n+    ;; resp. with 2^32 to get a USQ result in r25:r22\n+    subi    r25, exp_hi (-32)\n+    XJMP    __fixunssfsi\n+ENDF __fractsfuhq\n+#endif  /* L_fractsfuhq */\n+\n+#if defined (L_fractsfsa)\n+DEFUN __fractsfsa\n+    ;; Multiply with 2^16 to get a SA result in r25:r22\n+    subi    r25, exp_hi (-16)\n+    XJMP    __fixsfsi\n+ENDF __fractsfsa\n+#endif  /* L_fractsfsa */\n+\n+#if defined (L_fractsfusa)\n+DEFUN __fractsfusa\n+    ;; Multiply with 2^16 to get a USA result in r25:r22\n+    subi    r25, exp_hi (-16)\n+    XJMP    __fixunssfsi\n+ENDF __fractsfusa\n+#endif  /* L_fractsfusa */\n+\n+\n+;; For multiplication the functions here are called directly from\n+;; avr-fixed.md instead of using the standard libcall mechanisms.\n+;; This can make better code because GCC knows exactly which\n+;; of the call-used registers (not all of them) are clobbered.  */\n+\n+/*******************************************************\n+    Fractional  Multiplication  8 x 8  without MUL\n+*******************************************************/\n+\n+#if defined (L_mulqq3) && !defined (__AVR_HAVE_MUL__)\n+;;; R23 = R24 * R25\n+;;; Clobbers: __tmp_reg__, R22, R24, R25\n+;;; Rounding: ???\n+DEFUN __mulqq3\n+    XCALL   __fmuls\n+    ;; TR 18037 requires that  (-1) * (-1)  does not overflow\n+    ;; The only input that can produce  -1  is  (-1)^2.\n+    dec     r23\n+    brvs    0f\n+    inc     r23\n+0:  ret\n+ENDF  __mulqq3\n+#endif /* L_mulqq3 && ! HAVE_MUL */\n+\n+/*******************************************************\n+    Fractional Multiply  .16 x .16  with and without MUL\n+*******************************************************/\n+\n+#if defined (L_mulhq3)\n+;;; Same code with and without MUL, but the interfaces differ:\n+;;; no MUL: (R25:R24) = (R22:R23) * (R24:R25)\n+;;;         Clobbers: ABI, called by optabs\n+;;; MUL:    (R25:R24) = (R19:R18) * (R27:R26)\n+;;;         Clobbers: __tmp_reg__, R22, R23\n+;;; Rounding:  -0.5 LSB  <= error  <=  0.5 LSB\n+DEFUN   __mulhq3\n+    XCALL   __mulhisi3\n+    ;; Shift result into place\n+    lsl     r23\n+    rol     r24\n+    rol     r25\n+    brvs    1f\n+    ;; Round\n+    sbrc    r23, 7\n+    adiw    r24, 1\n+    ret\n+1:  ;; Overflow.  TR 18037 requires  (-1)^2  not to overflow\n+    ldi     r24, lo8 (0x7fff)\n+    ldi     r25, hi8 (0x7fff)\n+    ret\n+ENDF __mulhq3\n+#endif  /* defined (L_mulhq3) */\n+\n+#if defined (L_muluhq3)\n+;;; Same code with and without MUL, but the interfaces differ:\n+;;; no MUL: (R25:R24) *= (R23:R22)\n+;;;         Clobbers: ABI, called by optabs\n+;;; MUL:    (R25:R24) = (R19:R18) * (R27:R26)\n+;;;         Clobbers: __tmp_reg__, R22, R23\n+;;; Rounding:  -0.5 LSB  <  error  <=  0.5 LSB\n+DEFUN   __muluhq3\n+    XCALL   __umulhisi3\n+    ;; Round\n+    sbrc    r23, 7\n+    adiw    r24, 1\n+    ret\n+ENDF __muluhq3\n+#endif  /* L_muluhq3 */\n+\n+\n+/*******************************************************\n+    Fixed  Multiply  8.8 x 8.8  with and without MUL\n+*******************************************************/\n+\n+#if defined (L_mulha3)\n+;;; Same code with and without MUL, but the interfaces differ:\n+;;; no MUL: (R25:R24) = (R22:R23) * (R24:R25)\n+;;;         Clobbers: ABI, called by optabs\n+;;; MUL:    (R25:R24) = (R19:R18) * (R27:R26)\n+;;;         Clobbers: __tmp_reg__, R22, R23\n+;;; Rounding:  -0.5 LSB  <=  error  <=  0.5 LSB\n+DEFUN   __mulha3\n+    XCALL   __mulhisi3\n+    XJMP    __muluha3_round\n+ENDF __mulha3\n+#endif  /* L_mulha3 */\n+\n+#if defined (L_muluha3)\n+;;; Same code with and without MUL, but the interfaces differ:\n+;;; no MUL: (R25:R24) *= (R23:R22)\n+;;;         Clobbers: ABI, called by optabs\n+;;; MUL:    (R25:R24) = (R19:R18) * (R27:R26)\n+;;;         Clobbers: __tmp_reg__, R22, R23\n+;;; Rounding:  -0.5 LSB  <  error  <=  0.5 LSB\n+DEFUN   __muluha3\n+    XCALL   __umulhisi3\n+    XJMP    __muluha3_round\n+ENDF __muluha3\n+#endif  /* L_muluha3 */\n+\n+#if defined (L_muluha3_round)\n+DEFUN   __muluha3_round\n+    ;; Shift result into place\n+    mov     r25, r24\n+    mov     r24, r23\n+    ;; Round\n+    sbrc    r22, 7\n+    adiw    r24, 1\n+    ret\n+ENDF __muluha3_round\n+#endif  /* L_muluha3_round */\n+\n+\n+/*******************************************************\n+    Fixed  Multiplication  16.16 x 16.16\n+*******************************************************/\n+\n+#if defined (__AVR_HAVE_MUL__)\n+\n+;; Multiplier\n+#define A0  16\n+#define A1  A0+1\n+#define A2  A1+1\n+#define A3  A2+1\n+\n+;; Multiplicand\n+#define B0  20\n+#define B1  B0+1\n+#define B2  B1+1\n+#define B3  B2+1\n+\n+;; Result\n+#define C0  24\n+#define C1  C0+1\n+#define C2  C1+1\n+#define C3  C2+1\n+\n+#if defined (L_mulusa3)\n+;;; (C3:C0) = (A3:A0) * (B3:B0)\n+;;; Clobbers: __tmp_reg__\n+;;; Rounding:  -0.5 LSB  <  error  <=  0.5 LSB\n+DEFUN   __mulusa3\n+    ;; Some of the MUL instructions have LSBs outside the result.\n+    ;; Don't ignore these LSBs in order to tame rounding error.\n+    ;; Use C2/C3 for these LSBs.\n+\n+    clr C0\n+    clr C1\n+    mul A0, B0  $  movw C2, r0\n+\n+    mul A1, B0  $  add  C3, r0  $  adc C0, r1\n+    mul A0, B1  $  add  C3, r0  $  adc C0, r1  $  rol C1\n+    \n+    ;; Round\n+    sbrc C3, 7\n+    adiw C0, 1\n+    \n+    ;; The following MULs don't have LSBs outside the result.\n+    ;; C2/C3 is the high part.\n+\n+    mul  A0, B2  $  add C0, r0  $  adc C1, r1  $  sbc  C2, C2\n+    mul  A1, B1  $  add C0, r0  $  adc C1, r1  $  sbci C2, 0\n+    mul  A2, B0  $  add C0, r0  $  adc C1, r1  $  sbci C2, 0\n+    neg  C2\n+\n+    mul  A0, B3  $  add C1, r0  $  adc C2, r1  $  sbc  C3, C3\n+    mul  A1, B2  $  add C1, r0  $  adc C2, r1  $  sbci C3, 0\n+    mul  A2, B1  $  add C1, r0  $  adc C2, r1  $  sbci C3, 0\n+    mul  A3, B0  $  add C1, r0  $  adc C2, r1  $  sbci C3, 0\n+    neg  C3\n+    \n+    mul  A1, B3  $  add C2, r0  $  adc C3, r1\n+    mul  A2, B2  $  add C2, r0  $  adc C3, r1\n+    mul  A3, B1  $  add C2, r0  $  adc C3, r1\n+    \n+    mul  A2, B3  $  add C3, r0\n+    mul  A3, B2  $  add C3, r0\n+\n+    clr  __zero_reg__\n+    ret\n+ENDF __mulusa3\n+#endif /* L_mulusa3 */\n+\n+#if defined (L_mulsa3)\n+;;; (C3:C0) = (A3:A0) * (B3:B0)\n+;;; Clobbers: __tmp_reg__\n+;;; Rounding:  -0.5 LSB  <=  error  <=  0.5 LSB\n+DEFUN __mulsa3\n+    XCALL   __mulusa3\n+    tst     B3\n+    brpl    1f\n+    sub     C2, A0\n+    sbc     C3, A1\n+1:  sbrs    A3, 7\n+    ret\n+    sub     C2, B0\n+    sbc     C3, B1\n+    ret\n+ENDF __mulsa3\n+#endif /* L_mulsa3 */\n+\n+#undef A0\n+#undef A1\n+#undef A2\n+#undef A3\n+#undef B0\n+#undef B1\n+#undef B2\n+#undef B3\n+#undef C0\n+#undef C1\n+#undef C2\n+#undef C3\n+\n+#else /* __AVR_HAVE_MUL__ */\n+\n+#define A0 18\n+#define A1 A0+1\n+#define A2 A0+2\n+#define A3 A0+3\n+\n+#define B0 22\n+#define B1 B0+1\n+#define B2 B0+2\n+#define B3 B0+3\n+\n+#define C0  22\n+#define C1  C0+1\n+#define C2  C0+2\n+#define C3  C0+3\n+\n+;; __tmp_reg__\n+#define CC0  0\n+;; __zero_reg__\n+#define CC1  1\n+#define CC2  16\n+#define CC3  17\n+\n+#define AA0  26\n+#define AA1  AA0+1\n+#define AA2  30\n+#define AA3  AA2+1\n+\n+#if defined (L_mulsa3)\n+;;; (R25:R22)  *=  (R21:R18)\n+;;; Clobbers: ABI, called by optabs\n+;;; Rounding:  -1 LSB  <=  error  <=  1 LSB\n+DEFUN   __mulsa3\n+    push    B0\n+    push    B1\n+    bst     B3, 7\n+    XCALL   __mulusa3\n+    ;; A survived in  31:30:27:26\n+    rcall 1f\n+    pop     AA1\n+    pop     AA0\n+    bst     AA3, 7\n+1:  brtc  9f\n+    ;; 1-extend A/B\n+    sub     C2, AA0\n+    sbc     C3, AA1\n+9:  ret\n+ENDF __mulsa3\n+#endif  /* L_mulsa3 */\n+\n+#if defined (L_mulusa3)\n+;;; (R25:R22)  *=  (R21:R18)\n+;;; Clobbers: ABI, called by optabs and __mulsua\n+;;; Rounding:  -1 LSB  <=  error  <=  1 LSB\n+;;; Does not clobber T and A[] survives in 26, 27, 30, 31\n+DEFUN   __mulusa3\n+    push    CC2\n+    push    CC3\n+    ; clear result\n+    clr     __tmp_reg__\n+    wmov    CC2, CC0\n+    ; save multiplicand\n+    wmov    AA0, A0\n+    wmov    AA2, A2\n+    rjmp 3f\n+\n+    ;; Loop the integral part\n+\n+1:  ;; CC += A * 2^n;  n >= 0\n+    add  CC0,A0  $  adc CC1,A1  $  adc  CC2,A2  $  adc  CC3,A3\n+\n+2:  ;; A <<= 1\n+    lsl  A0      $  rol A1      $  rol  A2      $  rol  A3\n+\n+3:  ;; IBIT(B) >>= 1\n+    ;; Carry = n-th bit of B;  n >= 0\n+    lsr     B3\n+    ror     B2\n+    brcs 1b\n+    sbci    B3, 0\n+    brne 2b\n+\n+    ;; Loop the fractional part\n+    ;; B2/B3 is 0 now, use as guard bits for rounding\n+    ;; Restore multiplicand\n+    wmov    A0, AA0\n+    wmov    A2, AA2\n+    rjmp 5f\n+\n+4:  ;; CC += A:Guard * 2^n;  n < 0\n+    add  B3,B2 $  adc  CC0,A0  $  adc  CC1,A1  $  adc  CC2,A2  $  adc  CC3,A3\n+5:\n+    ;; A:Guard >>= 1\n+    lsr  A3   $  ror  A2  $  ror  A1  $  ror   A0  $   ror  B2\n+\n+    ;; FBIT(B) <<= 1\n+    ;; Carry = n-th bit of B;  n < 0\n+    lsl     B0\n+    rol     B1\n+    brcs 4b\n+    sbci    B0, 0\n+    brne 5b\n+\n+    ;; Move result into place and round\n+    lsl     B3\n+    wmov    C2, CC2\n+    wmov    C0, CC0\n+    clr     __zero_reg__\n+    adc     C0, __zero_reg__\n+    adc     C1, __zero_reg__\n+    adc     C2, __zero_reg__\n+    adc     C3, __zero_reg__\n+    \n+    ;; Epilogue\n+    pop     CC3\n+    pop     CC2\n+    ret\n+ENDF __mulusa3\n+#endif  /* L_mulusa3 */\n+\n+#undef A0\n+#undef A1\n+#undef A2\n+#undef A3\n+#undef B0\n+#undef B1\n+#undef B2\n+#undef B3\n+#undef C0\n+#undef C1\n+#undef C2\n+#undef C3\n+#undef AA0\n+#undef AA1\n+#undef AA2\n+#undef AA3\n+#undef CC0\n+#undef CC1\n+#undef CC2\n+#undef CC3\n+\n+#endif /* __AVR_HAVE_MUL__ */\n+\n+/*******************************************************\n+      Fractional Division 8 / 8\n+*******************************************************/\n+\n+#define r_divd  r25     /* dividend */\n+#define r_quo   r24     /* quotient */\n+#define r_div   r22     /* divisor */\n+\n+#if defined (L_divqq3)\n+DEFUN   __divqq3\n+    mov     r0, r_divd\n+    eor     r0, r_div\n+    sbrc    r_div, 7\n+    neg     r_div\n+    sbrc    r_divd, 7\n+    neg     r_divd\n+    cp      r_divd, r_div\n+    breq    __divqq3_minus1  ; if equal return -1\n+    XCALL   __udivuqq3\n+    lsr     r_quo\n+    sbrc    r0, 7   ; negate result if needed\n+    neg     r_quo\n+    ret\n+__divqq3_minus1:\n+    ldi     r_quo, 0x80\n+    ret\n+ENDF __divqq3\n+#endif  /* defined (L_divqq3) */\n+\n+#if defined (L_udivuqq3)\n+DEFUN   __udivuqq3\n+    clr     r_quo           ; clear quotient\n+    inc     __zero_reg__    ; init loop counter, used per shift\n+__udivuqq3_loop:\n+    lsl     r_divd          ; shift dividend\n+    brcs    0f              ; dividend overflow\n+    cp      r_divd,r_div    ; compare dividend & divisor\n+    brcc    0f              ; dividend >= divisor\n+    rol     r_quo           ; shift quotient (with CARRY)\n+    rjmp    __udivuqq3_cont\n+0:\n+    sub     r_divd,r_div    ; restore dividend\n+    lsl     r_quo           ; shift quotient (without CARRY)\n+__udivuqq3_cont:\n+    lsl     __zero_reg__    ; shift loop-counter bit\n+    brne    __udivuqq3_loop\n+    com     r_quo           ; complement result\n+                            ; because C flag was complemented in loop\n+    ret\n+ENDF __udivuqq3\n+#endif  /* defined (L_udivuqq3) */\n+\n+#undef  r_divd\n+#undef  r_quo\n+#undef  r_div\n+\n+\n+/*******************************************************\n+    Fractional Division 16 / 16\n+*******************************************************/\n+#define r_divdL 26     /* dividend Low */\n+#define r_divdH 27     /* dividend Hig */\n+#define r_quoL  24     /* quotient Low */\n+#define r_quoH  25     /* quotient High */\n+#define r_divL  22     /* divisor */\n+#define r_divH  23     /* divisor */\n+#define r_cnt   21\n+\n+#if defined (L_divhq3)\n+DEFUN   __divhq3\n+    mov     r0, r_divdH\n+    eor     r0, r_divH\n+    sbrs    r_divH, 7\n+    rjmp    1f\n+    NEG2    r_divL\n+1:\n+    sbrs    r_divdH, 7\n+    rjmp    2f\n+    NEG2    r_divdL\n+2:\n+    cp      r_divdL, r_divL\n+    cpc     r_divdH, r_divH\n+    breq    __divhq3_minus1  ; if equal return -1\n+    XCALL   __udivuhq3\n+    lsr     r_quoH\n+    ror     r_quoL\n+    brpl    9f\n+    ;; negate result if needed\n+    NEG2    r_quoL\n+9:\n+    ret\n+__divhq3_minus1:\n+    ldi     r_quoH, 0x80\n+    clr     r_quoL\n+    ret\n+ENDF __divhq3\n+#endif  /* defined (L_divhq3) */\n+\n+#if defined (L_udivuhq3)\n+DEFUN   __udivuhq3\n+    sub     r_quoH,r_quoH   ; clear quotient and carry\n+    ;; FALLTHRU\n+ENDF __udivuhq3\n+\n+DEFUN   __udivuha3_common\n+    clr     r_quoL          ; clear quotient\n+    ldi     r_cnt,16        ; init loop counter\n+__udivuhq3_loop:\n+    rol     r_divdL         ; shift dividend (with CARRY)\n+    rol     r_divdH\n+    brcs    __udivuhq3_ep   ; dividend overflow\n+    cp      r_divdL,r_divL  ; compare dividend & divisor\n+    cpc     r_divdH,r_divH\n+    brcc    __udivuhq3_ep   ; dividend >= divisor\n+    rol     r_quoL          ; shift quotient (with CARRY)\n+    rjmp    __udivuhq3_cont\n+__udivuhq3_ep:\n+    sub     r_divdL,r_divL  ; restore dividend\n+    sbc     r_divdH,r_divH\n+    lsl     r_quoL          ; shift quotient (without CARRY)\n+__udivuhq3_cont:\n+    rol     r_quoH          ; shift quotient\n+    dec     r_cnt           ; decrement loop counter\n+    brne    __udivuhq3_loop\n+    com     r_quoL          ; complement result\n+    com     r_quoH          ; because C flag was complemented in loop\n+    ret\n+ENDF __udivuha3_common\n+#endif  /* defined (L_udivuhq3) */\n+\n+/*******************************************************\n+    Fixed Division 8.8 / 8.8\n+*******************************************************/\n+#if defined (L_divha3)\n+DEFUN   __divha3\n+    mov     r0, r_divdH\n+    eor     r0, r_divH\n+    sbrs    r_divH, 7\n+    rjmp    1f\n+    NEG2    r_divL\n+1:\n+    sbrs    r_divdH, 7\n+    rjmp    2f\n+    NEG2    r_divdL\n+2:\n+    XCALL   __udivuha3\n+    sbrs    r0, 7   ; negate result if needed\n+    ret\n+    NEG2    r_quoL\n+    ret\n+ENDF __divha3\n+#endif  /* defined (L_divha3) */\n+\n+#if defined (L_udivuha3)\n+DEFUN   __udivuha3\n+    mov     r_quoH, r_divdL\n+    mov     r_divdL, r_divdH\n+    clr     r_divdH\n+    lsl     r_quoH     ; shift quotient into carry\n+    XJMP    __udivuha3_common ; same as fractional after rearrange\n+ENDF __udivuha3\n+#endif  /* defined (L_udivuha3) */\n+\n+#undef  r_divdL\n+#undef  r_divdH\n+#undef  r_quoL\n+#undef  r_quoH\n+#undef  r_divL\n+#undef  r_divH\n+#undef  r_cnt\n+\n+/*******************************************************\n+    Fixed Division 16.16 / 16.16\n+*******************************************************/\n+\n+#define r_arg1L  24    /* arg1 gets passed already in place */\n+#define r_arg1H  25\n+#define r_arg1HL 26\n+#define r_arg1HH 27\n+#define r_divdL  26    /* dividend Low */\n+#define r_divdH  27\n+#define r_divdHL 30\n+#define r_divdHH 31    /* dividend High */\n+#define r_quoL   22    /* quotient Low */\n+#define r_quoH   23\n+#define r_quoHL  24\n+#define r_quoHH  25    /* quotient High */\n+#define r_divL   18    /* divisor Low */\n+#define r_divH   19\n+#define r_divHL  20\n+#define r_divHH  21    /* divisor High */\n+#define r_cnt  __zero_reg__  /* loop count (0 after the loop!) */\n+\n+#if defined (L_divsa3)\n+DEFUN   __divsa3\n+    mov     r0, r_arg1HH\n+    eor     r0, r_divHH\n+    sbrs    r_divHH, 7\n+    rjmp    1f\n+    NEG4    r_divL\n+1:\n+    sbrs    r_arg1HH, 7\n+    rjmp    2f\n+    NEG4    r_arg1L\n+2:\n+    XCALL   __udivusa3\n+    sbrs    r0, 7   ; negate result if needed\n+    ret\n+    NEG4    r_quoL\n+    ret\n+ENDF __divsa3\n+#endif  /* defined (L_divsa3) */\n+\n+#if defined (L_udivusa3)\n+DEFUN   __udivusa3\n+    ldi     r_divdHL, 32    ; init loop counter\n+    mov     r_cnt, r_divdHL\n+    clr     r_divdHL\n+    clr     r_divdHH\n+    wmov    r_quoL, r_divdHL\n+    lsl     r_quoHL         ; shift quotient into carry\n+    rol     r_quoHH\n+__udivusa3_loop:\n+    rol     r_divdL         ; shift dividend (with CARRY)\n+    rol     r_divdH\n+    rol     r_divdHL\n+    rol     r_divdHH\n+    brcs    __udivusa3_ep   ; dividend overflow\n+    cp      r_divdL,r_divL  ; compare dividend & divisor\n+    cpc     r_divdH,r_divH\n+    cpc     r_divdHL,r_divHL\n+    cpc     r_divdHH,r_divHH\n+    brcc    __udivusa3_ep   ; dividend >= divisor\n+    rol     r_quoL          ; shift quotient (with CARRY)\n+    rjmp    __udivusa3_cont\n+__udivusa3_ep:\n+    sub     r_divdL,r_divL  ; restore dividend\n+    sbc     r_divdH,r_divH\n+    sbc     r_divdHL,r_divHL\n+    sbc     r_divdHH,r_divHH\n+    lsl     r_quoL          ; shift quotient (without CARRY)\n+__udivusa3_cont:\n+    rol     r_quoH          ; shift quotient\n+    rol     r_quoHL\n+    rol     r_quoHH\n+    dec     r_cnt           ; decrement loop counter\n+    brne    __udivusa3_loop\n+    com     r_quoL          ; complement result\n+    com     r_quoH          ; because C flag was complemented in loop\n+    com     r_quoHL\n+    com     r_quoHH\n+    ret\n+ENDF __udivusa3\n+#endif  /* defined (L_udivusa3) */\n+\n+#undef  r_arg1L\n+#undef  r_arg1H\n+#undef  r_arg1HL\n+#undef  r_arg1HH\n+#undef  r_divdL\n+#undef  r_divdH\n+#undef  r_divdHL\n+#undef  r_divdHH\n+#undef  r_quoL\n+#undef  r_quoH\n+#undef  r_quoHL\n+#undef  r_quoHH\n+#undef  r_divL\n+#undef  r_divH\n+#undef  r_divHL\n+#undef  r_divHH\n+#undef  r_cnt"}, {"sha": "6b9879ee7d70ac7ad43d17edea0e735723ed18de", "filename": "libgcc/config/avr/lib1funcs.S", "status": "modified", "additions": 273, "deletions": 150, "changes": 423, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/libgcc%2Fconfig%2Favr%2Flib1funcs.S", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/libgcc%2Fconfig%2Favr%2Flib1funcs.S", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgcc%2Fconfig%2Favr%2Flib1funcs.S?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -91,6 +91,35 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n .endfunc\n .endm\n \n+;; Negate a 2-byte value held in consecutive registers\n+.macro NEG2  reg\n+    com     \\reg+1\n+    neg     \\reg\n+    sbci    \\reg+1, -1\n+.endm\n+\n+;; Negate a 4-byte value held in consecutive registers\n+.macro NEG4  reg\n+    com     \\reg+3\n+    com     \\reg+2\n+    com     \\reg+1\n+.if \\reg >= 16\n+    neg     \\reg\n+    sbci    \\reg+1, -1\n+    sbci    \\reg+2, -1\n+    sbci    \\reg+3, -1\n+.else\n+    com     \\reg\n+    adc     \\reg,   __zero_reg__\n+    adc     \\reg+1, __zero_reg__\n+    adc     \\reg+2, __zero_reg__\n+    adc     \\reg+3, __zero_reg__\n+.endif\n+.endm\n+\n+#define exp_lo(N)  hlo8 ((N) << 23)\n+#define exp_hi(N)  hhi8 ((N) << 23)\n+\n \f\n .section .text.libgcc.mul, \"ax\", @progbits\n \n@@ -126,175 +155,246 @@ ENDF __mulqi3\n \t\n #endif \t/* defined (L_mulqi3) */\n \n-#if defined (L_mulqihi3)\n-DEFUN __mulqihi3\n-\tclr\tr25\n-\tsbrc\tr24, 7\n-\tdec\tr25\n-\tclr\tr23\n-\tsbrc\tr22, 7\n-\tdec\tr22\n-\tXJMP\t__mulhi3\n-ENDF __mulqihi3:\n-#endif /* defined (L_mulqihi3) */\n+\n+/*******************************************************\n+    Widening Multiplication  16 = 8 x 8  without MUL\n+    Multiplication  16 x 16  without MUL\n+*******************************************************/\n+\n+#define A0  r22\n+#define A1  r23\n+#define B0  r24\n+#define BB0 r20\n+#define B1  r25\n+;; Output overlaps input, thus expand result in CC0/1\n+#define C0  r24\n+#define C1  r25\n+#define CC0  __tmp_reg__\n+#define CC1  R21\n \n #if defined (L_umulqihi3)\n+;;; R25:R24 = (unsigned int) R22 * (unsigned int) R24\n+;;; (C1:C0) = (unsigned int) A0  * (unsigned int) B0\n+;;; Clobbers: __tmp_reg__, R21..R23\n DEFUN __umulqihi3\n-\tclr\tr25\n-\tclr\tr23\n-\tXJMP\t__mulhi3\n+    clr     A1\n+    clr     B1\n+    XJMP    __mulhi3\n ENDF __umulqihi3\n-#endif /* defined (L_umulqihi3) */\n+#endif /* L_umulqihi3 */\n \n-/*******************************************************\n-    Multiplication  16 x 16  without MUL\n-*******************************************************/\n-#if defined (L_mulhi3)\n-#define\tr_arg1L\tr24\t\t/* multiplier Low */\n-#define\tr_arg1H\tr25\t\t/* multiplier High */\n-#define\tr_arg2L\tr22\t\t/* multiplicand Low */\n-#define\tr_arg2H\tr23\t\t/* multiplicand High */\n-#define r_resL\t__tmp_reg__\t/* result Low */\n-#define r_resH  r21\t\t/* result High */\n+#if defined (L_mulqihi3)\n+;;; R25:R24 = (signed int) R22 * (signed int) R24\n+;;; (C1:C0) = (signed int) A0  * (signed int) B0\n+;;; Clobbers: __tmp_reg__, R20..R23\n+DEFUN __mulqihi3\n+    ;; Sign-extend B0\n+    clr     B1\n+    sbrc    B0, 7\n+    com     B1\n+    ;; The multiplication runs twice as fast if A1 is zero, thus:\n+    ;; Zero-extend A0\n+    clr     A1\n+#ifdef __AVR_HAVE_JMP_CALL__\n+    ;; Store  B0 * sign of A\n+    clr     BB0\n+    sbrc    A0, 7\n+    mov     BB0, B0\n+    call    __mulhi3\n+#else /* have no CALL */\n+    ;; Skip sign-extension of A if A >= 0\n+    ;; Same size as with the first alternative but avoids errata skip\n+    ;; and is faster if A >= 0\n+    sbrs    A0, 7\n+    rjmp    __mulhi3\n+    ;; If  A < 0  store B\n+    mov     BB0, B0\n+    rcall   __mulhi3\n+#endif /* HAVE_JMP_CALL */\n+    ;; 1-extend A after the multiplication\n+    sub     C1, BB0\n+    ret\n+ENDF __mulqihi3\n+#endif /* L_mulqihi3 */\n \n+#if defined (L_mulhi3)\n+;;; R25:R24 = R23:R22 * R25:R24\n+;;; (C1:C0) = (A1:A0) * (B1:B0)\n+;;; Clobbers: __tmp_reg__, R21..R23\n DEFUN __mulhi3\n-\tclr\tr_resH\t\t; clear result\n-\tclr\tr_resL\t\t; clear result\n-__mulhi3_loop:\n-\tsbrs\tr_arg1L,0\n-\trjmp\t__mulhi3_skip1\n-\tadd\tr_resL,r_arg2L\t; result + multiplicand\n-\tadc\tr_resH,r_arg2H\n-__mulhi3_skip1:\t\n-\tadd\tr_arg2L,r_arg2L\t; shift multiplicand\n-\tadc\tr_arg2H,r_arg2H\n-\n-\tcp\tr_arg2L,__zero_reg__\n-\tcpc\tr_arg2H,__zero_reg__\n-\tbreq\t__mulhi3_exit\t; while multiplicand != 0\n-\n-\tlsr\tr_arg1H\t\t; gets LSB of multiplier\n-\tror\tr_arg1L\n-\tsbiw\tr_arg1L,0\n-\tbrne\t__mulhi3_loop\t; exit if multiplier = 0\n-__mulhi3_exit:\n-\tmov\tr_arg1H,r_resH\t; result to return register\n-\tmov\tr_arg1L,r_resL\n-\tret\n-ENDF __mulhi3\n \n-#undef r_arg1L\n-#undef r_arg1H\n-#undef r_arg2L\n-#undef r_arg2H\n-#undef r_resL \t\n-#undef r_resH \n+    ;; Clear result\n+    clr     CC0\n+    clr     CC1\n+    rjmp 3f\n+1:\n+    ;; Bit n of A is 1  -->  C += B << n\n+    add     CC0, B0\n+    adc     CC1, B1\n+2:\n+    lsl     B0\n+    rol     B1\n+3:\n+    ;; If B == 0 we are ready\n+    sbiw    B0, 0\n+    breq 9f\n+\n+    ;; Carry = n-th bit of A\n+    lsr     A1\n+    ror     A0\n+    ;; If bit n of A is set, then go add  B * 2^n  to  C\n+    brcs 1b\n+\n+    ;; Carry = 0  -->  The ROR above acts like  CP A0, 0\n+    ;; Thus, it is sufficient to CPC the high part to test A against 0\n+    cpc     A1, __zero_reg__\n+    ;; Only proceed if A != 0\n+    brne    2b\n+9:\n+    ;; Move Result into place\n+    mov     C0, CC0\n+    mov     C1, CC1\n+    ret\n+ENDF  __mulhi3\n+#endif /* L_mulhi3 */\n \n-#endif /* defined (L_mulhi3) */\n+#undef A0\n+#undef A1\n+#undef B0\n+#undef BB0\n+#undef B1\n+#undef C0\n+#undef C1\n+#undef CC0\n+#undef CC1\n+\n+\f\n+#define A0 22\n+#define A1 A0+1\n+#define A2 A0+2\n+#define A3 A0+3\n+\n+#define B0 18\n+#define B1 B0+1\n+#define B2 B0+2\n+#define B3 B0+3\n+\n+#define CC0 26\n+#define CC1 CC0+1\n+#define CC2 30\n+#define CC3 CC2+1\n+\n+#define C0 22\n+#define C1 C0+1\n+#define C2 C0+2\n+#define C3 C0+3\n \n /*******************************************************\n     Widening Multiplication  32 = 16 x 16  without MUL\n *******************************************************/\n \n-#if defined (L_mulhisi3)\n-DEFUN __mulhisi3\n-;;; FIXME: This is dead code (noone calls it)\n-    mov_l   r18, r24\n-    mov_h   r19, r25\n-    clr     r24\n-    sbrc    r23, 7\n-    dec     r24\n-    mov     r25, r24\n-    clr     r20\n-    sbrc    r19, 7\n-    dec     r20\n-    mov     r21, r20\n-    XJMP    __mulsi3\n-ENDF __mulhisi3\n-#endif /* defined (L_mulhisi3) */\n-\n #if defined (L_umulhisi3)\n DEFUN __umulhisi3\n-;;; FIXME: This is dead code (noone calls it)\n-    mov_l   r18, r24\n-    mov_h   r19, r25\n-    clr     r24\n-    clr     r25\n-    mov_l   r20, r24\n-    mov_h   r21, r25\n+    wmov    B0, 24\n+    ;; Zero-extend B\n+    clr     B2\n+    clr     B3\n+    ;; Zero-extend A\n+    wmov    A2, B2\n     XJMP    __mulsi3\n ENDF __umulhisi3\n-#endif /* defined (L_umulhisi3) */\n+#endif /* L_umulhisi3 */\n+\n+#if defined (L_mulhisi3)\n+DEFUN __mulhisi3\n+    wmov    B0, 24\n+    ;; Sign-extend B\n+    lsl     r25\n+    sbc     B2, B2\n+    mov     B3, B2\n+#ifdef __AVR_ERRATA_SKIP_JMP_CALL__\n+    ;; Sign-extend A\n+    clr     A2\n+    sbrc    A1, 7\n+    com     A2\n+    mov     A3, A2\n+    XJMP __mulsi3\n+#else /*  no __AVR_ERRATA_SKIP_JMP_CALL__ */\n+    ;; Zero-extend A and __mulsi3 will run at least twice as fast\n+    ;; compared to a sign-extended A.\n+    clr     A2\n+    clr     A3\n+    sbrs    A1, 7\n+    XJMP __mulsi3\n+    ;; If  A < 0  then perform the  B * 0xffff.... before the\n+    ;; very multiplication by initializing the high part of the\n+    ;; result CC with -B.\n+    wmov    CC2, A2\n+    sub     CC2, B0\n+    sbc     CC3, B1\n+    XJMP __mulsi3_helper\n+#endif /*  __AVR_ERRATA_SKIP_JMP_CALL__ */\n+ENDF __mulhisi3\n+#endif /* L_mulhisi3 */\n+\n \n-#if defined (L_mulsi3)\n /*******************************************************\n     Multiplication  32 x 32  without MUL\n *******************************************************/\n-#define r_arg1L  r22\t\t/* multiplier Low */\n-#define r_arg1H  r23\n-#define\tr_arg1HL r24\n-#define\tr_arg1HH r25\t\t/* multiplier High */\n-\n-#define\tr_arg2L  r18\t\t/* multiplicand Low */\n-#define\tr_arg2H  r19\t\n-#define\tr_arg2HL r20\n-#define\tr_arg2HH r21\t\t/* multiplicand High */\n-\t\n-#define r_resL\t r26\t\t/* result Low */\n-#define r_resH   r27\n-#define r_resHL\t r30\n-#define r_resHH  r31\t\t/* result High */\n \n+#if defined (L_mulsi3)\n DEFUN __mulsi3\n-\tclr\tr_resHH\t\t; clear result\n-\tclr\tr_resHL\t\t; clear result\n-\tclr\tr_resH\t\t; clear result\n-\tclr\tr_resL\t\t; clear result\n-__mulsi3_loop:\n-\tsbrs\tr_arg1L,0\n-\trjmp\t__mulsi3_skip1\n-\tadd\tr_resL,r_arg2L\t\t; result + multiplicand\n-\tadc\tr_resH,r_arg2H\n-\tadc\tr_resHL,r_arg2HL\n-\tadc\tr_resHH,r_arg2HH\n-__mulsi3_skip1:\n-\tadd\tr_arg2L,r_arg2L\t\t; shift multiplicand\n-\tadc\tr_arg2H,r_arg2H\n-\tadc\tr_arg2HL,r_arg2HL\n-\tadc\tr_arg2HH,r_arg2HH\n-\t\n-\tlsr\tr_arg1HH\t; gets LSB of multiplier\n-\tror\tr_arg1HL\n-\tror\tr_arg1H\n-\tror\tr_arg1L\n-\tbrne\t__mulsi3_loop\n-\tsbiw\tr_arg1HL,0\n-\tcpc\tr_arg1H,r_arg1L\n-\tbrne\t__mulsi3_loop\t\t; exit if multiplier = 0\n-__mulsi3_exit:\n-\tmov_h\tr_arg1HH,r_resHH\t; result to return register\n-\tmov_l\tr_arg1HL,r_resHL\n-\tmov_h\tr_arg1H,r_resH\n-\tmov_l\tr_arg1L,r_resL\n-\tret\n-ENDF __mulsi3\n+    ;; Clear result\n+    clr     CC2\n+    clr     CC3\n+    ;; FALLTHRU\n+ENDF  __mulsi3\n \n-#undef r_arg1L \n-#undef r_arg1H \n-#undef r_arg1HL\n-#undef r_arg1HH\n-             \n-#undef r_arg2L \n-#undef r_arg2H \n-#undef r_arg2HL\n-#undef r_arg2HH\n-             \n-#undef r_resL  \n-#undef r_resH  \n-#undef r_resHL \n-#undef r_resHH \n+DEFUN __mulsi3_helper\n+    clr     CC0\n+    clr     CC1\n+    rjmp 3f\n+\n+1:  ;; If bit n of A is set, then add  B * 2^n  to the result in CC\n+    ;; CC += B\n+    add  CC0,B0  $  adc  CC1,B1  $  adc  CC2,B2  $  adc  CC3,B3\n+\n+2:  ;; B <<= 1\n+    lsl  B0      $  rol  B1      $  rol  B2      $  rol  B3\n+    \n+3:  ;; A >>= 1:  Carry = n-th bit of A\n+    lsr  A3      $  ror  A2      $  ror  A1      $  ror  A0\n+\n+    brcs 1b\n+    ;; Only continue if  A != 0\n+    sbci    A1, 0\n+    brne 2b\n+    sbiw    A2, 0\n+    brne 2b\n+\n+    ;; All bits of A are consumed:  Copy result to return register C\n+    wmov    C0, CC0\n+    wmov    C2, CC2\n+    ret\n+ENDF __mulsi3_helper\n+#endif /* L_mulsi3 */\n \n-#endif /* defined (L_mulsi3) */\n+#undef A0\n+#undef A1\n+#undef A2\n+#undef A3\n+#undef B0\n+#undef B1\n+#undef B2\n+#undef B3\n+#undef C0\n+#undef C1\n+#undef C2\n+#undef C3\n+#undef CC0\n+#undef CC1\n+#undef CC2\n+#undef CC3\n \n #endif /* !defined (__AVR_HAVE_MUL__) */\n ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n@@ -316,7 +416,7 @@ ENDF __mulsi3\n #define C3 C0+3\n \n /*******************************************************\n-    Widening Multiplication  32 = 16 x 16\n+    Widening Multiplication  32 = 16 x 16  with MUL\n *******************************************************/\n                               \n #if defined (L_mulhisi3)\n@@ -364,7 +464,17 @@ DEFUN __umulhisi3\n     mul     A1, B1\n     movw    C2, r0\n     mul     A0, B1\n+#ifdef __AVR_HAVE_JMP_CALL__\n+    ;; This function is used by many other routines, often multiple times.\n+    ;; Therefore, if the flash size is not too limited, avoid the RCALL\n+    ;; and inverst 6 Bytes to speed things up.\n+    add     C1, r0\n+    adc     C2, r1\n+    clr     __zero_reg__\n+    adc     C3, __zero_reg__\n+#else\n     rcall   1f\n+#endif\n     mul     A1, B0\n 1:  add     C1, r0\n     adc     C2, r1\n@@ -375,7 +485,7 @@ ENDF __umulhisi3\n #endif /* L_umulhisi3 */\n \n /*******************************************************\n-    Widening Multiplication  32 = 16 x 32\n+    Widening Multiplication  32 = 16 x 32  with MUL\n *******************************************************/\n \n #if defined (L_mulshisi3)\n@@ -425,7 +535,7 @@ ENDF __muluhisi3\n #endif /* L_muluhisi3 */\n \n /*******************************************************\n-    Multiplication  32 x 32\n+    Multiplication  32 x 32  with MUL\n *******************************************************/\n \n #if defined (L_mulsi3)\n@@ -468,7 +578,7 @@ ENDF __mulsi3\n #endif /* __AVR_HAVE_MUL__ */\n \n /*******************************************************\n-       Multiplication 24 x 24\n+       Multiplication 24 x 24 with MUL\n *******************************************************/\n \n #if defined (L_mulpsi3)\n@@ -1247,6 +1357,19 @@ __divmodsi4_exit:\n ENDF __divmodsi4\n #endif /* defined (L_divmodsi4) */\n \n+#undef r_remHH\n+#undef r_remHL\n+#undef r_remH\n+#undef r_remL\n+#undef r_arg1HH\n+#undef r_arg1HL\n+#undef r_arg1H\n+#undef r_arg1L\n+#undef r_arg2HH\n+#undef r_arg2HL\n+#undef r_arg2H\n+#undef r_arg2L\n+#undef r_cnt\n \n /*******************************************************\n        Division 64 / 64\n@@ -2757,9 +2880,7 @@ DEFUN __fmulsu_exit\n     XJMP  __fmul\n 1:  XCALL __fmul\n     ;; C = -C iff A0.7 = 1\n-    com  C1\n-    neg  C0\n-    sbci C1, -1\n+    NEG2 C0\n     ret\n ENDF __fmulsu_exit\n #endif /* L_fmulsu */\n@@ -2794,3 +2915,5 @@ ENDF __fmul\n #undef B1\n #undef C0\n #undef C1\n+\n+#include \"lib1funcs-fixed.S\""}, {"sha": "6f783cd9d52517af4e97eecd0a47a2ab14c75f08", "filename": "libgcc/config/avr/t-avr", "status": "modified", "additions": 65, "deletions": 0, "changes": 65, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e55e40561955a4e732e8b503e37ca148fe162909/libgcc%2Fconfig%2Favr%2Ft-avr", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e55e40561955a4e732e8b503e37ca148fe162909/libgcc%2Fconfig%2Favr%2Ft-avr", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgcc%2Fconfig%2Favr%2Ft-avr?ref=e55e40561955a4e732e8b503e37ca148fe162909", "patch": "@@ -2,6 +2,7 @@ LIB1ASMSRC = avr/lib1funcs.S\n LIB1ASMFUNCS = \\\n \t_mulqi3 \\\n \t_mulhi3 \\\n+\t_mulqihi3 _umulqihi3 \\\n \t_mulpsi3 _mulsqipsi3 \\\n \t_mulhisi3 \\\n \t_umulhisi3 \\\n@@ -55,6 +56,24 @@ LIB1ASMFUNCS = \\\n \t_cmpdi2 _cmpdi2_s8 \\\n \t_fmul _fmuls _fmulsu\n \n+# Fixed point routines in avr/lib1funcs-fixed.S\n+LIB1ASMFUNCS += \\\n+\t_fractqqsf _fractuqqsf \\\n+\t_fracthqsf _fractuhqsf _fracthasf _fractuhasf \\\n+\t_fractsasf _fractusasf _fractsqsf _fractusqsf \\\n+\t\\\n+\t_fractsfqq _fractsfuqq \\\n+\t_fractsfhq _fractsfuhq _fractsfha _fractsfuha \\\n+\t_fractsfsa _fractsfusa \\\n+\t_mulqq3 \\\n+\t_mulhq3 _muluhq3 \\\n+\t_mulha3 _muluha3 _muluha3_round \\\n+\t_mulsa3 _mulusa3 \\\n+\t_divqq3 _udivuqq3 \\\n+\t_divhq3 _udivuhq3 \\\n+\t_divha3 _udivuha3 \\\n+\t_divsa3 _udivusa3\n+\n LIB2FUNCS_EXCLUDE = \\\n \t_moddi3 _umoddi3 \\\n \t_clz\n@@ -81,3 +100,49 @@ libgcc-objects += $(patsubst %,%$(objext),$(hiintfuncs16))\n ifeq ($(enable_shared),yes)\n libgcc-s-objects += $(patsubst %,%_s$(objext),$(hiintfuncs16))\n endif\n+\n+\n+# Filter out supported conversions from fixed-bit.c\n+\n+conv_XY=$(conv)$(mode1)$(mode2)\n+conv_X=$(conv)$(mode)\n+\n+# Conversions supported by the compiler\n+\n+convf_modes =\t QI UQI QQ UQQ \\\n+\t\t HI UHI HQ UHQ HA UHA \\\n+\t\t SI USI SQ USQ SA USA \\\n+\t\t DI UDI DQ UDQ DA UDA \\\n+\t\t TI UTI TQ UTQ TA UTA\n+\n+LIB2FUNCS_EXCLUDE += \\\n+\t$(foreach conv,_fract _fractuns,\\\n+\t$(foreach mode1,$(convf_modes),\\\n+\t$(foreach mode2,$(convf_modes),$(conv_XY))))\n+\n+# Conversions supported by lib1funcs-fixed.S\n+\n+conv_to_sf_modes   = QQ UQQ HQ UHQ HA UHA SQ USQ SA USA\n+conv_from_sf_modes = QQ UQQ HQ UHQ HA UHA        SA USA\n+\n+LIB2FUNCS_EXCLUDE += \\\n+\t$(foreach conv,_fract, \\\n+\t$(foreach mode1,$(conv_to_sf_modes), \\\n+\t$(foreach mode2,SF,$(conv_XY))))\n+\n+LIB2FUNCS_EXCLUDE += \\\n+\t$(foreach conv,_fract,\\\n+\t$(foreach mode1,SF,\\\n+\t$(foreach mode2,$(conv_from_sf_modes),$(conv_XY))))\n+\n+# Arithmetik supported by the compiler\n+\n+allfix_modes = QQ UQQ HQ UHQ HA UHA SQ USQ SA USA DA UDA DQ UDQ TQ UTQ TA UTA\n+\n+LIB2FUNCS_EXCLUDE += \\\n+\t$(foreach conv,_add _sub,\\\n+\t$(foreach mode,$(allfix_modes),$(conv_X)3))\n+\n+LIB2FUNCS_EXCLUDE += \\\n+\t$(foreach conv,_lshr _ashl _ashr _cmp,\\\n+\t$(foreach mode,$(allfix_modes),$(conv_X)))"}]}