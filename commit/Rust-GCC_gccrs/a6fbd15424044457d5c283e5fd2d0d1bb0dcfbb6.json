{"sha": "a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YTZmYmQxNTQyNDA0NDQ1N2Q1YzI4M2U1ZmQyZDBkMWJiMGRjZmJiNg==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2017-11-13T10:26:13Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2017-11-13T10:26:13Z"}, "message": "re PR tree-optimization/78821 (GCC7: Copying whole 32 bits structure field by field not optimised into copying whole 32 bits at once)\n\n\tPR tree-optimization/78821\n\t* gimple-ssa-store-merging.c (compatible_load_p): Don't require\n\tthat bit_not_p is the same.\n\t(imm_store_chain_info::coalesce_immediate_stores): Likewise.\n\t(split_group): Count precisely bit_not_p bits in each statement.\n\t(invert_op): New function.\n\t(imm_store_chain_info::output_merged_store): Use invert_op to\n\temit BIT_XOR_EXPR with a xor_mask instead of BIT_NOT_EXPR if some\n\tbut not all orig_stores have BIT_NOT_EXPR in the corresponding spots.\n\n\t* gcc.dg/store_merging_15.c: New test.\n\nFrom-SVN: r254679", "tree": {"sha": "c4644b84d36195da7ac02fcc2c2a4ac621c3a29f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/c4644b84d36195da7ac02fcc2c2a4ac621c3a29f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "a1b5dd18ae7e335058f2f096b54a251b19eb9d2d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a1b5dd18ae7e335058f2f096b54a251b19eb9d2d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a1b5dd18ae7e335058f2f096b54a251b19eb9d2d"}], "stats": {"total": 193, "additions": 175, "deletions": 18}, "files": [{"sha": "20391e3743202650352879578436a40c54fdcca3", "filename": "gcc/ChangeLog", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6", "patch": "@@ -1,3 +1,15 @@\n+2017-11-13  Jakub Jelinek  <jakub@redhat.com>\n+\n+\tPR tree-optimization/78821\n+\t* gimple-ssa-store-merging.c (compatible_load_p): Don't require\n+\tthat bit_not_p is the same.\n+\t(imm_store_chain_info::coalesce_immediate_stores): Likewise.\n+\t(split_group): Count precisely bit_not_p bits in each statement.\n+\t(invert_op): New function.\n+\t(imm_store_chain_info::output_merged_store): Use invert_op to\n+\temit BIT_XOR_EXPR with a xor_mask instead of BIT_NOT_EXPR if some\n+\tbut not all orig_stores have BIT_NOT_EXPR in the corresponding spots.\n+\n 2017-11-13  Martin Liska  <mliska@suse.cz>\n \n \t* gcov.c (struct coverage_info): Remove typedef of coverage_t."}, {"sha": "b8920d92b1dffd73c9d47406645a09e9f33e97b2", "filename": "gcc/gimple-ssa-store-merging.c", "status": "modified", "additions": 104, "deletions": 18, "changes": 122, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6/gcc%2Fgimple-ssa-store-merging.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6/gcc%2Fgimple-ssa-store-merging.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-ssa-store-merging.c?ref=a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6", "patch": "@@ -1039,7 +1039,6 @@ compatible_load_p (merged_store_group *merged_store,\n {\n   store_immediate_info *infof = merged_store->stores[0];\n   if (!info->ops[idx].base_addr\n-      || info->ops[idx].bit_not_p != infof->ops[idx].bit_not_p\n       || (info->ops[idx].bitpos - infof->ops[idx].bitpos\n \t  != info->bitpos - infof->bitpos)\n       || !operand_equal_p (info->ops[idx].base_addr,\n@@ -1179,8 +1178,7 @@ imm_store_chain_info::coalesce_immediate_stores ()\n \t Merge it into the current store group.  There can be gaps in between\n \t the stores, but there can't be gaps in between bitregions.  */\n       else if (info->bitregion_start <= merged_store->bitregion_end\n-\t       && info->rhs_code == merged_store->stores[0]->rhs_code\n-\t       && info->bit_not_p == merged_store->stores[0]->bit_not_p)\n+\t       && info->rhs_code == merged_store->stores[0]->rhs_code)\n \t{\n \t  store_immediate_info *infof = merged_store->stores[0];\n \n@@ -1501,16 +1499,14 @@ split_group (merged_store_group *group, bool allow_unaligned_store,\n       total_orig[0] = 1; /* The orig store.  */\n       info = group->stores[0];\n       if (info->ops[0].base_addr)\n-\ttotal_orig[0] += 1 + info->ops[0].bit_not_p;\n+\ttotal_orig[0]++;\n       if (info->ops[1].base_addr)\n-\ttotal_orig[0] += 1 + info->ops[1].bit_not_p;\n+\ttotal_orig[0]++;\n       switch (info->rhs_code)\n \t{\n \tcase BIT_AND_EXPR:\n \tcase BIT_IOR_EXPR:\n \tcase BIT_XOR_EXPR:\n-\t  if (info->bit_not_p)\n-\t    total_orig[0]++; /* The orig BIT_NOT_EXPR stmt.  */\n \t  total_orig[0]++; /* The orig BIT_*_EXPR stmt.  */\n \t  break;\n \tdefault:\n@@ -1519,7 +1515,12 @@ split_group (merged_store_group *group, bool allow_unaligned_store,\n       total_orig[0] *= group->stores.length ();\n \n       FOR_EACH_VEC_ELT (group->stores, i, info)\n-\ttotal_new[0] += count_multiple_uses (info);\n+\t{\n+\t  total_new[0] += count_multiple_uses (info);\n+\t  total_orig[0] += (info->bit_not_p\n+\t\t\t    + info->ops[0].bit_not_p\n+\t\t\t    + info->ops[1].bit_not_p);\n+\t}\n     }\n \n   if (!allow_unaligned_load)\n@@ -1659,40 +1660,119 @@ split_group (merged_store_group *group, bool allow_unaligned_store,\n \n   if (total_orig)\n     {\n+      unsigned int i;\n+      struct split_store *store;\n       /* If we are reusing some original stores and any of the\n \t original SSA_NAMEs had multiple uses, we need to subtract\n \t those now before we add the new ones.  */\n       if (total_new[0] && any_orig)\n \t{\n-\t  unsigned int i;\n-\t  struct split_store *store;\n \t  FOR_EACH_VEC_ELT (*split_stores, i, store)\n \t    if (store->orig)\n \t      total_new[0] -= count_multiple_uses (store->orig_stores[0]);\n \t}\n       total_new[0] += ret; /* The new store.  */\n       store_immediate_info *info = group->stores[0];\n       if (info->ops[0].base_addr)\n-\ttotal_new[0] += ret * (1 + info->ops[0].bit_not_p);\n+\ttotal_new[0] += ret;\n       if (info->ops[1].base_addr)\n-\ttotal_new[0] += ret * (1 + info->ops[1].bit_not_p);\n+\ttotal_new[0] += ret;\n       switch (info->rhs_code)\n \t{\n \tcase BIT_AND_EXPR:\n \tcase BIT_IOR_EXPR:\n \tcase BIT_XOR_EXPR:\n-\t  if (info->bit_not_p)\n-\t    total_new[0] += ret; /* The new BIT_NOT_EXPR stmt.  */\n \t  total_new[0] += ret; /* The new BIT_*_EXPR stmt.  */\n \t  break;\n \tdefault:\n \t  break;\n \t}\n+      FOR_EACH_VEC_ELT (*split_stores, i, store)\n+\t{\n+\t  unsigned int j;\n+\t  bool bit_not_p[3] = { false, false, false };\n+\t  /* If all orig_stores have certain bit_not_p set, then\n+\t     we'd use a BIT_NOT_EXPR stmt and need to account for it.\n+\t     If some orig_stores have certain bit_not_p set, then\n+\t     we'd use a BIT_XOR_EXPR with a mask and need to account for\n+\t     it.  */\n+\t  FOR_EACH_VEC_ELT (store->orig_stores, j, info)\n+\t    {\n+\t      if (info->ops[0].bit_not_p)\n+\t\tbit_not_p[0] = true;\n+\t      if (info->ops[1].bit_not_p)\n+\t\tbit_not_p[1] = true;\n+\t      if (info->bit_not_p)\n+\t\tbit_not_p[2] = true;\n+\t    }\n+\t  total_new[0] += bit_not_p[0] + bit_not_p[1] + bit_not_p[2];\n+\t}\n+\n     }\n \n   return ret;\n }\n \n+/* Return the operation through which the operand IDX (if < 2) or\n+   result (IDX == 2) should be inverted.  If NOP_EXPR, no inversion\n+   is done, if BIT_NOT_EXPR, all bits are inverted, if BIT_XOR_EXPR,\n+   the bits should be xored with mask.  */\n+\n+static enum tree_code\n+invert_op (split_store *split_store, int idx, tree int_type, tree &mask)\n+{\n+  unsigned int i;\n+  store_immediate_info *info;\n+  unsigned int cnt = 0;\n+  FOR_EACH_VEC_ELT (split_store->orig_stores, i, info)\n+    {\n+      bool bit_not_p = idx < 2 ? info->ops[idx].bit_not_p : info->bit_not_p;\n+      if (bit_not_p)\n+\t++cnt;\n+    }\n+  mask = NULL_TREE;\n+  if (cnt == 0)\n+    return NOP_EXPR;\n+  if (cnt == split_store->orig_stores.length ())\n+    return BIT_NOT_EXPR;\n+\n+  unsigned HOST_WIDE_INT try_bitpos = split_store->bytepos * BITS_PER_UNIT;\n+  unsigned buf_size = split_store->size / BITS_PER_UNIT;\n+  unsigned char *buf\n+    = XALLOCAVEC (unsigned char, buf_size);\n+  memset (buf, ~0U, buf_size);\n+  FOR_EACH_VEC_ELT (split_store->orig_stores, i, info)\n+    {\n+      bool bit_not_p = idx < 2 ? info->ops[idx].bit_not_p : info->bit_not_p;\n+      if (!bit_not_p)\n+\tcontinue;\n+      /* Clear regions with bit_not_p and invert afterwards, rather than\n+\t clear regions with !bit_not_p, so that gaps in between stores aren't\n+\t set in the mask.  */\n+      unsigned HOST_WIDE_INT bitsize = info->bitsize;\n+      unsigned int pos_in_buffer = 0;\n+      if (info->bitpos < try_bitpos)\n+\t{\n+\t  gcc_assert (info->bitpos + bitsize > try_bitpos);\n+\t  bitsize -= (try_bitpos - info->bitpos);\n+\t}\n+      else\n+\tpos_in_buffer = info->bitpos - try_bitpos;\n+      if (pos_in_buffer + bitsize > split_store->size)\n+\tbitsize = split_store->size - pos_in_buffer;\n+      unsigned char *p = buf + (pos_in_buffer / BITS_PER_UNIT);\n+      if (BYTES_BIG_ENDIAN)\n+\tclear_bit_region_be (p, (BITS_PER_UNIT - 1\n+\t\t\t\t - (pos_in_buffer % BITS_PER_UNIT)), bitsize);\n+      else\n+\tclear_bit_region (p, pos_in_buffer % BITS_PER_UNIT, bitsize);\n+    }\n+  for (unsigned int i = 0; i < buf_size; ++i)\n+    buf[i] = ~buf[i];\n+  mask = native_interpret_expr (int_type, buf, buf_size);\n+  return BIT_XOR_EXPR;\n+}\n+\n /* Given a merged store group GROUP output the widened version of it.\n    The store chain is against the base object BASE.\n    Try store sizes of at most MAX_STORE_BITSIZE bits wide and don't output\n@@ -1899,10 +1979,13 @@ imm_store_chain_info::output_merged_store (merged_store_group *group)\n \t\t      gimple_seq_add_stmt_without_update (&seq, stmt);\n \t\t    }\n \t\t  ops[j] = gimple_assign_lhs (stmt);\n-\t\t  if (op.bit_not_p)\n+\t\t  tree xor_mask;\n+\t\t  enum tree_code inv_op\n+\t\t    = invert_op (split_store, j, int_type, xor_mask);\n+\t\t  if (inv_op != NOP_EXPR)\n \t\t    {\n \t\t      stmt = gimple_build_assign (make_ssa_name (int_type),\n-\t\t\t\t\t\t  BIT_NOT_EXPR, ops[j]);\n+\t\t\t\t\t\t  inv_op, ops[j], xor_mask);\n \t\t      gimple_set_location (stmt, load_loc);\n \t\t      ops[j] = gimple_assign_lhs (stmt);\n \n@@ -1952,10 +2035,13 @@ imm_store_chain_info::output_merged_store (merged_store_group *group)\n \t      else\n \t\tgimple_seq_add_stmt_without_update (&seq, stmt);\n \t      src = gimple_assign_lhs (stmt);\n-\t      if (split_store->orig_stores[0]->bit_not_p)\n+\t      tree xor_mask;\n+\t      enum tree_code inv_op;\n+\t      inv_op = invert_op (split_store, 2, int_type, xor_mask);\n+\t      if (inv_op != NOP_EXPR)\n \t\t{\n \t\t  stmt = gimple_build_assign (make_ssa_name (int_type),\n-\t\t\t\t\t      BIT_NOT_EXPR, src);\n+\t\t\t\t\t      inv_op, src, xor_mask);\n \t\t  gimple_set_location (stmt, bit_loc);\n \t\t  if (load_addr[1] == NULL_TREE && gsi_bb (load_gsi[0]))\n \t\t    gimple_seq_add_stmt_without_update (&load_seq[0], stmt);"}, {"sha": "76fd1bace1c0400165fed6dc09409b699fe995e1", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6", "patch": "@@ -1,5 +1,8 @@\n 2017-11-13  Jakub Jelinek  <jakub@redhat.com>\n \n+\tPR tree-optimization/78821\n+\t* gcc.dg/store_merging_15.c: New test.\n+\n \tPR tree-optimization/82954\n \t* gcc.c-torture/execute/pr82954.c: New test.\n "}, {"sha": "57075ebea28257ab13eddcf38e51104944bd3829", "filename": "gcc/testsuite/gcc.dg/store_merging_15.c", "status": "added", "additions": 56, "deletions": 0, "changes": 56, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_15.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_15.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_15.c?ref=a6fbd15424044457d5c283e5fd2d0d1bb0dcfbb6", "patch": "@@ -0,0 +1,56 @@\n+/* { dg-do compile } */\n+/* { dg-require-effective-target store_merge } */\n+/* { dg-options \"-O2 -fdump-tree-store-merging\" } */\n+\n+struct S { unsigned char a, b; unsigned short c; unsigned char d, e, f, g; unsigned long long h; };\n+\n+__attribute__((noipa)) void\n+f1 (struct S *__restrict p, struct S *__restrict q)\n+{\n+  p->a = ~q->a;\n+  p->b = q->b;\n+  p->c = ~q->c;\n+  p->d = ~q->d;\n+  p->e = q->e;\n+  p->f = ~q->f;\n+  p->g = ~q->g;\n+}\n+\n+__attribute__((noipa)) void\n+f2 (struct S *__restrict p, struct S *__restrict q)\n+{\n+  p->a = ~(unsigned char) (p->a & q->a);\n+  p->b = ((unsigned char) ~p->b) & q->b;\n+  p->c = p->c & (unsigned short) ~q->c;\n+  p->d = p->d & q->d;\n+  p->e = p->e & (unsigned char) ~q->e;\n+  p->f = p->f & (unsigned char) ~q->f;\n+  p->g = ~(unsigned char) (p->g & q->g);\n+}\n+\n+struct S s = { 20, 21, 22, 23, 24, 25, 26, 27 };\n+struct S u = { 28, 29, 30, 31, 32, 33, 34, 35 };\n+struct S v = { 36, 37, 38, 39, 40, 41, 42, 43 };\n+\n+int\n+main ()\n+{\n+  asm volatile (\"\" : : : \"memory\");\n+  f1 (&s, &u);\n+  asm volatile (\"\" : : : \"memory\");\n+  if (s.a != (unsigned char) ~28 || s.b != 29\n+      || s.c != (unsigned short) ~30 || s.d != (unsigned char) ~31\n+      || s.e != 32 || s.f != (unsigned char) ~33 || s.g != (unsigned char) ~34\n+      || s.h != 27)\n+    __builtin_abort ();\n+  f2 (&u, &v);\n+  asm volatile (\"\" : : : \"memory\");\n+  if (u.a != (unsigned char) ~(28 & 36) || u.b != (((unsigned char) ~29) & 37)\n+      || u.c != (30 & (unsigned short) ~38) || u.d != (31 & 39)\n+      || u.e != (32 & (unsigned char) ~40) || u.f != (33 & (unsigned char) ~41)\n+      || u.g != (unsigned char) ~(34 & 42) || u.h != 35)\n+    __builtin_abort ();\n+  return 0;\n+}\n+\n+/* { dg-final { scan-tree-dump-times \"Merging successful\" 2 \"store-merging\" } } */"}]}