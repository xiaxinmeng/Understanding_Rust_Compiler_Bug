{"sha": "ba6aa47470550065d1ff8a8acb40654cdc85fbd9", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YmE2YWE0NzQ3MDU1MDA2NWQxZmY4YThhY2I0MDY1NGNkYzg1ZmJkOQ==", "commit": {"author": {"name": "Bill Schmidt", "email": "wschmidt@linux.ibm.com", "date": "2021-08-11T19:56:26Z"}, "committer": {"name": "Bill Schmidt", "email": "wschmidt@linux.ibm.com", "date": "2021-08-11T21:34:23Z"}, "message": "rs6000: Add VSX builtins\n\n2021-08-11  Bill Schmidt  <wschmidt@linux.ibm.com>\n\ngcc/\n\t* config/rs6000/rs6000-builtin-new.def: Add vsx stanza.", "tree": {"sha": "b01799f33100084f82f8bc30c1ce2fe88bd8f193", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/b01799f33100084f82f8bc30c1ce2fe88bd8f193"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/ba6aa47470550065d1ff8a8acb40654cdc85fbd9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ba6aa47470550065d1ff8a8acb40654cdc85fbd9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ba6aa47470550065d1ff8a8acb40654cdc85fbd9", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ba6aa47470550065d1ff8a8acb40654cdc85fbd9/comments", "author": null, "committer": null, "parents": [{"sha": "6cc92e946edab03b26f8aaca23064adf664433f9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6cc92e946edab03b26f8aaca23064adf664433f9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6cc92e946edab03b26f8aaca23064adf664433f9"}], "stats": {"total": 857, "additions": 857, "deletions": 0}, "files": [{"sha": "b5d3570e8dcafde9a65a77b661f881efa7490709", "filename": "gcc/config/rs6000/rs6000-builtin-new.def", "status": "modified", "additions": 857, "deletions": 0, "changes": 857, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ba6aa47470550065d1ff8a8acb40654cdc85fbd9/gcc%2Fconfig%2Frs6000%2Frs6000-builtin-new.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ba6aa47470550065d1ff8a8acb40654cdc85fbd9/gcc%2Fconfig%2Frs6000%2Frs6000-builtin-new.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000-builtin-new.def?ref=ba6aa47470550065d1ff8a8acb40654cdc85fbd9", "patch": "@@ -1028,3 +1028,860 @@\n \n   const vss __builtin_vec_set_v8hi (vss, signed short, const int<3>);\n     VEC_SET_V8HI nothing {set}\n+\n+\n+; VSX builtins.\n+[vsx]\n+  pure vd __builtin_altivec_lvx_v2df (signed long, const void *);\n+    LVX_V2DF altivec_lvx_v2df {ldvec}\n+\n+  pure vsll __builtin_altivec_lvx_v2di (signed long, const void *);\n+    LVX_V2DI altivec_lvx_v2di {ldvec}\n+\n+  pure vd __builtin_altivec_lvxl_v2df (signed long, const void *);\n+    LVXL_V2DF altivec_lvxl_v2df {ldvec}\n+\n+  pure vsll __builtin_altivec_lvxl_v2di (signed long, const void *);\n+    LVXL_V2DI altivec_lvxl_v2di {ldvec}\n+\n+  const vd __builtin_altivec_nabs_v2df (vd);\n+    NABS_V2DF vsx_nabsv2df2 {}\n+\n+  const vsll __builtin_altivec_nabs_v2di (vsll);\n+    NABS_V2DI nabsv2di2 {}\n+\n+  void __builtin_altivec_stvx_v2df (vd, signed long, void *);\n+    STVX_V2DF altivec_stvx_v2df {stvec}\n+\n+  void __builtin_altivec_stvx_v2di (vsll, signed long, void *);\n+    STVX_V2DI altivec_stvx_v2di {stvec}\n+\n+  void __builtin_altivec_stvxl_v2df (vd, signed long, void *);\n+    STVXL_V2DF altivec_stvxl_v2df {stvec}\n+\n+  void __builtin_altivec_stvxl_v2di (vsll, signed long, void *);\n+    STVXL_V2DI altivec_stvxl_v2di {stvec}\n+\n+  const vd __builtin_altivec_vand_v2df (vd, vd);\n+    VAND_V2DF andv2df3 {}\n+\n+  const vsll __builtin_altivec_vand_v2di (vsll, vsll);\n+    VAND_V2DI andv2di3 {}\n+\n+  const vull __builtin_altivec_vand_v2di_uns (vull, vull);\n+    VAND_V2DI_UNS andv2di3 {}\n+\n+  const vd __builtin_altivec_vandc_v2df (vd, vd);\n+    VANDC_V2DF andcv2df3 {}\n+\n+  const vsll __builtin_altivec_vandc_v2di (vsll, vsll);\n+    VANDC_V2DI andcv2di3 {}\n+\n+  const vull __builtin_altivec_vandc_v2di_uns (vull, vull);\n+    VANDC_V2DI_UNS andcv2di3 {}\n+\n+  const vsll __builtin_altivec_vcmpequd (vull, vull);\n+    VCMPEQUD vector_eqv2di {}\n+\n+  const int __builtin_altivec_vcmpequd_p (int, vsll, vsll);\n+    VCMPEQUD_P vector_eq_v2di_p {pred}\n+\n+  const vsll __builtin_altivec_vcmpgtsd (vsll, vsll);\n+    VCMPGTSD vector_gtv2di {}\n+\n+  const int __builtin_altivec_vcmpgtsd_p (int, vsll, vsll);\n+    VCMPGTSD_P vector_gt_v2di_p {pred}\n+\n+  const vsll __builtin_altivec_vcmpgtud (vull, vull);\n+    VCMPGTUD vector_gtuv2di {}\n+\n+  const int __builtin_altivec_vcmpgtud_p (int, vsll, vsll);\n+    VCMPGTUD_P vector_gtu_v2di_p {pred}\n+\n+  const vd __builtin_altivec_vnor_v2df (vd, vd);\n+    VNOR_V2DF norv2df3 {}\n+\n+  const vsll __builtin_altivec_vnor_v2di (vsll, vsll);\n+    VNOR_V2DI norv2di3 {}\n+\n+  const vull __builtin_altivec_vnor_v2di_uns (vull, vull);\n+    VNOR_V2DI_UNS norv2di3 {}\n+\n+  const vd __builtin_altivec_vor_v2df (vd, vd);\n+    VOR_V2DF iorv2df3 {}\n+\n+  const vsll __builtin_altivec_vor_v2di (vsll, vsll);\n+    VOR_V2DI iorv2di3 {}\n+\n+  const vull __builtin_altivec_vor_v2di_uns (vull, vull);\n+    VOR_V2DI_UNS iorv2di3 {}\n+\n+  const vd __builtin_altivec_vperm_2df (vd, vd, vuc);\n+    VPERM_2DF altivec_vperm_v2df {}\n+\n+  const vsll __builtin_altivec_vperm_2di (vsll, vsll, vuc);\n+    VPERM_2DI altivec_vperm_v2di {}\n+\n+  const vull __builtin_altivec_vperm_2di_uns (vull, vull, vuc);\n+    VPERM_2DI_UNS altivec_vperm_v2di_uns {}\n+\n+  const vd __builtin_altivec_vreve_v2df (vd);\n+    VREVE_V2DF altivec_vrevev2df2 {}\n+\n+  const vsll __builtin_altivec_vreve_v2di (vsll);\n+    VREVE_V2DI altivec_vrevev2di2 {}\n+\n+  const vd __builtin_altivec_vsel_2df (vd, vd, vd);\n+    VSEL_2DF vector_select_v2df {}\n+\n+  const vsll __builtin_altivec_vsel_2di (vsll, vsll, vsll);\n+    VSEL_2DI_B vector_select_v2di {}\n+\n+  const vull __builtin_altivec_vsel_2di_uns (vull, vull, vull);\n+    VSEL_2DI_UNS vector_select_v2di_uns {}\n+\n+  const vd __builtin_altivec_vsldoi_2df (vd, vd, const int<4>);\n+    VSLDOI_2DF altivec_vsldoi_v2df {}\n+\n+  const vsll __builtin_altivec_vsldoi_2di (vsll, vsll, const int<4>);\n+    VSLDOI_2DI altivec_vsldoi_v2di {}\n+\n+  const vd __builtin_altivec_vxor_v2df (vd, vd);\n+    VXOR_V2DF xorv2df3 {}\n+\n+  const vsll __builtin_altivec_vxor_v2di (vsll, vsll);\n+    VXOR_V2DI xorv2di3 {}\n+\n+  const vull __builtin_altivec_vxor_v2di_uns (vull, vull);\n+    VXOR_V2DI_UNS xorv2di3 {}\n+\n+  const signed __int128 __builtin_vec_ext_v1ti (vsq, signed int);\n+    VEC_EXT_V1TI nothing {extract}\n+\n+  const double __builtin_vec_ext_v2df (vd, signed int);\n+    VEC_EXT_V2DF nothing {extract}\n+\n+  const signed long long __builtin_vec_ext_v2di (vsll, signed int);\n+    VEC_EXT_V2DI nothing {extract}\n+\n+  const vsq __builtin_vec_init_v1ti (signed __int128);\n+    VEC_INIT_V1TI nothing {init}\n+\n+  const vd __builtin_vec_init_v2df (double, double);\n+    VEC_INIT_V2DF nothing {init}\n+\n+  const vsll __builtin_vec_init_v2di (signed long long, signed long long);\n+    VEC_INIT_V2DI nothing {init}\n+\n+  const vsq __builtin_vec_set_v1ti (vsq, signed __int128, const int<0,0>);\n+    VEC_SET_V1TI nothing {set}\n+\n+  const vd __builtin_vec_set_v2df (vd, double, const int<1>);\n+    VEC_SET_V2DF nothing {set}\n+\n+  const vsll __builtin_vec_set_v2di (vsll, signed long long, const int<1>);\n+    VEC_SET_V2DI nothing {set}\n+\n+  const vsc __builtin_vsx_cmpge_16qi (vsc, vsc);\n+    CMPGE_16QI vector_nltv16qi {}\n+\n+  const vsll __builtin_vsx_cmpge_2di (vsll, vsll);\n+    CMPGE_2DI vector_nltv2di {}\n+\n+  const vsi __builtin_vsx_cmpge_4si (vsi, vsi);\n+    CMPGE_4SI vector_nltv4si {}\n+\n+  const vss __builtin_vsx_cmpge_8hi (vss, vss);\n+    CMPGE_8HI vector_nltv8hi {}\n+\n+  const vsc __builtin_vsx_cmpge_u16qi (vuc, vuc);\n+    CMPGE_U16QI vector_nltuv16qi {}\n+\n+  const vsll __builtin_vsx_cmpge_u2di (vull, vull);\n+    CMPGE_U2DI vector_nltuv2di {}\n+\n+  const vsi __builtin_vsx_cmpge_u4si (vui, vui);\n+    CMPGE_U4SI vector_nltuv4si {}\n+\n+  const vss __builtin_vsx_cmpge_u8hi (vus, vus);\n+    CMPGE_U8HI vector_nltuv8hi {}\n+\n+  const vsc __builtin_vsx_cmple_16qi (vsc, vsc);\n+    CMPLE_16QI vector_ngtv16qi {}\n+\n+  const vsll __builtin_vsx_cmple_2di (vsll, vsll);\n+    CMPLE_2DI vector_ngtv2di {}\n+\n+  const vsi __builtin_vsx_cmple_4si (vsi, vsi);\n+    CMPLE_4SI vector_ngtv4si {}\n+\n+  const vss __builtin_vsx_cmple_8hi (vss, vss);\n+    CMPLE_8HI vector_ngtv8hi {}\n+\n+  const vsc __builtin_vsx_cmple_u16qi (vsc, vsc);\n+    CMPLE_U16QI vector_ngtuv16qi {}\n+\n+  const vsll __builtin_vsx_cmple_u2di (vsll, vsll);\n+    CMPLE_U2DI vector_ngtuv2di {}\n+\n+  const vsi __builtin_vsx_cmple_u4si (vsi, vsi);\n+    CMPLE_U4SI vector_ngtuv4si {}\n+\n+  const vss __builtin_vsx_cmple_u8hi (vss, vss);\n+    CMPLE_U8HI vector_ngtuv8hi {}\n+\n+  const vd __builtin_vsx_concat_2df (double, double);\n+    CONCAT_2DF vsx_concat_v2df {}\n+\n+  const vsll __builtin_vsx_concat_2di (signed long long, signed long long);\n+    CONCAT_2DI vsx_concat_v2di {}\n+\n+  const vd __builtin_vsx_cpsgndp (vd, vd);\n+    CPSGNDP vector_copysignv2df3 {}\n+\n+  const vf __builtin_vsx_cpsgnsp (vf, vf);\n+    CPSGNSP vector_copysignv4sf3 {}\n+\n+  const vsll __builtin_vsx_div_2di (vsll, vsll);\n+    DIV_V2DI vsx_div_v2di {}\n+\n+  const vd __builtin_vsx_doublee_v4sf (vf);\n+    DOUBLEE_V4SF doubleev4sf2 {}\n+\n+  const vd __builtin_vsx_doublee_v4si (vsi);\n+    DOUBLEE_V4SI doubleev4si2 {}\n+\n+  const vd __builtin_vsx_doubleh_v4sf (vf);\n+    DOUBLEH_V4SF doublehv4sf2 {}\n+\n+  const vd __builtin_vsx_doubleh_v4si (vsi);\n+    DOUBLEH_V4SI doublehv4si2 {}\n+\n+  const vd __builtin_vsx_doublel_v4sf (vf);\n+    DOUBLEL_V4SF doublelv4sf2 {}\n+\n+  const vd __builtin_vsx_doublel_v4si (vsi);\n+    DOUBLEL_V4SI doublelv4si2 {}\n+\n+  const vd __builtin_vsx_doubleo_v4sf (vf);\n+    DOUBLEO_V4SF doubleov4sf2 {}\n+\n+  const vd __builtin_vsx_doubleo_v4si (vsi);\n+    DOUBLEO_V4SI doubleov4si2 {}\n+\n+  const vf __builtin_vsx_floate_v2df (vd);\n+    FLOATE_V2DF floatev2df {}\n+\n+  const vf __builtin_vsx_floate_v2di (vsll);\n+    FLOATE_V2DI floatev2di {}\n+\n+  const vf __builtin_vsx_floato_v2df (vd);\n+    FLOATO_V2DF floatov2df {}\n+\n+  const vf __builtin_vsx_floato_v2di (vsll);\n+    FLOATO_V2DI floatov2di {}\n+\n+  pure vsq __builtin_vsx_ld_elemrev_v1ti (signed long, const void *);\n+    LD_ELEMREV_V1TI vsx_ld_elemrev_v1ti {ldvec,endian}\n+\n+  pure vd __builtin_vsx_ld_elemrev_v2df (signed long, const void *);\n+    LD_ELEMREV_V2DF vsx_ld_elemrev_v2df {ldvec,endian}\n+\n+  pure vsll __builtin_vsx_ld_elemrev_v2di (signed long, const void *);\n+    LD_ELEMREV_V2DI vsx_ld_elemrev_v2di {ldvec,endian}\n+\n+  pure vf __builtin_vsx_ld_elemrev_v4sf (signed long, const void *);\n+    LD_ELEMREV_V4SF vsx_ld_elemrev_v4sf {ldvec,endian}\n+\n+  pure vsi __builtin_vsx_ld_elemrev_v4si (signed long, const void *);\n+    LD_ELEMREV_V4SI vsx_ld_elemrev_v4si {ldvec,endian}\n+\n+  pure vss __builtin_vsx_ld_elemrev_v8hi (signed long, const void *);\n+    LD_ELEMREV_V8HI vsx_ld_elemrev_v8hi {ldvec,endian}\n+\n+  pure vsc __builtin_vsx_ld_elemrev_v16qi (signed long, const void *);\n+    LD_ELEMREV_V16QI vsx_ld_elemrev_v16qi {ldvec,endian}\n+\n+; TODO: There is apparent intent in rs6000-builtin.def to have\n+; RS6000_BTC_SPECIAL processing for LXSDX, LXVDSX, and STXSDX, but there are\n+; no def_builtin calls for any of them.  At some point, we may want to add a\n+; set of built-ins for whichever vector types make sense for these.\n+\n+  pure vsq __builtin_vsx_lxvd2x_v1ti (signed long, const void *);\n+    LXVD2X_V1TI vsx_load_v1ti {ldvec}\n+\n+  pure vd __builtin_vsx_lxvd2x_v2df (signed long, const void *);\n+    LXVD2X_V2DF vsx_load_v2df {ldvec}\n+\n+  pure vsll __builtin_vsx_lxvd2x_v2di (signed long, const void *);\n+    LXVD2X_V2DI vsx_load_v2di {ldvec}\n+\n+  pure vsc __builtin_vsx_lxvw4x_v16qi (signed long, const void *);\n+    LXVW4X_V16QI vsx_load_v16qi {ldvec}\n+\n+  pure vf __builtin_vsx_lxvw4x_v4sf (signed long, const void *);\n+    LXVW4X_V4SF vsx_load_v4sf {ldvec}\n+\n+  pure vsi __builtin_vsx_lxvw4x_v4si (signed long, const void *);\n+    LXVW4X_V4SI vsx_load_v4si {ldvec}\n+\n+  pure vss __builtin_vsx_lxvw4x_v8hi (signed long, const void *);\n+    LXVW4X_V8HI vsx_load_v8hi {ldvec}\n+\n+  const vd __builtin_vsx_mergeh_2df (vd, vd);\n+    VEC_MERGEH_V2DF vsx_mergeh_v2df {}\n+\n+  const vsll __builtin_vsx_mergeh_2di (vsll, vsll);\n+    VEC_MERGEH_V2DI vsx_mergeh_v2di {}\n+\n+  const vd __builtin_vsx_mergel_2df (vd, vd);\n+    VEC_MERGEL_V2DF vsx_mergel_v2df {}\n+\n+  const vsll __builtin_vsx_mergel_2di (vsll, vsll);\n+    VEC_MERGEL_V2DI vsx_mergel_v2di {}\n+\n+  const vsll __builtin_vsx_mul_2di (vsll, vsll);\n+    MUL_V2DI vsx_mul_v2di {}\n+\n+  const vsq __builtin_vsx_set_1ti (vsq, signed __int128, const int<0,0>);\n+    SET_1TI vsx_set_v1ti {set}\n+\n+  const vd __builtin_vsx_set_2df (vd, double, const int<0,1>);\n+    SET_2DF vsx_set_v2df {set}\n+\n+  const vsll __builtin_vsx_set_2di (vsll, signed long long, const int<0,1>);\n+    SET_2DI vsx_set_v2di {set}\n+\n+  const vd __builtin_vsx_splat_2df (double);\n+    SPLAT_2DF vsx_splat_v2df {}\n+\n+  const vsll __builtin_vsx_splat_2di (signed long long);\n+    SPLAT_2DI vsx_splat_v2di {}\n+\n+  void __builtin_vsx_st_elemrev_v1ti (vsq, signed long, void *);\n+    ST_ELEMREV_V1TI vsx_st_elemrev_v1ti {stvec,endian}\n+\n+  void __builtin_vsx_st_elemrev_v2df (vd, signed long, void *);\n+    ST_ELEMREV_V2DF vsx_st_elemrev_v2df {stvec,endian}\n+\n+  void __builtin_vsx_st_elemrev_v2di (vsll, signed long, void *);\n+    ST_ELEMREV_V2DI vsx_st_elemrev_v2di {stvec,endian}\n+\n+  void __builtin_vsx_st_elemrev_v4sf (vf, signed long, void *);\n+    ST_ELEMREV_V4SF vsx_st_elemrev_v4sf {stvec,endian}\n+\n+  void __builtin_vsx_st_elemrev_v4si (vsi, signed long, void *);\n+    ST_ELEMREV_V4SI vsx_st_elemrev_v4si {stvec,endian}\n+\n+  void __builtin_vsx_st_elemrev_v8hi (vss, signed long, void *);\n+    ST_ELEMREV_V8HI vsx_st_elemrev_v8hi {stvec,endian}\n+\n+  void __builtin_vsx_st_elemrev_v16qi (vsc, signed long, void *);\n+    ST_ELEMREV_V16QI vsx_st_elemrev_v16qi {stvec,endian}\n+\n+  void __builtin_vsx_stxvd2x_v1ti (vsq, signed long, void *);\n+    STXVD2X_V1TI vsx_store_v1ti {stvec}\n+\n+  void __builtin_vsx_stxvd2x_v2df (vd, signed long, void *);\n+    STXVD2X_V2DF vsx_store_v2df {stvec}\n+\n+  void __builtin_vsx_stxvd2x_v2di (vsll, signed long, void *);\n+    STXVD2X_V2DI vsx_store_v2di {stvec}\n+\n+  void __builtin_vsx_stxvw4x_v4sf (vf, signed long, void *);\n+    STXVW4X_V4SF vsx_store_v4sf {stvec}\n+\n+  void __builtin_vsx_stxvw4x_v4si (vsi, signed long, void *);\n+    STXVW4X_V4SI vsx_store_v4si {stvec}\n+\n+  void __builtin_vsx_stxvw4x_v8hi (vss, signed long, void *);\n+    STXVW4X_V8HI vsx_store_v8hi {stvec}\n+\n+  void __builtin_vsx_stxvw4x_v16qi (vsc, signed long, void *);\n+    STXVW4X_V16QI vsx_store_v16qi {stvec}\n+\n+  const vull __builtin_vsx_udiv_2di (vull, vull);\n+    UDIV_V2DI vsx_udiv_v2di {}\n+\n+  const vd __builtin_vsx_uns_doublee_v4si (vsi);\n+    UNS_DOUBLEE_V4SI unsdoubleev4si2 {}\n+\n+  const vd __builtin_vsx_uns_doubleh_v4si (vsi);\n+    UNS_DOUBLEH_V4SI unsdoublehv4si2 {}\n+\n+  const vd __builtin_vsx_uns_doublel_v4si (vsi);\n+    UNS_DOUBLEL_V4SI unsdoublelv4si2 {}\n+\n+  const vd __builtin_vsx_uns_doubleo_v4si (vsi);\n+    UNS_DOUBLEO_V4SI unsdoubleov4si2 {}\n+\n+  const vf __builtin_vsx_uns_floate_v2di (vsll);\n+    UNS_FLOATE_V2DI unsfloatev2di {}\n+\n+  const vf __builtin_vsx_uns_floato_v2di (vsll);\n+    UNS_FLOATO_V2DI unsfloatov2di {}\n+\n+; These are duplicates of __builtin_altivec_* counterparts, and are being\n+; kept for backwards compatibility.  The reason for their existence is\n+; unclear.  TODO: Consider deprecation/removal at some point.\n+  const vsc __builtin_vsx_vperm_16qi (vsc, vsc, vuc);\n+    VPERM_16QI_X altivec_vperm_v16qi {}\n+\n+  const vuc __builtin_vsx_vperm_16qi_uns (vuc, vuc, vuc);\n+    VPERM_16QI_UNS_X altivec_vperm_v16qi_uns {}\n+\n+  const vsq __builtin_vsx_vperm_1ti (vsq, vsq, vsc);\n+    VPERM_1TI_X altivec_vperm_v1ti {}\n+\n+  const vsq __builtin_vsx_vperm_1ti_uns (vsq, vsq, vsc);\n+    VPERM_1TI_UNS_X altivec_vperm_v1ti_uns {}\n+\n+  const vd __builtin_vsx_vperm_2df (vd, vd, vuc);\n+    VPERM_2DF_X altivec_vperm_v2df {}\n+\n+  const vsll __builtin_vsx_vperm_2di (vsll, vsll, vuc);\n+    VPERM_2DI_X altivec_vperm_v2di {}\n+\n+  const vull __builtin_vsx_vperm_2di_uns (vull, vull, vuc);\n+    VPERM_2DI_UNS_X altivec_vperm_v2di_uns {}\n+\n+  const vf __builtin_vsx_vperm_4sf (vf, vf, vuc);\n+    VPERM_4SF_X altivec_vperm_v4sf {}\n+\n+  const vsi __builtin_vsx_vperm_4si (vsi, vsi, vuc);\n+    VPERM_4SI_X altivec_vperm_v4si {}\n+\n+  const vui __builtin_vsx_vperm_4si_uns (vui, vui, vuc);\n+    VPERM_4SI_UNS_X altivec_vperm_v4si_uns {}\n+\n+  const vss __builtin_vsx_vperm_8hi (vss, vss, vuc);\n+    VPERM_8HI_X altivec_vperm_v8hi {}\n+\n+  const vus __builtin_vsx_vperm_8hi_uns (vus, vus, vuc);\n+    VPERM_8HI_UNS_X altivec_vperm_v8hi_uns {}\n+\n+  const vsll __builtin_vsx_vsigned_v2df (vd);\n+    VEC_VSIGNED_V2DF vsx_xvcvdpsxds {}\n+\n+  const vsi __builtin_vsx_vsigned_v4sf (vf);\n+    VEC_VSIGNED_V4SF vsx_xvcvspsxws {}\n+\n+  const vsi __builtin_vsx_vsignede_v2df (vd);\n+    VEC_VSIGNEDE_V2DF vsignede_v2df {}\n+\n+  const vsi __builtin_vsx_vsignedo_v2df (vd);\n+    VEC_VSIGNEDO_V2DF vsignedo_v2df {}\n+\n+  const vsll __builtin_vsx_vunsigned_v2df (vd);\n+    VEC_VUNSIGNED_V2DF vsx_xvcvdpsxds {}\n+\n+  const vsi __builtin_vsx_vunsigned_v4sf (vf);\n+    VEC_VUNSIGNED_V4SF vsx_xvcvspsxws {}\n+\n+  const vsi __builtin_vsx_vunsignede_v2df (vd);\n+    VEC_VUNSIGNEDE_V2DF vunsignede_v2df {}\n+\n+  const vsi __builtin_vsx_vunsignedo_v2df (vd);\n+    VEC_VUNSIGNEDO_V2DF vunsignedo_v2df {}\n+\n+  const vf __builtin_vsx_xscvdpsp (double);\n+    XSCVDPSP vsx_xscvdpsp {}\n+\n+  const double __builtin_vsx_xscvspdp (vf);\n+    XSCVSPDP vsx_xscvspdp {}\n+\n+  const double __builtin_vsx_xsmaxdp (double, double);\n+    XSMAXDP smaxdf3 {}\n+\n+  const double __builtin_vsx_xsmindp (double, double);\n+    XSMINDP smindf3 {}\n+\n+  const double __builtin_vsx_xsrdpi (double);\n+    XSRDPI vsx_xsrdpi {}\n+\n+  const double __builtin_vsx_xsrdpic (double);\n+    XSRDPIC vsx_xsrdpic {}\n+\n+  const double __builtin_vsx_xsrdpim (double);\n+    XSRDPIM floordf2 {}\n+\n+  const double __builtin_vsx_xsrdpip (double);\n+    XSRDPIP ceildf2 {}\n+\n+  const double __builtin_vsx_xsrdpiz (double);\n+    XSRDPIZ btruncdf2 {}\n+\n+  const signed int __builtin_vsx_xstdivdp_fe (double, double);\n+    XSTDIVDP_FE vsx_tdivdf3_fe {}\n+\n+  const signed int __builtin_vsx_xstdivdp_fg (double, double);\n+    XSTDIVDP_FG vsx_tdivdf3_fg {}\n+\n+  const signed int __builtin_vsx_xstsqrtdp_fe (double);\n+    XSTSQRTDP_FE vsx_tsqrtdf2_fe {}\n+\n+  const signed int __builtin_vsx_xstsqrtdp_fg (double);\n+    XSTSQRTDP_FG vsx_tsqrtdf2_fg {}\n+\n+  const vd __builtin_vsx_xvabsdp (vd);\n+    XVABSDP absv2df2 {}\n+\n+  const vf __builtin_vsx_xvabssp (vf);\n+    XVABSSP absv4sf2 {}\n+\n+  fpmath vd __builtin_vsx_xvadddp (vd, vd);\n+    XVADDDP addv2df3 {}\n+\n+  fpmath vf __builtin_vsx_xvaddsp (vf, vf);\n+    XVADDSP addv4sf3 {}\n+\n+  const vd __builtin_vsx_xvcmpeqdp (vd, vd);\n+    XVCMPEQDP vector_eqv2df {}\n+\n+  const signed int __builtin_vsx_xvcmpeqdp_p (signed int, vd, vd);\n+    XVCMPEQDP_P vector_eq_v2df_p {pred}\n+\n+  const vf __builtin_vsx_xvcmpeqsp (vf, vf);\n+    XVCMPEQSP vector_eqv4sf {}\n+\n+  const signed int __builtin_vsx_xvcmpeqsp_p (signed int, vf, vf);\n+    XVCMPEQSP_P vector_eq_v4sf_p {pred}\n+\n+  const vd __builtin_vsx_xvcmpgedp (vd, vd);\n+    XVCMPGEDP vector_gev2df {}\n+\n+  const signed int __builtin_vsx_xvcmpgedp_p (signed int, vd, vd);\n+    XVCMPGEDP_P vector_ge_v2df_p {pred}\n+\n+  const vf __builtin_vsx_xvcmpgesp (vf, vf);\n+    XVCMPGESP vector_gev4sf {}\n+\n+  const signed int __builtin_vsx_xvcmpgesp_p (signed int, vf, vf);\n+    XVCMPGESP_P vector_ge_v4sf_p {pred}\n+\n+  const vd __builtin_vsx_xvcmpgtdp (vd, vd);\n+    XVCMPGTDP vector_gtv2df {}\n+\n+  const signed int __builtin_vsx_xvcmpgtdp_p (signed int, vd, vd);\n+    XVCMPGTDP_P vector_gt_v2df_p {pred}\n+\n+  const vf __builtin_vsx_xvcmpgtsp (vf, vf);\n+    XVCMPGTSP vector_gtv4sf {}\n+\n+  const signed int __builtin_vsx_xvcmpgtsp_p (signed int, vf, vf);\n+    XVCMPGTSP_P vector_gt_v4sf_p {pred}\n+\n+  const vf __builtin_vsx_xvcvdpsp (vd);\n+    XVCVDPSP vsx_xvcvdpsp {}\n+\n+  const vsll __builtin_vsx_xvcvdpsxds (vd);\n+    XVCVDPSXDS vsx_fix_truncv2dfv2di2 {}\n+\n+  const vsll __builtin_vsx_xvcvdpsxds_scale (vd, const int);\n+    XVCVDPSXDS_SCALE vsx_xvcvdpsxds_scale {}\n+\n+  const vsi __builtin_vsx_xvcvdpsxws (vd);\n+    XVCVDPSXWS vsx_xvcvdpsxws {}\n+\n+  const vsll __builtin_vsx_xvcvdpuxds (vd);\n+    XVCVDPUXDS vsx_fixuns_truncv2dfv2di2 {}\n+\n+  const vsll __builtin_vsx_xvcvdpuxds_scale (vd, const int);\n+    XVCVDPUXDS_SCALE vsx_xvcvdpuxds_scale {}\n+\n+  const vull __builtin_vsx_xvcvdpuxds_uns (vd);\n+    XVCVDPUXDS_UNS vsx_fixuns_truncv2dfv2di2 {}\n+\n+  const vsi __builtin_vsx_xvcvdpuxws (vd);\n+    XVCVDPUXWS vsx_xvcvdpuxws {}\n+\n+  const vd __builtin_vsx_xvcvspdp (vf);\n+    XVCVSPDP vsx_xvcvspdp {}\n+\n+  const vsll __builtin_vsx_xvcvspsxds (vf);\n+    XVCVSPSXDS vsx_xvcvspsxds {}\n+\n+  const vsi __builtin_vsx_xvcvspsxws (vf);\n+    XVCVSPSXWS vsx_fix_truncv4sfv4si2 {}\n+\n+  const vsll __builtin_vsx_xvcvspuxds (vf);\n+    XVCVSPUXDS vsx_xvcvspuxds {}\n+\n+  const vsi __builtin_vsx_xvcvspuxws (vf);\n+    XVCVSPUXWS vsx_fixuns_truncv4sfv4si2 {}\n+\n+  const vd __builtin_vsx_xvcvsxddp (vsll);\n+    XVCVSXDDP vsx_floatv2div2df2 {}\n+\n+  const vd __builtin_vsx_xvcvsxddp_scale (vsll, const int<5>);\n+    XVCVSXDDP_SCALE vsx_xvcvsxddp_scale {}\n+\n+  const vf __builtin_vsx_xvcvsxdsp (vsll);\n+    XVCVSXDSP vsx_xvcvsxdsp {}\n+\n+  const vd __builtin_vsx_xvcvsxwdp (vsi);\n+    XVCVSXWDP vsx_xvcvsxwdp {}\n+\n+  const vf __builtin_vsx_xvcvsxwsp (vsi);\n+    XVCVSXWSP vsx_floatv4siv4sf2 {}\n+\n+  const vd __builtin_vsx_xvcvuxddp (vsll);\n+    XVCVUXDDP vsx_floatunsv2div2df2 {}\n+\n+  const vd __builtin_vsx_xvcvuxddp_scale (vsll, const int<5>);\n+    XVCVUXDDP_SCALE vsx_xvcvuxddp_scale {}\n+\n+  const vd __builtin_vsx_xvcvuxddp_uns (vull);\n+    XVCVUXDDP_UNS vsx_floatunsv2div2df2 {}\n+\n+  const vf __builtin_vsx_xvcvuxdsp (vull);\n+    XVCVUXDSP vsx_xvcvuxdsp {}\n+\n+  const vd __builtin_vsx_xvcvuxwdp (vsi);\n+    XVCVUXWDP vsx_xvcvuxwdp {}\n+\n+  const vf __builtin_vsx_xvcvuxwsp (vsi);\n+    XVCVUXWSP vsx_floatunsv4siv4sf2 {}\n+\n+  fpmath vd __builtin_vsx_xvdivdp (vd, vd);\n+    XVDIVDP divv2df3 {}\n+\n+  fpmath vf __builtin_vsx_xvdivsp (vf, vf);\n+    XVDIVSP divv4sf3 {}\n+\n+  const vd __builtin_vsx_xvmadddp (vd, vd, vd);\n+    XVMADDDP fmav2df4 {}\n+\n+  const vf __builtin_vsx_xvmaddsp (vf, vf, vf);\n+    XVMADDSP fmav4sf4 {}\n+\n+  const vd __builtin_vsx_xvmaxdp (vd, vd);\n+    XVMAXDP smaxv2df3 {}\n+\n+  const vf __builtin_vsx_xvmaxsp (vf, vf);\n+    XVMAXSP smaxv4sf3 {}\n+\n+  const vd __builtin_vsx_xvmindp (vd, vd);\n+    XVMINDP sminv2df3 {}\n+\n+  const vf __builtin_vsx_xvminsp (vf, vf);\n+    XVMINSP sminv4sf3 {}\n+\n+  const vd __builtin_vsx_xvmsubdp (vd, vd, vd);\n+    XVMSUBDP fmsv2df4 {}\n+\n+  const vf __builtin_vsx_xvmsubsp (vf, vf, vf);\n+    XVMSUBSP fmsv4sf4 {}\n+\n+  fpmath vd __builtin_vsx_xvmuldp (vd, vd);\n+    XVMULDP mulv2df3 {}\n+\n+  fpmath vf __builtin_vsx_xvmulsp (vf, vf);\n+    XVMULSP mulv4sf3 {}\n+\n+  const vd __builtin_vsx_xvnabsdp (vd);\n+    XVNABSDP vsx_nabsv2df2 {}\n+\n+  const vf __builtin_vsx_xvnabssp (vf);\n+    XVNABSSP vsx_nabsv4sf2 {}\n+\n+  const vd __builtin_vsx_xvnegdp (vd);\n+    XVNEGDP negv2df2 {}\n+\n+  const vf __builtin_vsx_xvnegsp (vf);\n+    XVNEGSP negv4sf2 {}\n+\n+  const vd __builtin_vsx_xvnmadddp (vd, vd, vd);\n+    XVNMADDDP nfmav2df4 {}\n+\n+  const vf __builtin_vsx_xvnmaddsp (vf, vf, vf);\n+    XVNMADDSP nfmav4sf4 {}\n+\n+  const vd __builtin_vsx_xvnmsubdp (vd, vd, vd);\n+    XVNMSUBDP nfmsv2df4 {}\n+\n+  const vf __builtin_vsx_xvnmsubsp (vf, vf, vf);\n+    XVNMSUBSP nfmsv4sf4 {}\n+\n+  const vd __builtin_vsx_xvrdpi (vd);\n+    XVRDPI vsx_xvrdpi {}\n+\n+  const vd __builtin_vsx_xvrdpic (vd);\n+    XVRDPIC vsx_xvrdpic {}\n+\n+  const vd __builtin_vsx_xvrdpim (vd);\n+    XVRDPIM vsx_floorv2df2 {}\n+\n+  const vd __builtin_vsx_xvrdpip (vd);\n+    XVRDPIP vsx_ceilv2df2 {}\n+\n+  const vd __builtin_vsx_xvrdpiz (vd);\n+    XVRDPIZ vsx_btruncv2df2 {}\n+\n+  fpmath vd __builtin_vsx_xvrecipdivdp (vd, vd);\n+    RECIP_V2DF recipv2df3 {}\n+\n+  fpmath vf __builtin_vsx_xvrecipdivsp (vf, vf);\n+    RECIP_V4SF recipv4sf3 {}\n+\n+  const vd __builtin_vsx_xvredp (vd);\n+    XVREDP vsx_frev2df2 {}\n+\n+  const vf __builtin_vsx_xvresp (vf);\n+    XVRESP vsx_frev4sf2 {}\n+\n+  const vf __builtin_vsx_xvrspi (vf);\n+    XVRSPI vsx_xvrspi {}\n+\n+  const vf __builtin_vsx_xvrspic (vf);\n+    XVRSPIC vsx_xvrspic {}\n+\n+  const vf __builtin_vsx_xvrspim (vf);\n+    XVRSPIM vsx_floorv4sf2 {}\n+\n+  const vf __builtin_vsx_xvrspip (vf);\n+    XVRSPIP vsx_ceilv4sf2 {}\n+\n+  const vf __builtin_vsx_xvrspiz (vf);\n+    XVRSPIZ vsx_btruncv4sf2 {}\n+\n+  const vd __builtin_vsx_xvrsqrtdp (vd);\n+    RSQRT_2DF rsqrtv2df2 {}\n+\n+  const vf __builtin_vsx_xvrsqrtsp (vf);\n+    RSQRT_4SF rsqrtv4sf2 {}\n+\n+  const vd __builtin_vsx_xvrsqrtedp (vd);\n+    XVRSQRTEDP rsqrtev2df2 {}\n+\n+  const vf __builtin_vsx_xvrsqrtesp (vf);\n+    XVRSQRTESP rsqrtev4sf2 {}\n+\n+  const vd __builtin_vsx_xvsqrtdp (vd);\n+    XVSQRTDP sqrtv2df2 {}\n+\n+  const vf __builtin_vsx_xvsqrtsp (vf);\n+    XVSQRTSP sqrtv4sf2 {}\n+\n+  fpmath vd __builtin_vsx_xvsubdp (vd, vd);\n+    XVSUBDP subv2df3 {}\n+\n+  fpmath vf __builtin_vsx_xvsubsp (vf, vf);\n+    XVSUBSP subv4sf3 {}\n+\n+  const signed int __builtin_vsx_xvtdivdp_fe (vd, vd);\n+    XVTDIVDP_FE vsx_tdivv2df3_fe {}\n+\n+  const signed int __builtin_vsx_xvtdivdp_fg (vd, vd);\n+    XVTDIVDP_FG vsx_tdivv2df3_fg {}\n+\n+  const signed int __builtin_vsx_xvtdivsp_fe (vf, vf);\n+    XVTDIVSP_FE vsx_tdivv4sf3_fe {}\n+\n+  const signed int __builtin_vsx_xvtdivsp_fg (vf, vf);\n+    XVTDIVSP_FG vsx_tdivv4sf3_fg {}\n+\n+  const signed int __builtin_vsx_xvtsqrtdp_fe (vd);\n+    XVTSQRTDP_FE vsx_tsqrtv2df2_fe {}\n+\n+  const signed int __builtin_vsx_xvtsqrtdp_fg (vd);\n+    XVTSQRTDP_FG vsx_tsqrtv2df2_fg {}\n+\n+  const signed int __builtin_vsx_xvtsqrtsp_fe (vf);\n+    XVTSQRTSP_FE vsx_tsqrtv4sf2_fe {}\n+\n+  const signed int __builtin_vsx_xvtsqrtsp_fg (vf);\n+    XVTSQRTSP_FG vsx_tsqrtv4sf2_fg {}\n+\n+  const vf __builtin_vsx_xxmrghw (vf, vf);\n+    XXMRGHW_4SF vsx_xxmrghw_v4sf {}\n+\n+  const vsi __builtin_vsx_xxmrghw_4si (vsi, vsi);\n+    XXMRGHW_4SI vsx_xxmrghw_v4si {}\n+\n+  const vf __builtin_vsx_xxmrglw (vf, vf);\n+    XXMRGLW_4SF vsx_xxmrglw_v4sf {}\n+\n+  const vsi __builtin_vsx_xxmrglw_4si (vsi, vsi);\n+    XXMRGLW_4SI vsx_xxmrglw_v4si {}\n+\n+  const vsc __builtin_vsx_xxpermdi_16qi (vsc, vsc, const int<2>);\n+    XXPERMDI_16QI vsx_xxpermdi_v16qi {}\n+\n+  const vsq __builtin_vsx_xxpermdi_1ti (vsq, vsq, const int<2>);\n+    XXPERMDI_1TI vsx_xxpermdi_v1ti {}\n+\n+  const vd __builtin_vsx_xxpermdi_2df (vd, vd, const int<2>);\n+    XXPERMDI_2DF vsx_xxpermdi_v2df {}\n+\n+  const vsll __builtin_vsx_xxpermdi_2di (vsll, vsll, const int<2>);\n+    XXPERMDI_2DI vsx_xxpermdi_v2di {}\n+\n+  const vf __builtin_vsx_xxpermdi_4sf (vf, vf, const int<2>);\n+    XXPERMDI_4SF vsx_xxpermdi_v4sf {}\n+\n+  const vsi __builtin_vsx_xxpermdi_4si (vsi, vsi, const int<2>);\n+    XXPERMDI_4SI vsx_xxpermdi_v4si {}\n+\n+  const vss __builtin_vsx_xxpermdi_8hi (vss, vss, const int<2>);\n+    XXPERMDI_8HI vsx_xxpermdi_v8hi {}\n+\n+  const vsc __builtin_vsx_xxsel_16qi (vsc, vsc, vsc);\n+    XXSEL_16QI vector_select_v16qi {}\n+\n+  const vuc __builtin_vsx_xxsel_16qi_uns (vuc, vuc, vuc);\n+    XXSEL_16QI_UNS vector_select_v16qi_uns {}\n+\n+  const vsq __builtin_vsx_xxsel_1ti (vsq, vsq, vsq);\n+    XXSEL_1TI vector_select_v1ti {}\n+\n+  const vsq __builtin_vsx_xxsel_1ti_uns (vsq, vsq, vsq);\n+    XXSEL_1TI_UNS vector_select_v1ti_uns {}\n+\n+  const vd __builtin_vsx_xxsel_2df (vd, vd, vd);\n+    XXSEL_2DF vector_select_v2df {}\n+\n+  const vsll __builtin_vsx_xxsel_2di (vsll, vsll, vsll);\n+    XXSEL_2DI vector_select_v2di {}\n+\n+  const vull __builtin_vsx_xxsel_2di_uns (vull, vull, vull);\n+    XXSEL_2DI_UNS vector_select_v2di_uns {}\n+\n+  const vf __builtin_vsx_xxsel_4sf (vf, vf, vf);\n+    XXSEL_4SF vector_select_v4sf {}\n+\n+  const vsi __builtin_vsx_xxsel_4si (vsi, vsi, vsi);\n+    XXSEL_4SI vector_select_v4si {}\n+\n+  const vui __builtin_vsx_xxsel_4si_uns (vui, vui, vui);\n+    XXSEL_4SI_UNS vector_select_v4si_uns {}\n+\n+  const vss __builtin_vsx_xxsel_8hi (vss, vss, vss);\n+    XXSEL_8HI vector_select_v8hi {}\n+\n+  const vus __builtin_vsx_xxsel_8hi_uns (vus, vus, vus);\n+    XXSEL_8HI_UNS vector_select_v8hi_uns {}\n+\n+  const vsc __builtin_vsx_xxsldwi_16qi (vsc, vsc, const int<2>);\n+    XXSLDWI_16QI vsx_xxsldwi_v16qi {}\n+\n+  const vd __builtin_vsx_xxsldwi_2df (vd, vd, const int<2>);\n+    XXSLDWI_2DF vsx_xxsldwi_v2df {}\n+\n+  const vsll __builtin_vsx_xxsldwi_2di (vsll, vsll, const int<2>);\n+    XXSLDWI_2DI vsx_xxsldwi_v2di {}\n+\n+  const vf __builtin_vsx_xxsldwi_4sf (vf, vf, const int<2>);\n+    XXSLDWI_4SF vsx_xxsldwi_v4sf {}\n+\n+  const vsi __builtin_vsx_xxsldwi_4si (vsi, vsi, const int<2>);\n+    XXSLDWI_4SI vsx_xxsldwi_v4si {}\n+\n+  const vss __builtin_vsx_xxsldwi_8hi (vss, vss, const int<2>);\n+    XXSLDWI_8HI vsx_xxsldwi_v8hi {}\n+\n+  const vd __builtin_vsx_xxspltd_2df (vd, const int<1>);\n+    XXSPLTD_V2DF vsx_xxspltd_v2df {}\n+\n+  const vsll __builtin_vsx_xxspltd_2di (vsll, const int<1>);\n+    XXSPLTD_V2DI vsx_xxspltd_v2di {}"}]}