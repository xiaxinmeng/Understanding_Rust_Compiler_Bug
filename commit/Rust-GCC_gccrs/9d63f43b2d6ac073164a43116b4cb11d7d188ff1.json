{"sha": "9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OWQ2M2Y0M2IyZDZhYzA3MzE2NGE0MzExNmI0Y2IxMWQ3ZDE4OGZmMQ==", "commit": {"author": {"name": "Tamar Christina", "email": "tamar.christina@arm.com", "date": "2019-01-10T03:30:59Z"}, "committer": {"name": "Tamar Christina", "email": "tnfchris@gcc.gnu.org", "date": "2019-01-10T03:30:59Z"}, "message": "aarch64-builtins.c (enum aarch64_type_qualifiers): Add qualifier_lane_pair_index.\n\ngcc/ChangeLog:\n\n2019-01-10  Tamar Christina  <tamar.christina@arm.com>\n\n\t* config/aarch64/aarch64-builtins.c (enum aarch64_type_qualifiers): Add qualifier_lane_pair_index.\n\t(emit-rtl.h): Include.\n\t(TYPES_QUADOP_LANE_PAIR): New.\n\t(aarch64_simd_expand_args): Use it.\n\t(aarch64_simd_expand_builtin): Likewise.\n\t(AARCH64_SIMD_FCMLA_LANEQ_BUILTINS, aarch64_fcmla_laneq_builtin_datum): New.\n\t(FCMLA_LANEQ_BUILTIN, AARCH64_SIMD_FCMLA_LANEQ_BUILTIN_BASE,\n\tAARCH64_SIMD_FCMLA_LANEQ_BUILTINS, aarch64_fcmla_lane_builtin_data,\n\taarch64_init_fcmla_laneq_builtins, aarch64_expand_fcmla_builtin): New.\n\t(aarch64_init_builtins): Add aarch64_init_fcmla_laneq_builtins.\n\t(aarch64_expand_buildin): Add AARCH64_SIMD_BUILTIN_FCMLA_LANEQ0_V2SF,\n\tAARCH64_SIMD_BUILTIN_FCMLA_LANEQ90_V2SF, AARCH64_SIMD_BUILTIN_FCMLA_LANEQ180_V2SF,\n \tAARCH64_SIMD_BUILTIN_FCMLA_LANEQ2700_V2SF, AARCH64_SIMD_BUILTIN_FCMLA_LANEQ0_V4HF,\n\tAARCH64_SIMD_BUILTIN_FCMLA_LANEQ90_V4HF, AARCH64_SIMD_BUILTIN_FCMLA_LANEQ180_V4HF,\n\tAARCH64_SIMD_BUILTIN_FCMLA_LANEQ270_V4HF.\n\t* config/aarch64/iterators.md (FCMLA_maybe_lane): New.\n\t* config/aarch64/aarch64-c.c (aarch64_update_cpp_builtins): Add __ARM_FEATURE_COMPLEX.\n\t* config/aarch64/aarch64-simd-builtins.def (fcadd90, fcadd270, fcmla0, fcmla90,\n\tfcmla180, fcmla270, fcmla_lane0, fcmla_lane90, fcmla_lane180, fcmla_lane270,\n\tfcmla_laneq0, fcmla_laneq90, fcmla_laneq180, fcmla_laneq270,\n\tfcmlaq_lane0, fcmlaq_lane90, fcmlaq_lane180, fcmlaq_lane270): New.\n\t* config/aarch64/aarch64-simd.md (aarch64_fcmla_lane<rot><mode>,\n\taarch64_fcmla_laneq<rot>v4hf, aarch64_fcmlaq_lane<rot><mode>,aarch64_fcadd<rot><mode>,\n\taarch64_fcmla<rot><mode>): New.\n\t* config/aarch64/arm_neon.h:\n\t(vcadd_rot90_f16): New.\n\t(vcaddq_rot90_f16): New.\n\t(vcadd_rot270_f16): New.\n\t(vcaddq_rot270_f16): New.\n\t(vcmla_f16): New.\n\t(vcmlaq_f16): New.\n\t(vcmla_lane_f16): New.\n\t(vcmla_laneq_f16): New.\n\t(vcmlaq_lane_f16): New.\n\t(vcmlaq_rot90_lane_f16): New.\n\t(vcmla_rot90_laneq_f16): New.\n\t(vcmla_rot90_lane_f16): New.\n\t(vcmlaq_rot90_f16): New.\n\t(vcmla_rot90_f16): New.\n\t(vcmlaq_laneq_f16): New.\n\t(vcmla_rot180_laneq_f16): New.\n\t(vcmla_rot180_lane_f16): New.\n\t(vcmlaq_rot180_f16): New.\n\t(vcmla_rot180_f16): New.\n\t(vcmlaq_rot90_laneq_f16): New.\n\t(vcmlaq_rot270_laneq_f16): New.\n\t(vcmlaq_rot270_lane_f16): New.\n\t(vcmla_rot270_laneq_f16): New.\n\t(vcmlaq_rot270_f16): New.\n\t(vcmla_rot270_f16): New.\n\t(vcmlaq_rot180_laneq_f16): New.\n\t(vcmlaq_rot180_lane_f16): New.\n\t(vcmla_rot270_lane_f16): New.\n\t(vcadd_rot90_f32): New.\n\t(vcaddq_rot90_f32): New.\n\t(vcaddq_rot90_f64): New.\n\t(vcadd_rot270_f32): New.\n\t(vcaddq_rot270_f32): New.\n\t(vcaddq_rot270_f64): New.\n\t(vcmla_f32): New.\n\t(vcmlaq_f32): New.\n\t(vcmlaq_f64): New.\n\t(vcmla_lane_f32): New.\n\t(vcmla_laneq_f32): New.\n\t(vcmlaq_lane_f32): New.\n\t(vcmlaq_laneq_f32): New.\n\t(vcmla_rot90_f32): New.\n\t(vcmlaq_rot90_f32): New.\n\t(vcmlaq_rot90_f64): New.\n\t(vcmla_rot90_lane_f32): New.\n\t(vcmla_rot90_laneq_f32): New.\n\t(vcmlaq_rot90_lane_f32): New.\n\t(vcmlaq_rot90_laneq_f32): New.\n\t(vcmla_rot180_f32): New.\n\t(vcmlaq_rot180_f32): New.\n\t(vcmlaq_rot180_f64): New.\n\t(vcmla_rot180_lane_f32): New.\n\t(vcmla_rot180_laneq_f32): New.\n\t(vcmlaq_rot180_lane_f32): New.\n\t(vcmlaq_rot180_laneq_f32): New.\n\t(vcmla_rot270_f32): New.\n\t(vcmlaq_rot270_f32): New.\n\t(vcmlaq_rot270_f64): New.\n\t(vcmla_rot270_lane_f32): New.\n\t(vcmla_rot270_laneq_f32): New.\n\t(vcmlaq_rot270_lane_f32): New.\n\t(vcmlaq_rot270_laneq_f32): New.\n\t* config/aarch64/aarch64.h (TARGET_COMPLEX): New.\n\t* config/aarch64/iterators.md (UNSPEC_FCADD90, UNSPEC_FCADD270,\n\tUNSPEC_FCMLA, UNSPEC_FCMLA90, UNSPEC_FCMLA180, UNSPEC_FCMLA270): New.\n\t(FCADD, FCMLA): New.\n\t(rot): New.\n\t* config/arm/types.md (neon_fcadd, neon_fcmla): New.\n\ngcc/testsuite/ChangeLog:\n\n2019-01-10  Tamar Christina  <tamar.christina@arm.com>\n\n\t* gcc.target/aarch64/advsimd-intrinsics/vector-complex.c: New test.\n\t* gcc.target/aarch64/advsimd-intrinsics/vector-complex_f16.c: New test.\n\nFrom-SVN: r267795", "tree": {"sha": "ff1fe9854c761350cac2cec86677b38c23c8e1df", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ff1fe9854c761350cac2cec86677b38c23c8e1df"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/comments", "author": {"login": "TamarChristinaArm", "id": 48126768, "node_id": "MDQ6VXNlcjQ4MTI2NzY4", "avatar_url": "https://avatars.githubusercontent.com/u/48126768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TamarChristinaArm", "html_url": "https://github.com/TamarChristinaArm", "followers_url": "https://api.github.com/users/TamarChristinaArm/followers", "following_url": "https://api.github.com/users/TamarChristinaArm/following{/other_user}", "gists_url": "https://api.github.com/users/TamarChristinaArm/gists{/gist_id}", "starred_url": "https://api.github.com/users/TamarChristinaArm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TamarChristinaArm/subscriptions", "organizations_url": "https://api.github.com/users/TamarChristinaArm/orgs", "repos_url": "https://api.github.com/users/TamarChristinaArm/repos", "events_url": "https://api.github.com/users/TamarChristinaArm/events{/privacy}", "received_events_url": "https://api.github.com/users/TamarChristinaArm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "90c3d78f51581b466fe2f5f3a626f9f398774d35", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/90c3d78f51581b466fe2f5f3a626f9f398774d35", "html_url": "https://github.com/Rust-GCC/gccrs/commit/90c3d78f51581b466fe2f5f3a626f9f398774d35"}], "stats": {"total": 1437, "additions": 1435, "deletions": 2}, "files": [{"sha": "f80cad99048dae8070dc7adfac7909e86f82f84c", "filename": "gcc/ChangeLog", "status": "modified", "additions": 96, "deletions": 0, "changes": 96, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -1,3 +1,99 @@\n+2019-01-10  Tamar Christina  <tamar.christina@arm.com>\n+\n+\t* config/aarch64/aarch64-builtins.c (enum aarch64_type_qualifiers): Add qualifier_lane_pair_index.\n+\t(emit-rtl.h): Include.\n+\t(TYPES_QUADOP_LANE_PAIR): New.\n+\t(aarch64_simd_expand_args): Use it.\n+\t(aarch64_simd_expand_builtin): Likewise.\n+\t(AARCH64_SIMD_FCMLA_LANEQ_BUILTINS, aarch64_fcmla_laneq_builtin_datum): New.\n+\t(FCMLA_LANEQ_BUILTIN, AARCH64_SIMD_FCMLA_LANEQ_BUILTIN_BASE,\n+\tAARCH64_SIMD_FCMLA_LANEQ_BUILTINS, aarch64_fcmla_lane_builtin_data,\n+\taarch64_init_fcmla_laneq_builtins, aarch64_expand_fcmla_builtin): New.\n+\t(aarch64_init_builtins): Add aarch64_init_fcmla_laneq_builtins.\n+\t(aarch64_expand_buildin): Add AARCH64_SIMD_BUILTIN_FCMLA_LANEQ0_V2SF,\n+\tAARCH64_SIMD_BUILTIN_FCMLA_LANEQ90_V2SF, AARCH64_SIMD_BUILTIN_FCMLA_LANEQ180_V2SF,\n+ \tAARCH64_SIMD_BUILTIN_FCMLA_LANEQ2700_V2SF, AARCH64_SIMD_BUILTIN_FCMLA_LANEQ0_V4HF,\n+\tAARCH64_SIMD_BUILTIN_FCMLA_LANEQ90_V4HF, AARCH64_SIMD_BUILTIN_FCMLA_LANEQ180_V4HF,\n+\tAARCH64_SIMD_BUILTIN_FCMLA_LANEQ270_V4HF.\n+\t* config/aarch64/iterators.md (FCMLA_maybe_lane): New.\n+\t* config/aarch64/aarch64-c.c (aarch64_update_cpp_builtins): Add __ARM_FEATURE_COMPLEX.\n+\t* config/aarch64/aarch64-simd-builtins.def (fcadd90, fcadd270, fcmla0, fcmla90,\n+\tfcmla180, fcmla270, fcmla_lane0, fcmla_lane90, fcmla_lane180, fcmla_lane270,\n+\tfcmla_laneq0, fcmla_laneq90, fcmla_laneq180, fcmla_laneq270,\n+\tfcmlaq_lane0, fcmlaq_lane90, fcmlaq_lane180, fcmlaq_lane270): New.\n+\t* config/aarch64/aarch64-simd.md (aarch64_fcmla_lane<rot><mode>,\n+\taarch64_fcmla_laneq<rot>v4hf, aarch64_fcmlaq_lane<rot><mode>,aarch64_fcadd<rot><mode>,\n+\taarch64_fcmla<rot><mode>): New.\n+\t* config/aarch64/arm_neon.h:\n+\t(vcadd_rot90_f16): New.\n+\t(vcaddq_rot90_f16): New.\n+\t(vcadd_rot270_f16): New.\n+\t(vcaddq_rot270_f16): New.\n+\t(vcmla_f16): New.\n+\t(vcmlaq_f16): New.\n+\t(vcmla_lane_f16): New.\n+\t(vcmla_laneq_f16): New.\n+\t(vcmlaq_lane_f16): New.\n+\t(vcmlaq_rot90_lane_f16): New.\n+\t(vcmla_rot90_laneq_f16): New.\n+\t(vcmla_rot90_lane_f16): New.\n+\t(vcmlaq_rot90_f16): New.\n+\t(vcmla_rot90_f16): New.\n+\t(vcmlaq_laneq_f16): New.\n+\t(vcmla_rot180_laneq_f16): New.\n+\t(vcmla_rot180_lane_f16): New.\n+\t(vcmlaq_rot180_f16): New.\n+\t(vcmla_rot180_f16): New.\n+\t(vcmlaq_rot90_laneq_f16): New.\n+\t(vcmlaq_rot270_laneq_f16): New.\n+\t(vcmlaq_rot270_lane_f16): New.\n+\t(vcmla_rot270_laneq_f16): New.\n+\t(vcmlaq_rot270_f16): New.\n+\t(vcmla_rot270_f16): New.\n+\t(vcmlaq_rot180_laneq_f16): New.\n+\t(vcmlaq_rot180_lane_f16): New.\n+\t(vcmla_rot270_lane_f16): New.\n+\t(vcadd_rot90_f32): New.\n+\t(vcaddq_rot90_f32): New.\n+\t(vcaddq_rot90_f64): New.\n+\t(vcadd_rot270_f32): New.\n+\t(vcaddq_rot270_f32): New.\n+\t(vcaddq_rot270_f64): New.\n+\t(vcmla_f32): New.\n+\t(vcmlaq_f32): New.\n+\t(vcmlaq_f64): New.\n+\t(vcmla_lane_f32): New.\n+\t(vcmla_laneq_f32): New.\n+\t(vcmlaq_lane_f32): New.\n+\t(vcmlaq_laneq_f32): New.\n+\t(vcmla_rot90_f32): New.\n+\t(vcmlaq_rot90_f32): New.\n+\t(vcmlaq_rot90_f64): New.\n+\t(vcmla_rot90_lane_f32): New.\n+\t(vcmla_rot90_laneq_f32): New.\n+\t(vcmlaq_rot90_lane_f32): New.\n+\t(vcmlaq_rot90_laneq_f32): New.\n+\t(vcmla_rot180_f32): New.\n+\t(vcmlaq_rot180_f32): New.\n+\t(vcmlaq_rot180_f64): New.\n+\t(vcmla_rot180_lane_f32): New.\n+\t(vcmla_rot180_laneq_f32): New.\n+\t(vcmlaq_rot180_lane_f32): New.\n+\t(vcmlaq_rot180_laneq_f32): New.\n+\t(vcmla_rot270_f32): New.\n+\t(vcmlaq_rot270_f32): New.\n+\t(vcmlaq_rot270_f64): New.\n+\t(vcmla_rot270_lane_f32): New.\n+\t(vcmla_rot270_laneq_f32): New.\n+\t(vcmlaq_rot270_lane_f32): New.\n+\t(vcmlaq_rot270_laneq_f32): New.\n+\t* config/aarch64/aarch64.h (TARGET_COMPLEX): New.\n+\t* config/aarch64/iterators.md (UNSPEC_FCADD90, UNSPEC_FCADD270,\n+\tUNSPEC_FCMLA, UNSPEC_FCMLA90, UNSPEC_FCMLA180, UNSPEC_FCMLA270): New.\n+\t(FCADD, FCMLA): New.\n+\t(rot): New.\n+\t* config/arm/types.md (neon_fcadd, neon_fcmla): New.\n+\n 2019-01-09  Sandra Loosemore  <sandra@codesourcery.com>\n \n \tPR other/16615"}, {"sha": "df0e035e39a94b7978f7c30317779dbdda7c182e", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "modified", "additions": 184, "deletions": 2, "changes": 186, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -42,6 +42,7 @@\n #include \"langhooks.h\"\n #include \"gimple-iterator.h\"\n #include \"case-cfn-macros.h\"\n+#include \"emit-rtl.h\"\n \n #define v8qi_UP  E_V8QImode\n #define v4hi_UP  E_V4HImode\n@@ -102,7 +103,10 @@ enum aarch64_type_qualifiers\n   /* Lane indices - must be in range, and flipped for bigendian.  */\n   qualifier_lane_index = 0x200,\n   /* Lane indices for single lane structure loads and stores.  */\n-  qualifier_struct_load_store_lane_index = 0x400\n+  qualifier_struct_load_store_lane_index = 0x400,\n+  /* Lane indices selected in pairs. - must be in range, and flipped for\n+     bigendian.  */\n+  qualifier_lane_pair_index = 0x800,\n };\n \n typedef struct\n@@ -171,6 +175,11 @@ aarch64_types_ternopu_imm_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n #define TYPES_TERNOPUI (aarch64_types_ternopu_imm_qualifiers)\n \n \n+static enum aarch64_type_qualifiers\n+aarch64_types_quadop_lane_pair_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_none, qualifier_none, qualifier_none,\n+      qualifier_none, qualifier_lane_pair_index };\n+#define TYPES_QUADOP_LANE_PAIR (aarch64_types_quadop_lane_pair_qualifiers)\n static enum aarch64_type_qualifiers\n aarch64_types_quadop_lane_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_none, qualifier_none,\n@@ -356,6 +365,18 @@ static aarch64_simd_builtin_datum aarch64_simd_builtin_data[] = {\n   CRC32_BUILTIN (crc32cw, SI) \\\n   CRC32_BUILTIN (crc32cx, DI)\n \n+/* The next 8 FCMLA instrinsics require some special handling compared the\n+   normal simd intrinsics.  */\n+#define AARCH64_SIMD_FCMLA_LANEQ_BUILTINS \\\n+  FCMLA_LANEQ_BUILTIN (0, v2sf, fcmla, V2SF, false) \\\n+  FCMLA_LANEQ_BUILTIN (90, v2sf, fcmla, V2SF, false) \\\n+  FCMLA_LANEQ_BUILTIN (180, v2sf, fcmla, V2SF, false) \\\n+  FCMLA_LANEQ_BUILTIN (270, v2sf, fcmla, V2SF, false) \\\n+  FCMLA_LANEQ_BUILTIN (0, v4hf, fcmla_laneq, V4HF, true) \\\n+  FCMLA_LANEQ_BUILTIN (90, v4hf, fcmla_laneq, V4HF, true) \\\n+  FCMLA_LANEQ_BUILTIN (180, v4hf, fcmla_laneq, V4HF, true) \\\n+  FCMLA_LANEQ_BUILTIN (270, v4hf, fcmla_laneq, V4HF, true) \\\n+\n typedef struct\n {\n   const char *name;\n@@ -364,9 +385,22 @@ typedef struct\n   unsigned int fcode;\n } aarch64_crc_builtin_datum;\n \n+/* Hold information about how to expand the FCMLA_LANEQ builtins.  */\n+typedef struct\n+{\n+  const char *name;\n+  machine_mode mode;\n+  const enum insn_code icode;\n+  unsigned int fcode;\n+  bool lane;\n+} aarch64_fcmla_laneq_builtin_datum;\n+\n #define CRC32_BUILTIN(N, M) \\\n   AARCH64_BUILTIN_##N,\n \n+#define FCMLA_LANEQ_BUILTIN(I, N, X, M, T) \\\n+  AARCH64_SIMD_BUILTIN_FCMLA_LANEQ##I##_##M,\n+\n #undef VAR1\n #define VAR1(T, N, MAP, A) \\\n   AARCH64_SIMD_BUILTIN_##T##_##N##A,\n@@ -399,6 +433,9 @@ enum aarch64_builtins\n   AARCH64_PAUTH_BUILTIN_AUTIA1716,\n   AARCH64_PAUTH_BUILTIN_PACIA1716,\n   AARCH64_PAUTH_BUILTIN_XPACLRI,\n+  /* Special cased Armv8.3-A Complex FMA by Lane quad Builtins.  */\n+  AARCH64_SIMD_FCMLA_LANEQ_BUILTIN_BASE,\n+  AARCH64_SIMD_FCMLA_LANEQ_BUILTINS\n   AARCH64_BUILTIN_MAX\n };\n \n@@ -410,6 +447,18 @@ static aarch64_crc_builtin_datum aarch64_crc_builtin_data[] = {\n   AARCH64_CRC32_BUILTINS\n };\n \n+\n+#undef FCMLA_LANEQ_BUILTIN\n+#define FCMLA_LANEQ_BUILTIN(I, N, X, M, T) \\\n+  {\"__builtin_aarch64_fcmla_laneq\"#I#N, E_##M##mode, CODE_FOR_aarch64_##X##I##N, \\\n+   AARCH64_SIMD_BUILTIN_FCMLA_LANEQ##I##_##M, T},\n+\n+/* This structure contains how to manage the mapping form the builtin to the\n+   instruction to generate in the backend and how to invoke the instruction.  */\n+static aarch64_fcmla_laneq_builtin_datum aarch64_fcmla_lane_builtin_data[] {\n+  AARCH64_SIMD_FCMLA_LANEQ_BUILTINS\n+};\n+\n #undef CRC32_BUILTIN\n \n static GTY(()) tree aarch64_builtin_decls[AARCH64_BUILTIN_MAX];\n@@ -746,6 +795,34 @@ aarch64_init_simd_builtin_scalar_types (void)\n \n static bool aarch64_simd_builtins_initialized_p = false;\n \n+/* Due to the architecture not providing lane variant of the lane instructions\n+   for fcmla we can't use the standard simd builtin expansion code, but we\n+   still want the majority of the validation that would normally be done.  */\n+\n+void\n+aarch64_init_fcmla_laneq_builtins (void)\n+{\n+  unsigned int i = 0;\n+\n+  for (i = 0; i < ARRAY_SIZE (aarch64_fcmla_lane_builtin_data); ++i)\n+    {\n+      aarch64_fcmla_laneq_builtin_datum* d\n+\t= &aarch64_fcmla_lane_builtin_data[i];\n+      tree argtype = aarch64_lookup_simd_builtin_type (d->mode, qualifier_none);\n+      machine_mode quadmode = GET_MODE_2XWIDER_MODE (d->mode).require ();\n+      tree quadtype\n+\t= aarch64_lookup_simd_builtin_type (quadmode, qualifier_none);\n+      tree lanetype\n+\t= aarch64_simd_builtin_std_type (SImode, qualifier_lane_pair_index);\n+      tree ftype = build_function_type_list (argtype, argtype, argtype,\n+\t\t\t\t\t     quadtype, lanetype, NULL_TREE);\n+      tree fndecl = add_builtin_function (d->name, ftype, d->fcode,\n+\t\t\t\t\t  BUILT_IN_MD, NULL, NULL_TREE);\n+\n+      aarch64_builtin_decls[d->fcode] = fndecl;\n+    }\n+}\n+\n void\n aarch64_init_simd_builtins (void)\n {\n@@ -1001,7 +1078,10 @@ aarch64_init_builtins (void)\n   aarch64_init_fp16_types ();\n \n   if (TARGET_SIMD)\n-    aarch64_init_simd_builtins ();\n+    {\n+      aarch64_init_simd_builtins ();\n+      aarch64_init_fcmla_laneq_builtins ();\n+    }\n \n   aarch64_init_crc32_builtins ();\n   aarch64_init_builtin_rsqrt ();\n@@ -1031,6 +1111,7 @@ typedef enum\n   SIMD_ARG_CONSTANT,\n   SIMD_ARG_LANE_INDEX,\n   SIMD_ARG_STRUCT_LOAD_STORE_LANE_INDEX,\n+  SIMD_ARG_LANE_PAIR_INDEX,\n   SIMD_ARG_STOP\n } builtin_simd_arg;\n \n@@ -1102,6 +1183,22 @@ aarch64_simd_expand_args (rtx target, int icode, int have_retval,\n \t\t  /* Keep to GCC-vector-extension lane indices in the RTL.  */\n \t\t  op[opc] = aarch64_endian_lane_rtx (vmode, INTVAL (op[opc]));\n \t\t}\n+\t      /* If the lane index isn't a constant then error out.  */\n+\t      goto constant_arg;\n+\n+\t    case SIMD_ARG_LANE_PAIR_INDEX:\n+\t      /* Must be a previous operand into which this is an index and\n+\t\t index is restricted to nunits / 2.  */\n+\t      gcc_assert (opc > 0);\n+\t      if (CONST_INT_P (op[opc]))\n+\t\t{\n+\t\t  machine_mode vmode = insn_data[icode].operand[opc - 1].mode;\n+\t\t  unsigned int nunits\n+\t\t    = GET_MODE_NUNITS (vmode).to_constant ();\n+\t\t  aarch64_simd_lane_bounds (op[opc], 0, nunits / 2, exp);\n+\t\t  /* Keep to GCC-vector-extension lane indices in the RTL.  */\n+\t\t  op[opc] = aarch64_endian_lane_rtx (vmode, INTVAL (op[opc]));\n+\t\t}\n \t      /* Fall through - if the lane index isn't a constant then\n \t\t the next case will error.  */\n \t      /* FALLTHRU */\n@@ -1215,6 +1312,8 @@ aarch64_simd_expand_builtin (int fcode, tree exp, rtx target)\n \n       if (d->qualifiers[qualifiers_k] & qualifier_lane_index)\n \targs[k] = SIMD_ARG_LANE_INDEX;\n+      else if (d->qualifiers[qualifiers_k] & qualifier_lane_pair_index)\n+\targs[k] = SIMD_ARG_LANE_PAIR_INDEX;\n       else if (d->qualifiers[qualifiers_k] & qualifier_struct_load_store_lane_index)\n \targs[k] = SIMD_ARG_STRUCT_LOAD_STORE_LANE_INDEX;\n       else if (d->qualifiers[qualifiers_k] & qualifier_immediate)\n@@ -1317,6 +1416,79 @@ aarch64_expand_builtin_rsqrt (int fcode, tree exp, rtx target)\n   return target;\n }\n \n+/* Expand a FCMLA lane expression EXP with code FCODE and\n+   result going to TARGET if that is convenient.  */\n+\n+rtx\n+aarch64_expand_fcmla_builtin (tree exp, rtx target, int fcode)\n+{\n+  int bcode = fcode - AARCH64_SIMD_FCMLA_LANEQ_BUILTIN_BASE - 1;\n+  aarch64_fcmla_laneq_builtin_datum* d\n+    = &aarch64_fcmla_lane_builtin_data[bcode];\n+  machine_mode quadmode = GET_MODE_2XWIDER_MODE (d->mode).require ();\n+  rtx op0 = force_reg (d->mode, expand_normal (CALL_EXPR_ARG (exp, 0)));\n+  rtx op1 = force_reg (d->mode, expand_normal (CALL_EXPR_ARG (exp, 1)));\n+  rtx op2 = force_reg (quadmode, expand_normal (CALL_EXPR_ARG (exp, 2)));\n+  tree tmp = CALL_EXPR_ARG (exp, 3);\n+  rtx lane_idx = expand_expr (tmp, NULL_RTX, VOIDmode, EXPAND_INITIALIZER);\n+\n+  /* Validate that the lane index is a constant.  */\n+  if (!CONST_INT_P (lane_idx))\n+    {\n+      error (\"%Kargument %d must be a constant immediate\", exp, 4);\n+      return const0_rtx;\n+    }\n+\n+  /* Validate that the index is within the expected range.  */\n+  int nunits = GET_MODE_NUNITS (quadmode).to_constant ();\n+  aarch64_simd_lane_bounds (lane_idx, 0, nunits / 2, exp);\n+\n+  /* Keep to GCC-vector-extension lane indices in the RTL.  */\n+  lane_idx = aarch64_endian_lane_rtx (quadmode, INTVAL (lane_idx));\n+\n+  /* Generate the correct register and mode.  */\n+  int lane = INTVAL (lane_idx);\n+\n+  if (lane < nunits / 4)\n+    op2 = simplify_gen_subreg (d->mode, op2, quadmode, 0);\n+  else\n+    {\n+      /* Select the upper 64 bits, either a V2SF or V4HF, this however\n+\t is quite messy, as the operation required even though simple\n+\t doesn't have a simple RTL pattern, and seems it's quite hard to\n+\t define using a single RTL pattern.  The target generic version\n+\t gen_highpart_mode generates code that isn't optimal.  */\n+      rtx temp1 = gen_reg_rtx (d->mode);\n+      rtx temp2 = gen_reg_rtx (DImode);\n+      temp1 = simplify_gen_subreg (d->mode, op2, quadmode, 0);\n+      temp1 = simplify_gen_subreg (V2DImode, temp1, d->mode, 0);\n+      emit_insn (gen_aarch64_get_lanev2di (temp2, temp1     , const1_rtx));\n+      op2 = simplify_gen_subreg (d->mode, temp2, GET_MODE (temp2), 0);\n+\n+      /* And recalculate the index.  */\n+      lane -= nunits / 4;\n+    }\n+\n+  if (!target)\n+    target = gen_reg_rtx (d->mode);\n+  else\n+    target = force_reg (d->mode, target);\n+\n+  rtx pat = NULL_RTX;\n+\n+  if (d->lane)\n+    pat = GEN_FCN (d->icode) (target, op0, op1, op2,\n+\t\t\t      gen_int_mode (lane, SImode));\n+  else\n+    pat = GEN_FCN (d->icode) (target, op0, op1, op2);\n+\n+  if (!pat)\n+    return NULL_RTX;\n+\n+  emit_insn (pat);\n+  return target;\n+}\n+\n /* Expand an expression EXP that calls a built-in function,\n    with result going to TARGET if that's convenient.  */\n rtx\n@@ -1395,6 +1567,16 @@ aarch64_expand_builtin (tree exp,\n \t}\n \n       return target;\n+\n+    case AARCH64_SIMD_BUILTIN_FCMLA_LANEQ0_V2SF:\n+    case AARCH64_SIMD_BUILTIN_FCMLA_LANEQ90_V2SF:\n+    case AARCH64_SIMD_BUILTIN_FCMLA_LANEQ180_V2SF:\n+    case AARCH64_SIMD_BUILTIN_FCMLA_LANEQ270_V2SF:\n+    case AARCH64_SIMD_BUILTIN_FCMLA_LANEQ0_V4HF:\n+    case AARCH64_SIMD_BUILTIN_FCMLA_LANEQ90_V4HF:\n+    case AARCH64_SIMD_BUILTIN_FCMLA_LANEQ180_V4HF:\n+    case AARCH64_SIMD_BUILTIN_FCMLA_LANEQ270_V4HF:\n+      return aarch64_expand_fcmla_builtin (exp, target, fcode);\n     }\n \n   if (fcode >= AARCH64_SIMD_BUILTIN_BASE && fcode <= AARCH64_SIMD_BUILTIN_MAX)"}, {"sha": "fcb1e80177dc549ba03b09778618a91f022777b7", "filename": "gcc/config/aarch64/aarch64-c.c", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Faarch64-c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Faarch64-c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-c.c?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -109,6 +109,7 @@ aarch64_update_cpp_builtins (cpp_reader *pfile)\n \n   aarch64_def_or_undef (TARGET_CRC32, \"__ARM_FEATURE_CRC32\", pfile);\n   aarch64_def_or_undef (TARGET_DOTPROD, \"__ARM_FEATURE_DOTPROD\", pfile);\n+  aarch64_def_or_undef (TARGET_COMPLEX, \"__ARM_FEATURE_COMPLEX\", pfile);\n \n   cpp_undef (pfile, \"__AARCH64_CMODEL_TINY__\");\n   cpp_undef (pfile, \"__AARCH64_CMODEL_SMALL__\");"}, {"sha": "17bb0c4869b12ede2fc51a8f89d841ded8fac230", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -217,6 +217,25 @@\n   BUILTIN_VB (QUADOP_LANE, sdot_laneq, 0)\n   BUILTIN_VB (QUADOPU_LANE, udot_laneq, 0)\n \n+  /* Implemented by aarch64_fcadd<rot><mode>.   */\n+  BUILTIN_VHSDF (BINOP, fcadd90, 0)\n+  BUILTIN_VHSDF (BINOP, fcadd270, 0)\n+\n+  /* Implemented by aarch64_fcmla{_lane}{q}<rot><mode>.   */\n+  BUILTIN_VHSDF (TERNOP, fcmla0, 0)\n+  BUILTIN_VHSDF (TERNOP, fcmla90, 0)\n+  BUILTIN_VHSDF (TERNOP, fcmla180, 0)\n+  BUILTIN_VHSDF (TERNOP, fcmla270, 0)\n+  BUILTIN_VHSDF (QUADOP_LANE_PAIR, fcmla_lane0, 0)\n+  BUILTIN_VHSDF (QUADOP_LANE_PAIR, fcmla_lane90, 0)\n+  BUILTIN_VHSDF (QUADOP_LANE_PAIR, fcmla_lane180, 0)\n+  BUILTIN_VHSDF (QUADOP_LANE_PAIR, fcmla_lane270, 0)\n+\n+  BUILTIN_VQ_HSF (QUADOP_LANE_PAIR, fcmlaq_lane0, 0)\n+  BUILTIN_VQ_HSF (QUADOP_LANE_PAIR, fcmlaq_lane90, 0)\n+  BUILTIN_VQ_HSF (QUADOP_LANE_PAIR, fcmlaq_lane180, 0)\n+  BUILTIN_VQ_HSF (QUADOP_LANE_PAIR, fcmlaq_lane270, 0)\n+\n   BUILTIN_VDQ_I (SHIFTIMM, ashr, 3)\n   VAR1 (SHIFTIMM, ashr_simd, 0, di)\n   BUILTIN_VDQ_I (SHIFTIMM, lshr, 3)"}, {"sha": "be6c27d319a1ca6fee581d8f8856a4dff8f4a060", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 64, "deletions": 0, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -419,6 +419,70 @@\n }\n )\n \n+;; The fcadd and fcmla patterns are made UNSPEC for the explicitly due to the\n+;; fact that their usage need to guarantee that the source vectors are\n+;; contiguous.  It would be wrong to describe the operation without being able\n+;; to describe the permute that is also required, but even if that is done\n+;; the permute would have been created as a LOAD_LANES which means the values\n+;; in the registers are in the wrong order.\n+(define_insn \"aarch64_fcadd<rot><mode>\"\n+  [(set (match_operand:VHSDF 0 \"register_operand\" \"=w\")\n+\t(unspec:VHSDF [(match_operand:VHSDF 1 \"register_operand\" \"w\")\n+\t\t       (match_operand:VHSDF 2 \"register_operand\" \"w\")]\n+\t\t       FCADD))]\n+  \"TARGET_COMPLEX\"\n+  \"fcadd\\t%0.<Vtype>, %1.<Vtype>, %2.<Vtype>, #<rot>\"\n+  [(set_attr \"type\" \"neon_fcadd\")]\n+)\n+\n+(define_insn \"aarch64_fcmla<rot><mode>\"\n+  [(set (match_operand:VHSDF 0 \"register_operand\" \"=w\")\n+\t(plus:VHSDF (match_operand:VHSDF 1 \"register_operand\" \"0\")\n+\t\t    (unspec:VHSDF [(match_operand:VHSDF 2 \"register_operand\" \"w\")\n+\t\t\t\t   (match_operand:VHSDF 3 \"register_operand\" \"w\")]\n+\t\t\t\t   FCMLA)))]\n+  \"TARGET_COMPLEX\"\n+  \"fcmla\\t%0.<Vtype>, %2.<Vtype>, %3.<Vtype>, #<rot>\"\n+  [(set_attr \"type\" \"neon_fcmla\")]\n+)\n+\n+\n+(define_insn \"aarch64_fcmla_lane<rot><mode>\"\n+  [(set (match_operand:VHSDF 0 \"register_operand\" \"=w\")\n+\t(plus:VHSDF (match_operand:VHSDF 1 \"register_operand\" \"0\")\n+\t\t    (unspec:VHSDF [(match_operand:VHSDF 2 \"register_operand\" \"w\")\n+\t\t\t\t   (match_operand:VHSDF 3 \"register_operand\" \"w\")\n+\t\t\t\t   (match_operand:SI 4 \"const_int_operand\" \"n\")]\n+\t\t\t\t   FCMLA)))]\n+  \"TARGET_COMPLEX\"\n+  \"fcmla\\t%0.<Vtype>, %2.<Vtype>, %3.<FCMLA_maybe_lane>, #<rot>\"\n+  [(set_attr \"type\" \"neon_fcmla\")]\n+)\n+\n+(define_insn \"aarch64_fcmla_laneq<rot>v4hf\"\n+  [(set (match_operand:V4HF 0 \"register_operand\" \"=w\")\n+\t(plus:V4HF (match_operand:V4HF 1 \"register_operand\" \"0\")\n+\t\t   (unspec:V4HF [(match_operand:V4HF 2 \"register_operand\" \"w\")\n+\t\t\t\t (match_operand:V8HF 3 \"register_operand\" \"w\")\n+\t\t\t\t (match_operand:SI 4 \"const_int_operand\" \"n\")]\n+\t\t\t\t FCMLA)))]\n+  \"TARGET_COMPLEX\"\n+  \"fcmla\\t%0.4h, %2.4h, %3.h[%4], #<rot>\"\n+  [(set_attr \"type\" \"neon_fcmla\")]\n+)\n+\n+(define_insn \"aarch64_fcmlaq_lane<rot><mode>\"\n+  [(set (match_operand:VQ_HSF 0 \"register_operand\" \"=w\")\n+\t(plus:VQ_HSF (match_operand:VQ_HSF 1 \"register_operand\" \"0\")\n+\t\t     (unspec:VQ_HSF [(match_operand:VQ_HSF 2 \"register_operand\" \"w\")\n+\t\t\t\t     (match_operand:<VHALF> 3 \"register_operand\" \"w\")\n+\t\t\t\t     (match_operand:SI 4 \"const_int_operand\" \"n\")]\n+\t\t\t\t     FCMLA)))]\n+  \"TARGET_COMPLEX\"\n+  \"fcmla\\t%0.<Vtype>, %2.<Vtype>, %3.<FCMLA_maybe_lane>, #<rot>\"\n+  [(set_attr \"type\" \"neon_fcmla\")]\n+)\n+\n ;; These instructions map to the __builtins for the Dot Product operations.\n (define_insn \"aarch64_<sur>dot<vsi2qi>\"\n   [(set (match_operand:VS 0 \"register_operand\" \"=w\")"}, {"sha": "7bd3bf525dd71347a12ed9cd2227bc2cd6e9cc55", "filename": "gcc/config/aarch64/aarch64.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Faarch64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Faarch64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.h?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -273,6 +273,9 @@ extern unsigned aarch64_architecture_version;\n /* ARMv8.3-A features.  */\n #define TARGET_ARMV8_3\t(AARCH64_ISA_V8_3)\n \n+/* Armv8.3-a Complex number extension to AdvSIMD extensions.  */\n+#define TARGET_COMPLEX (TARGET_SIMD && TARGET_ARMV8_3)\n+\n /* Make sure this is always defined so we don't have to check for ifdefs\n    but rather use normal ifs.  */\n #ifndef TARGET_FIX_ERR_A53_835769_DEFAULT"}, {"sha": "90fce333d09e2c0989737b0c9bed925869dd620c", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 475, "deletions": 0, "changes": 475, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -33294,6 +33294,481 @@ vbcaxq_s64 (int64x2_t __a, int64x2_t __b, int64x2_t __c)\n   return __builtin_aarch64_bcaxqv2di (__a, __b, __c);\n }\n \n+#pragma GCC pop_options\n+\n+/* AdvSIMD Complex numbers intrinsics.  */\n+\n+#pragma GCC push_options\n+#pragma GCC target((\"arch=armv8.3-a\"))\n+\n+#pragma GCC push_options\n+#pragma GCC target((\"+fp16\"))\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcadd_rot90_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+  return __builtin_aarch64_fcadd90v4hf (__a, __b);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcaddq_rot90_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+  return __builtin_aarch64_fcadd90v8hf (__a, __b);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcadd_rot270_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+  return __builtin_aarch64_fcadd270v4hf (__a, __b);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcaddq_rot270_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+  return __builtin_aarch64_fcadd270v8hf (__a, __b);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return __builtin_aarch64_fcmla0v4hf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return __builtin_aarch64_fcmla0v8hf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_lane_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b,\n+\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane0v4hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_laneq_f16 (float16x4_t __r, float16x4_t __a, float16x8_t __b,\n+\t\t const int __index)\n+{\n+  return __builtin_aarch64_fcmla_laneq0v4hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_lane_f16 (float16x8_t __r, float16x8_t __a, float16x4_t __b,\n+\t\t const int __index)\n+{\n+  return __builtin_aarch64_fcmlaq_lane0v8hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot90_lane_f16 (float16x8_t __r, float16x8_t __a, float16x4_t __b,\n+\t\t       const int __index)\n+{\n+  return __builtin_aarch64_fcmlaq_lane90v8hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot90_laneq_f16 (float16x4_t __r, float16x4_t __a, float16x8_t __b,\n+\t\t       const int __index)\n+{\n+  return __builtin_aarch64_fcmla_laneq90v4hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot90_lane_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b,\n+\t\t      const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane90v4hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot90_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return __builtin_aarch64_fcmla90v8hf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot90_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return __builtin_aarch64_fcmla90v4hf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_laneq_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b,\n+\t\t  const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane0v8hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot180_laneq_f16 (float16x4_t __r, float16x4_t __a, float16x8_t __b,\n+\t\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmla_laneq180v4hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot180_lane_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b,\n+\t\t       const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane180v4hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot180_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return __builtin_aarch64_fcmla180v8hf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot180_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return __builtin_aarch64_fcmla180v4hf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot90_laneq_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b,\n+\t\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane90v8hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot270_laneq_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b,\n+\t\t\t const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane270v8hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot270_lane_f16 (float16x8_t __r, float16x8_t __a, float16x4_t __b,\n+\t\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmlaq_lane270v8hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot270_laneq_f16 (float16x4_t __r, float16x4_t __a, float16x8_t __b,\n+\t\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmla_laneq270v4hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot270_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return __builtin_aarch64_fcmla270v8hf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot270_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return __builtin_aarch64_fcmla270v4hf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot180_laneq_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b,\n+\t\t\t const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane180v8hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot180_lane_f16 (float16x8_t __r, float16x8_t __a, float16x4_t __b,\n+\t\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmlaq_lane180v8hf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot270_lane_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b,\n+\t\t       const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane270v4hf (__r, __a, __b, __index);\n+}\n+#pragma GCC pop_options\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcadd_rot90_f32 (float32x2_t __a, float32x2_t __b)\n+{\n+  return __builtin_aarch64_fcadd90v2sf (__a, __b);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcaddq_rot90_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+  return __builtin_aarch64_fcadd90v4sf (__a, __b);\n+}\n+\n+__extension__ extern __inline float64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcaddq_rot90_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+  return __builtin_aarch64_fcadd90v2df (__a, __b);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcadd_rot270_f32 (float32x2_t __a, float32x2_t __b)\n+{\n+  return __builtin_aarch64_fcadd270v2sf (__a, __b);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcaddq_rot270_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+  return __builtin_aarch64_fcadd270v4sf (__a, __b);\n+}\n+\n+__extension__ extern __inline float64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcaddq_rot270_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+  return __builtin_aarch64_fcadd270v2df (__a, __b);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return __builtin_aarch64_fcmla0v2sf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return __builtin_aarch64_fcmla0v4sf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_f64 (float64x2_t __r, float64x2_t __a, float64x2_t __b)\n+{\n+  return __builtin_aarch64_fcmla0v2df (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_lane_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b,\n+\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane0v2sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_laneq_f32 (float32x2_t __r, float32x2_t __a, float32x4_t __b,\n+\t\t const int __index)\n+{\n+  return __builtin_aarch64_fcmla_laneq0v2sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_lane_f32 (float32x4_t __r, float32x4_t __a, float32x2_t __b,\n+\t\t const int __index)\n+{\n+  return __builtin_aarch64_fcmlaq_lane0v4sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_laneq_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b,\n+\t\t  const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane0v4sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot90_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return __builtin_aarch64_fcmla90v2sf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot90_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return __builtin_aarch64_fcmla90v4sf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot90_f64 (float64x2_t __r, float64x2_t __a, float64x2_t __b)\n+{\n+  return __builtin_aarch64_fcmla90v2df (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot90_lane_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b,\n+\t\t      const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane90v2sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot90_laneq_f32 (float32x2_t __r, float32x2_t __a, float32x4_t __b,\n+\t\t       const int __index)\n+{\n+  return __builtin_aarch64_fcmla_laneq90v2sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot90_lane_f32 (float32x4_t __r, float32x4_t __a, float32x2_t __b,\n+\t\t       const int __index)\n+{\n+  return __builtin_aarch64_fcmlaq_lane90v4sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot90_laneq_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b,\n+\t\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane90v4sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot180_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return __builtin_aarch64_fcmla180v2sf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot180_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return __builtin_aarch64_fcmla180v4sf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot180_f64 (float64x2_t __r, float64x2_t __a, float64x2_t __b)\n+{\n+  return __builtin_aarch64_fcmla180v2df (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot180_lane_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b,\n+\t\t       const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane180v2sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot180_laneq_f32 (float32x2_t __r, float32x2_t __a, float32x4_t __b,\n+\t\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmla_laneq180v2sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot180_lane_f32 (float32x4_t __r, float32x4_t __a, float32x2_t __b,\n+\t\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmlaq_lane180v4sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot180_laneq_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b,\n+\t\t\t const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane180v4sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot270_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return __builtin_aarch64_fcmla270v2sf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot270_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return __builtin_aarch64_fcmla270v4sf (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot270_f64 (float64x2_t __r, float64x2_t __a, float64x2_t __b)\n+{\n+  return __builtin_aarch64_fcmla270v2df (__r, __a, __b);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot270_lane_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b,\n+\t\t       const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane270v2sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmla_rot270_laneq_f32 (float32x2_t __r, float32x2_t __a, float32x4_t __b,\n+\t\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmla_laneq270v2sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot270_lane_f32 (float32x4_t __r, float32x4_t __a, float32x2_t __b,\n+\t\t\tconst int __index)\n+{\n+  return __builtin_aarch64_fcmlaq_lane270v4sf (__r, __a, __b, __index);\n+}\n+\n+__extension__ extern __inline float32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcmlaq_rot270_laneq_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b,\n+\t\t\t const int __index)\n+{\n+  return __builtin_aarch64_fcmla_lane270v4sf (__r, __a, __b, __index);\n+}\n \n #pragma GCC pop_options\n "}, {"sha": "85fa1619ceb8c998cf57a08415a8f133acd6cf71", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -485,6 +485,12 @@\n     UNSPEC_COND_GE\t; Used in aarch64-sve.md.\n     UNSPEC_COND_GT\t; Used in aarch64-sve.md.\n     UNSPEC_LASTB\t; Used in aarch64-sve.md.\n+    UNSPEC_FCADD90\t; Used in aarch64-simd.md.\n+    UNSPEC_FCADD270\t; Used in aarch64-simd.md.\n+    UNSPEC_FCMLA\t; Used in aarch64-simd.md.\n+    UNSPEC_FCMLA90\t; Used in aarch64-simd.md.\n+    UNSPEC_FCMLA180\t; Used in aarch64-simd.md.\n+    UNSPEC_FCMLA270\t; Used in aarch64-simd.md.\n ])\n \n ;; ------------------------------------------------------------------\n@@ -1134,6 +1140,13 @@\n \t\t\t (VNx16SI \"vnx4bi\") (VNx16SF \"vnx4bi\")\n \t\t\t (VNx8DI \"vnx2bi\") (VNx8DF \"vnx2bi\")])\n \n+;; On AArch64 the By element instruction doesn't have a 2S variant.\n+;; However because the instruction always selects a pair of values\n+;; The normal 3SAME instruction can be used here instead.\n+(define_mode_attr FCMLA_maybe_lane [(V2SF \"<Vtype>\") (V4SF \"<Vetype>[%4]\")\n+\t\t\t\t    (V4HF \"<Vetype>[%4]\") (V8HF \"<Vetype>[%4]\")\n+\t\t\t\t    ])\n+\n ;; -------------------------------------------------------------------\n ;; Code Iterators\n ;; -------------------------------------------------------------------\n@@ -1587,6 +1600,14 @@\n \t\t\t\t      UNSPEC_COND_EQ UNSPEC_COND_NE\n \t\t\t\t      UNSPEC_COND_GE UNSPEC_COND_GT])\n \n+(define_int_iterator FCADD [UNSPEC_FCADD90\n+\t\t\t    UNSPEC_FCADD270])\n+\n+(define_int_iterator FCMLA [UNSPEC_FCMLA\n+\t\t\t    UNSPEC_FCMLA90\n+\t\t\t    UNSPEC_FCMLA180\n+\t\t\t    UNSPEC_FCMLA270])\n+\n ;; Iterators for atomic operations.\n \n (define_int_iterator ATOMIC_LDOP\n@@ -1848,6 +1869,13 @@\n \t\t\t        (UNSPEC_COND_MAX \"fmaxnm\")\n \t\t\t        (UNSPEC_COND_MIN \"fminnm\")])\n \n+(define_int_attr rot [(UNSPEC_FCADD90 \"90\")\n+\t\t      (UNSPEC_FCADD270 \"270\")\n+\t\t      (UNSPEC_FCMLA \"0\")\n+\t\t      (UNSPEC_FCMLA90 \"90\")\n+\t\t      (UNSPEC_FCMLA180 \"180\")\n+\t\t      (UNSPEC_FCMLA270 \"270\")])\n+\n (define_int_attr sve_fmla_op [(UNSPEC_COND_FMLA \"fmla\")\n \t\t\t      (UNSPEC_COND_FMLS \"fmls\")\n \t\t\t      (UNSPEC_COND_FNMLA \"fnmla\")"}, {"sha": "f8f8dd09077a5c9d3691c95c6676ee36114786e4", "filename": "gcc/config/arm/types.md", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Farm%2Ftypes.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Fconfig%2Farm%2Ftypes.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Ftypes.md?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -763,6 +763,9 @@\n   neon_sub_halve,\\\n   neon_sub_halve_q,\\\n   neon_sub_halve_narrow_q,\\\n+\\\n+  neon_fcadd,\\\n+  neon_fcmla,\\\n \\\n   neon_abs,\\\n   neon_abs_q,\\"}, {"sha": "2df44c759cc8599bbc661b552c462595514516b1", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -1,3 +1,8 @@\n+2019-01-10  Tamar Christina  <tamar.christina@arm.com>\n+\n+\t* gcc.target/aarch64/advsimd-intrinsics/vector-complex.c: New test.\n+\t* gcc.target/aarch64/advsimd-intrinsics/vector-complex_f16.c: New test.\n+\n 2019-01-10  Tamar Christina  <tamar.christina@arm.com>\n \n \t* lib/target-supports.exp"}, {"sha": "b7c999333ed3a7aa9708bca3a0510ba754b7e4d4", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vector-complex.c", "status": "added", "additions": 251, "deletions": 0, "changes": 251, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvector-complex.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvector-complex.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvector-complex.c?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -0,0 +1,251 @@\n+/* { dg-skip-if \"\" { arm-*-* } } */\n+/* { dg-do assemble } */\n+/* { dg-require-effective-target arm_v8_3a_complex_neon_ok } */\n+/* { dg-add-options arm_v8_3a_complex_neon }  */\n+/* { dg-additional-options \"-O2 -save-temps\" } */\n+\n+#include <arm_neon.h>\n+\n+float32x2_t\n+test_vcadd_rot90_f32 (float32x2_t __a, float32x2_t __b)\n+{\n+  return vcadd_rot90_f32 (__a, __b);\n+}\n+\n+float32x4_t\n+test_vcaddq_rot90_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+  return vcaddq_rot90_f32 (__a, __b);\n+}\n+\n+#ifdef __ARM_ARCH_ISA_A64\n+float64x2_t\n+test_vcaddq_rot90_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+  return vcaddq_rot90_f64 (__a, __b);\n+}\n+#endif\n+\n+float32x2_t\n+test_vcadd_rot270_f32 (float32x2_t __a, float32x2_t __b)\n+{\n+  return vcadd_rot270_f32 (__a, __b);\n+}\n+\n+float32x4_t\n+test_vcaddq_rot270_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+  return vcaddq_rot270_f32 (__a, __b);\n+}\n+\n+#ifdef __ARM_ARCH_ISA_A64\n+float64x2_t\n+test_vcaddq_rot270_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+  return vcaddq_rot270_f64 (__a, __b);\n+}\n+#endif\n+\n+float32x2_t\n+test_vcmla_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return vcmla_f32 (__r, __a, __b);\n+}\n+\n+float32x4_t\n+test_vcmlaq_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return vcmlaq_f32 (__r, __a, __b);\n+}\n+\n+#ifdef __ARM_ARCH_ISA_A64\n+float64x2_t\n+test_vcmlaq_f64 (float64x2_t __r, float64x2_t __a, float64x2_t __b)\n+{\n+  return vcmlaq_f64 (__r, __a, __b);\n+}\n+#endif\n+\n+float32x2_t\n+test_vcmla_lane_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return vcmla_lane_f32 (__r, __a, __b, 0);\n+}\n+\n+float32x2_t\n+test_vcmla_laneq_f32 (float32x2_t __r, float32x2_t __a, float32x4_t __b)\n+{\n+  return vcmla_laneq_f32 (__r, __a, __b, 1);\n+}\n+\n+float32x4_t\n+test_vcmlaq_lane_f32 (float32x4_t __r, float32x4_t __a, float32x2_t __b)\n+{\n+  return vcmlaq_lane_f32 (__r, __a, __b, 0);\n+}\n+\n+float32x4_t\n+test_vcmlaq_laneq_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return vcmlaq_laneq_f32 (__r, __a, __b, 1);\n+}\n+\n+float32x2_t\n+test_vcmla_rot90_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return vcmla_rot90_f32 (__r, __a, __b);\n+}\n+\n+float32x4_t\n+test_vcmlaq_rot90_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return vcmlaq_rot90_f32 (__r, __a, __b);\n+}\n+\n+#ifdef __ARM_ARCH_ISA_A64\n+float64x2_t\n+test_vcmlaq_rot90_f64 (float64x2_t __r, float64x2_t __a, float64x2_t __b)\n+{\n+  return vcmlaq_rot90_f64 (__r, __a, __b);\n+}\n+#endif\n+\n+float32x2_t\n+test_vcmla_rot90_lane_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return vcmla_rot90_lane_f32 (__r, __a, __b, 0);\n+}\n+\n+float32x2_t\n+test_vcmla_rot90_laneq_f32 (float32x2_t __r, float32x2_t __a, float32x4_t __b)\n+{\n+  return vcmla_rot90_laneq_f32 (__r, __a, __b, 1);\n+}\n+\n+float32x4_t\n+test_vcmlaq_rot90_lane_f32 (float32x4_t __r, float32x4_t __a, float32x2_t __b)\n+{\n+  return vcmlaq_rot90_lane_f32 (__r, __a, __b, 0);\n+}\n+\n+float32x4_t\n+test_vcmlaq_rot90_laneq_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return vcmlaq_rot90_laneq_f32 (__r, __a, __b, 1);\n+}\n+\n+float32x2_t\n+test_vcmla_rot180_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return vcmla_rot180_f32 (__r, __a, __b);\n+}\n+\n+float32x4_t\n+test_vcmlaq_rot180_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return vcmlaq_rot180_f32 (__r, __a, __b);\n+}\n+\n+#ifdef __ARM_ARCH_ISA_A64\n+float64x2_t\n+test_vcmlaq_rot180_f64 (float64x2_t __r, float64x2_t __a, float64x2_t __b)\n+{\n+  return vcmlaq_rot180_f64 (__r, __a, __b);\n+}\n+#endif\n+\n+float32x2_t\n+test_vcmla_rot180_lane_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return vcmla_rot180_lane_f32 (__r, __a, __b, 0);\n+}\n+\n+float32x2_t\n+test_vcmla_rot180_laneq_f32 (float32x2_t __r, float32x2_t __a, float32x4_t __b)\n+{\n+  return vcmla_rot180_laneq_f32 (__r, __a, __b, 1);\n+}\n+\n+float32x4_t\n+test_vcmlaq_rot180_lane_f32 (float32x4_t __r, float32x4_t __a, float32x2_t __b)\n+{\n+  return vcmlaq_rot180_lane_f32 (__r, __a, __b, 0);\n+}\n+\n+float32x4_t\n+test_vcmlaq_rot180_laneq_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return vcmlaq_rot180_laneq_f32 (__r, __a, __b, 1);\n+}\n+\n+float32x2_t\n+test_vcmla_rot270_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return vcmla_rot270_f32 (__r, __a, __b);\n+}\n+\n+float32x4_t\n+test_vcmlaq_rot270_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return vcmlaq_rot270_f32 (__r, __a, __b);\n+}\n+\n+#ifdef __ARM_ARCH_ISA_A64\n+float64x2_t\n+test_vcmlaq_rot270_f64 (float64x2_t __r, float64x2_t __a, float64x2_t __b)\n+{\n+  return vcmlaq_rot270_f64 (__r, __a, __b);\n+}\n+#endif\n+\n+float32x2_t\n+test_vcmla_rot270_lane_f32 (float32x2_t __r, float32x2_t __a, float32x2_t __b)\n+{\n+  return vcmla_rot270_lane_f32 (__r, __a, __b, 0);\n+}\n+\n+float32x2_t\n+test_vcmla_rot270_laneq_f32 (float32x2_t __r, float32x2_t __a, float32x4_t __b)\n+{\n+  return vcmla_rot270_laneq_f32 (__r, __a, __b, 1);\n+}\n+\n+float32x4_t\n+test_vcmlaq_rot270_lane_f32 (float32x4_t __r, float32x4_t __a, float32x2_t __b)\n+{\n+  return vcmlaq_rot270_lane_f32 (__r, __a, __b, 0);\n+}\n+\n+float32x4_t\n+test_vcmlaq_rot270_laneq_f32 (float32x4_t __r, float32x4_t __a, float32x4_t __b)\n+{\n+  return vcmlaq_rot270_laneq_f32 (__r, __a, __b, 1);\n+}\n+\n+/* { dg-final { scan-assembler-times {fcadd\\tv[0-9]+.2d, v[0-9]+.2d, v[0-9]+.2d, #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcadd\\tv[0-9]+.2d, v[0-9]+.2d, v[0-9]+.2d, #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcadd\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.2s, #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcadd\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.2s, #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcadd\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4s, #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcadd\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4s, #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.2d, v[0-9]+.2d, v[0-9]+.2d, #0} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.2d, v[0-9]+.2d, v[0-9]+.2d, #180} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.2d, v[0-9]+.2d, v[0-9]+.2d, #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.2d, v[0-9]+.2d, v[0-9]+.2d, #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.2s, #0} 3 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.2s, #180} 3 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.2s, #270} 3 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.2s, v[0-9]+.2s, v[0-9]+.2s, #90} 3 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4s, #0} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4s, #180} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4s, #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4s, #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.s\\[0\\], #0} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.s\\[0\\], #180} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.s\\[0\\], #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.s\\[0\\], #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.s\\[1\\], #0} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.s\\[1\\], #180} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.s\\[1\\], #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.s\\[1\\], #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {dup\\td[0-9]+, v[0-9]+.d\\[1\\]} 4 { target { aarch64*-*-* } } } } */"}, {"sha": "dbcebcbfba67172de25bb3ab743270cacf7c9f96", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vector-complex_f16.c", "status": "added", "additions": 306, "deletions": 0, "changes": 306, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvector-complex_f16.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9d63f43b2d6ac073164a43116b4cb11d7d188ff1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvector-complex_f16.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvector-complex_f16.c?ref=9d63f43b2d6ac073164a43116b4cb11d7d188ff1", "patch": "@@ -0,0 +1,306 @@\n+/* { dg-skip-if \"\" { arm-*-* } } */\n+/* { dg-do assemble } */\n+/* { dg-require-effective-target arm_v8_3a_complex_neon_ok } */\n+/* { dg-require-effective-target arm_v8_2a_fp16_scalar_ok } */\n+/* { dg-add-options arm_v8_3a_complex_neon } */\n+/* { dg-additional-options \"-O2 -march=armv8.3-a+fp16 -save-temps\" } */\n+\n+#include <arm_neon.h>\n+\n+float16x4_t\n+test_vcadd_rot90_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+  return vcadd_rot90_f16 (__a, __b);\n+}\n+\n+float16x8_t\n+test_vcaddq_rot90_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+  return vcaddq_rot90_f16 (__a, __b);\n+}\n+\n+float16x4_t\n+test_vcadd_rot270_f16 (float16x4_t __a, float16x4_t __b)\n+{\n+  return vcadd_rot270_f16 (__a, __b);\n+}\n+\n+float16x8_t\n+test_vcaddq_rot270_f16 (float16x8_t __a, float16x8_t __b)\n+{\n+  return vcaddq_rot270_f16 (__a, __b);\n+}\n+\n+float16x4_t\n+test_vcmla_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_f16 (__r, __a, __b);\n+}\n+\n+float16x8_t\n+test_vcmlaq_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_f16 (__r, __a, __b);\n+}\n+\n+float16x4_t\n+test_vcmla_lane_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_lane_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x4_t\n+test_vcmla_laneq_f16 (float16x4_t __r, float16x4_t __a, float16x8_t __b)\n+{\n+  return vcmla_laneq_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x8_t\n+test_vcmlaq_lane_f16 (float16x8_t __r, float16x8_t __a, float16x4_t __b)\n+{\n+  return vcmlaq_lane_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x8_t\n+test_vcmlaq_laneq_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_laneq_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x4_t\n+test_vcmla_lane_f16_2 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_lane_f16 (__r, __a, __b, 1);\n+}\n+\n+float16x4_t\n+test_vcmla_laneq_f16_2 (float16x4_t __r, float16x4_t __a, float16x8_t __b)\n+{\n+  return vcmla_laneq_f16 (__r, __a, __b, 3);\n+}\n+\n+float16x8_t\n+test_vcmlaq_lane_f16_2 (float16x8_t __r, float16x8_t __a, float16x4_t __b)\n+{\n+  return vcmlaq_lane_f16 (__r, __a, __b, 1);\n+}\n+\n+float16x8_t\n+test_vcmlaq_laneq_f16_2 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_laneq_f16 (__r, __a, __b, 3);\n+}\n+\n+float16x4_t\n+test_vcmla_rot90_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_rot90_f16 (__r, __a, __b);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot90_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_rot90_f16 (__r, __a, __b);\n+}\n+\n+float16x4_t\n+test_vcmla_rot90_lane_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_rot90_lane_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x4_t\n+test_vcmla_rot90_laneq_f16 (float16x4_t __r, float16x4_t __a, float16x8_t __b)\n+{\n+  return vcmla_rot90_laneq_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot90_lane_f16 (float16x8_t __r, float16x8_t __a, float16x4_t __b)\n+{\n+  return vcmlaq_rot90_lane_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot90_laneq_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_rot90_laneq_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x4_t\n+test_vcmla_rot90_lane_f16_2 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_rot90_lane_f16 (__r, __a, __b, 1);\n+}\n+\n+float16x4_t\n+test_vcmla_rot90_laneq_f16_2 (float16x4_t __r, float16x4_t __a, float16x8_t __b)\n+{\n+  return vcmla_rot90_laneq_f16 (__r, __a, __b, 3);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot90_lane_f16_2 (float16x8_t __r, float16x8_t __a, float16x4_t __b)\n+{\n+  return vcmlaq_rot90_lane_f16 (__r, __a, __b, 1);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot90_laneq_f16_2 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_rot90_laneq_f16 (__r, __a, __b, 3);\n+}\n+\n+float16x4_t\n+test_vcmla_rot180_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_rot180_f16 (__r, __a, __b);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot180_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_rot180_f16 (__r, __a, __b);\n+}\n+\n+float16x4_t\n+test_vcmla_rot180_lane_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_rot180_lane_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x4_t\n+test_vcmla_rot180_laneq_f16 (float16x4_t __r, float16x4_t __a, float16x8_t __b)\n+{\n+  return vcmla_rot180_laneq_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot180_lane_f16 (float16x8_t __r, float16x8_t __a, float16x4_t __b)\n+{\n+  return vcmlaq_rot180_lane_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot180_laneq_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_rot180_laneq_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x4_t\n+test_vcmla_rot180_lane_f16_2 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_rot180_lane_f16 (__r, __a, __b, 1);\n+}\n+\n+float16x4_t\n+test_vcmla_rot180_laneq_f16_2 (float16x4_t __r, float16x4_t __a, float16x8_t __b)\n+{\n+  return vcmla_rot180_laneq_f16 (__r, __a, __b, 3);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot180_lane_f16_2 (float16x8_t __r, float16x8_t __a, float16x4_t __b)\n+{\n+  return vcmlaq_rot180_lane_f16 (__r, __a, __b, 1);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot180_laneq_f16_2 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_rot180_laneq_f16 (__r, __a, __b, 3);\n+}\n+\n+float16x4_t\n+test_vcmla_rot270_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_rot270_f16 (__r, __a, __b);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot270_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_rot270_f16 (__r, __a, __b);\n+}\n+\n+float16x4_t\n+test_vcmla_rot270_lane_f16 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_rot270_lane_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x4_t\n+test_vcmla_rot270_laneq_f16 (float16x4_t __r, float16x4_t __a, float16x8_t __b)\n+{\n+  return vcmla_rot270_laneq_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot270_lane_f16 (float16x8_t __r, float16x8_t __a, float16x4_t __b)\n+{\n+  return vcmlaq_rot270_lane_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot270_laneq_f16 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_rot270_laneq_f16 (__r, __a, __b, 0);\n+}\n+\n+float16x4_t\n+test_vcmla_rot270_lane_f16_2 (float16x4_t __r, float16x4_t __a, float16x4_t __b)\n+{\n+  return vcmla_rot270_lane_f16 (__r, __a, __b, 1);\n+}\n+\n+float16x4_t\n+test_vcmla_rot270_laneq_f16_2 (float16x4_t __r, float16x4_t __a, float16x8_t __b)\n+{\n+  return vcmla_rot270_laneq_f16 (__r, __a, __b, 3);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot270_lane_f16_2 (float16x8_t __r, float16x8_t __a, float16x4_t __b)\n+{\n+  return vcmlaq_rot270_lane_f16 (__r, __a, __b, 1);\n+}\n+\n+float16x8_t\n+test_vcmlaq_rot270_laneq_f16_2 (float16x8_t __r, float16x8_t __a, float16x8_t __b)\n+{\n+  return vcmlaq_rot270_laneq_f16 (__r, __a, __b, 3);\n+}\n+\n+/* { dg-final { scan-assembler-times {dup\\td[0-9]+, v[0-9]+.d\\[1\\]} 4 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcadd\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.4h, #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcadd\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.4h, #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcadd\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8h, #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcadd\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8h, #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.4h, #0} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.4h, #180} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.4h, #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.4h, #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.h\\[0\\], #0} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.h\\[0\\], #180} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.h\\[0\\], #270} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.h\\[0\\], #90} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.h\\[1\\], #0} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.h\\[1\\], #180} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.h\\[1\\], #270} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.4h, v[0-9]+.4h, v[0-9]+.h\\[1\\], #90} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8h, #0} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8h, #180} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8h, #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8h, #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[0\\], #0} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[0\\], #180} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[0\\], #270} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[0\\], #90} 2 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[1\\], #0} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[1\\], #180} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[1\\], #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[1\\], #90} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[3\\], #0} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[3\\], #180} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[3\\], #270} 1 { target { aarch64*-*-* } } } } */\n+/* { dg-final { scan-assembler-times {fcmla\\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.h\\[3\\], #90} 1 { target { aarch64*-*-* } } } } */"}]}