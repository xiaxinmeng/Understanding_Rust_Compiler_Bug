{"sha": "66f206b85395c273980e2b81a54dbddc4897e4a7", "node_id": "C_kwDOANBUbNoAKDY2ZjIwNmI4NTM5NWMyNzM5ODBlMmI4MWE1NGRiZGRjNDg5N2U0YTc", "commit": {"author": {"name": "Jonathan Wright", "email": "jonathan.wright@arm.com", "date": "2021-08-09T14:26:48Z"}, "committer": {"name": "Jonathan Wright", "email": "jonathan.wright@arm.com", "date": "2021-11-04T14:54:36Z"}, "message": "aarch64: Add machine modes for Neon vector-tuple types\n\nUntil now, GCC has used large integer machine modes (OI, CI and XI)\nto model Neon vector-tuple types. This is suboptimal for many\nreasons, the most notable are:\n\n 1) Large integer modes are opaque and modifying one vector in the\n    tuple requires a lot of inefficient set/get gymnastics. The\n    result is a lot of superfluous move instructions.\n 2) Large integer modes do not map well to types that are tuples of\n    64-bit vectors - we need additional zero-padding which again\n    results in superfluous move instructions.\n\nThis patch adds new machine modes that better model the C-level Neon\nvector-tuple types. The approach is somewhat similar to that already\nused for SVE vector-tuple types.\n\nAll of the AArch64 backend patterns and builtins that manipulate Neon\nvector tuples are updated to use the new machine modes. This has the\neffect of significantly reducing the amount of boiler-plate code in\nthe arm_neon.h header.\n\nWhile this patch increases the quality of code generated in many\ninstances, there is still room for significant improvement - which\nwill be attempted in subsequent patches.\n\ngcc/ChangeLog:\n\n2021-08-09  Jonathan Wright  <jonathan.wright@arm.com>\n\t    Richard Sandiford  <richard.sandiford@arm.com>\n\n\t* config/aarch64/aarch64-builtins.c (v2x8qi_UP): Define.\n\t(v2x4hi_UP): Likewise.\n\t(v2x4hf_UP): Likewise.\n\t(v2x4bf_UP): Likewise.\n\t(v2x2si_UP): Likewise.\n\t(v2x2sf_UP): Likewise.\n\t(v2x1di_UP): Likewise.\n\t(v2x1df_UP): Likewise.\n\t(v2x16qi_UP): Likewise.\n\t(v2x8hi_UP): Likewise.\n\t(v2x8hf_UP): Likewise.\n\t(v2x8bf_UP): Likewise.\n\t(v2x4si_UP): Likewise.\n\t(v2x4sf_UP): Likewise.\n\t(v2x2di_UP): Likewise.\n\t(v2x2df_UP): Likewise.\n\t(v3x8qi_UP): Likewise.\n\t(v3x4hi_UP): Likewise.\n\t(v3x4hf_UP): Likewise.\n\t(v3x4bf_UP): Likewise.\n\t(v3x2si_UP): Likewise.\n\t(v3x2sf_UP): Likewise.\n\t(v3x1di_UP): Likewise.\n\t(v3x1df_UP): Likewise.\n\t(v3x16qi_UP): Likewise.\n\t(v3x8hi_UP): Likewise.\n\t(v3x8hf_UP): Likewise.\n\t(v3x8bf_UP): Likewise.\n\t(v3x4si_UP): Likewise.\n\t(v3x4sf_UP): Likewise.\n\t(v3x2di_UP): Likewise.\n\t(v3x2df_UP): Likewise.\n\t(v4x8qi_UP): Likewise.\n\t(v4x4hi_UP): Likewise.\n\t(v4x4hf_UP): Likewise.\n\t(v4x4bf_UP): Likewise.\n\t(v4x2si_UP): Likewise.\n\t(v4x2sf_UP): Likewise.\n\t(v4x1di_UP): Likewise.\n\t(v4x1df_UP): Likewise.\n\t(v4x16qi_UP): Likewise.\n\t(v4x8hi_UP): Likewise.\n\t(v4x8hf_UP): Likewise.\n\t(v4x8bf_UP): Likewise.\n\t(v4x4si_UP): Likewise.\n\t(v4x4sf_UP): Likewise.\n\t(v4x2di_UP): Likewise.\n\t(v4x2df_UP): Likewise.\n\t(TYPES_GETREGP): Delete.\n\t(TYPES_SETREGP): Likewise.\n\t(TYPES_LOADSTRUCT_U): Define.\n\t(TYPES_LOADSTRUCT_P): Likewise.\n\t(TYPES_LOADSTRUCT_LANE_U): Likewise.\n\t(TYPES_LOADSTRUCT_LANE_P): Likewise.\n\t(TYPES_STORE1P): Move for consistency.\n\t(TYPES_STORESTRUCT_U): Define.\n\t(TYPES_STORESTRUCT_P): Likewise.\n\t(TYPES_STORESTRUCT_LANE_U): Likewise.\n\t(TYPES_STORESTRUCT_LANE_P): Likewise.\n\t(aarch64_simd_tuple_types): Define.\n\t(aarch64_lookup_simd_builtin_type): Handle tuple type lookup.\n\t(aarch64_init_simd_builtin_functions): Update frontend lookup\n\tfor builtin functions after handling arm_neon.h pragma.\n\t(register_tuple_type): Manually set modes of single-integer\n\ttuple types. Record tuple types.\n\t* config/aarch64/aarch64-modes.def\n\t(ADV_SIMD_D_REG_STRUCT_MODES): Define D-register tuple modes.\n\t(ADV_SIMD_Q_REG_STRUCT_MODES): Define Q-register tuple modes.\n\t(SVE_MODES): Give single-vector modes priority over vector-\n\ttuple modes.\n\t(VECTOR_MODES_WITH_PREFIX): Set partial-vector mode order to\n\tbe after all single-vector modes.\n\t* config/aarch64/aarch64-simd-builtins.def: Update builtin\n\tgenerator macros to reflect modifications to the backend\n\tpatterns.\n\t* config/aarch64/aarch64-simd.md (aarch64_simd_ld2<mode>):\n\tUse vector-tuple mode iterator and rename to...\n\t(aarch64_simd_ld2<vstruct_elt>): This.\n\t(aarch64_simd_ld2r<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_simd_ld2r<vstruct_elt>): This.\n\t(aarch64_vec_load_lanesoi_lane<mode>): Use vector-tuple mode\n\titerator and rename to...\n\t(aarch64_vec_load_lanes<mode>_lane<vstruct_elt>): This.\n\t(vec_load_lanesoi<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(vec_load_lanes<mode><vstruct_elt>): This.\n\t(aarch64_simd_st2<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_simd_st2<vstruct_elt>): This.\n\t(aarch64_vec_store_lanesoi_lane<mode>): Use vector-tuple mode\n\titerator and rename to...\n\t(aarch64_vec_store_lanes<mode>_lane<vstruct_elt>): This.\n\t(vec_store_lanesoi<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(vec_store_lanes<mode><vstruct_elt>): This.\n\t(aarch64_simd_ld3<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_simd_ld3<vstruct_elt>): This.\n\t(aarch64_simd_ld3r<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_simd_ld3r<vstruct_elt>): This.\n\t(aarch64_vec_load_lanesci_lane<mode>): Use vector-tuple mode\n\titerator and rename to...\n\t(vec_load_lanesci<mode>): This.\n\t(aarch64_simd_st3<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_simd_st3<vstruct_elt>): This.\n\t(aarch64_vec_store_lanesci_lane<mode>): Use vector-tuple mode\n\titerator and rename to...\n\t(vec_store_lanesci<mode>): This.\n\t(aarch64_simd_ld4<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_simd_ld4<vstruct_elt>): This.\n\t(aarch64_simd_ld4r<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_simd_ld4r<vstruct_elt>): This.\n\t(aarch64_vec_load_lanesxi_lane<mode>): Use vector-tuple mode\n\titerator and rename to...\n\t(vec_load_lanesxi<mode>): This.\n\t(aarch64_simd_st4<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_simd_st4<vstruct_elt>): This.\n\t(aarch64_vec_store_lanesxi_lane<mode>): Use vector-tuple mode\n\titerator and rename to...\n\t(vec_store_lanesxi<mode>): This.\n\t(mov<mode>): Define for Neon vector-tuple modes.\n\t(aarch64_ld1x3<VALLDIF:mode>): Use vector-tuple mode iterator\n\tand rename to...\n\t(aarch64_ld1x3<vstruct_elt>): This.\n\t(aarch64_ld1_x3_<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_ld1_x3_<vstruct_elt>): This.\n\t(aarch64_ld1x4<VALLDIF:mode>): Use vector-tuple mode iterator\n\tand rename to...\n\t(aarch64_ld1x4<vstruct_elt>): This.\n\t(aarch64_ld1_x4_<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_ld1_x4_<vstruct_elt>): This.\n\t(aarch64_st1x2<VALLDIF:mode>): Use vector-tuple mode iterator\n\tand rename to...\n\t(aarch64_st1x2<vstruct_elt>): This.\n\t(aarch64_st1_x2_<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_st1_x2_<vstruct_elt>): This.\n\t(aarch64_st1x3<VALLDIF:mode>): Use vector-tuple mode iterator\n\tand rename to...\n\t(aarch64_st1x3<vstruct_elt>): This.\n\t(aarch64_st1_x3_<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_st1_x3_<vstruct_elt>): This.\n\t(aarch64_st1x4<VALLDIF:mode>): Use vector-tuple mode iterator\n\tand rename to...\n\t(aarch64_st1x4<vstruct_elt>): This.\n\t(aarch64_st1_x4_<mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_st1_x4_<vstruct_elt>): This.\n\t(*aarch64_mov<mode>): Define for vector-tuple modes.\n\t(*aarch64_be_mov<mode>): Likewise.\n\t(aarch64_ld<VSTRUCT:nregs>r<VALLDIF:mode>): Use vector-tuple\n\tmode iterator and rename to...\n\t(aarch64_ld<nregs>r<vstruct_elt>): This.\n\t(aarch64_ld2<mode>_dreg): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_ld2<vstruct_elt>_dreg): This.\n\t(aarch64_ld3<mode>_dreg): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_ld3<vstruct_elt>_dreg): This.\n\t(aarch64_ld4<mode>_dreg): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_ld4<vstruct_elt>_dreg): This.\n\t(aarch64_ld<VSTRUCT:nregs><VDC:mode>): Use vector-tuple mode\n\titerator and rename to...\n\t(aarch64_ld<nregs><vstruct_elt>): Use vector-tuple mode\n\titerator and rename to...\n\t(aarch64_ld<VSTRUCT:nregs><VQ:mode>): Use vector-tuple mode\n\t(aarch64_ld1x2<VQ:mode>): Delete.\n\t(aarch64_ld1x2<VDC:mode>): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_ld1x2<vstruct_elt>): This.\n\t(aarch64_ld<VSTRUCT:nregs>_lane<VALLDIF:mode>): Use vector-\n\ttuple mode iterator and rename to...\n\t(aarch64_ld<nregs>_lane<vstruct_elt>): This.\n\t(aarch64_get_dreg<VSTRUCT:mode><VDC:mode>): Delete.\n\t(aarch64_get_qreg<VSTRUCT:mode><VQ:mode>): Likewise.\n\t(aarch64_st2<mode>_dreg): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_st2<vstruct_elt>_dreg): This.\n\t(aarch64_st3<mode>_dreg): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_st3<vstruct_elt>_dreg): This.\n\t(aarch64_st4<mode>_dreg): Use vector-tuple mode iterator and\n\trename to...\n\t(aarch64_st4<vstruct_elt>_dreg): This.\n\t(aarch64_st<VSTRUCT:nregs><VDC:mode>): Use vector-tuple mode\n\titerator and rename to...\n\t(aarch64_st<nregs><vstruct_elt>): This.\n\t(aarch64_st<VSTRUCT:nregs><VQ:mode>): Use vector-tuple mode\n\titerator and rename to aarch64_st<nregs><vstruct_elt>.\n\t(aarch64_st<VSTRUCT:nregs>_lane<VALLDIF:mode>): Use vector-\n\ttuple mode iterator and rename to...\n\t(aarch64_st<nregs>_lane<vstruct_elt>): This.\n\t(aarch64_set_qreg<VSTRUCT:mode><VQ:mode>): Delete.\n\t(aarch64_simd_ld1<mode>_x2): Use vector-tuple mode iterator\n\tand rename to...\n\t(aarch64_simd_ld1<vstruct_elt>_x2): This.\n\t* config/aarch64/aarch64.c (aarch64_advsimd_struct_mode_p):\n\tRefactor to include new vector-tuple modes.\n\t(aarch64_classify_vector_mode): Add cases for new vector-\n\ttuple modes.\n\t(aarch64_advsimd_partial_struct_mode_p): Define.\n\t(aarch64_advsimd_full_struct_mode_p): Likewise.\n\t(aarch64_advsimd_vector_array_mode): Likewise.\n\t(aarch64_sve_data_mode): Change location in file.\n\t(aarch64_array_mode): Handle case of Neon vector-tuple modes.\n\t(aarch64_hard_regno_nregs): Handle case of partial Neon\n\tvector structures.\n\t(aarch64_classify_address): Refactor to include handling of\n\tNeon vector-tuple modes.\n\t(aarch64_print_operand): Print \"d\" for \"%R\" for a partial\n\tNeon vector structure.\n\t(aarch64_expand_vec_perm_1): Use new vector-tuple mode.\n\t(aarch64_modes_tieable_p): Prevent tieing Neon partial struct\n\tmodes with scalar machines modes larger than 8 bytes.\n\t(aarch64_can_change_mode_class): Don't allow changes between\n\tpartial and full Neon vector-structure modes.\n\t* config/aarch64/arm_neon.h (vst2_lane_f16): Use updated\n\tbuiltin and remove boiler-plate code for opaque mode.\n\t(vst2_lane_f32): Likewise.\n\t(vst2_lane_f64): Likewise.\n\t(vst2_lane_p8): Likewise.\n\t(vst2_lane_p16): Likewise.\n\t(vst2_lane_p64): Likewise.\n\t(vst2_lane_s8): Likewise.\n\t(vst2_lane_s16): Likewise.\n\t(vst2_lane_s32): Likewise.\n\t(vst2_lane_s64): Likewise.\n\t(vst2_lane_u8): Likewise.\n\t(vst2_lane_u16): Likewise.\n\t(vst2_lane_u32): Likewise.\n\t(vst2_lane_u64): Likewise.\n\t(vst2q_lane_f16): Likewise.\n\t(vst2q_lane_f32): Likewise.\n\t(vst2q_lane_f64): Likewise.\n\t(vst2q_lane_p8): Likewise.\n\t(vst2q_lane_p16): Likewise.\n\t(vst2q_lane_p64): Likewise.\n\t(vst2q_lane_s8): Likewise.\n\t(vst2q_lane_s16): Likewise.\n\t(vst2q_lane_s32): Likewise.\n\t(vst2q_lane_s64): Likewise.\n\t(vst2q_lane_u8): Likewise.\n\t(vst2q_lane_u16): Likewise.\n\t(vst2q_lane_u32): Likewise.\n\t(vst2q_lane_u64): Likewise.\n\t(vst3_lane_f16): Likewise.\n\t(vst3_lane_f32): Likewise.\n\t(vst3_lane_f64): Likewise.\n\t(vst3_lane_p8): Likewise.\n\t(vst3_lane_p16): Likewise.\n\t(vst3_lane_p64): Likewise.\n\t(vst3_lane_s8): Likewise.\n\t(vst3_lane_s16): Likewise.\n\t(vst3_lane_s32): Likewise.\n\t(vst3_lane_s64): Likewise.\n\t(vst3_lane_u8): Likewise.\n\t(vst3_lane_u16): Likewise.\n\t(vst3_lane_u32): Likewise.\n\t(vst3_lane_u64): Likewise.\n\t(vst3q_lane_f16): Likewise.\n\t(vst3q_lane_f32): Likewise.\n\t(vst3q_lane_f64): Likewise.\n\t(vst3q_lane_p8): Likewise.\n\t(vst3q_lane_p16): Likewise.\n\t(vst3q_lane_p64): Likewise.\n\t(vst3q_lane_s8): Likewise.\n\t(vst3q_lane_s16): Likewise.\n\t(vst3q_lane_s32): Likewise.\n\t(vst3q_lane_s64): Likewise.\n\t(vst3q_lane_u8): Likewise.\n\t(vst3q_lane_u16): Likewise.\n\t(vst3q_lane_u32): Likewise.\n\t(vst3q_lane_u64): Likewise.\n\t(vst4_lane_f16): Likewise.\n\t(vst4_lane_f32): Likewise.\n\t(vst4_lane_f64): Likewise.\n\t(vst4_lane_p8): Likewise.\n\t(vst4_lane_p16): Likewise.\n\t(vst4_lane_p64): Likewise.\n\t(vst4_lane_s8): Likewise.\n\t(vst4_lane_s16): Likewise.\n\t(vst4_lane_s32): Likewise.\n\t(vst4_lane_s64): Likewise.\n\t(vst4_lane_u8): Likewise.\n\t(vst4_lane_u16): Likewise.\n\t(vst4_lane_u32): Likewise.\n\t(vst4_lane_u64): Likewise.\n\t(vst4q_lane_f16): Likewise.\n\t(vst4q_lane_f32): Likewise.\n\t(vst4q_lane_f64): Likewise.\n\t(vst4q_lane_p8): Likewise.\n\t(vst4q_lane_p16): Likewise.\n\t(vst4q_lane_p64): Likewise.\n\t(vst4q_lane_s8): Likewise.\n\t(vst4q_lane_s16): Likewise.\n\t(vst4q_lane_s32): Likewise.\n\t(vst4q_lane_s64): Likewise.\n\t(vst4q_lane_u8): Likewise.\n\t(vst4q_lane_u16): Likewise.\n\t(vst4q_lane_u32): Likewise.\n\t(vst4q_lane_u64): Likewise.\n\t(vtbl3_s8): Likewise.\n\t(vtbl3_u8): Likewise.\n\t(vtbl3_p8): Likewise.\n\t(vtbl4_s8): Likewise.\n\t(vtbl4_u8): Likewise.\n\t(vtbl4_p8): Likewise.\n\t(vld1_u8_x3): Likewise.\n\t(vld1_s8_x3): Likewise.\n\t(vld1_u16_x3): Likewise.\n\t(vld1_s16_x3): Likewise.\n\t(vld1_u32_x3): Likewise.\n\t(vld1_s32_x3): Likewise.\n\t(vld1_u64_x3): Likewise.\n\t(vld1_s64_x3): Likewise.\n\t(vld1_f16_x3): Likewise.\n\t(vld1_f32_x3): Likewise.\n\t(vld1_f64_x3): Likewise.\n\t(vld1_p8_x3): Likewise.\n\t(vld1_p16_x3): Likewise.\n\t(vld1_p64_x3): Likewise.\n\t(vld1q_u8_x3): Likewise.\n\t(vld1q_s8_x3): Likewise.\n\t(vld1q_u16_x3): Likewise.\n\t(vld1q_s16_x3): Likewise.\n\t(vld1q_u32_x3): Likewise.\n\t(vld1q_s32_x3): Likewise.\n\t(vld1q_u64_x3): Likewise.\n\t(vld1q_s64_x3): Likewise.\n\t(vld1q_f16_x3): Likewise.\n\t(vld1q_f32_x3): Likewise.\n\t(vld1q_f64_x3): Likewise.\n\t(vld1q_p8_x3): Likewise.\n\t(vld1q_p16_x3): Likewise.\n\t(vld1q_p64_x3): Likewise.\n\t(vld1_u8_x2): Likewise.\n\t(vld1_s8_x2): Likewise.\n\t(vld1_u16_x2): Likewise.\n\t(vld1_s16_x2): Likewise.\n\t(vld1_u32_x2): Likewise.\n\t(vld1_s32_x2): Likewise.\n\t(vld1_u64_x2): Likewise.\n\t(vld1_s64_x2): Likewise.\n\t(vld1_f16_x2): Likewise.\n\t(vld1_f32_x2): Likewise.\n\t(vld1_f64_x2): Likewise.\n\t(vld1_p8_x2): Likewise.\n\t(vld1_p16_x2): Likewise.\n\t(vld1_p64_x2): Likewise.\n\t(vld1q_u8_x2): Likewise.\n\t(vld1q_s8_x2): Likewise.\n\t(vld1q_u16_x2): Likewise.\n\t(vld1q_s16_x2): Likewise.\n\t(vld1q_u32_x2): Likewise.\n\t(vld1q_s32_x2): Likewise.\n\t(vld1q_u64_x2): Likewise.\n\t(vld1q_s64_x2): Likewise.\n\t(vld1q_f16_x2): Likewise.\n\t(vld1q_f32_x2): Likewise.\n\t(vld1q_f64_x2): Likewise.\n\t(vld1q_p8_x2): Likewise.\n\t(vld1q_p16_x2): Likewise.\n\t(vld1q_p64_x2): Likewise.\n\t(vld1_s8_x4): Likewise.\n\t(vld1q_s8_x4): Likewise.\n\t(vld1_s16_x4): Likewise.\n\t(vld1q_s16_x4): Likewise.\n\t(vld1_s32_x4): Likewise.\n\t(vld1q_s32_x4): Likewise.\n\t(vld1_u8_x4): Likewise.\n\t(vld1q_u8_x4): Likewise.\n\t(vld1_u16_x4): Likewise.\n\t(vld1q_u16_x4): Likewise.\n\t(vld1_u32_x4): Likewise.\n\t(vld1q_u32_x4): Likewise.\n\t(vld1_f16_x4): Likewise.\n\t(vld1q_f16_x4): Likewise.\n\t(vld1_f32_x4): Likewise.\n\t(vld1q_f32_x4): Likewise.\n\t(vld1_p8_x4): Likewise.\n\t(vld1q_p8_x4): Likewise.\n\t(vld1_p16_x4): Likewise.\n\t(vld1q_p16_x4): Likewise.\n\t(vld1_s64_x4): Likewise.\n\t(vld1_u64_x4): Likewise.\n\t(vld1_p64_x4): Likewise.\n\t(vld1q_s64_x4): Likewise.\n\t(vld1q_u64_x4): Likewise.\n\t(vld1q_p64_x4): Likewise.\n\t(vld1_f64_x4): Likewise.\n\t(vld1q_f64_x4): Likewise.\n\t(vld2_s64): Likewise.\n\t(vld2_u64): Likewise.\n\t(vld2_f64): Likewise.\n\t(vld2_s8): Likewise.\n\t(vld2_p8): Likewise.\n\t(vld2_p64): Likewise.\n\t(vld2_s16): Likewise.\n\t(vld2_p16): Likewise.\n\t(vld2_s32): Likewise.\n\t(vld2_u8): Likewise.\n\t(vld2_u16): Likewise.\n\t(vld2_u32): Likewise.\n\t(vld2_f16): Likewise.\n\t(vld2_f32): Likewise.\n\t(vld2q_s8): Likewise.\n\t(vld2q_p8): Likewise.\n\t(vld2q_s16): Likewise.\n\t(vld2q_p16): Likewise.\n\t(vld2q_p64): Likewise.\n\t(vld2q_s32): Likewise.\n\t(vld2q_s64): Likewise.\n\t(vld2q_u8): Likewise.\n\t(vld2q_u16): Likewise.\n\t(vld2q_u32): Likewise.\n\t(vld2q_u64): Likewise.\n\t(vld2q_f16): Likewise.\n\t(vld2q_f32): Likewise.\n\t(vld2q_f64): Likewise.\n\t(vld3_s64): Likewise.\n\t(vld3_u64): Likewise.\n\t(vld3_f64): Likewise.\n\t(vld3_s8): Likewise.\n\t(vld3_p8): Likewise.\n\t(vld3_s16): Likewise.\n\t(vld3_p16): Likewise.\n\t(vld3_s32): Likewise.\n\t(vld3_u8): Likewise.\n\t(vld3_u16): Likewise.\n\t(vld3_u32): Likewise.\n\t(vld3_f16): Likewise.\n\t(vld3_f32): Likewise.\n\t(vld3_p64): Likewise.\n\t(vld3q_s8): Likewise.\n\t(vld3q_p8): Likewise.\n\t(vld3q_s16): Likewise.\n\t(vld3q_p16): Likewise.\n\t(vld3q_s32): Likewise.\n\t(vld3q_s64): Likewise.\n\t(vld3q_u8): Likewise.\n\t(vld3q_u16): Likewise.\n\t(vld3q_u32): Likewise.\n\t(vld3q_u64): Likewise.\n\t(vld3q_f16): Likewise.\n\t(vld3q_f32): Likewise.\n\t(vld3q_f64): Likewise.\n\t(vld3q_p64): Likewise.\n\t(vld4_s64): Likewise.\n\t(vld4_u64): Likewise.\n\t(vld4_f64): Likewise.\n\t(vld4_s8): Likewise.\n\t(vld4_p8): Likewise.\n\t(vld4_s16): Likewise.\n\t(vld4_p16): Likewise.\n\t(vld4_s32): Likewise.\n\t(vld4_u8): Likewise.\n\t(vld4_u16): Likewise.\n\t(vld4_u32): Likewise.\n\t(vld4_f16): Likewise.\n\t(vld4_f32): Likewise.\n\t(vld4_p64): Likewise.\n\t(vld4q_s8): Likewise.\n\t(vld4q_p8): Likewise.\n\t(vld4q_s16): Likewise.\n\t(vld4q_p16): Likewise.\n\t(vld4q_s32): Likewise.\n\t(vld4q_s64): Likewise.\n\t(vld4q_u8): Likewise.\n\t(vld4q_u16): Likewise.\n\t(vld4q_u32): Likewise.\n\t(vld4q_u64): Likewise.\n\t(vld4q_f16): Likewise.\n\t(vld4q_f32): Likewise.\n\t(vld4q_f64): Likewise.\n\t(vld4q_p64): Likewise.\n\t(vld2_dup_s8): Likewise.\n\t(vld2_dup_s16): Likewise.\n\t(vld2_dup_s32): Likewise.\n\t(vld2_dup_f16): Likewise.\n\t(vld2_dup_f32): Likewise.\n\t(vld2_dup_f64): Likewise.\n\t(vld2_dup_u8): Likewise.\n\t(vld2_dup_u16): Likewise.\n\t(vld2_dup_u32): Likewise.\n\t(vld2_dup_p8): Likewise.\n\t(vld2_dup_p16): Likewise.\n\t(vld2_dup_p64): Likewise.\n\t(vld2_dup_s64): Likewise.\n\t(vld2_dup_u64): Likewise.\n\t(vld2q_dup_s8): Likewise.\n\t(vld2q_dup_p8): Likewise.\n\t(vld2q_dup_s16): Likewise.\n\t(vld2q_dup_p16): Likewise.\n\t(vld2q_dup_s32): Likewise.\n\t(vld2q_dup_s64): Likewise.\n\t(vld2q_dup_u8): Likewise.\n\t(vld2q_dup_u16): Likewise.\n\t(vld2q_dup_u32): Likewise.\n\t(vld2q_dup_u64): Likewise.\n\t(vld2q_dup_f16): Likewise.\n\t(vld2q_dup_f32): Likewise.\n\t(vld2q_dup_f64): Likewise.\n\t(vld2q_dup_p64): Likewise.\n\t(vld3_dup_s64): Likewise.\n\t(vld3_dup_u64): Likewise.\n\t(vld3_dup_f64): Likewise.\n\t(vld3_dup_s8): Likewise.\n\t(vld3_dup_p8): Likewise.\n\t(vld3_dup_s16): Likewise.\n\t(vld3_dup_p16): Likewise.\n\t(vld3_dup_s32): Likewise.\n\t(vld3_dup_u8): Likewise.\n\t(vld3_dup_u16): Likewise.\n\t(vld3_dup_u32): Likewise.\n\t(vld3_dup_f16): Likewise.\n\t(vld3_dup_f32): Likewise.\n\t(vld3_dup_p64): Likewise.\n\t(vld3q_dup_s8): Likewise.\n\t(vld3q_dup_p8): Likewise.\n\t(vld3q_dup_s16): Likewise.\n\t(vld3q_dup_p16): Likewise.\n\t(vld3q_dup_s32): Likewise.\n\t(vld3q_dup_s64): Likewise.\n\t(vld3q_dup_u8): Likewise.\n\t(vld3q_dup_u16): Likewise.\n\t(vld3q_dup_u32): Likewise.\n\t(vld3q_dup_u64): Likewise.\n\t(vld3q_dup_f16): Likewise.\n\t(vld3q_dup_f32): Likewise.\n\t(vld3q_dup_f64): Likewise.\n\t(vld3q_dup_p64): Likewise.\n\t(vld4_dup_s64): Likewise.\n\t(vld4_dup_u64): Likewise.\n\t(vld4_dup_f64): Likewise.\n\t(vld4_dup_s8): Likewise.\n\t(vld4_dup_p8): Likewise.\n\t(vld4_dup_s16): Likewise.\n\t(vld4_dup_p16): Likewise.\n\t(vld4_dup_s32): Likewise.\n\t(vld4_dup_u8): Likewise.\n\t(vld4_dup_u16): Likewise.\n\t(vld4_dup_u32): Likewise.\n\t(vld4_dup_f16): Likewise.\n\t(vld4_dup_f32): Likewise.\n\t(vld4_dup_p64): Likewise.\n\t(vld4q_dup_s8): Likewise.\n\t(vld4q_dup_p8): Likewise.\n\t(vld4q_dup_s16): Likewise.\n\t(vld4q_dup_p16): Likewise.\n\t(vld4q_dup_s32): Likewise.\n\t(vld4q_dup_s64): Likewise.\n\t(vld4q_dup_u8): Likewise.\n\t(vld4q_dup_u16): Likewise.\n\t(vld4q_dup_u32): Likewise.\n\t(vld4q_dup_u64): Likewise.\n\t(vld4q_dup_f16): Likewise.\n\t(vld4q_dup_f32): Likewise.\n\t(vld4q_dup_f64): Likewise.\n\t(vld4q_dup_p64): Likewise.\n\t(vld2_lane_u8): Likewise.\n\t(vld2_lane_u16): Likewise.\n\t(vld2_lane_u32): Likewise.\n\t(vld2_lane_u64): Likewise.\n\t(vld2_lane_s8): Likewise.\n\t(vld2_lane_s16): Likewise.\n\t(vld2_lane_s32): Likewise.\n\t(vld2_lane_s64): Likewise.\n\t(vld2_lane_f16): Likewise.\n\t(vld2_lane_f32): Likewise.\n\t(vld2_lane_f64): Likewise.\n\t(vld2_lane_p8): Likewise.\n\t(vld2_lane_p16): Likewise.\n\t(vld2_lane_p64): Likewise.\n\t(vld2q_lane_u8): Likewise.\n\t(vld2q_lane_u16): Likewise.\n\t(vld2q_lane_u32): Likewise.\n\t(vld2q_lane_u64): Likewise.\n\t(vld2q_lane_s8): Likewise.\n\t(vld2q_lane_s16): Likewise.\n\t(vld2q_lane_s32): Likewise.\n\t(vld2q_lane_s64): Likewise.\n\t(vld2q_lane_f16): Likewise.\n\t(vld2q_lane_f32): Likewise.\n\t(vld2q_lane_f64): Likewise.\n\t(vld2q_lane_p8): Likewise.\n\t(vld2q_lane_p16): Likewise.\n\t(vld2q_lane_p64): Likewise.\n\t(vld3_lane_u8): Likewise.\n\t(vld3_lane_u16): Likewise.\n\t(vld3_lane_u32): Likewise.\n\t(vld3_lane_u64): Likewise.\n\t(vld3_lane_s8): Likewise.\n\t(vld3_lane_s16): Likewise.\n\t(vld3_lane_s32): Likewise.\n\t(vld3_lane_s64): Likewise.\n\t(vld3_lane_f16): Likewise.\n\t(vld3_lane_f32): Likewise.\n\t(vld3_lane_f64): Likewise.\n\t(vld3_lane_p8): Likewise.\n\t(vld3_lane_p16): Likewise.\n\t(vld3_lane_p64): Likewise.\n\t(vld3q_lane_u8): Likewise.\n\t(vld3q_lane_u16): Likewise.\n\t(vld3q_lane_u32): Likewise.\n\t(vld3q_lane_u64): Likewise.\n\t(vld3q_lane_s8): Likewise.\n\t(vld3q_lane_s16): Likewise.\n\t(vld3q_lane_s32): Likewise.\n\t(vld3q_lane_s64): Likewise.\n\t(vld3q_lane_f16): Likewise.\n\t(vld3q_lane_f32): Likewise.\n\t(vld3q_lane_f64): Likewise.\n\t(vld3q_lane_p8): Likewise.\n\t(vld3q_lane_p16): Likewise.\n\t(vld3q_lane_p64): Likewise.\n\t(vld4_lane_u8): Likewise.\n\t(vld4_lane_u16): Likewise.\n\t(vld4_lane_u32): Likewise.\n\t(vld4_lane_u64): Likewise.\n\t(vld4_lane_s8): Likewise.\n\t(vld4_lane_s16): Likewise.\n\t(vld4_lane_s32): Likewise.\n\t(vld4_lane_s64): Likewise.\n\t(vld4_lane_f16): Likewise.\n\t(vld4_lane_f32): Likewise.\n\t(vld4_lane_f64): Likewise.\n\t(vld4_lane_p8): Likewise.\n\t(vld4_lane_p16): Likewise.\n\t(vld4_lane_p64): Likewise.\n\t(vld4q_lane_u8): Likewise.\n\t(vld4q_lane_u16): Likewise.\n\t(vld4q_lane_u32): Likewise.\n\t(vld4q_lane_u64): Likewise.\n\t(vld4q_lane_s8): Likewise.\n\t(vld4q_lane_s16): Likewise.\n\t(vld4q_lane_s32): Likewise.\n\t(vld4q_lane_s64): Likewise.\n\t(vld4q_lane_f16): Likewise.\n\t(vld4q_lane_f32): Likewise.\n\t(vld4q_lane_f64): Likewise.\n\t(vld4q_lane_p8): Likewise.\n\t(vld4q_lane_p16): Likewise.\n\t(vld4q_lane_p64): Likewise.\n\t(vqtbl2_s8): Likewise.\n\t(vqtbl2_u8): Likewise.\n\t(vqtbl2_p8): Likewise.\n\t(vqtbl2q_s8): Likewise.\n\t(vqtbl2q_u8): Likewise.\n\t(vqtbl2q_p8): Likewise.\n\t(vqtbl3_s8): Likewise.\n\t(vqtbl3_u8): Likewise.\n\t(vqtbl3_p8): Likewise.\n\t(vqtbl3q_s8): Likewise.\n\t(vqtbl3q_u8): Likewise.\n\t(vqtbl3q_p8): Likewise.\n\t(vqtbl4_s8): Likewise.\n\t(vqtbl4_u8): Likewise.\n\t(vqtbl4_p8): Likewise.\n\t(vqtbl4q_s8): Likewise.\n\t(vqtbl4q_u8): Likewise.\n\t(vqtbl4q_p8): Likewise.\n\t(vqtbx2_s8): Likewise.\n\t(vqtbx2_u8): Likewise.\n\t(vqtbx2_p8): Likewise.\n\t(vqtbx2q_s8): Likewise.\n\t(vqtbx2q_u8): Likewise.\n\t(vqtbx2q_p8): Likewise.\n\t(vqtbx3_s8): Likewise.\n\t(vqtbx3_u8): Likewise.\n\t(vqtbx3_p8): Likewise.\n\t(vqtbx3q_s8): Likewise.\n\t(vqtbx3q_u8): Likewise.\n\t(vqtbx3q_p8): Likewise.\n\t(vqtbx4_s8): Likewise.\n\t(vqtbx4_u8): Likewise.\n\t(vqtbx4_p8): Likewise.\n\t(vqtbx4q_s8): Likewise.\n\t(vqtbx4q_u8): Likewise.\n\t(vqtbx4q_p8): Likewise.\n\t(vst1_s64_x2): Likewise.\n\t(vst1_u64_x2): Likewise.\n\t(vst1_f64_x2): Likewise.\n\t(vst1_s8_x2): Likewise.\n\t(vst1_p8_x2): Likewise.\n\t(vst1_s16_x2): Likewise.\n\t(vst1_p16_x2): Likewise.\n\t(vst1_s32_x2): Likewise.\n\t(vst1_u8_x2): Likewise.\n\t(vst1_u16_x2): Likewise.\n\t(vst1_u32_x2): Likewise.\n\t(vst1_f16_x2): Likewise.\n\t(vst1_f32_x2): Likewise.\n\t(vst1_p64_x2): Likewise.\n\t(vst1q_s8_x2): Likewise.\n\t(vst1q_p8_x2): Likewise.\n\t(vst1q_s16_x2): Likewise.\n\t(vst1q_p16_x2): Likewise.\n\t(vst1q_s32_x2): Likewise.\n\t(vst1q_s64_x2): Likewise.\n\t(vst1q_u8_x2): Likewise.\n\t(vst1q_u16_x2): Likewise.\n\t(vst1q_u32_x2): Likewise.\n\t(vst1q_u64_x2): Likewise.\n\t(vst1q_f16_x2): Likewise.\n\t(vst1q_f32_x2): Likewise.\n\t(vst1q_f64_x2): Likewise.\n\t(vst1q_p64_x2): Likewise.\n\t(vst1_s64_x3): Likewise.\n\t(vst1_u64_x3): Likewise.\n\t(vst1_f64_x3): Likewise.\n\t(vst1_s8_x3): Likewise.\n\t(vst1_p8_x3): Likewise.\n\t(vst1_s16_x3): Likewise.\n\t(vst1_p16_x3): Likewise.\n\t(vst1_s32_x3): Likewise.\n\t(vst1_u8_x3): Likewise.\n\t(vst1_u16_x3): Likewise.\n\t(vst1_u32_x3): Likewise.\n\t(vst1_f16_x3): Likewise.\n\t(vst1_f32_x3): Likewise.\n\t(vst1_p64_x3): Likewise.\n\t(vst1q_s8_x3): Likewise.\n\t(vst1q_p8_x3): Likewise.\n\t(vst1q_s16_x3): Likewise.\n\t(vst1q_p16_x3): Likewise.\n\t(vst1q_s32_x3): Likewise.\n\t(vst1q_s64_x3): Likewise.\n\t(vst1q_u8_x3): Likewise.\n\t(vst1q_u16_x3): Likewise.\n\t(vst1q_u32_x3): Likewise.\n\t(vst1q_u64_x3): Likewise.\n\t(vst1q_f16_x3): Likewise.\n\t(vst1q_f32_x3): Likewise.\n\t(vst1q_f64_x3): Likewise.\n\t(vst1q_p64_x3): Likewise.\n\t(vst1_s8_x4): Likewise.\n\t(vst1q_s8_x4): Likewise.\n\t(vst1_s16_x4): Likewise.\n\t(vst1q_s16_x4): Likewise.\n\t(vst1_s32_x4): Likewise.\n\t(vst1q_s32_x4): Likewise.\n\t(vst1_u8_x4): Likewise.\n\t(vst1q_u8_x4): Likewise.\n\t(vst1_u16_x4): Likewise.\n\t(vst1q_u16_x4): Likewise.\n\t(vst1_u32_x4): Likewise.\n\t(vst1q_u32_x4): Likewise.\n\t(vst1_f16_x4): Likewise.\n\t(vst1q_f16_x4): Likewise.\n\t(vst1_f32_x4): Likewise.\n\t(vst1q_f32_x4): Likewise.\n\t(vst1_p8_x4): Likewise.\n\t(vst1q_p8_x4): Likewise.\n\t(vst1_p16_x4): Likewise.\n\t(vst1q_p16_x4): Likewise.\n\t(vst1_s64_x4): Likewise.\n\t(vst1_u64_x4): Likewise.\n\t(vst1_p64_x4): Likewise.\n\t(vst1q_s64_x4): Likewise.\n\t(vst1q_u64_x4): Likewise.\n\t(vst1q_p64_x4): Likewise.\n\t(vst1_f64_x4): Likewise.\n\t(vst1q_f64_x4): Likewise.\n\t(vst2_s64): Likewise.\n\t(vst2_u64): Likewise.\n\t(vst2_f64): Likewise.\n\t(vst2_s8): Likewise.\n\t(vst2_p8): Likewise.\n\t(vst2_s16): Likewise.\n\t(vst2_p16): Likewise.\n\t(vst2_s32): Likewise.\n\t(vst2_u8): Likewise.\n\t(vst2_u16): Likewise.\n\t(vst2_u32): Likewise.\n\t(vst2_f16): Likewise.\n\t(vst2_f32): Likewise.\n\t(vst2_p64): Likewise.\n\t(vst2q_s8): Likewise.\n\t(vst2q_p8): Likewise.\n\t(vst2q_s16): Likewise.\n\t(vst2q_p16): Likewise.\n\t(vst2q_s32): Likewise.\n\t(vst2q_s64): Likewise.\n\t(vst2q_u8): Likewise.\n\t(vst2q_u16): Likewise.\n\t(vst2q_u32): Likewise.\n\t(vst2q_u64): Likewise.\n\t(vst2q_f16): Likewise.\n\t(vst2q_f32): Likewise.\n\t(vst2q_f64): Likewise.\n\t(vst2q_p64): Likewise.\n\t(vst3_s64): Likewise.\n\t(vst3_u64): Likewise.\n\t(vst3_f64): Likewise.\n\t(vst3_s8): Likewise.\n\t(vst3_p8): Likewise.\n\t(vst3_s16): Likewise.\n\t(vst3_p16): Likewise.\n\t(vst3_s32): Likewise.\n\t(vst3_u8): Likewise.\n\t(vst3_u16): Likewise.\n\t(vst3_u32): Likewise.\n\t(vst3_f16): Likewise.\n\t(vst3_f32): Likewise.\n\t(vst3_p64): Likewise.\n\t(vst3q_s8): Likewise.\n\t(vst3q_p8): Likewise.\n\t(vst3q_s16): Likewise.\n\t(vst3q_p16): Likewise.\n\t(vst3q_s32): Likewise.\n\t(vst3q_s64): Likewise.\n\t(vst3q_u8): Likewise.\n\t(vst3q_u16): Likewise.\n\t(vst3q_u32): Likewise.\n\t(vst3q_u64): Likewise.\n\t(vst3q_f16): Likewise.\n\t(vst3q_f32): Likewise.\n\t(vst3q_f64): Likewise.\n\t(vst3q_p64): Likewise.\n\t(vst4_s64): Likewise.\n\t(vst4_u64): Likewise.\n\t(vst4_f64): Likewise.\n\t(vst4_s8): Likewise.\n\t(vst4_p8): Likewise.\n\t(vst4_s16): Likewise.\n\t(vst4_p16): Likewise.\n\t(vst4_s32): Likewise.\n\t(vst4_u8): Likewise.\n\t(vst4_u16): Likewise.\n\t(vst4_u32): Likewise.\n\t(vst4_f16): Likewise.\n\t(vst4_f32): Likewise.\n\t(vst4_p64): Likewise.\n\t(vst4q_s8): Likewise.\n\t(vst4q_p8): Likewise.\n\t(vst4q_s16): Likewise.\n\t(vst4q_p16): Likewise.\n\t(vst4q_s32): Likewise.\n\t(vst4q_s64): Likewise.\n\t(vst4q_u8): Likewise.\n\t(vst4q_u16): Likewise.\n\t(vst4q_u32): Likewise.\n\t(vst4q_u64): Likewise.\n\t(vst4q_f16): Likewise.\n\t(vst4q_f32): Likewise.\n\t(vst4q_f64): Likewise.\n\t(vst4q_p64): Likewise.\n\t(vtbx4_s8): Likewise.\n\t(vtbx4_u8): Likewise.\n\t(vtbx4_p8): Likewise.\n\t(vld1_bf16_x2): Likewise.\n\t(vld1q_bf16_x2): Likewise.\n\t(vld1_bf16_x3): Likewise.\n\t(vld1q_bf16_x3): Likewise.\n\t(vld1_bf16_x4): Likewise.\n\t(vld1q_bf16_x4): Likewise.\n\t(vld2_bf16): Likewise.\n\t(vld2q_bf16): Likewise.\n\t(vld2_dup_bf16): Likewise.\n\t(vld2q_dup_bf16): Likewise.\n\t(vld3_bf16): Likewise.\n\t(vld3q_bf16): Likewise.\n\t(vld3_dup_bf16): Likewise.\n\t(vld3q_dup_bf16): Likewise.\n\t(vld4_bf16): Likewise.\n\t(vld4q_bf16): Likewise.\n\t(vld4_dup_bf16): Likewise.\n\t(vld4q_dup_bf16): Likewise.\n\t(vst1_bf16_x2): Likewise.\n\t(vst1q_bf16_x2): Likewise.\n\t(vst1_bf16_x3): Likewise.\n\t(vst1q_bf16_x3): Likewise.\n\t(vst1_bf16_x4): Likewise.\n\t(vst1q_bf16_x4): Likewise.\n\t(vst2_bf16): Likewise.\n\t(vst2q_bf16): Likewise.\n\t(vst3_bf16): Likewise.\n\t(vst3q_bf16): Likewise.\n\t(vst4_bf16): Likewise.\n\t(vst4q_bf16): Likewise.\n\t(vld2_lane_bf16): Likewise.\n\t(vld2q_lane_bf16): Likewise.\n\t(vld3_lane_bf16): Likewise.\n\t(vld3q_lane_bf16): Likewise.\n\t(vld4_lane_bf16): Likewise.\n\t(vld4q_lane_bf16): Likewise.\n\t(vst2_lane_bf16): Likewise.\n\t(vst2q_lane_bf16): Likewise.\n\t(vst3_lane_bf16): Likewise.\n\t(vst3q_lane_bf16): Likewise.\n\t(vst4_lane_bf16): Likewise.\n\t(vst4q_lane_bf16): Likewise.\n\t* config/aarch64/geniterators.sh: Modify iterator regex to\n\tmatch new vector-tuple modes.\n\t* config/aarch64/iterators.md (insn_count): Extend mode\n\tattribute with vector-tuple type information.\n\t(nregs): Likewise.\n\t(Vendreg): Likewise.\n\t(Vetype): Likewise.\n\t(Vtype): Likewise.\n\t(VSTRUCT_2D): New mode iterator.\n\t(VSTRUCT_2DNX): Likewise.\n\t(VSTRUCT_2DX): Likewise.\n\t(VSTRUCT_2Q): Likewise.\n\t(VSTRUCT_2QD): Likewise.\n\t(VSTRUCT_3D): Likewise.\n\t(VSTRUCT_3DNX): Likewise.\n\t(VSTRUCT_3DX): Likewise.\n\t(VSTRUCT_3Q): Likewise.\n\t(VSTRUCT_3QD): Likewise.\n\t(VSTRUCT_4D): Likewise.\n\t(VSTRUCT_4DNX): Likewise.\n\t(VSTRUCT_4DX): Likewise.\n\t(VSTRUCT_4Q): Likewise.\n\t(VSTRUCT_4QD): Likewise.\n\t(VSTRUCT_D): Likewise.\n\t(VSTRUCT_Q): Likewise.\n\t(VSTRUCT_QD): Likewise.\n\t(VSTRUCT_ELT): New mode attribute.\n\t(vstruct_elt): Likewise.\n\t* genmodes.c (VECTOR_MODE): Add default prefix and order\n\tparameters.\n\t(VECTOR_MODE_WITH_PREFIX): Define.\n\t(make_vector_mode): Add mode prefix and order parameters.\n\ngcc/testsuite/ChangeLog:\n\n\t* gcc.target/aarch64/advsimd-intrinsics/bf16_vldN_lane_2.c:\n\tRelax incorrect register number requirement.\n\t* gcc.target/aarch64/sve/pcs/struct_3_256.c: Accept\n\tequivalent codegen with fmov.", "tree": {"sha": "b405ef729ea395a3da2f42ecb78a1fdef0853e91", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/b405ef729ea395a3da2f42ecb78a1fdef0853e91"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/66f206b85395c273980e2b81a54dbddc4897e4a7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/66f206b85395c273980e2b81a54dbddc4897e4a7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/66f206b85395c273980e2b81a54dbddc4897e4a7", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/66f206b85395c273980e2b81a54dbddc4897e4a7/comments", "author": {"login": "jwright-arm", "id": 31624044, "node_id": "MDQ6VXNlcjMxNjI0MDQ0", "avatar_url": "https://avatars.githubusercontent.com/u/31624044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwright-arm", "html_url": "https://github.com/jwright-arm", "followers_url": "https://api.github.com/users/jwright-arm/followers", "following_url": "https://api.github.com/users/jwright-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jwright-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwright-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwright-arm/subscriptions", "organizations_url": "https://api.github.com/users/jwright-arm/orgs", "repos_url": "https://api.github.com/users/jwright-arm/repos", "events_url": "https://api.github.com/users/jwright-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jwright-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jwright-arm", "id": 31624044, "node_id": "MDQ6VXNlcjMxNjI0MDQ0", "avatar_url": "https://avatars.githubusercontent.com/u/31624044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwright-arm", "html_url": "https://github.com/jwright-arm", "followers_url": "https://api.github.com/users/jwright-arm/followers", "following_url": "https://api.github.com/users/jwright-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jwright-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwright-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwright-arm/subscriptions", "organizations_url": "https://api.github.com/users/jwright-arm/orgs", "repos_url": "https://api.github.com/users/jwright-arm/repos", "events_url": "https://api.github.com/users/jwright-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jwright-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "4e5929e4575922015ff4634af4dea59c59a44f10", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4e5929e4575922015ff4634af4dea59c59a44f10", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4e5929e4575922015ff4634af4dea59c59a44f10"}], "stats": {"total": 7300, "additions": 2155, "deletions": 5145}, "files": [{"sha": "ed91c2b0997671571fbf03db1ca1a6d7b879e303", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "modified", "additions": 127, "deletions": 17, "changes": 144, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=66f206b85395c273980e2b81a54dbddc4897e4a7", "patch": "@@ -75,6 +75,54 @@\n #define bf_UP    E_BFmode\n #define v4bf_UP  E_V4BFmode\n #define v8bf_UP  E_V8BFmode\n+#define v2x8qi_UP  E_V2x8QImode\n+#define v2x4hi_UP  E_V2x4HImode\n+#define v2x4hf_UP  E_V2x4HFmode\n+#define v2x4bf_UP  E_V2x4BFmode\n+#define v2x2si_UP  E_V2x2SImode\n+#define v2x2sf_UP  E_V2x2SFmode\n+#define v2x1di_UP  E_V2x1DImode\n+#define v2x1df_UP  E_V2x1DFmode\n+#define v2x16qi_UP E_V2x16QImode\n+#define v2x8hi_UP  E_V2x8HImode\n+#define v2x8hf_UP  E_V2x8HFmode\n+#define v2x8bf_UP  E_V2x8BFmode\n+#define v2x4si_UP  E_V2x4SImode\n+#define v2x4sf_UP  E_V2x4SFmode\n+#define v2x2di_UP  E_V2x2DImode\n+#define v2x2df_UP  E_V2x2DFmode\n+#define v3x8qi_UP  E_V3x8QImode\n+#define v3x4hi_UP  E_V3x4HImode\n+#define v3x4hf_UP  E_V3x4HFmode\n+#define v3x4bf_UP  E_V3x4BFmode\n+#define v3x2si_UP  E_V3x2SImode\n+#define v3x2sf_UP  E_V3x2SFmode\n+#define v3x1di_UP  E_V3x1DImode\n+#define v3x1df_UP  E_V3x1DFmode\n+#define v3x16qi_UP E_V3x16QImode\n+#define v3x8hi_UP  E_V3x8HImode\n+#define v3x8hf_UP  E_V3x8HFmode\n+#define v3x8bf_UP  E_V3x8BFmode\n+#define v3x4si_UP  E_V3x4SImode\n+#define v3x4sf_UP  E_V3x4SFmode\n+#define v3x2di_UP  E_V3x2DImode\n+#define v3x2df_UP  E_V3x2DFmode\n+#define v4x8qi_UP  E_V4x8QImode\n+#define v4x4hi_UP  E_V4x4HImode\n+#define v4x4hf_UP  E_V4x4HFmode\n+#define v4x4bf_UP  E_V4x4BFmode\n+#define v4x2si_UP  E_V4x2SImode\n+#define v4x2sf_UP  E_V4x2SFmode\n+#define v4x1di_UP  E_V4x1DImode\n+#define v4x1df_UP  E_V4x1DFmode\n+#define v4x16qi_UP E_V4x16QImode\n+#define v4x8hi_UP  E_V4x8HImode\n+#define v4x8hf_UP  E_V4x8HFmode\n+#define v4x8bf_UP  E_V4x8BFmode\n+#define v4x4si_UP  E_V4x4SImode\n+#define v4x4sf_UP  E_V4x4SFmode\n+#define v4x2di_UP  E_V4x2DImode\n+#define v4x2df_UP  E_V4x2DFmode\n #define UP(X) X##_UP\n \n #define SIMD_MAX_BUILTIN_ARGS 5\n@@ -228,7 +276,6 @@ aarch64_types_binop_pppu_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_poly, qualifier_poly, qualifier_poly, qualifier_unsigned };\n #define TYPES_TERNOP_PPPU (aarch64_types_binop_pppu_qualifiers)\n \n-\n static enum aarch64_type_qualifiers\n aarch64_types_quadop_lane_pair_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_none, qualifier_none,\n@@ -264,10 +311,6 @@ aarch64_types_quadopu_imm_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n       qualifier_unsigned, qualifier_immediate };\n #define TYPES_QUADOPUI (aarch64_types_quadopu_imm_qualifiers)\n \n-static enum aarch64_type_qualifiers\n-aarch64_types_binop_imm_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n-  = { qualifier_poly, qualifier_none, qualifier_immediate };\n-#define TYPES_GETREGP (aarch64_types_binop_imm_p_qualifiers)\n static enum aarch64_type_qualifiers\n aarch64_types_binop_imm_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_none, qualifier_immediate };\n@@ -291,10 +334,6 @@ aarch64_types_shift2_to_unsigned_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_unsigned, qualifier_unsigned, qualifier_none, qualifier_immediate };\n #define TYPES_SHIFT2IMM_UUSS (aarch64_types_shift2_to_unsigned_qualifiers)\n \n-static enum aarch64_type_qualifiers\n-aarch64_types_ternop_s_imm_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n-  = { qualifier_none, qualifier_none, qualifier_poly, qualifier_immediate};\n-#define TYPES_SETREGP (aarch64_types_ternop_s_imm_p_qualifiers)\n static enum aarch64_type_qualifiers\n aarch64_types_ternop_s_imm_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_none, qualifier_none, qualifier_immediate};\n@@ -330,11 +369,30 @@ aarch64_types_load1_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_const_pointer_map_mode };\n #define TYPES_LOAD1 (aarch64_types_load1_qualifiers)\n #define TYPES_LOADSTRUCT (aarch64_types_load1_qualifiers)\n+static enum aarch64_type_qualifiers\n+aarch64_types_load1_u_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_unsigned, qualifier_const_pointer_map_mode };\n+#define TYPES_LOADSTRUCT_U (aarch64_types_load1_u_qualifiers)\n+static enum aarch64_type_qualifiers\n+aarch64_types_load1_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_poly, qualifier_const_pointer_map_mode };\n+#define TYPES_LOADSTRUCT_P (aarch64_types_load1_p_qualifiers)\n+\n static enum aarch64_type_qualifiers\n aarch64_types_loadstruct_lane_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_const_pointer_map_mode,\n       qualifier_none, qualifier_struct_load_store_lane_index };\n #define TYPES_LOADSTRUCT_LANE (aarch64_types_loadstruct_lane_qualifiers)\n+static enum aarch64_type_qualifiers\n+aarch64_types_loadstruct_lane_u_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_unsigned, qualifier_const_pointer_map_mode,\n+      qualifier_unsigned, qualifier_struct_load_store_lane_index };\n+#define TYPES_LOADSTRUCT_LANE_U (aarch64_types_loadstruct_lane_u_qualifiers)\n+static enum aarch64_type_qualifiers\n+aarch64_types_loadstruct_lane_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_poly, qualifier_const_pointer_map_mode,\n+      qualifier_poly, qualifier_struct_load_store_lane_index };\n+#define TYPES_LOADSTRUCT_LANE_P (aarch64_types_loadstruct_lane_p_qualifiers)\n \n static enum aarch64_type_qualifiers\n aarch64_types_bsl_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n@@ -358,19 +416,35 @@ aarch64_types_bsl_u_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n    qualifier_map_mode | qualifier_pointer to build a pointer to the\n    element type of the vector.  */\n static enum aarch64_type_qualifiers\n-aarch64_types_store1_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n-  = { qualifier_void, qualifier_pointer_map_mode, qualifier_poly };\n-#define TYPES_STORE1P (aarch64_types_store1_p_qualifiers)\n-static enum aarch64_type_qualifiers\n aarch64_types_store1_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_void, qualifier_pointer_map_mode, qualifier_none };\n #define TYPES_STORE1 (aarch64_types_store1_qualifiers)\n #define TYPES_STORESTRUCT (aarch64_types_store1_qualifiers)\n+static enum aarch64_type_qualifiers\n+aarch64_types_store1_u_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_void, qualifier_pointer_map_mode, qualifier_unsigned };\n+#define TYPES_STORESTRUCT_U (aarch64_types_store1_u_qualifiers)\n+static enum aarch64_type_qualifiers\n+aarch64_types_store1_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_void, qualifier_pointer_map_mode, qualifier_poly };\n+#define TYPES_STORE1P (aarch64_types_store1_p_qualifiers)\n+#define TYPES_STORESTRUCT_P (aarch64_types_store1_p_qualifiers)\n+\n static enum aarch64_type_qualifiers\n aarch64_types_storestruct_lane_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_void, qualifier_pointer_map_mode,\n       qualifier_none, qualifier_struct_load_store_lane_index };\n #define TYPES_STORESTRUCT_LANE (aarch64_types_storestruct_lane_qualifiers)\n+static enum aarch64_type_qualifiers\n+aarch64_types_storestruct_lane_u_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_void, qualifier_pointer_map_mode,\n+      qualifier_unsigned, qualifier_struct_load_store_lane_index };\n+#define TYPES_STORESTRUCT_LANE_U (aarch64_types_storestruct_lane_u_qualifiers)\n+static enum aarch64_type_qualifiers\n+aarch64_types_storestruct_lane_p_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n+  = { qualifier_void, qualifier_pointer_map_mode,\n+      qualifier_poly, qualifier_struct_load_store_lane_index };\n+#define TYPES_STORESTRUCT_LANE_P (aarch64_types_storestruct_lane_p_qualifiers)\n \n #define CF0(N, X) CODE_FOR_aarch64_##N##X\n #define CF1(N, X) CODE_FOR_##N##X##1\n@@ -644,6 +718,8 @@ static GTY(()) struct aarch64_simd_type_info aarch64_simd_types [] = {\n };\n #undef ENTRY\n \n+static GTY(()) tree aarch64_simd_tuple_types[ARM_NEON_H_TYPES_LAST][3];\n+\n static GTY(()) tree aarch64_simd_intOI_type_node = NULL_TREE;\n static GTY(()) tree aarch64_simd_intCI_type_node = NULL_TREE;\n static GTY(()) tree aarch64_simd_intXI_type_node = NULL_TREE;\n@@ -764,9 +840,16 @@ aarch64_lookup_simd_builtin_type (machine_mode mode,\n     return aarch64_simd_builtin_std_type (mode, q);\n \n   for (i = 0; i < nelts; i++)\n-    if (aarch64_simd_types[i].mode == mode\n-\t&& aarch64_simd_types[i].q == q)\n-      return aarch64_simd_types[i].itype;\n+    {\n+      if (aarch64_simd_types[i].mode == mode\n+\t  && aarch64_simd_types[i].q == q)\n+\treturn aarch64_simd_types[i].itype;\n+      if (aarch64_simd_tuple_types[i][0] != NULL_TREE)\n+\tfor (int j = 0; j < 3; j++)\n+\t  if (TYPE_MODE (aarch64_simd_tuple_types[i][j]) == mode\n+\t      && aarch64_simd_types[i].q == q)\n+\t    return aarch64_simd_tuple_types[i][j];\n+    }\n \n   return NULL_TREE;\n }\n@@ -1173,7 +1256,17 @@ aarch64_init_simd_builtin_functions (bool called_from_pragma)\n \n       tree attrs = aarch64_get_attributes (d->flags, d->mode);\n \n-      fndecl = aarch64_general_add_builtin (namebuf, ftype, fcode, attrs);\n+      if (called_from_pragma)\n+\t{\n+\t  unsigned int raw_code\n+\t\t= (fcode << AARCH64_BUILTIN_SHIFT) | AARCH64_BUILTIN_GENERAL;\n+\t  fndecl = simulate_builtin_function_decl (input_location, namebuf,\n+\t\t\t\t\t\t   ftype, raw_code, NULL,\n+\t\t\t\t\t\t   attrs);\n+\t}\n+      else\n+\tfndecl = aarch64_general_add_builtin (namebuf, ftype, fcode, attrs);\n+\n       aarch64_builtin_decls[fcode] = fndecl;\n     }\n }\n@@ -1195,6 +1288,16 @@ register_tuple_type (unsigned int num_vectors, unsigned int type_index)\n \n   tree vector_type = type->itype;\n   tree array_type = build_array_type_nelts (vector_type, num_vectors);\n+  if (type->mode == DImode)\n+    {\n+      if (num_vectors == 2)\n+\tSET_TYPE_MODE (array_type, V2x1DImode);\n+      else if (num_vectors == 3)\n+\tSET_TYPE_MODE (array_type, V3x1DImode);\n+      else if (num_vectors == 4)\n+\tSET_TYPE_MODE (array_type, V4x1DImode);\n+    }\n+\n   unsigned int alignment\n \t= (known_eq (GET_MODE_SIZE (type->mode), 16) ? 128 : 64);\n   gcc_assert (TYPE_MODE_RAW (array_type) == TYPE_MODE (array_type)\n@@ -1209,6 +1312,13 @@ register_tuple_type (unsigned int num_vectors, unsigned int type_index)\n \t\t\t\t\t\t\t\t    1));\n   gcc_assert (TYPE_MODE_RAW (t) == TYPE_MODE (t)\n \t      && TYPE_ALIGN (t) == alignment);\n+\n+  if (num_vectors == 2)\n+    aarch64_simd_tuple_types[type_index][0] = t;\n+  else if (num_vectors == 3)\n+    aarch64_simd_tuple_types[type_index][1] = t;\n+  else if (num_vectors == 4)\n+    aarch64_simd_tuple_types[type_index][2] = t;\n }\n \n static bool"}, {"sha": "ac97d222789c6701d858c014736f8c211512a4d9", "filename": "gcc/config/aarch64/aarch64-modes.def", "status": "modified", "additions": 66, "deletions": 10, "changes": 76, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def?ref=66f206b85395c273980e2b81a54dbddc4897e4a7", "patch": "@@ -81,13 +81,69 @@ INT_MODE (OI, 32);\n INT_MODE (CI, 48);\n INT_MODE (XI, 64);\n \n+/* Define Advanced SIMD modes for structures of 2, 3 and 4 d-registers.  */\n+#define ADV_SIMD_D_REG_STRUCT_MODES(NVECS, VB, VH, VS, VD) \\\n+  VECTOR_MODES_WITH_PREFIX (V##NVECS##x, INT, 8, 3); \\\n+  VECTOR_MODES_WITH_PREFIX (V##NVECS##x, FLOAT, 8, 3); \\\n+  VECTOR_MODE_WITH_PREFIX (V##NVECS##x, FLOAT, DF, 1, 3); \\\n+  VECTOR_MODE_WITH_PREFIX (V##NVECS##x, INT, DI, 1, 3); \\\n+  \\\n+  ADJUST_NUNITS (VB##QI, NVECS * 8); \\\n+  ADJUST_NUNITS (VH##HI, NVECS * 4); \\\n+  ADJUST_NUNITS (VS##SI, NVECS * 2); \\\n+  ADJUST_NUNITS (VD##DI, NVECS); \\\n+  ADJUST_NUNITS (VH##BF, NVECS * 4); \\\n+  ADJUST_NUNITS (VH##HF, NVECS * 4); \\\n+  ADJUST_NUNITS (VS##SF, NVECS * 2); \\\n+  ADJUST_NUNITS (VD##DF, NVECS); \\\n+  \\\n+  ADJUST_ALIGNMENT (VB##QI, 8); \\\n+  ADJUST_ALIGNMENT (VH##HI, 8); \\\n+  ADJUST_ALIGNMENT (VS##SI, 8); \\\n+  ADJUST_ALIGNMENT (VD##DI, 8); \\\n+  ADJUST_ALIGNMENT (VH##BF, 8); \\\n+  ADJUST_ALIGNMENT (VH##HF, 8); \\\n+  ADJUST_ALIGNMENT (VS##SF, 8); \\\n+  ADJUST_ALIGNMENT (VD##DF, 8);\n+\n+ADV_SIMD_D_REG_STRUCT_MODES (2, V2x8, V2x4, V2x2, V2x1)\n+ADV_SIMD_D_REG_STRUCT_MODES (3, V3x8, V3x4, V3x2, V3x1)\n+ADV_SIMD_D_REG_STRUCT_MODES (4, V4x8, V4x4, V4x2, V4x1)\n+\n+/* Define Advanced SIMD modes for structures of 2, 3 and 4 q-registers.  */\n+#define ADV_SIMD_Q_REG_STRUCT_MODES(NVECS, VB, VH, VS, VD) \\\n+  VECTOR_MODES_WITH_PREFIX (V##NVECS##x, INT, 16, 3); \\\n+  VECTOR_MODES_WITH_PREFIX (V##NVECS##x, FLOAT, 16, 3); \\\n+  \\\n+  ADJUST_NUNITS (VB##QI, NVECS * 16); \\\n+  ADJUST_NUNITS (VH##HI, NVECS * 8); \\\n+  ADJUST_NUNITS (VS##SI, NVECS * 4); \\\n+  ADJUST_NUNITS (VD##DI, NVECS * 2); \\\n+  ADJUST_NUNITS (VH##BF, NVECS * 8); \\\n+  ADJUST_NUNITS (VH##HF, NVECS * 8); \\\n+  ADJUST_NUNITS (VS##SF, NVECS * 4); \\\n+  ADJUST_NUNITS (VD##DF, NVECS * 2); \\\n+  \\\n+  ADJUST_ALIGNMENT (VB##QI, 16); \\\n+  ADJUST_ALIGNMENT (VH##HI, 16); \\\n+  ADJUST_ALIGNMENT (VS##SI, 16); \\\n+  ADJUST_ALIGNMENT (VD##DI, 16); \\\n+  ADJUST_ALIGNMENT (VH##BF, 16); \\\n+  ADJUST_ALIGNMENT (VH##HF, 16); \\\n+  ADJUST_ALIGNMENT (VS##SF, 16); \\\n+  ADJUST_ALIGNMENT (VD##DF, 16);\n+\n+ADV_SIMD_Q_REG_STRUCT_MODES (2, V2x16, V2x8, V2x4, V2x2)\n+ADV_SIMD_Q_REG_STRUCT_MODES (3, V3x16, V3x8, V3x4, V3x2)\n+ADV_SIMD_Q_REG_STRUCT_MODES (4, V4x16, V4x8, V4x4, V4x2)\n+\n /* Define SVE modes for NVECS vectors.  VB, VH, VS and VD are the prefixes\n    for 8-bit, 16-bit, 32-bit and 64-bit elements respectively.  It isn't\n    strictly necessary to set the alignment here, since the default would\n    be clamped to BIGGEST_ALIGNMENT anyhow, but it seems clearer.  */\n #define SVE_MODES(NVECS, VB, VH, VS, VD) \\\n-  VECTOR_MODES_WITH_PREFIX (VNx, INT, 16 * NVECS, 0); \\\n-  VECTOR_MODES_WITH_PREFIX (VNx, FLOAT, 16 * NVECS, 0); \\\n+  VECTOR_MODES_WITH_PREFIX (VNx, INT, 16 * NVECS, NVECS == 1 ? 1 : 4); \\\n+  VECTOR_MODES_WITH_PREFIX (VNx, FLOAT, 16 * NVECS, NVECS == 1 ? 1 : 4); \\\n   \\\n   ADJUST_NUNITS (VB##QI, aarch64_sve_vg * NVECS * 8); \\\n   ADJUST_NUNITS (VH##HI, aarch64_sve_vg * NVECS * 4); \\\n@@ -123,14 +179,14 @@ SVE_MODES (4, VNx64, VNx32, VNx16, VNx8)\n    In memory they occupy contiguous locations, in the same way as fixed-length\n    vectors.  E.g. VNx8QImode is half the size of VNx16QImode.\n \n-   Passing 1 as the final argument ensures that the modes come after all\n-   other modes in the GET_MODE_WIDER chain, so that we never pick them\n-   in preference to a full vector mode.  */\n-VECTOR_MODES_WITH_PREFIX (VNx, INT, 2, 1);\n-VECTOR_MODES_WITH_PREFIX (VNx, INT, 4, 1);\n-VECTOR_MODES_WITH_PREFIX (VNx, INT, 8, 1);\n-VECTOR_MODES_WITH_PREFIX (VNx, FLOAT, 4, 1);\n-VECTOR_MODES_WITH_PREFIX (VNx, FLOAT, 8, 1);\n+   Passing 2 as the final argument ensures that the modes come after all\n+   other single-vector modes in the GET_MODE_WIDER chain, so that we never\n+   pick them in preference to a full vector mode.  */\n+VECTOR_MODES_WITH_PREFIX (VNx, INT, 2, 2);\n+VECTOR_MODES_WITH_PREFIX (VNx, INT, 4, 2);\n+VECTOR_MODES_WITH_PREFIX (VNx, INT, 8, 2);\n+VECTOR_MODES_WITH_PREFIX (VNx, FLOAT, 4, 2);\n+VECTOR_MODES_WITH_PREFIX (VNx, FLOAT, 8, 2);\n \n ADJUST_NUNITS (VNx2QI, aarch64_sve_vg);\n ADJUST_NUNITS (VNx2HI, aarch64_sve_vg);"}, {"sha": "6546e9149198ce29fcdff9c38318435ecd33a419", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 88, "deletions": 58, "changes": 146, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=66f206b85395c273980e2b81a54dbddc4897e4a7", "patch": "@@ -76,59 +76,86 @@\n   BUILTIN_VSDQ_I (BINOP_SSU, suqadd, 0, NONE)\n   BUILTIN_VSDQ_I (BINOP_UUS, usqadd, 0, NONE)\n \n-  /* Implemented by aarch64_get_dreg<VSTRUCT:mode><VDC:mode>.  */\n-  BUILTIN_VDC (GETREG, get_dregoi, 0, AUTO_FP)\n-  BUILTIN_VDC (GETREG, get_dregci, 0, AUTO_FP)\n-  BUILTIN_VDC (GETREG, get_dregxi, 0, AUTO_FP)\n-  VAR1 (GETREGP, get_dregoi, 0, AUTO_FP, di)\n-  VAR1 (GETREGP, get_dregci, 0, AUTO_FP, di)\n-  VAR1 (GETREGP, get_dregxi, 0, AUTO_FP, di)\n-  /* Implemented by aarch64_get_qreg<VSTRUCT:mode><VQ:mode>.  */\n-  BUILTIN_VQ (GETREG, get_qregoi, 0, AUTO_FP)\n-  BUILTIN_VQ (GETREG, get_qregci, 0, AUTO_FP)\n-  BUILTIN_VQ (GETREG, get_qregxi, 0, AUTO_FP)\n-  VAR1 (GETREGP, get_qregoi, 0, AUTO_FP, v2di)\n-  VAR1 (GETREGP, get_qregci, 0, AUTO_FP, v2di)\n-  VAR1 (GETREGP, get_qregxi, 0, AUTO_FP, v2di)\n-  /* Implemented by aarch64_set_qreg<VSTRUCT:mode><VQ:mode>.  */\n-  BUILTIN_VQ (SETREG, set_qregoi, 0, AUTO_FP)\n-  BUILTIN_VQ (SETREG, set_qregci, 0, AUTO_FP)\n-  BUILTIN_VQ (SETREG, set_qregxi, 0, AUTO_FP)\n-  VAR1 (SETREGP, set_qregoi, 0, AUTO_FP, v2di)\n-  VAR1 (SETREGP, set_qregci, 0, AUTO_FP, v2di)\n-  VAR1 (SETREGP, set_qregxi, 0, AUTO_FP, v2di)\n-  /* Implemented by aarch64_ld1x2<VQ:mode>. */\n-  BUILTIN_VQ (LOADSTRUCT, ld1x2, 0, LOAD)\n-  /* Implemented by aarch64_ld1x2<VDC:mode>. */\n-  BUILTIN_VDC (LOADSTRUCT, ld1x2, 0, LOAD)\n-  /* Implemented by aarch64_ld<VSTRUCT:nregs><VDC:mode>.  */\n-  BUILTIN_VDC (LOADSTRUCT, ld2, 0, LOAD)\n-  BUILTIN_VDC (LOADSTRUCT, ld3, 0, LOAD)\n-  BUILTIN_VDC (LOADSTRUCT, ld4, 0, LOAD)\n-  /* Implemented by aarch64_ld<VSTRUCT:nregs><VQ:mode>.  */\n-  BUILTIN_VQ (LOADSTRUCT, ld2, 0, LOAD)\n-  BUILTIN_VQ (LOADSTRUCT, ld3, 0, LOAD)\n-  BUILTIN_VQ (LOADSTRUCT, ld4, 0, LOAD)\n-  /* Implemented by aarch64_ld<VSTRUCT:nregs>r<VALLDIF:mode>.  */\n+  /* Implemented by aarch64_ld1x2<vstruct_elt>. */\n+  BUILTIN_VALLDIF (LOADSTRUCT, ld1x2, 0, LOAD)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_U, ld1x2, 0, LOAD)\n+  BUILTIN_VALLP (LOADSTRUCT_P, ld1x2, 0, LOAD)\n+  /* Implemented by aarch64_ld1x3<vstruct_elt>.  */\n+  BUILTIN_VALLDIF (LOADSTRUCT, ld1x3, 0, LOAD)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_U, ld1x3, 0, LOAD)\n+  BUILTIN_VALLP (LOADSTRUCT_P, ld1x3, 0, LOAD)\n+  /* Implemented by aarch64_ld1x4<vstruct_elt>.  */\n+  BUILTIN_VALLDIF (LOADSTRUCT, ld1x4, 0, LOAD)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_U, ld1x4, 0, LOAD)\n+  BUILTIN_VALLP (LOADSTRUCT_P, ld1x4, 0, LOAD)\n+\n+  /* Implemented by aarch64_st1x2<vstruct_elt>.  */\n+  BUILTIN_VALLDIF (STORESTRUCT, st1x2, 0, STORE)\n+  BUILTIN_VSDQ_I_DI (STORESTRUCT_U, st1x2, 0, STORE)\n+  BUILTIN_VALLP (STORESTRUCT_P, st1x2, 0, STORE)\n+  /* Implemented by aarch64_st1x3<vstruct_elt>.  */\n+  BUILTIN_VALLDIF (STORESTRUCT, st1x3, 0, STORE)\n+  BUILTIN_VSDQ_I_DI (STORESTRUCT_U, st1x3, 0, STORE)\n+  BUILTIN_VALLP (STORESTRUCT_P, st1x3, 0, STORE)\n+  /* Implemented by aarch64_st1x4<vstruct_elt>.  */\n+  BUILTIN_VALLDIF (STORESTRUCT, st1x4, 0, STORE)\n+  BUILTIN_VSDQ_I_DI (STORESTRUCT_U, st1x4, 0, STORE)\n+  BUILTIN_VALLP (STORESTRUCT_P, st1x4, 0, STORE)\n+\n+  /* Implemented by aarch64_ld<nregs><vstruct_elt>.  */\n+  BUILTIN_VALLDIF (LOADSTRUCT, ld2, 0, LOAD)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_U, ld2, 0, LOAD)\n+  BUILTIN_VALLP (LOADSTRUCT_P, ld2, 0, LOAD)\n+  BUILTIN_VALLDIF (LOADSTRUCT, ld3, 0, LOAD)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_U, ld3, 0, LOAD)\n+  BUILTIN_VALLP (LOADSTRUCT_P, ld3, 0, LOAD)\n+  BUILTIN_VALLDIF (LOADSTRUCT, ld4, 0, LOAD)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_U, ld4, 0, LOAD)\n+  BUILTIN_VALLP (LOADSTRUCT_P, ld4, 0, LOAD)\n+\n+  /* Implemented by aarch64_st<nregs><vstruct_elt>.  */\n+  BUILTIN_VALLDIF (STORESTRUCT, st2, 0, STORE)\n+  BUILTIN_VSDQ_I_DI (STORESTRUCT_U, st2, 0, STORE)\n+  BUILTIN_VALLP (STORESTRUCT_P, st2, 0, STORE)\n+  BUILTIN_VALLDIF (STORESTRUCT, st3, 0, STORE)\n+  BUILTIN_VSDQ_I_DI (STORESTRUCT_U, st3, 0, STORE)\n+  BUILTIN_VALLP (STORESTRUCT_P, st3, 0, STORE)\n+  BUILTIN_VALLDIF (STORESTRUCT, st4, 0, STORE)\n+  BUILTIN_VSDQ_I_DI (STORESTRUCT_U, st4, 0, STORE)\n+  BUILTIN_VALLP (STORESTRUCT_P, st4, 0, STORE)\n+\n+  /* Implemented by aarch64_ld<nregs>r<vstruct_elt>.  */\n   BUILTIN_VALLDIF (LOADSTRUCT, ld2r, 0, LOAD)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_U, ld2r, 0, LOAD)\n+  BUILTIN_VALLP (LOADSTRUCT_P, ld2r, 0, LOAD)\n   BUILTIN_VALLDIF (LOADSTRUCT, ld3r, 0, LOAD)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_U, ld3r, 0, LOAD)\n+  BUILTIN_VALLP (LOADSTRUCT_P, ld3r, 0, LOAD)\n   BUILTIN_VALLDIF (LOADSTRUCT, ld4r, 0, LOAD)\n-  /* Implemented by aarch64_ld<VSTRUCT:nregs>_lane<VQ:mode>.  */\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_U, ld4r, 0, LOAD)\n+  BUILTIN_VALLP (LOADSTRUCT_P, ld4r, 0, LOAD)\n+\n+  /* Implemented by aarch64_ld<nregs>_lane<vstruct_elt>.  */\n   BUILTIN_VALLDIF (LOADSTRUCT_LANE, ld2_lane, 0, ALL)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_LANE_U, ld2_lane, 0, ALL)\n+  BUILTIN_VALLP (LOADSTRUCT_LANE_P, ld2_lane, 0, ALL)\n   BUILTIN_VALLDIF (LOADSTRUCT_LANE, ld3_lane, 0, ALL)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_LANE_U, ld3_lane, 0, ALL)\n+  BUILTIN_VALLP (LOADSTRUCT_LANE_P, ld3_lane, 0, ALL)\n   BUILTIN_VALLDIF (LOADSTRUCT_LANE, ld4_lane, 0, ALL)\n-  /* Implemented by aarch64_st<VSTRUCT:nregs><VDC:mode>.  */\n-  BUILTIN_VDC (STORESTRUCT, st2, 0, STORE)\n-  BUILTIN_VDC (STORESTRUCT, st3, 0, STORE)\n-  BUILTIN_VDC (STORESTRUCT, st4, 0, STORE)\n-  /* Implemented by aarch64_st<VSTRUCT:nregs><VQ:mode>.  */\n-  BUILTIN_VQ (STORESTRUCT, st2, 0, STORE)\n-  BUILTIN_VQ (STORESTRUCT, st3, 0, STORE)\n-  BUILTIN_VQ (STORESTRUCT, st4, 0, STORE)\n+  BUILTIN_VSDQ_I_DI (LOADSTRUCT_LANE_U, ld4_lane, 0, ALL)\n+  BUILTIN_VALLP (LOADSTRUCT_LANE_P, ld4_lane, 0, ALL)\n \n+  /* Implemented by aarch64_st<nregs>_lane<vstruct_elt>.  */\n   BUILTIN_VALLDIF (STORESTRUCT_LANE, st2_lane, 0, ALL)\n+  BUILTIN_VSDQ_I_DI (STORESTRUCT_LANE_U, st2_lane, 0, ALL)\n+  BUILTIN_VALLP (STORESTRUCT_LANE_P, st2_lane, 0, ALL)\n   BUILTIN_VALLDIF (STORESTRUCT_LANE, st3_lane, 0, ALL)\n+  BUILTIN_VSDQ_I_DI (STORESTRUCT_LANE_U, st3_lane, 0, ALL)\n+  BUILTIN_VALLP (STORESTRUCT_LANE_P, st3_lane, 0, ALL)\n   BUILTIN_VALLDIF (STORESTRUCT_LANE, st4_lane, 0, ALL)\n+  BUILTIN_VSDQ_I_DI (STORESTRUCT_LANE_U, st4_lane, 0, ALL)\n+  BUILTIN_VALLP (STORESTRUCT_LANE_P, st4_lane, 0, ALL)\n \n   BUILTIN_VQW (BINOP, saddl2, 0, NONE)\n   BUILTIN_VQW (BINOP, uaddl2, 0, NONE)\n@@ -657,21 +684,6 @@\n   BUILTIN_VALL_F16 (STORE1, st1, 0, STORE)\n   VAR1 (STORE1P, st1, 0, STORE, v2di)\n \n-  /* Implemented by aarch64_ld1x3<VALLDIF:mode>.  */\n-  BUILTIN_VALLDIF (LOADSTRUCT, ld1x3, 0, LOAD)\n-\n-  /* Implemented by aarch64_ld1x4<VALLDIF:mode>.  */\n-  BUILTIN_VALLDIF (LOADSTRUCT, ld1x4, 0, LOAD)\n-\n-  /* Implemented by aarch64_st1x2<VALLDIF:mode>.  */\n-  BUILTIN_VALLDIF (STORESTRUCT, st1x2, 0, STORE)\n-\n-  /* Implemented by aarch64_st1x3<VALLDIF:mode>.  */\n-  BUILTIN_VALLDIF (STORESTRUCT, st1x3, 0, STORE)\n-\n-  /* Implemented by aarch64_st1x4<VALLDIF:mode>.  */\n-  BUILTIN_VALLDIF (STORESTRUCT, st1x4, 0, STORE)\n-\n   /* Implemented by fma<mode>4.  */\n   BUILTIN_VHSDF (TERNOP, fma, 4, FP)\n   VAR1 (TERNOP, fma, 4, FP, hf)\n@@ -726,12 +738,21 @@\n \n   /* Implemented by aarch64_qtbl2<mode>.  */\n   VAR2 (BINOP, qtbl2, 0, NONE, v8qi, v16qi)\n+  VAR2 (BINOPU, qtbl2, 0, NONE, v8qi, v16qi)\n+  VAR2 (BINOP_PPU, qtbl2, 0, NONE, v8qi, v16qi)\n+  VAR2 (BINOP_SSU, qtbl2, 0, NONE, v8qi, v16qi)\n \n   /* Implemented by aarch64_qtbl3<mode>.  */\n   VAR2 (BINOP, qtbl3, 0, NONE, v8qi, v16qi)\n+  VAR2 (BINOPU, qtbl3, 0, NONE, v8qi, v16qi)\n+  VAR2 (BINOP_PPU, qtbl3, 0, NONE, v8qi, v16qi)\n+  VAR2 (BINOP_SSU, qtbl3, 0, NONE, v8qi, v16qi)\n \n   /* Implemented by aarch64_qtbl4<mode>.  */\n   VAR2 (BINOP, qtbl4, 0, NONE, v8qi, v16qi)\n+  VAR2 (BINOPU, qtbl4, 0, NONE, v8qi, v16qi)\n+  VAR2 (BINOP_PPU, qtbl4, 0, NONE, v8qi, v16qi)\n+  VAR2 (BINOP_SSU, qtbl4, 0, NONE, v8qi, v16qi)\n \n   /* Implemented by aarch64_qtbx1<mode>.  */\n   VAR2 (TERNOP, qtbx1, 0, NONE, v8qi, v16qi)\n@@ -741,12 +762,21 @@\n \n   /* Implemented by aarch64_qtbx2<mode>.  */\n   VAR2 (TERNOP, qtbx2, 0, NONE, v8qi, v16qi)\n+  VAR2 (TERNOPU, qtbx2, 0, NONE, v8qi, v16qi)\n+  VAR2 (TERNOP_PPPU, qtbx2, 0, NONE, v8qi, v16qi)\n+  VAR2 (TERNOP_SSSU, qtbx2, 0, NONE, v8qi, v16qi)\n \n   /* Implemented by aarch64_qtbx3<mode>.  */\n   VAR2 (TERNOP, qtbx3, 0, NONE, v8qi, v16qi)\n+  VAR2 (TERNOPU, qtbx3, 0, NONE, v8qi, v16qi)\n+  VAR2 (TERNOP_PPPU, qtbx3, 0, NONE, v8qi, v16qi)\n+  VAR2 (TERNOP_SSSU, qtbx3, 0, NONE, v8qi, v16qi)\n \n   /* Implemented by aarch64_qtbx4<mode>.  */\n   VAR2 (TERNOP, qtbx4, 0, NONE, v8qi, v16qi)\n+  VAR2 (TERNOPU, qtbx4, 0, NONE, v8qi, v16qi)\n+  VAR2 (TERNOP_PPPU, qtbx4, 0, NONE, v8qi, v16qi)\n+  VAR2 (TERNOP_SSSU, qtbx4, 0, NONE, v8qi, v16qi)\n \n   /* Builtins for ARMv8.1-A Adv.SIMD instructions.  */\n "}, {"sha": "bff76e4b6e97db2613ab0ce1d721bf1828f0671b", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 470, "deletions": 418, "changes": 888, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=66f206b85395c273980e2b81a54dbddc4897e4a7", "patch": "@@ -6768,304 +6768,310 @@\n \n ;; Patterns for vector struct loads and stores.\n \n-(define_insn \"aarch64_simd_ld2<mode>\"\n-  [(set (match_operand:OI 0 \"register_operand\" \"=w\")\n-\t(unspec:OI [(match_operand:OI 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD2))]\n+(define_insn \"aarch64_simd_ld2<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_2Q 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_2Q [\n+\t  (match_operand:VSTRUCT_2Q 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t  UNSPEC_LD2))]\n   \"TARGET_SIMD\"\n   \"ld2\\\\t{%S0.<Vtype> - %T0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load2_2reg<q>\")]\n )\n \n-(define_insn \"aarch64_simd_ld2r<mode>\"\n-  [(set (match_operand:OI 0 \"register_operand\" \"=w\")\n-       (unspec:OI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-                   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY) ]\n-                  UNSPEC_LD2_DUP))]\n+(define_insn \"aarch64_simd_ld2r<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_2QD 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_2QD [\n+\t  (match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+          UNSPEC_LD2_DUP))]\n   \"TARGET_SIMD\"\n   \"ld2r\\\\t{%S0.<Vtype> - %T0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load2_all_lanes<q>\")]\n )\n \n-(define_insn \"aarch64_vec_load_lanesoi_lane<mode>\"\n-  [(set (match_operand:OI 0 \"register_operand\" \"=w\")\n-\t(unspec:OI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (match_operand:OI 2 \"register_operand\" \"0\")\n-\t\t    (match_operand:SI 3 \"immediate_operand\" \"i\")\n-\t\t    (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY) ]\n-\t\t   UNSPEC_LD2_LANE))]\n+(define_insn \"aarch64_vec_load_lanes<mode>_lane<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_2QD 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_2QD [\n+\t\t(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n+\t\t(match_operand:VSTRUCT_2QD 2 \"register_operand\" \"0\")\n+\t\t(match_operand:SI 3 \"immediate_operand\" \"i\")]\n+\t\tUNSPEC_LD2_LANE))]\n   \"TARGET_SIMD\"\n   {\n-    operands[3] = aarch64_endian_lane_rtx (<MODE>mode, INTVAL (operands[3]));\n+    operands[3] = aarch64_endian_lane_rtx (<VSTRUCT_ELT>mode,\n+\t\t\t\t\t   INTVAL (operands[3]));\n     return \"ld2\\\\t{%S0.<Vetype> - %T0.<Vetype>}[%3], %1\";\n   }\n   [(set_attr \"type\" \"neon_load2_one_lane\")]\n )\n \n-(define_expand \"vec_load_lanesoi<mode>\"\n-  [(set (match_operand:OI 0 \"register_operand\")\n-\t(unspec:OI [(match_operand:OI 1 \"aarch64_simd_struct_operand\")\n-\t\t    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD2))]\n+(define_expand \"vec_load_lanes<mode><vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_2Q 0 \"register_operand\")\n+\t(unspec:VSTRUCT_2Q [\n+\t\t(match_operand:VSTRUCT_2Q 1 \"aarch64_simd_struct_operand\")]\n+\t\tUNSPEC_LD2))]\n   \"TARGET_SIMD\"\n {\n   if (BYTES_BIG_ENDIAN)\n     {\n-      rtx tmp = gen_reg_rtx (OImode);\n-      rtx mask = aarch64_reverse_mask (<MODE>mode, <nunits>);\n-      emit_insn (gen_aarch64_simd_ld2<mode> (tmp, operands[1]));\n-      emit_insn (gen_aarch64_rev_reglistoi (operands[0], tmp, mask));\n+      rtx tmp = gen_reg_rtx (<MODE>mode);\n+      rtx mask = aarch64_reverse_mask (<VSTRUCT_ELT>mode,\n+\t\t\tGET_MODE_NUNITS (<MODE>mode).to_constant () / <nregs>);\n+      emit_insn (gen_aarch64_simd_ld2<vstruct_elt> (tmp, operands[1]));\n+      emit_insn (gen_aarch64_rev_reglist<mode> (operands[0], tmp, mask));\n     }\n   else\n-    emit_insn (gen_aarch64_simd_ld2<mode> (operands[0], operands[1]));\n+    emit_insn (gen_aarch64_simd_ld2<vstruct_elt> (operands[0], operands[1]));\n   DONE;\n })\n \n-(define_insn \"aarch64_simd_st2<mode>\"\n-  [(set (match_operand:OI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:OI [(match_operand:OI 1 \"register_operand\" \"w\")\n-                    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-                   UNSPEC_ST2))]\n+(define_insn \"aarch64_simd_st2<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_2Q 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_2Q [\n+\t\t(match_operand:VSTRUCT_2Q 1 \"register_operand\" \"w\")]\n+                UNSPEC_ST2))]\n   \"TARGET_SIMD\"\n   \"st2\\\\t{%S1.<Vtype> - %T1.<Vtype>}, %0\"\n   [(set_attr \"type\" \"neon_store2_2reg<q>\")]\n )\n \n ;; RTL uses GCC vector extension indices, so flip only for assembly.\n-(define_insn \"aarch64_vec_store_lanesoi_lane<mode>\"\n+(define_insn \"aarch64_vec_store_lanes<mode>_lane<vstruct_elt>\"\n   [(set (match_operand:BLK 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:BLK [(match_operand:OI 1 \"register_operand\" \"w\")\n-\t\t    (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)\n-\t\t    (match_operand:SI 2 \"immediate_operand\" \"i\")]\n-\t\t   UNSPEC_ST2_LANE))]\n+\t(unspec:BLK [(match_operand:VSTRUCT_2QD 1 \"register_operand\" \"w\")\n+\t\t     (match_operand:SI 2 \"immediate_operand\" \"i\")]\n+\t\t     UNSPEC_ST2_LANE))]\n   \"TARGET_SIMD\"\n   {\n-    operands[2] = aarch64_endian_lane_rtx (<MODE>mode, INTVAL (operands[2]));\n+    operands[2] = aarch64_endian_lane_rtx (<VSTRUCT_ELT>mode,\n+\t\t\t\t\t   INTVAL (operands[2]));\n     return \"st2\\\\t{%S1.<Vetype> - %T1.<Vetype>}[%2], %0\";\n   }\n   [(set_attr \"type\" \"neon_store2_one_lane<q>\")]\n )\n \n-(define_expand \"vec_store_lanesoi<mode>\"\n-  [(set (match_operand:OI 0 \"aarch64_simd_struct_operand\")\n-\t(unspec:OI [(match_operand:OI 1 \"register_operand\")\n-                    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+(define_expand \"vec_store_lanes<mode><vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_2Q 0 \"aarch64_simd_struct_operand\")\n+\t(unspec:VSTRUCT_2Q [(match_operand:VSTRUCT_2Q 1 \"register_operand\")]\n                    UNSPEC_ST2))]\n   \"TARGET_SIMD\"\n {\n   if (BYTES_BIG_ENDIAN)\n     {\n-      rtx tmp = gen_reg_rtx (OImode);\n-      rtx mask = aarch64_reverse_mask (<MODE>mode, <nunits>);\n-      emit_insn (gen_aarch64_rev_reglistoi (tmp, operands[1], mask));\n-      emit_insn (gen_aarch64_simd_st2<mode> (operands[0], tmp));\n+      rtx tmp = gen_reg_rtx (<MODE>mode);\n+      rtx mask = aarch64_reverse_mask (<VSTRUCT_ELT>mode,\n+\t\t\tGET_MODE_NUNITS (<MODE>mode).to_constant () / <nregs>);\n+      emit_insn (gen_aarch64_rev_reglist<mode> (tmp, operands[1], mask));\n+      emit_insn (gen_aarch64_simd_st2<vstruct_elt> (operands[0], tmp));\n     }\n   else\n-    emit_insn (gen_aarch64_simd_st2<mode> (operands[0], operands[1]));\n+    emit_insn (gen_aarch64_simd_st2<vstruct_elt> (operands[0], operands[1]));\n   DONE;\n })\n \n-(define_insn \"aarch64_simd_ld3<mode>\"\n-  [(set (match_operand:CI 0 \"register_operand\" \"=w\")\n-\t(unspec:CI [(match_operand:CI 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD3))]\n+(define_insn \"aarch64_simd_ld3<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_3Q 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_3Q [\n+\t  (match_operand:VSTRUCT_3Q 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t  UNSPEC_LD3))]\n   \"TARGET_SIMD\"\n   \"ld3\\\\t{%S0.<Vtype> - %U0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load3_3reg<q>\")]\n )\n \n-(define_insn \"aarch64_simd_ld3r<mode>\"\n-  [(set (match_operand:CI 0 \"register_operand\" \"=w\")\n-       (unspec:CI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-                   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY) ]\n-                  UNSPEC_LD3_DUP))]\n+(define_insn \"aarch64_simd_ld3r<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_3QD 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_3QD [\n+\t  (match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+          UNSPEC_LD3_DUP))]\n   \"TARGET_SIMD\"\n   \"ld3r\\\\t{%S0.<Vtype> - %U0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load3_all_lanes<q>\")]\n )\n \n-(define_insn \"aarch64_vec_load_lanesci_lane<mode>\"\n-  [(set (match_operand:CI 0 \"register_operand\" \"=w\")\n-\t(unspec:CI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (match_operand:CI 2 \"register_operand\" \"0\")\n-\t\t    (match_operand:SI 3 \"immediate_operand\" \"i\")\n-\t\t    (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD3_LANE))]\n+(define_insn \"aarch64_vec_load_lanes<mode>_lane<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_3QD 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_3QD [\n+\t\t(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n+\t\t(match_operand:VSTRUCT_3QD 2 \"register_operand\" \"0\")\n+\t\t(match_operand:SI 3 \"immediate_operand\" \"i\")]\n+\t\tUNSPEC_LD3_LANE))]\n   \"TARGET_SIMD\"\n {\n-    operands[3] = aarch64_endian_lane_rtx (<MODE>mode, INTVAL (operands[3]));\n+    operands[3] = aarch64_endian_lane_rtx (<VSTRUCT_ELT>mode,\n+\t\t\t\t\t   INTVAL (operands[3]));\n     return \"ld3\\\\t{%S0.<Vetype> - %U0.<Vetype>}[%3], %1\";\n }\n   [(set_attr \"type\" \"neon_load3_one_lane\")]\n )\n \n-(define_expand \"vec_load_lanesci<mode>\"\n-  [(set (match_operand:CI 0 \"register_operand\")\n-\t(unspec:CI [(match_operand:CI 1 \"aarch64_simd_struct_operand\")\n-\t\t    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD3))]\n+(define_expand \"vec_load_lanes<mode><vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_3Q 0 \"register_operand\")\n+\t(unspec:VSTRUCT_3Q [\n+\t\t(match_operand:VSTRUCT_3Q 1 \"aarch64_simd_struct_operand\")]\n+\t\tUNSPEC_LD3))]\n   \"TARGET_SIMD\"\n {\n   if (BYTES_BIG_ENDIAN)\n     {\n-      rtx tmp = gen_reg_rtx (CImode);\n-      rtx mask = aarch64_reverse_mask (<MODE>mode, <nunits>);\n-      emit_insn (gen_aarch64_simd_ld3<mode> (tmp, operands[1]));\n-      emit_insn (gen_aarch64_rev_reglistci (operands[0], tmp, mask));\n+      rtx tmp = gen_reg_rtx (<MODE>mode);\n+      rtx mask = aarch64_reverse_mask (<VSTRUCT_ELT>mode,\n+\t\t\tGET_MODE_NUNITS (<MODE>mode).to_constant () / <nregs>);\n+      emit_insn (gen_aarch64_simd_ld3<vstruct_elt> (tmp, operands[1]));\n+      emit_insn (gen_aarch64_rev_reglist<mode> (operands[0], tmp, mask));\n     }\n   else\n-    emit_insn (gen_aarch64_simd_ld3<mode> (operands[0], operands[1]));\n+    emit_insn (gen_aarch64_simd_ld3<vstruct_elt> (operands[0], operands[1]));\n   DONE;\n })\n \n-(define_insn \"aarch64_simd_st3<mode>\"\n-  [(set (match_operand:CI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:CI [(match_operand:CI 1 \"register_operand\" \"w\")\n-                    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+(define_insn \"aarch64_simd_st3<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_3Q 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_3Q [(match_operand:VSTRUCT_3Q 1 \"register_operand\" \"w\")]\n                    UNSPEC_ST3))]\n   \"TARGET_SIMD\"\n   \"st3\\\\t{%S1.<Vtype> - %U1.<Vtype>}, %0\"\n   [(set_attr \"type\" \"neon_store3_3reg<q>\")]\n )\n \n ;; RTL uses GCC vector extension indices, so flip only for assembly.\n-(define_insn \"aarch64_vec_store_lanesci_lane<mode>\"\n+(define_insn \"aarch64_vec_store_lanes<mode>_lane<vstruct_elt>\"\n   [(set (match_operand:BLK 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:BLK [(match_operand:CI 1 \"register_operand\" \"w\")\n-\t\t     (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)\n+\t(unspec:BLK [(match_operand:VSTRUCT_3QD 1 \"register_operand\" \"w\")\n \t\t     (match_operand:SI 2 \"immediate_operand\" \"i\")]\n-\t\t    UNSPEC_ST3_LANE))]\n+\t\t     UNSPEC_ST3_LANE))]\n   \"TARGET_SIMD\"\n   {\n-    operands[2] = aarch64_endian_lane_rtx (<MODE>mode, INTVAL (operands[2]));\n+    operands[2] = aarch64_endian_lane_rtx (<VSTRUCT_ELT>mode,\n+\t\t\t\t\t   INTVAL (operands[2]));\n     return \"st3\\\\t{%S1.<Vetype> - %U1.<Vetype>}[%2], %0\";\n   }\n   [(set_attr \"type\" \"neon_store3_one_lane<q>\")]\n )\n \n-(define_expand \"vec_store_lanesci<mode>\"\n-  [(set (match_operand:CI 0 \"aarch64_simd_struct_operand\")\n-\t(unspec:CI [(match_operand:CI 1 \"register_operand\")\n-                    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-                   UNSPEC_ST3))]\n+(define_expand \"vec_store_lanes<mode><vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_3Q 0 \"aarch64_simd_struct_operand\")\n+\t(unspec:VSTRUCT_3Q [\n+\t\t(match_operand:VSTRUCT_3Q 1 \"register_operand\")]\n+                UNSPEC_ST3))]\n   \"TARGET_SIMD\"\n {\n   if (BYTES_BIG_ENDIAN)\n     {\n-      rtx tmp = gen_reg_rtx (CImode);\n-      rtx mask = aarch64_reverse_mask (<MODE>mode, <nunits>);\n-      emit_insn (gen_aarch64_rev_reglistci (tmp, operands[1], mask));\n-      emit_insn (gen_aarch64_simd_st3<mode> (operands[0], tmp));\n+      rtx tmp = gen_reg_rtx (<MODE>mode);\n+      rtx mask = aarch64_reverse_mask (<VSTRUCT_ELT>mode,\n+\t\t\tGET_MODE_NUNITS (<MODE>mode).to_constant () / <nregs>);\n+      emit_insn (gen_aarch64_rev_reglist<mode> (tmp, operands[1], mask));\n+      emit_insn (gen_aarch64_simd_st3<vstruct_elt> (operands[0], tmp));\n     }\n   else\n-    emit_insn (gen_aarch64_simd_st3<mode> (operands[0], operands[1]));\n+    emit_insn (gen_aarch64_simd_st3<vstruct_elt> (operands[0], operands[1]));\n   DONE;\n })\n \n-(define_insn \"aarch64_simd_ld4<mode>\"\n-  [(set (match_operand:XI 0 \"register_operand\" \"=w\")\n-\t(unspec:XI [(match_operand:XI 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD4))]\n+(define_insn \"aarch64_simd_ld4<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_4Q 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_4Q [\n+\t  (match_operand:VSTRUCT_4Q 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t  UNSPEC_LD4))]\n   \"TARGET_SIMD\"\n   \"ld4\\\\t{%S0.<Vtype> - %V0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load4_4reg<q>\")]\n )\n \n-(define_insn \"aarch64_simd_ld4r<mode>\"\n-  [(set (match_operand:XI 0 \"register_operand\" \"=w\")\n-       (unspec:XI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-                   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY) ]\n-                  UNSPEC_LD4_DUP))]\n+(define_insn \"aarch64_simd_ld4r<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_4QD 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_4QD [\n+\t  (match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+          UNSPEC_LD4_DUP))]\n   \"TARGET_SIMD\"\n   \"ld4r\\\\t{%S0.<Vtype> - %V0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load4_all_lanes<q>\")]\n )\n \n-(define_insn \"aarch64_vec_load_lanesxi_lane<mode>\"\n-  [(set (match_operand:XI 0 \"register_operand\" \"=w\")\n-\t(unspec:XI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (match_operand:XI 2 \"register_operand\" \"0\")\n-\t\t    (match_operand:SI 3 \"immediate_operand\" \"i\")\n-\t\t    (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD4_LANE))]\n+(define_insn \"aarch64_vec_load_lanes<mode>_lane<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_4QD 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_4QD [\n+\t\t(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n+\t\t(match_operand:VSTRUCT_4QD 2 \"register_operand\" \"0\")\n+\t\t(match_operand:SI 3 \"immediate_operand\" \"i\")]\n+\t\tUNSPEC_LD4_LANE))]\n   \"TARGET_SIMD\"\n {\n-    operands[3] = aarch64_endian_lane_rtx (<MODE>mode, INTVAL (operands[3]));\n+    operands[3] = aarch64_endian_lane_rtx (<VSTRUCT_ELT>mode,\n+\t\t\t\t\t   INTVAL (operands[3]));\n     return \"ld4\\\\t{%S0.<Vetype> - %V0.<Vetype>}[%3], %1\";\n }\n   [(set_attr \"type\" \"neon_load4_one_lane\")]\n )\n \n-(define_expand \"vec_load_lanesxi<mode>\"\n-  [(set (match_operand:XI 0 \"register_operand\")\n-\t(unspec:XI [(match_operand:XI 1 \"aarch64_simd_struct_operand\")\n-\t\t    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD4))]\n+(define_expand \"vec_load_lanes<mode><vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_4Q 0 \"register_operand\")\n+\t(unspec:VSTRUCT_4Q [\n+\t\t(match_operand:VSTRUCT_4Q 1 \"aarch64_simd_struct_operand\")]\n+\t\tUNSPEC_LD4))]\n   \"TARGET_SIMD\"\n {\n   if (BYTES_BIG_ENDIAN)\n     {\n-      rtx tmp = gen_reg_rtx (XImode);\n-      rtx mask = aarch64_reverse_mask (<MODE>mode, <nunits>);\n-      emit_insn (gen_aarch64_simd_ld4<mode> (tmp, operands[1]));\n-      emit_insn (gen_aarch64_rev_reglistxi (operands[0], tmp, mask));\n+      rtx tmp = gen_reg_rtx (<MODE>mode);\n+      rtx mask = aarch64_reverse_mask (<VSTRUCT_ELT>mode,\n+\t\t\tGET_MODE_NUNITS (<MODE>mode).to_constant () / <nregs>);\n+      emit_insn (gen_aarch64_simd_ld4<vstruct_elt> (tmp, operands[1]));\n+      emit_insn (gen_aarch64_rev_reglist<mode> (operands[0], tmp, mask));\n     }\n   else\n-    emit_insn (gen_aarch64_simd_ld4<mode> (operands[0], operands[1]));\n+    emit_insn (gen_aarch64_simd_ld4<vstruct_elt> (operands[0], operands[1]));\n   DONE;\n })\n \n-(define_insn \"aarch64_simd_st4<mode>\"\n-  [(set (match_operand:XI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:XI [(match_operand:XI 1 \"register_operand\" \"w\")\n-                    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-                   UNSPEC_ST4))]\n+(define_insn \"aarch64_simd_st4<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_4Q 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_4Q [\n+\t\t(match_operand:VSTRUCT_4Q 1 \"register_operand\" \"w\")]\n+                UNSPEC_ST4))]\n   \"TARGET_SIMD\"\n   \"st4\\\\t{%S1.<Vtype> - %V1.<Vtype>}, %0\"\n   [(set_attr \"type\" \"neon_store4_4reg<q>\")]\n )\n \n ;; RTL uses GCC vector extension indices, so flip only for assembly.\n-(define_insn \"aarch64_vec_store_lanesxi_lane<mode>\"\n+(define_insn \"aarch64_vec_store_lanes<mode>_lane<vstruct_elt>\"\n   [(set (match_operand:BLK 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:BLK [(match_operand:XI 1 \"register_operand\" \"w\")\n-\t\t     (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)\n+\t(unspec:BLK [(match_operand:VSTRUCT_4QD 1 \"register_operand\" \"w\")\n \t\t     (match_operand:SI 2 \"immediate_operand\" \"i\")]\n-\t\t    UNSPEC_ST4_LANE))]\n+\t\t     UNSPEC_ST4_LANE))]\n   \"TARGET_SIMD\"\n   {\n-    operands[2] = aarch64_endian_lane_rtx (<MODE>mode, INTVAL (operands[2]));\n+    operands[2] = aarch64_endian_lane_rtx (<VSTRUCT_ELT>mode,\n+\t\t\t\t\t   INTVAL (operands[2]));\n     return \"st4\\\\t{%S1.<Vetype> - %V1.<Vetype>}[%2], %0\";\n   }\n   [(set_attr \"type\" \"neon_store4_one_lane<q>\")]\n )\n \n-(define_expand \"vec_store_lanesxi<mode>\"\n-  [(set (match_operand:XI 0 \"aarch64_simd_struct_operand\")\n-\t(unspec:XI [(match_operand:XI 1 \"register_operand\")\n-                    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+(define_expand \"vec_store_lanes<mode><vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_4Q 0 \"aarch64_simd_struct_operand\")\n+\t(unspec:VSTRUCT_4Q [(match_operand:VSTRUCT_4Q 1 \"register_operand\")]\n                    UNSPEC_ST4))]\n   \"TARGET_SIMD\"\n {\n   if (BYTES_BIG_ENDIAN)\n     {\n-      rtx tmp = gen_reg_rtx (XImode);\n-      rtx mask = aarch64_reverse_mask (<MODE>mode, <nunits>);\n-      emit_insn (gen_aarch64_rev_reglistxi (tmp, operands[1], mask));\n-      emit_insn (gen_aarch64_simd_st4<mode> (operands[0], tmp));\n+      rtx tmp = gen_reg_rtx (<MODE>mode);\n+      rtx mask = aarch64_reverse_mask (<VSTRUCT_ELT>mode,\n+\t\t\tGET_MODE_NUNITS (<MODE>mode).to_constant () / <nregs>);\n+      emit_insn (gen_aarch64_rev_reglist<mode> (tmp, operands[1], mask));\n+      emit_insn (gen_aarch64_simd_st4<vstruct_elt> (operands[0], tmp));\n     }\n   else\n-    emit_insn (gen_aarch64_simd_st4<mode> (operands[0], operands[1]));\n+    emit_insn (gen_aarch64_simd_st4<vstruct_elt> (operands[0], operands[1]));\n   DONE;\n })\n \n (define_insn_and_split \"aarch64_rev_reglist<mode>\"\n-[(set (match_operand:VSTRUCT 0 \"register_operand\" \"=&w\")\n-\t(unspec:VSTRUCT\n-\t           [(match_operand:VSTRUCT 1 \"register_operand\" \"w\")\n+[(set (match_operand:VSTRUCT_QD 0 \"register_operand\" \"=&w\")\n+\t(unspec:VSTRUCT_QD\n+\t           [(match_operand:VSTRUCT_QD 1 \"register_operand\" \"w\")\n \t\t    (match_operand:V16QI 2 \"register_operand\" \"w\")]\n                    UNSPEC_REV_REGLIST))]\n   \"TARGET_SIMD\"\n@@ -7074,7 +7080,7 @@\n   [(const_int 0)]\n {\n   int i;\n-  int nregs = GET_MODE_SIZE (<MODE>mode) / UNITS_PER_VREG;\n+  int nregs = GET_MODE_SIZE (<MODE>mode).to_constant () / UNITS_PER_VREG;\n   for (i = 0; i < nregs; i++)\n     {\n       rtx op0 = gen_rtx_REG (V16QImode, REGNO (operands[0]) + i);\n@@ -7089,6 +7095,18 @@\n \n ;; Reload patterns for AdvSIMD register list operands.\n \n+(define_expand \"mov<mode>\"\n+  [(set (match_operand:VSTRUCT_QD 0 \"nonimmediate_operand\")\n+\t(match_operand:VSTRUCT_QD 1 \"general_operand\"))]\n+  \"TARGET_SIMD\"\n+{\n+  if (can_create_pseudo_p ())\n+    {\n+      if (GET_CODE (operands[0]) != REG)\n+\toperands[1] = force_reg (<MODE>mode, operands[1]);\n+    }\n+})\n+\n (define_expand \"mov<mode>\"\n   [(set (match_operand:VSTRUCT 0 \"nonimmediate_operand\")\n \t(match_operand:VSTRUCT 1 \"general_operand\"))]\n@@ -7101,114 +7119,121 @@\n     }\n })\n \n-\n-(define_expand \"aarch64_ld1x3<VALLDIF:mode>\"\n-  [(match_operand:CI 0 \"register_operand\")\n-   (match_operand:DI 1 \"register_operand\")\n-   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+(define_expand \"aarch64_ld1x3<vstruct_elt>\"\n+  [(match_operand:VSTRUCT_3QD 0 \"register_operand\")\n+   (match_operand:DI 1 \"register_operand\")]\n   \"TARGET_SIMD\"\n {\n-  rtx mem = gen_rtx_MEM (CImode, operands[1]);\n-  emit_insn (gen_aarch64_ld1_x3_<VALLDIF:mode> (operands[0], mem));\n+  rtx mem = gen_rtx_MEM (<MODE>mode, operands[1]);\n+  emit_insn (gen_aarch64_ld1_x3_<vstruct_elt> (operands[0], mem));\n   DONE;\n })\n \n-(define_insn \"aarch64_ld1_x3_<mode>\"\n-  [(set (match_operand:CI 0 \"register_operand\" \"=w\")\n-        (unspec:CI\n-\t  [(match_operand:CI 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t   (unspec:VALLDIF [(const_int 3)] UNSPEC_VSTRUCTDUMMY)] UNSPEC_LD1))]\n+(define_insn \"aarch64_ld1_x3_<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_3QD 0 \"register_operand\" \"=w\")\n+        (unspec:VSTRUCT_3QD\n+\t  [(match_operand:VSTRUCT_3QD 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t  UNSPEC_LD1))]\n   \"TARGET_SIMD\"\n   \"ld1\\\\t{%S0.<Vtype> - %U0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load1_3reg<q>\")]\n )\n \n-(define_expand \"aarch64_ld1x4<VALLDIF:mode>\"\n-  [(match_operand:XI 0 \"register_operand\" \"=w\")\n-   (match_operand:DI 1 \"register_operand\" \"r\")\n-   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+(define_expand \"aarch64_ld1x4<vstruct_elt>\"\n+  [(match_operand:VSTRUCT_4QD 0 \"register_operand\" \"=w\")\n+   (match_operand:DI 1 \"register_operand\" \"r\")]\n   \"TARGET_SIMD\"\n {\n-  rtx mem = gen_rtx_MEM (XImode, operands[1]);\n-  emit_insn (gen_aarch64_ld1_x4_<VALLDIF:mode> (operands[0], mem));\n+  rtx mem = gen_rtx_MEM (<MODE>mode, operands[1]);\n+  emit_insn (gen_aarch64_ld1_x4_<vstruct_elt> (operands[0], mem));\n   DONE;\n })\n \n-(define_insn \"aarch64_ld1_x4_<mode>\"\n-  [(set (match_operand:XI 0 \"register_operand\" \"=w\")\n-\t(unspec:XI\n-\t  [(match_operand:XI 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t   (unspec:VALLDIF [(const_int 4)] UNSPEC_VSTRUCTDUMMY)]\n+(define_insn \"aarch64_ld1_x4_<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_4QD 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_4QD\n+\t  [(match_operand:VSTRUCT_4QD 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n \tUNSPEC_LD1))]\n   \"TARGET_SIMD\"\n   \"ld1\\\\t{%S0.<Vtype> - %V0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load1_4reg<q>\")]\n )\n \n-(define_expand \"aarch64_st1x2<VALLDIF:mode>\"\n+(define_expand \"aarch64_st1x2<vstruct_elt>\"\n   [(match_operand:DI 0 \"register_operand\")\n-   (match_operand:OI 1 \"register_operand\")\n-   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+   (match_operand:VSTRUCT_2QD 1 \"register_operand\")]\n   \"TARGET_SIMD\"\n {\n-  rtx mem = gen_rtx_MEM (OImode, operands[0]);\n-  emit_insn (gen_aarch64_st1_x2_<VALLDIF:mode> (mem, operands[1]));\n+  rtx mem = gen_rtx_MEM (<MODE>mode, operands[0]);\n+  emit_insn (gen_aarch64_st1_x2_<vstruct_elt> (mem, operands[1]));\n   DONE;\n })\n \n-(define_insn \"aarch64_st1_x2_<mode>\"\n-   [(set (match_operand:OI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t (unspec:OI\n-\t  [(match_operand:OI 1 \"register_operand\" \"w\")\n-          (unspec:VALLDIF [(const_int 2)] UNSPEC_VSTRUCTDUMMY)] UNSPEC_ST1))]\n+(define_insn \"aarch64_st1_x2_<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_2QD 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_2QD\n+\t\t[(match_operand:VSTRUCT_2QD 1 \"register_operand\" \"w\")]\n+\t\tUNSPEC_ST1))]\n   \"TARGET_SIMD\"\n   \"st1\\\\t{%S1.<Vtype> - %T1.<Vtype>}, %0\"\n   [(set_attr \"type\" \"neon_store1_2reg<q>\")]\n )\n \n-(define_expand \"aarch64_st1x3<VALLDIF:mode>\"\n+(define_expand \"aarch64_st1x3<vstruct_elt>\"\n   [(match_operand:DI 0 \"register_operand\")\n-   (match_operand:CI 1 \"register_operand\")\n-   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+   (match_operand:VSTRUCT_3QD 1 \"register_operand\")]\n   \"TARGET_SIMD\"\n {\n-  rtx mem = gen_rtx_MEM (CImode, operands[0]);\n-  emit_insn (gen_aarch64_st1_x3_<VALLDIF:mode> (mem, operands[1]));\n+  rtx mem = gen_rtx_MEM (<MODE>mode, operands[0]);\n+  emit_insn (gen_aarch64_st1_x3_<vstruct_elt> (mem, operands[1]));\n   DONE;\n })\n \n-(define_insn \"aarch64_st1_x3_<mode>\"\n-   [(set (match_operand:CI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:CI\n-         [(match_operand:CI 1 \"register_operand\" \"w\")\n-\t  (unspec:VALLDIF [(const_int 3)] UNSPEC_VSTRUCTDUMMY)] UNSPEC_ST1))]\n+(define_insn \"aarch64_st1_x3_<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_3QD 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_3QD\n+\t\t[(match_operand:VSTRUCT_3QD 1 \"register_operand\" \"w\")]\n+\t\tUNSPEC_ST1))]\n   \"TARGET_SIMD\"\n   \"st1\\\\t{%S1.<Vtype> - %U1.<Vtype>}, %0\"\n   [(set_attr \"type\" \"neon_store1_3reg<q>\")]\n )\n \n-(define_expand \"aarch64_st1x4<VALLDIF:mode>\"\n+(define_expand \"aarch64_st1x4<vstruct_elt>\"\n   [(match_operand:DI 0 \"register_operand\" \"\")\n-   (match_operand:XI 1 \"register_operand\" \"\")\n-   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+   (match_operand:VSTRUCT_4QD 1 \"register_operand\" \"\")]\n   \"TARGET_SIMD\"\n {\n-  rtx mem = gen_rtx_MEM (XImode, operands[0]);\n-  emit_insn (gen_aarch64_st1_x4_<VALLDIF:mode> (mem, operands[1]));\n+  rtx mem = gen_rtx_MEM (<MODE>mode, operands[0]);\n+  emit_insn (gen_aarch64_st1_x4_<vstruct_elt> (mem, operands[1]));\n   DONE;\n })\n \n-(define_insn \"aarch64_st1_x4_<mode>\"\n-  [(set (match_operand:XI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:XI\n-\t   [(match_operand:XI 1 \"register_operand\" \"w\")\n-\t   (unspec:VALLDIF [(const_int 4)] UNSPEC_VSTRUCTDUMMY)]\n-\tUNSPEC_ST1))]\n+(define_insn \"aarch64_st1_x4_<vstruct_elt>\"\n+  [(set (match_operand:VSTRUCT_4QD 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_4QD\n+\t\t[(match_operand:VSTRUCT_4QD 1 \"register_operand\" \"w\")]\n+\t\tUNSPEC_ST1))]\n   \"TARGET_SIMD\"\n   \"st1\\\\t{%S1.<Vtype> - %V1.<Vtype>}, %0\"\n   [(set_attr \"type\" \"neon_store1_4reg<q>\")]\n )\n \n+(define_insn \"*aarch64_mov<mode>\"\n+  [(set (match_operand:VSTRUCT_QD 0 \"aarch64_simd_nonimmediate_operand\" \"=w,Utv,w\")\n+\t(match_operand:VSTRUCT_QD 1 \"aarch64_simd_general_operand\" \" w,w,Utv\"))]\n+  \"TARGET_SIMD && !BYTES_BIG_ENDIAN\n+   && (register_operand (operands[0], <MODE>mode)\n+       || register_operand (operands[1], <MODE>mode))\"\n+  \"@\n+   #\n+   st1\\\\t{%S1.<Vtype> - %<Vendreg>1.<Vtype>}, %0\n+   ld1\\\\t{%S0.<Vtype> - %<Vendreg>0.<Vtype>}, %1\"\n+  [(set_attr \"type\" \"multiple,neon_store<nregs>_<nregs>reg_q,\\\n+\t\t     neon_load<nregs>_<nregs>reg_q\")\n+   (set_attr \"length\" \"<insn_count>,4,4\")]\n+)\n+\n (define_insn \"*aarch64_mov<mode>\"\n   [(set (match_operand:VSTRUCT 0 \"aarch64_simd_nonimmediate_operand\" \"=w,Utv,w\")\n \t(match_operand:VSTRUCT 1 \"aarch64_simd_general_operand\" \" w,w,Utv\"))]\n@@ -7243,6 +7268,34 @@\n   [(set_attr \"type\" \"neon_store1_1reg<q>\")]\n )\n \n+(define_insn \"*aarch64_be_mov<mode>\"\n+  [(set (match_operand:VSTRUCT_2D 0 \"nonimmediate_operand\" \"=w,m,w\")\n+\t(match_operand:VSTRUCT_2D 1 \"general_operand\"      \" w,w,m\"))]\n+  \"TARGET_SIMD && BYTES_BIG_ENDIAN\n+   && (register_operand (operands[0], <MODE>mode)\n+       || register_operand (operands[1], <MODE>mode))\"\n+  \"@\n+   #\n+   stp\\\\t%d1, %R1, %0\n+   ldp\\\\t%d0, %R0, %1\"\n+  [(set_attr \"type\" \"multiple,neon_stp,neon_ldp\")\n+   (set_attr \"length\" \"8,4,4\")]\n+)\n+\n+(define_insn \"*aarch64_be_mov<mode>\"\n+  [(set (match_operand:VSTRUCT_2Q 0 \"nonimmediate_operand\" \"=w,m,w\")\n+\t(match_operand:VSTRUCT_2Q 1 \"general_operand\"      \" w,w,m\"))]\n+  \"TARGET_SIMD && BYTES_BIG_ENDIAN\n+   && (register_operand (operands[0], <MODE>mode)\n+       || register_operand (operands[1], <MODE>mode))\"\n+  \"@\n+   #\n+   stp\\\\t%q1, %R1, %0\n+   ldp\\\\t%q0, %R0, %1\"\n+  [(set_attr \"type\" \"multiple,neon_stp_q,neon_ldp_q\")\n+   (set_attr \"length\" \"8,4,4\")]\n+)\n+\n (define_insn \"*aarch64_be_movoi\"\n   [(set (match_operand:OI 0 \"nonimmediate_operand\" \"=w,m,w\")\n \t(match_operand:OI 1 \"general_operand\"      \" w,w,m\"))]\n@@ -7257,6 +7310,17 @@\n    (set_attr \"length\" \"8,4,4\")]\n )\n \n+(define_insn \"*aarch64_be_mov<mode>\"\n+  [(set (match_operand:VSTRUCT_3QD 0 \"nonimmediate_operand\" \"=w,o,w\")\n+\t(match_operand:VSTRUCT_3QD 1 \"general_operand\"      \" w,w,o\"))]\n+  \"TARGET_SIMD && BYTES_BIG_ENDIAN\n+   && (register_operand (operands[0], <MODE>mode)\n+       || register_operand (operands[1], <MODE>mode))\"\n+  \"#\"\n+  [(set_attr \"type\" \"multiple\")\n+   (set_attr \"length\" \"12,8,8\")]\n+)\n+\n (define_insn \"*aarch64_be_movci\"\n   [(set (match_operand:CI 0 \"nonimmediate_operand\" \"=w,o,w\")\n \t(match_operand:CI 1 \"general_operand\"      \" w,w,o\"))]\n@@ -7268,6 +7332,17 @@\n    (set_attr \"length\" \"12,4,4\")]\n )\n \n+(define_insn \"*aarch64_be_mov<mode>\"\n+  [(set (match_operand:VSTRUCT_4QD 0 \"nonimmediate_operand\" \"=w,o,w\")\n+\t(match_operand:VSTRUCT_4QD 1 \"general_operand\"      \" w,w,o\"))]\n+  \"TARGET_SIMD && BYTES_BIG_ENDIAN\n+   && (register_operand (operands[0], <MODE>mode)\n+       || register_operand (operands[1], <MODE>mode))\"\n+  \"#\"\n+  [(set_attr \"type\" \"multiple\")\n+   (set_attr \"length\" \"16,8,8\")]\n+)\n+\n (define_insn \"*aarch64_be_movxi\"\n   [(set (match_operand:XI 0 \"nonimmediate_operand\" \"=w,o,w\")\n \t(match_operand:XI 1 \"general_operand\"      \" w,w,o\"))]\n@@ -7279,6 +7354,16 @@\n    (set_attr \"length\" \"16,4,4\")]\n )\n \n+(define_split\n+  [(set (match_operand:VSTRUCT_2QD 0 \"register_operand\")\n+\t(match_operand:VSTRUCT_2QD 1 \"register_operand\"))]\n+  \"TARGET_SIMD && reload_completed\"\n+  [(const_int 0)]\n+{\n+  aarch64_simd_emit_reg_reg_move (operands, <VSTRUCT_ELT>mode, 2);\n+  DONE;\n+})\n+\n (define_split\n   [(set (match_operand:OI 0 \"register_operand\")\n \t(match_operand:OI 1 \"register_operand\"))]\n@@ -7289,6 +7374,42 @@\n   DONE;\n })\n \n+(define_split\n+  [(set (match_operand:VSTRUCT_3QD 0 \"nonimmediate_operand\")\n+\t(match_operand:VSTRUCT_3QD 1 \"general_operand\"))]\n+  \"TARGET_SIMD && reload_completed\"\n+  [(const_int 0)]\n+{\n+  if (register_operand (operands[0], <MODE>mode)\n+      && register_operand (operands[1], <MODE>mode))\n+    {\n+      aarch64_simd_emit_reg_reg_move (operands, <VSTRUCT_ELT>mode, 3);\n+      DONE;\n+    }\n+  else if (BYTES_BIG_ENDIAN)\n+    {\n+      int elt_size = GET_MODE_SIZE (<MODE>mode).to_constant () / <nregs>;\n+      machine_mode pair_mode = elt_size == 16 ? V2x16QImode : V2x8QImode;\n+      emit_move_insn (simplify_gen_subreg (pair_mode, operands[0],\n+\t\t\t\t\t   <MODE>mode, 0),\n+\t\t      simplify_gen_subreg (pair_mode, operands[1],\n+\t\t\t\t\t   <MODE>mode, 0));\n+      emit_move_insn (gen_lowpart (<VSTRUCT_ELT>mode,\n+\t\t\t\t   simplify_gen_subreg (<VSTRUCT_ELT>mode,\n+\t\t\t\t\t\t\toperands[0],\n+\t\t\t\t\t\t\t<MODE>mode,\n+\t\t\t\t\t\t\t2 * elt_size)),\n+\t\t      gen_lowpart (<VSTRUCT_ELT>mode,\n+\t\t\t\t   simplify_gen_subreg (<VSTRUCT_ELT>mode,\n+\t\t\t\t\t\t\toperands[1],\n+\t\t\t\t\t\t\t<MODE>mode,\n+\t\t\t\t\t\t\t2 * elt_size)));\n+      DONE;\n+    }\n+  else\n+    FAIL;\n+})\n+\n (define_split\n   [(set (match_operand:CI 0 \"nonimmediate_operand\")\n \t(match_operand:CI 1 \"general_operand\"))]\n@@ -7317,6 +7438,36 @@\n     FAIL;\n })\n \n+(define_split\n+  [(set (match_operand:VSTRUCT_4QD 0 \"nonimmediate_operand\")\n+\t(match_operand:VSTRUCT_4QD 1 \"general_operand\"))]\n+  \"TARGET_SIMD && reload_completed\"\n+  [(const_int 0)]\n+{\n+  if (register_operand (operands[0], <MODE>mode)\n+      && register_operand (operands[1], <MODE>mode))\n+    {\n+      aarch64_simd_emit_reg_reg_move (operands, <VSTRUCT_ELT>mode, 4);\n+      DONE;\n+    }\n+  else if (BYTES_BIG_ENDIAN)\n+    {\n+      int elt_size = GET_MODE_SIZE (<MODE>mode).to_constant () / <nregs>;\n+      machine_mode pair_mode = elt_size == 16 ? V2x16QImode : V2x8QImode;\n+      emit_move_insn (simplify_gen_subreg (pair_mode, operands[0],\n+\t\t\t\t\t   <MODE>mode, 0),\n+\t\t      simplify_gen_subreg (pair_mode, operands[1],\n+\t\t\t\t\t   <MODE>mode, 0));\n+      emit_move_insn (simplify_gen_subreg (pair_mode, operands[0],\n+\t\t\t\t\t   <MODE>mode, 2 * elt_size),\n+\t\t      simplify_gen_subreg (pair_mode, operands[1],\n+\t\t\t\t\t   <MODE>mode, 2 * elt_size));\n+      DONE;\n+    }\n+  else\n+    FAIL;\n+})\n+\n (define_split\n   [(set (match_operand:XI 0 \"nonimmediate_operand\")\n \t(match_operand:XI 1 \"general_operand\"))]\n@@ -7341,91 +7492,85 @@\n     FAIL;\n })\n \n-(define_expand \"aarch64_ld<VSTRUCT:nregs>r<VALLDIF:mode>\"\n-  [(match_operand:VSTRUCT 0 \"register_operand\")\n-   (match_operand:DI 1 \"register_operand\")\n-   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+(define_expand \"aarch64_ld<nregs>r<vstruct_elt>\"\n+  [(match_operand:VSTRUCT_QD 0 \"register_operand\")\n+   (match_operand:DI 1 \"register_operand\")]\n   \"TARGET_SIMD\"\n {\n   rtx mem = gen_rtx_MEM (BLKmode, operands[1]);\n-  set_mem_size (mem, GET_MODE_SIZE (GET_MODE_INNER (<VALLDIF:MODE>mode))\n-\t\t     * <VSTRUCT:nregs>);\n+  set_mem_size (mem, GET_MODE_SIZE (GET_MODE_INNER (<MODE>mode)) * <nregs>);\n \n-  emit_insn (gen_aarch64_simd_ld<VSTRUCT:nregs>r<VALLDIF:mode> (operands[0],\n-\t\t\t\t\t\t\t\tmem));\n+  emit_insn (gen_aarch64_simd_ld<nregs>r<vstruct_elt> (operands[0], mem));\n   DONE;\n })\n \n-(define_insn \"aarch64_ld2<mode>_dreg\"\n-  [(set (match_operand:OI 0 \"register_operand\" \"=w\")\n-\t(unspec:OI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:VD [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD2_DREG))]\n+(define_insn \"aarch64_ld2<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_2DNX 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_2DNX [\n+\t  (match_operand:VSTRUCT_2DNX 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t  UNSPEC_LD2_DREG))]\n   \"TARGET_SIMD\"\n   \"ld2\\\\t{%S0.<Vtype> - %T0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load2_2reg<q>\")]\n )\n \n-(define_insn \"aarch64_ld2<mode>_dreg\"\n-  [(set (match_operand:OI 0 \"register_operand\" \"=w\")\n-\t(unspec:OI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:DX [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD2_DREG))]\n+(define_insn \"aarch64_ld2<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_2DX 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_2DX [\n+\t  (match_operand:VSTRUCT_2DX 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t  UNSPEC_LD2_DREG))]\n   \"TARGET_SIMD\"\n   \"ld1\\\\t{%S0.1d - %T0.1d}, %1\"\n   [(set_attr \"type\" \"neon_load1_2reg<q>\")]\n )\n \n-(define_insn \"aarch64_ld3<mode>_dreg\"\n-  [(set (match_operand:CI 0 \"register_operand\" \"=w\")\n-\t(unspec:CI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:VD [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD3_DREG))]\n+(define_insn \"aarch64_ld3<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_3DNX 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_3DNX [\n+\t  (match_operand:VSTRUCT_3DNX 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t  UNSPEC_LD3_DREG))]\n   \"TARGET_SIMD\"\n   \"ld3\\\\t{%S0.<Vtype> - %U0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load3_3reg<q>\")]\n )\n \n-(define_insn \"aarch64_ld3<mode>_dreg\"\n-  [(set (match_operand:CI 0 \"register_operand\" \"=w\")\n-\t(unspec:CI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:DX [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD3_DREG))]\n+(define_insn \"aarch64_ld3<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_3DX 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_3DX [\n+\t  (match_operand:VSTRUCT_3DX 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t  UNSPEC_LD3_DREG))]\n   \"TARGET_SIMD\"\n   \"ld1\\\\t{%S0.1d - %U0.1d}, %1\"\n   [(set_attr \"type\" \"neon_load1_3reg<q>\")]\n )\n \n-(define_insn \"aarch64_ld4<mode>_dreg\"\n-  [(set (match_operand:XI 0 \"register_operand\" \"=w\")\n-\t(unspec:XI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:VD [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD4_DREG))]\n+(define_insn \"aarch64_ld4<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_4DNX 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_4DNX [\n+\t  (match_operand:VSTRUCT_4DNX 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t  UNSPEC_LD4_DREG))]\n   \"TARGET_SIMD\"\n   \"ld4\\\\t{%S0.<Vtype> - %V0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load4_4reg<q>\")]\n )\n \n-(define_insn \"aarch64_ld4<mode>_dreg\"\n-  [(set (match_operand:XI 0 \"register_operand\" \"=w\")\n-\t(unspec:XI [(match_operand:BLK 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:DX [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD4_DREG))]\n+(define_insn \"aarch64_ld4<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_4DX 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_4DX [\n+\t  (match_operand:VSTRUCT_4DX 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t  UNSPEC_LD4_DREG))]\n   \"TARGET_SIMD\"\n   \"ld1\\\\t{%S0.1d - %V0.1d}, %1\"\n   [(set_attr \"type\" \"neon_load1_4reg<q>\")]\n )\n \n-(define_expand \"aarch64_ld<VSTRUCT:nregs><VDC:mode>\"\n- [(match_operand:VSTRUCT 0 \"register_operand\")\n-  (match_operand:DI 1 \"register_operand\")\n-  (unspec:VDC [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+(define_expand \"aarch64_ld<nregs><vstruct_elt>\"\n+ [(match_operand:VSTRUCT_D 0 \"register_operand\")\n+  (match_operand:DI 1 \"register_operand\")]\n   \"TARGET_SIMD\"\n {\n-  rtx mem = gen_rtx_MEM (BLKmode, operands[1]);\n-  set_mem_size (mem, <VSTRUCT:nregs> * 8);\n-\n-  emit_insn (gen_aarch64_ld<VSTRUCT:nregs><VDC:mode>_dreg (operands[0], mem));\n+  rtx mem = gen_rtx_MEM (<MODE>mode, operands[1]);\n+  emit_insn (gen_aarch64_ld<nregs><vstruct_elt>_dreg (operands[0], mem));\n   DONE;\n })\n \n@@ -7444,97 +7589,42 @@\n   DONE;\n })\n \n-(define_expand \"aarch64_ld<VSTRUCT:nregs><VQ:mode>\"\n- [(match_operand:VSTRUCT 0 \"register_operand\")\n-  (match_operand:DI 1 \"register_operand\")\n-  (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-  \"TARGET_SIMD\"\n-{\n-  machine_mode mode = <VSTRUCT:MODE>mode;\n-  rtx mem = gen_rtx_MEM (mode, operands[1]);\n-\n-  emit_insn (gen_aarch64_simd_ld<VSTRUCT:nregs><VQ:mode> (operands[0], mem));\n-  DONE;\n-})\n-\n-(define_expand \"aarch64_ld1x2<VQ:mode>\"\n- [(match_operand:OI 0 \"register_operand\")\n-  (match_operand:DI 1 \"register_operand\")\n-  (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+(define_expand \"aarch64_ld<nregs><vstruct_elt>\"\n+ [(match_operand:VSTRUCT_Q 0 \"register_operand\")\n+  (match_operand:DI 1 \"register_operand\")]\n   \"TARGET_SIMD\"\n {\n-  machine_mode mode = OImode;\n-  rtx mem = gen_rtx_MEM (mode, operands[1]);\n-\n-  emit_insn (gen_aarch64_simd_ld1<VQ:mode>_x2 (operands[0], mem));\n+  rtx mem = gen_rtx_MEM (<MODE>mode, operands[1]);\n+  emit_insn (gen_aarch64_simd_ld<nregs><vstruct_elt> (operands[0], mem));\n   DONE;\n })\n \n-(define_expand \"aarch64_ld1x2<VDC:mode>\"\n- [(match_operand:OI 0 \"register_operand\")\n-  (match_operand:DI 1 \"register_operand\")\n-  (unspec:VDC [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+(define_expand \"aarch64_ld1x2<vstruct_elt>\"\n+ [(match_operand:VSTRUCT_2QD 0 \"register_operand\")\n+  (match_operand:DI 1 \"register_operand\")]\n   \"TARGET_SIMD\"\n {\n-  machine_mode mode = OImode;\n+  machine_mode mode = <MODE>mode;\n   rtx mem = gen_rtx_MEM (mode, operands[1]);\n \n-  emit_insn (gen_aarch64_simd_ld1<VDC:mode>_x2 (operands[0], mem));\n+  emit_insn (gen_aarch64_simd_ld1<vstruct_elt>_x2 (operands[0], mem));\n   DONE;\n })\n \n-\n-(define_expand \"aarch64_ld<VSTRUCT:nregs>_lane<VALLDIF:mode>\"\n-  [(match_operand:VSTRUCT 0 \"register_operand\")\n+(define_expand \"aarch64_ld<nregs>_lane<vstruct_elt>\"\n+  [(match_operand:VSTRUCT_QD 0 \"register_operand\")\n \t(match_operand:DI 1 \"register_operand\")\n-\t(match_operand:VSTRUCT 2 \"register_operand\")\n-\t(match_operand:SI 3 \"immediate_operand\")\n-\t(unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+\t(match_operand:VSTRUCT_QD 2 \"register_operand\")\n+\t(match_operand:SI 3 \"immediate_operand\")]\n   \"TARGET_SIMD\"\n {\n   rtx mem = gen_rtx_MEM (BLKmode, operands[1]);\n-  set_mem_size (mem, GET_MODE_SIZE (GET_MODE_INNER (<VALLDIF:MODE>mode))\n-\t\t     * <VSTRUCT:nregs>);\n-\n-  aarch64_simd_lane_bounds (operands[3], 0, <VALLDIF:nunits>, NULL);\n-  emit_insn (gen_aarch64_vec_load_lanes<VSTRUCT:mode>_lane<VALLDIF:mode> (\n-\toperands[0], mem, operands[2], operands[3]));\n-  DONE;\n-})\n-\n-;; Expanders for builtins to extract vector registers from large\n-;; opaque integer modes.\n-\n-;; D-register list.\n+  set_mem_size (mem, GET_MODE_SIZE (GET_MODE_INNER (<MODE>mode)) * <nregs>);\n \n-(define_expand \"aarch64_get_dreg<VSTRUCT:mode><VDC:mode>\"\n- [(match_operand:VDC 0 \"register_operand\")\n-  (match_operand:VSTRUCT 1 \"register_operand\")\n-  (match_operand:SI 2 \"immediate_operand\")]\n-  \"TARGET_SIMD\"\n-{\n-  int part = INTVAL (operands[2]);\n-  rtx temp = gen_reg_rtx (<VDC:VDBL>mode);\n-  int offset = part * 16;\n-\n-  emit_move_insn (temp, gen_rtx_SUBREG (<VDC:VDBL>mode, operands[1], offset));\n-  emit_move_insn (operands[0], gen_lowpart (<VDC:MODE>mode, temp));\n-  DONE;\n-})\n-\n-;; Q-register list.\n-\n-(define_expand \"aarch64_get_qreg<VSTRUCT:mode><VQ:mode>\"\n- [(match_operand:VQ 0 \"register_operand\")\n-  (match_operand:VSTRUCT 1 \"register_operand\")\n-  (match_operand:SI 2 \"immediate_operand\")]\n-  \"TARGET_SIMD\"\n-{\n-  int part = INTVAL (operands[2]);\n-  int offset = part * 16;\n-\n-  emit_move_insn (operands[0],\n-\t\t  gen_rtx_SUBREG (<VQ:MODE>mode, operands[1], offset));\n+  aarch64_simd_lane_bounds (operands[3], 0,\n+\t\tGET_MODE_NUNITS (<MODE>mode).to_constant () / <nregs>, NULL);\n+  emit_insn (gen_aarch64_vec_load_lanes<mode>_lane<vstruct_elt> (operands[0],\n+\t\t\t\tmem, operands[2], operands[3]));\n   DONE;\n })\n \n@@ -7581,7 +7671,7 @@\n \n (define_insn \"aarch64_qtbl2<mode>\"\n   [(set (match_operand:VB 0 \"register_operand\" \"=w\")\n-\t(unspec:VB [(match_operand:OI 1 \"register_operand\" \"w\")\n+\t(unspec:VB [(match_operand:V2x16QI 1 \"register_operand\" \"w\")\n \t\t      (match_operand:VB 2 \"register_operand\" \"w\")]\n \t\t      UNSPEC_TBL))]\n   \"TARGET_SIMD\"\n@@ -7592,7 +7682,7 @@\n (define_insn \"aarch64_qtbx2<mode>\"\n   [(set (match_operand:VB 0 \"register_operand\" \"=w\")\n \t(unspec:VB [(match_operand:VB 1 \"register_operand\" \"0\")\n-\t\t      (match_operand:OI 2 \"register_operand\" \"w\")\n+\t\t      (match_operand:V2x16QI 2 \"register_operand\" \"w\")\n \t\t      (match_operand:VB 3 \"register_operand\" \"w\")]\n \t\t      UNSPEC_TBX))]\n   \"TARGET_SIMD\"\n@@ -7604,7 +7694,7 @@\n \n (define_insn \"aarch64_qtbl3<mode>\"\n   [(set (match_operand:VB 0 \"register_operand\" \"=w\")\n-\t(unspec:VB [(match_operand:CI 1 \"register_operand\" \"w\")\n+\t(unspec:VB [(match_operand:V3x16QI 1 \"register_operand\" \"w\")\n \t\t      (match_operand:VB 2 \"register_operand\" \"w\")]\n \t\t      UNSPEC_TBL))]\n   \"TARGET_SIMD\"\n@@ -7615,7 +7705,7 @@\n (define_insn \"aarch64_qtbx3<mode>\"\n   [(set (match_operand:VB 0 \"register_operand\" \"=w\")\n \t(unspec:VB [(match_operand:VB 1 \"register_operand\" \"0\")\n-\t\t      (match_operand:CI 2 \"register_operand\" \"w\")\n+\t\t      (match_operand:V3x16QI 2 \"register_operand\" \"w\")\n \t\t      (match_operand:VB 3 \"register_operand\" \"w\")]\n \t\t      UNSPEC_TBX))]\n   \"TARGET_SIMD\"\n@@ -7627,7 +7717,7 @@\n \n (define_insn \"aarch64_qtbl4<mode>\"\n   [(set (match_operand:VB 0 \"register_operand\" \"=w\")\n-\t(unspec:VB [(match_operand:XI 1 \"register_operand\" \"w\")\n+\t(unspec:VB [(match_operand:V4x16QI 1 \"register_operand\" \"w\")\n \t\t      (match_operand:VB 2 \"register_operand\" \"w\")]\n \t\t      UNSPEC_TBL))]\n   \"TARGET_SIMD\"\n@@ -7638,7 +7728,7 @@\n (define_insn \"aarch64_qtbx4<mode>\"\n   [(set (match_operand:VB 0 \"register_operand\" \"=w\")\n \t(unspec:VB [(match_operand:VB 1 \"register_operand\" \"0\")\n-\t\t      (match_operand:XI 2 \"register_operand\" \"w\")\n+\t\t      (match_operand:V4x16QI 2 \"register_operand\" \"w\")\n \t\t      (match_operand:VB 3 \"register_operand\" \"w\")]\n \t\t      UNSPEC_TBX))]\n   \"TARGET_SIMD\"\n@@ -7647,10 +7737,10 @@\n )\n \n (define_insn_and_split \"aarch64_combinev16qi\"\n-  [(set (match_operand:OI 0 \"register_operand\" \"=w\")\n-\t(unspec:OI [(match_operand:V16QI 1 \"register_operand\" \"w\")\n-\t\t    (match_operand:V16QI 2 \"register_operand\" \"w\")]\n-\t\t   UNSPEC_CONCAT))]\n+  [(set (match_operand:V2x16QI 0 \"register_operand\" \"=w\")\n+\t(unspec:V2x16QI [(match_operand:V16QI 1 \"register_operand\" \"w\")\n+\t\t\t (match_operand:V16QI 2 \"register_operand\" \"w\")]\n+\t\t\tUNSPEC_CONCAT))]\n   \"TARGET_SIMD\"\n   \"#\"\n   \"&& reload_completed\"\n@@ -7706,105 +7796,99 @@\n   [(set_attr \"type\" \"neon_rev<q>\")]\n )\n \n-(define_insn \"aarch64_st2<mode>_dreg\"\n-  [(set (match_operand:BLK 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:BLK [(match_operand:OI 1 \"register_operand\" \"w\")\n-                    (unspec:VD [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-                   UNSPEC_ST2))]\n+(define_insn \"aarch64_st2<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_2DNX 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_2DNX [\n+\t\t(match_operand:VSTRUCT_2DNX 1 \"register_operand\" \"w\")]\n+\t\tUNSPEC_ST2))]\n   \"TARGET_SIMD\"\n   \"st2\\\\t{%S1.<Vtype> - %T1.<Vtype>}, %0\"\n   [(set_attr \"type\" \"neon_store2_2reg\")]\n )\n \n-(define_insn \"aarch64_st2<mode>_dreg\"\n-  [(set (match_operand:BLK 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:BLK [(match_operand:OI 1 \"register_operand\" \"w\")\n-                    (unspec:DX [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-                   UNSPEC_ST2))]\n+(define_insn \"aarch64_st2<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_2DX 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_2DX [\n+\t\t(match_operand:VSTRUCT_2DX 1 \"register_operand\" \"w\")]\n+\t\tUNSPEC_ST2))]\n   \"TARGET_SIMD\"\n   \"st1\\\\t{%S1.1d - %T1.1d}, %0\"\n   [(set_attr \"type\" \"neon_store1_2reg\")]\n )\n \n-(define_insn \"aarch64_st3<mode>_dreg\"\n-  [(set (match_operand:BLK 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:BLK [(match_operand:CI 1 \"register_operand\" \"w\")\n-                    (unspec:VD [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-                   UNSPEC_ST3))]\n+(define_insn \"aarch64_st3<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_3DNX 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_3DNX [\n+\t\t(match_operand:VSTRUCT_3DNX 1 \"register_operand\" \"w\")]\n+\t\tUNSPEC_ST3))]\n   \"TARGET_SIMD\"\n   \"st3\\\\t{%S1.<Vtype> - %U1.<Vtype>}, %0\"\n   [(set_attr \"type\" \"neon_store3_3reg\")]\n )\n \n-(define_insn \"aarch64_st3<mode>_dreg\"\n-  [(set (match_operand:BLK 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:BLK [(match_operand:CI 1 \"register_operand\" \"w\")\n-                    (unspec:DX [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-                   UNSPEC_ST3))]\n+(define_insn \"aarch64_st3<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_3DX 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_3DX [\n+\t\t(match_operand:VSTRUCT_3DX 1 \"register_operand\" \"w\")]\n+\t\tUNSPEC_ST3))]\n   \"TARGET_SIMD\"\n   \"st1\\\\t{%S1.1d - %U1.1d}, %0\"\n   [(set_attr \"type\" \"neon_store1_3reg\")]\n )\n \n-(define_insn \"aarch64_st4<mode>_dreg\"\n-  [(set (match_operand:BLK 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:BLK [(match_operand:XI 1 \"register_operand\" \"w\")\n-                    (unspec:VD [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-                   UNSPEC_ST4))]\n+(define_insn \"aarch64_st4<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_4DNX 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_4DNX [\n+\t\t(match_operand:VSTRUCT_4DNX 1 \"register_operand\" \"w\")]\n+\t\tUNSPEC_ST4))]\n   \"TARGET_SIMD\"\n   \"st4\\\\t{%S1.<Vtype> - %V1.<Vtype>}, %0\"\n   [(set_attr \"type\" \"neon_store4_4reg\")]\n )\n \n-(define_insn \"aarch64_st4<mode>_dreg\"\n-  [(set (match_operand:BLK 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n-\t(unspec:BLK [(match_operand:XI 1 \"register_operand\" \"w\")\n-                    (unspec:DX [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-                   UNSPEC_ST4))]\n+(define_insn \"aarch64_st4<vstruct_elt>_dreg\"\n+  [(set (match_operand:VSTRUCT_4DX 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:VSTRUCT_4DX [\n+\t\t(match_operand:VSTRUCT_4DX 1 \"register_operand\" \"w\")]\n+\t\tUNSPEC_ST4))]\n   \"TARGET_SIMD\"\n   \"st1\\\\t{%S1.1d - %V1.1d}, %0\"\n   [(set_attr \"type\" \"neon_store1_4reg\")]\n )\n \n-(define_expand \"aarch64_st<VSTRUCT:nregs><VDC:mode>\"\n+(define_expand \"aarch64_st<nregs><vstruct_elt>\"\n  [(match_operand:DI 0 \"register_operand\")\n-  (match_operand:VSTRUCT 1 \"register_operand\")\n-  (unspec:VDC [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+  (match_operand:VSTRUCT_D 1 \"register_operand\")]\n   \"TARGET_SIMD\"\n {\n-  rtx mem = gen_rtx_MEM (BLKmode, operands[0]);\n-  set_mem_size (mem, <VSTRUCT:nregs> * 8);\n-\n-  emit_insn (gen_aarch64_st<VSTRUCT:nregs><VDC:mode>_dreg (mem, operands[1]));\n+  rtx mem = gen_rtx_MEM (<MODE>mode, operands[0]);\n+  emit_insn (gen_aarch64_st<nregs><vstruct_elt>_dreg (mem, operands[1]));\n   DONE;\n })\n \n-(define_expand \"aarch64_st<VSTRUCT:nregs><VQ:mode>\"\n+(define_expand \"aarch64_st<nregs><vstruct_elt>\"\n  [(match_operand:DI 0 \"register_operand\")\n-  (match_operand:VSTRUCT 1 \"register_operand\")\n-  (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+  (match_operand:VSTRUCT_Q 1 \"register_operand\")]\n   \"TARGET_SIMD\"\n {\n-  machine_mode mode = <VSTRUCT:MODE>mode;\n-  rtx mem = gen_rtx_MEM (mode, operands[0]);\n-\n-  emit_insn (gen_aarch64_simd_st<VSTRUCT:nregs><VQ:mode> (mem, operands[1]));\n+  rtx mem = gen_rtx_MEM (<MODE>mode, operands[0]);\n+  emit_insn (gen_aarch64_simd_st<nregs><vstruct_elt> (mem, operands[1]));\n   DONE;\n })\n \n-(define_expand \"aarch64_st<VSTRUCT:nregs>_lane<VALLDIF:mode>\"\n+(define_expand \"aarch64_st<nregs>_lane<vstruct_elt>\"\n  [(match_operand:DI 0 \"register_operand\")\n-  (match_operand:VSTRUCT 1 \"register_operand\")\n-  (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)\n+  (match_operand:VSTRUCT_QD 1 \"register_operand\")\n   (match_operand:SI 2 \"immediate_operand\")]\n   \"TARGET_SIMD\"\n {\n   rtx mem = gen_rtx_MEM (BLKmode, operands[0]);\n-  set_mem_size (mem, GET_MODE_SIZE (GET_MODE_INNER (<VALLDIF:MODE>mode))\n-\t\t     * <VSTRUCT:nregs>);\n+  set_mem_size (mem, GET_MODE_SIZE (GET_MODE_INNER (<MODE>mode)) * <nregs>);\n \n-  emit_insn (gen_aarch64_vec_store_lanes<VSTRUCT:mode>_lane<VALLDIF:mode> (\n-\t\tmem, operands[1], operands[2]));\n+  aarch64_simd_lane_bounds (operands[2], 0,\n+\t\tGET_MODE_NUNITS (<MODE>mode).to_constant () / <nregs>, NULL);\n+  emit_insn (gen_aarch64_vec_store_lanes<mode>_lane<vstruct_elt> (mem,\n+\t\t\t\t\toperands[1], operands[2]));\n   DONE;\n })\n \n@@ -7823,28 +7907,6 @@\n   DONE;\n })\n \n-;; Expander for builtins to insert vector registers into large\n-;; opaque integer modes.\n-\n-;; Q-register list.  We don't need a D-reg inserter as we zero\n-;; extend them in arm_neon.h and insert the resulting Q-regs.\n-\n-(define_expand \"aarch64_set_qreg<VSTRUCT:mode><VQ:mode>\"\n- [(match_operand:VSTRUCT 0 \"register_operand\")\n-  (match_operand:VSTRUCT 1 \"register_operand\")\n-  (match_operand:VQ 2 \"register_operand\")\n-  (match_operand:SI 3 \"immediate_operand\")]\n-  \"TARGET_SIMD\"\n-{\n-  int part = INTVAL (operands[3]);\n-  int offset = part * 16;\n-\n-  emit_move_insn (operands[0], operands[1]);\n-  emit_move_insn (gen_rtx_SUBREG (<VQ:MODE>mode, operands[0], offset),\n-\t\t  operands[2]);\n-  DONE;\n-})\n-\n ;; Standard pattern name vec_init<mode><Vel>.\n \n (define_expand \"vec_init<mode><Vel>\"\n@@ -7874,21 +7936,11 @@\n   [(set_attr \"type\" \"neon_load1_all_lanes\")]\n )\n \n-(define_insn \"aarch64_simd_ld1<mode>_x2\"\n-  [(set (match_operand:OI 0 \"register_operand\" \"=w\")\n-\t(unspec:OI [(match_operand:OI 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:VQ [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD1))]\n-  \"TARGET_SIMD\"\n-  \"ld1\\\\t{%S0.<Vtype> - %T0.<Vtype>}, %1\"\n-  [(set_attr \"type\" \"neon_load1_2reg<q>\")]\n-)\n-\n-(define_insn \"aarch64_simd_ld1<mode>_x2\"\n-  [(set (match_operand:OI 0 \"register_operand\" \"=w\")\n-\t(unspec:OI [(match_operand:OI 1 \"aarch64_simd_struct_operand\" \"Utv\")\n-\t\t    (unspec:VDC [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n-\t\t   UNSPEC_LD1))]\n+(define_insn \"aarch64_simd_ld1<vstruct_elt>_x2\"\n+  [(set (match_operand:VSTRUCT_2QD 0 \"register_operand\" \"=w\")\n+\t(unspec:VSTRUCT_2QD [\n+\t    (match_operand:VSTRUCT_2QD 1 \"aarch64_simd_struct_operand\" \"Utv\")]\n+\t    UNSPEC_LD1))]\n   \"TARGET_SIMD\"\n   \"ld1\\\\t{%S0.<Vtype> - %T0.<Vtype>}, %1\"\n   [(set_attr \"type\" \"neon_load1_2reg<q>\")]"}, {"sha": "f7c3f80023b7770fa85d1afba21b6e7d480ab975", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 168, "deletions": 36, "changes": 204, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=66f206b85395c273980e2b81a54dbddc4897e4a7", "patch": "@@ -2863,14 +2863,6 @@ aarch64_estimated_sve_vq ()\n   return estimated_poly_value (BITS_PER_SVE_VECTOR) / 128;\n }\n \n-/* Return true if MODE is any of the Advanced SIMD structure modes.  */\n-bool\n-aarch64_advsimd_struct_mode_p (machine_mode mode)\n-{\n-  return (TARGET_SIMD\n-\t  && (mode == OImode || mode == CImode || mode == XImode));\n-}\n-\n /* Return true if MODE is an SVE predicate mode.  */\n static bool\n aarch64_sve_pred_mode_p (machine_mode mode)\n@@ -2901,9 +2893,6 @@ const unsigned int VEC_ANY_DATA = VEC_ADVSIMD | VEC_SVE_DATA;\n static unsigned int\n aarch64_classify_vector_mode (machine_mode mode)\n {\n-  if (aarch64_advsimd_struct_mode_p (mode))\n-    return VEC_ADVSIMD | VEC_STRUCT;\n-\n   if (aarch64_sve_pred_mode_p (mode))\n     return VEC_SVE_PRED;\n \n@@ -2970,6 +2959,65 @@ aarch64_classify_vector_mode (machine_mode mode)\n     case E_VNx8DFmode:\n       return TARGET_SVE ? VEC_SVE_DATA | VEC_STRUCT : 0;\n \n+    case E_OImode:\n+    case E_CImode:\n+    case E_XImode:\n+      return TARGET_SIMD ? VEC_ADVSIMD | VEC_STRUCT : 0;\n+\n+    /* Structures of 64-bit Advanced SIMD vectors.  */\n+    case E_V2x8QImode:\n+    case E_V2x4HImode:\n+    case E_V2x2SImode:\n+    case E_V2x1DImode:\n+    case E_V2x4BFmode:\n+    case E_V2x4HFmode:\n+    case E_V2x2SFmode:\n+    case E_V2x1DFmode:\n+    case E_V3x8QImode:\n+    case E_V3x4HImode:\n+    case E_V3x2SImode:\n+    case E_V3x1DImode:\n+    case E_V3x4BFmode:\n+    case E_V3x4HFmode:\n+    case E_V3x2SFmode:\n+    case E_V3x1DFmode:\n+    case E_V4x8QImode:\n+    case E_V4x4HImode:\n+    case E_V4x2SImode:\n+    case E_V4x1DImode:\n+    case E_V4x4BFmode:\n+    case E_V4x4HFmode:\n+    case E_V4x2SFmode:\n+    case E_V4x1DFmode:\n+      return TARGET_SIMD ? VEC_ADVSIMD | VEC_STRUCT | VEC_PARTIAL : 0;\n+\n+    /* Structures of 128-bit Advanced SIMD vectors.  */\n+    case E_V2x16QImode:\n+    case E_V2x8HImode:\n+    case E_V2x4SImode:\n+    case E_V2x2DImode:\n+    case E_V2x8BFmode:\n+    case E_V2x8HFmode:\n+    case E_V2x4SFmode:\n+    case E_V2x2DFmode:\n+    case E_V3x16QImode:\n+    case E_V3x8HImode:\n+    case E_V3x4SImode:\n+    case E_V3x2DImode:\n+    case E_V3x8BFmode:\n+    case E_V3x8HFmode:\n+    case E_V3x4SFmode:\n+    case E_V3x2DFmode:\n+    case E_V4x16QImode:\n+    case E_V4x8HImode:\n+    case E_V4x4SImode:\n+    case E_V4x2DImode:\n+    case E_V4x8BFmode:\n+    case E_V4x8HFmode:\n+    case E_V4x4SFmode:\n+    case E_V4x2DFmode:\n+      return TARGET_SIMD ? VEC_ADVSIMD | VEC_STRUCT : 0;\n+\n     /* 64-bit Advanced SIMD vectors.  */\n     case E_V8QImode:\n     case E_V4HImode:\n@@ -2995,6 +3043,29 @@ aarch64_classify_vector_mode (machine_mode mode)\n     }\n }\n \n+/* Return true if MODE is any of the Advanced SIMD structure modes.  */\n+bool\n+aarch64_advsimd_struct_mode_p (machine_mode mode)\n+{\n+  unsigned int vec_flags = aarch64_classify_vector_mode (mode);\n+  return (vec_flags & VEC_ADVSIMD) && (vec_flags & VEC_STRUCT);\n+}\n+\n+/* Return true if MODE is an Advanced SIMD D-register structure mode.  */\n+static bool\n+aarch64_advsimd_partial_struct_mode_p (machine_mode mode)\n+{\n+  return (aarch64_classify_vector_mode (mode)\n+\t  == (VEC_ADVSIMD | VEC_STRUCT | VEC_PARTIAL));\n+}\n+\n+/* Return true if MODE is an Advanced SIMD Q-register structure mode.  */\n+static bool\n+aarch64_advsimd_full_struct_mode_p (machine_mode mode)\n+{\n+  return (aarch64_classify_vector_mode (mode) == (VEC_ADVSIMD | VEC_STRUCT));\n+}\n+\n /* Return true if MODE is any of the data vector modes, including\n    structure modes.  */\n static bool\n@@ -3037,14 +3108,53 @@ aarch64_vl_bytes (machine_mode mode, unsigned int vec_flags)\n   return BYTES_PER_SVE_PRED;\n }\n \n+/* Given an Advanced SIMD vector mode MODE and a tuple size NELEMS, return the\n+   corresponding vector structure mode.  */\n+static opt_machine_mode\n+aarch64_advsimd_vector_array_mode (machine_mode mode,\n+\t\t\t\t   unsigned HOST_WIDE_INT nelems)\n+{\n+  unsigned int flags = VEC_ADVSIMD | VEC_STRUCT;\n+  if (known_eq (GET_MODE_SIZE (mode), 8))\n+    flags |= VEC_PARTIAL;\n+\n+  machine_mode struct_mode;\n+  FOR_EACH_MODE_IN_CLASS (struct_mode, GET_MODE_CLASS (mode))\n+    if (aarch64_classify_vector_mode (struct_mode) == flags\n+\t&& GET_MODE_INNER (struct_mode) == GET_MODE_INNER (mode)\n+\t&& known_eq (GET_MODE_NUNITS (struct_mode),\n+\t     GET_MODE_NUNITS (mode) * nelems))\n+      return struct_mode;\n+  return opt_machine_mode ();\n+}\n+\n+/* Return the SVE vector mode that has NUNITS elements of mode INNER_MODE.  */\n+\n+opt_machine_mode\n+aarch64_sve_data_mode (scalar_mode inner_mode, poly_uint64 nunits)\n+{\n+  enum mode_class mclass = (is_a <scalar_float_mode> (inner_mode)\n+\t\t\t    ? MODE_VECTOR_FLOAT : MODE_VECTOR_INT);\n+  machine_mode mode;\n+  FOR_EACH_MODE_IN_CLASS (mode, mclass)\n+    if (inner_mode == GET_MODE_INNER (mode)\n+\t&& known_eq (nunits, GET_MODE_NUNITS (mode))\n+\t&& aarch64_sve_data_mode_p (mode))\n+      return mode;\n+  return opt_machine_mode ();\n+}\n+\n /* Implement target hook TARGET_ARRAY_MODE.  */\n static opt_machine_mode\n aarch64_array_mode (machine_mode mode, unsigned HOST_WIDE_INT nelems)\n {\n   if (aarch64_classify_vector_mode (mode) == VEC_SVE_DATA\n       && IN_RANGE (nelems, 2, 4))\n-    return mode_for_vector (GET_MODE_INNER (mode),\n-\t\t\t    GET_MODE_NUNITS (mode) * nelems);\n+    return aarch64_sve_data_mode (GET_MODE_INNER (mode),\n+\t\t\t\t  GET_MODE_NUNITS (mode) * nelems);\n+  if (aarch64_classify_vector_mode (mode) == VEC_ADVSIMD\n+      && IN_RANGE (nelems, 2, 4))\n+    return aarch64_advsimd_vector_array_mode (mode, nelems);\n \n   return opt_machine_mode ();\n }\n@@ -3121,22 +3231,6 @@ aarch64_get_mask_mode (machine_mode mode)\n   return default_get_mask_mode (mode);\n }\n \n-/* Return the SVE vector mode that has NUNITS elements of mode INNER_MODE.  */\n-\n-opt_machine_mode\n-aarch64_sve_data_mode (scalar_mode inner_mode, poly_uint64 nunits)\n-{\n-  enum mode_class mclass = (is_a <scalar_float_mode> (inner_mode)\n-\t\t\t    ? MODE_VECTOR_FLOAT : MODE_VECTOR_INT);\n-  machine_mode mode;\n-  FOR_EACH_MODE_IN_CLASS (mode, mclass)\n-    if (inner_mode == GET_MODE_INNER (mode)\n-\t&& known_eq (nunits, GET_MODE_NUNITS (mode))\n-\t&& aarch64_sve_data_mode_p (mode))\n-      return mode;\n-  return opt_machine_mode ();\n-}\n-\n /* Return the integer element mode associated with SVE mode MODE.  */\n \n static scalar_int_mode\n@@ -3261,6 +3355,8 @@ aarch64_hard_regno_nregs (unsigned regno, machine_mode mode)\n \tif (vec_flags & VEC_SVE_DATA)\n \t  return exact_div (GET_MODE_SIZE (mode),\n \t\t\t    aarch64_vl_bytes (mode, vec_flags)).to_constant ();\n+\tif (vec_flags == (VEC_ADVSIMD | VEC_STRUCT | VEC_PARTIAL))\n+\t  return GET_MODE_SIZE (mode).to_constant () / 8;\n \treturn CEIL (lowest_size, UNITS_PER_VREG);\n       }\n     case PR_REGS:\n@@ -9890,21 +9986,39 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t     instruction (only big endian will get here).\n \t     For ldp/stp instructions, the offset is scaled for the size of a\n \t     single element of the pair.  */\n-\t  if (mode == OImode)\n+\t  if (aarch64_advsimd_partial_struct_mode_p (mode)\n+\t      && known_eq (GET_MODE_SIZE (mode), 16))\n+\t    return aarch64_offset_7bit_signed_scaled_p (DImode, offset);\n+\t  if (aarch64_advsimd_full_struct_mode_p (mode)\n+\t      && known_eq (GET_MODE_SIZE (mode), 32))\n \t    return aarch64_offset_7bit_signed_scaled_p (TImode, offset);\n \n \t  /* Three 9/12 bit offsets checks because CImode will emit three\n \t     ldr/str instructions (only big endian will get here).  */\n-\t  if (mode == CImode)\n+\t  if (aarch64_advsimd_partial_struct_mode_p (mode)\n+\t      && known_eq (GET_MODE_SIZE (mode), 24))\n+\t    return (aarch64_offset_7bit_signed_scaled_p (DImode, offset)\n+\t\t    && (aarch64_offset_9bit_signed_unscaled_p (DImode,\n+\t\t\t\t\t\t\t       offset + 16)\n+\t\t\t|| offset_12bit_unsigned_scaled_p (DImode,\n+\t\t\t\t\t\t\t   offset + 16)));\n+\t  if (aarch64_advsimd_full_struct_mode_p (mode)\n+\t      && known_eq (GET_MODE_SIZE (mode), 48))\n \t    return (aarch64_offset_7bit_signed_scaled_p (TImode, offset)\n-\t\t    && (aarch64_offset_9bit_signed_unscaled_p (V16QImode,\n+\t\t    && (aarch64_offset_9bit_signed_unscaled_p (TImode,\n \t\t\t\t\t\t\t       offset + 32)\n-\t\t\t|| offset_12bit_unsigned_scaled_p (V16QImode,\n+\t\t\t|| offset_12bit_unsigned_scaled_p (TImode,\n \t\t\t\t\t\t\t   offset + 32)));\n \n \t  /* Two 7bit offsets checks because XImode will emit two ldp/stp\n \t     instructions (only big endian will get here).  */\n-\t  if (mode == XImode)\n+\t  if (aarch64_advsimd_partial_struct_mode_p (mode)\n+\t      && known_eq (GET_MODE_SIZE (mode), 32))\n+\t    return (aarch64_offset_7bit_signed_scaled_p (DImode, offset)\n+\t\t    && aarch64_offset_7bit_signed_scaled_p (DImode,\n+\t\t\t\t\t\t\t    offset + 16));\n+\t  if (aarch64_advsimd_full_struct_mode_p (mode)\n+\t      && known_eq (GET_MODE_SIZE (mode), 64))\n \t    return (aarch64_offset_7bit_signed_scaled_p (TImode, offset)\n \t\t    && aarch64_offset_7bit_signed_scaled_p (TImode,\n \t\t\t\t\t\t\t    offset + 32));\n@@ -10991,7 +11105,10 @@ aarch64_print_operand (FILE *f, rtx x, int code)\n       break;\n \n     case 'R':\n-      if (REG_P (x) && FP_REGNUM_P (REGNO (x)))\n+      if (REG_P (x) && FP_REGNUM_P (REGNO (x))\n+\t  && (aarch64_advsimd_partial_struct_mode_p (GET_MODE (x))))\n+\tasm_fprintf (f, \"d%d\", REGNO (x) - V0_REGNUM + 1);\n+      else if (REG_P (x) && FP_REGNUM_P (REGNO (x)))\n \tasm_fprintf (f, \"q%d\", REGNO (x) - V0_REGNUM + 1);\n       else if (REG_P (x) && GP_REGNUM_P (REGNO (x)))\n \tasm_fprintf (f, \"x%d\", REGNO (x) - R0_REGNUM + 1);\n@@ -22343,7 +22460,7 @@ aarch64_expand_vec_perm_1 (rtx target, rtx op0, rtx op1, rtx sel)\n \t}\n       else\n \t{\n-\t  pair = gen_reg_rtx (OImode);\n+\t  pair = gen_reg_rtx (V2x16QImode);\n \t  emit_insn (gen_aarch64_combinev16qi (pair, op0, op1));\n \t  emit_insn (gen_aarch64_qtbl2v16qi (target, pair, sel));\n \t}\n@@ -23320,6 +23437,12 @@ aarch64_expand_sve_vcond (machine_mode data_mode, machine_mode cmp_mode,\n static bool\n aarch64_modes_tieable_p (machine_mode mode1, machine_mode mode2)\n {\n+  if ((aarch64_advsimd_partial_struct_mode_p (mode1)\n+       != aarch64_advsimd_partial_struct_mode_p (mode2))\n+      && maybe_gt (GET_MODE_SIZE (mode1), 8)\n+      && maybe_gt (GET_MODE_SIZE (mode2), 8))\n+    return false;\n+\n   if (GET_MODE_CLASS (mode1) == GET_MODE_CLASS (mode2))\n     return true;\n \n@@ -25175,6 +25298,10 @@ aarch64_can_change_mode_class (machine_mode from,\n   bool from_pred_p = (from_flags & VEC_SVE_PRED);\n   bool to_pred_p = (to_flags & VEC_SVE_PRED);\n \n+  bool from_full_advsimd_struct_p = (from_flags == (VEC_ADVSIMD | VEC_STRUCT));\n+  bool to_partial_advsimd_struct_p = (to_flags == (VEC_ADVSIMD | VEC_STRUCT\n+\t\t\t\t\t\t   | VEC_PARTIAL));\n+\n   /* Don't allow changes between predicate modes and other modes.\n      Only predicate registers can hold predicate modes and only\n      non-predicate registers can hold non-predicate modes, so any\n@@ -25195,6 +25322,11 @@ aarch64_can_change_mode_class (machine_mode from,\n \t  || GET_MODE_UNIT_SIZE (from) != GET_MODE_UNIT_SIZE (to)))\n     return false;\n \n+  /* Don't allow changes between partial and full Advanced SIMD structure\n+     modes.  */\n+  if (from_full_advsimd_struct_p && to_partial_advsimd_struct_p)\n+    return false;\n+\n   if (maybe_ne (BITS_PER_SVE_VECTOR, 128u))\n     {\n       /* Don't allow changes between SVE modes and other modes that might"}, {"sha": "9838c39df60e0b812711683be3c8f62f912243bb", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 924, "deletions": 4590, "changes": 5514, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=66f206b85395c273980e2b81a54dbddc4897e4a7"}, {"sha": "52e07d4fef6cdc88cd8c8f7a67cb6fd2c19ea282", "filename": "gcc/config/aarch64/geniterators.sh", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Fgeniterators.sh", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Fgeniterators.sh", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fgeniterators.sh?ref=66f206b85395c273980e2b81a54dbddc4897e4a7", "patch": "@@ -64,7 +64,7 @@ iterdef {\n \tgsub(/ *\"[^\"]*\" *\\)/, \"\", s)\n \tgsub(/\\( */, \"\", s)\n \n-\tif (s !~ /^[A-Za-z0-9_]+ \\[[A-Z0-9 ]*\\]$/)\n+\tif (s !~ /^[A-Za-z0-9_]+ \\[[A-Za-z0-9 ]*\\]$/)\n \t\tnext\n \tsub(/\\[ */, \"\", s)\n \tsub(/ *\\]/, \"\", s)"}, {"sha": "bdc8ba3576cf2c9b4ae96b45a382234e4e25b13f", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 297, "deletions": 5, "changes": 302, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=66f206b85395c273980e2b81a54dbddc4897e4a7", "patch": "@@ -299,6 +299,102 @@\n ;; Advanced SIMD opaque structure modes.\n (define_mode_iterator VSTRUCT [OI CI XI])\n \n+;; Advanced SIMD 64-bit vector structure modes.\n+(define_mode_iterator VSTRUCT_D [V2x8QI V2x4HI V2x2SI V2x1DI\n+\t\t\t\t V2x4HF V2x2SF V2x1DF V2x4BF\n+\t\t\t\t V3x8QI V3x4HI V3x2SI V3x1DI\n+\t\t\t\t V3x4HF V3x2SF V3x1DF V3x4BF\n+\t\t\t\t V4x8QI V4x4HI V4x2SI V4x1DI\n+\t\t\t\t V4x4HF V4x2SF V4x1DF V4x4BF])\n+\n+;; Advanced SIMD 64-bit 2-vector structure modes.\n+(define_mode_iterator VSTRUCT_2D [V2x8QI V2x4HI V2x2SI V2x1DI\n+\t\t\t\t  V2x4HF V2x2SF V2x1DF V2x4BF])\n+\n+;; Advanced SIMD 64-bit 3-vector structure modes.\n+(define_mode_iterator VSTRUCT_3D [V3x8QI V3x4HI V3x2SI V3x1DI\n+\t\t\t\t  V3x4HF V3x2SF V3x1DF V3x4BF])\n+\n+;; Advanced SIMD 64-bit 4-vector structure modes.\n+(define_mode_iterator VSTRUCT_4D [V4x8QI V4x4HI V4x2SI V4x1DI\n+\t\t\t\t  V4x4HF V4x2SF V4x1DF V4x4BF])\n+\n+;; Advanced SIMD 64-bit 2-vector structure modes minus V2x1DI and V2x1DF.\n+(define_mode_iterator VSTRUCT_2DNX [V2x8QI V2x4HI V2x2SI V2x4HF\n+\t\t\t\t    V2x2SF V2x4BF])\n+\n+;; Advanced SIMD 64-bit 3-vector structure modes minus V3x1DI and V3x1DF.\n+(define_mode_iterator VSTRUCT_3DNX [V3x8QI V3x4HI V3x2SI V3x4HF\n+\t\t\t\t    V3x2SF V3x4BF])\n+\n+;; Advanced SIMD 64-bit 4-vector structure modes minus V4x1DI and V4x1DF.\n+(define_mode_iterator VSTRUCT_4DNX [V4x8QI V4x4HI V4x2SI V4x4HF\n+\t\t\t\t    V4x2SF V4x4BF])\n+\n+;; Advanced SIMD 64-bit structure modes with 64-bit elements.\n+(define_mode_iterator VSTRUCT_DX [V2x1DI V2x1DF V3x1DI V3x1DF V4x1DI V4x1DF])\n+\n+;; Advanced SIMD 64-bit 2-vector structure modes with 64-bit elements.\n+(define_mode_iterator VSTRUCT_2DX [V2x1DI V2x1DF])\n+\n+;; Advanced SIMD 64-bit 3-vector structure modes with 64-bit elements.\n+(define_mode_iterator VSTRUCT_3DX [V3x1DI V3x1DF])\n+\n+;; Advanced SIMD 64-bit 4-vector structure modes with 64-bit elements.\n+(define_mode_iterator VSTRUCT_4DX [V4x1DI V4x1DF])\n+\n+;; Advanced SIMD 128-bit vector structure modes.\n+(define_mode_iterator VSTRUCT_Q [V2x16QI V2x8HI V2x4SI V2x2DI\n+\t\t\t\t V2x8HF V2x4SF V2x2DF V2x8BF\n+\t\t\t\t V3x16QI V3x8HI V3x4SI V3x2DI\n+\t\t\t\t V3x8HF V3x4SF V3x2DF V3x8BF\n+\t\t\t\t V4x16QI V4x8HI V4x4SI V4x2DI\n+\t\t\t\t V4x8HF V4x4SF V4x2DF V4x8BF])\n+\n+;; Advanced SIMD 128-bit 2-vector structure modes.\n+(define_mode_iterator VSTRUCT_2Q [V2x16QI V2x8HI V2x4SI V2x2DI\n+\t\t\t\t  V2x8HF V2x4SF V2x2DF V2x8BF])\n+\n+;; Advanced SIMD 128-bit 3-vector structure modes.\n+(define_mode_iterator VSTRUCT_3Q [V3x16QI V3x8HI V3x4SI V3x2DI\n+\t\t\t\t  V3x8HF V3x4SF V3x2DF V3x8BF])\n+\n+;; Advanced SIMD 128-bit 4-vector structure modes.\n+(define_mode_iterator VSTRUCT_4Q [V4x16QI V4x8HI V4x4SI V4x2DI\n+\t\t\t\t  V4x8HF V4x4SF V4x2DF V4x8BF])\n+\n+;; Advanced SIMD 2-vector structure modes.\n+(define_mode_iterator VSTRUCT_2QD [V2x8QI V2x4HI V2x2SI V2x1DI\n+\t\t\t\t   V2x4HF V2x2SF V2x1DF V2x4BF\n+\t\t\t\t   V2x16QI V2x8HI V2x4SI V2x2DI\n+\t\t\t\t   V2x8HF V2x4SF V2x2DF V2x8BF])\n+\n+;; Advanced SIMD 3-vector structure modes.\n+(define_mode_iterator VSTRUCT_3QD [V3x8QI V3x4HI V3x2SI V3x1DI\n+\t\t\t\t   V3x4HF V3x2SF V3x1DF V3x4BF\n+\t\t\t\t   V3x16QI V3x8HI V3x4SI V3x2DI\n+\t\t\t\t   V3x8HF V3x4SF V3x2DF V3x8BF])\n+\n+;; Advanced SIMD 4-vector structure modes.\n+(define_mode_iterator VSTRUCT_4QD [V4x8QI V4x4HI V4x2SI V4x1DI\n+\t\t\t\t   V4x4HF V4x2SF V4x1DF V4x4BF\n+\t\t\t\t   V4x16QI V4x8HI V4x4SI V4x2DI\n+\t\t\t\t   V4x8HF V4x4SF V4x2DF V4x8BF])\n+\n+;; Advanced SIMD vector structure modes.\n+(define_mode_iterator VSTRUCT_QD [V2x8QI V2x4HI V2x2SI V2x1DI\n+\t\t\t\t  V2x4HF V2x2SF V2x1DF V2x4BF\n+\t\t\t\t  V3x8QI V3x4HI V3x2SI V3x1DI\n+\t\t\t\t  V3x4HF V3x2SF V3x1DF V3x4BF\n+\t\t\t\t  V4x8QI V4x4HI V4x2SI V4x1DI\n+\t\t\t\t  V4x4HF V4x2SF V4x1DF V4x4BF\n+\t\t\t\t  V2x16QI V2x8HI V2x4SI V2x2DI\n+\t\t\t\t  V2x8HF V2x4SF V2x2DF V2x8BF\n+\t\t\t\t  V3x16QI V3x8HI V3x4SI V3x2DI\n+\t\t\t\t  V3x8HF V3x4SF V3x2DF V3x8BF\n+\t\t\t\t  V4x16QI V4x8HI V4x4SI V4x2DI\n+\t\t\t\t  V4x8HF V4x4SF V4x2DF V4x8BF])\n+\n ;; Double scalar modes\n (define_mode_iterator DX [DI DF])\n \n@@ -1021,7 +1117,31 @@\n                          (DI   \"1d\") (DF    \"1d\")\n                          (V2DI \"2d\") (V2SF \"2s\")\n \t\t\t (V4SF \"4s\") (V2DF \"2d\")\n-\t\t\t (V4HF \"4h\") (V8HF \"8h\")])\n+\t\t\t (V4HF \"4h\") (V8HF \"8h\")\n+\t\t\t (V2x8QI \"8b\") (V2x4HI \"4h\")\n+\t\t\t (V2x2SI \"2s\") (V2x1DI  \"1d\")\n+\t\t\t (V2x4HF \"4h\") (V2x2SF \"2s\")\n+\t\t\t (V2x1DF \"1d\") (V2x4BF \"4h\")\n+\t\t\t (V2x16QI \"16b\") (V2x8HI \"8h\")\n+\t\t\t (V2x4SI \"4s\") (V2x2DI \"2d\")\n+\t\t\t (V2x8HF \"8h\") (V2x4SF \"4s\")\n+\t\t\t (V2x2DF \"2d\") (V2x8BF \"8h\")\n+\t\t\t (V3x8QI \"8b\") (V3x4HI \"4h\")\n+\t\t\t (V3x2SI \"2s\") (V3x1DI \"1d\")\n+\t\t\t (V3x4HF \"4h\") (V3x2SF \"2s\")\n+\t\t\t (V3x1DF \"1d\") (V3x4BF \"4h\")\n+\t\t\t (V3x16QI \"16b\") (V3x8HI \"8h\")\n+\t\t\t (V3x4SI \"4s\") (V3x2DI \"2d\")\n+\t\t\t (V3x8HF \"8h\") (V3x4SF \"4s\")\n+\t\t\t (V3x2DF \"2d\") (V3x8BF \"8h\")\n+\t\t\t (V4x8QI \"8b\") (V4x4HI \"4h\")\n+\t\t\t (V4x2SI \"2s\") (V4x1DI \"1d\")\n+\t\t\t (V4x4HF \"4h\") (V4x2SF \"2s\")\n+\t\t\t (V4x1DF \"1d\") (V4x4BF \"4h\")\n+\t\t\t (V4x16QI \"16b\") (V4x8HI \"8h\")\n+\t\t\t (V4x4SI \"4s\") (V4x2DI \"2d\")\n+\t\t\t (V4x8HF \"8h\") (V4x4SF \"4s\")\n+\t\t\t (V4x2DF \"2d\") (V4x8BF \"8h\")])\n \n ;; Map mode to type used in widening multiplies.\n (define_mode_attr Vcondtype [(V4HI \"4h\") (V8HI \"4h\") (V2SI \"2s\") (V4SI \"2s\")])\n@@ -1059,6 +1179,30 @@\n \t\t\t  (V4HF \"h\") (V8HF  \"h\")\n \t\t\t  (V2SF \"s\") (V4SF  \"s\")\n \t\t\t  (V2DF \"d\")\n+\t\t\t  (V2x8QI \"b\") (V2x4HI \"h\")\n+\t\t\t  (V2x2SI \"s\") (V2x1DI \"d\")\n+\t\t\t  (V2x4HF \"h\") (V2x2SF \"s\")\n+\t\t\t  (V2x1DF \"d\") (V2x4BF \"h\")\n+\t\t\t  (V2x16QI \"b\") (V2x8HI \"h\")\n+\t\t\t  (V2x4SI \"s\") (V2x2DI \"d\")\n+\t\t\t  (V2x8HF \"h\") (V2x4SF \"s\")\n+\t\t\t  (V2x2DF \"d\") (V2x8BF \"h\")\n+\t\t\t  (V3x8QI \"b\") (V3x4HI \"h\")\n+\t\t\t  (V3x2SI \"s\") (V3x1DI \"d\")\n+\t\t\t  (V3x4HF \"h\") (V3x2SF \"s\")\n+\t\t\t  (V3x1DF \"d\") (V3x4BF \"h\")\n+\t\t\t  (V3x16QI \"b\") (V3x8HI \"h\")\n+\t\t\t  (V3x4SI \"s\") (V3x2DI \"d\")\n+\t\t\t  (V3x8HF \"h\") (V3x4SF \"s\")\n+\t\t\t  (V3x2DF \"d\") (V3x8BF \"h\")\n+\t\t\t  (V4x8QI \"b\") (V4x4HI \"h\")\n+\t\t\t  (V4x2SI \"s\") (V4x1DI \"d\")\n+\t\t\t  (V4x4HF \"h\") (V4x2SF \"s\")\n+\t\t\t  (V4x1DF \"d\") (V4x4BF \"h\")\n+\t\t\t  (V4x16QI \"b\") (V4x8HI \"h\")\n+\t\t\t  (V4x4SI \"s\") (V4x2DI \"d\")\n+\t\t\t  (V4x8HF \"h\") (V4x4SF \"s\")\n+\t\t\t  (V4x2DF \"d\") (V4x8BF \"h\")\n \t\t\t  (VNx16BI \"b\") (VNx8BI \"h\") (VNx4BI \"s\") (VNx2BI \"d\")\n \t\t\t  (VNx16QI \"b\") (VNx8QI \"b\") (VNx4QI \"b\") (VNx2QI \"b\")\n \t\t\t  (VNx8HI \"h\") (VNx4HI \"h\") (VNx2HI \"h\")\n@@ -1138,6 +1282,58 @@\n \t\t\t  (SI   \"8b\")  (SF    \"8b\")\n \t\t\t  (V4BF \"8b\")  (V8BF  \"16b\")])\n \n+;; Advanced SIMD vector structure to element modes.\n+(define_mode_attr VSTRUCT_ELT [(V2x8QI \"V8QI\") (V2x4HI \"V4HI\")\n+\t\t\t       (V2x2SI \"V2SI\") (V2x1DI \"DI\")\n+\t\t\t       (V2x4HF \"V4HF\") (V2x2SF \"V2SF\")\n+\t\t\t       (V2x1DF \"DF\") (V2x4BF \"V4BF\")\n+\t\t\t       (V3x8QI \"V8QI\") (V3x4HI \"V4HI\")\n+\t\t\t       (V3x2SI \"V2SI\") (V3x1DI \"DI\")\n+\t\t\t       (V3x4HF \"V4HF\") (V3x2SF \"V2SF\")\n+\t\t\t       (V3x1DF \"DF\") (V3x4BF \"V4BF\")\n+\t\t\t       (V4x8QI \"V8QI\") (V4x4HI \"V4HI\")\n+\t\t\t       (V4x2SI \"V2SI\") (V4x1DI \"DI\")\n+\t\t\t       (V4x4HF \"V4HF\") (V4x2SF \"V2SF\")\n+\t\t\t       (V4x1DF \"DF\") (V4x4BF \"V4BF\")\n+\t\t\t       (V2x16QI \"V16QI\") (V2x8HI \"V8HI\")\n+\t\t\t       (V2x4SI \"V4SI\") (V2x2DI \"V2DI\")\n+\t\t\t       (V2x8HF \"V8HF\") (V2x4SF \"V4SF\")\n+\t\t\t       (V2x2DF \"V2DF\") (V2x8BF \"V8BF\")\n+\t\t\t       (V3x16QI \"V16QI\") (V3x8HI \"V8HI\")\n+\t\t\t       (V3x4SI \"V4SI\") (V3x2DI \"V2DI\")\n+\t\t\t       (V3x8HF \"V8HF\") (V3x4SF \"V4SF\")\n+\t\t\t       (V3x2DF \"V2DF\") (V3x8BF \"V8BF\")\n+\t\t\t       (V4x16QI \"V16QI\") (V4x8HI \"V8HI\")\n+\t\t\t       (V4x4SI \"V4SI\") (V4x2DI \"V2DI\")\n+\t\t\t       (V4x8HF \"V8HF\") (V4x4SF \"V4SF\")\n+\t\t\t       (V4x2DF \"V2DF\") (V4x8BF \"V8BF\")])\n+\n+;; Advanced SIMD vector structure to element modes in lower case.\n+(define_mode_attr vstruct_elt [(V2x8QI \"v8qi\") (V2x4HI \"v4hi\")\n+\t\t\t       (V2x2SI \"v2si\") (V2x1DI \"di\")\n+\t\t\t       (V2x4HF \"v4hf\") (V2x2SF \"v2sf\")\n+\t\t\t       (V2x1DF \"df\") (V2x4BF \"v4bf\")\n+\t\t\t       (V3x8QI \"v8qi\") (V3x4HI \"v4hi\")\n+\t\t\t       (V3x2SI \"v2si\") (V3x1DI \"di\")\n+\t\t\t       (V3x4HF \"v4hf\") (V3x2SF \"v2sf\")\n+\t\t\t       (V3x1DF \"df\") (V3x4BF \"v4bf\")\n+\t\t\t       (V4x8QI \"v8qi\") (V4x4HI \"v4hi\")\n+\t\t\t       (V4x2SI \"v2si\") (V4x1DI \"di\")\n+\t\t\t       (V4x4HF \"v4hf\") (V4x2SF \"v2sf\")\n+\t\t\t       (V4x1DF \"df\") (V4x4BF \"v4bf\")\n+\t\t\t       (V2x16QI \"v16qi\") (V2x8HI \"v8hi\")\n+\t\t\t       (V2x4SI \"v4si\") (V2x2DI \"v2di\")\n+\t\t\t       (V2x8HF \"v8hf\") (V2x4SF \"v4sf\")\n+\t\t\t       (V2x2DF \"v2df\") (V2x8BF \"v8bf\")\n+\t\t\t       (V3x16QI \"v16qi\") (V3x8HI \"v8hi\")\n+\t\t\t       (V3x4SI \"v4si\") (V3x2DI \"v2di\")\n+\t\t\t       (V3x8HF \"v8hf\") (V3x4SF \"v4sf\")\n+\t\t\t       (V3x2DF \"v2df\") (V3x8BF \"v8bf\")\n+\t\t\t       (V4x16QI \"v16qi\") (V4x8HI \"v8hi\")\n+\t\t\t       (V4x4SI \"v4si\") (V4x2DI \"v2di\")\n+\t\t\t       (V4x8HF \"v8hf\") (V4x4SF \"v4sf\")\n+\t\t\t       (V4x2DF \"v2df\") (V4x8BF \"v8bf\")])\n+\n ;; Define element mode for each vector mode.\n (define_mode_attr VEL [(V8QI  \"QI\") (V16QI \"QI\")\n \t\t       (V4HI \"HI\") (V8HI  \"HI\")\n@@ -1492,12 +1688,60 @@\n (define_mode_attr vwx [(V4HI \"x\") (V8HI \"x\") (HI \"x\")\n \t\t       (V2SI \"w\") (V4SI \"w\") (SI \"w\")])\n \n-(define_mode_attr Vendreg [(OI \"T\") (CI \"U\") (XI \"V\")])\n+(define_mode_attr Vendreg [(OI \"T\") (CI \"U\") (XI \"V\")\n+\t\t\t   (V2x8QI \"T\") (V2x16QI \"T\")\n+\t\t\t   (V2x4HI \"T\") (V2x8HI \"T\")\n+\t\t\t   (V2x2SI \"T\") (V2x4SI \"T\")\n+\t\t\t   (V2x1DI \"T\") (V2x2DI \"T\")\n+\t\t\t   (V2x4HF \"T\") (V2x8HF \"T\")\n+\t\t\t   (V2x2SF \"T\") (V2x4SF \"T\")\n+\t\t\t   (V2x1DF \"T\") (V2x2DF \"T\")\n+\t\t\t   (V2x4BF \"T\") (V2x8BF \"T\")\n+\t\t\t   (V3x8QI \"U\") (V3x16QI \"U\")\n+\t\t\t   (V3x4HI \"U\") (V3x8HI \"U\")\n+\t\t\t   (V3x2SI \"U\") (V3x4SI \"U\")\n+\t\t\t   (V3x1DI \"U\") (V3x2DI \"U\")\n+\t\t\t   (V3x4HF \"U\") (V3x8HF \"U\")\n+\t\t\t   (V3x2SF \"U\") (V3x4SF \"U\")\n+\t\t\t   (V3x1DF \"U\") (V3x2DF \"U\")\n+\t\t\t   (V3x4BF \"U\") (V3x8BF \"U\")\n+\t\t\t   (V4x8QI \"V\") (V4x16QI \"V\")\n+\t\t\t   (V4x4HI \"V\") (V4x8HI \"V\")\n+\t\t\t   (V4x2SI \"V\") (V4x4SI \"V\")\n+\t\t\t   (V4x1DI \"V\") (V4x2DI \"V\")\n+\t\t\t   (V4x4HF \"V\") (V4x8HF \"V\")\n+\t\t\t   (V4x2SF \"V\") (V4x4SF \"V\")\n+\t\t\t   (V4x1DF \"V\") (V4x2DF \"V\")\n+\t\t\t   (V4x4BF \"V\") (V4x8BF \"V\")])\n \n ;; This is both the number of Q-Registers needed to hold the corresponding\n ;; opaque large integer mode, and the number of elements touched by the\n ;; ld..._lane and st..._lane operations.\n-(define_mode_attr nregs [(OI \"2\") (CI \"3\") (XI \"4\")])\n+(define_mode_attr nregs [(OI \"2\") (CI \"3\") (XI \"4\")\n+\t\t\t (V2x8QI \"2\") (V2x16QI \"2\")\n+\t\t\t (V2x4HI \"2\") (V2x8HI \"2\")\n+\t\t\t (V2x2SI \"2\") (V2x4SI \"2\")\n+\t\t\t (V2x1DI \"2\") (V2x2DI \"2\")\n+\t\t\t (V2x4HF \"2\") (V2x8HF \"2\")\n+\t\t\t (V2x2SF \"2\") (V2x4SF \"2\")\n+\t\t\t (V2x1DF \"2\") (V2x2DF \"2\")\n+\t\t\t (V2x4BF \"2\") (V2x8BF \"2\")\n+\t\t\t (V3x8QI \"3\") (V3x16QI \"3\")\n+\t\t\t (V3x4HI \"3\") (V3x8HI \"3\")\n+\t\t\t (V3x2SI \"3\") (V3x4SI \"3\")\n+\t\t\t (V3x1DI \"3\") (V3x2DI \"3\")\n+\t\t\t (V3x4HF \"3\") (V3x8HF \"3\")\n+\t\t\t (V3x2SF \"3\") (V3x4SF \"3\")\n+\t\t\t (V3x1DF \"3\") (V3x2DF \"3\")\n+\t\t\t (V3x4BF \"3\") (V3x8BF \"3\")\n+\t\t\t (V4x8QI \"4\") (V4x16QI \"4\")\n+\t\t\t (V4x4HI \"4\") (V4x8HI \"4\")\n+\t\t\t (V4x2SI \"4\") (V4x4SI \"4\")\n+\t\t\t (V4x1DI \"4\") (V4x2DI \"4\")\n+\t\t\t (V4x4HF \"4\") (V4x8HF \"4\")\n+\t\t\t (V4x2SF \"4\") (V4x4SF \"4\")\n+\t\t\t (V4x1DF \"4\") (V4x2DF \"4\")\n+\t\t\t (V4x4BF \"4\") (V4x8BF \"4\")])\n \n ;; Mode for atomic operation suffixes\n (define_mode_attr atomic_sfx\n@@ -1575,7 +1819,31 @@\n \t\t     (V4BF \"\") (V8BF \"_q\")\n \t\t     (V2SF \"\") (V4SF  \"_q\")\n \t\t\t       (V2DF  \"_q\")\n-\t\t     (QI \"\") (HI \"\") (SI \"\") (DI \"\") (HF \"\") (SF \"\") (DF \"\")])\n+\t\t     (QI \"\") (HI \"\") (SI \"\") (DI \"\") (HF \"\") (SF \"\") (DF \"\")\n+\t\t     (V2x8QI \"\") (V2x16QI \"_q\")\n+\t\t     (V2x4HI \"\") (V2x8HI \"_q\")\n+\t\t     (V2x2SI \"\") (V2x4SI \"_q\")\n+\t\t     (V2x1DI \"\") (V2x2DI \"_q\")\n+\t\t     (V2x4HF \"\") (V2x8HF \"_q\")\n+\t\t     (V2x2SF \"\") (V2x4SF \"_q\")\n+\t\t     (V2x1DF \"\") (V2x2DF \"_q\")\n+\t\t     (V2x4BF \"\") (V2x8BF \"_q\")\n+\t\t     (V3x8QI \"\") (V3x16QI \"_q\")\n+\t\t     (V3x4HI \"\") (V3x8HI \"_q\")\n+\t\t     (V3x2SI \"\") (V3x4SI \"_q\")\n+\t\t     (V3x1DI \"\") (V3x2DI \"_q\")\n+\t\t     (V3x4HF \"\") (V3x8HF \"_q\")\n+\t\t     (V3x2SF \"\") (V3x4SF \"_q\")\n+\t\t     (V3x1DF \"\") (V3x2DF \"_q\")\n+\t\t     (V3x4BF \"\") (V3x8BF \"_q\")\n+\t\t     (V4x8QI \"\") (V4x16QI \"_q\")\n+\t\t     (V4x4HI \"\") (V4x8HI \"_q\")\n+\t\t     (V4x2SI \"\") (V4x4SI \"_q\")\n+\t\t     (V4x1DI \"\") (V4x2DI \"_q\")\n+\t\t     (V4x4HF \"\") (V4x8HF \"_q\")\n+\t\t     (V4x2SF \"\") (V4x4SF \"_q\")\n+\t\t     (V4x1DF \"\") (V4x2DF \"_q\")\n+\t\t     (V4x4BF \"\") (V4x8BF \"_q\")])\n \n (define_mode_attr vp [(V8QI \"v\") (V16QI \"v\")\n \t\t      (V4HI \"v\") (V8HI  \"v\")\n@@ -1597,7 +1865,31 @@\n (define_mode_attr Vbfdottype [(V2SF \"4h\") (V4SF \"8h\")])\n \n ;; Sum of lengths of instructions needed to move vector registers of a mode.\n-(define_mode_attr insn_count [(OI \"8\") (CI \"12\") (XI \"16\")])\n+(define_mode_attr insn_count [(OI \"8\") (CI \"12\") (XI \"16\")\n+\t\t\t      (V2x8QI \"8\") (V2x16QI \"8\")\n+\t\t\t      (V2x4HI \"8\") (V2x8HI \"8\")\n+\t\t\t      (V2x2SI \"8\") (V2x4SI \"8\")\n+\t\t\t      (V2x1DI \"8\") (V2x2DI \"8\")\n+\t\t\t      (V2x4HF \"8\") (V2x8HF \"8\")\n+\t\t\t      (V2x2SF \"8\") (V2x4SF \"8\")\n+\t\t\t      (V2x1DF \"8\") (V2x2DF \"8\")\n+\t\t\t      (V2x4BF \"8\") (V2x8BF \"8\")\n+\t\t\t      (V3x8QI \"12\") (V3x16QI \"12\")\n+\t\t\t      (V3x4HI \"12\") (V3x8HI \"12\")\n+\t\t\t      (V3x2SI \"12\") (V3x4SI \"12\")\n+\t\t\t      (V3x1DI \"12\") (V3x2DI \"12\")\n+\t\t\t      (V3x4HF \"12\") (V3x8HF \"12\")\n+\t\t\t      (V3x2SF \"12\") (V3x4SF \"12\")\n+\t\t\t      (V3x1DF \"12\") (V3x2DF \"12\")\n+\t\t\t      (V3x4BF \"12\") (V3x8BF \"12\")\n+\t\t\t      (V4x8QI \"16\") (V4x16QI \"16\")\n+\t\t\t      (V4x4HI \"16\") (V4x8HI \"16\")\n+\t\t\t      (V4x2SI \"16\") (V4x4SI \"16\")\n+\t\t\t      (V4x1DI \"16\") (V4x2DI \"16\")\n+\t\t\t      (V4x4HF \"16\") (V4x8HF \"16\")\n+\t\t\t      (V4x2SF \"16\") (V4x4SF \"16\")\n+\t\t\t      (V4x1DF \"16\") (V4x2DF \"16\")\n+\t\t\t      (V4x4BF \"16\") (V4x8BF \"16\")])\n \n ;; -fpic small model GOT reloc modifers: gotpage_lo15/lo14 for ILP64/32.\n ;; No need of iterator for -fPIC as it use got_lo12 for both modes."}, {"sha": "c9af4efba466f67736c7c01bdbe20f17f9af8b5e", "filename": "gcc/genmodes.c", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fgenmodes.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Fgenmodes.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgenmodes.c?ref=66f206b85395c273980e2b81a54dbddc4897e4a7", "patch": "@@ -749,12 +749,15 @@ make_partial_integer_mode (const char *base, const char *name,\n \n /* A single vector mode can be specified by naming its component\n    mode and the number of components.  */\n-#define VECTOR_MODE(C, M, N) \\\n-  make_vector_mode (MODE_##C, #M, N, __FILE__, __LINE__);\n+#define VECTOR_MODE_WITH_PREFIX(PREFIX, C, M, N, ORDER) \\\n+  make_vector_mode (MODE_##C, #PREFIX, #M, N, ORDER, __FILE__, __LINE__);\n+#define VECTOR_MODE(C, M, N) VECTOR_MODE_WITH_PREFIX(V, C, M, N, 0);\n static void ATTRIBUTE_UNUSED\n make_vector_mode (enum mode_class bclass,\n+\t\t  const char *prefix,\n \t\t  const char *base,\n \t\t  unsigned int ncomponents,\n+\t\t  unsigned int order,\n \t\t  const char *file, unsigned int line)\n {\n   struct mode_data *v;\n@@ -778,7 +781,7 @@ make_vector_mode (enum mode_class bclass,\n       return;\n     }\n \n-  if ((size_t)snprintf (namebuf, sizeof namebuf, \"V%u%s\",\n+  if ((size_t)snprintf (namebuf, sizeof namebuf, \"%s%u%s\", prefix,\n \t\t\tncomponents, base) >= sizeof namebuf)\n     {\n       error (\"%s:%d: mode name \\\"%s\\\" is too long\",\n@@ -787,6 +790,7 @@ make_vector_mode (enum mode_class bclass,\n     }\n \n   v = new_mode (vclass, xstrdup (namebuf), file, line);\n+  v->order = order;\n   v->ncomponents = ncomponents;\n   v->component = component;\n }"}, {"sha": "87b5fc36656326ecf0c6e39a0c5ffc881f3cdf74", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/bf16_vldN_lane_2.c", "status": "modified", "additions": 3, "deletions": 7, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_vldN_lane_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_vldN_lane_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_vldN_lane_2.c?ref=66f206b85395c273980e2b81a54dbddc4897e4a7", "patch": "@@ -17,36 +17,32 @@ test_vld2q_lane_bf16 (const bfloat16_t *ptr, bfloat16x8x2_t b)\n   return vld2q_lane_bf16 (ptr, b, 2);\n }\n \n-/* { dg-final { scan-assembler-times \"ld2\\\\t{v2.h - v3.h}\\\\\\[2\\\\\\], \\\\\\[x0\\\\\\]\" 2 } } */\n+/* { dg-final { scan-assembler-times \"ld2\\\\t{v\\[0-9\\]+.h - v\\[0-9\\]+.h}\\\\\\[2\\\\\\], \\\\\\[x0\\\\\\]\" 2 } } */\n \n bfloat16x4x3_t\n test_vld3_lane_bf16 (const bfloat16_t *ptr, bfloat16x4x3_t b)\n {\n   return vld3_lane_bf16 (ptr, b, 2);\n }\n \n-/* { dg-final { scan-assembler-times \"ld3\\t{v4.h - v6.h}\\\\\\[2\\\\\\], \\\\\\[x0\\\\\\]\" 1 } } */\n-\n bfloat16x8x3_t\n test_vld3q_lane_bf16 (const bfloat16_t *ptr, bfloat16x8x3_t b)\n {\n   return vld3q_lane_bf16 (ptr, b, 2);\n }\n \n-/* { dg-final { scan-assembler-times \"ld3\\t{v1.h - v3.h}\\\\\\[2\\\\\\], \\\\\\[x0\\\\\\]\" 1 } } */\n+/* { dg-final { scan-assembler-times \"ld3\\t{v\\[0-9\\]+.h - v\\[0-9\\]+.h}\\\\\\[2\\\\\\], \\\\\\[x0\\\\\\]\" 2 } } */\n \n bfloat16x4x4_t\n test_vld4_lane_bf16 (const bfloat16_t *ptr, bfloat16x4x4_t b)\n {\n   return vld4_lane_bf16 (ptr, b, 2);\n }\n \n-/* { dg-final { scan-assembler-times \"ld4\\t{v4.h - v7.h}\\\\\\[2\\\\\\], \\\\\\[x0\\\\\\]\" 1 } } */\n-\n bfloat16x8x4_t\n test_vld4q_lane_bf16 (const bfloat16_t *ptr, bfloat16x8x4_t b)\n {\n   return vld4q_lane_bf16 (ptr, b, 2);\n }\n \n-/* { dg-final { scan-assembler-times \"ld4\\t{v0.h - v3.h}\\\\\\[2\\\\\\], \\\\\\[x0\\\\\\]\" 1 } } */\n+/* { dg-final { scan-assembler-times \"ld4\\t{v\\[0-9\\]+.h - v\\[0-9\\]+.h}\\\\\\[2\\\\\\], \\\\\\[x0\\\\\\]\" 2 } } */"}, {"sha": "af0af29d5150275dbdbcd2ac3c871ff1efa391b6", "filename": "gcc/testsuite/gcc.target/aarch64/sve/pcs/struct_3_256.c", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpcs%2Fstruct_3_256.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/66f206b85395c273980e2b81a54dbddc4897e4a7/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpcs%2Fstruct_3_256.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpcs%2Fstruct_3_256.c?ref=66f206b85395c273980e2b81a54dbddc4897e4a7", "patch": "@@ -933,7 +933,11 @@ SEL2 (struct, nonpst1)\n \n /*\n ** test_nonpst1:\n+** (\n+**\tfmov\td0, d3\n+** |\n **\tmov\tv0\\.8b, v3\\.8b\n+** )\n **\tret\n */\n /* { dg-final { scan-assembler-not {\\t\\.variant_pcs\\ttest_nonpst1\\n} } } */"}]}