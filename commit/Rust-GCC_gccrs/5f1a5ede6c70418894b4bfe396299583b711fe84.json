{"sha": "5f1a5ede6c70418894b4bfe396299583b711fe84", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NWYxYTVlZGU2YzcwNDE4ODk0YjRiZmUzOTYyOTk1ODNiNzExZmU4NA==", "commit": {"author": {"name": "Paolo Carlini", "email": "pcarlini@suse.de", "date": "2004-06-08T22:19:18Z"}, "committer": {"name": "Paolo Carlini", "email": "paolo@gcc.gnu.org", "date": "2004-06-08T22:19:18Z"}, "message": "pool_allocator.h: Convert to a global free-list, as per the original SGI/HP design...\n\n2004-06-08  Paolo Carlini  <pcarlini@suse.de>\n\n\t* include/ext/pool_allocator.h: Convert to a global free-list,\n\tas per the original SGI/HP design: move the implementation\n\tdetails to struct __pool_base, from which __pool_alloc derives.\n\t* src/allocator.cc: Instantiate __pool_base.\n\nFrom-SVN: r82794", "tree": {"sha": "509c4ff49d44a6e76feffef71a465a4b031d5d87", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/509c4ff49d44a6e76feffef71a465a4b031d5d87"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/5f1a5ede6c70418894b4bfe396299583b711fe84", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5f1a5ede6c70418894b4bfe396299583b711fe84", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5f1a5ede6c70418894b4bfe396299583b711fe84", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5f1a5ede6c70418894b4bfe396299583b711fe84/comments", "author": null, "committer": null, "parents": [{"sha": "908d0773f08485a63523e21520b948fe91998279", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/908d0773f08485a63523e21520b948fe91998279", "html_url": "https://github.com/Rust-GCC/gccrs/commit/908d0773f08485a63523e21520b948fe91998279"}], "stats": {"total": 284, "additions": 153, "deletions": 131}, "files": [{"sha": "17afc04c8f0bded0be40375bb7d0250b2734997b", "filename": "libstdc++-v3/ChangeLog", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5f1a5ede6c70418894b4bfe396299583b711fe84/libstdc%2B%2B-v3%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5f1a5ede6c70418894b4bfe396299583b711fe84/libstdc%2B%2B-v3%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2FChangeLog?ref=5f1a5ede6c70418894b4bfe396299583b711fe84", "patch": "@@ -1,3 +1,10 @@\n+2004-06-08  Paolo Carlini  <pcarlini@suse.de>\n+\n+\t* include/ext/pool_allocator.h: Convert to a global free-list,\n+\tas per the original SGI/HP design: move the implementation\n+\tdetails to struct __pool_base, from which __pool_alloc derives.\n+\t* src/allocator.cc: Instantiate __pool_base.\n+\n 2004-06-07  Dhruv Matani  <dhruvbird@gmx.net>\n \t    Paolo Carlini  <pcarlini@suse.de>\n "}, {"sha": "2f0aec50362365afd124363e4775f65624c965bc", "filename": "libstdc++-v3/include/ext/pool_allocator.h", "status": "modified", "additions": 144, "deletions": 131, "changes": 275, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5f1a5ede6c70418894b4bfe396299583b711fe84/libstdc%2B%2B-v3%2Finclude%2Fext%2Fpool_allocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5f1a5ede6c70418894b4bfe396299583b711fe84/libstdc%2B%2B-v3%2Finclude%2Fext%2Fpool_allocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fext%2Fpool_allocator.h?ref=5f1a5ede6c70418894b4bfe396299583b711fe84", "patch": "@@ -71,11 +71,70 @@ namespace __gnu_cxx\n    *     information that we can return the object to the proper free list\n    *     without permanently losing part of the object.\n    *\n+   *  The template parameter specifies whether more than one thread may use\n+   *  this allocator.  It is safe to allocate an object from one instance\n+   *  of the allocator and deallocate it with another one.  This effectively\n+   *  transfers its ownership to the second one.  This may have undesirable\n+   *  effects on reference locality.\n+   *\n    *  @endif\n    *  (See @link Allocators allocators info @endlink for more.)\n    */\n+  template<bool __threads>\n+    struct __pool_base\n+    {\n+      enum { _S_align = 8 };\n+      enum { _S_max_bytes = 128 };\n+      enum { _S_freelists = _S_max_bytes / _S_align };\n+      \n+      union _Obj\n+      {\n+\tunion _Obj* _M_free_list_link;\n+\tchar        _M_client_data[1];    // The client sees this.\n+      };\n+      \n+      static _Obj* volatile         _S_free_list[_S_freelists];\n+      \n+      // Chunk allocation state.\n+      static char*                  _S_start_free;\n+      static char*                  _S_end_free;\n+      static size_t                 _S_heap_size;\n+      \n+      static _STL_mutex_lock        _S_lock;\n+      static _Atomic_word\t    _S_force_new;\n+      \n+      static size_t\n+      _S_round_up(size_t __bytes)\n+      { return ((__bytes + (size_t)_S_align - 1) & ~((size_t)_S_align - 1)); }\n+      \n+      static size_t\n+      _S_freelist_index(size_t __bytes)\n+      { return ((__bytes + (size_t)_S_align - 1) / (size_t)_S_align - 1); }\n+    \n+      // Returns an object of size __n, and optionally adds to size __n\n+      // free list.\n+      static void*\n+      _S_refill(size_t __n);\n+      \n+      // Allocates a chunk for nobjs of size size.  nobjs may be reduced\n+      // if it is inconvenient to allocate the requested number.\n+      static char*\n+      _S_chunk_alloc(size_t __n, int& __nobjs);\n+      \n+      // It would be nice to use _STL_auto_lock here.  But we need a\n+      // test whether threads are in use.\n+      struct _Lock\n+      {\n+\t_Lock() { if (__threads) _S_lock._M_acquire_lock(); }\n+\t~_Lock() { if (__threads) _S_lock._M_release_lock(); }\n+      } __attribute__ ((__unused__));\n+      friend struct _Lock;\n+    };\n+\n+  typedef __pool_base<true> __pool_alloc_base;\n+\n   template<typename _Tp>\n-    class __pool_alloc\n+    class __pool_alloc : private __pool_alloc_base\n     {\n     public:\n       typedef size_t     size_type;\n@@ -123,54 +182,6 @@ namespace __gnu_cxx\n \n       void\n       deallocate(pointer __p, size_type __n);      \n-\n-    private:\n-      enum {_S_align = 8};\n-      enum {_S_max_bytes = 128};\n-      enum {_S_freelists = _S_max_bytes / _S_align};\n-\n-      union _Obj\n-      {\n-        union _Obj* _M_free_list_link;\n-        char        _M_client_data[1];    // The client sees this.\n-      };\n-\n-      static _Obj* volatile         _S_free_list[_S_freelists];\n-\n-      // Chunk allocation state.\n-      static char*                  _S_start_free;\n-      static char*                  _S_end_free;\n-      static size_t                 _S_heap_size;\n-\n-      static _STL_mutex_lock        _S_lock;\n-      static _Atomic_word\t    _S_force_new;\n-\n-      static size_t\n-      _S_round_up(size_t __bytes)\n-      { return ((__bytes + (size_t)_S_align - 1) & ~((size_t)_S_align - 1)); }\n-\n-      static size_t\n-      _S_freelist_index(size_t __bytes)\n-      { return ((__bytes + (size_t)_S_align - 1)/(size_t)_S_align - 1); }\n-\n-      // Returns an object of size __n, and optionally adds to size __n\n-      // free list.\n-      static void*\n-      _S_refill(size_t __n);\n-\n-      // Allocates a chunk for nobjs of size size.  nobjs may be reduced\n-      // if it is inconvenient to allocate the requested number.\n-      static char*\n-      _S_chunk_alloc(size_t __n, int& __nobjs);\n-\n-      // It would be nice to use _STL_auto_lock here.  But we need a\n-      // test whether threads are in use.\n-      struct _Lock\n-      {\n-        _Lock() { _S_lock._M_acquire_lock(); }\n-        ~_Lock() { _S_lock._M_release_lock(); }\n-      } __attribute__ ((__unused__));\n-      friend struct _Lock;\n     };\n \n   template<typename _Tp>\n@@ -186,82 +197,83 @@ namespace __gnu_cxx\n   // Allocate memory in large chunks in order to avoid fragmenting the\n   // heap too much.  Assume that __n is properly aligned.  We hold\n   // the allocation lock.\n-  template<typename _Tp>\n+  template<bool __threads>\n     char*\n-    __pool_alloc<_Tp>::_S_chunk_alloc(size_t __n, int& __nobjs)\n+    __pool_base<__threads>::_S_chunk_alloc(size_t __n, int& __nobjs)\n     {\n       char* __result;\n       size_t __total_bytes = __n * __nobjs;\n       size_t __bytes_left = _S_end_free - _S_start_free;\n-\n+      \n       if (__bytes_left >= __total_bytes)\n-        {\n-          __result = _S_start_free;\n-          _S_start_free += __total_bytes;\n-          return __result ;\n-        }\n+\t{\n+\t  __result = _S_start_free;\n+\t  _S_start_free += __total_bytes;\n+\t  return __result ;\n+\t}\n       else if (__bytes_left >= __n)\n-        {\n-          __nobjs = (int)(__bytes_left/__n);\n-          __total_bytes = __n * __nobjs;\n-          __result = _S_start_free;\n-          _S_start_free += __total_bytes;\n-          return __result;\n-        }\n+\t{\n+\t  __nobjs = (int)(__bytes_left / __n);\n+\t  __total_bytes = __n * __nobjs;\n+\t  __result = _S_start_free;\n+\t  _S_start_free += __total_bytes;\n+\t  return __result;\n+\t}\n       else\n-        {\n-          size_t __bytes_to_get =\n-            2 * __total_bytes + _S_round_up(_S_heap_size >> 4);\n-          // Try to make use of the left-over piece.\n-          if (__bytes_left > 0)\n-            {\n-              _Obj* volatile* __free_list =\n-                _S_free_list + _S_freelist_index(__bytes_left);\n-\n-              ((_Obj*)(void*)_S_start_free)->_M_free_list_link = *__free_list;\n-              *__free_list = (_Obj*)(void*)_S_start_free;\n-            }\n-          _S_start_free = static_cast<char*>(::operator new(__bytes_to_get));\n-          if (_S_start_free == 0)\n-            {\n-              size_t __i;\n-              _Obj* volatile* __free_list;\n-              _Obj* __p;\n-              // Try to make do with what we have.  That can't hurt.  We\n-              // do not try smaller requests, since that tends to result\n-              // in disaster on multi-process machines.\n-              __i = __n;\n-              for (; __i <= (size_t) _S_max_bytes; __i += (size_t) _S_align)\n-                {\n-                  __free_list = _S_free_list + _S_freelist_index(__i);\n-                  __p = *__free_list;\n-                  if (__p != 0)\n-                    {\n-                      *__free_list = __p -> _M_free_list_link;\n-                      _S_start_free = (char*)__p;\n-                      _S_end_free = _S_start_free + __i;\n-                      return _S_chunk_alloc(__n, __nobjs);\n-                      // Any leftover piece will eventually make it to the\n-                      // right free list.\n-                    }\n-                }\n-              _S_end_free = 0;        // In case of exception.\n-              _S_start_free = static_cast<char*>(::operator new(__bytes_to_get));\n-              // This should either throw an exception or remedy the situation.\n-              // Thus we assume it succeeded.\n-            }\n-          _S_heap_size += __bytes_to_get;\n-          _S_end_free = _S_start_free + __bytes_to_get;\n-          return _S_chunk_alloc(__n, __nobjs);\n-        }\n+\t{\n+\t  size_t __bytes_to_get = (2 * __total_bytes\n+\t\t\t\t   + _S_round_up(_S_heap_size >> 4));\n+\t  // Try to make use of the left-over piece.\n+\t  if (__bytes_left > 0)\n+\t    {\n+\t      _Obj* volatile* __free_list = (_S_free_list\n+\t\t\t\t\t     + _S_freelist_index(__bytes_left));\n+\t      \n+\t      ((_Obj*)(void*)_S_start_free)->_M_free_list_link = *__free_list;\n+\t      *__free_list = (_Obj*)(void*)_S_start_free;\n+\t    }\n+\t  \n+\t  _S_start_free = static_cast<char*>(::operator new(__bytes_to_get));\n+\t  if (_S_start_free == 0)\n+\t    {\n+\t      size_t __i;\n+\t      _Obj* volatile* __free_list;\n+\t      _Obj* __p;\n+\t      // Try to make do with what we have.  That can't hurt.  We\n+\t      // do not try smaller requests, since that tends to result\n+\t      // in disaster on multi-process machines.\n+\t      __i = __n;\n+\t      for (; __i <= (size_t) _S_max_bytes; __i += (size_t) _S_align)\n+\t\t{\n+\t\t  __free_list = _S_free_list + _S_freelist_index(__i);\n+\t\t  __p = *__free_list;\n+\t\t  if (__p != 0)\n+\t\t    {\n+\t\t      *__free_list = __p -> _M_free_list_link;\n+\t\t      _S_start_free = (char*)__p;\n+\t\t      _S_end_free = _S_start_free + __i;\n+\t\t      return _S_chunk_alloc(__n, __nobjs);\n+\t\t      // Any leftover piece will eventually make it to the\n+\t\t      // right free list.\n+\t\t    }\n+\t\t}\n+\t      _S_end_free = 0;        // In case of exception.\n+\t      _S_start_free = static_cast<char*>(::operator new(__bytes_to_get));\n+\t      // This should either throw an exception or remedy the situation.\n+\t      // Thus we assume it succeeded.\n+\t    }\n+\t  _S_heap_size += __bytes_to_get;\n+\t  _S_end_free = _S_start_free + __bytes_to_get;\n+\t  return _S_chunk_alloc(__n, __nobjs);\n+\t}\n     }\n-\n+  \n   // Returns an object of size __n, and optionally adds to \"size\n   // __n\"'s free list.  We assume that __n is properly aligned.  We\n   // hold the allocation lock.\n-  template<typename _Tp>\n+  template<bool __threads>\n     void*\n-    __pool_alloc<_Tp>::_S_refill(size_t __n)\n+    __pool_base<__threads>::_S_refill(size_t __n)\n     {\n       int __nobjs = 20;\n       char* __chunk = _S_chunk_alloc(__n, __nobjs);\n@@ -270,18 +282,18 @@ namespace __gnu_cxx\n       _Obj* __current_obj;\n       _Obj* __next_obj;\n       int __i;\n-\n+      \n       if (1 == __nobjs)\n-        return __chunk;\n+\treturn __chunk;\n       __free_list = _S_free_list + _S_freelist_index(__n);\n-\n+      \n       // Build free list in chunk.\n       __result = (_Obj*)(void*)__chunk;\n       *__free_list = __next_obj = (_Obj*)(void*)(__chunk + __n);\n       for (__i = 1; ; __i++)\n-        {\n+\t{\n \t  __current_obj = __next_obj;\n-          __next_obj = (_Obj*)(void*)((char*)__next_obj + __n);\n+\t  __next_obj = (_Obj*)(void*)((char*)__next_obj + __n);\n \t  if (__nobjs - 1 == __i)\n \t    {\n \t      __current_obj -> _M_free_list_link = 0;\n@@ -329,7 +341,7 @@ namespace __gnu_cxx\n \t\t    __ret = static_cast<_Tp*>(_S_refill(_S_round_up(__bytes)));\n \t\t  else\n \t\t    {\n-\t\t      *__free_list = __result -> _M_free_list_link;\n+\t\t      *__free_list = __result->_M_free_list_link;\n \t\t      __ret = reinterpret_cast<_Tp*>(__result);\n \t\t    }\n \t\t  if (__builtin_expect(__ret == 0, 0))\n@@ -367,25 +379,26 @@ namespace __gnu_cxx\n \t}\n     }\n \n-  template<typename _Tp>\n-    typename __pool_alloc<_Tp>::_Obj* volatile\n-    __pool_alloc<_Tp>::_S_free_list[_S_freelists];\n+  template<bool __threads>\n+    typename __pool_base<__threads>::_Obj* volatile\n+    __pool_base<__threads>::_S_free_list[_S_freelists];\n \n-  template<typename _Tp>\n-    char* __pool_alloc<_Tp>::_S_start_free = 0;\n+  template<bool __threads>\n+    char* __pool_base<__threads>::_S_start_free = 0;\n \n-  template<typename _Tp>\n-    char* __pool_alloc<_Tp>::_S_end_free = 0;\n+  template<bool __threads>\n+    char* __pool_base<__threads>::_S_end_free = 0;\n \n-  template<typename _Tp>\n-    size_t __pool_alloc<_Tp>::_S_heap_size = 0;\n+  template<bool __threads>\n+    size_t __pool_base<__threads>::_S_heap_size = 0;\n \n-  template<typename _Tp>\n+  template<bool __threads>\n     _STL_mutex_lock\n-    __pool_alloc<_Tp>::_S_lock __STL_MUTEX_INITIALIZER;\n+    __pool_base<__threads>::_S_lock __STL_MUTEX_INITIALIZER;\n \n-  template<typename _Tp> _Atomic_word\n-  __pool_alloc<_Tp>::_S_force_new = 0;\n+  template<bool __threads>\n+    _Atomic_word\n+    __pool_base<__threads>::_S_force_new = 0;\n } // namespace __gnu_cxx\n \n #endif"}, {"sha": "3a0efedbc4037c1f999c15360c5bfdacb4440852", "filename": "libstdc++-v3/src/allocator.cc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5f1a5ede6c70418894b4bfe396299583b711fe84/libstdc%2B%2B-v3%2Fsrc%2Fallocator.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5f1a5ede6c70418894b4bfe396299583b711fe84/libstdc%2B%2B-v3%2Fsrc%2Fallocator.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Fsrc%2Fallocator.cc?ref=5f1a5ede6c70418894b4bfe396299583b711fe84", "patch": "@@ -46,4 +46,6 @@ namespace __gnu_cxx\n   // Static members of __pool_alloc.\n   template class __pool_alloc<char>;\n   template class __pool_alloc<wchar_t>;\n+\n+  template class __pool_base<true>;\n } // namespace __gnu_cxx"}]}