{"sha": "e2724e63c6d26b05a51a519f4715265e8c82419a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZTI3MjRlNjNjNmQyNmIwNWE1MWE1MTlmNDcxNTI2NWU4YzgyNDE5YQ==", "commit": {"author": {"name": "Bernd Schmidt", "email": "bernds@codesourcery.com", "date": "2011-10-21T13:35:44Z"}, "committer": {"name": "Bernd Schmidt", "email": "bernds@gcc.gnu.org", "date": "2011-10-21T13:35:44Z"}, "message": "reg-notes.def (DEP_CONTROL): New.\n\n\t* reg-notes.def (DEP_CONTROL): New.\n\t* sched-ebb.c (add_deps_for_risky_insns): Add a REG_DEP_CONTROL when\n\tnot doing speculation.\n\t* rtlanal.c (record_hard_reg_sets, find_all_hard_reg_sets,\n\trecord_hard_reg_uses_1, record_hard_reg_uses): New functions.\n\t* function.c (record_hard_reg_sets, record_hard_reg_uses,\n\trecord_hard_reg_uses_1): Remove; move to rtlanal.c.\n\t* lists.c (copy_INSN_LIST, concat_INSN_LIST): New functions.\n\t* haifa-sched.c: Swap includes of \"rtl.h\" and \"hard-reg-set.h\".\n\t(MUST_RECOMPUTE_SPEC_P): New macro.\n\t(real_insn_for_shadow): New function.\n\t(cond_clobbered_p, recompute_todo_spec, check_clobbered_conditions,\n\ttoggle_cancelled_flags): New static functions.\n\t(schedule_insn): Relax an assert to only check for empty hard back\n\tdependencies.  Skip cancelled dependencies.  Call\n\tcheck_clobbered_conditions.\n\t(copy_insn_list): Remove function, renamed moved to lists.c.\n\t(save_backtrack_point): Use new spelling copy_INSN_LIST.\n\t(unschedule_insns_until): Ensure TODO_SPEC is reset properly.\n\t(restore_last_backtrack_point): Likewise.  Call toggle_cancelled_flags.\n\t(estimate_insn_tick): Ignore cancelled dependencies.\n\t(haifa_speculate_insn): Move declaration.\n\t(try_ready): Move code into recompute_todo_spec and call it.  Tweak\n\tsome asserts.  Ensure predicated patterns are restored if necessary.\n\tDump DEP_CONTROL flag.\n\t(haifa_change_pattern): Merge with sched_change_pattern.\n\t(sched_change_pattern): Remove function.\n\t* sched-deps.c (NON_FLUSH_JUMP_KIND, NON_FLUSH_JUMP): Remove.  All\n\tuses changed to simply not test NON_FLUSH_JUMP_P.\n\t(ds_to_dk, dk_to_ds, dump_dep, ds_to_dt, dump_ds, check_dep): Handle\n\tREG_DEP_CONTROL.\n\t(dep_spec_p): If DO_PREDICATION, REG_DEP_CONTROL is speculative.\n\t(reg_pending_control_uses, control_dependency_cache): New static\n\tvariables.\n\t(sched_get_reverse_condition_uncached): New function.\n\t(sd_find_dep_between): Remove pointless assert.  Look in\n\tcontrol_dependency_cache.\n\t(ask_dependency_caches, set_dependency_caches, sd_delete_dep,\n\textend_dependency_caches, sched_deps_finish): Handle REG_DEP_CONTROL\n\tand control_dependency_cache.\n\t(sd_unresolve_dep): Use dep_spec_p.\n\t(add_dependence): Now a wrapper around add_dependence_1, handling\n\tREG_DEP_CONTROL specially.\n\t(flush_pending_lists): Clear pending_jump_insns.\n\t(sched_analyze_1): Handle pending_jump_insns like a memory flush.\n\t(sched_analyze_2): Unconditionally add to pending memory flushes,\n\tkeep previous behaviour but apply it to pending_jump_insns instead.\n\t(sched_analyze_insn): Defer adding jump reg dependencies using\n\treg_pending_control_uses; add them to the control_uses list.  Handle\n\tpending_jump_insns and control_uses when adding dependence lists.\n\t(deps_analyze_insn): Update INSN_COND_DEPS.\n\t(deps_analyze_insn): Add jumps to pending_jump_insns rather than\n\tlast_pending_memory_flush.\n\t(init_deps): Initialize pending_jump_insns.\n\t(free_deps): Free control_uses.\n\t(remove_from_deps): Remove from pending_jump_insns.\n\t(init_deps_global): Allocate reg_pending_control_uses).\n\t(finish_deps_global): Free it.\n\t(add_dependence_1): Renamed from add_dependence.  Handle\n\tREG_DEP_CONTROL.\n\t* rtl.h (record_hard_reg_uses, find_all_hard_reg_sets): Declare.\n\t(copy_INSN_LIST, concat_INSN_LIST): Declare.\n\t* sched-int.h (struct deps_reg): Add control_uses.\n\t(struct deps_desc): Add pending_jump_insns.\n\t(struct _haifa_deps_insn_data): Add cond_deps.\n\t(struct _haifa_insn_data): Add must_recompute_spec and predicated_pat.\n\t(INSN_COND_DEPS, PREDICATED_PAT): New macros.\n\t(BITS_PER_DEP_WEAK): Adjust for two extra bits in the word.\n\t(DEP_CONTROL): New macro.\n\t(DEP_TYPES): Include it.\n\t(HARD_DEP): Adjust definition.\n\t(DEP_CANCELLED): New macro.\n\t(enum SCHED_FLAGS): Add DO_PREDICATION.\n\t(sched_get_reverse_condition_uncached, real_insn_for_shadow): Declare.\n\t* sched-rgn.c (concat_INSN_LIST): Remove function.\n\t(deps_join): Handle pending_jump_insns.\n\t(free_pending_lists): Likewise.\n\t* config/c6x/c6x.c (c6x_set_sched_flags): Set DO_PREDICATION for final\n\tschedule.\n\nFrom-SVN: r180302", "tree": {"sha": "1c0a8e176ba35a8459d7c1186a3754909d829c2b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1c0a8e176ba35a8459d7c1186a3754909d829c2b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/e2724e63c6d26b05a51a519f4715265e8c82419a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e2724e63c6d26b05a51a519f4715265e8c82419a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e2724e63c6d26b05a51a519f4715265e8c82419a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e2724e63c6d26b05a51a519f4715265e8c82419a/comments", "author": null, "committer": null, "parents": [{"sha": "b9af306b979306215b1f999ca2636a0874717c9c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b9af306b979306215b1f999ca2636a0874717c9c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b9af306b979306215b1f999ca2636a0874717c9c"}], "stats": {"total": 1077, "additions": 836, "deletions": 241}, "files": [{"sha": "16da4d66a0f3039407ad9b29de57e1515d2f738f", "filename": "gcc/ChangeLog", "status": "modified", "additions": 82, "deletions": 0, "changes": 82, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -1,3 +1,85 @@\n+2011-10-21  Bernd Schmidt  <bernds@codesourcery.com>\n+\n+\t* reg-notes.def (DEP_CONTROL): New.\n+\t* sched-ebb.c (add_deps_for_risky_insns): Add a REG_DEP_CONTROL when\n+\tnot doing speculation.\n+\t* rtlanal.c (record_hard_reg_sets, find_all_hard_reg_sets,\n+\trecord_hard_reg_uses_1, record_hard_reg_uses): New functions.\n+\t* function.c (record_hard_reg_sets, record_hard_reg_uses,\n+\trecord_hard_reg_uses_1): Remove; move to rtlanal.c.\n+\t* lists.c (copy_INSN_LIST, concat_INSN_LIST): New functions.\n+\t* haifa-sched.c: Swap includes of \"rtl.h\" and \"hard-reg-set.h\".\n+\t(MUST_RECOMPUTE_SPEC_P): New macro.\n+\t(real_insn_for_shadow): New function.\n+\t(cond_clobbered_p, recompute_todo_spec, check_clobbered_conditions,\n+\ttoggle_cancelled_flags): New static functions.\n+\t(schedule_insn): Relax an assert to only check for empty hard back\n+\tdependencies.  Skip cancelled dependencies.  Call\n+\tcheck_clobbered_conditions.\n+\t(copy_insn_list): Remove function, renamed moved to lists.c.\n+\t(save_backtrack_point): Use new spelling copy_INSN_LIST.\n+\t(unschedule_insns_until): Ensure TODO_SPEC is reset properly.\n+\t(restore_last_backtrack_point): Likewise.  Call toggle_cancelled_flags.\n+\t(estimate_insn_tick): Ignore cancelled dependencies.\n+\t(haifa_speculate_insn): Move declaration.\n+\t(try_ready): Move code into recompute_todo_spec and call it.  Tweak\n+\tsome asserts.  Ensure predicated patterns are restored if necessary.\n+\tDump DEP_CONTROL flag.\n+\t(haifa_change_pattern): Merge with sched_change_pattern.\n+\t(sched_change_pattern): Remove function.\n+\t* sched-deps.c (NON_FLUSH_JUMP_KIND, NON_FLUSH_JUMP): Remove.  All\n+\tuses changed to simply not test NON_FLUSH_JUMP_P.\n+\t(ds_to_dk, dk_to_ds, dump_dep, ds_to_dt, dump_ds, check_dep): Handle\n+\tREG_DEP_CONTROL.\n+\t(dep_spec_p): If DO_PREDICATION, REG_DEP_CONTROL is speculative.\n+\t(reg_pending_control_uses, control_dependency_cache): New static\n+\tvariables.\n+\t(sched_get_reverse_condition_uncached): New function.\n+\t(sd_find_dep_between): Remove pointless assert.  Look in\n+\tcontrol_dependency_cache.\n+\t(ask_dependency_caches, set_dependency_caches, sd_delete_dep,\n+\textend_dependency_caches, sched_deps_finish): Handle REG_DEP_CONTROL\n+\tand control_dependency_cache.\n+\t(sd_unresolve_dep): Use dep_spec_p.\n+\t(add_dependence): Now a wrapper around add_dependence_1, handling\n+\tREG_DEP_CONTROL specially.\n+\t(flush_pending_lists): Clear pending_jump_insns.\n+\t(sched_analyze_1): Handle pending_jump_insns like a memory flush.\n+\t(sched_analyze_2): Unconditionally add to pending memory flushes,\n+\tkeep previous behaviour but apply it to pending_jump_insns instead.\n+\t(sched_analyze_insn): Defer adding jump reg dependencies using\n+\treg_pending_control_uses; add them to the control_uses list.  Handle\n+\tpending_jump_insns and control_uses when adding dependence lists.\n+\t(deps_analyze_insn): Update INSN_COND_DEPS.\n+\t(deps_analyze_insn): Add jumps to pending_jump_insns rather than\n+\tlast_pending_memory_flush.\n+\t(init_deps): Initialize pending_jump_insns.\n+\t(free_deps): Free control_uses.\n+\t(remove_from_deps): Remove from pending_jump_insns.\n+\t(init_deps_global): Allocate reg_pending_control_uses).\n+\t(finish_deps_global): Free it.\n+\t(add_dependence_1): Renamed from add_dependence.  Handle\n+\tREG_DEP_CONTROL.\n+\t* rtl.h (record_hard_reg_uses, find_all_hard_reg_sets): Declare.\n+\t(copy_INSN_LIST, concat_INSN_LIST): Declare.\n+\t* sched-int.h (struct deps_reg): Add control_uses.\n+\t(struct deps_desc): Add pending_jump_insns.\n+\t(struct _haifa_deps_insn_data): Add cond_deps.\n+\t(struct _haifa_insn_data): Add must_recompute_spec and predicated_pat.\n+\t(INSN_COND_DEPS, PREDICATED_PAT): New macros.\n+\t(BITS_PER_DEP_WEAK): Adjust for two extra bits in the word.\n+\t(DEP_CONTROL): New macro.\n+\t(DEP_TYPES): Include it.\n+\t(HARD_DEP): Adjust definition.\n+\t(DEP_CANCELLED): New macro.\n+\t(enum SCHED_FLAGS): Add DO_PREDICATION.\n+\t(sched_get_reverse_condition_uncached, real_insn_for_shadow): Declare.\n+\t* sched-rgn.c (concat_INSN_LIST): Remove function.\n+\t(deps_join): Handle pending_jump_insns.\n+\t(free_pending_lists): Likewise.\n+\t* config/c6x/c6x.c (c6x_set_sched_flags): Set DO_PREDICATION for final\n+\tschedule.\n+\n 2011-10-21  Georg-Johann Lay  <avr@gjlay.de>\n \n \tPR target/50820"}, {"sha": "6712dea4a1440ad66311c446c30f71211c8b882b", "filename": "gcc/config/c6x/c6x.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fconfig%2Fc6x%2Fc6x.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fconfig%2Fc6x%2Fc6x.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fc6x%2Fc6x.c?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -3927,7 +3927,7 @@ c6x_set_sched_flags (spec_info_t spec_info)\n \n   if (*flags & SCHED_EBB)\n     {\n-      *flags |= DO_BACKTRACKING;\n+      *flags |= DO_BACKTRACKING | DO_PREDICATION;\n     }\n \n   spec_info->mask = 0;"}, {"sha": "a9c7d8b3b84791a75e2ffe0ae55f6df80b0f9e84", "filename": "gcc/function.c", "status": "modified", "additions": 0, "deletions": 30, "changes": 30, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Ffunction.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Ffunction.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffunction.c?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -2892,17 +2892,6 @@ assign_parm_setup_block (struct assign_parm_data_all *all,\n   SET_DECL_RTL (parm, stack_parm);\n }\n \n-/* A subroutine of assign_parm_setup_reg, called through note_stores.\n-   This collects sets and clobbers of hard registers in a HARD_REG_SET,\n-   which is pointed to by DATA.  */\n-static void\n-record_hard_reg_sets (rtx x, const_rtx pat ATTRIBUTE_UNUSED, void *data)\n-{\n-  HARD_REG_SET *pset = (HARD_REG_SET *)data;\n-  if (REG_P (x) && HARD_REGISTER_P (x))\n-    add_to_hard_reg_set (pset, GET_MODE (x), REGNO (x));\n-}\n-\n /* A subroutine of assign_parms.  Allocate a pseudo to hold the current\n    parameter.  Get it there.  Perform all ABI specified conversions.  */\n \n@@ -5289,25 +5278,6 @@ prologue_epilogue_contains (const_rtx insn)\n \n #ifdef HAVE_simple_return\n \n-/* A for_each_rtx subroutine of record_hard_reg_sets.  */\n-static int\n-record_hard_reg_uses_1 (rtx *px, void *data)\n-{\n-  rtx x = *px;\n-  HARD_REG_SET *pused = (HARD_REG_SET *)data;\n-\n-  if (REG_P (x) && REGNO (x) < FIRST_PSEUDO_REGISTER)\n-    add_to_hard_reg_set (pused, GET_MODE (x), REGNO (x));\n-  return 0;\n-}\n-\n-/* Like record_hard_reg_sets, but called through note_uses.  */\n-static void\n-record_hard_reg_uses (rtx *px, void *data)\n-{\n-  for_each_rtx (px, record_hard_reg_uses_1, data);\n-}\n-\n /* Return true if INSN requires the stack frame to be set up.\n    PROLOGUE_USED contains the hard registers used in the function\n    prologue.  SET_UP_BY_PROLOGUE is the set of registers we expect the"}, {"sha": "d87a608b901fff5d73ecb9455300567c9ed0ede5", "filename": "gcc/haifa-sched.c", "status": "modified", "additions": 381, "deletions": 98, "changes": 479, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fhaifa-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fhaifa-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fhaifa-sched.c?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -129,9 +129,9 @@ along with GCC; see the file COPYING3.  If not see\n #include \"coretypes.h\"\n #include \"tm.h\"\n #include \"diagnostic-core.h\"\n+#include \"hard-reg-set.h\"\n #include \"rtl.h\"\n #include \"tm_p.h\"\n-#include \"hard-reg-set.h\"\n #include \"regs.h\"\n #include \"function.h\"\n #include \"flags.h\"\n@@ -213,6 +213,7 @@ struct common_sched_info_def *common_sched_info;\n #define INTER_TICK(INSN) (HID (INSN)->inter_tick)\n #define FEEDS_BACKTRACK_INSN(INSN) (HID (INSN)->feeds_backtrack_insn)\n #define SHADOW_P(INSN) (HID (INSN)->shadow_p)\n+#define MUST_RECOMPUTE_SPEC_P(INSN) (HID (INSN)->must_recompute_spec)\n \n /* If INSN_TICK of an instruction is equal to INVALID_TICK,\n    then it should be recalculated from scratch.  */\n@@ -706,6 +707,24 @@ record_delay_slot_pair (rtx i1, rtx i2, int cycles, int stages)\n   *slot = p;\n }\n \n+/* Examine the delay pair hashtable to see if INSN is a shadow for another,\n+   and return the other insn if so.  Return NULL otherwise.  */\n+rtx\n+real_insn_for_shadow (rtx insn)\n+{\n+  struct delay_pair *pair;\n+\n+  if (delay_htab == NULL)\n+    return NULL_RTX;\n+\n+  pair\n+    = (struct delay_pair *)htab_find_with_hash (delay_htab_i2, insn,\n+\t\t\t\t\t\thtab_hash_pointer (insn));\n+  if (!pair || pair->stages > 0)\n+    return NULL_RTX;\n+  return pair->i1;\n+}\n+\n /* For a pair P of insns, return the fixed distance in cycles from the first\n    insn after which the second must be scheduled.  */\n static int\n@@ -820,14 +839,15 @@ static void change_queue_index (rtx, int);\n \n static void extend_h_i_d (void);\n static void init_h_i_d (rtx);\n+static int haifa_speculate_insn (rtx, ds_t, rtx *);\n static void generate_recovery_code (rtx);\n static void process_insn_forw_deps_be_in_spec (rtx, rtx, ds_t);\n static void begin_speculative_block (rtx);\n static void add_to_speculative_block (rtx);\n static void init_before_recovery (basic_block *);\n static void create_check_block_twin (rtx, bool);\n static void fix_recovery_deps (basic_block);\n-static void haifa_change_pattern (rtx, rtx);\n+static bool haifa_change_pattern (rtx, rtx);\n static void dump_new_block_header (int, basic_block, rtx, rtx);\n static void restore_bb_notes (basic_block);\n static void fix_jump_move (rtx);\n@@ -1056,7 +1076,178 @@ print_curr_reg_pressure (void)\n     }\n   fprintf (sched_dump, \"\\n\");\n }\n+\f\n+/* Determine if INSN has a condition that is clobbered if a register\n+   in SET_REGS is modified.  */\n+static bool\n+cond_clobbered_p (rtx insn, HARD_REG_SET set_regs)\n+{\n+  rtx pat = PATTERN (insn);\n+  gcc_assert (GET_CODE (pat) == COND_EXEC);\n+  if (TEST_HARD_REG_BIT (set_regs, REGNO (XEXP (COND_EXEC_TEST (pat), 0))))\n+    {\n+      sd_iterator_def sd_it;\n+      dep_t dep;\n+      haifa_change_pattern (insn, ORIG_PAT (insn));\n+      FOR_EACH_DEP (insn, SD_LIST_BACK, sd_it, dep)\n+\tDEP_STATUS (dep) &= ~DEP_CANCELLED;\n+      TODO_SPEC (insn) = HARD_DEP;\n+      if (sched_verbose >= 2)\n+\tfprintf (sched_dump,\n+\t\t \";;\\t\\tdequeue insn %s because of clobbered condition\\n\",\n+\t\t (*current_sched_info->print_insn) (insn, 0));\n+      return true;\n+    }\n+\n+  return false;\n+}\n+\n+/* Look at the remaining dependencies for insn NEXT, and compute and return\n+   the TODO_SPEC value we should use for it.  This is called after one of\n+   NEXT's dependencies has been resolved.  */\n+\n+static ds_t\n+recompute_todo_spec (rtx next)\n+{\n+  ds_t new_ds;\n+  sd_iterator_def sd_it;\n+  dep_t dep, control_dep = NULL;\n+  int n_spec = 0;\n+  int n_control = 0;\n+  bool first_p = true;\n+\n+  if (sd_lists_empty_p (next, SD_LIST_BACK))\n+    /* NEXT has all its dependencies resolved.  */\n+    return 0;\n+\n+  if (!sd_lists_empty_p (next, SD_LIST_HARD_BACK))\n+    return HARD_DEP;\n+\n+  /* Now we've got NEXT with speculative deps only.\n+     1. Look at the deps to see what we have to do.\n+     2. Check if we can do 'todo'.  */\n+  new_ds = 0;\n+\n+  FOR_EACH_DEP (next, SD_LIST_BACK, sd_it, dep)\n+    {\n+      ds_t ds = DEP_STATUS (dep) & SPECULATIVE;\n+\n+      if (DEBUG_INSN_P (DEP_PRO (dep)) && !DEBUG_INSN_P (next))\n+\tcontinue;\n+\n+      if (ds)\n+\t{\n+\t  n_spec++;\n+\t  if (first_p)\n+\t    {\n+\t      first_p = false;\n+\n+\t      new_ds = ds;\n+\t    }\n+\t  else\n+\t    new_ds = ds_merge (new_ds, ds);\n+\t}\n+      if (DEP_TYPE (dep) == REG_DEP_CONTROL)\n+\t{\n+\t  n_control++;\n+\t  control_dep = dep;\n+\t  DEP_STATUS (dep) &= ~DEP_CANCELLED;\n+\t}\n+    }\n+\n+  if (n_control == 1 && n_spec == 0)\n+    {\n+      rtx pro, other, new_pat;\n+      rtx cond = NULL_RTX;\n+      bool success;\n+      rtx prev = NULL_RTX;\n+      int i;\n+      unsigned regno;\n+  \n+      if ((current_sched_info->flags & DO_PREDICATION) == 0\n+\t  || (ORIG_PAT (next) != NULL_RTX\n+\t      && PREDICATED_PAT (next) == NULL_RTX))\n+\treturn HARD_DEP;\n+\n+      pro = DEP_PRO (control_dep);\n+      other = real_insn_for_shadow (pro);\n+      if (other != NULL_RTX)\n+\tpro = other;\n+\n+      cond = sched_get_reverse_condition_uncached (pro);\n+      regno = REGNO (XEXP (cond, 0));\n+\n+      /* Find the last scheduled insn that modifies the condition register.\n+\t If we have a true dependency on it, it sets it to the correct value,\n+\t otherwise it must be a later insn scheduled in-between that clobbers\n+\t the condition.  */\n+      FOR_EACH_VEC_ELT_REVERSE (rtx, scheduled_insns, i, prev)\n+\t{\n+\t  sd_iterator_def sd_it;\n+\t  dep_t dep;\n+\t  HARD_REG_SET t;\n+\t  bool found;\n+\n+\t  find_all_hard_reg_sets (prev, &t);\n+\t  if (!TEST_HARD_REG_BIT (t, regno))\n+\t    continue;\n+\n+\t  found = false;\n+\t  FOR_EACH_DEP (next, SD_LIST_RES_BACK, sd_it, dep)\n+\t    {\n+\t      if (DEP_PRO (dep) == prev && DEP_TYPE (dep) == REG_DEP_TRUE)\n+\t\t{\n+\t\t  found = true;\n+\t\t  break;\n+\t\t}\n+\t    }\n+\t  if (!found)\n+\t    return HARD_DEP;\n+\t  break;\n+\t}\n+      if (ORIG_PAT (next) == NULL_RTX)\n+\t{\n+\t  ORIG_PAT (next) = PATTERN (next);\n \n+\t  new_pat = gen_rtx_COND_EXEC (VOIDmode, cond, PATTERN (next));\n+\t  success = haifa_change_pattern (next, new_pat);\n+\t  if (!success)\n+\t    return HARD_DEP;\n+\t  PREDICATED_PAT (next) = new_pat;\n+\t}\n+      else if (PATTERN (next) != PREDICATED_PAT (next))\n+\t{\n+\t  bool success = haifa_change_pattern (next,\n+\t\t\t\t\t       PREDICATED_PAT (next));\n+\t  gcc_assert (success);\n+\t}\n+      DEP_STATUS (control_dep) |= DEP_CANCELLED;\n+      return DEP_CONTROL;\n+    }\n+\n+  if (PREDICATED_PAT (next) != NULL_RTX)\n+    {\n+      int tick = INSN_TICK (next);\n+      bool success = haifa_change_pattern (next,\n+\t\t\t\t\t   ORIG_PAT (next));\n+      INSN_TICK (next) = tick;\n+      gcc_assert (success);\n+    }\n+\n+  /* We can't handle the case where there are both speculative and control\n+     dependencies, so we return HARD_DEP in such a case.  Also fail if\n+     we have speculative dependencies with not enough points, or more than\n+     one control dependency.  */\n+  if ((n_spec > 0 && n_control > 0)\n+      || (n_spec > 0\n+\t  /* Too few points?  */\n+\t  && ds_weak (new_ds) < spec_info->data_weakness_cutoff)\n+      || (n_control > 1))\n+    return HARD_DEP;\n+\n+  return new_ds;\n+}\n+\f\n /* Pointer to the last instruction scheduled.  */\n static rtx last_scheduled_insn;\n \n@@ -1963,6 +2154,51 @@ sched_setup_bb_reg_pressure_info (basic_block bb, rtx after)\n   setup_insn_max_reg_pressure (after, false);\n }\n \f\n+/* If doing predication while scheduling, verify whether INSN, which\n+   has just been scheduled, clobbers the conditions of any\n+   instructions that must be predicated in order to break their\n+   dependencies.  If so, remove them from the queues so that they will\n+   only be scheduled once their control dependency is resolved.  */\n+\n+static void\n+check_clobbered_conditions (rtx insn)\n+{\n+  HARD_REG_SET t;\n+  int i;\n+\n+  if ((current_sched_info->flags & DO_PREDICATION) == 0)\n+    return;\n+\n+  find_all_hard_reg_sets (insn, &t);\n+\n+ restart:\n+  for (i = 0; i < ready.n_ready; i++)\n+    {\n+      rtx x = ready_element (&ready, i);\n+      if (TODO_SPEC (x) == DEP_CONTROL && cond_clobbered_p (x, t))\n+\t{\n+\t  ready_remove_insn (x);\n+\t  goto restart;\n+\t}\n+    }\n+  for (i = 0; i <= max_insn_queue_index; i++)\n+    {\n+      rtx link;\n+      int q = NEXT_Q_AFTER (q_ptr, i);\n+\n+    restart_queue:\n+      for (link = insn_queue[q]; link; link = XEXP (link, 1))\n+\t{\n+\t  rtx x = XEXP (link, 0);\n+\t  if (TODO_SPEC (x) == DEP_CONTROL && cond_clobbered_p (x, t))\n+\t    {\n+\t      queue_remove (x);\n+\t      goto restart_queue;\n+\t    }\n+\t}\n+    }\n+}\n+\f\n /* A structure that holds local state for the loop in schedule_block.  */\n struct sched_block_state\n {\n@@ -2023,7 +2259,7 @@ schedule_insn (rtx insn)\n \n   /* Scheduling instruction should have all its dependencies resolved and\n      should have been removed from the ready list.  */\n-  gcc_assert (sd_lists_empty_p (insn, SD_LIST_BACK));\n+  gcc_assert (sd_lists_empty_p (insn, SD_LIST_HARD_BACK));\n \n   /* Reset debug insns invalidated by moving this insn.  */\n   if (MAY_HAVE_DEBUG_INSNS && !DEBUG_INSN_P (insn))\n@@ -2033,6 +2269,12 @@ schedule_insn (rtx insn)\n \trtx dbg = DEP_PRO (dep);\n \tstruct reg_use_data *use, *next;\n \n+\tif (DEP_STATUS (dep) & DEP_CANCELLED)\n+\t  {\n+\t    sd_iterator_next (&sd_it);\n+\t    continue;\n+\t  }\n+\n \tgcc_assert (DEBUG_INSN_P (dbg));\n \n \tif (sched_verbose >= 6)\n@@ -2086,17 +2328,36 @@ schedule_insn (rtx insn)\n      INSN_TICK untouched.  This is a machine-dependent issue, actually.  */\n   INSN_TICK (insn) = clock_var;\n \n+  check_clobbered_conditions (insn);\n+\n   /* Update dependent instructions.  */\n   for (sd_it = sd_iterator_start (insn, SD_LIST_FORW);\n        sd_iterator_cond (&sd_it, &dep);)\n     {\n       rtx next = DEP_CON (dep);\n+      bool cancelled = (DEP_STATUS (dep) & DEP_CANCELLED) != 0;\n \n       /* Resolve the dependence between INSN and NEXT.\n \t sd_resolve_dep () moves current dep to another list thus\n \t advancing the iterator.  */\n       sd_resolve_dep (sd_it);\n \n+      if (cancelled)\n+\t{\n+\t  if (QUEUE_INDEX (next) != QUEUE_SCHEDULED)\n+\t    {\n+\t      int tick = INSN_TICK (next);\n+\t      gcc_assert (ORIG_PAT (next) != NULL_RTX);\n+\t      haifa_change_pattern (next, ORIG_PAT (next));\n+\t      INSN_TICK (next) = tick;\n+\t      if (sd_lists_empty_p (next, SD_LIST_BACK))\n+\t\tTODO_SPEC (next) = 0;\n+\t      else if (!sd_lists_empty_p (next, SD_LIST_HARD_BACK))\n+\t\tTODO_SPEC (next) = HARD_DEP;\n+\t    }\n+\t  continue;\n+\t}\n+\n       /* Don't bother trying to mark next as ready if insn is a debug\n \t insn.  If insn is the last hard dependency, it will have\n \t already been discounted.  */\n@@ -2270,24 +2531,6 @@ mark_backtrack_feeds (rtx insn, int set_p)\n     }\n }\n \n-/* Make a copy of the INSN_LIST list LINK and return it.  */\n-static rtx\n-copy_insn_list (rtx link)\n-{\n-  rtx new_queue;\n-  rtx *pqueue = &new_queue;\n-\n-  for (; link; link = XEXP (link, 1))\n-    {\n-      rtx x = XEXP (link, 0);\n-      rtx newlink = alloc_INSN_LIST (x, NULL);\n-      *pqueue = newlink;\n-      pqueue = &XEXP (newlink, 1);\n-    }\n-  *pqueue = NULL_RTX;\n-  return new_queue;\n-}\n-\n /* Save the current scheduler state so that we can backtrack to it\n    later if necessary.  PAIR gives the insns that make it necessary to\n    save this point.  SCHED_BLOCK is the local state of schedule_block\n@@ -2314,7 +2557,7 @@ save_backtrack_point (struct delay_pair *pair,\n   for (i = 0; i <= max_insn_queue_index; i++)\n     {\n       int q = NEXT_Q_AFTER (q_ptr, i);\n-      save->insn_queue[i] = copy_insn_list (insn_queue[q]);\n+      save->insn_queue[i] = copy_INSN_LIST (insn_queue[q]);\n     }\n \n   save->clock_var = clock_var;\n@@ -2351,13 +2594,62 @@ save_backtrack_point (struct delay_pair *pair,\n     }\n }\n \n+/* Walk the ready list and all queues. If any insns have unresolved backwards\n+   dependencies, these must be cancelled deps, broken by predication.  Set or\n+   clear (depending on SET) the DEP_CANCELLED bit in DEP_STATUS.  */\n+\n+static void\n+toggle_cancelled_flags (bool set)\n+{\n+  int i;\n+  sd_iterator_def sd_it;\n+  dep_t dep;\n+\n+  if (ready.n_ready > 0)\n+    {\n+      rtx *first = ready_lastpos (&ready);\n+      for (i = 0; i < ready.n_ready; i++)\n+\tFOR_EACH_DEP (first[i], SD_LIST_BACK, sd_it, dep)\n+\t  if (!DEBUG_INSN_P (DEP_PRO (dep)))\n+\t    {\n+\t      if (set)\n+\t\tDEP_STATUS (dep) |= DEP_CANCELLED;\n+\t      else\n+\t\tDEP_STATUS (dep) &= ~DEP_CANCELLED;\n+\t    }\n+    }\n+  for (i = 0; i <= max_insn_queue_index; i++)\n+    {\n+      int q = NEXT_Q_AFTER (q_ptr, i);\n+      rtx link;\n+      for (link = insn_queue[q]; link; link = XEXP (link, 1))\n+\t{\n+\t  rtx insn = XEXP (link, 0);\n+\t  FOR_EACH_DEP (insn, SD_LIST_BACK, sd_it, dep)\n+\t    if (!DEBUG_INSN_P (DEP_PRO (dep)))\n+\t      {\n+\t\tif (set)\n+\t\t  DEP_STATUS (dep) |= DEP_CANCELLED;\n+\t\telse\n+\t\t  DEP_STATUS (dep) &= ~DEP_CANCELLED;\n+\t      }\n+\t}\n+    }\n+}\n+\n /* Pop entries from the SCHEDULED_INSNS vector up to and including INSN.\n    Restore their dependencies to an unresolved state, and mark them as\n    queued nowhere.  */\n \n static void\n unschedule_insns_until (rtx insn)\n {\n+  VEC (rtx, heap) *recompute_vec;\n+\n+  recompute_vec = VEC_alloc (rtx, heap, 0);\n+\n+  /* Make two passes over the insns to be unscheduled.  First, we clear out\n+     dependencies and other trivial bookkeeping.  */\n   for (;;)\n     {\n       rtx last;\n@@ -2379,14 +2671,40 @@ unschedule_insns_until (rtx insn)\n \t   sd_iterator_cond (&sd_it, &dep);)\n \t{\n \t  rtx con = DEP_CON (dep);\n-\t  TODO_SPEC (con) = HARD_DEP;\n-\t  INSN_TICK (con) = INVALID_TICK;\n \t  sd_unresolve_dep (sd_it);\n+\t  if (!MUST_RECOMPUTE_SPEC_P (con))\n+\t    {\n+\t      MUST_RECOMPUTE_SPEC_P (con) = 1;\n+\t      VEC_safe_push (rtx, heap, recompute_vec, con);\n+\t    }\n \t}\n \n       if (last == insn)\n \tbreak;\n     }\n+\n+  /* A second pass, to update ready and speculation status for insns\n+     depending on the unscheduled ones.  The first pass must have\n+     popped the scheduled_insns vector up to the point where we\n+     restart scheduling, as recompute_todo_spec requires it to be\n+     up-to-date.  */\n+  while (!VEC_empty (rtx, recompute_vec))\n+    {\n+      rtx con;\n+\n+      con = VEC_pop (rtx, recompute_vec);\n+      MUST_RECOMPUTE_SPEC_P (con) = 0;\n+      if (!sd_lists_empty_p (con, SD_LIST_HARD_BACK))\n+\t{\n+\t  TODO_SPEC (con) = HARD_DEP;\n+\t  INSN_TICK (con) = INVALID_TICK;\n+\t  if (PREDICATED_PAT (con) != NULL_RTX)\n+\t    haifa_change_pattern (con, ORIG_PAT (con));\n+\t}\n+      else if (QUEUE_INDEX (con) != QUEUE_SCHEDULED)\n+\tTODO_SPEC (con) = recompute_todo_spec (con);\n+    }\n+  VEC_free (rtx, heap, recompute_vec);\n }\n \n /* Restore scheduler state from the topmost entry on the backtracking queue.\n@@ -2396,7 +2714,6 @@ unschedule_insns_until (rtx insn)\n \n static void\n restore_last_backtrack_point (struct sched_block_state *psched_block)\n-\n {\n   rtx link;\n   int i;\n@@ -2420,8 +2737,9 @@ restore_last_backtrack_point (struct sched_block_state *psched_block)\n       rtx *first = ready_lastpos (&ready);\n       for (i = 0; i < ready.n_ready; i++)\n \t{\n-\t  QUEUE_INDEX (first[i]) = QUEUE_NOWHERE;\n-\t  INSN_TICK (first[i]) = INVALID_TICK;\n+\t  rtx insn = first[i];\n+\t  QUEUE_INDEX (insn) = QUEUE_NOWHERE;\n+\t  INSN_TICK (insn) = INVALID_TICK;\n \t}\n     }\n   for (i = 0; i <= max_insn_queue_index; i++)\n@@ -2445,8 +2763,10 @@ restore_last_backtrack_point (struct sched_block_state *psched_block)\n       rtx *first = ready_lastpos (&ready);\n       for (i = 0; i < ready.n_ready; i++)\n \t{\n-\t  QUEUE_INDEX (first[i]) = QUEUE_READY;\n-\t  INSN_TICK (first[i]) = save->clock_var;\n+\t  rtx insn = first[i];\n+\t  QUEUE_INDEX (insn) = QUEUE_READY;\n+\t  TODO_SPEC (insn) = recompute_todo_spec (insn);\n+\t  INSN_TICK (insn) = save->clock_var;\n \t}\n     }\n \n@@ -2462,11 +2782,14 @@ restore_last_backtrack_point (struct sched_block_state *psched_block)\n \t{\n \t  rtx x = XEXP (link, 0);\n \t  QUEUE_INDEX (x) = i;\n+\t  TODO_SPEC (x) = recompute_todo_spec (x);\n \t  INSN_TICK (x) = save->clock_var + i;\n \t}\n     }\n   free (save->insn_queue);\n \n+  toggle_cancelled_flags (true);\n+\n   clock_var = save->clock_var;\n   last_clock_var = save->last_clock_var;\n   cycle_issued_insns = save->cycle_issued_insns;\n@@ -2547,6 +2870,9 @@ estimate_insn_tick (bitmap processed, rtx insn, int budget)\n       rtx pro = DEP_PRO (dep);\n       int t;\n \n+      if (DEP_STATUS (dep) & DEP_CANCELLED)\n+\tcontinue;\n+\n       if (QUEUE_INDEX (pro) == QUEUE_SCHEDULED)\n \tgcc_assert (INSN_TICK (pro) + dep_cost (dep) <= INSN_TICK (insn));\n       else\n@@ -4217,6 +4543,7 @@ schedule_block (basic_block *target_bb)\n \t  gcc_assert (failed);\n \n \t  failed_insn = failed->delay_pair->i1;\n+\t  toggle_cancelled_flags (false);\n \t  unschedule_insns_until (failed_insn);\n \t  while (failed != backtrack_queue)\n \t    free_topmost_backtrack_point (true);\n@@ -4732,8 +5059,6 @@ fix_inter_tick (rtx head, rtx tail)\n   bitmap_clear (&processed);\n }\n \n-static int haifa_speculate_insn (rtx, ds_t, rtx *);\n-\n /* Check if NEXT is ready to be added to the ready or queue list.\n    If \"yes\", add it to the proper list.\n    Returns:\n@@ -4747,57 +5072,15 @@ try_ready (rtx next)\n \n   old_ts = TODO_SPEC (next);\n \n-  gcc_assert (!(old_ts & ~(SPECULATIVE | HARD_DEP))\n+  gcc_assert (!(old_ts & ~(SPECULATIVE | HARD_DEP | DEP_CONTROL))\n \t      && ((old_ts & HARD_DEP)\n-\t\t  || (old_ts & SPECULATIVE)));\n-\n-  if (sd_lists_empty_p (next, SD_LIST_BACK))\n-    /* NEXT has all its dependencies resolved.  */\n-    new_ts = 0;\n-  else\n-    {\n-      /* One of the NEXT's dependencies has been resolved.\n-\t Recalculate NEXT's status.  */\n-\n-      if (!sd_lists_empty_p (next, SD_LIST_HARD_BACK))\n-\tnew_ts = HARD_DEP;\n-      else\n-\t/* Now we've got NEXT with speculative deps only.\n-\t   1. Look at the deps to see what we have to do.\n-\t   2. Check if we can do 'todo'.  */\n-\t{\n-\t  sd_iterator_def sd_it;\n-\t  dep_t dep;\n-\t  bool first_p = true;\n-\n-\t  new_ts = 0;\n-\n-\t  FOR_EACH_DEP (next, SD_LIST_BACK, sd_it, dep)\n-\t    {\n-\t      ds_t ds = DEP_STATUS (dep) & SPECULATIVE;\n+\t\t  || (old_ts & SPECULATIVE)\n+\t\t  || (old_ts & DEP_CONTROL)));\n \n-\t      if (DEBUG_INSN_P (DEP_PRO (dep))\n-\t\t  && !DEBUG_INSN_P (next))\n-\t\tcontinue;\n-\n-\t      if (first_p)\n-\t\t{\n-\t\t  first_p = false;\n-\n-\t\t  new_ts = ds;\n-\t\t}\n-\t      else\n-\t\tnew_ts = ds_merge (new_ts, ds);\n-\t    }\n-\n-\t  if (ds_weak (new_ts) < spec_info->data_weakness_cutoff)\n-\t    /* Too few points.  */\n-\t    new_ts = HARD_DEP;\n-\t}\n-    }\n+  new_ts = recompute_todo_spec (next);\n \n   if (new_ts & HARD_DEP)\n-    gcc_assert (new_ts == HARD_DEP && new_ts == old_ts\n+    gcc_assert (new_ts == old_ts\n \t\t&& QUEUE_INDEX (next) == QUEUE_NOWHERE);\n   else if (current_sched_info->new_ready)\n     new_ts = current_sched_info->new_ready (next, new_ts);\n@@ -4820,7 +5103,7 @@ try_ready (rtx next)\n       int res;\n       rtx new_pat;\n \n-      gcc_assert (!(new_ts & ~SPECULATIVE));\n+      gcc_assert ((new_ts & SPECULATIVE) && !(new_ts & ~SPECULATIVE));\n \n       res = haifa_speculate_insn (next, new_ts, &new_pat);\n \n@@ -4846,7 +5129,8 @@ try_ready (rtx next)\n \t       save it.  */\n \t    ORIG_PAT (next) = PATTERN (next);\n \n-\t  haifa_change_pattern (next, new_pat);\n+\t  res = haifa_change_pattern (next, new_pat);\n+\t  gcc_assert (res);\n \t  break;\n \n \tdefault:\n@@ -4871,16 +5155,19 @@ try_ready (rtx next)\n       /*gcc_assert (QUEUE_INDEX (next) == QUEUE_NOWHERE);*/\n \n       change_queue_index (next, QUEUE_NOWHERE);\n+\n       return -1;\n     }\n   else if (!(new_ts & BEGIN_SPEC)\n-\t   && ORIG_PAT (next) && !IS_SPECULATION_CHECK_P (next))\n+\t   && ORIG_PAT (next) && PREDICATED_PAT (next) == NULL_RTX\n+\t   && !IS_SPECULATION_CHECK_P (next))\n     /* We should change pattern of every previously speculative\n        instruction - and we determine if NEXT was speculative by using\n        ORIG_PAT field.  Except one case - speculation checks have ORIG_PAT\n        pat too, so skip them.  */\n     {\n-      haifa_change_pattern (next, ORIG_PAT (next));\n+      bool success = haifa_change_pattern (next, ORIG_PAT (next));\n+      gcc_assert (success);\n       ORIG_PAT (next) = 0;\n     }\n \n@@ -4898,7 +5185,8 @@ try_ready (rtx next)\n           if (new_ts & BE_IN_CONTROL)\n             fprintf (spec_info->dump, \"; in-control-spec;\");\n         }\n-\n+      if (TODO_SPEC (next) & DEP_CONTROL)\n+\tfprintf (sched_dump, \" predicated\");\n       fprintf (sched_dump, \"\\n\");\n     }\n \n@@ -5874,38 +6162,33 @@ fix_recovery_deps (basic_block rec)\n   add_jump_dependencies (insn, jump);\n }\n \n-/* Change pattern of INSN to NEW_PAT.  */\n-void\n-sched_change_pattern (rtx insn, rtx new_pat)\n+/* Change pattern of INSN to NEW_PAT.  Invalidate cached haifa\n+   instruction data.  */\n+static bool\n+haifa_change_pattern (rtx insn, rtx new_pat)\n {\n   sd_iterator_def sd_it;\n   dep_t dep;\n   int t;\n \n   t = validate_change (insn, &PATTERN (insn), new_pat, 0);\n-  gcc_assert (t);\n+  if (!t)\n+    return false;\n   dfa_clear_single_insn_cache (insn);\n \n-  for (sd_it = sd_iterator_start (insn, (SD_LIST_FORW | SD_LIST_BACK\n-\t\t\t\t\t | SD_LIST_RES_BACK));\n-       sd_iterator_cond (&sd_it, &dep);)\n+  sd_it = sd_iterator_start (insn,\n+\t\t\t     SD_LIST_FORW | SD_LIST_BACK | SD_LIST_RES_BACK);\n+  while (sd_iterator_cond (&sd_it, &dep))\n     {\n       DEP_COST (dep) = UNKNOWN_DEP_COST;\n       sd_iterator_next (&sd_it);\n     }\n-}\n-\n-/* Change pattern of INSN to NEW_PAT.  Invalidate cached haifa\n-   instruction data.  */\n-static void\n-haifa_change_pattern (rtx insn, rtx new_pat)\n-{\n-  sched_change_pattern (insn, new_pat);\n \n   /* Invalidate INSN_COST, so it'll be recalculated.  */\n   INSN_COST (insn) = -1;\n   /* Invalidate INSN_TICK, so it'll be recalculated.  */\n   INSN_TICK (insn) = INVALID_TICK;\n+  return true;\n }\n \n /* -1 - can't speculate,"}, {"sha": "a962d3e01f3bcff30dd6c3494fe5eab5f058b2f4", "filename": "gcc/lists.c", "status": "modified", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Flists.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Flists.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flists.c?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -164,6 +164,37 @@ free_INSN_LIST_list (rtx *listp)\n   free_list (listp, &unused_insn_list);\n }\n \n+/* Make a copy of the INSN_LIST list LINK and return it.  */\n+rtx\n+copy_INSN_LIST (rtx link)\n+{\n+  rtx new_queue;\n+  rtx *pqueue = &new_queue;\n+\n+  for (; link; link = XEXP (link, 1))\n+    {\n+      rtx x = XEXP (link, 0);\n+      rtx newlink = alloc_INSN_LIST (x, NULL);\n+      *pqueue = newlink;\n+      pqueue = &XEXP (newlink, 1);\n+    }\n+  *pqueue = NULL_RTX;\n+  return new_queue;\n+}\n+\n+/* Duplicate the INSN_LIST elements of COPY and prepend them to OLD.  */\n+rtx\n+concat_INSN_LIST (rtx copy, rtx old)\n+{\n+  rtx new_rtx = old;\n+  for (; copy ; copy = XEXP (copy, 1))\n+    {\n+      new_rtx = alloc_INSN_LIST (XEXP (copy, 0), new_rtx);\n+      PUT_REG_NOTE_KIND (new_rtx, REG_NOTE_KIND (copy));\n+    }\n+  return new_rtx;\n+}\n+\n /* This function will free up an individual EXPR_LIST node.  */\n void\n free_EXPR_LIST_node (rtx ptr)"}, {"sha": "d103afee0185dccd6b1bffeb53265d2706b7be34", "filename": "gcc/reg-notes.def", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Freg-notes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Freg-notes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Freg-notes.def?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -92,6 +92,7 @@ REG_NOTE (LABEL_OPERAND)\n    respectively.  */\n REG_NOTE (DEP_OUTPUT)\n REG_NOTE (DEP_ANTI)\n+REG_NOTE (DEP_CONTROL)\n \n /* REG_BR_PROB is attached to JUMP_INSNs and CALL_INSNs.  It has an\n    integer value.  For jumps, it is the probability that this is a"}, {"sha": "81958a59c2ef9e4c5712c8525a3e2f1fba06ac92", "filename": "gcc/rtl.h", "status": "modified", "additions": 13, "deletions": 6, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -1941,6 +1941,11 @@ extern rtx find_last_value (rtx, rtx *, rtx, int);\n extern int refers_to_regno_p (unsigned int, unsigned int, const_rtx, rtx *);\n extern int reg_overlap_mentioned_p (const_rtx, const_rtx);\n extern const_rtx set_of (const_rtx, const_rtx);\n+extern void record_hard_reg_sets (rtx, const_rtx, void *);\n+extern void record_hard_reg_uses (rtx *, void *);\n+#ifdef HARD_CONST\n+extern void find_all_hard_reg_sets (const_rtx, HARD_REG_SET *);\n+#endif\n extern void note_stores (const_rtx, void (*) (rtx, const_rtx, void *), void *);\n extern void note_uses (rtx *, void (*) (rtx *, void *), void *);\n extern int dead_or_set_p (const_rtx, const_rtx);\n@@ -2036,12 +2041,14 @@ extern void subreg_get_info (unsigned int, enum machine_mode,\n \n /* lists.c */\n \n-extern void free_EXPR_LIST_list\t\t(rtx *);\n-extern void free_INSN_LIST_list\t\t(rtx *);\n-extern void free_EXPR_LIST_node\t\t(rtx);\n-extern void free_INSN_LIST_node\t\t(rtx);\n-extern rtx alloc_INSN_LIST\t\t\t(rtx, rtx);\n-extern rtx alloc_EXPR_LIST\t\t\t(int, rtx, rtx);\n+extern void free_EXPR_LIST_list (rtx *);\n+extern void free_INSN_LIST_list (rtx *);\n+extern void free_EXPR_LIST_node (rtx);\n+extern void free_INSN_LIST_node (rtx);\n+extern rtx alloc_INSN_LIST (rtx, rtx);\n+extern rtx copy_INSN_LIST (rtx);\n+extern rtx concat_INSN_LIST (rtx, rtx);\n+extern rtx alloc_EXPR_LIST (int, rtx, rtx);\n extern void remove_free_INSN_LIST_elem (rtx, rtx *);\n extern rtx remove_list_elem (rtx, rtx *);\n extern rtx remove_free_INSN_LIST_node (rtx *);"}, {"sha": "b98fa92cdae930e13eb65e1923458e5df2ba0a02", "filename": "gcc/rtlanal.c", "status": "modified", "additions": 50, "deletions": 0, "changes": 50, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Frtlanal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Frtlanal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtlanal.c?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -999,6 +999,56 @@ set_of (const_rtx pat, const_rtx insn)\n   note_stores (INSN_P (insn) ? PATTERN (insn) : insn, set_of_1, &data);\n   return data.found;\n }\n+\n+/* This function, called through note_stores, collects sets and\n+   clobbers of hard registers in a HARD_REG_SET, which is pointed to\n+   by DATA.  */\n+void\n+record_hard_reg_sets (rtx x, const_rtx pat ATTRIBUTE_UNUSED, void *data)\n+{\n+  HARD_REG_SET *pset = (HARD_REG_SET *)data;\n+  if (REG_P (x) && HARD_REGISTER_P (x))\n+    add_to_hard_reg_set (pset, GET_MODE (x), REGNO (x));\n+}\n+\n+/* Examine INSN, and compute the set of hard registers written by it.\n+   Store it in *PSET.  Should only be called after reload.  */\n+void\n+find_all_hard_reg_sets (const_rtx insn, HARD_REG_SET *pset)\n+{\n+  rtx link;\n+\n+  CLEAR_HARD_REG_SET (*pset);\n+  note_stores (PATTERN (insn), record_hard_reg_sets, pset);\n+  if (CALL_P (insn))\n+    IOR_HARD_REG_SET (*pset, call_used_reg_set);\n+  for (link = REG_NOTES (insn); link; link = XEXP (link, 1))\n+    if (REG_NOTE_KIND (link) == REG_INC)\n+      record_hard_reg_sets (XEXP (link, 0), NULL, pset);\n+}\n+\n+/* A for_each_rtx subroutine of record_hard_reg_uses.  */\n+static int\n+record_hard_reg_uses_1 (rtx *px, void *data)\n+{\n+  rtx x = *px;\n+  HARD_REG_SET *pused = (HARD_REG_SET *)data;\n+\n+  if (REG_P (x) && REGNO (x) < FIRST_PSEUDO_REGISTER)\n+    {\n+      int nregs = hard_regno_nregs[REGNO (x)][GET_MODE (x)];\n+      while (nregs-- > 0)\n+\tSET_HARD_REG_BIT (*pused, REGNO (x) + nregs);\n+    }\n+  return 0;\n+}\n+\n+/* Like record_hard_reg_sets, but called through note_uses.  */\n+void\n+record_hard_reg_uses (rtx *px, void *data)\n+{\n+  for_each_rtx (px, record_hard_reg_uses_1, data);\n+}\n \f\n /* Given an INSN, return a SET expression if this insn has only a single SET.\n    It may also have CLOBBERs, USEs, or SET whose output"}, {"sha": "b669cd34957452f7b1ce5514b9b9cf4803c2fd77", "filename": "gcc/sched-deps.c", "status": "modified", "additions": 226, "deletions": 64, "changes": 290, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fsched-deps.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fsched-deps.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-deps.c?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -52,12 +52,6 @@ along with GCC; see the file COPYING3.  If not see\n #define CHECK (false)\n #endif\n \n-/* In deps->last_pending_memory_flush marks JUMP_INSNs that weren't\n-   added to the list because of flush_pending_lists, stands just\n-   for itself and not for any other pending memory reads/writes.  */\n-#define NON_FLUSH_JUMP_KIND REG_DEP_ANTI\n-#define NON_FLUSH_JUMP_P(x) (REG_NOTE_KIND (x) == NON_FLUSH_JUMP_KIND)\n-\n /* Holds current parameters for the dependency analyzer.  */\n struct sched_deps_info_def *sched_deps_info;\n \n@@ -74,6 +68,9 @@ ds_to_dk (ds_t ds)\n   if (ds & DEP_OUTPUT)\n     return REG_DEP_OUTPUT;\n \n+  if (ds & DEP_CONTROL)\n+    return REG_DEP_CONTROL;\n+\n   gcc_assert (ds & DEP_ANTI);\n \n   return REG_DEP_ANTI;\n@@ -91,6 +88,9 @@ dk_to_ds (enum reg_note dk)\n     case REG_DEP_OUTPUT:\n       return DEP_OUTPUT;\n \n+    case REG_DEP_CONTROL:\n+      return DEP_CONTROL;\n+\n     default:\n       gcc_assert (dk == REG_DEP_ANTI);\n       return DEP_ANTI;\n@@ -187,6 +187,10 @@ dump_dep (FILE *dump, dep_t dep, int flags)\n \t  t = 'o';\n \t  break;\n \n+\tcase REG_DEP_CONTROL:\n+\t  t = 'c';\n+\t  break;\n+\n \tcase REG_DEP_ANTI:\n \t  t = 'a';\n \t  break;\n@@ -420,13 +424,22 @@ static bool\n dep_spec_p (dep_t dep)\n {\n   if (current_sched_info->flags & DO_SPECULATION)\n-    return (DEP_STATUS (dep) & SPECULATIVE) != 0;\n+    {\n+      if (DEP_STATUS (dep) & SPECULATIVE)\n+\treturn true;\n+    }\n+  if (current_sched_info->flags & DO_PREDICATION)\n+    {\n+      if (DEP_TYPE (dep) == REG_DEP_CONTROL)\n+\treturn true;\n+    }\n   return false;\n }\n \n static regset reg_pending_sets;\n static regset reg_pending_clobbers;\n static regset reg_pending_uses;\n+static regset reg_pending_control_uses;\n static enum reg_pending_barrier_mode reg_pending_barrier;\n \n /* Hard registers implicitly clobbered or used (or may be implicitly\n@@ -454,10 +467,12 @@ static HARD_REG_SET implicit_reg_pending_uses;\n static bitmap_head *true_dependency_cache = NULL;\n static bitmap_head *output_dependency_cache = NULL;\n static bitmap_head *anti_dependency_cache = NULL;\n+static bitmap_head *control_dependency_cache = NULL;\n static bitmap_head *spec_dependency_cache = NULL;\n static int cache_size;\n \n static int deps_may_trap_p (const_rtx);\n+static void add_dependence_1 (rtx, rtx, enum reg_note);\n static void add_dependence_list (rtx, rtx, int, enum reg_note);\n static void add_dependence_list_and_free (struct deps_desc *, rtx,\n \t\t\t\t\t  rtx *, int, enum reg_note);\n@@ -538,6 +553,27 @@ sched_get_condition_with_rev_uncached (const_rtx insn, bool *rev)\n   return 0;\n }\n \n+/* Return the condition under which INSN does not execute (i.e.  the\n+   not-taken condition for a conditional branch), or NULL if we cannot\n+   find such a condition.  The caller should make a copy of the condition\n+   before using it.  */\n+rtx\n+sched_get_reverse_condition_uncached (const_rtx insn)\n+{\n+  bool rev;\n+  rtx cond = sched_get_condition_with_rev_uncached (insn, &rev);\n+  if (cond == NULL_RTX)\n+    return cond;\n+  if (!rev)\n+    {\n+      enum rtx_code revcode = reversed_comparison_code (cond, insn);\n+      cond = gen_rtx_fmt_ee (revcode, GET_MODE (cond),\n+\t\t\t     XEXP (cond, 0),\n+\t\t\t     XEXP (cond, 1));\n+    }\n+  return cond;\n+}\n+\n /* Caching variant of sched_get_condition_with_rev_uncached.\n    We only do actual work the first time we come here for an insn; the\n    results are cached in INSN_CACHED_COND and INSN_REVERSE_COND.  */\n@@ -861,12 +897,10 @@ sd_find_dep_between (rtx pro, rtx con, bool resolved_p)\n       int elem_luid = INSN_LUID (pro);\n       int insn_luid = INSN_LUID (con);\n \n-      gcc_assert (output_dependency_cache != NULL\n-\t\t  && anti_dependency_cache != NULL);\n-\n       if (!bitmap_bit_p (&true_dependency_cache[insn_luid], elem_luid)\n \t  && !bitmap_bit_p (&output_dependency_cache[insn_luid], elem_luid)\n-\t  && !bitmap_bit_p (&anti_dependency_cache[insn_luid], elem_luid))\n+\t  && !bitmap_bit_p (&anti_dependency_cache[insn_luid], elem_luid)\n+\t  && !bitmap_bit_p (&control_dependency_cache[insn_luid], elem_luid))\n \treturn NULL;\n     }\n \n@@ -919,7 +953,8 @@ ask_dependency_caches (dep_t dep)\n \n   gcc_assert (true_dependency_cache != NULL\n \t      && output_dependency_cache != NULL\n-\t      && anti_dependency_cache != NULL);\n+\t      && anti_dependency_cache != NULL\n+\t      && control_dependency_cache != NULL);\n \n   if (!(current_sched_info->flags & USE_DEPS_LIST))\n     {\n@@ -931,6 +966,8 @@ ask_dependency_caches (dep_t dep)\n \tpresent_dep_type = REG_DEP_OUTPUT;\n       else if (bitmap_bit_p (&anti_dependency_cache[insn_luid], elem_luid))\n \tpresent_dep_type = REG_DEP_ANTI;\n+      else if (bitmap_bit_p (&control_dependency_cache[insn_luid], elem_luid))\n+\tpresent_dep_type = REG_DEP_CONTROL;\n       else\n \t/* There is no existing dep so it should be created.  */\n \treturn DEP_CREATED;\n@@ -949,6 +986,8 @@ ask_dependency_caches (dep_t dep)\n \tpresent_dep_types |= DEP_OUTPUT;\n       if (bitmap_bit_p (&anti_dependency_cache[insn_luid], elem_luid))\n \tpresent_dep_types |= DEP_ANTI;\n+      if (bitmap_bit_p (&control_dependency_cache[insn_luid], elem_luid))\n+\tpresent_dep_types |= DEP_CONTROL;\n \n       if (present_dep_types == 0)\n \t/* There is no existing dep so it should be created.  */\n@@ -1002,6 +1041,10 @@ set_dependency_caches (dep_t dep)\n \t  bitmap_set_bit (&anti_dependency_cache[insn_luid], elem_luid);\n \t  break;\n \n+\tcase REG_DEP_CONTROL:\n+\t  bitmap_set_bit (&control_dependency_cache[insn_luid], elem_luid);\n+\t  break;\n+\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -1016,6 +1059,8 @@ set_dependency_caches (dep_t dep)\n \tbitmap_set_bit (&output_dependency_cache[insn_luid], elem_luid);\n       if (ds & DEP_ANTI)\n \tbitmap_set_bit (&anti_dependency_cache[insn_luid], elem_luid);\n+      if (ds & DEP_CONTROL)\n+\tbitmap_set_bit (&control_dependency_cache[insn_luid], elem_luid);\n \n       if (ds & SPECULATIVE)\n \t{\n@@ -1047,6 +1092,10 @@ update_dependency_caches (dep_t dep, enum reg_note old_type)\n \t  bitmap_clear_bit (&anti_dependency_cache[insn_luid], elem_luid);\n \t  break;\n \n+\tcase REG_DEP_CONTROL:\n+\t  bitmap_clear_bit (&control_dependency_cache[insn_luid], elem_luid);\n+\t  break;\n+\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -1330,8 +1379,7 @@ sd_unresolve_dep (sd_iterator_def sd_it)\n   rtx pro = DEP_PRO (dep);\n   rtx con = DEP_CON (dep);\n \n-  if ((current_sched_info->flags & DO_SPECULATION)\n-      && (DEP_STATUS (dep) & SPECULATIVE))\n+  if (dep_spec_p (dep))\n     move_dep_link (DEP_NODE_BACK (node), INSN_RESOLVED_BACK_DEPS (con),\n \t\t   INSN_SPEC_BACK_DEPS (con));\n   else\n@@ -1382,6 +1430,7 @@ sd_delete_dep (sd_iterator_def sd_it)\n \n       bitmap_clear_bit (&true_dependency_cache[insn_luid], elem_luid);\n       bitmap_clear_bit (&anti_dependency_cache[insn_luid], elem_luid);\n+      bitmap_clear_bit (&control_dependency_cache[insn_luid], elem_luid);\n       bitmap_clear_bit (&output_dependency_cache[insn_luid], elem_luid);\n \n       if (current_sched_info->flags & DO_SPECULATION)\n@@ -1447,6 +1496,53 @@ sd_debug_lists (rtx insn, sd_list_types_def types)\n   fprintf (stderr, \"\\n\");\n }\n \n+/* A wrapper around add_dependence_1, to add a dependence of CON on\n+   PRO, with type DEP_TYPE.  This function implements special handling\n+   for REG_DEP_CONTROL dependencies.  For these, we optionally promote\n+   the type to REG_DEP_ANTI if we can determine that predication is\n+   impossible; otherwise we add additional true dependencies on the\n+   INSN_COND_DEPS list of the jump (which PRO must be).  */\n+void\n+add_dependence (rtx con, rtx pro, enum reg_note dep_type)\n+{\n+  /* A REG_DEP_CONTROL dependence may be eliminated through predication,\n+     so we must also make the insn dependent on the setter of the\n+     condition.  */\n+  if (dep_type == REG_DEP_CONTROL)\n+    {\n+      rtx real_pro = pro;\n+      rtx other = real_insn_for_shadow (real_pro);\n+      rtx cond;\n+\n+      if (other != NULL_RTX)\n+\treal_pro = other;\n+      cond = sched_get_reverse_condition_uncached (real_pro);\n+      /* Verify that the insn does not use a different value in\n+\t the condition register than the one that was present at\n+\t the jump.  */\n+      if (cond == NULL_RTX)\n+\tdep_type = REG_DEP_ANTI;\n+      else if (INSN_CACHED_COND (real_pro) == const_true_rtx)\n+\t{\n+\t  HARD_REG_SET uses;\n+\t  CLEAR_HARD_REG_SET (uses);\n+\t  note_uses (&PATTERN (con), record_hard_reg_uses, &uses);\n+\t  if (TEST_HARD_REG_BIT (uses, REGNO (XEXP (cond, 0))))\n+\t    dep_type = REG_DEP_ANTI;\n+\t}\n+      if (dep_type == REG_DEP_CONTROL)\n+\t{\n+\t  if (sched_verbose >= 5)\n+\t    fprintf (sched_dump, \"making DEP_CONTROL for %d\\n\",\n+\t\t     INSN_UID (real_pro));\n+\t  add_dependence_list (con, INSN_COND_DEPS (real_pro), 0,\n+\t\t\t       REG_DEP_TRUE);\n+\t}\n+    }\n+\t  \n+  add_dependence_1 (con, pro, dep_type);\n+}\n+\n /* A convenience wrapper to operate on an entire list.  */\n \n static void\n@@ -1662,6 +1758,10 @@ flush_pending_lists (struct deps_desc *deps, rtx insn, int for_read,\n   add_dependence_list_and_free (deps, insn,\n                                 &deps->last_pending_memory_flush, 1,\n                                 for_read ? REG_DEP_ANTI : REG_DEP_OUTPUT);\n+\n+  add_dependence_list_and_free (deps, insn, &deps->pending_jump_insns, 1,\n+\t\t\t\tREG_DEP_ANTI);\n+\n   if (!deps->readonly)\n     {\n       free_EXPR_LIST_list (&deps->pending_write_mems);\n@@ -1783,10 +1883,12 @@ ds_to_dt (ds_t ds)\n     return REG_DEP_TRUE;\n   else if (ds & DEP_OUTPUT)\n     return REG_DEP_OUTPUT;\n+  else if (ds & DEP_ANTI)\n+    return REG_DEP_ANTI;\n   else\n     {\n-      gcc_assert (ds & DEP_ANTI);\n-      return REG_DEP_ANTI;\n+      gcc_assert (ds & DEP_CONTROL);\n+      return REG_DEP_CONTROL;\n     }\n }\n \n@@ -2394,6 +2496,8 @@ sched_analyze_1 (struct deps_desc *deps, rtx x, rtx insn)\n \n \t  add_dependence_list (insn, deps->last_pending_memory_flush, 1,\n \t\t\t       REG_DEP_ANTI);\n+\t  add_dependence_list (insn, deps->pending_jump_insns, 1,\n+\t\t\t       REG_DEP_CONTROL);\n \n           if (!deps->readonly)\n             add_insn_mem_dependence (deps, false, insn, dest);\n@@ -2541,23 +2645,22 @@ sched_analyze_2 (struct deps_desc *deps, rtx x, rtx insn)\n \t      }\n \n \t    for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n-\t      {\n-\t\tif (! NON_FLUSH_JUMP_P (u))\n-\t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t\telse if (deps_may_trap_p (x))\n-\t\t  {\n-\t\t    if ((sched_deps_info->generate_spec_deps)\n-\t\t\t&& sel_sched_p () && (spec_info->mask & BEGIN_CONTROL))\n-\t\t      {\n-\t\t\tds_t ds = set_dep_weak (DEP_ANTI, BEGIN_CONTROL,\n-\t\t\t\t\t\tMAX_DEP_WEAK);\n-\n-\t\t\tnote_dep (XEXP (u, 0), ds);\n-\t\t      }\n-\t\t    else\n-\t\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t\t  }\n-\t      }\n+\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\n+\t    for (u = deps->pending_jump_insns; u; u = XEXP (u, 1))\n+\t      if (deps_may_trap_p (x))\n+\t\t{\n+\t\t  if ((sched_deps_info->generate_spec_deps)\n+\t\t      && sel_sched_p () && (spec_info->mask & BEGIN_CONTROL))\n+\t\t    {\n+\t\t      ds_t ds = set_dep_weak (DEP_ANTI, BEGIN_CONTROL,\n+\t\t\t\t\t      MAX_DEP_WEAK);\n+\t\t      \n+\t\t      note_dep (XEXP (u, 0), ds);\n+\t\t    }\n+\t\t  else\n+\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_CONTROL);\n+\t\t}\n \t  }\n \n \t/* Always add these dependencies to pending_reads, since\n@@ -2776,29 +2879,19 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \n           if (sched_deps_info->compute_jump_reg_dependencies)\n             {\n-              regset_head tmp;\n-              INIT_REG_SET (&tmp);\n-\n-              (*sched_deps_info->compute_jump_reg_dependencies) (insn, &tmp);\n+              (*sched_deps_info->compute_jump_reg_dependencies)\n+\t\t(insn, reg_pending_control_uses);\n \n               /* Make latency of jump equal to 0 by using anti-dependence.  */\n-              EXECUTE_IF_SET_IN_REG_SET (&tmp, 0, i, rsi)\n+              EXECUTE_IF_SET_IN_REG_SET (reg_pending_control_uses, 0, i, rsi)\n                 {\n                   struct deps_reg *reg_last = &deps->reg_last[i];\n                   add_dependence_list (insn, reg_last->sets, 0, REG_DEP_ANTI);\n                   add_dependence_list (insn, reg_last->implicit_sets,\n \t\t\t\t       0, REG_DEP_ANTI);\n                   add_dependence_list (insn, reg_last->clobbers, 0,\n \t\t\t\t       REG_DEP_ANTI);\n-\n-                  if (!deps->readonly)\n-                    {\n-                      reg_last->uses_length++;\n-                      reg_last->uses = alloc_INSN_LIST (insn, reg_last->uses);\n-                    }\n                 }\n-\n-              CLEAR_REG_SET (&tmp);\n             }\n \n \t  /* All memory writes and volatile reads must happen before the\n@@ -2828,6 +2921,8 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \n \t  add_dependence_list (insn, deps->last_pending_memory_flush, 1,\n \t\t\t       REG_DEP_ANTI);\n+\t  add_dependence_list (insn, deps->pending_jump_insns, 1,\n+\t\t\t       REG_DEP_ANTI);\n \t}\n     }\n \n@@ -2863,13 +2958,15 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \t\t\t   REG_DEP_ANTI);\n \n       for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n-\tif (! NON_FLUSH_JUMP_P (u) || !sel_sched_p ())\n+\tif (!sel_sched_p ())\n \t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n \n       EXECUTE_IF_SET_IN_REG_SET (reg_pending_uses, 0, i, rsi)\n \t{\n \t  struct deps_reg *reg_last = &deps->reg_last[i];\n \t  add_dependence_list (insn, reg_last->sets, 1, REG_DEP_ANTI);\n+\t  /* There's no point in making REG_DEP_CONTROL dependencies for\n+\t     debug insns.  */\n \t  add_dependence_list (insn, reg_last->clobbers, 1, REG_DEP_ANTI);\n \n \t  if (!deps->readonly)\n@@ -2953,6 +3050,8 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \t      add_dependence_list (insn, reg_last->implicit_sets, 0,\n \t\t\t\t   REG_DEP_ANTI);\n \t      add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);\n+\t      add_dependence_list (insn, reg_last->control_uses, 0,\n+\t\t\t\t   REG_DEP_CONTROL);\n \n \t      if (!deps->readonly)\n \t\t{\n@@ -2969,6 +3068,8 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \t\t\t\t   REG_DEP_ANTI);\n \t      add_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_OUTPUT);\n \t      add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);\n+\t      add_dependence_list (insn, reg_last->control_uses, 0,\n+\t\t\t\t   REG_DEP_CONTROL);\n \n \t      if (!deps->readonly)\n \t\treg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n@@ -2989,6 +3090,9 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \t\t\t\t\t\tREG_DEP_ANTI);\n \t\t  add_dependence_list_and_free (deps, insn, &reg_last->uses, 0,\n \t\t\t\t\t\tREG_DEP_ANTI);\n+\t\t  add_dependence_list_and_free (deps, insn,\n+\t\t\t\t\t\t&reg_last->control_uses, 0,\n+\t\t\t\t\t\tREG_DEP_ANTI);\n \t\t  add_dependence_list_and_free\n \t\t    (deps, insn, &reg_last->clobbers, 0, REG_DEP_OUTPUT);\n \n@@ -3005,6 +3109,8 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \t\t  add_dependence_list (insn, reg_last->implicit_sets, 0,\n \t\t\t\t       REG_DEP_ANTI);\n \t\t  add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);\n+\t\t  add_dependence_list (insn, reg_last->control_uses, 0,\n+\t\t\t\t       REG_DEP_CONTROL);\n \t\t}\n \n \t      if (!deps->readonly)\n@@ -3027,6 +3133,8 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \t\t\t\t\t    REG_DEP_OUTPUT);\n \t      add_dependence_list_and_free (deps, insn, &reg_last->uses, 0,\n \t\t\t\t\t    REG_DEP_ANTI);\n+\t      add_dependence_list (insn, reg_last->control_uses, 0,\n+\t\t\t\t   REG_DEP_CONTROL);\n \n \t      if (!deps->readonly)\n \t\t{\n@@ -3036,6 +3144,15 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \t\t}\n \t    }\n \t}\n+      if (!deps->readonly)\n+\t{\n+\t  EXECUTE_IF_SET_IN_REG_SET (reg_pending_control_uses, 0, i, rsi)\n+\t    {\n+\t      struct deps_reg *reg_last = &deps->reg_last[i];\n+\t      reg_last->control_uses\n+\t\t= alloc_INSN_LIST (insn, reg_last->control_uses);\n+\t    }\n+\t}\n     }\n \n   for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n@@ -3045,6 +3162,7 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \tadd_dependence_list (insn, reg_last->sets, 0, REG_DEP_ANTI);\n \tadd_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_ANTI);\n \tadd_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);\n+\tadd_dependence_list (insn, reg_last->control_uses, 0, REG_DEP_ANTI);\n \n \tif (!deps->readonly)\n \t  reg_last->implicit_sets\n@@ -3068,6 +3186,7 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n   CLEAR_REG_SET (reg_pending_uses);\n   CLEAR_REG_SET (reg_pending_clobbers);\n   CLEAR_REG_SET (reg_pending_sets);\n+  CLEAR_REG_SET (reg_pending_control_uses);\n   CLEAR_HARD_REG_SET (implicit_reg_pending_clobbers);\n   CLEAR_HARD_REG_SET (implicit_reg_pending_uses);\n \n@@ -3099,6 +3218,9 @@ sched_analyze_insn (struct deps_desc *deps, rtx x, rtx insn)\n \t      struct deps_reg *reg_last = &deps->reg_last[i];\n \t      add_dependence_list_and_free (deps, insn, &reg_last->uses, 0,\n \t\t\t\t\t    REG_DEP_ANTI);\n+\t      add_dependence_list_and_free (deps, insn,\n+\t\t\t\t\t    &reg_last->control_uses, 0,\n+\t\t\t\t\t    REG_DEP_CONTROL);\n \t      add_dependence_list_and_free (deps, insn, &reg_last->sets, 0,\n \t\t\t\t\t    reg_pending_barrier == TRUE_BARRIER\n \t\t\t\t\t    ? REG_DEP_TRUE : REG_DEP_ANTI);\n@@ -3312,7 +3434,33 @@ deps_analyze_insn (struct deps_desc *deps, rtx insn)\n \n   /* Record the condition for this insn.  */\n   if (NONDEBUG_INSN_P (insn))\n-    sched_get_condition_with_rev (insn, NULL);\n+    {\n+      rtx t;\n+      sched_get_condition_with_rev (insn, NULL);\n+      t = INSN_CACHED_COND (insn);\n+      INSN_COND_DEPS (insn) = NULL_RTX;\n+      if (reload_completed\n+\t  && (current_sched_info->flags & DO_PREDICATION)\n+\t  && COMPARISON_P (t)\n+\t  && REG_P (XEXP (t, 0))\n+\t  && CONSTANT_P (XEXP (t, 1)))\n+\t{\n+\t  unsigned int regno;\n+\t  int nregs;\n+\t  t = XEXP (t, 0);\n+\t  regno = REGNO (t);\n+\t  nregs = hard_regno_nregs[regno][GET_MODE (t)];\n+\t  t = NULL_RTX;\n+\t  while (nregs-- > 0)\n+\t    {\n+\t      struct deps_reg *reg_last = &deps->reg_last[regno + nregs];\n+\t      t = concat_INSN_LIST (reg_last->sets, t);\n+\t      t = concat_INSN_LIST (reg_last->clobbers, t);\n+\t      t = concat_INSN_LIST (reg_last->implicit_sets, t);\n+\t    }\n+\t  INSN_COND_DEPS (insn) = t;\n+\t}\n+    }\n \n   if (JUMP_P (insn))\n     {\n@@ -3326,15 +3474,8 @@ deps_analyze_insn (struct deps_desc *deps, rtx insn)\n           if (deps->pending_flush_length++ > MAX_PENDING_LIST_LENGTH)\n             flush_pending_lists (deps, insn, true, true);\n           else\n-\t    {\n-\t      deps->last_pending_memory_flush\n-\t\t= alloc_INSN_LIST (insn, deps->last_pending_memory_flush);\n-\t      /* Signal to sched_analyze_insn that this jump stands\n-\t\t just for its own, not any other pending memory\n-\t\t reads/writes flush_pending_lists had to flush.  */\n-\t      PUT_REG_NOTE_KIND (deps->last_pending_memory_flush,\n-\t\t\t\t NON_FLUSH_JUMP_KIND);\n-\t    }\n+\t    deps->pending_jump_insns\n+              = alloc_INSN_LIST (insn, deps->pending_jump_insns);\n         }\n \n       /* For each insn which shouldn't cross a jump, add a dependence.  */\n@@ -3584,6 +3725,7 @@ init_deps (struct deps_desc *deps, bool lazy_reg_last)\n   deps->pending_read_mems = 0;\n   deps->pending_write_insns = 0;\n   deps->pending_write_mems = 0;\n+  deps->pending_jump_insns = 0;\n   deps->pending_read_list_length = 0;\n   deps->pending_write_list_length = 0;\n   deps->pending_flush_length = 0;\n@@ -3644,6 +3786,8 @@ free_deps (struct deps_desc *deps)\n \tfree_INSN_LIST_list (&reg_last->sets);\n       if (reg_last->implicit_sets)\n \tfree_INSN_LIST_list (&reg_last->implicit_sets);\n+      if (reg_last->control_uses)\n+\tfree_INSN_LIST_list (&reg_last->control_uses);\n       if (reg_last->clobbers)\n \tfree_INSN_LIST_list (&reg_last->clobbers);\n     }\n@@ -3672,6 +3816,9 @@ remove_from_deps (struct deps_desc *deps, rtx insn)\n   removed = remove_from_both_dependence_lists (insn, &deps->pending_write_insns,\n                                                &deps->pending_write_mems);\n   deps->pending_write_list_length -= removed;\n+\n+  removed = remove_from_dependence_list (insn, &deps->pending_jump_insns);\n+  deps->pending_flush_length -= removed;\n   removed = remove_from_dependence_list (insn, &deps->last_pending_memory_flush);\n   deps->pending_flush_length -= removed;\n \n@@ -3766,6 +3913,8 @@ extend_dependency_caches (int n, bool create_p)\n \t\t\t\t\t    output_dependency_cache, luid);\n       anti_dependency_cache = XRESIZEVEC (bitmap_head, anti_dependency_cache,\n \t\t\t\t\t  luid);\n+      control_dependency_cache = XRESIZEVEC (bitmap_head, control_dependency_cache,\n+\t\t\t\t\t  luid);\n \n       if (current_sched_info->flags & DO_SPECULATION)\n         spec_dependency_cache = XRESIZEVEC (bitmap_head, spec_dependency_cache,\n@@ -3776,6 +3925,7 @@ extend_dependency_caches (int n, bool create_p)\n \t  bitmap_initialize (&true_dependency_cache[i], 0);\n \t  bitmap_initialize (&output_dependency_cache[i], 0);\n \t  bitmap_initialize (&anti_dependency_cache[i], 0);\n+\t  bitmap_initialize (&control_dependency_cache[i], 0);\n \n           if (current_sched_info->flags & DO_SPECULATION)\n             bitmap_initialize (&spec_dependency_cache[i], 0);\n@@ -3805,6 +3955,7 @@ sched_deps_finish (void)\n \t  bitmap_clear (&true_dependency_cache[i]);\n \t  bitmap_clear (&output_dependency_cache[i]);\n \t  bitmap_clear (&anti_dependency_cache[i]);\n+\t  bitmap_clear (&control_dependency_cache[i]);\n \n           if (sched_deps_info->generate_spec_deps)\n             bitmap_clear (&spec_dependency_cache[i]);\n@@ -3815,6 +3966,8 @@ sched_deps_finish (void)\n       output_dependency_cache = NULL;\n       free (anti_dependency_cache);\n       anti_dependency_cache = NULL;\n+      free (control_dependency_cache);\n+      control_dependency_cache = NULL;\n \n       if (sched_deps_info->generate_spec_deps)\n         {\n@@ -3836,6 +3989,7 @@ init_deps_global (void)\n   reg_pending_sets = ALLOC_REG_SET (&reg_obstack);\n   reg_pending_clobbers = ALLOC_REG_SET (&reg_obstack);\n   reg_pending_uses = ALLOC_REG_SET (&reg_obstack);\n+  reg_pending_control_uses = ALLOC_REG_SET (&reg_obstack);\n   reg_pending_barrier = NOT_A_BARRIER;\n \n   if (!sel_sched_p () || sched_emulate_haifa_p)\n@@ -3860,6 +4014,7 @@ finish_deps_global (void)\n   FREE_REG_SET (reg_pending_sets);\n   FREE_REG_SET (reg_pending_clobbers);\n   FREE_REG_SET (reg_pending_uses);\n+  FREE_REG_SET (reg_pending_control_uses);\n }\n \n /* Estimate the weakness of dependence between MEM1 and MEM2.  */\n@@ -3893,8 +4048,8 @@ estimate_dep_weak (rtx mem1, rtx mem2)\n /* Add or update backward dependence between INSN and ELEM with type DEP_TYPE.\n    This function can handle same INSN and ELEM (INSN == ELEM).\n    It is a convenience wrapper.  */\n-void\n-add_dependence (rtx insn, rtx elem, enum reg_note dep_type)\n+static void\n+add_dependence_1 (rtx insn, rtx elem, enum reg_note dep_type)\n {\n   ds_t ds;\n   bool internal;\n@@ -3903,6 +4058,8 @@ add_dependence (rtx insn, rtx elem, enum reg_note dep_type)\n     ds = DEP_TRUE;\n   else if (dep_type == REG_DEP_OUTPUT)\n     ds = DEP_OUTPUT;\n+  else if (dep_type == REG_DEP_CONTROL)\n+    ds = DEP_CONTROL;\n   else\n     {\n       gcc_assert (dep_type == REG_DEP_ANTI);\n@@ -4169,10 +4326,12 @@ dump_ds (FILE *f, ds_t s)\n \n   if (s & DEP_TRUE)\n     fprintf (f, \"DEP_TRUE; \");\n-  if (s & DEP_ANTI)\n-    fprintf (f, \"DEP_ANTI; \");\n   if (s & DEP_OUTPUT)\n     fprintf (f, \"DEP_OUTPUT; \");\n+  if (s & DEP_ANTI)\n+    fprintf (f, \"DEP_ANTI; \");\n+  if (s & DEP_CONTROL)\n+    fprintf (f, \"DEP_CONTROL; \");\n \n   fprintf (f, \"}\");\n }\n@@ -4207,10 +4366,13 @@ check_dep (dep_t dep, bool relaxed_p)\n   else if (dt == REG_DEP_OUTPUT)\n     gcc_assert ((ds & DEP_OUTPUT)\n \t\t&& !(ds & DEP_TRUE));\n-  else\n-    gcc_assert ((dt == REG_DEP_ANTI)\n-\t\t&& (ds & DEP_ANTI)\n+  else if (dt == REG_DEP_ANTI)\n+    gcc_assert ((ds & DEP_ANTI)\n \t\t&& !(ds & (DEP_OUTPUT | DEP_TRUE)));\n+  else\n+    gcc_assert (dt == REG_DEP_CONTROL\n+\t\t&& (ds & DEP_CONTROL)\n+\t\t&& !(ds & (DEP_OUTPUT | DEP_ANTI | DEP_TRUE)));\n \n   /* HARD_DEP can not appear in dep_status of a link.  */\n   gcc_assert (!(ds & HARD_DEP));"}, {"sha": "9a28e2bfc4b3db4b0bf98f1de42accf9a1243f6e", "filename": "gcc/sched-ebb.c", "status": "modified", "additions": 11, "deletions": 20, "changes": 31, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fsched-ebb.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fsched-ebb.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-ebb.c?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -431,32 +431,23 @@ add_deps_for_risky_insns (rtx head, rtx tail)\n \t\t rank.  */\n \t      if (! sched_insns_conditions_mutex_p (insn, prev))\n \t\t{\n-\t\t  dep_def _dep, *dep = &_dep;\n-\n-\t\t  init_dep (dep, prev, insn, REG_DEP_ANTI);\n-\n-\t\t  if (!(current_sched_info->flags & USE_DEPS_LIST))\n+\t\t  if ((current_sched_info->flags & DO_SPECULATION)\n+\t\t      && (spec_info->mask & BEGIN_CONTROL))\n \t\t    {\n-\t\t      enum DEPS_ADJUST_RESULT res;\n+\t\t      dep_def _dep, *dep = &_dep;\n \n-\t\t      res = sd_add_or_update_dep (dep, false);\n+\t\t      init_dep (dep, prev, insn, REG_DEP_ANTI);\n \n-\t\t      /* We can't change an existing dependency with\n-\t\t\t DEP_ANTI.  */\n-\t\t      gcc_assert (res != DEP_CHANGED);\n-\t\t    }\n-\t\t  else\n-\t\t    {\n-\t\t      if ((current_sched_info->flags & DO_SPECULATION)\n-\t\t\t  && (spec_info->mask & BEGIN_CONTROL))\n-\t\t\tDEP_STATUS (dep) = set_dep_weak (DEP_ANTI, BEGIN_CONTROL,\n-\t\t\t\t\t\t\t MAX_DEP_WEAK);\n+\t\t      if (current_sched_info->flags & USE_DEPS_LIST)\n+\t\t\t{\n+\t\t\t  DEP_STATUS (dep) = set_dep_weak (DEP_ANTI, BEGIN_CONTROL,\n+\t\t\t\t\t\t\t   MAX_DEP_WEAK);\n \n+\t\t\t}\n \t\t      sd_add_or_update_dep (dep, false);\n-\n-\t\t      /* Dep_status could have been changed.\n-\t\t\t No assertion here.  */\n \t\t    }\n+\t\t  else\n+\t\t    add_dependence (insn, prev, REG_DEP_CONTROL);\n \t\t}\n \n \t      break;"}, {"sha": "9b29ea158cb8b3052ed132c79922879f3fbfdcbd", "filename": "gcc/sched-int.h", "status": "modified", "additions": 34, "deletions": 7, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fsched-int.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fsched-int.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-int.h?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -424,6 +424,7 @@ struct deps_reg\n   rtx uses;\n   rtx sets;\n   rtx implicit_sets;\n+  rtx control_uses;\n   rtx clobbers;\n   int uses_length;\n   int clobbers_length;\n@@ -453,6 +454,9 @@ struct deps_desc\n   /* An EXPR_LIST containing all MEM rtx's which are pending writes.  */\n   rtx pending_write_mems;\n \n+  /* An INSN_LIST containing all jump insns.  */\n+  rtx pending_jump_insns;\n+\n   /* We must prevent the above lists from ever growing too large since\n      the number of dependencies produced is at least O(N*N),\n      and execution time is at least O(4*N*N), as a function of the\n@@ -464,8 +468,9 @@ struct deps_desc\n   /* Indicates the length of the pending_write list.  */\n   int pending_write_list_length;\n \n-  /* Length of the pending memory flush list. Large functions with no\n-     calls may build up extremely large lists.  */\n+  /* Length of the pending memory flush list plus the length of the pending\n+     jump insn list.  Large functions with no calls may build up extremely\n+     large lists.  */\n   int pending_flush_length;\n \n   /* The last insn upon which all memory references must depend.\n@@ -699,6 +704,10 @@ struct _haifa_deps_insn_data\n      condition that has been clobbered by a subsequent insn.  */\n   rtx cond;\n \n+  /* For a conditional insn, a list of insns that could set the condition\n+     register.  Used when generating control dependencies.  */\n+  rtx cond_deps;\n+\n   /* True if the condition in 'cond' should be reversed to get the actual\n      condition.  */\n   unsigned int reverse_cond : 1;\n@@ -799,6 +808,10 @@ struct _haifa_insn_data\n      real insns following them.  */\n   unsigned int shadow_p : 1;\n \n+  /* Used internally in unschedule_insns_until to mark insns that must have\n+     their TODO_SPEC recomputed.  */\n+  unsigned int must_recompute_spec : 1;\n+\n   /* '> 0' if priority is valid,\n      '== 0' if priority was not yet computed,\n      '< 0' if priority in invalid and should be recomputed.  */\n@@ -819,6 +832,10 @@ struct _haifa_insn_data\n   /* Original pattern of the instruction.  */\n   rtx orig_pat;\n \n+  /* For insns with DEP_CONTROL dependencies, the predicated pattern if it\n+     was ever successfully constructed.  */\n+  rtx predicated_pat;\n+\n   /* The following array contains info how the insn increases register\n      pressure.  There is an element for each cover class of pseudos\n      referenced in insns.  */\n@@ -880,6 +897,7 @@ extern VEC(haifa_deps_insn_data_def, heap) *h_d_i_d;\n #define INSN_SPEC_BACK_DEPS(INSN) (HDID (INSN)->spec_back_deps)\n #define INSN_CACHED_COND(INSN)\t(HDID (INSN)->cond)\n #define INSN_REVERSE_COND(INSN) (HDID (INSN)->reverse_cond)\n+#define INSN_COND_DEPS(INSN)\t(HDID (INSN)->cond_deps)\n #define CANT_MOVE(INSN)\t(HDID (INSN)->cant_move)\n #define CANT_MOVE_BY_LUID(LUID)\t(VEC_index (haifa_deps_insn_data_def, h_d_i_d, \\\n                                             LUID)->cant_move)\n@@ -893,6 +911,7 @@ extern VEC(haifa_deps_insn_data_def, heap) *h_d_i_d;\n #define CHECK_SPEC(INSN) (HID (INSN)->check_spec)\n #define RECOVERY_BLOCK(INSN) (HID (INSN)->recovery_block)\n #define ORIG_PAT(INSN) (HID (INSN)->orig_pat)\n+#define PREDICATED_PAT(INSN) (HID (INSN)->predicated_pat)\n \n /* INSN is either a simple or a branchy speculation check.  */\n #define IS_SPECULATION_CHECK_P(INSN) \\\n@@ -932,10 +951,11 @@ extern VEC(haifa_deps_insn_data_def, heap) *h_d_i_d;\n /* We exclude sign bit.  */\n #define BITS_PER_DEP_STATUS (HOST_BITS_PER_INT - 1)\n \n-/* First '4' stands for 3 dep type bits and HARD_DEP bit.\n+/* First '6' stands for 4 dep type bits and the HARD_DEP and DEP_CANCELLED\n+   bits.\n    Second '4' stands for BEGIN_{DATA, CONTROL}, BE_IN_{DATA, CONTROL}\n    dep weakness.  */\n-#define BITS_PER_DEP_WEAK ((BITS_PER_DEP_STATUS - 4) / 4)\n+#define BITS_PER_DEP_WEAK ((BITS_PER_DEP_STATUS - 6) / 4)\n \n /* Mask of speculative weakness in dep_status.  */\n #define DEP_WEAK_MASK ((1 << BITS_PER_DEP_WEAK) - 1)\n@@ -1009,13 +1029,16 @@ enum SPEC_TYPES_OFFSETS {\n #define DEP_TRUE (((ds_t) 1) << (BE_IN_CONTROL_BITS_OFFSET + BITS_PER_DEP_WEAK))\n #define DEP_OUTPUT (DEP_TRUE << 1)\n #define DEP_ANTI (DEP_OUTPUT << 1)\n+#define DEP_CONTROL (DEP_ANTI << 1)\n \n-#define DEP_TYPES (DEP_TRUE | DEP_OUTPUT | DEP_ANTI)\n+#define DEP_TYPES (DEP_TRUE | DEP_OUTPUT | DEP_ANTI | DEP_CONTROL)\n \n /* Instruction has non-speculative dependence.  This bit represents the\n    property of an instruction - not the one of a dependence.\n    Therefore, it can appear only in TODO_SPEC field of an instruction.  */\n-#define HARD_DEP (DEP_ANTI << 1)\n+#define HARD_DEP (DEP_CONTROL << 1)\n+\n+#define DEP_CANCELLED (HARD_DEP << 1)\n \n /* This represents the results of calling sched-deps.c functions,\n    which modify dependencies.  */\n@@ -1041,7 +1064,8 @@ enum SCHED_FLAGS {\n      Requires USE_DEPS_LIST set.  */\n   DO_SPECULATION = USE_DEPS_LIST << 1,\n   DO_BACKTRACKING = DO_SPECULATION << 1,\n-  SCHED_RGN = DO_BACKTRACKING << 1,\n+  DO_PREDICATION = DO_BACKTRACKING << 1,\n+  SCHED_RGN = DO_PREDICATION << 1,\n   SCHED_EBB = SCHED_RGN << 1,\n   /* Scheduler can possibly create new basic blocks.  Used for assertions.  */\n   NEW_BBS = SCHED_EBB << 1,\n@@ -1202,6 +1226,7 @@ extern struct sched_deps_info_def *sched_deps_info;\n \n \n /* Functions in sched-deps.c.  */\n+extern rtx sched_get_reverse_condition_uncached (const_rtx);\n extern bool sched_insns_conditions_mutex_p (const_rtx, const_rtx);\n extern bool sched_insn_is_legitimate_for_speculation_p (const_rtx, ds_t);\n extern void add_dependence (rtx, rtx, enum reg_note);\n@@ -1337,6 +1362,7 @@ extern bool sched_no_dce;\n \n extern void set_modulo_params (int, int, int, int);\n extern void record_delay_slot_pair (rtx, rtx, int, int);\n+extern rtx real_insn_for_shadow (rtx);\n extern void discard_delay_pairs_above (int);\n extern void free_delay_pairs (void);\n extern void add_delay_dependencies (rtx);\n@@ -1527,3 +1553,4 @@ extern void print_pattern (char *, const_rtx, int);\n extern void print_value (char *, const_rtx, int);\n \n #endif /* GCC_SCHED_INT_H */\n+"}, {"sha": "8e948880999c0df567d7e4eeca9a7a7849bb999d", "filename": "gcc/sched-rgn.c", "status": "modified", "additions": 6, "deletions": 15, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fsched-rgn.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e2724e63c6d26b05a51a519f4715265e8c82419a/gcc%2Fsched-rgn.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-rgn.c?ref=e2724e63c6d26b05a51a519f4715265e8c82419a", "patch": "@@ -234,7 +234,6 @@ static void add_branch_dependences (rtx, rtx);\n static void compute_block_dependences (int);\n \n static void schedule_region (int);\n-static rtx concat_INSN_LIST (rtx, rtx);\n static void concat_insn_mem_list (rtx, rtx, rtx *, rtx *);\n static void propagate_deps (int, struct deps_desc *);\n static void free_pending_lists (void);\n@@ -2552,20 +2551,6 @@ add_branch_dependences (rtx head, rtx tail)\n \n static struct deps_desc *bb_deps;\n \n-/* Duplicate the INSN_LIST elements of COPY and prepend them to OLD.  */\n-\n-static rtx\n-concat_INSN_LIST (rtx copy, rtx old)\n-{\n-  rtx new_rtx = old;\n-  for (; copy ; copy = XEXP (copy, 1))\n-    {\n-      new_rtx = alloc_INSN_LIST (XEXP (copy, 0), new_rtx);\n-      PUT_REG_NOTE_KIND (new_rtx, REG_NOTE_KIND (copy));\n-    }\n-  return new_rtx;\n-}\n-\n static void\n concat_insn_mem_list (rtx copy_insns, rtx copy_mems, rtx *old_insns_p,\n \t\t      rtx *old_mems_p)\n@@ -2619,6 +2604,9 @@ deps_join (struct deps_desc *succ_deps, struct deps_desc *pred_deps)\n                         &succ_deps->pending_write_insns,\n                         &succ_deps->pending_write_mems);\n \n+  succ_deps->pending_jump_insns\n+    = concat_INSN_LIST (pred_deps->pending_jump_insns,\n+                        succ_deps->pending_jump_insns);\n   succ_deps->last_pending_memory_flush\n     = concat_INSN_LIST (pred_deps->last_pending_memory_flush,\n                         succ_deps->last_pending_memory_flush);\n@@ -2670,12 +2658,14 @@ propagate_deps (int bb, struct deps_desc *pred_deps)\n   bb_deps[bb].pending_read_mems = pred_deps->pending_read_mems;\n   bb_deps[bb].pending_write_insns = pred_deps->pending_write_insns;\n   bb_deps[bb].pending_write_mems = pred_deps->pending_write_mems;\n+  bb_deps[bb].pending_jump_insns = pred_deps->pending_jump_insns;\n \n   /* Can't allow these to be freed twice.  */\n   pred_deps->pending_read_insns = 0;\n   pred_deps->pending_read_mems = 0;\n   pred_deps->pending_write_insns = 0;\n   pred_deps->pending_write_mems = 0;\n+  pred_deps->pending_jump_insns = 0;\n }\n \n /* Compute dependences inside bb.  In a multiple blocks region:\n@@ -2754,6 +2744,7 @@ free_pending_lists (void)\n       free_INSN_LIST_list (&bb_deps[bb].pending_write_insns);\n       free_EXPR_LIST_list (&bb_deps[bb].pending_read_mems);\n       free_EXPR_LIST_list (&bb_deps[bb].pending_write_mems);\n+      free_INSN_LIST_list (&bb_deps[bb].pending_jump_insns);\n     }\n }\n \f"}]}