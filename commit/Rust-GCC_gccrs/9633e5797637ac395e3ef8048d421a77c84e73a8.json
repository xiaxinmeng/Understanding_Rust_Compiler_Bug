{"sha": "9633e5797637ac395e3ef8048d421a77c84e73a8", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OTYzM2U1Nzk3NjM3YWMzOTVlM2VmODA0OGQ0MjFhNzdjODRlNzNhOA==", "commit": {"author": {"name": "Jonathan Wright", "email": "jonathan.wright@arm.com", "date": "2021-02-02T15:21:12Z"}, "committer": {"name": "Jonathan Wright", "email": "jonathan.wright@arm.com", "date": "2021-02-03T14:02:34Z"}, "message": "aarch64: Use RTL builtins for [su]mlsl_high_lane[q] intrinsics\n\nRewrite [su]mlsl_high_lane[q] Neon intrinsics to use RTL builtins\nrather than inline assembly code, allowing for better scheduling and\noptimization.\n\ngcc/ChangeLog:\n\n2021-02-02  Jonathan Wright  <jonathan.wright@arm.com>\n\n\t* config/aarch64/aarch64-simd-builtins.def: Add\n\t[su]mlsl_hi_lane[q] builtin macro generators.\n\t* config/aarch64/aarch64-simd.md\n\t(aarch64_<su>mlsl_hi_lane<mode>_insn): Define.\n\t(aarch64_<su>mlsl_hi_lane<mode>): Define.\n\t(aarch64_<su>mlsl_hi_laneq<mode>_insn): Define.\n\t(aarch64_<su>mlsl_hi_laneq<mode>): Define.\n\t* config/aarch64/arm_neon.h (vmlsl_high_lane_s16): Use RTL\n\tbuiltin instead of inline asm.\n\t(vmlsl_high_lane_s32): Likewise.\n\t(vmlsl_high_lane_u16): Likewise.\n\t(vmlsl_high_lane_u32): Likewise.\n\t(vmlsl_high_laneq_s16): Likewise.\n\t(vmlsl_high_laneq_s32): Likewise.\n\t(vmlsl_high_laneq_u16): Likewise.\n\t(vmlsl_high_laneq_u32): Likewise.\n\t(vmlal_high_laneq_u32): Likewise.", "tree": {"sha": "245c0d6ccece52b8b3ecb0d71a4900f8d91c960b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/245c0d6ccece52b8b3ecb0d71a4900f8d91c960b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/9633e5797637ac395e3ef8048d421a77c84e73a8", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9633e5797637ac395e3ef8048d421a77c84e73a8", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9633e5797637ac395e3ef8048d421a77c84e73a8", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9633e5797637ac395e3ef8048d421a77c84e73a8/comments", "author": {"login": "jwright-arm", "id": 31624044, "node_id": "MDQ6VXNlcjMxNjI0MDQ0", "avatar_url": "https://avatars.githubusercontent.com/u/31624044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwright-arm", "html_url": "https://github.com/jwright-arm", "followers_url": "https://api.github.com/users/jwright-arm/followers", "following_url": "https://api.github.com/users/jwright-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jwright-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwright-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwright-arm/subscriptions", "organizations_url": "https://api.github.com/users/jwright-arm/orgs", "repos_url": "https://api.github.com/users/jwright-arm/repos", "events_url": "https://api.github.com/users/jwright-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jwright-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jwright-arm", "id": 31624044, "node_id": "MDQ6VXNlcjMxNjI0MDQ0", "avatar_url": "https://avatars.githubusercontent.com/u/31624044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwright-arm", "html_url": "https://github.com/jwright-arm", "followers_url": "https://api.github.com/users/jwright-arm/followers", "following_url": "https://api.github.com/users/jwright-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jwright-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwright-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwright-arm/subscriptions", "organizations_url": "https://api.github.com/users/jwright-arm/orgs", "repos_url": "https://api.github.com/users/jwright-arm/repos", "events_url": "https://api.github.com/users/jwright-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jwright-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "9a00ff96fad209ebde56b227d313cad5d769dc55", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9a00ff96fad209ebde56b227d313cad5d769dc55", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9a00ff96fad209ebde56b227d313cad5d769dc55"}], "stats": {"total": 237, "additions": 133, "deletions": 104}, "files": [{"sha": "b787cb9788e758c1f103eab366b7aed4dc457830", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9633e5797637ac395e3ef8048d421a77c84e73a8/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9633e5797637ac395e3ef8048d421a77c84e73a8/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=9633e5797637ac395e3ef8048d421a77c84e73a8", "patch": "@@ -319,6 +319,11 @@\n   BUILTIN_VQ_HSI (QUADOPU_LANE, umlal_hi_lane, 0, NONE)\n   BUILTIN_VQ_HSI (QUADOPU_LANE, umlal_hi_laneq, 0, NONE)\n \n+  BUILTIN_VQ_HSI (QUADOP_LANE, smlsl_hi_lane, 0, NONE)\n+  BUILTIN_VQ_HSI (QUADOP_LANE, smlsl_hi_laneq, 0, NONE)\n+  BUILTIN_VQ_HSI (QUADOPU_LANE, umlsl_hi_lane, 0, NONE)\n+  BUILTIN_VQ_HSI (QUADOPU_LANE, umlsl_hi_laneq, 0, NONE)\n+\n   BUILTIN_VSD_HSI (BINOP, sqdmull, 0, NONE)\n   BUILTIN_VSD_HSI (TERNOP_LANE, sqdmull_lane, 0, NONE)\n   BUILTIN_VSD_HSI (TERNOP_LANE, sqdmull_laneq, 0, NONE)"}, {"sha": "393bab1920100badef21479b2f25cb6e1880c927", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 72, "deletions": 0, "changes": 72, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9633e5797637ac395e3ef8048d421a77c84e73a8/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9633e5797637ac395e3ef8048d421a77c84e73a8/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=9633e5797637ac395e3ef8048d421a77c84e73a8", "patch": "@@ -2377,6 +2377,78 @@\n   [(set_attr \"type\" \"neon_mla_<Vetype>_scalar_long\")]\n )\n \n+(define_insn \"aarch64_<su>mlsl_hi_lane<mode>_insn\"\n+  [(set (match_operand:<VWIDE> 0 \"register_operand\" \"=w\")\n+\t(minus:<VWIDE>\n+\t  (match_operand:<VWIDE> 1 \"register_operand\" \"0\")\n+\t  (mult:<VWIDE>\n+\t    (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>\n+\t      (match_operand:VQ_HSI 2 \"register_operand\" \"w\")\n+\t      (match_operand:VQ_HSI 3 \"vect_par_cnst_hi_half\" \"\")))\n+\t    (ANY_EXTEND:<VWIDE> (vec_duplicate:<VHALF>\n+\t      (vec_select:<VEL>\n+\t\t(match_operand:<VCOND> 4 \"register_operand\" \"<vwx>\")\n+\t\t(parallel [(match_operand:SI 5 \"immediate_operand\" \"i\")]))))\n+\t  )))]\n+  \"TARGET_SIMD\"\n+  {\n+    operands[5] = aarch64_endian_lane_rtx (<VCOND>mode, INTVAL (operands[5]));\n+    return \"<su>mlsl2\\\\t%0.<Vwtype>, %2.<Vtype>, %4.<Vetype>[%5]\";\n+  }\n+  [(set_attr \"type\" \"neon_mla_<Vetype>_scalar_long\")]\n+)\n+\n+(define_expand \"aarch64_<su>mlsl_hi_lane<mode>\"\n+  [(match_operand:<VWIDE> 0 \"register_operand\")\n+   (match_operand:<VWIDE> 1 \"register_operand\")\n+   (ANY_EXTEND:<VWIDE>(match_operand:VQ_HSI 2 \"register_operand\"))\n+   (match_operand:<VCOND> 3 \"register_operand\")\n+   (match_operand:SI 4 \"immediate_operand\")]\n+  \"TARGET_SIMD\"\n+{\n+  rtx p = aarch64_simd_vect_par_cnst_half (<MODE>mode, <nunits>, true);\n+  emit_insn (gen_aarch64_<su>mlsl_hi_lane<mode>_insn (operands[0],\n+\t     operands[1], operands[2], p, operands[3], operands[4]));\n+  DONE;\n+}\n+)\n+\n+(define_insn \"aarch64_<su>mlsl_hi_laneq<mode>_insn\"\n+  [(set (match_operand:<VWIDE> 0 \"register_operand\" \"=w\")\n+\t(minus:<VWIDE>\n+\t  (match_operand:<VWIDE> 1 \"register_operand\" \"0\")\n+\t  (mult:<VWIDE>\n+\t    (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>\n+\t      (match_operand:VQ_HSI 2 \"register_operand\" \"w\")\n+\t      (match_operand:VQ_HSI 3 \"vect_par_cnst_hi_half\" \"\")))\n+\t    (ANY_EXTEND:<VWIDE> (vec_duplicate:<VHALF>\n+\t      (vec_select:<VEL>\n+\t\t(match_operand:<VCONQ> 4 \"register_operand\" \"<vwx>\")\n+\t\t(parallel [(match_operand:SI 5 \"immediate_operand\" \"i\")]))))\n+\t  )))]\n+  \"TARGET_SIMD\"\n+  {\n+    operands[5] = aarch64_endian_lane_rtx (<VCONQ>mode, INTVAL (operands[5]));\n+    return \"<su>mlsl2\\\\t%0.<Vwtype>, %2.<Vtype>, %4.<Vetype>[%5]\";\n+  }\n+  [(set_attr \"type\" \"neon_mla_<Vetype>_scalar_long\")]\n+)\n+\n+(define_expand \"aarch64_<su>mlsl_hi_laneq<mode>\"\n+  [(match_operand:<VWIDE> 0 \"register_operand\")\n+   (match_operand:<VWIDE> 1 \"register_operand\")\n+   (ANY_EXTEND:<VWIDE>(match_operand:VQ_HSI 2 \"register_operand\"))\n+   (match_operand:<VCONQ> 3 \"register_operand\")\n+   (match_operand:SI 4 \"immediate_operand\")]\n+  \"TARGET_SIMD\"\n+{\n+  rtx p = aarch64_simd_vect_par_cnst_half (<MODE>mode, <nunits>, true);\n+  emit_insn (gen_aarch64_<su>mlsl_hi_laneq<mode>_insn (operands[0],\n+\t     operands[1], operands[2], p, operands[3], operands[4]));\n+  DONE;\n+}\n+)\n+\n ;; FP vector operations.\n ;; AArch64 AdvSIMD supports single-precision (32-bit) and \n ;; double-precision (64-bit) floating-point data types and arithmetic as"}, {"sha": "d50bd65c497a02ea67c4aa02aff29f1ae7223b4e", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 56, "deletions": 104, "changes": 160, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9633e5797637ac395e3ef8048d421a77c84e73a8/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9633e5797637ac395e3ef8048d421a77c84e73a8/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=9633e5797637ac395e3ef8048d421a77c84e73a8", "patch": "@@ -7598,117 +7598,69 @@ vmls_u32 (uint32x2_t __a, uint32x2_t __b, uint32x2_t __c)\n                                                  (int32x2_t) __c);\n }\n \n-#define vmlsl_high_lane_s16(a, b, c, d)                                 \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x4_t c_ = (c);                                              \\\n-       int16x8_t b_ = (b);                                              \\\n-       int32x4_t a_ = (a);                                              \\\n-       int32x4_t result;                                                \\\n-       __asm__ (\"smlsl2 %0.4s, %2.8h, %3.h[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline int32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlsl_high_lane_s16(int32x4_t __a, int16x8_t __b, int16x4_t __v,\n+\t\t    const int __lane)\n+{\n+  return __builtin_aarch64_smlsl_hi_lanev8hi (__a, __b, __v, __lane);\n+}\n \n-#define vmlsl_high_lane_s32(a, b, c, d)                                 \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x2_t c_ = (c);                                              \\\n-       int32x4_t b_ = (b);                                              \\\n-       int64x2_t a_ = (a);                                              \\\n-       int64x2_t result;                                                \\\n-       __asm__ (\"smlsl2 %0.2d, %2.4s, %3.s[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline int64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlsl_high_lane_s32(int64x2_t __a, int32x4_t __b, int32x2_t __v,\n+\t\t    const int __lane)\n+{\n+  return __builtin_aarch64_smlsl_hi_lanev4si (__a, __b, __v, __lane);\n+}\n \n-#define vmlsl_high_lane_u16(a, b, c, d)                                 \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x4_t c_ = (c);                                             \\\n-       uint16x8_t b_ = (b);                                             \\\n-       uint32x4_t a_ = (a);                                             \\\n-       uint32x4_t result;                                               \\\n-       __asm__ (\"umlsl2 %0.4s, %2.8h, %3.h[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline uint32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlsl_high_lane_u16(uint32x4_t __a, uint16x8_t __b, uint16x4_t __v,\n+\t\t    const int __lane)\n+{\n+  return __builtin_aarch64_umlsl_hi_lanev8hi_uuuus (__a, __b, __v, __lane);\n+}\n \n-#define vmlsl_high_lane_u32(a, b, c, d)                                 \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x2_t c_ = (c);                                             \\\n-       uint32x4_t b_ = (b);                                             \\\n-       uint64x2_t a_ = (a);                                             \\\n-       uint64x2_t result;                                               \\\n-       __asm__ (\"umlsl2 %0.2d, %2.4s, %3.s[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline uint64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlsl_high_lane_u32(uint64x2_t __a, uint32x4_t __b, uint32x2_t __v,\n+\t\t    const int __lane)\n+{\n+  return __builtin_aarch64_umlsl_hi_lanev4si_uuuus (__a, __b, __v, __lane);\n+}\n \n-#define vmlsl_high_laneq_s16(a, b, c, d)                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x8_t c_ = (c);                                              \\\n-       int16x8_t b_ = (b);                                              \\\n-       int32x4_t a_ = (a);                                              \\\n-       int32x4_t result;                                                \\\n-       __asm__ (\"smlsl2 %0.4s, %2.8h, %3.h[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline int32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlsl_high_laneq_s16(int32x4_t __a, int16x8_t __b, int16x8_t __v,\n+\t\t     const int __lane)\n+{\n+  return __builtin_aarch64_smlsl_hi_laneqv8hi (__a, __b, __v, __lane);\n+}\n \n-#define vmlsl_high_laneq_s32(a, b, c, d)                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x4_t c_ = (c);                                              \\\n-       int32x4_t b_ = (b);                                              \\\n-       int64x2_t a_ = (a);                                              \\\n-       int64x2_t result;                                                \\\n-       __asm__ (\"smlsl2 %0.2d, %2.4s, %3.s[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline int64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlsl_high_laneq_s32(int64x2_t __a, int32x4_t __b, int32x4_t __v,\n+\t\t     const int __lane)\n+{\n+  return __builtin_aarch64_smlsl_hi_laneqv4si (__a, __b, __v, __lane);\n+}\n \n-#define vmlsl_high_laneq_u16(a, b, c, d)                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x8_t c_ = (c);                                             \\\n-       uint16x8_t b_ = (b);                                             \\\n-       uint32x4_t a_ = (a);                                             \\\n-       uint32x4_t result;                                               \\\n-       __asm__ (\"umlsl2 %0.4s, %2.8h, %3.h[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline uint32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlsl_high_laneq_u16(uint32x4_t __a, uint16x8_t __b, uint16x8_t __v,\n+\t\t     const int __lane)\n+{\n+  return __builtin_aarch64_umlsl_hi_laneqv8hi_uuuus (__a, __b, __v, __lane);\n+}\n \n-#define vmlsl_high_laneq_u32(a, b, c, d)                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x4_t c_ = (c);                                             \\\n-       uint32x4_t b_ = (b);                                             \\\n-       uint64x2_t a_ = (a);                                             \\\n-       uint64x2_t result;                                               \\\n-       __asm__ (\"umlsl2 %0.2d, %2.4s, %3.s[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline uint64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlsl_high_laneq_u32(uint64x2_t __a, uint32x4_t __b, uint32x4_t __v,\n+\t\t     const int __lane)\n+{\n+  return __builtin_aarch64_umlsl_hi_laneqv4si_uuuus (__a, __b, __v, __lane);\n+}\n \n __extension__ extern __inline int32x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))"}]}