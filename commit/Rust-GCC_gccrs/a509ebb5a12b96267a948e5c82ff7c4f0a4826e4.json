{"sha": "a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YTUwOWViYjVhMTJiOTYyNjdhOTQ4ZTVjODJmZjdjNGYwYTQ4MjZlNA==", "commit": {"author": {"name": "Razya Ladelsky", "email": "razya@gcc.gnu.org", "date": "2007-10-29T11:05:04Z"}, "committer": {"name": "Razya Ladelsky", "email": "razya@gcc.gnu.org", "date": "2007-10-29T11:05:04Z"}, "message": "2007-09-23  Razya Ladelsky\n            Zdenek Dvorak\n\n        OMP_ATOMIC Changes,\n        Reduction support for automatic parallelization.\n\n        * expr.c (expand_expr_real_1): Add cases for OMP_ATOMIC_LOAD,\n        OMP_ATOMIC_STORE.\n        * Makefile.in: Add dependencies to expr.o, tree-parloops.o, omp-low.o\n        * tree-pretty-print.c (dump_generic_node): Add OMP_ATOMIC_LOAD\n        and OMP_ATOMIC_STORE.\n        * tree.h (OMP_DIRECTIVE_P): Add OMP_ATOMIC_LOAD,\n        OMP_ATOMIC_STORE.\n        * gimple-low.c (lower_stmt): Same.\n        * gimplify.c (gimplify_expr): Same.\n        (gimplify_omp_atomic_fetch_op, gimplify_omp_atomic_pipeline,\n        gimplify_omp_atomic_mutex): Remove.\n        (gimplify_omp_atomic): Change it to simply gimplify the\n        statement instead of expanding it.\n        * omp-low.c: Add includes to optabs.h, cfgloop.h.\n        (expand_omp_atomic, expand_omp_atomic_pipeline,\n        goa_stabilize_expr, expand_omp_atomic_mutex,\n        expand_omp_atomic_fetch_op): New functions to implement\n        expansion of OMP_ATOMIC.\n        (expand_omp, build_omp_regions_1): Add support for\n        OMP_ATOMIC_LOAD/OMP_ATOMIC_STORE.\n        * tree-cfg.c (make_edges): add case for OMP_ATOMIC_LOAD,\n        OMP_ATOMIC_STORE.\n        * tree-gimple.c (is_gimple_stmt): Add OMP_ATOMIC_LOAD,\n        OMP_ATOMIC_STORE.\n        * tree-parloops.c: add include to tree-vectorizer.h.\n        (reduction_info): New structure for reduction.\n        (reduction_list): New list to represent list of reductions\n        per loop.\n        (struct data_arg): New helper structure for reduction.\n        (reduction_info_hash, reduction_info_eq, reduction_phi,\n        initialize_reductions,\n        create_call_for_reduction, create_phi_for_local_result,\n        create_call_for_reduction_1, create_loads_for_reductions,\n        create_final_loads_for_reduction): New functions.\n        (loop_parallel_p): Identify reductions, add reduction_list parameter.\n        (separate_decls_in_loop_name): Support reduction variables.\n        (separate_decls_in_loop): Add reduction_list and ld_st_data arguments,\n        call create_loads_for_reduction for each reduction.\n        (canonicalize_loop_ivs): Identify reductions, add reduction_list\n        parameter.\n        (transform_to_exit_first_loop): Add reduction support, add\n        reduction_list parameter.\n        (gen_parallel_loop): Add reduction_list parameter. Add call\n        separate_decls_in_loop with\n        the new argument. Traverse reductions and call\n        initialize_reductions, create_call_for_reduction.\n        (parallelize_loops): Create and delete the reduction list.\n        (add_field_for_name): Change use of data parameter. Add fields for\n        reductions.\n        * tree-vectorizer.h (vect_analyze_loop_form): Add declaration.\n        * tree-vect-analyze.c (vect_analyze_loop_form): export it.\n        * tree.def: Add definitions for OMP_ATOMIC_LOAD,\n        OMP_ATOMIC_STORE.\n        * tree-inline.c (estimate_num_insns_1): add cases for\n        OMP_ATOMIC_LOAD, OMP_ATOMIC_STORE.\n        * tree-cfg.c (make_edges): Add OMP_ATOMIC_LOAD,\n        OMP_ATOMIC_STORE.\n        * tree-ssa-operands.c (get_addr_dereference_operands):\n        New function. Subroutine of get_indirect_ref_operands.\n        (get_indirect_ref_operands): Call get_addr_dereference_operands.\n        (get_expr_operands): Support OMP_ATOMIC_LOAD, OMP_ATOMIC_STORE.\n\nFrom-SVN: r129716", "tree": {"sha": "6b8a35dcec87c3829ed38ac34249604c36916f9c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6b8a35dcec87c3829ed38ac34249604c36916f9c"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/comments", "author": null, "committer": null, "parents": [{"sha": "245a5fe5c0ae5dd48f7adc806117264543f39fdd", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/245a5fe5c0ae5dd48f7adc806117264543f39fdd", "html_url": "https://github.com/Rust-GCC/gccrs/commit/245a5fe5c0ae5dd48f7adc806117264543f39fdd"}], "stats": {"total": 1770, "additions": 1378, "deletions": 392}, "files": [{"sha": "04087012430f0f738c1f41cad12ae61f88e58698", "filename": "gcc/ChangeLog", "status": "modified", "additions": 69, "deletions": 0, "changes": 69, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -1,3 +1,72 @@\n+2007-09-23  Razya Ladelsky\n+            Zdenek Dvorak\n+\n+        OMP_ATOMIC Changes,\n+        Reduction support for automatic parallelization.\n+\n+        * expr.c (expand_expr_real_1): Add cases for OMP_ATOMIC_LOAD,\n+        OMP_ATOMIC_STORE.\n+        * Makefile.in: Add dependencies to expr.o, tree-parloops.o, omp-low.o\n+        * tree-pretty-print.c (dump_generic_node): Add OMP_ATOMIC_LOAD\n+        and OMP_ATOMIC_STORE.\n+        * tree.h (OMP_DIRECTIVE_P): Add OMP_ATOMIC_LOAD,\n+        OMP_ATOMIC_STORE.\n+        * gimple-low.c (lower_stmt): Same.\n+        * gimplify.c (gimplify_expr): Same.\n+        (gimplify_omp_atomic_fetch_op, gimplify_omp_atomic_pipeline,\n+        gimplify_omp_atomic_mutex): Remove.\n+        (gimplify_omp_atomic): Change it to simply gimplify the\n+        statement instead of expanding it.\n+        * omp-low.c: Add includes to optabs.h, cfgloop.h.\n+        (expand_omp_atomic, expand_omp_atomic_pipeline,\n+        goa_stabilize_expr, expand_omp_atomic_mutex,\n+        expand_omp_atomic_fetch_op): New functions to implement\n+        expansion of OMP_ATOMIC.\n+        (expand_omp, build_omp_regions_1): Add support for\n+        OMP_ATOMIC_LOAD/OMP_ATOMIC_STORE.\n+        * tree-cfg.c (make_edges): add case for OMP_ATOMIC_LOAD,\n+        OMP_ATOMIC_STORE.\n+        * tree-gimple.c (is_gimple_stmt): Add OMP_ATOMIC_LOAD,\n+        OMP_ATOMIC_STORE.\n+        * tree-parloops.c: add include to tree-vectorizer.h.\n+        (reduction_info): New structure for reduction.\n+        (reduction_list): New list to represent list of reductions\n+        per loop.\n+        (struct data_arg): New helper structure for reduction.\n+        (reduction_info_hash, reduction_info_eq, reduction_phi,\n+        initialize_reductions,\n+        create_call_for_reduction, create_phi_for_local_result,\n+        create_call_for_reduction_1, create_loads_for_reductions,\n+        create_final_loads_for_reduction): New functions.\n+        (loop_parallel_p): Identify reductions, add reduction_list parameter.\n+        (separate_decls_in_loop_name): Support reduction variables.\n+        (separate_decls_in_loop): Add reduction_list and ld_st_data arguments,\n+        call create_loads_for_reduction for each reduction.\n+        (canonicalize_loop_ivs): Identify reductions, add reduction_list\n+        parameter.\n+        (transform_to_exit_first_loop): Add reduction support, add\n+        reduction_list parameter.\n+        (gen_parallel_loop): Add reduction_list parameter. Add call\n+        separate_decls_in_loop with\n+        the new argument. Traverse reductions and call\n+        initialize_reductions, create_call_for_reduction.\n+        (parallelize_loops): Create and delete the reduction list.\n+        (add_field_for_name): Change use of data parameter. Add fields for\n+        reductions.\n+        * tree-vectorizer.h (vect_analyze_loop_form): Add declaration.\n+        * tree-vect-analyze.c (vect_analyze_loop_form): export it.\n+        * tree.def: Add definitions for OMP_ATOMIC_LOAD,\n+        OMP_ATOMIC_STORE.\n+        * tree-inline.c (estimate_num_insns_1): add cases for\n+        OMP_ATOMIC_LOAD, OMP_ATOMIC_STORE.\n+        * tree-cfg.c (make_edges): Add OMP_ATOMIC_LOAD,\n+        OMP_ATOMIC_STORE.\n+        * tree-ssa-operands.c (get_addr_dereference_operands):\n+        New function. Subroutine of get_indirect_ref_operands.\n+        (get_indirect_ref_operands): Call get_addr_dereference_operands.\n+        (get_expr_operands): Support OMP_ATOMIC_LOAD, OMP_ATOMIC_STORE.\n+\n+\n 2007-10-29  Hans-Peter Nilsson  <hp@axis.com>\n \n \t* config/cris/cris.c: Include df.h."}, {"sha": "cc47ed036557e69f05e3c40384ff93dc720b57b4", "filename": "gcc/Makefile.in", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -2234,7 +2234,7 @@ gimple-low.o : gimple-low.c $(CONFIG_H) $(SYSTEM_H) $(TREE_H) \\\n omp-low.o : omp-low.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(TREE_H) \\\n    $(RTL_H) $(TREE_GIMPLE_H) $(TREE_INLINE_H) langhooks.h $(DIAGNOSTIC_H) \\\n    $(TREE_FLOW_H) $(TIMEVAR_H) $(FLAGS_H) $(EXPR_H) toplev.h tree-pass.h \\\n-   $(GGC_H) $(SPLAY_TREE_H)\n+   $(GGC_H) $(SPLAY_TREE_H) $(OPTABS_H) $(CFGLOOP_H)\n tree-browser.o : tree-browser.c tree-browser.def $(CONFIG_H) $(SYSTEM_H) \\\n    $(TREE_H) $(TREE_INLINE_H) $(DIAGNOSTIC_H) $(HASHTAB_H) \\\n    $(TM_H) coretypes.h\n@@ -2278,7 +2278,8 @@ tree-loop-linear.o: tree-loop-linear.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n    $(TARGET_H) tree-chrec.h $(OBSTACK_H)\n tree-parloops.o: tree-parloops.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    $(TREE_FLOW_H) $(TREE_H) $(RTL_H) $(CFGLOOP_H) $(TREE_DATA_REF_H) $(GGC_H) \\\n-   $(DIAGNOSTIC_H) tree-pass.h $(SCEV_H) langhooks.h gt-tree-parloops.h\n+   $(DIAGNOSTIC_H) tree-pass.h $(SCEV_H) langhooks.h gt-tree-parloops.h \\\n+   tree-vectorizer.h\n tree-stdarg.o: tree-stdarg.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    $(TREE_H) $(FUNCTION_H) $(DIAGNOSTIC_H) $(TREE_FLOW_H) tree-pass.h \\\n    tree-stdarg.h $(TARGET_H) langhooks.h\n@@ -2390,7 +2391,7 @@ expr.o : expr.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    typeclass.h hard-reg-set.h toplev.h hard-reg-set.h except.h reload.h \\\n    $(GGC_H) langhooks.h intl.h $(TM_P_H) $(REAL_H) $(TARGET_H) \\\n    tree-iterator.h gt-expr.h $(MACHMODE_H) $(TIMEVAR_H) $(TREE_FLOW_H) \\\n-   tree-pass.h $(DF_H)\n+   tree-pass.h $(DF_H) $(DIAGNOSTIC_H)\n dojump.o : dojump.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) $(TREE_H) \\\n    $(FLAGS_H) $(FUNCTION_H) $(EXPR_H) $(OPTABS_H) $(INSN_ATTR_H) insn-config.h \\\n    langhooks.h $(GGC_H) gt-dojump.h"}, {"sha": "46cca7aced560078674f0fa69a7b08ba1ec2548c", "filename": "gcc/expr.c", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Fexpr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Fexpr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -53,6 +53,7 @@ along with GCC; see the file COPYING3.  If not see\n #include \"target.h\"\n #include \"timevar.h\"\n #include \"df.h\"\n+#include \"diagnostic.h\"\n \n /* Decide whether a function's arguments should be processed\n    from first to last or from last to first.\n@@ -9360,6 +9361,13 @@ expand_expr_real_1 (tree exp, rtx target, enum machine_mode tmode,\n \tgoto binop;\n       }\n \n+    case OMP_ATOMIC_LOAD:\n+    case OMP_ATOMIC_STORE:\n+      /* OMP expansion is not run when there were errors, so these codes\n+\t\t  can get here.  */\n+      gcc_assert (errorcount != 0);\n+      return NULL_RTX;\n+\n     default:\n       return lang_hooks.expand_expr (exp, original_target, tmode,\n \t\t\t\t     modifier, alt_rtl);"}, {"sha": "302efb55b32d854b3f49330b45aeeb61b04ec76d", "filename": "gcc/gimple-low.c", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Fgimple-low.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Fgimple-low.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-low.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -251,6 +251,8 @@ lower_stmt (tree_stmt_iterator *tsi, struct lower_data *data)\n     case OMP_ORDERED:\n     case OMP_CRITICAL:\n     case OMP_RETURN:\n+    case OMP_ATOMIC_LOAD:\n+    case OMP_ATOMIC_STORE:\n     case OMP_CONTINUE:\n       break;\n "}, {"sha": "038337732b534d8c47e1ceab74e0c1c4df1c303a", "filename": "gcc/gimplify.c", "status": "modified", "additions": 21, "deletions": 235, "changes": 256, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Fgimplify.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Fgimplify.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimplify.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -5273,7 +5273,7 @@ gimplify_omp_workshare (tree *expr_p, tree *pre_p)\n    EXPR is this stabilized form.  */\n \n static bool\n-goa_lhs_expr_p (const_tree expr, const_tree addr)\n+goa_lhs_expr_p (tree expr, tree addr)\n {\n   /* Also include casts to other type variants.  The C front end is fond\n      of adding these for e.g. volatile variables.  This is like \n@@ -5293,66 +5293,7 @@ goa_lhs_expr_p (const_tree expr, const_tree addr)\n   return false;\n }\n \n-/* A subroutine of gimplify_omp_atomic.  Attempt to implement the atomic\n-   operation as a __sync_fetch_and_op builtin.  INDEX is log2 of the\n-   size of the data type, and thus usable to find the index of the builtin\n-   decl.  Returns GS_UNHANDLED if the expression is not of the proper form.  */\n-\n-static enum gimplify_status\n-gimplify_omp_atomic_fetch_op (tree *expr_p, tree addr, tree rhs, int index)\n-{\n-  enum built_in_function base;\n-  tree decl, itype;\n-  enum insn_code *optab;\n-\n-  /* Check for one of the supported fetch-op operations.  */\n-  switch (TREE_CODE (rhs))\n-    {\n-    case POINTER_PLUS_EXPR:\n-    case PLUS_EXPR:\n-      base = BUILT_IN_FETCH_AND_ADD_N;\n-      optab = sync_add_optab;\n-      break;\n-    case MINUS_EXPR:\n-      base = BUILT_IN_FETCH_AND_SUB_N;\n-      optab = sync_add_optab;\n-      break;\n-    case BIT_AND_EXPR:\n-      base = BUILT_IN_FETCH_AND_AND_N;\n-      optab = sync_and_optab;\n-      break;\n-    case BIT_IOR_EXPR:\n-      base = BUILT_IN_FETCH_AND_OR_N;\n-      optab = sync_ior_optab;\n-      break;\n-    case BIT_XOR_EXPR:\n-      base = BUILT_IN_FETCH_AND_XOR_N;\n-      optab = sync_xor_optab;\n-      break;\n-    default:\n-      return GS_UNHANDLED;\n-    }\n-\n-  /* Make sure the expression is of the proper form.  */\n-  if (goa_lhs_expr_p (TREE_OPERAND (rhs, 0), addr))\n-    rhs = TREE_OPERAND (rhs, 1);\n-  else if (commutative_tree_code (TREE_CODE (rhs))\n-\t   && goa_lhs_expr_p (TREE_OPERAND (rhs, 1), addr))\n-    rhs = TREE_OPERAND (rhs, 0);\n-  else\n-    return GS_UNHANDLED;\n-\n-  decl = built_in_decls[base + index + 1];\n-  itype = TREE_TYPE (TREE_TYPE (decl));\n-\n-  if (optab[TYPE_MODE (itype)] == CODE_FOR_nothing)\n-    return GS_UNHANDLED;\n-\n-  *expr_p = build_call_expr (decl, 2, addr, fold_convert (itype, rhs));\n-  return GS_OK;\n-}\n-\n-/* A subroutine of gimplify_omp_atomic_pipeline.  Walk *EXPR_P and replace\n+/* Walk *EXPR_P and replace\n    appearances of *LHS_ADDR with LHS_VAR.  If an expression does not involve\n    the lhs, evaluate it into a temporary.  Return 1 if the lhs appeared as\n    a subexpression, 0 if it did not, or -1 if an error was encountered.  */\n@@ -5396,144 +5337,6 @@ goa_stabilize_expr (tree *expr_p, tree *pre_p, tree lhs_addr, tree lhs_var)\n   return saw_lhs;\n }\n \n-/* A subroutine of gimplify_omp_atomic.  Implement the atomic operation as:\n-\n-\toldval = *addr;\n-      repeat:\n-\tnewval = rhs;\t// with oldval replacing *addr in rhs\n-\toldval = __sync_val_compare_and_swap (addr, oldval, newval);\n-\tif (oldval != newval)\n-\t  goto repeat;\n-\n-   INDEX is log2 of the size of the data type, and thus usable to find the\n-   index of the builtin decl.  */\n-\n-static enum gimplify_status\n-gimplify_omp_atomic_pipeline (tree *expr_p, tree *pre_p, tree addr,\n-\t\t\t      tree rhs, int index)\n-{\n-  tree oldval, oldival, oldival2, newval, newival, label;\n-  tree type, itype, cmpxchg, x, iaddr;\n-\n-  cmpxchg = built_in_decls[BUILT_IN_VAL_COMPARE_AND_SWAP_N + index + 1];\n-  type = TYPE_MAIN_VARIANT (TREE_TYPE (TREE_TYPE (addr)));\n-  itype = TREE_TYPE (TREE_TYPE (cmpxchg));\n-\n-  if (sync_compare_and_swap[TYPE_MODE (itype)] == CODE_FOR_nothing)\n-    return GS_UNHANDLED;\n-\n-  oldval = create_tmp_var (type, NULL);\n-  newval = create_tmp_var (type, NULL);\n-\n-  /* Precompute as much of RHS as possible.  In the same walk, replace\n-     occurrences of the lhs value with our temporary.  */\n-  if (goa_stabilize_expr (&rhs, pre_p, addr, oldval) < 0)\n-    return GS_ERROR;\n-\n-  x = build_fold_indirect_ref (addr);\n-  x = build_gimple_modify_stmt (oldval, x);\n-  gimplify_and_add (x, pre_p);\n-\n-  /* For floating-point values, we'll need to view-convert them to integers\n-     so that we can perform the atomic compare and swap.  Simplify the \n-     following code by always setting up the \"i\"ntegral variables.  */\n-  if (INTEGRAL_TYPE_P (type) || POINTER_TYPE_P (type))\n-    {\n-      oldival = oldval;\n-      newival = newval;\n-      iaddr = addr;\n-    }\n-  else\n-    {\n-      oldival = create_tmp_var (itype, NULL);\n-      newival = create_tmp_var (itype, NULL);\n-\n-      x = build1 (VIEW_CONVERT_EXPR, itype, oldval);\n-      x = build_gimple_modify_stmt (oldival, x);\n-      gimplify_and_add (x, pre_p);\n-      iaddr = fold_convert (build_pointer_type (itype), addr);\n-    }\n-\n-  oldival2 = create_tmp_var (itype, NULL);\n-\n-  label = create_artificial_label ();\n-  x = build1 (LABEL_EXPR, void_type_node, label);\n-  gimplify_and_add (x, pre_p);\n-\n-  x = build_gimple_modify_stmt (newval, rhs);\n-  gimplify_and_add (x, pre_p);\n-\n-  if (newval != newival)\n-    {\n-      x = build1 (VIEW_CONVERT_EXPR, itype, newval);\n-      x = build_gimple_modify_stmt (newival, x);\n-      gimplify_and_add (x, pre_p);\n-    }\n-\n-  x = build_gimple_modify_stmt (oldival2, fold_convert (itype, oldival));\n-  gimplify_and_add (x, pre_p);\n-\n-  x = build_call_expr (cmpxchg, 3, iaddr, fold_convert (itype, oldival),\n-\t\t       fold_convert (itype, newival));\n-  if (oldval == oldival)\n-    x = fold_convert (type, x);\n-  x = build_gimple_modify_stmt (oldival, x);\n-  gimplify_and_add (x, pre_p);\n-\n-  /* For floating point, be prepared for the loop backedge.  */\n-  if (oldval != oldival)\n-    {\n-      x = build1 (VIEW_CONVERT_EXPR, type, oldival);\n-      x = build_gimple_modify_stmt (oldval, x);\n-      gimplify_and_add (x, pre_p);\n-    }\n-\n-  /* Note that we always perform the comparison as an integer, even for\n-     floating point.  This allows the atomic operation to properly \n-     succeed even with NaNs and -0.0.  */\n-  x = build3 (COND_EXPR, void_type_node,\n-\t      build2 (NE_EXPR, boolean_type_node,\n-\t\t      fold_convert (itype, oldival), oldival2),\n-\t      build1 (GOTO_EXPR, void_type_node, label), NULL);\n-  gimplify_and_add (x, pre_p);\n-\n-  *expr_p = NULL;\n-  return GS_ALL_DONE;\n-}\n-\n-/* A subroutine of gimplify_omp_atomic.  Implement the atomic operation as:\n-\n-\tGOMP_atomic_start ();\n-\t*addr = rhs;\n-\tGOMP_atomic_end ();\n-\n-   The result is not globally atomic, but works so long as all parallel\n-   references are within #pragma omp atomic directives.  According to\n-   responses received from omp@openmp.org, appears to be within spec.\n-   Which makes sense, since that's how several other compilers handle\n-   this situation as well.  */\n-\n-static enum gimplify_status\n-gimplify_omp_atomic_mutex (tree *expr_p, tree *pre_p, tree addr, tree rhs)\n-{\n-  tree t;\n-\n-  t = built_in_decls[BUILT_IN_GOMP_ATOMIC_START];\n-  t = build_call_expr (t, 0);\n-  gimplify_and_add (t, pre_p);\n-\n-  t = build_fold_indirect_ref (addr);\n-  t = build_gimple_modify_stmt (t, rhs);\n-  gimplify_and_add (t, pre_p);\n-  \n-  t = built_in_decls[BUILT_IN_GOMP_ATOMIC_END];\n-  t = build_call_expr (t, 0);\n-  gimplify_and_add (t, pre_p);\n-\n-  *expr_p = NULL;\n-  return GS_ALL_DONE;\n-}\n-\n /* Gimplify an OMP_ATOMIC statement.  */\n \n static enum gimplify_status\n@@ -5542,46 +5345,26 @@ gimplify_omp_atomic (tree *expr_p, tree *pre_p)\n   tree addr = TREE_OPERAND (*expr_p, 0);\n   tree rhs = TREE_OPERAND (*expr_p, 1);\n   tree type = TYPE_MAIN_VARIANT (TREE_TYPE (TREE_TYPE (addr)));\n-  HOST_WIDE_INT index;\n+  tree tmp_load, load, store;\n \n-  /* Make sure the type is one of the supported sizes.  */\n-  index = tree_low_cst (TYPE_SIZE_UNIT (type), 1);\n-  index = exact_log2 (index);\n-  if (index >= 0 && index <= 4)\n-    {\n-      enum gimplify_status gs;\n-      unsigned int align;\n-\n-      if (DECL_P (TREE_OPERAND (addr, 0)))\n-\talign = DECL_ALIGN_UNIT (TREE_OPERAND (addr, 0));\n-      else if (TREE_CODE (TREE_OPERAND (addr, 0)) == COMPONENT_REF\n-\t       && TREE_CODE (TREE_OPERAND (TREE_OPERAND (addr, 0), 1))\n-\t\t  == FIELD_DECL)\n-\talign = DECL_ALIGN_UNIT (TREE_OPERAND (TREE_OPERAND (addr, 0), 1));\n-      else\n-\talign = TYPE_ALIGN_UNIT (type);\n+   tmp_load = create_tmp_var (type, NULL);\n+   if (goa_stabilize_expr (&rhs, pre_p, addr, tmp_load) < 0)\n+     return GS_ERROR;\n \n-      /* __sync builtins require strict data alignment.  */\n-      if (exact_log2 (align) >= index)\n-\t{\n-\t  /* When possible, use specialized atomic update functions.  */\n-\t  if (INTEGRAL_TYPE_P (type) || POINTER_TYPE_P (type))\n-\t    {\n-\t      gs = gimplify_omp_atomic_fetch_op (expr_p, addr, rhs, index);\n-\t      if (gs != GS_UNHANDLED)\n-\t\treturn gs;\n-\t    }\n+   if (gimplify_expr (&addr, pre_p, NULL, is_gimple_val, fb_rvalue)\n+       != GS_ALL_DONE)\n+     return GS_ERROR;\n \n-\t  /* If we don't have specialized __sync builtins, try and implement\n-\t     as a compare and swap loop.  */\n-\t  gs = gimplify_omp_atomic_pipeline (expr_p, pre_p, addr, rhs, index);\n-\t  if (gs != GS_UNHANDLED)\n-\t    return gs;\n-\t}\n-    }\n+   load = build2 (OMP_ATOMIC_LOAD, void_type_node, tmp_load, addr);\n+   append_to_statement_list (load, pre_p);\n+   if (gimplify_expr (&rhs, pre_p, NULL, is_gimple_val, fb_rvalue)\n+       != GS_ALL_DONE)\n+     return GS_ERROR;\n+   store = build1 (OMP_ATOMIC_STORE, void_type_node, rhs);\n+   *expr_p = store;\n+\n+   return GS_ALL_DONE;\n \n-  /* The ultimate fallback is wrapping the operation in a mutex.  */\n-  return gimplify_omp_atomic_mutex (expr_p, pre_p, addr, rhs);\n }\n \n /*  Gimplifies the expression tree pointed to by EXPR_P.  Return 0 if\n@@ -6057,6 +5840,9 @@ gimplify_expr (tree *expr_p, tree *pre_p, tree *post_p,\n \n \tcase OMP_RETURN:\n \tcase OMP_CONTINUE:\n+        case OMP_ATOMIC_LOAD:\n+        case OMP_ATOMIC_STORE:\n+\n \t  ret = GS_ALL_DONE;\n \t  break;\n "}, {"sha": "20f36e047b1e8b41bf990203f7ebfb311d0f7cab", "filename": "gcc/omp-low.c", "status": "modified", "additions": 369, "deletions": 4, "changes": 373, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Fomp-low.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Fomp-low.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fomp-low.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -41,7 +41,8 @@ along with GCC; see the file COPYING3.  If not see\n #include \"ggc.h\"\n #include \"except.h\"\n #include \"splay-tree.h\"\n-\n+#include \"optabs.h\"\n+#include \"cfgloop.h\"\n \n /* Lowering of OpenMP parallel and workshare constructs proceeds in two \n    phases.  The first phase scans the function looking for OMP statements\n@@ -2597,7 +2598,7 @@ expand_omp_parallel (struct omp_region *region)\n       rebuild_cgraph_edges ();\n       pop_cfun ();\n     }\n-\n+  \n   /* Emit a library call to launch the children threads.  */\n   expand_parallel_call (region, new_bb, entry_stmt, ws_args);\n   update_ssa (TODO_update_ssa_only_virtuals);\n@@ -3540,6 +3541,355 @@ expand_omp_synch (struct omp_region *region)\n     }\n }\n \n+/* A subroutine of expand_omp_atomic.  Attempt to implement the atomic\n+   operation as a __sync_fetch_and_op builtin.  INDEX is log2 of the\n+   size of the data type, and thus usable to find the index of the builtin\n+   decl.  Returns false if the expression is not of the proper form.  */\n+\n+static bool\n+expand_omp_atomic_fetch_op (basic_block load_bb,\n+\t\t\t    tree addr, tree loaded_val,\n+\t\t\t    tree stored_val, int index)\n+{\n+  enum built_in_function base;\n+  tree decl, itype, call;\n+  enum insn_code *optab;\n+  tree rhs;\n+  basic_block store_bb = single_succ (load_bb);\n+  block_stmt_iterator bsi;\n+  tree stmt;\n+\n+  /* We expect to find the following sequences:\n+   \n+   load_bb:\n+       OMP_ATOMIC_LOAD (tmp, mem)\n+\n+   store_bb:\n+       val = tmp OP something; (or: something OP tmp)\n+       OMP_STORE (val) \n+\n+  ???FIXME: Allow a more flexible sequence.  \n+  Perhaps use data flow to pick the statements.\n+  \n+  */\n+\n+  bsi = bsi_after_labels (store_bb);\n+  stmt = bsi_stmt (bsi);\n+  if (TREE_CODE (stmt) != GIMPLE_MODIFY_STMT)\n+    return false;\n+  bsi_next (&bsi);\n+  if (TREE_CODE (bsi_stmt (bsi)) != OMP_ATOMIC_STORE)\n+    return false;\n+\n+  if (!operand_equal_p (GIMPLE_STMT_OPERAND (stmt, 0), stored_val, 0))\n+    return false;\n+\n+  rhs = GIMPLE_STMT_OPERAND (stmt, 1);\n+\n+  /* Check for one of the supported fetch-op operations.  */\n+  switch (TREE_CODE (rhs))\n+    {\n+    case PLUS_EXPR:\n+    case POINTER_PLUS_EXPR:\n+      base = BUILT_IN_FETCH_AND_ADD_N;\n+      optab = sync_add_optab;\n+      break;\n+    case MINUS_EXPR:\n+      base = BUILT_IN_FETCH_AND_SUB_N;\n+      optab = sync_add_optab;\n+      break;\n+    case BIT_AND_EXPR:\n+      base = BUILT_IN_FETCH_AND_AND_N;\n+      optab = sync_and_optab;\n+      break;\n+    case BIT_IOR_EXPR:\n+      base = BUILT_IN_FETCH_AND_OR_N;\n+      optab = sync_ior_optab;\n+      break;\n+    case BIT_XOR_EXPR:\n+      base = BUILT_IN_FETCH_AND_XOR_N;\n+      optab = sync_xor_optab;\n+      break;\n+    default:\n+      return false;\n+    }\n+  /* Make sure the expression is of the proper form.  */\n+  if (operand_equal_p (TREE_OPERAND (rhs, 0), loaded_val, 0))\n+    rhs = TREE_OPERAND (rhs, 1);\n+  else if (commutative_tree_code (TREE_CODE (rhs))\n+\t   && operand_equal_p (TREE_OPERAND (rhs, 1), loaded_val, 0))\n+    rhs = TREE_OPERAND (rhs, 0);\n+  else\n+    return false;\n+\n+  decl = built_in_decls[base + index + 1];\n+  itype = TREE_TYPE (TREE_TYPE (decl));\n+\n+  if (optab[TYPE_MODE (itype)] == CODE_FOR_nothing)\n+    return false;\n+\n+  bsi = bsi_last (load_bb);\n+  gcc_assert (TREE_CODE (bsi_stmt (bsi)) == OMP_ATOMIC_LOAD);\n+  call = build_call_expr (decl, 2, addr, fold_convert (itype, rhs));\n+  force_gimple_operand_bsi (&bsi, call, true, NULL_TREE, true, BSI_SAME_STMT);\n+  bsi_remove (&bsi, true);\n+\n+  bsi = bsi_last (store_bb);\n+  gcc_assert (TREE_CODE (bsi_stmt (bsi)) == OMP_ATOMIC_STORE);\n+  bsi_remove (&bsi, true);\n+  bsi = bsi_last (store_bb);\n+  bsi_remove (&bsi, true);\n+\n+  if (gimple_in_ssa_p (cfun))\n+    update_ssa (TODO_update_ssa_no_phi);\n+\n+  return true;\n+}\n+\n+/* A subroutine of expand_omp_atomic.  Implement the atomic operation as:\n+\n+      oldval = *addr;\n+      repeat:\n+        newval = rhs;\t // with oldval replacing *addr in rhs\n+\toldval = __sync_val_compare_and_swap (addr, oldval, newval);\n+\tif (oldval != newval)\n+\t  goto repeat;\n+\n+   INDEX is log2 of the size of the data type, and thus usable to find the\n+   index of the builtin decl.  */\n+\n+static bool\n+expand_omp_atomic_pipeline (basic_block load_bb, basic_block store_bb,\n+\t\t\t    tree addr, tree loaded_val, tree stored_val,\n+\t\t\t    int index)\n+{\n+  tree loadedi, storedi, initial, new_stored, new_storedi, old_vali;\n+  tree type, itype, cmpxchg, iaddr;\n+  block_stmt_iterator bsi;\n+  basic_block loop_header = single_succ (load_bb);\n+  tree phi, x;\n+  edge e;\n+\n+  cmpxchg = built_in_decls[BUILT_IN_VAL_COMPARE_AND_SWAP_N + index + 1];\n+  type = TYPE_MAIN_VARIANT (TREE_TYPE (TREE_TYPE (addr)));\n+  itype = TREE_TYPE (TREE_TYPE (cmpxchg));\n+\n+  if (sync_compare_and_swap[TYPE_MODE (itype)] == CODE_FOR_nothing)\n+    return false;\n+\n+  /* Load the initial value, replacing the OMP_ATOMIC_LOAD.  */\n+  bsi = bsi_last (load_bb);\n+  gcc_assert (TREE_CODE (bsi_stmt (bsi)) == OMP_ATOMIC_LOAD);\n+  initial = force_gimple_operand_bsi (&bsi, build_fold_indirect_ref (addr),\n+\t\t\t\t      true, NULL_TREE, true, BSI_SAME_STMT);\n+  /* Move the value to the LOADED_VAL temporary.  */\n+  if (gimple_in_ssa_p (cfun))\n+    {\n+      gcc_assert (phi_nodes (loop_header) == NULL_TREE);\n+      phi = create_phi_node (loaded_val, loop_header);\n+      SSA_NAME_DEF_STMT (loaded_val) = phi;\n+      SET_USE (PHI_ARG_DEF_PTR_FROM_EDGE (phi, single_succ_edge (load_bb)),\n+\t       initial);\n+    }\n+  else\n+    bsi_insert_before (&bsi,\n+\t\t       build_gimple_modify_stmt (loaded_val, initial),\n+\t\t       BSI_SAME_STMT);\n+  bsi_remove (&bsi, true);\n+\n+  bsi = bsi_last (store_bb);\n+  gcc_assert (TREE_CODE (bsi_stmt (bsi)) == OMP_ATOMIC_STORE);\n+\n+  /* For floating-point values, we'll need to view-convert them to integers\n+     so that we can perform the atomic compare and swap.  Simplify the \n+     following code by always setting up the \"i\"ntegral variables.  */\n+  if (INTEGRAL_TYPE_P (type) || POINTER_TYPE_P (type))\n+    {\n+      loadedi = loaded_val;\n+      storedi = stored_val;\n+      iaddr = addr;\n+    }\n+  else\n+    {\n+      loadedi = force_gimple_operand_bsi (&bsi,\n+\t\t\t\t\t  build1 (VIEW_CONVERT_EXPR, itype,\n+\t\t\t\t\t\t  loaded_val), true,\n+\t\t\t\t\t  NULL_TREE, true, BSI_SAME_STMT);\n+      storedi =\n+\tforce_gimple_operand_bsi (&bsi,\n+\t\t\t\t  build1 (VIEW_CONVERT_EXPR, itype,\n+\t\t\t\t\t  stored_val), true, NULL_TREE, true,\n+\t\t\t\t  BSI_SAME_STMT);\n+      iaddr = fold_convert (build_pointer_type (itype), addr);\n+    }\n+\n+  /* Build the compare&swap statement.  */\n+  new_storedi = build_call_expr (cmpxchg, 3, iaddr, loadedi, storedi);\n+  new_storedi = force_gimple_operand_bsi (&bsi,\n+\t\t\t\t\t  fold_convert (itype, new_storedi),\n+\t\t\t\t\t  true, NULL_TREE,\n+\t\t\t\t\t  true, BSI_SAME_STMT);\n+  if (storedi == stored_val)\n+    new_stored = new_storedi;\n+  else\n+    new_stored = force_gimple_operand_bsi (&bsi,\n+\t\t\t\t\t   build1 (VIEW_CONVERT_EXPR, type,\n+\t\t\t\t\t\t   new_storedi), true,\n+\t\t\t\t\t   NULL_TREE, true, BSI_SAME_STMT);\n+\n+  if (gimple_in_ssa_p (cfun))\n+    old_vali = loadedi;\n+  else\n+    {\n+      old_vali = create_tmp_var (itype, NULL);\n+      x = build_gimple_modify_stmt (old_vali, loadedi);\n+      bsi_insert_before (&bsi, x, BSI_SAME_STMT);\n+\n+      x = build_gimple_modify_stmt (loaded_val, new_stored);\n+      bsi_insert_before (&bsi, x, BSI_SAME_STMT);\n+    }\n+\n+  /* Note that we always perform the comparison as an integer, even for\n+     floating point.  This allows the atomic operation to properly \n+     succeed even with NaNs and -0.0.  */\n+  x = build3 (COND_EXPR, void_type_node,\n+\t      build2 (NE_EXPR, boolean_type_node,\n+\t\t      new_storedi, old_vali), NULL_TREE, NULL_TREE);\n+  bsi_insert_before (&bsi, x, BSI_SAME_STMT);\n+\n+  /* Update cfg.  */\n+  e = single_succ_edge (store_bb);\n+  e->flags &= ~EDGE_FALLTHRU;\n+  e->flags |= EDGE_FALSE_VALUE;\n+\n+  e = make_edge (store_bb, loop_header, EDGE_TRUE_VALUE);\n+\n+  /* Copy the new value to loaded_val (we already did that before the condition\n+     if we are not in SSA).  */\n+  if (gimple_in_ssa_p (cfun))\n+    {\n+      phi = phi_nodes (loop_header);\n+      SET_USE (PHI_ARG_DEF_PTR_FROM_EDGE (phi, e), new_stored);\n+    }\n+\n+  /* Remove OMP_ATOMIC_STORE.  */\n+  bsi_remove (&bsi, true);\n+\n+  if (gimple_in_ssa_p (cfun))\n+    update_ssa (TODO_update_ssa_no_phi);\n+\n+  return true;\n+}\n+\n+/* A subroutine of expand_omp_atomic.  Implement the atomic operation as:\n+\n+\t\t \t\t  GOMP_atomic_start ();\n+\t\t \t\t  *addr = rhs;\n+\t\t \t\t  GOMP_atomic_end ();\n+\n+   The result is not globally atomic, but works so long as all parallel\n+   references are within #pragma omp atomic directives.  According to\n+   responses received from omp@openmp.org, appears to be within spec.\n+   Which makes sense, since that's how several other compilers handle\n+   this situation as well.  \n+   LOADED_VAL and ADDR are the operands of OMP_ATOMIC_LOAD we're expanding. \n+   STORED_VAL is the operand of the matching OMP_ATOMIC_STORE.\n+\n+   We replace \n+   OMP_ATOMIC_LOAD (loaded_val, addr) with  \n+   loaded_val = *addr;\n+\n+   and replace\n+   OMP_ATOMIC_ATORE (stored_val)  with\n+   *addr = stored_val;  \n+*/\n+\n+static bool\n+expand_omp_atomic_mutex (basic_block load_bb, basic_block store_bb,\n+\t\t\t tree addr, tree loaded_val, tree stored_val)\n+{\n+  block_stmt_iterator bsi;\n+  tree t;\n+\n+  bsi = bsi_last (load_bb);\n+  gcc_assert (TREE_CODE (bsi_stmt (bsi)) == OMP_ATOMIC_LOAD);\n+\n+  t = built_in_decls[BUILT_IN_GOMP_ATOMIC_START];\n+  t = build_function_call_expr (t, 0);\n+  force_gimple_operand_bsi (&bsi, t, true, NULL_TREE, true, BSI_SAME_STMT);\n+\n+  t = build_gimple_modify_stmt (loaded_val, build_fold_indirect_ref (addr));\n+  if (gimple_in_ssa_p (cfun))\n+    SSA_NAME_DEF_STMT (loaded_val) = t;\n+  bsi_insert_before (&bsi, t, BSI_SAME_STMT);\n+  bsi_remove (&bsi, true);\n+\n+  bsi = bsi_last (store_bb);\n+  gcc_assert (TREE_CODE (bsi_stmt (bsi)) == OMP_ATOMIC_STORE);\n+\n+  t = build_gimple_modify_stmt (build_fold_indirect_ref (unshare_expr (addr)),\n+\t\t\t\tstored_val);\n+  bsi_insert_before (&bsi, t, BSI_SAME_STMT);\n+\n+  t = built_in_decls[BUILT_IN_GOMP_ATOMIC_END];\n+  t = build_function_call_expr (t, 0);\n+  force_gimple_operand_bsi (&bsi, t, true, NULL_TREE, true, BSI_SAME_STMT);\n+  bsi_remove (&bsi, true);\n+\n+  if (gimple_in_ssa_p (cfun))\n+    update_ssa (TODO_update_ssa_no_phi);\n+  return true;\n+}\n+\n+/* Expand an OMP_ATOMIC statement.  We try to expand \n+   using expand_omp_atomic_fetch_op. If it failed, we try to \n+   call expand_omp_atomic_pipeline, and if it fails too, the\n+   ultimate fallback is wrapping the operation in a mutex\n+   (expand_omp_atomic_mutex).  REGION is the atomic region built \n+   by build_omp_regions_1().  */ \n+\n+static void\n+expand_omp_atomic (struct omp_region *region)\n+{\n+  basic_block load_bb = region->entry, store_bb = region->exit;\n+  tree load = last_stmt (load_bb), store = last_stmt (store_bb);\n+  tree loaded_val = TREE_OPERAND (load, 0);\n+  tree addr = TREE_OPERAND (load, 1);\n+  tree stored_val = TREE_OPERAND (store, 0);\n+  tree type = TYPE_MAIN_VARIANT (TREE_TYPE (TREE_TYPE (addr)));\n+  HOST_WIDE_INT index;\n+\n+  /* Make sure the type is one of the supported sizes.  */\n+  index = tree_low_cst (TYPE_SIZE_UNIT (type), 1);\n+  index = exact_log2 (index);\n+  if (index >= 0 && index <= 4)\n+    {\n+      unsigned int align = TYPE_ALIGN_UNIT (type);\n+\n+      /* __sync builtins require strict data alignment.  */\n+      if (exact_log2 (align) >= index)\n+\t{\n+\t  /* When possible, use specialized atomic update functions.  */\n+\t  if ((INTEGRAL_TYPE_P (type) || POINTER_TYPE_P (type))\n+\t      && store_bb == single_succ (load_bb))\n+\t    {\n+\t      if (expand_omp_atomic_fetch_op (load_bb, addr,\n+\t\t\t\t\t      loaded_val, stored_val, index))\n+\t\treturn;\n+\t    }\n+\n+\t  /* If we don't have specialized __sync builtins, try and implement\n+\t     as a compare and swap loop.  */\n+\t  if (expand_omp_atomic_pipeline (load_bb, store_bb, addr,\n+\t\t\t\t\t  loaded_val, stored_val, index))\n+\t    return;\n+\t}\n+    }\n+\n+  /* The ultimate fallback is wrapping the operation in a mutex.  */\n+  expand_omp_atomic_mutex (load_bb, store_bb, addr, loaded_val, stored_val);\n+}\n+\n \n /* Expand the parallel region tree rooted at REGION.  Expansion\n    proceeds in depth-first order.  Innermost regions are expanded\n@@ -3584,6 +3934,11 @@ expand_omp (struct omp_region *region)\n \t  expand_omp_synch (region);\n \t  break;\n \n+\tcase OMP_ATOMIC_LOAD:\n+\t  expand_omp_atomic (region);\n+\t  break;\n+\n+\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -3614,7 +3969,6 @@ build_omp_regions_1 (basic_block bb, struct omp_region *parent,\n \n       stmt = bsi_stmt (si);\n       code = TREE_CODE (stmt);\n-\n       if (code == OMP_RETURN)\n \t{\n \t  /* STMT is the return point out of region PARENT.  Mark it\n@@ -3630,6 +3984,17 @@ build_omp_regions_1 (basic_block bb, struct omp_region *parent,\n \t  if (region->type == OMP_PARALLEL)\n \t    determine_parallel_type (region);\n \t}\n+      else if (code == OMP_ATOMIC_STORE)\n+\t{\n+\t  /* OMP_ATOMIC_STORE is analoguous to OMP_RETURN, but matches with\n+\t     OMP_ATOMIC_LOAD.  */\n+\t  gcc_assert (parent);\n+\t  gcc_assert (parent->type == OMP_ATOMIC_LOAD);\n+\t  region = parent;\n+\t  region->exit = bb;\n+\t  parent = parent->outer;\n+\t}\n+\n       else if (code == OMP_CONTINUE)\n \t{\n \t  gcc_assert (parent);\n@@ -3638,7 +4003,7 @@ build_omp_regions_1 (basic_block bb, struct omp_region *parent,\n       else if (code == OMP_SECTIONS_SWITCH)\n \t{\n \t  /* OMP_SECTIONS_SWITCH is part of OMP_SECTIONS, and we do nothing for\n-\t     it.  */\n+\t     it.  */ ;\n \t}\n       else\n \t{"}, {"sha": "6bf1f60080398b4dffe62da2361f9f9c3a14c0e0", "filename": "gcc/tree-cfg.c", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-cfg.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-cfg.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-cfg.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -524,6 +524,13 @@ make_edges (void)\n \t      fallthru = false;\n \t      break;\n \n+\n+            case OMP_ATOMIC_LOAD:\n+            case OMP_ATOMIC_STORE:\n+               fallthru = true;\n+               break;\n+\n+\n \t    case OMP_RETURN:\n \t      /* In the case of an OMP_SECTION, the edge will go somewhere\n \t\t other than the next block.  This will be created later.  */"}, {"sha": "92c7b41bb8c5e880f38a784898b856c1eb85b06a", "filename": "gcc/tree-gimple.c", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-gimple.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-gimple.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-gimple.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -240,6 +240,8 @@ is_gimple_stmt (tree t)\n     case OMP_CRITICAL:\n     case OMP_RETURN:\n     case OMP_CONTINUE:\n+    case OMP_ATOMIC_LOAD:\n+    case OMP_ATOMIC_STORE:\n       /* These are always void.  */\n       return true;\n "}, {"sha": "ac8b5b5a400096ca855aed6d004e7ad9313db0d9", "filename": "gcc/tree-inline.c", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-inline.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-inline.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-inline.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -2154,6 +2154,7 @@ estimate_num_insns_1 (tree *tp, int *walk_subtrees, void *data)\n     case OMP_RETURN:\n     case OMP_CONTINUE:\n     case OMP_SECTIONS_SWITCH:\n+    case OMP_ATOMIC_STORE:\n       break;\n \n     /* We don't account constants for now.  Assume that the cost is amortized\n@@ -2384,6 +2385,7 @@ estimate_num_insns_1 (tree *tp, int *walk_subtrees, void *data)\n     case OMP_ORDERED:\n     case OMP_CRITICAL:\n     case OMP_ATOMIC:\n+    case OMP_ATOMIC_LOAD:\n       /* OpenMP directives are generally very expensive.  */\n       d->count += d->weights->omp_cost;\n       break;"}, {"sha": "34c4639e570521a0cb276a5525d22349d9a6b65d", "filename": "gcc/tree-parloops.c", "status": "modified", "additions": 799, "deletions": 119, "changes": 918, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-parloops.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-parloops.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-parloops.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -35,6 +35,7 @@ Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA\n #include \"tree-scalar-evolution.h\"\n #include \"hashtab.h\"\n #include \"langhooks.h\"\n+#include \"tree-vectorizer.h\"\n \n /* This pass tries to distribute iterations of loops into several threads.\n    The implementation is straightforward -- for each loop we test whether its\n@@ -61,10 +62,247 @@ Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA\n    -- handling of common scalar dependence patterns (accumulation, ...)\n    -- handling of non-innermost loops  */\n \n+/*  \n+  Reduction handling:\n+  currently we use vect_is_simple_reduction() to detect reduction patterns.\n+  The code transformation will be introduced by an example.\n+  \n+  source code:\n+\n+parloop\n+{\n+  int sum=1;\n+\n+  for (i = 0; i < N/1000; i++)\n+   {\n+    x[i] = i + 3;\n+    sum+=x[i];\n+   }\n+}\n+\n+gimple code:\n+\n+header_bb:\n+\n+  # sum_24 = PHI <sum_14(3), 1(2)>;\n+  # i_21 = PHI <i_15(3), 0(2)>;\n+<L0>:;\n+  D.2191_10 = i_21 + 3;\n+  x[i_21] = D.2191_10;\n+  sum_14 = D.2191_10 + sum_24;\n+  i_15 = i_21 + 1;\n+  if (N_8 > i_15) goto <L0>; else goto <L2>;\n+\n+exit_bb:\n+\n+  # sum_25 = PHI <sum_14(3)>;\n+<L2>:;\n+\n+\n+after reduction transformation (only relevant parts):\n+\n+parloop\n+{\n+\n+....\n+\n+<L16>:;\n+  D.2241_2 = (unsigned int) N_8;\n+  D.2242_26 = D.2241_2 - 1;\n+  if (D.2242_26 > 399) goto <L26>; else goto <L27>;\n+\n+#two new variables are created for each reduction: \n+\"reduction\" is the variable holding the neutral element \n+for the particular operation, e.g. 0 for PLUS_EXPR,\n+1 for MULT_EXPR, etc.\n+\"reduction_initial\" is the initial value given by the user.\n+It is kept and will be used after the parallel computing \n+is done.#\n+\n+<L26>:;\n+  reduction.38_42 = 0;     \n+  reduction_initial.39_43 = 1;        \n+  x.40_44 = &x;\n+  .paral_data_store.47.D.2261 = D.2242_26;\n+  .paral_data_store.47.reduction.38 = reduction.38_42;\n+  .paral_data_store.47.x.40 = x.40_44;\n+  __builtin_GOMP_parallel_start (parloop._loopfn.0, &.paral_data_store.47, 4);\n+  parloop._loopfn.0 (&.paral_data_store.47);\n+  __builtin_GOMP_parallel_end ();\n+\n+# collecting the result after the join of the threads is done at\n+  create_loads_for_reductions().  \n+  a new variable \"reduction_final\" is created.  It calculates the\n+  final value from the initial value and the value computed by \n+  the threads.  #\n+  \n+  .paral_data_load.48_49 = &.paral_data_store.47;        \n+  reduction_final.49_50 = .paral_data_load.48_49->reduction.38;\n+  reduction_final.49_51 = reduction_initial.39_43 + reduction_final.49_50;\n+  ivtmp.37_36 = D.2242_26;\n+  i_37 = (int) ivtmp.37_36;\n+  D.2191_38 = i_37 + 3;\n+  x[i_37] = D.2191_38;\n+  sum_40 = D.2191_38 + reduction_final.49_51;\n+  i_41 = i_37 + 1;\n+  goto <bb 8> (<L2>);\n+\n+  # sum_25 = PHI <sum_40(4), sum_9(6)>;\n+<L2>:;\n+  printf (&\"sum is %d\\n\"[0], sum_25);\n+\n+...\n+\n+}\n+\n+parloop._loopfn.0 (.paral_data_param)\n+{\n+ ...\n+\n+<L28>:;\n+  .paral_data_param_52 = .paral_data_param_75;\n+  .paral_data_load.48_48 = (struct .paral_data.46 *) .paral_data_param_52;\n+  D.2289_46 = .paral_data_load.48_48->D.2261;\n+  reduction.43_45 = .paral_data_load.48_48->reduction.38;\n+   x.45_47 = .paral_data_load.48_48->x.40;\n+  # SUCC: 23 [100.0%]  (fallthru)\n+\n+  # BLOCK 23\n+  # PRED: 21 [100.0%]  (fallthru)\n+<L30>:;\n+  D.2292_60 = __builtin_omp_get_num_threads ();\n+  D.2293_61 = (unsigned int) D.2292_60;\n+  D.2294_62 = __builtin_omp_get_thread_num ();\n+  D.2295_63 = (unsigned int) D.2294_62;\n+  D.2296_64 = D.2289_46 / D.2293_61;\n+  D.2297_65 = D.2293_61 * D.2296_64;\n+  D.2298_66 = D.2297_65 != D.2289_46;\n+  D.2299_67 = D.2296_64 + D.2298_66;\n+  D.2300_68 = D.2299_67 * D.2295_63;\n+  D.2301_69 = D.2299_67 + D.2300_68;\n+  D.2302_70 = MIN_EXPR <D.2301_69, D.2289_46>;\n+  ivtmp.41_54 = D.2300_68;\n+  if (D.2300_68 >= D.2302_70) goto <L31>; else goto <L32>;\n+  # SUCC: 26 [100.0%]  (false) 24 (true)\n+\n+  # BLOCK 26\n+  # PRED: 23 [100.0%]  (false)\n+<L32>:;\n+  # SUCC: 4 [100.0%]  (fallthru)\n+\n+  # BLOCK 4\n+  # PRED: 5 [100.0%]  (true) 26 [100.0%]  (fallthru)\n+  # ivtmp.41_31 = PHI <ivtmp.41_30(5), ivtmp.41_54(26)>;\n+  # sum.42_32 = PHI <sum.42_14(5), reduction.43_45(26)>;\n+<L0>:;\n+  # SUCC: 19 [100.0%]  (fallthru)\n+\n+  # BLOCK 19\n+  # PRED: 4 [100.0%]  (fallthru)\n+  # sum.42_24 = PHI <sum.42_32(4)>;\n+  # ivtmp.41_17 = PHI <ivtmp.41_31(4)>;\n+  i.44_21 = (int) ivtmp.41_17;\n+  D.2310_10 = i.44_21 + 3;\n+  (*x.45_47)[i.44_21] = D.2310_10;\n+  sum.42_14 = D.2310_10 + sum.42_24;\n+  i.44_15 = i.44_21 + 1;\n+  # SUCC: 5 [100.0%]  (fallthru)\n+\n+  # BLOCK 5\n+  # PRED: 19 [100.0%]  (fallthru)\n+<L17>:;\n+  ivtmp.41_30 = ivtmp.41_31 + 1;\n+  if (ivtmp.41_30 < D.2302_70) goto <L0>; else goto <L31>;\n+  # SUCC: 4 [100.0%]  (true) 24 (false)\n+\n+  # Adding this reduction phi is done at\n+  create_phi_for_local_result() #\n+\n+  # BLOCK 24\n+  # PRED: 5 (false) 23 (true)\n+  # reduction.38_56 = PHI <sum.42_14(5), 0(23)>;\n+    <L31>:;\n+  __builtin_GOMP_barrier ();\n+  # SUCC: 25 [100.0%]  (fallthru)\n+\n+  # Creating the atomic operation is\n+  done at create_call_for_reduction_1()  #\n+\n+  # BLOCK 25\n+  # PRED: 24 [100.0%]  (fallthru)\n+  D.2306_57 = &.paral_data_load.48_48->reduction.38;\n+  D.2307_58 = (unsigned int) reduction.38_56;\n+  D.2308_59 = __sync_fetch_and_add_4 (D.2306_57, D.2307_58);\n+  # SUCC: 22 [100.0%]  (fallthru)\n+\n+  # BLOCK 22\n+  # PRED: 25 [100.0%]  (fallthru)\n+  <L29>:;\n+  return;\n+  # SUCC: EXIT\n+  \n+}\n+\n+*/\n+\n /* Minimal number of iterations of a loop that should be executed in each\n    thread.  */\n #define MIN_PER_THREAD 100\n \n+/* Element of the hashtable, representing a \n+   reduction in the current loop.  */\n+struct reduction_info\n+{\n+  tree reduc_stmt;\t\t/* reduction statement.  */\n+  tree reduc_phi;\t\t/* The phi node defining the reduction.  */\n+  enum tree_code reduction_code;\t/* code for the reduction operation.  */\n+  tree keep_res;\t\t/* The PHI_RESULT of this phi is the resulting value \n+\t\t\t\t   of the reduction variable when existing the loop. */\n+  tree initial_value;\t\t/* An ssa name representing a new variable holding\n+\t\t\t\t   the initial value of the reduction var before entering the loop.   */\n+  tree field;\t\t\t/*  the name of the field in the parloop data structure intended for reduction.  */\n+  tree reduction_init;\t\t/* An ssa name representing a new variable which will be \n+\t\t\t\t   assigned the proper reduction initialization value (init).  */\n+  tree init;\t\t\t/* reduction initialization value.  */\n+  tree new_phi;\t\t\t/* (helper field) Newly created phi node whose result \n+\t\t\t\t   will be passed to the atomic operation.  Represents\n+\t\t\t\t   the local result each thread computed for the reduction\n+\t\t\t\t   operation.  */\n+};\n+\n+/* Equality and hash functions for hashtab code.  */\n+\n+static int\n+reduction_info_eq (const void *aa, const void *bb)\n+{\n+  const struct reduction_info *a = (const struct reduction_info *) aa;\n+  const struct reduction_info *b = (const struct reduction_info *) bb;\n+\n+  return (a->reduc_phi == b->reduc_phi);\n+}\n+\n+static hashval_t\n+reduction_info_hash (const void *aa)\n+{\n+  const struct reduction_info *a = (const struct reduction_info *) aa;\n+\n+  return htab_hash_pointer (a->reduc_phi);\n+}\n+\n+static struct reduction_info *\n+reduction_phi (htab_t reduction_list, tree phi)\n+{\n+  struct reduction_info tmpred, *red;\n+\n+  if (htab_elements (reduction_list) == 0)\n+    return NULL;\n+\n+  tmpred.reduc_phi = phi;\n+  red = htab_find (reduction_list, &tmpred);\n+\n+  return red;\n+}\n+\n /* Element of hashtable of names to copy.  */\n \n struct name_to_copy_elt\n@@ -80,34 +318,36 @@ struct name_to_copy_elt\n static int\n name_to_copy_elt_eq (const void *aa, const void *bb)\n {\n-  struct name_to_copy_elt *a = (struct name_to_copy_elt *) aa;\n-  struct name_to_copy_elt *b = (struct name_to_copy_elt *) bb;\n+  const struct name_to_copy_elt *a = (const struct name_to_copy_elt *) aa;\n+  const struct name_to_copy_elt *b = (const struct name_to_copy_elt *) bb;\n \n   return a->version == b->version;\n }\n \n static hashval_t\n name_to_copy_elt_hash (const void *aa)\n {\n-  struct name_to_copy_elt *a = (struct name_to_copy_elt *) aa;\n+  const struct name_to_copy_elt *a = (const struct name_to_copy_elt *) aa;\n \n   return (hashval_t) a->version;\n }\n \n /* Returns true if the iterations of LOOP are independent on each other (that\n    is, if we can execute them in parallel), and if LOOP satisfies other\n    conditions that we need to be able to parallelize it.  Description of number\n-   of iterations is stored to NITER.  */\n+   of iterations is stored to NITER.  Reduction analysis is done, if\n+   reductions are found, they are inserted to the REDUCTION_LIST.  */  \n \n static bool\n-loop_parallel_p (struct loop *loop, struct tree_niter_desc *niter)\n+loop_parallel_p (struct loop *loop, htab_t reduction_list, struct tree_niter_desc *niter)\n {\n   edge exit = single_dom_exit (loop);\n-  VEC (ddr_p, heap) *dependence_relations;\n-  VEC (data_reference_p, heap) *datarefs;\n+  VEC (ddr_p, heap) * dependence_relations;\n+  VEC (data_reference_p, heap) * datarefs;\n   lambda_trans_matrix trans;\n   bool ret = false;\n   tree phi;\n+  loop_vec_info simple_loop_info;\n \n   /* Only consider innermost loops with just one exit.  The innermost-loop\n      restriction is not necessary, but it makes things simpler.  */\n@@ -127,15 +367,99 @@ loop_parallel_p (struct loop *loop, struct tree_niter_desc *niter)\n       return false;\n     }\n \n+  simple_loop_info = vect_analyze_loop_form (loop);\n+\n+  for (phi = phi_nodes (loop->header); phi; phi = PHI_CHAIN (phi))\n+    {\n+      tree reduc_stmt = NULL, operation;\n+\n+      /* ??? TODO: Change this into a generic function that \n+         recognizes reductions.  */\n+      if (!is_gimple_reg (PHI_RESULT (phi)))\n+\tcontinue;\n+      if (simple_loop_info)\n+\treduc_stmt = vect_is_simple_reduction (simple_loop_info, phi);\n+\n+      /*  Create a reduction_info struct, initialize it and insert it to \n+         the reduction list.  */\n+\n+      if (reduc_stmt)\n+\t{\n+\t  PTR *slot;\n+\t  struct reduction_info *new_reduction;\n+\n+\t  if (dump_file && (dump_flags & TDF_DETAILS))\n+\t    {\n+\t      fprintf (dump_file,\n+\t\t       \"Detected reduction. reduction stmt is: \\n\");\n+\t      print_generic_stmt (dump_file, reduc_stmt, 0);\n+\t      fprintf (dump_file, \"\\n\");\n+\t    }\n+\n+\t  new_reduction = XCNEW (struct reduction_info);\n+\n+\t  new_reduction->reduc_stmt = reduc_stmt;\n+\t  new_reduction->reduc_phi = phi;\n+\t  operation = GIMPLE_STMT_OPERAND (reduc_stmt, 1);\n+\t  new_reduction->reduction_code = TREE_CODE (operation);\n+\t  slot = htab_find_slot (reduction_list, new_reduction, INSERT);\n+\t  *slot = new_reduction;\n+\t}\n+    }\n+\n   for (phi = phi_nodes (exit->dest); phi; phi = PHI_CHAIN (phi))\n     {\n+      struct reduction_info *red;\n+      imm_use_iterator imm_iter;\n+      use_operand_p use_p;\n+      tree reduc_phi;\n+\n       tree val = PHI_ARG_DEF_FROM_EDGE (phi, exit);\n \n       if (is_gimple_reg (val))\n \t{\n \t  if (dump_file && (dump_flags & TDF_DETAILS))\n-\t    fprintf (dump_file, \"  FAILED: value used outside loop\\n\");\n-\t  return false;\n+\t    {\n+\t      fprintf (dump_file, \"phi is \");\n+\t      print_generic_expr (dump_file, phi, 0);\n+\t      fprintf (dump_file, \"arg of phi to exit:   value \");\n+\t      print_generic_expr (dump_file, val, 0);\n+\t      fprintf (dump_file, \" used outside loop\\n\");\n+\t      fprintf (dump_file,\n+\t\t       \"  checking if it a part of reduction pattern:  \\n\");\n+\t    }\n+\t  if (htab_elements (reduction_list) == 0)\n+\t    {\n+\t      if (dump_file && (dump_flags & TDF_DETAILS))\n+\t\tfprintf (dump_file,\n+\t\t\t \"  FAILED: it is not a part of reduction.\\n\");\n+\t      return false;\n+\t    }\n+\t  reduc_phi = NULL;\n+\t  FOR_EACH_IMM_USE_FAST (use_p, imm_iter, val)\n+\t  {\n+\t    if (flow_bb_inside_loop_p (loop, bb_for_stmt (USE_STMT (use_p))))\n+\t      {\n+\t\treduc_phi = USE_STMT (use_p);\n+\t\tbreak;\n+\t      }\n+\t  }\n+\t  red = reduction_phi (reduction_list, reduc_phi);\n+\t  if (red == NULL)\n+\t    {\n+\t      if (dump_file && (dump_flags & TDF_DETAILS))\n+\t\tfprintf (dump_file,\n+\t\t\t \"  FAILED: it is not a part of reduction.\\n\");\n+\t      return false;\n+\t    }\n+\t  if (dump_file && (dump_flags & TDF_DETAILS))\n+\t    {\n+\t      fprintf (dump_file, \"reduction phi is  \");\n+\t      print_generic_expr (dump_file, red->reduc_phi, 0);\n+\t      fprintf (dump_file, \"reduction stmt is  \");\n+\t      print_generic_expr (dump_file, red->reduc_stmt, 0);\n+\t    }\n+\n \t}\n     }\n \n@@ -146,13 +470,18 @@ loop_parallel_p (struct loop *loop, struct tree_niter_desc *niter)\n       tree def = PHI_RESULT (phi);\n       affine_iv iv;\n \n-      if (is_gimple_reg (def)\n-\t  && !simple_iv (loop, phi, def, &iv, true))\n+      if (is_gimple_reg (def) && !simple_iv (loop, phi, def, &iv, true))\n \t{\n-\t  if (dump_file && (dump_flags & TDF_DETAILS))\n-\t    fprintf (dump_file,\n-\t\t     \"  FAILED: scalar dependency between iterations\\n\");\n-\t  return false;\n+\t  struct reduction_info *red;\n+\n+\t  red = reduction_phi (reduction_list, phi);\n+\t  if (red == NULL)\n+\t    {\n+\t      if (dump_file && (dump_flags & TDF_DETAILS))\n+\t\tfprintf (dump_file,\n+\t\t\t \"  FAILED: scalar dependency between iterations\\n\");\n+\t      return false;\n+\t    }\n \t}\n     }\n \n@@ -183,7 +512,8 @@ loop_parallel_p (struct loop *loop, struct tree_niter_desc *niter)\n \tfprintf (dump_file, \"  SUCCESS: may be parallelized\\n\");\n     }\n   else if (dump_file && (dump_flags & TDF_DETAILS))\n-    fprintf (dump_file, \"  FAILED: data dependencies exist across iterations\\n\");\n+    fprintf (dump_file,\n+\t     \"  FAILED: data dependencies exist across iterations\\n\");\n \n   free_dependence_relations (dependence_relations);\n   free_data_refs (datarefs);\n@@ -211,8 +541,9 @@ take_address_of (tree var, tree type, struct loop *loop, htab_t decl_address)\n       bvar = create_tmp_var (type, get_name (var));\n       add_referenced_var (bvar);\n       stmt = build_gimple_modify_stmt (bvar,\n-\t\t     fold_convert (type,\n-\t\t\t\t   build_addr (var, current_function_decl)));\n+\t\t\t\t       fold_convert (type,\n+\t\t\t\t\t\t     build_addr (var,\n+\t\t\t\t\t\t\t\t current_function_decl)));\n       name = make_ssa_name (bvar, stmt);\n       GIMPLE_STMT_OPERAND (stmt, 0) = name;\n       bsi_insert_on_edge_immediate (entry, stmt);\n@@ -230,19 +561,80 @@ take_address_of (tree var, tree type, struct loop *loop, htab_t decl_address)\n     return name;\n \n   bvar = SSA_NAME_VAR (name);\n-  stmt = build_gimple_modify_stmt (bvar,\n-\t\t fold_convert (type, name));\n+  stmt = build_gimple_modify_stmt (bvar, fold_convert (type, name));\n   name = make_ssa_name (bvar, stmt);\n   GIMPLE_STMT_OPERAND (stmt, 0) = name;\n   bsi_insert_on_edge_immediate (entry, stmt);\n \n   return name;\n }\n \n-/* Eliminates references to local variables in *TP out of LOOP.  DECL_ADDRESS\n-   contains addresses of the references that had their address taken already.\n-   If the expression is changed, CHANGED is set to true.  Callback for\n-   walk_tree.  */\n+/* Callback for htab_traverse.  Create the initialization statement\n+   for reduction described in SLOT, and place it at the preheader of \n+   the loop described in DATA.  */\n+\n+static int\n+initialize_reductions (void **slot, void *data)\n+{\n+  tree t, stmt;\n+  tree init, c;\n+  tree name, name1;\n+  tree bvar, type, arg;\n+  edge e;\n+\n+  struct reduction_info *reduc = *slot;\n+  struct loop *loop = (struct loop *) data;\n+\n+  /* Create initialization in preheader: \n+     reduction_variable = initialization value of reduction.  */\n+\n+  /* In the phi node at the header, replace the argument coming \n+     from the preheader with the reduction initialization value.  */\n+\n+  /* Create a new variable to initialize the reduction.  */\n+  type = TREE_TYPE (PHI_RESULT (reduc->reduc_phi));\n+  bvar = create_tmp_var (type, \"reduction\");\n+  add_referenced_var (bvar);\n+\n+  c = build_omp_clause (OMP_CLAUSE_REDUCTION);\n+  OMP_CLAUSE_REDUCTION_CODE (c) = reduc->reduction_code;\n+  OMP_CLAUSE_DECL (c) =\n+    SSA_NAME_VAR (GIMPLE_STMT_OPERAND (reduc->reduc_stmt, 0));\n+\n+  init = omp_reduction_init (c, TREE_TYPE (bvar));\n+  reduc->init = init;\n+\n+  t = build_gimple_modify_stmt (bvar, init);\n+  name = make_ssa_name (bvar, t);\n+\n+  GIMPLE_STMT_OPERAND (t, 0) = name;\n+  SSA_NAME_DEF_STMT (name) = t;\n+\n+  /* Replace the argument \n+     representing the initialization value.  Keeping the old value \n+     in a new variable \"reduction_initial\", that will be taken in \n+     consideration after the parallel computing is done.  */\n+\n+  e = loop_preheader_edge (loop);\n+  arg = PHI_ARG_DEF_FROM_EDGE (reduc->reduc_phi, e);\n+  /* Create new variable to hold the initial value.  */\n+  type = TREE_TYPE (bvar);\n+  bvar = create_tmp_var (type, \"reduction_initial\");\n+  add_referenced_var (bvar);\n+\n+  stmt = build_gimple_modify_stmt (bvar, arg);\n+  name1 = make_ssa_name (bvar, stmt);\n+  GIMPLE_STMT_OPERAND (stmt, 0) = name1;\n+  SSA_NAME_DEF_STMT (name1) = stmt;\n+\n+  bsi_insert_on_edge_immediate (e, stmt);\n+  bsi_insert_on_edge_immediate (e, t);\n+  SET_USE (PHI_ARG_DEF_PTR_FROM_EDGE\n+\t   (reduc->reduc_phi, loop_preheader_edge (loop)), name);\n+  reduc->initial_value = name1;\n+  reduc->reduction_init = name;\n+  return 1;\n+}\n \n struct elv_data\n {\n@@ -251,8 +643,13 @@ struct elv_data\n   bool changed;\n };\n \n+/* Eliminates references to local variables in *TP out of LOOP.  DECL_ADDRESS\n+   contains addresses of the references that had their address taken already.\n+   If the expression is changed, CHANGED is set to true.  Callback for\n+   walk_tree.  */\n+\n static tree\n-eliminate_local_variables_1 (tree *tp, int *walk_subtrees, void *data)\n+eliminate_local_variables_1 (tree * tp, int *walk_subtrees, void *data)\n {\n   struct elv_data *dta = data;\n   tree t = *tp, var, addr, addr_type, type;\n@@ -291,8 +688,7 @@ eliminate_local_variables_1 (tree *tp, int *walk_subtrees, void *data)\n       return NULL_TREE;\n     }\n \n-  if (!EXPR_P (t)\n-      && !GIMPLE_STMT_P (t))\n+  if (!EXPR_P (t) && !GIMPLE_STMT_P (t))\n     *walk_subtrees = 0;\n \n   return NULL_TREE;\n@@ -318,12 +714,13 @@ eliminate_local_variables_stmt (struct loop *loop, tree stmt,\n     update_stmt (stmt);\n }\n \n-/* Eliminates the references to local variables from LOOP.  This includes:\n-\n-   1) Taking address of a local variable -- these are moved out of the loop\n-      (and temporary variable is created to hold the address if necessary).\n+/* Eliminates the references to local variables from LOOP.  \n+   This includes:\n+   1) Taking address of a local variable -- these are moved out of the \n+   loop (and temporary variable is created to hold the address if \n+   necessary).\n    2) Dereferencing a local variable -- these are replaced with indirect\n-      references.  */\n+   references.  */\n \n static void\n eliminate_local_variables (struct loop *loop)\n@@ -388,7 +785,7 @@ separate_decls_in_loop_name (tree name,\n       *dslot = nielt;\n \n       /* Ensure that when we meet this decl next time, we won't duplicate\n-\t it again.  */\n+         it again.  */\n       nuid = DECL_UID (var_copy);\n       ielt.uid = nuid;\n       dslot = htab_find_slot_with_hash (decl_copies, &ielt, nuid, INSERT);\n@@ -439,47 +836,114 @@ separate_decls_in_loop_stmt (struct loop *loop, tree stmt,\n   mark_virtual_ops_for_renaming (stmt);\n \n   FOR_EACH_PHI_OR_STMT_DEF (def, stmt, oi, SSA_OP_DEF)\n-    {\n-      name = DEF_FROM_PTR (def);\n-      gcc_assert (TREE_CODE (name) == SSA_NAME);\n-      copy = separate_decls_in_loop_name (name, name_copies, decl_copies,\n-\t\t\t\t\t  false);\n-      gcc_assert (copy == name);\n-    }\n+  {\n+    name = DEF_FROM_PTR (def);\n+    gcc_assert (TREE_CODE (name) == SSA_NAME);\n+    copy = separate_decls_in_loop_name (name, name_copies, decl_copies,\n+\t\t\t\t\tfalse);\n+    gcc_assert (copy == name);\n+  }\n \n   FOR_EACH_PHI_OR_STMT_USE (use, stmt, oi, SSA_OP_USE)\n-    {\n-      name = USE_FROM_PTR (use);\n-      if (TREE_CODE (name) != SSA_NAME)\n-\tcontinue;\n-\n-      copy_name_p = expr_invariant_in_loop_p (loop, name);\n-      copy = separate_decls_in_loop_name (name, name_copies, decl_copies,\n-\t\t\t\t\t  copy_name_p);\n-      SET_USE (use, copy);\n-    }\n+  {\n+    name = USE_FROM_PTR (use);\n+    if (TREE_CODE (name) != SSA_NAME)\n+      continue;\n+\n+    copy_name_p = expr_invariant_in_loop_p (loop, name);\n+    copy = separate_decls_in_loop_name (name, name_copies, decl_copies,\n+\t\t\t\t\tcopy_name_p);\n+    SET_USE (use, copy);\n+  }\n }\n \n+/* A helper structure for passing the TYPE and REDUCTION_LIST\n+   to the DATA parameter of add_field_for_name.  */\n+struct data_arg \n+{\n+  tree type;\n+  htab_t reduction_list;\n+};\n+\n /* Callback for htab_traverse.  Adds a field corresponding to a ssa name\n-   described in SLOT to the type passed in DATA.  */\n+   described in SLOT. The type is passed in DATA.  The Reduction list\n+   is also passes in DATA.  */\n \n static int\n add_field_for_name (void **slot, void *data)\n {\n+  tree stmt;\n+  use_operand_p use_p = NULL;\n+\n   struct name_to_copy_elt *elt = *slot;\n-  tree type = data;\n+  struct data_arg *data_arg = (struct data_arg *) data;\n+  tree type = data_arg->type;\n   tree name = ssa_name (elt->version);\n   tree var = SSA_NAME_VAR (name);\n   tree field = build_decl (FIELD_DECL, DECL_NAME (var), TREE_TYPE (var));\n \n   insert_field_into_struct (type, field);\n   elt->field = field;\n+\n+  /* Find uses of name to determine if this name is related to \n+     a reduction phi, and if so, record the field in the reduction struct.  */\n+\n+  if ((htab_elements (data_arg->reduction_list) > 0) \n+      && single_imm_use (elt->new_name, &use_p, &stmt)\n+      && TREE_CODE (stmt) == PHI_NODE)\n+    {\n+      /* check if STMT is a REDUC_PHI of some reduction.  */\n+      struct reduction_info *red;\n+\n+      red = reduction_phi (data_arg->reduction_list ,stmt);\n+      if (red)\n+\tred->field = field;\n+    }\n+\n   return 1;\n }\n \n-/* Callback for htab_traverse.  Creates loads to a field of LOAD in LOAD_BB and\n-   store to a field of STORE in STORE_BB for the ssa name and its duplicate\n-   specified in SLOT.  */\n+/* Callback for htab_traverse.  A local result is the intermediate result \n+   computed by a single \n+   thread, or the intial value in case no iteration was executed.\n+   This function creates a phi node reflecting these values.  \n+   The phi's result will be stored in NEW_PHI field of the \n+   reduction's data structure.  */ \n+\n+static int\n+create_phi_for_local_result (void **slot, void *data)\n+{\n+  struct reduction_info *reduc = *slot;\n+  struct loop *loop = data;\n+  edge e;\n+  tree new_phi;\n+  basic_block store_bb;\n+  tree local_res;\n+\n+  /* STORE_BB is the block where the phi \n+     should be stored.  It is the destination of the loop exit.  \n+     (Find the fallthru edge from OMP_CONTINUE).  */\n+  store_bb = FALLTHRU_EDGE (loop->latch)->dest;\n+\n+  /* STORE_BB has two predecessors.  One coming from  the loop\n+     (the reduction's result is computed at the loop),\n+     and another coming from a block preceding the loop, \n+     when no iterations \n+     are executed (the initial value should be taken).  */ \n+  if (EDGE_PRED (store_bb, 0) == FALLTHRU_EDGE (loop->latch))\n+    e = EDGE_PRED (store_bb, 1);\n+  else\n+    e = EDGE_PRED (store_bb, 0);\n+  local_res = make_ssa_name (SSA_NAME_VAR (reduc->reduction_init), NULL_TREE);\n+  new_phi = create_phi_node (local_res, store_bb);\n+  SSA_NAME_DEF_STMT (local_res) = new_phi;\n+  add_phi_arg (new_phi, reduc->init, e);\n+  add_phi_arg (new_phi, GIMPLE_STMT_OPERAND (reduc->reduc_stmt, 0),\n+\t       FALLTHRU_EDGE (loop->latch));\n+  reduc->new_phi = new_phi;\n+\n+  return 1;\n+}\n \n struct clsn_data\n {\n@@ -490,6 +954,159 @@ struct clsn_data\n   basic_block load_bb;\n };\n \n+/* Callback for htab_traverse.  Create an atomic instruction for the\n+   reduction described in SLOT.  \n+   DATA annotates the place in memory the atomic operation relates to,\n+   and the basic block it needs to be generated in.  */\n+\n+static int\n+create_call_for_reduction_1 (void **slot, void *data)\n+{\n+  struct reduction_info *reduc = *slot;\n+  struct clsn_data *clsn_data = data;\n+  block_stmt_iterator bsi;\n+  tree type = TREE_TYPE (PHI_RESULT (reduc->reduc_phi));\n+  tree struct_type = TREE_TYPE (TREE_TYPE (clsn_data->load));\n+  tree load_struct;\n+  basic_block bb;\n+  basic_block new_bb;\n+  edge e;\n+  tree t, addr, addr_type, ref, x;\n+  tree tmp_load, load, name;\n+\n+  load_struct = fold_build1 (INDIRECT_REF, struct_type, clsn_data->load);\n+  t = build3 (COMPONENT_REF, type, load_struct, reduc->field, NULL_TREE);\n+  addr_type = build_pointer_type (type);\n+\n+  addr = build_addr (t, current_function_decl);\n+\n+  /* Create phi node.  */\n+  bb = clsn_data->load_bb;\n+\n+  e = split_block (bb, t);\n+  new_bb = e->dest;\n+\n+  tmp_load = create_tmp_var (TREE_TYPE (TREE_TYPE (addr)), NULL);\n+  add_referenced_var (tmp_load);\n+  tmp_load = make_ssa_name (tmp_load, NULL);\n+  load = build2 (OMP_ATOMIC_LOAD, void_type_node, tmp_load, addr);\n+  SSA_NAME_DEF_STMT (tmp_load) = load;\n+  bsi = bsi_start (new_bb);\n+  bsi_insert_after (&bsi, load, BSI_NEW_STMT);\n+\n+  e = split_block (new_bb, load);\n+  new_bb = e->dest;\n+  bsi = bsi_start (new_bb);\n+  ref = tmp_load;\n+  x =\n+    fold_build2 (reduc->reduction_code,\n+\t\t TREE_TYPE (PHI_RESULT (reduc->new_phi)), ref,\n+\t\t PHI_RESULT (reduc->new_phi));\n+\n+  name =\n+    force_gimple_operand_bsi (&bsi, x, true, NULL_TREE, true,\n+\t\t\t      BSI_CONTINUE_LINKING);\n+\n+  x = build1 (OMP_ATOMIC_STORE, void_type_node, name);\n+\n+  bsi_insert_after (&bsi, x, BSI_NEW_STMT);\n+  return 1;\n+}\n+\n+/* Create the atomic operation at the join point of the threads.  \n+   REDUCTION_LIST describes the reductions in the LOOP.  \n+   LD_ST_DATA describes the shared data structure where \n+   shared data is stored in and loaded from.  */\n+static void\n+create_call_for_reduction (struct loop *loop, htab_t reduction_list, \n+\t\t\t   struct clsn_data *ld_st_data)\n+{\n+  htab_traverse (reduction_list, create_phi_for_local_result, loop);\n+  /* Find the fallthru edge from OMP_CONTINUE.  */\n+  ld_st_data->load_bb = FALLTHRU_EDGE (loop->latch)->dest;\n+  htab_traverse (reduction_list, create_call_for_reduction_1, ld_st_data);\n+}\n+\n+/* Callback for htab_traverse.  Create a new variable that loads the \n+   final reduction value at the  \n+   join point of all threads, adds the initial value the reduction \n+   variable had before the parallel computation started, and \n+   inserts it in the right place.  */\n+\n+static int\n+create_loads_for_reductions (void **slot, void *data)\n+{\n+  struct reduction_info *red = *slot;\n+  struct clsn_data *clsn_data = data;\n+  tree stmt;\n+  block_stmt_iterator bsi;\n+  tree type = TREE_TYPE (red->reduction_init);\n+  tree struct_type = TREE_TYPE (TREE_TYPE (clsn_data->load));\n+  tree load_struct;\n+  tree bvar, name;\n+  tree x;\n+\n+  bsi = bsi_after_labels (clsn_data->load_bb);\n+  load_struct = fold_build1 (INDIRECT_REF, struct_type, clsn_data->load);\n+  load_struct = build3 (COMPONENT_REF, type, load_struct, red->field,\n+\t\t\tNULL_TREE);\n+  bvar = create_tmp_var (type, \"reduction_final\");\n+  add_referenced_var (bvar);\n+\n+  /* Apply operation between the new variable which is the result\n+     of computation all threads, and the initial value which is kept\n+     at reduction->inital_value.  */\n+\n+  stmt = build_gimple_modify_stmt (bvar, load_struct);\n+  name = make_ssa_name (bvar, stmt);\n+  GIMPLE_STMT_OPERAND (stmt, 0) = name;\n+  SSA_NAME_DEF_STMT (name) = stmt;\n+\n+  bsi_insert_after (&bsi, stmt, BSI_NEW_STMT);\n+\n+  x =\n+    fold_build2 (red->reduction_code, TREE_TYPE (load_struct),\n+\t\t name, red->initial_value);\n+  name = PHI_RESULT (red->keep_res);\n+  stmt = build_gimple_modify_stmt (name, x);\n+  GIMPLE_STMT_OPERAND (stmt, 0) = name;\n+  SSA_NAME_DEF_STMT (name) = stmt;\n+\n+  bsi_insert_after (&bsi, stmt, BSI_NEW_STMT);\n+\n+  remove_phi_node (red->keep_res, NULL_TREE, false);\n+\n+  return 1;\n+}\n+\n+/* Load the reduction result that was stored in LD_ST_DATA.  \n+   REDUCTION_LIST describes the list of reductions that the\n+   loades should be generated for.  */\n+static void\n+create_final_loads_for_reduction (htab_t reduction_list, \n+\t\t\t\t  struct clsn_data *ld_st_data)\n+{\n+  block_stmt_iterator bsi;\n+  tree t;\n+\n+  bsi = bsi_after_labels (ld_st_data->load_bb);\n+  t = build_fold_addr_expr (ld_st_data->store);\n+  t =\n+    build_gimple_modify_stmt (ld_st_data->load,\n+\t\t\t      build_fold_addr_expr (ld_st_data->store));\n+\n+  bsi_insert_before (&bsi, t, BSI_NEW_STMT);\n+  SSA_NAME_DEF_STMT (ld_st_data->load) = t;\n+  GIMPLE_STMT_OPERAND (t, 0) = ld_st_data->load;\n+\n+  htab_traverse (reduction_list, create_loads_for_reductions, ld_st_data);\n+\n+}\n+\n+/* Callback for htab_traverse.  Creates loads to a field of LOAD in LOAD_BB and\n+   store to a field of STORE in STORE_BB for the ssa name and its duplicate\n+   specified in SLOT.  */\n+\n static int\n create_loads_and_stores_for_name (void **slot, void *data)\n {\n@@ -502,19 +1119,19 @@ create_loads_and_stores_for_name (void **slot, void *data)\n   tree load_struct;\n \n   bsi = bsi_last (clsn_data->store_bb);\n-  stmt = build_gimple_modify_stmt (\n-\t\t build3 (COMPONENT_REF, type, clsn_data->store, elt->field,\n-\t\t\t NULL_TREE),\n-\t\t ssa_name (elt->version));\n+  stmt =\n+    build_gimple_modify_stmt (build3\n+\t\t\t      (COMPONENT_REF, type, clsn_data->store,\n+\t\t\t       elt->field, NULL_TREE),\n+\t\t\t      ssa_name (elt->version));\n   mark_virtual_ops_for_renaming (stmt);\n   bsi_insert_after (&bsi, stmt, BSI_NEW_STMT);\n \n   bsi = bsi_last (clsn_data->load_bb);\n   load_struct = fold_build1 (INDIRECT_REF, struct_type, clsn_data->load);\n-  stmt = build_gimple_modify_stmt (\n-\t\t elt->new_name,\n-\t\t build3 (COMPONENT_REF, type, load_struct, elt->field,\n-\t\t\t NULL_TREE));\n+  stmt = build_gimple_modify_stmt (elt->new_name,\n+\t\t\t\t   build3 (COMPONENT_REF, type, load_struct,\n+\t\t\t\t\t   elt->field, NULL_TREE));\n   SSA_NAME_DEF_STMT (elt->new_name) = stmt;\n   bsi_insert_after (&bsi, stmt, BSI_NEW_STMT);\n \n@@ -549,11 +1166,17 @@ create_loads_and_stores_for_name (void **slot, void *data)\n    `old' is stored to *ARG_STRUCT and `new' is stored to NEW_ARG_STRUCT.  The\n    pointer `new' is intentionally not initialized (the loop will be split to a\n    separate function later, and `new' will be initialized from its arguments).\n-   */\n+   LD_ST_DATA holds information about the shared data structure used to pass\n+   information among the threads.  It is initialized here, and \n+   gen_parallel_loop will pass it to create_call_for_reduction that \n+   needs this information.  REDUCTION_LIST describes the reductions \n+   in LOOP.  */\n \n static void\n-separate_decls_in_loop (struct loop *loop, tree *arg_struct,\n-\t\t\ttree *new_arg_struct)\n+separate_decls_in_loop (struct loop *loop, htab_t reduction_list, \n+\t\t\ttree * arg_struct, tree * new_arg_struct, \n+\t\t\tstruct clsn_data *ld_st_data)\n+\n {\n   basic_block bb1 = split_edge (loop_preheader_edge (loop));\n   basic_block bb0 = single_pred (bb1);\n@@ -584,19 +1207,23 @@ separate_decls_in_loop (struct loop *loop, tree *arg_struct,\n   if (htab_elements (name_copies) == 0)\n     {\n       /* It may happen that there is nothing to copy (if there are only\n-\t loop carried and external variables in the loop).  */\n+         loop carried and external variables in the loop).  */\n       *arg_struct = NULL;\n       *new_arg_struct = NULL;\n     }\n   else\n     {\n+      struct data_arg data_arg;\n+\n       /* Create the type for the structure to store the ssa names to.  */\n       type = lang_hooks.types.make_type (RECORD_TYPE);\n       type_name = build_decl (TYPE_DECL, create_tmp_var_name (\".paral_data\"),\n \t\t\t      type);\n       TYPE_NAME (type) = type_name;\n \n-      htab_traverse (name_copies, add_field_for_name, type);\n+      data_arg.type = type;\n+      data_arg.reduction_list = reduction_list;\n+      htab_traverse (name_copies, add_field_for_name, &data_arg);\n       layout_type (type);\n \n       /* Create the loads and stores.  */\n@@ -606,12 +1233,22 @@ separate_decls_in_loop (struct loop *loop, tree *arg_struct,\n       add_referenced_var (nvar);\n       *new_arg_struct = make_ssa_name (nvar, NULL_TREE);\n \n-      clsn_data.store = *arg_struct;\n-      clsn_data.load = *new_arg_struct;\n-      clsn_data.store_bb = bb0;\n-      clsn_data.load_bb = bb1;\n+      ld_st_data->store = *arg_struct;\n+      ld_st_data->load = *new_arg_struct;\n+      ld_st_data->store_bb = bb0;\n+      ld_st_data->load_bb = bb1;\n       htab_traverse (name_copies, create_loads_and_stores_for_name,\n-\t\t     &clsn_data);\n+\t\t     ld_st_data);\n+\n+      /* Load the calculation from memory into a new \n+         reduction variable (after the join of the threads).  */\n+      if (htab_elements (reduction_list) > 0)\n+\t{\n+\t  clsn_data.load = make_ssa_name (nvar, NULL_TREE);\n+\t  clsn_data.load_bb = single_dom_exit (loop)->dest;\n+\t  clsn_data.store = ld_st_data->store;\n+\t  create_final_loads_for_reduction (reduction_list, &clsn_data);\n+\t}\n     }\n \n   htab_delete (decl_copies);\n@@ -692,24 +1329,25 @@ create_loop_fn (void)\n \n /* Bases all the induction variables in LOOP on a single induction variable\n    (unsigned with base 0 and step 1), whose final value is compared with\n-   NIT.  The induction variable is incremented in the loop latch.  */\n+   NIT.  The induction variable is incremented in the loop latch.  \n+   REDUCTION_LIST describes the reductions in LOOP.  */\n \n static void\n-canonicalize_loop_ivs (struct loop *loop, tree nit)\n+canonicalize_loop_ivs (struct loop *loop, htab_t reduction_list, tree nit)\n {\n   unsigned precision = TYPE_PRECISION (TREE_TYPE (nit));\n   tree phi, prev, res, type, var_before, val, atype, t, next;\n   block_stmt_iterator bsi;\n   bool ok;\n   affine_iv iv;\n   edge exit = single_dom_exit (loop);\n+  struct reduction_info *red;\n \n   for (phi = phi_nodes (loop->header); phi; phi = PHI_CHAIN (phi))\n     {\n       res = PHI_RESULT (phi);\n \n-      if (is_gimple_reg (res)\n-\t  && TYPE_PRECISION (TREE_TYPE (res)) > precision)\n+      if (is_gimple_reg (res) && TYPE_PRECISION (TREE_TYPE (res)) > precision)\n \tprecision = TYPE_PRECISION (TREE_TYPE (res));\n     }\n \n@@ -726,16 +1364,22 @@ canonicalize_loop_ivs (struct loop *loop, tree nit)\n       next = PHI_CHAIN (phi);\n       res = PHI_RESULT (phi);\n \n-      if (!is_gimple_reg (res)\n-\t  || res == var_before)\n+      if (!is_gimple_reg (res) || res == var_before)\n \t{\n \t  prev = phi;\n \t  continue;\n \t}\n-      \n-      ok = simple_iv (loop, phi, res, &iv, true);\n-      gcc_assert (ok);\n \n+      ok = simple_iv (loop, phi, res, &iv, true);\n+      red = reduction_phi (reduction_list, phi);\n+      /* We preserve the reduction phi nodes.  */\n+      if (!ok && red)\n+\t{\n+\t  prev = phi;\n+\t  continue;\n+\t}\n+      else\n+\tgcc_assert (ok);\n       remove_phi_node (phi, prev, false);\n \n       atype = TREE_TYPE (res);\n@@ -773,10 +1417,11 @@ canonicalize_loop_ivs (struct loop *loop, tree nit)\n    follows the loop exit.  In this case, it would be better not to copy the\n    body of the loop, but only move the entry of the loop directly before the\n    exit check and increase the number of iterations of the loop by one.\n-   This may need some additional preconditioning in case NIT = ~0.  */\n+   This may need some additional preconditioning in case NIT = ~0.  \n+   REDUCTION_LIST describes the reductions in LOOP.  */\n \n static void\n-transform_to_exit_first_loop (struct loop *loop, tree nit)\n+transform_to_exit_first_loop (struct loop *loop, htab_t reduction_list, tree nit)\n {\n   basic_block *bbs, *nbbs, ex_bb, orig_header;\n   unsigned n;\n@@ -825,17 +1470,36 @@ transform_to_exit_first_loop (struct loop *loop, tree nit)\n   ex_bb = nbbs[0];\n   free (nbbs);\n \n-  /* The only gimple reg that should be copied out of the loop is the\n-     control variable.  */\n+  /* Other than reductions, the only gimple reg that should be copied \n+   out of the loop is the control variable.  */\n+\n   control_name = NULL_TREE;\n   for (phi = phi_nodes (ex_bb); phi; phi = PHI_CHAIN (phi))\n     {\n       res = PHI_RESULT (phi);\n       if (!is_gimple_reg (res))\n \tcontinue;\n \n-      gcc_assert (control_name == NULL_TREE\n-\t\t  && SSA_NAME_VAR (res) == SSA_NAME_VAR (control));\n+      /* Check if it is a part of reduction.  If it is,\n+         keep the phi at the reduction's keep_res field.  The  \n+         PHI_RESULT of this phi is the resulting value of the reduction \n+         variable when exiting the loop.  */\n+\n+      exit = single_dom_exit (loop);\n+\n+      if (htab_elements (reduction_list) > 0) \n+\t{\n+\t  struct reduction_info *red;\n+\n+\t  tree val = PHI_ARG_DEF_FROM_EDGE (phi, exit);\n+\n+\t  red = reduction_phi (reduction_list, SSA_NAME_DEF_STMT (val));\n+\t  if (red)\n+\t    red->keep_res = phi;\n+\t}\n+      else\n+\tgcc_assert (control_name == NULL_TREE\n+\t\t    && SSA_NAME_VAR (res) == SSA_NAME_VAR (control));\n       control_name = res;\n     }\n   gcc_assert (control_name != NULL_TREE);\n@@ -872,9 +1536,8 @@ create_parallel_loop (struct loop *loop, tree loop_fn, tree data,\n \n   t = build_omp_clause (OMP_CLAUSE_NUM_THREADS);\n   OMP_CLAUSE_NUM_THREADS_EXPR (t)\n-\t  = build_int_cst (integer_type_node, n_threads);\n-  t = build4 (OMP_PARALLEL, void_type_node, NULL_TREE, t,\n-\t      loop_fn, data);\n+    = build_int_cst (integer_type_node, n_threads);\n+  t = build4 (OMP_PARALLEL, void_type_node, NULL_TREE, t, loop_fn, data);\n \n   bsi_insert_after (&bsi, t, BSI_NEW_STMT);\n \n@@ -889,7 +1552,8 @@ create_parallel_loop (struct loop *loop, tree loop_fn, tree data,\n       SSA_NAME_DEF_STMT (param) = t;\n \n       t = build_gimple_modify_stmt (new_data,\n-\t\t  fold_convert (TREE_TYPE (new_data), param));\n+\t\t\t\t    fold_convert (TREE_TYPE (new_data),\n+\t\t\t\t\t\t  param));\n       bsi_insert_before (&bsi, t, BSI_SAME_STMT);\n       SSA_NAME_DEF_STMT (new_data) = t;\n     }\n@@ -949,11 +1613,11 @@ create_parallel_loop (struct loop *loop, tree loop_fn, tree data,\n   OMP_FOR_CLAUSES (for_stmt) = t;\n   OMP_FOR_INIT (for_stmt) = build_gimple_modify_stmt (initvar, cvar_init);\n   OMP_FOR_COND (for_stmt) = cond;\n-  OMP_FOR_INCR (for_stmt) = build_gimple_modify_stmt (\n-\t\t\t\tcvar_base,\n-\t\t\t\tbuild2 (PLUS_EXPR, type,\n-\t\t\t\t\tcvar_base,\n-\t\t\t\t\tbuild_int_cst (type, 1)));\n+  OMP_FOR_INCR (for_stmt) = build_gimple_modify_stmt (cvar_base,\n+\t\t\t\t\t\t      build2 (PLUS_EXPR, type,\n+\t\t\t\t\t\t\t      cvar_base,\n+\t\t\t\t\t\t\t      build_int_cst\n+\t\t\t\t\t\t\t      (type, 1)));\n   OMP_FOR_BODY (for_stmt) = NULL_TREE;\n   OMP_FOR_PRE_BODY (for_stmt) = NULL_TREE;\n \n@@ -975,16 +1639,18 @@ create_parallel_loop (struct loop *loop, tree loop_fn, tree data,\n }\n \n /* Generates code to execute the iterations of LOOP in N_THREADS threads in\n-   parallel.  NITER describes number of iterations of LOOP.  */\n+   parallel.  NITER describes number of iterations of LOOP.  \n+   REDUCTION_LIST describes the reductions existant in the LOOP.  */\n \n static void\n-gen_parallel_loop (struct loop *loop, unsigned n_threads,\n-\t\t   struct tree_niter_desc *niter)\n+gen_parallel_loop (struct loop *loop, htab_t reduction_list, \n+\t\t   unsigned n_threads, struct tree_niter_desc *niter)\n {\n   struct loop *nloop;\n   tree many_iterations_cond, type, nit;\n   tree stmts, arg_struct, new_arg_struct;\n   basic_block parallel_head;\n+  struct clsn_data clsn_data;\n   unsigned prob;\n \n   /* From\n@@ -1006,8 +1672,8 @@ gen_parallel_loop (struct loop *loop, unsigned n_threads,\n      ---------------------------------------------------------------------\n \n      if (MAY_BE_ZERO\n-\t || NITER < MIN_PER_THREAD * N_THREADS)\n-       goto original;\n+     || NITER < MIN_PER_THREAD * N_THREADS)\n+     goto original;\n \n      BODY1;\n      store all local loop-invariant variables used in body of the loop to DATA.\n@@ -1017,8 +1683,8 @@ gen_parallel_loop (struct loop *loop, unsigned n_threads,\n      BODY2;\n      BODY1;\n      OMP_CONTINUE;\n-     OMP_RETURN\t\t-- OMP_FOR\n-     OMP_RETURN\t\t-- OMP_PARALLEL\n+     OMP_RETURN         -- OMP_FOR\n+     OMP_RETURN         -- OMP_PARALLEL\n      goto end;\n \n      original:\n@@ -1039,30 +1705,29 @@ gen_parallel_loop (struct loop *loop, unsigned n_threads,\n      number of iterations is large enough, and we will transform it into the\n      loop that will be split to loop_fn, the new one will be used for the\n      remaining iterations.  */\n-  \n+\n   type = TREE_TYPE (niter->niter);\n   nit = force_gimple_operand (unshare_expr (niter->niter), &stmts, true,\n \t\t\t      NULL_TREE);\n   if (stmts)\n     bsi_insert_on_edge_immediate (loop_preheader_edge (loop), stmts);\n \n   many_iterations_cond =\n-\t  fold_build2 (GE_EXPR, boolean_type_node,\n-\t\t       nit, build_int_cst (type, MIN_PER_THREAD * n_threads));\n+    fold_build2 (GE_EXPR, boolean_type_node,\n+\t\t nit, build_int_cst (type, MIN_PER_THREAD * n_threads));\n   many_iterations_cond\n-\t  = fold_build2 (TRUTH_AND_EXPR, boolean_type_node,\n-\t\t\t invert_truthvalue (unshare_expr (niter->may_be_zero)),\n-\t\t\t many_iterations_cond);\n+    = fold_build2 (TRUTH_AND_EXPR, boolean_type_node,\n+\t\t   invert_truthvalue (unshare_expr (niter->may_be_zero)),\n+\t\t   many_iterations_cond);\n   many_iterations_cond\n-\t  = force_gimple_operand (many_iterations_cond, &stmts,\n-\t\t\t\t  false, NULL_TREE);\n+    = force_gimple_operand (many_iterations_cond, &stmts, false, NULL_TREE);\n   if (stmts)\n     bsi_insert_on_edge_immediate (loop_preheader_edge (loop), stmts);\n   if (!is_gimple_condexpr (many_iterations_cond))\n     {\n       many_iterations_cond\n-\t      = force_gimple_operand (many_iterations_cond, &stmts,\n-\t\t\t\t      true, NULL_TREE);\n+\t= force_gimple_operand (many_iterations_cond, &stmts,\n+\t\t\t\ttrue, NULL_TREE);\n       if (stmts)\n \tbsi_insert_on_edge_immediate (loop_preheader_edge (loop), stmts);\n     }\n@@ -1077,21 +1742,29 @@ gen_parallel_loop (struct loop *loop, unsigned n_threads,\n   free_original_copy_tables ();\n \n   /* Base all the induction variables in LOOP on a single control one.  */\n-  canonicalize_loop_ivs (loop, nit);\n+  canonicalize_loop_ivs (loop, reduction_list, nit);\n \n   /* Ensure that the exit condition is the first statement in the loop.  */\n-  transform_to_exit_first_loop (loop, nit);\n+  transform_to_exit_first_loop (loop, reduction_list, nit);\n+\n+\n+  /* Generate intializations for reductions.  */\n+\n+  if (htab_elements (reduction_list) > 0)  \n+    htab_traverse (reduction_list, initialize_reductions, loop);\n \n   /* Eliminate the references to local variables from the loop.  */\n   eliminate_local_variables (loop);\n \n   /* In the old loop, move all variables non-local to the loop to a structure\n      and back, and create separate decls for the variables used in loop.  */\n-  separate_decls_in_loop (loop, &arg_struct, &new_arg_struct);\n+  separate_decls_in_loop (loop, reduction_list, &arg_struct, &new_arg_struct, &clsn_data);\n \n   /* Create the parallel constructs.  */\n   parallel_head = create_parallel_loop (loop, create_loop_fn (), arg_struct,\n \t\t\t\t\tnew_arg_struct, n_threads);\n+  if (htab_elements (reduction_list) > 0)   \n+    create_call_for_reduction (loop, reduction_list, &clsn_data);\n \n   scev_reset ();\n \n@@ -1103,6 +1776,7 @@ gen_parallel_loop (struct loop *loop, unsigned n_threads,\n      a separate expand_omp pass, since it is more efficient, and less likely to\n      cause troubles with further analyses not being able to deal with the\n      OMP trees.  */\n+\n   omp_expand_local (parallel_head);\n }\n \n@@ -1118,30 +1792,36 @@ parallelize_loops (void)\n   struct loop *loop;\n   struct tree_niter_desc niter_desc;\n   loop_iterator li;\n+  htab_t reduction_list;\n \n   /* Do not parallelize loops in the functions created by parallelization.  */\n   if (parallelized_function_p (cfun->decl))\n     return false;\n \n+  reduction_list = htab_create (10, reduction_info_hash,\n+                                reduction_info_eq, free);\n+\n   FOR_EACH_LOOP (li, loop, 0)\n     {\n+      htab_empty (reduction_list);\n       if (/* Do not bother with loops in cold areas.  */\n \t  !maybe_hot_bb_p (loop->header)\n \t  /* Or loops that roll too little.  */\n \t  || expected_loop_iterations (loop) <= n_threads\n \t  /* And of course, the loop must be parallelizable.  */\n \t  || !can_duplicate_loop_p (loop)\n-\t  || !loop_parallel_p (loop, &niter_desc))\n+\t  || !loop_parallel_p (loop, reduction_list, &niter_desc))\n \tcontinue;\n \n       changed = true;\n-      gen_parallel_loop (loop, n_threads, &niter_desc);\n+      gen_parallel_loop (loop, reduction_list, n_threads, &niter_desc);\n       verify_flow_info ();\n       verify_dominators (CDI_DOMINATORS);\n       verify_loop_structure ();\n       verify_loop_closed_ssa ();\n     }\n \n+  htab_delete (reduction_list);\n   return changed;\n }\n "}, {"sha": "d59ec894229753686ec05746a187415c526ad981", "filename": "gcc/tree-pretty-print.c", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-pretty-print.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-pretty-print.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-pretty-print.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -1941,6 +1941,23 @@ dump_generic_node (pretty_printer *buffer, tree node, int spc, int flags,\n       dump_generic_node (buffer, TREE_OPERAND (node, 1), spc, flags, false);\n       break;\n \n+    case OMP_ATOMIC_LOAD:\n+      pp_string (buffer, \"#pragma omp atomic_load\");\n+      newline_and_indent (buffer, spc + 2);\n+      dump_generic_node (buffer, TREE_OPERAND (node, 0), spc, flags, false);\n+      pp_space (buffer);\n+      pp_character (buffer, '=');\n+      pp_space (buffer);\n+      pp_character (buffer, '*');\n+      dump_generic_node (buffer, TREE_OPERAND (node, 1), spc, flags, false);\n+      break;\n+\n+    case OMP_ATOMIC_STORE:\n+      pp_string (buffer, \"#pragma omp atomic_store (\");\n+      dump_generic_node (buffer, TREE_OPERAND (node, 0), spc, flags, false);\n+      pp_character (buffer, ')');\n+      break;\n+\n     case OMP_SINGLE:\n       pp_string (buffer, \"#pragma omp single\");\n       dump_omp_clauses (buffer, OMP_SINGLE_CLAUSES (node), spc, flags);"}, {"sha": "846fbb784eba093f685f268da368cd4c2f3010d7", "filename": "gcc/tree-ssa-operands.c", "status": "modified", "additions": 62, "deletions": 28, "changes": 90, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-ssa-operands.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-ssa-operands.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-operands.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -1624,38 +1624,20 @@ add_stmt_operand (tree *var_p, stmt_ann_t s_ann, int flags)\n     add_virtual_operand (var, s_ann, flags, NULL_TREE, 0, -1, false);\n }\n \n-\n-/* A subroutine of get_expr_operands to handle INDIRECT_REF,\n-   ALIGN_INDIRECT_REF and MISALIGNED_INDIRECT_REF.  \n-\n-   STMT is the statement being processed, EXPR is the INDIRECT_REF\n-      that got us here.\n-   \n-   FLAGS is as in get_expr_operands.\n-\n-   FULL_REF contains the full pointer dereference expression, if we\n-      have it, or NULL otherwise.\n-\n-   OFFSET and SIZE are the location of the access inside the\n-      dereferenced pointer, if known.\n-\n-   RECURSE_ON_BASE should be set to true if we want to continue\n-      calling get_expr_operands on the base pointer, and false if\n-      something else will do it for us.  */\n+/* Subroutine of get_indirect_ref_operands.  ADDR is the address\n+   that is dereferenced, the meaning of the rest of the arguments\n+   is the same as in get_indirect_ref_operands.  */\n \n static void\n-get_indirect_ref_operands (tree stmt, tree expr, int flags,\n-\t\t\t   tree full_ref,\n-\t\t\t   HOST_WIDE_INT offset, HOST_WIDE_INT size,\n-\t\t\t   bool recurse_on_base)\n-{\n-  tree *pptr = &TREE_OPERAND (expr, 0);\n-  tree ptr = *pptr;\n+get_addr_dereference_operands (tree stmt, tree *addr, int flags,\n+                                                      tree full_ref,\n+                                                       HOST_WIDE_INT offset, HOST_WIDE_INT size,\n+                                                       bool recurse_on_base)\n+  {\n+ tree ptr = *addr;\n   stmt_ann_t s_ann = stmt_ann (stmt);\n \n   s_ann->references_memory = true;\n-  if (TREE_THIS_VOLATILE (expr))\n-    s_ann->has_volatile_ops = true; \n \n   if (SSA_VAR_P (ptr))\n     {\n@@ -1725,9 +1707,42 @@ get_indirect_ref_operands (tree stmt, tree expr, int flags,\n \n   /* If requested, add a USE operand for the base pointer.  */\n   if (recurse_on_base)\n-    get_expr_operands (stmt, pptr, opf_use);\n+    get_expr_operands (stmt, addr, opf_use);\n }\n \n+/* A subroutine of get_expr_operands to handle INDIRECT_REF,\n+   ALIGN_INDIRECT_REF and MISALIGNED_INDIRECT_REF.  \n+\n+   STMT is the statement being processed, EXPR is the INDIRECT_REF\n+      that got us here.\n+   \n+   FLAGS is as in get_expr_operands.\n+\n+   FULL_REF contains the full pointer dereference expression, if we\n+      have it, or NULL otherwise.\n+\n+   OFFSET and SIZE are the location of the access inside the\n+      dereferenced pointer, if known.\n+\n+   RECURSE_ON_BASE should be set to true if we want to continue\n+      calling get_expr_operands on the base pointer, and false if\n+      something else will do it for us.  */\n+\n+static void\n+get_indirect_ref_operands (tree stmt, tree expr, int flags,\n+\t\t \t\t \t\t    tree full_ref,\n+\t\t \t\t \t\t    HOST_WIDE_INT offset, HOST_WIDE_INT size,\n+\t\t \t\t \t\t    bool recurse_on_base)\n+{\n+  tree *pptr = &TREE_OPERAND (expr, 0);\n+  stmt_ann_t s_ann = stmt_ann (stmt);\n+\n+  if (TREE_THIS_VOLATILE (expr))\n+    s_ann->has_volatile_ops = true; \n+\n+  get_addr_dereference_operands (stmt, pptr, flags, full_ref,\n+\t\t \t\t \t\t \t\t  offset, size, recurse_on_base);\n+}\n \n /* A subroutine of get_expr_operands to handle TARGET_MEM_REF.  */\n \n@@ -2334,6 +2349,25 @@ get_expr_operands (tree stmt, tree *expr_p, int flags)\n \treturn;\n       }\n \n+    case OMP_ATOMIC_LOAD:\n+      {\n+\t\t tree *addr = &TREE_OPERAND (expr, 1);\n+\t\t get_expr_operands (stmt, &TREE_OPERAND (expr, 0), opf_def);\n+\n+\t\t if (TREE_CODE (*addr) == ADDR_EXPR)\n+\t\t   get_expr_operands (stmt, &TREE_OPERAND (*addr, 0), opf_def);\n+\t\t else\n+\t\t   get_addr_dereference_operands (stmt, addr, opf_def,\n+\t\t \t\t \t\t \t\t \t\t  NULL_TREE, 0, -1, true);\n+\t\t return;\n+      }\n+\n+    case OMP_ATOMIC_STORE:\n+      {\n+\t\t get_expr_operands (stmt, &TREE_OPERAND (expr, 0), opf_use);\n+\t\t return;\n+      }\n+\n     case BLOCK:\n     case FUNCTION_DECL:\n     case EXC_PTR_EXPR:"}, {"sha": "f741903b718253651c3afca54aada8b6bb0c53c2", "filename": "gcc/tree-vect-analyze.c", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-vect-analyze.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-vect-analyze.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-analyze.c?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -42,7 +42,6 @@ along with GCC; see the file COPYING3.  If not see\n #include \"recog.h\"\n \n /* Main analysis functions.  */\n-static loop_vec_info vect_analyze_loop_form (struct loop *);\n static bool vect_analyze_data_refs (loop_vec_info);\n static bool vect_mark_stmts_to_be_vectorized (loop_vec_info);\n static void vect_analyze_scalar_cycles (loop_vec_info);\n@@ -3998,7 +3997,7 @@ vect_analyze_loop_1 (struct loop *loop)\n    - the loop exit condition is simple enough, and the number of iterations\n      can be analyzed (a countable loop).  */\n \n-static loop_vec_info\n+loop_vec_info\n vect_analyze_loop_form (struct loop *loop)\n {\n   loop_vec_info loop_vinfo;"}, {"sha": "cd583f6b9dac5bd21a56de944bcce11a7c4a8552", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -666,7 +666,7 @@ extern stmt_vec_info new_stmt_vec_info (tree stmt, loop_vec_info);\n /* Driver for analysis stage.  */\n extern loop_vec_info vect_analyze_loop (struct loop *);\n extern void vect_free_slp_tree (slp_tree);\n-\n+extern loop_vec_info vect_analyze_loop_form (struct loop *);\n \n /** In tree-vect-patterns.c  **/\n /* Pattern recognition functions."}, {"sha": "5693749a51f93b60aeef1b5c3b0064799905354d", "filename": "gcc/tree.def", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree.def?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -1059,6 +1059,18 @@ DEFTREECODE (OMP_CONTINUE, \"omp_continue\", tcc_statement, 2)\n \tbuild_fold_indirect_ref of the address.  */\n DEFTREECODE (OMP_ATOMIC, \"omp_atomic\", tcc_statement, 2)\n \n+/* Codes used for lowering of OMP_ATOMIC.  Although the form of the OMP_ATOMIC\n+   statement is very simple (just in form mem op= expr), various implicit\n+   conversions may cause the expression become more complex, so that it does\n+   not fit the gimple grammar very well.  To overcome this problem, OMP_ATOMIC\n+   is rewritten as a sequence of two codes in gimplification:\n+\n+   OMP_LOAD (tmp, mem)\n+   val = some computations involving tmp;\n+   OMP_STORE (val)  */\n+DEFTREECODE (OMP_ATOMIC_LOAD, \"omp_atomic_load\", tcc_statement, 2)\n+DEFTREECODE (OMP_ATOMIC_STORE, \"omp_atomic_store\", tcc_statement, 1)\n+\n /* OpenMP clauses.  */\n DEFTREECODE (OMP_CLAUSE, \"omp_clause\", tcc_exceptional, 0)\n "}, {"sha": "da71a8c667632339e1c3fd42af7d691340d8276a", "filename": "gcc/tree.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a509ebb5a12b96267a948e5c82ff7c4f0a4826e4/gcc%2Ftree.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree.h?ref=a509ebb5a12b96267a948e5c82ff7c4f0a4826e4", "patch": "@@ -195,6 +195,8 @@ extern const enum tree_code_class tree_code_type[];\n      || TREE_CODE (NODE) == OMP_ORDERED\t\t\t\\\n      || TREE_CODE (NODE) == OMP_CRITICAL\t\t\\\n      || TREE_CODE (NODE) == OMP_RETURN\t\t\t\\\n+     || TREE_CODE (NODE) == OMP_ATOMIC_LOAD                            \\\n+     || TREE_CODE (NODE) == OMP_ATOMIC_STORE                           \\\n      || TREE_CODE (NODE) == OMP_CONTINUE)\n \n /* Number of argument-words in each kind of tree-node.  */"}]}