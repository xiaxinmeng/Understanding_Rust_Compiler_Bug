{"sha": "685fe0325cddc64776cc8e6c504ef40b35c846e0", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6Njg1ZmUwMzI1Y2RkYzY0Nzc2Y2M4ZTZjNTA0ZWY0MGIzNWM4NDZlMA==", "commit": {"author": {"name": "Richard Henderson", "email": "rth@redhat.com", "date": "2004-01-28T18:49:26Z"}, "committer": {"name": "Richard Henderson", "email": "rth@gcc.gnu.org", "date": "2004-01-28T18:49:26Z"}, "message": "ggc.h (ggc_free): Declare.\n\n        * ggc.h (ggc_free): Declare.\n        * ggc-common.c (ggc_realloc): Use it.\n        * ggc-page.c: Remove lots of inline markers.\n        (globals): Add free_object_list.\n        (ggc_alloc): Tidy.\n        (ggc_free, validate_free_objects): New.\n        (poison_pages): Provide default.\n        (ggc_collect): Call validate_free_objects; emit markers to\n        the debug file.\n\nFrom-SVN: r76801", "tree": {"sha": "b2610d09544f2d611e1c960a1c767328e33944ed", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/b2610d09544f2d611e1c960a1c767328e33944ed"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/685fe0325cddc64776cc8e6c504ef40b35c846e0", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/685fe0325cddc64776cc8e6c504ef40b35c846e0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/685fe0325cddc64776cc8e6c504ef40b35c846e0", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/685fe0325cddc64776cc8e6c504ef40b35c846e0/comments", "author": null, "committer": null, "parents": [{"sha": "42e5a9b95768990d55a44628a1faa1f7a3d0ab31", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/42e5a9b95768990d55a44628a1faa1f7a3d0ab31", "html_url": "https://github.com/Rust-GCC/gccrs/commit/42e5a9b95768990d55a44628a1faa1f7a3d0ab31"}], "stats": {"total": 225, "additions": 184, "deletions": 41}, "files": [{"sha": "64fbd7e420b0cbb87a18702a4d17f462d18372fb", "filename": "gcc/ChangeLog", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/685fe0325cddc64776cc8e6c504ef40b35c846e0/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/685fe0325cddc64776cc8e6c504ef40b35c846e0/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=685fe0325cddc64776cc8e6c504ef40b35c846e0", "patch": "@@ -1,3 +1,15 @@\n+2004-01-28  Richard Henderson  <rth@redhat.com>\n+\n+\t* ggc.h (ggc_free): Declare.\n+\t* ggc-common.c (ggc_realloc): Use it.\n+\t* ggc-page.c: Remove lots of inline markers.\n+\t(globals): Add free_object_list.\n+\t(ggc_alloc): Tidy.\n+\t(ggc_free, validate_free_objects): New.\n+\t(poison_pages): Provide default.\n+\t(ggc_collect): Call validate_free_objects; emit markers to\n+\tthe debug file.\n+\n 2004-01-28  Zack Weinberg  <zack@codesourcery.com>\n \t    Jim Wilson  <wilson@specifixinc.com>\n "}, {"sha": "cf9bd0056968bb5d10a307dd2556090b547335ef", "filename": "gcc/ggc-common.c", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/685fe0325cddc64776cc8e6c504ef40b35c846e0/gcc%2Fggc-common.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/685fe0325cddc64776cc8e6c504ef40b35c846e0/gcc%2Fggc-common.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fggc-common.c?ref=685fe0325cddc64776cc8e6c504ef40b35c846e0", "patch": "@@ -147,6 +147,7 @@ ggc_realloc (void *x, size_t size)\n     return ggc_alloc (size);\n \n   old_size = ggc_get_size (x);\n+\n   if (size <= old_size)\n     {\n       /* Mark the unwanted memory as unaccessible.  We also need to make\n@@ -176,7 +177,7 @@ ggc_realloc (void *x, size_t size)\n   memcpy (r, x, old_size);\n \n   /* The old object is not supposed to be used anymore.  */\n-  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (x, old_size));\n+  ggc_free (x);\n \n   return r;\n }"}, {"sha": "e6f78461ae7fa52bce6574839358a9637820b534", "filename": "gcc/ggc-page.c", "status": "modified", "additions": 168, "deletions": 40, "changes": 208, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/685fe0325cddc64776cc8e6c504ef40b35c846e0/gcc%2Fggc-page.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/685fe0325cddc64776cc8e6c504ef40b35c846e0/gcc%2Fggc-page.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fggc-page.c?ref=685fe0325cddc64776cc8e6c504ef40b35c846e0", "patch": "@@ -401,6 +401,17 @@ static struct globals\n      zero otherwise.  We allocate them all together, to enable a\n      better runtime data access pattern.  */\n   unsigned long **save_in_use;\n+\n+#ifdef ENABLE_GC_ALWAYS_COLLECT\n+  /* List of free objects to be verified as actually free on the\n+     next collection.  */\n+  struct free_object\n+  {\n+    void *object;\n+    struct free_object *next;\n+  } *free_object_list;\n+#endif\n+\n #ifdef GATHER_STATISTICS\n   struct\n   {\n@@ -466,10 +477,6 @@ static void compute_inverse (unsigned);\n static inline void adjust_depth (void);\n static void move_ptes_to_front (int, int);\n \n-#ifdef ENABLE_GC_CHECKING\n-static void poison_pages (void);\n-#endif\n-\n void debug_print_page_list (int);\n static void push_depth (unsigned int);\n static void push_by_depth (page_entry *, unsigned long *);\n@@ -894,7 +901,7 @@ adjust_depth (void)\n \n /* For a page that is no longer needed, put it on the free page list.  */\n \n-static inline void\n+static void\n free_page (page_entry *entry)\n {\n   if (GGC_DEBUG_LEVEL >= 2)\n@@ -1049,16 +1056,19 @@ ggc_alloc_zone (size_t size, struct alloc_zone *zone ATTRIBUTE_UNUSED)\n void *\n ggc_alloc (size_t size)\n {\n-  unsigned order, word, bit, object_offset;\n+  size_t order, word, bit, object_offset, object_size;\n   struct page_entry *entry;\n   void *result;\n \n   if (size <= 256)\n-    order = size_lookup[size];\n+    {\n+      order = size_lookup[size];\n+      object_size = OBJECT_SIZE (order);\n+    }\n   else\n     {\n       order = 9;\n-      while (size > OBJECT_SIZE (order))\n+      while (size > (object_size = OBJECT_SIZE (order)))\n \torder++;\n     }\n \n@@ -1121,7 +1131,7 @@ ggc_alloc (size_t size)\n       /* Next time, try the next bit.  */\n       entry->next_bit_hint = hint + 1;\n \n-      object_offset = hint * OBJECT_SIZE (order);\n+      object_offset = hint * object_size;\n     }\n \n   /* Set the in-use bit.  */\n@@ -1149,16 +1159,16 @@ ggc_alloc (size_t size)\n      exact same semantics in presence of memory bugs, regardless of\n      ENABLE_VALGRIND_CHECKING.  We override this request below.  Drop the\n      handle to avoid handle leak.  */\n-  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (result, OBJECT_SIZE (order)));\n+  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (result, object_size));\n \n   /* `Poison' the entire allocated object, including any padding at\n      the end.  */\n-  memset (result, 0xaf, OBJECT_SIZE (order));\n+  memset (result, 0xaf, object_size);\n \n   /* Make the bytes after the end of the object unaccessible.  Discard the\n      handle to avoid handle leak.  */\n   VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS ((char *) result + size,\n-\t\t\t\t\t    OBJECT_SIZE (order) - size));\n+\t\t\t\t\t    object_size - size));\n #endif\n \n   /* Tell Valgrind that the memory is there, but its content isn't\n@@ -1168,37 +1178,39 @@ ggc_alloc (size_t size)\n \n   /* Keep track of how many bytes are being allocated.  This\n      information is used in deciding when to collect.  */\n-  G.allocated += OBJECT_SIZE (order);\n+  G.allocated += object_size;\n \n #ifdef GATHER_STATISTICS\n   {\n-    G.stats.total_overhead += OBJECT_SIZE (order) - size;\n-    G.stats.total_allocated += OBJECT_SIZE(order);\n-    G.stats.total_overhead_per_order[order] += OBJECT_SIZE (order) - size;\n-    G.stats.total_allocated_per_order[order] += OBJECT_SIZE (order);\n-\n-    if (size <= 32){\n-      G.stats.total_overhead_under32 += OBJECT_SIZE (order) - size;\n-      G.stats.total_allocated_under32 += OBJECT_SIZE(order);\n-    }\n+    size_t overhead = object_size - size;\n \n-    if (size <= 64){\n-      G.stats.total_overhead_under64 += OBJECT_SIZE (order) - size;\n-      G.stats.total_allocated_under64 += OBJECT_SIZE(order);\n-    }\n-  \n-    if (size <= 128){\n-      G.stats.total_overhead_under128 += OBJECT_SIZE (order) - size;\n-      G.stats.total_allocated_under128 += OBJECT_SIZE(order);\n-    }\n+    G.stats.total_overhead += overhead;\n+    G.stats.total_allocated += object_size;\n+    G.stats.total_overhead_per_order[order] += overhead;\n+    G.stats.total_allocated_per_order[order] += object_size;\n \n+    if (size <= 32)\n+      {\n+\tG.stats.total_overhead_under32 += overhead;\n+\tG.stats.total_allocated_under32 += object_size;\n+      }\n+    if (size <= 64)\n+      {\n+\tG.stats.total_overhead_under64 += overhead;\n+\tG.stats.total_allocated_under64 += object_size;\n+      }\n+    if (size <= 128)\n+      {\n+\tG.stats.total_overhead_under128 += overhead;\n+\tG.stats.total_allocated_under128 += object_size;\n+      }\n   }\n #endif\n-  \n+\n   if (GGC_DEBUG_LEVEL >= 3)\n     fprintf (G.debug_file,\n \t     \"Allocating object, requested size=%lu, actual=%lu at %p on %p\\n\",\n-\t     (unsigned long) size, (unsigned long) OBJECT_SIZE (order), result,\n+\t     (unsigned long) size, (unsigned long) object_size, result,\n \t     (void *) entry);\n \n   return result;\n@@ -1279,6 +1291,78 @@ ggc_get_size (const void *p)\n   page_entry *pe = lookup_page_table_entry (p);\n   return OBJECT_SIZE (pe->order);\n }\n+\n+/* Release the memory for object P.  */\n+\n+void\n+ggc_free (void *p)\n+{\n+  page_entry *pe = lookup_page_table_entry (p);\n+  size_t order = pe->order;\n+  size_t size = OBJECT_SIZE (order);\n+\n+  if (GGC_DEBUG_LEVEL >= 3)\n+    fprintf (G.debug_file,\n+\t     \"Freeing object, actual size=%lu, at %p on %p\\n\",\n+\t     (unsigned long) size, p, (void *) pe);\n+\n+#ifdef ENABLE_GC_CHECKING\n+  /* Poison the data, to indicate the data is garbage.  */\n+  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (p, size));\n+  memset (p, 0xa5, size);\n+#endif\n+  /* Let valgrind know the object is free.  */\n+  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (p, size));\n+\n+#ifdef ENABLE_GC_ALWAYS_COLLECT\n+  /* In the completely-anal-checking mode, we do *not* immediately free\n+     the data, but instead verify that the data is *actually* not \n+     reachable the next time we collect.  */\n+  {\n+    struct free_object *fo = xmalloc (sizeof (struct free_object));\n+    fo->object = p;\n+    fo->next = G.free_object_list;\n+    G.free_object_list = fo;\n+  }\n+#else\n+  {\n+    unsigned int bit_offset, word, bit;\n+\n+    G.allocated -= size;\n+\n+    /* Mark the object not-in-use.  */\n+    bit_offset = OFFSET_TO_BIT (((const char *) p) - pe->page, order);\n+    word = bit_offset / HOST_BITS_PER_LONG;\n+    bit = bit_offset % HOST_BITS_PER_LONG;\n+    pe->in_use_p[word] &= ~(1UL << bit);\n+\n+    if (pe->num_free_objects++ == 0)\n+      {\n+\t/* If the page is completely full, then it's supposed to\n+\t   be after all pages that aren't.  Since we've freed one\n+\t   object from a page that was full, we need to move the\n+\t   page to the head of the list.  */\n+\n+\tpage_entry *p, *q;\n+\tfor (q = NULL, p = G.pages[order]; ; q = p, p = p->next)\n+\t  if (p == pe)\n+\t    break;\n+\tif (q && q->num_free_objects == 0)\n+\t  {\n+\t    p = pe->next;\n+\t    q->next = p;\n+\t    if (!p)\n+\t      G.page_tails[order] = q;\n+\t    pe->next = G.pages[order];\n+\t    G.pages[order] = pe;\n+\t  }\n+\n+\t/* Reset the hint bit to point to the only free object.  */\n+\tpe->next_bit_hint = bit_offset;\n+      }\n+  }\n+#endif\n+}\n \f\n /* Subroutine of init_ggc which computes the pair of numbers used to\n    perform division by OBJECT_SIZE (order) and fills in inverse_table[].\n@@ -1567,7 +1651,7 @@ ggc_pop_context (void)\n \f\n /* Unmark all objects.  */\n \n-static inline void\n+static void\n clear_marks (void)\n {\n   unsigned order;\n@@ -1612,7 +1696,7 @@ clear_marks (void)\n /* Free all empty pages.  Partially empty pages need no attention\n    because the `mark' bit doubles as an `unused' bit.  */\n \n-static inline void\n+static void\n sweep_pages (void)\n {\n   unsigned order;\n@@ -1721,7 +1805,7 @@ sweep_pages (void)\n #ifdef ENABLE_GC_CHECKING\n /* Clobber all free objects.  */\n \n-static inline void\n+static void\n poison_pages (void)\n {\n   unsigned order;\n@@ -1767,6 +1851,49 @@ poison_pages (void)\n \t}\n     }\n }\n+#else\n+#define poison_pages()\n+#endif\n+\n+#ifdef ENABLE_GC_ALWAYS_COLLECT\n+/* Validate that the reportedly free objects actually are.  */\n+\n+static void\n+validate_free_objects (void)\n+{\n+  struct free_object *f, *next, *still_free = NULL;\n+\n+  for (f = G.free_object_list; f ; f = next)\n+    {\n+      page_entry *pe = lookup_page_table_entry (f->object);\n+      size_t bit, word;\n+\n+      bit = OFFSET_TO_BIT ((char *)f->object - pe->page, pe->order);\n+      word = bit / HOST_BITS_PER_LONG;\n+      bit = bit % HOST_BITS_PER_LONG;\n+      next = f->next;\n+\n+      /* Make certain it isn't visible from any root.  Notice that we\n+\t do this check before sweep_pages merges save_in_use_p.  */\n+      if (pe->in_use_p[word] & (1UL << bit))\n+\tabort ();\n+\n+      /* If the object comes from an outer context, then retain the\n+\t free_object entry, so that we can verify that the address\n+\t isn't live on the stack in some outer context.  */\n+      if (pe->context_depth != G.context_depth)\n+\t{\n+\t  f->next = still_free;\n+\t  still_free = f;\n+\t}\n+      else\n+\tfree (f);\n+    }\n+\n+  G.free_object_list = still_free;\n+}\n+#else\n+#define validate_free_objects()\n #endif\n \n /* Top level mark-and-sweep routine.  */\n@@ -1788,6 +1915,8 @@ ggc_collect (void)\n   timevar_push (TV_GC);\n   if (!quiet_flag)\n     fprintf (stderr, \" {GC %luk -> \", (unsigned long) G.allocated / 1024);\n+  if (GGC_DEBUG_LEVEL >= 2)\n+    fprintf (G.debug_file, \"BEGIN COLLECTING\\n\");\n \n   /* Zero the total allocated bytes.  This will be recalculated in the\n      sweep phase.  */\n@@ -1802,11 +1931,8 @@ ggc_collect (void)\n \n   clear_marks ();\n   ggc_mark_roots ();\n-\n-#ifdef ENABLE_GC_CHECKING\n   poison_pages ();\n-#endif\n-\n+  validate_free_objects ();\n   sweep_pages ();\n \n   G.allocated_last_gc = G.allocated;\n@@ -1815,6 +1941,8 @@ ggc_collect (void)\n \n   if (!quiet_flag)\n     fprintf (stderr, \"%luk}\", (unsigned long) G.allocated / 1024);\n+  if (GGC_DEBUG_LEVEL >= 2)\n+    fprintf (G.debug_file, \"END COLLECTING\\n\");\n }\n \n /* Print allocation statistics.  */"}, {"sha": "fbd1e3489e2a3bc35b190f16ea2c00db233bcb6c", "filename": "gcc/ggc.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/685fe0325cddc64776cc8e6c504ef40b35c846e0/gcc%2Fggc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/685fe0325cddc64776cc8e6c504ef40b35c846e0/gcc%2Fggc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fggc.h?ref=685fe0325cddc64776cc8e6c504ef40b35c846e0", "patch": "@@ -223,6 +223,8 @@ extern void *ggc_alloc_cleared_zone (size_t, struct alloc_zone *);\n extern void *ggc_realloc (void *, size_t);\n /* Like ggc_alloc_cleared, but performs a multiplication.  */\n extern void *ggc_calloc (size_t, size_t);\n+/* Free a block.  To be used when known for certain it's not reachable.  */\n+extern void ggc_free (void *);\n \n #define ggc_alloc_rtx(CODE)                    \\\n   ((rtx) ggc_alloc_typed (gt_ggc_e_7rtx_def, RTX_SIZE (CODE)))"}]}