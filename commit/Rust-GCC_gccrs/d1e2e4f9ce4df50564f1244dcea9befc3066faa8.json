{"sha": "d1e2e4f9ce4df50564f1244dcea9befc3066faa8", "node_id": "C_kwDOANBUbNoAKGQxZTJlNGY5Y2U0ZGY1MDU2NGYxMjQ0ZGNlYTliZWZjMzA2NmZhYTg", "commit": {"author": {"name": "Martin Jambor", "email": "mjambor@suse.cz", "date": "2021-10-27T12:49:01Z"}, "committer": {"name": "Martin Jambor", "email": "mjambor@suse.cz", "date": "2021-10-27T12:49:56Z"}, "message": "ipa-cp: Fix updating of profile counts and self-gen value evaluation\n\nIPA-CP does not do a reasonable job when it is updating profile counts\nafter it has created clones of recursive functions.  This patch\naddresses that by:\n\n1. Only updating counts for special-context clones.  When a clone is\ncreated for all contexts, the original is going to be dead and the\ncgraph machinery has copied counts to the new node which is the right\nthing to do.  Therefore updating counts has been moved from\ncreate_specialized_node to decide_about_value and\ndecide_whether_version_node.\n\n2. The current profile updating code artificially increased the assumed\nold count when the sum of counts of incoming edges to both the\noriginal and new node were bigger than the count of the original\nnode.  This always happened when self-recursive edge from the clone\nwas also redirected to the clone because both the original edge and\nits clone had original high counts.  This clutch was removed and\nreplaced by the next point.\n\n3. When cloning also redirects a self-recursive clone to the clone\nitself, new logic has been added to divide the counts brought by such\nrecursive edges between the original node and the clone.  This is\nimpossible to do well without special knowledge about the function and\nwhich non-recursive entry calls are responsible for what portion of\nrecursion depth, so the approach taken is rather crude.\n\nFor local nodes, we detect the case when the original node is never\ncalled (in the training run at least) with another value and if so,\nsteal all its counts like if it was dead.  If that is not the case, we\ntry to divide the count brought by recursive edges (or rather not\nbrought by direct edges) proportionally to the counts brought by\nnon-recursive edges - but with artificial limits in place so that we\ndo not take too many or too few, because that was happening with\ndetrimental effect in mcf_r.\n\n4. When cloning creates extra clones for values brought by a formerly\nself-recursive edge with an arithmetic pass-through jump function on\nit, such as it does in exchange2_r, all such clones are processed at\nonce rather than one after another.  The counts of all such nodes are\ndistributed evenly (modulo even-formerly-non-recursive-edges) and the\nwhole situation is then fixed up so that the edge counts fit.  This is\nwhat new function update_counts_for_self_gen_clones does.\n\n5. When values brought by a formerly self-recursive edge with an\narithmetic pass-through jump function on it are evaluated by\nheuristics which assumes vast majority of node counts are result of\nrecursive calls and so we simply divide those with the number of\nclones there would be if we created another one.\n\n6. The mechanisms in init_caller_stats and gather_caller_stats and\nget_info_about_necessary_edges was enhanced to gather data required\nfor the above and a missing check not to count dead incoming edges was\nalso added.\n\ngcc/ChangeLog:\n\n2021-10-15  Martin Jambor  <mjambor@suse.cz>\n\n\t* ipa-cp.c (struct caller_statistics): New fields rec_count_sum,\n\tn_nonrec_calls and itself, document all fields.\n\t(init_caller_stats): Initialize the above new fields.\n\t(gather_caller_stats): Gather self-recursive counts and calls number.\n\t(get_info_about_necessary_edges): Gather counts of self-recursive and\n\tother edges bringing in the requested value separately.\n\t(dump_profile_updates): Rework to dump info about a single node only.\n\t(lenient_count_portion_handling): New function.\n\t(struct gather_other_count_struct): New type.\n\t(gather_count_of_non_rec_edges): New function.\n\t(struct desc_incoming_count_struct): New type.\n\t(analyze_clone_icoming_counts): New function.\n\t(adjust_clone_incoming_counts): Likewise.\n\t(update_counts_for_self_gen_clones): Likewise.\n\t(update_profiling_info): Rewritten.\n\t(update_specialized_profile): Adjust call to dump_profile_updates.\n\t(create_specialized_node): Do not update profiling info.\n\t(decide_about_value): New parameter self_gen_clones, either push new\n\tclones into it or updat their profile counts.  For self-recursively\n\tgenerated values, use a portion of the node count instead of count\n\tfrom self-recursive edges to estimate goodness.\n\t(decide_whether_version_node): Gather clones for self-generated values\n\tin a new vector, update their profiles at once at the end.", "tree": {"sha": "b29359c07351b1beba8dab42f6566b1da7de89e2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/b29359c07351b1beba8dab42f6566b1da7de89e2"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/d1e2e4f9ce4df50564f1244dcea9befc3066faa8", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIzBAABCgAdFiEE5elVTFt/d09Vsoczv2PBvD+kNUAFAmF5SvUACgkQv2PBvD+k\nNUAfJRAAjirLk1lt5FUf6o5b6UXQ3UOc1g1OWcgev6ao185EyVjoXS2N8DmsWt4c\nSk46Dpkmfp+rIlwicBrp2sV7v8Sfj5otaqZtpl1e2ijcZ5RtKuuXebvZX+o2lMn0\nXLLckMS1kIZvbRv5bplnC/5YQ8gX2PK1cRWGn0GO7oJKcbox3voVjk8jAOFE0X9v\n6GxXNwbMY6NWgUKWCUhZP2i4z8SvUk1kWQG+u3pocr6PQ1ayeg3xfGG6FKN5saUC\nWgdCCMwZz+b55RV0Xch1W7in8/hq0rAT77W0iDnCW3imA0oWt7Jb2jqto421OPrq\nx4UE3gi75NuAlOaQdSQnsOpVUT1kFlcsbyln+r8kTiP+s5b63J6dct6YTqzryBeO\nHwE44l93Se9w1VAIDuWuZeThvg7U80xrtx1NhwH+IKlHM3Jz+Y9NYGBx65BeQLZn\nvIB1eLIICBPC+vyXHdlPV4SzM08Degt08WCqhS9mY3RHFc7DLIYflpanhaoK93ZG\nZW/wGC3C/5tOnnL50rvsT13T7mFLWC554o7IN3ZL27ipeP4kQkifWk50tPJm/jba\nmUYam1QXlrqE6grFJUhB71QooXSq8WTyh8MOEJUeyB0UQINW9VBzIjY2LvVeVj6F\nY8ZRJQPoQdDhnSbxNHEMlI1z3Yq+MLu1EvZXy/iy+wiy13sXj8Y=\n=HqWt\n-----END PGP SIGNATURE-----", "payload": "tree b29359c07351b1beba8dab42f6566b1da7de89e2\nparent b528e226d19335796c355d202c8e8686506680cd\nauthor Martin Jambor <mjambor@suse.cz> 1635338941 +0200\ncommitter Martin Jambor <mjambor@suse.cz> 1635338996 +0200\n\nipa-cp: Fix updating of profile counts and self-gen value evaluation\n\nIPA-CP does not do a reasonable job when it is updating profile counts\nafter it has created clones of recursive functions.  This patch\naddresses that by:\n\n1. Only updating counts for special-context clones.  When a clone is\ncreated for all contexts, the original is going to be dead and the\ncgraph machinery has copied counts to the new node which is the right\nthing to do.  Therefore updating counts has been moved from\ncreate_specialized_node to decide_about_value and\ndecide_whether_version_node.\n\n2. The current profile updating code artificially increased the assumed\nold count when the sum of counts of incoming edges to both the\noriginal and new node were bigger than the count of the original\nnode.  This always happened when self-recursive edge from the clone\nwas also redirected to the clone because both the original edge and\nits clone had original high counts.  This clutch was removed and\nreplaced by the next point.\n\n3. When cloning also redirects a self-recursive clone to the clone\nitself, new logic has been added to divide the counts brought by such\nrecursive edges between the original node and the clone.  This is\nimpossible to do well without special knowledge about the function and\nwhich non-recursive entry calls are responsible for what portion of\nrecursion depth, so the approach taken is rather crude.\n\nFor local nodes, we detect the case when the original node is never\ncalled (in the training run at least) with another value and if so,\nsteal all its counts like if it was dead.  If that is not the case, we\ntry to divide the count brought by recursive edges (or rather not\nbrought by direct edges) proportionally to the counts brought by\nnon-recursive edges - but with artificial limits in place so that we\ndo not take too many or too few, because that was happening with\ndetrimental effect in mcf_r.\n\n4. When cloning creates extra clones for values brought by a formerly\nself-recursive edge with an arithmetic pass-through jump function on\nit, such as it does in exchange2_r, all such clones are processed at\nonce rather than one after another.  The counts of all such nodes are\ndistributed evenly (modulo even-formerly-non-recursive-edges) and the\nwhole situation is then fixed up so that the edge counts fit.  This is\nwhat new function update_counts_for_self_gen_clones does.\n\n5. When values brought by a formerly self-recursive edge with an\narithmetic pass-through jump function on it are evaluated by\nheuristics which assumes vast majority of node counts are result of\nrecursive calls and so we simply divide those with the number of\nclones there would be if we created another one.\n\n6. The mechanisms in init_caller_stats and gather_caller_stats and\nget_info_about_necessary_edges was enhanced to gather data required\nfor the above and a missing check not to count dead incoming edges was\nalso added.\n\ngcc/ChangeLog:\n\n2021-10-15  Martin Jambor  <mjambor@suse.cz>\n\n\t* ipa-cp.c (struct caller_statistics): New fields rec_count_sum,\n\tn_nonrec_calls and itself, document all fields.\n\t(init_caller_stats): Initialize the above new fields.\n\t(gather_caller_stats): Gather self-recursive counts and calls number.\n\t(get_info_about_necessary_edges): Gather counts of self-recursive and\n\tother edges bringing in the requested value separately.\n\t(dump_profile_updates): Rework to dump info about a single node only.\n\t(lenient_count_portion_handling): New function.\n\t(struct gather_other_count_struct): New type.\n\t(gather_count_of_non_rec_edges): New function.\n\t(struct desc_incoming_count_struct): New type.\n\t(analyze_clone_icoming_counts): New function.\n\t(adjust_clone_incoming_counts): Likewise.\n\t(update_counts_for_self_gen_clones): Likewise.\n\t(update_profiling_info): Rewritten.\n\t(update_specialized_profile): Adjust call to dump_profile_updates.\n\t(create_specialized_node): Do not update profiling info.\n\t(decide_about_value): New parameter self_gen_clones, either push new\n\tclones into it or updat their profile counts.  For self-recursively\n\tgenerated values, use a portion of the node count instead of count\n\tfrom self-recursive edges to estimate goodness.\n\t(decide_whether_version_node): Gather clones for self-generated values\n\tin a new vector, update their profiles at once at the end.\n"}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d1e2e4f9ce4df50564f1244dcea9befc3066faa8", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d1e2e4f9ce4df50564f1244dcea9befc3066faa8", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d1e2e4f9ce4df50564f1244dcea9befc3066faa8/comments", "author": {"login": "jamborm", "id": 2180070, "node_id": "MDQ6VXNlcjIxODAwNzA=", "avatar_url": "https://avatars.githubusercontent.com/u/2180070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamborm", "html_url": "https://github.com/jamborm", "followers_url": "https://api.github.com/users/jamborm/followers", "following_url": "https://api.github.com/users/jamborm/following{/other_user}", "gists_url": "https://api.github.com/users/jamborm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamborm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamborm/subscriptions", "organizations_url": "https://api.github.com/users/jamborm/orgs", "repos_url": "https://api.github.com/users/jamborm/repos", "events_url": "https://api.github.com/users/jamborm/events{/privacy}", "received_events_url": "https://api.github.com/users/jamborm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jamborm", "id": 2180070, "node_id": "MDQ6VXNlcjIxODAwNzA=", "avatar_url": "https://avatars.githubusercontent.com/u/2180070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamborm", "html_url": "https://github.com/jamborm", "followers_url": "https://api.github.com/users/jamborm/followers", "following_url": "https://api.github.com/users/jamborm/following{/other_user}", "gists_url": "https://api.github.com/users/jamborm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamborm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamborm/subscriptions", "organizations_url": "https://api.github.com/users/jamborm/orgs", "repos_url": "https://api.github.com/users/jamborm/repos", "events_url": "https://api.github.com/users/jamborm/events{/privacy}", "received_events_url": "https://api.github.com/users/jamborm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b528e226d19335796c355d202c8e8686506680cd", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b528e226d19335796c355d202c8e8686506680cd", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b528e226d19335796c355d202c8e8686506680cd"}], "stats": {"total": 520, "additions": 441, "deletions": 79}, "files": [{"sha": "b254b9b9de65313ed6d89833c371dd29466a48dd", "filename": "gcc/ipa-cp.c", "status": "modified", "additions": 441, "deletions": 79, "changes": 520, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d1e2e4f9ce4df50564f1244dcea9befc3066faa8/gcc%2Fipa-cp.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d1e2e4f9ce4df50564f1244dcea9befc3066faa8/gcc%2Fipa-cp.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fipa-cp.c?ref=d1e2e4f9ce4df50564f1244dcea9befc3066faa8", "patch": "@@ -701,20 +701,36 @@ ipcp_versionable_function_p (struct cgraph_node *node)\n \n struct caller_statistics\n {\n+  /* If requested (see below), self-recursive call counts are summed into this\n+     field.  */\n+  profile_count rec_count_sum;\n+  /* The sum of all ipa counts of all the other (non-recursive) calls.  */\n   profile_count count_sum;\n+  /* Sum of all frequencies for all calls.  */\n   sreal freq_sum;\n+  /* Number of calls and hot calls respectively.  */\n   int n_calls, n_hot_calls;\n+  /* If itself is set up, also count the number of non-self-recursive\n+     calls.  */\n+  int n_nonrec_calls;\n+  /* If non-NULL, this is the node itself and calls from it should have their\n+     counts included in rec_count_sum and not count_sum.  */\n+  cgraph_node *itself;\n };\n \n-/* Initialize fields of STAT to zeroes.  */\n+/* Initialize fields of STAT to zeroes and optionally set it up so that edges\n+   from IGNORED_CALLER are not counted.  */\n \n static inline void\n-init_caller_stats (struct caller_statistics *stats)\n+init_caller_stats (caller_statistics *stats, cgraph_node *itself = NULL)\n {\n+  stats->rec_count_sum = profile_count::zero ();\n   stats->count_sum = profile_count::zero ();\n   stats->n_calls = 0;\n   stats->n_hot_calls = 0;\n+  stats->n_nonrec_calls = 0;\n   stats->freq_sum = 0;\n+  stats->itself = itself;\n }\n \n /* Worker callback of cgraph_for_node_and_aliases accumulating statistics of\n@@ -729,10 +745,22 @@ gather_caller_stats (struct cgraph_node *node, void *data)\n   for (cs = node->callers; cs; cs = cs->next_caller)\n     if (!cs->caller->thunk)\n       {\n-        if (cs->count.ipa ().initialized_p ())\n-\t  stats->count_sum += cs->count.ipa ();\n+\tipa_node_params *info = ipa_node_params_sum->get (cs->caller);\n+\tif (info && info->node_dead)\n+\t  continue;\n+\n+\tif (cs->count.ipa ().initialized_p ())\n+\t  {\n+\t    if (stats->itself && stats->itself == cs->caller)\n+\t      stats->rec_count_sum += cs->count.ipa ();\n+\t    else\n+\t      stats->count_sum += cs->count.ipa ();\n+\t  }\n \tstats->freq_sum += cs->sreal_frequency ();\n \tstats->n_calls++;\n+\tif (stats->itself && stats->itself != cs->caller)\n+\t  stats->n_nonrec_calls++;\n+\n \tif (cs->maybe_hot_p ())\n \t  stats->n_hot_calls ++;\n       }\n@@ -4202,19 +4230,22 @@ get_next_cgraph_edge_clone (struct cgraph_edge *cs)\n \n /* Given VAL that is intended for DEST, iterate over all its sources and if any\n    of them is viable and hot, return true.  In that case, for those that still\n-   hold, add their edge frequency and their number into *FREQUENCY and\n-   *CALLER_COUNT respectively.  */\n+   hold, add their edge frequency and their number and cumulative profile\n+   counts of self-ecursive and other edges into *FREQUENCY, *CALLER_COUNT,\n+   REC_COUNT_SUM and NONREC_COUNT_SUM respectively.  */\n \n template <typename valtype>\n static bool\n get_info_about_necessary_edges (ipcp_value<valtype> *val, cgraph_node *dest,\n-\t\t\t\tsreal *freq_sum, profile_count *count_sum,\n-\t\t\t\tint *caller_count)\n+\t\t\t\tsreal *freq_sum, int *caller_count,\n+\t\t\t\tprofile_count *rec_count_sum,\n+\t\t\t\tprofile_count *nonrec_count_sum)\n {\n   ipcp_value_source<valtype> *src;\n   sreal freq = 0;\n   int count = 0;\n-  profile_count cnt = profile_count::zero ();\n+  profile_count rec_cnt = profile_count::zero ();\n+  profile_count nonrec_cnt = profile_count::zero ();\n   bool hot = false;\n   bool non_self_recursive = false;\n \n@@ -4227,11 +4258,15 @@ get_info_about_necessary_edges (ipcp_value<valtype> *val, cgraph_node *dest,\n \t    {\n \t      count++;\n \t      freq += cs->sreal_frequency ();\n-\t      if (cs->count.ipa ().initialized_p ())\n-\t        cnt += cs->count.ipa ();\n \t      hot |= cs->maybe_hot_p ();\n \t      if (cs->caller != dest)\n-\t\tnon_self_recursive = true;\n+\t\t{\n+\t\t  non_self_recursive = true;\n+\t\t  if (cs->count.ipa ().initialized_p ())\n+\t\t    rec_cnt += cs->count.ipa ();\n+\t\t}\n+\t      else if (cs->count.ipa ().initialized_p ())\n+\t        nonrec_cnt += cs->count.ipa ();\n \t    }\n \t  cs = get_next_cgraph_edge_clone (cs);\n \t}\n@@ -4243,8 +4278,9 @@ get_info_about_necessary_edges (ipcp_value<valtype> *val, cgraph_node *dest,\n     return false;\n \n   *freq_sum = freq;\n-  *count_sum = cnt;\n   *caller_count = count;\n+  *rec_count_sum = rec_cnt;\n+  *nonrec_count_sum = nonrec_cnt;\n \n   if (!hot && ipa_node_params_sum->get (dest)->node_within_scc)\n     {\n@@ -4349,112 +4385,399 @@ get_replacement_map (class ipa_node_params *info, tree value, int parm_num,\n   return replace_map;\n }\n \n-/* Dump new profiling counts */\n+/* Dump new profiling counts of NODE.  SPEC is true when NODE is a specialzied\n+   one, otherwise it will be referred to as the original node.  */\n \n static void\n-dump_profile_updates (struct cgraph_node *orig_node,\n-\t\t      struct cgraph_node *new_node)\n+dump_profile_updates (cgraph_node *node, bool spec)\n {\n-  struct cgraph_edge *cs;\n+  if (spec)\n+    fprintf (dump_file, \"     setting count of the specialized node %s to \",\n+\t     node->dump_name ());\n+  else\n+    fprintf (dump_file, \"     setting count of the original node %s to \",\n+\t     node->dump_name ());\n \n-  fprintf (dump_file, \"    setting count of the specialized node to \");\n-  new_node->count.dump (dump_file);\n+  node->count.dump (dump_file);\n   fprintf (dump_file, \"\\n\");\n-  for (cs = new_node->callees; cs; cs = cs->next_callee)\n+  for (cgraph_edge *cs = node->callees; cs; cs = cs->next_callee)\n     {\n-      fprintf (dump_file, \"      edge to %s has count \",\n+      fprintf (dump_file, \"       edge to %s has count \",\n \t       cs->callee->dump_name ());\n       cs->count.dump (dump_file);\n       fprintf (dump_file, \"\\n\");\n     }\n+}\n \n-  fprintf (dump_file, \"    setting count of the original node to \");\n-  orig_node->count.dump (dump_file);\n-  fprintf (dump_file, \"\\n\");\n-  for (cs = orig_node->callees; cs; cs = cs->next_callee)\n+/* With partial train run we do not want to assume that original's count is\n+   zero whenever we redurect all executed edges to clone.  Simply drop profile\n+   to local one in this case.  In eany case, return the new value.  ORIG_NODE\n+   is the original node and its count has not been updaed yet.  */\n+\n+profile_count\n+lenient_count_portion_handling (profile_count remainder, cgraph_node *orig_node)\n+{\n+  if (remainder.ipa_p () && !remainder.ipa ().nonzero_p ()\n+      && orig_node->count.ipa_p () && orig_node->count.ipa ().nonzero_p ()\n+      && opt_for_fn (orig_node->decl, flag_profile_partial_training))\n+    remainder = remainder.guessed_local ();\n+\n+  return remainder;\n+}\n+\n+/* Structure to sum counts coming from nodes other than the original node and\n+   its clones.  */\n+\n+struct gather_other_count_struct\n+{\n+  cgraph_node *orig;\n+  profile_count other_count;\n+};\n+\n+/* Worker callback of call_for_symbol_thunks_and_aliases summing the number of\n+   counts that come from non-self-recursive calls..  */\n+\n+static bool\n+gather_count_of_non_rec_edges (cgraph_node *node, void *data)\n+{\n+  gather_other_count_struct *desc = (gather_other_count_struct *) data;\n+  for (cgraph_edge *cs = node->callers; cs; cs = cs->next_caller)\n+    if (cs->caller != desc->orig && cs->caller->clone_of != desc->orig)\n+      desc->other_count += cs->count.ipa ();\n+  return false;\n+}\n+\n+/* Structure to help analyze if we need to boost counts of some clones of some\n+   non-recursive edges to match the new callee count.  */\n+\n+struct desc_incoming_count_struct\n+{\n+  cgraph_node *orig;\n+  hash_set <cgraph_edge *> *processed_edges;\n+  profile_count count;\n+  unsigned unproc_orig_rec_edges;\n+};\n+\n+/* Go over edges calling NODE and its thunks and gather information about\n+   incoming counts so that we know if we need to make any adjustments.  */\n+\n+static void\n+analyze_clone_icoming_counts (cgraph_node *node,\n+\t\t\t      desc_incoming_count_struct *desc)\n+{\n+  for (cgraph_edge *cs = node->callers; cs; cs = cs->next_caller)\n+    if (cs->caller->thunk)\n+      {\n+\tanalyze_clone_icoming_counts (cs->caller, desc);\n+\tcontinue;\n+      }\n+    else\n+      {\n+\tif (cs->count.initialized_p ())\n+\t  desc->count += cs->count.ipa ();\n+\tif (!desc->processed_edges->contains (cs)\n+\t    && cs->caller->clone_of == desc->orig)\n+\t  desc->unproc_orig_rec_edges++;\n+      }\n+}\n+\n+/* If caller edge counts of a clone created for a self-recursive arithmetic\n+   jump function must be adjusted because it is coming from a the \"seed\" clone\n+   for the first value and so has been excessively scaled back as if it was not\n+   a recursive call, adjust it so that the incoming counts of NODE match its\n+   count. NODE is the node or its thunk.  */\n+\n+static void\n+adjust_clone_incoming_counts (cgraph_node *node,\n+\t\t\t      desc_incoming_count_struct *desc)\n+{\n+  for (cgraph_edge *cs = node->callers; cs; cs = cs->next_caller)\n+    if (cs->caller->thunk)\n+      {\n+\tadjust_clone_incoming_counts (cs->caller, desc);\n+\tprofile_count sum = profile_count::zero ();\n+\tfor (cgraph_edge *e = cs->caller->callers; e; e = e->next_caller)\n+\t  if (e->count.initialized_p ())\n+\t    sum += e->count.ipa ();\n+\tcs->count = cs->count.combine_with_ipa_count (sum);\n+      }\n+    else if (!desc->processed_edges->contains (cs)\n+\t     && cs->caller->clone_of == desc->orig)\n+      {\n+\tcs->count += desc->count;\n+\tif (dump_file)\n+\t  {\n+\t    fprintf (dump_file, \"       Adjusted count of an incoming edge of \"\n+\t\t     \"a clone %s -> %s to \", cs->caller->dump_name (),\n+\t\t     cs->callee->dump_name ());\n+\t    cs->count.dump (dump_file);\n+\t    fprintf (dump_file, \"\\n\");\n+\t  }\n+      }\n+}\n+\n+/* When ORIG_NODE has been cloned for values which have been generated fora\n+   self-recursive call as a result of an arithmetic pass-through\n+   jump-functions, adjust its count together with counts of all such clones in\n+   SELF_GEN_CLONES which also at this point contains ORIG_NODE itself.\n+\n+   The function sums the counts of the original node and all its clones that\n+   cannot be attributed to a specific clone because it comes from a\n+   non-recursive edge.  This sum is then evenly divided between the clones and\n+   on top of that each one gets all the counts which can be attributed directly\n+   to it.  */\n+\n+static void\n+update_counts_for_self_gen_clones (cgraph_node *orig_node,\n+\t\t\t\t   const vec<cgraph_node *> &self_gen_clones)\n+{\n+  profile_count redist_sum = orig_node->count.ipa ();\n+  if (!(redist_sum > profile_count::zero ()))\n+    return;\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"     Updating profile of self recursive clone \"\n+\t     \"series\\n\");\n+\n+  gather_other_count_struct gocs;\n+  gocs.orig = orig_node;\n+  gocs.other_count = profile_count::zero ();\n+\n+  auto_vec <profile_count, 8> other_edges_count;\n+  for (cgraph_node *n : self_gen_clones)\n+    {\n+      gocs.other_count = profile_count::zero ();\n+      n->call_for_symbol_thunks_and_aliases (gather_count_of_non_rec_edges,\n+\t\t\t\t\t     &gocs, false);\n+      other_edges_count.safe_push (gocs.other_count);\n+      redist_sum -= gocs.other_count;\n+    }\n+\n+  hash_set<cgraph_edge *> processed_edges;\n+  unsigned i = 0;\n+  for (cgraph_node *n : self_gen_clones)\n+    {\n+      profile_count orig_count = n->count;\n+      profile_count new_count\n+\t= (redist_sum.apply_scale (1, self_gen_clones.length ())\n+\t   + other_edges_count[i]);\n+      new_count = lenient_count_portion_handling (new_count, orig_node);\n+      n->count = new_count;\n+      profile_count::adjust_for_ipa_scaling (&new_count, &orig_count);\n+      for (cgraph_edge *cs = n->callees; cs; cs = cs->next_callee)\n+\t{\n+\t  cs->count = cs->count.apply_scale (new_count, orig_count);\n+\t  processed_edges.add (cs);\n+\t}\n+      for (cgraph_edge *cs = n->indirect_calls; cs; cs = cs->next_callee)\n+\tcs->count = cs->count.apply_scale (new_count, orig_count);\n+\n+      i++;\n+    }\n+\n+  /* There are still going to be edges to ORIG_NODE that have one or more\n+     clones coming from another node clone in SELF_GEN_CLONES and which we\n+     scaled by the same amount, which means that the total incoming sum of\n+     counts to ORIG_NODE will be too high, scale such edges back.  */\n+  for (cgraph_edge *cs = orig_node->callees; cs; cs = cs->next_callee)\n     {\n-      fprintf (dump_file, \"      edge to %s is left with \",\n-\t       cs->callee->dump_name ());\n-      cs->count.dump (dump_file);\n-      fprintf (dump_file, \"\\n\");\n+      if (cs->callee->ultimate_alias_target () == orig_node)\n+\t{\n+\t  unsigned den = 0;\n+\t  for (cgraph_edge *e = cs; e; e = get_next_cgraph_edge_clone (e))\n+\t    if (e->callee->ultimate_alias_target () == orig_node\n+\t\t&& processed_edges.contains (e))\n+\t      den++;\n+\t  if (den > 0)\n+\t    for (cgraph_edge *e = cs; e; e = get_next_cgraph_edge_clone (e))\n+\t      if (e->callee->ultimate_alias_target () == orig_node\n+\t\t  && processed_edges.contains (e))\n+\t\te->count = e->count.apply_scale (1, den);\n+\t}\n+    }\n+\n+  /* Edges from the seeds of the valus generated for arithmetic jump-functions\n+     along self-recursive edges are likely to have fairly low count and so\n+     edges from them to nodes in the self_gen_clones do not correspond to the\n+     artificially distributed count of the nodes, the total sum of incoming\n+     edges to some clones might be too low.  Detect this situation and correct\n+     it.  */\n+  for (cgraph_node *n : self_gen_clones)\n+    {\n+      if (!(n->count.ipa () > profile_count::zero ()))\n+\tcontinue;\n+\n+      desc_incoming_count_struct desc;\n+      desc.orig = orig_node;\n+      desc.processed_edges = &processed_edges;\n+      desc.count = profile_count::zero ();\n+      desc.unproc_orig_rec_edges = 0;\n+      analyze_clone_icoming_counts (n, &desc);\n+\n+      if (n->count.differs_from_p (desc.count))\n+\t{\n+\t  if (n->count > desc.count\n+\t      && desc.unproc_orig_rec_edges > 0)\n+\t    {\n+\t      desc.count = n->count - desc.count;\n+\t      desc.count\n+\t\t= desc.count.apply_scale (1, desc.unproc_orig_rec_edges);\n+\t      adjust_clone_incoming_counts (n, &desc);\n+\t    }\n+\t  else if (dump_file)\n+\t    fprintf (dump_file,\n+\t\t     \"       Unable to fix up incoming counts for %s.\\n\",\n+\t\t     n->dump_name ());\n+\t}\n     }\n+\n+  if (dump_file)\n+    for (cgraph_node *n : self_gen_clones)\n+      dump_profile_updates (n, n != orig_node);\n+  return;\n }\n \n /* After a specialized NEW_NODE version of ORIG_NODE has been created, update\n-   their profile information to reflect this.  */\n+   their profile information to reflect this.  This function should not be used\n+   for clones generated for arithmetic pass-through jump functions on a\n+   self-recursive call graph edge, that situation is handled by\n+   update_counts_for_self_gen_clones.  */\n \n static void\n update_profiling_info (struct cgraph_node *orig_node,\n \t\t       struct cgraph_node *new_node)\n {\n-  struct cgraph_edge *cs;\n   struct caller_statistics stats;\n-  profile_count new_sum, orig_sum;\n-  profile_count remainder, orig_node_count = orig_node->count;\n-  profile_count orig_new_node_count = new_node->count;\n+  profile_count new_sum;\n+  profile_count remainder, orig_node_count = orig_node->count.ipa ();\n \n-  if (!(orig_node_count.ipa () > profile_count::zero ()))\n+  if (!(orig_node_count > profile_count::zero ()))\n     return;\n \n-  init_caller_stats (&stats);\n-  orig_node->call_for_symbol_thunks_and_aliases (gather_caller_stats, &stats,\n-\t\t\t\t\t       false);\n-  orig_sum = stats.count_sum;\n-  init_caller_stats (&stats);\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"     Updating profile from original count: \");\n+      orig_node_count.dump (dump_file);\n+      fprintf (dump_file, \"\\n\");\n+    }\n+\n+  init_caller_stats (&stats, new_node);\n   new_node->call_for_symbol_thunks_and_aliases (gather_caller_stats, &stats,\n \t\t\t\t\t      false);\n   new_sum = stats.count_sum;\n \n-  if (orig_node_count < orig_sum + new_sum)\n+  if (new_sum > orig_node_count)\n     {\n-      if (dump_file)\n+      /* TODO: Perhaps this should be gcc_unreachable ()?  */\n+      remainder = profile_count::zero ().guessed_local ();\n+    }\n+  else if (stats.rec_count_sum.nonzero_p ())\n+    {\n+      int new_nonrec_calls = stats.n_nonrec_calls;\n+      /* There are self-recursive edges which are likely to bring in the\n+\t majority of calls but which we must divide in between the original and\n+\t new node.  */\n+      init_caller_stats (&stats, orig_node);\n+      orig_node->call_for_symbol_thunks_and_aliases (gather_caller_stats,\n+\t\t\t\t\t\t     &stats, false);\n+      int orig_nonrec_calls = stats.n_nonrec_calls;\n+      profile_count orig_nonrec_call_count = stats.count_sum;\n+\n+      if (orig_node->local)\n \t{\n-\t  fprintf (dump_file, \"    Problem: node %s has too low count \",\n-\t\t   orig_node->dump_name ());\n-\t  orig_node_count.dump (dump_file);\n-\t  fprintf (dump_file, \"while the sum of incoming count is \");\n-\t  (orig_sum + new_sum).dump (dump_file);\n-\t  fprintf (dump_file, \"\\n\");\n+\t  if (!orig_nonrec_call_count.nonzero_p ())\n+\t    {\n+\t      if (dump_file)\n+\t\tfprintf (dump_file, \"       The original is local and the only \"\n+\t\t\t \"incoming edges from non-dead callers with nonzero \"\n+\t\t\t \"counts are self-recursive, assuming it is cold.\\n\");\n+\t      /* The NEW_NODE count and counts of all its outgoing edges\n+\t\t are still unmodified copies of ORIG_NODE's.  Just clear\n+\t\t the latter and bail out.  */\n+\t      profile_count zero;\n+              if (opt_for_fn (orig_node->decl, flag_profile_partial_training))\n+                zero = profile_count::zero ().guessed_local ();\n+\t      else\n+\t\tzero = profile_count::adjusted_zero ();\n+\t      orig_node->count = zero;\n+\t      for (cgraph_edge *cs = orig_node->callees;\n+\t\t   cs;\n+\t\t   cs = cs->next_callee)\n+\t\tcs->count = zero;\n+\t      for (cgraph_edge *cs = orig_node->indirect_calls;\n+\t\t   cs;\n+\t\t   cs = cs->next_callee)\n+\t\tcs->count = zero;\n+\t      return;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  /* Let's behave as if there was another caller that accounts for all\n+\t     the calls that were either indirect or from other compilation\n+\t     units. */\n+\t  orig_nonrec_calls++;\n+\t  profile_count pretend_caller_count\n+\t    = (orig_node_count - new_sum - orig_nonrec_call_count\n+\t       - stats.rec_count_sum);\n+\t  orig_nonrec_call_count += pretend_caller_count;\n \t}\n \n-      orig_node_count = (orig_sum + new_sum).apply_scale (12, 10);\n+      /* Divide all \"unexplained\" counts roughly proportionally to sums of\n+\t counts of non-recursive calls.\n+\n+\t We put rather arbitrary limits on how many counts we claim because the\n+\t number of non-self-recursive incoming count is only a rough guideline\n+\t and there are cases (such as mcf) where using it blindly just takes\n+\t too many.  And if lattices are considered in the opposite order we\n+\t could also take too few.  */\n+      profile_count unexp = orig_node_count - new_sum - orig_nonrec_call_count;\n+\n+      int limit_den = 2 * (orig_nonrec_calls + new_nonrec_calls);\n+      profile_count new_part\n+\t= MAX(MIN (unexp.apply_scale (new_sum,\n+\t\t\t\t      new_sum + orig_nonrec_call_count),\n+\t\t   unexp.apply_scale (limit_den - 1, limit_den)),\n+\t      unexp.apply_scale (new_nonrec_calls, limit_den));\n       if (dump_file)\n \t{\n-\t  fprintf (dump_file, \"      proceeding by pretending it was \");\n-\t  orig_node_count.dump (dump_file);\n-\t  fprintf (dump_file, \"\\n\");\n+\t  fprintf (dump_file, \"       Claiming \");\n+\t  new_part.dump (dump_file);\n+\t  fprintf (dump_file, \" of unexplained \");\n+\t  unexp.dump (dump_file);\n+\t  fprintf (dump_file, \" counts because of self-recursive \"\n+\t\t   \"calls\\n\");\n \t}\n+      new_sum += new_part;\n+      remainder = lenient_count_portion_handling (orig_node_count - new_sum,\n+\t\t\t\t\t\t  orig_node);\n     }\n-\n-  remainder = orig_node_count.combine_with_ipa_count (orig_node_count.ipa ()\n-\t\t\t\t\t\t      - new_sum.ipa ());\n-\n-  /* With partial train run we do not want to assume that original's\n-     count is zero whenever we redurect all executed edges to clone.\n-     Simply drop profile to local one in this case.  */\n-  if (remainder.ipa_p () && !remainder.ipa ().nonzero_p ()\n-      && orig_node->count.ipa_p () && orig_node->count.ipa ().nonzero_p ()\n-      && flag_profile_partial_training)\n-    remainder = remainder.guessed_local ();\n+  else\n+    remainder = lenient_count_portion_handling (orig_node_count - new_sum,\n+\t\t\t\t\t\torig_node);\n \n   new_sum = orig_node_count.combine_with_ipa_count (new_sum);\n   new_node->count = new_sum;\n   orig_node->count = remainder;\n \n+  profile_count orig_new_node_count = orig_node_count;\n   profile_count::adjust_for_ipa_scaling (&new_sum, &orig_new_node_count);\n-  for (cs = new_node->callees; cs; cs = cs->next_callee)\n+  for (cgraph_edge *cs = new_node->callees; cs; cs = cs->next_callee)\n     cs->count = cs->count.apply_scale (new_sum, orig_new_node_count);\n-  for (cs = new_node->indirect_calls; cs; cs = cs->next_callee)\n+  for (cgraph_edge *cs = new_node->indirect_calls; cs; cs = cs->next_callee)\n     cs->count = cs->count.apply_scale (new_sum, orig_new_node_count);\n \n   profile_count::adjust_for_ipa_scaling (&remainder, &orig_node_count);\n-  for (cs = orig_node->callees; cs; cs = cs->next_callee)\n+  for (cgraph_edge *cs = orig_node->callees; cs; cs = cs->next_callee)\n     cs->count = cs->count.apply_scale (remainder, orig_node_count);\n-  for (cs = orig_node->indirect_calls; cs; cs = cs->next_callee)\n+  for (cgraph_edge *cs = orig_node->indirect_calls; cs; cs = cs->next_callee)\n     cs->count = cs->count.apply_scale (remainder, orig_node_count);\n \n   if (dump_file)\n-    dump_profile_updates (orig_node, new_node);\n+    {\n+      dump_profile_updates (new_node, true);\n+      dump_profile_updates (orig_node, false);\n+    }\n }\n \n /* Update the respective profile of specialized NEW_NODE and the original\n@@ -4495,7 +4818,10 @@ update_specialized_profile (struct cgraph_node *new_node,\n     }\n \n   if (dump_file)\n-    dump_profile_updates (orig_node, new_node);\n+    {\n+      dump_profile_updates (new_node, true);\n+      dump_profile_updates (orig_node, false);\n+    }\n }\n \n static void adjust_references_in_caller (cgraph_edge *cs,\n@@ -4795,8 +5121,7 @@ create_specialized_node (struct cgraph_node *node,\n       if (aggvals)\n \tipa_dump_agg_replacement_values (dump_file, aggvals);\n     }\n-  ipa_check_create_node_params ();\n-  update_profiling_info (node, new_node);\n+\n   new_info = ipa_node_params_sum->get (new_node);\n   new_info->ipcp_orig_node = node;\n   new_node->ipcp_clone = true;\n@@ -5621,17 +5946,20 @@ ipcp_val_agg_replacement_ok_p (ipa_agg_replacement_value *,\n /* Decide whether to create a special version of NODE for value VAL of\n    parameter at the given INDEX.  If OFFSET is -1, the value is for the\n    parameter itself, otherwise it is stored at the given OFFSET of the\n-   parameter.  AVALS describes the other already known values.  */\n+   parameter.  AVALS describes the other already known values.  SELF_GEN_CLONES\n+   is a vector which contains clones created for self-recursive calls with an\n+   arithmetic pass-through jump function.  */\n \n template <typename valtype>\n static bool\n decide_about_value (struct cgraph_node *node, int index, HOST_WIDE_INT offset,\n-\t\t    ipcp_value<valtype> *val, ipa_auto_call_arg_values *avals)\n+\t\t    ipcp_value<valtype> *val, ipa_auto_call_arg_values *avals,\n+\t\t    vec<cgraph_node *> *self_gen_clones)\n {\n   struct ipa_agg_replacement_value *aggvals;\n   int caller_count;\n   sreal freq_sum;\n-  profile_count count_sum;\n+  profile_count count_sum, rec_count_sum;\n   vec<cgraph_edge *> callers;\n \n   if (val->spec_node)\n@@ -5647,13 +5975,31 @@ decide_about_value (struct cgraph_node *node, int index, HOST_WIDE_INT offset,\n \t\t val->local_size_cost + overall_size);\n       return false;\n     }\n-  else if (!get_info_about_necessary_edges (val, node, &freq_sum, &count_sum,\n-\t\t\t\t\t    &caller_count))\n+  else if (!get_info_about_necessary_edges (val, node, &freq_sum, &caller_count,\n+\t\t\t\t\t    &rec_count_sum, &count_sum))\n     return false;\n \n   if (!dbg_cnt (ipa_cp_values))\n     return false;\n \n+  if (val->self_recursion_generated_p ())\n+    {\n+      /* The edge counts in this case might not have been adjusted yet.\n+\t Nevertleless, even if they were it would be only a guesswork which we\n+\t can do now.  The recursive part of the counts can be derived from the\n+\t count of the original node anyway.  */\n+      if (node->count.ipa ().nonzero_p ())\n+\t{\n+\t  unsigned dem = self_gen_clones->length () + 1;\n+\t  rec_count_sum = node->count.ipa ().apply_scale (1, dem);\n+\t}\n+      else\n+\trec_count_sum = profile_count::zero ();\n+    }\n+\n+  /* get_info_about_necessary_edges only sums up ipa counts.  */\n+  count_sum += rec_count_sum;\n+\n   if (dump_file && (dump_flags & TDF_DETAILS))\n     {\n       fprintf (dump_file, \" - considering value \");\n@@ -5694,6 +6040,12 @@ decide_about_value (struct cgraph_node *node, int index, HOST_WIDE_INT offset,\n \t\t\t\t\t\t      offset, val->value));\n   val->spec_node = create_specialized_node (node, known_csts, known_contexts,\n \t\t\t\t\t    aggvals, callers);\n+\n+  if (val->self_recursion_generated_p ())\n+    self_gen_clones->safe_push (val->spec_node);\n+  else\n+    update_profiling_info (node, val->spec_node);\n+\n   callers.release ();\n   overall_size += val->local_size_cost;\n   if (dump_file && (dump_flags & TDF_DETAILS))\n@@ -5722,6 +6074,7 @@ decide_whether_version_node (struct cgraph_node *node)\n     fprintf (dump_file, \"\\nEvaluating opportunities for %s.\\n\",\n \t     node->dump_name ());\n \n+  auto_vec <cgraph_node *, 9> self_gen_clones;\n   ipa_auto_call_arg_values avals;\n   gather_context_independent_values (info, &avals, false, NULL);\n \n@@ -5736,7 +6089,8 @@ decide_whether_version_node (struct cgraph_node *node)\n \t{\n \t  ipcp_value<tree> *val;\n \t  for (val = lat->values; val; val = val->next)\n-\t    ret |= decide_about_value (node, i, -1, val, &avals);\n+\t    ret |= decide_about_value (node, i, -1, val, &avals,\n+\t\t\t\t       &self_gen_clones);\n \t}\n \n       if (!plats->aggs_bottom)\n@@ -5750,18 +6104,26 @@ decide_whether_version_node (struct cgraph_node *node)\n \t\t&& (plats->aggs_contain_variable\n \t\t    || !aglat->is_single_const ()))\n \t      for (val = aglat->values; val; val = val->next)\n-\t\tret |= decide_about_value (node, i, aglat->offset, val, &avals);\n+\t\tret |= decide_about_value (node, i, aglat->offset, val, &avals,\n+\t\t\t\t\t   &self_gen_clones);\n \t}\n \n       if (!ctxlat->bottom\n \t  && avals.m_known_contexts[i].useless_p ())\n \t{\n \t  ipcp_value<ipa_polymorphic_call_context> *val;\n \t  for (val = ctxlat->values; val; val = val->next)\n-\t    ret |= decide_about_value (node, i, -1, val, &avals);\n+\t    ret |= decide_about_value (node, i, -1, val, &avals,\n+\t\t\t\t       &self_gen_clones);\n \t}\n     }\n \n+  if (!self_gen_clones.is_empty ())\n+    {\n+      self_gen_clones.safe_push (node);\n+      update_counts_for_self_gen_clones (node, self_gen_clones);\n+    }\n+\n   if (info->do_clone_for_all_contexts)\n     {\n       if (!dbg_cnt (ipa_cp_values))"}]}