{"sha": "6aa5370cccf7c0475192dc8c641450722ae1e477", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NmFhNTM3MGNjY2Y3YzA0NzUxOTJkYzhjNjQxNDUwNzIyYWUxZTQ3Nw==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2019-12-19T13:25:22Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2019-12-19T13:25:22Z"}, "message": "[AArch64] Handle arguments and return types with partial SVE modes\n\nPartial SVE modes can be picked up and used by the vector_size(N)\nattribute.[*] This means that we need to cope with arguments and return\nvalues with partial SVE modes, which previously triggered asserts like:\n\n  /* Generic vectors that map to SVE modes with -msve-vector-bits=N are\n     passed by reference, not by value.  */\n  gcc_assert (!aarch64_sve_mode_p (mode));\n\nThe ABI for these types is fixed from pre-SVE days, and must in any case\nbe the same for all -msve-vector-bits=N values.  All we need to do is\nensure that the vectors are passed and returned in the traditional way.\n\n[*] Advanced SIMD always wins for 64-bit and 128-bit vectors though.\n\n2019-12-19  Richard Sandiford  <richard.sandiford@arm.com>\n\ngcc/\n\t* config/aarch64/aarch64.c (aarch64_function_value_1): New function,\n\tsplit out from...\n\t(aarch64_function_value): ...here.  Handle partial SVE modes by\n\tpretending that they have the associated/traditional integer mode,\n\tthen wrap the result in the real mode.\n\t(aarch64_layout_arg): Take an orig_mode argument and pass it to\n\taarch64_function_arg_alignment.  Handle partial SVE modes analogously\n\tto aarch64_function_value.\n\t(aarch64_function_arg): Update call accordingly.\n\t(aarch64_function_arg_advance): Likewise.\n\ngcc/testsuite/\n\t* gcc.target/aarch64/sve/pcs/gnu_vectors_3.c: New test.\n\nFrom-SVN: r279571", "tree": {"sha": "61d4a2e4e9b39e828075801fd0a7f05cb89e6464", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/61d4a2e4e9b39e828075801fd0a7f05cb89e6464"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6aa5370cccf7c0475192dc8c641450722ae1e477", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6aa5370cccf7c0475192dc8c641450722ae1e477", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6aa5370cccf7c0475192dc8c641450722ae1e477", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6aa5370cccf7c0475192dc8c641450722ae1e477/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "022d11a3b58900556b9d2c2336cc3cf1f996b182", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/022d11a3b58900556b9d2c2336cc3cf1f996b182", "html_url": "https://github.com/Rust-GCC/gccrs/commit/022d11a3b58900556b9d2c2336cc3cf1f996b182"}], "stats": {"total": 170, "additions": 150, "deletions": 20}, "files": [{"sha": "63a5ee430db91c66ca67abd30c5e97914d271d6f", "filename": "gcc/ChangeLog", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6aa5370cccf7c0475192dc8c641450722ae1e477/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6aa5370cccf7c0475192dc8c641450722ae1e477/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=6aa5370cccf7c0475192dc8c641450722ae1e477", "patch": "@@ -1,3 +1,16 @@\n+2019-12-19  Richard Sandiford  <richard.sandiford@arm.com>\n+\n+\t* config/aarch64/aarch64.c (aarch64_function_value_1): New function,\n+\tsplit out from...\n+\t(aarch64_function_value): ...here.  Handle partial SVE modes by\n+\tpretending that they have the associated/traditional integer mode,\n+\tthen wrap the result in the real mode.\n+\t(aarch64_layout_arg): Take an orig_mode argument and pass it to\n+\taarch64_function_arg_alignment.  Handle partial SVE modes analogously\n+\tto aarch64_function_value.\n+\t(aarch64_function_arg): Update call accordingly.\n+\t(aarch64_function_arg_advance): Likewise.\n+\n 2019-12-19  Jan Hubicka  <hubicka@ucw.cz>\n \t    Xi Ruoyao  <xry111@mengyan1223.wang>\n "}, {"sha": "cf6aa7e4c9f49a0796bf38cf3506231cecec84cb", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 75, "deletions": 20, "changes": 95, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6aa5370cccf7c0475192dc8c641450722ae1e477/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6aa5370cccf7c0475192dc8c641450722ae1e477/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=6aa5370cccf7c0475192dc8c641450722ae1e477", "patch": "@@ -4948,22 +4948,12 @@ aarch64_return_in_msb (const_tree valtype)\n   return true;\n }\n \n-/* Implement TARGET_FUNCTION_VALUE.\n-   Define how to find the value returned by a function.  */\n-\n+/* Subroutine of aarch64_function_value.  MODE is the mode of the argument\n+   after promotion, and after partial SVE types have been replaced by\n+   their integer equivalents.  */\n static rtx\n-aarch64_function_value (const_tree type, const_tree func,\n-\t\t\tbool outgoing ATTRIBUTE_UNUSED)\n+aarch64_function_value_1 (const_tree type, machine_mode mode)\n {\n-  machine_mode mode;\n-  int unsignedp;\n-  int count;\n-  machine_mode ag_mode;\n-\n-  mode = TYPE_MODE (type);\n-  if (INTEGRAL_TYPE_P (type))\n-    mode = promote_function_mode (type, mode, &unsignedp, func, 1);\n-\n   unsigned int num_zr, num_pr;\n   if (type && aarch64_sve_argument_p (type, &num_zr, &num_pr))\n     {\n@@ -4998,6 +4988,8 @@ aarch64_function_value (const_tree type, const_tree func,\n \t}\n     }\n \n+  int count;\n+  machine_mode ag_mode;\n   if (aarch64_vfp_is_call_or_return_candidate (mode, type,\n \t\t\t\t\t       &ag_mode, &count, NULL))\n     {\n@@ -5026,6 +5018,42 @@ aarch64_function_value (const_tree type, const_tree func,\n     return gen_rtx_REG (mode, R0_REGNUM);\n }\n \n+/* Implement TARGET_FUNCTION_VALUE.\n+   Define how to find the value returned by a function.  */\n+\n+static rtx\n+aarch64_function_value (const_tree type, const_tree func,\n+\t\t\tbool outgoing ATTRIBUTE_UNUSED)\n+{\n+  machine_mode mode;\n+  int unsignedp;\n+\n+  mode = TYPE_MODE (type);\n+  if (INTEGRAL_TYPE_P (type))\n+    mode = promote_function_mode (type, mode, &unsignedp, func, 1);\n+\n+  /* Vector types can acquire a partial SVE mode using things like\n+     __attribute__((vector_size(N))), and this is potentially useful.\n+     However, the choice of mode doesn't affect the type's ABI identity,\n+     so we should treat the types as though they had the associated\n+     integer mode, just like they did before SVE was introduced.\n+\n+     We know that the vector must be 128 bits or smaller, otherwise we'd\n+     have returned it in memory instead.  */\n+  unsigned int vec_flags = aarch64_classify_vector_mode (mode);\n+  if ((vec_flags & VEC_ANY_SVE) && (vec_flags & VEC_PARTIAL))\n+    {\n+      scalar_int_mode int_mode = int_mode_for_mode (mode).require ();\n+      rtx reg = aarch64_function_value_1 (type, int_mode);\n+      /* Vector types are never returned in the MSB and are never split.  */\n+      gcc_assert (REG_P (reg) && GET_MODE (reg) == int_mode);\n+      rtx pair = gen_rtx_EXPR_LIST (VOIDmode, reg, const0_rtx);\n+      return gen_rtx_PARALLEL (VOIDmode, gen_rtvec (1, pair));\n+    }\n+\n+  return aarch64_function_value_1 (type, mode);\n+}\n+\n /* Implements TARGET_FUNCTION_VALUE_REGNO_P.\n    Return true if REGNO is the number of a hard register in which the values\n    of called function may come back.  */\n@@ -5151,10 +5179,14 @@ aarch64_function_arg_alignment (machine_mode mode, const_tree type,\n }\n \n /* Layout a function argument according to the AAPCS64 rules.  The rule\n-   numbers refer to the rule numbers in the AAPCS64.  */\n+   numbers refer to the rule numbers in the AAPCS64.  ORIG_MODE is the\n+   mode that was originally given to us by the target hook, whereas the\n+   mode in ARG might be the result of replacing partial SVE modes with\n+   the equivalent integer mode.  */\n \n static void\n-aarch64_layout_arg (cumulative_args_t pcum_v, const function_arg_info &arg)\n+aarch64_layout_arg (cumulative_args_t pcum_v, const function_arg_info &arg,\n+\t\t    machine_mode orig_mode)\n {\n   CUMULATIVE_ARGS *pcum = get_cumulative_args (pcum_v);\n   tree type = arg.type;\n@@ -5168,6 +5200,29 @@ aarch64_layout_arg (cumulative_args_t pcum_v, const function_arg_info &arg)\n   if (pcum->aapcs_arg_processed)\n     return;\n \n+  /* Vector types can acquire a partial SVE mode using things like\n+     __attribute__((vector_size(N))), and this is potentially useful.\n+     However, the choice of mode doesn't affect the type's ABI identity,\n+     so we should treat the types as though they had the associated\n+     integer mode, just like they did before SVE was introduced.\n+\n+     We know that the vector must be 128 bits or smaller, otherwise we'd\n+     have passed it by reference instead.  */\n+  unsigned int vec_flags = aarch64_classify_vector_mode (mode);\n+  if ((vec_flags & VEC_ANY_SVE) && (vec_flags & VEC_PARTIAL))\n+    {\n+      function_arg_info tmp_arg = arg;\n+      tmp_arg.mode = int_mode_for_mode (mode).require ();\n+      aarch64_layout_arg (pcum_v, tmp_arg, orig_mode);\n+      if (rtx reg = pcum->aapcs_reg)\n+\t{\n+\t  gcc_assert (REG_P (reg) && GET_MODE (reg) == tmp_arg.mode);\n+\t  rtx pair = gen_rtx_EXPR_LIST (VOIDmode, reg, const0_rtx);\n+\t  pcum->aapcs_reg = gen_rtx_PARALLEL (mode, gen_rtvec (1, pair));\n+\t}\n+      return;\n+    }\n+\n   pcum->aapcs_arg_processed = true;\n \n   unsigned int num_zr, num_pr;\n@@ -5289,7 +5344,7 @@ aarch64_layout_arg (cumulative_args_t pcum_v, const function_arg_info &arg)\n \t     comparison is there because for > 16 * BITS_PER_UNIT\n \t     alignment nregs should be > 2 and therefore it should be\n \t     passed by reference rather than value.  */\n-\t  && (aarch64_function_arg_alignment (mode, type, &abi_break)\n+\t  && (aarch64_function_arg_alignment (orig_mode, type, &abi_break)\n \t      == 16 * BITS_PER_UNIT))\n \t{\n \t  if (abi_break && warn_psabi && currently_expanding_gimple_stmt)\n@@ -5332,7 +5387,7 @@ aarch64_layout_arg (cumulative_args_t pcum_v, const function_arg_info &arg)\n on_stack:\n   pcum->aapcs_stack_words = size / UNITS_PER_WORD;\n \n-  if (aarch64_function_arg_alignment (mode, type, &abi_break)\n+  if (aarch64_function_arg_alignment (orig_mode, type, &abi_break)\n       == 16 * BITS_PER_UNIT)\n     {\n       int new_size = ROUND_UP (pcum->aapcs_stack_size, 16 / UNITS_PER_WORD);\n@@ -5360,7 +5415,7 @@ aarch64_function_arg (cumulative_args_t pcum_v, const function_arg_info &arg)\n   if (arg.end_marker_p ())\n     return gen_int_mode (pcum->pcs_variant, DImode);\n \n-  aarch64_layout_arg (pcum_v, arg);\n+  aarch64_layout_arg (pcum_v, arg, arg.mode);\n   return pcum->aapcs_reg;\n }\n \n@@ -5425,7 +5480,7 @@ aarch64_function_arg_advance (cumulative_args_t pcum_v,\n       || pcum->pcs_variant == ARM_PCS_SIMD\n       || pcum->pcs_variant == ARM_PCS_SVE)\n     {\n-      aarch64_layout_arg (pcum_v, arg);\n+      aarch64_layout_arg (pcum_v, arg, arg.mode);\n       gcc_assert ((pcum->aapcs_reg != NULL_RTX)\n \t\t  != (pcum->aapcs_stack_words != 0));\n       pcum->aapcs_arg_processed = false;"}, {"sha": "3385971dba85fec0e98acdfe72b8e98be811ccfe", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6aa5370cccf7c0475192dc8c641450722ae1e477/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6aa5370cccf7c0475192dc8c641450722ae1e477/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=6aa5370cccf7c0475192dc8c641450722ae1e477", "patch": "@@ -1,3 +1,7 @@\n+2019-12-19  Richard Sandiford  <richard.sandiford@arm.com>\n+\n+\t* gcc.target/aarch64/sve/pcs/gnu_vectors_3.c: New test.\n+\n 2019-12-19  Feng Xue  <fxue@os.amperecomputing.com>\n \n \tPR ipa/92794"}, {"sha": "70c24725de4a6e754902955fda8bca36553225c5", "filename": "gcc/testsuite/gcc.target/aarch64/sve/pcs/gnu_vectors_3.c", "status": "added", "additions": 58, "deletions": 0, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6aa5370cccf7c0475192dc8c641450722ae1e477/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpcs%2Fgnu_vectors_3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6aa5370cccf7c0475192dc8c641450722ae1e477/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpcs%2Fgnu_vectors_3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpcs%2Fgnu_vectors_3.c?ref=6aa5370cccf7c0475192dc8c641450722ae1e477", "patch": "@@ -0,0 +1,58 @@\n+/* { dg-options \"-O -msve-vector-bits=256\" } */\n+\n+typedef unsigned char int8x4_t __attribute__((vector_size (4)));\n+\n+/*\n+** passthru_x0:\n+**\tret\n+*/\n+int8x4_t passthru_x0 (int8x4_t x0) { return x0; }\n+\n+/*\n+** passthru_x1:\n+**\tmov\tw0, w1\n+**\tret\n+*/\n+int8x4_t passthru_x1 (int8x4_t x0, int8x4_t x1) { return x1; }\n+\n+int8x4_t load (int8x4_t *x0) { return *x0; }\n+\n+void store (int8x4_t *x0, int8x4_t x1) { *x0 = x1; }\n+\n+/*\n+** stack_callee:\n+**\tptrue\tp[0-7], vl32\n+**\tld1b\t(z[0-9]+\\.d), \\1/z, \\[sp\\]\n+**\tst1b\t\\2, \\1, \\[x0\\]\n+**\tret\n+*/\n+__attribute__((noipa))\n+void stack_callee (int8x4_t *x0, int8x4_t x1, int8x4_t x2, int8x4_t x3,\n+\t\t   int8x4_t x4, int8x4_t x5, int8x4_t x6, int8x4_t x7,\n+\t\t   int8x4_t stack0)\n+{\n+  *x0 = stack0;\n+}\n+\n+/*\n+** stack_callee:\n+**\t\\.\\.\\.\n+**\tptrue\tp[0-7], vl32\n+**\t\\.\\.\\.\n+**\tld1b\t(z[0-9]+\\.d), \\1/z, \\[x0\\]\n+**\t\\.\\.\\.\n+**\tst1b\t\\2, \\1, \\[sp\\]\n+**\t\\.\\.\\.\n+**\tret\n+*/\n+void stack_caller (int8x4_t *x0, int8x4_t x1)\n+{\n+  stack_callee (x0, x1, x1, x1, x1, x1, x1, x1, *x0);\n+}\n+\n+/* { dg-final { scan-assembler {\\tmov\\tw2, w} } } */\n+/* { dg-final { scan-assembler {\\tmov\\tw3, w} } } */\n+/* { dg-final { scan-assembler {\\tmov\\tw4, w} } } */\n+/* { dg-final { scan-assembler {\\tmov\\tw5, w} } } */\n+/* { dg-final { scan-assembler {\\tmov\\tw6, w} } } */\n+/* { dg-final { scan-assembler {\\tmov\\tw7, w} } } */"}]}