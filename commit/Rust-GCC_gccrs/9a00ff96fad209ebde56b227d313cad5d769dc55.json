{"sha": "9a00ff96fad209ebde56b227d313cad5d769dc55", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OWEwMGZmOTZmYWQyMDllYmRlNTZiMjI3ZDMxM2NhZDVkNzY5ZGM1NQ==", "commit": {"author": {"name": "Jonathan Wright", "email": "jonathan.wright@arm.com", "date": "2021-02-02T01:31:32Z"}, "committer": {"name": "Jonathan Wright", "email": "jonathan.wright@arm.com", "date": "2021-02-03T14:01:15Z"}, "message": "aarch64: Use RTL builtins for [su]mlal_high_lane[q] intrinsics\n\nRewrite [su]mlal_high_lane[q] Neon intrinsics to use RTL builtins\nrather than inline assembly code, allowing for better scheduling and\noptimization.\n\ngcc/ChangeLog:\n\n2021-02-02  Jonathan Wright  <jonathan.wright@arm.com>\n\n\t* config/aarch64/aarch64-simd-builtins.def: Add\n\t[su]mlal_hi_lane[q] builtin generator macros.\n\t* config/aarch64/aarch64-simd.md\n\t(aarch64_<su>mlal_hi_lane<mode>_insn): Define.\n\t(aarch64_<su>mlal_hi_lane<mode>): Define.\n\t(aarch64_<su>mlal_hi_laneq<mode>_insn): Define.\n\t(aarch64_<su>mlal_hi_laneq<mode>): Define.\n\t* config/aarch64/arm_neon.h (vmlal_high_lane_s16): Use RTL\n\tbuiltin instead of inline asm.\n\t(vmlal_high_lane_s32): Likewise.\n\t(vmlal_high_lane_u16): Likewise.\n\t(vmlal_high_lane_u32): Likewise.\n\t(vmlal_high_laneq_s16): Likewise.\n\t(vmlal_high_laneq_s32): Likewise.\n\t(vmlal_high_laneq_u16): Likewise.\n\t(vmlal_high_laneq_u32): Likewise.", "tree": {"sha": "308dbfa2da68f7caa25034c945720d990bc43827", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/308dbfa2da68f7caa25034c945720d990bc43827"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/9a00ff96fad209ebde56b227d313cad5d769dc55", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9a00ff96fad209ebde56b227d313cad5d769dc55", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9a00ff96fad209ebde56b227d313cad5d769dc55", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9a00ff96fad209ebde56b227d313cad5d769dc55/comments", "author": {"login": "jwright-arm", "id": 31624044, "node_id": "MDQ6VXNlcjMxNjI0MDQ0", "avatar_url": "https://avatars.githubusercontent.com/u/31624044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwright-arm", "html_url": "https://github.com/jwright-arm", "followers_url": "https://api.github.com/users/jwright-arm/followers", "following_url": "https://api.github.com/users/jwright-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jwright-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwright-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwright-arm/subscriptions", "organizations_url": "https://api.github.com/users/jwright-arm/orgs", "repos_url": "https://api.github.com/users/jwright-arm/repos", "events_url": "https://api.github.com/users/jwright-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jwright-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jwright-arm", "id": 31624044, "node_id": "MDQ6VXNlcjMxNjI0MDQ0", "avatar_url": "https://avatars.githubusercontent.com/u/31624044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwright-arm", "html_url": "https://github.com/jwright-arm", "followers_url": "https://api.github.com/users/jwright-arm/followers", "following_url": "https://api.github.com/users/jwright-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jwright-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwright-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwright-arm/subscriptions", "organizations_url": "https://api.github.com/users/jwright-arm/orgs", "repos_url": "https://api.github.com/users/jwright-arm/repos", "events_url": "https://api.github.com/users/jwright-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jwright-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b2c4cf7b19d2441307132727dde0fb63f27d1530", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b2c4cf7b19d2441307132727dde0fb63f27d1530", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b2c4cf7b19d2441307132727dde0fb63f27d1530"}], "stats": {"total": 235, "additions": 131, "deletions": 104}, "files": [{"sha": "55cc2a49b3b2818bdb42d04c9e61a302f3509a64", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a00ff96fad209ebde56b227d313cad5d769dc55/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a00ff96fad209ebde56b227d313cad5d769dc55/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=9a00ff96fad209ebde56b227d313cad5d769dc55", "patch": "@@ -314,6 +314,11 @@\n   BUILTIN_VD_HSI (QUADOPU_LANE, vec_umlsl_lane_, 0, NONE)\n   BUILTIN_VD_HSI (QUADOPU_LANE, vec_umlsl_laneq_, 0, NONE)\n \n+  BUILTIN_VQ_HSI (QUADOP_LANE, smlal_hi_lane, 0, NONE)\n+  BUILTIN_VQ_HSI (QUADOP_LANE, smlal_hi_laneq, 0, NONE)\n+  BUILTIN_VQ_HSI (QUADOPU_LANE, umlal_hi_lane, 0, NONE)\n+  BUILTIN_VQ_HSI (QUADOPU_LANE, umlal_hi_laneq, 0, NONE)\n+\n   BUILTIN_VSD_HSI (BINOP, sqdmull, 0, NONE)\n   BUILTIN_VSD_HSI (TERNOP_LANE, sqdmull_lane, 0, NONE)\n   BUILTIN_VSD_HSI (TERNOP_LANE, sqdmull_laneq, 0, NONE)"}, {"sha": "60ecd130ef0a52a05082871c2fed05c176a91694", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 70, "deletions": 0, "changes": 70, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a00ff96fad209ebde56b227d313cad5d769dc55/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a00ff96fad209ebde56b227d313cad5d769dc55/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=9a00ff96fad209ebde56b227d313cad5d769dc55", "patch": "@@ -2287,6 +2287,76 @@\n   [(set_attr \"type\" \"neon_mla_<Vetype>_scalar_long\")]\n )\n \n+(define_insn \"aarch64_<su>mlal_hi_lane<mode>_insn\"\n+  [(set (match_operand:<VWIDE> 0 \"register_operand\" \"=w\")\n+\t(plus:<VWIDE>\n+\t  (mult:<VWIDE>\n+\t    (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>\n+\t      (match_operand:VQ_HSI 2 \"register_operand\" \"w\")\n+\t      (match_operand:VQ_HSI 3 \"vect_par_cnst_hi_half\" \"\")))\n+\t    (ANY_EXTEND:<VWIDE> (vec_duplicate:<VHALF>\n+\t      (vec_select:<VEL>\n+\t\t(match_operand:<VCOND> 4 \"register_operand\" \"<vwx>\")\n+\t\t(parallel [(match_operand:SI 5 \"immediate_operand\" \"i\")])))))\n+\t  (match_operand:<VWIDE> 1 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+  {\n+    operands[5] = aarch64_endian_lane_rtx (<VCOND>mode, INTVAL (operands[5]));\n+    return \"<su>mlal2\\\\t%0.<Vwtype>, %2.<Vtype>, %4.<Vetype>[%5]\";\n+  }\n+  [(set_attr \"type\" \"neon_mla_<Vetype>_scalar_long\")]\n+)\n+\n+(define_expand \"aarch64_<su>mlal_hi_lane<mode>\"\n+  [(match_operand:<VWIDE> 0 \"register_operand\")\n+   (match_operand:<VWIDE> 1 \"register_operand\")\n+   (ANY_EXTEND:<VWIDE>(match_operand:VQ_HSI 2 \"register_operand\"))\n+   (match_operand:<VCOND> 3 \"register_operand\")\n+   (match_operand:SI 4 \"immediate_operand\")]\n+  \"TARGET_SIMD\"\n+{\n+  rtx p = aarch64_simd_vect_par_cnst_half (<MODE>mode, <nunits>, true);\n+  emit_insn (gen_aarch64_<su>mlal_hi_lane<mode>_insn (operands[0],\n+\t     operands[1], operands[2], p, operands[3], operands[4]));\n+  DONE;\n+}\n+)\n+\n+(define_insn \"aarch64_<su>mlal_hi_laneq<mode>_insn\"\n+  [(set (match_operand:<VWIDE> 0 \"register_operand\" \"=w\")\n+\t(plus:<VWIDE>\n+\t  (mult:<VWIDE>\n+\t    (ANY_EXTEND:<VWIDE> (vec_select:<VHALF>\n+\t      (match_operand:VQ_HSI 2 \"register_operand\" \"w\")\n+\t      (match_operand:VQ_HSI 3 \"vect_par_cnst_hi_half\" \"\")))\n+\t    (ANY_EXTEND:<VWIDE> (vec_duplicate:<VHALF>\n+\t      (vec_select:<VEL>\n+\t\t(match_operand:<VCONQ> 4 \"register_operand\" \"<vwx>\")\n+\t\t(parallel [(match_operand:SI 5 \"immediate_operand\" \"i\")])))))\n+\t  (match_operand:<VWIDE> 1 \"register_operand\" \"0\")))]\n+  \"TARGET_SIMD\"\n+  {\n+    operands[5] = aarch64_endian_lane_rtx (<VCONQ>mode, INTVAL (operands[5]));\n+    return \"<su>mlal2\\\\t%0.<Vwtype>, %2.<Vtype>, %4.<Vetype>[%5]\";\n+  }\n+  [(set_attr \"type\" \"neon_mla_<Vetype>_scalar_long\")]\n+)\n+\n+(define_expand \"aarch64_<su>mlal_hi_laneq<mode>\"\n+  [(match_operand:<VWIDE> 0 \"register_operand\")\n+   (match_operand:<VWIDE> 1 \"register_operand\")\n+   (ANY_EXTEND:<VWIDE>(match_operand:VQ_HSI 2 \"register_operand\"))\n+   (match_operand:<VCONQ> 3 \"register_operand\")\n+   (match_operand:SI 4 \"immediate_operand\")]\n+  \"TARGET_SIMD\"\n+{\n+  rtx p = aarch64_simd_vect_par_cnst_half (<MODE>mode, <nunits>, true);\n+  emit_insn (gen_aarch64_<su>mlal_hi_laneq<mode>_insn (operands[0],\n+\t     operands[1], operands[2], p, operands[3], operands[4]));\n+  DONE;\n+}\n+)\n+\n (define_insn \"aarch64_vec_<su>mlsl_lane<Qlane>\"\n   [(set (match_operand:<VWIDE> 0 \"register_operand\" \"=w\")\n    (minus:<VWIDE>"}, {"sha": "bfe6b2b9d47251975fd394490117eef2676561cb", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 56, "deletions": 104, "changes": 160, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a00ff96fad209ebde56b227d313cad5d769dc55/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a00ff96fad209ebde56b227d313cad5d769dc55/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=9a00ff96fad209ebde56b227d313cad5d769dc55", "patch": "@@ -7152,117 +7152,69 @@ vmla_u32 (uint32x2_t __a, uint32x2_t __b, uint32x2_t __c)\n                                                  (int32x2_t) __c);\n }\n \n-#define vmlal_high_lane_s16(a, b, c, d)                                 \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x4_t c_ = (c);                                              \\\n-       int16x8_t b_ = (b);                                              \\\n-       int32x4_t a_ = (a);                                              \\\n-       int32x4_t result;                                                \\\n-       __asm__ (\"smlal2 %0.4s, %2.8h, %3.h[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline int32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlal_high_lane_s16(int32x4_t __a, int16x8_t __b, int16x4_t __v,\n+\t\t    const int __lane)\n+{\n+  return __builtin_aarch64_smlal_hi_lanev8hi (__a, __b, __v, __lane);\n+}\n \n-#define vmlal_high_lane_s32(a, b, c, d)                                 \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x2_t c_ = (c);                                              \\\n-       int32x4_t b_ = (b);                                              \\\n-       int64x2_t a_ = (a);                                              \\\n-       int64x2_t result;                                                \\\n-       __asm__ (\"smlal2 %0.2d, %2.4s, %3.s[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline int64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlal_high_lane_s32(int64x2_t __a, int32x4_t __b, int32x2_t __v,\n+\t\t    const int __lane)\n+{\n+  return __builtin_aarch64_smlal_hi_lanev4si (__a, __b, __v, __lane);\n+}\n \n-#define vmlal_high_lane_u16(a, b, c, d)                                 \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x4_t c_ = (c);                                             \\\n-       uint16x8_t b_ = (b);                                             \\\n-       uint32x4_t a_ = (a);                                             \\\n-       uint32x4_t result;                                               \\\n-       __asm__ (\"umlal2 %0.4s, %2.8h, %3.h[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline uint32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlal_high_lane_u16(uint32x4_t __a, uint16x8_t __b, uint16x4_t __v,\n+\t\t    const int __lane)\n+{\n+  return __builtin_aarch64_umlal_hi_lanev8hi_uuuus (__a, __b, __v, __lane);\n+}\n \n-#define vmlal_high_lane_u32(a, b, c, d)                                 \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x2_t c_ = (c);                                             \\\n-       uint32x4_t b_ = (b);                                             \\\n-       uint64x2_t a_ = (a);                                             \\\n-       uint64x2_t result;                                               \\\n-       __asm__ (\"umlal2 %0.2d, %2.4s, %3.s[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline uint64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlal_high_lane_u32(uint64x2_t __a, uint32x4_t __b, uint32x2_t __v,\n+\t\t    const int __lane)\n+{\n+  return __builtin_aarch64_umlal_hi_lanev4si_uuuus (__a, __b, __v, __lane);\n+}\n \n-#define vmlal_high_laneq_s16(a, b, c, d)                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int16x8_t c_ = (c);                                              \\\n-       int16x8_t b_ = (b);                                              \\\n-       int32x4_t a_ = (a);                                              \\\n-       int32x4_t result;                                                \\\n-       __asm__ (\"smlal2 %0.4s, %2.8h, %3.h[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline int32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlal_high_laneq_s16(int32x4_t __a, int16x8_t __b, int16x8_t __v,\n+\t\t     const int __lane)\n+{\n+  return __builtin_aarch64_smlal_hi_laneqv8hi (__a, __b, __v, __lane);\n+}\n \n-#define vmlal_high_laneq_s32(a, b, c, d)                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       int32x4_t c_ = (c);                                              \\\n-       int32x4_t b_ = (b);                                              \\\n-       int64x2_t a_ = (a);                                              \\\n-       int64x2_t result;                                                \\\n-       __asm__ (\"smlal2 %0.2d, %2.4s, %3.s[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline int64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlal_high_laneq_s32(int64x2_t __a, int32x4_t __b, int32x4_t __v,\n+\t\t     const int __lane)\n+{\n+  return __builtin_aarch64_smlal_hi_laneqv4si (__a, __b, __v, __lane);\n+}\n \n-#define vmlal_high_laneq_u16(a, b, c, d)                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint16x8_t c_ = (c);                                             \\\n-       uint16x8_t b_ = (b);                                             \\\n-       uint32x4_t a_ = (a);                                             \\\n-       uint32x4_t result;                                               \\\n-       __asm__ (\"umlal2 %0.4s, %2.8h, %3.h[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"x\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline uint32x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlal_high_laneq_u16(uint32x4_t __a, uint16x8_t __b, uint16x8_t __v,\n+\t\t     const int __lane)\n+{\n+  return __builtin_aarch64_umlal_hi_laneqv8hi_uuuus (__a, __b, __v, __lane);\n+}\n \n-#define vmlal_high_laneq_u32(a, b, c, d)                                \\\n-  __extension__                                                         \\\n-    ({                                                                  \\\n-       uint32x4_t c_ = (c);                                             \\\n-       uint32x4_t b_ = (b);                                             \\\n-       uint64x2_t a_ = (a);                                             \\\n-       uint64x2_t result;                                               \\\n-       __asm__ (\"umlal2 %0.2d, %2.4s, %3.s[%4]\"                         \\\n-                : \"=w\"(result)                                          \\\n-                : \"0\"(a_), \"w\"(b_), \"w\"(c_), \"i\"(d)                     \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n+__extension__ extern __inline uint64x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vmlal_high_laneq_u32(uint64x2_t __a, uint32x4_t __b, uint32x4_t __v,\n+\t\t     const int __lane)\n+{\n+  return __builtin_aarch64_umlal_hi_laneqv4si_uuuus (__a, __b, __v, __lane);\n+}\n \n __extension__ extern __inline int32x4_t\n __attribute__ ((__always_inline__, __gnu_inline__, __artificial__))"}]}