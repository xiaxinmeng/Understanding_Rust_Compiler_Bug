{"sha": "299456f3c2978c03a18fc419dc7e29b65e5844f0", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6Mjk5NDU2ZjNjMjk3OGMwM2ExOGZjNDE5ZGM3ZTI5YjY1ZTU4NDRmMA==", "commit": {"author": {"name": "Ben Elliston", "email": "bje@au.ibm.com", "date": "2009-10-26T21:59:17Z"}, "committer": {"name": "Ulrich Weigand", "email": "uweigand@gcc.gnu.org", "date": "2009-10-26T21:59:17Z"}, "message": "config.gcc (spu-*-elf*): Add spu_cache.h to extra_headers.\n\n2009-10-26  Ben Elliston  <bje@au.ibm.com>\n\t    Michael Meissner  <meissner@linux.vnet.ibm.com>\n\t    Ulrich Weigand  <uweigand@de.ibm.com>\n\n\t* config.gcc (spu-*-elf*): Add spu_cache.h to extra_headers.\n\t* config/spu/spu_cache.h: New file.\n\n\t* config/spu/cachemgr.c: New file.\n\t* config/spu/cache.S: New file.\n\n\t* config/spu/spu.h (ASM_OUTPUT_SYMBOL_REF): Define.\n\t(ADDR_SPACE_EA): Define.\n\t(TARGET_ADDR_SPACE_KEYWORDS): Define.\n\t* config/spu/spu.c (EAmode): New macro.\n\t(TARGET_ADDR_SPACE_POINTER_MODE): Define.\n\t(TARGET_ADDR_SPACE_ADDRESS_MODE): Likewise.\n\t(TARGET_ADDR_SPACE_LEGITIMATE_ADDRESS_P): Likewise.\n\t(TARGET_ADDR_SPACE_LEGITIMIZE_ADDRESS): Likewise.\n\t(TARGET_ADDR_SPACE_SUBSET_P): Likewise.\n\t(TARGET_ADDR_SPACE_CONVERT): Likewise.\n\t(TARGET_ASM_SELECT_SECTION): Likewise.\n\t(TARGET_ASM_UNIQUE_SECTION): Likewise.\n\t(TARGET_ASM_UNALIGNED_SI_OP): Likewise.\n\t(TARGET_ASM_ALIGNED_DI_OP): Likewise.\n\t(ea_symbol_ref): New function.\n\t(spu_legitimate_constant_p): Handle __ea qualified addresses.\n\t(spu_addr_space_legitimate_address_p): New function.\n\t(spu_addr_space_legitimize_address): Likewise.\n\t(cache_fetch): New global.\n\t(cache_fetch_dirty): Likewise.\n\t(ea_alias_set): Likewise.\n\t(ea_load_store): New function.\n\t(ea_load_store_inline): Likewise.\n\t(expand_ea_mem): Likewise.\n\t(spu_expand_mov): Handle __ea qualified memory references.\n\t(spu_addr_space_pointer_mode): New function.\n\t(spu_addr_space_address_mode): Likewise.\n\t(spu_addr_space_subset_p): Likewise.\n\t(spu_addr_space_convert): Likewise.\n\t(spu_section_type_flags): Handle \"._ea\" section.\n\t(spu_select_section): New function.\n\t(spu_unique_section): Likewise.\n\t* config/spu/spu-c.c (spu_cpu_cpp_builtins): Support __EA32__\n\tand __EA64__ predefined macros.\n\t* config/spu/spu-elf.h (LIB_SPEC): Handle -mcache-size= and\n\t-matomic-updates switches.\n\n\t* config/spu/t-spu-elf (MULTILIB_OPTIONS): Define.\n\t(EXTRA_MULTILIB_PARTS): Add libgcc_cachemgr.a,\n\tlibgcc_cachemgr_nonatomic.a, libgcc_cache8k.a, libgcc_cache16k.a,\n\tlibgcc_cache32k.a, libgcc_cache64k.a, libgcc_cache128k.a.\n\t($(T)cachemgr.o, $(T)cachemgr_nonatomic.o): New target.\n\t($(T)cache8k.o, $(T)cache16k.o, $(T)cache32k.o, $(T)cache64k.o,\n\t$(T)cache128k.o): Likewise.\n\t($(T)libgcc_%.a): Likewise.\n\n\t* config/spu/spu.h (TARGET_DEFAULT): Add MASK_ADDRESS_SPACE_CONVERSION.\n\t* config/spu/spu.opt (-mea32/-mea64): Add switches.\n\t(-maddress-space-conversion): Likewise.\n\t(-mcache-size=): Likewise.\n\t(-matomic-updates): Likewise.\n\t* doc/invoke.texi (-mea32/-mea64): Document.\n\t(-maddress-space-conversion): Likewise.\n\t(-mcache-size=): Likewise.\n\t(-matomic-updates): Likewise.\n\nCo-Authored-By: Michael Meissner <meissner@linux.vnet.ibm.com>\nCo-Authored-By: Ulrich Weigand <uweigand@de.ibm.com>\n\nFrom-SVN: r153575", "tree": {"sha": "143dcc98602489eb982d8267ad6b5f0e37794d35", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/143dcc98602489eb982d8267ad6b5f0e37794d35"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/299456f3c2978c03a18fc419dc7e29b65e5844f0", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/299456f3c2978c03a18fc419dc7e29b65e5844f0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/299456f3c2978c03a18fc419dc7e29b65e5844f0", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/299456f3c2978c03a18fc419dc7e29b65e5844f0/comments", "author": null, "committer": null, "parents": [{"sha": "36c5e70a3aab4f1c25bfe706648aab258e89be1a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/36c5e70a3aab4f1c25bfe706648aab258e89be1a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/36c5e70a3aab4f1c25bfe706648aab258e89be1a"}], "stats": {"total": 1220, "additions": 1209, "deletions": 11}, "files": [{"sha": "3014db71c12cbfd5a64425407c01073898e5a42a", "filename": "gcc/ChangeLog", "status": "modified", "additions": 66, "deletions": 0, "changes": 66, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -1,3 +1,69 @@\n+2009-10-26  Ben Elliston  <bje@au.ibm.com>\n+\t    Michael Meissner  <meissner@linux.vnet.ibm.com>\n+\t    Ulrich Weigand  <uweigand@de.ibm.com>\n+\n+\t* config.gcc (spu-*-elf*): Add spu_cache.h to extra_headers.\n+\t* config/spu/spu_cache.h: New file.\n+\n+\t* config/spu/cachemgr.c: New file.\n+\t* config/spu/cache.S: New file.\n+\n+\t* config/spu/spu.h (ASM_OUTPUT_SYMBOL_REF): Define.\n+\t(ADDR_SPACE_EA): Define.\n+\t(TARGET_ADDR_SPACE_KEYWORDS): Define.\n+\t* config/spu/spu.c (EAmode): New macro.\n+\t(TARGET_ADDR_SPACE_POINTER_MODE): Define.\n+\t(TARGET_ADDR_SPACE_ADDRESS_MODE): Likewise.\n+\t(TARGET_ADDR_SPACE_LEGITIMATE_ADDRESS_P): Likewise.\n+\t(TARGET_ADDR_SPACE_LEGITIMIZE_ADDRESS): Likewise.\n+\t(TARGET_ADDR_SPACE_SUBSET_P): Likewise.\n+\t(TARGET_ADDR_SPACE_CONVERT): Likewise.\n+\t(TARGET_ASM_SELECT_SECTION): Likewise.\n+\t(TARGET_ASM_UNIQUE_SECTION): Likewise.\n+\t(TARGET_ASM_UNALIGNED_SI_OP): Likewise.\n+\t(TARGET_ASM_ALIGNED_DI_OP): Likewise.\n+\t(ea_symbol_ref): New function.\n+\t(spu_legitimate_constant_p): Handle __ea qualified addresses.\n+\t(spu_addr_space_legitimate_address_p): New function.\n+\t(spu_addr_space_legitimize_address): Likewise.\n+\t(cache_fetch): New global.\n+\t(cache_fetch_dirty): Likewise.\n+\t(ea_alias_set): Likewise.\n+\t(ea_load_store): New function.\n+\t(ea_load_store_inline): Likewise.\n+\t(expand_ea_mem): Likewise.\n+\t(spu_expand_mov): Handle __ea qualified memory references.\n+\t(spu_addr_space_pointer_mode): New function.\n+\t(spu_addr_space_address_mode): Likewise.\n+\t(spu_addr_space_subset_p): Likewise.\n+\t(spu_addr_space_convert): Likewise.\n+\t(spu_section_type_flags): Handle \"._ea\" section.\n+\t(spu_select_section): New function.\n+\t(spu_unique_section): Likewise.\n+\t* config/spu/spu-c.c (spu_cpu_cpp_builtins): Support __EA32__\n+\tand __EA64__ predefined macros.\n+\t* config/spu/spu-elf.h (LIB_SPEC): Handle -mcache-size= and\n+\t-matomic-updates switches.\n+\n+\t* config/spu/t-spu-elf (MULTILIB_OPTIONS): Define.\n+\t(EXTRA_MULTILIB_PARTS): Add libgcc_cachemgr.a,\n+\tlibgcc_cachemgr_nonatomic.a, libgcc_cache8k.a, libgcc_cache16k.a,\n+\tlibgcc_cache32k.a, libgcc_cache64k.a, libgcc_cache128k.a.\n+\t($(T)cachemgr.o, $(T)cachemgr_nonatomic.o): New target.\n+\t($(T)cache8k.o, $(T)cache16k.o, $(T)cache32k.o, $(T)cache64k.o,\n+\t$(T)cache128k.o): Likewise.\n+\t($(T)libgcc_%.a): Likewise.\n+\n+\t* config/spu/spu.h (TARGET_DEFAULT): Add MASK_ADDRESS_SPACE_CONVERSION.\n+\t* config/spu/spu.opt (-mea32/-mea64): Add switches.\n+\t(-maddress-space-conversion): Likewise.\n+\t(-mcache-size=): Likewise.\n+\t(-matomic-updates): Likewise.\n+\t* doc/invoke.texi (-mea32/-mea64): Document.\n+\t(-maddress-space-conversion): Likewise.\n+\t(-mcache-size=): Likewise.\n+\t(-matomic-updates): Likewise.\n+\n 2009-10-26  Ben Elliston  <bje@au.ibm.com>\n \t    Michael Meissner  <meissner@linux.vnet.ibm.com>\n \t    Ulrich Weigand  <uweigand@de.ibm.com>"}, {"sha": "4f388babe6a36e160f0f4a586153f92ece3e3c17", "filename": "gcc/config.gcc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -2449,7 +2449,7 @@ sparc64-*-netbsd*)\n spu-*-elf*)\n \ttm_file=\"dbxelf.h elfos.h spu/spu-elf.h spu/spu.h newlib-stdint.h\"\n \ttmake_file=\"spu/t-spu-elf\"\n-\textra_headers=\"spu_intrinsics.h spu_internals.h vmx2spu.h spu_mfcio.h vec_types.h\"\n+\textra_headers=\"spu_intrinsics.h spu_internals.h vmx2spu.h spu_mfcio.h vec_types.h spu_cache.h\"\n \textra_modes=spu/spu-modes.def\n \tc_target_objs=\"${c_target_objs} spu-c.o\"\n \tcxx_target_objs=\"${cxx_target_objs} spu-c.o\""}, {"sha": "9ffb6a0d1948bcf55fcb22ebc603b650d9438394", "filename": "gcc/config/spu/cache.S", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fcache.S", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fcache.S", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fcache.S?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -0,0 +1,43 @@\n+/* Copyright (C) 2008, 2009 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+Under Section 7 of GPL version 3, you are granted additional\n+permissions described in the GCC Runtime Library Exception, version\n+3.1, as published by the Free Software Foundation.\n+\n+You should have received a copy of the GNU General Public License and\n+a copy of the GCC Runtime Library Exception along with this program;\n+see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+<http://www.gnu.org/licenses/>.  */\n+\n+\t.data\n+\t.p2align 7\n+\t.global __cache\n+__cache:\n+\t.rept __CACHE_SIZE__ * 8\n+\t.fill 128\n+\t.endr\n+\n+\t.p2align 7\n+\t.global __cache_tag_array\n+__cache_tag_array:\n+\t.rept __CACHE_SIZE__ * 2\n+\t.long 1, 1, 1, 1\n+\t.fill 128-16\n+\t.endr\n+__end_cache_tag_array:\n+\n+\t.globl __cache_tag_array_size\n+\t.set __cache_tag_array_size, __end_cache_tag_array-__cache_tag_array\n+"}, {"sha": "e7abd5e62dbdc283ccc6c596b353cf9de7904c31", "filename": "gcc/config/spu/cachemgr.c", "status": "added", "additions": 438, "deletions": 0, "changes": 438, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fcachemgr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fcachemgr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fcachemgr.c?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -0,0 +1,438 @@\n+/* Copyright (C) 2008, 2009 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+Under Section 7 of GPL version 3, you are granted additional\n+permissions described in the GCC Runtime Library Exception, version\n+3.1, as published by the Free Software Foundation.\n+\n+You should have received a copy of the GNU General Public License and\n+a copy of the GCC Runtime Library Exception along with this program;\n+see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include <spu_mfcio.h>\n+#include <spu_internals.h>\n+#include <spu_intrinsics.h>\n+#include <spu_cache.h>\n+\n+extern unsigned long long __ea_local_store;\n+extern char __cache_tag_array_size;\n+\n+#define LINE_SIZE 128\n+#define TAG_MASK (LINE_SIZE - 1)\n+\n+#define WAYS 4\n+#define SET_MASK ((int) &__cache_tag_array_size - LINE_SIZE)\n+\n+#define CACHE_LINES ((int) &__cache_tag_array_size /\t\t\\\n+\t\t     sizeof (struct __cache_tag_array) * WAYS)\n+\n+struct __cache_tag_array\n+{\n+  unsigned int tag_lo[WAYS];\n+  unsigned int tag_hi[WAYS];\n+  void *base[WAYS];\n+  int reserved[WAYS];\n+  vector unsigned short dirty_bits[WAYS];\n+};\n+\n+extern struct __cache_tag_array __cache_tag_array[];\n+extern char __cache[];\n+\n+/* In order to make the code seem a little cleaner, and to avoid having\n+   64/32 bit ifdefs all over the place, we use macros.  */\n+\n+#ifdef __EA64__\n+typedef unsigned long long addr;\n+\n+#define CHECK_TAG(_entry, _way, _tag)\t\t\t\\\n+  ((_entry)->tag_lo[(_way)] == ((_tag) & 0xFFFFFFFF)\t\\\n+   && (_entry)->tag_hi[(_way)] == ((_tag) >> 32))\n+\n+#define GET_TAG(_entry, _way) \\\n+  ((unsigned long long)(_entry)->tag_hi[(_way)] << 32\t\\\n+   | (unsigned long long)(_entry)->tag_lo[(_way)])\n+\n+#define SET_TAG(_entry, _way, _tag)\t\t\t\\\n+  (_entry)->tag_lo[(_way)] = (_tag) & 0xFFFFFFFF;\t\\\n+  (_entry)->tag_hi[(_way)] = (_tag) >> 32\n+\n+#else /*__EA32__*/\n+typedef unsigned long addr;\n+\n+#define CHECK_TAG(_entry, _way, _tag)\t\t\t\\\n+  ((_entry)->tag_lo[(_way)] == (_tag))\n+\n+#define GET_TAG(_entry, _way)\t\t\t\t\\\n+  ((_entry)->tag_lo[(_way)])\n+\n+#define SET_TAG(_entry, _way, _tag)\t\t\t\\\n+  (_entry)->tag_lo[(_way)] = (_tag)\n+\n+#endif\n+\n+/* In GET_ENTRY, we cast away the high 32 bits,\n+   as the tag is only in the low 32.  */\n+\n+#define GET_ENTRY(_addr)\t\t\t\t\t\t   \\\n+  ((struct __cache_tag_array *)\t\t\t\t\t\t   \\\n+   si_to_uint (si_a (si_and (si_from_uint ((unsigned int) (addr) (_addr)), \\\n+\t\t\t     si_from_uint (SET_MASK)),\t\t\t   \\\n+\t       si_from_uint ((unsigned int) __cache_tag_array))))\n+\n+#define GET_CACHE_LINE(_addr, _way) \\\n+  ((void *) (__cache + ((_addr) & SET_MASK) * WAYS) + ((_way) * LINE_SIZE));\n+\n+#define CHECK_DIRTY(_vec) (si_to_uint (si_orx ((qword) (_vec))))\n+#define SET_EMPTY(_entry, _way) ((_entry)->tag_lo[(_way)] = 1)\n+#define CHECK_EMPTY(_entry, _way) ((_entry)->tag_lo[(_way)] == 1)\n+\n+#define LS_FLAG 0x80000000\n+#define SET_IS_LS(_entry, _way) ((_entry)->reserved[(_way)] |= LS_FLAG)\n+#define CHECK_IS_LS(_entry, _way) ((_entry)->reserved[(_way)] & LS_FLAG)\n+#define GET_LRU(_entry, _way) ((_entry)->reserved[(_way)] & ~LS_FLAG)\n+\n+static int dma_tag = 32;\n+\n+static void\n+__cache_evict_entry (struct __cache_tag_array *entry, int way)\n+{\n+  addr tag = GET_TAG (entry, way);\n+\n+  if (CHECK_DIRTY (entry->dirty_bits[way]) && !CHECK_IS_LS (entry, way))\n+    {\n+#ifdef NONATOMIC\n+      /* Non-atomic writes.  */\n+      unsigned int oldmask, mach_stat;\n+      char *line = ((void *) 0);\n+\n+      /* Enter critical section.  */\n+      mach_stat = spu_readch (SPU_RdMachStat);\n+      spu_idisable ();\n+\n+      /* Issue DMA request.  */\n+      line = GET_CACHE_LINE (entry->tag_lo[way], way);\n+      mfc_put (line, tag, LINE_SIZE, dma_tag, 0, 0);\n+\n+      /* Wait for DMA completion.  */\n+      oldmask = mfc_read_tag_mask ();\n+      mfc_write_tag_mask (1 << dma_tag);\n+      mfc_read_tag_status_all ();\n+      mfc_write_tag_mask (oldmask);\n+\n+      /* Leave critical section.  */\n+      if (__builtin_expect (mach_stat & 1, 0))\n+\tspu_ienable ();\n+#else\n+      /* Allocate a buffer large enough that we know it has 128 bytes\n+         that are 128 byte aligned (for DMA). */\n+\n+      char buffer[LINE_SIZE + 127];\n+      qword *buf_ptr = (qword *) (((unsigned int) (buffer) + 127) & ~127);\n+      qword *line = GET_CACHE_LINE (entry->tag_lo[way], way);\n+      qword bits;\n+      unsigned int mach_stat;\n+\n+      /* Enter critical section.  */\n+      mach_stat = spu_readch (SPU_RdMachStat);\n+      spu_idisable ();\n+\n+      do\n+\t{\n+\t  /* We atomically read the current memory into a buffer\n+\t     modify the dirty bytes in the buffer, and write it\n+\t     back. If writeback fails, loop and try again.  */\n+\n+\t  mfc_getllar (buf_ptr, tag, 0, 0);\n+\t  mfc_read_atomic_status ();\n+\n+\t  /* The method we're using to write 16 dirty bytes into\n+\t     the buffer at a time uses fsmb which in turn uses\n+\t     the least significant 16 bits of word 0, so we\n+\t     load the bits and rotate so that the first bit of\n+\t     the bitmap is in the first bit that fsmb will use.  */\n+\n+\t  bits = (qword) entry->dirty_bits[way];\n+\t  bits = si_rotqbyi (bits, -2);\n+\n+\t  /* Si_fsmb creates the mask of dirty bytes.\n+\t     Use selb to nab the appropriate bits.  */\n+\t  buf_ptr[0] = si_selb (buf_ptr[0], line[0], si_fsmb (bits));\n+\n+\t  /* Rotate to next 16 byte section of cache.  */\n+\t  bits = si_rotqbyi (bits, 2);\n+\n+\t  buf_ptr[1] = si_selb (buf_ptr[1], line[1], si_fsmb (bits));\n+\t  bits = si_rotqbyi (bits, 2);\n+\t  buf_ptr[2] = si_selb (buf_ptr[2], line[2], si_fsmb (bits));\n+\t  bits = si_rotqbyi (bits, 2);\n+\t  buf_ptr[3] = si_selb (buf_ptr[3], line[3], si_fsmb (bits));\n+\t  bits = si_rotqbyi (bits, 2);\n+\t  buf_ptr[4] = si_selb (buf_ptr[4], line[4], si_fsmb (bits));\n+\t  bits = si_rotqbyi (bits, 2);\n+\t  buf_ptr[5] = si_selb (buf_ptr[5], line[5], si_fsmb (bits));\n+\t  bits = si_rotqbyi (bits, 2);\n+\t  buf_ptr[6] = si_selb (buf_ptr[6], line[6], si_fsmb (bits));\n+\t  bits = si_rotqbyi (bits, 2);\n+\t  buf_ptr[7] = si_selb (buf_ptr[7], line[7], si_fsmb (bits));\n+\t  bits = si_rotqbyi (bits, 2);\n+\n+\t  mfc_putllc (buf_ptr, tag, 0, 0);\n+\t}\n+      while (mfc_read_atomic_status ());\n+\n+      /* Leave critical section.  */\n+      if (__builtin_expect (mach_stat & 1, 0))\n+\tspu_ienable ();\n+#endif\n+    }\n+\n+  /* In any case, marking the lo tag with 1 which denotes empty.  */\n+  SET_EMPTY (entry, way);\n+  entry->dirty_bits[way] = (vector unsigned short) si_from_uint (0);\n+}\n+\n+void\n+__cache_evict (__ea void *ea)\n+{\n+  addr tag = (addr) ea & ~TAG_MASK;\n+  struct __cache_tag_array *entry = GET_ENTRY (ea);\n+  int i = 0;\n+\n+  /* Cycles through all the possible ways an address could be at\n+     and evicts the way if found.  */\n+\n+  for (i = 0; i < WAYS; i++)\n+    if (CHECK_TAG (entry, i, tag))\n+      __cache_evict_entry (entry, i);\n+}\n+\n+static void *\n+__cache_fill (int way, addr tag)\n+{\n+  unsigned int oldmask, mach_stat;\n+  char *line = ((void *) 0);\n+\n+  /* Reserve our DMA tag.  */\n+  if (dma_tag == 32)\n+    dma_tag = mfc_tag_reserve ();\n+\n+  /* Enter critical section.  */\n+  mach_stat = spu_readch (SPU_RdMachStat);\n+  spu_idisable ();\n+\n+  /* Issue DMA request.  */\n+  line = GET_CACHE_LINE (tag, way);\n+  mfc_get (line, tag, LINE_SIZE, dma_tag, 0, 0);\n+\n+  /* Wait for DMA completion.  */\n+  oldmask = mfc_read_tag_mask ();\n+  mfc_write_tag_mask (1 << dma_tag);\n+  mfc_read_tag_status_all ();\n+  mfc_write_tag_mask (oldmask);\n+\n+  /* Leave critical section.  */\n+  if (__builtin_expect (mach_stat & 1, 0))\n+    spu_ienable ();\n+\n+  return (void *) line;\n+}\n+\n+static void\n+__cache_miss (__ea void *ea, struct __cache_tag_array *entry, int way)\n+{\n+\n+  addr tag = (addr) ea & ~TAG_MASK;\n+  unsigned int lru = 0;\n+  int i = 0;\n+  int idx = 0;\n+\n+  /* If way > 4, then there are no empty slots, so we must evict\n+     the least recently used entry. */\n+  if (way >= 4)\n+    {\n+      for (i = 0; i < WAYS; i++)\n+\t{\n+\t  if (GET_LRU (entry, i) > lru)\n+\t    {\n+\t      lru = GET_LRU (entry, i);\n+\t      idx = i;\n+\t    }\n+\t}\n+      __cache_evict_entry (entry, idx);\n+      way = idx;\n+    }\n+\n+  /* Set the empty entry's tag and fill it's cache line. */\n+\n+  SET_TAG (entry, way, tag);\n+  entry->reserved[way] = 0;\n+\n+  /* Check if the address is just an effective address within the\n+     SPU's local store. */\n+\n+  /* Because the LS is not 256k aligned, we can't do a nice and mask\n+     here to compare, so we must check the whole range.  */\n+\n+  if ((addr) ea >= (addr) __ea_local_store\n+      && (addr) ea < (addr) (__ea_local_store + 0x40000))\n+    {\n+      SET_IS_LS (entry, way);\n+      entry->base[way] =\n+\t(void *) ((unsigned int) ((addr) ea -\n+\t\t\t\t  (addr) __ea_local_store) & ~0x7f);\n+    }\n+  else\n+    {\n+      entry->base[way] = __cache_fill (way, tag);\n+    }\n+}\n+\n+void *\n+__cache_fetch_dirty (__ea void *ea, int n_bytes_dirty)\n+{\n+#ifdef __EA64__\n+  unsigned int tag_hi;\n+  qword etag_hi;\n+#endif\n+  unsigned int tag_lo;\n+  struct __cache_tag_array *entry;\n+\n+  qword etag_lo;\n+  qword equal;\n+  qword bit_mask;\n+  qword way;\n+\n+  /* This first chunk, we merely fill the pointer and tag.  */\n+\n+  entry = GET_ENTRY (ea);\n+\n+#ifndef __EA64__\n+  tag_lo =\n+    si_to_uint (si_andc\n+\t\t(si_shufb\n+\t\t (si_from_uint ((addr) ea), si_from_uint (0),\n+\t\t  si_from_uint (0x00010203)), si_from_uint (TAG_MASK)));\n+#else\n+  tag_lo =\n+    si_to_uint (si_andc\n+\t\t(si_shufb\n+\t\t (si_from_ullong ((addr) ea), si_from_uint (0),\n+\t\t  si_from_uint (0x04050607)), si_from_uint (TAG_MASK)));\n+\n+  tag_hi =\n+    si_to_uint (si_shufb\n+\t\t(si_from_ullong ((addr) ea), si_from_uint (0),\n+\t\t si_from_uint (0x00010203)));\n+#endif\n+\n+  /* Increment LRU in reserved bytes.  */\n+  si_stqd (si_ai (si_lqd (si_from_ptr (entry), 48), 1),\n+\t   si_from_ptr (entry), 48);\n+\n+missreturn:\n+  /* Check if the entry's lo_tag is equal to the address' lo_tag.  */\n+  etag_lo = si_lqd (si_from_ptr (entry), 0);\n+  equal = si_ceq (etag_lo, si_from_uint (tag_lo));\n+#ifdef __EA64__\n+  /* And the high tag too.  */\n+  etag_hi = si_lqd (si_from_ptr (entry), 16);\n+  equal = si_and (equal, (si_ceq (etag_hi, si_from_uint (tag_hi))));\n+#endif\n+\n+  if ((si_to_uint (si_orx (equal)) == 0))\n+    goto misshandler;\n+\n+  if (n_bytes_dirty)\n+    {\n+      /* way = 0x40,0x50,0x60,0x70 for each way, which is also the\n+         offset of the appropriate dirty bits.  */\n+      way = si_shli (si_clz (si_gbb (equal)), 2);\n+\n+      /* To create the bit_mask, we set it to all 1s (uint -1), then we\n+         shift it over (128 - n_bytes_dirty) times.  */\n+\n+      bit_mask = si_from_uint (-1);\n+\n+      bit_mask =\n+\tsi_shlqby (bit_mask, si_from_uint ((LINE_SIZE - n_bytes_dirty) / 8));\n+\n+      bit_mask =\n+\tsi_shlqbi (bit_mask, si_from_uint ((LINE_SIZE - n_bytes_dirty) % 8));\n+\n+      /* Rotate it around to the correct offset.  */\n+      bit_mask =\n+\tsi_rotqby (bit_mask,\n+\t\t   si_from_uint (-1 * ((addr) ea & TAG_MASK) / 8));\n+\n+      bit_mask =\n+\tsi_rotqbi (bit_mask,\n+\t\t   si_from_uint (-1 * ((addr) ea & TAG_MASK) % 8));\n+\n+      /* Update the dirty bits.  */\n+      si_stqx (si_or (si_lqx (si_from_ptr (entry), way), bit_mask),\n+\t       si_from_ptr (entry), way);\n+    };\n+\n+  /* We've definitely found the right entry, set LRU (reserved) to 0\n+     maintaining the LS flag (MSB).  */\n+\n+  si_stqd (si_andc\n+\t   (si_lqd (si_from_ptr (entry), 48),\n+\t    si_and (equal, si_from_uint (~(LS_FLAG)))),\n+\t   si_from_ptr (entry), 48);\n+\n+  return (void *)\n+    si_to_uint (si_a\n+\t\t(si_orx\n+\t\t (si_and (si_lqd (si_from_ptr (entry), 32), equal)),\n+\t\t si_from_uint (((unsigned int) (addr) ea) & TAG_MASK)));\n+\n+misshandler:\n+  equal = si_ceqi (etag_lo, 1);\n+  __cache_miss (ea, entry, (si_to_uint (si_clz (si_gbb (equal))) - 16) >> 2);\n+  goto missreturn;\n+}\n+\n+void *\n+__cache_fetch (__ea void *ea)\n+{\n+  return __cache_fetch_dirty (ea, 0);\n+}\n+\n+void\n+__cache_touch (__ea void *ea __attribute__ ((unused)))\n+{\n+  /* NO-OP for now.  */\n+}\n+\n+void __cache_flush (void) __attribute__ ((destructor));\n+void\n+__cache_flush (void)\n+{\n+  struct __cache_tag_array *entry = __cache_tag_array;\n+  unsigned int i;\n+  int j;\n+\n+  /* Cycle through each cache entry and evict all used ways.  */\n+\n+  for (i = 0; i < CACHE_LINES / WAYS; i++)\n+    {\n+      for (j = 0; j < WAYS; j++)\n+\tif (!CHECK_EMPTY (entry, j))\n+\t  __cache_evict_entry (entry, j);\n+\n+      entry++;\n+    }\n+}"}, {"sha": "380af402c48241847410e187622dd173f0eaa059", "filename": "gcc/config/spu/spu-c.c", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu-c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu-c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu-c.c?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -201,6 +201,17 @@ spu_cpu_cpp_builtins (struct cpp_reader *pfile)\n   if (spu_arch == PROCESSOR_CELLEDP)\n     builtin_define_std (\"__SPU_EDP__\");\n   builtin_define_std (\"__vector=__attribute__((__spu_vector__))\");\n+  switch (spu_ea_model)\n+    {\n+    case 32:\n+      builtin_define_std (\"__EA32__\");\n+      break;\n+    case 64:\n+      builtin_define_std (\"__EA64__\");\n+      break;\n+    default:\n+       gcc_unreachable ();\n+    }\n \n   if (!flag_iso)\n     {"}, {"sha": "68982002103dd29d04027a2f3033f1c2d915f757", "filename": "gcc/config/spu/spu-elf.h", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu-elf.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu-elf.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu-elf.h?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -68,8 +68,14 @@\n \n #define LINK_SPEC \"%{mlarge-mem: --defsym __stack=0xfffffff0 }\"\n \n-#define LIB_SPEC \\\n-\t\"-( %{!shared:%{g*:-lg}} -lc -lgloss -)\"\n+#define LIB_SPEC \"-( %{!shared:%{g*:-lg}} -lc -lgloss -) \\\n+    %{mno-atomic-updates:-lgcc_cachemgr_nonatomic; :-lgcc_cachemgr} \\\n+    %{mcache-size=128:-lgcc_cache128k; \\\n+      mcache-size=64 :-lgcc_cache64k; \\\n+      mcache-size=32 :-lgcc_cache32k; \\\n+      mcache-size=16 :-lgcc_cache16k; \\\n+      mcache-size=8  :-lgcc_cache8k; \\\n+                     :-lgcc_cache64k}\"\n \n /* Turn off warnings in the assembler too. */\n #undef ASM_SPEC"}, {"sha": "2888da6728183c2ee75779bcc9f5ba4e36a5cf49", "filename": "gcc/config/spu/spu.c", "status": "modified", "additions": 489, "deletions": 2, "changes": 491, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu.c?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -154,6 +154,8 @@ static tree spu_builtin_decl (unsigned, bool);\n static unsigned char spu_scalar_mode_supported_p (enum machine_mode mode);\n static unsigned char spu_vector_mode_supported_p (enum machine_mode mode);\n static bool spu_legitimate_address_p (enum machine_mode, rtx, bool);\n+static bool spu_addr_space_legitimate_address_p (enum machine_mode, rtx,\n+\t\t\t\t\t\t bool, addr_space_t);\n static rtx adjust_operand (rtx op, HOST_WIDE_INT * start);\n static rtx get_pic_reg (void);\n static int need_to_save_reg (int regno, int saving);\n@@ -203,15 +205,23 @@ static bool spu_return_in_memory (const_tree type, const_tree fntype);\n static void fix_range (const char *);\n static void spu_encode_section_info (tree, rtx, int);\n static rtx spu_legitimize_address (rtx, rtx, enum machine_mode);\n+static rtx spu_addr_space_legitimize_address (rtx, rtx, enum machine_mode,\n+\t\t\t\t\t      addr_space_t);\n static tree spu_builtin_mul_widen_even (tree);\n static tree spu_builtin_mul_widen_odd (tree);\n static tree spu_builtin_mask_for_load (void);\n static int spu_builtin_vectorization_cost (bool);\n static bool spu_vector_alignment_reachable (const_tree, bool);\n static tree spu_builtin_vec_perm (tree, tree *);\n+static enum machine_mode spu_addr_space_pointer_mode (addr_space_t);\n+static enum machine_mode spu_addr_space_address_mode (addr_space_t);\n+static bool spu_addr_space_subset_p (addr_space_t, addr_space_t);\n+static rtx spu_addr_space_convert (rtx, tree, tree);\n static int spu_sms_res_mii (struct ddg *g);\n static void asm_file_start (void);\n static unsigned int spu_section_type_flags (tree, const char *, int);\n+static section *spu_select_section (tree, int, unsigned HOST_WIDE_INT);\n+static void spu_unique_section (tree, int);\n static rtx spu_expand_load (rtx, rtx, rtx, int);\n static void spu_trampoline_init (rtx, tree, rtx);\n \n@@ -270,6 +280,10 @@ spu_libgcc_cmp_return_mode (void);\n \n static enum machine_mode\n spu_libgcc_shift_count_mode (void);\n+\n+/* Pointer mode for __ea references.  */\n+#define EAmode (spu_ea_model != 32 ? DImode : SImode)\n+\n \f\n /*  Table of machine attributes.  */\n static const struct attribute_spec spu_attribute_table[] =\n@@ -282,6 +296,25 @@ static const struct attribute_spec spu_attribute_table[] =\n \f\n /*  TARGET overrides.  */\n \n+#undef TARGET_ADDR_SPACE_POINTER_MODE\n+#define TARGET_ADDR_SPACE_POINTER_MODE spu_addr_space_pointer_mode\n+\n+#undef TARGET_ADDR_SPACE_ADDRESS_MODE\n+#define TARGET_ADDR_SPACE_ADDRESS_MODE spu_addr_space_address_mode\n+\n+#undef TARGET_ADDR_SPACE_LEGITIMATE_ADDRESS_P\n+#define TARGET_ADDR_SPACE_LEGITIMATE_ADDRESS_P \\\n+  spu_addr_space_legitimate_address_p\n+\n+#undef TARGET_ADDR_SPACE_LEGITIMIZE_ADDRESS\n+#define TARGET_ADDR_SPACE_LEGITIMIZE_ADDRESS spu_addr_space_legitimize_address\n+\n+#undef TARGET_ADDR_SPACE_SUBSET_P\n+#define TARGET_ADDR_SPACE_SUBSET_P spu_addr_space_subset_p\n+\n+#undef TARGET_ADDR_SPACE_CONVERT\n+#define TARGET_ADDR_SPACE_CONVERT spu_addr_space_convert\n+\n #undef TARGET_INIT_BUILTINS\n #define TARGET_INIT_BUILTINS spu_init_builtins\n #undef TARGET_BUILTIN_DECL\n@@ -296,6 +329,15 @@ static const struct attribute_spec spu_attribute_table[] =\n #undef TARGET_LEGITIMIZE_ADDRESS\n #define TARGET_LEGITIMIZE_ADDRESS spu_legitimize_address\n \n+/* The current assembler doesn't like .4byte foo@ppu, so use the normal .long\n+   and .quad for the debugger.  When it is known that the assembler is fixed,\n+   these can be removed.  */\n+#undef TARGET_ASM_UNALIGNED_SI_OP\n+#define TARGET_ASM_UNALIGNED_SI_OP\t\"\\t.long\\t\"\n+\n+#undef TARGET_ASM_ALIGNED_DI_OP\n+#define TARGET_ASM_ALIGNED_DI_OP\t\"\\t.quad\\t\"\n+\n /* The .8byte directive doesn't seem to work well for a 32 bit\n    architecture. */\n #undef TARGET_ASM_UNALIGNED_DI_OP\n@@ -412,6 +454,12 @@ static const struct attribute_spec spu_attribute_table[] =\n #undef TARGET_SECTION_TYPE_FLAGS\n #define TARGET_SECTION_TYPE_FLAGS spu_section_type_flags\n \n+#undef TARGET_ASM_SELECT_SECTION\n+#define TARGET_ASM_SELECT_SECTION  spu_select_section\n+\n+#undef TARGET_ASM_UNIQUE_SECTION\n+#define TARGET_ASM_UNIQUE_SECTION  spu_unique_section\n+\n #undef TARGET_LEGITIMATE_ADDRESS_P\n #define TARGET_LEGITIMATE_ADDRESS_P spu_legitimate_address_p\n \n@@ -3613,6 +3661,29 @@ exp2_immediate_p (rtx op, enum machine_mode mode, int low, int high)\n   return FALSE;\n }\n \n+/* Return true if X is a SYMBOL_REF to an __ea qualified variable.  */\n+\n+static int\n+ea_symbol_ref (rtx *px, void *data ATTRIBUTE_UNUSED)\n+{\n+  rtx x = *px;\n+  tree decl;\n+\n+  if (GET_CODE (x) == CONST && GET_CODE (XEXP (x, 0)) == PLUS)\n+    {\n+      rtx plus = XEXP (x, 0);\n+      rtx op0 = XEXP (plus, 0);\n+      rtx op1 = XEXP (plus, 1);\n+      if (GET_CODE (op1) == CONST_INT)\n+\tx = op0;\n+    }\n+\n+  return (GET_CODE (x) == SYMBOL_REF\n+ \t  && (decl = SYMBOL_REF_DECL (x)) != 0\n+ \t  && TREE_CODE (decl) == VAR_DECL\n+ \t  && TYPE_ADDR_SPACE (TREE_TYPE (decl)));\n+}\n+\n /* We accept:\n    - any 32-bit constant (SImode, SFmode)\n    - any constant that can be generated with fsmbi (any mode)\n@@ -3624,6 +3695,12 @@ spu_legitimate_constant_p (rtx x)\n {\n   if (GET_CODE (x) == HIGH)\n     x = XEXP (x, 0);\n+\n+  /* Reject any __ea qualified reference.  These can't appear in\n+     instructions but must be forced to the constant pool.  */\n+  if (for_each_rtx (&x, ea_symbol_ref, 0))\n+    return 0;\n+\n   /* V4SI with all identical symbols is valid. */\n   if (!flag_pic\n       && GET_MODE (x) == V4SImode\n@@ -3662,8 +3739,14 @@ spu_legitimate_address_p (enum machine_mode mode,\n   switch (GET_CODE (x))\n     {\n     case LABEL_REF:\n+      return !TARGET_LARGE_MEM;\n+\n     case SYMBOL_REF:\n     case CONST:\n+      /* Keep __ea references until reload so that spu_expand_mov can see them\n+\t in MEMs.  */\n+      if (ea_symbol_ref (&x, 0))\n+\treturn !reload_in_progress && !reload_completed;\n       return !TARGET_LARGE_MEM;\n \n     case CONST_INT:\n@@ -3707,6 +3790,20 @@ spu_legitimate_address_p (enum machine_mode mode,\n   return FALSE;\n }\n \n+/* Like spu_legitimate_address_p, except with named addresses.  */\n+static bool\n+spu_addr_space_legitimate_address_p (enum machine_mode mode, rtx x,\n+\t\t\t\t     bool reg_ok_strict, addr_space_t as)\n+{\n+  if (as == ADDR_SPACE_EA)\n+    return (REG_P (x) && (GET_MODE (x) == EAmode));\n+\n+  else if (as != ADDR_SPACE_GENERIC)\n+    gcc_unreachable ();\n+\n+  return spu_legitimate_address_p (mode, x, reg_ok_strict);\n+}\n+\n /* When the address is reg + const_int, force the const_int into a\n    register.  */\n rtx\n@@ -3738,6 +3835,17 @@ spu_legitimize_address (rtx x, rtx oldx ATTRIBUTE_UNUSED,\n   return x;\n }\n \n+/* Like spu_legitimate_address, except with named address support.  */\n+static rtx\n+spu_addr_space_legitimize_address (rtx x, rtx oldx, enum machine_mode mode,\n+\t\t\t\t   addr_space_t as)\n+{\n+  if (as != ADDR_SPACE_GENERIC)\n+    return x;\n+\n+  return spu_legitimize_address (x, oldx, mode);\n+}\n+\n /* Handle an attribute requiring a FUNCTION_DECL; arguments as in\n    struct attribute_spec.handler.  */\n static tree\n@@ -4241,6 +4349,233 @@ address_needs_split (rtx mem)\n   return 0;\n }\n \n+static GTY(()) rtx cache_fetch;\t\t  /* __cache_fetch function */\n+static GTY(()) rtx cache_fetch_dirty;\t  /* __cache_fetch_dirty function */\n+static alias_set_type ea_alias_set = -1;  /* alias set for __ea memory */\n+\n+/* MEM is known to be an __ea qualified memory access.  Emit a call to\n+   fetch the ppu memory to local store, and return its address in local\n+   store.  */\n+\n+static void\n+ea_load_store (rtx mem, bool is_store, rtx ea_addr, rtx data_addr)\n+{\n+  if (is_store)\n+    {\n+      rtx ndirty = GEN_INT (GET_MODE_SIZE (GET_MODE (mem)));\n+      if (!cache_fetch_dirty)\n+\tcache_fetch_dirty = init_one_libfunc (\"__cache_fetch_dirty\");\n+      emit_library_call_value (cache_fetch_dirty, data_addr, LCT_NORMAL, Pmode,\n+\t\t\t       2, ea_addr, EAmode, ndirty, SImode);\n+    }\n+  else\n+    {\n+      if (!cache_fetch)\n+\tcache_fetch = init_one_libfunc (\"__cache_fetch\");\n+      emit_library_call_value (cache_fetch, data_addr, LCT_NORMAL, Pmode,\n+\t\t\t       1, ea_addr, EAmode);\n+    }\n+}\n+\n+/* Like ea_load_store, but do the cache tag comparison and, for stores,\n+   dirty bit marking, inline.\n+\n+   The cache control data structure is an array of\n+\n+   struct __cache_tag_array\n+     {\n+        unsigned int tag_lo[4];\n+        unsigned int tag_hi[4];\n+        void *data_pointer[4];\n+        int reserved[4];\n+        vector unsigned short dirty_bits[4];\n+     }  */\n+\n+static void\n+ea_load_store_inline (rtx mem, bool is_store, rtx ea_addr, rtx data_addr)\n+{\n+  rtx ea_addr_si;\n+  HOST_WIDE_INT v;\n+  rtx tag_size_sym = gen_rtx_SYMBOL_REF (Pmode, \"__cache_tag_array_size\");\n+  rtx tag_arr_sym = gen_rtx_SYMBOL_REF (Pmode, \"__cache_tag_array\");\n+  rtx index_mask = gen_reg_rtx (SImode);\n+  rtx tag_arr = gen_reg_rtx (Pmode);\n+  rtx splat_mask = gen_reg_rtx (TImode);\n+  rtx splat = gen_reg_rtx (V4SImode);\n+  rtx splat_hi = NULL_RTX;\n+  rtx tag_index = gen_reg_rtx (Pmode);\n+  rtx block_off = gen_reg_rtx (SImode);\n+  rtx tag_addr = gen_reg_rtx (Pmode);\n+  rtx tag = gen_reg_rtx (V4SImode);\n+  rtx cache_tag = gen_reg_rtx (V4SImode);\n+  rtx cache_tag_hi = NULL_RTX;\n+  rtx cache_ptrs = gen_reg_rtx (TImode);\n+  rtx cache_ptrs_si = gen_reg_rtx (SImode);\n+  rtx tag_equal = gen_reg_rtx (V4SImode);\n+  rtx tag_equal_hi = NULL_RTX;\n+  rtx tag_eq_pack = gen_reg_rtx (V4SImode);\n+  rtx tag_eq_pack_si = gen_reg_rtx (SImode);\n+  rtx eq_index = gen_reg_rtx (SImode);\n+  rtx bcomp, hit_label, hit_ref, cont_label, insn;\n+\n+  if (spu_ea_model != 32)\n+    {\n+      splat_hi = gen_reg_rtx (V4SImode);\n+      cache_tag_hi = gen_reg_rtx (V4SImode);\n+      tag_equal_hi = gen_reg_rtx (V4SImode);\n+    }\n+\n+  emit_move_insn (index_mask, plus_constant (tag_size_sym, -128));\n+  emit_move_insn (tag_arr, tag_arr_sym);\n+  v = 0x0001020300010203LL;\n+  emit_move_insn (splat_mask, immed_double_const (v, v, TImode));\n+  ea_addr_si = ea_addr;\n+  if (spu_ea_model != 32)\n+    ea_addr_si = convert_to_mode (SImode, ea_addr, 1);\n+\n+  /* tag_index = ea_addr & (tag_array_size - 128)  */\n+  emit_insn (gen_andsi3 (tag_index, ea_addr_si, index_mask));\n+\n+  /* splat ea_addr to all 4 slots.  */\n+  emit_insn (gen_shufb (splat, ea_addr_si, ea_addr_si, splat_mask));\n+  /* Similarly for high 32 bits of ea_addr.  */\n+  if (spu_ea_model != 32)\n+    emit_insn (gen_shufb (splat_hi, ea_addr, ea_addr, splat_mask));\n+\n+  /* block_off = ea_addr & 127  */\n+  emit_insn (gen_andsi3 (block_off, ea_addr_si, spu_const (SImode, 127)));\n+\n+  /* tag_addr = tag_arr + tag_index  */\n+  emit_insn (gen_addsi3 (tag_addr, tag_arr, tag_index));\n+\n+  /* Read cache tags.  */\n+  emit_move_insn (cache_tag, gen_rtx_MEM (V4SImode, tag_addr));\n+  if (spu_ea_model != 32)\n+    emit_move_insn (cache_tag_hi, gen_rtx_MEM (V4SImode,\n+\t\t\t\t\t       plus_constant (tag_addr, 16)));\n+\n+  /* tag = ea_addr & -128  */\n+  emit_insn (gen_andv4si3 (tag, splat, spu_const (V4SImode, -128)));\n+\n+  /* Read all four cache data pointers.  */\n+  emit_move_insn (cache_ptrs, gen_rtx_MEM (TImode,\n+\t\t\t\t\t   plus_constant (tag_addr, 32)));\n+\n+  /* Compare tags.  */\n+  emit_insn (gen_ceq_v4si (tag_equal, tag, cache_tag));\n+  if (spu_ea_model != 32)\n+    {\n+      emit_insn (gen_ceq_v4si (tag_equal_hi, splat_hi, cache_tag_hi));\n+      emit_insn (gen_andv4si3 (tag_equal, tag_equal, tag_equal_hi));\n+    }\n+\n+  /* At most one of the tags compare equal, so tag_equal has one\n+     32-bit slot set to all 1's, with the other slots all zero.\n+     gbb picks off low bit from each byte in the 128-bit registers,\n+     so tag_eq_pack is one of 0xf000, 0x0f00, 0x00f0, 0x000f, assuming\n+     we have a hit.  */\n+  emit_insn (gen_spu_gbb (tag_eq_pack, spu_gen_subreg (V16QImode, tag_equal)));\n+  emit_insn (gen_spu_convert (tag_eq_pack_si, tag_eq_pack));\n+\n+  /* So counting leading zeros will set eq_index to 16, 20, 24 or 28.  */\n+  emit_insn (gen_clzsi2 (eq_index, tag_eq_pack_si));\n+\n+  /* Allowing us to rotate the corresponding cache data pointer to slot0.\n+     (rotating eq_index mod 16 bytes).  */\n+  emit_insn (gen_rotqby_ti (cache_ptrs, cache_ptrs, eq_index));\n+  emit_insn (gen_spu_convert (cache_ptrs_si, cache_ptrs));\n+\n+  /* Add block offset to form final data address.  */\n+  emit_insn (gen_addsi3 (data_addr, cache_ptrs_si, block_off));\n+\n+  /* Check that we did hit.  */\n+  hit_label = gen_label_rtx ();\n+  hit_ref = gen_rtx_LABEL_REF (VOIDmode, hit_label);\n+  bcomp = gen_rtx_NE (SImode, tag_eq_pack_si, const0_rtx);\n+  insn = emit_jump_insn (gen_rtx_SET (VOIDmode, pc_rtx,\n+\t\t\t\t      gen_rtx_IF_THEN_ELSE (VOIDmode, bcomp,\n+\t\t\t\t\t\t\t    hit_ref, pc_rtx)));\n+  /* Say that this branch is very likely to happen.  */\n+  v = REG_BR_PROB_BASE - REG_BR_PROB_BASE / 100 - 1;\n+  REG_NOTES (insn)\n+    = gen_rtx_EXPR_LIST (REG_BR_PROB, GEN_INT (v), REG_NOTES (insn));\n+\n+  ea_load_store (mem, is_store, ea_addr, data_addr);\n+  cont_label = gen_label_rtx ();\n+  emit_jump_insn (gen_jump (cont_label));\n+  emit_barrier ();\n+\n+  emit_label (hit_label);\n+\n+  if (is_store)\n+    {\n+      HOST_WIDE_INT v_hi;\n+      rtx dirty_bits = gen_reg_rtx (TImode);\n+      rtx dirty_off = gen_reg_rtx (SImode);\n+      rtx dirty_128 = gen_reg_rtx (TImode);\n+      rtx neg_block_off = gen_reg_rtx (SImode);\n+\n+      /* Set up mask with one dirty bit per byte of the mem we are\n+\t writing, starting from top bit.  */\n+      v_hi = v = -1;\n+      v <<= (128 - GET_MODE_SIZE (GET_MODE (mem))) & 63;\n+      if ((128 - GET_MODE_SIZE (GET_MODE (mem))) >= 64)\n+\t{\n+\t  v_hi = v;\n+\t  v = 0;\n+\t}\n+      emit_move_insn (dirty_bits, immed_double_const (v, v_hi, TImode));\n+\n+      /* Form index into cache dirty_bits.  eq_index is one of\n+\t 0x10, 0x14, 0x18 or 0x1c.  Multiplying by 4 gives us\n+\t 0x40, 0x50, 0x60 or 0x70 which just happens to be the\n+\t offset to each of the four dirty_bits elements.  */\n+      emit_insn (gen_ashlsi3 (dirty_off, eq_index, spu_const (SImode, 2)));\n+\n+      emit_insn (gen_spu_lqx (dirty_128, tag_addr, dirty_off));\n+\n+      /* Rotate bit mask to proper bit.  */\n+      emit_insn (gen_negsi2 (neg_block_off, block_off));\n+      emit_insn (gen_rotqbybi_ti (dirty_bits, dirty_bits, neg_block_off));\n+      emit_insn (gen_rotqbi_ti (dirty_bits, dirty_bits, neg_block_off));\n+\n+      /* Or in the new dirty bits.  */\n+      emit_insn (gen_iorti3 (dirty_128, dirty_bits, dirty_128));\n+\n+      /* Store.  */\n+      emit_insn (gen_spu_stqx (dirty_128, tag_addr, dirty_off));\n+    }\n+\n+  emit_label (cont_label);\n+}\n+\n+static rtx\n+expand_ea_mem (rtx mem, bool is_store)\n+{\n+  rtx ea_addr;\n+  rtx data_addr = gen_reg_rtx (Pmode);\n+  rtx new_mem;\n+\n+  ea_addr = force_reg (EAmode, XEXP (mem, 0));\n+  if (optimize_size || optimize == 0)\n+    ea_load_store (mem, is_store, ea_addr, data_addr);\n+  else\n+    ea_load_store_inline (mem, is_store, ea_addr, data_addr);\n+\n+  if (ea_alias_set == -1)\n+    ea_alias_set = new_alias_set ();\n+\n+  /* We generate a new MEM RTX to refer to the copy of the data\n+     in the cache.  We do not copy memory attributes (except the\n+     alignment) from the original MEM, as they may no longer apply\n+     to the cache copy.  */\n+  new_mem = gen_rtx_MEM (GET_MODE (mem), data_addr);\n+  set_mem_alias_set (new_mem, ea_alias_set);\n+  set_mem_align (new_mem, MIN (MEM_ALIGN (mem), 128 * 8));\n+\n+  return new_mem;\n+}\n+\n int\n spu_expand_mov (rtx * ops, enum machine_mode mode)\n {\n@@ -4298,9 +4633,17 @@ spu_expand_mov (rtx * ops, enum machine_mode mode)\n \t}\n     }\n   if (MEM_P (ops[0]))\n-    return spu_split_store (ops);\n+    {\n+      if (MEM_ADDR_SPACE (ops[0]))\n+\tops[0] = expand_ea_mem (ops[0], true);\n+      return spu_split_store (ops);\n+    }\n   if (MEM_P (ops[1]))\n-    return spu_split_load (ops);\n+    {\n+      if (MEM_ADDR_SPACE (ops[1]))\n+\tops[1] = expand_ea_mem (ops[1], false);\n+      return spu_split_load (ops);\n+    }\n \n   return 0;\n }\n@@ -6442,6 +6785,113 @@ spu_builtin_vec_perm (tree type, tree *mask_element_type)\n   return d->fndecl;\n }\n \n+/* Return the appropriate mode for a named address pointer.  */\n+static enum machine_mode\n+spu_addr_space_pointer_mode (addr_space_t addrspace)\n+{\n+  switch (addrspace)\n+    {\n+    case ADDR_SPACE_GENERIC:\n+      return ptr_mode;\n+    case ADDR_SPACE_EA:\n+      return EAmode;\n+    default:\n+      gcc_unreachable ();\n+    }\n+}\n+\n+/* Return the appropriate mode for a named address address.  */\n+static enum machine_mode\n+spu_addr_space_address_mode (addr_space_t addrspace)\n+{\n+  switch (addrspace)\n+    {\n+    case ADDR_SPACE_GENERIC:\n+      return Pmode;\n+    case ADDR_SPACE_EA:\n+      return EAmode;\n+    default:\n+      gcc_unreachable ();\n+    }\n+}\n+\n+/* Determine if one named address space is a subset of another.  */\n+\n+static bool\n+spu_addr_space_subset_p (addr_space_t subset, addr_space_t superset)\n+{\n+  gcc_assert (subset == ADDR_SPACE_GENERIC || subset == ADDR_SPACE_EA);\n+  gcc_assert (superset == ADDR_SPACE_GENERIC || superset == ADDR_SPACE_EA);\n+\n+  if (subset == superset)\n+    return true;\n+\n+  /* If we have -mno-address-space-conversion, treat __ea and generic as not\n+     being subsets but instead as disjoint address spaces.  */\n+  else if (!TARGET_ADDRESS_SPACE_CONVERSION)\n+    return false;\n+\n+  else\n+    return (subset == ADDR_SPACE_GENERIC && superset == ADDR_SPACE_EA);\n+}\n+\n+/* Convert from one address space to another.  */\n+static rtx\n+spu_addr_space_convert (rtx op, tree from_type, tree to_type)\n+{\n+  addr_space_t from_as = TYPE_ADDR_SPACE (TREE_TYPE (from_type));\n+  addr_space_t to_as = TYPE_ADDR_SPACE (TREE_TYPE (to_type));\n+\n+  gcc_assert (from_as == ADDR_SPACE_GENERIC || from_as == ADDR_SPACE_EA);\n+  gcc_assert (to_as == ADDR_SPACE_GENERIC || to_as == ADDR_SPACE_EA);\n+\n+  if (to_as == ADDR_SPACE_GENERIC && from_as == ADDR_SPACE_EA)\n+    {\n+      rtx result, ls;\n+\n+      ls = gen_const_mem (DImode,\n+\t\t\t  gen_rtx_SYMBOL_REF (Pmode, \"__ea_local_store\"));\n+      set_mem_align (ls, 128);\n+\n+      result = gen_reg_rtx (Pmode);\n+      ls = force_reg (Pmode, convert_modes (Pmode, DImode, ls, 1));\n+      op = force_reg (Pmode, convert_modes (Pmode, EAmode, op, 1));\n+      ls = emit_conditional_move (ls, NE, op, const0_rtx, Pmode,\n+\t\t\t\t\t  ls, const0_rtx, Pmode, 1);\n+\n+      emit_insn (gen_subsi3 (result, op, ls));\n+\n+      return result;\n+    }\n+\n+  else if (to_as == ADDR_SPACE_EA && from_as == ADDR_SPACE_GENERIC)\n+    {\n+      rtx result, ls;\n+\n+      ls = gen_const_mem (DImode,\n+\t\t\t  gen_rtx_SYMBOL_REF (Pmode, \"__ea_local_store\"));\n+      set_mem_align (ls, 128);\n+\n+      result = gen_reg_rtx (EAmode);\n+      ls = force_reg (EAmode, convert_modes (EAmode, DImode, ls, 1));\n+      op = force_reg (Pmode, op);\n+      ls = emit_conditional_move (ls, NE, op, const0_rtx, Pmode,\n+\t\t\t\t\t  ls, const0_rtx, EAmode, 1);\n+      op = force_reg (EAmode, convert_modes (EAmode, Pmode, op, 1));\n+\n+      if (EAmode == SImode)\n+\temit_insn (gen_addsi3 (result, op, ls));\n+      else\n+\temit_insn (gen_adddi3 (result, op, ls));\n+\n+      return result;\n+    }\n+\n+  else\n+    gcc_unreachable ();\n+}\n+\n+\n /* Count the total number of instructions in each pipe and return the\n    maximum, which is used as the Minimum Iteration Interval (MII)\n    in the modulo scheduler.  get_pipe() will return -2, -1, 0, or 1.\n@@ -6534,9 +6984,46 @@ spu_section_type_flags (tree decl, const char *name, int reloc)\n   /* .toe needs to have type @nobits.  */\n   if (strcmp (name, \".toe\") == 0)\n     return SECTION_BSS;\n+  /* Don't load _ea into the current address space.  */\n+  if (strcmp (name, \"._ea\") == 0)\n+    return SECTION_WRITE | SECTION_DEBUG;\n   return default_section_type_flags (decl, name, reloc);\n }\n \n+/* Implement targetm.select_section.  */\n+static section *\n+spu_select_section (tree decl, int reloc, unsigned HOST_WIDE_INT align)\n+{\n+  /* Variables and constants defined in the __ea address space\n+     go into a special section named \"._ea\".  */\n+  if (TREE_TYPE (decl) != error_mark_node\n+      && TYPE_ADDR_SPACE (TREE_TYPE (decl)) == ADDR_SPACE_EA)\n+    {\n+      /* We might get called with string constants, but get_named_section\n+\t doesn't like them as they are not DECLs.  Also, we need to set\n+\t flags in that case.  */\n+      if (!DECL_P (decl))\n+\treturn get_section (\"._ea\", SECTION_WRITE | SECTION_DEBUG, NULL);\n+\n+      return get_named_section (decl, \"._ea\", reloc);\n+    }\n+\n+  return default_elf_select_section (decl, reloc, align);\n+}\n+\n+/* Implement targetm.unique_section.  */\n+static void\n+spu_unique_section (tree decl, int reloc)\n+{\n+  /* We don't support unique section names in the __ea address\n+     space for now.  */\n+  if (TREE_TYPE (decl) != error_mark_node\n+      && TYPE_ADDR_SPACE (TREE_TYPE (decl)) != 0)\n+    return;\n+\n+  default_unique_section (decl, reloc);\n+}\n+\n /* Generate a constant or register which contains 2^SCALE.  We assume\n    the result is valid for MODE.  Currently, MODE must be V4SFmode and\n    SCALE must be SImode. */"}, {"sha": "369e6d76e9d0873151ca585faffa9a63aa3d9eb9", "filename": "gcc/config/spu/spu.h", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu.h?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -51,7 +51,7 @@ extern GTY(()) int spu_tune;\n /* Default target_flags if no switches specified.  */\n #ifndef TARGET_DEFAULT\n #define TARGET_DEFAULT (MASK_ERROR_RELOC | MASK_SAFE_DMA | MASK_BRANCH_HINTS \\\n-\t\t\t| MASK_SAFE_HINTS)\n+\t\t\t| MASK_SAFE_HINTS | MASK_ADDRESS_SPACE_CONVERSION)\n #endif\n \n \f\n@@ -469,6 +469,17 @@ targetm.resolve_overloaded_builtin = spu_resolve_overloaded_builtin;\t\\\n #define ASM_OUTPUT_LABELREF(FILE, NAME) \\\n   asm_fprintf (FILE, \"%U%s\", default_strip_name_encoding (NAME))\n \n+#define ASM_OUTPUT_SYMBOL_REF(FILE, X) \\\n+  do\t\t\t\t\t\t\t\\\n+    {\t\t\t\t\t\t\t\\\n+      tree decl;\t\t\t\t\t\\\n+      assemble_name (FILE, XSTR ((X), 0));\t\t\\\n+      if ((decl = SYMBOL_REF_DECL ((X))) != 0\t\t\\\n+\t  && TREE_CODE (decl) == VAR_DECL\t\t\\\n+\t  && TYPE_ADDR_SPACE (TREE_TYPE (decl)))\t\\\n+\tfputs (\"@ppu\", FILE);\t\t\t\t\\\n+    } while (0)\n+\n \f\n /* Instruction Output */\n #define REGISTER_NAMES \\\n@@ -590,6 +601,13 @@ targetm.resolve_overloaded_builtin = spu_resolve_overloaded_builtin;\t\\\n   } while (0)\n \n \n+/* Address spaces.  */\n+#define ADDR_SPACE_EA\t1\n+\n+/* Named address space keywords.  */\n+#define TARGET_ADDR_SPACE_KEYWORDS ADDR_SPACE_KEYWORD (\"__ea\", ADDR_SPACE_EA)\n+\n+\n /* Builtins.  */\n \n enum spu_builtin_type"}, {"sha": "4ad7128de513adfc39047d2793c0637690b67d97", "filename": "gcc/config/spu/spu.opt", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu.opt?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -82,3 +82,24 @@ Generate code for given CPU\n mtune=\n Target RejectNegative Joined Var(spu_tune_string)\n Schedule code for given CPU\n+\n+mea32\n+Target Report RejectNegative Var(spu_ea_model,32) Init(32)\n+Access variables in 32-bit PPU objects (default)\n+\n+mea64\n+Target Report RejectNegative Var(spu_ea_model,64) VarExists\n+Access variables in 64-bit PPU objects\n+\n+maddress-space-conversion\n+Target Report Mask(ADDRESS_SPACE_CONVERSION)\n+Allow conversions between __ea and generic pointers (default)\n+\n+mcache-size=\n+Target Report RejectNegative Joined UInteger\n+Size (in KB) of software data cache\n+\n+matomic-updates\n+Target Report\n+Atomically write back software data cache lines (default)\n+"}, {"sha": "66a679be5a0b19163526f6824dd2d94cc0117cd0", "filename": "gcc/config/spu/spu_cache.h", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu_cache.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Fspu_cache.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu_cache.h?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -0,0 +1,39 @@\n+/* Copyright (C) 2008, 2009 Free Software Foundation, Inc.\n+\n+   This file is free software; you can redistribute it and/or modify it under\n+   the terms of the GNU General Public License as published by the Free\n+   Software Foundation; either version 3 of the License, or (at your option)\n+   any later version.\n+\n+   This file is distributed in the hope that it will be useful, but WITHOUT\n+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+   FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+   for more details.\n+\n+   Under Section 7 of GPL version 3, you are granted additional\n+   permissions described in the GCC Runtime Library Exception, version\n+   3.1, as published by the Free Software Foundation.\n+\n+   You should have received a copy of the GNU General Public License and\n+   a copy of the GCC Runtime Library Exception along with this program;\n+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+#ifndef _SPU_CACHE_H\n+#define _SPU_CACHE_H\n+\n+void *__cache_fetch_dirty (__ea void *ea, int n_bytes_dirty);\n+void *__cache_fetch (__ea void *ea);\n+void __cache_evict (__ea void *ea);\n+void __cache_flush (void);\n+void __cache_touch (__ea void *ea);\n+\n+#define cache_fetch_dirty(_ea, _n_bytes_dirty) \\\n+     __cache_fetch_dirty(_ea, _n_bytes_dirty)\n+\n+#define cache_fetch(_ea) __cache_fetch(_ea)\n+#define cache_touch(_ea) __cache_touch(_ea)\n+#define cache_evict(_ea) __cache_evict(_ea)\n+#define cache_flush() __cache_flush()\n+\n+#endif"}, {"sha": "a54ede9fa253ba38701e3ace3e81e303a6ced941", "filename": "gcc/config/spu/t-spu-elf", "status": "modified", "additions": 29, "deletions": 4, "changes": 33, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Ft-spu-elf", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fconfig%2Fspu%2Ft-spu-elf", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Ft-spu-elf?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -66,14 +66,39 @@ fp-bit.c: $(srcdir)/config/fp-bit.c $(srcdir)/config/spu/t-spu-elf\n # Don't let CTOR_LIST end up in sdata section.\n CRTSTUFF_T_CFLAGS =\n \n-#MULTILIB_OPTIONS=mlarge-mem/mtest-abi\n-#MULTILIB_DIRNAMES=large-mem test-abi\n-#MULTILIB_MATCHES=\n+# Multi-lib support.\n+MULTILIB_OPTIONS=mea64\n \n # Neither gcc or newlib seem to have a standard way to generate multiple\n # crt*.o files.  So we don't use the standard crt0.o name anymore.\n \n-EXTRA_MULTILIB_PARTS = crtbegin.o crtend.o\n+EXTRA_MULTILIB_PARTS = crtbegin.o crtend.o libgcc_cachemgr.a libgcc_cachemgr_nonatomic.a \\\n+\tlibgcc_cache8k.a libgcc_cache16k.a libgcc_cache32k.a libgcc_cache64k.a libgcc_cache128k.a\n+\n+$(T)cachemgr.o: $(srcdir)/config/spu/cachemgr.c\n+\t$(GCC_FOR_TARGET) $(LIBGCC2_CFLAGS) $(MULTILIB_CFLAGS) -c $< -o $@\n+\n+# Specialised rule to add a -D flag.\n+$(T)cachemgr_nonatomic.o: $(srcdir)/config/spu/cachemgr.c\n+\t$(GCC_FOR_TARGET) $(LIBGCC2_CFLAGS) $(MULTILIB_CFLAGS) -DNONATOMIC -c $< -o $@\n+\n+$(T)libgcc_%.a: $(T)%.o\n+\t$(AR_FOR_TARGET) -rcs $@ $<\n+\n+$(T)cache8k.o: $(srcdir)/config/spu/cache.S\n+\t$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -D__CACHE_SIZE__=8 -o $@ -c $<\n+\n+$(T)cache16k.o: $(srcdir)/config/spu/cache.S\n+\t$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -D__CACHE_SIZE__=16 -o $@ -c $<\n+\n+$(T)cache32k.o: $(srcdir)/config/spu/cache.S\n+\t$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -D__CACHE_SIZE__=32 -o $@ -c $<\n+\n+$(T)cache64k.o: $(srcdir)/config/spu/cache.S\n+\t$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -D__CACHE_SIZE__=64 -o $@ -c $<\n+\n+$(T)cache128k.o: $(srcdir)/config/spu/cache.S\n+\t$(GCC_FOR_TARGET) $(MULTILIB_CFLAGS) -D__CACHE_SIZE__=128 -o $@ -c $<\n \n LIBGCC = stmp-multilib\n INSTALL_LIBGCC = install-multilib"}, {"sha": "4a9ffbfaf7f04caaa15db5322ad66b8e28f81ac1", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 45, "deletions": 1, "changes": 46, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/299456f3c2978c03a18fc419dc7e29b65e5844f0/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=299456f3c2978c03a18fc419dc7e29b65e5844f0", "patch": "@@ -846,7 +846,11 @@ See RS/6000 and PowerPC Options.\n -msafe-dma -munsafe-dma @gol\n -mbranch-hints @gol\n -msmall-mem -mlarge-mem -mstdmain @gol\n--mfixed-range=@var{register-range}}\n+-mfixed-range=@var{register-range} @gol\n+-mea32 -mea64 @gol\n+-maddress-space-conversion -mno-address-space-conversion @gol\n+-mcache-size=@var{cache-size} @gol\n+-matomic-updates -mno-atomic-updates}\n \n @emph{System V Options}\n @gccoptlist{-Qy  -Qn  -YP,@var{paths}  -Ym,@var{dir}}\n@@ -16358,6 +16362,46 @@ useful when compiling kernel code.  A register range is specified as\n two registers separated by a dash.  Multiple register ranges can be\n specified separated by a comma.\n \n+@item -mea32\n+@itemx -mea64\n+@opindex mea32\n+@opindex mea64\n+Compile code assuming that pointers to the PPU address space accessed\n+via the @code{__ea} named address space qualifier are either 32 or 64\n+bits wide.  The default is 32 bits.  As this is an ABI changing option,\n+all object code in an executable must be compiled with the same setting.\n+\n+@item -maddress-space-conversion\n+@itemx -mno-address-space-conversion\n+@opindex maddress-space-conversion\n+@opindex mno-address-space-conversion\n+Allow/disallow treating the @code{__ea} address space as superset\n+of the generic address space.  This enables explicit type casts\n+between @code{__ea} and generic pointer as well as implicit\n+conversions of generic pointers to @code{__ea} pointers.  The\n+default is to allow address space pointer conversions.\n+\n+@item -mcache-size=@var{cache-size}\n+@opindex mcache-size\n+This option controls the version of libgcc that the compiler links to an\n+executable and selects a software-managed cache for accessing variables\n+in the @code{__ea} address space with a particular cache size.  Possible\n+options for @var{cache-size} are @samp{8}, @samp{16}, @samp{32}, @samp{64}\n+and @samp{128}.  The default cache size is 64KB.\n+\n+@item -matomic-updates\n+@itemx -mno-atomic-updates\n+@opindex matomic-updates\n+@opindex mno-atomic-updates\n+This option controls the version of libgcc that the compiler links to an\n+executable and selects whether atomic updates to the software-managed\n+cache of PPU-side variables are used.  If you use atomic updates, changes\n+to a PPU variable from SPU code using the @code{__ea} named address space\n+qualifier will not interfere with changes to other PPU variables residing\n+in the same cache line from PPU code.  If you do not use atomic updates,\n+such interference may occur; however, writing back cache lines will be\n+more efficient.  The default behavior is to use atomic updates.\n+\n @item -mdual-nops\n @itemx -mdual-nops=@var{n}\n @opindex mdual-nops"}]}