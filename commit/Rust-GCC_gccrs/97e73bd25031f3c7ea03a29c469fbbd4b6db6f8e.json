{"sha": "97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OTdlNzNiZDI1MDMxZjNjN2VhMDNhMjljNDY5ZmJiZDRiNmRiNmY4ZQ==", "commit": {"author": {"name": "Richard Henderson", "email": "rth@redhat.com", "date": "2004-06-29T16:25:28Z"}, "committer": {"name": "Richard Henderson", "email": "rth@gcc.gnu.org", "date": "2004-06-29T16:25:28Z"}, "message": "tree-sra.c: Rewrite from scratch.\n\n        * tree-sra.c: Rewrite from scratch.  Handle nested aggregates.\n        * gcc.dg/tree-ssa/20040430-1.c: Expect zero if's.\n\nFrom-SVN: r83858", "tree": {"sha": "c9c561efa532402bf1acaf7f1d82d09d1a0490d3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/c9c561efa532402bf1acaf7f1d82d09d1a0490d3"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e/comments", "author": null, "committer": null, "parents": [{"sha": "ba60e4754adc4181784207396de5abbd349d894f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ba60e4754adc4181784207396de5abbd349d894f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ba60e4754adc4181784207396de5abbd349d894f"}], "stats": {"total": 2515, "additions": 1641, "deletions": 874}, "files": [{"sha": "a87da45d8918ce0d8b34c81b0b03a9e09458b5e2", "filename": "gcc/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e", "patch": "@@ -1,3 +1,7 @@\n+2004-06-29  Richard Henderson  <rth@redhat.com>\n+\n+\t* tree-sra.c: Rewrite from scratch.  Handle nested aggregates.\n+\n 2004-06-29  Nathan Sidwell  <nathan@codesourcery.com>\n \n \t* vec.h (VEC_T_safe_push, VEC_T_safe_insert): Tweak for when"}, {"sha": "e60bb1e5a53d026bea7d7d6379ed6406896b0284", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e", "patch": "@@ -1,3 +1,7 @@\n+2004-06-29  Richard Henderson  <rth@redhat.com>\n+\n+\t* gcc.dg/tree-ssa/20040430-1.c: Expect zero if's.\n+\n 2004-06-29  Paul Brook  <paul@codesourcery.com>\n \n \t* g++.old-deja/g++.abi/arraynew.C: Handle ARM EABI cookies."}, {"sha": "4fbd950f3dd9d7d2dbabe5aa0db08e1d070c7056", "filename": "gcc/testsuite/gcc.dg/tree-ssa/20040430-1.c", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2F20040430-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2F20040430-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2F20040430-1.c?ref=97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e", "patch": "@@ -1,7 +1,7 @@\n /* PR middle-end/14470.  Similar to\n    gcc.c-torture/execute/20040313-1.c, but with a compile time test to\n-   make sure the second if() is removed.  We should actually get rid\n-   of the first if() too, but we're not that smart yet.  */\n+   make sure the second if() is removed.  */\n+/* Update: We now remove both ifs.  Whee. */\n \n /* { dg-do run } */\n /* { dg-options \"-O2 -fdump-tree-optimized\" } */\n@@ -22,4 +22,4 @@ int main()\n   return 0;\n }\n \n-/* { dg-final { scan-tree-dump-times \"if \" 1 \"optimized\"} } */\n+/* { dg-final { scan-tree-dump-times \"if \" 0 \"optimized\"} } */"}, {"sha": "f0d6da92372ae1b666b968c6708be17b0a4697e2", "filename": "gcc/tree-sra.c", "status": "modified", "additions": 1630, "deletions": 871, "changes": 2501, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e/gcc%2Ftree-sra.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e/gcc%2Ftree-sra.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-sra.c?ref=97e73bd25031f3c7ea03a29c469fbbd4b6db6f8e", "patch": "@@ -44,26 +44,34 @@ Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n #include \"timevar.h\"\n #include \"flags.h\"\n #include \"bitmap.h\"\n+#include \"obstack.h\"\n+#include \"target.h\"\n \n \n-/* Maximum number of fields that a structure should have to be scalarized.\n-   FIXME  This limit has been arbitrarily set to 5.  Experiment to find a\n-\t  sensible setting.  */\n-#define MAX_NFIELDS_FOR_SRA\t5\n+/* This object of this pass is to replace a non-addressable aggregate with a\n+   set of independent variables.  Most of the time, all of these variables\n+   will be scalars.  But a secondary objective is to break up larger \n+   aggregates into smaller aggregates.  In the process we may find that some\n+   bits of the larger aggregate can be deleted as unreferenced.\n \n-/* Codes indicating how to copy one structure into another.  */\n-enum sra_copy_mode { SCALAR_SCALAR, FIELD_SCALAR, SCALAR_FIELD };\n+   This substitution is done globally.  More localized substitutions would\n+   be the purvey of a load-store motion pass.\n+\n+   The optimization proceeds in phases:\n+\n+     (1) Identify variables that have types that are candidates for\n+\t decomposition.\n+\n+     (2) Scan the function looking for the ways these variables are used.\n+\t In particular we're interested in the number of times a variable\n+\t (or member) is needed as a complete unit, and the number of times\n+\t a variable (or member) is copied.\n+\n+     (3) Based on the usage profile, instantiate substitution variables.\n+\n+     (4) Scan the function making replacements.\n+*/\n \n-/* Local functions.  */\n-static inline bool can_be_scalarized_p (tree);\n-static tree create_scalar_copies (tree lhs, tree rhs, enum sra_copy_mode mode);\n-static inline void scalarize_component_ref (tree, tree *tp);\n-static void scalarize_structures (void);\n-static void scalarize_stmt (block_stmt_iterator *);\n-static void scalarize_modify_expr (block_stmt_iterator *);\n-static void scalarize_call_expr (block_stmt_iterator *);\n-static void scalarize_asm_expr (block_stmt_iterator *);\n-static void scalarize_return_expr (block_stmt_iterator *);\n \n /* The set of aggregate variables that are candidates for scalarization.  */\n static bitmap sra_candidates;\n@@ -72,85 +80,68 @@ static bitmap sra_candidates;\n    beginning of the function.  */\n static bitmap needs_copy_in;\n \n-/* This structure holds the mapping between and element of an aggregate\n-   and the scalar replacement variable.  */\n+/* Sets of bit pairs that cache type decomposition and instantiation.  */\n+static bitmap sra_type_decomp_cache;\n+static bitmap sra_type_inst_cache;\n+\n+/* One of these structures is created for each candidate aggregate\n+   and each (accessed) member of such an aggregate.  */\n struct sra_elt\n {\n-  enum tree_code kind;\n-  tree base;\n-  tree field;\n-  tree replace;\n-};\n-    \n-static htab_t sra_map;\n+  /* A tree of the elements.  Used when we want to traverse everything.  */\n+  struct sra_elt *parent;\n+  struct sra_elt *children;\n+  struct sra_elt *sibling;\n \n-static hashval_t\n-sra_elt_hash (const void *x)\n-{\n-  const struct sra_elt *e = x;\n-  hashval_t h = (size_t) e->base * e->kind;\n-  if (e->kind == COMPONENT_REF)\n-    h ^= (size_t) e->field;\n-  return h;\n-}\n+  /* If this element is a root, then this is the VAR_DECL.  If this is\n+     a sub-element, this is some token used to identify the reference.\n+     In the case of COMPONENT_REF, this is the FIELD_DECL.  In the case\n+     of an ARRAY_REF, this is the (constant) index.  In the case of a\n+     complex number, this is a zero or one.  */\n+  tree element;\n \n-static int\n-sra_elt_eq (const void *x, const void *y)\n-{\n-  const struct sra_elt *a = x;\n-  const struct sra_elt *b = y;\n+  /* The type of the element.  */\n+  tree type;\n \n-  if (a->kind != b->kind)\n-    return false;\n-  if (a->base != b->base)\n-    return false;\n-  if (a->kind == COMPONENT_REF)\n-    if (a->field != b->field)\n-      return false;\n+  /* A VAR_DECL, for any sub-element we've decided to replace.  */\n+  tree replacement;\n \n-  return true;\n-}\n+  /* The number of times the element is referenced as a whole.  I.e.\n+     given \"a.b.c\", this would be incremented for C, but not for A or B.  */\n+  unsigned int n_uses;\n \n-/* Mark all the variables in V_MAY_DEF operands for STMT for renaming.\n-   This becomes necessary when we modify all of a non-scalar.  */\n+  /* The number of times the element is copied to or from another\n+     scalarizable element.  */\n+  unsigned int n_copies;\n \n-static void\n-mark_all_v_may_defs (tree stmt)\n-{\n-  v_may_def_optype v_may_defs;\n-  size_t i, n;\n+  /* True if TYPE is scalar.  */\n+  bool is_scalar;\n \n-  get_stmt_operands (stmt);\n-  v_may_defs = V_MAY_DEF_OPS (stmt_ann (stmt));\n-  n = NUM_V_MAY_DEFS (v_may_defs);\n+  /* True if we saw something about this element that prevents scalarization,\n+     such as non-constant indexing.  */\n+  bool cannot_scalarize;\n \n-  for (i = 0; i < n; i++)\n-    {\n-      tree sym = V_MAY_DEF_RESULT (v_may_defs, i);\n-      bitmap_set_bit (vars_to_rename, var_ann (sym)->uid);\n-    }\n-}\n+  /* True if we've decided that structure-to-structure assignment\n+     should happen via memcpy and not per-element.  */\n+  bool use_block_copy;\n \n-/* Mark all the variables in V_MUST_DEF operands for STMT for renaming.\n-   This becomes necessary when we modify all of a non-scalar.  */\n+  /* A flag for use with/after random access traversals.  */\n+  bool visited;\n+};\n \n-static void\n-mark_all_v_must_defs (tree stmt)\n-{\n-  v_must_def_optype v_must_defs;\n-  size_t i, n;\n+/* Random access to the child of a parent is performed by hashing.\n+   This prevents quadratic behaviour, and allows SRA to function\n+   reasonably on larger records.  */\n+static htab_t sra_map;\n \n-  get_stmt_operands (stmt);\n-  v_must_defs = V_MUST_DEF_OPS (stmt_ann (stmt));\n-  n = NUM_V_MUST_DEFS (v_must_defs);\n+/* All structures are allocated out of the following obstack.  */\n+static struct obstack sra_obstack;\n \n-  for (i = 0; i < n; i++)\n-    {\n-      tree sym = V_MUST_DEF_OP (v_must_defs, i);\n-      bitmap_set_bit (vars_to_rename, var_ann (sym)->uid);\n-    }\n-}\n+/* Debugging functions.  */\n+static void dump_sra_elt_name (FILE *, struct sra_elt *);\n+extern void debug_sra_elt_name (struct sra_elt *);\n \n+\f\n /* Return true if DECL is an SRA candidate.  */\n \n static bool\n@@ -159,157 +150,102 @@ is_sra_candidate_decl (tree decl)\n   return DECL_P (decl) && bitmap_bit_p (sra_candidates, var_ann (decl)->uid);\n }\n \n-/* Return true if EXP is of the form <ref decl>, where REF is one of the\n-   field access references we handle and DECL is an SRA candidate.   */\n+/* Return true if TYPE is a scalar type.  */\n \n static bool\n-is_sra_candidate_ref (tree exp)\n+is_sra_scalar_type (tree type)\n {\n-  switch (TREE_CODE (exp))\n-    {\n-    case COMPONENT_REF:\n-    case REALPART_EXPR:\n-    case IMAGPART_EXPR:\n-      return is_sra_candidate_decl (TREE_OPERAND (exp, 0));\n-\n-    default:\n-      break;\n-    }\n-\n-  return false;\n+  enum tree_code code = TREE_CODE (type);\n+  return (code == INTEGER_TYPE || code == REAL_TYPE || code == VECTOR_TYPE\n+\t  || code == ENUMERAL_TYPE || code == BOOLEAN_TYPE\n+\t  || code == CHAR_TYPE || code == POINTER_TYPE || code == OFFSET_TYPE\n+\t  || code == REFERENCE_TYPE);\n }\n \n-/* Return true if EXP is of the form <ref decl>, where REF is a nest of\n-   references handled by handle_components_p and DECL is an SRA candidate. \n-   *VAR_P is set to DECL.  */\n+/* Return true if TYPE can be decomposed into a set of independent variables.\n+\n+   Note that this doesn't imply that all elements of TYPE can be\n+   instantiated, just that if we decide to break up the type into\n+   separate pieces that it can be done.  */\n \n static bool\n-is_sra_candidate_complex_ref (tree exp, tree *var_p)\n+type_can_be_decomposed_p (tree type)\n {\n-  tree orig_exp = exp;\n-\n-  while (TREE_CODE (exp) == REALPART_EXPR || TREE_CODE (exp) == IMAGPART_EXPR\n-\t || handled_component_p (exp))\n-    exp = TREE_OPERAND (exp, 0);\n+  unsigned int cache = TYPE_UID (TYPE_MAIN_VARIANT (type)) * 2;\n+  tree t;\n \n-  if (orig_exp != exp && is_sra_candidate_decl (exp))\n-    {\n-      *var_p = exp;\n-      return true;\n-    }\n-\n-  return false;\n-}\n-\n-/* Return the scalar in SRA_MAP[VAR_IX][FIELD_IX].  If none exists, create\n-   a new scalar with type TYPE.  */\n+  /* Avoid searching the same type twice.  */\n+  if (bitmap_bit_p (sra_type_decomp_cache, cache+0))\n+    return true;\n+  if (bitmap_bit_p (sra_type_decomp_cache, cache+1))\n+    return false;\n \n-static tree\n-lookup_scalar (struct sra_elt *key, tree type)\n-{\n-  struct sra_elt **slot, *res;\n+  /* The type must have a definite non-zero size.  */\n+  if (TYPE_SIZE (type) == NULL || integer_zerop (TYPE_SIZE (type)))\n+    goto fail;\n \n-  slot = (struct sra_elt **) htab_find_slot (sra_map, key, INSERT);\n-  res = *slot;\n-  if (!res)\n+  /* The type must be a non-union aggregate.  */\n+  switch (TREE_CODE (type))\n     {\n-      res = xmalloc (sizeof (*res));\n-      *slot = res;\n-      *res = *key;\n-      res->replace = make_rename_temp (type, \"SR\");\n+    case RECORD_TYPE:\n+      {\n+\tbool saw_one_field = false;\n \n-      if (DECL_NAME (key->base) && !DECL_IGNORED_P (key->base))\n-\t{\n-\t  char *name = NULL;\n-\t  switch (key->kind)\n-\t    {\n-\t    case COMPONENT_REF:\n-\t      if (!DECL_NAME (key->field))\n-\t\tbreak;\n-\t      name = concat (IDENTIFIER_POINTER (DECL_NAME (key->base)),\n-\t\t\t     \"$\",\n-\t\t\t     IDENTIFIER_POINTER (DECL_NAME (key->field)),\n-\t\t\t     NULL);\n-\t      break;\n-\t    case REALPART_EXPR:\n-\t      name = concat (IDENTIFIER_POINTER (DECL_NAME (key->base)),\n-\t\t\t     \"$real\", NULL);\n-\t      break;\n-\t    case IMAGPART_EXPR:\n-\t      name = concat (IDENTIFIER_POINTER (DECL_NAME (key->base)),\n-\t\t\t     \"$imag\", NULL);\n-\t      break;\n-\t    default:\n-\t      abort ();\n-\t    }\n-\t  if (name)\n+\tfor (t = TYPE_FIELDS (type); t ; t = TREE_CHAIN (t))\n+\t  if (TREE_CODE (t) == FIELD_DECL)\n \t    {\n-\t      DECL_NAME (res->replace) = get_identifier (name);\n-\t      free (name);\n-\t    }\n-\t}\n-\n-      DECL_SOURCE_LOCATION (res->replace) = DECL_SOURCE_LOCATION (key->base);\n-      TREE_NO_WARNING (res->replace) = TREE_NO_WARNING (key->base);\n-      DECL_ARTIFICIAL (res->replace) = DECL_ARTIFICIAL (key->base);\n-    }\n-\n-  return res->replace;\n-}\n-\n-\n-/* Given a structure reference VAR.FIELD, return a scalar representing it.\n-   If no scalar is found, a new one is created and added to the SRA_MAP\n-   matrix.  */\n-\n-static tree\n-get_scalar_for_field (tree var, tree field)\n-{\n-  struct sra_elt key;\n-\n-#ifdef ENABLE_CHECKING\n-  /* Validate that FIELD actually exists in VAR's type.  */\n-  {\n-    tree f;\n-    for (f = TYPE_FIELDS (TREE_TYPE (var)); f ; f = TREE_CHAIN (f))\n-      if (f == field)\n-\tgoto found;\n-    abort ();\n-   found:;\n-  }\n-#endif\n+\t      /* Reject incorrectly represented bit fields.  */\n+\t      if (DECL_BIT_FIELD (t)\n+\t\t  && (tree_low_cst (DECL_SIZE (t), 1)\n+\t\t      != TYPE_PRECISION (TREE_TYPE (t))))\n+\t\tgoto fail;\n \n-  key.kind = COMPONENT_REF;\n-  key.base = var;\n-  key.field = field;\n+\t      saw_one_field = true;\n+\t    }\n \n-  return lookup_scalar (&key, TREE_TYPE (field));\n-}\n+\t/* Record types must have at least one field.  */\n+\tif (!saw_one_field)\n+\t  goto fail;\n+      }\n+      break;\n \n+    case ARRAY_TYPE:\n+      /* Array types must have a fixed lower and upper bound.  */\n+      t = TYPE_DOMAIN (type);\n+      if (t == NULL)\n+\tgoto fail;\n+      if (TYPE_MIN_VALUE (t) == NULL || !TREE_CONSTANT (TYPE_MIN_VALUE (t)))\n+\tgoto fail;\n+      if (TYPE_MAX_VALUE (t) == NULL || !TREE_CONSTANT (TYPE_MAX_VALUE (t)))\n+\tgoto fail;\n+      break;\n \n-/* Similarly for the parts of a complex type.  */\n+    case COMPLEX_TYPE:\n+      break;\n \n-static tree\n-get_scalar_for_complex_part (tree var, enum tree_code part)\n-{\n-  struct sra_elt key;\n+    default:\n+      goto fail;\n+    }\n \n-  key.kind = part;\n-  key.base = var;\n+  bitmap_set_bit (sra_type_decomp_cache, cache+0);\n+  return true;\n \n-  return lookup_scalar (&key, TREE_TYPE (TREE_TYPE (var)));\n+ fail:\n+  bitmap_set_bit (sra_type_decomp_cache, cache+1);\n+  return false;\n }\n \n-/* Return true if the fields of VAR can be replaced by scalar temporaries.\n-   This only happens if VAR is not call-clobbered and it contains less\n-   than MAX_NFIELDS_FOR_SRA scalar fields.  */\n+/* Return true if DECL can be decomposed into a set of independent\n+   (though not necessarily scalar) variables.  */\n \n-static inline bool\n-can_be_scalarized_p (tree var)\n+static bool\n+decl_can_be_decomposed_p (tree var)\n {\n-  tree field, type;\n-  int nfields;\n+  /* Early out for scalars.  */\n+  if (is_sra_scalar_type (TREE_TYPE (var)))\n+    return false;\n \n+  /* The variable must not be aliased.  */\n   if (!is_gimple_non_addressable (var))\n     {\n       if (dump_file && (dump_flags & TDF_DETAILS))\n@@ -321,6 +257,7 @@ can_be_scalarized_p (tree var)\n       return false;\n     }\n \n+  /* The variable must not be volatile.  */\n   if (TREE_THIS_VOLATILE (var))\n     {\n       if (dump_file && (dump_flags & TDF_DETAILS))\n@@ -332,879 +269,1701 @@ can_be_scalarized_p (tree var)\n       return false;\n     }\n \n-  /* Any COMPLEX_TYPE that has reached this point can be scalarized.  */\n-  if (TREE_CODE (TREE_TYPE (var)) == COMPLEX_TYPE)\n+  /* We must be able to decompose the variable's type.  */\n+  if (!type_can_be_decomposed_p (TREE_TYPE (var)))\n+    {\n+      if (dump_file && (dump_flags & TDF_DETAILS))\n+\t{\n+\t  fprintf (dump_file, \"Cannot scalarize variable \");\n+\t  print_generic_expr (dump_file, var, dump_flags);\n+\t  fprintf (dump_file, \" because its type cannot be decomposed\\n\");\n+\t}\n+      return false;\n+    }\n+\n+  return true;\n+}\n+\n+/* Return true if TYPE can be *completely* decomposed into scalars.  */\n+\n+static bool\n+type_can_instantiate_all_elements (tree type)\n+{\n+  if (is_sra_scalar_type (type))\n     return true;\n+  if (!type_can_be_decomposed_p (type))\n+    return false;\n \n-  type = TREE_TYPE (var);\n-  nfields = 0;\n-  for (field = TYPE_FIELDS (type); field; field = TREE_CHAIN (field))\n+  switch (TREE_CODE (type))\n     {\n-      if (TREE_CODE (field) != FIELD_DECL)\n-\tcontinue;\n+    case RECORD_TYPE:\n+      {\n+\tunsigned int cache = TYPE_UID (TYPE_MAIN_VARIANT (type)) * 2;\n+\ttree f;\n \n-      /* FIXME: We really should recurse down the type hierarchy and\n-\t scalarize the fields at the leaves.  */\n-      if (AGGREGATE_TYPE_P (TREE_TYPE (field)))\n-\t{\n-\t  if (dump_file && (dump_flags & TDF_DETAILS))\n-\t    {\n-\t      fprintf (dump_file, \"Cannot scalarize variable \");\n-\t      print_generic_expr (dump_file, var, dump_flags);\n-\t      fprintf (dump_file,\n-\t\t       \" because it contains an aggregate type field, \");\n-\t      print_generic_expr (dump_file, field, dump_flags);\n-\t      fprintf (dump_file, \"\\n\");\n-\t    }\n+\tif (bitmap_bit_p (sra_type_inst_cache, cache+0))\n+\t  return true;\n+\tif (bitmap_bit_p (sra_type_inst_cache, cache+1))\n \t  return false;\n-\t}\n \n-      /* FIXME: Similarly.  Indeed, considering that we treat complex\n-\t as an aggregate, this is exactly the same problem.\n-\t Structures with __complex__ fields are tested in the libstdc++\n-\t testsuite: 26_numerics/complex_inserters_extractors.cc.  */\n-      if (TREE_CODE (TREE_TYPE (field)) == COMPLEX_TYPE)\n-\t{\n-\t  if (dump_file && (dump_flags & TDF_DETAILS))\n+\tfor (f = TYPE_FIELDS (type); f ; f = TREE_CHAIN (f))\n+\t  if (TREE_CODE (f) == FIELD_DECL)\n \t    {\n-\t      fprintf (dump_file, \"Cannot scalarize variable \");\n-\t      print_generic_expr (dump_file, var, dump_flags);\n-\t      fprintf (dump_file,\n-\t\t       \" because it contains a __complex__ field, \");\n-\t      print_generic_expr (dump_file, field, dump_flags);\n-\t      fprintf (dump_file, \"\\n\");\n+\t      if (!type_can_instantiate_all_elements (TREE_TYPE (f)))\n+\t\t{\n+\t\t  bitmap_set_bit (sra_type_inst_cache, cache+1);\n+\t\t  return false;\n+\t\t}\n \t    }\n-\t  return false;\n-\t}\n \n-      /* FIXME.  We don't scalarize structures with bit fields yet.  To\n-\t support this, we should make sure that all the fields fit in one\n-\t word and modify every operation done on the scalarized bit fields\n-\t to mask them properly.  */\n-      if (DECL_BIT_FIELD (field))\n-\t{\n-\t  if (dump_file && (dump_flags & TDF_DETAILS))\n-\t    {\n-\t      fprintf (dump_file, \"Cannot scalarize variable \");\n-\t      print_generic_expr (dump_file, var, dump_flags);\n-\t      fprintf (dump_file,\n-\t\t       \" because it contains a bit-field, \");\n-\t      print_generic_expr (dump_file, field, dump_flags);\n-\t      fprintf (dump_file, \"\\n\");\n-\t    }\n-\t  return false;\n-\t}\n+\tbitmap_set_bit (sra_type_inst_cache, cache+0);\n+\treturn true;\n+      }\n \n-      nfields++;\n-      if (nfields > MAX_NFIELDS_FOR_SRA)\n-\t{\n-\t  if (dump_file && (dump_flags & TDF_DETAILS))\n-\t    {\n-\t      fprintf (dump_file, \"Cannot scalarize variable \");\n-\t      print_generic_expr (dump_file, var, dump_flags);\n-\t      fprintf (dump_file,\n-\t\t       \" because it contains more than %d fields\\n\", \n-\t\t       MAX_NFIELDS_FOR_SRA);\n-\t    }\n-\t  return false;\n-\t}\n-    }\n+    case ARRAY_TYPE:\n+      return type_can_instantiate_all_elements (TREE_TYPE (type));\n \n-  /* If the structure had no FIELD_DECLs, then don't bother\n-     scalarizing it.  */\n-  return nfields > 0;\n-}\n+    case COMPLEX_TYPE:\n+      return true;\n \n+    default:\n+      abort ();\n+    }\n+}\n \n-/* Replace the COMPONENT_REF, REALPART_EXPR or IMAGPART_EXPR pointed-to by\n-   TP inside STMT with the corresponding scalar replacement from SRA_MAP.  */\n+/* Test whether ELT or some sub-element cannot be scalarized.  */\n \n-static inline void\n-scalarize_component_ref (tree stmt, tree *tp)\n+static bool\n+can_completely_scalarize_p (struct sra_elt *elt)\n {\n-  tree t = *tp, obj = TREE_OPERAND (t, 0);\n+  struct sra_elt *c;\n+\n+  if (elt->cannot_scalarize)\n+    return false;\n+\n+  for (c = elt->children; c ; c = c->sibling)\n+    if (!can_completely_scalarize_p (c))\n+      return false;\n+\n+  return true;\n+}\n \n-  /* When scalarizing a function argument, we will need to insert copy-in\n-     operations from the original PARM_DECLs. Note that these copy-in\n-     operations may end up being dead, but we won't know until we rename\n-     the new variables into SSA.  */\n-  if (TREE_CODE (obj) == PARM_DECL)\n-    bitmap_set_bit (needs_copy_in, var_ann (obj)->uid);\n+\f\n+/* A simplified tree hashing algorithm that only handles the types of\n+   trees we expect to find in sra_elt->element.  */\n \n+static hashval_t\n+sra_hash_tree (tree t)\n+{\n   switch (TREE_CODE (t))\n     {\n-    case COMPONENT_REF:\n-      t = get_scalar_for_field (obj, TREE_OPERAND (t, 1));\n-      break;\n-    case REALPART_EXPR:\n-    case IMAGPART_EXPR:\n-      t = get_scalar_for_complex_part (obj, TREE_CODE (t));\n-      break;\n+    case VAR_DECL:\n+    case PARM_DECL:\n+    case RESULT_DECL:\n+    case FIELD_DECL:\n+      return DECL_UID (t);\n+    case INTEGER_CST:\n+      return TREE_INT_CST_LOW (t) ^ TREE_INT_CST_HIGH (t);\n     default:\n       abort ();\n     }\n-\n-  *tp = t;\n-  modify_stmt (stmt);\n }\n \n+/* Hash function for type SRA_PAIR.  */\n \n-/* Scalarize the structure assignment for the statement pointed by SI_P.  */\n-\n-static void\n-scalarize_structure_assignment (block_stmt_iterator *si_p)\n+static hashval_t\n+sra_elt_hash (const void *x)\n {\n-  var_ann_t lhs_ann, rhs_ann;\n-  tree lhs, rhs, list, orig_stmt;\n-  bool lhs_can, rhs_can;\n-\n-  orig_stmt = bsi_stmt (*si_p);\n-  lhs = TREE_OPERAND (orig_stmt, 0);\n-  rhs = TREE_OPERAND (orig_stmt, 1);\n-  list = NULL_TREE;\n+  const struct sra_elt *e = x;\n+  const struct sra_elt *p;\n+  hashval_t h;\n \n-#if defined ENABLE_CHECKING\n-  if (TREE_CODE (orig_stmt) != MODIFY_EXPR)\n-    abort ();\n-#endif\n+  h = sra_hash_tree (e->element);\n \n-  /* Remove all type casts from RHS.  This may seem heavy handed but\n-     it's actually safe and it is necessary in the presence of C++\n-     reinterpret_cast<> where structure assignments of different\n-     structures will be present in the IL.  This was the case of PR\n-     13347 (http://gcc.gnu.org/bugzilla/show_bug.cgi?id=13347) which\n-     had something like this:\n-\n-\tstruct A f;\n-     \tstruct B g;\n-\tf = (struct A)g;\n-\n-     Both 'f' and 'g' were scalarizable, but the presence of the type\n-     cast was causing SRA to not replace the RHS of the assignment\n-     with g's scalar replacements.  Furthermore, the fact that this\n-     assignment reached this point without causing syntax errors means\n-     that the type cast is safe and that a field-by-field assignment\n-     from 'g' into 'f' is the right thing to do.  */\n-  STRIP_NOPS (rhs);\n-\n-  lhs_ann = DECL_P (lhs) ? var_ann (lhs) : NULL;\n-  rhs_ann = DECL_P (rhs) ? var_ann (rhs) : NULL;\n-\n-#if defined ENABLE_CHECKING\n-  /* Two different variables should not have the same UID.  */\n-  if (lhs_ann\n-      && rhs_ann\n-      && lhs != rhs\n-      && lhs_ann->uid == rhs_ann->uid)\n-    abort ();\n-#endif\n+  /* Take into account everything back up the chain.  Given that chain\n+     lengths are rarely very long, this should be acceptable.  If we\n+     truely identify this as a performance problem, it should work to\n+     hash the pointer value \"e->parent\".  */\n+  for (p = e->parent; p ; p = p->parent)\n+    h = (h * 65521) ^ sra_hash_tree (p->element);\n \n-  lhs_can = lhs_ann && bitmap_bit_p (sra_candidates, lhs_ann->uid);\n-  rhs_can = rhs_ann && bitmap_bit_p (sra_candidates, rhs_ann->uid);\n+  return h;\n+}\n+  \n+/* Equality function for type SRA_PAIR.  */\n \n-  /* Both LHS and RHS are scalarizable.  */\n-  if (lhs_can && rhs_can)\n-    list = create_scalar_copies (lhs, rhs, SCALAR_SCALAR);\n+static int\n+sra_elt_eq (const void *x, const void *y)\n+{\n+  const struct sra_elt *a = x;\n+  const struct sra_elt *b = y;\n \n-  /* Only RHS is scalarizable.  */\n-  else if (rhs_can)\n-    list = create_scalar_copies (lhs, rhs, FIELD_SCALAR);\n+  if (a->parent != b->parent)\n+    return false;\n \n-  /* Only LHS is scalarizable.  */\n-  else if (lhs_can)\n-    list = create_scalar_copies (lhs, rhs, SCALAR_FIELD);\n+  /* All the field/decl stuff is unique.  */\n+  if (a->element == b->element)\n+    return true;\n \n-  /* If neither side is scalarizable, do nothing else.  */\n+  /* The only thing left is integer equality.  */\n+  if (TREE_CODE (a->element) == INTEGER_CST\n+      && TREE_CODE (b->element) == INTEGER_CST)\n+    return tree_int_cst_equal (a->element, b->element);\n   else\n-    return;\n-\n-  /* Set line number information for our replacements.  */\n-  if (EXPR_HAS_LOCATION (orig_stmt))\n-    annotate_all_with_locus (&list, EXPR_LOCATION (orig_stmt));\n-\n-  /* Replace the existing statement with the newly created list of\n-     scalarized copies.  When replacing the original statement, the first\n-     copy replaces it and the remaining copies are inserted either after\n-     the first copy or on the outgoing edges of the original statement's\n-     block.  */\n-  {\n-    tree_stmt_iterator tsi = tsi_start (list);\n-    bsi_replace (si_p, tsi_stmt (tsi), true);\n-    tsi_delink (&tsi);\n-    if (stmt_ends_bb_p (orig_stmt))\n-      insert_edge_copies (list, bb_for_stmt (orig_stmt));\n-    else\n-      bsi_insert_after (si_p, list, BSI_CONTINUE_LINKING);\n-  }\n+    return false;\n }\n \n+/* Create or return the SRA_ELT structure for CHILD in PARENT.  PARENT\n+   may be null, in which case CHILD must be a DECL.  */\n \n-/* Traverse all the referenced variables in the program looking for\n-   structures that could be replaced with scalars.  */\n-\n-static bool\n-find_candidates_for_sra (void)\n+static struct sra_elt *\n+lookup_element (struct sra_elt *parent, tree child, tree type,\n+\t\tenum insert_option insert)\n {\n-  size_t i;\n-  bool any_set = false;\n+  struct sra_elt dummy;\n+  struct sra_elt **slot;\n+  struct sra_elt *elt;\n \n-  for (i = 0; i < num_referenced_vars; i++)\n+  dummy.parent = parent;\n+  dummy.element = child;\n+\n+  slot = (struct sra_elt **) htab_find_slot (sra_map, &dummy, insert);\n+  if (!slot && insert == NO_INSERT)\n+    return NULL;\n+\n+  elt = *slot;\n+  if (!elt && insert == INSERT)\n     {\n-      tree var = referenced_var (i);\n+      *slot = elt = obstack_alloc (&sra_obstack, sizeof (*elt));\n+      memset (elt, 0, sizeof (*elt));\n+\n+      elt->parent = parent;\n+      elt->element = child;\n+      elt->type = type;\n+      elt->is_scalar = is_sra_scalar_type (type);\n+\n+      if (parent)\n+\t{\n+\t  elt->sibling = parent->children;\n+\t  parent->children = elt;\n+\t}\n \n-      if ((TREE_CODE (TREE_TYPE (var)) == RECORD_TYPE\n-\t   || TREE_CODE (TREE_TYPE (var)) == COMPLEX_TYPE)\n-\t  && can_be_scalarized_p (var))\n+      /* If this is a parameter, then if we want to scalarize, we have\n+\t one copy from the true function parameter.  Count it now.  */\n+      if (TREE_CODE (child) == PARM_DECL)\n \t{\n-\t  bitmap_set_bit (sra_candidates, var_ann (var)->uid);\n-\t  any_set = true;\n+\t  elt->n_copies = 1;\n+\t  bitmap_set_bit (needs_copy_in, var_ann (child)->uid);\n \t}\n     }\n \n-  return any_set;\n+  return elt;\n }\n \n+/* Return true if the ARRAY_REF in EXPR is a constant, in bounds access.  */\n \n-/* Insert STMT on all the outgoing edges out of BB.  Note that if BB\n-   has more than one edge, STMT will be replicated for each edge.  Also,\n-   abnormal edges will be ignored.  */\n-\n-void\n-insert_edge_copies (tree stmt, basic_block bb)\n+static bool\n+is_valid_const_index (tree expr)\n {\n-  edge e;\n-  bool first_copy;\n+  tree dom, t, index = TREE_OPERAND (expr, 1);\n \n-  first_copy = true;\n-  for (e = bb->succ; e; e = e->succ_next)\n-    {\n-      /* We don't need to insert copies on abnormal edges.  The\n-\t value of the scalar replacement is not guaranteed to\n-\t be valid through an abnormal edge.  */\n-      if (!(e->flags & EDGE_ABNORMAL))\n-\t{\n-\t  if (first_copy)\n-\t    {\n-\t      bsi_insert_on_edge (e, stmt);\n-\t      first_copy = false;\n-\t    }\n-\t  else\n-\t    bsi_insert_on_edge (e, lhd_unsave_expr_now (stmt));\n-\t}\n-    }\n-}\n+  if (TREE_CODE (index) != INTEGER_CST)\n+    return false;\n \n+  /* Watch out for stupid user tricks, indexing outside the array.\n \n-/* Append a new assignment statement to TSI.  */\n+     Careful, we're not called only on scalarizable types, so do not\n+     assume constant array bounds.  We needn't do anything with such\n+     cases, since they'll be referring to objects that we should have\n+     already rejected for scalarization, so returning false is fine.  */\n \n-static tree\n-csc_assign (tree_stmt_iterator *tsi, tree lhs, tree rhs)\n+  dom = TYPE_DOMAIN (TREE_TYPE (TREE_OPERAND (expr, 0)));\n+  if (dom == NULL)\n+    return false;\n+\n+  t = TYPE_MIN_VALUE (dom);\n+  if (!t || TREE_CODE (t) != INTEGER_CST)\n+    return false;\n+  if (tree_int_cst_lt (index, t))\n+    return false;\n+\n+  t = TYPE_MAX_VALUE (dom);\n+  if (!t || TREE_CODE (t) != INTEGER_CST)\n+    return false;\n+  if (tree_int_cst_lt (t, index))\n+    return false;\n+\n+  return true;\n+}\n+\n+/* Create or return the SRA_ELT structure for EXPR if the expression \n+   refers to a scalarizable variable.  */\n+\n+static struct sra_elt *\n+maybe_lookup_element_for_expr (tree expr)\n {\n-  tree stmt = build (MODIFY_EXPR, TREE_TYPE (lhs), lhs, rhs);\n-  modify_stmt (stmt);\n-  tsi_link_after (tsi, stmt, TSI_NEW_STMT);\n-  return stmt;\n+  struct sra_elt *elt;\n+  tree child;\n+\n+  switch (TREE_CODE (expr))\n+    {\n+    case VAR_DECL:\n+    case PARM_DECL:\n+    case RESULT_DECL:\n+      if (is_sra_candidate_decl (expr))\n+\treturn lookup_element (NULL, expr, TREE_TYPE (expr), INSERT);\n+      return NULL;\n+\n+    case ARRAY_REF:\n+      /* We can't scalarize variable array indicies.  */\n+      if (is_valid_const_index (expr))\n+        child = TREE_OPERAND (expr, 1);\n+      else\n+\treturn NULL;\n+      break;\n+\n+    case COMPONENT_REF:\n+      /* Don't look through unions.  */\n+      if (TREE_CODE (TREE_TYPE (TREE_OPERAND (expr, 0))) != RECORD_TYPE)\n+\treturn NULL;\n+      child = TREE_OPERAND (expr, 1);\n+      break;\n+\n+    case REALPART_EXPR:\n+      child = integer_zero_node;\n+      break;\n+    case IMAGPART_EXPR:\n+      child = integer_one_node;\n+      break;\n+\n+    default:\n+      return NULL;\n+    }\n+\n+  elt = maybe_lookup_element_for_expr (TREE_OPERAND (expr, 0));\n+  if (elt)\n+    return lookup_element (elt, child, TREE_TYPE (expr), INSERT);\n+  return NULL;\n+}\n+\n+\f\n+/* Functions to walk just enough of the tree to see all scalarizable\n+   references, and categorize them.  */\n+\n+/* A set of callbacks for phases 2 and 4.  They'll be invoked for the\n+   various kinds of references seen.  In all cases, *BSI is an iterator\n+   pointing to the statement being processed.  */\n+struct sra_walk_fns\n+{\n+  /* Invoked when ELT is required as a unit.  Note that ELT might refer to\n+     a leaf node, in which case this is a simple scalar reference.  *EXPR_P\n+     points to the location of the expression.  IS_OUTPUT is true if this\n+     is a left-hand-side reference.  */\n+  void (*use) (struct sra_elt *elt, tree *expr_p,\n+\t       block_stmt_iterator *bsi, bool is_output);\n+\n+  /* Invoked when we have a copy between two scalarizable references.  */\n+  void (*copy) (struct sra_elt *lhs_elt, struct sra_elt *rhs_elt,\n+\t\tblock_stmt_iterator *bsi);\n+\n+  /* Invoked when ELT is initialized from a constant.  VALUE may be NULL,\n+     in which case it should be treated as an empty CONSTRUCTOR.  */\n+  void (*init) (struct sra_elt *elt, tree value, block_stmt_iterator *bsi);\n+\n+  /* Invoked when we have a copy between one scalarizable reference ELT\n+     and one non-scalarizable reference OTHER.  IS_OUTPUT is true if ELT\n+     is on the left-hand side.  */\n+  void (*ldst) (struct sra_elt *elt, tree other,\n+\t\tblock_stmt_iterator *bsi, bool is_output);\n+\n+  /* True during phase 2, false during phase 4.  */\n+  /* ??? This is a hack.  */\n+  bool initial_scan;\n+};\n+\n+#ifdef ENABLE_CHECKING\n+/* Invoked via walk_tree, if *TP contains an candidate decl, return it.  */\n+\n+static tree\n+sra_find_candidate_decl (tree *tp, int *walk_subtrees,\n+\t\t\t void *data ATTRIBUTE_UNUSED)\n+{\n+  tree t = *tp;\n+  enum tree_code code = TREE_CODE (t);\n+\n+  if (code == VAR_DECL || code == PARM_DECL || code == RESULT_DECL)\n+    {\n+      *walk_subtrees = 0;\n+      if (is_sra_candidate_decl (t))\n+\treturn t;\n+    }\n+  else if (TYPE_P (t))\n+    *walk_subtrees = 0;\n+\n+  return NULL;\n+}\n+#endif\n+\n+/* Walk most expressions looking for a scalarizable aggregate.\n+   If we find one, invoke FNS->USE.  */\n+\n+static void\n+sra_walk_expr (tree *expr_p, block_stmt_iterator *bsi, bool is_output,\n+\t       const struct sra_walk_fns *fns)\n+{\n+  tree expr = *expr_p;\n+  tree inner = expr;\n+\n+  /* We're looking to collect a reference expression between EXPR and INNER,\n+     such that INNER is a scalarizable decl and all other nodes through EXPR\n+     are references that we can scalarize.  If we come across something that\n+     we can't scalarize, we reset EXPR.  This has the effect of making it\n+     appear that we're referring to the larger expression as a whole.  */\n+\n+  while (1)\n+    switch (TREE_CODE (inner))\n+      {\n+      case VAR_DECL:\n+      case PARM_DECL:\n+      case RESULT_DECL:\n+\t/* If there is a scalarizable decl at the bottom, then process it.  */\n+\tif (is_sra_candidate_decl (inner))\n+\t  {\n+\t    struct sra_elt *elt = maybe_lookup_element_for_expr (expr);\n+\t    fns->use (elt, expr_p, bsi, is_output);\n+\t  }\n+\treturn;\n+\n+      case ARRAY_REF:\n+\t/* Non-constant index means any member may be accessed.  Prevent the\n+\t   expression from being scalarized.  If we were to treat this as a\n+\t   reference to the whole array, we can wind up with a single dynamic\n+\t   index reference inside a loop being overridden by several constant\n+\t   index references during loop setup.  It's possible that this could\n+\t   be avoided by using dynamic usage counts based on BB trip counts\n+\t   (based on loop analysis or profiling), but that hardly seems worth \n+\t   the effort.  */\n+\t/* ??? Hack.  Figure out how to push this into the scan routines\n+\t   without duplicating too much code.  */\n+\tif (!is_valid_const_index (inner))\n+\t  {\n+\t    if (fns->initial_scan)\n+\t      {\n+\t\tstruct sra_elt *elt\n+\t\t  = maybe_lookup_element_for_expr (TREE_OPERAND (inner, 0));\n+\t\tif (elt)\n+\t\t  elt->cannot_scalarize = true;\n+\t      }\n+\t    return;\n+\t  }\n+\t/* ??? Are we assured that non-constant bounds and stride will have\n+\t   the same value everywhere?  I don't think Fortran will...  */\n+\tif (TREE_OPERAND (inner, 2) || TREE_OPERAND (inner, 3))\n+\t  goto use_all;\n+\tinner = TREE_OPERAND (inner, 0);\n+\tbreak;\n+\n+      case COMPONENT_REF:\n+\t/* A reference to a union member constitutes a reference to the\n+\t   entire union.  */\n+\tif (TREE_CODE (TREE_TYPE (TREE_OPERAND (inner, 0))) != RECORD_TYPE)\n+\t  goto use_all;\n+\t/* ??? See above re non-constant stride.  */\n+\tif (TREE_OPERAND (inner, 2))\n+\t  goto use_all;\n+\tinner = TREE_OPERAND (inner, 0);\n+\tbreak;\n+\n+      case REALPART_EXPR:\n+      case IMAGPART_EXPR:\n+\tinner = TREE_OPERAND (inner, 0);\n+\tbreak;\n+\n+      case BIT_FIELD_REF:\n+\t/* A bit field reference (access to *multiple* fields simultaneously)\n+\t   is not currently scalarized.  Consider this an access to the \n+\t   complete outer element, to which walk_tree will bring us next.  */\n+\tgoto use_all;\n+\n+      case ARRAY_RANGE_REF:\n+\t/* Similarly, an subrange reference is used to modify indexing.  Which\n+\t   means that the canonical element names that we have won't work.  */\n+\tgoto use_all;\n+\n+      case VIEW_CONVERT_EXPR:\n+      case NOP_EXPR:\n+\t/* Similarly, a view/nop explicitly wants to look at an object in a\n+\t   type other than the one we've scalarized.  */\n+\tgoto use_all;\n+\n+      use_all:\n+        expr_p = &TREE_OPERAND (inner, 0);\n+\tinner = expr = *expr_p;\n+\tbreak;\n+\n+      default:\n+#ifdef ENABLE_CHECKING\n+\t/* Validate that we're not missing any references.  */\n+\tif (walk_tree (&inner, sra_find_candidate_decl, NULL, NULL))\n+\t  abort ();\n+#endif\n+\treturn;\n+      }\n+}\n+\n+/* Walk a TREE_LIST of values looking for scalarizable aggregates.\n+   If we find one, invoke FNS->USE.  */\n+\n+static void\n+sra_walk_tree_list (tree list, block_stmt_iterator *bsi, bool is_output,\n+\t\t    const struct sra_walk_fns *fns)\n+{\n+  tree op;\n+  for (op = list; op ; op = TREE_CHAIN (op))\n+    sra_walk_expr (&TREE_VALUE (op), bsi, is_output, fns);\n+}\n+\n+/* Walk the arguments of a CALL_EXPR looking for scalarizable aggregates.\n+   If we find one, invoke FNS->USE.  */\n+\n+static void\n+sra_walk_call_expr (tree expr, block_stmt_iterator *bsi,\n+\t\t    const struct sra_walk_fns *fns)\n+{\n+  sra_walk_tree_list (TREE_OPERAND (expr, 1), bsi, false, fns);\n+}\n+\n+/* Walk the inputs and outputs of an ASM_EXPR looking for scalarizable\n+   aggregates.  If we find one, invoke FNS->USE.  */\n+\n+static void\n+sra_walk_asm_expr (tree expr, block_stmt_iterator *bsi,\n+\t\t   const struct sra_walk_fns *fns)\n+{\n+  sra_walk_tree_list (ASM_INPUTS (expr), bsi, false, fns);\n+  sra_walk_tree_list (ASM_OUTPUTS (expr), bsi, true, fns);\n+}\n+\n+/* Walk a MODIFY_EXPR and categorize the assignment appropriately.  */\n+\n+static void\n+sra_walk_modify_expr (tree expr, block_stmt_iterator *bsi,\n+\t\t      const struct sra_walk_fns *fns)\n+{\n+  struct sra_elt *lhs_elt, *rhs_elt;\n+  tree lhs, rhs;\n+\n+  lhs = TREE_OPERAND (expr, 0);\n+  rhs = TREE_OPERAND (expr, 1);\n+  lhs_elt = maybe_lookup_element_for_expr (lhs);\n+  rhs_elt = maybe_lookup_element_for_expr (rhs);\n+\n+  /* If both sides are scalarizable, this is a COPY operation.  */\n+  if (lhs_elt && rhs_elt)\n+    {\n+      fns->copy (lhs_elt, rhs_elt, bsi);\n+      return;\n+    }\n+\n+  if (lhs_elt)\n+    {\n+      /* If this is an assignment from a constant, or constructor, then\n+\t we have access to all of the elements individually.  Invoke INIT.  */\n+      if (TREE_CODE (rhs) == COMPLEX_EXPR\n+\t  || TREE_CODE (rhs) == COMPLEX_CST\n+\t  || TREE_CODE (rhs) == CONSTRUCTOR)\n+\tfns->init (lhs_elt, rhs, bsi);\n+\n+      /* If this is an assignment from read-only memory, treat this as if\n+\t we'd been passed the constructor directly.  Invoke INIT.  */\n+      else if (TREE_CODE (rhs) == VAR_DECL\n+\t       && TREE_STATIC (rhs)\n+\t       && TREE_READONLY (rhs)\n+\t       && targetm.binds_local_p (rhs))\n+\t{\n+\t  if (DECL_INITIAL (rhs) != error_mark_node)\n+\t    fns->init (lhs_elt, DECL_INITIAL (rhs), bsi);\n+\t}\n+\n+      /* If this is a copy from a non-scalarizable lvalue, invoke LDST.\n+\t The lvalue requirement prevents us from trying to directly scalarize\n+\t the result of a function call.  Which would result in trying to call\n+\t the function multiple times, and other evil things.  */\n+      else if (!lhs_elt->is_scalar && is_gimple_addr_expr_arg (rhs))\n+\tfns->ldst (lhs_elt, rhs, bsi, true);\n+\t\n+      /* Otherwise we're being used in some context that requires the\n+\t aggregate to be seen as a whole.  Invoke USE.  */\n+      else\n+\tfns->use (lhs_elt, &TREE_OPERAND (expr, 0), bsi, true);\n+    }\n+  else\n+    {\n+      /* LHS_ELT being null only means that the LHS as a whole is not a\n+\t scalarizable reference.  There may be occurrences of scalarizable\n+\t variables within, which implies a USE.  */\n+      sra_walk_expr (&TREE_OPERAND (expr, 0), bsi, true, fns);\n+    }\n+\n+  /* Likewise for the right-hand side.  The only difference here is that\n+     we don't have to handle constants, and the RHS may be a call.  */\n+  if (rhs_elt)\n+    {\n+      if (!rhs_elt->is_scalar)\n+\tfns->ldst (rhs_elt, lhs, bsi, false);\n+      else\n+\tfns->use (rhs_elt, &TREE_OPERAND (expr, 1), bsi, false);\n+    }\n+  else if (TREE_CODE (rhs) == CALL_EXPR)\n+    sra_walk_call_expr (rhs, bsi, fns);\n+  else\n+    sra_walk_expr (&TREE_OPERAND (expr, 1), bsi, false, fns);\n+}\n+\n+/* Entry point to the walk functions.  Search the entire function,\n+   invoking the callbacks in FNS on each of the references to\n+   scalarizable variables.  */\n+\n+static void\n+sra_walk_function (const struct sra_walk_fns *fns)\n+{\n+  basic_block bb;\n+  block_stmt_iterator si;\n+\n+  /* ??? Phase 4 could derive some benefit to walking the function in\n+     dominator tree order.  */\n+\n+  FOR_EACH_BB (bb)\n+    for (si = bsi_start (bb); !bsi_end_p (si); bsi_next (&si))\n+      {\n+\ttree stmt, t;\n+\tstmt_ann_t ann;\n+\n+\tstmt = bsi_stmt (si);\n+\tann = stmt_ann (stmt);\n+\n+\t/* If the statement has no virtual operands, then it doesn't\n+\t   make any structure references that we care about.  */\n+\tif (NUM_V_MAY_DEFS (V_MAY_DEF_OPS (ann)) == 0\n+\t    && NUM_VUSES (VUSE_OPS (ann)) == 0\n+\t    && NUM_V_MUST_DEFS (V_MUST_DEF_OPS (ann)) == 0)\n+\t  continue;\n+\n+\tswitch (TREE_CODE (stmt))\n+\t  {\n+\t  case RETURN_EXPR:\n+\t    /* If we have \"return <retval>\" then the return value is\n+\t       already exposed for our pleasure.  Walk it as a USE to\n+\t       force all the components back in place for the return.\n+\n+\t       If we have an embedded assignment, then <retval> is of\n+\t       a type that gets returned in registers in this ABI, and\n+\t       we do not wish to extend their lifetimes.  Treat this\n+\t       as a USE of the variable on the RHS of this assignment.  */\n+\n+\t    t = TREE_OPERAND (stmt, 0);\n+\t    if (TREE_CODE (t) == MODIFY_EXPR)\n+\t      sra_walk_expr (&TREE_OPERAND (t, 1), &si, false, fns);\n+\t    else\n+\t      sra_walk_expr (&TREE_OPERAND (stmt, 0), &si, false, fns);\n+\t    break;\n+\n+\t  case MODIFY_EXPR:\n+\t    sra_walk_modify_expr (stmt, &si, fns);\n+\t    break;\n+\t  case CALL_EXPR:\n+\t    sra_walk_call_expr (stmt, &si, fns);\n+\t    break;\n+\t  case ASM_EXPR:\n+\t    sra_walk_asm_expr (stmt, &si, fns);\n+\t    break;\n+\n+\t  default:\n+\t    break;\n+\t  }\n+      }\n+}\n+\f\n+/* Phase One: Scan all referenced variables in the program looking for\n+   structures that could be decomposed.  */\n+\n+static bool\n+find_candidates_for_sra (void)\n+{\n+  size_t i;\n+  bool any_set = false;\n+\n+  for (i = 0; i < num_referenced_vars; i++)\n+    {\n+      tree var = referenced_var (i);\n+      if (decl_can_be_decomposed_p (var))\n+        {\n+          bitmap_set_bit (sra_candidates, var_ann (var)->uid);\n+          any_set = true;\n+        }\n+    }\n+ \n+  return any_set;\n+}\n+\n+\f\n+/* Phase Two: Scan all references to scalarizable variables.  Count the\n+   number of times they are used or copied respectively.  */\n+\n+/* Callbacks to fill in SRA_WALK_FNS.  Everything but USE is\n+   considered a copy, because we can decompose the reference such that\n+   the sub-elements needn't be contiguous.  */\n+\n+static void\n+scan_use (struct sra_elt *elt, tree *expr_p ATTRIBUTE_UNUSED,\n+\t  block_stmt_iterator *bsi ATTRIBUTE_UNUSED,\n+\t  bool is_output ATTRIBUTE_UNUSED)\n+{\n+  elt->n_uses += 1;\n+}\n+\n+static void\n+scan_copy (struct sra_elt *lhs_elt, struct sra_elt *rhs_elt,\n+\t   block_stmt_iterator *bsi ATTRIBUTE_UNUSED)\n+{\n+  lhs_elt->n_copies += 1;\n+  rhs_elt->n_copies += 1;\n+}\n+\n+static void\n+scan_init (struct sra_elt *lhs_elt, tree rhs ATTRIBUTE_UNUSED,\n+\t   block_stmt_iterator *bsi ATTRIBUTE_UNUSED)\n+{\n+  lhs_elt->n_copies += 1;\n+}\n+\n+static void\n+scan_ldst (struct sra_elt *elt, tree other ATTRIBUTE_UNUSED,\n+\t   block_stmt_iterator *bsi ATTRIBUTE_UNUSED,\n+\t   bool is_output ATTRIBUTE_UNUSED)\n+{\n+  elt->n_copies += 1;\n+}\n+\n+/* Dump the values we collected during the scanning phase.  */\n+\n+static void\n+scan_dump (struct sra_elt *elt)\n+{\n+  struct sra_elt *c;\n+\n+  dump_sra_elt_name (dump_file, elt);\n+  fprintf (dump_file, \": n_uses=%u n_copies=%u\\n\", elt->n_uses, elt->n_copies);\n+\n+  for (c = elt->children; c ; c = c->sibling)\n+    scan_dump (c);\n+}\n+\n+/* Entry point to phase 2.  Scan the entire function, building up\n+   scalarization data structures, recording copies and uses.  */\n+\n+static void\n+scan_function (void)\n+{\n+  static const struct sra_walk_fns fns = {\n+    scan_use, scan_copy, scan_init, scan_ldst, true\n+  };\n+\n+  sra_walk_function (&fns);\n+\n+  if (dump_file && (dump_flags & TDF_DETAILS))\n+    {\n+      size_t i;\n+\n+      fputs (\"\\nScan results:\\n\", dump_file);\n+      EXECUTE_IF_SET_IN_BITMAP (sra_candidates, 0, i,\n+\t{\n+\t  tree var = referenced_var (i);\n+\t  struct sra_elt *elt = lookup_element (NULL, var, NULL, NO_INSERT);\n+\t  if (elt)\n+\t    scan_dump (elt);\n+\t});\n+      fputc ('\\n', dump_file);\n+    }\n+}\n+\f\n+/* Phase Three: Make decisions about which variables to scalarize, if any.\n+   All elements to be scalarized have replacement variables made for them.  */\n+\n+/* A subroutine of build_element_name.  Recursively build the element\n+   name on the obstack.  */\n+\n+static void\n+build_element_name_1 (struct sra_elt *elt)\n+{\n+  tree t;\n+  char buffer[32];\n+\n+  if (elt->parent)\n+    {\n+      build_element_name_1 (elt->parent);\n+      obstack_1grow (&sra_obstack, '$');\n+\n+      if (TREE_CODE (elt->parent->type) == COMPLEX_TYPE)\n+\t{\n+\t  if (elt->element == integer_zero_node)\n+\t    obstack_grow (&sra_obstack, \"real\", 4);\n+\t  else\n+\t    obstack_grow (&sra_obstack, \"imag\", 4);\n+\t  return;\n+\t}\n+    }\n+\n+  t = elt->element;\n+  if (TREE_CODE (t) == INTEGER_CST)\n+    {\n+      /* ??? Eh.  Don't bother doing double-wide printing.  */\n+      sprintf (buffer, HOST_WIDE_INT_PRINT_DEC, TREE_INT_CST_LOW (t));\n+      obstack_grow (&sra_obstack, buffer, strlen (buffer));\n+    }\n+  else\n+    {\n+      tree name = DECL_NAME (t);\n+      if (name)\n+\tobstack_grow (&sra_obstack, IDENTIFIER_POINTER (name),\n+\t\t      IDENTIFIER_LENGTH (name));\n+      else\n+\t{\n+\t  sprintf (buffer, \"D%u\", DECL_UID (t));\n+\t  obstack_grow (&sra_obstack, buffer, strlen (buffer));\n+\t}\n+    }\n+}\n+\n+/* Construct a pretty variable name for an element's replacement variable.\n+   The name is built on the obstack.  */\n+\n+static char *\n+build_element_name (struct sra_elt *elt)\n+{\n+  build_element_name_1 (elt);\n+  obstack_1grow (&sra_obstack, '\\0');\n+  return obstack_finish (&sra_obstack);\n+}\n+\n+/* Instantiate an element as an independent variable.  */\n+\n+static void\n+instantiate_element (struct sra_elt *elt)\n+{\n+  struct sra_elt *base_elt;\n+  tree var, base;\n+\n+  for (base_elt = elt; base_elt->parent; base_elt = base_elt->parent)\n+    continue;\n+  base = base_elt->element;\n+\n+  elt->replacement = var = make_rename_temp (elt->type, \"SR\");\n+  DECL_SOURCE_LOCATION (var) = DECL_SOURCE_LOCATION (base);\n+  TREE_NO_WARNING (var) = TREE_NO_WARNING (base);\n+  DECL_ARTIFICIAL (var) = DECL_ARTIFICIAL (base);\n+\n+  if (DECL_NAME (base) && !DECL_IGNORED_P (base))\n+    {\n+      char *pretty_name = build_element_name (elt);\n+      DECL_NAME (var) = get_identifier (pretty_name);\n+      obstack_free (&sra_obstack, pretty_name);\n+    }\n+\n+  if (dump_file)\n+    {\n+      fputs (\"  \", dump_file);\n+      dump_sra_elt_name (dump_file, elt);\n+      fputs (\" -> \", dump_file);\n+      print_generic_expr (dump_file, var, dump_flags);\n+      fputc ('\\n', dump_file);\n+    }\n+}\n+\n+/* Make one pass across an element tree deciding whether or not it's\n+   profitable to instantiate individual leaf scalars.\n+\n+   PARENT_USES and PARENT_COPIES are the sum of the N_USES and N_COPIES\n+   fields all the way up the tree.  */\n+\n+static void\n+decide_instantiation_1 (struct sra_elt *elt, unsigned int parent_uses,\n+\t\t\tunsigned int parent_copies)\n+{\n+  if (dump_file && !elt->parent)\n+    {\n+      fputs (\"Initial instantiation for \", dump_file);\n+      dump_sra_elt_name (dump_file, elt);\n+      fputc ('\\n', dump_file);\n+    }\n+\n+  if (elt->cannot_scalarize)\n+    return;\n+\n+  if (elt->is_scalar)\n+    {\n+      /* The decision is simple: instantiate if we're used more frequently\n+\t than the parent needs to be seen as a complete unit.  */\n+      if (elt->n_uses + elt->n_copies + parent_copies > parent_uses)\n+\tinstantiate_element (elt);\n+    }\n+  else\n+    {\n+      struct sra_elt *c;\n+      unsigned int this_uses = elt->n_uses + parent_uses;\n+      unsigned int this_copies = elt->n_copies + parent_copies;\n+\n+      for (c = elt->children; c ; c = c->sibling)\n+\tdecide_instantiation_1 (c, this_uses, this_copies);\n+    }\n+}\n+\n+/* Compute the size and number of all instantiated elements below ELT.\n+   We will only care about this if the size of the complete structure\n+   fits in a HOST_WIDE_INT, so we don't have to worry about overflow.  */\n+\n+static unsigned int\n+sum_instantiated_sizes (struct sra_elt *elt, unsigned HOST_WIDE_INT *sizep)\n+{\n+  if (elt->replacement)\n+    {\n+      *sizep += TREE_INT_CST_LOW (TYPE_SIZE_UNIT (elt->type));\n+      return 1;\n+    }\n+  else\n+    {\n+      struct sra_elt *c;\n+      unsigned int count = 0;\n+\n+      for (c = elt->children; c ; c = c->sibling)\n+\tcount += sum_instantiated_sizes (c, sizep);\n+\n+      return count;\n+    }\n+}\n+\n+/* Instantiate fields in ELT->TYPE that are not currently present as\n+   children of ELT.  */\n+\n+static void instantiate_missing_elements (struct sra_elt *elt);\n+\n+static void\n+instantiate_missing_elements_1 (struct sra_elt *elt, tree child, tree type)\n+{\n+  struct sra_elt *sub = lookup_element (elt, child, type, INSERT);\n+  if (sub->is_scalar)\n+    {\n+      if (sub->replacement == NULL)\n+\tinstantiate_element (sub);\n+    }\n+  else\n+    instantiate_missing_elements (sub);\n+}\n+\n+static void\n+instantiate_missing_elements (struct sra_elt *elt)\n+{\n+  tree type = elt->type;\n+\n+  switch (TREE_CODE (type))\n+    {\n+    case RECORD_TYPE:\n+      {\n+\ttree f;\n+\tfor (f = TYPE_FIELDS (type); f ; f = TREE_CHAIN (f))\n+\t  if (TREE_CODE (f) == FIELD_DECL)\n+\t    instantiate_missing_elements_1 (elt, f, TREE_TYPE (f));\n+\tbreak;\n+      }\n+\n+    case ARRAY_TYPE:\n+      {\n+\ttree i, max, subtype;\n+\n+\ti = TYPE_MIN_VALUE (TYPE_DOMAIN (type));\n+\tmax = TYPE_MAX_VALUE (TYPE_DOMAIN (type));\n+\tsubtype = TREE_TYPE (type);\n+\n+\twhile (1)\n+\t  {\n+\t    instantiate_missing_elements_1 (elt, i, subtype);\n+\t    if (tree_int_cst_equal (i, max))\n+\t      break;\n+\t    i = int_const_binop (PLUS_EXPR, i, integer_one_node, true);\n+\t  }\n+\n+\tbreak;\n+      }\n+\n+    case COMPLEX_TYPE:\n+      type = TREE_TYPE (type);\n+      instantiate_missing_elements_1 (elt, integer_zero_node, type);\n+      instantiate_missing_elements_1 (elt, integer_one_node, type);\n+      break;\n+\n+    default:\n+      abort ();\n+    }\n+}\n+\n+/* Make one pass across an element tree deciding whether to perform block\n+   or element copies.  If we decide on element copies, instantiate all\n+   elements.  Return true if there are any instantiated sub-elements.  */\n+\n+static bool\n+decide_block_copy (struct sra_elt *elt)\n+{\n+  struct sra_elt *c;\n+  bool any_inst;\n+\n+  /* If scalarization is disabled, respect it.  */\n+  if (elt->cannot_scalarize)\n+    {\n+      elt->use_block_copy = 1;\n+\n+      if (dump_file)\n+\t{\n+\t  fputs (\"Scalarization disabled for \", dump_file);\n+\t  dump_sra_elt_name (dump_file, elt);\n+\t  fputc ('\\n', dump_file);\n+\t}\n+\n+      return false;\n+    }\n+\n+  /* Don't decide if we've no uses.  */\n+  if (elt->n_uses == 0 && elt->n_copies == 0)\n+    ;\n+\n+  else if (!elt->is_scalar)\n+    {\n+      tree size_tree = TYPE_SIZE_UNIT (elt->type);\n+      bool use_block_copy = true;\n+\n+      /* Don't bother trying to figure out the rest if the structure is\n+\t so large we can't do easy arithmetic.  This also forces block\n+\t copies for variable sized structures.  */\n+      if (host_integerp (size_tree, 1))\n+\t{\n+\t  unsigned HOST_WIDE_INT full_size, inst_size = 0;\n+\t  unsigned int inst_count;\n+\n+\t  full_size = tree_low_cst (size_tree, 1);\n+\n+\t  /* ??? What to do here.  If there are two fields, and we've only \n+\t     instantiated one, then instantiating the other is clearly a win.\n+\t     If there are a large number of fields then the size of the copy\n+\t     is much more of a factor.  */\n+\n+\t  /* If the structure is small, and we've made copies, go ahead\n+\t     and instantiate, hoping that the copies will go away.  */\n+\t  if (full_size <= (unsigned) MOVE_RATIO * UNITS_PER_WORD\n+\t      && elt->n_copies > elt->n_uses)\n+\t    use_block_copy = false;\n+\t  else\n+\t    {\n+\t      inst_count = sum_instantiated_sizes (elt, &inst_size);\n+\n+\t      if (inst_size * 4 >= full_size * 3)\n+\t\tuse_block_copy = false;\n+\t    }\n+\n+\t  /* In order to avoid block copy, we have to be able to instantiate\n+\t     all elements of the type.  See if this is possible.  */\n+\t  if (!use_block_copy\n+\t      && (!can_completely_scalarize_p (elt)\n+\t\t  || !type_can_instantiate_all_elements (elt->type)))\n+\t    use_block_copy = true;\n+\t}\n+      elt->use_block_copy = use_block_copy;\n+\n+      if (dump_file)\n+\t{\n+\t  fprintf (dump_file, \"Using %s for \",\n+\t\t   use_block_copy ? \"block-copy\" : \"element-copy\");\n+\t  dump_sra_elt_name (dump_file, elt);\n+\t  fputc ('\\n', dump_file);\n+\t}\n+\n+      if (!use_block_copy)\n+\t{\n+\t  instantiate_missing_elements (elt);\n+\t  return true;\n+\t}\n+    }\n+\n+  any_inst = elt->replacement != NULL;\n+\n+  for (c = elt->children; c ; c = c->sibling)\n+    any_inst |= decide_block_copy (c);\n+\n+  return any_inst;\n+}\n+\n+/* Entry point to phase 3.  Instantiate scalar replacement variables.  */\n+\n+static void\n+decide_instantiations (void)\n+{\n+  unsigned int i;\n+  bool cleared_any;\n+  struct bitmap_head_def done_head;\n+\n+  /* We cannot clear bits from a bitmap we're iterating over,\n+     so save up all the bits to clear until the end.  */\n+  bitmap_initialize (&done_head, 1);\n+  cleared_any = false;\n+\n+  EXECUTE_IF_SET_IN_BITMAP (sra_candidates, 0, i,\n+    {\n+      tree var = referenced_var (i);\n+      struct sra_elt *elt = lookup_element (NULL, var, NULL, NO_INSERT);\n+      if (elt)\n+\t{\n+\t  decide_instantiation_1 (elt, 0, 0);\n+\t  if (!decide_block_copy (elt))\n+\t    elt = NULL;\n+\t}\n+      if (!elt)\n+\t{\n+\t  bitmap_set_bit (&done_head, i);\n+\t  cleared_any = true;\n+\t}\n+    });\n+\n+  if (cleared_any)\n+    {\n+      bitmap_operation (sra_candidates, sra_candidates, &done_head, \n+\t\t\tBITMAP_AND_COMPL);\n+      bitmap_operation (needs_copy_in, needs_copy_in, &done_head, \n+\t\t\tBITMAP_AND_COMPL);\n+    }\n+  bitmap_clear (&done_head);\n+\n+  if (dump_file)\n+    fputc ('\\n', dump_file);\n+}\n+\n+\f\n+/* Phase Four: Update the function to match the replacements created.  */\n+\n+/* Mark all the variables in V_MAY_DEF or V_MUST_DEF operands for STMT for\n+   renaming. This becomes necessary when we modify all of a non-scalar.  */\n+\n+static void\n+mark_all_v_defs (tree stmt)\n+{\n+  v_may_def_optype v_may_defs;\n+  v_must_def_optype v_must_defs;\n+  size_t i, n;\n+\n+  get_stmt_operands (stmt);\n+\n+  v_may_defs = V_MAY_DEF_OPS (stmt_ann (stmt));\n+  n = NUM_V_MAY_DEFS (v_may_defs);\n+  for (i = 0; i < n; i++)\n+    {\n+      tree sym = V_MAY_DEF_RESULT (v_may_defs, i);\n+      if (TREE_CODE (sym) == SSA_NAME)\n+\tsym = SSA_NAME_VAR (sym);\n+      bitmap_set_bit (vars_to_rename, var_ann (sym)->uid);\n+    }\n+\n+  v_must_defs = V_MUST_DEF_OPS (stmt_ann (stmt));\n+  n = NUM_V_MUST_DEFS (v_must_defs);\n+  for (i = 0; i < n; i++)\n+    {\n+      tree sym = V_MUST_DEF_OP (v_must_defs, i);\n+      if (TREE_CODE (sym) == SSA_NAME)\n+\tsym = SSA_NAME_VAR (sym);\n+      bitmap_set_bit (vars_to_rename, var_ann (sym)->uid);\n+    }\n }\n \n-/* A subroutine of create_scalar_copies.  Construct a COMPONENT_REF\n-   expression for BASE referencing FIELD.  INDEX is the field index.  */\n+/* Build a single level component reference to ELT rooted at BASE.  */\n \n static tree\n-csc_build_component_ref (tree base, tree field)\n+generate_one_element_ref (struct sra_elt *elt, tree base)\n {\n-  switch (TREE_CODE (base))\n+  switch (TREE_CODE (TREE_TYPE (base)))\n     {\n-    case CONSTRUCTOR:\n-      /* Only appears on RHS.  The only remaining CONSTRUCTORS for\n-\t record types that should remain are empty, and imply that\n-\t the entire structure should be zeroed.  */\n-      if (CONSTRUCTOR_ELTS (base))\n-\tabort ();\n-      return fold_convert (TREE_TYPE (field), integer_zero_node);\n+    case RECORD_TYPE:\n+      return build (COMPONENT_REF, elt->type, base, elt->element, NULL);\n \n-    default:\n-      /* Avoid sharing BASE when building the different COMPONENT_REFs.\n-\t We let the first field have the original version.  */\n-      if (field != TYPE_FIELDS (TREE_TYPE (base)))\n-\tbase = unshare_expr (base);\n-      break;\n+    case ARRAY_TYPE:\n+      return build (ARRAY_REF, elt->type, base, elt->element, NULL, NULL);\n \n-    case VAR_DECL:\n-    case PARM_DECL:\n-      /* Special case for the above -- decls are always shared.  */\n-      break;\n-    }\n+    case COMPLEX_TYPE:\n+      if (elt->element == integer_zero_node)\n+\treturn build (REALPART_EXPR, elt->type, base);\n+      else\n+\treturn build (IMAGPART_EXPR, elt->type, base);\n \n-  return build (COMPONENT_REF, TREE_TYPE (field), base, field, NULL_TREE);\n+    default:\n+      abort ();\n+    }\n }\n \n-/* Similarly for REALPART_EXPR and IMAGPART_EXPR for complex types.  */\n+/* Build a full component reference to ELT rooted at its native variable.  */\n \n static tree\n-csc_build_complex_part (tree base, enum tree_code part)\n+generate_element_ref (struct sra_elt *elt)\n+{\n+  if (elt->parent)\n+    return generate_one_element_ref (elt, generate_element_ref (elt->parent));\n+  else\n+    return elt->element;\n+}\n+\n+/* Generate a set of assignment statements in *LIST_P to copy all\n+   instantiated elements under ELT to or from the equivalent structure\n+   rooted at EXPR.  COPY_OUT controls the direction of the copy, with\n+   true meaning to copy out of EXPR into ELT.  */\n+\n+static void\n+generate_copy_inout (struct sra_elt *elt, bool copy_out, tree expr,\n+\t\t     tree *list_p)\n {\n-  switch (TREE_CODE (base))\n+  struct sra_elt *c;\n+  tree t;\n+\n+  if (elt->replacement)\n     {\n-    case COMPLEX_CST:\n-      if (part == REALPART_EXPR)\n-\treturn TREE_REALPART (base);\n+      if (copy_out)\n+\tt = build (MODIFY_EXPR, void_type_node, elt->replacement, expr);\n       else\n-\treturn TREE_IMAGPART (base);\n+\tt = build (MODIFY_EXPR, void_type_node, expr, elt->replacement);\n+      append_to_statement_list (t, list_p);\n+    }\n+  else\n+    {\n+      for (c = elt->children; c ; c = c->sibling)\n+\t{\n+\t  t = generate_one_element_ref (c, unshare_expr (expr));\n+\t  generate_copy_inout (c, copy_out, t, list_p);\n+\t}\n+    }\n+}\n \n-    case COMPLEX_EXPR:\n-      if (part == REALPART_EXPR)\n-        return TREE_OPERAND (base, 0);\n-      else\n-        return TREE_OPERAND (base, 1);\n+/* Generate a set of assignment statements in *LIST_P to copy all instantiated\n+   elements under SRC to their counterparts under DST.  There must be a 1-1\n+   correspondence of instantiated elements.  */\n \n-    default:\n-      /* Avoid sharing BASE when building the different references.\n-\t We let the real part have the original version.  */\n-      if (part != REALPART_EXPR)\n-\tbase = unshare_expr (base);\n-      break;\n+static void\n+generate_element_copy (struct sra_elt *dst, struct sra_elt *src, tree *list_p)\n+{\n+  struct sra_elt *dc, *sc;\n \n-    case VAR_DECL:\n-    case PARM_DECL:\n-      /* Special case for the above -- decls are always shared.  */\n-      break;\n+  for (dc = dst->children; dc ; dc = dc->sibling)\n+    {\n+      sc = lookup_element (src, dc->element, NULL, NO_INSERT);\n+      if (sc == NULL)\n+\tabort ();\n+      generate_element_copy (dc, sc, list_p);\n     }\n \n-  return build1 (part, TREE_TYPE (TREE_TYPE (base)), base);\n-}\n-\n-/* Create and return a list of assignments to perform a scalarized\n-   structure assignment 'LHS = RHS'.  Both LHS and RHS are assumed to be\n-   of an aggregate or complex type.  Three types of copies may be specified:\n+  if (dst->replacement)\n+    {\n+      tree t;\n \n-   SCALAR_SCALAR will emit assignments for all the scalar temporaries\n-      corresponding to the fields of LHS and RHS.\n+      if (src->replacement == NULL)\n+\tabort ();\n \n-   FIELD_SCALAR will emit assignments from the scalar replacements of\n-      RHS into each of the fields of the LHS.\n+      t = build (MODIFY_EXPR, void_type_node, dst->replacement,\n+\t\t src->replacement);\n+      append_to_statement_list (t, list_p);\n+    }\n+}\n \n-   SCALAR_FIELD will emit assignments from each field of the RHS into\n-      the scalar replacements of the LHS.  */\n+/* Generate a set of assignment statements in *LIST_P to zero all instantiated\n+   elements under ELT.  In addition, do not assign to elements that have been\n+   marked VISITED but do reset the visited flag; this allows easy coordination\n+   with generate_element_init.  */\n \n-static tree\n-create_scalar_copies (tree lhs, tree rhs, enum sra_copy_mode mode)\n+static void\n+generate_element_zero (struct sra_elt *elt, tree *list_p)\n {\n-  tree type, list;\n-  tree_stmt_iterator tsi;\n-\n-#if defined ENABLE_CHECKING\n-  /* Sanity checking.  Check that we are not trying to scalarize a\n-     non-decl.  */\n-  if (!DECL_P (lhs) && (mode == SCALAR_FIELD || mode == SCALAR_SCALAR))\n-    abort ();\n-  if (!DECL_P (rhs) && (mode == FIELD_SCALAR || mode == SCALAR_SCALAR))\n-    abort ();\n-#endif\n+  struct sra_elt *c;\n \n-  type = TREE_TYPE (lhs);\n-  list = alloc_stmt_list ();\n-  tsi = tsi_start (list);\n+  for (c = elt->children; c ; c = c->sibling)\n+    generate_element_zero (c, list_p);\n \n-  /* VA_ARG_EXPRs have side effects, so we need to copy it first to a\n-     temporary before scalarizing.  FIXME: This should disappear once\n-     VA_ARG_EXPRs are properly lowered.  */\n-  if (TREE_CODE (rhs) == VA_ARG_EXPR)\n+  if (elt->visited)\n+    elt->visited = false;\n+  else if (elt->replacement)\n     {\n-      tree stmt, tmp;\n-\n-      /* Add TMP = VA_ARG_EXPR <>  */\n-      tmp = make_rename_temp (TREE_TYPE (rhs), NULL);\n-      stmt = csc_assign (&tsi, tmp, rhs);\n+      tree t;\n \n-      /* Mark all the variables in VDEF operands for renaming, because\n-\t the VA_ARG_EXPR will now be in a different statement.  */\n-      mark_all_v_may_defs (stmt);\n-      mark_all_v_must_defs (stmt);\n+      if (elt->is_scalar)\n+\tt = fold_convert (elt->type, integer_zero_node);\n+      else\n+\t/* We generated a replacement for a non-scalar?  */\n+\tabort ();\n \n-      /* Set RHS to be the new temporary TMP.  */\n-      rhs = tmp;\n+      t = build (MODIFY_EXPR, void_type_node, elt->replacement, t);\n+      append_to_statement_list (t, list_p);\n     }\n+}\n \n-  /* When making *_SCALAR copies from PARM_DECLs, we will need to insert\n-     copy-in operations from the original PARM_DECLs.  Note that these\n-     copy-in operations may end up being dead, but we won't know until\n-     we rename the new variables into SSA.  */\n-  if ((mode == SCALAR_SCALAR || mode == FIELD_SCALAR)\n-      && TREE_CODE (rhs) == PARM_DECL)\n-    bitmap_set_bit (needs_copy_in, var_ann (rhs)->uid);\n+/* Generate a set of assignment statements in *LIST_P to set all instantiated\n+   elements under ELT with the contents of the initializer INIT.  In addition,\n+   mark all assigned elements VISITED; this allows easy coordination with\n+   generate_element_zero.  */\n \n-  /* Now create scalar copies for each individual field according to MODE.  */\n-  if (TREE_CODE (type) == COMPLEX_TYPE)\n-    {\n-      /* Create scalar copies of both the real and imaginary parts.  */\n-      tree real_lhs, real_rhs, imag_lhs, imag_rhs;\n+static void\n+generate_element_init (struct sra_elt *elt, tree init, tree *list_p)\n+{\n+  enum tree_code init_code = TREE_CODE (init);\n+  struct sra_elt *sub;\n+  tree t;\n \n-      if (mode == SCALAR_FIELD)\n-\t{\n-\t  real_rhs = csc_build_complex_part (rhs, REALPART_EXPR);\n-\t  imag_rhs = csc_build_complex_part (rhs, IMAGPART_EXPR);\n-\t}\n-      else\n+  if (elt->is_scalar)\n+    {\n+      if (elt->replacement)\n \t{\n-\t  real_rhs = get_scalar_for_complex_part (rhs, REALPART_EXPR);\n-\t  imag_rhs = get_scalar_for_complex_part (rhs, IMAGPART_EXPR);\n+\t  t = build (MODIFY_EXPR, void_type_node, elt->replacement, init);\n+\t  append_to_statement_list (t, list_p);\n+\t  elt->visited = true;\n \t}\n+      return;\n+    }\n \n-      if (mode == FIELD_SCALAR)\n+  switch (init_code)\n+    {\n+    case COMPLEX_CST:\n+    case COMPLEX_EXPR:\n+      for (sub = elt->children; sub ; sub = sub->sibling)\n \t{\n-\t  /* In this case we do not need to create but one statement,\n-\t     since we can create a new complex value whole.  */\n-\n-\t  if (TREE_CONSTANT (real_rhs) && TREE_CONSTANT (imag_rhs))\n-\t    rhs = build_complex (type, real_rhs, imag_rhs);\n+\t  if (sub->element == integer_zero_node)\n+\t    t = (init_code == COMPLEX_EXPR\n+\t\t ? TREE_OPERAND (init, 0) : TREE_REALPART (init));\n \t  else\n-\t    rhs = build (COMPLEX_EXPR, type, real_rhs, imag_rhs);\n-\t  csc_assign (&tsi, lhs, rhs);\n-\t}\n-      else\n-\t{\n-\t  real_lhs = get_scalar_for_complex_part (lhs, REALPART_EXPR);\n-\t  imag_lhs = get_scalar_for_complex_part (lhs, IMAGPART_EXPR);\n-\n-\t  csc_assign (&tsi, real_lhs, real_rhs);\n-\t  csc_assign (&tsi, imag_lhs, imag_rhs);\n+\t    t = (init_code == COMPLEX_EXPR\n+\t\t ? TREE_OPERAND (init, 1) : TREE_IMAGPART (init));\n+\t  generate_element_init (sub, t, list_p);\n \t}\n-    }\n-  else\n-    {\n-      tree lf, rf;\n-\n-      /* ??? C++ generates copies between different pointer-to-member\n-\t structures of different types.  To combat this, we must track\n-\t the field of both the left and right structures, so that we\n-\t index the variables with fields of their own type.  */\n+      break;\n \n-      for (lf = TYPE_FIELDS (type), rf = TYPE_FIELDS (TREE_TYPE (rhs));\n-\t   lf;\n-\t   lf = TREE_CHAIN (lf), rf = TREE_CHAIN (rf))\n+    case CONSTRUCTOR:\n+      for (t = CONSTRUCTOR_ELTS (init); t ; t = TREE_CHAIN (t))\n \t{\n-\t  tree lhs_var, rhs_var;\n-\n-\t  /* Only copy FIELD_DECLs.  */\n-\t  if (TREE_CODE (lf) != FIELD_DECL)\n+\t  sub = lookup_element (elt, TREE_PURPOSE (t), NULL, NO_INSERT);\n+\t  if (sub == NULL)\n \t    continue;\n+\t  generate_element_init (sub, TREE_VALUE (t), list_p);\n+\t}\n+      break;\n \n-\t  if (mode == FIELD_SCALAR)\n-\t    lhs_var = csc_build_component_ref (lhs, lf);\n-\t  else\n-\t    lhs_var = get_scalar_for_field (lhs, lf);\n+    default:\n+      abort ();\n+    }\n+}\n \n-\t  if (mode == SCALAR_FIELD)\n-\t    rhs_var = csc_build_component_ref (rhs, rf);\n-\t  else\n-\t    rhs_var = get_scalar_for_field (rhs, rf);\n+/* Insert STMT on all the outgoing edges out of BB.  Note that if BB\n+   has more than one edge, STMT will be replicated for each edge.  Also,\n+   abnormal edges will be ignored.  */\n \n-\t  csc_assign (&tsi, lhs_var, rhs_var);\n-\t}\n-    }\n+void\n+insert_edge_copies (tree stmt, basic_block bb)\n+{\n+  edge e;\n+  bool first_copy;\n \n-  /* All the scalar copies just created will either create new definitions\n-     or remove existing definitions of LHS, so we need to mark it for\n-     renaming.  */\n-  if (TREE_SIDE_EFFECTS (list))\n+  first_copy = true;\n+  for (e = bb->succ; e; e = e->succ_next)\n     {\n-      if (mode == SCALAR_FIELD || mode == SCALAR_SCALAR)\n-\t{\n-\t  /* If the LHS has been scalarized, mark it for renaming.  */\n-\t  bitmap_set_bit (vars_to_rename, var_ann (lhs)->uid);\n-\t}\n-      else if (mode == FIELD_SCALAR)\n+      /* We don't need to insert copies on abnormal edges.  The\n+\t value of the scalar replacement is not guaranteed to\n+\t be valid through an abnormal edge.  */\n+      if (!(e->flags & EDGE_ABNORMAL))\n \t{\n-\t  /* Otherwise, mark all the symbols in the VDEFs for the last\n-\t     scalarized statement just created.  Since all the statements\n-\t     introduce the same VDEFs, we only need to check the last one.  */\n-\t  mark_all_v_may_defs (tsi_stmt (tsi));\n-\t  mark_all_v_must_defs (tsi_stmt (tsi));\n+\t  if (first_copy)\n+\t    {\n+\t      bsi_insert_on_edge (e, stmt);\n+\t      first_copy = false;\n+\t    }\n+\t  else\n+\t    bsi_insert_on_edge (e, lhd_unsave_expr_now (stmt));\n \t}\n-      else\n-\tabort ();\n     }\n-\n-  return list;\n }\n \n-/* A helper function that creates the copies, updates line info,\n-   and emits the code either before or after BSI.  */\n+/* Helper function to insert LIST before BSI, and set up line number info.  */\n \n static void\n-emit_scalar_copies (block_stmt_iterator *bsi, tree lhs, tree rhs,\n-\t\t    enum sra_copy_mode mode)\n+sra_insert_before (block_stmt_iterator *bsi, tree list)\n {\n-  tree list = create_scalar_copies (lhs, rhs, mode);\n   tree stmt = bsi_stmt (*bsi);\n \n   if (EXPR_HAS_LOCATION (stmt))\n     annotate_all_with_locus (&list, EXPR_LOCATION (stmt));\n-\n   bsi_insert_before (bsi, list, BSI_SAME_STMT);\n }\n \n-/* Traverse all the statements in the function replacing references to\n-   scalarizable structures with their corresponding scalar temporaries.  */\n+/* Similarly, but insert after BSI.  Handles insertion onto edges as well.  */\n \n static void\n-scalarize_structures (void)\n+sra_insert_after (block_stmt_iterator *bsi, tree list)\n {\n-  basic_block bb;\n-  block_stmt_iterator si;\n-  size_t i;\n-\n-  FOR_EACH_BB (bb)\n-    for (si = bsi_start (bb); !bsi_end_p (si); bsi_next (&si))\n-      {\n-\ttree stmt;\n-\tstmt_ann_t ann;\n-\n-\tstmt = bsi_stmt (si);\n-\tann = stmt_ann (stmt);\n-\n-\t/* If the statement has no virtual operands, then it doesn't make\n-\t   structure references that we care about.  */\n-\tif (NUM_V_MAY_DEFS (V_MAY_DEF_OPS (ann)) == 0\n-\t    && NUM_VUSES (VUSE_OPS (ann)) == 0\n-\t    && NUM_V_MUST_DEFS (V_MUST_DEF_OPS (ann)) == 0)\n-\t  continue;\n-\n-\t/* Structure references may only appear in certain statements.  */\n-\tif (TREE_CODE (stmt) != MODIFY_EXPR\n-\t    && TREE_CODE (stmt) != CALL_EXPR\n-\t    && TREE_CODE (stmt) != RETURN_EXPR\n-\t    && TREE_CODE (stmt) != ASM_EXPR)\n-\t  continue;\n-\n-\tscalarize_stmt (&si);\n-      }\n+  tree stmt = bsi_stmt (*bsi);\n \n-  /* Initialize the scalar replacements for every structure that is a\n-     function argument.  */\n-  EXECUTE_IF_SET_IN_BITMAP (needs_copy_in, 0, i,\n-    {\n-      tree var = referenced_var (i);\n-      tree list = create_scalar_copies (var, var, SCALAR_FIELD);\n-      bsi_insert_on_edge (ENTRY_BLOCK_PTR->succ, list);\n-    });\n+  if (EXPR_HAS_LOCATION (stmt))\n+    annotate_all_with_locus (&list, EXPR_LOCATION (stmt));\n \n-  /* Commit edge insertions.  */\n-  bsi_commit_edge_inserts (NULL);\n+  if (stmt_ends_bb_p (stmt))\n+    insert_edge_copies (list, bsi->bb);\n+  else\n+    bsi_insert_after (bsi, list, BSI_CONTINUE_LINKING);\n }\n \n-\n-/* Scalarize structure references in the statement pointed by SI_P.  */\n+/* Similarly, but replace the statement at BSI.  */\n \n static void\n-scalarize_stmt (block_stmt_iterator *si_p)\n+sra_replace (block_stmt_iterator *bsi, tree list)\n {\n-  tree stmt = bsi_stmt (*si_p);\n-\n-  /* Handle assignments.  */\n-  if (TREE_CODE (stmt) == MODIFY_EXPR\n-      && TREE_CODE (TREE_OPERAND (stmt, 1)) != CALL_EXPR)\n-    scalarize_modify_expr (si_p);\n-\n-  /* Handle RETURN_EXPR.  */\n-  else if (TREE_CODE (stmt) == RETURN_EXPR)\n-    scalarize_return_expr (si_p);\n-\n-  /* Handle function calls (note that this must be handled after\n-     MODIFY_EXPR and RETURN_EXPR because a function call can appear in\n-     both).  */\n-  else if (get_call_expr_in (stmt) != NULL_TREE)\n-    scalarize_call_expr (si_p);\n-\n-  /* Handle ASM_EXPRs.  */\n-  else if (TREE_CODE (stmt) == ASM_EXPR)\n-    scalarize_asm_expr (si_p);\n+  sra_insert_before (bsi, list);\n+  bsi_remove (bsi);\n+  if (bsi_end_p (*bsi))\n+    *bsi = bsi_last (bsi->bb);\n+  else\n+    bsi_prev (bsi);\n }\n \n-\n-/* Helper for scalarize_stmt to handle assignments.  */\n+/* Scalarize a USE.  To recap, this is either a simple reference to ELT,\n+   if elt is scalar, or some ocurrence of ELT that requires a complete\n+   aggregate.  IS_OUTPUT is true if ELT is being modified.  */\n \n static void\n-scalarize_modify_expr (block_stmt_iterator *si_p)\n+scalarize_use (struct sra_elt *elt, tree *expr_p, block_stmt_iterator *bsi,\n+\t       bool is_output)\n {\n-  tree stmt = bsi_stmt (*si_p);\n-  tree lhs = TREE_OPERAND (stmt, 0);\n-  tree rhs = TREE_OPERAND (stmt, 1);\n-  tree var = NULL_TREE;\n+  tree list = NULL, stmt = bsi_stmt (*bsi);\n \n-  /* Found AGGREGATE.FIELD = ...  */\n-  if (is_sra_candidate_ref (lhs))\n+  if (elt->replacement)\n     {\n-      tree sym;\n-      v_may_def_optype v_may_defs;\n-\n-      scalarize_component_ref (stmt, &TREE_OPERAND (stmt, 0));\n-\n-      /* Mark the LHS to be renamed, as we have just removed the previous\n-\t V_MAY_DEF for AGGREGATE.  The statement should have exactly one \n-\t V_MAY_DEF for variable AGGREGATE.  */\n-      v_may_defs = STMT_V_MAY_DEF_OPS (stmt);\n-      if (NUM_V_MAY_DEFS (v_may_defs) != 1)\n-\tabort ();\n-      sym = SSA_NAME_VAR (V_MAY_DEF_RESULT (v_may_defs, 0));\n-      bitmap_set_bit (vars_to_rename, var_ann (sym)->uid);\n+      /* If we have a replacement, then updating the reference is as\n+\t simple as modifying the existing statement in place.  */\n+      if (is_output)\n+\tmark_all_v_defs (stmt);\n+      *expr_p = elt->replacement;\n+      modify_stmt (stmt);\n     }\n-\n-  /* Found ... = AGGREGATE.FIELD  */\n-  else if (is_sra_candidate_ref (rhs))\n-    scalarize_component_ref (stmt, &TREE_OPERAND (stmt, 1));\n-\n-  /* Found a complex reference nesting involving a candidate decl.  This\n-     should only occur if the above condition is false if a BIT_FIELD_REF or\n-     VIEW_CONVERT_EXPR is involved.  This is similar to a CALL_EXPR, if the\n-     operand of the BIT_FIELD_REF is a scalarizable structure, we need to\n-     copy from its scalar replacements before doing the bitfield operation.\n-\n-     FIXME: BIT_FIELD_REFs are often generated by fold-const.c.  This is\n-     not always desirable because they obfuscate the original predicates,\n-     limiting what the tree optimizers may do.  For instance, in\n-     testsuite/g++.dg/opt/nrv4.C the use of SRA allows the optimizers to\n-     optimize function main() to 'return 0;'.  However, the folder\n-     generates a BIT_FIELD_REF operation for one of the comparisons,\n-     preventing the optimizers from removing all the redundant\n-     operations.  */\n-  else if (is_sra_candidate_complex_ref (rhs, &var))\n+  else\n     {\n-      emit_scalar_copies (si_p, var, var, FIELD_SCALAR);\n-\n-      /* If the LHS of the assignment is also a scalarizable structure, insert\n-\t copies into the scalar replacements after the call.  */\n-      if (is_sra_candidate_decl (lhs))\n+      /* Otherwise we need some copies.  If ELT is being read, then we want\n+\t to store all (modified) sub-elements back into the structure before\n+\t the reference takes place.  If ELT is being written, then we want to\n+\t load the changed values back into our shadow variables.  */\n+      /* ??? We don't check modified for reads, we just always write all of\n+\t the values.  We should be able to record the SSA number of the VOP\n+\t for which the values were last read.  If that number matches the\n+\t SSA number of the VOP in the current statement, then we needn't\n+\t emit an assignment.  This would also eliminate double writes when\n+\t a structure is passed as more than one argument to a function call.\n+\t This optimization would be most effective if sra_walk_function\n+\t processed the blocks in dominator order.  */\n+\n+      generate_copy_inout (elt, is_output, generate_element_ref (elt), &list);\n+      if (list == NULL)\n+\treturn;\n+      if (is_output)\n \t{\n-\t  tree list = create_scalar_copies (lhs, lhs, SCALAR_FIELD);\n-\t  if (EXPR_HAS_LOCATION (stmt))\n-\t    annotate_all_with_locus (&list, EXPR_LOCATION (stmt));\n-\t  if (stmt_ends_bb_p (stmt))\n-\t    insert_edge_copies (list, bb_for_stmt (stmt));\n-\t  else\n-\t    bsi_insert_after (si_p, list, BSI_NEW_STMT);\n+\t  mark_all_v_defs (expr_first (list));\n+\t  sra_insert_after (bsi, list);\n \t}\n+      else\n+\tsra_insert_before (bsi, list);\n     }\n-\n-  /* Found AGGREGATE = ... or ... = AGGREGATE  */\n-  else if (DECL_P (lhs) || DECL_P (rhs))\n-    scalarize_structure_assignment (si_p);\n }\n \n+/* Scalarize a COPY.  To recap, this is an assignment statement between\n+   two scalarizable references, LHS_ELT and RHS_ELT.  */\n \n-/* Scalarize structure references in LIST.  Use DONE to avoid duplicates.  */\n-\n-static inline void\n-scalarize_tree_list (tree list, block_stmt_iterator *si_p, bitmap done)\n+static void\n+scalarize_copy (struct sra_elt *lhs_elt, struct sra_elt *rhs_elt,\n+\t\tblock_stmt_iterator *bsi)\n {\n-  tree op;\n+  tree list, stmt;\n \n-  for (op = list; op; op = TREE_CHAIN (op))\n+  if (lhs_elt->replacement && rhs_elt->replacement)\n     {\n-      tree arg = TREE_VALUE (op);\n+      /* If we have two scalar operands, modify the existing statement.  */\n+      stmt = bsi_stmt (*bsi);\n \n-      if (is_sra_candidate_decl (arg))\n-\t{\n-\t  int index = var_ann (arg)->uid;\n-\t  if (!bitmap_bit_p (done, index))\n-\t    {\n-\t      emit_scalar_copies (si_p, arg, arg, FIELD_SCALAR);\n-\t      bitmap_set_bit (done, index);\n-\t    }\n-\t}\n-      else if (is_sra_candidate_ref (arg))\n+#ifdef ENABLE_CHECKING\n+      /* See the commentary in sra_walk_function concerning\n+\t RETURN_EXPR, and why we should never see one here.  */\n+      if (TREE_CODE (stmt) != MODIFY_EXPR)\n+\tabort ();\n+#endif\n+\n+      TREE_OPERAND (stmt, 0) = lhs_elt->replacement;\n+      TREE_OPERAND (stmt, 1) = rhs_elt->replacement;\n+      modify_stmt (stmt);\n+    }\n+  else if (lhs_elt->use_block_copy || rhs_elt->use_block_copy)\n+    {\n+      /* If either side requires a block copy, then sync the RHS back\n+\t to the original structure, leave the original assignment \n+\t statement (which will perform the block copy), then load the\n+\t LHS values out of its now-updated original structure.  */\n+      /* ??? Could perform a modified pair-wise element copy.  That\n+\t would at least allow those elements that are instantiated in\n+\t both structures to be optimized well.  */\n+\n+      list = NULL;\n+      generate_copy_inout (rhs_elt, false,\n+\t\t\t   generate_element_ref (rhs_elt), &list);\n+      if (list)\n \t{\n-\t  tree stmt = bsi_stmt (*si_p);\n-\t  scalarize_component_ref (stmt, &TREE_VALUE (op));\n+\t  mark_all_v_defs (expr_first (list));\n+\t  sra_insert_before (bsi, list);\n \t}\n+\n+      list = NULL;\n+      generate_copy_inout (lhs_elt, true,\n+\t\t\t   generate_element_ref (lhs_elt), &list);\n+      if (list)\n+\tsra_insert_after (bsi, list);\n     }\n-}\n+  else\n+    {\n+      /* Otherwise both sides must be fully instantiated.  In which\n+\t case perform pair-wise element assignments and replace the\n+\t original block copy statement.  */\n+\n+      stmt = bsi_stmt (*bsi);\n+      mark_all_v_defs (stmt);\n \n+      list = NULL;\n+      generate_element_copy (lhs_elt, rhs_elt, &list);\n+      if (list == NULL)\n+\tabort ();\n+      sra_replace (bsi, list);\n+    }\n+}\n \n-/* Helper for scalarize_stmt to handle function calls.  */\n+/* Scalarize an INIT.  To recap, this is an assignment to a scalarizable\n+   reference from some form of constructor: CONSTRUCTOR, COMPLEX_CST or\n+   COMPLEX_EXPR.  If RHS is NULL, it should be treated as an empty\n+   CONSTRUCTOR.  */\n \n static void\n-scalarize_call_expr (block_stmt_iterator *si_p)\n+scalarize_init (struct sra_elt *lhs_elt, tree rhs, block_stmt_iterator *bsi)\n {\n-  tree stmt = bsi_stmt (*si_p);\n-  tree call = (TREE_CODE (stmt) == MODIFY_EXPR) ? TREE_OPERAND (stmt, 1) : stmt;\n-  struct bitmap_head_def done_head;\n+  tree list = NULL;\n \n-  /* First scalarize the arguments.  Order is important, because the copy\n-     operations for the arguments need to go before the call.\n-     Scalarization of the return value needs to go after the call.  */\n-  bitmap_initialize (&done_head, 1);\n-  scalarize_tree_list (TREE_OPERAND (call, 1), si_p, &done_head);\n-  bitmap_clear (&done_head);\n+  /* Generate initialization statements for all members extant in the RHS.  */\n+  if (rhs)\n+    generate_element_init (lhs_elt, rhs, &list);\n \n-  /* Scalarize the return value, if any.  */\n-  if (TREE_CODE (stmt) == MODIFY_EXPR)\n-    {\n-      tree var = get_base_address (TREE_OPERAND (stmt, 0));\n+  /* CONSTRUCTOR is defined such that any member not mentioned is assigned\n+     a zero value.  Initialize the rest of the instantiated elements.  */\n+  generate_element_zero (lhs_elt, &list);\n+  if (list == NULL)\n+    return;\n \n-      /* If the LHS of the assignment is a scalarizable structure, insert\n-\t copies into the scalar replacements after the call.  */\n-      if (is_sra_candidate_decl (var))\n-\t{\n-\t  tree list = create_scalar_copies (var, var, SCALAR_FIELD);\n-\t  if (EXPR_HAS_LOCATION (stmt))\n-\t    annotate_all_with_locus (&list, EXPR_LOCATION (stmt));\n-\t  if (stmt_ends_bb_p (stmt))\n-\t    insert_edge_copies (list, bb_for_stmt (stmt));\n-\t  else\n-\t    bsi_insert_after (si_p, list, BSI_NEW_STMT);\n-\t}\n+  if (lhs_elt->use_block_copy)\n+    {\n+      /* Since LHS is not fully instantiated, we must leave the structure\n+\t assignment in place.  Treating this case differently from a USE\n+\t exposes constants to later optimizations.  */\n+      mark_all_v_defs (expr_first (list));\n+      sra_insert_after (bsi, list);\n+    }\n+  else\n+    {\n+      /* The LHS is fully instantiated.  The list of initializations\n+\t replaces the original structure assignment.  */\n+      mark_all_v_defs (bsi_stmt (*bsi));\n+      sra_replace (bsi, list);\n     }\n }\n \n+/* A subroutine of scalarize_ldst called via walk_tree.  Set TREE_NO_TRAP\n+   on all INDIRECT_REFs.  */\n \n-/* Helper for scalarize_stmt to handle ASM_EXPRs.  */\n-\n-static void\n-scalarize_asm_expr (block_stmt_iterator *si_p)\n+static tree\n+mark_notrap (tree *tp, int *walk_subtrees, void *data ATTRIBUTE_UNUSED)\n {\n-  tree stmt = bsi_stmt (*si_p);\n-  struct bitmap_head_def done_head;\n+  tree t = *tp;\n \n-  bitmap_initialize (&done_head, 1);\n-  scalarize_tree_list (ASM_INPUTS (stmt), si_p, &done_head);\n-  scalarize_tree_list (ASM_OUTPUTS (stmt), si_p, &done_head);\n-  bitmap_clear (&done_head);\n+  if (TREE_CODE (t) == INDIRECT_REF)\n+    {\n+      TREE_THIS_NOTRAP (t) = 1;\n+      *walk_subtrees = 0;\n+    }\n+  else if (DECL_P (t) || TYPE_P (t))\n+    *walk_subtrees = 0;\n \n-  /* ??? Process outputs after the asm.  */\n+  return NULL;\n }\n \n-\n-/* Helper for scalarize_stmt to handle return expressions.  */\n+/* Scalarize a LDST.  To recap, this is an assignment between one scalarizable\n+   reference ELT and one non-scalarizable reference OTHER.  IS_OUTPUT is true\n+   if ELT is on the left-hand side.  */\n \n static void\n-scalarize_return_expr (block_stmt_iterator *si_p)\n+scalarize_ldst (struct sra_elt *elt, tree other,\n+\t\tblock_stmt_iterator *bsi, bool is_output)\n {\n-  tree stmt = bsi_stmt (*si_p);\n-  tree op = TREE_OPERAND (stmt, 0);\n-\n-  if (op == NULL_TREE)\n-    return;\n+  /* Shouldn't have gotten called for a scalar.  */\n+  if (elt->replacement)\n+    abort ();\n \n-  /* Handle a bare RESULT_DECL.  This will handle for types needed\n-     constructors, or possibly after NRV type optimizations.  */\n-  if (is_sra_candidate_decl (op))\n-    emit_scalar_copies (si_p, op, op, FIELD_SCALAR);\n-  else if (TREE_CODE (op) == MODIFY_EXPR)\n+  if (elt->use_block_copy)\n+    {\n+      /* Since ELT is not fully instantiated, we have to leave the\n+\t block copy in place.  Treat this as a USE.  */\n+      scalarize_use (elt, NULL, bsi, is_output);\n+    }\n+  else\n     {\n-      tree *rhs_p = &TREE_OPERAND (op, 1);\n-      tree rhs = *rhs_p;\n+      /* The interesting case is when ELT is fully instantiated.  In this\n+\t case we can have each element stored/loaded directly to/from the\n+\t corresponding slot in OTHER.  This avoids a block copy.  */\n \n-      /* Handle 'return STRUCTURE;'  */\n-      if (is_sra_candidate_decl (rhs))\n-\temit_scalar_copies (si_p, rhs, rhs, FIELD_SCALAR);\n+      tree list = NULL, stmt = bsi_stmt (*bsi);\n \n-      /* Handle 'return STRUCTURE.FIELD;'  */\n-      else if (is_sra_candidate_ref (rhs))\n-\tscalarize_component_ref (stmt, rhs_p);\n+      mark_all_v_defs (stmt);\n+      generate_copy_inout (elt, is_output, other, &list);\n+      if (list == NULL)\n+\tabort ();\n \n-      /* Handle 'return CALL_EXPR;'  */\n-      else if (TREE_CODE (rhs) == CALL_EXPR)\n+      /* Preserve EH semantics.  */\n+      if (stmt_ends_bb_p (stmt))\n \t{\n-\t  struct bitmap_head_def done_head;\n-\t  bitmap_initialize (&done_head, 1);\n-\t  scalarize_tree_list (TREE_OPERAND (rhs, 1), si_p, &done_head);\n-\t  bitmap_clear (&done_head);\n+\t  tree_stmt_iterator tsi;\n+\t  tree first;\n+\n+\t  /* Extract the first statement from LIST.  */\n+\t  tsi = tsi_start (list);\n+\t  first = tsi_stmt (tsi);\n+\t  tsi_delink (&tsi);\n+\n+\t  /* Replace the old statement with this new representative.  */\n+\t  bsi_replace (bsi, first, true);\n+\t  \n+\t  if (!tsi_end_p (tsi))\n+\t    {\n+\t      /* If any reference would trap, then they all would.  And more\n+\t\t to the point, the first would.  Therefore none of the rest\n+\t\t will trap since the first didn't.  Indicate this by\n+\t\t iterating over the remaining statements and set\n+\t\t TREE_THIS_NOTRAP in all INDIRECT_REFs.  */\n+\t      do\n+\t\t{\n+\t\t  walk_tree (tsi_stmt_ptr (tsi), mark_notrap, NULL, NULL);\n+\t\t  tsi_next (&tsi);\n+\t\t}\n+\t      while (!tsi_end_p (tsi));\n+\n+\t      insert_edge_copies (list, bsi->bb);\n+\t    }\n \t}\n+      else\n+\tsra_replace (bsi, list);\n     }\n }\n \n+/* Generate initializations for all scalarizable parameters.  */\n \n-/* Debugging dump for the scalar replacement map.  */\n-\n-static int\n-dump_sra_map_trav (void **slot, void *data)\n+static void\n+scalarize_parms (void)\n {\n-  struct sra_elt *e = *slot;\n-  FILE *f = data;\n+  tree list = NULL;\n+  size_t i;\n \n-  switch (e->kind)\n-    {\n-    case REALPART_EXPR:\n-      fputs (\"__real__ \", f);\n-      print_generic_expr (dump_file, e->base, dump_flags);\n-      fprintf (f, \" -> %s\\n\", get_name (e->replace));\n-      break;\n-    case IMAGPART_EXPR:\n-      fputs (\"__imag__ \", f);\n-      print_generic_expr (dump_file, e->base, dump_flags);\n-      fprintf (f, \" -> %s\\n\", get_name (e->replace));\n-      break;\n-    case COMPONENT_REF:\n-      print_generic_expr (dump_file, e->base, dump_flags);\n-      fprintf (f, \".%s -> %s\\n\", get_name (e->field), get_name (e->replace));\n-      break;\n-    default:\n-      abort ();\n-    }\n+  EXECUTE_IF_SET_IN_BITMAP (needs_copy_in, 0, i,\n+    { \n+      tree var = referenced_var (i);\n+      struct sra_elt *elt = lookup_element (NULL, var, NULL, NO_INSERT);\n+      generate_copy_inout (elt, true, var, &list);\n+    });\n \n-  return 1;\n+  if (list)\n+    insert_edge_copies (list, ENTRY_BLOCK_PTR);\n }\n \n+/* Entry point to phase 4.  Update the function to match replacements.  */\n+\n static void\n-dump_sra_map (FILE *f)\n+scalarize_function (void)\n {\n-  fputs (\"Scalar replacements:\\n\", f);\n-  htab_traverse_noresize (sra_map, dump_sra_map_trav, f);\n-  fputs (\"\\n\\n\", f);\n+  static const struct sra_walk_fns fns = {\n+    scalarize_use, scalarize_copy, scalarize_init, scalarize_ldst, false\n+  };\n+\n+  sra_walk_function (&fns);\n+  scalarize_parms ();\n+  bsi_commit_edge_inserts (NULL);\n }\n \n-/* Main entry point to Scalar Replacement of Aggregates (SRA).  This pass\n-   re-writes non-aliased structure references into scalar temporaries.  The\n-   goal is to expose some/all structures to the scalar optimizers.\n+\f\n+/* Debug helper function.  Print ELT in a nice human-readable format.  */\n \n-   Scalarization proceeds in two main phases.  First, every structure\n-   referenced in the program that complies with can_be_scalarized_p is\n-   marked for scalarization (find_candidates_for_sra).\n-   \n-   Second, a mapping between structure fields and scalar temporaries so\n-   that every time a particular field of a particular structure is\n-   referenced in the code, we replace it with its corresponding scalar\n-   temporary (scalarize_structures).\n+static void\n+dump_sra_elt_name (FILE *f, struct sra_elt *elt)\n+{\n+  if (elt->parent && TREE_CODE (elt->parent->type) == COMPLEX_TYPE)\n+    {\n+      fputs (elt->element == integer_zero_node ? \"__real__ \" : \"__imag__ \", f);\n+      dump_sra_elt_name (f, elt->parent);\n+    }\n+  else\n+    {\n+      if (elt->parent)\n+        dump_sra_elt_name (f, elt->parent);\n+      if (DECL_P (elt->element))\n+\t{\n+\t  if (TREE_CODE (elt->element) == FIELD_DECL)\n+\t    fputc ('.', f);\n+\t  print_generic_expr (f, elt->element, dump_flags);\n+\t}\n+      else\n+\tfprintf (f, \"[\" HOST_WIDE_INT_PRINT_DEC \"]\",\n+\t\t TREE_INT_CST_LOW (elt->element));\n+    }\n+}\n \n-   TODO\n+/* Likewise, but callable from the debugger.  */\n+\n+void\n+debug_sra_elt_name (struct sra_elt *elt)\n+{\n+  dump_sra_elt_name (stderr, elt);\n+  fputc ('\\n', stderr);\n+}\n \n-   1- Scalarize COMPLEX_TYPEs\n-   2- Scalarize ARRAY_REFs that are always referenced with a\n-      constant index.\n-   3- Timings to determine when scalarization is not profitable.\n-   4- Determine what's a good value for MAX_NFIELDS_FOR_SRA.  */\n+/* Main entry point.  */\n \n static void\n tree_sra (void)\n {\n   /* Initialize local variables.  */\n+  gcc_obstack_init (&sra_obstack);\n   sra_candidates = BITMAP_XMALLOC ();\n-  sra_map = NULL;\n-  needs_copy_in = NULL;\n+  needs_copy_in = BITMAP_XMALLOC ();\n+  sra_type_decomp_cache = BITMAP_XMALLOC ();\n+  sra_type_inst_cache = BITMAP_XMALLOC ();\n+  sra_map = htab_create (101, sra_elt_hash, sra_elt_eq, NULL);\n \n-  /* Find structures to be scalarized.  */\n-  if (!find_candidates_for_sra ())\n+  /* Scan.  If we find anything, instantiate and scalarize.  */\n+  if (find_candidates_for_sra ())\n     {\n-      BITMAP_XFREE (sra_candidates);\n-      return;\n+      scan_function ();\n+      decide_instantiations ();\n+      scalarize_function ();\n     }\n \n-  /* If we found any, re-write structure references with their\n-     corresponding scalar replacement.  */\n-  sra_map = htab_create (101, sra_elt_hash, sra_elt_eq, free);\n-  needs_copy_in = BITMAP_XMALLOC ();\n-\n-  scalarize_structures ();\n-\n-  if (dump_file)\n-    dump_sra_map (dump_file);\n-\n   /* Free allocated memory.  */\n   htab_delete (sra_map);\n   sra_map = NULL;\n-  BITMAP_XFREE (needs_copy_in);\n   BITMAP_XFREE (sra_candidates);\n+  BITMAP_XFREE (needs_copy_in);\n+  BITMAP_XFREE (sra_type_decomp_cache);\n+  BITMAP_XFREE (sra_type_inst_cache);\n+  obstack_free (&sra_obstack, NULL);\n }\n \n static bool"}]}