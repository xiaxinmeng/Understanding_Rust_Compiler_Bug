{"sha": "b076a3fd749f2060d33c477c70e18ed24ce83d96", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YjA3NmEzZmQ3NDlmMjA2MGQzM2M0NzdjNzBlMThlZDI0Y2U4M2Q5Ng==", "commit": {"author": {"name": "Zdenek Dvorak", "email": "rakdver@gcc.gnu.org", "date": "2006-02-14T13:51:51Z"}, "committer": {"name": "Zdenek Dvorak", "email": "rakdver@gcc.gnu.org", "date": "2006-02-14T13:51:51Z"}, "message": "Add forgotten file.\n\nFrom-SVN: r110965", "tree": {"sha": "03d9b3555c1a0c318b22f9e8de7ee6c772de99b9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/03d9b3555c1a0c318b22f9e8de7ee6c772de99b9"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/b076a3fd749f2060d33c477c70e18ed24ce83d96", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b076a3fd749f2060d33c477c70e18ed24ce83d96", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b076a3fd749f2060d33c477c70e18ed24ce83d96", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b076a3fd749f2060d33c477c70e18ed24ce83d96/comments", "author": null, "committer": null, "parents": [{"sha": "1768461812d5ccaf9f4a30336e6479eb03a29246", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1768461812d5ccaf9f4a30336e6479eb03a29246", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1768461812d5ccaf9f4a30336e6479eb03a29246"}], "stats": {"total": 1064, "additions": 1064, "deletions": 0}, "files": [{"sha": "911b389ecd2e0bc7d20d45faa410ec6e5f8dcb49", "filename": "gcc/tree-ssa-loop-prefetch.c", "status": "added", "additions": 1064, "deletions": 0, "changes": 1064, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b076a3fd749f2060d33c477c70e18ed24ce83d96/gcc%2Ftree-ssa-loop-prefetch.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b076a3fd749f2060d33c477c70e18ed24ce83d96/gcc%2Ftree-ssa-loop-prefetch.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-loop-prefetch.c?ref=b076a3fd749f2060d33c477c70e18ed24ce83d96", "patch": "@@ -0,0 +1,1064 @@\n+/* Array prefetching.\n+   Copyright (C) 2005 Free Software Foundation, Inc.\n+   \n+This file is part of GCC.\n+   \n+GCC is free software; you can redistribute it and/or modify it\n+under the terms of the GNU General Public License as published by the\n+Free Software Foundation; either version 2, or (at your option) any\n+later version.\n+   \n+GCC is distributed in the hope that it will be useful, but WITHOUT\n+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+   \n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"tree.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"output.h\"\n+#include \"diagnostic.h\"\n+#include \"tree-flow.h\"\n+#include \"tree-dump.h\"\n+#include \"timevar.h\"\n+#include \"cfgloop.h\"\n+#include \"varray.h\"\n+#include \"expr.h\"\n+#include \"tree-pass.h\"\n+#include \"ggc.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"hashtab.h\"\n+#include \"tree-chrec.h\"\n+#include \"tree-scalar-evolution.h\"\n+#include \"toplev.h\"\n+#include \"params.h\"\n+#include \"langhooks.h\"\n+\n+/* This pass inserts prefetch instructions to optimize cache usage during\n+   accesses to arrays in loops.  It processes loops sequentially and:\n+\n+   1) Gathers all memory references in the single loop.\n+   2) For each of the references it decides when it is profitable to prefetch\n+      it.  To do it, we evaluate the reuse among the accesses, and determines\n+      two values: PREFETCH_BEFORE (meaning that it only makes sense to do\n+      prefetching in the first PREFETCH_BEFORE iterations of the loop) and\n+      PREFETCH_MOD (meaning that it only makes sense to prefetch in the\n+      iterations of the loop that are zero modulo PREFETCH_MOD).  For example\n+      (assuming cache line size is 64 bytes, char has size 1 byte and there\n+      is no hardware sequential prefetch):\n+\n+      char *a;\n+      for (i = 0; i < max; i++)\n+\t{\n+\t  a[255] = ...;\t\t(0)\n+\t  a[i] = ...;\t\t(1)\n+\t  a[i + 64] = ...;\t(2)\n+\t  a[16*i] = ...;\t(3)\n+\t  a[187*i] = ...;\t(4)\n+\t  a[187*i + 50] = ...;\t(5)\n+\t}\n+\n+       (0) obviously has PREFETCH_BEFORE 1\n+       (1) has PREFETCH_BEFORE 64, since (2) accesses the same memory\n+           location 64 iterations before it, and PREFETCH_MOD 64 (since\n+\t   it hits the same cache line otherwise).\n+       (2) has PREFETCH_MOD 64\n+       (3) has PREFETCH_MOD 4\n+       (4) has PREFETCH_MOD 1.  We do not set PREFETCH_BEFORE here, since\n+           the cache line accessed by (4) is the same with probability only\n+\t   7/32.\n+       (5) has PREFETCH_MOD 1 as well.\n+\n+   3) We determine how much ahead we need to prefetch.  The number of\n+      iterations needed is time to fetch / time spent in one iteration of\n+      the loop.  The problem is that we do not know either of these values,\n+      so we just make a heuristic guess based on a magic (possibly)\n+      target-specific constant and size of the loop.\n+\n+   4) Determine which of the references we prefetch.  We take into account\n+      that there is a maximum number of simultaneous prefetches (provided\n+      by machine description).  We prefetch as many prefetches as possible\n+      while still within this bound (starting with those with lowest\n+      prefetch_mod, since they are responsible for most of the cache\n+      misses).\n+      \n+   5) We unroll and peel loops so that we are able to satisfy PREFETCH_MOD\n+      and PREFETCH_BEFORE requirements (within some bounds), and to avoid\n+      prefetching nonaccessed memory.\n+      TODO -- actually implement peeling.\n+      \n+   6) We actually emit the prefetch instructions.  ??? Perhaps emit the\n+      prefetch instructions with guards in cases where 5) was not sufficient\n+      to satisfy the constraints?\n+\n+   Some other TODO:\n+      -- write and use more general reuse analysis (that could be also used\n+\t in other cache aimed loop optimizations)\n+      -- make it behave sanely together with the prefetches given by user\n+\t (now we just ignore them; at the very least we should avoid\n+\t optimizing loops in that user put his own prefetches)\n+      -- we assume cache line size alignment of arrays; this could be\n+\t improved.  */\n+\n+/* Magic constants follow.  These should be replaced by machine specific\n+   numbers.  */\n+\n+/* A number that should rouhgly correspond to the number of instructions\n+   executed before the prefetch is completed.  */\n+\n+#ifndef PREFETCH_LATENCY\n+#define PREFETCH_LATENCY 200\n+#endif\n+\n+/* Number of prefetches that can run at the same time.  */\n+\n+#ifndef SIMULTANEOUS_PREFETCHES\n+#define SIMULTANEOUS_PREFETCHES 3\n+#endif\n+\n+/* True if write can be prefetched by a read prefetch.  */\n+\n+#ifndef WRITE_CAN_USE_READ_PREFETCH\n+#define WRITE_CAN_USE_READ_PREFETCH 1\n+#endif\n+\n+/* True if read can be prefetched by a write prefetch. */\n+\n+#ifndef READ_CAN_USE_WRITE_PREFETCH\n+#define READ_CAN_USE_WRITE_PREFETCH 0\n+#endif\n+\n+/* Cache line size.  Assumed to be a power of two.  */\n+\n+#ifndef PREFETCH_BLOCK\n+#define PREFETCH_BLOCK 32\n+#endif\n+\n+/* Do we have a forward hardware sequential prefetching?  */\n+\n+#ifndef HAVE_FORWARD_PREFETCH\n+#define HAVE_FORWARD_PREFETCH 0\n+#endif\n+\n+/* Do we have a backward hardware sequential prefetching?  */\n+\n+#ifndef HAVE_BACKWARD_PREFETCH\n+#define HAVE_BACKWARD_PREFETCH 0\n+#endif\n+\n+/* In some cases we are only able to determine that there is a certain\n+   probability that the two accesses hit the same cache line.  In this\n+   case, we issue the prefetches for both of them if this probability\n+   is less then (1000 - ACCEPTABLE_MISS_RATE) promile.  */\n+\n+#ifndef ACCEPTABLE_MISS_RATE\n+#define ACCEPTABLE_MISS_RATE 50\n+#endif\n+\n+#ifndef HAVE_prefetch\n+#define HAVE_prefetch 0\n+#endif\n+\n+/* The group of references between that reuse may occur.  */\n+\n+struct mem_ref_group\n+{\n+  tree base;\t\t\t/* Base of the reference.  */\n+  HOST_WIDE_INT step;\t\t/* Step of the reference.  */\n+  struct mem_ref *refs;\t\t/* References in the group.  */\n+  struct mem_ref_group *next;\t/* Next group of references.  */\n+};\n+\n+/* Assigned to PREFETCH_BEFORE when all iterations are to be prefetched.  */\n+\n+#define PREFETCH_ALL\t\t(~(unsigned HOST_WIDE_INT) 0)\n+\n+/* The memory reference.  */\n+\n+struct mem_ref\n+{\n+  tree stmt;\t\t\t/* Statement in that the reference appears.  */\n+  tree mem;\t\t\t/* The reference.  */\n+  HOST_WIDE_INT delta;\t\t/* Constant offset of the reference.  */\n+  bool write_p;\t\t\t/* Is it a write?  */\n+  struct mem_ref_group *group;\t/* The group of references it belongs to.  */\n+  unsigned HOST_WIDE_INT prefetch_mod;\n+\t\t\t\t/* Prefetch only each PREFETCH_MOD-th\n+\t\t\t\t   iteration.  */\n+  unsigned HOST_WIDE_INT prefetch_before;\n+\t\t\t\t/* Prefetch only first PREFETCH_BEFORE\n+\t\t\t\t   iterations.  */\n+  bool issue_prefetch_p;\t/* Should we really issue the prefetch?  */\n+  struct mem_ref *next;\t\t/* The next reference in the group.  */\n+};\n+\n+/* Dumps information obout reference REF to FILE.  */\n+\n+static void\n+dump_mem_ref (FILE *file, struct mem_ref *ref)\n+{\n+  fprintf (file, \"Reference %p:\\n\", (void *) ref);\n+\n+  fprintf (file, \"  group %p (base \", (void *) ref->group);\n+  print_generic_expr (file, ref->group->base, TDF_SLIM);\n+  fprintf (file, \", step \");\n+  fprintf (file, HOST_WIDE_INT_PRINT_DEC, ref->group->step);\n+  fprintf (file, \")\\n\");\n+\n+  fprintf (dump_file, \"  delta \");\n+  fprintf (file, HOST_WIDE_INT_PRINT_DEC, ref->delta);\n+  fprintf (file, \"\\n\");\n+\n+  fprintf (file, \"  %s\\n\", ref->write_p ? \"write\" : \"read\");\n+\n+  fprintf (file, \"\\n\");\n+}\n+\n+/* Finds a group with BASE and STEP in GROUPS, or creates one if it does not\n+   exist.  */\n+\n+static struct mem_ref_group *\n+find_or_create_group (struct mem_ref_group **groups, tree base,\n+\t\t      HOST_WIDE_INT step)\n+{\n+  struct mem_ref_group *group;\n+\n+  for (; *groups; groups = &(*groups)->next)\n+    {\n+      if ((*groups)->step == step\n+\t  && operand_equal_p ((*groups)->base, base, 0))\n+\treturn *groups;\n+\n+      /* Keep the list of groups sorted by decreasing step.  */\n+      if ((*groups)->step < step)\n+\tbreak;\n+    }\n+\n+  group = xcalloc (1, sizeof (struct mem_ref_group));\n+  group->base = base;\n+  group->step = step;\n+  group->refs = NULL;\n+  group->next = *groups;\n+  *groups = group;\n+\n+  return group;\n+}\n+\n+/* Records a memory reference MEM in GROUP with offset DELTA and write status\n+   WRITE_P.  The reference occurs in statement STMT.  */\n+\n+static void\n+record_ref (struct mem_ref_group *group, tree stmt, tree mem,\n+\t    HOST_WIDE_INT delta, bool write_p)\n+{\n+  struct mem_ref **aref;\n+\n+  /* Do not record the same address twice.  */\n+  for (aref = &group->refs; *aref; aref = &(*aref)->next)\n+    {\n+      /* It does not have to be possible for write reference to reuse the read\n+\t prefetch, or vice versa.  */\n+      if (!WRITE_CAN_USE_READ_PREFETCH\n+\t  && write_p\n+\t  && !(*aref)->write_p)\n+\tcontinue;\n+      if (!READ_CAN_USE_WRITE_PREFETCH\n+\t  && !write_p\n+\t  && (*aref)->write_p)\n+\tcontinue;\n+\n+      if ((*aref)->delta == delta)\n+\treturn;\n+    }\n+\n+  (*aref) = xcalloc (1, sizeof (struct mem_ref));\n+  (*aref)->stmt = stmt;\n+  (*aref)->mem = mem;\n+  (*aref)->delta = delta;\n+  (*aref)->write_p = write_p;\n+  (*aref)->prefetch_before = PREFETCH_ALL;\n+  (*aref)->prefetch_mod = 1;\n+  (*aref)->issue_prefetch_p = false;\n+  (*aref)->group = group;\n+  (*aref)->next = NULL;\n+\n+  if (dump_file && (dump_flags & TDF_DETAILS))\n+    dump_mem_ref (dump_file, *aref);\n+}\n+\n+/* Release memory references in GROUPS.  */\n+\n+static void\n+release_mem_refs (struct mem_ref_group *groups)\n+{\n+  struct mem_ref_group *next_g;\n+  struct mem_ref *ref, *next_r;\n+\n+  for (; groups; groups = next_g)\n+    {\n+      next_g = groups->next;\n+      for (ref = groups->refs; ref; ref = next_r)\n+\t{\n+\t  next_r = ref->next;\n+\t  free (ref);\n+\t}\n+      free (groups);\n+    }\n+}\n+\n+/* A structure used to pass arguments to idx_analyze_ref.  */\n+\n+struct ar_data\n+{\n+  struct loop *loop;\t\t\t/* Loop of the reference.  */\n+  tree stmt;\t\t\t\t/* Statement of the reference.  */\n+  HOST_WIDE_INT *step;\t\t\t/* Step of the memory reference.  */\n+  HOST_WIDE_INT *delta;\t\t\t/* Offset of the memory reference.  */\n+};\n+\n+/* Analyzes a single INDEX of a memory reference to obtain information\n+   described at analyze_ref.  Callback for for_each_index.  */\n+\n+static bool\n+idx_analyze_ref (tree base, tree *index, void *data)\n+{\n+  struct ar_data *ar_data = data;\n+  tree ibase, step, stepsize;\n+  HOST_WIDE_INT istep, idelta = 0, imult = 1;\n+  affine_iv iv;\n+\n+  if (TREE_CODE (base) == MISALIGNED_INDIRECT_REF\n+      || TREE_CODE (base) == ALIGN_INDIRECT_REF)\n+    return false;\n+\n+  if (!simple_iv (ar_data->loop, ar_data->stmt, *index, &iv, false))\n+    return false;\n+  ibase = iv.base;\n+  step = iv.step;\n+\n+  if (zero_p (step))\n+    istep = 0;\n+  else\n+    {\n+      if (!cst_and_fits_in_hwi (step))\n+\treturn false;\n+      istep = int_cst_value (step);\n+    }\n+\n+  if (TREE_CODE (ibase) == PLUS_EXPR\n+      && cst_and_fits_in_hwi (TREE_OPERAND (ibase, 1)))\n+    {\n+      idelta = int_cst_value (TREE_OPERAND (ibase, 1));\n+      ibase = TREE_OPERAND (ibase, 0);\n+    }\n+  if (cst_and_fits_in_hwi (ibase))\n+    {\n+      idelta += int_cst_value (ibase);\n+      ibase = build_int_cst_type (TREE_TYPE (ibase), 0);\n+    }\n+\n+  if (TREE_CODE (base) == ARRAY_REF)\n+    {\n+      stepsize = array_ref_element_size (base);\n+      if (!cst_and_fits_in_hwi (stepsize))\n+\treturn false;\n+      imult = int_cst_value (stepsize);\n+\n+      istep *= imult;\n+      idelta *= imult;\n+    }\n+\n+  *ar_data->step += istep;\n+  *ar_data->delta += idelta;\n+  *index = ibase;\n+\n+  return true;\n+}\n+\n+/* Tries to express REF in shape &BASE + STEP * iter + DELTA, where DELTA and\n+   STEP are integer constants and iter is number of iterations of LOOP.  The\n+   reference occurs in statement STMT.  */\n+\n+static bool\n+analyze_ref (struct loop *loop, tree ref, tree *base,\n+\t     HOST_WIDE_INT *step, HOST_WIDE_INT *delta,\n+\t     tree stmt)\n+{\n+  struct ar_data ar_data;\n+  tree off;\n+  HOST_WIDE_INT bit_offset;\n+\n+  *step = 0;\n+  *delta = 0;\n+\n+  /* First strip off the component references.  Ignore bitfields.  */\n+  if (TREE_CODE (ref) == COMPONENT_REF\n+      && DECL_NONADDRESSABLE_P (TREE_OPERAND (ref, 1)))\n+    ref = TREE_OPERAND (ref, 0);\n+\n+  for (; TREE_CODE (ref) == COMPONENT_REF; ref = TREE_OPERAND (ref, 0))\n+    {\n+      off = DECL_FIELD_BIT_OFFSET (TREE_OPERAND (ref, 1));\n+      bit_offset = TREE_INT_CST_LOW (off);\n+      gcc_assert (bit_offset % BITS_PER_UNIT == 0);\n+      \n+      *delta += bit_offset / BITS_PER_UNIT;\n+    }\n+\n+  *base = unshare_expr (ref);\n+  ar_data.loop = loop;\n+  ar_data.stmt = stmt;\n+  ar_data.step = step;\n+  ar_data.delta = delta;\n+  return for_each_index (base, idx_analyze_ref, &ar_data);\n+}\n+\n+/* Record a memory reference REF to the list REFS.  The reference occurs in\n+   LOOP in statement STMT and it is write if WRITE_P.  */\n+\n+static void\n+gather_memory_references_ref (struct loop *loop, struct mem_ref_group **refs,\n+\t\t\t      tree ref, bool write_p, tree stmt)\n+{\n+  tree base;\n+  HOST_WIDE_INT step, delta;\n+  struct mem_ref_group *agrp;\n+\n+  if (!analyze_ref (loop, ref, &base, &step, &delta, stmt))\n+    return;\n+\n+  /* Now we know that REF = &BASE + STEP * iter + DELTA, where DELTA and STEP\n+     are integer constants.  */\n+  agrp = find_or_create_group (refs, base, step);\n+  record_ref (agrp, stmt, ref, delta, write_p);\n+}\n+\n+/* Record the suitable memory references in LOOP.  */\n+\n+static struct mem_ref_group *\n+gather_memory_references (struct loop *loop)\n+{\n+  basic_block *body = get_loop_body_in_dom_order (loop);\n+  basic_block bb;\n+  unsigned i;\n+  block_stmt_iterator bsi;\n+  tree stmt, lhs, rhs;\n+  struct mem_ref_group *refs = NULL;\n+\n+  /* Scan the loop body in order, so that the former references precede the\n+     later ones.  */\n+  for (i = 0; i < loop->num_nodes; i++)\n+    {\n+      bb = body[i];\n+      if (bb->loop_father != loop)\n+\tcontinue;\n+\n+      for (bsi = bsi_start (bb); !bsi_end_p (bsi); bsi_next (&bsi))\n+\t{\n+\t  stmt = bsi_stmt (bsi);\n+\t  if (TREE_CODE (stmt) != MODIFY_EXPR)\n+\t    continue;\n+\n+\t  lhs = TREE_OPERAND (stmt, 0);\n+\t  rhs = TREE_OPERAND (stmt, 1);\n+\n+\t  if (REFERENCE_CLASS_P (rhs))\n+\t    gather_memory_references_ref (loop, &refs, rhs, false, stmt);\n+\t  if (REFERENCE_CLASS_P (lhs))\n+\t    gather_memory_references_ref (loop, &refs, lhs, true, stmt);\n+\t}\n+    }\n+  free (body);\n+\n+  return refs;\n+}\n+\n+/* Prune the prefetch candidate REF using the self-reuse.  */\n+\n+static void\n+prune_ref_by_self_reuse (struct mem_ref *ref)\n+{\n+  HOST_WIDE_INT step = ref->group->step;\n+  bool backward = step < 0;\n+\n+  if (step == 0)\n+    {\n+      /* Prefetch references to invariant address just once.  */\n+      ref->prefetch_before = 1;\n+      return;\n+    }\n+\n+  if (backward)\n+    step = -step;\n+\n+  if (step > PREFETCH_BLOCK)\n+    return;\n+\n+  if ((backward && HAVE_BACKWARD_PREFETCH)\n+      || (!backward && HAVE_FORWARD_PREFETCH))\n+    {\n+      ref->prefetch_before = 1;\n+      return;\n+    }\n+\n+  ref->prefetch_mod = PREFETCH_BLOCK / step;\n+}\n+\n+/* Divides X by BY, rounding down.  */\n+\n+static HOST_WIDE_INT\n+ddown (HOST_WIDE_INT x, unsigned HOST_WIDE_INT by)\n+{\n+  gcc_assert (by > 0);\n+\n+  if (x >= 0)\n+    return x / by;\n+  else\n+    return (x + by - 1) / by;\n+}\n+\n+/* Prune the prefetch candidate REF using the reuse with BY.\n+   If BY_IS_BEFORE is true, BY is before REF in the loop.  */\n+\n+static void\n+prune_ref_by_group_reuse (struct mem_ref *ref, struct mem_ref *by,\n+\t\t\t  bool by_is_before)\n+{\n+  HOST_WIDE_INT step = ref->group->step;\n+  bool backward = step < 0;\n+  HOST_WIDE_INT delta_r = ref->delta, delta_b = by->delta;\n+  HOST_WIDE_INT delta = delta_b - delta_r;\n+  HOST_WIDE_INT hit_from;\n+  unsigned HOST_WIDE_INT prefetch_before, prefetch_block;\n+\n+  if (delta == 0)\n+    {\n+      /* If the references has the same address, only prefetch the\n+\t former.  */\n+      if (by_is_before)\n+\tref->prefetch_before = 0;\n+      \n+      return;\n+    }\n+\n+  if (!step)\n+    {\n+      /* If the reference addresses are invariant and fall into the\n+\t same cache line, prefetch just the first one.  */\n+      if (!by_is_before)\n+\treturn;\n+\n+      if (ddown (ref->delta, PREFETCH_BLOCK)\n+\t  != ddown (by->delta, PREFETCH_BLOCK))\n+\treturn;\n+\n+      ref->prefetch_before = 0;\n+      return;\n+    }\n+\n+  /* Only prune the reference that is behind in the array.  */\n+  if (backward)\n+    {\n+      if (delta > 0)\n+\treturn;\n+\n+      /* Transform the data so that we may assume that the accesses\n+\t are forward.  */\n+      delta = - delta;\n+      step = -step;\n+      delta_r = PREFETCH_BLOCK - 1 - delta_r;\n+      delta_b = PREFETCH_BLOCK - 1 - delta_b;\n+    }\n+  else\n+    {\n+      if (delta < 0)\n+\treturn;\n+    }\n+\n+  /* Check whether the two references are likely to hit the same cache\n+     line, and how distant the iterations in that it occurs are from\n+     each other.  */\n+\n+  if (step <= PREFETCH_BLOCK)\n+    {\n+      /* The accesses are sure to meet.  Let us check when.  */\n+      hit_from = ddown (delta_b, PREFETCH_BLOCK) * PREFETCH_BLOCK;\n+      prefetch_before = (hit_from - delta_r + step - 1) / step;\n+\n+      if (prefetch_before < ref->prefetch_before)\n+\tref->prefetch_before = prefetch_before;\n+\n+      return;\n+    }\n+\n+  /* A more complicated case.  First let us ensure that size of cache line\n+     and step are coprime (here we assume that PREFETCH_BLOCK is a power\n+     of two.  */\n+  prefetch_block = PREFETCH_BLOCK;\n+  while ((step & 1) == 0\n+\t && prefetch_block > 1)\n+    {\n+      step >>= 1;\n+      prefetch_block >>= 1;\n+      delta >>= 1;\n+    }\n+\n+  /* Now step > prefetch_block, and step and prefetch_block are coprime.\n+     Determine the probability that the accesses hit the same cache line.  */\n+\n+  prefetch_before = delta / step;\n+  delta %= step;\n+  if ((unsigned HOST_WIDE_INT) delta\n+      <= (prefetch_block * ACCEPTABLE_MISS_RATE / 1000))\n+    {\n+      if (prefetch_before < ref->prefetch_before)\n+\tref->prefetch_before = prefetch_before;\n+\n+      return;\n+    }\n+\n+  /* Try also the following iteration.  */\n+  prefetch_before++;\n+  delta = step - delta;\n+  if ((unsigned HOST_WIDE_INT) delta\n+      <= (prefetch_block * ACCEPTABLE_MISS_RATE / 1000))\n+    {\n+      if (prefetch_before < ref->prefetch_before)\n+\tref->prefetch_before = prefetch_before;\n+\n+      return;\n+    }\n+\n+  /* The ref probably does not reuse by.  */\n+  return;\n+}\n+\n+/* Prune the prefetch candidate REF using the reuses with other references\n+   in REFS.  */\n+\n+static void\n+prune_ref_by_reuse (struct mem_ref *ref, struct mem_ref *refs)\n+{\n+  struct mem_ref *prune_by;\n+  bool before = true;\n+\n+  prune_ref_by_self_reuse (ref);\n+\n+  for (prune_by = refs; prune_by; prune_by = prune_by->next)\n+    {\n+      if (prune_by == ref)\n+\t{\n+\t  before = false;\n+\t  continue;\n+\t}\n+\n+      if (!WRITE_CAN_USE_READ_PREFETCH\n+\t  && ref->write_p\n+\t  && !prune_by->write_p)\n+\tcontinue;\n+      if (!READ_CAN_USE_WRITE_PREFETCH\n+\t  && !ref->write_p\n+\t  && prune_by->write_p)\n+\tcontinue;\n+\n+      prune_ref_by_group_reuse (ref, prune_by, before);\n+    }\n+}\n+\n+/* Prune the prefetch candidates in GROUP using the reuse analysis.  */\n+\n+static void\n+prune_group_by_reuse (struct mem_ref_group *group)\n+{\n+  struct mem_ref *ref_pruned;\n+\n+  for (ref_pruned = group->refs; ref_pruned; ref_pruned = ref_pruned->next)\n+    {\n+      prune_ref_by_reuse (ref_pruned, group->refs);\n+\n+      if (dump_file && (dump_flags & TDF_DETAILS))\n+\t{\n+\t  fprintf (dump_file, \"Reference %p:\", (void *) ref_pruned);\n+\n+\t  if (ref_pruned->prefetch_before == PREFETCH_ALL\n+\t      && ref_pruned->prefetch_mod == 1)\n+\t    fprintf (dump_file, \" no restrictions\");\n+\t  else if (ref_pruned->prefetch_before == 0)\n+\t    fprintf (dump_file, \" do not prefetch\");\n+\t  else if (ref_pruned->prefetch_before <= ref_pruned->prefetch_mod)\n+\t    fprintf (dump_file, \" prefetch once\");\n+\t  else\n+\t    {\n+\t      if (ref_pruned->prefetch_before != PREFETCH_ALL)\n+\t\t{\n+\t\t  fprintf (dump_file, \" prefetch before \");\n+\t\t  fprintf (dump_file, HOST_WIDE_INT_PRINT_DEC,\n+\t\t\t   ref_pruned->prefetch_before);\n+\t\t}\n+\t      if (ref_pruned->prefetch_mod != 1)\n+\t\t{\n+\t\t  fprintf (dump_file, \" prefetch mod \");\n+\t\t  fprintf (dump_file, HOST_WIDE_INT_PRINT_DEC,\n+\t\t\t   ref_pruned->prefetch_mod);\n+\t\t}\n+\t    }\n+\t  fprintf (dump_file, \"\\n\");\n+\t}\n+    }\n+}\n+\n+/* Prune the list of prefetch candidates GROUPS using the reuse analysis.  */\n+\n+static void\n+prune_by_reuse (struct mem_ref_group *groups)\n+{\n+  for (; groups; groups = groups->next)\n+    prune_group_by_reuse (groups);\n+}\n+\n+/* Returns true if we should issue prefetch for REF.  */\n+\n+static bool\n+should_issue_prefetch_p (struct mem_ref *ref)\n+{\n+  /* For now do not issue prefetches for only first few of the\n+     iterations.  */\n+  if (ref->prefetch_before != PREFETCH_ALL)\n+    return false;\n+\n+  return true;\n+}\n+\n+/* Decide which of the prefetch candidates in GROUPS to prefetch.\n+   AHEAD is the number of iterations to prefetch ahead (which corresponds\n+   to the number of simultaneous instances of one prefetch running at a\n+   time).  UNROLL_FACTOR is the factor by that the loop is going to be\n+   unrolled.  Returns true if there is anything to prefetch.  */\n+\n+static bool\n+schedule_prefetches (struct mem_ref_group *groups, unsigned unroll_factor,\n+\t\t     unsigned ahead)\n+{\n+  unsigned max_prefetches, n_prefetches;\n+  struct mem_ref *ref;\n+  bool any = false;\n+\n+  max_prefetches = (SIMULTANEOUS_PREFETCHES * unroll_factor) / ahead;\n+  if (max_prefetches > (unsigned) SIMULTANEOUS_PREFETCHES)\n+    max_prefetches = SIMULTANEOUS_PREFETCHES;\n+\n+  if (dump_file && (dump_flags & TDF_DETAILS))\n+    fprintf (dump_file, \"Max prefetches to issue: %d.\\n\", max_prefetches);\n+\n+  if (!max_prefetches)\n+    return false;\n+\n+  /* For now we just take memory references one by one and issue\n+     prefetches for as many as possible.  The groups are sorted\n+     starting with the largest step, since the references with\n+     large step are more likely to cause many cache mises.  */\n+\n+  for (; groups; groups = groups->next)\n+    for (ref = groups->refs; ref; ref = ref->next)\n+      {\n+\tif (!should_issue_prefetch_p (ref))\n+\t  continue;\n+\n+\tref->issue_prefetch_p = true;\n+\n+\t/* If prefetch_mod is less then unroll_factor, we need to insert\n+\t   several prefetches for the reference.  */\n+\tn_prefetches = ((unroll_factor + ref->prefetch_mod - 1)\n+\t\t\t/ ref->prefetch_mod);\n+\tif (max_prefetches <= n_prefetches)\n+\t  return true;\n+\n+\tmax_prefetches -= n_prefetches;\n+\tany = true;\n+      }\n+\n+  return any;\n+}\n+\n+/* Determine whether there is any reference suitable for prefetching\n+   in GROUPS.  */\n+\n+static bool\n+anything_to_prefetch_p (struct mem_ref_group *groups)\n+{\n+  struct mem_ref *ref;\n+\n+  for (; groups; groups = groups->next)\n+    for (ref = groups->refs; ref; ref = ref->next)\n+      if (should_issue_prefetch_p (ref))\n+\treturn true;\n+\n+  return false;\n+}\n+\n+/* Issue prefetches for the reference REF into loop as decided before.\n+   HEAD is the number of iterations to prefetch ahead.  UNROLL_FACTOR\n+   is the factor by thet LOOP was unrolled.  */\n+\n+static void\n+issue_prefetch_ref (struct mem_ref *ref, unsigned unroll_factor, unsigned ahead)\n+{\n+  HOST_WIDE_INT delta;\n+  tree addr, addr_base, prefetch, params, write_p;\n+  block_stmt_iterator bsi;\n+  unsigned n_prefetches, ap;\n+\n+  if (dump_file && (dump_flags & TDF_DETAILS))\n+    fprintf (dump_file, \"Issued prefetch for %p.\\n\", (void *) ref);\n+\n+  bsi = bsi_for_stmt (ref->stmt);\n+\n+  n_prefetches = ((unroll_factor + ref->prefetch_mod - 1)\n+\t\t  / ref->prefetch_mod);\n+  addr_base = build_fold_addr_expr_with_type (ref->mem, ptr_type_node);\n+  addr_base = force_gimple_operand_bsi (&bsi, unshare_expr (addr_base), true, NULL);\n+\n+  for (ap = 0; ap < n_prefetches; ap++)\n+    {\n+      /* Determine the address to prefetch.  */\n+      delta = (ahead + ap * ref->prefetch_mod) * ref->group->step;\n+      addr = fold_build2 (PLUS_EXPR, ptr_type_node,\n+\t\t\t  addr_base, build_int_cst (ptr_type_node, delta));\n+      addr = force_gimple_operand_bsi (&bsi, unshare_expr (addr), true, NULL);\n+\n+      /* Create the prefetch instruction.  */\n+      write_p = ref->write_p ? integer_one_node : integer_zero_node;\n+      params = tree_cons (NULL_TREE, addr,\n+\t\t\t  tree_cons (NULL_TREE, write_p, NULL_TREE));\n+\t\t\t\t \n+      prefetch = build_function_call_expr (built_in_decls[BUILT_IN_PREFETCH],\n+\t\t\t\t\t   params);\n+      bsi_insert_before (&bsi, prefetch, BSI_SAME_STMT);\n+    }\n+}\n+\n+/* Issue prefetches for the references in GROUPS into loop as decided before.\n+   HEAD is the number of iterations to prefetch ahead.  UNROLL_FACTOR is the\n+   factor by that LOOP was unrolled.  */\n+\n+static void\n+issue_prefetches (struct mem_ref_group *groups,\n+\t\t  unsigned unroll_factor, unsigned ahead)\n+{\n+  struct mem_ref *ref;\n+\n+  for (; groups; groups = groups->next)\n+    for (ref = groups->refs; ref; ref = ref->next)\n+      if (ref->issue_prefetch_p)\n+\tissue_prefetch_ref (ref, unroll_factor, ahead);\n+}\n+\n+/* Determines whether we can profitably unroll LOOP FACTOR times, and if\n+   this is the case, fill in DESC by the description of number of\n+   iterations.  */\n+\n+static bool\n+should_unroll_loop_p (struct loop *loop, struct tree_niter_desc *desc,\n+\t\t      unsigned factor)\n+{\n+  if (!can_unroll_loop_p (loop, factor, desc))\n+    return false;\n+\n+  /* We only consider loops without control flow for unrolling.  This is not\n+     a hard restriction -- tree_unroll_loop works with arbitrary loops\n+     as well; but the unrolling/prefetching is usually more profitable for\n+     loops consisting of a single basic block, and we want to limit the\n+     code growth.  */\n+  if (loop->num_nodes > 2)\n+    return false;\n+\n+  return true;\n+}\n+\n+/* Determine the coefficient by that unroll LOOP, from the information\n+   contained in the list of memory references REFS.  Description of\n+   umber of iterations of LOOP is stored to DESC.  AHEAD is the number\n+   of iterations ahead that we need to prefetch.  NINSNS is number of\n+   insns of the LOOP.  */\n+\n+static unsigned\n+determine_unroll_factor (struct loop *loop, struct mem_ref_group *refs,\n+\t\t\t unsigned ahead, unsigned ninsns,\n+\t\t\t struct tree_niter_desc *desc)\n+{\n+  unsigned upper_bound, size_factor, constraint_factor;\n+  unsigned factor, max_mod_constraint, ahead_factor;\n+  struct mem_ref_group *agp;\n+  struct mem_ref *ref;\n+\n+  upper_bound = PARAM_VALUE (PARAM_MAX_UNROLL_TIMES);\n+\n+  /* First check whether the loop is not too large to unroll.  */\n+  size_factor = PARAM_VALUE (PARAM_MAX_UNROLLED_INSNS) / ninsns;\n+  if (size_factor <= 1)\n+    return 1;\n+\n+  if (size_factor < upper_bound)\n+    upper_bound = size_factor;\n+\n+  max_mod_constraint = 1;\n+  for (agp = refs; agp; agp = agp->next)\n+    for (ref = agp->refs; ref; ref = ref->next)\n+      if (should_issue_prefetch_p (ref)\n+\t  && ref->prefetch_mod > max_mod_constraint)\n+\tmax_mod_constraint = ref->prefetch_mod;\n+\n+  /* Set constraint_factor as large as needed to be able to satisfy the\n+     largest modulo constraint.  */\n+  constraint_factor = max_mod_constraint;\n+\n+  /* If ahead is too large in comparison with the number of available\n+     prefetches, unroll the loop as much as needed to be able to prefetch\n+     at least partially some of the references in the loop.  */\n+  ahead_factor = ((ahead + SIMULTANEOUS_PREFETCHES - 1)\n+\t\t  / SIMULTANEOUS_PREFETCHES);\n+\n+  /* Unroll as much as useful, but bound the code size growth.  */\n+  if (constraint_factor < ahead_factor)\n+    factor = ahead_factor;\n+  else\n+    factor = constraint_factor;\n+  if (factor > upper_bound)\n+    factor = upper_bound;\n+\n+  if (!should_unroll_loop_p (loop, desc, factor))\n+    return 1;\n+\n+  return factor;\n+}\n+\n+/* Issue prefetch instructions for array references in LOOP.  Returns\n+   true if the LOOP was unrolled.  LOOPS is the array containing all\n+   loops.  */\n+\n+static bool\n+loop_prefetch_arrays (struct loops *loops, struct loop *loop)\n+{\n+  struct mem_ref_group *refs;\n+  unsigned ahead, ninsns, unroll_factor;\n+  struct tree_niter_desc desc;\n+  bool unrolled = false;\n+\n+  /* Step 1: gather the memory references.  */\n+  refs = gather_memory_references (loop);\n+\n+  /* Step 2: estimate the reuse effects.  */\n+  prune_by_reuse (refs);\n+\n+  if (!anything_to_prefetch_p (refs))\n+    goto fail;\n+\n+  /* Step 3: determine the ahead and unroll factor.  */\n+\n+  /* FIXME: We should use not size of the loop, but the average number of\n+     instructions executed per iteration of the loop.  */\n+  ninsns = tree_num_loop_insns (loop);\n+  ahead = (PREFETCH_LATENCY + ninsns - 1) / ninsns;\n+  unroll_factor = determine_unroll_factor (loop, refs, ahead, ninsns,\n+\t\t\t\t\t   &desc);\n+  if (dump_file && (dump_flags & TDF_DETAILS))\n+    fprintf (dump_file, \"Ahead %d, unroll factor %d\\n\", ahead, unroll_factor);\n+\n+  /* If the loop rolls less than the required unroll factor, prefetching\n+     is useless.  */\n+  if (unroll_factor > 1\n+      && cst_and_fits_in_hwi (desc.niter)\n+      && (unsigned HOST_WIDE_INT) int_cst_value (desc.niter) < unroll_factor)\n+    goto fail;\n+\n+  /* Step 4: what to prefetch?  */\n+  if (!schedule_prefetches (refs, unroll_factor, ahead))\n+    goto fail;\n+\n+  /* Step 5: unroll the loop.  TODO -- peeling of first and last few\n+     iterations so that we do not issue superfluous prefetches.  */\n+  if (unroll_factor != 1)\n+    {\n+      tree_unroll_loop (loops, loop, unroll_factor,\n+\t\t\tsingle_dom_exit (loop), &desc);\n+      unrolled = true;\n+    }\n+\n+  /* Step 6: issue the prefetches.  */\n+  issue_prefetches (refs, unroll_factor, ahead);\n+\n+fail:\n+  release_mem_refs (refs);\n+  return unrolled;\n+}\n+\n+/* Issue prefetch instructions for array references in LOOPS.  */\n+\n+void\n+tree_ssa_prefetch_arrays (struct loops *loops)\n+{\n+  unsigned i;\n+  struct loop *loop;\n+  bool unrolled = false;\n+\n+  if (!HAVE_prefetch\n+      /* It is possible to ask compiler for say -mtune=i486 -march=pentium4.\n+\t -mtune=i486 causes us having PREFETCH_BLOCK 0, since this is part\n+\t of processor costs and i486 does not have prefetch, but\n+\t -march=pentium4 causes HAVE_prefetch to be true.  Ugh.  */\n+      || PREFETCH_BLOCK == 0)\n+    return;\n+\n+  initialize_original_copy_tables ();\n+\n+  if (!built_in_decls[BUILT_IN_PREFETCH])\n+    {\n+      tree type = build_function_type (void_type_node,\n+\t\t\t\t       tree_cons (NULL_TREE,\n+\t\t\t\t\t\t  const_ptr_type_node,\n+\t\t\t\t\t\t  NULL_TREE));\n+      tree decl = lang_hooks.builtin_function (\"__builtin_prefetch\", type,\n+\t\t\tBUILT_IN_PREFETCH, BUILT_IN_NORMAL,\n+\t\t\tNULL, NULL_TREE);\n+      DECL_IS_NOVOPS (decl) = true;\n+      built_in_decls[BUILT_IN_PREFETCH] = decl;\n+    }\n+\n+  /* We assume that size of cache line is a power of two, so verify this\n+     here.  */\n+  gcc_assert ((PREFETCH_BLOCK & (PREFETCH_BLOCK - 1)) == 0);\n+\n+  for (i = loops->num - 1; i > 0; i--)\n+    {\n+      loop = loops->parray[i];\n+\n+      if (dump_file && (dump_flags & TDF_DETAILS))\n+\tfprintf (dump_file, \"Processing loop %d:\\n\", loop->num);\n+\n+      if (loop)\n+\tunrolled |= loop_prefetch_arrays (loops, loop);\n+\n+      if (dump_file && (dump_flags & TDF_DETAILS))\n+\tfprintf (dump_file, \"\\n\\n\");\n+    }\n+\n+  if (unrolled)\n+    {\n+      scev_reset ();\n+      cleanup_tree_cfg_loop ();\n+    }\n+\n+  free_original_copy_tables ();\n+}"}]}