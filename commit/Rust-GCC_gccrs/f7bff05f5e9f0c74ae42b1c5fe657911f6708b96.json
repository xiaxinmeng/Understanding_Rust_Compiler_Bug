{"sha": "f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "node_id": "C_kwDOANBUbNoAKGY3YmZmMDVmNWU5ZjBjNzRhZTQyYjFjNWZlNjU3OTExZjY3MDhiOTY", "commit": {"author": {"name": "Ju-Zhe Zhong", "email": "juzhe.zhong@rivai.ai", "date": "2023-02-03T07:35:52Z"}, "committer": {"name": "Kito Cheng", "email": "kito.cheng@sifive.com", "date": "2023-02-10T11:27:04Z"}, "message": "RISC-V: Add vadd.vx C API tests\n\ngcc/testsuite/ChangeLog:\n\n\t* gcc.target/riscv/rvv/base/vadd_vx_m_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_m_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_m_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_m_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_m_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_m_rv64-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_mu_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_mu_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_mu_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_mu_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_mu_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_mu_rv64-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_rv64-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tu_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tu_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tu_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tu_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tu_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tu_rv64-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tum_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tum_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tum_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tum_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tum_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tum_rv64-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tumu_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tumu_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tumu_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tumu_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tumu_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vadd_vx_tumu_rv64-3.c: New test.", "tree": {"sha": "c1f1905db879a841fe252b8315be5bcee5333eef", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/c1f1905db879a841fe252b8315be5bcee5333eef"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/comments", "author": {"login": "zhongjuzhe", "id": 66454988, "node_id": "MDQ6VXNlcjY2NDU0OTg4", "avatar_url": "https://avatars.githubusercontent.com/u/66454988?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhongjuzhe", "html_url": "https://github.com/zhongjuzhe", "followers_url": "https://api.github.com/users/zhongjuzhe/followers", "following_url": "https://api.github.com/users/zhongjuzhe/following{/other_user}", "gists_url": "https://api.github.com/users/zhongjuzhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhongjuzhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhongjuzhe/subscriptions", "organizations_url": "https://api.github.com/users/zhongjuzhe/orgs", "repos_url": "https://api.github.com/users/zhongjuzhe/repos", "events_url": "https://api.github.com/users/zhongjuzhe/events{/privacy}", "received_events_url": "https://api.github.com/users/zhongjuzhe/received_events", "type": "User", "site_admin": false}, "committer": {"login": "kito-cheng", "id": 2723185, "node_id": "MDQ6VXNlcjI3MjMxODU=", "avatar_url": "https://avatars.githubusercontent.com/u/2723185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kito-cheng", "html_url": "https://github.com/kito-cheng", "followers_url": "https://api.github.com/users/kito-cheng/followers", "following_url": "https://api.github.com/users/kito-cheng/following{/other_user}", "gists_url": "https://api.github.com/users/kito-cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/kito-cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kito-cheng/subscriptions", "organizations_url": "https://api.github.com/users/kito-cheng/orgs", "repos_url": "https://api.github.com/users/kito-cheng/repos", "events_url": "https://api.github.com/users/kito-cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/kito-cheng/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "9f35eb5d51bb4a3857633690c951ecb344a18e84", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9f35eb5d51bb4a3857633690c951ecb344a18e84", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9f35eb5d51bb4a3857633690c951ecb344a18e84"}], "stats": {"total": 10458, "additions": 10458, "deletions": 0}, "files": [{"sha": "6b7acc0aa8e5f14a6c40459129e673b321a10fd9", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_m_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv32-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "eb3d8fbe72bee1628b78b588bc440044980ad84a", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_m_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv32-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_m(mask,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "709807a38031989c208f2427c2f61d46bbe1d468", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_m_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv32-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_m(mask,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "4093b0d1c785ab8a35a5d7c1f2d551768df9150c", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_m_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv64-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "0d299eb37cb5c3ecf971bc57680a46eb0af3720b", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_m_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv64-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_m(mask,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "35b4566bdea23f4b27726786df5c57727185025b", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_m_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_m_rv64-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_m(mask,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "9edc13db72a6a9790810f8cf24a2ae3bd36967ce", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_mu_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv32-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "1c1ed0ab0fdba2c826e3b32653a261fc52913507", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_mu_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv32-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "3dd5bb2d9e72d3813fc31d8e7867296410b60c50", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_mu_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv32-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "37f76b20764b6aa36b01421ebdd1d7604d870fc8", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_mu_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv64-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "9729af79b2f8057728a18c9ea4613416b6d62d54", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_mu_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv64-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "2fb569920622b4ffd7c88504703d7e6e851276a4", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_mu_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_mu_rv64-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "f3d42825865a2650ead83dc1a5564b0424571677", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv32-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8(op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4(op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2(op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1(op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2(op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4(op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8(op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4(op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2(op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1(op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2(op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4(op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8(op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2(op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1(op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2(op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4(op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8(op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1(op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2(op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4(op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8(op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8(op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4(op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2(op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1(op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2(op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4(op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8(op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4(op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2(op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1(op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2(op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4(op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8(op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2(op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1(op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2(op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4(op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8(op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1(op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2(op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4(op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8(op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "28ccf7b5fd2216c6918cf0d507c2a0971a7e355e", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv32-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8(op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4(op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2(op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1(op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2(op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4(op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8(op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4(op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2(op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1(op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2(op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4(op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8(op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2(op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1(op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2(op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4(op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8(op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1(op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2(op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4(op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8(op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8(op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4(op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2(op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1(op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2(op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4(op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8(op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4(op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2(op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1(op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2(op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4(op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8(op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2(op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1(op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2(op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4(op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8(op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1(op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2(op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4(op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8(op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "8ccedcc899f2fc07615e944090894f08be5a7333", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv32-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8(op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4(op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2(op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1(op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2(op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4(op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8(op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4(op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2(op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1(op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2(op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4(op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8(op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2(op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1(op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2(op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4(op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8(op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1(op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2(op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4(op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8(op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8(op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4(op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2(op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1(op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2(op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4(op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8(op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4(op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2(op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1(op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2(op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4(op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8(op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2(op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1(op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2(op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4(op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8(op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1(op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2(op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4(op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8(op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "42586fbcd09909238458be847e5c33365ada18af", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv64-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8(op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4(op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2(op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1(op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2(op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4(op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8(op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4(op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2(op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1(op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2(op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4(op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8(op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2(op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1(op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2(op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4(op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8(op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1(op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2(op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4(op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8(op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8(op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4(op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2(op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1(op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2(op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4(op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8(op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4(op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2(op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1(op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2(op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4(op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8(op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2(op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1(op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2(op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4(op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8(op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1(op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2(op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4(op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8(op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "8f043ffde8a3d476b044725aaffe7ea82cf6cc1c", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv64-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8(op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4(op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2(op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1(op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2(op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4(op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8(op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4(op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2(op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1(op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2(op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4(op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8(op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2(op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1(op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2(op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4(op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8(op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1(op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2(op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4(op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8(op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8(op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4(op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2(op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1(op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2(op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4(op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8(op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4(op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2(op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1(op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2(op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4(op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8(op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2(op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1(op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2(op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4(op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8(op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1(op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2(op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4(op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8(op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "6e5c6d2fe271bd59934b702c572cf913610ac646", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_rv64-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8(op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4(op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2(op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1(op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2(op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4(op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8(op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4(op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2(op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1(op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2(op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4(op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8(op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2(op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1(op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2(op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4(op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8(op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1(op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2(op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4(op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8(op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8(op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4(op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2(op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1(op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2(op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4(op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8(op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4(op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2(op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1(op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2(op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4(op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8(op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2(op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1(op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2(op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4(op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8(op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1(op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2(op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4(op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8(op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "d53cf8df539074f3cf0b12ba85799260c5c138c0", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tu_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv32-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "e93c0716d2533a7bbbcfdd940f5ce6271fbe532a", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tu_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv32-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "182c3df1405158e6efecf41908e389d5820dd566", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tu_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv32-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "e83714091c65e23979be35ee3b66eda2c09baaf7", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tu_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv64-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "96ecbc4277f340a9f57f75a095941cd8ddc7ef0d", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tu_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv64-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "5ca3b2065b21f8ff47e38af41eeb68c09c79e0a4", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tu_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tu_rv64-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "e06ad17188d9f32ffbfd8199de958d403aedfdca", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tum_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv32-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "10eb980a55db73ab54734f20a4f3cec17f1ca143", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tum_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv32-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "969b39b91d12877f9dd87e539a983ab358e394b7", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tum_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv32-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "70afc183ec0ba1d3a269fe956cf9874c157b723e", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tum_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv64-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "f6b96d019f99ddd3ada82e39a6c60cc9000389fa", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tum_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv64-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "c56e9095e6403974a6494b142798f405bbff02ba", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tum_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tum_rv64-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "88dde18dfd3fcca02895c21907c0610f929fdd4f", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tumu_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv32-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "9fe512df2242358ac081fb94ca749a7a1c54aac1", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tumu_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv32-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "d5870d44cdff26587e36a0b6475cd331a23ac321", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tumu_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv32-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vadd\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "d8bcda48297ce8f0158e2189d7c23733d3777327", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tumu_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv64-1.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "5d38f645f2dcce18aa6d19923eda9d695796faf4", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tumu_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv64-2.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "07a4625ea98bde75e3406a8885798c7dc2b2bf2b", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vadd_vx_tumu_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7bff05f5e9f0c74ae42b1c5fe657911f6708b96/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvadd_vx_tumu_rv64-3.c?ref=f7bff05f5e9f0c74ae42b1c5fe657911f6708b96", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vadd_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vadd_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vadd_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vadd_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vadd_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vadd_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vadd_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i8m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vadd_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vadd_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vadd_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vadd_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vadd_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vadd_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i16m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vadd_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vadd_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vadd_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vadd_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vadd_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i32m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vadd_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vadd_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vadd_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vadd_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_i64m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vadd_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vadd_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vadd_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vadd_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vadd_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vadd_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vadd_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u8m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vadd_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vadd_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vadd_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vadd_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vadd_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vadd_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u16m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vadd_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vadd_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vadd_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vadd_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vadd_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u32m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vadd_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vadd_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vadd_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vadd_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vadd_vx_u64m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*mu\\s+vadd\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}]}