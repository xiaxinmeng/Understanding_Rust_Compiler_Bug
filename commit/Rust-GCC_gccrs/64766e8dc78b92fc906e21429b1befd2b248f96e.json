{"sha": "64766e8dc78b92fc906e21429b1befd2b248f96e", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NjQ3NjZlOGRjNzhiOTJmYzkwNmUyMTQyOWIxYmVmZDJiMjQ4Zjk2ZQ==", "commit": {"author": {"name": "Jan Hubicka", "email": "hubicka@ucw.cz", "date": "2017-10-11T15:17:23Z"}, "committer": {"name": "Jan Hubicka", "email": "hubicka@gcc.gnu.org", "date": "2017-10-11T15:17:23Z"}, "message": "config.gcc (i386, x86_64): Add extra objects.\n\n\n\n\t* config.gcc (i386, x86_64): Add extra objects.\n\t* i386/i386-protos.h (ix86_rip_relative_addr_p): Declare.\n\t(ix86_min_insn_size): Declare.\n\t(ix86_issue_rate): Declare.\n\t(ix86_adjust_cost): Declare.\n\t(ia32_multipass_dfa_lookahead): Declare.\n\t(ix86_macro_fusion_p): Declare.\n\t(ix86_macro_fusion_pair_p): Declare.\n\t(ix86_bd_has_dispatch): Declare.\n\t(ix86_bd_do_dispatch): Declare.\n\t(ix86_core2i7_init_hooks): Declare.\n\t(ix86_atom_sched_reorder): Declare.\n\t* i386/i386.c Move all CPU cost tables to x86-tune-costs.h.\n\t(COSTS_N_BYTES): Move to x86-tune-costs.h.\n\t(DUMMY_STRINGOP_ALGS):x86-tune-costs.h.\n\t(rip_relative_addr_p): Rename to ...\n\t(ix86_rip_relative_addr_p): ... this one; export.\n\t(memory_address_length): Update.\n\t(ix86_issue_rate): Move to x86-tune-sched.c.\n\t(ix86_flags_dependent): Move to x86-tune-sched.c.\n\t(ix86_agi_dependent): Move to x86-tune-sched.c.\n\t(exact_dependency_1): Move to x86-tune-sched.c.\n\t(exact_store_load_dependency): Move to x86-tune-sched.c.\n\t(ix86_adjust_cost): Move to x86-tune-sched.c.\n\t(ia32_multipass_dfa_lookahead): Move to x86-tune-sched.c.\n\t(ix86_macro_fusion_p): Move to x86-tune-sched.c.\n\t(ix86_macro_fusion_pair_p): Move to x86-tune-sched.c.\n\t(do_reorder_for_imul): Move to x86-tune-sched-atom.c.\n\t(swap_top_of_ready_list): Move to x86-tune-sched-atom.c.\n\t(ix86_sched_reorder): Move to x86-tune-sched-atom.c.\n\t(core2i7_first_cycle_multipass_init): Move to x86-tune-sched-core.c.\n\t(core2i7_dfa_post_advance_cycle): Move to x86-tune-sched-core.c.\n\t(min_insn_size): Rename to ...\n\t(ix86_min_insn_size): ... this one; export.\n\t(core2i7_first_cycle_multipass_begin): Move to x86-tune-sched-core.c.\n\t(core2i7_first_cycle_multipass_issue): Move to x86-tune-sched-core.c.\n\t(core2i7_first_cycle_multipass_backtrack): Move to x86-tune-sched-core.c.\n\t(core2i7_first_cycle_multipass_end): Move to x86-tune-sched-core.c.\n\t(core2i7_first_cycle_multipass_fini): Move to x86-tune-sched-core.c.\n\t(ix86_sched_init_global): Break up logic to ix86_core2i7_init_hooks.\n\t(ix86_avoid_jump_mispredicts): Update.\n\t(TARGET_SCHED_DISPATCH): Move to ix86-tune-sched-bd.c.\n\t(TARGET_SCHED_DISPATCH_DO): Move to ix86-tune-sched-bd.c.\n\t(TARGET_SCHED_REORDER): Move to ix86-tune-sched-bd.c.\n\t(DISPATCH_WINDOW_SIZE): Move to ix86-tune-sched-bd.c.\n\t(MAX_DISPATCH_WINDOWS): Move to ix86-tune-sched-bd.c.\n\t(MAX_INSN): Move to ix86-tune-sched-bd.c.\n\t(MAX_IMM): Move to ix86-tune-sched-bd.c.\n\t(MAX_IMM_SIZE): Move to ix86-tune-sched-bd.c.\n\t(MAX_IMM_32): Move to ix86-tune-sched-bd.c.\n\t(MAX_IMM_64): Move to ix86-tune-sched-bd.c.\n\t(MAX_LOAD): Move to ix86-tune-sched-bd.c.\n\t(MAX_STORE): Move to ix86-tune-sched-bd.c.\n\t(BIG): Move to ix86-tune-sched-bd.c.\n\t(enum dispatch_group): Move to ix86-tune-sched-bd.c.\n\t(enum insn_path): Move to ix86-tune-sched-bd.c.\n\t(get_mem_group): Move to ix86-tune-sched-bd.c.\n\t(is_cmp): Move to ix86-tune-sched-bd.c.\n\t(dispatch_violation): Move to ix86-tune-sched-bd.c.\n\t(is_branch): Move to ix86-tune-sched-bd.c.\n\t(is_prefetch): Move to ix86-tune-sched-bd.c.\n\t(init_window): Move to ix86-tune-sched-bd.c.\n\t(allocate_window): Move to ix86-tune-sched-bd.c.\n\t(init_dispatch_sched): Move to ix86-tune-sched-bd.c.\n\t(is_end_basic_block): Move to ix86-tune-sched-bd.c.\n\t(process_end_window): Move to ix86-tune-sched-bd.c.\n\t(allocate_next_window): Move to ix86-tune-sched-bd.c.\n\t(find_constant): Move to ix86-tune-sched-bd.c.\n\t(get_num_immediates): Move to ix86-tune-sched-bd.c.\n\t(has_immediate): Move to ix86-tune-sched-bd.c.\n\t(get_insn_path): Move to ix86-tune-sched-bd.c.\n\t(get_insn_group): Move to ix86-tune-sched-bd.c.\n\t(count_num_restricted): Move to ix86-tune-sched-bd.c.\n\t(fits_dispatch_window): Move to ix86-tune-sched-bd.c.\n\t(add_insn_window): Move to ix86-tune-sched-bd.c.\n\t(add_to_dispatch_window): Move to ix86-tune-sched-bd.c.\n\t(debug_dispatch_window_file): Move to ix86-tune-sched-bd.c.\n\t(debug_dispatch_window): Move to ix86-tune-sched-bd.c.\n\t(debug_insn_dispatch_info_file): Move to ix86-tune-sched-bd.c.\n\t(debug_ready_dispatch): Move to ix86-tune-sched-bd.c.\n\t(do_dispatch): Move to ix86-tune-sched-bd.c.\n\t(has_dispatch): Move to ix86-tune-sched-bd.c.\n\t* i386/t-i386: Add new object files.\n\t* i386/x86-tune-costs.h: New file.\n\t* i386/x86-tune-sched-atom.c: New file.\n\t* i386/x86-tune-sched-bd.c: New file.\n\t* i386/x86-tune-sched-core.c: New file.\n\t* i386/x86-tune-sched.c: New file.\n\nFrom-SVN: r253646", "tree": {"sha": "7fd4bb6c61b7eddcfa8419154bf12a7ec2b64e65", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/7fd4bb6c61b7eddcfa8419154bf12a7ec2b64e65"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/64766e8dc78b92fc906e21429b1befd2b248f96e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/64766e8dc78b92fc906e21429b1befd2b248f96e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/64766e8dc78b92fc906e21429b1befd2b248f96e", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/64766e8dc78b92fc906e21429b1befd2b248f96e/comments", "author": null, "committer": null, "parents": [{"sha": "db0d1bae4a36e255bdf676e5030294c27e4b6c86", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/db0d1bae4a36e255bdf676e5030294c27e4b6c86", "html_url": "https://github.com/Rust-GCC/gccrs/commit/db0d1bae4a36e255bdf676e5030294c27e4b6c86"}], "stats": {"total": 8012, "additions": 4145, "deletions": 3867}, "files": [{"sha": "8d09e1038012cee1365133363b8444ae8a3f5db0", "filename": "gcc/ChangeLog", "status": "modified", "additions": 91, "deletions": 0, "changes": 91, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=64766e8dc78b92fc906e21429b1befd2b248f96e", "patch": "@@ -1,3 +1,94 @@\n+2017-10-11  Jan Hubicka  <hubicka@ucw.cz>\n+\n+\t* config.gcc (i386, x86_64): Add extra objects.\n+\t* i386/i386-protos.h (ix86_rip_relative_addr_p): Declare.\n+\t(ix86_min_insn_size): Declare.\n+\t(ix86_issue_rate): Declare.\n+\t(ix86_adjust_cost): Declare.\n+\t(ia32_multipass_dfa_lookahead): Declare.\n+\t(ix86_macro_fusion_p): Declare.\n+\t(ix86_macro_fusion_pair_p): Declare.\n+\t(ix86_bd_has_dispatch): Declare.\n+\t(ix86_bd_do_dispatch): Declare.\n+\t(ix86_core2i7_init_hooks): Declare.\n+\t(ix86_atom_sched_reorder): Declare.\n+\t* i386/i386.c Move all CPU cost tables to x86-tune-costs.h.\n+\t(COSTS_N_BYTES): Move to x86-tune-costs.h.\n+\t(DUMMY_STRINGOP_ALGS):x86-tune-costs.h.\n+\t(rip_relative_addr_p): Rename to ...\n+\t(ix86_rip_relative_addr_p): ... this one; export.\n+\t(memory_address_length): Update.\n+\t(ix86_issue_rate): Move to x86-tune-sched.c.\n+\t(ix86_flags_dependent): Move to x86-tune-sched.c.\n+\t(ix86_agi_dependent): Move to x86-tune-sched.c.\n+\t(exact_dependency_1): Move to x86-tune-sched.c.\n+\t(exact_store_load_dependency): Move to x86-tune-sched.c.\n+\t(ix86_adjust_cost): Move to x86-tune-sched.c.\n+\t(ia32_multipass_dfa_lookahead): Move to x86-tune-sched.c.\n+\t(ix86_macro_fusion_p): Move to x86-tune-sched.c.\n+\t(ix86_macro_fusion_pair_p): Move to x86-tune-sched.c.\n+\t(do_reorder_for_imul): Move to x86-tune-sched-atom.c.\n+\t(swap_top_of_ready_list): Move to x86-tune-sched-atom.c.\n+\t(ix86_sched_reorder): Move to x86-tune-sched-atom.c.\n+\t(core2i7_first_cycle_multipass_init): Move to x86-tune-sched-core.c.\n+\t(core2i7_dfa_post_advance_cycle): Move to x86-tune-sched-core.c.\n+\t(min_insn_size): Rename to ...\n+\t(ix86_min_insn_size): ... this one; export.\n+\t(core2i7_first_cycle_multipass_begin): Move to x86-tune-sched-core.c.\n+\t(core2i7_first_cycle_multipass_issue): Move to x86-tune-sched-core.c.\n+\t(core2i7_first_cycle_multipass_backtrack): Move to x86-tune-sched-core.c.\n+\t(core2i7_first_cycle_multipass_end): Move to x86-tune-sched-core.c.\n+\t(core2i7_first_cycle_multipass_fini): Move to x86-tune-sched-core.c.\n+\t(ix86_sched_init_global): Break up logic to ix86_core2i7_init_hooks.\n+\t(ix86_avoid_jump_mispredicts): Update.\n+\t(TARGET_SCHED_DISPATCH): Move to ix86-tune-sched-bd.c.\n+\t(TARGET_SCHED_DISPATCH_DO): Move to ix86-tune-sched-bd.c.\n+\t(TARGET_SCHED_REORDER): Move to ix86-tune-sched-bd.c.\n+\t(DISPATCH_WINDOW_SIZE): Move to ix86-tune-sched-bd.c.\n+\t(MAX_DISPATCH_WINDOWS): Move to ix86-tune-sched-bd.c.\n+\t(MAX_INSN): Move to ix86-tune-sched-bd.c.\n+\t(MAX_IMM): Move to ix86-tune-sched-bd.c.\n+\t(MAX_IMM_SIZE): Move to ix86-tune-sched-bd.c.\n+\t(MAX_IMM_32): Move to ix86-tune-sched-bd.c.\n+\t(MAX_IMM_64): Move to ix86-tune-sched-bd.c.\n+\t(MAX_LOAD): Move to ix86-tune-sched-bd.c.\n+\t(MAX_STORE): Move to ix86-tune-sched-bd.c.\n+\t(BIG): Move to ix86-tune-sched-bd.c.\n+\t(enum dispatch_group): Move to ix86-tune-sched-bd.c.\n+\t(enum insn_path): Move to ix86-tune-sched-bd.c.\n+\t(get_mem_group): Move to ix86-tune-sched-bd.c.\n+\t(is_cmp): Move to ix86-tune-sched-bd.c.\n+\t(dispatch_violation): Move to ix86-tune-sched-bd.c.\n+\t(is_branch): Move to ix86-tune-sched-bd.c.\n+\t(is_prefetch): Move to ix86-tune-sched-bd.c.\n+\t(init_window): Move to ix86-tune-sched-bd.c.\n+\t(allocate_window): Move to ix86-tune-sched-bd.c.\n+\t(init_dispatch_sched): Move to ix86-tune-sched-bd.c.\n+\t(is_end_basic_block): Move to ix86-tune-sched-bd.c.\n+\t(process_end_window): Move to ix86-tune-sched-bd.c.\n+\t(allocate_next_window): Move to ix86-tune-sched-bd.c.\n+\t(find_constant): Move to ix86-tune-sched-bd.c.\n+\t(get_num_immediates): Move to ix86-tune-sched-bd.c.\n+\t(has_immediate): Move to ix86-tune-sched-bd.c.\n+\t(get_insn_path): Move to ix86-tune-sched-bd.c.\n+\t(get_insn_group): Move to ix86-tune-sched-bd.c.\n+\t(count_num_restricted): Move to ix86-tune-sched-bd.c.\n+\t(fits_dispatch_window): Move to ix86-tune-sched-bd.c.\n+\t(add_insn_window): Move to ix86-tune-sched-bd.c.\n+\t(add_to_dispatch_window): Move to ix86-tune-sched-bd.c.\n+\t(debug_dispatch_window_file): Move to ix86-tune-sched-bd.c.\n+\t(debug_dispatch_window): Move to ix86-tune-sched-bd.c.\n+\t(debug_insn_dispatch_info_file): Move to ix86-tune-sched-bd.c.\n+\t(debug_ready_dispatch): Move to ix86-tune-sched-bd.c.\n+\t(do_dispatch): Move to ix86-tune-sched-bd.c.\n+\t(has_dispatch): Move to ix86-tune-sched-bd.c.\n+\t* i386/t-i386: Add new object files.\n+\t* i386/x86-tune-costs.h: New file.\n+\t* i386/x86-tune-sched-atom.c: New file.\n+\t* i386/x86-tune-sched-bd.c: New file.\n+\t* i386/x86-tune-sched-core.c: New file.\n+\t* i386/x86-tune-sched.c: New file.\n+\n 2017-10-11  Liu Hao  <lh_mouse@126.com>\n \n \t* pretty-print.c [_WIN32] (colorize_init): Remove.  Use"}, {"sha": "22702396a9f68f11886f15df4562058680263fba", "filename": "gcc/config.gcc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=64766e8dc78b92fc906e21429b1befd2b248f96e", "patch": "@@ -360,6 +360,7 @@ i[34567]86-*-*)\n \tcpu_type=i386\n \tc_target_objs=\"i386-c.o\"\n \tcxx_target_objs=\"i386-c.o\"\n+\textra_objs=\"x86-tune-sched.o x86-tune-sched-bd.o x86-tune-sched-atom.o x86-tune-sched-core.o\"\n \textra_options=\"${extra_options} fused-madd.opt\"\n \textra_headers=\"cpuid.h mmintrin.h mm3dnow.h xmmintrin.h emmintrin.h\n \t\t       pmmintrin.h tmmintrin.h ammintrin.h smmintrin.h\n@@ -384,6 +385,7 @@ x86_64-*-*)\n \tc_target_objs=\"i386-c.o\"\n \tcxx_target_objs=\"i386-c.o\"\n \textra_options=\"${extra_options} fused-madd.opt\"\n+\textra_objs=\"x86-tune-sched.o x86-tune-sched-bd.o x86-tune-sched-atom.o x86-tune-sched-core.o\"\n \textra_headers=\"cpuid.h mmintrin.h mm3dnow.h xmmintrin.h emmintrin.h\n \t\t       pmmintrin.h tmmintrin.h ammintrin.h smmintrin.h\n \t\t       nmmintrin.h bmmintrin.h fma4intrin.h wmmintrin.h"}, {"sha": "ab3d8f95c5d0c5bdbf6b5ae8f200e9c5dc5a7dd0", "filename": "gcc/config/i386/i386-protos.h", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fi386-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fi386-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-protos.h?ref=64766e8dc78b92fc906e21429b1befd2b248f96e", "patch": "@@ -27,6 +27,7 @@ extern bool ix86_handle_option (struct gcc_options *opts,\n extern bool ix86_target_stack_probe (void);\n extern bool ix86_can_use_return_insn_p (void);\n extern void ix86_setup_frame_addresses (void);\n+extern bool ix86_rip_relative_addr_p (struct ix86_address *parts);\n \n extern HOST_WIDE_INT ix86_initial_elimination_offset (int, int);\n extern void ix86_expand_prologue (void);\n@@ -314,6 +315,21 @@ extern enum attr_cpu ix86_schedule;\n extern const char * ix86_output_call_insn (rtx_insn *insn, rtx call_op);\n extern bool ix86_operands_ok_for_move_multiple (rtx *operands, bool load,\n \t\t\t\t\t\tmachine_mode mode);\n+extern int ix86_min_insn_size (rtx_insn *);\n+\n+extern int ix86_issue_rate (void);\n+extern int ix86_adjust_cost (rtx_insn *insn, int dep_type, rtx_insn *dep_insn,\n+\t\t\t     int cost, unsigned int);\n+extern int ia32_multipass_dfa_lookahead (void);\n+extern bool ix86_macro_fusion_p (void);\n+extern bool ix86_macro_fusion_pair_p (rtx_insn *condgen, rtx_insn *condjmp);\n+\n+extern bool ix86_bd_has_dispatch (rtx_insn *insn, int action);\n+extern void ix86_bd_do_dispatch (rtx_insn *insn, int mode);\n+\n+extern void ix86_core2i7_init_hooks (void);\n+\n+extern int ix86_atom_sched_reorder (FILE *, int, rtx_insn **, int *, int);\n \n #ifdef RTX_CODE\n /* Target data for multipass lookahead scheduling."}, {"sha": "63058a8d19ca2402515860869897f3dbe3f1a15c", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 17, "deletions": 3867, "changes": 3884, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=64766e8dc78b92fc906e21429b1befd2b248f96e"}, {"sha": "8411a9680ff1d0e82929c3399d98dc215d37d6c8", "filename": "gcc/config/i386/t-i386", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Ft-i386", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Ft-i386", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Ft-i386?ref=64766e8dc78b92fc906e21429b1befd2b248f96e", "patch": "@@ -24,6 +24,22 @@ i386-c.o: $(srcdir)/config/i386/i386-c.c\n \t  $(COMPILE) $<\n \t  $(POSTCOMPILE)\n \n+x86-tune-sched.o: $(srcdir)/config/i386/x86-tune-sched.c\n+\t  $(COMPILE) $<\n+\t  $(POSTCOMPILE)\n+\n+x86-tune-sched-bd.o: $(srcdir)/config/i386/x86-tune-sched-bd.c\n+\t  $(COMPILE) $<\n+\t  $(POSTCOMPILE)\n+\n+x86-tune-sched-atom.o: $(srcdir)/config/i386/x86-tune-sched-atom.c\n+\t  $(COMPILE) $<\n+\t  $(POSTCOMPILE)\n+\n+x86-tune-sched-core.o: $(srcdir)/config/i386/x86-tune-sched-core.c\n+\t  $(COMPILE) $<\n+\t  $(POSTCOMPILE)\n+\n i386.o: i386-builtin-types.inc\n \n i386-builtin-types.inc: s-i386-bt ; @true"}, {"sha": "d27072c0901f1a70d16d62f2c9faaa662012474a", "filename": "gcc/config/i386/x86-tune-costs.h", "status": "added", "additions": 2083, "deletions": 0, "changes": 2083, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fx86-tune-costs.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fx86-tune-costs.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune-costs.h?ref=64766e8dc78b92fc906e21429b1befd2b248f96e", "patch": "@@ -0,0 +1,2083 @@\n+\n+/* Processor costs (relative to an add) */\n+/* We assume COSTS_N_INSNS is defined as (N)*4 and an addition is 2 bytes.  */\n+#define COSTS_N_BYTES(N) ((N) * 2)\n+\n+#define DUMMY_STRINGOP_ALGS {libcall, {{-1, libcall, false}}}\n+\n+static stringop_algs ix86_size_memcpy[2] = {\n+  {rep_prefix_1_byte, {{-1, rep_prefix_1_byte, false}}},\n+  {rep_prefix_1_byte, {{-1, rep_prefix_1_byte, false}}}};\n+static stringop_algs ix86_size_memset[2] = {\n+  {rep_prefix_1_byte, {{-1, rep_prefix_1_byte, false}}},\n+  {rep_prefix_1_byte, {{-1, rep_prefix_1_byte, false}}}};\n+\n+const\n+struct processor_costs ix86_size_cost = {/* costs for tuning for size */\n+  COSTS_N_BYTES (2),\t\t\t/* cost of an add instruction */\n+  COSTS_N_BYTES (3),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_BYTES (2),\t\t\t/* variable shift costs */\n+  COSTS_N_BYTES (3),\t\t\t/* constant shift costs */\n+  {COSTS_N_BYTES (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_BYTES (3),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_BYTES (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_BYTES (3),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_BYTES (5)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_BYTES (3),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_BYTES (3),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_BYTES (3),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_BYTES (3),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_BYTES (5)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_BYTES (3),\t\t\t/* cost of movsx */\n+  COSTS_N_BYTES (3),\t\t\t/* cost of movzx */\n+  0,\t\t\t\t\t/* \"large\" insn */\n+  2,\t\t\t\t\t/* MOVE_RATIO */\n+  2,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {2, 2, 2},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {2, 2, 2},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {2, 2, 2},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {2, 2, 2},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  3,\t\t\t\t\t/* cost of moving MMX register */\n+  {3, 3},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {3, 3},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  3,\t\t\t\t\t/* cost of moving SSE register */\n+  {3, 3, 3},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {3, 3, 3},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+  0,\t\t\t\t\t/* size of l1 cache  */\n+  0,\t\t\t\t\t/* size of l2 cache  */\n+  0,\t\t\t\t\t/* size of prefetch block */\n+  0,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_BYTES (2),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_BYTES (2),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_BYTES (2),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_BYTES (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_BYTES (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_BYTES (2),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  ix86_size_memcpy,\n+  ix86_size_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  1,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  1,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+/* Processor costs (relative to an add) */\n+static stringop_algs i386_memcpy[2] = {\n+  {rep_prefix_1_byte, {{-1, rep_prefix_1_byte, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static stringop_algs i386_memset[2] = {\n+  {rep_prefix_1_byte, {{-1, rep_prefix_1_byte, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+\n+static const\n+struct processor_costs i386_cost = {\t/* 386 specific costs */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (3),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (2),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (6),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (6),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (6),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (6),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (6)},\t\t\t/*\t\t\t      other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (23),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (23),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (23),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (23),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (23)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of movzx */\n+  15,\t\t\t\t\t/* \"large\" insn */\n+  3,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {2, 4, 2},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {2, 4, 2},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {8, 8, 8},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {8, 8, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {4, 8},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 8},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 8, 16},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 8, 16},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+  0,\t\t\t\t\t/* size of l1 cache  */\n+  0,\t\t\t\t\t/* size of l2 cache  */\n+  0,\t\t\t\t\t/* size of prefetch block */\n+  0,\t\t\t\t\t/* number of parallel prefetches */\n+  1,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (23),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (27),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (88),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (22),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (24),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (122),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  i386_memcpy,\n+  i386_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static stringop_algs i486_memcpy[2] = {\n+  {rep_prefix_4_byte, {{-1, rep_prefix_4_byte, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static stringop_algs i486_memset[2] = {\n+  {rep_prefix_4_byte, {{-1, rep_prefix_4_byte, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+\n+static const\n+struct processor_costs i486_cost = {\t/* 486 specific costs */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (3),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (2),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (12),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (12),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (12),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (12),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (12)},\t\t\t/*\t\t\t      other */\n+  1,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (40),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (40),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (40),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (40),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (40)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of movzx */\n+  15,\t\t\t\t\t/* \"large\" insn */\n+  3,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {2, 4, 2},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {2, 4, 2},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {8, 8, 8},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {8, 8, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {4, 8},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 8},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 8, 16},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 8, 16},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+  4,\t\t\t\t\t/* size of l1 cache.  486 has 8kB cache\n+\t\t\t\t\t   shared for code and data, so 4kB is\n+\t\t\t\t\t   not really precise.  */\n+  4,\t\t\t\t\t/* size of l2 cache  */\n+  0,\t\t\t\t\t/* size of prefetch block */\n+  0,\t\t\t\t\t/* number of parallel prefetches */\n+  1,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (16),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (73),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (83),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  i486_memcpy,\n+  i486_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static stringop_algs pentium_memcpy[2] = {\n+  {libcall, {{256, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static stringop_algs pentium_memset[2] = {\n+  {libcall, {{-1, rep_prefix_4_byte, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+\n+static const\n+struct processor_costs pentium_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (4),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (11),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (11),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (11),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (11),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (11)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (25),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (25),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (25),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (25),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (25)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  6,\t\t\t\t\t/* MOVE_RATIO */\n+  6,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {2, 4, 2},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {2, 4, 2},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {2, 2, 6},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 4, 6},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  8,\t\t\t\t\t/* cost of moving MMX register */\n+  {8, 8},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {8, 8},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 8, 16},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 8, 16},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+  8,\t\t\t\t\t/* size of l1 cache.  */\n+  8,\t\t\t\t\t/* size of l2 cache  */\n+  0,\t\t\t\t\t/* size of prefetch block */\n+  0,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (39),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (70),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  pentium_memcpy,\n+  pentium_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static const\n+struct processor_costs lakemont_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1) + 1,\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (11),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (11),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (11),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (11),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (11)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (25),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (25),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (25),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (25),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (25)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  17,\t\t\t\t\t/* MOVE_RATIO */\n+  6,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {2, 4, 2},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {2, 4, 2},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {2, 2, 6},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 4, 6},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  8,\t\t\t\t\t/* cost of moving MMX register */\n+  {8, 8},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {8, 8},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 8, 16},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 8, 16},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+  8,\t\t\t\t\t/* size of l1 cache.  */\n+  8,\t\t\t\t\t/* size of l2 cache  */\n+  0,\t\t\t\t\t/* size of prefetch block */\n+  0,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (39),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (70),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  pentium_memcpy,\n+  pentium_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+/* PentiumPro has optimized rep instructions for blocks aligned by 8 bytes\n+   (we ensure the alignment).  For small blocks inline loop is still a\n+   noticeable win, for bigger blocks either rep movsl or rep movsb is\n+   way to go.  Rep movsb has apparently more expensive startup time in CPU,\n+   but after 4K the difference is down in the noise.  */\n+static stringop_algs pentiumpro_memcpy[2] = {\n+  {rep_prefix_4_byte, {{128, loop, false}, {1024, unrolled_loop, false},\n+                       {8192, rep_prefix_4_byte, false},\n+                       {-1, rep_prefix_1_byte, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static stringop_algs pentiumpro_memset[2] = {\n+  {rep_prefix_4_byte, {{1024, unrolled_loop, false},\n+                       {8192, rep_prefix_4_byte, false},\n+                       {-1, libcall, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static const\n+struct processor_costs pentiumpro_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (4),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (4)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (17),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (17),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (17),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (17),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (17)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  6,\t\t\t\t\t/* MOVE_RATIO */\n+  2,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {4, 4, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {2, 2, 2},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {2, 2, 6},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 4, 6},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {2, 2},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {2, 2},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {2, 2, 8},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {2, 2, 8},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+  8,\t\t\t\t\t/* size of l1 cache.  */\n+  256,\t\t\t\t\t/* size of l2 cache  */\n+  32,\t\t\t\t\t/* size of prefetch block */\n+  6,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (5),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (56),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (56),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  pentiumpro_memcpy,\n+  pentiumpro_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static stringop_algs geode_memcpy[2] = {\n+  {libcall, {{256, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static stringop_algs geode_memset[2] = {\n+  {libcall, {{256, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static const\n+struct processor_costs geode_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (2),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (7),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (7),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (7)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (15),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (23),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (39),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (39),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (39)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  4,\t\t\t\t\t/* MOVE_RATIO */\n+  1,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {1, 1, 1},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {1, 1, 1},\t\t\t\t/* cost of storing integer registers */\n+  1,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {1, 1, 1},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 6, 6},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {2, 2},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {2, 2},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {2, 2, 8},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {2, 2, 8},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+  64,\t\t\t\t\t/* size of l1 cache.  */\n+  128,\t\t\t\t\t/* size of l2 cache.  */\n+  32,\t\t\t\t\t/* size of prefetch block */\n+  1,\t\t\t\t\t/* number of parallel prefetches */\n+  1,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (11),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (47),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (54),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  geode_memcpy,\n+  geode_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static stringop_algs k6_memcpy[2] = {\n+  {libcall, {{256, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static stringop_algs k6_memset[2] = {\n+  {libcall, {{256, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static const\n+struct processor_costs k6_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (3)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (18),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (18),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (18),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (18),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (18)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  4,\t\t\t\t\t/* MOVE_RATIO */\n+  3,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {4, 5, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {2, 3, 2},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {6, 6, 6},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 4, 4},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {2, 2},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {2, 2},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {2, 2, 8},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {2, 2, 8},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  6,\t\t\t\t\t/* MMX or SSE register to integer */\n+  32,\t\t\t\t\t/* size of l1 cache.  */\n+  32,\t\t\t\t\t/* size of l2 cache.  Some models\n+\t\t\t\t\t   have integrated l2 cache, but\n+\t\t\t\t\t   optimizing for k6 is not important\n+\t\t\t\t\t   enough to worry about that.  */\n+  32,\t\t\t\t\t/* size of prefetch block */\n+  1,\t\t\t\t\t/* number of parallel prefetches */\n+  1,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (56),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (56),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  k6_memcpy,\n+  k6_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+/* For some reason, Athlon deals better with REP prefix (relative to loops)\n+   compared to K8. Alignment becomes important after 8 bytes for memcpy and\n+   128 bytes for memset.  */\n+static stringop_algs athlon_memcpy[2] = {\n+  {libcall, {{2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static stringop_algs athlon_memset[2] = {\n+  {libcall, {{2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static const\n+struct processor_costs athlon_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (5),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (5),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (5),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (5),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (5)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (18),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (26),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (42),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (74),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (74)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  9,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {3, 4, 3},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {3, 4, 3},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {4, 4, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {4, 4},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 4, 6},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 4, 5},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  5,\t\t\t\t\t/* MMX or SSE register to integer */\n+  64,\t\t\t\t\t/* size of l1 cache.  */\n+  256,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  6,\t\t\t\t\t/* number of parallel prefetches */\n+  5,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (24),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (35),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  athlon_memcpy,\n+  athlon_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+/* K8 has optimized REP instruction for medium sized blocks, but for very\n+   small blocks it is better to use loop. For large blocks, libcall can\n+   do nontemporary accesses and beat inline considerably.  */\n+static stringop_algs k8_memcpy[2] = {\n+  {libcall, {{6, loop, false}, {14, unrolled_loop, false},\n+             {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{16, loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+static stringop_algs k8_memset[2] = {\n+  {libcall, {{8, loop, false}, {24, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{48, unrolled_loop, false},\n+             {8192, rep_prefix_8_byte, false}, {-1, libcall, false}}}};\n+static const\n+struct processor_costs k8_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (5)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (18),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (26),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (42),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (74),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (74)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  9,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {3, 4, 3},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {3, 4, 3},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {4, 4, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {3, 3},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 3, 6},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 4, 5},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  5,\t\t\t\t\t/* MMX or SSE register to integer */\n+  64,\t\t\t\t\t/* size of l1 cache.  */\n+  512,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  /* New AMD processors never drop prefetches; if they cannot be performed\n+     immediately, they are queued.  We set number of simultaneous prefetches\n+     to a large constant to reflect this (it probably is not a good idea not\n+     to limit number of prefetches at all, as their execution also takes some\n+     time).  */\n+  100,\t\t\t\t\t/* number of parallel prefetches */\n+  3,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (19),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (35),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  k8_memcpy,\n+  k8_memset,\n+  4,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  2,\t\t\t\t\t/* scalar load_cost.  */\n+  2,\t\t\t\t\t/* scalar_store_cost.  */\n+  5,\t\t\t\t\t/* vec_stmt_cost.  */\n+  0,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  2,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  2,\t\t\t\t\t/* vec_align_load_cost.  */\n+  3,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  3,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  2,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+/* AMDFAM10 has optimized REP instruction for medium sized blocks, but for\n+   very small blocks it is better to use loop. For large blocks, libcall can\n+   do nontemporary accesses and beat inline considerably.  */\n+static stringop_algs amdfam10_memcpy[2] = {\n+  {libcall, {{6, loop, false}, {14, unrolled_loop, false},\n+             {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{16, loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+static stringop_algs amdfam10_memset[2] = {\n+  {libcall, {{8, loop, false}, {24, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{48, unrolled_loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+struct processor_costs amdfam10_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (5)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (19),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (35),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (51),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (83),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (83)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  9,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {3, 4, 3},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {3, 4, 3},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {4, 4, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+ \t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {3, 3},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 4, 3},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 4, 5},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+  \t\t\t\t\t/* On K8:\n+  \t\t\t\t\t    MOVD reg64, xmmreg Double FSTORE 4\n+\t\t\t\t\t    MOVD reg32, xmmreg Double FSTORE 4\n+\t\t\t\t\t   On AMDFAM10:\n+\t\t\t\t\t    MOVD reg64, xmmreg Double FADD 3\n+\t\t\t\t\t\t\t       1/1  1/1\n+\t\t\t\t\t    MOVD reg32, xmmreg Double FADD 3\n+\t\t\t\t\t\t\t       1/1  1/1 */\n+  64,\t\t\t\t\t/* size of l1 cache.  */\n+  512,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  /* New AMD processors never drop prefetches; if they cannot be performed\n+     immediately, they are queued.  We set number of simultaneous prefetches\n+     to a large constant to reflect this (it probably is not a good idea not\n+     to limit number of prefetches at all, as their execution also takes some\n+     time).  */\n+  100,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (19),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (35),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  amdfam10_memcpy,\n+  amdfam10_memset,\n+  4,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  2,\t\t\t\t\t/* scalar load_cost.  */\n+  2,\t\t\t\t\t/* scalar_store_cost.  */\n+  6,\t\t\t\t\t/* vec_stmt_cost.  */\n+  0,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  2,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  2,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  2,\t\t\t\t\t/* vec_store_cost.  */\n+  2,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+/*  BDVER1 has optimized REP instruction for medium sized blocks, but for\n+    very small blocks it is better to use loop. For large blocks, libcall\n+    can do nontemporary accesses and beat inline considerably.  */\n+static stringop_algs bdver1_memcpy[2] = {\n+  {libcall, {{6, loop, false}, {14, unrolled_loop, false},\n+             {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{16, loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+static stringop_algs bdver1_memset[2] = {\n+  {libcall, {{8, loop, false}, {24, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{48, unrolled_loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+\n+const struct processor_costs bdver1_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (4),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (6),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (6)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (19),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (35),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (51),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (83),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (83)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  9,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {5, 5, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {5, 5, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 4, 8},\t\t\t\t/* cost of storing fp registers\n+ \t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {4, 4},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 4, 4},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 4, 4},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  2,\t\t\t\t\t/* MMX or SSE register to integer */\n+  \t\t\t\t\t/* On K8:\n+\t\t\t\t\t    MOVD reg64, xmmreg Double FSTORE 4\n+\t\t\t\t\t    MOVD reg32, xmmreg Double FSTORE 4\n+\t\t\t\t\t   On AMDFAM10:\n+\t\t\t\t\t    MOVD reg64, xmmreg Double FADD 3\n+\t\t\t\t\t\t\t       1/1  1/1\n+\t\t\t\t\t    MOVD reg32, xmmreg Double FADD 3\n+\t\t\t\t\t\t\t       1/1  1/1 */\n+  16,\t\t\t\t\t/* size of l1 cache.  */\n+  2048,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  /* New AMD processors never drop prefetches; if they cannot be performed\n+     immediately, they are queued.  We set number of simultaneous prefetches\n+     to a large constant to reflect this (it probably is not a good idea not\n+     to limit number of prefetches at all, as their execution also takes some\n+     time).  */\n+  100,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (42),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (52),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 2, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  bdver1_memcpy,\n+  bdver1_memset,\n+  6,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  4,\t\t\t\t\t/* scalar load_cost.  */\n+  4,\t\t\t\t\t/* scalar_store_cost.  */\n+  6,\t\t\t\t\t/* vec_stmt_cost.  */\n+  0,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  2,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  4,\t\t\t\t\t/* vec_align_load_cost.  */\n+  4,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  4,\t\t\t\t\t/* vec_store_cost.  */\n+  4,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  2,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+/*  BDVER2 has optimized REP instruction for medium sized blocks, but for\n+    very small blocks it is better to use loop. For large blocks, libcall\n+    can do nontemporary accesses and beat inline considerably.  */\n+\n+static stringop_algs bdver2_memcpy[2] = {\n+  {libcall, {{6, loop, false}, {14, unrolled_loop, false},\n+             {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{16, loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+static stringop_algs bdver2_memset[2] = {\n+  {libcall, {{8, loop, false}, {24, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{48, unrolled_loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+\n+const struct processor_costs bdver2_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (4),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (6),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (6)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (19),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (35),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (51),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (83),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (83)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  9,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {5, 5, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {5, 5, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 4, 8},\t\t\t\t/* cost of storing fp registers\n+ \t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {4, 4},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 4, 4},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 4, 4},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  2,\t\t\t\t\t/* MMX or SSE register to integer */\n+  \t\t\t\t\t/* On K8:\n+\t\t\t\t\t    MOVD reg64, xmmreg Double FSTORE 4\n+\t\t\t\t\t    MOVD reg32, xmmreg Double FSTORE 4\n+\t\t\t\t\t   On AMDFAM10:\n+\t\t\t\t\t    MOVD reg64, xmmreg Double FADD 3\n+\t\t\t\t\t\t\t       1/1  1/1\n+\t\t\t\t\t    MOVD reg32, xmmreg Double FADD 3\n+\t\t\t\t\t\t\t       1/1  1/1 */\n+  16,\t\t\t\t\t/* size of l1 cache.  */\n+  2048,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  /* New AMD processors never drop prefetches; if they cannot be performed\n+     immediately, they are queued.  We set number of simultaneous prefetches\n+     to a large constant to reflect this (it probably is not a good idea not\n+     to limit number of prefetches at all, as their execution also takes some\n+     time).  */\n+  100,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (42),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (52),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 2, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  bdver2_memcpy,\n+  bdver2_memset,\n+  6,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  4,\t\t\t\t\t/* scalar load_cost.  */\n+  4,\t\t\t\t\t/* scalar_store_cost.  */\n+  6,\t\t\t\t\t/* vec_stmt_cost.  */\n+  0,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  2,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  4,\t\t\t\t\t/* vec_align_load_cost.  */\n+  4,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  4,\t\t\t\t\t/* vec_store_cost.  */\n+  4,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  2,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+\n+  /*  BDVER3 has optimized REP instruction for medium sized blocks, but for\n+      very small blocks it is better to use loop. For large blocks, libcall\n+      can do nontemporary accesses and beat inline considerably.  */\n+static stringop_algs bdver3_memcpy[2] = {\n+  {libcall, {{6, loop, false}, {14, unrolled_loop, false},\n+             {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{16, loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+static stringop_algs bdver3_memset[2] = {\n+  {libcall, {{8, loop, false}, {24, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{48, unrolled_loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+struct processor_costs bdver3_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (4),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (6),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (6)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (19),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (35),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (51),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (83),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (83)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  9,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {5, 5, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {5, 5, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 4, 8},\t\t\t\t/* cost of storing fp registers\n+ \t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {4, 4},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 4, 4},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 4, 4},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  2,\t\t\t\t\t/* MMX or SSE register to integer */\n+  16,\t\t\t\t\t/* size of l1 cache.  */\n+  2048,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  /* New AMD processors never drop prefetches; if they cannot be performed\n+     immediately, they are queued.  We set number of simultaneous prefetches\n+     to a large constant to reflect this (it probably is not a good idea not\n+     to limit number of prefetches at all, as their execution also takes some\n+     time).  */\n+  100,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (42),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (52),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 2, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  bdver3_memcpy,\n+  bdver3_memset,\n+  6,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  4,\t\t\t\t\t/* scalar load_cost.  */\n+  4,\t\t\t\t\t/* scalar_store_cost.  */\n+  6,\t\t\t\t\t/* vec_stmt_cost.  */\n+  0,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  2,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  4,\t\t\t\t\t/* vec_align_load_cost.  */\n+  4,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  4,\t\t\t\t\t/* vec_store_cost.  */\n+  4,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  2,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+/*  BDVER4 has optimized REP instruction for medium sized blocks, but for\n+    very small blocks it is better to use loop. For large blocks, libcall\n+    can do nontemporary accesses and beat inline considerably.  */\n+static stringop_algs bdver4_memcpy[2] = {\n+  {libcall, {{6, loop, false}, {14, unrolled_loop, false},\n+             {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{16, loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+static stringop_algs bdver4_memset[2] = {\n+  {libcall, {{8, loop, false}, {24, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{48, unrolled_loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+struct processor_costs bdver4_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (4),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (6),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (6)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (19),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (35),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (51),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (83),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (83)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  9,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {5, 5, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {5, 5, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 4, 8},\t\t\t\t/* cost of storing fp registers\n+ \t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {4, 4},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 4, 4},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 4, 4},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  2,\t\t\t\t\t/* MMX or SSE register to integer */\n+  16,\t\t\t\t\t/* size of l1 cache.  */\n+  2048,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  /* New AMD processors never drop prefetches; if they cannot be performed\n+     immediately, they are queued.  We set number of simultaneous prefetches\n+     to a large constant to reflect this (it probably is not a good idea not\n+     to limit number of prefetches at all, as their execution also takes some\n+     time).  */\n+  100,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (42),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (52),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 2, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  bdver4_memcpy,\n+  bdver4_memset,\n+  6,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  4,\t\t\t\t\t/* scalar load_cost.  */\n+  4,\t\t\t\t\t/* scalar_store_cost.  */\n+  6,\t\t\t\t\t/* vec_stmt_cost.  */\n+  0,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  2,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  4,\t\t\t\t\t/* vec_align_load_cost.  */\n+  4,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  4,\t\t\t\t\t/* vec_store_cost.  */\n+  4,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  2,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+\n+/*  ZNVER1 has optimized REP instruction for medium sized blocks, but for\n+    very small blocks it is better to use loop.  For large blocks, libcall\n+    can do nontemporary accesses and beat inline considerably.  */\n+static stringop_algs znver1_memcpy[2] = {\n+  {libcall, {{6, loop, false}, {14, unrolled_loop, false},\n+\t     {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{16, loop, false}, {8192, rep_prefix_8_byte, false},\n+\t     {-1, libcall, false}}}};\n+static stringop_algs znver1_memset[2] = {\n+  {libcall, {{8, loop, false}, {24, unrolled_loop, false},\n+\t     {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{48, unrolled_loop, false}, {8192, rep_prefix_8_byte, false},\n+\t     {-1, libcall, false}}}};\n+struct processor_costs znver1_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction.  */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction.  */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs.  */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs.  */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI.  */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t HI.  */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI.  */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI.  */\n+   COSTS_N_INSNS (4)},\t\t\t/*\t\t\t      other.  */\n+  0,\t\t\t\t\t/* cost of multiply per each bit\n+\t\t\t\t\t    set.  */\n+  {COSTS_N_INSNS (19),\t\t\t/* cost of a divide/mod for QI.  */\n+   COSTS_N_INSNS (35),\t\t\t/*\t\t\t    HI.  */\n+   COSTS_N_INSNS (51),\t\t\t/*\t\t\t    SI.  */\n+   COSTS_N_INSNS (83),\t\t\t/*\t\t\t    DI.  */\n+   COSTS_N_INSNS (83)},\t\t\t/*\t\t\t    other.  */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx.  */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx.  */\n+  8,\t\t\t\t\t/* \"large\" insn.  */\n+  9,\t\t\t\t\t/* MOVE_RATIO.  */\n+  4,\t\t\t\t\t/* cost for loading QImode using\n+\t\t\t\t\t   movzbl.  */\n+  {5, 5, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer\n+\t\t\t\t\t   registers.  */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst.  */\n+  {5, 5, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t   \t\t\t   in SFmode, DFmode and XFmode.  */\n+  {4, 4, 8},\t\t\t\t/* cost of storing fp registers\n+ \t\t   \t\t\t   in SFmode, DFmode and XFmode.  */\n+  2,\t\t\t\t\t/* cost of moving MMX register.  */\n+  {4, 4},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode.  */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode.  */\n+  2,\t\t\t\t\t/* cost of moving SSE register.  */\n+  {4, 4, 4},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode.  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode.  */\n+  2,\t\t\t\t\t/* MMX or SSE register to integer.  */\n+  32,\t\t\t\t\t/* size of l1 cache.  */\n+  512,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block.  */\n+  /* New AMD processors never drop prefetches; if they cannot be performed\n+     immediately, they are queued.  We set number of simultaneous prefetches\n+     to a large constant to reflect this (it probably is not a good idea not\n+     to limit number of prefetches at all, as their execution also takes some\n+     time).  */\n+  100,\t\t\t\t\t/* number of parallel prefetches.  */\n+  3,\t\t\t\t\t/* Branch cost.  */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (42),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (52),\t\t\t/* cost of FSQRT instruction.  */\n+  /* Zen can execute 4 integer operations per cycle. FP operations take 3 cycles\n+     and it can execute 2 integer additions and 2 multiplications thus\n+     reassociation may make sense up to with of 6.  SPEC2k6 bencharks suggests\n+     that 4 works better than 6 probably due to register pressure.\n+\n+     Integer vector operations are taken by FP unit and execute 3 vector\n+     plus/minus operations per cycle but only one multiply.  This is adjusted\n+     in ix86_reassociation_width.  */\n+  4, 4, 3, 6,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  znver1_memcpy,\n+  znver1_memset,\n+  6,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  4,\t\t\t\t\t/* scalar load_cost.  */\n+  4,\t\t\t\t\t/* scalar_store_cost.  */\n+  6,\t\t\t\t\t/* vec_stmt_cost.  */\n+  0,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  2,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  4,\t\t\t\t\t/* vec_align_load_cost.  */\n+  4,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  4,\t\t\t\t\t/* vec_store_cost.  */\n+  4,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  2,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+  /* BTVER1 has optimized REP instruction for medium sized blocks, but for\n+     very small blocks it is better to use loop. For large blocks, libcall can\n+     do nontemporary accesses and beat inline considerably.  */\n+static stringop_algs btver1_memcpy[2] = {\n+  {libcall, {{6, loop, false}, {14, unrolled_loop, false},\n+             {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{16, loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+static stringop_algs btver1_memset[2] = {\n+  {libcall, {{8, loop, false}, {24, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{48, unrolled_loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+const struct processor_costs btver1_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (5)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (19),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (35),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (51),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (83),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (83)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  9,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {3, 4, 3},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {3, 4, 3},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {4, 4, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {3, 3},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 4, 3},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 4, 5},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+\t\t\t\t\t/* On K8:\n+\t\t\t\t\t   MOVD reg64, xmmreg Double FSTORE 4\n+\t\t\t\t\t   MOVD reg32, xmmreg Double FSTORE 4\n+\t\t\t\t\t   On AMDFAM10:\n+\t\t\t\t\t   MOVD reg64, xmmreg Double FADD 3\n+\t\t\t\t\t\t\t       1/1  1/1\n+\t\t\t\t\t    MOVD reg32, xmmreg Double FADD 3\n+\t\t\t\t\t\t\t       1/1  1/1 */\n+  32,\t\t\t\t\t/* size of l1 cache.  */\n+  512,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  100,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (19),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (35),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  btver1_memcpy,\n+  btver1_memset,\n+  4,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  2,\t\t\t\t\t/* scalar load_cost.  */\n+  2,\t\t\t\t\t/* scalar_store_cost.  */\n+  6,\t\t\t\t\t/* vec_stmt_cost.  */\n+  0,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  2,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  2,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  2,\t\t\t\t\t/* vec_store_cost.  */\n+  2,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static stringop_algs btver2_memcpy[2] = {\n+  {libcall, {{6, loop, false}, {14, unrolled_loop, false},\n+             {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{16, loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+static stringop_algs btver2_memset[2] = {\n+  {libcall, {{8, loop, false}, {24, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{48, unrolled_loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+const struct processor_costs btver2_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (5)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (19),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (35),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (51),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (83),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (83)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  9,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {3, 4, 3},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {3, 4, 3},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {4, 4, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {3, 3},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 4, 3},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 4, 5},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+\t\t\t\t\t/* On K8:\n+\t\t\t\t\t   MOVD reg64, xmmreg Double FSTORE 4\n+\t\t\t\t\t   MOVD reg32, xmmreg Double FSTORE 4\n+\t\t\t\t\t   On AMDFAM10:\n+\t\t\t\t\t   MOVD reg64, xmmreg Double FADD 3\n+\t\t\t\t\t\t\t       1/1  1/1\n+\t\t\t\t\t    MOVD reg32, xmmreg Double FADD 3\n+\t\t\t\t\t\t\t       1/1  1/1 */\n+  32,\t\t\t\t\t/* size of l1 cache.  */\n+  2048,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  100,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (19),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (35),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  btver2_memcpy,\n+  btver2_memset,\n+  4,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  2,\t\t\t\t\t/* scalar load_cost.  */\n+  2,\t\t\t\t\t/* scalar_store_cost.  */\n+  6,\t\t\t\t\t/* vec_stmt_cost.  */\n+  0,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  2,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  2,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  2,\t\t\t\t\t/* vec_store_cost.  */\n+  2,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static stringop_algs pentium4_memcpy[2] = {\n+  {libcall, {{12, loop_1_byte, false}, {-1, rep_prefix_4_byte, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+static stringop_algs pentium4_memset[2] = {\n+  {libcall, {{6, loop_1_byte, false}, {48, loop, false},\n+             {20480, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  DUMMY_STRINGOP_ALGS};\n+\n+static const\n+struct processor_costs pentium4_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (4),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (4),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (15),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (15),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (15),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (15),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (15)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (56),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (56),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (56),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (56),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (56)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  16,\t\t\t\t\t/* \"large\" insn */\n+  6,\t\t\t\t\t/* MOVE_RATIO */\n+  2,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {4, 5, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {2, 3, 2},\t\t\t\t/* cost of storing integer registers */\n+  2,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {2, 2, 6},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 4, 6},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {2, 2},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {2, 2},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  12,\t\t\t\t\t/* cost of moving SSE register */\n+  {12, 12, 12},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {2, 2, 8},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  10,\t\t\t\t\t/* MMX or SSE register to integer */\n+  8,\t\t\t\t\t/* size of l1 cache.  */\n+  256,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  6,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (5),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (7),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (43),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (43),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  pentium4_memcpy,\n+  pentium4_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static stringop_algs nocona_memcpy[2] = {\n+  {libcall, {{12, loop_1_byte, false}, {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{32, loop, false}, {20000, rep_prefix_8_byte, false},\n+             {100000, unrolled_loop, false}, {-1, libcall, false}}}};\n+\n+static stringop_algs nocona_memset[2] = {\n+  {libcall, {{6, loop_1_byte, false}, {48, loop, false},\n+             {20480, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{24, loop, false}, {64, unrolled_loop, false},\n+             {8192, rep_prefix_8_byte, false}, {-1, libcall, false}}}};\n+\n+static const\n+struct processor_costs nocona_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (10),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (10),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (10),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (10),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (10)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (66),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (66),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (66),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (66),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (66)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  16,\t\t\t\t\t/* \"large\" insn */\n+  17,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {4, 4, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  3,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {12, 12, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {4, 4, 4},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  6,\t\t\t\t\t/* cost of moving MMX register */\n+  {12, 12},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {12, 12},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  6,\t\t\t\t\t/* cost of moving SSE register */\n+  {12, 12, 12},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {12, 12, 12},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  8,\t\t\t\t\t/* MMX or SSE register to integer */\n+  8,\t\t\t\t\t/* size of l1 cache.  */\n+  1024,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  8,\t\t\t\t\t/* number of parallel prefetches */\n+  1,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (6),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (40),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (3),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (44),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 1, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  nocona_memcpy,\n+  nocona_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static stringop_algs atom_memcpy[2] = {\n+  {libcall, {{11, loop, false}, {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{32, loop, false}, {64, rep_prefix_4_byte, false},\n+             {8192, rep_prefix_8_byte, false}, {-1, libcall, false}}}};\n+static stringop_algs atom_memset[2] = {\n+  {libcall, {{8, loop, false}, {15, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{24, loop, false}, {32, unrolled_loop, false},\n+             {8192, rep_prefix_8_byte, false}, {-1, libcall, false}}}};\n+static const\n+struct processor_costs atom_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1) + 1,\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (2)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (18),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (26),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (42),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (74),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (74)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  17,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t\t/* cost for loading QImode using movzbl */\n+  {4, 4, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {12, 12, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {8, 8},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {8, 8},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {8, 8, 8},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {8, 8, 8},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  5,\t\t\t\t\t/* MMX or SSE register to integer */\n+  32,\t\t\t\t\t/* size of l1 cache.  */\n+  256,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  6,\t\t\t\t\t/* number of parallel prefetches */\n+  3,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (20),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (40),\t\t\t/* cost of FSQRT instruction.  */\n+  2, 2, 2, 2,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  atom_memcpy,\n+  atom_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static stringop_algs slm_memcpy[2] = {\n+  {libcall, {{11, loop, false}, {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{32, loop, false}, {64, rep_prefix_4_byte, false},\n+             {8192, rep_prefix_8_byte, false}, {-1, libcall, false}}}};\n+static stringop_algs slm_memset[2] = {\n+  {libcall, {{8, loop, false}, {15, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{24, loop, false}, {32, unrolled_loop, false},\n+             {8192, rep_prefix_8_byte, false}, {-1, libcall, false}}}};\n+static const\n+struct processor_costs slm_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1) + 1,\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (2)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (18),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (26),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (42),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (74),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (74)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  17,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t\t/* cost for loading QImode using movzbl */\n+  {4, 4, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {12, 12, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {8, 8},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {8, 8},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {8, 8, 8},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {8, 8, 8},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  5,\t\t\t\t\t/* MMX or SSE register to integer */\n+  32,\t\t\t\t\t/* size of l1 cache.  */\n+  256,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  6,\t\t\t\t\t/* number of parallel prefetches */\n+  3,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (20),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (40),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 2, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  slm_memcpy,\n+  slm_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  4,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+static stringop_algs intel_memcpy[2] = {\n+  {libcall, {{11, loop, false}, {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{32, loop, false}, {64, rep_prefix_4_byte, false},\n+             {8192, rep_prefix_8_byte, false}, {-1, libcall, false}}}};\n+static stringop_algs intel_memset[2] = {\n+  {libcall, {{8, loop, false}, {15, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{24, loop, false}, {32, unrolled_loop, false},\n+             {8192, rep_prefix_8_byte, false}, {-1, libcall, false}}}};\n+static const\n+struct processor_costs intel_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1) + 1,\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (2)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (18),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (26),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (42),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (74),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (74)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  17,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t\t/* cost for loading QImode using movzbl */\n+  {4, 4, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {12, 12, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {8, 8},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {8, 8},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {8, 8, 8},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {8, 8, 8},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  5,\t\t\t\t\t/* MMX or SSE register to integer */\n+  32,\t\t\t\t\t/* size of l1 cache.  */\n+  256,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  6,\t\t\t\t\t/* number of parallel prefetches */\n+  3,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (20),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (40),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 4, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  intel_memcpy,\n+  intel_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  4,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+/* Generic should produce code tuned for Core-i7 (and newer chips)\n+   and btver1 (and newer chips).  */\n+\n+static stringop_algs generic_memcpy[2] = {\n+  {libcall, {{32, loop, false}, {8192, rep_prefix_4_byte, false},\n+             {-1, libcall, false}}},\n+  {libcall, {{32, loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+static stringop_algs generic_memset[2] = {\n+  {libcall, {{32, loop, false}, {8192, rep_prefix_4_byte, false},\n+             {-1, libcall, false}}},\n+  {libcall, {{32, loop, false}, {8192, rep_prefix_8_byte, false},\n+             {-1, libcall, false}}}};\n+static const\n+struct processor_costs generic_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  /* On all chips taken into consideration lea is 2 cycles and more.  With\n+     this cost however our current implementation of synth_mult results in\n+     use of unnecessary temporary registers causing regression on several\n+     SPECfp benchmarks.  */\n+  COSTS_N_INSNS (1) + 1,\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (2)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (18),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (26),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (42),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (74),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (74)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  17,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {4, 4, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {12, 12, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {8, 8},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {8, 8},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {8, 8, 8},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {8, 8, 8},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  5,\t\t\t\t\t/* MMX or SSE register to integer */\n+  32,\t\t\t\t\t/* size of l1 cache.  */\n+  512,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  6,\t\t\t\t\t/* number of parallel prefetches */\n+  /* Benchmarks shows large regressions on K8 sixtrack benchmark when this\n+     value is increased to perhaps more appropriate value of 5.  */\n+  3,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (20),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (40),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 2, 1, 1,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  generic_memcpy,\n+  generic_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n+/* core_cost should produce code tuned for Core familly of CPUs.  */\n+static stringop_algs core_memcpy[2] = {\n+  {libcall, {{1024, rep_prefix_4_byte, true}, {-1, libcall, false}}},\n+  {libcall, {{24, loop, true}, {128, rep_prefix_8_byte, true},\n+             {-1, libcall, false}}}};\n+static stringop_algs core_memset[2] = {\n+  {libcall, {{6, loop_1_byte, true},\n+             {24, loop, true},\n+             {8192, rep_prefix_4_byte, true},\n+             {-1, libcall, false}}},\n+  {libcall, {{24, loop, true}, {512, rep_prefix_8_byte, true},\n+             {-1, libcall, false}}}};\n+\n+static const\n+struct processor_costs core_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  /* On all chips taken into consideration lea is 2 cycles and more.  With\n+     this cost however our current implementation of synth_mult results in\n+     use of unnecessary temporary registers causing regression on several\n+     SPECfp benchmarks.  */\n+  COSTS_N_INSNS (1) + 1,\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (2)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (18),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (26),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (42),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (74),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (74)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  17,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t     /* cost for loading QImode using movzbl */\n+  {4, 4, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {12, 12, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {8, 8},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {8, 8},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {8, 8, 8},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {8, 8, 8},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  5,\t\t\t\t\t/* MMX or SSE register to integer */\n+  64,\t\t\t\t\t/* size of l1 cache.  */\n+  512,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  6,\t\t\t\t\t/* number of parallel prefetches */\n+  /* FIXME perhaps more appropriate value is 5.  */\n+  3,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (20),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (40),\t\t\t/* cost of FSQRT instruction.  */\n+  1, 4, 2, 2,\t\t\t\t/* reassoc int, fp, vec_int, vec_fp.  */\n+  core_memcpy,\n+  core_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+"}, {"sha": "86942c0703d5e2631f82618f749096ca33868940", "filename": "gcc/config/i386/x86-tune-sched-atom.c", "status": "added", "additions": 244, "deletions": 0, "changes": 244, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-atom.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-atom.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-atom.c?ref=64766e8dc78b92fc906e21429b1befd2b248f96e", "patch": "@@ -0,0 +1,244 @@\n+/* Scheduler hooks for IA-32 which implement atom+ specific logic.\n+   Copyright (C) 1988-2017 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"backend.h\"\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"cfghooks.h\"\n+#include \"tm_p.h\"\n+#include \"insn-config.h\"\n+#include \"insn-attr.h\"\n+#include \"recog.h\"\n+#include \"target.h\"\n+#include \"rtl-iter.h\"\n+#include \"regset.h\"\n+#include \"sched-int.h\"\n+\n+/* Try to reorder ready list to take advantage of Atom pipelined IMUL\n+   execution. It is applied if\n+   (1) IMUL instruction is on the top of list;\n+   (2) There exists the only producer of independent IMUL instruction in\n+       ready list.\n+   Return index of IMUL producer if it was found and -1 otherwise.  */\n+static int\n+do_reorder_for_imul (rtx_insn **ready, int n_ready)\n+{\n+  rtx_insn *insn;\n+  rtx set, insn1, insn2;\n+  sd_iterator_def sd_it;\n+  dep_t dep;\n+  int index = -1;\n+  int i;\n+\n+  if (!TARGET_BONNELL)\n+    return index;\n+\n+  /* Check that IMUL instruction is on the top of ready list.  */\n+  insn = ready[n_ready - 1];\n+  set = single_set (insn);\n+  if (!set)\n+    return index;\n+  if (!(GET_CODE (SET_SRC (set)) == MULT\n+      && GET_MODE (SET_SRC (set)) == SImode))\n+    return index;\n+\n+  /* Search for producer of independent IMUL instruction.  */\n+  for (i = n_ready - 2; i >= 0; i--)\n+    {\n+      insn = ready[i];\n+      if (!NONDEBUG_INSN_P (insn))\n+\tcontinue;\n+      /* Skip IMUL instruction.  */\n+      insn2 = PATTERN (insn);\n+      if (GET_CODE (insn2) == PARALLEL)\n+\tinsn2 = XVECEXP (insn2, 0, 0);\n+      if (GET_CODE (insn2) == SET\n+\t  && GET_CODE (SET_SRC (insn2)) == MULT\n+\t  && GET_MODE (SET_SRC (insn2)) == SImode)\n+\tcontinue;\n+\n+      FOR_EACH_DEP (insn, SD_LIST_FORW, sd_it, dep)\n+\t{\n+\t  rtx con;\n+\t  con = DEP_CON (dep);\n+\t  if (!NONDEBUG_INSN_P (con))\n+\t    continue;\n+\t  insn1 = PATTERN (con);\n+\t  if (GET_CODE (insn1) == PARALLEL)\n+\t    insn1 = XVECEXP (insn1, 0, 0);\n+\n+\t  if (GET_CODE (insn1) == SET\n+\t      && GET_CODE (SET_SRC (insn1)) == MULT\n+\t      && GET_MODE (SET_SRC (insn1)) == SImode)\n+\t    {\n+\t      sd_iterator_def sd_it1;\n+\t      dep_t dep1;\n+\t      /* Check if there is no other dependee for IMUL.  */\n+\t      index = i;\n+\t      FOR_EACH_DEP (con, SD_LIST_BACK, sd_it1, dep1)\n+\t\t{\n+\t\t  rtx pro;\n+\t\t  pro = DEP_PRO (dep1);\n+\t\t  if (!NONDEBUG_INSN_P (pro))\n+\t\t    continue;\n+\t\t  if (pro != insn)\n+\t\t    index = -1;\n+\t\t}\n+\t      if (index >= 0)\n+\t\tbreak;\n+\t    }\n+\t}\n+      if (index >= 0)\n+\tbreak;\n+    }\n+  return index;\n+}\n+\n+/* Try to find the best candidate on the top of ready list if two insns\n+   have the same priority - candidate is best if its dependees were\n+   scheduled earlier. Applied for Silvermont only.\n+   Return true if top 2 insns must be interchanged.  */\n+static bool\n+swap_top_of_ready_list (rtx_insn **ready, int n_ready)\n+{\n+  rtx_insn *top = ready[n_ready - 1];\n+  rtx_insn *next = ready[n_ready - 2];\n+  rtx set;\n+  sd_iterator_def sd_it;\n+  dep_t dep;\n+  int clock1 = -1;\n+  int clock2 = -1;\n+  #define INSN_TICK(INSN) (HID (INSN)->tick)\n+\n+  if (!TARGET_SILVERMONT && !TARGET_INTEL)\n+    return false;\n+\n+  if (!NONDEBUG_INSN_P (top))\n+    return false;\n+  if (!NONJUMP_INSN_P (top))\n+    return false;\n+  if (!NONDEBUG_INSN_P (next))\n+    return false;\n+  if (!NONJUMP_INSN_P (next))\n+    return false;\n+  set = single_set (top);\n+  if (!set)\n+    return false;\n+  set = single_set (next);\n+  if (!set)\n+    return false;\n+\n+  if (INSN_PRIORITY_KNOWN (top) && INSN_PRIORITY_KNOWN (next))\n+    {\n+      if (INSN_PRIORITY (top) != INSN_PRIORITY (next))\n+\treturn false;\n+      /* Determine winner more precise.  */\n+      FOR_EACH_DEP (top, SD_LIST_RES_BACK, sd_it, dep)\n+\t{\n+\t  rtx pro;\n+\t  pro = DEP_PRO (dep);\n+\t  if (!NONDEBUG_INSN_P (pro))\n+\t    continue;\n+\t  if (INSN_TICK (pro) > clock1)\n+\t    clock1 = INSN_TICK (pro);\n+\t}\n+      FOR_EACH_DEP (next, SD_LIST_RES_BACK, sd_it, dep)\n+\t{\n+\t  rtx pro;\n+\t  pro = DEP_PRO (dep);\n+\t  if (!NONDEBUG_INSN_P (pro))\n+\t    continue;\n+\t  if (INSN_TICK (pro) > clock2)\n+\t    clock2 = INSN_TICK (pro);\n+\t}\n+\n+      if (clock1 == clock2)\n+\t{\n+\t  /* Determine winner - load must win. */\n+\t  enum attr_memory memory1, memory2;\n+\t  memory1 = get_attr_memory (top);\n+\t  memory2 = get_attr_memory (next);\n+\t  if (memory2 == MEMORY_LOAD && memory1 != MEMORY_LOAD)\n+\t    return true;\n+\t}\n+\treturn (bool) (clock2 < clock1);\n+    }\n+  return false;\n+  #undef INSN_TICK\n+}\n+\n+/* Perform possible reodering of ready list for Atom/Silvermont only.\n+   Return issue rate.  */\n+int\n+ix86_atom_sched_reorder (FILE *dump, int sched_verbose, rtx_insn **ready,\n+\t\t         int *pn_ready, int clock_var)\n+{\n+  int issue_rate = -1;\n+  int n_ready = *pn_ready;\n+  int i;\n+  rtx_insn *insn;\n+  int index = -1;\n+\n+  /* Set up issue rate.  */\n+  issue_rate = ix86_issue_rate ();\n+\n+  /* Do reodering for BONNELL/SILVERMONT only.  */\n+  if (!TARGET_BONNELL && !TARGET_SILVERMONT && !TARGET_INTEL)\n+    return issue_rate;\n+\n+  /* Nothing to do if ready list contains only 1 instruction.  */\n+  if (n_ready <= 1)\n+    return issue_rate;\n+\n+  /* Do reodering for post-reload scheduler only.  */\n+  if (!reload_completed)\n+    return issue_rate;\n+\n+  if ((index = do_reorder_for_imul (ready, n_ready)) >= 0)\n+    {\n+      if (sched_verbose > 1)\n+\tfprintf (dump, \";;\\tatom sched_reorder: put %d insn on top\\n\",\n+\t\t INSN_UID (ready[index]));\n+\n+      /* Put IMUL producer (ready[index]) at the top of ready list.  */\n+      insn = ready[index];\n+      for (i = index; i < n_ready - 1; i++)\n+\tready[i] = ready[i + 1];\n+      ready[n_ready - 1] = insn;\n+      return issue_rate;\n+    }\n+\n+  /* Skip selective scheduling since HID is not populated in it.  */\n+  if (clock_var != 0\n+      && !sel_sched_p ()\n+      && swap_top_of_ready_list (ready, n_ready))\n+    {\n+      if (sched_verbose > 1)\n+\tfprintf (dump, \";;\\tslm sched_reorder: swap %d and %d insns\\n\",\n+\t\t INSN_UID (ready[n_ready - 1]), INSN_UID (ready[n_ready - 2]));\n+      /* Swap 2 top elements of ready list.  */\n+      insn = ready[n_ready - 1];\n+      ready[n_ready - 1] = ready[n_ready - 2];\n+      ready[n_ready - 2] = insn;\n+    }\n+  return issue_rate;\n+}"}, {"sha": "c862fc156e218e44c6469e2cd136101c37351ccf", "filename": "gcc/config/i386/x86-tune-sched-bd.c", "status": "added", "additions": 822, "deletions": 0, "changes": 822, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-bd.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-bd.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-bd.c?ref=64766e8dc78b92fc906e21429b1befd2b248f96e", "patch": "@@ -0,0 +1,822 @@\n+/* Scheduler hooks for IA-32 which implement bdver1-4 specific logic.\n+   Copyright (C) 1988-2017 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"backend.h\"\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"cfghooks.h\"\n+#include \"tm_p.h\"\n+#include \"insn-config.h\"\n+#include \"insn-attr.h\"\n+#include \"recog.h\"\n+#include \"target.h\"\n+#include \"rtl-iter.h\"\n+#include \"regset.h\"\n+#include \"sched-int.h\"\n+\n+/* The size of the dispatch window is the total number of bytes of\n+   object code allowed in a window.  */\n+#define DISPATCH_WINDOW_SIZE 16\n+\n+/* Number of dispatch windows considered for scheduling.  */\n+#define MAX_DISPATCH_WINDOWS 3\n+\n+/* Maximum number of instructions in a window.  */\n+#define MAX_INSN 4\n+\n+/* Maximum number of immediate operands in a window.  */\n+#define MAX_IMM 4\n+\n+/* Maximum number of immediate bits allowed in a window.  */\n+#define MAX_IMM_SIZE 128\n+\n+/* Maximum number of 32 bit immediates allowed in a window.  */\n+#define MAX_IMM_32 4\n+\n+/* Maximum number of 64 bit immediates allowed in a window.  */\n+#define MAX_IMM_64 2\n+\n+/* Maximum total of loads or prefetches allowed in a window.  */\n+#define MAX_LOAD 2\n+\n+/* Maximum total of stores allowed in a window.  */\n+#define MAX_STORE 1\n+\n+#undef BIG\n+#define BIG 100\n+\n+\n+/* Dispatch groups.  Istructions that affect the mix in a dispatch window.  */\n+enum dispatch_group {\n+  disp_no_group = 0,\n+  disp_load,\n+  disp_store,\n+  disp_load_store,\n+  disp_prefetch,\n+  disp_imm,\n+  disp_imm_32,\n+  disp_imm_64,\n+  disp_branch,\n+  disp_cmp,\n+  disp_jcc,\n+  disp_last\n+};\n+\n+/* Number of allowable groups in a dispatch window.  It is an array\n+   indexed by dispatch_group enum.  100 is used as a big number,\n+   because the number of these kind of operations does not have any\n+   effect in dispatch window, but we need them for other reasons in\n+   the table.  */\n+static unsigned int num_allowable_groups[disp_last] = {\n+  0, 2, 1, 1, 2, 4, 4, 2, 1, BIG, BIG\n+};\n+\n+char group_name[disp_last + 1][16] = {\n+  \"disp_no_group\", \"disp_load\", \"disp_store\", \"disp_load_store\",\n+  \"disp_prefetch\", \"disp_imm\", \"disp_imm_32\", \"disp_imm_64\",\n+  \"disp_branch\", \"disp_cmp\", \"disp_jcc\", \"disp_last\"\n+};\n+\n+/* Instruction path.  */\n+enum insn_path {\n+  no_path = 0,\n+  path_single, /* Single micro op.  */\n+  path_double, /* Double micro op.  */\n+  path_multi,  /* Instructions with more than 2 micro op..  */\n+  last_path\n+};\n+\n+/* sched_insn_info defines a window to the instructions scheduled in\n+   the basic block.  It contains a pointer to the insn_info table and\n+   the instruction scheduled.\n+\n+   Windows are allocated for each basic block and are linked\n+   together.  */\n+typedef struct sched_insn_info_s {\n+  rtx insn;\n+  enum dispatch_group group;\n+  enum insn_path path;\n+  int byte_len;\n+  int imm_bytes;\n+} sched_insn_info;\n+\n+/* Linked list of dispatch windows.  This is a two way list of\n+   dispatch windows of a basic block.  It contains information about\n+   the number of uops in the window and the total number of\n+   instructions and of bytes in the object code for this dispatch\n+   window.  */\n+typedef struct dispatch_windows_s {\n+  int num_insn;            /* Number of insn in the window.  */\n+  int num_uops;            /* Number of uops in the window.  */\n+  int window_size;         /* Number of bytes in the window.  */\n+  int window_num;          /* Window number between 0 or 1.  */\n+  int num_imm;             /* Number of immediates in an insn.  */\n+  int num_imm_32;          /* Number of 32 bit immediates in an insn.  */\n+  int num_imm_64;          /* Number of 64 bit immediates in an insn.  */\n+  int imm_size;            /* Total immediates in the window.  */\n+  int num_loads;           /* Total memory loads in the window.  */\n+  int num_stores;          /* Total memory stores in the window.  */\n+  int violation;          /* Violation exists in window.  */\n+  sched_insn_info *window; /* Pointer to the window.  */\n+  struct dispatch_windows_s *next;\n+  struct dispatch_windows_s *prev;\n+} dispatch_windows;\n+\n+/* Immediate valuse used in an insn.  */\n+typedef struct imm_info_s\n+  {\n+    int imm;\n+    int imm32;\n+    int imm64;\n+  } imm_info;\n+\n+static dispatch_windows *dispatch_window_list;\n+static dispatch_windows *dispatch_window_list1;\n+\n+/* Get dispatch group of insn.  */\n+\n+static enum dispatch_group\n+get_mem_group (rtx_insn *insn)\n+{\n+  enum attr_memory memory;\n+\n+  if (INSN_CODE (insn) < 0)\n+    return disp_no_group;\n+  memory = get_attr_memory (insn);\n+  if (memory == MEMORY_STORE)\n+    return disp_store;\n+\n+  if (memory == MEMORY_LOAD)\n+    return disp_load;\n+\n+  if (memory == MEMORY_BOTH)\n+    return disp_load_store;\n+\n+  return disp_no_group;\n+}\n+\n+/* Return true if insn is a compare instruction.  */\n+\n+static bool\n+is_cmp (rtx_insn *insn)\n+{\n+  enum attr_type type;\n+\n+  type = get_attr_type (insn);\n+  return (type == TYPE_TEST\n+\t  || type == TYPE_ICMP\n+\t  || type == TYPE_FCMP\n+\t  || GET_CODE (PATTERN (insn)) == COMPARE);\n+}\n+\n+/* Return true if a dispatch violation encountered.  */\n+\n+static bool\n+dispatch_violation (void)\n+{\n+  if (dispatch_window_list->next)\n+    return dispatch_window_list->next->violation;\n+  return dispatch_window_list->violation;\n+}\n+\n+/* Return true if insn is a branch instruction.  */\n+\n+static bool\n+is_branch (rtx_insn *insn)\n+{\n+  return (CALL_P (insn) || JUMP_P (insn));\n+}\n+\n+/* Return true if insn is a prefetch instruction.  */\n+\n+static bool\n+is_prefetch (rtx_insn *insn)\n+{\n+  return NONJUMP_INSN_P (insn) && GET_CODE (PATTERN (insn)) == PREFETCH;\n+}\n+\n+/* This function initializes a dispatch window and the list container holding a\n+   pointer to the window.  */\n+\n+static void\n+init_window (int window_num)\n+{\n+  int i;\n+  dispatch_windows *new_list;\n+\n+  if (window_num == 0)\n+    new_list = dispatch_window_list;\n+  else\n+    new_list = dispatch_window_list1;\n+\n+  new_list->num_insn = 0;\n+  new_list->num_uops = 0;\n+  new_list->window_size = 0;\n+  new_list->next = NULL;\n+  new_list->prev = NULL;\n+  new_list->window_num = window_num;\n+  new_list->num_imm = 0;\n+  new_list->num_imm_32 = 0;\n+  new_list->num_imm_64 = 0;\n+  new_list->imm_size = 0;\n+  new_list->num_loads = 0;\n+  new_list->num_stores = 0;\n+  new_list->violation = false;\n+\n+  for (i = 0; i < MAX_INSN; i++)\n+    {\n+      new_list->window[i].insn = NULL;\n+      new_list->window[i].group = disp_no_group;\n+      new_list->window[i].path = no_path;\n+      new_list->window[i].byte_len = 0;\n+      new_list->window[i].imm_bytes = 0;\n+    }\n+  return;\n+}\n+\n+/* This function allocates and initializes a dispatch window and the\n+   list container holding a pointer to the window.  */\n+\n+static dispatch_windows *\n+allocate_window (void)\n+{\n+  dispatch_windows *new_list = XNEW (struct dispatch_windows_s);\n+  new_list->window = XNEWVEC (struct sched_insn_info_s, MAX_INSN + 1);\n+\n+  return new_list;\n+}\n+\n+/* This routine initializes the dispatch scheduling information.  It\n+   initiates building dispatch scheduler tables and constructs the\n+   first dispatch window.  */\n+\n+static void\n+init_dispatch_sched (void)\n+{\n+  /* Allocate a dispatch list and a window.  */\n+  dispatch_window_list = allocate_window ();\n+  dispatch_window_list1 = allocate_window ();\n+  init_window (0);\n+  init_window (1);\n+}\n+\n+/* This function returns true if a branch is detected.  End of a basic block\n+   does not have to be a branch, but here we assume only branches end a\n+   window.  */\n+\n+static bool\n+is_end_basic_block (enum dispatch_group group)\n+{\n+  return group == disp_branch;\n+}\n+\n+/* This function is called when the end of a window processing is reached.  */\n+\n+static void\n+process_end_window (void)\n+{\n+  gcc_assert (dispatch_window_list->num_insn <= MAX_INSN);\n+  if (dispatch_window_list->next)\n+    {\n+      gcc_assert (dispatch_window_list1->num_insn <= MAX_INSN);\n+      gcc_assert (dispatch_window_list->window_size\n+\t\t  + dispatch_window_list1->window_size <= 48);\n+      init_window (1);\n+    }\n+  init_window (0);\n+}\n+\n+/* Allocates a new dispatch window and adds it to WINDOW_LIST.\n+   WINDOW_NUM is either 0 or 1.  A maximum of two windows are generated\n+   for 48 bytes of instructions.  Note that these windows are not dispatch\n+   windows that their sizes are DISPATCH_WINDOW_SIZE.  */\n+\n+static dispatch_windows *\n+allocate_next_window (int window_num)\n+{\n+  if (window_num == 0)\n+    {\n+      if (dispatch_window_list->next)\n+\t  init_window (1);\n+      init_window (0);\n+      return dispatch_window_list;\n+    }\n+\n+  dispatch_window_list->next = dispatch_window_list1;\n+  dispatch_window_list1->prev = dispatch_window_list;\n+\n+  return dispatch_window_list1;\n+}\n+\n+/* Compute number of immediate operands of an instruction.  */\n+\n+static void\n+find_constant (rtx in_rtx, imm_info *imm_values)\n+{\n+  if (INSN_P (in_rtx))\n+    in_rtx = PATTERN (in_rtx);\n+  subrtx_iterator::array_type array;\n+  FOR_EACH_SUBRTX (iter, array, in_rtx, ALL)\n+    if (const_rtx x = *iter)\n+      switch (GET_CODE (x))\n+\t{\n+\tcase CONST:\n+\tcase SYMBOL_REF:\n+\tcase CONST_INT:\n+\t  (imm_values->imm)++;\n+\t  if (x86_64_immediate_operand (CONST_CAST_RTX (x), SImode))\n+\t    (imm_values->imm32)++;\n+\t  else\n+\t    (imm_values->imm64)++;\n+\t  break;\n+\n+\tcase CONST_DOUBLE:\n+\tcase CONST_WIDE_INT:\n+\t  (imm_values->imm)++;\n+\t  (imm_values->imm64)++;\n+\t  break;\n+\n+\tcase CODE_LABEL:\n+\t  if (LABEL_KIND (x) == LABEL_NORMAL)\n+\t    {\n+\t      (imm_values->imm)++;\n+\t      (imm_values->imm32)++;\n+\t    }\n+\t  break;\n+\n+\tdefault:\n+\t  break;\n+\t}\n+}\n+\n+/* Return total size of immediate operands of an instruction along with number\n+   of corresponding immediate-operands.  It initializes its parameters to zero\n+   befor calling FIND_CONSTANT.\n+   INSN is the input instruction.  IMM is the total of immediates.\n+   IMM32 is the number of 32 bit immediates.  IMM64 is the number of 64\n+   bit immediates.  */\n+\n+static int\n+get_num_immediates (rtx_insn *insn, int *imm, int *imm32, int *imm64)\n+{\n+  imm_info imm_values = {0, 0, 0};\n+\n+  find_constant (insn, &imm_values);\n+  *imm = imm_values.imm;\n+  *imm32 = imm_values.imm32;\n+  *imm64 = imm_values.imm64;\n+  return imm_values.imm32 * 4 + imm_values.imm64 * 8;\n+}\n+\n+/* This function indicates if an operand of an instruction is an\n+   immediate.  */\n+\n+static bool\n+has_immediate (rtx_insn *insn)\n+{\n+  int num_imm_operand;\n+  int num_imm32_operand;\n+  int num_imm64_operand;\n+\n+  if (insn)\n+    return get_num_immediates (insn, &num_imm_operand, &num_imm32_operand,\n+\t\t\t       &num_imm64_operand);\n+  return false;\n+}\n+\n+/* Return single or double path for instructions.  */\n+\n+static enum insn_path\n+get_insn_path (rtx_insn *insn)\n+{\n+  enum attr_amdfam10_decode path = get_attr_amdfam10_decode (insn);\n+\n+  if ((int)path == 0)\n+    return path_single;\n+\n+  if ((int)path == 1)\n+    return path_double;\n+\n+  return path_multi;\n+}\n+\n+/* Return insn dispatch group.  */\n+\n+static enum dispatch_group\n+get_insn_group (rtx_insn *insn)\n+{\n+  enum dispatch_group group = get_mem_group (insn);\n+  if (group)\n+    return group;\n+\n+  if (is_branch (insn))\n+    return disp_branch;\n+\n+  if (is_cmp (insn))\n+    return disp_cmp;\n+\n+  if (has_immediate (insn))\n+    return disp_imm;\n+\n+  if (is_prefetch (insn))\n+    return disp_prefetch;\n+\n+  return disp_no_group;\n+}\n+\n+/* Count number of GROUP restricted instructions in a dispatch\n+   window WINDOW_LIST.  */\n+\n+static int\n+count_num_restricted (rtx_insn *insn, dispatch_windows *window_list)\n+{\n+  enum dispatch_group group = get_insn_group (insn);\n+  int imm_size;\n+  int num_imm_operand;\n+  int num_imm32_operand;\n+  int num_imm64_operand;\n+\n+  if (group == disp_no_group)\n+    return 0;\n+\n+  if (group == disp_imm)\n+    {\n+      imm_size = get_num_immediates (insn, &num_imm_operand, &num_imm32_operand,\n+\t\t\t      &num_imm64_operand);\n+      if (window_list->imm_size + imm_size > MAX_IMM_SIZE\n+\t  || num_imm_operand + window_list->num_imm > MAX_IMM\n+\t  || (num_imm32_operand > 0\n+\t      && (window_list->num_imm_32 + num_imm32_operand > MAX_IMM_32\n+\t\t  || window_list->num_imm_64 * 2 + num_imm32_operand > MAX_IMM_32))\n+\t  || (num_imm64_operand > 0\n+\t      && (window_list->num_imm_64 + num_imm64_operand > MAX_IMM_64\n+\t\t  || window_list->num_imm_32 + num_imm64_operand * 2 > MAX_IMM_32))\n+\t  || (window_list->imm_size + imm_size == MAX_IMM_SIZE\n+\t      && num_imm64_operand > 0\n+\t      && ((window_list->num_imm_64 > 0\n+\t\t   && window_list->num_insn >= 2)\n+\t\t  || window_list->num_insn >= 3)))\n+\treturn BIG;\n+\n+      return 1;\n+    }\n+\n+  if ((group == disp_load_store\n+       && (window_list->num_loads >= MAX_LOAD\n+\t   || window_list->num_stores >= MAX_STORE))\n+      || ((group == disp_load\n+\t   || group == disp_prefetch)\n+\t  && window_list->num_loads >= MAX_LOAD)\n+      || (group == disp_store\n+\t  && window_list->num_stores >= MAX_STORE))\n+    return BIG;\n+\n+  return 1;\n+}\n+\n+/* This function returns true if insn satisfies dispatch rules on the\n+   last window scheduled.  */\n+\n+static bool\n+fits_dispatch_window (rtx_insn *insn)\n+{\n+  dispatch_windows *window_list = dispatch_window_list;\n+  dispatch_windows *window_list_next = dispatch_window_list->next;\n+  unsigned int num_restrict;\n+  enum dispatch_group group = get_insn_group (insn);\n+  enum insn_path path = get_insn_path (insn);\n+  int sum;\n+\n+  /* Make disp_cmp and disp_jcc get scheduled at the latest.  These\n+     instructions should be given the lowest priority in the\n+     scheduling process in Haifa scheduler to make sure they will be\n+     scheduled in the same dispatch window as the reference to them.  */\n+  if (group == disp_jcc || group == disp_cmp)\n+    return false;\n+\n+  /* Check nonrestricted.  */\n+  if (group == disp_no_group || group == disp_branch)\n+    return true;\n+\n+  /* Get last dispatch window.  */\n+  if (window_list_next)\n+    window_list = window_list_next;\n+\n+  if (window_list->window_num == 1)\n+    {\n+      sum = window_list->prev->window_size + window_list->window_size;\n+\n+      if (sum == 32\n+\t  || (ix86_min_insn_size (insn) + sum) >= 48)\n+\t/* Window 1 is full.  Go for next window.  */\n+\treturn true;\n+    }\n+\n+  num_restrict = count_num_restricted (insn, window_list);\n+\n+  if (num_restrict > num_allowable_groups[group])\n+    return false;\n+\n+  /* See if it fits in the first window.  */\n+  if (window_list->window_num == 0)\n+    {\n+      /* The first widow should have only single and double path\n+\t uops.  */\n+      if (path == path_double\n+\t  && (window_list->num_uops + 2) > MAX_INSN)\n+\treturn false;\n+      else if (path != path_single)\n+        return false;\n+    }\n+  return true;\n+}\n+\n+/* Add an instruction INSN with NUM_UOPS micro-operations to the\n+   dispatch window WINDOW_LIST.  */\n+\n+static void\n+add_insn_window (rtx_insn *insn, dispatch_windows *window_list, int num_uops)\n+{\n+  int byte_len = ix86_min_insn_size (insn);\n+  int num_insn = window_list->num_insn;\n+  int imm_size;\n+  sched_insn_info *window = window_list->window;\n+  enum dispatch_group group = get_insn_group (insn);\n+  enum insn_path path = get_insn_path (insn);\n+  int num_imm_operand;\n+  int num_imm32_operand;\n+  int num_imm64_operand;\n+\n+  if (!window_list->violation && group != disp_cmp\n+      && !fits_dispatch_window (insn))\n+    window_list->violation = true;\n+\n+  imm_size = get_num_immediates (insn, &num_imm_operand, &num_imm32_operand,\n+\t\t\t\t &num_imm64_operand);\n+\n+  /* Initialize window with new instruction.  */\n+  window[num_insn].insn = insn;\n+  window[num_insn].byte_len = byte_len;\n+  window[num_insn].group = group;\n+  window[num_insn].path = path;\n+  window[num_insn].imm_bytes = imm_size;\n+\n+  window_list->window_size += byte_len;\n+  window_list->num_insn = num_insn + 1;\n+  window_list->num_uops = window_list->num_uops + num_uops;\n+  window_list->imm_size += imm_size;\n+  window_list->num_imm += num_imm_operand;\n+  window_list->num_imm_32 += num_imm32_operand;\n+  window_list->num_imm_64 += num_imm64_operand;\n+\n+  if (group == disp_store)\n+    window_list->num_stores += 1;\n+  else if (group == disp_load\n+\t   || group == disp_prefetch)\n+    window_list->num_loads += 1;\n+  else if (group == disp_load_store)\n+    {\n+      window_list->num_stores += 1;\n+      window_list->num_loads += 1;\n+    }\n+}\n+\n+/* Adds a scheduled instruction, INSN, to the current dispatch window.\n+   If the total bytes of instructions or the number of instructions in\n+   the window exceed allowable, it allocates a new window.  */\n+\n+static void\n+add_to_dispatch_window (rtx_insn *insn)\n+{\n+  int byte_len;\n+  dispatch_windows *window_list;\n+  dispatch_windows *next_list;\n+  dispatch_windows *window0_list;\n+  enum insn_path path;\n+  enum dispatch_group insn_group;\n+  bool insn_fits;\n+  int num_insn;\n+  int num_uops;\n+  int window_num;\n+  int insn_num_uops;\n+  int sum;\n+\n+  if (INSN_CODE (insn) < 0)\n+    return;\n+\n+  byte_len = ix86_min_insn_size (insn);\n+  window_list = dispatch_window_list;\n+  next_list = window_list->next;\n+  path = get_insn_path (insn);\n+  insn_group = get_insn_group (insn);\n+\n+  /* Get the last dispatch window.  */\n+  if (next_list)\n+      window_list = dispatch_window_list->next;\n+\n+  if (path == path_single)\n+    insn_num_uops = 1;\n+  else if (path == path_double)\n+    insn_num_uops = 2;\n+  else\n+    insn_num_uops = (int) path;\n+\n+  /* If current window is full, get a new window.\n+     Window number zero is full, if MAX_INSN uops are scheduled in it.\n+     Window number one is full, if window zero's bytes plus window\n+     one's bytes is 32, or if the bytes of the new instruction added\n+     to the total makes it greater than 48, or it has already MAX_INSN\n+     instructions in it.  */\n+  num_insn = window_list->num_insn;\n+  num_uops = window_list->num_uops;\n+  window_num = window_list->window_num;\n+  insn_fits = fits_dispatch_window (insn);\n+\n+  if (num_insn >= MAX_INSN\n+      || num_uops + insn_num_uops > MAX_INSN\n+      || !(insn_fits))\n+    {\n+      window_num = ~window_num & 1;\n+      window_list = allocate_next_window (window_num);\n+    }\n+\n+  if (window_num == 0)\n+    {\n+      add_insn_window (insn, window_list, insn_num_uops);\n+      if (window_list->num_insn >= MAX_INSN\n+\t  && insn_group == disp_branch)\n+\t{\n+\t  process_end_window ();\n+\t  return;\n+\t}\n+    }\n+  else if (window_num == 1)\n+    {\n+      window0_list = window_list->prev;\n+      sum = window0_list->window_size + window_list->window_size;\n+      if (sum == 32\n+\t  || (byte_len + sum) >= 48)\n+\t{\n+\t  process_end_window ();\n+\t  window_list = dispatch_window_list;\n+\t}\n+\n+      add_insn_window (insn, window_list, insn_num_uops);\n+    }\n+  else\n+    gcc_unreachable ();\n+\n+  if (is_end_basic_block (insn_group))\n+    {\n+      /* End of basic block is reached do end-basic-block process.  */\n+      process_end_window ();\n+      return;\n+    }\n+}\n+\n+/* Print the dispatch window, WINDOW_NUM, to FILE.  */\n+\n+DEBUG_FUNCTION static void\n+debug_dispatch_window_file (FILE *file, int window_num)\n+{\n+  dispatch_windows *list;\n+  int i;\n+\n+  if (window_num == 0)\n+    list = dispatch_window_list;\n+  else\n+    list = dispatch_window_list1;\n+\n+  fprintf (file, \"Window #%d:\\n\", list->window_num);\n+  fprintf (file, \"  num_insn = %d, num_uops = %d, window_size = %d\\n\",\n+\t  list->num_insn, list->num_uops, list->window_size);\n+  fprintf (file, \"  num_imm = %d, num_imm_32 = %d, num_imm_64 = %d, imm_size = %d\\n\",\n+\t   list->num_imm, list->num_imm_32, list->num_imm_64, list->imm_size);\n+\n+  fprintf (file, \"  num_loads = %d, num_stores = %d\\n\", list->num_loads,\n+\t  list->num_stores);\n+  fprintf (file, \" insn info:\\n\");\n+\n+  for (i = 0; i < MAX_INSN; i++)\n+    {\n+      if (!list->window[i].insn)\n+\tbreak;\n+      fprintf (file, \"    group[%d] = %s, insn[%d] = %p, path[%d] = %d byte_len[%d] = %d, imm_bytes[%d] = %d\\n\",\n+\t      i, group_name[list->window[i].group],\n+\t      i, (void *)list->window[i].insn,\n+\t      i, list->window[i].path,\n+\t      i, list->window[i].byte_len,\n+\t      i, list->window[i].imm_bytes);\n+    }\n+}\n+\n+/* Print to stdout a dispatch window.  */\n+\n+DEBUG_FUNCTION void\n+debug_dispatch_window (int window_num)\n+{\n+  debug_dispatch_window_file (stdout, window_num);\n+}\n+\n+/* Print INSN dispatch information to FILE.  */\n+\n+DEBUG_FUNCTION static void\n+debug_insn_dispatch_info_file (FILE *file, rtx_insn *insn)\n+{\n+  int byte_len;\n+  enum insn_path path;\n+  enum dispatch_group group;\n+  int imm_size;\n+  int num_imm_operand;\n+  int num_imm32_operand;\n+  int num_imm64_operand;\n+\n+  if (INSN_CODE (insn) < 0)\n+    return;\n+\n+  byte_len = ix86_min_insn_size (insn);\n+  path = get_insn_path (insn);\n+  group = get_insn_group (insn);\n+  imm_size = get_num_immediates (insn, &num_imm_operand, &num_imm32_operand,\n+\t\t\t\t &num_imm64_operand);\n+\n+  fprintf (file, \" insn info:\\n\");\n+  fprintf (file, \"  group = %s, path = %d, byte_len = %d\\n\",\n+\t   group_name[group], path, byte_len);\n+  fprintf (file, \"  num_imm = %d, num_imm_32 = %d, num_imm_64 = %d, imm_size = %d\\n\",\n+\t   num_imm_operand, num_imm32_operand, num_imm64_operand, imm_size);\n+}\n+\n+/* Print to STDERR the status of the ready list with respect to\n+   dispatch windows.  */\n+\n+DEBUG_FUNCTION void\n+debug_ready_dispatch (void)\n+{\n+  int i;\n+  int no_ready = number_in_ready ();\n+\n+  fprintf (stdout, \"Number of ready: %d\\n\", no_ready);\n+\n+  for (i = 0; i < no_ready; i++)\n+    debug_insn_dispatch_info_file (stdout, get_ready_element (i));\n+}\n+\n+/* This routine is the driver of the dispatch scheduler.  */\n+\n+void\n+ix86_bd_do_dispatch (rtx_insn *insn, int mode)\n+{\n+  if (mode == DISPATCH_INIT)\n+    init_dispatch_sched ();\n+  else if (mode == ADD_TO_DISPATCH_WINDOW)\n+    add_to_dispatch_window (insn);\n+}\n+\n+/* Return TRUE if Dispatch Scheduling is supported.  */\n+\n+bool\n+ix86_bd_has_dispatch (rtx_insn *insn, int action)\n+{\n+  /* Current implementation of dispatch scheduler models buldozer only.  */\n+  if ((TARGET_BDVER1 || TARGET_BDVER2 || TARGET_BDVER3\n+      || TARGET_BDVER4) && flag_dispatch_scheduler)\n+    switch (action)\n+      {\n+      default:\n+\treturn false;\n+\n+      case IS_DISPATCH_ON:\n+\treturn true;\n+\n+      case IS_CMP:\n+\treturn is_cmp (insn);\n+\n+      case DISPATCH_VIOLATION:\n+\treturn dispatch_violation ();\n+\n+      case FITS_DISPATCH_WINDOW:\n+\treturn fits_dispatch_window (insn);\n+      }\n+\n+  return false;\n+}"}, {"sha": "67b14a708e8c306951eafca1b0d45122d5d1ee0b", "filename": "gcc/config/i386/x86-tune-sched-core.c", "status": "added", "additions": 255, "deletions": 0, "changes": 255, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-core.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-core.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-core.c?ref=64766e8dc78b92fc906e21429b1befd2b248f96e", "patch": "@@ -0,0 +1,255 @@\n+/* Scheduler hooks for IA-32 which implement bdver1-4 specific logic.\n+   Copyright (C) 1988-2017 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"backend.h\"\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"cfghooks.h\"\n+#include \"tm_p.h\"\n+#include \"insn-config.h\"\n+#include \"insn-attr.h\"\n+#include \"recog.h\"\n+#include \"target.h\"\n+#include \"rtl-iter.h\"\n+#include \"regset.h\"\n+#include \"sched-int.h\"\n+\n+\n+/* Model decoder of Core 2/i7.\n+   Below hooks for multipass scheduling (see haifa-sched.c:max_issue)\n+   track the instruction fetch block boundaries and make sure that long\n+   (9+ bytes) instructions are assigned to D0.  */\n+\n+/* Maximum length of an insn that can be handled by\n+   a secondary decoder unit.  '8' for Core 2/i7.  */\n+static int core2i7_secondary_decoder_max_insn_size;\n+\n+/* Ifetch block size, i.e., number of bytes decoder reads per cycle.\n+   '16' for Core 2/i7.  */\n+static int core2i7_ifetch_block_size;\n+\n+/* Maximum number of instructions decoder can handle per cycle.\n+   '6' for Core 2/i7.  */\n+static int core2i7_ifetch_block_max_insns;\n+\n+typedef struct ix86_first_cycle_multipass_data_ *\n+  ix86_first_cycle_multipass_data_t;\n+typedef const struct ix86_first_cycle_multipass_data_ *\n+  const_ix86_first_cycle_multipass_data_t;\n+\n+/* A variable to store target state across calls to max_issue within\n+   one cycle.  */\n+static struct ix86_first_cycle_multipass_data_ _ix86_first_cycle_multipass_data,\n+  *ix86_first_cycle_multipass_data = &_ix86_first_cycle_multipass_data;\n+\n+/* Initialize DATA.  */\n+static void\n+core2i7_first_cycle_multipass_init (void *_data)\n+{\n+  ix86_first_cycle_multipass_data_t data\n+    = (ix86_first_cycle_multipass_data_t) _data;\n+\n+  data->ifetch_block_len = 0;\n+  data->ifetch_block_n_insns = 0;\n+  data->ready_try_change = NULL;\n+  data->ready_try_change_size = 0;\n+}\n+\n+/* Advancing the cycle; reset ifetch block counts.  */\n+static void\n+core2i7_dfa_post_advance_cycle (void)\n+{\n+  ix86_first_cycle_multipass_data_t data = ix86_first_cycle_multipass_data;\n+\n+  gcc_assert (data->ifetch_block_n_insns <= core2i7_ifetch_block_max_insns);\n+\n+  data->ifetch_block_len = 0;\n+  data->ifetch_block_n_insns = 0;\n+}\n+\n+/* Filter out insns from ready_try that the core will not be able to issue\n+   on current cycle due to decoder.  */\n+static void\n+core2i7_first_cycle_multipass_filter_ready_try\n+(const_ix86_first_cycle_multipass_data_t data,\n+ signed char *ready_try, int n_ready, bool first_cycle_insn_p)\n+{\n+  while (n_ready--)\n+    {\n+      rtx_insn *insn;\n+      int insn_size;\n+\n+      if (ready_try[n_ready])\n+\tcontinue;\n+\n+      insn = get_ready_element (n_ready);\n+      insn_size = ix86_min_insn_size (insn);\n+\n+      if (/* If this is a too long an insn for a secondary decoder ...  */\n+\t  (!first_cycle_insn_p\n+\t   && insn_size > core2i7_secondary_decoder_max_insn_size)\n+\t  /* ... or it would not fit into the ifetch block ...  */\n+\t  || data->ifetch_block_len + insn_size > core2i7_ifetch_block_size\n+\t  /* ... or the decoder is full already ...  */\n+\t  || data->ifetch_block_n_insns + 1 > core2i7_ifetch_block_max_insns)\n+\t/* ... mask the insn out.  */\n+\t{\n+\t  ready_try[n_ready] = 1;\n+\n+\t  if (data->ready_try_change)\n+\t    bitmap_set_bit (data->ready_try_change, n_ready);\n+\t}\n+    }\n+}\n+\n+/* Prepare for a new round of multipass lookahead scheduling.  */\n+static void\n+core2i7_first_cycle_multipass_begin (void *_data,\n+\t\t\t\t     signed char *ready_try, int n_ready,\n+\t\t\t\t     bool first_cycle_insn_p)\n+{\n+  ix86_first_cycle_multipass_data_t data\n+    = (ix86_first_cycle_multipass_data_t) _data;\n+  const_ix86_first_cycle_multipass_data_t prev_data\n+    = ix86_first_cycle_multipass_data;\n+\n+  /* Restore the state from the end of the previous round.  */\n+  data->ifetch_block_len = prev_data->ifetch_block_len;\n+  data->ifetch_block_n_insns = prev_data->ifetch_block_n_insns;\n+\n+  /* Filter instructions that cannot be issued on current cycle due to\n+     decoder restrictions.  */\n+  core2i7_first_cycle_multipass_filter_ready_try (data, ready_try, n_ready,\n+\t\t\t\t\t\t  first_cycle_insn_p);\n+}\n+\n+/* INSN is being issued in current solution.  Account for its impact on\n+   the decoder model.  */\n+static void\n+core2i7_first_cycle_multipass_issue (void *_data,\n+\t\t\t\t     signed char *ready_try, int n_ready,\n+\t\t\t\t     rtx_insn *insn, const void *_prev_data)\n+{\n+  ix86_first_cycle_multipass_data_t data\n+    = (ix86_first_cycle_multipass_data_t) _data;\n+  const_ix86_first_cycle_multipass_data_t prev_data\n+    = (const_ix86_first_cycle_multipass_data_t) _prev_data;\n+\n+  int insn_size = ix86_min_insn_size (insn);\n+\n+  data->ifetch_block_len = prev_data->ifetch_block_len + insn_size;\n+  data->ifetch_block_n_insns = prev_data->ifetch_block_n_insns + 1;\n+  gcc_assert (data->ifetch_block_len <= core2i7_ifetch_block_size\n+\t      && data->ifetch_block_n_insns <= core2i7_ifetch_block_max_insns);\n+\n+  /* Allocate or resize the bitmap for storing INSN's effect on ready_try.  */\n+  if (!data->ready_try_change)\n+    {\n+      data->ready_try_change = sbitmap_alloc (n_ready);\n+      data->ready_try_change_size = n_ready;\n+    }\n+  else if (data->ready_try_change_size < n_ready)\n+    {\n+      data->ready_try_change = sbitmap_resize (data->ready_try_change,\n+\t\t\t\t\t       n_ready, 0);\n+      data->ready_try_change_size = n_ready;\n+    }\n+  bitmap_clear (data->ready_try_change);\n+\n+  /* Filter out insns from ready_try that the core will not be able to issue\n+     on current cycle due to decoder.  */\n+  core2i7_first_cycle_multipass_filter_ready_try (data, ready_try, n_ready,\n+\t\t\t\t\t\t  false);\n+}\n+\n+/* Revert the effect on ready_try.  */\n+static void\n+core2i7_first_cycle_multipass_backtrack (const void *_data,\n+\t\t\t\t\t signed char *ready_try,\n+\t\t\t\t\t int n_ready ATTRIBUTE_UNUSED)\n+{\n+  const_ix86_first_cycle_multipass_data_t data\n+    = (const_ix86_first_cycle_multipass_data_t) _data;\n+  unsigned int i = 0;\n+  sbitmap_iterator sbi;\n+\n+  gcc_assert (bitmap_last_set_bit (data->ready_try_change) < n_ready);\n+  EXECUTE_IF_SET_IN_BITMAP (data->ready_try_change, 0, i, sbi)\n+    {\n+      ready_try[i] = 0;\n+    }\n+}\n+\n+/* Save the result of multipass lookahead scheduling for the next round.  */\n+static void\n+core2i7_first_cycle_multipass_end (const void *_data)\n+{\n+  const_ix86_first_cycle_multipass_data_t data\n+    = (const_ix86_first_cycle_multipass_data_t) _data;\n+  ix86_first_cycle_multipass_data_t next_data\n+    = ix86_first_cycle_multipass_data;\n+\n+  if (data != NULL)\n+    {\n+      next_data->ifetch_block_len = data->ifetch_block_len;\n+      next_data->ifetch_block_n_insns = data->ifetch_block_n_insns;\n+    }\n+}\n+\n+/* Deallocate target data.  */\n+static void\n+core2i7_first_cycle_multipass_fini (void *_data)\n+{\n+  ix86_first_cycle_multipass_data_t data\n+    = (ix86_first_cycle_multipass_data_t) _data;\n+\n+  if (data->ready_try_change)\n+    {\n+      sbitmap_free (data->ready_try_change);\n+      data->ready_try_change = NULL;\n+      data->ready_try_change_size = 0;\n+    }\n+}\n+\n+void\n+ix86_core2i7_init_hooks (void)\n+{\n+  targetm.sched.dfa_post_advance_cycle\n+    = core2i7_dfa_post_advance_cycle;\n+  targetm.sched.first_cycle_multipass_init\n+    = core2i7_first_cycle_multipass_init;\n+  targetm.sched.first_cycle_multipass_begin\n+    = core2i7_first_cycle_multipass_begin;\n+  targetm.sched.first_cycle_multipass_issue\n+    = core2i7_first_cycle_multipass_issue;\n+  targetm.sched.first_cycle_multipass_backtrack\n+    = core2i7_first_cycle_multipass_backtrack;\n+  targetm.sched.first_cycle_multipass_end\n+    = core2i7_first_cycle_multipass_end;\n+  targetm.sched.first_cycle_multipass_fini\n+    = core2i7_first_cycle_multipass_fini;\n+\n+  /* Set decoder parameters.  */\n+  core2i7_secondary_decoder_max_insn_size = 8;\n+  core2i7_ifetch_block_size = 16;\n+  core2i7_ifetch_block_max_insns = 6;\n+}"}, {"sha": "51fa77c389abadb6c26753fb8148700a3060e1df", "filename": "gcc/config/i386/x86-tune-sched.c", "status": "added", "additions": 599, "deletions": 0, "changes": 599, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fx86-tune-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64766e8dc78b92fc906e21429b1befd2b248f96e/gcc%2Fconfig%2Fi386%2Fx86-tune-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune-sched.c?ref=64766e8dc78b92fc906e21429b1befd2b248f96e", "patch": "@@ -0,0 +1,599 @@\n+/* Scheduler hooks for IA-32 which implement CPU specific logic.\n+   Copyright (C) 1988-2017 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"backend.h\"\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"cfghooks.h\"\n+#include \"tm_p.h\"\n+#include \"insn-config.h\"\n+#include \"insn-attr.h\"\n+#include \"recog.h\"\n+#include \"target.h\"\n+\n+/* Return the maximum number of instructions a cpu can issue.  */\n+\n+int\n+ix86_issue_rate (void)\n+{\n+  switch (ix86_tune)\n+    {\n+    case PROCESSOR_PENTIUM:\n+    case PROCESSOR_LAKEMONT:\n+    case PROCESSOR_BONNELL:\n+    case PROCESSOR_SILVERMONT:\n+    case PROCESSOR_KNL:\n+    case PROCESSOR_KNM:\n+    case PROCESSOR_INTEL:\n+    case PROCESSOR_K6:\n+    case PROCESSOR_BTVER2:\n+    case PROCESSOR_PENTIUM4:\n+    case PROCESSOR_NOCONA:\n+      return 2;\n+\n+    case PROCESSOR_PENTIUMPRO:\n+    case PROCESSOR_ATHLON:\n+    case PROCESSOR_K8:\n+    case PROCESSOR_AMDFAM10:\n+    case PROCESSOR_GENERIC:\n+    case PROCESSOR_BTVER1:\n+      return 3;\n+\n+    case PROCESSOR_BDVER1:\n+    case PROCESSOR_BDVER2:\n+    case PROCESSOR_BDVER3:\n+    case PROCESSOR_BDVER4:\n+    case PROCESSOR_ZNVER1:\n+    case PROCESSOR_CORE2:\n+    case PROCESSOR_NEHALEM:\n+    case PROCESSOR_SANDYBRIDGE:\n+    case PROCESSOR_HASWELL:\n+      return 4;\n+\n+    default:\n+      return 1;\n+    }\n+}\n+\n+/* Return true iff USE_INSN has a memory address with operands set by\n+   SET_INSN.  */\n+\n+bool\n+ix86_agi_dependent (rtx_insn *set_insn, rtx_insn *use_insn)\n+{\n+  int i;\n+  extract_insn_cached (use_insn);\n+  for (i = recog_data.n_operands - 1; i >= 0; --i)\n+    if (MEM_P (recog_data.operand[i]))\n+      {\n+\trtx addr = XEXP (recog_data.operand[i], 0);\n+\tif (modified_in_p (addr, set_insn) != 0)\n+\t  {\n+\t    /* No AGI stall if SET_INSN is a push or pop and USE_INSN\n+\t       has SP based memory (unless index reg is modified in a pop).  */\n+\t    rtx set = single_set (set_insn);\n+\t    if (set\n+\t\t&& (push_operand (SET_DEST (set), GET_MODE (SET_DEST (set)))\n+\t\t    || pop_operand (SET_SRC (set), GET_MODE (SET_SRC (set)))))\n+\t      {\n+\t\tstruct ix86_address parts;\n+\t\tif (ix86_decompose_address (addr, &parts)\n+\t\t    && parts.base == stack_pointer_rtx\n+\t\t    && (parts.index == NULL_RTX\n+\t\t\t|| MEM_P (SET_DEST (set))\n+\t\t\t|| !modified_in_p (parts.index, set_insn)))\n+\t\t  return false;\n+\t      }\n+\t    return true;\n+\t  }\n+\treturn false;\n+      }\n+  return false;\n+}\n+\n+/* A subroutine of ix86_adjust_cost -- return TRUE iff INSN reads flags set\n+   by DEP_INSN and nothing set by DEP_INSN.  */\n+\n+static bool\n+ix86_flags_dependent (rtx_insn *insn, rtx_insn *dep_insn, enum attr_type insn_type)\n+{\n+  rtx set, set2;\n+\n+  /* Simplify the test for uninteresting insns.  */\n+  if (insn_type != TYPE_SETCC\n+      && insn_type != TYPE_ICMOV\n+      && insn_type != TYPE_FCMOV\n+      && insn_type != TYPE_IBR)\n+    return false;\n+\n+  if ((set = single_set (dep_insn)) != 0)\n+    {\n+      set = SET_DEST (set);\n+      set2 = NULL_RTX;\n+    }\n+  else if (GET_CODE (PATTERN (dep_insn)) == PARALLEL\n+\t   && XVECLEN (PATTERN (dep_insn), 0) == 2\n+\t   && GET_CODE (XVECEXP (PATTERN (dep_insn), 0, 0)) == SET\n+\t   && GET_CODE (XVECEXP (PATTERN (dep_insn), 0, 1)) == SET)\n+    {\n+      set = SET_DEST (XVECEXP (PATTERN (dep_insn), 0, 0));\n+      set2 = SET_DEST (XVECEXP (PATTERN (dep_insn), 0, 0));\n+    }\n+  else\n+    return false;\n+\n+  if (!REG_P (set) || REGNO (set) != FLAGS_REG)\n+    return false;\n+\n+  /* This test is true if the dependent insn reads the flags but\n+     not any other potentially set register.  */\n+  if (!reg_overlap_mentioned_p (set, PATTERN (insn)))\n+    return false;\n+\n+  if (set2 && reg_overlap_mentioned_p (set2, PATTERN (insn)))\n+    return false;\n+\n+  return true;\n+}\n+\n+/* Helper function for exact_store_load_dependency.\n+   Return true if addr is found in insn.  */\n+static bool\n+exact_dependency_1 (rtx addr, rtx insn)\n+{\n+  enum rtx_code code;\n+  const char *format_ptr;\n+  int i, j;\n+\n+  code = GET_CODE (insn);\n+  switch (code)\n+    {\n+    case MEM:\n+      if (rtx_equal_p (addr, insn))\n+\treturn true;\n+      break;\n+    case REG:\n+    CASE_CONST_ANY:\n+    case SYMBOL_REF:\n+    case CODE_LABEL:\n+    case PC:\n+    case CC0:\n+    case EXPR_LIST:\n+      return false;\n+    default:\n+      break;\n+    }\n+\n+  format_ptr = GET_RTX_FORMAT (code);\n+  for (i = 0; i < GET_RTX_LENGTH (code); i++)\n+    {\n+      switch (*format_ptr++)\n+\t{\n+\tcase 'e':\n+\t  if (exact_dependency_1 (addr, XEXP (insn, i)))\n+\t    return true;\n+\t  break;\n+\tcase 'E':\n+\t  for (j = 0; j < XVECLEN (insn, i); j++)\n+\t    if (exact_dependency_1 (addr, XVECEXP (insn, i, j)))\n+\t      return true;\n+\t  break;\n+\t}\n+    }\n+  return false;\n+}\n+\n+/* Return true if there exists exact dependency for store & load, i.e.\n+   the same memory address is used in them.  */\n+static bool\n+exact_store_load_dependency (rtx_insn *store, rtx_insn *load)\n+{\n+  rtx set1, set2;\n+\n+  set1 = single_set (store);\n+  if (!set1)\n+    return false;\n+  if (!MEM_P (SET_DEST (set1)))\n+    return false;\n+  set2 = single_set (load);\n+  if (!set2)\n+    return false;\n+  if (exact_dependency_1 (SET_DEST (set1), SET_SRC (set2)))\n+    return true;\n+  return false;\n+}\n+\n+\n+/* This function corrects the value of COST (latency) based on the relationship\n+   between INSN and DEP_INSN through a dependence of type DEP_TYPE, and strength\n+   DW.  It should return the new value.\n+\n+   On x86 CPUs this is most commonly used to model the fact that valus of\n+   registers used to compute address of memory operand  needs to be ready\n+   earlier than values of registers used in the actual operation.  */\n+\n+int\n+ix86_adjust_cost (rtx_insn *insn, int dep_type, rtx_insn *dep_insn, int cost,\n+\t\t  unsigned int)\n+{\n+  enum attr_type insn_type, dep_insn_type;\n+  enum attr_memory memory;\n+  rtx set, set2;\n+  int dep_insn_code_number;\n+\n+  /* Anti and output dependencies have zero cost on all CPUs.  */\n+  if (dep_type != 0)\n+    return 0;\n+\n+  dep_insn_code_number = recog_memoized (dep_insn);\n+\n+  /* If we can't recognize the insns, we can't really do anything.  */\n+  if (dep_insn_code_number < 0 || recog_memoized (insn) < 0)\n+    return cost;\n+\n+  insn_type = get_attr_type (insn);\n+  dep_insn_type = get_attr_type (dep_insn);\n+\n+  switch (ix86_tune)\n+    {\n+    case PROCESSOR_PENTIUM:\n+    case PROCESSOR_LAKEMONT:\n+      /* Address Generation Interlock adds a cycle of latency.  */\n+      if (insn_type == TYPE_LEA)\n+\t{\n+\t  rtx addr = PATTERN (insn);\n+\n+\t  if (GET_CODE (addr) == PARALLEL)\n+\t    addr = XVECEXP (addr, 0, 0);\n+\n+\t  gcc_assert (GET_CODE (addr) == SET);\n+\n+\t  addr = SET_SRC (addr);\n+\t  if (modified_in_p (addr, dep_insn))\n+\t    cost += 1;\n+\t}\n+      else if (ix86_agi_dependent (dep_insn, insn))\n+\tcost += 1;\n+\n+      /* ??? Compares pair with jump/setcc.  */\n+      if (ix86_flags_dependent (insn, dep_insn, insn_type))\n+\tcost = 0;\n+\n+      /* Floating point stores require value to be ready one cycle earlier.  */\n+      if (insn_type == TYPE_FMOV\n+\t  && get_attr_memory (insn) == MEMORY_STORE\n+\t  && !ix86_agi_dependent (dep_insn, insn))\n+\tcost += 1;\n+      break;\n+\n+    case PROCESSOR_PENTIUMPRO:\n+      /* INT->FP conversion is expensive.  */\n+      if (get_attr_fp_int_src (dep_insn))\n+\tcost += 5;\n+\n+      /* There is one cycle extra latency between an FP op and a store.  */\n+      if (insn_type == TYPE_FMOV\n+\t  && (set = single_set (dep_insn)) != NULL_RTX\n+\t  && (set2 = single_set (insn)) != NULL_RTX\n+\t  && rtx_equal_p (SET_DEST (set), SET_SRC (set2))\n+\t  && MEM_P (SET_DEST (set2)))\n+\tcost += 1;\n+\n+      memory = get_attr_memory (insn);\n+\n+      /* Show ability of reorder buffer to hide latency of load by executing\n+\t in parallel with previous instruction in case\n+\t previous instruction is not needed to compute the address.  */\n+      if ((memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n+\t  && !ix86_agi_dependent (dep_insn, insn))\n+\t{\n+\t  /* Claim moves to take one cycle, as core can issue one load\n+\t     at time and the next load can start cycle later.  */\n+\t  if (dep_insn_type == TYPE_IMOV\n+\t      || dep_insn_type == TYPE_FMOV)\n+\t    cost = 1;\n+\t  else if (cost > 1)\n+\t    cost--;\n+\t}\n+      break;\n+\n+    case PROCESSOR_K6:\n+     /* The esp dependency is resolved before\n+\tthe instruction is really finished.  */\n+      if ((insn_type == TYPE_PUSH || insn_type == TYPE_POP)\n+\t  && (dep_insn_type == TYPE_PUSH || dep_insn_type == TYPE_POP))\n+\treturn 1;\n+\n+      /* INT->FP conversion is expensive.  */\n+      if (get_attr_fp_int_src (dep_insn))\n+\tcost += 5;\n+\n+      memory = get_attr_memory (insn);\n+\n+      /* Show ability of reorder buffer to hide latency of load by executing\n+\t in parallel with previous instruction in case\n+\t previous instruction is not needed to compute the address.  */\n+      if ((memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n+\t  && !ix86_agi_dependent (dep_insn, insn))\n+\t{\n+\t  /* Claim moves to take one cycle, as core can issue one load\n+\t     at time and the next load can start cycle later.  */\n+\t  if (dep_insn_type == TYPE_IMOV\n+\t      || dep_insn_type == TYPE_FMOV)\n+\t    cost = 1;\n+\t  else if (cost > 2)\n+\t    cost -= 2;\n+\t  else\n+\t    cost = 1;\n+\t}\n+      break;\n+\n+    case PROCESSOR_AMDFAM10:\n+    case PROCESSOR_BDVER1:\n+    case PROCESSOR_BDVER2:\n+    case PROCESSOR_BDVER3:\n+    case PROCESSOR_BDVER4:\n+    case PROCESSOR_ZNVER1:\n+    case PROCESSOR_BTVER1:\n+    case PROCESSOR_BTVER2:\n+    case PROCESSOR_GENERIC:\n+      /* Stack engine allows to execute push&pop instructions in parall.  */\n+      if ((insn_type == TYPE_PUSH || insn_type == TYPE_POP)\n+\t  && (dep_insn_type == TYPE_PUSH || dep_insn_type == TYPE_POP))\n+\treturn 0;\n+      /* FALLTHRU */\n+\n+    case PROCESSOR_ATHLON:\n+    case PROCESSOR_K8:\n+      memory = get_attr_memory (insn);\n+\n+      /* Show ability of reorder buffer to hide latency of load by executing\n+\t in parallel with previous instruction in case\n+\t previous instruction is not needed to compute the address.  */\n+      if ((memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n+\t  && !ix86_agi_dependent (dep_insn, insn))\n+\t{\n+\t  enum attr_unit unit = get_attr_unit (insn);\n+\t  int loadcost = 3;\n+\n+\t  /* Because of the difference between the length of integer and\n+\t     floating unit pipeline preparation stages, the memory operands\n+\t     for floating point are cheaper.\n+\n+\t     ??? For Athlon it the difference is most probably 2.  */\n+\t  if (unit == UNIT_INTEGER || unit == UNIT_UNKNOWN)\n+\t    loadcost = 3;\n+\t  else\n+\t    loadcost = TARGET_ATHLON ? 2 : 0;\n+\n+\t  if (cost >= loadcost)\n+\t    cost -= loadcost;\n+\t  else\n+\t    cost = 0;\n+\t}\n+      break;\n+\n+    case PROCESSOR_CORE2:\n+    case PROCESSOR_NEHALEM:\n+    case PROCESSOR_SANDYBRIDGE:\n+    case PROCESSOR_HASWELL:\n+      /* Stack engine allows to execute push&pop instructions in parall.  */\n+      if ((insn_type == TYPE_PUSH || insn_type == TYPE_POP)\n+\t  && (dep_insn_type == TYPE_PUSH || dep_insn_type == TYPE_POP))\n+\treturn 0;\n+\n+      memory = get_attr_memory (insn);\n+\n+      /* Show ability of reorder buffer to hide latency of load by executing\n+\t in parallel with previous instruction in case\n+\t previous instruction is not needed to compute the address.  */\n+      if ((memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n+\t  && !ix86_agi_dependent (dep_insn, insn))\n+\t{\n+\t  if (cost >= 4)\n+\t    cost -= 4;\n+\t  else\n+\t    cost = 0;\n+\t}\n+      break;\n+\n+    case PROCESSOR_SILVERMONT:\n+    case PROCESSOR_KNL:\n+    case PROCESSOR_KNM:\n+    case PROCESSOR_INTEL:\n+      if (!reload_completed)\n+\treturn cost;\n+\n+      /* Increase cost of integer loads.  */\n+      memory = get_attr_memory (dep_insn);\n+      if (memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n+\t{\n+\t  enum attr_unit unit = get_attr_unit (dep_insn);\n+\t  if (unit == UNIT_INTEGER && cost == 1)\n+\t    {\n+\t      if (memory == MEMORY_LOAD)\n+\t\tcost = 3;\n+\t      else\n+\t\t{\n+\t\t  /* Increase cost of ld/st for short int types only\n+\t\t     because of store forwarding issue.  */\n+\t\t  rtx set = single_set (dep_insn);\n+\t\t  if (set && (GET_MODE (SET_DEST (set)) == QImode\n+\t\t\t      || GET_MODE (SET_DEST (set)) == HImode))\n+\t\t    {\n+\t\t      /* Increase cost of store/load insn if exact\n+\t\t\t dependence exists and it is load insn.  */\n+\t\t      enum attr_memory insn_memory = get_attr_memory (insn);\n+\t\t      if (insn_memory == MEMORY_LOAD\n+\t\t\t  && exact_store_load_dependency (dep_insn, insn))\n+\t\t\tcost = 3;\n+\t\t    }\n+\t\t}\n+\t    }\n+\t}\n+\n+    default:\n+      break;\n+    }\n+\n+  return cost;\n+}\n+\n+/* How many alternative schedules to try.  This should be as wide as the\n+   scheduling freedom in the DFA, but no wider.  Making this value too\n+   large results extra work for the scheduler.  */\n+\n+int\n+ia32_multipass_dfa_lookahead (void)\n+{\n+  /* Generally, we want haifa-sched:max_issue() to look ahead as far\n+     as many instructions can be executed on a cycle, i.e.,\n+     issue_rate.  */\n+  if (reload_completed)\n+    return ix86_issue_rate ();\n+  /* Don't use lookahead for pre-reload schedule to save compile time.  */\n+  return 0;\n+}\n+\n+/* Return true if target platform supports macro-fusion.  */\n+\n+bool\n+ix86_macro_fusion_p ()\n+{\n+  return TARGET_FUSE_CMP_AND_BRANCH;\n+}\n+\n+/* Check whether current microarchitecture support macro fusion\n+   for insn pair \"CONDGEN + CONDJMP\". Refer to\n+   \"Intel Architectures Optimization Reference Manual\". */\n+\n+bool\n+ix86_macro_fusion_pair_p (rtx_insn *condgen, rtx_insn *condjmp)\n+{\n+  rtx src, dest;\n+  enum rtx_code ccode;\n+  rtx compare_set = NULL_RTX, test_if, cond;\n+  rtx alu_set = NULL_RTX, addr = NULL_RTX;\n+\n+  if (!any_condjump_p (condjmp))\n+    return false;\n+\n+  unsigned int condreg1, condreg2;\n+  rtx cc_reg_1;\n+  targetm.fixed_condition_code_regs (&condreg1, &condreg2);\n+  cc_reg_1 = gen_rtx_REG (CCmode, condreg1);\n+  if (!reg_referenced_p (cc_reg_1, PATTERN (condjmp))\n+      || !condgen\n+      || !modified_in_p (cc_reg_1, condgen))\n+    return false;\n+\n+  if (get_attr_type (condgen) != TYPE_TEST\n+      && get_attr_type (condgen) != TYPE_ICMP\n+      && get_attr_type (condgen) != TYPE_INCDEC\n+      && get_attr_type (condgen) != TYPE_ALU)\n+    return false;\n+\n+  compare_set = single_set (condgen);\n+  if (compare_set == NULL_RTX\n+      && !TARGET_FUSE_ALU_AND_BRANCH)\n+    return false;\n+\n+  if (compare_set == NULL_RTX)\n+    {\n+      int i;\n+      rtx pat = PATTERN (condgen);\n+      for (i = 0; i < XVECLEN (pat, 0); i++)\n+\tif (GET_CODE (XVECEXP (pat, 0, i)) == SET)\n+\t  {\n+\t    rtx set_src = SET_SRC (XVECEXP (pat, 0, i));\n+\t    if (GET_CODE (set_src) == COMPARE)\n+\t      compare_set = XVECEXP (pat, 0, i);\n+\t    else\n+\t      alu_set = XVECEXP (pat, 0, i);\n+\t  }\n+    }\n+  if (compare_set == NULL_RTX)\n+    return false;\n+  src = SET_SRC (compare_set);\n+  if (GET_CODE (src) != COMPARE)\n+    return false;\n+\n+  /* Macro-fusion for cmp/test MEM-IMM + conditional jmp is not\n+     supported.  */\n+  if ((MEM_P (XEXP (src, 0))\n+       && CONST_INT_P (XEXP (src, 1)))\n+      || (MEM_P (XEXP (src, 1))\n+\t  && CONST_INT_P (XEXP (src, 0))))\n+    return false;\n+\n+  /* No fusion for RIP-relative address.  */\n+  if (MEM_P (XEXP (src, 0)))\n+    addr = XEXP (XEXP (src, 0), 0);\n+  else if (MEM_P (XEXP (src, 1)))\n+    addr = XEXP (XEXP (src, 1), 0);\n+\n+  if (addr) {\n+    ix86_address parts;\n+    int ok = ix86_decompose_address (addr, &parts);\n+    gcc_assert (ok);\n+\n+    if (ix86_rip_relative_addr_p (&parts))\n+      return false;\n+  }\n+\n+  test_if = SET_SRC (pc_set (condjmp));\n+  cond = XEXP (test_if, 0);\n+  ccode = GET_CODE (cond);\n+  /* Check whether conditional jump use Sign or Overflow Flags.  */\n+  if (!TARGET_FUSE_CMP_AND_BRANCH_SOFLAGS\n+      && (ccode == GE\n+          || ccode == GT\n+\t  || ccode == LE\n+\t  || ccode == LT))\n+    return false;\n+\n+  /* Return true for TYPE_TEST and TYPE_ICMP.  */\n+  if (get_attr_type (condgen) == TYPE_TEST\n+      || get_attr_type (condgen) == TYPE_ICMP)\n+    return true;\n+\n+  /* The following is the case that macro-fusion for alu + jmp.  */\n+  if (!TARGET_FUSE_ALU_AND_BRANCH || !alu_set)\n+    return false;\n+\n+  /* No fusion for alu op with memory destination operand.  */\n+  dest = SET_DEST (alu_set);\n+  if (MEM_P (dest))\n+    return false;\n+\n+  /* Macro-fusion for inc/dec + unsigned conditional jump is not\n+     supported.  */\n+  if (get_attr_type (condgen) == TYPE_INCDEC\n+      && (ccode == GEU\n+\t  || ccode == GTU\n+\t  || ccode == LEU\n+\t  || ccode == LTU))\n+    return false;\n+\n+  return true;\n+}\n+"}]}