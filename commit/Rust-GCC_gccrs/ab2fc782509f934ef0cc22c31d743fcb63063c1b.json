{"sha": "ab2fc782509f934ef0cc22c31d743fcb63063c1b", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YWIyZmM3ODI1MDlmOTM0ZWYwY2MyMmMzMWQ3NDNmY2I2MzA2M2MxYg==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2018-01-13T18:01:42Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2018-01-13T18:01:42Z"}, "message": "Use gather loads for strided accesses\n\nThis patch tries to use gather loads for strided accesses,\nrather than falling back to VMAT_ELEMENTWISE.\n\n2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood  <david.sherwood@arm.com>\n\ngcc/\n\t* tree-vectorizer.h (vect_create_data_ref_ptr): Take an extra\n\toptional tree argument.\n\t* tree-vect-data-refs.c (vect_check_gather_scatter): Check for\n\tnull target hooks.\n\t(vect_create_data_ref_ptr): Take the iv_step as an optional argument,\n\tbut continue to use the current value as a fallback.\n\t(bump_vector_ptr): Use operand_equal_p rather than tree_int_cst_compare\n\tto compare the updates.\n\t* tree-vect-stmts.c (vect_use_strided_gather_scatters_p): New function.\n\t(get_load_store_type): Use it when handling a strided access.\n\t(vect_get_strided_load_store_ops): New function.\n\t(vect_get_data_ptr_increment): Likewise.\n\t(vectorizable_load): Handle strided gather loads.  Always pass\n\ta step to vect_create_data_ref_ptr and bump_vector_ptr.\n\ngcc/testsuite/\n\t* gcc.target/aarch64/sve/strided_load_1.c: New test.\n\t* gcc.target/aarch64/sve/strided_load_2.c: Likewise.\n\t* gcc.target/aarch64/sve/strided_load_3.c: Likewise.\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r256641", "tree": {"sha": "6f7286ba241ca431dd03a3f8e051838488b4c200", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6f7286ba241ca431dd03a3f8e051838488b4c200"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/ab2fc782509f934ef0cc22c31d743fcb63063c1b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ab2fc782509f934ef0cc22c31d743fcb63063c1b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ab2fc782509f934ef0cc22c31d743fcb63063c1b", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ab2fc782509f934ef0cc22c31d743fcb63063c1b/comments", "author": null, "committer": null, "parents": [{"sha": "bfaa08b7ba1b00bbcc00bb76735c6b3547f5830f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bfaa08b7ba1b00bbcc00bb76735c6b3547f5830f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/bfaa08b7ba1b00bbcc00bb76735c6b3547f5830f"}], "stats": {"total": 308, "additions": 283, "deletions": 25}, "files": [{"sha": "fb7a205be00936a699c37da18bb353c76517a3c8", "filename": "gcc/ChangeLog", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=ab2fc782509f934ef0cc22c31d743fcb63063c1b", "patch": "@@ -1,3 +1,22 @@\n+2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* tree-vectorizer.h (vect_create_data_ref_ptr): Take an extra\n+\toptional tree argument.\n+\t* tree-vect-data-refs.c (vect_check_gather_scatter): Check for\n+\tnull target hooks.\n+\t(vect_create_data_ref_ptr): Take the iv_step as an optional argument,\n+\tbut continue to use the current value as a fallback.\n+\t(bump_vector_ptr): Use operand_equal_p rather than tree_int_cst_compare\n+\tto compare the updates.\n+\t* tree-vect-stmts.c (vect_use_strided_gather_scatters_p): New function.\n+\t(get_load_store_type): Use it when handling a strided access.\n+\t(vect_get_strided_load_store_ops): New function.\n+\t(vect_get_data_ptr_increment): Likewise.\n+\t(vectorizable_load): Handle strided gather loads.  Always pass\n+\ta step to vect_create_data_ref_ptr and bump_vector_ptr.\n+\n 2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n \t    Alan Hayward  <alan.hayward@arm.com>\n \t    David Sherwood  <david.sherwood@arm.com>"}, {"sha": "30bcb7a057c2e12895b18426acbf3545a15cebdf", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=ab2fc782509f934ef0cc22c31d743fcb63063c1b", "patch": "@@ -1,3 +1,11 @@\n+2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* gcc.target/aarch64/sve/strided_load_1.c: New test.\n+\t* gcc.target/aarch64/sve/strided_load_2.c: Likewise.\n+\t* gcc.target/aarch64/sve/strided_load_3.c: Likewise.\n+\n 2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n \t    Alan Hayward  <alan.hayward@arm.com>\n \t    David Sherwood  <david.sherwood@arm.com>"}, {"sha": "cab5021f692718079caa97fa129672d36791cb44", "filename": "gcc/testsuite/gcc.target/aarch64/sve/strided_load_1.c", "status": "added", "additions": 40, "deletions": 0, "changes": 40, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fstrided_load_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fstrided_load_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fstrided_load_1.c?ref=ab2fc782509f934ef0cc22c31d743fcb63063c1b", "patch": "@@ -0,0 +1,40 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O2 -ftree-vectorize --save-temps\" } */\n+\n+#include <stdint.h>\n+\n+#ifndef INDEX8\n+#define INDEX8 int8_t\n+#define INDEX16 int16_t\n+#define INDEX32 int32_t\n+#define INDEX64 int64_t\n+#endif\n+\n+#define TEST_LOOP(DATA_TYPE, BITS)\t\t\t\t\\\n+  void __attribute__ ((noinline, noclone))\t\t\t\\\n+  f_##DATA_TYPE##_##BITS (DATA_TYPE *restrict dest,\t\t\\\n+\t\t\t  DATA_TYPE *restrict src,\t\t\\\n+\t\t\t  INDEX##BITS stride, INDEX##BITS n)\t\\\n+  {\t\t\t\t\t\t\t\t\\\n+    for (INDEX##BITS i = 0; i < n; ++i)\t\t\t\t\\\n+      dest[i] += src[i * stride];\t\t\t\t\\\n+  }\n+\n+#define TEST_TYPE(T, DATA_TYPE)\t\t\t\\\n+  T (DATA_TYPE, 8)\t\t\t\t\\\n+  T (DATA_TYPE, 16)\t\t\t\t\\\n+  T (DATA_TYPE, 32)\t\t\t\t\\\n+  T (DATA_TYPE, 64)\n+\n+#define TEST_ALL(T)\t\t\t\t\\\n+  TEST_TYPE (T, int32_t)\t\t\t\\\n+  TEST_TYPE (T, uint32_t)\t\t\t\\\n+  TEST_TYPE (T, float)\t\t\t\t\\\n+  TEST_TYPE (T, int64_t)\t\t\t\\\n+  TEST_TYPE (T, uint64_t)\t\t\t\\\n+  TEST_TYPE (T, double)\n+\n+TEST_ALL (TEST_LOOP)\n+\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.s, p[0-7]/z, \\[x[0-9]+, z[0-9]+.s, sxtw 2\\]\\n} 9 } } */\n+/* { dg-final { scan-assembler-times {\\tld1d\\tz[0-9]+\\.d, p[0-7]/z, \\[x[0-9]+, z[0-9]+.d, lsl 3\\]\\n} 12 } } */"}, {"sha": "762805ff54ba54c73547dddade97f0cfa32054a4", "filename": "gcc/testsuite/gcc.target/aarch64/sve/strided_load_2.c", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fstrided_load_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fstrided_load_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fstrided_load_2.c?ref=ab2fc782509f934ef0cc22c31d743fcb63063c1b", "patch": "@@ -0,0 +1,18 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O2 -ftree-vectorize --save-temps\" } */\n+\n+#define INDEX8 uint8_t\n+#define INDEX16 uint16_t\n+#define INDEX32 uint32_t\n+#define INDEX64 uint64_t\n+\n+#include \"strided_load_1.c\"\n+\n+/* 8 and 16 bits are signed because the multiplication promotes to int.\n+   Using uxtw for all 9 would be OK.  */\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.s, p[0-7]/z, \\[x[0-9]+, z[0-9]+.s, sxtw 2\\]\\n} 6 } } */\n+/* The 32-bit loop needs to honor the defined overflow in uint32_t,\n+   so we vectorize the offset calculation.  This means that the\n+   64-bit version needs two copies.  */\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.s, p[0-7]/z, \\[x[0-9]+, z[0-9]+.s, uxtw 2\\]\\n} 3 } } */\n+/* { dg-final { scan-assembler-times {\\tld1d\\tz[0-9]+\\.d, p[0-7]/z, \\[x[0-9]+, z[0-9]+.d, lsl 3\\]\\n} 15 } } */"}, {"sha": "8f720dcc1b6a749ae85b729e6c23ae6a278bdbe5", "filename": "gcc/testsuite/gcc.target/aarch64/sve/strided_load_3.c", "status": "added", "additions": 32, "deletions": 0, "changes": 32, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fstrided_load_3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fstrided_load_3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fstrided_load_3.c?ref=ab2fc782509f934ef0cc22c31d743fcb63063c1b", "patch": "@@ -0,0 +1,32 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O2 -ftree-vectorize --save-temps\" } */\n+\n+#include <stdint.h>\n+\n+#define TEST_LOOP(DATA_TYPE, OTHER_TYPE)\t\t\t\t\\\n+  void __attribute__ ((noinline, noclone))\t\t\t\t\\\n+  f_##DATA_TYPE##_##BITS (DATA_TYPE *restrict dest,\t\t\t\\\n+\t\t\t  DATA_TYPE *restrict src,\t\t\t\\\n+\t\t\t  OTHER_TYPE *restrict other,\t\t\t\\\n+\t\t\t  OTHER_TYPE mask,\t\t\t\t\\\n+\t\t\t  int stride, int n)\t\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    for (int i = 0; i < n; ++i)\t\t\t\t\t\t\\\n+      dest[i] = src[i * stride] + (OTHER_TYPE) (other[i] | mask);\t\\\n+  }\n+\n+#define TEST_ALL(T)\t\t\t\t\\\n+  T (int32_t, int16_t)\t\t\t\t\\\n+  T (uint32_t, int16_t)\t\t\t\t\\\n+  T (float, int16_t)\t\t\t\t\\\n+  T (int64_t, int32_t)\t\t\t\t\\\n+  T (uint64_t, int32_t)\t\t\t\t\\\n+  T (double, int32_t)\n+\n+TEST_ALL (TEST_LOOP)\n+\n+/* { dg-final { scan-assembler-times {\\tld1h\\tz[0-9]+\\.h, p[0-7]/z, \\[x[0-9]+, x[0-9]+, lsl 1\\]\\n} 3 } } */\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.s, p[0-7]/z, \\[x[0-9]+, z[0-9]+.s, sxtw 2\\]\\n} 6 } } */\n+\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.s, p[0-7]/z, \\[x[0-9]+, x[0-9]+, lsl 2\\]\\n} 3 } } */\n+/* { dg-final { scan-assembler-times {\\tld1d\\tz[0-9]+\\.d, p[0-7]/z, \\[x[0-9]+, z[0-9]+.d, lsl 3\\]\\n} 6 } } */"}, {"sha": "69721a9a1f967991b50236bdc46b635137b2a96b", "filename": "gcc/tree-vect-data-refs.c", "status": "modified", "additions": 28, "deletions": 13, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftree-vect-data-refs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftree-vect-data-refs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-data-refs.c?ref=ab2fc782509f934ef0cc22c31d743fcb63063c1b", "patch": "@@ -3616,9 +3616,15 @@ vect_check_gather_scatter (gimple *stmt, loop_vec_info loop_vinfo,\n   else\n     {\n       if (DR_IS_READ (dr))\n-\tdecl = targetm.vectorize.builtin_gather (vectype, offtype, scale);\n+\t{\n+\t  if (targetm.vectorize.builtin_gather)\n+\t    decl = targetm.vectorize.builtin_gather (vectype, offtype, scale);\n+\t}\n       else\n-\tdecl = targetm.vectorize.builtin_scatter (vectype, offtype, scale);\n+\t{\n+\t  if (targetm.vectorize.builtin_scatter)\n+\t    decl = targetm.vectorize.builtin_scatter (vectype, offtype, scale);\n+\t}\n \n       if (!decl)\n \treturn false;\n@@ -4367,6 +4373,10 @@ vect_create_addr_base_for_vector_ref (gimple *stmt,\n \tto the initial address accessed by the data-ref in STMT.  This is\n \tsimilar to OFFSET, but OFFSET is counted in elements, while BYTE_OFFSET\n \tin bytes.\n+   8. IV_STEP (optional, defaults to NULL): the amount that should be added\n+\tto the IV during each iteration of the loop.  NULL says to move\n+\tby one copy of AGGR_TYPE up or down, depending on the step of the\n+\tdata reference.\n \n    Output:\n    1. Declare a new ptr to vector_type, and have it point to the base of the\n@@ -4399,7 +4409,8 @@ tree\n vect_create_data_ref_ptr (gimple *stmt, tree aggr_type, struct loop *at_loop,\n \t\t\t  tree offset, tree *initial_address,\n \t\t\t  gimple_stmt_iterator *gsi, gimple **ptr_incr,\n-\t\t\t  bool only_init, bool *inv_p, tree byte_offset)\n+\t\t\t  bool only_init, bool *inv_p, tree byte_offset,\n+\t\t\t  tree iv_step)\n {\n   const char *base_name;\n   stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n@@ -4423,7 +4434,8 @@ vect_create_data_ref_ptr (gimple *stmt, tree aggr_type, struct loop *at_loop,\n   tree step;\n   bb_vec_info bb_vinfo = STMT_VINFO_BB_VINFO (stmt_info);\n \n-  gcc_assert (TREE_CODE (aggr_type) == ARRAY_TYPE\n+  gcc_assert (iv_step != NULL_TREE\n+\t      || TREE_CODE (aggr_type) == ARRAY_TYPE\n \t      || TREE_CODE (aggr_type) == VECTOR_TYPE);\n \n   if (loop_vinfo)\n@@ -4564,14 +4576,17 @@ vect_create_data_ref_ptr (gimple *stmt, tree aggr_type, struct loop *at_loop,\n     aptr = aggr_ptr_init;\n   else\n     {\n-      /* The step of the aggregate pointer is the type size.  */\n-      tree iv_step = TYPE_SIZE_UNIT (aggr_type);\n-      /* One exception to the above is when the scalar step of the load in\n-\t LOOP is zero. In this case the step here is also zero.  */\n-      if (*inv_p)\n-\tiv_step = size_zero_node;\n-      else if (tree_int_cst_sgn (step) == -1)\n-\tiv_step = fold_build1 (NEGATE_EXPR, TREE_TYPE (iv_step), iv_step);\n+      if (iv_step == NULL_TREE)\n+\t{\n+\t  /* The step of the aggregate pointer is the type size.  */\n+\t  iv_step = TYPE_SIZE_UNIT (aggr_type);\n+\t  /* One exception to the above is when the scalar step of the load in\n+\t     LOOP is zero. In this case the step here is also zero.  */\n+\t  if (*inv_p)\n+\t    iv_step = size_zero_node;\n+\t  else if (tree_int_cst_sgn (step) == -1)\n+\t    iv_step = fold_build1 (NEGATE_EXPR, TREE_TYPE (iv_step), iv_step);\n+\t}\n \n       standard_iv_increment_position (loop, &incr_gsi, &insert_after);\n \n@@ -4704,7 +4719,7 @@ bump_vector_ptr (tree dataref_ptr, gimple *ptr_incr, gimple_stmt_iterator *gsi,\n       if (use == dataref_ptr)\n         SET_USE (use_p, new_dataref_ptr);\n       else\n-        gcc_assert (tree_int_cst_compare (use, update) == 0);\n+        gcc_assert (operand_equal_p (use, update, 0));\n     }\n \n   return new_dataref_ptr;"}, {"sha": "079cbddfd5ebb77876b2e1c7e0a8bb5d363ac00b", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 137, "deletions": 11, "changes": 148, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=ab2fc782509f934ef0cc22c31d743fcb63063c1b", "patch": "@@ -1849,6 +1849,44 @@ prepare_load_store_mask (tree mask_type, tree loop_mask, tree vec_mask,\n   return and_res;\n }\n \n+/* Return true if we can use gather/scatter internal functions to\n+   vectorize STMT, which is a grouped or strided load or store.\n+   When returning true, fill in GS_INFO with the information required\n+   to perform the operation.  */\n+\n+static bool\n+vect_use_strided_gather_scatters_p (gimple *stmt, loop_vec_info loop_vinfo,\n+\t\t\t\t    gather_scatter_info *gs_info)\n+{\n+  if (!vect_check_gather_scatter (stmt, loop_vinfo, gs_info)\n+      || gs_info->decl)\n+    return false;\n+\n+  scalar_mode element_mode = SCALAR_TYPE_MODE (gs_info->element_type);\n+  unsigned int element_bits = GET_MODE_BITSIZE (element_mode);\n+  tree offset_type = TREE_TYPE (gs_info->offset);\n+  unsigned int offset_bits = TYPE_PRECISION (offset_type);\n+\n+  /* Enforced by vect_check_gather_scatter.  */\n+  gcc_assert (element_bits >= offset_bits);\n+\n+  /* If the elements are wider than the offset, convert the offset to the\n+     same width, without changing its sign.  */\n+  if (element_bits > offset_bits)\n+    {\n+      bool unsigned_p = TYPE_UNSIGNED (offset_type);\n+      offset_type = build_nonstandard_integer_type (element_bits, unsigned_p);\n+      gs_info->offset = fold_convert (offset_type, gs_info->offset);\n+    }\n+\n+  if (dump_enabled_p ())\n+    dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t     \"using gather/scatter for strided/grouped access,\"\n+\t\t     \" scale = %d\\n\", gs_info->scale);\n+\n+  return true;\n+}\n+\n /* STMT is a non-strided load or store, meaning that it accesses\n    elements with a known constant step.  Return -1 if that step\n    is negative, 0 if it is zero, and 1 if it is greater than zero.  */\n@@ -2168,7 +2206,11 @@ get_load_store_type (gimple *stmt, tree vectype, bool slp, bool masked_p,\n   else if (STMT_VINFO_STRIDED_P (stmt_info))\n     {\n       gcc_assert (!slp);\n-      *memory_access_type = VMAT_ELEMENTWISE;\n+      if (loop_vinfo\n+\t  && vect_use_strided_gather_scatters_p (stmt, loop_vinfo, gs_info))\n+\t*memory_access_type = VMAT_GATHER_SCATTER;\n+      else\n+\t*memory_access_type = VMAT_ELEMENTWISE;\n     }\n   else\n     {\n@@ -2612,6 +2654,71 @@ vect_get_gather_scatter_ops (struct loop *loop, gimple *stmt,\n \t\t\t\t\t      offset_vectype);\n }\n \n+/* Prepare to implement a grouped or strided load or store using\n+   the gather load or scatter store operation described by GS_INFO.\n+   STMT is the load or store statement.\n+\n+   Set *DATAREF_BUMP to the amount that should be added to the base\n+   address after each copy of the vectorized statement.  Set *VEC_OFFSET\n+   to an invariant offset vector in which element I has the value\n+   I * DR_STEP / SCALE.  */\n+\n+static void\n+vect_get_strided_load_store_ops (gimple *stmt, loop_vec_info loop_vinfo,\n+\t\t\t\t gather_scatter_info *gs_info,\n+\t\t\t\t tree *dataref_bump, tree *vec_offset)\n+{\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+  gimple_seq stmts;\n+\n+  tree bump = size_binop (MULT_EXPR,\n+\t\t\t  fold_convert (sizetype, DR_STEP (dr)),\n+\t\t\t  size_int (TYPE_VECTOR_SUBPARTS (vectype)));\n+  *dataref_bump = force_gimple_operand (bump, &stmts, true, NULL_TREE);\n+  if (stmts)\n+    gsi_insert_seq_on_edge_immediate (loop_preheader_edge (loop), stmts);\n+\n+  /* The offset given in GS_INFO can have pointer type, so use the element\n+     type of the vector instead.  */\n+  tree offset_type = TREE_TYPE (gs_info->offset);\n+  tree offset_vectype = get_vectype_for_scalar_type (offset_type);\n+  offset_type = TREE_TYPE (offset_vectype);\n+\n+  /* Calculate X = DR_STEP / SCALE and convert it to the appropriate type.  */\n+  tree step = size_binop (EXACT_DIV_EXPR, DR_STEP (dr),\n+\t\t\t  ssize_int (gs_info->scale));\n+  step = fold_convert (offset_type, step);\n+  step = force_gimple_operand (step, &stmts, true, NULL_TREE);\n+\n+  /* Create {0, X, X*2, X*3, ...}.  */\n+  *vec_offset = gimple_build (&stmts, VEC_SERIES_EXPR, offset_vectype,\n+\t\t\t      build_zero_cst (offset_type), step);\n+  if (stmts)\n+    gsi_insert_seq_on_edge_immediate (loop_preheader_edge (loop), stmts);\n+}\n+\n+/* Return the amount that should be added to a vector pointer to move\n+   to the next or previous copy of AGGR_TYPE.  DR is the data reference\n+   being vectorized and MEMORY_ACCESS_TYPE describes the type of\n+   vectorization.  */\n+\n+static tree\n+vect_get_data_ptr_increment (data_reference *dr, tree aggr_type,\n+\t\t\t     vect_memory_access_type memory_access_type)\n+{\n+  if (memory_access_type == VMAT_INVARIANT)\n+    return size_zero_node;\n+\n+  tree iv_step = TYPE_SIZE_UNIT (aggr_type);\n+  tree step = vect_dr_behavior (dr)->step;\n+  if (tree_int_cst_sgn (step) == -1)\n+    iv_step = fold_build1 (NEGATE_EXPR, TREE_TYPE (iv_step), iv_step);\n+  return iv_step;\n+}\n+\n /* Check and perform vectorization of BUILT_IN_BSWAP{16,32,64}.  */\n \n static bool\n@@ -7412,6 +7519,9 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n       return true;\n     }\n \n+  if (memory_access_type == VMAT_GATHER_SCATTER)\n+    grouped_load = false;\n+\n   if (grouped_load)\n     {\n       first_stmt = GROUP_FIRST_ELEMENT (stmt_info);\n@@ -7623,13 +7733,29 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   if (memory_access_type == VMAT_CONTIGUOUS_REVERSE)\n     offset = size_int (-TYPE_VECTOR_SUBPARTS (vectype) + 1);\n \n-  if (memory_access_type == VMAT_LOAD_STORE_LANES)\n-    aggr_type = build_array_type_nelts (elem_type, vec_num * nunits);\n+  tree bump;\n+  tree vec_offset = NULL_TREE;\n+  if (STMT_VINFO_GATHER_SCATTER_P (stmt_info))\n+    {\n+      aggr_type = NULL_TREE;\n+      bump = NULL_TREE;\n+    }\n+  else if (memory_access_type == VMAT_GATHER_SCATTER)\n+    {\n+      aggr_type = elem_type;\n+      vect_get_strided_load_store_ops (stmt, loop_vinfo, &gs_info,\n+\t\t\t\t       &bump, &vec_offset);\n+    }\n   else\n-    aggr_type = vectype;\n+    {\n+      if (memory_access_type == VMAT_LOAD_STORE_LANES)\n+\taggr_type = build_array_type_nelts (elem_type, vec_num * nunits);\n+      else\n+\taggr_type = vectype;\n+      bump = vect_get_data_ptr_increment (dr, aggr_type, memory_access_type);\n+    }\n \n   tree vec_mask = NULL_TREE;\n-  tree vec_offset = NULL_TREE;\n   prev_stmt_info = NULL;\n   poly_uint64 group_elt = 0;\n   vec_loop_masks *masks = &LOOP_VINFO_MASKS (loop_vinfo);\n@@ -7661,7 +7787,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t= vect_create_data_ref_ptr (first_stmt_for_drptr, aggr_type,\n \t\t\t\t\t    at_loop, offset, &dummy, gsi,\n \t\t\t\t\t    &ptr_incr, simd_lane_access_p,\n-\t\t\t\t\t    &inv_p, byte_offset);\n+\t\t\t\t\t    &inv_p, byte_offset, bump);\n \t      /* Adjust the pointer by the difference to first_stmt.  */\n \t      data_reference_p ptrdr\n \t\t= STMT_VINFO_DATA_REF (vinfo_for_stmt (first_stmt_for_drptr));\n@@ -7683,7 +7809,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t      = vect_create_data_ref_ptr (first_stmt, aggr_type, at_loop,\n \t\t\t\t\t  offset, &dummy, gsi, &ptr_incr,\n \t\t\t\t\t  simd_lane_access_p, &inv_p,\n-\t\t\t\t\t  byte_offset);\n+\t\t\t\t\t  byte_offset, bump);\n \t  if (mask)\n \t    vec_mask = vect_get_vec_def_for_operand (mask, stmt,\n \t\t\t\t\t\t     mask_vectype);\n@@ -7692,7 +7818,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t{\n \t  if (dataref_offset)\n \t    dataref_offset = int_const_binop (PLUS_EXPR, dataref_offset,\n-\t\t\t\t\t      TYPE_SIZE_UNIT (aggr_type));\n+\t\t\t\t\t      bump);\n \t  else if (STMT_VINFO_GATHER_SCATTER_P (stmt_info))\n \t    {\n \t      gimple *def_stmt;\n@@ -7701,8 +7827,8 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t      vec_offset = vect_get_vec_def_for_stmt_copy (dt, vec_offset);\n \t    }\n \t  else\n-\t    dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi, stmt,\n-\t\t\t\t\t   TYPE_SIZE_UNIT (aggr_type));\n+\t    dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi,\n+\t\t\t\t\t   stmt, bump);\n \t  if (mask)\n \t    {\n \t      gimple *def_stmt;\n@@ -7778,7 +7904,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \n \t      if (i > 0)\n \t\tdataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi,\n-\t\t\t\t\t       stmt, NULL_TREE);\n+\t\t\t\t\t       stmt, bump);\n \n \t      /* 2. Create the vector-load in the loop.  */\n \t      switch (alignment_support_scheme)"}, {"sha": "c661578403a6fab60d2dfe0cb93242b4043c3196", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ab2fc782509f934ef0cc22c31d743fcb63063c1b/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=ab2fc782509f934ef0cc22c31d743fcb63063c1b", "patch": "@@ -1462,7 +1462,7 @@ extern void vect_record_base_alignments (vec_info *);\n extern tree vect_create_data_ref_ptr (gimple *, tree, struct loop *, tree,\n \t\t\t\t      tree *, gimple_stmt_iterator *,\n \t\t\t\t      gimple **, bool, bool *,\n-\t\t\t\t      tree = NULL_TREE);\n+\t\t\t\t      tree = NULL_TREE, tree = NULL_TREE);\n extern tree bump_vector_ptr (tree, gimple *, gimple_stmt_iterator *, gimple *,\n \t\t\t     tree);\n extern tree vect_create_destination_var (tree, tree);"}]}