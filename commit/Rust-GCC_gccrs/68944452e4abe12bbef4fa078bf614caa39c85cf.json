{"sha": "68944452e4abe12bbef4fa078bf614caa39c85cf", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6Njg5NDQ0NTJlNGFiZTEyYmJlZjRmYTA3OGJmNjE0Y2FhMzljODVjZg==", "commit": {"author": {"name": "Jeff Law", "email": "law@gcc.gnu.org", "date": "1995-11-27T07:33:58Z"}, "committer": {"name": "Jeff Law", "email": "law@gcc.gnu.org", "date": "1995-11-27T07:33:58Z"}, "message": "pa.md (abssi2): New pattern.\n\n\t* pa.md (abssi2): New pattern.\n\n\t* pa.c (secondary_reload_class): Loads from reg+d addresses into\n\tFP registers don't need secondary reloads.\n\t* pa.h: Delete soem #if 0 code.  Update some comments.\n\t(EXTRA_CONSTRAINT, case 'Q'): Only accept valid memory addresses.\n\n        * pa.h (RTX_COSTS): Tege's rewrite.\n\n\t* pa.c (hppa_legitimize_address): Generate unscaled indexed\n\taddressing for (plus (symbol_ref) (reg)).\n\t(emit_move_sequence): Set REGNO_POINTER_FLAG appropriately\n\tto encourage unscaled indexing modes.\n\t(basereg_operand): New function for unscaled index address support.\n\t* pa.md (unscaled indexing patterns): New patterns for unscaled\n\tindex address support.\n\n\t* pa.h (MOVE_RATIO): Define.\n\t* pa.md (movstrsi expander): Refine tests for when to use the\n\tlibrary routine instead of an inlined loop copy.  Provide an\n\tadditional scratch register for use in the inlined loop copy.\n\t(movstrsi_internal): Name the pattern for ease of use.  Add\n\tadditional scratch register.\n\t* pa.c (output_block_move): Greatly simplify.  Use 2X unrolled\n\tcopy loops to improve performance.\n\t(compute_movstrsi_length): Corresponding changes.\n\n\t* pa.c (print_operand): Handle 'y' case for reversed FP\n\tcomparisons.  Delete some #if 0 code.  Fix various comment typos.\n\t* pa.md (fcmp patterns): Try and reverse the comparison to avoid\n\tuseless add,tr insns.\n\nFrom-SVN: r10609", "tree": {"sha": "30d68e3fad5bd6943e4835f20b501338f3d62e0f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/30d68e3fad5bd6943e4835f20b501338f3d62e0f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/68944452e4abe12bbef4fa078bf614caa39c85cf", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/68944452e4abe12bbef4fa078bf614caa39c85cf", "html_url": "https://github.com/Rust-GCC/gccrs/commit/68944452e4abe12bbef4fa078bf614caa39c85cf", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/68944452e4abe12bbef4fa078bf614caa39c85cf/comments", "author": null, "committer": null, "parents": [{"sha": "926d1ca5a340838f7a5c053ae3645796f6a2b211", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/926d1ca5a340838f7a5c053ae3645796f6a2b211", "html_url": "https://github.com/Rust-GCC/gccrs/commit/926d1ca5a340838f7a5c053ae3645796f6a2b211"}], "stats": {"total": 851, "additions": 516, "deletions": 335}, "files": [{"sha": "3cc18eca7a57396c66fa24c609c44e8d2167164e", "filename": "gcc/config/pa/pa.c", "status": "modified", "additions": 169, "deletions": 266, "changes": 435, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/68944452e4abe12bbef4fa078bf614caa39c85cf/gcc%2Fconfig%2Fpa%2Fpa.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/68944452e4abe12bbef4fa078bf614caa39c85cf/gcc%2Fconfig%2Fpa%2Fpa.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fpa%2Fpa.c?ref=68944452e4abe12bbef4fa078bf614caa39c85cf", "patch": "@@ -637,6 +637,16 @@ hppa_legitimize_address (x, oldx, mode)\n   if (GET_CODE (x) == CONST)\n     x = XEXP (x, 0);\n \n+  /* Special case.  Get the SYMBOL_REF into a register and use indexing.\n+     That should always be safe.  */\n+  if (GET_CODE (x) == PLUS\n+      && GET_CODE (XEXP (x, 0)) == REG\n+      && GET_CODE (XEXP (x, 1)) == SYMBOL_REF)\n+    {\n+      rtx reg = force_reg (SImode, XEXP (x, 1));\n+      return force_reg (SImode, gen_rtx (PLUS, SImode, reg, XEXP (x, 0)));\n+    }\n+\n   /* Note we must reject symbols which represent function addresses\n      since the assembler/linker can't handle arithmetic on plabels.  */\n   if (GET_CODE (x) == PLUS\n@@ -793,7 +803,7 @@ emit_move_sequence (operands, mode, scratch_reg)\n \n   /* Handle secondary reloads for loads/stores of FP registers from\n      REG+D addresses where D does not fit in 5 bits, including \n-     (subreg (mem (addr)) cases.  */\n+     (subreg (mem (addr))) cases.  */\n   if (fp_reg_operand (operand0, mode)\n       && ((GET_CODE (operand1) == MEM\n \t   && ! memory_address_p (DFmode, XEXP (operand1, 0)))\n@@ -975,9 +985,9 @@ emit_move_sequence (operands, mode, scratch_reg)\n \t\t  operands[1] = force_const_mem (mode, operand1);\n \t\t  emit_move_sequence (operands, mode, temp);\n \t\t}\n-\t      /* Likewise for (const (plus (symbol) (const_int)) when generating\n-\t\t pic code during or after reload and const_int will not fit\n-\t\t in 14 bits.  */\n+\t      /* Likewise for (const (plus (symbol) (const_int))) when\n+\t\t generating pic code during or after reload and const_int\n+\t\t will not fit in 14 bits.  */\n \t      else if (GET_CODE (operand1) == CONST\n \t\t       && GET_CODE (XEXP (operand1, 0)) == PLUS\n \t\t       && GET_CODE (XEXP (XEXP (operand1, 0), 1)) == CONST_INT\n@@ -1008,6 +1018,14 @@ emit_move_sequence (operands, mode, scratch_reg)\n \t      else\n \t\ttemp = gen_reg_rtx (mode);\n \n+\t      /* Loading a SYMBOL_REF into a register makes that register\n+\t\t safe to be used as the base in an indexed address. \n+\n+\t\t Don't mark hard registers though.  That loses.  */\n+\t      if (REGNO (operand0) >= FIRST_PSEUDO_REGISTER)\n+\t\tREGNO_POINTER_FLAG (REGNO (operand0)) = 1;\n+\t      if (REGNO (temp) >= FIRST_PSEUDO_REGISTER)\n+\t\tREGNO_POINTER_FLAG (REGNO (temp)) = 1;\n \t      if (ishighonly)\n \t\tset = gen_rtx (SET, mode, operand0, temp);\n \t      else\n@@ -1457,172 +1475,108 @@ find_addr_reg (addr)\n \n /* Emit code to perform a block move.\n \n-   Restriction: If the length argument is non-constant, alignment\n-   must be 4.\n-\n    OPERANDS[0] is the destination pointer as a REG, clobbered.\n    OPERANDS[1] is the source pointer as a REG, clobbered.\n-   if SIZE_IS_CONSTANT\n-     OPERANDS[2] is a register for temporary storage.\n-     OPERANDS[4] is the size as a CONST_INT\n-   else\n-     OPERANDS[2] is a REG which will contain the size, clobbered.\n+   OPERANDS[2] is a register for temporary storage.\n+   OPERANDS[4] is the size as a CONST_INT\n    OPERANDS[3] is a register for temporary storage.\n-   OPERANDS[5] is the alignment safe to use, as a CONST_INT.  */\n+   OPERANDS[5] is the alignment safe to use, as a CONST_INT. \n+   OPERNADS[6] is another temporary register.   */\n \n char *\n output_block_move (operands, size_is_constant)\n      rtx *operands;\n      int size_is_constant;\n {\n   int align = INTVAL (operands[5]);\n-  unsigned long n_bytes;\n+  unsigned long n_bytes = INTVAL (operands[4]);\n \n   /* We can't move more than four bytes at a time because the PA\n      has no longer integer move insns.  (Could use fp mem ops?)  */\n   if (align > 4)\n     align = 4;\n \n-  if (size_is_constant)\n+  /* Note that we know each loop below will execute at least twice\n+     (else we would have open-coded the copy).  */\n+  switch (align)\n     {\n-      unsigned long offset;\n-      rtx temp;\n-\n-      n_bytes = INTVAL (operands[4]);\n-      if (n_bytes == 0)\n-\treturn \"\";\n-\n-      if (align >= 4)\n-\t{\n-\t  /* Don't unroll too large blocks.  */\n-\t  if (n_bytes > 32)\n-\t    goto copy_with_loop;\n-\n-\t  /* Read and store using two registers, and hide latency\n-\t     by deferring the stores until three instructions after\n-\t     the corresponding load.  The last load insn will read\n-\t     the entire word were the last bytes are, possibly past\n-\t     the end of the source block, but since loads are aligned,\n-\t     this is harmless.  */\n-\n-\t  output_asm_insn (\"ldws,ma 4(0,%1),%2\", operands);\n-\n-\t  for (offset = 4; offset < n_bytes; offset += 4)\n-\t    {\n+      case 4:\n+\t/* Pre-adjust the loop counter.  */\n+\toperands[4] = GEN_INT (n_bytes - 8);\n+\toutput_asm_insn (\"ldi %4,%2\", operands);\n+\n+\t/* Copying loop.  */\n+\toutput_asm_insn (\"ldws,ma 4(0,%1),%3\", operands);\n+\toutput_asm_insn (\"ldws,ma 4(0,%1),%6\", operands);\n+\toutput_asm_insn (\"stws,ma %3,4(0,%0)\", operands);\n+\toutput_asm_insn (\"addib,>= -8,%2,.-12\", operands);\n+\toutput_asm_insn (\"stws,ma %6,4(0,%0)\", operands);\n+\n+\t/* Handle the residual.  There could be up to 7 bytes of\n+\t   residual to copy!  */\n+\tif (n_bytes % 8 != 0)\n+\t  {\n+\t    operands[4] = GEN_INT (n_bytes % 4);\n+\t    if (n_bytes % 8 >= 4)\n \t      output_asm_insn (\"ldws,ma 4(0,%1),%3\", operands);\n-\t      output_asm_insn (\"stws,ma %2,4(0,%0)\", operands);\n+\t    if (n_bytes % 4 != 0)\n+\t      output_asm_insn (\"ldw 0(0,%1),%6\", operands);\n+\t    if (n_bytes % 8 >= 4)\n+\t      output_asm_insn (\"stws,ma %3,4(0,%0)\", operands);\n+\t    if (n_bytes % 4 != 0)\n+\t      output_asm_insn (\"stbys,e %6,%4(0,%0)\", operands);\n+\t  }\n+\treturn \"\";\n \n-\t      temp = operands[2];\n-\t      operands[2] = operands[3];\n-\t      operands[3] = temp;\n-\t    }\n-\t  if (n_bytes % 4 == 0)\n-\t    /* Store the last word.  */\n-\t    output_asm_insn (\"stw %2,0(0,%0)\", operands);\n-\t  else\n-\t    {\n-\t      /* Store the last, partial word.  */\n-\t      operands[4] = GEN_INT (n_bytes % 4);\n-\t      output_asm_insn (\"stbys,e %2,%4(0,%0)\", operands);\n-\t    }\n-\t  return \"\";\n-\t}\n+      case 2:\n+\t/* Pre-adjust the loop counter.  */\n+\toperands[4] = GEN_INT (n_bytes - 4);\n+\toutput_asm_insn (\"ldi %4,%2\", operands);\n \n-      if (align >= 2 && n_bytes >= 2)\n-\t{\n-\t  output_asm_insn (\"ldhs,ma 2(0,%1),%2\", operands);\n+\t/* Copying loop.  */\n+\toutput_asm_insn (\"ldhs,ma 2(0,%1),%3\", operands);\n+\toutput_asm_insn (\"ldhs,ma 2(0,%1),%6\", operands);\n+\toutput_asm_insn (\"sths,ma %3,2(0,%0)\", operands);\n+\toutput_asm_insn (\"addib,>= -4,%2,.-12\", operands);\n+\toutput_asm_insn (\"sths,ma %6,2(0,%0)\", operands);\n \n-\t  for (offset = 2; offset + 2 <= n_bytes; offset += 2)\n-\t    {\n+\t/* Handle the residual.  */\n+\tif (n_bytes % 4 != 0)\n+\t  {\n+\t    if (n_bytes % 4 >= 2)\n \t      output_asm_insn (\"ldhs,ma 2(0,%1),%3\", operands);\n-\t      output_asm_insn (\"sths,ma %2,2(0,%0)\", operands);\n+\t    if (n_bytes % 2 != 0)\n+\t      output_asm_insn (\"ldb 0(0,%1),%6\", operands);\n+\t    if (n_bytes % 4 >= 2)\n+\t      output_asm_insn (\"sths,ma %3,2(0,%0)\", operands);\n+\t    if (n_bytes % 2 != 0)\n+\t      output_asm_insn (\"stb %6,0(0,%0)\", operands);\n+\t  }\n+\treturn \"\";\n \n-\t      temp = operands[2];\n-\t      operands[2] = operands[3];\n-\t      operands[3] = temp;\n-\t    }\n-\t  if (n_bytes % 2 != 0)\n-\t    output_asm_insn (\"ldb 0(0,%1),%3\", operands);\n+      case 1:\n+\t/* Pre-adjust the loop counter.  */\n+\toperands[4] = GEN_INT (n_bytes - 2);\n+\toutput_asm_insn (\"ldi %4,%2\", operands);\n \n-\t  output_asm_insn (\"sths,ma %2,2(0,%0)\", operands);\n+\t/* Copying loop.  */\n+\toutput_asm_insn (\"ldbs,ma 1(0,%1),%3\", operands);\n+\toutput_asm_insn (\"ldbs,ma 1(0,%1),%6\", operands);\n+\toutput_asm_insn (\"stbs,ma %3,1(0,%0)\", operands);\n+\toutput_asm_insn (\"addib,>= -2,%2,.-12\", operands);\n+\toutput_asm_insn (\"stbs,ma %6,1(0,%0)\", operands);\n \n-\t  if (n_bytes % 2 != 0)\n+\t/* Handle the residual.  */\n+\tif (n_bytes % 2 != 0)\n+\t  {\n+\t    output_asm_insn (\"ldb 0(0,%1),%3\", operands);\n \t    output_asm_insn (\"stb %3,0(0,%0)\", operands);\n+\t  }\n+\treturn \"\";\n \n-\t  return \"\";\n-\t}\n-\n-      output_asm_insn (\"ldbs,ma 1(0,%1),%2\", operands);\n-\n-      for (offset = 1; offset + 1 <= n_bytes; offset += 1)\n-\t{\n-\t  output_asm_insn (\"ldbs,ma 1(0,%1),%3\", operands);\n-\t  output_asm_insn (\"stbs,ma %2,1(0,%0)\", operands);\n-\n-\t  temp = operands[2];\n-\t  operands[2] = operands[3];\n-\t  operands[3] = temp;\n-\t}\n-      output_asm_insn (\"stb %2,0(0,%0)\", operands);\n-\n-      return \"\";\n-    }\n-\n-  if (align != 4)\n-    abort();\n-\n- copy_with_loop:\n-\n-  if (size_is_constant)\n-    {\n-      /* Size is compile-time determined, and also not\n-\t very small (such small cases are handled above).  */\n-      operands[4] = GEN_INT (n_bytes - 4);\n-      output_asm_insn (\"ldo %4(0),%2\", operands);\n-    }\n-  else\n-    {\n-      /* Decrement counter by 4, and if it becomes negative, jump past the\n-\t word copying loop.  */\n-      output_asm_insn (\"addib,<,n -4,%2,.+16\", operands);\n-    }\n-\n-  /* Copying loop.  Note that the first load is in the annulled delay slot\n-     of addib.  Is it OK on PA to have a load in a delay slot, i.e. is a\n-     possible page fault stopped in time?  */\n-  output_asm_insn (\"ldws,ma 4(0,%1),%3\", operands);\n-  output_asm_insn (\"addib,>= -4,%2,.-4\", operands);\n-  output_asm_insn (\"stws,ma %3,4(0,%0)\", operands);\n-\n-  /* The counter is negative, >= -4.  The remaining number of bytes are\n-     determined by the two least significant bits.  */\n-\n-  if (size_is_constant)\n-    {\n-      if (n_bytes % 4 != 0)\n-\t{\n-\t  /* Read the entire word of the source block tail.  */\n-\t  output_asm_insn (\"ldw 0(0,%1),%3\", operands);\n-\t  operands[4] = GEN_INT (n_bytes % 4);\n-\t  output_asm_insn (\"stbys,e %3,%4(0,%0)\", operands);\n-\t}\n-    }\n-  else\n-    {\n-      /* Add 4 to counter.  If it becomes zero, we're done.  */\n-      output_asm_insn (\"addib,=,n 4,%2,.+16\", operands);\n-\n-      /* Read the entire word of the source block tail.  (Also this\n-\t load is in an annulled delay slot.)  */\n-      output_asm_insn (\"ldw 0(0,%1),%3\", operands);\n-\n-      /* Make %0 point at the first byte after the destination block.  */\n-      output_asm_insn (\"addl %2,%0,%0\", operands);\n-      /* Store the leftmost bytes, up to, but not including, the address\n-\t in %0.  */\n-      output_asm_insn (\"stbys,e %3,0(0,%0)\", operands);\n+      default:\n+\tabort ();\n     }\n-  return \"\";\n }\n \n /* Count the number of insns necessary to handle this block move.\n@@ -1635,106 +1589,33 @@ compute_movstrsi_length (insn)\n      rtx insn;\n {\n   rtx pat = PATTERN (insn);\n-  int size_is_constant;\n   int align = INTVAL (XEXP (XVECEXP (pat, 0, 6), 0));\n-  unsigned long n_bytes;\n-  int insn_count = 0;\n-\n-  if (GET_CODE (XEXP (XVECEXP (pat, 0, 5), 0)) == CONST_INT)\n-    {\n-      size_is_constant = 1;\n-      n_bytes = INTVAL (XEXP (XVECEXP (pat, 0, 5), 0));\n-    }\n-  else\n-    {\n-      size_is_constant = 0;\n-      n_bytes = 0;\n-    }\n+  unsigned long n_bytes = INTVAL (XEXP (XVECEXP (pat, 0, 5), 0));\n+  unsigned int n_insns = 0;\n \n   /* We can't move more than four bytes at a time because the PA\n      has no longer integer move insns.  (Could use fp mem ops?)  */\n   if (align > 4)\n     align = 4;\n \n-  if (size_is_constant)\n-    {\n-      unsigned long offset;\n-\n-      if (n_bytes == 0)\n-\treturn 0;\n-\n-      if (align >= 4)\n-\t{\n-\t  /* Don't unroll too large blocks.  */\n-\t  if (n_bytes > 32)\n-\t    goto copy_with_loop;\n-\n-\t  /* first load */\n-\t  insn_count = 1;\n-\n-\t  /* Count the unrolled insns.  */\n-\t  for (offset = 4; offset < n_bytes; offset += 4)\n-\t    insn_count += 2;\n-\n-\t  /* Count last store or partial store.  */\n-\t  insn_count += 1;\n-\t  return insn_count * 4;\n-\t}\n-\n-      if (align >= 2 && n_bytes >= 2)\n-\t{\n-\t  /* initial load.  */\n-\t  insn_count = 1;\n-\n-\t  /* Unrolled loop.  */\n-\t  for (offset = 2; offset + 2 <= n_bytes; offset += 2)\n-\t    insn_count += 2;\n-\n-\t  /* ??? odd load/store */\n-\t  if (n_bytes % 2 != 0)\n-\t    insn_count += 2;\n-\n-\t  /* ??? final store from loop.  */\n-\t  insn_count += 1;\n+  /* The basic opying loop.  */\n+  n_insns = 6;\n \n-\t  return insn_count * 4;\n-\t}\n-\n-      /* First load.  */\n-      insn_count = 1;\n-\n-      /* The unrolled loop.  */\n-      for (offset = 1; offset + 1 <= n_bytes; offset += 1)\n-\tinsn_count += 2;\n-\n-      /* Final store.  */\n-      insn_count += 1;\n-\n-      return insn_count * 4;\n-    }\n-\n-  if (align != 4)\n-    abort();\n-\n- copy_with_loop:\n-\n-  /* setup for constant and non-constant case.  */\n-  insn_count = 1;\n-\n-  /* The copying loop.  */\n-  insn_count += 3;\n-\n-  /* The counter is negative, >= -4.  The remaining number of bytes are\n-     determined by the two least significant bits.  */\n-\n-  if (size_is_constant)\n+  /* Residuals.  */\n+  if (n_bytes % (2 * align) != 0)\n     {\n-      if (n_bytes % 4 != 0)\n-\tinsn_count += 2;\n+      /* Any residual caused by unrolling the copy loop.  */\n+      if (n_bytes % (2 * align) > align)\n+\tn_insns += 1;\n+\n+      /* Any residual because the number of bytes was not a\n+\t multiple of the alignment.  */\n+      if (n_bytes % align != 0)\n+\tn_insns += 1;\n     }\n-  else\n-    insn_count += 4;\n-  return insn_count * 4;\n+\n+  /* Lengths are expressed in bytes now; each insn is 4 bytes.  */\n+  return n_insns * 4;\n }\n \f\n \n@@ -2363,7 +2244,7 @@ hppa_expand_prologue()\n      even be more efficient. \n \n      Avoid this if the callee saved register wasn't used (these are\n-     leaf functions.  */\n+     leaf functions).  */\n   if (flag_pic && regs_ever_live[PIC_OFFSET_TABLE_REGNUM_SAVED])\n     emit_move_insn (gen_rtx (REG, SImode, PIC_OFFSET_TABLE_REGNUM_SAVED),\n \t\t    gen_rtx (REG, SImode, PIC_OFFSET_TABLE_REGNUM));\n@@ -2511,9 +2392,8 @@ hppa_expand_epilogue ()\n       load_reg (2, - 20, STACK_POINTER_REGNUM);\n     }\n \n-  /* Reset stack pointer (and possibly frame pointer).  The stack */\n-  /* pointer is initially set to fp + 64 to avoid a race condition.\n-     ??? What race condition?!?  */\n+  /* Reset stack pointer (and possibly frame pointer).  The stack \n+     pointer is initially set to fp + 64 to avoid a race condition.  */\n   else if (frame_pointer_needed)\n     {\n       /* Emit a blockage insn here to keep these insns from being moved\n@@ -3004,6 +2884,27 @@ print_operand (file, x, code)\n \t  abort ();\n \t}\n       return;\n+    /* Reversed floating point comparison.  Need special conditions to\n+       deal with NaNs properly.  */\n+    case 'y':\n+      switch (GET_CODE (x))\n+\t{\n+\tcase EQ:\n+\t  fprintf (file, \"?=\");  break;\n+\tcase NE:\n+\t  fprintf (file, \"!?=\");  break;\n+\tcase GT:\n+\t  fprintf (file, \"!<=\");  break;\n+\tcase GE:\n+\t  fprintf (file, \"!<\");  break;\n+\tcase LT:\n+\t  fprintf (file, \"!>=\");  break;\n+\tcase LE:\n+\t  fprintf (file, \"!>\");  break;\n+\tdefault:\n+\t  abort ();\n+\t}\n+      return;\n     case 'S':\t\t\t/* Condition, operands are (S)wapped.  */\n       switch (GET_CODE (x))\n \t{\n@@ -3161,30 +3062,6 @@ print_operand (file, x, code)\n \t  break;\n \t}\n     }\n-#if 0\n-  /* The code here is completely wrong.  It attempts to extract parts of\n-     a CONST_DOUBLE which is wrong since REAL_ARITHMETIC is defined, and it\n-     extracts the wrong indices (0 instead of 2 and 1 instead of 3) using\n-     the wrong macro (XINT instead of XWINT).\n-     Just disable it for now, since the code will never be used anyway!  */\n-  else if (GET_CODE (x) == CONST_DOUBLE && GET_MODE (x) == SFmode)\n-    {\n-      union { double d; int i[2]; } u;\n-      union { float f; int i; } u1;\n-      u.i[0] = XINT (x, 0); u.i[1] = XINT (x, 1);\n-      u1.f = u.d;\n-      if (code == 'f')\n-\tfprintf (file, \"0r%.9g\", u1.f);\n-      else\n-\tfprintf (file, \"0x%x\", u1.i);\n-    }\n-  else if (GET_CODE (x) == CONST_DOUBLE && GET_MODE (x) != VOIDmode)\n-    {\n-      union { double d; int i[2]; } u;\n-      u.i[0] = XINT (x, 0); u.i[1] = XINT (x, 1);\n-      fprintf (file, \"0r%.20g\", u.d);\n-    }\n-#endif\n   else\n     output_addr_const (file, x);\n }\n@@ -3527,12 +3404,6 @@ secondary_reload_class (class, mode, in)\n   if (GET_CODE (in) == SUBREG)\n     in = SUBREG_REG (in);\n \n-  if (FP_REG_CLASS_P (class)\n-      && GET_CODE (in) == MEM\n-      && !memory_address_p (DFmode, XEXP (in, 0))\n-      && memory_address_p (SImode, XEXP (in, 0)))\n-    return GENERAL_REGS;\n-\n   return NO_REGS;\n }\n \n@@ -4431,6 +4302,38 @@ shadd_operand (op, mode)\n   return (GET_CODE (op) == CONST_INT && shadd_constant_p (INTVAL (op)));\n }\n \n+/* Return 1 if OP is valid as a base register in a reg + reg address.  */\n+\n+int\n+basereg_operand (op, mode)\n+     rtx op;\n+     enum machine_mode mode;\n+{\n+  /* Once reload has started everything is considered valid.  Reload should\n+     only create indexed addresses using the stack/frame pointer, and any\n+     others were checked for validity when created by the combine pass. \n+\n+     Also allow any register when TARGET_NO_SPACE_REGS is in effect since\n+     we don't have to worry about the braindamaged implicit space register\n+     selection using the basereg only (rather than effective address)\n+     screwing us over.  */\n+  if (TARGET_NO_SPACE_REGS || reload_in_progress || reload_completed)\n+    return (GET_CODE (op) == REG || GET_CODE (op) == CONST_INT);\n+\n+  /* Stack and frame pointers are always OK for indexing.  */\n+  if (op == stack_pointer_rtx || op == frame_pointer_rtx)\n+    return 1;\n+\n+  /* The only other valid OPs are pseudo registers with\n+     REGNO_POINTER_FLAG set.  */\n+  if (GET_CODE (op) != REG\n+      || REGNO (op) < FIRST_PSEUDO_REGISTER\n+      || ! register_operand (op, mode))\n+    return 0;\n+    \n+  return REGNO_POINTER_FLAG (REGNO (op));\n+}\n+\n /* Return 1 if this operand is anything other than a hard register.  */\n \n int"}, {"sha": "52c9fc965a2c9faf7ba96823504ca340b7201d5d", "filename": "gcc/config/pa/pa.h", "status": "modified", "additions": 27, "deletions": 57, "changes": 84, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/68944452e4abe12bbef4fa078bf614caa39c85cf/gcc%2Fconfig%2Fpa%2Fpa.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/68944452e4abe12bbef4fa078bf614caa39c85cf/gcc%2Fconfig%2Fpa%2Fpa.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fpa%2Fpa.h?ref=68944452e4abe12bbef4fa078bf614caa39c85cf", "patch": "@@ -1309,10 +1309,6 @@ extern struct rtx_def *hppa_builtin_saveregs ();\n    these things in insns and then not re-recognize the insns, causing\n    constrain_operands to fail.\n \n-   Also note `Q' accepts any memory operand during the reload pass.\n-   This includes out-of-range displacements in reg+d addressing.\n-   This makes for better code.  (??? For 2.5 address this issue).\n-\n    `R' is unused.\n \n    `S' is unused.\n@@ -1321,8 +1317,6 @@ extern struct rtx_def *hppa_builtin_saveregs ();\n #define EXTRA_CONSTRAINT(OP, C)\t\t\t\t\\\n   ((C) == 'Q' ?\t\t\t\t\t\t\\\n    (IS_RELOADING_PSEUDO_P (OP)\t\t\t\t\\\n-    || (GET_CODE (OP) == MEM \t\t\t\t\\\n-\t&& reload_in_progress)\t\t\t\t\\\n     || (GET_CODE (OP) == MEM\t\t\t\t\\\n \t&& memory_address_p (GET_MODE (OP), XEXP (OP, 0))\\\n \t&& ! symbolic_memory_operand (OP, VOIDmode)))\t\\\n@@ -1571,6 +1565,11 @@ while (0)\n    in one reasonably fast instruction.  */\n #define MOVE_MAX 8\n \n+/* Higher than the default as we prefer to use simple move insns\n+   (better scheduling and delay slot filling) and because our\n+   built-in block move is really a 2X unrolled loop.  */\n+#define MOVE_RATIO 4\n+\n /* Define if operations between registers always perform the operation\n    on the full register even if a narrower mode is specified.  */\n #define WORD_REGISTER_OPERATIONS\n@@ -1685,22 +1684,28 @@ while (0)\n    switch on CODE.  The purpose for the cost of MULT is to encourage\n    `synth_mult' to find a synthetic multiply when reasonable.  */\n \n-#define RTX_COSTS(X,CODE,OUTER_CODE) \\\n-  case MULT:\t\t\t\t\t\t\t\\\n-    return (TARGET_SNAKE && ! TARGET_DISABLE_FPREGS\t\t\\\n-\t    && ! TARGET_SOFT_FLOAT\t\t\t\t\\\n-\t    ? COSTS_N_INSNS (8) : COSTS_N_INSNS (20)); \t\t\\\n-  case DIV:\t\t\t\t\t\t\t\\\n-  case UDIV:\t\t\t\t\t\t\t\\\n-  case MOD:\t\t\t\t\t\t\t\\\n-  case UMOD:\t\t\t\t\t\t\t\\\n-    return COSTS_N_INSNS (60);\t\t\t\t\t\\\n-  case PLUS:\t\t\t\t\t\t\t\\\n-    if (GET_CODE (XEXP (X, 0)) == MULT\t\t\t\t\\\n-\t&& shadd_operand (XEXP (XEXP (X, 0), 1), VOIDmode))\t\\\n-      return (2 + rtx_cost (XEXP (XEXP (X, 0), 0), OUTER_CODE)\t\\\n-\t      + rtx_cost (XEXP (X, 1), OUTER_CODE));\t\t\\\n-    break;\n+#define RTX_COSTS(X,CODE,OUTER_CODE)\t\t\t\t\t\\\n+  case MULT:\t\t\t\t\t\t\t\t\\\n+    if (GET_MODE_CLASS (GET_MODE (X)) == MODE_FLOAT)\t\t\t\\\n+      return COSTS_N_INSNS (3);\t\t\t\t\t\t\\\n+    return (TARGET_SNAKE && ! TARGET_DISABLE_FPREGS && ! TARGET_SOFT_FLOAT) \\\n+\t    ? COSTS_N_INSNS (8) : COSTS_N_INSNS (20);\t\\\n+  case DIV:\t\t\t\t\t\t\t\t\\\n+    if (GET_MODE_CLASS (GET_MODE (X)) == MODE_FLOAT)\t\t\t\\\n+      return COSTS_N_INSNS (14);\t\t\t\t\t\\\n+  case UDIV:\t\t\t\t\t\t\t\t\\\n+  case MOD:\t\t\t\t\t\t\t\t\\\n+  case UMOD:\t\t\t\t\t\t\t\t\\\n+    return COSTS_N_INSNS (60);\t\t\t\t\t\t\\\n+  case PLUS: /* this includes shNadd insns */\t\t\t\t\\\n+  case MINUS:\t\t\t\t\t\t\t\t\\\n+    if (GET_MODE_CLASS (GET_MODE (X)) == MODE_FLOAT)\t\t\t\\\n+      return COSTS_N_INSNS (3);\t\t\t\t\t\t\\\n+    return COSTS_N_INSNS (1);\t\t\t\t\t\t\\\n+  case ASHIFT:\t\t\t\t\t\t\t\t\\\n+  case ASHIFTRT:\t\t\t\t\t\t\t\\\n+  case LSHIFTRT:\t\t\t\t\t\t\t\\\n+    return COSTS_N_INSNS (1);\n \n /* Adjust the cost of dependencies.  */\n \n@@ -2154,41 +2159,6 @@ extern struct rtx_def *legitimize_pic_address ();\n extern struct rtx_def *gen_cmp_fp ();\n extern void hppa_encode_label ();\n \n-#if 0\n-#define PREDICATE_CODES \\\n-  {\"reg_or_0_operand\", {SUBREG, REG, CONST_INT, CONST_DOUBLE}},\t\t\\\n-  {\"reg_or_cint_move_operand\", {SUBREG, REG, CONST_INT}},\t\t\\\n-  {\"arith_operand\", {SUBREG, REG, CONST_INT}},\t\t\t\t\\\n-  {\"arith32_operand\", {SUBREG, REG, CONST_INT}},\t\t\t\\\n-  {\"arith11_operand\", {SUBREG, REG, CONST_INT}},\t\t\t\\\n-  {\"arith5_operand\", {SUBREG, REG, CONST_INT}},\t\t\t\t\\\n-  {\"pre_cint_operand\", {CONST_INT}},\t\t\t\t\t\\\n-  {\"post_cint_operand\", {CONST_INT}},\t\t\t\t\t\\\n-  {\"int5_operand\", {CONST_INT}},\t\t\t\t\t\\\n-  {\"uint5_operand\", {CONST_INT}},\t\t\t\t\t\\\n-  {\"uint32_operand\", {CONST_INT}},\t\t\t\t\t\\\n-  {\"int11_operand\", {CONST_INT}},\t\t\t\t\t\\\n-  {\"and_operand\", {SUBREG, REG, CONST_INT}},\t\t\t\t\\\n-  {\"ior_operand\", {CONST_INT}},\t\t\t\t\t\t\\\n-  {\"lhs_lshift_operand\", {SUBREG, REG, CONST_INT}},\t\t\t\\\n-  {\"lhs_lshift_cint_operand\", {CONST_INT}},\t\t\t\t\\\n-  {\"plus_xor_ior_operator\", {PLUS, XOR, IOR}},\t\t\t\t\\\n-  {\"shadd_operand\", {CONST_INT}},\t\t\t\t\t\\\n-  {\"eq_neq_comparison_operator\", {EQ, NE}},\t\t\t\t\\\n-  {\"movb_comparison_operator\", {EQ, NE, LT, GE}},\t\t\t\\\n-  {\"pc_or_label_operand\", {LABEL_REF, PC}},\t\t\t\t\\\n-  {\"symbolic_operand\", {SYMBOL_REF, LABEL_REF, CONST}},\t\t\t\\\n-  {\"reg_or_nonsymb_mem_operand\", {SUBREG, REG, MEM}},\t\t\t\\\n-  {\"move_operand\", {SUBREG, REG, CONST_INT, MEM}},\t\t\t\\\n-  {\"pic_label_operand\", {LABEL_REF, CONST}},\t\t\t\t\\\n-  {\"function_label_operand\", {SYMBOL_REF}},\t\t\t\t\\\n-  {\"reg_or_0_or_nonsymb_mem_operand\", {SUBREG, REG, CONST_INT,\t\t\\\n-\t\t\t\t       CONST_DOUBLE, MEM}},\t\t\\\n-  {\"div_operand\", {REG, CONST_INT}},\t\t\t\t\t\\\n-  {\"call_operand_address\", {SYMBOL_REF, LABEL_REF, CONST_INT,\t\t\\\n-\t\t\t    CONST_DOUBLE, CONST, HIGH}},\n-#endif\n-\n /* We want __gcc_plt_call to appear in every program built by\n    gcc, so we make a reference to it out of __main.\n    We use the asm statement to fool the optimizer into not"}, {"sha": "47fb46da2d02d0fa9811a0f551305fd80e09df44", "filename": "gcc/config/pa/pa.md", "status": "modified", "additions": 320, "deletions": 12, "changes": 332, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/68944452e4abe12bbef4fa078bf614caa39c85cf/gcc%2Fconfig%2Fpa%2Fpa.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/68944452e4abe12bbef4fa078bf614caa39c85cf/gcc%2Fconfig%2Fpa%2Fpa.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fpa%2Fpa.md?ref=68944452e4abe12bbef4fa078bf614caa39c85cf", "patch": "@@ -386,7 +386,71 @@\n \t\t\t     [(match_operand:SF 0 \"reg_or_0_operand\" \"fG\")\n \t\t\t      (match_operand:SF 1 \"reg_or_0_operand\" \"fG\")]))]\n   \"! TARGET_SOFT_FLOAT\"\n-  \"fcmp,sgl,%Y2 %r0,%r1\"\n+  \"*\n+{\n+  rtx next_insn;\n+\n+  /* See if this is later used in a reversed FP branch.  If so, reverse our\n+     condition and the branch.  Doing so avoids a useless add,tr. \n+\n+     Don't do this if fcmp is in a delay slot since it's too much of a\n+     headache to track down things on multiple paths.  */\n+  if (dbr_sequence_length ())\n+    next_insn = NULL;\n+  else\n+    next_insn = NEXT_INSN (insn);\n+  while (next_insn)\n+    {\n+      /* Jumps, calls and labels stop our search.  */\n+      if (GET_CODE (next_insn) == JUMP_INSN\n+\t  || GET_CODE (next_insn) == CALL_INSN\n+\t  || GET_CODE (next_insn) == CODE_LABEL)\n+\tbreak;\n+\n+      /* As does another fcmp insn.  */\n+      if (GET_CODE (next_insn) == INSN\n+\t  && GET_CODE (PATTERN (next_insn)) == SET\n+\t  && GET_CODE (SET_DEST (PATTERN (next_insn))) == REG\n+\t  && REGNO (SET_DEST (PATTERN (next_insn))) == 0)\n+\tbreak;\n+\n+      if (GET_CODE (next_insn) == INSN\n+\t  && GET_CODE (PATTERN (next_insn)) == SEQUENCE)\n+\tnext_insn = XVECEXP (PATTERN (next_insn), 0, 0);\n+      else\n+\tnext_insn = NEXT_INSN (next_insn);\n+    }\n+\n+  /* Is NEXT_INSN a branch?  */\n+  if (next_insn\n+      && GET_CODE (next_insn) == JUMP_INSN)\n+    {\n+      rtx pattern = PATTERN (next_insn);\n+\n+      /* Is it a reversed fp conditional branch (eg uses add,tr) and\n+\t CCFP dies, then reverse our conditional and the branch to\n+\t avoid the add,tr.  */\n+      if (GET_CODE (pattern) == SET\n+\t  && SET_DEST (pattern) == pc_rtx\n+\t  && GET_CODE (SET_SRC (pattern)) == IF_THEN_ELSE\n+\t  && GET_CODE (XEXP (SET_SRC (pattern), 0)) == NE\n+\t  && GET_CODE (XEXP (XEXP (SET_SRC (pattern), 0), 0)) == REG\n+\t  && REGNO (XEXP (XEXP (SET_SRC (pattern), 0), 0)) == 0\n+\t  && GET_CODE (XEXP (SET_SRC (pattern), 1)) == PC\n+\t  && find_regno_note (next_insn, REG_DEAD, 0))\n+\n+\t{\n+\t  rtx tmp;\n+\n+\t  tmp = XEXP (SET_SRC (pattern), 1);\n+\t  XEXP (SET_SRC (pattern), 1) = XEXP (SET_SRC (pattern), 2);\n+\t  XEXP (SET_SRC (pattern), 2) = tmp;\n+\t  INSN_CODE (next_insn) = -1;\n+\t  return \\\"fcmp,sgl,%y2 %r0,%r1\\\";\n+\t}\n+    }\n+  return \\\"fcmp,sgl,%Y2 %r0,%r1\\\";\n+}\"\n   [(set_attr \"length\" \"4\")\n    (set_attr \"type\" \"fpcc\")])\n \n@@ -396,7 +460,71 @@\n \t\t\t     [(match_operand:DF 0 \"reg_or_0_operand\" \"fG\")\n \t\t\t      (match_operand:DF 1 \"reg_or_0_operand\" \"fG\")]))]\n   \"! TARGET_SOFT_FLOAT\"\n-  \"fcmp,dbl,%Y2 %r0,%r1\"\n+  \"*\n+{\n+  rtx next_insn;\n+\n+  /* See if this is later used in a reversed FP branch.  If so, reverse our\n+     condition and the branch.  Doing so avoids a useless add,tr. \n+\n+     Don't do this if fcmp is in a delay slot since it's too much of a\n+     headache to track down things on multiple paths.  */\n+  if (dbr_sequence_length ())\n+    next_insn = NULL;\n+  else\n+    next_insn = NEXT_INSN (insn);\n+  while (next_insn)\n+    {\n+      /* Jumps, calls and labels stop our search.  */\n+      if (GET_CODE (next_insn) == JUMP_INSN\n+\t  || GET_CODE (next_insn) == CALL_INSN\n+\t  || GET_CODE (next_insn) == CODE_LABEL)\n+\tbreak;\n+\n+      /* As does another fcmp insn.  */\n+      if (GET_CODE (next_insn) == INSN\n+\t  && GET_CODE (PATTERN (next_insn)) == SET\n+\t  && GET_CODE (SET_DEST (PATTERN (next_insn))) == REG\n+\t  && REGNO (SET_DEST (PATTERN (next_insn))) == 0)\n+\tbreak;\n+\n+      if (GET_CODE (next_insn) == INSN\n+\t  && GET_CODE (PATTERN (next_insn)) == SEQUENCE)\n+\tnext_insn = XVECEXP (PATTERN (next_insn), 0, 0);\n+      else\n+\tnext_insn = NEXT_INSN (next_insn);\n+    }\n+\n+  /* Is NEXT_INSN a branch?  */\n+  if (next_insn\n+      && GET_CODE (next_insn) == JUMP_INSN)\n+    {\n+      rtx pattern = PATTERN (next_insn);\n+\n+      /* Is it a reversed fp conditional branch (eg uses add,tr) and\n+\t CCFP dies, then reverse our conditional and the branch to\n+\t avoid the add,tr.  */\n+      if (GET_CODE (pattern) == SET\n+\t  && SET_DEST (pattern) == pc_rtx\n+\t  && GET_CODE (SET_SRC (pattern)) == IF_THEN_ELSE\n+\t  && GET_CODE (XEXP (SET_SRC (pattern), 0)) == NE\n+\t  && GET_CODE (XEXP (XEXP (SET_SRC (pattern), 0), 0)) == REG\n+\t  && REGNO (XEXP (XEXP (SET_SRC (pattern), 0), 0)) == 0\n+\t  && GET_CODE (XEXP (SET_SRC (pattern), 1)) == PC\n+\t  && find_regno_note (next_insn, REG_DEAD, 0))\n+\n+\t{\n+\t  rtx tmp;\n+\n+\t  tmp = XEXP (SET_SRC (pattern), 1);\n+\t  XEXP (SET_SRC (pattern), 1) = XEXP (SET_SRC (pattern), 2);\n+\t  XEXP (SET_SRC (pattern), 2) = tmp;\n+\t  INSN_CODE (next_insn) = -1;\n+\t  return \\\"fcmp,dbl,%y2 %r0,%r1\\\";\n+\t}\n+    }\n+  return \\\"fcmp,dbl,%Y2 %r0,%r1\\\";\n+}\"\n   [(set_attr \"length\" \"4\")\n    (set_attr \"type\" \"fpcc\")])\n \n@@ -761,6 +889,15 @@\n   comiclr,<< %2,%0,0\\;ldi %2,%0\"\n [(set_attr \"type\" \"multi,multi\")\n  (set_attr \"length\" \"8,8\")])\n+\n+(define_insn \"abssi2\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=r\")\n+\t(abs:SI (match_operand:SI 1 \"register_operand\" \"0\")))]\n+  \"\"\n+  \"comiclr,< 0,%0,0\\;subi 0,%0,%0\"\n+  [(set_attr \"type\" \"multi\")\n+   (set_attr \"length\" \"8\")])\n+\n ;;; Experimental conditional move patterns\n \n (define_expand \"movsicc\"\n@@ -1302,6 +1439,25 @@\n   [(set_attr \"type\" \"load\")\n    (set_attr \"length\" \"8\")])\n \n+(define_insn \"\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=r\")\n+\t(mem:SI (plus:SI (match_operand:SI 1 \"register_operand\" \"r\")\n+\t\t\t (match_operand:SI 2 \"basereg_operand\" \"r\"))))]\n+  \"! TARGET_DISABLE_INDEXING\"\n+  \"*\n+{\n+  /* Reload can create backwards (relative to cse) unscaled index\n+     address modes when eliminating registers and possibly for\n+     pseudos that don't get hard registers.  Deal with it.  */\n+  if (operands[1] == hard_frame_pointer_rtx\n+      || operands[1] == stack_pointer_rtx)\n+    return \\\"ldwx %2(0,%1),%0\\\";\n+  else\n+    return \\\"ldwx %1(0,%2),%0\\\";\n+}\"\n+  [(set_attr \"type\" \"load\")\n+   (set_attr \"length\" \"4\")])\n+\n ;; Load or store with base-register modification.\n \n (define_insn \"pre_ldwm\"\n@@ -1623,6 +1779,25 @@\n   [(set_attr \"type\" \"load\")\n    (set_attr \"length\" \"8\")])\n \n+(define_insn \"\"\n+  [(set (match_operand:HI 0 \"register_operand\" \"=r\")\n+\t(mem:HI (plus:SI (match_operand:SI 1 \"register_operand\" \"r\")\n+\t\t\t (match_operand:SI 2 \"basereg_operand\" \"r\"))))]\n+  \"! TARGET_DISABLE_INDEXING\"\n+  \"*\n+{\n+  /* Reload can create backwards (relative to cse) unscaled index\n+     address modes when eliminating registers and possibly for\n+     pseudos that don't get hard registers.  Deal with it.  */\n+  if (operands[1] == hard_frame_pointer_rtx\n+      || operands[1] == stack_pointer_rtx)\n+    return \\\"ldhx %2(0,%1),%0\\\";\n+  else\n+    return \\\"ldhx %1(0,%2),%0\\\";\n+}\"\n+  [(set_attr \"type\" \"load\")\n+   (set_attr \"length\" \"4\")])\n+\n (define_insn \"\"\n   [(set (match_operand:HI 3 \"register_operand\" \"=r\")\n \t(mem:HI (plus:SI (match_operand:SI 1 \"register_operand\" \"0\")\n@@ -1689,6 +1864,25 @@\n   [(set_attr \"type\" \"move,move,move,shift,load,store,move,fpalu\")\n    (set_attr \"length\" \"4,4,4,4,4,4,4,4\")])\n \n+(define_insn \"\"\n+  [(set (match_operand:QI 0 \"register_operand\" \"=r\")\n+\t(mem:QI (plus:SI (match_operand:SI 1 \"register_operand\" \"r\")\n+\t\t\t (match_operand:SI 2 \"basereg_operand\" \"r\"))))]\n+  \"! TARGET_DISABLE_INDEXING\"\n+  \"*\n+{\n+  /* Reload can create backwards (relative to cse) unscaled index\n+     address modes when eliminating registers and possibly for\n+     pseudos that don't get hard registers.  Deal with it.  */\n+  if (operands[1] == hard_frame_pointer_rtx\n+      || operands[1] == stack_pointer_rtx)\n+    return \\\"ldbx %2(0,%1),%0\\\";\n+  else\n+    return \\\"ldbx %1(0,%2),%0\\\";\n+}\"\n+  [(set_attr \"type\" \"load\")\n+   (set_attr \"length\" \"4\")])\n+\n (define_insn \"\"\n   [(set (match_operand:QI 3 \"register_operand\" \"=r\")\n \t(mem:QI (plus:SI (match_operand:SI 1 \"register_operand\" \"0\")\n@@ -1727,33 +1921,70 @@\n   \"\"\n   \"\n {\n-  /* If the blocks are not at least word-aligned and rather big (>16 items),\n-     or the size is indeterminate, don't inline the copy code.  A\n-     procedure call is better since it can check the alignment at\n-     runtime and make the optimal decisions.  */\n-     if (INTVAL (operands[3]) < 4\n-\t && (GET_CODE (operands[2]) != CONST_INT\n-\t     || (INTVAL (operands[2]) / INTVAL (operands[3]) > 8)))\n-       FAIL;\n+  int size, align;\n+  /* HP provides very fast block move library routine for the PA;\n+     this routine includes:\n+\n+\t4x4 byte at a time block moves,\n+\t1x4 byte at a time with alignment checked at runtime with\n+\t    attempts to align the source and destination as needed\n+\t1x1 byte loop\n+\n+     With that in mind, here's the heuristics to try and guess when\n+     the inlined block move will be better than the library block\n+     move:\n+\n+\tIf the size isn't constant, then always use the library routines.\n+\n+\tIf the size is large in respect to the known alignment, then use\n+\tthe library routines.\n+\n+\tIf the size is small in repsect to the known alignment, then open\n+\tcode the copy (since that will lead to better scheduling).\n+\n+        Else use the block move pattern.   */\n+\n+  /* Undetermined size, use the library routine.  */\n+  if (GET_CODE (operands[2]) != CONST_INT)\n+    FAIL;\n+\n+  size = INTVAL (operands[2]);\n+  align = INTVAL (operands[3]);\n+  align = align > 4 ? 4 : align;\n \n+  /* If size/alignment > 8 (eg size is large in respect to alignment),\n+     then use the library routines.  */\n+  if (size/align > 16)\n+    FAIL;\n+\n+  /* This does happen, but not often enough to worry much about.  */\n+  if (size/align < MOVE_RATIO)\n+    FAIL;\n+  \n+  /* Fall through means we're going to use our block move pattern.  */\n   operands[0] = copy_to_mode_reg (SImode, XEXP (operands[0], 0));\n   operands[1] = copy_to_mode_reg (SImode, XEXP (operands[1], 0));\n   operands[4] = gen_reg_rtx (SImode);\n   operands[5] = gen_reg_rtx (SImode);\n+  emit_insn (gen_movstrsi_internal (operands[0], operands[1], operands[4],\n+\t\t\t\t     operands[5], operands[2], operands[3],\n+\t\t\t\t     gen_reg_rtx (SImode)));\n+  DONE;\n }\")\n \n ;; The operand constraints are written like this to support both compile-time\n ;; and run-time determined byte count.  If the count is run-time determined,\n ;; the register with the byte count is clobbered by the copying code, and\n ;; therefore it is forced to operand 2.  If the count is compile-time\n ;; determined, we need two scratch registers for the unrolled code.\n-(define_insn \"\"\n+(define_insn \"movstrsi_internal\"\n   [(set (mem:BLK (match_operand:SI 0 \"register_operand\" \"+r,r\"))\n \t(mem:BLK (match_operand:SI 1 \"register_operand\" \"+r,r\")))\n    (clobber (match_dup 0))\n    (clobber (match_dup 1))\n    (clobber (match_operand:SI 2 \"register_operand\" \"=r,r\"))\t;loop cnt/tmp\n    (clobber (match_operand:SI 3 \"register_operand\" \"=&r,&r\"))\t;item tmp\n+   (clobber (match_operand:SI 6 \"register_operand\" \"=&r,&r\"))\t;item tmp2\n    (use (match_operand:SI 4 \"arith_operand\" \"J,2\"))\t ;byte count\n    (use (match_operand:SI 5 \"const_int_operand\" \"n,n\"))] ;alignment\n   \"\"\n@@ -1778,7 +2009,7 @@\n    && operands[1] != CONST0_RTX (DFmode)\n    && ! TARGET_SOFT_FLOAT\"\n   \"* return (which_alternative == 0 ? output_move_double (operands)\n-\t\t\t\t    : \\\" fldds%F1 %1,%0\\\");\"\n+\t\t\t\t    : \\\"fldds%F1 %1,%0\\\");\"\n   [(set_attr \"type\" \"move,fpload\")\n    (set_attr \"length\" \"16,4\")])\n \n@@ -1897,6 +2128,25 @@\n   [(set_attr \"type\" \"fpload\")\n    (set_attr \"length\" \"8\")])\n \n+(define_insn \"\"\n+  [(set (match_operand:DF 0 \"register_operand\" \"=fx\")\n+\t(mem:DF (plus:SI (match_operand:SI 1 \"register_operand\" \"r\")\n+\t\t\t (match_operand:SI 2 \"basereg_operand\" \"r\"))))]\n+  \"! TARGET_DISABLE_INDEXING && ! TARGET_SOFT_FLOAT\"\n+  \"*\n+{\n+  /* Reload can create backwards (relative to cse) unscaled index\n+     address modes when eliminating registers and possibly for\n+     pseudos that don't get hard registers.  Deal with it.  */\n+  if (operands[1] == hard_frame_pointer_rtx\n+      || operands[1] == stack_pointer_rtx)\n+    return \\\"flddx %2(0,%1),%0\\\";\n+  else\n+    return \\\"flddx %1(0,%2),%0\\\";\n+}\"\n+  [(set_attr \"type\" \"fpload\")\n+   (set_attr \"length\" \"4\")])\n+\n (define_insn \"\"\n   [(set (mem:DF (plus:SI (mult:SI (match_operand:SI 1 \"register_operand\" \"r\")\n \t\t\t\t  (const_int 8))\n@@ -1936,6 +2186,25 @@\n   [(set_attr \"type\" \"fpstore\")\n    (set_attr \"length\" \"8\")])\n \n+(define_insn \"\"\n+  [(set (mem:DF (plus:SI (match_operand:SI 1 \"register_operand\" \"r\")\n+\t\t\t (match_operand:SI 2 \"basereg_operand\" \"r\")))\n+\t(match_operand:DF 0 \"register_operand\" \"fx\"))]\n+  \"! TARGET_DISABLE_INDEXING && ! TARGET_SOFT_FLOAT\"\n+  \"*\n+{\n+  /* Reload can create backwards (relative to cse) unscaled index\n+     address modes when eliminating registers and possibly for\n+     pseudos that don't get hard registers.  Deal with it.  */\n+  if (operands[1] == hard_frame_pointer_rtx\n+      || operands[1] == stack_pointer_rtx)\n+    return \\\"fstdx %0,%2(0,%1)\\\";\n+  else\n+    return \\\"fstdx %0,%1(0,%2)\\\";\n+}\"\n+  [(set_attr \"type\" \"fpstore\")\n+   (set_attr \"length\" \"4\")])\n+\n (define_expand \"movdi\"\n   [(set (match_operand:DI 0 \"reg_or_nonsymb_mem_operand\" \"\")\n \t(match_operand:DI 1 \"general_operand\" \"\"))]\n@@ -2202,6 +2471,25 @@\n   [(set_attr \"type\" \"fpload\")\n    (set_attr \"length\" \"8\")])\n \n+(define_insn \"\"\n+  [(set (match_operand:SF 0 \"register_operand\" \"=fx\")\n+\t(mem:SF (plus:SI (match_operand:SI 1 \"register_operand\" \"r\")\n+\t\t\t (match_operand:SI 2 \"basereg_operand\" \"r\"))))]\n+  \"! TARGET_DISABLE_INDEXING && ! TARGET_SOFT_FLOAT\"\n+  \"*\n+{\n+  /* Reload can create backwards (relative to cse) unscaled index\n+     address modes when eliminating registers and possibly for\n+     pseudos that don't get hard registers.  Deal with it.  */\n+  if (operands[1] == hard_frame_pointer_rtx\n+      || operands[1] == stack_pointer_rtx)\n+    return \\\"fldwx %2(0,%1),%0\\\";\n+  else\n+    return \\\"fldwx %1(0,%2),%0\\\";\n+}\"\n+  [(set_attr \"type\" \"fpload\")\n+   (set_attr \"length\" \"4\")])\n+\n (define_insn \"\"\n   [(set (mem:SF (plus:SI (mult:SI (match_operand:SI 1 \"register_operand\" \"r\")\n \t\t\t\t  (const_int 4))\n@@ -2240,7 +2528,27 @@\n }\"\n   [(set_attr \"type\" \"fpstore\")\n    (set_attr \"length\" \"8\")])\n+\n+(define_insn \"\"\n+  [(set (mem:SF (plus:SI (match_operand:SI 1 \"register_operand\" \"r\")\n+\t\t\t (match_operand:SI 2 \"basereg_operand\" \"r\")))\n+      (match_operand:SF 0 \"register_operand\" \"fx\"))]\n+  \"! TARGET_DISABLE_INDEXING && ! TARGET_SOFT_FLOAT\"\n+  \"*\n+{\n+  /* Reload can create backwards (relative to cse) unscaled index\n+     address modes when eliminating registers and possibly for\n+     pseudos that don't get hard registers.  Deal with it.  */\n+  if (operands[1] == hard_frame_pointer_rtx\n+      || operands[1] == stack_pointer_rtx)\n+    return \\\"fstwx %0,%2(0,%1)\\\";\n+  else\n+    return \\\"fstwx %0,%1(0,%2)\\\";\n+}\"\n+  [(set_attr \"type\" \"fpstore\")\n+   (set_attr \"length\" \"4\")])\n \f\n+\n ;;- zero extension instructions\n \n (define_insn \"zero_extendhisi2\""}]}