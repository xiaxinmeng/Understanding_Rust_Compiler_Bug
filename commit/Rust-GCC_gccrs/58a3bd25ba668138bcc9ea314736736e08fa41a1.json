{"sha": "58a3bd25ba668138bcc9ea314736736e08fa41a1", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NThhM2JkMjViYTY2ODEzOGJjYzllYTMxNDczNjczNmUwOGZhNDFhMQ==", "commit": {"author": {"name": "Felix Yang", "email": "felix.yang@huawei.com", "date": "2014-12-08T14:19:44Z"}, "committer": {"name": "Fei Yang", "email": "fyang@gcc.gnu.org", "date": "2014-12-08T14:19:44Z"}, "message": "arm_neon.h (vrecpe_u32, [...]): Rewrite using builtin functions.\n\n        * config/aarch64/arm_neon.h (vrecpe_u32, vrecpeq_u32): Rewrite using\n        builtin functions.\n        (vfma_f32, vfmaq_f32, vfmaq_f64, vfma_n_f32, vfmaq_n_f32, vfmaq_n_f64,\n        vfms_f32, vfmsq_f32, vfmsq_f64): Likewise.\n        (vhsub_s8, vhsub_u8, vhsub_s16, vhsub_u16, vhsub_s32, vhsub_u32,\n        vhsubq_s8, vhsubq_u8, vhsubq_s16, vhsubq_u16, vhsubq_s32, vhsubq_u32,\n        vsubhn_s16, vsubhn_u16, vsubhn_s32, vsubhn_u32, vsubhn_s64, vsubhn_u66,\n        vrsubhn_s16, vrsubhn_u16, vrsubhn_s32, vrsubhn_u32, vrsubhn_s64,\n        vrsubhn_u64, vsubhn_high_s16, vsubhn_high_u16, vsubhn_high_s32,\n        vsubhn_high_u32, vsubhn_high_s64, vsubhn_high_u64, vrsubhn_high_s16,\n        vrsubhn_high_u16, vrsubhn_high_s32, vrsubhn_high_u32, vrsubhn_high_s64,\n        vrsubhn_high_u64): Likewise.\n        * config/aarch64/iterators.md (VDQ_SI): New mode iterator.\n        * config/aarch64/aarch64.md (define_c_enum \"unspec\"): Add UNSPEC_URECPE.\n        * config/aarch64/aarch64-simd.md (aarch64_urecpe<mode>): New pattern.\n        * config/aarch64/aarch64-simd-builtins.def (shsub, uhsub, subhn, rsubhn,\n        subhn2, rsubhn2, urecpe): New builtins.\n\nCo-Authored-By: Haijian Zhang <z.zhanghaijian@huawei.com>\nCo-Authored-By: Jiji Jiang <jiangjiji@huawei.com>\nCo-Authored-By: Pengfei Sui <suipengfei@huawei.com>\n\nFrom-SVN: r218484", "tree": {"sha": "68168c4b529252dbd92dbbb1b00de304b142ff87", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/68168c4b529252dbd92dbbb1b00de304b142ff87"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/58a3bd25ba668138bcc9ea314736736e08fa41a1", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/58a3bd25ba668138bcc9ea314736736e08fa41a1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/58a3bd25ba668138bcc9ea314736736e08fa41a1", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/58a3bd25ba668138bcc9ea314736736e08fa41a1/comments", "author": null, "committer": null, "parents": [{"sha": "28adf6e75ec45da4aa5da5f54fdc51e5ea2836c0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/28adf6e75ec45da4aa5da5f54fdc51e5ea2836c0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/28adf6e75ec45da4aa5da5f54fdc51e5ea2836c0"}], "stats": {"total": 1103, "additions": 575, "deletions": 528}, "files": [{"sha": "f01a99fd2d52ce7d9496dbdbcb5ffe7fdf191edb", "filename": "gcc/ChangeLog", "status": "modified", "additions": 23, "deletions": 1, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -1,3 +1,26 @@\n+2014-12-08  Felix Yang  <felix.yang@huawei.com>\n+\t    Haijian Zhang  <z.zhanghaijian@huawei.com>\n+\t    Jiji Jiang  <jiangjiji@huawei.com>\n+\t    Pengfei Sui  <suipengfei@huawei.com>\n+\n+\t* config/aarch64/arm_neon.h (vrecpe_u32, vrecpeq_u32): Rewrite using\n+\tbuiltin functions.\n+\t(vfma_f32, vfmaq_f32, vfmaq_f64, vfma_n_f32, vfmaq_n_f32, vfmaq_n_f64,\n+\tvfms_f32, vfmsq_f32, vfmsq_f64): Likewise.\n+\t(vhsub_s8, vhsub_u8, vhsub_s16, vhsub_u16, vhsub_s32, vhsub_u32,\n+\tvhsubq_s8, vhsubq_u8, vhsubq_s16, vhsubq_u16, vhsubq_s32, vhsubq_u32,\n+\tvsubhn_s16, vsubhn_u16, vsubhn_s32, vsubhn_u32, vsubhn_s64, vsubhn_u66,\n+\tvrsubhn_s16, vrsubhn_u16, vrsubhn_s32, vrsubhn_u32, vrsubhn_s64,\n+\tvrsubhn_u64, vsubhn_high_s16, vsubhn_high_u16, vsubhn_high_s32,\n+\tvsubhn_high_u32, vsubhn_high_s64, vsubhn_high_u64, vrsubhn_high_s16,\n+\tvrsubhn_high_u16, vrsubhn_high_s32, vrsubhn_high_u32, vrsubhn_high_s64,\n+\tvrsubhn_high_u64): Likewise.\n+\t* config/aarch64/iterators.md (VDQ_SI): New mode iterator.\n+\t* config/aarch64/aarch64.md (define_c_enum \"unspec\"): Add UNSPEC_URECPE.\n+\t* config/aarch64/aarch64-simd.md (aarch64_urecpe<mode>): New pattern.\n+\t* config/aarch64/aarch64-simd-builtins.def (shsub, uhsub, subhn, rsubhn,\n+\tsubhn2, rsubhn2, urecpe): New builtins.\n+\n 2014-12-08  Ilya Tocar  <ilya.tocar@intel.com>\n \n \t* config/i386/i386.c (ix86_expand_vec_perm_vpermi2): Handle v64qi.\n@@ -5997,7 +6020,6 @@\n \t* config/aarch64/aarch64-simd.md (*aarch64_simd_ld1r<mode>): Use\n \tVALL mode iterator instead of VALLDI.\n \n-\n 2014-11-14  Jan Hubicka  <hubicka@ucw.cz>\n \n \t* optc-save-gen.awk: Output cl_target_option_eq,"}, {"sha": "745f1079156f1fe90c720e5ba41e308eb91f7062", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -127,15 +127,21 @@\n   BUILTIN_VD_BHSI (BINOP, usubw, 0)\n   /* Implemented by aarch64_<sur>h<addsub><mode>.  */\n   BUILTIN_VDQ_BHSI (BINOP, shadd, 0)\n+  BUILTIN_VDQ_BHSI (BINOP, shsub, 0)\n   BUILTIN_VDQ_BHSI (BINOP, uhadd, 0)\n+  BUILTIN_VDQ_BHSI (BINOP, uhsub, 0)\n   BUILTIN_VDQ_BHSI (BINOP, srhadd, 0)\n   BUILTIN_VDQ_BHSI (BINOP, urhadd, 0)\n   /* Implemented by aarch64_<sur><addsub>hn<mode>.  */\n   BUILTIN_VQN (BINOP, addhn, 0)\n+  BUILTIN_VQN (BINOP, subhn, 0)\n   BUILTIN_VQN (BINOP, raddhn, 0)\n+  BUILTIN_VQN (BINOP, rsubhn, 0)\n   /* Implemented by aarch64_<sur><addsub>hn2<mode>.  */\n   BUILTIN_VQN (TERNOP, addhn2, 0)\n+  BUILTIN_VQN (TERNOP, subhn2, 0)\n   BUILTIN_VQN (TERNOP, raddhn2, 0)\n+  BUILTIN_VQN (TERNOP, rsubhn2, 0)\n \n   BUILTIN_VSQN_HSDI (UNOP, sqmovun, 0)\n   /* Implemented by aarch64_<sur>qmovn<mode>.  */\n@@ -338,6 +344,8 @@\n   BUILTIN_GPF (BINOP, frecps, 0)\n   BUILTIN_GPF (UNOP, frecpx, 0)\n \n+  BUILTIN_VDQ_SI (UNOP, urecpe, 0)\n+\n   BUILTIN_VDQF (UNOP, frecpe, 0)\n   BUILTIN_VDQF (BINOP, frecps, 0)\n "}, {"sha": "733512c427d77e750c4effb2ff3995f3011fc16d", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -4840,6 +4840,14 @@\n   [(set_attr \"type\" \"neon_fp_recps_<Vetype><q>\")]\n )\n \n+(define_insn \"aarch64_urecpe<mode>\"\n+  [(set (match_operand:VDQ_SI 0 \"register_operand\" \"=w\")\n+        (unspec:VDQ_SI [(match_operand:VDQ_SI 1 \"register_operand\" \"w\")]\n+                UNSPEC_URECPE))]\n+ \"TARGET_SIMD\"\n+ \"urecpe\\\\t%0.<Vtype>, %1.<Vtype>\"\n+  [(set_attr \"type\" \"neon_fp_recpe_<Vetype><q>\")])\n+\n ;; Standard pattern name vec_extract<mode>.\n \n (define_expand \"vec_extract<mode>\""}, {"sha": "97c1dff2ed6ae1ad7238739f9473b13147c637d2", "filename": "gcc/config/aarch64/aarch64.md", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Fconfig%2Faarch64%2Faarch64.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Fconfig%2Faarch64%2Faarch64.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.md?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -75,6 +75,7 @@\n     UNSPEC_CRC32H\n     UNSPEC_CRC32W\n     UNSPEC_CRC32X\n+    UNSPEC_URECPE\n     UNSPEC_FRECPE\n     UNSPEC_FRECPS\n     UNSPEC_FRECPX"}, {"sha": "0435f89c72895e819ab96a0267a25dc0b2f91a1a", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 316, "deletions": 525, "changes": 841, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -2287,6 +2287,246 @@ vqadd_u8 (uint8x8_t __a, uint8x8_t __b)\n   return __builtin_aarch64_uqaddv8qi_uuu (__a, __b);\n }\n \n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vhsub_s8 (int8x8_t __a, int8x8_t __b)\n+{\n+  return (int8x8_t)__builtin_aarch64_shsubv8qi (__a, __b);\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vhsub_s16 (int16x4_t __a, int16x4_t __b)\n+{\n+  return (int16x4_t) __builtin_aarch64_shsubv4hi (__a, __b);\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vhsub_s32 (int32x2_t __a, int32x2_t __b)\n+{\n+  return (int32x2_t) __builtin_aarch64_shsubv2si (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vhsub_u8 (uint8x8_t __a, uint8x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_uhsubv8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t  (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vhsub_u16 (uint16x4_t __a, uint16x4_t __b)\n+{\n+  return (uint16x4_t) __builtin_aarch64_uhsubv4hi ((int16x4_t) __a,\n+\t\t\t\t\t\t   (int16x4_t) __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vhsub_u32 (uint32x2_t __a, uint32x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_uhsubv2si ((int32x2_t) __a,\n+\t\t\t\t\t\t   (int32x2_t) __b);\n+}\n+\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vhsubq_s8 (int8x16_t __a, int8x16_t __b)\n+{\n+  return (int8x16_t) __builtin_aarch64_shsubv16qi (__a, __b);\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vhsubq_s16 (int16x8_t __a, int16x8_t __b)\n+{\n+  return (int16x8_t) __builtin_aarch64_shsubv8hi (__a, __b);\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vhsubq_s32 (int32x4_t __a, int32x4_t __b)\n+{\n+  return (int32x4_t) __builtin_aarch64_shsubv4si (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vhsubq_u8 (uint8x16_t __a, uint8x16_t __b)\n+{\n+  return (uint8x16_t) __builtin_aarch64_uhsubv16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t    (int8x16_t) __b);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vhsubq_u16 (uint16x8_t __a, uint16x8_t __b)\n+{\n+  return (uint16x8_t) __builtin_aarch64_uhsubv8hi ((int16x8_t) __a,\n+\t\t\t\t\t\t   (int16x8_t) __b);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vhsubq_u32 (uint32x4_t __a, uint32x4_t __b)\n+{\n+  return (uint32x4_t) __builtin_aarch64_uhsubv4si ((int32x4_t) __a,\n+\t\t\t\t\t\t   (int32x4_t) __b);\n+}\n+\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vsubhn_s16 (int16x8_t __a, int16x8_t __b)\n+{\n+  return (int8x8_t) __builtin_aarch64_subhnv8hi (__a, __b);\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vsubhn_s32 (int32x4_t __a, int32x4_t __b)\n+{\n+  return (int16x4_t) __builtin_aarch64_subhnv4si (__a, __b);\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vsubhn_s64 (int64x2_t __a, int64x2_t __b)\n+{\n+  return (int32x2_t) __builtin_aarch64_subhnv2di (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vsubhn_u16 (uint16x8_t __a, uint16x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_subhnv8hi ((int16x8_t) __a,\n+\t\t\t\t\t\t  (int16x8_t) __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vsubhn_u32 (uint32x4_t __a, uint32x4_t __b)\n+{\n+  return (uint16x4_t) __builtin_aarch64_subhnv4si ((int32x4_t) __a,\n+\t\t\t\t\t\t   (int32x4_t) __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vsubhn_u64 (uint64x2_t __a, uint64x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_subhnv2di ((int64x2_t) __a,\n+\t\t\t\t\t\t   (int64x2_t) __b);\n+}\n+\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vrsubhn_s16 (int16x8_t __a, int16x8_t __b)\n+{\n+  return (int8x8_t) __builtin_aarch64_rsubhnv8hi (__a, __b);\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vrsubhn_s32 (int32x4_t __a, int32x4_t __b)\n+{\n+  return (int16x4_t) __builtin_aarch64_rsubhnv4si (__a, __b);\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vrsubhn_s64 (int64x2_t __a, int64x2_t __b)\n+{\n+  return (int32x2_t) __builtin_aarch64_rsubhnv2di (__a, __b);\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vrsubhn_u16 (uint16x8_t __a, uint16x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_rsubhnv8hi ((int16x8_t) __a,\n+\t\t\t\t\t\t   (int16x8_t) __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vrsubhn_u32 (uint32x4_t __a, uint32x4_t __b)\n+{\n+  return (uint16x4_t) __builtin_aarch64_rsubhnv4si ((int32x4_t) __a,\n+\t\t\t\t\t\t    (int32x4_t) __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vrsubhn_u64 (uint64x2_t __a, uint64x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_rsubhnv2di ((int64x2_t) __a,\n+\t\t\t\t\t\t    (int64x2_t) __b);\n+}\n+\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vrsubhn_high_s16 (int8x8_t __a, int16x8_t __b, int16x8_t __c)\n+{\n+  return (int8x16_t) __builtin_aarch64_rsubhn2v8hi (__a, __b, __c);\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vrsubhn_high_s32 (int16x4_t __a, int32x4_t __b, int32x4_t __c)\n+{\n+  return (int16x8_t) __builtin_aarch64_rsubhn2v4si (__a, __b, __c);\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vrsubhn_high_s64 (int32x2_t __a, int64x2_t __b, int64x2_t __c)\n+{\n+  return (int32x4_t) __builtin_aarch64_rsubhn2v2di (__a, __b, __c);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vrsubhn_high_u16 (uint8x8_t __a, uint16x8_t __b, uint16x8_t __c)\n+{\n+  return (uint8x16_t) __builtin_aarch64_rsubhn2v8hi ((int8x8_t) __a,\n+\t\t\t\t\t\t     (int16x8_t) __b,\n+\t\t\t\t\t\t     (int16x8_t) __c);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vrsubhn_high_u32 (uint16x4_t __a, uint32x4_t __b, uint32x4_t __c)\n+{\n+  return (uint16x8_t) __builtin_aarch64_rsubhn2v4si ((int16x4_t) __a,\n+\t\t\t\t\t\t     (int32x4_t) __b,\n+\t\t\t\t\t\t     (int32x4_t) __c);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vrsubhn_high_u64 (uint32x2_t __a, uint64x2_t __b, uint64x2_t __c)\n+{\n+  return (uint32x4_t) __builtin_aarch64_rsubhn2v2di ((int32x2_t) __a,\n+\t\t\t\t\t\t     (int64x2_t) __b,\n+\t\t\t\t\t\t     (int64x2_t) __c);\n+}\n+\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vsubhn_high_s16 (int8x8_t __a, int16x8_t __b, int16x8_t __c)\n+{\n+  return (int8x16_t) __builtin_aarch64_subhn2v8hi (__a, __b, __c);\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vsubhn_high_s32 (int16x4_t __a, int32x4_t __b, int32x4_t __c)\n+{\n+  return (int16x8_t) __builtin_aarch64_subhn2v4si (__a, __b, __c);;\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vsubhn_high_s64 (int32x2_t __a, int64x2_t __b, int64x2_t __c)\n+{\n+  return (int32x4_t) __builtin_aarch64_subhn2v2di (__a, __b, __c);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vsubhn_high_u16 (uint8x8_t __a, uint16x8_t __b, uint16x8_t __c)\n+{\n+  return (uint8x16_t) __builtin_aarch64_subhn2v8hi ((int8x8_t) __a,\n+\t\t\t\t\t\t    (int16x8_t) __b,\n+\t\t\t\t\t\t    (int16x8_t) __c);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vsubhn_high_u32 (uint16x4_t __a, uint32x4_t __b, uint32x4_t __c)\n+{\n+  return (uint16x8_t) __builtin_aarch64_subhn2v4si ((int16x4_t) __a,\n+\t\t\t\t\t\t    (int32x4_t) __b,\n+\t\t\t\t\t\t    (int32x4_t) __c);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vsubhn_high_u64 (uint32x2_t __a, uint64x2_t __b, uint64x2_t __c)\n+{\n+  return (uint32x4_t) __builtin_aarch64_subhn2v2di ((int32x2_t) __a,\n+\t\t\t\t\t\t    (int64x2_t) __b,\n+\t\t\t\t\t\t    (int64x2_t) __c);\n+}\n+\n __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n vqadd_u16 (uint16x4_t __a, uint16x4_t __b)\n {\n@@ -5756,237 +5996,6 @@ vcvtxd_f32_f64 (float64_t a)\n   return result;\n }\n \n-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n-vfma_f32 (float32x2_t a, float32x2_t b, float32x2_t c)\n-{\n-  float32x2_t result;\n-  __asm__ (\"fmla %0.2s,%2.2s,%3.2s\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n-vfmaq_f32 (float32x4_t a, float32x4_t b, float32x4_t c)\n-{\n-  float32x4_t result;\n-  __asm__ (\"fmla %0.4s,%2.4s,%3.4s\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n-vfmaq_f64 (float64x2_t a, float64x2_t b, float64x2_t c)\n-{\n-  float64x2_t result;\n-  __asm__ (\"fmla %0.2d,%2.2d,%3.2d\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n-vfma_n_f32 (float32x2_t a, float32x2_t b, float32_t c)\n-{\n-  float32x2_t result;\n-  __asm__ (\"fmla %0.2s, %2.2s, %3.s[0]\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n-vfmaq_n_f32 (float32x4_t a, float32x4_t b, float32_t c)\n-{\n-  float32x4_t result;\n-  __asm__ (\"fmla %0.4s, %2.4s, %3.s[0]\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n-vfmaq_n_f64 (float64x2_t a, float64x2_t b, float64_t c)\n-{\n-  float64x2_t result;\n-  __asm__ (\"fmla %0.2d, %2.2d, %3.d[0]\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n-vfms_f32 (float32x2_t a, float32x2_t b, float32x2_t c)\n-{\n-  float32x2_t result;\n-  __asm__ (\"fmls %0.2s,%2.2s,%3.2s\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n-vfmsq_f32 (float32x4_t a, float32x4_t b, float32x4_t c)\n-{\n-  float32x4_t result;\n-  __asm__ (\"fmls %0.4s,%2.4s,%3.4s\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n-vfmsq_f64 (float64x2_t a, float64x2_t b, float64x2_t c)\n-{\n-  float64x2_t result;\n-  __asm__ (\"fmls %0.2d,%2.2d,%3.2d\"\n-           : \"=w\"(result)\n-           : \"0\"(a), \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vhsub_s8 (int8x8_t a, int8x8_t b)\n-{\n-  int8x8_t result;\n-  __asm__ (\"shsub %0.8b, %1.8b, %2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vhsub_s16 (int16x4_t a, int16x4_t b)\n-{\n-  int16x4_t result;\n-  __asm__ (\"shsub %0.4h, %1.4h, %2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n-vhsub_s32 (int32x2_t a, int32x2_t b)\n-{\n-  int32x2_t result;\n-  __asm__ (\"shsub %0.2s, %1.2s, %2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vhsub_u8 (uint8x8_t a, uint8x8_t b)\n-{\n-  uint8x8_t result;\n-  __asm__ (\"uhsub %0.8b, %1.8b, %2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vhsub_u16 (uint16x4_t a, uint16x4_t b)\n-{\n-  uint16x4_t result;\n-  __asm__ (\"uhsub %0.4h, %1.4h, %2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vhsub_u32 (uint32x2_t a, uint32x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"uhsub %0.2s, %1.2s, %2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vhsubq_s8 (int8x16_t a, int8x16_t b)\n-{\n-  int8x16_t result;\n-  __asm__ (\"shsub %0.16b, %1.16b, %2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vhsubq_s16 (int16x8_t a, int16x8_t b)\n-{\n-  int16x8_t result;\n-  __asm__ (\"shsub %0.8h, %1.8h, %2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n-vhsubq_s32 (int32x4_t a, int32x4_t b)\n-{\n-  int32x4_t result;\n-  __asm__ (\"shsub %0.4s, %1.4s, %2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vhsubq_u8 (uint8x16_t a, uint8x16_t b)\n-{\n-  uint8x16_t result;\n-  __asm__ (\"uhsub %0.16b, %1.16b, %2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vhsubq_u16 (uint16x8_t a, uint16x8_t b)\n-{\n-  uint16x8_t result;\n-  __asm__ (\"uhsub %0.8h, %1.8h, %2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vhsubq_u32 (uint32x4_t a, uint32x4_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"uhsub %0.4s, %1.4s, %2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vmla_n_f32 (float32x2_t a, float32x2_t b, float32_t c)\n {\n@@ -9774,37 +9783,15 @@ vqrdmulhq_n_s32 (int32x4_t a, int32_t b)\n     ({                                                                  \\\n        int64x2_t b_ = (b);                                              \\\n        uint32x2_t a_ = (a);                                             \\\n-       uint32x4_t result = vcombine_u32                                 \\\n-                             (a_, vcreate_u32                           \\\n-                                    (__AARCH64_UINT64_C (0x0)));        \\\n-       __asm__ (\"sqshrun2 %0.4s, %1.2d, #%2\"                            \\\n-                : \"+w\"(result)                                          \\\n-                : \"w\"(b_), \"i\"(c)                                       \\\n-                : /* No clobbers */);                                   \\\n-       result;                                                          \\\n-     })\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vrecpe_u32 (uint32x2_t a)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"urecpe %0.2s,%1.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vrecpeq_u32 (uint32x4_t a)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"urecpe %0.4s,%1.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n+       uint32x4_t result = vcombine_u32                                 \\\n+                             (a_, vcreate_u32                           \\\n+                                    (__AARCH64_UINT64_C (0x0)));        \\\n+       __asm__ (\"sqshrun2 %0.4s, %1.2d, #%2\"                            \\\n+                : \"+w\"(result)                                          \\\n+                : \"w\"(b_), \"i\"(c)                                       \\\n+                : /* No clobbers */);                                   \\\n+       result;                                                          \\\n+     })\n \n #define vrshrn_high_n_s16(a, b, c)                                      \\\n   __extension__                                                         \\\n@@ -10111,138 +10098,6 @@ vrsqrtss_f32 (float32_t a, float32_t b)\n   return result;\n }\n \n-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vrsubhn_high_s16 (int8x8_t a, int16x8_t b, int16x8_t c)\n-{\n-  int8x16_t result = vcombine_s8 (a, vcreate_s8 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"rsubhn2 %0.16b, %1.8h, %2.8h\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vrsubhn_high_s32 (int16x4_t a, int32x4_t b, int32x4_t c)\n-{\n-  int16x8_t result = vcombine_s16 (a, vcreate_s16 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"rsubhn2 %0.8h, %1.4s, %2.4s\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n-vrsubhn_high_s64 (int32x2_t a, int64x2_t b, int64x2_t c)\n-{\n-  int32x4_t result = vcombine_s32 (a, vcreate_s32 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"rsubhn2 %0.4s, %1.2d, %2.2d\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vrsubhn_high_u16 (uint8x8_t a, uint16x8_t b, uint16x8_t c)\n-{\n-  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"rsubhn2 %0.16b, %1.8h, %2.8h\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vrsubhn_high_u32 (uint16x4_t a, uint32x4_t b, uint32x4_t c)\n-{\n-  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"rsubhn2 %0.8h, %1.4s, %2.4s\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vrsubhn_high_u64 (uint32x2_t a, uint64x2_t b, uint64x2_t c)\n-{\n-  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"rsubhn2 %0.4s, %1.2d, %2.2d\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vrsubhn_s16 (int16x8_t a, int16x8_t b)\n-{\n-  int8x8_t result;\n-  __asm__ (\"rsubhn %0.8b, %1.8h, %2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vrsubhn_s32 (int32x4_t a, int32x4_t b)\n-{\n-  int16x4_t result;\n-  __asm__ (\"rsubhn %0.4h, %1.4s, %2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n-vrsubhn_s64 (int64x2_t a, int64x2_t b)\n-{\n-  int32x2_t result;\n-  __asm__ (\"rsubhn %0.2s, %1.2d, %2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vrsubhn_u16 (uint16x8_t a, uint16x8_t b)\n-{\n-  uint8x8_t result;\n-  __asm__ (\"rsubhn %0.8b, %1.8h, %2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vrsubhn_u32 (uint32x4_t a, uint32x4_t b)\n-{\n-  uint16x4_t result;\n-  __asm__ (\"rsubhn %0.4h, %1.4s, %2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vrsubhn_u64 (uint64x2_t a, uint64x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"rsubhn %0.2s, %1.2d, %2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n #define vshrn_high_n_s16(a, b, c)                                       \\\n   __extension__                                                         \\\n     ({                                                                  \\\n@@ -10774,137 +10629,6 @@ vrsubhn_u64 (uint64x2_t a, uint64x2_t b)\n                 : \"memory\");                                            \\\n      })\n \n-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vsubhn_high_s16 (int8x8_t a, int16x8_t b, int16x8_t c)\n-{\n-  int8x16_t result = vcombine_s8 (a, vcreate_s8 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"subhn2 %0.16b, %1.8h, %2.8h\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vsubhn_high_s32 (int16x4_t a, int32x4_t b, int32x4_t c)\n-{\n-  int16x8_t result = vcombine_s16 (a, vcreate_s16 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"subhn2 %0.8h, %1.4s, %2.4s\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n-vsubhn_high_s64 (int32x2_t a, int64x2_t b, int64x2_t c)\n-{\n-  int32x4_t result = vcombine_s32 (a, vcreate_s32 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"subhn2 %0.4s, %1.2d, %2.2d\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vsubhn_high_u16 (uint8x8_t a, uint16x8_t b, uint16x8_t c)\n-{\n-  uint8x16_t result = vcombine_u8 (a, vcreate_u8 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"subhn2 %0.16b, %1.8h, %2.8h\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vsubhn_high_u32 (uint16x4_t a, uint32x4_t b, uint32x4_t c)\n-{\n-  uint16x8_t result = vcombine_u16 (a, vcreate_u16 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"subhn2 %0.8h, %1.4s, %2.4s\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vsubhn_high_u64 (uint32x2_t a, uint64x2_t b, uint64x2_t c)\n-{\n-  uint32x4_t result = vcombine_u32 (a, vcreate_u32 (__AARCH64_UINT64_C (0x0)));\n-  __asm__ (\"subhn2 %0.4s, %1.2d, %2.2d\"\n-           : \"+w\"(result)\n-           : \"w\"(b), \"w\"(c)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vsubhn_s16 (int16x8_t a, int16x8_t b)\n-{\n-  int8x8_t result;\n-  __asm__ (\"subhn %0.8b, %1.8h, %2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vsubhn_s32 (int32x4_t a, int32x4_t b)\n-{\n-  int16x4_t result;\n-  __asm__ (\"subhn %0.4h, %1.4s, %2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n-vsubhn_s64 (int64x2_t a, int64x2_t b)\n-{\n-  int32x2_t result;\n-  __asm__ (\"subhn %0.2s, %1.2d, %2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vsubhn_u16 (uint16x8_t a, uint16x8_t b)\n-{\n-  uint8x8_t result;\n-  __asm__ (\"subhn %0.8b, %1.8h, %2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vsubhn_u32 (uint32x4_t a, uint32x4_t b)\n-{\n-  uint16x4_t result;\n-  __asm__ (\"subhn %0.4h, %1.4s, %2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vsubhn_u64 (uint64x2_t a, uint64x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"subhn %0.2s, %1.2d, %2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vtst_p8 (poly8x8_t a, poly8x8_t b)\n@@ -15425,6 +15149,42 @@ vfma_f64 (float64x1_t __a, float64x1_t __b, float64x1_t __c)\n   return (float64x1_t) {__builtin_fma (__b[0], __c[0], __a[0])};\n }\n \n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vfma_f32 (float32x2_t __a, float32x2_t __b, float32x2_t __c)\n+{\n+  return __builtin_aarch64_fmav2sf (__b, __c, __a);\n+}\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vfmaq_f32 (float32x4_t __a, float32x4_t __b, float32x4_t __c)\n+{\n+  return __builtin_aarch64_fmav4sf (__b, __c, __a);\n+}\n+\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vfmaq_f64 (float64x2_t __a, float64x2_t __b, float64x2_t __c)\n+{\n+  return __builtin_aarch64_fmav2df (__b, __c, __a);\n+}\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vfma_n_f32 (float32x2_t __a, float32x2_t __b, float32_t __c)\n+{\n+  return __builtin_aarch64_fmav2sf (__b, vdup_n_f32 (__c), __a);\n+}\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vfmaq_n_f32 (float32x4_t __a, float32x4_t __b, float32_t __c)\n+{\n+  return __builtin_aarch64_fmav4sf (__b, vdupq_n_f32 (__c), __a);\n+}\n+\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vfmaq_n_f64 (float64x2_t __a, float64x2_t __b, float64_t __c)\n+{\n+  return __builtin_aarch64_fmav2df (__b, vdupq_n_f64 (__c), __a);\n+}\n+\n /* vfma_lane  */\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n@@ -15536,6 +15296,25 @@ vfms_f64 (float64x1_t __a, float64x1_t __b, float64x1_t __c)\n   return (float64x1_t) {__builtin_fma (-__b[0], __c[0], __a[0])};\n }\n \n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vfms_f32 (float32x2_t __a, float32x2_t __b, float32x2_t __c)\n+{\n+  return __builtin_aarch64_fmav2sf (-__b, __c, __a);\n+}\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vfmsq_f32 (float32x4_t __a, float32x4_t __b, float32x4_t __c)\n+{\n+  return __builtin_aarch64_fmav4sf (-__b, __c, __a);\n+}\n+\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vfmsq_f64 (float64x2_t __a, float64x2_t __b, float64x2_t __c)\n+{\n+  return __builtin_aarch64_fmav2df (-__b, __c, __a);\n+}\n+\n+\n /* vfms_lane  */\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n@@ -20966,6 +20745,18 @@ vrbitq_u8 (uint8x16_t __a)\n \n /* vrecpe  */\n \n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vrecpe_u32 (uint32x2_t __a)\n+{\n+  return (uint32x2_t) __builtin_aarch64_urecpev2si ((int32x2_t) __a);\n+}\n+ \n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vrecpeq_u32 (uint32x4_t __a)\n+{\n+  return (uint32x4_t) __builtin_aarch64_urecpev4si ((int32x4_t) __a);\n+}\n+\n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vrecpes_f32 (float32_t __a)\n {"}, {"sha": "16a2647cc60fbd13945ee73671d15453a56773fa", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -128,6 +128,9 @@\n ;; Vector modes except double int.\n (define_mode_iterator VDQIF [V8QI V16QI V4HI V8HI V2SI V4SI V2SF V4SF V2DF])\n \n+;; Vector modes for S type.\n+(define_mode_iterator VDQ_SI [V2SI V4SI])\n+\n ;; Vector modes for Q and H types.\n (define_mode_iterator VDQQH [V8QI V16QI V4HI V8HI])\n "}, {"sha": "11bfcb7580a01ba85e2640da6696a3e551b5c506", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -1,3 +1,14 @@\n+2014-12-08  Felix Yang  <felix.yang@huawei.com>\n+\t   Haijian Zhang  <z.zhanghaijian@huawei.com>\n+\t   Jiji Jiang  <jiangjiji@huawei.com>\n+\t   Pengfei Sui  <suipengfei@huawei.com>\n+\n+\t* gcc.target/aarch64/vfma.c: New test.\n+\t* gcc.target/aarch64/vfma_n.c: New test.\n+\t* gcc.target/aarch64/vfms.c: New test.\n+\t* gcc.target/aarch64/narrow_high-intrinsics.c: Fix expected assembler\n+\tfor rsubhn2 & subhn2.\n+\n 2014-12-08  Ilya Enkovich  <ilya.enkovich@intel.com>\n \n \t* gcc.target/i386/chkp-bndret.c: New."}, {"sha": "7ff482ce812043f272fadc46016262347bf2f448", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vfma.c", "status": "added", "additions": 67, "deletions": 0, "changes": 67, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvfma.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvfma.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvfma.c?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -0,0 +1,67 @@\n+#include <arm_neon.h>\n+#include \"arm-neon-ref.h\"\n+#include \"compute-ref-data.h\"\n+\n+/* Expected results.  */\n+VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x4438ca3d, 0x44390a3d };\n+VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x44869eb8, 0x4486beb8, 0x4486deb8, 0x4486feb8 };\n+VECT_VAR_DECL(expected,hfloat,64,2) [] = { 0x408906e1532b8520, 0x40890ee1532b8520 };\n+\n+#define TEST_MSG \"VFMA/VFMAQ\"\n+void exec_vfma (void)\n+{\n+  /* Basic test: v4=vfma(v1,v2), then store the result.  */\n+#define TEST_VFMA(Q, T1, T2, W, N)\t\t\t\t\t\\\n+  VECT_VAR(vector_res, T1, W, N) =\t\t\t\t\t\\\n+    vfma##Q##_##T2##W(VECT_VAR(vector1, T1, W, N),\t\t\t\\\n+\t\t      VECT_VAR(vector2, T1, W, N),\t\t\t\\\n+\t\t\t  VECT_VAR(vector3, T1, W, N));\t\t\t\\\n+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector_res, T1, W, N))\n+\n+#define CHECK_VFMA_RESULTS(test_name,comment)\t\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    CHECK_FP(test_name, float, 32, 2, PRIx32, expected, comment);\t\\\n+    CHECK_FP(test_name, float, 32, 4, PRIx32, expected, comment);\t\\\n+\tCHECK_FP(test_name, float, 64, 2, PRIx64, expected, comment);\t\\\n+  }\t\n+\n+#define DECL_VABD_VAR(VAR)\t\t\t\\\n+  DECL_VARIABLE(VAR, float, 32, 2);\t\t\\\n+  DECL_VARIABLE(VAR, float, 32, 4);\t\t\\\n+  DECL_VARIABLE(VAR, float, 64, 2);\t\t\n+\n+  DECL_VABD_VAR(vector1);\n+  DECL_VABD_VAR(vector2);\n+  DECL_VABD_VAR(vector3);\n+  DECL_VABD_VAR(vector_res);\n+\n+  clean_results ();\n+\n+  /* Initialize input \"vector1\" from \"buffer\".  */\n+  VLOAD(vector1, buffer, , float, f, 32, 2);\n+  VLOAD(vector1, buffer, q, float, f, 32, 4);\n+  VLOAD(vector1, buffer, q, float, f, 64, 2);\n+\n+  /* Choose init value arbitrarily.  */\n+  VDUP(vector2, , float, f, 32, 2, 9.3f);\n+  VDUP(vector2, q, float, f, 32, 4, 29.7f);\n+  VDUP(vector2, q, float, f, 64, 2, 15.8f);\n+  \n+  /* Choose init value arbitrarily.  */\n+  VDUP(vector3, , float, f, 32, 2, 81.2f);\n+  VDUP(vector3, q, float, f, 32, 4, 36.8f);\n+  VDUP(vector3, q, float, f, 64, 2, 51.7f);\n+\n+  /* Execute the tests.  */\n+  TEST_VFMA(, float, f, 32, 2);\n+  TEST_VFMA(q, float, f, 32, 4);\n+  TEST_VFMA(q, float, f, 64, 2);\n+\n+  CHECK_VFMA_RESULTS (TEST_MSG, \"\");\n+}\n+\n+int main (void)\n+{\n+  exec_vfma ();\n+  return 0;\n+}"}, {"sha": "d773f8b30763a9a2c0b916b3a8e270b1afd965e1", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vfma_n.c", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvfma_n.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvfma_n.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvfma_n.c?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -0,0 +1,69 @@\n+#include <arm_neon.h>\n+#include \"arm-neon-ref.h\"\n+#include \"compute-ref-data.h\"\n+\n+/* Expected results.  */\n+VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0x4438ca3d, 0x44390a3d };\n+VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0x44869eb8, 0x4486beb8, 0x4486deb8, 0x4486feb8 };\n+VECT_VAR_DECL(expected,hfloat,64,2) [] = { 0x408906e1532b8520, 0x40890ee1532b8520 };\n+\n+#define VECT_VAR_ASSIGN(S,Q,T1,W) S##Q##_##T1##W\n+#define ASSIGN(S, Q, T, W, V) T##W##_t S##Q##_##T##W = V\n+#define TEST_MSG \"VFMA/VFMAQ\"\n+void exec_vfma_n (void)\n+{\n+  /* Basic test: v4=vfma_n(v1,v2), then store the result.  */\n+#define TEST_VFMA(Q, T1, T2, W, N)\t\t\t\t\t\\\n+  VECT_VAR(vector_res, T1, W, N) =\t\t\t\t\t\\\n+    vfma##Q##_n_##T2##W(VECT_VAR(vector1, T1, W, N),\t\t\t\\\n+\t\t      VECT_VAR(vector2, T1, W, N),\t\t\t\\\n+\t\t\t  VECT_VAR_ASSIGN(Scalar, Q, T1, W));\t\t\t\\\n+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector_res, T1, W, N))\n+\n+#define CHECK_VFMA_RESULTS(test_name,comment)\t\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    CHECK_FP(test_name, float, 32, 2, PRIx32, expected, comment);\t\\\n+    CHECK_FP(test_name, float, 32, 4, PRIx32, expected, comment);\t\\\n+\tCHECK_FP(test_name, float, 64, 2, PRIx64, expected, comment);\t\\\n+  }\t\n+\n+#define DECL_VABD_VAR(VAR)\t\t\t\\\n+  DECL_VARIABLE(VAR, float, 32, 2);\t\t\\\n+  DECL_VARIABLE(VAR, float, 32, 4);\t\t\\\n+  DECL_VARIABLE(VAR, float, 64, 2);\t\t\n+\n+  DECL_VABD_VAR(vector1);\n+  DECL_VABD_VAR(vector2);\n+  DECL_VABD_VAR(vector3);\n+  DECL_VABD_VAR(vector_res);\n+\n+  clean_results ();\n+\n+  /* Initialize input \"vector1\" from \"buffer\".  */\n+  VLOAD(vector1, buffer, , float, f, 32, 2);\n+  VLOAD(vector1, buffer, q, float, f, 32, 4);\n+  VLOAD(vector1, buffer, q, float, f, 64, 2);\n+\n+  /* Choose init value arbitrarily.  */\n+  VDUP(vector2, , float, f, 32, 2, 9.3f);\n+  VDUP(vector2, q, float, f, 32, 4, 29.7f);\n+  VDUP(vector2, q, float, f, 64, 2, 15.8f);\n+  \n+  /* Choose init value arbitrarily.  */\n+  ASSIGN(Scalar, , float, 32, 81.2f);\n+  ASSIGN(Scalar, q, float, 32, 36.8f);\n+  ASSIGN(Scalar, q, float, 64, 51.7f);\n+\n+  /* Execute the tests.  */\n+  TEST_VFMA(, float, f, 32, 2);\n+  TEST_VFMA(q, float, f, 32, 4);\n+  TEST_VFMA(q, float, f, 64, 2);\n+\n+  CHECK_VFMA_RESULTS (TEST_MSG, \"\");\n+}\n+\n+int main (void)\n+{\n+  exec_vfma_n ();\n+  return 0;\n+}"}, {"sha": "f70e56a04b59976c111d6914c96beebe29707f0d", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vfms.c", "status": "added", "additions": 67, "deletions": 0, "changes": 67, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvfms.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvfms.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvfms.c?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -0,0 +1,67 @@\n+#include <arm_neon.h>\n+#include \"arm-neon-ref.h\"\n+#include \"compute-ref-data.h\"\n+\n+/* Expected results.  */\n+VECT_VAR_DECL(expected,hfloat,32,2) [] = { 0xc440ca3d, 0xc4408a3d };\n+VECT_VAR_DECL(expected,hfloat,32,4) [] = { 0xc48a9eb8, 0xc48a7eb8, 0xc48a5eb8, 0xc48a3eb8 };\n+VECT_VAR_DECL(expected,hfloat,64,2) [] = { 0xc08a06e1532b8520, 0xc089fee1532b8520 };\n+\n+#define TEST_MSG \"VFMA/VFMAQ\"\n+void exec_vfms (void)\n+{\n+  /* Basic test: v4=vfms(v1,v2), then store the result.  */\n+#define TEST_VFMA(Q, T1, T2, W, N)\t\t\t\t\t\\\n+  VECT_VAR(vector_res, T1, W, N) =\t\t\t\t\t\\\n+    vfms##Q##_##T2##W(VECT_VAR(vector1, T1, W, N),\t\t\t\\\n+\t\t      VECT_VAR(vector2, T1, W, N),\t\t\t\\\n+\t\t\t  VECT_VAR(vector3, T1, W, N));\t\t\t\\\n+  vst1##Q##_##T2##W(VECT_VAR(result, T1, W, N), VECT_VAR(vector_res, T1, W, N))\n+\n+#define CHECK_VFMA_RESULTS(test_name,comment)\t\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    CHECK_FP(test_name, float, 32, 2, PRIx32, expected, comment);\t\\\n+    CHECK_FP(test_name, float, 32, 4, PRIx32, expected, comment);\t\\\n+\tCHECK_FP(test_name, float, 64, 2, PRIx64, expected, comment);\t\\\n+  }\t\n+\n+#define DECL_VABD_VAR(VAR)\t\t\t\\\n+  DECL_VARIABLE(VAR, float, 32, 2);\t\t\\\n+  DECL_VARIABLE(VAR, float, 32, 4);\t\t\\\n+  DECL_VARIABLE(VAR, float, 64, 2);\t\t\n+\n+  DECL_VABD_VAR(vector1);\n+  DECL_VABD_VAR(vector2);\n+  DECL_VABD_VAR(vector3);\n+  DECL_VABD_VAR(vector_res);\n+\n+  clean_results ();\n+\n+  /* Initialize input \"vector1\" from \"buffer\".  */\n+  VLOAD(vector1, buffer, , float, f, 32, 2);\n+  VLOAD(vector1, buffer, q, float, f, 32, 4);\n+  VLOAD(vector1, buffer, q, float, f, 64, 2);\n+\n+  /* Choose init value arbitrarily.  */\n+  VDUP(vector2, , float, f, 32, 2, 9.3f);\n+  VDUP(vector2, q, float, f, 32, 4, 29.7f);\n+  VDUP(vector2, q, float, f, 64, 2, 15.8f);\n+  \n+  /* Choose init value arbitrarily.  */\n+  VDUP(vector3, , float, f, 32, 2, 81.2f);\n+  VDUP(vector3, q, float, f, 32, 4, 36.8f);\n+  VDUP(vector3, q, float, f, 64, 2, 51.7f);\n+\n+  /* Execute the tests.  */\n+  TEST_VFMA(, float, f, 32, 2);\n+  TEST_VFMA(q, float, f, 32, 4);\n+  TEST_VFMA(q, float, f, 64, 2);\n+\n+  CHECK_VFMA_RESULTS (TEST_MSG, \"\");\n+}\n+\n+int main (void)\n+{\n+  exec_vfms ();\n+  return 0;\n+}"}, {"sha": "8b8a6302692f47cf7601f45e5209c8cd5049ca86", "filename": "gcc/testsuite/gcc.target/aarch64/narrow_high-intrinsics.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fnarrow_high-intrinsics.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/58a3bd25ba668138bcc9ea314736736e08fa41a1/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fnarrow_high-intrinsics.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fnarrow_high-intrinsics.c?ref=58a3bd25ba668138bcc9ea314736736e08fa41a1", "patch": "@@ -107,9 +107,9 @@ ONE (vmovn_high, uint16x8_t, uint16x4_t, uint32x4_t, u32)\n ONE (vmovn_high, uint32x4_t, uint32x2_t, uint64x2_t, u64)\n \n \n-/* { dg-final { scan-assembler-times \"\\\\tsubhn2 v\" 6} }  */\n+/* { dg-final { scan-assembler-times \"\\\\tsubhn2\\\\tv\" 6} }  */\n /* { dg-final { scan-assembler-times \"\\\\taddhn2\\\\tv\" 6} }  */\n-/* { dg-final { scan-assembler-times \"rsubhn2 v\" 6} }  */\n+/* { dg-final { scan-assembler-times \"rsubhn2\\\\tv\" 6} }  */\n /* { dg-final { scan-assembler-times \"raddhn2\\\\tv\" 6} }  */\n /* { dg-final { scan-assembler-times \"\\\\trshrn2 v\" 6} }  */\n /* { dg-final { scan-assembler-times \"\\\\tshrn2 v\" 6} }  */"}]}