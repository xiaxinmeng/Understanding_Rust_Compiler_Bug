{"sha": "22527b59dbb10204626aeb8660a8e262377ac354", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjI1MjdiNTlkYmIxMDIwNDYyNmFlYjg2NjBhOGUyNjIzNzdhYzM1NA==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2017-08-30T11:16:42Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2017-08-30T11:16:42Z"}, "message": "[47/77] Make subroutines of nonzero_bits operate on scalar_int_mode\n\nnonzero_bits1 assumed that all bits of a floating-point or vector mode\nwere needed.  It seems likely that fixed-point modes should have been\nhandled in the same way.  After excluding those, the only remaining\nmodes that are likely to be useful are scalar integer ones.\n\nThis patch moves the mode class check to nonzero_bits itself, along\nwith the handling of mode == VOIDmode.  The subroutines of nonzero_bits\ncan then take scalar_int_modes.\n\n2017-08-30  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood  <david.sherwood@arm.com>\n\ngcc/\n\t* rtlanal.c (nonzero_bits): Handle VOIDmode here rather than\n\tin subroutines.  Return the mode mask for non-integer modes.\n\t(cached_nonzero_bits): Change the type of the mode parameter\n\tto scalar_int_mode.\n\t(nonzero_bits1): Likewise.  Remove early exit for other mode\n\tclasses.  Handle CONST_INT_P first and then check whether X\n\talso has a scalar integer mode.\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r251499", "tree": {"sha": "2ca4ddec1ff92a9f35161b220099f7c87ddd730f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/2ca4ddec1ff92a9f35161b220099f7c87ddd730f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/22527b59dbb10204626aeb8660a8e262377ac354", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/22527b59dbb10204626aeb8660a8e262377ac354", "html_url": "https://github.com/Rust-GCC/gccrs/commit/22527b59dbb10204626aeb8660a8e262377ac354", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/22527b59dbb10204626aeb8660a8e262377ac354/comments", "author": null, "committer": null, "parents": [{"sha": "b4e6c85e3cfdd4e0863277c09461e3709c42981d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b4e6c85e3cfdd4e0863277c09461e3709c42981d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b4e6c85e3cfdd4e0863277c09461e3709c42981d"}], "stats": {"total": 125, "additions": 69, "deletions": 56}, "files": [{"sha": "08982d847fc39fc70a4987f53758edd42d66b49d", "filename": "gcc/ChangeLog", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/22527b59dbb10204626aeb8660a8e262377ac354/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/22527b59dbb10204626aeb8660a8e262377ac354/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=22527b59dbb10204626aeb8660a8e262377ac354", "patch": "@@ -1,3 +1,15 @@\n+2017-08-30  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* rtlanal.c (nonzero_bits): Handle VOIDmode here rather than\n+\tin subroutines.  Return the mode mask for non-integer modes.\n+\t(cached_nonzero_bits): Change the type of the mode parameter\n+\tto scalar_int_mode.\n+\t(nonzero_bits1): Likewise.  Remove early exit for other mode\n+\tclasses.  Handle CONST_INT_P first and then check whether X\n+\talso has a scalar integer mode.\n+\n 2017-08-30  Richard Sandiford  <richard.sandiford@linaro.org>\n \t    Alan Hayward  <alan.hayward@arm.com>\n \t    David Sherwood  <david.sherwood@arm.com>"}, {"sha": "c8d802f3f341910694dbaf49622de50fa4e8c4cb", "filename": "gcc/rtlanal.c", "status": "modified", "additions": 57, "deletions": 56, "changes": 113, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/22527b59dbb10204626aeb8660a8e262377ac354/gcc%2Frtlanal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/22527b59dbb10204626aeb8660a8e262377ac354/gcc%2Frtlanal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtlanal.c?ref=22527b59dbb10204626aeb8660a8e262377ac354", "patch": "@@ -43,10 +43,10 @@ static bool covers_regno_no_parallel_p (const_rtx, unsigned int);\n static int computed_jump_p_1 (const_rtx);\n static void parms_set (rtx, const_rtx, void *);\n \n-static unsigned HOST_WIDE_INT cached_nonzero_bits (const_rtx, machine_mode,\n+static unsigned HOST_WIDE_INT cached_nonzero_bits (const_rtx, scalar_int_mode,\n                                                    const_rtx, machine_mode,\n                                                    unsigned HOST_WIDE_INT);\n-static unsigned HOST_WIDE_INT nonzero_bits1 (const_rtx, machine_mode,\n+static unsigned HOST_WIDE_INT nonzero_bits1 (const_rtx, scalar_int_mode,\n \t\t\t\t\t     const_rtx, machine_mode,\n                                              unsigned HOST_WIDE_INT);\n static unsigned int cached_num_sign_bit_copies (const_rtx, machine_mode, const_rtx,\n@@ -4237,7 +4237,12 @@ default_address_cost (rtx x, machine_mode, addr_space_t, bool speed)\n unsigned HOST_WIDE_INT\n nonzero_bits (const_rtx x, machine_mode mode)\n {\n-  return cached_nonzero_bits (x, mode, NULL_RTX, VOIDmode, 0);\n+  if (mode == VOIDmode)\n+    mode = GET_MODE (x);\n+  scalar_int_mode int_mode;\n+  if (!is_a <scalar_int_mode> (mode, &int_mode))\n+    return GET_MODE_MASK (mode);\n+  return cached_nonzero_bits (x, int_mode, NULL_RTX, VOIDmode, 0);\n }\n \n unsigned int\n@@ -4281,7 +4286,7 @@ nonzero_bits_binary_arith_p (const_rtx x)\n    identical subexpressions on the first or the second level.  */\n \n static unsigned HOST_WIDE_INT\n-cached_nonzero_bits (const_rtx x, machine_mode mode, const_rtx known_x,\n+cached_nonzero_bits (const_rtx x, scalar_int_mode mode, const_rtx known_x,\n \t\t     machine_mode known_mode,\n \t\t     unsigned HOST_WIDE_INT known_ret)\n {\n@@ -4334,27 +4339,39 @@ cached_nonzero_bits (const_rtx x, machine_mode mode, const_rtx known_x,\n    an arithmetic operation, we can do better.  */\n \n static unsigned HOST_WIDE_INT\n-nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n+nonzero_bits1 (const_rtx x, scalar_int_mode mode, const_rtx known_x,\n \t       machine_mode known_mode,\n \t       unsigned HOST_WIDE_INT known_ret)\n {\n   unsigned HOST_WIDE_INT nonzero = GET_MODE_MASK (mode);\n   unsigned HOST_WIDE_INT inner_nz;\n   enum rtx_code code;\n   machine_mode inner_mode;\n+  scalar_int_mode xmode;\n+\n   unsigned int mode_width = GET_MODE_PRECISION (mode);\n \n-  /* For floating-point and vector values, assume all bits are needed.  */\n-  if (FLOAT_MODE_P (GET_MODE (x)) || FLOAT_MODE_P (mode)\n-      || VECTOR_MODE_P (GET_MODE (x)) || VECTOR_MODE_P (mode))\n+  if (CONST_INT_P (x))\n+    {\n+      if (SHORT_IMMEDIATES_SIGN_EXTEND\n+\t  && INTVAL (x) > 0\n+\t  && mode_width < BITS_PER_WORD\n+\t  && (UINTVAL (x) & (HOST_WIDE_INT_1U << (mode_width - 1))) != 0)\n+\treturn UINTVAL (x) | (HOST_WIDE_INT_M1U << mode_width);\n+\n+      return UINTVAL (x);\n+    }\n+\n+  if (!is_a <scalar_int_mode> (GET_MODE (x), &xmode))\n     return nonzero;\n+  unsigned int xmode_width = GET_MODE_PRECISION (xmode);\n \n   /* If X is wider than MODE, use its mode instead.  */\n-  if (GET_MODE_PRECISION (GET_MODE (x)) > mode_width)\n+  if (xmode_width > mode_width)\n     {\n-      mode = GET_MODE (x);\n+      mode = xmode;\n       nonzero = GET_MODE_MASK (mode);\n-      mode_width = GET_MODE_PRECISION (mode);\n+      mode_width = xmode_width;\n     }\n \n   if (mode_width > HOST_BITS_PER_WIDE_INT)\n@@ -4370,15 +4387,13 @@ nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n      not known to be zero.  */\n \n   if (!WORD_REGISTER_OPERATIONS\n-      && GET_MODE (x) != VOIDmode\n-      && GET_MODE (x) != mode\n-      && GET_MODE_PRECISION (GET_MODE (x)) <= BITS_PER_WORD\n-      && GET_MODE_PRECISION (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT\n-      && GET_MODE_PRECISION (mode) > GET_MODE_PRECISION (GET_MODE (x)))\n+      && mode_width > xmode_width\n+      && xmode_width <= BITS_PER_WORD\n+      && xmode_width <= HOST_BITS_PER_WIDE_INT)\n     {\n-      nonzero &= cached_nonzero_bits (x, GET_MODE (x),\n+      nonzero &= cached_nonzero_bits (x, xmode,\n \t\t\t\t      known_x, known_mode, known_ret);\n-      nonzero |= GET_MODE_MASK (mode) & ~GET_MODE_MASK (GET_MODE (x));\n+      nonzero |= GET_MODE_MASK (mode) & ~GET_MODE_MASK (xmode);\n       return nonzero;\n     }\n \n@@ -4395,7 +4410,8 @@ nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n \t we can do this only if the target does not support different pointer\n \t or address modes depending on the address space.  */\n       if (target_default_pointer_address_modes_p ()\n-\t  && POINTERS_EXTEND_UNSIGNED && GET_MODE (x) == Pmode\n+\t  && POINTERS_EXTEND_UNSIGNED\n+\t  && xmode == Pmode\n \t  && REG_POINTER (x)\n \t  && !targetm.have_ptr_extend ())\n \tnonzero &= GET_MODE_MASK (ptr_mode);\n@@ -4438,22 +4454,12 @@ nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n \treturn nonzero_for_hook;\n       }\n \n-    case CONST_INT:\n-      /* If X is negative in MODE, sign-extend the value.  */\n-      if (SHORT_IMMEDIATES_SIGN_EXTEND && INTVAL (x) > 0\n-\t  && mode_width < BITS_PER_WORD\n-\t  && (UINTVAL (x) & (HOST_WIDE_INT_1U << (mode_width - 1)))\n-\t     != 0)\n-\treturn UINTVAL (x) | (HOST_WIDE_INT_M1U << mode_width);\n-\n-      return UINTVAL (x);\n-\n     case MEM:\n       /* In many, if not most, RISC machines, reading a byte from memory\n \t zeros the rest of the register.  Noticing that fact saves a lot\n \t of extra zero-extends.  */\n-      if (load_extend_op (GET_MODE (x)) == ZERO_EXTEND)\n-\tnonzero &= GET_MODE_MASK (GET_MODE (x));\n+      if (load_extend_op (xmode) == ZERO_EXTEND)\n+\tnonzero &= GET_MODE_MASK (xmode);\n       break;\n \n     case EQ:  case NE:\n@@ -4470,7 +4476,7 @@ nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n \t operation in, and not the actual operation mode.  We can wind\n \t up with (subreg:DI (gt:V4HI x y)), and we don't have anything\n \t that describes the results of a vector compare.  */\n-      if (GET_MODE_CLASS (GET_MODE (x)) == MODE_INT\n+      if (GET_MODE_CLASS (xmode) == MODE_INT\n \t  && mode_width <= HOST_BITS_PER_WIDE_INT)\n \tnonzero = STORE_FLAG_VALUE;\n       break;\n@@ -4479,21 +4485,19 @@ nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n #if 0\n       /* Disabled to avoid exponential mutual recursion between nonzero_bits\n \t and num_sign_bit_copies.  */\n-      if (num_sign_bit_copies (XEXP (x, 0), GET_MODE (x))\n-\t  == GET_MODE_PRECISION (GET_MODE (x)))\n+      if (num_sign_bit_copies (XEXP (x, 0), xmode) == xmode_width)\n \tnonzero = 1;\n #endif\n \n-      if (GET_MODE_PRECISION (GET_MODE (x)) < mode_width)\n-\tnonzero |= (GET_MODE_MASK (mode) & ~GET_MODE_MASK (GET_MODE (x)));\n+      if (xmode_width < mode_width)\n+\tnonzero |= (GET_MODE_MASK (mode) & ~GET_MODE_MASK (xmode));\n       break;\n \n     case ABS:\n #if 0\n       /* Disabled to avoid exponential mutual recursion between nonzero_bits\n \t and num_sign_bit_copies.  */\n-      if (num_sign_bit_copies (XEXP (x, 0), GET_MODE (x))\n-\t  == GET_MODE_PRECISION (GET_MODE (x)))\n+      if (num_sign_bit_copies (XEXP (x, 0), xmode) == xmode_width)\n \tnonzero = 1;\n #endif\n       break;\n@@ -4566,7 +4570,7 @@ nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n \tunsigned HOST_WIDE_INT nz1\n \t  = cached_nonzero_bits (XEXP (x, 1), mode,\n \t\t\t\t known_x, known_mode, known_ret);\n-\tint sign_index = GET_MODE_PRECISION (GET_MODE (x)) - 1;\n+\tint sign_index = xmode_width - 1;\n \tint width0 = floor_log2 (nz0) + 1;\n \tint width1 = floor_log2 (nz1) + 1;\n \tint low0 = ctz_or_zero (nz0);\n@@ -4638,8 +4642,8 @@ nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n \t been zero-extended, we know that at least the high-order bits\n \t are zero, though others might be too.  */\n       if (SUBREG_PROMOTED_VAR_P (x) && SUBREG_PROMOTED_UNSIGNED_P (x))\n-\tnonzero = GET_MODE_MASK (GET_MODE (x))\n-\t\t  & cached_nonzero_bits (SUBREG_REG (x), GET_MODE (x),\n+\tnonzero = GET_MODE_MASK (xmode)\n+\t\t  & cached_nonzero_bits (SUBREG_REG (x), xmode,\n \t\t\t\t\t known_x, known_mode, known_ret);\n \n       /* If the inner mode is a single word for both the host and target\n@@ -4663,10 +4667,8 @@ nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n \t\t   ? val_signbit_known_set_p (inner_mode, nonzero)\n \t\t   : extend_op != ZERO_EXTEND)\n \t       || (!MEM_P (SUBREG_REG (x)) && !REG_P (SUBREG_REG (x))))\n-\t      && GET_MODE_PRECISION (GET_MODE (x))\n-\t\t  > GET_MODE_PRECISION (inner_mode))\n-\t    nonzero\n-\t      |= (GET_MODE_MASK (GET_MODE (x)) & ~GET_MODE_MASK (inner_mode));\n+\t      && xmode_width > GET_MODE_PRECISION (inner_mode))\n+\t    nonzero |= (GET_MODE_MASK (xmode) & ~GET_MODE_MASK (inner_mode));\n \t}\n       break;\n \n@@ -4675,27 +4677,25 @@ nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n     case ASHIFT:\n     case ROTATE:\n       /* The nonzero bits are in two classes: any bits within MODE\n-\t that aren't in GET_MODE (x) are always significant.  The rest of the\n+\t that aren't in xmode are always significant.  The rest of the\n \t nonzero bits are those that are significant in the operand of\n \t the shift when shifted the appropriate number of bits.  This\n \t shows that high-order bits are cleared by the right shift and\n \t low-order bits by left shifts.  */\n       if (CONST_INT_P (XEXP (x, 1))\n \t  && INTVAL (XEXP (x, 1)) >= 0\n \t  && INTVAL (XEXP (x, 1)) < HOST_BITS_PER_WIDE_INT\n-\t  && INTVAL (XEXP (x, 1)) < GET_MODE_PRECISION (GET_MODE (x)))\n+\t  && INTVAL (XEXP (x, 1)) < xmode_width)\n \t{\n-\t  machine_mode inner_mode = GET_MODE (x);\n-\t  unsigned int width = GET_MODE_PRECISION (inner_mode);\n \t  int count = INTVAL (XEXP (x, 1));\n-\t  unsigned HOST_WIDE_INT mode_mask = GET_MODE_MASK (inner_mode);\n+\t  unsigned HOST_WIDE_INT mode_mask = GET_MODE_MASK (xmode);\n \t  unsigned HOST_WIDE_INT op_nonzero\n \t    = cached_nonzero_bits (XEXP (x, 0), mode,\n \t\t\t\t   known_x, known_mode, known_ret);\n \t  unsigned HOST_WIDE_INT inner = op_nonzero & mode_mask;\n \t  unsigned HOST_WIDE_INT outer = 0;\n \n-\t  if (mode_width > width)\n+\t  if (mode_width > xmode_width)\n \t    outer = (op_nonzero & nonzero & ~mode_mask);\n \n \t  if (code == LSHIFTRT)\n@@ -4707,15 +4707,16 @@ nonzero_bits1 (const_rtx x, machine_mode mode, const_rtx known_x,\n \t      /* If the sign bit may have been nonzero before the shift, we\n \t\t need to mark all the places it could have been copied to\n \t\t by the shift as possibly nonzero.  */\n-\t      if (inner & (HOST_WIDE_INT_1U << (width - 1 - count)))\n-\t\tinner |= ((HOST_WIDE_INT_1U << count) - 1)\n-\t\t\t   << (width - count);\n+\t      if (inner & (HOST_WIDE_INT_1U << (xmode_width - 1 - count)))\n+\t\tinner |= (((HOST_WIDE_INT_1U << count) - 1)\n+\t\t\t  << (xmode_width - count));\n \t    }\n \t  else if (code == ASHIFT)\n \t    inner <<= count;\n \t  else\n-\t    inner = ((inner << (count % width)\n-\t\t      | (inner >> (width - (count % width)))) & mode_mask);\n+\t    inner = ((inner << (count % xmode_width)\n+\t\t      | (inner >> (xmode_width - (count % xmode_width))))\n+\t\t     & mode_mask);\n \n \t  nonzero &= (outer | inner);\n \t}"}]}