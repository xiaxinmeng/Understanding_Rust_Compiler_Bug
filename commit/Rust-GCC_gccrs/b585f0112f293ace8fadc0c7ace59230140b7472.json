{"sha": "b585f0112f293ace8fadc0c7ace59230140b7472", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YjU4NWYwMTEyZjI5M2FjZThmYWRjMGM3YWNlNTkyMzAxNDBiNzQ3Mg==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-08-03T12:00:45Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-08-03T12:00:45Z"}, "message": "aarch64: Split out aarch64_adjust_body_cost_sve\n\nThis patch splits the SVE-specific part of aarch64_adjust_body_cost\nout into its own subroutine, so that a future patch can call it\nmore than once.  I wondered about using a lambda to avoid having\nto pass all the arguments, but in the end this way seemed clearer.\n\ngcc/\n\t* config/aarch64/aarch64.c (aarch64_adjust_body_cost_sve): New\n\tfunction, split out from...\n\t(aarch64_adjust_body_cost): ...here.", "tree": {"sha": "e9deaad93d7986257ddbe78346bbe258ee4582ce", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/e9deaad93d7986257ddbe78346bbe258ee4582ce"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/b585f0112f293ace8fadc0c7ace59230140b7472", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b585f0112f293ace8fadc0c7ace59230140b7472", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b585f0112f293ace8fadc0c7ace59230140b7472", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b585f0112f293ace8fadc0c7ace59230140b7472/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "83d796d3e58badcb864d179b882979f714ffd162", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/83d796d3e58badcb864d179b882979f714ffd162", "html_url": "https://github.com/Rust-GCC/gccrs/commit/83d796d3e58badcb864d179b882979f714ffd162"}], "stats": {"total": 220, "additions": 127, "deletions": 93}, "files": [{"sha": "b14b6f22aec61e8af158c7a48ad06796424d98f6", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 127, "deletions": 93, "changes": 220, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b585f0112f293ace8fadc0c7ace59230140b7472/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b585f0112f293ace8fadc0c7ace59230140b7472/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=b585f0112f293ace8fadc0c7ace59230140b7472", "patch": "@@ -15488,6 +15488,126 @@ aarch64_estimate_min_cycles_per_iter\n   return cycles;\n }\n \n+/* Subroutine of aarch64_adjust_body_cost for handling SVE.\n+   Use ISSUE_INFO to work out how fast the SVE code can be issued and compare\n+   it to the equivalent value for scalar code (SCALAR_CYCLES_PER_ITER).\n+   If COULD_USE_ADVSIMD is true, also compare it to the issue rate of\n+   Advanced SIMD code (ADVSIMD_CYCLES_PER_ITER).\n+\n+   COSTS is as for aarch64_adjust_body_cost.  ORIG_BODY_COST is the cost\n+   originally passed to aarch64_adjust_body_cost and *BODY_COST is the current\n+   value of the adjusted cost.  *SHOULD_DISPARAGE is true if we think the loop\n+   body is too expensive.  */\n+\n+static fractional_cost\n+aarch64_adjust_body_cost_sve (const aarch64_vector_costs *costs,\n+\t\t\t      const aarch64_vec_issue_info *issue_info,\n+\t\t\t      fractional_cost scalar_cycles_per_iter,\n+\t\t\t      fractional_cost advsimd_cycles_per_iter,\n+\t\t\t      bool could_use_advsimd,\n+\t\t\t      unsigned int orig_body_cost,\n+\t\t\t      unsigned int *body_cost,\n+\t\t\t      bool *should_disparage)\n+{\n+  /* Estimate the minimum number of cycles per iteration needed to issue\n+     non-predicate operations.  */\n+  fractional_cost sve_nonpred_cycles_per_iter\n+    = aarch64_estimate_min_cycles_per_iter (&costs->sve_ops,\n+\t\t\t\t\t    issue_info->sve);\n+\n+  /* Separately estimate the minimum number of cycles per iteration needed\n+     to issue the predicate operations.  */\n+  fractional_cost sve_pred_issue_cycles_per_iter\n+    = { costs->sve_ops.pred_ops, issue_info->sve->pred_ops_per_cycle };\n+\n+  /* Calculate the overall limit on the number of cycles per iteration.  */\n+  fractional_cost sve_cycles_per_iter\n+    = std::max (sve_nonpred_cycles_per_iter, sve_pred_issue_cycles_per_iter);\n+\n+  if (dump_enabled_p ())\n+    {\n+      costs->sve_ops.dump ();\n+      dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t       \"  estimated cycles per iteration = %f\\n\",\n+\t\t       sve_cycles_per_iter.as_double ());\n+      dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t       \"  estimated cycles per iteration for non-predicate\"\n+\t\t       \" operations = %f\\n\",\n+\t\t       sve_nonpred_cycles_per_iter.as_double ());\n+      if (costs->sve_ops.pred_ops)\n+\tdump_printf_loc (MSG_NOTE, vect_location, \"  estimated cycles per\"\n+\t\t\t \" iteration for predicate operations = %d\\n\",\n+\t\t\t sve_pred_issue_cycles_per_iter.as_double ());\n+    }\n+\n+  /* If the scalar version of the loop could issue at least as\n+     quickly as the predicate parts of the SVE loop, make the SVE loop\n+     prohibitively expensive.  In this case vectorization is adding an\n+     overhead that the original scalar code didn't have.\n+\n+     This is mostly intended to detect cases in which WHILELOs dominate\n+     for very tight loops, which is something that normal latency-based\n+     costs would not model.  Adding this kind of cliffedge would be\n+     too drastic for scalar_cycles_per_iter vs. sve_cycles_per_iter;\n+     code in the caller handles that case in a more conservative way.  */\n+  fractional_cost sve_estimate = sve_pred_issue_cycles_per_iter + 1;\n+  if (scalar_cycles_per_iter < sve_estimate)\n+    {\n+      unsigned int min_cost\n+\t= orig_body_cost * estimated_poly_value (BYTES_PER_SVE_VECTOR);\n+      if (*body_cost < min_cost)\n+\t{\n+\t  if (dump_enabled_p ())\n+\t    dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t     \"Increasing body cost to %d because the\"\n+\t\t\t     \" scalar code could issue within the limit\"\n+\t\t\t     \" imposed by predicate operations\\n\",\n+\t\t\t     min_cost);\n+\t  *body_cost = min_cost;\n+\t  *should_disparage = true;\n+\t}\n+    }\n+\n+  /* If it appears that the Advanced SIMD version of a loop could issue\n+     more quickly than the SVE one, increase the SVE cost in proportion\n+     to the difference.  The intention is to make Advanced SIMD preferable\n+     in cases where an Advanced SIMD version exists, without increasing\n+     the costs so much that SVE won't be used at all.\n+\n+     The reasoning is similar to the scalar vs. predicate comparison above:\n+     if the issue rate of the SVE code is limited by predicate operations\n+     (i.e. if sve_pred_issue_cycles_per_iter > sve_nonpred_cycles_per_iter),\n+     and if the Advanced SIMD code could issue within the limit imposed\n+     by the predicate operations, the predicate operations are adding an\n+     overhead that the original code didn't have and so we should prefer\n+     the Advanced SIMD version.  However, if the predicate operations\n+     do not dominate in this way, we should only increase the cost of\n+     the SVE code if sve_cycles_per_iter is strictly greater than\n+     advsimd_cycles_per_iter.  Given rounding effects, this should mean\n+     that Advanced SIMD is either better or at least no worse.  */\n+  if (sve_nonpred_cycles_per_iter >= sve_pred_issue_cycles_per_iter)\n+    sve_estimate = sve_cycles_per_iter;\n+  if (could_use_advsimd && advsimd_cycles_per_iter < sve_estimate)\n+    {\n+      /* This ensures that min_cost > orig_body_cost * 2.  */\n+      unsigned int factor = fractional_cost::scale (1, sve_estimate,\n+\t\t\t\t\t\t    advsimd_cycles_per_iter);\n+      unsigned int min_cost = orig_body_cost * factor + 1;\n+      if (*body_cost < min_cost)\n+\t{\n+\t  if (dump_enabled_p ())\n+\t    dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t     \"Increasing body cost to %d because Advanced\"\n+\t\t\t     \" SIMD code could issue as quickly\\n\",\n+\t\t\t     min_cost);\n+\t  *body_cost = min_cost;\n+\t  *should_disparage = true;\n+\t}\n+    }\n+\n+  return sve_cycles_per_iter;\n+}\n+\n /* BODY_COST is the cost of a vector loop body recorded in COSTS.\n    Adjust the cost as necessary and return the new cost.  */\n static unsigned int\n@@ -15583,101 +15703,15 @@ aarch64_adjust_body_cost (aarch64_vector_costs *costs, unsigned int body_cost)\n \n   if ((costs->vec_flags & VEC_ANY_SVE) && issue_info->sve)\n     {\n-      /* Estimate the minimum number of cycles per iteration needed to issue\n-\t non-predicate operations.  */\n-      fractional_cost sve_cycles_per_iter\n-\t= aarch64_estimate_min_cycles_per_iter (&costs->sve_ops,\n-\t\t\t\t\t\tissue_info->sve);\n-\n-      /* Separately estimate the minimum number of cycles per iteration needed\n-\t to issue the predicate operations.  */\n-      fractional_cost pred_cycles_per_iter\n-\t= { costs->sve_ops.pred_ops, issue_info->sve->pred_ops_per_cycle };\n-\n       if (dump_enabled_p ())\n-\t{\n-\t  dump_printf_loc (MSG_NOTE, vect_location, \"SVE issue estimate:\\n\");\n-\t  costs->sve_ops.dump ();\n-\t  dump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t   \"  estimated cycles per iteration for non-predicate\"\n-\t\t\t   \" operations = %f\\n\",\n-\t\t\t   sve_cycles_per_iter.as_double ());\n-\t  if (costs->sve_ops.pred_ops)\n-\t    dump_printf_loc (MSG_NOTE, vect_location, \"  estimated cycles per\"\n-\t\t\t     \" iteration for predicate operations = %d\\n\",\n-\t\t\t     pred_cycles_per_iter.as_double ());\n-\t}\n-\n-      vector_cycles_per_iter = std::max (sve_cycles_per_iter,\n-\t\t\t\t\t pred_cycles_per_iter);\n+\tdump_printf_loc (MSG_NOTE, vect_location, \"SVE issue estimate:\\n\");\n       vector_reduction_latency = costs->sve_ops.reduction_latency;\n-\n-      /* If the scalar version of the loop could issue at least as\n-\t quickly as the predicate parts of the SVE loop, make the SVE loop\n-\t prohibitively expensive.  In this case vectorization is adding an\n-\t overhead that the original scalar code didn't have.\n-\n-\t This is mostly intended to detect cases in which WHILELOs dominate\n-\t for very tight loops, which is something that normal latency-based\n-\t costs would not model.  Adding this kind of cliffedge would be\n-\t too drastic for scalar_cycles_per_iter vs. sve_cycles_per_iter;\n-\t code later in the function handles that case in a more\n-\t conservative way.  */\n-      fractional_cost sve_estimate = pred_cycles_per_iter + 1;\n-      if (scalar_cycles_per_iter < sve_estimate)\n-\t{\n-\t  unsigned int min_cost\n-\t    = orig_body_cost * estimated_poly_value (BYTES_PER_SVE_VECTOR);\n-\t  if (body_cost < min_cost)\n-\t    {\n-\t      if (dump_enabled_p ())\n-\t\tdump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t\t \"Increasing body cost to %d because the\"\n-\t\t\t\t \" scalar code could issue within the limit\"\n-\t\t\t\t \" imposed by predicate operations\\n\",\n-\t\t\t\t min_cost);\n-\t      body_cost = min_cost;\n-\t      should_disparage = true;\n-\t    }\n-\t}\n-\n-      /* If it appears that the Advanced SIMD version of a loop could issue\n-\t more quickly than the SVE one, increase the SVE cost in proportion\n-\t to the difference.  The intention is to make Advanced SIMD preferable\n-\t in cases where an Advanced SIMD version exists, without increasing\n-\t the costs so much that SVE won't be used at all.\n-\n-\t The reasoning is similar to the scalar vs. predicate comparison above:\n-\t if the issue rate of the SVE code is limited by predicate operations\n-\t (i.e. if pred_cycles_per_iter > sve_cycles_per_iter), and if the\n-\t Advanced SIMD code could issue within the limit imposed by the\n-\t predicate operations, the predicate operations are adding an\n-\t overhead that the original code didn't have and so we should prefer\n-\t the Advanced SIMD version.  However, if the predicate operations\n-\t do not dominate in this way, we should only increase the cost of\n-\t the SVE code if sve_cycles_per_iter is strictly greater than\n-\t advsimd_cycles_per_iter.  Given rounding effects, this should mean\n-\t that Advanced SIMD is either better or at least no worse.  */\n-      if (sve_cycles_per_iter >= pred_cycles_per_iter)\n-\tsve_estimate = sve_cycles_per_iter;\n-      if (could_use_advsimd && advsimd_cycles_per_iter < sve_estimate)\n-\t{\n-\t  /* This ensures that min_cost > orig_body_cost * 2.  */\n-\t  unsigned int factor\n-\t    = fractional_cost::scale (1, sve_estimate,\n-\t\t\t\t      advsimd_cycles_per_iter);\n-\t  unsigned int min_cost = orig_body_cost * factor + 1;\n-\t  if (body_cost < min_cost)\n-\t    {\n-\t      if (dump_enabled_p ())\n-\t\tdump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t\t \"Increasing body cost to %d because Advanced\"\n-\t\t\t\t \" SIMD code could issue as quickly\\n\",\n-\t\t\t\t min_cost);\n-\t      body_cost = min_cost;\n-\t      should_disparage = true;\n-\t    }\n-\t}\n+      vector_cycles_per_iter\n+\t= aarch64_adjust_body_cost_sve (costs, issue_info,\n+\t\t\t\t\tscalar_cycles_per_iter,\n+\t\t\t\t\tadvsimd_cycles_per_iter,\n+\t\t\t\t\tcould_use_advsimd, orig_body_cost,\n+\t\t\t\t\t&body_cost, &should_disparage);\n     }\n \n   /* Decide whether to stick to latency-based costs or whether to try to"}]}