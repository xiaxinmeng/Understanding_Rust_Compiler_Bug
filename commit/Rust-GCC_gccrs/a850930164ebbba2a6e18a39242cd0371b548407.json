{"sha": "a850930164ebbba2a6e18a39242cd0371b548407", "node_id": "C_kwDOANBUbNoAKGE4NTA5MzAxNjRlYmJiYTJhNmUxOGEzOTI0MmNkMDM3MWI1NDg0MDc", "commit": {"author": {"name": "Andre Vieira", "email": "andre.simoesdiasvieira@arm.com", "date": "2022-03-22T11:28:03Z"}, "committer": {"name": "Andre Vieira", "email": "andre.simoesdiasvieira@arm.com", "date": "2022-03-22T11:56:43Z"}, "message": "aarch64: Add Neoverse-N2 tuning structs\n\nThis patch adds tuning structures for Neoverse N2.\n\n2022-03-22  Tamar Christina  <tamar.christina@arm.com>\n\t    Andre Vieira  <andre.simoesdiasvieira@arm.com>\n\n\t* config/aarch64/aarch64.cc (neoversen2_addrcost_table,\n\tneoversen2_regmove_cost, neoversen2_advsimd_vector_cost,\n\tneoversen2_sve_vector_cost, neoversen2_scalar_issue_info,\n\tneoversen2_advsimd_issue_info, neoversen2_sve_issue_info,\n\tneoversen2_vec_issue_info, neoversen2_tunings): New structs.\n\t(neoversen2_tunings): Use new structs and update tuning flags.\n\t(aarch64_vec_op_count::rename_cycles_per_iter): Enable for neoversen2\n\ttuning.", "tree": {"sha": "704d3148faabb2f56153dc3501f63a3df7db2de4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/704d3148faabb2f56153dc3501f63a3df7db2de4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a850930164ebbba2a6e18a39242cd0371b548407", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a850930164ebbba2a6e18a39242cd0371b548407", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a850930164ebbba2a6e18a39242cd0371b548407", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a850930164ebbba2a6e18a39242cd0371b548407/comments", "author": {"login": "avieira-arm", "id": 68072104, "node_id": "MDQ6VXNlcjY4MDcyMTA0", "avatar_url": "https://avatars.githubusercontent.com/u/68072104?v=4", "gravatar_id": "", "url": "https://api.github.com/users/avieira-arm", "html_url": "https://github.com/avieira-arm", "followers_url": "https://api.github.com/users/avieira-arm/followers", "following_url": "https://api.github.com/users/avieira-arm/following{/other_user}", "gists_url": "https://api.github.com/users/avieira-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/avieira-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/avieira-arm/subscriptions", "organizations_url": "https://api.github.com/users/avieira-arm/orgs", "repos_url": "https://api.github.com/users/avieira-arm/repos", "events_url": "https://api.github.com/users/avieira-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/avieira-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "avieira-arm", "id": 68072104, "node_id": "MDQ6VXNlcjY4MDcyMTA0", "avatar_url": "https://avatars.githubusercontent.com/u/68072104?v=4", "gravatar_id": "", "url": "https://api.github.com/users/avieira-arm", "html_url": "https://github.com/avieira-arm", "followers_url": "https://api.github.com/users/avieira-arm/followers", "following_url": "https://api.github.com/users/avieira-arm/following{/other_user}", "gists_url": "https://api.github.com/users/avieira-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/avieira-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/avieira-arm/subscriptions", "organizations_url": "https://api.github.com/users/avieira-arm/orgs", "repos_url": "https://api.github.com/users/avieira-arm/repos", "events_url": "https://api.github.com/users/avieira-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/avieira-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "0bae246acc758d4b11dd575b05207fd69169109b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0bae246acc758d4b11dd575b05207fd69169109b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0bae246acc758d4b11dd575b05207fd69169109b"}], "stats": {"total": 196, "additions": 191, "deletions": 5}, "files": [{"sha": "c82b5a695a97cfccaa1e73f1f81d777d16e43816", "filename": "gcc/config/aarch64/aarch64.cc", "status": "modified", "additions": 191, "deletions": 5, "changes": 196, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a850930164ebbba2a6e18a39242cd0371b548407/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a850930164ebbba2a6e18a39242cd0371b548407/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.cc?ref=a850930164ebbba2a6e18a39242cd0371b548407", "patch": "@@ -519,6 +519,24 @@ static const struct cpu_addrcost_table neoversev1_addrcost_table =\n   0 /* imm_offset  */\n };\n \n+static const struct cpu_addrcost_table neoversen2_addrcost_table =\n+{\n+    {\n+      1, /* hi  */\n+      0, /* si  */\n+      0, /* di  */\n+      1, /* ti  */\n+    },\n+  0, /* pre_modify  */\n+  0, /* post_modify  */\n+  2, /* post_modify_ld3_st3  */\n+  2, /* post_modify_ld4_st4  */\n+  0, /* register_offset  */\n+  0, /* register_sextend  */\n+  0, /* register_zextend  */\n+  0 /* imm_offset  */\n+};\n+\n static const struct cpu_regmove_cost generic_regmove_cost =\n {\n   1, /* GP2GP  */\n@@ -624,6 +642,16 @@ static const struct cpu_regmove_cost a64fx_regmove_cost =\n   2 /* FP2FP  */\n };\n \n+static const struct cpu_regmove_cost neoversen2_regmove_cost =\n+{\n+  1, /* GP2GP  */\n+  /* Spilling to int<->fp instead of memory is recommended so set\n+     realistic costs compared to memmov_cost.  */\n+  3, /* GP2FP  */\n+  2, /* FP2GP  */\n+  2 /* FP2FP  */\n+};\n+\n /* Generic costs for Advanced SIMD vector operations.   */\n static const advsimd_vec_cost generic_advsimd_vector_cost =\n {\n@@ -2054,12 +2082,166 @@ static const struct tune_params neoverse512tvb_tunings =\n   &generic_prefetch_tune\n };\n \n+static const advsimd_vec_cost neoversen2_advsimd_vector_cost =\n+{\n+  2, /* int_stmt_cost  */\n+  2, /* fp_stmt_cost  */\n+  2, /* ld2_st2_permute_cost */\n+  2, /* ld3_st3_permute_cost  */\n+  3, /* ld4_st4_permute_cost  */\n+  3, /* permute_cost  */\n+  4, /* reduc_i8_cost  */\n+  4, /* reduc_i16_cost  */\n+  2, /* reduc_i32_cost  */\n+  2, /* reduc_i64_cost  */\n+  6, /* reduc_f16_cost  */\n+  4, /* reduc_f32_cost  */\n+  2, /* reduc_f64_cost  */\n+  2, /* store_elt_extra_cost  */\n+  /* This value is just inherited from the Cortex-A57 table.  */\n+  8, /* vec_to_scalar_cost  */\n+  /* This depends very much on what the scalar value is and\n+     where it comes from.  E.g. some constants take two dependent\n+     instructions or a load, while others might be moved from a GPR.\n+     4 seems to be a reasonable compromise in practice.  */\n+  4, /* scalar_to_vec_cost  */\n+  4, /* align_load_cost  */\n+  4, /* unalign_load_cost  */\n+  /* Although stores have a latency of 2 and compete for the\n+     vector pipes, in practice it's better not to model that.  */\n+  1, /* unalign_store_cost  */\n+  1  /* store_cost  */\n+};\n+\n+static const sve_vec_cost neoversen2_sve_vector_cost =\n+{\n+  {\n+    2, /* int_stmt_cost  */\n+    2, /* fp_stmt_cost  */\n+    3, /* ld2_st2_permute_cost  */\n+    4, /* ld3_st3_permute_cost  */\n+    4, /* ld4_st4_permute_cost  */\n+    3, /* permute_cost  */\n+    /* Theoretically, a reduction involving 15 scalar ADDs could\n+       complete in ~5 cycles and would have a cost of 15.  [SU]ADDV\n+       completes in 11 cycles, so give it a cost of 15 + 6.  */\n+    21, /* reduc_i8_cost  */\n+    /* Likewise for 7 scalar ADDs (~3 cycles) vs. 9: 7 + 6.  */\n+    13, /* reduc_i16_cost  */\n+    /* Likewise for 3 scalar ADDs (~2 cycles) vs. 8: 3 + 6.  */\n+    9, /* reduc_i32_cost  */\n+    /* Likewise for 1 scalar ADD (~1 cycles) vs. 2: 1 + 1.  */\n+    2, /* reduc_i64_cost  */\n+    /* Theoretically, a reduction involving 7 scalar FADDs could\n+       complete in ~8 cycles and would have a cost of 14.  FADDV\n+       completes in 6 cycles, so give it a cost of 14 - 2.  */\n+    12, /* reduc_f16_cost  */\n+    /* Likewise for 3 scalar FADDs (~4 cycles) vs. 4: 6 - 0.  */\n+    6, /* reduc_f32_cost  */\n+    /* Likewise for 1 scalar FADD (~2 cycles) vs. 2: 2 - 0.  */\n+    2, /* reduc_f64_cost  */\n+    2, /* store_elt_extra_cost  */\n+    /* This value is just inherited from the Cortex-A57 table.  */\n+    8, /* vec_to_scalar_cost  */\n+    /* See the comment above the Advanced SIMD versions.  */\n+    4, /* scalar_to_vec_cost  */\n+    4, /* align_load_cost  */\n+    4, /* unalign_load_cost  */\n+    /* Although stores have a latency of 2 and compete for the\n+       vector pipes, in practice it's better not to model that.  */\n+    1, /* unalign_store_cost  */\n+    1  /* store_cost  */\n+  },\n+  3, /* clast_cost  */\n+  10, /* fadda_f16_cost  */\n+  6, /* fadda_f32_cost  */\n+  4, /* fadda_f64_cost  */\n+  /* A strided Advanced SIMD x64 load would take two parallel FP loads\n+     (8 cycles) plus an insertion (2 cycles).  Assume a 64-bit SVE gather\n+     is 1 cycle more.  The Advanced SIMD version is costed as 2 scalar loads\n+     (cost 8) and a vec_construct (cost 2).  Add a full vector operation\n+     (cost 2) to that, to avoid the difference being lost in rounding.\n+\n+     There is no easy comparison between a strided Advanced SIMD x32 load\n+     and an SVE 32-bit gather, but cost an SVE 32-bit gather as 1 vector\n+     operation more than a 64-bit gather.  */\n+  14, /* gather_load_x32_cost  */\n+  12, /* gather_load_x64_cost  */\n+  3 /* scatter_store_elt_cost  */\n+};\n+\n+static const aarch64_scalar_vec_issue_info neoversen2_scalar_issue_info =\n+{\n+  3, /* loads_stores_per_cycle  */\n+  2, /* stores_per_cycle  */\n+  4, /* general_ops_per_cycle  */\n+  0, /* fp_simd_load_general_ops  */\n+  1 /* fp_simd_store_general_ops  */\n+};\n+\n+static const aarch64_advsimd_vec_issue_info neoversen2_advsimd_issue_info =\n+{\n+  {\n+    3, /* loads_stores_per_cycle  */\n+    2, /* stores_per_cycle  */\n+    2, /* general_ops_per_cycle  */\n+    0, /* fp_simd_load_general_ops  */\n+    1 /* fp_simd_store_general_ops  */\n+  },\n+  2, /* ld2_st2_general_ops  */\n+  2, /* ld3_st3_general_ops  */\n+  3 /* ld4_st4_general_ops  */\n+};\n+\n+static const aarch64_sve_vec_issue_info neoversen2_sve_issue_info =\n+{\n+  {\n+    {\n+      3, /* loads_per_cycle  */\n+      2, /* stores_per_cycle  */\n+      2, /* general_ops_per_cycle  */\n+      0, /* fp_simd_load_general_ops  */\n+      1 /* fp_simd_store_general_ops  */\n+    },\n+    2, /* ld2_st2_general_ops  */\n+    3, /* ld3_st3_general_ops  */\n+    3 /* ld4_st4_general_ops  */\n+  },\n+  2, /* pred_ops_per_cycle  */\n+  2, /* while_pred_ops  */\n+  2, /* int_cmp_pred_ops  */\n+  1, /* fp_cmp_pred_ops  */\n+  1, /* gather_scatter_pair_general_ops  */\n+  1 /* gather_scatter_pair_pred_ops  */\n+};\n+\n+static const aarch64_vec_issue_info neoversen2_vec_issue_info =\n+{\n+  &neoversen2_scalar_issue_info,\n+  &neoversen2_advsimd_issue_info,\n+  &neoversen2_sve_issue_info\n+};\n+\n+/* Neoverse N2 costs for vector insn classes.  */\n+static const struct cpu_vector_cost neoversen2_vector_cost =\n+{\n+  1, /* scalar_int_stmt_cost  */\n+  2, /* scalar_fp_stmt_cost  */\n+  4, /* scalar_load_cost  */\n+  1, /* scalar_store_cost  */\n+  1, /* cond_taken_branch_cost  */\n+  1, /* cond_not_taken_branch_cost  */\n+  &neoversen2_advsimd_vector_cost, /* advsimd  */\n+  &neoversen2_sve_vector_cost, /* sve  */\n+  &neoversen2_vec_issue_info /* issue_info  */\n+};\n+\n static const struct tune_params neoversen2_tunings =\n {\n   &cortexa76_extra_costs,\n-  &generic_addrcost_table,\n-  &generic_regmove_cost,\n-  &cortexa57_vector_cost,\n+  &neoversen2_addrcost_table,\n+  &neoversen2_regmove_cost,\n+  &neoversen2_vector_cost,\n   &generic_branch_cost,\n   &generic_approx_modes,\n   SVE_128, /* sve_width  */\n@@ -2076,7 +2258,10 @@ static const struct tune_params neoversen2_tunings =\n   2,\t/* min_div_recip_mul_df.  */\n   0,\t/* max_case_values.  */\n   tune_params::AUTOPREFETCHER_WEAK,\t/* autoprefetcher_model.  */\n-  (AARCH64_EXTRA_TUNE_CHEAP_SHIFT_EXTEND),\t/* tune_flags.  */\n+  (AARCH64_EXTRA_TUNE_CHEAP_SHIFT_EXTEND\n+   | AARCH64_EXTRA_TUNE_CSE_SVE_VL_CONSTANTS\n+   | AARCH64_EXTRA_TUNE_USE_NEW_VECTOR_COSTS\n+   | AARCH64_EXTRA_TUNE_MATCHED_VECTOR_THROUGHPUT),\t/* tune_flags.  */\n   &generic_prefetch_tune\n };\n \n@@ -14970,7 +15155,8 @@ aarch64_vec_op_count::sve_issue_info () const\n fractional_cost\n aarch64_vec_op_count::rename_cycles_per_iter () const\n {\n-  if (sve_issue_info () == &neoverse512tvb_sve_issue_info)\n+  if (sve_issue_info () == &neoverse512tvb_sve_issue_info\n+      || sve_issue_info () == &neoversen2_sve_issue_info)\n     /* + 1 for an addition.  We've already counted a general op for each\n        store, so we don't need to account for stores separately.  The branch\n        reads no registers and so does not need to be counted either."}]}