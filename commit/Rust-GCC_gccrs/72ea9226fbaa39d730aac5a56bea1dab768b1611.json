{"sha": "72ea9226fbaa39d730aac5a56bea1dab768b1611", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NzJlYTkyMjZmYmFhMzlkNzMwYWFjNWE1NmJlYTFkYWI3NjhiMTYxMQ==", "commit": {"author": {"name": "Benjamin Kosnik", "email": "bkoz@redhat.com", "date": "2002-06-25T16:45:01Z"}, "committer": {"name": "Benjamin Kosnik", "email": "bkoz@gcc.gnu.org", "date": "2002-06-25T16:45:01Z"}, "message": "stl_alloc.h: Additional formatting.\n\n\n2002-06-25  Benjamin Kosnik  <bkoz@redhat.com>\n\n\t* include/bits/stl_alloc.h: Additional formatting.\n\nFrom-SVN: r54990", "tree": {"sha": "d5f5d82869d37f0cfcf5c39b83c141b13bed80a7", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d5f5d82869d37f0cfcf5c39b83c141b13bed80a7"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/72ea9226fbaa39d730aac5a56bea1dab768b1611", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/72ea9226fbaa39d730aac5a56bea1dab768b1611", "html_url": "https://github.com/Rust-GCC/gccrs/commit/72ea9226fbaa39d730aac5a56bea1dab768b1611", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/72ea9226fbaa39d730aac5a56bea1dab768b1611/comments", "author": null, "committer": null, "parents": [{"sha": "da15dae6f5381cc92e8149ae3da488bb2e1cb843", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/da15dae6f5381cc92e8149ae3da488bb2e1cb843", "html_url": "https://github.com/Rust-GCC/gccrs/commit/da15dae6f5381cc92e8149ae3da488bb2e1cb843"}], "stats": {"total": 1547, "additions": 779, "deletions": 768}, "files": [{"sha": "5ef95d2d3231a2dd0fda5a5d2b721e4cfb460571", "filename": "libstdc++-v3/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/72ea9226fbaa39d730aac5a56bea1dab768b1611/libstdc%2B%2B-v3%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/72ea9226fbaa39d730aac5a56bea1dab768b1611/libstdc%2B%2B-v3%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2FChangeLog?ref=72ea9226fbaa39d730aac5a56bea1dab768b1611", "patch": "@@ -1,3 +1,7 @@\n+2002-06-25  Benjamin Kosnik  <bkoz@redhat.com>\n+\n+\t* include/bits/stl_alloc.h: Additional formatting.\n+\n 2002-06-24  Phil Edwards  <pme@gcc.gnu.org>\n \n \t* include/bits/stl_alloc.h:  Reformat as per C++STYLE."}, {"sha": "805b0f05372800963f6b8d36dad6c94c4abea990", "filename": "libstdc++-v3/include/bits/stl_alloc.h", "status": "modified", "additions": 775, "deletions": 768, "changes": 1543, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/72ea9226fbaa39d730aac5a56bea1dab768b1611/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fstl_alloc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/72ea9226fbaa39d730aac5a56bea1dab768b1611/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fstl_alloc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fstl_alloc.h?ref=72ea9226fbaa39d730aac5a56bea1dab768b1611", "patch": "@@ -87,100 +87,98 @@\n \n namespace std\n {\n-\n-/**\n- *  @if maint\n- *  A new-based allocator, as required by the standard.  Allocation and\n- *  deallocation forward to global new and delete.  \"SGI\" style, minus\n- *  reallocate().\n- *  @endif\n- *  (See @link Allocators allocators info @endlink for more.)\n-*/\n-class __new_alloc\n-{\n-public:\n-  static void*\n-  allocate(size_t __n)\n+  /**\n+   *  @if maint\n+   *  A new-based allocator, as required by the standard.  Allocation and\n+   *  deallocation forward to global new and delete.  \"SGI\" style, minus\n+   *  reallocate().\n+   *  @endif\n+   *  (See @link Allocators allocators info @endlink for more.)\n+   */\n+  class __new_alloc\n+  {\n+  public:\n+    static void*\n+    allocate(size_t __n)\n     { return ::operator new(__n); }\n-\n-  static void\n-  deallocate(void* __p, size_t)\n+    \n+    static void\n+    deallocate(void* __p, size_t)\n     { ::operator delete(__p); }\n-};\n-\n-\n-/**\n- *  @if maint\n- *  A malloc-based allocator.  Typically slower than the\n- *  __default_alloc_template (below).  Typically thread-safe and more\n- *  storage efficient.  The template argument is unused and is only present\n- *  to permit multiple instantiations (but see __default_alloc_template\n- *  for caveats).  \"SGI\" style, plus __set_malloc_handler for OOM conditions.\n- *  @endif\n- *  (See @link Allocators allocators info @endlink for more.)\n-*/\n-template <int __inst>\n-  class __malloc_alloc_template\n-{\n-private:\n-  static void* _S_oom_malloc(size_t);\n-  static void* _S_oom_realloc(void*, size_t);\n-  static void (* __malloc_alloc_oom_handler)();\n-\n-public:\n-  static void*\n-  allocate(size_t __n)\n-  {\n-    void* __result = malloc(__n);\n-    if (0 == __result) __result = _S_oom_malloc(__n);\n-    return __result;\n-  }\n-\n-  static void\n-  deallocate(void* __p, size_t /* __n */)\n-  { free(__p); }\n-\n-  static void*\n-  reallocate(void* __p, size_t /* old_sz */, size_t __new_sz)\n-  {\n-    void* __result = realloc(__p, __new_sz);\n-    if (0 == __result) __result = _S_oom_realloc(__p, __new_sz);\n-    return __result;\n-  }\n-\n-  static void (* __set_malloc_handler(void (*__f)()))()\n-  {\n-    void (* __old)() = __malloc_alloc_oom_handler;\n-    __malloc_alloc_oom_handler = __f;\n-    return(__old);\n-  }\n-};\n-\n-// malloc_alloc out-of-memory handling\n-template <int __inst>\n-  void (* __malloc_alloc_template<__inst>::__malloc_alloc_oom_handler)() = 0;\n+  };\n \n-template <int __inst>\n-  void*\n-  __malloc_alloc_template<__inst>::\n-  _S_oom_malloc(size_t __n)\n-  {\n-    void (* __my_malloc_handler)();\n-    void* __result;\n+  /**\n+   *  @if maint\n+   *  A malloc-based allocator.  Typically slower than the\n+   *  __default_alloc_template (below).  Typically thread-safe and more\n+   *  storage efficient.  The template argument is unused and is only present\n+   *  to permit multiple instantiations (but see __default_alloc_template\n+   *  for caveats).  \"SGI\" style, plus __set_malloc_handler for OOM conditions.\n+   *  @endif\n+   *  (See @link Allocators allocators info @endlink for more.)\n+   */\n+  template<int __inst>\n+    class __malloc_alloc_template\n+    {\n+    private:\n+      static void* _S_oom_malloc(size_t);\n+      static void* _S_oom_realloc(void*, size_t);\n+      static void (* __malloc_alloc_oom_handler)();\n+\n+    public:\n+      static void*\n+      allocate(size_t __n)\n+      {\n+\tvoid* __result = malloc(__n);\n+\tif (0 == __result) __result = _S_oom_malloc(__n);\n+\treturn __result;\n+      }\n+      \n+      static void\n+      deallocate(void* __p, size_t /* __n */)\n+      { free(__p); }\n+      \n+      static void*\n+      reallocate(void* __p, size_t /* old_sz */, size_t __new_sz)\n+      {\n+\tvoid* __result = realloc(__p, __new_sz);\n+\tif (0 == __result) \n+\t  __result = _S_oom_realloc(__p, __new_sz);\n+\treturn __result;\n+      }\n \n-    for (;;)\n+      static void (* __set_malloc_handler(void (*__f)()))()\n       {\n-        __my_malloc_handler = __malloc_alloc_oom_handler;\n-        if (0 == __my_malloc_handler)\n-          std::__throw_bad_alloc();\n-        (*__my_malloc_handler)();\n-        __result = malloc(__n);\n-        if (__result)\n-          return(__result);\n+\tvoid (* __old)() = __malloc_alloc_oom_handler;\n+\t__malloc_alloc_oom_handler = __f;\n+\treturn(__old);\n       }\n-  }\n+    };\n+\n+  // malloc_alloc out-of-memory handling\n+  template<int __inst>\n+    void (* __malloc_alloc_template<__inst>::__malloc_alloc_oom_handler)() = 0;\n \n-template <int __inst>\n+  template<int __inst>\n+    void*\n+    __malloc_alloc_template<__inst>::_S_oom_malloc(size_t __n)\n+    {\n+      void (* __my_malloc_handler)();\n+      void* __result;\n+      \n+      for (;;)\n+\t{\n+\t  __my_malloc_handler = __malloc_alloc_oom_handler;\n+\t  if (0 == __my_malloc_handler)\n+          std::__throw_bad_alloc();\n+\t  (*__my_malloc_handler)();\n+\t  __result = malloc(__n);\n+\t  if (__result)\n+\t    return(__result);\n+\t}\n+    }\n+  \n+  template<int __inst>\n   void*\n   __malloc_alloc_template<__inst>::\n   _S_oom_realloc(void* __p, size_t __n)\n@@ -201,749 +199,758 @@ template <int __inst>\n   }\n \n \n-// Determines the underlying allocator choice for the node allocator.\n+  // Determines the underlying allocator choice for the node allocator.\n #ifdef __USE_MALLOC\n   typedef __malloc_alloc_template<0>  __mem_interface;\n #else\n   typedef __new_alloc                 __mem_interface;\n #endif\n \n \n-/**\n- *  @if maint\n- *  This is used primarily (only?) in _Alloc_traits and other places to\n- *  help provide the _Alloc_type typedef.\n- *\n- *  This is neither \"standard\"-conforming nor \"SGI\".  The _Alloc parameter\n- *  must be \"SGI\" style.\n- *  @endif\n- *  (See @link Allocators allocators info @endlink for more.)\n-*/\n-template<class _Tp, class _Alloc>\n+  /**\n+   *  @if maint\n+   *  This is used primarily (only?) in _Alloc_traits and other places to\n+   *  help provide the _Alloc_type typedef.\n+   *\n+   *  This is neither \"standard\"-conforming nor \"SGI\".  The _Alloc parameter\n+   *  must be \"SGI\" style.\n+   *  @endif\n+   *  (See @link Allocators allocators info @endlink for more.)\n+   */\n+  template<typename _Tp, typename _Alloc>\n   class __simple_alloc\n-{\n-public:\n-  static _Tp*\n-  allocate(size_t __n)\n+  {\n+  public:\n+    static _Tp*\n+    allocate(size_t __n)\n     { return 0 == __n ? 0 : (_Tp*) _Alloc::allocate(__n * sizeof (_Tp)); }\n \n-  static _Tp*\n-  allocate()\n+    static _Tp*\n+    allocate()\n     { return (_Tp*) _Alloc::allocate(sizeof (_Tp)); }\n \n-  static void\n-  deallocate(_Tp* __p, size_t __n)\n+    static void\n+    deallocate(_Tp* __p, size_t __n)\n     { if (0 != __n) _Alloc::deallocate(__p, __n * sizeof (_Tp)); }\n \n-  static void\n-  deallocate(_Tp* __p)\n+    static void\n+    deallocate(_Tp* __p)\n     { _Alloc::deallocate(__p, sizeof (_Tp)); }\n-};\n-\n-\n-/**\n- *  @if maint\n- *  An adaptor for an underlying allocator (_Alloc) to check the size\n- *  arguments for debugging.  Errors are reported using assert; these\n- *  checks can be disabled via NDEBUG, but the space penalty is still\n- *  paid, therefore it is far better to just use the underlying allocator\n- *  by itelf when no checking is desired.\n- *\n- *  \"There is some evidence that this can confuse Purify.\" - SGI comment\n- *\n- *  This adaptor is \"SGI\" style.  The _Alloc parameter must also be \"SGI\".\n- *  @endif\n- *  (See @link Allocators allocators info @endlink for more.)\n-*/\n-template <class _Alloc>\n-  class __debug_alloc\n-{\n-private:\n-  enum {_S_extra = 8};  // Size of space used to store size.  Note that this\n-                        // must be large enough to preserve alignment.\n-\n-public:\n-  static void*\n-  allocate(size_t __n)\n-  {\n-    char* __result = (char*)_Alloc::allocate(__n + (int) _S_extra);\n-    *(size_t*)__result = __n;\n-    return __result + (int) _S_extra;\n-  }\n-\n-  static void\n-  deallocate(void* __p, size_t __n)\n-  {\n-    char* __real_p = (char*)__p - (int) _S_extra;\n-    assert(*(size_t*)__real_p == __n);\n-    _Alloc::deallocate(__real_p, __n + (int) _S_extra);\n-  }\n-\n-  static void*\n-  reallocate(void* __p, size_t __old_sz, size_t __new_sz)\n-  {\n-    char* __real_p = (char*)__p - (int) _S_extra;\n-    assert(*(size_t*)__real_p == __old_sz);\n-    char* __result = (char*)\n-      _Alloc::reallocate(__real_p, __old_sz + (int) _S_extra,\n-                                   __new_sz + (int) _S_extra);\n-    *(size_t*)__result = __new_sz;\n-    return __result + (int) _S_extra;\n-  }\n-};\n-\n-\n-#ifdef __USE_MALLOC\n-\n-typedef __mem_interface __alloc;\n-typedef __mem_interface __single_client_alloc;\n-\n-#else\n-\n-\n-/**\n- *  @if maint\n- *  Default node allocator.  \"SGI\" style.  Uses __mem_interface for its\n- *  underlying requests (and makes as few requests as possible).\n- *  **** Currently __mem_interface is always __new_alloc, never __malloc*.\n- *\n- *  Important implementation properties:\n- *  1. If the clients request an object of size > _MAX_BYTES, the resulting\n- *     object will be obtained directly from the underlying __mem_interface.\n- *  2. In all other cases, we allocate an object of size exactly\n- *     _S_round_up(requested_size).  Thus the client has enough size\n- *     information that we can return the object to the proper free list\n- *     without permanently losing part of the object.\n- *\n- *  The first template parameter specifies whether more than one thread may\n- *  use this allocator.  It is safe to allocate an object from one instance\n- *  of a default_alloc and deallocate it with another one.  This effectively\n- *  transfers its ownership to the second one.  This may have undesirable\n- *  effects on reference locality.\n- *\n- *  The second parameter is unused and serves only to allow the creation of\n- *  multiple default_alloc instances.  Note that containers built on different\n- *  allocator instances have different types, limiting the utility of this\n- *  approach.  If you do not wish to share the free lists with the main\n- *  default_alloc instance, instantiate this with a non-zero __inst.\n- *\n- *  @endif\n- *  (See @link Allocators allocators info @endlink for more.)\n-*/\n-template<bool __threads, int __inst>\n-  class __default_alloc_template\n-{\n-private:\n-  enum {_ALIGN = 8};\n-  enum {_MAX_BYTES = 128};\n-  enum {_NFREELISTS = _MAX_BYTES / _ALIGN};\n-\n-  union _Obj\n-  {\n-    union _Obj* _M_free_list_link;\n-    char        _M_client_data[1];    // The client sees this.\n   };\n \n-  static _Obj* volatile         _S_free_list[_NFREELISTS];\n-\n-  // Chunk allocation state.\n-  static char*                  _S_start_free;\n-  static char*                  _S_end_free;\n-  static size_t                 _S_heap_size;\n-\n-  static _STL_mutex_lock        _S_node_allocator_lock;\n-\n-  static size_t\n-  _S_round_up(size_t __bytes)\n-    { return (((__bytes) + (size_t) _ALIGN-1) & ~((size_t) _ALIGN - 1)); }\n \n-  static size_t\n-  _S_freelist_index(size_t __bytes)\n-    { return (((__bytes) + (size_t)_ALIGN-1)/(size_t)_ALIGN - 1); }\n-\n-  // Returns an object of size __n, and optionally adds to size __n\n-  // free list.\n-  static void*\n-  _S_refill(size_t __n);\n-\n-  // Allocates a chunk for nobjs of size size.  nobjs may be reduced\n-  // if it is inconvenient to allocate the requested number.\n-  static char*\n-  _S_chunk_alloc(size_t __size, int& __nobjs);\n-\n-  // It would be nice to use _STL_auto_lock here.  But we need a\n-  // test whether threads are in use.\n-  struct _Lock\n-  {\n-    _Lock() { if (__threads) _S_node_allocator_lock._M_acquire_lock(); }\n-    ~_Lock() { if (__threads) _S_node_allocator_lock._M_release_lock(); }\n-  } __attribute__ ((__unused__));\n-  friend struct _Lock;\n-\n-public:\n-  // __n must be > 0\n-  static void*\n-  allocate(size_t __n)\n-  {\n-    void* __ret = 0;\n-\n-    if (__n > (size_t) _MAX_BYTES)\n-      __ret = __mem_interface::allocate(__n);\n-    else\n+  /**\n+   *  @if maint\n+   *  An adaptor for an underlying allocator (_Alloc) to check the size\n+   *  arguments for debugging.  Errors are reported using assert; these\n+   *  checks can be disabled via NDEBUG, but the space penalty is still\n+   *  paid, therefore it is far better to just use the underlying allocator\n+   *  by itelf when no checking is desired.\n+   *\n+   *  \"There is some evidence that this can confuse Purify.\" - SGI comment\n+   *\n+   *  This adaptor is \"SGI\" style.  The _Alloc parameter must also be \"SGI\".\n+   *  @endif\n+   *  (See @link Allocators allocators info @endlink for more.)\n+   */\n+  template<typename _Alloc>\n+    class __debug_alloc\n     {\n-      _Obj* volatile* __my_free_list = _S_free_list + _S_freelist_index(__n);\n-      // Acquire the lock here with a constructor call.  This ensures that\n-      // it is released in exit or during stack unwinding.\n-      _Lock __lock_instance;\n-      _Obj* __restrict__ __result = *__my_free_list;\n-      if (__result == 0)\n-        __ret = _S_refill(_S_round_up(__n));\n-      else\n-        {\n-          *__my_free_list = __result -> _M_free_list_link;\n-          __ret = __result;\n-        }\n-    }\n-    return __ret;\n-  };\n-\n-  // __p may not be 0\n-  static void\n-  deallocate(void* __p, size_t __n)\n-  {\n-    if (__n > (size_t) _MAX_BYTES)\n-      __mem_interface::deallocate(__p, __n);\n-    else\n-    {\n-      _Obj* volatile*  __my_free_list = _S_free_list + _S_freelist_index(__n);\n-      _Obj* __q = (_Obj*)__p;\n-\n-      // Acquire the lock here with a constructor call.  This ensures that\n-      // it is released in exit or during stack unwinding.\n-      _Lock __lock_instance;\n-      __q -> _M_free_list_link = *__my_free_list;\n-      *__my_free_list = __q;\n-    }\n-  }\n-\n-  static void*\n-  reallocate(void* __p, size_t __old_sz, size_t __new_sz);\n-};\n-\n-\n-template<bool __threads, int __inst>\n-  inline bool\n-  operator==(const __default_alloc_template<__threads,__inst>&,\n-             const __default_alloc_template<__threads,__inst>&)\n-  { return true; }\n-\n-template<bool __threads, int __inst>\n-  inline bool\n-  operator!=(const __default_alloc_template<__threads,__inst>&,\n-             const __default_alloc_template<__threads,__inst>&)\n-  { return false; }\n-\n-\n-// We allocate memory in large chunks in order to avoid fragmenting the\n-// malloc heap (or whatever __mem_interface is using) too much.  We assume\n-// that __size is properly aligned.  We hold the allocation lock.\n-template<bool __threads, int __inst>\n-  char*\n-  __default_alloc_template<__threads, __inst>::\n-  _S_chunk_alloc(size_t __size, int& __nobjs)\n-  {\n-    char* __result;\n-    size_t __total_bytes = __size * __nobjs;\n-    size_t __bytes_left = _S_end_free - _S_start_free;\n-\n-    if (__bytes_left >= __total_bytes)\n+    private:\n+      // Size of space used to store size.  Note that this must be\n+      // large enough to preserve alignment.\n+      enum {_S_extra = 8};  \n+\n+    public:\n+      static void*\n+      allocate(size_t __n)\n       {\n-        __result = _S_start_free;\n-        _S_start_free += __total_bytes;\n-        return(__result);\n+\tchar* __result = (char*)_Alloc::allocate(__n + (int) _S_extra);\n+\t*(size_t*)__result = __n;\n+\treturn __result + (int) _S_extra;\n       }\n-    else if (__bytes_left >= __size)\n+      \n+      static void\n+      deallocate(void* __p, size_t __n)\n       {\n-        __nobjs = (int)(__bytes_left/__size);\n-        __total_bytes = __size * __nobjs;\n-        __result = _S_start_free;\n-        _S_start_free += __total_bytes;\n-        return(__result);\n+\tchar* __real_p = (char*)__p - (int) _S_extra;\n+\tassert(*(size_t*)__real_p == __n);\n+\t_Alloc::deallocate(__real_p, __n + (int) _S_extra);\n       }\n-    else\n+      \n+      static void*\n+      reallocate(void* __p, size_t __old_sz, size_t __new_sz)\n       {\n-        size_t __bytes_to_get =\n-          2 * __total_bytes + _S_round_up(_S_heap_size >> 4);\n-        // Try to make use of the left-over piece.\n-        if (__bytes_left > 0)\n-          {\n-            _Obj* volatile* __my_free_list =\n-              _S_free_list + _S_freelist_index(__bytes_left);\n-\n-            ((_Obj*)_S_start_free) -> _M_free_list_link = *__my_free_list;\n-            *__my_free_list = (_Obj*)_S_start_free;\n-          }\n-        _S_start_free = (char*) __mem_interface::allocate(__bytes_to_get);\n-        if (0 == _S_start_free)\n-          {\n-            size_t __i;\n-            _Obj* volatile* __my_free_list;\n-            _Obj* __p;\n-            // Try to make do with what we have.  That can't hurt.  We\n-            // do not try smaller requests, since that tends to result\n-            // in disaster on multi-process machines.\n-            __i = __size;\n-            for (; __i <= (size_t) _MAX_BYTES; __i += (size_t) _ALIGN)\n-              {\n-                __my_free_list = _S_free_list + _S_freelist_index(__i);\n-                __p = *__my_free_list;\n-                if (0 != __p)\n-                  {\n-                    *__my_free_list = __p -> _M_free_list_link;\n-                    _S_start_free = (char*)__p;\n-                    _S_end_free = _S_start_free + __i;\n-                    return(_S_chunk_alloc(__size, __nobjs));\n-                    // Any leftover piece will eventually make it to the\n-                    // right free list.\n-                  }\n-              }\n-            _S_end_free = 0;        // In case of exception.\n-            _S_start_free = (char*)__mem_interface::allocate(__bytes_to_get);\n-            // This should either throw an exception or remedy the situation.\n-            // Thus we assume it succeeded.\n-          }\n-        _S_heap_size += __bytes_to_get;\n-        _S_end_free = _S_start_free + __bytes_to_get;\n-        return(_S_chunk_alloc(__size, __nobjs));\n+\tchar* __real_p = (char*)__p - (int) _S_extra;\n+\tassert(*(size_t*)__real_p == __old_sz);\n+\tchar* __result = (char*)\n+\t  _Alloc::reallocate(__real_p, __old_sz + (int) _S_extra,\n+\t\t\t     __new_sz + (int) _S_extra);\n+\t*(size_t*)__result = __new_sz;\n+\treturn __result + (int) _S_extra;\n       }\n-  }\n+    };\n+  \n+  \n+#ifdef __USE_MALLOC\n+  \n+  typedef __mem_interface __alloc;\n+  typedef __mem_interface __single_client_alloc;\n \n+#else\n \n-// Returns an object of size __n, and optionally adds to \"size\n-// __n\"'s free list.  We assume that __n is properly aligned.  We\n-// hold the allocation lock.\n-template<bool __threads, int __inst>\n-  void*\n-  __default_alloc_template<__threads, __inst>::\n-  _S_refill(size_t __n)\n-  {\n-    int __nobjs = 20;\n-    char* __chunk = _S_chunk_alloc(__n, __nobjs);\n-    _Obj* volatile* __my_free_list;\n-    _Obj* __result;\n-    _Obj* __current_obj;\n-    _Obj* __next_obj;\n-    int __i;\n-\n-    if (1 == __nobjs)\n-      return(__chunk);\n-    __my_free_list = _S_free_list + _S_freelist_index(__n);\n-\n-    /* Build free list in chunk */\n-    __result = (_Obj*)__chunk;\n-    *__my_free_list = __next_obj = (_Obj*)(__chunk + __n);\n-    for (__i = 1; ; __i++)\n+\n+  /**\n+   *  @if maint\n+   *  Default node allocator.  \"SGI\" style.  Uses __mem_interface for its\n+   *  underlying requests (and makes as few requests as possible).\n+   *  **** Currently __mem_interface is always __new_alloc, never __malloc*.\n+   *\n+   *  Important implementation properties:\n+   *  1. If the clients request an object of size > _MAX_BYTES, the resulting\n+   *     object will be obtained directly from the underlying __mem_interface.\n+   *  2. In all other cases, we allocate an object of size exactly\n+   *     _S_round_up(requested_size).  Thus the client has enough size\n+   *     information that we can return the object to the proper free list\n+   *     without permanently losing part of the object.\n+   *\n+   *  The first template parameter specifies whether more than one thread may\n+   *  use this allocator.  It is safe to allocate an object from one instance\n+   *  of a default_alloc and deallocate it with another one.  This effectively\n+   *  transfers its ownership to the second one.  This may have undesirable\n+   *  effects on reference locality.\n+   *\n+   *  The second parameter is unused and serves only to allow the creation of\n+   *  multiple default_alloc instances.  Note that containers built on different\n+   *  allocator instances have different types, limiting the utility of this\n+   *  approach.  If you do not wish to share the free lists with the main\n+   *  default_alloc instance, instantiate this with a non-zero __inst.\n+   *\n+   *  @endif\n+   *  (See @link Allocators allocators info @endlink for more.)\n+   */\n+  template<bool __threads, int __inst>\n+    class __default_alloc_template\n     {\n-      __current_obj = __next_obj;\n-      __next_obj = (_Obj*)((char*)__next_obj + __n);\n-      if (__nobjs - 1 == __i)\n+    private:\n+      enum {_ALIGN = 8};\n+      enum {_MAX_BYTES = 128};\n+      enum {_NFREELISTS = _MAX_BYTES / _ALIGN};\n+      \n+      union _Obj\n       {\n-        __current_obj -> _M_free_list_link = 0;\n-        break;\n-      }\n-      else\n+\tunion _Obj* _M_free_list_link;\n+\tchar        _M_client_data[1];    // The client sees this.\n+      };\n+      \n+      static _Obj* volatile         _S_free_list[_NFREELISTS];\n+      \n+      // Chunk allocation state.\n+      static char*                  _S_start_free;\n+      static char*                  _S_end_free;\n+      static size_t                 _S_heap_size;\n+      \n+      static _STL_mutex_lock        _S_node_allocator_lock;\n+      \n+      static size_t\n+      _S_round_up(size_t __bytes)\n+      { return (((__bytes) + (size_t) _ALIGN-1) & ~((size_t) _ALIGN - 1)); }\n+      \n+      static size_t\n+      _S_freelist_index(size_t __bytes)\n+      { return (((__bytes) + (size_t)_ALIGN-1)/(size_t)_ALIGN - 1); }\n+      \n+      // Returns an object of size __n, and optionally adds to size __n\n+      // free list.\n+      static void*\n+      _S_refill(size_t __n);\n+      \n+      // Allocates a chunk for nobjs of size size.  nobjs may be reduced\n+      // if it is inconvenient to allocate the requested number.\n+      static char*\n+      _S_chunk_alloc(size_t __size, int& __nobjs);\n+      \n+      // It would be nice to use _STL_auto_lock here.  But we need a\n+      // test whether threads are in use.\n+      struct _Lock\n+      {\n+\t_Lock() { if (__threads) _S_node_allocator_lock._M_acquire_lock(); }\n+\t~_Lock() { if (__threads) _S_node_allocator_lock._M_release_lock(); }\n+      } __attribute__ ((__unused__));\n+      friend struct _Lock;\n+      \n+    public:\n+      // __n must be > 0\n+      static void*\n+      allocate(size_t __n)\n       {\n-        __current_obj -> _M_free_list_link = __next_obj;\n+\tvoid* __ret = 0;\n+\t\n+\tif (__n > (size_t) _MAX_BYTES)\n+\t  __ret = __mem_interface::allocate(__n);\n+\telse\n+\t  {\n+\t    _Obj* volatile* __my_free_list = _S_free_list \n+\t                                     + _S_freelist_index(__n);\n+\t    // Acquire the lock here with a constructor call.  This\n+\t    // ensures that it is released in exit or during stack\n+\t    // unwinding.\n+\t    _Lock __lock_instance;\n+\t    _Obj* __restrict__ __result = *__my_free_list;\n+\t    if (__result == 0)\n+\t      __ret = _S_refill(_S_round_up(__n));\n+\t    else\n+\t      {\n+\t\t*__my_free_list = __result -> _M_free_list_link;\n+\t\t__ret = __result;\n+\t      }\n+\t  }\n+\treturn __ret;\n+      };\n+      \n+      // __p may not be 0\n+      static void\n+      deallocate(void* __p, size_t __n)\n+      {\n+\tif (__n > (size_t) _MAX_BYTES)\n+\t  __mem_interface::deallocate(__p, __n);\n+\telse\n+\t  {\n+\t    _Obj* volatile*  __my_free_list = _S_free_list \n+\t      + _S_freelist_index(__n);\n+\t    _Obj* __q = (_Obj*)__p;\n+\t    \n+\t    // Acquire the lock here with a constructor call.  This\n+\t    // ensures that it is released in exit or during stack\n+\t    // unwinding.\n+\t    _Lock __lock_instance;\n+\t    __q -> _M_free_list_link = *__my_free_list;\n+\t    *__my_free_list = __q;\n+\t  }\n       }\n+      \n+      static void*\n+      reallocate(void* __p, size_t __old_sz, size_t __new_sz);\n+    };\n+\n+  \n+  template<bool __threads, int __inst>\n+    inline bool\n+    operator==(const __default_alloc_template<__threads,__inst>&,\n+\t       const __default_alloc_template<__threads,__inst>&)\n+    { return true; }\n+\n+  template<bool __threads, int __inst>\n+    inline bool\n+    operator!=(const __default_alloc_template<__threads,__inst>&,\n+\t       const __default_alloc_template<__threads,__inst>&)\n+    { return false; }\n+\n+\n+  // We allocate memory in large chunks in order to avoid fragmenting the\n+  // malloc heap (or whatever __mem_interface is using) too much.  We assume\n+  // that __size is properly aligned.  We hold the allocation lock.\n+  template<bool __threads, int __inst>\n+    char*\n+    __default_alloc_template<__threads, __inst>::\n+    _S_chunk_alloc(size_t __size, int& __nobjs)\n+    {\n+      char* __result;\n+      size_t __total_bytes = __size * __nobjs;\n+      size_t __bytes_left = _S_end_free - _S_start_free;\n+      \n+      if (__bytes_left >= __total_bytes)\n+\t{\n+\t  __result = _S_start_free;\n+\t  _S_start_free += __total_bytes;\n+\t  return(__result);\n+\t}\n+      else if (__bytes_left >= __size)\n+\t{\n+\t  __nobjs = (int)(__bytes_left/__size);\n+\t  __total_bytes = __size * __nobjs;\n+\t  __result = _S_start_free;\n+\t  _S_start_free += __total_bytes;\n+\t  return(__result);\n+\t}\n+      else\n+\t{\n+\t  size_t __bytes_to_get =\n+\t    2 * __total_bytes + _S_round_up(_S_heap_size >> 4);\n+\t  // Try to make use of the left-over piece.\n+\t  if (__bytes_left > 0)\n+\t    {\n+\t      _Obj* volatile* __my_free_list =\n+\t\t_S_free_list + _S_freelist_index(__bytes_left);\n+\t      \n+\t      ((_Obj*)_S_start_free) -> _M_free_list_link = *__my_free_list;\n+\t      *__my_free_list = (_Obj*)_S_start_free;\n+\t    }\n+\t  _S_start_free = (char*) __mem_interface::allocate(__bytes_to_get);\n+\t  if (0 == _S_start_free)\n+\t    {\n+\t      size_t __i;\n+\t      _Obj* volatile* __my_free_list;\n+\t      _Obj* __p;\n+\t      // Try to make do with what we have.  That can't hurt.  We\n+\t      // do not try smaller requests, since that tends to result\n+\t      // in disaster on multi-process machines.\n+\t      __i = __size;\n+\t      for (; __i <= (size_t) _MAX_BYTES; __i += (size_t) _ALIGN)\n+\t\t{\n+\t\t  __my_free_list = _S_free_list + _S_freelist_index(__i);\n+\t\t  __p = *__my_free_list;\n+\t\t  if (0 != __p)\n+\t\t    {\n+\t\t      *__my_free_list = __p -> _M_free_list_link;\n+\t\t      _S_start_free = (char*)__p;\n+\t\t      _S_end_free = _S_start_free + __i;\n+\t\t      return(_S_chunk_alloc(__size, __nobjs));\n+\t\t      // Any leftover piece will eventually make it to the\n+\t\t      // right free list.\n+\t\t    }\n+\t\t}\n+\t      _S_end_free = 0;        // In case of exception.\n+\t      _S_start_free = (char*)__mem_interface::allocate(__bytes_to_get);\n+\t      // This should either throw an exception or remedy the situation.\n+\t      // Thus we assume it succeeded.\n+\t    }\n+\t  _S_heap_size += __bytes_to_get;\n+\t  _S_end_free = _S_start_free + __bytes_to_get;\n+\t  return(_S_chunk_alloc(__size, __nobjs));\n+\t}\n     }\n-    return(__result);\n-  }\n-\n-\n-template<bool threads, int inst>\n-  void*\n-  __default_alloc_template<threads, inst>::\n-  reallocate(void* __p, size_t __old_sz, size_t __new_sz)\n-  {\n-    void* __result;\n-    size_t __copy_sz;\n-\n-    if (__old_sz > (size_t) _MAX_BYTES && __new_sz > (size_t) _MAX_BYTES)\n-      return(realloc(__p, __new_sz));\n-    if (_S_round_up(__old_sz) == _S_round_up(__new_sz))\n-      return(__p);\n-    __result = allocate(__new_sz);\n-    __copy_sz = __new_sz > __old_sz? __old_sz : __new_sz;\n-    memcpy(__result, __p, __copy_sz);\n-    deallocate(__p, __old_sz);\n-    return(__result);\n-  }\n-\n-template<bool __threads, int __inst>\n-_STL_mutex_lock\n-__default_alloc_template<__threads,__inst>::_S_node_allocator_lock\n-__STL_MUTEX_INITIALIZER;\n+  \n+\n+  // Returns an object of size __n, and optionally adds to \"size\n+  // __n\"'s free list.  We assume that __n is properly aligned.  We\n+  // hold the allocation lock.\n+  template<bool __threads, int __inst>\n+    void*\n+    __default_alloc_template<__threads, __inst>::_S_refill(size_t __n)\n+    {\n+      int __nobjs = 20;\n+      char* __chunk = _S_chunk_alloc(__n, __nobjs);\n+      _Obj* volatile* __my_free_list;\n+      _Obj* __result;\n+      _Obj* __current_obj;\n+      _Obj* __next_obj;\n+      int __i;\n+      \n+      if (1 == __nobjs)\n+\treturn(__chunk);\n+      __my_free_list = _S_free_list + _S_freelist_index(__n);\n+      \n+      // Build free list in chunk.\n+      __result = (_Obj*)__chunk;\n+      *__my_free_list = __next_obj = (_Obj*)(__chunk + __n);\n+      for (__i = 1; ; __i++)\n+\t{\n+\t  __current_obj = __next_obj;\n+\t  __next_obj = (_Obj*)((char*)__next_obj + __n);\n+\t  if (__nobjs - 1 == __i)\n+\t    {\n+\t      __current_obj -> _M_free_list_link = 0;\n+\t      break;\n+\t    }\n+\t  else\n+\t    __current_obj -> _M_free_list_link = __next_obj;\n+\t}\n+      return(__result);\n+    }\n+  \n \n-template<bool __threads, int __inst>\n-char* __default_alloc_template<__threads,__inst>::_S_start_free = 0;\n+  template<bool threads, int inst>\n+    void*\n+    __default_alloc_template<threads, inst>::\n+    reallocate(void* __p, size_t __old_sz, size_t __new_sz)\n+    {\n+      void* __result;\n+      size_t __copy_sz;\n+      \n+      if (__old_sz > (size_t) _MAX_BYTES && __new_sz > (size_t) _MAX_BYTES)\n+\treturn(realloc(__p, __new_sz));\n+      if (_S_round_up(__old_sz) == _S_round_up(__new_sz))\n+\treturn(__p);\n+      __result = allocate(__new_sz);\n+      __copy_sz = __new_sz > __old_sz? __old_sz : __new_sz;\n+      memcpy(__result, __p, __copy_sz);\n+      deallocate(__p, __old_sz);\n+      return(__result);\n+    }\n \n-template<bool __threads, int __inst>\n-char* __default_alloc_template<__threads,__inst>::_S_end_free = 0;\n+  template<bool __threads, int __inst>\n+    _STL_mutex_lock\n+    __default_alloc_template<__threads,__inst>::_S_node_allocator_lock\n+    __STL_MUTEX_INITIALIZER;\n \n-template<bool __threads, int __inst>\n-size_t __default_alloc_template<__threads,__inst>::_S_heap_size = 0;\n+  template<bool __threads, int __inst>\n+    char* __default_alloc_template<__threads,__inst>::_S_start_free = 0;\n \n-template<bool __threads, int __inst>\n-typename __default_alloc_template<__threads,__inst>::_Obj* volatile\n-__default_alloc_template<__threads,__inst>::_S_free_list[_NFREELISTS];\n+  template<bool __threads, int __inst>\n+    char* __default_alloc_template<__threads,__inst>::_S_end_free = 0;\n \n-typedef __default_alloc_template<true,0>    __alloc;\n-typedef __default_alloc_template<false,0>   __single_client_alloc;\n+  template<bool __threads, int __inst>\n+    size_t __default_alloc_template<__threads,__inst>::_S_heap_size = 0;\n \n+  template<bool __threads, int __inst>\n+    typename __default_alloc_template<__threads,__inst>::_Obj* volatile\n+    __default_alloc_template<__threads,__inst>::_S_free_list[_NFREELISTS];\n \n+  typedef __default_alloc_template<true,0>    __alloc;\n+  typedef __default_alloc_template<false,0>   __single_client_alloc;\n #endif /* ! __USE_MALLOC */\n \n \n-/**\n- *  This is a \"standard\" allocator, as per [20.4].  The private _Alloc is\n- *  \"SGI\" style.  (See comments at the top of stl_alloc.h.)\n- *\n- *  The underlying allocator behaves as follows.\n- *  - if __USE_MALLOC then\n- *    - thread safety depends on malloc and is entirely out of our hands\n- *    - __malloc_alloc_template is used for memory requests\n- *  - else (the default)\n- *    - __default_alloc_template is used via two typedefs\n- *    - \"__single_client_alloc\" typedef does no locking for threads\n- *    - \"__alloc\" typedef is threadsafe via the locks\n- *    - __new_alloc is used for memory requests\n- *\n- *  (See @link Allocators allocators info @endlink for more.)\n-*/\n-template <class _Tp>\n-class allocator\n-{\n-  typedef __alloc _Alloc;          // The underlying allocator.\n-public:\n-  typedef size_t     size_type;\n-  typedef ptrdiff_t  difference_type;\n-  typedef _Tp*       pointer;\n-  typedef const _Tp* const_pointer;\n-  typedef _Tp&       reference;\n-  typedef const _Tp& const_reference;\n-  typedef _Tp        value_type;\n-\n-  template <class _Tp1> struct rebind {\n-    typedef allocator<_Tp1> other;\n-  };\n-\n-  allocator() throw() {}\n-  allocator(const allocator&) throw() {}\n-  template <class _Tp1> allocator(const allocator<_Tp1>&) throw() {}\n-  ~allocator() throw() {}\n-\n-  pointer address(reference __x) const { return &__x; }\n-  const_pointer address(const_reference __x) const { return &__x; }\n-\n-  // __n is permitted to be 0.  The C++ standard says nothing about what\n-  // the return value is when __n == 0.\n-  _Tp*\n-  allocate(size_type __n, const void* = 0)\n-  {\n-    return __n != 0 ? static_cast<_Tp*>(_Alloc::allocate(__n * sizeof(_Tp)))\n-                    : 0;\n-  }\n-\n-  // __p is not permitted to be a null pointer.\n-  void\n-  deallocate(pointer __p, size_type __n)\n-    { _Alloc::deallocate(__p, __n * sizeof(_Tp)); }\n-\n-  size_type\n-  max_size() const throw() { return size_t(-1) / sizeof(_Tp); }\n-\n-  void construct(pointer __p, const _Tp& __val) { new(__p) _Tp(__val); }\n-  void destroy(pointer __p) { __p->~_Tp(); }\n-};\n-\n-template<>\n-class allocator<void>\n-{\n-public:\n-  typedef size_t      size_type;\n-  typedef ptrdiff_t   difference_type;\n-  typedef void*       pointer;\n-  typedef const void* const_pointer;\n-  typedef void        value_type;\n-\n-  template <class _Tp1> struct rebind {\n-    typedef allocator<_Tp1> other;\n-  };\n-};\n-\n-\n-template <class _T1, class _T2>\n-  inline bool\n-  operator==(const allocator<_T1>&, const allocator<_T2>&)\n-  { return true; }\n-\n-template <class _T1, class _T2>\n-  inline bool\n-  operator!=(const allocator<_T1>&, const allocator<_T2>&)\n-  { return false; }\n-\n-\n-/**\n- *  @if maint\n- *  Allocator adaptor to turn an \"SGI\" style allocator (e.g., __alloc,\n- *  __malloc_alloc_template) into a \"standard\" conforming allocator.  Note\n- *  that this adaptor does *not* assume that all objects of the underlying\n- *  alloc class are identical, nor does it assume that all of the underlying\n- *  alloc's member functions are static member functions.  Note, also, that\n- *  __allocator<_Tp, __alloc> is essentially the same thing as allocator<_Tp>.\n- *  @endif\n- *  (See @link Allocators allocators info @endlink for more.)\n-*/\n-template <class _Tp, class _Alloc>\n-  struct __allocator\n-{\n-  _Alloc __underlying_alloc;\n-\n-  typedef size_t    size_type;\n-  typedef ptrdiff_t difference_type;\n-  typedef _Tp*       pointer;\n-  typedef const _Tp* const_pointer;\n-  typedef _Tp&       reference;\n-  typedef const _Tp& const_reference;\n-  typedef _Tp        value_type;\n-\n-  template <class _Tp1> struct rebind {\n-    typedef __allocator<_Tp1, _Alloc> other;\n-  };\n-\n-  __allocator() throw() {}\n-  __allocator(const __allocator& __a) throw()\n-    : __underlying_alloc(__a.__underlying_alloc) {}\n-  template <class _Tp1>\n-  __allocator(const __allocator<_Tp1, _Alloc>& __a) throw()\n-    : __underlying_alloc(__a.__underlying_alloc) {}\n-  ~__allocator() throw() {}\n-\n-  pointer address(reference __x) const { return &__x; }\n-  const_pointer address(const_reference __x) const { return &__x; }\n+  /**\n+   *  This is a \"standard\" allocator, as per [20.4].  The private _Alloc is\n+   *  \"SGI\" style.  (See comments at the top of stl_alloc.h.)\n+   *\n+   *  The underlying allocator behaves as follows.\n+   *  - if __USE_MALLOC then\n+   *    - thread safety depends on malloc and is entirely out of our hands\n+   *    - __malloc_alloc_template is used for memory requests\n+   *  - else (the default)\n+   *    - __default_alloc_template is used via two typedefs\n+   *    - \"__single_client_alloc\" typedef does no locking for threads\n+   *    - \"__alloc\" typedef is threadsafe via the locks\n+   *    - __new_alloc is used for memory requests\n+   *\n+   *  (See @link Allocators allocators info @endlink for more.)\n+   */\n+  template<typename _Tp>\n+    class allocator\n+    {\n+      typedef __alloc _Alloc;          // The underlying allocator.\n+    public:\n+      typedef size_t     size_type;\n+      typedef ptrdiff_t  difference_type;\n+      typedef _Tp*       pointer;\n+      typedef const _Tp* const_pointer;\n+      typedef _Tp&       reference;\n+      typedef const _Tp& const_reference;\n+      typedef _Tp        value_type;\n+      \n+      template<typename _Tp1> \n+        struct rebind \n+\t{ typedef allocator<_Tp1> other; };\n+\n+      allocator() throw() {}\n+      allocator(const allocator&) throw() {}\n+      template<typename _Tp1> \n+        allocator(const allocator<_Tp1>&) throw() {}\n+      ~allocator() throw() {}\n+      \n+      pointer \n+      address(reference __x) const { return &__x; }\n+\n+      const_pointer \n+      address(const_reference __x) const { return &__x; }\n+      \n+      // __n is permitted to be 0.  The C++ standard says nothing about what\n+      // the return value is when __n == 0.\n+      _Tp*\n+      allocate(size_type __n, const void* = 0)\n+      {\n+\treturn __n != 0 \n+\t  ? static_cast<_Tp*>(_Alloc::allocate(__n * sizeof(_Tp))) : 0;\n+      }\n \n-  // __n is permitted to be 0.\n-  _Tp*\n-  allocate(size_type __n, const void* = 0)\n-  {\n-    return __n != 0\n+      // __p is not permitted to be a null pointer.\n+      void\n+      deallocate(pointer __p, size_type __n)\n+      { _Alloc::deallocate(__p, __n * sizeof(_Tp)); }\n+      \n+      size_type\n+      max_size() const throw() { return size_t(-1) / sizeof(_Tp); }\n+      \n+      void construct(pointer __p, const _Tp& __val) { new(__p) _Tp(__val); }\n+      void destroy(pointer __p) { __p->~_Tp(); }\n+    };\n+\n+  template<>\n+    class allocator<void>\n+    {\n+    public:\n+      typedef size_t      size_type;\n+      typedef ptrdiff_t   difference_type;\n+      typedef void*       pointer;\n+      typedef const void* const_pointer;\n+      typedef void        value_type;\n+      \n+      template<typename _Tp1> \n+        struct rebind \n+        { typedef allocator<_Tp1> other; };\n+    };\n+  \n+\n+  template<typename _T1, typename _T2>\n+    inline bool\n+    operator==(const allocator<_T1>&, const allocator<_T2>&)\n+    { return true; }\n+\n+  template<typename _T1, typename _T2>\n+    inline bool\n+    operator!=(const allocator<_T1>&, const allocator<_T2>&)\n+    { return false; }\n+\n+\n+  /**\n+   *  @if maint\n+   *  Allocator adaptor to turn an \"SGI\" style allocator (e.g., __alloc,\n+   *  __malloc_alloc_template) into a \"standard\" conforming allocator.  Note\n+   *  that this adaptor does *not* assume that all objects of the underlying\n+   *  alloc class are identical, nor does it assume that all of the underlying\n+   *  alloc's member functions are static member functions.  Note, also, that\n+   *  __allocator<_Tp, __alloc> is essentially the same thing as allocator<_Tp>.\n+   *  @endif\n+   *  (See @link Allocators allocators info @endlink for more.)\n+   */\n+  template<typename _Tp, typename _Alloc>\n+    struct __allocator\n+    {\n+      _Alloc __underlying_alloc;\n+      \n+      typedef size_t    size_type;\n+      typedef ptrdiff_t difference_type;\n+      typedef _Tp*       pointer;\n+      typedef const _Tp* const_pointer;\n+      typedef _Tp&       reference;\n+      typedef const _Tp& const_reference;\n+      typedef _Tp        value_type;\n+      \n+      template<typename _Tp1> \n+        struct rebind \n+\t{ typedef __allocator<_Tp1, _Alloc> other; };\n+\n+      __allocator() throw() {}\n+      __allocator(const __allocator& __a) throw()\n+      : __underlying_alloc(__a.__underlying_alloc) {}\n+\n+      template<typename _Tp1>\n+        __allocator(const __allocator<_Tp1, _Alloc>& __a) throw()\n+        : __underlying_alloc(__a.__underlying_alloc) {}\n+\n+      ~__allocator() throw() {}\n+\n+      pointer \n+      address(reference __x) const { return &__x; }\n+\n+      const_pointer \n+      address(const_reference __x) const { return &__x; }\n+\n+    // __n is permitted to be 0.\n+    _Tp*\n+    allocate(size_type __n, const void* = 0)\n+    {\n+      return __n != 0\n         ? static_cast<_Tp*>(__underlying_alloc.allocate(__n * sizeof(_Tp)))\n         : 0;\n-  }\n+    }\n \n-  // __p is not permitted to be a null pointer.\n-  void\n-  deallocate(pointer __p, size_type __n)\n+    // __p is not permitted to be a null pointer.\n+    void\n+    deallocate(pointer __p, size_type __n)\n     { __underlying_alloc.deallocate(__p, __n * sizeof(_Tp)); }\n \n-  size_type\n-  max_size() const throw() { return size_t(-1) / sizeof(_Tp); }\n+    size_type\n+    max_size() const throw() { return size_t(-1) / sizeof(_Tp); }\n \n-  void construct(pointer __p, const _Tp& __val) { new(__p) _Tp(__val); }\n-  void destroy(pointer __p) { __p->~_Tp(); }\n-};\n+    void \n+    construct(pointer __p, const _Tp& __val) { new(__p) _Tp(__val); }\n \n-template <class _Alloc>\n-class __allocator<void, _Alloc>\n-{\n-  typedef size_t      size_type;\n-  typedef ptrdiff_t   difference_type;\n-  typedef void*       pointer;\n-  typedef const void* const_pointer;\n-  typedef void        value_type;\n-\n-  template <class _Tp1> struct rebind {\n-    typedef __allocator<_Tp1, _Alloc> other;\n+    void \n+    destroy(pointer __p) { __p->~_Tp(); }\n   };\n-};\n-\n-template <class _Tp, class _Alloc>\n-  inline bool\n-  operator==(const __allocator<_Tp,_Alloc>& __a1,\n-             const __allocator<_Tp,_Alloc>& __a2)\n-  { return __a1.__underlying_alloc == __a2.__underlying_alloc; }\n-\n-template <class _Tp, class _Alloc>\n-  inline bool\n-  operator!=(const __allocator<_Tp, _Alloc>& __a1,\n-                         const __allocator<_Tp, _Alloc>& __a2)\n-  { return __a1.__underlying_alloc != __a2.__underlying_alloc; }\n \n-\n-//@{\n-/** Comparison operators for all of the predifined SGI-style allocators.\n- *  This ensures that __allocator<malloc_alloc> (for example) will work\n- *  correctly.  As required, all allocators compare equal.\n-*/\n-template <int inst>\n-  inline bool\n-  operator==(const __malloc_alloc_template<inst>&,\n-             const __malloc_alloc_template<inst>&)\n-  { return true; }\n-\n-template <int __inst>\n-  inline bool\n-  operator!=(const __malloc_alloc_template<__inst>&,\n-             const __malloc_alloc_template<__inst>&)\n-  { return false; }\n-\n-template <class _Alloc>\n-  inline bool\n-  operator==(const __debug_alloc<_Alloc>&,\n-             const __debug_alloc<_Alloc>&)\n-  { return true; }\n-\n-template <class _Alloc>\n-  inline bool\n-  operator!=(const __debug_alloc<_Alloc>&,\n-             const __debug_alloc<_Alloc>&)\n-  { return false; }\n-//@}\n-\n-\n-/**\n- *  @if maint\n- *  Another allocator adaptor:  _Alloc_traits.  This serves two purposes.\n- *  First, make it possible to write containers that can use either \"SGI\"\n- *  style allocators or \"standard\" allocators.  Second, provide a mechanism\n- *  so that containers can query whether or not the allocator has distinct\n- *  instances.  If not, the container can avoid wasting a word of memory to\n- *  store an empty object.  For examples of use, see stl_vector.h, etc, or\n- *  any of the other classes derived from this one.\n- *\n- *  This adaptor uses partial specialization.  The general case of\n- *  _Alloc_traits<_Tp, _Alloc> assumes that _Alloc is a\n- *  standard-conforming allocator, possibly with non-equal instances and\n- *  non-static members.  (It still behaves correctly even if _Alloc has\n- *  static member and if all instances are equal.  Refinements affect\n- *  performance, not correctness.)\n- *\n- *  There are always two members:  allocator_type, which is a standard-\n- *  conforming allocator type for allocating objects of type _Tp, and\n- *  _S_instanceless, a static const member of type bool.  If\n- *  _S_instanceless is true, this means that there is no difference\n- *  between any two instances of type allocator_type.  Furthermore, if\n- *  _S_instanceless is true, then _Alloc_traits has one additional\n- *  member:  _Alloc_type.  This type encapsulates allocation and\n- *  deallocation of objects of type _Tp through a static interface; it\n- *  has two member functions, whose signatures are\n- *\n- *  -  static _Tp* allocate(size_t)\n- *  -  static void deallocate(_Tp*, size_t)\n- *\n- *  The size_t parameters are \"standard\" style (see top of stl_alloc.h) in\n- *  that they take counts, not sizes.\n- *\n- *  @endif\n- *  (See @link Allocators allocators info @endlink for more.)\n-*/\n-//@{\n-// The fully general version.\n-template <class _Tp, class _Allocator>\n-struct _Alloc_traits\n-{\n-  static const bool _S_instanceless = false;\n-  typedef typename _Allocator::template rebind<_Tp>::other allocator_type;\n-};\n-\n-template <class _Tp, class _Allocator>\n-const bool _Alloc_traits<_Tp, _Allocator>::_S_instanceless;\n-\n-/// The version for the default allocator.\n-template <class _Tp, class _Tp1>\n-struct _Alloc_traits<_Tp, allocator<_Tp1> >\n-{\n-  static const bool _S_instanceless = true;\n-  typedef __simple_alloc<_Tp, __alloc> _Alloc_type;\n-  typedef allocator<_Tp> allocator_type;\n-};\n-//@}\n-\n-//@{\n-/// Versions for the predefined \"SGI\" style allocators.\n-template <class _Tp, int __inst>\n-struct _Alloc_traits<_Tp, __malloc_alloc_template<__inst> >\n-{\n-  static const bool _S_instanceless = true;\n-  typedef __simple_alloc<_Tp, __malloc_alloc_template<__inst> > _Alloc_type;\n-  typedef __allocator<_Tp, __malloc_alloc_template<__inst> > allocator_type;\n-};\n+  template<typename _Alloc>\n+    class __allocator<void, _Alloc>\n+    {\n+      typedef size_t      size_type;\n+      typedef ptrdiff_t   difference_type;\n+      typedef void*       pointer;\n+      typedef const void* const_pointer;\n+      typedef void        value_type;\n+      \n+      template<typename _Tp1> \n+        struct rebind \n+\t{ typedef __allocator<_Tp1, _Alloc> other; };\n+    };\n+\n+  template<typename _Tp, typename _Alloc>\n+    inline bool\n+    operator==(const __allocator<_Tp,_Alloc>& __a1,\n+\t       const __allocator<_Tp,_Alloc>& __a2)\n+    { return __a1.__underlying_alloc == __a2.__underlying_alloc; }\n+\n+  template<typename _Tp, typename _Alloc>\n+    inline bool\n+    operator!=(const __allocator<_Tp, _Alloc>& __a1, \n+\t       const __allocator<_Tp, _Alloc>& __a2)\n+    { return __a1.__underlying_alloc != __a2.__underlying_alloc; }\n+\n+\n+  //@{\n+  /** Comparison operators for all of the predifined SGI-style allocators.\n+   *  This ensures that __allocator<malloc_alloc> (for example) will work\n+   *  correctly.  As required, all allocators compare equal.\n+   */\n+  template<int inst>\n+    inline bool\n+    operator==(const __malloc_alloc_template<inst>&, \n+\t       const __malloc_alloc_template<inst>&)\n+    { return true; }\n+\n+  template<int __inst>\n+    inline bool\n+    operator!=(const __malloc_alloc_template<__inst>&, \n+\t       const __malloc_alloc_template<__inst>&)\n+    { return false; }\n+\n+  template<typename _Alloc>\n+    inline bool\n+    operator==(const __debug_alloc<_Alloc>&, const __debug_alloc<_Alloc>&)\n+    { return true; }\n+\n+  template<typename _Alloc>\n+    inline bool\n+    operator!=(const __debug_alloc<_Alloc>&, const __debug_alloc<_Alloc>&)\n+    { return false; }\n+  //@}\n+\n+\n+  /**\n+   *  @if maint\n+   *  Another allocator adaptor:  _Alloc_traits.  This serves two purposes.\n+   *  First, make it possible to write containers that can use either \"SGI\"\n+   *  style allocators or \"standard\" allocators.  Second, provide a mechanism\n+   *  so that containers can query whether or not the allocator has distinct\n+   *  instances.  If not, the container can avoid wasting a word of memory to\n+   *  store an empty object.  For examples of use, see stl_vector.h, etc, or\n+   *  any of the other classes derived from this one.\n+   *\n+   *  This adaptor uses partial specialization.  The general case of\n+   *  _Alloc_traits<_Tp, _Alloc> assumes that _Alloc is a\n+   *  standard-conforming allocator, possibly with non-equal instances and\n+   *  non-static members.  (It still behaves correctly even if _Alloc has\n+   *  static member and if all instances are equal.  Refinements affect\n+   *  performance, not correctness.)\n+   *\n+   *  There are always two members:  allocator_type, which is a standard-\n+   *  conforming allocator type for allocating objects of type _Tp, and\n+   *  _S_instanceless, a static const member of type bool.  If\n+   *  _S_instanceless is true, this means that there is no difference\n+   *  between any two instances of type allocator_type.  Furthermore, if\n+   *  _S_instanceless is true, then _Alloc_traits has one additional\n+   *  member:  _Alloc_type.  This type encapsulates allocation and\n+   *  deallocation of objects of type _Tp through a static interface; it\n+   *  has two member functions, whose signatures are\n+   *\n+   *  -  static _Tp* allocate(size_t)\n+   *  -  static void deallocate(_Tp*, size_t)\n+   *\n+   *  The size_t parameters are \"standard\" style (see top of stl_alloc.h) in\n+   *  that they take counts, not sizes.\n+   *\n+   *  @endif\n+   *  (See @link Allocators allocators info @endlink for more.)\n+   */\n+  //@{\n+  // The fully general version.\n+  template<typename _Tp, typename _Allocator>\n+    struct _Alloc_traits\n+    {\n+      static const bool _S_instanceless = false;\n+      typedef typename _Allocator::template rebind<_Tp>::other allocator_type;\n+    };\n+  \n+  template<typename _Tp, typename _Allocator>\n+    const bool _Alloc_traits<_Tp, _Allocator>::_S_instanceless;\n+\n+  /// The version for the default allocator.\n+  template<typename _Tp, typename _Tp1>\n+    struct _Alloc_traits<_Tp, allocator<_Tp1> >\n+    {\n+      static const bool _S_instanceless = true;\n+      typedef __simple_alloc<_Tp, __alloc> _Alloc_type;\n+      typedef allocator<_Tp> allocator_type;\n+    };\n+  //@}\n+\n+  //@{\n+  /// Versions for the predefined \"SGI\" style allocators.\n+  template<typename _Tp, int __inst>\n+    struct _Alloc_traits<_Tp, __malloc_alloc_template<__inst> >\n+    {\n+      static const bool _S_instanceless = true;\n+      typedef __simple_alloc<_Tp, __malloc_alloc_template<__inst> > _Alloc_type;\n+      typedef __allocator<_Tp, __malloc_alloc_template<__inst> > allocator_type;\n+    };\n \n #ifndef __USE_MALLOC\n-template <class _Tp, bool __threads, int __inst>\n-struct _Alloc_traits<_Tp, __default_alloc_template<__threads, __inst> >\n-{\n-  static const bool _S_instanceless = true;\n-  typedef __simple_alloc<_Tp, __default_alloc_template<__threads, __inst> >\n-          _Alloc_type;\n-  typedef __allocator<_Tp, __default_alloc_template<__threads, __inst> >\n-          allocator_type;\n-};\n+  template<typename _Tp, bool __threads, int __inst>\n+    struct _Alloc_traits<_Tp, __default_alloc_template<__threads, __inst> >\n+    {\n+      static const bool _S_instanceless = true;\n+      typedef __simple_alloc<_Tp, __default_alloc_template<__threads, __inst> >\n+      _Alloc_type;\n+      typedef __allocator<_Tp, __default_alloc_template<__threads, __inst> >\n+      allocator_type;\n+    };\n #endif\n \n-template <class _Tp, class _Alloc>\n-struct _Alloc_traits<_Tp, __debug_alloc<_Alloc> >\n-{\n-  static const bool _S_instanceless = true;\n-  typedef __simple_alloc<_Tp, __debug_alloc<_Alloc> > _Alloc_type;\n-  typedef __allocator<_Tp, __debug_alloc<_Alloc> > allocator_type;\n-};\n-//@}\n-\n-//@{\n-/// Versions for the __allocator adaptor used with the predefined \"SGI\" style allocators.\n-template <class _Tp, class _Tp1, int __inst>\n-struct _Alloc_traits<_Tp,\n-                     __allocator<_Tp1, __malloc_alloc_template<__inst> > >\n-{\n-  static const bool _S_instanceless = true;\n-  typedef __simple_alloc<_Tp, __malloc_alloc_template<__inst> > _Alloc_type;\n-  typedef __allocator<_Tp, __malloc_alloc_template<__inst> > allocator_type;\n-};\n+  template<typename _Tp, typename _Alloc>\n+    struct _Alloc_traits<_Tp, __debug_alloc<_Alloc> >\n+    {\n+      static const bool _S_instanceless = true;\n+      typedef __simple_alloc<_Tp, __debug_alloc<_Alloc> > _Alloc_type;\n+      typedef __allocator<_Tp, __debug_alloc<_Alloc> > allocator_type;\n+    };\n+  //@}\n+\n+  //@{\n+  /// Versions for the __allocator adaptor used with the predefined\n+  /// \"SGI\" style allocators.\n+  template<typename _Tp, typename _Tp1, int __inst>\n+    struct _Alloc_traits<_Tp,\n+\t\t\t __allocator<_Tp1, __malloc_alloc_template<__inst> > >\n+    {\n+      static const bool _S_instanceless = true;\n+      typedef __simple_alloc<_Tp, __malloc_alloc_template<__inst> > _Alloc_type;\n+      typedef __allocator<_Tp, __malloc_alloc_template<__inst> > allocator_type;\n+    };\n \n #ifndef __USE_MALLOC\n-template <class _Tp, class _Tp1, bool __thr, int __inst>\n-struct _Alloc_traits<_Tp,\n-                      __allocator<_Tp1,\n-                                  __default_alloc_template<__thr, __inst> > >\n-{\n-  static const bool _S_instanceless = true;\n-  typedef __simple_alloc<_Tp, __default_alloc_template<__thr,__inst> >\n-          _Alloc_type;\n-  typedef __allocator<_Tp, __default_alloc_template<__thr,__inst> >\n-          allocator_type;\n-};\n+  template<typename _Tp, typename _Tp1, bool __thr, int __inst>\n+    struct _Alloc_traits<_Tp, __allocator<_Tp1, __default_alloc_template<__thr, __inst> > >\n+    {\n+      static const bool _S_instanceless = true;\n+      typedef __simple_alloc<_Tp, __default_alloc_template<__thr,__inst> >\n+      _Alloc_type;\n+      typedef __allocator<_Tp, __default_alloc_template<__thr,__inst> >\n+      allocator_type;\n+    };\n #endif\n \n-template <class _Tp, class _Tp1, class _Alloc>\n-struct _Alloc_traits<_Tp, __allocator<_Tp1, __debug_alloc<_Alloc> > >\n-{\n-  static const bool _S_instanceless = true;\n-  typedef __simple_alloc<_Tp, __debug_alloc<_Alloc> > _Alloc_type;\n-  typedef __allocator<_Tp, __debug_alloc<_Alloc> > allocator_type;\n-};\n-//@}\n-\n-// Inhibit implicit instantiations for required instantiations,\n-// which are defined via explicit instantiations elsewhere.\n-// NB: This syntax is a GNU extension.\n-extern template class allocator<char>;\n-extern template class allocator<wchar_t>;\n+  template<typename _Tp, typename _Tp1, typename _Alloc>\n+    struct _Alloc_traits<_Tp, __allocator<_Tp1, __debug_alloc<_Alloc> > >\n+    {\n+      static const bool _S_instanceless = true;\n+      typedef __simple_alloc<_Tp, __debug_alloc<_Alloc> > _Alloc_type;\n+      typedef __allocator<_Tp, __debug_alloc<_Alloc> > allocator_type;\n+    };\n+  //@}\n+\n+  // Inhibit implicit instantiations for required instantiations,\n+  // which are defined via explicit instantiations elsewhere.\n+  // NB: This syntax is a GNU extension.\n+  extern template class allocator<char>;\n+  extern template class allocator<wchar_t>;\n #ifdef __USE_MALLOC\n-extern template class __malloc_alloc_template<0>;\n+  extern template class __malloc_alloc_template<0>;\n #else\n-extern template class __default_alloc_template<true,0>;\n+  extern template class __default_alloc_template<true,0>;\n #endif\n } // namespace std\n \n-#endif /* __GLIBCPP_INTERNAL_ALLOC_H */\n+#endif "}]}