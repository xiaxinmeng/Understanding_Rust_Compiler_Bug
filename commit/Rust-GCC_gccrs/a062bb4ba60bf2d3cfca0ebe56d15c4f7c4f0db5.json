{"sha": "a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YTA2MmJiNGJhNjBiZjJkM2NmY2EwZWJlNTZkMTVjNGY3YzRmMGRiNQ==", "commit": {"author": {"name": "Andi Kleen", "email": "ak@linux.intel.com", "date": "2013-05-09T04:22:11Z"}, "committer": {"name": "Andi Kleen", "email": "ak@gcc.gnu.org", "date": "2013-05-09T04:22:11Z"}, "message": "Mark all member functions with memory models always inline v2\n\nWhen a non constant memory model is passed to __atomic_*\ngcc falls back to seq_cst. This drops any HLE acquire or release bits.\n\nThis can happen when <atomic> is used with -O0\nas the member functions are not always inlined then and the memory\nargument passed in ends up being non-constant.\n\nv2: Use _GLIBCXX_ALWAYS_INLINE\n\nlibstdc++-v3/:\n\n2013-05-08  Andi Kleen  <ak@linux.intel.com>\n\n\tPR target/55947\n\t* libstdc++-v3/include/bits/atomic_base.h\n\t(_GLIBCXX_ALWAYS_INLINE): Add new macro.\n\t(atomic_thread_fence, atomic_signal_fence, test_and_set,\n\tclear, store, load, exchange, compare_exchange_weak)\n\tcompare_exchange_strong, fetch_add, fetch_sub, fetch_and,\n\tfetch_or, fetch_xor): Mark _GLIBCXX_ALWAYS_INLINE.\n\nFrom-SVN: r198733", "tree": {"sha": "5cd84b58b1cfba1a839f177accc1bdf34f32a189", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/5cd84b58b1cfba1a839f177accc1bdf34f32a189"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5/comments", "author": null, "committer": null, "parents": [{"sha": "785b887ee8ef5c48ffe99f2a9c5bb9612f98423f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/785b887ee8ef5c48ffe99f2a9c5bb9612f98423f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/785b887ee8ef5c48ffe99f2a9c5bb9612f98423f"}], "stats": {"total": 98, "additions": 56, "deletions": 42}, "files": [{"sha": "2d91012d97431ae66e34cbee625835a97bbbb2a4", "filename": "libstdc++-v3/ChangeLog", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5/libstdc%2B%2B-v3%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5/libstdc%2B%2B-v3%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2FChangeLog?ref=a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5", "patch": "@@ -1,3 +1,13 @@\n+2013-05-08  Andi Kleen  <ak@linux.intel.com>\n+\n+\tPR target/55947\n+\t* libstdc++-v3/include/bits/atomic_base.h\n+\t(_GLIBCXX_ALWAYS_INLINE): Add new macro.\n+\t(atomic_thread_fence, atomic_signal_fence, test_and_set,\n+\tclear, store, load, exchange, compare_exchange_weak)\n+\tcompare_exchange_strong, fetch_add, fetch_sub, fetch_and,\n+\tfetch_or, fetch_xor): Mark _GLIBCXX_ALWAYS_INLINE.\n+\n 2013-05-08  Jason Merrill  <jason@redhat.com>\n \n \tAdd std::bad_array_new_length (N2932)"}, {"sha": "d9d755abba9bb07ac0a56340ea39aa4aaf8bfbce", "filename": "libstdc++-v3/include/bits/atomic_base.h", "status": "modified", "additions": 46, "deletions": 42, "changes": 88, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_base.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_base.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_base.h?ref=a062bb4ba60bf2d3cfca0ebe56d15c4f7c4f0db5", "patch": "@@ -37,6 +37,10 @@\n #include <stdint.h>\n #include <bits/atomic_lockfree_defines.h>\n \n+#ifndef _GLIBCXX_ALWAYS_INLINE\n+#define _GLIBCXX_ALWAYS_INLINE inline __attribute__((always_inline))\n+#endif\n+\n namespace std _GLIBCXX_VISIBILITY(default)\n {\n _GLIBCXX_BEGIN_NAMESPACE_VERSION\n@@ -94,11 +98,11 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       | (__m & __memory_order_modifier_mask));\n   }\n \n-  inline void\n+  _GLIBCXX_ALWAYS_INLINE void\n   atomic_thread_fence(memory_order __m) noexcept\n   { __atomic_thread_fence(__m); }\n \n-  inline void\n+  _GLIBCXX_ALWAYS_INLINE void\n   atomic_signal_fence(memory_order __m) noexcept\n   { __atomic_signal_fence(__m); }\n \n@@ -281,19 +285,19 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       : __atomic_flag_base{ _S_init(__i) }\n     { }\n \n-    bool\n+    _GLIBCXX_ALWAYS_INLINE bool\n     test_and_set(memory_order __m = memory_order_seq_cst) noexcept\n     {\n       return __atomic_test_and_set (&_M_i, __m);\n     }\n \n-    bool\n+    _GLIBCXX_ALWAYS_INLINE bool\n     test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept\n     {\n       return __atomic_test_and_set (&_M_i, __m);\n     }\n \n-    void\n+    _GLIBCXX_ALWAYS_INLINE void\n     clear(memory_order __m = memory_order_seq_cst) noexcept\n     {\n       memory_order __b = __m & __memory_order_mask;\n@@ -304,7 +308,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       __atomic_clear (&_M_i, __m);\n     }\n \n-    void\n+    _GLIBCXX_ALWAYS_INLINE void\n     clear(memory_order __m = memory_order_seq_cst) volatile noexcept\n     {\n       memory_order __b = __m & __memory_order_mask;\n@@ -463,7 +467,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       is_lock_free() const volatile noexcept\n       { return __atomic_is_lock_free(sizeof(_M_i), nullptr); }\n \n-      void\n+      _GLIBCXX_ALWAYS_INLINE void\n       store(__int_type __i, memory_order __m = memory_order_seq_cst) noexcept\n       {\n         memory_order __b = __m & __memory_order_mask;\n@@ -474,7 +478,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \t__atomic_store_n(&_M_i, __i, __m);\n       }\n \n-      void\n+      _GLIBCXX_ALWAYS_INLINE void\n       store(__int_type __i,\n \t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n@@ -486,7 +490,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \t__atomic_store_n(&_M_i, __i, __m);\n       }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       load(memory_order __m = memory_order_seq_cst) const noexcept\n       {\n        memory_order __b = __m & __memory_order_mask;\n@@ -496,7 +500,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \treturn __atomic_load_n(&_M_i, __m);\n       }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       {\n         memory_order __b = __m & __memory_order_mask;\n@@ -506,22 +510,22 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \treturn __atomic_load_n(&_M_i, __m);\n       }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       exchange(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn __atomic_exchange_n(&_M_i, __i, __m);\n       }\n \n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       exchange(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn __atomic_exchange_n(&_M_i, __i, __m);\n       }\n \n-      bool\n+      _GLIBCXX_ALWAYS_INLINE bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t\t    memory_order __m1, memory_order __m2) noexcept\n       {\n@@ -534,7 +538,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 1, __m1, __m2);\n       }\n \n-      bool\n+      _GLIBCXX_ALWAYS_INLINE bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t\t    memory_order __m1,\n \t\t\t    memory_order __m2) volatile noexcept\n@@ -548,23 +552,23 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 1, __m1, __m2);\n       }\n \n-      bool\n+      _GLIBCXX_ALWAYS_INLINE bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_weak(__i1, __i2, __m,\n \t\t\t\t     __cmpexch_failure_order(__m));\n       }\n \n-      bool\n+      _GLIBCXX_ALWAYS_INLINE bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t   memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_weak(__i1, __i2, __m,\n \t\t\t\t     __cmpexch_failure_order(__m));\n       }\n \n-      bool\n+      _GLIBCXX_ALWAYS_INLINE bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t\t      memory_order __m1, memory_order __m2) noexcept\n       {\n@@ -577,7 +581,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0, __m1, __m2);\n       }\n \n-      bool\n+      _GLIBCXX_ALWAYS_INLINE bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) volatile noexcept\n@@ -592,68 +596,68 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0, __m1, __m2);\n       }\n \n-      bool\n+      _GLIBCXX_ALWAYS_INLINE bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t\t      memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_strong(__i1, __i2, __m,\n \t\t\t\t       __cmpexch_failure_order(__m));\n       }\n \n-      bool\n+      _GLIBCXX_ALWAYS_INLINE bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_strong(__i1, __i2, __m,\n \t\t\t\t       __cmpexch_failure_order(__m));\n       }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       fetch_add(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_add(&_M_i, __i, __m); }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       fetch_add(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_add(&_M_i, __i, __m); }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       fetch_sub(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_sub(&_M_i, __i, __m); }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       fetch_sub(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_sub(&_M_i, __i, __m); }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       fetch_and(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_and(&_M_i, __i, __m); }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       fetch_and(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_and(&_M_i, __i, __m); }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       fetch_or(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_or(&_M_i, __i, __m); }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       fetch_or(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_or(&_M_i, __i, __m); }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       fetch_xor(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_xor(&_M_i, __i, __m); }\n \n-      __int_type\n+      _GLIBCXX_ALWAYS_INLINE __int_type\n       fetch_xor(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_xor(&_M_i, __i, __m); }\n@@ -770,7 +774,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       is_lock_free() const volatile noexcept\n       { return __atomic_is_lock_free(_M_type_size(1), nullptr); }\n \n-      void\n+      _GLIBCXX_ALWAYS_INLINE void\n       store(__pointer_type __p,\n \t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n@@ -783,7 +787,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \t__atomic_store_n(&_M_p, __p, __m);\n       }\n \n-      void\n+      _GLIBCXX_ALWAYS_INLINE void\n       store(__pointer_type __p,\n \t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n@@ -795,7 +799,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \t__atomic_store_n(&_M_p, __p, __m);\n       }\n \n-      __pointer_type\n+      _GLIBCXX_ALWAYS_INLINE __pointer_type\n       load(memory_order __m = memory_order_seq_cst) const noexcept\n       {\n         memory_order __b = __m & __memory_order_mask;\n@@ -805,7 +809,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \treturn __atomic_load_n(&_M_p, __m);\n       }\n \n-      __pointer_type\n+      _GLIBCXX_ALWAYS_INLINE __pointer_type\n       load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       {\n         memory_order __b = __m & __memory_order_mask;\n@@ -815,22 +819,22 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \treturn __atomic_load_n(&_M_p, __m);\n       }\n \n-      __pointer_type\n+      _GLIBCXX_ALWAYS_INLINE __pointer_type\n       exchange(__pointer_type __p,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn __atomic_exchange_n(&_M_p, __p, __m);\n       }\n \n \n-      __pointer_type\n+      _GLIBCXX_ALWAYS_INLINE __pointer_type\n       exchange(__pointer_type __p,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn __atomic_exchange_n(&_M_p, __p, __m);\n       }\n \n-      bool\n+      _GLIBCXX_ALWAYS_INLINE bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) noexcept\n@@ -844,7 +848,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \treturn __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0, __m1, __m2);\n       }\n \n-      bool\n+      _GLIBCXX_ALWAYS_INLINE bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) volatile noexcept\n@@ -859,22 +863,22 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \treturn __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0, __m1, __m2);\n       }\n \n-      __pointer_type\n+      _GLIBCXX_ALWAYS_INLINE __pointer_type\n       fetch_add(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_add(&_M_p, _M_type_size(__d), __m); }\n \n-      __pointer_type\n+      _GLIBCXX_ALWAYS_INLINE __pointer_type\n       fetch_add(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_add(&_M_p, _M_type_size(__d), __m); }\n \n-      __pointer_type\n+      _GLIBCXX_ALWAYS_INLINE __pointer_type\n       fetch_sub(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_sub(&_M_p, _M_type_size(__d), __m); }\n \n-      __pointer_type\n+      _GLIBCXX_ALWAYS_INLINE __pointer_type\n       fetch_sub(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_sub(&_M_p, _M_type_size(__d), __m); }"}]}