{"sha": "f7d0c571f76a47e432cecdc04d2d89dc734cd36a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZjdkMGM1NzFmNzZhNDdlNDMyY2VjZGMwNGQyZDg5ZGM3MzRjZDM2YQ==", "commit": {"author": {"name": "Zhenqiang Chen", "email": "zhenqiang.chen@arm.com", "date": "2012-09-19T07:40:15Z"}, "committer": {"name": "Bin Cheng", "email": "amker@gcc.gnu.org", "date": "2012-09-19T07:40:15Z"}, "message": "re PR middle-end/54364 (Tail call jumps not threaded)\n\n\tPR middle-end/54364\n\t* bb-reorder.c (connect_better_edge_p): New added.\n\t(find_traces_1_round): When optimizing for size, ignore edge frequency\n\tand probability, and handle all in one round.\n\t(bb_to_key): Use bb->index as key when optimizing for size.\n\t(better_edge_p): The bb with smaller index is better when optimizing\n\tfor size.\n\t(connect_traces): When optimizing for size, connect block n with\n\tblock n + 1; connect trace m with trace m + 1 if falling through.\n\t(gate_handle_reorder_blocks): Enable bbro when optimizing for -Os.\n\nFrom-SVN: r191462", "tree": {"sha": "d7178172c37ecaa8c9a22d89ab69939840cff4f8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d7178172c37ecaa8c9a22d89ab69939840cff4f8"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f7d0c571f76a47e432cecdc04d2d89dc734cd36a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f7d0c571f76a47e432cecdc04d2d89dc734cd36a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f7d0c571f76a47e432cecdc04d2d89dc734cd36a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f7d0c571f76a47e432cecdc04d2d89dc734cd36a/comments", "author": null, "committer": null, "parents": [{"sha": "878b569c6a4c24edc3bfd36adc1630e9f423278a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/878b569c6a4c24edc3bfd36adc1630e9f423278a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/878b569c6a4c24edc3bfd36adc1630e9f423278a"}], "stats": {"total": 218, "additions": 189, "deletions": 29}, "files": [{"sha": "936b7d0f5881b29fa17b81954018e605385c7ed6", "filename": "gcc/ChangeLog", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7d0c571f76a47e432cecdc04d2d89dc734cd36a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7d0c571f76a47e432cecdc04d2d89dc734cd36a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=f7d0c571f76a47e432cecdc04d2d89dc734cd36a", "patch": "@@ -1,3 +1,16 @@\n+2012-09-19  Zhenqiang Chen <zhenqiang.chen@arm.com>\n+\n+\tPR middle-end/54364\n+\t* bb-reorder.c (connect_better_edge_p): New added.\n+\t(find_traces_1_round): When optimizing for size, ignore edge frequency\n+\tand probability, and handle all in one round.\n+\t(bb_to_key): Use bb->index as key when optimizing for size.\n+\t(better_edge_p): The bb with smaller index is better when optimizing\n+\tfor size.\n+\t(connect_traces): When optimizing for size, connect block n with\n+\tblock n + 1; connect trace m with trace m + 1 if falling through.\n+\t(gate_handle_reorder_blocks): Enable bbro when optimizing for -Os.\n+\n 2012-09-19  Bin Cheng  <bin.cheng@arm.com>\n \n \t* fold-const.c (fold_truth_andor): Remove duplicated check on"}, {"sha": "6c6b456ab7cef38f21be463f554a10793c05c014", "filename": "gcc/bb-reorder.c", "status": "modified", "additions": 176, "deletions": 29, "changes": 205, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7d0c571f76a47e432cecdc04d2d89dc734cd36a/gcc%2Fbb-reorder.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7d0c571f76a47e432cecdc04d2d89dc734cd36a/gcc%2Fbb-reorder.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fbb-reorder.c?ref=f7d0c571f76a47e432cecdc04d2d89dc734cd36a", "patch": "@@ -56,6 +56,20 @@\n    The rest of traces are simply connected so there will be a jump to the\n    beginning of the rest of traces.\n \n+   The above description is for the full algorithm, which is used when the\n+   function is optimized for speed.  When the function is optimized for size,\n+   in order to reduce long jumps and connect more fallthru edges, the\n+   algorithm is modified as follows:\n+   (1) Break long traces to short ones.  A trace is broken at a block that has\n+   multiple predecessors/ successors during trace discovery.  When connecting\n+   traces, only connect Trace n with Trace n + 1.  This change reduces most\n+   long jumps compared with the above algorithm.\n+   (2) Ignore the edge probability and frequency for fallthru edges.\n+   (3) Keep the original order of blocks when there is no chance to fall\n+   through.  We rely on the results of cfg_cleanup.\n+\n+   To implement the change for code size optimization, block's index is\n+   selected as the key and all traces are found in one round.\n \n    References:\n \n@@ -181,6 +195,8 @@ static basic_block copy_bb (basic_block, edge, basic_block, int);\n static fibheapkey_t bb_to_key (basic_block);\n static bool better_edge_p (const_basic_block, const_edge, int, int, int, int,\n \t\t\t   const_edge);\n+static bool connect_better_edge_p (const_edge, bool, int, const_edge,\n+\t\t\t\t   struct trace *);\n static void connect_traces (int, struct trace *);\n static bool copy_bb_p (const_basic_block, int);\n static bool push_to_next_round_p (const_basic_block, int, int, int, gcov_type);\n@@ -440,6 +456,7 @@ find_traces_1_round (int branch_th, int exec_th, gcov_type count_th,\n   /* Heap for discarded basic blocks which are possible starting points for\n      the next round.  */\n   fibheap_t new_heap = fibheap_new ();\n+  bool for_size = optimize_function_for_size_p (cfun);\n \n   while (!fibheap_empty (*heap))\n     {\n@@ -459,10 +476,11 @@ find_traces_1_round (int branch_th, int exec_th, gcov_type count_th,\n       /* If the BB's frequency is too low, send BB to the next round.  When\n \t partitioning hot/cold blocks into separate sections, make sure all\n \t the cold blocks (and ONLY the cold blocks) go into the (extra) final\n-\t round.  */\n+\t round.  When optimizing for size, do not push to next round.  */\n \n-      if (push_to_next_round_p (bb, round, number_of_rounds, exec_th,\n-\t\t\t\tcount_th))\n+      if (!for_size\n+\t  && push_to_next_round_p (bb, round, number_of_rounds, exec_th,\n+\t\t\t\t   count_th))\n \t{\n \t  int key = bb_to_key (bb);\n \t  bbd[bb->index].heap = new_heap;\n@@ -533,10 +551,11 @@ find_traces_1_round (int branch_th, int exec_th, gcov_type count_th,\n \t\t}\n \n \t      /* Edge that cannot be fallthru or improbable or infrequent\n-\t\t successor (i.e. it is unsuitable successor).  */\n+\t\t successor (i.e. it is unsuitable successor).  When optimizing\n+\t\t for size, ignore the probability and frequency.  */\n \t      if (!(e->flags & EDGE_CAN_FALLTHRU) || (e->flags & EDGE_COMPLEX)\n-\t\t  || prob < branch_th || EDGE_FREQUENCY (e) < exec_th\n-\t\t  || e->count < count_th)\n+\t\t  || ((prob < branch_th || EDGE_FREQUENCY (e) < exec_th\n+\t\t      || e->count < count_th) && (!for_size)))\n \t\tcontinue;\n \n \t      /* If partitioning hot/cold basic blocks, don't consider edges\n@@ -558,6 +577,30 @@ find_traces_1_round (int branch_th, int exec_th, gcov_type count_th,\n \t      && copy_bb_p (best_edge->dest, 0))\n \t    best_edge = NULL;\n \n+\t  /* If the best destination has multiple successors or predecessors,\n+\t     don't allow it to be added when optimizing for size.  This makes\n+\t     sure predecessors with smaller index are handled before the best\n+\t     destinarion.  It breaks long trace and reduces long jumps.\n+\n+\t     Take if-then-else as an example.\n+\t\tA\n+\t       / \\\n+\t      B   C\n+\t       \\ /\n+\t\tD\n+\t     If we do not remove the best edge B->D/C->D, the final order might\n+\t     be A B D ... C.  C is at the end of the program.  If D's successors\n+\t     and D are complicated, might need long jumps for A->C and C->D.\n+\t     Similar issue for order: A C D ... B.\n+\n+\t     After removing the best edge, the final result will be ABCD/ ACBD.\n+\t     It does not add jump compared with the previous order.  But it\n+\t     reduces the possiblity of long jumps.  */\n+\t  if (best_edge && for_size\n+\t      && (EDGE_COUNT (best_edge->dest->succs) > 1\n+\t\t || EDGE_COUNT (best_edge->dest->preds) > 1))\n+\t    best_edge = NULL;\n+\n \t  /* Add all non-selected successors to the heaps.  */\n \t  FOR_EACH_EDGE (e, ei, bb->succs)\n \t    {\n@@ -599,11 +642,12 @@ find_traces_1_round (int branch_th, int exec_th, gcov_type count_th,\n \t\t    {\n \t\t      /* When partitioning hot/cold basic blocks, make sure\n \t\t\t the cold blocks (and only the cold blocks) all get\n-\t\t\t pushed to the last round of trace collection.  */\n+\t\t\t pushed to the last round of trace collection.  When\n+\t\t\t optimizing for size, do not push to next round.  */\n \n-\t\t      if (push_to_next_round_p (e->dest, round,\n-\t\t\t\t\t\tnumber_of_rounds,\n-\t\t\t\t\t\texec_th, count_th))\n+\t\t      if (!for_size && push_to_next_round_p (e->dest, round,\n+\t\t\t\t\t\t\t     number_of_rounds,\n+\t\t\t\t\t\t\t     exec_th, count_th))\n \t\t\twhich_heap = new_heap;\n \t\t    }\n \n@@ -685,6 +729,8 @@ find_traces_1_round (int branch_th, int exec_th, gcov_type count_th,\n \t\t  (i.e. 2 * B->frequency >= EDGE_FREQUENCY (AC) )\n \t\t  Best ordering is then A B C.\n \n+\t\t  When optimizing for size, A B C is always the best order.\n+\n \t\t  This situation is created for example by:\n \n \t\t  if (A) B;\n@@ -704,7 +750,8 @@ find_traces_1_round (int branch_th, int exec_th, gcov_type count_th,\n \t\t\t    & EDGE_CAN_FALLTHRU)\n \t\t\t&& !(single_succ_edge (e->dest)->flags & EDGE_COMPLEX)\n \t\t\t&& single_succ (e->dest) == best_edge->dest\n-\t\t\t&& 2 * e->dest->frequency >= EDGE_FREQUENCY (best_edge))\n+\t\t\t&& (2 * e->dest->frequency >= EDGE_FREQUENCY (best_edge)\n+\t\t\t    || for_size))\n \t\t      {\n \t\t\tbest_edge = e;\n \t\t\tif (dump_file)\n@@ -824,6 +871,10 @@ bb_to_key (basic_block bb)\n   edge_iterator ei;\n   int priority = 0;\n \n+  /* Use index as key to align with its original order.  */\n+  if (optimize_function_for_size_p (cfun))\n+    return bb->index;\n+\n   /* Do not start in probably never executed blocks.  */\n \n   if (BB_PARTITION (bb) == BB_COLD_PARTITION\n@@ -869,6 +920,11 @@ better_edge_p (const_basic_block bb, const_edge e, int prob, int freq,\n   int diff_prob = best_prob / 10;\n   int diff_freq = best_freq / 10;\n \n+  /* The smaller one is better to keep the original order.  */\n+  if (optimize_function_for_size_p (cfun))\n+    return !cur_best_edge\n+\t   || cur_best_edge->dest->index > e->dest->index;\n+\n   if (prob > best_prob + diff_prob)\n     /* The edge has higher probability than the temporary best edge.  */\n     is_better_edge = true;\n@@ -904,6 +960,73 @@ better_edge_p (const_basic_block bb, const_edge e, int prob, int freq,\n   return is_better_edge;\n }\n \n+/* Return true when the edge E is better than the temporary best edge\n+   CUR_BEST_EDGE.  If SRC_INDEX_P is true, the function compares the src bb of\n+   E and CUR_BEST_EDGE; otherwise it will compare the dest bb.\n+   BEST_LEN is the trace length of src (or dest) bb in CUR_BEST_EDGE.\n+   TRACES record the information about traces.\n+   When optimizing for size, the edge with smaller index is better.\n+   When optimizing for speed, the edge with bigger probability or longer trace\n+   is better.  */\n+\n+static bool\n+connect_better_edge_p (const_edge e, bool src_index_p, int best_len,\n+\t\t       const_edge cur_best_edge, struct trace *traces)\n+{\n+  int e_index;\n+  int b_index;\n+  bool is_better_edge;\n+\n+  if (!cur_best_edge)\n+    return true;\n+\n+  if (optimize_function_for_size_p (cfun))\n+    {\n+      e_index = src_index_p ? e->src->index : e->dest->index;\n+      b_index = src_index_p ? cur_best_edge->src->index\n+\t\t\t      : cur_best_edge->dest->index;\n+      /* The smaller one is better to keep the original order.  */\n+      return b_index > e_index;\n+    }\n+\n+  if (src_index_p)\n+    {\n+      e_index = e->src->index;\n+\n+      if (e->probability > cur_best_edge->probability)\n+\t/* The edge has higher probability than the temporary best edge.  */\n+\tis_better_edge = true;\n+      else if (e->probability < cur_best_edge->probability)\n+\t/* The edge has lower probability than the temporary best edge.  */\n+\tis_better_edge = false;\n+      else if (traces[bbd[e_index].end_of_trace].length > best_len)\n+\t/* The edge and the temporary best edge have equivalent probabilities.\n+\t   The edge with longer trace is better.  */\n+\tis_better_edge = true;\n+      else\n+\tis_better_edge = false;\n+    }\n+  else\n+    {\n+      e_index = e->dest->index;\n+\n+      if (e->probability > cur_best_edge->probability)\n+\t/* The edge has higher probability than the temporary best edge.  */\n+\tis_better_edge = true;\n+      else if (e->probability < cur_best_edge->probability)\n+\t/* The edge has lower probability than the temporary best edge.  */\n+\tis_better_edge = false;\n+      else if (traces[bbd[e_index].start_of_trace].length > best_len)\n+\t/* The edge and the temporary best edge have equivalent probabilities.\n+\t   The edge with longer trace is better.  */\n+\tis_better_edge = true;\n+      else\n+\tis_better_edge = false;\n+    }\n+\n+  return is_better_edge;\n+}\n+\n /* Connect traces in array TRACES, N_TRACES is the count of traces.  */\n \n static void\n@@ -917,6 +1040,7 @@ connect_traces (int n_traces, struct trace *traces)\n   int current_partition;\n   int freq_threshold;\n   gcov_type count_threshold;\n+  bool for_size = optimize_function_for_size_p (cfun);\n \n   freq_threshold = max_entry_frequency * DUPLICATION_THRESHOLD / 1000;\n   if (max_entry_count < INT_MAX / 1000)\n@@ -980,10 +1104,7 @@ connect_traces (int n_traces, struct trace *traces)\n \t\t  && bbd[si].end_of_trace >= 0\n \t\t  && !connected[bbd[si].end_of_trace]\n \t\t  && (BB_PARTITION (e->src) == current_partition)\n-\t\t  && (!best\n-\t\t      || e->probability > best->probability\n-\t\t      || (e->probability == best->probability\n-\t\t\t  && traces[bbd[si].end_of_trace].length > best_len)))\n+\t\t  && connect_better_edge_p (e, true, best_len, best, traces))\n \t\t{\n \t\t  best = e;\n \t\t  best_len = traces[bbd[si].end_of_trace].length;\n@@ -1026,17 +1147,52 @@ connect_traces (int n_traces, struct trace *traces)\n \t\t  && bbd[di].start_of_trace >= 0\n \t\t  && !connected[bbd[di].start_of_trace]\n \t\t  && (BB_PARTITION (e->dest) == current_partition)\n-\t\t  && (!best\n-\t\t      || e->probability > best->probability\n-\t\t      || (e->probability == best->probability\n-\t\t\t  && traces[bbd[di].start_of_trace].length > best_len)))\n+\t\t  && connect_better_edge_p (e, false, best_len, best, traces))\n \t\t{\n \t\t  best = e;\n \t\t  best_len = traces[bbd[di].start_of_trace].length;\n \t\t}\n \t    }\n \n-\t  if (best)\n+\t  if (for_size)\n+\t    {\n+\t      if (!best)\n+\t\t/* Stop finding the successor traces.  */\n+\t\tbreak;\n+\n+\t      /* It is OK to connect block n with block n + 1 or a block\n+\t\t before n.  For others, only connect to the loop header.  */\n+\t      if (best->dest->index > (traces[t].last->index + 1))\n+\t\t{\n+\t\t  int count = EDGE_COUNT (best->dest->preds);\n+\n+\t\t  FOR_EACH_EDGE (e, ei, best->dest->preds)\n+\t\t    if (e->flags & EDGE_DFS_BACK)\n+\t\t      count--;\n+\n+\t\t  /* If dest has multiple predecessors, skip it.  We expect\n+\t\t     that one predecessor with smaller index connects with it\n+\t\t     later.  */\n+\t\t  if (count != 1) \n+\t\t    break;\n+\t\t}\n+\n+\t      /* Only connect Trace n with Trace n + 1.  It is conservative\n+\t\t to keep the order as close as possible to the original order.\n+\t\t It also helps to reduce long jumps.  */\n+\t      if (last_trace != bbd[best->dest->index].start_of_trace - 1)\n+\t\tbreak;\n+\n+\t      if (dump_file)\n+\t\tfprintf (dump_file, \"Connection: %d %d\\n\",\n+\t\t\t best->src->index, best->dest->index);\n+\n+\t      t = bbd[best->dest->index].start_of_trace;\n+\t      traces[last_trace].last->aux = traces[t].first;\n+\t      connected[t] = true;\n+\t      last_trace = t;\n+\t    }\n+\t  else if (best)\n \t    {\n \t      if (dump_file)\n \t\t{\n@@ -2047,15 +2203,6 @@ gate_handle_reorder_blocks (void)\n {\n   if (targetm.cannot_modify_jumps_p ())\n     return false;\n-  /* Don't reorder blocks when optimizing for size because extra jump insns may\n-     be created; also barrier may create extra padding.\n-\n-     More correctly we should have a block reordering mode that tried to\n-     minimize the combined size of all the jumps.  This would more or less\n-     automatically remove extra jumps, but would also try to use more short\n-     jumps instead of long jumps.  */\n-  if (!optimize_function_for_speed_p (cfun))\n-    return false;\n   return (optimize > 0\n \t  && (flag_reorder_blocks || flag_reorder_blocks_and_partition));\n }"}]}