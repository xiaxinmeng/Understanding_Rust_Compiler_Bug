{"sha": "44cec3004a3519037c88ae0d464f9790ac998053", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NDRjZWMzMDA0YTM1MTkwMzdjODhhZTBkNDY0Zjk3OTBhYzk5ODA1Mw==", "commit": {"author": {"name": "Jeff Law", "email": "law@gcc.gnu.org", "date": "2001-07-09T19:47:27Z"}, "committer": {"name": "Jeff Law", "email": "law@gcc.gnu.org", "date": "2001-07-09T19:47:27Z"}, "message": "Forgot to commit with last batch.\n\nFrom-SVN: r43873", "tree": {"sha": "6a7040f8d80fe7ff4b7a087da4eb54d8dfcb9dbf", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6a7040f8d80fe7ff4b7a087da4eb54d8dfcb9dbf"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/44cec3004a3519037c88ae0d464f9790ac998053", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/44cec3004a3519037c88ae0d464f9790ac998053", "html_url": "https://github.com/Rust-GCC/gccrs/commit/44cec3004a3519037c88ae0d464f9790ac998053", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/44cec3004a3519037c88ae0d464f9790ac998053/comments", "author": null, "committer": null, "parents": [{"sha": "0b47e4c1cdabbb5e7bce3c0358fd82595be2fc60", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0b47e4c1cdabbb5e7bce3c0358fd82595be2fc60", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0b47e4c1cdabbb5e7bce3c0358fd82595be2fc60"}], "stats": {"total": 1221, "additions": 1221, "deletions": 0}, "files": [{"sha": "2316824a13125d3e4defda2ea4f027cb537265ae", "filename": "gcc/ssa-ccp.c", "status": "added", "additions": 1221, "deletions": 0, "changes": 1221, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/44cec3004a3519037c88ae0d464f9790ac998053/gcc%2Fssa-ccp.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/44cec3004a3519037c88ae0d464f9790ac998053/gcc%2Fssa-ccp.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fssa-ccp.c?ref=44cec3004a3519037c88ae0d464f9790ac998053", "patch": "@@ -0,0 +1,1221 @@\n+/* Conditional constant propagation pass for the GNU compiler.\n+   Copyright (C) 2000,2001 Free Software Foundation, Inc.\n+   Original framework by Daniel Berlin <dan@cgsoftware.com>\n+   Fleshed out and major cleanups by Jeff Law <law@redhat.com>\n+   \n+This file is part of GNU CC.\n+   \n+GNU CC is free software; you can redistribute it and/or modify it\n+under the terms of the GNU General Public License as published by the\n+Free Software Foundation; either version 2, or (at your option) any\n+later version.\n+   \n+GNU CC is distributed in the hope that it will be useful, but WITHOUT\n+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+   \n+You should have received a copy of the GNU General Public License\n+along with GNU CC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+/* Conditional constant propagation.\n+\n+   References:\n+\n+     Constant propagation with conditional branches,\n+     Wegman and Zadeck, ACM TOPLAS 13(2):181-210.\n+\n+     Building an Optimizing Compiler,\n+     Robert Morgan, Butterworth-Heinemann, 1998, Section 8.9.\n+\n+     Advanced Compiler Design and Implementation,\n+     Steven Muchnick, Morgan Kaufmann, 1997, Section 12.6\n+\n+   The overall structure is as follows:\n+\n+\t1. Run a simple SSA based DCE pass to remove any dead code.\n+\t2. Run CCP to compute what registers are known constants\n+\t   and what edges are not executable.  Remove unexecutable\n+\t   edges from the CFG and simplify PHI nodes.\n+\t3. Replace registers with constants where possible.\n+\t4. Remove unreachable blocks computed in step #2.\n+\t5. Another simple SSA DCE pass to remove dead code exposed\n+\t   by CCP.\n+\n+   When we exit, we are still in SSA form. \n+\n+\n+   Potential further enhancements:\n+\n+    1. Handle SUBREGs, STRICT_LOW_PART, etc in destinations more\n+       gracefully.\n+\n+    2. Handle insns with multiple outputs more gracefully.\n+\n+    3. Handle CONST_DOUBLE and symbolic constants.\n+\n+    4. Fold expressions after performing constant substitutions.  */\n+\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+\n+#include \"rtl.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"ssa.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"output.h\"\n+#include \"errors.h\"\n+#include \"ggc.h\"\n+#include \"df.h\"\n+#include \"function.h\"\n+\f\n+/* Possible lattice values.  */\n+\n+typedef enum\n+{\n+  UNDEFINED,\n+  CONSTANT,\n+  VARYING\n+} latticevalue;\n+\n+/* Main structure for CCP. \n+\n+   Contains the lattice value and, if it's a constant, the constant\n+   value.  */\n+typedef struct\n+{\n+  latticevalue lattice_val;\n+  rtx const_value;\n+} value;\n+\n+/* Array of values indexed by register number.  */\n+static value *values;\n+\n+/* A bitmap to keep track of executable blocks in the CFG.  */\n+static sbitmap executable_blocks;\n+\n+/* A bitmap for all executable edges in the CFG.  */\n+static sbitmap executable_edges;\n+\n+/* Array of edges on the work list.  */\n+static edge *edge_info;\n+\n+/* We need an edge list to be able to get indexes easily.  */\n+static struct edge_list *edges;\n+\n+/* For building/following use-def and def-use chains.  */\n+static struct df *df_analyzer;\n+\n+/* Current edge we are operating on, from the worklist */\n+static edge flow_edges;\n+\n+/* Bitmap of SSA edges which will need reexamination as their definition\n+   has changed.  */\n+static sbitmap ssa_edges;\n+\n+/* Simple macros to simplify code */\n+#define SSA_NAME(x) REGNO (SET_DEST (x))\n+#define PHI_PARMS(x) XVEC (SET_SRC (x), 0)\n+#define EIE(x,y) EDGE_INDEX (edges, x, y)\n+\n+rtx first_phi_node              PARAMS ((basic_block));\n+static void visit_phi_node             PARAMS ((rtx, basic_block));\n+static void visit_expression           PARAMS ((rtx, basic_block));\n+static void defs_to_undefined          PARAMS ((rtx));\n+static void defs_to_varying            PARAMS ((rtx));\n+static void examine_flow_edges         PARAMS ((void));\n+static void follow_def_use_chains      PARAMS ((void));\n+static void optimize_unexecutable_edges PARAMS ((struct edge_list *, sbitmap));\n+static void ssa_ccp_substitute_constants PARAMS ((void));\n+static void ssa_ccp_df_delete_unreachable_insns PARAMS ((void));\n+\n+/* Return the first PHI node in a basic block.  This routine knows\n+   what INSNs can start a basic block and what can validly follow\n+   them up to the first PHI node.\n+\n+   If the INSN chain or block structures are incorrect, then the behavior\n+   of this routine is undefined.  verify_flow_info will normally catch\n+   these problems in a more graceful manner.  */\n+rtx\n+first_phi_node (block)\n+     basic_block block;\n+{\n+  rtx insn = block->head;\n+\n+  /* Eat the optional CODE_LABEL at the start of the block.  */\n+  if (GET_CODE (insn) == CODE_LABEL)\n+    insn = NEXT_INSN (insn);\n+\n+  /* Eat the mandatory NOTE_INSN_BASIC_BLOCK.  */\n+  if (!NOTE_INSN_BASIC_BLOCK_P (insn) || NOTE_BASIC_BLOCK (insn) != block)\n+    abort ();\n+\n+  /* If there is a PHI node in this block, then it will be the next insn.  */\n+  return NEXT_INSN (insn);\n+}\n+\n+/* Loop through the PHI_NODE's parameters for BLOCK and compare their\n+   lattice values to determine PHI_NODE's lattice value.  */\n+static void\n+visit_phi_node (phi_node, block)\n+     rtx phi_node;\n+     basic_block block;\n+{\n+  unsigned int i;\n+  rtx phi_node_expr = NULL;\n+  unsigned int phi_node_name = SSA_NAME (PATTERN (phi_node));\n+  latticevalue phi_node_lattice_val = UNDEFINED;\n+  rtx pat = PATTERN (phi_node);\n+  rtvec phi_vec = XVEC (SET_SRC (pat), 0);\n+  unsigned int num_elem = GET_NUM_ELEM (phi_vec);\n+\n+  for (i = 0; i < num_elem; i += 2)\n+    {\n+      if (TEST_BIT (executable_edges,\n+\t\t    EIE (BASIC_BLOCK (INTVAL (RTVEC_ELT (phi_vec, i + 1))),\n+\t\t\t block)))\n+\t{\n+\t  unsigned int current_parm\n+\t    = REGNO (RTVEC_ELT (phi_vec, i));\n+\n+\t  latticevalue current_parm_lattice_val\n+\t    = values[current_parm].lattice_val;\n+\n+\t  /* If any node is VARYING, then new value of PHI_NODE\n+\t     is VARYING.  */\n+\t  if (current_parm_lattice_val == VARYING)\n+\t    {\n+\t      phi_node_lattice_val = VARYING;\n+\t      phi_node_expr = NULL;\n+\t      break;\n+\t    }\n+\n+\t  /* If we have more than one distinct constant, then the new\n+\t     value of PHI_NODE is VARYING.  */\n+\t  if (current_parm_lattice_val == CONSTANT\n+\t      && phi_node_lattice_val == CONSTANT\n+\t      && values[current_parm].const_value != phi_node_expr)\n+\t    {\n+\t      phi_node_lattice_val = VARYING;\n+\t      phi_node_expr = NULL;\n+\t      break;\n+\t    }\n+\n+\t  /* If the current value of PHI_NODE is UNDEFINED and one\n+\t     node in PHI_NODE is CONSTANT, then the new value of the\n+\t     PHI is that CONSTANT.  Note this can turn into VARYING\n+\t     if we find another distinct constant later.  */ \n+\t  if (phi_node_lattice_val == UNDEFINED\n+\t      && phi_node_expr == NULL\n+\t      && current_parm_lattice_val == CONSTANT)\n+\t    {\n+\t      phi_node_expr = values[current_parm].const_value;\n+\t      phi_node_lattice_val = CONSTANT;\n+\t      continue;\n+\t    }\n+\t}\n+    }\n+\n+  /* If the value of PHI_NODE changed, then we will need to\n+     re-execute uses of the output of PHI_NODE.  */\n+  if (phi_node_lattice_val != values[phi_node_name].lattice_val)\n+    {\n+      values[phi_node_name].lattice_val = phi_node_lattice_val;\n+      values[phi_node_name].const_value = phi_node_expr;\n+      SET_BIT (ssa_edges, phi_node_name);\n+    }\n+}\n+\n+/* Sets all defs in an insn to UNDEFINED.  */\n+static void\n+defs_to_undefined (insn)\n+     rtx insn;\n+{\n+  struct df_link *currdef;\n+  for (currdef = DF_INSN_DEFS (df_analyzer, insn); currdef;\n+       currdef = currdef->next)\n+    {\n+      if (values[DF_REF_REGNO (currdef->ref)].lattice_val != UNDEFINED)\n+\tSET_BIT (ssa_edges, DF_REF_REGNO (currdef->ref));\n+      values[DF_REF_REGNO (currdef->ref)].lattice_val = UNDEFINED;\n+    }\n+}\n+\n+/* Sets all defs in an insn to VARYING.  */\n+static void\n+defs_to_varying (insn)\n+     rtx insn;\n+{\n+  struct df_link *currdef;\n+  for (currdef = DF_INSN_DEFS (df_analyzer, insn); currdef;\n+       currdef = currdef->next)\n+    {\n+      if (values[DF_REF_REGNO (currdef->ref)].lattice_val != VARYING)\n+\tSET_BIT (ssa_edges, DF_REF_REGNO (currdef->ref));\n+      values[DF_REF_REGNO (currdef->ref)].lattice_val = VARYING;\n+    }\n+}\n+\n+/* Go through the expression, call the approriate evaluation routines\n+   to attempt cprop */\n+static void\n+visit_expression (insn, block)\n+     rtx insn;\n+     basic_block block;\n+{\n+  rtx src, dest, set;\n+\n+  set = single_set (insn);\n+  if (! set)\n+    {\n+      defs_to_varying (insn);\n+      return;\n+    }\n+\n+  src = SET_SRC (set);\n+  dest = SET_DEST (set);\n+\n+  /* We may want to refine this some day.  */\n+  if (GET_CODE (dest) != REG && dest != pc_rtx)\n+    {\n+      defs_to_varying (insn);\n+      return;\n+    }\n+\n+  /* Hard registers are not put in SSA form and thus we must consider\n+     them varying.  All the more reason to avoid hard registers in \n+     RTL until as late as possible in the compilation.  */\n+  if (GET_CODE (dest) == REG && REGNO (dest) < FIRST_PSEUDO_REGISTER)\n+    {\n+      defs_to_varying (insn);\n+      return;\n+    }\n+\n+  /* If this is assigning DEST to a constant, record that fact.  */\n+  if (GET_CODE (src) == CONST_INT && GET_CODE (insn) == INSN)\n+    {\n+      unsigned int resultreg = REGNO (dest);\n+\n+      values[resultreg].lattice_val = CONSTANT;\n+      values[resultreg].const_value = SET_SRC (PATTERN (insn));\n+      SET_BIT (ssa_edges, resultreg);\n+    }\n+\n+  /* If this is a copy operation, then we can copy the lattice values.  */\n+  else if (GET_CODE (src) == REG && GET_CODE (dest) == REG)\n+    {\n+      unsigned int old_value = REGNO (src);\n+      latticevalue old_lattice_value = values[old_value].lattice_val;\n+      unsigned int new_value = REGNO (dest);\n+\n+      /* Unless the lattice value is going to change, don't bother\n+         adding the \"new value\" into the worklist.  */\n+      if (values[new_value].lattice_val != old_lattice_value\n+\t  || values[new_value].const_value != values[old_value].const_value)\n+\tSET_BIT (ssa_edges, new_value);\n+\n+      /* Copy the old lattice node info into the new value lattice node.  */\n+      values[new_value].lattice_val = old_lattice_value;\n+      values[new_value].const_value = values[old_value].const_value;\n+    }\n+\n+  /* Handle jumps.  */\n+  else if (GET_CODE (insn) == JUMP_INSN)\n+    {\n+      rtx x = pc_set (insn);\n+      if (GET_CODE (src) != IF_THEN_ELSE)\n+\t{\n+\t  edge curredge;\n+\n+\t  /* This is a computed jump, table jump, or an unconditional\n+\t     jump.  For all these cases we want to mark all successor\n+\t     blocks as executable if they have not already been\n+\t     marked.\n+\n+\t     One day we may try do better with swtich tables and\n+\t     other computed jumps.  */\n+\t  for (curredge = block->succ; curredge;\n+\t       curredge = curredge->succ_next)\n+\t    {\n+\t      int index = EIE (curredge->src, curredge->dest);\n+\n+\t      if (TEST_BIT (executable_edges, index))\n+\t\tcontinue;\n+\n+\t      SET_BIT (executable_edges, index);\n+\t      edge_info[index] = flow_edges;\n+\t      flow_edges = curredge;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  edge curredge;\n+\t  enum rtx_code comparison_code;\n+\t  rtx comparison_src0;\n+\t  rtx comparison_src1;\n+\n+\t  comparison_code = GET_CODE (XEXP (src, 0));\n+\t  comparison_src0 = XEXP (XEXP (src, 0), 0);\n+\t  comparison_src1 = XEXP (XEXP (src, 0), 1);\n+\n+\t  /* If either operand is undefined, then there is nothing to\n+\t     do right now.  If/when operands are later defined we will\n+\t     revaluate the condition and take the appropriate action.  */\n+\t  if ((GET_CODE (comparison_src0) == REG\n+\t       && values[REGNO (comparison_src0)].lattice_val == UNDEFINED)\n+\t      || (GET_CODE (comparison_src1) == REG\n+\t          && values[REGNO (comparison_src1)].lattice_val == UNDEFINED))\n+\t    return;\n+\n+\t  /* If either operand is varying, then we must consider all\n+\t     paths as executable.  */\n+\t  if ((GET_CODE (comparison_src0) == REG\n+\t       && values[REGNO (comparison_src0)].lattice_val == VARYING)\n+\t      || (GET_CODE (comparison_src1) == REG\n+\t          && values[REGNO (comparison_src1)].lattice_val == VARYING))\n+\t    {\n+\t      for (curredge = block->succ; curredge;\n+\t           curredge = curredge->succ_next)\n+\t        {\n+\t          int index = EIE (curredge->src, curredge->dest);\n+\n+\t          if (TEST_BIT (executable_edges, index))\n+\t\t    continue;\n+\n+\t          SET_BIT (executable_edges, index);\n+\t          edge_info[index] = flow_edges;\n+\t          flow_edges = curredge;\n+\t        }\n+\t      return;\n+\t    }\n+\n+\t  /* Try to simplify the comparison.  */\n+\t  if (GET_CODE (comparison_src0) == REG\n+\t      && values[REGNO (comparison_src0)].lattice_val == CONSTANT)\n+\t    comparison_src0 = values[REGNO (comparison_src0)].const_value;\n+\n+\t  if (GET_CODE (comparison_src1) == REG\n+\t      && values[REGNO (comparison_src1)].lattice_val == CONSTANT)\n+\t    comparison_src1 = values[REGNO (comparison_src1)].const_value;\n+\n+\t  x = simplify_ternary_operation (IF_THEN_ELSE,\n+\t\t\t\t\t  VOIDmode,\n+\t\t\t\t\t  GET_MODE (XEXP (src, 0)),\n+\t\t\t\t\t  gen_rtx (comparison_code,\n+\t\t\t\t\t\t   GET_MODE (XEXP (src, 0)),\n+\t\t\t\t\t\t   comparison_src0,\n+\t\t\t\t\t\t   comparison_src1),\n+\t\t\t\t\t  XEXP (src, 1),\n+\t\t\t\t\t  XEXP (src, 2));\n+\n+\t  /* Walk through all the outgoing edges from this block and see\n+\t     which (if any) we should mark as executable.  */\n+\t  for (curredge = block->succ; curredge;\n+\t       curredge = curredge->succ_next)\n+\t    {\n+\t      int index = EIE (curredge->src, curredge->dest);\n+\n+\t      if (TEST_BIT (executable_edges, index))\n+\t\tcontinue;\n+\n+\t      /* If we were unable to simplify the expression at this\n+\t\t point, it's highly unlikely we'll be able to simplify\n+\t\t it later.  So consider all edges as executable if the\n+\t\t expression did not simplify.\n+\n+\t\t If the expression simplified to (pc), then we know we\n+\t\t will take the fall-thru edge, so mark it.  Similarly,\n+\t\t if the expression simplified to (label_ref ...), then\n+\t\t we know the branch will be taken and we mark that\n+\t\t edge as taken.  */\n+\t      if (!x\n+\t\t  || (x == pc_rtx\n+\t\t      && (curredge->flags & EDGE_FALLTHRU))\n+\t\t  || (GET_CODE (x) == LABEL_REF\n+\t\t      && ! (curredge->flags & EDGE_FALLTHRU)))\n+\t\t{\n+\t\t  SET_BIT (executable_edges, index);\n+\t\t  edge_info[index] = flow_edges;\n+\t\t  flow_edges = curredge;\n+\t\t}\n+\t    }\n+\t}\n+    }\n+  else if (!PHI_NODE_P (insn))\n+    {\n+      rtx simplified = NULL;\n+\n+      /* We've got some kind of INSN.  If it's simple, try to evaluate\n+\t it and record the results. \n+\n+\t We already know this insn is a single_set and that it sets\n+\t a pseudo register.   So we just need to extract the source\n+\t arguments, simplify them to constants if possible, then\n+\t simplify the expression as a whole if possible.  */\n+      switch (GET_RTX_CLASS (GET_CODE (src)))\n+\t{\n+\t  case '<':\n+\t    {\n+\t      rtx src0 = XEXP (src, 0);\n+\t      rtx src1 = XEXP (src, 1);\n+\t      enum machine_mode mode;\n+\n+\t      /* If either is undefined, then the result is undefined.  */\n+\t      if ((GET_CODE (src0) == REG\n+\t\t   && values[REGNO (src0)].lattice_val == UNDEFINED)\n+\t\t  || (GET_CODE (src1) == REG\n+\t\t      && values[REGNO (src1)].lattice_val == UNDEFINED))\n+\t\t{\n+\t\t  defs_to_undefined (insn);\n+\t\t  break;\n+\t\t}\n+\t\t\n+\t      /* Simplify source operands to whatever known values they\n+\t\t may have.  */\n+\t      if (GET_CODE (src0) == REG\n+\t\t  && values[REGNO (src0)].lattice_val == CONSTANT)\n+\t\tsrc0 = values[REGNO (src0)].const_value;\n+\n+\t      if (GET_CODE (src1) == REG\n+\t\t  && values[REGNO (src1)].lattice_val == CONSTANT)\n+\t\tsrc1 = values[REGNO (src1)].const_value;\n+\n+\t      /* See if the simplifier can determine if this operation\n+\t\t computes a constant value.  */\n+\t      mode = GET_MODE (src0);\n+\t      if (mode == VOIDmode)\n+\t\tmode = GET_MODE (src1);\n+\n+\t      simplified = simplify_relational_operation (GET_CODE (src),\n+\t\t\t\t\t\t\t  mode, src0, src1);\n+\t      break;\n+\n+\t    }\n+\n+\t  case '1':\n+\t    {\n+\t      rtx src0 = XEXP (src, 0);\n+\n+\t      /* If the operand is undefined, then the result is undefined.  */\n+\t      if (GET_CODE (src0) == REG\n+\t\t   && values[REGNO (src0)].lattice_val == UNDEFINED)\n+\t\t{\n+\t\t  defs_to_undefined (insn);\n+\t\t  break;\n+\t\t}\n+\t\t\n+\t      /* Simplify source operands to whatever known values they\n+\t\t may have.  */\n+\t      if (GET_CODE (src0) == REG\n+\t\t  && values[REGNO (src0)].lattice_val == CONSTANT)\n+\t\tsrc0 = values[REGNO (src0)].const_value;\n+\n+\t      /* See if the simplifier can determine if this operation\n+\t\t computes a constant value.  */\n+\t      simplified = simplify_unary_operation (GET_CODE (src),\n+\t\t\t\t\t\t     GET_MODE (src),\n+\t\t\t\t\t\t     src0,\n+\t\t\t\t\t\t     GET_MODE (src0));\n+\t      break;\n+\t    }\n+\n+\t  case '2':\n+\t  case 'c':\n+\t    {\n+\t      rtx src0 = XEXP (src, 0);\n+\t      rtx src1 = XEXP (src, 1);\n+\n+\t      /* If either is undefined, then the result is undefined.  */\n+\t      if ((GET_CODE (src0) == REG\n+\t\t   && values[REGNO (src0)].lattice_val == UNDEFINED)\n+\t\t  || (GET_CODE (src1) == REG\n+\t\t      && values[REGNO (src1)].lattice_val == UNDEFINED))\n+\t\t{\n+\t\t  defs_to_undefined (insn);\n+\t\t  break;\n+\t\t}\n+\t\t\n+\t      /* Simplify source operands to whatever known values they\n+\t\t may have.  */\n+\t      if (GET_CODE (src0) == REG\n+\t\t  && values[REGNO (src0)].lattice_val == CONSTANT)\n+\t\tsrc0 = values[REGNO (src0)].const_value;\n+\n+\t      if (GET_CODE (src1) == REG\n+\t\t  && values[REGNO (src1)].lattice_val == CONSTANT)\n+\t\tsrc1 = values[REGNO (src1)].const_value;\n+\n+\t      /* See if the simplifier can determine if this operation\n+\t\t computes a constant value.  */\n+\t      simplified = simplify_binary_operation (GET_CODE (src),\n+\t\t\t\t\t\t      GET_MODE (src),\n+\t\t\t\t\t\t      src0, src1);\n+\t      break;\n+\t    }\n+\n+\t  case '3':\n+\t  case 'b':\n+\t    {\n+\t      rtx src0 = XEXP (src, 0);\n+\t      rtx src1 = XEXP (src, 1);\n+\t      rtx src2 = XEXP (src, 2);\n+\n+\t      /* If either is undefined, then the result is undefined.  */\n+\t      if ((GET_CODE (src0) == REG\n+\t\t   && values[REGNO (src0)].lattice_val == UNDEFINED)\n+\t\t  || (GET_CODE (src1) == REG\n+\t\t      && values[REGNO (src1)].lattice_val == UNDEFINED)\n+\t\t  || (GET_CODE (src2) == REG\n+\t\t      && values[REGNO (src2)].lattice_val == UNDEFINED))\n+\t\t{\n+\t\t  defs_to_undefined (insn);\n+\t\t  break;\n+\t\t}\n+\t\t\n+\t      /* Simplify source operands to whatever known values they\n+\t\t may have.  */\n+\t      if (GET_CODE (src0) == REG\n+\t\t  && values[REGNO (src0)].lattice_val == CONSTANT)\n+\t\tsrc0 = values[REGNO (src0)].const_value;\n+\n+\t      if (GET_CODE (src1) == REG\n+\t\t  && values[REGNO (src1)].lattice_val == CONSTANT)\n+\t\tsrc1 = values[REGNO (src1)].const_value;\n+\n+\t      if (GET_CODE (src2) == REG\n+\t\t  && values[REGNO (src2)].lattice_val == CONSTANT)\n+\t\tsrc2 = values[REGNO (src2)].const_value;\n+\n+\t      /* See if the simplifier can determine if this operation\n+\t\t computes a constant value.  */\n+\t      simplified = simplify_ternary_operation (GET_CODE (src),\n+\t\t\t\t\t\t       GET_MODE (src),\n+\t\t\t\t\t\t       GET_MODE (src),\n+\t\t\t\t\t\t       src0, src1, src2);\n+\t      break;\n+\t    }\n+\t\n+\t  default:\n+\t    defs_to_varying (insn);\n+\t}\n+\n+      if (simplified && GET_CODE (simplified) == CONST_INT)\n+\t{\n+\t  if (values[REGNO (dest)].lattice_val != CONSTANT\n+\t      || values[REGNO (dest)].const_value != simplified)\n+\t    SET_BIT (ssa_edges, REGNO (dest));\n+\n+\t  values[REGNO (dest)].lattice_val = CONSTANT;\n+\t  values[REGNO (dest)].const_value = simplified;\n+\t}\n+      else\n+        defs_to_varying (insn);\n+    }\n+}\n+\n+/* Iterate over the FLOW_EDGES work list.  Simulate the target block\n+   for each edge.  */\n+static void\n+examine_flow_edges (void)\n+{\n+  while (flow_edges != NULL)\n+    {\n+      basic_block succ_block;\n+      rtx curr_phi_node;\n+\n+      /* Pull the next block to simulate off the worklist.  */\n+      succ_block = flow_edges->dest;\n+      flow_edges = edge_info[EIE (flow_edges->src, flow_edges->dest)];\n+\n+      /* There is nothing to do for the exit block.  */\n+      if (succ_block == EXIT_BLOCK_PTR)\n+\tcontinue;\n+\n+      /* Always simulate PHI nodes, even if we have simulated this block\n+\t before.  Note that all PHI nodes are consecutive within a block.  */\n+      for (curr_phi_node = first_phi_node (succ_block);\n+\t   PHI_NODE_P (curr_phi_node);\n+\t   curr_phi_node = NEXT_INSN (curr_phi_node))\n+\tvisit_phi_node (curr_phi_node, succ_block);\n+\n+      /* If this is the first time we've simulated this block, then we\n+\t must simulate each of its insns.  */\n+      if (!TEST_BIT (executable_blocks, succ_block->index))\n+\t{\n+\t  rtx currinsn;\n+\t  edge succ_edge = succ_block->succ;\n+\n+\t  /* Note that we have simulated this block.  */\n+\t  SET_BIT (executable_blocks, succ_block->index);\n+\n+\t  /* Simulate each insn within the block.  */\n+\t  currinsn = succ_block->head;\n+\t  while (currinsn != succ_block->end)\n+\t    {\n+\t      if (INSN_P (currinsn))\n+\t\tvisit_expression (currinsn, succ_block);\n+\n+\t      currinsn = NEXT_INSN (currinsn);\n+\t    }\n+\t  \n+\t  /* Don't forget the last insn in the block.  */\n+\t  if (INSN_P (currinsn))\n+\t    visit_expression (currinsn, succ_block);\n+\t  \n+\t  /* If we haven't looked at the next block, and it has a\n+\t     single successor, add it onto the worklist.  This is because\n+\t     if we only have one successor, we know it gets executed,\n+\t     so we don't have to wait for cprop to tell us. */\n+\t  if (succ_edge != NULL\n+\t      && succ_edge->succ_next == NULL\n+\t      && !TEST_BIT (executable_edges,\n+\t\t\t    EIE (succ_edge->src, succ_edge->dest)))\n+\t    {\n+\t      SET_BIT (executable_edges,\n+\t\t       EIE (succ_edge->src, succ_edge->dest));\n+\t      edge_info[EIE (succ_edge->src, succ_edge->dest)] = flow_edges;\n+\t      flow_edges = succ_edge;\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Follow the def-use chains for each definition on the worklist and\n+   simulate the uses of the definition.  */\n+\n+static void\n+follow_def_use_chains ()\n+{\n+  /* Iterate over all the entries on the SSA_EDGES worklist.  */\n+  while (sbitmap_first_set_bit (ssa_edges) >= 0)\n+    {\n+      int member;\n+      struct df_link *curruse;\n+\n+      /* Pick an entry off the worklist (it does not matter which\n+\t entry we pick).  */\n+      member = sbitmap_first_set_bit (ssa_edges); \n+      RESET_BIT (ssa_edges, member);\n+\n+      /* Iterate through all the uses of this entry.  */\n+      for (curruse = df_analyzer->regs[member].uses; curruse;\n+\t   curruse = curruse->next)\n+\t{\n+\t  rtx useinsn;\n+\n+\t  useinsn = DF_REF_INSN (curruse->ref);\n+\t  if (PHI_NODE_P (useinsn))\n+\t    {\n+\t      if (TEST_BIT (executable_blocks, BLOCK_NUM (useinsn)))\n+\t\tvisit_phi_node (useinsn, BLOCK_FOR_INSN (useinsn));\n+\t    }\t  \n+\t  else\n+\t    {\n+\t      if (TEST_BIT (executable_blocks, BLOCK_NUM (useinsn)))\n+\t\tvisit_expression (useinsn, BLOCK_FOR_INSN (useinsn));\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Examine each edge to see if we were able to prove any were\n+   not executable. \n+\n+   If an edge is not executable, then we can remove its alternative\n+   in PHI nodes as the destination of the edge, we can simplify the\n+   conditional branch at the source of the edge, and we can remove\n+   the edge from the CFG.  Note we do not delete unreachable blocks\n+   yet as the DF analyzer can not deal with that yet.  */\n+static void\n+optimize_unexecutable_edges (edges, executable_edges)\n+     struct edge_list *edges;\n+     sbitmap executable_edges;\n+{\n+  int i;\n+\n+  for (i = 0; i < NUM_EDGES (edges); i++)\n+    {\n+      if (!TEST_BIT (executable_edges, i))\n+\t{\n+\t  edge edge = INDEX_EDGE (edges, i);\n+\n+\t  if (edge->flags & EDGE_ABNORMAL)\n+\t    continue;\n+\n+\t  /* We found an edge that is not executable.  First simplify\n+\t     the PHI nodes in the target block.  */\n+\t  if (edge->dest != EXIT_BLOCK_PTR)\n+\t    {\n+\t      rtx insn = first_phi_node (edge->dest);\n+\n+\t      while (PHI_NODE_P (insn))\n+\t\t{\n+\t\t  remove_phi_alternative (PATTERN (insn), edge->src);\n+\t\t  insn = NEXT_INSN (insn);\n+\t\t}\n+\t    }\n+\n+\t  /* Since the edge was not executable, remove it from the CFG.  */\n+\t  remove_edge (edge);\n+\t}\n+    }\n+\n+  /* We have removed all the unexecutable edges from the CFG.  Fix up\n+     the conditional jumps at the end of any affected block.\n+\n+     We have three cases to deal with:\n+\n+       a. Both outgoing edges are not executable.  This happens if the\n+\t  source block is not reachable.  We will deal with this by\n+\t  deleting all the insns in the block later.\n+\n+       b. The fall-thru edge is not executable.  In this case we\n+\t  change the conditional jump into an unconditional jump and\n+\t  add a BARRIER after the unconditional jump.  Note that since\n+\t  we are working on generic RTL we can change the jump in-place\n+\t  instead of dealing with the headache of reemitting the jump.\n+\n+       c. The branch taken edge is not executable.  In this case\n+\t  we turn the jump into (set (pc) (pc)) which is a nop-jump\n+          and we will remove the unrecognizable insn later.\n+\n+     In cases B & C we are removing uses of registers, so make sure\n+     to note those changes for the DF analyzer.  */\n+\n+  for (i = 0; i < n_basic_blocks; i++)\n+    {\n+      basic_block bb = BASIC_BLOCK (i);\n+      rtx insn = bb->end;\n+      edge edge = bb->succ;\n+\n+      /* If we have no predecessors, then this block is unreachable and\n+\t will be cleaned up when we remove unreachable blocks.  */\n+      if (bb->pred == NULL || GET_CODE (insn) != JUMP_INSN)\n+\tcontinue;\n+\n+      /* If this block ends in a conditional jump, but only has one\n+\t successor, then the jump needs adjustment.  */\n+      if (condjump_p (insn) && ! simplejump_p (insn)\n+\t  && bb->succ && bb->succ->succ_next == NULL)\n+\t{\n+\t  /* If the fallthru edge is the executable edge, then turn\n+\t     this jump into a nop jump, otherwise make it an unconditinoal\n+\t     jump to its target.  */\n+\t  if (edge->flags & EDGE_FALLTHRU)\n+\t    {\n+\t      PUT_CODE (insn, NOTE);\n+\t      NOTE_LINE_NUMBER (insn) = NOTE_INSN_DELETED;\n+\t    }\n+\t  else\n+\t    {\n+\t      SET_SRC (PATTERN (insn)) = gen_rtx_LABEL_REF (Pmode,\n+\t\t\t\t\t\t\t    JUMP_LABEL (insn));\n+\t      emit_barrier_after (insn);\n+\t      INSN_CODE (insn) = -1;\n+\t    }\n+\n+\t  /* Inform the DF analyzer that this insn changed.  */\n+\t  df_insn_modify (df_analyzer, BLOCK_FOR_INSN (insn), insn);\n+\t}\n+    }\n+}\n+ \n+/* Perform substitution of known values for pseudo registers.\n+\n+   ??? Note we do not do simplifications or constant folding here, it\n+   is unlikely that any significant simplifications can be done here\n+   anyway.  Consider that if the simplification would result in an\n+   expression that produces a constant value that the value would\n+   have been discovered and recorded already.\n+   \n+   We perform two transformations.  First, we initialize pseudos to their\n+   known constant values at their definition point.  Second, we try to\n+   replace uses with the known constant value.  */\n+\n+static void\n+ssa_ccp_substitute_constants ()\n+{\n+  int i;\n+\n+  for (i = FIRST_PSEUDO_REGISTER; i < VARRAY_SIZE (ssa_definition); i++)\n+    {\n+      if (values[i].lattice_val == CONSTANT)\n+\t{\n+\t  rtx def = VARRAY_RTX (ssa_definition, i);\n+\t  rtx set = single_set (def);\n+\t  struct df_link *curruse;\n+\n+\t  if (! set)\n+\t    continue;\n+\n+\t  /* Do not try to simplify PHI nodes down to a constant load.\n+\t     That will be done later as we translate out of SSA.  Also,\n+\t     doing that here could violate the rule that all PHI nodes\n+\t     are consecutive at the start of the basic block.  */\n+\t  if (! PHI_NODE_P (def))\n+\t    {\n+\t      SET_SRC (set) = values[i].const_value;\n+\t      INSN_CODE (def) = -1;\n+\t      df_insn_modify (df_analyzer, BLOCK_FOR_INSN (def), def);\n+\t    }\n+\n+\t  /* Iterate through all the uses of this entry and try replacements\n+\t     there too.  Note it is not particularly profitable to try\n+\t     and fold/simplify expressions here as most of the common\n+\t     cases were handled above.  */\n+\t  for (curruse = df_analyzer->regs[i].uses;\n+\t       curruse;\n+\t       curruse = curruse->next)\n+\t    {\n+\t      rtx useinsn;\n+\n+\t      useinsn = DF_REF_INSN (curruse->ref);\n+\n+\t      if (!INSN_DELETED_P (useinsn)\n+\t\t  && ! (GET_CODE (useinsn) == NOTE\n+\t\t\t&& NOTE_LINE_NUMBER (useinsn) == NOTE_INSN_DELETED)\n+\t\t  && (GET_CODE (useinsn) == INSN\n+\t\t      || GET_CODE (useinsn) == JUMP_INSN))\n+\t\t{\n+\t\t  validate_replace_src (regno_reg_rtx [i],\n+\t\t\t\t\tvalues[i].const_value,\n+\t\t\t\t\tuseinsn);\n+\t\t  INSN_CODE (useinsn) = -1;\n+\t\t  df_insn_modify (df_analyzer,\n+\t\t\t\t  BLOCK_FOR_INSN (useinsn),\n+\t\t\t\t  useinsn);\n+\t\t}\n+\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Now find all unreachable basic blocks.  All the insns in those\n+   blocks are unreachable, so delete them and mark any necessary\n+   updates for the DF analyzer.  */\n+\n+static void\n+ssa_ccp_df_delete_unreachable_insns ()\n+{\n+  int i;\n+\n+  /* Use the CFG to find all the reachable blocks.  */\n+  find_unreachable_blocks ();\n+\n+  /* Now we know what blocks are not reachable.  Mark all the insns\n+     in those blocks as deleted for the DF analyzer.   We'll let the\n+     normal flow code actually remove the unreachable blocks.  */\n+  for (i = n_basic_blocks - 1; i >= 0; --i)\n+    {\n+      basic_block b = BASIC_BLOCK (i);\n+\n+      if (b->aux != NULL)\n+\t/* This block was found.  Tidy up the mark.  */\n+\tb->aux = NULL;\n+      else\n+\t{\n+\t  rtx start = b->head;\n+\t  rtx end = b->end;\n+\t  rtx tmp;\n+\n+\t  /* Include any jump table following the basic block.  */\n+\t  end = b->end;\n+\t  if (GET_CODE (end) == JUMP_INSN\n+\t      && (tmp = JUMP_LABEL (end)) != NULL_RTX\n+\t      && (tmp = NEXT_INSN (tmp)) != NULL_RTX\n+\t      && GET_CODE (tmp) == JUMP_INSN\n+\t      && (GET_CODE (PATTERN (tmp)) == ADDR_VEC\n+\t          || GET_CODE (PATTERN (tmp)) == ADDR_DIFF_VEC))\n+\t    end = tmp;\n+\n+\t  while (1)\n+\t    {\n+\t      rtx next = NEXT_INSN (start);\n+\n+\t      if (GET_CODE (start) == INSN\n+\t\t  || GET_CODE (start) == CALL_INSN\n+\t\t  || GET_CODE (start) == JUMP_INSN)\n+\t\tdf_insn_delete (df_analyzer, BLOCK_FOR_INSN (start), start);\n+\n+\t      if (start == end)\n+\t\tbreak;\n+\t      start = next;\n+\t    }\n+\t}\n+    }\n+}\n+\n+\n+/* Main entry point for SSA Conditional Constant Propagation.\n+\n+   Long term it should accept as input the specific flow graph to\n+   operate on so that it can be called for sub-graphs.  */\n+\n+void\n+ssa_const_prop (void)\n+{\n+  unsigned int i;\n+  edge curredge;\n+\n+  /* We need alias analysis (for what?) */\n+  init_alias_analysis ();\n+\n+  df_analyzer = df_init ();\n+  df_analyse (df_analyzer, 0,\n+\t      DF_RD_CHAIN | DF_RU_CHAIN | DF_REG_INFO | DF_HARD_REGS);\n+\n+  /* We need mappings from insn to its containing block.  */\n+  compute_bb_for_insn (get_max_uid ());\n+\n+  /* Perform a quick and dirty dead code elimination pass.  This is not\n+     as aggressive as it could be, but it's good enough to clean up a\n+     lot of unwanted junk and it is fast.  */\n+  ssa_fast_dce (df_analyzer);\n+\n+  /* Build an edge list from the CFG.  */\n+  edges = create_edge_list ();\n+\n+  /* Initialize the values array with everything as undefined.  */\n+  values = (value *) xmalloc (VARRAY_SIZE (ssa_definition) * sizeof (value));\n+  for (i = 0; i < VARRAY_SIZE (ssa_definition); i++)\n+    {\n+      if (i < FIRST_PSEUDO_REGISTER)\n+        values[i].lattice_val = VARYING;\n+      else\n+\tvalues[i].lattice_val = UNDEFINED;\n+      values[i].const_value = NULL;\n+    }\n+\n+  ssa_edges = sbitmap_alloc (VARRAY_SIZE (ssa_definition));\n+  sbitmap_zero (ssa_edges);\n+\n+  executable_blocks = sbitmap_alloc (n_basic_blocks);\n+  sbitmap_zero (executable_blocks);\n+\n+  executable_edges = sbitmap_alloc (NUM_EDGES (edges));\n+  sbitmap_zero (executable_edges);\n+\n+  edge_info = (edge *) xmalloc (NUM_EDGES (edges) * sizeof (edge));\n+  flow_edges = ENTRY_BLOCK_PTR->succ;\n+\n+  /* Add the successors of the entry block to the edge worklist.  That\n+     is enough of a seed to get SSA-CCP started.  */\n+  for (curredge = ENTRY_BLOCK_PTR->succ; curredge;\n+       curredge = curredge->succ_next)\n+    {\n+      int index = EIE (curredge->src, curredge->dest);\n+      SET_BIT (executable_edges, index);\n+      edge_info[index] = curredge->succ_next;\n+    }\n+\n+  /* Iterate until until the worklists are empty.  */\n+  do\n+    {\n+      examine_flow_edges ();\n+      follow_def_use_chains ();\n+    }\n+  while (flow_edges != NULL);\n+\n+  /* Now perform substitutions based on the known constant values.  */\n+  ssa_ccp_substitute_constants ();\n+\n+  /* Remove unexecutable edges from the CFG and make appropriate\n+     adjustments to PHI nodes.  */\n+  optimize_unexecutable_edges (edges, executable_edges);\n+\n+  /* Now remove all unreachable insns and update the DF information.\n+     as appropriate.  */\n+  ssa_ccp_df_delete_unreachable_insns ();\n+\n+#if 0\n+  /* The DF analyzer expects the number of blocks to remain constant,\n+     so we can't remove unreachable blocks.\n+\n+     Code the DF analyzer calls expects there to be no unreachable\n+     blocks in the CFG.  So we can't leave unreachable blocks in the\n+     CFG.\n+\n+     So, there is no way to do an incremental update of the DF data\n+     at this point.  */\n+  df_analyse (df_analyzer, 0,\n+\t      DF_RD_CHAIN | DF_RU_CHAIN | DF_REG_INFO | DF_HARD_REGS);\n+#endif\n+\n+  /* Clean up any dead code exposed by SSA-CCP, do this after updating\n+     the dataflow information!  */\n+  ssa_fast_dce (df_analyzer);\n+\n+  free (values);\n+  values = NULL;\n+\n+  free (edge_info);\n+  edge_info = NULL;\n+\n+  sbitmap_free (executable_blocks);\n+  executable_blocks = NULL;\n+\n+  free_edge_list (edges);\n+  edges = NULL;\n+\n+  sbitmap_free (executable_edges);\n+  executable_edges = NULL;\n+\n+  df_finish (df_analyzer);\n+  end_alias_analysis ();\n+}\n+\n+static int\n+mark_references (current_rtx, data)\n+     rtx *current_rtx;\n+     void *data;\n+{\n+  rtx x = *current_rtx;\n+  sbitmap worklist = (sbitmap)data;\n+\n+  if (x == NULL_RTX)\n+    return 0;\n+\n+  if (GET_CODE (x) == SET)\n+    {\n+      rtx dest = SET_DEST (x);\n+\n+      if (GET_CODE (dest) == STRICT_LOW_PART\n+\t  || GET_CODE (dest) == SUBREG\n+\t  || GET_CODE (dest) == SIGN_EXTRACT\n+\t  || GET_CODE (dest) == ZERO_EXTRACT)\n+\t{\n+\t  rtx reg;\n+\n+\t  reg = dest;\n+\n+\t  while (GET_CODE (reg) == STRICT_LOW_PART\n+\t\t || GET_CODE (reg) == SUBREG\n+\t\t || GET_CODE (reg) == SIGN_EXTRACT\n+\t\t || GET_CODE (reg) == ZERO_EXTRACT)\n+\t    reg = XEXP (reg, 0);\n+\n+\t  if (GET_CODE (reg) == REG)\n+\t    SET_BIT (worklist, REGNO (reg));\n+\t}\n+\n+      if (GET_CODE (dest) == REG)\n+\t{\n+\t  for_each_rtx (&SET_SRC (x), mark_references, data);\n+\t  return -1;\n+\t}\n+\n+      return 0;\n+    }\n+  else if (GET_CODE (x) == REG)\n+    {\n+      SET_BIT (worklist, REGNO (x));\n+      return -1;\n+    }\n+  else if (GET_CODE (x) == CLOBBER)\n+    return -1;\n+  else\n+    return 0;\n+}\n+\n+static void\n+ssa_fast_dce (df)\n+     struct df *df;\n+{\n+  sbitmap worklist = sbitmap_alloc (VARRAY_SIZE (ssa_definition));\n+  sbitmap_ones (worklist);\n+\n+  /* Iterate on the worklist until there's no definitions left to\n+     examine.  */\n+  while (sbitmap_first_set_bit (worklist) >= 0)\n+    {\n+      struct df_link *curruse;\n+      int reg, found_use;\n+\n+      /* Remove an item from the worklist.  */\n+      reg = sbitmap_first_set_bit (worklist);\n+      RESET_BIT (worklist, reg);\n+\n+      /* We never consider deleting assignments to hard regs or things\n+\t which do not have SSA definitions, or things we have already\n+\t deleted, or things with unusual side effects.  */\n+      if (reg < FIRST_PSEUDO_REGISTER\n+\t  || ! VARRAY_RTX (ssa_definition, reg)\n+\t  || INSN_DELETED_P (VARRAY_RTX (ssa_definition, reg))\n+\t  || (GET_CODE (VARRAY_RTX (ssa_definition, reg)) == NOTE\n+\t      && (NOTE_LINE_NUMBER (VARRAY_RTX (ssa_definition, reg))\n+\t\t  == NOTE_INSN_DELETED))\n+\t  || side_effects_p (PATTERN (VARRAY_RTX (ssa_definition, reg))))\n+\tcontinue;\n+      \n+      /* Iterate over the uses of this register.  If we can not find\n+\t any uses that have not been deleted, then the definition of\n+\t this register is dead.  */\n+      found_use = 0;\n+      for (curruse = df->regs[reg].uses; curruse; curruse = curruse->next)\n+\t{\n+\t  rtx useinsn;\n+\n+\t  if (curruse->ref\n+\t      && DF_REF_INSN (curruse->ref)\n+\t      && ! INSN_DELETED_P (DF_REF_INSN (curruse->ref))\n+\t      && ! (GET_CODE (DF_REF_INSN (curruse->ref)) == NOTE\n+\t\t    && (NOTE_LINE_NUMBER (DF_REF_INSN (curruse->ref))\n+\t\t\t== NOTE_INSN_DELETED))\n+\t      && DF_REF_INSN (curruse->ref) != VARRAY_RTX (ssa_definition, reg))\n+\t    {\n+\t      found_use = 1;\n+\t      break;\n+\t    }\n+\t}\n+\n+      /* If we did not find a use of this register, then the definition\n+\t of this register is dead.  */\n+\t     \n+      if (! found_use)\n+\t{\n+\t  rtx def = VARRAY_RTX (ssa_definition, reg);\n+\n+\t  /* Add all registers referenced by INSN to the work\n+\t     list.  */\n+\t  for_each_rtx (&PATTERN (def), mark_references, worklist);\n+\n+\t  /* Inform the analyzer that this insn is going to be\n+\t     deleted.  */\n+\t  df_insn_delete (df, BLOCK_FOR_INSN (def), def);\n+\n+\t  if (PHI_NODE_P (def))\n+\t    {\n+\t      if (def == BLOCK_FOR_INSN (def)->head\n+\t\t  && def == BLOCK_FOR_INSN (def)->end)\n+\t\t{\n+\t\t  /* Delete it.  */\n+\t\t  PUT_CODE (def, NOTE);\n+\t\t  NOTE_LINE_NUMBER (def) = NOTE_INSN_DELETED;\n+\t\t}\n+\t      else if (def == BLOCK_FOR_INSN (def)->head)\n+\t        {\n+\t\t  BLOCK_FOR_INSN (def)->head = NEXT_INSN (def);\n+\t\t  flow_delete_insn (def);\n+\t\t}\n+\t      else if (def == BLOCK_FOR_INSN (def)->end)\n+\t\t{\n+\t\t  BLOCK_FOR_INSN (def)->end = PREV_INSN (def);\n+\t\t  flow_delete_insn (def);\n+\t\t}\n+\t      else\n+\t\tflow_delete_insn (def);\n+\t    }\n+\t  else\n+\t    {\n+\t      flow_delete_insn (def);\n+\t    }\n+\t  VARRAY_RTX (ssa_definition, reg) = NULL;\n+\t}\n+    }\n+}"}]}