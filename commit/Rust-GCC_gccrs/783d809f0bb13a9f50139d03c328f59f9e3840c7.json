{"sha": "783d809f0bb13a9f50139d03c328f59f9e3840c7", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NzgzZDgwOWYwYmIxM2E5ZjUwMTM5ZDAzYzMyOGY1OWY5ZTM4NDBjNw==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-08-05T13:03:23Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-08-05T13:03:23Z"}, "message": "vect: Move costing helpers from aarch64 code\n\naarch64.c has various routines to test for specific kinds of\nvector statement cost.  The routines aren't really target-specific,\nso following a suggestion from Richi, this patch moves them to a new\nsection of tree-vectorizer.h.\n\ngcc/\n\t* tree-vectorizer.h (vect_is_store_elt_extraction, vect_is_reduction)\n\t(vect_reduc_type, vect_embedded_comparison_type, vect_comparison_type)\n\t(vect_is_extending_load, vect_is_integer_truncation): New functions,\n\tmoved from aarch64.c but given different names.\n\t* config/aarch64/aarch64.c (aarch64_is_store_elt_extraction)\n\t(aarch64_is_reduction, aarch64_reduc_type)\n\t(aarch64_embedded_comparison_type, aarch64_comparison_type)\n\t(aarch64_extending_load_p, aarch64_integer_truncation_p): Delete\n\tin favor of the above.  Update callers accordingly.", "tree": {"sha": "9508468cf3b62e3ba71bb3249fe5fb1a2540d75e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9508468cf3b62e3ba71bb3249fe5fb1a2540d75e"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/783d809f0bb13a9f50139d03c328f59f9e3840c7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/783d809f0bb13a9f50139d03c328f59f9e3840c7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/783d809f0bb13a9f50139d03c328f59f9e3840c7", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/783d809f0bb13a9f50139d03c328f59f9e3840c7/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "c1cdabe3aab817d95a8db00a8b5e9f6bcdea936f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c1cdabe3aab817d95a8db00a8b5e9f6bcdea936f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c1cdabe3aab817d95a8db00a8b5e9f6bcdea936f"}], "stats": {"total": 229, "additions": 118, "deletions": 111}, "files": [{"sha": "4cd4b037f2606e515ad8f4669d2cd13a509dd0a4", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 14, "deletions": 111, "changes": 125, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/783d809f0bb13a9f50139d03c328f59f9e3840c7/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/783d809f0bb13a9f50139d03c328f59f9e3840c7/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=783d809f0bb13a9f50139d03c328f59f9e3840c7", "patch": "@@ -14820,40 +14820,6 @@ aarch64_builtin_vectorization_cost (enum vect_cost_for_stmt type_of_cost,\n     }\n }\n \n-/* Return true if an operaton of kind KIND for STMT_INFO represents\n-   the extraction of an element from a vector in preparation for\n-   storing the element to memory.  */\n-static bool\n-aarch64_is_store_elt_extraction (vect_cost_for_stmt kind,\n-\t\t\t\t stmt_vec_info stmt_info)\n-{\n-  return (kind == vec_to_scalar\n-\t  && STMT_VINFO_DATA_REF (stmt_info)\n-\t  && DR_IS_WRITE (STMT_VINFO_DATA_REF (stmt_info)));\n-}\n-\n-/* Return true if STMT_INFO represents part of a reduction.  */\n-static bool\n-aarch64_is_reduction (stmt_vec_info stmt_info)\n-{\n-  return (STMT_VINFO_REDUC_DEF (stmt_info)\n-\t  || VECTORIZABLE_CYCLE_DEF (STMT_VINFO_DEF_TYPE (stmt_info)));\n-}\n-\n-/* If STMT_INFO describes a reduction, return the type of reduction\n-   it describes, otherwise return -1.  */\n-static int\n-aarch64_reduc_type (vec_info *vinfo, stmt_vec_info stmt_info)\n-{\n-  if (loop_vec_info loop_vinfo = dyn_cast<loop_vec_info> (vinfo))\n-    if (STMT_VINFO_REDUC_DEF (stmt_info))\n-      {\n-\tstmt_vec_info reduc_info = info_for_reduction (loop_vinfo, stmt_info);\n-\treturn int (STMT_VINFO_REDUC_TYPE (reduc_info));\n-      }\n-  return -1;\n-}\n-\n /* Return true if an access of kind KIND for STMT_INFO represents one\n    vector of an LD[234] or ST[234] operation.  Return the total number of\n    vectors (2, 3 or 4) if so, otherwise return a value outside that range.  */\n@@ -14874,32 +14840,6 @@ aarch64_ld234_st234_vectors (vect_cost_for_stmt kind, stmt_vec_info stmt_info)\n   return 0;\n }\n \n-/* If STMT_INFO is a COND_EXPR that includes an embedded comparison, return the\n-   scalar type of the values being compared.  Return null otherwise.  */\n-static tree\n-aarch64_embedded_comparison_type (stmt_vec_info stmt_info)\n-{\n-  if (auto *assign = dyn_cast<gassign *> (stmt_info->stmt))\n-    if (gimple_assign_rhs_code (assign) == COND_EXPR)\n-      {\n-\ttree cond = gimple_assign_rhs1 (assign);\n-\tif (COMPARISON_CLASS_P (cond))\n-\t  return TREE_TYPE (TREE_OPERAND (cond, 0));\n-      }\n-  return NULL_TREE;\n-}\n-\n-/* If STMT_INFO is a comparison or contains an embedded comparison, return the\n-   scalar type of the values being compared.  Return null otherwise.  */\n-static tree\n-aarch64_comparison_type (stmt_vec_info stmt_info)\n-{\n-  if (auto *assign = dyn_cast<gassign *> (stmt_info->stmt))\n-    if (TREE_CODE_CLASS (gimple_assign_rhs_code (assign)) == tcc_comparison)\n-      return TREE_TYPE (gimple_assign_rhs1 (assign));\n-  return aarch64_embedded_comparison_type (stmt_info);\n-}\n-\n /* Return true if creating multiple copies of STMT_INFO for Advanced SIMD\n    vectors would produce a series of LDP or STP operations.  KIND is the\n    kind of statement that STMT_INFO represents.  */\n@@ -14926,43 +14866,6 @@ aarch64_advsimd_ldp_stp_p (enum vect_cost_for_stmt kind,\n   return is_gimple_assign (stmt_info->stmt);\n }\n \n-/* Return true if STMT_INFO extends the result of a load.  */\n-static bool\n-aarch64_extending_load_p (class vec_info *vinfo, stmt_vec_info stmt_info)\n-{\n-  gassign *assign = dyn_cast <gassign *> (stmt_info->stmt);\n-  if (!assign || !CONVERT_EXPR_CODE_P (gimple_assign_rhs_code (assign)))\n-    return false;\n-\n-  tree rhs = gimple_assign_rhs1 (stmt_info->stmt);\n-  tree lhs_type = TREE_TYPE (gimple_assign_lhs (assign));\n-  tree rhs_type = TREE_TYPE (rhs);\n-  if (!INTEGRAL_TYPE_P (lhs_type)\n-      || !INTEGRAL_TYPE_P (rhs_type)\n-      || TYPE_PRECISION (lhs_type) <= TYPE_PRECISION (rhs_type))\n-    return false;\n-\n-  stmt_vec_info def_stmt_info = vinfo->lookup_def (rhs);\n-  return (def_stmt_info\n-\t  && STMT_VINFO_DATA_REF (def_stmt_info)\n-\t  && DR_IS_READ (STMT_VINFO_DATA_REF (def_stmt_info)));\n-}\n-\n-/* Return true if STMT_INFO is an integer truncation.  */\n-static bool\n-aarch64_integer_truncation_p (stmt_vec_info stmt_info)\n-{\n-  gassign *assign = dyn_cast <gassign *> (stmt_info->stmt);\n-  if (!assign || !CONVERT_EXPR_CODE_P (gimple_assign_rhs_code (assign)))\n-    return false;\n-\n-  tree lhs_type = TREE_TYPE (gimple_assign_lhs (assign));\n-  tree rhs_type = TREE_TYPE (gimple_assign_rhs1 (assign));\n-  return (INTEGRAL_TYPE_P (lhs_type)\n-\t  && INTEGRAL_TYPE_P (rhs_type)\n-\t  && TYPE_PRECISION (lhs_type) < TYPE_PRECISION (rhs_type));\n-}\n-\n /* Return true if STMT_INFO is the second part of a two-statement multiply-add\n    or multiply-subtract sequence that might be suitable for fusing into a\n    single instruction.  If VEC_FLAGS is zero, analyze the operation as\n@@ -15065,7 +14968,7 @@ aarch64_sve_in_loop_reduction_latency (vec_info *vinfo,\n \t\t\t\t       tree vectype,\n \t\t\t\t       const sve_vec_cost *sve_costs)\n {\n-  switch (aarch64_reduc_type (vinfo, stmt_info))\n+  switch (vect_reduc_type (vinfo, stmt_info))\n     {\n     case EXTRACT_LAST_REDUCTION:\n       return sve_costs->clast_cost;\n@@ -15156,7 +15059,7 @@ aarch64_detect_scalar_stmt_subtype (vec_info *vinfo, vect_cost_for_stmt kind,\n {\n   /* Detect an extension of a loaded value.  In general, we'll be able to fuse\n      the extension with the load.  */\n-  if (kind == scalar_stmt && aarch64_extending_load_p (vinfo, stmt_info))\n+  if (kind == scalar_stmt && vect_is_extending_load (vinfo, stmt_info))\n     return 0;\n \n   return stmt_cost;\n@@ -15188,7 +15091,7 @@ aarch64_detect_vector_stmt_subtype (vec_info *vinfo, vect_cost_for_stmt kind,\n   /* Detect cases in which vec_to_scalar is describing the extraction of a\n      vector element in preparation for a scalar store.  The store itself is\n      costed separately.  */\n-  if (aarch64_is_store_elt_extraction (kind, stmt_info))\n+  if (vect_is_store_elt_extraction (kind, stmt_info))\n     return simd_costs->store_elt_extra_cost;\n \n   /* Detect SVE gather loads, which are costed as a single scalar_load\n@@ -15227,7 +15130,7 @@ aarch64_detect_vector_stmt_subtype (vec_info *vinfo, vect_cost_for_stmt kind,\n      instruction like FADDP or MAXV.  */\n   if (kind == vec_to_scalar\n       && where == vect_epilogue\n-      && aarch64_is_reduction (stmt_info))\n+      && vect_is_reduction (stmt_info))\n     switch (GET_MODE_INNER (TYPE_MODE (vectype)))\n       {\n       case E_QImode:\n@@ -15277,12 +15180,12 @@ aarch64_sve_adjust_stmt_cost (class vec_info *vinfo, vect_cost_for_stmt kind,\n      on the fly.  Optimistically assume that a load followed by an extension\n      will fold to this form during combine, and that the extension therefore\n      comes for free.  */\n-  if (kind == vector_stmt && aarch64_extending_load_p (vinfo, stmt_info))\n+  if (kind == vector_stmt && vect_is_extending_load (vinfo, stmt_info))\n     stmt_cost = 0;\n \n   /* For similar reasons, vector_stmt integer truncations are a no-op,\n      because we can just ignore the unused upper bits of the source.  */\n-  if (kind == vector_stmt && aarch64_integer_truncation_p (stmt_info))\n+  if (kind == vector_stmt && vect_is_integer_truncation (stmt_info))\n     stmt_cost = 0;\n \n   /* Advanced SIMD can load and store pairs of registers using LDP and STP,\n@@ -15357,7 +15260,7 @@ aarch64_adjust_stmt_cost (vect_cost_for_stmt kind, stmt_vec_info stmt_info,\n \t}\n \n       if (kind == vector_stmt || kind == vec_to_scalar)\n-\tif (tree cmp_type = aarch64_embedded_comparison_type (stmt_info))\n+\tif (tree cmp_type = vect_embedded_comparison_type (stmt_info))\n \t  {\n \t    if (FLOAT_TYPE_P (cmp_type))\n \t      stmt_cost += simd_costs->fp_stmt_cost;\n@@ -15367,7 +15270,7 @@ aarch64_adjust_stmt_cost (vect_cost_for_stmt kind, stmt_vec_info stmt_info,\n     }\n \n   if (kind == scalar_stmt)\n-    if (tree cmp_type = aarch64_embedded_comparison_type (stmt_info))\n+    if (tree cmp_type = vect_embedded_comparison_type (stmt_info))\n       {\n \tif (FLOAT_TYPE_P (cmp_type))\n \t  stmt_cost += aarch64_tune_params.vec_costs->scalar_fp_stmt_cost;\n@@ -15417,12 +15320,12 @@ aarch64_count_ops (class vec_info *vinfo, aarch64_vector_costs *costs,\n   /* Calculate the minimum cycles per iteration imposed by a reduction\n      operation.  */\n   if ((kind == vector_stmt || kind == vec_to_scalar)\n-      && aarch64_is_reduction (stmt_info))\n+      && vect_is_reduction (stmt_info))\n     {\n       unsigned int base\n \t= aarch64_in_loop_reduction_latency (vinfo, stmt_info, vectype,\n \t\t\t\t\t     vec_flags);\n-      if (aarch64_reduc_type (vinfo, stmt_info) == FOLD_LEFT_REDUCTION)\n+      if (vect_reduc_type (vinfo, stmt_info) == FOLD_LEFT_REDUCTION)\n \t{\n \t  if (aarch64_sve_mode_p (TYPE_MODE (vectype)))\n \t    {\n@@ -15521,7 +15424,7 @@ aarch64_count_ops (class vec_info *vinfo, aarch64_vector_costs *costs,\n \n   /* Add any embedded comparison operations.  */\n   if ((kind == scalar_stmt || kind == vector_stmt || kind == vec_to_scalar)\n-      && aarch64_embedded_comparison_type (stmt_info))\n+      && vect_embedded_comparison_type (stmt_info))\n     ops->general_ops += num_copies;\n \n   /* Detect COND_REDUCTIONs and things that would need to become\n@@ -15530,15 +15433,15 @@ aarch64_count_ops (class vec_info *vinfo, aarch64_vector_costs *costs,\n      have only accounted for one.  */\n   if (vec_flags && (kind == vector_stmt || kind == vec_to_scalar))\n     {\n-      int reduc_type = aarch64_reduc_type (vinfo, stmt_info);\n+      int reduc_type = vect_reduc_type (vinfo, stmt_info);\n       if ((reduc_type == EXTRACT_LAST_REDUCTION && (vec_flags & VEC_ADVSIMD))\n \t  || reduc_type == COND_REDUCTION)\n \tops->general_ops += num_copies;\n     }\n \n   /* Count the predicate operations needed by an SVE comparison.  */\n   if (sve_issue && (kind == vector_stmt || kind == vec_to_scalar))\n-    if (tree type = aarch64_comparison_type (stmt_info))\n+    if (tree type = vect_comparison_type (stmt_info))\n       {\n \tunsigned int base = (FLOAT_TYPE_P (type)\n \t\t\t     ? sve_issue->fp_cmp_pred_ops\n@@ -15616,7 +15519,7 @@ aarch64_add_stmt_cost (class vec_info *vinfo, void *data, int count,\n \t  /* If we scalarize a strided store, the vectorizer costs one\n \t     vec_to_scalar for each element.  However, we can store the first\n \t     element using an FP store without a separate extract step.  */\n-\t  if (aarch64_is_store_elt_extraction (kind, stmt_info))\n+\t  if (vect_is_store_elt_extraction (kind, stmt_info))\n \t    count -= 1;\n \n \t  stmt_cost = aarch64_detect_scalar_stmt_subtype"}, {"sha": "686644b42881badc463069f917f58a1f804139f1", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 104, "deletions": 0, "changes": 104, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/783d809f0bb13a9f50139d03c328f59f9e3840c7/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/783d809f0bb13a9f50139d03c328f59f9e3840c7/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=783d809f0bb13a9f50139d03c328f59f9e3840c7", "patch": "@@ -2192,4 +2192,108 @@ extern vect_pattern_decl_t slp_patterns[];\n /* Number of supported pattern matchers.  */\n extern size_t num__slp_patterns;\n \n+/* ----------------------------------------------------------------------\n+   Target support routines\n+   -----------------------------------------------------------------------\n+   The following routines are provided to simplify costing decisions in\n+   target code.  Please add more as needed.  */\n+\n+/* Return true if an operaton of kind KIND for STMT_INFO represents\n+   the extraction of an element from a vector in preparation for\n+   storing the element to memory.  */\n+inline bool\n+vect_is_store_elt_extraction (vect_cost_for_stmt kind, stmt_vec_info stmt_info)\n+{\n+  return (kind == vec_to_scalar\n+\t  && STMT_VINFO_DATA_REF (stmt_info)\n+\t  && DR_IS_WRITE (STMT_VINFO_DATA_REF (stmt_info)));\n+}\n+\n+/* Return true if STMT_INFO represents part of a reduction.  */\n+inline bool\n+vect_is_reduction (stmt_vec_info stmt_info)\n+{\n+  return (STMT_VINFO_REDUC_DEF (stmt_info)\n+\t  || VECTORIZABLE_CYCLE_DEF (STMT_VINFO_DEF_TYPE (stmt_info)));\n+}\n+\n+/* If STMT_INFO describes a reduction, return the vect_reduction_type\n+   of the reduction it describes, otherwise return -1.  */\n+inline int\n+vect_reduc_type (vec_info *vinfo, stmt_vec_info stmt_info)\n+{\n+  if (loop_vec_info loop_vinfo = dyn_cast<loop_vec_info> (vinfo))\n+    if (STMT_VINFO_REDUC_DEF (stmt_info))\n+      {\n+\tstmt_vec_info reduc_info = info_for_reduction (loop_vinfo, stmt_info);\n+\treturn int (STMT_VINFO_REDUC_TYPE (reduc_info));\n+      }\n+  return -1;\n+}\n+\n+/* If STMT_INFO is a COND_EXPR that includes an embedded comparison, return the\n+   scalar type of the values being compared.  Return null otherwise.  */\n+inline tree\n+vect_embedded_comparison_type (stmt_vec_info stmt_info)\n+{\n+  if (auto *assign = dyn_cast<gassign *> (stmt_info->stmt))\n+    if (gimple_assign_rhs_code (assign) == COND_EXPR)\n+      {\n+\ttree cond = gimple_assign_rhs1 (assign);\n+\tif (COMPARISON_CLASS_P (cond))\n+\t  return TREE_TYPE (TREE_OPERAND (cond, 0));\n+      }\n+  return NULL_TREE;\n+}\n+\n+/* If STMT_INFO is a comparison or contains an embedded comparison, return the\n+   scalar type of the values being compared.  Return null otherwise.  */\n+inline tree\n+vect_comparison_type (stmt_vec_info stmt_info)\n+{\n+  if (auto *assign = dyn_cast<gassign *> (stmt_info->stmt))\n+    if (TREE_CODE_CLASS (gimple_assign_rhs_code (assign)) == tcc_comparison)\n+      return TREE_TYPE (gimple_assign_rhs1 (assign));\n+  return vect_embedded_comparison_type (stmt_info);\n+}\n+\n+/* Return true if STMT_INFO extends the result of a load.  */\n+inline bool\n+vect_is_extending_load (class vec_info *vinfo, stmt_vec_info stmt_info)\n+{\n+  /* Although this is quite large for an inline function, this part\n+     at least should be inline.  */\n+  gassign *assign = dyn_cast <gassign *> (stmt_info->stmt);\n+  if (!assign || !CONVERT_EXPR_CODE_P (gimple_assign_rhs_code (assign)))\n+    return false;\n+\n+  tree rhs = gimple_assign_rhs1 (stmt_info->stmt);\n+  tree lhs_type = TREE_TYPE (gimple_assign_lhs (assign));\n+  tree rhs_type = TREE_TYPE (rhs);\n+  if (!INTEGRAL_TYPE_P (lhs_type)\n+      || !INTEGRAL_TYPE_P (rhs_type)\n+      || TYPE_PRECISION (lhs_type) <= TYPE_PRECISION (rhs_type))\n+    return false;\n+\n+  stmt_vec_info def_stmt_info = vinfo->lookup_def (rhs);\n+  return (def_stmt_info\n+\t  && STMT_VINFO_DATA_REF (def_stmt_info)\n+\t  && DR_IS_READ (STMT_VINFO_DATA_REF (def_stmt_info)));\n+}\n+\n+/* Return true if STMT_INFO is an integer truncation.  */\n+inline bool\n+vect_is_integer_truncation (stmt_vec_info stmt_info)\n+{\n+  gassign *assign = dyn_cast <gassign *> (stmt_info->stmt);\n+  if (!assign || !CONVERT_EXPR_CODE_P (gimple_assign_rhs_code (assign)))\n+    return false;\n+\n+  tree lhs_type = TREE_TYPE (gimple_assign_lhs (assign));\n+  tree rhs_type = TREE_TYPE (gimple_assign_rhs1 (assign));\n+  return (INTEGRAL_TYPE_P (lhs_type)\n+\t  && INTEGRAL_TYPE_P (rhs_type)\n+\t  && TYPE_PRECISION (lhs_type) < TYPE_PRECISION (rhs_type));\n+}\n+\n #endif  /* GCC_TREE_VECTORIZER_H  */"}]}