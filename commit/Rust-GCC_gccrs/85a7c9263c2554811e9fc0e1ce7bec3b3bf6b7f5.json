{"sha": "85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODVhN2M5MjYzYzI1NTQ4MTFlOWZjMGUxY2U3YmVjM2IzYmY2YjdmNQ==", "commit": {"author": {"name": "Bill Schmidt", "email": "wschmidt@linux.vnet.ibm.com", "date": "2016-04-27T20:22:27Z"}, "committer": {"name": "William Schmidt", "email": "wschmidt@gcc.gnu.org", "date": "2016-04-27T20:22:27Z"}, "message": "altivec.md (altivec_lvx_<mode>): Remove.\n\n2016-04-27  Bill Schmidt  <wschmidt@linux.vnet.ibm.com>\n\n\t* config/rs6000/altivec.md (altivec_lvx_<mode>): Remove.\n\t(altivec_lvx_<mode>_internal): Document.\n\t(altivec_lvx_<mode>_2op): New define_insn.\n\t(altivec_lvx_<mode>_1op): Likewise.\n\t(altivec_lvx_<mode>_2op_si): Likewise.\n\t(altivec_lvx_<mode>_1op_si): Likewise.\n\t(altivec_stvx_<mode>): Remove.\n\t(altivec_stvx_<mode>_internal): Document.\n\t(altivec_stvx_<mode>_2op): New define_insn.\n\t(altivec_stvx_<mode>_1op): Likewise.\n\t(altivec_stvx_<mode>_2op_si): Likewise.\n\t(altivec_stvx_<mode>_1op_si): Likewise.\n\t* config/rs6000/rs6000-c.c (altivec_resolve_overloaded_builtin):\n\tExpand vec_ld and vec_st during parsing.\n\t* config/rs6000/rs6000.c (altivec_expand_lvx_be): Commentary\n\tchanges.\n\t(altivec_expand_stvx_be): Likewise.\n\t(altivec_expand_lv_builtin): Expand lvx built-ins to expose the\n\taddress-masking behavior in RTL.\n\t(altivec_expand_stv_builtin): Expand stvx built-ins to expose the\n\taddress-masking behavior in RTL.\n\t(altivec_expand_builtin): Change builtin code arguments for calls\n\tto altivec_expand_stv_builtin and altivec_expand_lv_builtin.\n\t(insn_is_swappable_p): Avoid incorrect swap optimization in the\n\tpresence of lvx/stvx patterns.\n\t(alignment_with_canonical_addr): New function.\n\t(alignment_mask): Likewise.\n\t(find_alignment_op): Likewise.\n\t(recombine_lvx_pattern): Likewise.\n\t(recombine_stvx_pattern): Likewise.\n\t(recombine_lvx_stvx_patterns): Likewise.\n\t(rs6000_analyze_swaps): Perform a pre-pass to recognize lvx and\n\tstvx patterns from expand.\n\t* config/rs6000/vector.md (vector_altivec_load_<mode>): Use new\n\texpansions.\n\t(vector_altivec_store_<mode>): Likewise.\n\nFrom-SVN: r235533", "tree": {"sha": "21409eb76e5b7b7ed915d171d9cab6320d4baca6", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/21409eb76e5b7b7ed915d171d9cab6320d4baca6"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/comments", "author": {"login": "wschmidt-ibm", "id": 5520937, "node_id": "MDQ6VXNlcjU1MjA5Mzc=", "avatar_url": "https://avatars.githubusercontent.com/u/5520937?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wschmidt-ibm", "html_url": "https://github.com/wschmidt-ibm", "followers_url": "https://api.github.com/users/wschmidt-ibm/followers", "following_url": "https://api.github.com/users/wschmidt-ibm/following{/other_user}", "gists_url": "https://api.github.com/users/wschmidt-ibm/gists{/gist_id}", "starred_url": "https://api.github.com/users/wschmidt-ibm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wschmidt-ibm/subscriptions", "organizations_url": "https://api.github.com/users/wschmidt-ibm/orgs", "repos_url": "https://api.github.com/users/wschmidt-ibm/repos", "events_url": "https://api.github.com/users/wschmidt-ibm/events{/privacy}", "received_events_url": "https://api.github.com/users/wschmidt-ibm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "523d72071960ddca69139b9fd96ad8c8ce79ac0e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/523d72071960ddca69139b9fd96ad8c8ce79ac0e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/523d72071960ddca69139b9fd96ad8c8ce79ac0e"}], "stats": {"total": 750, "additions": 671, "deletions": 79}, "files": [{"sha": "029a40219c8a7d7ee5e8708143d5e1e98dfd984b", "filename": "gcc/ChangeLog", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5", "patch": "@@ -1,3 +1,42 @@\n+2016-04-27  Bill Schmidt  <wschmidt@linux.vnet.ibm.com>\n+\n+\t* config/rs6000/altivec.md (altivec_lvx_<mode>): Remove.\n+\t(altivec_lvx_<mode>_internal): Document.\n+\t(altivec_lvx_<mode>_2op): New define_insn.\n+\t(altivec_lvx_<mode>_1op): Likewise.\n+\t(altivec_lvx_<mode>_2op_si): Likewise.\n+\t(altivec_lvx_<mode>_1op_si): Likewise.\n+\t(altivec_stvx_<mode>): Remove.\n+\t(altivec_stvx_<mode>_internal): Document.\n+\t(altivec_stvx_<mode>_2op): New define_insn.\n+\t(altivec_stvx_<mode>_1op): Likewise.\n+\t(altivec_stvx_<mode>_2op_si): Likewise.\n+\t(altivec_stvx_<mode>_1op_si): Likewise.\n+\t* config/rs6000/rs6000-c.c (altivec_resolve_overloaded_builtin):\n+\tExpand vec_ld and vec_st during parsing.\n+\t* config/rs6000/rs6000.c (altivec_expand_lvx_be): Commentary\n+\tchanges.\n+\t(altivec_expand_stvx_be): Likewise.\n+\t(altivec_expand_lv_builtin): Expand lvx built-ins to expose the\n+\taddress-masking behavior in RTL.\n+\t(altivec_expand_stv_builtin): Expand stvx built-ins to expose the\n+\taddress-masking behavior in RTL.\n+\t(altivec_expand_builtin): Change builtin code arguments for calls\n+\tto altivec_expand_stv_builtin and altivec_expand_lv_builtin.\n+\t(insn_is_swappable_p): Avoid incorrect swap optimization in the\n+\tpresence of lvx/stvx patterns.\n+\t(alignment_with_canonical_addr): New function.\n+\t(alignment_mask): Likewise.\n+\t(find_alignment_op): Likewise.\n+\t(recombine_lvx_pattern): Likewise.\n+\t(recombine_stvx_pattern): Likewise.\n+\t(recombine_lvx_stvx_patterns): Likewise.\n+\t(rs6000_analyze_swaps): Perform a pre-pass to recognize lvx and\n+\tstvx patterns from expand.\n+\t* config/rs6000/vector.md (vector_altivec_load_<mode>): Use new\n+\texpansions.\n+\t(vector_altivec_store_<mode>): Likewise.\n+\n 2016-04-26  Evandro Menezes  <e.menezes@samsung.com>\n \n \t* config/aarch64/aarch64.md"}, {"sha": "7a8c8ebf3f3f5733a3a9fd95aacf698af323e550", "filename": "gcc/config/rs6000/altivec.md", "status": "modified", "additions": 77, "deletions": 27, "changes": 104, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/gcc%2Fconfig%2Frs6000%2Faltivec.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/gcc%2Fconfig%2Frs6000%2Faltivec.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Faltivec.md?ref=85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5", "patch": "@@ -2514,20 +2514,9 @@\n   \"lvxl %0,%y1\"\n   [(set_attr \"type\" \"vecload\")])\n \n-(define_expand \"altivec_lvx_<mode>\"\n-  [(parallel\n-    [(set (match_operand:VM2 0 \"register_operand\" \"=v\")\n-\t  (match_operand:VM2 1 \"memory_operand\" \"Z\"))\n-     (unspec [(const_int 0)] UNSPEC_LVX)])]\n-  \"TARGET_ALTIVEC\"\n-{\n-  if (!BYTES_BIG_ENDIAN && VECTOR_ELT_ORDER_BIG)\n-    {\n-      altivec_expand_lvx_be (operands[0], operands[1], <MODE>mode, UNSPEC_LVX);\n-      DONE;\n-    }\n-})\n-\n+; This version of lvx is used only in cases where we need to force an lvx\n+; over any other load, and we don't care about losing CSE opportunities.\n+; Its primary use is for prologue register saves.\n (define_insn \"altivec_lvx_<mode>_internal\"\n   [(parallel\n     [(set (match_operand:VM2 0 \"register_operand\" \"=v\")\n@@ -2537,20 +2526,45 @@\n   \"lvx %0,%y1\"\n   [(set_attr \"type\" \"vecload\")])\n \n-(define_expand \"altivec_stvx_<mode>\"\n-  [(parallel\n-    [(set (match_operand:VM2 0 \"memory_operand\" \"=Z\")\n-\t  (match_operand:VM2 1 \"register_operand\" \"v\"))\n-     (unspec [(const_int 0)] UNSPEC_STVX)])]\n-  \"TARGET_ALTIVEC\"\n-{\n-  if (!BYTES_BIG_ENDIAN && VECTOR_ELT_ORDER_BIG)\n-    {\n-      altivec_expand_stvx_be (operands[0], operands[1], <MODE>mode, UNSPEC_STVX);\n-      DONE;\n-    }\n-})\n+; The next two patterns embody what lvx should usually look like.\n+(define_insn \"altivec_lvx_<mode>_2op\"\n+  [(set (match_operand:VM2 0 \"register_operand\" \"=v\")\n+        (mem:VM2 (and:DI (plus:DI (match_operand:DI 1 \"register_operand\" \"b\")\n+                                  (match_operand:DI 2 \"register_operand\" \"r\"))\n+\t\t         (const_int -16))))]\n+  \"TARGET_ALTIVEC && TARGET_64BIT\"\n+  \"lvx %0,%1,%2\"\n+  [(set_attr \"type\" \"vecload\")])\n \n+(define_insn \"altivec_lvx_<mode>_1op\"\n+  [(set (match_operand:VM2 0 \"register_operand\" \"=v\")\n+        (mem:VM2 (and:DI (match_operand:DI 1 \"register_operand\" \"r\")\n+\t\t\t (const_int -16))))]\n+  \"TARGET_ALTIVEC && TARGET_64BIT\"\n+  \"lvx %0,0,%1\"\n+  [(set_attr \"type\" \"vecload\")])\n+\n+; 32-bit versions of the above.\n+(define_insn \"altivec_lvx_<mode>_2op_si\"\n+  [(set (match_operand:VM2 0 \"register_operand\" \"=v\")\n+        (mem:VM2 (and:SI (plus:SI (match_operand:SI 1 \"register_operand\" \"b\")\n+                                  (match_operand:SI 2 \"register_operand\" \"r\"))\n+\t\t         (const_int -16))))]\n+  \"TARGET_ALTIVEC && TARGET_32BIT\"\n+  \"lvx %0,%1,%2\"\n+  [(set_attr \"type\" \"vecload\")])\n+\n+(define_insn \"altivec_lvx_<mode>_1op_si\"\n+  [(set (match_operand:VM2 0 \"register_operand\" \"=v\")\n+        (mem:VM2 (and:SI (match_operand:SI 1 \"register_operand\" \"r\")\n+\t\t\t (const_int -16))))]\n+  \"TARGET_ALTIVEC && TARGET_32BIT\"\n+  \"lvx %0,0,%1\"\n+  [(set_attr \"type\" \"vecload\")])\n+\n+; This version of stvx is used only in cases where we need to force an stvx\n+; over any other store, and we don't care about losing CSE opportunities.\n+; Its primary use is for epilogue register restores.\n (define_insn \"altivec_stvx_<mode>_internal\"\n   [(parallel\n     [(set (match_operand:VM2 0 \"memory_operand\" \"=Z\")\n@@ -2560,6 +2574,42 @@\n   \"stvx %1,%y0\"\n   [(set_attr \"type\" \"vecstore\")])\n \n+; The next two patterns embody what stvx should usually look like.\n+(define_insn \"altivec_stvx_<mode>_2op\"\n+  [(set (mem:VM2 (and:DI (plus:DI (match_operand:DI 1 \"register_operand\" \"b\")\n+  \t                          (match_operand:DI 2 \"register_operand\" \"r\"))\n+\t                 (const_int -16)))\n+        (match_operand:VM2 0 \"register_operand\" \"v\"))]\n+  \"TARGET_ALTIVEC && TARGET_64BIT\"\n+  \"stvx %0,%1,%2\"\n+  [(set_attr \"type\" \"vecstore\")])\n+\n+(define_insn \"altivec_stvx_<mode>_1op\"\n+  [(set (mem:VM2 (and:DI (match_operand:DI 1 \"register_operand\" \"r\")\n+\t                 (const_int -16)))\n+        (match_operand:VM2 0 \"register_operand\" \"v\"))]\n+  \"TARGET_ALTIVEC && TARGET_64BIT\"\n+  \"stvx %0,0,%1\"\n+  [(set_attr \"type\" \"vecstore\")])\n+\n+; 32-bit versions of the above.\n+(define_insn \"altivec_stvx_<mode>_2op_si\"\n+  [(set (mem:VM2 (and:SI (plus:SI (match_operand:SI 1 \"register_operand\" \"b\")\n+  \t                          (match_operand:SI 2 \"register_operand\" \"r\"))\n+\t                 (const_int -16)))\n+        (match_operand:VM2 0 \"register_operand\" \"v\"))]\n+  \"TARGET_ALTIVEC && TARGET_32BIT\"\n+  \"stvx %0,%1,%2\"\n+  [(set_attr \"type\" \"vecstore\")])\n+\n+(define_insn \"altivec_stvx_<mode>_1op_si\"\n+  [(set (mem:VM2 (and:SI (match_operand:SI 1 \"register_operand\" \"r\")\n+\t                 (const_int -16)))\n+        (match_operand:VM2 0 \"register_operand\" \"v\"))]\n+  \"TARGET_ALTIVEC && TARGET_32BIT\"\n+  \"stvx %0,0,%1\"\n+  [(set_attr \"type\" \"vecstore\")])\n+\n (define_expand \"altivec_stvxl_<mode>\"\n   [(parallel\n     [(set (match_operand:VM2 0 \"memory_operand\" \"=Z\")"}, {"sha": "55751a670de3972f331a500311a2c4c119f7cdf2", "filename": "gcc/config/rs6000/rs6000-c.c", "status": "modified", "additions": 124, "deletions": 0, "changes": 124, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/gcc%2Fconfig%2Frs6000%2Frs6000-c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/gcc%2Fconfig%2Frs6000%2Frs6000-c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000-c.c?ref=85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5", "patch": "@@ -4800,6 +4800,130 @@ assignment for unaligned loads and stores\");\n       return stmt;\n     }\n \n+  /* Expand vec_ld into an expression that masks the address and\n+     performs the load.  We need to expand this early to allow\n+     the best aliasing, as by the time we get into RTL we no longer\n+     are able to honor __restrict__, for example.  We may want to\n+     consider this for all memory access built-ins.\n+\n+     When -maltivec=be is specified, simply punt to existing\n+     built-in processing.  */\n+  if (fcode == ALTIVEC_BUILTIN_VEC_LD\n+      && (BYTES_BIG_ENDIAN || !VECTOR_ELT_ORDER_BIG))\n+    {\n+      tree arg0 = (*arglist)[0];\n+      tree arg1 = (*arglist)[1];\n+\n+      /* Strip qualifiers like \"const\" from the pointer arg.  */\n+      tree arg1_type = TREE_TYPE (arg1);\n+      tree inner_type = TREE_TYPE (arg1_type);\n+      if (TYPE_QUALS (TREE_TYPE (arg1_type)) != 0)\n+\t{\n+\t  arg1_type = build_pointer_type (build_qualified_type (inner_type,\n+\t\t\t\t\t\t\t\t0));\n+\t  arg1 = fold_convert (arg1_type, arg1);\n+\t}\n+\n+      /* Construct the masked address.  Let existing error handling take\n+\t over if we don't have a constant offset.  */\n+      arg0 = fold (arg0);\n+\n+      if (TREE_CODE (arg0) == INTEGER_CST)\n+\t{\n+\t  if (!ptrofftype_p (TREE_TYPE (arg0)))\n+\t    arg0 = build1 (NOP_EXPR, sizetype, arg0);\n+\n+\t  tree arg1_type = TREE_TYPE (arg1);\n+\t  tree addr = fold_build2_loc (loc, POINTER_PLUS_EXPR, arg1_type,\n+\t\t\t\t       arg1, arg0);\n+\t  tree aligned = fold_build2_loc (loc, BIT_AND_EXPR, arg1_type, addr,\n+\t\t\t\t\t  build_int_cst (arg1_type, -16));\n+\n+\t  /* Find the built-in to get the return type so we can convert\n+\t     the result properly (or fall back to default handling if the\n+\t     arguments aren't compatible).  */\n+\t  for (desc = altivec_overloaded_builtins;\n+\t       desc->code && desc->code != fcode; desc++)\n+\t    continue;\n+\n+\t  for (; desc->code == fcode; desc++)\n+\t    if (rs6000_builtin_type_compatible (TREE_TYPE (arg0), desc->op1)\n+\t\t&& (rs6000_builtin_type_compatible (TREE_TYPE (arg1),\n+\t\t\t\t\t\t    desc->op2)))\n+\t      {\n+\t\ttree ret_type = rs6000_builtin_type (desc->ret_type);\n+\t\tif (TYPE_MODE (ret_type) == V2DImode)\n+\t\t  /* Type-based aliasing analysis thinks vector long\n+\t\t     and vector long long are different and will put them\n+\t\t     in distinct alias classes.  Force our return type\n+\t\t     to be a may-alias type to avoid this.  */\n+\t\t  ret_type\n+\t\t    = build_pointer_type_for_mode (ret_type, Pmode,\n+\t\t\t\t\t\t   true/*can_alias_all*/);\n+\t\telse\n+\t\t  ret_type = build_pointer_type (ret_type);\n+\t\taligned = build1 (NOP_EXPR, ret_type, aligned);\n+\t\ttree ret_val = build_indirect_ref (loc, aligned, RO_NULL);\n+\t\treturn ret_val;\n+\t      }\n+\t}\n+    }\n+\n+  /* Similarly for stvx.  */\n+  if (fcode == ALTIVEC_BUILTIN_VEC_ST\n+      && (BYTES_BIG_ENDIAN || !VECTOR_ELT_ORDER_BIG))\n+    {\n+      tree arg0 = (*arglist)[0];\n+      tree arg1 = (*arglist)[1];\n+      tree arg2 = (*arglist)[2];\n+\n+      /* Construct the masked address.  Let existing error handling take\n+\t over if we don't have a constant offset.  */\n+      arg1 = fold (arg1);\n+\n+      if (TREE_CODE (arg1) == INTEGER_CST)\n+\t{\n+\t  if (!ptrofftype_p (TREE_TYPE (arg1)))\n+\t    arg1 = build1 (NOP_EXPR, sizetype, arg1);\n+\n+\t  tree arg2_type = TREE_TYPE (arg2);\n+\t  tree addr = fold_build2_loc (loc, POINTER_PLUS_EXPR, arg2_type,\n+\t\t\t\t       arg2, arg1);\n+\t  tree aligned = fold_build2_loc (loc, BIT_AND_EXPR, arg2_type, addr,\n+\t\t\t\t\t  build_int_cst (arg2_type, -16));\n+\n+\t  /* Find the built-in to make sure a compatible one exists; if not\n+\t     we fall back to default handling to get the error message.  */\n+\t  for (desc = altivec_overloaded_builtins;\n+\t       desc->code && desc->code != fcode; desc++)\n+\t    continue;\n+\n+\t  for (; desc->code == fcode; desc++)\n+\t    if (rs6000_builtin_type_compatible (TREE_TYPE (arg0), desc->op1)\n+\t\t&& rs6000_builtin_type_compatible (TREE_TYPE (arg1), desc->op2)\n+\t\t&& rs6000_builtin_type_compatible (TREE_TYPE (arg2),\n+\t\t\t\t\t\t   desc->op3))\n+\t      {\n+\t\ttree arg0_type = TREE_TYPE (arg0);\n+\t\tif (TYPE_MODE (arg0_type) == V2DImode)\n+\t\t  /* Type-based aliasing analysis thinks vector long\n+\t\t     and vector long long are different and will put them\n+\t\t     in distinct alias classes.  Force our address type\n+\t\t     to be a may-alias type to avoid this.  */\n+\t\t  arg0_type\n+\t\t    = build_pointer_type_for_mode (arg0_type, Pmode,\n+\t\t\t\t\t\t   true/*can_alias_all*/);\n+\t\telse\n+\t\t  arg0_type = build_pointer_type (arg0_type);\n+\t\taligned = build1 (NOP_EXPR, arg0_type, aligned);\n+\t\ttree stg = build_indirect_ref (loc, aligned, RO_NULL);\n+\t\ttree retval = build2 (MODIFY_EXPR, TREE_TYPE (stg), stg,\n+\t\t\t\t      convert (TREE_TYPE (stg), arg0));\n+\t\treturn retval;\n+\t      }\n+\t}\n+    }\n+\n   for (n = 0;\n        !VOID_TYPE_P (TREE_VALUE (fnargs)) && n < nargs;\n        fnargs = TREE_CHAIN (fnargs), n++)"}, {"sha": "fba4f9ea71a51f6a6cd05bb218e94341e1d2ac8b", "filename": "gcc/config/rs6000/rs6000.c", "status": "modified", "additions": 415, "deletions": 50, "changes": 465, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/gcc%2Fconfig%2Frs6000%2Frs6000.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/gcc%2Fconfig%2Frs6000%2Frs6000.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.c?ref=85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5", "patch": "@@ -13025,9 +13025,9 @@ swap_selector_for_mode (machine_mode mode)\n   return force_reg (V16QImode, gen_rtx_CONST_VECTOR (V16QImode, gen_rtvec_v (16, perm)));\n }\n \n-/* Generate code for an \"lvx\", \"lvxl\", or \"lve*x\" built-in for a little endian target\n-   with -maltivec=be specified.  Issue the load followed by an element-reversing\n-   permute.  */\n+/* Generate code for an \"lvxl\", or \"lve*x\" built-in for a little endian target\n+   with -maltivec=be specified.  Issue the load followed by an element-\n+   reversing permute.  */\n void\n altivec_expand_lvx_be (rtx op0, rtx op1, machine_mode mode, unsigned unspec)\n {\n@@ -13043,8 +13043,8 @@ altivec_expand_lvx_be (rtx op0, rtx op1, machine_mode mode, unsigned unspec)\n   emit_insn (gen_rtx_SET (op0, vperm));\n }\n \n-/* Generate code for a \"stvx\" or \"stvxl\" built-in for a little endian target\n-   with -maltivec=be specified.  Issue the store preceded by an element-reversing\n+/* Generate code for a \"stvxl\" built-in for a little endian target with\n+   -maltivec=be specified.  Issue the store preceded by an element-reversing\n    permute.  */\n void\n altivec_expand_stvx_be (rtx op0, rtx op1, machine_mode mode, unsigned unspec)\n@@ -13106,22 +13106,65 @@ altivec_expand_lv_builtin (enum insn_code icode, tree exp, rtx target, bool blk)\n \n   op1 = copy_to_mode_reg (mode1, op1);\n \n-  if (op0 == const0_rtx)\n-    {\n-      addr = gen_rtx_MEM (blk ? BLKmode : tmode, op1);\n-    }\n-  else\n-    {\n-      op0 = copy_to_mode_reg (mode0, op0);\n-      addr = gen_rtx_MEM (blk ? BLKmode : tmode, gen_rtx_PLUS (Pmode, op0, op1));\n-    }\n+  /* For LVX, express the RTL accurately by ANDing the address with -16.\n+     LVXL and LVE*X expand to use UNSPECs to hide their special behavior,\n+     so the raw address is fine.  */\n+  switch (icode)\n+    {\n+    case CODE_FOR_altivec_lvx_v2df_2op:\n+    case CODE_FOR_altivec_lvx_v2di_2op:\n+    case CODE_FOR_altivec_lvx_v4sf_2op:\n+    case CODE_FOR_altivec_lvx_v4si_2op:\n+    case CODE_FOR_altivec_lvx_v8hi_2op:\n+    case CODE_FOR_altivec_lvx_v16qi_2op:\n+      {\n+\trtx rawaddr;\n+\tif (op0 == const0_rtx)\n+\t  rawaddr = op1;\n+\telse\n+\t  {\n+\t    op0 = copy_to_mode_reg (mode0, op0);\n+\t    rawaddr = gen_rtx_PLUS (Pmode, op1, op0);\n+\t  }\n+\taddr = gen_rtx_AND (Pmode, rawaddr, gen_rtx_CONST_INT (Pmode, -16));\n+\taddr = gen_rtx_MEM (blk ? BLKmode : tmode, addr);\n \n-  pat = GEN_FCN (icode) (target, addr);\n+\t/* For -maltivec=be, emit the load and follow it up with a\n+\t   permute to swap the elements.  */\n+\tif (!BYTES_BIG_ENDIAN && VECTOR_ELT_ORDER_BIG)\n+\t  {\n+\t    rtx temp = gen_reg_rtx (tmode);\n+\t    emit_insn (gen_rtx_SET (temp, addr));\n \n-  if (! pat)\n-    return 0;\n-  emit_insn (pat);\n+\t    rtx sel = swap_selector_for_mode (tmode);\n+\t    rtx vperm = gen_rtx_UNSPEC (tmode, gen_rtvec (3, temp, temp, sel),\n+\t\t\t\t\tUNSPEC_VPERM);\n+\t    emit_insn (gen_rtx_SET (target, vperm));\n+\t  }\n+\telse\n+\t  emit_insn (gen_rtx_SET (target, addr));\n \n+\tbreak;\n+      }\n+\n+    default:\n+      if (op0 == const0_rtx)\n+\taddr = gen_rtx_MEM (blk ? BLKmode : tmode, op1);\n+      else\n+\t{\n+\t  op0 = copy_to_mode_reg (mode0, op0);\n+\t  addr = gen_rtx_MEM (blk ? BLKmode : tmode,\n+\t\t\t      gen_rtx_PLUS (Pmode, op1, op0));\n+\t}\n+\n+      pat = GEN_FCN (icode) (target, addr);\n+      if (! pat)\n+\treturn 0;\n+      emit_insn (pat);\n+\n+      break;\n+    }\n+  \n   return target;\n }\n \n@@ -13208,7 +13251,7 @@ altivec_expand_stv_builtin (enum insn_code icode, tree exp)\n   rtx op0 = expand_normal (arg0);\n   rtx op1 = expand_normal (arg1);\n   rtx op2 = expand_normal (arg2);\n-  rtx pat, addr;\n+  rtx pat, addr, rawaddr;\n   machine_mode tmode = insn_data[icode].operand[0].mode;\n   machine_mode smode = insn_data[icode].operand[1].mode;\n   machine_mode mode1 = Pmode;\n@@ -13220,24 +13263,69 @@ altivec_expand_stv_builtin (enum insn_code icode, tree exp)\n       || arg2 == error_mark_node)\n     return const0_rtx;\n \n-  if (! (*insn_data[icode].operand[1].predicate) (op0, smode))\n-    op0 = copy_to_mode_reg (smode, op0);\n-\n   op2 = copy_to_mode_reg (mode2, op2);\n \n-  if (op1 == const0_rtx)\n-    {\n-      addr = gen_rtx_MEM (tmode, op2);\n-    }\n-  else\n-    {\n-      op1 = copy_to_mode_reg (mode1, op1);\n-      addr = gen_rtx_MEM (tmode, gen_rtx_PLUS (Pmode, op1, op2));\n-    }\n+  /* For STVX, express the RTL accurately by ANDing the address with -16.\n+     STVXL and STVE*X expand to use UNSPECs to hide their special behavior,\n+     so the raw address is fine.  */\n+  switch (icode)\n+    {\n+    case CODE_FOR_altivec_stvx_v2df_2op:\n+    case CODE_FOR_altivec_stvx_v2di_2op:\n+    case CODE_FOR_altivec_stvx_v4sf_2op:\n+    case CODE_FOR_altivec_stvx_v4si_2op:\n+    case CODE_FOR_altivec_stvx_v8hi_2op:\n+    case CODE_FOR_altivec_stvx_v16qi_2op:\n+      {\n+\tif (op1 == const0_rtx)\n+\t  rawaddr = op2;\n+\telse\n+\t  {\n+\t    op1 = copy_to_mode_reg (mode1, op1);\n+\t    rawaddr = gen_rtx_PLUS (Pmode, op2, op1);\n+\t  }\n+\n+\taddr = gen_rtx_AND (Pmode, rawaddr, gen_rtx_CONST_INT (Pmode, -16));\n+\taddr = gen_rtx_MEM (tmode, addr);\n+\n+\top0 = copy_to_mode_reg (tmode, op0);\n+\n+\t/* For -maltivec=be, emit a permute to swap the elements, followed\n+\t   by the store.  */\n+\tif (!BYTES_BIG_ENDIAN && VECTOR_ELT_ORDER_BIG)\n+\t  {\n+\t    rtx temp = gen_reg_rtx (tmode);\n+\t    rtx sel = swap_selector_for_mode (tmode);\n+\t    rtx vperm = gen_rtx_UNSPEC (tmode, gen_rtvec (3, op0, op0, sel),\n+\t\t\t\t\tUNSPEC_VPERM);\n+\t    emit_insn (gen_rtx_SET (temp, vperm));\n+\t    emit_insn (gen_rtx_SET (addr, temp));\n+\t  }\n+\telse\n+\t  emit_insn (gen_rtx_SET (addr, op0));\n+\n+\tbreak;\n+      }\n+\n+    default:\n+      {\n+\tif (! (*insn_data[icode].operand[1].predicate) (op0, smode))\n+\t  op0 = copy_to_mode_reg (smode, op0);\n+\n+\tif (op1 == const0_rtx)\n+\t  addr = gen_rtx_MEM (tmode, op2);\n+\telse\n+\t  {\n+\t    op1 = copy_to_mode_reg (mode1, op1);\n+\t    addr = gen_rtx_MEM (tmode, gen_rtx_PLUS (Pmode, op2, op1));\n+\t  }\n+\n+\tpat = GEN_FCN (icode) (addr, op0);\n+\tif (pat)\n+\t  emit_insn (pat);\n+      }\n+    }      \n \n-  pat = GEN_FCN (icode) (addr, op0);\n-  if (pat)\n-    emit_insn (pat);\n   return NULL_RTX;\n }\n \n@@ -14073,18 +14161,18 @@ altivec_expand_builtin (tree exp, rtx target, bool *expandedp)\n   switch (fcode)\n     {\n     case ALTIVEC_BUILTIN_STVX_V2DF:\n-      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v2df, exp);\n+      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v2df_2op, exp);\n     case ALTIVEC_BUILTIN_STVX_V2DI:\n-      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v2di, exp);\n+      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v2di_2op, exp);\n     case ALTIVEC_BUILTIN_STVX_V4SF:\n-      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v4sf, exp);\n+      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v4sf_2op, exp);\n     case ALTIVEC_BUILTIN_STVX:\n     case ALTIVEC_BUILTIN_STVX_V4SI:\n-      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v4si, exp);\n+      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v4si_2op, exp);\n     case ALTIVEC_BUILTIN_STVX_V8HI:\n-      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v8hi, exp);\n+      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v8hi_2op, exp);\n     case ALTIVEC_BUILTIN_STVX_V16QI:\n-      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v16qi, exp);\n+      return altivec_expand_stv_builtin (CODE_FOR_altivec_stvx_v16qi_2op, exp);\n     case ALTIVEC_BUILTIN_STVEBX:\n       return altivec_expand_stv_builtin (CODE_FOR_altivec_stvebx, exp);\n     case ALTIVEC_BUILTIN_STVEHX:\n@@ -14272,23 +14360,23 @@ altivec_expand_builtin (tree exp, rtx target, bool *expandedp)\n       return altivec_expand_lv_builtin (CODE_FOR_altivec_lvxl_v16qi,\n \t\t\t\t\texp, target, false);\n     case ALTIVEC_BUILTIN_LVX_V2DF:\n-      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v2df,\n+      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v2df_2op,\n \t\t\t\t\texp, target, false);\n     case ALTIVEC_BUILTIN_LVX_V2DI:\n-      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v2di,\n+      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v2di_2op,\n \t\t\t\t\texp, target, false);\n     case ALTIVEC_BUILTIN_LVX_V4SF:\n-      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v4sf,\n+      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v4sf_2op,\n \t\t\t\t\texp, target, false);\n     case ALTIVEC_BUILTIN_LVX:\n     case ALTIVEC_BUILTIN_LVX_V4SI:\n-      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v4si,\n+      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v4si_2op,\n \t\t\t\t\texp, target, false);\n     case ALTIVEC_BUILTIN_LVX_V8HI:\n-      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v8hi,\n+      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v8hi_2op,\n \t\t\t\t\texp, target, false);\n     case ALTIVEC_BUILTIN_LVX_V16QI:\n-      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v16qi,\n+      return altivec_expand_lv_builtin (CODE_FOR_altivec_lvx_v16qi_2op,\n \t\t\t\t\texp, target, false);\n     case ALTIVEC_BUILTIN_LVLX:\n       return altivec_expand_lv_builtin (CODE_FOR_altivec_lvlx,\n@@ -37139,14 +37227,21 @@ insn_is_swappable_p (swap_web_entry *insn_entry, rtx insn,\n      fix them up by converting them to permuting ones.  Exceptions:\n      UNSPEC_LVE, UNSPEC_LVX, and UNSPEC_STVX, which have a PARALLEL\n      body instead of a SET; and UNSPEC_STVE, which has an UNSPEC\n-     for the SET source.  */\n+     for the SET source.  Also we must now make an exception for lvx\n+     and stvx when they are not in the UNSPEC_LVX/STVX form (with the\n+     explicit \"& -16\") since this leads to unrecognizable insns.  */\n   rtx body = PATTERN (insn);\n   int i = INSN_UID (insn);\n \n   if (insn_entry[i].is_load)\n     {\n       if (GET_CODE (body) == SET)\n \t{\n+\t  rtx rhs = SET_SRC (body);\n+\t  gcc_assert (GET_CODE (rhs) == MEM);\n+\t  if (GET_CODE (XEXP (rhs, 0)) == AND)\n+\t    return 0;\n+\n \t  *special = SH_NOSWAP_LD;\n \t  return 1;\n \t}\n@@ -37156,8 +37251,14 @@ insn_is_swappable_p (swap_web_entry *insn_entry, rtx insn,\n \n   if (insn_entry[i].is_store)\n     {\n-      if (GET_CODE (body) == SET && GET_CODE (SET_SRC (body)) != UNSPEC)\n+      if (GET_CODE (body) == SET\n+\t  && GET_CODE (SET_SRC (body)) != UNSPEC)\n \t{\n+\t  rtx lhs = SET_DEST (body);\n+\t  gcc_assert (GET_CODE (lhs) == MEM);\n+\t  if (GET_CODE (XEXP (lhs, 0)) == AND)\n+\t    return 0;\n+\t  \n \t  *special = SH_NOSWAP_ST;\n \t  return 1;\n \t}\n@@ -37827,26 +37928,290 @@ dump_swap_insn_table (swap_web_entry *insn_entry)\n   fputs (\"\\n\", dump_file);\n }\n \n+/* Return RTX with its address canonicalized to (reg) or (+ reg reg).\n+   Here RTX is an (& addr (const_int -16)).  Always return a new copy\n+   to avoid problems with combine.  */\n+static rtx\n+alignment_with_canonical_addr (rtx align)\n+{\n+  rtx canon;\n+  rtx addr = XEXP (align, 0);\n+\n+  if (REG_P (addr))\n+    canon = addr;\n+\n+  else if (GET_CODE (addr) == PLUS)\n+    {\n+      rtx addrop0 = XEXP (addr, 0);\n+      rtx addrop1 = XEXP (addr, 1);\n+\n+      if (!REG_P (addrop0))\n+\taddrop0 = force_reg (GET_MODE (addrop0), addrop0);\n+\n+      if (!REG_P (addrop1))\n+\taddrop1 = force_reg (GET_MODE (addrop1), addrop1);\n+\n+      canon = gen_rtx_PLUS (GET_MODE (addr), addrop0, addrop1);\n+    }\n+\n+  else\n+    canon = force_reg (GET_MODE (addr), addr);\n+\n+  return gen_rtx_AND (GET_MODE (align), canon, GEN_INT (-16));\n+}\n+\n+/* Check whether an rtx is an alignment mask, and if so, return \n+   a fully-expanded rtx for the masking operation.  */\n+static rtx\n+alignment_mask (rtx_insn *insn)\n+{\n+  rtx body = PATTERN (insn);\n+\n+  if (GET_CODE (body) != SET\n+      || GET_CODE (SET_SRC (body)) != AND\n+      || !REG_P (XEXP (SET_SRC (body), 0)))\n+    return 0;\n+\n+  rtx mask = XEXP (SET_SRC (body), 1);\n+\n+  if (GET_CODE (mask) == CONST_INT)\n+    {\n+      if (INTVAL (mask) == -16)\n+\treturn alignment_with_canonical_addr (SET_SRC (body));\n+      else\n+\treturn 0;\n+    }\n+\n+  if (!REG_P (mask))\n+    return 0;\n+\n+  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+  df_ref use;\n+  rtx real_mask = 0;\n+\n+  FOR_EACH_INSN_INFO_USE (use, insn_info)\n+    {\n+      if (!rtx_equal_p (DF_REF_REG (use), mask))\n+\tcontinue;\n+\n+      struct df_link *def_link = DF_REF_CHAIN (use);\n+      if (!def_link || def_link->next)\n+\treturn 0;\n+\n+      rtx_insn *const_insn = DF_REF_INSN (def_link->ref);\n+      rtx const_body = PATTERN (const_insn);\n+      if (GET_CODE (const_body) != SET)\n+\treturn 0;\n+\n+      real_mask = SET_SRC (const_body);\n+\n+      if (GET_CODE (real_mask) != CONST_INT\n+\t  || INTVAL (real_mask) != -16)\n+\treturn 0;\n+    }\n+\n+  if (real_mask == 0)\n+    return 0;\n+\n+  return alignment_with_canonical_addr (SET_SRC (body));\n+}\n+\n+/* Given INSN that's a load or store based at BASE_REG, look for a\n+   feeding computation that aligns its address on a 16-byte boundary.  */\n+static rtx\n+find_alignment_op (rtx_insn *insn, rtx base_reg)\n+{\n+  df_ref base_use;\n+  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+  rtx and_operation = 0;\n+\n+  FOR_EACH_INSN_INFO_USE (base_use, insn_info)\n+    {\n+      if (!rtx_equal_p (DF_REF_REG (base_use), base_reg))\n+\tcontinue;\n+\n+      struct df_link *base_def_link = DF_REF_CHAIN (base_use);\n+      if (!base_def_link || base_def_link->next)\n+\tbreak;\n+\n+      rtx_insn *and_insn = DF_REF_INSN (base_def_link->ref);\n+      and_operation = alignment_mask (and_insn);\n+      if (and_operation != 0)\n+\tbreak;\n+    }\n+\n+  return and_operation;\n+}\n+\n+struct del_info { bool replace; rtx_insn *replace_insn; };\n+\n+/* If INSN is the load for an lvx pattern, put it in canonical form.  */\n+static void\n+recombine_lvx_pattern (rtx_insn *insn, del_info *to_delete)\n+{\n+  rtx body = PATTERN (insn);\n+  gcc_assert (GET_CODE (body) == SET\n+\t      && GET_CODE (SET_SRC (body)) == VEC_SELECT\n+\t      && GET_CODE (XEXP (SET_SRC (body), 0)) == MEM);\n+\n+  rtx mem = XEXP (SET_SRC (body), 0);\n+  rtx base_reg = XEXP (mem, 0);\n+\n+  rtx and_operation = find_alignment_op (insn, base_reg);\n+\n+  if (and_operation != 0)\n+    {\n+      df_ref def;\n+      struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+      FOR_EACH_INSN_INFO_DEF (def, insn_info)\n+\t{\n+\t  struct df_link *link = DF_REF_CHAIN (def);\n+\t  if (!link || link->next)\n+\t    break;\n+\n+\t  rtx_insn *swap_insn = DF_REF_INSN (link->ref);\n+\t  if (!insn_is_swap_p (swap_insn)\n+\t      || insn_is_load_p (swap_insn)\n+\t      || insn_is_store_p (swap_insn))\n+\t    break;\n+\n+\t  /* Expected lvx pattern found.  Change the swap to\n+\t     a copy, and propagate the AND operation into the\n+\t     load.  */\n+\t  to_delete[INSN_UID (swap_insn)].replace = true;\n+\t  to_delete[INSN_UID (swap_insn)].replace_insn = swap_insn;\n+\n+\t  XEXP (mem, 0) = and_operation;\n+\t  SET_SRC (body) = mem;\n+\t  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n+\t  df_insn_rescan (insn);\n+\t\t  \n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"lvx opportunity found at %d\\n\",\n+\t\t     INSN_UID (insn));\n+\t}\n+    }\n+}\n+\n+/* If INSN is the store for an stvx pattern, put it in canonical form.  */\n+static void\n+recombine_stvx_pattern (rtx_insn *insn, del_info *to_delete)\n+{\n+  rtx body = PATTERN (insn);\n+  gcc_assert (GET_CODE (body) == SET\n+\t      && GET_CODE (SET_DEST (body)) == MEM\n+\t      && GET_CODE (SET_SRC (body)) == VEC_SELECT);\n+  rtx mem = SET_DEST (body);\n+  rtx base_reg = XEXP (mem, 0);\n+\n+  rtx and_operation = find_alignment_op (insn, base_reg);\n+\n+  if (and_operation != 0)\n+    {\n+      rtx src_reg = XEXP (SET_SRC (body), 0);\n+      df_ref src_use;\n+      struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+      FOR_EACH_INSN_INFO_USE (src_use, insn_info)\n+\t{\n+\t  if (!rtx_equal_p (DF_REF_REG (src_use), src_reg))\n+\t    continue;\n+\n+\t  struct df_link *link = DF_REF_CHAIN (src_use);\n+\t  if (!link || link->next)\n+\t    break;\n+\n+\t  rtx_insn *swap_insn = DF_REF_INSN (link->ref);\n+\t  if (!insn_is_swap_p (swap_insn)\n+\t      || insn_is_load_p (swap_insn)\n+\t      || insn_is_store_p (swap_insn))\n+\t    break;\n+\n+\t  /* Expected stvx pattern found.  Change the swap to\n+\t     a copy, and propagate the AND operation into the\n+\t     store.  */\n+\t  to_delete[INSN_UID (swap_insn)].replace = true;\n+\t  to_delete[INSN_UID (swap_insn)].replace_insn = swap_insn;\n+\n+\t  XEXP (mem, 0) = and_operation;\n+\t  SET_SRC (body) = src_reg;\n+\t  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n+\t  df_insn_rescan (insn);\n+\t\t  \n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"stvx opportunity found at %d\\n\",\n+\t\t     INSN_UID (insn));\n+\t}\n+    }\n+}\n+\n+/* Look for patterns created from builtin lvx and stvx calls, and\n+   canonicalize them to be properly recognized as such.  */\n+static void\n+recombine_lvx_stvx_patterns (function *fun)\n+{\n+  int i;\n+  basic_block bb;\n+  rtx_insn *insn;\n+\n+  int num_insns = get_max_uid ();\n+  del_info *to_delete = XCNEWVEC (del_info, num_insns);\n+\n+  FOR_ALL_BB_FN (bb, fun)\n+    FOR_BB_INSNS (bb, insn)\n+    {\n+      if (!NONDEBUG_INSN_P (insn))\n+\tcontinue;\n+\n+      if (insn_is_load_p (insn) && insn_is_swap_p (insn))\n+\trecombine_lvx_pattern (insn, to_delete);\n+      else if (insn_is_store_p (insn) && insn_is_swap_p (insn))\n+\trecombine_stvx_pattern (insn, to_delete);\n+    }\n+\n+  /* Turning swaps into copies is delayed until now, to avoid problems\n+     with deleting instructions during the insn walk.  */\n+  for (i = 0; i < num_insns; i++)\n+    if (to_delete[i].replace)\n+      {\n+\trtx swap_body = PATTERN (to_delete[i].replace_insn);\n+\trtx src_reg = XEXP (SET_SRC (swap_body), 0);\n+\trtx copy = gen_rtx_SET (SET_DEST (swap_body), src_reg);\n+\trtx_insn *new_insn = emit_insn_before (copy,\n+\t\t\t\t\t       to_delete[i].replace_insn);\n+\tset_block_for_insn (new_insn,\n+\t\t\t    BLOCK_FOR_INSN (to_delete[i].replace_insn));\n+\tdf_insn_rescan (new_insn);\n+\tdf_insn_delete (to_delete[i].replace_insn);\n+\tremove_insn (to_delete[i].replace_insn);\n+\tto_delete[i].replace_insn->set_deleted ();\n+      }\n+  \n+  free (to_delete);\n+}\n+\n /* Main entry point for this pass.  */\n unsigned int\n rs6000_analyze_swaps (function *fun)\n {\n   swap_web_entry *insn_entry;\n   basic_block bb;\n-  rtx_insn *insn;\n+  rtx_insn *insn, *curr_insn = 0;\n \n   /* Dataflow analysis for use-def chains.  */\n   df_set_flags (DF_RD_PRUNE_DEAD_DEFS);\n   df_chain_add_problem (DF_DU_CHAIN | DF_UD_CHAIN);\n   df_analyze ();\n   df_set_flags (DF_DEFER_INSN_RESCAN);\n \n+  /* Pre-pass to recombine lvx and stvx patterns so we don't lose info.  */\n+  recombine_lvx_stvx_patterns (fun);\n+\n   /* Allocate structure to represent webs of insns.  */\n   insn_entry = XCNEWVEC (swap_web_entry, get_max_uid ());\n \n   /* Walk the insns to gather basic data.  */\n   FOR_ALL_BB_FN (bb, fun)\n-    FOR_BB_INSNS (bb, insn)\n+    FOR_BB_INSNS_SAFE (bb, insn, curr_insn)\n     {\n       unsigned int uid = INSN_UID (insn);\n       if (NONDEBUG_INSN_P (insn))"}, {"sha": "5c66fe4b115d2e1b9128019f7760d9dcad404a6c", "filename": "gcc/config/rs6000/vector.md", "status": "modified", "additions": 16, "deletions": 2, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/gcc%2Fconfig%2Frs6000%2Fvector.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5/gcc%2Fconfig%2Frs6000%2Fvector.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fvector.md?ref=85a7c9263c2554811e9fc0e1ce7bec3b3bf6b7f5", "patch": "@@ -167,7 +167,14 @@\n   if (VECTOR_MEM_VSX_P (<MODE>mode))\n     {\n       operands[1] = rs6000_address_for_altivec (operands[1]);\n-      emit_insn (gen_altivec_lvx_<mode> (operands[0], operands[1]));\n+      rtx and_op = XEXP (operands[1], 0);\n+      gcc_assert (GET_CODE (and_op) == AND);\n+      rtx addr = XEXP (and_op, 0);\n+      if (GET_CODE (addr) == PLUS)\n+        emit_insn (gen_altivec_lvx_<mode>_2op (operands[0], XEXP (addr, 0),\n+\t                                       XEXP (addr, 1)));\n+      else\n+        emit_insn (gen_altivec_lvx_<mode>_1op (operands[0], operands[1]));\n       DONE;\n     }\n }\")\n@@ -183,7 +190,14 @@\n   if (VECTOR_MEM_VSX_P (<MODE>mode))\n     {\n       operands[0] = rs6000_address_for_altivec (operands[0]);\n-      emit_insn (gen_altivec_stvx_<mode> (operands[0], operands[1]));\n+      rtx and_op = XEXP (operands[0], 0);\n+      gcc_assert (GET_CODE (and_op) == AND);\n+      rtx addr = XEXP (and_op, 0);\n+      if (GET_CODE (addr) == PLUS)\n+        emit_insn (gen_altivec_stvx_<mode>_2op (operands[1], XEXP (addr, 0),\n+\t                                        XEXP (addr, 1)));\n+      else\n+        emit_insn (gen_altivec_stvx_<mode>_1op (operands[1], operands[0]));\n       DONE;\n     }\n }\")"}]}