{"sha": "5d8c099edebfe908256c2bd77a0e2b67182b0f57", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NWQ4YzA5OWVkZWJmZTkwODI1NmMyYmQ3N2EwZTJiNjcxODJiMGY1Nw==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-10-10T16:52:09Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-10-10T16:52:09Z"}, "message": "runtime: copy channel code from Go 1.7 runtime\n    \n    Change the compiler to use the new routines. Drop the separation of\n    small and large values when sending on a channel. Allocate the select\n    struct on the stack. Remove the old C implementation of channels. Adjust\n    the garbage collector for the new data structure.\n    \n    Bring in part of the tracing code, enough for the channel code to call.\n    \n    Bump the permitted number of allocations in one of the tests in\n    context_test.go. The difference is that now receiving from a channel\n    allocates a sudog, which the C code used to simply put on the\n    stack. This will be somewhat better when we port proc.go.\n    \n    Reviewed-on: https://go-review.googlesource.com/30714\n\nFrom-SVN: r240941", "tree": {"sha": "6033d9fc6d1e3f5c3019c9b13d28e70655697a67", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6033d9fc6d1e3f5c3019c9b13d28e70655697a67"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/5d8c099edebfe908256c2bd77a0e2b67182b0f57", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5d8c099edebfe908256c2bd77a0e2b67182b0f57", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5d8c099edebfe908256c2bd77a0e2b67182b0f57", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5d8c099edebfe908256c2bd77a0e2b67182b0f57/comments", "author": null, "committer": null, "parents": [{"sha": "40962ac03a869cf7e07e0672c0a649371896277b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/40962ac03a869cf7e07e0672c0a649371896277b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/40962ac03a869cf7e07e0672c0a649371896277b"}], "stats": {"total": 3974, "additions": 2676, "deletions": 1298}, "files": [{"sha": "05752dac7a2112343e13046af89c83bb385cbe7a", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -1,4 +1,4 @@\n-2431267d513804a3b1aa71adde9aefba9e3c3c59\n+9401e714d690e3907a64ac5c8cd5aed9e28f511b\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "e0df77dee67c4d4c2c72ff9941c820e586847ae0", "filename": "gcc/go/gofrontend/escape.cc", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Fescape.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Fescape.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fescape.cc?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -293,7 +293,6 @@ Node::op_format() const\n \t\t  break;\n \n \t\tcase Runtime::MAKECHAN:\n-\t\tcase Runtime::MAKECHANBIG:\n \t\tcase Runtime::MAKEMAP:\n \t\tcase Runtime::MAKESLICE1:\n \t\tcase Runtime::MAKESLICE2:\n@@ -1229,7 +1228,6 @@ Escape_analysis_assign::expression(Expression** pexpr)\n \t\tbreak;\n \n \t      case Runtime::MAKECHAN:\n-\t      case Runtime::MAKECHANBIG:\n \t      case Runtime::MAKEMAP:\n \t      case Runtime::MAKESLICE1:\n \t      case Runtime::MAKESLICE2:\n@@ -1838,7 +1836,6 @@ Escape_analysis_assign::assign(Node* dst, Node* src)\n \t\t    }\n \n \t\t  case Runtime::MAKECHAN:\n-\t\t  case Runtime::MAKECHANBIG:\n \t\t  case Runtime::MAKEMAP:\n \t\t  case Runtime::MAKESLICE1:\n \t\t  case Runtime::MAKESLICE2:\n@@ -2612,7 +2609,6 @@ Escape_analysis_flood::flood(Level level, Node* dst, Node* src,\n \t\t      break;\n \n \t\t    case Runtime::MAKECHAN:\n-\t\t    case Runtime::MAKECHANBIG:\n \t\t    case Runtime::MAKEMAP:\n \t\t    case Runtime::MAKESLICE1:\n \t\t    case Runtime::MAKESLICE2:"}, {"sha": "36000ead4aeafc4141a150e5b3b0fe3ff6cfb4b4", "filename": "gcc/go/gofrontend/expressions.cc", "status": "modified", "additions": 30, "deletions": 8, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -3604,6 +3604,7 @@ Unsafe_type_conversion_expression::do_get_backend(Translate_context* context)\n               || et->channel_type() != NULL\n               || et->map_type() != NULL\n               || et->function_type() != NULL\n+\t      || et->integer_type() != NULL\n               || et->is_nil_type());\n   else if (et->is_unsafe_pointer_type())\n     go_assert(t->points_to() != NULL);\n@@ -7077,6 +7078,7 @@ Builtin_call_expression::do_flatten(Gogo*, Named_object*,\n       break;\n \n     case BUILTIN_LEN:\n+    case BUILTIN_CAP:\n       Expression_list::iterator pa = this->args()->begin();\n       if (!(*pa)->is_variable()\n \t  && ((*pa)->type()->map_type() != NULL\n@@ -7217,10 +7219,7 @@ Builtin_call_expression::lower_make()\n \t\t\t      Expression::make_nil(loc),\n \t\t\t      Expression::make_nil(loc));\n   else if (is_chan)\n-    call = Runtime::make_call((have_big_args\n-\t\t\t       ? Runtime::MAKECHANBIG\n-\t\t\t       : Runtime::MAKECHAN),\n-\t\t\t      loc, 2, type_arg, len_arg);\n+    call = Runtime::make_call(Runtime::MAKECHAN, loc, 2, type_arg, len_arg);\n   else\n     go_unreachable();\n \n@@ -8300,7 +8299,31 @@ Builtin_call_expression::do_get_backend(Translate_context* context)\n \t\tthis->seen_ = false;\n \t      }\n \t    else if (arg_type->channel_type() != NULL)\n-              val = Runtime::make_call(Runtime::CHAN_CAP, location, 1, arg);\n+\t      {\n+\t\t// The second field is the capacity.  If the pointer\n+\t\t// is nil, the capacity is zero.\n+\t\tType* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n+\t\tType* pint_type = Type::make_pointer_type(int_type);\n+\t\tExpression* parg = Expression::make_unsafe_cast(uintptr_type,\n+\t\t\t\t\t\t\t\targ,\n+\t\t\t\t\t\t\t\tlocation);\n+\t\tint off = int_type->integer_type()->bits() / 8;\n+\t\tExpression* eoff = Expression::make_integer_ul(off,\n+\t\t\t\t\t\t\t       uintptr_type,\n+\t\t\t\t\t\t\t       location);\n+\t\tparg = Expression::make_binary(OPERATOR_PLUS, parg, eoff,\n+\t\t\t\t\t       location);\n+\t\tparg = Expression::make_unsafe_cast(pint_type, parg, location);\n+\t\tExpression* nil = Expression::make_nil(location);\n+\t\tnil = Expression::make_cast(pint_type, nil, location);\n+\t\tExpression* cmp = Expression::make_binary(OPERATOR_EQEQ,\n+\t\t\t\t\t\t\t  arg, nil, location);\n+\t\tExpression* zero = Expression::make_integer_ul(0, int_type,\n+\t\t\t\t\t\t\t       location);\n+\t\tExpression* indir = Expression::make_unary(OPERATOR_MULT,\n+\t\t\t\t\t\t\t   parg, location);\n+\t\tval = Expression::make_conditional(cmp, zero, indir, location);\n+\t      }\n \t    else\n \t      go_unreachable();\n \t  }\n@@ -13729,9 +13752,8 @@ Receive_expression::do_get_backend(Translate_context* context)\n   Expression* recv_addr =\n     Expression::make_temporary_reference(this->temp_receiver_, loc);\n   recv_addr = Expression::make_unary(OPERATOR_AND, recv_addr, loc);\n-  Expression* recv =\n-    Runtime::make_call(Runtime::RECEIVE, loc, 3,\n-\t\t       td, this->channel_, recv_addr);\n+  Expression* recv = Runtime::make_call(Runtime::CHANRECV1, loc, 3,\n+\t\t\t\t\ttd, this->channel_, recv_addr);\n   return Expression::make_compound(recv, recv_ref, loc)->get_backend(context);\n }\n "}, {"sha": "168f473933f5ec0383ea023dcd8669d9b2c73d45", "filename": "gcc/go/gofrontend/runtime.def", "status": "modified", "additions": 6, "deletions": 13, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Fruntime.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Fruntime.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fruntime.def?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -127,28 +127,21 @@ DEF_GO_RUNTIME(MAPITERNEXT, \"runtime.mapiternext\", P1(POINTER), R0())\n \n \n // Make a channel.\n-DEF_GO_RUNTIME(MAKECHAN, \"__go_new_channel\", P2(TYPE, UINTPTR), R1(CHAN))\n-DEF_GO_RUNTIME(MAKECHANBIG, \"__go_new_channel_big\", P2(TYPE, UINT64), R1(CHAN))\n+DEF_GO_RUNTIME(MAKECHAN, \"runtime.makechan\", P2(TYPE, INT64), R1(CHAN))\n \n-// Get the capacity of a channel (the size of the buffer).\n-DEF_GO_RUNTIME(CHAN_CAP, \"__go_chan_cap\", P1(CHAN), R1(INT))\n-\n-// Send a small value on a channel.\n-DEF_GO_RUNTIME(SEND_SMALL, \"__go_send_small\", P3(TYPE, CHAN, UINT64), R0())\n-\n-// Send a big value on a channel.\n-DEF_GO_RUNTIME(SEND_BIG, \"__go_send_big\", P3(TYPE, CHAN, POINTER), R0())\n+// Send a value on a channel.\n+DEF_GO_RUNTIME(CHANSEND, \"runtime.chansend1\", P3(TYPE, CHAN, POINTER), R0())\n \n // Receive a value from a channel.\n-DEF_GO_RUNTIME(RECEIVE, \"__go_receive\", P3(TYPE, CHAN, POINTER), R0())\n+DEF_GO_RUNTIME(CHANRECV1, \"runtime.chanrecv1\", P3(TYPE, CHAN, POINTER), R0())\n \n // Receive a value from a channel returning whether it is closed.\n DEF_GO_RUNTIME(CHANRECV2, \"runtime.chanrecv2\", P3(TYPE, CHAN, POINTER),\n \t       R1(BOOL))\n \n \n // Start building a select statement.\n-DEF_GO_RUNTIME(NEWSELECT, \"runtime.newselect\", P1(INT32), R1(POINTER))\n+DEF_GO_RUNTIME(NEWSELECT, \"runtime.newselect\", P3(POINTER, INT64, INT32), R0())\n \n // Add a default clause to a select statement.\n DEF_GO_RUNTIME(SELECTDEFAULT, \"runtime.selectdefault\",\n@@ -202,7 +195,7 @@ DEF_GO_RUNTIME(RUNTIME_ERROR, \"__go_runtime_error\", P1(INT32), R0())\n \n \n // Close.\n-DEF_GO_RUNTIME(CLOSE, \"__go_builtin_close\", P1(CHAN), R0())\n+DEF_GO_RUNTIME(CLOSE, \"runtime.closechan\", P1(CHAN), R0())\n \n \n // Copy."}, {"sha": "41f1ffb06ac7286c45fff50420d455844263e911", "filename": "gcc/go/gofrontend/statements.cc", "status": "modified", "additions": 31, "deletions": 31, "changes": 62, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Fstatements.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Fstatements.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fstatements.cc?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -4330,7 +4330,6 @@ Send_statement::do_get_backend(Translate_context* context)\n \t\t\t\t\t\t       element_type,\n \t\t\t\t\t\t       this->val_, loc);\n \n-  bool is_small;\n   bool can_take_address;\n   switch (element_type->base()->classification())\n     {\n@@ -4340,25 +4339,18 @@ Send_statement::do_get_backend(Translate_context* context)\n     case Type::TYPE_POINTER:\n     case Type::TYPE_MAP:\n     case Type::TYPE_CHANNEL:\n-      is_small = true;\n-      can_take_address = false;\n-      break;\n-\n     case Type::TYPE_FLOAT:\n     case Type::TYPE_COMPLEX:\n     case Type::TYPE_STRING:\n     case Type::TYPE_INTERFACE:\n-      is_small = false;\n       can_take_address = false;\n       break;\n \n     case Type::TYPE_STRUCT:\n-      is_small = false;\n       can_take_address = true;\n       break;\n \n     case Type::TYPE_ARRAY:\n-      is_small = false;\n       can_take_address = !element_type->is_slice_type();\n       break;\n \n@@ -4384,36 +4376,28 @@ Send_statement::do_get_backend(Translate_context* context)\n   Expression* td = Expression::make_type_descriptor(this->channel_->type(),\n \t\t\t\t\t\t    loc);\n \n-  Runtime::Function code;\n   Bstatement* btemp = NULL;\n-  if (is_small)\n-      {\n-\t// Type is small enough to handle as uint64.\n-\tcode = Runtime::SEND_SMALL;\n-\tval = Expression::make_unsafe_cast(Type::lookup_integer_type(\"uint64\"),\n-\t\t\t\t\t   val, loc);\n-      }\n-  else if (can_take_address)\n-    {\n-      // Must pass address of value.  The function doesn't change the\n-      // value, so just take its address directly.\n-      code = Runtime::SEND_BIG;\n+  if (can_take_address)\n+    {\n+      // The function doesn't change the value, so just take its\n+      // address directly.\n       val = Expression::make_unary(OPERATOR_AND, val, loc);\n     }\n   else\n     {\n-      // Must pass address of value, but the value is small enough\n-      // that it might be in registers.  Copy value into temporary\n-      // variable to take address.\n-      code = Runtime::SEND_BIG;\n+      // The value is not in a variable, or is small enough that it\n+      // might be in a register, and taking the address would push it\n+      // on the stack.  Copy it into a temporary variable to take the\n+      // address.\n       Temporary_statement* temp = Statement::make_temporary(element_type,\n \t\t\t\t\t\t\t    val, loc);\n       Expression* ref = Expression::make_temporary_reference(temp, loc);\n       val = Expression::make_unary(OPERATOR_AND, ref, loc);\n       btemp = temp->get_backend(context);\n     }\n \n-  Expression* call = Runtime::make_call(code, loc, 3, td, this->channel_, val);\n+  Expression* call = Runtime::make_call(Runtime::CHANSEND, loc, 3, td,\n+\t\t\t\t\tthis->channel_, val);\n \n   context->gogo()->lower_expression(context->function(), NULL, &call);\n   Bexpression* bcall = call->get_backend(context);\n@@ -4491,6 +4475,7 @@ Select_clauses::Select_clause::lower(Gogo* gogo, Named_object* function,\n   Location loc = this->location_;\n \n   Expression* selref = Expression::make_temporary_reference(sel, loc);\n+  selref = Expression::make_unary(OPERATOR_AND, selref, loc);\n \n   Expression* index_expr = Expression::make_integer_ul(this->index_, NULL,\n \t\t\t\t\t\t       loc);\n@@ -4854,6 +4839,7 @@ Select_clauses::get_backend(Translate_context* context,\n     }\n \n   Expression* selref = Expression::make_temporary_reference(sel, location);\n+  selref = Expression::make_unary(OPERATOR_AND, selref, location);\n   Expression* call = Runtime::make_call(Runtime::SELECTGO, location, 1,\n \t\t\t\t\tselref);\n   context->gogo()->lower_expression(context->function(), NULL, &call);\n@@ -4920,13 +4906,27 @@ Select_statement::do_lower(Gogo* gogo, Named_object* function,\n \n   go_assert(this->sel_ == NULL);\n \n-  Expression* size_expr = Expression::make_integer_ul(this->clauses_->size(),\n-\t\t\t\t\t\t      NULL, loc);\n-  Expression* call = Runtime::make_call(Runtime::NEWSELECT, loc, 1, size_expr);\n-\n-  this->sel_ = Statement::make_temporary(NULL, call, loc);\n+  int ncases = this->clauses_->size();\n+  Type* selstruct_type = Channel_type::select_type(ncases);\n+  this->sel_ = Statement::make_temporary(selstruct_type, NULL, loc);\n   b->add_statement(this->sel_);\n \n+  int64_t selstruct_size;\n+  if (!selstruct_type->backend_type_size(gogo, &selstruct_size))\n+    {\n+      go_assert(saw_errors());\n+      return Statement::make_error_statement(loc);\n+    }\n+\n+  Expression* ref = Expression::make_temporary_reference(this->sel_, loc);\n+  ref = Expression::make_unary(OPERATOR_AND, ref, loc);\n+  Expression* selstruct_size_expr =\n+    Expression::make_integer_int64(selstruct_size, NULL, loc);\n+  Expression* size_expr = Expression::make_integer_ul(ncases, NULL, loc);\n+  Expression* call = Runtime::make_call(Runtime::NEWSELECT, loc, 3,\n+\t\t\t\t\tref, selstruct_size_expr, size_expr);\n+  b->add_statement(Statement::make_statement(call, true));\n+\n   this->clauses_->lower(gogo, function, b, this->sel_);\n   this->is_lowered_ = true;\n   b->add_statement(this);"}, {"sha": "38613bb920ea82c99d149be6553c60e4d9303118", "filename": "gcc/go/gofrontend/types.cc", "status": "modified", "additions": 47, "deletions": 0, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Ftypes.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Ftypes.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Ftypes.cc?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -7771,6 +7771,53 @@ Channel_type::do_import(Import* imp)\n   return Type::make_channel_type(may_send, may_receive, element_type);\n }\n \n+// Return the type to manage a select statement with ncases case\n+// statements.  A value of this type is allocated on the stack.  This\n+// must match the type hselect in libgo/go/runtime/select.go.\n+\n+Type*\n+Channel_type::select_type(int ncases)\n+{\n+  Type* unsafe_pointer_type = Type::make_pointer_type(Type::make_void_type());\n+  Type* uint16_type = Type::lookup_integer_type(\"uint16\");\n+\n+  static Struct_type* scase_type;\n+  if (scase_type == NULL)\n+    {\n+      Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n+      Type* uint64_type = Type::lookup_integer_type(\"uint64\");\n+      scase_type =\n+\tType::make_builtin_struct_type(7,\n+\t\t\t\t       \"elem\", unsafe_pointer_type,\n+\t\t\t\t       \"chan\", unsafe_pointer_type,\n+\t\t\t\t       \"pc\", uintptr_type,\n+\t\t\t\t       \"kind\", uint16_type,\n+\t\t\t\t       \"index\", uint16_type,\n+\t\t\t\t       \"receivedp\", unsafe_pointer_type,\n+\t\t\t\t       \"releasetime\", uint64_type);\n+      scase_type->set_is_struct_incomparable();\n+    }\n+\n+  Expression* ncases_expr =\n+    Expression::make_integer_ul(ncases, NULL, Linemap::predeclared_location());\n+  Array_type* scases = Type::make_array_type(scase_type, ncases_expr);\n+  scases->set_is_array_incomparable();\n+  Array_type* order = Type::make_array_type(uint16_type, ncases_expr);\n+  order->set_is_array_incomparable();\n+\n+  Struct_type* ret =\n+    Type::make_builtin_struct_type(7,\n+\t\t\t\t   \"tcase\", uint16_type,\n+\t\t\t\t   \"ncase\", uint16_type,\n+\t\t\t\t   \"pollorder\", unsafe_pointer_type,\n+\t\t\t\t   \"lockorder\", unsafe_pointer_type,\n+\t\t\t\t   \"scase\", scases,\n+\t\t\t\t   \"lockorderarr\", order,\n+\t\t\t\t   \"pollorderarr\", order);\n+  ret->set_is_struct_incomparable();\n+  return ret;\n+}\n+\n // Make a new channel type.\n \n Channel_type*"}, {"sha": "58d60e5a0118ab51d04b08bacf70c9634150795f", "filename": "gcc/go/gofrontend/types.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Ftypes.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/gcc%2Fgo%2Fgofrontend%2Ftypes.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Ftypes.h?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -2809,6 +2809,9 @@ class Channel_type : public Type\n   static Type*\n   make_chan_type_descriptor_type();\n \n+  static Type*\n+  select_type(int ncases);\n+\n  protected:\n   int\n   do_traverse(Traverse* traverse)"}, {"sha": "bc47be6773af59a5746320c0091a1fcdd71da0f0", "filename": "libgo/Makefile.am", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2FMakefile.am", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2FMakefile.am", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2FMakefile.am?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -520,7 +520,6 @@ runtime_files = \\\n \t$(runtime_thread_files) \\\n \truntime/yield.c \\\n \t$(rtems_task_variable_add_file) \\\n-\tchan.c \\\n \tcpuprof.c \\\n \tgo-iface.c \\\n \tlfstack.c \\"}, {"sha": "5806d75f77bc8d494efbe999bb2fe8a83580a087", "filename": "libgo/Makefile.in", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2FMakefile.in?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -263,9 +263,9 @@ am__objects_6 = go-append.lo go-assert.lo go-assert-interface.lo \\\n \t$(am__objects_1) mfixalloc.lo mgc0.lo mheap.lo msize.lo \\\n \t$(am__objects_2) panic.lo parfor.lo print.lo proc.lo \\\n \truntime.lo signal_unix.lo thread.lo $(am__objects_3) yield.lo \\\n-\t$(am__objects_4) chan.lo cpuprof.lo go-iface.lo lfstack.lo \\\n-\tmalloc.lo mprof.lo netpoll.lo rdebug.lo reflect.lo runtime1.lo \\\n-\tsema.lo sigqueue.lo string.lo time.lo $(am__objects_5)\n+\t$(am__objects_4) cpuprof.lo go-iface.lo lfstack.lo malloc.lo \\\n+\tmprof.lo netpoll.lo rdebug.lo reflect.lo runtime1.lo sema.lo \\\n+\tsigqueue.lo string.lo time.lo $(am__objects_5)\n am_libgo_llgo_la_OBJECTS = $(am__objects_6)\n libgo_llgo_la_OBJECTS = $(am_libgo_llgo_la_OBJECTS)\n libgo_llgo_la_LINK = $(LIBTOOL) --tag=CC $(AM_LIBTOOLFLAGS) \\\n@@ -921,7 +921,6 @@ runtime_files = \\\n \t$(runtime_thread_files) \\\n \truntime/yield.c \\\n \t$(rtems_task_variable_add_file) \\\n-\tchan.c \\\n \tcpuprof.c \\\n \tgo-iface.c \\\n \tlfstack.c \\\n@@ -1557,7 +1556,6 @@ mostlyclean-compile:\n distclean-compile:\n \t-rm -f *.tab.c\n \n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/chan.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/cpuprof.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/env_posix.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/getncpu-bsd.Plo@am__quote@"}, {"sha": "cdfec079c372f1f9bef2ebe29ee2999090eb5870", "filename": "libgo/go/context/context_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fcontext%2Fcontext_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fcontext%2Fcontext_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fcontext%2Fcontext_test.go?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -381,7 +381,7 @@ func TestAllocs(t *testing.T) {\n \t\t\t\t<-c.Done()\n \t\t\t},\n \t\t\tlimit:      8,\n-\t\t\tgccgoLimit: 15,\n+\t\t\tgccgoLimit: 18,\n \t\t},\n \t\t{\n \t\t\tdesc: \"WithCancel(bg)\","}, {"sha": "eb2cad6d1dc900c5b78422d233f784c8c0441c04", "filename": "libgo/go/runtime/chan.go", "status": "added", "additions": 724, "deletions": 0, "changes": 724, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fruntime%2Fchan.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fruntime%2Fchan.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fchan.go?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -0,0 +1,724 @@\n+// Copyright 2014 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime\n+\n+// This file contains the implementation of Go channels.\n+\n+// Invariants:\n+//  At least one of c.sendq and c.recvq is empty.\n+// For buffered channels, also:\n+//  c.qcount > 0 implies that c.recvq is empty.\n+//  c.qcount < c.dataqsiz implies that c.sendq is empty.\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"unsafe\"\n+)\n+\n+// For gccgo, use go:linkname to rename compiler-called functions to\n+// themselves, so that the compiler will export them.\n+//\n+//go:linkname makechan runtime.makechan\n+//go:linkname chansend1 runtime.chansend1\n+//go:linkname chanrecv1 runtime.chanrecv1\n+//go:linkname chanrecv2 runtime.chanrecv2\n+//go:linkname closechan runtime.closechan\n+\n+const (\n+\tmaxAlign  = 8\n+\thchanSize = unsafe.Sizeof(hchan{}) + uintptr(-int(unsafe.Sizeof(hchan{}))&(maxAlign-1))\n+\tdebugChan = false\n+)\n+\n+type hchan struct {\n+\tqcount   uint           // total data in the queue\n+\tdataqsiz uint           // size of the circular queue\n+\tbuf      unsafe.Pointer // points to an array of dataqsiz elements\n+\telemsize uint16\n+\tclosed   uint32\n+\telemtype *_type // element type\n+\tsendx    uint   // send index\n+\trecvx    uint   // receive index\n+\trecvq    waitq  // list of recv waiters\n+\tsendq    waitq  // list of send waiters\n+\n+\t// lock protects all fields in hchan, as well as several\n+\t// fields in sudogs blocked on this channel.\n+\t//\n+\t// Do not change another G's status while holding this lock\n+\t// (in particular, do not ready a G), as this can deadlock\n+\t// with stack shrinking.\n+\tlock mutex\n+}\n+\n+type waitq struct {\n+\tfirst *sudog\n+\tlast  *sudog\n+}\n+\n+//go:linkname reflect_makechan reflect.makechan\n+func reflect_makechan(t *chantype, size int64) *hchan {\n+\treturn makechan(t, size)\n+}\n+\n+func makechan(t *chantype, size int64) *hchan {\n+\telem := t.elem\n+\n+\t// compiler checks this but be safe.\n+\tif elem.size >= 1<<16 {\n+\t\tthrow(\"makechan: invalid channel element type\")\n+\t}\n+\tif hchanSize%maxAlign != 0 || elem.align > maxAlign {\n+\t\tthrow(\"makechan: bad alignment\")\n+\t}\n+\tif size < 0 || int64(uintptr(size)) != size || (elem.size > 0 && uintptr(size) > (_MaxMem-hchanSize)/elem.size) {\n+\t\tpanic(plainError(\"makechan: size out of range\"))\n+\t}\n+\n+\tvar c *hchan\n+\tif elem.kind&kindNoPointers != 0 || size == 0 {\n+\t\t// Allocate memory in one call.\n+\t\t// Hchan does not contain pointers interesting for GC in this case:\n+\t\t// buf points into the same allocation, elemtype is persistent.\n+\t\t// SudoG's are referenced from their owning thread so they can't be collected.\n+\t\t// TODO(dvyukov,rlh): Rethink when collector can move allocated objects.\n+\t\tc = (*hchan)(mallocgc(hchanSize+uintptr(size)*elem.size, nil, true))\n+\t\tif size > 0 && elem.size != 0 {\n+\t\t\tc.buf = add(unsafe.Pointer(c), hchanSize)\n+\t\t} else {\n+\t\t\t// race detector uses this location for synchronization\n+\t\t\t// Also prevents us from pointing beyond the allocation (see issue 9401).\n+\t\t\tc.buf = unsafe.Pointer(c)\n+\t\t}\n+\t} else {\n+\t\tc = new(hchan)\n+\t\tc.buf = newarray(elem, int(size))\n+\t}\n+\tc.elemsize = uint16(elem.size)\n+\tc.elemtype = elem\n+\tc.dataqsiz = uint(size)\n+\n+\tif debugChan {\n+\t\tprint(\"makechan: chan=\", c, \"; elemsize=\", elem.size, \"; dataqsiz=\", size, \"\\n\")\n+\t}\n+\treturn c\n+}\n+\n+// chanbuf(c, i) is pointer to the i'th slot in the buffer.\n+func chanbuf(c *hchan, i uint) unsafe.Pointer {\n+\treturn add(c.buf, uintptr(i)*uintptr(c.elemsize))\n+}\n+\n+// entry point for c <- x from compiled code\n+//go:nosplit\n+func chansend1(t *chantype, c *hchan, elem unsafe.Pointer) {\n+\tchansend(t, c, elem, true, getcallerpc(unsafe.Pointer(&t)))\n+}\n+\n+/*\n+ * generic single channel send/recv\n+ * If block is not nil,\n+ * then the protocol will not\n+ * sleep but return if it could\n+ * not complete.\n+ *\n+ * sleep can wake up with g.param == nil\n+ * when a channel involved in the sleep has\n+ * been closed.  it is easiest to loop and re-run\n+ * the operation; we'll see that it's now closed.\n+ */\n+func chansend(t *chantype, c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool {\n+\tif raceenabled {\n+\t\traceReadObjectPC(t.elem, ep, callerpc, funcPC(chansend))\n+\t}\n+\tif msanenabled {\n+\t\tmsanread(ep, t.elem.size)\n+\t}\n+\n+\tif c == nil {\n+\t\tif !block {\n+\t\t\treturn false\n+\t\t}\n+\t\tgopark(nil, nil, \"chan send (nil chan)\", traceEvGoStop, 2)\n+\t\tthrow(\"unreachable\")\n+\t}\n+\n+\tif debugChan {\n+\t\tprint(\"chansend: chan=\", c, \"\\n\")\n+\t}\n+\n+\tif raceenabled {\n+\t\tracereadpc(unsafe.Pointer(c), callerpc, funcPC(chansend))\n+\t}\n+\n+\t// Fast path: check for failed non-blocking operation without acquiring the lock.\n+\t//\n+\t// After observing that the channel is not closed, we observe that the channel is\n+\t// not ready for sending. Each of these observations is a single word-sized read\n+\t// (first c.closed and second c.recvq.first or c.qcount depending on kind of channel).\n+\t// Because a closed channel cannot transition from 'ready for sending' to\n+\t// 'not ready for sending', even if the channel is closed between the two observations,\n+\t// they imply a moment between the two when the channel was both not yet closed\n+\t// and not ready for sending. We behave as if we observed the channel at that moment,\n+\t// and report that the send cannot proceed.\n+\t//\n+\t// It is okay if the reads are reordered here: if we observe that the channel is not\n+\t// ready for sending and then observe that it is not closed, that implies that the\n+\t// channel wasn't closed during the first observation.\n+\tif !block && c.closed == 0 && ((c.dataqsiz == 0 && c.recvq.first == nil) ||\n+\t\t(c.dataqsiz > 0 && c.qcount == c.dataqsiz)) {\n+\t\treturn false\n+\t}\n+\n+\tvar t0 int64\n+\tif blockprofilerate > 0 {\n+\t\tt0 = cputicks()\n+\t}\n+\n+\tlock(&c.lock)\n+\n+\tif c.closed != 0 {\n+\t\tunlock(&c.lock)\n+\t\tpanic(plainError(\"send on closed channel\"))\n+\t}\n+\n+\tif sg := c.recvq.dequeue(); sg != nil {\n+\t\t// Found a waiting receiver. We pass the value we want to send\n+\t\t// directly to the receiver, bypassing the channel buffer (if any).\n+\t\tsend(c, sg, ep, func() { unlock(&c.lock) })\n+\t\treturn true\n+\t}\n+\n+\tif c.qcount < c.dataqsiz {\n+\t\t// Space is available in the channel buffer. Enqueue the element to send.\n+\t\tqp := chanbuf(c, c.sendx)\n+\t\tif raceenabled {\n+\t\t\traceacquire(qp)\n+\t\t\tracerelease(qp)\n+\t\t}\n+\t\ttypedmemmove(c.elemtype, qp, ep)\n+\t\tc.sendx++\n+\t\tif c.sendx == c.dataqsiz {\n+\t\t\tc.sendx = 0\n+\t\t}\n+\t\tc.qcount++\n+\t\tunlock(&c.lock)\n+\t\treturn true\n+\t}\n+\n+\tif !block {\n+\t\tunlock(&c.lock)\n+\t\treturn false\n+\t}\n+\n+\t// Block on the channel. Some receiver will complete our operation for us.\n+\tgp := getg()\n+\tmysg := acquireSudog()\n+\tmysg.releasetime = 0\n+\tif t0 != 0 {\n+\t\tmysg.releasetime = -1\n+\t}\n+\t// No stack splits between assigning elem and enqueuing mysg\n+\t// on gp.waiting where copystack can find it.\n+\tmysg.elem = ep\n+\tmysg.waitlink = nil\n+\tmysg.g = gp\n+\tmysg.selectdone = nil\n+\tmysg.c = c\n+\tgp.waiting = mysg\n+\tgp.param = nil\n+\tc.sendq.enqueue(mysg)\n+\tgoparkunlock(&c.lock, \"chan send\", traceEvGoBlockSend, 3)\n+\n+\t// someone woke us up.\n+\tif mysg != gp.waiting {\n+\t\tthrow(\"G waiting list is corrupted\")\n+\t}\n+\tgp.waiting = nil\n+\tif gp.param == nil {\n+\t\tif c.closed == 0 {\n+\t\t\tthrow(\"chansend: spurious wakeup\")\n+\t\t}\n+\t\tpanic(plainError(\"send on closed channel\"))\n+\t}\n+\tgp.param = nil\n+\tif mysg.releasetime > 0 {\n+\t\tblockevent(mysg.releasetime-t0, 2)\n+\t}\n+\tmysg.c = nil\n+\treleaseSudog(mysg)\n+\treturn true\n+}\n+\n+// send processes a send operation on an empty channel c.\n+// The value ep sent by the sender is copied to the receiver sg.\n+// The receiver is then woken up to go on its merry way.\n+// Channel c must be empty and locked.  send unlocks c with unlockf.\n+// sg must already be dequeued from c.\n+// ep must be non-nil and point to the heap or the caller's stack.\n+func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func()) {\n+\tif raceenabled {\n+\t\tif c.dataqsiz == 0 {\n+\t\t\tracesync(c, sg)\n+\t\t} else {\n+\t\t\t// Pretend we go through the buffer, even though\n+\t\t\t// we copy directly. Note that we need to increment\n+\t\t\t// the head/tail locations only when raceenabled.\n+\t\t\tqp := chanbuf(c, c.recvx)\n+\t\t\traceacquire(qp)\n+\t\t\tracerelease(qp)\n+\t\t\traceacquireg(sg.g, qp)\n+\t\t\tracereleaseg(sg.g, qp)\n+\t\t\tc.recvx++\n+\t\t\tif c.recvx == c.dataqsiz {\n+\t\t\t\tc.recvx = 0\n+\t\t\t}\n+\t\t\tc.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz\n+\t\t}\n+\t}\n+\tif sg.elem != nil {\n+\t\tsendDirect(c.elemtype, sg, ep)\n+\t\tsg.elem = nil\n+\t}\n+\tgp := sg.g\n+\tunlockf()\n+\tgp.param = unsafe.Pointer(sg)\n+\tif sg.releasetime != 0 {\n+\t\tsg.releasetime = cputicks()\n+\t}\n+\tgoready(gp, 4)\n+}\n+\n+func sendDirect(t *_type, sg *sudog, src unsafe.Pointer) {\n+\t// Send on an unbuffered or empty-buffered channel is the only operation\n+\t// in the entire runtime where one goroutine\n+\t// writes to the stack of another goroutine. The GC assumes that\n+\t// stack writes only happen when the goroutine is running and are\n+\t// only done by that goroutine. Using a write barrier is sufficient to\n+\t// make up for violating that assumption, but the write barrier has to work.\n+\t// typedmemmove will call heapBitsBulkBarrier, but the target bytes\n+\t// are not in the heap, so that will not help. We arrange to call\n+\t// memmove and typeBitsBulkBarrier instead.\n+\n+\t// Once we read sg.elem out of sg, it will no longer\n+\t// be updated if the destination's stack gets copied (shrunk).\n+\t// So make sure that no preemption points can happen between read & use.\n+\tdst := sg.elem\n+\tmemmove(dst, src, t.size)\n+\ttypeBitsBulkBarrier(t, uintptr(dst), t.size)\n+}\n+\n+func closechan(c *hchan) {\n+\tif c == nil {\n+\t\tpanic(plainError(\"close of nil channel\"))\n+\t}\n+\n+\tlock(&c.lock)\n+\tif c.closed != 0 {\n+\t\tunlock(&c.lock)\n+\t\tpanic(plainError(\"close of closed channel\"))\n+\t}\n+\n+\tif raceenabled {\n+\t\tcallerpc := getcallerpc(unsafe.Pointer(&c))\n+\t\tracewritepc(unsafe.Pointer(c), callerpc, funcPC(closechan))\n+\t\tracerelease(unsafe.Pointer(c))\n+\t}\n+\n+\tc.closed = 1\n+\n+\tvar glist *g\n+\n+\t// release all readers\n+\tfor {\n+\t\tsg := c.recvq.dequeue()\n+\t\tif sg == nil {\n+\t\t\tbreak\n+\t\t}\n+\t\tif sg.elem != nil {\n+\t\t\tmemclr(sg.elem, uintptr(c.elemsize))\n+\t\t\tsg.elem = nil\n+\t\t}\n+\t\tif sg.releasetime != 0 {\n+\t\t\tsg.releasetime = cputicks()\n+\t\t}\n+\t\tgp := sg.g\n+\t\tgp.param = nil\n+\t\tif raceenabled {\n+\t\t\traceacquireg(gp, unsafe.Pointer(c))\n+\t\t}\n+\t\tgp.schedlink.set(glist)\n+\t\tglist = gp\n+\t}\n+\n+\t// release all writers (they will panic)\n+\tfor {\n+\t\tsg := c.sendq.dequeue()\n+\t\tif sg == nil {\n+\t\t\tbreak\n+\t\t}\n+\t\tsg.elem = nil\n+\t\tif sg.releasetime != 0 {\n+\t\t\tsg.releasetime = cputicks()\n+\t\t}\n+\t\tgp := sg.g\n+\t\tgp.param = nil\n+\t\tif raceenabled {\n+\t\t\traceacquireg(gp, unsafe.Pointer(c))\n+\t\t}\n+\t\tgp.schedlink.set(glist)\n+\t\tglist = gp\n+\t}\n+\tunlock(&c.lock)\n+\n+\t// Ready all Gs now that we've dropped the channel lock.\n+\tfor glist != nil {\n+\t\tgp := glist\n+\t\tglist = glist.schedlink.ptr()\n+\t\tgp.schedlink = 0\n+\t\tgoready(gp, 3)\n+\t}\n+}\n+\n+// entry points for <- c from compiled code\n+//go:nosplit\n+func chanrecv1(t *chantype, c *hchan, elem unsafe.Pointer) {\n+\tchanrecv(t, c, elem, true)\n+}\n+\n+//go:nosplit\n+func chanrecv2(t *chantype, c *hchan, elem unsafe.Pointer) (received bool) {\n+\t_, received = chanrecv(t, c, elem, true)\n+\treturn\n+}\n+\n+// chanrecv receives on channel c and writes the received data to ep.\n+// ep may be nil, in which case received data is ignored.\n+// If block == false and no elements are available, returns (false, false).\n+// Otherwise, if c is closed, zeros *ep and returns (true, false).\n+// Otherwise, fills in *ep with an element and returns (true, true).\n+// A non-nil ep must point to the heap or the caller's stack.\n+func chanrecv(t *chantype, c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) {\n+\t// raceenabled: don't need to check ep, as it is always on the stack\n+\t// or is new memory allocated by reflect.\n+\n+\tif debugChan {\n+\t\tprint(\"chanrecv: chan=\", c, \"\\n\")\n+\t}\n+\n+\tif c == nil {\n+\t\tif !block {\n+\t\t\treturn\n+\t\t}\n+\t\tgopark(nil, nil, \"chan receive (nil chan)\", traceEvGoStop, 2)\n+\t\tthrow(\"unreachable\")\n+\t}\n+\n+\t// Fast path: check for failed non-blocking operation without acquiring the lock.\n+\t//\n+\t// After observing that the channel is not ready for receiving, we observe that the\n+\t// channel is not closed. Each of these observations is a single word-sized read\n+\t// (first c.sendq.first or c.qcount, and second c.closed).\n+\t// Because a channel cannot be reopened, the later observation of the channel\n+\t// being not closed implies that it was also not closed at the moment of the\n+\t// first observation. We behave as if we observed the channel at that moment\n+\t// and report that the receive cannot proceed.\n+\t//\n+\t// The order of operations is important here: reversing the operations can lead to\n+\t// incorrect behavior when racing with a close.\n+\tif !block && (c.dataqsiz == 0 && c.sendq.first == nil ||\n+\t\tc.dataqsiz > 0 && atomic.Loaduint(&c.qcount) == 0) &&\n+\t\tatomic.Load(&c.closed) == 0 {\n+\t\treturn\n+\t}\n+\n+\tvar t0 int64\n+\tif blockprofilerate > 0 {\n+\t\tt0 = cputicks()\n+\t}\n+\n+\tlock(&c.lock)\n+\n+\tif c.closed != 0 && c.qcount == 0 {\n+\t\tif raceenabled {\n+\t\t\traceacquire(unsafe.Pointer(c))\n+\t\t}\n+\t\tunlock(&c.lock)\n+\t\tif ep != nil {\n+\t\t\tmemclr(ep, uintptr(c.elemsize))\n+\t\t}\n+\t\treturn true, false\n+\t}\n+\n+\tif sg := c.sendq.dequeue(); sg != nil {\n+\t\t// Found a waiting sender. If buffer is size 0, receive value\n+\t\t// directly from sender. Otherwise, receive from head of queue\n+\t\t// and add sender's value to the tail of the queue (both map to\n+\t\t// the same buffer slot because the queue is full).\n+\t\trecv(c, sg, ep, func() { unlock(&c.lock) })\n+\t\treturn true, true\n+\t}\n+\n+\tif c.qcount > 0 {\n+\t\t// Receive directly from queue\n+\t\tqp := chanbuf(c, c.recvx)\n+\t\tif raceenabled {\n+\t\t\traceacquire(qp)\n+\t\t\tracerelease(qp)\n+\t\t}\n+\t\tif ep != nil {\n+\t\t\ttypedmemmove(c.elemtype, ep, qp)\n+\t\t}\n+\t\tmemclr(qp, uintptr(c.elemsize))\n+\t\tc.recvx++\n+\t\tif c.recvx == c.dataqsiz {\n+\t\t\tc.recvx = 0\n+\t\t}\n+\t\tc.qcount--\n+\t\tunlock(&c.lock)\n+\t\treturn true, true\n+\t}\n+\n+\tif !block {\n+\t\tunlock(&c.lock)\n+\t\treturn false, false\n+\t}\n+\n+\t// no sender available: block on this channel.\n+\tgp := getg()\n+\tmysg := acquireSudog()\n+\tmysg.releasetime = 0\n+\tif t0 != 0 {\n+\t\tmysg.releasetime = -1\n+\t}\n+\t// No stack splits between assigning elem and enqueuing mysg\n+\t// on gp.waiting where copystack can find it.\n+\tmysg.elem = ep\n+\tmysg.waitlink = nil\n+\tgp.waiting = mysg\n+\tmysg.g = gp\n+\tmysg.selectdone = nil\n+\tmysg.c = c\n+\tgp.param = nil\n+\tc.recvq.enqueue(mysg)\n+\tgoparkunlock(&c.lock, \"chan receive\", traceEvGoBlockRecv, 3)\n+\n+\t// someone woke us up\n+\tif mysg != gp.waiting {\n+\t\tthrow(\"G waiting list is corrupted\")\n+\t}\n+\tgp.waiting = nil\n+\tif mysg.releasetime > 0 {\n+\t\tblockevent(mysg.releasetime-t0, 2)\n+\t}\n+\tclosed := gp.param == nil\n+\tgp.param = nil\n+\tmysg.c = nil\n+\treleaseSudog(mysg)\n+\treturn true, !closed\n+}\n+\n+// recv processes a receive operation on a full channel c.\n+// There are 2 parts:\n+// 1) The value sent by the sender sg is put into the channel\n+//    and the sender is woken up to go on its merry way.\n+// 2) The value received by the receiver (the current G) is\n+//    written to ep.\n+// For synchronous channels, both values are the same.\n+// For asynchronous channels, the receiver gets its data from\n+// the channel buffer and the sender's data is put in the\n+// channel buffer.\n+// Channel c must be full and locked. recv unlocks c with unlockf.\n+// sg must already be dequeued from c.\n+// A non-nil ep must point to the heap or the caller's stack.\n+func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func()) {\n+\tif c.dataqsiz == 0 {\n+\t\tif raceenabled {\n+\t\t\tracesync(c, sg)\n+\t\t}\n+\t\tif ep != nil {\n+\t\t\t// copy data from sender\n+\t\t\t// ep points to our own stack or heap, so nothing\n+\t\t\t// special (ala sendDirect) needed here.\n+\t\t\ttypedmemmove(c.elemtype, ep, sg.elem)\n+\t\t}\n+\t} else {\n+\t\t// Queue is full. Take the item at the\n+\t\t// head of the queue. Make the sender enqueue\n+\t\t// its item at the tail of the queue. Since the\n+\t\t// queue is full, those are both the same slot.\n+\t\tqp := chanbuf(c, c.recvx)\n+\t\tif raceenabled {\n+\t\t\traceacquire(qp)\n+\t\t\tracerelease(qp)\n+\t\t\traceacquireg(sg.g, qp)\n+\t\t\tracereleaseg(sg.g, qp)\n+\t\t}\n+\t\t// copy data from queue to receiver\n+\t\tif ep != nil {\n+\t\t\ttypedmemmove(c.elemtype, ep, qp)\n+\t\t}\n+\t\t// copy data from sender to queue\n+\t\ttypedmemmove(c.elemtype, qp, sg.elem)\n+\t\tc.recvx++\n+\t\tif c.recvx == c.dataqsiz {\n+\t\t\tc.recvx = 0\n+\t\t}\n+\t\tc.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz\n+\t}\n+\tsg.elem = nil\n+\tgp := sg.g\n+\tunlockf()\n+\tgp.param = unsafe.Pointer(sg)\n+\tif sg.releasetime != 0 {\n+\t\tsg.releasetime = cputicks()\n+\t}\n+\tgoready(gp, 4)\n+}\n+\n+// compiler implements\n+//\n+//\tselect {\n+//\tcase c <- v:\n+//\t\t... foo\n+//\tdefault:\n+//\t\t... bar\n+//\t}\n+//\n+// as\n+//\n+//\tif selectnbsend(c, v) {\n+//\t\t... foo\n+//\t} else {\n+//\t\t... bar\n+//\t}\n+//\n+func selectnbsend(t *chantype, c *hchan, elem unsafe.Pointer) (selected bool) {\n+\treturn chansend(t, c, elem, false, getcallerpc(unsafe.Pointer(&t)))\n+}\n+\n+// compiler implements\n+//\n+//\tselect {\n+//\tcase v = <-c:\n+//\t\t... foo\n+//\tdefault:\n+//\t\t... bar\n+//\t}\n+//\n+// as\n+//\n+//\tif selectnbrecv(&v, c) {\n+//\t\t... foo\n+//\t} else {\n+//\t\t... bar\n+//\t}\n+//\n+func selectnbrecv(t *chantype, elem unsafe.Pointer, c *hchan) (selected bool) {\n+\tselected, _ = chanrecv(t, c, elem, false)\n+\treturn\n+}\n+\n+// compiler implements\n+//\n+//\tselect {\n+//\tcase v, ok = <-c:\n+//\t\t... foo\n+//\tdefault:\n+//\t\t... bar\n+//\t}\n+//\n+// as\n+//\n+//\tif c != nil && selectnbrecv2(&v, &ok, c) {\n+//\t\t... foo\n+//\t} else {\n+//\t\t... bar\n+//\t}\n+//\n+func selectnbrecv2(t *chantype, elem unsafe.Pointer, received *bool, c *hchan) (selected bool) {\n+\t// TODO(khr): just return 2 values from this function, now that it is in Go.\n+\tselected, *received = chanrecv(t, c, elem, false)\n+\treturn\n+}\n+\n+//go:linkname reflect_chansend reflect.chansend\n+func reflect_chansend(t *chantype, c *hchan, elem unsafe.Pointer, nb bool) (selected bool) {\n+\treturn chansend(t, c, elem, !nb, getcallerpc(unsafe.Pointer(&t)))\n+}\n+\n+//go:linkname reflect_chanrecv reflect.chanrecv\n+func reflect_chanrecv(t *chantype, c *hchan, nb bool, elem unsafe.Pointer) (selected bool, received bool) {\n+\treturn chanrecv(t, c, elem, !nb)\n+}\n+\n+//go:linkname reflect_chanlen reflect.chanlen\n+func reflect_chanlen(c *hchan) int {\n+\tif c == nil {\n+\t\treturn 0\n+\t}\n+\treturn int(c.qcount)\n+}\n+\n+//go:linkname reflect_chancap reflect.chancap\n+func reflect_chancap(c *hchan) int {\n+\tif c == nil {\n+\t\treturn 0\n+\t}\n+\treturn int(c.dataqsiz)\n+}\n+\n+//go:linkname reflect_chanclose reflect.chanclose\n+func reflect_chanclose(c *hchan) {\n+\tclosechan(c)\n+}\n+\n+func (q *waitq) enqueue(sgp *sudog) {\n+\tsgp.next = nil\n+\tx := q.last\n+\tif x == nil {\n+\t\tsgp.prev = nil\n+\t\tq.first = sgp\n+\t\tq.last = sgp\n+\t\treturn\n+\t}\n+\tsgp.prev = x\n+\tx.next = sgp\n+\tq.last = sgp\n+}\n+\n+func (q *waitq) dequeue() *sudog {\n+\tfor {\n+\t\tsgp := q.first\n+\t\tif sgp == nil {\n+\t\t\treturn nil\n+\t\t}\n+\t\ty := sgp.next\n+\t\tif y == nil {\n+\t\t\tq.first = nil\n+\t\t\tq.last = nil\n+\t\t} else {\n+\t\t\ty.prev = nil\n+\t\t\tq.first = y\n+\t\t\tsgp.next = nil // mark as removed (see dequeueSudog)\n+\t\t}\n+\n+\t\t// if sgp participates in a select and is already signaled, ignore it\n+\t\tif sgp.selectdone != nil {\n+\t\t\t// claim the right to signal\n+\t\t\tif *sgp.selectdone != 0 || !atomic.Cas(sgp.selectdone, 0, 1) {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn sgp\n+\t}\n+}\n+\n+func racesync(c *hchan, sg *sudog) {\n+\tracerelease(chanbuf(c, 0))\n+\traceacquireg(sg.g, chanbuf(c, 0))\n+\tracereleaseg(sg.g, chanbuf(c, 0))\n+\traceacquire(chanbuf(c, 0))\n+}"}, {"sha": "688efcdcb83712fa995ae4819efefe1c11dd6b6d", "filename": "libgo/go/runtime/runtime2.go", "status": "modified", "additions": 2, "deletions": 6, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fruntime%2Fruntime2.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fruntime%2Fruntime2.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime2.go?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -231,9 +231,6 @@ func (mp *muintptr) set(m *m) { *mp = muintptr(unsafe.Pointer(m)) }\n //\n // sudogs are allocated from a special pool. Use acquireSudog and\n // releaseSudog to allocate and free them.\n-/*\n-Commented out for gccgo for now.\n-\n type sudog struct {\n \t// The following fields are protected by the hchan.lock of the\n \t// channel this sudog is blocking on. shrinkstack depends on\n@@ -253,7 +250,6 @@ type sudog struct {\n \twaitlink    *sudog // g.waiting list\n \tc           *hchan // channel\n }\n-*/\n \n type gcstats struct {\n \t// the struct must consist of only uint64's,\n@@ -364,7 +360,7 @@ type g struct {\n \tgopc     uintptr // pc of go statement that created this goroutine\n \tstartpc  uintptr // pc of goroutine function\n \tracectx  uintptr\n-\t// Not for gccgo for now: waiting        *sudog    // sudog structures this g is waiting on (that have a valid elem ptr); in lock order\n+\twaiting  *sudog // sudog structures this g is waiting on (that have a valid elem ptr); in lock order\n \t// Not for gccgo: cgoCtxt        []uintptr // cgo traceback context\n \n \t// Per-G GC state\n@@ -528,7 +524,7 @@ type p struct {\n \tgfree    *g\n \tgfreecnt int32\n \n-\t// Not for gccgo for now: sudogcache []*sudog\n+\tsudogcache []*sudog\n \t// Not for gccgo for now: sudogbuf   [128]*sudog\n \n \t// Not for gccgo for now: tracebuf traceBufPtr"}, {"sha": "08446a1ffddaf4853e7abb5083817a45257e1905", "filename": "libgo/go/runtime/select.go", "status": "added", "additions": 697, "deletions": 0, "changes": 697, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fruntime%2Fselect.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fruntime%2Fselect.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fselect.go?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -0,0 +1,697 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime\n+\n+// This file contains the implementation of Go select statements.\n+\n+import (\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+// For gccgo, use go:linkname to rename compiler-called functions to\n+// themselves, so that the compiler will export them.\n+//\n+//go:linkname newselect runtime.newselect\n+//go:linkname selectdefault runtime.selectdefault\n+//go:linkname selectsend runtime.selectsend\n+//go:linkname selectrecv runtime.selectrecv\n+//go:linkname selectrecv2 runtime.selectrecv2\n+//go:linkname selectgo runtime.selectgo\n+\n+const (\n+\tdebugSelect = false\n+\n+\t// scase.kind\n+\tcaseRecv = iota\n+\tcaseSend\n+\tcaseDefault\n+)\n+\n+// Select statement header.\n+// Known to compiler.\n+// Changes here must also be made in src/cmd/internal/gc/select.go's selecttype.\n+type hselect struct {\n+\ttcase     uint16   // total count of scase[]\n+\tncase     uint16   // currently filled scase[]\n+\tpollorder *uint16  // case poll order\n+\tlockorder *uint16  // channel lock order\n+\tscase     [1]scase // one per case (in order of appearance)\n+}\n+\n+// Select case descriptor.\n+// Known to compiler.\n+// Changes here must also be made in src/cmd/internal/gc/select.go's selecttype.\n+type scase struct {\n+\telem        unsafe.Pointer // data element\n+\tc           *hchan         // chan\n+\tpc          uintptr        // return pc\n+\tkind        uint16\n+\tindex       uint16 // case index\n+\treceivedp   *bool  // pointer to received bool (recv2)\n+\treleasetime int64\n+}\n+\n+var (\n+\tchansendpc = funcPC(chansend)\n+\tchanrecvpc = funcPC(chanrecv)\n+)\n+\n+func selectsize(size uintptr) uintptr {\n+\tselsize := unsafe.Sizeof(hselect{}) +\n+\t\t(size-1)*unsafe.Sizeof(hselect{}.scase[0]) +\n+\t\tsize*unsafe.Sizeof(*hselect{}.lockorder) +\n+\t\tsize*unsafe.Sizeof(*hselect{}.pollorder)\n+\treturn round(selsize, sys.Int64Align)\n+}\n+\n+func newselect(sel *hselect, selsize int64, size int32) {\n+\tif selsize != int64(selectsize(uintptr(size))) {\n+\t\tprint(\"runtime: bad select size \", selsize, \", want \", selectsize(uintptr(size)), \"\\n\")\n+\t\tthrow(\"bad select size\")\n+\t}\n+\tif size != int32(uint16(size)) {\n+\t\tthrow(\"select size too large\")\n+\t}\n+\tsel.tcase = uint16(size)\n+\tsel.ncase = 0\n+\tsel.lockorder = (*uint16)(add(unsafe.Pointer(&sel.scase), uintptr(size)*unsafe.Sizeof(hselect{}.scase[0])))\n+\tsel.pollorder = (*uint16)(add(unsafe.Pointer(sel.lockorder), uintptr(size)*unsafe.Sizeof(*hselect{}.lockorder)))\n+\n+\t// For gccgo the temporary variable will not have been zeroed.\n+\tmemclr(unsafe.Pointer(&sel.scase), uintptr(size)*unsafe.Sizeof(hselect{}.scase[0])+uintptr(size)*unsafe.Sizeof(*hselect{}.lockorder)+uintptr(size)*unsafe.Sizeof(*hselect{}.pollorder))\n+\n+\tif debugSelect {\n+\t\tprint(\"newselect s=\", sel, \" size=\", size, \"\\n\")\n+\t}\n+}\n+\n+func selectsend(sel *hselect, c *hchan, elem unsafe.Pointer, index int32) {\n+\t// nil cases do not compete\n+\tif c != nil {\n+\t\tselectsendImpl(sel, c, getcallerpc(unsafe.Pointer(&sel)), elem, index)\n+\t}\n+\treturn\n+}\n+\n+// cut in half to give stack a chance to split\n+func selectsendImpl(sel *hselect, c *hchan, pc uintptr, elem unsafe.Pointer, index int32) {\n+\ti := sel.ncase\n+\tif i >= sel.tcase {\n+\t\tthrow(\"selectsend: too many cases\")\n+\t}\n+\tsel.ncase = i + 1\n+\tcas := (*scase)(add(unsafe.Pointer(&sel.scase), uintptr(i)*unsafe.Sizeof(sel.scase[0])))\n+\n+\tcas.pc = pc\n+\tcas.c = c\n+\tcas.index = uint16(index)\n+\tcas.kind = caseSend\n+\tcas.elem = elem\n+\n+\tif debugSelect {\n+\t\tprint(\"selectsend s=\", sel, \" pc=\", hex(cas.pc), \" chan=\", cas.c, \" index=\", cas.index, \"\\n\")\n+\t}\n+}\n+\n+func selectrecv(sel *hselect, c *hchan, elem unsafe.Pointer, index int32) {\n+\t// nil cases do not compete\n+\tif c != nil {\n+\t\tselectrecvImpl(sel, c, getcallerpc(unsafe.Pointer(&sel)), elem, nil, index)\n+\t}\n+\treturn\n+}\n+\n+func selectrecv2(sel *hselect, c *hchan, elem unsafe.Pointer, received *bool, index int32) {\n+\t// nil cases do not compete\n+\tif c != nil {\n+\t\tselectrecvImpl(sel, c, getcallerpc(unsafe.Pointer(&sel)), elem, received, index)\n+\t}\n+\treturn\n+}\n+\n+func selectrecvImpl(sel *hselect, c *hchan, pc uintptr, elem unsafe.Pointer, received *bool, index int32) {\n+\ti := sel.ncase\n+\tif i >= sel.tcase {\n+\t\tthrow(\"selectrecv: too many cases\")\n+\t}\n+\tsel.ncase = i + 1\n+\tcas := (*scase)(add(unsafe.Pointer(&sel.scase), uintptr(i)*unsafe.Sizeof(sel.scase[0])))\n+\tcas.pc = pc\n+\tcas.c = c\n+\tcas.index = uint16(index)\n+\tcas.kind = caseRecv\n+\tcas.elem = elem\n+\tcas.receivedp = received\n+\n+\tif debugSelect {\n+\t\tprint(\"selectrecv s=\", sel, \" pc=\", hex(cas.pc), \" chan=\", cas.c, \" index=\", cas.index, \"\\n\")\n+\t}\n+}\n+\n+func selectdefault(sel *hselect, index int32) {\n+\tselectdefaultImpl(sel, getcallerpc(unsafe.Pointer(&sel)), index)\n+\treturn\n+}\n+\n+func selectdefaultImpl(sel *hselect, callerpc uintptr, index int32) {\n+\ti := sel.ncase\n+\tif i >= sel.tcase {\n+\t\tthrow(\"selectdefault: too many cases\")\n+\t}\n+\tsel.ncase = i + 1\n+\tcas := (*scase)(add(unsafe.Pointer(&sel.scase), uintptr(i)*unsafe.Sizeof(sel.scase[0])))\n+\tcas.pc = callerpc\n+\tcas.c = nil\n+\tcas.index = uint16(index)\n+\tcas.kind = caseDefault\n+\n+\tif debugSelect {\n+\t\tprint(\"selectdefault s=\", sel, \" pc=\", hex(cas.pc), \" index=\", cas.index, \"\\n\")\n+\t}\n+}\n+\n+func sellock(scases []scase, lockorder []uint16) {\n+\tvar c *hchan\n+\tfor _, o := range lockorder {\n+\t\tc0 := scases[o].c\n+\t\tif c0 != nil && c0 != c {\n+\t\t\tc = c0\n+\t\t\tlock(&c.lock)\n+\t\t}\n+\t}\n+}\n+\n+func selunlock(scases []scase, lockorder []uint16) {\n+\t// We must be very careful here to not touch sel after we have unlocked\n+\t// the last lock, because sel can be freed right after the last unlock.\n+\t// Consider the following situation.\n+\t// First M calls runtime\u00b7park() in runtime\u00b7selectgo() passing the sel.\n+\t// Once runtime\u00b7park() has unlocked the last lock, another M makes\n+\t// the G that calls select runnable again and schedules it for execution.\n+\t// When the G runs on another M, it locks all the locks and frees sel.\n+\t// Now if the first M touches sel, it will access freed memory.\n+\tn := len(scases)\n+\tr := 0\n+\t// skip the default case\n+\tif n > 0 && scases[lockorder[0]].c == nil {\n+\t\tr = 1\n+\t}\n+\tfor i := n - 1; i >= r; i-- {\n+\t\tc := scases[lockorder[i]].c\n+\t\tif i > 0 && c == scases[lockorder[i-1]].c {\n+\t\t\tcontinue // will unlock it on the next iteration\n+\t\t}\n+\t\tunlock(&c.lock)\n+\t}\n+}\n+\n+func selparkcommit(gp *g, _ unsafe.Pointer) bool {\n+\t// This must not access gp's stack (see gopark). In\n+\t// particular, it must not access the *hselect. That's okay,\n+\t// because by the time this is called, gp.waiting has all\n+\t// channels in lock order.\n+\tvar lastc *hchan\n+\tfor sg := gp.waiting; sg != nil; sg = sg.waitlink {\n+\t\tif sg.c != lastc && lastc != nil {\n+\t\t\t// As soon as we unlock the channel, fields in\n+\t\t\t// any sudog with that channel may change,\n+\t\t\t// including c and waitlink. Since multiple\n+\t\t\t// sudogs may have the same channel, we unlock\n+\t\t\t// only after we've passed the last instance\n+\t\t\t// of a channel.\n+\t\t\tunlock(&lastc.lock)\n+\t\t}\n+\t\tlastc = sg.c\n+\t}\n+\tif lastc != nil {\n+\t\tunlock(&lastc.lock)\n+\t}\n+\treturn true\n+}\n+\n+func block() {\n+\tgopark(nil, nil, \"select (no cases)\", traceEvGoStop, 1) // forever\n+}\n+\n+// selectgo implements the select statement.\n+//\n+// *sel is on the current goroutine's stack (regardless of any\n+// escaping in selectgo).\n+//\n+// selectgo does not return. Instead, it overwrites its return PC and\n+// returns directly to the triggered select case. Because of this, it\n+// cannot appear at the top of a split stack.\n+func selectgo(sel *hselect) int32 {\n+\t_, index := selectgoImpl(sel)\n+\treturn int32(index)\n+}\n+\n+// selectgoImpl returns scase.pc and scase.so for the select\n+// case which fired.\n+func selectgoImpl(sel *hselect) (uintptr, uint16) {\n+\tif debugSelect {\n+\t\tprint(\"select: sel=\", sel, \"\\n\")\n+\t}\n+\n+\tscaseslice := slice{unsafe.Pointer(&sel.scase), int(sel.ncase), int(sel.ncase)}\n+\tscases := *(*[]scase)(unsafe.Pointer(&scaseslice))\n+\n+\tvar t0 int64\n+\tif blockprofilerate > 0 {\n+\t\tt0 = cputicks()\n+\t\tfor i := 0; i < int(sel.ncase); i++ {\n+\t\t\tscases[i].releasetime = -1\n+\t\t}\n+\t}\n+\n+\t// The compiler rewrites selects that statically have\n+\t// only 0 or 1 cases plus default into simpler constructs.\n+\t// The only way we can end up with such small sel.ncase\n+\t// values here is for a larger select in which most channels\n+\t// have been nilled out. The general code handles those\n+\t// cases correctly, and they are rare enough not to bother\n+\t// optimizing (and needing to test).\n+\n+\t// generate permuted order\n+\tpollslice := slice{unsafe.Pointer(sel.pollorder), int(sel.ncase), int(sel.ncase)}\n+\tpollorder := *(*[]uint16)(unsafe.Pointer(&pollslice))\n+\tfor i := 1; i < int(sel.ncase); i++ {\n+\t\tj := int(fastrand1()) % (i + 1)\n+\t\tpollorder[i] = pollorder[j]\n+\t\tpollorder[j] = uint16(i)\n+\t}\n+\n+\t// sort the cases by Hchan address to get the locking order.\n+\t// simple heap sort, to guarantee n log n time and constant stack footprint.\n+\tlockslice := slice{unsafe.Pointer(sel.lockorder), int(sel.ncase), int(sel.ncase)}\n+\tlockorder := *(*[]uint16)(unsafe.Pointer(&lockslice))\n+\tfor i := 0; i < int(sel.ncase); i++ {\n+\t\tj := i\n+\t\t// Start with the pollorder to permute cases on the same channel.\n+\t\tc := scases[pollorder[i]].c\n+\t\tfor j > 0 && scases[lockorder[(j-1)/2]].c.sortkey() < c.sortkey() {\n+\t\t\tk := (j - 1) / 2\n+\t\t\tlockorder[j] = lockorder[k]\n+\t\t\tj = k\n+\t\t}\n+\t\tlockorder[j] = pollorder[i]\n+\t}\n+\tfor i := int(sel.ncase) - 1; i >= 0; i-- {\n+\t\to := lockorder[i]\n+\t\tc := scases[o].c\n+\t\tlockorder[i] = lockorder[0]\n+\t\tj := 0\n+\t\tfor {\n+\t\t\tk := j*2 + 1\n+\t\t\tif k >= i {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tif k+1 < i && scases[lockorder[k]].c.sortkey() < scases[lockorder[k+1]].c.sortkey() {\n+\t\t\t\tk++\n+\t\t\t}\n+\t\t\tif c.sortkey() < scases[lockorder[k]].c.sortkey() {\n+\t\t\t\tlockorder[j] = lockorder[k]\n+\t\t\t\tj = k\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tbreak\n+\t\t}\n+\t\tlockorder[j] = o\n+\t}\n+\t/*\n+\t\tfor i := 0; i+1 < int(sel.ncase); i++ {\n+\t\t\tif scases[lockorder[i]].c.sortkey() > scases[lockorder[i+1]].c.sortkey() {\n+\t\t\t\tprint(\"i=\", i, \" x=\", lockorder[i], \" y=\", lockorder[i+1], \"\\n\")\n+\t\t\t\tthrow(\"select: broken sort\")\n+\t\t\t}\n+\t\t}\n+\t*/\n+\n+\t// lock all the channels involved in the select\n+\tsellock(scases, lockorder)\n+\n+\tvar (\n+\t\tgp     *g\n+\t\tdone   uint32\n+\t\tsg     *sudog\n+\t\tc      *hchan\n+\t\tk      *scase\n+\t\tsglist *sudog\n+\t\tsgnext *sudog\n+\t\tqp     unsafe.Pointer\n+\t\tnextp  **sudog\n+\t)\n+\n+loop:\n+\t// pass 1 - look for something already waiting\n+\tvar dfl *scase\n+\tvar cas *scase\n+\tfor i := 0; i < int(sel.ncase); i++ {\n+\t\tcas = &scases[pollorder[i]]\n+\t\tc = cas.c\n+\n+\t\tswitch cas.kind {\n+\t\tcase caseRecv:\n+\t\t\tsg = c.sendq.dequeue()\n+\t\t\tif sg != nil {\n+\t\t\t\tgoto recv\n+\t\t\t}\n+\t\t\tif c.qcount > 0 {\n+\t\t\t\tgoto bufrecv\n+\t\t\t}\n+\t\t\tif c.closed != 0 {\n+\t\t\t\tgoto rclose\n+\t\t\t}\n+\n+\t\tcase caseSend:\n+\t\t\tif raceenabled {\n+\t\t\t\tracereadpc(unsafe.Pointer(c), cas.pc, chansendpc)\n+\t\t\t}\n+\t\t\tif c.closed != 0 {\n+\t\t\t\tgoto sclose\n+\t\t\t}\n+\t\t\tsg = c.recvq.dequeue()\n+\t\t\tif sg != nil {\n+\t\t\t\tgoto send\n+\t\t\t}\n+\t\t\tif c.qcount < c.dataqsiz {\n+\t\t\t\tgoto bufsend\n+\t\t\t}\n+\n+\t\tcase caseDefault:\n+\t\t\tdfl = cas\n+\t\t}\n+\t}\n+\n+\tif dfl != nil {\n+\t\tselunlock(scases, lockorder)\n+\t\tcas = dfl\n+\t\tgoto retc\n+\t}\n+\n+\t// pass 2 - enqueue on all chans\n+\tgp = getg()\n+\tdone = 0\n+\tif gp.waiting != nil {\n+\t\tthrow(\"gp.waiting != nil\")\n+\t}\n+\tnextp = &gp.waiting\n+\tfor _, casei := range lockorder {\n+\t\tcas = &scases[casei]\n+\t\tc = cas.c\n+\t\tsg := acquireSudog()\n+\t\tsg.g = gp\n+\t\t// Note: selectdone is adjusted for stack copies in stack1.go:adjustsudogs\n+\t\tsg.selectdone = (*uint32)(noescape(unsafe.Pointer(&done)))\n+\t\t// No stack splits between assigning elem and enqueuing\n+\t\t// sg on gp.waiting where copystack can find it.\n+\t\tsg.elem = cas.elem\n+\t\tsg.releasetime = 0\n+\t\tif t0 != 0 {\n+\t\t\tsg.releasetime = -1\n+\t\t}\n+\t\tsg.c = c\n+\t\t// Construct waiting list in lock order.\n+\t\t*nextp = sg\n+\t\tnextp = &sg.waitlink\n+\n+\t\tswitch cas.kind {\n+\t\tcase caseRecv:\n+\t\t\tc.recvq.enqueue(sg)\n+\n+\t\tcase caseSend:\n+\t\t\tc.sendq.enqueue(sg)\n+\t\t}\n+\t}\n+\n+\t// wait for someone to wake us up\n+\tgp.param = nil\n+\tgopark(selparkcommit, nil, \"select\", traceEvGoBlockSelect, 2)\n+\n+\t// someone woke us up\n+\tsellock(scases, lockorder)\n+\tsg = (*sudog)(gp.param)\n+\tgp.param = nil\n+\n+\t// pass 3 - dequeue from unsuccessful chans\n+\t// otherwise they stack up on quiet channels\n+\t// record the successful case, if any.\n+\t// We singly-linked up the SudoGs in lock order.\n+\tcas = nil\n+\tsglist = gp.waiting\n+\t// Clear all elem before unlinking from gp.waiting.\n+\tfor sg1 := gp.waiting; sg1 != nil; sg1 = sg1.waitlink {\n+\t\tsg1.selectdone = nil\n+\t\tsg1.elem = nil\n+\t\tsg1.c = nil\n+\t}\n+\tgp.waiting = nil\n+\n+\tfor _, casei := range lockorder {\n+\t\tk = &scases[casei]\n+\t\tif sglist.releasetime > 0 {\n+\t\t\tk.releasetime = sglist.releasetime\n+\t\t}\n+\t\tif sg == sglist {\n+\t\t\t// sg has already been dequeued by the G that woke us up.\n+\t\t\tcas = k\n+\t\t} else {\n+\t\t\tc = k.c\n+\t\t\tif k.kind == caseSend {\n+\t\t\t\tc.sendq.dequeueSudoG(sglist)\n+\t\t\t} else {\n+\t\t\t\tc.recvq.dequeueSudoG(sglist)\n+\t\t\t}\n+\t\t}\n+\t\tsgnext = sglist.waitlink\n+\t\tsglist.waitlink = nil\n+\t\treleaseSudog(sglist)\n+\t\tsglist = sgnext\n+\t}\n+\n+\tif cas == nil {\n+\t\t// This can happen if we were woken up by a close().\n+\t\t// TODO: figure that out explicitly so we don't need this loop.\n+\t\tgoto loop\n+\t}\n+\n+\tc = cas.c\n+\n+\tif debugSelect {\n+\t\tprint(\"wait-return: sel=\", sel, \" c=\", c, \" cas=\", cas, \" kind=\", cas.kind, \"\\n\")\n+\t}\n+\n+\tif cas.kind == caseRecv {\n+\t\tif cas.receivedp != nil {\n+\t\t\t*cas.receivedp = true\n+\t\t}\n+\t}\n+\n+\tif raceenabled {\n+\t\tif cas.kind == caseRecv && cas.elem != nil {\n+\t\t\traceWriteObjectPC(c.elemtype, cas.elem, cas.pc, chanrecvpc)\n+\t\t} else if cas.kind == caseSend {\n+\t\t\traceReadObjectPC(c.elemtype, cas.elem, cas.pc, chansendpc)\n+\t\t}\n+\t}\n+\tif msanenabled {\n+\t\tif cas.kind == caseRecv && cas.elem != nil {\n+\t\t\tmsanwrite(cas.elem, c.elemtype.size)\n+\t\t} else if cas.kind == caseSend {\n+\t\t\tmsanread(cas.elem, c.elemtype.size)\n+\t\t}\n+\t}\n+\n+\tselunlock(scases, lockorder)\n+\tgoto retc\n+\n+bufrecv:\n+\t// can receive from buffer\n+\tif raceenabled {\n+\t\tif cas.elem != nil {\n+\t\t\traceWriteObjectPC(c.elemtype, cas.elem, cas.pc, chanrecvpc)\n+\t\t}\n+\t\traceacquire(chanbuf(c, c.recvx))\n+\t\tracerelease(chanbuf(c, c.recvx))\n+\t}\n+\tif msanenabled && cas.elem != nil {\n+\t\tmsanwrite(cas.elem, c.elemtype.size)\n+\t}\n+\tif cas.receivedp != nil {\n+\t\t*cas.receivedp = true\n+\t}\n+\tqp = chanbuf(c, c.recvx)\n+\tif cas.elem != nil {\n+\t\ttypedmemmove(c.elemtype, cas.elem, qp)\n+\t}\n+\tmemclr(qp, uintptr(c.elemsize))\n+\tc.recvx++\n+\tif c.recvx == c.dataqsiz {\n+\t\tc.recvx = 0\n+\t}\n+\tc.qcount--\n+\tselunlock(scases, lockorder)\n+\tgoto retc\n+\n+bufsend:\n+\t// can send to buffer\n+\tif raceenabled {\n+\t\traceacquire(chanbuf(c, c.sendx))\n+\t\tracerelease(chanbuf(c, c.sendx))\n+\t\traceReadObjectPC(c.elemtype, cas.elem, cas.pc, chansendpc)\n+\t}\n+\tif msanenabled {\n+\t\tmsanread(cas.elem, c.elemtype.size)\n+\t}\n+\ttypedmemmove(c.elemtype, chanbuf(c, c.sendx), cas.elem)\n+\tc.sendx++\n+\tif c.sendx == c.dataqsiz {\n+\t\tc.sendx = 0\n+\t}\n+\tc.qcount++\n+\tselunlock(scases, lockorder)\n+\tgoto retc\n+\n+recv:\n+\t// can receive from sleeping sender (sg)\n+\trecv(c, sg, cas.elem, func() { selunlock(scases, lockorder) })\n+\tif debugSelect {\n+\t\tprint(\"syncrecv: sel=\", sel, \" c=\", c, \"\\n\")\n+\t}\n+\tif cas.receivedp != nil {\n+\t\t*cas.receivedp = true\n+\t}\n+\tgoto retc\n+\n+rclose:\n+\t// read at end of closed channel\n+\tselunlock(scases, lockorder)\n+\tif cas.receivedp != nil {\n+\t\t*cas.receivedp = false\n+\t}\n+\tif cas.elem != nil {\n+\t\tmemclr(cas.elem, uintptr(c.elemsize))\n+\t}\n+\tif raceenabled {\n+\t\traceacquire(unsafe.Pointer(c))\n+\t}\n+\tgoto retc\n+\n+send:\n+\t// can send to a sleeping receiver (sg)\n+\tif raceenabled {\n+\t\traceReadObjectPC(c.elemtype, cas.elem, cas.pc, chansendpc)\n+\t}\n+\tif msanenabled {\n+\t\tmsanread(cas.elem, c.elemtype.size)\n+\t}\n+\tsend(c, sg, cas.elem, func() { selunlock(scases, lockorder) })\n+\tif debugSelect {\n+\t\tprint(\"syncsend: sel=\", sel, \" c=\", c, \"\\n\")\n+\t}\n+\tgoto retc\n+\n+retc:\n+\tif cas.releasetime > 0 {\n+\t\tblockevent(cas.releasetime-t0, 2)\n+\t}\n+\treturn cas.pc, cas.index\n+\n+sclose:\n+\t// send on closed channel\n+\tselunlock(scases, lockorder)\n+\tpanic(plainError(\"send on closed channel\"))\n+}\n+\n+func (c *hchan) sortkey() uintptr {\n+\t// TODO(khr): if we have a moving garbage collector, we'll need to\n+\t// change this function.\n+\treturn uintptr(unsafe.Pointer(c))\n+}\n+\n+// A runtimeSelect is a single case passed to rselect.\n+// This must match ../reflect/value.go:/runtimeSelect\n+type runtimeSelect struct {\n+\tdir selectDir\n+\ttyp unsafe.Pointer // channel type (not used here)\n+\tch  *hchan         // channel\n+\tval unsafe.Pointer // ptr to data (SendDir) or ptr to receive buffer (RecvDir)\n+}\n+\n+// These values must match ../reflect/value.go:/SelectDir.\n+type selectDir int\n+\n+const (\n+\t_             selectDir = iota\n+\tselectSend              // case Chan <- Send\n+\tselectRecv              // case <-Chan:\n+\tselectDefault           // default\n+)\n+\n+//go:linkname reflect_rselect reflect.rselect\n+func reflect_rselect(cases []runtimeSelect) (chosen int, recvOK bool) {\n+\t// flagNoScan is safe here, because all objects are also referenced from cases.\n+\tsize := selectsize(uintptr(len(cases)))\n+\tsel := (*hselect)(mallocgc(size, nil, true))\n+\tnewselect(sel, int64(size), int32(len(cases)))\n+\tr := new(bool)\n+\tfor i := range cases {\n+\t\trc := &cases[i]\n+\t\tswitch rc.dir {\n+\t\tcase selectDefault:\n+\t\t\tselectdefaultImpl(sel, uintptr(i), 0)\n+\t\tcase selectSend:\n+\t\t\tif rc.ch == nil {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tselectsendImpl(sel, rc.ch, uintptr(i), rc.val, 0)\n+\t\tcase selectRecv:\n+\t\t\tif rc.ch == nil {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tselectrecvImpl(sel, rc.ch, uintptr(i), rc.val, r, 0)\n+\t\t}\n+\t}\n+\n+\tpc, _ := selectgoImpl(sel)\n+\tchosen = int(pc)\n+\trecvOK = *r\n+\treturn\n+}\n+\n+func (q *waitq) dequeueSudoG(sgp *sudog) {\n+\tx := sgp.prev\n+\ty := sgp.next\n+\tif x != nil {\n+\t\tif y != nil {\n+\t\t\t// middle of queue\n+\t\t\tx.next = y\n+\t\t\ty.prev = x\n+\t\t\tsgp.next = nil\n+\t\t\tsgp.prev = nil\n+\t\t\treturn\n+\t\t}\n+\t\t// end of queue\n+\t\tx.next = nil\n+\t\tq.last = x\n+\t\tsgp.prev = nil\n+\t\treturn\n+\t}\n+\tif y != nil {\n+\t\t// start of queue\n+\t\ty.prev = nil\n+\t\tq.first = y\n+\t\tsgp.next = nil\n+\t\treturn\n+\t}\n+\n+\t// x==y==nil. Either sgp is the only element in the queue,\n+\t// or it has already been removed. Use q.first to disambiguate.\n+\tif q.first == sgp {\n+\t\tq.first = nil\n+\t\tq.last = nil\n+\t}\n+}"}, {"sha": "c687cbf6220417fa7ce934ec2ea1d42549620d2f", "filename": "libgo/go/runtime/stubs.go", "status": "modified", "additions": 64, "deletions": 0, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fruntime%2Fstubs.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fruntime%2Fstubs.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstubs.go?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -384,3 +384,67 @@ func errno() int\n func entersyscall(int32)\n func entersyscallblock(int32)\n func exitsyscall(int32)\n+func gopark(func(*g, unsafe.Pointer) bool, unsafe.Pointer, string, byte, int)\n+func goparkunlock(*mutex, string, byte, int)\n+func goready(*g, int)\n+\n+// Temporary for gccgo until we port mprof.go.\n+var blockprofilerate uint64\n+\n+func blockevent(cycles int64, skip int) {}\n+\n+// Temporary hack for gccgo until we port proc.go.\n+//go:nosplit\n+func acquireSudog() *sudog {\n+\tmp := acquirem()\n+\tpp := mp.p.ptr()\n+\tif len(pp.sudogcache) == 0 {\n+\t\tpp.sudogcache = append(pp.sudogcache, new(sudog))\n+\t}\n+\tn := len(pp.sudogcache)\n+\ts := pp.sudogcache[n-1]\n+\tpp.sudogcache[n-1] = nil\n+\tpp.sudogcache = pp.sudogcache[:n-1]\n+\tif s.elem != nil {\n+\t\tthrow(\"acquireSudog: found s.elem != nil in cache\")\n+\t}\n+\treleasem(mp)\n+\treturn s\n+}\n+\n+// Temporary hack for gccgo until we port proc.go.\n+//go:nosplit\n+func releaseSudog(s *sudog) {\n+\tif s.elem != nil {\n+\t\tthrow(\"runtime: sudog with non-nil elem\")\n+\t}\n+\tif s.selectdone != nil {\n+\t\tthrow(\"runtime: sudog with non-nil selectdone\")\n+\t}\n+\tif s.next != nil {\n+\t\tthrow(\"runtime: sudog with non-nil next\")\n+\t}\n+\tif s.prev != nil {\n+\t\tthrow(\"runtime: sudog with non-nil prev\")\n+\t}\n+\tif s.waitlink != nil {\n+\t\tthrow(\"runtime: sudog with non-nil waitlink\")\n+\t}\n+\tif s.c != nil {\n+\t\tthrow(\"runtime: sudog with non-nil c\")\n+\t}\n+\tgp := getg()\n+\tif gp.param != nil {\n+\t\tthrow(\"runtime: releaseSudog with non-nil gp.param\")\n+\t}\n+\tmp := acquirem() // avoid rescheduling to another P\n+\tpp := mp.p.ptr()\n+\tpp.sudogcache = append(pp.sudogcache, s)\n+\treleasem(mp)\n+}\n+\n+// Temporary hack for gccgo until we port the garbage collector.\n+func typeBitsBulkBarrier(typ *_type, p, size uintptr) {}\n+\n+// Temporary for gccgo until we port print.go.\n+type hex uint64"}, {"sha": "35126f19a294afbca0430cf06774e63481cd8bd0", "filename": "libgo/go/runtime/trace.go", "status": "added", "additions": 1008, "deletions": 0, "changes": 1008, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fruntime%2Ftrace.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fgo%2Fruntime%2Ftrace.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ftrace.go?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -0,0 +1,1008 @@\n+// Copyright 2014 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Go execution tracer.\n+// The tracer captures a wide range of execution events like goroutine\n+// creation/blocking/unblocking, syscall enter/exit/block, GC-related events,\n+// changes of heap size, processor start/stop, etc and writes them to a buffer\n+// in a compact form. A precise nanosecond-precision timestamp and a stack\n+// trace is captured for most events.\n+// See https://golang.org/s/go15trace for more info.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+// Event types in the trace, args are given in square brackets.\n+const (\n+\ttraceEvNone           = 0  // unused\n+\ttraceEvBatch          = 1  // start of per-P batch of events [pid, timestamp]\n+\ttraceEvFrequency      = 2  // contains tracer timer frequency [frequency (ticks per second)]\n+\ttraceEvStack          = 3  // stack [stack id, number of PCs, array of {PC, func string ID, file string ID, line}]\n+\ttraceEvGomaxprocs     = 4  // current value of GOMAXPROCS [timestamp, GOMAXPROCS, stack id]\n+\ttraceEvProcStart      = 5  // start of P [timestamp, thread id]\n+\ttraceEvProcStop       = 6  // stop of P [timestamp]\n+\ttraceEvGCStart        = 7  // GC start [timestamp, seq, stack id]\n+\ttraceEvGCDone         = 8  // GC done [timestamp]\n+\ttraceEvGCScanStart    = 9  // GC scan start [timestamp]\n+\ttraceEvGCScanDone     = 10 // GC scan done [timestamp]\n+\ttraceEvGCSweepStart   = 11 // GC sweep start [timestamp, stack id]\n+\ttraceEvGCSweepDone    = 12 // GC sweep done [timestamp]\n+\ttraceEvGoCreate       = 13 // goroutine creation [timestamp, new goroutine id, new stack id, stack id]\n+\ttraceEvGoStart        = 14 // goroutine starts running [timestamp, goroutine id, seq]\n+\ttraceEvGoEnd          = 15 // goroutine ends [timestamp]\n+\ttraceEvGoStop         = 16 // goroutine stops (like in select{}) [timestamp, stack]\n+\ttraceEvGoSched        = 17 // goroutine calls Gosched [timestamp, stack]\n+\ttraceEvGoPreempt      = 18 // goroutine is preempted [timestamp, stack]\n+\ttraceEvGoSleep        = 19 // goroutine calls Sleep [timestamp, stack]\n+\ttraceEvGoBlock        = 20 // goroutine blocks [timestamp, stack]\n+\ttraceEvGoUnblock      = 21 // goroutine is unblocked [timestamp, goroutine id, seq, stack]\n+\ttraceEvGoBlockSend    = 22 // goroutine blocks on chan send [timestamp, stack]\n+\ttraceEvGoBlockRecv    = 23 // goroutine blocks on chan recv [timestamp, stack]\n+\ttraceEvGoBlockSelect  = 24 // goroutine blocks on select [timestamp, stack]\n+\ttraceEvGoBlockSync    = 25 // goroutine blocks on Mutex/RWMutex [timestamp, stack]\n+\ttraceEvGoBlockCond    = 26 // goroutine blocks on Cond [timestamp, stack]\n+\ttraceEvGoBlockNet     = 27 // goroutine blocks on network [timestamp, stack]\n+\ttraceEvGoSysCall      = 28 // syscall enter [timestamp, stack]\n+\ttraceEvGoSysExit      = 29 // syscall exit [timestamp, goroutine id, seq, real timestamp]\n+\ttraceEvGoSysBlock     = 30 // syscall blocks [timestamp]\n+\ttraceEvGoWaiting      = 31 // denotes that goroutine is blocked when tracing starts [timestamp, goroutine id]\n+\ttraceEvGoInSyscall    = 32 // denotes that goroutine is in syscall when tracing starts [timestamp, goroutine id]\n+\ttraceEvHeapAlloc      = 33 // memstats.heap_live change [timestamp, heap_alloc]\n+\ttraceEvNextGC         = 34 // memstats.next_gc change [timestamp, next_gc]\n+\ttraceEvTimerGoroutine = 35 // denotes timer goroutine [timer goroutine id]\n+\ttraceEvFutileWakeup   = 36 // denotes that the previous wakeup of this goroutine was futile [timestamp]\n+\ttraceEvString         = 37 // string dictionary entry [ID, length, string]\n+\ttraceEvGoStartLocal   = 38 // goroutine starts running on the same P as the last event [timestamp, goroutine id]\n+\ttraceEvGoUnblockLocal = 39 // goroutine is unblocked on the same P as the last event [timestamp, goroutine id, stack]\n+\ttraceEvGoSysExitLocal = 40 // syscall exit on the same P as the last event [timestamp, goroutine id, real timestamp]\n+\ttraceEvCount          = 41\n+)\n+\n+const (\n+\t// Timestamps in trace are cputicks/traceTickDiv.\n+\t// This makes absolute values of timestamp diffs smaller,\n+\t// and so they are encoded in less number of bytes.\n+\t// 64 on x86 is somewhat arbitrary (one tick is ~20ns on a 3GHz machine).\n+\t// The suggested increment frequency for PowerPC's time base register is\n+\t// 512 MHz according to Power ISA v2.07 section 6.2, so we use 16 on ppc64\n+\t// and ppc64le.\n+\t// Tracing won't work reliably for architectures where cputicks is emulated\n+\t// by nanotime, so the value doesn't matter for those architectures.\n+\ttraceTickDiv = 16 + 48*(sys.Goarch386|sys.GoarchAmd64|sys.GoarchAmd64p32)\n+\t// Maximum number of PCs in a single stack trace.\n+\t// Since events contain only stack id rather than whole stack trace,\n+\t// we can allow quite large values here.\n+\ttraceStackSize = 128\n+\t// Identifier of a fake P that is used when we trace without a real P.\n+\ttraceGlobProc = -1\n+\t// Maximum number of bytes to encode uint64 in base-128.\n+\ttraceBytesPerNumber = 10\n+\t// Shift of the number of arguments in the first event byte.\n+\ttraceArgCountShift = 6\n+\t// Flag passed to traceGoPark to denote that the previous wakeup of this\n+\t// goroutine was futile. For example, a goroutine was unblocked on a mutex,\n+\t// but another goroutine got ahead and acquired the mutex before the first\n+\t// goroutine is scheduled, so the first goroutine has to block again.\n+\t// Such wakeups happen on buffered channels and sync.Mutex,\n+\t// but are generally not interesting for end user.\n+\ttraceFutileWakeup byte = 128\n+)\n+\n+// trace is global tracing context.\n+var trace struct {\n+\tlock          mutex       // protects the following members\n+\tlockOwner     *g          // to avoid deadlocks during recursive lock locks\n+\tenabled       bool        // when set runtime traces events\n+\tshutdown      bool        // set when we are waiting for trace reader to finish after setting enabled to false\n+\theaderWritten bool        // whether ReadTrace has emitted trace header\n+\tfooterWritten bool        // whether ReadTrace has emitted trace footer\n+\tshutdownSema  uint32      // used to wait for ReadTrace completion\n+\tseqStart      uint64      // sequence number when tracing was started\n+\tticksStart    int64       // cputicks when tracing was started\n+\tticksEnd      int64       // cputicks when tracing was stopped\n+\ttimeStart     int64       // nanotime when tracing was started\n+\ttimeEnd       int64       // nanotime when tracing was stopped\n+\tseqGC         uint64      // GC start/done sequencer\n+\treading       traceBufPtr // buffer currently handed off to user\n+\tempty         traceBufPtr // stack of empty buffers\n+\tfullHead      traceBufPtr // queue of full buffers\n+\tfullTail      traceBufPtr\n+\treader        *g              // goroutine that called ReadTrace, or nil\n+\tstackTab      traceStackTable // maps stack traces to unique ids\n+\n+\t// Dictionary for traceEvString.\n+\t// Currently this is used only for func/file:line info after tracing session,\n+\t// so we assume single-threaded access.\n+\tstrings   map[string]uint64\n+\tstringSeq uint64\n+\n+\tbufLock mutex       // protects buf\n+\tbuf     traceBufPtr // global trace buffer, used when running without a p\n+}\n+\n+// traceBufHeader is per-P tracing buffer.\n+type traceBufHeader struct {\n+\tlink      traceBufPtr             // in trace.empty/full\n+\tlastTicks uint64                  // when we wrote the last event\n+\tpos       int                     // next write offset in arr\n+\tstk       [traceStackSize]uintptr // scratch buffer for traceback\n+}\n+\n+// traceBuf is per-P tracing buffer.\n+type traceBuf struct {\n+\ttraceBufHeader\n+\tarr [64<<10 - unsafe.Sizeof(traceBufHeader{})]byte // underlying buffer for traceBufHeader.buf\n+}\n+\n+// traceBufPtr is a *traceBuf that is not traced by the garbage\n+// collector and doesn't have write barriers. traceBufs are not\n+// allocated from the GC'd heap, so this is safe, and are often\n+// manipulated in contexts where write barriers are not allowed, so\n+// this is necessary.\n+type traceBufPtr uintptr\n+\n+func (tp traceBufPtr) ptr() *traceBuf   { return (*traceBuf)(unsafe.Pointer(tp)) }\n+func (tp *traceBufPtr) set(b *traceBuf) { *tp = traceBufPtr(unsafe.Pointer(b)) }\n+func traceBufPtrOf(b *traceBuf) traceBufPtr {\n+\treturn traceBufPtr(unsafe.Pointer(b))\n+}\n+\n+/*\n+Commented out for gccgo for now.\n+\n+// StartTrace enables tracing for the current process.\n+// While tracing, the data will be buffered and available via ReadTrace.\n+// StartTrace returns an error if tracing is already enabled.\n+// Most clients should use the runtime/trace package or the testing package's\n+// -test.trace flag instead of calling StartTrace directly.\n+func StartTrace() error {\n+\t// Stop the world, so that we can take a consistent snapshot\n+\t// of all goroutines at the beginning of the trace.\n+\tstopTheWorld(\"start tracing\")\n+\n+\t// We are in stop-the-world, but syscalls can finish and write to trace concurrently.\n+\t// Exitsyscall could check trace.enabled long before and then suddenly wake up\n+\t// and decide to write to trace at a random point in time.\n+\t// However, such syscall will use the global trace.buf buffer, because we've\n+\t// acquired all p's by doing stop-the-world. So this protects us from such races.\n+\tlock(&trace.bufLock)\n+\n+\tif trace.enabled || trace.shutdown {\n+\t\tunlock(&trace.bufLock)\n+\t\tstartTheWorld()\n+\t\treturn errorString(\"tracing is already enabled\")\n+\t}\n+\n+\t// Can't set trace.enabled yet. While the world is stopped, exitsyscall could\n+\t// already emit a delayed event (see exitTicks in exitsyscall) if we set trace.enabled here.\n+\t// That would lead to an inconsistent trace:\n+\t// - either GoSysExit appears before EvGoInSyscall,\n+\t// - or GoSysExit appears for a goroutine for which we don't emit EvGoInSyscall below.\n+\t// To instruct traceEvent that it must not ignore events below, we set startingtrace.\n+\t// trace.enabled is set afterwards once we have emitted all preliminary events.\n+\t_g_ := getg()\n+\t_g_.m.startingtrace = true\n+\tfor _, gp := range allgs {\n+\t\tstatus := readgstatus(gp)\n+\t\tif status != _Gdead {\n+\t\t\ttraceGoCreate(gp, gp.startpc) // also resets gp.traceseq/tracelastp\n+\t\t}\n+\t\tif status == _Gwaiting {\n+\t\t\t// traceEvGoWaiting is implied to have seq=1.\n+\t\t\tgp.traceseq++\n+\t\t\ttraceEvent(traceEvGoWaiting, -1, uint64(gp.goid))\n+\t\t}\n+\t\tif status == _Gsyscall {\n+\t\t\tgp.traceseq++\n+\t\t\ttraceEvent(traceEvGoInSyscall, -1, uint64(gp.goid))\n+\t\t} else {\n+\t\t\tgp.sysblocktraced = false\n+\t\t}\n+\t}\n+\ttraceProcStart()\n+\ttraceGoStart()\n+\t// Note: ticksStart needs to be set after we emit traceEvGoInSyscall events.\n+\t// If we do it the other way around, it is possible that exitsyscall will\n+\t// query sysexitticks after ticksStart but before traceEvGoInSyscall timestamp.\n+\t// It will lead to a false conclusion that cputicks is broken.\n+\ttrace.ticksStart = cputicks()\n+\ttrace.timeStart = nanotime()\n+\ttrace.headerWritten = false\n+\ttrace.footerWritten = false\n+\ttrace.strings = make(map[string]uint64)\n+\ttrace.stringSeq = 0\n+\ttrace.seqGC = 0\n+\t_g_.m.startingtrace = false\n+\ttrace.enabled = true\n+\n+\tunlock(&trace.bufLock)\n+\n+\tstartTheWorld()\n+\treturn nil\n+}\n+\n+// StopTrace stops tracing, if it was previously enabled.\n+// StopTrace only returns after all the reads for the trace have completed.\n+func StopTrace() {\n+\t// Stop the world so that we can collect the trace buffers from all p's below,\n+\t// and also to avoid races with traceEvent.\n+\tstopTheWorld(\"stop tracing\")\n+\n+\t// See the comment in StartTrace.\n+\tlock(&trace.bufLock)\n+\n+\tif !trace.enabled {\n+\t\tunlock(&trace.bufLock)\n+\t\tstartTheWorld()\n+\t\treturn\n+\t}\n+\n+\ttraceGoSched()\n+\n+\tfor _, p := range &allp {\n+\t\tif p == nil {\n+\t\t\tbreak\n+\t\t}\n+\t\tbuf := p.tracebuf\n+\t\tif buf != 0 {\n+\t\t\ttraceFullQueue(buf)\n+\t\t\tp.tracebuf = 0\n+\t\t}\n+\t}\n+\tif trace.buf != 0 && trace.buf.ptr().pos != 0 {\n+\t\tbuf := trace.buf\n+\t\ttrace.buf = 0\n+\t\ttraceFullQueue(buf)\n+\t}\n+\n+\tfor {\n+\t\ttrace.ticksEnd = cputicks()\n+\t\ttrace.timeEnd = nanotime()\n+\t\t// Windows time can tick only every 15ms, wait for at least one tick.\n+\t\tif trace.timeEnd != trace.timeStart {\n+\t\t\tbreak\n+\t\t}\n+\t\tosyield()\n+\t}\n+\n+\ttrace.enabled = false\n+\ttrace.shutdown = true\n+\tunlock(&trace.bufLock)\n+\n+\tstartTheWorld()\n+\n+\t// The world is started but we've set trace.shutdown, so new tracing can't start.\n+\t// Wait for the trace reader to flush pending buffers and stop.\n+\tsemacquire(&trace.shutdownSema, false)\n+\tif raceenabled {\n+\t\traceacquire(unsafe.Pointer(&trace.shutdownSema))\n+\t}\n+\n+\t// The lock protects us from races with StartTrace/StopTrace because they do stop-the-world.\n+\tlock(&trace.lock)\n+\tfor _, p := range &allp {\n+\t\tif p == nil {\n+\t\t\tbreak\n+\t\t}\n+\t\tif p.tracebuf != 0 {\n+\t\t\tthrow(\"trace: non-empty trace buffer in proc\")\n+\t\t}\n+\t}\n+\tif trace.buf != 0 {\n+\t\tthrow(\"trace: non-empty global trace buffer\")\n+\t}\n+\tif trace.fullHead != 0 || trace.fullTail != 0 {\n+\t\tthrow(\"trace: non-empty full trace buffer\")\n+\t}\n+\tif trace.reading != 0 || trace.reader != nil {\n+\t\tthrow(\"trace: reading after shutdown\")\n+\t}\n+\tfor trace.empty != 0 {\n+\t\tbuf := trace.empty\n+\t\ttrace.empty = buf.ptr().link\n+\t\tsysFree(unsafe.Pointer(buf), unsafe.Sizeof(*buf.ptr()), &memstats.other_sys)\n+\t}\n+\ttrace.strings = nil\n+\ttrace.shutdown = false\n+\tunlock(&trace.lock)\n+}\n+\n+// ReadTrace returns the next chunk of binary tracing data, blocking until data\n+// is available. If tracing is turned off and all the data accumulated while it\n+// was on has been returned, ReadTrace returns nil. The caller must copy the\n+// returned data before calling ReadTrace again.\n+// ReadTrace must be called from one goroutine at a time.\n+func ReadTrace() []byte {\n+\t// This function may need to lock trace.lock recursively\n+\t// (goparkunlock -> traceGoPark -> traceEvent -> traceFlush).\n+\t// To allow this we use trace.lockOwner.\n+\t// Also this function must not allocate while holding trace.lock:\n+\t// allocation can call heap allocate, which will try to emit a trace\n+\t// event while holding heap lock.\n+\tlock(&trace.lock)\n+\ttrace.lockOwner = getg()\n+\n+\tif trace.reader != nil {\n+\t\t// More than one goroutine reads trace. This is bad.\n+\t\t// But we rather do not crash the program because of tracing,\n+\t\t// because tracing can be enabled at runtime on prod servers.\n+\t\ttrace.lockOwner = nil\n+\t\tunlock(&trace.lock)\n+\t\tprintln(\"runtime: ReadTrace called from multiple goroutines simultaneously\")\n+\t\treturn nil\n+\t}\n+\t// Recycle the old buffer.\n+\tif buf := trace.reading; buf != 0 {\n+\t\tbuf.ptr().link = trace.empty\n+\t\ttrace.empty = buf\n+\t\ttrace.reading = 0\n+\t}\n+\t// Write trace header.\n+\tif !trace.headerWritten {\n+\t\ttrace.headerWritten = true\n+\t\ttrace.lockOwner = nil\n+\t\tunlock(&trace.lock)\n+\t\treturn []byte(\"go 1.7 trace\\x00\\x00\\x00\\x00\")\n+\t}\n+\t// Wait for new data.\n+\tif trace.fullHead == 0 && !trace.shutdown {\n+\t\ttrace.reader = getg()\n+\t\tgoparkunlock(&trace.lock, \"trace reader (blocked)\", traceEvGoBlock, 2)\n+\t\tlock(&trace.lock)\n+\t}\n+\t// Write a buffer.\n+\tif trace.fullHead != 0 {\n+\t\tbuf := traceFullDequeue()\n+\t\ttrace.reading = buf\n+\t\ttrace.lockOwner = nil\n+\t\tunlock(&trace.lock)\n+\t\treturn buf.ptr().arr[:buf.ptr().pos]\n+\t}\n+\t// Write footer with timer frequency.\n+\tif !trace.footerWritten {\n+\t\ttrace.footerWritten = true\n+\t\t// Use float64 because (trace.ticksEnd - trace.ticksStart) * 1e9 can overflow int64.\n+\t\tfreq := float64(trace.ticksEnd-trace.ticksStart) * 1e9 / float64(trace.timeEnd-trace.timeStart) / traceTickDiv\n+\t\ttrace.lockOwner = nil\n+\t\tunlock(&trace.lock)\n+\t\tvar data []byte\n+\t\tdata = append(data, traceEvFrequency|0<<traceArgCountShift)\n+\t\tdata = traceAppend(data, uint64(freq))\n+\t\tif timers.gp != nil {\n+\t\t\tdata = append(data, traceEvTimerGoroutine|0<<traceArgCountShift)\n+\t\t\tdata = traceAppend(data, uint64(timers.gp.goid))\n+\t\t}\n+\t\t// This will emit a bunch of full buffers, we will pick them up\n+\t\t// on the next iteration.\n+\t\ttrace.stackTab.dump()\n+\t\treturn data\n+\t}\n+\t// Done.\n+\tif trace.shutdown {\n+\t\ttrace.lockOwner = nil\n+\t\tunlock(&trace.lock)\n+\t\tif raceenabled {\n+\t\t\t// Model synchronization on trace.shutdownSema, which race\n+\t\t\t// detector does not see. This is required to avoid false\n+\t\t\t// race reports on writer passed to trace.Start.\n+\t\t\tracerelease(unsafe.Pointer(&trace.shutdownSema))\n+\t\t}\n+\t\t// trace.enabled is already reset, so can call traceable functions.\n+\t\tsemrelease(&trace.shutdownSema)\n+\t\treturn nil\n+\t}\n+\t// Also bad, but see the comment above.\n+\ttrace.lockOwner = nil\n+\tunlock(&trace.lock)\n+\tprintln(\"runtime: spurious wakeup of trace reader\")\n+\treturn nil\n+}\n+\n+// traceReader returns the trace reader that should be woken up, if any.\n+func traceReader() *g {\n+\tif trace.reader == nil || (trace.fullHead == 0 && !trace.shutdown) {\n+\t\treturn nil\n+\t}\n+\tlock(&trace.lock)\n+\tif trace.reader == nil || (trace.fullHead == 0 && !trace.shutdown) {\n+\t\tunlock(&trace.lock)\n+\t\treturn nil\n+\t}\n+\tgp := trace.reader\n+\ttrace.reader = nil\n+\tunlock(&trace.lock)\n+\treturn gp\n+}\n+\n+// traceProcFree frees trace buffer associated with pp.\n+func traceProcFree(pp *p) {\n+\tbuf := pp.tracebuf\n+\tpp.tracebuf = 0\n+\tif buf == 0 {\n+\t\treturn\n+\t}\n+\tlock(&trace.lock)\n+\ttraceFullQueue(buf)\n+\tunlock(&trace.lock)\n+}\n+\n+// traceFullQueue queues buf into queue of full buffers.\n+func traceFullQueue(buf traceBufPtr) {\n+\tbuf.ptr().link = 0\n+\tif trace.fullHead == 0 {\n+\t\ttrace.fullHead = buf\n+\t} else {\n+\t\ttrace.fullTail.ptr().link = buf\n+\t}\n+\ttrace.fullTail = buf\n+}\n+\n+// traceFullDequeue dequeues from queue of full buffers.\n+func traceFullDequeue() traceBufPtr {\n+\tbuf := trace.fullHead\n+\tif buf == 0 {\n+\t\treturn 0\n+\t}\n+\ttrace.fullHead = buf.ptr().link\n+\tif trace.fullHead == 0 {\n+\t\ttrace.fullTail = 0\n+\t}\n+\tbuf.ptr().link = 0\n+\treturn buf\n+}\n+\n+// traceEvent writes a single event to trace buffer, flushing the buffer if necessary.\n+// ev is event type.\n+// If skip > 0, write current stack id as the last argument (skipping skip top frames).\n+// If skip = 0, this event type should contain a stack, but we don't want\n+// to collect and remember it for this particular call.\n+func traceEvent(ev byte, skip int, args ...uint64) {\n+\tmp, pid, bufp := traceAcquireBuffer()\n+\t// Double-check trace.enabled now that we've done m.locks++ and acquired bufLock.\n+\t// This protects from races between traceEvent and StartTrace/StopTrace.\n+\n+\t// The caller checked that trace.enabled == true, but trace.enabled might have been\n+\t// turned off between the check and now. Check again. traceLockBuffer did mp.locks++,\n+\t// StopTrace does stopTheWorld, and stopTheWorld waits for mp.locks to go back to zero,\n+\t// so if we see trace.enabled == true now, we know it's true for the rest of the function.\n+\t// Exitsyscall can run even during stopTheWorld. The race with StartTrace/StopTrace\n+\t// during tracing in exitsyscall is resolved by locking trace.bufLock in traceLockBuffer.\n+\tif !trace.enabled && !mp.startingtrace {\n+\t\ttraceReleaseBuffer(pid)\n+\t\treturn\n+\t}\n+\tbuf := (*bufp).ptr()\n+\tconst maxSize = 2 + 5*traceBytesPerNumber // event type, length, sequence, timestamp, stack id and two add params\n+\tif buf == nil || len(buf.arr)-buf.pos < maxSize {\n+\t\tbuf = traceFlush(traceBufPtrOf(buf)).ptr()\n+\t\t(*bufp).set(buf)\n+\t}\n+\n+\tticks := uint64(cputicks()) / traceTickDiv\n+\ttickDiff := ticks - buf.lastTicks\n+\tif buf.pos == 0 {\n+\t\tbuf.byte(traceEvBatch | 1<<traceArgCountShift)\n+\t\tbuf.varint(uint64(pid))\n+\t\tbuf.varint(ticks)\n+\t\ttickDiff = 0\n+\t}\n+\tbuf.lastTicks = ticks\n+\tnarg := byte(len(args))\n+\tif skip >= 0 {\n+\t\tnarg++\n+\t}\n+\t// We have only 2 bits for number of arguments.\n+\t// If number is >= 3, then the event type is followed by event length in bytes.\n+\tif narg > 3 {\n+\t\tnarg = 3\n+\t}\n+\tstartPos := buf.pos\n+\tbuf.byte(ev | narg<<traceArgCountShift)\n+\tvar lenp *byte\n+\tif narg == 3 {\n+\t\t// Reserve the byte for length assuming that length < 128.\n+\t\tbuf.varint(0)\n+\t\tlenp = &buf.arr[buf.pos-1]\n+\t}\n+\tbuf.varint(tickDiff)\n+\tfor _, a := range args {\n+\t\tbuf.varint(a)\n+\t}\n+\tif skip == 0 {\n+\t\tbuf.varint(0)\n+\t} else if skip > 0 {\n+\t\t_g_ := getg()\n+\t\tgp := mp.curg\n+\t\tvar nstk int\n+\t\tif gp == _g_ {\n+\t\t\tnstk = callers(skip, buf.stk[:])\n+\t\t} else if gp != nil {\n+\t\t\tgp = mp.curg\n+\t\t\t// This may happen when tracing a system call,\n+\t\t\t// so we must lock the stack.\n+\t\t\tif gcTryLockStackBarriers(gp) {\n+\t\t\t\tnstk = gcallers(gp, skip, buf.stk[:])\n+\t\t\t\tgcUnlockStackBarriers(gp)\n+\t\t\t}\n+\t\t}\n+\t\tif nstk > 0 {\n+\t\t\tnstk-- // skip runtime.goexit\n+\t\t}\n+\t\tif nstk > 0 && gp.goid == 1 {\n+\t\t\tnstk-- // skip runtime.main\n+\t\t}\n+\t\tid := trace.stackTab.put(buf.stk[:nstk])\n+\t\tbuf.varint(uint64(id))\n+\t}\n+\tevSize := buf.pos - startPos\n+\tif evSize > maxSize {\n+\t\tthrow(\"invalid length of trace event\")\n+\t}\n+\tif lenp != nil {\n+\t\t// Fill in actual length.\n+\t\t*lenp = byte(evSize - 2)\n+\t}\n+\ttraceReleaseBuffer(pid)\n+}\n+\n+// traceAcquireBuffer returns trace buffer to use and, if necessary, locks it.\n+func traceAcquireBuffer() (mp *m, pid int32, bufp *traceBufPtr) {\n+\tmp = acquirem()\n+\tif p := mp.p.ptr(); p != nil {\n+\t\treturn mp, p.id, &p.tracebuf\n+\t}\n+\tlock(&trace.bufLock)\n+\treturn mp, traceGlobProc, &trace.buf\n+}\n+\n+// traceReleaseBuffer releases a buffer previously acquired with traceAcquireBuffer.\n+func traceReleaseBuffer(pid int32) {\n+\tif pid == traceGlobProc {\n+\t\tunlock(&trace.bufLock)\n+\t}\n+\treleasem(getg().m)\n+}\n+\n+// traceFlush puts buf onto stack of full buffers and returns an empty buffer.\n+func traceFlush(buf traceBufPtr) traceBufPtr {\n+\towner := trace.lockOwner\n+\tdolock := owner == nil || owner != getg().m.curg\n+\tif dolock {\n+\t\tlock(&trace.lock)\n+\t}\n+\tif buf != 0 {\n+\t\ttraceFullQueue(buf)\n+\t}\n+\tif trace.empty != 0 {\n+\t\tbuf = trace.empty\n+\t\ttrace.empty = buf.ptr().link\n+\t} else {\n+\t\tbuf = traceBufPtr(sysAlloc(unsafe.Sizeof(traceBuf{}), &memstats.other_sys))\n+\t\tif buf == 0 {\n+\t\t\tthrow(\"trace: out of memory\")\n+\t\t}\n+\t}\n+\tbufp := buf.ptr()\n+\tbufp.link.set(nil)\n+\tbufp.pos = 0\n+\tbufp.lastTicks = 0\n+\tif dolock {\n+\t\tunlock(&trace.lock)\n+\t}\n+\treturn buf\n+}\n+\n+func traceString(buf *traceBuf, s string) (uint64, *traceBuf) {\n+\tif s == \"\" {\n+\t\treturn 0, buf\n+\t}\n+\tif id, ok := trace.strings[s]; ok {\n+\t\treturn id, buf\n+\t}\n+\n+\ttrace.stringSeq++\n+\tid := trace.stringSeq\n+\ttrace.strings[s] = id\n+\n+\tsize := 1 + 2*traceBytesPerNumber + len(s)\n+\tif len(buf.arr)-buf.pos < size {\n+\t\tbuf = traceFlush(traceBufPtrOf(buf)).ptr()\n+\t}\n+\tbuf.byte(traceEvString)\n+\tbuf.varint(id)\n+\tbuf.varint(uint64(len(s)))\n+\tbuf.pos += copy(buf.arr[buf.pos:], s)\n+\treturn id, buf\n+}\n+\n+// traceAppend appends v to buf in little-endian-base-128 encoding.\n+func traceAppend(buf []byte, v uint64) []byte {\n+\tfor ; v >= 0x80; v >>= 7 {\n+\t\tbuf = append(buf, 0x80|byte(v))\n+\t}\n+\tbuf = append(buf, byte(v))\n+\treturn buf\n+}\n+\n+// varint appends v to buf in little-endian-base-128 encoding.\n+func (buf *traceBuf) varint(v uint64) {\n+\tpos := buf.pos\n+\tfor ; v >= 0x80; v >>= 7 {\n+\t\tbuf.arr[pos] = 0x80 | byte(v)\n+\t\tpos++\n+\t}\n+\tbuf.arr[pos] = byte(v)\n+\tpos++\n+\tbuf.pos = pos\n+}\n+\n+// byte appends v to buf.\n+func (buf *traceBuf) byte(v byte) {\n+\tbuf.arr[buf.pos] = v\n+\tbuf.pos++\n+}\n+\n+*/\n+\n+// traceStackTable maps stack traces (arrays of PC's) to unique uint32 ids.\n+// It is lock-free for reading.\n+type traceStackTable struct {\n+\tlock mutex\n+\tseq  uint32\n+\tmem  traceAlloc\n+\ttab  [1 << 13]traceStackPtr\n+}\n+\n+// traceStack is a single stack in traceStackTable.\n+type traceStack struct {\n+\tlink traceStackPtr\n+\thash uintptr\n+\tid   uint32\n+\tn    int\n+\tstk  [0]uintptr // real type [n]uintptr\n+}\n+\n+type traceStackPtr uintptr\n+\n+/*\n+Commented out for gccgo for now.\n+\n+func (tp traceStackPtr) ptr() *traceStack { return (*traceStack)(unsafe.Pointer(tp)) }\n+\n+// stack returns slice of PCs.\n+func (ts *traceStack) stack() []uintptr {\n+\treturn (*[traceStackSize]uintptr)(unsafe.Pointer(&ts.stk))[:ts.n]\n+}\n+\n+// put returns a unique id for the stack trace pcs and caches it in the table,\n+// if it sees the trace for the first time.\n+func (tab *traceStackTable) put(pcs []uintptr) uint32 {\n+\tif len(pcs) == 0 {\n+\t\treturn 0\n+\t}\n+\thash := memhash(unsafe.Pointer(&pcs[0]), 0, uintptr(len(pcs))*unsafe.Sizeof(pcs[0]))\n+\t// First, search the hashtable w/o the mutex.\n+\tif id := tab.find(pcs, hash); id != 0 {\n+\t\treturn id\n+\t}\n+\t// Now, double check under the mutex.\n+\tlock(&tab.lock)\n+\tif id := tab.find(pcs, hash); id != 0 {\n+\t\tunlock(&tab.lock)\n+\t\treturn id\n+\t}\n+\t// Create new record.\n+\ttab.seq++\n+\tstk := tab.newStack(len(pcs))\n+\tstk.hash = hash\n+\tstk.id = tab.seq\n+\tstk.n = len(pcs)\n+\tstkpc := stk.stack()\n+\tfor i, pc := range pcs {\n+\t\tstkpc[i] = pc\n+\t}\n+\tpart := int(hash % uintptr(len(tab.tab)))\n+\tstk.link = tab.tab[part]\n+\tatomicstorep(unsafe.Pointer(&tab.tab[part]), unsafe.Pointer(stk))\n+\tunlock(&tab.lock)\n+\treturn stk.id\n+}\n+\n+// find checks if the stack trace pcs is already present in the table.\n+func (tab *traceStackTable) find(pcs []uintptr, hash uintptr) uint32 {\n+\tpart := int(hash % uintptr(len(tab.tab)))\n+Search:\n+\tfor stk := tab.tab[part].ptr(); stk != nil; stk = stk.link.ptr() {\n+\t\tif stk.hash == hash && stk.n == len(pcs) {\n+\t\t\tfor i, stkpc := range stk.stack() {\n+\t\t\t\tif stkpc != pcs[i] {\n+\t\t\t\t\tcontinue Search\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn stk.id\n+\t\t}\n+\t}\n+\treturn 0\n+}\n+\n+// newStack allocates a new stack of size n.\n+func (tab *traceStackTable) newStack(n int) *traceStack {\n+\treturn (*traceStack)(tab.mem.alloc(unsafe.Sizeof(traceStack{}) + uintptr(n)*sys.PtrSize))\n+}\n+\n+// dump writes all previously cached stacks to trace buffers,\n+// releases all memory and resets state.\n+func (tab *traceStackTable) dump() {\n+\tframes := make(map[uintptr]traceFrame)\n+\tvar tmp [(2 + 4*traceStackSize) * traceBytesPerNumber]byte\n+\tbuf := traceFlush(0).ptr()\n+\tfor _, stk := range tab.tab {\n+\t\tstk := stk.ptr()\n+\t\tfor ; stk != nil; stk = stk.link.ptr() {\n+\t\t\ttmpbuf := tmp[:0]\n+\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(stk.id))\n+\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(stk.n))\n+\t\t\tfor _, pc := range stk.stack() {\n+\t\t\t\tvar frame traceFrame\n+\t\t\t\tframe, buf = traceFrameForPC(buf, frames, pc)\n+\t\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(pc))\n+\t\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(frame.funcID))\n+\t\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(frame.fileID))\n+\t\t\t\ttmpbuf = traceAppend(tmpbuf, uint64(frame.line))\n+\t\t\t}\n+\t\t\t// Now copy to the buffer.\n+\t\t\tsize := 1 + traceBytesPerNumber + len(tmpbuf)\n+\t\t\tif len(buf.arr)-buf.pos < size {\n+\t\t\t\tbuf = traceFlush(traceBufPtrOf(buf)).ptr()\n+\t\t\t}\n+\t\t\tbuf.byte(traceEvStack | 3<<traceArgCountShift)\n+\t\t\tbuf.varint(uint64(len(tmpbuf)))\n+\t\t\tbuf.pos += copy(buf.arr[buf.pos:], tmpbuf)\n+\t\t}\n+\t}\n+\n+\tlock(&trace.lock)\n+\ttraceFullQueue(traceBufPtrOf(buf))\n+\tunlock(&trace.lock)\n+\n+\ttab.mem.drop()\n+\t*tab = traceStackTable{}\n+}\n+\n+type traceFrame struct {\n+\tfuncID uint64\n+\tfileID uint64\n+\tline   uint64\n+}\n+\n+func traceFrameForPC(buf *traceBuf, frames map[uintptr]traceFrame, pc uintptr) (traceFrame, *traceBuf) {\n+\tif frame, ok := frames[pc]; ok {\n+\t\treturn frame, buf\n+\t}\n+\n+\tvar frame traceFrame\n+\tf := findfunc(pc)\n+\tif f == nil {\n+\t\tframes[pc] = frame\n+\t\treturn frame, buf\n+\t}\n+\n+\tfn := funcname(f)\n+\tconst maxLen = 1 << 10\n+\tif len(fn) > maxLen {\n+\t\tfn = fn[len(fn)-maxLen:]\n+\t}\n+\tframe.funcID, buf = traceString(buf, fn)\n+\tfile, line := funcline(f, pc-sys.PCQuantum)\n+\tframe.line = uint64(line)\n+\tif len(file) > maxLen {\n+\t\tfile = file[len(file)-maxLen:]\n+\t}\n+\tframe.fileID, buf = traceString(buf, file)\n+\treturn frame, buf\n+}\n+\n+*/\n+\n+// traceAlloc is a non-thread-safe region allocator.\n+// It holds a linked list of traceAllocBlock.\n+type traceAlloc struct {\n+\thead traceAllocBlockPtr\n+\toff  uintptr\n+}\n+\n+// traceAllocBlock is a block in traceAlloc.\n+//\n+// traceAllocBlock is allocated from non-GC'd memory, so it must not\n+// contain heap pointers. Writes to pointers to traceAllocBlocks do\n+// not need write barriers.\n+type traceAllocBlock struct {\n+\tnext traceAllocBlockPtr\n+\tdata [64<<10 - sys.PtrSize]byte\n+}\n+\n+type traceAllocBlockPtr uintptr\n+\n+func (p traceAllocBlockPtr) ptr() *traceAllocBlock   { return (*traceAllocBlock)(unsafe.Pointer(p)) }\n+func (p *traceAllocBlockPtr) set(x *traceAllocBlock) { *p = traceAllocBlockPtr(unsafe.Pointer(x)) }\n+\n+/*\n+Commented out for gccgo for now.\n+\n+// alloc allocates n-byte block.\n+func (a *traceAlloc) alloc(n uintptr) unsafe.Pointer {\n+\tn = round(n, sys.PtrSize)\n+\tif a.head == 0 || a.off+n > uintptr(len(a.head.ptr().data)) {\n+\t\tif n > uintptr(len(a.head.ptr().data)) {\n+\t\t\tthrow(\"trace: alloc too large\")\n+\t\t}\n+\t\tblock := (*traceAllocBlock)(sysAlloc(unsafe.Sizeof(traceAllocBlock{}), &memstats.other_sys))\n+\t\tif block == nil {\n+\t\t\tthrow(\"trace: out of memory\")\n+\t\t}\n+\t\tblock.next.set(a.head.ptr())\n+\t\ta.head.set(block)\n+\t\ta.off = 0\n+\t}\n+\tp := &a.head.ptr().data[a.off]\n+\ta.off += n\n+\treturn unsafe.Pointer(p)\n+}\n+\n+// drop frees all previously allocated memory and resets the allocator.\n+func (a *traceAlloc) drop() {\n+\tfor a.head != 0 {\n+\t\tblock := a.head.ptr()\n+\t\ta.head.set(block.next.ptr())\n+\t\tsysFree(unsafe.Pointer(block), unsafe.Sizeof(traceAllocBlock{}), &memstats.other_sys)\n+\t}\n+}\n+\n+// The following functions write specific events to trace.\n+\n+func traceGomaxprocs(procs int32) {\n+\ttraceEvent(traceEvGomaxprocs, 1, uint64(procs))\n+}\n+\n+func traceProcStart() {\n+\ttraceEvent(traceEvProcStart, -1, uint64(getg().m.id))\n+}\n+\n+func traceProcStop(pp *p) {\n+\t// Sysmon and stopTheWorld can stop Ps blocked in syscalls,\n+\t// to handle this we temporary employ the P.\n+\tmp := acquirem()\n+\toldp := mp.p\n+\tmp.p.set(pp)\n+\ttraceEvent(traceEvProcStop, -1)\n+\tmp.p = oldp\n+\treleasem(mp)\n+}\n+\n+func traceGCStart() {\n+\ttraceEvent(traceEvGCStart, 3, trace.seqGC)\n+\ttrace.seqGC++\n+}\n+\n+func traceGCDone() {\n+\ttraceEvent(traceEvGCDone, -1)\n+}\n+\n+func traceGCScanStart() {\n+\ttraceEvent(traceEvGCScanStart, -1)\n+}\n+\n+func traceGCScanDone() {\n+\ttraceEvent(traceEvGCScanDone, -1)\n+}\n+\n+func traceGCSweepStart() {\n+\ttraceEvent(traceEvGCSweepStart, 1)\n+}\n+\n+func traceGCSweepDone() {\n+\ttraceEvent(traceEvGCSweepDone, -1)\n+}\n+\n+func traceGoCreate(newg *g, pc uintptr) {\n+\tnewg.traceseq = 0\n+\tnewg.tracelastp = getg().m.p\n+\t// +PCQuantum because traceFrameForPC expects return PCs and subtracts PCQuantum.\n+\tid := trace.stackTab.put([]uintptr{pc + sys.PCQuantum})\n+\ttraceEvent(traceEvGoCreate, 2, uint64(newg.goid), uint64(id))\n+}\n+\n+func traceGoStart() {\n+\t_g_ := getg().m.curg\n+\t_p_ := _g_.m.p\n+\t_g_.traceseq++\n+\tif _g_.tracelastp == _p_ {\n+\t\ttraceEvent(traceEvGoStartLocal, -1, uint64(_g_.goid))\n+\t} else {\n+\t\t_g_.tracelastp = _p_\n+\t\ttraceEvent(traceEvGoStart, -1, uint64(_g_.goid), _g_.traceseq)\n+\t}\n+}\n+\n+func traceGoEnd() {\n+\ttraceEvent(traceEvGoEnd, -1)\n+}\n+\n+func traceGoSched() {\n+\t_g_ := getg()\n+\t_g_.tracelastp = _g_.m.p\n+\ttraceEvent(traceEvGoSched, 1)\n+}\n+\n+func traceGoPreempt() {\n+\t_g_ := getg()\n+\t_g_.tracelastp = _g_.m.p\n+\ttraceEvent(traceEvGoPreempt, 1)\n+}\n+\n+func traceGoPark(traceEv byte, skip int, gp *g) {\n+\tif traceEv&traceFutileWakeup != 0 {\n+\t\ttraceEvent(traceEvFutileWakeup, -1)\n+\t}\n+\ttraceEvent(traceEv & ^traceFutileWakeup, skip)\n+}\n+\n+func traceGoUnpark(gp *g, skip int) {\n+\t_p_ := getg().m.p\n+\tgp.traceseq++\n+\tif gp.tracelastp == _p_ {\n+\t\ttraceEvent(traceEvGoUnblockLocal, skip, uint64(gp.goid))\n+\t} else {\n+\t\tgp.tracelastp = _p_\n+\t\ttraceEvent(traceEvGoUnblock, skip, uint64(gp.goid), gp.traceseq)\n+\t}\n+}\n+\n+func traceGoSysCall() {\n+\ttraceEvent(traceEvGoSysCall, 1)\n+}\n+\n+func traceGoSysExit(ts int64) {\n+\tif ts != 0 && ts < trace.ticksStart {\n+\t\t// There is a race between the code that initializes sysexitticks\n+\t\t// (in exitsyscall, which runs without a P, and therefore is not\n+\t\t// stopped with the rest of the world) and the code that initializes\n+\t\t// a new trace. The recorded sysexitticks must therefore be treated\n+\t\t// as \"best effort\". If they are valid for this trace, then great,\n+\t\t// use them for greater accuracy. But if they're not valid for this\n+\t\t// trace, assume that the trace was started after the actual syscall\n+\t\t// exit (but before we actually managed to start the goroutine,\n+\t\t// aka right now), and assign a fresh time stamp to keep the log consistent.\n+\t\tts = 0\n+\t}\n+\t_g_ := getg().m.curg\n+\t_g_.traceseq++\n+\t_g_.tracelastp = _g_.m.p\n+\ttraceEvent(traceEvGoSysExit, -1, uint64(_g_.goid), _g_.traceseq, uint64(ts)/traceTickDiv)\n+}\n+\n+func traceGoSysBlock(pp *p) {\n+\t// Sysmon and stopTheWorld can declare syscalls running on remote Ps as blocked,\n+\t// to handle this we temporary employ the P.\n+\tmp := acquirem()\n+\toldp := mp.p\n+\tmp.p.set(pp)\n+\ttraceEvent(traceEvGoSysBlock, -1)\n+\tmp.p = oldp\n+\treleasem(mp)\n+}\n+\n+func traceHeapAlloc() {\n+\ttraceEvent(traceEvHeapAlloc, -1, memstats.heap_live)\n+}\n+\n+func traceNextGC() {\n+\ttraceEvent(traceEvNextGC, -1, memstats.next_gc)\n+}\n+\n+*/"}, {"sha": "6e4c8fd89203fe85b2a11c5c8211de99e0ab48b7", "filename": "libgo/runtime/chan.goc", "status": "removed", "additions": 0, "deletions": 1130, "changes": 1130, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/40962ac03a869cf7e07e0672c0a649371896277b/libgo%2Fruntime%2Fchan.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/40962ac03a869cf7e07e0672c0a649371896277b/libgo%2Fruntime%2Fchan.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fchan.goc?ref=40962ac03a869cf7e07e0672c0a649371896277b", "patch": "@@ -1,1130 +0,0 @@\n-// Copyright 2009 The Go Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style\n-// license that can be found in the LICENSE file.\n-\n-package runtime\n-#include \"runtime.h\"\n-#include \"arch.h\"\n-#include \"go-type.h\"\n-#include \"malloc.h\"\n-#include \"chan.h\"\n-\n-uint32 runtime_Hchansize = sizeof(Hchan);\n-\n-static\tvoid\tdequeueg(WaitQ*);\n-static\tSudoG*\tdequeue(WaitQ*);\n-static\tvoid\tenqueue(WaitQ*, SudoG*);\n-\n-static Hchan*\n-makechan(ChanType *t, int64 hint)\n-{\n-\tHchan *c;\n-\tuintptr n;\n-\tconst Type *elem;\n-\n-\telem = t->__element_type;\n-\n-\t// compiler checks this but be safe.\n-\tif(elem->__size >= (1<<16))\n-\t\truntime_throw(\"makechan: invalid channel element type\");\n-\n-\tif(hint < 0 || (intgo)hint != hint || (elem->__size > 0 && (uintptr)hint > (MaxMem - sizeof(*c)) / elem->__size))\n-\t\truntime_panicstring(\"makechan: size out of range\");\n-\n-\tn = sizeof(*c);\n-\tn = ROUND(n, elem->__align);\n-\n-\t// allocate memory in one call\n-\tc = (Hchan*)runtime_mallocgc(sizeof(*c) + hint*elem->__size, (uintptr)t | TypeInfo_Chan, 0);\n-\tc->elemsize = elem->__size;\n-\tc->elemtype = elem;\n-\tc->dataqsiz = hint;\n-\n-\tif(debug)\n-\t\truntime_printf(\"makechan: chan=%p; elemsize=%D; dataqsiz=%D\\n\",\n-\t\t\tc, (int64)elem->__size, (int64)c->dataqsiz);\n-\n-\treturn c;\n-}\n-\n-func reflect.makechan(t *ChanType, size uint64) (c *Hchan) {\n-\tc = makechan(t, size);\n-}\n-\n-Hchan*\n-__go_new_channel(ChanType *t, uintptr hint)\n-{\n-\treturn makechan(t, hint);\n-}\n-\n-Hchan*\n-__go_new_channel_big(ChanType *t, uint64 hint)\n-{\n-\treturn makechan(t, hint);\n-}\n-\n-/*\n- * generic single channel send/recv\n- * if the bool pointer is nil,\n- * then the full exchange will\n- * occur. if pres is not nil,\n- * then the protocol will not\n- * sleep but return if it could\n- * not complete.\n- *\n- * sleep can wake up with g->param == nil\n- * when a channel involved in the sleep has\n- * been closed.  it is easiest to loop and re-run\n- * the operation; we'll see that it's now closed.\n- */\n-static bool\n-chansend(ChanType *t, Hchan *c, byte *ep, bool block, void *pc)\n-{\n-\tUSED(pc);\n-\tSudoG *sg;\n-\tSudoG mysg;\n-\tG* gp;\n-\tint64 t0;\n-\tG* g;\n-\n-\tg = runtime_g();\n-\n-\tif(c == nil) {\n-\t\tUSED(t);\n-\t\tif(!block)\n-\t\t\treturn false;\n-\t\truntime_park(nil, nil, \"chan send (nil chan)\");\n-\t\treturn false;  // not reached\n-\t}\n-\n-\tif(runtime_gcwaiting())\n-\t\truntime_gosched();\n-\n-\tif(debug) {\n-\t\truntime_printf(\"chansend: chan=%p\\n\", c);\n-\t}\n-\n-\tt0 = 0;\n-\tmysg.releasetime = 0;\n-\tif(runtime_blockprofilerate > 0) {\n-\t\tt0 = runtime_cputicks();\n-\t\tmysg.releasetime = -1;\n-\t}\n-\n-\truntime_lock(c);\n-\tif(c->closed)\n-\t\tgoto closed;\n-\n-\tif(c->dataqsiz > 0)\n-\t\tgoto asynch;\n-\n-\tsg = dequeue(&c->recvq);\n-\tif(sg != nil) {\n-\t\truntime_unlock(c);\n-\n-\t\tgp = sg->g;\n-\t\tgp->param = sg;\n-\t\tif(sg->elem != nil)\n-\t\t\truntime_memmove(sg->elem, ep, c->elemsize);\n-\t\tif(sg->releasetime)\n-\t\t\tsg->releasetime = runtime_cputicks();\n-\t\truntime_ready(gp);\n-\t\treturn true;\n-\t}\n-\n-\tif(!block) {\n-\t\truntime_unlock(c);\n-\t\treturn false;\n-\t}\n-\n-\tmysg.elem = ep;\n-\tmysg.g = g;\n-\tmysg.selectdone = nil;\n-\tg->param = nil;\n-\tenqueue(&c->sendq, &mysg);\n-\truntime_parkunlock(c, \"chan send\");\n-\n-\tif(g->param == nil) {\n-\t\truntime_lock(c);\n-\t\tif(!c->closed)\n-\t\t\truntime_throw(\"chansend: spurious wakeup\");\n-\t\tgoto closed;\n-\t}\n-\n-\tif(mysg.releasetime > 0)\n-\t\truntime_blockevent(mysg.releasetime - t0, 2);\n-\n-\treturn true;\n-\n-asynch:\n-\tif(c->closed)\n-\t\tgoto closed;\n-\n-\tif(c->qcount >= c->dataqsiz) {\n-\t\tif(!block) {\n-\t\t\truntime_unlock(c);\n-\t\t\treturn false;\n-\t\t}\n-\t\tmysg.g = g;\n-\t\tmysg.elem = nil;\n-\t\tmysg.selectdone = nil;\n-\t\tenqueue(&c->sendq, &mysg);\n-\t\truntime_parkunlock(c, \"chan send\");\n-\n-\t\truntime_lock(c);\n-\t\tgoto asynch;\n-\t}\n-\n-\truntime_memmove(chanbuf(c, c->sendx), ep, c->elemsize);\n-\tif(++c->sendx == c->dataqsiz)\n-\t\tc->sendx = 0;\n-\tc->qcount++;\n-\n-\tsg = dequeue(&c->recvq);\n-\tif(sg != nil) {\n-\t\tgp = sg->g;\n-\t\truntime_unlock(c);\n-\t\tif(sg->releasetime)\n-\t\t\tsg->releasetime = runtime_cputicks();\n-\t\truntime_ready(gp);\n-\t} else\n-\t\truntime_unlock(c);\n-\tif(mysg.releasetime > 0)\n-\t\truntime_blockevent(mysg.releasetime - t0, 2);\n-\treturn true;\n-\n-closed:\n-\truntime_unlock(c);\n-\truntime_panicstring(\"send on closed channel\");\n-\treturn false;  // not reached\n-}\n-\n-\n-static bool\n-chanrecv(ChanType *t, Hchan* c, byte *ep, bool block, bool *received)\n-{\n-\tSudoG *sg;\n-\tSudoG mysg;\n-\tG *gp;\n-\tint64 t0;\n-\tG *g;\n-\n-\tif(runtime_gcwaiting())\n-\t\truntime_gosched();\n-\n-\tif(debug)\n-\t\truntime_printf(\"chanrecv: chan=%p\\n\", c);\n-\n-\tg = runtime_g();\n-\n-\tif(c == nil) {\n-\t\tUSED(t);\n-\t\tif(!block)\n-\t\t\treturn false;\n-\t\truntime_park(nil, nil, \"chan receive (nil chan)\");\n-\t\treturn false;  // not reached\n-\t}\n-\n-\tt0 = 0;\n-\tmysg.releasetime = 0;\n-\tif(runtime_blockprofilerate > 0) {\n-\t\tt0 = runtime_cputicks();\n-\t\tmysg.releasetime = -1;\n-\t}\n-\n-\truntime_lock(c);\n-\tif(c->dataqsiz > 0)\n-\t\tgoto asynch;\n-\n-\tif(c->closed)\n-\t\tgoto closed;\n-\n-\tsg = dequeue(&c->sendq);\n-\tif(sg != nil) {\n-\t\truntime_unlock(c);\n-\n-\t\tif(ep != nil)\n-\t\t\truntime_memmove(ep, sg->elem, c->elemsize);\n-\t\tgp = sg->g;\n-\t\tgp->param = sg;\n-\t\tif(sg->releasetime)\n-\t\t\tsg->releasetime = runtime_cputicks();\n-\t\truntime_ready(gp);\n-\n-\t\tif(received != nil)\n-\t\t\t*received = true;\n-\t\treturn true;\n-\t}\n-\n-\tif(!block) {\n-\t\truntime_unlock(c);\n-\t\treturn false;\n-\t}\n-\n-\tmysg.elem = ep;\n-\tmysg.g = g;\n-\tmysg.selectdone = nil;\n-\tg->param = nil;\n-\tenqueue(&c->recvq, &mysg);\n-\truntime_parkunlock(c, \"chan receive\");\n-\n-\tif(g->param == nil) {\n-\t\truntime_lock(c);\n-\t\tif(!c->closed)\n-\t\t\truntime_throw(\"chanrecv: spurious wakeup\");\n-\t\tgoto closed;\n-\t}\n-\n-\tif(received != nil)\n-\t\t*received = true;\n-\tif(mysg.releasetime > 0)\n-\t\truntime_blockevent(mysg.releasetime - t0, 2);\n-\treturn true;\n-\n-asynch:\n-\tif(c->qcount <= 0) {\n-\t\tif(c->closed)\n-\t\t\tgoto closed;\n-\n-\t\tif(!block) {\n-\t\t\truntime_unlock(c);\n-\t\t\tif(received != nil)\n-\t\t\t\t*received = false;\n-\t\t\treturn false;\n-\t\t}\n-\t\tmysg.g = g;\n-\t\tmysg.elem = nil;\n-\t\tmysg.selectdone = nil;\n-\t\tenqueue(&c->recvq, &mysg);\n-\t\truntime_parkunlock(c, \"chan receive\");\n-\n-\t\truntime_lock(c);\n-\t\tgoto asynch;\n-\t}\n-\n-\tif(ep != nil)\n-\t\truntime_memmove(ep, chanbuf(c, c->recvx), c->elemsize);\n-\truntime_memclr(chanbuf(c, c->recvx), c->elemsize);\n-\tif(++c->recvx == c->dataqsiz)\n-\t\tc->recvx = 0;\n-\tc->qcount--;\n-\n-\tsg = dequeue(&c->sendq);\n-\tif(sg != nil) {\n-\t\tgp = sg->g;\n-\t\truntime_unlock(c);\n-\t\tif(sg->releasetime)\n-\t\t\tsg->releasetime = runtime_cputicks();\n-\t\truntime_ready(gp);\n-\t} else\n-\t\truntime_unlock(c);\n-\n-\tif(received != nil)\n-\t\t*received = true;\n-\tif(mysg.releasetime > 0)\n-\t\truntime_blockevent(mysg.releasetime - t0, 2);\n-\treturn true;\n-\n-closed:\n-\tif(ep != nil)\n-\t\truntime_memclr(ep, c->elemsize);\n-\tif(received != nil)\n-\t\t*received = false;\n-\truntime_unlock(c);\n-\tif(mysg.releasetime > 0)\n-\t\truntime_blockevent(mysg.releasetime - t0, 2);\n-\treturn true;\n-}\n-\n-// The compiler generates a call to __go_send_small to send a value 8\n-// bytes or smaller.\n-void\n-__go_send_small(ChanType *t, Hchan* c, uint64 val)\n-{\n-\tunion\n-\t{\n-\t\tbyte b[sizeof(uint64)];\n-\t\tuint64 v;\n-\t} u;\n-\tbyte *v;\n-\n-\tu.v = val;\n-#ifndef WORDS_BIGENDIAN\n-\tv = u.b;\n-#else\n-\tv = u.b + sizeof(uint64) - t->__element_type->__size;\n-#endif\n-\tchansend(t, c, v, true, runtime_getcallerpc(&t));\n-}\n-\n-// The compiler generates a call to __go_send_big to send a value\n-// larger than 8 bytes or smaller.\n-void\n-__go_send_big(ChanType *t, Hchan* c, byte* v)\n-{\n-\tchansend(t, c, v, true, runtime_getcallerpc(&t));\n-}\n-\n-// The compiler generates a call to __go_receive to receive a\n-// value from a channel.\n-void\n-__go_receive(ChanType *t, Hchan* c, byte* v)\n-{\n-\tchanrecv(t, c, v, true, nil);\n-}\n-\n-_Bool runtime_chanrecv2(ChanType *t, Hchan* c, byte* v)\n-  __asm__ (GOSYM_PREFIX \"runtime.chanrecv2\");\n-\n-_Bool\n-runtime_chanrecv2(ChanType *t, Hchan* c, byte* v)\n-{\n-\tbool received = false;\n-\n-\tchanrecv(t, c, v, true, &received);\n-\treturn received;\n-}\n-\n-// compiler implements\n-//\n-//\tselect {\n-//\tcase c <- v:\n-//\t\t... foo\n-//\tdefault:\n-//\t\t... bar\n-//\t}\n-//\n-// as\n-//\n-//\tif selectnbsend(c, v) {\n-//\t\t... foo\n-//\t} else {\n-//\t\t... bar\n-//\t}\n-//\n-func selectnbsend(t *ChanType, c *Hchan, elem *byte) (selected bool) {\n-\tselected = chansend(t, c, elem, false, runtime_getcallerpc(&t));\n-}\n-\n-// compiler implements\n-//\n-//\tselect {\n-//\tcase v = <-c:\n-//\t\t... foo\n-//\tdefault:\n-//\t\t... bar\n-//\t}\n-//\n-// as\n-//\n-//\tif selectnbrecv(&v, c) {\n-//\t\t... foo\n-//\t} else {\n-//\t\t... bar\n-//\t}\n-//\n-func selectnbrecv(t *ChanType, elem *byte, c *Hchan) (selected bool) {\n-\tselected = chanrecv(t, c, elem, false, nil);\n-}\n-\n-// compiler implements\n-//\n-//\tselect {\n-//\tcase v, ok = <-c:\n-//\t\t... foo\n-//\tdefault:\n-//\t\t... bar\n-//\t}\n-//\n-// as\n-//\n-//\tif c != nil && selectnbrecv2(&v, &ok, c) {\n-//\t\t... foo\n-//\t} else {\n-//\t\t... bar\n-//\t}\n-//\n-func selectnbrecv2(t *ChanType, elem *byte, received *bool, c *Hchan) (selected bool) {\n-\tbool r;\n-\n-\tselected = chanrecv(t, c, elem, false, received == nil ? nil : &r);\n-\tif(received != nil)\n-\t\t*received = r;\n-}\n-\n-func reflect.chansend(t *ChanType, c *Hchan, elem *byte, nb bool) (selected bool) {\n-\tselected = chansend(t, c, elem, !nb, runtime_getcallerpc(&t));\n-}\n-\n-func reflect.chanrecv(t *ChanType, c *Hchan, nb bool, elem *byte) (selected bool, received bool) {\n-\treceived = false;\n-\tselected = chanrecv(t, c, elem, !nb, &received);\n-}\n-\n-static Select* newselect(int32);\n-\n-func newselect(size int32) (sel *byte) {\n- \tsel = (byte*)newselect(size);\n-}\n-\n-static Select*\n-newselect(int32 size)\n-{\n-\tint32 n;\n-\tSelect *sel;\n-\n-\tn = 0;\n-\tif(size > 1)\n-\t\tn = size-1;\n-\n-\t// allocate all the memory we need in a single allocation\n-\t// start with Select with size cases\n-\t// then lockorder with size entries\n-\t// then pollorder with size entries\n-\tsel = runtime_mal(sizeof(*sel) +\n-\t\tn*sizeof(sel->scase[0]) +\n-\t\tsize*sizeof(sel->lockorder[0]) +\n-\t\tsize*sizeof(sel->pollorder[0]));\n-\n-\tsel->tcase = size;\n-\tsel->ncase = 0;\n-\tsel->lockorder = (void*)(sel->scase + size);\n-\tsel->pollorder = (void*)(sel->lockorder + size);\n-\n-\tif(debug)\n-\t\truntime_printf(\"newselect s=%p size=%d\\n\", sel, size);\n-\treturn sel;\n-}\n-\n-// cut in half to give stack a chance to split\n-static void selectsend(Select *sel, Hchan *c, int index, void *elem);\n-\n-func selectsend(sel *Select, c *Hchan, elem *byte, index int32) {\n-\t// nil cases do not compete\n-\tif(c != nil)\n-\t\tselectsend(sel, c, index, elem);\n-}\n-\n-static void\n-selectsend(Select *sel, Hchan *c, int index, void *elem)\n-{\n-\tint32 i;\n-\tScase *cas;\n-\n-\ti = sel->ncase;\n-\tif(i >= sel->tcase)\n-\t\truntime_throw(\"selectsend: too many cases\");\n-\tsel->ncase = i+1;\n-\tcas = &sel->scase[i];\n-\n-\tcas->index = index;\n-\tcas->chan = c;\n-\tcas->kind = CaseSend;\n-\tcas->sg.elem = elem;\n-\n-\tif(debug)\n-\t\truntime_printf(\"selectsend s=%p index=%d chan=%p\\n\",\n-\t\t\tsel, cas->index, cas->chan);\n-}\n-\n-// cut in half to give stack a chance to split\n-static void selectrecv(Select *sel, Hchan *c, int index, void *elem, bool*);\n-\n-func selectrecv(sel *Select, c *Hchan, elem *byte, index int32) {\n-\t// nil cases do not compete\n-\tif(c != nil)\n-\t\tselectrecv(sel, c, index, elem, nil);\n-}\n-\n-func selectrecv2(sel *Select, c *Hchan, elem *byte, received *bool, index int32) {\n-\t// nil cases do not compete\n-\tif(c != nil)\n-\t\tselectrecv(sel, c, index, elem, received);\n-}\n-\n-static void\n-selectrecv(Select *sel, Hchan *c, int index, void *elem, bool *received)\n-{\n-\tint32 i;\n-\tScase *cas;\n-\n-\ti = sel->ncase;\n-\tif(i >= sel->tcase)\n-\t\truntime_throw(\"selectrecv: too many cases\");\n-\tsel->ncase = i+1;\n-\tcas = &sel->scase[i];\n-\tcas->index = index;\n-\tcas->chan = c;\n-\n-\tcas->kind = CaseRecv;\n-\tcas->sg.elem = elem;\n-\tcas->receivedp = received;\n-\n-\tif(debug)\n-\t\truntime_printf(\"selectrecv s=%p index=%d chan=%p\\n\",\n-\t\t\tsel, cas->index, cas->chan);\n-}\n-\n-// cut in half to give stack a chance to split\n-static void selectdefault(Select*, int);\n-\n-func selectdefault(sel *Select, index int32) {\n-\tselectdefault(sel, index);\n-}\n-\n-static void\n-selectdefault(Select *sel, int32 index)\n-{\n-\tint32 i;\n-\tScase *cas;\n-\n-\ti = sel->ncase;\n-\tif(i >= sel->tcase)\n-\t\truntime_throw(\"selectdefault: too many cases\");\n-\tsel->ncase = i+1;\n-\tcas = &sel->scase[i];\n-\tcas->index = index;\n-\tcas->chan = nil;\n-\n-\tcas->kind = CaseDefault;\n-\n-\tif(debug)\n-\t\truntime_printf(\"selectdefault s=%p index=%d\\n\",\n-\t\t\tsel, cas->index);\n-}\n-\n-static void\n-sellock(Select *sel)\n-{\n-\tuint32 i;\n-\tHchan *c, *c0;\n-\n-\tc = nil;\n-\tfor(i=0; i<sel->ncase; i++) {\n-\t\tc0 = sel->lockorder[i];\n-\t\tif(c0 && c0 != c) {\n-\t\t\tc = sel->lockorder[i];\n-\t\t\truntime_lock(c);\n-\t\t}\n-\t}\n-}\n-\n-static void\n-selunlock(Select *sel)\n-{\n-\tint32 i, n, r;\n-\tHchan *c;\n-\n-\t// We must be very careful here to not touch sel after we have unlocked\n-\t// the last lock, because sel can be freed right after the last unlock.\n-\t// Consider the following situation.\n-\t// First M calls runtime_park() in runtime_selectgo() passing the sel.\n-\t// Once runtime_park() has unlocked the last lock, another M makes\n-\t// the G that calls select runnable again and schedules it for execution.\n-\t// When the G runs on another M, it locks all the locks and frees sel.\n-\t// Now if the first M touches sel, it will access freed memory.\n-\tn = (int32)sel->ncase;\n-\tr = 0;\n-\t// skip the default case\n-\tif(n>0 && sel->lockorder[0] == nil)\n-\t\tr = 1;\n-\tfor(i = n-1; i >= r; i--) {\n-\t\tc = sel->lockorder[i];\n-\t\tif(i>0 && sel->lockorder[i-1] == c)\n-\t\t\tcontinue;  // will unlock it on the next iteration\n-\t\truntime_unlock(c);\n-\t}\n-}\n-\n-static bool\n-selparkcommit(G *gp, void *sel)\n-{\n-\tUSED(gp);\n-\tselunlock(sel);\n-\treturn true;\n-}\n-\n-func block() {\n-\truntime_park(nil, nil, \"select (no cases)\");\t// forever\n-}\n-\n-static int selectgo(Select**);\n-\n-// selectgo(sel *byte);\n-\n-func selectgo(sel *Select) (ret int32) {\n-\treturn selectgo(&sel);\n-}\n-\n-static int\n-selectgo(Select **selp)\n-{\n-\tSelect *sel;\n-\tuint32 o, i, j, k, done;\n-\tint64 t0;\n-\tScase *cas, *dfl;\n-\tHchan *c;\n-\tSudoG *sg;\n-\tG *gp;\n-\tint index;\n-\tG *g;\n-\n-\tsel = *selp;\n-\tif(runtime_gcwaiting())\n-\t\truntime_gosched();\n-\n-\tif(debug)\n-\t\truntime_printf(\"select: sel=%p\\n\", sel);\n-\n-\tg = runtime_g();\n-\n-\tt0 = 0;\n-\tif(runtime_blockprofilerate > 0) {\n-\t\tt0 = runtime_cputicks();\n-\t\tfor(i=0; i<sel->ncase; i++)\n-\t\t\tsel->scase[i].sg.releasetime = -1;\n-\t}\n-\n-\t// The compiler rewrites selects that statically have\n-\t// only 0 or 1 cases plus default into simpler constructs.\n-\t// The only way we can end up with such small sel->ncase\n-\t// values here is for a larger select in which most channels\n-\t// have been nilled out.  The general code handles those\n-\t// cases correctly, and they are rare enough not to bother\n-\t// optimizing (and needing to test).\n-\n-\t// generate permuted order\n-\tfor(i=0; i<sel->ncase; i++)\n-\t\tsel->pollorder[i] = i;\n-\tfor(i=1; i<sel->ncase; i++) {\n-\t\to = sel->pollorder[i];\n-\t\tj = runtime_fastrand1()%(i+1);\n-\t\tsel->pollorder[i] = sel->pollorder[j];\n-\t\tsel->pollorder[j] = o;\n-\t}\n-\n-\t// sort the cases by Hchan address to get the locking order.\n-\t// simple heap sort, to guarantee n log n time and constant stack footprint.\n-\tfor(i=0; i<sel->ncase; i++) {\n-\t\tj = i;\n-\t\tc = sel->scase[j].chan;\n-\t\twhile(j > 0 && sel->lockorder[k=(j-1)/2] < c) {\n-\t\t\tsel->lockorder[j] = sel->lockorder[k];\n-\t\t\tj = k;\n-\t\t}\n-\t\tsel->lockorder[j] = c;\n-\t}\n-\tfor(i=sel->ncase; i-->0; ) {\n-\t\tc = sel->lockorder[i];\n-\t\tsel->lockorder[i] = sel->lockorder[0];\n-\t\tj = 0;\n-\t\tfor(;;) {\n-\t\t\tk = j*2+1;\n-\t\t\tif(k >= i)\n-\t\t\t\tbreak;\n-\t\t\tif(k+1 < i && sel->lockorder[k] < sel->lockorder[k+1])\n-\t\t\t\tk++;\n-\t\t\tif(c < sel->lockorder[k]) {\n-\t\t\t\tsel->lockorder[j] = sel->lockorder[k];\n-\t\t\t\tj = k;\n-\t\t\t\tcontinue;\n-\t\t\t}\n-\t\t\tbreak;\n-\t\t}\n-\t\tsel->lockorder[j] = c;\n-\t}\n-\t/*\n-\tfor(i=0; i+1<sel->ncase; i++)\n-\t\tif(sel->lockorder[i] > sel->lockorder[i+1]) {\n-\t\t\truntime_printf(\"i=%d %p %p\\n\", i, sel->lockorder[i], sel->lockorder[i+1]);\n-\t\t\truntime_throw(\"select: broken sort\");\n-\t\t}\n-\t*/\n-\tsellock(sel);\n-\n-loop:\n-\t// pass 1 - look for something already waiting\n-\tdfl = nil;\n-\tfor(i=0; i<sel->ncase; i++) {\n-\t\to = sel->pollorder[i];\n-\t\tcas = &sel->scase[o];\n-\t\tc = cas->chan;\n-\n-\t\tswitch(cas->kind) {\n-\t\tcase CaseRecv:\n-\t\t\tif(c->dataqsiz > 0) {\n-\t\t\t\tif(c->qcount > 0)\n-\t\t\t\t\tgoto asyncrecv;\n-\t\t\t} else {\n-\t\t\t\tsg = dequeue(&c->sendq);\n-\t\t\t\tif(sg != nil)\n-\t\t\t\t\tgoto syncrecv;\n-\t\t\t}\n-\t\t\tif(c->closed)\n-\t\t\t\tgoto rclose;\n-\t\t\tbreak;\n-\n-\t\tcase CaseSend:\n-\t\t\tif(c->closed)\n-\t\t\t\tgoto sclose;\n-\t\t\tif(c->dataqsiz > 0) {\n-\t\t\t\tif(c->qcount < c->dataqsiz)\n-\t\t\t\t\tgoto asyncsend;\n-\t\t\t} else {\n-\t\t\t\tsg = dequeue(&c->recvq);\n-\t\t\t\tif(sg != nil)\n-\t\t\t\t\tgoto syncsend;\n-\t\t\t}\n-\t\t\tbreak;\n-\n-\t\tcase CaseDefault:\n-\t\t\tdfl = cas;\n-\t\t\tbreak;\n-\t\t}\n-\t}\n-\n-\tif(dfl != nil) {\n-\t\tselunlock(sel);\n-\t\tcas = dfl;\n-\t\tgoto retc;\n-\t}\n-\n-\n-\t// pass 2 - enqueue on all chans\n-\tdone = 0;\n-\tfor(i=0; i<sel->ncase; i++) {\n-\t\to = sel->pollorder[i];\n-\t\tcas = &sel->scase[o];\n-\t\tc = cas->chan;\n-\t\tsg = &cas->sg;\n-\t\tsg->g = g;\n-\t\tsg->selectdone = &done;\n-\n-\t\tswitch(cas->kind) {\n-\t\tcase CaseRecv:\n-\t\t\tenqueue(&c->recvq, sg);\n-\t\t\tbreak;\n-\n-\t\tcase CaseSend:\n-\t\t\tenqueue(&c->sendq, sg);\n-\t\t\tbreak;\n-\t\t}\n-\t}\n-\n-\tg->param = nil;\n-\truntime_park(selparkcommit, sel, \"select\");\n-\n-\tsellock(sel);\n-\tsg = g->param;\n-\n-\t// pass 3 - dequeue from unsuccessful chans\n-\t// otherwise they stack up on quiet channels\n-\tfor(i=0; i<sel->ncase; i++) {\n-\t\tcas = &sel->scase[i];\n-\t\tif(cas != (Scase*)sg) {\n-\t\t\tc = cas->chan;\n-\t\t\tif(cas->kind == CaseSend)\n-\t\t\t\tdequeueg(&c->sendq);\n-\t\t\telse\n-\t\t\t\tdequeueg(&c->recvq);\n-\t\t}\n-\t}\n-\n-\tif(sg == nil)\n-\t\tgoto loop;\n-\n-\tcas = (Scase*)sg;\n-\tc = cas->chan;\n-\n-\tif(c->dataqsiz > 0)\n-\t\truntime_throw(\"selectgo: shouldn't happen\");\n-\n-\tif(debug)\n-\t\truntime_printf(\"wait-return: sel=%p c=%p cas=%p kind=%d\\n\",\n-\t\t\tsel, c, cas, cas->kind);\n-\n-\tif(cas->kind == CaseRecv) {\n-\t\tif(cas->receivedp != nil)\n-\t\t\t*cas->receivedp = true;\n-\t}\n-\n-\tselunlock(sel);\n-\tgoto retc;\n-\n-asyncrecv:\n-\t// can receive from buffer\n-\tif(cas->receivedp != nil)\n-\t\t*cas->receivedp = true;\n-\tif(cas->sg.elem != nil)\n-\t\truntime_memmove(cas->sg.elem, chanbuf(c, c->recvx), c->elemsize);\n-\truntime_memclr(chanbuf(c, c->recvx), c->elemsize);\n-\tif(++c->recvx == c->dataqsiz)\n-\t\tc->recvx = 0;\n-\tc->qcount--;\n-\tsg = dequeue(&c->sendq);\n-\tif(sg != nil) {\n-\t\tgp = sg->g;\n-\t\tselunlock(sel);\n-\t\tif(sg->releasetime)\n-\t\t\tsg->releasetime = runtime_cputicks();\n-\t\truntime_ready(gp);\n-\t} else {\n-\t\tselunlock(sel);\n-\t}\n-\tgoto retc;\n-\n-asyncsend:\n-\t// can send to buffer\n-\truntime_memmove(chanbuf(c, c->sendx), cas->sg.elem, c->elemsize);\n-\tif(++c->sendx == c->dataqsiz)\n-\t\tc->sendx = 0;\n-\tc->qcount++;\n-\tsg = dequeue(&c->recvq);\n-\tif(sg != nil) {\n-\t\tgp = sg->g;\n-\t\tselunlock(sel);\n-\t\tif(sg->releasetime)\n-\t\t\tsg->releasetime = runtime_cputicks();\n-\t\truntime_ready(gp);\n-\t} else {\n-\t\tselunlock(sel);\n-\t}\n-\tgoto retc;\n-\n-syncrecv:\n-\t// can receive from sleeping sender (sg)\n-\tselunlock(sel);\n-\tif(debug)\n-\t\truntime_printf(\"syncrecv: sel=%p c=%p o=%d\\n\", sel, c, o);\n-\tif(cas->receivedp != nil)\n-\t\t*cas->receivedp = true;\n-\tif(cas->sg.elem != nil)\n-\t\truntime_memmove(cas->sg.elem, sg->elem, c->elemsize);\n-\tgp = sg->g;\n-\tgp->param = sg;\n-\tif(sg->releasetime)\n-\t\tsg->releasetime = runtime_cputicks();\n-\truntime_ready(gp);\n-\tgoto retc;\n-\n-rclose:\n-\t// read at end of closed channel\n-\tselunlock(sel);\n-\tif(cas->receivedp != nil)\n-\t\t*cas->receivedp = false;\n-\tif(cas->sg.elem != nil)\n-\t\truntime_memclr(cas->sg.elem, c->elemsize);\n-\tgoto retc;\n-\n-syncsend:\n-\t// can send to sleeping receiver (sg)\n-\tselunlock(sel);\n-\tif(debug)\n-\t\truntime_printf(\"syncsend: sel=%p c=%p o=%d\\n\", sel, c, o);\n-\tif(sg->elem != nil)\n-\t\truntime_memmove(sg->elem, cas->sg.elem, c->elemsize);\n-\tgp = sg->g;\n-\tgp->param = sg;\n-\tif(sg->releasetime)\n-\t\tsg->releasetime = runtime_cputicks();\n-\truntime_ready(gp);\n-\n-retc:\n-\t// return index corresponding to chosen case\n-\tindex = cas->index;\n-\tif(cas->sg.releasetime > 0)\n-\t\truntime_blockevent(cas->sg.releasetime - t0, 2);\n-\truntime_free(sel);\n-\treturn index;\n-\n-sclose:\n-\t// send on closed channel\n-\tselunlock(sel);\n-\truntime_panicstring(\"send on closed channel\");\n-\treturn 0;  // not reached\n-}\n-\n-// This struct must match ../reflect/value.go:/runtimeSelect.\n-typedef struct runtimeSelect runtimeSelect;\n-struct runtimeSelect\n-{\n-\tintgo dir;\n-\tChanType *typ;\n-\tHchan *ch;\n-\tbyte *val;\n-};\n-\n-// This enum must match ../reflect/value.go:/SelectDir.\n-enum SelectDir {\n-\tSelectSend = 1,\n-\tSelectRecv,\n-\tSelectDefault,\n-};\n-\n-func reflect.rselect(cases Slice) (chosen int, recvOK bool) {\n-\tint32 i;\n-\tSelect *sel;\n-\truntimeSelect* rcase, *rc;\n-\n-\tchosen = -1;\n-\trecvOK = false;\n-\n-\trcase = (runtimeSelect*)cases.__values;\n-\n-\tsel = newselect(cases.__count);\n-\tfor(i=0; i<cases.__count; i++) {\n-\t\trc = &rcase[i];\n-\t\tswitch(rc->dir) {\n-\t\tcase SelectDefault:\n-\t\t\tselectdefault(sel, i);\n-\t\t\tbreak;\n-\t\tcase SelectSend:\n-\t\t\tif(rc->ch == nil)\n-\t\t\t\tbreak;\n-\t\t\tselectsend(sel, rc->ch, i, rc->val);\n-\t\t\tbreak;\n-\t\tcase SelectRecv:\n-\t\t\tif(rc->ch == nil)\n-\t\t\t\tbreak;\n-\t\t\tselectrecv(sel, rc->ch, i, rc->val, &recvOK);\n-\t\t\tbreak;\n-\t\t}\n-\t}\n-\n-\tchosen = (intgo)(uintptr)selectgo(&sel);\n-}\n-\n-static void closechan(Hchan *c, void *pc);\n-\n-func closechan(c *Hchan) {\n-\tclosechan(c, runtime_getcallerpc(&c));\n-}\n-\n-func reflect.chanclose(c *Hchan) {\n-\tclosechan(c, runtime_getcallerpc(&c));\n-}\n-\n-static void\n-closechan(Hchan *c, void *pc)\n-{\n-\tUSED(pc);\n-\tSudoG *sg;\n-\tG* gp;\n-\n-\tif(c == nil)\n-\t\truntime_panicstring(\"close of nil channel\");\n-\n-\tif(runtime_gcwaiting())\n-\t\truntime_gosched();\n-\n-\truntime_lock(c);\n-\tif(c->closed) {\n-\t\truntime_unlock(c);\n-\t\truntime_panicstring(\"close of closed channel\");\n-\t}\n-\tc->closed = true;\n-\n-\t// release all readers\n-\tfor(;;) {\n-\t\tsg = dequeue(&c->recvq);\n-\t\tif(sg == nil)\n-\t\t\tbreak;\n-\t\tgp = sg->g;\n-\t\tgp->param = nil;\n-\t\tif(sg->releasetime)\n-\t\t\tsg->releasetime = runtime_cputicks();\n-\t\truntime_ready(gp);\n-\t}\n-\n-\t// release all writers\n-\tfor(;;) {\n-\t\tsg = dequeue(&c->sendq);\n-\t\tif(sg == nil)\n-\t\t\tbreak;\n-\t\tgp = sg->g;\n-\t\tgp->param = nil;\n-\t\tif(sg->releasetime)\n-\t\t\tsg->releasetime = runtime_cputicks();\n-\t\truntime_ready(gp);\n-\t}\n-\n-\truntime_unlock(c);\n-}\n-\n-void\n-__go_builtin_close(Hchan *c)\n-{\n-\truntime_closechan(c);\n-}\n-\n-func reflect.chanlen(c *Hchan) (len int) {\n-\tif(c == nil)\n-\t\tlen = 0;\n-\telse\n-\t\tlen = c->qcount;\n-}\n-\n-func reflect.chancap(c *Hchan) (cap int) {\n-\tif(c == nil)\n-\t\tcap = 0;\n-\telse\n-\t\tcap = c->dataqsiz;\n-}\n-\n-intgo\n-__go_chan_cap(Hchan *c)\n-{\n-\treturn reflect_chancap(c);\n-}\n-\n-static SudoG*\n-dequeue(WaitQ *q)\n-{\n-\tSudoG *sgp;\n-\n-loop:\n-\tsgp = q->first;\n-\tif(sgp == nil)\n-\t\treturn nil;\n-\tq->first = sgp->link;\n-\n-\t// if sgp participates in a select and is already signaled, ignore it\n-\tif(sgp->selectdone != nil) {\n-\t\t// claim the right to signal\n-\t\tif(*sgp->selectdone != 0 || !runtime_cas(sgp->selectdone, 0, 1))\n-\t\t\tgoto loop;\n-\t}\n-\n-\treturn sgp;\n-}\n-\n-static void\n-dequeueg(WaitQ *q)\n-{\n-\tSudoG **l, *sgp, *prevsgp;\n-\tG *g;\n-\n-\tg = runtime_g();\n-\tprevsgp = nil;\n-\tfor(l=&q->first; (sgp=*l) != nil; l=&sgp->link, prevsgp=sgp) {\n-\t\tif(sgp->g == g) {\n-\t\t\t*l = sgp->link;\n-\t\t\tif(q->last == sgp)\n-\t\t\t\tq->last = prevsgp;\n-\t\t\tbreak;\n-\t\t}\n-\t}\n-}\n-\n-static void\n-enqueue(WaitQ *q, SudoG *sgp)\n-{\n-\tsgp->link = nil;\n-\tif(q->first == nil) {\n-\t\tq->first = sgp;\n-\t\tq->last = sgp;\n-\t\treturn;\n-\t}\n-\tq->last->link = sgp;\n-\tq->last = sgp;\n-}"}, {"sha": "473f365b921836e183f0c1d93451aff21780eaf7", "filename": "libgo/runtime/chan.h", "status": "removed", "additions": 0, "deletions": 76, "changes": 76, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/40962ac03a869cf7e07e0672c0a649371896277b/libgo%2Fruntime%2Fchan.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/40962ac03a869cf7e07e0672c0a649371896277b/libgo%2Fruntime%2Fchan.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fchan.h?ref=40962ac03a869cf7e07e0672c0a649371896277b", "patch": "@@ -1,76 +0,0 @@\n-// Copyright 2009 The Go Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style\n-// license that can be found in the LICENSE file.\n-\n-typedef\tstruct\tWaitQ\tWaitQ;\n-typedef\tstruct\tSudoG\tSudoG;\n-typedef\tstruct\tSelect\tSelect;\n-typedef\tstruct\tScase\tScase;\n-\n-typedef struct\t__go_type_descriptor\tType;\n-typedef struct\t__go_channel_type\tChanType;\n-\n-struct\tSudoG\n-{\n-\tG*\tg;\n-\tuint32*\tselectdone;\n-\tSudoG*\tlink;\n-\tint64\treleasetime;\n-\tbyte*\telem;\t\t// data element\n-\tuint32  ticket;\n-};\n-\n-struct\tWaitQ\n-{\n-\tSudoG*\tfirst;\n-\tSudoG*\tlast;\n-};\n-\n-// The garbage collector is assuming that Hchan can only contain pointers into the stack\n-// and cannot contain pointers into the heap.\n-struct\tHchan\n-{\n-\tuintgo\tqcount;\t\t\t// total data in the q\n-\tuintgo\tdataqsiz;\t\t// size of the circular q\n-\tuint16\telemsize;\n-\tuint16\tpad;\t\t\t// ensures proper alignment of the buffer that follows Hchan in memory\n-\tbool\tclosed;\n-\tconst Type* elemtype;\t\t// element type\n-\tuintgo\tsendx;\t\t\t// send index\n-\tuintgo\trecvx;\t\t\t// receive index\n-\tWaitQ\trecvq;\t\t\t// list of recv waiters\n-\tWaitQ\tsendq;\t\t\t// list of send waiters\n-\tLock;\n-};\n-\n-// Buffer follows Hchan immediately in memory.\n-// chanbuf(c, i) is pointer to the i'th slot in the buffer.\n-#define chanbuf(c, i) ((byte*)((c)+1)+(uintptr)(c)->elemsize*(i))\n-\n-enum\n-{\n-\tdebug = 0,\n-\n-\t// Scase.kind\n-\tCaseRecv,\n-\tCaseSend,\n-\tCaseDefault,\n-};\n-\n-struct\tScase\n-{\n-\tSudoG\tsg;\t\t\t// must be first member (cast to Scase)\n-\tHchan*\tchan;\t\t\t// chan\n-\tuint16\tkind;\n-\tuint16\tindex;\t\t\t// index to return\n-\tbool*\treceivedp;\t\t// pointer to received bool (recv2)\n-};\n-\n-struct\tSelect\n-{\n-\tuint16\ttcase;\t\t\t// total count of scase[]\n-\tuint16\tncase;\t\t\t// currently filled scase[]\n-\tuint16*\tpollorder;\t\t// case poll order\n-\tHchan**\tlockorder;\t\t// channel lock order\n-\tScase\tscase[1];\t\t// one per case (in order of appearance)\n-};"}, {"sha": "a7d87a79ad12c4644a54d83cdfea3941c14d4893", "filename": "libgo/runtime/go-cgo.c", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fgo-cgo.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fgo-cgo.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-cgo.c?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -10,7 +10,8 @@\n #include \"go-panic.h\"\n #include \"go-type.h\"\n \n-extern void __go_receive (ChanType *, Hchan *, byte *);\n+extern void chanrecv1 (ChanType *, Hchan *, void *)\n+  __asm__ (GOSYM_PREFIX \"runtime.chanrecv1\");\n \n /* Prepare to call from code written in Go to code written in C or\n    C++.  This takes the current goroutine out of the Go scheduler, as\n@@ -97,7 +98,7 @@ syscall_cgocallback ()\n \t Go.  In the case of -buildmode=c-archive or c-shared, this\n \t call may be coming in before package initialization is\n \t complete.  Wait until it is.  */\n-      __go_receive (NULL, runtime_main_init_done, NULL);\n+      chanrecv1 (NULL, runtime_main_init_done, NULL);\n     }\n \n   mp = runtime_m ();"}, {"sha": "3cc0c1dfbad2387ac06f5d2f6fba1c6625810e04", "filename": "libgo/runtime/heapdump.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fheapdump.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fheapdump.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fheapdump.c?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -462,7 +462,7 @@ dumpparams(void)\n \telse\n \t\tdumpbool(true); // big-endian ptrs\n \tdumpint(PtrSize);\n-\tdumpint(runtime_Hchansize);\n+\tdumpint(hchanSize);\n \tdumpint((uintptr)runtime_mheap.arena_start);\n \tdumpint((uintptr)runtime_mheap.arena_used);\n \tdumpint(0);\n@@ -769,7 +769,7 @@ dumpefacetypes(void *obj __attribute__ ((unused)), uintptr size, const Type *typ\n \tcase TypeInfo_Chan:\n \t\tif(type->__size == 0) // channels may have zero-sized objects in them\n \t\t\tbreak;\n-\t\tfor(i = runtime_Hchansize; i <= size - type->__size; i += type->__size) {\n+\t\tfor(i = hchanSize; i <= size - type->__size; i += type->__size) {\n \t\t\t//playgcprog(i, (uintptr*)type->gc + 1, dumpeface_callback, obj);\n \t\t}\n \t\tbreak;"}, {"sha": "ac6e396de55716a6fcfc6e805027be594d54bd66", "filename": "libgo/runtime/mgc0.c", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fmgc0.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fmgc0.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmgc0.c?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -56,7 +56,6 @@\n #include \"arch.h\"\n #include \"malloc.h\"\n #include \"mgc0.h\"\n-#include \"chan.h\"\n #include \"go-type.h\"\n \n // Map gccgo field names to gc field names.\n@@ -1112,15 +1111,13 @@ scanblock(Workbuf *wbuf, bool keepworking)\n \t\t\t// There are no heap pointers in struct Hchan,\n \t\t\t// so we can ignore the leading sizeof(Hchan) bytes.\n \t\t\tif(!(chantype->elem->__code & kindNoPointers)) {\n-\t\t\t\t// Channel's buffer follows Hchan immediately in memory.\n-\t\t\t\t// Size of buffer (cap(c)) is second int in the chan struct.\n-\t\t\t\tchancap = ((uintgo*)chan)[1];\n-\t\t\t\tif(chancap > 0) {\n+\t\t\t\tchancap = chan->dataqsiz;\n+\t\t\t\tif(chancap > 0 && markonly(chan->buf)) {\n \t\t\t\t\t// TODO(atom): split into two chunks so that only the\n \t\t\t\t\t// in-use part of the circular buffer is scanned.\n \t\t\t\t\t// (Channel routines zero the unused part, so the current\n \t\t\t\t\t// code does not lead to leaks, it's just a little inefficient.)\n-\t\t\t\t\t*sbuf.obj.pos++ = (Obj){(byte*)chan+runtime_Hchansize, chancap*chantype->elem->__size,\n+\t\t\t\t\t*sbuf.obj.pos++ = (Obj){chan->buf, chancap*chantype->elem->__size,\n \t\t\t\t\t\t(uintptr)chantype->elem->__gc | PRECISE | LOOP};\n \t\t\t\t\tif(sbuf.obj.pos == sbuf.obj.end)\n \t\t\t\t\t\tflushobjbuf(&sbuf);"}, {"sha": "02b62bef44e2772cbdc7add7ff8f2f097eb1fb25", "filename": "libgo/runtime/proc.c", "status": "modified", "additions": 42, "deletions": 2, "changes": 44, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fproc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fproc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fproc.c?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -564,7 +564,8 @@ static struct __go_channel_type chan_bool_type_descriptor =\n     CHANNEL_BOTH_DIR\n   };\n \n-extern Hchan *__go_new_channel (ChanType *, uintptr);\n+extern Hchan *makechan (ChanType *, int64)\n+  __asm__ (GOSYM_PREFIX \"runtime.makechan\");\n extern void closechan(Hchan *) __asm__ (GOSYM_PREFIX \"runtime.closechan\");\n \n static void\n@@ -613,7 +614,7 @@ runtime_main(void* dummy __attribute__((unused)))\n \t\truntime_throw(\"runtime_main not on m0\");\n \t__go_go(runtime_MHeap_Scavenger, nil);\n \n-\truntime_main_init_done = __go_new_channel(&chan_bool_type_descriptor, 0);\n+\truntime_main_init_done = makechan(&chan_bool_type_descriptor, 0);\n \n \t_cgo_notify_runtime_init_done();\n \n@@ -853,6 +854,14 @@ runtime_ready(G *gp)\n \tg->m->locks--;\n }\n \n+void goready(G*, int) __asm__ (GOSYM_PREFIX \"runtime.goready\");\n+\n+void\n+goready(G* gp, int traceskip __attribute__ ((unused)))\n+{\n+\truntime_ready(gp);\n+}\n+\n int32\n runtime_gcprocs(void)\n {\n@@ -1898,6 +1907,22 @@ runtime_park(bool(*unlockf)(G*, void*), void *lock, const char *reason)\n \truntime_mcall(park0);\n }\n \n+void gopark(FuncVal *, void *, String, byte, int)\n+  __asm__ (\"runtime.gopark\");\n+\n+void\n+gopark(FuncVal *unlockf, void *lock, String reason,\n+       byte traceEv __attribute__ ((unused)),\n+       int traceskip __attribute__ ((unused)))\n+{\n+\tif(g->atomicstatus != _Grunning)\n+\t\truntime_throw(\"bad g status\");\n+\tg->m->waitlock = lock;\n+\tg->m->waitunlockf = unlockf == nil ? nil : (void*)unlockf->fn;\n+\tg->waitreason = reason;\n+\truntime_mcall(park0);\n+}\n+\n static bool\n parkunlock(G *gp, void *lock)\n {\n@@ -1914,6 +1939,21 @@ runtime_parkunlock(Lock *lock, const char *reason)\n \truntime_park(parkunlock, lock, reason);\n }\n \n+void goparkunlock(Lock *, String, byte, int)\n+  __asm__ (GOSYM_PREFIX \"runtime.goparkunlock\");\n+\n+void\n+goparkunlock(Lock *lock, String reason, byte traceEv __attribute__ ((unused)),\n+\t     int traceskip __attribute__ ((unused)))\n+{\n+\tif(g->atomicstatus != _Grunning)\n+\t\truntime_throw(\"bad g status\");\n+\tg->m->waitlock = lock;\n+\tg->m->waitunlockf = parkunlock;\n+\tg->waitreason = reason;\n+\truntime_mcall(park0);\n+}\n+\n // runtime_park continuation on g0.\n static void\n park0(G *gp)"}, {"sha": "6f96b2bde6c814be8a8bb12d6a7ba6381358a9a5", "filename": "libgo/runtime/runtime.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fruntime.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fruntime.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.h?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -66,7 +66,7 @@ typedef\tstruct\tFuncVal\t\tFuncVal;\n typedef\tstruct\tSigTab\t\tSigTab;\n typedef\tstruct\tmcache\t\tMCache;\n typedef struct\tFixAlloc\tFixAlloc;\n-typedef\tstruct\tHchan\t\tHchan;\n+typedef\tstruct\thchan\t\tHchan;\n typedef\tstruct\tTimers\t\tTimers;\n typedef\tstruct\tTimer\t\tTimer;\n typedef\tstruct\tgcstats\t\tGCStats;\n@@ -75,6 +75,7 @@ typedef\tstruct\tParFor\t\tParFor;\n typedef\tstruct\tParForThread\tParForThread;\n typedef\tstruct\tcgoMal\t\tCgoMal;\n typedef\tstruct\tPollDesc\tPollDesc;\n+typedef\tstruct\tsudog\t\tSudoG;\n \n typedef\tstruct\t__go_open_array\t\tSlice;\n typedef struct\t__go_interface\t\tIface;\n@@ -294,7 +295,6 @@ extern\tuint32\truntime_panicking;\n extern\tint8*\truntime_goos;\n extern\tint32\truntime_ncpu;\n extern \tvoid\t(*runtime_sysargs)(int32, uint8**);\n-extern\tuint32\truntime_Hchansize;\n extern\tstruct debugVars runtime_debug;\n extern\tuintptr\truntime_maxstacksize;\n "}, {"sha": "b0d198e6073446096592467ae7f3d4d287158a05", "filename": "libgo/runtime/sema.goc", "status": "modified", "additions": 7, "deletions": 8, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fsema.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5d8c099edebfe908256c2bd77a0e2b67182b0f57/libgo%2Fruntime%2Fsema.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fsema.goc?ref=5d8c099edebfe908256c2bd77a0e2b67182b0f57", "patch": "@@ -19,7 +19,6 @@\n \n package sync\n #include \"runtime.h\"\n-#include \"chan.h\"\n #include \"arch.h\"\n \n typedef struct SemaWaiter SemaWaiter;\n@@ -373,7 +372,7 @@ func runtime_notifyListWait(l *notifyList, t uint32) {\n \tif (l->tail == nil) {\n \t\tl->head = &s;\n \t} else {\n-\t\tl->tail->link = &s;\n+\t\tl->tail->next = &s;\n \t}\n \tl->tail = &s;\n \truntime_parkunlock(&l->lock, \"semacquire\");\n@@ -409,8 +408,8 @@ func runtime_notifyListNotifyAll(l *notifyList) {\n \n \t// Go through the local list and ready all waiters.\n \twhile (s != nil) {\n-\t\tSudoG* next = s->link;\n-\t\ts->link = nil;\n+\t\tSudoG* next = s->next;\n+\t\ts->next = nil;\n \t\treadyWithTime(s, 4);\n \t\ts = next;\n \t}\n@@ -442,19 +441,19 @@ func runtime_notifyListNotifyOne(l *notifyList) {\n \t// needs to be notified. If it hasn't made it to the list yet we won't\n \t// find it, but it won't park itself once it sees the new notify number.\n \truntime_atomicstore(&l->notify, t+1);\n-\tfor (p = nil, s = l->head; s != nil; p = s, s = s->link) {\n+\tfor (p = nil, s = l->head; s != nil; p = s, s = s->next) {\n \t\tif (s->ticket == t) {\n-\t\t\tSudoG *n = s->link;\n+\t\t\tSudoG *n = s->next;\n \t\t\tif (p != nil) {\n-\t\t\t\tp->link = n;\n+\t\t\t\tp->next = n;\n \t\t\t} else {\n \t\t\t\tl->head = n;\n \t\t\t}\n \t\t\tif (n == nil) {\n \t\t\t\tl->tail = p;\n \t\t\t}\n \t\t\truntime_unlock(&l->lock);\n-\t\t\ts->link = nil;\n+\t\t\ts->next = nil;\n \t\t\treadyWithTime(s, 4);\n \t\t\treturn;\n \t\t}"}]}