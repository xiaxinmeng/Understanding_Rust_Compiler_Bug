{"sha": "636e80eea24b780f1d5f4c14c58fc00001df8508", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NjM2ZTgwZWVhMjRiNzgwZjFkNWY0YzE0YzU4ZmMwMDAwMWRmODUwOA==", "commit": {"author": {"name": "Martin Jambor", "email": "mjambor@suse.cz", "date": "2020-01-29T12:13:13Z"}, "committer": {"name": "Martin Jambor", "email": "mjambor@suse.cz", "date": "2020-01-29T12:13:13Z"}, "message": "SRA: Total scalarization after access propagation [PR92706]\n\n2020-01-29  Martin Jambor  <mjambor@suse.cz>\n\n\tPR tree-optimization/92706\n\t* tree-sra.c (struct access): Adjust comment of\n\tgrp_total_scalarization.\n\t(find_access_in_subtree): Look for single children spanning an entire\n\taccess.\n\t(scalarizable_type_p): Allow register accesses, adjust callers.\n\t(completely_scalarize): Remove function.\n\t(scalarize_elem): Likewise.\n\t(create_total_scalarization_access): Likewise.\n\t(sort_and_splice_var_accesses): Do not track total scalarization\n\tflags.\n\t(analyze_access_subtree): New parameter totally, adjust to new meaning\n\tof grp_total_scalarization.\n\t(analyze_access_trees): Pass new parameter to analyze_access_subtree.\n\t(can_totally_scalarize_forest_p): New function.\n\t(create_total_scalarization_access): Likewise.\n\t(create_total_access_and_reshape): Likewise.\n\t(total_should_skip_creating_access): Likewise.\n\t(totally_scalarize_subtree): Likewise.\n\t(analyze_all_variable_accesses): Perform total scalarization after\n\tsubaccess propagation using the new functions above.\n\t(initialize_constant_pool_replacements): Output initializers by\n\ttraversing the access tree.\n\n\ttestsuite/\n\t* gcc.dg/tree-ssa/pr92706-2.c: New test.\n\t* gcc.dg/guality/pr59776.c: Xfail tests for s2.g.", "tree": {"sha": "e3d0ede705fa9cf1b3272eddcac76f0fdc84e853", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/e3d0ede705fa9cf1b3272eddcac76f0fdc84e853"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/636e80eea24b780f1d5f4c14c58fc00001df8508", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/636e80eea24b780f1d5f4c14c58fc00001df8508", "html_url": "https://github.com/Rust-GCC/gccrs/commit/636e80eea24b780f1d5f4c14c58fc00001df8508", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/636e80eea24b780f1d5f4c14c58fc00001df8508/comments", "author": {"login": "jamborm", "id": 2180070, "node_id": "MDQ6VXNlcjIxODAwNzA=", "avatar_url": "https://avatars.githubusercontent.com/u/2180070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamborm", "html_url": "https://github.com/jamborm", "followers_url": "https://api.github.com/users/jamborm/followers", "following_url": "https://api.github.com/users/jamborm/following{/other_user}", "gists_url": "https://api.github.com/users/jamborm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamborm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamborm/subscriptions", "organizations_url": "https://api.github.com/users/jamborm/orgs", "repos_url": "https://api.github.com/users/jamborm/repos", "events_url": "https://api.github.com/users/jamborm/events{/privacy}", "received_events_url": "https://api.github.com/users/jamborm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jamborm", "id": 2180070, "node_id": "MDQ6VXNlcjIxODAwNzA=", "avatar_url": "https://avatars.githubusercontent.com/u/2180070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamborm", "html_url": "https://github.com/jamborm", "followers_url": "https://api.github.com/users/jamborm/followers", "following_url": "https://api.github.com/users/jamborm/following{/other_user}", "gists_url": "https://api.github.com/users/jamborm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamborm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamborm/subscriptions", "organizations_url": "https://api.github.com/users/jamborm/orgs", "repos_url": "https://api.github.com/users/jamborm/repos", "events_url": "https://api.github.com/users/jamborm/events{/privacy}", "received_events_url": "https://api.github.com/users/jamborm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "5b9e89c922dc2e7e8b8da644bd3a8917c16b22ac", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5b9e89c922dc2e7e8b8da644bd3a8917c16b22ac", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5b9e89c922dc2e7e8b8da644bd3a8917c16b22ac"}], "stats": {"total": 721, "additions": 537, "deletions": 184}, "files": [{"sha": "61da54df34638e9aeea7960e3f5e5d92c703f423", "filename": "gcc/ChangeLog", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/636e80eea24b780f1d5f4c14c58fc00001df8508/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/636e80eea24b780f1d5f4c14c58fc00001df8508/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=636e80eea24b780f1d5f4c14c58fc00001df8508", "patch": "@@ -1,3 +1,29 @@\n+2020-01-29  Martin Jambor  <mjambor@suse.cz>\n+\n+\tPR tree-optimization/92706\n+\t* tree-sra.c (struct access): Adjust comment of\n+\tgrp_total_scalarization.\n+\t(find_access_in_subtree): Look for single children spanning an entire\n+\taccess.\n+\t(scalarizable_type_p): Allow register accesses, adjust callers.\n+\t(completely_scalarize): Remove function.\n+\t(scalarize_elem): Likewise.\n+\t(create_total_scalarization_access): Likewise.\n+\t(sort_and_splice_var_accesses): Do not track total scalarization\n+\tflags.\n+\t(analyze_access_subtree): New parameter totally, adjust to new meaning\n+\tof grp_total_scalarization.\n+\t(analyze_access_trees): Pass new parameter to analyze_access_subtree.\n+\t(can_totally_scalarize_forest_p): New function.\n+\t(create_total_scalarization_access): Likewise.\n+\t(create_total_access_and_reshape): Likewise.\n+\t(total_should_skip_creating_access): Likewise.\n+\t(totally_scalarize_subtree): Likewise.\n+\t(analyze_all_variable_accesses): Perform total scalarization after\n+\tsubaccess propagation using the new functions above.\n+\t(initialize_constant_pool_replacements): Output initializers by\n+\ttraversing the access tree.\n+\n 2020-01-29  Martin Jambor  <mjambor@suse.cz>\n \n \t* tree-sra.c (verify_sra_access_forest): New function."}, {"sha": "38758207989c09359bfe6ccc6bcd74f42dd949df", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/636e80eea24b780f1d5f4c14c58fc00001df8508/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/636e80eea24b780f1d5f4c14c58fc00001df8508/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=636e80eea24b780f1d5f4c14c58fc00001df8508", "patch": "@@ -1,3 +1,9 @@\n+2020-01-29  Martin Jambor  <mjambor@suse.cz>\n+\n+\tPR tree-optimization/92706\n+\t* gcc.dg/tree-ssa/pr92706-2.c: New test.\n+\t* gcc.dg/guality/pr59776.c: Xfail tests for s2.g.\n+\n 2020-01-28  Jan Hubicka  <hubicka@ucw.cz>\n \n \t* gcc.dg/tree-prof/indir-call-prof-2.c: New testcase."}, {"sha": "6c1c8165b70748bbf181202a695730fb2bf485b6", "filename": "gcc/testsuite/gcc.dg/guality/pr59776.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/636e80eea24b780f1d5f4c14c58fc00001df8508/gcc%2Ftestsuite%2Fgcc.dg%2Fguality%2Fpr59776.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/636e80eea24b780f1d5f4c14c58fc00001df8508/gcc%2Ftestsuite%2Fgcc.dg%2Fguality%2Fpr59776.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fguality%2Fpr59776.c?ref=636e80eea24b780f1d5f4c14c58fc00001df8508", "patch": "@@ -12,11 +12,11 @@ foo (struct S *p)\n   struct S s1, s2;\t\t\t/* { dg-final { gdb-test pr59776.c:17 \"s1.f\" \"5.0\" } } */\n   s1 = *p;\t\t\t\t/* { dg-final { gdb-test pr59776.c:17 \"s1.g\" \"6.0\" } } */\n   s2 = s1;\t\t\t\t/* { dg-final { gdb-test pr59776.c:17 \"s2.f\" \"0.0\" } } */\n-  *(int *) &s2.f = 0;\t\t\t/* { dg-final { gdb-test pr59776.c:17 \"s2.g\" \"6.0\" } } */\n+  *(int *) &s2.f = 0;\t\t\t/* { dg-final { gdb-test pr59776.c:17 \"s2.g\" \"6.0\" { xfail *-*-* } } } */\n   asm volatile (NOP : : : \"memory\");\t/* { dg-final { gdb-test pr59776.c:20 \"s1.f\" \"5.0\" } } */\n   asm volatile (NOP : : : \"memory\");\t/* { dg-final { gdb-test pr59776.c:20 \"s1.g\" \"6.0\" } } */\n   s2 = s1;\t\t\t\t/* { dg-final { gdb-test pr59776.c:20 \"s2.f\" \"5.0\" } } */\n-  asm volatile (NOP : : : \"memory\");\t/* { dg-final { gdb-test pr59776.c:20 \"s2.g\" \"6.0\" } } */\n+  asm volatile (NOP : : : \"memory\");\t/* { dg-final { gdb-test pr59776.c:20 \"s2.g\" \"6.0\" { xfail *-*-* } } } */\n   asm volatile (NOP : : : \"memory\");\n }\n "}, {"sha": "37ab9765db0177c9284b82520870aab0e03e1e38", "filename": "gcc/testsuite/gcc.dg/tree-ssa/pr92706-2.c", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/636e80eea24b780f1d5f4c14c58fc00001df8508/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr92706-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/636e80eea24b780f1d5f4c14c58fc00001df8508/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr92706-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr92706-2.c?ref=636e80eea24b780f1d5f4c14c58fc00001df8508", "patch": "@@ -0,0 +1,19 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-O2 -fdump-tree-esra\" } */\n+\n+typedef __UINT64_TYPE__ uint64_t;\n+typedef __UINT32_TYPE__ uint32_t;\n+struct S { uint32_t i[2]; } __attribute__((aligned(__alignof__(uint64_t))));\n+typedef uint64_t my_int64 __attribute__((may_alias));\n+uint64_t load (void *p)\n+{\n+  struct S u, v, w;\n+  uint64_t tem;\n+  tem = *(my_int64 *)p;\n+  *(my_int64 *)&v = tem;\n+  u = v;\n+  w = u;\n+  return *(my_int64 *)&w;\n+}\n+\n+/* { dg-final { scan-tree-dump \"Created a replacement for v\" \"esra\" } } */"}, {"sha": "2b0849858de5a62e4915d4e89d651e1ca598b368", "filename": "gcc/tree-sra.c", "status": "modified", "additions": 484, "deletions": 182, "changes": 666, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/636e80eea24b780f1d5f4c14c58fc00001df8508/gcc%2Ftree-sra.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/636e80eea24b780f1d5f4c14c58fc00001df8508/gcc%2Ftree-sra.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-sra.c?ref=636e80eea24b780f1d5f4c14c58fc00001df8508", "patch": "@@ -211,8 +211,11 @@ struct access\n      is not propagated in the access tree in any direction.  */\n   unsigned grp_scalar_write : 1;\n \n-  /* Is this access an artificial one created to scalarize some record\n-     entirely? */\n+  /* In a root of an access tree, true means that the entire tree should be\n+     totally scalarized - that all scalar leafs should be scalarized and\n+     non-root grp_total_scalarization accesses should be honored.  Otherwise,\n+     non-root accesses with grp_total_scalarization should never get scalar\n+     replacements.  */\n   unsigned grp_total_scalarization : 1;\n \n   /* Other passes of the analysis use this bit to make function\n@@ -485,6 +488,15 @@ find_access_in_subtree (struct access *access, HOST_WIDE_INT offset,\n       access = child;\n     }\n \n+  /* Total scalarization does not replace single field structures with their\n+     single field but rather creates an access for them underneath.  Look for\n+     it.  */\n+  if (access)\n+    while (access->first_child\n+\t   && access->first_child->offset == offset\n+\t   && access->first_child->size == size)\n+      access = access->first_child;\n+\n   return access;\n }\n \n@@ -856,7 +868,8 @@ create_access (tree expr, gimple *stmt, bool write)\n static bool\n scalarizable_type_p (tree type, bool const_decl)\n {\n-  gcc_assert (!is_gimple_reg_type (type));\n+  if (is_gimple_reg_type (type))\n+    return true;\n   if (type_contains_placeholder_p (type))\n     return false;\n \n@@ -871,8 +884,7 @@ scalarizable_type_p (tree type, bool const_decl)\n \t  if (DECL_BIT_FIELD (fld))\n \t    return false;\n \n-\t  if (!is_gimple_reg_type (ft)\n-\t      && !scalarizable_type_p (ft, const_decl))\n+\t  if (!scalarizable_type_p (ft, const_decl))\n \t    return false;\n \t}\n \n@@ -902,8 +914,7 @@ scalarizable_type_p (tree type, bool const_decl)\n \treturn false;\n \n       tree elem = TREE_TYPE (type);\n-      if (!is_gimple_reg_type (elem)\n-\t  && !scalarizable_type_p (elem, const_decl))\n+      if (!scalarizable_type_p (elem, const_decl))\n \treturn false;\n       return true;\n     }\n@@ -912,114 +923,6 @@ scalarizable_type_p (tree type, bool const_decl)\n   }\n }\n \n-static void scalarize_elem (tree, HOST_WIDE_INT, HOST_WIDE_INT, bool, tree, tree);\n-\n-/* Create total_scalarization accesses for all scalar fields of a member\n-   of type DECL_TYPE conforming to scalarizable_type_p.  BASE\n-   must be the top-most VAR_DECL representing the variable; within that,\n-   OFFSET locates the member and REF must be the memory reference expression for\n-   the member.  */\n-\n-static void\n-completely_scalarize (tree base, tree decl_type, HOST_WIDE_INT offset, tree ref)\n-{\n-  switch (TREE_CODE (decl_type))\n-    {\n-    case RECORD_TYPE:\n-      for (tree fld = TYPE_FIELDS (decl_type); fld; fld = DECL_CHAIN (fld))\n-\tif (TREE_CODE (fld) == FIELD_DECL)\n-\t  {\n-\t    HOST_WIDE_INT pos = offset + int_bit_position (fld);\n-\t    tree ft = TREE_TYPE (fld);\n-\t    tree nref = build3 (COMPONENT_REF, ft, ref, fld, NULL_TREE);\n-\n-\t    scalarize_elem (base, pos, tree_to_uhwi (DECL_SIZE (fld)),\n-\t\t\t    TYPE_REVERSE_STORAGE_ORDER (decl_type),\n-\t\t\t    nref, ft);\n-\t  }\n-      break;\n-    case ARRAY_TYPE:\n-      {\n-\ttree elemtype = TREE_TYPE (decl_type);\n-\ttree elem_size = TYPE_SIZE (elemtype);\n-\tgcc_assert (elem_size && tree_fits_shwi_p (elem_size));\n-\tHOST_WIDE_INT el_size = tree_to_shwi (elem_size);\n-\tgcc_assert (el_size > 0);\n-\n-\ttree minidx = TYPE_MIN_VALUE (TYPE_DOMAIN (decl_type));\n-\tgcc_assert (TREE_CODE (minidx) == INTEGER_CST);\n-\ttree maxidx = TYPE_MAX_VALUE (TYPE_DOMAIN (decl_type));\n-\t/* Skip (some) zero-length arrays; others have MAXIDX == MINIDX - 1.  */\n-\tif (maxidx)\n-\t  {\n-\t    gcc_assert (TREE_CODE (maxidx) == INTEGER_CST);\n-\t    tree domain = TYPE_DOMAIN (decl_type);\n-\t    /* MINIDX and MAXIDX are inclusive, and must be interpreted in\n-\t       DOMAIN (e.g. signed int, whereas min/max may be size_int).  */\n-\t    offset_int idx = wi::to_offset (minidx);\n-\t    offset_int max = wi::to_offset (maxidx);\n-\t    if (!TYPE_UNSIGNED (domain))\n-\t      {\n-\t\tidx = wi::sext (idx, TYPE_PRECISION (domain));\n-\t\tmax = wi::sext (max, TYPE_PRECISION (domain));\n-\t      }\n-\t    for (int el_off = offset; idx <= max; ++idx)\n-\t      {\n-\t\ttree nref = build4 (ARRAY_REF, elemtype,\n-\t\t\t\t    ref,\n-\t\t\t\t    wide_int_to_tree (domain, idx),\n-\t\t\t\t    NULL_TREE, NULL_TREE);\n-\t\tscalarize_elem (base, el_off, el_size,\n-\t\t\t\tTYPE_REVERSE_STORAGE_ORDER (decl_type),\n-\t\t\t\tnref, elemtype);\n-\t\tel_off += el_size;\n-\t      }\n-\t  }\n-      }\n-      break;\n-    default:\n-      gcc_unreachable ();\n-    }\n-}\n-\n-/* Create total_scalarization accesses for a member of type TYPE, which must\n-   satisfy either is_gimple_reg_type or scalarizable_type_p.  BASE must be the\n-   top-most VAR_DECL representing the variable; within that, POS and SIZE locate\n-   the member, REVERSE gives its torage order. and REF must be the reference\n-   expression for it.  */\n-\n-static void\n-scalarize_elem (tree base, HOST_WIDE_INT pos, HOST_WIDE_INT size, bool reverse,\n-\t\ttree ref, tree type)\n-{\n-  if (is_gimple_reg_type (type))\n-  {\n-    struct access *access = create_access_1 (base, pos, size);\n-    access->expr = ref;\n-    access->type = type;\n-    access->grp_total_scalarization = 1;\n-    access->reverse = reverse;\n-    /* Accesses for intraprocedural SRA can have their stmt NULL.  */\n-  }\n-  else\n-    completely_scalarize (base, type, pos, ref);\n-}\n-\n-/* Create a total_scalarization access for VAR as a whole.  VAR must be of a\n-   RECORD_TYPE or ARRAY_TYPE conforming to scalarizable_type_p.  */\n-\n-static void\n-create_total_scalarization_access (tree var)\n-{\n-  HOST_WIDE_INT size = tree_to_uhwi (DECL_SIZE (var));\n-  struct access *access;\n-\n-  access = create_access_1 (var, 0, size);\n-  access->expr = var;\n-  access->type = TREE_TYPE (var);\n-  access->grp_total_scalarization = 1;\n-}\n-\n /* Return true if REF has an VIEW_CONVERT_EXPR somewhere in it.  */\n \n static inline bool\n@@ -2029,7 +1932,6 @@ sort_and_splice_var_accesses (tree var)\n       bool grp_assignment_read = access->grp_assignment_read;\n       bool grp_assignment_write = access->grp_assignment_write;\n       bool multiple_scalar_reads = false;\n-      bool total_scalarization = access->grp_total_scalarization;\n       bool grp_partial_lhs = access->grp_partial_lhs;\n       bool first_scalar = is_gimple_reg_type (access->type);\n       bool unscalarizable_region = access->grp_unscalarizable_region;\n@@ -2081,7 +1983,6 @@ sort_and_splice_var_accesses (tree var)\n \t  grp_assignment_write |= ac2->grp_assignment_write;\n \t  grp_partial_lhs |= ac2->grp_partial_lhs;\n \t  unscalarizable_region |= ac2->grp_unscalarizable_region;\n-\t  total_scalarization |= ac2->grp_total_scalarization;\n \t  relink_to_new_repr (access, ac2);\n \n \t  /* If there are both aggregate-type and scalar-type accesses with\n@@ -2122,9 +2023,7 @@ sort_and_splice_var_accesses (tree var)\n       access->grp_scalar_write = grp_scalar_write;\n       access->grp_assignment_read = grp_assignment_read;\n       access->grp_assignment_write = grp_assignment_write;\n-      access->grp_hint = total_scalarization\n-\t|| (multiple_scalar_reads && !constant_decl_p (var));\n-      access->grp_total_scalarization = total_scalarization;\n+      access->grp_hint = multiple_scalar_reads && !constant_decl_p (var);\n       access->grp_partial_lhs = grp_partial_lhs;\n       access->grp_unscalarizable_region = unscalarizable_region;\n       access->grp_same_access_path = grp_same_access_path;\n@@ -2420,15 +2319,16 @@ expr_with_var_bounded_array_refs_p (tree expr)\n }\n \n /* Analyze the subtree of accesses rooted in ROOT, scheduling replacements when\n-   both seeming beneficial and when ALLOW_REPLACEMENTS allows it.  Also set all\n-   sorts of access flags appropriately along the way, notably always set\n-   grp_read and grp_assign_read according to MARK_READ and grp_write when\n-   MARK_WRITE is true.\n+   both seeming beneficial and when ALLOW_REPLACEMENTS allows it.  If TOTALLY\n+   is set, we are totally scalarizing the aggregate.  Also set all sorts of\n+   access flags appropriately along the way, notably always set grp_read and\n+   grp_assign_read according to MARK_READ and grp_write when MARK_WRITE is\n+   true.\n \n    Creating a replacement for a scalar access is considered beneficial if its\n-   grp_hint is set (this means we are either attempting total scalarization or\n-   there is more than one direct read access) or according to the following\n-   table:\n+   grp_hint ot TOTALLY is set (this means either that there is more than one\n+   direct read access or that we are attempting total scalarization) or\n+   according to the following table:\n \n    Access written to through a scalar type (once or more times)\n    |\n@@ -2459,7 +2359,7 @@ expr_with_var_bounded_array_refs_p (tree expr)\n \n static bool\n analyze_access_subtree (struct access *root, struct access *parent,\n-\t\t\tbool allow_replacements)\n+\t\t\tbool allow_replacements, bool totally)\n {\n   struct access *child;\n   HOST_WIDE_INT limit = root->offset + root->size;\n@@ -2477,8 +2377,6 @@ analyze_access_subtree (struct access *root, struct access *parent,\n \troot->grp_write = 1;\n       if (parent->grp_assignment_write)\n \troot->grp_assignment_write = 1;\n-      if (parent->grp_total_scalarization)\n-\troot->grp_total_scalarization = 1;\n       if (!parent->grp_same_access_path)\n \troot->grp_same_access_path = 0;\n     }\n@@ -2493,18 +2391,20 @@ analyze_access_subtree (struct access *root, struct access *parent,\n     {\n       hole |= covered_to < child->offset;\n       sth_created |= analyze_access_subtree (child, root,\n-\t\t\t\t\t     allow_replacements && !scalar);\n+\t\t\t\t\t     allow_replacements && !scalar,\n+\t\t\t\t\t     totally);\n \n       root->grp_unscalarized_data |= child->grp_unscalarized_data;\n-      root->grp_total_scalarization &= child->grp_total_scalarization;\n       if (child->grp_covered)\n \tcovered_to += child->size;\n       else\n \thole = true;\n     }\n \n   if (allow_replacements && scalar && !root->first_child\n-      && (root->grp_hint\n+      && (totally || !root->grp_total_scalarization)\n+      && (totally\n+\t  || root->grp_hint\n \t  || ((root->grp_scalar_read || root->grp_assignment_read)\n \t      && (root->grp_scalar_write || root->grp_assignment_write))))\n     {\n@@ -2546,6 +2446,7 @@ analyze_access_subtree (struct access *root, struct access *parent,\n     {\n       if (allow_replacements\n \t  && scalar && !root->first_child\n+\t  && !root->grp_total_scalarization\n \t  && (root->grp_scalar_write || root->grp_assignment_write)\n \t  && !bitmap_bit_p (cannot_scalarize_away_bitmap,\n \t\t\t    DECL_UID (root->base)))\n@@ -2566,7 +2467,7 @@ analyze_access_subtree (struct access *root, struct access *parent,\n \troot->grp_total_scalarization = 0;\n     }\n \n-  if (!hole || root->grp_total_scalarization)\n+  if (!hole || totally)\n     root->grp_covered = 1;\n   else if (root->grp_write || comes_initialized_p (root->base))\n     root->grp_unscalarized_data = 1; /* not covered and written to */\n@@ -2582,7 +2483,8 @@ analyze_access_trees (struct access *access)\n \n   while (access)\n     {\n-      if (analyze_access_subtree (access, NULL, true))\n+      if (analyze_access_subtree (access, NULL, true,\n+\t\t\t\t  access->grp_total_scalarization))\n \tret = true;\n       access = access->next_grp;\n     }\n@@ -2855,6 +2757,369 @@ propagate_all_subaccesses (void)\n     }\n }\n \n+/* Return true if the forest beginning with ROOT does not contain\n+   unscalarizable regions or non-byte aligned accesses.  */\n+\n+static bool\n+can_totally_scalarize_forest_p (struct access *root)\n+{\n+  struct access *access = root;\n+  do\n+    {\n+      if (access->grp_unscalarizable_region\n+\t  || (access->offset % BITS_PER_UNIT) != 0\n+\t  || (access->size % BITS_PER_UNIT) != 0\n+\t  || (is_gimple_reg_type (access->type)\n+\t      && access->first_child))\n+\treturn false;\n+\n+      if (access->first_child)\n+\taccess = access->first_child;\n+      else if (access->next_sibling)\n+\taccess = access->next_sibling;\n+      else\n+\t{\n+\t  while (access->parent && !access->next_sibling)\n+\t    access = access->parent;\n+\t  if (access->next_sibling)\n+\t    access = access->next_sibling;\n+\t  else\n+\t    {\n+\t      gcc_assert (access == root);\n+\t      root = root->next_grp;\n+\t      access = root;\n+\t    }\n+\t}\n+    }\n+  while (access);\n+  return true;\n+}\n+\n+/* Create and return an ACCESS in PARENT spanning from POS with SIZE, TYPE and\n+   reference EXPR for total scalarization purposes and mark it as such.  Within\n+   the children of PARENT, link it in between PTR and NEXT_SIBLING.  */\n+\n+static struct access *\n+create_total_scalarization_access (struct access *parent, HOST_WIDE_INT pos,\n+\t\t\t\t   HOST_WIDE_INT size, tree type, tree expr,\n+\t\t\t\t   struct access **ptr,\n+\t\t\t\t   struct access *next_sibling)\n+{\n+  struct access *access = access_pool.allocate ();\n+  memset (access, 0, sizeof (struct access));\n+  access->base = parent->base;\n+  access->offset = pos;\n+  access->size = size;\n+  access->expr = expr;\n+  access->type = type;\n+  access->parent = parent;\n+  access->grp_write = parent->grp_write;\n+  access->grp_total_scalarization = 1;\n+  access->grp_hint = 1;\n+  access->grp_same_access_path = path_comparable_for_same_access (expr);\n+  access->reverse = reverse_storage_order_for_component_p (expr);\n+\n+  access->next_sibling = next_sibling;\n+  *ptr = access;\n+  return access;\n+}\n+\n+/* Create and return an ACCESS in PARENT spanning from POS with SIZE, TYPE and\n+   reference EXPR for total scalarization purposes and mark it as such, link it\n+   at *PTR and reshape the tree so that those elements at *PTR and their\n+   siblings which fall within the part described by POS and SIZE are moved to\n+   be children of the new access.  If a partial overlap is detected, return\n+   NULL.  */\n+\n+static struct access *\n+create_total_access_and_reshape (struct access *parent, HOST_WIDE_INT pos,\n+\t\t\t\t HOST_WIDE_INT size, tree type, tree expr,\n+\t\t\t\t struct access **ptr)\n+{\n+  struct access **p = ptr;\n+\n+  while (*p && (*p)->offset < pos + size)\n+    {\n+      if ((*p)->offset + (*p)->size > pos + size)\n+\treturn NULL;\n+      p = &(*p)->next_sibling;\n+    }\n+\n+  struct access *next_child = *ptr;\n+  struct access *new_acc\n+    = create_total_scalarization_access (parent, pos, size, type, expr,\n+\t\t\t\t\t ptr, *p);\n+  if (p != ptr)\n+    {\n+      new_acc->first_child = next_child;\n+      *p = NULL;\n+      for (struct access *a = next_child; a; a = a->next_sibling)\n+\ta->parent = new_acc;\n+    }\n+  return new_acc;\n+}\n+\n+static bool totally_scalarize_subtree (struct access *root);\n+\n+/* Return true if INNER is either the same type as OUTER or if it is the type\n+   of a record field in OUTER at offset zero, possibly in nested\n+   sub-records.  */\n+\n+static bool\n+access_and_field_type_match_p (tree outer, tree inner)\n+{\n+  if (TYPE_MAIN_VARIANT (outer) == TYPE_MAIN_VARIANT (inner))\n+    return true;\n+  if (TREE_CODE (outer) != RECORD_TYPE)\n+    return false;\n+  tree fld = TYPE_FIELDS (outer);\n+  while (fld)\n+    {\n+     if (TREE_CODE (fld) == FIELD_DECL)\n+       {\n+\tif (!zerop (DECL_FIELD_OFFSET (fld)))\n+\t  return false;\n+\tif (TYPE_MAIN_VARIANT (TREE_TYPE (fld)) == inner)\n+\t  return true;\n+\tif (TREE_CODE (TREE_TYPE (fld)) == RECORD_TYPE)\n+\t  fld = TYPE_FIELDS (TREE_TYPE (fld));\n+\telse\n+\t  return false;\n+       }\n+     else\n+       fld = DECL_CHAIN (fld);\n+    }\n+  return false;\n+}\n+\n+/* Return type of total_should_skip_creating_access indicating whether a total\n+   scalarization access for a field/element should be created, whether it\n+   already exists or whether the entire total scalarization has to fail.  */\n+\n+enum total_sra_field_state {TOTAL_FLD_CREATE, TOTAL_FLD_DONE, TOTAL_FLD_FAILED};\n+\n+/* Do all the necessary steps in total scalarization when the given aggregate\n+   type has a TYPE at POS with the given SIZE should be put into PARENT and\n+   when we have processed all its siblings with smaller offsets up until and\n+   including LAST_SEEN_SIBLING (which can be NULL).\n+\n+   If some further siblings are to be skipped, set *LAST_SEEN_SIBLING as\n+   appropriate.  Return TOTAL_FLD_CREATE id the caller should carry on with\n+   creating a new access, TOTAL_FLD_DONE if access or accesses capable of\n+   representing the described part of the aggregate for the purposes of total\n+   scalarization already exist or TOTAL_FLD_FAILED if there is a problem which\n+   prevents total scalarization from happening at all.  */\n+\n+static enum total_sra_field_state\n+total_should_skip_creating_access (struct access *parent,\n+\t\t\t\t   struct access **last_seen_sibling,\n+\t\t\t\t   tree type, HOST_WIDE_INT pos,\n+\t\t\t\t   HOST_WIDE_INT size)\n+{\n+  struct access *next_child;\n+  if (!*last_seen_sibling)\n+    next_child = parent->first_child;\n+  else\n+    next_child = (*last_seen_sibling)->next_sibling;\n+\n+  /* First, traverse the chain of siblings until it points to an access with\n+     offset at least equal to POS.  Check all skipped accesses whether they\n+     span the POS boundary and if so, return with a failure.  */\n+  while (next_child && next_child->offset < pos)\n+    {\n+      if (next_child->offset + next_child->size > pos)\n+\treturn TOTAL_FLD_FAILED;\n+      *last_seen_sibling = next_child;\n+      next_child = next_child->next_sibling;\n+    }\n+\n+  /* Now check whether next_child has exactly the right POS and SIZE and if so,\n+     whether it can represent what we need and can be totally scalarized\n+     itself.  */\n+  if (next_child && next_child->offset == pos\n+      && next_child->size == size)\n+    {\n+      if (!is_gimple_reg_type (next_child->type)\n+\t  && (!access_and_field_type_match_p (type, next_child->type)\n+\t      || !totally_scalarize_subtree (next_child)))\n+\treturn TOTAL_FLD_FAILED;\n+\n+      *last_seen_sibling = next_child;\n+      return TOTAL_FLD_DONE;\n+    }\n+\n+  /* If the child we're looking at would partially overlap, we just cannot\n+     totally scalarize.  */\n+  if (next_child\n+      && next_child->offset < pos + size\n+      && next_child->offset + next_child->size > pos + size)\n+    return TOTAL_FLD_FAILED;\n+\n+  if (is_gimple_reg_type (type))\n+    {\n+      /* We don't scalarize accesses that are children of other scalar type\n+\t accesses, so if we go on and create an access for a register type,\n+\t there should not be any pre-existing children.  There are rare cases\n+\t where the requested type is a vector but we already have register\n+\t accesses for all its elements which is equally good.  Detect that\n+\t situation or whether we need to bail out.  */\n+\n+      HOST_WIDE_INT covered = pos;\n+      bool skipping = false;\n+      while (next_child\n+\t     && next_child->offset + next_child->size <= pos + size)\n+\t{\n+\t  if (next_child->offset != covered\n+\t      || !is_gimple_reg_type (next_child->type))\n+\t    return TOTAL_FLD_FAILED;\n+\n+\t  covered += next_child->size;\n+\t  *last_seen_sibling = next_child;\n+\t  next_child = next_child->next_sibling;\n+\t  skipping = true;\n+\t}\n+\n+      if (skipping)\n+\t{\n+\t  if (covered != pos + size)\n+\t    return TOTAL_FLD_FAILED;\n+\t  else\n+\t    return TOTAL_FLD_DONE;\n+\t}\n+    }\n+\n+  return TOTAL_FLD_CREATE;\n+}\n+\n+/* Go over sub-tree rooted in ROOT and attempt to create scalar accesses\n+   spanning all uncovered areas covered by ROOT, return false if the attempt\n+   failed.  All created accesses will have grp_unscalarizable_region set (and\n+   should be ignored if the function returns false).  */\n+\n+static bool\n+totally_scalarize_subtree (struct access *root)\n+{\n+  gcc_checking_assert (!root->grp_unscalarizable_region);\n+  gcc_checking_assert (!is_gimple_reg_type (root->type));\n+\n+  struct access *last_seen_sibling = NULL;\n+\n+  switch (TREE_CODE (root->type))\n+    {\n+    case RECORD_TYPE:\n+      for (tree fld = TYPE_FIELDS (root->type); fld; fld = DECL_CHAIN (fld))\n+\tif (TREE_CODE (fld) == FIELD_DECL)\n+\t  {\n+\t    tree ft = TREE_TYPE (fld);\n+\t    HOST_WIDE_INT fsize = tree_to_uhwi (DECL_SIZE (fld));\n+\t    if (!fsize)\n+\t      continue;\n+\n+\t    HOST_WIDE_INT pos = root->offset + int_bit_position (fld);\n+\t    enum total_sra_field_state\n+\t      state = total_should_skip_creating_access (root,\n+\t\t\t\t\t\t\t &last_seen_sibling,\n+\t\t\t\t\t\t\t ft, pos, fsize);\n+\t    switch (state)\n+\t      {\n+\t      case TOTAL_FLD_FAILED:\n+\t\treturn false;\n+\t      case TOTAL_FLD_DONE:\n+\t\tcontinue;\n+\t      case TOTAL_FLD_CREATE:\n+\t\tbreak;\n+\t      default:\n+\t\tgcc_unreachable ();\n+\t      }\n+\n+\t    struct access **p = (last_seen_sibling\n+\t\t\t\t ? &last_seen_sibling->next_sibling\n+\t\t\t\t : &root->first_child);\n+\t    tree nref = build3 (COMPONENT_REF, ft, root->expr, fld, NULL_TREE);\n+\t    struct access *new_child\n+\t      = create_total_access_and_reshape (root, pos, fsize, ft, nref, p);\n+\t    if (!new_child)\n+\t      return false;\n+\n+\t    if (!is_gimple_reg_type (ft)\n+\t\t&& !totally_scalarize_subtree (new_child))\n+\t      return false;\n+\t    last_seen_sibling = new_child;\n+\t  }\n+      break;\n+    case ARRAY_TYPE:\n+      {\n+\ttree elemtype = TREE_TYPE (root->type);\n+\ttree elem_size = TYPE_SIZE (elemtype);\n+\tgcc_assert (elem_size && tree_fits_shwi_p (elem_size));\n+\tHOST_WIDE_INT el_size = tree_to_shwi (elem_size);\n+\tgcc_assert (el_size > 0);\n+\n+\ttree minidx = TYPE_MIN_VALUE (TYPE_DOMAIN (root->type));\n+\tgcc_assert (TREE_CODE (minidx) == INTEGER_CST);\n+\ttree maxidx = TYPE_MAX_VALUE (TYPE_DOMAIN (root->type));\n+\t/* Skip (some) zero-length arrays; others have MAXIDX == MINIDX - 1.  */\n+\tif (!maxidx)\n+\t  goto out;\n+\tgcc_assert (TREE_CODE (maxidx) == INTEGER_CST);\n+\ttree domain = TYPE_DOMAIN (root->type);\n+\t/* MINIDX and MAXIDX are inclusive, and must be interpreted in\n+\t   DOMAIN (e.g. signed int, whereas min/max may be size_int).  */\n+\toffset_int idx = wi::to_offset (minidx);\n+\toffset_int max = wi::to_offset (maxidx);\n+\tif (!TYPE_UNSIGNED (domain))\n+\t  {\n+\t    idx = wi::sext (idx, TYPE_PRECISION (domain));\n+\t    max = wi::sext (max, TYPE_PRECISION (domain));\n+\t  }\n+\tfor (HOST_WIDE_INT pos = root->offset;\n+\t     idx <= max;\n+\t     pos += el_size, ++idx)\n+\t  {\n+\t    enum total_sra_field_state\n+\t      state = total_should_skip_creating_access (root,\n+\t\t\t\t\t\t\t &last_seen_sibling,\n+\t\t\t\t\t\t\t elemtype, pos,\n+\t\t\t\t\t\t\t el_size);\n+\t    switch (state)\n+\t      {\n+\t      case TOTAL_FLD_FAILED:\n+\t\treturn false;\n+\t      case TOTAL_FLD_DONE:\n+\t\tcontinue;\n+\t      case TOTAL_FLD_CREATE:\n+\t\tbreak;\n+\t      default:\n+\t\tgcc_unreachable ();\n+\t      }\n+\n+\t    struct access **p = (last_seen_sibling\n+\t\t\t\t ? &last_seen_sibling->next_sibling\n+\t\t\t\t : &root->first_child);\n+\t    tree nref = build4 (ARRAY_REF, elemtype, root->expr,\n+\t\t\t\twide_int_to_tree (domain, idx),\n+\t\t\t\tNULL_TREE, NULL_TREE);\n+\t    struct access *new_child\n+\t      = create_total_access_and_reshape (root, pos, el_size, elemtype,\n+\t\t\t\t\t\t nref, p);\n+\t    if (!new_child)\n+\t      return false;\n+\n+\t    if (!is_gimple_reg_type (elemtype)\n+\t\t&& !totally_scalarize_subtree (new_child))\n+\t      return false;\n+\t    last_seen_sibling = new_child;\n+\t  }\n+      }\n+      break;\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+ out:\n+  return true;\n+}\n+\n /* Go through all accesses collected throughout the (intraprocedural) analysis\n    stage, exclude overlapping ones, identify representatives and build trees\n    out of them, making decisions about scalarization on the way.  Return true\n@@ -2867,8 +3132,22 @@ analyze_all_variable_accesses (void)\n   bitmap tmp = BITMAP_ALLOC (NULL);\n   bitmap_iterator bi;\n   unsigned i;\n-  bool optimize_speed_p = !optimize_function_for_size_p (cfun);\n \n+  bitmap_copy (tmp, candidate_bitmap);\n+  EXECUTE_IF_SET_IN_BITMAP (tmp, 0, i, bi)\n+    {\n+      tree var = candidate (i);\n+      struct access *access;\n+\n+      access = sort_and_splice_var_accesses (var);\n+      if (!access || !build_access_trees (access))\n+\tdisqualify_candidate (var,\n+\t\t\t      \"No or inhibitingly overlapping accesses.\");\n+    }\n+\n+  propagate_all_subaccesses ();\n+\n+  bool optimize_speed_p = !optimize_function_for_size_p (cfun);\n   /* If the user didn't set PARAM_SRA_MAX_SCALARIZATION_SIZE_<...>,\n      fall back to a target default.  */\n   unsigned HOST_WIDE_INT max_scalarization_size\n@@ -2884,54 +3163,63 @@ analyze_all_variable_accesses (void)\n       if (global_options_set.x_param_sra_max_scalarization_size_size)\n \tmax_scalarization_size = param_sra_max_scalarization_size_size;\n     }\n-\n   max_scalarization_size *= BITS_PER_UNIT;\n \n   EXECUTE_IF_SET_IN_BITMAP (candidate_bitmap, 0, i, bi)\n     if (bitmap_bit_p (should_scalarize_away_bitmap, i)\n \t&& !bitmap_bit_p (cannot_scalarize_away_bitmap, i))\n       {\n \ttree var = candidate (i);\n+\tif (!VAR_P (var))\n+\t  continue;\n \n-\tif (VAR_P (var) && scalarizable_type_p (TREE_TYPE (var),\n-\t\t\t\t\t\tconstant_decl_p (var)))\n+\tif (tree_to_uhwi (TYPE_SIZE (TREE_TYPE (var))) > max_scalarization_size)\n \t  {\n-\t    if (tree_to_uhwi (TYPE_SIZE (TREE_TYPE (var)))\n-\t\t<= max_scalarization_size)\n-\t      {\n-\t\tcreate_total_scalarization_access (var);\n-\t\tcompletely_scalarize (var, TREE_TYPE (var), 0, var);\n-\t\tstatistics_counter_event (cfun,\n-\t\t\t\t\t  \"Totally-scalarized aggregates\", 1);\n-\t\tif (dump_file && (dump_flags & TDF_DETAILS))\n-\t\t  {\n-\t\t    fprintf (dump_file, \"Will attempt to totally scalarize \");\n-\t\t    print_generic_expr (dump_file, var);\n-\t\t    fprintf (dump_file, \" (UID: %u): \\n\", DECL_UID (var));\n-\t\t  }\n-\t      }\n-\t    else if (dump_file && (dump_flags & TDF_DETAILS))\n+\t    if (dump_file && (dump_flags & TDF_DETAILS))\n \t      {\n \t\tfprintf (dump_file, \"Too big to totally scalarize: \");\n \t\tprint_generic_expr (dump_file, var);\n \t\tfprintf (dump_file, \" (UID: %u)\\n\", DECL_UID (var));\n \t      }\n+\t    continue;\n \t  }\n-      }\n \n-  bitmap_copy (tmp, candidate_bitmap);\n-  EXECUTE_IF_SET_IN_BITMAP (tmp, 0, i, bi)\n-    {\n-      tree var = candidate (i);\n-      struct access *access;\n+\tbool all_types_ok = true;\n+\tfor (struct access *access = get_first_repr_for_decl (var);\n+\t     access;\n+\t     access = access->next_grp)\n+\t  if (!can_totally_scalarize_forest_p (access)\n+\t      || !scalarizable_type_p (access->type, constant_decl_p (var)))\n+\t    {\n+\t      all_types_ok = false;\n+\t      break;\n+\t    }\n+\tif (!all_types_ok)\n+\t  continue;\n \n-      access = sort_and_splice_var_accesses (var);\n-      if (!access || !build_access_trees (access))\n-\tdisqualify_candidate (var,\n-\t\t\t      \"No or inhibitingly overlapping accesses.\");\n-    }\n+\tif (dump_file && (dump_flags & TDF_DETAILS))\n+\t  {\n+\t    fprintf (dump_file, \"Will attempt to totally scalarize \");\n+\t    print_generic_expr (dump_file, var);\n+\t    fprintf (dump_file, \" (UID: %u): \\n\", DECL_UID (var));\n+\t  }\n+\tbool scalarized = true;\n+\tfor (struct access *access = get_first_repr_for_decl (var);\n+\t     access;\n+\t     access = access->next_grp)\n+\t  if (!is_gimple_reg_type (access->type)\n+\t      && !totally_scalarize_subtree (access))\n+\t    {\n+\t      scalarized = false;\n+\t      break;\n+\t    }\n \n-  propagate_all_subaccesses ();\n+\tif (scalarized)\n+\t  for (struct access *access = get_first_repr_for_decl (var);\n+\t       access;\n+\t       access = access->next_grp)\n+\t    access->grp_total_scalarization = true;\n+      }\n \n   if (flag_checking)\n     verify_all_sra_access_forests ();\n@@ -3804,25 +4092,39 @@ initialize_constant_pool_replacements (void)\n       tree var = candidate (i);\n       if (!constant_decl_p (var))\n \tcontinue;\n-      vec<access_p> *access_vec = get_base_access_vector (var);\n-      if (!access_vec)\n-\tcontinue;\n-      for (unsigned i = 0; i < access_vec->length (); i++)\n+\n+      struct access *access = get_first_repr_for_decl (var);\n+\n+      while (access)\n \t{\n-\t  struct access *access = (*access_vec)[i];\n-\t  if (!access->replacement_decl)\n-\t    continue;\n-\t  gassign *stmt\n-\t    = gimple_build_assign (get_access_replacement (access),\n-\t\t\t\t   unshare_expr (access->expr));\n-\t  if (dump_file && (dump_flags & TDF_DETAILS))\n+\t  if (access->replacement_decl)\n \t    {\n-\t      fprintf (dump_file, \"Generating constant initializer: \");\n-\t      print_gimple_stmt (dump_file, stmt, 0);\n-\t      fprintf (dump_file, \"\\n\");\n+\t      gassign *stmt\n+\t\t= gimple_build_assign (get_access_replacement (access),\n+\t\t\t\t       unshare_expr (access->expr));\n+\t      if (dump_file && (dump_flags & TDF_DETAILS))\n+\t\t{\n+\t\t  fprintf (dump_file, \"Generating constant initializer: \");\n+\t\t  print_gimple_stmt (dump_file, stmt, 0);\n+\t\t  fprintf (dump_file, \"\\n\");\n+\t\t}\n+\t      gsi_insert_after (&gsi, stmt, GSI_NEW_STMT);\n+\t      update_stmt (stmt);\n+\t    }\n+\n+\t  if (access->first_child)\n+\t    access = access->first_child;\n+\t  else if (access->next_sibling)\n+\t    access = access->next_sibling;\n+\t  else\n+\t    {\n+\t      while (access->parent && !access->next_sibling)\n+\t\taccess = access->parent;\n+\t      if (access->next_sibling)\n+\t\taccess = access->next_sibling;\n+\t      else\n+\t\taccess = access->next_grp;\n \t    }\n-\t  gsi_insert_after (&gsi, stmt, GSI_NEW_STMT);\n-\t  update_stmt (stmt);\n \t}\n     }\n "}]}