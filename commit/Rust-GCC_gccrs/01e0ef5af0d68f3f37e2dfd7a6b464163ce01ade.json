{"sha": "01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDFlMGVmNWFmMGQ2OGYzZjM3ZTJkZmQ3YTZiNDY0MTYzY2UwMWFkZQ==", "commit": {"author": {"name": "Zdenek Dvorak", "email": "dvorakz@suse.cz", "date": "2006-09-28T12:02:11Z"}, "committer": {"name": "Zdenek Dvorak", "email": "rakdver@gcc.gnu.org", "date": "2006-09-28T12:02:11Z"}, "message": "loop.texi: New file.\n\n\t* doc/loop.texi: New file.\n\t* doc/gccint.texi: Reference loop.texi.\n\t* Makefile.in (loop.texi): Add.\n\nFrom-SVN: r117277", "tree": {"sha": "bec34abc87a3dcd7158d4005b56599307901db0d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/bec34abc87a3dcd7158d4005b56599307901db0d"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade", "html_url": "https://github.com/Rust-GCC/gccrs/commit/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade/comments", "author": null, "committer": null, "parents": [{"sha": "c6c621d4153e799e4618023a7252244810e1fde7", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c6c621d4153e799e4618023a7252244810e1fde7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c6c621d4153e799e4618023a7252244810e1fde7"}], "stats": {"total": 558, "additions": 557, "deletions": 1}, "files": [{"sha": "1044cc057de47b08d7f466c694fae8e24ab12fd7", "filename": "gcc/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade", "patch": "@@ -1,3 +1,9 @@\n+2006-09-28  Zdenek Dvorak <dvorakz@suse.cz>\n+\n+\t* doc/loop.texi: New file.\n+\t* doc/gccint.texi: Reference loop.texi.\n+\t* Makefile.in (loop.texi): Add.\n+\n 2006-09-27  Geoffrey Keating  <geoffk@apple.com>\n \n \t* config/darwin.h (ENABLE_EXECUTE_STACK): New, use getpagesize not"}, {"sha": "082d0f726a32df98a193e06b35dd21cc977d1660", "filename": "gcc/Makefile.in", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade", "patch": "@@ -3380,7 +3380,8 @@ TEXI_GCCINT_FILES = gccint.texi gcc-common.texi gcc-vers.texi\t\t\\\n \t rtl.texi md.texi tm.texi hostconfig.texi fragments.texi\t\\\n \t configfiles.texi collect2.texi headerdirs.texi funding.texi\t\\\n \t gnu.texi gpl.texi fdl.texi contrib.texi languages.texi\t\t\\\n-\t sourcebuild.texi gty.texi libgcc.texi cfg.texi tree-ssa.texi\n+\t sourcebuild.texi gty.texi libgcc.texi cfg.texi tree-ssa.texi\t\\\n+\t loop.texi\n \n TEXI_GCCINSTALL_FILES = install.texi install-old.texi fdl.texi\n "}, {"sha": "071c74cd4d7324a91c9f57f1982103ccdb63da4b", "filename": "gcc/doc/gccint.texi", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade/gcc%2Fdoc%2Fgccint.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade/gcc%2Fdoc%2Fgccint.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Fgccint.texi?ref=01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade", "patch": "@@ -111,6 +111,7 @@ Additional tutorial information is linked to from\n * RTL::             The intermediate representation that most passes work on.\n * Control Flow::    Maintaining and manipulating the control flow graph.\n * Tree SSA::        Analysis and optimization of the tree representation.\n+* Loop Representation:: Analysis and representation of loops\n * Machine Desc::    How to write machine description instruction patterns.\n * Target Macros::   How to write the machine description C macros and functions.\n * Host Config::     Writing the @file{xm-@var{machine}.h} file.\n@@ -141,6 +142,7 @@ Additional tutorial information is linked to from\n @include passes.texi\n @include c-tree.texi\n @include tree-ssa.texi\n+@include loop.texi\n @include rtl.texi\n @include cfg.texi\n @include md.texi"}, {"sha": "1282f22149babe00be76faa37b9b0fbf37297aea", "filename": "gcc/doc/loop.texi", "status": "added", "additions": 547, "deletions": 0, "changes": 547, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade/gcc%2Fdoc%2Floop.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade/gcc%2Fdoc%2Floop.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Floop.texi?ref=01e0ef5af0d68f3f37e2dfd7a6b464163ce01ade", "patch": "@@ -0,0 +1,547 @@\n+@c Copyright (c) 2006 Free Software Foundation, Inc.\n+@c Free Software Foundation, Inc.\n+@c This is part of the GCC manual.\n+@c For copying conditions, see the file gcc.texi.\n+\n+@c ---------------------------------------------------------------------\n+@c Loop Representation\n+@c ---------------------------------------------------------------------\n+\n+@node Loop Representation\n+@chapter Analysis and Representation of Loops\n+\n+GCC provides extensive infrastructure for work with natural loops, i.e.,\n+strongly connected components of CFG with only one entry block.  This\n+chapter describes representation of loops in GCC, both on GIMPLE and in\n+RTL, as well as the interfaces to loop-related analyses (induction\n+variable analysis and number of iterations analysis).\n+\n+@menu\n+* Loop representation::\t\tRepresentation and analysis of loops.\n+* Loop querying::\t\tGetting information about loops.\n+* Loop manipulation::\t\tLoop manipulation functions.\n+* LCSSA::\t\t\tLoop-closed SSA form.\n+* Scalar evolutions::   \tInduction variables on GIMPLE.\n+* loop-iv::\t\t\tInduction variables on RTL.\n+* Number of iterations::\tNumber of iterations analysis.\n+* Dependency analysis::\t\tData dependency analysis.\n+* Lambda::\t\t\tLinear loop transformations framework.\n+@end menu\n+\n+@node Loop representation\n+@section Loop representation\n+@cindex Loop representation\n+@cindex Loop analysis\n+\n+This chapter describes the representation of loops in GCC, and functions\n+that can be used to build, modify and analyze this representation.  Most\n+of the interfaces and data structures are declared in @file{cfgloop.h}.\n+At the moment, loop structures are analyzed and this information is\n+updated only by the optimization passes that deal with loops, but some\n+efforts are being made to make it available throughout most of the\n+optimization passes.\n+\n+In general, a natural loop has one entry block (header) and possibly\n+several back edges (latches) leading to the header from the inside of\n+the loop.  Loops with several latches may appear if several loops share\n+a single header, or if there is a branching in the middle of the loop.\n+The representation of loops in GCC however allows only loops with a\n+single latch.  During loop analysis, headers of such loops are split and\n+forwarder blocks are created in order to disambiguate their structures.\n+A heuristic based on profile information is used to determine whether\n+the latches correspond to sub-loops or to control flow in a single loop.\n+This means that the analysis sometimes changes the CFG, and if you run\n+it in the middle of an optimization pass, you must be able to deal with\n+the new blocks.\n+\n+Body of the loop is the set of blocks that are dominated by its header,\n+and reachable from its latch against the direction of edges in CFG.  The\n+loops are organized in a containment hierarchy (tree) such that all the\n+loops immediately contained inside loop L are the children of L in the\n+tree.  This tree is represented by the @code{struct loops} structure.\n+The root of this tree is a fake loop that contains all blocks in the\n+function.  Each of the loops is represented in a @code{struct loop}\n+structure.  Each loop is assigned an index (@code{num} field of the\n+@code{struct loop} structure), and the pointer to the loop is stored in\n+the corresponding field of the @code{parray} field of the loops\n+structure.  Index of a sub-loop is always greater than the index of its\n+super-loop.  The indices do not have to be continuous, there may be\n+empty (@code{NULL}) entries in the @code{parray} created by deleting\n+loops.  The index of a loop never changes.  The first unused index is\n+stored in the @code{num} field of the loops structure.\n+\n+Each basic block contains the reference to the innermost loop it belongs\n+to (@code{loop_father}).  For this reason, it is only possible to have\n+one @code{struct loops} structure initialized at the same time for each\n+CFG.  It is recommended to use the global variable @code{current_loops}\n+to contain the @code{struct loops} structure, especially if the loop\n+structures are updated throughout several passes.  Many of the loop\n+manipulation functions assume that dominance information is up-to-date.\n+\n+The loops are analyzed through @code{loop_optimizer_init} function.  The\n+argument of this function is a set of flags represented in an integer\n+bitmask.  These flags specify what other properties of the loop\n+structures should be calculated/enforced and preserved later:\n+\n+@itemize\n+@item @code{LOOPS_HAVE_PREHEADERS}: Forwarder blocks are created in such\n+a way that each loop has only one entry edge, and additionally, the\n+source block of this entry edge has only one successor.  This creates a\n+natural place where the code can be moved out of the loop, and ensures\n+that the entry edge of the loop leads from its immediate super-loop.\n+@item @code{LOOPS_HAVE_SIMPLE_LATCHES}: Forwarder blocks are created to\n+force the latch block of each loop to have only one successor.  This\n+ensures that the latch of the loop does not belong to any of its\n+sub-loops, and makes manipulation with the loops significantly easier.\n+Most of the loop manipulation functions assume that the loops are in\n+this shape.  Note that with this flag, the ``normal'' loop without any\n+control flow inside and with one exit consists of two basic blocks.\n+@item @code{LOOPS_HAVE_MARKED_IRREDUCIBLE_REGIONS}: Basic blocks and\n+edges in the strongly connected components that are not natural loops\n+(have more than one entry block) are marked with\n+@code{BB_IRREDUCIBLE_LOOP} and @code{EDGE_IRREDUCIBLE_LOOP} flags.  The\n+flag is not set for blocks and edges that belong to natural loops that\n+are in such an irreducible region (but it is set for the entry and exit\n+edges of such a loop, if they lead to/from this region).\n+@item @code{LOOPS_HAVE_MARKED_SINGLE_EXITS}: If a loop has exactly one\n+exit edge, this edge is stored in @code{single_exit} field of the loop\n+structure.  @code{NULL} is stored there otherwise.\n+@end itemize\n+\n+These properties may also be computed/enforced later, using functions\n+@code{create_preheaders}, @code{force_single_succ_latches},\n+@code{mark_irreducible_loops} and @code{mark_single_exit_loops}.\n+\n+The memory occupied by the loops structures should be freed with\n+@code{loop_optimizer_finalize} function.\n+\n+The CFG manipulation functions in general do not update loop structures.\n+Specialized versions that additionally do so are provided for the most\n+common tasks.  On GIMPLE, @code{cleanup_tree_cfg_loop} function can be\n+used to cleanup CFG while updating the loops structures if\n+@code{current_loops} is set.\n+\n+@node Loop querying\n+@section Loop querying\n+@cindex Loop querying\n+\n+The functions to query the information about loops are declared in\n+@file{cfgloop.h}.  Some of the information can be taken directly from\n+the structures.  @code{loop_father} field of each basic block contains\n+the innermost loop to that the block belongs.  The most useful fields of\n+loop structure (that are kept up-to-date at all times) are:\n+\n+@itemize\n+@item @code{header}, @code{latch}: Header and latch basic blocks of the\n+loop.\n+@item @code{num_nodes}: Number of basic blocks in the loop (including\n+the basic blocks of the sub-loops).\n+@item @code{depth}: The depth of the loop in the loops tree, i.e., the\n+number of super-loops of the loop.\n+@item @code{outer}, @code{inner}, @code{next}: The super-loop, the first\n+sub-loop, and the sibling of the loop in the loops tree.\n+@item @code{single_exit}: The exit edge of the loop, if the loop has\n+exactly one exit and the loops were analyzed with\n+LOOPS_HAVE_MARKED_SINGLE_EXITS.\n+@end itemize\n+\n+There are other fields in the loop structures, many of them used only by\n+some of the passes, or not updated during CFG changes; in general, they\n+should not be accessed directly.\n+\n+The most important functions to query loop structures are:\n+\n+@itemize\n+@item @code{flow_loops_dump}: Dumps the information about loops to a\n+file.\n+@item @code{verify_loop_structure}: Checks consistency of the loop\n+structures.\n+@item @code{loop_latch_edge}: Returns the latch edge of a loop.\n+@item @code{loop_preheader_edge}: If loops have preheaders, returns\n+the preheader edge of a loop.\n+@item @code{flow_loop_nested_p}: Tests whether loop is a sub-loop of\n+another loop.\n+@item @code{flow_bb_inside_loop_p}: Tests whether a basic block belongs\n+to a loop (including its sub-loops).\n+@item @code{find_common_loop}: Finds the common super-loop of two loops.\n+@item @code{superloop_at_depth}: Returns the super-loop of a loop with\n+the given depth.\n+@item @code{tree_num_loop_insns}, @code{num_loop_insns}: Estimates the\n+number of insns in the loop, on GIMPLE and on RTL.\n+@item @code{loop_exit_edge_p}: Tests whether edge is an exit from a\n+loop.\n+@item @code{mark_loop_exit_edges}: Marks all exit edges of all loops\n+with @code{EDGE_LOOP_EXIT} flag.\n+@item @code{get_loop_body}, @code{get_loop_body_in_dom_order},\n+@code{get_loop_body_in_bfs_order}: Enumerates the basic blocks in the\n+loop in depth-first search order in reversed CFG, ordered by dominance\n+relation, and breath-first search order, respectively.\n+@item @code{get_loop_exit_edges}: Enumerates the exit edges of a loop.\n+@item @code{just_once_each_iteration_p}: Returns true if the basic block\n+is executed exactly once during each iteration of a loop (that is, it\n+does not belong to a sub-loop, and it dominates the latch of the loop).\n+@end itemize\n+\n+@node Loop manipulation\n+@section Loop manipulation\n+@cindex Loop manipulation\n+\n+The loops tree can be manipulated using the following functions:\n+\n+@itemize\n+@item @code{flow_loop_tree_node_add}: Adds a node to the tree.\n+@item @code{flow_loop_tree_node_remove}: Removes a node from the tree.\n+@item @code{add_bb_to_loop}: Adds a basic block to a loop.\n+@item @code{remove_bb_from_loops}: Removes a basic block from loops.\n+@end itemize\n+\n+The specialized versions of several low-level CFG functions that also\n+update loop structures are provided:\n+\n+@itemize\n+@item @code{loop_split_edge_with}: Splits an edge, and places a\n+specified RTL code on it.  On GIMPLE, the function can still be used,\n+but the code must be NULL.\n+@item @code{bsi_insert_on_edge_immediate_loop}: Inserts code on edge,\n+splitting it if necessary.  Only works on GIMPLE.\n+@item @code{remove_path}: Removes an edge and all blocks it dominates.\n+@item @code{loop_commit_inserts}: Commits insertions scheduled on edges,\n+and sets loops for the new blocks.  This function can only be used on\n+GIMPLE.\n+@item @code{split_loop_exit_edge}: Splits exit edge of the loop,\n+ensuring that PHI node arguments remain in the loop (this ensures that\n+loop-closed SSA form is preserved).  Only useful on GIMPLE.\n+@end itemize\n+\n+Finally, there are some higher-level loop transformations implemented.\n+While some of them are written so that they should work on non-innermost\n+loops, they are mostly untested in that case, and at the moment, they\n+are only reliable for the innermost loops:\n+\n+@itemize\n+@item @code{create_iv}: Creates a new induction variable.  Only works on\n+GIMPLE.  @code{standard_iv_increment_position} can be used to find a\n+suitable place for the iv increment.\n+@item @code{duplicate_loop_to_header_edge},\n+@code{tree_duplicate_loop_to_header_edge}: These functions (on RTL and\n+on GIMPLE) duplicate the body of the loop prescribed number of times on\n+one of the edges entering loop header, thus performing either loop\n+unrolling or loop peeling.  @code{can_duplicate_loop_p}\n+(@code{can_unroll_loop_p} on GIMPLE) must be true for the duplicated\n+loop.\n+@item @code{loop_version}, @code{tree_ssa_loop_version}: These function\n+create a copy of a loop, and a branch before them that selects one of\n+them depending on the prescribed condition.  This is useful for\n+optimizations that need to verify some assumptions in runtime (one of\n+the copies of the loop is usually left unchanged, while the other one is\n+transformed in some way).\n+@item @code{tree_unroll_loop}: Unrolls the loop, including peeling the\n+extra iterations to make the number of iterations divisible by unroll\n+factor, updating the exit condition, and removing the exits that now\n+cannot be taken.  Works only on GIMPLE.\n+@end itemize\n+\n+@node LCSSA\n+@section Loop-closed SSA form\n+@cindex LCSSA\n+@cindex Loop-closed SSA form\n+\n+Throughout the loop optimizations on tree level, one extra condition is\n+enforced on the SSA form:  No SSA name is used outside of the loop in\n+that it is defined.  The SSA form satisfying this condition is called\n+``loop-closed SSA form'' -- LCSSA.  To enforce LCSSA, PHI nodes must be\n+created at the exits of the loops for the SSA names that are used\n+outside of them.  Only the real operands (not virtual SSA names) are\n+held in LCSSA, in order to save memory.\n+\n+There are various benefits of LCSSA:\n+\n+@itemize\n+@item Many optimizations (value range analysis, final value\n+replacement) are interested in the values that are defined in the loop\n+and used outside of it, i.e., exactly those for that we create new PHI\n+nodes.\n+@item In induction variable analysis, it is not necessary to specify the\n+loop in that the analysis should be performed -- the scalar evolution\n+analysis always returns the results with respect to the loop in that the\n+SSA name is defined.\n+@item It makes updating of SSA form during loop transformations simpler.\n+Without LCSSA, operations like loop unrolling may force creation of PHI\n+nodes arbitrarily far from the loop, while in LCSSA, the SSA form can be\n+updated locally.  However, since we only keep real operands in LCSSA, we\n+cannot use this advantage (we could have local updating of real\n+operands, but it is not much more efficient than to use generic SSA form\n+updating for it as well; the amount of changes to SSA is the same).\n+@end itemize\n+\n+However, it also means LCSSA must be updated.  This is usually\n+straightforward, unless you create a new value in loop and use it\n+outside, or unless you manipulate loop exit edges (functions are\n+provided to make these manipulations simple).\n+@code{rewrite_into_loop_closed_ssa} is used to rewrite SSA form to\n+LCSSA, and @code{verify_loop_closed_ssa} to check that the invariant of\n+LCSSA is preserved.\n+\n+@node Scalar evolutions\n+@section Scalar evolutions\n+@cindex Scalar evolutions\n+@cindex IV analysis on GIMPLE\n+\n+Scalar evolutions (SCEV) are used to represent results of induction\n+variable analysis on GIMPLE.  They enable us to represent variables with\n+complicated behavior in a simple and consistent way (we only use it to\n+express values of polynomial induction variables, but it is possible to\n+extend it).  The interfaces to SCEV analysis are declared in\n+@file{tree-scalar-evolution.h}.  To use scalar evolutions analysis,\n+@code{scev_initialize} must be used.  To stop using SCEV,\n+@code{scev_finalize} should be used.  SCEV analysis caches results in\n+order to save time and memory.  This cache however is made invalid by\n+most of the loop transformations, including removal of code.  If such a\n+transformation is performed, @code{scev_reset} must be called to clean\n+the caches.\n+\n+Given an SSA name, its behavior in loops can be analyzed using the\n+@code{analyze_scalar_evolution} function.  The returned SCEV however\n+does not have to be fully analyzed and it may contain references to\n+other SSA names defined in the loop.  To resolve these (potentially\n+recursive) references, @code{instantiate_parameters} or\n+@code{resolve_mixers} functions must be used.\n+@code{instantiate_parameters} is useful when you use the results of SCEV\n+only for some analysis, and when you work with whole nest of loops at\n+once.  It will try replacing all SSA names by their SCEV in all loops,\n+including the super-loops of the current loop, thus providing a complete\n+information about the behavior of the variable in the loop nest.\n+@code{resolve_mixers} is useful if you work with only one loop at a\n+time, and if you possibly need to create code based on the value of the\n+induction variable.  It will only resolve the SSA names defined in the\n+current loop, leaving the SSA names defined outside unchanged, even if\n+their evolution in the outer loops is known.\n+\n+The SCEV is a normal tree expression, except for the fact that it may\n+contain several special tree nodes.  One of them is\n+@code{SCEV_NOT_KNOWN}, used for SSA names whose value cannot be\n+expressed.  The other one is @code{POLYNOMIAL_CHREC}.  Polynomial chrec\n+has three arguments -- base, step and loop (both base and step may\n+contain further polynomial chrecs).  Type of the expression and of base\n+and step must be the same.  A variable has evolution\n+@code{POLYNOMIAL_CHREC(base, step, loop)} if it is (in the specified\n+loop) equivalent to @code{x_1} in the following example\n+\n+@smallexample\n+while (...)\n+  @{\n+    x_1 = phi (base, x_2);\n+    x_2 = x_1 + step;\n+  @}\n+@end smallexample\n+\n+Note that this includes the language restrictions on the operations.\n+For example, if we compile C code and @code{x} has signed type, then the\n+overflow in addition would cause undefined behavior, and we may assume\n+that this does not happen.  Hence, the value with this SCEV cannot\n+overflow (which restricts the number of iterations of such a loop).\n+\n+In many cases, one wants to restrict the attention just to affine\n+induction variables.  In this case, the extra expressive power of SCEV\n+is not useful, and may complicate the optimizations.  In this case,\n+@code{simple_iv} function may be used to analyze a value -- the result\n+is a loop-invariant base and step.\n+\n+@node loop-iv\n+@section IV analysis on RTL\n+@cindex IV analysis on RTL\n+\n+The induction variable on RTL is simple and only allows analysis of\n+affine induction variables, and only in one loop at once.  The interface\n+is declared in @file{cfgloop.h}.  Before analyzing induction variables\n+in a loop L, @code{iv_analysis_loop_init} function must be called on L.\n+After the analysis (possibly calling @code{iv_analysis_loop_init} for\n+several loops) is finished, @code{iv_analysis_done} should be called.\n+The following functions can be used to access the results of the\n+analysis:\n+\n+@itemize\n+@item @code{iv_analyze}: Analyzes a single register used in the given\n+insn.  If no use of the register in this insn is found, the following\n+insns are scanned, so that this function can be called on the insn\n+returned by get_condition.\n+@item @code{iv_analyze_result}: Analyzes result of the assignment in the\n+given insn.\n+@item @code{iv_analyze_expr}: Analyzes a more complicated expression.\n+All its operands are analyzed by @code{iv_analyze}, and hence they must\n+be used in the specified insn or one of the following insns.\n+@end itemize\n+\n+The description of the induction variable is provided in @code{struct\n+rtx_iv}.  In order to handle subregs, the representation is a bit\n+complicated; if the value of the @code{extend} field is not\n+@code{UNKNOWN}, the value of the induction variable in the i-th\n+iteration is\n+\n+@smallexample\n+delta + mult * extend_@{extend_mode@} (subreg_@{mode@} (base + i * step)),\n+@end smallexample\n+\n+with the following exception:  if @code{first_special} is true, then the\n+value in the first iteration (when @code{i} is zero) is @code{delta +\n+mult * base}.  However, if @code{extend} is equal to @code{UNKNOWN},\n+then @code{first_special} must be false, @code{delta} 0, @code{mult} 1\n+and the value in the i-th iteration is\n+\n+@smallexample\n+subreg_@{mode@} (base + i * step)\n+@end smallexample\n+\n+The function @code{get_iv_value} can be used to perform these\n+calculations.\n+\n+@node Number of iterations\n+@section Number of iterations analysis\n+@cindex Number of iterations analysis\n+\n+Both on GIMPLE and on RTL, there are functions available to determine\n+the number of iterations of a loop, with a similar interface.  In many\n+cases, it is not possible to determine number of iterations\n+unconditionally -- the determined number is correct only if some\n+assumptions are satisfied.  The analysis tries to verify these\n+conditions using the information contained in the program; if it fails,\n+the conditions are returned together with the result.  The following\n+information and conditions are provided by the analysis:\n+\n+@itemize\n+@item @code{assumptions}: If this condition is false, the rest of\n+the information is invalid.\n+@item @code{noloop_assumptions} on RTL, @code{may_be_zero} on GIMPLE: If\n+this condition is true, the loop exits in the first iteration.\n+@item @code{infinite}: If this condition is true, the loop is infinite.\n+This condition is only available on RTL.  On GIMPLE, conditions for\n+finiteness of the loop are included in @code{assumptions}.\n+@item @code{niter_expr} on RTL, @code{niter} on GIMPLE: The expression\n+that gives number of iterations.  The number of iterations is defined as\n+the number of executions of the loop latch.\n+@end itemize\n+\n+Both on GIMPLE and on RTL, it necessary for the induction variable\n+analysis framework to be initialized (SCEV on GIMPLE, loop-iv on RTL).\n+On GIMPLE, the results are stored to @code{struct tree_niter_desc}\n+structure.  Number of iterations before the loop is exited through a\n+given exit can be determined using @code{number_of_iterations_exit}\n+function.  On RTL, the results are returned in @code{struct niter_desc}\n+structure.  The corresponding function is named\n+@code{check_simple_exit}.  There are also functions that pass through\n+all the exits of a loop and try to find one with easy to determine\n+number of iterations -- @code{find_loop_niter} on GIMPLE and\n+@code{find_simple_exit} on RTL.  Finally, there are functions that\n+provide the same information, but additionally cache it, so that\n+repeated calls to number of iterations are not so costly --\n+@code{number_of_iterations_in_loop} on GIMPLE and\n+@code{get_simple_loop_desc} on RTL.\n+\n+Note that some of these functions may behave slightly differently than\n+others -- some of them return only the expression for the number of\n+iterations, and fail if there are some assumptions.  The function\n+@code{number_of_iterations_in_loop} works only for single-exit loops,\n+and it returns the value for number of iterations higher by one with\n+respect to all other functions (i.e., it returns number of executions of\n+the exit statement, not of the loop latch).\n+\n+@node Dependency analysis\n+@section Data Dependency Analysis\n+@cindex Data Dependency Analysis\n+\n+The code for the data dependence analysis can be found in\n+@file{tree-data-ref.c} and its interface and data structures are\n+described in @file{tree-data-ref.h}.  The function that computes the\n+data dependences for all the array and pointer references for a given\n+loop is @code{compute_data_dependences_for_loop}.  This function is\n+currently used by the linear loop transform and the vectorization\n+passes.  Before calling this function, one has to allocate two vectors:\n+a first vector will contain the set of data references that are\n+contained in the analyzed loop body, and the second vector will contain\n+the dependence relations between the data references.  Thus if the\n+vector of data references is of size @code{n}, the vector containing the\n+dependence relations will contain @code{n*n} elements.  However if the\n+analyzed loop contains side effects, such as calls that potentially can\n+interfere with the data references in the current analyzed loop, the\n+analysis stops while scanning the loop body for data references, and\n+inserts a single @code{chrec_dont_know} in the dependence relation\n+array.\n+\n+The data references are discovered in a particular order during the\n+scanning of the loop body: the loop body is analyzed in execution order,\n+and the data references of each statement are pushed at the end of the\n+data reference array.  Two data references syntactically occur in the\n+program in the same order as in the array of data references.  This\n+syntactic order is important in some classical data dependence tests,\n+and mapping this order to the elements of this array avoids costly\n+queries to the loop body representation.\n+\n+The structure describing the relation between two data references is\n+@code{data_dependence_relation} and the shorter name for a pointer to\n+such a structure is @code{ddr_p}.  This structure contains:\n+\n+@itemize\n+@item a pointer to each data reference,\n+@item a tree node @code{are_dependent} that is set to @code{chrec_known}\n+if the analysis has proved that there is no dependence between these two\n+data references, @code{chrec_dont_know} if the analysis was not able to\n+determine any useful result and potentially there could exist a\n+dependence between these data references, and @code{are_dependent} is\n+set to @code{NULL_TREE} if there exist a dependence relation between the\n+data references, and the description of this dependence relation is\n+given in the @code{subscripts}, @code{dir_vects}, and @code{dist_vects}\n+arrays,\n+@item a boolean that determines whether the dependence relation can be\n+represented by a classical distance vector, \n+@item an array @code{subscripts} that contains a description of each\n+subscript of the data references.  Given two array accesses a\n+subscript is the tuple composed of the access functions for a given\n+dimension.  For example, given @code{A[f1][f2][f3]} and\n+@code{B[g1][g2][g3]}, there are three subscripts: @code{(f1, g1), (f2,\n+g2), (f3, g3)}.\n+@item two arrays @code{dir_vects} and @code{dist_vects} that contain\n+classical representations of the data dependences under the form of\n+direction and distance dependence vectors,\n+@item an array of loops @code{loop_nest} that contains the loops to\n+which the distance and direction vectors refer to.\n+@end itemize\n+\n+Several functions for pretty printing the information extracted by the\n+data dependence analysis are available: @code{dump_ddrs} prints with a\n+maximum verbosity the details of a data dependence relations array,\n+@code{dump_dist_dir_vectors} prints only the classical distance and\n+direction vectors for a data dependence relations array, and\n+@code{dump_data_references} prints the details of the data references\n+contained in a data reference array.\n+\n+@node Lambda\n+@section Linear loop transformations framework\n+@cindex Linear loop transformations framework\n+\n+Lambda is a framework that allows transformations of loops using\n+non-singular matrix based transformations of the iteration space and\n+loop bounds. This allows compositions of skewing, scaling, interchange,\n+and reversal transformations.  These transformations are often used to\n+improve cache behavior or remove inner loop dependencies to allow\n+parallelization and vectorization to take place.\n+\n+To perform these transformations, Lambda requires that the loopnest be\n+converted into a internal form that can be matrix transformed easily.\n+To do this conversion, the function\n+@code{gcc_loopnest_to_lambda_loopnest} is provided.  If the loop cannot\n+be transformed using lambda, this function will return NULL.\n+\n+Once a @code{lambda_loopnest} is obtained from the conversion function,\n+it can be transformed by using @code{lambda_loopnest_transform}, which\n+takes a transformation matrix to apply.  Note that it is up to the\n+caller to verify that the transformation matrix is legal to apply to the\n+loop (dependence respecting, etc).  Lambda simply applies whatever\n+matrix it is told to provide.  It can be extended to make legal matrices\n+out of any non-singular matrix, but this is not currently implemented.\n+Legality of a matrix for a given loopnest can be verified using\n+@code{lambda_transform_legal_p}.\n+\n+Given a transformed loopnest, conversion back into gcc IR is done by\n+@code{lambda_loopnest_to_gcc_loopnest}.  This function will modify the\n+loops so that they match the transformed loopnest.\n+"}]}