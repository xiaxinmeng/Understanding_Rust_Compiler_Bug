{"sha": "fe2d45c77f86ebbed8a1ef8c6c59e61daace584f", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZmUyZDQ1Yzc3Zjg2ZWJiZWQ4YTFlZjhjNmM1OWU2MWRhYWNlNTg0Zg==", "commit": {"author": {"name": "Fariborz Jahanian", "email": "fjahanian@apple.com", "date": "2005-05-16T15:24:09Z"}, "committer": {"name": "Fariborz Jahanian", "email": "fjahanian@gcc.gnu.org", "date": "2005-05-16T15:24:09Z"}, "message": "Fix vec_merge patterns for Altivec ppc.\n\nOKed by Geoff Keating.\n\nFrom-SVN: r99779", "tree": {"sha": "202d426bd636d7b2a88c36bb80f40c814484c971", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/202d426bd636d7b2a88c36bb80f40c814484c971"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/fe2d45c77f86ebbed8a1ef8c6c59e61daace584f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fe2d45c77f86ebbed8a1ef8c6c59e61daace584f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fe2d45c77f86ebbed8a1ef8c6c59e61daace584f", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fe2d45c77f86ebbed8a1ef8c6c59e61daace584f/comments", "author": null, "committer": null, "parents": [{"sha": "7b8a92e190511b7ed41567a9df7024ae329fda7e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7b8a92e190511b7ed41567a9df7024ae329fda7e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7b8a92e190511b7ed41567a9df7024ae329fda7e"}], "stats": {"total": 729, "additions": 696, "deletions": 33}, "files": [{"sha": "e8d0a176e3d0e69296c79f364e9bfc314528a522", "filename": "gcc/ChangeLog", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fe2d45c77f86ebbed8a1ef8c6c59e61daace584f/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fe2d45c77f86ebbed8a1ef8c6c59e61daace584f/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=fe2d45c77f86ebbed8a1ef8c6c59e61daace584f", "patch": "@@ -1,3 +1,10 @@\n+2005-05-16  Fariborz Jahanian <fjahanian@apple.com>\n+\n+\t* config/rs6000/altivec.md (altivec_vmrghb, altivec_vmrghh, \n+\taltivec_vmrghw, altivec_vmrglb, altivec_vmrglh, altivec_vmrglw):\n+\tNew values for vec_select definitions and bitmask for element\n+\tselection.\n+\t \n 2005-05-16  Kazu Hirata  <kazu@cs.umass.edu>\n \n \t* dwarf2out.c (used_rtx_varray): Rename to used_rtx_array."}, {"sha": "418228d7bb3708d1ce6a3b4957daa29e5b0de183", "filename": "gcc/config/rs6000/altivec.md", "status": "modified", "additions": 89, "deletions": 33, "changes": 122, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fe2d45c77f86ebbed8a1ef8c6c59e61daace584f/gcc%2Fconfig%2Frs6000%2Faltivec.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fe2d45c77f86ebbed8a1ef8c6c59e61daace584f/gcc%2Fconfig%2Frs6000%2Faltivec.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Faltivec.md?ref=fe2d45c77f86ebbed8a1ef8c6c59e61daace584f", "patch": "@@ -673,109 +673,165 @@\n (define_insn \"altivec_vmrghb\"\n   [(set (match_operand:V16QI 0 \"register_operand\" \"=v\")\n         (vec_merge:V16QI (vec_select:V16QI (match_operand:V16QI 1 \"register_operand\" \"v\")\n-\t\t\t\t\t   (parallel [(const_int 8)\n+\t\t\t\t\t   (parallel [(const_int 0)\n+\t\t\t\t\t   \t      (const_int 8)\n+\t\t\t\t\t   \t      (const_int 1)\n \t\t\t\t\t   \t      (const_int 9)\n+\t\t\t\t\t   \t      (const_int 2)\n \t\t\t\t\t   \t      (const_int 10)\n-\t\t\t\t\t   \t      (const_int 11)\n+\t\t\t\t\t\t      (const_int 3)\n+\t\t\t\t\t\t      (const_int 11)\n+\t\t\t\t\t   \t      (const_int 4)\n \t\t\t\t\t   \t      (const_int 12)\n+\t\t\t\t\t   \t      (const_int 5)\n \t\t\t\t\t   \t      (const_int 13)\n-\t\t\t\t\t\t      (const_int 14)\n-\t\t\t\t\t\t      (const_int 15)\n+\t\t\t\t\t   \t      (const_int 6)\n+\t\t\t\t\t   \t      (const_int 14)\n+\t\t\t\t\t   \t      (const_int 7)\n+\t\t\t\t\t\t      (const_int 15)]))\n+                        (vec_select:V16QI (match_operand:V16QI 2 \"register_operand\" \"v\")\n+\t\t\t\t\t   (parallel [(const_int 8)\n \t\t\t\t\t   \t      (const_int 0)\n+\t\t\t\t\t   \t      (const_int 9)\n \t\t\t\t\t   \t      (const_int 1)\n+\t\t\t\t\t   \t      (const_int 10)\n \t\t\t\t\t   \t      (const_int 2)\n-\t\t\t\t\t   \t      (const_int 3)\n+\t\t\t\t\t\t      (const_int 11)\n+\t\t\t\t\t\t      (const_int 3)\n+\t\t\t\t\t   \t      (const_int 12)\n \t\t\t\t\t   \t      (const_int 4)\n+\t\t\t\t\t   \t      (const_int 13)\n \t\t\t\t\t   \t      (const_int 5)\n+\t\t\t\t\t   \t      (const_int 14)\n \t\t\t\t\t   \t      (const_int 6)\n+\t\t\t\t\t   \t      (const_int 15)\n \t\t\t\t\t\t      (const_int 7)]))\n-                      (match_operand:V16QI 2 \"register_operand\" \"v\")\n-\t\t      (const_int 255)))]\n+\t\t      (const_int 21845)))]\n   \"TARGET_ALTIVEC\"\n   \"vmrghb %0,%1,%2\"\n   [(set_attr \"type\" \"vecperm\")])\n \n (define_insn \"altivec_vmrghh\"\n   [(set (match_operand:V8HI 0 \"register_operand\" \"=v\")\n         (vec_merge:V8HI (vec_select:V8HI (match_operand:V8HI 1 \"register_operand\" \"v\")\n-\t\t\t\t\t   (parallel [(const_int 4)\n+\t\t\t\t\t   (parallel [(const_int 0)\n+\t\t\t\t\t   \t      (const_int 4)\n+\t\t\t\t\t   \t      (const_int 1)\n \t\t\t\t\t   \t      (const_int 5)\n+\t\t\t\t\t   \t      (const_int 2)\n \t\t\t\t\t   \t      (const_int 6)\n-\t\t\t\t\t   \t      (const_int 7)\n+\t\t\t\t\t   \t      (const_int 3)\n+\t\t\t\t\t   \t      (const_int 7)]))\n+                        (vec_select:V8HI (match_operand:V8HI 2 \"register_operand\" \"v\")\n+\t\t\t\t\t   (parallel [(const_int 4)\n \t\t\t\t\t   \t      (const_int 0)\n+\t\t\t\t\t   \t      (const_int 5)\n \t\t\t\t\t   \t      (const_int 1)\n+\t\t\t\t\t   \t      (const_int 6)\n \t\t\t\t\t   \t      (const_int 2)\n+\t\t\t\t\t   \t      (const_int 7)\n \t\t\t\t\t   \t      (const_int 3)]))\n-                      (match_operand:V8HI 2 \"register_operand\" \"v\")\n-\t\t      (const_int 15)))]\n+\t\t      (const_int 85)))]\n   \"TARGET_ALTIVEC\"\n   \"vmrghh %0,%1,%2\"\n   [(set_attr \"type\" \"vecperm\")])\n \n (define_insn \"altivec_vmrghw\"\n   [(set (match_operand:V4SI 0 \"register_operand\" \"=v\")\n         (vec_merge:V4SI (vec_select:V4SI (match_operand:V4SI 1 \"register_operand\" \"v\")\n+\t\t\t\t\t (parallel [(const_int 0)\n+\t\t\t\t\t \t    (const_int 2)\n+\t\t\t\t\t\t    (const_int 1)\n+\t\t\t\t\t\t    (const_int 3)]))\n+                        (vec_select:V4SI (match_operand:V4SI 2 \"register_operand\" \"v\")\n \t\t\t\t\t (parallel [(const_int 2)\n-\t\t\t\t\t \t    (const_int 3)\n-\t\t\t\t\t\t    (const_int 0)\n+\t\t\t\t\t \t    (const_int 0)\n+\t\t\t\t\t\t    (const_int 3)\n \t\t\t\t\t\t    (const_int 1)]))\n-                      (match_operand:V4SI 2 \"register_operand\" \"v\")\n-\t\t      (const_int 12)))]\n+\t\t      (const_int 5)))]\n   \"TARGET_ALTIVEC\"\n   \"vmrghw %0,%1,%2\"\n   [(set_attr \"type\" \"vecperm\")])\n \n (define_insn \"altivec_vmrglb\"\n   [(set (match_operand:V16QI 0 \"register_operand\" \"=v\")\n-        (vec_merge:V16QI (vec_select:V16QI (match_operand:V16QI 2 \"register_operand\" \"v\")\n-\t\t\t\t\t   (parallel [(const_int 0)\n+        (vec_merge:V16QI (vec_select:V16QI (match_operand:V16QI 1 \"register_operand\" \"v\")\n+\t\t\t\t\t   (parallel [(const_int 8)\n+\t\t\t\t\t   \t      (const_int 0)\n+\t\t\t\t\t   \t      (const_int 9)\n \t\t\t\t\t   \t      (const_int 1)\n+\t\t\t\t\t   \t      (const_int 10)\n \t\t\t\t\t   \t      (const_int 2)\n-\t\t\t\t\t   \t      (const_int 3)\n+\t\t\t\t\t\t      (const_int 11)\n+\t\t\t\t\t\t      (const_int 3)\n+\t\t\t\t\t   \t      (const_int 12)\n \t\t\t\t\t   \t      (const_int 4)\n+\t\t\t\t\t   \t      (const_int 13)\n \t\t\t\t\t   \t      (const_int 5)\n-\t\t\t\t\t\t      (const_int 6)\n-\t\t\t\t\t\t      (const_int 7)\n+\t\t\t\t\t   \t      (const_int 14)\n+\t\t\t\t\t   \t      (const_int 6)\n+\t\t\t\t\t   \t      (const_int 15)\n+\t\t\t\t\t\t      (const_int 7)]))\n+                      (vec_select:V16QI (match_operand:V16QI 2 \"register_operand\" \"v\")\n+\t\t\t\t\t   (parallel [(const_int 0)\n \t\t\t\t\t   \t      (const_int 8)\n+\t\t\t\t\t   \t      (const_int 1)\n \t\t\t\t\t   \t      (const_int 9)\n+\t\t\t\t\t   \t      (const_int 2)\n \t\t\t\t\t   \t      (const_int 10)\n-\t\t\t\t\t   \t      (const_int 11)\n+\t\t\t\t\t\t      (const_int 3)\n+\t\t\t\t\t\t      (const_int 11)\n+\t\t\t\t\t   \t      (const_int 4)\n \t\t\t\t\t   \t      (const_int 12)\n+\t\t\t\t\t   \t      (const_int 5)\n \t\t\t\t\t   \t      (const_int 13)\n+\t\t\t\t\t   \t      (const_int 6)\n \t\t\t\t\t   \t      (const_int 14)\n+\t\t\t\t\t   \t      (const_int 7)\n \t\t\t\t\t\t      (const_int 15)]))\n-                      (match_operand:V16QI 1 \"register_operand\" \"v\")\n-\t\t      (const_int 255)))]\n+\t\t      (const_int 21845)))]\n   \"TARGET_ALTIVEC\"\n   \"vmrglb %0,%1,%2\"\n   [(set_attr \"type\" \"vecperm\")])\n \n (define_insn \"altivec_vmrglh\"\n   [(set (match_operand:V8HI 0 \"register_operand\" \"=v\")\n-        (vec_merge:V8HI (vec_select:V8HI (match_operand:V8HI 2 \"register_operand\" \"v\")\n-\t\t\t\t\t   (parallel [(const_int 0)\n+        (vec_merge:V8HI (vec_select:V8HI (match_operand:V8HI 1 \"register_operand\" \"v\")\n+\t\t\t\t\t   (parallel [(const_int 4)\n+\t\t\t\t\t   \t      (const_int 0)\n+\t\t\t\t\t   \t      (const_int 5)\n \t\t\t\t\t   \t      (const_int 1)\n+\t\t\t\t\t   \t      (const_int 6)\n \t\t\t\t\t   \t      (const_int 2)\n-\t\t\t\t\t   \t      (const_int 3)\n+\t\t\t\t\t   \t      (const_int 7)\n+\t\t\t\t\t   \t      (const_int 3)]))\n+                        (vec_select:V8HI (match_operand:V8HI 2 \"register_operand\" \"v\")\n+\t\t\t\t\t   (parallel [(const_int 0)\n \t\t\t\t\t   \t      (const_int 4)\n+\t\t\t\t\t   \t      (const_int 1)\n \t\t\t\t\t   \t      (const_int 5)\n+\t\t\t\t\t   \t      (const_int 2)\n \t\t\t\t\t   \t      (const_int 6)\n+\t\t\t\t\t   \t      (const_int 3)\n \t\t\t\t\t   \t      (const_int 7)]))\n-                      (match_operand:V8HI 1 \"register_operand\" \"v\")\n-\t\t      (const_int 15)))]\n+\t\t      (const_int 85)))]\n   \"TARGET_ALTIVEC\"\n   \"vmrglh %0,%1,%2\"\n   [(set_attr \"type\" \"vecperm\")])\n \n (define_insn \"altivec_vmrglw\"\n   [(set (match_operand:V4SI 0 \"register_operand\" \"=v\")\n-        (vec_merge:V4SI (vec_select:V4SI (match_operand:V4SI 2 \"register_operand\" \"v\")\n+        (vec_merge:V4SI (vec_select:V4SI (match_operand:V4SI 1 \"register_operand\" \"v\")\n+\t\t\t\t\t (parallel [(const_int 2)\n+\t\t\t\t\t \t    (const_int 0)\n+\t\t\t\t\t\t    (const_int 3)\n+\t\t\t\t\t\t    (const_int 1)]))\n+                        (vec_select:V4SI (match_operand:V4SI 2 \"register_operand\" \"v\")\n \t\t\t\t\t (parallel [(const_int 0)\n-\t\t\t\t\t \t    (const_int 1)\n-\t\t\t\t\t\t    (const_int 2)\n+\t\t\t\t\t \t    (const_int 2)\n+\t\t\t\t\t\t    (const_int 1)\n \t\t\t\t\t\t    (const_int 3)]))\n-                      (match_operand:V4SI 1 \"register_operand\" \"v\")\n-\t\t      (const_int 12)))]\n+\t\t      (const_int 5)))]\n   \"TARGET_ALTIVEC\"\n   \"vmrglw %0,%1,%2\"\n   [(set_attr \"type\" \"vecperm\")])"}, {"sha": "9eef9847a82999a7caf3fea0bb0819302f3bbeca", "filename": "gcc/testsuite/gcc.dg/ppc-vec-merge.c", "status": "added", "additions": 600, "deletions": 0, "changes": 600, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fe2d45c77f86ebbed8a1ef8c6c59e61daace584f/gcc%2Ftestsuite%2Fgcc.dg%2Fppc-vec-merge.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fe2d45c77f86ebbed8a1ef8c6c59e61daace584f/gcc%2Ftestsuite%2Fgcc.dg%2Fppc-vec-merge.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fppc-vec-merge.c?ref=fe2d45c77f86ebbed8a1ef8c6c59e61daace584f", "patch": "@@ -0,0 +1,600 @@\n+/* { dg-do run { target powerpc*-*-* } } */\n+/* { dg-options \"-faltivec -O2\" } */\n+\n+int printf(const char * , ...);\n+extern void abort();\n+\n+void foo(char *bS, char *bS_edge, int field_MBAFF, int top){\n+  char intra[16]       __attribute__ ((aligned(16)));\n+  signed short mv_const[8] __attribute__((aligned(16)));\n+  \n+  vector signed short v_three, v_ref_mask00, v_ref_mask01, v_vec_maskv, v_vec_maskh;\n+  vector unsigned char v_permv, v_permh, v_bS, v_bSh, v_bSv, v_cbp_maskv, v_cbp_maskvn, v_cbp_maskh, v_cbp_maskhn, v_intra_maskh, v_intra_maskv, v_intra_maskhn, v_intra_maskvn;\n+  vector unsigned char tmp7, tmp8, tmp9, tmp10, v_c1, v_cbp1, v_cbp2, v_pocl, v_poch;\n+  vector signed short v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10, v11, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5, tmp6;\n+  vector signed short idx0;\n+  vector signed short tmp00, tmp01, tmp02, tmp03;\n+  vector unsigned char v_zero   = (vector unsigned char) {'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p'};\n+  v_three  = (vector signed short) vec_ld (0, (vector signed short *) mv_const);\n+\n+  vector unsigned char v_coef_mask = vec_ld(0, (vector unsigned char *)mv_const);\n+  vector unsigned char v_coef_mask_hi = vec_splat(v_coef_mask, 0);\n+  vector unsigned char v_coef_mask_lo = vec_splat(v_coef_mask, 1);\n+  v_coef_mask = vec_sld(v_coef_mask_hi, v_coef_mask_lo, 8);\n+  vector unsigned char v_bit_mask = vec_sub(vec_splat_u8(7), vec_lvsl(0, (unsigned char *)0));\n+  v_bit_mask  = vec_sld(vec_sld(v_bit_mask, v_bit_mask, 8), v_bit_mask, 8);\n+  v_bit_mask  = vec_sl(vec_splat_u8(1), v_bit_mask);\n+  tmp5        = (vector signed short) vec_and(v_coef_mask, v_bit_mask);\n+\n+  intra[0] = 1;\n+  tmp8     = vec_ld (0, (vector unsigned char *) intra);\n+  tmp9     = vec_ld (0, (vector unsigned char *) mv_const);\n+  tmp10    = vec_ld (0, (vector unsigned char *) mv_const);\n+  v_permv  = vec_ld (0, (vector unsigned char *) mv_const);\n+  v_permh  = vec_ld (0, (vector unsigned char *) mv_const);\n+  tmp6     = vec_ld (0, (vector signed short *) mv_const);\n+\n+  tmp8     = vec_splat((vector unsigned char) tmp8, 0);\n+  tmp9     = vec_splat((vector unsigned char) tmp9, 12);\n+  tmp10    = vec_splat((vector unsigned char) tmp10, 12);\n+  tmp9     = vec_sld ((vector unsigned char) tmp9,(vector unsigned char) tmp8, 12);\n+  tmp10    = vec_sld ((vector unsigned char) tmp10, (vector unsigned char) tmp8, 12);\n+  v_intra_maskv  = vec_or (tmp9, tmp8);\n+  v_intra_maskh  = vec_or (tmp10, tmp8);\n+  v_intra_maskv  = (vector unsigned char) vec_cmpgt ((vector unsigned char) v_intra_maskv, (vector unsigned char) v_zero);\n+  v_intra_maskh  = (vector unsigned char) vec_cmpgt ((vector unsigned char) v_intra_maskh, (vector unsigned char) v_zero);\n+\n+  tmp9   = vec_lvsl (4 + (top<<2), (unsigned char *) 0x0);\n+  v_cbp1 = vec_perm ((vector unsigned char) tmp6, (vector unsigned char) tmp6, tmp9);\n+  v_cbp2 = (vector unsigned char) vec_perm ((vector unsigned char) tmp5, (vector unsigned char) tmp5, (vector unsigned char) v_permv);\n+  v_cbp1 = (vector unsigned char) vec_sld  ((vector unsigned char) v_cbp1,(vector unsigned char) v_cbp2, 12);\n+  v_cbp_maskv = vec_or (v_cbp1, v_cbp2);\n+\n+  tmp9   = vec_lvsl (12 + (top<<2), (unsigned char *) 0x0);\n+  v_cbp1 = vec_perm ((vector unsigned char) tmp6, (vector unsigned char) tmp6, tmp9);\n+  v_cbp2 = (vector unsigned char) vec_perm ((vector unsigned char) tmp5, (vector unsigned char) tmp5, (vector unsigned char) v_permh);\n+  v_cbp1 = (vector unsigned char) vec_sld  ((vector unsigned char) v_cbp1,(vector unsigned char) v_cbp2, 12);\n+  v_cbp_maskh = vec_or (v_cbp1, v_cbp2);\n+\n+  v_cbp_maskv = (vector unsigned char) vec_cmpgt ((vector unsigned char) v_cbp_maskv, (vector unsigned char) v_zero);\n+  v_cbp_maskh = (vector unsigned char) vec_cmpgt ((vector unsigned char) v_cbp_maskh, (vector unsigned char) v_zero);\n+\n+  intra[0]  =0; \n+  intra[1]  =1;\n+  intra[2]  =2;\n+  intra[3]  =3;\n+  intra[4]  =4;\n+  intra[5]  = 5;\n+  intra[6]  =6;\n+  intra[7]  =7;\n+  intra[8]  =8;\n+  intra[9]  =9;\n+  intra[10] =9;\n+  intra[11] =9;\n+  intra[12] = 0xff;\n+\n+  idx0   = vec_ld (0, (signed short *) intra);\n+  \n+  v_c1   = (vector unsigned char)  {'1','2','3','4','5','6','7','8','1','2','3','4','5','6','7','8'};\n+\n+  if (field_MBAFF){\n+    v0   = (vector signed short) vec_and ((vector unsigned char) idx0, v_c1);\n+    idx0 = (vector signed short) vec_sra ((vector unsigned char) idx0, v_c1);\n+\n+    v1   = vec_sld (v0, v0, 15);\n+    v1   = (vector signed short) vec_pack (v1, v0);\n+    \n+    v2   = vec_sld (v1, v1, 2);\n+    v3   = vec_sld (v1, v1, 10);\n+    \n+    v4   = (vector signed short) vec_cmpeq ((vector signed char) v1, (vector signed char) v2);\n+    v5   = (vector signed short) vec_cmpeq ((vector signed char) v1, (vector signed char) v3);\n+    v6   = (vector signed short) vec_cmpeq ((vector signed char) v2, (vector signed char) v3);\n+  }\n+  else  {\n+    v4 = v5 = v6 = vec_nor (v_zero, v_zero);\n+  }\n+\n+  tmp1   = (vector signed short) vec_sl ((vector unsigned char) idx0, v_c1);\n+  v_c1   = vec_mergeh ((vector unsigned char) v_zero, v_c1);\n+  tmp1   = (vector signed short) vec_add (tmp1, (vector signed short) v_c1); \n+\n+  v_pocl = vec_ld (0, (vector unsigned char *) mv_const);\n+  v_poch = vec_ld (0, (vector unsigned char *) mv_const);\n+  tmp2   = (vector signed short) vec_perm (v_pocl, v_poch, (vector unsigned char) tmp1);      \n+\n+  v_pocl = vec_ld (0,  (vector unsigned char *) mv_const);\n+  v_poch = vec_ld (16, (vector unsigned char *) mv_const);\n+  tmp1   = (vector signed short) vec_perm (v_pocl, v_poch, (vector unsigned char) tmp1);\n+  tmp1   = vec_sel (tmp1, tmp2, (vector unsigned short) {0xffff,0xffff,0,0,0,0,0,0});\n+\n+  tmp3   = (vector signed short) vec_splat ((vector unsigned char) idx0, 12);\n+  v_c1   = (vector unsigned char) vec_nor (v_zero, v_zero);\n+  tmp0   = (vector signed short) vec_cmpeq ((vector signed char) idx0, (vector signed char) v_c1);\n+  tmp1   = vec_sel (tmp1, (vector signed short) tmp3, (vector unsigned short) tmp0);\n+\n+  tmp2   = vec_sld (tmp1, tmp1, 15);\n+  tmp1   = (vector signed short) vec_pack (tmp2, tmp1);\n+  \n+  tmp2   = vec_sld (tmp1, tmp1, 2);\n+  tmp3   = vec_sld (tmp1, tmp1, 10);\n+\n+  tmp0   = (vector signed short) vec_cmpeq ((vector signed char) tmp1, (vector signed char) tmp2);\n+  tmp4   = (vector signed short) vec_cmpeq ((vector signed char) tmp1, (vector signed char) tmp3);\n+  tmp1   = (vector signed short) vec_cmpeq ((vector signed char) tmp2, (vector signed char) tmp3);\n+  tmp0   = vec_and (tmp0, v4);\n+  tmp4   = vec_and (tmp4, v5);\n+  tmp1   = vec_and (tmp1, v6);\n+  tmp2   = vec_sld ((vector signed short) tmp0, (vector signed short) tmp0, 8);\n+  tmp3   = vec_sld ((vector signed short) tmp4, (vector signed short) tmp4, 8);\n+  tmp5   = vec_sld ((vector signed short) tmp1, (vector signed short) tmp1, 8);\n+  tmp0   = vec_and (tmp0, tmp2);\n+  tmp4   = vec_and (tmp4, tmp3);\n+  tmp1   = vec_and (tmp1, tmp5);\n+  v_ref_mask00 = vec_mergeh ((vector signed short) tmp0, (vector signed short) v_c1);\n+  v_ref_mask01 = vec_mergeh ((vector signed short) tmp4, (vector signed short) tmp1);\n+  v_ref_mask00 = (vector signed short) vec_mergeh ((vector unsigned char) v_ref_mask00, (vector unsigned char) v_ref_mask00);\n+  v_ref_mask01 = (vector signed short) vec_mergeh ((vector unsigned char) v_ref_mask01, (vector unsigned char) v_ref_mask01);\n+  \n+  v0     = vec_ld (0,  (vector signed short *) mv_const);\n+  v1     = vec_ld (16, (vector signed short *) mv_const);\n+  v4     = vec_ld (64, (vector signed short *) mv_const);\n+  v5     = vec_ld (80, (vector signed short *) mv_const);\n+  v8     = vec_ld (0,  (vector signed short *) mv_const);\n+  v9     = vec_ld (16, (vector signed short *) mv_const);\n+\n+  tmp0   = (vector signed short) vec_perm ((vector unsigned char) v8, \n+\t\t(vector unsigned char) v8, (vector unsigned char) {0,1,2,3,8,9,10,11,4,5,6,7,12,13,14,15});\n+  tmp1   = (vector signed short) vec_mergeh ((vector signed int) v0, (vector signed int) v1);\n+  tmp2   = vec_sld (tmp1, tmp1, 8);\n+  tmp3   = vec_sub (vec_max (tmp0, tmp1), vec_min (tmp0, tmp1));\n+  tmp4   = vec_sub (vec_max (tmp0, tmp2), vec_min (tmp0, tmp2));\n+  tmp3   = (vector signed short) vec_cmpgt (tmp3, v_three); \n+  tmp4   = (vector signed short) vec_cmpgt (tmp4, v_three);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  tmp0   = (vector signed short) vec_perm ((vector unsigned char) v9, (vector unsigned char) v9, \n+\t\t\t(vector unsigned char) {0,1,2,3,8,9,10,11,4,5,6,7,12,13,14,15});\n+  tmp1   = (vector signed short) vec_mergeh ((vector signed int) v4, (vector signed int) v5);\n+  tmp2   = vec_sld (tmp1, tmp1, 8);\n+  tmp5   = vec_sub (vec_max (tmp0, tmp1), vec_min (tmp0, tmp1));\n+  tmp6   = vec_sub (vec_max (tmp0, tmp2), vec_min (tmp0, tmp2));\n+  tmp5   = (vector signed short) vec_cmpgt (tmp5, v_three); \n+  tmp6   = (vector signed short) vec_cmpgt (tmp6, v_three);\n+  tmp0   = vec_sld (tmp5, tmp5, 14);\n+  tmp1   = vec_sld (tmp6, tmp6, 14);\n+  tmp5   = vec_or (tmp0, tmp5);\n+  tmp6   = vec_or (tmp1, tmp6);\n+  \n+  tmp3   = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp5);\n+  tmp4   = (vector signed short) vec_pack ((vector unsigned int) tmp4, (vector unsigned int) tmp6);\n+  tmp5   = vec_sld (tmp3, tmp3, 12);\n+  tmp6   = vec_sld (tmp4, tmp4, 12);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  tmp00  = (vector signed short) vec_pack ((vector unsigned short) tmp3, (vector unsigned short) tmp4);\n+\n+  tmp0   = (vector signed short) vec_mergeh ((vector signed int) v0, (vector signed int) v1);\n+  tmp1   = (vector signed short) vec_mergel ((vector signed int) v0, (vector signed int) v1);\n+  tmp2   = vec_sld (tmp1, tmp1, 8);\n+  tmp3   = vec_sub (vec_max (tmp0, tmp1), vec_min (tmp0, tmp1));\n+  tmp4   = vec_sub (vec_max (tmp0, tmp2), vec_min (tmp0, tmp2));\n+  tmp3   = (vector signed short) vec_cmpgt (tmp3, v_three); \n+  tmp4   = (vector signed short) vec_cmpgt (tmp4, v_three);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  \n+  tmp0   = (vector signed short) vec_mergeh ((vector signed int) v4, (vector signed int) v5);\n+  tmp1   = (vector signed short) vec_mergel ((vector signed int) v4, (vector signed int) v5);\n+  tmp2   = vec_sld (tmp1, tmp1, 8);\n+  tmp5   = vec_sub (vec_max (tmp0, tmp1), vec_min (tmp0, tmp1));\n+  tmp6   = vec_sub (vec_max (tmp0, tmp2), vec_min (tmp0, tmp2));\n+  tmp5   = (vector signed short) vec_cmpgt (tmp5, v_three); \n+  tmp6   = (vector signed short) vec_cmpgt (tmp6, v_three);\n+  tmp0   = vec_sld (tmp5, tmp5, 14);\n+  tmp1   = vec_sld (tmp6, tmp6, 14);\n+  tmp5   = vec_or (tmp0, tmp5);\n+  tmp6   = vec_or (tmp1, tmp6);\n+  \n+  tmp3   = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp5);\n+  tmp4   = (vector signed short) vec_pack ((vector unsigned int) tmp4, (vector unsigned int) tmp6);\n+  tmp5   = vec_sld (tmp3, tmp3, 12);\n+  tmp6   = vec_sld (tmp4, tmp4, 12);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  tmp01  = (vector signed short) vec_pack ((vector unsigned short) tmp3, (vector unsigned short) tmp4);\n+\n+  v2     = vec_ld (32, (vector signed short *) mv_const);\n+  v3     = vec_ld (48, (vector signed short *) mv_const);\n+  v6     = vec_ld (96, (vector signed short *) mv_const);\n+  v7     = vec_ld (112,(vector signed short *) mv_const);\n+\n+  tmp0   = (vector signed short) vec_mergel ((vector signed int) v0, (vector signed int) v1);\n+  tmp1   = (vector signed short) vec_mergeh ((vector signed int) v2, (vector signed int) v3);\n+  tmp2   = vec_sld (tmp1, tmp1, 8);\n+  tmp3   = vec_sub (vec_max (tmp0, tmp1), vec_min (tmp0, tmp1));\n+  tmp4   = vec_sub (vec_max (tmp0, tmp2), vec_min (tmp0, tmp2));\n+  tmp3   = (vector signed short) vec_cmpgt (tmp3, v_three); \n+  tmp4   = (vector signed short) vec_cmpgt (tmp4, v_three);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  \n+  tmp0   = (vector signed short) vec_mergel ((vector signed int) v4, (vector signed int) v5);\n+  tmp1   = (vector signed short) vec_mergeh ((vector signed int) v6, (vector signed int) v7);\n+  tmp2   = vec_sld (tmp1, tmp1, 8);\n+  tmp5   = vec_sub (vec_max (tmp0, tmp1), vec_min (tmp0, tmp1));\n+  tmp6   = vec_sub (vec_max (tmp0, tmp2), vec_min (tmp0, tmp2));\n+  tmp5   = (vector signed short) vec_cmpgt (tmp5, v_three); \n+  tmp6   = (vector signed short) vec_cmpgt (tmp6, v_three);\n+  tmp0   = vec_sld (tmp5, tmp5, 14);\n+  tmp1   = vec_sld (tmp6, tmp6, 14);\n+  tmp5   = vec_or (tmp0, tmp5);\n+  tmp6   = vec_or (tmp1, tmp6);\n+  \n+  tmp3   = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp5);\n+  tmp4   = (vector signed short) vec_pack ((vector unsigned int) tmp4, (vector unsigned int) tmp6);\n+  tmp5   = vec_sld (tmp3, tmp3, 12);\n+  tmp6   = vec_sld (tmp4, tmp4, 12);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  tmp02  = (vector signed short) vec_pack ((vector unsigned short) tmp3, (vector unsigned short) tmp4);\n+\n+  tmp0   = (vector signed short) vec_mergeh ((vector signed int) v2, (vector signed int) v3);\n+  tmp1   = (vector signed short) vec_mergel ((vector signed int) v2, (vector signed int) v3);\n+  tmp2   = vec_sld (tmp1, tmp1, 8);\n+  tmp3   = vec_sub (vec_max (tmp0, tmp1), vec_min (tmp0, tmp1));\n+  tmp4   = vec_sub (vec_max (tmp0, tmp2), vec_min (tmp0, tmp2));\n+  tmp3   = (vector signed short) vec_cmpgt (tmp3, v_three); \n+  tmp4   = (vector signed short) vec_cmpgt (tmp4, v_three);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  \n+  tmp0   = (vector signed short) vec_mergeh ((vector signed int) v6, (vector signed int) v7);\n+  tmp1   = (vector signed short) vec_mergel ((vector signed int) v6, (vector signed int) v7);\n+  tmp2   = vec_sld (tmp1, tmp1, 8);\n+  tmp5   = vec_sub (vec_max (tmp0, tmp1), vec_min (tmp0, tmp1));\n+  tmp6   = vec_sub (vec_max (tmp0, tmp2), vec_min (tmp0, tmp2));\n+  tmp5   = (vector signed short) vec_cmpgt (tmp5, v_three); \n+  tmp6   = (vector signed short) vec_cmpgt (tmp6, v_three);\n+  tmp0   = vec_sld (tmp5, tmp5, 14);\n+  tmp1   = vec_sld (tmp6, tmp6, 14);\n+  tmp5   = vec_or (tmp0, tmp5);\n+  tmp6   = vec_or (tmp1, tmp6);\n+  \n+  tmp3   = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp5);\n+  tmp4   = (vector signed short) vec_pack ((vector unsigned int) tmp4, (vector unsigned int) tmp6);\n+  tmp5   = vec_sld (tmp3, tmp3, 12);\n+  tmp6   = vec_sld (tmp4, tmp4, 12);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  tmp03  = (vector signed short) vec_pack ((vector unsigned short) tmp3, (vector unsigned short) tmp4);\n+\n+  tmp0   = (vector signed short) vec_pack ((vector unsigned int) tmp00, (vector unsigned int) tmp01);\n+  tmp1   = (vector signed short) vec_pack ((vector unsigned int) tmp02, (vector unsigned int) tmp03);\n+  tmp2   = (vector signed short) vec_mergeh ((vector signed int) tmp0, (vector signed int) tmp1);\n+  tmp3   = (vector signed short) vec_mergel ((vector signed int) tmp0, (vector signed int) tmp1);\n+  tmp4   = (vector signed short) vec_mergeh ((vector signed int) tmp2, (vector signed int) tmp3);\n+  tmp5   = (vector signed short) vec_mergel ((vector signed int) tmp2, (vector signed int) tmp3);\n+  tmp4   = vec_and (v_ref_mask00, tmp4);\n+  tmp5   = vec_and (v_ref_mask01, tmp5);\n+\n+  tmp0   = vec_nor (v_ref_mask00, v_ref_mask01);\n+  tmp1   = vec_and (v_ref_mask00, v_ref_mask01);\n+  tmp2   = vec_and (tmp4, tmp5);\n+  tmp2   = vec_and (tmp2, tmp1);\n+  tmp3   = vec_nor (tmp4, tmp5);\n+  tmp3   = vec_nor (tmp3, tmp1);\n+  v_vec_maskv = vec_or (tmp0, tmp2);\n+  v_vec_maskv = vec_or (v_vec_maskv, tmp3);\n+\n+  intra[0]  = 1; \n+  intra[1]  = 1;\n+  intra[2]  = 2;\n+  intra[3]  = 3;\n+  intra[4]  = 2;\n+  intra[5]  = 2;\n+  intra[6]  = 2;\n+  intra[7]  = 1;\n+  intra[8]  = 1;\n+  intra[9]  = 5;\n+  intra[10] = 5;\n+  intra[11] = 5;\n+  \n+  intra[13] = 0;\n+  intra[14] = 0;\n+  intra[15] = 0;\n+\n+  idx0   = vec_ld (0, (signed short *) intra);\n+  \n+  v_c1   = (vector unsigned char)  {'1','2','3','4','5','6','7','8','1','2','3','4','5','6','7','8'};\n+\n+  if (field_MBAFF){\n+    v8   = (vector signed short) vec_and ((vector unsigned char) idx0, v_c1);\n+    idx0 = (vector signed short) vec_sra ((vector unsigned char) idx0, v_c1);\n+\n+    v9   = vec_sld (v8, v8, 15);\n+    v9   = (vector signed short) vec_pack (v9, v8);\n+    \n+    v10  = vec_sld (v9, v9, 2);\n+    v11  = vec_sld (v9, v9, 10);\n+    \n+    v8   = (vector signed short) vec_cmpeq ((vector signed char) v9, (vector signed char) v10);\n+    v9   = (vector signed short) vec_cmpeq ((vector signed char) v9, (vector signed char) v11);\n+    v10  = (vector signed short) vec_cmpeq ((vector signed char) v10, (vector signed char) v11);\n+  }\n+  else  {\n+    v8 = v9 = v10 = vec_nor (v_zero, v_zero);\n+  }\n+\n+  tmp1   = (vector signed short) vec_sl ((vector unsigned char) idx0, v_c1);\n+  \n+if (1){\n+\tint m;\n+\tunsigned char toto2[16] __attribute__((aligned(16)));\n+\t\n+\tprintf(\"vc1\\n\");\n+\tvec_st(v_c1, 0, (unsigned char *) toto2);\n+\tfor (m=0; m<16;m++) {printf(\"%c \", toto2[m]);}\n+\t\n+\tprintf(\"\\nv_zero\\n\");\n+\t\n+\tvec_st (v_zero, 0, (unsigned char *) toto2);\n+\tfor (m=0; m< 16; m++) {printf(\"%c \", toto2[m]);}\n+\tprintf(\"\\n\");\n+}\n+\n+  v_c1   = vec_mergeh ((vector unsigned char) v_zero, v_c1);\n+  tmp1   = (vector signed short) vec_add (tmp1, (vector signed short) v_c1); \n+\n+if (1){\n+\tvector unsigned char vres = \n+        (vector unsigned char){'a','1','b','2','c','3','d','4','e','5','f','6','g','7','h','8'};\n+\tunsigned char toto2[16] __attribute__((aligned(16)));\n+\tint m;\n+\t\n+\tprintf(\"vc1\\n\");\n+\tvec_st(v_c1, 0, (unsigned char *) toto2);\n+\tfor (m=0; m<16;m++) {printf(\"%c \", toto2[m]);}\n+\tprintf(\"\\n\");\n+\tif (!vec_all_eq (vres, v_c1))\n+\t  abort();\n+}\n+\n+  v_pocl = vec_ld (32, (vector unsigned char *) mv_const);\n+  v_poch = vec_ld (48, (vector unsigned char *) mv_const);\n+  tmp2   = (vector signed short) vec_perm (v_pocl, v_poch, (vector unsigned char) tmp1);      \n+\n+  v_pocl = vec_ld (0,  (vector unsigned char *) mv_const);\n+  v_poch = vec_ld (16, (vector unsigned char *) mv_const);\n+\n+  tmp1   = (vector signed short) vec_perm (v_pocl, v_poch, (vector unsigned char) tmp1);\n+ \n+  tmp1   = vec_sel (tmp1, tmp2, (vector unsigned short) {0xffff,0xffff,0,0,0,0,0,0});\n+\n+\n+  tmp3   = (vector signed short) vec_splat ((vector unsigned char) idx0, 12);\n+  v_c1   = (vector unsigned char) vec_nor (v_zero, v_zero);\n+  tmp0   = (vector signed short) vec_cmpeq ((vector signed char) idx0, (vector signed char) v_c1);\n+  tmp1   = vec_sel (tmp1, (vector signed short) tmp3, (vector unsigned short) tmp0);\n+\n+  tmp2   = vec_sld (tmp1, tmp1, 15);\n+  tmp1   = (vector signed short) vec_pack (tmp2, tmp1);\n+  \n+\n+  tmp2   = vec_sld (tmp1, tmp1, 2);\n+  tmp3   = vec_sld (tmp1, tmp1, 10);\n+\n+  tmp0   = (vector signed short) vec_cmpeq ((vector signed char) tmp1, (vector signed char) tmp2);\n+  tmp4   = (vector signed short) vec_cmpeq ((vector signed char) tmp1, (vector signed char) tmp3);\n+  tmp1   = (vector signed short) vec_cmpeq ((vector signed char) tmp2, (vector signed char) tmp3);\n+  tmp0   = vec_and (tmp0, v8);\n+  tmp4   = vec_and (tmp4, v9);\n+  tmp1   = vec_and (tmp1, v10);\n+  tmp2   = vec_sld ((vector signed short) tmp0, (vector signed short) tmp0, 8);\n+  tmp3   = vec_sld ((vector signed short) tmp4, (vector signed short) tmp4, 8);\n+  tmp5   = vec_sld ((vector signed short) tmp1, (vector signed short) tmp1, 8);\n+  tmp0   = vec_and (tmp0, tmp2);\n+  tmp4   = vec_and (tmp4, tmp3);\n+  tmp1   = vec_and (tmp1, tmp5);\n+  v_ref_mask00 = vec_mergeh ((vector signed short) tmp0, (vector signed short) v_c1);\n+  v_ref_mask01 = vec_mergeh ((vector signed short) tmp4, (vector signed short) tmp1);\n+  v_ref_mask00 = (vector signed short) vec_mergeh ((vector unsigned char) v_ref_mask00, (vector unsigned char) v_ref_mask00);\n+  v_ref_mask01 = (vector signed short) vec_mergeh ((vector unsigned char) v_ref_mask01, (vector unsigned char) v_ref_mask01);\n+  \n+\n+  v_permv= vec_ld (0, (vector unsigned char *) mv_const);\n+  v8     = vec_ld (0,  (vector signed short *) mv_const);\n+  v9     = vec_ld (16, (vector signed short *) mv_const);\n+  tmp2   = vec_perm (v0, v0, v_permv);\n+  tmp3   = vec_sub (vec_max (v8, v0), vec_min (v8, v0));\n+  tmp4   = vec_sub (vec_max (v8, tmp2), vec_min (v8, tmp2));\n+  tmp3   = (vector signed short) vec_cmpgt (tmp3, v_three); \n+  tmp4   = (vector signed short) vec_cmpgt (tmp4, v_three);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  \n+  tmp2   = vec_perm (v2, v2, v_permv);\n+  tmp5   = vec_sub (vec_max (v9, v2), vec_min (v9, v2));\n+  tmp6   = vec_sub (vec_max (v9, tmp2), vec_min (v9, tmp2));\n+  tmp5   = (vector signed short) vec_cmpgt (tmp5, v_three); \n+  tmp6   = (vector signed short) vec_cmpgt (tmp6, v_three);\n+  tmp0   = vec_sld (tmp5, tmp5, 14);\n+  tmp1   = vec_sld (tmp6, tmp6, 14);\n+  tmp5   = vec_or (tmp0, tmp5);\n+  tmp6   = vec_or (tmp1, tmp6);\n+  \n+  tmp3   = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp5);\n+  tmp4   = (vector signed short) vec_pack ((vector unsigned int) tmp4, (vector unsigned int) tmp6);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  tmp00  = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp4);\n+\n+  tmp2   = vec_perm (v1, v1, v_permv);\n+  tmp3   = vec_sub (vec_max (v0, v1), vec_min (v0, v1));\n+  tmp4   = vec_sub (vec_max (v0, tmp2), vec_min (v0, tmp2));\n+  tmp3   = (vector signed short) vec_cmpgt (tmp3, v_three); \n+  tmp4   = (vector signed short) vec_cmpgt (tmp4, v_three);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  \n+  tmp2   = vec_perm (v3, v3, v_permv);\n+  tmp5   = vec_sub (vec_max (v2, v3), vec_min (v2, v3));\n+  tmp6   = vec_sub (vec_max (v2, tmp2), vec_min (v2, tmp2));\n+  tmp5   = (vector signed short) vec_cmpgt (tmp5, v_three); \n+  tmp6   = (vector signed short) vec_cmpgt (tmp6, v_three);\n+  tmp0   = vec_sld (tmp5, tmp5, 14);\n+  tmp1   = vec_sld (tmp6, tmp6, 14);\n+  tmp5   = vec_or (tmp0, tmp5);\n+  tmp6   = vec_or (tmp1, tmp6);\n+  \n+  tmp3   = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp5);\n+  tmp4   = (vector signed short) vec_pack ((vector unsigned int) tmp4, (vector unsigned int) tmp6);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  tmp01  = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp4);\n+\n+  tmp2   = vec_perm (v4, v4, v_permv);\n+  tmp3   = vec_sub (vec_max (v1, v4), vec_min (v1, v4));\n+  tmp4   = vec_sub (vec_max (v1, tmp2), vec_min (v1, tmp2));\n+  tmp3   = (vector signed short) vec_cmpgt (tmp3, v_three); \n+  tmp4   = (vector signed short) vec_cmpgt (tmp4, v_three);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  \n+  tmp2   = vec_perm (v6, v6, v_permv);\n+  tmp5   = vec_sub (vec_max (v3, v6), vec_min (v3, v6));\n+  tmp6   = vec_sub (vec_max (v3, tmp2), vec_min (v3, tmp2));\n+  tmp5   = (vector signed short) vec_cmpgt (tmp5, v_three); \n+  tmp6   = (vector signed short) vec_cmpgt (tmp6, v_three);\n+  tmp0   = vec_sld (tmp5, tmp5, 14);\n+  tmp1   = vec_sld (tmp6, tmp6, 14);\n+  tmp5   = vec_or (tmp0, tmp5);\n+  tmp6   = vec_or (tmp1, tmp6);\n+  \n+  tmp3   = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp5);\n+  tmp4   = (vector signed short) vec_pack ((vector unsigned int) tmp4, (vector unsigned int) tmp6);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  tmp02  = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp4);\n+\n+\n+  tmp2   = vec_perm (v5, v5, v_permv);\n+  tmp3   = vec_sub (vec_max (v4, v5), vec_min (v4, v5));\n+  tmp4   = vec_sub (vec_max (v4, tmp2), vec_min (v4, tmp2));\n+  tmp3   = (vector signed short) vec_cmpgt (tmp3, v_three); \n+  tmp4   = (vector signed short) vec_cmpgt (tmp4, v_three);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  \n+  tmp2   = vec_perm (v7, v7, v_permv);\n+  tmp5   = vec_sub (vec_max (v6, v7), vec_min (v6, v7));\n+  tmp6   = vec_sub (vec_max (v6, tmp2), vec_min (v6, tmp2));\n+  tmp5   = (vector signed short) vec_cmpgt (tmp5, v_three); \n+  tmp6   = (vector signed short) vec_cmpgt (tmp6, v_three);\n+  tmp0   = vec_sld (tmp5, tmp5, 14);\n+  tmp1   = vec_sld (tmp6, tmp6, 14);\n+  tmp5   = vec_or (tmp0, tmp5);\n+  tmp6   = vec_or (tmp1, tmp6);\n+  \n+  tmp3   = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp5);\n+  tmp4   = (vector signed short) vec_pack ((vector unsigned int) tmp4, (vector unsigned int) tmp6);\n+  tmp5   = vec_sld (tmp3, tmp3, 14);\n+  tmp6   = vec_sld (tmp4, tmp4, 14);\n+  tmp3   = vec_or (tmp3, tmp5);\n+  tmp4   = vec_or (tmp4, tmp6);\n+  tmp03  = (vector signed short) vec_pack ((vector unsigned int) tmp3, (vector unsigned int) tmp4);\n+\n+  tmp0   = (vector signed short) vec_pack ((vector unsigned short) tmp00, (vector unsigned short) tmp01);\n+  tmp1   = (vector signed short) vec_pack ((vector unsigned short) tmp02, (vector unsigned short) tmp03);\n+  tmp2   = (vector signed short) vec_mergeh ((vector signed int) tmp0, (vector signed int) tmp1);\n+  tmp3   = (vector signed short) vec_mergel ((vector signed int) tmp0, (vector signed int) tmp1);\n+  tmp4   = (vector signed short) vec_mergeh ((vector signed int) tmp2, (vector signed int) tmp3);\n+  tmp5   = (vector signed short) vec_mergel ((vector signed int) tmp2, (vector signed int) tmp3);\n+  tmp4   = vec_and (v_ref_mask00, tmp4);\n+  tmp5   = vec_and (v_ref_mask01, tmp5);\n+\n+  tmp0   = vec_nor (v_ref_mask00, v_ref_mask01);\n+  tmp1   = vec_and (v_ref_mask00, v_ref_mask01);\n+  tmp2   = vec_and (tmp4, tmp5);\n+  tmp2   = vec_and (tmp2, tmp1);\n+  tmp3   = vec_nor (tmp4, tmp5);\n+  tmp3   = vec_nor (tmp3, tmp1);\n+  v_vec_maskh = vec_or (tmp0, tmp2);\n+  v_vec_maskh = vec_or (v_vec_maskh, tmp3);\n+\n+\n+  v_intra_maskvn = vec_nor (v_intra_maskv, v_intra_maskv);\n+  v_intra_maskhn = vec_nor (v_intra_maskh, v_intra_maskh);\n+  v_cbp_maskvn = (vector unsigned char) vec_cmpeq ((vector unsigned char) v_cbp_maskv, (vector unsigned char) v_zero);\n+  v_cbp_maskhn = (vector unsigned char) vec_cmpeq ((vector unsigned char) v_cbp_maskh, (vector unsigned char) v_zero);\n+\n+  v_cbp_maskv  = vec_and (v_cbp_maskv, v_intra_maskvn);\n+  v_cbp_maskh  = vec_and (v_cbp_maskh, v_intra_maskhn);\n+  v_vec_maskv  = vec_and (v_vec_maskv, (vector signed short) v_intra_maskvn);\n+  v_vec_maskv  = vec_and (v_vec_maskv, (vector signed short) v_cbp_maskvn);\n+  v_vec_maskh  = vec_and (v_vec_maskh, (vector signed short) v_intra_maskhn);\n+  v_vec_maskh  = vec_and (v_vec_maskh, (vector signed short) v_cbp_maskhn);\n+\n+  tmp9        = vec_splat_u8(2);\n+  tmp8        = vec_splat_u8(1);\n+  v_bS        = vec_ld (0, (vector unsigned char *) mv_const);\n+  \n+  v_bSv       = vec_and ((vector unsigned char) v_bS, (vector unsigned char)v_intra_maskv);\n+  tmp7        = vec_and ((vector unsigned char)tmp9, (vector unsigned char)v_cbp_maskv);\n+  tmp6        = (vector signed short) vec_and ((vector unsigned char)tmp8, (vector unsigned char)v_vec_maskv);\n+  tmp7        = vec_or  ((vector unsigned char)tmp7, (vector unsigned char)tmp6);\n+  v_bSv       = vec_or  ((vector unsigned char)tmp7, (vector unsigned char)v_bSv);\n+\n+  v_bS        = vec_ld (0, (vector unsigned char *) mv_const);\n+  v_bSh       = vec_and ((vector unsigned char) v_bS, (vector unsigned char)v_intra_maskh);\n+  tmp7        = vec_and ((vector unsigned char)tmp9, (vector unsigned char)v_cbp_maskh);\n+  tmp6        = (vector signed short) vec_and ((vector unsigned char)tmp8, (vector unsigned char)v_vec_maskh);\n+  tmp7        = vec_or  ((vector unsigned char)tmp7, (vector unsigned char)tmp6);\n+  v_bSh       = vec_or  ((vector unsigned char)tmp7, (vector unsigned char)v_bSh);\n+\n+  v_permh     = (vector unsigned char) vec_ld (0 , (vector unsigned char *) mv_const);\n+  v_permv     = (vector unsigned char) vec_ld (0, (vector unsigned char *) mv_const);\n+  v_bSv       = vec_and (v_bSv, v_permv);\n+  v_bSh       = vec_and (v_bSh, v_permh);\n+\n+  vec_st (v_bSv, 0, (unsigned char *) mv_const);\n+  vec_st (v_bSh, 0, (unsigned char *) mv_const);\n+\n+  v_bSv = vec_mergeh (v_bSv, v_bSv);\n+  v_bSv = vec_mergeh (v_bSv, v_bSv);\n+  v_bSh = vec_mergeh (v_bSh, v_bSh);\n+  v_bSh = vec_mergeh (v_bSh, v_bSh);\n+    \n+  vec_st (v_bSv, 0, (vector unsigned char *) mv_const);\n+  vec_st (v_bSh, 0,(vector unsigned char *) mv_const);\n+}\n+\n+\n+int main(int argc, char **argv)\n+{\n+    char toto[32] __attribute__((aligned(16)));\n+    foo(toto, toto, 0, 0);\n+    return 0;\n+}"}]}