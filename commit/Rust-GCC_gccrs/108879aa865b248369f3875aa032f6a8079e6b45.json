{"sha": "108879aa865b248369f3875aa032f6a8079e6b45", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MTA4ODc5YWE4NjViMjQ4MzY5ZjM4NzVhYTAzMmY2YTgwNzllNmI0NQ==", "commit": {"author": {"name": "Jan Hubicka", "email": "jh@suse.cz", "date": "2011-11-18T02:18:28Z"}, "committer": {"name": "Jan Hubicka", "email": "hubicka@gcc.gnu.org", "date": "2011-11-18T02:18:28Z"}, "message": "re PR target/51134 (x86 memset/memcpy expansion is broken)\n\n\n\tPR bootstrap/51134\n\t* i386.c (atom_cost): Fix 32bit memset description.\n\t(expand_set_or_movmem_via_loop_with_iter): Output proper bounds check for epilogue loops.\n\t(expand_movmem_epilogue): Handle epilogues up to size 15 w/o producing byte loop.\n\t(decide_alg): sse_loop is not useable wthen SSE2 is disabled; when not optimizing always\n\tuse rep movsb or lincall; do not produce word sized loops when optimizing memset for\n\tsize (to avoid need for large constants).\n\t(ix86_expand_movmem): Get into sync with ix86_expand_setmem; choose unroll factors\n\tbetter; always do 128bit moves when producing SSE loops; do not produce loopy epilogue\n\twhen size is too small.\n\t(promote_duplicated_reg_to_size): Do not look into desired alignments when\n\tdoing vector expansion.\n\t(ix86_expand_setmem): Track better when promoted value is available; choose unroll factors\n\tmore sanely.; output loopy epilogue only when needed.\n\nFrom-SVN: r181466", "tree": {"sha": "64a73a0ee2ea3895ec62cd12754303cd51ecc8e8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/64a73a0ee2ea3895ec62cd12754303cd51ecc8e8"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/108879aa865b248369f3875aa032f6a8079e6b45", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/108879aa865b248369f3875aa032f6a8079e6b45", "html_url": "https://github.com/Rust-GCC/gccrs/commit/108879aa865b248369f3875aa032f6a8079e6b45", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/108879aa865b248369f3875aa032f6a8079e6b45/comments", "author": null, "committer": null, "parents": [{"sha": "1d794721acc6b43208bfea764034835172805ebd", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1d794721acc6b43208bfea764034835172805ebd", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1d794721acc6b43208bfea764034835172805ebd"}], "stats": {"total": 233, "additions": 173, "deletions": 60}, "files": [{"sha": "bfc9461a95b64b8ebe2bba62a79f3c9904732bd4", "filename": "gcc/ChangeLog", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/108879aa865b248369f3875aa032f6a8079e6b45/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/108879aa865b248369f3875aa032f6a8079e6b45/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=108879aa865b248369f3875aa032f6a8079e6b45", "patch": "@@ -1,3 +1,20 @@\n+2011-11-17  Jan Hubicka  <jh@suse.cz>\n+\n+\tPR bootstrap/51134\n+\t* i386.c (atom_cost): Fix 32bit memset description.\n+\t(expand_set_or_movmem_via_loop_with_iter): Output proper bounds check for epilogue loops.\n+\t(expand_movmem_epilogue): Handle epilogues up to size 15 w/o producing byte loop.\n+\t(decide_alg): sse_loop is not useable wthen SSE2 is disabled; when not optimizing always\n+\tuse rep movsb or lincall; do not produce word sized loops when optimizing memset for\n+\tsize (to avoid need for large constants).\n+\t(ix86_expand_movmem): Get into sync with ix86_expand_setmem; choose unroll factors\n+\tbetter; always do 128bit moves when producing SSE loops; do not produce loopy epilogue\n+\twhen size is too small.\n+\t(promote_duplicated_reg_to_size): Do not look into desired alignments when\n+\tdoing vector expansion.\n+\t(ix86_expand_setmem): Track better when promoted value is available; choose unroll factors\n+\tmore sanely.; output loopy epilogue only when needed.\n+\n 2011-11-17  Steve Ellcey  <sje@cup.hp.com>\n \n \tPR middle-end/51144"}, {"sha": "993b767ee0bb31c56d5dfee2e26c29ba648cffd0", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 156, "deletions": 60, "changes": 216, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/108879aa865b248369f3875aa032f6a8079e6b45/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/108879aa865b248369f3875aa032f6a8079e6b45/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=108879aa865b248369f3875aa032f6a8079e6b45", "patch": "@@ -1785,7 +1785,7 @@ struct processor_costs atom_cost = {\n      if that fails.  */\n   {{{libcall, {{4096, sse_loop}, {4096, unrolled_loop}, {-1, libcall}}}, /* Known alignment.  */\n     {libcall, {{4096, sse_loop}, {4096, unrolled_loop}, {-1, libcall}}}},\n-   {{libcall, {{-1, libcall}}},\t\t\t       /* Unknown alignment.  */\n+   {{libcall, {{2048, sse_loop}, {2048, unrolled_loop}, {-1, libcall}}}, /* Unknown alignment.  */\n     {libcall, {{2048, sse_loop}, {2048, unrolled_loop},\n \t       {-1, libcall}}}}},\n \n@@ -21149,20 +21149,25 @@ expand_set_or_movmem_via_loop_with_iter (rtx destmem, rtx srcmem,\n \n   top_label = gen_label_rtx ();\n   out_label = gen_label_rtx ();\n-  if (!reuse_iter)\n-    iter = gen_reg_rtx (iter_mode);\n-\n   size = expand_simple_binop (iter_mode, AND, count, piece_size_mask,\n-\t\t\t      NULL, 1, OPTAB_DIRECT);\n-  /* Those two should combine.  */\n-  if (piece_size == const1_rtx)\n+\t\t               NULL, 1, OPTAB_DIRECT);\n+  if (!reuse_iter)\n     {\n-      emit_cmp_and_jump_insns (size, const0_rtx, EQ, NULL_RTX, iter_mode,\n+      iter = gen_reg_rtx (iter_mode);\n+      /* Those two should combine.  */\n+      if (piece_size == const1_rtx)\n+\t{\n+\t  emit_cmp_and_jump_insns (size, const0_rtx, EQ, NULL_RTX, iter_mode,\n+\t\t\t\t   true, out_label);\n+\t  predict_jump (REG_BR_PROB_BASE * 10 / 100);\n+\t}\n+      emit_move_insn (iter, const0_rtx);\n+    }\n+  else\n+    {\n+      emit_cmp_and_jump_insns (iter, size, GE, NULL_RTX, iter_mode,\n \t\t\t       true, out_label);\n-      predict_jump (REG_BR_PROB_BASE * 10 / 100);\n     }\n-  if (!reuse_iter)\n-    emit_move_insn (iter, const0_rtx);\n \n   emit_label (top_label);\n \n@@ -21460,7 +21465,7 @@ expand_movmem_epilogue (rtx destmem, rtx srcmem,\n       gcc_assert (remainder_size == 0);\n       return;\n     }\n-  if (max_size > 8)\n+  if (max_size > 16)\n     {\n       count = expand_simple_binop (GET_MODE (count), AND, count, GEN_INT (max_size - 1),\n \t\t\t\t    count, 1, OPTAB_DIRECT);\n@@ -21475,6 +21480,25 @@ expand_movmem_epilogue (rtx destmem, rtx srcmem,\n    */\n   if (TARGET_SINGLE_STRINGOP)\n     {\n+      if (max_size > 8)\n+\t{\n+\t  rtx label = ix86_expand_aligntest (count, 8, true);\n+\t  if (TARGET_64BIT)\n+\t    {\n+\t      src = change_address (srcmem, DImode, srcptr);\n+\t      dest = change_address (destmem, DImode, destptr);\n+\t      emit_insn (gen_strmov (destptr, dest, srcptr, src));\n+\t    }\n+\t  else\n+\t    {\n+\t      src = change_address (srcmem, SImode, srcptr);\n+\t      dest = change_address (destmem, SImode, destptr);\n+\t      emit_insn (gen_strmov (destptr, dest, srcptr, src));\n+\t      emit_insn (gen_strmov (destptr, dest, srcptr, src));\n+\t    }\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n       if (max_size > 4)\n \t{\n \t  rtx label = ix86_expand_aligntest (count, 4, true);\n@@ -21508,6 +21532,35 @@ expand_movmem_epilogue (rtx destmem, rtx srcmem,\n       rtx offset = force_reg (Pmode, const0_rtx);\n       rtx tmp;\n \n+      if (max_size > 8)\n+\t{\n+\t  rtx label = ix86_expand_aligntest (count, 8, true);\n+\t  if (TARGET_64BIT)\n+\t    {\n+\t      src = change_address (srcmem, DImode, srcptr);\n+\t      dest = change_address (destmem, DImode, destptr);\n+\t      emit_move_insn (dest, src);\n+\t      tmp = expand_simple_binop (Pmode, PLUS, offset, GEN_INT (8), NULL,\n+\t\t\t\t         true, OPTAB_LIB_WIDEN);\n+\t    }\n+\t  else\n+\t    {\n+\t      src = change_address (srcmem, SImode, srcptr);\n+\t      dest = change_address (destmem, SImode, destptr);\n+\t      emit_move_insn (dest, src);\n+\t      tmp = expand_simple_binop (Pmode, PLUS, offset, GEN_INT (4), NULL,\n+\t\t\t\t         true, OPTAB_LIB_WIDEN);\n+\t      if (tmp != offset)\n+\t         emit_move_insn (offset, tmp);\n+\t      tmp = expand_simple_binop (Pmode, PLUS, offset, GEN_INT (4), NULL,\n+\t\t\t\t         true, OPTAB_LIB_WIDEN);\n+\t      emit_move_insn (dest, src);\n+\t    }\n+\t  if (tmp != offset)\n+\t    emit_move_insn (offset, tmp);\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n       if (max_size > 4)\n \t{\n \t  rtx label = ix86_expand_aligntest (count, 4, true);\n@@ -21588,17 +21641,28 @@ expand_setmem_epilogue (rtx destmem, rtx destptr, rtx promoted_to_vector_value,\n \t Remaining part we'll move using Pmode and narrower modes.  */\n \n       if (promoted_to_vector_value)\n-\twhile (remainder_size >= 16)\n-\t  {\n-\t    if (GET_MODE (destmem) != move_mode)\n-\t      destmem = adjust_automodify_address_nv (destmem, move_mode,\n-\t\t\t\t\t\t      destptr, offset);\n-\t    emit_strset (destmem, promoted_to_vector_value, destptr,\n-\t\t\t move_mode, offset);\n-\n-\t    offset += 16;\n-\t    remainder_size -= 16;\n-\t  }\n+\t{\n+\t  if (promoted_to_vector_value)\n+\t    {\n+\t      if (max_size >= GET_MODE_SIZE (V4SImode))\n+\t\tmove_mode = V4SImode;\n+\t      else if (max_size >= GET_MODE_SIZE (DImode))\n+\t\tmove_mode = DImode;\n+\t    }\n+\t  while (remainder_size >= GET_MODE_SIZE (move_mode))\n+\t    {\n+\t      if (GET_MODE (destmem) != move_mode)\n+\t\tdestmem = adjust_automodify_address_nv (destmem, move_mode,\n+\t\t\t\t\t\t\tdestptr, offset);\n+\t      emit_strset (destmem,\n+\t\t\t   promoted_to_vector_value,\n+\t\t\t   destptr,\n+\t\t\t   move_mode, offset);\n+\n+\t      offset += GET_MODE_SIZE (move_mode);\n+\t      remainder_size -= GET_MODE_SIZE (move_mode);\n+\t    }\n+\t}\n \n       /* Move the remaining part of epilogue - its size might be\n \t a size of the widest mode.  */\n@@ -22022,10 +22086,11 @@ decide_alg (HOST_WIDE_INT count, HOST_WIDE_INT expected_size, bool memset,\n \t\t\t     || (memset\n \t\t\t\t ? fixed_regs[AX_REG] : fixed_regs[SI_REG]));\n \n-#define ALG_USABLE_P(alg) (rep_prefix_usable\t\t\t\\\n-\t\t\t   || (alg != rep_prefix_1_byte\t\t\\\n-\t\t\t       && alg != rep_prefix_4_byte      \\\n-\t\t\t       && alg != rep_prefix_8_byte))\n+#define ALG_USABLE_P(alg) ((rep_prefix_usable\t\t\t\\\n+\t\t\t    || (alg != rep_prefix_1_byte\t\\\n+\t\t\t        && alg != rep_prefix_4_byte      \\\n+\t\t\t        && alg != rep_prefix_8_byte))    \\\n+\t\t\t   && (TARGET_SSE2 || alg != sse_loop))\n   const struct processor_costs *cost;\n \n   /* Even if the string operation call is cold, we still might spend a lot\n@@ -22037,6 +22102,9 @@ decide_alg (HOST_WIDE_INT count, HOST_WIDE_INT expected_size, bool memset,\n   else\n     optimize_for_speed = true;\n \n+  if (!optimize)\n+    return (rep_prefix_usable ? rep_prefix_1_byte : libcall);\n+\n   cost = optimize_for_speed ? ix86_cost : &ix86_size_cost;\n \n   *dynamic_check = -1;\n@@ -22049,10 +22117,10 @@ decide_alg (HOST_WIDE_INT count, HOST_WIDE_INT expected_size, bool memset,\n   /* rep; movq or rep; movl is the smallest variant.  */\n   else if (!optimize_for_speed)\n     {\n-      if (!count || (count & 3))\n-\treturn rep_prefix_usable ? rep_prefix_1_byte : loop_1_byte;\n+      if (!count || (count & 3) || memset)\n+\treturn rep_prefix_usable ? rep_prefix_1_byte : libcall;\n       else\n-\treturn rep_prefix_usable ? rep_prefix_4_byte : loop;\n+\treturn rep_prefix_usable ? rep_prefix_4_byte : libcall;\n     }\n   /* Very tiny blocks are best handled via the loop, REP is expensive to setup.\n    */\n@@ -22106,13 +22174,11 @@ decide_alg (HOST_WIDE_INT count, HOST_WIDE_INT expected_size, bool memset,\n       int max = -1;\n       enum stringop_alg alg;\n       int i;\n-      bool any_alg_usable_p = true;\n       bool only_libcall_fits = true;\n \n       for (i = 0; i < MAX_STRINGOP_ALGS; i++)\n \t{\n \t  enum stringop_alg candidate = algs->size[i].alg;\n-\t  any_alg_usable_p = any_alg_usable_p && ALG_USABLE_P (candidate);\n \n \t  if (candidate != libcall && candidate\n \t      && ALG_USABLE_P (candidate))\n@@ -22124,7 +22190,7 @@ decide_alg (HOST_WIDE_INT count, HOST_WIDE_INT expected_size, bool memset,\n       /* If there aren't any usable algorithms, then recursing on\n \t smaller sizes isn't going to find anything.  Just return the\n \t simple byte-at-a-time copy loop.  */\n-      if (!any_alg_usable_p || only_libcall_fits)\n+      if (only_libcall_fits)\n \t{\n \t  /* Pick something reasonable.  */\n \t  if (TARGET_INLINE_STRINGOPS_DYNAMICALLY)\n@@ -22253,7 +22319,7 @@ ix86_expand_movmem (rtx dst, rtx src, rtx count_exp, rtx align_exp,\n   int dynamic_check;\n   bool need_zero_guard = false;\n   bool align_unknown;\n-  int unroll_factor;\n+  unsigned int unroll_factor;\n   enum machine_mode move_mode;\n   rtx loop_iter = NULL_RTX;\n   int dst_offset, src_offset;\n@@ -22316,14 +22382,28 @@ ix86_expand_movmem (rtx dst, rtx src, rtx count_exp, rtx align_exp,\n     case unrolled_loop:\n       need_zero_guard = true;\n       move_mode = Pmode;\n-      unroll_factor = TARGET_64BIT ? 4 : 2;\n+      unroll_factor = 1;\n+      /* Select maximal available 1,2 or 4 unroll factor.\n+\t In 32bit we can not afford to use 4 registers inside the loop.  */\n+      if (!count)\n+\tunroll_factor = TARGET_64BIT ? 4 : 2;\n+      else\n+\twhile (GET_MODE_SIZE (move_mode) * unroll_factor * 2 < count\n+\t       && unroll_factor < (TARGET_64BIT ? 4 :2))\n+\t  unroll_factor *= 2;\n       size_needed = GET_MODE_SIZE (move_mode) * unroll_factor;\n       break;\n     case sse_loop:\n       need_zero_guard = true;\n       /* Use SSE instructions, if possible.  */\n-      move_mode = align_unknown ? DImode : V4SImode;\n-      unroll_factor = TARGET_64BIT ? 4 : 2;\n+      move_mode = V4SImode;\n+      /* Select maximal available 1,2 or 4 unroll factor.  */\n+      if (!count)\n+\tunroll_factor = 4;\n+      else\n+\twhile (GET_MODE_SIZE (move_mode) * unroll_factor * 2 < count\n+\t       && unroll_factor < 4)\n+\t  unroll_factor *= 2;\n       size_needed = GET_MODE_SIZE (move_mode) * unroll_factor;\n       break;\n     case rep_prefix_8_byte:\n@@ -22568,7 +22648,13 @@ ix86_expand_movmem (rtx dst, rtx src, rtx count_exp, rtx align_exp,\n   if (alg == sse_loop || alg == unrolled_loop)\n     {\n       rtx tmp;\n-      if (align_unknown && unroll_factor > 1)\n+      int remainder_size = epilogue_size_needed;\n+\n+      /* We may not need the epilgoue loop at all when the count is known\n+\t and alignment is not adjusted.  */\n+      if (count && desired_align <= align)\n+\tremainder_size = count % epilogue_size_needed;\n+      if (remainder_size > 31)\n \t{\n \t  /* Reduce epilogue's size by creating not-unrolled loop.  If we won't\n \t     do this, we can have very big epilogue - when alignment is statically\n@@ -22710,7 +22796,7 @@ promote_duplicated_reg_to_size (rtx val, int size_needed, int desired_align, int\n {\n   rtx promoted_val = NULL_RTX;\n \n-  if (size_needed > 8 || (desired_align > align && desired_align > 8))\n+  if (size_needed > 8)\n     {\n       /* We want to promote to vector register, so we expect that at least SSE\n \t is available.  */\n@@ -22724,7 +22810,7 @@ promote_duplicated_reg_to_size (rtx val, int size_needed, int desired_align, int\n       else\n \tpromoted_val = promote_duplicated_reg (V4SImode, val);\n     }\n-  else if (size_needed > 4 || (desired_align > align && desired_align > 4))\n+  else if (size_needed > 4)\n     {\n       gcc_assert (TARGET_64BIT);\n       promoted_val = promote_duplicated_reg (DImode, val);\n@@ -22764,6 +22850,7 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n   unsigned int unroll_factor;\n   enum machine_mode move_mode;\n   rtx loop_iter = NULL_RTX;\n+  bool early_jump = false;\n \n   if (CONST_INT_P (align_exp))\n     align = INTVAL (align_exp);\n@@ -22783,7 +22870,7 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n   /* Step 0: Decide on preferred algorithm, desired alignment and\n      size of chunks to be copied by main loop.  */\n \n-  align_unknown = CONST_INT_P (align_exp) && INTVAL (align_exp) > 0;\n+  align_unknown = !(CONST_INT_P (align_exp) && INTVAL (align_exp) > 0);\n   alg = decide_alg (count, expected_size, true, &dynamic_check, align_unknown);\n   desired_align = decide_alignment (align, alg, expected_size);\n   unroll_factor = 1;\n@@ -22813,19 +22900,25 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n       move_mode = Pmode;\n       unroll_factor = 1;\n       /* Select maximal available 1,2 or 4 unroll factor.  */\n-      while (GET_MODE_SIZE (move_mode) * unroll_factor * 2 < count\n-\t     && unroll_factor < 4)\n-\tunroll_factor *= 2;\n+      if (!count)\n+\tunroll_factor = 4;\n+      else\n+\twhile (GET_MODE_SIZE (move_mode) * unroll_factor * 2 < count\n+\t       && unroll_factor < 4)\n+\t  unroll_factor *= 2;\n       size_needed = GET_MODE_SIZE (move_mode) * unroll_factor;\n       break;\n     case sse_loop:\n       need_zero_guard = true;\n       move_mode = TARGET_64BIT ? V2DImode : V4SImode;\n       unroll_factor = 1;\n       /* Select maximal available 1,2 or 4 unroll factor.  */\n-      while (GET_MODE_SIZE (move_mode) * unroll_factor * 2 < count\n-\t     && unroll_factor < 4)\n-\tunroll_factor *= 2;\n+      if (!count)\n+\tunroll_factor = 4;\n+      else\n+\twhile (GET_MODE_SIZE (move_mode) * unroll_factor * 2 < count\n+\t       && unroll_factor < 4)\n+\t  unroll_factor *= 2;\n       size_needed = GET_MODE_SIZE (move_mode) * unroll_factor;\n       break;\n     case rep_prefix_8_byte:\n@@ -22904,6 +22997,7 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n               emit_move_insn (loop_iter, const0_rtx);\n \t    }\n \t  label = gen_label_rtx ();\n+\t  early_jump = true;\n \t  emit_cmp_and_jump_insns (count_exp,\n \t\t\t\t   GEN_INT (epilogue_size_needed),\n \t\t\t\t   LTU, 0, counter_mode (count_exp), 1, label);\n@@ -23016,7 +23110,7 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n       vec_promoted_val =\n \tpromote_duplicated_reg_to_size (gpr_promoted_val,\n \t\t\t\t\tGET_MODE_SIZE (move_mode),\n-\t\t\t\t\tdesired_align, align);\n+\t\t\t\t\tGET_MODE_SIZE (move_mode), align);\n       loop_iter = expand_set_or_movmem_via_loop_with_iter (dst, NULL, destreg,\n \t\t\t\t     NULL, vec_promoted_val, count_exp,\n \t\t\t\t     loop_iter, move_mode, unroll_factor,\n@@ -23065,21 +23159,26 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n       LABEL_NUSES (label) = 1;\n       /* We can not rely on fact that promoved value is known.  */\n       vec_promoted_val = 0;\n-      gpr_promoted_val = 0;\n+      if (early_jump)\n+        gpr_promoted_val = 0;\n     }\n  epilogue:\n   if (alg == unrolled_loop || alg == sse_loop)\n     {\n       rtx tmp;\n-      if (align_unknown && unroll_factor > 1\n-\t  && epilogue_size_needed >= GET_MODE_SIZE (move_mode)\n-\t  && vec_promoted_val)\n+      int remainder_size = epilogue_size_needed;\n+      if (count && desired_align <= align)\n+\tremainder_size = count % epilogue_size_needed;\n+      /* We may not need the epilgoue loop at all when the count is known\n+\t and alignment is not adjusted.  */\n+      if (remainder_size > 31 \n+\t  && (alg == sse_loop ? vec_promoted_val : gpr_promoted_val))\n \t{\n \t  /* Reduce epilogue's size by creating not-unrolled loop.  If we won't\n \t     do this, we can have very big epilogue - when alignment is statically\n \t     unknown we'll have the epilogue byte by byte which may be very slow.  */\n \t  loop_iter = expand_set_or_movmem_via_loop_with_iter (dst, NULL, destreg,\n-\t      NULL, vec_promoted_val, count_exp,\n+\t      NULL, (alg == sse_loop ? vec_promoted_val : gpr_promoted_val), count_exp,\n \t      loop_iter, move_mode, 1,\n \t      expected_size, false);\n \t  dst = change_address (dst, BLKmode, destreg);\n@@ -23090,17 +23189,14 @@ ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n       if (tmp != destreg)\n \temit_move_insn (destreg, tmp);\n     }\n-  if (count_exp == const0_rtx)\n+  if (count_exp == const0_rtx || epilogue_size_needed <= 1)\n     ;\n-  else if (!gpr_promoted_val && epilogue_size_needed > 1)\n+  else if (!gpr_promoted_val)\n     expand_setmem_epilogue_via_loop (dst, destreg, val_exp, count_exp,\n \t\t\t\t     epilogue_size_needed);\n   else\n-    {\n-      if (epilogue_size_needed > 1)\n-\texpand_setmem_epilogue (dst, destreg, vec_promoted_val, gpr_promoted_val,\n-\t\t\t\tval_exp, count_exp, epilogue_size_needed);\n-    }\n+    expand_setmem_epilogue (dst, destreg, vec_promoted_val, gpr_promoted_val,\n+\t\t\t    val_exp, count_exp, epilogue_size_needed);\n   if (jump_around_label)\n     emit_label (jump_around_label);\n   return true;"}]}