{"sha": "e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZTBiZDZjOWYwYWE2N2Q4OGJiYjIwMDE5MzYyYTQ1NzJmYzVmYWMzYw==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2017-09-12T13:27:55Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2017-09-12T13:27:55Z"}, "message": "Turn SLOW_UNALIGNED_ACCESS into a target hook\n\n2017-09-12  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood <david.sherwood@arm.com>\n\ngcc/\n\t* defaults.h (SLOW_UNALIGNED_ACCESS): Delete.\n\t* target.def (slow_unaligned_access): New hook.\n\t* targhooks.h (default_slow_unaligned_access): Declare.\n\t* targhooks.c (default_slow_unaligned_access): New function.\n\t* doc/tm.texi.in (SLOW_UNALIGNED_ACCESS): Replace with...\n\t(TARGET_SLOW_UNALIGNED_ACCESS): ...this.\n\t* doc/tm.texi: Regenerate.\n\t* config/alpha/alpha.h (SLOW_UNALIGNED_ACCESS): Delete.\n\t* config/arm/arm.h (SLOW_UNALIGNED_ACCESS): Delete.\n\t* config/i386/i386.h (SLOW_UNALIGNED_ACCESS): Delete commented-out\n\tdefinition.\n\t* config/powerpcspe/powerpcspe.h (SLOW_UNALIGNED_ACCESS): Delete.\n\t* config/powerpcspe/powerpcspe.c (TARGET_SLOW_UNALIGNED_ACCESS):\n\tRedefine.\n\t(rs6000_slow_unaligned_access): New function.\n\t(rs6000_emit_move): Use it instead of SLOW_UNALIGNED_ACCESS.\n\t(expand_block_compare): Likewise.\n\t(expand_strn_compare): Likewise.\n\t(rs6000_rtx_costs): Likewise.\n\t* config/riscv/riscv.h (SLOW_UNALIGNED_ACCESS): Delete.\n\t(riscv_slow_unaligned_access): Likewise.\n\t* config/riscv/riscv.c (riscv_slow_unaligned_access): Rename to...\n\t(riscv_slow_unaligned_access_p): ...this and make static.\n\t(riscv_option_override): Update accordingly.\n\t(riscv_slow_unaligned_access): New function.\n\t(TARGET_SLOW_UNALIGNED_ACCESS): Redefine.\n\t* config/rs6000/rs6000.h (SLOW_UNALIGNED_ACCESS): Delete.\n\t* config/rs6000/rs6000.c (TARGET_SLOW_UNALIGNED_ACCESS): Redefine.\n\t(rs6000_slow_unaligned_access): New function.\n\t(rs6000_emit_move): Use it instead of SLOW_UNALIGNED_ACCESS.\n\t(rs6000_rtx_costs): Likewise.\n\t* config/rs6000/rs6000-string.c (expand_block_compare)\n\t(expand_strn_compare): Use targetm.slow_unaligned_access instead\n\tof SLOW_UNALIGNED_ACCESS.\n\t* config/tilegx/tilegx.h (SLOW_UNALIGNED_ACCESS): Delete.\n\t* config/tilepro/tilepro.h (SLOW_UNALIGNED_ACCESS): Delete.\n\t* calls.c (expand_call): Use targetm.slow_unaligned_access instead\n\tof SLOW_UNALIGNED_ACCESS.\n\t* expmed.c (simple_mem_bitfield_p): Likewise.\n\t* expr.c (alignment_for_piecewise_move): Likewise.\n\t(emit_group_load_1): Likewise.\n\t(emit_group_store): Likewise.\n\t(copy_blkmode_from_reg): Likewise.\n\t(emit_push_insn): Likewise.\n\t(expand_assignment): Likewise.\n\t(store_field): Likewise.\n\t(expand_expr_real_1): Likewise.\n\t* gimple-fold.c (gimple_fold_builtin_memory_op): Likewise.\n\t* lra-constraints.c (simplify_operand_subreg): Likewise.\n\t* stor-layout.c (bit_field_mode_iterator::next_mode): Likewise.\n\t* gimple-ssa-store-merging.c: Likewise in block comment at start\n\tof file.\n\t* tree-ssa-strlen.c: Include target.h.\n\t(handle_builtin_memcmp): Use targetm.slow_unaligned_access instead\n\tof SLOW_UNALIGNED_ACCESS.\n\t* system.h (SLOW_UNALIGNED_ACCESS): Poison.\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r252009", "tree": {"sha": "3ca0e90f453bcc3361eb758707d8da86a656477f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/3ca0e90f453bcc3361eb758707d8da86a656477f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/comments", "author": null, "committer": null, "parents": [{"sha": "41defab318e4b5d8b87ba2b3512b02cb49c748a9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/41defab318e4b5d8b87ba2b3512b02cb49c748a9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/41defab318e4b5d8b87ba2b3512b02cb49c748a9"}], "stats": {"total": 346, "additions": 204, "deletions": 142}, "files": [{"sha": "ce59e7a2ba94d2aa796b78ae5e851df7a007155a", "filename": "gcc/ChangeLog", "status": "modified", "additions": 61, "deletions": 0, "changes": 61, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -1,3 +1,64 @@\n+2017-09-12  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood <david.sherwood@arm.com>\n+\n+\t* defaults.h (SLOW_UNALIGNED_ACCESS): Delete.\n+\t* target.def (slow_unaligned_access): New hook.\n+\t* targhooks.h (default_slow_unaligned_access): Declare.\n+\t* targhooks.c (default_slow_unaligned_access): New function.\n+\t* doc/tm.texi.in (SLOW_UNALIGNED_ACCESS): Replace with...\n+\t(TARGET_SLOW_UNALIGNED_ACCESS): ...this.\n+\t* doc/tm.texi: Regenerate.\n+\t* config/alpha/alpha.h (SLOW_UNALIGNED_ACCESS): Delete.\n+\t* config/arm/arm.h (SLOW_UNALIGNED_ACCESS): Delete.\n+\t* config/i386/i386.h (SLOW_UNALIGNED_ACCESS): Delete commented-out\n+\tdefinition.\n+\t* config/powerpcspe/powerpcspe.h (SLOW_UNALIGNED_ACCESS): Delete.\n+\t* config/powerpcspe/powerpcspe.c (TARGET_SLOW_UNALIGNED_ACCESS):\n+\tRedefine.\n+\t(rs6000_slow_unaligned_access): New function.\n+\t(rs6000_emit_move): Use it instead of SLOW_UNALIGNED_ACCESS.\n+\t(expand_block_compare): Likewise.\n+\t(expand_strn_compare): Likewise.\n+\t(rs6000_rtx_costs): Likewise.\n+\t* config/riscv/riscv.h (SLOW_UNALIGNED_ACCESS): Delete.\n+\t(riscv_slow_unaligned_access): Likewise.\n+\t* config/riscv/riscv.c (riscv_slow_unaligned_access): Rename to...\n+\t(riscv_slow_unaligned_access_p): ...this and make static.\n+\t(riscv_option_override): Update accordingly.\n+\t(riscv_slow_unaligned_access): New function.\n+\t(TARGET_SLOW_UNALIGNED_ACCESS): Redefine.\n+\t* config/rs6000/rs6000.h (SLOW_UNALIGNED_ACCESS): Delete.\n+\t* config/rs6000/rs6000.c (TARGET_SLOW_UNALIGNED_ACCESS): Redefine.\n+\t(rs6000_slow_unaligned_access): New function.\n+\t(rs6000_emit_move): Use it instead of SLOW_UNALIGNED_ACCESS.\n+\t(rs6000_rtx_costs): Likewise.\n+\t* config/rs6000/rs6000-string.c (expand_block_compare)\n+\t(expand_strn_compare): Use targetm.slow_unaligned_access instead\n+\tof SLOW_UNALIGNED_ACCESS.\n+\t* config/tilegx/tilegx.h (SLOW_UNALIGNED_ACCESS): Delete.\n+\t* config/tilepro/tilepro.h (SLOW_UNALIGNED_ACCESS): Delete.\n+\t* calls.c (expand_call): Use targetm.slow_unaligned_access instead\n+\tof SLOW_UNALIGNED_ACCESS.\n+\t* expmed.c (simple_mem_bitfield_p): Likewise.\n+\t* expr.c (alignment_for_piecewise_move): Likewise.\n+\t(emit_group_load_1): Likewise.\n+\t(emit_group_store): Likewise.\n+\t(copy_blkmode_from_reg): Likewise.\n+\t(emit_push_insn): Likewise.\n+\t(expand_assignment): Likewise.\n+\t(store_field): Likewise.\n+\t(expand_expr_real_1): Likewise.\n+\t* gimple-fold.c (gimple_fold_builtin_memory_op): Likewise.\n+\t* lra-constraints.c (simplify_operand_subreg): Likewise.\n+\t* stor-layout.c (bit_field_mode_iterator::next_mode): Likewise.\n+\t* gimple-ssa-store-merging.c: Likewise in block comment at start\n+\tof file.\n+\t* tree-ssa-strlen.c: Include target.h.\n+\t(handle_builtin_memcmp): Use targetm.slow_unaligned_access instead\n+\tof SLOW_UNALIGNED_ACCESS.\n+\t* system.h (SLOW_UNALIGNED_ACCESS): Poison.\n+\n 2017-09-12  Richard Sandiford  <richard.sandiford@linaro.org>\n \n \tPR rtl-optimization/82185"}, {"sha": "f55e89882deaba665740a0383317ba9443f9a52c", "filename": "gcc/calls.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fcalls.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fcalls.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcalls.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -3135,8 +3135,8 @@ expand_call (tree exp, rtx target, int ignore)\n \t    && target\n \t    && MEM_P (target)\n \t    && !(MEM_ALIGN (target) < TYPE_ALIGN (rettype)\n-\t\t && SLOW_UNALIGNED_ACCESS (TYPE_MODE (rettype),\n-\t\t\t\t\t   MEM_ALIGN (target))))\n+\t\t && targetm.slow_unaligned_access (TYPE_MODE (rettype),\n+\t\t\t\t\t\t   MEM_ALIGN (target))))\n \t  structure_value_addr = XEXP (target, 0);\n \telse\n \t  {"}, {"sha": "7d81ac4cf3c0de7a7199525106faabca296f1902", "filename": "gcc/config/alpha/alpha.h", "status": "modified", "additions": 0, "deletions": 6, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Falpha%2Falpha.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Falpha%2Falpha.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Falpha%2Falpha.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -300,12 +300,6 @@ extern enum alpha_fp_trap_mode alpha_fptm;\n \n #define STRICT_ALIGNMENT 1\n \n-/* Set this nonzero if unaligned move instructions are extremely slow.\n-\n-   On the Alpha, they trap.  */\n-\n-#define SLOW_UNALIGNED_ACCESS(MODE, ALIGN) 1\n-\n /* Standard register usage.  */\n \n /* Number of actual hardware registers."}, {"sha": "bef66027687ce2376ccbe90d4049b17c6dd9c27c", "filename": "gcc/config/arm/arm.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Farm%2Farm.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Farm%2Farm.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -1917,8 +1917,6 @@ enum arm_auto_incmodes\n /* Nonzero if access to memory by bytes is slow and undesirable.  */\n #define SLOW_BYTE_ACCESS 0\n \n-#define SLOW_UNALIGNED_ACCESS(MODE, ALIGN) 1\n-\n /* Immediate shift counts are truncated by the output routines (or was it\n    the assembler?).  Shift counts in a register are truncated by ARM.  Note\n    that the native compiler puts too large (> 32) immediate shift counts"}, {"sha": "e8ed8976f44029e3f213406000b8f8f6f10d3cd9", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -2017,20 +2017,6 @@ do {\t\t\t\t\t\t\t\\\n /* Nonzero if access to memory by shorts is slow and undesirable.  */\n #define SLOW_SHORT_ACCESS 0\n \n-/* Define this macro to be the value 1 if unaligned accesses have a\n-   cost many times greater than aligned accesses, for example if they\n-   are emulated in a trap handler.\n-\n-   When this macro is nonzero, the compiler will act as if\n-   `STRICT_ALIGNMENT' were nonzero when generating code for block\n-   moves.  This can cause significantly more instructions to be\n-   produced.  Therefore, do not set this macro nonzero if unaligned\n-   accesses only add a cycle or two to the time for a memory access.\n-\n-   If the value of this macro is always zero, it need not be defined.  */\n-\n-/* #define SLOW_UNALIGNED_ACCESS(MODE, ALIGN) 0 */\n-\n /* Define this macro if it is as good or better to call a constant\n    function address than to call an address kept in a register.\n "}, {"sha": "446a8bbe1eae143975e3202c5003c2b3aa15a052", "filename": "gcc/config/powerpcspe/powerpcspe.c", "status": "modified", "additions": 33, "deletions": 14, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Fpowerpcspe%2Fpowerpcspe.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Fpowerpcspe%2Fpowerpcspe.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fpowerpcspe%2Fpowerpcspe.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -1986,6 +1986,9 @@ static const struct attribute_spec rs6000_attribute_table[] =\n #undef TARGET_HARD_REGNO_CALL_PART_CLOBBERED\n #define TARGET_HARD_REGNO_CALL_PART_CLOBBERED \\\n   rs6000_hard_regno_call_part_clobbered\n+\n+#undef TARGET_SLOW_UNALIGNED_ACCESS\n+#define TARGET_SLOW_UNALIGNED_ACCESS rs6000_slow_unaligned_access\n \f\n \n /* Processor table.  */\n@@ -8366,6 +8369,21 @@ rs6000_data_alignment (tree type, unsigned int align, enum data_align how)\n   return align;\n }\n \n+/* Implement TARGET_SLOW_UNALIGNED_ACCESS.  Altivec vector memory\n+   instructions simply ignore the low bits; SPE vector memory\n+   instructions trap on unaligned accesses; VSX memory instructions are\n+   aligned to 4 or 8 bytes.  */\n+\n+static bool\n+rs6000_slow_unaligned_access (machine_mode mode, unsigned int align)\n+{\n+  return (STRICT_ALIGNMENT\n+\t  || (!TARGET_EFFICIENT_UNALIGNED_VSX\n+\t      && ((SCALAR_FLOAT_MODE_NOT_VECTOR_P (mode) && align < 32)\n+\t\t  || ((VECTOR_MODE_P (mode) || FLOAT128_VECTOR_P (mode))\n+\t\t      && (int) align < VECTOR_ALIGN (mode)))));\n+}\n+\n /* Previous GCC releases forced all vector types to have 16-byte alignment.  */\n \n bool\n@@ -11015,13 +11033,14 @@ rs6000_emit_move (rtx dest, rtx source, machine_mode mode)\n   if (GET_CODE (operands[0]) == MEM\n       && GET_CODE (operands[1]) == MEM\n       && mode == DImode\n-      && (SLOW_UNALIGNED_ACCESS (DImode, MEM_ALIGN (operands[0]))\n-\t  || SLOW_UNALIGNED_ACCESS (DImode, MEM_ALIGN (operands[1])))\n-      && ! (SLOW_UNALIGNED_ACCESS (SImode, (MEM_ALIGN (operands[0]) > 32\n-\t\t\t\t\t    ? 32 : MEM_ALIGN (operands[0])))\n-\t    || SLOW_UNALIGNED_ACCESS (SImode, (MEM_ALIGN (operands[1]) > 32\n-\t\t\t\t\t       ? 32\n-\t\t\t\t\t       : MEM_ALIGN (operands[1]))))\n+      && (rs6000_slow_unaligned_access (DImode, MEM_ALIGN (operands[0]))\n+\t  || rs6000_slow_unaligned_access (DImode, MEM_ALIGN (operands[1])))\n+      && ! (rs6000_slow_unaligned_access (SImode,\n+\t\t\t\t\t  (MEM_ALIGN (operands[0]) > 32\n+\t\t\t\t\t   ? 32 : MEM_ALIGN (operands[0])))\n+\t    || rs6000_slow_unaligned_access (SImode,\n+\t\t\t\t\t     (MEM_ALIGN (operands[1]) > 32\n+\t\t\t\t\t      ? 32 : MEM_ALIGN (operands[1]))))\n       && ! MEM_VOLATILE_P (operands [0])\n       && ! MEM_VOLATILE_P (operands [1]))\n     {\n@@ -19989,9 +20008,9 @@ expand_block_compare (rtx operands[])\n \n   unsigned int base_align = UINTVAL (align_rtx) / BITS_PER_UNIT;\n \n-  /* SLOW_UNALIGNED_ACCESS -- don't do unaligned stuff.  */\n-  if (SLOW_UNALIGNED_ACCESS (word_mode, MEM_ALIGN (orig_src1))\n-      || SLOW_UNALIGNED_ACCESS (word_mode, MEM_ALIGN (orig_src2)))\n+  /* rs6000_slow_unaligned_access -- don't do unaligned stuff.  */\n+  if (rs6000_slow_unaligned_access (word_mode, MEM_ALIGN (orig_src1))\n+      || rs6000_slow_unaligned_access (word_mode, MEM_ALIGN (orig_src2)))\n     return false;\n \n   gcc_assert (GET_MODE (target) == SImode);\n@@ -20380,9 +20399,9 @@ expand_strn_compare (rtx operands[], int no_length)\n   int align1 = MEM_ALIGN (orig_src1) / BITS_PER_UNIT;\n   int align2 = MEM_ALIGN (orig_src2) / BITS_PER_UNIT;\n \n-  /* SLOW_UNALIGNED_ACCESS -- don't do unaligned stuff.  */\n-  if (SLOW_UNALIGNED_ACCESS (word_mode, align1)\n-      || SLOW_UNALIGNED_ACCESS (word_mode, align2))\n+  /* rs6000_slow_unaligned_access -- don't do unaligned stuff.  */\n+  if (rs6000_slow_unaligned_access (word_mode, align1)\n+      || rs6000_slow_unaligned_access (word_mode, align2))\n     return false;\n \n   gcc_assert (GET_MODE (target) == SImode);\n@@ -37439,7 +37458,7 @@ rs6000_rtx_costs (rtx x, machine_mode mode, int outer_code,\n \t than generating address, e.g., (plus (reg) (const)).\n \t L1 cache latency is about two instructions.  */\n       *total = !speed ? COSTS_N_INSNS (1) + 1 : COSTS_N_INSNS (2);\n-      if (SLOW_UNALIGNED_ACCESS (mode, MEM_ALIGN (x)))\n+      if (rs6000_slow_unaligned_access (mode, MEM_ALIGN (x)))\n \t*total += COSTS_N_INSNS (100);\n       return true;\n "}, {"sha": "a3b234706faf426f5fd1ccdb14cb976a12079464", "filename": "gcc/config/powerpcspe/powerpcspe.h", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Fpowerpcspe%2Fpowerpcspe.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Fpowerpcspe%2Fpowerpcspe.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fpowerpcspe%2Fpowerpcspe.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -998,20 +998,6 @@ enum data_align { align_abi, align_opt, align_both };\n /* Nonzero if move instructions will actually fail to work\n    when given unaligned data.  */\n #define STRICT_ALIGNMENT 0\n-\n-/* Define this macro to be the value 1 if unaligned accesses have a cost\n-   many times greater than aligned accesses, for example if they are\n-   emulated in a trap handler.  */\n-/* Altivec vector memory instructions simply ignore the low bits; SPE vector\n-   memory instructions trap on unaligned accesses; VSX memory instructions are\n-   aligned to 4 or 8 bytes.  */\n-#define SLOW_UNALIGNED_ACCESS(MODE, ALIGN)\t\t\t\t\\\n-  (STRICT_ALIGNMENT\t\t\t\t\t\t\t\\\n-   || (!TARGET_EFFICIENT_UNALIGNED_VSX\t\t\t\t\t\\\n-       && ((SCALAR_FLOAT_MODE_NOT_VECTOR_P (MODE) && (ALIGN) < 32)\t\\\n-\t   || ((VECTOR_MODE_P (MODE) || FLOAT128_VECTOR_P (MODE))\t\\\n-\t       && (int) (ALIGN) < VECTOR_ALIGN (MODE)))))\n-\n \f\n /* Standard register usage.  */\n "}, {"sha": "0e440f7a0d3874a24e66eb06649838bb3217dfa3", "filename": "gcc/config/riscv/riscv.c", "status": "modified", "additions": 14, "deletions": 3, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Friscv%2Friscv.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Friscv%2Friscv.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -217,7 +217,7 @@ struct riscv_cpu_info {\n /* Global variables for machine-dependent things.  */\n \n /* Whether unaligned accesses execute very slowly.  */\n-bool riscv_slow_unaligned_access;\n+static bool riscv_slow_unaligned_access_p;\n \n /* Which tuning parameters to use.  */\n static const struct riscv_tune_info *tune_info;\n@@ -3744,8 +3744,8 @@ riscv_option_override (void)\n   /* Use -mtune's setting for slow_unaligned_access, even when optimizing\n      for size.  For architectures that trap and emulate unaligned accesses,\n      the performance cost is too great, even for -Os.  */\n-  riscv_slow_unaligned_access = (cpu->tune_info->slow_unaligned_access\n-\t\t\t\t || TARGET_STRICT_ALIGN);\n+  riscv_slow_unaligned_access_p = (cpu->tune_info->slow_unaligned_access\n+\t\t\t\t   || TARGET_STRICT_ALIGN);\n \n   /* If the user hasn't specified a branch cost, use the processor's\n      default.  */\n@@ -3966,6 +3966,14 @@ riscv_cannot_copy_insn_p (rtx_insn *insn)\n   return recog_memoized (insn) >= 0 && get_attr_cannot_copy (insn);\n }\n \n+/* Implement TARGET_SLOW_UNALIGNED_ACCESS.  */\n+\n+static bool\n+riscv_slow_unaligned_access (machine_mode, unsigned int)\n+{\n+  return riscv_slow_unaligned_access_p;\n+}\n+\n /* Initialize the GCC target structure.  */\n #undef TARGET_ASM_ALIGNED_HI_OP\n #define TARGET_ASM_ALIGNED_HI_OP \"\\t.half\\t\"\n@@ -4102,6 +4110,9 @@ riscv_cannot_copy_insn_p (rtx_insn *insn)\n #undef TARGET_MODES_TIEABLE_P\n #define TARGET_MODES_TIEABLE_P riscv_modes_tieable_p\n \n+#undef TARGET_SLOW_UNALIGNED_ACCESS\n+#define TARGET_SLOW_UNALIGNED_ACCESS riscv_slow_unaligned_access\n+\n struct gcc_target targetm = TARGET_INITIALIZER;\n \n #include \"gt-riscv.h\""}, {"sha": "d851fd861a2e7d812d76d383a5cc061426d1f3e6", "filename": "gcc/config/riscv/riscv.h", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Friscv%2Friscv.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Friscv%2Friscv.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -130,8 +130,6 @@ along with GCC; see the file COPYING3.  If not see\n    of the privileged architecture.  */\n #define STRICT_ALIGNMENT TARGET_STRICT_ALIGN\n \n-#define SLOW_UNALIGNED_ACCESS(MODE, ALIGN) riscv_slow_unaligned_access\n-\n /* Define this if you wish to imitate the way many other C compilers\n    handle alignment of bitfields and the structures that contain\n    them.\n@@ -854,7 +852,6 @@ while (0)\n \n #ifndef USED_FOR_TARGET\n extern const enum reg_class riscv_regno_to_class[];\n-extern bool riscv_slow_unaligned_access;\n #endif\n \n #define ASM_PREFERRED_EH_DATA_FORMAT(CODE,GLOBAL) \\"}, {"sha": "19463c98687fcd792ed578ad74742e32c26cf5ae", "filename": "gcc/config/rs6000/rs6000-string.c", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Frs6000%2Frs6000-string.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Frs6000%2Frs6000-string.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000-string.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -32,6 +32,7 @@\n #include \"explow.h\"\n #include \"expr.h\"\n #include \"output.h\"\n+#include \"target.h\"\n \n /* Expand a block clear operation, and return 1 if successful.  Return 0\n    if we should let the compiler generate normal code.\n@@ -338,9 +339,9 @@ expand_block_compare (rtx operands[])\n \n   unsigned int base_align = UINTVAL (align_rtx) / BITS_PER_UNIT;\n \n-  /* SLOW_UNALIGNED_ACCESS -- don't do unaligned stuff.  */\n-  if (SLOW_UNALIGNED_ACCESS (word_mode, MEM_ALIGN (orig_src1))\n-      || SLOW_UNALIGNED_ACCESS (word_mode, MEM_ALIGN (orig_src2)))\n+  /* targetm.slow_unaligned_access -- don't do unaligned stuff.  */\n+  if (targetm.slow_unaligned_access (word_mode, MEM_ALIGN (orig_src1))\n+      || targetm.slow_unaligned_access (word_mode, MEM_ALIGN (orig_src2)))\n     return false;\n \n   gcc_assert (GET_MODE (target) == SImode);\n@@ -729,9 +730,9 @@ expand_strn_compare (rtx operands[], int no_length)\n   int align1 = MEM_ALIGN (orig_src1) / BITS_PER_UNIT;\n   int align2 = MEM_ALIGN (orig_src2) / BITS_PER_UNIT;\n \n-  /* SLOW_UNALIGNED_ACCESS -- don't do unaligned stuff.  */\n-  if (SLOW_UNALIGNED_ACCESS (word_mode, align1)\n-      || SLOW_UNALIGNED_ACCESS (word_mode, align2))\n+  /* targetm.slow_unaligned_access -- don't do unaligned stuff.  */\n+  if (targetm.slow_unaligned_access (word_mode, align1)\n+      || targetm.slow_unaligned_access (word_mode, align2))\n     return false;\n \n   gcc_assert (GET_MODE (target) == SImode);"}, {"sha": "2ff7e1e307dd459d4bd484237cdcb3bb58a11483", "filename": "gcc/config/rs6000/rs6000.c", "status": "modified", "additions": 26, "deletions": 8, "changes": 34, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Frs6000%2Frs6000.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Frs6000%2Frs6000.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -1976,6 +1976,9 @@ static const struct attribute_spec rs6000_attribute_table[] =\n #undef TARGET_HARD_REGNO_CALL_PART_CLOBBERED\n #define TARGET_HARD_REGNO_CALL_PART_CLOBBERED \\\n   rs6000_hard_regno_call_part_clobbered\n+\n+#undef TARGET_SLOW_UNALIGNED_ACCESS\n+#define TARGET_SLOW_UNALIGNED_ACCESS rs6000_slow_unaligned_access\n \f\n \n /* Processor table.  */\n@@ -7902,6 +7905,20 @@ rs6000_data_alignment (tree type, unsigned int align, enum data_align how)\n   return align;\n }\n \n+/* Implement TARGET_SLOW_UNALIGNED_ACCESS.  Altivec vector memory\n+   instructions simply ignore the low bits; VSX memory instructions\n+   are aligned to 4 or 8 bytes.  */\n+\n+static bool\n+rs6000_slow_unaligned_access (machine_mode mode, unsigned int align)\n+{\n+  return (STRICT_ALIGNMENT\n+\t  || (!TARGET_EFFICIENT_UNALIGNED_VSX\n+\t      && ((SCALAR_FLOAT_MODE_NOT_VECTOR_P (mode) && align < 32)\n+\t\t  || ((VECTOR_MODE_P (mode) || FLOAT128_VECTOR_P (mode))\n+\t\t      && (int) align < VECTOR_ALIGN (mode)))));\n+}\n+\n /* Previous GCC releases forced all vector types to have 16-byte alignment.  */\n \n bool\n@@ -10500,13 +10517,14 @@ rs6000_emit_move (rtx dest, rtx source, machine_mode mode)\n   if (GET_CODE (operands[0]) == MEM\n       && GET_CODE (operands[1]) == MEM\n       && mode == DImode\n-      && (SLOW_UNALIGNED_ACCESS (DImode, MEM_ALIGN (operands[0]))\n-\t  || SLOW_UNALIGNED_ACCESS (DImode, MEM_ALIGN (operands[1])))\n-      && ! (SLOW_UNALIGNED_ACCESS (SImode, (MEM_ALIGN (operands[0]) > 32\n-\t\t\t\t\t    ? 32 : MEM_ALIGN (operands[0])))\n-\t    || SLOW_UNALIGNED_ACCESS (SImode, (MEM_ALIGN (operands[1]) > 32\n-\t\t\t\t\t       ? 32\n-\t\t\t\t\t       : MEM_ALIGN (operands[1]))))\n+      && (rs6000_slow_unaligned_access (DImode, MEM_ALIGN (operands[0]))\n+\t  || rs6000_slow_unaligned_access (DImode, MEM_ALIGN (operands[1])))\n+      && ! (rs6000_slow_unaligned_access (SImode,\n+\t\t\t\t\t  (MEM_ALIGN (operands[0]) > 32\n+\t\t\t\t\t   ? 32 : MEM_ALIGN (operands[0])))\n+\t    || rs6000_slow_unaligned_access (SImode,\n+\t\t\t\t\t     (MEM_ALIGN (operands[1]) > 32\n+\t\t\t\t\t      ? 32 : MEM_ALIGN (operands[1]))))\n       && ! MEM_VOLATILE_P (operands [0])\n       && ! MEM_VOLATILE_P (operands [1]))\n     {\n@@ -34252,7 +34270,7 @@ rs6000_rtx_costs (rtx x, machine_mode mode, int outer_code,\n \t than generating address, e.g., (plus (reg) (const)).\n \t L1 cache latency is about two instructions.  */\n       *total = !speed ? COSTS_N_INSNS (1) + 1 : COSTS_N_INSNS (2);\n-      if (SLOW_UNALIGNED_ACCESS (mode, MEM_ALIGN (x)))\n+      if (rs6000_slow_unaligned_access (mode, MEM_ALIGN (x)))\n \t*total += COSTS_N_INSNS (100);\n       return true;\n "}, {"sha": "4e2d0bb5422bb37a6ea8c36c5e93780160f64fe3", "filename": "gcc/config/rs6000/rs6000.h", "status": "modified", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Frs6000%2Frs6000.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Frs6000%2Frs6000.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -968,19 +968,6 @@ enum data_align { align_abi, align_opt, align_both };\n /* Nonzero if move instructions will actually fail to work\n    when given unaligned data.  */\n #define STRICT_ALIGNMENT 0\n-\n-/* Define this macro to be the value 1 if unaligned accesses have a cost\n-   many times greater than aligned accesses, for example if they are\n-   emulated in a trap handler.  */\n-/* Altivec vector memory instructions simply ignore the low bits; VSX memory\n-   instructions are aligned to 4 or 8 bytes.  */\n-#define SLOW_UNALIGNED_ACCESS(MODE, ALIGN)\t\t\t\t\\\n-  (STRICT_ALIGNMENT\t\t\t\t\t\t\t\\\n-   || (!TARGET_EFFICIENT_UNALIGNED_VSX\t\t\t\t\t\\\n-       && ((SCALAR_FLOAT_MODE_NOT_VECTOR_P (MODE) && (ALIGN) < 32)\t\\\n-\t   || ((VECTOR_MODE_P (MODE) || FLOAT128_VECTOR_P (MODE))\t\\\n-\t       && (int) (ALIGN) < VECTOR_ALIGN (MODE)))))\n-\n \f\n /* Standard register usage.  */\n "}, {"sha": "bbeefa7d08ba2e24631ffea13706a595c85de327", "filename": "gcc/config/tilegx/tilegx.h", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Ftilegx%2Ftilegx.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Ftilegx%2Ftilegx.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Ftilegx%2Ftilegx.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -94,9 +94,6 @@\n #define BIGGEST_FIELD_ALIGNMENT 128\n #define WIDEST_HARDWARE_FP_SIZE 64\n \n-/* Unaligned moves trap and are very slow.  */\n-#define SLOW_UNALIGNED_ACCESS(MODE, ALIGN) 1\n-\n /* Make strings word-aligned so strcpy from constants will be\n    faster.  */\n #define CONSTANT_ALIGNMENT(EXP, ALIGN)  \\"}, {"sha": "221f32a62c131f8cf74441f6899cec50cbfd6b07", "filename": "gcc/config/tilepro/tilepro.h", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Ftilepro%2Ftilepro.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fconfig%2Ftilepro%2Ftilepro.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Ftilepro%2Ftilepro.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -58,9 +58,6 @@\n #define FASTEST_ALIGNMENT 32\n #define BIGGEST_FIELD_ALIGNMENT 64\n \n-/* Unaligned moves trap and are very slow.  */\n-#define SLOW_UNALIGNED_ACCESS(MODE, ALIGN) 1\n-\n /* Make strings word-aligned so strcpy from constants will be\n    faster.  */\n #define CONSTANT_ALIGNMENT(EXP, ALIGN)  \\"}, {"sha": "d3265fcefcbcf15b832cccb28f6735d5f5bca7e1", "filename": "gcc/defaults.h", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fdefaults.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fdefaults.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdefaults.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -1170,10 +1170,6 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n #define ATTRIBUTE_ALIGNED_VALUE BIGGEST_ALIGNMENT\n #endif\n \n-#ifndef SLOW_UNALIGNED_ACCESS\n-#define SLOW_UNALIGNED_ACCESS(MODE, ALIGN) STRICT_ALIGNMENT\n-#endif\n-\n /* For most ports anything that evaluates to a constant symbolic\n    or integer value is acceptable as a constant address.  */\n #ifndef CONSTANT_ADDRESS_P"}, {"sha": "eb8a6189b95b0bb32c9b22fc7c62bc914e921594", "filename": "gcc/doc/tm.texi", "status": "modified", "additions": 11, "deletions": 12, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fdoc%2Ftm.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fdoc%2Ftm.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Ftm.texi?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -6386,23 +6386,22 @@ may eliminate subsequent memory access if subsequent accesses occur to\n other fields in the same word of the structure, but to different bytes.\n @end defmac\n \n-@defmac SLOW_UNALIGNED_ACCESS (@var{mode}, @var{alignment})\n-Define this macro to be the value 1 if memory accesses described by the\n+@deftypefn {Target Hook} bool TARGET_SLOW_UNALIGNED_ACCESS (machine_mode @var{mode}, unsigned int @var{align})\n+This hook returns true if memory accesses described by the\n @var{mode} and @var{alignment} parameters have a cost many times greater\n-than aligned accesses, for example if they are emulated in a trap\n-handler.  This macro is invoked only for unaligned accesses, i.e. when\n+than aligned accesses, for example if they are emulated in a trap handler.\n+This hook is invoked only for unaligned accesses, i.e. when\n @code{@var{alignment} < GET_MODE_ALIGNMENT (@var{mode})}.\n \n-When this macro is nonzero, the compiler will act as if\n-@code{STRICT_ALIGNMENT} were nonzero when generating code for block\n+When this hook returns true, the compiler will act as if\n+@code{STRICT_ALIGNMENT} were true when generating code for block\n moves.  This can cause significantly more instructions to be produced.\n-Therefore, do not set this macro nonzero if unaligned accesses only add a\n-cycle or two to the time for a memory access.\n+Therefore, do not make this hook return true if unaligned accesses only\n+add a cycle or two to the time for a memory access.\n \n-If the value of this macro is always zero, it need not be defined.  If\n-this macro is defined, it should produce a nonzero value when\n-@code{STRICT_ALIGNMENT} is nonzero.\n-@end defmac\n+The hook must return true whenever @code{STRICT_ALIGNMENT} is true.\n+The default implementation returns @code{STRICT_ALIGNMENT}.\n+@end deftypefn\n \n @defmac MOVE_RATIO (@var{speed})\n The threshold of number of scalar memory-to-memory move insns, @emph{below}"}, {"sha": "ce51bbad22c026f9e8453d553170567537c8c603", "filename": "gcc/doc/tm.texi.in", "status": "modified", "additions": 1, "deletions": 17, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fdoc%2Ftm.texi.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fdoc%2Ftm.texi.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Ftm.texi.in?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -4559,23 +4559,7 @@ may eliminate subsequent memory access if subsequent accesses occur to\n other fields in the same word of the structure, but to different bytes.\n @end defmac\n \n-@defmac SLOW_UNALIGNED_ACCESS (@var{mode}, @var{alignment})\n-Define this macro to be the value 1 if memory accesses described by the\n-@var{mode} and @var{alignment} parameters have a cost many times greater\n-than aligned accesses, for example if they are emulated in a trap\n-handler.  This macro is invoked only for unaligned accesses, i.e. when\n-@code{@var{alignment} < GET_MODE_ALIGNMENT (@var{mode})}.\n-\n-When this macro is nonzero, the compiler will act as if\n-@code{STRICT_ALIGNMENT} were nonzero when generating code for block\n-moves.  This can cause significantly more instructions to be produced.\n-Therefore, do not set this macro nonzero if unaligned accesses only add a\n-cycle or two to the time for a memory access.\n-\n-If the value of this macro is always zero, it need not be defined.  If\n-this macro is defined, it should produce a nonzero value when\n-@code{STRICT_ALIGNMENT} is nonzero.\n-@end defmac\n+@hook TARGET_SLOW_UNALIGNED_ACCESS\n \n @defmac MOVE_RATIO (@var{speed})\n The threshold of number of scalar memory-to-memory move insns, @emph{below}"}, {"sha": "c61a8db43e1eb00bdf61afcce0ffdd83a3edd213", "filename": "gcc/expmed.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fexpmed.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fexpmed.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpmed.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -569,7 +569,7 @@ simple_mem_bitfield_p (rtx op0, unsigned HOST_WIDE_INT bitsize,\n   return (MEM_P (op0)\n \t  && bitnum % BITS_PER_UNIT == 0\n \t  && bitsize == GET_MODE_BITSIZE (mode)\n-\t  && (!SLOW_UNALIGNED_ACCESS (mode, MEM_ALIGN (op0))\n+\t  && (!targetm.slow_unaligned_access (mode, MEM_ALIGN (op0))\n \t      || (bitnum % GET_MODE_ALIGNMENT (mode) == 0\n \t\t  && MEM_ALIGN (op0) >= GET_MODE_ALIGNMENT (mode))));\n }"}, {"sha": "989badcac59ed60a98def8b15a26126bb9b04215", "filename": "gcc/expr.c", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fexpr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fexpr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -730,7 +730,7 @@ alignment_for_piecewise_move (unsigned int max_pieces, unsigned int align)\n \t{\n \t  tmode = mode_iter.require ();\n \t  if (GET_MODE_SIZE (tmode) > max_pieces\n-\t      || SLOW_UNALIGNED_ACCESS (tmode, align))\n+\t      || targetm.slow_unaligned_access (tmode, align))\n \t    break;\n \t  xmode = tmode;\n \t}\n@@ -2179,7 +2179,7 @@ emit_group_load_1 (rtx *tmps, rtx dst, rtx orig_src, tree type, int ssize)\n \n       /* Optimize the access just a bit.  */\n       if (MEM_P (src)\n-\t  && (! SLOW_UNALIGNED_ACCESS (mode, MEM_ALIGN (src))\n+\t  && (! targetm.slow_unaligned_access (mode, MEM_ALIGN (src))\n \t      || MEM_ALIGN (src) >= GET_MODE_ALIGNMENT (mode))\n \t  && bytepos * BITS_PER_UNIT % GET_MODE_ALIGNMENT (mode) == 0\n \t  && bytelen == GET_MODE_SIZE (mode))\n@@ -2584,7 +2584,7 @@ emit_group_store (rtx orig_dst, rtx src, tree type ATTRIBUTE_UNUSED, int ssize)\n \n       /* Optimize the access just a bit.  */\n       else if (MEM_P (dest)\n-\t       && (!SLOW_UNALIGNED_ACCESS (mode, MEM_ALIGN (dest))\n+\t       && (!targetm.slow_unaligned_access (mode, MEM_ALIGN (dest))\n \t\t   || MEM_ALIGN (dest) >= GET_MODE_ALIGNMENT (mode))\n \t       && bytepos * BITS_PER_UNIT % GET_MODE_ALIGNMENT (mode) == 0\n \t       && bytelen == GET_MODE_SIZE (mode))\n@@ -2653,7 +2653,7 @@ copy_blkmode_from_reg (rtx target, rtx srcreg, tree type)\n \n   /* We can use a single move if we have an exact mode for the size.  */\n   else if (MEM_P (target)\n-\t   && (!SLOW_UNALIGNED_ACCESS (mode, MEM_ALIGN (target))\n+\t   && (!targetm.slow_unaligned_access (mode, MEM_ALIGN (target))\n \t       || MEM_ALIGN (target) >= GET_MODE_ALIGNMENT (mode))\n \t   && bytes == GET_MODE_SIZE (mode))\n   {\n@@ -4348,7 +4348,7 @@ emit_push_insn (rtx x, machine_mode mode, tree type, rtx size,\n \t  /* Here we avoid the case of a structure whose weak alignment\n \t     forces many pushes of a small amount of data,\n \t     and such small pushes do rounding that causes trouble.  */\n-\t  && ((! SLOW_UNALIGNED_ACCESS (word_mode, align))\n+\t  && ((!targetm.slow_unaligned_access (word_mode, align))\n \t      || align >= BIGGEST_ALIGNMENT\n \t      || (PUSH_ROUNDING (align / BITS_PER_UNIT)\n \t\t  == (align / BITS_PER_UNIT)))\n@@ -4947,7 +4947,7 @@ expand_assignment (tree to, tree from, bool nontemporal)\n \t  < GET_MODE_ALIGNMENT (mode))\n       && (((icode = optab_handler (movmisalign_optab, mode))\n \t   != CODE_FOR_nothing)\n-\t  || SLOW_UNALIGNED_ACCESS (mode, align)))\n+\t  || targetm.slow_unaligned_access (mode, align)))\n     {\n       rtx reg, mem;\n \n@@ -6783,7 +6783,7 @@ store_field (rtx target, HOST_WIDE_INT bitsize, HOST_WIDE_INT bitpos,\n       || (mode != BLKmode\n \t  && ((((MEM_ALIGN (target) < GET_MODE_ALIGNMENT (mode))\n \t\t|| bitpos % GET_MODE_ALIGNMENT (mode))\n-\t       && SLOW_UNALIGNED_ACCESS (mode, MEM_ALIGN (target)))\n+\t       && targetm.slow_unaligned_access (mode, MEM_ALIGN (target)))\n \t      || (bitpos % BITS_PER_UNIT != 0)))\n       || (bitsize >= 0 && mode != BLKmode\n \t  && GET_MODE_BITSIZE (mode) > bitsize)\n@@ -10229,7 +10229,7 @@ expand_expr_real_1 (tree exp, rtx target, machine_mode tmode,\n \t\texpand_insn (icode, 2, ops);\n \t\ttemp = ops[0].value;\n \t      }\n-\t    else if (SLOW_UNALIGNED_ACCESS (mode, align))\n+\t    else if (targetm.slow_unaligned_access (mode, align))\n \t      temp = extract_bit_field (temp, GET_MODE_BITSIZE (mode),\n \t\t\t\t\t0, TYPE_UNSIGNED (TREE_TYPE (exp)),\n \t\t\t\t\t(modifier == EXPAND_STACK_PARM\n@@ -10663,7 +10663,8 @@ expand_expr_real_1 (tree exp, rtx target, machine_mode tmode,\n \t\t     && ((modifier == EXPAND_CONST_ADDRESS\n \t\t\t  || modifier == EXPAND_INITIALIZER)\n \t\t\t ? STRICT_ALIGNMENT\n-\t\t\t : SLOW_UNALIGNED_ACCESS (mode1, MEM_ALIGN (op0))))\n+\t\t\t : targetm.slow_unaligned_access (mode1,\n+\t\t\t\t\t\t\t  MEM_ALIGN (op0))))\n \t\t    || (bitpos % BITS_PER_UNIT != 0)))\n \t    /* If the type and the field are a constant size and the\n \t       size of the type isn't the same size as the bitfield,"}, {"sha": "a1dce4c5c6bfaba872d029329188688c97e761fb", "filename": "gcc/gimple-fold.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fgimple-fold.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fgimple-fold.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-fold.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -756,7 +756,7 @@ gimple_fold_builtin_memory_op (gimple_stmt_iterator *gsi,\n \t\t  /* If the destination pointer is not aligned we must be able\n \t\t     to emit an unaligned store.  */\n \t\t  && (dest_align >= GET_MODE_ALIGNMENT (mode)\n-\t\t      || !SLOW_UNALIGNED_ACCESS (mode, dest_align)\n+\t\t      || !targetm.slow_unaligned_access (mode, dest_align)\n \t\t      || (optab_handler (movmisalign_optab, mode)\n \t\t\t  != CODE_FOR_nothing)))\n \t\t{\n@@ -769,7 +769,7 @@ gimple_fold_builtin_memory_op (gimple_stmt_iterator *gsi,\n \t\t  if (tem)\n \t\t    srcmem = tem;\n \t\t  else if (src_align < GET_MODE_ALIGNMENT (mode)\n-\t\t\t   && SLOW_UNALIGNED_ACCESS (mode, src_align)\n+\t\t\t   && targetm.slow_unaligned_access (mode, src_align)\n \t\t\t   && (optab_handler (movmisalign_optab, mode)\n \t\t\t       == CODE_FOR_nothing))\n \t\t    srcmem = NULL_TREE;"}, {"sha": "a4a38b104bfcc454ed49a38946706b00d997aeba", "filename": "gcc/gimple-ssa-store-merging.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fgimple-ssa-store-merging.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fgimple-ssa-store-merging.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-ssa-store-merging.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -63,7 +63,7 @@\n    of a size that is a power of 2.  For example it can try to emit a 40-bit\n    store as a 32-bit store followed by an 8-bit store.\n    We try to emit as wide stores as we can while respecting STRICT_ALIGNMENT or\n-   SLOW_UNALIGNED_ACCESS rules.\n+   TARGET_SLOW_UNALIGNED_ACCESS rules.\n \n    Note on endianness and example:\n    Consider 2 contiguous 16-bit stores followed by 2 contiguous 8-bit stores:"}, {"sha": "427f1d9649245b4c6f077d9da9cea43b4f29d919", "filename": "gcc/lra-constraints.c", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Flra-constraints.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Flra-constraints.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flra-constraints.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -1557,9 +1557,10 @@ simplify_operand_subreg (int nop, machine_mode reg_mode)\n \t\t&& GET_MODE_SIZE (innermode) <= UNITS_PER_WORD\n \t\t&& WORD_REGISTER_OPERATIONS)\n \t      && (!(MEM_ALIGN (subst) < GET_MODE_ALIGNMENT (mode)\n-\t\t    && SLOW_UNALIGNED_ACCESS (mode, MEM_ALIGN (subst)))\n+\t\t    && targetm.slow_unaligned_access (mode, MEM_ALIGN (subst)))\n \t\t  || (MEM_ALIGN (reg) < GET_MODE_ALIGNMENT (innermode)\n-\t\t      && SLOW_UNALIGNED_ACCESS (innermode, MEM_ALIGN (reg)))))\n+\t\t      && targetm.slow_unaligned_access (innermode,\n+\t\t\t\t\t\t\tMEM_ALIGN (reg)))))\n \t    return true;\n \n \t  *curr_id->operand_loc[nop] = operand;"}, {"sha": "a6d430760fc4b376717df21b75f6131137a2c887", "filename": "gcc/stor-layout.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fstor-layout.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fstor-layout.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fstor-layout.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -2793,7 +2793,7 @@ bit_field_mode_iterator::next_mode (scalar_int_mode *out_mode)\n \n       /* Stop if the mode requires too much alignment.  */\n       if (GET_MODE_ALIGNMENT (mode) > m_align\n-\t  && SLOW_UNALIGNED_ACCESS (mode, m_align))\n+\t  && targetm.slow_unaligned_access (mode, m_align))\n \tbreak;\n \n       *out_mode = mode;"}, {"sha": "ef025c04d431daad35ce7d5ee8f2a7b6875a796f", "filename": "gcc/system.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fsystem.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Fsystem.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsystem.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -912,7 +912,7 @@ extern void fancy_abort (const char *, int, const char *)\n \tCLEAR_BY_PIECES_P MOVE_BY_PIECES_P SET_BY_PIECES_P\t\t\\\n \tSTORE_BY_PIECES_P TARGET_FLT_EVAL_METHOD\t\t\t\\\n \tHARD_REGNO_CALL_PART_CLOBBERED HARD_REGNO_MODE_OK\t\t\\\n-\tMODES_TIEABLE_P FUNCTION_ARG_PADDING\n+\tMODES_TIEABLE_P FUNCTION_ARG_PADDING SLOW_UNALIGNED_ACCESS\n \n /* Target macros only used for code built for the target, that have\n    moved to libgcc-tm.h or have never been present elsewhere.  */"}, {"sha": "032765069c4954079837a1c6f39ef33ce17286bb", "filename": "gcc/target.def", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Ftarget.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Ftarget.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarget.def?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -3511,6 +3511,25 @@ negative number from this hook.\",\n  int, (machine_mode mode),\n  default_compare_by_pieces_branch_ratio)\n \n+DEFHOOK\n+(slow_unaligned_access,\n+ \"This hook returns true if memory accesses described by the\\n\\\n+@var{mode} and @var{alignment} parameters have a cost many times greater\\n\\\n+than aligned accesses, for example if they are emulated in a trap handler.\\n\\\n+This hook is invoked only for unaligned accesses, i.e. when\\n\\\n+@code{@var{alignment} < GET_MODE_ALIGNMENT (@var{mode})}.\\n\\\n+\\n\\\n+When this hook returns true, the compiler will act as if\\n\\\n+@code{STRICT_ALIGNMENT} were true when generating code for block\\n\\\n+moves.  This can cause significantly more instructions to be produced.\\n\\\n+Therefore, do not make this hook return true if unaligned accesses only\\n\\\n+add a cycle or two to the time for a memory access.\\n\\\n+\\n\\\n+The hook must return true whenever @code{STRICT_ALIGNMENT} is true.\\n\\\n+The default implementation returns @code{STRICT_ALIGNMENT}.\",\n+ bool, (machine_mode mode, unsigned int align),\n+ default_slow_unaligned_access)\n+\n DEFHOOK\n (optab_supported_p,\n  \"Return true if the optimizers should use optab @var{op} with\\n\\"}, {"sha": "dd6491e077bb3b98379d0ca2b0cabe0c63141035", "filename": "gcc/targhooks.c", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Ftarghooks.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Ftarghooks.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarghooks.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -1558,6 +1558,14 @@ default_register_move_cost (machine_mode mode ATTRIBUTE_UNUSED,\n #endif\n }\n \n+/* The default implementation of TARGET_SLOW_UNALIGNED_ACCESS.  */\n+\n+bool\n+default_slow_unaligned_access (machine_mode, unsigned int)\n+{\n+  return STRICT_ALIGNMENT;\n+}\n+\n /* For hooks which use the MOVE_RATIO macro, this gives the legacy default\n    behavior.  SPEED_P is true if we are compiling for speed.  */\n "}, {"sha": "a12f07892d677248a9b7e101a374c2ebce74fdea", "filename": "gcc/targhooks.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Ftarghooks.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Ftarghooks.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarghooks.h?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -197,6 +197,7 @@ extern tree default_builtin_tm_load_store (tree);\n extern int default_memory_move_cost (machine_mode, reg_class_t, bool);\n extern int default_register_move_cost (machine_mode, reg_class_t,\n \t\t\t\t       reg_class_t);\n+extern bool default_slow_unaligned_access (machine_mode, unsigned int);\n \n extern bool default_use_by_pieces_infrastructure_p (unsigned HOST_WIDE_INT,\n \t\t\t\t\t\t    unsigned int,"}, {"sha": "4ec0dacf38aa07175c5517ec6323646d9e4ee1ec", "filename": "gcc/tree-ssa-strlen.c", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Ftree-ssa-strlen.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c/gcc%2Ftree-ssa-strlen.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-strlen.c?ref=e0bd6c9f0aa67d88bbb20019362a4572fc5fac3c", "patch": "@@ -45,6 +45,7 @@ along with GCC; see the file COPYING3.  If not see\n #include \"ipa-chkp.h\"\n #include \"tree-hash-traits.h\"\n #include \"builtins.h\"\n+#include \"target.h\"\n \n /* A vector indexed by SSA_NAME_VERSION.  0 means unknown, positive value\n    is an index into strinfo vector, negative value stands for\n@@ -2124,7 +2125,7 @@ handle_builtin_memcmp (gimple_stmt_iterator *gsi)\n       unsigned align = MIN (align1, align2);\n       scalar_int_mode mode;\n       if (int_mode_for_size (leni, 1).exists (&mode)\n-\t  && (align >= leni || !SLOW_UNALIGNED_ACCESS (mode, align)))\n+\t  && (align >= leni || !targetm.slow_unaligned_access (mode, align)))\n \t{\n \t  location_t loc = gimple_location (stmt2);\n \t  tree type, off;"}]}