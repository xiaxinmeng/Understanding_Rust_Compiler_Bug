{"sha": "7211512a5f817ad08e333774bd186c0c5ff48533", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NzIxMTUxMmE1ZjgxN2FkMDhlMzMzNzc0YmQxODZjMGM1ZmY0ODUzMw==", "commit": {"author": {"name": "Alan Lawrence", "email": "alan.lawrence@arm.com", "date": "2014-04-30T17:04:53Z"}, "committer": {"name": "Alan Lawrence", "email": "alalaw01@gcc.gnu.org", "date": "2014-04-30T17:04:53Z"}, "message": "Rewrite AArch64 UZP Intrinsics using __builtin_shuffle.\n\ngcc/testsuite/ChangeLog:\n\n\t* gcc.target/aarch64/vuzps32_1.c: Expect zip1/2 insn rather than uzp1/2.\n\t* gcc.target/aarch64/vuzpu32_1.c: Likewise.\n\t* gcc.target/aarch64/vuzpf32_1.c: Likewise.\n\ngcc/ChangeLog:\n\n\t* config/aarch64/arm_neon.h (vuzp1_f32, vuzp1_p8, vuzp1_p16, vuzp1_s8,\n\tvuzp1_s16, vuzp1_s32, vuzp1_u8, vuzp1_u16, vuzp1_u32, vuzp1q_f32,\n\tvuzp1q_f64, vuzp1q_p8, vuzp1q_p16, vuzp1q_s8, vuzp1q_s16, vuzp1q_s32,\n\tvuzp1q_s64, vuzp1q_u8, vuzp1q_u16, vuzp1q_u32, vuzp1q_u64, vuzp2_f32,\n\tvuzp2_p8, vuzp2_p16, vuzp2_s8, vuzp2_s16, vuzp2_s32, vuzp2_u8,\n\tvuzp2_u16, vuzp2_u32, vuzp2q_f32, vuzp2q_f64, vuzp2q_p8, vuzp2q_p16,\n\tvuzp2q_s8, vuzp2q_s16, vuzp2q_s32, vuzp2q_s64, vuzp2q_u8, vuzp2q_u16,\n\tvuzp2q_u32, vuzp2q_u64): Replace temporary asm with __builtin_shuffle.\n\nFrom-SVN: r209943", "tree": {"sha": "2c739fd3839c731fc8af42242db26fa7508865f8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/2c739fd3839c731fc8af42242db26fa7508865f8"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/7211512a5f817ad08e333774bd186c0c5ff48533", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7211512a5f817ad08e333774bd186c0c5ff48533", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7211512a5f817ad08e333774bd186c0c5ff48533", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7211512a5f817ad08e333774bd186c0c5ff48533/comments", "author": null, "committer": null, "parents": [{"sha": "e3fe9b5b5ae3daf2f6c3cde35da6c75a3b8ff0bb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e3fe9b5b5ae3daf2f6c3cde35da6c75a3b8ff0bb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e3fe9b5b5ae3daf2f6c3cde35da6c75a3b8ff0bb"}], "stats": {"total": 1364, "additions": 676, "deletions": 688}, "files": [{"sha": "e3ae8dcfc81eec33e33c32a52d5d0f740f6267c3", "filename": "gcc/ChangeLog", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=7211512a5f817ad08e333774bd186c0c5ff48533", "patch": "@@ -1,3 +1,14 @@\n+2014-04-30  Alan Lawrence  <alan.lawrence@arm.com>\n+\n+\t* config/aarch64/arm_neon.h (vuzp1_f32, vuzp1_p8, vuzp1_p16, vuzp1_s8,\n+\tvuzp1_s16, vuzp1_s32, vuzp1_u8, vuzp1_u16, vuzp1_u32, vuzp1q_f32,\n+\tvuzp1q_f64, vuzp1q_p8, vuzp1q_p16, vuzp1q_s8, vuzp1q_s16, vuzp1q_s32,\n+\tvuzp1q_s64, vuzp1q_u8, vuzp1q_u16, vuzp1q_u32, vuzp1q_u64, vuzp2_f32,\n+\tvuzp2_p8, vuzp2_p16, vuzp2_s8, vuzp2_s16, vuzp2_s32, vuzp2_u8,\n+\tvuzp2_u16, vuzp2_u32, vuzp2q_f32, vuzp2q_f64, vuzp2q_p8, vuzp2q_p16,\n+\tvuzp2q_s8, vuzp2q_s16, vuzp2q_s32, vuzp2q_s64, vuzp2q_u8, vuzp2q_u16,\n+\tvuzp2q_u32, vuzp2q_u64): Replace temporary asm with __builtin_shuffle.\n+\n 2014-04-30  Joern Rennecke  <joern.rennecke@embecosm.com>\n \n \t* config/arc/arc.opt (mlra): Move comment above option name"}, {"sha": "f6213ce2aeac465d6117f5fbb1885a6dec9512e1", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 653, "deletions": 682, "changes": 1335, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=7211512a5f817ad08e333774bd186c0c5ff48533", "patch": "@@ -13199,467 +13199,6 @@ vtstq_p16 (poly16x8_t a, poly16x8_t b)\n            : /* No clobbers */);\n   return result;\n }\n-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n-vuzp1_f32 (float32x2_t a, float32x2_t b)\n-{\n-  float32x2_t result;\n-  __asm__ (\"uzp1 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n-vuzp1_p8 (poly8x8_t a, poly8x8_t b)\n-{\n-  poly8x8_t result;\n-  __asm__ (\"uzp1 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n-vuzp1_p16 (poly16x4_t a, poly16x4_t b)\n-{\n-  poly16x4_t result;\n-  __asm__ (\"uzp1 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vuzp1_s8 (int8x8_t a, int8x8_t b)\n-{\n-  int8x8_t result;\n-  __asm__ (\"uzp1 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vuzp1_s16 (int16x4_t a, int16x4_t b)\n-{\n-  int16x4_t result;\n-  __asm__ (\"uzp1 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n-vuzp1_s32 (int32x2_t a, int32x2_t b)\n-{\n-  int32x2_t result;\n-  __asm__ (\"uzp1 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vuzp1_u8 (uint8x8_t a, uint8x8_t b)\n-{\n-  uint8x8_t result;\n-  __asm__ (\"uzp1 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vuzp1_u16 (uint16x4_t a, uint16x4_t b)\n-{\n-  uint16x4_t result;\n-  __asm__ (\"uzp1 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vuzp1_u32 (uint32x2_t a, uint32x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"uzp1 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n-vuzp1q_f32 (float32x4_t a, float32x4_t b)\n-{\n-  float32x4_t result;\n-  __asm__ (\"uzp1 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n-vuzp1q_f64 (float64x2_t a, float64x2_t b)\n-{\n-  float64x2_t result;\n-  __asm__ (\"uzp1 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n-vuzp1q_p8 (poly8x16_t a, poly8x16_t b)\n-{\n-  poly8x16_t result;\n-  __asm__ (\"uzp1 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n-vuzp1q_p16 (poly16x8_t a, poly16x8_t b)\n-{\n-  poly16x8_t result;\n-  __asm__ (\"uzp1 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vuzp1q_s8 (int8x16_t a, int8x16_t b)\n-{\n-  int8x16_t result;\n-  __asm__ (\"uzp1 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vuzp1q_s16 (int16x8_t a, int16x8_t b)\n-{\n-  int16x8_t result;\n-  __asm__ (\"uzp1 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n-vuzp1q_s32 (int32x4_t a, int32x4_t b)\n-{\n-  int32x4_t result;\n-  __asm__ (\"uzp1 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n-vuzp1q_s64 (int64x2_t a, int64x2_t b)\n-{\n-  int64x2_t result;\n-  __asm__ (\"uzp1 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vuzp1q_u8 (uint8x16_t a, uint8x16_t b)\n-{\n-  uint8x16_t result;\n-  __asm__ (\"uzp1 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vuzp1q_u16 (uint16x8_t a, uint16x8_t b)\n-{\n-  uint16x8_t result;\n-  __asm__ (\"uzp1 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vuzp1q_u32 (uint32x4_t a, uint32x4_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"uzp1 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vuzp1q_u64 (uint64x2_t a, uint64x2_t b)\n-{\n-  uint64x2_t result;\n-  __asm__ (\"uzp1 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n-vuzp2_f32 (float32x2_t a, float32x2_t b)\n-{\n-  float32x2_t result;\n-  __asm__ (\"uzp2 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n-vuzp2_p8 (poly8x8_t a, poly8x8_t b)\n-{\n-  poly8x8_t result;\n-  __asm__ (\"uzp2 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n-vuzp2_p16 (poly16x4_t a, poly16x4_t b)\n-{\n-  poly16x4_t result;\n-  __asm__ (\"uzp2 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vuzp2_s8 (int8x8_t a, int8x8_t b)\n-{\n-  int8x8_t result;\n-  __asm__ (\"uzp2 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vuzp2_s16 (int16x4_t a, int16x4_t b)\n-{\n-  int16x4_t result;\n-  __asm__ (\"uzp2 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n-vuzp2_s32 (int32x2_t a, int32x2_t b)\n-{\n-  int32x2_t result;\n-  __asm__ (\"uzp2 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vuzp2_u8 (uint8x8_t a, uint8x8_t b)\n-{\n-  uint8x8_t result;\n-  __asm__ (\"uzp2 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vuzp2_u16 (uint16x4_t a, uint16x4_t b)\n-{\n-  uint16x4_t result;\n-  __asm__ (\"uzp2 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vuzp2_u32 (uint32x2_t a, uint32x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"uzp2 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n-vuzp2q_f32 (float32x4_t a, float32x4_t b)\n-{\n-  float32x4_t result;\n-  __asm__ (\"uzp2 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n-vuzp2q_f64 (float64x2_t a, float64x2_t b)\n-{\n-  float64x2_t result;\n-  __asm__ (\"uzp2 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n-vuzp2q_p8 (poly8x16_t a, poly8x16_t b)\n-{\n-  poly8x16_t result;\n-  __asm__ (\"uzp2 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n-vuzp2q_p16 (poly16x8_t a, poly16x8_t b)\n-{\n-  poly16x8_t result;\n-  __asm__ (\"uzp2 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vuzp2q_s8 (int8x16_t a, int8x16_t b)\n-{\n-  int8x16_t result;\n-  __asm__ (\"uzp2 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vuzp2q_s16 (int16x8_t a, int16x8_t b)\n-{\n-  int16x8_t result;\n-  __asm__ (\"uzp2 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n-vuzp2q_s32 (int32x4_t a, int32x4_t b)\n-{\n-  int32x4_t result;\n-  __asm__ (\"uzp2 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n-vuzp2q_s64 (int64x2_t a, int64x2_t b)\n-{\n-  int64x2_t result;\n-  __asm__ (\"uzp2 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vuzp2q_u8 (uint8x16_t a, uint8x16_t b)\n-{\n-  uint8x16_t result;\n-  __asm__ (\"uzp2 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vuzp2q_u16 (uint16x8_t a, uint16x8_t b)\n-{\n-  uint16x8_t result;\n-  __asm__ (\"uzp2 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vuzp2q_u32 (uint32x4_t a, uint32x4_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"uzp2 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vuzp2q_u64 (uint64x2_t a, uint64x2_t b)\n-{\n-  uint64x2_t result;\n-  __asm__ (\"uzp2 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n \n /* End of temporary inline asm implementations.  */\n \n@@ -24844,407 +24383,839 @@ vst4q_f64 (float64_t * __a, float64x2x4_t val)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vsubd_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  return __a - __b;\n+  return __a - __b;\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vsubd_u64 (uint64x1_t __a, uint64x1_t __b)\n+{\n+  return __a - __b;\n+}\n+\n+/* vtbx1  */\n+\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vtbx1_s8 (int8x8_t __r, int8x8_t __tab, int8x8_t __idx)\n+{\n+  uint8x8_t __mask = vclt_u8 (vreinterpret_u8_s8 (__idx),\n+\t\t\t      vmov_n_u8 (8));\n+  int8x8_t __tbl = vtbl1_s8 (__tab, __idx);\n+\n+  return vbsl_s8 (__mask, __tbl, __r);\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vtbx1_u8 (uint8x8_t __r, uint8x8_t __tab, uint8x8_t __idx)\n+{\n+  uint8x8_t __mask = vclt_u8 (__idx, vmov_n_u8 (8));\n+  uint8x8_t __tbl = vtbl1_u8 (__tab, __idx);\n+\n+  return vbsl_u8 (__mask, __tbl, __r);\n+}\n+\n+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n+vtbx1_p8 (poly8x8_t __r, poly8x8_t __tab, uint8x8_t __idx)\n+{\n+  uint8x8_t __mask = vclt_u8 (__idx, vmov_n_u8 (8));\n+  poly8x8_t __tbl = vtbl1_p8 (__tab, __idx);\n+\n+  return vbsl_p8 (__mask, __tbl, __r);\n+}\n+\n+/* vtbx3  */\n+\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vtbx3_s8 (int8x8_t __r, int8x8x3_t __tab, int8x8_t __idx)\n+{\n+  uint8x8_t __mask = vclt_u8 (vreinterpret_u8_s8 (__idx),\n+\t\t\t      vmov_n_u8 (24));\n+  int8x8_t __tbl = vtbl3_s8 (__tab, __idx);\n+\n+  return vbsl_s8 (__mask, __tbl, __r);\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vtbx3_u8 (uint8x8_t __r, uint8x8x3_t __tab, uint8x8_t __idx)\n+{\n+  uint8x8_t __mask = vclt_u8 (__idx, vmov_n_u8 (24));\n+  uint8x8_t __tbl = vtbl3_u8 (__tab, __idx);\n+\n+  return vbsl_u8 (__mask, __tbl, __r);\n+}\n+\n+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n+vtbx3_p8 (poly8x8_t __r, poly8x8x3_t __tab, uint8x8_t __idx)\n+{\n+  uint8x8_t __mask = vclt_u8 (__idx, vmov_n_u8 (24));\n+  poly8x8_t __tbl = vtbl3_p8 (__tab, __idx);\n+\n+  return vbsl_p8 (__mask, __tbl, __r);\n+}\n+\n+/* vtrn */\n+\n+__extension__ static __inline float32x2x2_t __attribute__ ((__always_inline__))\n+vtrn_f32 (float32x2_t a, float32x2_t b)\n+{\n+  return (float32x2x2_t) {vtrn1_f32 (a, b), vtrn2_f32 (a, b)};\n+}\n+\n+__extension__ static __inline poly8x8x2_t __attribute__ ((__always_inline__))\n+vtrn_p8 (poly8x8_t a, poly8x8_t b)\n+{\n+  return (poly8x8x2_t) {vtrn1_p8 (a, b), vtrn2_p8 (a, b)};\n+}\n+\n+__extension__ static __inline poly16x4x2_t __attribute__ ((__always_inline__))\n+vtrn_p16 (poly16x4_t a, poly16x4_t b)\n+{\n+  return (poly16x4x2_t) {vtrn1_p16 (a, b), vtrn2_p16 (a, b)};\n+}\n+\n+__extension__ static __inline int8x8x2_t __attribute__ ((__always_inline__))\n+vtrn_s8 (int8x8_t a, int8x8_t b)\n+{\n+  return (int8x8x2_t) {vtrn1_s8 (a, b), vtrn2_s8 (a, b)};\n+}\n+\n+__extension__ static __inline int16x4x2_t __attribute__ ((__always_inline__))\n+vtrn_s16 (int16x4_t a, int16x4_t b)\n+{\n+  return (int16x4x2_t) {vtrn1_s16 (a, b), vtrn2_s16 (a, b)};\n+}\n+\n+__extension__ static __inline int32x2x2_t __attribute__ ((__always_inline__))\n+vtrn_s32 (int32x2_t a, int32x2_t b)\n+{\n+  return (int32x2x2_t) {vtrn1_s32 (a, b), vtrn2_s32 (a, b)};\n+}\n+\n+__extension__ static __inline uint8x8x2_t __attribute__ ((__always_inline__))\n+vtrn_u8 (uint8x8_t a, uint8x8_t b)\n+{\n+  return (uint8x8x2_t) {vtrn1_u8 (a, b), vtrn2_u8 (a, b)};\n+}\n+\n+__extension__ static __inline uint16x4x2_t __attribute__ ((__always_inline__))\n+vtrn_u16 (uint16x4_t a, uint16x4_t b)\n+{\n+  return (uint16x4x2_t) {vtrn1_u16 (a, b), vtrn2_u16 (a, b)};\n+}\n+\n+__extension__ static __inline uint32x2x2_t __attribute__ ((__always_inline__))\n+vtrn_u32 (uint32x2_t a, uint32x2_t b)\n+{\n+  return (uint32x2x2_t) {vtrn1_u32 (a, b), vtrn2_u32 (a, b)};\n+}\n+\n+__extension__ static __inline float32x4x2_t __attribute__ ((__always_inline__))\n+vtrnq_f32 (float32x4_t a, float32x4_t b)\n+{\n+  return (float32x4x2_t) {vtrn1q_f32 (a, b), vtrn2q_f32 (a, b)};\n+}\n+\n+__extension__ static __inline poly8x16x2_t __attribute__ ((__always_inline__))\n+vtrnq_p8 (poly8x16_t a, poly8x16_t b)\n+{\n+  return (poly8x16x2_t) {vtrn1q_p8 (a, b), vtrn2q_p8 (a, b)};\n+}\n+\n+__extension__ static __inline poly16x8x2_t __attribute__ ((__always_inline__))\n+vtrnq_p16 (poly16x8_t a, poly16x8_t b)\n+{\n+  return (poly16x8x2_t) {vtrn1q_p16 (a, b), vtrn2q_p16 (a, b)};\n+}\n+\n+__extension__ static __inline int8x16x2_t __attribute__ ((__always_inline__))\n+vtrnq_s8 (int8x16_t a, int8x16_t b)\n+{\n+  return (int8x16x2_t) {vtrn1q_s8 (a, b), vtrn2q_s8 (a, b)};\n+}\n+\n+__extension__ static __inline int16x8x2_t __attribute__ ((__always_inline__))\n+vtrnq_s16 (int16x8_t a, int16x8_t b)\n+{\n+  return (int16x8x2_t) {vtrn1q_s16 (a, b), vtrn2q_s16 (a, b)};\n+}\n+\n+__extension__ static __inline int32x4x2_t __attribute__ ((__always_inline__))\n+vtrnq_s32 (int32x4_t a, int32x4_t b)\n+{\n+  return (int32x4x2_t) {vtrn1q_s32 (a, b), vtrn2q_s32 (a, b)};\n+}\n+\n+__extension__ static __inline uint8x16x2_t __attribute__ ((__always_inline__))\n+vtrnq_u8 (uint8x16_t a, uint8x16_t b)\n+{\n+  return (uint8x16x2_t) {vtrn1q_u8 (a, b), vtrn2q_u8 (a, b)};\n+}\n+\n+__extension__ static __inline uint16x8x2_t __attribute__ ((__always_inline__))\n+vtrnq_u16 (uint16x8_t a, uint16x8_t b)\n+{\n+  return (uint16x8x2_t) {vtrn1q_u16 (a, b), vtrn2q_u16 (a, b)};\n+}\n+\n+__extension__ static __inline uint32x4x2_t __attribute__ ((__always_inline__))\n+vtrnq_u32 (uint32x4_t a, uint32x4_t b)\n+{\n+  return (uint32x4x2_t) {vtrn1q_u32 (a, b), vtrn2q_u32 (a, b)};\n+}\n+\n+/* vtst */\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vtst_s8 (int8x8_t __a, int8x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_cmtstv8qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vtst_s16 (int16x4_t __a, int16x4_t __b)\n+{\n+  return (uint16x4_t) __builtin_aarch64_cmtstv4hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vtst_s32 (int32x2_t __a, int32x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_cmtstv2si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vtst_s64 (int64x1_t __a, int64x1_t __b)\n+{\n+  return (__a & __b) ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vtst_u8 (uint8x8_t __a, uint8x8_t __b)\n+{\n+  return (uint8x8_t) __builtin_aarch64_cmtstv8qi ((int8x8_t) __a,\n+\t\t\t\t\t\t (int8x8_t) __b);\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vtst_u16 (uint16x4_t __a, uint16x4_t __b)\n+{\n+  return (uint16x4_t) __builtin_aarch64_cmtstv4hi ((int16x4_t) __a,\n+\t\t\t\t\t\t  (int16x4_t) __b);\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vtst_u32 (uint32x2_t __a, uint32x2_t __b)\n+{\n+  return (uint32x2_t) __builtin_aarch64_cmtstv2si ((int32x2_t) __a,\n+\t\t\t\t\t\t  (int32x2_t) __b);\n+}\n+\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vtst_u64 (uint64x1_t __a, uint64x1_t __b)\n+{\n+  return (__a & __b) ? -1ll : 0ll;\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vtstq_s8 (int8x16_t __a, int8x16_t __b)\n+{\n+  return (uint8x16_t) __builtin_aarch64_cmtstv16qi (__a, __b);\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vtstq_s16 (int16x8_t __a, int16x8_t __b)\n+{\n+  return (uint16x8_t) __builtin_aarch64_cmtstv8hi (__a, __b);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vtstq_s32 (int32x4_t __a, int32x4_t __b)\n+{\n+  return (uint32x4_t) __builtin_aarch64_cmtstv4si (__a, __b);\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vtstq_s64 (int64x2_t __a, int64x2_t __b)\n+{\n+  return (uint64x2_t) __builtin_aarch64_cmtstv2di (__a, __b);\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vsubd_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vtstq_u8 (uint8x16_t __a, uint8x16_t __b)\n {\n-  return __a - __b;\n+  return (uint8x16_t) __builtin_aarch64_cmtstv16qi ((int8x16_t) __a,\n+\t\t\t\t\t\t   (int8x16_t) __b);\n }\n \n-/* vtbx1  */\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vtbx1_s8 (int8x8_t __r, int8x8_t __tab, int8x8_t __idx)\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vtstq_u16 (uint16x8_t __a, uint16x8_t __b)\n {\n-  uint8x8_t __mask = vclt_u8 (vreinterpret_u8_s8 (__idx),\n-\t\t\t      vmov_n_u8 (8));\n-  int8x8_t __tbl = vtbl1_s8 (__tab, __idx);\n-\n-  return vbsl_s8 (__mask, __tbl, __r);\n+  return (uint16x8_t) __builtin_aarch64_cmtstv8hi ((int16x8_t) __a,\n+\t\t\t\t\t\t  (int16x8_t) __b);\n }\n \n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vtbx1_u8 (uint8x8_t __r, uint8x8_t __tab, uint8x8_t __idx)\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vtstq_u32 (uint32x4_t __a, uint32x4_t __b)\n {\n-  uint8x8_t __mask = vclt_u8 (__idx, vmov_n_u8 (8));\n-  uint8x8_t __tbl = vtbl1_u8 (__tab, __idx);\n+  return (uint32x4_t) __builtin_aarch64_cmtstv4si ((int32x4_t) __a,\n+\t\t\t\t\t\t  (int32x4_t) __b);\n+}\n \n-  return vbsl_u8 (__mask, __tbl, __r);\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vtstq_u64 (uint64x2_t __a, uint64x2_t __b)\n+{\n+  return (uint64x2_t) __builtin_aarch64_cmtstv2di ((int64x2_t) __a,\n+\t\t\t\t\t\t  (int64x2_t) __b);\n }\n \n-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n-vtbx1_p8 (poly8x8_t __r, poly8x8_t __tab, uint8x8_t __idx)\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vtstd_s64 (int64x1_t __a, int64x1_t __b)\n {\n-  uint8x8_t __mask = vclt_u8 (__idx, vmov_n_u8 (8));\n-  poly8x8_t __tbl = vtbl1_p8 (__tab, __idx);\n+  return (__a & __b) ? -1ll : 0ll;\n+}\n \n-  return vbsl_p8 (__mask, __tbl, __r);\n+__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n+vtstd_u64 (uint64x1_t __a, uint64x1_t __b)\n+{\n+  return (__a & __b) ? -1ll : 0ll;\n }\n \n-/* vtbx3  */\n+/* vuqadd */\n \n __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vtbx3_s8 (int8x8_t __r, int8x8x3_t __tab, int8x8_t __idx)\n+vuqadd_s8 (int8x8_t __a, uint8x8_t __b)\n {\n-  uint8x8_t __mask = vclt_u8 (vreinterpret_u8_s8 (__idx),\n-\t\t\t      vmov_n_u8 (24));\n-  int8x8_t __tbl = vtbl3_s8 (__tab, __idx);\n-\n-  return vbsl_s8 (__mask, __tbl, __r);\n+  return (int8x8_t) __builtin_aarch64_suqaddv8qi (__a, (int8x8_t) __b);\n }\n \n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vtbx3_u8 (uint8x8_t __r, uint8x8x3_t __tab, uint8x8_t __idx)\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vuqadd_s16 (int16x4_t __a, uint16x4_t __b)\n {\n-  uint8x8_t __mask = vclt_u8 (__idx, vmov_n_u8 (24));\n-  uint8x8_t __tbl = vtbl3_u8 (__tab, __idx);\n-\n-  return vbsl_u8 (__mask, __tbl, __r);\n+  return (int16x4_t) __builtin_aarch64_suqaddv4hi (__a, (int16x4_t) __b);\n }\n \n-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n-vtbx3_p8 (poly8x8_t __r, poly8x8x3_t __tab, uint8x8_t __idx)\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vuqadd_s32 (int32x2_t __a, uint32x2_t __b)\n {\n-  uint8x8_t __mask = vclt_u8 (__idx, vmov_n_u8 (24));\n-  poly8x8_t __tbl = vtbl3_p8 (__tab, __idx);\n-\n-  return vbsl_p8 (__mask, __tbl, __r);\n+  return (int32x2_t) __builtin_aarch64_suqaddv2si (__a, (int32x2_t) __b);\n }\n \n-/* vtrn */\n-\n-__extension__ static __inline float32x2x2_t __attribute__ ((__always_inline__))\n-vtrn_f32 (float32x2_t a, float32x2_t b)\n+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n+vuqadd_s64 (int64x1_t __a, uint64x1_t __b)\n {\n-  return (float32x2x2_t) {vtrn1_f32 (a, b), vtrn2_f32 (a, b)};\n+  return (int64x1_t) __builtin_aarch64_suqadddi (__a, (int64x1_t) __b);\n }\n \n-__extension__ static __inline poly8x8x2_t __attribute__ ((__always_inline__))\n-vtrn_p8 (poly8x8_t a, poly8x8_t b)\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vuqaddq_s8 (int8x16_t __a, uint8x16_t __b)\n {\n-  return (poly8x8x2_t) {vtrn1_p8 (a, b), vtrn2_p8 (a, b)};\n+  return (int8x16_t) __builtin_aarch64_suqaddv16qi (__a, (int8x16_t) __b);\n }\n \n-__extension__ static __inline poly16x4x2_t __attribute__ ((__always_inline__))\n-vtrn_p16 (poly16x4_t a, poly16x4_t b)\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vuqaddq_s16 (int16x8_t __a, uint16x8_t __b)\n {\n-  return (poly16x4x2_t) {vtrn1_p16 (a, b), vtrn2_p16 (a, b)};\n+  return (int16x8_t) __builtin_aarch64_suqaddv8hi (__a, (int16x8_t) __b);\n }\n \n-__extension__ static __inline int8x8x2_t __attribute__ ((__always_inline__))\n-vtrn_s8 (int8x8_t a, int8x8_t b)\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vuqaddq_s32 (int32x4_t __a, uint32x4_t __b)\n {\n-  return (int8x8x2_t) {vtrn1_s8 (a, b), vtrn2_s8 (a, b)};\n+  return (int32x4_t) __builtin_aarch64_suqaddv4si (__a, (int32x4_t) __b);\n }\n \n-__extension__ static __inline int16x4x2_t __attribute__ ((__always_inline__))\n-vtrn_s16 (int16x4_t a, int16x4_t b)\n+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n+vuqaddq_s64 (int64x2_t __a, uint64x2_t __b)\n {\n-  return (int16x4x2_t) {vtrn1_s16 (a, b), vtrn2_s16 (a, b)};\n+  return (int64x2_t) __builtin_aarch64_suqaddv2di (__a, (int64x2_t) __b);\n }\n \n-__extension__ static __inline int32x2x2_t __attribute__ ((__always_inline__))\n-vtrn_s32 (int32x2_t a, int32x2_t b)\n+__extension__ static __inline int8x1_t __attribute__ ((__always_inline__))\n+vuqaddb_s8 (int8x1_t __a, uint8x1_t __b)\n {\n-  return (int32x2x2_t) {vtrn1_s32 (a, b), vtrn2_s32 (a, b)};\n+  return (int8x1_t) __builtin_aarch64_suqaddqi (__a, (int8x1_t) __b);\n }\n \n-__extension__ static __inline uint8x8x2_t __attribute__ ((__always_inline__))\n-vtrn_u8 (uint8x8_t a, uint8x8_t b)\n+__extension__ static __inline int16x1_t __attribute__ ((__always_inline__))\n+vuqaddh_s16 (int16x1_t __a, uint16x1_t __b)\n {\n-  return (uint8x8x2_t) {vtrn1_u8 (a, b), vtrn2_u8 (a, b)};\n+  return (int16x1_t) __builtin_aarch64_suqaddhi (__a, (int16x1_t) __b);\n }\n \n-__extension__ static __inline uint16x4x2_t __attribute__ ((__always_inline__))\n-vtrn_u16 (uint16x4_t a, uint16x4_t b)\n+__extension__ static __inline int32x1_t __attribute__ ((__always_inline__))\n+vuqadds_s32 (int32x1_t __a, uint32x1_t __b)\n {\n-  return (uint16x4x2_t) {vtrn1_u16 (a, b), vtrn2_u16 (a, b)};\n+  return (int32x1_t) __builtin_aarch64_suqaddsi (__a, (int32x1_t) __b);\n }\n \n-__extension__ static __inline uint32x2x2_t __attribute__ ((__always_inline__))\n-vtrn_u32 (uint32x2_t a, uint32x2_t b)\n+__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n+vuqaddd_s64 (int64x1_t __a, uint64x1_t __b)\n {\n-  return (uint32x2x2_t) {vtrn1_u32 (a, b), vtrn2_u32 (a, b)};\n+  return (int64x1_t) __builtin_aarch64_suqadddi (__a, (int64x1_t) __b);\n }\n \n-__extension__ static __inline float32x4x2_t __attribute__ ((__always_inline__))\n-vtrnq_f32 (float32x4_t a, float32x4_t b)\n+#define __DEFINTERLEAVE(op, rettype, intype, funcsuffix, Q) \t\t\\\n+  __extension__ static __inline rettype\t\t\t\t\t\\\n+  __attribute__ ((__always_inline__))\t\t\t\t\t\\\n+  v ## op ## Q ## _ ## funcsuffix (intype a, intype b)\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    return (rettype) {v ## op ## 1 ## Q ## _ ## funcsuffix (a, b),\t\\\n+\t\t      v ## op ## 2 ## Q ## _ ## funcsuffix (a, b)};\t\\\n+  }\n+\n+#define __INTERLEAVE_LIST(op)\t\t\t\t\t\\\n+  __DEFINTERLEAVE (op, float32x2x2_t, float32x2_t, f32,)\t\\\n+  __DEFINTERLEAVE (op, poly8x8x2_t, poly8x8_t, p8,)\t\t\\\n+  __DEFINTERLEAVE (op, poly16x4x2_t, poly16x4_t, p16,)\t\t\\\n+  __DEFINTERLEAVE (op, int8x8x2_t, int8x8_t, s8,)\t\t\\\n+  __DEFINTERLEAVE (op, int16x4x2_t, int16x4_t, s16,)\t\t\\\n+  __DEFINTERLEAVE (op, int32x2x2_t, int32x2_t, s32,)\t\t\\\n+  __DEFINTERLEAVE (op, uint8x8x2_t, uint8x8_t, u8,)\t\t\\\n+  __DEFINTERLEAVE (op, uint16x4x2_t, uint16x4_t, u16,)\t\t\\\n+  __DEFINTERLEAVE (op, uint32x2x2_t, uint32x2_t, u32,)\t\t\\\n+  __DEFINTERLEAVE (op, float32x4x2_t, float32x4_t, f32, q)\t\\\n+  __DEFINTERLEAVE (op, poly8x16x2_t, poly8x16_t, p8, q)\t\t\\\n+  __DEFINTERLEAVE (op, poly16x8x2_t, poly16x8_t, p16, q)\t\\\n+  __DEFINTERLEAVE (op, int8x16x2_t, int8x16_t, s8, q)\t\t\\\n+  __DEFINTERLEAVE (op, int16x8x2_t, int16x8_t, s16, q)\t\t\\\n+  __DEFINTERLEAVE (op, int32x4x2_t, int32x4_t, s32, q)\t\t\\\n+  __DEFINTERLEAVE (op, uint8x16x2_t, uint8x16_t, u8, q)\t\t\\\n+  __DEFINTERLEAVE (op, uint16x8x2_t, uint16x8_t, u16, q)\t\\\n+  __DEFINTERLEAVE (op, uint32x4x2_t, uint32x4_t, u32, q)\n+\n+/* vuzp */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vuzp1_f32 (float32x2_t __a, float32x2_t __b)\n {\n-  return (float32x4x2_t) {vtrn1q_f32 (a, b), vtrn2q_f32 (a, b)};\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {0, 2});\n+#endif\n }\n \n-__extension__ static __inline poly8x16x2_t __attribute__ ((__always_inline__))\n-vtrnq_p8 (poly8x16_t a, poly8x16_t b)\n+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n+vuzp1_p8 (poly8x8_t __a, poly8x8_t __b)\n {\n-  return (poly8x16x2_t) {vtrn1q_p8 (a, b), vtrn2q_p8 (a, b)};\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {9, 11, 13, 15, 1, 3, 5, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {0, 2, 4, 6, 8, 10, 12, 14});\n+#endif\n }\n \n-__extension__ static __inline poly16x8x2_t __attribute__ ((__always_inline__))\n-vtrnq_p16 (poly16x8_t a, poly16x8_t b)\n+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n+vuzp1_p16 (poly16x4_t __a, poly16x4_t __b)\n {\n-  return (poly16x8x2_t) {vtrn1q_p16 (a, b), vtrn2q_p16 (a, b)};\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {5, 7, 1, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {0, 2, 4, 6});\n+#endif\n }\n \n-__extension__ static __inline int8x16x2_t __attribute__ ((__always_inline__))\n-vtrnq_s8 (int8x16_t a, int8x16_t b)\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vuzp1_s8 (int8x8_t __a, int8x8_t __b)\n {\n-  return (int8x16x2_t) {vtrn1q_s8 (a, b), vtrn2q_s8 (a, b)};\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {9, 11, 13, 15, 1, 3, 5, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {0, 2, 4, 6, 8, 10, 12, 14});\n+#endif\n }\n \n-__extension__ static __inline int16x8x2_t __attribute__ ((__always_inline__))\n-vtrnq_s16 (int16x8_t a, int16x8_t b)\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vuzp1_s16 (int16x4_t __a, int16x4_t __b)\n {\n-  return (int16x8x2_t) {vtrn1q_s16 (a, b), vtrn2q_s16 (a, b)};\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {5, 7, 1, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {0, 2, 4, 6});\n+#endif\n }\n \n-__extension__ static __inline int32x4x2_t __attribute__ ((__always_inline__))\n-vtrnq_s32 (int32x4_t a, int32x4_t b)\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vuzp1_s32 (int32x2_t __a, int32x2_t __b)\n {\n-  return (int32x4x2_t) {vtrn1q_s32 (a, b), vtrn2q_s32 (a, b)};\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {0, 2});\n+#endif\n }\n \n-__extension__ static __inline uint8x16x2_t __attribute__ ((__always_inline__))\n-vtrnq_u8 (uint8x16_t a, uint8x16_t b)\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vuzp1_u8 (uint8x8_t __a, uint8x8_t __b)\n {\n-  return (uint8x16x2_t) {vtrn1q_u8 (a, b), vtrn2q_u8 (a, b)};\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {9, 11, 13, 15, 1, 3, 5, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {0, 2, 4, 6, 8, 10, 12, 14});\n+#endif\n }\n \n-__extension__ static __inline uint16x8x2_t __attribute__ ((__always_inline__))\n-vtrnq_u16 (uint16x8_t a, uint16x8_t b)\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vuzp1_u16 (uint16x4_t __a, uint16x4_t __b)\n {\n-  return (uint16x8x2_t) {vtrn1q_u16 (a, b), vtrn2q_u16 (a, b)};\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {5, 7, 1, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {0, 2, 4, 6});\n+#endif\n }\n \n-__extension__ static __inline uint32x4x2_t __attribute__ ((__always_inline__))\n-vtrnq_u32 (uint32x4_t a, uint32x4_t b)\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vuzp1_u32 (uint32x2_t __a, uint32x2_t __b)\n {\n-  return (uint32x4x2_t) {vtrn1q_u32 (a, b), vtrn2q_u32 (a, b)};\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {0, 2});\n+#endif\n }\n \n-/* vtst */\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vtst_s8 (int8x8_t __a, int8x8_t __b)\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vuzp1q_f32 (float32x4_t __a, float32x4_t __b)\n {\n-  return (uint8x8_t) __builtin_aarch64_cmtstv8qi (__a, __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {5, 7, 1, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {0, 2, 4, 6});\n+#endif\n }\n \n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vtst_s16 (int16x4_t __a, int16x4_t __b)\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vuzp1q_f64 (float64x2_t __a, float64x2_t __b)\n {\n-  return (uint16x4_t) __builtin_aarch64_cmtstv4hi (__a, __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {0, 2});\n+#endif\n }\n \n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vtst_s32 (int32x2_t __a, int32x2_t __b)\n+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n+vuzp1q_p8 (poly8x16_t __a, poly8x16_t __b)\n {\n-  return (uint32x2_t) __builtin_aarch64_cmtstv2si (__a, __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {17, 19, 21, 23, 25, 27, 29, 31, 1, 3, 5, 7, 9, 11, 13, 15});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30});\n+#endif\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vtst_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n+vuzp1q_p16 (poly16x8_t __a, poly16x8_t __b)\n {\n-  return (__a & __b) ? -1ll : 0ll;\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {9, 11, 13, 15, 1, 3, 5, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {0, 2, 4, 6, 8, 10, 12, 14});\n+#endif\n }\n \n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vtst_u8 (uint8x8_t __a, uint8x8_t __b)\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vuzp1q_s8 (int8x16_t __a, int8x16_t __b)\n {\n-  return (uint8x8_t) __builtin_aarch64_cmtstv8qi ((int8x8_t) __a,\n-\t\t\t\t\t\t (int8x8_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x16_t) {17, 19, 21, 23, 25, 27, 29, 31, 1, 3, 5, 7, 9, 11, 13, 15});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x16_t) {0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30});\n+#endif\n }\n \n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vtst_u16 (uint16x4_t __a, uint16x4_t __b)\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vuzp1q_s16 (int16x8_t __a, int16x8_t __b)\n {\n-  return (uint16x4_t) __builtin_aarch64_cmtstv4hi ((int16x4_t) __a,\n-\t\t\t\t\t\t  (int16x4_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {9, 11, 13, 15, 1, 3, 5, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {0, 2, 4, 6, 8, 10, 12, 14});\n+#endif\n }\n \n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vtst_u32 (uint32x2_t __a, uint32x2_t __b)\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vuzp1q_s32 (int32x4_t __a, int32x4_t __b)\n {\n-  return (uint32x2_t) __builtin_aarch64_cmtstv2si ((int32x2_t) __a,\n-\t\t\t\t\t\t  (int32x2_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {5, 7, 1, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {0, 2, 4, 6});\n+#endif\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vtst_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n+vuzp1q_s64 (int64x2_t __a, int64x2_t __b)\n {\n-  return (__a & __b) ? -1ll : 0ll;\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {0, 2});\n+#endif\n }\n \n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vtstq_s8 (int8x16_t __a, int8x16_t __b)\n+vuzp1q_u8 (uint8x16_t __a, uint8x16_t __b)\n {\n-  return (uint8x16_t) __builtin_aarch64_cmtstv16qi (__a, __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x16_t) {17, 19, 21, 23, 25, 27, 29, 31, 1, 3, 5, 7, 9, 11, 13, 15});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x16_t) {0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30});\n+#endif\n }\n \n __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vtstq_s16 (int16x8_t __a, int16x8_t __b)\n+vuzp1q_u16 (uint16x8_t __a, uint16x8_t __b)\n {\n-  return (uint16x8_t) __builtin_aarch64_cmtstv8hi (__a, __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {9, 11, 13, 15, 1, 3, 5, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {0, 2, 4, 6, 8, 10, 12, 14});\n+#endif\n }\n \n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vtstq_s32 (int32x4_t __a, int32x4_t __b)\n+vuzp1q_u32 (uint32x4_t __a, uint32x4_t __b)\n {\n-  return (uint32x4_t) __builtin_aarch64_cmtstv4si (__a, __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {5, 7, 1, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {0, 2, 4, 6});\n+#endif\n }\n \n __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vtstq_s64 (int64x2_t __a, int64x2_t __b)\n+vuzp1q_u64 (uint64x2_t __a, uint64x2_t __b)\n {\n-  return (uint64x2_t) __builtin_aarch64_cmtstv2di (__a, __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {0, 2});\n+#endif\n }\n \n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vtstq_u8 (uint8x16_t __a, uint8x16_t __b)\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vuzp2_f32 (float32x2_t __a, float32x2_t __b)\n {\n-  return (uint8x16_t) __builtin_aarch64_cmtstv16qi ((int8x16_t) __a,\n-\t\t\t\t\t\t   (int8x16_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {1, 3});\n+#endif\n }\n \n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vtstq_u16 (uint16x8_t __a, uint16x8_t __b)\n+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n+vuzp2_p8 (poly8x8_t __a, poly8x8_t __b)\n {\n-  return (uint16x8_t) __builtin_aarch64_cmtstv8hi ((int16x8_t) __a,\n-\t\t\t\t\t\t  (int16x8_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {8, 10, 12, 14, 0, 2, 4, 6});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {1, 3, 5, 7, 9, 11, 13, 15});\n+#endif\n }\n \n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vtstq_u32 (uint32x4_t __a, uint32x4_t __b)\n+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n+vuzp2_p16 (poly16x4_t __a, poly16x4_t __b)\n {\n-  return (uint32x4_t) __builtin_aarch64_cmtstv4si ((int32x4_t) __a,\n-\t\t\t\t\t\t  (int32x4_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {4, 6, 0, 2});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {1, 3, 5, 7});\n+#endif\n }\n \n-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vtstq_u64 (uint64x2_t __a, uint64x2_t __b)\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vuzp2_s8 (int8x8_t __a, int8x8_t __b)\n {\n-  return (uint64x2_t) __builtin_aarch64_cmtstv2di ((int64x2_t) __a,\n-\t\t\t\t\t\t  (int64x2_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {8, 10, 12, 14, 0, 2, 4, 6});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {1, 3, 5, 7, 9, 11, 13, 15});\n+#endif\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vtstd_s64 (int64x1_t __a, int64x1_t __b)\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vuzp2_s16 (int16x4_t __a, int16x4_t __b)\n {\n-  return (__a & __b) ? -1ll : 0ll;\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {4, 6, 0, 2});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {1, 3, 5, 7});\n+#endif\n }\n \n-__extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n-vtstd_u64 (uint64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vuzp2_s32 (int32x2_t __a, int32x2_t __b)\n {\n-  return (__a & __b) ? -1ll : 0ll;\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {1, 3});\n+#endif\n }\n \n-/* vuqadd */\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vuzp2_u8 (uint8x8_t __a, uint8x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {8, 10, 12, 14, 0, 2, 4, 6});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {1, 3, 5, 7, 9, 11, 13, 15});\n+#endif\n+}\n \n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vuqadd_s8 (int8x8_t __a, uint8x8_t __b)\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vuzp2_u16 (uint16x4_t __a, uint16x4_t __b)\n {\n-  return (int8x8_t) __builtin_aarch64_suqaddv8qi (__a, (int8x8_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {4, 6, 0, 2});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {1, 3, 5, 7});\n+#endif\n }\n \n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vuqadd_s16 (int16x4_t __a, uint16x4_t __b)\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vuzp2_u32 (uint32x2_t __a, uint32x2_t __b)\n {\n-  return (int16x4_t) __builtin_aarch64_suqaddv4hi (__a, (int16x4_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {1, 3});\n+#endif\n }\n \n-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n-vuqadd_s32 (int32x2_t __a, uint32x2_t __b)\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vuzp2q_f32 (float32x4_t __a, float32x4_t __b)\n {\n-  return (int32x2_t) __builtin_aarch64_suqaddv2si (__a, (int32x2_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {4, 6, 0, 2});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {1, 3, 5, 7});\n+#endif\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vuqadd_s64 (int64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vuzp2q_f64 (float64x2_t __a, float64x2_t __b)\n {\n-  return (int64x1_t) __builtin_aarch64_suqadddi (__a, (int64x1_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {1, 3});\n+#endif\n+}\n+\n+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n+vuzp2q_p8 (poly8x16_t __a, poly8x16_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x16_t) {16, 18, 20, 22, 24, 26, 28, 30, 0, 2, 4, 6, 8, 10, 12, 14});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x16_t) {1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31});\n+#endif\n+}\n+\n+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n+vuzp2q_p16 (poly16x8_t __a, poly16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {8, 10, 12, 14, 0, 2, 4, 6});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {1, 3, 5, 7, 9, 11, 13, 15});\n+#endif\n }\n \n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vuqaddq_s8 (int8x16_t __a, uint8x16_t __b)\n+vuzp2q_s8 (int8x16_t __a, int8x16_t __b)\n {\n-  return (int8x16_t) __builtin_aarch64_suqaddv16qi (__a, (int8x16_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x16_t) {16, 18, 20, 22, 24, 26, 28, 30, 0, 2, 4, 6, 8, 10, 12, 14});\n+#else\n+  return __builtin_shuffle (__a, __b,\n+      (uint8x16_t) {1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31});\n+#endif\n }\n \n __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vuqaddq_s16 (int16x8_t __a, uint16x8_t __b)\n+vuzp2q_s16 (int16x8_t __a, int16x8_t __b)\n {\n-  return (int16x8_t) __builtin_aarch64_suqaddv8hi (__a, (int16x8_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {8, 10, 12, 14, 0, 2, 4, 6});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {1, 3, 5, 7, 9, 11, 13, 15});\n+#endif\n }\n \n __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n-vuqaddq_s32 (int32x4_t __a, uint32x4_t __b)\n+vuzp2q_s32 (int32x4_t __a, int32x4_t __b)\n {\n-  return (int32x4_t) __builtin_aarch64_suqaddv4si (__a, (int32x4_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {4, 6, 0, 2});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {1, 3, 5, 7});\n+#endif\n }\n \n __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n-vuqaddq_s64 (int64x2_t __a, uint64x2_t __b)\n+vuzp2q_s64 (int64x2_t __a, int64x2_t __b)\n {\n-  return (int64x2_t) __builtin_aarch64_suqaddv2di (__a, (int64x2_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {1, 3});\n+#endif\n }\n \n-__extension__ static __inline int8x1_t __attribute__ ((__always_inline__))\n-vuqaddb_s8 (int8x1_t __a, uint8x1_t __b)\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vuzp2q_u8 (uint8x16_t __a, uint8x16_t __b)\n {\n-  return (int8x1_t) __builtin_aarch64_suqaddqi (__a, (int8x1_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {16, 18, 20, 22, 24, 26, 28, 30, 0, 2, 4, 6, 8, 10, 12, 14});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31});\n+#endif\n }\n \n-__extension__ static __inline int16x1_t __attribute__ ((__always_inline__))\n-vuqaddh_s16 (int16x1_t __a, uint16x1_t __b)\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vuzp2q_u16 (uint16x8_t __a, uint16x8_t __b)\n {\n-  return (int16x1_t) __builtin_aarch64_suqaddhi (__a, (int16x1_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {8, 10, 12, 14, 0, 2, 4, 6});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {1, 3, 5, 7, 9, 11, 13, 15});\n+#endif\n }\n \n-__extension__ static __inline int32x1_t __attribute__ ((__always_inline__))\n-vuqadds_s32 (int32x1_t __a, uint32x1_t __b)\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vuzp2q_u32 (uint32x4_t __a, uint32x4_t __b)\n {\n-  return (int32x1_t) __builtin_aarch64_suqaddsi (__a, (int32x1_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {4, 6, 0, 2});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {1, 3, 5, 7});\n+#endif\n }\n \n-__extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n-vuqaddd_s64 (int64x1_t __a, uint64x1_t __b)\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vuzp2q_u64 (uint64x2_t __a, uint64x2_t __b)\n {\n-  return (int64x1_t) __builtin_aarch64_suqadddi (__a, (int64x1_t) __b);\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {1, 3});\n+#endif\n }\n \n-#define __DEFINTERLEAVE(op, rettype, intype, funcsuffix, Q) \t\t\\\n-  __extension__ static __inline rettype\t\t\t\t\t\\\n-  __attribute__ ((__always_inline__))\t\t\t\t\t\\\n-  v ## op ## Q ## _ ## funcsuffix (intype a, intype b)\t\t\t\\\n-  {\t\t\t\t\t\t\t\t\t\\\n-    return (rettype) {v ## op ## 1 ## Q ## _ ## funcsuffix (a, b),\t\\\n-\t\t      v ## op ## 2 ## Q ## _ ## funcsuffix (a, b)};\t\\\n-  }\n-\n-#define __INTERLEAVE_LIST(op)\t\t\t\t\t\\\n-  __DEFINTERLEAVE (op, float32x2x2_t, float32x2_t, f32,)\t\\\n-  __DEFINTERLEAVE (op, poly8x8x2_t, poly8x8_t, p8,)\t\t\\\n-  __DEFINTERLEAVE (op, poly16x4x2_t, poly16x4_t, p16,)\t\t\\\n-  __DEFINTERLEAVE (op, int8x8x2_t, int8x8_t, s8,)\t\t\\\n-  __DEFINTERLEAVE (op, int16x4x2_t, int16x4_t, s16,)\t\t\\\n-  __DEFINTERLEAVE (op, int32x2x2_t, int32x2_t, s32,)\t\t\\\n-  __DEFINTERLEAVE (op, uint8x8x2_t, uint8x8_t, u8,)\t\t\\\n-  __DEFINTERLEAVE (op, uint16x4x2_t, uint16x4_t, u16,)\t\t\\\n-  __DEFINTERLEAVE (op, uint32x2x2_t, uint32x2_t, u32,)\t\t\\\n-  __DEFINTERLEAVE (op, float32x4x2_t, float32x4_t, f32, q)\t\\\n-  __DEFINTERLEAVE (op, poly8x16x2_t, poly8x16_t, p8, q)\t\t\\\n-  __DEFINTERLEAVE (op, poly16x8x2_t, poly16x8_t, p16, q)\t\\\n-  __DEFINTERLEAVE (op, int8x16x2_t, int8x16_t, s8, q)\t\t\\\n-  __DEFINTERLEAVE (op, int16x8x2_t, int16x8_t, s16, q)\t\t\\\n-  __DEFINTERLEAVE (op, int32x4x2_t, int32x4_t, s32, q)\t\t\\\n-  __DEFINTERLEAVE (op, uint8x16x2_t, uint8x16_t, u8, q)\t\t\\\n-  __DEFINTERLEAVE (op, uint16x8x2_t, uint16x8_t, u16, q)\t\\\n-  __DEFINTERLEAVE (op, uint32x4x2_t, uint32x4_t, u32, q)\n-\n-/* vuzp */\n-\n __INTERLEAVE_LIST (uzp)\n \n /* vzip */"}, {"sha": "00ae0d12f54d8cbb28ba40132cf544d1aa49ae2e", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=7211512a5f817ad08e333774bd186c0c5ff48533", "patch": "@@ -1,3 +1,9 @@\n+2014-04-30  Alan Lawrence  <alan.lawrence@arm.com>\n+\n+\t* gcc.target/aarch64/vuzps32_1.c: Expect zip1/2 insn rather than uzp1/2.\n+\t* gcc.target/aarch64/vuzpu32_1.c: Likewise.\n+\t* gcc.target/aarch64/vuzpf32_1.c: Likewise.\n+\n 2014-04-30  Alan Lawrence  <alan.lawrence@arm.com>\n \n \t* gcc.target/aarch64/simd/vuzpf32_1.c: New file."}, {"sha": "0daba1c7f937937f33c98f55520ac3d272be0c46", "filename": "gcc/testsuite/gcc.target/aarch64/simd/vuzpf32_1.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvuzpf32_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvuzpf32_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvuzpf32_1.c?ref=7211512a5f817ad08e333774bd186c0c5ff48533", "patch": "@@ -6,6 +6,6 @@\n #include <arm_neon.h>\n #include \"vuzpf32.x\"\n \n-/* { dg-final { scan-assembler-times \"uzp1\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n-/* { dg-final { scan-assembler-times \"uzp2\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n+/* { dg-final { scan-assembler-times \"zip1\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n+/* { dg-final { scan-assembler-times \"zip2\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n /* { dg-final { cleanup-saved-temps } } */"}, {"sha": "af48d63a67e217422dbaf50bcac4b3e7d1713460", "filename": "gcc/testsuite/gcc.target/aarch64/simd/vuzps32_1.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvuzps32_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvuzps32_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvuzps32_1.c?ref=7211512a5f817ad08e333774bd186c0c5ff48533", "patch": "@@ -6,6 +6,6 @@\n #include <arm_neon.h>\n #include \"vuzps32.x\"\n \n-/* { dg-final { scan-assembler-times \"uzp1\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n-/* { dg-final { scan-assembler-times \"uzp2\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n+/* { dg-final { scan-assembler-times \"zip1\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n+/* { dg-final { scan-assembler-times \"zip2\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n /* { dg-final { cleanup-saved-temps } } */"}, {"sha": "05e1c95d42d8b31ae814b54fa34bcb0514e6373d", "filename": "gcc/testsuite/gcc.target/aarch64/simd/vuzpu32_1.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvuzpu32_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7211512a5f817ad08e333774bd186c0c5ff48533/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvuzpu32_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvuzpu32_1.c?ref=7211512a5f817ad08e333774bd186c0c5ff48533", "patch": "@@ -6,6 +6,6 @@\n #include <arm_neon.h>\n #include \"vuzpu32.x\"\n \n-/* { dg-final { scan-assembler-times \"uzp1\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n-/* { dg-final { scan-assembler-times \"uzp2\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n+/* { dg-final { scan-assembler-times \"zip1\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n+/* { dg-final { scan-assembler-times \"zip2\\[ \\t\\]+v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s, ?v\\[0-9\\]+\\.2s!?\\(?:\\[ \\t\\]+@\\[a-zA-Z0-9 \\]+\\)?\\n\" 1 } } */\n /* { dg-final { cleanup-saved-temps } } */"}]}