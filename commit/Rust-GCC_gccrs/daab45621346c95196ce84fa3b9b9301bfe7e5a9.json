{"sha": "daab45621346c95196ce84fa3b9b9301bfe7e5a9", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZGFhYjQ1NjIxMzQ2Yzk1MTk2Y2U4NGZhM2I5YjkzMDFiZmU3ZTVhOQ==", "commit": {"author": {"name": "Jiong Wang", "email": "jiong.wang@arm.com", "date": "2016-05-17T16:39:39Z"}, "committer": {"name": "Jiong Wang", "email": "jiwang@gcc.gnu.org", "date": "2016-05-17T16:39:39Z"}, "message": "[AArch64, 3/4] Reimplement multiply by element to get rid of inline assembly\n\ngcc/\n\t* config/aarch64/aarch64-simd.md (vmul_n_f32): Remove inline assembly.\n\tUse builtin.\n\t(vmul_n_s16): Likewise.\n\t(vmul_n_s32): Likewise.\n\t(vmul_n_u16): Likewise.\n\t(vmul_n_u32): Likewise.\n\t(vmulq_n_f32): Likewise.\n\t(vmulq_n_f64): Likewise.\n\t(vmulq_n_s16): Likewise.\n\t(vmulq_n_s32): Likewise.\n\t(vmulq_n_u16): Likewise.\n\t(vmulq_n_u32): Likewise.\n\ngcc/testsuite/\n\t* gcc.target/aarch64/simd/vmul_elem_1.c: Use intrinsics.\n\nFrom-SVN: r236333", "tree": {"sha": "cdb3cb878c699d53fb3399281f591dda61194fcb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/cdb3cb878c699d53fb3399281f591dda61194fcb"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/daab45621346c95196ce84fa3b9b9301bfe7e5a9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/daab45621346c95196ce84fa3b9b9301bfe7e5a9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/daab45621346c95196ce84fa3b9b9301bfe7e5a9", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/daab45621346c95196ce84fa3b9b9301bfe7e5a9/comments", "author": null, "committer": null, "parents": [{"sha": "22330033389ee5b16f5923ace84ad551bd778adf", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/22330033389ee5b16f5923ace84ad551bd778adf", "html_url": "https://github.com/Rust-GCC/gccrs/commit/22330033389ee5b16f5923ace84ad551bd778adf"}], "stats": {"total": 296, "additions": 131, "deletions": 165}, "files": [{"sha": "2030c85be73bf0702131d24a8c4c646ddd18de99", "filename": "gcc/ChangeLog", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/daab45621346c95196ce84fa3b9b9301bfe7e5a9/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/daab45621346c95196ce84fa3b9b9301bfe7e5a9/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=daab45621346c95196ce84fa3b9b9301bfe7e5a9", "patch": "@@ -1,3 +1,18 @@\n+2016-05-17  Jiong Wang  <jiong.wang@arm.com>\n+\n+\t* config/aarch64/aarch64-simd.md (vmul_n_f32): Remove inline assembly.\n+\tUse builtin.\n+\t(vmul_n_s16): Likewise.\n+\t(vmul_n_s32): Likewise.\n+\t(vmul_n_u16): Likewise.\n+\t(vmul_n_u32): Likewise.\n+\t(vmulq_n_f32): Likewise.\n+\t(vmulq_n_f64): Likewise.\n+\t(vmulq_n_s16): Likewise.\n+\t(vmulq_n_s32): Likewise.\n+\t(vmulq_n_u16): Likewise.\n+\t(vmulq_n_u32): Likewise.\n+\n 2016-05-17  Jiong Wang  <jiong.wang@arm.com>\n \n \t* config/aarch64/aarch64-simd.md (*aarch64_mul3_elt_to_128df): Extend to"}, {"sha": "84931aeec2d885f8552197fe8a72500f127e2bbb", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 68, "deletions": 121, "changes": 189, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/daab45621346c95196ce84fa3b9b9301bfe7e5a9/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/daab45621346c95196ce84fa3b9b9301bfe7e5a9/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=daab45621346c95196ce84fa3b9b9301bfe7e5a9", "patch": "@@ -7938,61 +7938,6 @@ vmovn_u64 (uint64x2_t a)\n   return result;\n }\n \n-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n-vmul_n_f32 (float32x2_t a, float32_t b)\n-{\n-  float32x2_t result;\n-  __asm__ (\"fmul %0.2s,%1.2s,%2.s[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vmul_n_s16 (int16x4_t a, int16_t b)\n-{\n-  int16x4_t result;\n-  __asm__ (\"mul %0.4h,%1.4h,%2.h[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"x\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n-vmul_n_s32 (int32x2_t a, int32_t b)\n-{\n-  int32x2_t result;\n-  __asm__ (\"mul %0.2s,%1.2s,%2.s[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vmul_n_u16 (uint16x4_t a, uint16_t b)\n-{\n-  uint16x4_t result;\n-  __asm__ (\"mul %0.4h,%1.4h,%2.h[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"x\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vmul_n_u32 (uint32x2_t a, uint32_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"mul %0.2s,%1.2s,%2.s[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n #define vmull_high_lane_s16(a, b, c)                                    \\\n   __extension__                                                         \\\n     ({                                                                  \\\n@@ -8443,72 +8388,6 @@ vmull_u32 (uint32x2_t a, uint32x2_t b)\n   return result;\n }\n \n-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n-vmulq_n_f32 (float32x4_t a, float32_t b)\n-{\n-  float32x4_t result;\n-  __asm__ (\"fmul %0.4s,%1.4s,%2.s[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n-vmulq_n_f64 (float64x2_t a, float64_t b)\n-{\n-  float64x2_t result;\n-  __asm__ (\"fmul %0.2d,%1.2d,%2.d[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vmulq_n_s16 (int16x8_t a, int16_t b)\n-{\n-  int16x8_t result;\n-  __asm__ (\"mul %0.8h,%1.8h,%2.h[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"x\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n-vmulq_n_s32 (int32x4_t a, int32_t b)\n-{\n-  int32x4_t result;\n-  __asm__ (\"mul %0.4s,%1.4s,%2.s[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vmulq_n_u16 (uint16x8_t a, uint16_t b)\n-{\n-  uint16x8_t result;\n-  __asm__ (\"mul %0.8h,%1.8h,%2.h[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"x\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vmulq_n_u32 (uint32x4_t a, uint32_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"mul %0.4s,%1.4s,%2.s[0]\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n vmvn_p8 (poly8x8_t a)\n {\n@@ -18924,6 +18803,74 @@ vmulq_laneq_u32 (uint32x4_t __a, uint32x4_t __b, const int __lane)\n   return __a * __aarch64_vget_lane_any (__b, __lane);\n }\n \n+/* vmul_n.  */\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vmul_n_f32 (float32x2_t __a, float32_t __b)\n+{\n+  return __a * __b;\n+}\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vmulq_n_f32 (float32x4_t __a, float32_t __b)\n+{\n+  return __a * __b;\n+}\n+\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vmulq_n_f64 (float64x2_t __a, float64_t __b)\n+{\n+  return __a * __b;\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vmul_n_s16 (int16x4_t __a, int16_t __b)\n+{\n+  return __a * __b;\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vmulq_n_s16 (int16x8_t __a, int16_t __b)\n+{\n+  return __a * __b;\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vmul_n_s32 (int32x2_t __a, int32_t __b)\n+{\n+  return __a * __b;\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vmulq_n_s32 (int32x4_t __a, int32_t __b)\n+{\n+  return __a * __b;\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vmul_n_u16 (uint16x4_t __a, uint16_t __b)\n+{\n+  return __a * __b;\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vmulq_n_u16 (uint16x8_t __a, uint16_t __b)\n+{\n+  return __a * __b;\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vmul_n_u32 (uint32x2_t __a, uint32_t __b)\n+{\n+  return __a * __b;\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vmulq_n_u32 (uint32x4_t __a, uint32_t __b)\n+{\n+  return __a * __b;\n+}\n+\n /* vneg  */\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))"}, {"sha": "b507061213b5a0b28507ddc29d58e08f9fd078b7", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/daab45621346c95196ce84fa3b9b9301bfe7e5a9/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/daab45621346c95196ce84fa3b9b9301bfe7e5a9/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=daab45621346c95196ce84fa3b9b9301bfe7e5a9", "patch": "@@ -1,3 +1,7 @@\n+2016-05-17  Jiong Wang  <jiong.wang@arm.com>\n+\n+\t* gcc.target/aarch64/simd/vmul_elem_1.c: Use intrinsics.\n+\n 2016-05-17  Jiong Wang  <jiong.wang@arm.com>\n \n \t* gcc.target/aarch64/simd/vmul_elem_1.c: New."}, {"sha": "155cac3b4a5579318244533c3ab590250c150dd6", "filename": "gcc/testsuite/gcc.target/aarch64/simd/vmul_elem_1.c", "status": "modified", "additions": 44, "deletions": 44, "changes": 88, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/daab45621346c95196ce84fa3b9b9301bfe7e5a9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvmul_elem_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/daab45621346c95196ce84fa3b9b9301bfe7e5a9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvmul_elem_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvmul_elem_1.c?ref=daab45621346c95196ce84fa3b9b9301bfe7e5a9", "patch": "@@ -142,13 +142,13 @@ check_v2sf (float32_t elemA, float32_t elemB)\n   int32_t indx;\n   const float32_t vec32x2_buf[2] = {A, B};\n   float32x2_t vec32x2_src = vld1_f32 (vec32x2_buf);\n-  float32x2_t vec32x2_res = vec32x2_src * elemA;\n+  float32x2_t vec32x2_res = vmul_n_f32 (vec32x2_src, elemA);\n \n   for (indx = 0; indx < 2; indx++)\n     if (* (uint32_t *) &vec32x2_res[indx] != * (uint32_t *) &expected2_1[indx])\n       abort ();\n \n-  vec32x2_res = vec32x2_src * elemB;\n+  vec32x2_res = vmul_n_f32 (vec32x2_src, elemB);\n \n   for (indx = 0; indx < 2; indx++)\n     if (* (uint32_t *) &vec32x2_res[indx] != * (uint32_t *) &expected2_2[indx])\n@@ -163,25 +163,25 @@ check_v4sf (float32_t elemA, float32_t elemB, float32_t elemC, float32_t elemD)\n   int32_t indx;\n   const float32_t vec32x4_buf[4] = {A, B, C, D};\n   float32x4_t vec32x4_src = vld1q_f32 (vec32x4_buf);\n-  float32x4_t vec32x4_res = vec32x4_src * elemA;\n+  float32x4_t vec32x4_res = vmulq_n_f32 (vec32x4_src, elemA);\n \n   for (indx = 0; indx < 4; indx++)\n     if (* (uint32_t *) &vec32x4_res[indx] != * (uint32_t *) &expected4_1[indx])\n       abort ();\n \n-  vec32x4_res = vec32x4_src * elemB;\n+  vec32x4_res = vmulq_n_f32 (vec32x4_src, elemB);\n \n   for (indx = 0; indx < 4; indx++)\n     if (* (uint32_t *) &vec32x4_res[indx] != * (uint32_t *) &expected4_2[indx])\n       abort ();\n \n-  vec32x4_res = vec32x4_src * elemC;\n+  vec32x4_res = vmulq_n_f32 (vec32x4_src, elemC);\n \n   for (indx = 0; indx < 4; indx++)\n     if (* (uint32_t *) &vec32x4_res[indx] != * (uint32_t *) &expected4_3[indx])\n       abort ();\n \n-  vec32x4_res = vec32x4_src * elemD;\n+  vec32x4_res = vmulq_n_f32 (vec32x4_src, elemD);\n \n   for (indx = 0; indx < 4; indx++)\n     if (* (uint32_t *) &vec32x4_res[indx] != * (uint32_t *) &expected4_4[indx])\n@@ -196,13 +196,13 @@ check_v2df (float64_t elemdC, float64_t elemdD)\n   int32_t indx;\n   const float64_t vec64x2_buf[2] = {AD, BD};\n   float64x2_t vec64x2_src = vld1q_f64 (vec64x2_buf);\n-  float64x2_t vec64x2_res = vec64x2_src * elemdC;\n+  float64x2_t vec64x2_res = vmulq_n_f64 (vec64x2_src, elemdC);\n \n   for (indx = 0; indx < 2; indx++)\n     if (* (uint64_t *) &vec64x2_res[indx] != * (uint64_t *) &expectedd2_1[indx])\n       abort ();\n \n-  vec64x2_res = vec64x2_src * elemdD;\n+  vec64x2_res = vmulq_n_f64 (vec64x2_src, elemdD);\n \n   for (indx = 0; indx < 2; indx++)\n     if (* (uint64_t *) &vec64x2_res[indx] != * (uint64_t *) &expectedd2_2[indx])\n@@ -217,13 +217,13 @@ check_v2si (int32_t elemsA, int32_t elemsB)\n   int32_t indx;\n   const int32_t vecs32x2_buf[2] = {AS, BS};\n   int32x2_t vecs32x2_src = vld1_s32 (vecs32x2_buf);\n-  int32x2_t vecs32x2_res = vecs32x2_src * elemsA;\n+  int32x2_t vecs32x2_res = vmul_n_s32 (vecs32x2_src, elemsA);\n \n   for (indx = 0; indx < 2; indx++)\n     if (vecs32x2_res[indx] != expecteds2_1[indx])\n       abort ();\n \n-  vecs32x2_res = vecs32x2_src * elemsB;\n+  vecs32x2_res = vmul_n_s32 (vecs32x2_src, elemsB);\n \n   for (indx = 0; indx < 2; indx++)\n     if (vecs32x2_res[indx] != expecteds2_2[indx])\n@@ -236,13 +236,13 @@ check_v2si_unsigned (uint32_t elemusA, uint32_t elemusB)\n   int indx;\n   const uint32_t vecus32x2_buf[2] = {AUS, BUS};\n   uint32x2_t vecus32x2_src = vld1_u32 (vecus32x2_buf);\n-  uint32x2_t vecus32x2_res = vecus32x2_src * elemusA;\n+  uint32x2_t vecus32x2_res = vmul_n_u32 (vecus32x2_src, elemusA);\n \n   for (indx = 0; indx < 2; indx++)\n     if (vecus32x2_res[indx] != expectedus2_1[indx])\n       abort ();\n \n-  vecus32x2_res = vecus32x2_src * elemusB;\n+  vecus32x2_res = vmul_n_u32 (vecus32x2_src, elemusB);\n \n   for (indx = 0; indx < 2; indx++)\n     if (vecus32x2_res[indx] != expectedus2_2[indx])\n@@ -257,25 +257,25 @@ check_v4si (int32_t elemsA, int32_t elemsB, int32_t elemsC, int32_t elemsD)\n   int32_t indx;\n   const int32_t vecs32x4_buf[4] = {AS, BS, CS, DS};\n   int32x4_t vecs32x4_src = vld1q_s32 (vecs32x4_buf);\n-  int32x4_t vecs32x4_res = vecs32x4_src * elemsA;\n+  int32x4_t vecs32x4_res = vmulq_n_s32 (vecs32x4_src, elemsA);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecs32x4_res[indx] != expecteds4_1[indx])\n       abort ();\n \n-  vecs32x4_res = vecs32x4_src * elemsB;\n+  vecs32x4_res = vmulq_n_s32 (vecs32x4_src, elemsB);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecs32x4_res[indx] != expecteds4_2[indx])\n       abort ();\n \n-  vecs32x4_res = vecs32x4_src * elemsC;\n+  vecs32x4_res = vmulq_n_s32 (vecs32x4_src, elemsC);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecs32x4_res[indx] != expecteds4_3[indx])\n       abort ();\n \n-  vecs32x4_res = vecs32x4_src * elemsD;\n+  vecs32x4_res = vmulq_n_s32 (vecs32x4_src, elemsD);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecs32x4_res[indx] != expecteds4_4[indx])\n@@ -289,25 +289,25 @@ check_v4si_unsigned (uint32_t elemusA, uint32_t elemusB, uint32_t elemusC,\n   int indx;\n   const uint32_t vecus32x4_buf[4] = {AUS, BUS, CUS, DUS};\n   uint32x4_t vecus32x4_src = vld1q_u32 (vecus32x4_buf);\n-  uint32x4_t vecus32x4_res = vecus32x4_src * elemusA;\n+  uint32x4_t vecus32x4_res = vmulq_n_u32 (vecus32x4_src, elemusA);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecus32x4_res[indx] != expectedus4_1[indx])\n       abort ();\n \n-  vecus32x4_res = vecus32x4_src * elemusB;\n+  vecus32x4_res = vmulq_n_u32 (vecus32x4_src, elemusB);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecus32x4_res[indx] != expectedus4_2[indx])\n       abort ();\n \n-  vecus32x4_res = vecus32x4_src * elemusC;\n+  vecus32x4_res = vmulq_n_u32 (vecus32x4_src, elemusC);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecus32x4_res[indx] != expectedus4_3[indx])\n       abort ();\n \n-  vecus32x4_res = vecus32x4_src * elemusD;\n+  vecus32x4_res = vmulq_n_u32 (vecus32x4_src, elemusD);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecus32x4_res[indx] != expectedus4_4[indx])\n@@ -323,25 +323,25 @@ check_v4hi (int16_t elemhA, int16_t elemhB, int16_t elemhC, int16_t elemhD)\n   int32_t indx;\n   const int16_t vech16x4_buf[4] = {AH, BH, CH, DH};\n   int16x4_t vech16x4_src = vld1_s16 (vech16x4_buf);\n-  int16x4_t vech16x4_res = vech16x4_src * elemhA;\n+  int16x4_t vech16x4_res = vmul_n_s16 (vech16x4_src, elemhA);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vech16x4_res[indx] != expectedh4_1[indx])\n       abort ();\n \n-  vech16x4_res = vech16x4_src * elemhB;\n+  vech16x4_res = vmul_n_s16 (vech16x4_src, elemhB);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vech16x4_res[indx] != expectedh4_2[indx])\n       abort ();\n \n-  vech16x4_res = vech16x4_src * elemhC;\n+  vech16x4_res = vmul_n_s16 (vech16x4_src, elemhC);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vech16x4_res[indx] != expectedh4_3[indx])\n       abort ();\n \n-  vech16x4_res = vech16x4_src * elemhD;\n+  vech16x4_res = vmul_n_s16 (vech16x4_src, elemhD);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vech16x4_res[indx] != expectedh4_4[indx])\n@@ -355,25 +355,25 @@ check_v4hi_unsigned (uint16_t elemuhA, uint16_t elemuhB, uint16_t elemuhC,\n   int indx;\n   const uint16_t vecuh16x4_buf[4] = {AUH, BUH, CUH, DUH};\n   uint16x4_t vecuh16x4_src = vld1_u16 (vecuh16x4_buf);\n-  uint16x4_t vecuh16x4_res = vecuh16x4_src * elemuhA;\n+  uint16x4_t vecuh16x4_res = vmul_n_u16 (vecuh16x4_src, elemuhA);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecuh16x4_res[indx] != expecteduh4_1[indx])\n       abort ();\n \n-  vecuh16x4_res = vecuh16x4_src * elemuhB;\n+  vecuh16x4_res = vmul_n_u16 (vecuh16x4_src, elemuhB);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecuh16x4_res[indx] != expecteduh4_2[indx])\n       abort ();\n \n-  vecuh16x4_res = vecuh16x4_src * elemuhC;\n+  vecuh16x4_res = vmul_n_u16 (vecuh16x4_src, elemuhC);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecuh16x4_res[indx] != expecteduh4_3[indx])\n       abort ();\n \n-  vecuh16x4_res = vecuh16x4_src * elemuhD;\n+  vecuh16x4_res = vmul_n_u16 (vecuh16x4_src, elemuhD);\n \n   for (indx = 0; indx < 4; indx++)\n     if (vecuh16x4_res[indx] != expecteduh4_4[indx])\n@@ -389,49 +389,49 @@ check_v8hi (int16_t elemhA, int16_t elemhB, int16_t elemhC, int16_t elemhD,\n   int32_t indx;\n   const int16_t vech16x8_buf[8] = {AH, BH, CH, DH, EH, FH, GH, HH};\n   int16x8_t vech16x8_src = vld1q_s16 (vech16x8_buf);\n-  int16x8_t vech16x8_res = vech16x8_src * elemhA;\n+  int16x8_t vech16x8_res = vmulq_n_s16 (vech16x8_src, elemhA);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vech16x8_res[indx] != expectedh8_1[indx])\n       abort ();\n \n-  vech16x8_res = vech16x8_src * elemhB;\n+  vech16x8_res = vmulq_n_s16 (vech16x8_src, elemhB);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vech16x8_res[indx] != expectedh8_2[indx])\n       abort ();\n \n-  vech16x8_res = vech16x8_src * elemhC;\n+  vech16x8_res = vmulq_n_s16 (vech16x8_src, elemhC);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vech16x8_res[indx] != expectedh8_3[indx])\n       abort ();\n \n-  vech16x8_res = vech16x8_src * elemhD;\n+  vech16x8_res = vmulq_n_s16 (vech16x8_src, elemhD);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vech16x8_res[indx] != expectedh8_4[indx])\n       abort ();\n \n-  vech16x8_res = vech16x8_src * elemhE;\n+  vech16x8_res = vmulq_n_s16 (vech16x8_src, elemhE);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vech16x8_res[indx] != expectedh8_5[indx])\n       abort ();\n \n-  vech16x8_res = vech16x8_src * elemhF;\n+  vech16x8_res = vmulq_n_s16 (vech16x8_src, elemhF);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vech16x8_res[indx] != expectedh8_6[indx])\n       abort ();\n \n-  vech16x8_res = vech16x8_src * elemhG;\n+  vech16x8_res = vmulq_n_s16 (vech16x8_src, elemhG);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vech16x8_res[indx] != expectedh8_7[indx])\n       abort ();\n \n-  vech16x8_res = vech16x8_src * elemhH;\n+  vech16x8_res = vmulq_n_s16 (vech16x8_src, elemhH);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vech16x8_res[indx] != expectedh8_8[indx])\n@@ -446,49 +446,49 @@ check_v8hi_unsigned (uint16_t elemuhA, uint16_t elemuhB, uint16_t elemuhC,\n   int indx;\n   const uint16_t vecuh16x8_buf[8] = {AUH, BUH, CUH, DUH, EUH, FUH, GUH, HUH};\n   uint16x8_t vecuh16x8_src = vld1q_u16 (vecuh16x8_buf);\n-  uint16x8_t vecuh16x8_res = vecuh16x8_src * elemuhA;\n+  uint16x8_t vecuh16x8_res = vmulq_n_u16 (vecuh16x8_src, elemuhA);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vecuh16x8_res[indx] != expecteduh8_1[indx])\n       abort ();\n \n-  vecuh16x8_res = vecuh16x8_src * elemuhB;\n+  vecuh16x8_res = vmulq_n_u16 (vecuh16x8_src, elemuhB);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vecuh16x8_res[indx] != expecteduh8_2[indx])\n       abort ();\n \n-  vecuh16x8_res = vecuh16x8_src * elemuhC;\n+  vecuh16x8_res = vmulq_n_u16 (vecuh16x8_src, elemuhC);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vecuh16x8_res[indx] != expecteduh8_3[indx])\n       abort ();\n \n-  vecuh16x8_res = vecuh16x8_src * elemuhD;\n+  vecuh16x8_res = vmulq_n_u16 (vecuh16x8_src, elemuhD);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vecuh16x8_res[indx] != expecteduh8_4[indx])\n       abort ();\n \n-  vecuh16x8_res = vecuh16x8_src * elemuhE;\n+  vecuh16x8_res = vmulq_n_u16 (vecuh16x8_src, elemuhE);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vecuh16x8_res[indx] != expecteduh8_5[indx])\n       abort ();\n \n-  vecuh16x8_res = vecuh16x8_src * elemuhF;\n+  vecuh16x8_res = vmulq_n_u16 (vecuh16x8_src, elemuhF);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vecuh16x8_res[indx] != expecteduh8_6[indx])\n       abort ();\n \n-  vecuh16x8_res = vecuh16x8_src * elemuhG;\n+  vecuh16x8_res = vmulq_n_u16 (vecuh16x8_src, elemuhG);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vecuh16x8_res[indx] != expecteduh8_7[indx])\n       abort ();\n \n-  vecuh16x8_res = vecuh16x8_src * elemuhH;\n+  vecuh16x8_res = vmulq_n_u16 (vecuh16x8_src, elemuhH);\n \n   for (indx = 0; indx < 8; indx++)\n     if (vecuh16x8_res[indx] != expecteduh8_8[indx])"}]}