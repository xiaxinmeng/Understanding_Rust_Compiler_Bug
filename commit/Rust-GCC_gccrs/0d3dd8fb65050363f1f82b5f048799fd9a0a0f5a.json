{"sha": "0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MGQzZGQ4ZmI2NTA1MDM2M2YxZjgyYjVmMDQ4Nzk5ZmQ5YTBhMGY1YQ==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-12-19T18:00:35Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2016-12-19T18:00:35Z"}, "message": "runtime: copy cgo support from Go 1.7 runtime\n    \n    Remove support for _cgo_allocate.  It was removed from the gc\n    toolchain in Go 1.5, so it is unlikely that anybody is trying to use it.\n    \n    Reviewed-on: https://go-review.googlesource.com/34557\n\nFrom-SVN: r243805", "tree": {"sha": "2ebab7c43a3260f883a2cf83ca162d10a8850870", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/2ebab7c43a3260f883a2cf83ca162d10a8850870"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/comments", "author": null, "committer": null, "parents": [{"sha": "4daecdb62396a1571f3cba861a0068ab539f8e28", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4daecdb62396a1571f3cba861a0068ab539f8e28", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4daecdb62396a1571f3cba861a0068ab539f8e28"}], "stats": {"total": 1264, "additions": 630, "deletions": 634}, "files": [{"sha": "b8468965dba6c6fd853013f987d0fb0237db0be6", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -1,4 +1,4 @@\n-e6fb629c5b246bceab5fc8e8613cf2cf82b1e98f\n+4a0bb435bbb1d1516b486d1998e8dc184576db61\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "a55fb436bc563fd055f1345c457c972dbdf7a5cb", "filename": "libgo/go/runtime/cgo_gccgo.go", "status": "added", "additions": 110, "deletions": 0, "changes": 110, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fcgo_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fcgo_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fcgo_gccgo.go?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -0,0 +1,110 @@\n+// Copyright 2016 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t_ \"unsafe\"\n+)\n+\n+// For historical reasons these functions are called as though they\n+// were in the syscall package.\n+//go:linkname Cgocall syscall.Cgocall\n+//go:linkname CgocallDone syscall.CgocallDone\n+//go:linkname CgocallBack syscall.CgocallBack\n+//go:linkname CgocallBackDone syscall.CgocallBackDone\n+\n+// A routine that may be called by SWIG.\n+//go:linkname _cgo_panic _cgo_panic\n+\n+// iscgo is set to true if the cgo tool sets the C variable runtime_iscgo\n+// to true.\n+var iscgo bool\n+\n+// cgoHasExtraM is set on startup when an extra M is created for cgo.\n+// The extra M must be created before any C/C++ code calls cgocallback.\n+var cgoHasExtraM bool\n+\n+// Cgocall prepares to call from code written in Go to code written in\n+// C/C++. This takes the current goroutine out of the Go scheduler, as\n+// though it were making a system call. Otherwise the program can\n+// lookup if the C code blocks. The idea is to call this function,\n+// then immediately call the C/C++ function. After the C/C++ function\n+// returns, call cgocalldone. The usual Go code would look like\n+//     syscall.Cgocall()\n+//     defer syscall.Cgocalldone()\n+//     cfunction()\n+func Cgocall() {\n+\tlockOSThread()\n+\tmp := getg().m\n+\tmp.ncgocall++\n+\tmp.ncgo++\n+\tentersyscall(0)\n+}\n+\n+// CgocallDone prepares to return to Go code from C/C++ code.\n+func CgocallDone() {\n+\tgp := getg()\n+\tif gp == nil {\n+\t\tthrow(\"no g in CgocallDone\")\n+\t}\n+\tgp.m.ncgo--\n+\n+\t// If we are invoked because the C function called _cgo_panic,\n+\t// then _cgo_panic will already have exited syscall mode.\n+\tif gp.atomicstatus == _Gsyscall {\n+\t\texitsyscall(0)\n+\t}\n+\n+\tunlockOSThread()\n+}\n+\n+// CgocallBack is used when calling from C/C++ code into Go code.\n+// The usual approach is\n+//     syscall.CgocallBack()\n+//     defer syscall.CgocallBackDone()\n+//     gofunction()\n+//go:nosplit\n+func CgocallBack() {\n+\tif getg() == nil || getg().m == nil {\n+\t\tneedm(0)\n+\t\tmp := getg().m\n+\t\tmp.dropextram = true\n+\t}\n+\n+\texitsyscall(0)\n+\n+\tif getg().m.ncgo == 0 {\n+\t\t// The C call to Go came from a thread created by C.\n+\t\t// The C call to Go came from a thread not currently running\n+\t\t// any Go. In the case of -buildmode=c-archive or c-shared,\n+\t\t// this call may be coming in before package initialization\n+\t\t// is complete. Wait until it is.\n+\t\t<-main_init_done\n+\t}\n+\n+\tmp := getg().m\n+\tif mp.needextram || atomic.Load(&extraMWaiters) > 0 {\n+\t\tmp.needextram = false\n+\t\tnewextram()\n+\t}\n+}\n+\n+// CgocallBackDone prepares to return to C/C++ code that has called\n+// into Go code.\n+func CgocallBackDone() {\n+\tentersyscall(0)\n+\tmp := getg().m\n+\tif mp.dropextram && mp.ncgo == 0 {\n+\t\tmp.dropextram = false\n+\t\tdropm()\n+\t}\n+}\n+\n+// _cgo_panic may be called by SWIG code to panic.\n+func _cgo_panic(p *byte) {\n+\texitsyscall(0)\n+\tpanic(gostringnocopy(p))\n+}"}, {"sha": "bcdd6cd6961a6c7ea9bf6e31872cb785c050a58e", "filename": "libgo/go/runtime/cgo_mmap.go", "status": "removed", "additions": 0, "deletions": 43, "changes": 43, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4daecdb62396a1571f3cba861a0068ab539f8e28/libgo%2Fgo%2Fruntime%2Fcgo_mmap.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4daecdb62396a1571f3cba861a0068ab539f8e28/libgo%2Fgo%2Fruntime%2Fcgo_mmap.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fcgo_mmap.go?ref=4daecdb62396a1571f3cba861a0068ab539f8e28", "patch": "@@ -1,43 +0,0 @@\n-// Copyright 2015 The Go Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style\n-// license that can be found in the LICENSE file.\n-\n-// +build ignore\n-\n-// Support for memory sanitizer. See runtime/cgo/mmap.go.\n-\n-// +build linux,amd64\n-\n-package runtime\n-\n-import \"unsafe\"\n-\n-// _cgo_mmap is filled in by runtime/cgo when it is linked into the\n-// program, so it is only non-nil when using cgo.\n-//go:linkname _cgo_mmap _cgo_mmap\n-var _cgo_mmap unsafe.Pointer\n-\n-func mmap(addr unsafe.Pointer, n uintptr, prot, flags, fd int32, off uint32) unsafe.Pointer {\n-\tif _cgo_mmap != nil {\n-\t\t// Make ret a uintptr so that writing to it in the\n-\t\t// function literal does not trigger a write barrier.\n-\t\t// A write barrier here could break because of the way\n-\t\t// that mmap uses the same value both as a pointer and\n-\t\t// an errno value.\n-\t\t// TODO: Fix mmap to return two values.\n-\t\tvar ret uintptr\n-\t\tsystemstack(func() {\n-\t\t\tret = callCgoMmap(addr, n, prot, flags, fd, off)\n-\t\t})\n-\t\treturn unsafe.Pointer(ret)\n-\t}\n-\treturn sysMmap(addr, n, prot, flags, fd, off)\n-}\n-\n-// sysMmap calls the mmap system call. It is implemented in assembly.\n-func sysMmap(addr unsafe.Pointer, n uintptr, prot, flags, fd int32, off uint32) unsafe.Pointer\n-\n-// cgoMmap calls the mmap function in the runtime/cgo package on the\n-// callCgoMmap calls the mmap function in the runtime/cgo package\n-// using the GCC calling convention. It is implemented in assembly.\n-func callCgoMmap(addr unsafe.Pointer, n uintptr, prot, flags, fd int32, off uint32) uintptr"}, {"sha": "f45ab2538a70f76311a91fb3bc232276cd0f4819", "filename": "libgo/go/runtime/os_gccgo.go", "status": "modified", "additions": 38, "deletions": 0, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fos_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fos_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fos_gccgo.go?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -8,6 +8,44 @@ import (\n \t\"unsafe\"\n )\n \n+// Temporary for C code to call:\n+//go:linkname minit runtime.minit\n+\n+// minit is called to initialize a new m (including the bootstrap m).\n+// Called on the new thread, cannot allocate memory.\n+func minit() {\n+\t// Initialize signal handling.\n+\t_g_ := getg()\n+\n+\tvar st _stack_t\n+\tsigaltstack(nil, &st)\n+\tif st.ss_flags&_SS_DISABLE != 0 {\n+\t\tsignalstack(_g_.m.gsignalstack, _g_.m.gsignalstacksize)\n+\t\t_g_.m.newSigstack = true\n+\t} else {\n+\t\t_g_.m.newSigstack = false\n+\t}\n+\n+\t// FIXME: We should set _g_.m.procid here.\n+\n+\t// restore signal mask from m.sigmask and unblock essential signals\n+\tnmask := _g_.m.sigmask\n+\tfor i := range sigtable {\n+\t\tif sigtable[i].flags&_SigUnblock != 0 {\n+\t\t\tsigdelset(&nmask, int32(i))\n+\t\t}\n+\t}\n+\tsigprocmask(_SIG_SETMASK, &nmask, nil)\n+}\n+\n+// Called from dropm to undo the effect of an minit.\n+//go:nosplit\n+func unminit() {\n+\tif getg().m.newSigstack {\n+\t\tsignalstack(nil, 0)\n+\t}\n+}\n+\n var urandom_dev = []byte(\"/dev/urandom\\x00\")\n \n func getRandomData(r []byte) {"}, {"sha": "fa90a282866f2013bbe0113e56cbeab8a9a23766", "filename": "libgo/go/runtime/proc.go", "status": "added", "additions": 330, "deletions": 0, "changes": 330, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fproc.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fproc.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fproc.go?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -0,0 +1,330 @@\n+// Copyright 2014 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"unsafe\"\n+)\n+\n+// Functions temporarily called by C code.\n+//go:linkname newextram runtime.newextram\n+\n+// Functions temporarily in C that have not yet been ported.\n+func allocm(*p, bool, *unsafe.Pointer, *uintptr) *m\n+func malg(bool, bool, *unsafe.Pointer, *uintptr) *g\n+func allgadd(*g)\n+\n+// C functions for ucontext management.\n+func setGContext()\n+func makeGContext(*g, unsafe.Pointer, uintptr)\n+\n+// main_init_done is a signal used by cgocallbackg that initialization\n+// has been completed. It is made before _cgo_notify_runtime_init_done,\n+// so all cgo calls can rely on it existing. When main_init is complete,\n+// it is closed, meaning cgocallbackg can reliably receive from it.\n+var main_init_done chan bool\n+\n+// If asked to move to or from a Gscanstatus this will throw. Use the castogscanstatus\n+// and casfrom_Gscanstatus instead.\n+// casgstatus will loop if the g->atomicstatus is in a Gscan status until the routine that\n+// put it in the Gscan state is finished.\n+//go:nosplit\n+func casgstatus(gp *g, oldval, newval uint32) {\n+\tif (oldval&_Gscan != 0) || (newval&_Gscan != 0) || oldval == newval {\n+\t\tsystemstack(func() {\n+\t\t\tprint(\"runtime: casgstatus: oldval=\", hex(oldval), \" newval=\", hex(newval), \"\\n\")\n+\t\t\tthrow(\"casgstatus: bad incoming values\")\n+\t\t})\n+\t}\n+\n+\tif oldval == _Grunning && gp.gcscanvalid {\n+\t\t// If oldvall == _Grunning, then the actual status must be\n+\t\t// _Grunning or _Grunning|_Gscan; either way,\n+\t\t// we own gp.gcscanvalid, so it's safe to read.\n+\t\t// gp.gcscanvalid must not be true when we are running.\n+\t\tprint(\"runtime: casgstatus \", hex(oldval), \"->\", hex(newval), \" gp.status=\", hex(gp.atomicstatus), \" gp.gcscanvalid=true\\n\")\n+\t\tthrow(\"casgstatus\")\n+\t}\n+\n+\t// See http://golang.org/cl/21503 for justification of the yield delay.\n+\tconst yieldDelay = 5 * 1000\n+\tvar nextYield int64\n+\n+\t// loop if gp->atomicstatus is in a scan state giving\n+\t// GC time to finish and change the state to oldval.\n+\tfor i := 0; !atomic.Cas(&gp.atomicstatus, oldval, newval); i++ {\n+\t\tif oldval == _Gwaiting && gp.atomicstatus == _Grunnable {\n+\t\t\tsystemstack(func() {\n+\t\t\t\tthrow(\"casgstatus: waiting for Gwaiting but is Grunnable\")\n+\t\t\t})\n+\t\t}\n+\t\t// Help GC if needed.\n+\t\t// if gp.preemptscan && !gp.gcworkdone && (oldval == _Grunning || oldval == _Gsyscall) {\n+\t\t// \tgp.preemptscan = false\n+\t\t// \tsystemstack(func() {\n+\t\t// \t\tgcphasework(gp)\n+\t\t// \t})\n+\t\t// }\n+\t\t// But meanwhile just yield.\n+\t\tif i == 0 {\n+\t\t\tnextYield = nanotime() + yieldDelay\n+\t\t}\n+\t\tif nanotime() < nextYield {\n+\t\t\tfor x := 0; x < 10 && gp.atomicstatus != oldval; x++ {\n+\t\t\t\tprocyield(1)\n+\t\t\t}\n+\t\t} else {\n+\t\t\tosyield()\n+\t\t\tnextYield = nanotime() + yieldDelay/2\n+\t\t}\n+\t}\n+\tif newval == _Grunning && gp.gcscanvalid {\n+\t\t// Run queueRescan on the system stack so it has more space.\n+\t\tsystemstack(func() { queueRescan(gp) })\n+\t}\n+}\n+\n+// needm is called when a cgo callback happens on a\n+// thread without an m (a thread not created by Go).\n+// In this case, needm is expected to find an m to use\n+// and return with m, g initialized correctly.\n+// Since m and g are not set now (likely nil, but see below)\n+// needm is limited in what routines it can call. In particular\n+// it can only call nosplit functions (textflag 7) and cannot\n+// do any scheduling that requires an m.\n+//\n+// In order to avoid needing heavy lifting here, we adopt\n+// the following strategy: there is a stack of available m's\n+// that can be stolen. Using compare-and-swap\n+// to pop from the stack has ABA races, so we simulate\n+// a lock by doing an exchange (via casp) to steal the stack\n+// head and replace the top pointer with MLOCKED (1).\n+// This serves as a simple spin lock that we can use even\n+// without an m. The thread that locks the stack in this way\n+// unlocks the stack by storing a valid stack head pointer.\n+//\n+// In order to make sure that there is always an m structure\n+// available to be stolen, we maintain the invariant that there\n+// is always one more than needed. At the beginning of the\n+// program (if cgo is in use) the list is seeded with a single m.\n+// If needm finds that it has taken the last m off the list, its job\n+// is - once it has installed its own m so that it can do things like\n+// allocate memory - to create a spare m and put it on the list.\n+//\n+// Each of these extra m's also has a g0 and a curg that are\n+// pressed into service as the scheduling stack and current\n+// goroutine for the duration of the cgo callback.\n+//\n+// When the callback is done with the m, it calls dropm to\n+// put the m back on the list.\n+//go:nosplit\n+func needm(x byte) {\n+\tif iscgo && !cgoHasExtraM {\n+\t\t// Can happen if C/C++ code calls Go from a global ctor.\n+\t\t// Can not throw, because scheduler is not initialized yet.\n+\t\twrite(2, unsafe.Pointer(&earlycgocallback[0]), int32(len(earlycgocallback)))\n+\t\texit(1)\n+\t}\n+\n+\t// Lock extra list, take head, unlock popped list.\n+\t// nilokay=false is safe here because of the invariant above,\n+\t// that the extra list always contains or will soon contain\n+\t// at least one m.\n+\tmp := lockextra(false)\n+\n+\t// Set needextram when we've just emptied the list,\n+\t// so that the eventual call into cgocallbackg will\n+\t// allocate a new m for the extra list. We delay the\n+\t// allocation until then so that it can be done\n+\t// after exitsyscall makes sure it is okay to be\n+\t// running at all (that is, there's no garbage collection\n+\t// running right now).\n+\tmp.needextram = mp.schedlink == 0\n+\tunlockextra(mp.schedlink.ptr())\n+\n+\t// Save and block signals before installing g.\n+\t// Once g is installed, any incoming signals will try to execute,\n+\t// but we won't have the sigaltstack settings and other data\n+\t// set up appropriately until the end of minit, which will\n+\t// unblock the signals. This is the same dance as when\n+\t// starting a new m to run Go code via newosproc.\n+\tmsigsave(mp)\n+\tsigblock()\n+\n+\t// Install g (= m->curg).\n+\tsetg(mp.curg)\n+\tatomic.Store(&mp.curg.atomicstatus, _Gsyscall)\n+\tsetGContext()\n+\n+\t// Initialize this thread to use the m.\n+\tminit()\n+}\n+\n+var earlycgocallback = []byte(\"fatal error: cgo callback before cgo call\\n\")\n+\n+// newextram allocates m's and puts them on the extra list.\n+// It is called with a working local m, so that it can do things\n+// like call schedlock and allocate.\n+func newextram() {\n+\tc := atomic.Xchg(&extraMWaiters, 0)\n+\tif c > 0 {\n+\t\tfor i := uint32(0); i < c; i++ {\n+\t\t\toneNewExtraM()\n+\t\t}\n+\t} else {\n+\t\t// Make sure there is at least one extra M.\n+\t\tmp := lockextra(true)\n+\t\tunlockextra(mp)\n+\t\tif mp == nil {\n+\t\t\toneNewExtraM()\n+\t\t}\n+\t}\n+}\n+\n+// oneNewExtraM allocates an m and puts it on the extra list.\n+func oneNewExtraM() {\n+\t// Create extra goroutine locked to extra m.\n+\t// The goroutine is the context in which the cgo callback will run.\n+\t// The sched.pc will never be returned to, but setting it to\n+\t// goexit makes clear to the traceback routines where\n+\t// the goroutine stack ends.\n+\tvar g0SP unsafe.Pointer\n+\tvar g0SPSize uintptr\n+\tmp := allocm(nil, true, &g0SP, &g0SPSize)\n+\tgp := malg(true, false, nil, nil)\n+\tgp.gcscanvalid = true // fresh G, so no dequeueRescan necessary\n+\tgp.gcRescan = -1\n+\n+\t// malg returns status as Gidle, change to Gdead before adding to allg\n+\t// where GC will see it.\n+\t// gccgo uses Gdead here, not Gsyscall, because the split\n+\t// stack context is not initialized.\n+\tcasgstatus(gp, _Gidle, _Gdead)\n+\tgp.m = mp\n+\tmp.curg = gp\n+\tmp.locked = _LockInternal\n+\tmp.lockedg = gp\n+\tgp.lockedm = mp\n+\tgp.goid = int64(atomic.Xadd64(&sched.goidgen, 1))\n+\tif raceenabled {\n+\t\tgp.racectx = racegostart(funcPC(newextram))\n+\t}\n+\t// put on allg for garbage collector\n+\tallgadd(gp)\n+\n+\t// The context for gp will be set up in needm.\n+\t// Here we need to set the context for g0.\n+\tmakeGContext(mp.g0, g0SP, g0SPSize)\n+\n+\t// Add m to the extra list.\n+\tmnext := lockextra(true)\n+\tmp.schedlink.set(mnext)\n+\tunlockextra(mp)\n+}\n+\n+// dropm is called when a cgo callback has called needm but is now\n+// done with the callback and returning back into the non-Go thread.\n+// It puts the current m back onto the extra list.\n+//\n+// The main expense here is the call to signalstack to release the\n+// m's signal stack, and then the call to needm on the next callback\n+// from this thread. It is tempting to try to save the m for next time,\n+// which would eliminate both these costs, but there might not be\n+// a next time: the current thread (which Go does not control) might exit.\n+// If we saved the m for that thread, there would be an m leak each time\n+// such a thread exited. Instead, we acquire and release an m on each\n+// call. These should typically not be scheduling operations, just a few\n+// atomics, so the cost should be small.\n+//\n+// TODO(rsc): An alternative would be to allocate a dummy pthread per-thread\n+// variable using pthread_key_create. Unlike the pthread keys we already use\n+// on OS X, this dummy key would never be read by Go code. It would exist\n+// only so that we could register at thread-exit-time destructor.\n+// That destructor would put the m back onto the extra list.\n+// This is purely a performance optimization. The current version,\n+// in which dropm happens on each cgo call, is still correct too.\n+// We may have to keep the current version on systems with cgo\n+// but without pthreads, like Windows.\n+func dropm() {\n+\t// Clear m and g, and return m to the extra list.\n+\t// After the call to setg we can only call nosplit functions\n+\t// with no pointer manipulation.\n+\tmp := getg().m\n+\n+\t// Block signals before unminit.\n+\t// Unminit unregisters the signal handling stack (but needs g on some systems).\n+\t// Setg(nil) clears g, which is the signal handler's cue not to run Go handlers.\n+\t// It's important not to try to handle a signal between those two steps.\n+\tsigmask := mp.sigmask\n+\tsigblock()\n+\tunminit()\n+\n+\t// gccgo sets the stack to Gdead here, because the splitstack\n+\t// context is not initialized.\n+\tmp.curg.atomicstatus = _Gdead\n+\tmp.curg.gcstack = nil\n+\tmp.curg.gcnextsp = nil\n+\n+\tmnext := lockextra(true)\n+\tmp.schedlink.set(mnext)\n+\n+\tsetg(nil)\n+\n+\t// Commit the release of mp.\n+\tunlockextra(mp)\n+\n+\tmsigrestore(sigmask)\n+}\n+\n+// A helper function for EnsureDropM.\n+func getm() uintptr {\n+\treturn uintptr(unsafe.Pointer(getg().m))\n+}\n+\n+var extram uintptr\n+var extraMWaiters uint32\n+\n+// lockextra locks the extra list and returns the list head.\n+// The caller must unlock the list by storing a new list head\n+// to extram. If nilokay is true, then lockextra will\n+// return a nil list head if that's what it finds. If nilokay is false,\n+// lockextra will keep waiting until the list head is no longer nil.\n+//go:nosplit\n+func lockextra(nilokay bool) *m {\n+\tconst locked = 1\n+\n+\tincr := false\n+\tfor {\n+\t\told := atomic.Loaduintptr(&extram)\n+\t\tif old == locked {\n+\t\t\tyield := osyield\n+\t\t\tyield()\n+\t\t\tcontinue\n+\t\t}\n+\t\tif old == 0 && !nilokay {\n+\t\t\tif !incr {\n+\t\t\t\t// Add 1 to the number of threads\n+\t\t\t\t// waiting for an M.\n+\t\t\t\t// This is cleared by newextram.\n+\t\t\t\tatomic.Xadd(&extraMWaiters, 1)\n+\t\t\t\tincr = true\n+\t\t\t}\n+\t\t\tusleep(1)\n+\t\t\tcontinue\n+\t\t}\n+\t\tif atomic.Casuintptr(&extram, old, locked) {\n+\t\t\treturn (*m)(unsafe.Pointer(old))\n+\t\t}\n+\t\tyield := osyield\n+\t\tyield()\n+\t\tcontinue\n+\t}\n+}\n+\n+//go:nosplit\n+func unlockextra(mp *m) {\n+\tatomic.Storeuintptr(&extram, uintptr(unsafe.Pointer(mp)))\n+}"}, {"sha": "978a3172d0fcfb9cc15daf742077f4022844fee8", "filename": "libgo/go/runtime/runtime2.go", "status": "modified", "additions": 0, "deletions": 10, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fruntime2.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fruntime2.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime2.go?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -479,8 +479,6 @@ type m struct {\n \tdropextram bool // drop after call is done\n \n \tgcing int32\n-\n-\tcgomal *cgoMal // allocations via _cgo_allocate\n }\n \n type p struct {\n@@ -801,14 +799,6 @@ var (\n // array.\n type g_ucontext_t [(_sizeof_ucontext_t + 15) / unsafe.Sizeof(unsafe.Pointer(nil))]unsafe.Pointer\n \n-// cgoMal tracks allocations made by _cgo_allocate\n-// FIXME: _cgo_allocate has been removed from gc and can probably be\n-// removed from gccgo too.\n-type cgoMal struct {\n-\tnext  *cgoMal\n-\talloc unsafe.Pointer\n-}\n-\n // sigset is the Go version of the C type sigset_t.\n // _sigset_t is defined by the Makefile from <signal.h>.\n type sigset _sigset_t"}, {"sha": "181aebe64b31b8ea58cb5b614b6cd04ccfe86970", "filename": "libgo/go/runtime/signal1_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fsignal1_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fsignal1_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fsignal1_unix.go?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -327,7 +327,7 @@ func ensureSigM() {\n //go:norace\n //go:nowritebarrierrec\n func badsignal(sig uintptr, c *sigctxt) {\n-\tneedm()\n+\tneedm(0)\n \tif !sigsend(uint32(sig)) {\n \t\t// A foreign thread received the signal sig, and the\n \t\t// Go code does not want to handle it."}, {"sha": "4e5044f26dd05b8077d416a5c9497b1aad86b0d2", "filename": "libgo/go/runtime/signal_gccgo.go", "status": "modified", "additions": 24, "deletions": 11, "changes": 35, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fsignal_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fsignal_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fsignal_gccgo.go?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -17,18 +17,19 @@ import (\n func sigaction(signum int32, act *_sigaction, oact *_sigaction) int32\n \n //extern sigprocmask\n-func sigprocmask(how int32, set *_sigset_t, oldset *_sigset_t) int32\n+func sigprocmask(how int32, set *sigset, oldset *sigset) int32\n \n-// The argument should be simply *_sigset_t, but that fails on GNU/Linux\n-// which sometimes uses _sigset_t and sometimes uses ___sigset_t.\n //extern sigfillset\n-func sigfillset(set unsafe.Pointer) int32\n+func sigfillset(set *sigset) int32\n \n //extern sigemptyset\n-func sigemptyset(set *_sigset_t) int32\n+func sigemptyset(set *sigset) int32\n \n //extern sigaddset\n-func sigaddset(set *_sigset_t, signum int32) int32\n+func sigaddset(set *sigset, signum int32) int32\n+\n+//extern sigdelset\n+func sigdelset(set *sigset, signum int32) int32\n \n //extern sigaltstack\n func sigaltstack(ss *_stack_t, oss *_stack_t) int32\n@@ -56,10 +57,20 @@ func (c *sigctxt) sigcode() uint64 {\n \treturn uint64(c.info.si_code)\n }\n \n+//go:nosplit\n+func msigsave(mp *m) {\n+\tsigprocmask(_SIG_SETMASK, nil, &mp.sigmask)\n+}\n+\n+//go:nosplit\n+func msigrestore(sigmask sigset) {\n+\tsigprocmask(_SIG_SETMASK, &sigmask, nil)\n+}\n+\n //go:nosplit\n func sigblock() {\n-\tvar set _sigset_t\n-\tsigfillset(unsafe.Pointer(&set))\n+\tvar set sigset\n+\tsigfillset(&set)\n \tsigprocmask(_SIG_SETMASK, &set, nil)\n }\n \n@@ -81,7 +92,7 @@ func setsig(i int32, fn uintptr, restart bool) {\n \tif restart {\n \t\tsa.sa_flags |= _SA_RESTART\n \t}\n-\tsigfillset(unsafe.Pointer(&sa.sa_mask))\n+\tsigfillset((*sigset)(unsafe.Pointer(&sa.sa_mask)))\n \tsetSigactionHandler(&sa, fn)\n \tsigaction(i, &sa, nil)\n }\n@@ -117,10 +128,12 @@ func getsig(i int32) uintptr {\n \treturn getSigactionHandler(&sa)\n }\n \n+func signalstack(p unsafe.Pointer, n uintptr)\n+\n //go:nosplit\n //go:nowritebarrierrec\n func updatesigmask(m sigmask) {\n-\tvar mask _sigset_t\n+\tvar mask sigset\n \tsigemptyset(&mask)\n \tfor i := int32(0); i < _NSIG; i++ {\n \t\tif m[(i-1)/32]&(1<<((uint(i)-1)&31)) != 0 {\n@@ -131,7 +144,7 @@ func updatesigmask(m sigmask) {\n }\n \n func unblocksig(sig int32) {\n-\tvar mask _sigset_t\n+\tvar mask sigset\n \tsigemptyset(&mask)\n \tsigaddset(&mask, sig)\n \tsigprocmask(_SIG_UNBLOCK, &mask, nil)"}, {"sha": "766bb7dc0fb1e615e49f5ab4e4f33b3ad05f79ba", "filename": "libgo/go/runtime/signal_sighandler.go", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fsignal_sighandler.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fsignal_sighandler.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fsignal_sighandler.go?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -52,8 +52,8 @@ func sighandler(sig uint32, info *_siginfo_t, ctxt unsafe.Pointer, gp *g) {\n \n \t\t// All signals were blocked due to the sigaction mask;\n \t\t// unblock them.\n-\t\tvar set _sigset_t\n-\t\tsigfillset(unsafe.Pointer(&set))\n+\t\tvar set sigset\n+\t\tsigfillset(&set)\n \t\tsigprocmask(_SIG_UNBLOCK, &set, nil)\n \n \t\tsigpanic()"}, {"sha": "dde9ebdfdd6eea4c2ccb985ce6f30400d7de4c03", "filename": "libgo/go/runtime/stubs.go", "status": "modified", "additions": 29, "deletions": 2, "changes": 31, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fstubs.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fgo%2Fruntime%2Fstubs.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstubs.go?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -248,6 +248,24 @@ func funcPC(f interface{}) uintptr {\n \treturn **(**uintptr)(i.data)\n }\n \n+// For gccgo, to communicate from the C code to the Go code.\n+//go:linkname setIsCgo runtime.setIsCgo\n+func setIsCgo() {\n+\tiscgo = true\n+}\n+\n+// Temporary for gccgo until we port proc.go.\n+//go:linkname makeMainInitDone runtime.makeMainInitDone\n+func makeMainInitDone() {\n+\tmain_init_done = make(chan bool)\n+}\n+\n+// Temporary for gccgo until we port proc.go.\n+//go:linkname closeMainInitDone runtime.closeMainInitDone\n+func closeMainInitDone() {\n+\tclose(main_init_done)\n+}\n+\n // For gccgo, to communicate from the C code to the Go code.\n //go:linkname setCpuidECX runtime.setCpuidECX\n func setCpuidECX(v uint32) {\n@@ -301,6 +319,9 @@ var writeBarrier struct {\n \talignme uint64 // guarantee alignment so that compiler can use a 32 or 64-bit load\n }\n \n+func queueRescan(*g) {\n+}\n+\n // Here for gccgo until we port atomic_pointer.go and mgc.go.\n //go:nosplit\n func casp(ptr *unsafe.Pointer, old, new unsafe.Pointer) bool {\n@@ -446,6 +467,8 @@ func cpuprofAdd(stk []uintptr) {\n func Breakpoint()\n func LockOSThread()\n func UnlockOSThread()\n+func lockOSThread()\n+func unlockOSThread()\n func allm() *m\n func allgs() []*g\n \n@@ -499,8 +522,6 @@ func getZerobase() *uintptr {\n }\n \n // Temporary for gccgo until we port proc.go.\n-func needm()\n-func dropm()\n func sigprof()\n func mcount() int32\n func gcount() int32\n@@ -529,6 +550,12 @@ func getsched() *schedt {\n \treturn &sched\n }\n \n+// Temporary for gccgo until we port proc.go.\n+//go:linkname getCgoHasExtraM runtime.getCgoHasExtraM\n+func getCgoHasExtraM() *bool {\n+\treturn &cgoHasExtraM\n+}\n+\n // Throw and rethrow an exception.\n func throwException()\n func rethrowException()"}, {"sha": "e80b6b519e18c47d916630ccb3f01a70ace88c42", "filename": "libgo/runtime/go-cgo.c", "status": "modified", "additions": 0, "deletions": 192, "changes": 192, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fgo-cgo.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fgo-cgo.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-cgo.c?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -5,193 +5,6 @@\n    license that can be found in the LICENSE file.  */\n \n #include \"runtime.h\"\n-#include \"go-alloc.h\"\n-#include \"go-type.h\"\n-\n-extern void chanrecv1 (ChanType *, Hchan *, void *)\n-  __asm__ (GOSYM_PREFIX \"runtime.chanrecv1\");\n-\n-/* Prepare to call from code written in Go to code written in C or\n-   C++.  This takes the current goroutine out of the Go scheduler, as\n-   though it were making a system call.  Otherwise the program can\n-   lock up if the C code goes to sleep on a mutex or for some other\n-   reason.  This idea is to call this function, then immediately call\n-   the C/C++ function.  After the C/C++ function returns, call\n-   syscall_cgocalldone.  The usual Go code would look like\n-\n-       syscall.Cgocall()\n-       defer syscall.Cgocalldone()\n-       cfunction()\n-\n-   */\n-\n-/* We let Go code call these via the syscall package.  */\n-void syscall_cgocall(void) __asm__ (GOSYM_PREFIX \"syscall.Cgocall\");\n-void syscall_cgocalldone(void) __asm__ (GOSYM_PREFIX \"syscall.CgocallDone\");\n-void syscall_cgocallback(void) __asm__ (GOSYM_PREFIX \"syscall.CgocallBack\");\n-void syscall_cgocallbackdone(void) __asm__ (GOSYM_PREFIX \"syscall.CgocallBackDone\");\n-\n-void\n-syscall_cgocall ()\n-{\n-  M* m;\n-\n-  if (runtime_needextram && runtime_cas (&runtime_needextram, 1, 0))\n-    runtime_newextram ();\n-\n-  runtime_lockOSThread();\n-\n-  m = runtime_m ();\n-  ++m->ncgocall;\n-  ++m->ncgo;\n-  runtime_entersyscall (0);\n-}\n-\n-/* Prepare to return to Go code from C/C++ code.  */\n-\n-void\n-syscall_cgocalldone ()\n-{\n-  G* g;\n-\n-  g = runtime_g ();\n-  __go_assert (g != NULL);\n-  --g->m->ncgo;\n-  if (g->m->ncgo == 0)\n-    {\n-      /* We are going back to Go, and we are not in a recursive call.\n-\t Let the garbage collector clean up any unreferenced\n-\t memory.  */\n-      g->m->cgomal = NULL;\n-    }\n-\n-  /* If we are invoked because the C function called _cgo_panic, then\n-     _cgo_panic will already have exited syscall mode.  */\n-  if (g->atomicstatus == _Gsyscall)\n-    runtime_exitsyscall (0);\n-\n-  runtime_unlockOSThread();\n-}\n-\n-/* Call back from C/C++ code to Go code.  */\n-\n-void\n-syscall_cgocallback ()\n-{\n-  M *mp;\n-\n-  mp = runtime_m ();\n-  if (mp == NULL)\n-    {\n-      runtime_needm ();\n-      mp = runtime_m ();\n-      mp->dropextram = true;\n-    }\n-\n-  runtime_exitsyscall (0);\n-\n-  if (runtime_m ()->ncgo == 0)\n-    {\n-      /* The C call to Go came from a thread not currently running any\n-\t Go.  In the case of -buildmode=c-archive or c-shared, this\n-\t call may be coming in before package initialization is\n-\t complete.  Wait until it is.  */\n-      chanrecv1 (NULL, runtime_main_init_done, NULL);\n-    }\n-\n-  mp = runtime_m ();\n-  if (mp->needextram)\n-    {\n-      mp->needextram = 0;\n-      runtime_newextram ();\n-    }\n-}\n-\n-/* Prepare to return to C/C++ code from a callback to Go code.  */\n-\n-void\n-syscall_cgocallbackdone ()\n-{\n-  M *mp;\n-\n-  runtime_entersyscall (0);\n-  mp = runtime_m ();\n-  if (mp->dropextram && mp->ncgo == 0)\n-    {\n-      mp->dropextram = false;\n-      runtime_dropm ();\n-    }\n-}\n-\n-/* Allocate memory and save it in a list visible to the Go garbage\n-   collector.  */\n-\n-void *\n-alloc_saved (size_t n)\n-{\n-  void *ret;\n-  M *m;\n-  CgoMal *c;\n-\n-  ret = __go_alloc (n);\n-\n-  m = runtime_m ();\n-  c = (CgoMal *) __go_alloc (sizeof (CgoMal));\n-  c->next = m->cgomal;\n-  c->alloc = ret;\n-  m->cgomal = c;\n-\n-  return ret;\n-}\n-\n-/* These are routines used by SWIG.  The gc runtime library provides\n-   the same routines under the same name, though in that case the code\n-   is required to import runtime/cgo.  */\n-\n-void *\n-_cgo_allocate (size_t n)\n-{\n-  void *ret;\n-\n-  runtime_exitsyscall (0);\n-  ret = alloc_saved (n);\n-  runtime_entersyscall (0);\n-  return ret;\n-}\n-\n-extern const struct __go_type_descriptor string_type_descriptor\n-  __asm__ (GOSYM_PREFIX \"__go_tdn_string\");\n-\n-void\n-_cgo_panic (const char *p)\n-{\n-  intgo len;\n-  unsigned char *data;\n-  String *ps;\n-  Eface e;\n-  const struct __go_type_descriptor *td;\n-\n-  runtime_exitsyscall (0);\n-  len = __builtin_strlen (p);\n-  data = alloc_saved (len);\n-  __builtin_memcpy (data, p, len);\n-  ps = alloc_saved (sizeof *ps);\n-  ps->str = data;\n-  ps->len = len;\n-  td = &string_type_descriptor;\n-  memcpy(&e._type, &td, sizeof td); /* This is a const_cast.  */\n-  e.data = ps;\n-\n-  /* We don't call runtime_entersyscall here, because normally what\n-     will happen is that we will walk up the stack to a Go deferred\n-     function that calls recover.  However, this will do the wrong\n-     thing if this panic is recovered and the stack unwinding is\n-     caught by a C++ exception handler.  It might be possible to\n-     handle this by calling runtime_entersyscall in the personality\n-     function in go-unwind.c.  FIXME.  */\n-\n-  runtime_panic (e);\n-}\n \n /* Used for _cgo_wait_runtime_init_done.  This is based on code in\n    runtime/cgo/gcc_libinit.c in the master library.  */\n@@ -249,8 +62,3 @@ _cgo_notify_runtime_init_done (void)\n // runtime_iscgo is set to true if some cgo code is linked in.\n // This is done by a constructor in the cgo generated code.\n _Bool runtime_iscgo;\n-\n-// runtime_cgoHasExtraM is set on startup when an extra M is created\n-// for cgo.  The extra M must be created before any C/C++ code calls\n-// cgocallback.\n-_Bool runtime_cgoHasExtraM;"}, {"sha": "a404eb9b68173f02123966e5b57f083962beb53f", "filename": "libgo/runtime/go-libmain.c", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fgo-libmain.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fgo-libmain.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-libmain.c?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -61,6 +61,7 @@ initfn (int argc, char **argv, char** env __attribute__ ((unused)))\n \n   runtime_isarchive = true;\n \n+  setIsCgo ();\n   runtime_cpuinit ();\n   runtime_initsig(true);\n "}, {"sha": "b8c9af1e79b987de74971efcd85c6a9c30bd41a3", "filename": "libgo/runtime/go-main.c", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fgo-main.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fgo-main.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-main.c?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -46,6 +46,9 @@ main (int argc, char **argv)\n     return 0;\n   runtime_isstarted = true;\n \n+  if (runtime_iscgo)\n+    setIsCgo ();\n+\n   __go_end = (uintptr)_end;\n   runtime_cpuinit ();\n   runtime_check ();"}, {"sha": "f13d5b3a99ee7290f393239ee8de5b3294637538", "filename": "libgo/runtime/malloc.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fmalloc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fmalloc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmalloc.h?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -543,4 +543,3 @@ int32\truntime_setgcpercent(int32)\n #define PoisonStack ((uintptr)0x6868686868686868ULL)\n \n struct Workbuf;\n-void\truntime_proc_scan(struct Workbuf**, void (*)(struct Workbuf**, Obj));"}, {"sha": "aa8404e216741eeb4247d5ac58bb23f810501cc1", "filename": "libgo/runtime/mgc0.c", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fmgc0.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fmgc0.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmgc0.c?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -1283,7 +1283,6 @@ markroot(ParFor *desc, uint32 i)\n \t\tenqueue1(&wbuf, (Obj){(byte*)&runtime_allm, sizeof runtime_allm, 0});\n \t\tenqueue1(&wbuf, (Obj){(byte*)&runtime_allp, sizeof runtime_allp, 0});\n \t\tenqueue1(&wbuf, (Obj){(byte*)&work, sizeof work, 0});\n-\t\truntime_proc_scan(&wbuf, enqueue1);\n \t\tbreak;\n \n \tcase RootFinalizers:"}, {"sha": "586a6323e6dc769862673400c74a4ddae8aac191", "filename": "libgo/runtime/proc.c", "status": "modified", "additions": 68, "deletions": 321, "changes": 389, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fproc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fproc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fproc.c?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -359,16 +359,16 @@ enum\n };\n \n extern Sched* runtime_getsched() __asm__ (GOSYM_PREFIX \"runtime.getsched\");\n+extern bool* runtime_getCgoHasExtraM()\n+  __asm__ (GOSYM_PREFIX \"runtime.getCgoHasExtraM\");\n \n Sched*\truntime_sched;\n int32\truntime_gomaxprocs;\n-uint32\truntime_needextram = 1;\n M\truntime_m0;\n G\truntime_g0;\t// idle goroutine for m0\n G*\truntime_lastg;\n M*\truntime_allm;\n P**\truntime_allp;\n-M*\truntime_extram;\n int8*\truntime_goos;\n int32\truntime_ncpu;\n bool\truntime_precisestack;\n@@ -418,7 +418,9 @@ static void pidleput(P*);\n static void injectglist(G*);\n static bool preemptall(void);\n static bool exitsyscallfast(void);\n-static void allgadd(G*);\n+\n+void allgadd(G*)\n+  __asm__(GOSYM_PREFIX \"runtime.allgadd\");\n \n bool runtime_isstarted;\n \n@@ -498,55 +500,6 @@ struct field_align\n   Hchan *p;\n };\n \n-// main_init_done is a signal used by cgocallbackg that initialization\n-// has been completed.  It is made before _cgo_notify_runtime_init_done,\n-// so all cgo calls can rely on it existing.  When main_init is\n-// complete, it is closed, meaning cgocallbackg can reliably receive\n-// from it.\n-Hchan *runtime_main_init_done;\n-\n-// The chan bool type, for runtime_main_init_done.\n-\n-extern const struct __go_type_descriptor bool_type_descriptor\n-  __asm__ (GOSYM_PREFIX \"__go_tdn_bool\");\n-\n-static struct __go_channel_type chan_bool_type_descriptor =\n-  {\n-    /* __common */\n-    {\n-      /* __code */\n-      GO_CHAN,\n-      /* __align */\n-      __alignof (Hchan *),\n-      /* __field_align */\n-      offsetof (struct field_align, p) - 1,\n-      /* __size */\n-      sizeof (Hchan *),\n-      /* __hash */\n-      0, /* This value doesn't matter.  */\n-      /* __hashfn */\n-      NULL,\n-      /* __equalfn */\n-      NULL,\n-      /* __gc */\n-      NULL, /* This value doesn't matter */\n-      /* __reflection */\n-      NULL, /* This value doesn't matter */\n-      /* __uncommon */\n-      NULL,\n-      /* __pointer_to_this */\n-      NULL\n-    },\n-    /* __element_type */\n-    &bool_type_descriptor,\n-    /* __dir */\n-    CHANNEL_BOTH_DIR\n-  };\n-\n-extern Hchan *makechan (ChanType *, int64)\n-  __asm__ (GOSYM_PREFIX \"runtime.makechan\");\n-extern void closechan(Hchan *) __asm__ (GOSYM_PREFIX \"runtime.closechan\");\n-\n static void\n initDone(void *arg __attribute__ ((unused))) {\n \truntime_unlockOSThread();\n@@ -593,13 +546,13 @@ runtime_main(void* dummy __attribute__((unused)))\n \t\truntime_throw(\"runtime_main not on m0\");\n \t__go_go(runtime_MHeap_Scavenger, nil);\n \n-\truntime_main_init_done = makechan(&chan_bool_type_descriptor, 0);\n+\tmakeMainInitDone();\n \n \t_cgo_notify_runtime_init_done();\n \n \tmain_init();\n \n-\tclosechan(runtime_main_init_done);\n+\tcloseMainInitDone();\n \n \tif(g->_defer != &d || (void*)d.pfn != initDone)\n \t\truntime_throw(\"runtime: bad defer entry after init\");\n@@ -1043,10 +996,12 @@ runtime_mstart(void* mp)\n \t// Install signal handlers; after minit so that minit can\n \t// prepare the thread to be able to handle the signals.\n \tif(m == &runtime_m0) {\n-\t\tif(runtime_iscgo && !runtime_cgoHasExtraM) {\n-\t\t\truntime_cgoHasExtraM = true;\n-\t\t\truntime_newextram();\n-\t\t\truntime_needextram = 0;\n+\t\tif(runtime_iscgo) {\n+\t\t\tbool* cgoHasExtraM = runtime_getCgoHasExtraM();\n+\t\t\tif(!*cgoHasExtraM) {\n+\t\t\t\t*cgoHasExtraM = true;\n+\t\t\t\truntime_newextram();\n+\t\t\t}\n \t\t}\n \t\truntime_initsig(false);\n \t}\n@@ -1079,10 +1034,13 @@ struct CgoThreadStart\n \tvoid (*fn)(void);\n };\n \n+M* runtime_allocm(P*, bool, byte**, uintptr*)\n+\t__asm__(GOSYM_PREFIX \"runtime.allocm\");\n+\n // Allocate a new m unassociated with any thread.\n // Can use p for allocation context if needed.\n M*\n-runtime_allocm(P *p, int32 stacksize, byte** ret_g0_stack, uintptr* ret_g0_stacksize)\n+runtime_allocm(P *p, bool allocatestack, byte** ret_g0_stack, uintptr* ret_g0_stacksize)\n {\n \tM *mp;\n \n@@ -1099,7 +1057,7 @@ runtime_allocm(P *p, int32 stacksize, byte** ret_g0_stack, uintptr* ret_g0_stack\n \n \tmp = runtime_mal(sizeof *mp);\n \tmcommoninit(mp);\n-\tmp->g0 = runtime_malg(stacksize, ret_g0_stack, ret_g0_stacksize);\n+\tmp->g0 = runtime_malg(allocatestack, false, ret_g0_stack, ret_g0_stacksize);\n \tmp->g0->m = mp;\n \n \tif(p == (P*)g->m->p)\n@@ -1125,90 +1083,26 @@ allocg(void)\n \treturn gp;\n }\n \n-static M* lockextra(bool nilokay);\n-static void unlockextra(M*);\n+void setGContext(void) __asm__ (GOSYM_PREFIX \"runtime.setGContext\");\n \n-// needm is called when a cgo callback happens on a\n-// thread without an m (a thread not created by Go).\n-// In this case, needm is expected to find an m to use\n-// and return with m, g initialized correctly.\n-// Since m and g are not set now (likely nil, but see below)\n-// needm is limited in what routines it can call. In particular\n-// it can only call nosplit functions (textflag 7) and cannot\n-// do any scheduling that requires an m.\n-//\n-// In order to avoid needing heavy lifting here, we adopt\n-// the following strategy: there is a stack of available m's\n-// that can be stolen. Using compare-and-swap\n-// to pop from the stack has ABA races, so we simulate\n-// a lock by doing an exchange (via casp) to steal the stack\n-// head and replace the top pointer with MLOCKED (1).\n-// This serves as a simple spin lock that we can use even\n-// without an m. The thread that locks the stack in this way\n-// unlocks the stack by storing a valid stack head pointer.\n-//\n-// In order to make sure that there is always an m structure\n-// available to be stolen, we maintain the invariant that there\n-// is always one more than needed. At the beginning of the\n-// program (if cgo is in use) the list is seeded with a single m.\n-// If needm finds that it has taken the last m off the list, its job\n-// is - once it has installed its own m so that it can do things like\n-// allocate memory - to create a spare m and put it on the list.\n-//\n-// Each of these extra m's also has a g0 and a curg that are\n-// pressed into service as the scheduling stack and current\n-// goroutine for the duration of the cgo callback.\n-//\n-// When the callback is done with the m, it calls dropm to\n-// put the m back on the list.\n-//\n-// Unlike the gc toolchain, we start running on curg, since we are\n-// just going to return and let the caller continue.\n+// setGContext sets up a new goroutine context for the current g.\n void\n-runtime_needm(void)\n+setGContext()\n {\n-\tM *mp;\n+\tint val;\n \n-\tif(runtime_needextram) {\n-\t\t// Can happen if C/C++ code calls Go from a global ctor.\n-\t\t// Can not throw, because scheduler is not initialized yet.\n-\t\tint rv __attribute__((unused));\n-\t\trv = runtime_write(2, \"fatal error: cgo callback before cgo call\\n\",\n-\t\t\tsizeof(\"fatal error: cgo callback before cgo call\\n\")-1);\n-\t\truntime_exit(1);\n-\t}\n-\n-\t// Lock extra list, take head, unlock popped list.\n-\t// nilokay=false is safe here because of the invariant above,\n-\t// that the extra list always contains or will soon contain\n-\t// at least one m.\n-\tmp = lockextra(false);\n-\n-\t// Set needextram when we've just emptied the list,\n-\t// so that the eventual call into cgocallbackg will\n-\t// allocate a new m for the extra list. We delay the\n-\t// allocation until then so that it can be done\n-\t// after exitsyscall makes sure it is okay to be\n-\t// running at all (that is, there's no garbage collection\n-\t// running right now).\n-\tmp->needextram = mp->schedlink == 0;\n-\tunlockextra((M*)mp->schedlink);\n-\n-\t// Install g (= m->curg).\n-\truntime_setg(mp->curg);\n-\n-\t// Initialize g's context as in mstart.\n \tinitcontext();\n-\tg->atomicstatus = _Gsyscall;\n \tg->entry = nil;\n \tg->param = nil;\n #ifdef USING_SPLIT_STACK\n \t__splitstack_getcontext(&g->stackcontext[0]);\n+\tval = 0;\n+\t__splitstack_block_signals(&val, nil);\n #else\n-\tg->gcinitialsp = &mp;\n+\tg->gcinitialsp = &val;\n \tg->gcstack = nil;\n \tg->gcstacksize = 0;\n-\tg->gcnextsp = &mp;\n+\tg->gcnextsp = &val;\n #endif\n \tgetcontext(ucontext_arg(&g->context[0]));\n \n@@ -1219,168 +1113,21 @@ runtime_needm(void)\n \t\tpfn(gp);\n \t\t*(int*)0x22 = 0x22;\n \t}\n-\n-\t// Initialize this thread to use the m.\n-\truntime_minit();\n-\n-#ifdef USING_SPLIT_STACK\n-\t{\n-\t\tint dont_block_signals = 0;\n-\t\t__splitstack_block_signals(&dont_block_signals, nil);\n-\t}\n-#endif\n }\n \n-// newextram allocates an m and puts it on the extra list.\n-// It is called with a working local m, so that it can do things\n-// like call schedlock and allocate.\n+void makeGContext(G*, byte*, uintptr)\n+\t__asm__(GOSYM_PREFIX \"runtime.makeGContext\");\n+\n+// makeGContext makes a new context for a g.\n void\n-runtime_newextram(void)\n-{\n-\tM *mp, *mnext;\n-\tG *gp;\n-\tbyte *g0_sp, *sp;\n-\tuintptr g0_spsize, spsize;\n+makeGContext(G* gp, byte* sp, uintptr spsize) {\n \tucontext_t *uc;\n \n-\t// Create extra goroutine locked to extra m.\n-\t// The goroutine is the context in which the cgo callback will run.\n-\t// The sched.pc will never be returned to, but setting it to\n-\t// runtime.goexit makes clear to the traceback routines where\n-\t// the goroutine stack ends.\n-\tmp = runtime_allocm(nil, StackMin, &g0_sp, &g0_spsize);\n-\tgp = runtime_malg(StackMin, &sp, &spsize);\n-\tgp->atomicstatus = _Gdead;\n-\tgp->m = mp;\n-\tmp->curg = gp;\n-\tmp->locked = _LockInternal;\n-\tmp->lockedg = gp;\n-\tgp->lockedm = mp;\n-\tgp->goid = runtime_xadd64(&runtime_sched->goidgen, 1);\n-\t// put on allg for garbage collector\n-\tallgadd(gp);\n-\n-\t// The context for gp will be set up in runtime_needm.  But\n-\t// here we need to set up the context for g0.\n-\tuc = ucontext_arg(&mp->g0->context[0]);\n+\tuc = ucontext_arg(&gp->context[0]);\n \tgetcontext(uc);\n-\tuc->uc_stack.ss_sp = g0_sp;\n-\tuc->uc_stack.ss_size = (size_t)g0_spsize;\n+\tuc->uc_stack.ss_sp = sp;\n+\tuc->uc_stack.ss_size = (size_t)spsize;\n \tmakecontext(uc, kickoff, 0);\n-\n-\t// Add m to the extra list.\n-\tmnext = lockextra(true);\n-\tmp->schedlink = (uintptr)mnext;\n-\tunlockextra(mp);\n-}\n-\n-// dropm is called when a cgo callback has called needm but is now\n-// done with the callback and returning back into the non-Go thread.\n-// It puts the current m back onto the extra list.\n-//\n-// The main expense here is the call to signalstack to release the\n-// m's signal stack, and then the call to needm on the next callback\n-// from this thread. It is tempting to try to save the m for next time,\n-// which would eliminate both these costs, but there might not be\n-// a next time: the current thread (which Go does not control) might exit.\n-// If we saved the m for that thread, there would be an m leak each time\n-// such a thread exited. Instead, we acquire and release an m on each\n-// call. These should typically not be scheduling operations, just a few\n-// atomics, so the cost should be small.\n-//\n-// TODO(rsc): An alternative would be to allocate a dummy pthread per-thread\n-// variable using pthread_key_create. Unlike the pthread keys we already use\n-// on OS X, this dummy key would never be read by Go code. It would exist\n-// only so that we could register at thread-exit-time destructor.\n-// That destructor would put the m back onto the extra list.\n-// This is purely a performance optimization. The current version,\n-// in which dropm happens on each cgo call, is still correct too.\n-// We may have to keep the current version on systems with cgo\n-// but without pthreads, like Windows.\n-void\n-runtime_dropm(void)\n-{\n-\tM *mp, *mnext;\n-\n-\t// Undo whatever initialization minit did during needm.\n-\truntime_unminit();\n-\n-\t// Clear m and g, and return m to the extra list.\n-\t// After the call to setg we can only call nosplit functions.\n-\tmp = g->m;\n-\truntime_setg(nil);\n-\n-\tmp->curg->atomicstatus = _Gdead;\n-\tmp->curg->gcstack = nil;\n-\tmp->curg->gcnextsp = nil;\n-\n-\tmnext = lockextra(true);\n-\tmp->schedlink = (uintptr)mnext;\n-\tunlockextra(mp);\n-}\n-\n-#define MLOCKED ((M*)1)\n-\n-// lockextra locks the extra list and returns the list head.\n-// The caller must unlock the list by storing a new list head\n-// to runtime.extram. If nilokay is true, then lockextra will\n-// return a nil list head if that's what it finds. If nilokay is false,\n-// lockextra will keep waiting until the list head is no longer nil.\n-static M*\n-lockextra(bool nilokay)\n-{\n-\tM *mp;\n-\tvoid (*yield)(void);\n-\n-\tfor(;;) {\n-\t\tmp = runtime_atomicloadp(&runtime_extram);\n-\t\tif(mp == MLOCKED) {\n-\t\t\tyield = runtime_osyield;\n-\t\t\tyield();\n-\t\t\tcontinue;\n-\t\t}\n-\t\tif(mp == nil && !nilokay) {\n-\t\t\truntime_usleep(1);\n-\t\t\tcontinue;\n-\t\t}\n-\t\tif(!runtime_casp(&runtime_extram, mp, MLOCKED)) {\n-\t\t\tyield = runtime_osyield;\n-\t\t\tyield();\n-\t\t\tcontinue;\n-\t\t}\n-\t\tbreak;\n-\t}\n-\treturn mp;\n-}\n-\n-static void\n-unlockextra(M *mp)\n-{\n-\truntime_atomicstorep(&runtime_extram, mp);\n-}\n-\n-static int32\n-countextra()\n-{\n-\tM *mp, *mc;\n-\tint32 c;\n-\n-\tfor(;;) {\n-\t\tmp = runtime_atomicloadp(&runtime_extram);\n-\t\tif(mp == MLOCKED) {\n-\t\t\truntime_osyield();\n-\t\t\tcontinue;\n-\t\t}\n-\t\tif(!runtime_casp(&runtime_extram, mp, MLOCKED)) {\n-\t\t\truntime_osyield();\n-\t\t\tcontinue;\n-\t\t}\n-\t\tc = 0;\n-\t\tfor(mc = mp; mc != nil; mc = (M*)mc->schedlink)\n-\t\t\tc++;\n-\t\truntime_atomicstorep(&runtime_extram, mp);\n-\t\treturn c;\n-\t}\n }\n \n // Create a new m.  It will start off with a call to fn, or else the scheduler.\n@@ -1389,7 +1136,7 @@ newm(void(*fn)(void), P *p)\n {\n \tM *mp;\n \n-\tmp = runtime_allocm(p, -1, nil, nil);\n+\tmp = runtime_allocm(p, false, nil, nil);\n \tmp->nextp = (uintptr)p;\n \tmp->mstartfn = (uintptr)(void*)fn;\n \n@@ -2287,16 +2034,35 @@ syscall_runtime_AfterFork(void)\n \n // Allocate a new g, with a stack big enough for stacksize bytes.\n G*\n-runtime_malg(int32 stacksize, byte** ret_stack, uintptr* ret_stacksize)\n+runtime_malg(bool allocatestack, bool signalstack, byte** ret_stack, uintptr* ret_stacksize)\n {\n+\tuintptr stacksize;\n \tG *newg;\n+\tbyte* unused_stack;\n+\tuintptr unused_stacksize;\n+#if USING_SPLIT_STACK\n+\tint dont_block_signals = 0;\n+\tsize_t ss_stacksize;\n+#endif\n \n+\tif (ret_stack == nil) {\n+\t\tret_stack = &unused_stack;\n+\t}\n+\tif (ret_stacksize == nil) {\n+\t\tret_stacksize = &unused_stacksize;\n+\t}\n \tnewg = allocg();\n-\tif(stacksize >= 0) {\n-#if USING_SPLIT_STACK\n-\t\tint dont_block_signals = 0;\n-\t\tsize_t ss_stacksize;\n+\tif(allocatestack) {\n+\t\tstacksize = StackMin;\n+\t\tif(signalstack) {\n+\t\t\tstacksize = 32 * 1024; // OS X wants >= 8K, GNU/Linux >= 2K\n+#ifdef SIGSTKSZ\n+\t\t\tif(stacksize < SIGSTKSZ)\n+\t\t\t\tstacksize = SIGSTKSZ;\n+#endif\n+\t\t}\n \n+#if USING_SPLIT_STACK\n \t\t*ret_stack = __splitstack_makecontext(stacksize,\n \t\t\t\t\t\t      &newg->stackcontext[0],\n \t\t\t\t\t\t      &ss_stacksize);\n@@ -2361,7 +2127,7 @@ __go_go(void (*fn)(void*), void* arg)\n \t} else {\n \t\tuintptr malsize;\n \n-\t\tnewg = runtime_malg(StackMin, &sp, &malsize);\n+\t\tnewg = runtime_malg(true, false, &sp, &malsize);\n \t\tspsize = (size_t)malsize;\n \t\tallgadd(newg);\n \t}\n@@ -2376,30 +2142,17 @@ __go_go(void (*fn)(void*), void* arg)\n \t}\n \tnewg->goid = p->goidcache++;\n \n-\t{\n-\t\t// Avoid warnings about variables clobbered by\n-\t\t// longjmp.\n-\t\tbyte * volatile vsp = sp;\n-\t\tsize_t volatile vspsize = spsize;\n-\t\tG * volatile vnewg = newg;\n-\t\tucontext_t * volatile uc;\n-\n-\t\tuc = ucontext_arg(&vnewg->context[0]);\n-\t\tgetcontext(uc);\n-\t\tuc->uc_stack.ss_sp = vsp;\n-\t\tuc->uc_stack.ss_size = vspsize;\n-\t\tmakecontext(uc, kickoff, 0);\n+\tmakeGContext(newg, sp, (uintptr)spsize);\n \n-\t\trunqput(p, vnewg);\n+\trunqput(p, newg);\n \n-\t\tif(runtime_atomicload(&runtime_sched->npidle) != 0 && runtime_atomicload(&runtime_sched->nmspinning) == 0 && fn != runtime_main)  // TODO: fast atomic\n-\t\t\twakep();\n-\t\tg->m->locks--;\n-\t\treturn vnewg;\n-\t}\n+\tif(runtime_atomicload(&runtime_sched->npidle) != 0 && runtime_atomicload(&runtime_sched->nmspinning) == 0 && fn != runtime_main)  // TODO: fast atomic\n+\t\twakep();\n+\tg->m->locks--;\n+\treturn newg;\n }\n \n-static void\n+void\n allgadd(G *gp)\n {\n \tG **new;\n@@ -2902,7 +2655,7 @@ checkdead(void)\n \t}\n \n \t// -1 for sysmon\n-\trun = runtime_sched->mcount - runtime_sched->nmidle - runtime_sched->nmidlelocked - 1 - countextra();\n+\trun = runtime_sched->mcount - runtime_sched->nmidle - runtime_sched->nmidlelocked - 1;\n \tif(run > 0)\n \t\treturn;\n \t// If we are dying because of a signal caught on an already idle thread,\n@@ -3534,12 +3287,6 @@ sync_atomic_runtime_procUnpin()\n \tprocUnpin();\n }\n \n-void\n-runtime_proc_scan(struct Workbuf** wbufp, void (*enqueue1)(struct Workbuf**, Obj))\n-{\n-\tenqueue1(wbufp, (Obj){(byte*)&runtime_main_init_done, sizeof runtime_main_init_done, 0});\n-}\n-\n // Return whether we are waiting for a GC.  This gc toolchain uses\n // preemption instead.\n bool"}, {"sha": "6cbf02df3689e778205ad9afe40aad0bdd3eb1f5", "filename": "libgo/runtime/runtime.h", "status": "modified", "additions": 21, "deletions": 16, "changes": 37, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fruntime.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fruntime.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.h?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -52,7 +52,7 @@ typedef uintptr\t\tuintreg;\n \n /* Defined types.  */\n \n-typedef\tuint8\t\t\tbool;\n+typedef\t_Bool\t\t\tbool;\n typedef\tuint8\t\t\tbyte;\n typedef\tstruct\tg\t\tG;\n typedef\tstruct\tmutex\t\tLock;\n@@ -240,7 +240,6 @@ extern\tM*\truntime_allm;\n extern\tP**\truntime_allp;\n extern\tSched*  runtime_sched;\n extern\tint32\truntime_gomaxprocs;\n-extern\tuint32\truntime_needextram;\n extern\tuint32\truntime_panicking(void)\n   __asm__ (GOSYM_PREFIX \"runtime.getPanicking\");\n extern\tint8*\truntime_goos;\n@@ -298,15 +297,13 @@ void\truntime_ready(G*);\n String\truntime_getenv(const char*);\n int32\truntime_atoi(const byte*, intgo);\n void*\truntime_mstart(void*);\n-G*\truntime_malg(int32, byte**, uintptr*);\n+G*\truntime_malg(bool, bool, byte**, uintptr*)\n+\t__asm__(GOSYM_PREFIX \"runtime.malg\");\n void\truntime_mpreinit(M*);\n-void\truntime_minit(void);\n-void\truntime_unminit(void);\n-void\truntime_needm(void)\n-  __asm__ (GOSYM_PREFIX \"runtime.needm\");\n-void\truntime_dropm(void)\n-  __asm__ (GOSYM_PREFIX \"runtime.dropm\");\n-void\truntime_signalstack(byte*, int32);\n+void\truntime_minit(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.minit\");\n+void\truntime_signalstack(byte*, uintptr)\n+  __asm__ (GOSYM_PREFIX \"runtime.signalstack\");\n MCache*\truntime_allocmcache(void)\n   __asm__ (GOSYM_PREFIX \"runtime.allocmcache\");\n void\truntime_freemcache(MCache*);\n@@ -345,7 +342,8 @@ int32\truntime_round2(int32 x); // round x up to a power of 2.\n \n void runtime_setg(G*)\n   __asm__ (GOSYM_PREFIX \"runtime.setg\");\n-void runtime_newextram(void);\n+void runtime_newextram(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.newextram\");\n #define runtime_exit(s) exit(s)\n #define runtime_breakpoint() __builtin_trap()\n void\truntime_gosched(void);\n@@ -523,9 +521,12 @@ void\truntime_procyield(uint32)\n   __asm__(GOSYM_PREFIX \"runtime.procyield\");\n void\truntime_osyield(void)\n   __asm__(GOSYM_PREFIX \"runtime.osyield\");\n-void\truntime_lockOSThread(void);\n-void\truntime_unlockOSThread(void);\n-bool\truntime_lockedOSThread(void);\n+void\truntime_lockOSThread(void)\n+  __asm__(GOSYM_PREFIX \"runtime.lockOSThread\");\n+void\truntime_unlockOSThread(void)\n+  __asm__(GOSYM_PREFIX \"runtime.unlockOSThread\");\n+bool\truntime_lockedOSThread(void)\n+  __asm__(GOSYM_PREFIX \"runtime.lockedOSThread\");\n \n void\truntime_printcreatedby(G*)\n   __asm__(GOSYM_PREFIX \"runtime.printcreatedby\");\n@@ -587,14 +588,18 @@ struct time_now_ret now() __asm__ (GOSYM_PREFIX \"time.now\")\n extern void _cgo_wait_runtime_init_done (void);\n extern void _cgo_notify_runtime_init_done (void);\n extern _Bool runtime_iscgo;\n-extern _Bool runtime_cgoHasExtraM;\n-extern Hchan *runtime_main_init_done;\n extern uintptr __go_end __attribute__ ((weak));\n extern void *getitab(const struct __go_type_descriptor *,\n \t\t     const struct __go_type_descriptor *,\n \t\t     _Bool)\n   __asm__ (GOSYM_PREFIX \"runtime.getitab\");\n \n extern void runtime_cpuinit(void);\n+extern void setIsCgo(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.setIsCgo\");\n extern void setCpuidECX(uint32)\n   __asm__ (GOSYM_PREFIX \"runtime.setCpuidECX\");\n+extern void makeMainInitDone(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.makeMainInitDone\");\n+extern void closeMainInitDone(void)\n+  __asm__ (GOSYM_PREFIX \"runtime.closeMainInitDone\");"}, {"sha": "ad167877bc890221e29771aaf07e7fce148545b5", "filename": "libgo/runtime/runtime_c.c", "status": "modified", "additions": 2, "deletions": 33, "changes": 35, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fruntime_c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a/libgo%2Fruntime%2Fruntime_c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime_c.c?ref=0d3dd8fb65050363f1f82b5f048799fd9a0a0f5a", "patch": "@@ -99,43 +99,12 @@ runtime_cputicks(void)\n void\n runtime_mpreinit(M *mp)\n {\n-\tint32 stacksize = 32 * 1024;\t// OS X wants >=8K, Linux >=2K\n-\n-#ifdef SIGSTKSZ\n-\tif(stacksize < SIGSTKSZ)\n-\t\tstacksize = SIGSTKSZ;\n-#endif\n-\n-\tmp->gsignal = runtime_malg(stacksize, (byte**)&mp->gsignalstack, &mp->gsignalstacksize);\n+\tmp->gsignal = runtime_malg(true, true, (byte**)&mp->gsignalstack, &mp->gsignalstacksize);\n \tmp->gsignal->m = mp;\n }\n \n-// Called to initialize a new m (including the bootstrap m).\n-// Called on the new thread, can not allocate memory.\n-void\n-runtime_minit(void)\n-{\n-\tM* m;\n-\tsigset_t sigs;\n-\n-\t// Initialize signal handling.\n-\tm = runtime_m();\n-\truntime_signalstack(m->gsignalstack, m->gsignalstacksize);\n-\tif (sigemptyset(&sigs) != 0)\n-\t\truntime_throw(\"sigemptyset\");\n-\tpthread_sigmask(SIG_SETMASK, &sigs, nil);\n-}\n-\n-// Called from dropm to undo the effect of an minit.\n-void\n-runtime_unminit(void)\n-{\n-\truntime_signalstack(nil, 0);\n-}\n-\n-\n void\n-runtime_signalstack(byte *p, int32 n)\n+runtime_signalstack(byte *p, uintptr n)\n {\n \tstack_t st;\n "}]}