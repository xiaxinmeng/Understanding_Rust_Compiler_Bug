{"sha": "c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YzNhOGY5NjRhYjMzMGQ4ZTY0YmI0ZDMzYzQ2MmM2NGY0ZmEzNWFlYg==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2018-01-03T21:47:34Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2018-01-03T21:47:34Z"}, "message": "Make vectorizable_load/store handle IFN_MASK_LOAD/STORE\n\nAfter the previous patches, it's easier to see that the remaining\ninlined transform code in vectorizable_mask_load_store is just a\ncut-down version of the VMAT_CONTIGUOUS handling in vectorizable_load\nand vectorizable_store.  This patch therefore makes those functions\nhandle masked loads and stores instead.\n\nThis makes it easier to handle more forms of masked load and store\nwithout duplicating logic from the unmasked forms.  It also helps with\nsupport for fully-masked loops.\n\n2018-01-03  Richard Sandiford  <richard.sandiford@linaro.org>\n\ngcc/\n\t* tree-vect-stmts.c (vect_get_store_rhs): New function.\n\t(vectorizable_mask_load_store): Delete.\n\t(vectorizable_call): Return false for masked loads and stores.\n\t(vectorizable_store): Handle IFN_MASK_STORE.  Use vect_get_store_rhs\n\tinstead of gimple_assign_rhs1.\n\t(vectorizable_load): Handle IFN_MASK_LOAD.\n\t(vect_transform_stmt): Don't set is_store for call_vec_info_type.\n\nFrom-SVN: r256216", "tree": {"sha": "06bd50ff8379abd5eac104fd39994ff29828baa5", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/06bd50ff8379abd5eac104fd39994ff29828baa5"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb/comments", "author": null, "committer": null, "parents": [{"sha": "c48d2d35a122798c047ca7a0f7e0d64293023c44", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c48d2d35a122798c047ca7a0f7e0d64293023c44", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c48d2d35a122798c047ca7a0f7e0d64293023c44"}], "stats": {"total": 608, "additions": 269, "deletions": 339}, "files": [{"sha": "8a905da7edc0bdf018d0800bfc7f0337a2c30cac", "filename": "gcc/ChangeLog", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb", "patch": "@@ -1,3 +1,13 @@\n+2018-01-03  Richard Sandiford  <richard.sandiford@linaro.org>\n+\n+\t* tree-vect-stmts.c (vect_get_store_rhs): New function.\n+\t(vectorizable_mask_load_store): Delete.\n+\t(vectorizable_call): Return false for masked loads and stores.\n+\t(vectorizable_store): Handle IFN_MASK_STORE.  Use vect_get_store_rhs\n+\tinstead of gimple_assign_rhs1.\n+\t(vectorizable_load): Handle IFN_MASK_LOAD.\n+\t(vect_transform_stmt): Don't set is_store for call_vec_info_type.\n+\n 2018-01-03  Richard Sandiford  <richard.sandiford@linaro.org>\n \n \t* tree-vect-stmts.c (vect_build_gather_load_calls): New function,"}, {"sha": "16088b080fee6a0411d818e1dad7c91cbae9caf4", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 259, "deletions": 339, "changes": 598, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=c3a8f964ab330d8e64bb4d33c462c64f4fa35aeb", "patch": "@@ -1726,6 +1726,26 @@ perm_mask_for_reverse (tree vectype)\n   return vect_gen_perm_mask_checked (vectype, indices);\n }\n \n+/* STMT is either a masked or unconditional store.  Return the value\n+   being stored.  */\n+\n+static tree\n+vect_get_store_rhs (gimple *stmt)\n+{\n+  if (gassign *assign = dyn_cast <gassign *> (stmt))\n+    {\n+      gcc_assert (gimple_assign_single_p (assign));\n+      return gimple_assign_rhs1 (assign);\n+    }\n+  if (gcall *call = dyn_cast <gcall *> (stmt))\n+    {\n+      internal_fn ifn = gimple_call_internal_fn (call);\n+      gcc_assert (ifn == IFN_MASK_STORE);\n+      return gimple_call_arg (stmt, 3);\n+    }\n+  gcc_unreachable ();\n+}\n+\n /* A subroutine of get_load_store_type, with a subset of the same\n    arguments.  Handle the case where STMT is part of a grouped load\n    or store.\n@@ -2398,251 +2418,6 @@ vect_build_gather_load_calls (gimple *stmt, gimple_stmt_iterator *gsi,\n     }\n }\n \n-/* Function vectorizable_mask_load_store.\n-\n-   Check if STMT performs a conditional load or store that can be vectorized.\n-   If VEC_STMT is also passed, vectorize the STMT: create a vectorized\n-   stmt to replace it, put it in VEC_STMT, and insert it at GSI.\n-   Return FALSE if not a vectorizable STMT, TRUE otherwise.  */\n-\n-static bool\n-vectorizable_mask_load_store (gimple *stmt, gimple_stmt_iterator *gsi,\n-\t\t\t      gimple **vec_stmt, slp_tree slp_node)\n-{\n-  tree vec_dest = NULL;\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n-  stmt_vec_info prev_stmt_info;\n-  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n-  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n-  bool nested_in_vect_loop = nested_in_vect_loop_p (loop, stmt);\n-  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n-  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n-  tree rhs_vectype = NULL_TREE;\n-  tree mask_vectype;\n-  tree elem_type;\n-  gimple *new_stmt;\n-  tree dummy;\n-  tree dataref_ptr = NULL_TREE;\n-  gimple *ptr_incr;\n-  int ncopies;\n-  int i;\n-  bool inv_p;\n-  gather_scatter_info gs_info;\n-  vec_load_store_type vls_type;\n-  tree mask;\n-  gimple *def_stmt;\n-  enum vect_def_type dt;\n-\n-  if (slp_node != NULL)\n-    return false;\n-\n-  ncopies = vect_get_num_copies (loop_vinfo, vectype);\n-  gcc_assert (ncopies >= 1);\n-\n-  /* FORNOW. This restriction should be relaxed.  */\n-  if (nested_in_vect_loop && ncopies > 1)\n-    {\n-      if (dump_enabled_p ())\n-\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-\t\t\t \"multiple types in nested loop.\");\n-      return false;\n-    }\n-\n-  if (!STMT_VINFO_RELEVANT_P (stmt_info))\n-    return false;\n-\n-  if (STMT_VINFO_DEF_TYPE (stmt_info) != vect_internal_def\n-      && ! vec_stmt)\n-    return false;\n-\n-  if (!STMT_VINFO_DATA_REF (stmt_info))\n-    return false;\n-\n-  mask = gimple_call_arg (stmt, 2);\n-  if (!vect_check_load_store_mask (stmt, mask, &mask_vectype))\n-    return false;\n-\n-  elem_type = TREE_TYPE (vectype);\n-\n-  if (gimple_call_internal_fn (stmt) == IFN_MASK_STORE)\n-    {\n-      tree rhs = gimple_call_arg (stmt, 3);\n-      if (!vect_check_store_rhs (stmt, rhs, &rhs_vectype, &vls_type))\n-\treturn false;\n-    }\n-  else\n-    vls_type = VLS_LOAD;\n-\n-  vect_memory_access_type memory_access_type;\n-  if (!get_load_store_type (stmt, vectype, false, vls_type, ncopies,\n-\t\t\t    &memory_access_type, &gs_info))\n-    return false;\n-\n-  if (memory_access_type == VMAT_GATHER_SCATTER)\n-    {\n-      tree arglist = TYPE_ARG_TYPES (TREE_TYPE (gs_info.decl));\n-      tree masktype\n-\t= TREE_VALUE (TREE_CHAIN (TREE_CHAIN (TREE_CHAIN (arglist))));\n-      if (TREE_CODE (masktype) == INTEGER_TYPE)\n-\t{\n-\t  if (dump_enabled_p ())\n-\t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-\t\t\t     \"masked gather with integer mask not supported.\");\n-\t  return false;\n-\t}\n-    }\n-  else if (memory_access_type != VMAT_CONTIGUOUS)\n-    {\n-      if (dump_enabled_p ())\n-\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-\t\t\t \"unsupported access type for masked %s.\\n\",\n-\t\t\t vls_type == VLS_LOAD ? \"load\" : \"store\");\n-      return false;\n-    }\n-  else if (!VECTOR_MODE_P (TYPE_MODE (vectype))\n-\t   || !can_vec_mask_load_store_p (TYPE_MODE (vectype),\n-\t\t\t\t\t  TYPE_MODE (mask_vectype),\n-\t\t\t\t\t  vls_type == VLS_LOAD))\n-    return false;\n-\n-  if (!vec_stmt) /* transformation not required.  */\n-    {\n-      STMT_VINFO_MEMORY_ACCESS_TYPE (stmt_info) = memory_access_type;\n-      STMT_VINFO_TYPE (stmt_info) = call_vec_info_type;\n-      if (vls_type == VLS_LOAD)\n-\tvect_model_load_cost (stmt_info, ncopies, memory_access_type,\n-\t\t\t      NULL, NULL, NULL);\n-      else\n-\tvect_model_store_cost (stmt_info, ncopies, memory_access_type,\n-\t\t\t       vls_type, NULL, NULL, NULL);\n-      return true;\n-    }\n-  gcc_assert (memory_access_type == STMT_VINFO_MEMORY_ACCESS_TYPE (stmt_info));\n-\n-  /* Transform.  */\n-\n-  if (memory_access_type == VMAT_GATHER_SCATTER)\n-    {\n-      vect_build_gather_load_calls (stmt, gsi, vec_stmt, &gs_info, mask);\n-      return true;\n-    }\n-  else if (vls_type != VLS_LOAD)\n-    {\n-      tree vec_rhs = NULL_TREE, vec_mask = NULL_TREE;\n-      prev_stmt_info = NULL;\n-      LOOP_VINFO_HAS_MASK_STORE (loop_vinfo) = true;\n-      for (i = 0; i < ncopies; i++)\n-\t{\n-\t  unsigned align, misalign;\n-\n-\t  if (i == 0)\n-\t    {\n-\t      tree rhs = gimple_call_arg (stmt, 3);\n-\t      vec_rhs = vect_get_vec_def_for_operand (rhs, stmt);\n-\t      vec_mask = vect_get_vec_def_for_operand (mask, stmt,\n-\t\t\t\t\t\t       mask_vectype);\n-\t      /* We should have catched mismatched types earlier.  */\n-\t      gcc_assert (useless_type_conversion_p (vectype,\n-\t\t\t\t\t\t     TREE_TYPE (vec_rhs)));\n-\t      dataref_ptr = vect_create_data_ref_ptr (stmt, vectype, NULL,\n-\t\t\t\t\t\t      NULL_TREE, &dummy, gsi,\n-\t\t\t\t\t\t      &ptr_incr, false, &inv_p);\n-\t      gcc_assert (!inv_p);\n-\t    }\n-\t  else\n-\t    {\n-\t      vect_is_simple_use (vec_rhs, loop_vinfo, &def_stmt, &dt);\n-\t      vec_rhs = vect_get_vec_def_for_stmt_copy (dt, vec_rhs);\n-\t      vect_is_simple_use (vec_mask, loop_vinfo, &def_stmt, &dt);\n-\t      vec_mask = vect_get_vec_def_for_stmt_copy (dt, vec_mask);\n-\t      dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi, stmt,\n-\t\t\t\t\t     TYPE_SIZE_UNIT (vectype));\n-\t    }\n-\n-\t  align = DR_TARGET_ALIGNMENT (dr);\n-\t  if (aligned_access_p (dr))\n-\t    misalign = 0;\n-\t  else if (DR_MISALIGNMENT (dr) == -1)\n-\t    {\n-\t      align = TYPE_ALIGN_UNIT (elem_type);\n-\t      misalign = 0;\n-\t    }\n-\t  else\n-\t    misalign = DR_MISALIGNMENT (dr);\n-\t  set_ptr_info_alignment (get_ptr_info (dataref_ptr), align,\n-\t\t\t\t  misalign);\n-\t  tree ptr = build_int_cst (TREE_TYPE (gimple_call_arg (stmt, 1)),\n-\t\t\t\t    misalign ? least_bit_hwi (misalign) : align);\n-\t  gcall *call\n-\t    = gimple_build_call_internal (IFN_MASK_STORE, 4, dataref_ptr,\n-\t\t\t\t\t  ptr, vec_mask, vec_rhs);\n-\t  gimple_call_set_nothrow (call, true);\n-\t  new_stmt = call;\n-\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n-\t  if (i == 0)\n-\t    STMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = new_stmt;\n-\t  else\n-\t    STMT_VINFO_RELATED_STMT (prev_stmt_info) = new_stmt;\n-\t  prev_stmt_info = vinfo_for_stmt (new_stmt);\n-\t}\n-    }\n-  else\n-    {\n-      tree vec_mask = NULL_TREE;\n-      prev_stmt_info = NULL;\n-      vec_dest = vect_create_destination_var (gimple_call_lhs (stmt), vectype);\n-      for (i = 0; i < ncopies; i++)\n-\t{\n-\t  unsigned align, misalign;\n-\n-\t  if (i == 0)\n-\t    {\n-\t      vec_mask = vect_get_vec_def_for_operand (mask, stmt,\n-\t\t\t\t\t\t       mask_vectype);\n-\t      dataref_ptr = vect_create_data_ref_ptr (stmt, vectype, NULL,\n-\t\t\t\t\t\t      NULL_TREE, &dummy, gsi,\n-\t\t\t\t\t\t      &ptr_incr, false, &inv_p);\n-\t      gcc_assert (!inv_p);\n-\t    }\n-\t  else\n-\t    {\n-\t      vect_is_simple_use (vec_mask, loop_vinfo, &def_stmt, &dt);\n-\t      vec_mask = vect_get_vec_def_for_stmt_copy (dt, vec_mask);\n-\t      dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi, stmt,\n-\t\t\t\t\t     TYPE_SIZE_UNIT (vectype));\n-\t    }\n-\n-\t  align = DR_TARGET_ALIGNMENT (dr);\n-\t  if (aligned_access_p (dr))\n-\t    misalign = 0;\n-\t  else if (DR_MISALIGNMENT (dr) == -1)\n-\t    {\n-\t      align = TYPE_ALIGN_UNIT (elem_type);\n-\t      misalign = 0;\n-\t    }\n-\t  else\n-\t    misalign = DR_MISALIGNMENT (dr);\n-\t  set_ptr_info_alignment (get_ptr_info (dataref_ptr), align,\n-\t\t\t\t  misalign);\n-\t  tree ptr = build_int_cst (TREE_TYPE (gimple_call_arg (stmt, 1)),\n-\t\t\t\t    misalign ? least_bit_hwi (misalign) : align);\n-\t  gcall *call\n-\t    = gimple_build_call_internal (IFN_MASK_LOAD, 3, dataref_ptr,\n-\t\t\t\t\t  ptr, vec_mask);\n-\t  gimple_call_set_lhs (call, make_ssa_name (vec_dest));\n-\t  gimple_call_set_nothrow (call, true);\n-\t  vect_finish_stmt_generation (stmt, call, gsi);\n-\t  if (i == 0)\n-\t    STMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = call;\n-\t  else\n-\t    STMT_VINFO_RELATED_STMT (prev_stmt_info) = call;\n-\t  prev_stmt_info = vinfo_for_stmt (call);\n-\t}\n-    }\n-\n-  return true;\n-}\n-\n /* Check and perform vectorization of BUILT_IN_BSWAP{16,32,64}.  */\n \n static bool\n@@ -2833,8 +2608,8 @@ vectorizable_call (gimple *gs, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   if (gimple_call_internal_p (stmt)\n       && (gimple_call_internal_fn (stmt) == IFN_MASK_LOAD\n \t  || gimple_call_internal_fn (stmt) == IFN_MASK_STORE))\n-    return vectorizable_mask_load_store (stmt, gsi, vec_stmt,\n-\t\t\t\t\t slp_node);\n+    /* Handled by vectorizable_load and vectorizable_store.  */\n+    return false;\n \n   if (gimple_call_lhs (stmt) == NULL_TREE\n       || TREE_CODE (gimple_call_lhs (stmt)) != SSA_NAME)\n@@ -5851,7 +5626,6 @@ static bool\n vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n                     slp_tree slp_node)\n {\n-  tree scalar_dest;\n   tree data_ref;\n   tree op;\n   tree vec_oprnd = NULL_TREE;\n@@ -5900,28 +5674,48 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \n   /* Is vectorizable store? */\n \n-  if (!is_gimple_assign (stmt))\n-    return false;\n+  tree mask = NULL_TREE, mask_vectype = NULL_TREE;\n+  if (is_gimple_assign (stmt))\n+    {\n+      tree scalar_dest = gimple_assign_lhs (stmt);\n+      if (TREE_CODE (scalar_dest) == VIEW_CONVERT_EXPR\n+\t  && is_pattern_stmt_p (stmt_info))\n+\tscalar_dest = TREE_OPERAND (scalar_dest, 0);\n+      if (TREE_CODE (scalar_dest) != ARRAY_REF\n+\t  && TREE_CODE (scalar_dest) != BIT_FIELD_REF\n+\t  && TREE_CODE (scalar_dest) != INDIRECT_REF\n+\t  && TREE_CODE (scalar_dest) != COMPONENT_REF\n+\t  && TREE_CODE (scalar_dest) != IMAGPART_EXPR\n+\t  && TREE_CODE (scalar_dest) != REALPART_EXPR\n+\t  && TREE_CODE (scalar_dest) != MEM_REF)\n+\treturn false;\n+    }\n+  else\n+    {\n+      gcall *call = dyn_cast <gcall *> (stmt);\n+      if (!call || !gimple_call_internal_p (call, IFN_MASK_STORE))\n+\treturn false;\n \n-  scalar_dest = gimple_assign_lhs (stmt);\n-  if (TREE_CODE (scalar_dest) == VIEW_CONVERT_EXPR\n-      && is_pattern_stmt_p (stmt_info))\n-    scalar_dest = TREE_OPERAND (scalar_dest, 0);\n-  if (TREE_CODE (scalar_dest) != ARRAY_REF\n-      && TREE_CODE (scalar_dest) != BIT_FIELD_REF\n-      && TREE_CODE (scalar_dest) != INDIRECT_REF\n-      && TREE_CODE (scalar_dest) != COMPONENT_REF\n-      && TREE_CODE (scalar_dest) != IMAGPART_EXPR\n-      && TREE_CODE (scalar_dest) != REALPART_EXPR\n-      && TREE_CODE (scalar_dest) != MEM_REF)\n-    return false;\n+      if (slp_node != NULL)\n+\t{\n+\t  if (dump_enabled_p ())\n+\t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t     \"SLP of masked stores not supported.\\n\");\n+\t  return false;\n+\t}\n+\n+      ref_type = TREE_TYPE (gimple_call_arg (call, 1));\n+      mask = gimple_call_arg (call, 2);\n+      if (!vect_check_load_store_mask (stmt, mask, &mask_vectype))\n+\treturn false;\n+    }\n+\n+  op = vect_get_store_rhs (stmt);\n \n   /* Cannot have hybrid store SLP -- that would mean storing to the\n      same location twice.  */\n   gcc_assert (slp == PURE_SLP_STMT (stmt_info));\n \n-  gcc_assert (gimple_assign_single_p (stmt));\n-\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info), rhs_vectype = NULL_TREE;\n   poly_uint64 nunits = TYPE_VECTOR_SUBPARTS (vectype);\n \n@@ -5952,18 +5746,12 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n       return false;\n     }\n \n-  op = gimple_assign_rhs1 (stmt);\n   if (!vect_check_store_rhs (stmt, op, &rhs_vectype, &vls_type))\n     return false;\n \n   elem_type = TREE_TYPE (vectype);\n   vec_mode = TYPE_MODE (vectype);\n \n-  /* FORNOW. In some cases can vectorize even if data-type not supported\n-     (e.g. - array initialization with 0).  */\n-  if (optab_handler (mov_optab, vec_mode) == CODE_FOR_nothing)\n-    return false;\n-\n   if (!STMT_VINFO_DATA_REF (stmt_info))\n     return false;\n \n@@ -5972,6 +5760,28 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t\t    &memory_access_type, &gs_info))\n     return false;\n \n+  if (mask)\n+    {\n+      if (memory_access_type != VMAT_CONTIGUOUS)\n+\t{\n+\t  if (dump_enabled_p ())\n+\t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t     \"unsupported access type for masked store.\\n\");\n+\t  return false;\n+\t}\n+      if (!VECTOR_MODE_P (vec_mode)\n+\t  || !can_vec_mask_load_store_p (vec_mode, TYPE_MODE (mask_vectype),\n+\t\t\t\t\t false))\n+\treturn false;\n+    }\n+  else\n+    {\n+      /* FORNOW. In some cases can vectorize even if data-type not supported\n+\t (e.g. - array initialization with 0).  */\n+      if (optab_handler (mov_optab, vec_mode) == CODE_FOR_nothing)\n+\treturn false;\n+    }\n+\n   if (!vec_stmt) /* transformation not required.  */\n     {\n       STMT_VINFO_MEMORY_ACCESS_TYPE (stmt_info) = memory_access_type;\n@@ -5990,7 +5800,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \n   if (memory_access_type == VMAT_GATHER_SCATTER)\n     {\n-      tree vec_oprnd0 = NULL_TREE, vec_oprnd1 = NULL_TREE, op, src;\n+      tree vec_oprnd0 = NULL_TREE, vec_oprnd1 = NULL_TREE, src;\n       tree arglist = TYPE_ARG_TYPES (TREE_TYPE (gs_info.decl));\n       tree rettype, srctype, ptrtype, idxtype, masktype, scaletype;\n       tree ptr, mask, var, scale, perm_mask = NULL_TREE;\n@@ -6069,7 +5879,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t  if (j == 0)\n \t    {\n \t      src = vec_oprnd1\n-\t\t= vect_get_vec_def_for_operand (gimple_assign_rhs1 (stmt), stmt);\n+\t\t= vect_get_vec_def_for_operand (op, stmt);\n \t      op = vec_oprnd0\n \t\t= vect_get_vec_def_for_operand (gs_info.offset, stmt);\n \t    }\n@@ -6169,7 +5979,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n           first_stmt = SLP_TREE_SCALAR_STMTS (slp_node)[0]; \n \t  gcc_assert (GROUP_FIRST_ELEMENT (vinfo_for_stmt (first_stmt)) == first_stmt);\n           first_dr = STMT_VINFO_DATA_REF (vinfo_for_stmt (first_stmt));\n-\t  op = gimple_assign_rhs1 (first_stmt);\n+\t  op = vect_get_store_rhs (first_stmt);\n         } \n       else\n         /* VEC_NUM is the number of vect stmts to be created for this \n@@ -6338,7 +6148,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t    elsz = tree_to_uhwi (TYPE_SIZE_UNIT (TREE_TYPE (vectype)));\n \t  for (j = 0; j < ncopies; j++)\n \t    {\n-\t      /* We've set op and dt above, from gimple_assign_rhs1(stmt),\n+\t      /* We've set op and dt above, from vect_get_store_rhs,\n \t\t and first_stmt == stmt.  */\n \t      if (j == 0)\n \t\t{\n@@ -6350,8 +6160,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t    }\n \t\t  else\n \t\t    {\n-\t\t      gcc_assert (gimple_assign_single_p (next_stmt));\n-\t\t      op = gimple_assign_rhs1 (next_stmt);\n+\t\t      op = vect_get_store_rhs (next_stmt);\n \t\t      vec_oprnd = vect_get_vec_def_for_operand (op, next_stmt);\n \t\t    }\n \t\t}\n@@ -6438,8 +6247,9 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   alignment_support_scheme = vect_supportable_dr_alignment (first_dr, false);\n   gcc_assert (alignment_support_scheme);\n   /* Targets with store-lane instructions must not require explicit\n-     realignment.  */\n-  gcc_assert (memory_access_type != VMAT_LOAD_STORE_LANES\n+     realignment.  vect_supportable_dr_alignment always returns either\n+     dr_aligned or dr_unaligned_supported for masked operations.  */\n+  gcc_assert ((memory_access_type != VMAT_LOAD_STORE_LANES && !mask)\n \t      || alignment_support_scheme == dr_aligned\n \t      || alignment_support_scheme == dr_unaligned_supported);\n \n@@ -6452,6 +6262,9 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   else\n     aggr_type = vectype;\n \n+  if (mask)\n+    LOOP_VINFO_HAS_MASK_STORE (loop_vinfo) = true;\n+\n   /* In case the vectorization factor (VF) is bigger than the number\n      of elements that we can fit in a vectype (nunits), we have to generate\n      more than one vector stmt - i.e - we need to \"unroll\" the\n@@ -6492,6 +6305,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   */\n \n   prev_stmt_info = NULL;\n+  tree vec_mask = NULL_TREE;\n   for (j = 0; j < ncopies; j++)\n     {\n \n@@ -6522,15 +6336,15 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t     Therefore, NEXT_STMT can't be NULL_TREE.  In case that\n \t\t     there is no interleaving, GROUP_SIZE is 1, and only one\n \t\t     iteration of the loop will be executed.  */\n-\t\t  gcc_assert (next_stmt\n-\t\t\t      && gimple_assign_single_p (next_stmt));\n-\t\t  op = gimple_assign_rhs1 (next_stmt);\n-\n+\t\t  op = vect_get_store_rhs (next_stmt);\n \t\t  vec_oprnd = vect_get_vec_def_for_operand (op, next_stmt);\n \t\t  dr_chain.quick_push (vec_oprnd);\n \t\t  oprnds.quick_push (vec_oprnd);\n \t\t  next_stmt = GROUP_NEXT_ELEMENT (vinfo_for_stmt (next_stmt));\n \t\t}\n+\t      if (mask)\n+\t\tvec_mask = vect_get_vec_def_for_operand (mask, stmt,\n+\t\t\t\t\t\t\t mask_vectype);\n \t    }\n \n \t  /* We should have catched mismatched types earlier.  */\n@@ -6575,6 +6389,11 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t      dr_chain[i] = vec_oprnd;\n \t      oprnds[i] = vec_oprnd;\n \t    }\n+\t  if (mask)\n+\t    {\n+\t      vect_is_simple_use (vec_mask, vinfo, &def_stmt, &dt);\n+\t      vec_mask = vect_get_vec_def_for_stmt_copy (dt, vec_mask);\n+\t    }\n \t  if (dataref_offset)\n \t    dataref_offset\n \t      = int_const_binop (PLUS_EXPR, dataref_offset,\n@@ -6635,29 +6454,16 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t   vect_permute_store_chain().  */\n \t\tvec_oprnd = result_chain[i];\n \n-\t      data_ref = fold_build2 (MEM_REF, vectype,\n-\t\t\t\t      dataref_ptr,\n-\t\t\t\t      dataref_offset\n-\t\t\t\t      ? dataref_offset\n-\t\t\t\t      : build_int_cst (ref_type, 0));\n \t      align = DR_TARGET_ALIGNMENT (first_dr);\n \t      if (aligned_access_p (first_dr))\n \t\tmisalign = 0;\n \t      else if (DR_MISALIGNMENT (first_dr) == -1)\n \t\t{\n \t\t  align = dr_alignment (vect_dr_behavior (first_dr));\n \t\t  misalign = 0;\n-\t\t  TREE_TYPE (data_ref)\n-\t\t    = build_aligned_type (TREE_TYPE (data_ref),\n-\t\t\t\t\t  align * BITS_PER_UNIT);\n \t\t}\n \t      else\n-\t\t{\n-\t\t  TREE_TYPE (data_ref)\n-\t\t    = build_aligned_type (TREE_TYPE (data_ref),\n-\t\t\t\t\t  TYPE_ALIGN (elem_type));\n-\t\t  misalign = DR_MISALIGNMENT (first_dr);\n-\t\t}\n+\t\tmisalign = DR_MISALIGNMENT (first_dr);\n \t      if (dataref_offset == NULL_TREE\n \t\t  && TREE_CODE (dataref_ptr) == SSA_NAME)\n \t\tset_ptr_info_alignment (get_ptr_info (dataref_ptr), align,\n@@ -6667,7 +6473,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t{\n \t\t  tree perm_mask = perm_mask_for_reverse (vectype);\n \t\t  tree perm_dest \n-\t\t    = vect_create_destination_var (gimple_assign_rhs1 (stmt),\n+\t\t    = vect_create_destination_var (vect_get_store_rhs (stmt),\n \t\t\t\t\t\t   vectype);\n \t\t  tree new_temp = make_ssa_name (perm_dest);\n \n@@ -6682,7 +6488,36 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t}\n \n \t      /* Arguments are ready.  Create the new vector stmt.  */\n-\t      new_stmt = gimple_build_assign (data_ref, vec_oprnd);\n+\t      if (mask)\n+\t\t{\n+\t\t  align = least_bit_hwi (misalign | align);\n+\t\t  tree ptr = build_int_cst (ref_type, align);\n+\t\t  gcall *call\n+\t\t    = gimple_build_call_internal (IFN_MASK_STORE, 4,\n+\t\t\t\t\t\t  dataref_ptr, ptr,\n+\t\t\t\t\t\t  vec_mask, vec_oprnd);\n+\t\t  gimple_call_set_nothrow (call, true);\n+\t\t  new_stmt = call;\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  data_ref = fold_build2 (MEM_REF, vectype,\n+\t\t\t\t\t  dataref_ptr,\n+\t\t\t\t\t  dataref_offset\n+\t\t\t\t\t  ? dataref_offset\n+\t\t\t\t\t  : build_int_cst (ref_type, 0));\n+\t\t  if (aligned_access_p (first_dr))\n+\t\t    ;\n+\t\t  else if (DR_MISALIGNMENT (first_dr) == -1)\n+\t\t    TREE_TYPE (data_ref)\n+\t\t      = build_aligned_type (TREE_TYPE (data_ref),\n+\t\t\t\t\t    align * BITS_PER_UNIT);\n+\t\t  else\n+\t\t    TREE_TYPE (data_ref)\n+\t\t      = build_aligned_type (TREE_TYPE (data_ref),\n+\t\t\t\t\t    TYPE_ALIGN (elem_type));\n+\t\t  new_stmt = gimple_build_assign (data_ref, vec_oprnd);\n+\t\t}\n \t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n \n \t      if (slp)\n@@ -6865,7 +6700,6 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   int vec_num;\n   bool slp = (slp_node != NULL);\n   bool slp_perm = false;\n-  enum tree_code code;\n   bb_vec_info bb_vinfo = STMT_VINFO_BB_VINFO (stmt_info);\n   poly_uint64 vf;\n   tree aggr_type;\n@@ -6880,24 +6714,46 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n       && ! vec_stmt)\n     return false;\n \n-  /* Is vectorizable load? */\n-  if (!is_gimple_assign (stmt))\n-    return false;\n+  tree mask = NULL_TREE, mask_vectype = NULL_TREE;\n+  if (is_gimple_assign (stmt))\n+    {\n+      scalar_dest = gimple_assign_lhs (stmt);\n+      if (TREE_CODE (scalar_dest) != SSA_NAME)\n+\treturn false;\n \n-  scalar_dest = gimple_assign_lhs (stmt);\n-  if (TREE_CODE (scalar_dest) != SSA_NAME)\n-    return false;\n+      tree_code code = gimple_assign_rhs_code (stmt);\n+      if (code != ARRAY_REF\n+\t  && code != BIT_FIELD_REF\n+\t  && code != INDIRECT_REF\n+\t  && code != COMPONENT_REF\n+\t  && code != IMAGPART_EXPR\n+\t  && code != REALPART_EXPR\n+\t  && code != MEM_REF\n+\t  && TREE_CODE_CLASS (code) != tcc_declaration)\n+\treturn false;\n+    }\n+  else\n+    {\n+      gcall *call = dyn_cast <gcall *> (stmt);\n+      if (!call || !gimple_call_internal_p (call, IFN_MASK_LOAD))\n+\treturn false;\n \n-  code = gimple_assign_rhs_code (stmt);\n-  if (code != ARRAY_REF\n-      && code != BIT_FIELD_REF\n-      && code != INDIRECT_REF\n-      && code != COMPONENT_REF\n-      && code != IMAGPART_EXPR\n-      && code != REALPART_EXPR\n-      && code != MEM_REF\n-      && TREE_CODE_CLASS (code) != tcc_declaration)\n-    return false;\n+      scalar_dest = gimple_call_lhs (call);\n+      if (!scalar_dest)\n+\treturn false;\n+\n+      if (slp_node != NULL)\n+\t{\n+\t  if (dump_enabled_p ())\n+\t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t     \"SLP of masked loads not supported.\\n\");\n+\t  return false;\n+\t}\n+\n+      mask = gimple_call_arg (call, 2);\n+      if (!vect_check_load_store_mask (stmt, mask, &mask_vectype))\n+\treturn false;\n+    }\n \n   if (!STMT_VINFO_DATA_REF (stmt_info))\n     return false;\n@@ -7008,6 +6864,38 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t\t    &memory_access_type, &gs_info))\n     return false;\n \n+  if (mask)\n+    {\n+      if (memory_access_type == VMAT_CONTIGUOUS)\n+\t{\n+\t  if (!VECTOR_MODE_P (TYPE_MODE (vectype))\n+\t      || !can_vec_mask_load_store_p (TYPE_MODE (vectype),\n+\t\t\t\t\t     TYPE_MODE (mask_vectype), true))\n+\t    return false;\n+\t}\n+      else if (memory_access_type == VMAT_GATHER_SCATTER)\n+\t{\n+\t  tree arglist = TYPE_ARG_TYPES (TREE_TYPE (gs_info.decl));\n+\t  tree masktype\n+\t    = TREE_VALUE (TREE_CHAIN (TREE_CHAIN (TREE_CHAIN (arglist))));\n+\t  if (TREE_CODE (masktype) == INTEGER_TYPE)\n+\t    {\n+\t      if (dump_enabled_p ())\n+\t\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t\t \"masked gather with integer mask not\"\n+\t\t\t\t \" supported.\");\n+\t      return false;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  if (dump_enabled_p ())\n+\t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t     \"unsupported access type for masked load.\\n\");\n+\t  return false;\n+\t}\n+    }\n+\n   if (!vec_stmt) /* transformation not required.  */\n     {\n       if (!slp)\n@@ -7034,7 +6922,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \n   if (memory_access_type == VMAT_GATHER_SCATTER)\n     {\n-      vect_build_gather_load_calls (stmt, gsi, vec_stmt, &gs_info, NULL_TREE);\n+      vect_build_gather_load_calls (stmt, gsi, vec_stmt, &gs_info, mask);\n       return true;\n     }\n \n@@ -7471,6 +7359,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   else\n     aggr_type = vectype;\n \n+  tree vec_mask = NULL_TREE;\n   prev_stmt_info = NULL;\n   poly_uint64 group_elt = 0;\n   for (j = 0; j < ncopies; j++)\n@@ -7518,13 +7407,26 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t\t\t\t  offset, &dummy, gsi, &ptr_incr,\n \t\t\t\t\t  simd_lane_access_p, &inv_p,\n \t\t\t\t\t  byte_offset);\n+\t  if (mask)\n+\t    vec_mask = vect_get_vec_def_for_operand (mask, stmt,\n+\t\t\t\t\t\t     mask_vectype);\n \t}\n-      else if (dataref_offset)\n-\tdataref_offset = int_const_binop (PLUS_EXPR, dataref_offset,\n-\t\t\t\t\t  TYPE_SIZE_UNIT (aggr_type));\n       else\n-        dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi, stmt,\n-\t\t\t\t       TYPE_SIZE_UNIT (aggr_type));\n+\t{\n+\t  if (dataref_offset)\n+\t    dataref_offset = int_const_binop (PLUS_EXPR, dataref_offset,\n+\t\t\t\t\t      TYPE_SIZE_UNIT (aggr_type));\n+\t  else\n+\t    dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi, stmt,\n+\t\t\t\t\t   TYPE_SIZE_UNIT (aggr_type));\n+\t  if (mask)\n+\t    {\n+\t      gimple *def_stmt;\n+\t      vect_def_type dt;\n+\t      vect_is_simple_use (vec_mask, vinfo, &def_stmt, &dt);\n+\t      vec_mask = vect_get_vec_def_for_stmt_copy (dt, vec_mask);\n+\t    }\n+\t}\n \n       if (grouped_load || slp_perm)\n \tdr_chain.create (vec_num);\n@@ -7572,11 +7474,6 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t  {\n \t\t    unsigned int align, misalign;\n \n-\t\t    data_ref\n-\t\t      = fold_build2 (MEM_REF, vectype, dataref_ptr,\n-\t\t\t\t     dataref_offset\n-\t\t\t\t     ? dataref_offset\n-\t\t\t\t     : build_int_cst (ref_type, 0));\n \t\t    align = DR_TARGET_ALIGNMENT (dr);\n \t\t    if (alignment_support_scheme == dr_aligned)\n \t\t      {\n@@ -7587,21 +7484,44 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t      {\n \t\t\talign = dr_alignment (vect_dr_behavior (first_dr));\n \t\t\tmisalign = 0;\n-\t\t\tTREE_TYPE (data_ref)\n-\t\t\t  = build_aligned_type (TREE_TYPE (data_ref),\n-\t\t\t\t\t\talign * BITS_PER_UNIT);\n \t\t      }\n \t\t    else\n-\t\t      {\n-\t\t\tTREE_TYPE (data_ref)\n-\t\t\t  = build_aligned_type (TREE_TYPE (data_ref),\n-\t\t\t\t\t\tTYPE_ALIGN (elem_type));\n-\t\t\tmisalign = DR_MISALIGNMENT (first_dr);\n-\t\t      }\n+\t\t      misalign = DR_MISALIGNMENT (first_dr);\n \t\t    if (dataref_offset == NULL_TREE\n \t\t\t&& TREE_CODE (dataref_ptr) == SSA_NAME)\n \t\t      set_ptr_info_alignment (get_ptr_info (dataref_ptr),\n \t\t\t\t\t      align, misalign);\n+\n+\t\t    if (mask)\n+\t\t      {\n+\t\t\talign = least_bit_hwi (misalign | align);\n+\t\t\ttree ptr = build_int_cst (ref_type, align);\n+\t\t\tgcall *call\n+\t\t\t  = gimple_build_call_internal (IFN_MASK_LOAD, 3,\n+\t\t\t\t\t\t\tdataref_ptr, ptr,\n+\t\t\t\t\t\t\tvec_mask);\n+\t\t\tgimple_call_set_nothrow (call, true);\n+\t\t\tnew_stmt = call;\n+\t\t\tdata_ref = NULL_TREE;\n+\t\t      }\n+\t\t    else\n+\t\t      {\n+\t\t\tdata_ref\n+\t\t\t  = fold_build2 (MEM_REF, vectype, dataref_ptr,\n+\t\t\t\t\t dataref_offset\n+\t\t\t\t\t ? dataref_offset\n+\t\t\t\t\t : build_int_cst (ref_type, 0));\n+\t\t\tif (alignment_support_scheme == dr_aligned)\n+\t\t\t  ;\n+\t\t\telse if (DR_MISALIGNMENT (first_dr) == -1)\n+\t\t\t  TREE_TYPE (data_ref)\n+\t\t\t    = build_aligned_type (TREE_TYPE (data_ref),\n+\t\t\t\t\t\t  align * BITS_PER_UNIT);\n+\t\t\telse\n+\t\t\t  TREE_TYPE (data_ref)\n+\t\t\t    = build_aligned_type (TREE_TYPE (data_ref),\n+\t\t\t\t\t\t  TYPE_ALIGN (elem_type));\n+\t\t      }\n \t\t    break;\n \t\t  }\n \t\tcase dr_explicit_realign:\n@@ -7677,9 +7597,11 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t  gcc_unreachable ();\n \t\t}\n \t      vec_dest = vect_create_destination_var (scalar_dest, vectype);\n-\t      new_stmt = gimple_build_assign (vec_dest, data_ref);\n+\t      /* DATA_REF is null if we've already built the statement.  */\n+\t      if (data_ref)\n+\t\tnew_stmt = gimple_build_assign (vec_dest, data_ref);\n \t      new_temp = make_ssa_name (vec_dest, new_stmt);\n-\t      gimple_assign_set_lhs (new_stmt, new_temp);\n+\t      gimple_set_lhs (new_stmt, new_temp);\n \t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n \n \t      /* 3. Handle explicit realignment if necessary/supported.\n@@ -8898,8 +8820,6 @@ vect_transform_stmt (gimple *stmt, gimple_stmt_iterator *gsi,\n     case call_vec_info_type:\n       done = vectorizable_call (stmt, gsi, &vec_stmt, slp_node);\n       stmt = gsi_stmt (*gsi);\n-      if (gimple_call_internal_p (stmt, IFN_MASK_STORE))\n-\tis_store = true;\n       break;\n \n     case call_simd_clone_vec_info_type:"}]}