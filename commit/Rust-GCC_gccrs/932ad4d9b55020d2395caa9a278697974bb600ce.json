{"sha": "932ad4d9b55020d2395caa9a278697974bb600ce", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OTMyYWQ0ZDliNTUwMjBkMjM5NWNhYTlhMjc4Njk3OTc0YmI2MDBjZQ==", "commit": {"author": {"name": "Steven Bosscher", "email": "steven@gcc.gnu.org", "date": "2006-12-10T10:59:19Z"}, "committer": {"name": "Steven Bosscher", "email": "steven@gcc.gnu.org", "date": "2006-12-10T10:59:19Z"}, "message": "cse.c: (struct cse_basic_block_data): Remove LAST field.\n\n\t* cse.c: (struct cse_basic_block_data): Remove LAST field.\n\t(struct branch_path): Remove BRANCH and TAKEN fields. Add new\n\tBB field.\n\t(cse_visited_basic_blocks): New static bitmap.\n\t(cse_end_of_basic_block, cse_basic_block): Remove.\n\t(cse_find_path, cse_dump_path, cse_prescan_path,\n\tcse_extended_basic_block): New static functions.\n\t(cse_insn): Don't CSE over setjmp calls.  Use the CFG to find\n\tbasic block boundaries.  Don't record jump equivalences here.\n\tUpdate the CFG after doing in-place replacement of the SET_SRC.\n\t(cse_main): Rewrite.  Look for extended basic block headers\n\tand call cse_extended_basic_block on them until all paths that\n\tstart at this header are exhausted.\n\t(rest_of_handle_cse): Verify that the CFG is incrementally updated\n\tand correct after cse_main.\n\tDon't call delete_trivially_dead_insns, let cfgcleanup do that.\n\t(rest_of_handle_cse2): Verify the CFG here, too, after cse_main.\n\t(pass_cse): Add TODO_verify_flow.\n\t(pass_cse2): Likewise.\n\nFrom-SVN: r119706", "tree": {"sha": "12674a0e4957c90392cd1cdebd9abf48f1a23fe2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/12674a0e4957c90392cd1cdebd9abf48f1a23fe2"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/932ad4d9b55020d2395caa9a278697974bb600ce", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/932ad4d9b55020d2395caa9a278697974bb600ce", "html_url": "https://github.com/Rust-GCC/gccrs/commit/932ad4d9b55020d2395caa9a278697974bb600ce", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/932ad4d9b55020d2395caa9a278697974bb600ce/comments", "author": null, "committer": null, "parents": [{"sha": "15447faef3ab7462fa6b6cb39f744126b3c9a315", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/15447faef3ab7462fa6b6cb39f744126b3c9a315", "html_url": "https://github.com/Rust-GCC/gccrs/commit/15447faef3ab7462fa6b6cb39f744126b3c9a315"}], "stats": {"total": 804, "additions": 429, "deletions": 375}, "files": [{"sha": "b984524281dc897657eb4994df51744741104988", "filename": "gcc/ChangeLog", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/932ad4d9b55020d2395caa9a278697974bb600ce/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/932ad4d9b55020d2395caa9a278697974bb600ce/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=932ad4d9b55020d2395caa9a278697974bb600ce", "patch": "@@ -1,3 +1,25 @@\n+2006-12-10  Steven Bosscher  <steven@gcc.gnu.org>\n+\n+\t* cse.c: (struct cse_basic_block_data): Remove LAST field.\n+\t(struct branch_path): Remove BRANCH and TAKEN fields. Add new\n+\tBB field.\n+\t(cse_visited_basic_blocks): New static bitmap.\n+\t(cse_end_of_basic_block, cse_basic_block): Remove.\n+\t(cse_find_path, cse_dump_path, cse_prescan_path,\n+\tcse_extended_basic_block): New static functions.\n+\t(cse_insn): Don't CSE over setjmp calls.  Use the CFG to find\n+\tbasic block boundaries.  Don't record jump equivalences here.\n+\tUpdate the CFG after doing in-place replacement of the SET_SRC.\n+\t(cse_main): Rewrite.  Look for extended basic block headers\n+\tand call cse_extended_basic_block on them until all paths that\n+\tstart at this header are exhausted.\n+\t(rest_of_handle_cse): Verify that the CFG is incrementally updated\n+\tand correct after cse_main.\n+\tDon't call delete_trivially_dead_insns, let cfgcleanup do that.\n+\t(rest_of_handle_cse2): Verify the CFG here, too, after cse_main.\n+\t(pass_cse): Add TODO_verify_flow.\n+\t(pass_cse2): Likewise.\n+\n 2006-12-10  Rask Ingemann Lambertsen  <rask@sygehus.dk>\n \n \t* reload1.c (choose_reload_regs): Don't set byte offset when"}, {"sha": "015e1de66d4e471c8f71ad3cf05b0ee2ec33372c", "filename": "gcc/cse.c", "status": "modified", "additions": 407, "deletions": 375, "changes": 782, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/932ad4d9b55020d2395caa9a278697974bb600ce/gcc%2Fcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/932ad4d9b55020d2395caa9a278697974bb600ce/gcc%2Fcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcse.c?ref=932ad4d9b55020d2395caa9a278697974bb600ce", "patch": "@@ -336,11 +336,11 @@ static unsigned int cse_reg_info_table_size;\n static unsigned int cse_reg_info_table_first_uninitialized;\n \n /* The timestamp at the beginning of the current run of\n-   cse_basic_block.  We increment this variable at the beginning of\n-   the current run of cse_basic_block.  The timestamp field of a\n+   cse_extended_basic_block.  We increment this variable at the beginning of\n+   the current run of cse_extended_basic_block.  The timestamp field of a\n    cse_reg_info entry matches the value of this variable if and only\n    if the entry has been initialized during the current run of\n-   cse_basic_block.  */\n+   cse_extended_basic_block.  */\n static unsigned int cse_reg_info_timestamp;\n \n /* A HARD_REG_SET containing all the hard registers for which there is\n@@ -536,7 +536,8 @@ static struct table_elt *free_element_chain;\n static int constant_pool_entries_cost;\n static int constant_pool_entries_regcost;\n \n-/* This data describes a block that will be processed by cse_basic_block.  */\n+/* This data describes a block that will be processed by\n+   cse_extended_basic_block.  */\n \n struct cse_basic_block_data\n {\n@@ -546,20 +547,20 @@ struct cse_basic_block_data\n   int high_cuid;\n   /* Total number of SETs in block.  */\n   int nsets;\n-  /* Last insn in the block.  */\n-  rtx last;\n   /* Size of current branch path, if any.  */\n   int path_size;\n-  /* Current branch path, indicating which branches will be taken.  */\n+  /* Current path, indicating which basic_blocks will be processed.  */\n   struct branch_path\n     {\n-      /* The branch insn.  */\n-      rtx branch;\n-      /* Whether it should be taken or not.  */\n-      enum taken {PATH_TAKEN, PATH_NOT_TAKEN} status;\n+      /* The basic block for this path entry.  */\n+      basic_block bb;\n     } *path;\n };\n \n+/* A simple bitmap to track which basic blocks have been visited\n+   already as part of an already processed extended basic block.  */\n+static sbitmap cse_visited_basic_blocks;\n+\n static bool fixed_base_plus_p (rtx x);\n static int notreg_cost (rtx, enum rtx_code);\n static int approx_reg_cost_1 (rtx *, void *);\n@@ -602,11 +603,10 @@ static void record_jump_equiv (rtx, bool);\n static void record_jump_cond (enum rtx_code, enum machine_mode, rtx, rtx,\n \t\t\t      int);\n static void cse_insn (rtx, rtx);\n-static void cse_end_of_basic_block (rtx, struct cse_basic_block_data *,\n-\t\t\t\t    int);\n+static void cse_prescan_path (struct cse_basic_block_data *);\n static void invalidate_from_clobbers (rtx);\n static rtx cse_process_notes (rtx, rtx);\n-static rtx cse_basic_block (rtx, rtx, struct branch_path *);\n+static void cse_extended_basic_block (struct cse_basic_block_data *);\n static void count_reg_usage (rtx, int *, rtx, int);\n static int check_for_label_ref (rtx *, void *);\n extern void dump_class (struct table_elt*);\n@@ -4862,6 +4862,14 @@ cse_insn (rtx insn, rtx libcall_insn)\n \n \t      validate_change (insn, &SET_SRC (sets[i].rtl), new, 1);\n \t      apply_change_group ();\n+\n+\t      /* With non-call exceptions, if this was an insn that could\n+\t\t trap, we may have made it non-throwing now.  For example\n+\t\t we may have replaced a load with a register.  */\n+\t      if (flag_non_call_exceptions\n+\t\t  && insn == BB_END (BLOCK_FOR_INSN (insn)))\n+\t\tpurge_dead_edges (BLOCK_FOR_INSN (insn));\n+\n \t      break;\n \t    }\n \n@@ -5317,6 +5325,15 @@ cse_insn (rtx insn, rtx libcall_insn)\n       && MEM_VOLATILE_P (PATTERN (insn)))\n     flush_hash_table ();\n \n+  /* Don't cse over a call to setjmp; on some machines (eg VAX)\n+     the regs restored by the longjmp come from a later time\n+     than the setjmp.  */\n+  if (CALL_P (insn) && find_reg_note (insn, REG_SETJMP, NULL))\n+    {\n+      flush_hash_table ();\n+      goto done;\n+    }\n+\n   /* Make sure registers mentioned in destinations\n      are safe for use in an expression to be inserted.\n      This removes from the hash table\n@@ -5578,15 +5595,15 @@ cse_insn (rtx insn, rtx libcall_insn)\n       if ((src_ent->first_reg == REGNO (SET_DEST (sets[0].rtl)))\n \t  && ! find_reg_note (insn, REG_RETVAL, NULL_RTX))\n \t{\n-\t  rtx prev = insn;\n \t  /* Scan for the previous nonnote insn, but stop at a basic\n \t     block boundary.  */\n+\t  rtx prev = insn;\n+\t  rtx bb_head = BB_HEAD (BLOCK_FOR_INSN (insn));\n \t  do\n \t    {\n \t      prev = PREV_INSN (prev);\n \t    }\n-\t  while (prev && NOTE_P (prev)\n-\t\t && NOTE_LINE_NUMBER (prev) != NOTE_INSN_BASIC_BLOCK);\n+\t  while (prev != bb_head && NOTE_P (prev));\n \n \t  /* Do not swap the registers around if the previous instruction\n \t     attaches a REG_EQUIV note to REG1.\n@@ -5599,8 +5616,7 @@ cse_insn (rtx insn, rtx libcall_insn)\n \t     This section previously turned the REG_EQUIV into a REG_EQUAL\n \t     note.  We cannot do that because REG_EQUIV may provide an\n \t     uninitialized stack slot when REG_PARM_STACK_SPACE is used.  */\n-\n-\t  if (prev != 0 && NONJUMP_INSN_P (prev)\n+\t  if (NONJUMP_INSN_P (prev)\n \t      && GET_CODE (PATTERN (prev)) == SET\n \t      && SET_DEST (PATTERN (prev)) == SET_SRC (sets[0].rtl)\n \t      && ! find_reg_note (prev, REG_EQUIV, NULL_RTX))\n@@ -5627,12 +5643,7 @@ cse_insn (rtx insn, rtx libcall_insn)\n \t}\n     }\n \n-  /* If this is a conditional jump insn, record any known equivalences due to\n-     the condition being tested.  */\n-\n-  if (n_sets == 1 && any_condjump_p (insn))\n-    record_jump_equiv (insn, false);\n-\n+done:;\n #ifdef HAVE_cc0\n   /* If the previous insn set CC0 and this insn no longer references CC0,\n      delete the previous insn.  Here we use the fact that nothing expects CC0\n@@ -5796,168 +5807,337 @@ cse_process_notes (rtx x, rtx object)\n   return x;\n }\n \f\n-/* Find the end of INSN's basic block and return its range,\n-   the total number of SETs in all the insns of the block, the last insn of the\n-   block, and the branch path.\n+/* Find a path in the CFG, starting with FIRST_BB to perform CSE on.\n \n-   The branch path indicates which branches should be followed.  If a nonzero\n-   path size is specified, the block should be rescanned and a different set\n-   of branches will be taken.  The branch path is only used if\n-   FLAG_CSE_FOLLOW_JUMPS is nonzero.\n+   DATA is a pointer to a struct cse_basic_block_data, that is used to\n+   describe the path.\n+   It is filled with a queue of basic blocks, starting with FIRST_BB\n+   and following a trace through the CFG.\n+  \n+   If all paths starting at FIRST_BB have been followed, or no new path\n+   starting at FIRST_BB can be constructed, this function returns FALSE.\n+   Otherwise, DATA->path is filled and the function returns TRUE indicating\n+   that a path to follow was found.\n \n-   DATA is a pointer to a struct cse_basic_block_data, defined below, that is\n-   used to describe the block.  It is filled in with the information about\n-   the current block.  The incoming structure's branch path, if any, is used\n-   to construct the output branch path.  */\n+   If FOLLOW_JUMPS is false, the maximum path lenghth is 1 and the only\n+   block in the path will be FIRST_BB.  */\n \n-static void\n-cse_end_of_basic_block (rtx insn, struct cse_basic_block_data *data,\n-\t\t\tint follow_jumps)\n+static bool\n+cse_find_path (basic_block first_bb, struct cse_basic_block_data *data,\n+\t       int follow_jumps)\n {\n-  rtx p = insn, q;\n-  int nsets = 0;\n-  int low_cuid = INSN_CUID (insn), high_cuid = INSN_CUID (insn);\n-  rtx next = INSN_P (insn) ? insn : next_real_insn (insn);\n-  int path_size = data->path_size;\n-  int path_entry = 0;\n-  int i;\n+  basic_block bb;\n+  edge e;\n+  int path_size;\n+ \n+  SET_BIT (cse_visited_basic_blocks, first_bb->index);\n \n-  /* Update the previous branch path, if any.  If the last branch was\n-     previously PATH_TAKEN, mark it PATH_NOT_TAKEN.\n-     If it was previously PATH_NOT_TAKEN,\n-     shorten the path by one and look at the previous branch.  We know that\n-     at least one branch must have been taken if PATH_SIZE is nonzero.  */\n-  while (path_size > 0)\n+  /* See if there is a previous path.  */\n+  path_size = data->path_size;\n+\n+  /* There is a previous path.  Make sure it started with FIRST_BB.  */\n+  if (path_size)\n+    gcc_assert (data->path[0].bb == first_bb);\n+\n+  /* There was only one basic block in the last path.  Clear the path and\n+     return, so that paths starting at another basic block can be tried.  */\n+  if (path_size == 1)\n+    {\n+      path_size = 0;\n+      goto done;\n+    }\n+\n+  /* If the path was empty from the beginning, construct a new path.  */\n+  if (path_size == 0)\n+    data->path[path_size++].bb = first_bb;\n+  else\n     {\n-      if (data->path[path_size - 1].status != PATH_NOT_TAKEN)\n+      /* Otherwise, path_size must be equal to or greater than 2, because\n+\t a previous path exists that is at least two basic blocks long.\n+\n+\t Update the previous branch path, if any.  If the last branch was\n+\t previously along the branch edge, take the fallthrough edge now.  */\n+      while (path_size >= 2)\n \t{\n-\t  data->path[path_size - 1].status = PATH_NOT_TAKEN;\n-\t  break;\n+\t  basic_block last_bb_in_path, previous_bb_in_path;\n+\t  edge e;\n+\n+\t  --path_size;\n+\t  last_bb_in_path = data->path[path_size].bb;\n+\t  previous_bb_in_path = data->path[path_size - 1].bb;\n+\n+\t  /* If we previously followed a path along the branch edge, try\n+\t     the fallthru edge now.  */\n+\t  if (EDGE_COUNT (previous_bb_in_path->succs) == 2\n+\t      && any_condjump_p (BB_END (previous_bb_in_path))\n+\t      && (e = find_edge (previous_bb_in_path, last_bb_in_path))\n+\t      && e == BRANCH_EDGE (previous_bb_in_path))\n+\t    {\n+\t      bb = FALLTHRU_EDGE (previous_bb_in_path)->dest;\n+\t      if (bb != EXIT_BLOCK_PTR\n+\t\t  && single_pred_p (bb))\n+\t\t{\n+#if ENABLE_CHECKING\n+\t\t  /* We should only see blocks here that we have not\n+\t\t     visited yet.  */\n+\t\t  gcc_assert (!TEST_BIT (cse_visited_basic_blocks, bb->index));\n+#endif\n+\t\t  SET_BIT (cse_visited_basic_blocks, bb->index);\n+\t\t  data->path[path_size++].bb = bb;\n+\t\t  break;\n+\t\t}\n+\t    }\n+\n+\t  data->path[path_size].bb = NULL;\n+\t}\n+\n+      /* If only one block remains in the path, bail.  */\n+      if (path_size == 1)\n+\t{\n+\t  path_size = 0;\n+\t  goto done;\n \t}\n-      else\n-\tpath_size--;\n     }\n \n-  /* If the first instruction is marked with QImode, that means we've\n-     already processed this block.  Our caller will look at DATA->LAST\n-     to figure out where to go next.  We want to return the next block\n-     in the instruction stream, not some branched-to block somewhere\n-     else.  We accomplish this by pretending our called forbid us to\n-     follow jumps.  */\n-  if (GET_MODE (insn) == QImode)\n-    follow_jumps = 0;\n-\n-  /* Scan to end of this basic block.  */\n-  while (p && !LABEL_P (p))\n+  /* Extend the path if possible.  */\n+  if (follow_jumps)\n     {\n-      /* Don't cse over a call to setjmp; on some machines (eg VAX)\n-\t the regs restored by the longjmp come from\n-\t a later time than the setjmp.  */\n-      if (PREV_INSN (p) && CALL_P (PREV_INSN (p))\n-\t  && find_reg_note (PREV_INSN (p), REG_SETJMP, NULL))\n-\tbreak;\n+      bb = data->path[path_size - 1].bb;\n+      while (bb && path_size < PARAM_VALUE (PARAM_MAX_CSE_PATH_LENGTH))\n+\t{\n+\t  if (single_succ_p (bb))\n+\t    e = single_succ_edge (bb);\n+\t  else if (EDGE_COUNT (bb->succs) == 2\n+\t\t   && any_condjump_p (BB_END (bb)))\n+\t    {\n+\t      /* First try to follow the branch.  If that doesn't lead\n+\t\t to a useful path, follow the fallthru edge.  */\n+\t      e = BRANCH_EDGE (bb);\n+\t      if (!single_pred_p (e->dest))\n+\t\te = FALLTHRU_EDGE (bb);\n+\t    }\n+\t  else\n+\t    e = NULL;\n \n-      /* A PARALLEL can have lots of SETs in it,\n-\t especially if it is really an ASM_OPERANDS.  */\n-      if (INSN_P (p) && GET_CODE (PATTERN (p)) == PARALLEL)\n-\tnsets += XVECLEN (PATTERN (p), 0);\n-      else if (!NOTE_P (p))\n-\tnsets += 1;\n+\t  if (e && e->dest != EXIT_BLOCK_PTR\n+\t      && single_pred_p (e->dest))\n+\t    {\n+\t      basic_block bb2 = e->dest;\n \n-      /* Ignore insns made by CSE; they cannot affect the boundaries of\n-\t the basic block.  */\n+#if ENABLE_CHECKING\n+\t      /* We should only see blocks here that we have not\n+\t\t visited yet.  */\n+\t      gcc_assert (!TEST_BIT (cse_visited_basic_blocks, bb2->index));\n+#endif\n+\t      SET_BIT (cse_visited_basic_blocks, bb2->index);\n+\t      data->path[path_size++].bb = bb2;\n+\t      bb = bb2;\n+\t    }\n+\t  else\n+\t    bb = NULL;\n+\t}\n+    }\n+\n+done:\n+  data->path_size = path_size;\n+  return path_size != 0;\n+}\n+\f\n+/* Dump the path in DATA to file F.  NSETS is the number of sets\n+   in the path.  */\n+\n+static void\n+cse_dump_path (struct cse_basic_block_data *data, int nsets, FILE *f)\n+{\n+  int path_entry;\n+\n+  fprintf (f, \";; Following path with %d sets: \", nsets);\n+  for (path_entry = 0; path_entry < data->path_size; path_entry++)\n+    fprintf (f, \"%d \", (data->path[path_entry].bb)->index);\n+  fputc ('\\n', dump_file);\n+  fflush (f);\n+}\n+\n+\f\n+/* Scan to the end of the path described by DATA.  Return an estimate of\n+   the total number of SETs, and the lowest and highest insn CUID, of all\n+   insns in the path.  */\n+\n+static void\n+cse_prescan_path (struct cse_basic_block_data *data)\n+{\n+  int nsets = 0;\n+  int low_cuid = -1, high_cuid = -1; /* FIXME low_cuid not computed correctly */\n+  int path_size = data->path_size;\n+  int path_entry;\n+\n+  /* Scan to end of each basic block in the path.  */\n+  for (path_entry = 0; path_entry < path_size; path_entry++) \n+    {\n+      basic_block bb;\n+      rtx insn;\n \n-      if (INSN_UID (p) <= max_uid && INSN_CUID (p) > high_cuid)\n-\thigh_cuid = INSN_CUID (p);\n-      if (INSN_UID (p) <= max_uid && INSN_CUID (p) < low_cuid)\n-\tlow_cuid = INSN_CUID (p);\n+      bb = data->path[path_entry].bb;\n \n-      /* See if this insn is in our branch path.  If it is and we are to\n-\t take it, do so.  */\n-      if (path_entry < path_size && data->path[path_entry].branch == p)\n+      FOR_BB_INSNS (bb, insn)\n \t{\n-\t  if (data->path[path_entry].status != PATH_NOT_TAKEN)\n-\t    p = JUMP_LABEL (p);\n+\t  if (!INSN_P (insn))\n+\t    continue;\n \n-\t  /* Point to next entry in path, if any.  */\n-\t  path_entry++;\n+\t  /* A PARALLEL can have lots of SETs in it,\n+\t     especially if it is really an ASM_OPERANDS.  */\n+\t  if (GET_CODE (PATTERN (insn)) == PARALLEL)\n+\t    nsets += XVECLEN (PATTERN (insn), 0);\n+\t  else\n+\t    nsets += 1;\n+\n+\t  /* Ignore insns made by CSE in a previous traversal of this\n+\t     basic block.  They cannot affect the boundaries of the\n+\t     basic block.\n+\t     FIXME: When we only visit each basic block at most once,\n+\t\t    this can go away.  */\n+\t  if (INSN_UID (insn) <= max_uid && INSN_CUID (insn) > high_cuid)\n+\t    high_cuid = INSN_CUID (insn);\n+\t  if (INSN_UID (insn) <= max_uid && INSN_CUID (insn) < low_cuid)\n+\t    low_cuid = INSN_CUID (insn);\n \t}\n+    }\n+\n+  data->low_cuid = low_cuid;\n+  data->high_cuid = high_cuid;\n+  data->nsets = nsets;\n+}\n+\f\n+/* Process a single extended basic block described by EBB_DATA.  */\n \n-      /* If this is a conditional jump, we can follow it if -fcse-follow-jumps\n-\t was specified, we haven't reached our maximum path length, there are\n-\t insns following the target of the jump, this is the only use of the\n-\t jump label, and the target label is preceded by a BARRIER.  */\n-      else if (follow_jumps\n-\t       && path_size < PARAM_VALUE (PARAM_MAX_CSE_PATH_LENGTH) - 1\n-\t       && JUMP_P (p)\n-\t       && GET_CODE (PATTERN (p)) == SET\n-\t       && GET_CODE (SET_SRC (PATTERN (p))) == IF_THEN_ELSE\n-\t       && JUMP_LABEL (p) != 0\n-\t       && LABEL_NUSES (JUMP_LABEL (p)) == 1\n-\t       && NEXT_INSN (JUMP_LABEL (p)) != 0)\n+static void\n+cse_extended_basic_block (struct cse_basic_block_data *ebb_data)\n+{\n+  int path_size = ebb_data->path_size;\n+  int path_entry;\n+  int num_insns = 0;\n+\n+  /* Allocate the space needed by qty_table.  */\n+  qty_table = XNEWVEC (struct qty_table_elem, max_qty);\n+\n+  new_basic_block ();\n+  for (path_entry = 0; path_entry < path_size; path_entry++)\n+    {\n+      basic_block bb;\n+      rtx insn;\n+      rtx libcall_insn = NULL_RTX;\n+      int no_conflict = 0;\n+\n+      bb = ebb_data->path[path_entry].bb;\n+      FOR_BB_INSNS (bb, insn)\n \t{\n-\t  for (q = PREV_INSN (JUMP_LABEL (p)); q; q = PREV_INSN (q))\n-\t    if ((!NOTE_P (q)\n-\t\t || (PREV_INSN (q) && CALL_P (PREV_INSN (q))\n-\t\t     && find_reg_note (PREV_INSN (q), REG_SETJMP, NULL)))\n-\t\t&& (!LABEL_P (q) || LABEL_NUSES (q) != 0))\n-\t      break;\n+\t  /* If we have processed 1,000 insns, flush the hash table to\n+\t     avoid extreme quadratic behavior.  We must not include NOTEs\n+\t     in the count since there may be more of them when generating\n+\t     debugging information.  If we clear the table at different\n+\t     times, code generated with -g -O might be different than code\n+\t     generated with -O but not -g.\n+\n+\t     FIXME: This is a real kludge and needs to be done some other\n+\t\t    way.  */\n+\t  if (INSN_P (insn)\n+\t      && num_insns++ > PARAM_VALUE (PARAM_MAX_CSE_INSNS))\n+\t    {\n+\t      flush_hash_table ();\n+\t      num_insns = 0;\n+\t    }\n \n-\t  /* If we ran into a BARRIER, this code is an extension of the\n-\t     basic block when the branch is taken.  */\n-\t  if (follow_jumps && q != 0 && BARRIER_P (q))\n+\t  if (INSN_P (insn))\n \t    {\n-\t      /* Don't allow ourself to keep walking around an\n-\t\t always-executed loop.  */\n-\t      if (next_real_insn (q) == next)\n+\t      /* Process notes first so we have all notes in canonical forms\n+\t\t when looking for duplicate operations.  */\n+\t      if (REG_NOTES (insn))\n+\t\tREG_NOTES (insn) = cse_process_notes (REG_NOTES (insn),\n+\t\t\t\t\t\t      NULL_RTX);\n+\n+\t      /* Track when we are inside in LIBCALL block.  Inside such\n+\t\t a block we do not want to record destinations.  The last\n+\t\t insn of a LIBCALL block is not considered to be part of\n+\t\t the block, since its destination is the result of the\n+\t\t block and hence should be recorded.  */\n+\t      if (REG_NOTES (insn) != 0)\n \t\t{\n-\t\t  p = NEXT_INSN (p);\n-\t\t  continue;\n+\t\t  rtx p;\n+\n+\t\t  if ((p = find_reg_note (insn, REG_LIBCALL, NULL_RTX)))\n+\t\t    libcall_insn = XEXP (p, 0);\n+\t\t  else if (find_reg_note (insn, REG_RETVAL, NULL_RTX))\n+\t\t    {\n+\t\t      /* Keep libcall_insn for the last SET insn of\n+\t\t\t a no-conflict block to prevent changing the\n+\t\t\t destination.  */\n+\t\t      if (!no_conflict)\n+\t\t\tlibcall_insn = NULL_RTX;\n+\t\t      else\n+\t\t\tno_conflict = -1;\n+\t\t    }\n+\t\t  else if (find_reg_note (insn, REG_NO_CONFLICT, NULL_RTX))\n+\t\t    no_conflict = 1;\n \t\t}\n \n-\t      /* Similarly, don't put a branch in our path more than once.  */\n-\t      for (i = 0; i < path_entry; i++)\n-\t\tif (data->path[i].branch == p)\n-\t\t  break;\n+\t      cse_insn (insn, libcall_insn);\n \n-\t      if (i != path_entry)\n-\t\tbreak;\n+\t      /* If we kept libcall_insn for a no-conflict bock,\n+\t\t clear it here.  */\n+\t      if (no_conflict == -1)\n+\t\t{\n+\t\t  libcall_insn = NULL_RTX;\n+\t\t  no_conflict = 0;\n+\t\t}\n+\t    \n+\t      /* If we haven't already found an insn where we added a LABEL_REF,\n+\t\t check this one.  */\n+\t      if (NONJUMP_INSN_P (insn) && ! recorded_label_ref\n+\t\t  && for_each_rtx (&PATTERN (insn), check_for_label_ref,\n+\t\t\t\t   (void *) insn))\n+\t\trecorded_label_ref = 1;\n+\t    }\n+\t}\n \n-\t      data->path[path_entry].branch = p;\n-\t      data->path[path_entry++].status = PATH_TAKEN;\n+      /* Make sure that libcalls don't span multiple basic blocks.  */\n+      gcc_assert (libcall_insn == NULL_RTX);\n \n-\t      /* This branch now ends our path.  It was possible that we\n-\t\t didn't see this branch the last time around (when the\n-\t\t insn in front of the target was a JUMP_INSN that was\n-\t\t turned into a no-op).  */\n-\t      path_size = path_entry;\n+#ifdef HAVE_cc0\n+      /* Clear the CC0-tracking related insns, they can't provide\n+\t useful information across basic block boundaries.  */\n+      prev_insn_cc0 = 0;\n+      prev_insn = 0;\n+#endif\n \n-\t      p = JUMP_LABEL (p);\n-\t      /* Mark block so we won't scan it again later.  */\n-\t      PUT_MODE (NEXT_INSN (p), QImode);\n-\t    }\n+      /* If we changed a conditional jump, we may have terminated\n+\t the path we are following.  Check that by verifying that\n+\t the edge we would take still exists.  If the edge does\n+\t not exist anymore, purge the remainder of the path.\n+\t Note that this will cause us to return to the caller.  */\n+      if (path_entry < path_size - 1)\n+\t{\n+\t  basic_block next_bb = ebb_data->path[path_entry + 1].bb;\n+\t  if (!find_edge (bb, next_bb))\n+\t    ebb_data->path_size = path_entry + 1;\n \t}\n-      p = NEXT_INSN (p);\n-    }\n \n-  data->low_cuid = low_cuid;\n-  data->high_cuid = high_cuid;\n-  data->nsets = nsets;\n-  data->last = p;\n-\n-  /* If all jumps in the path are not taken, set our path length to zero\n-     so a rescan won't be done.  */\n-  for (i = path_size - 1; i >= 0; i--)\n-    if (data->path[i].status != PATH_NOT_TAKEN)\n-      break;\n+      /* If this is a conditional jump insn, record any known\n+\t equivalences due to the condition being tested.  */\n+      insn = BB_END (bb);\n+      if (path_entry < path_size - 1\n+\t  && JUMP_P (insn)\n+\t  && single_set (insn)\n+\t  && any_condjump_p (insn))\n+\t{\n+\t  basic_block next_bb = ebb_data->path[path_entry + 1].bb;\n+\t  bool taken = (next_bb == BRANCH_EDGE (bb)->dest);\n+\t  record_jump_equiv (insn, taken);\n+\t}\n+    }\n \n-  if (i == -1)\n-    data->path_size = 0;\n-  else\n-    data->path_size = path_size;\n+  gcc_assert (next_qty <= max_qty);\n \n-  /* End the current branch path.  */\n-  data->path[path_size].branch = 0;\n+  free (qty_table);\n }\n \f\n /* Perform cse on the instructions of a function.\n@@ -5968,251 +6148,98 @@ cse_end_of_basic_block (rtx insn, struct cse_basic_block_data *data,\n    in conditional jump instructions.  */\n \n int\n-cse_main (rtx f, int nregs)\n+cse_main (rtx f ATTRIBUTE_UNUSED, int nregs)\n {\n-  struct cse_basic_block_data val;\n-  rtx insn = f;\n-  int i;\n+  struct cse_basic_block_data ebb_data;\n+  basic_block bb;\n+  int *dfs_order = XNEWVEC (int, last_basic_block);\n+  int i, n_blocks;\n \n   init_cse_reg_info (nregs);\n \n-  val.path = XNEWVEC (struct branch_path, PARAM_VALUE (PARAM_MAX_CSE_PATH_LENGTH));\n+  ebb_data.path = XNEWVEC (struct branch_path,\n+\t\t\t   PARAM_VALUE (PARAM_MAX_CSE_PATH_LENGTH));\n \n   cse_jumps_altered = 0;\n   recorded_label_ref = 0;\n   constant_pool_entries_cost = 0;\n   constant_pool_entries_regcost = 0;\n-  val.path_size = 0;\n+  ebb_data.path_size = 0;\n+  ebb_data.nsets = 0;\n   rtl_hooks = cse_rtl_hooks;\n \n   init_recog ();\n   init_alias_analysis ();\n \n   reg_eqv_table = XNEWVEC (struct reg_eqv_elem, nregs);\n \n-  /* Find the largest uid.  */\n+  /* Set up the table of already visited basic blocks.  */\n+  cse_visited_basic_blocks = sbitmap_alloc (last_basic_block);\n+  sbitmap_zero (cse_visited_basic_blocks);\n \n+  /* Compute the mapping from uids to cuids.\n+     CUIDs are numbers assigned to insns, like uids, except that\n+     that cuids increase monotonically through the code.  */\n   max_uid = get_max_uid ();\n   uid_cuid = XCNEWVEC (int, max_uid + 1);\n-\n-  /* Compute the mapping from uids to cuids.\n-     CUIDs are numbers assigned to insns, like uids,\n-     except that cuids increase monotonically through the code.  */\n-\n-  for (insn = f, i = 0; insn; insn = NEXT_INSN (insn))\n-    INSN_CUID (insn) = ++i;\n-\n-  /* Loop over basic blocks.\n-     Compute the maximum number of qty's needed for each basic block\n-     (which is 2 for each SET).  */\n-  insn = f;\n-  while (insn)\n+  i = 0;\n+  FOR_EACH_BB (bb)\n     {\n-      cse_end_of_basic_block (insn, &val, flag_cse_follow_jumps);\n-\n-      /* If this basic block was already processed or has no sets, skip it.  */\n-      if (val.nsets == 0 || GET_MODE (insn) == QImode)\n-\t{\n-\t  PUT_MODE (insn, VOIDmode);\n-\t  insn = (val.last ? NEXT_INSN (val.last) : 0);\n-\t  val.path_size = 0;\n-\t  continue;\n-\t}\n-\n-      cse_basic_block_start = val.low_cuid;\n-      cse_basic_block_end = val.high_cuid;\n-      max_qty = val.nsets * 2;\n-\n-      if (dump_file)\n-\tfprintf (dump_file, \";; Processing block from %d to %d, %d sets.\\n\",\n-\t\t INSN_UID (insn), val.last ? INSN_UID (val.last) : 0,\n-\t\t val.nsets);\n-\n-      /* Make MAX_QTY bigger to give us room to optimize\n-\t past the end of this basic block, if that should prove useful.  */\n-      if (max_qty < 500)\n-\tmax_qty = 500;\n-\n-      /* If this basic block is being extended by following certain jumps,\n-         (see `cse_end_of_basic_block'), we reprocess the code from the start.\n-         Otherwise, we start after this basic block.  */\n-      if (val.path_size > 0)\n-\tcse_basic_block (insn, val.last, val.path);\n-      else\n-\t{\n-\t  int old_cse_jumps_altered = cse_jumps_altered;\n-\t  rtx temp;\n-\n-\t  /* When cse changes a conditional jump to an unconditional\n-\t     jump, we want to reprocess the block, since it will give\n-\t     us a new branch path to investigate.  */\n-\t  cse_jumps_altered = 0;\n-\t  temp = cse_basic_block (insn, val.last, val.path);\n-\t  if (cse_jumps_altered == 0 || flag_cse_follow_jumps)\n-\t    insn = temp;\n-\n-\t  cse_jumps_altered |= old_cse_jumps_altered;\n-\t}\n+      rtx insn;\n+      FOR_BB_INSNS (bb, insn)\n+\tINSN_CUID (insn) = ++i;\n     }\n \n-  /* Clean up.  */\n-  end_alias_analysis ();\n-  free (uid_cuid);\n-  free (reg_eqv_table);\n-  free (val.path);\n-  rtl_hooks = general_rtl_hooks;\n-\n-  return cse_jumps_altered || recorded_label_ref;\n-}\n-\n-/* Process a single basic block.  FROM and TO and the limits of the basic\n-   block.  NEXT_BRANCH points to the branch path when following jumps or\n-   a null path when not following jumps.  */\n-\n-static rtx\n-cse_basic_block (rtx from, rtx to, struct branch_path *next_branch)\n-{\n-  rtx insn;\n-  int to_usage = 0;\n-  rtx libcall_insn = NULL_RTX;\n-  int num_insns = 0;\n-  int no_conflict = 0;\n-\n-  /* Allocate the space needed by qty_table.  */\n-  qty_table = XNEWVEC (struct qty_table_elem, max_qty);\n-\n-  new_basic_block ();\n-\n-  /* TO might be a label.  If so, protect it from being deleted.  */\n-  if (to != 0 && LABEL_P (to))\n-    ++LABEL_NUSES (to);\n-\n-  for (insn = from; insn != to; insn = NEXT_INSN (insn))\n+  /* Loop over basic blocks in DFS order,\n+     excluding the ENTRY and EXIT blocks.  */\n+  n_blocks = pre_and_rev_post_order_compute (dfs_order, NULL, false);\n+  i = 0;\n+  while (i < n_blocks)\n     {\n-      enum rtx_code code = GET_CODE (insn);\n-\n-      /* If we have processed 1,000 insns, flush the hash table to\n-\t avoid extreme quadratic behavior.  We must not include NOTEs\n-\t in the count since there may be more of them when generating\n-\t debugging information.  If we clear the table at different\n-\t times, code generated with -g -O might be different than code\n-\t generated with -O but not -g.\n-\n-\t ??? This is a real kludge and needs to be done some other way.\n-\t Perhaps for 2.9.  */\n-      if (code != NOTE && num_insns++ > PARAM_VALUE (PARAM_MAX_CSE_INSNS))\n+      /* Find the first block in the DFS queue that we have not yet\n+\t processed before.  */\n+      do\n \t{\n-\t  flush_hash_table ();\n-\t  num_insns = 0;\n+\t  bb = BASIC_BLOCK (dfs_order[i++]);\n \t}\n+      while (TEST_BIT (cse_visited_basic_blocks, bb->index)\n+\t     && i < n_blocks);\n \n-      /* See if this is a branch that is part of the path.  If so, and it is\n-\t to be taken, do so.  */\n-      if (next_branch->branch == insn)\n+      /* Find all paths starting with BB, and process them.  */\n+      while (cse_find_path (bb, &ebb_data, flag_cse_follow_jumps))\n \t{\n-\t  enum taken status = next_branch++->status;\n-\t  if (status != PATH_NOT_TAKEN)\n-\t    {\n-\t      gcc_assert (status == PATH_TAKEN);\n-\t      if (any_condjump_p (insn))\n-\t\trecord_jump_equiv (insn, true);\n-\n-\t      /* Set the last insn as the jump insn; it doesn't affect cc0.\n-\t\t Then follow this branch.  */\n-#ifdef HAVE_cc0\n-\t      prev_insn_cc0 = 0;\n-\t      prev_insn = insn;\n-#endif\n-\t      insn = JUMP_LABEL (insn);\n-\t      continue;\n-\t    }\n-\t}\n-\n-      if (GET_MODE (insn) == QImode)\n-\tPUT_MODE (insn, VOIDmode);\n-\n-      if (GET_RTX_CLASS (code) == RTX_INSN)\n-\t{\n-\t  rtx p;\n-\n-\t  /* Process notes first so we have all notes in canonical forms when\n-\t     looking for duplicate operations.  */\n-\n-\t  if (REG_NOTES (insn))\n-\t    REG_NOTES (insn) = cse_process_notes (REG_NOTES (insn), NULL_RTX);\n-\n-\t  /* Track when we are inside in LIBCALL block.  Inside such a block,\n-\t     we do not want to record destinations.  The last insn of a\n-\t     LIBCALL block is not considered to be part of the block, since\n-\t     its destination is the result of the block and hence should be\n-\t     recorded.  */\n-\n-\t  if (REG_NOTES (insn) != 0)\n-\t    {\n-\t      if ((p = find_reg_note (insn, REG_LIBCALL, NULL_RTX)))\n-\t\tlibcall_insn = XEXP (p, 0);\n-\t      else if (find_reg_note (insn, REG_RETVAL, NULL_RTX))\n-\t\t{\n-\t\t  /* Keep libcall_insn for the last SET insn of a no-conflict\n-\t\t     block to prevent changing the destination.  */\n-\t\t  if (! no_conflict)\n-\t\t    libcall_insn = 0;\n-\t\t  else\n-\t\t    no_conflict = -1;\n-\t\t}\n-\t      else if (find_reg_note (insn, REG_NO_CONFLICT, NULL_RTX))\n-\t\tno_conflict = 1;\n-\t    }\n-\n-\t  cse_insn (insn, libcall_insn);\n-\n-\t  if (no_conflict == -1)\n-\t    {\n-\t      libcall_insn = 0;\n-\t      no_conflict = 0;\n-\t    }\n-\t    \n-\t  /* If we haven't already found an insn where we added a LABEL_REF,\n-\t     check this one.  */\n-\t  if (NONJUMP_INSN_P (insn) && ! recorded_label_ref\n-\t      && for_each_rtx (&PATTERN (insn), check_for_label_ref,\n-\t\t\t       (void *) insn))\n-\t    recorded_label_ref = 1;\n-\t}\n+\t  /* Pre-scan the path.  */\n+\t  cse_prescan_path (&ebb_data);\n \n-      /* If INSN is now an unconditional jump, skip to the end of our\n-\t basic block by pretending that we just did the last insn in the\n-\t basic block.  If we are jumping to the end of our block, show\n-\t that we can have one usage of TO.  */\n-\n-      if (any_uncondjump_p (insn))\n-\t{\n-\t  if (to == 0)\n-\t    {\n-\t      free (qty_table);\n-\t      return 0;\n-\t    }\n+\t  /* If this basic block has no sets, skip it.  */\n+\t  if (ebb_data.nsets == 0)\n+\t    continue;\n \n-\t  if (JUMP_LABEL (insn) == to)\n-\t    to_usage = 1;\n+\t  /* Get a reasonable extimate for the maximum number of qty's\n+\t     needed for this path.  For this, we take the number of sets\n+\t     and multiply that by MAX_RECOG_OPERANDS.  */\n+\t  max_qty = ebb_data.nsets * MAX_RECOG_OPERANDS;\n+\t  cse_basic_block_start = ebb_data.low_cuid;\n+\t  cse_basic_block_end = ebb_data.high_cuid;\n \n-\t  /* Maybe TO was deleted because the jump is unconditional.\n-\t     If so, there is nothing left in this basic block.  */\n-\t  /* ??? Perhaps it would be smarter to set TO\n-\t     to whatever follows this insn,\n-\t     and pretend the basic block had always ended here.  */\n-\t  if (INSN_DELETED_P (to))\n-\t    break;\n+\t  /* Dump the path we're about to process.  */\n+\t  if (dump_file)\n+\t    cse_dump_path (&ebb_data, ebb_data.nsets, dump_file);\n \n-\t  insn = PREV_INSN (to);\n+\t  cse_extended_basic_block (&ebb_data);\n \t}\n     }\n \n-  gcc_assert (next_qty <= max_qty);\n-\n-  free (qty_table);\n+  /* Clean up.  */\n+  end_alias_analysis ();\n+  free (uid_cuid);\n+  free (reg_eqv_table);\n+  free (ebb_data.path);\n+  sbitmap_free (cse_visited_basic_blocks);\n+  free (dfs_order);\n+  rtl_hooks = general_rtl_hooks;\n \n-  return to ? NEXT_INSN (to) : 0;\n+  return cse_jumps_altered || recorded_label_ref;\n }\n \f\n /* Called via for_each_rtx to see if an insn is using a LABEL_REF for which\n@@ -6929,30 +6956,30 @@ gate_handle_cse (void)\n static unsigned int\n rest_of_handle_cse (void)\n {\n+static int counter = 0;\n   int tem;\n-\n+counter++;\n   if (dump_file)\n     dump_flow_info (dump_file, dump_flags);\n \n   reg_scan (get_insns (), max_reg_num ());\n \n   tem = cse_main (get_insns (), max_reg_num ());\n-  if (tem)\n-    rebuild_jump_labels (get_insns ());\n-  if (purge_all_dead_edges ())\n-    delete_unreachable_blocks ();\n-\n-  delete_trivially_dead_insns (get_insns (), max_reg_num ());\n \n   /* If we are not running more CSE passes, then we are no longer\n      expecting CSE to be run.  But always rerun it in a cheap mode.  */\n   cse_not_expected = !flag_rerun_cse_after_loop && !flag_gcse;\n \n+  /* If there are dead edges to purge, we haven't properly updated\n+     the CFG incrementally.  */\n+  gcc_assert (!purge_all_dead_edges ());\n+\n   if (tem)\n-    delete_dead_jumptables ();\n+    rebuild_jump_labels (get_insns ());\n \n   if (tem || optimize > 1)\n     cleanup_cfg (CLEANUP_EXPENSIVE);\n+\n   return 0;\n }\n \n@@ -6970,7 +6997,8 @@ struct tree_opt_pass pass_cse =\n   0,                                    /* properties_destroyed */\n   0,                                    /* todo_flags_start */\n   TODO_dump_func |\n-  TODO_ggc_collect,                     /* todo_flags_finish */\n+  TODO_ggc_collect |\n+  TODO_verify_flow,                     /* todo_flags_finish */\n   's'                                   /* letter */\n };\n \n@@ -6998,7 +7026,10 @@ rest_of_handle_cse2 (void)\n      bypassed safely.  */\n   cse_condition_code_reg ();\n \n-  purge_all_dead_edges ();\n+  /* If there are dead edges to purge, we haven't properly updated\n+     the CFG incrementally.  */\n+  gcc_assert (!purge_all_dead_edges ());\n+\n   delete_trivially_dead_insns (get_insns (), max_reg_num ());\n \n   if (tem)\n@@ -7029,7 +7060,8 @@ struct tree_opt_pass pass_cse2 =\n   0,                                    /* properties_destroyed */\n   0,                                    /* todo_flags_start */\n   TODO_dump_func |\n-  TODO_ggc_collect,                     /* todo_flags_finish */\n+  TODO_ggc_collect |\n+  TODO_verify_flow,                     /* todo_flags_finish */\n   't'                                   /* letter */\n };\n "}]}