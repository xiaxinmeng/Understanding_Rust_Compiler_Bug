{"sha": "535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NTM1ZTdjMTE0YTdhZDJhZDdhNmEwZGVmODhjZjk0NDhmY2Q1ZjAyOQ==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2018-01-13T17:59:32Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2018-01-13T17:59:32Z"}, "message": "Handle peeling for alignment with masking\n\nThis patch adds support for aligning vectors by using a partial\nfirst iteration.  E.g. if the start pointer is 3 elements beyond\nan aligned address, the first iteration will have a mask in which\nthe first three elements are false.\n\nOn SVE, the optimisation is only useful for vector-length-specific\ncode.  Vector-length-agnostic code doesn't try to align vectors\nsince the vector length might not be a power of 2.\n\n2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood  <david.sherwood@arm.com>\n\ngcc/\n\t* tree-vectorizer.h (_loop_vec_info::mask_skip_niters): New field.\n\t(LOOP_VINFO_MASK_SKIP_NITERS): New macro.\n\t(vect_use_loop_mask_for_alignment_p): New function.\n\t(vect_prepare_for_masked_peels, vect_gen_while_not): Declare.\n\t* tree-vect-loop-manip.c (vect_set_loop_masks_directly): Add an\n\tniters_skip argument.  Make sure that the first niters_skip elements\n\tof the first iteration are inactive.\n\t(vect_set_loop_condition_masked): Handle LOOP_VINFO_MASK_SKIP_NITERS.\n\tUpdate call to vect_set_loop_masks_directly.\n\t(get_misalign_in_elems): New function, split out from...\n\t(vect_gen_prolog_loop_niters): ...here.\n\t(vect_update_init_of_dr): Take a code argument that specifies whether\n\tthe adjustment should be added or subtracted.\n\t(vect_update_init_of_drs): Likewise.\n\t(vect_prepare_for_masked_peels): New function.\n\t(vect_do_peeling): Skip prologue peeling if we're using a mask\n\tinstead.  Update call to vect_update_inits_of_drs.\n\t* tree-vect-loop.c (_loop_vec_info::_loop_vec_info): Initialize\n\tmask_skip_niters.\n\t(vect_analyze_loop_2): Allow fully-masked loops with peeling for\n\talignment.  Do not include the number of peeled iterations in\n\tthe minimum threshold in that case.\n\t(vectorizable_induction): Adjust the start value down by\n\tLOOP_VINFO_MASK_SKIP_NITERS iterations.\n\t(vect_transform_loop): Call vect_prepare_for_masked_peels.\n\tTake the number of skipped iterations into account when calculating\n\tthe loop bounds.\n\t* tree-vect-stmts.c (vect_gen_while_not): New function.\n\ngcc/testsuite/\n\t* gcc.target/aarch64/sve/nopeel_1.c: New test.\n\t* gcc.target/aarch64/sve/peel_ind_1.c: Likewise.\n\t* gcc.target/aarch64/sve/peel_ind_1_run.c: Likewise.\n\t* gcc.target/aarch64/sve/peel_ind_2.c: Likewise.\n\t* gcc.target/aarch64/sve/peel_ind_2_run.c: Likewise.\n\t* gcc.target/aarch64/sve/peel_ind_3.c: Likewise.\n\t* gcc.target/aarch64/sve/peel_ind_3_run.c: Likewise.\n\t* gcc.target/aarch64/sve/peel_ind_4.c: Likewise.\n\t* gcc.target/aarch64/sve/peel_ind_4_run.c: Likewise.\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r256630", "tree": {"sha": "1c9a22e58ae70f7dd6784c23c21a355f50625864", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1c9a22e58ae70f7dd6784c23c21a355f50625864"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "html_url": "https://github.com/Rust-GCC/gccrs/commit/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/comments", "author": null, "committer": null, "parents": [{"sha": "c2700f7466bac153def05a0e070aa78cd2ffc0ae", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c2700f7466bac153def05a0e070aa78cd2ffc0ae", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c2700f7466bac153def05a0e070aa78cd2ffc0ae"}], "stats": {"total": 649, "additions": 567, "deletions": 82}, "files": [{"sha": "c4cbe9d89e8bdbe6ba9c00f291d978fceed9583c", "filename": "gcc/ChangeLog", "status": "modified", "additions": 33, "deletions": 0, "changes": 33, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -1,3 +1,36 @@\n+2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* tree-vectorizer.h (_loop_vec_info::mask_skip_niters): New field.\n+\t(LOOP_VINFO_MASK_SKIP_NITERS): New macro.\n+\t(vect_use_loop_mask_for_alignment_p): New function.\n+\t(vect_prepare_for_masked_peels, vect_gen_while_not): Declare.\n+\t* tree-vect-loop-manip.c (vect_set_loop_masks_directly): Add an\n+\tniters_skip argument.  Make sure that the first niters_skip elements\n+\tof the first iteration are inactive.\n+\t(vect_set_loop_condition_masked): Handle LOOP_VINFO_MASK_SKIP_NITERS.\n+\tUpdate call to vect_set_loop_masks_directly.\n+\t(get_misalign_in_elems): New function, split out from...\n+\t(vect_gen_prolog_loop_niters): ...here.\n+\t(vect_update_init_of_dr): Take a code argument that specifies whether\n+\tthe adjustment should be added or subtracted.\n+\t(vect_update_init_of_drs): Likewise.\n+\t(vect_prepare_for_masked_peels): New function.\n+\t(vect_do_peeling): Skip prologue peeling if we're using a mask\n+\tinstead.  Update call to vect_update_inits_of_drs.\n+\t* tree-vect-loop.c (_loop_vec_info::_loop_vec_info): Initialize\n+\tmask_skip_niters.\n+\t(vect_analyze_loop_2): Allow fully-masked loops with peeling for\n+\talignment.  Do not include the number of peeled iterations in\n+\tthe minimum threshold in that case.\n+\t(vectorizable_induction): Adjust the start value down by\n+\tLOOP_VINFO_MASK_SKIP_NITERS iterations.\n+\t(vect_transform_loop): Call vect_prepare_for_masked_peels.\n+\tTake the number of skipped iterations into account when calculating\n+\tthe loop bounds.\n+\t* tree-vect-stmts.c (vect_gen_while_not): New function.\n+\n 2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n \t    Alan Hayward  <alan.hayward@arm.com>\n \t    David Sherwood  <david.sherwood@arm.com>"}, {"sha": "55e58c2343418319e1e1003af53e7d3a6f8b314f", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -1,3 +1,17 @@\n+2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* gcc.target/aarch64/sve/nopeel_1.c: New test.\n+\t* gcc.target/aarch64/sve/peel_ind_1.c: Likewise.\n+\t* gcc.target/aarch64/sve/peel_ind_1_run.c: Likewise.\n+\t* gcc.target/aarch64/sve/peel_ind_2.c: Likewise.\n+\t* gcc.target/aarch64/sve/peel_ind_2_run.c: Likewise.\n+\t* gcc.target/aarch64/sve/peel_ind_3.c: Likewise.\n+\t* gcc.target/aarch64/sve/peel_ind_3_run.c: Likewise.\n+\t* gcc.target/aarch64/sve/peel_ind_4.c: Likewise.\n+\t* gcc.target/aarch64/sve/peel_ind_4_run.c: Likewise.\n+\n 2018-01-13  Richard Sandiford  <richard.sandiford@linaro.org>\n \t    Alan Hayward  <alan.hayward@arm.com>\n \t    David Sherwood  <david.sherwood@arm.com>"}, {"sha": "d77c32c3885e852da41fd6f78674a6d97ca3c760", "filename": "gcc/testsuite/gcc.target/aarch64/sve/nopeel_1.c", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fnopeel_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fnopeel_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fnopeel_1.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -0,0 +1,39 @@\n+/* { dg-options \"-O2 -ftree-vectorize -msve-vector-bits=256\" } */\n+\n+#include <stdint.h>\n+\n+#define TEST(NAME, TYPE)\t\t\t\\\n+ void\t\t\t\t\t\t\\\n+ NAME##1 (TYPE *x, int n)\t\t\t\\\n+ {\t\t\t\t\t\t\\\n+   for (int i = 0; i < n; ++i)\t\t\t\\\n+     x[i] += 1;\t\t\t\t\t\\\n+ }\t\t\t\t\t\t\\\n+ TYPE NAME##_array[1024];\t\t\t\\\n+ void\t\t\t\t\t\t\\\n+ NAME##2 (void)\t\t\t\t\t\\\n+ {\t\t\t\t\t\t\\\n+   for (int i = 1; i < 200; ++i)\t\t\\\n+     NAME##_array[i] += 1;\t\t\t\\\n+ }\n+\n+TEST (s8, int8_t)\n+TEST (u8, uint8_t)\n+TEST (s16, int16_t)\n+TEST (u16, uint16_t)\n+TEST (s32, int32_t)\n+TEST (u32, uint32_t)\n+TEST (s64, int64_t)\n+TEST (u64, uint64_t)\n+TEST (f16, _Float16)\n+TEST (f32, float)\n+TEST (f64, double)\n+\n+/* No scalar memory accesses.  */\n+/* { dg-final { scan-assembler-not {[wx][0-9]*, \\[} } } */\n+/* 2 for each NAME##1 test, one in the header and one in the main loop\n+   and 1 for each NAME##2 test, in the main loop only.  */\n+/* { dg-final { scan-assembler-times {\\twhilelo\\tp[0-7]\\.b,} 6 } } */\n+/* { dg-final { scan-assembler-times {\\twhilelo\\tp[0-7]\\.h,} 9 } } */\n+/* { dg-final { scan-assembler-times {\\twhilelo\\tp[0-7]\\.s,} 9 } } */\n+/* { dg-final { scan-assembler-times {\\twhilelo\\tp[0-7]\\.d,} 9 } } */"}, {"sha": "864026499cd31419afc8ac229f7f87d47ac76b00", "filename": "gcc/testsuite/gcc.target/aarch64/sve/peel_ind_1.c", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_1.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -0,0 +1,27 @@\n+/* { dg-do compile } */\n+/* Pick an arbitrary target for which unaligned accesses are more\n+   expensive.  */\n+/* { dg-options \"-O3 -msve-vector-bits=256 -mtune=thunderx\" } */\n+\n+#define N 512\n+#define START 1\n+#define END 505\n+\n+int x[N] __attribute__((aligned(32)));\n+\n+void __attribute__((noinline, noclone))\n+foo (void)\n+{\n+  unsigned int v = 0;\n+  for (unsigned int i = START; i < END; ++i)\n+    {\n+      x[i] = v;\n+      v += 5;\n+    }\n+}\n+\n+/* We should operate on aligned vectors.  */\n+/* { dg-final { scan-assembler {\\tadrp\\tx[0-9]+, x\\n} } } */\n+/* We should use an induction that starts at -5, with only the last\n+   7 elements of the first iteration being active.  */\n+/* { dg-final { scan-assembler {\\tindex\\tz[0-9]+\\.s, #-5, #5\\n} } } */"}, {"sha": "3fa0e46522d5e3ace1b3b92f41d84657ff3eb279", "filename": "gcc/testsuite/gcc.target/aarch64/sve/peel_ind_1_run.c", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_1_run.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_1_run.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_1_run.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -0,0 +1,18 @@\n+/* { dg-do run { target aarch64_sve_hw } } */\n+/* { dg-options \"-O3 -mtune=thunderx\" } */\n+/* { dg-options \"-O3 -mtune=thunderx -msve-vector-bits=256\" { target aarch64_sve256_hw } } */\n+\n+#include \"peel_ind_1.c\"\n+\n+int __attribute__ ((optimize (1)))\n+main (void)\n+{\n+  foo ();\n+  for (int i = 0; i < N; ++i)\n+    {\n+      if (x[i] != (i < START || i >= END ? 0 : (i - START) * 5))\n+\t__builtin_abort ();\n+      asm volatile (\"\" ::: \"memory\");\n+    }\n+  return 0;\n+}"}, {"sha": "2bfc09a7602de4a208e11be1c6df67513cebba84", "filename": "gcc/testsuite/gcc.target/aarch64/sve/peel_ind_2.c", "status": "added", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_2.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -0,0 +1,22 @@\n+/* { dg-do compile } */\n+/* Pick an arbitrary target for which unaligned accesses are more\n+   expensive.  */\n+/* { dg-options \"-O3 -msve-vector-bits=256 -mtune=thunderx\" } */\n+\n+#define N 512\n+#define START 7\n+#define END 22\n+\n+int x[N] __attribute__((aligned(32)));\n+\n+void __attribute__((noinline, noclone))\n+foo (void)\n+{\n+  for (unsigned int i = START; i < END; ++i)\n+    x[i] = i;\n+}\n+\n+/* We should operate on aligned vectors.  */\n+/* { dg-final { scan-assembler {\\tadrp\\tx[0-9]+, x\\n} } } */\n+/* We should unroll the loop three times.  */\n+/* { dg-final { scan-assembler-times \"\\tst1w\\t\" 3 } } */"}, {"sha": "9c5ae1bd06867ce074b185711bb61234c57c0faa", "filename": "gcc/testsuite/gcc.target/aarch64/sve/peel_ind_2_run.c", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_2_run.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_2_run.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_2_run.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -0,0 +1,18 @@\n+/* { dg-do run { target aarch64_sve_hw } } */\n+/* { dg-options \"-O3 -mtune=thunderx\" } */\n+/* { dg-options \"-O3 -mtune=thunderx -msve-vector-bits=256\" { target aarch64_sve256_hw } } */\n+\n+#include \"peel_ind_2.c\"\n+\n+int __attribute__ ((optimize (1)))\n+main (void)\n+{\n+  foo ();\n+  for (int i = 0; i < N; ++i)\n+    {\n+      if (x[i] != (i < START || i >= END ? 0 : i))\n+\t__builtin_abort ();\n+      asm volatile (\"\" ::: \"memory\");\n+    }\n+  return 0;\n+}"}, {"sha": "8364dc6107ad89bfdbf2e595e087172dca472c31", "filename": "gcc/testsuite/gcc.target/aarch64/sve/peel_ind_3.c", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_3.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -0,0 +1,21 @@\n+/* { dg-do compile } */\n+/* Pick an arbitrary target for which unaligned accesses are more\n+   expensive.  */\n+/* { dg-options \"-O3 -msve-vector-bits=256 -mtune=thunderx\" } */\n+\n+#define N 32\n+#define MAX_START 8\n+#define COUNT 16\n+\n+int x[MAX_START][N] __attribute__((aligned(32)));\n+\n+void __attribute__((noinline, noclone))\n+foo (int start)\n+{\n+  for (int i = start; i < start + COUNT; ++i)\n+    x[start][i] = i;\n+}\n+\n+/* We should operate on aligned vectors.  */\n+/* { dg-final { scan-assembler {\\tadrp\\tx[0-9]+, x\\n} } } */\n+/* { dg-final { scan-assembler {\\tubfx\\t} } } */"}, {"sha": "384a38eb8ec55f19f06268aaf2f70aaf76d807dc", "filename": "gcc/testsuite/gcc.target/aarch64/sve/peel_ind_3_run.c", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_3_run.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_3_run.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_3_run.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -0,0 +1,21 @@\n+/* { dg-do run { target aarch64_sve_hw } } */\n+/* { dg-options \"-O3 -mtune=thunderx\" } */\n+/* { dg-options \"-O3 -mtune=thunderx -msve-vector-bits=256\" { target aarch64_sve256_hw } } */\n+\n+#include \"peel_ind_3.c\"\n+\n+int __attribute__ ((optimize (1)))\n+main (void)\n+{\n+  for (int start = 0; start < MAX_START; ++start)\n+    {\n+      foo (start);\n+      for (int i = 0; i < N; ++i)\n+\t{\n+\t  if (x[start][i] != (i < start || i >= start + COUNT ? 0 : i))\n+\t    __builtin_abort ();\n+\t  asm volatile (\"\" ::: \"memory\");\n+\t}\n+    }\n+  return 0;\n+}"}, {"sha": "5b5d88ad521ef9d58549c116c1d724e2bde497cf", "filename": "gcc/testsuite/gcc.target/aarch64/sve/peel_ind_4.c", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_4.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_4.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_4.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -0,0 +1,21 @@\n+/* { dg-do compile } */\n+/* Pick an arbitrary target for which unaligned accesses are more\n+   expensive.  */\n+/* { dg-options \"-Ofast -msve-vector-bits=256 -mtune=thunderx -fno-vect-cost-model\" } */\n+\n+#define START 1\n+#define END 505\n+\n+void __attribute__((noinline, noclone))\n+foo (double *x)\n+{\n+  double v = 10.0;\n+  for (unsigned int i = START; i < END; ++i)\n+    {\n+      x[i] = v;\n+      v += 5.0;\n+    }\n+}\n+\n+/* We should operate on aligned vectors.  */\n+/* { dg-final { scan-assembler {\\tubfx\\t} } } */"}, {"sha": "7834a6298083cb1f1b03070ce33e0e9ed1279257", "filename": "gcc/testsuite/gcc.target/aarch64/sve/peel_ind_4_run.c", "status": "added", "additions": 29, "deletions": 0, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_4_run.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_4_run.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fpeel_ind_4_run.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -0,0 +1,29 @@\n+/* { dg-do run { target aarch64_sve_hw } } */\n+/* { dg-options \"-Ofast -mtune=thunderx\" } */\n+/* { dg-options \"-Ofast -mtune=thunderx -mtune=thunderx\" { target aarch64_sve256_hw } } */\n+\n+#include \"peel_ind_4.c\"\n+\n+int __attribute__ ((optimize (1)))\n+main (void)\n+{\n+  double x[END + 1];\n+  for (int i = 0; i < END + 1; ++i)\n+    {\n+      x[i] = i;\n+      asm volatile (\"\" ::: \"memory\");\n+    }\n+  foo (x);\n+  for (int i = 0; i < END + 1; ++i)\n+    {\n+      double expected;\n+      if (i < START || i >= END)\n+\texpected = i;\n+      else\n+\texpected = 10 + (i - START) * 5;\n+      if (x[i] != expected)\n+\t__builtin_abort ();\n+      asm volatile (\"\" ::: \"memory\");\n+    }\n+  return 0;\n+}"}, {"sha": "b9bb04756cc047b20a209df27d0cf4fd5631392e", "filename": "gcc/tree-vect-loop-manip.c", "status": "modified", "additions": 213, "deletions": 53, "changes": 266, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftree-vect-loop-manip.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftree-vect-loop-manip.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop-manip.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -384,6 +384,11 @@ vect_maybe_permute_loop_masks (gimple_seq *seq, rgroup_masks *dest_rgm,\n    times and has been vectorized according to LOOP_VINFO.  Each iteration\n    of the vectorized loop handles VF iterations of the scalar loop.\n \n+   If NITERS_SKIP is nonnull, the first iteration of the vectorized loop\n+   starts with NITERS_SKIP dummy iterations of the scalar loop before\n+   the real work starts.  The mask elements for these dummy iterations\n+   must be 0, to ensure that the extra iterations do not have an effect.\n+\n    It is known that:\n \n      NITERS * RGM->max_nscalars_per_iter\n@@ -395,7 +400,7 @@ vect_maybe_permute_loop_masks (gimple_seq *seq, rgroup_masks *dest_rgm,\n \n    might overflow before hitting a value above:\n \n-     NITERS * RGM->max_nscalars_per_iter\n+     (NITERS + NITERS_SKIP) * RGM->max_nscalars_per_iter\n \n    This means that we cannot guarantee that such an induction variable\n    would ever hit a value that produces a set of all-false masks for RGM.  */\n@@ -405,18 +410,21 @@ vect_set_loop_masks_directly (struct loop *loop, loop_vec_info loop_vinfo,\n \t\t\t      gimple_seq *preheader_seq,\n \t\t\t      gimple_stmt_iterator loop_cond_gsi,\n \t\t\t      rgroup_masks *rgm, tree vf,\n-\t\t\t      tree niters, bool might_wrap_p)\n+\t\t\t      tree niters, tree niters_skip,\n+\t\t\t      bool might_wrap_p)\n {\n   tree compare_type = LOOP_VINFO_MASK_COMPARE_TYPE (loop_vinfo);\n   tree mask_type = rgm->mask_type;\n   unsigned int nscalars_per_iter = rgm->max_nscalars_per_iter;\n   poly_uint64 nscalars_per_mask = TYPE_VECTOR_SUBPARTS (mask_type);\n \n   /* Calculate the maximum number of scalar values that the rgroup\n-     handles in total and the number that it handles for each iteration\n-     of the vector loop.  */\n+     handles in total, the number that it handles for each iteration\n+     of the vector loop, and the number that it should skip during the\n+     first iteration of the vector loop.  */\n   tree nscalars_total = niters;\n   tree nscalars_step = vf;\n+  tree nscalars_skip = niters_skip;\n   if (nscalars_per_iter != 1)\n     {\n       /* We checked before choosing to use a fully-masked loop that these\n@@ -426,6 +434,9 @@ vect_set_loop_masks_directly (struct loop *loop, loop_vec_info loop_vinfo,\n \t\t\t\t     nscalars_total, factor);\n       nscalars_step = gimple_build (preheader_seq, MULT_EXPR, compare_type,\n \t\t\t\t    nscalars_step, factor);\n+      if (nscalars_skip)\n+\tnscalars_skip = gimple_build (preheader_seq, MULT_EXPR, compare_type,\n+\t\t\t\t      nscalars_skip, factor);\n     }\n \n   /* Create an induction variable that counts the number of scalars\n@@ -438,37 +449,79 @@ vect_set_loop_masks_directly (struct loop *loop, loop_vec_info loop_vinfo,\n   create_iv (zero_index, nscalars_step, NULL_TREE, loop, &incr_gsi,\n \t     insert_after, &index_before_incr, &index_after_incr);\n \n-  tree test_index, test_limit;\n+  tree test_index, test_limit, first_limit;\n   gimple_stmt_iterator *test_gsi;\n   if (might_wrap_p)\n     {\n       /* In principle the loop should stop iterating once the incremented\n-\t IV reaches a value greater than or equal to NSCALAR_TOTAL.\n-\t However, there's no guarantee that the IV hits a value above\n-\t this value before wrapping around.  We therefore adjust the\n-\t limit down by one IV step:\n+\t IV reaches a value greater than or equal to:\n+\n+\t   NSCALARS_TOTAL +[infinite-prec] NSCALARS_SKIP\n+\n+\t However, there's no guarantee that this addition doesn't overflow\n+\t the comparison type, or that the IV hits a value above it before\n+\t wrapping around.  We therefore adjust the limit down by one\n+\t IV step:\n \n-\t   NSCALARS_TOTAL -[infinite-prec] NSCALARS_STEP\n+\t   (NSCALARS_TOTAL +[infinite-prec] NSCALARS_SKIP)\n+\t   -[infinite-prec] NSCALARS_STEP\n \n \t and compare the IV against this limit _before_ incrementing it.\n \t Since the comparison type is unsigned, we actually want the\n \t subtraction to saturate at zero:\n \n-\t   NSCALARS_TOTAL -[sat] NSCALARS_STEP.  */\n+\t   (NSCALARS_TOTAL +[infinite-prec] NSCALARS_SKIP)\n+\t   -[sat] NSCALARS_STEP\n+\n+\t And since NSCALARS_SKIP < NSCALARS_STEP, we can reassociate this as:\n+\n+\t   NSCALARS_TOTAL -[sat] (NSCALARS_STEP - NSCALARS_SKIP)\n+\n+\t where the rightmost subtraction can be done directly in\n+\t COMPARE_TYPE.  */\n       test_index = index_before_incr;\n+      tree adjust = nscalars_step;\n+      if (nscalars_skip)\n+\tadjust = gimple_build (preheader_seq, MINUS_EXPR, compare_type,\n+\t\t\t       adjust, nscalars_skip);\n       test_limit = gimple_build (preheader_seq, MAX_EXPR, compare_type,\n-\t\t\t\t nscalars_total, nscalars_step);\n+\t\t\t\t nscalars_total, adjust);\n       test_limit = gimple_build (preheader_seq, MINUS_EXPR, compare_type,\n-\t\t\t\t test_limit, nscalars_step);\n+\t\t\t\t test_limit, adjust);\n       test_gsi = &incr_gsi;\n+\n+      /* Get a safe limit for the first iteration.  */\n+      if (nscalars_skip)\n+\t{\n+\t  /* The first vector iteration can handle at most NSCALARS_STEP\n+\t     scalars.  NSCALARS_STEP <= CONST_LIMIT, and adding\n+\t     NSCALARS_SKIP to that cannot overflow.  */\n+\t  tree const_limit = build_int_cst (compare_type,\n+\t\t\t\t\t    LOOP_VINFO_VECT_FACTOR (loop_vinfo)\n+\t\t\t\t\t    * nscalars_per_iter);\n+\t  first_limit = gimple_build (preheader_seq, MIN_EXPR, compare_type,\n+\t\t\t\t      nscalars_total, const_limit);\n+\t  first_limit = gimple_build (preheader_seq, PLUS_EXPR, compare_type,\n+\t\t\t\t      first_limit, nscalars_skip);\n+\t}\n+      else\n+\t/* For the first iteration it doesn't matter whether the IV hits\n+\t   a value above NSCALARS_TOTAL.  That only matters for the latch\n+\t   condition.  */\n+\tfirst_limit = nscalars_total;\n     }\n   else\n     {\n       /* Test the incremented IV, which will always hit a value above\n \t the bound before wrapping.  */\n       test_index = index_after_incr;\n       test_limit = nscalars_total;\n+      if (nscalars_skip)\n+\ttest_limit = gimple_build (preheader_seq, PLUS_EXPR, compare_type,\n+\t\t\t\t   test_limit, nscalars_skip);\n       test_gsi = &loop_cond_gsi;\n+\n+      first_limit = test_limit;\n     }\n \n   /* Provide a definition of each mask in the group.  */\n@@ -487,7 +540,7 @@ vect_set_loop_masks_directly (struct loop *loop, loop_vec_info loop_vinfo,\n \t to have a full mask.  */\n       poly_uint64 const_limit;\n       bool first_iteration_full\n-\t= (poly_int_tree_p (nscalars_total, &const_limit)\n+\t= (poly_int_tree_p (first_limit, &const_limit)\n \t   && known_ge (const_limit, (i + 1) * nscalars_per_mask));\n \n       /* Rather than have a new IV that starts at BIAS and goes up to\n@@ -504,12 +557,13 @@ vect_set_loop_masks_directly (struct loop *loop, loop_vec_info loop_vinfo,\n \t\t\t\t\t  bias_tree);\n \t}\n \n-      /* Create the initial mask.  */\n+      /* Create the initial mask.  First include all scalars that\n+\t are within the loop limit.  */\n       tree init_mask = NULL_TREE;\n       if (!first_iteration_full)\n \t{\n \t  tree start, end;\n-\t  if (nscalars_total == test_limit)\n+\t  if (first_limit == test_limit)\n \t    {\n \t      /* Use a natural test between zero (the initial IV value)\n \t\t and the loop limit.  The \"else\" block would be valid too,\n@@ -520,15 +574,34 @@ vect_set_loop_masks_directly (struct loop *loop, loop_vec_info loop_vinfo,\n \t    }\n \t  else\n \t    {\n+\t      /* FIRST_LIMIT is the maximum number of scalars handled by the\n+\t\t first iteration of the vector loop.  Test the portion\n+\t\t associated with this mask.  */\n \t      start = bias_tree;\n-\t      end = nscalars_total;\n+\t      end = first_limit;\n \t    }\n \n \t  init_mask = make_temp_ssa_name (mask_type, NULL, \"max_mask\");\n \t  tmp_stmt = vect_gen_while (init_mask, start, end);\n \t  gimple_seq_add_stmt (preheader_seq, tmp_stmt);\n \t}\n \n+      /* Now AND out the bits that are within the number of skipped\n+\t scalars.  */\n+      poly_uint64 const_skip;\n+      if (nscalars_skip\n+\t  && !(poly_int_tree_p (nscalars_skip, &const_skip)\n+\t       && known_le (const_skip, bias)))\n+\t{\n+\t  tree unskipped_mask = vect_gen_while_not (preheader_seq, mask_type,\n+\t\t\t\t\t\t    bias_tree, nscalars_skip);\n+\t  if (init_mask)\n+\t    init_mask = gimple_build (preheader_seq, BIT_AND_EXPR, mask_type,\n+\t\t\t\t      init_mask, unskipped_mask);\n+\t  else\n+\t    init_mask = unskipped_mask;\n+\t}\n+\n       if (!init_mask)\n \t/* First iteration is full.  */\n \tinit_mask = build_minus_one_cst (mask_type);\n@@ -586,6 +659,9 @@ vect_set_loop_condition_masked (struct loop *loop, loop_vec_info loop_vinfo,\n   else\n     niters = gimple_convert (&preheader_seq, compare_type, niters);\n \n+  /* Convert skip_niters to the right type.  */\n+  tree niters_skip = LOOP_VINFO_MASK_SKIP_NITERS (loop_vinfo);\n+\n   /* Now calculate the value that the induction variable must be able\n      to hit in order to ensure that we end the loop with an all-false mask.\n      This involves adding the maximum number of inactive trailing scalar\n@@ -594,6 +670,15 @@ vect_set_loop_condition_masked (struct loop *loop, loop_vec_info loop_vinfo,\n   bool known_max_iters = max_loop_iterations (loop, &iv_limit);\n   if (known_max_iters)\n     {\n+      if (niters_skip)\n+\t{\n+\t  /* Add the maximum number of skipped iterations to the\n+\t     maximum iteration count.  */\n+\t  if (TREE_CODE (niters_skip) == INTEGER_CST)\n+\t    iv_limit += wi::to_widest (niters_skip);\n+\t  else\n+\t    iv_limit += max_vf - 1;\n+\t}\n       /* IV_LIMIT is the maximum number of latch iterations, which is also\n \t the maximum in-range IV value.  Round this value down to the previous\n \t vector alignment boundary and then add an extra full iteration.  */\n@@ -639,7 +724,8 @@ vect_set_loop_condition_masked (struct loop *loop, loop_vec_info loop_vinfo,\n \ttest_mask = vect_set_loop_masks_directly (loop, loop_vinfo,\n \t\t\t\t\t\t  &preheader_seq,\n \t\t\t\t\t\t  loop_cond_gsi, rgm, vf,\n-\t\t\t\t\t\t  niters, might_wrap_p);\n+\t\t\t\t\t\t  niters, niters_skip,\n+\t\t\t\t\t\t  might_wrap_p);\n       }\n \n   /* Emit all accumulated statements.  */\n@@ -1463,6 +1549,46 @@ vect_update_ivs_after_vectorizer (loop_vec_info loop_vinfo,\n     }\n }\n \n+/* Return a gimple value containing the misalignment (measured in vector\n+   elements) for the loop described by LOOP_VINFO, i.e. how many elements\n+   it is away from a perfectly aligned address.  Add any new statements\n+   to SEQ.  */\n+\n+static tree\n+get_misalign_in_elems (gimple **seq, loop_vec_info loop_vinfo)\n+{\n+  struct data_reference *dr = LOOP_VINFO_UNALIGNED_DR (loop_vinfo);\n+  gimple *dr_stmt = DR_STMT (dr);\n+  stmt_vec_info stmt_info = vinfo_for_stmt (dr_stmt);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+\n+  unsigned int target_align = DR_TARGET_ALIGNMENT (dr);\n+  gcc_assert (target_align != 0);\n+\n+  bool negative = tree_int_cst_compare (DR_STEP (dr), size_zero_node) < 0;\n+  tree offset = (negative\n+\t\t ? size_int (-TYPE_VECTOR_SUBPARTS (vectype) + 1)\n+\t\t : size_zero_node);\n+  tree start_addr = vect_create_addr_base_for_vector_ref (dr_stmt, seq,\n+\t\t\t\t\t\t\t  offset);\n+  tree type = unsigned_type_for (TREE_TYPE (start_addr));\n+  tree target_align_minus_1 = build_int_cst (type, target_align - 1);\n+  HOST_WIDE_INT elem_size\n+    = int_cst_value (TYPE_SIZE_UNIT (TREE_TYPE (vectype)));\n+  tree elem_size_log = build_int_cst (type, exact_log2 (elem_size));\n+\n+  /* Create:  misalign_in_bytes = addr & (target_align - 1).  */\n+  tree int_start_addr = fold_convert (type, start_addr);\n+  tree misalign_in_bytes = fold_build2 (BIT_AND_EXPR, type, int_start_addr,\n+\t\t\t\t\ttarget_align_minus_1);\n+\n+  /* Create:  misalign_in_elems = misalign_in_bytes / element_size.  */\n+  tree misalign_in_elems = fold_build2 (RSHIFT_EXPR, type, misalign_in_bytes,\n+\t\t\t\t\telem_size_log);\n+\n+  return misalign_in_elems;\n+}\n+\n /* Function vect_gen_prolog_loop_niters\n \n    Generate the number of iterations which should be peeled as prolog for the\n@@ -1474,7 +1600,7 @@ vect_update_ivs_after_vectorizer (loop_vec_info loop_vinfo,\n    If the misalignment of DR is known at compile time:\n      addr_mis = int mis = DR_MISALIGNMENT (dr);\n    Else, compute address misalignment in bytes:\n-     addr_mis = addr & (vectype_align - 1)\n+     addr_mis = addr & (target_align - 1)\n \n    prolog_niters = ((VF - addr_mis/elem_size)&(VF-1))/step\n \n@@ -1521,33 +1647,17 @@ vect_gen_prolog_loop_niters (loop_vec_info loop_vinfo,\n     }\n   else\n     {\n-      bool negative = tree_int_cst_compare (DR_STEP (dr), size_zero_node) < 0;\n-      tree offset = negative\n-\t  ? size_int (-TYPE_VECTOR_SUBPARTS (vectype) + 1) : size_zero_node;\n-      tree start_addr = vect_create_addr_base_for_vector_ref (dr_stmt,\n-\t\t\t\t\t\t\t      &stmts, offset);\n-      tree type = unsigned_type_for (TREE_TYPE (start_addr));\n-      tree target_align_minus_1 = build_int_cst (type, target_align - 1);\n+      tree misalign_in_elems = get_misalign_in_elems (&stmts, loop_vinfo);\n+      tree type = TREE_TYPE (misalign_in_elems);\n       HOST_WIDE_INT elem_size\n \t= int_cst_value (TYPE_SIZE_UNIT (TREE_TYPE (vectype)));\n-      tree elem_size_log = build_int_cst (type, exact_log2 (elem_size));\n       HOST_WIDE_INT align_in_elems = target_align / elem_size;\n       tree align_in_elems_minus_1 = build_int_cst (type, align_in_elems - 1);\n       tree align_in_elems_tree = build_int_cst (type, align_in_elems);\n-      tree misalign_in_bytes;\n-      tree misalign_in_elems;\n-\n-      /* Create:  misalign_in_bytes = addr & (target_align - 1).  */\n-      misalign_in_bytes\n-\t= fold_build2 (BIT_AND_EXPR, type, fold_convert (type, start_addr),\n-\t\t       target_align_minus_1);\n-\n-      /* Create:  misalign_in_elems = misalign_in_bytes / element_size.  */\n-      misalign_in_elems\n-\t= fold_build2 (RSHIFT_EXPR, type, misalign_in_bytes, elem_size_log);\n \n       /* Create:  (niters_type) ((align_in_elems - misalign_in_elems)\n \t\t\t\t & (align_in_elems - 1)).  */\n+      bool negative = tree_int_cst_compare (DR_STEP (dr), size_zero_node) < 0;\n       if (negative)\n \titers = fold_build2 (MINUS_EXPR, type, misalign_in_elems,\n \t\t\t     align_in_elems_tree);\n@@ -1587,35 +1697,35 @@ vect_gen_prolog_loop_niters (loop_vec_info loop_vinfo,\n \n /* Function vect_update_init_of_dr\n \n-   NITERS iterations were peeled from LOOP.  DR represents a data reference\n-   in LOOP.  This function updates the information recorded in DR to\n-   account for the fact that the first NITERS iterations had already been\n-   executed.  Specifically, it updates the OFFSET field of DR.  */\n+   If CODE is PLUS, the vector loop starts NITERS iterations after the\n+   scalar one, otherwise CODE is MINUS and the vector loop starts NITERS\n+   iterations before the scalar one (using masking to skip inactive\n+   elements).  This function updates the information recorded in DR to\n+   account for the difference.  Specifically, it updates the OFFSET\n+   field of DR.  */\n \n static void\n-vect_update_init_of_dr (struct data_reference *dr, tree niters)\n+vect_update_init_of_dr (struct data_reference *dr, tree niters, tree_code code)\n {\n   tree offset = DR_OFFSET (dr);\n \n   niters = fold_build2 (MULT_EXPR, sizetype,\n \t\t\tfold_convert (sizetype, niters),\n \t\t\tfold_convert (sizetype, DR_STEP (dr)));\n-  offset = fold_build2 (PLUS_EXPR, sizetype,\n+  offset = fold_build2 (code, sizetype,\n \t\t\tfold_convert (sizetype, offset), niters);\n   DR_OFFSET (dr) = offset;\n }\n \n \n /* Function vect_update_inits_of_drs\n \n-   NITERS iterations were peeled from the loop represented by LOOP_VINFO.\n-   This function updates the information recorded for the data references in\n-   the loop to account for the fact that the first NITERS iterations had\n-   already been executed.  Specifically, it updates the initial_condition of\n-   the access_function of all the data_references in the loop.  */\n+   Apply vect_update_inits_of_dr to all accesses in LOOP_VINFO.\n+   CODE and NITERS are as for vect_update_inits_of_dr.  */\n \n static void\n-vect_update_inits_of_drs (loop_vec_info loop_vinfo, tree niters)\n+vect_update_inits_of_drs (loop_vec_info loop_vinfo, tree niters,\n+\t\t\t  tree_code code)\n {\n   unsigned int i;\n   vec<data_reference_p> datarefs = LOOP_VINFO_DATAREFS (loop_vinfo);\n@@ -1642,9 +1752,57 @@ vect_update_inits_of_drs (loop_vec_info loop_vinfo, tree niters)\n     }\n \n   FOR_EACH_VEC_ELT (datarefs, i, dr)\n-    vect_update_init_of_dr (dr, niters);\n+    vect_update_init_of_dr (dr, niters, code);\n }\n \n+/* For the information recorded in LOOP_VINFO prepare the loop for peeling\n+   by masking.  This involves calculating the number of iterations to\n+   be peeled and then aligning all memory references appropriately.  */\n+\n+void\n+vect_prepare_for_masked_peels (loop_vec_info loop_vinfo)\n+{\n+  tree misalign_in_elems;\n+  tree type = LOOP_VINFO_MASK_COMPARE_TYPE (loop_vinfo);\n+\n+  gcc_assert (vect_use_loop_mask_for_alignment_p (loop_vinfo));\n+\n+  /* From the information recorded in LOOP_VINFO get the number of iterations\n+     that need to be skipped via masking.  */\n+  if (LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo) > 0)\n+    {\n+      poly_int64 misalign = (LOOP_VINFO_VECT_FACTOR (loop_vinfo)\n+\t\t\t     - LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo));\n+      misalign_in_elems = build_int_cst (type, misalign);\n+    }\n+  else\n+    {\n+      gimple_seq seq1 = NULL, seq2 = NULL;\n+      misalign_in_elems = get_misalign_in_elems (&seq1, loop_vinfo);\n+      misalign_in_elems = fold_convert (type, misalign_in_elems);\n+      misalign_in_elems = force_gimple_operand (misalign_in_elems,\n+\t\t\t\t\t\t&seq2, true, NULL_TREE);\n+      gimple_seq_add_seq (&seq1, seq2);\n+      if (seq1)\n+\t{\n+\t  edge pe = loop_preheader_edge (LOOP_VINFO_LOOP (loop_vinfo));\n+\t  basic_block new_bb = gsi_insert_seq_on_edge_immediate (pe, seq1);\n+\t  gcc_assert (!new_bb);\n+\t}\n+    }\n+\n+  if (dump_enabled_p ())\n+    {\n+      dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t       \"misalignment for fully-masked loop: \");\n+      dump_generic_expr (MSG_NOTE, TDF_SLIM, misalign_in_elems);\n+      dump_printf (MSG_NOTE, \"\\n\");\n+    }\n+\n+  LOOP_VINFO_MASK_SKIP_NITERS (loop_vinfo) = misalign_in_elems;\n+\n+  vect_update_inits_of_drs (loop_vinfo, misalign_in_elems, MINUS_EXPR);\n+}\n \n /* This function builds ni_name = number of iterations.  Statements\n    are emitted on the loop preheader edge.  If NEW_VAR_P is not NULL, set\n@@ -2250,7 +2408,9 @@ vect_do_peeling (loop_vec_info loop_vinfo, tree niters, tree nitersm1,\n   int bound_prolog = 0;\n   poly_uint64 bound_scalar = 0;\n   int estimated_vf;\n-  int prolog_peeling = LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo);\n+  int prolog_peeling = 0;\n+  if (!vect_use_loop_mask_for_alignment_p (loop_vinfo))\n+    prolog_peeling = LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo);\n   bool epilog_peeling = (LOOP_VINFO_PEELING_FOR_NITER (loop_vinfo)\n \t\t\t || LOOP_VINFO_PEELING_FOR_GAPS (loop_vinfo));\n \n@@ -2367,7 +2527,7 @@ vect_do_peeling (loop_vec_info loop_vinfo, tree niters, tree nitersm1,\n \t  scale_loop_profile (prolog, prob_prolog, bound_prolog);\n \t}\n       /* Update init address of DRs.  */\n-      vect_update_inits_of_drs (loop_vinfo, niters_prolog);\n+      vect_update_inits_of_drs (loop_vinfo, niters_prolog, PLUS_EXPR);\n       /* Update niters for vector loop.  */\n       LOOP_VINFO_NITERS (loop_vinfo)\n \t= fold_build2 (MINUS_EXPR, type, niters, niters_prolog);"}, {"sha": "d7cc12ff70de778cedf768a7a6f08bc33d617267", "filename": "gcc/tree-vect-loop.c", "status": "modified", "additions": 58, "deletions": 29, "changes": 87, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftree-vect-loop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftree-vect-loop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -1121,6 +1121,7 @@ _loop_vec_info::_loop_vec_info (struct loop *loop_in)\n     versioning_threshold (0),\n     vectorization_factor (0),\n     max_vectorization_factor (0),\n+    mask_skip_niters (NULL_TREE),\n     mask_compare_type (NULL_TREE),\n     unaligned_dr (NULL),\n     peeling_for_alignment (0),\n@@ -2269,16 +2270,6 @@ vect_analyze_loop_2 (loop_vec_info loop_vinfo, bool &fatal)\n \t\t\t \" gaps is required.\\n\");\n     }\n \n-  if (LOOP_VINFO_CAN_FULLY_MASK_P (loop_vinfo)\n-      && LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo))\n-    {\n-      LOOP_VINFO_CAN_FULLY_MASK_P (loop_vinfo) = false;\n-      if (dump_enabled_p ())\n-\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-\t\t\t \"can't use a fully-masked loop because peeling for\"\n-\t\t\t \" alignment is required.\\n\");\n-    }\n-\n   /* Decide whether to use a fully-masked loop for this vectorization\n      factor.  */\n   LOOP_VINFO_FULLY_MASKED_P (loop_vinfo)\n@@ -2379,18 +2370,21 @@ vect_analyze_loop_2 (loop_vec_info loop_vinfo, bool &fatal)\n      increase threshold for this case if necessary.  */\n   if (LOOP_REQUIRES_VERSIONING (loop_vinfo))\n     {\n-      poly_uint64 niters_th;\n+      poly_uint64 niters_th = 0;\n \n-      /* Niters for peeled prolog loop.  */\n-      if (LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo) < 0)\n+      if (!vect_use_loop_mask_for_alignment_p (loop_vinfo))\n \t{\n-\t  struct data_reference *dr = LOOP_VINFO_UNALIGNED_DR (loop_vinfo);\n-\t  tree vectype = STMT_VINFO_VECTYPE (vinfo_for_stmt (DR_STMT (dr)));\n-\n-\t  niters_th = TYPE_VECTOR_SUBPARTS (vectype) - 1;\n+\t  /* Niters for peeled prolog loop.  */\n+\t  if (LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo) < 0)\n+\t    {\n+\t      struct data_reference *dr = LOOP_VINFO_UNALIGNED_DR (loop_vinfo);\n+\t      tree vectype\n+\t\t= STMT_VINFO_VECTYPE (vinfo_for_stmt (DR_STMT (dr)));\n+\t      niters_th += TYPE_VECTOR_SUBPARTS (vectype) - 1;\n+\t    }\n+\t  else\n+\t    niters_th += LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo);\n \t}\n-      else\n-\tniters_th = LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo);\n \n       /* Niters for at least one iteration of vectorized loop.  */\n       if (!LOOP_VINFO_FULLY_MASKED_P (loop_vinfo))\n@@ -7336,9 +7330,28 @@ vectorizable_induction (gimple *phi,\n   init_expr = PHI_ARG_DEF_FROM_EDGE (phi,\n \t\t\t\t     loop_preheader_edge (iv_loop));\n \n-  /* Convert the step to the desired type.  */\n+  /* Convert the initial value and step to the desired type.  */\n   stmts = NULL;\n+  init_expr = gimple_convert (&stmts, TREE_TYPE (vectype), init_expr);\n   step_expr = gimple_convert (&stmts, TREE_TYPE (vectype), step_expr);\n+\n+  /* If we are using the loop mask to \"peel\" for alignment then we need\n+     to adjust the start value here.  */\n+  tree skip_niters = LOOP_VINFO_MASK_SKIP_NITERS (loop_vinfo);\n+  if (skip_niters != NULL_TREE)\n+    {\n+      if (FLOAT_TYPE_P (vectype))\n+\tskip_niters = gimple_build (&stmts, FLOAT_EXPR, TREE_TYPE (vectype),\n+\t\t\t\t    skip_niters);\n+      else\n+\tskip_niters = gimple_convert (&stmts, TREE_TYPE (vectype),\n+\t\t\t\t      skip_niters);\n+      tree skip_step = gimple_build (&stmts, MULT_EXPR, TREE_TYPE (vectype),\n+\t\t\t\t     skip_niters, step_expr);\n+      init_expr = gimple_build (&stmts, MINUS_EXPR, TREE_TYPE (vectype),\n+\t\t\t\tinit_expr, skip_step);\n+    }\n+\n   if (stmts)\n     {\n       new_bb = gsi_insert_seq_on_edge_immediate (pe, stmts);\n@@ -8209,6 +8222,11 @@ vect_transform_loop (loop_vec_info loop_vinfo)\n \n   split_edge (loop_preheader_edge (loop));\n \n+  if (LOOP_VINFO_FULLY_MASKED_P (loop_vinfo)\n+      && vect_use_loop_mask_for_alignment_p (loop_vinfo))\n+    /* This will deal with any possible peeling.  */\n+    vect_prepare_for_masked_peels (loop_vinfo);\n+\n   /* FORNOW: the vectorizer supports only loops which body consist\n      of one basic block (header + empty latch). When the vectorizer will\n      support more involved loop forms, the order by which the BBs are\n@@ -8488,29 +8506,40 @@ vect_transform_loop (loop_vec_info loop_vinfo)\n   /* +1 to convert latch counts to loop iteration counts,\n      -min_epilogue_iters to remove iterations that cannot be performed\n        by the vector code.  */\n-  int bias = 1 - min_epilogue_iters;\n+  int bias_for_lowest = 1 - min_epilogue_iters;\n+  int bias_for_assumed = bias_for_lowest;\n+  int alignment_npeels = LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo);\n+  if (alignment_npeels && LOOP_VINFO_FULLY_MASKED_P (loop_vinfo))\n+    {\n+      /* When the amount of peeling is known at compile time, the first\n+\t iteration will have exactly alignment_npeels active elements.\n+\t In the worst case it will have at least one.  */\n+      int min_first_active = (alignment_npeels > 0 ? alignment_npeels : 1);\n+      bias_for_lowest += lowest_vf - min_first_active;\n+      bias_for_assumed += assumed_vf - min_first_active;\n+    }\n   /* In these calculations the \"- 1\" converts loop iteration counts\n      back to latch counts.  */\n   if (loop->any_upper_bound)\n     loop->nb_iterations_upper_bound\n       = (final_iter_may_be_partial\n-\t ? wi::udiv_ceil (loop->nb_iterations_upper_bound + bias,\n+\t ? wi::udiv_ceil (loop->nb_iterations_upper_bound + bias_for_lowest,\n \t\t\t  lowest_vf) - 1\n-\t : wi::udiv_floor (loop->nb_iterations_upper_bound + bias,\n+\t : wi::udiv_floor (loop->nb_iterations_upper_bound + bias_for_lowest,\n \t\t\t   lowest_vf) - 1);\n   if (loop->any_likely_upper_bound)\n     loop->nb_iterations_likely_upper_bound\n       = (final_iter_may_be_partial\n-\t ? wi::udiv_ceil (loop->nb_iterations_likely_upper_bound + bias,\n-\t\t\t  lowest_vf) - 1\n-\t : wi::udiv_floor (loop->nb_iterations_likely_upper_bound + bias,\n-\t\t\t   lowest_vf) - 1);\n+\t ? wi::udiv_ceil (loop->nb_iterations_likely_upper_bound\n+\t\t\t  + bias_for_lowest, lowest_vf) - 1\n+\t : wi::udiv_floor (loop->nb_iterations_likely_upper_bound\n+\t\t\t   + bias_for_lowest, lowest_vf) - 1);\n   if (loop->any_estimate)\n     loop->nb_iterations_estimate\n       = (final_iter_may_be_partial\n-\t ? wi::udiv_ceil (loop->nb_iterations_estimate + bias,\n+\t ? wi::udiv_ceil (loop->nb_iterations_estimate + bias_for_assumed,\n \t\t\t  assumed_vf) - 1\n-\t : wi::udiv_floor (loop->nb_iterations_estimate + bias,\n+\t : wi::udiv_floor (loop->nb_iterations_estimate + bias_for_assumed,\n \t\t\t   assumed_vf) - 1);\n \n   if (dump_enabled_p ())"}, {"sha": "c8850d4f3317bc4b11e829aebde72c03ac4fd45f", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -9991,3 +9991,16 @@ vect_gen_while (tree mask, tree start_index, tree end_index)\n   gimple_call_set_lhs (call, mask);\n   return call;\n }\n+\n+/* Generate a vector mask of type MASK_TYPE for which index I is false iff\n+   J + START_INDEX < END_INDEX for all J <= I.  Add the statements to SEQ.  */\n+\n+tree\n+vect_gen_while_not (gimple_seq *seq, tree mask_type, tree start_index,\n+\t\t    tree end_index)\n+{\n+  tree tmp = make_ssa_name (mask_type);\n+  gcall *call = vect_gen_while (tmp, start_index, end_index);\n+  gimple_seq_add_stmt (seq, call);\n+  return gimple_build (seq, BIT_NOT_EXPR, mask_type, tmp);\n+}"}, {"sha": "c01bd9bf105ab498bdb402c1afec377a575dfe55", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/535e7c114a7ad2ad7a6a0def88cf9448fcd5f029/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=535e7c114a7ad2ad7a6a0def88cf9448fcd5f029", "patch": "@@ -351,6 +351,12 @@ typedef struct _loop_vec_info : public vec_info {\n      on inactive scalars.  */\n   vec_loop_masks masks;\n \n+  /* If we are using a loop mask to align memory addresses, this variable\n+     contains the number of vector elements that we should skip in the\n+     first iteration of the vector loop (i.e. the number of leading\n+     elements that should be false in the first mask).  */\n+  tree mask_skip_niters;\n+\n   /* Type of the variables to use in the WHILE_ULT call for fully-masked\n      loops.  */\n   tree mask_compare_type;\n@@ -480,6 +486,7 @@ typedef struct _loop_vec_info : public vec_info {\n #define LOOP_VINFO_VECT_FACTOR(L)          (L)->vectorization_factor\n #define LOOP_VINFO_MAX_VECT_FACTOR(L)      (L)->max_vectorization_factor\n #define LOOP_VINFO_MASKS(L)                (L)->masks\n+#define LOOP_VINFO_MASK_SKIP_NITERS(L)     (L)->mask_skip_niters\n #define LOOP_VINFO_MASK_COMPARE_TYPE(L)    (L)->mask_compare_type\n #define LOOP_VINFO_PTR_MASK(L)             (L)->ptr_mask\n #define LOOP_VINFO_LOOP_NEST(L)            (L)->loop_nest\n@@ -1230,6 +1237,17 @@ unlimited_cost_model (loop_p loop)\n   return (flag_vect_cost_model == VECT_COST_MODEL_UNLIMITED);\n }\n \n+/* Return true if the loop described by LOOP_VINFO is fully-masked and\n+   if the first iteration should use a partial mask in order to achieve\n+   alignment.  */\n+\n+static inline bool\n+vect_use_loop_mask_for_alignment_p (loop_vec_info loop_vinfo)\n+{\n+  return (LOOP_VINFO_FULLY_MASKED_P (loop_vinfo)\n+\t  && LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo));\n+}\n+\n /* Return the number of vectors of type VECTYPE that are needed to get\n    NUNITS elements.  NUNITS should be based on the vectorization factor,\n    so it is always a known multiple of the number of elements in VECTYPE.  */\n@@ -1328,6 +1346,7 @@ extern void vect_loop_versioning (loop_vec_info, unsigned int, bool,\n \t\t\t\t  poly_uint64);\n extern struct loop *vect_do_peeling (loop_vec_info, tree, tree,\n \t\t\t\t     tree *, tree *, tree *, int, bool, bool);\n+extern void vect_prepare_for_masked_peels (loop_vec_info);\n extern source_location find_loop_location (struct loop *);\n extern bool vect_can_advance_ivs_p (loop_vec_info);\n \n@@ -1393,6 +1412,7 @@ extern tree vect_gen_perm_mask_any (tree, const vec_perm_indices &);\n extern tree vect_gen_perm_mask_checked (tree, const vec_perm_indices &);\n extern void optimize_mask_stores (struct loop*);\n extern gcall *vect_gen_while (tree, tree, tree);\n+extern tree vect_gen_while_not (gimple_seq *, tree, tree, tree);\n \n /* In tree-vect-data-refs.c.  */\n extern bool vect_can_force_dr_alignment_p (const_tree, unsigned int);"}]}