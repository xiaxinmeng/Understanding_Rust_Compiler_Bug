{"sha": "ac8e10519a298cdad3acb50506af3eec79995729", "node_id": "MDY6Q29tbWl0NzI0NzEyOmFjOGUxMDUxOWEyOThjZGFkM2FjYjUwNTA2YWYzZWVjNzk5OTU3Mjk=", "commit": {"author": {"name": "Corey Richardson", "email": "corey@octayn.net", "date": "2015-01-06T23:02:00Z"}, "committer": {"name": "Corey Richardson", "email": "corey@octayn.net", "date": "2015-01-06T23:02:00Z"}, "message": "Stricter rules surrounding adjacent nonterminals and sequences", "tree": {"sha": "c6c5ec15bf47d71b45ff455e5745ecd06d81582d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/c6c5ec15bf47d71b45ff455e5745ecd06d81582d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ac8e10519a298cdad3acb50506af3eec79995729", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ac8e10519a298cdad3acb50506af3eec79995729", "html_url": "https://github.com/rust-lang/rust/commit/ac8e10519a298cdad3acb50506af3eec79995729", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ac8e10519a298cdad3acb50506af3eec79995729/comments", "author": {"login": "emberian", "id": 704250, "node_id": "MDQ6VXNlcjcwNDI1MA==", "avatar_url": "https://avatars.githubusercontent.com/u/704250?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emberian", "html_url": "https://github.com/emberian", "followers_url": "https://api.github.com/users/emberian/followers", "following_url": "https://api.github.com/users/emberian/following{/other_user}", "gists_url": "https://api.github.com/users/emberian/gists{/gist_id}", "starred_url": "https://api.github.com/users/emberian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emberian/subscriptions", "organizations_url": "https://api.github.com/users/emberian/orgs", "repos_url": "https://api.github.com/users/emberian/repos", "events_url": "https://api.github.com/users/emberian/events{/privacy}", "received_events_url": "https://api.github.com/users/emberian/received_events", "type": "User", "site_admin": false}, "committer": {"login": "emberian", "id": 704250, "node_id": "MDQ6VXNlcjcwNDI1MA==", "avatar_url": "https://avatars.githubusercontent.com/u/704250?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emberian", "html_url": "https://github.com/emberian", "followers_url": "https://api.github.com/users/emberian/followers", "following_url": "https://api.github.com/users/emberian/following{/other_user}", "gists_url": "https://api.github.com/users/emberian/gists{/gist_id}", "starred_url": "https://api.github.com/users/emberian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emberian/subscriptions", "organizations_url": "https://api.github.com/users/emberian/orgs", "repos_url": "https://api.github.com/users/emberian/repos", "events_url": "https://api.github.com/users/emberian/events{/privacy}", "received_events_url": "https://api.github.com/users/emberian/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "e0b4287df6635158043e7015d89b619af7f7748d", "url": "https://api.github.com/repos/rust-lang/rust/commits/e0b4287df6635158043e7015d89b619af7f7748d", "html_url": "https://github.com/rust-lang/rust/commit/e0b4287df6635158043e7015d89b619af7f7748d"}], "stats": {"total": 112, "additions": 82, "deletions": 30}, "files": [{"sha": "14e0be2cf166add9c482f93564b29c564107d054", "filename": "src/libcore/macros.rs", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/ac8e10519a298cdad3acb50506af3eec79995729/src%2Flibcore%2Fmacros.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac8e10519a298cdad3acb50506af3eec79995729/src%2Flibcore%2Fmacros.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fmacros.rs?ref=ac8e10519a298cdad3acb50506af3eec79995729", "patch": "@@ -186,8 +186,11 @@ macro_rules! write {\n #[macro_export]\n #[stable]\n macro_rules! writeln {\n-    ($dst:expr, $fmt:expr $($arg:tt)*) => (\n+    ($dst:expr, $fmt:expr, $($arg:tt)*) => (\n         write!($dst, concat!($fmt, \"\\n\") $($arg)*)\n+    );\n+    ($dst:expr, $fmt:expr) => (\n+        write!($dst, concat!($fmt, \"\\n\"))\n     )\n }\n "}, {"sha": "9e1b18ad18a85cc90099b6165fbc760165044abc", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 74, "deletions": 29, "changes": 103, "blob_url": "https://github.com/rust-lang/rust/blob/ac8e10519a298cdad3acb50506af3eec79995729/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac8e10519a298cdad3acb50506af3eec79995729/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=ac8e10519a298cdad3acb50506af3eec79995729", "patch": "@@ -292,58 +292,102 @@ fn check_lhs_nt_follows(cx: &mut ExtCtxt, lhs: &NamedMatch, sp: Span) {\n         _ => cx.span_bug(sp, \"wrong-structured lhs for follow check\")\n     };\n \n-    check_matcher(cx, matcher, &Eof);\n+    check_matcher(cx, matcher.iter(), &Eof);\n     // we don't abort on errors on rejection, the driver will do that for us\n     // after parsing/expansion. we can report every error in every macro this way.\n }\n \n-fn check_matcher(cx: &mut ExtCtxt, matcher: &[TokenTree], follow: &Token) {\n+// returns the last token that was checked, for TtSequence. this gets used later on.\n+fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n+-> Option<(Span, Token)> where I: Iterator<Item=&'a TokenTree> {\n     use print::pprust::token_to_string;\n \n-    // 1. If there are no tokens in M, accept\n-    if matcher.is_empty() {\n-        return;\n-    }\n+    let mut last = None;\n \n     // 2. For each token T in M:\n-    let mut tokens = matcher.iter().peekable();\n+    let mut tokens = matcher.peekable();\n     while let Some(token) = tokens.next() {\n-        match *token {\n+        last = match *token {\n             TtToken(sp, MatchNt(ref name, ref frag_spec, _, _)) => {\n                 // ii. If T is a simple NT, look ahead to the next token T' in\n                 // M.\n                 let next_token = match tokens.peek() {\n                     // If T' closes a complex NT, replace T' with F\n-                    Some(&&TtToken(_, CloseDelim(_))) => follow,\n-                    Some(&&TtToken(_, ref tok)) => tok,\n-                    // T' is any NT (this catches complex NTs, the next\n-                    // iteration will die if it's a TtDelimited).\n-                    Some(_) => continue,\n+                    Some(&&TtToken(_, CloseDelim(_))) => follow.clone(),\n+                    Some(&&TtToken(_, ref tok)) => tok.clone(),\n+                    Some(&&TtSequence(sp, _)) => {\n+                        cx.span_err(sp, format!(\"`${0}:{1}` is followed by a sequence \\\n+                                                 repetition, which is not allowed for `{1}` \\\n+                                                 fragments\", name.as_str(), frag_spec.as_str())[]);\n+                        Eof\n+                    },\n+                    // die next iteration\n+                    Some(&&TtDelimited(_, ref delim)) => delim.close_token(),\n                     // else, we're at the end of the macro or sequence\n-                    None => follow\n+                    None => follow.clone()\n                 };\n \n+                let tok = if let TtToken(_, ref tok) = *token { tok } else { unreachable!() };\n                 // If T' is in the set FOLLOW(NT), continue. Else, reject.\n-                match *next_token {\n-                    Eof | MatchNt(..) => continue,\n-                    _ if is_in_follow(cx, next_token, frag_spec.as_str()) => continue,\n-                    ref tok => cx.span_err(sp, format!(\"`${0}:{1}` is followed by `{2}`, which \\\n-                                                        is not allowed for `{1}` fragments\",\n-                                                        name.as_str(), frag_spec.as_str(),\n-                                                        token_to_string(tok))[])\n+                match &next_token {\n+                    &Eof => return Some((sp, tok.clone())),\n+                    _ if is_in_follow(cx, &next_token, frag_spec.as_str()) => continue,\n+                    next => {\n+                        cx.span_err(sp, format!(\"`${0}:{1}` is followed by `{2}`, which \\\n+                                                 is not allowed for `{1}` fragments\",\n+                                                 name.as_str(), frag_spec.as_str(),\n+                                                 token_to_string(next))[]);\n+                        continue\n+                    },\n                 }\n             },\n-            TtSequence(_, ref seq) => {\n+            TtSequence(sp, ref seq) => {\n                 // iii. Else, T is a complex NT.\n                 match seq.separator {\n                     // If T has the form $(...)U+ or $(...)U* for some token U,\n                     // run the algorithm on the contents with F set to U. If it\n                     // accepts, continue, else, reject.\n-                    Some(ref u) => check_matcher(cx, seq.tts[], u),\n-                    // If T has the form $(...)+ or $(...)*, run the algorithm\n-                    // on the contents with F set to EOF. If it accepts,\n-                    // continue, else, reject.\n-                    None => check_matcher(cx, seq.tts[], &Eof)\n+                    Some(ref u) => {\n+                        let last = check_matcher(cx, seq.tts.iter(), u);\n+                        match last {\n+                            // Since the delimiter isn't required after the last repetition, make\n+                            // sure that the *next* token is sane. This doesn't actually compute\n+                            // the FIRST of the rest of the matcher yet, it only considers single\n+                            // tokens and simple NTs. This is imprecise, but conservatively\n+                            // correct.\n+                            Some((span, tok)) => {\n+                                let fol = match tokens.peek() {\n+                                    Some(&&TtToken(_, ref tok)) => tok.clone(),\n+                                    Some(&&TtDelimited(_, ref delim)) => delim.close_token(),\n+                                    Some(_) => {\n+                                        cx.span_err(sp, \"sequence repetition followed by \\\n+                                                another sequence repetition, which is not allowed\");\n+                                        Eof\n+                                    },\n+                                    None => Eof\n+                                };\n+                                check_matcher(cx, Some(&TtToken(span, tok.clone())).into_iter(),\n+                                              &fol)\n+                            },\n+                            None => last,\n+                        }\n+                    },\n+                    // If T has the form $(...)+ or $(...)*, run the algorithm on the contents with\n+                    // F set to the token following the sequence. If it accepts, continue, else,\n+                    // reject.\n+                    None => {\n+                        let fol = match tokens.peek() {\n+                            Some(&&TtToken(_, ref tok)) => tok.clone(),\n+                            Some(&&TtDelimited(_, ref delim)) => delim.close_token(),\n+                            Some(_) => {\n+                                cx.span_err(sp, \"sequence repetition followed by another \\\n+                                             sequence repetition, which is not allowed\");\n+                                Eof\n+                            },\n+                            None => Eof\n+                        };\n+                        check_matcher(cx, seq.tts.iter(), &fol)\n+                    }\n                 }\n             },\n             TtToken(..) => {\n@@ -352,11 +396,12 @@ fn check_matcher(cx: &mut ExtCtxt, matcher: &[TokenTree], follow: &Token) {\n             },\n             TtDelimited(_, ref tts) => {\n                 // if we don't pass in that close delimiter, we'll incorrectly consider the matcher\n-                // `{ $foo:ty }` as having a follow that isn't `}`\n-                check_matcher(cx, tts.tts[], &tts.close_token())\n+                // `{ $foo:ty }` as having a follow that isn't `RBrace`\n+                check_matcher(cx, tts.tts.iter(), &tts.close_token())\n             }\n         }\n     }\n+    last\n }\n \n fn is_in_follow(cx: &ExtCtxt, tok: &Token, frag: &str) -> bool {"}, {"sha": "15f6d88fd8998da324e6d34fd415ec4d6bdedc3b", "filename": "src/test/compile-fail/macro-input-future-proofing.rs", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ac8e10519a298cdad3acb50506af3eec79995729/src%2Ftest%2Fcompile-fail%2Fmacro-input-future-proofing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac8e10519a298cdad3acb50506af3eec79995729/src%2Ftest%2Fcompile-fail%2Fmacro-input-future-proofing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmacro-input-future-proofing.rs?ref=ac8e10519a298cdad3acb50506af3eec79995729", "patch": "@@ -20,6 +20,10 @@ macro_rules! errors_everywhere {\n     ($pa:pat , ) => ();\n     ($pa:pat | ) => (); //~ ERROR `$pa:pat` is followed by `|`\n     ($pa:pat $pb:pat $ty:ty ,) => ();\n+    //~^ ERROR `$pa:pat` is followed by `$pb:pat`, which is not allowed\n+    //~^^ ERROR `$pb:pat` is followed by `$ty:ty`, which is not allowed\n+    ($($ty:ty)* -) => (); //~ ERROR `$ty:ty` is followed by `-`\n+    ($($a:ty, $b:ty)* -) => (); //~ ERROR `$b:ty` is followed by `-`\n     ($($ty:ty)-+) => (); //~ ERROR `$ty:ty` is followed by `-`, which is not allowed for `ty`\n }\n "}]}