{"sha": "9d20a46799178df9d2fb28dfec95ba55cbfb7f9c", "node_id": "MDY6Q29tbWl0NzI0NzEyOjlkMjBhNDY3OTkxNzhkZjlkMmZiMjhkZmVjOTViYTU1Y2JmYjdmOWM=", "commit": {"author": {"name": "Huon Wilson", "email": "dbau.pp+github@gmail.com", "date": "2014-11-19T01:52:44Z"}, "committer": {"name": "Huon Wilson", "email": "dbau.pp+github@gmail.com", "date": "2014-11-19T02:10:43Z"}, "message": "Update src/grammar for language changes.", "tree": {"sha": "97799687a667d4b8b230e43ca061d3fa9ca01bfe", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/97799687a667d4b8b230e43ca061d3fa9ca01bfe"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/9d20a46799178df9d2fb28dfec95ba55cbfb7f9c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/9d20a46799178df9d2fb28dfec95ba55cbfb7f9c", "html_url": "https://github.com/rust-lang/rust/commit/9d20a46799178df9d2fb28dfec95ba55cbfb7f9c", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/9d20a46799178df9d2fb28dfec95ba55cbfb7f9c/comments", "author": {"login": "huonw", "id": 1203825, "node_id": "MDQ6VXNlcjEyMDM4MjU=", "avatar_url": "https://avatars.githubusercontent.com/u/1203825?v=4", "gravatar_id": "", "url": "https://api.github.com/users/huonw", "html_url": "https://github.com/huonw", "followers_url": "https://api.github.com/users/huonw/followers", "following_url": "https://api.github.com/users/huonw/following{/other_user}", "gists_url": "https://api.github.com/users/huonw/gists{/gist_id}", "starred_url": "https://api.github.com/users/huonw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/huonw/subscriptions", "organizations_url": "https://api.github.com/users/huonw/orgs", "repos_url": "https://api.github.com/users/huonw/repos", "events_url": "https://api.github.com/users/huonw/events{/privacy}", "received_events_url": "https://api.github.com/users/huonw/received_events", "type": "User", "site_admin": false}, "committer": {"login": "huonw", "id": 1203825, "node_id": "MDQ6VXNlcjEyMDM4MjU=", "avatar_url": "https://avatars.githubusercontent.com/u/1203825?v=4", "gravatar_id": "", "url": "https://api.github.com/users/huonw", "html_url": "https://github.com/huonw", "followers_url": "https://api.github.com/users/huonw/followers", "following_url": "https://api.github.com/users/huonw/following{/other_user}", "gists_url": "https://api.github.com/users/huonw/gists{/gist_id}", "starred_url": "https://api.github.com/users/huonw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/huonw/subscriptions", "organizations_url": "https://api.github.com/users/huonw/orgs", "repos_url": "https://api.github.com/users/huonw/repos", "events_url": "https://api.github.com/users/huonw/events{/privacy}", "received_events_url": "https://api.github.com/users/huonw/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "5b5638f6863803477d56e200d6a9a208015838c1", "url": "https://api.github.com/repos/rust-lang/rust/commits/5b5638f6863803477d56e200d6a9a208015838c1", "html_url": "https://github.com/rust-lang/rust/commit/5b5638f6863803477d56e200d6a9a208015838c1"}], "stats": {"total": 41, "additions": 21, "deletions": 20}, "files": [{"sha": "e3ff20f7874bf88239f76197eda765a377d7ecf7", "filename": "src/grammar/verify.rs", "status": "modified", "additions": 21, "deletions": 20, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/9d20a46799178df9d2fb28dfec95ba55cbfb7f9c/src%2Fgrammar%2Fverify.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9d20a46799178df9d2fb28dfec95ba55cbfb7f9c/src%2Fgrammar%2Fverify.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fgrammar%2Fverify.rs?ref=9d20a46799178df9d2fb28dfec95ba55cbfb7f9c", "patch": "@@ -26,21 +26,21 @@ use std::io::File;\n \n use syntax::parse;\n use syntax::parse::lexer;\n-use rustc::driver::{session, config};\n+use rustc::session::{mod, config};\n \n use syntax::ast;\n use syntax::ast::Name;\n use syntax::parse::token;\n use syntax::parse::lexer::TokenAndSpan;\n \n-fn parse_token_list(file: &str) -> HashMap<String, Token> {\n-    fn id() -> Token {\n+fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n+    fn id() -> token::Token {\n         token::Ident(ast::Ident { name: Name(0), ctxt: 0, }, token::Plain)\n     }\n \n     let mut res = HashMap::new();\n \n-    res.insert(\"-1\".to_string(), EOF);\n+    res.insert(\"-1\".to_string(), token::Eof);\n \n     for line in file.split('\\n') {\n         let eq = match line.trim().rfind('=') {\n@@ -60,7 +60,7 @@ fn parse_token_list(file: &str) -> HashMap<String, Token> {\n             \"INT_SUFFIX\"        => id(),\n             \"SHL\"               => token::BinOp(token::Shl),\n             \"LBRACE\"            => token::OpenDelim(token::Brace),\n-            \"RARROW\"            => token::Rarrow,\n+            \"RARROW\"            => token::RArrow,\n             \"LIT_STR\"           => token::Literal(token::Str_(Name(0))),\n             \"DOTDOT\"            => token::DotDot,\n             \"MOD_SEP\"           => token::ModSep,\n@@ -78,7 +78,7 @@ fn parse_token_list(file: &str) -> HashMap<String, Token> {\n             \"LIFETIME\"          => token::Lifetime(ast::Ident { name: Name(0), ctxt: 0 }),\n             \"CARET\"             => token::BinOp(token::Caret),\n             \"TILDE\"             => token::Tilde,\n-            \"IDENT\"             => token::Id(),\n+            \"IDENT\"             => id(),\n             \"PLUS\"              => token::BinOp(token::Plus),\n             \"LIT_CHAR\"          => token::Literal(token::Char(Name(0))),\n             \"LIT_BYTE\"          => token::Literal(token::Byte(Name(0))),\n@@ -119,7 +119,7 @@ fn parse_token_list(file: &str) -> HashMap<String, Token> {\n     res\n }\n \n-fn str_to_binop(s: &str) -> BinOpToken {\n+fn str_to_binop(s: &str) -> token::BinOpToken {\n     match s {\n         \"+\"     => token::Plus,\n         \"/\"     => token::Slash,\n@@ -167,7 +167,7 @@ fn count(lit: &str) -> uint {\n     lit.chars().take_while(|c| *c == '#').count()\n }\n \n-fn parse_antlr_token(s: &str, tokens: &HashMap<String, Token>) -> TokenAndSpan {\n+fn parse_antlr_token(s: &str, tokens: &HashMap<String, token::Token>) -> TokenAndSpan {\n     let re = regex!(\n       r\"\\[@(?P<seq>\\d+),(?P<start>\\d+):(?P<end>\\d+)='(?P<content>.+?)',<(?P<toknum>-?\\d+)>,\\d+:\\d+]\"\n     );\n@@ -178,7 +178,7 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, Token>) -> TokenAndSpan {\n     let toknum = m.name(\"toknum\");\n     let content = m.name(\"content\");\n \n-    let proto_tok = tokens.get(&toknum).expect(format!(\"didn't find token {} in the map\",\n+    let proto_tok = tokens.get(toknum).expect(format!(\"didn't find token {} in the map\",\n                                                               toknum).as_slice());\n \n     let nm = parse::token::intern(content);\n@@ -206,7 +206,8 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, Token>) -> TokenAndSpan {\n         ref t => t.clone()\n     };\n \n-    let offset = if real_tok == EOF {\n+    let offset = if real_tok == token::Eof\n+ {\n         1\n     } else {\n         0\n@@ -224,7 +225,7 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, Token>) -> TokenAndSpan {\n     }\n }\n \n-fn tok_cmp(a: &Token, b: &Token) -> bool {\n+fn tok_cmp(a: &token::Token, b: &token::Token) -> bool {\n     match a {\n         &token::Ident(id, _) => match b {\n                 &token::Ident(id2, _) => id == id2,\n@@ -242,25 +243,25 @@ fn main() {\n \n     let args = std::os::args();\n \n-    let mut token_file = File::open(&Path::new(args.get(2).as_slice()));\n+    let mut token_file = File::open(&Path::new(args[2].as_slice()));\n     let token_map = parse_token_list(token_file.read_to_string().unwrap().as_slice());\n \n     let mut stdin = std::io::stdin();\n     let mut antlr_tokens = stdin.lines().map(|l| parse_antlr_token(l.unwrap().as_slice().trim(),\n                                                                    &token_map));\n \n-    let code = File::open(&Path::new(args.get(1).as_slice())).unwrap().read_to_string().unwrap();\n+    let code = File::open(&Path::new(args[1].as_slice())).unwrap().read_to_string().unwrap();\n     let options = config::basic_options();\n     let session = session::build_session(options, None,\n-                                         syntax::diagnostics::registry::Registry::new([]));\n+                                         syntax::diagnostics::registry::Registry::new(&[]));\n     let filemap = parse::string_to_filemap(&session.parse_sess,\n                                            code,\n                                            String::from_str(\"<n/a>\"));\n     let mut lexer = lexer::StringReader::new(session.diagnostic(), filemap);\n \n     for antlr_tok in antlr_tokens {\n         let rustc_tok = next(&mut lexer);\n-        if rustc_tok.tok == EOF && antlr_tok.tok == EOF {\n+        if rustc_tok.tok == token::Eof && antlr_tok.tok == token::Eof {\n             continue\n         }\n \n@@ -294,11 +295,11 @@ fn main() {\n             token::Literal(token::StrRaw(..)),\n             token::Literal(token::Binary(..)),\n             token::Literal(token::BinaryRaw(..)),\n-            Ident(..),\n-            Lifetime(..),\n-            Interpolated(..),\n-            DocComment(..),\n-            Shebang(..)\n+            token::Ident(..),\n+            token::Lifetime(..),\n+            token::Interpolated(..),\n+            token::DocComment(..),\n+            token::Shebang(..)\n         );\n     }\n }"}]}