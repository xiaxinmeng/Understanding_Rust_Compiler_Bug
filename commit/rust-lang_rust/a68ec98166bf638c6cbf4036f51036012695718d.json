{"sha": "a68ec98166bf638c6cbf4036f51036012695718d", "node_id": "MDY6Q29tbWl0NzI0NzEyOmE2OGVjOTgxNjZiZjYzOGM2Y2JmNDAzNmY1MTAzNjAxMjY5NTcxOGQ=", "commit": {"author": {"name": "Aaron Turon", "email": "aturon@mozilla.com", "date": "2014-11-14T22:33:51Z"}, "committer": {"name": "Aaron Turon", "email": "aturon@mozilla.com", "date": "2014-11-21T01:19:24Z"}, "message": "Rewrite sync::mutex as thin layer over native mutexes\n\nPreviously, sync::mutex had to split between green and native runtime\nsystems and thus could not simply use the native mutex facility.\n\nThis commit rewrites sync::mutex to link directly to native mutexes; in\nthe future, the two will probably be coalesced into a single\nmodule (once librustrt is pulled into libstd wholesale).", "tree": {"sha": "7634a3e7aa83af8bc5edf8e13f3addaf4645abd0", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/7634a3e7aa83af8bc5edf8e13f3addaf4645abd0"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a68ec98166bf638c6cbf4036f51036012695718d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a68ec98166bf638c6cbf4036f51036012695718d", "html_url": "https://github.com/rust-lang/rust/commit/a68ec98166bf638c6cbf4036f51036012695718d", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a68ec98166bf638c6cbf4036f51036012695718d/comments", "author": {"login": "aturon", "id": 709807, "node_id": "MDQ6VXNlcjcwOTgwNw==", "avatar_url": "https://avatars.githubusercontent.com/u/709807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aturon", "html_url": "https://github.com/aturon", "followers_url": "https://api.github.com/users/aturon/followers", "following_url": "https://api.github.com/users/aturon/following{/other_user}", "gists_url": "https://api.github.com/users/aturon/gists{/gist_id}", "starred_url": "https://api.github.com/users/aturon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aturon/subscriptions", "organizations_url": "https://api.github.com/users/aturon/orgs", "repos_url": "https://api.github.com/users/aturon/repos", "events_url": "https://api.github.com/users/aturon/events{/privacy}", "received_events_url": "https://api.github.com/users/aturon/received_events", "type": "User", "site_admin": false}, "committer": {"login": "aturon", "id": 709807, "node_id": "MDQ6VXNlcjcwOTgwNw==", "avatar_url": "https://avatars.githubusercontent.com/u/709807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aturon", "html_url": "https://github.com/aturon", "followers_url": "https://api.github.com/users/aturon/followers", "following_url": "https://api.github.com/users/aturon/following{/other_user}", "gists_url": "https://api.github.com/users/aturon/gists{/gist_id}", "starred_url": "https://api.github.com/users/aturon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aturon/subscriptions", "organizations_url": "https://api.github.com/users/aturon/orgs", "repos_url": "https://api.github.com/users/aturon/repos", "events_url": "https://api.github.com/users/aturon/events{/privacy}", "received_events_url": "https://api.github.com/users/aturon/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "91a2c0d51241677d71b8c0abc80535e580fe3939", "url": "https://api.github.com/repos/rust-lang/rust/commits/91a2c0d51241677d71b8c0abc80535e580fe3939", "html_url": "https://github.com/rust-lang/rust/commit/91a2c0d51241677d71b8c0abc80535e580fe3939"}], "stats": {"total": 535, "additions": 12, "deletions": 523}, "files": [{"sha": "ec5b08fa754be9b21da96bab98db8d26dcdafb63", "filename": "src/libsync/lib.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/a68ec98166bf638c6cbf4036f51036012695718d/src%2Flibsync%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a68ec98166bf638c6cbf4036f51036012695718d/src%2Flibsync%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsync%2Flib.rs?ref=a68ec98166bf638c6cbf4036f51036012695718d", "patch": "@@ -54,7 +54,6 @@ pub mod atomic;\n \n // Concurrent data structures\n \n-mod mpsc_intrusive;\n pub mod spsc_queue;\n pub mod mpsc_queue;\n pub mod mpmc_bounded_queue;"}, {"sha": "1f7841de7c128995975ee49a46e230b76d117cae", "filename": "src/libsync/mpsc_intrusive.rs", "status": "removed", "additions": 0, "deletions": 144, "changes": 144, "blob_url": "https://github.com/rust-lang/rust/blob/91a2c0d51241677d71b8c0abc80535e580fe3939/src%2Flibsync%2Fmpsc_intrusive.rs", "raw_url": "https://github.com/rust-lang/rust/raw/91a2c0d51241677d71b8c0abc80535e580fe3939/src%2Flibsync%2Fmpsc_intrusive.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsync%2Fmpsc_intrusive.rs?ref=91a2c0d51241677d71b8c0abc80535e580fe3939", "patch": "@@ -1,144 +0,0 @@\n-/* Copyright (c) 2010-2011 Dmitry Vyukov. All rights reserved.\n- * Redistribution and use in source and binary forms, with or without\n- * modification, are permitted provided that the following conditions are met:\n- *\n- *    1. Redistributions of source code must retain the above copyright notice,\n- *       this list of conditions and the following disclaimer.\n- *\n- *    2. Redistributions in binary form must reproduce the above copyright\n- *       notice, this list of conditions and the following disclaimer in the\n- *       documentation and/or other materials provided with the distribution.\n- *\n- * THIS SOFTWARE IS PROVIDED BY DMITRY VYUKOV \"AS IS\" AND ANY EXPRESS OR IMPLIED\n- * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n- * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO\n- * EVENT SHALL DMITRY VYUKOV OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n- * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,\n- * OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n- * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n- * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n- * EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n- *\n- * The views and conclusions contained in the software and documentation are\n- * those of the authors and should not be interpreted as representing official\n- * policies, either expressed or implied, of Dmitry Vyukov.\n- */\n-\n-//! A mostly lock-free multi-producer, single consumer queue.\n-//!\n-//! This module implements an intrusive MPSC queue. This queue is incredibly\n-//! unsafe (due to use of unsafe pointers for nodes), and hence is not public.\n-\n-#![experimental]\n-\n-// http://www.1024cores.net/home/lock-free-algorithms\n-//                         /queues/intrusive-mpsc-node-based-queue\n-\n-use core::prelude::*;\n-\n-use core::atomic;\n-use core::mem;\n-use core::cell::UnsafeCell;\n-\n-// NB: all links are done as AtomicUint instead of AtomicPtr to allow for static\n-// initialization.\n-\n-pub struct Node<T> {\n-    pub next: atomic::AtomicUint,\n-    pub data: T,\n-}\n-\n-pub struct DummyNode {\n-    pub next: atomic::AtomicUint,\n-}\n-\n-pub struct Queue<T> {\n-    pub head: atomic::AtomicUint,\n-    pub tail: UnsafeCell<*mut Node<T>>,\n-    pub stub: DummyNode,\n-}\n-\n-impl<T: Send> Queue<T> {\n-    pub fn new() -> Queue<T> {\n-        Queue {\n-            head: atomic::AtomicUint::new(0),\n-            tail: UnsafeCell::new(0 as *mut Node<T>),\n-            stub: DummyNode {\n-                next: atomic::AtomicUint::new(0),\n-            },\n-        }\n-    }\n-\n-    pub unsafe fn push(&self, node: *mut Node<T>) {\n-        (*node).next.store(0, atomic::Release);\n-        let prev = self.head.swap(node as uint, atomic::AcqRel);\n-\n-        // Note that this code is slightly modified to allow static\n-        // initialization of these queues with rust's flavor of static\n-        // initialization.\n-        if prev == 0 {\n-            self.stub.next.store(node as uint, atomic::Release);\n-        } else {\n-            let prev = prev as *mut Node<T>;\n-            (*prev).next.store(node as uint, atomic::Release);\n-        }\n-    }\n-\n-    /// You'll note that the other MPSC queue in std::sync is non-intrusive and\n-    /// returns a `PopResult` here to indicate when the queue is inconsistent.\n-    /// An \"inconsistent state\" in the other queue means that a pusher has\n-    /// pushed, but it hasn't finished linking the rest of the chain.\n-    ///\n-    /// This queue also suffers from this problem, but I currently haven't been\n-    /// able to detangle when this actually happens. This code is translated\n-    /// verbatim from the website above, and is more complicated than the\n-    /// non-intrusive version.\n-    ///\n-    /// Right now consumers of this queue must be ready for this fact. Just\n-    /// because `pop` returns `None` does not mean that there is not data\n-    /// on the queue.\n-    pub unsafe fn pop(&self) -> Option<*mut Node<T>> {\n-        let tail = *self.tail.get();\n-        let mut tail = if !tail.is_null() {tail} else {\n-            mem::transmute(&self.stub)\n-        };\n-        let mut next = (*tail).next(atomic::Relaxed);\n-        if tail as uint == &self.stub as *const DummyNode as uint {\n-            if next.is_null() {\n-                return None;\n-            }\n-            *self.tail.get() = next;\n-            tail = next;\n-            next = (*next).next(atomic::Relaxed);\n-        }\n-        if !next.is_null() {\n-            *self.tail.get() = next;\n-            return Some(tail);\n-        }\n-        let head = self.head.load(atomic::Acquire) as *mut Node<T>;\n-        if tail != head {\n-            return None;\n-        }\n-        let stub = mem::transmute(&self.stub);\n-        self.push(stub);\n-        next = (*tail).next(atomic::Relaxed);\n-        if !next.is_null() {\n-            *self.tail.get() = next;\n-            return Some(tail);\n-        }\n-        return None\n-    }\n-}\n-\n-impl<T: Send> Node<T> {\n-    pub fn new(t: T) -> Node<T> {\n-        Node {\n-            data: t,\n-            next: atomic::AtomicUint::new(0),\n-        }\n-    }\n-    pub unsafe fn next(&self, ord: atomic::Ordering) -> *mut Node<T> {\n-        mem::transmute::<uint, *mut Node<T>>(self.next.load(ord))\n-    }\n-}"}, {"sha": "6672126f55c0eb902eaec84238ff1f9ee1124305", "filename": "src/libsync/mutex.rs", "status": "modified", "additions": 12, "deletions": 378, "changes": 390, "blob_url": "https://github.com/rust-lang/rust/blob/a68ec98166bf638c6cbf4036f51036012695718d/src%2Flibsync%2Fmutex.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a68ec98166bf638c6cbf4036f51036012695718d/src%2Flibsync%2Fmutex.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsync%2Fmutex.rs?ref=a68ec98166bf638c6cbf4036f51036012695718d", "patch": "@@ -8,80 +8,22 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-//! A proper mutex implementation regardless of the \"flavor of task\" which is\n-//! acquiring the lock.\n+//! A simple native mutex implementation. Warning: this API is likely\n+//! to change soon.\n \n-// # Implementation of Rust mutexes\n-//\n-// Most answers to the question of \"how do I use a mutex\" are \"use pthreads\",\n-// but for Rust this isn't quite sufficient. Green threads cannot acquire an OS\n-// mutex because they can context switch among many OS threads, leading to\n-// deadlocks with other green threads.\n-//\n-// Another problem for green threads grabbing an OS mutex is that POSIX dictates\n-// that unlocking a mutex on a different thread from where it was locked is\n-// undefined behavior. Remember that green threads can migrate among OS threads,\n-// so this would mean that we would have to pin green threads to OS threads,\n-// which is less than ideal.\n-//\n-// ## Using deschedule/reawaken\n-//\n-// We already have primitives for descheduling/reawakening tasks, so they're the\n-// first obvious choice when implementing a mutex. The idea would be to have a\n-// concurrent queue that everyone is pushed on to, and then the owner of the\n-// mutex is the one popping from the queue.\n-//\n-// Unfortunately, this is not very performant for native tasks. The suspected\n-// reason for this is that each native thread is suspended on its own condition\n-// variable, unique from all the other threads. In this situation, the kernel\n-// has no idea what the scheduling semantics are of the user program, so all of\n-// the threads are distributed among all cores on the system. This ends up\n-// having very expensive wakeups of remote cores high up in the profile when\n-// handing off the mutex among native tasks. On the other hand, when using an OS\n-// mutex, the kernel knows that all native threads are contended on the same\n-// mutex, so they're in theory all migrated to a single core (fast context\n-// switching).\n-//\n-// ## Mixing implementations\n-//\n-// From that above information, we have two constraints. The first is that\n-// green threads can't touch os mutexes, and the second is that native tasks\n-// pretty much *must* touch an os mutex.\n-//\n-// As a compromise, the queueing implementation is used for green threads and\n-// the os mutex is used for native threads (why not have both?). This ends up\n-// leading to fairly decent performance for both native threads and green\n-// threads on various workloads (uncontended and contended).\n-//\n-// The crux of this implementation is an atomic work which is CAS'd on many\n-// times in order to manage a few flags about who's blocking where and whether\n-// it's locked or not.\n+#![allow(dead_code)]\n \n use core::prelude::*;\n use self::Flavor::*;\n \n use alloc::boxed::Box;\n-use core::atomic;\n-use core::mem;\n-use core::cell::UnsafeCell;\n-use rustrt::local::Local;\n use rustrt::mutex;\n-use rustrt::task::{BlockedTask, Task};\n-use rustrt::thread::Thread;\n-\n-use mpsc_intrusive as q;\n \n pub const LOCKED: uint = 1 << 0;\n-pub const GREEN_BLOCKED: uint = 1 << 1;\n-pub const NATIVE_BLOCKED: uint = 1 << 2;\n+pub const BLOCKED: uint = 1 << 1;\n \n /// A mutual exclusion primitive useful for protecting shared data\n ///\n-/// This mutex is an implementation of a lock for all flavors of tasks which may\n-/// be grabbing. A common problem with green threads is that they cannot grab\n-/// locks (if they reschedule during the lock a contender could deadlock the\n-/// system), but this mutex does *not* suffer this problem.\n-///\n /// This mutex will properly block tasks waiting for the lock to become\n /// available. The mutex can also be statically initialized or created via a\n /// `new` constructor.\n@@ -107,14 +49,6 @@ pub struct Mutex {\n     lock: Box<StaticMutex>,\n }\n \n-#[deriving(PartialEq, Show)]\n-enum Flavor {\n-    Unlocked,\n-    TryLockAcquisition,\n-    GreenAcquisition,\n-    NativeAcquisition,\n-}\n-\n /// The static mutex type is provided to allow for static allocation of mutexes.\n ///\n /// Note that this is a separate type because using a Mutex correctly means that\n@@ -137,310 +71,35 @@ enum Flavor {\n /// // lock is unlocked here.\n /// ```\n pub struct StaticMutex {\n-    /// Current set of flags on this mutex\n-    state: atomic::AtomicUint,\n-    /// an OS mutex used by native threads\n     lock: mutex::StaticNativeMutex,\n-\n-    /// Type of locking operation currently on this mutex\n-    flavor: UnsafeCell<Flavor>,\n-    /// uint-cast of the green thread waiting for this mutex\n-    green_blocker: UnsafeCell<uint>,\n-    /// uint-cast of the native thread waiting for this mutex\n-    native_blocker: UnsafeCell<uint>,\n-\n-    /// A concurrent mpsc queue used by green threads, along with a count used\n-    /// to figure out when to dequeue and enqueue.\n-    q: q::Queue<uint>,\n-    green_cnt: atomic::AtomicUint,\n }\n \n /// An RAII implementation of a \"scoped lock\" of a mutex. When this structure is\n /// dropped (falls out of scope), the lock will be unlocked.\n #[must_use]\n pub struct Guard<'a> {\n-    lock: &'a StaticMutex,\n+    guard: mutex::LockGuard<'a>,\n+}\n+\n+fn lift_guard(guard: mutex::LockGuard) -> Guard {\n+    Guard { guard: guard }\n }\n \n /// Static initialization of a mutex. This constant can be used to initialize\n /// other mutex constants.\n pub const MUTEX_INIT: StaticMutex = StaticMutex {\n-    lock: mutex::NATIVE_MUTEX_INIT,\n-    state: atomic::INIT_ATOMIC_UINT,\n-    flavor: UnsafeCell { value: Unlocked },\n-    green_blocker: UnsafeCell { value: 0 },\n-    native_blocker: UnsafeCell { value: 0 },\n-    green_cnt: atomic::INIT_ATOMIC_UINT,\n-    q: q::Queue {\n-        head: atomic::INIT_ATOMIC_UINT,\n-        tail: UnsafeCell { value: 0 as *mut q::Node<uint> },\n-        stub: q::DummyNode {\n-            next: atomic::INIT_ATOMIC_UINT,\n-        }\n-    }\n+    lock: mutex::NATIVE_MUTEX_INIT\n };\n \n impl StaticMutex {\n     /// Attempts to grab this lock, see `Mutex::try_lock`\n     pub fn try_lock<'a>(&'a self) -> Option<Guard<'a>> {\n-        // Attempt to steal the mutex from an unlocked state.\n-        //\n-        // FIXME: this can mess up the fairness of the mutex, seems bad\n-        match self.state.compare_and_swap(0, LOCKED, atomic::SeqCst) {\n-            0 => {\n-                // After acquiring the mutex, we can safely access the inner\n-                // fields.\n-                let prev = unsafe {\n-                    mem::replace(&mut *self.flavor.get(), TryLockAcquisition)\n-                };\n-                assert_eq!(prev, Unlocked);\n-                Some(Guard::new(self))\n-            }\n-            _ => None\n-        }\n+        unsafe { self.lock.trylock().map(lift_guard) }\n     }\n \n     /// Acquires this lock, see `Mutex::lock`\n     pub fn lock<'a>(&'a self) -> Guard<'a> {\n-        // First, attempt to steal the mutex from an unlocked state. The \"fast\n-        // path\" needs to have as few atomic instructions as possible, and this\n-        // one cmpxchg is already pretty expensive.\n-        //\n-        // FIXME: this can mess up the fairness of the mutex, seems bad\n-        match self.try_lock() {\n-            Some(guard) => return guard,\n-            None => {}\n-        }\n-\n-        // After we've failed the fast path, then we delegate to the different\n-        // locking protocols for green/native tasks. This will select two tasks\n-        // to continue further (one native, one green).\n-        let t: Box<Task> = Local::take();\n-        let can_block = t.can_block();\n-        let native_bit;\n-        if can_block {\n-            self.native_lock(t);\n-            native_bit = NATIVE_BLOCKED;\n-        } else {\n-            self.green_lock(t);\n-            native_bit = GREEN_BLOCKED;\n-        }\n-\n-        // After we've arbitrated among task types, attempt to re-acquire the\n-        // lock (avoids a deschedule). This is very important to do in order to\n-        // allow threads coming out of the native_lock function to try their\n-        // best to not hit a cvar in deschedule.\n-        let mut old = match self.state.compare_and_swap(0, LOCKED,\n-                                                        atomic::SeqCst) {\n-            0 => {\n-                let flavor = if can_block {\n-                    NativeAcquisition\n-                } else {\n-                    GreenAcquisition\n-                };\n-                // We've acquired the lock, so this unsafe access to flavor is\n-                // allowed.\n-                unsafe { *self.flavor.get() = flavor; }\n-                return Guard::new(self)\n-            }\n-            old => old,\n-        };\n-\n-        // Alright, everything else failed. We need to deschedule ourselves and\n-        // flag ourselves as waiting. Note that this case should only happen\n-        // regularly in native/green contention. Due to try_lock and the header\n-        // of lock stealing the lock, it's also possible for native/native\n-        // contention to hit this location, but as less common.\n-        let t: Box<Task> = Local::take();\n-        t.deschedule(1, |task| {\n-            let task = unsafe { task.cast_to_uint() };\n-\n-            // These accesses are protected by the respective native/green\n-            // mutexes which were acquired above.\n-            let prev = if can_block {\n-                unsafe { mem::replace(&mut *self.native_blocker.get(), task) }\n-            } else {\n-                unsafe { mem::replace(&mut *self.green_blocker.get(), task) }\n-            };\n-            assert_eq!(prev, 0);\n-\n-            loop {\n-                assert_eq!(old & native_bit, 0);\n-                // If the old state was locked, then we need to flag ourselves\n-                // as blocking in the state. If the old state was unlocked, then\n-                // we attempt to acquire the mutex. Everything here is a CAS\n-                // loop that'll eventually make progress.\n-                if old & LOCKED != 0 {\n-                    old = match self.state.compare_and_swap(old,\n-                                                            old | native_bit,\n-                                                            atomic::SeqCst) {\n-                        n if n == old => return Ok(()),\n-                        n => n\n-                    };\n-                } else {\n-                    assert_eq!(old, 0);\n-                    old = match self.state.compare_and_swap(old,\n-                                                            old | LOCKED,\n-                                                            atomic::SeqCst) {\n-                        n if n == old => {\n-                            // After acquiring the lock, we have access to the\n-                            // flavor field, and we've regained access to our\n-                            // respective native/green blocker field.\n-                            let prev = if can_block {\n-                                unsafe {\n-                                    *self.native_blocker.get() = 0;\n-                                    mem::replace(&mut *self.flavor.get(),\n-                                                 NativeAcquisition)\n-                                }\n-                            } else {\n-                                unsafe {\n-                                    *self.green_blocker.get() = 0;\n-                                    mem::replace(&mut *self.flavor.get(),\n-                                                 GreenAcquisition)\n-                                }\n-                            };\n-                            assert_eq!(prev, Unlocked);\n-                            return Err(unsafe {\n-                                BlockedTask::cast_from_uint(task)\n-                            })\n-                        }\n-                        n => n,\n-                    };\n-                }\n-            }\n-        });\n-\n-        Guard::new(self)\n-    }\n-\n-    // Tasks which can block are super easy. These tasks just call the blocking\n-    // `lock()` function on an OS mutex\n-    fn native_lock(&self, t: Box<Task>) {\n-        Local::put(t);\n-        unsafe { self.lock.lock_noguard(); }\n-    }\n-\n-    fn native_unlock(&self) {\n-        unsafe { self.lock.unlock_noguard(); }\n-    }\n-\n-    fn green_lock(&self, t: Box<Task>) {\n-        // Green threads flag their presence with an atomic counter, and if they\n-        // fail to be the first to the mutex, they enqueue themselves on a\n-        // concurrent internal queue with a stack-allocated node.\n-        //\n-        // FIXME: There isn't a cancellation currently of an enqueue, forcing\n-        //        the unlocker to spin for a bit.\n-        if self.green_cnt.fetch_add(1, atomic::SeqCst) == 0 {\n-            Local::put(t);\n-            return\n-        }\n-\n-        let mut node = q::Node::new(0);\n-        t.deschedule(1, |task| {\n-            unsafe {\n-                node.data = task.cast_to_uint();\n-                self.q.push(&mut node);\n-            }\n-            Ok(())\n-        });\n-    }\n-\n-    fn green_unlock(&self) {\n-        // If we're the only green thread, then no need to check the queue,\n-        // otherwise the fixme above forces us to spin for a bit.\n-        if self.green_cnt.fetch_sub(1, atomic::SeqCst) == 1 { return }\n-        let node;\n-        loop {\n-            match unsafe { self.q.pop() } {\n-                Some(t) => { node = t; break; }\n-                None => Thread::yield_now(),\n-            }\n-        }\n-        let task = unsafe { BlockedTask::cast_from_uint((*node).data) };\n-        task.wake().map(|t| t.reawaken());\n-    }\n-\n-    fn unlock(&self) {\n-        // Unlocking this mutex is a little tricky. We favor any task that is\n-        // manually blocked (not in each of the separate locks) in order to help\n-        // provide a little fairness (green threads will wake up the pending\n-        // native thread and native threads will wake up the pending green\n-        // thread).\n-        //\n-        // There's also the question of when we unlock the actual green/native\n-        // locking halves as well. If we're waking up someone, then we can wait\n-        // to unlock until we've acquired the task to wake up (we're guaranteed\n-        // the mutex memory is still valid when there's contenders), but as soon\n-        // as we don't find any contenders we must unlock the mutex, and *then*\n-        // flag the mutex as unlocked.\n-        //\n-        // This flagging can fail, leading to another round of figuring out if a\n-        // task needs to be woken, and in this case it's ok that the \"mutex\n-        // halves\" are unlocked, we're just mainly dealing with the atomic state\n-        // of the outer mutex.\n-        let flavor = unsafe { mem::replace(&mut *self.flavor.get(), Unlocked) };\n-\n-        let mut state = self.state.load(atomic::SeqCst);\n-        let mut unlocked = false;\n-        let task;\n-        loop {\n-            assert!(state & LOCKED != 0);\n-            if state & GREEN_BLOCKED != 0 {\n-                self.unset(state, GREEN_BLOCKED);\n-                task = unsafe {\n-                    *self.flavor.get() = GreenAcquisition;\n-                    let task = mem::replace(&mut *self.green_blocker.get(), 0);\n-                    BlockedTask::cast_from_uint(task)\n-                };\n-                break;\n-            } else if state & NATIVE_BLOCKED != 0 {\n-                self.unset(state, NATIVE_BLOCKED);\n-                task = unsafe {\n-                    *self.flavor.get() = NativeAcquisition;\n-                    let task = mem::replace(&mut *self.native_blocker.get(), 0);\n-                    BlockedTask::cast_from_uint(task)\n-                };\n-                break;\n-            } else {\n-                assert_eq!(state, LOCKED);\n-                if !unlocked {\n-                    match flavor {\n-                        GreenAcquisition => { self.green_unlock(); }\n-                        NativeAcquisition => { self.native_unlock(); }\n-                        TryLockAcquisition => {}\n-                        Unlocked => unreachable!(),\n-                    }\n-                    unlocked = true;\n-                }\n-                match self.state.compare_and_swap(LOCKED, 0, atomic::SeqCst) {\n-                    LOCKED => return,\n-                    n => { state = n; }\n-                }\n-            }\n-        }\n-        if !unlocked {\n-            match flavor {\n-                GreenAcquisition => { self.green_unlock(); }\n-                NativeAcquisition => { self.native_unlock(); }\n-                TryLockAcquisition => {}\n-                Unlocked => unreachable!(),\n-            }\n-        }\n-\n-        task.wake().map(|t| t.reawaken());\n-    }\n-\n-    /// Loops around a CAS to unset the `bit` in `state`\n-    fn unset(&self, mut state: uint, bit: uint) {\n-        loop {\n-            assert!(state & bit != 0);\n-            let new = state ^ bit;\n-            match self.state.compare_and_swap(state, new, atomic::SeqCst) {\n-                n if n == state => break,\n-                n => { state = n; }\n-            }\n-        }\n+        lift_guard(unsafe { self.lock.lock() })\n     }\n \n     /// Deallocates resources associated with this static mutex.\n@@ -463,12 +122,6 @@ impl Mutex {\n     pub fn new() -> Mutex {\n         Mutex {\n             lock: box StaticMutex {\n-                state: atomic::AtomicUint::new(0),\n-                flavor: UnsafeCell::new(Unlocked),\n-                green_blocker: UnsafeCell::new(0),\n-                native_blocker: UnsafeCell::new(0),\n-                green_cnt: atomic::AtomicUint::new(0),\n-                q: q::Queue::new(),\n                 lock: unsafe { mutex::StaticNativeMutex::new() },\n             }\n         }\n@@ -494,25 +147,6 @@ impl Mutex {\n     pub fn lock<'a>(&'a self) -> Guard<'a> { self.lock.lock() }\n }\n \n-impl<'a> Guard<'a> {\n-    fn new<'b>(lock: &'b StaticMutex) -> Guard<'b> {\n-        if cfg!(debug) {\n-            // once we've acquired a lock, it's ok to access the flavor\n-            assert!(unsafe { *lock.flavor.get() != Unlocked });\n-            assert!(lock.state.load(atomic::SeqCst) & LOCKED != 0);\n-        }\n-        Guard { lock: lock }\n-    }\n-}\n-\n-#[unsafe_destructor]\n-impl<'a> Drop for Guard<'a> {\n-    #[inline]\n-    fn drop(&mut self) {\n-        self.lock.unlock();\n-    }\n-}\n-\n impl Drop for Mutex {\n     fn drop(&mut self) {\n         // This is actually safe b/c we know that there is no further usage of"}]}